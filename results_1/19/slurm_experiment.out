Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:23.099127: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:23.099401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:23.100545: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:24.352314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:27.631475 22895071117440 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:27.633154 22895071117440 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:27.932868 22895071117440 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:27.933309 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:27.933574 22895071117440 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.142473 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.142707 22895071117440 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.393123 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.393373 22895071117440 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.737804 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.738036 22895071117440 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.161050 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:29.161287 22895071117440 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.696279 22895071117440 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:29.696517 22895071117440 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:29.734924 22895071117440 run.py:166] Dataset not found in ./datasets_1/19/CLRS30_v1.0.0. Downloading...
I0302 18:56:45.134870 22895071117440 dataset_info.py:482] Load dataset info from ./datasets_1/19/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:45.137414 22895071117440 dataset_info.py:482] Load dataset info from ./datasets_1/19/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:45.138242 22895071117440 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/19/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:56:45.138320 22895071117440 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/19/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:58.862058 22895071117440 run.py:483] Algo bellman_ford step 0 current loss 8.521322, current_train_items 32.
I0302 18:57:01.674481 22895071117440 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.376953125, 'score': 0.376953125, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:01.674681 22895071117440 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.377, val scores are: bellman_ford: 0.377
I0302 18:57:10.660933 22895071117440 run.py:483] Algo bellman_ford step 1 current loss 534.871399, current_train_items 64.
I0302 18:57:20.368029 22895071117440 run.py:483] Algo bellman_ford step 2 current loss 1069523.875000, current_train_items 96.
I0302 18:57:30.265606 22895071117440 run.py:483] Algo bellman_ford step 3 current loss 1388347.750000, current_train_items 128.
I0302 18:57:38.179372 22895071117440 run.py:483] Algo bellman_ford step 4 current loss 15025534.000000, current_train_items 160.
I0302 18:57:38.196825 22895071117440 run.py:483] Algo bellman_ford step 5 current loss 20.420433, current_train_items 192.
I0302 18:57:38.213123 22895071117440 run.py:483] Algo bellman_ford step 6 current loss 82.217194, current_train_items 224.
I0302 18:57:38.234033 22895071117440 run.py:483] Algo bellman_ford step 7 current loss 14170.947266, current_train_items 256.
I0302 18:57:38.261402 22895071117440 run.py:483] Algo bellman_ford step 8 current loss 1532000.000000, current_train_items 288.
I0302 18:57:38.291255 22895071117440 run.py:483] Algo bellman_ford step 9 current loss 252811040.000000, current_train_items 320.
I0302 18:57:38.307832 22895071117440 run.py:483] Algo bellman_ford step 10 current loss 2.733357, current_train_items 352.
I0302 18:57:38.323944 22895071117440 run.py:483] Algo bellman_ford step 11 current loss 45.853161, current_train_items 384.
I0302 18:57:38.344414 22895071117440 run.py:483] Algo bellman_ford step 12 current loss 15167.278320, current_train_items 416.
I0302 18:57:38.372719 22895071117440 run.py:483] Algo bellman_ford step 13 current loss 2705735.000000, current_train_items 448.
I0302 18:57:38.398642 22895071117440 run.py:483] Algo bellman_ford step 14 current loss 171717.953125, current_train_items 480.
I0302 18:57:38.415378 22895071117440 run.py:483] Algo bellman_ford step 15 current loss 2.360960, current_train_items 512.
I0302 18:57:38.430964 22895071117440 run.py:483] Algo bellman_ford step 16 current loss 43.939590, current_train_items 544.
I0302 18:57:38.453658 22895071117440 run.py:483] Algo bellman_ford step 17 current loss 207389.531250, current_train_items 576.
I0302 18:57:38.480086 22895071117440 run.py:483] Algo bellman_ford step 18 current loss 84486.890625, current_train_items 608.
I0302 18:57:38.510429 22895071117440 run.py:483] Algo bellman_ford step 19 current loss 913740.875000, current_train_items 640.
I0302 18:57:38.526803 22895071117440 run.py:483] Algo bellman_ford step 20 current loss 2.267864, current_train_items 672.
I0302 18:57:38.541834 22895071117440 run.py:483] Algo bellman_ford step 21 current loss 111.972435, current_train_items 704.
I0302 18:57:38.563336 22895071117440 run.py:483] Algo bellman_ford step 22 current loss 186532.890625, current_train_items 736.
I0302 18:57:38.589936 22895071117440 run.py:483] Algo bellman_ford step 23 current loss 533684.125000, current_train_items 768.
I0302 18:57:38.618704 22895071117440 run.py:483] Algo bellman_ford step 24 current loss 4618447.000000, current_train_items 800.
I0302 18:57:38.635225 22895071117440 run.py:483] Algo bellman_ford step 25 current loss 6.597663, current_train_items 832.
I0302 18:57:38.650776 22895071117440 run.py:483] Algo bellman_ford step 26 current loss 28.470984, current_train_items 864.
I0302 18:57:38.672854 22895071117440 run.py:483] Algo bellman_ford step 27 current loss 7783.389648, current_train_items 896.
I0302 18:57:38.700429 22895071117440 run.py:483] Algo bellman_ford step 28 current loss 5746.890625, current_train_items 928.
I0302 18:57:38.730782 22895071117440 run.py:483] Algo bellman_ford step 29 current loss 3755043.500000, current_train_items 960.
I0302 18:57:38.747241 22895071117440 run.py:483] Algo bellman_ford step 30 current loss 1.463881, current_train_items 992.
I0302 18:57:38.762029 22895071117440 run.py:483] Algo bellman_ford step 31 current loss 20.132584, current_train_items 1024.
I0302 18:57:38.783101 22895071117440 run.py:483] Algo bellman_ford step 32 current loss 375.600769, current_train_items 1056.
I0302 18:57:38.811426 22895071117440 run.py:483] Algo bellman_ford step 33 current loss 3997.632812, current_train_items 1088.
I0302 18:57:38.840250 22895071117440 run.py:483] Algo bellman_ford step 34 current loss 114265.890625, current_train_items 1120.
I0302 18:57:38.856662 22895071117440 run.py:483] Algo bellman_ford step 35 current loss 2.394713, current_train_items 1152.
I0302 18:57:38.871819 22895071117440 run.py:483] Algo bellman_ford step 36 current loss 9.782114, current_train_items 1184.
I0302 18:57:38.893446 22895071117440 run.py:483] Algo bellman_ford step 37 current loss 264.831879, current_train_items 1216.
I0302 18:57:38.920716 22895071117440 run.py:483] Algo bellman_ford step 38 current loss 492.593414, current_train_items 1248.
W0302 18:57:38.942379 22895071117440 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:57:44.296978 22895071117440 run.py:483] Algo bellman_ford step 39 current loss 9601805312.000000, current_train_items 1280.
I0302 18:57:44.315475 22895071117440 run.py:483] Algo bellman_ford step 40 current loss 2.121446, current_train_items 1312.
I0302 18:57:44.331455 22895071117440 run.py:483] Algo bellman_ford step 41 current loss 18.984236, current_train_items 1344.
I0302 18:57:44.352758 22895071117440 run.py:483] Algo bellman_ford step 42 current loss 215.481339, current_train_items 1376.
I0302 18:57:44.380982 22895071117440 run.py:483] Algo bellman_ford step 43 current loss 491.484528, current_train_items 1408.
I0302 18:57:44.411057 22895071117440 run.py:483] Algo bellman_ford step 44 current loss 212047.250000, current_train_items 1440.
I0302 18:57:44.428945 22895071117440 run.py:483] Algo bellman_ford step 45 current loss 1.341407, current_train_items 1472.
I0302 18:57:44.444938 22895071117440 run.py:483] Algo bellman_ford step 46 current loss 2.174128, current_train_items 1504.
I0302 18:57:44.465663 22895071117440 run.py:483] Algo bellman_ford step 47 current loss 921.568604, current_train_items 1536.
I0302 18:57:44.490609 22895071117440 run.py:483] Algo bellman_ford step 48 current loss 3488.533203, current_train_items 1568.
I0302 18:57:44.517630 22895071117440 run.py:483] Algo bellman_ford step 49 current loss 930669.375000, current_train_items 1600.
I0302 18:57:44.534811 22895071117440 run.py:483] Algo bellman_ford step 50 current loss 1.050509, current_train_items 1632.
I0302 18:57:44.544050 22895071117440 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.50390625, 'score': 0.50390625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:57:44.544161 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.377, current avg val score is 0.504, val scores are: bellman_ford: 0.504
I0302 18:57:44.572205 22895071117440 run.py:483] Algo bellman_ford step 51 current loss 1.803970, current_train_items 1664.
I0302 18:57:44.593692 22895071117440 run.py:483] Algo bellman_ford step 52 current loss 87.622078, current_train_items 1696.
I0302 18:57:44.620291 22895071117440 run.py:483] Algo bellman_ford step 53 current loss 88.268921, current_train_items 1728.
I0302 18:57:44.650557 22895071117440 run.py:483] Algo bellman_ford step 54 current loss 3276.521240, current_train_items 1760.
I0302 18:57:44.668519 22895071117440 run.py:483] Algo bellman_ford step 55 current loss 1.461118, current_train_items 1792.
I0302 18:57:44.684130 22895071117440 run.py:483] Algo bellman_ford step 56 current loss 2.599939, current_train_items 1824.
I0302 18:57:44.705523 22895071117440 run.py:483] Algo bellman_ford step 57 current loss 22.593992, current_train_items 1856.
I0302 18:57:44.731169 22895071117440 run.py:483] Algo bellman_ford step 58 current loss 29.082310, current_train_items 1888.
I0302 18:57:44.761697 22895071117440 run.py:483] Algo bellman_ford step 59 current loss 1393.791992, current_train_items 1920.
I0302 18:57:44.779376 22895071117440 run.py:483] Algo bellman_ford step 60 current loss 0.905504, current_train_items 1952.
W0302 18:57:44.788756 22895071117440 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:57:50.250060 22895071117440 run.py:483] Algo bellman_ford step 61 current loss 1.506772, current_train_items 1984.
I0302 18:57:50.272569 22895071117440 run.py:483] Algo bellman_ford step 62 current loss 3.906679, current_train_items 2016.
I0302 18:57:50.300404 22895071117440 run.py:483] Algo bellman_ford step 63 current loss 15.113038, current_train_items 2048.
I0302 18:57:50.332782 22895071117440 run.py:483] Algo bellman_ford step 64 current loss 974.014648, current_train_items 2080.
I0302 18:57:50.350579 22895071117440 run.py:483] Algo bellman_ford step 65 current loss 0.916930, current_train_items 2112.
I0302 18:57:50.366140 22895071117440 run.py:483] Algo bellman_ford step 66 current loss 1.407429, current_train_items 2144.
I0302 18:57:50.389108 22895071117440 run.py:483] Algo bellman_ford step 67 current loss 4.344729, current_train_items 2176.
I0302 18:57:50.415497 22895071117440 run.py:483] Algo bellman_ford step 68 current loss 5.378597, current_train_items 2208.
I0302 18:57:50.445521 22895071117440 run.py:483] Algo bellman_ford step 69 current loss 34.644207, current_train_items 2240.
I0302 18:57:50.462659 22895071117440 run.py:483] Algo bellman_ford step 70 current loss 0.746356, current_train_items 2272.
I0302 18:57:50.478461 22895071117440 run.py:483] Algo bellman_ford step 71 current loss 1.479273, current_train_items 2304.
I0302 18:57:50.500557 22895071117440 run.py:483] Algo bellman_ford step 72 current loss 2.451507, current_train_items 2336.
I0302 18:57:50.528095 22895071117440 run.py:483] Algo bellman_ford step 73 current loss 3.051417, current_train_items 2368.
I0302 18:57:50.558626 22895071117440 run.py:483] Algo bellman_ford step 74 current loss 6.942666, current_train_items 2400.
I0302 18:57:50.576092 22895071117440 run.py:483] Algo bellman_ford step 75 current loss 0.590290, current_train_items 2432.
I0302 18:57:50.592135 22895071117440 run.py:483] Algo bellman_ford step 76 current loss 1.540401, current_train_items 2464.
I0302 18:57:50.613582 22895071117440 run.py:483] Algo bellman_ford step 77 current loss 2.049411, current_train_items 2496.
I0302 18:57:50.640019 22895071117440 run.py:483] Algo bellman_ford step 78 current loss 2.265960, current_train_items 2528.
I0302 18:57:50.667589 22895071117440 run.py:483] Algo bellman_ford step 79 current loss 29.154510, current_train_items 2560.
I0302 18:57:50.685259 22895071117440 run.py:483] Algo bellman_ford step 80 current loss 0.707730, current_train_items 2592.
I0302 18:57:50.700716 22895071117440 run.py:483] Algo bellman_ford step 81 current loss 1.288884, current_train_items 2624.
I0302 18:57:50.722044 22895071117440 run.py:483] Algo bellman_ford step 82 current loss 1.979054, current_train_items 2656.
I0302 18:57:50.748678 22895071117440 run.py:483] Algo bellman_ford step 83 current loss 2.076466, current_train_items 2688.
I0302 18:57:50.777146 22895071117440 run.py:483] Algo bellman_ford step 84 current loss 2.472832, current_train_items 2720.
I0302 18:57:50.794050 22895071117440 run.py:483] Algo bellman_ford step 85 current loss 0.821862, current_train_items 2752.
I0302 18:57:50.809767 22895071117440 run.py:483] Algo bellman_ford step 86 current loss 1.363314, current_train_items 2784.
I0302 18:57:50.832531 22895071117440 run.py:483] Algo bellman_ford step 87 current loss 2.033693, current_train_items 2816.
I0302 18:57:50.860230 22895071117440 run.py:483] Algo bellman_ford step 88 current loss 2.085520, current_train_items 2848.
I0302 18:57:50.889948 22895071117440 run.py:483] Algo bellman_ford step 89 current loss 2.836584, current_train_items 2880.
I0302 18:57:50.907746 22895071117440 run.py:483] Algo bellman_ford step 90 current loss 0.767379, current_train_items 2912.
I0302 18:57:50.923324 22895071117440 run.py:483] Algo bellman_ford step 91 current loss 1.254941, current_train_items 2944.
I0302 18:57:50.944751 22895071117440 run.py:483] Algo bellman_ford step 92 current loss 1.584228, current_train_items 2976.
I0302 18:57:50.973139 22895071117440 run.py:483] Algo bellman_ford step 93 current loss 2.335372, current_train_items 3008.
I0302 18:57:51.002846 22895071117440 run.py:483] Algo bellman_ford step 94 current loss 8.137512, current_train_items 3040.
I0302 18:57:51.020272 22895071117440 run.py:483] Algo bellman_ford step 95 current loss 0.663150, current_train_items 3072.
I0302 18:57:51.035596 22895071117440 run.py:483] Algo bellman_ford step 96 current loss 1.171746, current_train_items 3104.
I0302 18:57:51.056953 22895071117440 run.py:483] Algo bellman_ford step 97 current loss 1.699195, current_train_items 3136.
I0302 18:57:51.084341 22895071117440 run.py:483] Algo bellman_ford step 98 current loss 1.867191, current_train_items 3168.
I0302 18:57:51.114154 22895071117440 run.py:483] Algo bellman_ford step 99 current loss 2.127589, current_train_items 3200.
I0302 18:57:51.130954 22895071117440 run.py:483] Algo bellman_ford step 100 current loss 0.580614, current_train_items 3232.
I0302 18:57:51.140368 22895071117440 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.7529296875, 'score': 0.7529296875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:57:51.140481 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.504, current avg val score is 0.753, val scores are: bellman_ford: 0.753
I0302 18:57:51.168402 22895071117440 run.py:483] Algo bellman_ford step 101 current loss 1.231654, current_train_items 3264.
I0302 18:57:51.189694 22895071117440 run.py:483] Algo bellman_ford step 102 current loss 1.384429, current_train_items 3296.
I0302 18:57:51.216382 22895071117440 run.py:483] Algo bellman_ford step 103 current loss 1.612252, current_train_items 3328.
I0302 18:57:51.248356 22895071117440 run.py:483] Algo bellman_ford step 104 current loss 2.178340, current_train_items 3360.
I0302 18:57:51.266109 22895071117440 run.py:483] Algo bellman_ford step 105 current loss 0.610663, current_train_items 3392.
I0302 18:57:51.281641 22895071117440 run.py:483] Algo bellman_ford step 106 current loss 1.053692, current_train_items 3424.
I0302 18:57:51.303108 22895071117440 run.py:483] Algo bellman_ford step 107 current loss 1.376450, current_train_items 3456.
I0302 18:57:51.329706 22895071117440 run.py:483] Algo bellman_ford step 108 current loss 1.646323, current_train_items 3488.
I0302 18:57:51.357973 22895071117440 run.py:483] Algo bellman_ford step 109 current loss 1.773212, current_train_items 3520.
I0302 18:57:51.375035 22895071117440 run.py:483] Algo bellman_ford step 110 current loss 0.611406, current_train_items 3552.
I0302 18:57:51.390161 22895071117440 run.py:483] Algo bellman_ford step 111 current loss 1.055553, current_train_items 3584.
I0302 18:57:51.411170 22895071117440 run.py:483] Algo bellman_ford step 112 current loss 1.300969, current_train_items 3616.
I0302 18:57:51.438153 22895071117440 run.py:483] Algo bellman_ford step 113 current loss 1.748589, current_train_items 3648.
I0302 18:57:51.466425 22895071117440 run.py:483] Algo bellman_ford step 114 current loss 1.733621, current_train_items 3680.
I0302 18:57:51.483464 22895071117440 run.py:483] Algo bellman_ford step 115 current loss 0.584422, current_train_items 3712.
I0302 18:57:51.499001 22895071117440 run.py:483] Algo bellman_ford step 116 current loss 1.199782, current_train_items 3744.
I0302 18:57:51.521146 22895071117440 run.py:483] Algo bellman_ford step 117 current loss 1.557534, current_train_items 3776.
I0302 18:57:51.547705 22895071117440 run.py:483] Algo bellman_ford step 118 current loss 1.559240, current_train_items 3808.
I0302 18:57:51.575278 22895071117440 run.py:483] Algo bellman_ford step 119 current loss 1.760549, current_train_items 3840.
I0302 18:57:51.592644 22895071117440 run.py:483] Algo bellman_ford step 120 current loss 0.591931, current_train_items 3872.
I0302 18:57:51.608230 22895071117440 run.py:483] Algo bellman_ford step 121 current loss 0.995167, current_train_items 3904.
I0302 18:57:51.630289 22895071117440 run.py:483] Algo bellman_ford step 122 current loss 1.393496, current_train_items 3936.
I0302 18:57:51.658025 22895071117440 run.py:483] Algo bellman_ford step 123 current loss 1.592688, current_train_items 3968.
I0302 18:57:51.690825 22895071117440 run.py:483] Algo bellman_ford step 124 current loss 1.998756, current_train_items 4000.
I0302 18:57:51.708168 22895071117440 run.py:483] Algo bellman_ford step 125 current loss 0.688412, current_train_items 4032.
I0302 18:57:51.723815 22895071117440 run.py:483] Algo bellman_ford step 126 current loss 1.156940, current_train_items 4064.
I0302 18:57:51.745560 22895071117440 run.py:483] Algo bellman_ford step 127 current loss 1.567795, current_train_items 4096.
I0302 18:57:51.772470 22895071117440 run.py:483] Algo bellman_ford step 128 current loss 1.503905, current_train_items 4128.
I0302 18:57:51.801981 22895071117440 run.py:483] Algo bellman_ford step 129 current loss 1.618588, current_train_items 4160.
I0302 18:57:51.819184 22895071117440 run.py:483] Algo bellman_ford step 130 current loss 0.568822, current_train_items 4192.
I0302 18:57:51.834340 22895071117440 run.py:483] Algo bellman_ford step 131 current loss 0.792579, current_train_items 4224.
I0302 18:57:51.856142 22895071117440 run.py:483] Algo bellman_ford step 132 current loss 1.493046, current_train_items 4256.
I0302 18:57:51.883270 22895071117440 run.py:483] Algo bellman_ford step 133 current loss 1.575957, current_train_items 4288.
I0302 18:57:51.911979 22895071117440 run.py:483] Algo bellman_ford step 134 current loss 1.951168, current_train_items 4320.
I0302 18:57:51.928697 22895071117440 run.py:483] Algo bellman_ford step 135 current loss 0.475688, current_train_items 4352.
I0302 18:57:51.943906 22895071117440 run.py:483] Algo bellman_ford step 136 current loss 1.076179, current_train_items 4384.
I0302 18:57:51.965957 22895071117440 run.py:483] Algo bellman_ford step 137 current loss 1.733542, current_train_items 4416.
I0302 18:57:51.992808 22895071117440 run.py:483] Algo bellman_ford step 138 current loss 1.779529, current_train_items 4448.
I0302 18:57:52.022972 22895071117440 run.py:483] Algo bellman_ford step 139 current loss 1.930337, current_train_items 4480.
I0302 18:57:52.040164 22895071117440 run.py:483] Algo bellman_ford step 140 current loss 0.442602, current_train_items 4512.
I0302 18:57:52.055694 22895071117440 run.py:483] Algo bellman_ford step 141 current loss 1.104515, current_train_items 4544.
I0302 18:57:52.077044 22895071117440 run.py:483] Algo bellman_ford step 142 current loss 1.471515, current_train_items 4576.
I0302 18:57:52.104758 22895071117440 run.py:483] Algo bellman_ford step 143 current loss 1.705484, current_train_items 4608.
I0302 18:57:52.133825 22895071117440 run.py:483] Algo bellman_ford step 144 current loss 1.729812, current_train_items 4640.
I0302 18:57:52.150882 22895071117440 run.py:483] Algo bellman_ford step 145 current loss 0.517149, current_train_items 4672.
I0302 18:57:52.166116 22895071117440 run.py:483] Algo bellman_ford step 146 current loss 1.013432, current_train_items 4704.
I0302 18:57:52.186972 22895071117440 run.py:483] Algo bellman_ford step 147 current loss 1.479543, current_train_items 4736.
I0302 18:57:52.214105 22895071117440 run.py:483] Algo bellman_ford step 148 current loss 1.722564, current_train_items 4768.
I0302 18:57:52.241882 22895071117440 run.py:483] Algo bellman_ford step 149 current loss 1.698582, current_train_items 4800.
I0302 18:57:52.259075 22895071117440 run.py:483] Algo bellman_ford step 150 current loss 0.587148, current_train_items 4832.
I0302 18:57:52.266979 22895071117440 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8408203125, 'score': 0.8408203125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:57:52.267090 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.753, current avg val score is 0.841, val scores are: bellman_ford: 0.841
I0302 18:57:52.294259 22895071117440 run.py:483] Algo bellman_ford step 151 current loss 0.762248, current_train_items 4864.
I0302 18:57:52.315496 22895071117440 run.py:483] Algo bellman_ford step 152 current loss 1.272124, current_train_items 4896.
I0302 18:57:52.342171 22895071117440 run.py:483] Algo bellman_ford step 153 current loss 1.444579, current_train_items 4928.
I0302 18:57:52.375723 22895071117440 run.py:483] Algo bellman_ford step 154 current loss 1.742198, current_train_items 4960.
I0302 18:57:52.393496 22895071117440 run.py:483] Algo bellman_ford step 155 current loss 0.579656, current_train_items 4992.
I0302 18:57:52.409182 22895071117440 run.py:483] Algo bellman_ford step 156 current loss 0.961052, current_train_items 5024.
I0302 18:57:52.430458 22895071117440 run.py:483] Algo bellman_ford step 157 current loss 1.300513, current_train_items 5056.
I0302 18:57:52.456111 22895071117440 run.py:483] Algo bellman_ford step 158 current loss 1.489000, current_train_items 5088.
I0302 18:57:52.485480 22895071117440 run.py:483] Algo bellman_ford step 159 current loss 1.808726, current_train_items 5120.
I0302 18:57:52.502916 22895071117440 run.py:483] Algo bellman_ford step 160 current loss 0.557329, current_train_items 5152.
I0302 18:57:52.518587 22895071117440 run.py:483] Algo bellman_ford step 161 current loss 0.866668, current_train_items 5184.
I0302 18:57:52.539022 22895071117440 run.py:483] Algo bellman_ford step 162 current loss 1.290940, current_train_items 5216.
I0302 18:57:52.565961 22895071117440 run.py:483] Algo bellman_ford step 163 current loss 1.546623, current_train_items 5248.
I0302 18:57:52.594709 22895071117440 run.py:483] Algo bellman_ford step 164 current loss 1.667462, current_train_items 5280.
I0302 18:57:52.611923 22895071117440 run.py:483] Algo bellman_ford step 165 current loss 0.541407, current_train_items 5312.
I0302 18:57:52.627648 22895071117440 run.py:483] Algo bellman_ford step 166 current loss 1.037391, current_train_items 5344.
I0302 18:57:52.648472 22895071117440 run.py:483] Algo bellman_ford step 167 current loss 1.243425, current_train_items 5376.
I0302 18:57:52.675790 22895071117440 run.py:483] Algo bellman_ford step 168 current loss 1.397232, current_train_items 5408.
I0302 18:57:52.704764 22895071117440 run.py:483] Algo bellman_ford step 169 current loss 2.085741, current_train_items 5440.
I0302 18:57:52.722302 22895071117440 run.py:483] Algo bellman_ford step 170 current loss 0.645073, current_train_items 5472.
I0302 18:57:52.737942 22895071117440 run.py:483] Algo bellman_ford step 171 current loss 0.944622, current_train_items 5504.
I0302 18:57:52.758615 22895071117440 run.py:483] Algo bellman_ford step 172 current loss 1.404122, current_train_items 5536.
I0302 18:57:52.785797 22895071117440 run.py:483] Algo bellman_ford step 173 current loss 1.597406, current_train_items 5568.
I0302 18:57:52.818581 22895071117440 run.py:483] Algo bellman_ford step 174 current loss 1.945433, current_train_items 5600.
I0302 18:57:52.835677 22895071117440 run.py:483] Algo bellman_ford step 175 current loss 0.521627, current_train_items 5632.
I0302 18:57:52.851177 22895071117440 run.py:483] Algo bellman_ford step 176 current loss 0.941264, current_train_items 5664.
I0302 18:57:52.873004 22895071117440 run.py:483] Algo bellman_ford step 177 current loss 1.396028, current_train_items 5696.
I0302 18:57:52.898223 22895071117440 run.py:483] Algo bellman_ford step 178 current loss 1.316188, current_train_items 5728.
I0302 18:57:52.926182 22895071117440 run.py:483] Algo bellman_ford step 179 current loss 1.696113, current_train_items 5760.
I0302 18:57:52.943668 22895071117440 run.py:483] Algo bellman_ford step 180 current loss 0.628362, current_train_items 5792.
I0302 18:57:52.959294 22895071117440 run.py:483] Algo bellman_ford step 181 current loss 0.868421, current_train_items 5824.
I0302 18:57:52.981581 22895071117440 run.py:483] Algo bellman_ford step 182 current loss 1.365353, current_train_items 5856.
I0302 18:57:53.008408 22895071117440 run.py:483] Algo bellman_ford step 183 current loss 1.362074, current_train_items 5888.
I0302 18:57:53.035837 22895071117440 run.py:483] Algo bellman_ford step 184 current loss 1.921251, current_train_items 5920.
I0302 18:57:53.052864 22895071117440 run.py:483] Algo bellman_ford step 185 current loss 0.402380, current_train_items 5952.
I0302 18:57:53.068769 22895071117440 run.py:483] Algo bellman_ford step 186 current loss 0.997978, current_train_items 5984.
I0302 18:57:53.089337 22895071117440 run.py:483] Algo bellman_ford step 187 current loss 1.188067, current_train_items 6016.
I0302 18:57:53.115883 22895071117440 run.py:483] Algo bellman_ford step 188 current loss 1.596828, current_train_items 6048.
I0302 18:57:53.146861 22895071117440 run.py:483] Algo bellman_ford step 189 current loss 2.165903, current_train_items 6080.
I0302 18:57:53.164021 22895071117440 run.py:483] Algo bellman_ford step 190 current loss 0.422432, current_train_items 6112.
I0302 18:57:53.179709 22895071117440 run.py:483] Algo bellman_ford step 191 current loss 0.865526, current_train_items 6144.
I0302 18:57:53.201905 22895071117440 run.py:483] Algo bellman_ford step 192 current loss 1.533166, current_train_items 6176.
I0302 18:57:53.228695 22895071117440 run.py:483] Algo bellman_ford step 193 current loss 1.422067, current_train_items 6208.
I0302 18:57:53.257626 22895071117440 run.py:483] Algo bellman_ford step 194 current loss 1.607922, current_train_items 6240.
I0302 18:57:53.275027 22895071117440 run.py:483] Algo bellman_ford step 195 current loss 0.435712, current_train_items 6272.
I0302 18:57:53.290562 22895071117440 run.py:483] Algo bellman_ford step 196 current loss 0.825402, current_train_items 6304.
I0302 18:57:53.311383 22895071117440 run.py:483] Algo bellman_ford step 197 current loss 1.230741, current_train_items 6336.
I0302 18:57:53.339259 22895071117440 run.py:483] Algo bellman_ford step 198 current loss 1.545807, current_train_items 6368.
I0302 18:57:53.369240 22895071117440 run.py:483] Algo bellman_ford step 199 current loss 1.534861, current_train_items 6400.
I0302 18:57:53.386591 22895071117440 run.py:483] Algo bellman_ford step 200 current loss 0.507092, current_train_items 6432.
I0302 18:57:53.394585 22895071117440 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.8486328125, 'score': 0.8486328125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:57:53.394691 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.841, current avg val score is 0.849, val scores are: bellman_ford: 0.849
I0302 18:57:53.425146 22895071117440 run.py:483] Algo bellman_ford step 201 current loss 0.831800, current_train_items 6464.
I0302 18:57:53.447448 22895071117440 run.py:483] Algo bellman_ford step 202 current loss 1.246852, current_train_items 6496.
I0302 18:57:53.476506 22895071117440 run.py:483] Algo bellman_ford step 203 current loss 1.519817, current_train_items 6528.
I0302 18:57:53.508939 22895071117440 run.py:483] Algo bellman_ford step 204 current loss 1.718691, current_train_items 6560.
I0302 18:57:53.526716 22895071117440 run.py:483] Algo bellman_ford step 205 current loss 0.493796, current_train_items 6592.
I0302 18:57:53.541546 22895071117440 run.py:483] Algo bellman_ford step 206 current loss 0.753331, current_train_items 6624.
I0302 18:57:53.563739 22895071117440 run.py:483] Algo bellman_ford step 207 current loss 1.399206, current_train_items 6656.
I0302 18:57:53.591226 22895071117440 run.py:483] Algo bellman_ford step 208 current loss 1.466934, current_train_items 6688.
I0302 18:57:53.619424 22895071117440 run.py:483] Algo bellman_ford step 209 current loss 1.514256, current_train_items 6720.
I0302 18:57:53.636480 22895071117440 run.py:483] Algo bellman_ford step 210 current loss 0.630253, current_train_items 6752.
I0302 18:57:53.651825 22895071117440 run.py:483] Algo bellman_ford step 211 current loss 0.800313, current_train_items 6784.
I0302 18:57:53.673672 22895071117440 run.py:483] Algo bellman_ford step 212 current loss 1.544440, current_train_items 6816.
I0302 18:57:53.701533 22895071117440 run.py:483] Algo bellman_ford step 213 current loss 1.503439, current_train_items 6848.
I0302 18:57:53.727488 22895071117440 run.py:483] Algo bellman_ford step 214 current loss 1.277056, current_train_items 6880.
I0302 18:57:53.744936 22895071117440 run.py:483] Algo bellman_ford step 215 current loss 0.408076, current_train_items 6912.
I0302 18:57:53.760059 22895071117440 run.py:483] Algo bellman_ford step 216 current loss 0.760967, current_train_items 6944.
I0302 18:57:53.781126 22895071117440 run.py:483] Algo bellman_ford step 217 current loss 1.351062, current_train_items 6976.
I0302 18:57:53.807532 22895071117440 run.py:483] Algo bellman_ford step 218 current loss 1.534326, current_train_items 7008.
I0302 18:57:53.835546 22895071117440 run.py:483] Algo bellman_ford step 219 current loss 1.575790, current_train_items 7040.
I0302 18:57:53.852596 22895071117440 run.py:483] Algo bellman_ford step 220 current loss 0.448398, current_train_items 7072.
I0302 18:57:53.867865 22895071117440 run.py:483] Algo bellman_ford step 221 current loss 0.897502, current_train_items 7104.
I0302 18:57:53.890071 22895071117440 run.py:483] Algo bellman_ford step 222 current loss 1.275761, current_train_items 7136.
I0302 18:57:53.916460 22895071117440 run.py:483] Algo bellman_ford step 223 current loss 1.365928, current_train_items 7168.
I0302 18:57:53.945854 22895071117440 run.py:483] Algo bellman_ford step 224 current loss 1.577136, current_train_items 7200.
I0302 18:57:53.962866 22895071117440 run.py:483] Algo bellman_ford step 225 current loss 0.500837, current_train_items 7232.
I0302 18:57:53.978234 22895071117440 run.py:483] Algo bellman_ford step 226 current loss 0.909280, current_train_items 7264.
I0302 18:57:53.999812 22895071117440 run.py:483] Algo bellman_ford step 227 current loss 1.326169, current_train_items 7296.
I0302 18:57:54.026937 22895071117440 run.py:483] Algo bellman_ford step 228 current loss 1.403593, current_train_items 7328.
I0302 18:57:54.056734 22895071117440 run.py:483] Algo bellman_ford step 229 current loss 1.783094, current_train_items 7360.
I0302 18:57:54.073645 22895071117440 run.py:483] Algo bellman_ford step 230 current loss 0.501182, current_train_items 7392.
I0302 18:57:54.089194 22895071117440 run.py:483] Algo bellman_ford step 231 current loss 0.828397, current_train_items 7424.
I0302 18:57:54.111028 22895071117440 run.py:483] Algo bellman_ford step 232 current loss 1.185745, current_train_items 7456.
I0302 18:57:54.138537 22895071117440 run.py:483] Algo bellman_ford step 233 current loss 1.388183, current_train_items 7488.
I0302 18:57:54.168973 22895071117440 run.py:483] Algo bellman_ford step 234 current loss 1.548575, current_train_items 7520.
I0302 18:57:54.186195 22895071117440 run.py:483] Algo bellman_ford step 235 current loss 0.544589, current_train_items 7552.
I0302 18:57:54.201326 22895071117440 run.py:483] Algo bellman_ford step 236 current loss 0.808821, current_train_items 7584.
I0302 18:57:54.222410 22895071117440 run.py:483] Algo bellman_ford step 237 current loss 1.160047, current_train_items 7616.
I0302 18:57:54.249206 22895071117440 run.py:483] Algo bellman_ford step 238 current loss 1.238373, current_train_items 7648.
I0302 18:57:54.278258 22895071117440 run.py:483] Algo bellman_ford step 239 current loss 1.407480, current_train_items 7680.
I0302 18:57:54.295153 22895071117440 run.py:483] Algo bellman_ford step 240 current loss 0.445919, current_train_items 7712.
I0302 18:57:54.310686 22895071117440 run.py:483] Algo bellman_ford step 241 current loss 0.969201, current_train_items 7744.
I0302 18:57:54.331953 22895071117440 run.py:483] Algo bellman_ford step 242 current loss 1.133141, current_train_items 7776.
I0302 18:57:54.358414 22895071117440 run.py:483] Algo bellman_ford step 243 current loss 1.251924, current_train_items 7808.
I0302 18:57:54.385298 22895071117440 run.py:483] Algo bellman_ford step 244 current loss 1.278316, current_train_items 7840.
I0302 18:57:54.402491 22895071117440 run.py:483] Algo bellman_ford step 245 current loss 0.470119, current_train_items 7872.
I0302 18:57:54.417850 22895071117440 run.py:483] Algo bellman_ford step 246 current loss 0.750348, current_train_items 7904.
I0302 18:57:54.438427 22895071117440 run.py:483] Algo bellman_ford step 247 current loss 1.178677, current_train_items 7936.
I0302 18:57:54.466165 22895071117440 run.py:483] Algo bellman_ford step 248 current loss 1.377917, current_train_items 7968.
I0302 18:57:54.495568 22895071117440 run.py:483] Algo bellman_ford step 249 current loss 1.193157, current_train_items 8000.
I0302 18:57:54.512561 22895071117440 run.py:483] Algo bellman_ford step 250 current loss 0.482163, current_train_items 8032.
I0302 18:57:54.520708 22895071117440 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8125, 'score': 0.8125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:57:54.520815 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.812, val scores are: bellman_ford: 0.812
I0302 18:57:54.537072 22895071117440 run.py:483] Algo bellman_ford step 251 current loss 0.909460, current_train_items 8064.
I0302 18:57:54.559674 22895071117440 run.py:483] Algo bellman_ford step 252 current loss 1.133723, current_train_items 8096.
I0302 18:57:54.588021 22895071117440 run.py:483] Algo bellman_ford step 253 current loss 1.282958, current_train_items 8128.
I0302 18:57:54.618988 22895071117440 run.py:483] Algo bellman_ford step 254 current loss 1.432504, current_train_items 8160.
I0302 18:57:54.636768 22895071117440 run.py:483] Algo bellman_ford step 255 current loss 0.570753, current_train_items 8192.
I0302 18:57:54.652378 22895071117440 run.py:483] Algo bellman_ford step 256 current loss 0.801047, current_train_items 8224.
I0302 18:57:54.673045 22895071117440 run.py:483] Algo bellman_ford step 257 current loss 1.014606, current_train_items 8256.
I0302 18:57:54.699826 22895071117440 run.py:483] Algo bellman_ford step 258 current loss 1.306862, current_train_items 8288.
I0302 18:57:54.728105 22895071117440 run.py:483] Algo bellman_ford step 259 current loss 1.415413, current_train_items 8320.
I0302 18:57:54.746198 22895071117440 run.py:483] Algo bellman_ford step 260 current loss 0.489264, current_train_items 8352.
I0302 18:57:54.761640 22895071117440 run.py:483] Algo bellman_ford step 261 current loss 0.816350, current_train_items 8384.
I0302 18:57:54.782837 22895071117440 run.py:483] Algo bellman_ford step 262 current loss 1.230499, current_train_items 8416.
I0302 18:57:54.809175 22895071117440 run.py:483] Algo bellman_ford step 263 current loss 1.238379, current_train_items 8448.
I0302 18:57:54.837182 22895071117440 run.py:483] Algo bellman_ford step 264 current loss 1.340522, current_train_items 8480.
I0302 18:57:54.854361 22895071117440 run.py:483] Algo bellman_ford step 265 current loss 0.428750, current_train_items 8512.
I0302 18:57:54.869858 22895071117440 run.py:483] Algo bellman_ford step 266 current loss 0.789728, current_train_items 8544.
I0302 18:57:54.892189 22895071117440 run.py:483] Algo bellman_ford step 267 current loss 1.240967, current_train_items 8576.
I0302 18:57:54.920281 22895071117440 run.py:483] Algo bellman_ford step 268 current loss 1.412969, current_train_items 8608.
I0302 18:57:54.948177 22895071117440 run.py:483] Algo bellman_ford step 269 current loss 1.378085, current_train_items 8640.
I0302 18:57:54.965891 22895071117440 run.py:483] Algo bellman_ford step 270 current loss 0.447788, current_train_items 8672.
I0302 18:57:54.981065 22895071117440 run.py:483] Algo bellman_ford step 271 current loss 0.656536, current_train_items 8704.
I0302 18:57:55.003213 22895071117440 run.py:483] Algo bellman_ford step 272 current loss 1.100870, current_train_items 8736.
I0302 18:57:55.030308 22895071117440 run.py:483] Algo bellman_ford step 273 current loss 1.275039, current_train_items 8768.
I0302 18:57:55.058137 22895071117440 run.py:483] Algo bellman_ford step 274 current loss 1.394517, current_train_items 8800.
I0302 18:57:55.075879 22895071117440 run.py:483] Algo bellman_ford step 275 current loss 0.497271, current_train_items 8832.
I0302 18:57:55.091473 22895071117440 run.py:483] Algo bellman_ford step 276 current loss 0.669071, current_train_items 8864.
I0302 18:57:55.113822 22895071117440 run.py:483] Algo bellman_ford step 277 current loss 1.329412, current_train_items 8896.
I0302 18:57:55.141685 22895071117440 run.py:483] Algo bellman_ford step 278 current loss 1.650229, current_train_items 8928.
I0302 18:57:55.170925 22895071117440 run.py:483] Algo bellman_ford step 279 current loss 1.523589, current_train_items 8960.
I0302 18:57:55.187940 22895071117440 run.py:483] Algo bellman_ford step 280 current loss 0.490377, current_train_items 8992.
I0302 18:57:55.203654 22895071117440 run.py:483] Algo bellman_ford step 281 current loss 0.884542, current_train_items 9024.
I0302 18:57:55.225675 22895071117440 run.py:483] Algo bellman_ford step 282 current loss 1.078914, current_train_items 9056.
I0302 18:57:55.252543 22895071117440 run.py:483] Algo bellman_ford step 283 current loss 1.324386, current_train_items 9088.
I0302 18:57:55.283023 22895071117440 run.py:483] Algo bellman_ford step 284 current loss 1.759989, current_train_items 9120.
I0302 18:57:55.300575 22895071117440 run.py:483] Algo bellman_ford step 285 current loss 0.442857, current_train_items 9152.
I0302 18:57:55.316372 22895071117440 run.py:483] Algo bellman_ford step 286 current loss 0.827489, current_train_items 9184.
I0302 18:57:55.337352 22895071117440 run.py:483] Algo bellman_ford step 287 current loss 1.147317, current_train_items 9216.
I0302 18:57:55.364915 22895071117440 run.py:483] Algo bellman_ford step 288 current loss 1.403828, current_train_items 9248.
I0302 18:57:55.394708 22895071117440 run.py:483] Algo bellman_ford step 289 current loss 1.564879, current_train_items 9280.
I0302 18:57:55.412372 22895071117440 run.py:483] Algo bellman_ford step 290 current loss 0.385194, current_train_items 9312.
I0302 18:57:55.427842 22895071117440 run.py:483] Algo bellman_ford step 291 current loss 0.816195, current_train_items 9344.
I0302 18:57:55.449179 22895071117440 run.py:483] Algo bellman_ford step 292 current loss 1.019754, current_train_items 9376.
I0302 18:57:55.475943 22895071117440 run.py:483] Algo bellman_ford step 293 current loss 1.244299, current_train_items 9408.
I0302 18:57:55.504832 22895071117440 run.py:483] Algo bellman_ford step 294 current loss 1.339748, current_train_items 9440.
I0302 18:57:55.522132 22895071117440 run.py:483] Algo bellman_ford step 295 current loss 0.385981, current_train_items 9472.
I0302 18:57:55.537047 22895071117440 run.py:483] Algo bellman_ford step 296 current loss 0.758774, current_train_items 9504.
I0302 18:57:55.559071 22895071117440 run.py:483] Algo bellman_ford step 297 current loss 1.107564, current_train_items 9536.
I0302 18:57:55.587003 22895071117440 run.py:483] Algo bellman_ford step 298 current loss 1.270943, current_train_items 9568.
I0302 18:57:55.615515 22895071117440 run.py:483] Algo bellman_ford step 299 current loss 1.333344, current_train_items 9600.
I0302 18:57:55.633532 22895071117440 run.py:483] Algo bellman_ford step 300 current loss 0.452030, current_train_items 9632.
I0302 18:57:55.641448 22895071117440 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.8427734375, 'score': 0.8427734375, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:57:55.641552 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.843, val scores are: bellman_ford: 0.843
I0302 18:57:55.657526 22895071117440 run.py:483] Algo bellman_ford step 301 current loss 0.753487, current_train_items 9664.
I0302 18:57:55.678668 22895071117440 run.py:483] Algo bellman_ford step 302 current loss 1.148813, current_train_items 9696.
I0302 18:57:55.705639 22895071117440 run.py:483] Algo bellman_ford step 303 current loss 1.121130, current_train_items 9728.
I0302 18:57:55.735996 22895071117440 run.py:483] Algo bellman_ford step 304 current loss 1.523035, current_train_items 9760.
I0302 18:57:55.753545 22895071117440 run.py:483] Algo bellman_ford step 305 current loss 0.441219, current_train_items 9792.
I0302 18:57:55.769429 22895071117440 run.py:483] Algo bellman_ford step 306 current loss 0.913757, current_train_items 9824.
I0302 18:57:55.791654 22895071117440 run.py:483] Algo bellman_ford step 307 current loss 1.099435, current_train_items 9856.
I0302 18:57:55.819468 22895071117440 run.py:483] Algo bellman_ford step 308 current loss 1.229005, current_train_items 9888.
I0302 18:57:55.848622 22895071117440 run.py:483] Algo bellman_ford step 309 current loss 1.339323, current_train_items 9920.
I0302 18:57:55.865776 22895071117440 run.py:483] Algo bellman_ford step 310 current loss 0.486817, current_train_items 9952.
I0302 18:57:55.880886 22895071117440 run.py:483] Algo bellman_ford step 311 current loss 0.833457, current_train_items 9984.
I0302 18:57:55.902312 22895071117440 run.py:483] Algo bellman_ford step 312 current loss 1.157629, current_train_items 10016.
I0302 18:57:55.930332 22895071117440 run.py:483] Algo bellman_ford step 313 current loss 1.378991, current_train_items 10048.
I0302 18:57:55.959844 22895071117440 run.py:483] Algo bellman_ford step 314 current loss 1.264711, current_train_items 10080.
I0302 18:57:55.976952 22895071117440 run.py:483] Algo bellman_ford step 315 current loss 0.501284, current_train_items 10112.
I0302 18:57:55.992370 22895071117440 run.py:483] Algo bellman_ford step 316 current loss 0.734835, current_train_items 10144.
I0302 18:57:56.013262 22895071117440 run.py:483] Algo bellman_ford step 317 current loss 0.919460, current_train_items 10176.
I0302 18:57:56.039151 22895071117440 run.py:483] Algo bellman_ford step 318 current loss 1.183500, current_train_items 10208.
I0302 18:57:56.067620 22895071117440 run.py:483] Algo bellman_ford step 319 current loss 1.547514, current_train_items 10240.
I0302 18:57:56.084923 22895071117440 run.py:483] Algo bellman_ford step 320 current loss 0.383740, current_train_items 10272.
I0302 18:57:56.100315 22895071117440 run.py:483] Algo bellman_ford step 321 current loss 0.896088, current_train_items 10304.
I0302 18:57:56.121571 22895071117440 run.py:483] Algo bellman_ford step 322 current loss 1.036294, current_train_items 10336.
I0302 18:57:56.147790 22895071117440 run.py:483] Algo bellman_ford step 323 current loss 0.997143, current_train_items 10368.
I0302 18:57:56.177275 22895071117440 run.py:483] Algo bellman_ford step 324 current loss 1.369233, current_train_items 10400.
I0302 18:57:56.194568 22895071117440 run.py:483] Algo bellman_ford step 325 current loss 0.498330, current_train_items 10432.
I0302 18:57:56.209751 22895071117440 run.py:483] Algo bellman_ford step 326 current loss 0.721325, current_train_items 10464.
I0302 18:57:56.230690 22895071117440 run.py:483] Algo bellman_ford step 327 current loss 1.116844, current_train_items 10496.
I0302 18:57:56.258365 22895071117440 run.py:483] Algo bellman_ford step 328 current loss 1.376460, current_train_items 10528.
I0302 18:57:56.288214 22895071117440 run.py:483] Algo bellman_ford step 329 current loss 1.382913, current_train_items 10560.
I0302 18:57:56.305572 22895071117440 run.py:483] Algo bellman_ford step 330 current loss 0.458754, current_train_items 10592.
I0302 18:57:56.320852 22895071117440 run.py:483] Algo bellman_ford step 331 current loss 0.876917, current_train_items 10624.
I0302 18:57:56.342199 22895071117440 run.py:483] Algo bellman_ford step 332 current loss 1.151341, current_train_items 10656.
I0302 18:57:56.369524 22895071117440 run.py:483] Algo bellman_ford step 333 current loss 1.250970, current_train_items 10688.
I0302 18:57:56.400176 22895071117440 run.py:483] Algo bellman_ford step 334 current loss 1.592150, current_train_items 10720.
I0302 18:57:56.417693 22895071117440 run.py:483] Algo bellman_ford step 335 current loss 0.413951, current_train_items 10752.
I0302 18:57:56.433613 22895071117440 run.py:483] Algo bellman_ford step 336 current loss 0.909261, current_train_items 10784.
I0302 18:57:56.456435 22895071117440 run.py:483] Algo bellman_ford step 337 current loss 1.289776, current_train_items 10816.
I0302 18:57:56.483272 22895071117440 run.py:483] Algo bellman_ford step 338 current loss 1.308823, current_train_items 10848.
I0302 18:57:56.513963 22895071117440 run.py:483] Algo bellman_ford step 339 current loss 1.622680, current_train_items 10880.
I0302 18:57:56.531354 22895071117440 run.py:483] Algo bellman_ford step 340 current loss 0.385240, current_train_items 10912.
I0302 18:57:56.546379 22895071117440 run.py:483] Algo bellman_ford step 341 current loss 0.871610, current_train_items 10944.
I0302 18:57:56.567494 22895071117440 run.py:483] Algo bellman_ford step 342 current loss 1.198200, current_train_items 10976.
I0302 18:57:56.593519 22895071117440 run.py:483] Algo bellman_ford step 343 current loss 1.166069, current_train_items 11008.
I0302 18:57:56.623546 22895071117440 run.py:483] Algo bellman_ford step 344 current loss 1.337218, current_train_items 11040.
I0302 18:57:56.641191 22895071117440 run.py:483] Algo bellman_ford step 345 current loss 0.427470, current_train_items 11072.
I0302 18:57:56.656429 22895071117440 run.py:483] Algo bellman_ford step 346 current loss 0.775965, current_train_items 11104.
I0302 18:57:56.679066 22895071117440 run.py:483] Algo bellman_ford step 347 current loss 1.249459, current_train_items 11136.
I0302 18:57:56.705854 22895071117440 run.py:483] Algo bellman_ford step 348 current loss 1.188800, current_train_items 11168.
I0302 18:57:56.736091 22895071117440 run.py:483] Algo bellman_ford step 349 current loss 1.435553, current_train_items 11200.
I0302 18:57:56.753536 22895071117440 run.py:483] Algo bellman_ford step 350 current loss 0.485834, current_train_items 11232.
I0302 18:57:56.761444 22895071117440 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.8349609375, 'score': 0.8349609375, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:57:56.761548 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.835, val scores are: bellman_ford: 0.835
I0302 18:57:56.777086 22895071117440 run.py:483] Algo bellman_ford step 351 current loss 0.748555, current_train_items 11264.
I0302 18:57:56.799108 22895071117440 run.py:483] Algo bellman_ford step 352 current loss 1.063653, current_train_items 11296.
I0302 18:57:56.826369 22895071117440 run.py:483] Algo bellman_ford step 353 current loss 1.383605, current_train_items 11328.
I0302 18:57:56.858200 22895071117440 run.py:483] Algo bellman_ford step 354 current loss 1.514103, current_train_items 11360.
I0302 18:57:56.875976 22895071117440 run.py:483] Algo bellman_ford step 355 current loss 0.443195, current_train_items 11392.
I0302 18:57:56.891648 22895071117440 run.py:483] Algo bellman_ford step 356 current loss 0.856176, current_train_items 11424.
I0302 18:57:56.913281 22895071117440 run.py:483] Algo bellman_ford step 357 current loss 1.029870, current_train_items 11456.
I0302 18:57:56.941257 22895071117440 run.py:483] Algo bellman_ford step 358 current loss 1.363310, current_train_items 11488.
I0302 18:57:56.972366 22895071117440 run.py:483] Algo bellman_ford step 359 current loss 1.527792, current_train_items 11520.
I0302 18:57:56.990129 22895071117440 run.py:483] Algo bellman_ford step 360 current loss 0.387572, current_train_items 11552.
I0302 18:57:57.006092 22895071117440 run.py:483] Algo bellman_ford step 361 current loss 0.700794, current_train_items 11584.
I0302 18:57:57.026871 22895071117440 run.py:483] Algo bellman_ford step 362 current loss 0.975931, current_train_items 11616.
I0302 18:57:57.053987 22895071117440 run.py:483] Algo bellman_ford step 363 current loss 1.249931, current_train_items 11648.
I0302 18:57:57.081507 22895071117440 run.py:483] Algo bellman_ford step 364 current loss 1.228953, current_train_items 11680.
I0302 18:57:57.099343 22895071117440 run.py:483] Algo bellman_ford step 365 current loss 0.397428, current_train_items 11712.
I0302 18:57:57.114839 22895071117440 run.py:483] Algo bellman_ford step 366 current loss 0.849516, current_train_items 11744.
I0302 18:57:57.135907 22895071117440 run.py:483] Algo bellman_ford step 367 current loss 0.935427, current_train_items 11776.
I0302 18:57:57.164026 22895071117440 run.py:483] Algo bellman_ford step 368 current loss 1.275013, current_train_items 11808.
I0302 18:57:57.191911 22895071117440 run.py:483] Algo bellman_ford step 369 current loss 1.255672, current_train_items 11840.
I0302 18:57:57.209360 22895071117440 run.py:483] Algo bellman_ford step 370 current loss 0.387068, current_train_items 11872.
I0302 18:57:57.224693 22895071117440 run.py:483] Algo bellman_ford step 371 current loss 0.684771, current_train_items 11904.
I0302 18:57:57.246033 22895071117440 run.py:483] Algo bellman_ford step 372 current loss 1.081088, current_train_items 11936.
I0302 18:57:57.273803 22895071117440 run.py:483] Algo bellman_ford step 373 current loss 1.165519, current_train_items 11968.
I0302 18:57:57.305478 22895071117440 run.py:483] Algo bellman_ford step 374 current loss 1.492289, current_train_items 12000.
I0302 18:57:57.322586 22895071117440 run.py:483] Algo bellman_ford step 375 current loss 0.441676, current_train_items 12032.
I0302 18:57:57.337765 22895071117440 run.py:483] Algo bellman_ford step 376 current loss 0.715027, current_train_items 12064.
I0302 18:57:57.360319 22895071117440 run.py:483] Algo bellman_ford step 377 current loss 1.157100, current_train_items 12096.
I0302 18:57:57.387372 22895071117440 run.py:483] Algo bellman_ford step 378 current loss 1.033849, current_train_items 12128.
I0302 18:57:57.417501 22895071117440 run.py:483] Algo bellman_ford step 379 current loss 1.438735, current_train_items 12160.
I0302 18:57:57.435239 22895071117440 run.py:483] Algo bellman_ford step 380 current loss 0.398079, current_train_items 12192.
I0302 18:57:57.451180 22895071117440 run.py:483] Algo bellman_ford step 381 current loss 0.797768, current_train_items 12224.
I0302 18:57:57.473138 22895071117440 run.py:483] Algo bellman_ford step 382 current loss 1.150428, current_train_items 12256.
I0302 18:57:57.501078 22895071117440 run.py:483] Algo bellman_ford step 383 current loss 1.432167, current_train_items 12288.
I0302 18:57:57.531496 22895071117440 run.py:483] Algo bellman_ford step 384 current loss 1.529427, current_train_items 12320.
I0302 18:57:57.549266 22895071117440 run.py:483] Algo bellman_ford step 385 current loss 0.459318, current_train_items 12352.
I0302 18:57:57.564987 22895071117440 run.py:483] Algo bellman_ford step 386 current loss 0.685661, current_train_items 12384.
I0302 18:57:57.586787 22895071117440 run.py:483] Algo bellman_ford step 387 current loss 1.149923, current_train_items 12416.
I0302 18:57:57.613065 22895071117440 run.py:483] Algo bellman_ford step 388 current loss 1.106088, current_train_items 12448.
I0302 18:57:57.643600 22895071117440 run.py:483] Algo bellman_ford step 389 current loss 1.485500, current_train_items 12480.
I0302 18:57:57.660732 22895071117440 run.py:483] Algo bellman_ford step 390 current loss 0.443362, current_train_items 12512.
I0302 18:57:57.676079 22895071117440 run.py:483] Algo bellman_ford step 391 current loss 0.755138, current_train_items 12544.
I0302 18:57:57.697842 22895071117440 run.py:483] Algo bellman_ford step 392 current loss 1.077636, current_train_items 12576.
I0302 18:57:57.726722 22895071117440 run.py:483] Algo bellman_ford step 393 current loss 1.328697, current_train_items 12608.
I0302 18:57:57.755780 22895071117440 run.py:483] Algo bellman_ford step 394 current loss 1.348463, current_train_items 12640.
I0302 18:57:57.773482 22895071117440 run.py:483] Algo bellman_ford step 395 current loss 0.351510, current_train_items 12672.
I0302 18:57:57.789069 22895071117440 run.py:483] Algo bellman_ford step 396 current loss 0.851546, current_train_items 12704.
I0302 18:57:57.810937 22895071117440 run.py:483] Algo bellman_ford step 397 current loss 1.412027, current_train_items 12736.
I0302 18:57:57.839470 22895071117440 run.py:483] Algo bellman_ford step 398 current loss 1.448753, current_train_items 12768.
I0302 18:57:57.867387 22895071117440 run.py:483] Algo bellman_ford step 399 current loss 1.523193, current_train_items 12800.
I0302 18:57:57.884636 22895071117440 run.py:483] Algo bellman_ford step 400 current loss 0.337944, current_train_items 12832.
I0302 18:57:57.892392 22895071117440 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:57:57.892498 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:57:57.908747 22895071117440 run.py:483] Algo bellman_ford step 401 current loss 0.718373, current_train_items 12864.
I0302 18:57:57.930895 22895071117440 run.py:483] Algo bellman_ford step 402 current loss 0.956487, current_train_items 12896.
I0302 18:57:57.958767 22895071117440 run.py:483] Algo bellman_ford step 403 current loss 1.182223, current_train_items 12928.
I0302 18:57:57.987133 22895071117440 run.py:483] Algo bellman_ford step 404 current loss 1.341851, current_train_items 12960.
I0302 18:57:58.004804 22895071117440 run.py:483] Algo bellman_ford step 405 current loss 0.536939, current_train_items 12992.
I0302 18:57:58.019526 22895071117440 run.py:483] Algo bellman_ford step 406 current loss 0.706193, current_train_items 13024.
I0302 18:57:58.041011 22895071117440 run.py:483] Algo bellman_ford step 407 current loss 1.119645, current_train_items 13056.
I0302 18:57:58.069562 22895071117440 run.py:483] Algo bellman_ford step 408 current loss 1.159741, current_train_items 13088.
I0302 18:57:58.099035 22895071117440 run.py:483] Algo bellman_ford step 409 current loss 1.370989, current_train_items 13120.
I0302 18:57:58.116183 22895071117440 run.py:483] Algo bellman_ford step 410 current loss 0.412342, current_train_items 13152.
I0302 18:57:58.131215 22895071117440 run.py:483] Algo bellman_ford step 411 current loss 0.614192, current_train_items 13184.
I0302 18:57:58.152820 22895071117440 run.py:483] Algo bellman_ford step 412 current loss 1.000512, current_train_items 13216.
I0302 18:57:58.180220 22895071117440 run.py:483] Algo bellman_ford step 413 current loss 1.129910, current_train_items 13248.
I0302 18:57:58.210084 22895071117440 run.py:483] Algo bellman_ford step 414 current loss 1.354248, current_train_items 13280.
I0302 18:57:58.227120 22895071117440 run.py:483] Algo bellman_ford step 415 current loss 0.455954, current_train_items 13312.
I0302 18:57:58.242080 22895071117440 run.py:483] Algo bellman_ford step 416 current loss 0.682646, current_train_items 13344.
I0302 18:57:58.264231 22895071117440 run.py:483] Algo bellman_ford step 417 current loss 1.060345, current_train_items 13376.
I0302 18:57:58.293486 22895071117440 run.py:483] Algo bellman_ford step 418 current loss 1.263534, current_train_items 13408.
I0302 18:57:58.323320 22895071117440 run.py:483] Algo bellman_ford step 419 current loss 1.341684, current_train_items 13440.
I0302 18:57:58.340419 22895071117440 run.py:483] Algo bellman_ford step 420 current loss 0.449728, current_train_items 13472.
I0302 18:57:58.355508 22895071117440 run.py:483] Algo bellman_ford step 421 current loss 0.673839, current_train_items 13504.
I0302 18:57:58.377580 22895071117440 run.py:483] Algo bellman_ford step 422 current loss 1.043648, current_train_items 13536.
I0302 18:57:58.404298 22895071117440 run.py:483] Algo bellman_ford step 423 current loss 1.062539, current_train_items 13568.
I0302 18:57:58.433676 22895071117440 run.py:483] Algo bellman_ford step 424 current loss 1.240104, current_train_items 13600.
I0302 18:57:58.450787 22895071117440 run.py:483] Algo bellman_ford step 425 current loss 0.417790, current_train_items 13632.
I0302 18:57:58.465731 22895071117440 run.py:483] Algo bellman_ford step 426 current loss 0.522109, current_train_items 13664.
I0302 18:57:58.486515 22895071117440 run.py:483] Algo bellman_ford step 427 current loss 1.065265, current_train_items 13696.
I0302 18:57:58.513119 22895071117440 run.py:483] Algo bellman_ford step 428 current loss 1.091942, current_train_items 13728.
I0302 18:57:58.542861 22895071117440 run.py:483] Algo bellman_ford step 429 current loss 1.239995, current_train_items 13760.
I0302 18:57:58.559954 22895071117440 run.py:483] Algo bellman_ford step 430 current loss 0.449959, current_train_items 13792.
I0302 18:57:58.575037 22895071117440 run.py:483] Algo bellman_ford step 431 current loss 0.790083, current_train_items 13824.
I0302 18:57:58.596952 22895071117440 run.py:483] Algo bellman_ford step 432 current loss 1.171914, current_train_items 13856.
I0302 18:57:58.623327 22895071117440 run.py:483] Algo bellman_ford step 433 current loss 1.107326, current_train_items 13888.
I0302 18:57:58.652706 22895071117440 run.py:483] Algo bellman_ford step 434 current loss 1.431696, current_train_items 13920.
I0302 18:57:58.669721 22895071117440 run.py:483] Algo bellman_ford step 435 current loss 0.470484, current_train_items 13952.
I0302 18:57:58.685291 22895071117440 run.py:483] Algo bellman_ford step 436 current loss 0.728520, current_train_items 13984.
I0302 18:57:58.706021 22895071117440 run.py:483] Algo bellman_ford step 437 current loss 1.029941, current_train_items 14016.
I0302 18:57:58.732427 22895071117440 run.py:483] Algo bellman_ford step 438 current loss 1.028281, current_train_items 14048.
I0302 18:57:58.762079 22895071117440 run.py:483] Algo bellman_ford step 439 current loss 1.167078, current_train_items 14080.
I0302 18:57:58.778946 22895071117440 run.py:483] Algo bellman_ford step 440 current loss 0.399971, current_train_items 14112.
I0302 18:57:58.794157 22895071117440 run.py:483] Algo bellman_ford step 441 current loss 0.736776, current_train_items 14144.
I0302 18:57:58.815268 22895071117440 run.py:483] Algo bellman_ford step 442 current loss 1.109712, current_train_items 14176.
I0302 18:57:58.842352 22895071117440 run.py:483] Algo bellman_ford step 443 current loss 1.162286, current_train_items 14208.
I0302 18:57:58.870543 22895071117440 run.py:483] Algo bellman_ford step 444 current loss 1.227746, current_train_items 14240.
I0302 18:57:58.887608 22895071117440 run.py:483] Algo bellman_ford step 445 current loss 0.368104, current_train_items 14272.
I0302 18:57:58.903654 22895071117440 run.py:483] Algo bellman_ford step 446 current loss 1.023856, current_train_items 14304.
I0302 18:57:58.925584 22895071117440 run.py:483] Algo bellman_ford step 447 current loss 1.024208, current_train_items 14336.
I0302 18:57:58.951207 22895071117440 run.py:483] Algo bellman_ford step 448 current loss 1.152142, current_train_items 14368.
I0302 18:57:58.978842 22895071117440 run.py:483] Algo bellman_ford step 449 current loss 1.147401, current_train_items 14400.
I0302 18:57:58.995848 22895071117440 run.py:483] Algo bellman_ford step 450 current loss 0.542203, current_train_items 14432.
I0302 18:57:59.003871 22895071117440 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:57:59.003986 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:57:59.020236 22895071117440 run.py:483] Algo bellman_ford step 451 current loss 0.729790, current_train_items 14464.
I0302 18:57:59.041861 22895071117440 run.py:483] Algo bellman_ford step 452 current loss 0.914570, current_train_items 14496.
I0302 18:57:59.070359 22895071117440 run.py:483] Algo bellman_ford step 453 current loss 1.304750, current_train_items 14528.
I0302 18:57:59.100580 22895071117440 run.py:483] Algo bellman_ford step 454 current loss 1.409270, current_train_items 14560.
I0302 18:57:59.118589 22895071117440 run.py:483] Algo bellman_ford step 455 current loss 0.514718, current_train_items 14592.
I0302 18:57:59.134452 22895071117440 run.py:483] Algo bellman_ford step 456 current loss 0.670379, current_train_items 14624.
I0302 18:57:59.155785 22895071117440 run.py:483] Algo bellman_ford step 457 current loss 0.998289, current_train_items 14656.
I0302 18:57:59.182917 22895071117440 run.py:483] Algo bellman_ford step 458 current loss 1.044038, current_train_items 14688.
I0302 18:57:59.211466 22895071117440 run.py:483] Algo bellman_ford step 459 current loss 1.317844, current_train_items 14720.
I0302 18:57:59.228546 22895071117440 run.py:483] Algo bellman_ford step 460 current loss 0.361621, current_train_items 14752.
I0302 18:57:59.244297 22895071117440 run.py:483] Algo bellman_ford step 461 current loss 0.812067, current_train_items 14784.
I0302 18:57:59.266573 22895071117440 run.py:483] Algo bellman_ford step 462 current loss 0.957379, current_train_items 14816.
I0302 18:57:59.294277 22895071117440 run.py:483] Algo bellman_ford step 463 current loss 1.246157, current_train_items 14848.
I0302 18:57:59.325196 22895071117440 run.py:483] Algo bellman_ford step 464 current loss 1.451453, current_train_items 14880.
I0302 18:57:59.342838 22895071117440 run.py:483] Algo bellman_ford step 465 current loss 0.433683, current_train_items 14912.
I0302 18:57:59.357918 22895071117440 run.py:483] Algo bellman_ford step 466 current loss 0.614210, current_train_items 14944.
I0302 18:57:59.380176 22895071117440 run.py:483] Algo bellman_ford step 467 current loss 1.136873, current_train_items 14976.
I0302 18:57:59.406364 22895071117440 run.py:483] Algo bellman_ford step 468 current loss 1.264657, current_train_items 15008.
I0302 18:57:59.436969 22895071117440 run.py:483] Algo bellman_ford step 469 current loss 1.347707, current_train_items 15040.
I0302 18:57:59.454010 22895071117440 run.py:483] Algo bellman_ford step 470 current loss 0.350320, current_train_items 15072.
I0302 18:57:59.469469 22895071117440 run.py:483] Algo bellman_ford step 471 current loss 0.689310, current_train_items 15104.
I0302 18:57:59.491982 22895071117440 run.py:483] Algo bellman_ford step 472 current loss 1.013894, current_train_items 15136.
I0302 18:57:59.519932 22895071117440 run.py:483] Algo bellman_ford step 473 current loss 1.262254, current_train_items 15168.
I0302 18:57:59.550571 22895071117440 run.py:483] Algo bellman_ford step 474 current loss 1.430347, current_train_items 15200.
I0302 18:57:59.568014 22895071117440 run.py:483] Algo bellman_ford step 475 current loss 0.432621, current_train_items 15232.
I0302 18:57:59.583267 22895071117440 run.py:483] Algo bellman_ford step 476 current loss 0.676289, current_train_items 15264.
I0302 18:57:59.605537 22895071117440 run.py:483] Algo bellman_ford step 477 current loss 0.970839, current_train_items 15296.
I0302 18:57:59.631617 22895071117440 run.py:483] Algo bellman_ford step 478 current loss 1.141433, current_train_items 15328.
I0302 18:57:59.661957 22895071117440 run.py:483] Algo bellman_ford step 479 current loss 1.730845, current_train_items 15360.
I0302 18:57:59.679279 22895071117440 run.py:483] Algo bellman_ford step 480 current loss 0.484750, current_train_items 15392.
I0302 18:57:59.694744 22895071117440 run.py:483] Algo bellman_ford step 481 current loss 0.825200, current_train_items 15424.
I0302 18:57:59.715807 22895071117440 run.py:483] Algo bellman_ford step 482 current loss 1.015507, current_train_items 15456.
I0302 18:57:59.743320 22895071117440 run.py:483] Algo bellman_ford step 483 current loss 1.135067, current_train_items 15488.
I0302 18:57:59.773381 22895071117440 run.py:483] Algo bellman_ford step 484 current loss 1.328331, current_train_items 15520.
I0302 18:57:59.790431 22895071117440 run.py:483] Algo bellman_ford step 485 current loss 0.402868, current_train_items 15552.
I0302 18:57:59.805977 22895071117440 run.py:483] Algo bellman_ford step 486 current loss 0.800907, current_train_items 15584.
I0302 18:57:59.827839 22895071117440 run.py:483] Algo bellman_ford step 487 current loss 1.030620, current_train_items 15616.
I0302 18:57:59.854876 22895071117440 run.py:483] Algo bellman_ford step 488 current loss 1.075009, current_train_items 15648.
I0302 18:57:59.883796 22895071117440 run.py:483] Algo bellman_ford step 489 current loss 1.220366, current_train_items 15680.
I0302 18:57:59.901003 22895071117440 run.py:483] Algo bellman_ford step 490 current loss 0.401784, current_train_items 15712.
I0302 18:57:59.916666 22895071117440 run.py:483] Algo bellman_ford step 491 current loss 0.661604, current_train_items 15744.
I0302 18:57:59.938014 22895071117440 run.py:483] Algo bellman_ford step 492 current loss 0.881914, current_train_items 15776.
I0302 18:57:59.965179 22895071117440 run.py:483] Algo bellman_ford step 493 current loss 1.139971, current_train_items 15808.
I0302 18:57:59.995862 22895071117440 run.py:483] Algo bellman_ford step 494 current loss 1.416210, current_train_items 15840.
I0302 18:58:00.013046 22895071117440 run.py:483] Algo bellman_ford step 495 current loss 0.369165, current_train_items 15872.
I0302 18:58:00.028392 22895071117440 run.py:483] Algo bellman_ford step 496 current loss 0.648491, current_train_items 15904.
I0302 18:58:00.051216 22895071117440 run.py:483] Algo bellman_ford step 497 current loss 1.148465, current_train_items 15936.
I0302 18:58:00.079509 22895071117440 run.py:483] Algo bellman_ford step 498 current loss 1.095914, current_train_items 15968.
I0302 18:58:00.108805 22895071117440 run.py:483] Algo bellman_ford step 499 current loss 1.062210, current_train_items 16000.
I0302 18:58:00.126201 22895071117440 run.py:483] Algo bellman_ford step 500 current loss 0.424930, current_train_items 16032.
I0302 18:58:00.134043 22895071117440 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.841796875, 'score': 0.841796875, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:00.134150 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.849, current avg val score is 0.842, val scores are: bellman_ford: 0.842
I0302 18:58:00.149538 22895071117440 run.py:483] Algo bellman_ford step 501 current loss 0.571047, current_train_items 16064.
I0302 18:58:00.171574 22895071117440 run.py:483] Algo bellman_ford step 502 current loss 0.961283, current_train_items 16096.
I0302 18:58:00.200310 22895071117440 run.py:483] Algo bellman_ford step 503 current loss 1.177121, current_train_items 16128.
I0302 18:58:00.232368 22895071117440 run.py:483] Algo bellman_ford step 504 current loss 1.489872, current_train_items 16160.
I0302 18:58:00.249985 22895071117440 run.py:483] Algo bellman_ford step 505 current loss 0.373412, current_train_items 16192.
I0302 18:58:00.264632 22895071117440 run.py:483] Algo bellman_ford step 506 current loss 0.596698, current_train_items 16224.
I0302 18:58:00.285236 22895071117440 run.py:483] Algo bellman_ford step 507 current loss 0.964823, current_train_items 16256.
I0302 18:58:00.313332 22895071117440 run.py:483] Algo bellman_ford step 508 current loss 1.222569, current_train_items 16288.
I0302 18:58:00.344089 22895071117440 run.py:483] Algo bellman_ford step 509 current loss 1.349015, current_train_items 16320.
I0302 18:58:00.361460 22895071117440 run.py:483] Algo bellman_ford step 510 current loss 0.409633, current_train_items 16352.
I0302 18:58:00.376771 22895071117440 run.py:483] Algo bellman_ford step 511 current loss 0.710334, current_train_items 16384.
I0302 18:58:00.398183 22895071117440 run.py:483] Algo bellman_ford step 512 current loss 0.934489, current_train_items 16416.
I0302 18:58:00.425764 22895071117440 run.py:483] Algo bellman_ford step 513 current loss 1.140379, current_train_items 16448.
I0302 18:58:00.455774 22895071117440 run.py:483] Algo bellman_ford step 514 current loss 1.311823, current_train_items 16480.
I0302 18:58:00.472945 22895071117440 run.py:483] Algo bellman_ford step 515 current loss 0.437226, current_train_items 16512.
I0302 18:58:00.487938 22895071117440 run.py:483] Algo bellman_ford step 516 current loss 0.651947, current_train_items 16544.
I0302 18:58:00.509935 22895071117440 run.py:483] Algo bellman_ford step 517 current loss 0.911376, current_train_items 16576.
I0302 18:58:00.538403 22895071117440 run.py:483] Algo bellman_ford step 518 current loss 1.095794, current_train_items 16608.
I0302 18:58:00.568023 22895071117440 run.py:483] Algo bellman_ford step 519 current loss 1.358096, current_train_items 16640.
I0302 18:58:00.585042 22895071117440 run.py:483] Algo bellman_ford step 520 current loss 0.404891, current_train_items 16672.
I0302 18:58:00.600535 22895071117440 run.py:483] Algo bellman_ford step 521 current loss 0.669800, current_train_items 16704.
I0302 18:58:00.622062 22895071117440 run.py:483] Algo bellman_ford step 522 current loss 0.825548, current_train_items 16736.
I0302 18:58:00.650112 22895071117440 run.py:483] Algo bellman_ford step 523 current loss 1.203832, current_train_items 16768.
I0302 18:58:00.679243 22895071117440 run.py:483] Algo bellman_ford step 524 current loss 1.583023, current_train_items 16800.
I0302 18:58:00.696479 22895071117440 run.py:483] Algo bellman_ford step 525 current loss 0.360536, current_train_items 16832.
I0302 18:58:00.712001 22895071117440 run.py:483] Algo bellman_ford step 526 current loss 0.734684, current_train_items 16864.
I0302 18:58:00.733548 22895071117440 run.py:483] Algo bellman_ford step 527 current loss 0.970567, current_train_items 16896.
I0302 18:58:00.759791 22895071117440 run.py:483] Algo bellman_ford step 528 current loss 0.970878, current_train_items 16928.
I0302 18:58:00.789811 22895071117440 run.py:483] Algo bellman_ford step 529 current loss 1.410234, current_train_items 16960.
I0302 18:58:00.807247 22895071117440 run.py:483] Algo bellman_ford step 530 current loss 0.415431, current_train_items 16992.
I0302 18:58:00.822497 22895071117440 run.py:483] Algo bellman_ford step 531 current loss 0.784588, current_train_items 17024.
I0302 18:58:00.843061 22895071117440 run.py:483] Algo bellman_ford step 532 current loss 0.837951, current_train_items 17056.
I0302 18:58:00.870413 22895071117440 run.py:483] Algo bellman_ford step 533 current loss 1.171389, current_train_items 17088.
I0302 18:58:00.900527 22895071117440 run.py:483] Algo bellman_ford step 534 current loss 1.327106, current_train_items 17120.
I0302 18:58:00.917973 22895071117440 run.py:483] Algo bellman_ford step 535 current loss 0.409844, current_train_items 17152.
I0302 18:58:00.932805 22895071117440 run.py:483] Algo bellman_ford step 536 current loss 0.627097, current_train_items 17184.
I0302 18:58:00.953293 22895071117440 run.py:483] Algo bellman_ford step 537 current loss 0.909703, current_train_items 17216.
I0302 18:58:00.979650 22895071117440 run.py:483] Algo bellman_ford step 538 current loss 0.967100, current_train_items 17248.
I0302 18:58:01.008341 22895071117440 run.py:483] Algo bellman_ford step 539 current loss 1.136845, current_train_items 17280.
I0302 18:58:01.025148 22895071117440 run.py:483] Algo bellman_ford step 540 current loss 0.389705, current_train_items 17312.
I0302 18:58:01.039818 22895071117440 run.py:483] Algo bellman_ford step 541 current loss 0.566275, current_train_items 17344.
I0302 18:58:01.061495 22895071117440 run.py:483] Algo bellman_ford step 542 current loss 1.082191, current_train_items 17376.
I0302 18:58:01.088796 22895071117440 run.py:483] Algo bellman_ford step 543 current loss 1.123101, current_train_items 17408.
I0302 18:58:01.118614 22895071117440 run.py:483] Algo bellman_ford step 544 current loss 1.216012, current_train_items 17440.
I0302 18:58:01.135843 22895071117440 run.py:483] Algo bellman_ford step 545 current loss 0.432066, current_train_items 17472.
I0302 18:58:01.151118 22895071117440 run.py:483] Algo bellman_ford step 546 current loss 0.770411, current_train_items 17504.
I0302 18:58:01.172409 22895071117440 run.py:483] Algo bellman_ford step 547 current loss 0.994598, current_train_items 17536.
I0302 18:58:01.200139 22895071117440 run.py:483] Algo bellman_ford step 548 current loss 1.272938, current_train_items 17568.
I0302 18:58:01.227559 22895071117440 run.py:483] Algo bellman_ford step 549 current loss 1.195355, current_train_items 17600.
I0302 18:58:01.244632 22895071117440 run.py:483] Algo bellman_ford step 550 current loss 0.459925, current_train_items 17632.
I0302 18:58:01.252565 22895071117440 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:01.252669 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.849, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 18:58:01.282161 22895071117440 run.py:483] Algo bellman_ford step 551 current loss 0.615528, current_train_items 17664.
I0302 18:58:01.303217 22895071117440 run.py:483] Algo bellman_ford step 552 current loss 0.850414, current_train_items 17696.
I0302 18:58:01.332295 22895071117440 run.py:483] Algo bellman_ford step 553 current loss 1.129552, current_train_items 17728.
I0302 18:58:01.362287 22895071117440 run.py:483] Algo bellman_ford step 554 current loss 1.153458, current_train_items 17760.
I0302 18:58:01.380343 22895071117440 run.py:483] Algo bellman_ford step 555 current loss 0.397516, current_train_items 17792.
I0302 18:58:01.396048 22895071117440 run.py:483] Algo bellman_ford step 556 current loss 0.661030, current_train_items 17824.
I0302 18:58:01.418520 22895071117440 run.py:483] Algo bellman_ford step 557 current loss 1.055644, current_train_items 17856.
I0302 18:58:01.445486 22895071117440 run.py:483] Algo bellman_ford step 558 current loss 1.271699, current_train_items 17888.
I0302 18:58:01.476689 22895071117440 run.py:483] Algo bellman_ford step 559 current loss 1.375177, current_train_items 17920.
I0302 18:58:01.494128 22895071117440 run.py:483] Algo bellman_ford step 560 current loss 0.465002, current_train_items 17952.
I0302 18:58:01.509749 22895071117440 run.py:483] Algo bellman_ford step 561 current loss 0.611902, current_train_items 17984.
I0302 18:58:01.531835 22895071117440 run.py:483] Algo bellman_ford step 562 current loss 0.963454, current_train_items 18016.
I0302 18:58:01.559123 22895071117440 run.py:483] Algo bellman_ford step 563 current loss 1.042918, current_train_items 18048.
I0302 18:58:01.587256 22895071117440 run.py:483] Algo bellman_ford step 564 current loss 1.260144, current_train_items 18080.
I0302 18:58:01.604807 22895071117440 run.py:483] Algo bellman_ford step 565 current loss 0.453641, current_train_items 18112.
I0302 18:58:01.620623 22895071117440 run.py:483] Algo bellman_ford step 566 current loss 0.739710, current_train_items 18144.
I0302 18:58:01.643226 22895071117440 run.py:483] Algo bellman_ford step 567 current loss 1.161822, current_train_items 18176.
I0302 18:58:01.670694 22895071117440 run.py:483] Algo bellman_ford step 568 current loss 1.002429, current_train_items 18208.
I0302 18:58:01.699628 22895071117440 run.py:483] Algo bellman_ford step 569 current loss 1.199158, current_train_items 18240.
I0302 18:58:01.716682 22895071117440 run.py:483] Algo bellman_ford step 570 current loss 0.414671, current_train_items 18272.
I0302 18:58:01.732058 22895071117440 run.py:483] Algo bellman_ford step 571 current loss 0.714493, current_train_items 18304.
I0302 18:58:01.754663 22895071117440 run.py:483] Algo bellman_ford step 572 current loss 1.118289, current_train_items 18336.
I0302 18:58:01.783248 22895071117440 run.py:483] Algo bellman_ford step 573 current loss 1.366323, current_train_items 18368.
I0302 18:58:01.811865 22895071117440 run.py:483] Algo bellman_ford step 574 current loss 1.238690, current_train_items 18400.
I0302 18:58:01.829627 22895071117440 run.py:483] Algo bellman_ford step 575 current loss 0.454370, current_train_items 18432.
I0302 18:58:01.844959 22895071117440 run.py:483] Algo bellman_ford step 576 current loss 0.749615, current_train_items 18464.
I0302 18:58:01.866798 22895071117440 run.py:483] Algo bellman_ford step 577 current loss 0.978867, current_train_items 18496.
I0302 18:58:01.894713 22895071117440 run.py:483] Algo bellman_ford step 578 current loss 1.018318, current_train_items 18528.
I0302 18:58:01.925761 22895071117440 run.py:483] Algo bellman_ford step 579 current loss 1.311040, current_train_items 18560.
I0302 18:58:01.943197 22895071117440 run.py:483] Algo bellman_ford step 580 current loss 0.443647, current_train_items 18592.
I0302 18:58:01.958642 22895071117440 run.py:483] Algo bellman_ford step 581 current loss 0.655554, current_train_items 18624.
I0302 18:58:01.979218 22895071117440 run.py:483] Algo bellman_ford step 582 current loss 0.799894, current_train_items 18656.
I0302 18:58:02.005595 22895071117440 run.py:483] Algo bellman_ford step 583 current loss 0.912528, current_train_items 18688.
I0302 18:58:02.035213 22895071117440 run.py:483] Algo bellman_ford step 584 current loss 1.261300, current_train_items 18720.
I0302 18:58:02.052433 22895071117440 run.py:483] Algo bellman_ford step 585 current loss 0.404688, current_train_items 18752.
I0302 18:58:02.068018 22895071117440 run.py:483] Algo bellman_ford step 586 current loss 0.695177, current_train_items 18784.
I0302 18:58:02.088629 22895071117440 run.py:483] Algo bellman_ford step 587 current loss 0.680892, current_train_items 18816.
I0302 18:58:02.115067 22895071117440 run.py:483] Algo bellman_ford step 588 current loss 1.002514, current_train_items 18848.
I0302 18:58:02.146198 22895071117440 run.py:483] Algo bellman_ford step 589 current loss 1.262989, current_train_items 18880.
I0302 18:58:02.163876 22895071117440 run.py:483] Algo bellman_ford step 590 current loss 0.409803, current_train_items 18912.
I0302 18:58:02.179494 22895071117440 run.py:483] Algo bellman_ford step 591 current loss 0.709026, current_train_items 18944.
I0302 18:58:02.202323 22895071117440 run.py:483] Algo bellman_ford step 592 current loss 0.904150, current_train_items 18976.
I0302 18:58:02.229769 22895071117440 run.py:483] Algo bellman_ford step 593 current loss 1.090711, current_train_items 19008.
I0302 18:58:02.258955 22895071117440 run.py:483] Algo bellman_ford step 594 current loss 1.352982, current_train_items 19040.
I0302 18:58:02.276420 22895071117440 run.py:483] Algo bellman_ford step 595 current loss 0.377289, current_train_items 19072.
I0302 18:58:02.292034 22895071117440 run.py:483] Algo bellman_ford step 596 current loss 0.712449, current_train_items 19104.
I0302 18:58:02.314608 22895071117440 run.py:483] Algo bellman_ford step 597 current loss 0.951512, current_train_items 19136.
I0302 18:58:02.342094 22895071117440 run.py:483] Algo bellman_ford step 598 current loss 1.020283, current_train_items 19168.
I0302 18:58:02.371804 22895071117440 run.py:483] Algo bellman_ford step 599 current loss 1.161054, current_train_items 19200.
I0302 18:58:02.389338 22895071117440 run.py:483] Algo bellman_ford step 600 current loss 0.436930, current_train_items 19232.
I0302 18:58:02.397233 22895071117440 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.8662109375, 'score': 0.8662109375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:02.397340 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.879, current avg val score is 0.866, val scores are: bellman_ford: 0.866
I0302 18:58:02.413030 22895071117440 run.py:483] Algo bellman_ford step 601 current loss 0.575436, current_train_items 19264.
I0302 18:58:02.434578 22895071117440 run.py:483] Algo bellman_ford step 602 current loss 0.839388, current_train_items 19296.
I0302 18:58:02.461445 22895071117440 run.py:483] Algo bellman_ford step 603 current loss 1.044991, current_train_items 19328.
I0302 18:58:02.490019 22895071117440 run.py:483] Algo bellman_ford step 604 current loss 1.125579, current_train_items 19360.
I0302 18:58:02.507822 22895071117440 run.py:483] Algo bellman_ford step 605 current loss 0.398732, current_train_items 19392.
I0302 18:58:02.522637 22895071117440 run.py:483] Algo bellman_ford step 606 current loss 0.657752, current_train_items 19424.
I0302 18:58:02.544784 22895071117440 run.py:483] Algo bellman_ford step 607 current loss 0.984054, current_train_items 19456.
I0302 18:58:02.570956 22895071117440 run.py:483] Algo bellman_ford step 608 current loss 1.076327, current_train_items 19488.
I0302 18:58:02.600282 22895071117440 run.py:483] Algo bellman_ford step 609 current loss 1.193912, current_train_items 19520.
I0302 18:58:02.617352 22895071117440 run.py:483] Algo bellman_ford step 610 current loss 0.365207, current_train_items 19552.
I0302 18:58:02.632867 22895071117440 run.py:483] Algo bellman_ford step 611 current loss 0.611492, current_train_items 19584.
I0302 18:58:02.654492 22895071117440 run.py:483] Algo bellman_ford step 612 current loss 0.838924, current_train_items 19616.
I0302 18:58:02.679866 22895071117440 run.py:483] Algo bellman_ford step 613 current loss 1.022571, current_train_items 19648.
I0302 18:58:02.709593 22895071117440 run.py:483] Algo bellman_ford step 614 current loss 1.340544, current_train_items 19680.
I0302 18:58:02.726369 22895071117440 run.py:483] Algo bellman_ford step 615 current loss 0.288773, current_train_items 19712.
I0302 18:58:02.741962 22895071117440 run.py:483] Algo bellman_ford step 616 current loss 0.683599, current_train_items 19744.
I0302 18:58:02.762819 22895071117440 run.py:483] Algo bellman_ford step 617 current loss 0.921116, current_train_items 19776.
I0302 18:58:02.789077 22895071117440 run.py:483] Algo bellman_ford step 618 current loss 0.933835, current_train_items 19808.
I0302 18:58:02.818425 22895071117440 run.py:483] Algo bellman_ford step 619 current loss 1.122200, current_train_items 19840.
I0302 18:58:02.835590 22895071117440 run.py:483] Algo bellman_ford step 620 current loss 0.417937, current_train_items 19872.
I0302 18:58:02.851269 22895071117440 run.py:483] Algo bellman_ford step 621 current loss 0.793857, current_train_items 19904.
I0302 18:58:02.872851 22895071117440 run.py:483] Algo bellman_ford step 622 current loss 1.040738, current_train_items 19936.
I0302 18:58:02.899436 22895071117440 run.py:483] Algo bellman_ford step 623 current loss 1.253032, current_train_items 19968.
I0302 18:58:02.928304 22895071117440 run.py:483] Algo bellman_ford step 624 current loss 1.280839, current_train_items 20000.
I0302 18:58:02.945378 22895071117440 run.py:483] Algo bellman_ford step 625 current loss 0.467306, current_train_items 20032.
I0302 18:58:02.960315 22895071117440 run.py:483] Algo bellman_ford step 626 current loss 0.664176, current_train_items 20064.
I0302 18:58:02.982401 22895071117440 run.py:483] Algo bellman_ford step 627 current loss 0.911166, current_train_items 20096.
I0302 18:58:03.010449 22895071117440 run.py:483] Algo bellman_ford step 628 current loss 1.045781, current_train_items 20128.
I0302 18:58:03.040230 22895071117440 run.py:483] Algo bellman_ford step 629 current loss 1.109689, current_train_items 20160.
I0302 18:58:03.057318 22895071117440 run.py:483] Algo bellman_ford step 630 current loss 0.361077, current_train_items 20192.
I0302 18:58:03.072412 22895071117440 run.py:483] Algo bellman_ford step 631 current loss 0.604432, current_train_items 20224.
I0302 18:58:03.093641 22895071117440 run.py:483] Algo bellman_ford step 632 current loss 1.010014, current_train_items 20256.
I0302 18:58:03.120499 22895071117440 run.py:483] Algo bellman_ford step 633 current loss 1.022327, current_train_items 20288.
I0302 18:58:03.148972 22895071117440 run.py:483] Algo bellman_ford step 634 current loss 1.157781, current_train_items 20320.
I0302 18:58:03.166112 22895071117440 run.py:483] Algo bellman_ford step 635 current loss 0.497024, current_train_items 20352.
I0302 18:58:03.181282 22895071117440 run.py:483] Algo bellman_ford step 636 current loss 0.580256, current_train_items 20384.
I0302 18:58:03.202662 22895071117440 run.py:483] Algo bellman_ford step 637 current loss 0.912092, current_train_items 20416.
I0302 18:58:03.230992 22895071117440 run.py:483] Algo bellman_ford step 638 current loss 1.001775, current_train_items 20448.
I0302 18:58:03.257484 22895071117440 run.py:483] Algo bellman_ford step 639 current loss 1.138200, current_train_items 20480.
I0302 18:58:03.274538 22895071117440 run.py:483] Algo bellman_ford step 640 current loss 0.409011, current_train_items 20512.
I0302 18:58:03.289777 22895071117440 run.py:483] Algo bellman_ford step 641 current loss 0.704695, current_train_items 20544.
I0302 18:58:03.310192 22895071117440 run.py:483] Algo bellman_ford step 642 current loss 0.958426, current_train_items 20576.
I0302 18:58:03.336537 22895071117440 run.py:483] Algo bellman_ford step 643 current loss 0.989882, current_train_items 20608.
I0302 18:58:03.366015 22895071117440 run.py:483] Algo bellman_ford step 644 current loss 1.283711, current_train_items 20640.
I0302 18:58:03.382885 22895071117440 run.py:483] Algo bellman_ford step 645 current loss 0.396698, current_train_items 20672.
I0302 18:58:03.399110 22895071117440 run.py:483] Algo bellman_ford step 646 current loss 0.814080, current_train_items 20704.
I0302 18:58:03.419819 22895071117440 run.py:483] Algo bellman_ford step 647 current loss 0.839341, current_train_items 20736.
I0302 18:58:03.447661 22895071117440 run.py:483] Algo bellman_ford step 648 current loss 1.222531, current_train_items 20768.
I0302 18:58:03.477802 22895071117440 run.py:483] Algo bellman_ford step 649 current loss 1.137605, current_train_items 20800.
I0302 18:58:03.494973 22895071117440 run.py:483] Algo bellman_ford step 650 current loss 0.435475, current_train_items 20832.
I0302 18:58:03.503151 22895071117440 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:03.503255 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.879, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:58:03.531034 22895071117440 run.py:483] Algo bellman_ford step 651 current loss 0.699798, current_train_items 20864.
I0302 18:58:03.552128 22895071117440 run.py:483] Algo bellman_ford step 652 current loss 0.933629, current_train_items 20896.
I0302 18:58:03.578709 22895071117440 run.py:483] Algo bellman_ford step 653 current loss 1.002170, current_train_items 20928.
I0302 18:58:03.608486 22895071117440 run.py:483] Algo bellman_ford step 654 current loss 1.179617, current_train_items 20960.
I0302 18:58:03.626379 22895071117440 run.py:483] Algo bellman_ford step 655 current loss 0.387481, current_train_items 20992.
I0302 18:58:03.641909 22895071117440 run.py:483] Algo bellman_ford step 656 current loss 0.864788, current_train_items 21024.
I0302 18:58:03.664145 22895071117440 run.py:483] Algo bellman_ford step 657 current loss 0.990253, current_train_items 21056.
I0302 18:58:03.691346 22895071117440 run.py:483] Algo bellman_ford step 658 current loss 1.050313, current_train_items 21088.
I0302 18:58:03.720496 22895071117440 run.py:483] Algo bellman_ford step 659 current loss 1.113678, current_train_items 21120.
I0302 18:58:03.737829 22895071117440 run.py:483] Algo bellman_ford step 660 current loss 0.414771, current_train_items 21152.
I0302 18:58:03.753242 22895071117440 run.py:483] Algo bellman_ford step 661 current loss 0.560390, current_train_items 21184.
I0302 18:58:03.774748 22895071117440 run.py:483] Algo bellman_ford step 662 current loss 0.877779, current_train_items 21216.
I0302 18:58:03.801949 22895071117440 run.py:483] Algo bellman_ford step 663 current loss 1.139263, current_train_items 21248.
I0302 18:58:03.833260 22895071117440 run.py:483] Algo bellman_ford step 664 current loss 1.708387, current_train_items 21280.
I0302 18:58:03.851464 22895071117440 run.py:483] Algo bellman_ford step 665 current loss 0.406645, current_train_items 21312.
I0302 18:58:03.867251 22895071117440 run.py:483] Algo bellman_ford step 666 current loss 0.632738, current_train_items 21344.
I0302 18:58:03.889744 22895071117440 run.py:483] Algo bellman_ford step 667 current loss 1.069641, current_train_items 21376.
I0302 18:58:03.916729 22895071117440 run.py:483] Algo bellman_ford step 668 current loss 1.101760, current_train_items 21408.
I0302 18:58:03.945670 22895071117440 run.py:483] Algo bellman_ford step 669 current loss 1.317293, current_train_items 21440.
I0302 18:58:03.963300 22895071117440 run.py:483] Algo bellman_ford step 670 current loss 0.324291, current_train_items 21472.
I0302 18:58:03.978634 22895071117440 run.py:483] Algo bellman_ford step 671 current loss 0.546317, current_train_items 21504.
I0302 18:58:04.001184 22895071117440 run.py:483] Algo bellman_ford step 672 current loss 0.896364, current_train_items 21536.
I0302 18:58:04.027614 22895071117440 run.py:483] Algo bellman_ford step 673 current loss 1.131359, current_train_items 21568.
I0302 18:58:04.058522 22895071117440 run.py:483] Algo bellman_ford step 674 current loss 1.182567, current_train_items 21600.
I0302 18:58:04.075970 22895071117440 run.py:483] Algo bellman_ford step 675 current loss 0.403063, current_train_items 21632.
I0302 18:58:04.090972 22895071117440 run.py:483] Algo bellman_ford step 676 current loss 0.510366, current_train_items 21664.
I0302 18:58:04.112465 22895071117440 run.py:483] Algo bellman_ford step 677 current loss 0.904393, current_train_items 21696.
I0302 18:58:04.140498 22895071117440 run.py:483] Algo bellman_ford step 678 current loss 0.999326, current_train_items 21728.
I0302 18:58:04.170468 22895071117440 run.py:483] Algo bellman_ford step 679 current loss 1.243669, current_train_items 21760.
I0302 18:58:04.187992 22895071117440 run.py:483] Algo bellman_ford step 680 current loss 0.449592, current_train_items 21792.
I0302 18:58:04.203604 22895071117440 run.py:483] Algo bellman_ford step 681 current loss 0.689128, current_train_items 21824.
I0302 18:58:04.225489 22895071117440 run.py:483] Algo bellman_ford step 682 current loss 0.966972, current_train_items 21856.
I0302 18:58:04.252111 22895071117440 run.py:483] Algo bellman_ford step 683 current loss 0.982193, current_train_items 21888.
I0302 18:58:04.282679 22895071117440 run.py:483] Algo bellman_ford step 684 current loss 1.286776, current_train_items 21920.
I0302 18:58:04.300065 22895071117440 run.py:483] Algo bellman_ford step 685 current loss 0.484336, current_train_items 21952.
I0302 18:58:04.315371 22895071117440 run.py:483] Algo bellman_ford step 686 current loss 0.620733, current_train_items 21984.
I0302 18:58:04.337394 22895071117440 run.py:483] Algo bellman_ford step 687 current loss 0.878478, current_train_items 22016.
I0302 18:58:04.362884 22895071117440 run.py:483] Algo bellman_ford step 688 current loss 0.865003, current_train_items 22048.
I0302 18:58:04.394500 22895071117440 run.py:483] Algo bellman_ford step 689 current loss 1.272757, current_train_items 22080.
I0302 18:58:04.411873 22895071117440 run.py:483] Algo bellman_ford step 690 current loss 0.414366, current_train_items 22112.
I0302 18:58:04.427801 22895071117440 run.py:483] Algo bellman_ford step 691 current loss 0.814126, current_train_items 22144.
I0302 18:58:04.450841 22895071117440 run.py:483] Algo bellman_ford step 692 current loss 0.858078, current_train_items 22176.
I0302 18:58:04.477564 22895071117440 run.py:483] Algo bellman_ford step 693 current loss 1.108733, current_train_items 22208.
I0302 18:58:04.508317 22895071117440 run.py:483] Algo bellman_ford step 694 current loss 1.267608, current_train_items 22240.
I0302 18:58:04.526106 22895071117440 run.py:483] Algo bellman_ford step 695 current loss 0.464455, current_train_items 22272.
I0302 18:58:04.541455 22895071117440 run.py:483] Algo bellman_ford step 696 current loss 0.683686, current_train_items 22304.
I0302 18:58:04.561902 22895071117440 run.py:483] Algo bellman_ford step 697 current loss 1.036445, current_train_items 22336.
I0302 18:58:04.589459 22895071117440 run.py:483] Algo bellman_ford step 698 current loss 0.958509, current_train_items 22368.
I0302 18:58:04.620203 22895071117440 run.py:483] Algo bellman_ford step 699 current loss 1.223875, current_train_items 22400.
I0302 18:58:04.637807 22895071117440 run.py:483] Algo bellman_ford step 700 current loss 0.392570, current_train_items 22432.
I0302 18:58:04.645680 22895071117440 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:04.645786 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.883, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 18:58:04.661347 22895071117440 run.py:483] Algo bellman_ford step 701 current loss 0.521002, current_train_items 22464.
I0302 18:58:04.682905 22895071117440 run.py:483] Algo bellman_ford step 702 current loss 1.029760, current_train_items 22496.
I0302 18:58:04.708916 22895071117440 run.py:483] Algo bellman_ford step 703 current loss 1.110885, current_train_items 22528.
I0302 18:58:04.737458 22895071117440 run.py:483] Algo bellman_ford step 704 current loss 1.113793, current_train_items 22560.
I0302 18:58:04.755041 22895071117440 run.py:483] Algo bellman_ford step 705 current loss 0.387023, current_train_items 22592.
I0302 18:58:04.770098 22895071117440 run.py:483] Algo bellman_ford step 706 current loss 0.730247, current_train_items 22624.
I0302 18:58:04.791508 22895071117440 run.py:483] Algo bellman_ford step 707 current loss 0.835834, current_train_items 22656.
I0302 18:58:04.818281 22895071117440 run.py:483] Algo bellman_ford step 708 current loss 0.939717, current_train_items 22688.
I0302 18:58:04.848532 22895071117440 run.py:483] Algo bellman_ford step 709 current loss 1.215137, current_train_items 22720.
I0302 18:58:04.865663 22895071117440 run.py:483] Algo bellman_ford step 710 current loss 0.387282, current_train_items 22752.
I0302 18:58:04.880985 22895071117440 run.py:483] Algo bellman_ford step 711 current loss 0.618603, current_train_items 22784.
I0302 18:58:04.902984 22895071117440 run.py:483] Algo bellman_ford step 712 current loss 0.959589, current_train_items 22816.
I0302 18:58:04.930954 22895071117440 run.py:483] Algo bellman_ford step 713 current loss 1.196885, current_train_items 22848.
I0302 18:58:04.958593 22895071117440 run.py:483] Algo bellman_ford step 714 current loss 1.374051, current_train_items 22880.
I0302 18:58:04.975637 22895071117440 run.py:483] Algo bellman_ford step 715 current loss 0.381779, current_train_items 22912.
I0302 18:58:04.991225 22895071117440 run.py:483] Algo bellman_ford step 716 current loss 0.719657, current_train_items 22944.
I0302 18:58:05.013843 22895071117440 run.py:483] Algo bellman_ford step 717 current loss 0.983951, current_train_items 22976.
I0302 18:58:05.039943 22895071117440 run.py:483] Algo bellman_ford step 718 current loss 0.937350, current_train_items 23008.
I0302 18:58:05.069298 22895071117440 run.py:483] Algo bellman_ford step 719 current loss 1.144187, current_train_items 23040.
I0302 18:58:05.086307 22895071117440 run.py:483] Algo bellman_ford step 720 current loss 0.454379, current_train_items 23072.
I0302 18:58:05.101907 22895071117440 run.py:483] Algo bellman_ford step 721 current loss 0.725672, current_train_items 23104.
I0302 18:58:05.123057 22895071117440 run.py:483] Algo bellman_ford step 722 current loss 0.820405, current_train_items 23136.
I0302 18:58:05.149640 22895071117440 run.py:483] Algo bellman_ford step 723 current loss 0.904978, current_train_items 23168.
I0302 18:58:05.178218 22895071117440 run.py:483] Algo bellman_ford step 724 current loss 1.134781, current_train_items 23200.
I0302 18:58:05.195347 22895071117440 run.py:483] Algo bellman_ford step 725 current loss 0.331337, current_train_items 23232.
I0302 18:58:05.210989 22895071117440 run.py:483] Algo bellman_ford step 726 current loss 0.699077, current_train_items 23264.
I0302 18:58:05.232917 22895071117440 run.py:483] Algo bellman_ford step 727 current loss 1.216789, current_train_items 23296.
I0302 18:58:05.259642 22895071117440 run.py:483] Algo bellman_ford step 728 current loss 1.079325, current_train_items 23328.
I0302 18:58:05.289085 22895071117440 run.py:483] Algo bellman_ford step 729 current loss 1.241874, current_train_items 23360.
I0302 18:58:05.306102 22895071117440 run.py:483] Algo bellman_ford step 730 current loss 0.336076, current_train_items 23392.
I0302 18:58:05.321396 22895071117440 run.py:483] Algo bellman_ford step 731 current loss 0.695064, current_train_items 23424.
I0302 18:58:05.342916 22895071117440 run.py:483] Algo bellman_ford step 732 current loss 0.941382, current_train_items 23456.
I0302 18:58:05.370126 22895071117440 run.py:483] Algo bellman_ford step 733 current loss 1.011950, current_train_items 23488.
I0302 18:58:05.400247 22895071117440 run.py:483] Algo bellman_ford step 734 current loss 1.204709, current_train_items 23520.
I0302 18:58:05.417375 22895071117440 run.py:483] Algo bellman_ford step 735 current loss 0.366884, current_train_items 23552.
I0302 18:58:05.432932 22895071117440 run.py:483] Algo bellman_ford step 736 current loss 0.612069, current_train_items 23584.
I0302 18:58:05.453912 22895071117440 run.py:483] Algo bellman_ford step 737 current loss 0.871063, current_train_items 23616.
I0302 18:58:05.481298 22895071117440 run.py:483] Algo bellman_ford step 738 current loss 1.020382, current_train_items 23648.
I0302 18:58:05.508845 22895071117440 run.py:483] Algo bellman_ford step 739 current loss 0.966627, current_train_items 23680.
I0302 18:58:05.525886 22895071117440 run.py:483] Algo bellman_ford step 740 current loss 0.463798, current_train_items 23712.
I0302 18:58:05.541287 22895071117440 run.py:483] Algo bellman_ford step 741 current loss 0.695712, current_train_items 23744.
I0302 18:58:05.562434 22895071117440 run.py:483] Algo bellman_ford step 742 current loss 0.784766, current_train_items 23776.
I0302 18:58:05.588540 22895071117440 run.py:483] Algo bellman_ford step 743 current loss 0.861911, current_train_items 23808.
I0302 18:58:05.619622 22895071117440 run.py:483] Algo bellman_ford step 744 current loss 1.489743, current_train_items 23840.
I0302 18:58:05.637269 22895071117440 run.py:483] Algo bellman_ford step 745 current loss 0.379142, current_train_items 23872.
I0302 18:58:05.652456 22895071117440 run.py:483] Algo bellman_ford step 746 current loss 0.732257, current_train_items 23904.
I0302 18:58:05.673691 22895071117440 run.py:483] Algo bellman_ford step 747 current loss 0.986829, current_train_items 23936.
I0302 18:58:05.701313 22895071117440 run.py:483] Algo bellman_ford step 748 current loss 1.464184, current_train_items 23968.
I0302 18:58:05.730394 22895071117440 run.py:483] Algo bellman_ford step 749 current loss 1.454990, current_train_items 24000.
I0302 18:58:05.747236 22895071117440 run.py:483] Algo bellman_ford step 750 current loss 0.385978, current_train_items 24032.
I0302 18:58:05.755118 22895071117440 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:05.755223 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.883, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:05.771476 22895071117440 run.py:483] Algo bellman_ford step 751 current loss 0.758782, current_train_items 24064.
I0302 18:58:05.792685 22895071117440 run.py:483] Algo bellman_ford step 752 current loss 0.857869, current_train_items 24096.
I0302 18:58:05.819493 22895071117440 run.py:483] Algo bellman_ford step 753 current loss 1.028625, current_train_items 24128.
I0302 18:58:05.850091 22895071117440 run.py:483] Algo bellman_ford step 754 current loss 1.114733, current_train_items 24160.
I0302 18:58:05.867771 22895071117440 run.py:483] Algo bellman_ford step 755 current loss 0.438840, current_train_items 24192.
I0302 18:58:05.882742 22895071117440 run.py:483] Algo bellman_ford step 756 current loss 0.625385, current_train_items 24224.
I0302 18:58:05.905348 22895071117440 run.py:483] Algo bellman_ford step 757 current loss 0.847982, current_train_items 24256.
I0302 18:58:05.932088 22895071117440 run.py:483] Algo bellman_ford step 758 current loss 0.979519, current_train_items 24288.
I0302 18:58:05.961396 22895071117440 run.py:483] Algo bellman_ford step 759 current loss 1.115057, current_train_items 24320.
I0302 18:58:05.978842 22895071117440 run.py:483] Algo bellman_ford step 760 current loss 0.299944, current_train_items 24352.
I0302 18:58:05.994236 22895071117440 run.py:483] Algo bellman_ford step 761 current loss 0.721847, current_train_items 24384.
I0302 18:58:06.015755 22895071117440 run.py:483] Algo bellman_ford step 762 current loss 0.900144, current_train_items 24416.
I0302 18:58:06.042072 22895071117440 run.py:483] Algo bellman_ford step 763 current loss 0.912023, current_train_items 24448.
I0302 18:58:06.071843 22895071117440 run.py:483] Algo bellman_ford step 764 current loss 1.212377, current_train_items 24480.
I0302 18:58:06.089227 22895071117440 run.py:483] Algo bellman_ford step 765 current loss 0.381155, current_train_items 24512.
I0302 18:58:06.104823 22895071117440 run.py:483] Algo bellman_ford step 766 current loss 0.720643, current_train_items 24544.
I0302 18:58:06.126456 22895071117440 run.py:483] Algo bellman_ford step 767 current loss 0.922478, current_train_items 24576.
I0302 18:58:06.154079 22895071117440 run.py:483] Algo bellman_ford step 768 current loss 1.003771, current_train_items 24608.
I0302 18:58:06.184338 22895071117440 run.py:483] Algo bellman_ford step 769 current loss 1.041277, current_train_items 24640.
I0302 18:58:06.201930 22895071117440 run.py:483] Algo bellman_ford step 770 current loss 0.331995, current_train_items 24672.
I0302 18:58:06.217621 22895071117440 run.py:483] Algo bellman_ford step 771 current loss 0.743892, current_train_items 24704.
I0302 18:58:06.238846 22895071117440 run.py:483] Algo bellman_ford step 772 current loss 0.817868, current_train_items 24736.
I0302 18:58:06.265600 22895071117440 run.py:483] Algo bellman_ford step 773 current loss 0.844284, current_train_items 24768.
I0302 18:58:06.296211 22895071117440 run.py:483] Algo bellman_ford step 774 current loss 1.187082, current_train_items 24800.
I0302 18:58:06.314026 22895071117440 run.py:483] Algo bellman_ford step 775 current loss 0.290426, current_train_items 24832.
I0302 18:58:06.330172 22895071117440 run.py:483] Algo bellman_ford step 776 current loss 0.625239, current_train_items 24864.
I0302 18:58:06.352044 22895071117440 run.py:483] Algo bellman_ford step 777 current loss 0.929178, current_train_items 24896.
I0302 18:58:06.378273 22895071117440 run.py:483] Algo bellman_ford step 778 current loss 0.959527, current_train_items 24928.
I0302 18:58:06.407228 22895071117440 run.py:483] Algo bellman_ford step 779 current loss 1.039832, current_train_items 24960.
I0302 18:58:06.424627 22895071117440 run.py:483] Algo bellman_ford step 780 current loss 0.442410, current_train_items 24992.
I0302 18:58:06.440089 22895071117440 run.py:483] Algo bellman_ford step 781 current loss 0.708194, current_train_items 25024.
I0302 18:58:06.461005 22895071117440 run.py:483] Algo bellman_ford step 782 current loss 0.823995, current_train_items 25056.
I0302 18:58:06.488041 22895071117440 run.py:483] Algo bellman_ford step 783 current loss 0.991323, current_train_items 25088.
I0302 18:58:06.518862 22895071117440 run.py:483] Algo bellman_ford step 784 current loss 1.415109, current_train_items 25120.
I0302 18:58:06.536253 22895071117440 run.py:483] Algo bellman_ford step 785 current loss 0.428980, current_train_items 25152.
I0302 18:58:06.552124 22895071117440 run.py:483] Algo bellman_ford step 786 current loss 0.670288, current_train_items 25184.
I0302 18:58:06.573351 22895071117440 run.py:483] Algo bellman_ford step 787 current loss 0.856999, current_train_items 25216.
I0302 18:58:06.600659 22895071117440 run.py:483] Algo bellman_ford step 788 current loss 0.999446, current_train_items 25248.
I0302 18:58:06.628126 22895071117440 run.py:483] Algo bellman_ford step 789 current loss 1.136368, current_train_items 25280.
I0302 18:58:06.645758 22895071117440 run.py:483] Algo bellman_ford step 790 current loss 0.349156, current_train_items 25312.
I0302 18:58:06.661688 22895071117440 run.py:483] Algo bellman_ford step 791 current loss 0.816738, current_train_items 25344.
I0302 18:58:06.681915 22895071117440 run.py:483] Algo bellman_ford step 792 current loss 0.757304, current_train_items 25376.
I0302 18:58:06.708694 22895071117440 run.py:483] Algo bellman_ford step 793 current loss 0.984671, current_train_items 25408.
I0302 18:58:06.737027 22895071117440 run.py:483] Algo bellman_ford step 794 current loss 1.042761, current_train_items 25440.
I0302 18:58:06.754263 22895071117440 run.py:483] Algo bellman_ford step 795 current loss 0.382720, current_train_items 25472.
I0302 18:58:06.769266 22895071117440 run.py:483] Algo bellman_ford step 796 current loss 0.606359, current_train_items 25504.
I0302 18:58:06.791218 22895071117440 run.py:483] Algo bellman_ford step 797 current loss 0.934216, current_train_items 25536.
I0302 18:58:06.817252 22895071117440 run.py:483] Algo bellman_ford step 798 current loss 1.086208, current_train_items 25568.
I0302 18:58:06.846042 22895071117440 run.py:483] Algo bellman_ford step 799 current loss 1.257100, current_train_items 25600.
I0302 18:58:06.863665 22895071117440 run.py:483] Algo bellman_ford step 800 current loss 0.241406, current_train_items 25632.
I0302 18:58:06.871473 22895071117440 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:06.871579 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.883, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:58:06.887528 22895071117440 run.py:483] Algo bellman_ford step 801 current loss 0.708369, current_train_items 25664.
I0302 18:58:06.909934 22895071117440 run.py:483] Algo bellman_ford step 802 current loss 0.987993, current_train_items 25696.
I0302 18:58:06.937459 22895071117440 run.py:483] Algo bellman_ford step 803 current loss 1.167316, current_train_items 25728.
I0302 18:58:06.966979 22895071117440 run.py:483] Algo bellman_ford step 804 current loss 1.078665, current_train_items 25760.
I0302 18:58:06.984606 22895071117440 run.py:483] Algo bellman_ford step 805 current loss 0.356948, current_train_items 25792.
I0302 18:58:06.999703 22895071117440 run.py:483] Algo bellman_ford step 806 current loss 0.647840, current_train_items 25824.
I0302 18:58:07.021656 22895071117440 run.py:483] Algo bellman_ford step 807 current loss 0.936104, current_train_items 25856.
I0302 18:58:07.048476 22895071117440 run.py:483] Algo bellman_ford step 808 current loss 0.879643, current_train_items 25888.
I0302 18:58:07.076994 22895071117440 run.py:483] Algo bellman_ford step 809 current loss 1.241417, current_train_items 25920.
I0302 18:58:07.094226 22895071117440 run.py:483] Algo bellman_ford step 810 current loss 0.464888, current_train_items 25952.
I0302 18:58:07.110019 22895071117440 run.py:483] Algo bellman_ford step 811 current loss 0.863539, current_train_items 25984.
I0302 18:58:07.130876 22895071117440 run.py:483] Algo bellman_ford step 812 current loss 0.785459, current_train_items 26016.
I0302 18:58:07.157041 22895071117440 run.py:483] Algo bellman_ford step 813 current loss 0.764349, current_train_items 26048.
I0302 18:58:07.186177 22895071117440 run.py:483] Algo bellman_ford step 814 current loss 1.177900, current_train_items 26080.
I0302 18:58:07.203289 22895071117440 run.py:483] Algo bellman_ford step 815 current loss 0.397915, current_train_items 26112.
I0302 18:58:07.218819 22895071117440 run.py:483] Algo bellman_ford step 816 current loss 0.760248, current_train_items 26144.
I0302 18:58:07.240771 22895071117440 run.py:483] Algo bellman_ford step 817 current loss 0.851414, current_train_items 26176.
I0302 18:58:07.268724 22895071117440 run.py:483] Algo bellman_ford step 818 current loss 1.315896, current_train_items 26208.
I0302 18:58:07.299847 22895071117440 run.py:483] Algo bellman_ford step 819 current loss 1.543761, current_train_items 26240.
I0302 18:58:07.317177 22895071117440 run.py:483] Algo bellman_ford step 820 current loss 0.451580, current_train_items 26272.
I0302 18:58:07.332213 22895071117440 run.py:483] Algo bellman_ford step 821 current loss 0.637728, current_train_items 26304.
I0302 18:58:07.353211 22895071117440 run.py:483] Algo bellman_ford step 822 current loss 0.879942, current_train_items 26336.
I0302 18:58:07.380163 22895071117440 run.py:483] Algo bellman_ford step 823 current loss 1.140282, current_train_items 26368.
I0302 18:58:07.408711 22895071117440 run.py:483] Algo bellman_ford step 824 current loss 4.296087, current_train_items 26400.
I0302 18:58:07.425780 22895071117440 run.py:483] Algo bellman_ford step 825 current loss 0.421270, current_train_items 26432.
I0302 18:58:07.441378 22895071117440 run.py:483] Algo bellman_ford step 826 current loss 0.758274, current_train_items 26464.
I0302 18:58:07.463325 22895071117440 run.py:483] Algo bellman_ford step 827 current loss 0.943084, current_train_items 26496.
I0302 18:58:07.490439 22895071117440 run.py:483] Algo bellman_ford step 828 current loss 1.175043, current_train_items 26528.
I0302 18:58:07.519390 22895071117440 run.py:483] Algo bellman_ford step 829 current loss 1.238300, current_train_items 26560.
I0302 18:58:07.536674 22895071117440 run.py:483] Algo bellman_ford step 830 current loss 0.379018, current_train_items 26592.
I0302 18:58:07.551819 22895071117440 run.py:483] Algo bellman_ford step 831 current loss 0.615626, current_train_items 26624.
I0302 18:58:07.573310 22895071117440 run.py:483] Algo bellman_ford step 832 current loss 0.919146, current_train_items 26656.
I0302 18:58:07.600424 22895071117440 run.py:483] Algo bellman_ford step 833 current loss 1.059407, current_train_items 26688.
I0302 18:58:07.628472 22895071117440 run.py:483] Algo bellman_ford step 834 current loss 1.051709, current_train_items 26720.
I0302 18:58:07.645707 22895071117440 run.py:483] Algo bellman_ford step 835 current loss 0.417448, current_train_items 26752.
I0302 18:58:07.660836 22895071117440 run.py:483] Algo bellman_ford step 836 current loss 0.591331, current_train_items 26784.
I0302 18:58:07.682497 22895071117440 run.py:483] Algo bellman_ford step 837 current loss 1.079012, current_train_items 26816.
I0302 18:58:07.709673 22895071117440 run.py:483] Algo bellman_ford step 838 current loss 0.906190, current_train_items 26848.
I0302 18:58:07.739964 22895071117440 run.py:483] Algo bellman_ford step 839 current loss 1.281850, current_train_items 26880.
I0302 18:58:07.756762 22895071117440 run.py:483] Algo bellman_ford step 840 current loss 0.459807, current_train_items 26912.
I0302 18:58:07.772112 22895071117440 run.py:483] Algo bellman_ford step 841 current loss 0.717362, current_train_items 26944.
I0302 18:58:07.793999 22895071117440 run.py:483] Algo bellman_ford step 842 current loss 1.017889, current_train_items 26976.
I0302 18:58:07.819644 22895071117440 run.py:483] Algo bellman_ford step 843 current loss 0.877649, current_train_items 27008.
I0302 18:58:07.847939 22895071117440 run.py:483] Algo bellman_ford step 844 current loss 1.033009, current_train_items 27040.
I0302 18:58:07.865248 22895071117440 run.py:483] Algo bellman_ford step 845 current loss 0.395261, current_train_items 27072.
I0302 18:58:07.880682 22895071117440 run.py:483] Algo bellman_ford step 846 current loss 0.729804, current_train_items 27104.
I0302 18:58:07.903315 22895071117440 run.py:483] Algo bellman_ford step 847 current loss 1.007627, current_train_items 27136.
I0302 18:58:07.929646 22895071117440 run.py:483] Algo bellman_ford step 848 current loss 0.930133, current_train_items 27168.
I0302 18:58:07.959198 22895071117440 run.py:483] Algo bellman_ford step 849 current loss 1.290976, current_train_items 27200.
I0302 18:58:07.976311 22895071117440 run.py:483] Algo bellman_ford step 850 current loss 0.357408, current_train_items 27232.
I0302 18:58:07.984435 22895071117440 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.857421875, 'score': 0.857421875, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:07.984539 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.883, current avg val score is 0.857, val scores are: bellman_ford: 0.857
I0302 18:58:08.000101 22895071117440 run.py:483] Algo bellman_ford step 851 current loss 0.622969, current_train_items 27264.
I0302 18:58:08.021947 22895071117440 run.py:483] Algo bellman_ford step 852 current loss 0.871680, current_train_items 27296.
I0302 18:58:08.050543 22895071117440 run.py:483] Algo bellman_ford step 853 current loss 1.002581, current_train_items 27328.
I0302 18:58:08.079333 22895071117440 run.py:483] Algo bellman_ford step 854 current loss 1.132307, current_train_items 27360.
I0302 18:58:08.097572 22895071117440 run.py:483] Algo bellman_ford step 855 current loss 0.456513, current_train_items 27392.
I0302 18:58:08.113050 22895071117440 run.py:483] Algo bellman_ford step 856 current loss 0.509301, current_train_items 27424.
I0302 18:58:08.134104 22895071117440 run.py:483] Algo bellman_ford step 857 current loss 0.766739, current_train_items 27456.
I0302 18:58:08.161986 22895071117440 run.py:483] Algo bellman_ford step 858 current loss 1.029558, current_train_items 27488.
I0302 18:58:08.190350 22895071117440 run.py:483] Algo bellman_ford step 859 current loss 1.086540, current_train_items 27520.
I0302 18:58:08.207854 22895071117440 run.py:483] Algo bellman_ford step 860 current loss 0.417757, current_train_items 27552.
I0302 18:58:08.223626 22895071117440 run.py:483] Algo bellman_ford step 861 current loss 0.747749, current_train_items 27584.
I0302 18:58:08.244742 22895071117440 run.py:483] Algo bellman_ford step 862 current loss 0.883748, current_train_items 27616.
I0302 18:58:08.271590 22895071117440 run.py:483] Algo bellman_ford step 863 current loss 1.019748, current_train_items 27648.
I0302 18:58:08.300257 22895071117440 run.py:483] Algo bellman_ford step 864 current loss 1.258277, current_train_items 27680.
I0302 18:58:08.317122 22895071117440 run.py:483] Algo bellman_ford step 865 current loss 0.405487, current_train_items 27712.
I0302 18:58:08.332022 22895071117440 run.py:483] Algo bellman_ford step 866 current loss 0.690797, current_train_items 27744.
I0302 18:58:08.354075 22895071117440 run.py:483] Algo bellman_ford step 867 current loss 1.048809, current_train_items 27776.
I0302 18:58:08.380665 22895071117440 run.py:483] Algo bellman_ford step 868 current loss 0.931246, current_train_items 27808.
I0302 18:58:08.411298 22895071117440 run.py:483] Algo bellman_ford step 869 current loss 1.121593, current_train_items 27840.
I0302 18:58:08.428771 22895071117440 run.py:483] Algo bellman_ford step 870 current loss 0.377054, current_train_items 27872.
I0302 18:58:08.444522 22895071117440 run.py:483] Algo bellman_ford step 871 current loss 0.633672, current_train_items 27904.
I0302 18:58:08.464863 22895071117440 run.py:483] Algo bellman_ford step 872 current loss 0.738812, current_train_items 27936.
I0302 18:58:08.492471 22895071117440 run.py:483] Algo bellman_ford step 873 current loss 1.084957, current_train_items 27968.
I0302 18:58:08.520943 22895071117440 run.py:483] Algo bellman_ford step 874 current loss 1.109602, current_train_items 28000.
I0302 18:58:08.538436 22895071117440 run.py:483] Algo bellman_ford step 875 current loss 0.387084, current_train_items 28032.
I0302 18:58:08.554301 22895071117440 run.py:483] Algo bellman_ford step 876 current loss 0.638774, current_train_items 28064.
I0302 18:58:08.577450 22895071117440 run.py:483] Algo bellman_ford step 877 current loss 1.063392, current_train_items 28096.
I0302 18:58:08.603424 22895071117440 run.py:483] Algo bellman_ford step 878 current loss 0.936204, current_train_items 28128.
I0302 18:58:08.633596 22895071117440 run.py:483] Algo bellman_ford step 879 current loss 1.312441, current_train_items 28160.
I0302 18:58:08.650570 22895071117440 run.py:483] Algo bellman_ford step 880 current loss 0.485399, current_train_items 28192.
I0302 18:58:08.665582 22895071117440 run.py:483] Algo bellman_ford step 881 current loss 0.689876, current_train_items 28224.
I0302 18:58:08.686223 22895071117440 run.py:483] Algo bellman_ford step 882 current loss 0.771382, current_train_items 28256.
I0302 18:58:08.713558 22895071117440 run.py:483] Algo bellman_ford step 883 current loss 1.090971, current_train_items 28288.
I0302 18:58:08.744048 22895071117440 run.py:483] Algo bellman_ford step 884 current loss 1.462301, current_train_items 28320.
I0302 18:58:08.761679 22895071117440 run.py:483] Algo bellman_ford step 885 current loss 0.340230, current_train_items 28352.
I0302 18:58:08.777123 22895071117440 run.py:483] Algo bellman_ford step 886 current loss 0.681248, current_train_items 28384.
I0302 18:58:08.798617 22895071117440 run.py:483] Algo bellman_ford step 887 current loss 0.971208, current_train_items 28416.
I0302 18:58:08.824946 22895071117440 run.py:483] Algo bellman_ford step 888 current loss 1.057131, current_train_items 28448.
I0302 18:58:08.855394 22895071117440 run.py:483] Algo bellman_ford step 889 current loss 1.234359, current_train_items 28480.
I0302 18:58:08.872856 22895071117440 run.py:483] Algo bellman_ford step 890 current loss 0.398006, current_train_items 28512.
I0302 18:58:08.888413 22895071117440 run.py:483] Algo bellman_ford step 891 current loss 0.638906, current_train_items 28544.
I0302 18:58:08.909860 22895071117440 run.py:483] Algo bellman_ford step 892 current loss 1.032107, current_train_items 28576.
I0302 18:58:08.937670 22895071117440 run.py:483] Algo bellman_ford step 893 current loss 1.015685, current_train_items 28608.
I0302 18:58:08.966446 22895071117440 run.py:483] Algo bellman_ford step 894 current loss 1.055576, current_train_items 28640.
I0302 18:58:08.983360 22895071117440 run.py:483] Algo bellman_ford step 895 current loss 0.379613, current_train_items 28672.
I0302 18:58:08.998287 22895071117440 run.py:483] Algo bellman_ford step 896 current loss 0.582856, current_train_items 28704.
I0302 18:58:09.020264 22895071117440 run.py:483] Algo bellman_ford step 897 current loss 0.965480, current_train_items 28736.
I0302 18:58:09.047669 22895071117440 run.py:483] Algo bellman_ford step 898 current loss 1.068590, current_train_items 28768.
I0302 18:58:09.078868 22895071117440 run.py:483] Algo bellman_ford step 899 current loss 1.386088, current_train_items 28800.
I0302 18:58:09.096181 22895071117440 run.py:483] Algo bellman_ford step 900 current loss 0.430685, current_train_items 28832.
I0302 18:58:09.104066 22895071117440 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:09.104171 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.883, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 18:58:09.132401 22895071117440 run.py:483] Algo bellman_ford step 901 current loss 0.689891, current_train_items 28864.
I0302 18:58:09.154379 22895071117440 run.py:483] Algo bellman_ford step 902 current loss 0.893134, current_train_items 28896.
I0302 18:58:09.183166 22895071117440 run.py:483] Algo bellman_ford step 903 current loss 0.914589, current_train_items 28928.
I0302 18:58:09.213399 22895071117440 run.py:483] Algo bellman_ford step 904 current loss 1.066400, current_train_items 28960.
I0302 18:58:09.231181 22895071117440 run.py:483] Algo bellman_ford step 905 current loss 0.328226, current_train_items 28992.
I0302 18:58:09.247015 22895071117440 run.py:483] Algo bellman_ford step 906 current loss 0.745611, current_train_items 29024.
I0302 18:58:09.268648 22895071117440 run.py:483] Algo bellman_ford step 907 current loss 0.780061, current_train_items 29056.
I0302 18:58:09.296895 22895071117440 run.py:483] Algo bellman_ford step 908 current loss 1.034244, current_train_items 29088.
I0302 18:58:09.326788 22895071117440 run.py:483] Algo bellman_ford step 909 current loss 1.188154, current_train_items 29120.
I0302 18:58:09.343919 22895071117440 run.py:483] Algo bellman_ford step 910 current loss 0.354329, current_train_items 29152.
I0302 18:58:09.359999 22895071117440 run.py:483] Algo bellman_ford step 911 current loss 0.768087, current_train_items 29184.
I0302 18:58:09.382168 22895071117440 run.py:483] Algo bellman_ford step 912 current loss 0.944270, current_train_items 29216.
I0302 18:58:09.407836 22895071117440 run.py:483] Algo bellman_ford step 913 current loss 0.862251, current_train_items 29248.
I0302 18:58:09.438112 22895071117440 run.py:483] Algo bellman_ford step 914 current loss 1.258687, current_train_items 29280.
I0302 18:58:09.455149 22895071117440 run.py:483] Algo bellman_ford step 915 current loss 0.352551, current_train_items 29312.
I0302 18:58:09.470683 22895071117440 run.py:483] Algo bellman_ford step 916 current loss 0.626246, current_train_items 29344.
I0302 18:58:09.492647 22895071117440 run.py:483] Algo bellman_ford step 917 current loss 1.107269, current_train_items 29376.
I0302 18:58:09.520040 22895071117440 run.py:483] Algo bellman_ford step 918 current loss 1.171091, current_train_items 29408.
I0302 18:58:09.550608 22895071117440 run.py:483] Algo bellman_ford step 919 current loss 1.393014, current_train_items 29440.
I0302 18:58:09.567975 22895071117440 run.py:483] Algo bellman_ford step 920 current loss 0.365863, current_train_items 29472.
I0302 18:58:09.583626 22895071117440 run.py:483] Algo bellman_ford step 921 current loss 0.668089, current_train_items 29504.
I0302 18:58:09.605771 22895071117440 run.py:483] Algo bellman_ford step 922 current loss 0.843434, current_train_items 29536.
I0302 18:58:09.632427 22895071117440 run.py:483] Algo bellman_ford step 923 current loss 0.829181, current_train_items 29568.
I0302 18:58:09.661546 22895071117440 run.py:483] Algo bellman_ford step 924 current loss 1.118964, current_train_items 29600.
I0302 18:58:09.678577 22895071117440 run.py:483] Algo bellman_ford step 925 current loss 0.499070, current_train_items 29632.
I0302 18:58:09.693662 22895071117440 run.py:483] Algo bellman_ford step 926 current loss 0.579334, current_train_items 29664.
I0302 18:58:09.715372 22895071117440 run.py:483] Algo bellman_ford step 927 current loss 0.863551, current_train_items 29696.
I0302 18:58:09.743231 22895071117440 run.py:483] Algo bellman_ford step 928 current loss 1.005354, current_train_items 29728.
I0302 18:58:09.771599 22895071117440 run.py:483] Algo bellman_ford step 929 current loss 1.045939, current_train_items 29760.
I0302 18:58:09.788772 22895071117440 run.py:483] Algo bellman_ford step 930 current loss 0.383968, current_train_items 29792.
I0302 18:58:09.804056 22895071117440 run.py:483] Algo bellman_ford step 931 current loss 0.761800, current_train_items 29824.
I0302 18:58:09.825525 22895071117440 run.py:483] Algo bellman_ford step 932 current loss 0.848575, current_train_items 29856.
I0302 18:58:09.851844 22895071117440 run.py:483] Algo bellman_ford step 933 current loss 0.976901, current_train_items 29888.
I0302 18:58:09.881922 22895071117440 run.py:483] Algo bellman_ford step 934 current loss 1.176924, current_train_items 29920.
I0302 18:58:09.898988 22895071117440 run.py:483] Algo bellman_ford step 935 current loss 0.411554, current_train_items 29952.
I0302 18:58:09.914313 22895071117440 run.py:483] Algo bellman_ford step 936 current loss 0.662387, current_train_items 29984.
I0302 18:58:09.935107 22895071117440 run.py:483] Algo bellman_ford step 937 current loss 0.741222, current_train_items 30016.
I0302 18:58:09.961694 22895071117440 run.py:483] Algo bellman_ford step 938 current loss 0.966814, current_train_items 30048.
I0302 18:58:09.991763 22895071117440 run.py:483] Algo bellman_ford step 939 current loss 1.307754, current_train_items 30080.
I0302 18:58:10.008788 22895071117440 run.py:483] Algo bellman_ford step 940 current loss 0.468191, current_train_items 30112.
I0302 18:58:10.024132 22895071117440 run.py:483] Algo bellman_ford step 941 current loss 0.771640, current_train_items 30144.
I0302 18:58:10.046161 22895071117440 run.py:483] Algo bellman_ford step 942 current loss 0.979216, current_train_items 30176.
I0302 18:58:10.073305 22895071117440 run.py:483] Algo bellman_ford step 943 current loss 0.897213, current_train_items 30208.
I0302 18:58:10.103604 22895071117440 run.py:483] Algo bellman_ford step 944 current loss 1.343138, current_train_items 30240.
I0302 18:58:10.120655 22895071117440 run.py:483] Algo bellman_ford step 945 current loss 0.330681, current_train_items 30272.
I0302 18:58:10.135509 22895071117440 run.py:483] Algo bellman_ford step 946 current loss 0.655048, current_train_items 30304.
I0302 18:58:10.158478 22895071117440 run.py:483] Algo bellman_ford step 947 current loss 0.923105, current_train_items 30336.
I0302 18:58:10.185072 22895071117440 run.py:483] Algo bellman_ford step 948 current loss 0.840876, current_train_items 30368.
I0302 18:58:10.214399 22895071117440 run.py:483] Algo bellman_ford step 949 current loss 1.233541, current_train_items 30400.
I0302 18:58:10.231571 22895071117440 run.py:483] Algo bellman_ford step 950 current loss 0.412308, current_train_items 30432.
I0302 18:58:10.239674 22895071117440 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:10.239781 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.892, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 18:58:10.255778 22895071117440 run.py:483] Algo bellman_ford step 951 current loss 0.597924, current_train_items 30464.
I0302 18:58:10.277497 22895071117440 run.py:483] Algo bellman_ford step 952 current loss 0.866857, current_train_items 30496.
I0302 18:58:10.305011 22895071117440 run.py:483] Algo bellman_ford step 953 current loss 1.092297, current_train_items 30528.
I0302 18:58:10.332342 22895071117440 run.py:483] Algo bellman_ford step 954 current loss 0.973714, current_train_items 30560.
I0302 18:58:10.350016 22895071117440 run.py:483] Algo bellman_ford step 955 current loss 0.326804, current_train_items 30592.
I0302 18:58:10.365342 22895071117440 run.py:483] Algo bellman_ford step 956 current loss 0.673966, current_train_items 30624.
I0302 18:58:10.387131 22895071117440 run.py:483] Algo bellman_ford step 957 current loss 0.958972, current_train_items 30656.
I0302 18:58:10.412704 22895071117440 run.py:483] Algo bellman_ford step 958 current loss 0.908358, current_train_items 30688.
I0302 18:58:10.442276 22895071117440 run.py:483] Algo bellman_ford step 959 current loss 1.089607, current_train_items 30720.
I0302 18:58:10.459671 22895071117440 run.py:483] Algo bellman_ford step 960 current loss 0.357611, current_train_items 30752.
I0302 18:58:10.475737 22895071117440 run.py:483] Algo bellman_ford step 961 current loss 0.786604, current_train_items 30784.
I0302 18:58:10.495952 22895071117440 run.py:483] Algo bellman_ford step 962 current loss 0.695168, current_train_items 30816.
I0302 18:58:10.521215 22895071117440 run.py:483] Algo bellman_ford step 963 current loss 0.824264, current_train_items 30848.
I0302 18:58:10.550891 22895071117440 run.py:483] Algo bellman_ford step 964 current loss 1.159699, current_train_items 30880.
I0302 18:58:10.568063 22895071117440 run.py:483] Algo bellman_ford step 965 current loss 0.548038, current_train_items 30912.
I0302 18:58:10.583172 22895071117440 run.py:483] Algo bellman_ford step 966 current loss 0.606109, current_train_items 30944.
I0302 18:58:10.605202 22895071117440 run.py:483] Algo bellman_ford step 967 current loss 0.992543, current_train_items 30976.
I0302 18:58:10.632636 22895071117440 run.py:483] Algo bellman_ford step 968 current loss 1.074547, current_train_items 31008.
I0302 18:58:10.661513 22895071117440 run.py:483] Algo bellman_ford step 969 current loss 1.268795, current_train_items 31040.
I0302 18:58:10.678750 22895071117440 run.py:483] Algo bellman_ford step 970 current loss 0.433571, current_train_items 31072.
I0302 18:58:10.694483 22895071117440 run.py:483] Algo bellman_ford step 971 current loss 0.671519, current_train_items 31104.
I0302 18:58:10.716078 22895071117440 run.py:483] Algo bellman_ford step 972 current loss 0.950320, current_train_items 31136.
I0302 18:58:10.744003 22895071117440 run.py:483] Algo bellman_ford step 973 current loss 1.057570, current_train_items 31168.
I0302 18:58:10.774584 22895071117440 run.py:483] Algo bellman_ford step 974 current loss 1.263997, current_train_items 31200.
I0302 18:58:10.792045 22895071117440 run.py:483] Algo bellman_ford step 975 current loss 0.473508, current_train_items 31232.
I0302 18:58:10.807370 22895071117440 run.py:483] Algo bellman_ford step 976 current loss 0.544847, current_train_items 31264.
I0302 18:58:10.828449 22895071117440 run.py:483] Algo bellman_ford step 977 current loss 0.885008, current_train_items 31296.
I0302 18:58:10.854799 22895071117440 run.py:483] Algo bellman_ford step 978 current loss 0.979758, current_train_items 31328.
I0302 18:58:10.883507 22895071117440 run.py:483] Algo bellman_ford step 979 current loss 1.101223, current_train_items 31360.
I0302 18:58:10.900424 22895071117440 run.py:483] Algo bellman_ford step 980 current loss 0.313511, current_train_items 31392.
I0302 18:58:10.915805 22895071117440 run.py:483] Algo bellman_ford step 981 current loss 0.657778, current_train_items 31424.
I0302 18:58:10.937031 22895071117440 run.py:483] Algo bellman_ford step 982 current loss 0.744764, current_train_items 31456.
I0302 18:58:10.962882 22895071117440 run.py:483] Algo bellman_ford step 983 current loss 0.892487, current_train_items 31488.
I0302 18:58:10.992529 22895071117440 run.py:483] Algo bellman_ford step 984 current loss 1.030415, current_train_items 31520.
I0302 18:58:11.010020 22895071117440 run.py:483] Algo bellman_ford step 985 current loss 0.363106, current_train_items 31552.
I0302 18:58:11.026020 22895071117440 run.py:483] Algo bellman_ford step 986 current loss 0.673962, current_train_items 31584.
I0302 18:58:11.046694 22895071117440 run.py:483] Algo bellman_ford step 987 current loss 0.869325, current_train_items 31616.
I0302 18:58:11.073659 22895071117440 run.py:483] Algo bellman_ford step 988 current loss 0.894448, current_train_items 31648.
I0302 18:58:11.102569 22895071117440 run.py:483] Algo bellman_ford step 989 current loss 0.930390, current_train_items 31680.
I0302 18:58:11.119771 22895071117440 run.py:483] Algo bellman_ford step 990 current loss 0.341474, current_train_items 31712.
I0302 18:58:11.135013 22895071117440 run.py:483] Algo bellman_ford step 991 current loss 0.588299, current_train_items 31744.
I0302 18:58:11.156424 22895071117440 run.py:483] Algo bellman_ford step 992 current loss 0.868806, current_train_items 31776.
I0302 18:58:11.181790 22895071117440 run.py:483] Algo bellman_ford step 993 current loss 0.872553, current_train_items 31808.
I0302 18:58:11.213168 22895071117440 run.py:483] Algo bellman_ford step 994 current loss 1.316811, current_train_items 31840.
I0302 18:58:11.230170 22895071117440 run.py:483] Algo bellman_ford step 995 current loss 0.439136, current_train_items 31872.
I0302 18:58:11.245265 22895071117440 run.py:483] Algo bellman_ford step 996 current loss 0.649270, current_train_items 31904.
I0302 18:58:11.266471 22895071117440 run.py:483] Algo bellman_ford step 997 current loss 0.805380, current_train_items 31936.
I0302 18:58:11.293971 22895071117440 run.py:483] Algo bellman_ford step 998 current loss 0.922671, current_train_items 31968.
I0302 18:58:11.322821 22895071117440 run.py:483] Algo bellman_ford step 999 current loss 1.015918, current_train_items 32000.
I0302 18:58:11.340257 22895071117440 run.py:483] Algo bellman_ford step 1000 current loss 0.331161, current_train_items 32032.
I0302 18:58:11.347966 22895071117440 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:11.348072 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.892, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 18:58:11.375378 22895071117440 run.py:483] Algo bellman_ford step 1001 current loss 0.714025, current_train_items 32064.
I0302 18:58:11.397004 22895071117440 run.py:483] Algo bellman_ford step 1002 current loss 0.764596, current_train_items 32096.
I0302 18:58:11.423296 22895071117440 run.py:483] Algo bellman_ford step 1003 current loss 1.013511, current_train_items 32128.
I0302 18:58:11.455587 22895071117440 run.py:483] Algo bellman_ford step 1004 current loss 1.180201, current_train_items 32160.
I0302 18:58:11.473660 22895071117440 run.py:483] Algo bellman_ford step 1005 current loss 0.361097, current_train_items 32192.
I0302 18:58:11.488829 22895071117440 run.py:483] Algo bellman_ford step 1006 current loss 0.653917, current_train_items 32224.
I0302 18:58:11.511294 22895071117440 run.py:483] Algo bellman_ford step 1007 current loss 0.803780, current_train_items 32256.
I0302 18:58:11.537600 22895071117440 run.py:483] Algo bellman_ford step 1008 current loss 0.855613, current_train_items 32288.
I0302 18:58:11.568439 22895071117440 run.py:483] Algo bellman_ford step 1009 current loss 0.970814, current_train_items 32320.
I0302 18:58:11.585668 22895071117440 run.py:483] Algo bellman_ford step 1010 current loss 0.322075, current_train_items 32352.
I0302 18:58:11.601039 22895071117440 run.py:483] Algo bellman_ford step 1011 current loss 0.629783, current_train_items 32384.
I0302 18:58:11.623603 22895071117440 run.py:483] Algo bellman_ford step 1012 current loss 0.816981, current_train_items 32416.
I0302 18:58:11.651371 22895071117440 run.py:483] Algo bellman_ford step 1013 current loss 1.036523, current_train_items 32448.
I0302 18:58:11.681480 22895071117440 run.py:483] Algo bellman_ford step 1014 current loss 1.007921, current_train_items 32480.
I0302 18:58:11.698844 22895071117440 run.py:483] Algo bellman_ford step 1015 current loss 0.405303, current_train_items 32512.
I0302 18:58:11.714382 22895071117440 run.py:483] Algo bellman_ford step 1016 current loss 0.694035, current_train_items 32544.
I0302 18:58:11.736602 22895071117440 run.py:483] Algo bellman_ford step 1017 current loss 0.978433, current_train_items 32576.
I0302 18:58:11.764122 22895071117440 run.py:483] Algo bellman_ford step 1018 current loss 0.909147, current_train_items 32608.
I0302 18:58:11.796482 22895071117440 run.py:483] Algo bellman_ford step 1019 current loss 1.390142, current_train_items 32640.
I0302 18:58:11.813597 22895071117440 run.py:483] Algo bellman_ford step 1020 current loss 0.387608, current_train_items 32672.
I0302 18:58:11.828642 22895071117440 run.py:483] Algo bellman_ford step 1021 current loss 0.597544, current_train_items 32704.
I0302 18:58:11.850171 22895071117440 run.py:483] Algo bellman_ford step 1022 current loss 0.711202, current_train_items 32736.
I0302 18:58:11.876593 22895071117440 run.py:483] Algo bellman_ford step 1023 current loss 0.900415, current_train_items 32768.
I0302 18:58:11.904682 22895071117440 run.py:483] Algo bellman_ford step 1024 current loss 1.081647, current_train_items 32800.
I0302 18:58:11.922154 22895071117440 run.py:483] Algo bellman_ford step 1025 current loss 0.343917, current_train_items 32832.
I0302 18:58:11.938068 22895071117440 run.py:483] Algo bellman_ford step 1026 current loss 0.643152, current_train_items 32864.
I0302 18:58:11.959786 22895071117440 run.py:483] Algo bellman_ford step 1027 current loss 0.864691, current_train_items 32896.
I0302 18:58:11.987821 22895071117440 run.py:483] Algo bellman_ford step 1028 current loss 1.082796, current_train_items 32928.
I0302 18:58:12.016728 22895071117440 run.py:483] Algo bellman_ford step 1029 current loss 1.135084, current_train_items 32960.
I0302 18:58:12.034069 22895071117440 run.py:483] Algo bellman_ford step 1030 current loss 0.456849, current_train_items 32992.
I0302 18:58:12.048967 22895071117440 run.py:483] Algo bellman_ford step 1031 current loss 0.552997, current_train_items 33024.
I0302 18:58:12.070348 22895071117440 run.py:483] Algo bellman_ford step 1032 current loss 0.793830, current_train_items 33056.
I0302 18:58:12.096948 22895071117440 run.py:483] Algo bellman_ford step 1033 current loss 0.920465, current_train_items 33088.
I0302 18:58:12.127424 22895071117440 run.py:483] Algo bellman_ford step 1034 current loss 1.168160, current_train_items 33120.
I0302 18:58:12.144539 22895071117440 run.py:483] Algo bellman_ford step 1035 current loss 0.343015, current_train_items 33152.
I0302 18:58:12.159825 22895071117440 run.py:483] Algo bellman_ford step 1036 current loss 0.577156, current_train_items 33184.
I0302 18:58:12.182153 22895071117440 run.py:483] Algo bellman_ford step 1037 current loss 0.761320, current_train_items 33216.
I0302 18:58:12.209850 22895071117440 run.py:483] Algo bellman_ford step 1038 current loss 1.055990, current_train_items 33248.
I0302 18:58:12.241417 22895071117440 run.py:483] Algo bellman_ford step 1039 current loss 1.250819, current_train_items 33280.
I0302 18:58:12.258442 22895071117440 run.py:483] Algo bellman_ford step 1040 current loss 0.327719, current_train_items 33312.
I0302 18:58:12.274435 22895071117440 run.py:483] Algo bellman_ford step 1041 current loss 0.753975, current_train_items 33344.
I0302 18:58:12.296736 22895071117440 run.py:483] Algo bellman_ford step 1042 current loss 0.929339, current_train_items 33376.
I0302 18:58:12.324052 22895071117440 run.py:483] Algo bellman_ford step 1043 current loss 0.865878, current_train_items 33408.
I0302 18:58:12.353704 22895071117440 run.py:483] Algo bellman_ford step 1044 current loss 1.001172, current_train_items 33440.
I0302 18:58:12.370850 22895071117440 run.py:483] Algo bellman_ford step 1045 current loss 0.257564, current_train_items 33472.
I0302 18:58:12.385819 22895071117440 run.py:483] Algo bellman_ford step 1046 current loss 0.502488, current_train_items 33504.
I0302 18:58:12.407885 22895071117440 run.py:483] Algo bellman_ford step 1047 current loss 0.873721, current_train_items 33536.
I0302 18:58:12.435769 22895071117440 run.py:483] Algo bellman_ford step 1048 current loss 1.117927, current_train_items 33568.
I0302 18:58:12.465987 22895071117440 run.py:483] Algo bellman_ford step 1049 current loss 1.013722, current_train_items 33600.
I0302 18:58:12.483142 22895071117440 run.py:483] Algo bellman_ford step 1050 current loss 0.327505, current_train_items 33632.
I0302 18:58:12.491132 22895071117440 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:12.491239 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.897, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 18:58:12.506793 22895071117440 run.py:483] Algo bellman_ford step 1051 current loss 0.576576, current_train_items 33664.
I0302 18:58:12.529243 22895071117440 run.py:483] Algo bellman_ford step 1052 current loss 0.857318, current_train_items 33696.
I0302 18:58:12.555656 22895071117440 run.py:483] Algo bellman_ford step 1053 current loss 0.879495, current_train_items 33728.
I0302 18:58:12.585717 22895071117440 run.py:483] Algo bellman_ford step 1054 current loss 1.061980, current_train_items 33760.
I0302 18:58:12.603332 22895071117440 run.py:483] Algo bellman_ford step 1055 current loss 0.384303, current_train_items 33792.
I0302 18:58:12.619008 22895071117440 run.py:483] Algo bellman_ford step 1056 current loss 0.641510, current_train_items 33824.
I0302 18:58:12.640847 22895071117440 run.py:483] Algo bellman_ford step 1057 current loss 0.881540, current_train_items 33856.
I0302 18:58:12.668846 22895071117440 run.py:483] Algo bellman_ford step 1058 current loss 0.965153, current_train_items 33888.
I0302 18:58:12.696085 22895071117440 run.py:483] Algo bellman_ford step 1059 current loss 0.927738, current_train_items 33920.
I0302 18:58:12.713316 22895071117440 run.py:483] Algo bellman_ford step 1060 current loss 0.312594, current_train_items 33952.
I0302 18:58:12.728846 22895071117440 run.py:483] Algo bellman_ford step 1061 current loss 0.649378, current_train_items 33984.
I0302 18:58:12.748706 22895071117440 run.py:483] Algo bellman_ford step 1062 current loss 0.708159, current_train_items 34016.
I0302 18:58:12.775094 22895071117440 run.py:483] Algo bellman_ford step 1063 current loss 1.071715, current_train_items 34048.
I0302 18:58:12.805371 22895071117440 run.py:483] Algo bellman_ford step 1064 current loss 1.289622, current_train_items 34080.
I0302 18:58:12.822331 22895071117440 run.py:483] Algo bellman_ford step 1065 current loss 0.349988, current_train_items 34112.
I0302 18:58:12.837538 22895071117440 run.py:483] Algo bellman_ford step 1066 current loss 0.635422, current_train_items 34144.
I0302 18:58:12.859233 22895071117440 run.py:483] Algo bellman_ford step 1067 current loss 0.889973, current_train_items 34176.
I0302 18:58:12.886430 22895071117440 run.py:483] Algo bellman_ford step 1068 current loss 0.825826, current_train_items 34208.
I0302 18:58:12.917640 22895071117440 run.py:483] Algo bellman_ford step 1069 current loss 1.161083, current_train_items 34240.
I0302 18:58:12.934825 22895071117440 run.py:483] Algo bellman_ford step 1070 current loss 0.345587, current_train_items 34272.
I0302 18:58:12.950729 22895071117440 run.py:483] Algo bellman_ford step 1071 current loss 0.727271, current_train_items 34304.
I0302 18:58:12.972054 22895071117440 run.py:483] Algo bellman_ford step 1072 current loss 0.842120, current_train_items 34336.
I0302 18:58:12.999458 22895071117440 run.py:483] Algo bellman_ford step 1073 current loss 0.789947, current_train_items 34368.
I0302 18:58:13.027910 22895071117440 run.py:483] Algo bellman_ford step 1074 current loss 0.986465, current_train_items 34400.
I0302 18:58:13.045355 22895071117440 run.py:483] Algo bellman_ford step 1075 current loss 0.301673, current_train_items 34432.
I0302 18:58:13.061126 22895071117440 run.py:483] Algo bellman_ford step 1076 current loss 0.944563, current_train_items 34464.
I0302 18:58:13.082728 22895071117440 run.py:483] Algo bellman_ford step 1077 current loss 0.895416, current_train_items 34496.
I0302 18:58:13.110324 22895071117440 run.py:483] Algo bellman_ford step 1078 current loss 0.961045, current_train_items 34528.
I0302 18:58:13.139183 22895071117440 run.py:483] Algo bellman_ford step 1079 current loss 1.080366, current_train_items 34560.
I0302 18:58:13.156224 22895071117440 run.py:483] Algo bellman_ford step 1080 current loss 0.367460, current_train_items 34592.
I0302 18:58:13.171818 22895071117440 run.py:483] Algo bellman_ford step 1081 current loss 0.679726, current_train_items 34624.
I0302 18:58:13.192768 22895071117440 run.py:483] Algo bellman_ford step 1082 current loss 0.864760, current_train_items 34656.
I0302 18:58:13.219146 22895071117440 run.py:483] Algo bellman_ford step 1083 current loss 0.899170, current_train_items 34688.
I0302 18:58:13.249195 22895071117440 run.py:483] Algo bellman_ford step 1084 current loss 1.209543, current_train_items 34720.
I0302 18:58:13.266616 22895071117440 run.py:483] Algo bellman_ford step 1085 current loss 0.391467, current_train_items 34752.
I0302 18:58:13.282292 22895071117440 run.py:483] Algo bellman_ford step 1086 current loss 0.595821, current_train_items 34784.
I0302 18:58:13.303664 22895071117440 run.py:483] Algo bellman_ford step 1087 current loss 0.778041, current_train_items 34816.
I0302 18:58:13.329342 22895071117440 run.py:483] Algo bellman_ford step 1088 current loss 0.717569, current_train_items 34848.
I0302 18:58:13.357534 22895071117440 run.py:483] Algo bellman_ford step 1089 current loss 0.918105, current_train_items 34880.
I0302 18:58:13.375023 22895071117440 run.py:483] Algo bellman_ford step 1090 current loss 0.419015, current_train_items 34912.
I0302 18:58:13.390883 22895071117440 run.py:483] Algo bellman_ford step 1091 current loss 0.669879, current_train_items 34944.
I0302 18:58:13.411963 22895071117440 run.py:483] Algo bellman_ford step 1092 current loss 0.901827, current_train_items 34976.
I0302 18:58:13.439706 22895071117440 run.py:483] Algo bellman_ford step 1093 current loss 1.043217, current_train_items 35008.
I0302 18:58:13.469911 22895071117440 run.py:483] Algo bellman_ford step 1094 current loss 0.986194, current_train_items 35040.
I0302 18:58:13.486751 22895071117440 run.py:483] Algo bellman_ford step 1095 current loss 0.405872, current_train_items 35072.
I0302 18:58:13.501885 22895071117440 run.py:483] Algo bellman_ford step 1096 current loss 0.718370, current_train_items 35104.
I0302 18:58:13.523548 22895071117440 run.py:483] Algo bellman_ford step 1097 current loss 0.881253, current_train_items 35136.
I0302 18:58:13.550817 22895071117440 run.py:483] Algo bellman_ford step 1098 current loss 0.949677, current_train_items 35168.
I0302 18:58:13.579893 22895071117440 run.py:483] Algo bellman_ford step 1099 current loss 1.034119, current_train_items 35200.
I0302 18:58:13.597329 22895071117440 run.py:483] Algo bellman_ford step 1100 current loss 0.397033, current_train_items 35232.
I0302 18:58:13.605055 22895071117440 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:13.605160 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.897, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:58:13.634489 22895071117440 run.py:483] Algo bellman_ford step 1101 current loss 0.662175, current_train_items 35264.
I0302 18:58:13.655506 22895071117440 run.py:483] Algo bellman_ford step 1102 current loss 0.770792, current_train_items 35296.
I0302 18:58:13.683823 22895071117440 run.py:483] Algo bellman_ford step 1103 current loss 0.895052, current_train_items 35328.
I0302 18:58:13.715537 22895071117440 run.py:483] Algo bellman_ford step 1104 current loss 1.202893, current_train_items 35360.
I0302 18:58:13.733156 22895071117440 run.py:483] Algo bellman_ford step 1105 current loss 0.377243, current_train_items 35392.
I0302 18:58:13.748322 22895071117440 run.py:483] Algo bellman_ford step 1106 current loss 0.640493, current_train_items 35424.
I0302 18:58:13.770006 22895071117440 run.py:483] Algo bellman_ford step 1107 current loss 0.834534, current_train_items 35456.
I0302 18:58:13.796299 22895071117440 run.py:483] Algo bellman_ford step 1108 current loss 1.010757, current_train_items 35488.
I0302 18:58:13.824037 22895071117440 run.py:483] Algo bellman_ford step 1109 current loss 0.957472, current_train_items 35520.
I0302 18:58:13.841282 22895071117440 run.py:483] Algo bellman_ford step 1110 current loss 0.402964, current_train_items 35552.
I0302 18:58:13.856690 22895071117440 run.py:483] Algo bellman_ford step 1111 current loss 0.651805, current_train_items 35584.
I0302 18:58:13.878161 22895071117440 run.py:483] Algo bellman_ford step 1112 current loss 0.755389, current_train_items 35616.
I0302 18:58:13.903512 22895071117440 run.py:483] Algo bellman_ford step 1113 current loss 0.843388, current_train_items 35648.
I0302 18:58:13.934190 22895071117440 run.py:483] Algo bellman_ford step 1114 current loss 1.200100, current_train_items 35680.
I0302 18:58:13.951242 22895071117440 run.py:483] Algo bellman_ford step 1115 current loss 0.360747, current_train_items 35712.
I0302 18:58:13.966183 22895071117440 run.py:483] Algo bellman_ford step 1116 current loss 0.608352, current_train_items 35744.
I0302 18:58:13.988233 22895071117440 run.py:483] Algo bellman_ford step 1117 current loss 0.795358, current_train_items 35776.
I0302 18:58:14.015678 22895071117440 run.py:483] Algo bellman_ford step 1118 current loss 0.841605, current_train_items 35808.
I0302 18:58:14.045655 22895071117440 run.py:483] Algo bellman_ford step 1119 current loss 1.267425, current_train_items 35840.
I0302 18:58:14.062952 22895071117440 run.py:483] Algo bellman_ford step 1120 current loss 0.411528, current_train_items 35872.
I0302 18:58:14.077762 22895071117440 run.py:483] Algo bellman_ford step 1121 current loss 0.508667, current_train_items 35904.
I0302 18:58:14.099471 22895071117440 run.py:483] Algo bellman_ford step 1122 current loss 0.825596, current_train_items 35936.
I0302 18:58:14.125240 22895071117440 run.py:483] Algo bellman_ford step 1123 current loss 0.858863, current_train_items 35968.
I0302 18:58:14.156031 22895071117440 run.py:483] Algo bellman_ford step 1124 current loss 1.097742, current_train_items 36000.
I0302 18:58:14.173156 22895071117440 run.py:483] Algo bellman_ford step 1125 current loss 0.345956, current_train_items 36032.
I0302 18:58:14.187996 22895071117440 run.py:483] Algo bellman_ford step 1126 current loss 0.545800, current_train_items 36064.
I0302 18:58:14.209442 22895071117440 run.py:483] Algo bellman_ford step 1127 current loss 0.840921, current_train_items 36096.
I0302 18:58:14.236379 22895071117440 run.py:483] Algo bellman_ford step 1128 current loss 0.987048, current_train_items 36128.
I0302 18:58:14.263572 22895071117440 run.py:483] Algo bellman_ford step 1129 current loss 1.202477, current_train_items 36160.
I0302 18:58:14.280776 22895071117440 run.py:483] Algo bellman_ford step 1130 current loss 0.402020, current_train_items 36192.
I0302 18:58:14.295945 22895071117440 run.py:483] Algo bellman_ford step 1131 current loss 0.572557, current_train_items 36224.
I0302 18:58:14.317495 22895071117440 run.py:483] Algo bellman_ford step 1132 current loss 0.845894, current_train_items 36256.
I0302 18:58:14.344439 22895071117440 run.py:483] Algo bellman_ford step 1133 current loss 0.969568, current_train_items 36288.
I0302 18:58:14.372960 22895071117440 run.py:483] Algo bellman_ford step 1134 current loss 0.987781, current_train_items 36320.
I0302 18:58:14.389880 22895071117440 run.py:483] Algo bellman_ford step 1135 current loss 0.361140, current_train_items 36352.
I0302 18:58:14.405218 22895071117440 run.py:483] Algo bellman_ford step 1136 current loss 0.744630, current_train_items 36384.
I0302 18:58:14.425914 22895071117440 run.py:483] Algo bellman_ford step 1137 current loss 0.747951, current_train_items 36416.
I0302 18:58:14.453804 22895071117440 run.py:483] Algo bellman_ford step 1138 current loss 0.986237, current_train_items 36448.
I0302 18:58:14.482860 22895071117440 run.py:483] Algo bellman_ford step 1139 current loss 0.898329, current_train_items 36480.
I0302 18:58:14.499894 22895071117440 run.py:483] Algo bellman_ford step 1140 current loss 0.333807, current_train_items 36512.
I0302 18:58:14.515385 22895071117440 run.py:483] Algo bellman_ford step 1141 current loss 0.589696, current_train_items 36544.
I0302 18:58:14.536059 22895071117440 run.py:483] Algo bellman_ford step 1142 current loss 0.748250, current_train_items 36576.
I0302 18:58:14.562219 22895071117440 run.py:483] Algo bellman_ford step 1143 current loss 0.925033, current_train_items 36608.
I0302 18:58:14.591265 22895071117440 run.py:483] Algo bellman_ford step 1144 current loss 1.013299, current_train_items 36640.
I0302 18:58:14.608546 22895071117440 run.py:483] Algo bellman_ford step 1145 current loss 0.497251, current_train_items 36672.
I0302 18:58:14.623929 22895071117440 run.py:483] Algo bellman_ford step 1146 current loss 0.623145, current_train_items 36704.
I0302 18:58:14.645538 22895071117440 run.py:483] Algo bellman_ford step 1147 current loss 0.788526, current_train_items 36736.
I0302 18:58:14.672581 22895071117440 run.py:483] Algo bellman_ford step 1148 current loss 0.922900, current_train_items 36768.
I0302 18:58:14.699788 22895071117440 run.py:483] Algo bellman_ford step 1149 current loss 0.936673, current_train_items 36800.
I0302 18:58:14.716936 22895071117440 run.py:483] Algo bellman_ford step 1150 current loss 0.403991, current_train_items 36832.
I0302 18:58:14.724997 22895071117440 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:14.725104 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 18:58:14.740891 22895071117440 run.py:483] Algo bellman_ford step 1151 current loss 0.610316, current_train_items 36864.
I0302 18:58:14.763255 22895071117440 run.py:483] Algo bellman_ford step 1152 current loss 0.911058, current_train_items 36896.
I0302 18:58:14.792296 22895071117440 run.py:483] Algo bellman_ford step 1153 current loss 0.943424, current_train_items 36928.
W0302 18:58:14.812669 22895071117440 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:20.214762 22895071117440 run.py:483] Algo bellman_ford step 1154 current loss 1.051382, current_train_items 36960.
I0302 18:58:20.234012 22895071117440 run.py:483] Algo bellman_ford step 1155 current loss 0.454815, current_train_items 36992.
I0302 18:58:20.249602 22895071117440 run.py:483] Algo bellman_ford step 1156 current loss 0.591347, current_train_items 37024.
I0302 18:58:20.271391 22895071117440 run.py:483] Algo bellman_ford step 1157 current loss 0.743618, current_train_items 37056.
I0302 18:58:20.298680 22895071117440 run.py:483] Algo bellman_ford step 1158 current loss 0.983214, current_train_items 37088.
I0302 18:58:20.326987 22895071117440 run.py:483] Algo bellman_ford step 1159 current loss 0.971969, current_train_items 37120.
I0302 18:58:20.345497 22895071117440 run.py:483] Algo bellman_ford step 1160 current loss 0.493391, current_train_items 37152.
I0302 18:58:20.361425 22895071117440 run.py:483] Algo bellman_ford step 1161 current loss 0.577189, current_train_items 37184.
I0302 18:58:20.382200 22895071117440 run.py:483] Algo bellman_ford step 1162 current loss 0.815786, current_train_items 37216.
I0302 18:58:20.409396 22895071117440 run.py:483] Algo bellman_ford step 1163 current loss 0.897553, current_train_items 37248.
I0302 18:58:20.439650 22895071117440 run.py:483] Algo bellman_ford step 1164 current loss 1.256399, current_train_items 37280.
I0302 18:58:20.457212 22895071117440 run.py:483] Algo bellman_ford step 1165 current loss 0.307968, current_train_items 37312.
I0302 18:58:20.472653 22895071117440 run.py:483] Algo bellman_ford step 1166 current loss 0.621886, current_train_items 37344.
I0302 18:58:20.494505 22895071117440 run.py:483] Algo bellman_ford step 1167 current loss 0.995835, current_train_items 37376.
I0302 18:58:20.522010 22895071117440 run.py:483] Algo bellman_ford step 1168 current loss 1.019355, current_train_items 37408.
I0302 18:58:20.553398 22895071117440 run.py:483] Algo bellman_ford step 1169 current loss 1.120115, current_train_items 37440.
I0302 18:58:20.571835 22895071117440 run.py:483] Algo bellman_ford step 1170 current loss 0.338616, current_train_items 37472.
I0302 18:58:20.587945 22895071117440 run.py:483] Algo bellman_ford step 1171 current loss 0.770967, current_train_items 37504.
I0302 18:58:20.609573 22895071117440 run.py:483] Algo bellman_ford step 1172 current loss 0.987309, current_train_items 37536.
I0302 18:58:20.637803 22895071117440 run.py:483] Algo bellman_ford step 1173 current loss 1.008770, current_train_items 37568.
I0302 18:58:20.668066 22895071117440 run.py:483] Algo bellman_ford step 1174 current loss 1.146857, current_train_items 37600.
I0302 18:58:20.686179 22895071117440 run.py:483] Algo bellman_ford step 1175 current loss 0.327246, current_train_items 37632.
I0302 18:58:20.701927 22895071117440 run.py:483] Algo bellman_ford step 1176 current loss 0.671769, current_train_items 37664.
I0302 18:58:20.723044 22895071117440 run.py:483] Algo bellman_ford step 1177 current loss 0.826501, current_train_items 37696.
I0302 18:58:20.749888 22895071117440 run.py:483] Algo bellman_ford step 1178 current loss 0.865148, current_train_items 37728.
I0302 18:58:20.781134 22895071117440 run.py:483] Algo bellman_ford step 1179 current loss 1.314747, current_train_items 37760.
I0302 18:58:20.798774 22895071117440 run.py:483] Algo bellman_ford step 1180 current loss 0.314978, current_train_items 37792.
I0302 18:58:20.814444 22895071117440 run.py:483] Algo bellman_ford step 1181 current loss 0.752982, current_train_items 37824.
I0302 18:58:20.835978 22895071117440 run.py:483] Algo bellman_ford step 1182 current loss 0.762377, current_train_items 37856.
I0302 18:58:20.862320 22895071117440 run.py:483] Algo bellman_ford step 1183 current loss 0.854213, current_train_items 37888.
I0302 18:58:20.892386 22895071117440 run.py:483] Algo bellman_ford step 1184 current loss 1.093899, current_train_items 37920.
I0302 18:58:20.910604 22895071117440 run.py:483] Algo bellman_ford step 1185 current loss 0.437000, current_train_items 37952.
I0302 18:58:20.926040 22895071117440 run.py:483] Algo bellman_ford step 1186 current loss 0.592729, current_train_items 37984.
I0302 18:58:20.947656 22895071117440 run.py:483] Algo bellman_ford step 1187 current loss 0.784048, current_train_items 38016.
I0302 18:58:20.974707 22895071117440 run.py:483] Algo bellman_ford step 1188 current loss 0.867923, current_train_items 38048.
I0302 18:58:21.003400 22895071117440 run.py:483] Algo bellman_ford step 1189 current loss 1.046838, current_train_items 38080.
I0302 18:58:21.021611 22895071117440 run.py:483] Algo bellman_ford step 1190 current loss 0.419534, current_train_items 38112.
I0302 18:58:21.037437 22895071117440 run.py:483] Algo bellman_ford step 1191 current loss 0.546658, current_train_items 38144.
I0302 18:58:21.059653 22895071117440 run.py:483] Algo bellman_ford step 1192 current loss 0.925457, current_train_items 38176.
I0302 18:58:21.087176 22895071117440 run.py:483] Algo bellman_ford step 1193 current loss 1.009203, current_train_items 38208.
I0302 18:58:21.117187 22895071117440 run.py:483] Algo bellman_ford step 1194 current loss 0.993121, current_train_items 38240.
I0302 18:58:21.134758 22895071117440 run.py:483] Algo bellman_ford step 1195 current loss 0.389264, current_train_items 38272.
I0302 18:58:21.150172 22895071117440 run.py:483] Algo bellman_ford step 1196 current loss 0.693990, current_train_items 38304.
I0302 18:58:21.171155 22895071117440 run.py:483] Algo bellman_ford step 1197 current loss 0.902804, current_train_items 38336.
I0302 18:58:21.199385 22895071117440 run.py:483] Algo bellman_ford step 1198 current loss 0.962073, current_train_items 38368.
I0302 18:58:21.230217 22895071117440 run.py:483] Algo bellman_ford step 1199 current loss 1.087735, current_train_items 38400.
I0302 18:58:21.248421 22895071117440 run.py:483] Algo bellman_ford step 1200 current loss 0.439266, current_train_items 38432.
I0302 18:58:21.258012 22895071117440 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:21.258117 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:58:21.274239 22895071117440 run.py:483] Algo bellman_ford step 1201 current loss 0.656051, current_train_items 38464.
I0302 18:58:21.296286 22895071117440 run.py:483] Algo bellman_ford step 1202 current loss 0.754800, current_train_items 38496.
I0302 18:58:21.324951 22895071117440 run.py:483] Algo bellman_ford step 1203 current loss 0.945723, current_train_items 38528.
I0302 18:58:21.355678 22895071117440 run.py:483] Algo bellman_ford step 1204 current loss 1.246010, current_train_items 38560.
I0302 18:58:21.373887 22895071117440 run.py:483] Algo bellman_ford step 1205 current loss 0.327209, current_train_items 38592.
I0302 18:58:21.389341 22895071117440 run.py:483] Algo bellman_ford step 1206 current loss 0.550798, current_train_items 38624.
I0302 18:58:21.410538 22895071117440 run.py:483] Algo bellman_ford step 1207 current loss 0.928933, current_train_items 38656.
I0302 18:58:21.438383 22895071117440 run.py:483] Algo bellman_ford step 1208 current loss 0.969045, current_train_items 38688.
I0302 18:58:21.469979 22895071117440 run.py:483] Algo bellman_ford step 1209 current loss 1.156729, current_train_items 38720.
I0302 18:58:21.487507 22895071117440 run.py:483] Algo bellman_ford step 1210 current loss 0.329784, current_train_items 38752.
I0302 18:58:21.502887 22895071117440 run.py:483] Algo bellman_ford step 1211 current loss 0.696600, current_train_items 38784.
I0302 18:58:21.524272 22895071117440 run.py:483] Algo bellman_ford step 1212 current loss 0.865416, current_train_items 38816.
I0302 18:58:21.550447 22895071117440 run.py:483] Algo bellman_ford step 1213 current loss 0.845682, current_train_items 38848.
I0302 18:58:21.580163 22895071117440 run.py:483] Algo bellman_ford step 1214 current loss 1.080544, current_train_items 38880.
I0302 18:58:21.597707 22895071117440 run.py:483] Algo bellman_ford step 1215 current loss 0.341988, current_train_items 38912.
I0302 18:58:21.612925 22895071117440 run.py:483] Algo bellman_ford step 1216 current loss 0.572753, current_train_items 38944.
I0302 18:58:21.634491 22895071117440 run.py:483] Algo bellman_ford step 1217 current loss 0.814745, current_train_items 38976.
I0302 18:58:21.663030 22895071117440 run.py:483] Algo bellman_ford step 1218 current loss 0.983751, current_train_items 39008.
I0302 18:58:21.693413 22895071117440 run.py:483] Algo bellman_ford step 1219 current loss 1.293974, current_train_items 39040.
I0302 18:58:21.711583 22895071117440 run.py:483] Algo bellman_ford step 1220 current loss 0.372551, current_train_items 39072.
I0302 18:58:21.726740 22895071117440 run.py:483] Algo bellman_ford step 1221 current loss 0.642324, current_train_items 39104.
I0302 18:58:21.749343 22895071117440 run.py:483] Algo bellman_ford step 1222 current loss 0.916782, current_train_items 39136.
I0302 18:58:21.777174 22895071117440 run.py:483] Algo bellman_ford step 1223 current loss 0.946602, current_train_items 39168.
I0302 18:58:21.806962 22895071117440 run.py:483] Algo bellman_ford step 1224 current loss 1.069867, current_train_items 39200.
I0302 18:58:21.824477 22895071117440 run.py:483] Algo bellman_ford step 1225 current loss 0.360226, current_train_items 39232.
I0302 18:58:21.839968 22895071117440 run.py:483] Algo bellman_ford step 1226 current loss 0.667762, current_train_items 39264.
I0302 18:58:21.862573 22895071117440 run.py:483] Algo bellman_ford step 1227 current loss 0.859422, current_train_items 39296.
I0302 18:58:21.889320 22895071117440 run.py:483] Algo bellman_ford step 1228 current loss 0.811473, current_train_items 39328.
I0302 18:58:21.920791 22895071117440 run.py:483] Algo bellman_ford step 1229 current loss 1.277540, current_train_items 39360.
I0302 18:58:21.938459 22895071117440 run.py:483] Algo bellman_ford step 1230 current loss 0.372387, current_train_items 39392.
I0302 18:58:21.953976 22895071117440 run.py:483] Algo bellman_ford step 1231 current loss 0.596424, current_train_items 39424.
I0302 18:58:21.976260 22895071117440 run.py:483] Algo bellman_ford step 1232 current loss 1.082437, current_train_items 39456.
I0302 18:58:22.003836 22895071117440 run.py:483] Algo bellman_ford step 1233 current loss 1.215431, current_train_items 39488.
I0302 18:58:22.032140 22895071117440 run.py:483] Algo bellman_ford step 1234 current loss 1.357690, current_train_items 39520.
I0302 18:58:22.050071 22895071117440 run.py:483] Algo bellman_ford step 1235 current loss 0.322620, current_train_items 39552.
I0302 18:58:22.065054 22895071117440 run.py:483] Algo bellman_ford step 1236 current loss 0.579226, current_train_items 39584.
I0302 18:58:22.086455 22895071117440 run.py:483] Algo bellman_ford step 1237 current loss 0.789986, current_train_items 39616.
I0302 18:58:22.114019 22895071117440 run.py:483] Algo bellman_ford step 1238 current loss 0.957195, current_train_items 39648.
I0302 18:58:22.145363 22895071117440 run.py:483] Algo bellman_ford step 1239 current loss 1.047380, current_train_items 39680.
I0302 18:58:22.163380 22895071117440 run.py:483] Algo bellman_ford step 1240 current loss 0.355552, current_train_items 39712.
I0302 18:58:22.178742 22895071117440 run.py:483] Algo bellman_ford step 1241 current loss 0.653068, current_train_items 39744.
I0302 18:58:22.200720 22895071117440 run.py:483] Algo bellman_ford step 1242 current loss 0.793681, current_train_items 39776.
I0302 18:58:22.228155 22895071117440 run.py:483] Algo bellman_ford step 1243 current loss 0.967446, current_train_items 39808.
I0302 18:58:22.258437 22895071117440 run.py:483] Algo bellman_ford step 1244 current loss 1.046875, current_train_items 39840.
I0302 18:58:22.275754 22895071117440 run.py:483] Algo bellman_ford step 1245 current loss 0.314704, current_train_items 39872.
I0302 18:58:22.290992 22895071117440 run.py:483] Algo bellman_ford step 1246 current loss 0.471469, current_train_items 39904.
I0302 18:58:22.311882 22895071117440 run.py:483] Algo bellman_ford step 1247 current loss 0.814119, current_train_items 39936.
I0302 18:58:22.339723 22895071117440 run.py:483] Algo bellman_ford step 1248 current loss 1.022404, current_train_items 39968.
I0302 18:58:22.369247 22895071117440 run.py:483] Algo bellman_ford step 1249 current loss 1.273465, current_train_items 40000.
I0302 18:58:22.387051 22895071117440 run.py:483] Algo bellman_ford step 1250 current loss 0.389949, current_train_items 40032.
I0302 18:58:22.395502 22895071117440 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.861328125, 'score': 0.861328125, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:22.395609 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.861, val scores are: bellman_ford: 0.861
I0302 18:58:22.411617 22895071117440 run.py:483] Algo bellman_ford step 1251 current loss 0.595305, current_train_items 40064.
I0302 18:58:22.433397 22895071117440 run.py:483] Algo bellman_ford step 1252 current loss 0.907687, current_train_items 40096.
I0302 18:58:22.459406 22895071117440 run.py:483] Algo bellman_ford step 1253 current loss 0.923627, current_train_items 40128.
I0302 18:58:22.487213 22895071117440 run.py:483] Algo bellman_ford step 1254 current loss 0.915712, current_train_items 40160.
I0302 18:58:22.505793 22895071117440 run.py:483] Algo bellman_ford step 1255 current loss 0.329988, current_train_items 40192.
I0302 18:58:22.521522 22895071117440 run.py:483] Algo bellman_ford step 1256 current loss 0.625795, current_train_items 40224.
I0302 18:58:22.543015 22895071117440 run.py:483] Algo bellman_ford step 1257 current loss 0.827808, current_train_items 40256.
I0302 18:58:22.569556 22895071117440 run.py:483] Algo bellman_ford step 1258 current loss 0.856317, current_train_items 40288.
I0302 18:58:22.598950 22895071117440 run.py:483] Algo bellman_ford step 1259 current loss 0.997443, current_train_items 40320.
I0302 18:58:22.616984 22895071117440 run.py:483] Algo bellman_ford step 1260 current loss 0.468104, current_train_items 40352.
I0302 18:58:22.633258 22895071117440 run.py:483] Algo bellman_ford step 1261 current loss 0.715345, current_train_items 40384.
I0302 18:58:22.655311 22895071117440 run.py:483] Algo bellman_ford step 1262 current loss 0.985086, current_train_items 40416.
I0302 18:58:22.679964 22895071117440 run.py:483] Algo bellman_ford step 1263 current loss 0.738446, current_train_items 40448.
I0302 18:58:22.711861 22895071117440 run.py:483] Algo bellman_ford step 1264 current loss 1.181078, current_train_items 40480.
I0302 18:58:22.729492 22895071117440 run.py:483] Algo bellman_ford step 1265 current loss 0.317928, current_train_items 40512.
I0302 18:58:22.744820 22895071117440 run.py:483] Algo bellman_ford step 1266 current loss 0.675801, current_train_items 40544.
I0302 18:58:22.765503 22895071117440 run.py:483] Algo bellman_ford step 1267 current loss 0.891042, current_train_items 40576.
I0302 18:58:22.792434 22895071117440 run.py:483] Algo bellman_ford step 1268 current loss 0.943942, current_train_items 40608.
I0302 18:58:22.822894 22895071117440 run.py:483] Algo bellman_ford step 1269 current loss 1.077573, current_train_items 40640.
I0302 18:58:22.841273 22895071117440 run.py:483] Algo bellman_ford step 1270 current loss 0.326019, current_train_items 40672.
I0302 18:58:22.856709 22895071117440 run.py:483] Algo bellman_ford step 1271 current loss 0.598133, current_train_items 40704.
I0302 18:58:22.877452 22895071117440 run.py:483] Algo bellman_ford step 1272 current loss 0.784271, current_train_items 40736.
I0302 18:58:22.903391 22895071117440 run.py:483] Algo bellman_ford step 1273 current loss 0.780537, current_train_items 40768.
I0302 18:58:22.933789 22895071117440 run.py:483] Algo bellman_ford step 1274 current loss 1.076970, current_train_items 40800.
I0302 18:58:22.952111 22895071117440 run.py:483] Algo bellman_ford step 1275 current loss 0.354325, current_train_items 40832.
I0302 18:58:22.967795 22895071117440 run.py:483] Algo bellman_ford step 1276 current loss 0.635640, current_train_items 40864.
I0302 18:58:22.987918 22895071117440 run.py:483] Algo bellman_ford step 1277 current loss 0.781181, current_train_items 40896.
I0302 18:58:23.014600 22895071117440 run.py:483] Algo bellman_ford step 1278 current loss 0.787019, current_train_items 40928.
I0302 18:58:23.043984 22895071117440 run.py:483] Algo bellman_ford step 1279 current loss 1.073998, current_train_items 40960.
I0302 18:58:23.061669 22895071117440 run.py:483] Algo bellman_ford step 1280 current loss 0.402547, current_train_items 40992.
I0302 18:58:23.076856 22895071117440 run.py:483] Algo bellman_ford step 1281 current loss 0.557096, current_train_items 41024.
I0302 18:58:23.098848 22895071117440 run.py:483] Algo bellman_ford step 1282 current loss 0.857689, current_train_items 41056.
I0302 18:58:23.126712 22895071117440 run.py:483] Algo bellman_ford step 1283 current loss 1.005546, current_train_items 41088.
I0302 18:58:23.155781 22895071117440 run.py:483] Algo bellman_ford step 1284 current loss 1.092218, current_train_items 41120.
I0302 18:58:23.174129 22895071117440 run.py:483] Algo bellman_ford step 1285 current loss 0.372438, current_train_items 41152.
I0302 18:58:23.189753 22895071117440 run.py:483] Algo bellman_ford step 1286 current loss 0.634972, current_train_items 41184.
I0302 18:58:23.212560 22895071117440 run.py:483] Algo bellman_ford step 1287 current loss 0.921985, current_train_items 41216.
I0302 18:58:23.240090 22895071117440 run.py:483] Algo bellman_ford step 1288 current loss 0.967504, current_train_items 41248.
I0302 18:58:23.270316 22895071117440 run.py:483] Algo bellman_ford step 1289 current loss 1.048437, current_train_items 41280.
I0302 18:58:23.288301 22895071117440 run.py:483] Algo bellman_ford step 1290 current loss 0.450377, current_train_items 41312.
I0302 18:58:23.304015 22895071117440 run.py:483] Algo bellman_ford step 1291 current loss 0.720253, current_train_items 41344.
I0302 18:58:23.324859 22895071117440 run.py:483] Algo bellman_ford step 1292 current loss 0.754218, current_train_items 41376.
I0302 18:58:23.352098 22895071117440 run.py:483] Algo bellman_ford step 1293 current loss 0.963680, current_train_items 41408.
I0302 18:58:23.383270 22895071117440 run.py:483] Algo bellman_ford step 1294 current loss 1.072249, current_train_items 41440.
I0302 18:58:23.400944 22895071117440 run.py:483] Algo bellman_ford step 1295 current loss 0.368063, current_train_items 41472.
I0302 18:58:23.416286 22895071117440 run.py:483] Algo bellman_ford step 1296 current loss 0.548909, current_train_items 41504.
I0302 18:58:23.438424 22895071117440 run.py:483] Algo bellman_ford step 1297 current loss 0.853645, current_train_items 41536.
I0302 18:58:23.464030 22895071117440 run.py:483] Algo bellman_ford step 1298 current loss 0.727166, current_train_items 41568.
I0302 18:58:23.491676 22895071117440 run.py:483] Algo bellman_ford step 1299 current loss 0.990788, current_train_items 41600.
I0302 18:58:23.509489 22895071117440 run.py:483] Algo bellman_ford step 1300 current loss 0.386977, current_train_items 41632.
I0302 18:58:23.517135 22895071117440 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:23.517241 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:58:23.533213 22895071117440 run.py:483] Algo bellman_ford step 1301 current loss 0.552748, current_train_items 41664.
I0302 18:58:23.555976 22895071117440 run.py:483] Algo bellman_ford step 1302 current loss 0.896393, current_train_items 41696.
I0302 18:58:23.584121 22895071117440 run.py:483] Algo bellman_ford step 1303 current loss 1.114970, current_train_items 41728.
I0302 18:58:23.612952 22895071117440 run.py:483] Algo bellman_ford step 1304 current loss 1.234753, current_train_items 41760.
I0302 18:58:23.630760 22895071117440 run.py:483] Algo bellman_ford step 1305 current loss 0.322010, current_train_items 41792.
I0302 18:58:23.646128 22895071117440 run.py:483] Algo bellman_ford step 1306 current loss 0.649009, current_train_items 41824.
I0302 18:58:23.667608 22895071117440 run.py:483] Algo bellman_ford step 1307 current loss 0.765314, current_train_items 41856.
I0302 18:58:23.694238 22895071117440 run.py:483] Algo bellman_ford step 1308 current loss 0.828763, current_train_items 41888.
I0302 18:58:23.724128 22895071117440 run.py:483] Algo bellman_ford step 1309 current loss 0.895511, current_train_items 41920.
I0302 18:58:23.741715 22895071117440 run.py:483] Algo bellman_ford step 1310 current loss 0.283743, current_train_items 41952.
I0302 18:58:23.757541 22895071117440 run.py:483] Algo bellman_ford step 1311 current loss 0.561017, current_train_items 41984.
I0302 18:58:23.778297 22895071117440 run.py:483] Algo bellman_ford step 1312 current loss 0.798788, current_train_items 42016.
I0302 18:58:23.806206 22895071117440 run.py:483] Algo bellman_ford step 1313 current loss 0.973921, current_train_items 42048.
I0302 18:58:23.836378 22895071117440 run.py:483] Algo bellman_ford step 1314 current loss 1.093709, current_train_items 42080.
I0302 18:58:23.853961 22895071117440 run.py:483] Algo bellman_ford step 1315 current loss 0.266799, current_train_items 42112.
I0302 18:58:23.869448 22895071117440 run.py:483] Algo bellman_ford step 1316 current loss 0.578102, current_train_items 42144.
I0302 18:58:23.891129 22895071117440 run.py:483] Algo bellman_ford step 1317 current loss 0.824074, current_train_items 42176.
I0302 18:58:23.918894 22895071117440 run.py:483] Algo bellman_ford step 1318 current loss 0.835427, current_train_items 42208.
I0302 18:58:23.948366 22895071117440 run.py:483] Algo bellman_ford step 1319 current loss 1.014413, current_train_items 42240.
I0302 18:58:23.965723 22895071117440 run.py:483] Algo bellman_ford step 1320 current loss 0.338070, current_train_items 42272.
I0302 18:58:23.981071 22895071117440 run.py:483] Algo bellman_ford step 1321 current loss 0.598435, current_train_items 42304.
I0302 18:58:24.002943 22895071117440 run.py:483] Algo bellman_ford step 1322 current loss 1.215123, current_train_items 42336.
I0302 18:58:24.031004 22895071117440 run.py:483] Algo bellman_ford step 1323 current loss 0.975761, current_train_items 42368.
I0302 18:58:24.059847 22895071117440 run.py:483] Algo bellman_ford step 1324 current loss 1.190986, current_train_items 42400.
I0302 18:58:24.077549 22895071117440 run.py:483] Algo bellman_ford step 1325 current loss 0.415490, current_train_items 42432.
I0302 18:58:24.092708 22895071117440 run.py:483] Algo bellman_ford step 1326 current loss 0.577163, current_train_items 42464.
I0302 18:58:24.113368 22895071117440 run.py:483] Algo bellman_ford step 1327 current loss 0.787556, current_train_items 42496.
I0302 18:58:24.140526 22895071117440 run.py:483] Algo bellman_ford step 1328 current loss 0.961568, current_train_items 42528.
I0302 18:58:24.170890 22895071117440 run.py:483] Algo bellman_ford step 1329 current loss 1.140027, current_train_items 42560.
I0302 18:58:24.188245 22895071117440 run.py:483] Algo bellman_ford step 1330 current loss 0.445210, current_train_items 42592.
I0302 18:58:24.203377 22895071117440 run.py:483] Algo bellman_ford step 1331 current loss 0.659906, current_train_items 42624.
I0302 18:58:24.224030 22895071117440 run.py:483] Algo bellman_ford step 1332 current loss 0.782157, current_train_items 42656.
I0302 18:58:24.251066 22895071117440 run.py:483] Algo bellman_ford step 1333 current loss 0.903854, current_train_items 42688.
I0302 18:58:24.279629 22895071117440 run.py:483] Algo bellman_ford step 1334 current loss 0.954332, current_train_items 42720.
I0302 18:58:24.297511 22895071117440 run.py:483] Algo bellman_ford step 1335 current loss 0.371126, current_train_items 42752.
I0302 18:58:24.312494 22895071117440 run.py:483] Algo bellman_ford step 1336 current loss 0.610502, current_train_items 42784.
I0302 18:58:24.334797 22895071117440 run.py:483] Algo bellman_ford step 1337 current loss 0.847619, current_train_items 42816.
I0302 18:58:24.361761 22895071117440 run.py:483] Algo bellman_ford step 1338 current loss 1.029079, current_train_items 42848.
I0302 18:58:24.393652 22895071117440 run.py:483] Algo bellman_ford step 1339 current loss 1.033699, current_train_items 42880.
I0302 18:58:24.410937 22895071117440 run.py:483] Algo bellman_ford step 1340 current loss 0.361221, current_train_items 42912.
I0302 18:58:24.426264 22895071117440 run.py:483] Algo bellman_ford step 1341 current loss 0.651913, current_train_items 42944.
I0302 18:58:24.447743 22895071117440 run.py:483] Algo bellman_ford step 1342 current loss 0.798201, current_train_items 42976.
I0302 18:58:24.474573 22895071117440 run.py:483] Algo bellman_ford step 1343 current loss 0.834995, current_train_items 43008.
I0302 18:58:24.503764 22895071117440 run.py:483] Algo bellman_ford step 1344 current loss 1.039171, current_train_items 43040.
I0302 18:58:24.521770 22895071117440 run.py:483] Algo bellman_ford step 1345 current loss 0.383930, current_train_items 43072.
I0302 18:58:24.537073 22895071117440 run.py:483] Algo bellman_ford step 1346 current loss 0.603601, current_train_items 43104.
I0302 18:58:24.559516 22895071117440 run.py:483] Algo bellman_ford step 1347 current loss 0.868064, current_train_items 43136.
I0302 18:58:24.585652 22895071117440 run.py:483] Algo bellman_ford step 1348 current loss 0.854332, current_train_items 43168.
I0302 18:58:24.614106 22895071117440 run.py:483] Algo bellman_ford step 1349 current loss 1.073448, current_train_items 43200.
I0302 18:58:24.631785 22895071117440 run.py:483] Algo bellman_ford step 1350 current loss 0.355339, current_train_items 43232.
I0302 18:58:24.639837 22895071117440 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:24.639953 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 18:58:24.656205 22895071117440 run.py:483] Algo bellman_ford step 1351 current loss 0.585406, current_train_items 43264.
I0302 18:58:24.679320 22895071117440 run.py:483] Algo bellman_ford step 1352 current loss 0.695161, current_train_items 43296.
I0302 18:58:24.705564 22895071117440 run.py:483] Algo bellman_ford step 1353 current loss 0.827000, current_train_items 43328.
I0302 18:58:24.737292 22895071117440 run.py:483] Algo bellman_ford step 1354 current loss 1.086981, current_train_items 43360.
I0302 18:58:24.755387 22895071117440 run.py:483] Algo bellman_ford step 1355 current loss 0.362914, current_train_items 43392.
I0302 18:58:24.770689 22895071117440 run.py:483] Algo bellman_ford step 1356 current loss 0.579557, current_train_items 43424.
I0302 18:58:24.791411 22895071117440 run.py:483] Algo bellman_ford step 1357 current loss 0.743501, current_train_items 43456.
I0302 18:58:24.818268 22895071117440 run.py:483] Algo bellman_ford step 1358 current loss 0.844656, current_train_items 43488.
I0302 18:58:24.849164 22895071117440 run.py:483] Algo bellman_ford step 1359 current loss 1.204287, current_train_items 43520.
I0302 18:58:24.867271 22895071117440 run.py:483] Algo bellman_ford step 1360 current loss 0.378953, current_train_items 43552.
I0302 18:58:24.883524 22895071117440 run.py:483] Algo bellman_ford step 1361 current loss 0.710391, current_train_items 43584.
I0302 18:58:24.905392 22895071117440 run.py:483] Algo bellman_ford step 1362 current loss 0.865505, current_train_items 43616.
I0302 18:58:24.932308 22895071117440 run.py:483] Algo bellman_ford step 1363 current loss 0.989668, current_train_items 43648.
I0302 18:58:24.963520 22895071117440 run.py:483] Algo bellman_ford step 1364 current loss 1.086085, current_train_items 43680.
I0302 18:58:24.981096 22895071117440 run.py:483] Algo bellman_ford step 1365 current loss 0.349019, current_train_items 43712.
I0302 18:58:24.996288 22895071117440 run.py:483] Algo bellman_ford step 1366 current loss 0.536533, current_train_items 43744.
I0302 18:58:25.016915 22895071117440 run.py:483] Algo bellman_ford step 1367 current loss 0.666253, current_train_items 43776.
I0302 18:58:25.042306 22895071117440 run.py:483] Algo bellman_ford step 1368 current loss 0.794839, current_train_items 43808.
I0302 18:58:25.072102 22895071117440 run.py:483] Algo bellman_ford step 1369 current loss 1.166028, current_train_items 43840.
I0302 18:58:25.090170 22895071117440 run.py:483] Algo bellman_ford step 1370 current loss 0.338639, current_train_items 43872.
I0302 18:58:25.106193 22895071117440 run.py:483] Algo bellman_ford step 1371 current loss 0.639622, current_train_items 43904.
I0302 18:58:25.127810 22895071117440 run.py:483] Algo bellman_ford step 1372 current loss 0.771743, current_train_items 43936.
I0302 18:58:25.155914 22895071117440 run.py:483] Algo bellman_ford step 1373 current loss 0.942990, current_train_items 43968.
I0302 18:58:25.186632 22895071117440 run.py:483] Algo bellman_ford step 1374 current loss 1.016567, current_train_items 44000.
I0302 18:58:25.204550 22895071117440 run.py:483] Algo bellman_ford step 1375 current loss 0.328297, current_train_items 44032.
I0302 18:58:25.219821 22895071117440 run.py:483] Algo bellman_ford step 1376 current loss 0.574550, current_train_items 44064.
I0302 18:58:25.240847 22895071117440 run.py:483] Algo bellman_ford step 1377 current loss 0.803920, current_train_items 44096.
I0302 18:58:25.268568 22895071117440 run.py:483] Algo bellman_ford step 1378 current loss 0.819034, current_train_items 44128.
I0302 18:58:25.298578 22895071117440 run.py:483] Algo bellman_ford step 1379 current loss 0.975349, current_train_items 44160.
I0302 18:58:25.316408 22895071117440 run.py:483] Algo bellman_ford step 1380 current loss 0.393296, current_train_items 44192.
I0302 18:58:25.331982 22895071117440 run.py:483] Algo bellman_ford step 1381 current loss 0.647175, current_train_items 44224.
I0302 18:58:25.353171 22895071117440 run.py:483] Algo bellman_ford step 1382 current loss 0.735629, current_train_items 44256.
I0302 18:58:25.380367 22895071117440 run.py:483] Algo bellman_ford step 1383 current loss 0.900215, current_train_items 44288.
I0302 18:58:25.407718 22895071117440 run.py:483] Algo bellman_ford step 1384 current loss 0.990989, current_train_items 44320.
I0302 18:58:25.425590 22895071117440 run.py:483] Algo bellman_ford step 1385 current loss 0.330396, current_train_items 44352.
I0302 18:58:25.441039 22895071117440 run.py:483] Algo bellman_ford step 1386 current loss 0.637420, current_train_items 44384.
I0302 18:58:25.461156 22895071117440 run.py:483] Algo bellman_ford step 1387 current loss 0.688431, current_train_items 44416.
I0302 18:58:25.488590 22895071117440 run.py:483] Algo bellman_ford step 1388 current loss 0.825595, current_train_items 44448.
I0302 18:58:25.518336 22895071117440 run.py:483] Algo bellman_ford step 1389 current loss 0.947217, current_train_items 44480.
I0302 18:58:25.536123 22895071117440 run.py:483] Algo bellman_ford step 1390 current loss 0.349176, current_train_items 44512.
I0302 18:58:25.551391 22895071117440 run.py:483] Algo bellman_ford step 1391 current loss 0.574883, current_train_items 44544.
I0302 18:58:25.573433 22895071117440 run.py:483] Algo bellman_ford step 1392 current loss 0.784911, current_train_items 44576.
I0302 18:58:25.600229 22895071117440 run.py:483] Algo bellman_ford step 1393 current loss 0.879176, current_train_items 44608.
I0302 18:58:25.629780 22895071117440 run.py:483] Algo bellman_ford step 1394 current loss 1.047317, current_train_items 44640.
I0302 18:58:25.646960 22895071117440 run.py:483] Algo bellman_ford step 1395 current loss 0.288809, current_train_items 44672.
I0302 18:58:25.662008 22895071117440 run.py:483] Algo bellman_ford step 1396 current loss 0.547309, current_train_items 44704.
I0302 18:58:25.683426 22895071117440 run.py:483] Algo bellman_ford step 1397 current loss 0.880930, current_train_items 44736.
I0302 18:58:25.711817 22895071117440 run.py:483] Algo bellman_ford step 1398 current loss 1.012423, current_train_items 44768.
I0302 18:58:25.740486 22895071117440 run.py:483] Algo bellman_ford step 1399 current loss 0.970571, current_train_items 44800.
I0302 18:58:25.758564 22895071117440 run.py:483] Algo bellman_ford step 1400 current loss 0.430879, current_train_items 44832.
I0302 18:58:25.766586 22895071117440 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:58:25.766690 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 18:58:25.782330 22895071117440 run.py:483] Algo bellman_ford step 1401 current loss 0.630940, current_train_items 44864.
I0302 18:58:25.805020 22895071117440 run.py:483] Algo bellman_ford step 1402 current loss 0.933644, current_train_items 44896.
I0302 18:58:25.833562 22895071117440 run.py:483] Algo bellman_ford step 1403 current loss 1.097825, current_train_items 44928.
I0302 18:58:25.864445 22895071117440 run.py:483] Algo bellman_ford step 1404 current loss 1.144166, current_train_items 44960.
I0302 18:58:25.882582 22895071117440 run.py:483] Algo bellman_ford step 1405 current loss 0.454726, current_train_items 44992.
I0302 18:58:25.898370 22895071117440 run.py:483] Algo bellman_ford step 1406 current loss 0.662319, current_train_items 45024.
I0302 18:58:25.920270 22895071117440 run.py:483] Algo bellman_ford step 1407 current loss 0.762489, current_train_items 45056.
I0302 18:58:25.947906 22895071117440 run.py:483] Algo bellman_ford step 1408 current loss 0.924545, current_train_items 45088.
I0302 18:58:25.978184 22895071117440 run.py:483] Algo bellman_ford step 1409 current loss 1.096786, current_train_items 45120.
I0302 18:58:25.995851 22895071117440 run.py:483] Algo bellman_ford step 1410 current loss 0.364763, current_train_items 45152.
I0302 18:58:26.011504 22895071117440 run.py:483] Algo bellman_ford step 1411 current loss 0.564364, current_train_items 45184.
I0302 18:58:26.032903 22895071117440 run.py:483] Algo bellman_ford step 1412 current loss 0.786224, current_train_items 45216.
I0302 18:58:26.058669 22895071117440 run.py:483] Algo bellman_ford step 1413 current loss 0.714040, current_train_items 45248.
I0302 18:58:26.087821 22895071117440 run.py:483] Algo bellman_ford step 1414 current loss 0.889935, current_train_items 45280.
I0302 18:58:26.105539 22895071117440 run.py:483] Algo bellman_ford step 1415 current loss 0.330303, current_train_items 45312.
I0302 18:58:26.121391 22895071117440 run.py:483] Algo bellman_ford step 1416 current loss 0.695604, current_train_items 45344.
I0302 18:58:26.142412 22895071117440 run.py:483] Algo bellman_ford step 1417 current loss 0.737156, current_train_items 45376.
I0302 18:58:26.168552 22895071117440 run.py:483] Algo bellman_ford step 1418 current loss 0.940511, current_train_items 45408.
I0302 18:58:26.198987 22895071117440 run.py:483] Algo bellman_ford step 1419 current loss 0.973053, current_train_items 45440.
I0302 18:58:26.217298 22895071117440 run.py:483] Algo bellman_ford step 1420 current loss 0.437545, current_train_items 45472.
I0302 18:58:26.232661 22895071117440 run.py:483] Algo bellman_ford step 1421 current loss 0.593792, current_train_items 45504.
I0302 18:58:26.254546 22895071117440 run.py:483] Algo bellman_ford step 1422 current loss 0.781595, current_train_items 45536.
I0302 18:58:26.282915 22895071117440 run.py:483] Algo bellman_ford step 1423 current loss 0.947100, current_train_items 45568.
I0302 18:58:26.311971 22895071117440 run.py:483] Algo bellman_ford step 1424 current loss 1.026895, current_train_items 45600.
I0302 18:58:26.329714 22895071117440 run.py:483] Algo bellman_ford step 1425 current loss 0.318257, current_train_items 45632.
I0302 18:58:26.345620 22895071117440 run.py:483] Algo bellman_ford step 1426 current loss 0.582536, current_train_items 45664.
I0302 18:58:26.368061 22895071117440 run.py:483] Algo bellman_ford step 1427 current loss 0.845478, current_train_items 45696.
I0302 18:58:26.395538 22895071117440 run.py:483] Algo bellman_ford step 1428 current loss 0.879981, current_train_items 45728.
I0302 18:58:26.423885 22895071117440 run.py:483] Algo bellman_ford step 1429 current loss 0.889070, current_train_items 45760.
I0302 18:58:26.441576 22895071117440 run.py:483] Algo bellman_ford step 1430 current loss 0.353777, current_train_items 45792.
I0302 18:58:26.456704 22895071117440 run.py:483] Algo bellman_ford step 1431 current loss 0.582175, current_train_items 45824.
I0302 18:58:26.479593 22895071117440 run.py:483] Algo bellman_ford step 1432 current loss 0.898582, current_train_items 45856.
I0302 18:58:26.505277 22895071117440 run.py:483] Algo bellman_ford step 1433 current loss 0.787095, current_train_items 45888.
I0302 18:58:26.533855 22895071117440 run.py:483] Algo bellman_ford step 1434 current loss 0.949889, current_train_items 45920.
I0302 18:58:26.551484 22895071117440 run.py:483] Algo bellman_ford step 1435 current loss 0.368083, current_train_items 45952.
I0302 18:58:26.566913 22895071117440 run.py:483] Algo bellman_ford step 1436 current loss 0.710265, current_train_items 45984.
I0302 18:58:26.588163 22895071117440 run.py:483] Algo bellman_ford step 1437 current loss 0.789742, current_train_items 46016.
I0302 18:58:26.615055 22895071117440 run.py:483] Algo bellman_ford step 1438 current loss 0.822749, current_train_items 46048.
I0302 18:58:26.645991 22895071117440 run.py:483] Algo bellman_ford step 1439 current loss 1.149467, current_train_items 46080.
I0302 18:58:26.664063 22895071117440 run.py:483] Algo bellman_ford step 1440 current loss 0.383390, current_train_items 46112.
I0302 18:58:26.679712 22895071117440 run.py:483] Algo bellman_ford step 1441 current loss 0.714323, current_train_items 46144.
I0302 18:58:26.701174 22895071117440 run.py:483] Algo bellman_ford step 1442 current loss 0.723764, current_train_items 46176.
I0302 18:58:26.728570 22895071117440 run.py:483] Algo bellman_ford step 1443 current loss 0.864666, current_train_items 46208.
I0302 18:58:26.762058 22895071117440 run.py:483] Algo bellman_ford step 1444 current loss 1.228304, current_train_items 46240.
I0302 18:58:26.779947 22895071117440 run.py:483] Algo bellman_ford step 1445 current loss 0.374488, current_train_items 46272.
I0302 18:58:26.794770 22895071117440 run.py:483] Algo bellman_ford step 1446 current loss 0.623190, current_train_items 46304.
I0302 18:58:26.816569 22895071117440 run.py:483] Algo bellman_ford step 1447 current loss 0.930856, current_train_items 46336.
I0302 18:58:26.843672 22895071117440 run.py:483] Algo bellman_ford step 1448 current loss 1.072351, current_train_items 46368.
I0302 18:58:26.872477 22895071117440 run.py:483] Algo bellman_ford step 1449 current loss 1.273159, current_train_items 46400.
I0302 18:58:26.890455 22895071117440 run.py:483] Algo bellman_ford step 1450 current loss 0.346122, current_train_items 46432.
I0302 18:58:26.898265 22895071117440 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.861328125, 'score': 0.861328125, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:58:26.898369 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.915, current avg val score is 0.861, val scores are: bellman_ford: 0.861
I0302 18:58:26.913879 22895071117440 run.py:483] Algo bellman_ford step 1451 current loss 0.582431, current_train_items 46464.
I0302 18:58:26.935307 22895071117440 run.py:483] Algo bellman_ford step 1452 current loss 0.744886, current_train_items 46496.
I0302 18:58:26.963780 22895071117440 run.py:483] Algo bellman_ford step 1453 current loss 1.015612, current_train_items 46528.
I0302 18:58:26.995357 22895071117440 run.py:483] Algo bellman_ford step 1454 current loss 1.125900, current_train_items 46560.
I0302 18:58:27.013385 22895071117440 run.py:483] Algo bellman_ford step 1455 current loss 0.363808, current_train_items 46592.
I0302 18:58:27.028724 22895071117440 run.py:483] Algo bellman_ford step 1456 current loss 0.611434, current_train_items 46624.
I0302 18:58:27.049391 22895071117440 run.py:483] Algo bellman_ford step 1457 current loss 0.615651, current_train_items 46656.
I0302 18:58:27.076971 22895071117440 run.py:483] Algo bellman_ford step 1458 current loss 0.907473, current_train_items 46688.
I0302 18:58:27.106506 22895071117440 run.py:483] Algo bellman_ford step 1459 current loss 1.071213, current_train_items 46720.
I0302 18:58:27.124294 22895071117440 run.py:483] Algo bellman_ford step 1460 current loss 0.398772, current_train_items 46752.
I0302 18:58:27.139462 22895071117440 run.py:483] Algo bellman_ford step 1461 current loss 0.637463, current_train_items 46784.
I0302 18:58:27.160066 22895071117440 run.py:483] Algo bellman_ford step 1462 current loss 0.693265, current_train_items 46816.
I0302 18:58:27.186831 22895071117440 run.py:483] Algo bellman_ford step 1463 current loss 0.851874, current_train_items 46848.
I0302 18:58:27.215377 22895071117440 run.py:483] Algo bellman_ford step 1464 current loss 1.044670, current_train_items 46880.
I0302 18:58:27.233139 22895071117440 run.py:483] Algo bellman_ford step 1465 current loss 0.421115, current_train_items 46912.
I0302 18:58:27.248702 22895071117440 run.py:483] Algo bellman_ford step 1466 current loss 0.649801, current_train_items 46944.
I0302 18:58:27.269774 22895071117440 run.py:483] Algo bellman_ford step 1467 current loss 0.718489, current_train_items 46976.
I0302 18:58:27.295166 22895071117440 run.py:483] Algo bellman_ford step 1468 current loss 0.781936, current_train_items 47008.
I0302 18:58:27.323290 22895071117440 run.py:483] Algo bellman_ford step 1469 current loss 0.953521, current_train_items 47040.
I0302 18:58:27.341329 22895071117440 run.py:483] Algo bellman_ford step 1470 current loss 0.350305, current_train_items 47072.
I0302 18:58:27.357385 22895071117440 run.py:483] Algo bellman_ford step 1471 current loss 0.602031, current_train_items 47104.
I0302 18:58:27.378375 22895071117440 run.py:483] Algo bellman_ford step 1472 current loss 0.686156, current_train_items 47136.
I0302 18:58:27.405591 22895071117440 run.py:483] Algo bellman_ford step 1473 current loss 0.887541, current_train_items 47168.
I0302 18:58:27.435795 22895071117440 run.py:483] Algo bellman_ford step 1474 current loss 1.030535, current_train_items 47200.
I0302 18:58:27.453923 22895071117440 run.py:483] Algo bellman_ford step 1475 current loss 0.464355, current_train_items 47232.
I0302 18:58:27.469532 22895071117440 run.py:483] Algo bellman_ford step 1476 current loss 0.569292, current_train_items 47264.
I0302 18:58:27.490927 22895071117440 run.py:483] Algo bellman_ford step 1477 current loss 0.720878, current_train_items 47296.
I0302 18:58:27.517693 22895071117440 run.py:483] Algo bellman_ford step 1478 current loss 0.850273, current_train_items 47328.
I0302 18:58:27.548925 22895071117440 run.py:483] Algo bellman_ford step 1479 current loss 1.048023, current_train_items 47360.
I0302 18:58:27.566228 22895071117440 run.py:483] Algo bellman_ford step 1480 current loss 0.347710, current_train_items 47392.
I0302 18:58:27.581218 22895071117440 run.py:483] Algo bellman_ford step 1481 current loss 0.569419, current_train_items 47424.
I0302 18:58:27.601780 22895071117440 run.py:483] Algo bellman_ford step 1482 current loss 0.710479, current_train_items 47456.
I0302 18:58:27.629464 22895071117440 run.py:483] Algo bellman_ford step 1483 current loss 0.903769, current_train_items 47488.
I0302 18:58:27.657667 22895071117440 run.py:483] Algo bellman_ford step 1484 current loss 0.996301, current_train_items 47520.
I0302 18:58:27.675531 22895071117440 run.py:483] Algo bellman_ford step 1485 current loss 0.366195, current_train_items 47552.
I0302 18:58:27.690711 22895071117440 run.py:483] Algo bellman_ford step 1486 current loss 0.615819, current_train_items 47584.
I0302 18:58:27.712462 22895071117440 run.py:483] Algo bellman_ford step 1487 current loss 0.903224, current_train_items 47616.
I0302 18:58:27.738172 22895071117440 run.py:483] Algo bellman_ford step 1488 current loss 0.890471, current_train_items 47648.
I0302 18:58:27.768469 22895071117440 run.py:483] Algo bellman_ford step 1489 current loss 1.254410, current_train_items 47680.
I0302 18:58:27.786473 22895071117440 run.py:483] Algo bellman_ford step 1490 current loss 0.421364, current_train_items 47712.
I0302 18:58:27.802054 22895071117440 run.py:483] Algo bellman_ford step 1491 current loss 0.574358, current_train_items 47744.
I0302 18:58:27.823489 22895071117440 run.py:483] Algo bellman_ford step 1492 current loss 0.744346, current_train_items 47776.
I0302 18:58:27.850461 22895071117440 run.py:483] Algo bellman_ford step 1493 current loss 0.829723, current_train_items 47808.
I0302 18:58:27.881878 22895071117440 run.py:483] Algo bellman_ford step 1494 current loss 1.011208, current_train_items 47840.
I0302 18:58:27.899286 22895071117440 run.py:483] Algo bellman_ford step 1495 current loss 0.378271, current_train_items 47872.
I0302 18:58:27.914849 22895071117440 run.py:483] Algo bellman_ford step 1496 current loss 0.639207, current_train_items 47904.
I0302 18:58:27.936401 22895071117440 run.py:483] Algo bellman_ford step 1497 current loss 0.832912, current_train_items 47936.
I0302 18:58:27.964527 22895071117440 run.py:483] Algo bellman_ford step 1498 current loss 1.005238, current_train_items 47968.
I0302 18:58:27.995740 22895071117440 run.py:483] Algo bellman_ford step 1499 current loss 1.059998, current_train_items 48000.
I0302 18:58:28.013972 22895071117440 run.py:483] Algo bellman_ford step 1500 current loss 0.388736, current_train_items 48032.
I0302 18:58:28.021670 22895071117440 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:58:28.021802 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.915, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 18:58:28.050470 22895071117440 run.py:483] Algo bellman_ford step 1501 current loss 0.576221, current_train_items 48064.
I0302 18:58:28.072710 22895071117440 run.py:483] Algo bellman_ford step 1502 current loss 0.736587, current_train_items 48096.
I0302 18:58:28.099866 22895071117440 run.py:483] Algo bellman_ford step 1503 current loss 0.825978, current_train_items 48128.
I0302 18:58:28.129876 22895071117440 run.py:483] Algo bellman_ford step 1504 current loss 0.942310, current_train_items 48160.
I0302 18:58:28.147829 22895071117440 run.py:483] Algo bellman_ford step 1505 current loss 0.336453, current_train_items 48192.
I0302 18:58:28.163230 22895071117440 run.py:483] Algo bellman_ford step 1506 current loss 0.562778, current_train_items 48224.
I0302 18:58:28.184044 22895071117440 run.py:483] Algo bellman_ford step 1507 current loss 0.820614, current_train_items 48256.
I0302 18:58:28.210949 22895071117440 run.py:483] Algo bellman_ford step 1508 current loss 0.854496, current_train_items 48288.
I0302 18:58:28.240596 22895071117440 run.py:483] Algo bellman_ford step 1509 current loss 0.905971, current_train_items 48320.
I0302 18:58:28.258101 22895071117440 run.py:483] Algo bellman_ford step 1510 current loss 0.356587, current_train_items 48352.
I0302 18:58:28.273513 22895071117440 run.py:483] Algo bellman_ford step 1511 current loss 0.680040, current_train_items 48384.
I0302 18:58:28.295169 22895071117440 run.py:483] Algo bellman_ford step 1512 current loss 0.769368, current_train_items 48416.
I0302 18:58:28.321089 22895071117440 run.py:483] Algo bellman_ford step 1513 current loss 0.812980, current_train_items 48448.
I0302 18:58:28.352127 22895071117440 run.py:483] Algo bellman_ford step 1514 current loss 1.203564, current_train_items 48480.
I0302 18:58:28.369760 22895071117440 run.py:483] Algo bellman_ford step 1515 current loss 0.307400, current_train_items 48512.
I0302 18:58:28.384720 22895071117440 run.py:483] Algo bellman_ford step 1516 current loss 0.610006, current_train_items 48544.
I0302 18:58:28.405914 22895071117440 run.py:483] Algo bellman_ford step 1517 current loss 0.795708, current_train_items 48576.
I0302 18:58:28.433668 22895071117440 run.py:483] Algo bellman_ford step 1518 current loss 0.901577, current_train_items 48608.
I0302 18:58:28.463002 22895071117440 run.py:483] Algo bellman_ford step 1519 current loss 1.056556, current_train_items 48640.
I0302 18:58:28.480384 22895071117440 run.py:483] Algo bellman_ford step 1520 current loss 0.324073, current_train_items 48672.
I0302 18:58:28.495425 22895071117440 run.py:483] Algo bellman_ford step 1521 current loss 0.649917, current_train_items 48704.
I0302 18:58:28.517411 22895071117440 run.py:483] Algo bellman_ford step 1522 current loss 0.797092, current_train_items 48736.
I0302 18:58:28.544561 22895071117440 run.py:483] Algo bellman_ford step 1523 current loss 0.878902, current_train_items 48768.
I0302 18:58:28.574251 22895071117440 run.py:483] Algo bellman_ford step 1524 current loss 1.105518, current_train_items 48800.
I0302 18:58:28.591824 22895071117440 run.py:483] Algo bellman_ford step 1525 current loss 0.387018, current_train_items 48832.
I0302 18:58:28.606804 22895071117440 run.py:483] Algo bellman_ford step 1526 current loss 0.534210, current_train_items 48864.
I0302 18:58:28.629046 22895071117440 run.py:483] Algo bellman_ford step 1527 current loss 0.779571, current_train_items 48896.
I0302 18:58:28.656196 22895071117440 run.py:483] Algo bellman_ford step 1528 current loss 0.822505, current_train_items 48928.
I0302 18:58:28.686362 22895071117440 run.py:483] Algo bellman_ford step 1529 current loss 1.026687, current_train_items 48960.
I0302 18:58:28.703889 22895071117440 run.py:483] Algo bellman_ford step 1530 current loss 0.420640, current_train_items 48992.
I0302 18:58:28.718972 22895071117440 run.py:483] Algo bellman_ford step 1531 current loss 0.614331, current_train_items 49024.
I0302 18:58:28.740031 22895071117440 run.py:483] Algo bellman_ford step 1532 current loss 0.785016, current_train_items 49056.
I0302 18:58:28.767307 22895071117440 run.py:483] Algo bellman_ford step 1533 current loss 0.851883, current_train_items 49088.
I0302 18:58:28.798053 22895071117440 run.py:483] Algo bellman_ford step 1534 current loss 1.154181, current_train_items 49120.
I0302 18:58:28.815836 22895071117440 run.py:483] Algo bellman_ford step 1535 current loss 0.418615, current_train_items 49152.
I0302 18:58:28.831482 22895071117440 run.py:483] Algo bellman_ford step 1536 current loss 0.586193, current_train_items 49184.
I0302 18:58:28.852944 22895071117440 run.py:483] Algo bellman_ford step 1537 current loss 0.789555, current_train_items 49216.
I0302 18:58:28.880298 22895071117440 run.py:483] Algo bellman_ford step 1538 current loss 0.889163, current_train_items 49248.
I0302 18:58:28.912849 22895071117440 run.py:483] Algo bellman_ford step 1539 current loss 1.207101, current_train_items 49280.
I0302 18:58:28.930357 22895071117440 run.py:483] Algo bellman_ford step 1540 current loss 0.430982, current_train_items 49312.
I0302 18:58:28.945340 22895071117440 run.py:483] Algo bellman_ford step 1541 current loss 0.593380, current_train_items 49344.
I0302 18:58:28.966694 22895071117440 run.py:483] Algo bellman_ford step 1542 current loss 0.742621, current_train_items 49376.
I0302 18:58:28.993430 22895071117440 run.py:483] Algo bellman_ford step 1543 current loss 0.826698, current_train_items 49408.
I0302 18:58:29.023139 22895071117440 run.py:483] Algo bellman_ford step 1544 current loss 0.974108, current_train_items 49440.
I0302 18:58:29.040684 22895071117440 run.py:483] Algo bellman_ford step 1545 current loss 0.332121, current_train_items 49472.
I0302 18:58:29.055998 22895071117440 run.py:483] Algo bellman_ford step 1546 current loss 0.611063, current_train_items 49504.
I0302 18:58:29.077286 22895071117440 run.py:483] Algo bellman_ford step 1547 current loss 0.748740, current_train_items 49536.
I0302 18:58:29.105385 22895071117440 run.py:483] Algo bellman_ford step 1548 current loss 1.021344, current_train_items 49568.
I0302 18:58:29.136851 22895071117440 run.py:483] Algo bellman_ford step 1549 current loss 1.191948, current_train_items 49600.
I0302 18:58:29.154443 22895071117440 run.py:483] Algo bellman_ford step 1550 current loss 0.373273, current_train_items 49632.
I0302 18:58:29.162529 22895071117440 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:58:29.162634 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:58:29.179321 22895071117440 run.py:483] Algo bellman_ford step 1551 current loss 0.629052, current_train_items 49664.
I0302 18:58:29.200949 22895071117440 run.py:483] Algo bellman_ford step 1552 current loss 0.725479, current_train_items 49696.
I0302 18:58:29.228536 22895071117440 run.py:483] Algo bellman_ford step 1553 current loss 0.813444, current_train_items 49728.
I0302 18:58:29.257661 22895071117440 run.py:483] Algo bellman_ford step 1554 current loss 0.854356, current_train_items 49760.
I0302 18:58:29.275499 22895071117440 run.py:483] Algo bellman_ford step 1555 current loss 0.293326, current_train_items 49792.
I0302 18:58:29.290818 22895071117440 run.py:483] Algo bellman_ford step 1556 current loss 0.634744, current_train_items 49824.
I0302 18:58:29.312967 22895071117440 run.py:483] Algo bellman_ford step 1557 current loss 0.919639, current_train_items 49856.
I0302 18:58:29.340586 22895071117440 run.py:483] Algo bellman_ford step 1558 current loss 0.837776, current_train_items 49888.
I0302 18:58:29.371314 22895071117440 run.py:483] Algo bellman_ford step 1559 current loss 1.119285, current_train_items 49920.
I0302 18:58:29.389343 22895071117440 run.py:483] Algo bellman_ford step 1560 current loss 0.342388, current_train_items 49952.
I0302 18:58:29.405100 22895071117440 run.py:483] Algo bellman_ford step 1561 current loss 0.577111, current_train_items 49984.
I0302 18:58:29.426689 22895071117440 run.py:483] Algo bellman_ford step 1562 current loss 0.698037, current_train_items 50016.
I0302 18:58:29.454516 22895071117440 run.py:483] Algo bellman_ford step 1563 current loss 0.789157, current_train_items 50048.
I0302 18:58:29.485722 22895071117440 run.py:483] Algo bellman_ford step 1564 current loss 0.924468, current_train_items 50080.
I0302 18:58:29.503272 22895071117440 run.py:483] Algo bellman_ford step 1565 current loss 0.350614, current_train_items 50112.
I0302 18:58:29.518759 22895071117440 run.py:483] Algo bellman_ford step 1566 current loss 0.637938, current_train_items 50144.
I0302 18:58:29.540365 22895071117440 run.py:483] Algo bellman_ford step 1567 current loss 0.698544, current_train_items 50176.
I0302 18:58:29.567076 22895071117440 run.py:483] Algo bellman_ford step 1568 current loss 0.958613, current_train_items 50208.
I0302 18:58:29.598341 22895071117440 run.py:483] Algo bellman_ford step 1569 current loss 1.074707, current_train_items 50240.
I0302 18:58:29.616210 22895071117440 run.py:483] Algo bellman_ford step 1570 current loss 0.357557, current_train_items 50272.
I0302 18:58:29.631997 22895071117440 run.py:483] Algo bellman_ford step 1571 current loss 0.539890, current_train_items 50304.
I0302 18:58:29.654332 22895071117440 run.py:483] Algo bellman_ford step 1572 current loss 0.770482, current_train_items 50336.
I0302 18:58:29.681359 22895071117440 run.py:483] Algo bellman_ford step 1573 current loss 0.776717, current_train_items 50368.
I0302 18:58:29.709579 22895071117440 run.py:483] Algo bellman_ford step 1574 current loss 0.930786, current_train_items 50400.
I0302 18:58:29.727730 22895071117440 run.py:483] Algo bellman_ford step 1575 current loss 0.375380, current_train_items 50432.
I0302 18:58:29.743415 22895071117440 run.py:483] Algo bellman_ford step 1576 current loss 0.604088, current_train_items 50464.
I0302 18:58:29.763664 22895071117440 run.py:483] Algo bellman_ford step 1577 current loss 0.703012, current_train_items 50496.
I0302 18:58:29.791745 22895071117440 run.py:483] Algo bellman_ford step 1578 current loss 0.935740, current_train_items 50528.
I0302 18:58:29.821574 22895071117440 run.py:483] Algo bellman_ford step 1579 current loss 0.868185, current_train_items 50560.
I0302 18:58:29.839238 22895071117440 run.py:483] Algo bellman_ford step 1580 current loss 0.429080, current_train_items 50592.
I0302 18:58:29.854166 22895071117440 run.py:483] Algo bellman_ford step 1581 current loss 0.650231, current_train_items 50624.
I0302 18:58:29.875427 22895071117440 run.py:483] Algo bellman_ford step 1582 current loss 0.701716, current_train_items 50656.
I0302 18:58:29.902621 22895071117440 run.py:483] Algo bellman_ford step 1583 current loss 0.935221, current_train_items 50688.
I0302 18:58:29.933461 22895071117440 run.py:483] Algo bellman_ford step 1584 current loss 1.012118, current_train_items 50720.
I0302 18:58:29.951267 22895071117440 run.py:483] Algo bellman_ford step 1585 current loss 0.359345, current_train_items 50752.
I0302 18:58:29.966376 22895071117440 run.py:483] Algo bellman_ford step 1586 current loss 0.628591, current_train_items 50784.
I0302 18:58:29.988075 22895071117440 run.py:483] Algo bellman_ford step 1587 current loss 0.731907, current_train_items 50816.
I0302 18:58:30.014942 22895071117440 run.py:483] Algo bellman_ford step 1588 current loss 0.960291, current_train_items 50848.
I0302 18:58:30.044787 22895071117440 run.py:483] Algo bellman_ford step 1589 current loss 1.369224, current_train_items 50880.
I0302 18:58:30.062891 22895071117440 run.py:483] Algo bellman_ford step 1590 current loss 0.421461, current_train_items 50912.
I0302 18:58:30.078478 22895071117440 run.py:483] Algo bellman_ford step 1591 current loss 0.573090, current_train_items 50944.
I0302 18:58:30.099115 22895071117440 run.py:483] Algo bellman_ford step 1592 current loss 0.681585, current_train_items 50976.
I0302 18:58:30.127273 22895071117440 run.py:483] Algo bellman_ford step 1593 current loss 1.007812, current_train_items 51008.
I0302 18:58:30.158476 22895071117440 run.py:483] Algo bellman_ford step 1594 current loss 1.236351, current_train_items 51040.
I0302 18:58:30.176552 22895071117440 run.py:483] Algo bellman_ford step 1595 current loss 0.452248, current_train_items 51072.
I0302 18:58:30.191347 22895071117440 run.py:483] Algo bellman_ford step 1596 current loss 0.596765, current_train_items 51104.
I0302 18:58:30.213805 22895071117440 run.py:483] Algo bellman_ford step 1597 current loss 0.893752, current_train_items 51136.
I0302 18:58:30.240163 22895071117440 run.py:483] Algo bellman_ford step 1598 current loss 0.884432, current_train_items 51168.
I0302 18:58:30.270674 22895071117440 run.py:483] Algo bellman_ford step 1599 current loss 1.202360, current_train_items 51200.
I0302 18:58:30.288775 22895071117440 run.py:483] Algo bellman_ford step 1600 current loss 0.367856, current_train_items 51232.
I0302 18:58:30.296621 22895071117440 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:58:30.296726 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:58:30.312518 22895071117440 run.py:483] Algo bellman_ford step 1601 current loss 0.617167, current_train_items 51264.
I0302 18:58:30.335397 22895071117440 run.py:483] Algo bellman_ford step 1602 current loss 0.905191, current_train_items 51296.
I0302 18:58:30.363498 22895071117440 run.py:483] Algo bellman_ford step 1603 current loss 0.959869, current_train_items 51328.
I0302 18:58:30.394078 22895071117440 run.py:483] Algo bellman_ford step 1604 current loss 1.031888, current_train_items 51360.
I0302 18:58:30.412156 22895071117440 run.py:483] Algo bellman_ford step 1605 current loss 0.356260, current_train_items 51392.
I0302 18:58:30.427009 22895071117440 run.py:483] Algo bellman_ford step 1606 current loss 0.660733, current_train_items 51424.
I0302 18:58:30.448280 22895071117440 run.py:483] Algo bellman_ford step 1607 current loss 0.987044, current_train_items 51456.
I0302 18:58:30.476682 22895071117440 run.py:483] Algo bellman_ford step 1608 current loss 1.211820, current_train_items 51488.
I0302 18:58:30.507807 22895071117440 run.py:483] Algo bellman_ford step 1609 current loss 1.333768, current_train_items 51520.
I0302 18:58:30.525452 22895071117440 run.py:483] Algo bellman_ford step 1610 current loss 0.446193, current_train_items 51552.
I0302 18:58:30.540984 22895071117440 run.py:483] Algo bellman_ford step 1611 current loss 0.794117, current_train_items 51584.
I0302 18:58:30.562772 22895071117440 run.py:483] Algo bellman_ford step 1612 current loss 0.815022, current_train_items 51616.
I0302 18:58:30.590479 22895071117440 run.py:483] Algo bellman_ford step 1613 current loss 1.154451, current_train_items 51648.
I0302 18:58:30.619704 22895071117440 run.py:483] Algo bellman_ford step 1614 current loss 1.265624, current_train_items 51680.
I0302 18:58:30.637461 22895071117440 run.py:483] Algo bellman_ford step 1615 current loss 0.335687, current_train_items 51712.
I0302 18:58:30.653038 22895071117440 run.py:483] Algo bellman_ford step 1616 current loss 0.589774, current_train_items 51744.
I0302 18:58:30.674583 22895071117440 run.py:483] Algo bellman_ford step 1617 current loss 0.886710, current_train_items 51776.
I0302 18:58:30.701344 22895071117440 run.py:483] Algo bellman_ford step 1618 current loss 0.800392, current_train_items 51808.
I0302 18:58:30.732936 22895071117440 run.py:483] Algo bellman_ford step 1619 current loss 1.161395, current_train_items 51840.
I0302 18:58:30.750727 22895071117440 run.py:483] Algo bellman_ford step 1620 current loss 0.397980, current_train_items 51872.
I0302 18:58:30.765885 22895071117440 run.py:483] Algo bellman_ford step 1621 current loss 0.577000, current_train_items 51904.
I0302 18:58:30.787704 22895071117440 run.py:483] Algo bellman_ford step 1622 current loss 0.790863, current_train_items 51936.
I0302 18:58:30.815314 22895071117440 run.py:483] Algo bellman_ford step 1623 current loss 0.970989, current_train_items 51968.
I0302 18:58:30.845691 22895071117440 run.py:483] Algo bellman_ford step 1624 current loss 1.033836, current_train_items 52000.
I0302 18:58:30.863240 22895071117440 run.py:483] Algo bellman_ford step 1625 current loss 0.375797, current_train_items 52032.
I0302 18:58:30.878708 22895071117440 run.py:483] Algo bellman_ford step 1626 current loss 0.550866, current_train_items 52064.
I0302 18:58:30.900094 22895071117440 run.py:483] Algo bellman_ford step 1627 current loss 0.798787, current_train_items 52096.
I0302 18:58:30.928994 22895071117440 run.py:483] Algo bellman_ford step 1628 current loss 0.917042, current_train_items 52128.
I0302 18:58:30.958450 22895071117440 run.py:483] Algo bellman_ford step 1629 current loss 0.987962, current_train_items 52160.
I0302 18:58:30.976360 22895071117440 run.py:483] Algo bellman_ford step 1630 current loss 0.364031, current_train_items 52192.
I0302 18:58:30.992263 22895071117440 run.py:483] Algo bellman_ford step 1631 current loss 0.643727, current_train_items 52224.
I0302 18:58:31.014548 22895071117440 run.py:483] Algo bellman_ford step 1632 current loss 0.910985, current_train_items 52256.
I0302 18:58:31.041096 22895071117440 run.py:483] Algo bellman_ford step 1633 current loss 0.844011, current_train_items 52288.
I0302 18:58:31.071546 22895071117440 run.py:483] Algo bellman_ford step 1634 current loss 0.959589, current_train_items 52320.
I0302 18:58:31.089389 22895071117440 run.py:483] Algo bellman_ford step 1635 current loss 0.301202, current_train_items 52352.
I0302 18:58:31.104737 22895071117440 run.py:483] Algo bellman_ford step 1636 current loss 0.585483, current_train_items 52384.
I0302 18:58:31.126663 22895071117440 run.py:483] Algo bellman_ford step 1637 current loss 0.819124, current_train_items 52416.
I0302 18:58:31.154718 22895071117440 run.py:483] Algo bellman_ford step 1638 current loss 0.811336, current_train_items 52448.
I0302 18:58:31.187314 22895071117440 run.py:483] Algo bellman_ford step 1639 current loss 1.036295, current_train_items 52480.
I0302 18:58:31.204624 22895071117440 run.py:483] Algo bellman_ford step 1640 current loss 0.317083, current_train_items 52512.
I0302 18:58:31.219624 22895071117440 run.py:483] Algo bellman_ford step 1641 current loss 0.527932, current_train_items 52544.
I0302 18:58:31.241674 22895071117440 run.py:483] Algo bellman_ford step 1642 current loss 0.822723, current_train_items 52576.
I0302 18:58:31.269309 22895071117440 run.py:483] Algo bellman_ford step 1643 current loss 0.964154, current_train_items 52608.
I0302 18:58:31.298528 22895071117440 run.py:483] Algo bellman_ford step 1644 current loss 0.991038, current_train_items 52640.
I0302 18:58:31.316064 22895071117440 run.py:483] Algo bellman_ford step 1645 current loss 0.392763, current_train_items 52672.
I0302 18:58:31.331331 22895071117440 run.py:483] Algo bellman_ford step 1646 current loss 0.580691, current_train_items 52704.
I0302 18:58:31.353816 22895071117440 run.py:483] Algo bellman_ford step 1647 current loss 0.823843, current_train_items 52736.
I0302 18:58:31.379779 22895071117440 run.py:483] Algo bellman_ford step 1648 current loss 0.743923, current_train_items 52768.
I0302 18:58:31.410972 22895071117440 run.py:483] Algo bellman_ford step 1649 current loss 1.044142, current_train_items 52800.
I0302 18:58:31.428943 22895071117440 run.py:483] Algo bellman_ford step 1650 current loss 0.559967, current_train_items 52832.
I0302 18:58:31.436907 22895071117440 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:58:31.437011 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.922, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 18:58:31.465602 22895071117440 run.py:483] Algo bellman_ford step 1651 current loss 0.590277, current_train_items 52864.
I0302 18:58:31.488535 22895071117440 run.py:483] Algo bellman_ford step 1652 current loss 0.887376, current_train_items 52896.
I0302 18:58:31.515826 22895071117440 run.py:483] Algo bellman_ford step 1653 current loss 0.830132, current_train_items 52928.
I0302 18:58:31.545452 22895071117440 run.py:483] Algo bellman_ford step 1654 current loss 1.140911, current_train_items 52960.
I0302 18:58:31.563845 22895071117440 run.py:483] Algo bellman_ford step 1655 current loss 0.416009, current_train_items 52992.
I0302 18:58:31.579387 22895071117440 run.py:483] Algo bellman_ford step 1656 current loss 0.631068, current_train_items 53024.
I0302 18:58:31.601007 22895071117440 run.py:483] Algo bellman_ford step 1657 current loss 0.799046, current_train_items 53056.
I0302 18:58:31.627140 22895071117440 run.py:483] Algo bellman_ford step 1658 current loss 0.888331, current_train_items 53088.
I0302 18:58:31.655661 22895071117440 run.py:483] Algo bellman_ford step 1659 current loss 0.995093, current_train_items 53120.
I0302 18:58:31.673587 22895071117440 run.py:483] Algo bellman_ford step 1660 current loss 0.454493, current_train_items 53152.
I0302 18:58:31.689225 22895071117440 run.py:483] Algo bellman_ford step 1661 current loss 0.559824, current_train_items 53184.
I0302 18:58:31.709941 22895071117440 run.py:483] Algo bellman_ford step 1662 current loss 0.826570, current_train_items 53216.
I0302 18:58:31.738548 22895071117440 run.py:483] Algo bellman_ford step 1663 current loss 0.897679, current_train_items 53248.
I0302 18:58:31.769570 22895071117440 run.py:483] Algo bellman_ford step 1664 current loss 1.137498, current_train_items 53280.
I0302 18:58:31.787555 22895071117440 run.py:483] Algo bellman_ford step 1665 current loss 0.363775, current_train_items 53312.
I0302 18:58:31.802829 22895071117440 run.py:483] Algo bellman_ford step 1666 current loss 0.620936, current_train_items 53344.
I0302 18:58:31.823952 22895071117440 run.py:483] Algo bellman_ford step 1667 current loss 0.731901, current_train_items 53376.
I0302 18:58:31.850508 22895071117440 run.py:483] Algo bellman_ford step 1668 current loss 0.749009, current_train_items 53408.
I0302 18:58:31.879172 22895071117440 run.py:483] Algo bellman_ford step 1669 current loss 1.053124, current_train_items 53440.
I0302 18:58:31.897133 22895071117440 run.py:483] Algo bellman_ford step 1670 current loss 0.400188, current_train_items 53472.
I0302 18:58:31.912500 22895071117440 run.py:483] Algo bellman_ford step 1671 current loss 0.649934, current_train_items 53504.
I0302 18:58:31.933428 22895071117440 run.py:483] Algo bellman_ford step 1672 current loss 0.735816, current_train_items 53536.
I0302 18:58:31.961471 22895071117440 run.py:483] Algo bellman_ford step 1673 current loss 0.888625, current_train_items 53568.
I0302 18:58:31.992325 22895071117440 run.py:483] Algo bellman_ford step 1674 current loss 0.971997, current_train_items 53600.
I0302 18:58:32.010364 22895071117440 run.py:483] Algo bellman_ford step 1675 current loss 0.356690, current_train_items 53632.
I0302 18:58:32.026012 22895071117440 run.py:483] Algo bellman_ford step 1676 current loss 0.605266, current_train_items 53664.
I0302 18:58:32.046964 22895071117440 run.py:483] Algo bellman_ford step 1677 current loss 0.626179, current_train_items 53696.
I0302 18:58:32.073922 22895071117440 run.py:483] Algo bellman_ford step 1678 current loss 0.948411, current_train_items 53728.
I0302 18:58:32.105174 22895071117440 run.py:483] Algo bellman_ford step 1679 current loss 1.059667, current_train_items 53760.
I0302 18:58:32.123194 22895071117440 run.py:483] Algo bellman_ford step 1680 current loss 0.330661, current_train_items 53792.
I0302 18:58:32.138178 22895071117440 run.py:483] Algo bellman_ford step 1681 current loss 0.498967, current_train_items 53824.
I0302 18:58:32.158777 22895071117440 run.py:483] Algo bellman_ford step 1682 current loss 0.816187, current_train_items 53856.
I0302 18:58:32.184919 22895071117440 run.py:483] Algo bellman_ford step 1683 current loss 0.828023, current_train_items 53888.
I0302 18:58:32.215158 22895071117440 run.py:483] Algo bellman_ford step 1684 current loss 0.972250, current_train_items 53920.
I0302 18:58:32.233438 22895071117440 run.py:483] Algo bellman_ford step 1685 current loss 0.329401, current_train_items 53952.
I0302 18:58:32.249001 22895071117440 run.py:483] Algo bellman_ford step 1686 current loss 0.599690, current_train_items 53984.
I0302 18:58:32.270559 22895071117440 run.py:483] Algo bellman_ford step 1687 current loss 0.677626, current_train_items 54016.
I0302 18:58:32.297960 22895071117440 run.py:483] Algo bellman_ford step 1688 current loss 0.831627, current_train_items 54048.
I0302 18:58:32.330002 22895071117440 run.py:483] Algo bellman_ford step 1689 current loss 1.096088, current_train_items 54080.
I0302 18:58:32.347816 22895071117440 run.py:483] Algo bellman_ford step 1690 current loss 0.320933, current_train_items 54112.
I0302 18:58:32.363689 22895071117440 run.py:483] Algo bellman_ford step 1691 current loss 0.673526, current_train_items 54144.
I0302 18:58:32.385420 22895071117440 run.py:483] Algo bellman_ford step 1692 current loss 0.807543, current_train_items 54176.
I0302 18:58:32.413542 22895071117440 run.py:483] Algo bellman_ford step 1693 current loss 0.894819, current_train_items 54208.
I0302 18:58:32.444118 22895071117440 run.py:483] Algo bellman_ford step 1694 current loss 1.058734, current_train_items 54240.
I0302 18:58:32.461947 22895071117440 run.py:483] Algo bellman_ford step 1695 current loss 0.374652, current_train_items 54272.
I0302 18:58:32.477628 22895071117440 run.py:483] Algo bellman_ford step 1696 current loss 0.489792, current_train_items 54304.
I0302 18:58:32.497756 22895071117440 run.py:483] Algo bellman_ford step 1697 current loss 0.791984, current_train_items 54336.
I0302 18:58:32.523600 22895071117440 run.py:483] Algo bellman_ford step 1698 current loss 0.774768, current_train_items 54368.
I0302 18:58:32.554145 22895071117440 run.py:483] Algo bellman_ford step 1699 current loss 1.077749, current_train_items 54400.
I0302 18:58:32.572537 22895071117440 run.py:483] Algo bellman_ford step 1700 current loss 0.429836, current_train_items 54432.
I0302 18:58:32.580488 22895071117440 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:58:32.580595 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:58:32.596563 22895071117440 run.py:483] Algo bellman_ford step 1701 current loss 0.609938, current_train_items 54464.
I0302 18:58:32.618127 22895071117440 run.py:483] Algo bellman_ford step 1702 current loss 0.711335, current_train_items 54496.
I0302 18:58:32.645692 22895071117440 run.py:483] Algo bellman_ford step 1703 current loss 0.762525, current_train_items 54528.
I0302 18:58:32.675764 22895071117440 run.py:483] Algo bellman_ford step 1704 current loss 1.099020, current_train_items 54560.
I0302 18:58:32.694431 22895071117440 run.py:483] Algo bellman_ford step 1705 current loss 0.375769, current_train_items 54592.
I0302 18:58:32.709652 22895071117440 run.py:483] Algo bellman_ford step 1706 current loss 0.524838, current_train_items 54624.
I0302 18:58:32.731178 22895071117440 run.py:483] Algo bellman_ford step 1707 current loss 0.754335, current_train_items 54656.
I0302 18:58:32.758435 22895071117440 run.py:483] Algo bellman_ford step 1708 current loss 0.857942, current_train_items 54688.
I0302 18:58:32.789916 22895071117440 run.py:483] Algo bellman_ford step 1709 current loss 0.972109, current_train_items 54720.
I0302 18:58:32.807835 22895071117440 run.py:483] Algo bellman_ford step 1710 current loss 0.377232, current_train_items 54752.
I0302 18:58:32.823370 22895071117440 run.py:483] Algo bellman_ford step 1711 current loss 0.624500, current_train_items 54784.
I0302 18:58:32.845362 22895071117440 run.py:483] Algo bellman_ford step 1712 current loss 0.805721, current_train_items 54816.
I0302 18:58:32.871932 22895071117440 run.py:483] Algo bellman_ford step 1713 current loss 0.855324, current_train_items 54848.
I0302 18:58:32.901949 22895071117440 run.py:483] Algo bellman_ford step 1714 current loss 0.860628, current_train_items 54880.
I0302 18:58:32.919754 22895071117440 run.py:483] Algo bellman_ford step 1715 current loss 0.386441, current_train_items 54912.
I0302 18:58:32.934637 22895071117440 run.py:483] Algo bellman_ford step 1716 current loss 0.550939, current_train_items 54944.
I0302 18:58:32.956806 22895071117440 run.py:483] Algo bellman_ford step 1717 current loss 0.797387, current_train_items 54976.
I0302 18:58:32.982471 22895071117440 run.py:483] Algo bellman_ford step 1718 current loss 0.826586, current_train_items 55008.
I0302 18:58:33.012610 22895071117440 run.py:483] Algo bellman_ford step 1719 current loss 0.969630, current_train_items 55040.
I0302 18:58:33.030381 22895071117440 run.py:483] Algo bellman_ford step 1720 current loss 0.371927, current_train_items 55072.
I0302 18:58:33.045425 22895071117440 run.py:483] Algo bellman_ford step 1721 current loss 0.652671, current_train_items 55104.
I0302 18:58:33.066595 22895071117440 run.py:483] Algo bellman_ford step 1722 current loss 0.663388, current_train_items 55136.
I0302 18:58:33.093700 22895071117440 run.py:483] Algo bellman_ford step 1723 current loss 0.783639, current_train_items 55168.
I0302 18:58:33.123547 22895071117440 run.py:483] Algo bellman_ford step 1724 current loss 0.981798, current_train_items 55200.
I0302 18:58:33.141204 22895071117440 run.py:483] Algo bellman_ford step 1725 current loss 0.342759, current_train_items 55232.
I0302 18:58:33.156724 22895071117440 run.py:483] Algo bellman_ford step 1726 current loss 0.667571, current_train_items 55264.
I0302 18:58:33.179631 22895071117440 run.py:483] Algo bellman_ford step 1727 current loss 0.761475, current_train_items 55296.
I0302 18:58:33.206907 22895071117440 run.py:483] Algo bellman_ford step 1728 current loss 0.790635, current_train_items 55328.
I0302 18:58:33.236763 22895071117440 run.py:483] Algo bellman_ford step 1729 current loss 0.999261, current_train_items 55360.
I0302 18:58:33.254563 22895071117440 run.py:483] Algo bellman_ford step 1730 current loss 0.295310, current_train_items 55392.
I0302 18:58:33.269919 22895071117440 run.py:483] Algo bellman_ford step 1731 current loss 0.556052, current_train_items 55424.
I0302 18:58:33.291400 22895071117440 run.py:483] Algo bellman_ford step 1732 current loss 0.788212, current_train_items 55456.
I0302 18:58:33.318809 22895071117440 run.py:483] Algo bellman_ford step 1733 current loss 0.865472, current_train_items 55488.
I0302 18:58:33.346979 22895071117440 run.py:483] Algo bellman_ford step 1734 current loss 1.061978, current_train_items 55520.
I0302 18:58:33.364678 22895071117440 run.py:483] Algo bellman_ford step 1735 current loss 0.395552, current_train_items 55552.
I0302 18:58:33.380151 22895071117440 run.py:483] Algo bellman_ford step 1736 current loss 0.596315, current_train_items 55584.
I0302 18:58:33.401697 22895071117440 run.py:483] Algo bellman_ford step 1737 current loss 0.732788, current_train_items 55616.
I0302 18:58:33.429389 22895071117440 run.py:483] Algo bellman_ford step 1738 current loss 0.929855, current_train_items 55648.
I0302 18:58:33.459912 22895071117440 run.py:483] Algo bellman_ford step 1739 current loss 1.024869, current_train_items 55680.
I0302 18:58:33.477685 22895071117440 run.py:483] Algo bellman_ford step 1740 current loss 0.346728, current_train_items 55712.
I0302 18:58:33.493105 22895071117440 run.py:483] Algo bellman_ford step 1741 current loss 0.589415, current_train_items 55744.
I0302 18:58:33.514710 22895071117440 run.py:483] Algo bellman_ford step 1742 current loss 0.643895, current_train_items 55776.
I0302 18:58:33.542399 22895071117440 run.py:483] Algo bellman_ford step 1743 current loss 0.790217, current_train_items 55808.
I0302 18:58:33.571218 22895071117440 run.py:483] Algo bellman_ford step 1744 current loss 1.002620, current_train_items 55840.
I0302 18:58:33.589062 22895071117440 run.py:483] Algo bellman_ford step 1745 current loss 0.315673, current_train_items 55872.
I0302 18:58:33.604619 22895071117440 run.py:483] Algo bellman_ford step 1746 current loss 0.627670, current_train_items 55904.
I0302 18:58:33.625909 22895071117440 run.py:483] Algo bellman_ford step 1747 current loss 0.838303, current_train_items 55936.
I0302 18:58:33.653214 22895071117440 run.py:483] Algo bellman_ford step 1748 current loss 0.887720, current_train_items 55968.
I0302 18:58:33.681757 22895071117440 run.py:483] Algo bellman_ford step 1749 current loss 1.026317, current_train_items 56000.
I0302 18:58:33.699625 22895071117440 run.py:483] Algo bellman_ford step 1750 current loss 0.295240, current_train_items 56032.
I0302 18:58:33.707467 22895071117440 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:58:33.707571 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:58:33.723768 22895071117440 run.py:483] Algo bellman_ford step 1751 current loss 0.570080, current_train_items 56064.
I0302 18:58:33.745678 22895071117440 run.py:483] Algo bellman_ford step 1752 current loss 0.726734, current_train_items 56096.
I0302 18:58:33.774601 22895071117440 run.py:483] Algo bellman_ford step 1753 current loss 0.862008, current_train_items 56128.
I0302 18:58:33.806251 22895071117440 run.py:483] Algo bellman_ford step 1754 current loss 1.022658, current_train_items 56160.
I0302 18:58:33.824604 22895071117440 run.py:483] Algo bellman_ford step 1755 current loss 0.315270, current_train_items 56192.
I0302 18:58:33.840109 22895071117440 run.py:483] Algo bellman_ford step 1756 current loss 0.580044, current_train_items 56224.
I0302 18:58:33.862093 22895071117440 run.py:483] Algo bellman_ford step 1757 current loss 0.763257, current_train_items 56256.
I0302 18:58:33.890138 22895071117440 run.py:483] Algo bellman_ford step 1758 current loss 0.919111, current_train_items 56288.
I0302 18:58:33.918209 22895071117440 run.py:483] Algo bellman_ford step 1759 current loss 0.883805, current_train_items 56320.
I0302 18:58:33.936232 22895071117440 run.py:483] Algo bellman_ford step 1760 current loss 0.387487, current_train_items 56352.
I0302 18:58:33.951782 22895071117440 run.py:483] Algo bellman_ford step 1761 current loss 0.566113, current_train_items 56384.
I0302 18:58:33.973953 22895071117440 run.py:483] Algo bellman_ford step 1762 current loss 0.804725, current_train_items 56416.
I0302 18:58:34.003103 22895071117440 run.py:483] Algo bellman_ford step 1763 current loss 0.889630, current_train_items 56448.
I0302 18:58:34.035060 22895071117440 run.py:483] Algo bellman_ford step 1764 current loss 0.983227, current_train_items 56480.
I0302 18:58:34.052671 22895071117440 run.py:483] Algo bellman_ford step 1765 current loss 0.344355, current_train_items 56512.
I0302 18:58:34.067760 22895071117440 run.py:483] Algo bellman_ford step 1766 current loss 0.562216, current_train_items 56544.
I0302 18:58:34.089118 22895071117440 run.py:483] Algo bellman_ford step 1767 current loss 0.683378, current_train_items 56576.
I0302 18:58:34.116467 22895071117440 run.py:483] Algo bellman_ford step 1768 current loss 0.878040, current_train_items 56608.
I0302 18:58:34.145060 22895071117440 run.py:483] Algo bellman_ford step 1769 current loss 0.993077, current_train_items 56640.
I0302 18:58:34.163525 22895071117440 run.py:483] Algo bellman_ford step 1770 current loss 0.349216, current_train_items 56672.
I0302 18:58:34.179203 22895071117440 run.py:483] Algo bellman_ford step 1771 current loss 0.662944, current_train_items 56704.
I0302 18:58:34.201362 22895071117440 run.py:483] Algo bellman_ford step 1772 current loss 0.854426, current_train_items 56736.
I0302 18:58:34.229548 22895071117440 run.py:483] Algo bellman_ford step 1773 current loss 0.938477, current_train_items 56768.
I0302 18:58:34.258686 22895071117440 run.py:483] Algo bellman_ford step 1774 current loss 1.079415, current_train_items 56800.
I0302 18:58:34.276726 22895071117440 run.py:483] Algo bellman_ford step 1775 current loss 0.358671, current_train_items 56832.
I0302 18:58:34.292469 22895071117440 run.py:483] Algo bellman_ford step 1776 current loss 0.655458, current_train_items 56864.
I0302 18:58:34.314179 22895071117440 run.py:483] Algo bellman_ford step 1777 current loss 0.925819, current_train_items 56896.
I0302 18:58:34.341252 22895071117440 run.py:483] Algo bellman_ford step 1778 current loss 0.895757, current_train_items 56928.
I0302 18:58:34.369745 22895071117440 run.py:483] Algo bellman_ford step 1779 current loss 0.858572, current_train_items 56960.
I0302 18:58:34.387549 22895071117440 run.py:483] Algo bellman_ford step 1780 current loss 0.315708, current_train_items 56992.
I0302 18:58:34.402784 22895071117440 run.py:483] Algo bellman_ford step 1781 current loss 0.562369, current_train_items 57024.
I0302 18:58:34.424103 22895071117440 run.py:483] Algo bellman_ford step 1782 current loss 0.688485, current_train_items 57056.
I0302 18:58:34.451444 22895071117440 run.py:483] Algo bellman_ford step 1783 current loss 0.904277, current_train_items 57088.
I0302 18:58:34.480872 22895071117440 run.py:483] Algo bellman_ford step 1784 current loss 0.950100, current_train_items 57120.
I0302 18:58:34.499131 22895071117440 run.py:483] Algo bellman_ford step 1785 current loss 0.354811, current_train_items 57152.
I0302 18:58:34.514944 22895071117440 run.py:483] Algo bellman_ford step 1786 current loss 0.615938, current_train_items 57184.
I0302 18:58:34.535775 22895071117440 run.py:483] Algo bellman_ford step 1787 current loss 0.752799, current_train_items 57216.
I0302 18:58:34.561966 22895071117440 run.py:483] Algo bellman_ford step 1788 current loss 0.794478, current_train_items 57248.
I0302 18:58:34.591991 22895071117440 run.py:483] Algo bellman_ford step 1789 current loss 0.884610, current_train_items 57280.
I0302 18:58:34.610147 22895071117440 run.py:483] Algo bellman_ford step 1790 current loss 0.260443, current_train_items 57312.
I0302 18:58:34.626290 22895071117440 run.py:483] Algo bellman_ford step 1791 current loss 0.578506, current_train_items 57344.
I0302 18:58:34.648094 22895071117440 run.py:483] Algo bellman_ford step 1792 current loss 0.700279, current_train_items 57376.
I0302 18:58:34.674426 22895071117440 run.py:483] Algo bellman_ford step 1793 current loss 0.899260, current_train_items 57408.
I0302 18:58:34.704366 22895071117440 run.py:483] Algo bellman_ford step 1794 current loss 1.012255, current_train_items 57440.
I0302 18:58:34.721884 22895071117440 run.py:483] Algo bellman_ford step 1795 current loss 0.408728, current_train_items 57472.
I0302 18:58:34.737111 22895071117440 run.py:483] Algo bellman_ford step 1796 current loss 0.629493, current_train_items 57504.
I0302 18:58:34.759470 22895071117440 run.py:483] Algo bellman_ford step 1797 current loss 0.780196, current_train_items 57536.
I0302 18:58:34.787137 22895071117440 run.py:483] Algo bellman_ford step 1798 current loss 0.905391, current_train_items 57568.
I0302 18:58:34.818135 22895071117440 run.py:483] Algo bellman_ford step 1799 current loss 1.031909, current_train_items 57600.
I0302 18:58:34.836211 22895071117440 run.py:483] Algo bellman_ford step 1800 current loss 0.433827, current_train_items 57632.
I0302 18:58:34.844156 22895071117440 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:58:34.844260 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:58:34.859973 22895071117440 run.py:483] Algo bellman_ford step 1801 current loss 0.505716, current_train_items 57664.
I0302 18:58:34.882529 22895071117440 run.py:483] Algo bellman_ford step 1802 current loss 0.730265, current_train_items 57696.
I0302 18:58:34.909481 22895071117440 run.py:483] Algo bellman_ford step 1803 current loss 0.815278, current_train_items 57728.
I0302 18:58:34.938924 22895071117440 run.py:483] Algo bellman_ford step 1804 current loss 1.352117, current_train_items 57760.
I0302 18:58:34.957279 22895071117440 run.py:483] Algo bellman_ford step 1805 current loss 0.359569, current_train_items 57792.
I0302 18:58:34.972455 22895071117440 run.py:483] Algo bellman_ford step 1806 current loss 0.538747, current_train_items 57824.
I0302 18:58:34.993817 22895071117440 run.py:483] Algo bellman_ford step 1807 current loss 0.785266, current_train_items 57856.
I0302 18:58:35.020105 22895071117440 run.py:483] Algo bellman_ford step 1808 current loss 0.848568, current_train_items 57888.
I0302 18:58:35.051249 22895071117440 run.py:483] Algo bellman_ford step 1809 current loss 1.130772, current_train_items 57920.
I0302 18:58:35.068569 22895071117440 run.py:483] Algo bellman_ford step 1810 current loss 0.450588, current_train_items 57952.
I0302 18:58:35.083804 22895071117440 run.py:483] Algo bellman_ford step 1811 current loss 0.627048, current_train_items 57984.
I0302 18:58:35.105722 22895071117440 run.py:483] Algo bellman_ford step 1812 current loss 0.880335, current_train_items 58016.
I0302 18:58:35.130866 22895071117440 run.py:483] Algo bellman_ford step 1813 current loss 0.814694, current_train_items 58048.
I0302 18:58:35.160615 22895071117440 run.py:483] Algo bellman_ford step 1814 current loss 1.052071, current_train_items 58080.
I0302 18:58:35.178302 22895071117440 run.py:483] Algo bellman_ford step 1815 current loss 0.333746, current_train_items 58112.
I0302 18:58:35.194123 22895071117440 run.py:483] Algo bellman_ford step 1816 current loss 0.761147, current_train_items 58144.
I0302 18:58:35.215374 22895071117440 run.py:483] Algo bellman_ford step 1817 current loss 0.699614, current_train_items 58176.
I0302 18:58:35.242623 22895071117440 run.py:483] Algo bellman_ford step 1818 current loss 0.913486, current_train_items 58208.
I0302 18:58:35.272476 22895071117440 run.py:483] Algo bellman_ford step 1819 current loss 1.013738, current_train_items 58240.
I0302 18:58:35.290249 22895071117440 run.py:483] Algo bellman_ford step 1820 current loss 0.420454, current_train_items 58272.
I0302 18:58:35.306043 22895071117440 run.py:483] Algo bellman_ford step 1821 current loss 0.580231, current_train_items 58304.
I0302 18:58:35.329629 22895071117440 run.py:483] Algo bellman_ford step 1822 current loss 0.781137, current_train_items 58336.
I0302 18:58:35.357511 22895071117440 run.py:483] Algo bellman_ford step 1823 current loss 0.813544, current_train_items 58368.
I0302 18:58:35.386068 22895071117440 run.py:483] Algo bellman_ford step 1824 current loss 0.899668, current_train_items 58400.
I0302 18:58:35.403548 22895071117440 run.py:483] Algo bellman_ford step 1825 current loss 0.404677, current_train_items 58432.
I0302 18:58:35.419047 22895071117440 run.py:483] Algo bellman_ford step 1826 current loss 0.653561, current_train_items 58464.
I0302 18:58:35.440147 22895071117440 run.py:483] Algo bellman_ford step 1827 current loss 0.664882, current_train_items 58496.
I0302 18:58:35.468298 22895071117440 run.py:483] Algo bellman_ford step 1828 current loss 0.982285, current_train_items 58528.
I0302 18:58:35.500494 22895071117440 run.py:483] Algo bellman_ford step 1829 current loss 1.151115, current_train_items 58560.
I0302 18:58:35.518274 22895071117440 run.py:483] Algo bellman_ford step 1830 current loss 0.346680, current_train_items 58592.
I0302 18:58:35.533890 22895071117440 run.py:483] Algo bellman_ford step 1831 current loss 0.658527, current_train_items 58624.
I0302 18:58:35.557087 22895071117440 run.py:483] Algo bellman_ford step 1832 current loss 0.866085, current_train_items 58656.
I0302 18:58:35.584590 22895071117440 run.py:483] Algo bellman_ford step 1833 current loss 0.916117, current_train_items 58688.
I0302 18:58:35.613849 22895071117440 run.py:483] Algo bellman_ford step 1834 current loss 0.896117, current_train_items 58720.
I0302 18:58:35.631720 22895071117440 run.py:483] Algo bellman_ford step 1835 current loss 0.238174, current_train_items 58752.
I0302 18:58:35.647320 22895071117440 run.py:483] Algo bellman_ford step 1836 current loss 0.549544, current_train_items 58784.
I0302 18:58:35.669826 22895071117440 run.py:483] Algo bellman_ford step 1837 current loss 0.854022, current_train_items 58816.
I0302 18:58:35.696566 22895071117440 run.py:483] Algo bellman_ford step 1838 current loss 0.838686, current_train_items 58848.
I0302 18:58:35.726892 22895071117440 run.py:483] Algo bellman_ford step 1839 current loss 0.944616, current_train_items 58880.
I0302 18:58:35.744431 22895071117440 run.py:483] Algo bellman_ford step 1840 current loss 0.338644, current_train_items 58912.
I0302 18:58:35.759950 22895071117440 run.py:483] Algo bellman_ford step 1841 current loss 0.559987, current_train_items 58944.
I0302 18:58:35.781971 22895071117440 run.py:483] Algo bellman_ford step 1842 current loss 0.704165, current_train_items 58976.
I0302 18:58:35.809705 22895071117440 run.py:483] Algo bellman_ford step 1843 current loss 0.838131, current_train_items 59008.
I0302 18:58:35.839791 22895071117440 run.py:483] Algo bellman_ford step 1844 current loss 1.017119, current_train_items 59040.
I0302 18:58:35.857819 22895071117440 run.py:483] Algo bellman_ford step 1845 current loss 0.367010, current_train_items 59072.
I0302 18:58:35.873059 22895071117440 run.py:483] Algo bellman_ford step 1846 current loss 0.544675, current_train_items 59104.
I0302 18:58:35.894862 22895071117440 run.py:483] Algo bellman_ford step 1847 current loss 0.781739, current_train_items 59136.
I0302 18:58:35.922414 22895071117440 run.py:483] Algo bellman_ford step 1848 current loss 0.824129, current_train_items 59168.
I0302 18:58:35.953796 22895071117440 run.py:483] Algo bellman_ford step 1849 current loss 0.991807, current_train_items 59200.
I0302 18:58:35.971742 22895071117440 run.py:483] Algo bellman_ford step 1850 current loss 0.360957, current_train_items 59232.
I0302 18:58:35.979665 22895071117440 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:58:35.979770 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 18:58:35.995859 22895071117440 run.py:483] Algo bellman_ford step 1851 current loss 0.559528, current_train_items 59264.
I0302 18:58:36.018313 22895071117440 run.py:483] Algo bellman_ford step 1852 current loss 0.746787, current_train_items 59296.
I0302 18:58:36.046453 22895071117440 run.py:483] Algo bellman_ford step 1853 current loss 0.832426, current_train_items 59328.
I0302 18:58:36.076837 22895071117440 run.py:483] Algo bellman_ford step 1854 current loss 0.965874, current_train_items 59360.
I0302 18:58:36.095025 22895071117440 run.py:483] Algo bellman_ford step 1855 current loss 0.378894, current_train_items 59392.
I0302 18:58:36.109933 22895071117440 run.py:483] Algo bellman_ford step 1856 current loss 0.555821, current_train_items 59424.
I0302 18:58:36.132195 22895071117440 run.py:483] Algo bellman_ford step 1857 current loss 0.785500, current_train_items 59456.
I0302 18:58:36.158586 22895071117440 run.py:483] Algo bellman_ford step 1858 current loss 0.730706, current_train_items 59488.
I0302 18:58:36.188572 22895071117440 run.py:483] Algo bellman_ford step 1859 current loss 0.930980, current_train_items 59520.
I0302 18:58:36.206634 22895071117440 run.py:483] Algo bellman_ford step 1860 current loss 0.277301, current_train_items 59552.
I0302 18:58:36.222618 22895071117440 run.py:483] Algo bellman_ford step 1861 current loss 0.489635, current_train_items 59584.
I0302 18:58:36.243891 22895071117440 run.py:483] Algo bellman_ford step 1862 current loss 0.745187, current_train_items 59616.
I0302 18:58:36.270875 22895071117440 run.py:483] Algo bellman_ford step 1863 current loss 0.801983, current_train_items 59648.
I0302 18:58:36.301480 22895071117440 run.py:483] Algo bellman_ford step 1864 current loss 0.964302, current_train_items 59680.
I0302 18:58:36.319083 22895071117440 run.py:483] Algo bellman_ford step 1865 current loss 0.339745, current_train_items 59712.
I0302 18:58:36.334502 22895071117440 run.py:483] Algo bellman_ford step 1866 current loss 0.652642, current_train_items 59744.
I0302 18:58:36.356873 22895071117440 run.py:483] Algo bellman_ford step 1867 current loss 0.917174, current_train_items 59776.
I0302 18:58:36.384331 22895071117440 run.py:483] Algo bellman_ford step 1868 current loss 0.864486, current_train_items 59808.
I0302 18:58:36.413848 22895071117440 run.py:483] Algo bellman_ford step 1869 current loss 0.828443, current_train_items 59840.
I0302 18:58:36.431913 22895071117440 run.py:483] Algo bellman_ford step 1870 current loss 0.346333, current_train_items 59872.
I0302 18:58:36.447385 22895071117440 run.py:483] Algo bellman_ford step 1871 current loss 0.501927, current_train_items 59904.
I0302 18:58:36.468374 22895071117440 run.py:483] Algo bellman_ford step 1872 current loss 0.673481, current_train_items 59936.
I0302 18:58:36.495660 22895071117440 run.py:483] Algo bellman_ford step 1873 current loss 0.764316, current_train_items 59968.
I0302 18:58:36.524877 22895071117440 run.py:483] Algo bellman_ford step 1874 current loss 0.975878, current_train_items 60000.
I0302 18:58:36.543025 22895071117440 run.py:483] Algo bellman_ford step 1875 current loss 0.363656, current_train_items 60032.
I0302 18:58:36.558453 22895071117440 run.py:483] Algo bellman_ford step 1876 current loss 0.539538, current_train_items 60064.
I0302 18:58:36.579821 22895071117440 run.py:483] Algo bellman_ford step 1877 current loss 0.864362, current_train_items 60096.
I0302 18:58:36.606018 22895071117440 run.py:483] Algo bellman_ford step 1878 current loss 0.793355, current_train_items 60128.
I0302 18:58:36.638361 22895071117440 run.py:483] Algo bellman_ford step 1879 current loss 1.134055, current_train_items 60160.
I0302 18:58:36.655720 22895071117440 run.py:483] Algo bellman_ford step 1880 current loss 0.328585, current_train_items 60192.
I0302 18:58:36.671512 22895071117440 run.py:483] Algo bellman_ford step 1881 current loss 0.644932, current_train_items 60224.
I0302 18:58:36.692363 22895071117440 run.py:483] Algo bellman_ford step 1882 current loss 0.706650, current_train_items 60256.
I0302 18:58:36.719723 22895071117440 run.py:483] Algo bellman_ford step 1883 current loss 0.906593, current_train_items 60288.
I0302 18:58:36.749804 22895071117440 run.py:483] Algo bellman_ford step 1884 current loss 1.282449, current_train_items 60320.
I0302 18:58:36.767698 22895071117440 run.py:483] Algo bellman_ford step 1885 current loss 0.292113, current_train_items 60352.
I0302 18:58:36.783699 22895071117440 run.py:483] Algo bellman_ford step 1886 current loss 0.633304, current_train_items 60384.
I0302 18:58:36.804980 22895071117440 run.py:483] Algo bellman_ford step 1887 current loss 0.859133, current_train_items 60416.
I0302 18:58:36.831077 22895071117440 run.py:483] Algo bellman_ford step 1888 current loss 0.743344, current_train_items 60448.
I0302 18:58:36.862725 22895071117440 run.py:483] Algo bellman_ford step 1889 current loss 1.009261, current_train_items 60480.
I0302 18:58:36.880620 22895071117440 run.py:483] Algo bellman_ford step 1890 current loss 0.316498, current_train_items 60512.
I0302 18:58:36.896331 22895071117440 run.py:483] Algo bellman_ford step 1891 current loss 0.610488, current_train_items 60544.
I0302 18:58:36.917322 22895071117440 run.py:483] Algo bellman_ford step 1892 current loss 0.729939, current_train_items 60576.
I0302 18:58:36.945050 22895071117440 run.py:483] Algo bellman_ford step 1893 current loss 0.853310, current_train_items 60608.
I0302 18:58:36.977869 22895071117440 run.py:483] Algo bellman_ford step 1894 current loss 1.094339, current_train_items 60640.
I0302 18:58:36.995525 22895071117440 run.py:483] Algo bellman_ford step 1895 current loss 0.377903, current_train_items 60672.
I0302 18:58:37.010798 22895071117440 run.py:483] Algo bellman_ford step 1896 current loss 0.606484, current_train_items 60704.
I0302 18:58:37.032501 22895071117440 run.py:483] Algo bellman_ford step 1897 current loss 0.870712, current_train_items 60736.
I0302 18:58:37.059892 22895071117440 run.py:483] Algo bellman_ford step 1898 current loss 0.889814, current_train_items 60768.
I0302 18:58:37.090525 22895071117440 run.py:483] Algo bellman_ford step 1899 current loss 1.016599, current_train_items 60800.
I0302 18:58:37.108551 22895071117440 run.py:483] Algo bellman_ford step 1900 current loss 0.437877, current_train_items 60832.
I0302 18:58:37.116255 22895071117440 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:58:37.116359 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:58:37.132313 22895071117440 run.py:483] Algo bellman_ford step 1901 current loss 0.571101, current_train_items 60864.
I0302 18:58:37.154193 22895071117440 run.py:483] Algo bellman_ford step 1902 current loss 0.604400, current_train_items 60896.
I0302 18:58:37.180471 22895071117440 run.py:483] Algo bellman_ford step 1903 current loss 0.754452, current_train_items 60928.
I0302 18:58:37.210807 22895071117440 run.py:483] Algo bellman_ford step 1904 current loss 0.983078, current_train_items 60960.
I0302 18:58:37.228827 22895071117440 run.py:483] Algo bellman_ford step 1905 current loss 0.429026, current_train_items 60992.
I0302 18:58:37.244092 22895071117440 run.py:483] Algo bellman_ford step 1906 current loss 0.580667, current_train_items 61024.
I0302 18:58:37.264975 22895071117440 run.py:483] Algo bellman_ford step 1907 current loss 0.712737, current_train_items 61056.
I0302 18:58:37.291505 22895071117440 run.py:483] Algo bellman_ford step 1908 current loss 0.810510, current_train_items 61088.
I0302 18:58:37.321074 22895071117440 run.py:483] Algo bellman_ford step 1909 current loss 0.906323, current_train_items 61120.
I0302 18:58:37.338999 22895071117440 run.py:483] Algo bellman_ford step 1910 current loss 0.333134, current_train_items 61152.
I0302 18:58:37.354575 22895071117440 run.py:483] Algo bellman_ford step 1911 current loss 0.596878, current_train_items 61184.
I0302 18:58:37.376780 22895071117440 run.py:483] Algo bellman_ford step 1912 current loss 0.786489, current_train_items 61216.
I0302 18:58:37.403729 22895071117440 run.py:483] Algo bellman_ford step 1913 current loss 0.817776, current_train_items 61248.
I0302 18:58:37.434339 22895071117440 run.py:483] Algo bellman_ford step 1914 current loss 0.915769, current_train_items 61280.
I0302 18:58:37.452135 22895071117440 run.py:483] Algo bellman_ford step 1915 current loss 0.253698, current_train_items 61312.
I0302 18:58:37.467716 22895071117440 run.py:483] Algo bellman_ford step 1916 current loss 0.540797, current_train_items 61344.
I0302 18:58:37.489597 22895071117440 run.py:483] Algo bellman_ford step 1917 current loss 0.669257, current_train_items 61376.
I0302 18:58:37.517532 22895071117440 run.py:483] Algo bellman_ford step 1918 current loss 0.839892, current_train_items 61408.
I0302 18:58:37.548853 22895071117440 run.py:483] Algo bellman_ford step 1919 current loss 1.017927, current_train_items 61440.
I0302 18:58:37.566619 22895071117440 run.py:483] Algo bellman_ford step 1920 current loss 0.442321, current_train_items 61472.
I0302 18:58:37.582257 22895071117440 run.py:483] Algo bellman_ford step 1921 current loss 0.642680, current_train_items 61504.
I0302 18:58:37.603441 22895071117440 run.py:483] Algo bellman_ford step 1922 current loss 0.710850, current_train_items 61536.
I0302 18:58:37.629732 22895071117440 run.py:483] Algo bellman_ford step 1923 current loss 0.757221, current_train_items 61568.
I0302 18:58:37.659665 22895071117440 run.py:483] Algo bellman_ford step 1924 current loss 0.964302, current_train_items 61600.
I0302 18:58:37.677367 22895071117440 run.py:483] Algo bellman_ford step 1925 current loss 0.302810, current_train_items 61632.
I0302 18:58:37.692804 22895071117440 run.py:483] Algo bellman_ford step 1926 current loss 0.595100, current_train_items 61664.
I0302 18:58:37.713520 22895071117440 run.py:483] Algo bellman_ford step 1927 current loss 0.565982, current_train_items 61696.
I0302 18:58:37.739820 22895071117440 run.py:483] Algo bellman_ford step 1928 current loss 0.824810, current_train_items 61728.
I0302 18:58:37.768217 22895071117440 run.py:483] Algo bellman_ford step 1929 current loss 1.240789, current_train_items 61760.
I0302 18:58:37.785874 22895071117440 run.py:483] Algo bellman_ford step 1930 current loss 0.402749, current_train_items 61792.
I0302 18:58:37.800872 22895071117440 run.py:483] Algo bellman_ford step 1931 current loss 0.478032, current_train_items 61824.
I0302 18:58:37.823740 22895071117440 run.py:483] Algo bellman_ford step 1932 current loss 0.887699, current_train_items 61856.
I0302 18:58:37.851017 22895071117440 run.py:483] Algo bellman_ford step 1933 current loss 1.138177, current_train_items 61888.
I0302 18:58:37.879439 22895071117440 run.py:483] Algo bellman_ford step 1934 current loss 0.944935, current_train_items 61920.
I0302 18:58:37.897173 22895071117440 run.py:483] Algo bellman_ford step 1935 current loss 0.371150, current_train_items 61952.
I0302 18:58:37.912573 22895071117440 run.py:483] Algo bellman_ford step 1936 current loss 0.516147, current_train_items 61984.
I0302 18:58:37.934810 22895071117440 run.py:483] Algo bellman_ford step 1937 current loss 0.684601, current_train_items 62016.
I0302 18:58:37.961324 22895071117440 run.py:483] Algo bellman_ford step 1938 current loss 0.805866, current_train_items 62048.
I0302 18:58:37.991798 22895071117440 run.py:483] Algo bellman_ford step 1939 current loss 1.032206, current_train_items 62080.
I0302 18:58:38.009606 22895071117440 run.py:483] Algo bellman_ford step 1940 current loss 0.334058, current_train_items 62112.
I0302 18:58:38.025068 22895071117440 run.py:483] Algo bellman_ford step 1941 current loss 0.581643, current_train_items 62144.
I0302 18:58:38.046331 22895071117440 run.py:483] Algo bellman_ford step 1942 current loss 0.803194, current_train_items 62176.
I0302 18:58:38.072409 22895071117440 run.py:483] Algo bellman_ford step 1943 current loss 0.771786, current_train_items 62208.
I0302 18:58:38.102448 22895071117440 run.py:483] Algo bellman_ford step 1944 current loss 1.032886, current_train_items 62240.
I0302 18:58:38.120126 22895071117440 run.py:483] Algo bellman_ford step 1945 current loss 0.388119, current_train_items 62272.
I0302 18:58:38.135545 22895071117440 run.py:483] Algo bellman_ford step 1946 current loss 0.591170, current_train_items 62304.
I0302 18:58:38.157556 22895071117440 run.py:483] Algo bellman_ford step 1947 current loss 0.674160, current_train_items 62336.
I0302 18:58:38.185490 22895071117440 run.py:483] Algo bellman_ford step 1948 current loss 0.950736, current_train_items 62368.
I0302 18:58:38.214622 22895071117440 run.py:483] Algo bellman_ford step 1949 current loss 0.849256, current_train_items 62400.
I0302 18:58:38.232170 22895071117440 run.py:483] Algo bellman_ford step 1950 current loss 0.376218, current_train_items 62432.
I0302 18:58:38.240200 22895071117440 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:58:38.240306 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:58:38.256544 22895071117440 run.py:483] Algo bellman_ford step 1951 current loss 0.656355, current_train_items 62464.
I0302 18:58:38.278650 22895071117440 run.py:483] Algo bellman_ford step 1952 current loss 0.769789, current_train_items 62496.
I0302 18:58:38.308022 22895071117440 run.py:483] Algo bellman_ford step 1953 current loss 0.892468, current_train_items 62528.
I0302 18:58:38.338177 22895071117440 run.py:483] Algo bellman_ford step 1954 current loss 0.907753, current_train_items 62560.
I0302 18:58:38.356428 22895071117440 run.py:483] Algo bellman_ford step 1955 current loss 0.370994, current_train_items 62592.
I0302 18:58:38.371037 22895071117440 run.py:483] Algo bellman_ford step 1956 current loss 0.513175, current_train_items 62624.
I0302 18:58:38.393157 22895071117440 run.py:483] Algo bellman_ford step 1957 current loss 0.806598, current_train_items 62656.
I0302 18:58:38.421124 22895071117440 run.py:483] Algo bellman_ford step 1958 current loss 0.942220, current_train_items 62688.
I0302 18:58:38.451828 22895071117440 run.py:483] Algo bellman_ford step 1959 current loss 0.858519, current_train_items 62720.
I0302 18:58:38.469800 22895071117440 run.py:483] Algo bellman_ford step 1960 current loss 0.391490, current_train_items 62752.
I0302 18:58:38.485453 22895071117440 run.py:483] Algo bellman_ford step 1961 current loss 0.609935, current_train_items 62784.
I0302 18:58:38.506694 22895071117440 run.py:483] Algo bellman_ford step 1962 current loss 0.768637, current_train_items 62816.
I0302 18:58:38.533024 22895071117440 run.py:483] Algo bellman_ford step 1963 current loss 0.844962, current_train_items 62848.
I0302 18:58:38.565605 22895071117440 run.py:483] Algo bellman_ford step 1964 current loss 1.048619, current_train_items 62880.
I0302 18:58:38.583546 22895071117440 run.py:483] Algo bellman_ford step 1965 current loss 0.356853, current_train_items 62912.
I0302 18:58:38.598745 22895071117440 run.py:483] Algo bellman_ford step 1966 current loss 0.532985, current_train_items 62944.
I0302 18:58:38.620208 22895071117440 run.py:483] Algo bellman_ford step 1967 current loss 0.852713, current_train_items 62976.
I0302 18:58:38.646914 22895071117440 run.py:483] Algo bellman_ford step 1968 current loss 0.875350, current_train_items 63008.
I0302 18:58:38.676684 22895071117440 run.py:483] Algo bellman_ford step 1969 current loss 0.992440, current_train_items 63040.
I0302 18:58:38.694890 22895071117440 run.py:483] Algo bellman_ford step 1970 current loss 0.315892, current_train_items 63072.
I0302 18:58:38.710372 22895071117440 run.py:483] Algo bellman_ford step 1971 current loss 0.541738, current_train_items 63104.
I0302 18:58:38.731142 22895071117440 run.py:483] Algo bellman_ford step 1972 current loss 0.692933, current_train_items 63136.
I0302 18:58:38.758009 22895071117440 run.py:483] Algo bellman_ford step 1973 current loss 0.962614, current_train_items 63168.
I0302 18:58:38.790965 22895071117440 run.py:483] Algo bellman_ford step 1974 current loss 1.253005, current_train_items 63200.
I0302 18:58:38.808881 22895071117440 run.py:483] Algo bellman_ford step 1975 current loss 0.334121, current_train_items 63232.
I0302 18:58:38.824099 22895071117440 run.py:483] Algo bellman_ford step 1976 current loss 0.503041, current_train_items 63264.
I0302 18:58:38.844304 22895071117440 run.py:483] Algo bellman_ford step 1977 current loss 0.649086, current_train_items 63296.
I0302 18:58:38.870892 22895071117440 run.py:483] Algo bellman_ford step 1978 current loss 0.906737, current_train_items 63328.
I0302 18:58:38.901223 22895071117440 run.py:483] Algo bellman_ford step 1979 current loss 1.121548, current_train_items 63360.
I0302 18:58:38.918668 22895071117440 run.py:483] Algo bellman_ford step 1980 current loss 0.384749, current_train_items 63392.
I0302 18:58:38.934117 22895071117440 run.py:483] Algo bellman_ford step 1981 current loss 0.662762, current_train_items 63424.
I0302 18:58:38.955257 22895071117440 run.py:483] Algo bellman_ford step 1982 current loss 0.798239, current_train_items 63456.
I0302 18:58:38.981555 22895071117440 run.py:483] Algo bellman_ford step 1983 current loss 0.719910, current_train_items 63488.
I0302 18:58:39.014201 22895071117440 run.py:483] Algo bellman_ford step 1984 current loss 1.022946, current_train_items 63520.
I0302 18:58:39.032160 22895071117440 run.py:483] Algo bellman_ford step 1985 current loss 0.279752, current_train_items 63552.
I0302 18:58:39.047697 22895071117440 run.py:483] Algo bellman_ford step 1986 current loss 0.593713, current_train_items 63584.
I0302 18:58:39.067787 22895071117440 run.py:483] Algo bellman_ford step 1987 current loss 0.777096, current_train_items 63616.
I0302 18:58:39.094045 22895071117440 run.py:483] Algo bellman_ford step 1988 current loss 0.863674, current_train_items 63648.
I0302 18:58:39.124853 22895071117440 run.py:483] Algo bellman_ford step 1989 current loss 1.141749, current_train_items 63680.
I0302 18:58:39.142559 22895071117440 run.py:483] Algo bellman_ford step 1990 current loss 0.418287, current_train_items 63712.
I0302 18:58:39.158135 22895071117440 run.py:483] Algo bellman_ford step 1991 current loss 0.594310, current_train_items 63744.
I0302 18:58:39.179405 22895071117440 run.py:483] Algo bellman_ford step 1992 current loss 0.700609, current_train_items 63776.
I0302 18:58:39.206304 22895071117440 run.py:483] Algo bellman_ford step 1993 current loss 0.808696, current_train_items 63808.
I0302 18:58:39.232486 22895071117440 run.py:483] Algo bellman_ford step 1994 current loss 0.712171, current_train_items 63840.
I0302 18:58:39.250001 22895071117440 run.py:483] Algo bellman_ford step 1995 current loss 0.357149, current_train_items 63872.
I0302 18:58:39.265893 22895071117440 run.py:483] Algo bellman_ford step 1996 current loss 0.661643, current_train_items 63904.
I0302 18:58:39.286786 22895071117440 run.py:483] Algo bellman_ford step 1997 current loss 0.737063, current_train_items 63936.
I0302 18:58:39.314949 22895071117440 run.py:483] Algo bellman_ford step 1998 current loss 0.838734, current_train_items 63968.
I0302 18:58:39.344099 22895071117440 run.py:483] Algo bellman_ford step 1999 current loss 0.933969, current_train_items 64000.
I0302 18:58:39.361912 22895071117440 run.py:483] Algo bellman_ford step 2000 current loss 0.274949, current_train_items 64032.
I0302 18:58:39.369917 22895071117440 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.875, 'score': 0.875, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:58:39.370024 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.923, current avg val score is 0.875, val scores are: bellman_ford: 0.875
I0302 18:58:39.385918 22895071117440 run.py:483] Algo bellman_ford step 2001 current loss 0.469723, current_train_items 64064.
I0302 18:58:39.408552 22895071117440 run.py:483] Algo bellman_ford step 2002 current loss 0.946082, current_train_items 64096.
I0302 18:58:39.436655 22895071117440 run.py:483] Algo bellman_ford step 2003 current loss 0.801201, current_train_items 64128.
I0302 18:58:39.466434 22895071117440 run.py:483] Algo bellman_ford step 2004 current loss 1.102843, current_train_items 64160.
I0302 18:58:39.484605 22895071117440 run.py:483] Algo bellman_ford step 2005 current loss 0.427785, current_train_items 64192.
I0302 18:58:39.499989 22895071117440 run.py:483] Algo bellman_ford step 2006 current loss 0.603315, current_train_items 64224.
I0302 18:58:39.521719 22895071117440 run.py:483] Algo bellman_ford step 2007 current loss 0.724529, current_train_items 64256.
I0302 18:58:39.549992 22895071117440 run.py:483] Algo bellman_ford step 2008 current loss 0.862715, current_train_items 64288.
I0302 18:58:39.579717 22895071117440 run.py:483] Algo bellman_ford step 2009 current loss 0.921506, current_train_items 64320.
I0302 18:58:39.597701 22895071117440 run.py:483] Algo bellman_ford step 2010 current loss 0.384812, current_train_items 64352.
I0302 18:58:39.613200 22895071117440 run.py:483] Algo bellman_ford step 2011 current loss 0.505730, current_train_items 64384.
I0302 18:58:39.634945 22895071117440 run.py:483] Algo bellman_ford step 2012 current loss 0.781810, current_train_items 64416.
I0302 18:58:39.661610 22895071117440 run.py:483] Algo bellman_ford step 2013 current loss 0.892514, current_train_items 64448.
I0302 18:58:39.691260 22895071117440 run.py:483] Algo bellman_ford step 2014 current loss 0.993723, current_train_items 64480.
I0302 18:58:39.708682 22895071117440 run.py:483] Algo bellman_ford step 2015 current loss 0.321648, current_train_items 64512.
I0302 18:58:39.724502 22895071117440 run.py:483] Algo bellman_ford step 2016 current loss 0.618539, current_train_items 64544.
I0302 18:58:39.746375 22895071117440 run.py:483] Algo bellman_ford step 2017 current loss 0.869667, current_train_items 64576.
I0302 18:58:39.771688 22895071117440 run.py:483] Algo bellman_ford step 2018 current loss 0.762517, current_train_items 64608.
I0302 18:58:39.802565 22895071117440 run.py:483] Algo bellman_ford step 2019 current loss 1.015665, current_train_items 64640.
I0302 18:58:39.820057 22895071117440 run.py:483] Algo bellman_ford step 2020 current loss 0.363732, current_train_items 64672.
I0302 18:58:39.835321 22895071117440 run.py:483] Algo bellman_ford step 2021 current loss 0.549772, current_train_items 64704.
I0302 18:58:39.857147 22895071117440 run.py:483] Algo bellman_ford step 2022 current loss 0.866197, current_train_items 64736.
I0302 18:58:39.884369 22895071117440 run.py:483] Algo bellman_ford step 2023 current loss 0.756657, current_train_items 64768.
I0302 18:58:39.917432 22895071117440 run.py:483] Algo bellman_ford step 2024 current loss 1.008813, current_train_items 64800.
I0302 18:58:39.935448 22895071117440 run.py:483] Algo bellman_ford step 2025 current loss 0.414496, current_train_items 64832.
I0302 18:58:39.950618 22895071117440 run.py:483] Algo bellman_ford step 2026 current loss 0.488495, current_train_items 64864.
I0302 18:58:39.972323 22895071117440 run.py:483] Algo bellman_ford step 2027 current loss 0.799046, current_train_items 64896.
I0302 18:58:39.999584 22895071117440 run.py:483] Algo bellman_ford step 2028 current loss 0.866962, current_train_items 64928.
I0302 18:58:40.029355 22895071117440 run.py:483] Algo bellman_ford step 2029 current loss 0.946112, current_train_items 64960.
I0302 18:58:40.047129 22895071117440 run.py:483] Algo bellman_ford step 2030 current loss 0.396663, current_train_items 64992.
I0302 18:58:40.062807 22895071117440 run.py:483] Algo bellman_ford step 2031 current loss 0.624512, current_train_items 65024.
I0302 18:58:40.083727 22895071117440 run.py:483] Algo bellman_ford step 2032 current loss 0.682804, current_train_items 65056.
I0302 18:58:40.111961 22895071117440 run.py:483] Algo bellman_ford step 2033 current loss 0.900562, current_train_items 65088.
I0302 18:58:40.141006 22895071117440 run.py:483] Algo bellman_ford step 2034 current loss 0.918158, current_train_items 65120.
I0302 18:58:40.158434 22895071117440 run.py:483] Algo bellman_ford step 2035 current loss 0.344187, current_train_items 65152.
I0302 18:58:40.173473 22895071117440 run.py:483] Algo bellman_ford step 2036 current loss 0.569624, current_train_items 65184.
I0302 18:58:40.194313 22895071117440 run.py:483] Algo bellman_ford step 2037 current loss 0.716121, current_train_items 65216.
I0302 18:58:40.221472 22895071117440 run.py:483] Algo bellman_ford step 2038 current loss 0.881343, current_train_items 65248.
I0302 18:58:40.251848 22895071117440 run.py:483] Algo bellman_ford step 2039 current loss 0.972142, current_train_items 65280.
I0302 18:58:40.269915 22895071117440 run.py:483] Algo bellman_ford step 2040 current loss 0.376677, current_train_items 65312.
I0302 18:58:40.285255 22895071117440 run.py:483] Algo bellman_ford step 2041 current loss 0.597504, current_train_items 65344.
I0302 18:58:40.307526 22895071117440 run.py:483] Algo bellman_ford step 2042 current loss 0.886803, current_train_items 65376.
I0302 18:58:40.334834 22895071117440 run.py:483] Algo bellman_ford step 2043 current loss 0.788192, current_train_items 65408.
I0302 18:58:40.359508 22895071117440 run.py:483] Algo bellman_ford step 2044 current loss 0.812841, current_train_items 65440.
I0302 18:58:40.377304 22895071117440 run.py:483] Algo bellman_ford step 2045 current loss 0.302327, current_train_items 65472.
I0302 18:58:40.393345 22895071117440 run.py:483] Algo bellman_ford step 2046 current loss 0.821097, current_train_items 65504.
I0302 18:58:40.414828 22895071117440 run.py:483] Algo bellman_ford step 2047 current loss 0.979753, current_train_items 65536.
I0302 18:58:40.441926 22895071117440 run.py:483] Algo bellman_ford step 2048 current loss 0.744644, current_train_items 65568.
I0302 18:58:40.469785 22895071117440 run.py:483] Algo bellman_ford step 2049 current loss 0.934828, current_train_items 65600.
I0302 18:58:40.487537 22895071117440 run.py:483] Algo bellman_ford step 2050 current loss 0.373387, current_train_items 65632.
I0302 18:58:40.495540 22895071117440 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:58:40.495645 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.923, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 18:58:40.523983 22895071117440 run.py:483] Algo bellman_ford step 2051 current loss 0.606762, current_train_items 65664.
I0302 18:58:40.545207 22895071117440 run.py:483] Algo bellman_ford step 2052 current loss 0.655462, current_train_items 65696.
I0302 18:58:40.572261 22895071117440 run.py:483] Algo bellman_ford step 2053 current loss 0.749319, current_train_items 65728.
I0302 18:58:40.605485 22895071117440 run.py:483] Algo bellman_ford step 2054 current loss 1.117460, current_train_items 65760.
I0302 18:58:40.623547 22895071117440 run.py:483] Algo bellman_ford step 2055 current loss 0.343304, current_train_items 65792.
I0302 18:58:40.638973 22895071117440 run.py:483] Algo bellman_ford step 2056 current loss 0.555658, current_train_items 65824.
I0302 18:58:40.659759 22895071117440 run.py:483] Algo bellman_ford step 2057 current loss 0.762888, current_train_items 65856.
I0302 18:58:40.686800 22895071117440 run.py:483] Algo bellman_ford step 2058 current loss 0.728935, current_train_items 65888.
I0302 18:58:40.717175 22895071117440 run.py:483] Algo bellman_ford step 2059 current loss 0.994042, current_train_items 65920.
I0302 18:58:40.735476 22895071117440 run.py:483] Algo bellman_ford step 2060 current loss 0.401535, current_train_items 65952.
I0302 18:58:40.751258 22895071117440 run.py:483] Algo bellman_ford step 2061 current loss 0.568424, current_train_items 65984.
I0302 18:58:40.772413 22895071117440 run.py:483] Algo bellman_ford step 2062 current loss 0.826800, current_train_items 66016.
I0302 18:58:40.799951 22895071117440 run.py:483] Algo bellman_ford step 2063 current loss 0.895156, current_train_items 66048.
I0302 18:58:40.831697 22895071117440 run.py:483] Algo bellman_ford step 2064 current loss 1.070196, current_train_items 66080.
I0302 18:58:40.849267 22895071117440 run.py:483] Algo bellman_ford step 2065 current loss 0.335019, current_train_items 66112.
I0302 18:58:40.864570 22895071117440 run.py:483] Algo bellman_ford step 2066 current loss 0.576239, current_train_items 66144.
I0302 18:58:40.886402 22895071117440 run.py:483] Algo bellman_ford step 2067 current loss 0.750562, current_train_items 66176.
I0302 18:58:40.913944 22895071117440 run.py:483] Algo bellman_ford step 2068 current loss 0.829173, current_train_items 66208.
I0302 18:58:40.943595 22895071117440 run.py:483] Algo bellman_ford step 2069 current loss 1.053852, current_train_items 66240.
I0302 18:58:40.961511 22895071117440 run.py:483] Algo bellman_ford step 2070 current loss 0.378282, current_train_items 66272.
I0302 18:58:40.977330 22895071117440 run.py:483] Algo bellman_ford step 2071 current loss 0.637940, current_train_items 66304.
I0302 18:58:40.998724 22895071117440 run.py:483] Algo bellman_ford step 2072 current loss 0.752556, current_train_items 66336.
I0302 18:58:41.026399 22895071117440 run.py:483] Algo bellman_ford step 2073 current loss 0.974715, current_train_items 66368.
I0302 18:58:41.056534 22895071117440 run.py:483] Algo bellman_ford step 2074 current loss 0.979630, current_train_items 66400.
I0302 18:58:41.074283 22895071117440 run.py:483] Algo bellman_ford step 2075 current loss 0.404766, current_train_items 66432.
I0302 18:58:41.090401 22895071117440 run.py:483] Algo bellman_ford step 2076 current loss 0.689032, current_train_items 66464.
I0302 18:58:41.112079 22895071117440 run.py:483] Algo bellman_ford step 2077 current loss 0.848875, current_train_items 66496.
I0302 18:58:41.137300 22895071117440 run.py:483] Algo bellman_ford step 2078 current loss 0.870471, current_train_items 66528.
I0302 18:58:41.166574 22895071117440 run.py:483] Algo bellman_ford step 2079 current loss 1.096060, current_train_items 66560.
I0302 18:58:41.184105 22895071117440 run.py:483] Algo bellman_ford step 2080 current loss 0.327635, current_train_items 66592.
I0302 18:58:41.199454 22895071117440 run.py:483] Algo bellman_ford step 2081 current loss 0.549903, current_train_items 66624.
I0302 18:58:41.221008 22895071117440 run.py:483] Algo bellman_ford step 2082 current loss 0.766595, current_train_items 66656.
I0302 18:58:41.249509 22895071117440 run.py:483] Algo bellman_ford step 2083 current loss 0.870252, current_train_items 66688.
I0302 18:58:41.279978 22895071117440 run.py:483] Algo bellman_ford step 2084 current loss 1.070649, current_train_items 66720.
I0302 18:58:41.298315 22895071117440 run.py:483] Algo bellman_ford step 2085 current loss 0.399741, current_train_items 66752.
I0302 18:58:41.313907 22895071117440 run.py:483] Algo bellman_ford step 2086 current loss 0.690160, current_train_items 66784.
I0302 18:58:41.335363 22895071117440 run.py:483] Algo bellman_ford step 2087 current loss 0.862012, current_train_items 66816.
I0302 18:58:41.360348 22895071117440 run.py:483] Algo bellman_ford step 2088 current loss 0.691830, current_train_items 66848.
I0302 18:58:41.390357 22895071117440 run.py:483] Algo bellman_ford step 2089 current loss 0.985885, current_train_items 66880.
I0302 18:58:41.408246 22895071117440 run.py:483] Algo bellman_ford step 2090 current loss 0.332107, current_train_items 66912.
I0302 18:58:41.423919 22895071117440 run.py:483] Algo bellman_ford step 2091 current loss 0.591295, current_train_items 66944.
I0302 18:58:41.444806 22895071117440 run.py:483] Algo bellman_ford step 2092 current loss 0.793759, current_train_items 66976.
I0302 18:58:41.471526 22895071117440 run.py:483] Algo bellman_ford step 2093 current loss 0.830129, current_train_items 67008.
I0302 18:58:41.501377 22895071117440 run.py:483] Algo bellman_ford step 2094 current loss 1.159953, current_train_items 67040.
I0302 18:58:41.519002 22895071117440 run.py:483] Algo bellman_ford step 2095 current loss 0.351944, current_train_items 67072.
I0302 18:58:41.534335 22895071117440 run.py:483] Algo bellman_ford step 2096 current loss 0.574489, current_train_items 67104.
I0302 18:58:41.555388 22895071117440 run.py:483] Algo bellman_ford step 2097 current loss 0.839622, current_train_items 67136.
I0302 18:58:41.581848 22895071117440 run.py:483] Algo bellman_ford step 2098 current loss 0.752740, current_train_items 67168.
I0302 18:58:41.611380 22895071117440 run.py:483] Algo bellman_ford step 2099 current loss 0.938595, current_train_items 67200.
I0302 18:58:41.629462 22895071117440 run.py:483] Algo bellman_ford step 2100 current loss 0.356407, current_train_items 67232.
I0302 18:58:41.637196 22895071117440 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:58:41.637301 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:58:41.653058 22895071117440 run.py:483] Algo bellman_ford step 2101 current loss 0.508562, current_train_items 67264.
I0302 18:58:41.674849 22895071117440 run.py:483] Algo bellman_ford step 2102 current loss 0.806712, current_train_items 67296.
I0302 18:58:41.702784 22895071117440 run.py:483] Algo bellman_ford step 2103 current loss 0.869588, current_train_items 67328.
I0302 18:58:41.732408 22895071117440 run.py:483] Algo bellman_ford step 2104 current loss 0.928647, current_train_items 67360.
I0302 18:58:41.750499 22895071117440 run.py:483] Algo bellman_ford step 2105 current loss 0.377955, current_train_items 67392.
I0302 18:58:41.765075 22895071117440 run.py:483] Algo bellman_ford step 2106 current loss 0.448598, current_train_items 67424.
I0302 18:58:41.787100 22895071117440 run.py:483] Algo bellman_ford step 2107 current loss 0.794698, current_train_items 67456.
I0302 18:58:41.815695 22895071117440 run.py:483] Algo bellman_ford step 2108 current loss 0.977106, current_train_items 67488.
I0302 18:58:41.844271 22895071117440 run.py:483] Algo bellman_ford step 2109 current loss 0.860590, current_train_items 67520.
I0302 18:58:41.861866 22895071117440 run.py:483] Algo bellman_ford step 2110 current loss 0.406075, current_train_items 67552.
I0302 18:58:41.877203 22895071117440 run.py:483] Algo bellman_ford step 2111 current loss 0.649194, current_train_items 67584.
I0302 18:58:41.898078 22895071117440 run.py:483] Algo bellman_ford step 2112 current loss 0.889117, current_train_items 67616.
I0302 18:58:41.924596 22895071117440 run.py:483] Algo bellman_ford step 2113 current loss 0.818291, current_train_items 67648.
I0302 18:58:41.953146 22895071117440 run.py:483] Algo bellman_ford step 2114 current loss 0.986098, current_train_items 67680.
I0302 18:58:41.970919 22895071117440 run.py:483] Algo bellman_ford step 2115 current loss 0.400066, current_train_items 67712.
I0302 18:58:41.986217 22895071117440 run.py:483] Algo bellman_ford step 2116 current loss 0.566770, current_train_items 67744.
I0302 18:58:42.008396 22895071117440 run.py:483] Algo bellman_ford step 2117 current loss 0.827656, current_train_items 67776.
I0302 18:58:42.036399 22895071117440 run.py:483] Algo bellman_ford step 2118 current loss 0.978075, current_train_items 67808.
I0302 18:58:42.065463 22895071117440 run.py:483] Algo bellman_ford step 2119 current loss 0.921055, current_train_items 67840.
I0302 18:58:42.083208 22895071117440 run.py:483] Algo bellman_ford step 2120 current loss 0.342570, current_train_items 67872.
I0302 18:58:42.098340 22895071117440 run.py:483] Algo bellman_ford step 2121 current loss 0.560483, current_train_items 67904.
I0302 18:58:42.119774 22895071117440 run.py:483] Algo bellman_ford step 2122 current loss 0.662110, current_train_items 67936.
I0302 18:58:42.147346 22895071117440 run.py:483] Algo bellman_ford step 2123 current loss 0.775828, current_train_items 67968.
I0302 18:58:42.173736 22895071117440 run.py:483] Algo bellman_ford step 2124 current loss 0.853429, current_train_items 68000.
I0302 18:58:42.191370 22895071117440 run.py:483] Algo bellman_ford step 2125 current loss 0.293550, current_train_items 68032.
I0302 18:58:42.207094 22895071117440 run.py:483] Algo bellman_ford step 2126 current loss 0.606027, current_train_items 68064.
I0302 18:58:42.230258 22895071117440 run.py:483] Algo bellman_ford step 2127 current loss 0.894988, current_train_items 68096.
I0302 18:58:42.256868 22895071117440 run.py:483] Algo bellman_ford step 2128 current loss 0.923898, current_train_items 68128.
I0302 18:58:42.286279 22895071117440 run.py:483] Algo bellman_ford step 2129 current loss 0.917389, current_train_items 68160.
I0302 18:58:42.303763 22895071117440 run.py:483] Algo bellman_ford step 2130 current loss 0.348618, current_train_items 68192.
I0302 18:58:42.319654 22895071117440 run.py:483] Algo bellman_ford step 2131 current loss 0.612542, current_train_items 68224.
I0302 18:58:42.341512 22895071117440 run.py:483] Algo bellman_ford step 2132 current loss 0.735894, current_train_items 68256.
I0302 18:58:42.368126 22895071117440 run.py:483] Algo bellman_ford step 2133 current loss 0.857896, current_train_items 68288.
I0302 18:58:42.399330 22895071117440 run.py:483] Algo bellman_ford step 2134 current loss 0.995568, current_train_items 68320.
I0302 18:58:42.417033 22895071117440 run.py:483] Algo bellman_ford step 2135 current loss 0.363180, current_train_items 68352.
I0302 18:58:42.432549 22895071117440 run.py:483] Algo bellman_ford step 2136 current loss 0.544181, current_train_items 68384.
I0302 18:58:42.453955 22895071117440 run.py:483] Algo bellman_ford step 2137 current loss 0.810188, current_train_items 68416.
I0302 18:58:42.481656 22895071117440 run.py:483] Algo bellman_ford step 2138 current loss 0.939573, current_train_items 68448.
I0302 18:58:42.514476 22895071117440 run.py:483] Algo bellman_ford step 2139 current loss 1.119923, current_train_items 68480.
I0302 18:58:42.531998 22895071117440 run.py:483] Algo bellman_ford step 2140 current loss 0.382490, current_train_items 68512.
I0302 18:58:42.547206 22895071117440 run.py:483] Algo bellman_ford step 2141 current loss 0.495451, current_train_items 68544.
I0302 18:58:42.567180 22895071117440 run.py:483] Algo bellman_ford step 2142 current loss 0.659087, current_train_items 68576.
I0302 18:58:42.594350 22895071117440 run.py:483] Algo bellman_ford step 2143 current loss 0.798665, current_train_items 68608.
I0302 18:58:42.624543 22895071117440 run.py:483] Algo bellman_ford step 2144 current loss 1.051224, current_train_items 68640.
I0302 18:58:42.642181 22895071117440 run.py:483] Algo bellman_ford step 2145 current loss 0.274218, current_train_items 68672.
I0302 18:58:42.657927 22895071117440 run.py:483] Algo bellman_ford step 2146 current loss 0.651745, current_train_items 68704.
I0302 18:58:42.680233 22895071117440 run.py:483] Algo bellman_ford step 2147 current loss 0.996030, current_train_items 68736.
I0302 18:58:42.707614 22895071117440 run.py:483] Algo bellman_ford step 2148 current loss 0.893423, current_train_items 68768.
I0302 18:58:42.736884 22895071117440 run.py:483] Algo bellman_ford step 2149 current loss 0.939359, current_train_items 68800.
I0302 18:58:42.754400 22895071117440 run.py:483] Algo bellman_ford step 2150 current loss 0.409781, current_train_items 68832.
I0302 18:58:42.762339 22895071117440 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:58:42.762444 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 18:58:42.778503 22895071117440 run.py:483] Algo bellman_ford step 2151 current loss 0.531618, current_train_items 68864.
I0302 18:58:42.800106 22895071117440 run.py:483] Algo bellman_ford step 2152 current loss 0.726638, current_train_items 68896.
I0302 18:58:42.827152 22895071117440 run.py:483] Algo bellman_ford step 2153 current loss 0.861114, current_train_items 68928.
I0302 18:58:42.856642 22895071117440 run.py:483] Algo bellman_ford step 2154 current loss 1.025839, current_train_items 68960.
I0302 18:58:42.874525 22895071117440 run.py:483] Algo bellman_ford step 2155 current loss 0.381405, current_train_items 68992.
I0302 18:58:42.889742 22895071117440 run.py:483] Algo bellman_ford step 2156 current loss 0.484231, current_train_items 69024.
I0302 18:58:42.911482 22895071117440 run.py:483] Algo bellman_ford step 2157 current loss 0.726439, current_train_items 69056.
I0302 18:58:42.938761 22895071117440 run.py:483] Algo bellman_ford step 2158 current loss 0.830249, current_train_items 69088.
I0302 18:58:42.968621 22895071117440 run.py:483] Algo bellman_ford step 2159 current loss 1.001967, current_train_items 69120.
I0302 18:58:42.986669 22895071117440 run.py:483] Algo bellman_ford step 2160 current loss 0.402085, current_train_items 69152.
I0302 18:58:43.002040 22895071117440 run.py:483] Algo bellman_ford step 2161 current loss 0.501561, current_train_items 69184.
I0302 18:58:43.023415 22895071117440 run.py:483] Algo bellman_ford step 2162 current loss 0.777296, current_train_items 69216.
I0302 18:58:43.049112 22895071117440 run.py:483] Algo bellman_ford step 2163 current loss 0.782233, current_train_items 69248.
I0302 18:58:43.079737 22895071117440 run.py:483] Algo bellman_ford step 2164 current loss 1.088606, current_train_items 69280.
I0302 18:58:43.097190 22895071117440 run.py:483] Algo bellman_ford step 2165 current loss 0.345132, current_train_items 69312.
I0302 18:58:43.112962 22895071117440 run.py:483] Algo bellman_ford step 2166 current loss 0.612443, current_train_items 69344.
I0302 18:58:43.133614 22895071117440 run.py:483] Algo bellman_ford step 2167 current loss 0.684256, current_train_items 69376.
I0302 18:58:43.159388 22895071117440 run.py:483] Algo bellman_ford step 2168 current loss 0.821674, current_train_items 69408.
I0302 18:58:43.189283 22895071117440 run.py:483] Algo bellman_ford step 2169 current loss 1.095106, current_train_items 69440.
I0302 18:58:43.207030 22895071117440 run.py:483] Algo bellman_ford step 2170 current loss 0.300502, current_train_items 69472.
I0302 18:58:43.222420 22895071117440 run.py:483] Algo bellman_ford step 2171 current loss 0.601347, current_train_items 69504.
I0302 18:58:43.243415 22895071117440 run.py:483] Algo bellman_ford step 2172 current loss 0.706160, current_train_items 69536.
I0302 18:58:43.271349 22895071117440 run.py:483] Algo bellman_ford step 2173 current loss 0.859138, current_train_items 69568.
I0302 18:58:43.299378 22895071117440 run.py:483] Algo bellman_ford step 2174 current loss 0.924961, current_train_items 69600.
I0302 18:58:43.317332 22895071117440 run.py:483] Algo bellman_ford step 2175 current loss 0.377250, current_train_items 69632.
I0302 18:58:43.332456 22895071117440 run.py:483] Algo bellman_ford step 2176 current loss 0.521247, current_train_items 69664.
I0302 18:58:43.354260 22895071117440 run.py:483] Algo bellman_ford step 2177 current loss 0.879394, current_train_items 69696.
I0302 18:58:43.380308 22895071117440 run.py:483] Algo bellman_ford step 2178 current loss 0.775082, current_train_items 69728.
I0302 18:58:43.410693 22895071117440 run.py:483] Algo bellman_ford step 2179 current loss 0.875412, current_train_items 69760.
I0302 18:58:43.427944 22895071117440 run.py:483] Algo bellman_ford step 2180 current loss 0.285829, current_train_items 69792.
I0302 18:58:43.443229 22895071117440 run.py:483] Algo bellman_ford step 2181 current loss 0.529494, current_train_items 69824.
I0302 18:58:43.464333 22895071117440 run.py:483] Algo bellman_ford step 2182 current loss 0.760620, current_train_items 69856.
I0302 18:58:43.490510 22895071117440 run.py:483] Algo bellman_ford step 2183 current loss 0.779083, current_train_items 69888.
I0302 18:58:43.521394 22895071117440 run.py:483] Algo bellman_ford step 2184 current loss 1.087680, current_train_items 69920.
I0302 18:58:43.539422 22895071117440 run.py:483] Algo bellman_ford step 2185 current loss 0.307916, current_train_items 69952.
I0302 18:58:43.554496 22895071117440 run.py:483] Algo bellman_ford step 2186 current loss 0.441196, current_train_items 69984.
I0302 18:58:43.575463 22895071117440 run.py:483] Algo bellman_ford step 2187 current loss 0.745091, current_train_items 70016.
I0302 18:58:43.602466 22895071117440 run.py:483] Algo bellman_ford step 2188 current loss 0.829806, current_train_items 70048.
W0302 18:58:43.626005 22895071117440 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:58:49.032309 22895071117440 run.py:483] Algo bellman_ford step 2189 current loss 1.413050, current_train_items 70080.
I0302 18:58:49.051697 22895071117440 run.py:483] Algo bellman_ford step 2190 current loss 0.324196, current_train_items 70112.
I0302 18:58:49.067884 22895071117440 run.py:483] Algo bellman_ford step 2191 current loss 0.491664, current_train_items 70144.
I0302 18:58:49.089522 22895071117440 run.py:483] Algo bellman_ford step 2192 current loss 0.699877, current_train_items 70176.
W0302 18:58:49.109880 22895071117440 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:58:54.965295 22895071117440 run.py:483] Algo bellman_ford step 2193 current loss 1.062494, current_train_items 70208.
I0302 18:58:54.994530 22895071117440 run.py:483] Algo bellman_ford step 2194 current loss 0.933393, current_train_items 70240.
I0302 18:58:55.013175 22895071117440 run.py:483] Algo bellman_ford step 2195 current loss 0.445632, current_train_items 70272.
I0302 18:58:55.028883 22895071117440 run.py:483] Algo bellman_ford step 2196 current loss 0.733031, current_train_items 70304.
I0302 18:58:55.050232 22895071117440 run.py:483] Algo bellman_ford step 2197 current loss 0.697631, current_train_items 70336.
I0302 18:58:55.076516 22895071117440 run.py:483] Algo bellman_ford step 2198 current loss 0.884400, current_train_items 70368.
I0302 18:58:55.106643 22895071117440 run.py:483] Algo bellman_ford step 2199 current loss 0.992589, current_train_items 70400.
I0302 18:58:55.125181 22895071117440 run.py:483] Algo bellman_ford step 2200 current loss 0.363932, current_train_items 70432.
I0302 18:58:55.134297 22895071117440 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.134426 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:58:55.150673 22895071117440 run.py:483] Algo bellman_ford step 2201 current loss 0.595978, current_train_items 70464.
I0302 18:58:55.171940 22895071117440 run.py:483] Algo bellman_ford step 2202 current loss 0.705123, current_train_items 70496.
I0302 18:58:55.199337 22895071117440 run.py:483] Algo bellman_ford step 2203 current loss 0.826198, current_train_items 70528.
I0302 18:58:55.230332 22895071117440 run.py:483] Algo bellman_ford step 2204 current loss 0.837575, current_train_items 70560.
I0302 18:58:55.248798 22895071117440 run.py:483] Algo bellman_ford step 2205 current loss 0.260681, current_train_items 70592.
I0302 18:58:55.264433 22895071117440 run.py:483] Algo bellman_ford step 2206 current loss 0.513341, current_train_items 70624.
I0302 18:58:55.285556 22895071117440 run.py:483] Algo bellman_ford step 2207 current loss 0.741736, current_train_items 70656.
I0302 18:58:55.313240 22895071117440 run.py:483] Algo bellman_ford step 2208 current loss 0.877259, current_train_items 70688.
I0302 18:58:55.344505 22895071117440 run.py:483] Algo bellman_ford step 2209 current loss 0.972760, current_train_items 70720.
I0302 18:58:55.362429 22895071117440 run.py:483] Algo bellman_ford step 2210 current loss 0.380435, current_train_items 70752.
I0302 18:58:55.377804 22895071117440 run.py:483] Algo bellman_ford step 2211 current loss 0.552903, current_train_items 70784.
I0302 18:58:55.399702 22895071117440 run.py:483] Algo bellman_ford step 2212 current loss 0.780296, current_train_items 70816.
I0302 18:58:55.427596 22895071117440 run.py:483] Algo bellman_ford step 2213 current loss 0.870962, current_train_items 70848.
I0302 18:58:55.458851 22895071117440 run.py:483] Algo bellman_ford step 2214 current loss 0.935349, current_train_items 70880.
I0302 18:58:55.476881 22895071117440 run.py:483] Algo bellman_ford step 2215 current loss 0.424707, current_train_items 70912.
I0302 18:58:55.492188 22895071117440 run.py:483] Algo bellman_ford step 2216 current loss 0.551867, current_train_items 70944.
I0302 18:58:55.512500 22895071117440 run.py:483] Algo bellman_ford step 2217 current loss 0.802575, current_train_items 70976.
I0302 18:58:55.540875 22895071117440 run.py:483] Algo bellman_ford step 2218 current loss 0.934752, current_train_items 71008.
I0302 18:58:55.569990 22895071117440 run.py:483] Algo bellman_ford step 2219 current loss 0.901729, current_train_items 71040.
I0302 18:58:55.587705 22895071117440 run.py:483] Algo bellman_ford step 2220 current loss 0.302638, current_train_items 71072.
I0302 18:58:55.602879 22895071117440 run.py:483] Algo bellman_ford step 2221 current loss 0.518036, current_train_items 71104.
I0302 18:58:55.623940 22895071117440 run.py:483] Algo bellman_ford step 2222 current loss 0.752631, current_train_items 71136.
I0302 18:58:55.652373 22895071117440 run.py:483] Algo bellman_ford step 2223 current loss 0.913972, current_train_items 71168.
I0302 18:58:55.684150 22895071117440 run.py:483] Algo bellman_ford step 2224 current loss 1.077935, current_train_items 71200.
I0302 18:58:55.702237 22895071117440 run.py:483] Algo bellman_ford step 2225 current loss 0.309522, current_train_items 71232.
I0302 18:58:55.717735 22895071117440 run.py:483] Algo bellman_ford step 2226 current loss 0.550536, current_train_items 71264.
I0302 18:58:55.739453 22895071117440 run.py:483] Algo bellman_ford step 2227 current loss 0.754577, current_train_items 71296.
I0302 18:58:55.767412 22895071117440 run.py:483] Algo bellman_ford step 2228 current loss 0.926518, current_train_items 71328.
I0302 18:58:55.795346 22895071117440 run.py:483] Algo bellman_ford step 2229 current loss 0.910113, current_train_items 71360.
I0302 18:58:55.813360 22895071117440 run.py:483] Algo bellman_ford step 2230 current loss 0.366818, current_train_items 71392.
I0302 18:58:55.829033 22895071117440 run.py:483] Algo bellman_ford step 2231 current loss 0.712488, current_train_items 71424.
I0302 18:58:55.850414 22895071117440 run.py:483] Algo bellman_ford step 2232 current loss 0.777411, current_train_items 71456.
I0302 18:58:55.877416 22895071117440 run.py:483] Algo bellman_ford step 2233 current loss 0.834392, current_train_items 71488.
I0302 18:58:55.907023 22895071117440 run.py:483] Algo bellman_ford step 2234 current loss 1.056493, current_train_items 71520.
I0302 18:58:55.924911 22895071117440 run.py:483] Algo bellman_ford step 2235 current loss 0.328079, current_train_items 71552.
I0302 18:58:55.940467 22895071117440 run.py:483] Algo bellman_ford step 2236 current loss 0.561554, current_train_items 71584.
I0302 18:58:55.963045 22895071117440 run.py:483] Algo bellman_ford step 2237 current loss 0.759339, current_train_items 71616.
I0302 18:58:55.991099 22895071117440 run.py:483] Algo bellman_ford step 2238 current loss 0.849614, current_train_items 71648.
I0302 18:58:56.020563 22895071117440 run.py:483] Algo bellman_ford step 2239 current loss 0.791152, current_train_items 71680.
I0302 18:58:56.038297 22895071117440 run.py:483] Algo bellman_ford step 2240 current loss 0.430886, current_train_items 71712.
I0302 18:58:56.054244 22895071117440 run.py:483] Algo bellman_ford step 2241 current loss 0.602438, current_train_items 71744.
I0302 18:58:56.076471 22895071117440 run.py:483] Algo bellman_ford step 2242 current loss 0.755190, current_train_items 71776.
I0302 18:58:56.104326 22895071117440 run.py:483] Algo bellman_ford step 2243 current loss 0.901394, current_train_items 71808.
I0302 18:58:56.132850 22895071117440 run.py:483] Algo bellman_ford step 2244 current loss 0.777791, current_train_items 71840.
I0302 18:58:56.150668 22895071117440 run.py:483] Algo bellman_ford step 2245 current loss 0.295612, current_train_items 71872.
I0302 18:58:56.166266 22895071117440 run.py:483] Algo bellman_ford step 2246 current loss 0.630996, current_train_items 71904.
I0302 18:58:56.187150 22895071117440 run.py:483] Algo bellman_ford step 2247 current loss 0.788916, current_train_items 71936.
I0302 18:58:56.215849 22895071117440 run.py:483] Algo bellman_ford step 2248 current loss 0.806447, current_train_items 71968.
I0302 18:58:56.246520 22895071117440 run.py:483] Algo bellman_ford step 2249 current loss 0.943626, current_train_items 72000.
I0302 18:58:56.264504 22895071117440 run.py:483] Algo bellman_ford step 2250 current loss 0.368325, current_train_items 72032.
I0302 18:58:56.272972 22895071117440 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.273077 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 18:58:56.288865 22895071117440 run.py:483] Algo bellman_ford step 2251 current loss 0.591718, current_train_items 72064.
I0302 18:58:56.309691 22895071117440 run.py:483] Algo bellman_ford step 2252 current loss 0.709598, current_train_items 72096.
I0302 18:58:56.339587 22895071117440 run.py:483] Algo bellman_ford step 2253 current loss 1.042297, current_train_items 72128.
I0302 18:58:56.370975 22895071117440 run.py:483] Algo bellman_ford step 2254 current loss 1.171633, current_train_items 72160.
I0302 18:58:56.389280 22895071117440 run.py:483] Algo bellman_ford step 2255 current loss 0.391059, current_train_items 72192.
I0302 18:58:56.404792 22895071117440 run.py:483] Algo bellman_ford step 2256 current loss 0.531513, current_train_items 72224.
I0302 18:58:56.426145 22895071117440 run.py:483] Algo bellman_ford step 2257 current loss 0.741744, current_train_items 72256.
I0302 18:58:56.452467 22895071117440 run.py:483] Algo bellman_ford step 2258 current loss 0.853460, current_train_items 72288.
I0302 18:58:56.483698 22895071117440 run.py:483] Algo bellman_ford step 2259 current loss 1.142226, current_train_items 72320.
I0302 18:58:56.502162 22895071117440 run.py:483] Algo bellman_ford step 2260 current loss 0.331176, current_train_items 72352.
I0302 18:58:56.518249 22895071117440 run.py:483] Algo bellman_ford step 2261 current loss 0.738304, current_train_items 72384.
I0302 18:58:56.540688 22895071117440 run.py:483] Algo bellman_ford step 2262 current loss 1.055184, current_train_items 72416.
I0302 18:58:56.567768 22895071117440 run.py:483] Algo bellman_ford step 2263 current loss 1.087321, current_train_items 72448.
I0302 18:58:56.598166 22895071117440 run.py:483] Algo bellman_ford step 2264 current loss 1.258515, current_train_items 72480.
I0302 18:58:56.615977 22895071117440 run.py:483] Algo bellman_ford step 2265 current loss 0.346305, current_train_items 72512.
I0302 18:58:56.631178 22895071117440 run.py:483] Algo bellman_ford step 2266 current loss 0.538126, current_train_items 72544.
I0302 18:58:56.653323 22895071117440 run.py:483] Algo bellman_ford step 2267 current loss 0.822779, current_train_items 72576.
I0302 18:58:56.680815 22895071117440 run.py:483] Algo bellman_ford step 2268 current loss 1.036658, current_train_items 72608.
I0302 18:58:56.710664 22895071117440 run.py:483] Algo bellman_ford step 2269 current loss 1.080354, current_train_items 72640.
I0302 18:58:56.729265 22895071117440 run.py:483] Algo bellman_ford step 2270 current loss 0.299807, current_train_items 72672.
I0302 18:58:56.744636 22895071117440 run.py:483] Algo bellman_ford step 2271 current loss 0.571982, current_train_items 72704.
I0302 18:58:56.764851 22895071117440 run.py:483] Algo bellman_ford step 2272 current loss 0.773588, current_train_items 72736.
I0302 18:58:56.792917 22895071117440 run.py:483] Algo bellman_ford step 2273 current loss 0.938267, current_train_items 72768.
I0302 18:58:56.825515 22895071117440 run.py:483] Algo bellman_ford step 2274 current loss 1.252673, current_train_items 72800.
I0302 18:58:56.844024 22895071117440 run.py:483] Algo bellman_ford step 2275 current loss 0.404357, current_train_items 72832.
I0302 18:58:56.859470 22895071117440 run.py:483] Algo bellman_ford step 2276 current loss 0.517622, current_train_items 72864.
I0302 18:58:56.879047 22895071117440 run.py:483] Algo bellman_ford step 2277 current loss 0.695721, current_train_items 72896.
I0302 18:58:56.905874 22895071117440 run.py:483] Algo bellman_ford step 2278 current loss 0.893694, current_train_items 72928.
I0302 18:58:56.937122 22895071117440 run.py:483] Algo bellman_ford step 2279 current loss 1.025970, current_train_items 72960.
I0302 18:58:56.955218 22895071117440 run.py:483] Algo bellman_ford step 2280 current loss 0.344780, current_train_items 72992.
I0302 18:58:56.970216 22895071117440 run.py:483] Algo bellman_ford step 2281 current loss 0.653903, current_train_items 73024.
I0302 18:58:56.992397 22895071117440 run.py:483] Algo bellman_ford step 2282 current loss 0.816866, current_train_items 73056.
I0302 18:58:57.022585 22895071117440 run.py:483] Algo bellman_ford step 2283 current loss 1.055363, current_train_items 73088.
I0302 18:58:57.053843 22895071117440 run.py:483] Algo bellman_ford step 2284 current loss 1.041882, current_train_items 73120.
I0302 18:58:57.072141 22895071117440 run.py:483] Algo bellman_ford step 2285 current loss 0.355944, current_train_items 73152.
I0302 18:58:57.087819 22895071117440 run.py:483] Algo bellman_ford step 2286 current loss 0.533502, current_train_items 73184.
I0302 18:58:57.109009 22895071117440 run.py:483] Algo bellman_ford step 2287 current loss 0.731425, current_train_items 73216.
I0302 18:58:57.136130 22895071117440 run.py:483] Algo bellman_ford step 2288 current loss 0.796721, current_train_items 73248.
I0302 18:58:57.168791 22895071117440 run.py:483] Algo bellman_ford step 2289 current loss 1.063003, current_train_items 73280.
I0302 18:58:57.187144 22895071117440 run.py:483] Algo bellman_ford step 2290 current loss 0.353860, current_train_items 73312.
I0302 18:58:57.202164 22895071117440 run.py:483] Algo bellman_ford step 2291 current loss 0.478881, current_train_items 73344.
I0302 18:58:57.222845 22895071117440 run.py:483] Algo bellman_ford step 2292 current loss 0.652493, current_train_items 73376.
I0302 18:58:57.250764 22895071117440 run.py:483] Algo bellman_ford step 2293 current loss 0.832746, current_train_items 73408.
I0302 18:58:57.281621 22895071117440 run.py:483] Algo bellman_ford step 2294 current loss 0.986926, current_train_items 73440.
I0302 18:58:57.299405 22895071117440 run.py:483] Algo bellman_ford step 2295 current loss 0.306524, current_train_items 73472.
I0302 18:58:57.314595 22895071117440 run.py:483] Algo bellman_ford step 2296 current loss 0.569984, current_train_items 73504.
I0302 18:58:57.335034 22895071117440 run.py:483] Algo bellman_ford step 2297 current loss 0.643588, current_train_items 73536.
I0302 18:58:57.361786 22895071117440 run.py:483] Algo bellman_ford step 2298 current loss 0.773652, current_train_items 73568.
I0302 18:58:57.392457 22895071117440 run.py:483] Algo bellman_ford step 2299 current loss 0.994975, current_train_items 73600.
I0302 18:58:57.410765 22895071117440 run.py:483] Algo bellman_ford step 2300 current loss 0.314632, current_train_items 73632.
I0302 18:58:57.418788 22895071117440 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:58:57.418892 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 18:58:57.435132 22895071117440 run.py:483] Algo bellman_ford step 2301 current loss 0.624793, current_train_items 73664.
I0302 18:58:57.457346 22895071117440 run.py:483] Algo bellman_ford step 2302 current loss 0.786922, current_train_items 73696.
I0302 18:58:57.486626 22895071117440 run.py:483] Algo bellman_ford step 2303 current loss 0.824463, current_train_items 73728.
I0302 18:58:57.518026 22895071117440 run.py:483] Algo bellman_ford step 2304 current loss 0.909722, current_train_items 73760.
I0302 18:58:57.536579 22895071117440 run.py:483] Algo bellman_ford step 2305 current loss 0.311130, current_train_items 73792.
I0302 18:58:57.552364 22895071117440 run.py:483] Algo bellman_ford step 2306 current loss 0.652195, current_train_items 73824.
I0302 18:58:57.574030 22895071117440 run.py:483] Algo bellman_ford step 2307 current loss 0.732610, current_train_items 73856.
I0302 18:58:57.601054 22895071117440 run.py:483] Algo bellman_ford step 2308 current loss 0.826396, current_train_items 73888.
I0302 18:58:57.632634 22895071117440 run.py:483] Algo bellman_ford step 2309 current loss 0.958929, current_train_items 73920.
I0302 18:58:57.650810 22895071117440 run.py:483] Algo bellman_ford step 2310 current loss 0.407846, current_train_items 73952.
I0302 18:58:57.666217 22895071117440 run.py:483] Algo bellman_ford step 2311 current loss 0.602175, current_train_items 73984.
I0302 18:58:57.687521 22895071117440 run.py:483] Algo bellman_ford step 2312 current loss 0.812448, current_train_items 74016.
I0302 18:58:57.715096 22895071117440 run.py:483] Algo bellman_ford step 2313 current loss 0.789379, current_train_items 74048.
I0302 18:58:57.745908 22895071117440 run.py:483] Algo bellman_ford step 2314 current loss 0.917011, current_train_items 74080.
I0302 18:58:57.764025 22895071117440 run.py:483] Algo bellman_ford step 2315 current loss 0.473567, current_train_items 74112.
I0302 18:58:57.779285 22895071117440 run.py:483] Algo bellman_ford step 2316 current loss 0.625608, current_train_items 74144.
I0302 18:58:57.800326 22895071117440 run.py:483] Algo bellman_ford step 2317 current loss 0.710672, current_train_items 74176.
I0302 18:58:57.829051 22895071117440 run.py:483] Algo bellman_ford step 2318 current loss 0.847086, current_train_items 74208.
I0302 18:58:57.860248 22895071117440 run.py:483] Algo bellman_ford step 2319 current loss 1.021757, current_train_items 74240.
I0302 18:58:57.878373 22895071117440 run.py:483] Algo bellman_ford step 2320 current loss 0.450998, current_train_items 74272.
I0302 18:58:57.893516 22895071117440 run.py:483] Algo bellman_ford step 2321 current loss 0.520283, current_train_items 74304.
I0302 18:58:57.915546 22895071117440 run.py:483] Algo bellman_ford step 2322 current loss 0.777824, current_train_items 74336.
I0302 18:58:57.942911 22895071117440 run.py:483] Algo bellman_ford step 2323 current loss 0.860015, current_train_items 74368.
I0302 18:58:57.973883 22895071117440 run.py:483] Algo bellman_ford step 2324 current loss 1.100643, current_train_items 74400.
I0302 18:58:57.992049 22895071117440 run.py:483] Algo bellman_ford step 2325 current loss 0.290764, current_train_items 74432.
I0302 18:58:58.007193 22895071117440 run.py:483] Algo bellman_ford step 2326 current loss 0.541580, current_train_items 74464.
I0302 18:58:58.028120 22895071117440 run.py:483] Algo bellman_ford step 2327 current loss 0.746474, current_train_items 74496.
I0302 18:58:58.054410 22895071117440 run.py:483] Algo bellman_ford step 2328 current loss 0.733737, current_train_items 74528.
I0302 18:58:58.083653 22895071117440 run.py:483] Algo bellman_ford step 2329 current loss 0.817231, current_train_items 74560.
I0302 18:58:58.101785 22895071117440 run.py:483] Algo bellman_ford step 2330 current loss 0.437732, current_train_items 74592.
I0302 18:58:58.116659 22895071117440 run.py:483] Algo bellman_ford step 2331 current loss 0.631154, current_train_items 74624.
I0302 18:58:58.138719 22895071117440 run.py:483] Algo bellman_ford step 2332 current loss 0.883015, current_train_items 74656.
I0302 18:58:58.167321 22895071117440 run.py:483] Algo bellman_ford step 2333 current loss 0.845177, current_train_items 74688.
I0302 18:58:58.199065 22895071117440 run.py:483] Algo bellman_ford step 2334 current loss 0.921454, current_train_items 74720.
I0302 18:58:58.216871 22895071117440 run.py:483] Algo bellman_ford step 2335 current loss 0.389209, current_train_items 74752.
I0302 18:58:58.232563 22895071117440 run.py:483] Algo bellman_ford step 2336 current loss 0.537234, current_train_items 74784.
I0302 18:58:58.254507 22895071117440 run.py:483] Algo bellman_ford step 2337 current loss 0.714257, current_train_items 74816.
I0302 18:58:58.281920 22895071117440 run.py:483] Algo bellman_ford step 2338 current loss 0.919682, current_train_items 74848.
I0302 18:58:58.311768 22895071117440 run.py:483] Algo bellman_ford step 2339 current loss 0.987162, current_train_items 74880.
I0302 18:58:58.330120 22895071117440 run.py:483] Algo bellman_ford step 2340 current loss 0.462954, current_train_items 74912.
I0302 18:58:58.345640 22895071117440 run.py:483] Algo bellman_ford step 2341 current loss 0.518270, current_train_items 74944.
I0302 18:58:58.366575 22895071117440 run.py:483] Algo bellman_ford step 2342 current loss 0.684686, current_train_items 74976.
I0302 18:58:58.393456 22895071117440 run.py:483] Algo bellman_ford step 2343 current loss 0.853621, current_train_items 75008.
I0302 18:58:58.422454 22895071117440 run.py:483] Algo bellman_ford step 2344 current loss 0.803809, current_train_items 75040.
I0302 18:58:58.440393 22895071117440 run.py:483] Algo bellman_ford step 2345 current loss 0.387284, current_train_items 75072.
I0302 18:58:58.455477 22895071117440 run.py:483] Algo bellman_ford step 2346 current loss 0.488085, current_train_items 75104.
I0302 18:58:58.476933 22895071117440 run.py:483] Algo bellman_ford step 2347 current loss 0.678305, current_train_items 75136.
I0302 18:58:58.503068 22895071117440 run.py:483] Algo bellman_ford step 2348 current loss 0.796195, current_train_items 75168.
I0302 18:58:58.532291 22895071117440 run.py:483] Algo bellman_ford step 2349 current loss 0.831034, current_train_items 75200.
I0302 18:58:58.550437 22895071117440 run.py:483] Algo bellman_ford step 2350 current loss 0.272814, current_train_items 75232.
I0302 18:58:58.558368 22895071117440 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:58:58.558474 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:58:58.574511 22895071117440 run.py:483] Algo bellman_ford step 2351 current loss 0.477010, current_train_items 75264.
I0302 18:58:58.596373 22895071117440 run.py:483] Algo bellman_ford step 2352 current loss 0.710135, current_train_items 75296.
I0302 18:58:58.622963 22895071117440 run.py:483] Algo bellman_ford step 2353 current loss 0.705141, current_train_items 75328.
I0302 18:58:58.655282 22895071117440 run.py:483] Algo bellman_ford step 2354 current loss 1.093139, current_train_items 75360.
I0302 18:58:58.673756 22895071117440 run.py:483] Algo bellman_ford step 2355 current loss 0.421271, current_train_items 75392.
I0302 18:58:58.688680 22895071117440 run.py:483] Algo bellman_ford step 2356 current loss 0.590381, current_train_items 75424.
I0302 18:58:58.710268 22895071117440 run.py:483] Algo bellman_ford step 2357 current loss 0.752359, current_train_items 75456.
I0302 18:58:58.736419 22895071117440 run.py:483] Algo bellman_ford step 2358 current loss 0.701625, current_train_items 75488.
I0302 18:58:58.768365 22895071117440 run.py:483] Algo bellman_ford step 2359 current loss 1.054465, current_train_items 75520.
I0302 18:58:58.786516 22895071117440 run.py:483] Algo bellman_ford step 2360 current loss 0.353718, current_train_items 75552.
I0302 18:58:58.802832 22895071117440 run.py:483] Algo bellman_ford step 2361 current loss 0.661679, current_train_items 75584.
I0302 18:58:58.825188 22895071117440 run.py:483] Algo bellman_ford step 2362 current loss 0.675574, current_train_items 75616.
I0302 18:58:58.852037 22895071117440 run.py:483] Algo bellman_ford step 2363 current loss 0.857040, current_train_items 75648.
I0302 18:58:58.882444 22895071117440 run.py:483] Algo bellman_ford step 2364 current loss 0.933822, current_train_items 75680.
I0302 18:58:58.899981 22895071117440 run.py:483] Algo bellman_ford step 2365 current loss 0.295614, current_train_items 75712.
I0302 18:58:58.915337 22895071117440 run.py:483] Algo bellman_ford step 2366 current loss 0.575342, current_train_items 75744.
I0302 18:58:58.936939 22895071117440 run.py:483] Algo bellman_ford step 2367 current loss 0.888562, current_train_items 75776.
I0302 18:58:58.965455 22895071117440 run.py:483] Algo bellman_ford step 2368 current loss 0.816760, current_train_items 75808.
I0302 18:58:58.996670 22895071117440 run.py:483] Algo bellman_ford step 2369 current loss 0.939109, current_train_items 75840.
I0302 18:58:59.014990 22895071117440 run.py:483] Algo bellman_ford step 2370 current loss 0.328561, current_train_items 75872.
I0302 18:58:59.030478 22895071117440 run.py:483] Algo bellman_ford step 2371 current loss 0.611626, current_train_items 75904.
I0302 18:58:59.051486 22895071117440 run.py:483] Algo bellman_ford step 2372 current loss 0.747558, current_train_items 75936.
I0302 18:58:59.077945 22895071117440 run.py:483] Algo bellman_ford step 2373 current loss 1.050617, current_train_items 75968.
I0302 18:58:59.107791 22895071117440 run.py:483] Algo bellman_ford step 2374 current loss 1.042942, current_train_items 76000.
I0302 18:58:59.125779 22895071117440 run.py:483] Algo bellman_ford step 2375 current loss 0.303803, current_train_items 76032.
I0302 18:58:59.141434 22895071117440 run.py:483] Algo bellman_ford step 2376 current loss 0.646943, current_train_items 76064.
I0302 18:58:59.162692 22895071117440 run.py:483] Algo bellman_ford step 2377 current loss 0.848185, current_train_items 76096.
I0302 18:58:59.191265 22895071117440 run.py:483] Algo bellman_ford step 2378 current loss 0.925508, current_train_items 76128.
I0302 18:58:59.223001 22895071117440 run.py:483] Algo bellman_ford step 2379 current loss 1.007090, current_train_items 76160.
I0302 18:58:59.240798 22895071117440 run.py:483] Algo bellman_ford step 2380 current loss 0.330232, current_train_items 76192.
I0302 18:58:59.256489 22895071117440 run.py:483] Algo bellman_ford step 2381 current loss 0.552758, current_train_items 76224.
I0302 18:58:59.278293 22895071117440 run.py:483] Algo bellman_ford step 2382 current loss 0.771071, current_train_items 76256.
I0302 18:58:59.304581 22895071117440 run.py:483] Algo bellman_ford step 2383 current loss 0.673470, current_train_items 76288.
I0302 18:58:59.335320 22895071117440 run.py:483] Algo bellman_ford step 2384 current loss 0.987615, current_train_items 76320.
I0302 18:58:59.353477 22895071117440 run.py:483] Algo bellman_ford step 2385 current loss 0.334473, current_train_items 76352.
I0302 18:58:59.368797 22895071117440 run.py:483] Algo bellman_ford step 2386 current loss 0.502613, current_train_items 76384.
I0302 18:58:59.389395 22895071117440 run.py:483] Algo bellman_ford step 2387 current loss 0.742455, current_train_items 76416.
I0302 18:58:59.416923 22895071117440 run.py:483] Algo bellman_ford step 2388 current loss 0.795776, current_train_items 76448.
I0302 18:58:59.448879 22895071117440 run.py:483] Algo bellman_ford step 2389 current loss 0.874364, current_train_items 76480.
I0302 18:58:59.467154 22895071117440 run.py:483] Algo bellman_ford step 2390 current loss 0.320333, current_train_items 76512.
I0302 18:58:59.482580 22895071117440 run.py:483] Algo bellman_ford step 2391 current loss 0.525661, current_train_items 76544.
I0302 18:58:59.502373 22895071117440 run.py:483] Algo bellman_ford step 2392 current loss 0.646681, current_train_items 76576.
I0302 18:58:59.529833 22895071117440 run.py:483] Algo bellman_ford step 2393 current loss 0.816823, current_train_items 76608.
I0302 18:58:59.562942 22895071117440 run.py:483] Algo bellman_ford step 2394 current loss 1.088658, current_train_items 76640.
I0302 18:58:59.580878 22895071117440 run.py:483] Algo bellman_ford step 2395 current loss 0.362291, current_train_items 76672.
I0302 18:58:59.596085 22895071117440 run.py:483] Algo bellman_ford step 2396 current loss 0.567618, current_train_items 76704.
I0302 18:58:59.616410 22895071117440 run.py:483] Algo bellman_ford step 2397 current loss 0.733721, current_train_items 76736.
I0302 18:58:59.643526 22895071117440 run.py:483] Algo bellman_ford step 2398 current loss 0.993466, current_train_items 76768.
I0302 18:58:59.674731 22895071117440 run.py:483] Algo bellman_ford step 2399 current loss 1.035344, current_train_items 76800.
I0302 18:58:59.693184 22895071117440 run.py:483] Algo bellman_ford step 2400 current loss 0.249436, current_train_items 76832.
I0302 18:58:59.700910 22895071117440 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:58:59.701016 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:58:59.716819 22895071117440 run.py:483] Algo bellman_ford step 2401 current loss 0.537947, current_train_items 76864.
I0302 18:58:59.738699 22895071117440 run.py:483] Algo bellman_ford step 2402 current loss 0.721894, current_train_items 76896.
I0302 18:58:59.767633 22895071117440 run.py:483] Algo bellman_ford step 2403 current loss 1.007943, current_train_items 76928.
I0302 18:58:59.801731 22895071117440 run.py:483] Algo bellman_ford step 2404 current loss 1.063214, current_train_items 76960.
I0302 18:58:59.820187 22895071117440 run.py:483] Algo bellman_ford step 2405 current loss 0.259638, current_train_items 76992.
I0302 18:58:59.835534 22895071117440 run.py:483] Algo bellman_ford step 2406 current loss 0.645768, current_train_items 77024.
I0302 18:58:59.857323 22895071117440 run.py:483] Algo bellman_ford step 2407 current loss 0.721652, current_train_items 77056.
I0302 18:58:59.884039 22895071117440 run.py:483] Algo bellman_ford step 2408 current loss 0.835972, current_train_items 77088.
I0302 18:58:59.914035 22895071117440 run.py:483] Algo bellman_ford step 2409 current loss 1.138550, current_train_items 77120.
I0302 18:58:59.931688 22895071117440 run.py:483] Algo bellman_ford step 2410 current loss 0.242818, current_train_items 77152.
I0302 18:58:59.946869 22895071117440 run.py:483] Algo bellman_ford step 2411 current loss 0.535079, current_train_items 77184.
I0302 18:58:59.969623 22895071117440 run.py:483] Algo bellman_ford step 2412 current loss 0.956292, current_train_items 77216.
I0302 18:58:59.998030 22895071117440 run.py:483] Algo bellman_ford step 2413 current loss 0.918396, current_train_items 77248.
I0302 18:59:00.029644 22895071117440 run.py:483] Algo bellman_ford step 2414 current loss 1.121944, current_train_items 77280.
I0302 18:59:00.047676 22895071117440 run.py:483] Algo bellman_ford step 2415 current loss 0.342561, current_train_items 77312.
I0302 18:59:00.062379 22895071117440 run.py:483] Algo bellman_ford step 2416 current loss 0.531451, current_train_items 77344.
I0302 18:59:00.083877 22895071117440 run.py:483] Algo bellman_ford step 2417 current loss 0.809680, current_train_items 77376.
I0302 18:59:00.112647 22895071117440 run.py:483] Algo bellman_ford step 2418 current loss 1.016520, current_train_items 77408.
I0302 18:59:00.141445 22895071117440 run.py:483] Algo bellman_ford step 2419 current loss 1.009920, current_train_items 77440.
I0302 18:59:00.159163 22895071117440 run.py:483] Algo bellman_ford step 2420 current loss 0.309020, current_train_items 77472.
I0302 18:59:00.174241 22895071117440 run.py:483] Algo bellman_ford step 2421 current loss 0.572769, current_train_items 77504.
I0302 18:59:00.196468 22895071117440 run.py:483] Algo bellman_ford step 2422 current loss 0.779455, current_train_items 77536.
I0302 18:59:00.224293 22895071117440 run.py:483] Algo bellman_ford step 2423 current loss 0.848813, current_train_items 77568.
I0302 18:59:00.255639 22895071117440 run.py:483] Algo bellman_ford step 2424 current loss 0.865506, current_train_items 77600.
I0302 18:59:00.273548 22895071117440 run.py:483] Algo bellman_ford step 2425 current loss 0.266590, current_train_items 77632.
I0302 18:59:00.288719 22895071117440 run.py:483] Algo bellman_ford step 2426 current loss 0.511218, current_train_items 77664.
I0302 18:59:00.311382 22895071117440 run.py:483] Algo bellman_ford step 2427 current loss 0.823284, current_train_items 77696.
I0302 18:59:00.337950 22895071117440 run.py:483] Algo bellman_ford step 2428 current loss 0.865953, current_train_items 77728.
I0302 18:59:00.368047 22895071117440 run.py:483] Algo bellman_ford step 2429 current loss 0.871148, current_train_items 77760.
I0302 18:59:00.386009 22895071117440 run.py:483] Algo bellman_ford step 2430 current loss 0.438982, current_train_items 77792.
I0302 18:59:00.401562 22895071117440 run.py:483] Algo bellman_ford step 2431 current loss 0.572693, current_train_items 77824.
I0302 18:59:00.423832 22895071117440 run.py:483] Algo bellman_ford step 2432 current loss 0.734969, current_train_items 77856.
I0302 18:59:00.451198 22895071117440 run.py:483] Algo bellman_ford step 2433 current loss 0.736114, current_train_items 77888.
I0302 18:59:00.482192 22895071117440 run.py:483] Algo bellman_ford step 2434 current loss 0.836581, current_train_items 77920.
I0302 18:59:00.500294 22895071117440 run.py:483] Algo bellman_ford step 2435 current loss 0.358199, current_train_items 77952.
I0302 18:59:00.515793 22895071117440 run.py:483] Algo bellman_ford step 2436 current loss 0.646032, current_train_items 77984.
I0302 18:59:00.537608 22895071117440 run.py:483] Algo bellman_ford step 2437 current loss 0.776884, current_train_items 78016.
I0302 18:59:00.564039 22895071117440 run.py:483] Algo bellman_ford step 2438 current loss 0.831235, current_train_items 78048.
I0302 18:59:00.593741 22895071117440 run.py:483] Algo bellman_ford step 2439 current loss 0.826706, current_train_items 78080.
I0302 18:59:00.612221 22895071117440 run.py:483] Algo bellman_ford step 2440 current loss 0.461930, current_train_items 78112.
I0302 18:59:00.627481 22895071117440 run.py:483] Algo bellman_ford step 2441 current loss 0.447128, current_train_items 78144.
I0302 18:59:00.650253 22895071117440 run.py:483] Algo bellman_ford step 2442 current loss 0.820003, current_train_items 78176.
I0302 18:59:00.677832 22895071117440 run.py:483] Algo bellman_ford step 2443 current loss 0.853091, current_train_items 78208.
I0302 18:59:00.710578 22895071117440 run.py:483] Algo bellman_ford step 2444 current loss 0.909386, current_train_items 78240.
I0302 18:59:00.728541 22895071117440 run.py:483] Algo bellman_ford step 2445 current loss 0.371609, current_train_items 78272.
I0302 18:59:00.743912 22895071117440 run.py:483] Algo bellman_ford step 2446 current loss 0.604279, current_train_items 78304.
I0302 18:59:00.765972 22895071117440 run.py:483] Algo bellman_ford step 2447 current loss 0.689941, current_train_items 78336.
I0302 18:59:00.792783 22895071117440 run.py:483] Algo bellman_ford step 2448 current loss 0.789660, current_train_items 78368.
I0302 18:59:00.822720 22895071117440 run.py:483] Algo bellman_ford step 2449 current loss 0.889791, current_train_items 78400.
I0302 18:59:00.840376 22895071117440 run.py:483] Algo bellman_ford step 2450 current loss 0.401906, current_train_items 78432.
I0302 18:59:00.848018 22895071117440 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:00.848124 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 18:59:00.864068 22895071117440 run.py:483] Algo bellman_ford step 2451 current loss 0.480628, current_train_items 78464.
I0302 18:59:00.886323 22895071117440 run.py:483] Algo bellman_ford step 2452 current loss 0.767051, current_train_items 78496.
I0302 18:59:00.913868 22895071117440 run.py:483] Algo bellman_ford step 2453 current loss 0.768059, current_train_items 78528.
I0302 18:59:00.945403 22895071117440 run.py:483] Algo bellman_ford step 2454 current loss 0.961546, current_train_items 78560.
I0302 18:59:00.963837 22895071117440 run.py:483] Algo bellman_ford step 2455 current loss 0.291987, current_train_items 78592.
I0302 18:59:00.979084 22895071117440 run.py:483] Algo bellman_ford step 2456 current loss 0.561778, current_train_items 78624.
I0302 18:59:00.999578 22895071117440 run.py:483] Algo bellman_ford step 2457 current loss 0.666673, current_train_items 78656.
I0302 18:59:01.027245 22895071117440 run.py:483] Algo bellman_ford step 2458 current loss 0.809575, current_train_items 78688.
I0302 18:59:01.058988 22895071117440 run.py:483] Algo bellman_ford step 2459 current loss 1.036925, current_train_items 78720.
I0302 18:59:01.077351 22895071117440 run.py:483] Algo bellman_ford step 2460 current loss 0.320161, current_train_items 78752.
I0302 18:59:01.093300 22895071117440 run.py:483] Algo bellman_ford step 2461 current loss 0.602657, current_train_items 78784.
I0302 18:59:01.114198 22895071117440 run.py:483] Algo bellman_ford step 2462 current loss 0.715287, current_train_items 78816.
I0302 18:59:01.142668 22895071117440 run.py:483] Algo bellman_ford step 2463 current loss 0.867368, current_train_items 78848.
I0302 18:59:01.173122 22895071117440 run.py:483] Algo bellman_ford step 2464 current loss 0.863544, current_train_items 78880.
I0302 18:59:01.190777 22895071117440 run.py:483] Algo bellman_ford step 2465 current loss 0.293808, current_train_items 78912.
I0302 18:59:01.206039 22895071117440 run.py:483] Algo bellman_ford step 2466 current loss 0.618263, current_train_items 78944.
I0302 18:59:01.227641 22895071117440 run.py:483] Algo bellman_ford step 2467 current loss 0.716365, current_train_items 78976.
I0302 18:59:01.255682 22895071117440 run.py:483] Algo bellman_ford step 2468 current loss 0.830923, current_train_items 79008.
I0302 18:59:01.287693 22895071117440 run.py:483] Algo bellman_ford step 2469 current loss 0.934937, current_train_items 79040.
I0302 18:59:01.305937 22895071117440 run.py:483] Algo bellman_ford step 2470 current loss 0.376500, current_train_items 79072.
I0302 18:59:01.321584 22895071117440 run.py:483] Algo bellman_ford step 2471 current loss 0.494514, current_train_items 79104.
I0302 18:59:01.343288 22895071117440 run.py:483] Algo bellman_ford step 2472 current loss 0.807697, current_train_items 79136.
I0302 18:59:01.371724 22895071117440 run.py:483] Algo bellman_ford step 2473 current loss 0.944704, current_train_items 79168.
I0302 18:59:01.401666 22895071117440 run.py:483] Algo bellman_ford step 2474 current loss 0.922159, current_train_items 79200.
I0302 18:59:01.419975 22895071117440 run.py:483] Algo bellman_ford step 2475 current loss 0.349365, current_train_items 79232.
I0302 18:59:01.435519 22895071117440 run.py:483] Algo bellman_ford step 2476 current loss 0.633452, current_train_items 79264.
I0302 18:59:01.455441 22895071117440 run.py:483] Algo bellman_ford step 2477 current loss 0.624682, current_train_items 79296.
I0302 18:59:01.482187 22895071117440 run.py:483] Algo bellman_ford step 2478 current loss 0.829082, current_train_items 79328.
I0302 18:59:01.513953 22895071117440 run.py:483] Algo bellman_ford step 2479 current loss 1.135302, current_train_items 79360.
I0302 18:59:01.531578 22895071117440 run.py:483] Algo bellman_ford step 2480 current loss 0.338618, current_train_items 79392.
I0302 18:59:01.546553 22895071117440 run.py:483] Algo bellman_ford step 2481 current loss 0.635053, current_train_items 79424.
I0302 18:59:01.568793 22895071117440 run.py:483] Algo bellman_ford step 2482 current loss 0.907570, current_train_items 79456.
I0302 18:59:01.596697 22895071117440 run.py:483] Algo bellman_ford step 2483 current loss 1.000167, current_train_items 79488.
I0302 18:59:01.628715 22895071117440 run.py:483] Algo bellman_ford step 2484 current loss 1.020990, current_train_items 79520.
I0302 18:59:01.647271 22895071117440 run.py:483] Algo bellman_ford step 2485 current loss 0.324551, current_train_items 79552.
I0302 18:59:01.663015 22895071117440 run.py:483] Algo bellman_ford step 2486 current loss 0.639422, current_train_items 79584.
I0302 18:59:01.685153 22895071117440 run.py:483] Algo bellman_ford step 2487 current loss 0.781776, current_train_items 79616.
I0302 18:59:01.710145 22895071117440 run.py:483] Algo bellman_ford step 2488 current loss 0.730510, current_train_items 79648.
I0302 18:59:01.741503 22895071117440 run.py:483] Algo bellman_ford step 2489 current loss 0.894117, current_train_items 79680.
I0302 18:59:01.759902 22895071117440 run.py:483] Algo bellman_ford step 2490 current loss 0.318368, current_train_items 79712.
I0302 18:59:01.775445 22895071117440 run.py:483] Algo bellman_ford step 2491 current loss 0.600810, current_train_items 79744.
I0302 18:59:01.796400 22895071117440 run.py:483] Algo bellman_ford step 2492 current loss 0.693412, current_train_items 79776.
I0302 18:59:01.823658 22895071117440 run.py:483] Algo bellman_ford step 2493 current loss 0.880213, current_train_items 79808.
I0302 18:59:01.853157 22895071117440 run.py:483] Algo bellman_ford step 2494 current loss 0.868668, current_train_items 79840.
I0302 18:59:01.870974 22895071117440 run.py:483] Algo bellman_ford step 2495 current loss 0.285031, current_train_items 79872.
I0302 18:59:01.886706 22895071117440 run.py:483] Algo bellman_ford step 2496 current loss 0.480804, current_train_items 79904.
I0302 18:59:01.907847 22895071117440 run.py:483] Algo bellman_ford step 2497 current loss 0.706073, current_train_items 79936.
I0302 18:59:01.934441 22895071117440 run.py:483] Algo bellman_ford step 2498 current loss 0.867622, current_train_items 79968.
I0302 18:59:01.965408 22895071117440 run.py:483] Algo bellman_ford step 2499 current loss 0.891583, current_train_items 80000.
I0302 18:59:01.984128 22895071117440 run.py:483] Algo bellman_ford step 2500 current loss 0.470346, current_train_items 80032.
I0302 18:59:01.992056 22895071117440 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:01.992160 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:59:02.008178 22895071117440 run.py:483] Algo bellman_ford step 2501 current loss 0.502685, current_train_items 80064.
I0302 18:59:02.030421 22895071117440 run.py:483] Algo bellman_ford step 2502 current loss 0.869712, current_train_items 80096.
I0302 18:59:02.057515 22895071117440 run.py:483] Algo bellman_ford step 2503 current loss 0.833218, current_train_items 80128.
I0302 18:59:02.089461 22895071117440 run.py:483] Algo bellman_ford step 2504 current loss 1.080864, current_train_items 80160.
I0302 18:59:02.107847 22895071117440 run.py:483] Algo bellman_ford step 2505 current loss 0.311469, current_train_items 80192.
I0302 18:59:02.123256 22895071117440 run.py:483] Algo bellman_ford step 2506 current loss 0.667773, current_train_items 80224.
I0302 18:59:02.145164 22895071117440 run.py:483] Algo bellman_ford step 2507 current loss 0.793240, current_train_items 80256.
I0302 18:59:02.171795 22895071117440 run.py:483] Algo bellman_ford step 2508 current loss 0.849370, current_train_items 80288.
I0302 18:59:02.203337 22895071117440 run.py:483] Algo bellman_ford step 2509 current loss 0.955142, current_train_items 80320.
I0302 18:59:02.221146 22895071117440 run.py:483] Algo bellman_ford step 2510 current loss 0.301470, current_train_items 80352.
I0302 18:59:02.236756 22895071117440 run.py:483] Algo bellman_ford step 2511 current loss 0.687568, current_train_items 80384.
I0302 18:59:02.258216 22895071117440 run.py:483] Algo bellman_ford step 2512 current loss 0.815427, current_train_items 80416.
I0302 18:59:02.284915 22895071117440 run.py:483] Algo bellman_ford step 2513 current loss 0.841219, current_train_items 80448.
I0302 18:59:02.316486 22895071117440 run.py:483] Algo bellman_ford step 2514 current loss 1.025430, current_train_items 80480.
I0302 18:59:02.334685 22895071117440 run.py:483] Algo bellman_ford step 2515 current loss 0.312370, current_train_items 80512.
I0302 18:59:02.350096 22895071117440 run.py:483] Algo bellman_ford step 2516 current loss 0.531286, current_train_items 80544.
I0302 18:59:02.371643 22895071117440 run.py:483] Algo bellman_ford step 2517 current loss 0.810953, current_train_items 80576.
I0302 18:59:02.398843 22895071117440 run.py:483] Algo bellman_ford step 2518 current loss 0.782627, current_train_items 80608.
I0302 18:59:02.429676 22895071117440 run.py:483] Algo bellman_ford step 2519 current loss 0.909750, current_train_items 80640.
I0302 18:59:02.447506 22895071117440 run.py:483] Algo bellman_ford step 2520 current loss 0.333221, current_train_items 80672.
I0302 18:59:02.462731 22895071117440 run.py:483] Algo bellman_ford step 2521 current loss 0.567841, current_train_items 80704.
I0302 18:59:02.482857 22895071117440 run.py:483] Algo bellman_ford step 2522 current loss 0.643905, current_train_items 80736.
I0302 18:59:02.509816 22895071117440 run.py:483] Algo bellman_ford step 2523 current loss 0.718372, current_train_items 80768.
I0302 18:59:02.539857 22895071117440 run.py:483] Algo bellman_ford step 2524 current loss 0.833153, current_train_items 80800.
I0302 18:59:02.557895 22895071117440 run.py:483] Algo bellman_ford step 2525 current loss 0.359763, current_train_items 80832.
I0302 18:59:02.573161 22895071117440 run.py:483] Algo bellman_ford step 2526 current loss 0.529097, current_train_items 80864.
I0302 18:59:02.594514 22895071117440 run.py:483] Algo bellman_ford step 2527 current loss 0.816323, current_train_items 80896.
I0302 18:59:02.621437 22895071117440 run.py:483] Algo bellman_ford step 2528 current loss 0.827425, current_train_items 80928.
I0302 18:59:02.652811 22895071117440 run.py:483] Algo bellman_ford step 2529 current loss 1.057365, current_train_items 80960.
I0302 18:59:02.670655 22895071117440 run.py:483] Algo bellman_ford step 2530 current loss 0.382135, current_train_items 80992.
I0302 18:59:02.685564 22895071117440 run.py:483] Algo bellman_ford step 2531 current loss 0.504875, current_train_items 81024.
I0302 18:59:02.707060 22895071117440 run.py:483] Algo bellman_ford step 2532 current loss 0.834392, current_train_items 81056.
I0302 18:59:02.734493 22895071117440 run.py:483] Algo bellman_ford step 2533 current loss 0.770560, current_train_items 81088.
I0302 18:59:02.764558 22895071117440 run.py:483] Algo bellman_ford step 2534 current loss 0.891240, current_train_items 81120.
I0302 18:59:02.782387 22895071117440 run.py:483] Algo bellman_ford step 2535 current loss 0.312519, current_train_items 81152.
I0302 18:59:02.797326 22895071117440 run.py:483] Algo bellman_ford step 2536 current loss 0.515485, current_train_items 81184.
W0302 18:59:02.813008 22895071117440 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:08.359970 22895071117440 run.py:483] Algo bellman_ford step 2537 current loss 0.826461, current_train_items 81216.
I0302 18:59:08.389827 22895071117440 run.py:483] Algo bellman_ford step 2538 current loss 0.833647, current_train_items 81248.
I0302 18:59:08.421686 22895071117440 run.py:483] Algo bellman_ford step 2539 current loss 0.986493, current_train_items 81280.
I0302 18:59:08.440158 22895071117440 run.py:483] Algo bellman_ford step 2540 current loss 0.315107, current_train_items 81312.
I0302 18:59:08.455341 22895071117440 run.py:483] Algo bellman_ford step 2541 current loss 0.435942, current_train_items 81344.
I0302 18:59:08.477102 22895071117440 run.py:483] Algo bellman_ford step 2542 current loss 0.742880, current_train_items 81376.
I0302 18:59:08.503955 22895071117440 run.py:483] Algo bellman_ford step 2543 current loss 0.777158, current_train_items 81408.
I0302 18:59:08.534795 22895071117440 run.py:483] Algo bellman_ford step 2544 current loss 0.944525, current_train_items 81440.
I0302 18:59:08.552877 22895071117440 run.py:483] Algo bellman_ford step 2545 current loss 0.333314, current_train_items 81472.
I0302 18:59:08.568128 22895071117440 run.py:483] Algo bellman_ford step 2546 current loss 0.572468, current_train_items 81504.
I0302 18:59:08.590032 22895071117440 run.py:483] Algo bellman_ford step 2547 current loss 0.654251, current_train_items 81536.
I0302 18:59:08.619134 22895071117440 run.py:483] Algo bellman_ford step 2548 current loss 0.830527, current_train_items 81568.
I0302 18:59:08.648966 22895071117440 run.py:483] Algo bellman_ford step 2549 current loss 0.938779, current_train_items 81600.
I0302 18:59:08.666892 22895071117440 run.py:483] Algo bellman_ford step 2550 current loss 0.324896, current_train_items 81632.
I0302 18:59:08.676258 22895071117440 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:08.676363 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:08.692477 22895071117440 run.py:483] Algo bellman_ford step 2551 current loss 0.589955, current_train_items 81664.
I0302 18:59:08.714578 22895071117440 run.py:483] Algo bellman_ford step 2552 current loss 0.704461, current_train_items 81696.
I0302 18:59:08.744151 22895071117440 run.py:483] Algo bellman_ford step 2553 current loss 0.770220, current_train_items 81728.
I0302 18:59:08.775526 22895071117440 run.py:483] Algo bellman_ford step 2554 current loss 0.862680, current_train_items 81760.
I0302 18:59:08.794040 22895071117440 run.py:483] Algo bellman_ford step 2555 current loss 0.307473, current_train_items 81792.
I0302 18:59:08.809748 22895071117440 run.py:483] Algo bellman_ford step 2556 current loss 0.648941, current_train_items 81824.
I0302 18:59:08.831986 22895071117440 run.py:483] Algo bellman_ford step 2557 current loss 0.620171, current_train_items 81856.
I0302 18:59:08.860811 22895071117440 run.py:483] Algo bellman_ford step 2558 current loss 0.831512, current_train_items 81888.
I0302 18:59:08.893665 22895071117440 run.py:483] Algo bellman_ford step 2559 current loss 1.098775, current_train_items 81920.
I0302 18:59:08.912345 22895071117440 run.py:483] Algo bellman_ford step 2560 current loss 0.372756, current_train_items 81952.
I0302 18:59:08.928369 22895071117440 run.py:483] Algo bellman_ford step 2561 current loss 0.551007, current_train_items 81984.
I0302 18:59:08.950037 22895071117440 run.py:483] Algo bellman_ford step 2562 current loss 0.625625, current_train_items 82016.
I0302 18:59:08.979436 22895071117440 run.py:483] Algo bellman_ford step 2563 current loss 0.906966, current_train_items 82048.
I0302 18:59:09.010405 22895071117440 run.py:483] Algo bellman_ford step 2564 current loss 0.954024, current_train_items 82080.
I0302 18:59:09.028219 22895071117440 run.py:483] Algo bellman_ford step 2565 current loss 0.344326, current_train_items 82112.
I0302 18:59:09.043712 22895071117440 run.py:483] Algo bellman_ford step 2566 current loss 0.533536, current_train_items 82144.
I0302 18:59:09.065133 22895071117440 run.py:483] Algo bellman_ford step 2567 current loss 0.697491, current_train_items 82176.
I0302 18:59:09.094233 22895071117440 run.py:483] Algo bellman_ford step 2568 current loss 0.782014, current_train_items 82208.
I0302 18:59:09.124685 22895071117440 run.py:483] Algo bellman_ford step 2569 current loss 0.851224, current_train_items 82240.
I0302 18:59:09.142987 22895071117440 run.py:483] Algo bellman_ford step 2570 current loss 0.333924, current_train_items 82272.
I0302 18:59:09.158639 22895071117440 run.py:483] Algo bellman_ford step 2571 current loss 0.564190, current_train_items 82304.
I0302 18:59:09.179230 22895071117440 run.py:483] Algo bellman_ford step 2572 current loss 0.652385, current_train_items 82336.
I0302 18:59:09.207411 22895071117440 run.py:483] Algo bellman_ford step 2573 current loss 0.717438, current_train_items 82368.
I0302 18:59:09.239671 22895071117440 run.py:483] Algo bellman_ford step 2574 current loss 0.963958, current_train_items 82400.
I0302 18:59:09.258160 22895071117440 run.py:483] Algo bellman_ford step 2575 current loss 0.366148, current_train_items 82432.
I0302 18:59:09.273575 22895071117440 run.py:483] Algo bellman_ford step 2576 current loss 0.750232, current_train_items 82464.
I0302 18:59:09.294820 22895071117440 run.py:483] Algo bellman_ford step 2577 current loss 0.731385, current_train_items 82496.
I0302 18:59:09.323683 22895071117440 run.py:483] Algo bellman_ford step 2578 current loss 0.845373, current_train_items 82528.
I0302 18:59:09.353511 22895071117440 run.py:483] Algo bellman_ford step 2579 current loss 1.136393, current_train_items 82560.
I0302 18:59:09.371808 22895071117440 run.py:483] Algo bellman_ford step 2580 current loss 0.469314, current_train_items 82592.
I0302 18:59:09.387206 22895071117440 run.py:483] Algo bellman_ford step 2581 current loss 0.544589, current_train_items 82624.
I0302 18:59:09.408738 22895071117440 run.py:483] Algo bellman_ford step 2582 current loss 0.785185, current_train_items 82656.
I0302 18:59:09.436612 22895071117440 run.py:483] Algo bellman_ford step 2583 current loss 0.932687, current_train_items 82688.
I0302 18:59:09.468326 22895071117440 run.py:483] Algo bellman_ford step 2584 current loss 1.363453, current_train_items 82720.
I0302 18:59:09.486589 22895071117440 run.py:483] Algo bellman_ford step 2585 current loss 0.376326, current_train_items 82752.
I0302 18:59:09.502377 22895071117440 run.py:483] Algo bellman_ford step 2586 current loss 0.625398, current_train_items 82784.
I0302 18:59:09.523843 22895071117440 run.py:483] Algo bellman_ford step 2587 current loss 0.699986, current_train_items 82816.
I0302 18:59:09.551795 22895071117440 run.py:483] Algo bellman_ford step 2588 current loss 0.800687, current_train_items 82848.
I0302 18:59:09.583458 22895071117440 run.py:483] Algo bellman_ford step 2589 current loss 1.085499, current_train_items 82880.
I0302 18:59:09.601755 22895071117440 run.py:483] Algo bellman_ford step 2590 current loss 0.303603, current_train_items 82912.
I0302 18:59:09.617603 22895071117440 run.py:483] Algo bellman_ford step 2591 current loss 0.592728, current_train_items 82944.
I0302 18:59:09.639975 22895071117440 run.py:483] Algo bellman_ford step 2592 current loss 0.791691, current_train_items 82976.
I0302 18:59:09.669222 22895071117440 run.py:483] Algo bellman_ford step 2593 current loss 0.875584, current_train_items 83008.
I0302 18:59:09.698418 22895071117440 run.py:483] Algo bellman_ford step 2594 current loss 0.945425, current_train_items 83040.
I0302 18:59:09.716300 22895071117440 run.py:483] Algo bellman_ford step 2595 current loss 0.410935, current_train_items 83072.
I0302 18:59:09.731916 22895071117440 run.py:483] Algo bellman_ford step 2596 current loss 0.532468, current_train_items 83104.
I0302 18:59:09.754458 22895071117440 run.py:483] Algo bellman_ford step 2597 current loss 0.732290, current_train_items 83136.
I0302 18:59:09.782942 22895071117440 run.py:483] Algo bellman_ford step 2598 current loss 0.714470, current_train_items 83168.
I0302 18:59:09.813572 22895071117440 run.py:483] Algo bellman_ford step 2599 current loss 0.895559, current_train_items 83200.
I0302 18:59:09.832304 22895071117440 run.py:483] Algo bellman_ford step 2600 current loss 0.399740, current_train_items 83232.
I0302 18:59:09.840510 22895071117440 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:09.840615 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 18:59:09.856610 22895071117440 run.py:483] Algo bellman_ford step 2601 current loss 0.536292, current_train_items 83264.
I0302 18:59:09.878279 22895071117440 run.py:483] Algo bellman_ford step 2602 current loss 0.626190, current_train_items 83296.
I0302 18:59:09.904970 22895071117440 run.py:483] Algo bellman_ford step 2603 current loss 0.633905, current_train_items 83328.
I0302 18:59:09.936419 22895071117440 run.py:483] Algo bellman_ford step 2604 current loss 0.997000, current_train_items 83360.
I0302 18:59:09.954858 22895071117440 run.py:483] Algo bellman_ford step 2605 current loss 0.302333, current_train_items 83392.
I0302 18:59:09.970122 22895071117440 run.py:483] Algo bellman_ford step 2606 current loss 0.504285, current_train_items 83424.
I0302 18:59:09.992211 22895071117440 run.py:483] Algo bellman_ford step 2607 current loss 0.668860, current_train_items 83456.
I0302 18:59:10.021038 22895071117440 run.py:483] Algo bellman_ford step 2608 current loss 0.763955, current_train_items 83488.
I0302 18:59:10.052324 22895071117440 run.py:483] Algo bellman_ford step 2609 current loss 0.949457, current_train_items 83520.
I0302 18:59:10.070370 22895071117440 run.py:483] Algo bellman_ford step 2610 current loss 0.342499, current_train_items 83552.
I0302 18:59:10.085602 22895071117440 run.py:483] Algo bellman_ford step 2611 current loss 0.524950, current_train_items 83584.
I0302 18:59:10.106856 22895071117440 run.py:483] Algo bellman_ford step 2612 current loss 0.634771, current_train_items 83616.
I0302 18:59:10.134846 22895071117440 run.py:483] Algo bellman_ford step 2613 current loss 0.753809, current_train_items 83648.
I0302 18:59:10.164791 22895071117440 run.py:483] Algo bellman_ford step 2614 current loss 0.844711, current_train_items 83680.
I0302 18:59:10.182688 22895071117440 run.py:483] Algo bellman_ford step 2615 current loss 0.380756, current_train_items 83712.
I0302 18:59:10.197612 22895071117440 run.py:483] Algo bellman_ford step 2616 current loss 0.433882, current_train_items 83744.
I0302 18:59:10.220299 22895071117440 run.py:483] Algo bellman_ford step 2617 current loss 0.722742, current_train_items 83776.
I0302 18:59:10.249261 22895071117440 run.py:483] Algo bellman_ford step 2618 current loss 0.888338, current_train_items 83808.
I0302 18:59:10.278768 22895071117440 run.py:483] Algo bellman_ford step 2619 current loss 0.839849, current_train_items 83840.
I0302 18:59:10.296725 22895071117440 run.py:483] Algo bellman_ford step 2620 current loss 0.350210, current_train_items 83872.
I0302 18:59:10.312015 22895071117440 run.py:483] Algo bellman_ford step 2621 current loss 0.582576, current_train_items 83904.
I0302 18:59:10.333839 22895071117440 run.py:483] Algo bellman_ford step 2622 current loss 0.800362, current_train_items 83936.
I0302 18:59:10.361593 22895071117440 run.py:483] Algo bellman_ford step 2623 current loss 0.801198, current_train_items 83968.
I0302 18:59:10.391479 22895071117440 run.py:483] Algo bellman_ford step 2624 current loss 0.886188, current_train_items 84000.
I0302 18:59:10.409745 22895071117440 run.py:483] Algo bellman_ford step 2625 current loss 0.368763, current_train_items 84032.
I0302 18:59:10.424995 22895071117440 run.py:483] Algo bellman_ford step 2626 current loss 0.507336, current_train_items 84064.
I0302 18:59:10.446880 22895071117440 run.py:483] Algo bellman_ford step 2627 current loss 0.761063, current_train_items 84096.
I0302 18:59:10.475179 22895071117440 run.py:483] Algo bellman_ford step 2628 current loss 0.787679, current_train_items 84128.
I0302 18:59:10.505529 22895071117440 run.py:483] Algo bellman_ford step 2629 current loss 1.172866, current_train_items 84160.
I0302 18:59:10.523472 22895071117440 run.py:483] Algo bellman_ford step 2630 current loss 0.305118, current_train_items 84192.
I0302 18:59:10.538745 22895071117440 run.py:483] Algo bellman_ford step 2631 current loss 0.556830, current_train_items 84224.
I0302 18:59:10.560317 22895071117440 run.py:483] Algo bellman_ford step 2632 current loss 0.705813, current_train_items 84256.
I0302 18:59:10.587692 22895071117440 run.py:483] Algo bellman_ford step 2633 current loss 0.739626, current_train_items 84288.
I0302 18:59:10.620384 22895071117440 run.py:483] Algo bellman_ford step 2634 current loss 1.048178, current_train_items 84320.
I0302 18:59:10.638528 22895071117440 run.py:483] Algo bellman_ford step 2635 current loss 0.235735, current_train_items 84352.
I0302 18:59:10.653681 22895071117440 run.py:483] Algo bellman_ford step 2636 current loss 0.549625, current_train_items 84384.
I0302 18:59:10.675570 22895071117440 run.py:483] Algo bellman_ford step 2637 current loss 0.880177, current_train_items 84416.
I0302 18:59:10.704254 22895071117440 run.py:483] Algo bellman_ford step 2638 current loss 0.983305, current_train_items 84448.
I0302 18:59:10.735833 22895071117440 run.py:483] Algo bellman_ford step 2639 current loss 1.001722, current_train_items 84480.
I0302 18:59:10.753500 22895071117440 run.py:483] Algo bellman_ford step 2640 current loss 0.393533, current_train_items 84512.
I0302 18:59:10.768696 22895071117440 run.py:483] Algo bellman_ford step 2641 current loss 0.527463, current_train_items 84544.
I0302 18:59:10.789559 22895071117440 run.py:483] Algo bellman_ford step 2642 current loss 0.699348, current_train_items 84576.
I0302 18:59:10.817259 22895071117440 run.py:483] Algo bellman_ford step 2643 current loss 0.917920, current_train_items 84608.
I0302 18:59:10.849054 22895071117440 run.py:483] Algo bellman_ford step 2644 current loss 1.113790, current_train_items 84640.
I0302 18:59:10.866985 22895071117440 run.py:483] Algo bellman_ford step 2645 current loss 0.372507, current_train_items 84672.
I0302 18:59:10.882096 22895071117440 run.py:483] Algo bellman_ford step 2646 current loss 0.563452, current_train_items 84704.
I0302 18:59:10.903517 22895071117440 run.py:483] Algo bellman_ford step 2647 current loss 0.740403, current_train_items 84736.
I0302 18:59:10.932265 22895071117440 run.py:483] Algo bellman_ford step 2648 current loss 0.795542, current_train_items 84768.
I0302 18:59:10.965000 22895071117440 run.py:483] Algo bellman_ford step 2649 current loss 0.985064, current_train_items 84800.
I0302 18:59:10.982851 22895071117440 run.py:483] Algo bellman_ford step 2650 current loss 0.305437, current_train_items 84832.
I0302 18:59:10.990952 22895071117440 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:10.991059 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:11.006737 22895071117440 run.py:483] Algo bellman_ford step 2651 current loss 0.500185, current_train_items 84864.
I0302 18:59:11.028611 22895071117440 run.py:483] Algo bellman_ford step 2652 current loss 0.676817, current_train_items 84896.
I0302 18:59:11.058180 22895071117440 run.py:483] Algo bellman_ford step 2653 current loss 0.849527, current_train_items 84928.
I0302 18:59:11.089458 22895071117440 run.py:483] Algo bellman_ford step 2654 current loss 0.915968, current_train_items 84960.
I0302 18:59:11.108071 22895071117440 run.py:483] Algo bellman_ford step 2655 current loss 0.257557, current_train_items 84992.
I0302 18:59:11.123587 22895071117440 run.py:483] Algo bellman_ford step 2656 current loss 0.576990, current_train_items 85024.
I0302 18:59:11.145117 22895071117440 run.py:483] Algo bellman_ford step 2657 current loss 0.693002, current_train_items 85056.
I0302 18:59:11.172189 22895071117440 run.py:483] Algo bellman_ford step 2658 current loss 0.784155, current_train_items 85088.
I0302 18:59:11.205114 22895071117440 run.py:483] Algo bellman_ford step 2659 current loss 0.978065, current_train_items 85120.
I0302 18:59:11.223713 22895071117440 run.py:483] Algo bellman_ford step 2660 current loss 0.376172, current_train_items 85152.
I0302 18:59:11.239957 22895071117440 run.py:483] Algo bellman_ford step 2661 current loss 0.575387, current_train_items 85184.
I0302 18:59:11.261261 22895071117440 run.py:483] Algo bellman_ford step 2662 current loss 0.791208, current_train_items 85216.
I0302 18:59:11.289562 22895071117440 run.py:483] Algo bellman_ford step 2663 current loss 0.862821, current_train_items 85248.
I0302 18:59:11.322843 22895071117440 run.py:483] Algo bellman_ford step 2664 current loss 0.955276, current_train_items 85280.
I0302 18:59:11.340630 22895071117440 run.py:483] Algo bellman_ford step 2665 current loss 0.276897, current_train_items 85312.
I0302 18:59:11.356009 22895071117440 run.py:483] Algo bellman_ford step 2666 current loss 0.532481, current_train_items 85344.
I0302 18:59:11.378386 22895071117440 run.py:483] Algo bellman_ford step 2667 current loss 0.778832, current_train_items 85376.
I0302 18:59:11.406938 22895071117440 run.py:483] Algo bellman_ford step 2668 current loss 0.894759, current_train_items 85408.
I0302 18:59:11.436915 22895071117440 run.py:483] Algo bellman_ford step 2669 current loss 1.145087, current_train_items 85440.
I0302 18:59:11.455264 22895071117440 run.py:483] Algo bellman_ford step 2670 current loss 0.352093, current_train_items 85472.
I0302 18:59:11.471189 22895071117440 run.py:483] Algo bellman_ford step 2671 current loss 0.493435, current_train_items 85504.
I0302 18:59:11.493018 22895071117440 run.py:483] Algo bellman_ford step 2672 current loss 0.644752, current_train_items 85536.
I0302 18:59:11.522294 22895071117440 run.py:483] Algo bellman_ford step 2673 current loss 0.780006, current_train_items 85568.
I0302 18:59:11.554623 22895071117440 run.py:483] Algo bellman_ford step 2674 current loss 0.830934, current_train_items 85600.
I0302 18:59:11.573178 22895071117440 run.py:483] Algo bellman_ford step 2675 current loss 0.305884, current_train_items 85632.
I0302 18:59:11.588686 22895071117440 run.py:483] Algo bellman_ford step 2676 current loss 0.574743, current_train_items 85664.
I0302 18:59:11.611049 22895071117440 run.py:483] Algo bellman_ford step 2677 current loss 0.745790, current_train_items 85696.
I0302 18:59:11.637890 22895071117440 run.py:483] Algo bellman_ford step 2678 current loss 0.652615, current_train_items 85728.
I0302 18:59:11.667723 22895071117440 run.py:483] Algo bellman_ford step 2679 current loss 0.835969, current_train_items 85760.
I0302 18:59:11.685625 22895071117440 run.py:483] Algo bellman_ford step 2680 current loss 0.320466, current_train_items 85792.
I0302 18:59:11.701140 22895071117440 run.py:483] Algo bellman_ford step 2681 current loss 0.591823, current_train_items 85824.
I0302 18:59:11.723048 22895071117440 run.py:483] Algo bellman_ford step 2682 current loss 0.848180, current_train_items 85856.
I0302 18:59:11.752354 22895071117440 run.py:483] Algo bellman_ford step 2683 current loss 0.809988, current_train_items 85888.
I0302 18:59:11.783659 22895071117440 run.py:483] Algo bellman_ford step 2684 current loss 0.897068, current_train_items 85920.
I0302 18:59:11.802082 22895071117440 run.py:483] Algo bellman_ford step 2685 current loss 0.282290, current_train_items 85952.
I0302 18:59:11.817514 22895071117440 run.py:483] Algo bellman_ford step 2686 current loss 0.607482, current_train_items 85984.
I0302 18:59:11.839142 22895071117440 run.py:483] Algo bellman_ford step 2687 current loss 0.732656, current_train_items 86016.
I0302 18:59:11.867282 22895071117440 run.py:483] Algo bellman_ford step 2688 current loss 0.814289, current_train_items 86048.
I0302 18:59:11.896777 22895071117440 run.py:483] Algo bellman_ford step 2689 current loss 0.814647, current_train_items 86080.
I0302 18:59:11.915388 22895071117440 run.py:483] Algo bellman_ford step 2690 current loss 0.317318, current_train_items 86112.
I0302 18:59:11.931111 22895071117440 run.py:483] Algo bellman_ford step 2691 current loss 0.586382, current_train_items 86144.
I0302 18:59:11.953843 22895071117440 run.py:483] Algo bellman_ford step 2692 current loss 0.789140, current_train_items 86176.
I0302 18:59:11.983009 22895071117440 run.py:483] Algo bellman_ford step 2693 current loss 0.857894, current_train_items 86208.
I0302 18:59:12.014039 22895071117440 run.py:483] Algo bellman_ford step 2694 current loss 0.893009, current_train_items 86240.
I0302 18:59:12.032147 22895071117440 run.py:483] Algo bellman_ford step 2695 current loss 0.333741, current_train_items 86272.
I0302 18:59:12.047181 22895071117440 run.py:483] Algo bellman_ford step 2696 current loss 0.518003, current_train_items 86304.
I0302 18:59:12.069152 22895071117440 run.py:483] Algo bellman_ford step 2697 current loss 0.708606, current_train_items 86336.
I0302 18:59:12.096791 22895071117440 run.py:483] Algo bellman_ford step 2698 current loss 0.782239, current_train_items 86368.
I0302 18:59:12.126132 22895071117440 run.py:483] Algo bellman_ford step 2699 current loss 1.142699, current_train_items 86400.
I0302 18:59:12.144532 22895071117440 run.py:483] Algo bellman_ford step 2700 current loss 0.362568, current_train_items 86432.
I0302 18:59:12.152391 22895071117440 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:12.152494 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:59:12.168447 22895071117440 run.py:483] Algo bellman_ford step 2701 current loss 0.515014, current_train_items 86464.
I0302 18:59:12.190726 22895071117440 run.py:483] Algo bellman_ford step 2702 current loss 0.700723, current_train_items 86496.
I0302 18:59:12.221205 22895071117440 run.py:483] Algo bellman_ford step 2703 current loss 0.887778, current_train_items 86528.
I0302 18:59:12.254582 22895071117440 run.py:483] Algo bellman_ford step 2704 current loss 0.936531, current_train_items 86560.
I0302 18:59:12.273091 22895071117440 run.py:483] Algo bellman_ford step 2705 current loss 0.339394, current_train_items 86592.
I0302 18:59:12.288233 22895071117440 run.py:483] Algo bellman_ford step 2706 current loss 0.577796, current_train_items 86624.
I0302 18:59:12.310415 22895071117440 run.py:483] Algo bellman_ford step 2707 current loss 0.747292, current_train_items 86656.
I0302 18:59:12.337425 22895071117440 run.py:483] Algo bellman_ford step 2708 current loss 0.758995, current_train_items 86688.
I0302 18:59:12.366677 22895071117440 run.py:483] Algo bellman_ford step 2709 current loss 0.748210, current_train_items 86720.
I0302 18:59:12.385022 22895071117440 run.py:483] Algo bellman_ford step 2710 current loss 0.320040, current_train_items 86752.
I0302 18:59:12.400689 22895071117440 run.py:483] Algo bellman_ford step 2711 current loss 0.471691, current_train_items 86784.
I0302 18:59:12.422856 22895071117440 run.py:483] Algo bellman_ford step 2712 current loss 0.804687, current_train_items 86816.
I0302 18:59:12.451842 22895071117440 run.py:483] Algo bellman_ford step 2713 current loss 0.906041, current_train_items 86848.
I0302 18:59:12.484776 22895071117440 run.py:483] Algo bellman_ford step 2714 current loss 0.984689, current_train_items 86880.
I0302 18:59:12.502874 22895071117440 run.py:483] Algo bellman_ford step 2715 current loss 0.350650, current_train_items 86912.
I0302 18:59:12.517756 22895071117440 run.py:483] Algo bellman_ford step 2716 current loss 0.404593, current_train_items 86944.
I0302 18:59:12.540547 22895071117440 run.py:483] Algo bellman_ford step 2717 current loss 0.816897, current_train_items 86976.
I0302 18:59:12.567328 22895071117440 run.py:483] Algo bellman_ford step 2718 current loss 0.726642, current_train_items 87008.
I0302 18:59:12.598565 22895071117440 run.py:483] Algo bellman_ford step 2719 current loss 0.885359, current_train_items 87040.
I0302 18:59:12.616635 22895071117440 run.py:483] Algo bellman_ford step 2720 current loss 0.360066, current_train_items 87072.
I0302 18:59:12.631996 22895071117440 run.py:483] Algo bellman_ford step 2721 current loss 0.652680, current_train_items 87104.
I0302 18:59:12.653432 22895071117440 run.py:483] Algo bellman_ford step 2722 current loss 0.739964, current_train_items 87136.
I0302 18:59:12.682797 22895071117440 run.py:483] Algo bellman_ford step 2723 current loss 0.858414, current_train_items 87168.
I0302 18:59:12.713765 22895071117440 run.py:483] Algo bellman_ford step 2724 current loss 0.826243, current_train_items 87200.
I0302 18:59:12.731598 22895071117440 run.py:483] Algo bellman_ford step 2725 current loss 0.330572, current_train_items 87232.
I0302 18:59:12.746797 22895071117440 run.py:483] Algo bellman_ford step 2726 current loss 0.650137, current_train_items 87264.
I0302 18:59:12.768890 22895071117440 run.py:483] Algo bellman_ford step 2727 current loss 0.784046, current_train_items 87296.
I0302 18:59:12.797103 22895071117440 run.py:483] Algo bellman_ford step 2728 current loss 0.857924, current_train_items 87328.
I0302 18:59:12.827755 22895071117440 run.py:483] Algo bellman_ford step 2729 current loss 0.953655, current_train_items 87360.
I0302 18:59:12.845844 22895071117440 run.py:483] Algo bellman_ford step 2730 current loss 0.339367, current_train_items 87392.
I0302 18:59:12.861206 22895071117440 run.py:483] Algo bellman_ford step 2731 current loss 0.458685, current_train_items 87424.
I0302 18:59:12.882200 22895071117440 run.py:483] Algo bellman_ford step 2732 current loss 0.751078, current_train_items 87456.
I0302 18:59:12.909214 22895071117440 run.py:483] Algo bellman_ford step 2733 current loss 0.848052, current_train_items 87488.
I0302 18:59:12.940447 22895071117440 run.py:483] Algo bellman_ford step 2734 current loss 0.873042, current_train_items 87520.
I0302 18:59:12.958731 22895071117440 run.py:483] Algo bellman_ford step 2735 current loss 0.340673, current_train_items 87552.
I0302 18:59:12.973538 22895071117440 run.py:483] Algo bellman_ford step 2736 current loss 0.483542, current_train_items 87584.
I0302 18:59:12.995262 22895071117440 run.py:483] Algo bellman_ford step 2737 current loss 0.651948, current_train_items 87616.
I0302 18:59:13.022550 22895071117440 run.py:483] Algo bellman_ford step 2738 current loss 0.675746, current_train_items 87648.
I0302 18:59:13.052293 22895071117440 run.py:483] Algo bellman_ford step 2739 current loss 0.904381, current_train_items 87680.
I0302 18:59:13.070173 22895071117440 run.py:483] Algo bellman_ford step 2740 current loss 0.367983, current_train_items 87712.
I0302 18:59:13.085250 22895071117440 run.py:483] Algo bellman_ford step 2741 current loss 0.572248, current_train_items 87744.
I0302 18:59:13.107095 22895071117440 run.py:483] Algo bellman_ford step 2742 current loss 0.720205, current_train_items 87776.
I0302 18:59:13.136544 22895071117440 run.py:483] Algo bellman_ford step 2743 current loss 0.793086, current_train_items 87808.
I0302 18:59:13.167206 22895071117440 run.py:483] Algo bellman_ford step 2744 current loss 0.877644, current_train_items 87840.
I0302 18:59:13.185015 22895071117440 run.py:483] Algo bellman_ford step 2745 current loss 0.299316, current_train_items 87872.
I0302 18:59:13.200128 22895071117440 run.py:483] Algo bellman_ford step 2746 current loss 0.562798, current_train_items 87904.
I0302 18:59:13.222072 22895071117440 run.py:483] Algo bellman_ford step 2747 current loss 0.739334, current_train_items 87936.
I0302 18:59:13.250966 22895071117440 run.py:483] Algo bellman_ford step 2748 current loss 0.847366, current_train_items 87968.
I0302 18:59:13.281534 22895071117440 run.py:483] Algo bellman_ford step 2749 current loss 0.920716, current_train_items 88000.
I0302 18:59:13.299286 22895071117440 run.py:483] Algo bellman_ford step 2750 current loss 0.341381, current_train_items 88032.
I0302 18:59:13.306840 22895071117440 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:13.306953 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:59:13.322980 22895071117440 run.py:483] Algo bellman_ford step 2751 current loss 0.616672, current_train_items 88064.
I0302 18:59:13.345307 22895071117440 run.py:483] Algo bellman_ford step 2752 current loss 0.814371, current_train_items 88096.
I0302 18:59:13.374424 22895071117440 run.py:483] Algo bellman_ford step 2753 current loss 0.838617, current_train_items 88128.
I0302 18:59:13.404384 22895071117440 run.py:483] Algo bellman_ford step 2754 current loss 0.844273, current_train_items 88160.
I0302 18:59:13.422415 22895071117440 run.py:483] Algo bellman_ford step 2755 current loss 0.345376, current_train_items 88192.
I0302 18:59:13.438073 22895071117440 run.py:483] Algo bellman_ford step 2756 current loss 0.556589, current_train_items 88224.
I0302 18:59:13.458847 22895071117440 run.py:483] Algo bellman_ford step 2757 current loss 0.621064, current_train_items 88256.
I0302 18:59:13.487462 22895071117440 run.py:483] Algo bellman_ford step 2758 current loss 0.842013, current_train_items 88288.
I0302 18:59:13.520391 22895071117440 run.py:483] Algo bellman_ford step 2759 current loss 1.108884, current_train_items 88320.
I0302 18:59:13.538529 22895071117440 run.py:483] Algo bellman_ford step 2760 current loss 0.357262, current_train_items 88352.
I0302 18:59:13.554466 22895071117440 run.py:483] Algo bellman_ford step 2761 current loss 0.585830, current_train_items 88384.
I0302 18:59:13.576146 22895071117440 run.py:483] Algo bellman_ford step 2762 current loss 0.848191, current_train_items 88416.
I0302 18:59:13.604537 22895071117440 run.py:483] Algo bellman_ford step 2763 current loss 0.711385, current_train_items 88448.
I0302 18:59:13.635083 22895071117440 run.py:483] Algo bellman_ford step 2764 current loss 0.935490, current_train_items 88480.
I0302 18:59:13.652681 22895071117440 run.py:483] Algo bellman_ford step 2765 current loss 0.309641, current_train_items 88512.
I0302 18:59:13.668146 22895071117440 run.py:483] Algo bellman_ford step 2766 current loss 0.630611, current_train_items 88544.
I0302 18:59:13.689707 22895071117440 run.py:483] Algo bellman_ford step 2767 current loss 0.819572, current_train_items 88576.
I0302 18:59:13.718742 22895071117440 run.py:483] Algo bellman_ford step 2768 current loss 0.833151, current_train_items 88608.
I0302 18:59:13.749407 22895071117440 run.py:483] Algo bellman_ford step 2769 current loss 0.904905, current_train_items 88640.
I0302 18:59:13.767351 22895071117440 run.py:483] Algo bellman_ford step 2770 current loss 0.294615, current_train_items 88672.
I0302 18:59:13.783083 22895071117440 run.py:483] Algo bellman_ford step 2771 current loss 0.505648, current_train_items 88704.
I0302 18:59:13.804849 22895071117440 run.py:483] Algo bellman_ford step 2772 current loss 0.773030, current_train_items 88736.
I0302 18:59:13.832317 22895071117440 run.py:483] Algo bellman_ford step 2773 current loss 0.750723, current_train_items 88768.
I0302 18:59:13.863008 22895071117440 run.py:483] Algo bellman_ford step 2774 current loss 0.922256, current_train_items 88800.
I0302 18:59:13.881081 22895071117440 run.py:483] Algo bellman_ford step 2775 current loss 0.273591, current_train_items 88832.
I0302 18:59:13.897091 22895071117440 run.py:483] Algo bellman_ford step 2776 current loss 0.600873, current_train_items 88864.
I0302 18:59:13.918262 22895071117440 run.py:483] Algo bellman_ford step 2777 current loss 0.740338, current_train_items 88896.
I0302 18:59:13.945844 22895071117440 run.py:483] Algo bellman_ford step 2778 current loss 0.743340, current_train_items 88928.
I0302 18:59:13.976981 22895071117440 run.py:483] Algo bellman_ford step 2779 current loss 0.951094, current_train_items 88960.
I0302 18:59:13.994771 22895071117440 run.py:483] Algo bellman_ford step 2780 current loss 0.337584, current_train_items 88992.
I0302 18:59:14.009877 22895071117440 run.py:483] Algo bellman_ford step 2781 current loss 0.563683, current_train_items 89024.
I0302 18:59:14.031624 22895071117440 run.py:483] Algo bellman_ford step 2782 current loss 0.657364, current_train_items 89056.
I0302 18:59:14.059398 22895071117440 run.py:483] Algo bellman_ford step 2783 current loss 0.749523, current_train_items 89088.
I0302 18:59:14.090662 22895071117440 run.py:483] Algo bellman_ford step 2784 current loss 0.881477, current_train_items 89120.
I0302 18:59:14.108869 22895071117440 run.py:483] Algo bellman_ford step 2785 current loss 0.377077, current_train_items 89152.
I0302 18:59:14.124310 22895071117440 run.py:483] Algo bellman_ford step 2786 current loss 0.542795, current_train_items 89184.
I0302 18:59:14.144568 22895071117440 run.py:483] Algo bellman_ford step 2787 current loss 0.578100, current_train_items 89216.
I0302 18:59:14.171895 22895071117440 run.py:483] Algo bellman_ford step 2788 current loss 0.780454, current_train_items 89248.
I0302 18:59:14.199688 22895071117440 run.py:483] Algo bellman_ford step 2789 current loss 0.761734, current_train_items 89280.
I0302 18:59:14.217684 22895071117440 run.py:483] Algo bellman_ford step 2790 current loss 0.386443, current_train_items 89312.
I0302 18:59:14.233137 22895071117440 run.py:483] Algo bellman_ford step 2791 current loss 0.541755, current_train_items 89344.
I0302 18:59:14.254445 22895071117440 run.py:483] Algo bellman_ford step 2792 current loss 0.642872, current_train_items 89376.
I0302 18:59:14.281678 22895071117440 run.py:483] Algo bellman_ford step 2793 current loss 0.746443, current_train_items 89408.
I0302 18:59:14.314422 22895071117440 run.py:483] Algo bellman_ford step 2794 current loss 0.913477, current_train_items 89440.
I0302 18:59:14.332239 22895071117440 run.py:483] Algo bellman_ford step 2795 current loss 0.372580, current_train_items 89472.
I0302 18:59:14.347942 22895071117440 run.py:483] Algo bellman_ford step 2796 current loss 0.623538, current_train_items 89504.
I0302 18:59:14.369082 22895071117440 run.py:483] Algo bellman_ford step 2797 current loss 0.770167, current_train_items 89536.
I0302 18:59:14.397696 22895071117440 run.py:483] Algo bellman_ford step 2798 current loss 0.762866, current_train_items 89568.
I0302 18:59:14.427966 22895071117440 run.py:483] Algo bellman_ford step 2799 current loss 0.827281, current_train_items 89600.
I0302 18:59:14.446261 22895071117440 run.py:483] Algo bellman_ford step 2800 current loss 0.349034, current_train_items 89632.
I0302 18:59:14.466510 22895071117440 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:14.466616 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 18:59:14.482517 22895071117440 run.py:483] Algo bellman_ford step 2801 current loss 0.502098, current_train_items 89664.
I0302 18:59:14.504913 22895071117440 run.py:483] Algo bellman_ford step 2802 current loss 0.752905, current_train_items 89696.
I0302 18:59:14.533857 22895071117440 run.py:483] Algo bellman_ford step 2803 current loss 0.802380, current_train_items 89728.
I0302 18:59:14.565180 22895071117440 run.py:483] Algo bellman_ford step 2804 current loss 0.864383, current_train_items 89760.
I0302 18:59:14.583732 22895071117440 run.py:483] Algo bellman_ford step 2805 current loss 0.302882, current_train_items 89792.
I0302 18:59:14.599370 22895071117440 run.py:483] Algo bellman_ford step 2806 current loss 0.603457, current_train_items 89824.
I0302 18:59:14.620489 22895071117440 run.py:483] Algo bellman_ford step 2807 current loss 0.658493, current_train_items 89856.
I0302 18:59:14.649683 22895071117440 run.py:483] Algo bellman_ford step 2808 current loss 0.847510, current_train_items 89888.
I0302 18:59:14.681106 22895071117440 run.py:483] Algo bellman_ford step 2809 current loss 0.938494, current_train_items 89920.
I0302 18:59:14.699167 22895071117440 run.py:483] Algo bellman_ford step 2810 current loss 0.311761, current_train_items 89952.
I0302 18:59:14.714844 22895071117440 run.py:483] Algo bellman_ford step 2811 current loss 0.604487, current_train_items 89984.
I0302 18:59:14.735913 22895071117440 run.py:483] Algo bellman_ford step 2812 current loss 0.633033, current_train_items 90016.
I0302 18:59:14.765239 22895071117440 run.py:483] Algo bellman_ford step 2813 current loss 0.886692, current_train_items 90048.
I0302 18:59:14.795471 22895071117440 run.py:483] Algo bellman_ford step 2814 current loss 0.895267, current_train_items 90080.
I0302 18:59:14.813192 22895071117440 run.py:483] Algo bellman_ford step 2815 current loss 0.336574, current_train_items 90112.
I0302 18:59:14.828658 22895071117440 run.py:483] Algo bellman_ford step 2816 current loss 0.590937, current_train_items 90144.
I0302 18:59:14.850665 22895071117440 run.py:483] Algo bellman_ford step 2817 current loss 0.867263, current_train_items 90176.
I0302 18:59:14.876168 22895071117440 run.py:483] Algo bellman_ford step 2818 current loss 0.719754, current_train_items 90208.
I0302 18:59:14.906699 22895071117440 run.py:483] Algo bellman_ford step 2819 current loss 1.087038, current_train_items 90240.
I0302 18:59:14.924417 22895071117440 run.py:483] Algo bellman_ford step 2820 current loss 0.360789, current_train_items 90272.
I0302 18:59:14.939641 22895071117440 run.py:483] Algo bellman_ford step 2821 current loss 0.529614, current_train_items 90304.
I0302 18:59:14.961200 22895071117440 run.py:483] Algo bellman_ford step 2822 current loss 0.777107, current_train_items 90336.
I0302 18:59:14.989390 22895071117440 run.py:483] Algo bellman_ford step 2823 current loss 0.930500, current_train_items 90368.
I0302 18:59:15.022284 22895071117440 run.py:483] Algo bellman_ford step 2824 current loss 1.062772, current_train_items 90400.
I0302 18:59:15.040522 22895071117440 run.py:483] Algo bellman_ford step 2825 current loss 0.277894, current_train_items 90432.
I0302 18:59:15.055533 22895071117440 run.py:483] Algo bellman_ford step 2826 current loss 0.669433, current_train_items 90464.
I0302 18:59:15.077606 22895071117440 run.py:483] Algo bellman_ford step 2827 current loss 0.765978, current_train_items 90496.
I0302 18:59:15.106168 22895071117440 run.py:483] Algo bellman_ford step 2828 current loss 0.863249, current_train_items 90528.
I0302 18:59:15.135647 22895071117440 run.py:483] Algo bellman_ford step 2829 current loss 0.850732, current_train_items 90560.
I0302 18:59:15.153692 22895071117440 run.py:483] Algo bellman_ford step 2830 current loss 0.253535, current_train_items 90592.
I0302 18:59:15.169376 22895071117440 run.py:483] Algo bellman_ford step 2831 current loss 0.565539, current_train_items 90624.
I0302 18:59:15.190639 22895071117440 run.py:483] Algo bellman_ford step 2832 current loss 0.641538, current_train_items 90656.
I0302 18:59:15.218285 22895071117440 run.py:483] Algo bellman_ford step 2833 current loss 0.783385, current_train_items 90688.
I0302 18:59:15.248787 22895071117440 run.py:483] Algo bellman_ford step 2834 current loss 0.997420, current_train_items 90720.
I0302 18:59:15.266956 22895071117440 run.py:483] Algo bellman_ford step 2835 current loss 0.356524, current_train_items 90752.
I0302 18:59:15.281831 22895071117440 run.py:483] Algo bellman_ford step 2836 current loss 0.501339, current_train_items 90784.
I0302 18:59:15.303149 22895071117440 run.py:483] Algo bellman_ford step 2837 current loss 0.605083, current_train_items 90816.
I0302 18:59:15.331382 22895071117440 run.py:483] Algo bellman_ford step 2838 current loss 0.888037, current_train_items 90848.
I0302 18:59:15.364370 22895071117440 run.py:483] Algo bellman_ford step 2839 current loss 1.004461, current_train_items 90880.
I0302 18:59:15.382429 22895071117440 run.py:483] Algo bellman_ford step 2840 current loss 0.386613, current_train_items 90912.
I0302 18:59:15.397398 22895071117440 run.py:483] Algo bellman_ford step 2841 current loss 0.540843, current_train_items 90944.
I0302 18:59:15.419210 22895071117440 run.py:483] Algo bellman_ford step 2842 current loss 0.923924, current_train_items 90976.
I0302 18:59:15.448233 22895071117440 run.py:483] Algo bellman_ford step 2843 current loss 0.914462, current_train_items 91008.
I0302 18:59:15.479690 22895071117440 run.py:483] Algo bellman_ford step 2844 current loss 1.044926, current_train_items 91040.
I0302 18:59:15.497718 22895071117440 run.py:483] Algo bellman_ford step 2845 current loss 0.325306, current_train_items 91072.
I0302 18:59:15.513497 22895071117440 run.py:483] Algo bellman_ford step 2846 current loss 0.591477, current_train_items 91104.
I0302 18:59:15.535068 22895071117440 run.py:483] Algo bellman_ford step 2847 current loss 0.743235, current_train_items 91136.
I0302 18:59:15.563236 22895071117440 run.py:483] Algo bellman_ford step 2848 current loss 0.763073, current_train_items 91168.
I0302 18:59:15.595724 22895071117440 run.py:483] Algo bellman_ford step 2849 current loss 0.963492, current_train_items 91200.
I0302 18:59:15.613654 22895071117440 run.py:483] Algo bellman_ford step 2850 current loss 0.365987, current_train_items 91232.
I0302 18:59:15.621764 22895071117440 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:15.621875 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:15.637537 22895071117440 run.py:483] Algo bellman_ford step 2851 current loss 0.469955, current_train_items 91264.
I0302 18:59:15.659286 22895071117440 run.py:483] Algo bellman_ford step 2852 current loss 0.764672, current_train_items 91296.
I0302 18:59:15.688389 22895071117440 run.py:483] Algo bellman_ford step 2853 current loss 0.971738, current_train_items 91328.
I0302 18:59:15.719740 22895071117440 run.py:483] Algo bellman_ford step 2854 current loss 1.306778, current_train_items 91360.
I0302 18:59:15.737826 22895071117440 run.py:483] Algo bellman_ford step 2855 current loss 0.361630, current_train_items 91392.
I0302 18:59:15.753362 22895071117440 run.py:483] Algo bellman_ford step 2856 current loss 0.609390, current_train_items 91424.
I0302 18:59:15.775306 22895071117440 run.py:483] Algo bellman_ford step 2857 current loss 0.858585, current_train_items 91456.
I0302 18:59:15.802099 22895071117440 run.py:483] Algo bellman_ford step 2858 current loss 0.700384, current_train_items 91488.
I0302 18:59:15.831597 22895071117440 run.py:483] Algo bellman_ford step 2859 current loss 0.904164, current_train_items 91520.
I0302 18:59:15.850197 22895071117440 run.py:483] Algo bellman_ford step 2860 current loss 0.470886, current_train_items 91552.
I0302 18:59:15.866027 22895071117440 run.py:483] Algo bellman_ford step 2861 current loss 0.613914, current_train_items 91584.
I0302 18:59:15.887710 22895071117440 run.py:483] Algo bellman_ford step 2862 current loss 0.814838, current_train_items 91616.
I0302 18:59:15.916944 22895071117440 run.py:483] Algo bellman_ford step 2863 current loss 0.982548, current_train_items 91648.
I0302 18:59:15.945383 22895071117440 run.py:483] Algo bellman_ford step 2864 current loss 0.880731, current_train_items 91680.
I0302 18:59:15.963613 22895071117440 run.py:483] Algo bellman_ford step 2865 current loss 0.400137, current_train_items 91712.
I0302 18:59:15.979045 22895071117440 run.py:483] Algo bellman_ford step 2866 current loss 0.523484, current_train_items 91744.
I0302 18:59:16.000812 22895071117440 run.py:483] Algo bellman_ford step 2867 current loss 0.713826, current_train_items 91776.
I0302 18:59:16.028774 22895071117440 run.py:483] Algo bellman_ford step 2868 current loss 0.751358, current_train_items 91808.
I0302 18:59:16.060083 22895071117440 run.py:483] Algo bellman_ford step 2869 current loss 1.032432, current_train_items 91840.
I0302 18:59:16.078835 22895071117440 run.py:483] Algo bellman_ford step 2870 current loss 0.296067, current_train_items 91872.
I0302 18:59:16.094573 22895071117440 run.py:483] Algo bellman_ford step 2871 current loss 0.629912, current_train_items 91904.
I0302 18:59:16.116796 22895071117440 run.py:483] Algo bellman_ford step 2872 current loss 0.847253, current_train_items 91936.
I0302 18:59:16.145917 22895071117440 run.py:483] Algo bellman_ford step 2873 current loss 1.125632, current_train_items 91968.
I0302 18:59:16.175823 22895071117440 run.py:483] Algo bellman_ford step 2874 current loss 1.004273, current_train_items 92000.
I0302 18:59:16.194324 22895071117440 run.py:483] Algo bellman_ford step 2875 current loss 0.372739, current_train_items 92032.
I0302 18:59:16.209930 22895071117440 run.py:483] Algo bellman_ford step 2876 current loss 0.681413, current_train_items 92064.
I0302 18:59:16.231381 22895071117440 run.py:483] Algo bellman_ford step 2877 current loss 0.695520, current_train_items 92096.
I0302 18:59:16.260845 22895071117440 run.py:483] Algo bellman_ford step 2878 current loss 0.862522, current_train_items 92128.
I0302 18:59:16.290764 22895071117440 run.py:483] Algo bellman_ford step 2879 current loss 0.875755, current_train_items 92160.
I0302 18:59:16.309172 22895071117440 run.py:483] Algo bellman_ford step 2880 current loss 0.396140, current_train_items 92192.
I0302 18:59:16.324449 22895071117440 run.py:483] Algo bellman_ford step 2881 current loss 0.528559, current_train_items 92224.
I0302 18:59:16.346170 22895071117440 run.py:483] Algo bellman_ford step 2882 current loss 0.744944, current_train_items 92256.
I0302 18:59:16.374604 22895071117440 run.py:483] Algo bellman_ford step 2883 current loss 0.847344, current_train_items 92288.
I0302 18:59:16.403985 22895071117440 run.py:483] Algo bellman_ford step 2884 current loss 1.052381, current_train_items 92320.
I0302 18:59:16.422538 22895071117440 run.py:483] Algo bellman_ford step 2885 current loss 0.286706, current_train_items 92352.
I0302 18:59:16.438151 22895071117440 run.py:483] Algo bellman_ford step 2886 current loss 0.543915, current_train_items 92384.
I0302 18:59:16.460203 22895071117440 run.py:483] Algo bellman_ford step 2887 current loss 0.740659, current_train_items 92416.
I0302 18:59:16.487764 22895071117440 run.py:483] Algo bellman_ford step 2888 current loss 0.773985, current_train_items 92448.
I0302 18:59:16.519217 22895071117440 run.py:483] Algo bellman_ford step 2889 current loss 0.975687, current_train_items 92480.
I0302 18:59:16.537948 22895071117440 run.py:483] Algo bellman_ford step 2890 current loss 0.349566, current_train_items 92512.
I0302 18:59:16.553534 22895071117440 run.py:483] Algo bellman_ford step 2891 current loss 0.563650, current_train_items 92544.
I0302 18:59:16.576500 22895071117440 run.py:483] Algo bellman_ford step 2892 current loss 0.793219, current_train_items 92576.
I0302 18:59:16.603758 22895071117440 run.py:483] Algo bellman_ford step 2893 current loss 0.855379, current_train_items 92608.
I0302 18:59:16.634933 22895071117440 run.py:483] Algo bellman_ford step 2894 current loss 0.925409, current_train_items 92640.
I0302 18:59:16.652776 22895071117440 run.py:483] Algo bellman_ford step 2895 current loss 0.341700, current_train_items 92672.
I0302 18:59:16.668674 22895071117440 run.py:483] Algo bellman_ford step 2896 current loss 0.635286, current_train_items 92704.
I0302 18:59:16.689165 22895071117440 run.py:483] Algo bellman_ford step 2897 current loss 0.683396, current_train_items 92736.
I0302 18:59:16.717118 22895071117440 run.py:483] Algo bellman_ford step 2898 current loss 0.722385, current_train_items 92768.
I0302 18:59:16.745238 22895071117440 run.py:483] Algo bellman_ford step 2899 current loss 0.723037, current_train_items 92800.
I0302 18:59:16.763412 22895071117440 run.py:483] Algo bellman_ford step 2900 current loss 0.279676, current_train_items 92832.
I0302 18:59:16.771258 22895071117440 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:16.771364 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 18:59:16.786963 22895071117440 run.py:483] Algo bellman_ford step 2901 current loss 0.468961, current_train_items 92864.
I0302 18:59:16.808917 22895071117440 run.py:483] Algo bellman_ford step 2902 current loss 0.748072, current_train_items 92896.
I0302 18:59:16.837875 22895071117440 run.py:483] Algo bellman_ford step 2903 current loss 0.775783, current_train_items 92928.
I0302 18:59:16.869961 22895071117440 run.py:483] Algo bellman_ford step 2904 current loss 0.927226, current_train_items 92960.
I0302 18:59:16.888287 22895071117440 run.py:483] Algo bellman_ford step 2905 current loss 0.365946, current_train_items 92992.
I0302 18:59:16.903741 22895071117440 run.py:483] Algo bellman_ford step 2906 current loss 0.543424, current_train_items 93024.
I0302 18:59:16.926089 22895071117440 run.py:483] Algo bellman_ford step 2907 current loss 0.720222, current_train_items 93056.
I0302 18:59:16.955933 22895071117440 run.py:483] Algo bellman_ford step 2908 current loss 0.871784, current_train_items 93088.
I0302 18:59:16.987359 22895071117440 run.py:483] Algo bellman_ford step 2909 current loss 0.951898, current_train_items 93120.
I0302 18:59:17.005393 22895071117440 run.py:483] Algo bellman_ford step 2910 current loss 0.339866, current_train_items 93152.
I0302 18:59:17.021320 22895071117440 run.py:483] Algo bellman_ford step 2911 current loss 0.621113, current_train_items 93184.
I0302 18:59:17.043600 22895071117440 run.py:483] Algo bellman_ford step 2912 current loss 0.649039, current_train_items 93216.
I0302 18:59:17.071518 22895071117440 run.py:483] Algo bellman_ford step 2913 current loss 0.813770, current_train_items 93248.
I0302 18:59:17.103757 22895071117440 run.py:483] Algo bellman_ford step 2914 current loss 1.032047, current_train_items 93280.
I0302 18:59:17.121564 22895071117440 run.py:483] Algo bellman_ford step 2915 current loss 0.295092, current_train_items 93312.
I0302 18:59:17.136956 22895071117440 run.py:483] Algo bellman_ford step 2916 current loss 0.519439, current_train_items 93344.
I0302 18:59:17.158433 22895071117440 run.py:483] Algo bellman_ford step 2917 current loss 0.647569, current_train_items 93376.
I0302 18:59:17.188211 22895071117440 run.py:483] Algo bellman_ford step 2918 current loss 0.855342, current_train_items 93408.
I0302 18:59:17.221120 22895071117440 run.py:483] Algo bellman_ford step 2919 current loss 0.978356, current_train_items 93440.
I0302 18:59:17.239271 22895071117440 run.py:483] Algo bellman_ford step 2920 current loss 0.329371, current_train_items 93472.
I0302 18:59:17.254716 22895071117440 run.py:483] Algo bellman_ford step 2921 current loss 0.633357, current_train_items 93504.
I0302 18:59:17.277387 22895071117440 run.py:483] Algo bellman_ford step 2922 current loss 0.771306, current_train_items 93536.
I0302 18:59:17.305227 22895071117440 run.py:483] Algo bellman_ford step 2923 current loss 0.779936, current_train_items 93568.
I0302 18:59:17.338112 22895071117440 run.py:483] Algo bellman_ford step 2924 current loss 1.009539, current_train_items 93600.
I0302 18:59:17.356356 22895071117440 run.py:483] Algo bellman_ford step 2925 current loss 0.319103, current_train_items 93632.
I0302 18:59:17.371535 22895071117440 run.py:483] Algo bellman_ford step 2926 current loss 0.530389, current_train_items 93664.
I0302 18:59:17.393934 22895071117440 run.py:483] Algo bellman_ford step 2927 current loss 0.853036, current_train_items 93696.
I0302 18:59:17.423149 22895071117440 run.py:483] Algo bellman_ford step 2928 current loss 0.716210, current_train_items 93728.
I0302 18:59:17.454438 22895071117440 run.py:483] Algo bellman_ford step 2929 current loss 0.935445, current_train_items 93760.
I0302 18:59:17.472518 22895071117440 run.py:483] Algo bellman_ford step 2930 current loss 0.340344, current_train_items 93792.
I0302 18:59:17.487718 22895071117440 run.py:483] Algo bellman_ford step 2931 current loss 0.483637, current_train_items 93824.
I0302 18:59:17.510885 22895071117440 run.py:483] Algo bellman_ford step 2932 current loss 0.802530, current_train_items 93856.
I0302 18:59:17.538556 22895071117440 run.py:483] Algo bellman_ford step 2933 current loss 0.850550, current_train_items 93888.
I0302 18:59:17.569037 22895071117440 run.py:483] Algo bellman_ford step 2934 current loss 0.842764, current_train_items 93920.
I0302 18:59:17.587648 22895071117440 run.py:483] Algo bellman_ford step 2935 current loss 0.378686, current_train_items 93952.
I0302 18:59:17.602788 22895071117440 run.py:483] Algo bellman_ford step 2936 current loss 0.480021, current_train_items 93984.
I0302 18:59:17.624571 22895071117440 run.py:483] Algo bellman_ford step 2937 current loss 0.715350, current_train_items 94016.
I0302 18:59:17.651010 22895071117440 run.py:483] Algo bellman_ford step 2938 current loss 0.670223, current_train_items 94048.
I0302 18:59:17.682038 22895071117440 run.py:483] Algo bellman_ford step 2939 current loss 0.957054, current_train_items 94080.
I0302 18:59:17.699782 22895071117440 run.py:483] Algo bellman_ford step 2940 current loss 0.465084, current_train_items 94112.
I0302 18:59:17.715456 22895071117440 run.py:483] Algo bellman_ford step 2941 current loss 0.593916, current_train_items 94144.
I0302 18:59:17.738143 22895071117440 run.py:483] Algo bellman_ford step 2942 current loss 0.755851, current_train_items 94176.
I0302 18:59:17.767809 22895071117440 run.py:483] Algo bellman_ford step 2943 current loss 0.842829, current_train_items 94208.
I0302 18:59:17.800534 22895071117440 run.py:483] Algo bellman_ford step 2944 current loss 0.870701, current_train_items 94240.
I0302 18:59:17.818775 22895071117440 run.py:483] Algo bellman_ford step 2945 current loss 0.410333, current_train_items 94272.
I0302 18:59:17.834516 22895071117440 run.py:483] Algo bellman_ford step 2946 current loss 0.572943, current_train_items 94304.
I0302 18:59:17.856928 22895071117440 run.py:483] Algo bellman_ford step 2947 current loss 0.611453, current_train_items 94336.
I0302 18:59:17.885147 22895071117440 run.py:483] Algo bellman_ford step 2948 current loss 0.742532, current_train_items 94368.
I0302 18:59:17.915536 22895071117440 run.py:483] Algo bellman_ford step 2949 current loss 0.793405, current_train_items 94400.
I0302 18:59:17.933450 22895071117440 run.py:483] Algo bellman_ford step 2950 current loss 0.343984, current_train_items 94432.
I0302 18:59:17.941323 22895071117440 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:17.941430 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 18:59:17.957569 22895071117440 run.py:483] Algo bellman_ford step 2951 current loss 0.599935, current_train_items 94464.
I0302 18:59:17.978371 22895071117440 run.py:483] Algo bellman_ford step 2952 current loss 0.507081, current_train_items 94496.
I0302 18:59:18.006401 22895071117440 run.py:483] Algo bellman_ford step 2953 current loss 0.724442, current_train_items 94528.
I0302 18:59:18.038179 22895071117440 run.py:483] Algo bellman_ford step 2954 current loss 0.947689, current_train_items 94560.
I0302 18:59:18.056577 22895071117440 run.py:483] Algo bellman_ford step 2955 current loss 0.318087, current_train_items 94592.
I0302 18:59:18.071616 22895071117440 run.py:483] Algo bellman_ford step 2956 current loss 0.617983, current_train_items 94624.
I0302 18:59:18.093993 22895071117440 run.py:483] Algo bellman_ford step 2957 current loss 0.881459, current_train_items 94656.
I0302 18:59:18.123535 22895071117440 run.py:483] Algo bellman_ford step 2958 current loss 0.821168, current_train_items 94688.
I0302 18:59:18.154743 22895071117440 run.py:483] Algo bellman_ford step 2959 current loss 1.038746, current_train_items 94720.
I0302 18:59:18.173047 22895071117440 run.py:483] Algo bellman_ford step 2960 current loss 0.376888, current_train_items 94752.
I0302 18:59:18.189003 22895071117440 run.py:483] Algo bellman_ford step 2961 current loss 0.485066, current_train_items 94784.
I0302 18:59:18.210148 22895071117440 run.py:483] Algo bellman_ford step 2962 current loss 0.840170, current_train_items 94816.
I0302 18:59:18.237610 22895071117440 run.py:483] Algo bellman_ford step 2963 current loss 0.847998, current_train_items 94848.
I0302 18:59:18.269234 22895071117440 run.py:483] Algo bellman_ford step 2964 current loss 0.926370, current_train_items 94880.
I0302 18:59:18.286943 22895071117440 run.py:483] Algo bellman_ford step 2965 current loss 0.289088, current_train_items 94912.
I0302 18:59:18.302218 22895071117440 run.py:483] Algo bellman_ford step 2966 current loss 0.576134, current_train_items 94944.
I0302 18:59:18.323744 22895071117440 run.py:483] Algo bellman_ford step 2967 current loss 0.764599, current_train_items 94976.
I0302 18:59:18.351576 22895071117440 run.py:483] Algo bellman_ford step 2968 current loss 0.709151, current_train_items 95008.
I0302 18:59:18.383428 22895071117440 run.py:483] Algo bellman_ford step 2969 current loss 0.873231, current_train_items 95040.
I0302 18:59:18.401932 22895071117440 run.py:483] Algo bellman_ford step 2970 current loss 0.330965, current_train_items 95072.
I0302 18:59:18.417398 22895071117440 run.py:483] Algo bellman_ford step 2971 current loss 0.509459, current_train_items 95104.
I0302 18:59:18.439128 22895071117440 run.py:483] Algo bellman_ford step 2972 current loss 0.710967, current_train_items 95136.
I0302 18:59:18.466458 22895071117440 run.py:483] Algo bellman_ford step 2973 current loss 0.745130, current_train_items 95168.
I0302 18:59:18.499961 22895071117440 run.py:483] Algo bellman_ford step 2974 current loss 1.008304, current_train_items 95200.
I0302 18:59:18.518625 22895071117440 run.py:483] Algo bellman_ford step 2975 current loss 0.366325, current_train_items 95232.
I0302 18:59:18.534967 22895071117440 run.py:483] Algo bellman_ford step 2976 current loss 0.580159, current_train_items 95264.
I0302 18:59:18.556454 22895071117440 run.py:483] Algo bellman_ford step 2977 current loss 0.660802, current_train_items 95296.
I0302 18:59:18.584087 22895071117440 run.py:483] Algo bellman_ford step 2978 current loss 0.742197, current_train_items 95328.
I0302 18:59:18.616364 22895071117440 run.py:483] Algo bellman_ford step 2979 current loss 0.948803, current_train_items 95360.
I0302 18:59:18.634405 22895071117440 run.py:483] Algo bellman_ford step 2980 current loss 0.300851, current_train_items 95392.
I0302 18:59:18.649694 22895071117440 run.py:483] Algo bellman_ford step 2981 current loss 0.511925, current_train_items 95424.
I0302 18:59:18.672189 22895071117440 run.py:483] Algo bellman_ford step 2982 current loss 0.734644, current_train_items 95456.
I0302 18:59:18.702267 22895071117440 run.py:483] Algo bellman_ford step 2983 current loss 0.798721, current_train_items 95488.
I0302 18:59:18.733399 22895071117440 run.py:483] Algo bellman_ford step 2984 current loss 0.982863, current_train_items 95520.
I0302 18:59:18.752057 22895071117440 run.py:483] Algo bellman_ford step 2985 current loss 0.345660, current_train_items 95552.
I0302 18:59:18.768138 22895071117440 run.py:483] Algo bellman_ford step 2986 current loss 0.564890, current_train_items 95584.
I0302 18:59:18.790619 22895071117440 run.py:483] Algo bellman_ford step 2987 current loss 0.762186, current_train_items 95616.
I0302 18:59:18.818738 22895071117440 run.py:483] Algo bellman_ford step 2988 current loss 0.832146, current_train_items 95648.
I0302 18:59:18.850229 22895071117440 run.py:483] Algo bellman_ford step 2989 current loss 1.047952, current_train_items 95680.
I0302 18:59:18.868495 22895071117440 run.py:483] Algo bellman_ford step 2990 current loss 0.357986, current_train_items 95712.
I0302 18:59:18.884720 22895071117440 run.py:483] Algo bellman_ford step 2991 current loss 0.611947, current_train_items 95744.
I0302 18:59:18.906867 22895071117440 run.py:483] Algo bellman_ford step 2992 current loss 0.747404, current_train_items 95776.
I0302 18:59:18.935800 22895071117440 run.py:483] Algo bellman_ford step 2993 current loss 1.069853, current_train_items 95808.
I0302 18:59:18.968339 22895071117440 run.py:483] Algo bellman_ford step 2994 current loss 1.163582, current_train_items 95840.
I0302 18:59:18.986220 22895071117440 run.py:483] Algo bellman_ford step 2995 current loss 0.267676, current_train_items 95872.
I0302 18:59:19.001726 22895071117440 run.py:483] Algo bellman_ford step 2996 current loss 0.529384, current_train_items 95904.
I0302 18:59:19.024971 22895071117440 run.py:483] Algo bellman_ford step 2997 current loss 0.802486, current_train_items 95936.
I0302 18:59:19.054123 22895071117440 run.py:483] Algo bellman_ford step 2998 current loss 0.790066, current_train_items 95968.
I0302 18:59:19.084277 22895071117440 run.py:483] Algo bellman_ford step 2999 current loss 0.855810, current_train_items 96000.
I0302 18:59:19.102777 22895071117440 run.py:483] Algo bellman_ford step 3000 current loss 0.370385, current_train_items 96032.
I0302 18:59:19.110445 22895071117440 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:19.110553 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 18:59:19.126419 22895071117440 run.py:483] Algo bellman_ford step 3001 current loss 0.512392, current_train_items 96064.
I0302 18:59:19.148363 22895071117440 run.py:483] Algo bellman_ford step 3002 current loss 0.574893, current_train_items 96096.
I0302 18:59:19.177115 22895071117440 run.py:483] Algo bellman_ford step 3003 current loss 0.824237, current_train_items 96128.
I0302 18:59:19.207799 22895071117440 run.py:483] Algo bellman_ford step 3004 current loss 0.894636, current_train_items 96160.
I0302 18:59:19.226298 22895071117440 run.py:483] Algo bellman_ford step 3005 current loss 0.291225, current_train_items 96192.
I0302 18:59:19.241599 22895071117440 run.py:483] Algo bellman_ford step 3006 current loss 0.509463, current_train_items 96224.
I0302 18:59:19.264632 22895071117440 run.py:483] Algo bellman_ford step 3007 current loss 0.677749, current_train_items 96256.
I0302 18:59:19.293731 22895071117440 run.py:483] Algo bellman_ford step 3008 current loss 0.820183, current_train_items 96288.
I0302 18:59:19.327193 22895071117440 run.py:483] Algo bellman_ford step 3009 current loss 0.983688, current_train_items 96320.
I0302 18:59:19.345107 22895071117440 run.py:483] Algo bellman_ford step 3010 current loss 0.368927, current_train_items 96352.
I0302 18:59:19.360579 22895071117440 run.py:483] Algo bellman_ford step 3011 current loss 0.591899, current_train_items 96384.
I0302 18:59:19.382683 22895071117440 run.py:483] Algo bellman_ford step 3012 current loss 0.686835, current_train_items 96416.
I0302 18:59:19.410656 22895071117440 run.py:483] Algo bellman_ford step 3013 current loss 0.780754, current_train_items 96448.
I0302 18:59:19.442891 22895071117440 run.py:483] Algo bellman_ford step 3014 current loss 0.959216, current_train_items 96480.
I0302 18:59:19.461019 22895071117440 run.py:483] Algo bellman_ford step 3015 current loss 0.339335, current_train_items 96512.
I0302 18:59:19.476772 22895071117440 run.py:483] Algo bellman_ford step 3016 current loss 0.525481, current_train_items 96544.
I0302 18:59:19.498688 22895071117440 run.py:483] Algo bellman_ford step 3017 current loss 0.738939, current_train_items 96576.
I0302 18:59:19.526578 22895071117440 run.py:483] Algo bellman_ford step 3018 current loss 0.747573, current_train_items 96608.
I0302 18:59:19.556344 22895071117440 run.py:483] Algo bellman_ford step 3019 current loss 0.805277, current_train_items 96640.
I0302 18:59:19.574247 22895071117440 run.py:483] Algo bellman_ford step 3020 current loss 0.227955, current_train_items 96672.
I0302 18:59:19.589217 22895071117440 run.py:483] Algo bellman_ford step 3021 current loss 0.505955, current_train_items 96704.
I0302 18:59:19.612194 22895071117440 run.py:483] Algo bellman_ford step 3022 current loss 0.710331, current_train_items 96736.
I0302 18:59:19.640721 22895071117440 run.py:483] Algo bellman_ford step 3023 current loss 0.761903, current_train_items 96768.
I0302 18:59:19.671729 22895071117440 run.py:483] Algo bellman_ford step 3024 current loss 0.780405, current_train_items 96800.
I0302 18:59:19.689627 22895071117440 run.py:483] Algo bellman_ford step 3025 current loss 0.254991, current_train_items 96832.
I0302 18:59:19.704859 22895071117440 run.py:483] Algo bellman_ford step 3026 current loss 0.561432, current_train_items 96864.
I0302 18:59:19.725503 22895071117440 run.py:483] Algo bellman_ford step 3027 current loss 0.585789, current_train_items 96896.
I0302 18:59:19.753508 22895071117440 run.py:483] Algo bellman_ford step 3028 current loss 0.755841, current_train_items 96928.
I0302 18:59:19.783343 22895071117440 run.py:483] Algo bellman_ford step 3029 current loss 0.887268, current_train_items 96960.
I0302 18:59:19.801535 22895071117440 run.py:483] Algo bellman_ford step 3030 current loss 0.326431, current_train_items 96992.
I0302 18:59:19.816677 22895071117440 run.py:483] Algo bellman_ford step 3031 current loss 0.444636, current_train_items 97024.
I0302 18:59:19.839212 22895071117440 run.py:483] Algo bellman_ford step 3032 current loss 0.655703, current_train_items 97056.
I0302 18:59:19.868205 22895071117440 run.py:483] Algo bellman_ford step 3033 current loss 0.791474, current_train_items 97088.
I0302 18:59:19.900064 22895071117440 run.py:483] Algo bellman_ford step 3034 current loss 0.840824, current_train_items 97120.
I0302 18:59:19.917797 22895071117440 run.py:483] Algo bellman_ford step 3035 current loss 0.306786, current_train_items 97152.
I0302 18:59:19.933633 22895071117440 run.py:483] Algo bellman_ford step 3036 current loss 0.598074, current_train_items 97184.
I0302 18:59:19.955598 22895071117440 run.py:483] Algo bellman_ford step 3037 current loss 0.736248, current_train_items 97216.
I0302 18:59:19.984399 22895071117440 run.py:483] Algo bellman_ford step 3038 current loss 0.829106, current_train_items 97248.
I0302 18:59:20.014297 22895071117440 run.py:483] Algo bellman_ford step 3039 current loss 0.951113, current_train_items 97280.
I0302 18:59:20.032256 22895071117440 run.py:483] Algo bellman_ford step 3040 current loss 0.300543, current_train_items 97312.
I0302 18:59:20.047593 22895071117440 run.py:483] Algo bellman_ford step 3041 current loss 0.500668, current_train_items 97344.
I0302 18:59:20.069145 22895071117440 run.py:483] Algo bellman_ford step 3042 current loss 0.700449, current_train_items 97376.
I0302 18:59:20.097374 22895071117440 run.py:483] Algo bellman_ford step 3043 current loss 0.929981, current_train_items 97408.
I0302 18:59:20.127081 22895071117440 run.py:483] Algo bellman_ford step 3044 current loss 1.004108, current_train_items 97440.
I0302 18:59:20.145254 22895071117440 run.py:483] Algo bellman_ford step 3045 current loss 0.341451, current_train_items 97472.
I0302 18:59:20.160594 22895071117440 run.py:483] Algo bellman_ford step 3046 current loss 0.625754, current_train_items 97504.
I0302 18:59:20.182237 22895071117440 run.py:483] Algo bellman_ford step 3047 current loss 0.757491, current_train_items 97536.
I0302 18:59:20.211211 22895071117440 run.py:483] Algo bellman_ford step 3048 current loss 0.774950, current_train_items 97568.
I0302 18:59:20.241996 22895071117440 run.py:483] Algo bellman_ford step 3049 current loss 0.882297, current_train_items 97600.
I0302 18:59:20.260272 22895071117440 run.py:483] Algo bellman_ford step 3050 current loss 0.294504, current_train_items 97632.
I0302 18:59:20.268265 22895071117440 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 18:59:20.268371 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 18:59:20.284223 22895071117440 run.py:483] Algo bellman_ford step 3051 current loss 0.536055, current_train_items 97664.
I0302 18:59:20.306140 22895071117440 run.py:483] Algo bellman_ford step 3052 current loss 0.898495, current_train_items 97696.
I0302 18:59:20.335290 22895071117440 run.py:483] Algo bellman_ford step 3053 current loss 0.911717, current_train_items 97728.
I0302 18:59:20.364571 22895071117440 run.py:483] Algo bellman_ford step 3054 current loss 1.014109, current_train_items 97760.
I0302 18:59:20.382887 22895071117440 run.py:483] Algo bellman_ford step 3055 current loss 0.401786, current_train_items 97792.
I0302 18:59:20.398170 22895071117440 run.py:483] Algo bellman_ford step 3056 current loss 0.572462, current_train_items 97824.
I0302 18:59:20.419761 22895071117440 run.py:483] Algo bellman_ford step 3057 current loss 0.690434, current_train_items 97856.
I0302 18:59:20.447834 22895071117440 run.py:483] Algo bellman_ford step 3058 current loss 0.798129, current_train_items 97888.
I0302 18:59:20.477663 22895071117440 run.py:483] Algo bellman_ford step 3059 current loss 0.977290, current_train_items 97920.
I0302 18:59:20.495673 22895071117440 run.py:483] Algo bellman_ford step 3060 current loss 0.374406, current_train_items 97952.
I0302 18:59:20.511496 22895071117440 run.py:483] Algo bellman_ford step 3061 current loss 0.778639, current_train_items 97984.
I0302 18:59:20.532279 22895071117440 run.py:483] Algo bellman_ford step 3062 current loss 0.804532, current_train_items 98016.
I0302 18:59:20.561548 22895071117440 run.py:483] Algo bellman_ford step 3063 current loss 1.085009, current_train_items 98048.
I0302 18:59:20.593658 22895071117440 run.py:483] Algo bellman_ford step 3064 current loss 0.993272, current_train_items 98080.
I0302 18:59:20.611616 22895071117440 run.py:483] Algo bellman_ford step 3065 current loss 0.445621, current_train_items 98112.
I0302 18:59:20.626820 22895071117440 run.py:483] Algo bellman_ford step 3066 current loss 0.642367, current_train_items 98144.
I0302 18:59:20.649880 22895071117440 run.py:483] Algo bellman_ford step 3067 current loss 0.807254, current_train_items 98176.
I0302 18:59:20.677521 22895071117440 run.py:483] Algo bellman_ford step 3068 current loss 0.933883, current_train_items 98208.
I0302 18:59:20.710825 22895071117440 run.py:483] Algo bellman_ford step 3069 current loss 1.684267, current_train_items 98240.
I0302 18:59:20.729358 22895071117440 run.py:483] Algo bellman_ford step 3070 current loss 0.395926, current_train_items 98272.
I0302 18:59:20.745127 22895071117440 run.py:483] Algo bellman_ford step 3071 current loss 0.697842, current_train_items 98304.
I0302 18:59:20.766263 22895071117440 run.py:483] Algo bellman_ford step 3072 current loss 1.079288, current_train_items 98336.
I0302 18:59:20.793475 22895071117440 run.py:483] Algo bellman_ford step 3073 current loss 1.041347, current_train_items 98368.
I0302 18:59:20.825790 22895071117440 run.py:483] Algo bellman_ford step 3074 current loss 1.338723, current_train_items 98400.
I0302 18:59:20.843915 22895071117440 run.py:483] Algo bellman_ford step 3075 current loss 0.293963, current_train_items 98432.
I0302 18:59:20.859951 22895071117440 run.py:483] Algo bellman_ford step 3076 current loss 0.748539, current_train_items 98464.
I0302 18:59:20.881444 22895071117440 run.py:483] Algo bellman_ford step 3077 current loss 0.701147, current_train_items 98496.
I0302 18:59:20.910071 22895071117440 run.py:483] Algo bellman_ford step 3078 current loss 0.884708, current_train_items 98528.
I0302 18:59:20.939266 22895071117440 run.py:483] Algo bellman_ford step 3079 current loss 0.928044, current_train_items 98560.
I0302 18:59:20.956973 22895071117440 run.py:483] Algo bellman_ford step 3080 current loss 0.398831, current_train_items 98592.
I0302 18:59:20.972465 22895071117440 run.py:483] Algo bellman_ford step 3081 current loss 0.559191, current_train_items 98624.
I0302 18:59:20.994385 22895071117440 run.py:483] Algo bellman_ford step 3082 current loss 0.672025, current_train_items 98656.
I0302 18:59:21.022373 22895071117440 run.py:483] Algo bellman_ford step 3083 current loss 0.854789, current_train_items 98688.
I0302 18:59:21.053429 22895071117440 run.py:483] Algo bellman_ford step 3084 current loss 0.898409, current_train_items 98720.
I0302 18:59:21.071605 22895071117440 run.py:483] Algo bellman_ford step 3085 current loss 0.362009, current_train_items 98752.
I0302 18:59:21.087487 22895071117440 run.py:483] Algo bellman_ford step 3086 current loss 0.682106, current_train_items 98784.
I0302 18:59:21.108009 22895071117440 run.py:483] Algo bellman_ford step 3087 current loss 0.675528, current_train_items 98816.
I0302 18:59:21.136093 22895071117440 run.py:483] Algo bellman_ford step 3088 current loss 1.026755, current_train_items 98848.
I0302 18:59:21.168243 22895071117440 run.py:483] Algo bellman_ford step 3089 current loss 1.253404, current_train_items 98880.
I0302 18:59:21.186488 22895071117440 run.py:483] Algo bellman_ford step 3090 current loss 0.275176, current_train_items 98912.
I0302 18:59:21.201983 22895071117440 run.py:483] Algo bellman_ford step 3091 current loss 0.552531, current_train_items 98944.
I0302 18:59:21.222471 22895071117440 run.py:483] Algo bellman_ford step 3092 current loss 0.849464, current_train_items 98976.
I0302 18:59:21.250593 22895071117440 run.py:483] Algo bellman_ford step 3093 current loss 1.117798, current_train_items 99008.
I0302 18:59:21.280914 22895071117440 run.py:483] Algo bellman_ford step 3094 current loss 1.223632, current_train_items 99040.
I0302 18:59:21.298763 22895071117440 run.py:483] Algo bellman_ford step 3095 current loss 0.388021, current_train_items 99072.
I0302 18:59:21.314650 22895071117440 run.py:483] Algo bellman_ford step 3096 current loss 0.677911, current_train_items 99104.
I0302 18:59:21.337418 22895071117440 run.py:483] Algo bellman_ford step 3097 current loss 0.893037, current_train_items 99136.
I0302 18:59:21.365091 22895071117440 run.py:483] Algo bellman_ford step 3098 current loss 0.903656, current_train_items 99168.
I0302 18:59:21.395341 22895071117440 run.py:483] Algo bellman_ford step 3099 current loss 0.976730, current_train_items 99200.
I0302 18:59:21.413718 22895071117440 run.py:483] Algo bellman_ford step 3100 current loss 0.406392, current_train_items 99232.
I0302 18:59:21.421610 22895071117440 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 18:59:21.421714 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 18:59:21.438683 22895071117440 run.py:483] Algo bellman_ford step 3101 current loss 0.577664, current_train_items 99264.
I0302 18:59:21.460772 22895071117440 run.py:483] Algo bellman_ford step 3102 current loss 0.841909, current_train_items 99296.
I0302 18:59:21.490055 22895071117440 run.py:483] Algo bellman_ford step 3103 current loss 0.825405, current_train_items 99328.
I0302 18:59:21.522284 22895071117440 run.py:483] Algo bellman_ford step 3104 current loss 1.066733, current_train_items 99360.
I0302 18:59:21.541121 22895071117440 run.py:483] Algo bellman_ford step 3105 current loss 0.288779, current_train_items 99392.
I0302 18:59:21.556191 22895071117440 run.py:483] Algo bellman_ford step 3106 current loss 0.533857, current_train_items 99424.
I0302 18:59:21.578006 22895071117440 run.py:483] Algo bellman_ford step 3107 current loss 0.675518, current_train_items 99456.
I0302 18:59:21.607223 22895071117440 run.py:483] Algo bellman_ford step 3108 current loss 0.835021, current_train_items 99488.
I0302 18:59:21.636705 22895071117440 run.py:483] Algo bellman_ford step 3109 current loss 0.930335, current_train_items 99520.
I0302 18:59:21.654777 22895071117440 run.py:483] Algo bellman_ford step 3110 current loss 0.329419, current_train_items 99552.
I0302 18:59:21.670196 22895071117440 run.py:483] Algo bellman_ford step 3111 current loss 0.521920, current_train_items 99584.
I0302 18:59:21.691989 22895071117440 run.py:483] Algo bellman_ford step 3112 current loss 0.796646, current_train_items 99616.
I0302 18:59:21.720360 22895071117440 run.py:483] Algo bellman_ford step 3113 current loss 0.802295, current_train_items 99648.
I0302 18:59:21.751881 22895071117440 run.py:483] Algo bellman_ford step 3114 current loss 0.888371, current_train_items 99680.
I0302 18:59:21.770333 22895071117440 run.py:483] Algo bellman_ford step 3115 current loss 0.287703, current_train_items 99712.
I0302 18:59:21.785954 22895071117440 run.py:483] Algo bellman_ford step 3116 current loss 0.528381, current_train_items 99744.
I0302 18:59:21.807625 22895071117440 run.py:483] Algo bellman_ford step 3117 current loss 0.695483, current_train_items 99776.
I0302 18:59:21.836324 22895071117440 run.py:483] Algo bellman_ford step 3118 current loss 0.840369, current_train_items 99808.
I0302 18:59:21.866723 22895071117440 run.py:483] Algo bellman_ford step 3119 current loss 1.007119, current_train_items 99840.
I0302 18:59:21.884744 22895071117440 run.py:483] Algo bellman_ford step 3120 current loss 0.208471, current_train_items 99872.
I0302 18:59:21.900077 22895071117440 run.py:483] Algo bellman_ford step 3121 current loss 0.544816, current_train_items 99904.
I0302 18:59:21.921867 22895071117440 run.py:483] Algo bellman_ford step 3122 current loss 0.650757, current_train_items 99936.
I0302 18:59:21.950532 22895071117440 run.py:483] Algo bellman_ford step 3123 current loss 0.752777, current_train_items 99968.
I0302 18:59:21.981366 22895071117440 run.py:483] Algo bellman_ford step 3124 current loss 0.933549, current_train_items 100000.
I0302 18:59:21.999707 22895071117440 run.py:483] Algo bellman_ford step 3125 current loss 0.383700, current_train_items 100032.
I0302 18:59:22.015176 22895071117440 run.py:483] Algo bellman_ford step 3126 current loss 0.570220, current_train_items 100064.
I0302 18:59:22.036089 22895071117440 run.py:483] Algo bellman_ford step 3127 current loss 0.604090, current_train_items 100096.
I0302 18:59:22.065344 22895071117440 run.py:483] Algo bellman_ford step 3128 current loss 0.781562, current_train_items 100128.
I0302 18:59:22.095197 22895071117440 run.py:483] Algo bellman_ford step 3129 current loss 0.807027, current_train_items 100160.
I0302 18:59:22.113427 22895071117440 run.py:483] Algo bellman_ford step 3130 current loss 0.377436, current_train_items 100192.
I0302 18:59:22.128877 22895071117440 run.py:483] Algo bellman_ford step 3131 current loss 0.526778, current_train_items 100224.
I0302 18:59:22.151139 22895071117440 run.py:483] Algo bellman_ford step 3132 current loss 0.701553, current_train_items 100256.
I0302 18:59:22.178698 22895071117440 run.py:483] Algo bellman_ford step 3133 current loss 0.858377, current_train_items 100288.
I0302 18:59:22.211524 22895071117440 run.py:483] Algo bellman_ford step 3134 current loss 0.948832, current_train_items 100320.
I0302 18:59:22.229329 22895071117440 run.py:483] Algo bellman_ford step 3135 current loss 0.349096, current_train_items 100352.
I0302 18:59:22.244632 22895071117440 run.py:483] Algo bellman_ford step 3136 current loss 0.517811, current_train_items 100384.
I0302 18:59:22.266710 22895071117440 run.py:483] Algo bellman_ford step 3137 current loss 0.774534, current_train_items 100416.
I0302 18:59:22.294213 22895071117440 run.py:483] Algo bellman_ford step 3138 current loss 0.811821, current_train_items 100448.
I0302 18:59:22.325800 22895071117440 run.py:483] Algo bellman_ford step 3139 current loss 0.960745, current_train_items 100480.
I0302 18:59:22.343807 22895071117440 run.py:483] Algo bellman_ford step 3140 current loss 0.396502, current_train_items 100512.
I0302 18:59:22.359810 22895071117440 run.py:483] Algo bellman_ford step 3141 current loss 0.573397, current_train_items 100544.
I0302 18:59:22.380555 22895071117440 run.py:483] Algo bellman_ford step 3142 current loss 0.679656, current_train_items 100576.
I0302 18:59:22.410135 22895071117440 run.py:483] Algo bellman_ford step 3143 current loss 0.826598, current_train_items 100608.
I0302 18:59:22.440936 22895071117440 run.py:483] Algo bellman_ford step 3144 current loss 0.771767, current_train_items 100640.
I0302 18:59:22.458765 22895071117440 run.py:483] Algo bellman_ford step 3145 current loss 0.271564, current_train_items 100672.
I0302 18:59:22.474130 22895071117440 run.py:483] Algo bellman_ford step 3146 current loss 0.416141, current_train_items 100704.
I0302 18:59:22.495119 22895071117440 run.py:483] Algo bellman_ford step 3147 current loss 0.542698, current_train_items 100736.
I0302 18:59:22.521863 22895071117440 run.py:483] Algo bellman_ford step 3148 current loss 0.667376, current_train_items 100768.
I0302 18:59:22.554649 22895071117440 run.py:483] Algo bellman_ford step 3149 current loss 0.944582, current_train_items 100800.
I0302 18:59:22.572676 22895071117440 run.py:483] Algo bellman_ford step 3150 current loss 0.301070, current_train_items 100832.
I0302 18:59:22.580573 22895071117440 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 18:59:22.580680 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:22.596880 22895071117440 run.py:483] Algo bellman_ford step 3151 current loss 0.488135, current_train_items 100864.
I0302 18:59:22.619779 22895071117440 run.py:483] Algo bellman_ford step 3152 current loss 0.750815, current_train_items 100896.
I0302 18:59:22.648804 22895071117440 run.py:483] Algo bellman_ford step 3153 current loss 0.793962, current_train_items 100928.
I0302 18:59:22.681186 22895071117440 run.py:483] Algo bellman_ford step 3154 current loss 0.949628, current_train_items 100960.
I0302 18:59:22.699913 22895071117440 run.py:483] Algo bellman_ford step 3155 current loss 0.267894, current_train_items 100992.
I0302 18:59:22.715303 22895071117440 run.py:483] Algo bellman_ford step 3156 current loss 0.533897, current_train_items 101024.
I0302 18:59:22.737721 22895071117440 run.py:483] Algo bellman_ford step 3157 current loss 0.758678, current_train_items 101056.
I0302 18:59:22.765344 22895071117440 run.py:483] Algo bellman_ford step 3158 current loss 0.815615, current_train_items 101088.
I0302 18:59:22.795472 22895071117440 run.py:483] Algo bellman_ford step 3159 current loss 0.808845, current_train_items 101120.
I0302 18:59:22.813970 22895071117440 run.py:483] Algo bellman_ford step 3160 current loss 0.333638, current_train_items 101152.
I0302 18:59:22.829410 22895071117440 run.py:483] Algo bellman_ford step 3161 current loss 0.544522, current_train_items 101184.
I0302 18:59:22.850949 22895071117440 run.py:483] Algo bellman_ford step 3162 current loss 0.728552, current_train_items 101216.
I0302 18:59:22.878467 22895071117440 run.py:483] Algo bellman_ford step 3163 current loss 0.721940, current_train_items 101248.
I0302 18:59:22.910348 22895071117440 run.py:483] Algo bellman_ford step 3164 current loss 0.958150, current_train_items 101280.
I0302 18:59:22.928540 22895071117440 run.py:483] Algo bellman_ford step 3165 current loss 0.323869, current_train_items 101312.
I0302 18:59:22.944629 22895071117440 run.py:483] Algo bellman_ford step 3166 current loss 0.655385, current_train_items 101344.
I0302 18:59:22.966820 22895071117440 run.py:483] Algo bellman_ford step 3167 current loss 0.766051, current_train_items 101376.
I0302 18:59:22.995220 22895071117440 run.py:483] Algo bellman_ford step 3168 current loss 0.863864, current_train_items 101408.
I0302 18:59:23.025658 22895071117440 run.py:483] Algo bellman_ford step 3169 current loss 0.920239, current_train_items 101440.
I0302 18:59:23.044233 22895071117440 run.py:483] Algo bellman_ford step 3170 current loss 0.348436, current_train_items 101472.
I0302 18:59:23.060278 22895071117440 run.py:483] Algo bellman_ford step 3171 current loss 0.545627, current_train_items 101504.
I0302 18:59:23.082277 22895071117440 run.py:483] Algo bellman_ford step 3172 current loss 0.635374, current_train_items 101536.
I0302 18:59:23.110921 22895071117440 run.py:483] Algo bellman_ford step 3173 current loss 0.737842, current_train_items 101568.
I0302 18:59:23.139420 22895071117440 run.py:483] Algo bellman_ford step 3174 current loss 0.904176, current_train_items 101600.
I0302 18:59:23.157580 22895071117440 run.py:483] Algo bellman_ford step 3175 current loss 0.271141, current_train_items 101632.
I0302 18:59:23.173253 22895071117440 run.py:483] Algo bellman_ford step 3176 current loss 0.522337, current_train_items 101664.
I0302 18:59:23.194261 22895071117440 run.py:483] Algo bellman_ford step 3177 current loss 0.664456, current_train_items 101696.
I0302 18:59:23.220533 22895071117440 run.py:483] Algo bellman_ford step 3178 current loss 0.702679, current_train_items 101728.
I0302 18:59:23.252691 22895071117440 run.py:483] Algo bellman_ford step 3179 current loss 0.895509, current_train_items 101760.
I0302 18:59:23.270754 22895071117440 run.py:483] Algo bellman_ford step 3180 current loss 0.366762, current_train_items 101792.
I0302 18:59:23.286329 22895071117440 run.py:483] Algo bellman_ford step 3181 current loss 0.581688, current_train_items 101824.
I0302 18:59:23.308054 22895071117440 run.py:483] Algo bellman_ford step 3182 current loss 1.152968, current_train_items 101856.
I0302 18:59:23.336766 22895071117440 run.py:483] Algo bellman_ford step 3183 current loss 1.580422, current_train_items 101888.
I0302 18:59:23.370755 22895071117440 run.py:483] Algo bellman_ford step 3184 current loss 2.511868, current_train_items 101920.
I0302 18:59:23.389298 22895071117440 run.py:483] Algo bellman_ford step 3185 current loss 0.592598, current_train_items 101952.
I0302 18:59:23.404945 22895071117440 run.py:483] Algo bellman_ford step 3186 current loss 0.801582, current_train_items 101984.
I0302 18:59:23.427185 22895071117440 run.py:483] Algo bellman_ford step 3187 current loss 1.022937, current_train_items 102016.
I0302 18:59:23.455147 22895071117440 run.py:483] Algo bellman_ford step 3188 current loss 0.913578, current_train_items 102048.
I0302 18:59:23.485716 22895071117440 run.py:483] Algo bellman_ford step 3189 current loss 0.876298, current_train_items 102080.
I0302 18:59:23.504134 22895071117440 run.py:483] Algo bellman_ford step 3190 current loss 0.378035, current_train_items 102112.
I0302 18:59:23.519889 22895071117440 run.py:483] Algo bellman_ford step 3191 current loss 0.540848, current_train_items 102144.
I0302 18:59:23.542227 22895071117440 run.py:483] Algo bellman_ford step 3192 current loss 0.885734, current_train_items 102176.
I0302 18:59:23.570198 22895071117440 run.py:483] Algo bellman_ford step 3193 current loss 0.763300, current_train_items 102208.
I0302 18:59:23.601670 22895071117440 run.py:483] Algo bellman_ford step 3194 current loss 1.071579, current_train_items 102240.
I0302 18:59:23.619980 22895071117440 run.py:483] Algo bellman_ford step 3195 current loss 0.382917, current_train_items 102272.
I0302 18:59:23.635032 22895071117440 run.py:483] Algo bellman_ford step 3196 current loss 0.473037, current_train_items 102304.
I0302 18:59:23.656713 22895071117440 run.py:483] Algo bellman_ford step 3197 current loss 0.705221, current_train_items 102336.
I0302 18:59:23.685457 22895071117440 run.py:483] Algo bellman_ford step 3198 current loss 0.777577, current_train_items 102368.
I0302 18:59:23.717036 22895071117440 run.py:483] Algo bellman_ford step 3199 current loss 0.893466, current_train_items 102400.
I0302 18:59:23.735634 22895071117440 run.py:483] Algo bellman_ford step 3200 current loss 0.299498, current_train_items 102432.
I0302 18:59:23.743659 22895071117440 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 18:59:23.743767 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:23.759203 22895071117440 run.py:483] Algo bellman_ford step 3201 current loss 0.461199, current_train_items 102464.
I0302 18:59:23.781119 22895071117440 run.py:483] Algo bellman_ford step 3202 current loss 0.741227, current_train_items 102496.
I0302 18:59:23.811907 22895071117440 run.py:483] Algo bellman_ford step 3203 current loss 1.103264, current_train_items 102528.
I0302 18:59:23.843537 22895071117440 run.py:483] Algo bellman_ford step 3204 current loss 1.075613, current_train_items 102560.
I0302 18:59:23.862499 22895071117440 run.py:483] Algo bellman_ford step 3205 current loss 0.327059, current_train_items 102592.
I0302 18:59:23.877682 22895071117440 run.py:483] Algo bellman_ford step 3206 current loss 0.591914, current_train_items 102624.
I0302 18:59:23.898426 22895071117440 run.py:483] Algo bellman_ford step 3207 current loss 0.713167, current_train_items 102656.
I0302 18:59:23.927905 22895071117440 run.py:483] Algo bellman_ford step 3208 current loss 0.815526, current_train_items 102688.
I0302 18:59:23.959632 22895071117440 run.py:483] Algo bellman_ford step 3209 current loss 0.953969, current_train_items 102720.
I0302 18:59:23.977706 22895071117440 run.py:483] Algo bellman_ford step 3210 current loss 0.358952, current_train_items 102752.
I0302 18:59:23.992974 22895071117440 run.py:483] Algo bellman_ford step 3211 current loss 0.461961, current_train_items 102784.
I0302 18:59:24.014353 22895071117440 run.py:483] Algo bellman_ford step 3212 current loss 0.772796, current_train_items 102816.
I0302 18:59:24.042755 22895071117440 run.py:483] Algo bellman_ford step 3213 current loss 0.883740, current_train_items 102848.
I0302 18:59:24.073302 22895071117440 run.py:483] Algo bellman_ford step 3214 current loss 0.880551, current_train_items 102880.
I0302 18:59:24.091787 22895071117440 run.py:483] Algo bellman_ford step 3215 current loss 0.394217, current_train_items 102912.
I0302 18:59:24.107024 22895071117440 run.py:483] Algo bellman_ford step 3216 current loss 0.560545, current_train_items 102944.
I0302 18:59:24.129531 22895071117440 run.py:483] Algo bellman_ford step 3217 current loss 0.675248, current_train_items 102976.
I0302 18:59:24.156859 22895071117440 run.py:483] Algo bellman_ford step 3218 current loss 0.786899, current_train_items 103008.
I0302 18:59:24.187037 22895071117440 run.py:483] Algo bellman_ford step 3219 current loss 0.913614, current_train_items 103040.
I0302 18:59:24.205170 22895071117440 run.py:483] Algo bellman_ford step 3220 current loss 0.314287, current_train_items 103072.
I0302 18:59:24.220682 22895071117440 run.py:483] Algo bellman_ford step 3221 current loss 0.581560, current_train_items 103104.
I0302 18:59:24.243350 22895071117440 run.py:483] Algo bellman_ford step 3222 current loss 0.800004, current_train_items 103136.
I0302 18:59:24.272309 22895071117440 run.py:483] Algo bellman_ford step 3223 current loss 0.789965, current_train_items 103168.
I0302 18:59:24.301492 22895071117440 run.py:483] Algo bellman_ford step 3224 current loss 0.725959, current_train_items 103200.
I0302 18:59:24.319191 22895071117440 run.py:483] Algo bellman_ford step 3225 current loss 0.341591, current_train_items 103232.
I0302 18:59:24.333858 22895071117440 run.py:483] Algo bellman_ford step 3226 current loss 0.438397, current_train_items 103264.
I0302 18:59:24.355738 22895071117440 run.py:483] Algo bellman_ford step 3227 current loss 0.841972, current_train_items 103296.
I0302 18:59:24.385009 22895071117440 run.py:483] Algo bellman_ford step 3228 current loss 1.000429, current_train_items 103328.
I0302 18:59:24.419660 22895071117440 run.py:483] Algo bellman_ford step 3229 current loss 1.228643, current_train_items 103360.
I0302 18:59:24.437583 22895071117440 run.py:483] Algo bellman_ford step 3230 current loss 0.227331, current_train_items 103392.
I0302 18:59:24.453061 22895071117440 run.py:483] Algo bellman_ford step 3231 current loss 0.444450, current_train_items 103424.
I0302 18:59:24.474421 22895071117440 run.py:483] Algo bellman_ford step 3232 current loss 0.693496, current_train_items 103456.
I0302 18:59:24.502980 22895071117440 run.py:483] Algo bellman_ford step 3233 current loss 0.746146, current_train_items 103488.
I0302 18:59:24.535254 22895071117440 run.py:483] Algo bellman_ford step 3234 current loss 0.977278, current_train_items 103520.
I0302 18:59:24.553578 22895071117440 run.py:483] Algo bellman_ford step 3235 current loss 0.342502, current_train_items 103552.
I0302 18:59:24.569060 22895071117440 run.py:483] Algo bellman_ford step 3236 current loss 0.605466, current_train_items 103584.
I0302 18:59:24.590165 22895071117440 run.py:483] Algo bellman_ford step 3237 current loss 0.689649, current_train_items 103616.
I0302 18:59:24.619937 22895071117440 run.py:483] Algo bellman_ford step 3238 current loss 0.848185, current_train_items 103648.
I0302 18:59:24.650379 22895071117440 run.py:483] Algo bellman_ford step 3239 current loss 0.923764, current_train_items 103680.
I0302 18:59:24.668509 22895071117440 run.py:483] Algo bellman_ford step 3240 current loss 0.297314, current_train_items 103712.
I0302 18:59:24.684031 22895071117440 run.py:483] Algo bellman_ford step 3241 current loss 0.502645, current_train_items 103744.
I0302 18:59:24.705750 22895071117440 run.py:483] Algo bellman_ford step 3242 current loss 0.717697, current_train_items 103776.
I0302 18:59:24.733259 22895071117440 run.py:483] Algo bellman_ford step 3243 current loss 0.637881, current_train_items 103808.
I0302 18:59:24.764840 22895071117440 run.py:483] Algo bellman_ford step 3244 current loss 1.013070, current_train_items 103840.
I0302 18:59:24.782984 22895071117440 run.py:483] Algo bellman_ford step 3245 current loss 0.279904, current_train_items 103872.
I0302 18:59:24.798523 22895071117440 run.py:483] Algo bellman_ford step 3246 current loss 0.528677, current_train_items 103904.
I0302 18:59:24.819940 22895071117440 run.py:483] Algo bellman_ford step 3247 current loss 0.663398, current_train_items 103936.
I0302 18:59:24.848829 22895071117440 run.py:483] Algo bellman_ford step 3248 current loss 0.782613, current_train_items 103968.
I0302 18:59:24.878578 22895071117440 run.py:483] Algo bellman_ford step 3249 current loss 0.800169, current_train_items 104000.
I0302 18:59:24.896664 22895071117440 run.py:483] Algo bellman_ford step 3250 current loss 0.309457, current_train_items 104032.
I0302 18:59:24.904620 22895071117440 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 18:59:24.904725 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 18:59:24.920589 22895071117440 run.py:483] Algo bellman_ford step 3251 current loss 0.607423, current_train_items 104064.
I0302 18:59:24.941329 22895071117440 run.py:483] Algo bellman_ford step 3252 current loss 0.617216, current_train_items 104096.
I0302 18:59:24.971814 22895071117440 run.py:483] Algo bellman_ford step 3253 current loss 0.875395, current_train_items 104128.
I0302 18:59:25.003266 22895071117440 run.py:483] Algo bellman_ford step 3254 current loss 0.854119, current_train_items 104160.
I0302 18:59:25.021921 22895071117440 run.py:483] Algo bellman_ford step 3255 current loss 0.394287, current_train_items 104192.
I0302 18:59:25.036993 22895071117440 run.py:483] Algo bellman_ford step 3256 current loss 0.495053, current_train_items 104224.
I0302 18:59:25.057994 22895071117440 run.py:483] Algo bellman_ford step 3257 current loss 0.779336, current_train_items 104256.
I0302 18:59:25.087571 22895071117440 run.py:483] Algo bellman_ford step 3258 current loss 0.870046, current_train_items 104288.
I0302 18:59:25.120101 22895071117440 run.py:483] Algo bellman_ford step 3259 current loss 0.933427, current_train_items 104320.
I0302 18:59:25.138112 22895071117440 run.py:483] Algo bellman_ford step 3260 current loss 0.370194, current_train_items 104352.
I0302 18:59:25.153802 22895071117440 run.py:483] Algo bellman_ford step 3261 current loss 0.516258, current_train_items 104384.
I0302 18:59:25.175006 22895071117440 run.py:483] Algo bellman_ford step 3262 current loss 0.614307, current_train_items 104416.
I0302 18:59:25.203523 22895071117440 run.py:483] Algo bellman_ford step 3263 current loss 0.852390, current_train_items 104448.
I0302 18:59:25.235033 22895071117440 run.py:483] Algo bellman_ford step 3264 current loss 1.066756, current_train_items 104480.
I0302 18:59:25.253069 22895071117440 run.py:483] Algo bellman_ford step 3265 current loss 0.393308, current_train_items 104512.
I0302 18:59:25.268553 22895071117440 run.py:483] Algo bellman_ford step 3266 current loss 0.593290, current_train_items 104544.
I0302 18:59:25.289916 22895071117440 run.py:483] Algo bellman_ford step 3267 current loss 0.680731, current_train_items 104576.
I0302 18:59:25.316634 22895071117440 run.py:483] Algo bellman_ford step 3268 current loss 0.700231, current_train_items 104608.
I0302 18:59:25.350001 22895071117440 run.py:483] Algo bellman_ford step 3269 current loss 1.012597, current_train_items 104640.
I0302 18:59:25.368218 22895071117440 run.py:483] Algo bellman_ford step 3270 current loss 0.367117, current_train_items 104672.
I0302 18:59:25.384016 22895071117440 run.py:483] Algo bellman_ford step 3271 current loss 0.548139, current_train_items 104704.
I0302 18:59:25.405280 22895071117440 run.py:483] Algo bellman_ford step 3272 current loss 0.765104, current_train_items 104736.
I0302 18:59:25.432619 22895071117440 run.py:483] Algo bellman_ford step 3273 current loss 0.769738, current_train_items 104768.
I0302 18:59:25.461496 22895071117440 run.py:483] Algo bellman_ford step 3274 current loss 0.797216, current_train_items 104800.
I0302 18:59:25.480012 22895071117440 run.py:483] Algo bellman_ford step 3275 current loss 0.360387, current_train_items 104832.
I0302 18:59:25.495431 22895071117440 run.py:483] Algo bellman_ford step 3276 current loss 0.467395, current_train_items 104864.
I0302 18:59:25.516796 22895071117440 run.py:483] Algo bellman_ford step 3277 current loss 0.758789, current_train_items 104896.
I0302 18:59:25.546395 22895071117440 run.py:483] Algo bellman_ford step 3278 current loss 0.928134, current_train_items 104928.
I0302 18:59:25.577618 22895071117440 run.py:483] Algo bellman_ford step 3279 current loss 1.103667, current_train_items 104960.
I0302 18:59:25.595359 22895071117440 run.py:483] Algo bellman_ford step 3280 current loss 0.359759, current_train_items 104992.
I0302 18:59:25.610378 22895071117440 run.py:483] Algo bellman_ford step 3281 current loss 0.591534, current_train_items 105024.
I0302 18:59:25.631941 22895071117440 run.py:483] Algo bellman_ford step 3282 current loss 0.931462, current_train_items 105056.
I0302 18:59:25.661018 22895071117440 run.py:483] Algo bellman_ford step 3283 current loss 0.946722, current_train_items 105088.
I0302 18:59:25.692230 22895071117440 run.py:483] Algo bellman_ford step 3284 current loss 0.912041, current_train_items 105120.
I0302 18:59:25.711049 22895071117440 run.py:483] Algo bellman_ford step 3285 current loss 0.316665, current_train_items 105152.
I0302 18:59:25.726963 22895071117440 run.py:483] Algo bellman_ford step 3286 current loss 0.594867, current_train_items 105184.
I0302 18:59:25.748624 22895071117440 run.py:483] Algo bellman_ford step 3287 current loss 0.769589, current_train_items 105216.
I0302 18:59:25.775749 22895071117440 run.py:483] Algo bellman_ford step 3288 current loss 0.626438, current_train_items 105248.
I0302 18:59:25.805175 22895071117440 run.py:483] Algo bellman_ford step 3289 current loss 0.793902, current_train_items 105280.
I0302 18:59:25.823633 22895071117440 run.py:483] Algo bellman_ford step 3290 current loss 0.354786, current_train_items 105312.
I0302 18:59:25.839697 22895071117440 run.py:483] Algo bellman_ford step 3291 current loss 0.522348, current_train_items 105344.
I0302 18:59:25.861348 22895071117440 run.py:483] Algo bellman_ford step 3292 current loss 0.727286, current_train_items 105376.
I0302 18:59:25.890368 22895071117440 run.py:483] Algo bellman_ford step 3293 current loss 0.919454, current_train_items 105408.
I0302 18:59:25.921305 22895071117440 run.py:483] Algo bellman_ford step 3294 current loss 0.928376, current_train_items 105440.
I0302 18:59:25.939400 22895071117440 run.py:483] Algo bellman_ford step 3295 current loss 0.348104, current_train_items 105472.
I0302 18:59:25.954564 22895071117440 run.py:483] Algo bellman_ford step 3296 current loss 0.486251, current_train_items 105504.
I0302 18:59:25.976990 22895071117440 run.py:483] Algo bellman_ford step 3297 current loss 0.866816, current_train_items 105536.
I0302 18:59:26.006791 22895071117440 run.py:483] Algo bellman_ford step 3298 current loss 0.962651, current_train_items 105568.
I0302 18:59:26.038846 22895071117440 run.py:483] Algo bellman_ford step 3299 current loss 1.071860, current_train_items 105600.
I0302 18:59:26.057151 22895071117440 run.py:483] Algo bellman_ford step 3300 current loss 0.381862, current_train_items 105632.
I0302 18:59:26.065170 22895071117440 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 18:59:26.065276 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 18:59:26.081156 22895071117440 run.py:483] Algo bellman_ford step 3301 current loss 0.544062, current_train_items 105664.
I0302 18:59:26.102752 22895071117440 run.py:483] Algo bellman_ford step 3302 current loss 0.712779, current_train_items 105696.
I0302 18:59:26.130169 22895071117440 run.py:483] Algo bellman_ford step 3303 current loss 0.759335, current_train_items 105728.
I0302 18:59:26.159487 22895071117440 run.py:483] Algo bellman_ford step 3304 current loss 0.891061, current_train_items 105760.
I0302 18:59:26.177746 22895071117440 run.py:483] Algo bellman_ford step 3305 current loss 0.227772, current_train_items 105792.
I0302 18:59:26.193203 22895071117440 run.py:483] Algo bellman_ford step 3306 current loss 0.647195, current_train_items 105824.
I0302 18:59:26.215282 22895071117440 run.py:483] Algo bellman_ford step 3307 current loss 0.775310, current_train_items 105856.
I0302 18:59:26.243908 22895071117440 run.py:483] Algo bellman_ford step 3308 current loss 0.782001, current_train_items 105888.
I0302 18:59:26.276166 22895071117440 run.py:483] Algo bellman_ford step 3309 current loss 0.957150, current_train_items 105920.
I0302 18:59:26.293998 22895071117440 run.py:483] Algo bellman_ford step 3310 current loss 0.373543, current_train_items 105952.
I0302 18:59:26.309576 22895071117440 run.py:483] Algo bellman_ford step 3311 current loss 0.569755, current_train_items 105984.
I0302 18:59:26.331387 22895071117440 run.py:483] Algo bellman_ford step 3312 current loss 0.757812, current_train_items 106016.
I0302 18:59:26.360335 22895071117440 run.py:483] Algo bellman_ford step 3313 current loss 0.939944, current_train_items 106048.
I0302 18:59:26.389822 22895071117440 run.py:483] Algo bellman_ford step 3314 current loss 1.096056, current_train_items 106080.
I0302 18:59:26.407628 22895071117440 run.py:483] Algo bellman_ford step 3315 current loss 0.360888, current_train_items 106112.
I0302 18:59:26.423386 22895071117440 run.py:483] Algo bellman_ford step 3316 current loss 0.583942, current_train_items 106144.
I0302 18:59:26.444466 22895071117440 run.py:483] Algo bellman_ford step 3317 current loss 0.708802, current_train_items 106176.
I0302 18:59:26.472764 22895071117440 run.py:483] Algo bellman_ford step 3318 current loss 0.761978, current_train_items 106208.
I0302 18:59:26.504144 22895071117440 run.py:483] Algo bellman_ford step 3319 current loss 0.932993, current_train_items 106240.
I0302 18:59:26.522063 22895071117440 run.py:483] Algo bellman_ford step 3320 current loss 0.330645, current_train_items 106272.
I0302 18:59:26.537467 22895071117440 run.py:483] Algo bellman_ford step 3321 current loss 0.575183, current_train_items 106304.
I0302 18:59:26.559831 22895071117440 run.py:483] Algo bellman_ford step 3322 current loss 0.741434, current_train_items 106336.
I0302 18:59:26.588510 22895071117440 run.py:483] Algo bellman_ford step 3323 current loss 0.780210, current_train_items 106368.
I0302 18:59:26.618278 22895071117440 run.py:483] Algo bellman_ford step 3324 current loss 0.890706, current_train_items 106400.
I0302 18:59:26.636107 22895071117440 run.py:483] Algo bellman_ford step 3325 current loss 0.357666, current_train_items 106432.
I0302 18:59:26.651564 22895071117440 run.py:483] Algo bellman_ford step 3326 current loss 0.551656, current_train_items 106464.
I0302 18:59:26.672739 22895071117440 run.py:483] Algo bellman_ford step 3327 current loss 0.730570, current_train_items 106496.
I0302 18:59:26.700310 22895071117440 run.py:483] Algo bellman_ford step 3328 current loss 0.781281, current_train_items 106528.
I0302 18:59:26.730656 22895071117440 run.py:483] Algo bellman_ford step 3329 current loss 0.847327, current_train_items 106560.
I0302 18:59:26.748735 22895071117440 run.py:483] Algo bellman_ford step 3330 current loss 0.444391, current_train_items 106592.
I0302 18:59:26.763964 22895071117440 run.py:483] Algo bellman_ford step 3331 current loss 0.558683, current_train_items 106624.
I0302 18:59:26.785192 22895071117440 run.py:483] Algo bellman_ford step 3332 current loss 0.822491, current_train_items 106656.
I0302 18:59:26.812639 22895071117440 run.py:483] Algo bellman_ford step 3333 current loss 0.721423, current_train_items 106688.
I0302 18:59:26.842030 22895071117440 run.py:483] Algo bellman_ford step 3334 current loss 0.738774, current_train_items 106720.
I0302 18:59:26.859688 22895071117440 run.py:483] Algo bellman_ford step 3335 current loss 0.302895, current_train_items 106752.
I0302 18:59:26.874577 22895071117440 run.py:483] Algo bellman_ford step 3336 current loss 0.601543, current_train_items 106784.
I0302 18:59:26.896605 22895071117440 run.py:483] Algo bellman_ford step 3337 current loss 0.754707, current_train_items 106816.
I0302 18:59:26.926052 22895071117440 run.py:483] Algo bellman_ford step 3338 current loss 0.799346, current_train_items 106848.
I0302 18:59:26.958615 22895071117440 run.py:483] Algo bellman_ford step 3339 current loss 0.935576, current_train_items 106880.
I0302 18:59:26.976128 22895071117440 run.py:483] Algo bellman_ford step 3340 current loss 0.278460, current_train_items 106912.
I0302 18:59:26.991918 22895071117440 run.py:483] Algo bellman_ford step 3341 current loss 0.499081, current_train_items 106944.
I0302 18:59:27.013726 22895071117440 run.py:483] Algo bellman_ford step 3342 current loss 0.728346, current_train_items 106976.
I0302 18:59:27.042840 22895071117440 run.py:483] Algo bellman_ford step 3343 current loss 0.887476, current_train_items 107008.
I0302 18:59:27.071662 22895071117440 run.py:483] Algo bellman_ford step 3344 current loss 0.780772, current_train_items 107040.
I0302 18:59:27.089573 22895071117440 run.py:483] Algo bellman_ford step 3345 current loss 0.361785, current_train_items 107072.
I0302 18:59:27.104602 22895071117440 run.py:483] Algo bellman_ford step 3346 current loss 0.496409, current_train_items 107104.
I0302 18:59:27.126422 22895071117440 run.py:483] Algo bellman_ford step 3347 current loss 0.704062, current_train_items 107136.
I0302 18:59:27.154256 22895071117440 run.py:483] Algo bellman_ford step 3348 current loss 0.748481, current_train_items 107168.
I0302 18:59:27.182973 22895071117440 run.py:483] Algo bellman_ford step 3349 current loss 0.790214, current_train_items 107200.
I0302 18:59:27.200994 22895071117440 run.py:483] Algo bellman_ford step 3350 current loss 0.369543, current_train_items 107232.
I0302 18:59:27.208976 22895071117440 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 18:59:27.209086 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 18:59:27.225580 22895071117440 run.py:483] Algo bellman_ford step 3351 current loss 0.551157, current_train_items 107264.
I0302 18:59:27.247828 22895071117440 run.py:483] Algo bellman_ford step 3352 current loss 0.856940, current_train_items 107296.
I0302 18:59:27.277236 22895071117440 run.py:483] Algo bellman_ford step 3353 current loss 0.710820, current_train_items 107328.
I0302 18:59:27.309097 22895071117440 run.py:483] Algo bellman_ford step 3354 current loss 0.990882, current_train_items 107360.
I0302 18:59:27.327605 22895071117440 run.py:483] Algo bellman_ford step 3355 current loss 0.308149, current_train_items 107392.
I0302 18:59:27.342788 22895071117440 run.py:483] Algo bellman_ford step 3356 current loss 0.559552, current_train_items 107424.
I0302 18:59:27.364725 22895071117440 run.py:483] Algo bellman_ford step 3357 current loss 0.772634, current_train_items 107456.
I0302 18:59:27.392985 22895071117440 run.py:483] Algo bellman_ford step 3358 current loss 0.930015, current_train_items 107488.
I0302 18:59:27.425374 22895071117440 run.py:483] Algo bellman_ford step 3359 current loss 0.999089, current_train_items 107520.
I0302 18:59:27.443614 22895071117440 run.py:483] Algo bellman_ford step 3360 current loss 0.331515, current_train_items 107552.
I0302 18:59:27.459127 22895071117440 run.py:483] Algo bellman_ford step 3361 current loss 0.562949, current_train_items 107584.
I0302 18:59:27.481296 22895071117440 run.py:483] Algo bellman_ford step 3362 current loss 0.699267, current_train_items 107616.
I0302 18:59:27.509254 22895071117440 run.py:483] Algo bellman_ford step 3363 current loss 0.670593, current_train_items 107648.
I0302 18:59:27.539223 22895071117440 run.py:483] Algo bellman_ford step 3364 current loss 1.014328, current_train_items 107680.
I0302 18:59:27.557285 22895071117440 run.py:483] Algo bellman_ford step 3365 current loss 0.408119, current_train_items 107712.
I0302 18:59:27.572244 22895071117440 run.py:483] Algo bellman_ford step 3366 current loss 0.525776, current_train_items 107744.
I0302 18:59:27.594629 22895071117440 run.py:483] Algo bellman_ford step 3367 current loss 0.746988, current_train_items 107776.
I0302 18:59:27.622034 22895071117440 run.py:483] Algo bellman_ford step 3368 current loss 0.749630, current_train_items 107808.
I0302 18:59:27.651784 22895071117440 run.py:483] Algo bellman_ford step 3369 current loss 0.778083, current_train_items 107840.
I0302 18:59:27.670209 22895071117440 run.py:483] Algo bellman_ford step 3370 current loss 0.374851, current_train_items 107872.
I0302 18:59:27.686034 22895071117440 run.py:483] Algo bellman_ford step 3371 current loss 0.523463, current_train_items 107904.
I0302 18:59:27.707631 22895071117440 run.py:483] Algo bellman_ford step 3372 current loss 0.721904, current_train_items 107936.
I0302 18:59:27.737330 22895071117440 run.py:483] Algo bellman_ford step 3373 current loss 0.933678, current_train_items 107968.
I0302 18:59:27.767825 22895071117440 run.py:483] Algo bellman_ford step 3374 current loss 0.906620, current_train_items 108000.
I0302 18:59:27.786134 22895071117440 run.py:483] Algo bellman_ford step 3375 current loss 0.330031, current_train_items 108032.
I0302 18:59:27.801471 22895071117440 run.py:483] Algo bellman_ford step 3376 current loss 0.483067, current_train_items 108064.
I0302 18:59:27.822379 22895071117440 run.py:483] Algo bellman_ford step 3377 current loss 0.762663, current_train_items 108096.
I0302 18:59:27.849777 22895071117440 run.py:483] Algo bellman_ford step 3378 current loss 0.687395, current_train_items 108128.
I0302 18:59:27.881894 22895071117440 run.py:483] Algo bellman_ford step 3379 current loss 0.923999, current_train_items 108160.
I0302 18:59:27.899813 22895071117440 run.py:483] Algo bellman_ford step 3380 current loss 0.279581, current_train_items 108192.
I0302 18:59:27.915374 22895071117440 run.py:483] Algo bellman_ford step 3381 current loss 0.565174, current_train_items 108224.
I0302 18:59:27.937874 22895071117440 run.py:483] Algo bellman_ford step 3382 current loss 0.678330, current_train_items 108256.
I0302 18:59:27.965456 22895071117440 run.py:483] Algo bellman_ford step 3383 current loss 0.755447, current_train_items 108288.
I0302 18:59:27.997712 22895071117440 run.py:483] Algo bellman_ford step 3384 current loss 1.067441, current_train_items 108320.
I0302 18:59:28.016500 22895071117440 run.py:483] Algo bellman_ford step 3385 current loss 0.389806, current_train_items 108352.
I0302 18:59:28.032430 22895071117440 run.py:483] Algo bellman_ford step 3386 current loss 0.667708, current_train_items 108384.
I0302 18:59:28.053767 22895071117440 run.py:483] Algo bellman_ford step 3387 current loss 1.165567, current_train_items 108416.
I0302 18:59:28.081649 22895071117440 run.py:483] Algo bellman_ford step 3388 current loss 1.686854, current_train_items 108448.
I0302 18:59:28.110722 22895071117440 run.py:483] Algo bellman_ford step 3389 current loss 1.967892, current_train_items 108480.
I0302 18:59:28.129079 22895071117440 run.py:483] Algo bellman_ford step 3390 current loss 0.338959, current_train_items 108512.
I0302 18:59:28.144782 22895071117440 run.py:483] Algo bellman_ford step 3391 current loss 0.583135, current_train_items 108544.
I0302 18:59:28.166514 22895071117440 run.py:483] Algo bellman_ford step 3392 current loss 0.823405, current_train_items 108576.
I0302 18:59:28.194007 22895071117440 run.py:483] Algo bellman_ford step 3393 current loss 0.790681, current_train_items 108608.
I0302 18:59:28.223354 22895071117440 run.py:483] Algo bellman_ford step 3394 current loss 1.100712, current_train_items 108640.
I0302 18:59:28.241568 22895071117440 run.py:483] Algo bellman_ford step 3395 current loss 0.442579, current_train_items 108672.
I0302 18:59:28.257080 22895071117440 run.py:483] Algo bellman_ford step 3396 current loss 0.657577, current_train_items 108704.
I0302 18:59:28.278635 22895071117440 run.py:483] Algo bellman_ford step 3397 current loss 0.802046, current_train_items 108736.
I0302 18:59:28.307524 22895071117440 run.py:483] Algo bellman_ford step 3398 current loss 0.922742, current_train_items 108768.
I0302 18:59:28.337123 22895071117440 run.py:483] Algo bellman_ford step 3399 current loss 0.910145, current_train_items 108800.
I0302 18:59:28.355541 22895071117440 run.py:483] Algo bellman_ford step 3400 current loss 0.356806, current_train_items 108832.
I0302 18:59:28.363255 22895071117440 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.8681640625, 'score': 0.8681640625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 18:59:28.363362 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.868, val scores are: bellman_ford: 0.868
I0302 18:59:28.379709 22895071117440 run.py:483] Algo bellman_ford step 3401 current loss 0.635491, current_train_items 108864.
I0302 18:59:28.401860 22895071117440 run.py:483] Algo bellman_ford step 3402 current loss 0.707489, current_train_items 108896.
I0302 18:59:28.430790 22895071117440 run.py:483] Algo bellman_ford step 3403 current loss 0.901659, current_train_items 108928.
I0302 18:59:28.463515 22895071117440 run.py:483] Algo bellman_ford step 3404 current loss 1.148368, current_train_items 108960.
I0302 18:59:28.481710 22895071117440 run.py:483] Algo bellman_ford step 3405 current loss 0.279160, current_train_items 108992.
I0302 18:59:28.496713 22895071117440 run.py:483] Algo bellman_ford step 3406 current loss 0.645457, current_train_items 109024.
I0302 18:59:28.519243 22895071117440 run.py:483] Algo bellman_ford step 3407 current loss 0.807402, current_train_items 109056.
I0302 18:59:28.549079 22895071117440 run.py:483] Algo bellman_ford step 3408 current loss 0.934107, current_train_items 109088.
I0302 18:59:28.579278 22895071117440 run.py:483] Algo bellman_ford step 3409 current loss 0.927943, current_train_items 109120.
I0302 18:59:28.597296 22895071117440 run.py:483] Algo bellman_ford step 3410 current loss 0.333762, current_train_items 109152.
I0302 18:59:28.612774 22895071117440 run.py:483] Algo bellman_ford step 3411 current loss 0.793647, current_train_items 109184.
I0302 18:59:28.635123 22895071117440 run.py:483] Algo bellman_ford step 3412 current loss 0.655414, current_train_items 109216.
I0302 18:59:28.664960 22895071117440 run.py:483] Algo bellman_ford step 3413 current loss 0.935510, current_train_items 109248.
I0302 18:59:28.698655 22895071117440 run.py:483] Algo bellman_ford step 3414 current loss 1.103054, current_train_items 109280.
I0302 18:59:28.716601 22895071117440 run.py:483] Algo bellman_ford step 3415 current loss 0.326777, current_train_items 109312.
I0302 18:59:28.732082 22895071117440 run.py:483] Algo bellman_ford step 3416 current loss 0.539685, current_train_items 109344.
I0302 18:59:28.754122 22895071117440 run.py:483] Algo bellman_ford step 3417 current loss 0.792868, current_train_items 109376.
I0302 18:59:28.782856 22895071117440 run.py:483] Algo bellman_ford step 3418 current loss 0.877232, current_train_items 109408.
I0302 18:59:28.812760 22895071117440 run.py:483] Algo bellman_ford step 3419 current loss 0.778168, current_train_items 109440.
I0302 18:59:28.831002 22895071117440 run.py:483] Algo bellman_ford step 3420 current loss 0.404079, current_train_items 109472.
I0302 18:59:28.846114 22895071117440 run.py:483] Algo bellman_ford step 3421 current loss 0.561708, current_train_items 109504.
I0302 18:59:28.868087 22895071117440 run.py:483] Algo bellman_ford step 3422 current loss 0.682308, current_train_items 109536.
I0302 18:59:28.896731 22895071117440 run.py:483] Algo bellman_ford step 3423 current loss 0.748027, current_train_items 109568.
I0302 18:59:28.927209 22895071117440 run.py:483] Algo bellman_ford step 3424 current loss 1.132625, current_train_items 109600.
I0302 18:59:28.945027 22895071117440 run.py:483] Algo bellman_ford step 3425 current loss 0.325229, current_train_items 109632.
I0302 18:59:28.960861 22895071117440 run.py:483] Algo bellman_ford step 3426 current loss 0.572358, current_train_items 109664.
I0302 18:59:28.982442 22895071117440 run.py:483] Algo bellman_ford step 3427 current loss 0.654580, current_train_items 109696.
I0302 18:59:29.011265 22895071117440 run.py:483] Algo bellman_ford step 3428 current loss 0.821642, current_train_items 109728.
I0302 18:59:29.039555 22895071117440 run.py:483] Algo bellman_ford step 3429 current loss 0.708081, current_train_items 109760.
I0302 18:59:29.057569 22895071117440 run.py:483] Algo bellman_ford step 3430 current loss 0.326490, current_train_items 109792.
I0302 18:59:29.073191 22895071117440 run.py:483] Algo bellman_ford step 3431 current loss 0.580511, current_train_items 109824.
I0302 18:59:29.094666 22895071117440 run.py:483] Algo bellman_ford step 3432 current loss 0.695560, current_train_items 109856.
I0302 18:59:29.125145 22895071117440 run.py:483] Algo bellman_ford step 3433 current loss 1.041014, current_train_items 109888.
I0302 18:59:29.154976 22895071117440 run.py:483] Algo bellman_ford step 3434 current loss 0.862010, current_train_items 109920.
I0302 18:59:29.173098 22895071117440 run.py:483] Algo bellman_ford step 3435 current loss 0.368270, current_train_items 109952.
I0302 18:59:29.188503 22895071117440 run.py:483] Algo bellman_ford step 3436 current loss 0.510915, current_train_items 109984.
I0302 18:59:29.210833 22895071117440 run.py:483] Algo bellman_ford step 3437 current loss 0.741962, current_train_items 110016.
I0302 18:59:29.238482 22895071117440 run.py:483] Algo bellman_ford step 3438 current loss 0.813749, current_train_items 110048.
I0302 18:59:29.267482 22895071117440 run.py:483] Algo bellman_ford step 3439 current loss 0.847312, current_train_items 110080.
I0302 18:59:29.285546 22895071117440 run.py:483] Algo bellman_ford step 3440 current loss 0.354419, current_train_items 110112.
I0302 18:59:29.301262 22895071117440 run.py:483] Algo bellman_ford step 3441 current loss 0.551282, current_train_items 110144.
I0302 18:59:29.322568 22895071117440 run.py:483] Algo bellman_ford step 3442 current loss 0.689759, current_train_items 110176.
I0302 18:59:29.351423 22895071117440 run.py:483] Algo bellman_ford step 3443 current loss 0.891612, current_train_items 110208.
I0302 18:59:29.383262 22895071117440 run.py:483] Algo bellman_ford step 3444 current loss 0.865326, current_train_items 110240.
I0302 18:59:29.401195 22895071117440 run.py:483] Algo bellman_ford step 3445 current loss 0.330433, current_train_items 110272.
I0302 18:59:29.416915 22895071117440 run.py:483] Algo bellman_ford step 3446 current loss 0.583841, current_train_items 110304.
I0302 18:59:29.438966 22895071117440 run.py:483] Algo bellman_ford step 3447 current loss 0.693165, current_train_items 110336.
I0302 18:59:29.466767 22895071117440 run.py:483] Algo bellman_ford step 3448 current loss 0.704960, current_train_items 110368.
I0302 18:59:29.497711 22895071117440 run.py:483] Algo bellman_ford step 3449 current loss 0.967086, current_train_items 110400.
I0302 18:59:29.515932 22895071117440 run.py:483] Algo bellman_ford step 3450 current loss 0.364794, current_train_items 110432.
I0302 18:59:29.523916 22895071117440 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 18:59:29.524024 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:29.539977 22895071117440 run.py:483] Algo bellman_ford step 3451 current loss 0.589679, current_train_items 110464.
I0302 18:59:29.562145 22895071117440 run.py:483] Algo bellman_ford step 3452 current loss 0.674239, current_train_items 110496.
I0302 18:59:29.589882 22895071117440 run.py:483] Algo bellman_ford step 3453 current loss 0.786208, current_train_items 110528.
I0302 18:59:29.623167 22895071117440 run.py:483] Algo bellman_ford step 3454 current loss 0.961538, current_train_items 110560.
I0302 18:59:29.641523 22895071117440 run.py:483] Algo bellman_ford step 3455 current loss 0.300682, current_train_items 110592.
I0302 18:59:29.656473 22895071117440 run.py:483] Algo bellman_ford step 3456 current loss 0.590232, current_train_items 110624.
I0302 18:59:29.678128 22895071117440 run.py:483] Algo bellman_ford step 3457 current loss 0.716140, current_train_items 110656.
I0302 18:59:29.707278 22895071117440 run.py:483] Algo bellman_ford step 3458 current loss 0.791694, current_train_items 110688.
I0302 18:59:29.737029 22895071117440 run.py:483] Algo bellman_ford step 3459 current loss 0.927857, current_train_items 110720.
I0302 18:59:29.755189 22895071117440 run.py:483] Algo bellman_ford step 3460 current loss 0.326964, current_train_items 110752.
I0302 18:59:29.771063 22895071117440 run.py:483] Algo bellman_ford step 3461 current loss 0.579716, current_train_items 110784.
I0302 18:59:29.792392 22895071117440 run.py:483] Algo bellman_ford step 3462 current loss 0.814309, current_train_items 110816.
I0302 18:59:29.820378 22895071117440 run.py:483] Algo bellman_ford step 3463 current loss 0.835940, current_train_items 110848.
I0302 18:59:29.850061 22895071117440 run.py:483] Algo bellman_ford step 3464 current loss 0.903567, current_train_items 110880.
I0302 18:59:29.867796 22895071117440 run.py:483] Algo bellman_ford step 3465 current loss 0.312054, current_train_items 110912.
I0302 18:59:29.883289 22895071117440 run.py:483] Algo bellman_ford step 3466 current loss 0.643941, current_train_items 110944.
I0302 18:59:29.905463 22895071117440 run.py:483] Algo bellman_ford step 3467 current loss 0.783994, current_train_items 110976.
I0302 18:59:29.934786 22895071117440 run.py:483] Algo bellman_ford step 3468 current loss 0.869881, current_train_items 111008.
I0302 18:59:29.967283 22895071117440 run.py:483] Algo bellman_ford step 3469 current loss 0.996593, current_train_items 111040.
I0302 18:59:29.985444 22895071117440 run.py:483] Algo bellman_ford step 3470 current loss 0.331235, current_train_items 111072.
I0302 18:59:30.001601 22895071117440 run.py:483] Algo bellman_ford step 3471 current loss 0.562867, current_train_items 111104.
I0302 18:59:30.022845 22895071117440 run.py:483] Algo bellman_ford step 3472 current loss 0.687219, current_train_items 111136.
I0302 18:59:30.052048 22895071117440 run.py:483] Algo bellman_ford step 3473 current loss 0.792401, current_train_items 111168.
I0302 18:59:30.082637 22895071117440 run.py:483] Algo bellman_ford step 3474 current loss 0.937278, current_train_items 111200.
I0302 18:59:30.100905 22895071117440 run.py:483] Algo bellman_ford step 3475 current loss 0.284558, current_train_items 111232.
I0302 18:59:30.116356 22895071117440 run.py:483] Algo bellman_ford step 3476 current loss 0.520746, current_train_items 111264.
I0302 18:59:30.137107 22895071117440 run.py:483] Algo bellman_ford step 3477 current loss 0.723971, current_train_items 111296.
I0302 18:59:30.164266 22895071117440 run.py:483] Algo bellman_ford step 3478 current loss 0.644265, current_train_items 111328.
I0302 18:59:30.196010 22895071117440 run.py:483] Algo bellman_ford step 3479 current loss 1.050612, current_train_items 111360.
I0302 18:59:30.213907 22895071117440 run.py:483] Algo bellman_ford step 3480 current loss 0.302474, current_train_items 111392.
I0302 18:59:30.229075 22895071117440 run.py:483] Algo bellman_ford step 3481 current loss 0.511349, current_train_items 111424.
I0302 18:59:30.250401 22895071117440 run.py:483] Algo bellman_ford step 3482 current loss 0.632400, current_train_items 111456.
I0302 18:59:30.277620 22895071117440 run.py:483] Algo bellman_ford step 3483 current loss 0.761221, current_train_items 111488.
I0302 18:59:30.307539 22895071117440 run.py:483] Algo bellman_ford step 3484 current loss 0.872414, current_train_items 111520.
I0302 18:59:30.325664 22895071117440 run.py:483] Algo bellman_ford step 3485 current loss 0.291529, current_train_items 111552.
I0302 18:59:30.341449 22895071117440 run.py:483] Algo bellman_ford step 3486 current loss 0.623922, current_train_items 111584.
I0302 18:59:30.363330 22895071117440 run.py:483] Algo bellman_ford step 3487 current loss 0.839557, current_train_items 111616.
I0302 18:59:30.390779 22895071117440 run.py:483] Algo bellman_ford step 3488 current loss 0.779174, current_train_items 111648.
I0302 18:59:30.419889 22895071117440 run.py:483] Algo bellman_ford step 3489 current loss 0.783273, current_train_items 111680.
I0302 18:59:30.438112 22895071117440 run.py:483] Algo bellman_ford step 3490 current loss 0.332990, current_train_items 111712.
I0302 18:59:30.453316 22895071117440 run.py:483] Algo bellman_ford step 3491 current loss 0.424022, current_train_items 111744.
I0302 18:59:30.474369 22895071117440 run.py:483] Algo bellman_ford step 3492 current loss 0.744827, current_train_items 111776.
I0302 18:59:30.503601 22895071117440 run.py:483] Algo bellman_ford step 3493 current loss 0.887011, current_train_items 111808.
I0302 18:59:30.533533 22895071117440 run.py:483] Algo bellman_ford step 3494 current loss 0.880182, current_train_items 111840.
I0302 18:59:30.551561 22895071117440 run.py:483] Algo bellman_ford step 3495 current loss 0.314285, current_train_items 111872.
I0302 18:59:30.566828 22895071117440 run.py:483] Algo bellman_ford step 3496 current loss 0.547766, current_train_items 111904.
I0302 18:59:30.588279 22895071117440 run.py:483] Algo bellman_ford step 3497 current loss 0.776350, current_train_items 111936.
I0302 18:59:30.616593 22895071117440 run.py:483] Algo bellman_ford step 3498 current loss 0.778928, current_train_items 111968.
I0302 18:59:30.647827 22895071117440 run.py:483] Algo bellman_ford step 3499 current loss 0.942964, current_train_items 112000.
I0302 18:59:30.666035 22895071117440 run.py:483] Algo bellman_ford step 3500 current loss 0.317795, current_train_items 112032.
I0302 18:59:30.673760 22895071117440 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 18:59:30.673868 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 18:59:30.689350 22895071117440 run.py:483] Algo bellman_ford step 3501 current loss 0.461195, current_train_items 112064.
I0302 18:59:30.711347 22895071117440 run.py:483] Algo bellman_ford step 3502 current loss 0.661327, current_train_items 112096.
I0302 18:59:30.740487 22895071117440 run.py:483] Algo bellman_ford step 3503 current loss 0.835157, current_train_items 112128.
I0302 18:59:30.773750 22895071117440 run.py:483] Algo bellman_ford step 3504 current loss 0.916022, current_train_items 112160.
I0302 18:59:30.792383 22895071117440 run.py:483] Algo bellman_ford step 3505 current loss 0.398077, current_train_items 112192.
I0302 18:59:30.807076 22895071117440 run.py:483] Algo bellman_ford step 3506 current loss 0.517635, current_train_items 112224.
I0302 18:59:30.829474 22895071117440 run.py:483] Algo bellman_ford step 3507 current loss 0.755826, current_train_items 112256.
I0302 18:59:30.856825 22895071117440 run.py:483] Algo bellman_ford step 3508 current loss 0.780804, current_train_items 112288.
I0302 18:59:30.884717 22895071117440 run.py:483] Algo bellman_ford step 3509 current loss 1.014143, current_train_items 112320.
I0302 18:59:30.902468 22895071117440 run.py:483] Algo bellman_ford step 3510 current loss 0.388372, current_train_items 112352.
I0302 18:59:30.917651 22895071117440 run.py:483] Algo bellman_ford step 3511 current loss 0.461266, current_train_items 112384.
I0302 18:59:30.938822 22895071117440 run.py:483] Algo bellman_ford step 3512 current loss 0.682362, current_train_items 112416.
I0302 18:59:30.966246 22895071117440 run.py:483] Algo bellman_ford step 3513 current loss 0.775051, current_train_items 112448.
I0302 18:59:30.995707 22895071117440 run.py:483] Algo bellman_ford step 3514 current loss 0.852726, current_train_items 112480.
I0302 18:59:31.013294 22895071117440 run.py:483] Algo bellman_ford step 3515 current loss 0.334499, current_train_items 112512.
I0302 18:59:31.028171 22895071117440 run.py:483] Algo bellman_ford step 3516 current loss 0.493782, current_train_items 112544.
I0302 18:59:31.049928 22895071117440 run.py:483] Algo bellman_ford step 3517 current loss 0.674907, current_train_items 112576.
I0302 18:59:31.078430 22895071117440 run.py:483] Algo bellman_ford step 3518 current loss 0.769745, current_train_items 112608.
I0302 18:59:31.111567 22895071117440 run.py:483] Algo bellman_ford step 3519 current loss 1.035682, current_train_items 112640.
I0302 18:59:31.129577 22895071117440 run.py:483] Algo bellman_ford step 3520 current loss 0.337498, current_train_items 112672.
I0302 18:59:31.144745 22895071117440 run.py:483] Algo bellman_ford step 3521 current loss 0.497731, current_train_items 112704.
I0302 18:59:31.165913 22895071117440 run.py:483] Algo bellman_ford step 3522 current loss 0.564970, current_train_items 112736.
I0302 18:59:31.193954 22895071117440 run.py:483] Algo bellman_ford step 3523 current loss 0.801053, current_train_items 112768.
I0302 18:59:31.224294 22895071117440 run.py:483] Algo bellman_ford step 3524 current loss 0.968294, current_train_items 112800.
I0302 18:59:31.242232 22895071117440 run.py:483] Algo bellman_ford step 3525 current loss 0.391052, current_train_items 112832.
I0302 18:59:31.257274 22895071117440 run.py:483] Algo bellman_ford step 3526 current loss 0.505858, current_train_items 112864.
I0302 18:59:31.279049 22895071117440 run.py:483] Algo bellman_ford step 3527 current loss 0.692757, current_train_items 112896.
I0302 18:59:31.308970 22895071117440 run.py:483] Algo bellman_ford step 3528 current loss 0.726839, current_train_items 112928.
I0302 18:59:31.338379 22895071117440 run.py:483] Algo bellman_ford step 3529 current loss 0.821157, current_train_items 112960.
I0302 18:59:31.356113 22895071117440 run.py:483] Algo bellman_ford step 3530 current loss 0.290531, current_train_items 112992.
I0302 18:59:31.371594 22895071117440 run.py:483] Algo bellman_ford step 3531 current loss 0.553646, current_train_items 113024.
I0302 18:59:31.394682 22895071117440 run.py:483] Algo bellman_ford step 3532 current loss 0.797024, current_train_items 113056.
I0302 18:59:31.425560 22895071117440 run.py:483] Algo bellman_ford step 3533 current loss 1.109812, current_train_items 113088.
I0302 18:59:31.456535 22895071117440 run.py:483] Algo bellman_ford step 3534 current loss 1.114962, current_train_items 113120.
I0302 18:59:31.474726 22895071117440 run.py:483] Algo bellman_ford step 3535 current loss 0.317399, current_train_items 113152.
I0302 18:59:31.490176 22895071117440 run.py:483] Algo bellman_ford step 3536 current loss 0.557782, current_train_items 113184.
I0302 18:59:31.511612 22895071117440 run.py:483] Algo bellman_ford step 3537 current loss 0.833085, current_train_items 113216.
I0302 18:59:31.541008 22895071117440 run.py:483] Algo bellman_ford step 3538 current loss 0.833550, current_train_items 113248.
I0302 18:59:31.573457 22895071117440 run.py:483] Algo bellman_ford step 3539 current loss 1.017589, current_train_items 113280.
I0302 18:59:31.591358 22895071117440 run.py:483] Algo bellman_ford step 3540 current loss 0.402604, current_train_items 113312.
I0302 18:59:31.606612 22895071117440 run.py:483] Algo bellman_ford step 3541 current loss 0.767313, current_train_items 113344.
I0302 18:59:31.628973 22895071117440 run.py:483] Algo bellman_ford step 3542 current loss 0.889662, current_train_items 113376.
I0302 18:59:31.656772 22895071117440 run.py:483] Algo bellman_ford step 3543 current loss 0.761389, current_train_items 113408.
I0302 18:59:31.686975 22895071117440 run.py:483] Algo bellman_ford step 3544 current loss 0.938235, current_train_items 113440.
I0302 18:59:31.704710 22895071117440 run.py:483] Algo bellman_ford step 3545 current loss 0.303479, current_train_items 113472.
I0302 18:59:31.719821 22895071117440 run.py:483] Algo bellman_ford step 3546 current loss 0.557835, current_train_items 113504.
I0302 18:59:31.742147 22895071117440 run.py:483] Algo bellman_ford step 3547 current loss 0.905309, current_train_items 113536.
I0302 18:59:31.769150 22895071117440 run.py:483] Algo bellman_ford step 3548 current loss 0.888389, current_train_items 113568.
I0302 18:59:31.799574 22895071117440 run.py:483] Algo bellman_ford step 3549 current loss 1.054426, current_train_items 113600.
I0302 18:59:31.817444 22895071117440 run.py:483] Algo bellman_ford step 3550 current loss 0.328955, current_train_items 113632.
I0302 18:59:31.825364 22895071117440 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.87109375, 'score': 0.87109375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 18:59:31.825469 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.871, val scores are: bellman_ford: 0.871
I0302 18:59:31.842058 22895071117440 run.py:483] Algo bellman_ford step 3551 current loss 0.627405, current_train_items 113664.
I0302 18:59:31.865684 22895071117440 run.py:483] Algo bellman_ford step 3552 current loss 0.790874, current_train_items 113696.
I0302 18:59:31.894502 22895071117440 run.py:483] Algo bellman_ford step 3553 current loss 0.743580, current_train_items 113728.
I0302 18:59:31.926270 22895071117440 run.py:483] Algo bellman_ford step 3554 current loss 0.895344, current_train_items 113760.
I0302 18:59:31.944796 22895071117440 run.py:483] Algo bellman_ford step 3555 current loss 0.453344, current_train_items 113792.
I0302 18:59:31.960150 22895071117440 run.py:483] Algo bellman_ford step 3556 current loss 0.617952, current_train_items 113824.
I0302 18:59:31.981807 22895071117440 run.py:483] Algo bellman_ford step 3557 current loss 0.714803, current_train_items 113856.
I0302 18:59:32.011155 22895071117440 run.py:483] Algo bellman_ford step 3558 current loss 0.933386, current_train_items 113888.
I0302 18:59:32.041340 22895071117440 run.py:483] Algo bellman_ford step 3559 current loss 0.818118, current_train_items 113920.
I0302 18:59:32.059894 22895071117440 run.py:483] Algo bellman_ford step 3560 current loss 0.372917, current_train_items 113952.
I0302 18:59:32.075482 22895071117440 run.py:483] Algo bellman_ford step 3561 current loss 0.586346, current_train_items 113984.
I0302 18:59:32.098451 22895071117440 run.py:483] Algo bellman_ford step 3562 current loss 1.008591, current_train_items 114016.
I0302 18:59:32.126110 22895071117440 run.py:483] Algo bellman_ford step 3563 current loss 0.976090, current_train_items 114048.
I0302 18:59:32.159973 22895071117440 run.py:483] Algo bellman_ford step 3564 current loss 1.251692, current_train_items 114080.
I0302 18:59:32.178123 22895071117440 run.py:483] Algo bellman_ford step 3565 current loss 0.273978, current_train_items 114112.
I0302 18:59:32.193686 22895071117440 run.py:483] Algo bellman_ford step 3566 current loss 0.617288, current_train_items 114144.
I0302 18:59:32.215599 22895071117440 run.py:483] Algo bellman_ford step 3567 current loss 0.605710, current_train_items 114176.
I0302 18:59:32.243853 22895071117440 run.py:483] Algo bellman_ford step 3568 current loss 0.771827, current_train_items 114208.
I0302 18:59:32.275138 22895071117440 run.py:483] Algo bellman_ford step 3569 current loss 0.883704, current_train_items 114240.
I0302 18:59:32.293509 22895071117440 run.py:483] Algo bellman_ford step 3570 current loss 0.224870, current_train_items 114272.
I0302 18:59:32.308885 22895071117440 run.py:483] Algo bellman_ford step 3571 current loss 0.445236, current_train_items 114304.
I0302 18:59:32.330379 22895071117440 run.py:483] Algo bellman_ford step 3572 current loss 0.731955, current_train_items 114336.
I0302 18:59:32.357489 22895071117440 run.py:483] Algo bellman_ford step 3573 current loss 0.751812, current_train_items 114368.
I0302 18:59:32.389975 22895071117440 run.py:483] Algo bellman_ford step 3574 current loss 0.938967, current_train_items 114400.
I0302 18:59:32.408249 22895071117440 run.py:483] Algo bellman_ford step 3575 current loss 0.298655, current_train_items 114432.
I0302 18:59:32.424018 22895071117440 run.py:483] Algo bellman_ford step 3576 current loss 0.529278, current_train_items 114464.
I0302 18:59:32.445534 22895071117440 run.py:483] Algo bellman_ford step 3577 current loss 0.710265, current_train_items 114496.
I0302 18:59:32.473650 22895071117440 run.py:483] Algo bellman_ford step 3578 current loss 0.780621, current_train_items 114528.
I0302 18:59:32.503587 22895071117440 run.py:483] Algo bellman_ford step 3579 current loss 0.850839, current_train_items 114560.
I0302 18:59:32.521931 22895071117440 run.py:483] Algo bellman_ford step 3580 current loss 0.346903, current_train_items 114592.
I0302 18:59:32.537348 22895071117440 run.py:483] Algo bellman_ford step 3581 current loss 0.542036, current_train_items 114624.
I0302 18:59:32.560485 22895071117440 run.py:483] Algo bellman_ford step 3582 current loss 0.746766, current_train_items 114656.
I0302 18:59:32.589611 22895071117440 run.py:483] Algo bellman_ford step 3583 current loss 0.762381, current_train_items 114688.
I0302 18:59:32.621201 22895071117440 run.py:483] Algo bellman_ford step 3584 current loss 0.851670, current_train_items 114720.
I0302 18:59:32.639695 22895071117440 run.py:483] Algo bellman_ford step 3585 current loss 0.360477, current_train_items 114752.
I0302 18:59:32.655466 22895071117440 run.py:483] Algo bellman_ford step 3586 current loss 0.587656, current_train_items 114784.
I0302 18:59:32.676602 22895071117440 run.py:483] Algo bellman_ford step 3587 current loss 0.805960, current_train_items 114816.
I0302 18:59:32.705937 22895071117440 run.py:483] Algo bellman_ford step 3588 current loss 0.819386, current_train_items 114848.
I0302 18:59:32.737741 22895071117440 run.py:483] Algo bellman_ford step 3589 current loss 0.878573, current_train_items 114880.
I0302 18:59:32.756041 22895071117440 run.py:483] Algo bellman_ford step 3590 current loss 0.342140, current_train_items 114912.
I0302 18:59:32.771459 22895071117440 run.py:483] Algo bellman_ford step 3591 current loss 0.460987, current_train_items 114944.
I0302 18:59:32.794645 22895071117440 run.py:483] Algo bellman_ford step 3592 current loss 0.766826, current_train_items 114976.
I0302 18:59:32.824744 22895071117440 run.py:483] Algo bellman_ford step 3593 current loss 0.863373, current_train_items 115008.
I0302 18:59:32.856699 22895071117440 run.py:483] Algo bellman_ford step 3594 current loss 0.972069, current_train_items 115040.
I0302 18:59:32.874576 22895071117440 run.py:483] Algo bellman_ford step 3595 current loss 0.347528, current_train_items 115072.
I0302 18:59:32.890182 22895071117440 run.py:483] Algo bellman_ford step 3596 current loss 0.546201, current_train_items 115104.
I0302 18:59:32.911451 22895071117440 run.py:483] Algo bellman_ford step 3597 current loss 0.658444, current_train_items 115136.
I0302 18:59:32.938595 22895071117440 run.py:483] Algo bellman_ford step 3598 current loss 0.696716, current_train_items 115168.
I0302 18:59:32.969756 22895071117440 run.py:483] Algo bellman_ford step 3599 current loss 0.877726, current_train_items 115200.
I0302 18:59:32.988261 22895071117440 run.py:483] Algo bellman_ford step 3600 current loss 0.342930, current_train_items 115232.
I0302 18:59:32.996150 22895071117440 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 18:59:32.996256 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 18:59:33.012665 22895071117440 run.py:483] Algo bellman_ford step 3601 current loss 0.546187, current_train_items 115264.
I0302 18:59:33.035242 22895071117440 run.py:483] Algo bellman_ford step 3602 current loss 0.720865, current_train_items 115296.
I0302 18:59:33.063464 22895071117440 run.py:483] Algo bellman_ford step 3603 current loss 0.730508, current_train_items 115328.
I0302 18:59:33.097261 22895071117440 run.py:483] Algo bellman_ford step 3604 current loss 0.875554, current_train_items 115360.
I0302 18:59:33.115483 22895071117440 run.py:483] Algo bellman_ford step 3605 current loss 0.337625, current_train_items 115392.
I0302 18:59:33.131240 22895071117440 run.py:483] Algo bellman_ford step 3606 current loss 0.539806, current_train_items 115424.
I0302 18:59:33.151915 22895071117440 run.py:483] Algo bellman_ford step 3607 current loss 0.558724, current_train_items 115456.
I0302 18:59:33.178995 22895071117440 run.py:483] Algo bellman_ford step 3608 current loss 0.771977, current_train_items 115488.
I0302 18:59:33.208093 22895071117440 run.py:483] Algo bellman_ford step 3609 current loss 0.765075, current_train_items 115520.
I0302 18:59:33.226274 22895071117440 run.py:483] Algo bellman_ford step 3610 current loss 0.367321, current_train_items 115552.
I0302 18:59:33.241847 22895071117440 run.py:483] Algo bellman_ford step 3611 current loss 0.561530, current_train_items 115584.
I0302 18:59:33.264525 22895071117440 run.py:483] Algo bellman_ford step 3612 current loss 0.739771, current_train_items 115616.
I0302 18:59:33.292189 22895071117440 run.py:483] Algo bellman_ford step 3613 current loss 0.685013, current_train_items 115648.
I0302 18:59:33.321652 22895071117440 run.py:483] Algo bellman_ford step 3614 current loss 0.840232, current_train_items 115680.
I0302 18:59:33.339368 22895071117440 run.py:483] Algo bellman_ford step 3615 current loss 0.326382, current_train_items 115712.
I0302 18:59:33.355126 22895071117440 run.py:483] Algo bellman_ford step 3616 current loss 0.603616, current_train_items 115744.
I0302 18:59:33.376637 22895071117440 run.py:483] Algo bellman_ford step 3617 current loss 0.624023, current_train_items 115776.
I0302 18:59:33.405542 22895071117440 run.py:483] Algo bellman_ford step 3618 current loss 0.800623, current_train_items 115808.
I0302 18:59:33.433147 22895071117440 run.py:483] Algo bellman_ford step 3619 current loss 0.647920, current_train_items 115840.
I0302 18:59:33.451007 22895071117440 run.py:483] Algo bellman_ford step 3620 current loss 0.285705, current_train_items 115872.
I0302 18:59:33.466506 22895071117440 run.py:483] Algo bellman_ford step 3621 current loss 0.517821, current_train_items 115904.
I0302 18:59:33.487982 22895071117440 run.py:483] Algo bellman_ford step 3622 current loss 0.641709, current_train_items 115936.
I0302 18:59:33.516837 22895071117440 run.py:483] Algo bellman_ford step 3623 current loss 0.746849, current_train_items 115968.
I0302 18:59:33.546748 22895071117440 run.py:483] Algo bellman_ford step 3624 current loss 0.739302, current_train_items 116000.
I0302 18:59:33.564400 22895071117440 run.py:483] Algo bellman_ford step 3625 current loss 0.381635, current_train_items 116032.
I0302 18:59:33.579836 22895071117440 run.py:483] Algo bellman_ford step 3626 current loss 0.577064, current_train_items 116064.
I0302 18:59:33.601855 22895071117440 run.py:483] Algo bellman_ford step 3627 current loss 0.615047, current_train_items 116096.
I0302 18:59:33.630523 22895071117440 run.py:483] Algo bellman_ford step 3628 current loss 0.791902, current_train_items 116128.
I0302 18:59:33.660288 22895071117440 run.py:483] Algo bellman_ford step 3629 current loss 0.838630, current_train_items 116160.
I0302 18:59:33.678193 22895071117440 run.py:483] Algo bellman_ford step 3630 current loss 0.305869, current_train_items 116192.
I0302 18:59:33.693446 22895071117440 run.py:483] Algo bellman_ford step 3631 current loss 0.459380, current_train_items 116224.
I0302 18:59:33.715183 22895071117440 run.py:483] Algo bellman_ford step 3632 current loss 0.618809, current_train_items 116256.
I0302 18:59:33.743395 22895071117440 run.py:483] Algo bellman_ford step 3633 current loss 0.819951, current_train_items 116288.
I0302 18:59:33.773289 22895071117440 run.py:483] Algo bellman_ford step 3634 current loss 1.055790, current_train_items 116320.
I0302 18:59:33.790932 22895071117440 run.py:483] Algo bellman_ford step 3635 current loss 0.298648, current_train_items 116352.
I0302 18:59:33.806327 22895071117440 run.py:483] Algo bellman_ford step 3636 current loss 0.581124, current_train_items 116384.
I0302 18:59:33.827021 22895071117440 run.py:483] Algo bellman_ford step 3637 current loss 0.723702, current_train_items 116416.
I0302 18:59:33.855528 22895071117440 run.py:483] Algo bellman_ford step 3638 current loss 0.811163, current_train_items 116448.
I0302 18:59:33.885475 22895071117440 run.py:483] Algo bellman_ford step 3639 current loss 0.851429, current_train_items 116480.
I0302 18:59:33.903225 22895071117440 run.py:483] Algo bellman_ford step 3640 current loss 0.296713, current_train_items 116512.
I0302 18:59:33.918185 22895071117440 run.py:483] Algo bellman_ford step 3641 current loss 0.484211, current_train_items 116544.
I0302 18:59:33.940455 22895071117440 run.py:483] Algo bellman_ford step 3642 current loss 0.873894, current_train_items 116576.
I0302 18:59:33.971513 22895071117440 run.py:483] Algo bellman_ford step 3643 current loss 0.825964, current_train_items 116608.
I0302 18:59:34.002832 22895071117440 run.py:483] Algo bellman_ford step 3644 current loss 1.025336, current_train_items 116640.
I0302 18:59:34.020667 22895071117440 run.py:483] Algo bellman_ford step 3645 current loss 0.274984, current_train_items 116672.
I0302 18:59:34.036253 22895071117440 run.py:483] Algo bellman_ford step 3646 current loss 0.525939, current_train_items 116704.
I0302 18:59:34.059534 22895071117440 run.py:483] Algo bellman_ford step 3647 current loss 0.770274, current_train_items 116736.
I0302 18:59:34.087049 22895071117440 run.py:483] Algo bellman_ford step 3648 current loss 0.731958, current_train_items 116768.
I0302 18:59:34.118126 22895071117440 run.py:483] Algo bellman_ford step 3649 current loss 0.905974, current_train_items 116800.
I0302 18:59:34.135703 22895071117440 run.py:483] Algo bellman_ford step 3650 current loss 0.363275, current_train_items 116832.
I0302 18:59:34.143619 22895071117440 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 18:59:34.143726 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 18:59:34.160004 22895071117440 run.py:483] Algo bellman_ford step 3651 current loss 0.565086, current_train_items 116864.
I0302 18:59:34.181346 22895071117440 run.py:483] Algo bellman_ford step 3652 current loss 0.644018, current_train_items 116896.
I0302 18:59:34.209366 22895071117440 run.py:483] Algo bellman_ford step 3653 current loss 0.634288, current_train_items 116928.
I0302 18:59:34.240065 22895071117440 run.py:483] Algo bellman_ford step 3654 current loss 0.926928, current_train_items 116960.
I0302 18:59:34.258668 22895071117440 run.py:483] Algo bellman_ford step 3655 current loss 0.304093, current_train_items 116992.
I0302 18:59:34.273625 22895071117440 run.py:483] Algo bellman_ford step 3656 current loss 0.469827, current_train_items 117024.
I0302 18:59:34.295512 22895071117440 run.py:483] Algo bellman_ford step 3657 current loss 0.644179, current_train_items 117056.
I0302 18:59:34.323712 22895071117440 run.py:483] Algo bellman_ford step 3658 current loss 0.628936, current_train_items 117088.
I0302 18:59:34.354878 22895071117440 run.py:483] Algo bellman_ford step 3659 current loss 0.859811, current_train_items 117120.
I0302 18:59:34.373580 22895071117440 run.py:483] Algo bellman_ford step 3660 current loss 0.267770, current_train_items 117152.
I0302 18:59:34.389549 22895071117440 run.py:483] Algo bellman_ford step 3661 current loss 0.489718, current_train_items 117184.
I0302 18:59:34.410586 22895071117440 run.py:483] Algo bellman_ford step 3662 current loss 0.568566, current_train_items 117216.
I0302 18:59:34.440138 22895071117440 run.py:483] Algo bellman_ford step 3663 current loss 0.773390, current_train_items 117248.
I0302 18:59:34.472360 22895071117440 run.py:483] Algo bellman_ford step 3664 current loss 0.797990, current_train_items 117280.
I0302 18:59:34.490366 22895071117440 run.py:483] Algo bellman_ford step 3665 current loss 0.336721, current_train_items 117312.
I0302 18:59:34.505435 22895071117440 run.py:483] Algo bellman_ford step 3666 current loss 0.521131, current_train_items 117344.
I0302 18:59:34.527627 22895071117440 run.py:483] Algo bellman_ford step 3667 current loss 0.680154, current_train_items 117376.
I0302 18:59:34.556336 22895071117440 run.py:483] Algo bellman_ford step 3668 current loss 0.755611, current_train_items 117408.
I0302 18:59:34.588954 22895071117440 run.py:483] Algo bellman_ford step 3669 current loss 0.878553, current_train_items 117440.
I0302 18:59:34.607313 22895071117440 run.py:483] Algo bellman_ford step 3670 current loss 0.370073, current_train_items 117472.
I0302 18:59:34.623271 22895071117440 run.py:483] Algo bellman_ford step 3671 current loss 0.613864, current_train_items 117504.
I0302 18:59:34.644421 22895071117440 run.py:483] Algo bellman_ford step 3672 current loss 0.696348, current_train_items 117536.
I0302 18:59:34.672816 22895071117440 run.py:483] Algo bellman_ford step 3673 current loss 0.893853, current_train_items 117568.
I0302 18:59:34.704612 22895071117440 run.py:483] Algo bellman_ford step 3674 current loss 1.168818, current_train_items 117600.
I0302 18:59:34.722975 22895071117440 run.py:483] Algo bellman_ford step 3675 current loss 0.248537, current_train_items 117632.
I0302 18:59:34.738247 22895071117440 run.py:483] Algo bellman_ford step 3676 current loss 0.526667, current_train_items 117664.
I0302 18:59:34.759530 22895071117440 run.py:483] Algo bellman_ford step 3677 current loss 0.669739, current_train_items 117696.
I0302 18:59:34.788243 22895071117440 run.py:483] Algo bellman_ford step 3678 current loss 0.703108, current_train_items 117728.
I0302 18:59:34.819241 22895071117440 run.py:483] Algo bellman_ford step 3679 current loss 1.018011, current_train_items 117760.
I0302 18:59:34.837061 22895071117440 run.py:483] Algo bellman_ford step 3680 current loss 0.286674, current_train_items 117792.
I0302 18:59:34.852632 22895071117440 run.py:483] Algo bellman_ford step 3681 current loss 0.589463, current_train_items 117824.
I0302 18:59:34.874541 22895071117440 run.py:483] Algo bellman_ford step 3682 current loss 0.740569, current_train_items 117856.
I0302 18:59:34.902573 22895071117440 run.py:483] Algo bellman_ford step 3683 current loss 0.859695, current_train_items 117888.
I0302 18:59:34.933557 22895071117440 run.py:483] Algo bellman_ford step 3684 current loss 0.856393, current_train_items 117920.
I0302 18:59:34.951853 22895071117440 run.py:483] Algo bellman_ford step 3685 current loss 0.254038, current_train_items 117952.
I0302 18:59:34.967442 22895071117440 run.py:483] Algo bellman_ford step 3686 current loss 0.560145, current_train_items 117984.
I0302 18:59:34.988957 22895071117440 run.py:483] Algo bellman_ford step 3687 current loss 0.616472, current_train_items 118016.
I0302 18:59:35.018858 22895071117440 run.py:483] Algo bellman_ford step 3688 current loss 0.879358, current_train_items 118048.
I0302 18:59:35.051712 22895071117440 run.py:483] Algo bellman_ford step 3689 current loss 1.031282, current_train_items 118080.
I0302 18:59:35.070158 22895071117440 run.py:483] Algo bellman_ford step 3690 current loss 0.307193, current_train_items 118112.
I0302 18:59:35.085882 22895071117440 run.py:483] Algo bellman_ford step 3691 current loss 0.523482, current_train_items 118144.
I0302 18:59:35.105888 22895071117440 run.py:483] Algo bellman_ford step 3692 current loss 0.707811, current_train_items 118176.
I0302 18:59:35.136006 22895071117440 run.py:483] Algo bellman_ford step 3693 current loss 0.798034, current_train_items 118208.
I0302 18:59:35.166282 22895071117440 run.py:483] Algo bellman_ford step 3694 current loss 0.813832, current_train_items 118240.
I0302 18:59:35.184817 22895071117440 run.py:483] Algo bellman_ford step 3695 current loss 0.334891, current_train_items 118272.
I0302 18:59:35.200802 22895071117440 run.py:483] Algo bellman_ford step 3696 current loss 0.593633, current_train_items 118304.
I0302 18:59:35.222801 22895071117440 run.py:483] Algo bellman_ford step 3697 current loss 0.712097, current_train_items 118336.
I0302 18:59:35.251855 22895071117440 run.py:483] Algo bellman_ford step 3698 current loss 0.759969, current_train_items 118368.
I0302 18:59:35.281118 22895071117440 run.py:483] Algo bellman_ford step 3699 current loss 0.861834, current_train_items 118400.
I0302 18:59:35.299595 22895071117440 run.py:483] Algo bellman_ford step 3700 current loss 0.358598, current_train_items 118432.
I0302 18:59:35.307369 22895071117440 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 18:59:35.307475 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:59:35.323350 22895071117440 run.py:483] Algo bellman_ford step 3701 current loss 0.435682, current_train_items 118464.
I0302 18:59:35.345765 22895071117440 run.py:483] Algo bellman_ford step 3702 current loss 0.724569, current_train_items 118496.
I0302 18:59:35.376436 22895071117440 run.py:483] Algo bellman_ford step 3703 current loss 0.860983, current_train_items 118528.
I0302 18:59:35.407333 22895071117440 run.py:483] Algo bellman_ford step 3704 current loss 0.797335, current_train_items 118560.
I0302 18:59:35.425848 22895071117440 run.py:483] Algo bellman_ford step 3705 current loss 0.286859, current_train_items 118592.
I0302 18:59:35.441438 22895071117440 run.py:483] Algo bellman_ford step 3706 current loss 0.578765, current_train_items 118624.
I0302 18:59:35.463341 22895071117440 run.py:483] Algo bellman_ford step 3707 current loss 0.771942, current_train_items 118656.
I0302 18:59:35.490812 22895071117440 run.py:483] Algo bellman_ford step 3708 current loss 0.743156, current_train_items 118688.
I0302 18:59:35.521991 22895071117440 run.py:483] Algo bellman_ford step 3709 current loss 1.017701, current_train_items 118720.
I0302 18:59:35.540443 22895071117440 run.py:483] Algo bellman_ford step 3710 current loss 0.374818, current_train_items 118752.
I0302 18:59:35.556034 22895071117440 run.py:483] Algo bellman_ford step 3711 current loss 0.548423, current_train_items 118784.
I0302 18:59:35.577707 22895071117440 run.py:483] Algo bellman_ford step 3712 current loss 0.741758, current_train_items 118816.
I0302 18:59:35.607381 22895071117440 run.py:483] Algo bellman_ford step 3713 current loss 0.797199, current_train_items 118848.
I0302 18:59:35.639286 22895071117440 run.py:483] Algo bellman_ford step 3714 current loss 0.877075, current_train_items 118880.
I0302 18:59:35.657235 22895071117440 run.py:483] Algo bellman_ford step 3715 current loss 0.223393, current_train_items 118912.
I0302 18:59:35.672594 22895071117440 run.py:483] Algo bellman_ford step 3716 current loss 0.487304, current_train_items 118944.
I0302 18:59:35.694438 22895071117440 run.py:483] Algo bellman_ford step 3717 current loss 0.722333, current_train_items 118976.
I0302 18:59:35.723154 22895071117440 run.py:483] Algo bellman_ford step 3718 current loss 0.780739, current_train_items 119008.
I0302 18:59:35.752661 22895071117440 run.py:483] Algo bellman_ford step 3719 current loss 0.908636, current_train_items 119040.
I0302 18:59:35.770920 22895071117440 run.py:483] Algo bellman_ford step 3720 current loss 0.329856, current_train_items 119072.
I0302 18:59:35.786459 22895071117440 run.py:483] Algo bellman_ford step 3721 current loss 0.544774, current_train_items 119104.
I0302 18:59:35.808000 22895071117440 run.py:483] Algo bellman_ford step 3722 current loss 0.596987, current_train_items 119136.
I0302 18:59:35.835759 22895071117440 run.py:483] Algo bellman_ford step 3723 current loss 0.742872, current_train_items 119168.
I0302 18:59:35.866072 22895071117440 run.py:483] Algo bellman_ford step 3724 current loss 0.862705, current_train_items 119200.
I0302 18:59:35.884006 22895071117440 run.py:483] Algo bellman_ford step 3725 current loss 0.405276, current_train_items 119232.
I0302 18:59:35.899779 22895071117440 run.py:483] Algo bellman_ford step 3726 current loss 0.456442, current_train_items 119264.
I0302 18:59:35.922224 22895071117440 run.py:483] Algo bellman_ford step 3727 current loss 0.663457, current_train_items 119296.
I0302 18:59:35.951018 22895071117440 run.py:483] Algo bellman_ford step 3728 current loss 0.987067, current_train_items 119328.
I0302 18:59:35.981956 22895071117440 run.py:483] Algo bellman_ford step 3729 current loss 0.892077, current_train_items 119360.
I0302 18:59:36.000104 22895071117440 run.py:483] Algo bellman_ford step 3730 current loss 0.313434, current_train_items 119392.
I0302 18:59:36.015317 22895071117440 run.py:483] Algo bellman_ford step 3731 current loss 0.484436, current_train_items 119424.
I0302 18:59:36.037533 22895071117440 run.py:483] Algo bellman_ford step 3732 current loss 0.691638, current_train_items 119456.
I0302 18:59:36.064319 22895071117440 run.py:483] Algo bellman_ford step 3733 current loss 0.740090, current_train_items 119488.
I0302 18:59:36.095655 22895071117440 run.py:483] Algo bellman_ford step 3734 current loss 0.968983, current_train_items 119520.
I0302 18:59:36.113657 22895071117440 run.py:483] Algo bellman_ford step 3735 current loss 0.320852, current_train_items 119552.
I0302 18:59:36.128881 22895071117440 run.py:483] Algo bellman_ford step 3736 current loss 0.491903, current_train_items 119584.
I0302 18:59:36.149922 22895071117440 run.py:483] Algo bellman_ford step 3737 current loss 0.580035, current_train_items 119616.
I0302 18:59:36.178437 22895071117440 run.py:483] Algo bellman_ford step 3738 current loss 0.875508, current_train_items 119648.
I0302 18:59:36.209169 22895071117440 run.py:483] Algo bellman_ford step 3739 current loss 0.901868, current_train_items 119680.
I0302 18:59:36.227842 22895071117440 run.py:483] Algo bellman_ford step 3740 current loss 0.345539, current_train_items 119712.
I0302 18:59:36.243023 22895071117440 run.py:483] Algo bellman_ford step 3741 current loss 0.500500, current_train_items 119744.
I0302 18:59:36.264332 22895071117440 run.py:483] Algo bellman_ford step 3742 current loss 0.614163, current_train_items 119776.
I0302 18:59:36.292776 22895071117440 run.py:483] Algo bellman_ford step 3743 current loss 0.786067, current_train_items 119808.
I0302 18:59:36.321551 22895071117440 run.py:483] Algo bellman_ford step 3744 current loss 0.840168, current_train_items 119840.
I0302 18:59:36.339400 22895071117440 run.py:483] Algo bellman_ford step 3745 current loss 0.317631, current_train_items 119872.
I0302 18:59:36.354395 22895071117440 run.py:483] Algo bellman_ford step 3746 current loss 0.450238, current_train_items 119904.
I0302 18:59:36.376607 22895071117440 run.py:483] Algo bellman_ford step 3747 current loss 0.594950, current_train_items 119936.
I0302 18:59:36.405703 22895071117440 run.py:483] Algo bellman_ford step 3748 current loss 0.733324, current_train_items 119968.
I0302 18:59:36.438242 22895071117440 run.py:483] Algo bellman_ford step 3749 current loss 0.937718, current_train_items 120000.
I0302 18:59:36.456414 22895071117440 run.py:483] Algo bellman_ford step 3750 current loss 0.300404, current_train_items 120032.
I0302 18:59:36.464294 22895071117440 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 18:59:36.464420 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 18:59:36.480663 22895071117440 run.py:483] Algo bellman_ford step 3751 current loss 0.624344, current_train_items 120064.
I0302 18:59:36.502788 22895071117440 run.py:483] Algo bellman_ford step 3752 current loss 0.701378, current_train_items 120096.
I0302 18:59:36.531251 22895071117440 run.py:483] Algo bellman_ford step 3753 current loss 0.739857, current_train_items 120128.
I0302 18:59:36.564748 22895071117440 run.py:483] Algo bellman_ford step 3754 current loss 0.909129, current_train_items 120160.
I0302 18:59:36.582914 22895071117440 run.py:483] Algo bellman_ford step 3755 current loss 0.329702, current_train_items 120192.
I0302 18:59:36.598211 22895071117440 run.py:483] Algo bellman_ford step 3756 current loss 0.517895, current_train_items 120224.
I0302 18:59:36.620224 22895071117440 run.py:483] Algo bellman_ford step 3757 current loss 0.690920, current_train_items 120256.
I0302 18:59:36.648425 22895071117440 run.py:483] Algo bellman_ford step 3758 current loss 0.785445, current_train_items 120288.
I0302 18:59:36.677309 22895071117440 run.py:483] Algo bellman_ford step 3759 current loss 0.780838, current_train_items 120320.
I0302 18:59:36.695452 22895071117440 run.py:483] Algo bellman_ford step 3760 current loss 0.293116, current_train_items 120352.
I0302 18:59:36.711096 22895071117440 run.py:483] Algo bellman_ford step 3761 current loss 0.558020, current_train_items 120384.
I0302 18:59:36.731839 22895071117440 run.py:483] Algo bellman_ford step 3762 current loss 0.613836, current_train_items 120416.
I0302 18:59:36.759564 22895071117440 run.py:483] Algo bellman_ford step 3763 current loss 0.746564, current_train_items 120448.
I0302 18:59:36.790363 22895071117440 run.py:483] Algo bellman_ford step 3764 current loss 0.960445, current_train_items 120480.
I0302 18:59:36.808128 22895071117440 run.py:483] Algo bellman_ford step 3765 current loss 0.391643, current_train_items 120512.
I0302 18:59:36.823803 22895071117440 run.py:483] Algo bellman_ford step 3766 current loss 0.430494, current_train_items 120544.
I0302 18:59:36.845779 22895071117440 run.py:483] Algo bellman_ford step 3767 current loss 0.801426, current_train_items 120576.
I0302 18:59:36.874640 22895071117440 run.py:483] Algo bellman_ford step 3768 current loss 0.830506, current_train_items 120608.
I0302 18:59:36.904283 22895071117440 run.py:483] Algo bellman_ford step 3769 current loss 0.784402, current_train_items 120640.
I0302 18:59:36.922527 22895071117440 run.py:483] Algo bellman_ford step 3770 current loss 0.335171, current_train_items 120672.
I0302 18:59:36.938283 22895071117440 run.py:483] Algo bellman_ford step 3771 current loss 0.569561, current_train_items 120704.
I0302 18:59:36.958972 22895071117440 run.py:483] Algo bellman_ford step 3772 current loss 0.688251, current_train_items 120736.
I0302 18:59:36.986426 22895071117440 run.py:483] Algo bellman_ford step 3773 current loss 0.753370, current_train_items 120768.
I0302 18:59:37.014070 22895071117440 run.py:483] Algo bellman_ford step 3774 current loss 0.715286, current_train_items 120800.
I0302 18:59:37.032284 22895071117440 run.py:483] Algo bellman_ford step 3775 current loss 0.324986, current_train_items 120832.
I0302 18:59:37.048214 22895071117440 run.py:483] Algo bellman_ford step 3776 current loss 0.590178, current_train_items 120864.
I0302 18:59:37.069102 22895071117440 run.py:483] Algo bellman_ford step 3777 current loss 0.721297, current_train_items 120896.
I0302 18:59:37.097950 22895071117440 run.py:483] Algo bellman_ford step 3778 current loss 0.907697, current_train_items 120928.
I0302 18:59:37.128840 22895071117440 run.py:483] Algo bellman_ford step 3779 current loss 0.914622, current_train_items 120960.
I0302 18:59:37.146629 22895071117440 run.py:483] Algo bellman_ford step 3780 current loss 0.290745, current_train_items 120992.
I0302 18:59:37.162145 22895071117440 run.py:483] Algo bellman_ford step 3781 current loss 0.555271, current_train_items 121024.
I0302 18:59:37.183553 22895071117440 run.py:483] Algo bellman_ford step 3782 current loss 0.656929, current_train_items 121056.
I0302 18:59:37.210041 22895071117440 run.py:483] Algo bellman_ford step 3783 current loss 0.764817, current_train_items 121088.
I0302 18:59:37.243393 22895071117440 run.py:483] Algo bellman_ford step 3784 current loss 1.124550, current_train_items 121120.
I0302 18:59:37.261426 22895071117440 run.py:483] Algo bellman_ford step 3785 current loss 0.260014, current_train_items 121152.
I0302 18:59:37.276893 22895071117440 run.py:483] Algo bellman_ford step 3786 current loss 0.404257, current_train_items 121184.
I0302 18:59:37.298832 22895071117440 run.py:483] Algo bellman_ford step 3787 current loss 0.763523, current_train_items 121216.
I0302 18:59:37.326803 22895071117440 run.py:483] Algo bellman_ford step 3788 current loss 0.764816, current_train_items 121248.
I0302 18:59:37.357373 22895071117440 run.py:483] Algo bellman_ford step 3789 current loss 0.795343, current_train_items 121280.
I0302 18:59:37.375679 22895071117440 run.py:483] Algo bellman_ford step 3790 current loss 0.295441, current_train_items 121312.
I0302 18:59:37.391369 22895071117440 run.py:483] Algo bellman_ford step 3791 current loss 0.498726, current_train_items 121344.
I0302 18:59:37.412268 22895071117440 run.py:483] Algo bellman_ford step 3792 current loss 0.621459, current_train_items 121376.
I0302 18:59:37.441549 22895071117440 run.py:483] Algo bellman_ford step 3793 current loss 0.832919, current_train_items 121408.
I0302 18:59:37.471631 22895071117440 run.py:483] Algo bellman_ford step 3794 current loss 0.855932, current_train_items 121440.
I0302 18:59:37.489534 22895071117440 run.py:483] Algo bellman_ford step 3795 current loss 0.354637, current_train_items 121472.
I0302 18:59:37.505128 22895071117440 run.py:483] Algo bellman_ford step 3796 current loss 0.585387, current_train_items 121504.
I0302 18:59:37.527017 22895071117440 run.py:483] Algo bellman_ford step 3797 current loss 0.679201, current_train_items 121536.
I0302 18:59:37.556070 22895071117440 run.py:483] Algo bellman_ford step 3798 current loss 0.812500, current_train_items 121568.
I0302 18:59:37.585633 22895071117440 run.py:483] Algo bellman_ford step 3799 current loss 0.698729, current_train_items 121600.
I0302 18:59:37.603854 22895071117440 run.py:483] Algo bellman_ford step 3800 current loss 0.357204, current_train_items 121632.
I0302 18:59:37.611662 22895071117440 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 18:59:37.611768 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 18:59:37.627769 22895071117440 run.py:483] Algo bellman_ford step 3801 current loss 0.515428, current_train_items 121664.
I0302 18:59:37.650583 22895071117440 run.py:483] Algo bellman_ford step 3802 current loss 0.611264, current_train_items 121696.
I0302 18:59:37.679000 22895071117440 run.py:483] Algo bellman_ford step 3803 current loss 0.838364, current_train_items 121728.
I0302 18:59:37.708053 22895071117440 run.py:483] Algo bellman_ford step 3804 current loss 0.803271, current_train_items 121760.
I0302 18:59:37.726317 22895071117440 run.py:483] Algo bellman_ford step 3805 current loss 0.282585, current_train_items 121792.
I0302 18:59:37.741810 22895071117440 run.py:483] Algo bellman_ford step 3806 current loss 0.571895, current_train_items 121824.
I0302 18:59:37.763441 22895071117440 run.py:483] Algo bellman_ford step 3807 current loss 0.577924, current_train_items 121856.
I0302 18:59:37.791803 22895071117440 run.py:483] Algo bellman_ford step 3808 current loss 0.738504, current_train_items 121888.
I0302 18:59:37.823883 22895071117440 run.py:483] Algo bellman_ford step 3809 current loss 0.798714, current_train_items 121920.
I0302 18:59:37.841514 22895071117440 run.py:483] Algo bellman_ford step 3810 current loss 0.415938, current_train_items 121952.
I0302 18:59:37.857166 22895071117440 run.py:483] Algo bellman_ford step 3811 current loss 0.467375, current_train_items 121984.
I0302 18:59:37.878966 22895071117440 run.py:483] Algo bellman_ford step 3812 current loss 0.605703, current_train_items 122016.
I0302 18:59:37.909124 22895071117440 run.py:483] Algo bellman_ford step 3813 current loss 0.808707, current_train_items 122048.
I0302 18:59:37.941547 22895071117440 run.py:483] Algo bellman_ford step 3814 current loss 1.035491, current_train_items 122080.
I0302 18:59:37.959612 22895071117440 run.py:483] Algo bellman_ford step 3815 current loss 0.327024, current_train_items 122112.
I0302 18:59:37.975330 22895071117440 run.py:483] Algo bellman_ford step 3816 current loss 0.494346, current_train_items 122144.
I0302 18:59:37.997354 22895071117440 run.py:483] Algo bellman_ford step 3817 current loss 0.623227, current_train_items 122176.
I0302 18:59:38.025749 22895071117440 run.py:483] Algo bellman_ford step 3818 current loss 0.648405, current_train_items 122208.
I0302 18:59:38.056516 22895071117440 run.py:483] Algo bellman_ford step 3819 current loss 0.862341, current_train_items 122240.
I0302 18:59:38.074194 22895071117440 run.py:483] Algo bellman_ford step 3820 current loss 0.298326, current_train_items 122272.
I0302 18:59:38.089178 22895071117440 run.py:483] Algo bellman_ford step 3821 current loss 0.471811, current_train_items 122304.
I0302 18:59:38.110857 22895071117440 run.py:483] Algo bellman_ford step 3822 current loss 0.690320, current_train_items 122336.
I0302 18:59:38.139405 22895071117440 run.py:483] Algo bellman_ford step 3823 current loss 0.849205, current_train_items 122368.
I0302 18:59:38.170572 22895071117440 run.py:483] Algo bellman_ford step 3824 current loss 1.094837, current_train_items 122400.
I0302 18:59:38.188441 22895071117440 run.py:483] Algo bellman_ford step 3825 current loss 0.315553, current_train_items 122432.
I0302 18:59:38.203477 22895071117440 run.py:483] Algo bellman_ford step 3826 current loss 0.461990, current_train_items 122464.
I0302 18:59:38.225431 22895071117440 run.py:483] Algo bellman_ford step 3827 current loss 0.664844, current_train_items 122496.
I0302 18:59:38.254195 22895071117440 run.py:483] Algo bellman_ford step 3828 current loss 0.753170, current_train_items 122528.
I0302 18:59:38.283621 22895071117440 run.py:483] Algo bellman_ford step 3829 current loss 0.841182, current_train_items 122560.
I0302 18:59:38.301451 22895071117440 run.py:483] Algo bellman_ford step 3830 current loss 0.284529, current_train_items 122592.
I0302 18:59:38.316869 22895071117440 run.py:483] Algo bellman_ford step 3831 current loss 0.581879, current_train_items 122624.
I0302 18:59:38.337520 22895071117440 run.py:483] Algo bellman_ford step 3832 current loss 0.577452, current_train_items 122656.
I0302 18:59:38.366670 22895071117440 run.py:483] Algo bellman_ford step 3833 current loss 0.864013, current_train_items 122688.
I0302 18:59:38.396651 22895071117440 run.py:483] Algo bellman_ford step 3834 current loss 0.806088, current_train_items 122720.
I0302 18:59:38.414431 22895071117440 run.py:483] Algo bellman_ford step 3835 current loss 0.429827, current_train_items 122752.
I0302 18:59:38.429203 22895071117440 run.py:483] Algo bellman_ford step 3836 current loss 0.483051, current_train_items 122784.
I0302 18:59:38.451945 22895071117440 run.py:483] Algo bellman_ford step 3837 current loss 0.769332, current_train_items 122816.
I0302 18:59:38.480398 22895071117440 run.py:483] Algo bellman_ford step 3838 current loss 0.741016, current_train_items 122848.
I0302 18:59:38.510727 22895071117440 run.py:483] Algo bellman_ford step 3839 current loss 0.818987, current_train_items 122880.
I0302 18:59:38.528547 22895071117440 run.py:483] Algo bellman_ford step 3840 current loss 0.324537, current_train_items 122912.
I0302 18:59:38.543574 22895071117440 run.py:483] Algo bellman_ford step 3841 current loss 0.559793, current_train_items 122944.
I0302 18:59:38.564481 22895071117440 run.py:483] Algo bellman_ford step 3842 current loss 0.654620, current_train_items 122976.
I0302 18:59:38.593162 22895071117440 run.py:483] Algo bellman_ford step 3843 current loss 0.837024, current_train_items 123008.
I0302 18:59:38.625479 22895071117440 run.py:483] Algo bellman_ford step 3844 current loss 1.053049, current_train_items 123040.
I0302 18:59:38.643264 22895071117440 run.py:483] Algo bellman_ford step 3845 current loss 0.319749, current_train_items 123072.
I0302 18:59:38.658534 22895071117440 run.py:483] Algo bellman_ford step 3846 current loss 0.477758, current_train_items 123104.
I0302 18:59:38.681384 22895071117440 run.py:483] Algo bellman_ford step 3847 current loss 0.799100, current_train_items 123136.
I0302 18:59:38.708989 22895071117440 run.py:483] Algo bellman_ford step 3848 current loss 0.696877, current_train_items 123168.
I0302 18:59:38.739789 22895071117440 run.py:483] Algo bellman_ford step 3849 current loss 0.932805, current_train_items 123200.
I0302 18:59:38.757679 22895071117440 run.py:483] Algo bellman_ford step 3850 current loss 0.346427, current_train_items 123232.
I0302 18:59:38.765623 22895071117440 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 18:59:38.765729 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:38.781326 22895071117440 run.py:483] Algo bellman_ford step 3851 current loss 0.412508, current_train_items 123264.
I0302 18:59:38.802470 22895071117440 run.py:483] Algo bellman_ford step 3852 current loss 0.668512, current_train_items 123296.
I0302 18:59:38.832045 22895071117440 run.py:483] Algo bellman_ford step 3853 current loss 0.860493, current_train_items 123328.
I0302 18:59:38.865114 22895071117440 run.py:483] Algo bellman_ford step 3854 current loss 0.876312, current_train_items 123360.
I0302 18:59:38.883770 22895071117440 run.py:483] Algo bellman_ford step 3855 current loss 0.287441, current_train_items 123392.
I0302 18:59:38.898950 22895071117440 run.py:483] Algo bellman_ford step 3856 current loss 0.477753, current_train_items 123424.
I0302 18:59:38.919591 22895071117440 run.py:483] Algo bellman_ford step 3857 current loss 0.628017, current_train_items 123456.
I0302 18:59:38.948486 22895071117440 run.py:483] Algo bellman_ford step 3858 current loss 0.843169, current_train_items 123488.
I0302 18:59:38.981877 22895071117440 run.py:483] Algo bellman_ford step 3859 current loss 0.990765, current_train_items 123520.
I0302 18:59:39.000274 22895071117440 run.py:483] Algo bellman_ford step 3860 current loss 0.373848, current_train_items 123552.
I0302 18:59:39.016257 22895071117440 run.py:483] Algo bellman_ford step 3861 current loss 0.514933, current_train_items 123584.
I0302 18:59:39.037754 22895071117440 run.py:483] Algo bellman_ford step 3862 current loss 0.717919, current_train_items 123616.
I0302 18:59:39.066080 22895071117440 run.py:483] Algo bellman_ford step 3863 current loss 0.866159, current_train_items 123648.
I0302 18:59:39.099229 22895071117440 run.py:483] Algo bellman_ford step 3864 current loss 0.941733, current_train_items 123680.
I0302 18:59:39.117083 22895071117440 run.py:483] Algo bellman_ford step 3865 current loss 0.400454, current_train_items 123712.
I0302 18:59:39.132194 22895071117440 run.py:483] Algo bellman_ford step 3866 current loss 0.539566, current_train_items 123744.
I0302 18:59:39.153776 22895071117440 run.py:483] Algo bellman_ford step 3867 current loss 0.749828, current_train_items 123776.
I0302 18:59:39.181857 22895071117440 run.py:483] Algo bellman_ford step 3868 current loss 0.811791, current_train_items 123808.
I0302 18:59:39.212664 22895071117440 run.py:483] Algo bellman_ford step 3869 current loss 0.835950, current_train_items 123840.
I0302 18:59:39.231003 22895071117440 run.py:483] Algo bellman_ford step 3870 current loss 0.362578, current_train_items 123872.
I0302 18:59:39.246223 22895071117440 run.py:483] Algo bellman_ford step 3871 current loss 0.510598, current_train_items 123904.
I0302 18:59:39.268015 22895071117440 run.py:483] Algo bellman_ford step 3872 current loss 0.620649, current_train_items 123936.
I0302 18:59:39.295066 22895071117440 run.py:483] Algo bellman_ford step 3873 current loss 0.655040, current_train_items 123968.
I0302 18:59:39.325189 22895071117440 run.py:483] Algo bellman_ford step 3874 current loss 0.824837, current_train_items 124000.
I0302 18:59:39.343791 22895071117440 run.py:483] Algo bellman_ford step 3875 current loss 0.381636, current_train_items 124032.
I0302 18:59:39.359692 22895071117440 run.py:483] Algo bellman_ford step 3876 current loss 0.572372, current_train_items 124064.
I0302 18:59:39.381914 22895071117440 run.py:483] Algo bellman_ford step 3877 current loss 0.853176, current_train_items 124096.
I0302 18:59:39.410645 22895071117440 run.py:483] Algo bellman_ford step 3878 current loss 0.759453, current_train_items 124128.
I0302 18:59:39.441555 22895071117440 run.py:483] Algo bellman_ford step 3879 current loss 0.923975, current_train_items 124160.
I0302 18:59:39.459819 22895071117440 run.py:483] Algo bellman_ford step 3880 current loss 0.351131, current_train_items 124192.
I0302 18:59:39.475058 22895071117440 run.py:483] Algo bellman_ford step 3881 current loss 0.482564, current_train_items 124224.
I0302 18:59:39.497024 22895071117440 run.py:483] Algo bellman_ford step 3882 current loss 0.766012, current_train_items 124256.
I0302 18:59:39.525027 22895071117440 run.py:483] Algo bellman_ford step 3883 current loss 0.743906, current_train_items 124288.
I0302 18:59:39.554718 22895071117440 run.py:483] Algo bellman_ford step 3884 current loss 0.868658, current_train_items 124320.
I0302 18:59:39.573371 22895071117440 run.py:483] Algo bellman_ford step 3885 current loss 0.295089, current_train_items 124352.
I0302 18:59:39.589382 22895071117440 run.py:483] Algo bellman_ford step 3886 current loss 0.497536, current_train_items 124384.
I0302 18:59:39.610856 22895071117440 run.py:483] Algo bellman_ford step 3887 current loss 0.814044, current_train_items 124416.
I0302 18:59:39.639086 22895071117440 run.py:483] Algo bellman_ford step 3888 current loss 0.736188, current_train_items 124448.
I0302 18:59:39.670735 22895071117440 run.py:483] Algo bellman_ford step 3889 current loss 0.875957, current_train_items 124480.
I0302 18:59:39.689087 22895071117440 run.py:483] Algo bellman_ford step 3890 current loss 0.278289, current_train_items 124512.
I0302 18:59:39.704849 22895071117440 run.py:483] Algo bellman_ford step 3891 current loss 0.694313, current_train_items 124544.
I0302 18:59:39.727058 22895071117440 run.py:483] Algo bellman_ford step 3892 current loss 0.759725, current_train_items 124576.
I0302 18:59:39.753658 22895071117440 run.py:483] Algo bellman_ford step 3893 current loss 0.675053, current_train_items 124608.
I0302 18:59:39.784677 22895071117440 run.py:483] Algo bellman_ford step 3894 current loss 0.859208, current_train_items 124640.
I0302 18:59:39.802570 22895071117440 run.py:483] Algo bellman_ford step 3895 current loss 0.242829, current_train_items 124672.
I0302 18:59:39.818111 22895071117440 run.py:483] Algo bellman_ford step 3896 current loss 0.522302, current_train_items 124704.
I0302 18:59:39.841088 22895071117440 run.py:483] Algo bellman_ford step 3897 current loss 0.715884, current_train_items 124736.
I0302 18:59:39.869928 22895071117440 run.py:483] Algo bellman_ford step 3898 current loss 0.676133, current_train_items 124768.
I0302 18:59:39.900294 22895071117440 run.py:483] Algo bellman_ford step 3899 current loss 0.871485, current_train_items 124800.
I0302 18:59:39.918979 22895071117440 run.py:483] Algo bellman_ford step 3900 current loss 0.315037, current_train_items 124832.
I0302 18:59:39.926598 22895071117440 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 18:59:39.926704 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 18:59:39.942913 22895071117440 run.py:483] Algo bellman_ford step 3901 current loss 0.574908, current_train_items 124864.
I0302 18:59:39.964886 22895071117440 run.py:483] Algo bellman_ford step 3902 current loss 0.727243, current_train_items 124896.
I0302 18:59:39.993947 22895071117440 run.py:483] Algo bellman_ford step 3903 current loss 0.800176, current_train_items 124928.
I0302 18:59:40.025255 22895071117440 run.py:483] Algo bellman_ford step 3904 current loss 0.805191, current_train_items 124960.
I0302 18:59:40.043992 22895071117440 run.py:483] Algo bellman_ford step 3905 current loss 0.248271, current_train_items 124992.
I0302 18:59:40.059245 22895071117440 run.py:483] Algo bellman_ford step 3906 current loss 0.476723, current_train_items 125024.
I0302 18:59:40.080044 22895071117440 run.py:483] Algo bellman_ford step 3907 current loss 0.613652, current_train_items 125056.
I0302 18:59:40.108483 22895071117440 run.py:483] Algo bellman_ford step 3908 current loss 0.795352, current_train_items 125088.
I0302 18:59:40.139980 22895071117440 run.py:483] Algo bellman_ford step 3909 current loss 0.855923, current_train_items 125120.
I0302 18:59:40.157770 22895071117440 run.py:483] Algo bellman_ford step 3910 current loss 0.282351, current_train_items 125152.
I0302 18:59:40.172885 22895071117440 run.py:483] Algo bellman_ford step 3911 current loss 0.470262, current_train_items 125184.
I0302 18:59:40.195016 22895071117440 run.py:483] Algo bellman_ford step 3912 current loss 0.651291, current_train_items 125216.
I0302 18:59:40.222777 22895071117440 run.py:483] Algo bellman_ford step 3913 current loss 0.721920, current_train_items 125248.
I0302 18:59:40.250576 22895071117440 run.py:483] Algo bellman_ford step 3914 current loss 0.806258, current_train_items 125280.
I0302 18:59:40.268144 22895071117440 run.py:483] Algo bellman_ford step 3915 current loss 0.426604, current_train_items 125312.
I0302 18:59:40.283249 22895071117440 run.py:483] Algo bellman_ford step 3916 current loss 0.459271, current_train_items 125344.
I0302 18:59:40.306259 22895071117440 run.py:483] Algo bellman_ford step 3917 current loss 0.730496, current_train_items 125376.
I0302 18:59:40.333473 22895071117440 run.py:483] Algo bellman_ford step 3918 current loss 0.761182, current_train_items 125408.
I0302 18:59:40.363956 22895071117440 run.py:483] Algo bellman_ford step 3919 current loss 0.734610, current_train_items 125440.
I0302 18:59:40.381700 22895071117440 run.py:483] Algo bellman_ford step 3920 current loss 0.306042, current_train_items 125472.
I0302 18:59:40.397216 22895071117440 run.py:483] Algo bellman_ford step 3921 current loss 0.578437, current_train_items 125504.
I0302 18:59:40.419903 22895071117440 run.py:483] Algo bellman_ford step 3922 current loss 0.659533, current_train_items 125536.
I0302 18:59:40.448910 22895071117440 run.py:483] Algo bellman_ford step 3923 current loss 0.871772, current_train_items 125568.
I0302 18:59:40.481947 22895071117440 run.py:483] Algo bellman_ford step 3924 current loss 0.862275, current_train_items 125600.
I0302 18:59:40.499979 22895071117440 run.py:483] Algo bellman_ford step 3925 current loss 0.339234, current_train_items 125632.
I0302 18:59:40.515280 22895071117440 run.py:483] Algo bellman_ford step 3926 current loss 0.480374, current_train_items 125664.
I0302 18:59:40.536980 22895071117440 run.py:483] Algo bellman_ford step 3927 current loss 0.615230, current_train_items 125696.
I0302 18:59:40.564824 22895071117440 run.py:483] Algo bellman_ford step 3928 current loss 0.787175, current_train_items 125728.
I0302 18:59:40.595352 22895071117440 run.py:483] Algo bellman_ford step 3929 current loss 0.803834, current_train_items 125760.
I0302 18:59:40.612981 22895071117440 run.py:483] Algo bellman_ford step 3930 current loss 0.291310, current_train_items 125792.
I0302 18:59:40.628108 22895071117440 run.py:483] Algo bellman_ford step 3931 current loss 0.582562, current_train_items 125824.
I0302 18:59:40.649712 22895071117440 run.py:483] Algo bellman_ford step 3932 current loss 0.632704, current_train_items 125856.
I0302 18:59:40.679573 22895071117440 run.py:483] Algo bellman_ford step 3933 current loss 0.750161, current_train_items 125888.
I0302 18:59:40.710567 22895071117440 run.py:483] Algo bellman_ford step 3934 current loss 0.861987, current_train_items 125920.
I0302 18:59:40.728390 22895071117440 run.py:483] Algo bellman_ford step 3935 current loss 0.313846, current_train_items 125952.
I0302 18:59:40.743988 22895071117440 run.py:483] Algo bellman_ford step 3936 current loss 0.525431, current_train_items 125984.
I0302 18:59:40.765477 22895071117440 run.py:483] Algo bellman_ford step 3937 current loss 0.637027, current_train_items 126016.
I0302 18:59:40.794124 22895071117440 run.py:483] Algo bellman_ford step 3938 current loss 0.833581, current_train_items 126048.
I0302 18:59:40.825759 22895071117440 run.py:483] Algo bellman_ford step 3939 current loss 0.799704, current_train_items 126080.
I0302 18:59:40.843491 22895071117440 run.py:483] Algo bellman_ford step 3940 current loss 0.368170, current_train_items 126112.
I0302 18:59:40.858760 22895071117440 run.py:483] Algo bellman_ford step 3941 current loss 0.461605, current_train_items 126144.
I0302 18:59:40.880580 22895071117440 run.py:483] Algo bellman_ford step 3942 current loss 0.616285, current_train_items 126176.
I0302 18:59:40.907749 22895071117440 run.py:483] Algo bellman_ford step 3943 current loss 0.659129, current_train_items 126208.
I0302 18:59:40.939386 22895071117440 run.py:483] Algo bellman_ford step 3944 current loss 0.812701, current_train_items 126240.
I0302 18:59:40.957437 22895071117440 run.py:483] Algo bellman_ford step 3945 current loss 0.315026, current_train_items 126272.
I0302 18:59:40.972987 22895071117440 run.py:483] Algo bellman_ford step 3946 current loss 0.470354, current_train_items 126304.
I0302 18:59:40.994526 22895071117440 run.py:483] Algo bellman_ford step 3947 current loss 0.618131, current_train_items 126336.
I0302 18:59:41.021160 22895071117440 run.py:483] Algo bellman_ford step 3948 current loss 0.803061, current_train_items 126368.
I0302 18:59:41.052100 22895071117440 run.py:483] Algo bellman_ford step 3949 current loss 0.785062, current_train_items 126400.
I0302 18:59:41.069883 22895071117440 run.py:483] Algo bellman_ford step 3950 current loss 0.245215, current_train_items 126432.
I0302 18:59:41.077599 22895071117440 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 18:59:41.077703 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 18:59:41.094107 22895071117440 run.py:483] Algo bellman_ford step 3951 current loss 0.463765, current_train_items 126464.
I0302 18:59:41.116791 22895071117440 run.py:483] Algo bellman_ford step 3952 current loss 0.644245, current_train_items 126496.
I0302 18:59:41.145056 22895071117440 run.py:483] Algo bellman_ford step 3953 current loss 0.652903, current_train_items 126528.
I0302 18:59:41.175011 22895071117440 run.py:483] Algo bellman_ford step 3954 current loss 0.775262, current_train_items 126560.
I0302 18:59:41.193053 22895071117440 run.py:483] Algo bellman_ford step 3955 current loss 0.300910, current_train_items 126592.
I0302 18:59:41.208339 22895071117440 run.py:483] Algo bellman_ford step 3956 current loss 0.445439, current_train_items 126624.
I0302 18:59:41.230460 22895071117440 run.py:483] Algo bellman_ford step 3957 current loss 0.662676, current_train_items 126656.
I0302 18:59:41.260025 22895071117440 run.py:483] Algo bellman_ford step 3958 current loss 0.798544, current_train_items 126688.
I0302 18:59:41.293054 22895071117440 run.py:483] Algo bellman_ford step 3959 current loss 0.879056, current_train_items 126720.
I0302 18:59:41.311565 22895071117440 run.py:483] Algo bellman_ford step 3960 current loss 0.303373, current_train_items 126752.
I0302 18:59:41.327114 22895071117440 run.py:483] Algo bellman_ford step 3961 current loss 0.476223, current_train_items 126784.
I0302 18:59:41.347470 22895071117440 run.py:483] Algo bellman_ford step 3962 current loss 0.559094, current_train_items 126816.
I0302 18:59:41.376221 22895071117440 run.py:483] Algo bellman_ford step 3963 current loss 0.734320, current_train_items 126848.
I0302 18:59:41.407598 22895071117440 run.py:483] Algo bellman_ford step 3964 current loss 1.063154, current_train_items 126880.
I0302 18:59:41.425395 22895071117440 run.py:483] Algo bellman_ford step 3965 current loss 0.378857, current_train_items 126912.
I0302 18:59:41.440708 22895071117440 run.py:483] Algo bellman_ford step 3966 current loss 0.557014, current_train_items 126944.
I0302 18:59:41.463813 22895071117440 run.py:483] Algo bellman_ford step 3967 current loss 0.704148, current_train_items 126976.
I0302 18:59:41.492132 22895071117440 run.py:483] Algo bellman_ford step 3968 current loss 0.723568, current_train_items 127008.
I0302 18:59:41.522704 22895071117440 run.py:483] Algo bellman_ford step 3969 current loss 0.765351, current_train_items 127040.
I0302 18:59:41.541282 22895071117440 run.py:483] Algo bellman_ford step 3970 current loss 0.355966, current_train_items 127072.
I0302 18:59:41.556863 22895071117440 run.py:483] Algo bellman_ford step 3971 current loss 0.496534, current_train_items 127104.
I0302 18:59:41.579189 22895071117440 run.py:483] Algo bellman_ford step 3972 current loss 0.678905, current_train_items 127136.
I0302 18:59:41.608703 22895071117440 run.py:483] Algo bellman_ford step 3973 current loss 0.912850, current_train_items 127168.
I0302 18:59:41.637949 22895071117440 run.py:483] Algo bellman_ford step 3974 current loss 1.043876, current_train_items 127200.
I0302 18:59:41.656459 22895071117440 run.py:483] Algo bellman_ford step 3975 current loss 0.300991, current_train_items 127232.
I0302 18:59:41.672465 22895071117440 run.py:483] Algo bellman_ford step 3976 current loss 0.494712, current_train_items 127264.
I0302 18:59:41.693908 22895071117440 run.py:483] Algo bellman_ford step 3977 current loss 0.620574, current_train_items 127296.
I0302 18:59:41.723035 22895071117440 run.py:483] Algo bellman_ford step 3978 current loss 0.838225, current_train_items 127328.
I0302 18:59:41.753574 22895071117440 run.py:483] Algo bellman_ford step 3979 current loss 0.898724, current_train_items 127360.
I0302 18:59:41.771171 22895071117440 run.py:483] Algo bellman_ford step 3980 current loss 0.328521, current_train_items 127392.
I0302 18:59:41.786379 22895071117440 run.py:483] Algo bellman_ford step 3981 current loss 0.508460, current_train_items 127424.
I0302 18:59:41.807679 22895071117440 run.py:483] Algo bellman_ford step 3982 current loss 0.751152, current_train_items 127456.
I0302 18:59:41.836744 22895071117440 run.py:483] Algo bellman_ford step 3983 current loss 0.863035, current_train_items 127488.
I0302 18:59:41.866369 22895071117440 run.py:483] Algo bellman_ford step 3984 current loss 0.856839, current_train_items 127520.
I0302 18:59:41.884918 22895071117440 run.py:483] Algo bellman_ford step 3985 current loss 0.270738, current_train_items 127552.
I0302 18:59:41.900738 22895071117440 run.py:483] Algo bellman_ford step 3986 current loss 0.673199, current_train_items 127584.
I0302 18:59:41.921882 22895071117440 run.py:483] Algo bellman_ford step 3987 current loss 0.755039, current_train_items 127616.
I0302 18:59:41.950985 22895071117440 run.py:483] Algo bellman_ford step 3988 current loss 0.887354, current_train_items 127648.
I0302 18:59:41.982268 22895071117440 run.py:483] Algo bellman_ford step 3989 current loss 0.898071, current_train_items 127680.
I0302 18:59:42.001019 22895071117440 run.py:483] Algo bellman_ford step 3990 current loss 0.333667, current_train_items 127712.
I0302 18:59:42.016885 22895071117440 run.py:483] Algo bellman_ford step 3991 current loss 0.543047, current_train_items 127744.
I0302 18:59:42.037610 22895071117440 run.py:483] Algo bellman_ford step 3992 current loss 0.569658, current_train_items 127776.
I0302 18:59:42.065762 22895071117440 run.py:483] Algo bellman_ford step 3993 current loss 0.772771, current_train_items 127808.
I0302 18:59:42.096781 22895071117440 run.py:483] Algo bellman_ford step 3994 current loss 0.908578, current_train_items 127840.
I0302 18:59:42.114967 22895071117440 run.py:483] Algo bellman_ford step 3995 current loss 0.363649, current_train_items 127872.
I0302 18:59:42.130414 22895071117440 run.py:483] Algo bellman_ford step 3996 current loss 0.594489, current_train_items 127904.
I0302 18:59:42.152220 22895071117440 run.py:483] Algo bellman_ford step 3997 current loss 0.656719, current_train_items 127936.
I0302 18:59:42.180432 22895071117440 run.py:483] Algo bellman_ford step 3998 current loss 0.723339, current_train_items 127968.
I0302 18:59:42.212425 22895071117440 run.py:483] Algo bellman_ford step 3999 current loss 0.831034, current_train_items 128000.
I0302 18:59:42.230796 22895071117440 run.py:483] Algo bellman_ford step 4000 current loss 0.364850, current_train_items 128032.
I0302 18:59:42.238650 22895071117440 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 18:59:42.238756 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 18:59:42.254788 22895071117440 run.py:483] Algo bellman_ford step 4001 current loss 0.443583, current_train_items 128064.
I0302 18:59:42.276882 22895071117440 run.py:483] Algo bellman_ford step 4002 current loss 0.696498, current_train_items 128096.
I0302 18:59:42.305298 22895071117440 run.py:483] Algo bellman_ford step 4003 current loss 0.669605, current_train_items 128128.
I0302 18:59:42.335764 22895071117440 run.py:483] Algo bellman_ford step 4004 current loss 0.758069, current_train_items 128160.
I0302 18:59:42.354085 22895071117440 run.py:483] Algo bellman_ford step 4005 current loss 0.267430, current_train_items 128192.
I0302 18:59:42.369473 22895071117440 run.py:483] Algo bellman_ford step 4006 current loss 0.511137, current_train_items 128224.
I0302 18:59:42.391444 22895071117440 run.py:483] Algo bellman_ford step 4007 current loss 0.731465, current_train_items 128256.
I0302 18:59:42.420503 22895071117440 run.py:483] Algo bellman_ford step 4008 current loss 0.788732, current_train_items 128288.
I0302 18:59:42.449828 22895071117440 run.py:483] Algo bellman_ford step 4009 current loss 0.922145, current_train_items 128320.
I0302 18:59:42.467962 22895071117440 run.py:483] Algo bellman_ford step 4010 current loss 0.325513, current_train_items 128352.
I0302 18:59:42.483303 22895071117440 run.py:483] Algo bellman_ford step 4011 current loss 0.443693, current_train_items 128384.
I0302 18:59:42.504012 22895071117440 run.py:483] Algo bellman_ford step 4012 current loss 0.605542, current_train_items 128416.
I0302 18:59:42.532529 22895071117440 run.py:483] Algo bellman_ford step 4013 current loss 0.705965, current_train_items 128448.
I0302 18:59:42.563303 22895071117440 run.py:483] Algo bellman_ford step 4014 current loss 0.777576, current_train_items 128480.
I0302 18:59:42.581606 22895071117440 run.py:483] Algo bellman_ford step 4015 current loss 0.257533, current_train_items 128512.
I0302 18:59:42.596922 22895071117440 run.py:483] Algo bellman_ford step 4016 current loss 0.487848, current_train_items 128544.
I0302 18:59:42.618661 22895071117440 run.py:483] Algo bellman_ford step 4017 current loss 0.652914, current_train_items 128576.
I0302 18:59:42.647101 22895071117440 run.py:483] Algo bellman_ford step 4018 current loss 0.737830, current_train_items 128608.
I0302 18:59:42.676829 22895071117440 run.py:483] Algo bellman_ford step 4019 current loss 0.780653, current_train_items 128640.
I0302 18:59:42.694932 22895071117440 run.py:483] Algo bellman_ford step 4020 current loss 0.288004, current_train_items 128672.
I0302 18:59:42.710118 22895071117440 run.py:483] Algo bellman_ford step 4021 current loss 0.500486, current_train_items 128704.
I0302 18:59:42.732520 22895071117440 run.py:483] Algo bellman_ford step 4022 current loss 0.721732, current_train_items 128736.
I0302 18:59:42.760112 22895071117440 run.py:483] Algo bellman_ford step 4023 current loss 0.733867, current_train_items 128768.
I0302 18:59:42.793514 22895071117440 run.py:483] Algo bellman_ford step 4024 current loss 1.266829, current_train_items 128800.
I0302 18:59:42.811378 22895071117440 run.py:483] Algo bellman_ford step 4025 current loss 0.303070, current_train_items 128832.
I0302 18:59:42.826920 22895071117440 run.py:483] Algo bellman_ford step 4026 current loss 0.420304, current_train_items 128864.
I0302 18:59:42.848261 22895071117440 run.py:483] Algo bellman_ford step 4027 current loss 0.591433, current_train_items 128896.
I0302 18:59:42.875870 22895071117440 run.py:483] Algo bellman_ford step 4028 current loss 0.703415, current_train_items 128928.
I0302 18:59:42.905832 22895071117440 run.py:483] Algo bellman_ford step 4029 current loss 0.755976, current_train_items 128960.
I0302 18:59:42.923981 22895071117440 run.py:483] Algo bellman_ford step 4030 current loss 0.369516, current_train_items 128992.
I0302 18:59:42.939068 22895071117440 run.py:483] Algo bellman_ford step 4031 current loss 0.492102, current_train_items 129024.
I0302 18:59:42.961398 22895071117440 run.py:483] Algo bellman_ford step 4032 current loss 0.724954, current_train_items 129056.
I0302 18:59:42.990481 22895071117440 run.py:483] Algo bellman_ford step 4033 current loss 0.796986, current_train_items 129088.
I0302 18:59:43.020443 22895071117440 run.py:483] Algo bellman_ford step 4034 current loss 0.759508, current_train_items 129120.
I0302 18:59:43.038237 22895071117440 run.py:483] Algo bellman_ford step 4035 current loss 0.280610, current_train_items 129152.
I0302 18:59:43.053399 22895071117440 run.py:483] Algo bellman_ford step 4036 current loss 0.597475, current_train_items 129184.
I0302 18:59:43.074944 22895071117440 run.py:483] Algo bellman_ford step 4037 current loss 0.631842, current_train_items 129216.
I0302 18:59:43.104199 22895071117440 run.py:483] Algo bellman_ford step 4038 current loss 0.747580, current_train_items 129248.
I0302 18:59:43.135204 22895071117440 run.py:483] Algo bellman_ford step 4039 current loss 0.911052, current_train_items 129280.
I0302 18:59:43.152947 22895071117440 run.py:483] Algo bellman_ford step 4040 current loss 0.269844, current_train_items 129312.
I0302 18:59:43.168391 22895071117440 run.py:483] Algo bellman_ford step 4041 current loss 0.445624, current_train_items 129344.
I0302 18:59:43.189508 22895071117440 run.py:483] Algo bellman_ford step 4042 current loss 0.638836, current_train_items 129376.
I0302 18:59:43.218957 22895071117440 run.py:483] Algo bellman_ford step 4043 current loss 0.808141, current_train_items 129408.
I0302 18:59:43.248497 22895071117440 run.py:483] Algo bellman_ford step 4044 current loss 0.787014, current_train_items 129440.
I0302 18:59:43.266303 22895071117440 run.py:483] Algo bellman_ford step 4045 current loss 0.366107, current_train_items 129472.
I0302 18:59:43.281946 22895071117440 run.py:483] Algo bellman_ford step 4046 current loss 0.599277, current_train_items 129504.
I0302 18:59:43.303338 22895071117440 run.py:483] Algo bellman_ford step 4047 current loss 0.680746, current_train_items 129536.
I0302 18:59:43.330606 22895071117440 run.py:483] Algo bellman_ford step 4048 current loss 0.733150, current_train_items 129568.
I0302 18:59:43.362284 22895071117440 run.py:483] Algo bellman_ford step 4049 current loss 0.882599, current_train_items 129600.
I0302 18:59:43.380292 22895071117440 run.py:483] Algo bellman_ford step 4050 current loss 0.319076, current_train_items 129632.
I0302 18:59:43.387862 22895071117440 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 18:59:43.387976 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 18:59:43.404254 22895071117440 run.py:483] Algo bellman_ford step 4051 current loss 0.533694, current_train_items 129664.
I0302 18:59:43.425363 22895071117440 run.py:483] Algo bellman_ford step 4052 current loss 0.583812, current_train_items 129696.
I0302 18:59:43.454272 22895071117440 run.py:483] Algo bellman_ford step 4053 current loss 0.806368, current_train_items 129728.
I0302 18:59:43.487340 22895071117440 run.py:483] Algo bellman_ford step 4054 current loss 0.873128, current_train_items 129760.
I0302 18:59:43.505780 22895071117440 run.py:483] Algo bellman_ford step 4055 current loss 0.462444, current_train_items 129792.
I0302 18:59:43.520998 22895071117440 run.py:483] Algo bellman_ford step 4056 current loss 0.478779, current_train_items 129824.
I0302 18:59:43.542707 22895071117440 run.py:483] Algo bellman_ford step 4057 current loss 0.567531, current_train_items 129856.
I0302 18:59:43.570018 22895071117440 run.py:483] Algo bellman_ford step 4058 current loss 0.686581, current_train_items 129888.
I0302 18:59:43.599176 22895071117440 run.py:483] Algo bellman_ford step 4059 current loss 0.753162, current_train_items 129920.
I0302 18:59:43.617580 22895071117440 run.py:483] Algo bellman_ford step 4060 current loss 0.358164, current_train_items 129952.
I0302 18:59:43.633559 22895071117440 run.py:483] Algo bellman_ford step 4061 current loss 0.479491, current_train_items 129984.
I0302 18:59:43.654192 22895071117440 run.py:483] Algo bellman_ford step 4062 current loss 0.737782, current_train_items 130016.
I0302 18:59:43.681223 22895071117440 run.py:483] Algo bellman_ford step 4063 current loss 0.809710, current_train_items 130048.
I0302 18:59:43.713278 22895071117440 run.py:483] Algo bellman_ford step 4064 current loss 1.078316, current_train_items 130080.
I0302 18:59:43.731089 22895071117440 run.py:483] Algo bellman_ford step 4065 current loss 0.303030, current_train_items 130112.
I0302 18:59:43.745935 22895071117440 run.py:483] Algo bellman_ford step 4066 current loss 0.525717, current_train_items 130144.
I0302 18:59:43.769046 22895071117440 run.py:483] Algo bellman_ford step 4067 current loss 0.805943, current_train_items 130176.
I0302 18:59:43.797014 22895071117440 run.py:483] Algo bellman_ford step 4068 current loss 0.746264, current_train_items 130208.
I0302 18:59:43.828862 22895071117440 run.py:483] Algo bellman_ford step 4069 current loss 0.867756, current_train_items 130240.
I0302 18:59:43.847450 22895071117440 run.py:483] Algo bellman_ford step 4070 current loss 0.372307, current_train_items 130272.
I0302 18:59:43.863186 22895071117440 run.py:483] Algo bellman_ford step 4071 current loss 0.530880, current_train_items 130304.
I0302 18:59:43.884797 22895071117440 run.py:483] Algo bellman_ford step 4072 current loss 0.764591, current_train_items 130336.
I0302 18:59:43.911732 22895071117440 run.py:483] Algo bellman_ford step 4073 current loss 0.915369, current_train_items 130368.
I0302 18:59:43.941846 22895071117440 run.py:483] Algo bellman_ford step 4074 current loss 0.931167, current_train_items 130400.
I0302 18:59:43.960215 22895071117440 run.py:483] Algo bellman_ford step 4075 current loss 0.268410, current_train_items 130432.
I0302 18:59:43.976425 22895071117440 run.py:483] Algo bellman_ford step 4076 current loss 0.585823, current_train_items 130464.
I0302 18:59:43.998950 22895071117440 run.py:483] Algo bellman_ford step 4077 current loss 0.760030, current_train_items 130496.
I0302 18:59:44.026888 22895071117440 run.py:483] Algo bellman_ford step 4078 current loss 0.764223, current_train_items 130528.
I0302 18:59:44.056993 22895071117440 run.py:483] Algo bellman_ford step 4079 current loss 0.799356, current_train_items 130560.
I0302 18:59:44.074560 22895071117440 run.py:483] Algo bellman_ford step 4080 current loss 0.304774, current_train_items 130592.
I0302 18:59:44.090200 22895071117440 run.py:483] Algo bellman_ford step 4081 current loss 0.454762, current_train_items 130624.
I0302 18:59:44.112731 22895071117440 run.py:483] Algo bellman_ford step 4082 current loss 0.728943, current_train_items 130656.
I0302 18:59:44.142386 22895071117440 run.py:483] Algo bellman_ford step 4083 current loss 0.728110, current_train_items 130688.
I0302 18:59:44.170891 22895071117440 run.py:483] Algo bellman_ford step 4084 current loss 0.806426, current_train_items 130720.
I0302 18:59:44.189599 22895071117440 run.py:483] Algo bellman_ford step 4085 current loss 0.314969, current_train_items 130752.
I0302 18:59:44.205354 22895071117440 run.py:483] Algo bellman_ford step 4086 current loss 0.519782, current_train_items 130784.
I0302 18:59:44.226429 22895071117440 run.py:483] Algo bellman_ford step 4087 current loss 0.655622, current_train_items 130816.
I0302 18:59:44.254111 22895071117440 run.py:483] Algo bellman_ford step 4088 current loss 0.774603, current_train_items 130848.
I0302 18:59:44.283215 22895071117440 run.py:483] Algo bellman_ford step 4089 current loss 0.769941, current_train_items 130880.
I0302 18:59:44.301224 22895071117440 run.py:483] Algo bellman_ford step 4090 current loss 0.277781, current_train_items 130912.
I0302 18:59:44.316801 22895071117440 run.py:483] Algo bellman_ford step 4091 current loss 0.450975, current_train_items 130944.
I0302 18:59:44.338680 22895071117440 run.py:483] Algo bellman_ford step 4092 current loss 0.712828, current_train_items 130976.
I0302 18:59:44.367556 22895071117440 run.py:483] Algo bellman_ford step 4093 current loss 0.772010, current_train_items 131008.
I0302 18:59:44.399132 22895071117440 run.py:483] Algo bellman_ford step 4094 current loss 0.949581, current_train_items 131040.
I0302 18:59:44.417105 22895071117440 run.py:483] Algo bellman_ford step 4095 current loss 0.319094, current_train_items 131072.
I0302 18:59:44.432749 22895071117440 run.py:483] Algo bellman_ford step 4096 current loss 0.564508, current_train_items 131104.
I0302 18:59:44.454765 22895071117440 run.py:483] Algo bellman_ford step 4097 current loss 0.643320, current_train_items 131136.
I0302 18:59:44.483399 22895071117440 run.py:483] Algo bellman_ford step 4098 current loss 0.787822, current_train_items 131168.
I0302 18:59:44.515004 22895071117440 run.py:483] Algo bellman_ford step 4099 current loss 0.881572, current_train_items 131200.
I0302 18:59:44.533329 22895071117440 run.py:483] Algo bellman_ford step 4100 current loss 0.345962, current_train_items 131232.
I0302 18:59:44.541015 22895071117440 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 18:59:44.541120 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 18:59:44.556565 22895071117440 run.py:483] Algo bellman_ford step 4101 current loss 0.451575, current_train_items 131264.
I0302 18:59:44.577048 22895071117440 run.py:483] Algo bellman_ford step 4102 current loss 0.728959, current_train_items 131296.
I0302 18:59:44.606828 22895071117440 run.py:483] Algo bellman_ford step 4103 current loss 0.781425, current_train_items 131328.
I0302 18:59:44.639693 22895071117440 run.py:483] Algo bellman_ford step 4104 current loss 0.933301, current_train_items 131360.
I0302 18:59:44.657830 22895071117440 run.py:483] Algo bellman_ford step 4105 current loss 0.301690, current_train_items 131392.
I0302 18:59:44.673431 22895071117440 run.py:483] Algo bellman_ford step 4106 current loss 0.625395, current_train_items 131424.
I0302 18:59:44.695955 22895071117440 run.py:483] Algo bellman_ford step 4107 current loss 0.685912, current_train_items 131456.
I0302 18:59:44.724387 22895071117440 run.py:483] Algo bellman_ford step 4108 current loss 0.740428, current_train_items 131488.
I0302 18:59:44.757311 22895071117440 run.py:483] Algo bellman_ford step 4109 current loss 0.956124, current_train_items 131520.
I0302 18:59:44.775140 22895071117440 run.py:483] Algo bellman_ford step 4110 current loss 0.370664, current_train_items 131552.
I0302 18:59:44.791025 22895071117440 run.py:483] Algo bellman_ford step 4111 current loss 0.523626, current_train_items 131584.
I0302 18:59:44.812155 22895071117440 run.py:483] Algo bellman_ford step 4112 current loss 0.647865, current_train_items 131616.
I0302 18:59:44.841243 22895071117440 run.py:483] Algo bellman_ford step 4113 current loss 0.665704, current_train_items 131648.
I0302 18:59:44.872836 22895071117440 run.py:483] Algo bellman_ford step 4114 current loss 0.829820, current_train_items 131680.
I0302 18:59:44.890894 22895071117440 run.py:483] Algo bellman_ford step 4115 current loss 0.280335, current_train_items 131712.
I0302 18:59:44.906565 22895071117440 run.py:483] Algo bellman_ford step 4116 current loss 0.573876, current_train_items 131744.
I0302 18:59:44.929319 22895071117440 run.py:483] Algo bellman_ford step 4117 current loss 0.731799, current_train_items 131776.
I0302 18:59:44.956806 22895071117440 run.py:483] Algo bellman_ford step 4118 current loss 0.662283, current_train_items 131808.
I0302 18:59:44.988186 22895071117440 run.py:483] Algo bellman_ford step 4119 current loss 1.026574, current_train_items 131840.
I0302 18:59:45.006240 22895071117440 run.py:483] Algo bellman_ford step 4120 current loss 0.278459, current_train_items 131872.
I0302 18:59:45.021697 22895071117440 run.py:483] Algo bellman_ford step 4121 current loss 0.483346, current_train_items 131904.
I0302 18:59:45.044386 22895071117440 run.py:483] Algo bellman_ford step 4122 current loss 0.719288, current_train_items 131936.
I0302 18:59:45.073688 22895071117440 run.py:483] Algo bellman_ford step 4123 current loss 0.860885, current_train_items 131968.
I0302 18:59:45.104512 22895071117440 run.py:483] Algo bellman_ford step 4124 current loss 0.849554, current_train_items 132000.
I0302 18:59:45.122455 22895071117440 run.py:483] Algo bellman_ford step 4125 current loss 0.229112, current_train_items 132032.
I0302 18:59:45.137916 22895071117440 run.py:483] Algo bellman_ford step 4126 current loss 0.524074, current_train_items 132064.
I0302 18:59:45.160637 22895071117440 run.py:483] Algo bellman_ford step 4127 current loss 0.777218, current_train_items 132096.
I0302 18:59:45.188502 22895071117440 run.py:483] Algo bellman_ford step 4128 current loss 0.827569, current_train_items 132128.
I0302 18:59:45.218185 22895071117440 run.py:483] Algo bellman_ford step 4129 current loss 0.797461, current_train_items 132160.
I0302 18:59:45.236264 22895071117440 run.py:483] Algo bellman_ford step 4130 current loss 0.350948, current_train_items 132192.
I0302 18:59:45.251941 22895071117440 run.py:483] Algo bellman_ford step 4131 current loss 0.462914, current_train_items 132224.
I0302 18:59:45.273935 22895071117440 run.py:483] Algo bellman_ford step 4132 current loss 0.698730, current_train_items 132256.
I0302 18:59:45.303053 22895071117440 run.py:483] Algo bellman_ford step 4133 current loss 0.862944, current_train_items 132288.
I0302 18:59:45.335098 22895071117440 run.py:483] Algo bellman_ford step 4134 current loss 0.891948, current_train_items 132320.
I0302 18:59:45.353241 22895071117440 run.py:483] Algo bellman_ford step 4135 current loss 0.298586, current_train_items 132352.
I0302 18:59:45.368532 22895071117440 run.py:483] Algo bellman_ford step 4136 current loss 0.520033, current_train_items 132384.
I0302 18:59:45.389616 22895071117440 run.py:483] Algo bellman_ford step 4137 current loss 0.787858, current_train_items 132416.
I0302 18:59:45.417953 22895071117440 run.py:483] Algo bellman_ford step 4138 current loss 0.719561, current_train_items 132448.
I0302 18:59:45.449784 22895071117440 run.py:483] Algo bellman_ford step 4139 current loss 0.874122, current_train_items 132480.
I0302 18:59:45.468146 22895071117440 run.py:483] Algo bellman_ford step 4140 current loss 0.343019, current_train_items 132512.
I0302 18:59:45.483116 22895071117440 run.py:483] Algo bellman_ford step 4141 current loss 0.483954, current_train_items 132544.
I0302 18:59:45.506269 22895071117440 run.py:483] Algo bellman_ford step 4142 current loss 0.895597, current_train_items 132576.
I0302 18:59:45.533981 22895071117440 run.py:483] Algo bellman_ford step 4143 current loss 0.976876, current_train_items 132608.
I0302 18:59:45.564718 22895071117440 run.py:483] Algo bellman_ford step 4144 current loss 0.980706, current_train_items 132640.
I0302 18:59:45.582594 22895071117440 run.py:483] Algo bellman_ford step 4145 current loss 0.301864, current_train_items 132672.
I0302 18:59:45.597670 22895071117440 run.py:483] Algo bellman_ford step 4146 current loss 0.451552, current_train_items 132704.
I0302 18:59:45.619210 22895071117440 run.py:483] Algo bellman_ford step 4147 current loss 0.733034, current_train_items 132736.
I0302 18:59:45.646328 22895071117440 run.py:483] Algo bellman_ford step 4148 current loss 0.706440, current_train_items 132768.
I0302 18:59:45.676797 22895071117440 run.py:483] Algo bellman_ford step 4149 current loss 0.997914, current_train_items 132800.
I0302 18:59:45.694862 22895071117440 run.py:483] Algo bellman_ford step 4150 current loss 0.323212, current_train_items 132832.
I0302 18:59:45.702627 22895071117440 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 18:59:45.702732 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 18:59:45.718886 22895071117440 run.py:483] Algo bellman_ford step 4151 current loss 0.564372, current_train_items 132864.
I0302 18:59:45.741636 22895071117440 run.py:483] Algo bellman_ford step 4152 current loss 0.741829, current_train_items 132896.
I0302 18:59:45.770406 22895071117440 run.py:483] Algo bellman_ford step 4153 current loss 0.943050, current_train_items 132928.
I0302 18:59:45.803028 22895071117440 run.py:483] Algo bellman_ford step 4154 current loss 1.085249, current_train_items 132960.
I0302 18:59:45.821444 22895071117440 run.py:483] Algo bellman_ford step 4155 current loss 0.370833, current_train_items 132992.
I0302 18:59:45.837468 22895071117440 run.py:483] Algo bellman_ford step 4156 current loss 0.487572, current_train_items 133024.
I0302 18:59:45.858567 22895071117440 run.py:483] Algo bellman_ford step 4157 current loss 0.598096, current_train_items 133056.
I0302 18:59:45.886576 22895071117440 run.py:483] Algo bellman_ford step 4158 current loss 0.732920, current_train_items 133088.
I0302 18:59:45.917915 22895071117440 run.py:483] Algo bellman_ford step 4159 current loss 0.985696, current_train_items 133120.
I0302 18:59:45.936669 22895071117440 run.py:483] Algo bellman_ford step 4160 current loss 0.292110, current_train_items 133152.
I0302 18:59:45.952569 22895071117440 run.py:483] Algo bellman_ford step 4161 current loss 0.694346, current_train_items 133184.
I0302 18:59:45.972963 22895071117440 run.py:483] Algo bellman_ford step 4162 current loss 0.886305, current_train_items 133216.
I0302 18:59:46.001454 22895071117440 run.py:483] Algo bellman_ford step 4163 current loss 0.949199, current_train_items 133248.
I0302 18:59:46.032752 22895071117440 run.py:483] Algo bellman_ford step 4164 current loss 0.976321, current_train_items 133280.
I0302 18:59:46.050814 22895071117440 run.py:483] Algo bellman_ford step 4165 current loss 0.497869, current_train_items 133312.
I0302 18:59:46.066040 22895071117440 run.py:483] Algo bellman_ford step 4166 current loss 0.464993, current_train_items 133344.
I0302 18:59:46.088619 22895071117440 run.py:483] Algo bellman_ford step 4167 current loss 0.680514, current_train_items 133376.
I0302 18:59:46.117091 22895071117440 run.py:483] Algo bellman_ford step 4168 current loss 0.768467, current_train_items 133408.
I0302 18:59:46.147180 22895071117440 run.py:483] Algo bellman_ford step 4169 current loss 0.815502, current_train_items 133440.
I0302 18:59:46.165807 22895071117440 run.py:483] Algo bellman_ford step 4170 current loss 0.301289, current_train_items 133472.
I0302 18:59:46.180948 22895071117440 run.py:483] Algo bellman_ford step 4171 current loss 0.521282, current_train_items 133504.
I0302 18:59:46.202281 22895071117440 run.py:483] Algo bellman_ford step 4172 current loss 0.849923, current_train_items 133536.
I0302 18:59:46.230064 22895071117440 run.py:483] Algo bellman_ford step 4173 current loss 0.902156, current_train_items 133568.
I0302 18:59:46.262596 22895071117440 run.py:483] Algo bellman_ford step 4174 current loss 1.049664, current_train_items 133600.
I0302 18:59:46.281014 22895071117440 run.py:483] Algo bellman_ford step 4175 current loss 0.286911, current_train_items 133632.
I0302 18:59:46.297299 22895071117440 run.py:483] Algo bellman_ford step 4176 current loss 0.540536, current_train_items 133664.
I0302 18:59:46.318955 22895071117440 run.py:483] Algo bellman_ford step 4177 current loss 0.733034, current_train_items 133696.
I0302 18:59:46.347472 22895071117440 run.py:483] Algo bellman_ford step 4178 current loss 0.822984, current_train_items 133728.
I0302 18:59:46.379241 22895071117440 run.py:483] Algo bellman_ford step 4179 current loss 0.880737, current_train_items 133760.
I0302 18:59:46.397268 22895071117440 run.py:483] Algo bellman_ford step 4180 current loss 0.298730, current_train_items 133792.
I0302 18:59:46.412603 22895071117440 run.py:483] Algo bellman_ford step 4181 current loss 0.514839, current_train_items 133824.
I0302 18:59:46.435368 22895071117440 run.py:483] Algo bellman_ford step 4182 current loss 0.637461, current_train_items 133856.
I0302 18:59:46.462025 22895071117440 run.py:483] Algo bellman_ford step 4183 current loss 0.679981, current_train_items 133888.
I0302 18:59:46.493646 22895071117440 run.py:483] Algo bellman_ford step 4184 current loss 0.867668, current_train_items 133920.
I0302 18:59:46.512117 22895071117440 run.py:483] Algo bellman_ford step 4185 current loss 0.316887, current_train_items 133952.
I0302 18:59:46.527620 22895071117440 run.py:483] Algo bellman_ford step 4186 current loss 0.548435, current_train_items 133984.
I0302 18:59:46.549181 22895071117440 run.py:483] Algo bellman_ford step 4187 current loss 0.683472, current_train_items 134016.
I0302 18:59:46.577355 22895071117440 run.py:483] Algo bellman_ford step 4188 current loss 0.775539, current_train_items 134048.
I0302 18:59:46.607430 22895071117440 run.py:483] Algo bellman_ford step 4189 current loss 0.835602, current_train_items 134080.
I0302 18:59:46.625651 22895071117440 run.py:483] Algo bellman_ford step 4190 current loss 0.407324, current_train_items 134112.
I0302 18:59:46.641695 22895071117440 run.py:483] Algo bellman_ford step 4191 current loss 0.485978, current_train_items 134144.
I0302 18:59:46.661181 22895071117440 run.py:483] Algo bellman_ford step 4192 current loss 0.554533, current_train_items 134176.
I0302 18:59:46.690105 22895071117440 run.py:483] Algo bellman_ford step 4193 current loss 0.738074, current_train_items 134208.
I0302 18:59:46.723417 22895071117440 run.py:483] Algo bellman_ford step 4194 current loss 0.921079, current_train_items 134240.
I0302 18:59:46.741507 22895071117440 run.py:483] Algo bellman_ford step 4195 current loss 0.350470, current_train_items 134272.
I0302 18:59:46.756775 22895071117440 run.py:483] Algo bellman_ford step 4196 current loss 0.593029, current_train_items 134304.
I0302 18:59:46.778630 22895071117440 run.py:483] Algo bellman_ford step 4197 current loss 0.646311, current_train_items 134336.
I0302 18:59:46.806508 22895071117440 run.py:483] Algo bellman_ford step 4198 current loss 0.673481, current_train_items 134368.
I0302 18:59:46.837248 22895071117440 run.py:483] Algo bellman_ford step 4199 current loss 0.860796, current_train_items 134400.
I0302 18:59:46.855086 22895071117440 run.py:483] Algo bellman_ford step 4200 current loss 0.290158, current_train_items 134432.
I0302 18:59:46.862821 22895071117440 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 18:59:46.862936 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 18:59:46.879133 22895071117440 run.py:483] Algo bellman_ford step 4201 current loss 0.613454, current_train_items 134464.
I0302 18:59:46.900932 22895071117440 run.py:483] Algo bellman_ford step 4202 current loss 0.655697, current_train_items 134496.
I0302 18:59:46.929222 22895071117440 run.py:483] Algo bellman_ford step 4203 current loss 0.774208, current_train_items 134528.
I0302 18:59:46.960323 22895071117440 run.py:483] Algo bellman_ford step 4204 current loss 0.883522, current_train_items 134560.
I0302 18:59:46.978417 22895071117440 run.py:483] Algo bellman_ford step 4205 current loss 0.330619, current_train_items 134592.
I0302 18:59:46.993915 22895071117440 run.py:483] Algo bellman_ford step 4206 current loss 0.630521, current_train_items 134624.
I0302 18:59:47.015821 22895071117440 run.py:483] Algo bellman_ford step 4207 current loss 0.651107, current_train_items 134656.
I0302 18:59:47.045040 22895071117440 run.py:483] Algo bellman_ford step 4208 current loss 0.726086, current_train_items 134688.
I0302 18:59:47.076602 22895071117440 run.py:483] Algo bellman_ford step 4209 current loss 0.937898, current_train_items 134720.
I0302 18:59:47.094414 22895071117440 run.py:483] Algo bellman_ford step 4210 current loss 0.261922, current_train_items 134752.
I0302 18:59:47.109748 22895071117440 run.py:483] Algo bellman_ford step 4211 current loss 0.482107, current_train_items 134784.
I0302 18:59:47.131655 22895071117440 run.py:483] Algo bellman_ford step 4212 current loss 0.776548, current_train_items 134816.
I0302 18:59:47.160382 22895071117440 run.py:483] Algo bellman_ford step 4213 current loss 0.727239, current_train_items 134848.
I0302 18:59:47.190832 22895071117440 run.py:483] Algo bellman_ford step 4214 current loss 0.795469, current_train_items 134880.
I0302 18:59:47.208461 22895071117440 run.py:483] Algo bellman_ford step 4215 current loss 0.409779, current_train_items 134912.
I0302 18:59:47.223807 22895071117440 run.py:483] Algo bellman_ford step 4216 current loss 0.500453, current_train_items 134944.
I0302 18:59:47.244920 22895071117440 run.py:483] Algo bellman_ford step 4217 current loss 0.640637, current_train_items 134976.
I0302 18:59:47.273160 22895071117440 run.py:483] Algo bellman_ford step 4218 current loss 0.896518, current_train_items 135008.
I0302 18:59:47.303692 22895071117440 run.py:483] Algo bellman_ford step 4219 current loss 0.961327, current_train_items 135040.
I0302 18:59:47.321473 22895071117440 run.py:483] Algo bellman_ford step 4220 current loss 0.294904, current_train_items 135072.
I0302 18:59:47.336975 22895071117440 run.py:483] Algo bellman_ford step 4221 current loss 0.546769, current_train_items 135104.
I0302 18:59:47.358832 22895071117440 run.py:483] Algo bellman_ford step 4222 current loss 0.645120, current_train_items 135136.
I0302 18:59:47.388032 22895071117440 run.py:483] Algo bellman_ford step 4223 current loss 0.763231, current_train_items 135168.
I0302 18:59:47.420213 22895071117440 run.py:483] Algo bellman_ford step 4224 current loss 0.892255, current_train_items 135200.
I0302 18:59:47.437875 22895071117440 run.py:483] Algo bellman_ford step 4225 current loss 0.331423, current_train_items 135232.
I0302 18:59:47.453064 22895071117440 run.py:483] Algo bellman_ford step 4226 current loss 0.475992, current_train_items 135264.
I0302 18:59:47.474305 22895071117440 run.py:483] Algo bellman_ford step 4227 current loss 0.660396, current_train_items 135296.
I0302 18:59:47.501354 22895071117440 run.py:483] Algo bellman_ford step 4228 current loss 0.735776, current_train_items 135328.
I0302 18:59:47.530071 22895071117440 run.py:483] Algo bellman_ford step 4229 current loss 0.706947, current_train_items 135360.
I0302 18:59:47.548069 22895071117440 run.py:483] Algo bellman_ford step 4230 current loss 0.337662, current_train_items 135392.
I0302 18:59:47.563163 22895071117440 run.py:483] Algo bellman_ford step 4231 current loss 0.442825, current_train_items 135424.
I0302 18:59:47.584737 22895071117440 run.py:483] Algo bellman_ford step 4232 current loss 0.717664, current_train_items 135456.
I0302 18:59:47.613006 22895071117440 run.py:483] Algo bellman_ford step 4233 current loss 0.796255, current_train_items 135488.
I0302 18:59:47.643285 22895071117440 run.py:483] Algo bellman_ford step 4234 current loss 0.825773, current_train_items 135520.
I0302 18:59:47.661203 22895071117440 run.py:483] Algo bellman_ford step 4235 current loss 0.276310, current_train_items 135552.
I0302 18:59:47.677106 22895071117440 run.py:483] Algo bellman_ford step 4236 current loss 0.535578, current_train_items 135584.
I0302 18:59:47.699229 22895071117440 run.py:483] Algo bellman_ford step 4237 current loss 0.678658, current_train_items 135616.
I0302 18:59:47.727702 22895071117440 run.py:483] Algo bellman_ford step 4238 current loss 0.675876, current_train_items 135648.
I0302 18:59:47.760093 22895071117440 run.py:483] Algo bellman_ford step 4239 current loss 0.833421, current_train_items 135680.
I0302 18:59:47.777977 22895071117440 run.py:483] Algo bellman_ford step 4240 current loss 0.238132, current_train_items 135712.
I0302 18:59:47.793125 22895071117440 run.py:483] Algo bellman_ford step 4241 current loss 0.429827, current_train_items 135744.
I0302 18:59:47.814151 22895071117440 run.py:483] Algo bellman_ford step 4242 current loss 0.573361, current_train_items 135776.
I0302 18:59:47.843529 22895071117440 run.py:483] Algo bellman_ford step 4243 current loss 0.893942, current_train_items 135808.
I0302 18:59:47.873401 22895071117440 run.py:483] Algo bellman_ford step 4244 current loss 0.816453, current_train_items 135840.
I0302 18:59:47.891193 22895071117440 run.py:483] Algo bellman_ford step 4245 current loss 0.323677, current_train_items 135872.
I0302 18:59:47.906127 22895071117440 run.py:483] Algo bellman_ford step 4246 current loss 0.452041, current_train_items 135904.
I0302 18:59:47.929058 22895071117440 run.py:483] Algo bellman_ford step 4247 current loss 0.714075, current_train_items 135936.
I0302 18:59:47.957760 22895071117440 run.py:483] Algo bellman_ford step 4248 current loss 0.789126, current_train_items 135968.
I0302 18:59:47.990152 22895071117440 run.py:483] Algo bellman_ford step 4249 current loss 0.832592, current_train_items 136000.
I0302 18:59:48.007991 22895071117440 run.py:483] Algo bellman_ford step 4250 current loss 0.366576, current_train_items 136032.
I0302 18:59:48.015976 22895071117440 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 18:59:48.016083 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 18:59:48.031850 22895071117440 run.py:483] Algo bellman_ford step 4251 current loss 0.462874, current_train_items 136064.
I0302 18:59:48.053722 22895071117440 run.py:483] Algo bellman_ford step 4252 current loss 0.702729, current_train_items 136096.
I0302 18:59:48.082821 22895071117440 run.py:483] Algo bellman_ford step 4253 current loss 0.798970, current_train_items 136128.
I0302 18:59:48.114100 22895071117440 run.py:483] Algo bellman_ford step 4254 current loss 0.935338, current_train_items 136160.
I0302 18:59:48.132238 22895071117440 run.py:483] Algo bellman_ford step 4255 current loss 0.337555, current_train_items 136192.
I0302 18:59:48.147407 22895071117440 run.py:483] Algo bellman_ford step 4256 current loss 0.500938, current_train_items 136224.
I0302 18:59:48.168367 22895071117440 run.py:483] Algo bellman_ford step 4257 current loss 0.517132, current_train_items 136256.
I0302 18:59:48.196654 22895071117440 run.py:483] Algo bellman_ford step 4258 current loss 0.769213, current_train_items 136288.
I0302 18:59:48.228029 22895071117440 run.py:483] Algo bellman_ford step 4259 current loss 0.773473, current_train_items 136320.
I0302 18:59:48.246675 22895071117440 run.py:483] Algo bellman_ford step 4260 current loss 0.327212, current_train_items 136352.
I0302 18:59:48.262768 22895071117440 run.py:483] Algo bellman_ford step 4261 current loss 0.728530, current_train_items 136384.
I0302 18:59:48.284318 22895071117440 run.py:483] Algo bellman_ford step 4262 current loss 0.628845, current_train_items 136416.
I0302 18:59:48.312751 22895071117440 run.py:483] Algo bellman_ford step 4263 current loss 0.724539, current_train_items 136448.
I0302 18:59:48.344000 22895071117440 run.py:483] Algo bellman_ford step 4264 current loss 0.770167, current_train_items 136480.
I0302 18:59:48.361856 22895071117440 run.py:483] Algo bellman_ford step 4265 current loss 0.367022, current_train_items 136512.
I0302 18:59:48.377141 22895071117440 run.py:483] Algo bellman_ford step 4266 current loss 0.622809, current_train_items 136544.
I0302 18:59:48.398572 22895071117440 run.py:483] Algo bellman_ford step 4267 current loss 0.660984, current_train_items 136576.
I0302 18:59:48.426270 22895071117440 run.py:483] Algo bellman_ford step 4268 current loss 0.684649, current_train_items 136608.
I0302 18:59:48.456330 22895071117440 run.py:483] Algo bellman_ford step 4269 current loss 0.858447, current_train_items 136640.
I0302 18:59:48.474662 22895071117440 run.py:483] Algo bellman_ford step 4270 current loss 0.375093, current_train_items 136672.
I0302 18:59:48.490103 22895071117440 run.py:483] Algo bellman_ford step 4271 current loss 0.520324, current_train_items 136704.
I0302 18:59:48.510690 22895071117440 run.py:483] Algo bellman_ford step 4272 current loss 0.613530, current_train_items 136736.
I0302 18:59:48.538753 22895071117440 run.py:483] Algo bellman_ford step 4273 current loss 0.735091, current_train_items 136768.
I0302 18:59:48.568937 22895071117440 run.py:483] Algo bellman_ford step 4274 current loss 0.897218, current_train_items 136800.
I0302 18:59:48.587114 22895071117440 run.py:483] Algo bellman_ford step 4275 current loss 0.274397, current_train_items 136832.
I0302 18:59:48.602670 22895071117440 run.py:483] Algo bellman_ford step 4276 current loss 0.426447, current_train_items 136864.
I0302 18:59:48.623792 22895071117440 run.py:483] Algo bellman_ford step 4277 current loss 0.666379, current_train_items 136896.
I0302 18:59:48.653887 22895071117440 run.py:483] Algo bellman_ford step 4278 current loss 0.817864, current_train_items 136928.
I0302 18:59:48.685612 22895071117440 run.py:483] Algo bellman_ford step 4279 current loss 0.757430, current_train_items 136960.
I0302 18:59:48.703614 22895071117440 run.py:483] Algo bellman_ford step 4280 current loss 0.267900, current_train_items 136992.
I0302 18:59:48.718805 22895071117440 run.py:483] Algo bellman_ford step 4281 current loss 0.431673, current_train_items 137024.
I0302 18:59:48.740417 22895071117440 run.py:483] Algo bellman_ford step 4282 current loss 0.638985, current_train_items 137056.
I0302 18:59:48.767657 22895071117440 run.py:483] Algo bellman_ford step 4283 current loss 0.718281, current_train_items 137088.
I0302 18:59:48.800183 22895071117440 run.py:483] Algo bellman_ford step 4284 current loss 0.897745, current_train_items 137120.
I0302 18:59:48.818795 22895071117440 run.py:483] Algo bellman_ford step 4285 current loss 0.311305, current_train_items 137152.
I0302 18:59:48.834290 22895071117440 run.py:483] Algo bellman_ford step 4286 current loss 0.481969, current_train_items 137184.
I0302 18:59:48.855537 22895071117440 run.py:483] Algo bellman_ford step 4287 current loss 0.664092, current_train_items 137216.
I0302 18:59:48.885329 22895071117440 run.py:483] Algo bellman_ford step 4288 current loss 0.694707, current_train_items 137248.
I0302 18:59:48.916756 22895071117440 run.py:483] Algo bellman_ford step 4289 current loss 0.999665, current_train_items 137280.
I0302 18:59:48.934855 22895071117440 run.py:483] Algo bellman_ford step 4290 current loss 0.336062, current_train_items 137312.
I0302 18:59:48.950927 22895071117440 run.py:483] Algo bellman_ford step 4291 current loss 0.475825, current_train_items 137344.
I0302 18:59:48.973352 22895071117440 run.py:483] Algo bellman_ford step 4292 current loss 0.654278, current_train_items 137376.
I0302 18:59:49.003476 22895071117440 run.py:483] Algo bellman_ford step 4293 current loss 0.849427, current_train_items 137408.
I0302 18:59:49.036390 22895071117440 run.py:483] Algo bellman_ford step 4294 current loss 1.022390, current_train_items 137440.
I0302 18:59:49.054291 22895071117440 run.py:483] Algo bellman_ford step 4295 current loss 0.275515, current_train_items 137472.
I0302 18:59:49.069507 22895071117440 run.py:483] Algo bellman_ford step 4296 current loss 0.515598, current_train_items 137504.
I0302 18:59:49.090614 22895071117440 run.py:483] Algo bellman_ford step 4297 current loss 0.627144, current_train_items 137536.
I0302 18:59:49.119518 22895071117440 run.py:483] Algo bellman_ford step 4298 current loss 0.671468, current_train_items 137568.
I0302 18:59:49.146833 22895071117440 run.py:483] Algo bellman_ford step 4299 current loss 0.697614, current_train_items 137600.
I0302 18:59:49.165566 22895071117440 run.py:483] Algo bellman_ford step 4300 current loss 0.336815, current_train_items 137632.
I0302 18:59:49.173332 22895071117440 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 18:59:49.173437 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:49.189268 22895071117440 run.py:483] Algo bellman_ford step 4301 current loss 0.625475, current_train_items 137664.
I0302 18:59:49.211229 22895071117440 run.py:483] Algo bellman_ford step 4302 current loss 0.712139, current_train_items 137696.
I0302 18:59:49.241331 22895071117440 run.py:483] Algo bellman_ford step 4303 current loss 0.949174, current_train_items 137728.
I0302 18:59:49.274874 22895071117440 run.py:483] Algo bellman_ford step 4304 current loss 0.975454, current_train_items 137760.
I0302 18:59:49.293565 22895071117440 run.py:483] Algo bellman_ford step 4305 current loss 0.364408, current_train_items 137792.
I0302 18:59:49.308988 22895071117440 run.py:483] Algo bellman_ford step 4306 current loss 0.466145, current_train_items 137824.
I0302 18:59:49.332180 22895071117440 run.py:483] Algo bellman_ford step 4307 current loss 0.750450, current_train_items 137856.
I0302 18:59:49.361699 22895071117440 run.py:483] Algo bellman_ford step 4308 current loss 0.980989, current_train_items 137888.
I0302 18:59:49.395081 22895071117440 run.py:483] Algo bellman_ford step 4309 current loss 0.983922, current_train_items 137920.
I0302 18:59:49.413138 22895071117440 run.py:483] Algo bellman_ford step 4310 current loss 0.330411, current_train_items 137952.
I0302 18:59:49.428477 22895071117440 run.py:483] Algo bellman_ford step 4311 current loss 0.528210, current_train_items 137984.
I0302 18:59:49.449342 22895071117440 run.py:483] Algo bellman_ford step 4312 current loss 0.621330, current_train_items 138016.
I0302 18:59:49.478513 22895071117440 run.py:483] Algo bellman_ford step 4313 current loss 0.794660, current_train_items 138048.
I0302 18:59:49.510177 22895071117440 run.py:483] Algo bellman_ford step 4314 current loss 0.993276, current_train_items 138080.
I0302 18:59:49.527949 22895071117440 run.py:483] Algo bellman_ford step 4315 current loss 0.371935, current_train_items 138112.
I0302 18:59:49.543026 22895071117440 run.py:483] Algo bellman_ford step 4316 current loss 0.519383, current_train_items 138144.
I0302 18:59:49.565294 22895071117440 run.py:483] Algo bellman_ford step 4317 current loss 0.700968, current_train_items 138176.
I0302 18:59:49.593217 22895071117440 run.py:483] Algo bellman_ford step 4318 current loss 0.746764, current_train_items 138208.
I0302 18:59:49.623948 22895071117440 run.py:483] Algo bellman_ford step 4319 current loss 0.929632, current_train_items 138240.
I0302 18:59:49.641960 22895071117440 run.py:483] Algo bellman_ford step 4320 current loss 0.422312, current_train_items 138272.
I0302 18:59:49.657374 22895071117440 run.py:483] Algo bellman_ford step 4321 current loss 0.525365, current_train_items 138304.
I0302 18:59:49.678882 22895071117440 run.py:483] Algo bellman_ford step 4322 current loss 0.666104, current_train_items 138336.
I0302 18:59:49.706257 22895071117440 run.py:483] Algo bellman_ford step 4323 current loss 0.743032, current_train_items 138368.
I0302 18:59:49.737948 22895071117440 run.py:483] Algo bellman_ford step 4324 current loss 0.981205, current_train_items 138400.
I0302 18:59:49.755743 22895071117440 run.py:483] Algo bellman_ford step 4325 current loss 0.291250, current_train_items 138432.
I0302 18:59:49.771235 22895071117440 run.py:483] Algo bellman_ford step 4326 current loss 0.500126, current_train_items 138464.
I0302 18:59:49.793222 22895071117440 run.py:483] Algo bellman_ford step 4327 current loss 0.637518, current_train_items 138496.
I0302 18:59:49.823160 22895071117440 run.py:483] Algo bellman_ford step 4328 current loss 0.918035, current_train_items 138528.
I0302 18:59:49.853145 22895071117440 run.py:483] Algo bellman_ford step 4329 current loss 0.918298, current_train_items 138560.
I0302 18:59:49.871243 22895071117440 run.py:483] Algo bellman_ford step 4330 current loss 0.321875, current_train_items 138592.
I0302 18:59:49.886978 22895071117440 run.py:483] Algo bellman_ford step 4331 current loss 0.564100, current_train_items 138624.
I0302 18:59:49.909259 22895071117440 run.py:483] Algo bellman_ford step 4332 current loss 0.809722, current_train_items 138656.
I0302 18:59:49.937190 22895071117440 run.py:483] Algo bellman_ford step 4333 current loss 0.812800, current_train_items 138688.
I0302 18:59:49.966633 22895071117440 run.py:483] Algo bellman_ford step 4334 current loss 0.854322, current_train_items 138720.
I0302 18:59:49.984541 22895071117440 run.py:483] Algo bellman_ford step 4335 current loss 0.334374, current_train_items 138752.
I0302 18:59:49.999840 22895071117440 run.py:483] Algo bellman_ford step 4336 current loss 0.580652, current_train_items 138784.
I0302 18:59:50.022006 22895071117440 run.py:483] Algo bellman_ford step 4337 current loss 0.748730, current_train_items 138816.
I0302 18:59:50.051660 22895071117440 run.py:483] Algo bellman_ford step 4338 current loss 0.847005, current_train_items 138848.
I0302 18:59:50.081613 22895071117440 run.py:483] Algo bellman_ford step 4339 current loss 0.881580, current_train_items 138880.
I0302 18:59:50.099763 22895071117440 run.py:483] Algo bellman_ford step 4340 current loss 0.305897, current_train_items 138912.
I0302 18:59:50.114818 22895071117440 run.py:483] Algo bellman_ford step 4341 current loss 0.444527, current_train_items 138944.
I0302 18:59:50.136568 22895071117440 run.py:483] Algo bellman_ford step 4342 current loss 0.612852, current_train_items 138976.
I0302 18:59:50.165389 22895071117440 run.py:483] Algo bellman_ford step 4343 current loss 0.789243, current_train_items 139008.
I0302 18:59:50.196957 22895071117440 run.py:483] Algo bellman_ford step 4344 current loss 0.751607, current_train_items 139040.
I0302 18:59:50.214978 22895071117440 run.py:483] Algo bellman_ford step 4345 current loss 0.336350, current_train_items 139072.
I0302 18:59:50.230588 22895071117440 run.py:483] Algo bellman_ford step 4346 current loss 0.559899, current_train_items 139104.
I0302 18:59:50.252545 22895071117440 run.py:483] Algo bellman_ford step 4347 current loss 0.726915, current_train_items 139136.
I0302 18:59:50.281426 22895071117440 run.py:483] Algo bellman_ford step 4348 current loss 0.682864, current_train_items 139168.
I0302 18:59:50.311538 22895071117440 run.py:483] Algo bellman_ford step 4349 current loss 0.791079, current_train_items 139200.
I0302 18:59:50.329581 22895071117440 run.py:483] Algo bellman_ford step 4350 current loss 0.323684, current_train_items 139232.
I0302 18:59:50.337492 22895071117440 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 18:59:50.337596 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:59:50.353590 22895071117440 run.py:483] Algo bellman_ford step 4351 current loss 0.516739, current_train_items 139264.
I0302 18:59:50.376375 22895071117440 run.py:483] Algo bellman_ford step 4352 current loss 0.883048, current_train_items 139296.
I0302 18:59:50.404813 22895071117440 run.py:483] Algo bellman_ford step 4353 current loss 0.835266, current_train_items 139328.
I0302 18:59:50.436679 22895071117440 run.py:483] Algo bellman_ford step 4354 current loss 0.993265, current_train_items 139360.
I0302 18:59:50.455041 22895071117440 run.py:483] Algo bellman_ford step 4355 current loss 0.328286, current_train_items 139392.
I0302 18:59:50.470405 22895071117440 run.py:483] Algo bellman_ford step 4356 current loss 0.456380, current_train_items 139424.
I0302 18:59:50.492851 22895071117440 run.py:483] Algo bellman_ford step 4357 current loss 0.663385, current_train_items 139456.
I0302 18:59:50.519538 22895071117440 run.py:483] Algo bellman_ford step 4358 current loss 0.719896, current_train_items 139488.
I0302 18:59:50.549163 22895071117440 run.py:483] Algo bellman_ford step 4359 current loss 0.844797, current_train_items 139520.
I0302 18:59:50.567449 22895071117440 run.py:483] Algo bellman_ford step 4360 current loss 0.299916, current_train_items 139552.
I0302 18:59:50.583584 22895071117440 run.py:483] Algo bellman_ford step 4361 current loss 0.652956, current_train_items 139584.
I0302 18:59:50.604168 22895071117440 run.py:483] Algo bellman_ford step 4362 current loss 0.649687, current_train_items 139616.
I0302 18:59:50.633662 22895071117440 run.py:483] Algo bellman_ford step 4363 current loss 0.784809, current_train_items 139648.
I0302 18:59:50.666128 22895071117440 run.py:483] Algo bellman_ford step 4364 current loss 0.921802, current_train_items 139680.
I0302 18:59:50.683955 22895071117440 run.py:483] Algo bellman_ford step 4365 current loss 0.305019, current_train_items 139712.
I0302 18:59:50.699368 22895071117440 run.py:483] Algo bellman_ford step 4366 current loss 0.571345, current_train_items 139744.
I0302 18:59:50.720820 22895071117440 run.py:483] Algo bellman_ford step 4367 current loss 0.583835, current_train_items 139776.
I0302 18:59:50.748302 22895071117440 run.py:483] Algo bellman_ford step 4368 current loss 0.702275, current_train_items 139808.
I0302 18:59:50.779723 22895071117440 run.py:483] Algo bellman_ford step 4369 current loss 0.887703, current_train_items 139840.
I0302 18:59:50.797781 22895071117440 run.py:483] Algo bellman_ford step 4370 current loss 0.382619, current_train_items 139872.
I0302 18:59:50.813333 22895071117440 run.py:483] Algo bellman_ford step 4371 current loss 0.656147, current_train_items 139904.
I0302 18:59:50.834650 22895071117440 run.py:483] Algo bellman_ford step 4372 current loss 0.711605, current_train_items 139936.
I0302 18:59:50.862815 22895071117440 run.py:483] Algo bellman_ford step 4373 current loss 0.790879, current_train_items 139968.
I0302 18:59:50.893132 22895071117440 run.py:483] Algo bellman_ford step 4374 current loss 0.872279, current_train_items 140000.
I0302 18:59:50.911932 22895071117440 run.py:483] Algo bellman_ford step 4375 current loss 0.366307, current_train_items 140032.
I0302 18:59:50.927408 22895071117440 run.py:483] Algo bellman_ford step 4376 current loss 0.447602, current_train_items 140064.
I0302 18:59:50.948884 22895071117440 run.py:483] Algo bellman_ford step 4377 current loss 0.637944, current_train_items 140096.
I0302 18:59:50.976632 22895071117440 run.py:483] Algo bellman_ford step 4378 current loss 0.629096, current_train_items 140128.
I0302 18:59:51.007648 22895071117440 run.py:483] Algo bellman_ford step 4379 current loss 0.955456, current_train_items 140160.
I0302 18:59:51.025565 22895071117440 run.py:483] Algo bellman_ford step 4380 current loss 0.311896, current_train_items 140192.
I0302 18:59:51.040797 22895071117440 run.py:483] Algo bellman_ford step 4381 current loss 0.529375, current_train_items 140224.
I0302 18:59:51.062772 22895071117440 run.py:483] Algo bellman_ford step 4382 current loss 0.727048, current_train_items 140256.
I0302 18:59:51.091068 22895071117440 run.py:483] Algo bellman_ford step 4383 current loss 0.729693, current_train_items 140288.
I0302 18:59:51.122145 22895071117440 run.py:483] Algo bellman_ford step 4384 current loss 0.849734, current_train_items 140320.
I0302 18:59:51.140671 22895071117440 run.py:483] Algo bellman_ford step 4385 current loss 0.321089, current_train_items 140352.
I0302 18:59:51.156106 22895071117440 run.py:483] Algo bellman_ford step 4386 current loss 0.446985, current_train_items 140384.
I0302 18:59:51.177651 22895071117440 run.py:483] Algo bellman_ford step 4387 current loss 0.753815, current_train_items 140416.
I0302 18:59:51.206396 22895071117440 run.py:483] Algo bellman_ford step 4388 current loss 0.794613, current_train_items 140448.
I0302 18:59:51.235624 22895071117440 run.py:483] Algo bellman_ford step 4389 current loss 0.921630, current_train_items 140480.
I0302 18:59:51.254097 22895071117440 run.py:483] Algo bellman_ford step 4390 current loss 0.255048, current_train_items 140512.
I0302 18:59:51.269563 22895071117440 run.py:483] Algo bellman_ford step 4391 current loss 0.541819, current_train_items 140544.
I0302 18:59:51.290982 22895071117440 run.py:483] Algo bellman_ford step 4392 current loss 0.729030, current_train_items 140576.
I0302 18:59:51.319327 22895071117440 run.py:483] Algo bellman_ford step 4393 current loss 0.831968, current_train_items 140608.
I0302 18:59:51.350443 22895071117440 run.py:483] Algo bellman_ford step 4394 current loss 0.928532, current_train_items 140640.
I0302 18:59:51.368374 22895071117440 run.py:483] Algo bellman_ford step 4395 current loss 0.355458, current_train_items 140672.
I0302 18:59:51.383923 22895071117440 run.py:483] Algo bellman_ford step 4396 current loss 0.752392, current_train_items 140704.
I0302 18:59:51.405763 22895071117440 run.py:483] Algo bellman_ford step 4397 current loss 0.990322, current_train_items 140736.
I0302 18:59:51.434046 22895071117440 run.py:483] Algo bellman_ford step 4398 current loss 1.093365, current_train_items 140768.
I0302 18:59:51.464706 22895071117440 run.py:483] Algo bellman_ford step 4399 current loss 1.268506, current_train_items 140800.
I0302 18:59:51.483076 22895071117440 run.py:483] Algo bellman_ford step 4400 current loss 0.369354, current_train_items 140832.
I0302 18:59:51.490834 22895071117440 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 18:59:51.490948 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0302 18:59:51.507140 22895071117440 run.py:483] Algo bellman_ford step 4401 current loss 0.528719, current_train_items 140864.
I0302 18:59:51.529181 22895071117440 run.py:483] Algo bellman_ford step 4402 current loss 0.789743, current_train_items 140896.
I0302 18:59:51.557422 22895071117440 run.py:483] Algo bellman_ford step 4403 current loss 0.827579, current_train_items 140928.
I0302 18:59:51.592959 22895071117440 run.py:483] Algo bellman_ford step 4404 current loss 1.527964, current_train_items 140960.
I0302 18:59:51.611809 22895071117440 run.py:483] Algo bellman_ford step 4405 current loss 0.377994, current_train_items 140992.
I0302 18:59:51.627281 22895071117440 run.py:483] Algo bellman_ford step 4406 current loss 0.629512, current_train_items 141024.
I0302 18:59:51.649141 22895071117440 run.py:483] Algo bellman_ford step 4407 current loss 0.704431, current_train_items 141056.
I0302 18:59:51.677360 22895071117440 run.py:483] Algo bellman_ford step 4408 current loss 0.706326, current_train_items 141088.
I0302 18:59:51.709702 22895071117440 run.py:483] Algo bellman_ford step 4409 current loss 0.960009, current_train_items 141120.
I0302 18:59:51.727711 22895071117440 run.py:483] Algo bellman_ford step 4410 current loss 0.376266, current_train_items 141152.
I0302 18:59:51.743140 22895071117440 run.py:483] Algo bellman_ford step 4411 current loss 0.620172, current_train_items 141184.
I0302 18:59:51.765387 22895071117440 run.py:483] Algo bellman_ford step 4412 current loss 0.872123, current_train_items 141216.
I0302 18:59:51.795324 22895071117440 run.py:483] Algo bellman_ford step 4413 current loss 1.045759, current_train_items 141248.
I0302 18:59:51.824968 22895071117440 run.py:483] Algo bellman_ford step 4414 current loss 0.996633, current_train_items 141280.
I0302 18:59:51.843126 22895071117440 run.py:483] Algo bellman_ford step 4415 current loss 0.338686, current_train_items 141312.
I0302 18:59:51.858952 22895071117440 run.py:483] Algo bellman_ford step 4416 current loss 0.629977, current_train_items 141344.
I0302 18:59:51.880846 22895071117440 run.py:483] Algo bellman_ford step 4417 current loss 0.718466, current_train_items 141376.
I0302 18:59:51.909329 22895071117440 run.py:483] Algo bellman_ford step 4418 current loss 0.852294, current_train_items 141408.
I0302 18:59:51.940269 22895071117440 run.py:483] Algo bellman_ford step 4419 current loss 0.962362, current_train_items 141440.
I0302 18:59:51.958399 22895071117440 run.py:483] Algo bellman_ford step 4420 current loss 0.324770, current_train_items 141472.
I0302 18:59:51.973475 22895071117440 run.py:483] Algo bellman_ford step 4421 current loss 0.582594, current_train_items 141504.
I0302 18:59:51.995098 22895071117440 run.py:483] Algo bellman_ford step 4422 current loss 0.735744, current_train_items 141536.
I0302 18:59:52.023604 22895071117440 run.py:483] Algo bellman_ford step 4423 current loss 0.796337, current_train_items 141568.
I0302 18:59:52.055459 22895071117440 run.py:483] Algo bellman_ford step 4424 current loss 0.918589, current_train_items 141600.
I0302 18:59:52.073851 22895071117440 run.py:483] Algo bellman_ford step 4425 current loss 0.330935, current_train_items 141632.
I0302 18:59:52.088457 22895071117440 run.py:483] Algo bellman_ford step 4426 current loss 0.499148, current_train_items 141664.
I0302 18:59:52.111007 22895071117440 run.py:483] Algo bellman_ford step 4427 current loss 0.930792, current_train_items 141696.
I0302 18:59:52.139513 22895071117440 run.py:483] Algo bellman_ford step 4428 current loss 0.785954, current_train_items 141728.
I0302 18:59:52.170345 22895071117440 run.py:483] Algo bellman_ford step 4429 current loss 0.955936, current_train_items 141760.
I0302 18:59:52.188091 22895071117440 run.py:483] Algo bellman_ford step 4430 current loss 0.229005, current_train_items 141792.
I0302 18:59:52.202982 22895071117440 run.py:483] Algo bellman_ford step 4431 current loss 0.477371, current_train_items 141824.
I0302 18:59:52.225156 22895071117440 run.py:483] Algo bellman_ford step 4432 current loss 0.720821, current_train_items 141856.
I0302 18:59:52.252428 22895071117440 run.py:483] Algo bellman_ford step 4433 current loss 0.774844, current_train_items 141888.
I0302 18:59:52.283109 22895071117440 run.py:483] Algo bellman_ford step 4434 current loss 0.885188, current_train_items 141920.
I0302 18:59:52.301305 22895071117440 run.py:483] Algo bellman_ford step 4435 current loss 0.369153, current_train_items 141952.
I0302 18:59:52.316773 22895071117440 run.py:483] Algo bellman_ford step 4436 current loss 0.464051, current_train_items 141984.
I0302 18:59:52.339284 22895071117440 run.py:483] Algo bellman_ford step 4437 current loss 0.718506, current_train_items 142016.
I0302 18:59:52.366626 22895071117440 run.py:483] Algo bellman_ford step 4438 current loss 0.630627, current_train_items 142048.
I0302 18:59:52.398279 22895071117440 run.py:483] Algo bellman_ford step 4439 current loss 0.790654, current_train_items 142080.
I0302 18:59:52.416253 22895071117440 run.py:483] Algo bellman_ford step 4440 current loss 0.404093, current_train_items 142112.
I0302 18:59:52.431878 22895071117440 run.py:483] Algo bellman_ford step 4441 current loss 0.574737, current_train_items 142144.
I0302 18:59:52.453204 22895071117440 run.py:483] Algo bellman_ford step 4442 current loss 0.670432, current_train_items 142176.
I0302 18:59:52.481389 22895071117440 run.py:483] Algo bellman_ford step 4443 current loss 0.772479, current_train_items 142208.
I0302 18:59:52.513240 22895071117440 run.py:483] Algo bellman_ford step 4444 current loss 0.780359, current_train_items 142240.
I0302 18:59:52.531508 22895071117440 run.py:483] Algo bellman_ford step 4445 current loss 0.429655, current_train_items 142272.
I0302 18:59:52.546653 22895071117440 run.py:483] Algo bellman_ford step 4446 current loss 0.509621, current_train_items 142304.
I0302 18:59:52.568944 22895071117440 run.py:483] Algo bellman_ford step 4447 current loss 0.531412, current_train_items 142336.
I0302 18:59:52.598144 22895071117440 run.py:483] Algo bellman_ford step 4448 current loss 0.829849, current_train_items 142368.
I0302 18:59:52.629498 22895071117440 run.py:483] Algo bellman_ford step 4449 current loss 0.810191, current_train_items 142400.
I0302 18:59:52.647767 22895071117440 run.py:483] Algo bellman_ford step 4450 current loss 0.328171, current_train_items 142432.
I0302 18:59:52.655639 22895071117440 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 18:59:52.655744 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 18:59:52.671127 22895071117440 run.py:483] Algo bellman_ford step 4451 current loss 0.488750, current_train_items 142464.
I0302 18:59:52.693770 22895071117440 run.py:483] Algo bellman_ford step 4452 current loss 0.644486, current_train_items 142496.
I0302 18:59:52.721502 22895071117440 run.py:483] Algo bellman_ford step 4453 current loss 0.690732, current_train_items 142528.
I0302 18:59:52.751606 22895071117440 run.py:483] Algo bellman_ford step 4454 current loss 0.795883, current_train_items 142560.
I0302 18:59:52.770006 22895071117440 run.py:483] Algo bellman_ford step 4455 current loss 0.269115, current_train_items 142592.
I0302 18:59:52.785251 22895071117440 run.py:483] Algo bellman_ford step 4456 current loss 0.489951, current_train_items 142624.
I0302 18:59:52.807311 22895071117440 run.py:483] Algo bellman_ford step 4457 current loss 0.601562, current_train_items 142656.
I0302 18:59:52.834006 22895071117440 run.py:483] Algo bellman_ford step 4458 current loss 0.688452, current_train_items 142688.
I0302 18:59:52.867857 22895071117440 run.py:483] Algo bellman_ford step 4459 current loss 0.862146, current_train_items 142720.
I0302 18:59:52.886346 22895071117440 run.py:483] Algo bellman_ford step 4460 current loss 0.374886, current_train_items 142752.
I0302 18:59:52.902115 22895071117440 run.py:483] Algo bellman_ford step 4461 current loss 0.497187, current_train_items 142784.
I0302 18:59:52.924373 22895071117440 run.py:483] Algo bellman_ford step 4462 current loss 0.735606, current_train_items 142816.
I0302 18:59:52.951305 22895071117440 run.py:483] Algo bellman_ford step 4463 current loss 0.604333, current_train_items 142848.
I0302 18:59:52.982353 22895071117440 run.py:483] Algo bellman_ford step 4464 current loss 0.870494, current_train_items 142880.
I0302 18:59:53.000439 22895071117440 run.py:483] Algo bellman_ford step 4465 current loss 0.286325, current_train_items 142912.
I0302 18:59:53.015906 22895071117440 run.py:483] Algo bellman_ford step 4466 current loss 0.528436, current_train_items 142944.
I0302 18:59:53.037476 22895071117440 run.py:483] Algo bellman_ford step 4467 current loss 0.668560, current_train_items 142976.
I0302 18:59:53.067383 22895071117440 run.py:483] Algo bellman_ford step 4468 current loss 0.685452, current_train_items 143008.
I0302 18:59:53.099148 22895071117440 run.py:483] Algo bellman_ford step 4469 current loss 0.830524, current_train_items 143040.
I0302 18:59:53.117654 22895071117440 run.py:483] Algo bellman_ford step 4470 current loss 0.263766, current_train_items 143072.
I0302 18:59:53.133152 22895071117440 run.py:483] Algo bellman_ford step 4471 current loss 0.537543, current_train_items 143104.
I0302 18:59:53.154732 22895071117440 run.py:483] Algo bellman_ford step 4472 current loss 0.702336, current_train_items 143136.
I0302 18:59:53.182940 22895071117440 run.py:483] Algo bellman_ford step 4473 current loss 0.802617, current_train_items 143168.
I0302 18:59:53.216152 22895071117440 run.py:483] Algo bellman_ford step 4474 current loss 0.919271, current_train_items 143200.
I0302 18:59:53.234650 22895071117440 run.py:483] Algo bellman_ford step 4475 current loss 0.343716, current_train_items 143232.
I0302 18:59:53.250208 22895071117440 run.py:483] Algo bellman_ford step 4476 current loss 0.561456, current_train_items 143264.
I0302 18:59:53.271530 22895071117440 run.py:483] Algo bellman_ford step 4477 current loss 0.808032, current_train_items 143296.
I0302 18:59:53.299846 22895071117440 run.py:483] Algo bellman_ford step 4478 current loss 0.892495, current_train_items 143328.
I0302 18:59:53.332674 22895071117440 run.py:483] Algo bellman_ford step 4479 current loss 1.518299, current_train_items 143360.
I0302 18:59:53.350798 22895071117440 run.py:483] Algo bellman_ford step 4480 current loss 0.282268, current_train_items 143392.
I0302 18:59:53.366104 22895071117440 run.py:483] Algo bellman_ford step 4481 current loss 0.493810, current_train_items 143424.
I0302 18:59:53.388202 22895071117440 run.py:483] Algo bellman_ford step 4482 current loss 0.724222, current_train_items 143456.
I0302 18:59:53.416888 22895071117440 run.py:483] Algo bellman_ford step 4483 current loss 0.771466, current_train_items 143488.
I0302 18:59:53.445919 22895071117440 run.py:483] Algo bellman_ford step 4484 current loss 0.708263, current_train_items 143520.
I0302 18:59:53.464556 22895071117440 run.py:483] Algo bellman_ford step 4485 current loss 0.351978, current_train_items 143552.
I0302 18:59:53.480525 22895071117440 run.py:483] Algo bellman_ford step 4486 current loss 0.484454, current_train_items 143584.
I0302 18:59:53.503435 22895071117440 run.py:483] Algo bellman_ford step 4487 current loss 0.744898, current_train_items 143616.
I0302 18:59:53.532139 22895071117440 run.py:483] Algo bellman_ford step 4488 current loss 0.721263, current_train_items 143648.
I0302 18:59:53.562791 22895071117440 run.py:483] Algo bellman_ford step 4489 current loss 0.771869, current_train_items 143680.
I0302 18:59:53.581504 22895071117440 run.py:483] Algo bellman_ford step 4490 current loss 0.299762, current_train_items 143712.
I0302 18:59:53.596775 22895071117440 run.py:483] Algo bellman_ford step 4491 current loss 0.492689, current_train_items 143744.
I0302 18:59:53.619012 22895071117440 run.py:483] Algo bellman_ford step 4492 current loss 0.659127, current_train_items 143776.
I0302 18:59:53.647648 22895071117440 run.py:483] Algo bellman_ford step 4493 current loss 0.725026, current_train_items 143808.
I0302 18:59:53.679611 22895071117440 run.py:483] Algo bellman_ford step 4494 current loss 0.807486, current_train_items 143840.
I0302 18:59:53.697760 22895071117440 run.py:483] Algo bellman_ford step 4495 current loss 0.420803, current_train_items 143872.
I0302 18:59:53.713538 22895071117440 run.py:483] Algo bellman_ford step 4496 current loss 0.510492, current_train_items 143904.
I0302 18:59:53.736000 22895071117440 run.py:483] Algo bellman_ford step 4497 current loss 0.838866, current_train_items 143936.
I0302 18:59:53.762853 22895071117440 run.py:483] Algo bellman_ford step 4498 current loss 0.779171, current_train_items 143968.
I0302 18:59:53.791693 22895071117440 run.py:483] Algo bellman_ford step 4499 current loss 0.815642, current_train_items 144000.
I0302 18:59:53.810016 22895071117440 run.py:483] Algo bellman_ford step 4500 current loss 0.328163, current_train_items 144032.
I0302 18:59:53.817996 22895071117440 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 18:59:53.818102 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 18:59:53.834142 22895071117440 run.py:483] Algo bellman_ford step 4501 current loss 0.545300, current_train_items 144064.
I0302 18:59:53.857130 22895071117440 run.py:483] Algo bellman_ford step 4502 current loss 0.880576, current_train_items 144096.
I0302 18:59:53.886270 22895071117440 run.py:483] Algo bellman_ford step 4503 current loss 0.836546, current_train_items 144128.
I0302 18:59:53.918188 22895071117440 run.py:483] Algo bellman_ford step 4504 current loss 0.862389, current_train_items 144160.
I0302 18:59:53.937121 22895071117440 run.py:483] Algo bellman_ford step 4505 current loss 0.287409, current_train_items 144192.
I0302 18:59:53.952625 22895071117440 run.py:483] Algo bellman_ford step 4506 current loss 0.600800, current_train_items 144224.
I0302 18:59:53.974356 22895071117440 run.py:483] Algo bellman_ford step 4507 current loss 0.673782, current_train_items 144256.
I0302 18:59:54.002160 22895071117440 run.py:483] Algo bellman_ford step 4508 current loss 0.718871, current_train_items 144288.
I0302 18:59:54.032809 22895071117440 run.py:483] Algo bellman_ford step 4509 current loss 0.745479, current_train_items 144320.
I0302 18:59:54.050779 22895071117440 run.py:483] Algo bellman_ford step 4510 current loss 0.330743, current_train_items 144352.
I0302 18:59:54.066421 22895071117440 run.py:483] Algo bellman_ford step 4511 current loss 0.530139, current_train_items 144384.
I0302 18:59:54.088747 22895071117440 run.py:483] Algo bellman_ford step 4512 current loss 0.722830, current_train_items 144416.
I0302 18:59:54.117060 22895071117440 run.py:483] Algo bellman_ford step 4513 current loss 0.720443, current_train_items 144448.
I0302 18:59:54.147686 22895071117440 run.py:483] Algo bellman_ford step 4514 current loss 0.977791, current_train_items 144480.
I0302 18:59:54.165750 22895071117440 run.py:483] Algo bellman_ford step 4515 current loss 0.391345, current_train_items 144512.
I0302 18:59:54.181407 22895071117440 run.py:483] Algo bellman_ford step 4516 current loss 0.501797, current_train_items 144544.
I0302 18:59:54.202557 22895071117440 run.py:483] Algo bellman_ford step 4517 current loss 0.566899, current_train_items 144576.
I0302 18:59:54.231632 22895071117440 run.py:483] Algo bellman_ford step 4518 current loss 0.751597, current_train_items 144608.
I0302 18:59:54.265849 22895071117440 run.py:483] Algo bellman_ford step 4519 current loss 1.012838, current_train_items 144640.
I0302 18:59:54.283968 22895071117440 run.py:483] Algo bellman_ford step 4520 current loss 0.356269, current_train_items 144672.
I0302 18:59:54.299526 22895071117440 run.py:483] Algo bellman_ford step 4521 current loss 0.478749, current_train_items 144704.
I0302 18:59:54.321363 22895071117440 run.py:483] Algo bellman_ford step 4522 current loss 0.596248, current_train_items 144736.
I0302 18:59:54.351787 22895071117440 run.py:483] Algo bellman_ford step 4523 current loss 0.804644, current_train_items 144768.
I0302 18:59:54.382423 22895071117440 run.py:483] Algo bellman_ford step 4524 current loss 0.725389, current_train_items 144800.
I0302 18:59:54.400006 22895071117440 run.py:483] Algo bellman_ford step 4525 current loss 0.294962, current_train_items 144832.
I0302 18:59:54.415532 22895071117440 run.py:483] Algo bellman_ford step 4526 current loss 0.482349, current_train_items 144864.
I0302 18:59:54.438119 22895071117440 run.py:483] Algo bellman_ford step 4527 current loss 0.676518, current_train_items 144896.
I0302 18:59:54.466733 22895071117440 run.py:483] Algo bellman_ford step 4528 current loss 0.642322, current_train_items 144928.
I0302 18:59:54.498283 22895071117440 run.py:483] Algo bellman_ford step 4529 current loss 0.843310, current_train_items 144960.
I0302 18:59:54.516639 22895071117440 run.py:483] Algo bellman_ford step 4530 current loss 0.373879, current_train_items 144992.
I0302 18:59:54.531755 22895071117440 run.py:483] Algo bellman_ford step 4531 current loss 0.480856, current_train_items 145024.
I0302 18:59:54.552461 22895071117440 run.py:483] Algo bellman_ford step 4532 current loss 0.560578, current_train_items 145056.
I0302 18:59:54.579086 22895071117440 run.py:483] Algo bellman_ford step 4533 current loss 0.649714, current_train_items 145088.
I0302 18:59:54.611075 22895071117440 run.py:483] Algo bellman_ford step 4534 current loss 0.889175, current_train_items 145120.
I0302 18:59:54.629413 22895071117440 run.py:483] Algo bellman_ford step 4535 current loss 0.252597, current_train_items 145152.
I0302 18:59:54.644773 22895071117440 run.py:483] Algo bellman_ford step 4536 current loss 0.461584, current_train_items 145184.
I0302 18:59:54.666840 22895071117440 run.py:483] Algo bellman_ford step 4537 current loss 0.651550, current_train_items 145216.
I0302 18:59:54.694622 22895071117440 run.py:483] Algo bellman_ford step 4538 current loss 0.645279, current_train_items 145248.
I0302 18:59:54.727093 22895071117440 run.py:483] Algo bellman_ford step 4539 current loss 0.997414, current_train_items 145280.
I0302 18:59:54.744891 22895071117440 run.py:483] Algo bellman_ford step 4540 current loss 0.316583, current_train_items 145312.
I0302 18:59:54.760201 22895071117440 run.py:483] Algo bellman_ford step 4541 current loss 0.486532, current_train_items 145344.
I0302 18:59:54.781267 22895071117440 run.py:483] Algo bellman_ford step 4542 current loss 0.637329, current_train_items 145376.
I0302 18:59:54.809430 22895071117440 run.py:483] Algo bellman_ford step 4543 current loss 0.656028, current_train_items 145408.
I0302 18:59:54.841330 22895071117440 run.py:483] Algo bellman_ford step 4544 current loss 0.934880, current_train_items 145440.
I0302 18:59:54.859100 22895071117440 run.py:483] Algo bellman_ford step 4545 current loss 0.401013, current_train_items 145472.
I0302 18:59:54.874357 22895071117440 run.py:483] Algo bellman_ford step 4546 current loss 0.527875, current_train_items 145504.
I0302 18:59:54.895620 22895071117440 run.py:483] Algo bellman_ford step 4547 current loss 0.650683, current_train_items 145536.
I0302 18:59:54.923036 22895071117440 run.py:483] Algo bellman_ford step 4548 current loss 0.655181, current_train_items 145568.
I0302 18:59:54.954684 22895071117440 run.py:483] Algo bellman_ford step 4549 current loss 0.800270, current_train_items 145600.
I0302 18:59:54.972997 22895071117440 run.py:483] Algo bellman_ford step 4550 current loss 0.278568, current_train_items 145632.
I0302 18:59:54.980832 22895071117440 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 18:59:54.980945 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:54.997162 22895071117440 run.py:483] Algo bellman_ford step 4551 current loss 0.525845, current_train_items 145664.
I0302 18:59:55.018918 22895071117440 run.py:483] Algo bellman_ford step 4552 current loss 0.661801, current_train_items 145696.
I0302 18:59:55.048224 22895071117440 run.py:483] Algo bellman_ford step 4553 current loss 0.728196, current_train_items 145728.
I0302 18:59:55.080297 22895071117440 run.py:483] Algo bellman_ford step 4554 current loss 0.851527, current_train_items 145760.
I0302 18:59:55.098695 22895071117440 run.py:483] Algo bellman_ford step 4555 current loss 0.347810, current_train_items 145792.
I0302 18:59:55.113590 22895071117440 run.py:483] Algo bellman_ford step 4556 current loss 0.540823, current_train_items 145824.
I0302 18:59:55.134687 22895071117440 run.py:483] Algo bellman_ford step 4557 current loss 0.655996, current_train_items 145856.
I0302 18:59:55.163818 22895071117440 run.py:483] Algo bellman_ford step 4558 current loss 0.786638, current_train_items 145888.
I0302 18:59:55.195804 22895071117440 run.py:483] Algo bellman_ford step 4559 current loss 1.113277, current_train_items 145920.
I0302 18:59:55.213922 22895071117440 run.py:483] Algo bellman_ford step 4560 current loss 0.285177, current_train_items 145952.
I0302 18:59:55.229819 22895071117440 run.py:483] Algo bellman_ford step 4561 current loss 0.520944, current_train_items 145984.
I0302 18:59:55.250962 22895071117440 run.py:483] Algo bellman_ford step 4562 current loss 0.652239, current_train_items 146016.
I0302 18:59:55.278559 22895071117440 run.py:483] Algo bellman_ford step 4563 current loss 0.791331, current_train_items 146048.
I0302 18:59:55.310579 22895071117440 run.py:483] Algo bellman_ford step 4564 current loss 0.995442, current_train_items 146080.
I0302 18:59:55.328952 22895071117440 run.py:483] Algo bellman_ford step 4565 current loss 0.333269, current_train_items 146112.
I0302 18:59:55.344277 22895071117440 run.py:483] Algo bellman_ford step 4566 current loss 0.594182, current_train_items 146144.
I0302 18:59:55.366160 22895071117440 run.py:483] Algo bellman_ford step 4567 current loss 0.616677, current_train_items 146176.
I0302 18:59:55.395036 22895071117440 run.py:483] Algo bellman_ford step 4568 current loss 0.801054, current_train_items 146208.
I0302 18:59:55.424949 22895071117440 run.py:483] Algo bellman_ford step 4569 current loss 0.919989, current_train_items 146240.
I0302 18:59:55.442991 22895071117440 run.py:483] Algo bellman_ford step 4570 current loss 0.267518, current_train_items 146272.
I0302 18:59:55.458629 22895071117440 run.py:483] Algo bellman_ford step 4571 current loss 0.504606, current_train_items 146304.
I0302 18:59:55.480414 22895071117440 run.py:483] Algo bellman_ford step 4572 current loss 0.696228, current_train_items 146336.
I0302 18:59:55.507976 22895071117440 run.py:483] Algo bellman_ford step 4573 current loss 0.779982, current_train_items 146368.
I0302 18:59:55.537668 22895071117440 run.py:483] Algo bellman_ford step 4574 current loss 0.844316, current_train_items 146400.
I0302 18:59:55.555653 22895071117440 run.py:483] Algo bellman_ford step 4575 current loss 0.352540, current_train_items 146432.
I0302 18:59:55.571145 22895071117440 run.py:483] Algo bellman_ford step 4576 current loss 0.536649, current_train_items 146464.
I0302 18:59:55.592271 22895071117440 run.py:483] Algo bellman_ford step 4577 current loss 0.667418, current_train_items 146496.
I0302 18:59:55.620801 22895071117440 run.py:483] Algo bellman_ford step 4578 current loss 0.784190, current_train_items 146528.
I0302 18:59:55.651664 22895071117440 run.py:483] Algo bellman_ford step 4579 current loss 0.860985, current_train_items 146560.
I0302 18:59:55.669287 22895071117440 run.py:483] Algo bellman_ford step 4580 current loss 0.316234, current_train_items 146592.
I0302 18:59:55.684979 22895071117440 run.py:483] Algo bellman_ford step 4581 current loss 0.516877, current_train_items 146624.
I0302 18:59:55.706286 22895071117440 run.py:483] Algo bellman_ford step 4582 current loss 0.649465, current_train_items 146656.
I0302 18:59:55.734624 22895071117440 run.py:483] Algo bellman_ford step 4583 current loss 0.696048, current_train_items 146688.
I0302 18:59:55.765661 22895071117440 run.py:483] Algo bellman_ford step 4584 current loss 0.968839, current_train_items 146720.
I0302 18:59:55.784282 22895071117440 run.py:483] Algo bellman_ford step 4585 current loss 0.293019, current_train_items 146752.
I0302 18:59:55.800180 22895071117440 run.py:483] Algo bellman_ford step 4586 current loss 0.497925, current_train_items 146784.
I0302 18:59:55.821006 22895071117440 run.py:483] Algo bellman_ford step 4587 current loss 0.574243, current_train_items 146816.
I0302 18:59:55.849321 22895071117440 run.py:483] Algo bellman_ford step 4588 current loss 0.680329, current_train_items 146848.
I0302 18:59:55.879437 22895071117440 run.py:483] Algo bellman_ford step 4589 current loss 0.801409, current_train_items 146880.
I0302 18:59:55.897797 22895071117440 run.py:483] Algo bellman_ford step 4590 current loss 0.353160, current_train_items 146912.
I0302 18:59:55.913389 22895071117440 run.py:483] Algo bellman_ford step 4591 current loss 0.579643, current_train_items 146944.
I0302 18:59:55.934499 22895071117440 run.py:483] Algo bellman_ford step 4592 current loss 0.675534, current_train_items 146976.
I0302 18:59:55.963809 22895071117440 run.py:483] Algo bellman_ford step 4593 current loss 0.731167, current_train_items 147008.
I0302 18:59:55.995407 22895071117440 run.py:483] Algo bellman_ford step 4594 current loss 0.922790, current_train_items 147040.
I0302 18:59:56.013212 22895071117440 run.py:483] Algo bellman_ford step 4595 current loss 0.343656, current_train_items 147072.
I0302 18:59:56.028425 22895071117440 run.py:483] Algo bellman_ford step 4596 current loss 0.475302, current_train_items 147104.
I0302 18:59:56.052044 22895071117440 run.py:483] Algo bellman_ford step 4597 current loss 0.890799, current_train_items 147136.
I0302 18:59:56.079990 22895071117440 run.py:483] Algo bellman_ford step 4598 current loss 0.799088, current_train_items 147168.
I0302 18:59:56.109580 22895071117440 run.py:483] Algo bellman_ford step 4599 current loss 0.854434, current_train_items 147200.
I0302 18:59:56.127806 22895071117440 run.py:483] Algo bellman_ford step 4600 current loss 0.368986, current_train_items 147232.
I0302 18:59:56.135489 22895071117440 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 18:59:56.135595 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 18:59:56.151865 22895071117440 run.py:483] Algo bellman_ford step 4601 current loss 0.550183, current_train_items 147264.
I0302 18:59:56.174958 22895071117440 run.py:483] Algo bellman_ford step 4602 current loss 0.723902, current_train_items 147296.
I0302 18:59:56.203364 22895071117440 run.py:483] Algo bellman_ford step 4603 current loss 0.768410, current_train_items 147328.
I0302 18:59:56.235890 22895071117440 run.py:483] Algo bellman_ford step 4604 current loss 0.833252, current_train_items 147360.
I0302 18:59:56.254452 22895071117440 run.py:483] Algo bellman_ford step 4605 current loss 0.316965, current_train_items 147392.
I0302 18:59:56.269991 22895071117440 run.py:483] Algo bellman_ford step 4606 current loss 0.590985, current_train_items 147424.
I0302 18:59:56.291487 22895071117440 run.py:483] Algo bellman_ford step 4607 current loss 0.690495, current_train_items 147456.
I0302 18:59:56.320952 22895071117440 run.py:483] Algo bellman_ford step 4608 current loss 0.768854, current_train_items 147488.
I0302 18:59:56.350159 22895071117440 run.py:483] Algo bellman_ford step 4609 current loss 0.711846, current_train_items 147520.
I0302 18:59:56.368542 22895071117440 run.py:483] Algo bellman_ford step 4610 current loss 0.279166, current_train_items 147552.
I0302 18:59:56.383734 22895071117440 run.py:483] Algo bellman_ford step 4611 current loss 0.550865, current_train_items 147584.
I0302 18:59:56.406084 22895071117440 run.py:483] Algo bellman_ford step 4612 current loss 0.645555, current_train_items 147616.
I0302 18:59:56.434781 22895071117440 run.py:483] Algo bellman_ford step 4613 current loss 0.868025, current_train_items 147648.
I0302 18:59:56.466834 22895071117440 run.py:483] Algo bellman_ford step 4614 current loss 1.096069, current_train_items 147680.
I0302 18:59:56.485059 22895071117440 run.py:483] Algo bellman_ford step 4615 current loss 0.369675, current_train_items 147712.
I0302 18:59:56.500477 22895071117440 run.py:483] Algo bellman_ford step 4616 current loss 0.592012, current_train_items 147744.
I0302 18:59:56.522557 22895071117440 run.py:483] Algo bellman_ford step 4617 current loss 0.615292, current_train_items 147776.
I0302 18:59:56.550137 22895071117440 run.py:483] Algo bellman_ford step 4618 current loss 0.602693, current_train_items 147808.
I0302 18:59:56.581061 22895071117440 run.py:483] Algo bellman_ford step 4619 current loss 0.742226, current_train_items 147840.
I0302 18:59:56.598755 22895071117440 run.py:483] Algo bellman_ford step 4620 current loss 0.294872, current_train_items 147872.
I0302 18:59:56.613856 22895071117440 run.py:483] Algo bellman_ford step 4621 current loss 0.436455, current_train_items 147904.
I0302 18:59:56.635253 22895071117440 run.py:483] Algo bellman_ford step 4622 current loss 0.651996, current_train_items 147936.
I0302 18:59:56.663700 22895071117440 run.py:483] Algo bellman_ford step 4623 current loss 0.690019, current_train_items 147968.
I0302 18:59:56.693630 22895071117440 run.py:483] Algo bellman_ford step 4624 current loss 0.927612, current_train_items 148000.
I0302 18:59:56.711543 22895071117440 run.py:483] Algo bellman_ford step 4625 current loss 0.293140, current_train_items 148032.
I0302 18:59:56.726780 22895071117440 run.py:483] Algo bellman_ford step 4626 current loss 0.522292, current_train_items 148064.
I0302 18:59:56.749190 22895071117440 run.py:483] Algo bellman_ford step 4627 current loss 0.717237, current_train_items 148096.
I0302 18:59:56.778554 22895071117440 run.py:483] Algo bellman_ford step 4628 current loss 0.776553, current_train_items 148128.
I0302 18:59:56.811143 22895071117440 run.py:483] Algo bellman_ford step 4629 current loss 0.843396, current_train_items 148160.
I0302 18:59:56.828820 22895071117440 run.py:483] Algo bellman_ford step 4630 current loss 0.294477, current_train_items 148192.
I0302 18:59:56.844399 22895071117440 run.py:483] Algo bellman_ford step 4631 current loss 0.528616, current_train_items 148224.
I0302 18:59:56.867396 22895071117440 run.py:483] Algo bellman_ford step 4632 current loss 0.779335, current_train_items 148256.
I0302 18:59:56.894569 22895071117440 run.py:483] Algo bellman_ford step 4633 current loss 0.764172, current_train_items 148288.
I0302 18:59:56.926165 22895071117440 run.py:483] Algo bellman_ford step 4634 current loss 0.791677, current_train_items 148320.
I0302 18:59:56.944220 22895071117440 run.py:483] Algo bellman_ford step 4635 current loss 0.349806, current_train_items 148352.
I0302 18:59:56.960150 22895071117440 run.py:483] Algo bellman_ford step 4636 current loss 0.518263, current_train_items 148384.
I0302 18:59:56.982396 22895071117440 run.py:483] Algo bellman_ford step 4637 current loss 0.742546, current_train_items 148416.
I0302 18:59:57.012660 22895071117440 run.py:483] Algo bellman_ford step 4638 current loss 0.835089, current_train_items 148448.
I0302 18:59:57.044430 22895071117440 run.py:483] Algo bellman_ford step 4639 current loss 0.852812, current_train_items 148480.
I0302 18:59:57.062438 22895071117440 run.py:483] Algo bellman_ford step 4640 current loss 0.330941, current_train_items 148512.
I0302 18:59:57.077817 22895071117440 run.py:483] Algo bellman_ford step 4641 current loss 0.498865, current_train_items 148544.
I0302 18:59:57.100152 22895071117440 run.py:483] Algo bellman_ford step 4642 current loss 0.688372, current_train_items 148576.
I0302 18:59:57.128856 22895071117440 run.py:483] Algo bellman_ford step 4643 current loss 0.747135, current_train_items 148608.
I0302 18:59:57.160742 22895071117440 run.py:483] Algo bellman_ford step 4644 current loss 0.910897, current_train_items 148640.
I0302 18:59:57.178694 22895071117440 run.py:483] Algo bellman_ford step 4645 current loss 0.343770, current_train_items 148672.
I0302 18:59:57.194181 22895071117440 run.py:483] Algo bellman_ford step 4646 current loss 0.471836, current_train_items 148704.
I0302 18:59:57.215221 22895071117440 run.py:483] Algo bellman_ford step 4647 current loss 0.585014, current_train_items 148736.
I0302 18:59:57.242041 22895071117440 run.py:483] Algo bellman_ford step 4648 current loss 0.707702, current_train_items 148768.
I0302 18:59:57.271381 22895071117440 run.py:483] Algo bellman_ford step 4649 current loss 0.724662, current_train_items 148800.
I0302 18:59:57.289545 22895071117440 run.py:483] Algo bellman_ford step 4650 current loss 0.263765, current_train_items 148832.
I0302 18:59:57.297655 22895071117440 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 18:59:57.297760 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:59:57.313945 22895071117440 run.py:483] Algo bellman_ford step 4651 current loss 0.558586, current_train_items 148864.
I0302 18:59:57.336371 22895071117440 run.py:483] Algo bellman_ford step 4652 current loss 0.656650, current_train_items 148896.
I0302 18:59:57.366210 22895071117440 run.py:483] Algo bellman_ford step 4653 current loss 0.728333, current_train_items 148928.
I0302 18:59:57.397792 22895071117440 run.py:483] Algo bellman_ford step 4654 current loss 1.000244, current_train_items 148960.
I0302 18:59:57.416490 22895071117440 run.py:483] Algo bellman_ford step 4655 current loss 0.345461, current_train_items 148992.
I0302 18:59:57.431925 22895071117440 run.py:483] Algo bellman_ford step 4656 current loss 0.593507, current_train_items 149024.
I0302 18:59:57.454504 22895071117440 run.py:483] Algo bellman_ford step 4657 current loss 0.649322, current_train_items 149056.
I0302 18:59:57.482753 22895071117440 run.py:483] Algo bellman_ford step 4658 current loss 0.719654, current_train_items 149088.
I0302 18:59:57.512673 22895071117440 run.py:483] Algo bellman_ford step 4659 current loss 0.733942, current_train_items 149120.
I0302 18:59:57.531267 22895071117440 run.py:483] Algo bellman_ford step 4660 current loss 0.386284, current_train_items 149152.
I0302 18:59:57.547051 22895071117440 run.py:483] Algo bellman_ford step 4661 current loss 0.513308, current_train_items 149184.
I0302 18:59:57.569610 22895071117440 run.py:483] Algo bellman_ford step 4662 current loss 0.638445, current_train_items 149216.
I0302 18:59:57.597661 22895071117440 run.py:483] Algo bellman_ford step 4663 current loss 0.687384, current_train_items 149248.
I0302 18:59:57.632131 22895071117440 run.py:483] Algo bellman_ford step 4664 current loss 0.883343, current_train_items 149280.
I0302 18:59:57.650099 22895071117440 run.py:483] Algo bellman_ford step 4665 current loss 0.356547, current_train_items 149312.
I0302 18:59:57.665233 22895071117440 run.py:483] Algo bellman_ford step 4666 current loss 0.425531, current_train_items 149344.
I0302 18:59:57.686823 22895071117440 run.py:483] Algo bellman_ford step 4667 current loss 0.602270, current_train_items 149376.
I0302 18:59:57.716269 22895071117440 run.py:483] Algo bellman_ford step 4668 current loss 0.747776, current_train_items 149408.
I0302 18:59:57.748223 22895071117440 run.py:483] Algo bellman_ford step 4669 current loss 0.810137, current_train_items 149440.
I0302 18:59:57.766522 22895071117440 run.py:483] Algo bellman_ford step 4670 current loss 0.316159, current_train_items 149472.
I0302 18:59:57.781810 22895071117440 run.py:483] Algo bellman_ford step 4671 current loss 0.417458, current_train_items 149504.
I0302 18:59:57.802905 22895071117440 run.py:483] Algo bellman_ford step 4672 current loss 0.762763, current_train_items 149536.
I0302 18:59:57.830388 22895071117440 run.py:483] Algo bellman_ford step 4673 current loss 0.738611, current_train_items 149568.
I0302 18:59:57.860941 22895071117440 run.py:483] Algo bellman_ford step 4674 current loss 0.810580, current_train_items 149600.
I0302 18:59:57.879335 22895071117440 run.py:483] Algo bellman_ford step 4675 current loss 0.301419, current_train_items 149632.
I0302 18:59:57.895385 22895071117440 run.py:483] Algo bellman_ford step 4676 current loss 0.640707, current_train_items 149664.
I0302 18:59:57.917395 22895071117440 run.py:483] Algo bellman_ford step 4677 current loss 0.726171, current_train_items 149696.
I0302 18:59:57.947513 22895071117440 run.py:483] Algo bellman_ford step 4678 current loss 0.777038, current_train_items 149728.
I0302 18:59:57.978409 22895071117440 run.py:483] Algo bellman_ford step 4679 current loss 0.884414, current_train_items 149760.
I0302 18:59:57.996743 22895071117440 run.py:483] Algo bellman_ford step 4680 current loss 0.317343, current_train_items 149792.
I0302 18:59:58.011793 22895071117440 run.py:483] Algo bellman_ford step 4681 current loss 0.465764, current_train_items 149824.
I0302 18:59:58.033442 22895071117440 run.py:483] Algo bellman_ford step 4682 current loss 0.596255, current_train_items 149856.
I0302 18:59:58.061491 22895071117440 run.py:483] Algo bellman_ford step 4683 current loss 0.772058, current_train_items 149888.
I0302 18:59:58.093690 22895071117440 run.py:483] Algo bellman_ford step 4684 current loss 0.833102, current_train_items 149920.
I0302 18:59:58.112631 22895071117440 run.py:483] Algo bellman_ford step 4685 current loss 0.346346, current_train_items 149952.
I0302 18:59:58.128013 22895071117440 run.py:483] Algo bellman_ford step 4686 current loss 0.424167, current_train_items 149984.
I0302 18:59:58.149561 22895071117440 run.py:483] Algo bellman_ford step 4687 current loss 0.622811, current_train_items 150016.
I0302 18:59:58.178857 22895071117440 run.py:483] Algo bellman_ford step 4688 current loss 0.678971, current_train_items 150048.
I0302 18:59:58.211102 22895071117440 run.py:483] Algo bellman_ford step 4689 current loss 0.888823, current_train_items 150080.
I0302 18:59:58.229511 22895071117440 run.py:483] Algo bellman_ford step 4690 current loss 0.275062, current_train_items 150112.
I0302 18:59:58.245125 22895071117440 run.py:483] Algo bellman_ford step 4691 current loss 0.477886, current_train_items 150144.
I0302 18:59:58.266262 22895071117440 run.py:483] Algo bellman_ford step 4692 current loss 0.556005, current_train_items 150176.
I0302 18:59:58.294861 22895071117440 run.py:483] Algo bellman_ford step 4693 current loss 0.739929, current_train_items 150208.
I0302 18:59:58.324978 22895071117440 run.py:483] Algo bellman_ford step 4694 current loss 0.813482, current_train_items 150240.
I0302 18:59:58.342986 22895071117440 run.py:483] Algo bellman_ford step 4695 current loss 0.298432, current_train_items 150272.
I0302 18:59:58.358328 22895071117440 run.py:483] Algo bellman_ford step 4696 current loss 0.477607, current_train_items 150304.
I0302 18:59:58.381012 22895071117440 run.py:483] Algo bellman_ford step 4697 current loss 0.744787, current_train_items 150336.
I0302 18:59:58.409278 22895071117440 run.py:483] Algo bellman_ford step 4698 current loss 0.720727, current_train_items 150368.
I0302 18:59:58.438868 22895071117440 run.py:483] Algo bellman_ford step 4699 current loss 0.838502, current_train_items 150400.
I0302 18:59:58.457364 22895071117440 run.py:483] Algo bellman_ford step 4700 current loss 0.355249, current_train_items 150432.
I0302 18:59:58.465107 22895071117440 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 18:59:58.465215 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 18:59:58.480928 22895071117440 run.py:483] Algo bellman_ford step 4701 current loss 0.466506, current_train_items 150464.
I0302 18:59:58.503741 22895071117440 run.py:483] Algo bellman_ford step 4702 current loss 0.670731, current_train_items 150496.
I0302 18:59:58.534014 22895071117440 run.py:483] Algo bellman_ford step 4703 current loss 0.756701, current_train_items 150528.
I0302 18:59:58.567195 22895071117440 run.py:483] Algo bellman_ford step 4704 current loss 0.822013, current_train_items 150560.
I0302 18:59:58.585858 22895071117440 run.py:483] Algo bellman_ford step 4705 current loss 0.373294, current_train_items 150592.
I0302 18:59:58.601006 22895071117440 run.py:483] Algo bellman_ford step 4706 current loss 0.437470, current_train_items 150624.
I0302 18:59:58.623404 22895071117440 run.py:483] Algo bellman_ford step 4707 current loss 0.724326, current_train_items 150656.
I0302 18:59:58.653179 22895071117440 run.py:483] Algo bellman_ford step 4708 current loss 0.843387, current_train_items 150688.
I0302 18:59:58.683964 22895071117440 run.py:483] Algo bellman_ford step 4709 current loss 0.791555, current_train_items 150720.
I0302 18:59:58.701771 22895071117440 run.py:483] Algo bellman_ford step 4710 current loss 0.340640, current_train_items 150752.
I0302 18:59:58.717176 22895071117440 run.py:483] Algo bellman_ford step 4711 current loss 0.496719, current_train_items 150784.
I0302 18:59:58.739788 22895071117440 run.py:483] Algo bellman_ford step 4712 current loss 0.644151, current_train_items 150816.
I0302 18:59:58.768700 22895071117440 run.py:483] Algo bellman_ford step 4713 current loss 0.658590, current_train_items 150848.
I0302 18:59:58.798063 22895071117440 run.py:483] Algo bellman_ford step 4714 current loss 0.821643, current_train_items 150880.
I0302 18:59:58.815883 22895071117440 run.py:483] Algo bellman_ford step 4715 current loss 0.277157, current_train_items 150912.
I0302 18:59:58.831380 22895071117440 run.py:483] Algo bellman_ford step 4716 current loss 0.562339, current_train_items 150944.
I0302 18:59:58.851691 22895071117440 run.py:483] Algo bellman_ford step 4717 current loss 0.575948, current_train_items 150976.
I0302 18:59:58.879917 22895071117440 run.py:483] Algo bellman_ford step 4718 current loss 0.674404, current_train_items 151008.
I0302 18:59:58.912833 22895071117440 run.py:483] Algo bellman_ford step 4719 current loss 0.843511, current_train_items 151040.
I0302 18:59:58.930818 22895071117440 run.py:483] Algo bellman_ford step 4720 current loss 0.317140, current_train_items 151072.
I0302 18:59:58.945855 22895071117440 run.py:483] Algo bellman_ford step 4721 current loss 0.530229, current_train_items 151104.
I0302 18:59:58.968167 22895071117440 run.py:483] Algo bellman_ford step 4722 current loss 0.693610, current_train_items 151136.
I0302 18:59:58.995458 22895071117440 run.py:483] Algo bellman_ford step 4723 current loss 0.627596, current_train_items 151168.
I0302 18:59:59.027271 22895071117440 run.py:483] Algo bellman_ford step 4724 current loss 0.853813, current_train_items 151200.
I0302 18:59:59.044819 22895071117440 run.py:483] Algo bellman_ford step 4725 current loss 0.332972, current_train_items 151232.
I0302 18:59:59.060498 22895071117440 run.py:483] Algo bellman_ford step 4726 current loss 0.487225, current_train_items 151264.
I0302 18:59:59.083351 22895071117440 run.py:483] Algo bellman_ford step 4727 current loss 0.736422, current_train_items 151296.
I0302 18:59:59.110586 22895071117440 run.py:483] Algo bellman_ford step 4728 current loss 0.610548, current_train_items 151328.
I0302 18:59:59.137984 22895071117440 run.py:483] Algo bellman_ford step 4729 current loss 0.642232, current_train_items 151360.
I0302 18:59:59.156010 22895071117440 run.py:483] Algo bellman_ford step 4730 current loss 0.309077, current_train_items 151392.
I0302 18:59:59.171086 22895071117440 run.py:483] Algo bellman_ford step 4731 current loss 0.396601, current_train_items 151424.
I0302 18:59:59.192377 22895071117440 run.py:483] Algo bellman_ford step 4732 current loss 0.588760, current_train_items 151456.
I0302 18:59:59.220503 22895071117440 run.py:483] Algo bellman_ford step 4733 current loss 0.780335, current_train_items 151488.
I0302 18:59:59.251629 22895071117440 run.py:483] Algo bellman_ford step 4734 current loss 0.833291, current_train_items 151520.
I0302 18:59:59.269404 22895071117440 run.py:483] Algo bellman_ford step 4735 current loss 0.337635, current_train_items 151552.
I0302 18:59:59.285098 22895071117440 run.py:483] Algo bellman_ford step 4736 current loss 0.508397, current_train_items 151584.
I0302 18:59:59.306952 22895071117440 run.py:483] Algo bellman_ford step 4737 current loss 0.687708, current_train_items 151616.
I0302 18:59:59.334845 22895071117440 run.py:483] Algo bellman_ford step 4738 current loss 0.724310, current_train_items 151648.
I0302 18:59:59.364735 22895071117440 run.py:483] Algo bellman_ford step 4739 current loss 0.796179, current_train_items 151680.
I0302 18:59:59.382772 22895071117440 run.py:483] Algo bellman_ford step 4740 current loss 0.327080, current_train_items 151712.
I0302 18:59:59.397845 22895071117440 run.py:483] Algo bellman_ford step 4741 current loss 0.520959, current_train_items 151744.
I0302 18:59:59.418998 22895071117440 run.py:483] Algo bellman_ford step 4742 current loss 0.592423, current_train_items 151776.
I0302 18:59:59.447957 22895071117440 run.py:483] Algo bellman_ford step 4743 current loss 0.740766, current_train_items 151808.
I0302 18:59:59.477201 22895071117440 run.py:483] Algo bellman_ford step 4744 current loss 0.779530, current_train_items 151840.
I0302 18:59:59.495034 22895071117440 run.py:483] Algo bellman_ford step 4745 current loss 0.356628, current_train_items 151872.
I0302 18:59:59.510891 22895071117440 run.py:483] Algo bellman_ford step 4746 current loss 0.452422, current_train_items 151904.
I0302 18:59:59.534138 22895071117440 run.py:483] Algo bellman_ford step 4747 current loss 0.698242, current_train_items 151936.
I0302 18:59:59.562786 22895071117440 run.py:483] Algo bellman_ford step 4748 current loss 0.724776, current_train_items 151968.
I0302 18:59:59.593321 22895071117440 run.py:483] Algo bellman_ford step 4749 current loss 0.784164, current_train_items 152000.
I0302 18:59:59.611078 22895071117440 run.py:483] Algo bellman_ford step 4750 current loss 0.297505, current_train_items 152032.
I0302 18:59:59.619002 22895071117440 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 18:59:59.619107 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:59:59.635049 22895071117440 run.py:483] Algo bellman_ford step 4751 current loss 0.493691, current_train_items 152064.
I0302 18:59:59.657934 22895071117440 run.py:483] Algo bellman_ford step 4752 current loss 0.760580, current_train_items 152096.
I0302 18:59:59.688635 22895071117440 run.py:483] Algo bellman_ford step 4753 current loss 0.907154, current_train_items 152128.
I0302 18:59:59.719792 22895071117440 run.py:483] Algo bellman_ford step 4754 current loss 0.897011, current_train_items 152160.
I0302 18:59:59.738248 22895071117440 run.py:483] Algo bellman_ford step 4755 current loss 0.354782, current_train_items 152192.
I0302 18:59:59.753602 22895071117440 run.py:483] Algo bellman_ford step 4756 current loss 0.502107, current_train_items 152224.
I0302 18:59:59.776136 22895071117440 run.py:483] Algo bellman_ford step 4757 current loss 0.592605, current_train_items 152256.
I0302 18:59:59.804865 22895071117440 run.py:483] Algo bellman_ford step 4758 current loss 0.624222, current_train_items 152288.
I0302 18:59:59.836391 22895071117440 run.py:483] Algo bellman_ford step 4759 current loss 0.903993, current_train_items 152320.
I0302 18:59:59.854748 22895071117440 run.py:483] Algo bellman_ford step 4760 current loss 0.306749, current_train_items 152352.
I0302 18:59:59.870568 22895071117440 run.py:483] Algo bellman_ford step 4761 current loss 0.531495, current_train_items 152384.
I0302 18:59:59.892697 22895071117440 run.py:483] Algo bellman_ford step 4762 current loss 0.726545, current_train_items 152416.
I0302 18:59:59.920531 22895071117440 run.py:483] Algo bellman_ford step 4763 current loss 0.743232, current_train_items 152448.
I0302 18:59:59.951686 22895071117440 run.py:483] Algo bellman_ford step 4764 current loss 0.882067, current_train_items 152480.
I0302 18:59:59.969687 22895071117440 run.py:483] Algo bellman_ford step 4765 current loss 0.499922, current_train_items 152512.
I0302 18:59:59.984512 22895071117440 run.py:483] Algo bellman_ford step 4766 current loss 0.404525, current_train_items 152544.
I0302 19:00:00.007255 22895071117440 run.py:483] Algo bellman_ford step 4767 current loss 0.647870, current_train_items 152576.
I0302 19:00:00.034868 22895071117440 run.py:483] Algo bellman_ford step 4768 current loss 0.747986, current_train_items 152608.
I0302 19:00:00.064786 22895071117440 run.py:483] Algo bellman_ford step 4769 current loss 0.834283, current_train_items 152640.
I0302 19:00:00.083184 22895071117440 run.py:483] Algo bellman_ford step 4770 current loss 0.488448, current_train_items 152672.
I0302 19:00:00.098610 22895071117440 run.py:483] Algo bellman_ford step 4771 current loss 0.384559, current_train_items 152704.
I0302 19:00:00.119108 22895071117440 run.py:483] Algo bellman_ford step 4772 current loss 0.551351, current_train_items 152736.
I0302 19:00:00.147296 22895071117440 run.py:483] Algo bellman_ford step 4773 current loss 0.879952, current_train_items 152768.
I0302 19:00:00.178746 22895071117440 run.py:483] Algo bellman_ford step 4774 current loss 0.927723, current_train_items 152800.
I0302 19:00:00.197282 22895071117440 run.py:483] Algo bellman_ford step 4775 current loss 0.313547, current_train_items 152832.
I0302 19:00:00.212857 22895071117440 run.py:483] Algo bellman_ford step 4776 current loss 0.470446, current_train_items 152864.
I0302 19:00:00.234149 22895071117440 run.py:483] Algo bellman_ford step 4777 current loss 0.608743, current_train_items 152896.
I0302 19:00:00.262815 22895071117440 run.py:483] Algo bellman_ford step 4778 current loss 0.758423, current_train_items 152928.
I0302 19:00:00.292347 22895071117440 run.py:483] Algo bellman_ford step 4779 current loss 0.750022, current_train_items 152960.
I0302 19:00:00.310483 22895071117440 run.py:483] Algo bellman_ford step 4780 current loss 0.305115, current_train_items 152992.
I0302 19:00:00.325831 22895071117440 run.py:483] Algo bellman_ford step 4781 current loss 0.569137, current_train_items 153024.
I0302 19:00:00.347907 22895071117440 run.py:483] Algo bellman_ford step 4782 current loss 0.639971, current_train_items 153056.
I0302 19:00:00.377138 22895071117440 run.py:483] Algo bellman_ford step 4783 current loss 0.756057, current_train_items 153088.
I0302 19:00:00.408936 22895071117440 run.py:483] Algo bellman_ford step 4784 current loss 0.770930, current_train_items 153120.
I0302 19:00:00.427280 22895071117440 run.py:483] Algo bellman_ford step 4785 current loss 0.454967, current_train_items 153152.
I0302 19:00:00.443307 22895071117440 run.py:483] Algo bellman_ford step 4786 current loss 0.523705, current_train_items 153184.
I0302 19:00:00.465445 22895071117440 run.py:483] Algo bellman_ford step 4787 current loss 0.640681, current_train_items 153216.
I0302 19:00:00.493441 22895071117440 run.py:483] Algo bellman_ford step 4788 current loss 0.668146, current_train_items 153248.
I0302 19:00:00.523273 22895071117440 run.py:483] Algo bellman_ford step 4789 current loss 0.755280, current_train_items 153280.
I0302 19:00:00.541289 22895071117440 run.py:483] Algo bellman_ford step 4790 current loss 0.382123, current_train_items 153312.
I0302 19:00:00.556870 22895071117440 run.py:483] Algo bellman_ford step 4791 current loss 0.489028, current_train_items 153344.
I0302 19:00:00.578800 22895071117440 run.py:483] Algo bellman_ford step 4792 current loss 0.629834, current_train_items 153376.
I0302 19:00:00.606907 22895071117440 run.py:483] Algo bellman_ford step 4793 current loss 0.851820, current_train_items 153408.
I0302 19:00:00.639773 22895071117440 run.py:483] Algo bellman_ford step 4794 current loss 0.906734, current_train_items 153440.
I0302 19:00:00.657748 22895071117440 run.py:483] Algo bellman_ford step 4795 current loss 0.231003, current_train_items 153472.
I0302 19:00:00.673114 22895071117440 run.py:483] Algo bellman_ford step 4796 current loss 0.440713, current_train_items 153504.
I0302 19:00:00.695263 22895071117440 run.py:483] Algo bellman_ford step 4797 current loss 0.768473, current_train_items 153536.
I0302 19:00:00.723319 22895071117440 run.py:483] Algo bellman_ford step 4798 current loss 0.711830, current_train_items 153568.
I0302 19:00:00.753605 22895071117440 run.py:483] Algo bellman_ford step 4799 current loss 0.754733, current_train_items 153600.
I0302 19:00:00.771975 22895071117440 run.py:483] Algo bellman_ford step 4800 current loss 0.272603, current_train_items 153632.
I0302 19:00:00.779808 22895071117440 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:00.779921 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:00:00.795835 22895071117440 run.py:483] Algo bellman_ford step 4801 current loss 0.629364, current_train_items 153664.
I0302 19:00:00.818241 22895071117440 run.py:483] Algo bellman_ford step 4802 current loss 0.761996, current_train_items 153696.
I0302 19:00:00.846621 22895071117440 run.py:483] Algo bellman_ford step 4803 current loss 0.636926, current_train_items 153728.
I0302 19:00:00.880398 22895071117440 run.py:483] Algo bellman_ford step 4804 current loss 0.889381, current_train_items 153760.
I0302 19:00:00.898859 22895071117440 run.py:483] Algo bellman_ford step 4805 current loss 0.340129, current_train_items 153792.
I0302 19:00:00.914685 22895071117440 run.py:483] Algo bellman_ford step 4806 current loss 0.529401, current_train_items 153824.
I0302 19:00:00.936571 22895071117440 run.py:483] Algo bellman_ford step 4807 current loss 0.677912, current_train_items 153856.
I0302 19:00:00.965364 22895071117440 run.py:483] Algo bellman_ford step 4808 current loss 0.775268, current_train_items 153888.
I0302 19:00:00.996424 22895071117440 run.py:483] Algo bellman_ford step 4809 current loss 0.804823, current_train_items 153920.
I0302 19:00:01.014751 22895071117440 run.py:483] Algo bellman_ford step 4810 current loss 0.323741, current_train_items 153952.
I0302 19:00:01.030123 22895071117440 run.py:483] Algo bellman_ford step 4811 current loss 0.527034, current_train_items 153984.
I0302 19:00:01.051834 22895071117440 run.py:483] Algo bellman_ford step 4812 current loss 0.665992, current_train_items 154016.
I0302 19:00:01.079707 22895071117440 run.py:483] Algo bellman_ford step 4813 current loss 0.892748, current_train_items 154048.
I0302 19:00:01.112230 22895071117440 run.py:483] Algo bellman_ford step 4814 current loss 1.203356, current_train_items 154080.
I0302 19:00:01.130226 22895071117440 run.py:483] Algo bellman_ford step 4815 current loss 0.342516, current_train_items 154112.
I0302 19:00:01.145692 22895071117440 run.py:483] Algo bellman_ford step 4816 current loss 0.481423, current_train_items 154144.
I0302 19:00:01.168557 22895071117440 run.py:483] Algo bellman_ford step 4817 current loss 0.710502, current_train_items 154176.
I0302 19:00:01.197350 22895071117440 run.py:483] Algo bellman_ford step 4818 current loss 0.711511, current_train_items 154208.
I0302 19:00:01.224172 22895071117440 run.py:483] Algo bellman_ford step 4819 current loss 0.687452, current_train_items 154240.
I0302 19:00:01.242392 22895071117440 run.py:483] Algo bellman_ford step 4820 current loss 0.319579, current_train_items 154272.
I0302 19:00:01.257678 22895071117440 run.py:483] Algo bellman_ford step 4821 current loss 0.561548, current_train_items 154304.
I0302 19:00:01.279895 22895071117440 run.py:483] Algo bellman_ford step 4822 current loss 0.678749, current_train_items 154336.
I0302 19:00:01.308108 22895071117440 run.py:483] Algo bellman_ford step 4823 current loss 0.712128, current_train_items 154368.
I0302 19:00:01.339657 22895071117440 run.py:483] Algo bellman_ford step 4824 current loss 0.764860, current_train_items 154400.
I0302 19:00:01.357593 22895071117440 run.py:483] Algo bellman_ford step 4825 current loss 0.300056, current_train_items 154432.
I0302 19:00:01.373566 22895071117440 run.py:483] Algo bellman_ford step 4826 current loss 0.442124, current_train_items 154464.
I0302 19:00:01.395013 22895071117440 run.py:483] Algo bellman_ford step 4827 current loss 0.649138, current_train_items 154496.
I0302 19:00:01.423835 22895071117440 run.py:483] Algo bellman_ford step 4828 current loss 0.850194, current_train_items 154528.
I0302 19:00:01.455265 22895071117440 run.py:483] Algo bellman_ford step 4829 current loss 1.066237, current_train_items 154560.
I0302 19:00:01.473477 22895071117440 run.py:483] Algo bellman_ford step 4830 current loss 0.243378, current_train_items 154592.
I0302 19:00:01.488980 22895071117440 run.py:483] Algo bellman_ford step 4831 current loss 0.558605, current_train_items 154624.
I0302 19:00:01.511259 22895071117440 run.py:483] Algo bellman_ford step 4832 current loss 0.661191, current_train_items 154656.
I0302 19:00:01.540253 22895071117440 run.py:483] Algo bellman_ford step 4833 current loss 0.752380, current_train_items 154688.
I0302 19:00:01.570811 22895071117440 run.py:483] Algo bellman_ford step 4834 current loss 0.803877, current_train_items 154720.
I0302 19:00:01.588503 22895071117440 run.py:483] Algo bellman_ford step 4835 current loss 0.223394, current_train_items 154752.
I0302 19:00:01.603834 22895071117440 run.py:483] Algo bellman_ford step 4836 current loss 0.494486, current_train_items 154784.
I0302 19:00:01.624779 22895071117440 run.py:483] Algo bellman_ford step 4837 current loss 0.658604, current_train_items 154816.
I0302 19:00:01.653091 22895071117440 run.py:483] Algo bellman_ford step 4838 current loss 0.757030, current_train_items 154848.
I0302 19:00:01.681926 22895071117440 run.py:483] Algo bellman_ford step 4839 current loss 0.800910, current_train_items 154880.
I0302 19:00:01.699913 22895071117440 run.py:483] Algo bellman_ford step 4840 current loss 0.293029, current_train_items 154912.
I0302 19:00:01.715168 22895071117440 run.py:483] Algo bellman_ford step 4841 current loss 0.488142, current_train_items 154944.
I0302 19:00:01.736774 22895071117440 run.py:483] Algo bellman_ford step 4842 current loss 0.745906, current_train_items 154976.
I0302 19:00:01.764584 22895071117440 run.py:483] Algo bellman_ford step 4843 current loss 0.920820, current_train_items 155008.
I0302 19:00:01.796984 22895071117440 run.py:483] Algo bellman_ford step 4844 current loss 1.478423, current_train_items 155040.
I0302 19:00:01.815191 22895071117440 run.py:483] Algo bellman_ford step 4845 current loss 0.382423, current_train_items 155072.
I0302 19:00:01.830823 22895071117440 run.py:483] Algo bellman_ford step 4846 current loss 0.603245, current_train_items 155104.
I0302 19:00:01.852194 22895071117440 run.py:483] Algo bellman_ford step 4847 current loss 0.654800, current_train_items 155136.
I0302 19:00:01.880463 22895071117440 run.py:483] Algo bellman_ford step 4848 current loss 0.914584, current_train_items 155168.
I0302 19:00:01.911391 22895071117440 run.py:483] Algo bellman_ford step 4849 current loss 0.812879, current_train_items 155200.
I0302 19:00:01.929478 22895071117440 run.py:483] Algo bellman_ford step 4850 current loss 0.340141, current_train_items 155232.
I0302 19:00:01.937449 22895071117440 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:01.937556 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:00:01.953816 22895071117440 run.py:483] Algo bellman_ford step 4851 current loss 0.569235, current_train_items 155264.
I0302 19:00:01.974789 22895071117440 run.py:483] Algo bellman_ford step 4852 current loss 0.696628, current_train_items 155296.
I0302 19:00:02.002832 22895071117440 run.py:483] Algo bellman_ford step 4853 current loss 0.858980, current_train_items 155328.
I0302 19:00:02.034080 22895071117440 run.py:483] Algo bellman_ford step 4854 current loss 0.965684, current_train_items 155360.
I0302 19:00:02.052352 22895071117440 run.py:483] Algo bellman_ford step 4855 current loss 0.365773, current_train_items 155392.
I0302 19:00:02.068195 22895071117440 run.py:483] Algo bellman_ford step 4856 current loss 0.676796, current_train_items 155424.
I0302 19:00:02.090950 22895071117440 run.py:483] Algo bellman_ford step 4857 current loss 0.736728, current_train_items 155456.
I0302 19:00:02.120199 22895071117440 run.py:483] Algo bellman_ford step 4858 current loss 0.835637, current_train_items 155488.
I0302 19:00:02.153019 22895071117440 run.py:483] Algo bellman_ford step 4859 current loss 0.821381, current_train_items 155520.
I0302 19:00:02.171011 22895071117440 run.py:483] Algo bellman_ford step 4860 current loss 0.282568, current_train_items 155552.
I0302 19:00:02.187431 22895071117440 run.py:483] Algo bellman_ford step 4861 current loss 0.582189, current_train_items 155584.
I0302 19:00:02.208663 22895071117440 run.py:483] Algo bellman_ford step 4862 current loss 0.699241, current_train_items 155616.
I0302 19:00:02.237991 22895071117440 run.py:483] Algo bellman_ford step 4863 current loss 0.751558, current_train_items 155648.
I0302 19:00:02.267655 22895071117440 run.py:483] Algo bellman_ford step 4864 current loss 0.766324, current_train_items 155680.
I0302 19:00:02.285369 22895071117440 run.py:483] Algo bellman_ford step 4865 current loss 0.281477, current_train_items 155712.
I0302 19:00:02.300344 22895071117440 run.py:483] Algo bellman_ford step 4866 current loss 0.499636, current_train_items 155744.
I0302 19:00:02.322662 22895071117440 run.py:483] Algo bellman_ford step 4867 current loss 0.737310, current_train_items 155776.
I0302 19:00:02.352060 22895071117440 run.py:483] Algo bellman_ford step 4868 current loss 0.761475, current_train_items 155808.
I0302 19:00:02.381337 22895071117440 run.py:483] Algo bellman_ford step 4869 current loss 0.656967, current_train_items 155840.
I0302 19:00:02.399782 22895071117440 run.py:483] Algo bellman_ford step 4870 current loss 0.354571, current_train_items 155872.
I0302 19:00:02.415500 22895071117440 run.py:483] Algo bellman_ford step 4871 current loss 0.533784, current_train_items 155904.
I0302 19:00:02.436800 22895071117440 run.py:483] Algo bellman_ford step 4872 current loss 0.619384, current_train_items 155936.
I0302 19:00:02.463097 22895071117440 run.py:483] Algo bellman_ford step 4873 current loss 0.622612, current_train_items 155968.
I0302 19:00:02.492526 22895071117440 run.py:483] Algo bellman_ford step 4874 current loss 0.915989, current_train_items 156000.
I0302 19:00:02.510756 22895071117440 run.py:483] Algo bellman_ford step 4875 current loss 0.295320, current_train_items 156032.
I0302 19:00:02.526101 22895071117440 run.py:483] Algo bellman_ford step 4876 current loss 0.451993, current_train_items 156064.
I0302 19:00:02.546705 22895071117440 run.py:483] Algo bellman_ford step 4877 current loss 0.573874, current_train_items 156096.
I0302 19:00:02.573830 22895071117440 run.py:483] Algo bellman_ford step 4878 current loss 0.742050, current_train_items 156128.
I0302 19:00:02.604453 22895071117440 run.py:483] Algo bellman_ford step 4879 current loss 0.772984, current_train_items 156160.
I0302 19:00:02.622472 22895071117440 run.py:483] Algo bellman_ford step 4880 current loss 0.302634, current_train_items 156192.
I0302 19:00:02.637733 22895071117440 run.py:483] Algo bellman_ford step 4881 current loss 0.469340, current_train_items 156224.
I0302 19:00:02.659826 22895071117440 run.py:483] Algo bellman_ford step 4882 current loss 0.718982, current_train_items 156256.
I0302 19:00:02.688388 22895071117440 run.py:483] Algo bellman_ford step 4883 current loss 0.814158, current_train_items 156288.
I0302 19:00:02.719948 22895071117440 run.py:483] Algo bellman_ford step 4884 current loss 0.812403, current_train_items 156320.
I0302 19:00:02.738001 22895071117440 run.py:483] Algo bellman_ford step 4885 current loss 0.306161, current_train_items 156352.
I0302 19:00:02.753941 22895071117440 run.py:483] Algo bellman_ford step 4886 current loss 0.573529, current_train_items 156384.
I0302 19:00:02.775235 22895071117440 run.py:483] Algo bellman_ford step 4887 current loss 0.693043, current_train_items 156416.
I0302 19:00:02.803055 22895071117440 run.py:483] Algo bellman_ford step 4888 current loss 0.655252, current_train_items 156448.
I0302 19:00:02.833093 22895071117440 run.py:483] Algo bellman_ford step 4889 current loss 0.793947, current_train_items 156480.
I0302 19:00:02.851333 22895071117440 run.py:483] Algo bellman_ford step 4890 current loss 0.323434, current_train_items 156512.
I0302 19:00:02.866786 22895071117440 run.py:483] Algo bellman_ford step 4891 current loss 0.464661, current_train_items 156544.
I0302 19:00:02.887938 22895071117440 run.py:483] Algo bellman_ford step 4892 current loss 0.671468, current_train_items 156576.
I0302 19:00:02.914778 22895071117440 run.py:483] Algo bellman_ford step 4893 current loss 0.683793, current_train_items 156608.
I0302 19:00:02.944512 22895071117440 run.py:483] Algo bellman_ford step 4894 current loss 0.808761, current_train_items 156640.
I0302 19:00:02.962164 22895071117440 run.py:483] Algo bellman_ford step 4895 current loss 0.304937, current_train_items 156672.
I0302 19:00:02.977570 22895071117440 run.py:483] Algo bellman_ford step 4896 current loss 0.462413, current_train_items 156704.
I0302 19:00:02.998319 22895071117440 run.py:483] Algo bellman_ford step 4897 current loss 0.581993, current_train_items 156736.
I0302 19:00:03.025557 22895071117440 run.py:483] Algo bellman_ford step 4898 current loss 0.713057, current_train_items 156768.
I0302 19:00:03.056867 22895071117440 run.py:483] Algo bellman_ford step 4899 current loss 0.949362, current_train_items 156800.
I0302 19:00:03.075010 22895071117440 run.py:483] Algo bellman_ford step 4900 current loss 0.353474, current_train_items 156832.
I0302 19:00:03.082704 22895071117440 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:03.082836 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:03.098461 22895071117440 run.py:483] Algo bellman_ford step 4901 current loss 0.438739, current_train_items 156864.
I0302 19:00:03.120813 22895071117440 run.py:483] Algo bellman_ford step 4902 current loss 0.667479, current_train_items 156896.
I0302 19:00:03.151496 22895071117440 run.py:483] Algo bellman_ford step 4903 current loss 0.788147, current_train_items 156928.
I0302 19:00:03.182501 22895071117440 run.py:483] Algo bellman_ford step 4904 current loss 0.773287, current_train_items 156960.
I0302 19:00:03.201003 22895071117440 run.py:483] Algo bellman_ford step 4905 current loss 0.362872, current_train_items 156992.
I0302 19:00:03.216312 22895071117440 run.py:483] Algo bellman_ford step 4906 current loss 0.530053, current_train_items 157024.
I0302 19:00:03.238388 22895071117440 run.py:483] Algo bellman_ford step 4907 current loss 0.691922, current_train_items 157056.
I0302 19:00:03.267063 22895071117440 run.py:483] Algo bellman_ford step 4908 current loss 0.796662, current_train_items 157088.
I0302 19:00:03.297375 22895071117440 run.py:483] Algo bellman_ford step 4909 current loss 0.896357, current_train_items 157120.
I0302 19:00:03.315249 22895071117440 run.py:483] Algo bellman_ford step 4910 current loss 0.365707, current_train_items 157152.
I0302 19:00:03.330756 22895071117440 run.py:483] Algo bellman_ford step 4911 current loss 0.483933, current_train_items 157184.
I0302 19:00:03.352585 22895071117440 run.py:483] Algo bellman_ford step 4912 current loss 0.593005, current_train_items 157216.
I0302 19:00:03.382231 22895071117440 run.py:483] Algo bellman_ford step 4913 current loss 0.757868, current_train_items 157248.
I0302 19:00:03.412716 22895071117440 run.py:483] Algo bellman_ford step 4914 current loss 0.824220, current_train_items 157280.
I0302 19:00:03.430905 22895071117440 run.py:483] Algo bellman_ford step 4915 current loss 0.299007, current_train_items 157312.
I0302 19:00:03.445965 22895071117440 run.py:483] Algo bellman_ford step 4916 current loss 0.543120, current_train_items 157344.
I0302 19:00:03.467966 22895071117440 run.py:483] Algo bellman_ford step 4917 current loss 0.583447, current_train_items 157376.
I0302 19:00:03.495161 22895071117440 run.py:483] Algo bellman_ford step 4918 current loss 0.660756, current_train_items 157408.
I0302 19:00:03.526088 22895071117440 run.py:483] Algo bellman_ford step 4919 current loss 0.861009, current_train_items 157440.
I0302 19:00:03.544218 22895071117440 run.py:483] Algo bellman_ford step 4920 current loss 0.326327, current_train_items 157472.
I0302 19:00:03.558974 22895071117440 run.py:483] Algo bellman_ford step 4921 current loss 0.382676, current_train_items 157504.
I0302 19:00:03.581041 22895071117440 run.py:483] Algo bellman_ford step 4922 current loss 0.607944, current_train_items 157536.
I0302 19:00:03.609589 22895071117440 run.py:483] Algo bellman_ford step 4923 current loss 0.769310, current_train_items 157568.
I0302 19:00:03.640147 22895071117440 run.py:483] Algo bellman_ford step 4924 current loss 1.011068, current_train_items 157600.
I0302 19:00:03.658540 22895071117440 run.py:483] Algo bellman_ford step 4925 current loss 0.249207, current_train_items 157632.
I0302 19:00:03.674035 22895071117440 run.py:483] Algo bellman_ford step 4926 current loss 0.549614, current_train_items 157664.
I0302 19:00:03.696462 22895071117440 run.py:483] Algo bellman_ford step 4927 current loss 0.707927, current_train_items 157696.
I0302 19:00:03.723632 22895071117440 run.py:483] Algo bellman_ford step 4928 current loss 0.666968, current_train_items 157728.
I0302 19:00:03.754806 22895071117440 run.py:483] Algo bellman_ford step 4929 current loss 0.928395, current_train_items 157760.
I0302 19:00:03.772860 22895071117440 run.py:483] Algo bellman_ford step 4930 current loss 0.274339, current_train_items 157792.
I0302 19:00:03.787949 22895071117440 run.py:483] Algo bellman_ford step 4931 current loss 0.559932, current_train_items 157824.
I0302 19:00:03.809854 22895071117440 run.py:483] Algo bellman_ford step 4932 current loss 0.661485, current_train_items 157856.
I0302 19:00:03.837535 22895071117440 run.py:483] Algo bellman_ford step 4933 current loss 0.617511, current_train_items 157888.
I0302 19:00:03.868541 22895071117440 run.py:483] Algo bellman_ford step 4934 current loss 0.841032, current_train_items 157920.
I0302 19:00:03.886842 22895071117440 run.py:483] Algo bellman_ford step 4935 current loss 0.403081, current_train_items 157952.
I0302 19:00:03.902221 22895071117440 run.py:483] Algo bellman_ford step 4936 current loss 0.472941, current_train_items 157984.
I0302 19:00:03.925514 22895071117440 run.py:483] Algo bellman_ford step 4937 current loss 0.718471, current_train_items 158016.
I0302 19:00:03.954017 22895071117440 run.py:483] Algo bellman_ford step 4938 current loss 0.716408, current_train_items 158048.
I0302 19:00:03.986889 22895071117440 run.py:483] Algo bellman_ford step 4939 current loss 0.922664, current_train_items 158080.
I0302 19:00:04.004779 22895071117440 run.py:483] Algo bellman_ford step 4940 current loss 0.298426, current_train_items 158112.
I0302 19:00:04.019776 22895071117440 run.py:483] Algo bellman_ford step 4941 current loss 0.483947, current_train_items 158144.
I0302 19:00:04.041548 22895071117440 run.py:483] Algo bellman_ford step 4942 current loss 0.607862, current_train_items 158176.
I0302 19:00:04.070936 22895071117440 run.py:483] Algo bellman_ford step 4943 current loss 0.805115, current_train_items 158208.
I0302 19:00:04.100373 22895071117440 run.py:483] Algo bellman_ford step 4944 current loss 0.751646, current_train_items 158240.
I0302 19:00:04.118375 22895071117440 run.py:483] Algo bellman_ford step 4945 current loss 0.402850, current_train_items 158272.
I0302 19:00:04.133740 22895071117440 run.py:483] Algo bellman_ford step 4946 current loss 0.543951, current_train_items 158304.
I0302 19:00:04.155158 22895071117440 run.py:483] Algo bellman_ford step 4947 current loss 0.607619, current_train_items 158336.
I0302 19:00:04.183789 22895071117440 run.py:483] Algo bellman_ford step 4948 current loss 0.858259, current_train_items 158368.
I0302 19:00:04.215929 22895071117440 run.py:483] Algo bellman_ford step 4949 current loss 0.805357, current_train_items 158400.
I0302 19:00:04.233954 22895071117440 run.py:483] Algo bellman_ford step 4950 current loss 0.294090, current_train_items 158432.
I0302 19:00:04.241932 22895071117440 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:04.242037 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:00:04.257808 22895071117440 run.py:483] Algo bellman_ford step 4951 current loss 0.434866, current_train_items 158464.
I0302 19:00:04.279611 22895071117440 run.py:483] Algo bellman_ford step 4952 current loss 0.577756, current_train_items 158496.
I0302 19:00:04.308668 22895071117440 run.py:483] Algo bellman_ford step 4953 current loss 0.600857, current_train_items 158528.
I0302 19:00:04.341943 22895071117440 run.py:483] Algo bellman_ford step 4954 current loss 0.752724, current_train_items 158560.
I0302 19:00:04.360492 22895071117440 run.py:483] Algo bellman_ford step 4955 current loss 0.331703, current_train_items 158592.
I0302 19:00:04.376043 22895071117440 run.py:483] Algo bellman_ford step 4956 current loss 0.566520, current_train_items 158624.
I0302 19:00:04.398811 22895071117440 run.py:483] Algo bellman_ford step 4957 current loss 0.685580, current_train_items 158656.
I0302 19:00:04.427715 22895071117440 run.py:483] Algo bellman_ford step 4958 current loss 0.772196, current_train_items 158688.
I0302 19:00:04.456223 22895071117440 run.py:483] Algo bellman_ford step 4959 current loss 0.834468, current_train_items 158720.
I0302 19:00:04.474468 22895071117440 run.py:483] Algo bellman_ford step 4960 current loss 0.263649, current_train_items 158752.
I0302 19:00:04.490239 22895071117440 run.py:483] Algo bellman_ford step 4961 current loss 0.439052, current_train_items 158784.
I0302 19:00:04.511732 22895071117440 run.py:483] Algo bellman_ford step 4962 current loss 0.620905, current_train_items 158816.
I0302 19:00:04.539264 22895071117440 run.py:483] Algo bellman_ford step 4963 current loss 0.672626, current_train_items 158848.
I0302 19:00:04.570392 22895071117440 run.py:483] Algo bellman_ford step 4964 current loss 0.842352, current_train_items 158880.
I0302 19:00:04.588234 22895071117440 run.py:483] Algo bellman_ford step 4965 current loss 0.282665, current_train_items 158912.
I0302 19:00:04.603604 22895071117440 run.py:483] Algo bellman_ford step 4966 current loss 0.460524, current_train_items 158944.
I0302 19:00:04.626353 22895071117440 run.py:483] Algo bellman_ford step 4967 current loss 0.729909, current_train_items 158976.
I0302 19:00:04.655776 22895071117440 run.py:483] Algo bellman_ford step 4968 current loss 0.730937, current_train_items 159008.
I0302 19:00:04.684905 22895071117440 run.py:483] Algo bellman_ford step 4969 current loss 0.731779, current_train_items 159040.
I0302 19:00:04.703312 22895071117440 run.py:483] Algo bellman_ford step 4970 current loss 0.360096, current_train_items 159072.
I0302 19:00:04.718858 22895071117440 run.py:483] Algo bellman_ford step 4971 current loss 0.493336, current_train_items 159104.
I0302 19:00:04.741016 22895071117440 run.py:483] Algo bellman_ford step 4972 current loss 0.649648, current_train_items 159136.
I0302 19:00:04.770316 22895071117440 run.py:483] Algo bellman_ford step 4973 current loss 0.789181, current_train_items 159168.
I0302 19:00:04.800992 22895071117440 run.py:483] Algo bellman_ford step 4974 current loss 0.817324, current_train_items 159200.
I0302 19:00:04.819255 22895071117440 run.py:483] Algo bellman_ford step 4975 current loss 0.410253, current_train_items 159232.
I0302 19:00:04.835365 22895071117440 run.py:483] Algo bellman_ford step 4976 current loss 0.435416, current_train_items 159264.
I0302 19:00:04.857775 22895071117440 run.py:483] Algo bellman_ford step 4977 current loss 0.670030, current_train_items 159296.
I0302 19:00:04.886836 22895071117440 run.py:483] Algo bellman_ford step 4978 current loss 0.724866, current_train_items 159328.
I0302 19:00:04.918698 22895071117440 run.py:483] Algo bellman_ford step 4979 current loss 0.861476, current_train_items 159360.
I0302 19:00:04.936723 22895071117440 run.py:483] Algo bellman_ford step 4980 current loss 0.294339, current_train_items 159392.
I0302 19:00:04.952243 22895071117440 run.py:483] Algo bellman_ford step 4981 current loss 0.460354, current_train_items 159424.
I0302 19:00:04.975073 22895071117440 run.py:483] Algo bellman_ford step 4982 current loss 0.635436, current_train_items 159456.
I0302 19:00:05.001327 22895071117440 run.py:483] Algo bellman_ford step 4983 current loss 0.708051, current_train_items 159488.
I0302 19:00:05.031737 22895071117440 run.py:483] Algo bellman_ford step 4984 current loss 0.793418, current_train_items 159520.
I0302 19:00:05.050545 22895071117440 run.py:483] Algo bellman_ford step 4985 current loss 0.332065, current_train_items 159552.
I0302 19:00:05.066424 22895071117440 run.py:483] Algo bellman_ford step 4986 current loss 0.488524, current_train_items 159584.
I0302 19:00:05.086910 22895071117440 run.py:483] Algo bellman_ford step 4987 current loss 0.618619, current_train_items 159616.
I0302 19:00:05.114260 22895071117440 run.py:483] Algo bellman_ford step 4988 current loss 0.702176, current_train_items 159648.
I0302 19:00:05.147782 22895071117440 run.py:483] Algo bellman_ford step 4989 current loss 0.998626, current_train_items 159680.
I0302 19:00:05.165996 22895071117440 run.py:483] Algo bellman_ford step 4990 current loss 0.363491, current_train_items 159712.
I0302 19:00:05.181431 22895071117440 run.py:483] Algo bellman_ford step 4991 current loss 0.450392, current_train_items 159744.
I0302 19:00:05.203513 22895071117440 run.py:483] Algo bellman_ford step 4992 current loss 0.626121, current_train_items 159776.
I0302 19:00:05.231287 22895071117440 run.py:483] Algo bellman_ford step 4993 current loss 0.709186, current_train_items 159808.
I0302 19:00:05.262766 22895071117440 run.py:483] Algo bellman_ford step 4994 current loss 0.819665, current_train_items 159840.
I0302 19:00:05.280608 22895071117440 run.py:483] Algo bellman_ford step 4995 current loss 0.273368, current_train_items 159872.
I0302 19:00:05.296242 22895071117440 run.py:483] Algo bellman_ford step 4996 current loss 0.474730, current_train_items 159904.
I0302 19:00:05.317667 22895071117440 run.py:483] Algo bellman_ford step 4997 current loss 0.752631, current_train_items 159936.
I0302 19:00:05.346179 22895071117440 run.py:483] Algo bellman_ford step 4998 current loss 0.854137, current_train_items 159968.
I0302 19:00:05.377739 22895071117440 run.py:483] Algo bellman_ford step 4999 current loss 1.013655, current_train_items 160000.
I0302 19:00:05.396424 22895071117440 run.py:483] Algo bellman_ford step 5000 current loss 0.403274, current_train_items 160032.
I0302 19:00:05.404250 22895071117440 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:05.404356 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:00:05.420479 22895071117440 run.py:483] Algo bellman_ford step 5001 current loss 0.571719, current_train_items 160064.
I0302 19:00:05.442464 22895071117440 run.py:483] Algo bellman_ford step 5002 current loss 0.712202, current_train_items 160096.
I0302 19:00:05.469804 22895071117440 run.py:483] Algo bellman_ford step 5003 current loss 0.662149, current_train_items 160128.
I0302 19:00:05.500333 22895071117440 run.py:483] Algo bellman_ford step 5004 current loss 0.835204, current_train_items 160160.
I0302 19:00:05.519018 22895071117440 run.py:483] Algo bellman_ford step 5005 current loss 0.368543, current_train_items 160192.
I0302 19:00:05.534480 22895071117440 run.py:483] Algo bellman_ford step 5006 current loss 0.525214, current_train_items 160224.
I0302 19:00:05.556854 22895071117440 run.py:483] Algo bellman_ford step 5007 current loss 0.766642, current_train_items 160256.
I0302 19:00:05.587399 22895071117440 run.py:483] Algo bellman_ford step 5008 current loss 0.698791, current_train_items 160288.
I0302 19:00:05.616584 22895071117440 run.py:483] Algo bellman_ford step 5009 current loss 0.846399, current_train_items 160320.
I0302 19:00:05.634301 22895071117440 run.py:483] Algo bellman_ford step 5010 current loss 0.337815, current_train_items 160352.
I0302 19:00:05.649538 22895071117440 run.py:483] Algo bellman_ford step 5011 current loss 0.501765, current_train_items 160384.
I0302 19:00:05.671644 22895071117440 run.py:483] Algo bellman_ford step 5012 current loss 0.659983, current_train_items 160416.
I0302 19:00:05.699510 22895071117440 run.py:483] Algo bellman_ford step 5013 current loss 0.825110, current_train_items 160448.
I0302 19:00:05.731630 22895071117440 run.py:483] Algo bellman_ford step 5014 current loss 0.897032, current_train_items 160480.
I0302 19:00:05.749550 22895071117440 run.py:483] Algo bellman_ford step 5015 current loss 0.457178, current_train_items 160512.
I0302 19:00:05.765560 22895071117440 run.py:483] Algo bellman_ford step 5016 current loss 0.532264, current_train_items 160544.
I0302 19:00:05.786882 22895071117440 run.py:483] Algo bellman_ford step 5017 current loss 0.701815, current_train_items 160576.
I0302 19:00:05.815554 22895071117440 run.py:483] Algo bellman_ford step 5018 current loss 0.725269, current_train_items 160608.
I0302 19:00:05.848296 22895071117440 run.py:483] Algo bellman_ford step 5019 current loss 0.851967, current_train_items 160640.
I0302 19:00:05.866157 22895071117440 run.py:483] Algo bellman_ford step 5020 current loss 0.309608, current_train_items 160672.
I0302 19:00:05.881276 22895071117440 run.py:483] Algo bellman_ford step 5021 current loss 0.506042, current_train_items 160704.
I0302 19:00:05.904037 22895071117440 run.py:483] Algo bellman_ford step 5022 current loss 0.684541, current_train_items 160736.
I0302 19:00:05.932509 22895071117440 run.py:483] Algo bellman_ford step 5023 current loss 0.749158, current_train_items 160768.
I0302 19:00:05.963754 22895071117440 run.py:483] Algo bellman_ford step 5024 current loss 0.818819, current_train_items 160800.
I0302 19:00:05.981784 22895071117440 run.py:483] Algo bellman_ford step 5025 current loss 0.301203, current_train_items 160832.
I0302 19:00:05.996690 22895071117440 run.py:483] Algo bellman_ford step 5026 current loss 0.436789, current_train_items 160864.
I0302 19:00:06.018055 22895071117440 run.py:483] Algo bellman_ford step 5027 current loss 0.643256, current_train_items 160896.
I0302 19:00:06.045104 22895071117440 run.py:483] Algo bellman_ford step 5028 current loss 0.640411, current_train_items 160928.
I0302 19:00:06.075772 22895071117440 run.py:483] Algo bellman_ford step 5029 current loss 0.757422, current_train_items 160960.
I0302 19:00:06.094357 22895071117440 run.py:483] Algo bellman_ford step 5030 current loss 0.250778, current_train_items 160992.
I0302 19:00:06.109760 22895071117440 run.py:483] Algo bellman_ford step 5031 current loss 0.484182, current_train_items 161024.
I0302 19:00:06.132126 22895071117440 run.py:483] Algo bellman_ford step 5032 current loss 0.590919, current_train_items 161056.
I0302 19:00:06.159486 22895071117440 run.py:483] Algo bellman_ford step 5033 current loss 0.733597, current_train_items 161088.
I0302 19:00:06.191015 22895071117440 run.py:483] Algo bellman_ford step 5034 current loss 0.913704, current_train_items 161120.
I0302 19:00:06.209483 22895071117440 run.py:483] Algo bellman_ford step 5035 current loss 0.295430, current_train_items 161152.
I0302 19:00:06.225036 22895071117440 run.py:483] Algo bellman_ford step 5036 current loss 0.493238, current_train_items 161184.
I0302 19:00:06.246736 22895071117440 run.py:483] Algo bellman_ford step 5037 current loss 0.642137, current_train_items 161216.
I0302 19:00:06.276405 22895071117440 run.py:483] Algo bellman_ford step 5038 current loss 0.758447, current_train_items 161248.
I0302 19:00:06.307525 22895071117440 run.py:483] Algo bellman_ford step 5039 current loss 0.861332, current_train_items 161280.
I0302 19:00:06.325330 22895071117440 run.py:483] Algo bellman_ford step 5040 current loss 0.307720, current_train_items 161312.
I0302 19:00:06.340169 22895071117440 run.py:483] Algo bellman_ford step 5041 current loss 0.451914, current_train_items 161344.
I0302 19:00:06.362715 22895071117440 run.py:483] Algo bellman_ford step 5042 current loss 0.716406, current_train_items 161376.
I0302 19:00:06.390122 22895071117440 run.py:483] Algo bellman_ford step 5043 current loss 0.701839, current_train_items 161408.
I0302 19:00:06.420624 22895071117440 run.py:483] Algo bellman_ford step 5044 current loss 0.826438, current_train_items 161440.
I0302 19:00:06.438711 22895071117440 run.py:483] Algo bellman_ford step 5045 current loss 0.266693, current_train_items 161472.
I0302 19:00:06.454065 22895071117440 run.py:483] Algo bellman_ford step 5046 current loss 0.480937, current_train_items 161504.
I0302 19:00:06.474787 22895071117440 run.py:483] Algo bellman_ford step 5047 current loss 0.631514, current_train_items 161536.
I0302 19:00:06.503236 22895071117440 run.py:483] Algo bellman_ford step 5048 current loss 0.785299, current_train_items 161568.
I0302 19:00:06.534406 22895071117440 run.py:483] Algo bellman_ford step 5049 current loss 0.829126, current_train_items 161600.
I0302 19:00:06.552323 22895071117440 run.py:483] Algo bellman_ford step 5050 current loss 0.374612, current_train_items 161632.
I0302 19:00:06.560501 22895071117440 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:06.560605 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:06.576399 22895071117440 run.py:483] Algo bellman_ford step 5051 current loss 0.524483, current_train_items 161664.
I0302 19:00:06.599242 22895071117440 run.py:483] Algo bellman_ford step 5052 current loss 0.704200, current_train_items 161696.
I0302 19:00:06.628230 22895071117440 run.py:483] Algo bellman_ford step 5053 current loss 0.702051, current_train_items 161728.
I0302 19:00:06.658142 22895071117440 run.py:483] Algo bellman_ford step 5054 current loss 0.770325, current_train_items 161760.
I0302 19:00:06.676820 22895071117440 run.py:483] Algo bellman_ford step 5055 current loss 0.456795, current_train_items 161792.
I0302 19:00:06.693209 22895071117440 run.py:483] Algo bellman_ford step 5056 current loss 0.528762, current_train_items 161824.
I0302 19:00:06.716673 22895071117440 run.py:483] Algo bellman_ford step 5057 current loss 0.658835, current_train_items 161856.
I0302 19:00:06.744699 22895071117440 run.py:483] Algo bellman_ford step 5058 current loss 0.757540, current_train_items 161888.
I0302 19:00:06.775920 22895071117440 run.py:483] Algo bellman_ford step 5059 current loss 0.945751, current_train_items 161920.
I0302 19:00:06.794171 22895071117440 run.py:483] Algo bellman_ford step 5060 current loss 0.289824, current_train_items 161952.
I0302 19:00:06.809769 22895071117440 run.py:483] Algo bellman_ford step 5061 current loss 0.451385, current_train_items 161984.
I0302 19:00:06.830229 22895071117440 run.py:483] Algo bellman_ford step 5062 current loss 0.680060, current_train_items 162016.
I0302 19:00:06.858322 22895071117440 run.py:483] Algo bellman_ford step 5063 current loss 0.840250, current_train_items 162048.
I0302 19:00:06.887517 22895071117440 run.py:483] Algo bellman_ford step 5064 current loss 0.821253, current_train_items 162080.
I0302 19:00:06.905426 22895071117440 run.py:483] Algo bellman_ford step 5065 current loss 0.368274, current_train_items 162112.
I0302 19:00:06.920906 22895071117440 run.py:483] Algo bellman_ford step 5066 current loss 0.536666, current_train_items 162144.
I0302 19:00:06.943907 22895071117440 run.py:483] Algo bellman_ford step 5067 current loss 0.650867, current_train_items 162176.
I0302 19:00:06.971960 22895071117440 run.py:483] Algo bellman_ford step 5068 current loss 0.662553, current_train_items 162208.
I0302 19:00:07.003749 22895071117440 run.py:483] Algo bellman_ford step 5069 current loss 0.925658, current_train_items 162240.
I0302 19:00:07.022367 22895071117440 run.py:483] Algo bellman_ford step 5070 current loss 0.302130, current_train_items 162272.
I0302 19:00:07.038722 22895071117440 run.py:483] Algo bellman_ford step 5071 current loss 0.514050, current_train_items 162304.
I0302 19:00:07.059031 22895071117440 run.py:483] Algo bellman_ford step 5072 current loss 0.768271, current_train_items 162336.
I0302 19:00:07.088180 22895071117440 run.py:483] Algo bellman_ford step 5073 current loss 0.913309, current_train_items 162368.
I0302 19:00:07.116628 22895071117440 run.py:483] Algo bellman_ford step 5074 current loss 0.856108, current_train_items 162400.
I0302 19:00:07.134817 22895071117440 run.py:483] Algo bellman_ford step 5075 current loss 0.335480, current_train_items 162432.
I0302 19:00:07.150336 22895071117440 run.py:483] Algo bellman_ford step 5076 current loss 0.469730, current_train_items 162464.
I0302 19:00:07.171946 22895071117440 run.py:483] Algo bellman_ford step 5077 current loss 0.688177, current_train_items 162496.
I0302 19:00:07.200206 22895071117440 run.py:483] Algo bellman_ford step 5078 current loss 0.907259, current_train_items 162528.
I0302 19:00:07.229768 22895071117440 run.py:483] Algo bellman_ford step 5079 current loss 0.987366, current_train_items 162560.
I0302 19:00:07.247801 22895071117440 run.py:483] Algo bellman_ford step 5080 current loss 0.324587, current_train_items 162592.
I0302 19:00:07.262782 22895071117440 run.py:483] Algo bellman_ford step 5081 current loss 0.423645, current_train_items 162624.
I0302 19:00:07.284399 22895071117440 run.py:483] Algo bellman_ford step 5082 current loss 0.636443, current_train_items 162656.
I0302 19:00:07.313157 22895071117440 run.py:483] Algo bellman_ford step 5083 current loss 0.738542, current_train_items 162688.
I0302 19:00:07.344151 22895071117440 run.py:483] Algo bellman_ford step 5084 current loss 0.874238, current_train_items 162720.
I0302 19:00:07.363040 22895071117440 run.py:483] Algo bellman_ford step 5085 current loss 0.314047, current_train_items 162752.
I0302 19:00:07.378564 22895071117440 run.py:483] Algo bellman_ford step 5086 current loss 0.481173, current_train_items 162784.
I0302 19:00:07.400319 22895071117440 run.py:483] Algo bellman_ford step 5087 current loss 0.712672, current_train_items 162816.
I0302 19:00:07.429079 22895071117440 run.py:483] Algo bellman_ford step 5088 current loss 0.617390, current_train_items 162848.
I0302 19:00:07.460855 22895071117440 run.py:483] Algo bellman_ford step 5089 current loss 0.949677, current_train_items 162880.
I0302 19:00:07.479151 22895071117440 run.py:483] Algo bellman_ford step 5090 current loss 0.277520, current_train_items 162912.
I0302 19:00:07.494953 22895071117440 run.py:483] Algo bellman_ford step 5091 current loss 0.508913, current_train_items 162944.
I0302 19:00:07.515402 22895071117440 run.py:483] Algo bellman_ford step 5092 current loss 0.589453, current_train_items 162976.
I0302 19:00:07.542646 22895071117440 run.py:483] Algo bellman_ford step 5093 current loss 0.614776, current_train_items 163008.
I0302 19:00:07.572700 22895071117440 run.py:483] Algo bellman_ford step 5094 current loss 0.732110, current_train_items 163040.
I0302 19:00:07.590816 22895071117440 run.py:483] Algo bellman_ford step 5095 current loss 0.291493, current_train_items 163072.
I0302 19:00:07.606221 22895071117440 run.py:483] Algo bellman_ford step 5096 current loss 0.535017, current_train_items 163104.
I0302 19:00:07.628786 22895071117440 run.py:483] Algo bellman_ford step 5097 current loss 0.710829, current_train_items 163136.
I0302 19:00:07.657865 22895071117440 run.py:483] Algo bellman_ford step 5098 current loss 0.756362, current_train_items 163168.
I0302 19:00:07.690242 22895071117440 run.py:483] Algo bellman_ford step 5099 current loss 0.912651, current_train_items 163200.
I0302 19:00:07.708549 22895071117440 run.py:483] Algo bellman_ford step 5100 current loss 0.246626, current_train_items 163232.
I0302 19:00:07.716159 22895071117440 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:07.716263 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:07.732102 22895071117440 run.py:483] Algo bellman_ford step 5101 current loss 0.561076, current_train_items 163264.
I0302 19:00:07.752392 22895071117440 run.py:483] Algo bellman_ford step 5102 current loss 0.565759, current_train_items 163296.
I0302 19:00:07.781834 22895071117440 run.py:483] Algo bellman_ford step 5103 current loss 0.747139, current_train_items 163328.
I0302 19:00:07.813810 22895071117440 run.py:483] Algo bellman_ford step 5104 current loss 0.858623, current_train_items 163360.
I0302 19:00:07.832486 22895071117440 run.py:483] Algo bellman_ford step 5105 current loss 0.260908, current_train_items 163392.
I0302 19:00:07.848230 22895071117440 run.py:483] Algo bellman_ford step 5106 current loss 0.534451, current_train_items 163424.
I0302 19:00:07.870331 22895071117440 run.py:483] Algo bellman_ford step 5107 current loss 0.595844, current_train_items 163456.
I0302 19:00:07.897626 22895071117440 run.py:483] Algo bellman_ford step 5108 current loss 0.669585, current_train_items 163488.
I0302 19:00:07.928828 22895071117440 run.py:483] Algo bellman_ford step 5109 current loss 0.891542, current_train_items 163520.
I0302 19:00:07.946977 22895071117440 run.py:483] Algo bellman_ford step 5110 current loss 0.320101, current_train_items 163552.
I0302 19:00:07.962398 22895071117440 run.py:483] Algo bellman_ford step 5111 current loss 0.521715, current_train_items 163584.
I0302 19:00:07.983747 22895071117440 run.py:483] Algo bellman_ford step 5112 current loss 0.582608, current_train_items 163616.
I0302 19:00:08.011236 22895071117440 run.py:483] Algo bellman_ford step 5113 current loss 0.649788, current_train_items 163648.
I0302 19:00:08.041339 22895071117440 run.py:483] Algo bellman_ford step 5114 current loss 0.739796, current_train_items 163680.
I0302 19:00:08.059141 22895071117440 run.py:483] Algo bellman_ford step 5115 current loss 0.327272, current_train_items 163712.
I0302 19:00:08.074690 22895071117440 run.py:483] Algo bellman_ford step 5116 current loss 0.457604, current_train_items 163744.
I0302 19:00:08.097294 22895071117440 run.py:483] Algo bellman_ford step 5117 current loss 0.674012, current_train_items 163776.
I0302 19:00:08.124928 22895071117440 run.py:483] Algo bellman_ford step 5118 current loss 0.709366, current_train_items 163808.
I0302 19:00:08.156658 22895071117440 run.py:483] Algo bellman_ford step 5119 current loss 0.784597, current_train_items 163840.
I0302 19:00:08.175115 22895071117440 run.py:483] Algo bellman_ford step 5120 current loss 0.322409, current_train_items 163872.
I0302 19:00:08.190097 22895071117440 run.py:483] Algo bellman_ford step 5121 current loss 0.465644, current_train_items 163904.
I0302 19:00:08.211707 22895071117440 run.py:483] Algo bellman_ford step 5122 current loss 0.685395, current_train_items 163936.
I0302 19:00:08.238994 22895071117440 run.py:483] Algo bellman_ford step 5123 current loss 0.641910, current_train_items 163968.
I0302 19:00:08.269859 22895071117440 run.py:483] Algo bellman_ford step 5124 current loss 0.776820, current_train_items 164000.
I0302 19:00:08.287674 22895071117440 run.py:483] Algo bellman_ford step 5125 current loss 0.314255, current_train_items 164032.
I0302 19:00:08.302921 22895071117440 run.py:483] Algo bellman_ford step 5126 current loss 0.464598, current_train_items 164064.
I0302 19:00:08.324584 22895071117440 run.py:483] Algo bellman_ford step 5127 current loss 0.791883, current_train_items 164096.
I0302 19:00:08.352889 22895071117440 run.py:483] Algo bellman_ford step 5128 current loss 0.811842, current_train_items 164128.
I0302 19:00:08.384681 22895071117440 run.py:483] Algo bellman_ford step 5129 current loss 0.902248, current_train_items 164160.
I0302 19:00:08.402300 22895071117440 run.py:483] Algo bellman_ford step 5130 current loss 0.412260, current_train_items 164192.
I0302 19:00:08.417791 22895071117440 run.py:483] Algo bellman_ford step 5131 current loss 0.551368, current_train_items 164224.
I0302 19:00:08.438694 22895071117440 run.py:483] Algo bellman_ford step 5132 current loss 0.936892, current_train_items 164256.
I0302 19:00:08.467537 22895071117440 run.py:483] Algo bellman_ford step 5133 current loss 0.938444, current_train_items 164288.
I0302 19:00:08.497982 22895071117440 run.py:483] Algo bellman_ford step 5134 current loss 1.060807, current_train_items 164320.
I0302 19:00:08.515990 22895071117440 run.py:483] Algo bellman_ford step 5135 current loss 0.416331, current_train_items 164352.
I0302 19:00:08.531223 22895071117440 run.py:483] Algo bellman_ford step 5136 current loss 0.551840, current_train_items 164384.
I0302 19:00:08.553136 22895071117440 run.py:483] Algo bellman_ford step 5137 current loss 0.746895, current_train_items 164416.
I0302 19:00:08.581073 22895071117440 run.py:483] Algo bellman_ford step 5138 current loss 0.823655, current_train_items 164448.
I0302 19:00:08.612489 22895071117440 run.py:483] Algo bellman_ford step 5139 current loss 0.810646, current_train_items 164480.
I0302 19:00:08.630399 22895071117440 run.py:483] Algo bellman_ford step 5140 current loss 0.332532, current_train_items 164512.
I0302 19:00:08.646254 22895071117440 run.py:483] Algo bellman_ford step 5141 current loss 0.599327, current_train_items 164544.
I0302 19:00:08.668695 22895071117440 run.py:483] Algo bellman_ford step 5142 current loss 0.829622, current_train_items 164576.
I0302 19:00:08.697156 22895071117440 run.py:483] Algo bellman_ford step 5143 current loss 0.886767, current_train_items 164608.
I0302 19:00:08.728016 22895071117440 run.py:483] Algo bellman_ford step 5144 current loss 1.133004, current_train_items 164640.
I0302 19:00:08.746260 22895071117440 run.py:483] Algo bellman_ford step 5145 current loss 0.327153, current_train_items 164672.
I0302 19:00:08.761429 22895071117440 run.py:483] Algo bellman_ford step 5146 current loss 0.632604, current_train_items 164704.
I0302 19:00:08.781979 22895071117440 run.py:483] Algo bellman_ford step 5147 current loss 0.765354, current_train_items 164736.
I0302 19:00:08.809589 22895071117440 run.py:483] Algo bellman_ford step 5148 current loss 0.784405, current_train_items 164768.
I0302 19:00:08.839464 22895071117440 run.py:483] Algo bellman_ford step 5149 current loss 0.893394, current_train_items 164800.
I0302 19:00:08.857464 22895071117440 run.py:483] Algo bellman_ford step 5150 current loss 0.328429, current_train_items 164832.
I0302 19:00:08.865534 22895071117440 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:08.865638 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:00:08.881616 22895071117440 run.py:483] Algo bellman_ford step 5151 current loss 0.528812, current_train_items 164864.
I0302 19:00:08.903637 22895071117440 run.py:483] Algo bellman_ford step 5152 current loss 0.679498, current_train_items 164896.
I0302 19:00:08.930984 22895071117440 run.py:483] Algo bellman_ford step 5153 current loss 0.815167, current_train_items 164928.
I0302 19:00:08.961651 22895071117440 run.py:483] Algo bellman_ford step 5154 current loss 0.891135, current_train_items 164960.
I0302 19:00:08.980427 22895071117440 run.py:483] Algo bellman_ford step 5155 current loss 0.349798, current_train_items 164992.
I0302 19:00:08.995767 22895071117440 run.py:483] Algo bellman_ford step 5156 current loss 0.505938, current_train_items 165024.
I0302 19:00:09.018337 22895071117440 run.py:483] Algo bellman_ford step 5157 current loss 0.672058, current_train_items 165056.
I0302 19:00:09.048336 22895071117440 run.py:483] Algo bellman_ford step 5158 current loss 0.959268, current_train_items 165088.
I0302 19:00:09.078729 22895071117440 run.py:483] Algo bellman_ford step 5159 current loss 0.846879, current_train_items 165120.
I0302 19:00:09.097279 22895071117440 run.py:483] Algo bellman_ford step 5160 current loss 0.294280, current_train_items 165152.
I0302 19:00:09.112839 22895071117440 run.py:483] Algo bellman_ford step 5161 current loss 0.483919, current_train_items 165184.
I0302 19:00:09.135464 22895071117440 run.py:483] Algo bellman_ford step 5162 current loss 0.741948, current_train_items 165216.
I0302 19:00:09.164978 22895071117440 run.py:483] Algo bellman_ford step 5163 current loss 0.783369, current_train_items 165248.
I0302 19:00:09.194581 22895071117440 run.py:483] Algo bellman_ford step 5164 current loss 0.726005, current_train_items 165280.
I0302 19:00:09.212283 22895071117440 run.py:483] Algo bellman_ford step 5165 current loss 0.294612, current_train_items 165312.
I0302 19:00:09.227394 22895071117440 run.py:483] Algo bellman_ford step 5166 current loss 0.436874, current_train_items 165344.
I0302 19:00:09.248161 22895071117440 run.py:483] Algo bellman_ford step 5167 current loss 0.698863, current_train_items 165376.
I0302 19:00:09.276687 22895071117440 run.py:483] Algo bellman_ford step 5168 current loss 0.720344, current_train_items 165408.
I0302 19:00:09.308706 22895071117440 run.py:483] Algo bellman_ford step 5169 current loss 0.756176, current_train_items 165440.
I0302 19:00:09.327025 22895071117440 run.py:483] Algo bellman_ford step 5170 current loss 0.345060, current_train_items 165472.
I0302 19:00:09.342770 22895071117440 run.py:483] Algo bellman_ford step 5171 current loss 0.460744, current_train_items 165504.
I0302 19:00:09.364466 22895071117440 run.py:483] Algo bellman_ford step 5172 current loss 0.640689, current_train_items 165536.
I0302 19:00:09.393597 22895071117440 run.py:483] Algo bellman_ford step 5173 current loss 0.781003, current_train_items 165568.
I0302 19:00:09.423710 22895071117440 run.py:483] Algo bellman_ford step 5174 current loss 0.765585, current_train_items 165600.
I0302 19:00:09.441801 22895071117440 run.py:483] Algo bellman_ford step 5175 current loss 0.232744, current_train_items 165632.
I0302 19:00:09.457510 22895071117440 run.py:483] Algo bellman_ford step 5176 current loss 0.403912, current_train_items 165664.
I0302 19:00:09.479597 22895071117440 run.py:483] Algo bellman_ford step 5177 current loss 0.595339, current_train_items 165696.
I0302 19:00:09.507714 22895071117440 run.py:483] Algo bellman_ford step 5178 current loss 0.633628, current_train_items 165728.
I0302 19:00:09.537092 22895071117440 run.py:483] Algo bellman_ford step 5179 current loss 0.812935, current_train_items 165760.
I0302 19:00:09.554858 22895071117440 run.py:483] Algo bellman_ford step 5180 current loss 0.321447, current_train_items 165792.
I0302 19:00:09.570107 22895071117440 run.py:483] Algo bellman_ford step 5181 current loss 0.459990, current_train_items 165824.
I0302 19:00:09.591872 22895071117440 run.py:483] Algo bellman_ford step 5182 current loss 0.612401, current_train_items 165856.
I0302 19:00:09.620116 22895071117440 run.py:483] Algo bellman_ford step 5183 current loss 0.752153, current_train_items 165888.
I0302 19:00:09.649509 22895071117440 run.py:483] Algo bellman_ford step 5184 current loss 0.752739, current_train_items 165920.
I0302 19:00:09.668216 22895071117440 run.py:483] Algo bellman_ford step 5185 current loss 0.402369, current_train_items 165952.
I0302 19:00:09.684228 22895071117440 run.py:483] Algo bellman_ford step 5186 current loss 0.492969, current_train_items 165984.
I0302 19:00:09.705000 22895071117440 run.py:483] Algo bellman_ford step 5187 current loss 0.580472, current_train_items 166016.
I0302 19:00:09.734010 22895071117440 run.py:483] Algo bellman_ford step 5188 current loss 0.742057, current_train_items 166048.
I0302 19:00:09.763809 22895071117440 run.py:483] Algo bellman_ford step 5189 current loss 0.887280, current_train_items 166080.
I0302 19:00:09.782520 22895071117440 run.py:483] Algo bellman_ford step 5190 current loss 0.336480, current_train_items 166112.
I0302 19:00:09.798045 22895071117440 run.py:483] Algo bellman_ford step 5191 current loss 0.513949, current_train_items 166144.
I0302 19:00:09.819918 22895071117440 run.py:483] Algo bellman_ford step 5192 current loss 0.748360, current_train_items 166176.
I0302 19:00:09.847546 22895071117440 run.py:483] Algo bellman_ford step 5193 current loss 0.730200, current_train_items 166208.
I0302 19:00:09.878168 22895071117440 run.py:483] Algo bellman_ford step 5194 current loss 0.852853, current_train_items 166240.
I0302 19:00:09.896373 22895071117440 run.py:483] Algo bellman_ford step 5195 current loss 0.302266, current_train_items 166272.
I0302 19:00:09.911723 22895071117440 run.py:483] Algo bellman_ford step 5196 current loss 0.572983, current_train_items 166304.
I0302 19:00:09.933758 22895071117440 run.py:483] Algo bellman_ford step 5197 current loss 0.769892, current_train_items 166336.
I0302 19:00:09.963459 22895071117440 run.py:483] Algo bellman_ford step 5198 current loss 0.896648, current_train_items 166368.
I0302 19:00:09.993333 22895071117440 run.py:483] Algo bellman_ford step 5199 current loss 0.949458, current_train_items 166400.
I0302 19:00:10.011943 22895071117440 run.py:483] Algo bellman_ford step 5200 current loss 0.344954, current_train_items 166432.
I0302 19:00:10.019638 22895071117440 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:10.019742 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:10.035938 22895071117440 run.py:483] Algo bellman_ford step 5201 current loss 0.531073, current_train_items 166464.
I0302 19:00:10.059066 22895071117440 run.py:483] Algo bellman_ford step 5202 current loss 0.679392, current_train_items 166496.
I0302 19:00:10.088941 22895071117440 run.py:483] Algo bellman_ford step 5203 current loss 0.772874, current_train_items 166528.
I0302 19:00:10.121673 22895071117440 run.py:483] Algo bellman_ford step 5204 current loss 0.827880, current_train_items 166560.
I0302 19:00:10.140071 22895071117440 run.py:483] Algo bellman_ford step 5205 current loss 0.290975, current_train_items 166592.
I0302 19:00:10.155227 22895071117440 run.py:483] Algo bellman_ford step 5206 current loss 0.503471, current_train_items 166624.
I0302 19:00:10.177258 22895071117440 run.py:483] Algo bellman_ford step 5207 current loss 0.659974, current_train_items 166656.
I0302 19:00:10.204539 22895071117440 run.py:483] Algo bellman_ford step 5208 current loss 0.652611, current_train_items 166688.
I0302 19:00:10.238023 22895071117440 run.py:483] Algo bellman_ford step 5209 current loss 0.883252, current_train_items 166720.
I0302 19:00:10.256211 22895071117440 run.py:483] Algo bellman_ford step 5210 current loss 0.397978, current_train_items 166752.
I0302 19:00:10.271094 22895071117440 run.py:483] Algo bellman_ford step 5211 current loss 0.511218, current_train_items 166784.
I0302 19:00:10.293152 22895071117440 run.py:483] Algo bellman_ford step 5212 current loss 0.690673, current_train_items 166816.
I0302 19:00:10.323390 22895071117440 run.py:483] Algo bellman_ford step 5213 current loss 0.893279, current_train_items 166848.
I0302 19:00:10.353692 22895071117440 run.py:483] Algo bellman_ford step 5214 current loss 0.755265, current_train_items 166880.
I0302 19:00:10.371272 22895071117440 run.py:483] Algo bellman_ford step 5215 current loss 0.253122, current_train_items 166912.
I0302 19:00:10.386837 22895071117440 run.py:483] Algo bellman_ford step 5216 current loss 0.468489, current_train_items 166944.
I0302 19:00:10.408550 22895071117440 run.py:483] Algo bellman_ford step 5217 current loss 0.692970, current_train_items 166976.
I0302 19:00:10.436212 22895071117440 run.py:483] Algo bellman_ford step 5218 current loss 0.698818, current_train_items 167008.
I0302 19:00:10.467064 22895071117440 run.py:483] Algo bellman_ford step 5219 current loss 0.765596, current_train_items 167040.
I0302 19:00:10.485419 22895071117440 run.py:483] Algo bellman_ford step 5220 current loss 0.374899, current_train_items 167072.
I0302 19:00:10.500617 22895071117440 run.py:483] Algo bellman_ford step 5221 current loss 0.467106, current_train_items 167104.
I0302 19:00:10.521188 22895071117440 run.py:483] Algo bellman_ford step 5222 current loss 0.602188, current_train_items 167136.
I0302 19:00:10.550565 22895071117440 run.py:483] Algo bellman_ford step 5223 current loss 0.798069, current_train_items 167168.
I0302 19:00:10.580715 22895071117440 run.py:483] Algo bellman_ford step 5224 current loss 0.825386, current_train_items 167200.
I0302 19:00:10.598889 22895071117440 run.py:483] Algo bellman_ford step 5225 current loss 0.357662, current_train_items 167232.
I0302 19:00:10.614069 22895071117440 run.py:483] Algo bellman_ford step 5226 current loss 0.421456, current_train_items 167264.
I0302 19:00:10.636494 22895071117440 run.py:483] Algo bellman_ford step 5227 current loss 0.649310, current_train_items 167296.
I0302 19:00:10.665879 22895071117440 run.py:483] Algo bellman_ford step 5228 current loss 0.755930, current_train_items 167328.
I0302 19:00:10.697799 22895071117440 run.py:483] Algo bellman_ford step 5229 current loss 0.913792, current_train_items 167360.
I0302 19:00:10.715944 22895071117440 run.py:483] Algo bellman_ford step 5230 current loss 0.279463, current_train_items 167392.
I0302 19:00:10.731412 22895071117440 run.py:483] Algo bellman_ford step 5231 current loss 0.536283, current_train_items 167424.
I0302 19:00:10.753736 22895071117440 run.py:483] Algo bellman_ford step 5232 current loss 0.596108, current_train_items 167456.
I0302 19:00:10.781791 22895071117440 run.py:483] Algo bellman_ford step 5233 current loss 0.638038, current_train_items 167488.
I0302 19:00:10.814484 22895071117440 run.py:483] Algo bellman_ford step 5234 current loss 0.812689, current_train_items 167520.
I0302 19:00:10.832351 22895071117440 run.py:483] Algo bellman_ford step 5235 current loss 0.322352, current_train_items 167552.
I0302 19:00:10.847491 22895071117440 run.py:483] Algo bellman_ford step 5236 current loss 0.495917, current_train_items 167584.
I0302 19:00:10.869200 22895071117440 run.py:483] Algo bellman_ford step 5237 current loss 0.598392, current_train_items 167616.
I0302 19:00:10.899341 22895071117440 run.py:483] Algo bellman_ford step 5238 current loss 0.758655, current_train_items 167648.
I0302 19:00:10.930910 22895071117440 run.py:483] Algo bellman_ford step 5239 current loss 0.844910, current_train_items 167680.
I0302 19:00:10.949328 22895071117440 run.py:483] Algo bellman_ford step 5240 current loss 0.361181, current_train_items 167712.
I0302 19:00:10.964633 22895071117440 run.py:483] Algo bellman_ford step 5241 current loss 0.476384, current_train_items 167744.
I0302 19:00:10.986484 22895071117440 run.py:483] Algo bellman_ford step 5242 current loss 0.701442, current_train_items 167776.
I0302 19:00:11.014739 22895071117440 run.py:483] Algo bellman_ford step 5243 current loss 0.682735, current_train_items 167808.
I0302 19:00:11.043116 22895071117440 run.py:483] Algo bellman_ford step 5244 current loss 0.680220, current_train_items 167840.
I0302 19:00:11.061159 22895071117440 run.py:483] Algo bellman_ford step 5245 current loss 0.333031, current_train_items 167872.
I0302 19:00:11.076569 22895071117440 run.py:483] Algo bellman_ford step 5246 current loss 0.464794, current_train_items 167904.
I0302 19:00:11.099329 22895071117440 run.py:483] Algo bellman_ford step 5247 current loss 0.601687, current_train_items 167936.
I0302 19:00:11.128617 22895071117440 run.py:483] Algo bellman_ford step 5248 current loss 0.661826, current_train_items 167968.
I0302 19:00:11.160146 22895071117440 run.py:483] Algo bellman_ford step 5249 current loss 0.775308, current_train_items 168000.
I0302 19:00:11.177960 22895071117440 run.py:483] Algo bellman_ford step 5250 current loss 0.259429, current_train_items 168032.
I0302 19:00:11.185649 22895071117440 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:11.185756 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:00:11.201660 22895071117440 run.py:483] Algo bellman_ford step 5251 current loss 0.456577, current_train_items 168064.
I0302 19:00:11.224592 22895071117440 run.py:483] Algo bellman_ford step 5252 current loss 0.716231, current_train_items 168096.
I0302 19:00:11.254096 22895071117440 run.py:483] Algo bellman_ford step 5253 current loss 0.654869, current_train_items 168128.
I0302 19:00:11.283232 22895071117440 run.py:483] Algo bellman_ford step 5254 current loss 0.638089, current_train_items 168160.
I0302 19:00:11.301975 22895071117440 run.py:483] Algo bellman_ford step 5255 current loss 0.317552, current_train_items 168192.
I0302 19:00:11.317360 22895071117440 run.py:483] Algo bellman_ford step 5256 current loss 0.458566, current_train_items 168224.
I0302 19:00:11.338590 22895071117440 run.py:483] Algo bellman_ford step 5257 current loss 0.629626, current_train_items 168256.
I0302 19:00:11.367944 22895071117440 run.py:483] Algo bellman_ford step 5258 current loss 0.896296, current_train_items 168288.
I0302 19:00:11.396914 22895071117440 run.py:483] Algo bellman_ford step 5259 current loss 0.712995, current_train_items 168320.
I0302 19:00:11.415666 22895071117440 run.py:483] Algo bellman_ford step 5260 current loss 0.354794, current_train_items 168352.
I0302 19:00:11.431461 22895071117440 run.py:483] Algo bellman_ford step 5261 current loss 0.502927, current_train_items 168384.
I0302 19:00:11.452844 22895071117440 run.py:483] Algo bellman_ford step 5262 current loss 0.777877, current_train_items 168416.
I0302 19:00:11.482524 22895071117440 run.py:483] Algo bellman_ford step 5263 current loss 0.998997, current_train_items 168448.
I0302 19:00:11.512227 22895071117440 run.py:483] Algo bellman_ford step 5264 current loss 1.027387, current_train_items 168480.
I0302 19:00:11.530550 22895071117440 run.py:483] Algo bellman_ford step 5265 current loss 0.282059, current_train_items 168512.
I0302 19:00:11.545923 22895071117440 run.py:483] Algo bellman_ford step 5266 current loss 0.563385, current_train_items 168544.
I0302 19:00:11.568552 22895071117440 run.py:483] Algo bellman_ford step 5267 current loss 0.714023, current_train_items 168576.
I0302 19:00:11.596332 22895071117440 run.py:483] Algo bellman_ford step 5268 current loss 0.681225, current_train_items 168608.
I0302 19:00:11.625683 22895071117440 run.py:483] Algo bellman_ford step 5269 current loss 0.777329, current_train_items 168640.
I0302 19:00:11.644133 22895071117440 run.py:483] Algo bellman_ford step 5270 current loss 0.351910, current_train_items 168672.
I0302 19:00:11.659653 22895071117440 run.py:483] Algo bellman_ford step 5271 current loss 0.582809, current_train_items 168704.
I0302 19:00:11.680555 22895071117440 run.py:483] Algo bellman_ford step 5272 current loss 0.614830, current_train_items 168736.
I0302 19:00:11.708861 22895071117440 run.py:483] Algo bellman_ford step 5273 current loss 0.649245, current_train_items 168768.
I0302 19:00:11.738655 22895071117440 run.py:483] Algo bellman_ford step 5274 current loss 0.742747, current_train_items 168800.
I0302 19:00:11.756889 22895071117440 run.py:483] Algo bellman_ford step 5275 current loss 0.377256, current_train_items 168832.
I0302 19:00:11.772469 22895071117440 run.py:483] Algo bellman_ford step 5276 current loss 0.538529, current_train_items 168864.
I0302 19:00:11.793693 22895071117440 run.py:483] Algo bellman_ford step 5277 current loss 0.740754, current_train_items 168896.
I0302 19:00:11.822138 22895071117440 run.py:483] Algo bellman_ford step 5278 current loss 0.832654, current_train_items 168928.
I0302 19:00:11.853315 22895071117440 run.py:483] Algo bellman_ford step 5279 current loss 0.921121, current_train_items 168960.
I0302 19:00:11.871001 22895071117440 run.py:483] Algo bellman_ford step 5280 current loss 0.359964, current_train_items 168992.
I0302 19:00:11.886478 22895071117440 run.py:483] Algo bellman_ford step 5281 current loss 0.585176, current_train_items 169024.
I0302 19:00:11.909524 22895071117440 run.py:483] Algo bellman_ford step 5282 current loss 0.737227, current_train_items 169056.
I0302 19:00:11.938520 22895071117440 run.py:483] Algo bellman_ford step 5283 current loss 0.815685, current_train_items 169088.
I0302 19:00:11.968599 22895071117440 run.py:483] Algo bellman_ford step 5284 current loss 0.939270, current_train_items 169120.
I0302 19:00:11.986990 22895071117440 run.py:483] Algo bellman_ford step 5285 current loss 0.294914, current_train_items 169152.
I0302 19:00:12.002717 22895071117440 run.py:483] Algo bellman_ford step 5286 current loss 0.461043, current_train_items 169184.
I0302 19:00:12.025257 22895071117440 run.py:483] Algo bellman_ford step 5287 current loss 0.633407, current_train_items 169216.
I0302 19:00:12.053565 22895071117440 run.py:483] Algo bellman_ford step 5288 current loss 0.687072, current_train_items 169248.
I0302 19:00:12.085237 22895071117440 run.py:483] Algo bellman_ford step 5289 current loss 0.872863, current_train_items 169280.
I0302 19:00:12.103856 22895071117440 run.py:483] Algo bellman_ford step 5290 current loss 0.348992, current_train_items 169312.
I0302 19:00:12.119907 22895071117440 run.py:483] Algo bellman_ford step 5291 current loss 0.470051, current_train_items 169344.
I0302 19:00:12.141499 22895071117440 run.py:483] Algo bellman_ford step 5292 current loss 0.584234, current_train_items 169376.
I0302 19:00:12.169878 22895071117440 run.py:483] Algo bellman_ford step 5293 current loss 0.637127, current_train_items 169408.
I0302 19:00:12.199827 22895071117440 run.py:483] Algo bellman_ford step 5294 current loss 0.771689, current_train_items 169440.
I0302 19:00:12.217690 22895071117440 run.py:483] Algo bellman_ford step 5295 current loss 0.275065, current_train_items 169472.
I0302 19:00:12.232835 22895071117440 run.py:483] Algo bellman_ford step 5296 current loss 0.467459, current_train_items 169504.
I0302 19:00:12.254243 22895071117440 run.py:483] Algo bellman_ford step 5297 current loss 0.608071, current_train_items 169536.
I0302 19:00:12.283344 22895071117440 run.py:483] Algo bellman_ford step 5298 current loss 0.644887, current_train_items 169568.
I0302 19:00:12.316290 22895071117440 run.py:483] Algo bellman_ford step 5299 current loss 0.882817, current_train_items 169600.
I0302 19:00:12.334698 22895071117440 run.py:483] Algo bellman_ford step 5300 current loss 0.378603, current_train_items 169632.
I0302 19:00:12.342586 22895071117440 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:12.342690 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:00:12.358377 22895071117440 run.py:483] Algo bellman_ford step 5301 current loss 0.429882, current_train_items 169664.
I0302 19:00:12.381294 22895071117440 run.py:483] Algo bellman_ford step 5302 current loss 0.706193, current_train_items 169696.
I0302 19:00:12.411308 22895071117440 run.py:483] Algo bellman_ford step 5303 current loss 0.665203, current_train_items 169728.
I0302 19:00:12.443351 22895071117440 run.py:483] Algo bellman_ford step 5304 current loss 0.767404, current_train_items 169760.
I0302 19:00:12.462150 22895071117440 run.py:483] Algo bellman_ford step 5305 current loss 0.315666, current_train_items 169792.
I0302 19:00:12.477676 22895071117440 run.py:483] Algo bellman_ford step 5306 current loss 0.524956, current_train_items 169824.
I0302 19:00:12.500676 22895071117440 run.py:483] Algo bellman_ford step 5307 current loss 0.654714, current_train_items 169856.
I0302 19:00:12.530515 22895071117440 run.py:483] Algo bellman_ford step 5308 current loss 0.704706, current_train_items 169888.
I0302 19:00:12.559996 22895071117440 run.py:483] Algo bellman_ford step 5309 current loss 0.754084, current_train_items 169920.
I0302 19:00:12.578076 22895071117440 run.py:483] Algo bellman_ford step 5310 current loss 0.341627, current_train_items 169952.
I0302 19:00:12.593010 22895071117440 run.py:483] Algo bellman_ford step 5311 current loss 0.412992, current_train_items 169984.
I0302 19:00:12.614715 22895071117440 run.py:483] Algo bellman_ford step 5312 current loss 0.573901, current_train_items 170016.
I0302 19:00:12.643199 22895071117440 run.py:483] Algo bellman_ford step 5313 current loss 0.798184, current_train_items 170048.
I0302 19:00:12.675215 22895071117440 run.py:483] Algo bellman_ford step 5314 current loss 0.804226, current_train_items 170080.
I0302 19:00:12.693716 22895071117440 run.py:483] Algo bellman_ford step 5315 current loss 0.398334, current_train_items 170112.
I0302 19:00:12.708984 22895071117440 run.py:483] Algo bellman_ford step 5316 current loss 0.570334, current_train_items 170144.
I0302 19:00:12.730958 22895071117440 run.py:483] Algo bellman_ford step 5317 current loss 0.829375, current_train_items 170176.
I0302 19:00:12.758012 22895071117440 run.py:483] Algo bellman_ford step 5318 current loss 0.802248, current_train_items 170208.
I0302 19:00:12.789421 22895071117440 run.py:483] Algo bellman_ford step 5319 current loss 0.854608, current_train_items 170240.
I0302 19:00:12.807278 22895071117440 run.py:483] Algo bellman_ford step 5320 current loss 0.333414, current_train_items 170272.
I0302 19:00:12.822582 22895071117440 run.py:483] Algo bellman_ford step 5321 current loss 0.529501, current_train_items 170304.
I0302 19:00:12.845385 22895071117440 run.py:483] Algo bellman_ford step 5322 current loss 0.719961, current_train_items 170336.
I0302 19:00:12.872414 22895071117440 run.py:483] Algo bellman_ford step 5323 current loss 0.774660, current_train_items 170368.
I0302 19:00:12.904262 22895071117440 run.py:483] Algo bellman_ford step 5324 current loss 0.954309, current_train_items 170400.
I0302 19:00:12.922365 22895071117440 run.py:483] Algo bellman_ford step 5325 current loss 0.324660, current_train_items 170432.
I0302 19:00:12.937559 22895071117440 run.py:483] Algo bellman_ford step 5326 current loss 0.489719, current_train_items 170464.
I0302 19:00:12.958427 22895071117440 run.py:483] Algo bellman_ford step 5327 current loss 0.616342, current_train_items 170496.
I0302 19:00:12.988044 22895071117440 run.py:483] Algo bellman_ford step 5328 current loss 0.721596, current_train_items 170528.
I0302 19:00:13.018269 22895071117440 run.py:483] Algo bellman_ford step 5329 current loss 0.825830, current_train_items 170560.
I0302 19:00:13.036427 22895071117440 run.py:483] Algo bellman_ford step 5330 current loss 0.371038, current_train_items 170592.
I0302 19:00:13.051795 22895071117440 run.py:483] Algo bellman_ford step 5331 current loss 0.475511, current_train_items 170624.
I0302 19:00:13.073476 22895071117440 run.py:483] Algo bellman_ford step 5332 current loss 0.670224, current_train_items 170656.
I0302 19:00:13.100495 22895071117440 run.py:483] Algo bellman_ford step 5333 current loss 0.671578, current_train_items 170688.
I0302 19:00:13.130402 22895071117440 run.py:483] Algo bellman_ford step 5334 current loss 0.805281, current_train_items 170720.
I0302 19:00:13.148644 22895071117440 run.py:483] Algo bellman_ford step 5335 current loss 0.406890, current_train_items 170752.
I0302 19:00:13.164322 22895071117440 run.py:483] Algo bellman_ford step 5336 current loss 0.514056, current_train_items 170784.
I0302 19:00:13.186052 22895071117440 run.py:483] Algo bellman_ford step 5337 current loss 0.723587, current_train_items 170816.
I0302 19:00:13.213935 22895071117440 run.py:483] Algo bellman_ford step 5338 current loss 0.742652, current_train_items 170848.
I0302 19:00:13.245502 22895071117440 run.py:483] Algo bellman_ford step 5339 current loss 0.958255, current_train_items 170880.
I0302 19:00:13.263575 22895071117440 run.py:483] Algo bellman_ford step 5340 current loss 0.263527, current_train_items 170912.
I0302 19:00:13.279024 22895071117440 run.py:483] Algo bellman_ford step 5341 current loss 0.549518, current_train_items 170944.
I0302 19:00:13.302484 22895071117440 run.py:483] Algo bellman_ford step 5342 current loss 0.737567, current_train_items 170976.
I0302 19:00:13.331380 22895071117440 run.py:483] Algo bellman_ford step 5343 current loss 0.728148, current_train_items 171008.
I0302 19:00:13.361749 22895071117440 run.py:483] Algo bellman_ford step 5344 current loss 0.784608, current_train_items 171040.
I0302 19:00:13.379719 22895071117440 run.py:483] Algo bellman_ford step 5345 current loss 0.204217, current_train_items 171072.
I0302 19:00:13.394596 22895071117440 run.py:483] Algo bellman_ford step 5346 current loss 0.455823, current_train_items 171104.
I0302 19:00:13.416140 22895071117440 run.py:483] Algo bellman_ford step 5347 current loss 0.582912, current_train_items 171136.
I0302 19:00:13.445795 22895071117440 run.py:483] Algo bellman_ford step 5348 current loss 0.700360, current_train_items 171168.
I0302 19:00:13.477072 22895071117440 run.py:483] Algo bellman_ford step 5349 current loss 0.774904, current_train_items 171200.
I0302 19:00:13.495239 22895071117440 run.py:483] Algo bellman_ford step 5350 current loss 0.298672, current_train_items 171232.
I0302 19:00:13.503144 22895071117440 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:13.503250 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:13.518804 22895071117440 run.py:483] Algo bellman_ford step 5351 current loss 0.474339, current_train_items 171264.
I0302 19:00:13.541609 22895071117440 run.py:483] Algo bellman_ford step 5352 current loss 0.663797, current_train_items 171296.
I0302 19:00:13.569462 22895071117440 run.py:483] Algo bellman_ford step 5353 current loss 0.718263, current_train_items 171328.
I0302 19:00:13.600640 22895071117440 run.py:483] Algo bellman_ford step 5354 current loss 0.712579, current_train_items 171360.
I0302 19:00:13.619268 22895071117440 run.py:483] Algo bellman_ford step 5355 current loss 0.320166, current_train_items 171392.
I0302 19:00:13.634706 22895071117440 run.py:483] Algo bellman_ford step 5356 current loss 0.471212, current_train_items 171424.
I0302 19:00:13.656452 22895071117440 run.py:483] Algo bellman_ford step 5357 current loss 0.665897, current_train_items 171456.
I0302 19:00:13.683929 22895071117440 run.py:483] Algo bellman_ford step 5358 current loss 0.579683, current_train_items 171488.
I0302 19:00:13.714175 22895071117440 run.py:483] Algo bellman_ford step 5359 current loss 0.798445, current_train_items 171520.
I0302 19:00:13.732458 22895071117440 run.py:483] Algo bellman_ford step 5360 current loss 0.332437, current_train_items 171552.
I0302 19:00:13.748110 22895071117440 run.py:483] Algo bellman_ford step 5361 current loss 0.532506, current_train_items 171584.
I0302 19:00:13.770209 22895071117440 run.py:483] Algo bellman_ford step 5362 current loss 0.722036, current_train_items 171616.
I0302 19:00:13.798695 22895071117440 run.py:483] Algo bellman_ford step 5363 current loss 0.760900, current_train_items 171648.
I0302 19:00:13.829484 22895071117440 run.py:483] Algo bellman_ford step 5364 current loss 0.754234, current_train_items 171680.
I0302 19:00:13.848159 22895071117440 run.py:483] Algo bellman_ford step 5365 current loss 0.340233, current_train_items 171712.
I0302 19:00:13.863586 22895071117440 run.py:483] Algo bellman_ford step 5366 current loss 0.513333, current_train_items 171744.
I0302 19:00:13.885498 22895071117440 run.py:483] Algo bellman_ford step 5367 current loss 0.697526, current_train_items 171776.
I0302 19:00:13.913873 22895071117440 run.py:483] Algo bellman_ford step 5368 current loss 0.732429, current_train_items 171808.
I0302 19:00:13.943493 22895071117440 run.py:483] Algo bellman_ford step 5369 current loss 0.779095, current_train_items 171840.
I0302 19:00:13.961938 22895071117440 run.py:483] Algo bellman_ford step 5370 current loss 0.317044, current_train_items 171872.
I0302 19:00:13.977653 22895071117440 run.py:483] Algo bellman_ford step 5371 current loss 0.488122, current_train_items 171904.
I0302 19:00:13.999013 22895071117440 run.py:483] Algo bellman_ford step 5372 current loss 0.570728, current_train_items 171936.
I0302 19:00:14.027200 22895071117440 run.py:483] Algo bellman_ford step 5373 current loss 0.691479, current_train_items 171968.
I0302 19:00:14.055594 22895071117440 run.py:483] Algo bellman_ford step 5374 current loss 0.685466, current_train_items 172000.
I0302 19:00:14.073817 22895071117440 run.py:483] Algo bellman_ford step 5375 current loss 0.341292, current_train_items 172032.
I0302 19:00:14.089550 22895071117440 run.py:483] Algo bellman_ford step 5376 current loss 0.502941, current_train_items 172064.
I0302 19:00:14.111248 22895071117440 run.py:483] Algo bellman_ford step 5377 current loss 0.718810, current_train_items 172096.
I0302 19:00:14.138643 22895071117440 run.py:483] Algo bellman_ford step 5378 current loss 0.655731, current_train_items 172128.
I0302 19:00:14.166612 22895071117440 run.py:483] Algo bellman_ford step 5379 current loss 0.692019, current_train_items 172160.
I0302 19:00:14.184694 22895071117440 run.py:483] Algo bellman_ford step 5380 current loss 0.340580, current_train_items 172192.
I0302 19:00:14.200175 22895071117440 run.py:483] Algo bellman_ford step 5381 current loss 0.477092, current_train_items 172224.
I0302 19:00:14.222433 22895071117440 run.py:483] Algo bellman_ford step 5382 current loss 0.693802, current_train_items 172256.
I0302 19:00:14.250922 22895071117440 run.py:483] Algo bellman_ford step 5383 current loss 0.739698, current_train_items 172288.
I0302 19:00:14.280307 22895071117440 run.py:483] Algo bellman_ford step 5384 current loss 0.731355, current_train_items 172320.
I0302 19:00:14.298714 22895071117440 run.py:483] Algo bellman_ford step 5385 current loss 0.314792, current_train_items 172352.
I0302 19:00:14.313981 22895071117440 run.py:483] Algo bellman_ford step 5386 current loss 0.481686, current_train_items 172384.
I0302 19:00:14.335247 22895071117440 run.py:483] Algo bellman_ford step 5387 current loss 0.608962, current_train_items 172416.
I0302 19:00:14.362742 22895071117440 run.py:483] Algo bellman_ford step 5388 current loss 0.700265, current_train_items 172448.
I0302 19:00:14.393756 22895071117440 run.py:483] Algo bellman_ford step 5389 current loss 0.890773, current_train_items 172480.
I0302 19:00:14.412179 22895071117440 run.py:483] Algo bellman_ford step 5390 current loss 0.307910, current_train_items 172512.
I0302 19:00:14.428194 22895071117440 run.py:483] Algo bellman_ford step 5391 current loss 0.496443, current_train_items 172544.
I0302 19:00:14.449360 22895071117440 run.py:483] Algo bellman_ford step 5392 current loss 0.600530, current_train_items 172576.
I0302 19:00:14.479032 22895071117440 run.py:483] Algo bellman_ford step 5393 current loss 0.786656, current_train_items 172608.
I0302 19:00:14.510278 22895071117440 run.py:483] Algo bellman_ford step 5394 current loss 0.900124, current_train_items 172640.
I0302 19:00:14.528180 22895071117440 run.py:483] Algo bellman_ford step 5395 current loss 0.391138, current_train_items 172672.
I0302 19:00:14.543838 22895071117440 run.py:483] Algo bellman_ford step 5396 current loss 0.514321, current_train_items 172704.
I0302 19:00:14.565669 22895071117440 run.py:483] Algo bellman_ford step 5397 current loss 0.562321, current_train_items 172736.
I0302 19:00:14.594080 22895071117440 run.py:483] Algo bellman_ford step 5398 current loss 0.691241, current_train_items 172768.
I0302 19:00:14.625965 22895071117440 run.py:483] Algo bellman_ford step 5399 current loss 0.879070, current_train_items 172800.
I0302 19:00:14.644733 22895071117440 run.py:483] Algo bellman_ford step 5400 current loss 0.241928, current_train_items 172832.
I0302 19:00:14.652572 22895071117440 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:14.652682 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:14.669144 22895071117440 run.py:483] Algo bellman_ford step 5401 current loss 0.552985, current_train_items 172864.
I0302 19:00:14.692008 22895071117440 run.py:483] Algo bellman_ford step 5402 current loss 0.710001, current_train_items 172896.
I0302 19:00:14.722268 22895071117440 run.py:483] Algo bellman_ford step 5403 current loss 0.871608, current_train_items 172928.
I0302 19:00:14.754403 22895071117440 run.py:483] Algo bellman_ford step 5404 current loss 0.760479, current_train_items 172960.
I0302 19:00:14.772686 22895071117440 run.py:483] Algo bellman_ford step 5405 current loss 0.250270, current_train_items 172992.
I0302 19:00:14.787825 22895071117440 run.py:483] Algo bellman_ford step 5406 current loss 0.429559, current_train_items 173024.
I0302 19:00:14.809221 22895071117440 run.py:483] Algo bellman_ford step 5407 current loss 0.551085, current_train_items 173056.
I0302 19:00:14.836352 22895071117440 run.py:483] Algo bellman_ford step 5408 current loss 0.683608, current_train_items 173088.
I0302 19:00:14.866794 22895071117440 run.py:483] Algo bellman_ford step 5409 current loss 0.675057, current_train_items 173120.
I0302 19:00:14.885075 22895071117440 run.py:483] Algo bellman_ford step 5410 current loss 0.282538, current_train_items 173152.
I0302 19:00:14.900973 22895071117440 run.py:483] Algo bellman_ford step 5411 current loss 0.528184, current_train_items 173184.
I0302 19:00:14.923080 22895071117440 run.py:483] Algo bellman_ford step 5412 current loss 0.738504, current_train_items 173216.
I0302 19:00:14.951651 22895071117440 run.py:483] Algo bellman_ford step 5413 current loss 0.690049, current_train_items 173248.
I0302 19:00:14.983177 22895071117440 run.py:483] Algo bellman_ford step 5414 current loss 0.793138, current_train_items 173280.
I0302 19:00:15.001398 22895071117440 run.py:483] Algo bellman_ford step 5415 current loss 0.207592, current_train_items 173312.
I0302 19:00:15.017044 22895071117440 run.py:483] Algo bellman_ford step 5416 current loss 0.447993, current_train_items 173344.
I0302 19:00:15.039600 22895071117440 run.py:483] Algo bellman_ford step 5417 current loss 0.602383, current_train_items 173376.
I0302 19:00:15.069114 22895071117440 run.py:483] Algo bellman_ford step 5418 current loss 0.656599, current_train_items 173408.
I0302 19:00:15.101730 22895071117440 run.py:483] Algo bellman_ford step 5419 current loss 0.852887, current_train_items 173440.
I0302 19:00:15.119552 22895071117440 run.py:483] Algo bellman_ford step 5420 current loss 0.341490, current_train_items 173472.
I0302 19:00:15.135224 22895071117440 run.py:483] Algo bellman_ford step 5421 current loss 0.510824, current_train_items 173504.
I0302 19:00:15.156708 22895071117440 run.py:483] Algo bellman_ford step 5422 current loss 0.524494, current_train_items 173536.
I0302 19:00:15.184501 22895071117440 run.py:483] Algo bellman_ford step 5423 current loss 0.654864, current_train_items 173568.
I0302 19:00:15.213468 22895071117440 run.py:483] Algo bellman_ford step 5424 current loss 0.738993, current_train_items 173600.
I0302 19:00:15.231790 22895071117440 run.py:483] Algo bellman_ford step 5425 current loss 0.297992, current_train_items 173632.
I0302 19:00:15.246910 22895071117440 run.py:483] Algo bellman_ford step 5426 current loss 0.433806, current_train_items 173664.
I0302 19:00:15.269080 22895071117440 run.py:483] Algo bellman_ford step 5427 current loss 0.609452, current_train_items 173696.
I0302 19:00:15.297592 22895071117440 run.py:483] Algo bellman_ford step 5428 current loss 0.755458, current_train_items 173728.
I0302 19:00:15.329472 22895071117440 run.py:483] Algo bellman_ford step 5429 current loss 0.731432, current_train_items 173760.
I0302 19:00:15.347473 22895071117440 run.py:483] Algo bellman_ford step 5430 current loss 0.270357, current_train_items 173792.
I0302 19:00:15.363106 22895071117440 run.py:483] Algo bellman_ford step 5431 current loss 0.444621, current_train_items 173824.
I0302 19:00:15.385262 22895071117440 run.py:483] Algo bellman_ford step 5432 current loss 0.571097, current_train_items 173856.
I0302 19:00:15.416192 22895071117440 run.py:483] Algo bellman_ford step 5433 current loss 0.795364, current_train_items 173888.
I0302 19:00:15.449151 22895071117440 run.py:483] Algo bellman_ford step 5434 current loss 0.972220, current_train_items 173920.
I0302 19:00:15.466991 22895071117440 run.py:483] Algo bellman_ford step 5435 current loss 0.259197, current_train_items 173952.
I0302 19:00:15.482353 22895071117440 run.py:483] Algo bellman_ford step 5436 current loss 0.486460, current_train_items 173984.
I0302 19:00:15.504138 22895071117440 run.py:483] Algo bellman_ford step 5437 current loss 0.868406, current_train_items 174016.
I0302 19:00:15.531794 22895071117440 run.py:483] Algo bellman_ford step 5438 current loss 0.959410, current_train_items 174048.
I0302 19:00:15.566455 22895071117440 run.py:483] Algo bellman_ford step 5439 current loss 1.096257, current_train_items 174080.
I0302 19:00:15.584402 22895071117440 run.py:483] Algo bellman_ford step 5440 current loss 0.464969, current_train_items 174112.
I0302 19:00:15.599554 22895071117440 run.py:483] Algo bellman_ford step 5441 current loss 0.462143, current_train_items 174144.
I0302 19:00:15.620640 22895071117440 run.py:483] Algo bellman_ford step 5442 current loss 0.955261, current_train_items 174176.
I0302 19:00:15.648750 22895071117440 run.py:483] Algo bellman_ford step 5443 current loss 1.172366, current_train_items 174208.
I0302 19:00:15.680021 22895071117440 run.py:483] Algo bellman_ford step 5444 current loss 1.784971, current_train_items 174240.
I0302 19:00:15.698108 22895071117440 run.py:483] Algo bellman_ford step 5445 current loss 0.246951, current_train_items 174272.
I0302 19:00:15.713431 22895071117440 run.py:483] Algo bellman_ford step 5446 current loss 0.604320, current_train_items 174304.
I0302 19:00:15.735914 22895071117440 run.py:483] Algo bellman_ford step 5447 current loss 0.895527, current_train_items 174336.
I0302 19:00:15.765593 22895071117440 run.py:483] Algo bellman_ford step 5448 current loss 0.924886, current_train_items 174368.
I0302 19:00:15.796420 22895071117440 run.py:483] Algo bellman_ford step 5449 current loss 0.856946, current_train_items 174400.
I0302 19:00:15.814516 22895071117440 run.py:483] Algo bellman_ford step 5450 current loss 0.290638, current_train_items 174432.
I0302 19:00:15.822450 22895071117440 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:00:15.822556 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:15.838320 22895071117440 run.py:483] Algo bellman_ford step 5451 current loss 0.535521, current_train_items 174464.
I0302 19:00:15.860608 22895071117440 run.py:483] Algo bellman_ford step 5452 current loss 0.748611, current_train_items 174496.
I0302 19:00:15.888632 22895071117440 run.py:483] Algo bellman_ford step 5453 current loss 0.760951, current_train_items 174528.
I0302 19:00:15.919601 22895071117440 run.py:483] Algo bellman_ford step 5454 current loss 1.015985, current_train_items 174560.
I0302 19:00:15.938237 22895071117440 run.py:483] Algo bellman_ford step 5455 current loss 0.331583, current_train_items 174592.
I0302 19:00:15.953870 22895071117440 run.py:483] Algo bellman_ford step 5456 current loss 0.539282, current_train_items 174624.
I0302 19:00:15.976314 22895071117440 run.py:483] Algo bellman_ford step 5457 current loss 0.712112, current_train_items 174656.
I0302 19:00:16.005525 22895071117440 run.py:483] Algo bellman_ford step 5458 current loss 0.823953, current_train_items 174688.
I0302 19:00:16.034277 22895071117440 run.py:483] Algo bellman_ford step 5459 current loss 0.761623, current_train_items 174720.
I0302 19:00:16.052618 22895071117440 run.py:483] Algo bellman_ford step 5460 current loss 0.337814, current_train_items 174752.
I0302 19:00:16.068661 22895071117440 run.py:483] Algo bellman_ford step 5461 current loss 0.480260, current_train_items 174784.
I0302 19:00:16.090747 22895071117440 run.py:483] Algo bellman_ford step 5462 current loss 0.609253, current_train_items 174816.
I0302 19:00:16.118783 22895071117440 run.py:483] Algo bellman_ford step 5463 current loss 0.718382, current_train_items 174848.
I0302 19:00:16.149974 22895071117440 run.py:483] Algo bellman_ford step 5464 current loss 0.903689, current_train_items 174880.
I0302 19:00:16.168285 22895071117440 run.py:483] Algo bellman_ford step 5465 current loss 0.382249, current_train_items 174912.
I0302 19:00:16.183418 22895071117440 run.py:483] Algo bellman_ford step 5466 current loss 0.460511, current_train_items 174944.
I0302 19:00:16.204889 22895071117440 run.py:483] Algo bellman_ford step 5467 current loss 0.626142, current_train_items 174976.
I0302 19:00:16.232539 22895071117440 run.py:483] Algo bellman_ford step 5468 current loss 0.664165, current_train_items 175008.
I0302 19:00:16.263022 22895071117440 run.py:483] Algo bellman_ford step 5469 current loss 0.780920, current_train_items 175040.
I0302 19:00:16.281830 22895071117440 run.py:483] Algo bellman_ford step 5470 current loss 0.390267, current_train_items 175072.
I0302 19:00:16.297728 22895071117440 run.py:483] Algo bellman_ford step 5471 current loss 0.638352, current_train_items 175104.
I0302 19:00:16.319197 22895071117440 run.py:483] Algo bellman_ford step 5472 current loss 0.502828, current_train_items 175136.
I0302 19:00:16.347421 22895071117440 run.py:483] Algo bellman_ford step 5473 current loss 0.721974, current_train_items 175168.
I0302 19:00:16.379549 22895071117440 run.py:483] Algo bellman_ford step 5474 current loss 0.806745, current_train_items 175200.
I0302 19:00:16.398045 22895071117440 run.py:483] Algo bellman_ford step 5475 current loss 0.315226, current_train_items 175232.
I0302 19:00:16.413842 22895071117440 run.py:483] Algo bellman_ford step 5476 current loss 0.495794, current_train_items 175264.
I0302 19:00:16.435278 22895071117440 run.py:483] Algo bellman_ford step 5477 current loss 0.592556, current_train_items 175296.
I0302 19:00:16.464277 22895071117440 run.py:483] Algo bellman_ford step 5478 current loss 0.677020, current_train_items 175328.
I0302 19:00:16.494764 22895071117440 run.py:483] Algo bellman_ford step 5479 current loss 0.903111, current_train_items 175360.
I0302 19:00:16.513195 22895071117440 run.py:483] Algo bellman_ford step 5480 current loss 0.381170, current_train_items 175392.
I0302 19:00:16.529158 22895071117440 run.py:483] Algo bellman_ford step 5481 current loss 0.538091, current_train_items 175424.
I0302 19:00:16.552062 22895071117440 run.py:483] Algo bellman_ford step 5482 current loss 0.833659, current_train_items 175456.
I0302 19:00:16.581239 22895071117440 run.py:483] Algo bellman_ford step 5483 current loss 0.835298, current_train_items 175488.
I0302 19:00:16.613990 22895071117440 run.py:483] Algo bellman_ford step 5484 current loss 0.844815, current_train_items 175520.
I0302 19:00:16.632473 22895071117440 run.py:483] Algo bellman_ford step 5485 current loss 0.275679, current_train_items 175552.
I0302 19:00:16.647962 22895071117440 run.py:483] Algo bellman_ford step 5486 current loss 0.506733, current_train_items 175584.
I0302 19:00:16.668979 22895071117440 run.py:483] Algo bellman_ford step 5487 current loss 0.627056, current_train_items 175616.
I0302 19:00:16.699045 22895071117440 run.py:483] Algo bellman_ford step 5488 current loss 0.750212, current_train_items 175648.
I0302 19:00:16.729045 22895071117440 run.py:483] Algo bellman_ford step 5489 current loss 0.731684, current_train_items 175680.
I0302 19:00:16.747776 22895071117440 run.py:483] Algo bellman_ford step 5490 current loss 0.365013, current_train_items 175712.
I0302 19:00:16.763636 22895071117440 run.py:483] Algo bellman_ford step 5491 current loss 0.555373, current_train_items 175744.
I0302 19:00:16.785688 22895071117440 run.py:483] Algo bellman_ford step 5492 current loss 0.779253, current_train_items 175776.
I0302 19:00:16.813753 22895071117440 run.py:483] Algo bellman_ford step 5493 current loss 0.778298, current_train_items 175808.
I0302 19:00:16.844235 22895071117440 run.py:483] Algo bellman_ford step 5494 current loss 0.928339, current_train_items 175840.
I0302 19:00:16.862469 22895071117440 run.py:483] Algo bellman_ford step 5495 current loss 0.308147, current_train_items 175872.
I0302 19:00:16.878089 22895071117440 run.py:483] Algo bellman_ford step 5496 current loss 0.477085, current_train_items 175904.
I0302 19:00:16.900074 22895071117440 run.py:483] Algo bellman_ford step 5497 current loss 0.709726, current_train_items 175936.
I0302 19:00:16.928932 22895071117440 run.py:483] Algo bellman_ford step 5498 current loss 0.692992, current_train_items 175968.
I0302 19:00:16.960361 22895071117440 run.py:483] Algo bellman_ford step 5499 current loss 0.987861, current_train_items 176000.
I0302 19:00:16.979330 22895071117440 run.py:483] Algo bellman_ford step 5500 current loss 0.307225, current_train_items 176032.
I0302 19:00:16.987115 22895071117440 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:00:16.987223 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:17.003573 22895071117440 run.py:483] Algo bellman_ford step 5501 current loss 0.525650, current_train_items 176064.
I0302 19:00:17.026330 22895071117440 run.py:483] Algo bellman_ford step 5502 current loss 0.704693, current_train_items 176096.
I0302 19:00:17.054768 22895071117440 run.py:483] Algo bellman_ford step 5503 current loss 0.687672, current_train_items 176128.
I0302 19:00:17.086591 22895071117440 run.py:483] Algo bellman_ford step 5504 current loss 0.835357, current_train_items 176160.
I0302 19:00:17.105159 22895071117440 run.py:483] Algo bellman_ford step 5505 current loss 0.284287, current_train_items 176192.
I0302 19:00:17.120705 22895071117440 run.py:483] Algo bellman_ford step 5506 current loss 0.475789, current_train_items 176224.
I0302 19:00:17.141857 22895071117440 run.py:483] Algo bellman_ford step 5507 current loss 0.570192, current_train_items 176256.
I0302 19:00:17.169381 22895071117440 run.py:483] Algo bellman_ford step 5508 current loss 0.693467, current_train_items 176288.
I0302 19:00:17.199666 22895071117440 run.py:483] Algo bellman_ford step 5509 current loss 0.723520, current_train_items 176320.
I0302 19:00:17.217517 22895071117440 run.py:483] Algo bellman_ford step 5510 current loss 0.317393, current_train_items 176352.
I0302 19:00:17.233175 22895071117440 run.py:483] Algo bellman_ford step 5511 current loss 0.513286, current_train_items 176384.
I0302 19:00:17.256307 22895071117440 run.py:483] Algo bellman_ford step 5512 current loss 0.696220, current_train_items 176416.
I0302 19:00:17.284108 22895071117440 run.py:483] Algo bellman_ford step 5513 current loss 0.679061, current_train_items 176448.
I0302 19:00:17.313499 22895071117440 run.py:483] Algo bellman_ford step 5514 current loss 0.798015, current_train_items 176480.
I0302 19:00:17.331517 22895071117440 run.py:483] Algo bellman_ford step 5515 current loss 0.289967, current_train_items 176512.
I0302 19:00:17.346642 22895071117440 run.py:483] Algo bellman_ford step 5516 current loss 0.441909, current_train_items 176544.
I0302 19:00:17.369086 22895071117440 run.py:483] Algo bellman_ford step 5517 current loss 0.642563, current_train_items 176576.
I0302 19:00:17.397290 22895071117440 run.py:483] Algo bellman_ford step 5518 current loss 0.688656, current_train_items 176608.
I0302 19:00:17.429727 22895071117440 run.py:483] Algo bellman_ford step 5519 current loss 0.737077, current_train_items 176640.
I0302 19:00:17.447859 22895071117440 run.py:483] Algo bellman_ford step 5520 current loss 0.404251, current_train_items 176672.
I0302 19:00:17.463594 22895071117440 run.py:483] Algo bellman_ford step 5521 current loss 0.460164, current_train_items 176704.
I0302 19:00:17.485213 22895071117440 run.py:483] Algo bellman_ford step 5522 current loss 0.577973, current_train_items 176736.
I0302 19:00:17.514302 22895071117440 run.py:483] Algo bellman_ford step 5523 current loss 0.641721, current_train_items 176768.
I0302 19:00:17.546741 22895071117440 run.py:483] Algo bellman_ford step 5524 current loss 0.933051, current_train_items 176800.
I0302 19:00:17.564887 22895071117440 run.py:483] Algo bellman_ford step 5525 current loss 0.258513, current_train_items 176832.
I0302 19:00:17.579945 22895071117440 run.py:483] Algo bellman_ford step 5526 current loss 0.499049, current_train_items 176864.
I0302 19:00:17.602859 22895071117440 run.py:483] Algo bellman_ford step 5527 current loss 0.650908, current_train_items 176896.
I0302 19:00:17.630440 22895071117440 run.py:483] Algo bellman_ford step 5528 current loss 0.673309, current_train_items 176928.
I0302 19:00:17.659786 22895071117440 run.py:483] Algo bellman_ford step 5529 current loss 0.758521, current_train_items 176960.
I0302 19:00:17.678257 22895071117440 run.py:483] Algo bellman_ford step 5530 current loss 0.341905, current_train_items 176992.
I0302 19:00:17.693671 22895071117440 run.py:483] Algo bellman_ford step 5531 current loss 0.465098, current_train_items 177024.
I0302 19:00:17.716018 22895071117440 run.py:483] Algo bellman_ford step 5532 current loss 0.661383, current_train_items 177056.
I0302 19:00:17.744169 22895071117440 run.py:483] Algo bellman_ford step 5533 current loss 0.730756, current_train_items 177088.
I0302 19:00:17.777211 22895071117440 run.py:483] Algo bellman_ford step 5534 current loss 0.887156, current_train_items 177120.
I0302 19:00:17.795315 22895071117440 run.py:483] Algo bellman_ford step 5535 current loss 0.314828, current_train_items 177152.
I0302 19:00:17.810944 22895071117440 run.py:483] Algo bellman_ford step 5536 current loss 0.584808, current_train_items 177184.
I0302 19:00:17.832148 22895071117440 run.py:483] Algo bellman_ford step 5537 current loss 0.653520, current_train_items 177216.
I0302 19:00:17.859710 22895071117440 run.py:483] Algo bellman_ford step 5538 current loss 0.640801, current_train_items 177248.
I0302 19:00:17.892570 22895071117440 run.py:483] Algo bellman_ford step 5539 current loss 0.797668, current_train_items 177280.
I0302 19:00:17.910697 22895071117440 run.py:483] Algo bellman_ford step 5540 current loss 0.302706, current_train_items 177312.
I0302 19:00:17.926523 22895071117440 run.py:483] Algo bellman_ford step 5541 current loss 0.464415, current_train_items 177344.
I0302 19:00:17.949342 22895071117440 run.py:483] Algo bellman_ford step 5542 current loss 0.592353, current_train_items 177376.
I0302 19:00:17.977742 22895071117440 run.py:483] Algo bellman_ford step 5543 current loss 0.584318, current_train_items 177408.
I0302 19:00:18.008265 22895071117440 run.py:483] Algo bellman_ford step 5544 current loss 0.742639, current_train_items 177440.
I0302 19:00:18.026158 22895071117440 run.py:483] Algo bellman_ford step 5545 current loss 0.245706, current_train_items 177472.
I0302 19:00:18.042022 22895071117440 run.py:483] Algo bellman_ford step 5546 current loss 0.548384, current_train_items 177504.
I0302 19:00:18.063992 22895071117440 run.py:483] Algo bellman_ford step 5547 current loss 0.571316, current_train_items 177536.
I0302 19:00:18.091740 22895071117440 run.py:483] Algo bellman_ford step 5548 current loss 0.581773, current_train_items 177568.
I0302 19:00:18.121716 22895071117440 run.py:483] Algo bellman_ford step 5549 current loss 0.801279, current_train_items 177600.
I0302 19:00:18.139634 22895071117440 run.py:483] Algo bellman_ford step 5550 current loss 0.274042, current_train_items 177632.
I0302 19:00:18.147607 22895071117440 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:00:18.147712 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:18.163725 22895071117440 run.py:483] Algo bellman_ford step 5551 current loss 0.407090, current_train_items 177664.
I0302 19:00:18.187237 22895071117440 run.py:483] Algo bellman_ford step 5552 current loss 0.655267, current_train_items 177696.
I0302 19:00:18.215854 22895071117440 run.py:483] Algo bellman_ford step 5553 current loss 0.732083, current_train_items 177728.
I0302 19:00:18.249643 22895071117440 run.py:483] Algo bellman_ford step 5554 current loss 0.897001, current_train_items 177760.
I0302 19:00:18.268821 22895071117440 run.py:483] Algo bellman_ford step 5555 current loss 0.266456, current_train_items 177792.
I0302 19:00:18.284137 22895071117440 run.py:483] Algo bellman_ford step 5556 current loss 0.427918, current_train_items 177824.
I0302 19:00:18.306635 22895071117440 run.py:483] Algo bellman_ford step 5557 current loss 0.611208, current_train_items 177856.
I0302 19:00:18.334705 22895071117440 run.py:483] Algo bellman_ford step 5558 current loss 0.639493, current_train_items 177888.
I0302 19:00:18.366084 22895071117440 run.py:483] Algo bellman_ford step 5559 current loss 0.780037, current_train_items 177920.
I0302 19:00:18.384852 22895071117440 run.py:483] Algo bellman_ford step 5560 current loss 0.301411, current_train_items 177952.
I0302 19:00:18.400307 22895071117440 run.py:483] Algo bellman_ford step 5561 current loss 0.506541, current_train_items 177984.
I0302 19:00:18.421631 22895071117440 run.py:483] Algo bellman_ford step 5562 current loss 0.637452, current_train_items 178016.
I0302 19:00:18.450537 22895071117440 run.py:483] Algo bellman_ford step 5563 current loss 0.658436, current_train_items 178048.
I0302 19:00:18.482501 22895071117440 run.py:483] Algo bellman_ford step 5564 current loss 0.744766, current_train_items 178080.
I0302 19:00:18.500722 22895071117440 run.py:483] Algo bellman_ford step 5565 current loss 0.377819, current_train_items 178112.
I0302 19:00:18.515992 22895071117440 run.py:483] Algo bellman_ford step 5566 current loss 0.461793, current_train_items 178144.
I0302 19:00:18.538096 22895071117440 run.py:483] Algo bellman_ford step 5567 current loss 0.595531, current_train_items 178176.
I0302 19:00:18.567945 22895071117440 run.py:483] Algo bellman_ford step 5568 current loss 0.780243, current_train_items 178208.
I0302 19:00:18.599089 22895071117440 run.py:483] Algo bellman_ford step 5569 current loss 0.857318, current_train_items 178240.
I0302 19:00:18.617640 22895071117440 run.py:483] Algo bellman_ford step 5570 current loss 0.453120, current_train_items 178272.
I0302 19:00:18.633007 22895071117440 run.py:483] Algo bellman_ford step 5571 current loss 0.486907, current_train_items 178304.
I0302 19:00:18.654433 22895071117440 run.py:483] Algo bellman_ford step 5572 current loss 0.619729, current_train_items 178336.
I0302 19:00:18.682954 22895071117440 run.py:483] Algo bellman_ford step 5573 current loss 0.690498, current_train_items 178368.
I0302 19:00:18.714231 22895071117440 run.py:483] Algo bellman_ford step 5574 current loss 0.815971, current_train_items 178400.
I0302 19:00:18.732592 22895071117440 run.py:483] Algo bellman_ford step 5575 current loss 0.333564, current_train_items 178432.
I0302 19:00:18.748187 22895071117440 run.py:483] Algo bellman_ford step 5576 current loss 0.620235, current_train_items 178464.
I0302 19:00:18.769265 22895071117440 run.py:483] Algo bellman_ford step 5577 current loss 0.696542, current_train_items 178496.
I0302 19:00:18.797752 22895071117440 run.py:483] Algo bellman_ford step 5578 current loss 0.742213, current_train_items 178528.
I0302 19:00:18.830436 22895071117440 run.py:483] Algo bellman_ford step 5579 current loss 0.849952, current_train_items 178560.
I0302 19:00:18.848527 22895071117440 run.py:483] Algo bellman_ford step 5580 current loss 0.314748, current_train_items 178592.
I0302 19:00:18.863473 22895071117440 run.py:483] Algo bellman_ford step 5581 current loss 0.436409, current_train_items 178624.
I0302 19:00:18.886289 22895071117440 run.py:483] Algo bellman_ford step 5582 current loss 0.623904, current_train_items 178656.
I0302 19:00:18.914611 22895071117440 run.py:483] Algo bellman_ford step 5583 current loss 0.703221, current_train_items 178688.
I0302 19:00:18.945697 22895071117440 run.py:483] Algo bellman_ford step 5584 current loss 0.775499, current_train_items 178720.
I0302 19:00:18.964204 22895071117440 run.py:483] Algo bellman_ford step 5585 current loss 0.352368, current_train_items 178752.
I0302 19:00:18.979832 22895071117440 run.py:483] Algo bellman_ford step 5586 current loss 0.496465, current_train_items 178784.
I0302 19:00:19.003038 22895071117440 run.py:483] Algo bellman_ford step 5587 current loss 0.683325, current_train_items 178816.
I0302 19:00:19.032172 22895071117440 run.py:483] Algo bellman_ford step 5588 current loss 0.793895, current_train_items 178848.
I0302 19:00:19.060872 22895071117440 run.py:483] Algo bellman_ford step 5589 current loss 0.800352, current_train_items 178880.
I0302 19:00:19.079435 22895071117440 run.py:483] Algo bellman_ford step 5590 current loss 0.298441, current_train_items 178912.
I0302 19:00:19.095094 22895071117440 run.py:483] Algo bellman_ford step 5591 current loss 0.459296, current_train_items 178944.
I0302 19:00:19.115859 22895071117440 run.py:483] Algo bellman_ford step 5592 current loss 0.701178, current_train_items 178976.
I0302 19:00:19.144050 22895071117440 run.py:483] Algo bellman_ford step 5593 current loss 0.796058, current_train_items 179008.
I0302 19:00:19.175981 22895071117440 run.py:483] Algo bellman_ford step 5594 current loss 0.785453, current_train_items 179040.
I0302 19:00:19.194008 22895071117440 run.py:483] Algo bellman_ford step 5595 current loss 0.383220, current_train_items 179072.
I0302 19:00:19.209136 22895071117440 run.py:483] Algo bellman_ford step 5596 current loss 0.480892, current_train_items 179104.
I0302 19:00:19.230927 22895071117440 run.py:483] Algo bellman_ford step 5597 current loss 0.665763, current_train_items 179136.
I0302 19:00:19.259357 22895071117440 run.py:483] Algo bellman_ford step 5598 current loss 0.657349, current_train_items 179168.
I0302 19:00:19.286553 22895071117440 run.py:483] Algo bellman_ford step 5599 current loss 0.639224, current_train_items 179200.
I0302 19:00:19.304941 22895071117440 run.py:483] Algo bellman_ford step 5600 current loss 0.313159, current_train_items 179232.
I0302 19:00:19.312939 22895071117440 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:00:19.313046 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:19.329062 22895071117440 run.py:483] Algo bellman_ford step 5601 current loss 0.504592, current_train_items 179264.
I0302 19:00:19.351503 22895071117440 run.py:483] Algo bellman_ford step 5602 current loss 0.742808, current_train_items 179296.
I0302 19:00:19.380095 22895071117440 run.py:483] Algo bellman_ford step 5603 current loss 0.913061, current_train_items 179328.
I0302 19:00:19.409841 22895071117440 run.py:483] Algo bellman_ford step 5604 current loss 0.881408, current_train_items 179360.
I0302 19:00:19.428517 22895071117440 run.py:483] Algo bellman_ford step 5605 current loss 0.287229, current_train_items 179392.
I0302 19:00:19.443718 22895071117440 run.py:483] Algo bellman_ford step 5606 current loss 0.533176, current_train_items 179424.
I0302 19:00:19.465706 22895071117440 run.py:483] Algo bellman_ford step 5607 current loss 0.643376, current_train_items 179456.
I0302 19:00:19.492913 22895071117440 run.py:483] Algo bellman_ford step 5608 current loss 0.677369, current_train_items 179488.
I0302 19:00:19.524286 22895071117440 run.py:483] Algo bellman_ford step 5609 current loss 0.903201, current_train_items 179520.
I0302 19:00:19.542383 22895071117440 run.py:483] Algo bellman_ford step 5610 current loss 0.256307, current_train_items 179552.
I0302 19:00:19.557888 22895071117440 run.py:483] Algo bellman_ford step 5611 current loss 0.461449, current_train_items 179584.
I0302 19:00:19.579462 22895071117440 run.py:483] Algo bellman_ford step 5612 current loss 0.627314, current_train_items 179616.
I0302 19:00:19.608855 22895071117440 run.py:483] Algo bellman_ford step 5613 current loss 0.704906, current_train_items 179648.
I0302 19:00:19.638552 22895071117440 run.py:483] Algo bellman_ford step 5614 current loss 0.779351, current_train_items 179680.
I0302 19:00:19.656810 22895071117440 run.py:483] Algo bellman_ford step 5615 current loss 0.300014, current_train_items 179712.
I0302 19:00:19.672560 22895071117440 run.py:483] Algo bellman_ford step 5616 current loss 0.642306, current_train_items 179744.
I0302 19:00:19.694599 22895071117440 run.py:483] Algo bellman_ford step 5617 current loss 0.630937, current_train_items 179776.
I0302 19:00:19.722723 22895071117440 run.py:483] Algo bellman_ford step 5618 current loss 0.669867, current_train_items 179808.
I0302 19:00:19.754730 22895071117440 run.py:483] Algo bellman_ford step 5619 current loss 0.865108, current_train_items 179840.
I0302 19:00:19.772708 22895071117440 run.py:483] Algo bellman_ford step 5620 current loss 0.353972, current_train_items 179872.
I0302 19:00:19.788105 22895071117440 run.py:483] Algo bellman_ford step 5621 current loss 0.478897, current_train_items 179904.
I0302 19:00:19.809747 22895071117440 run.py:483] Algo bellman_ford step 5622 current loss 0.626060, current_train_items 179936.
I0302 19:00:19.837940 22895071117440 run.py:483] Algo bellman_ford step 5623 current loss 0.808595, current_train_items 179968.
I0302 19:00:19.869385 22895071117440 run.py:483] Algo bellman_ford step 5624 current loss 0.835454, current_train_items 180000.
I0302 19:00:19.887407 22895071117440 run.py:483] Algo bellman_ford step 5625 current loss 0.274734, current_train_items 180032.
I0302 19:00:19.903149 22895071117440 run.py:483] Algo bellman_ford step 5626 current loss 0.635693, current_train_items 180064.
I0302 19:00:19.925522 22895071117440 run.py:483] Algo bellman_ford step 5627 current loss 0.738410, current_train_items 180096.
I0302 19:00:19.953984 22895071117440 run.py:483] Algo bellman_ford step 5628 current loss 0.721047, current_train_items 180128.
I0302 19:00:19.985752 22895071117440 run.py:483] Algo bellman_ford step 5629 current loss 0.807008, current_train_items 180160.
I0302 19:00:20.003656 22895071117440 run.py:483] Algo bellman_ford step 5630 current loss 0.351127, current_train_items 180192.
I0302 19:00:20.018864 22895071117440 run.py:483] Algo bellman_ford step 5631 current loss 0.406521, current_train_items 180224.
I0302 19:00:20.041138 22895071117440 run.py:483] Algo bellman_ford step 5632 current loss 0.657528, current_train_items 180256.
I0302 19:00:20.069529 22895071117440 run.py:483] Algo bellman_ford step 5633 current loss 0.652413, current_train_items 180288.
I0302 19:00:20.101008 22895071117440 run.py:483] Algo bellman_ford step 5634 current loss 0.826126, current_train_items 180320.
I0302 19:00:20.118859 22895071117440 run.py:483] Algo bellman_ford step 5635 current loss 0.315602, current_train_items 180352.
I0302 19:00:20.134391 22895071117440 run.py:483] Algo bellman_ford step 5636 current loss 0.501825, current_train_items 180384.
I0302 19:00:20.156040 22895071117440 run.py:483] Algo bellman_ford step 5637 current loss 0.656321, current_train_items 180416.
I0302 19:00:20.183296 22895071117440 run.py:483] Algo bellman_ford step 5638 current loss 0.664644, current_train_items 180448.
I0302 19:00:20.212801 22895071117440 run.py:483] Algo bellman_ford step 5639 current loss 0.726807, current_train_items 180480.
I0302 19:00:20.231077 22895071117440 run.py:483] Algo bellman_ford step 5640 current loss 0.283990, current_train_items 180512.
I0302 19:00:20.246243 22895071117440 run.py:483] Algo bellman_ford step 5641 current loss 0.414804, current_train_items 180544.
I0302 19:00:20.267970 22895071117440 run.py:483] Algo bellman_ford step 5642 current loss 0.613498, current_train_items 180576.
I0302 19:00:20.296413 22895071117440 run.py:483] Algo bellman_ford step 5643 current loss 0.795492, current_train_items 180608.
I0302 19:00:20.326636 22895071117440 run.py:483] Algo bellman_ford step 5644 current loss 0.798957, current_train_items 180640.
I0302 19:00:20.344526 22895071117440 run.py:483] Algo bellman_ford step 5645 current loss 0.302894, current_train_items 180672.
I0302 19:00:20.360399 22895071117440 run.py:483] Algo bellman_ford step 5646 current loss 0.511875, current_train_items 180704.
I0302 19:00:20.381671 22895071117440 run.py:483] Algo bellman_ford step 5647 current loss 0.666572, current_train_items 180736.
I0302 19:00:20.410006 22895071117440 run.py:483] Algo bellman_ford step 5648 current loss 0.686383, current_train_items 180768.
I0302 19:00:20.442600 22895071117440 run.py:483] Algo bellman_ford step 5649 current loss 0.990703, current_train_items 180800.
I0302 19:00:20.460536 22895071117440 run.py:483] Algo bellman_ford step 5650 current loss 0.285388, current_train_items 180832.
I0302 19:00:20.468601 22895071117440 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:00:20.468706 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:00:20.484875 22895071117440 run.py:483] Algo bellman_ford step 5651 current loss 0.490530, current_train_items 180864.
I0302 19:00:20.506774 22895071117440 run.py:483] Algo bellman_ford step 5652 current loss 0.662383, current_train_items 180896.
I0302 19:00:20.537080 22895071117440 run.py:483] Algo bellman_ford step 5653 current loss 0.808724, current_train_items 180928.
I0302 19:00:20.569831 22895071117440 run.py:483] Algo bellman_ford step 5654 current loss 0.906488, current_train_items 180960.
I0302 19:00:20.588650 22895071117440 run.py:483] Algo bellman_ford step 5655 current loss 0.309781, current_train_items 180992.
I0302 19:00:20.604364 22895071117440 run.py:483] Algo bellman_ford step 5656 current loss 0.519759, current_train_items 181024.
I0302 19:00:20.625895 22895071117440 run.py:483] Algo bellman_ford step 5657 current loss 0.605408, current_train_items 181056.
I0302 19:00:20.655187 22895071117440 run.py:483] Algo bellman_ford step 5658 current loss 0.696810, current_train_items 181088.
I0302 19:00:20.686203 22895071117440 run.py:483] Algo bellman_ford step 5659 current loss 0.819886, current_train_items 181120.
I0302 19:00:20.704682 22895071117440 run.py:483] Algo bellman_ford step 5660 current loss 0.357380, current_train_items 181152.
I0302 19:00:20.720147 22895071117440 run.py:483] Algo bellman_ford step 5661 current loss 0.407784, current_train_items 181184.
I0302 19:00:20.741009 22895071117440 run.py:483] Algo bellman_ford step 5662 current loss 0.612858, current_train_items 181216.
I0302 19:00:20.771529 22895071117440 run.py:483] Algo bellman_ford step 5663 current loss 0.833078, current_train_items 181248.
I0302 19:00:20.803171 22895071117440 run.py:483] Algo bellman_ford step 5664 current loss 0.732308, current_train_items 181280.
I0302 19:00:20.821253 22895071117440 run.py:483] Algo bellman_ford step 5665 current loss 0.313060, current_train_items 181312.
I0302 19:00:20.836988 22895071117440 run.py:483] Algo bellman_ford step 5666 current loss 0.448894, current_train_items 181344.
I0302 19:00:20.858634 22895071117440 run.py:483] Algo bellman_ford step 5667 current loss 0.576579, current_train_items 181376.
I0302 19:00:20.888243 22895071117440 run.py:483] Algo bellman_ford step 5668 current loss 0.719339, current_train_items 181408.
I0302 19:00:20.918449 22895071117440 run.py:483] Algo bellman_ford step 5669 current loss 0.706407, current_train_items 181440.
I0302 19:00:20.936865 22895071117440 run.py:483] Algo bellman_ford step 5670 current loss 0.261898, current_train_items 181472.
I0302 19:00:20.952461 22895071117440 run.py:483] Algo bellman_ford step 5671 current loss 0.416429, current_train_items 181504.
I0302 19:00:20.973468 22895071117440 run.py:483] Algo bellman_ford step 5672 current loss 0.625382, current_train_items 181536.
I0302 19:00:21.001599 22895071117440 run.py:483] Algo bellman_ford step 5673 current loss 0.711928, current_train_items 181568.
I0302 19:00:21.031988 22895071117440 run.py:483] Algo bellman_ford step 5674 current loss 0.647629, current_train_items 181600.
I0302 19:00:21.049858 22895071117440 run.py:483] Algo bellman_ford step 5675 current loss 0.310362, current_train_items 181632.
I0302 19:00:21.065330 22895071117440 run.py:483] Algo bellman_ford step 5676 current loss 0.432955, current_train_items 181664.
I0302 19:00:21.087112 22895071117440 run.py:483] Algo bellman_ford step 5677 current loss 0.685710, current_train_items 181696.
I0302 19:00:21.116942 22895071117440 run.py:483] Algo bellman_ford step 5678 current loss 0.869358, current_train_items 181728.
I0302 19:00:21.147516 22895071117440 run.py:483] Algo bellman_ford step 5679 current loss 0.907312, current_train_items 181760.
I0302 19:00:21.165895 22895071117440 run.py:483] Algo bellman_ford step 5680 current loss 0.305532, current_train_items 181792.
I0302 19:00:21.181474 22895071117440 run.py:483] Algo bellman_ford step 5681 current loss 0.440154, current_train_items 181824.
I0302 19:00:21.203500 22895071117440 run.py:483] Algo bellman_ford step 5682 current loss 0.651152, current_train_items 181856.
I0302 19:00:21.231703 22895071117440 run.py:483] Algo bellman_ford step 5683 current loss 0.656384, current_train_items 181888.
I0302 19:00:21.261703 22895071117440 run.py:483] Algo bellman_ford step 5684 current loss 0.701400, current_train_items 181920.
I0302 19:00:21.280336 22895071117440 run.py:483] Algo bellman_ford step 5685 current loss 0.289488, current_train_items 181952.
I0302 19:00:21.296016 22895071117440 run.py:483] Algo bellman_ford step 5686 current loss 0.539087, current_train_items 181984.
I0302 19:00:21.317862 22895071117440 run.py:483] Algo bellman_ford step 5687 current loss 0.638217, current_train_items 182016.
I0302 19:00:21.346662 22895071117440 run.py:483] Algo bellman_ford step 5688 current loss 0.666965, current_train_items 182048.
I0302 19:00:21.377147 22895071117440 run.py:483] Algo bellman_ford step 5689 current loss 0.889140, current_train_items 182080.
I0302 19:00:21.395580 22895071117440 run.py:483] Algo bellman_ford step 5690 current loss 0.278054, current_train_items 182112.
I0302 19:00:21.411852 22895071117440 run.py:483] Algo bellman_ford step 5691 current loss 0.486440, current_train_items 182144.
I0302 19:00:21.433389 22895071117440 run.py:483] Algo bellman_ford step 5692 current loss 0.648423, current_train_items 182176.
I0302 19:00:21.461589 22895071117440 run.py:483] Algo bellman_ford step 5693 current loss 0.739301, current_train_items 182208.
I0302 19:00:21.491506 22895071117440 run.py:483] Algo bellman_ford step 5694 current loss 0.735505, current_train_items 182240.
I0302 19:00:21.509602 22895071117440 run.py:483] Algo bellman_ford step 5695 current loss 0.249945, current_train_items 182272.
I0302 19:00:21.525366 22895071117440 run.py:483] Algo bellman_ford step 5696 current loss 0.498157, current_train_items 182304.
I0302 19:00:21.547184 22895071117440 run.py:483] Algo bellman_ford step 5697 current loss 0.702582, current_train_items 182336.
I0302 19:00:21.576738 22895071117440 run.py:483] Algo bellman_ford step 5698 current loss 0.774795, current_train_items 182368.
I0302 19:00:21.608068 22895071117440 run.py:483] Algo bellman_ford step 5699 current loss 0.826156, current_train_items 182400.
I0302 19:00:21.626312 22895071117440 run.py:483] Algo bellman_ford step 5700 current loss 0.368885, current_train_items 182432.
I0302 19:00:21.634213 22895071117440 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:00:21.634316 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:21.649883 22895071117440 run.py:483] Algo bellman_ford step 5701 current loss 0.495836, current_train_items 182464.
I0302 19:00:21.671402 22895071117440 run.py:483] Algo bellman_ford step 5702 current loss 0.589543, current_train_items 182496.
I0302 19:00:21.700725 22895071117440 run.py:483] Algo bellman_ford step 5703 current loss 0.689468, current_train_items 182528.
I0302 19:00:21.733615 22895071117440 run.py:483] Algo bellman_ford step 5704 current loss 0.925775, current_train_items 182560.
I0302 19:00:21.752292 22895071117440 run.py:483] Algo bellman_ford step 5705 current loss 0.294236, current_train_items 182592.
I0302 19:00:21.767530 22895071117440 run.py:483] Algo bellman_ford step 5706 current loss 0.433681, current_train_items 182624.
I0302 19:00:21.789830 22895071117440 run.py:483] Algo bellman_ford step 5707 current loss 0.725953, current_train_items 182656.
I0302 19:00:21.818022 22895071117440 run.py:483] Algo bellman_ford step 5708 current loss 0.774872, current_train_items 182688.
I0302 19:00:21.849147 22895071117440 run.py:483] Algo bellman_ford step 5709 current loss 0.947641, current_train_items 182720.
I0302 19:00:21.867275 22895071117440 run.py:483] Algo bellman_ford step 5710 current loss 0.283140, current_train_items 182752.
I0302 19:00:21.882502 22895071117440 run.py:483] Algo bellman_ford step 5711 current loss 0.533236, current_train_items 182784.
I0302 19:00:21.903616 22895071117440 run.py:483] Algo bellman_ford step 5712 current loss 0.622290, current_train_items 182816.
I0302 19:00:21.931634 22895071117440 run.py:483] Algo bellman_ford step 5713 current loss 0.667404, current_train_items 182848.
I0302 19:00:21.962076 22895071117440 run.py:483] Algo bellman_ford step 5714 current loss 0.746286, current_train_items 182880.
I0302 19:00:21.980302 22895071117440 run.py:483] Algo bellman_ford step 5715 current loss 0.341285, current_train_items 182912.
I0302 19:00:21.995670 22895071117440 run.py:483] Algo bellman_ford step 5716 current loss 0.536830, current_train_items 182944.
I0302 19:00:22.017904 22895071117440 run.py:483] Algo bellman_ford step 5717 current loss 0.698579, current_train_items 182976.
I0302 19:00:22.045822 22895071117440 run.py:483] Algo bellman_ford step 5718 current loss 0.875824, current_train_items 183008.
I0302 19:00:22.077013 22895071117440 run.py:483] Algo bellman_ford step 5719 current loss 0.846811, current_train_items 183040.
I0302 19:00:22.095506 22895071117440 run.py:483] Algo bellman_ford step 5720 current loss 0.297154, current_train_items 183072.
I0302 19:00:22.110934 22895071117440 run.py:483] Algo bellman_ford step 5721 current loss 0.506001, current_train_items 183104.
I0302 19:00:22.132087 22895071117440 run.py:483] Algo bellman_ford step 5722 current loss 0.696480, current_train_items 183136.
I0302 19:00:22.160222 22895071117440 run.py:483] Algo bellman_ford step 5723 current loss 0.820354, current_train_items 183168.
I0302 19:00:22.190567 22895071117440 run.py:483] Algo bellman_ford step 5724 current loss 0.873294, current_train_items 183200.
I0302 19:00:22.208692 22895071117440 run.py:483] Algo bellman_ford step 5725 current loss 0.322862, current_train_items 183232.
I0302 19:00:22.224066 22895071117440 run.py:483] Algo bellman_ford step 5726 current loss 0.514182, current_train_items 183264.
I0302 19:00:22.246697 22895071117440 run.py:483] Algo bellman_ford step 5727 current loss 0.647707, current_train_items 183296.
I0302 19:00:22.274357 22895071117440 run.py:483] Algo bellman_ford step 5728 current loss 0.666392, current_train_items 183328.
I0302 19:00:22.305523 22895071117440 run.py:483] Algo bellman_ford step 5729 current loss 0.748364, current_train_items 183360.
I0302 19:00:22.323821 22895071117440 run.py:483] Algo bellman_ford step 5730 current loss 0.313293, current_train_items 183392.
I0302 19:00:22.339580 22895071117440 run.py:483] Algo bellman_ford step 5731 current loss 0.542847, current_train_items 183424.
I0302 19:00:22.361953 22895071117440 run.py:483] Algo bellman_ford step 5732 current loss 0.681173, current_train_items 183456.
I0302 19:00:22.389997 22895071117440 run.py:483] Algo bellman_ford step 5733 current loss 0.738107, current_train_items 183488.
I0302 19:00:22.421399 22895071117440 run.py:483] Algo bellman_ford step 5734 current loss 0.808133, current_train_items 183520.
I0302 19:00:22.439272 22895071117440 run.py:483] Algo bellman_ford step 5735 current loss 0.292323, current_train_items 183552.
I0302 19:00:22.454935 22895071117440 run.py:483] Algo bellman_ford step 5736 current loss 0.540611, current_train_items 183584.
I0302 19:00:22.476846 22895071117440 run.py:483] Algo bellman_ford step 5737 current loss 0.696205, current_train_items 183616.
I0302 19:00:22.504851 22895071117440 run.py:483] Algo bellman_ford step 5738 current loss 0.690165, current_train_items 183648.
I0302 19:00:22.536388 22895071117440 run.py:483] Algo bellman_ford step 5739 current loss 0.731137, current_train_items 183680.
I0302 19:00:22.554146 22895071117440 run.py:483] Algo bellman_ford step 5740 current loss 0.359938, current_train_items 183712.
I0302 19:00:22.569905 22895071117440 run.py:483] Algo bellman_ford step 5741 current loss 0.456816, current_train_items 183744.
I0302 19:00:22.592473 22895071117440 run.py:483] Algo bellman_ford step 5742 current loss 0.582952, current_train_items 183776.
I0302 19:00:22.621604 22895071117440 run.py:483] Algo bellman_ford step 5743 current loss 0.698873, current_train_items 183808.
I0302 19:00:22.653244 22895071117440 run.py:483] Algo bellman_ford step 5744 current loss 0.823655, current_train_items 183840.
I0302 19:00:22.671575 22895071117440 run.py:483] Algo bellman_ford step 5745 current loss 0.329305, current_train_items 183872.
I0302 19:00:22.687386 22895071117440 run.py:483] Algo bellman_ford step 5746 current loss 0.504665, current_train_items 183904.
I0302 19:00:22.709913 22895071117440 run.py:483] Algo bellman_ford step 5747 current loss 0.704971, current_train_items 183936.
I0302 19:00:22.738310 22895071117440 run.py:483] Algo bellman_ford step 5748 current loss 0.855685, current_train_items 183968.
I0302 19:00:22.768955 22895071117440 run.py:483] Algo bellman_ford step 5749 current loss 0.777197, current_train_items 184000.
I0302 19:00:22.787031 22895071117440 run.py:483] Algo bellman_ford step 5750 current loss 0.367622, current_train_items 184032.
I0302 19:00:22.794983 22895071117440 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:00:22.795087 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:00:22.811551 22895071117440 run.py:483] Algo bellman_ford step 5751 current loss 0.479192, current_train_items 184064.
I0302 19:00:22.833273 22895071117440 run.py:483] Algo bellman_ford step 5752 current loss 0.712559, current_train_items 184096.
I0302 19:00:22.861126 22895071117440 run.py:483] Algo bellman_ford step 5753 current loss 0.644137, current_train_items 184128.
I0302 19:00:22.893950 22895071117440 run.py:483] Algo bellman_ford step 5754 current loss 1.059924, current_train_items 184160.
I0302 19:00:22.912601 22895071117440 run.py:483] Algo bellman_ford step 5755 current loss 0.273597, current_train_items 184192.
I0302 19:00:22.927724 22895071117440 run.py:483] Algo bellman_ford step 5756 current loss 0.421334, current_train_items 184224.
I0302 19:00:22.949906 22895071117440 run.py:483] Algo bellman_ford step 5757 current loss 0.718675, current_train_items 184256.
I0302 19:00:22.979472 22895071117440 run.py:483] Algo bellman_ford step 5758 current loss 0.728563, current_train_items 184288.
I0302 19:00:23.010379 22895071117440 run.py:483] Algo bellman_ford step 5759 current loss 0.745186, current_train_items 184320.
I0302 19:00:23.028535 22895071117440 run.py:483] Algo bellman_ford step 5760 current loss 0.390242, current_train_items 184352.
I0302 19:00:23.044443 22895071117440 run.py:483] Algo bellman_ford step 5761 current loss 0.449771, current_train_items 184384.
I0302 19:00:23.066010 22895071117440 run.py:483] Algo bellman_ford step 5762 current loss 0.539192, current_train_items 184416.
I0302 19:00:23.093769 22895071117440 run.py:483] Algo bellman_ford step 5763 current loss 0.693795, current_train_items 184448.
I0302 19:00:23.125325 22895071117440 run.py:483] Algo bellman_ford step 5764 current loss 0.780588, current_train_items 184480.
I0302 19:00:23.143876 22895071117440 run.py:483] Algo bellman_ford step 5765 current loss 0.383607, current_train_items 184512.
I0302 19:00:23.159304 22895071117440 run.py:483] Algo bellman_ford step 5766 current loss 0.516364, current_train_items 184544.
I0302 19:00:23.180822 22895071117440 run.py:483] Algo bellman_ford step 5767 current loss 0.651991, current_train_items 184576.
I0302 19:00:23.210021 22895071117440 run.py:483] Algo bellman_ford step 5768 current loss 0.770668, current_train_items 184608.
I0302 19:00:23.240497 22895071117440 run.py:483] Algo bellman_ford step 5769 current loss 0.840691, current_train_items 184640.
I0302 19:00:23.258883 22895071117440 run.py:483] Algo bellman_ford step 5770 current loss 0.276786, current_train_items 184672.
I0302 19:00:23.274548 22895071117440 run.py:483] Algo bellman_ford step 5771 current loss 0.468294, current_train_items 184704.
I0302 19:00:23.294960 22895071117440 run.py:483] Algo bellman_ford step 5772 current loss 0.606041, current_train_items 184736.
I0302 19:00:23.322859 22895071117440 run.py:483] Algo bellman_ford step 5773 current loss 0.818351, current_train_items 184768.
I0302 19:00:23.352781 22895071117440 run.py:483] Algo bellman_ford step 5774 current loss 0.686999, current_train_items 184800.
I0302 19:00:23.371147 22895071117440 run.py:483] Algo bellman_ford step 5775 current loss 0.376982, current_train_items 184832.
I0302 19:00:23.387144 22895071117440 run.py:483] Algo bellman_ford step 5776 current loss 0.406321, current_train_items 184864.
I0302 19:00:23.408286 22895071117440 run.py:483] Algo bellman_ford step 5777 current loss 0.611524, current_train_items 184896.
I0302 19:00:23.437445 22895071117440 run.py:483] Algo bellman_ford step 5778 current loss 0.726929, current_train_items 184928.
I0302 19:00:23.468479 22895071117440 run.py:483] Algo bellman_ford step 5779 current loss 0.842349, current_train_items 184960.
I0302 19:00:23.486269 22895071117440 run.py:483] Algo bellman_ford step 5780 current loss 0.272135, current_train_items 184992.
I0302 19:00:23.501616 22895071117440 run.py:483] Algo bellman_ford step 5781 current loss 0.457192, current_train_items 185024.
I0302 19:00:23.523857 22895071117440 run.py:483] Algo bellman_ford step 5782 current loss 0.666358, current_train_items 185056.
I0302 19:00:23.551978 22895071117440 run.py:483] Algo bellman_ford step 5783 current loss 0.616042, current_train_items 185088.
I0302 19:00:23.580851 22895071117440 run.py:483] Algo bellman_ford step 5784 current loss 0.745212, current_train_items 185120.
I0302 19:00:23.599308 22895071117440 run.py:483] Algo bellman_ford step 5785 current loss 0.357241, current_train_items 185152.
I0302 19:00:23.614767 22895071117440 run.py:483] Algo bellman_ford step 5786 current loss 0.474770, current_train_items 185184.
I0302 19:00:23.635399 22895071117440 run.py:483] Algo bellman_ford step 5787 current loss 0.559142, current_train_items 185216.
I0302 19:00:23.665003 22895071117440 run.py:483] Algo bellman_ford step 5788 current loss 0.743963, current_train_items 185248.
I0302 19:00:23.695417 22895071117440 run.py:483] Algo bellman_ford step 5789 current loss 0.821393, current_train_items 185280.
I0302 19:00:23.714209 22895071117440 run.py:483] Algo bellman_ford step 5790 current loss 0.268110, current_train_items 185312.
I0302 19:00:23.730617 22895071117440 run.py:483] Algo bellman_ford step 5791 current loss 0.510283, current_train_items 185344.
I0302 19:00:23.750955 22895071117440 run.py:483] Algo bellman_ford step 5792 current loss 0.605841, current_train_items 185376.
I0302 19:00:23.779785 22895071117440 run.py:483] Algo bellman_ford step 5793 current loss 0.712837, current_train_items 185408.
I0302 19:00:23.810996 22895071117440 run.py:483] Algo bellman_ford step 5794 current loss 0.720846, current_train_items 185440.
I0302 19:00:23.829125 22895071117440 run.py:483] Algo bellman_ford step 5795 current loss 0.308930, current_train_items 185472.
I0302 19:00:23.844317 22895071117440 run.py:483] Algo bellman_ford step 5796 current loss 0.499548, current_train_items 185504.
I0302 19:00:23.866682 22895071117440 run.py:483] Algo bellman_ford step 5797 current loss 0.743089, current_train_items 185536.
I0302 19:00:23.895272 22895071117440 run.py:483] Algo bellman_ford step 5798 current loss 0.652878, current_train_items 185568.
I0302 19:00:23.927243 22895071117440 run.py:483] Algo bellman_ford step 5799 current loss 0.944718, current_train_items 185600.
I0302 19:00:23.945666 22895071117440 run.py:483] Algo bellman_ford step 5800 current loss 0.317535, current_train_items 185632.
I0302 19:00:23.953472 22895071117440 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:00:23.953577 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:00:23.970105 22895071117440 run.py:483] Algo bellman_ford step 5801 current loss 0.594582, current_train_items 185664.
I0302 19:00:23.991216 22895071117440 run.py:483] Algo bellman_ford step 5802 current loss 0.596917, current_train_items 185696.
I0302 19:00:24.020332 22895071117440 run.py:483] Algo bellman_ford step 5803 current loss 0.775732, current_train_items 185728.
I0302 19:00:24.053191 22895071117440 run.py:483] Algo bellman_ford step 5804 current loss 0.764485, current_train_items 185760.
I0302 19:00:24.071512 22895071117440 run.py:483] Algo bellman_ford step 5805 current loss 0.312277, current_train_items 185792.
I0302 19:00:24.087058 22895071117440 run.py:483] Algo bellman_ford step 5806 current loss 0.430185, current_train_items 185824.
I0302 19:00:24.109569 22895071117440 run.py:483] Algo bellman_ford step 5807 current loss 0.609383, current_train_items 185856.
I0302 19:00:24.138047 22895071117440 run.py:483] Algo bellman_ford step 5808 current loss 0.691337, current_train_items 185888.
I0302 19:00:24.168338 22895071117440 run.py:483] Algo bellman_ford step 5809 current loss 0.680313, current_train_items 185920.
I0302 19:00:24.186221 22895071117440 run.py:483] Algo bellman_ford step 5810 current loss 0.274530, current_train_items 185952.
I0302 19:00:24.201399 22895071117440 run.py:483] Algo bellman_ford step 5811 current loss 0.435454, current_train_items 185984.
I0302 19:00:24.223669 22895071117440 run.py:483] Algo bellman_ford step 5812 current loss 0.602243, current_train_items 186016.
I0302 19:00:24.253154 22895071117440 run.py:483] Algo bellman_ford step 5813 current loss 0.731193, current_train_items 186048.
I0302 19:00:24.284802 22895071117440 run.py:483] Algo bellman_ford step 5814 current loss 0.823498, current_train_items 186080.
I0302 19:00:24.302763 22895071117440 run.py:483] Algo bellman_ford step 5815 current loss 0.313813, current_train_items 186112.
I0302 19:00:24.318338 22895071117440 run.py:483] Algo bellman_ford step 5816 current loss 0.459125, current_train_items 186144.
I0302 19:00:24.340056 22895071117440 run.py:483] Algo bellman_ford step 5817 current loss 0.682003, current_train_items 186176.
I0302 19:00:24.367585 22895071117440 run.py:483] Algo bellman_ford step 5818 current loss 0.645898, current_train_items 186208.
I0302 19:00:24.399318 22895071117440 run.py:483] Algo bellman_ford step 5819 current loss 0.773664, current_train_items 186240.
I0302 19:00:24.417006 22895071117440 run.py:483] Algo bellman_ford step 5820 current loss 0.343561, current_train_items 186272.
I0302 19:00:24.432270 22895071117440 run.py:483] Algo bellman_ford step 5821 current loss 0.431538, current_train_items 186304.
I0302 19:00:24.453935 22895071117440 run.py:483] Algo bellman_ford step 5822 current loss 0.613483, current_train_items 186336.
I0302 19:00:24.482484 22895071117440 run.py:483] Algo bellman_ford step 5823 current loss 0.670812, current_train_items 186368.
I0302 19:00:24.513092 22895071117440 run.py:483] Algo bellman_ford step 5824 current loss 0.813746, current_train_items 186400.
I0302 19:00:24.530968 22895071117440 run.py:483] Algo bellman_ford step 5825 current loss 0.316462, current_train_items 186432.
I0302 19:00:24.546471 22895071117440 run.py:483] Algo bellman_ford step 5826 current loss 0.489204, current_train_items 186464.
I0302 19:00:24.568501 22895071117440 run.py:483] Algo bellman_ford step 5827 current loss 0.676286, current_train_items 186496.
I0302 19:00:24.598252 22895071117440 run.py:483] Algo bellman_ford step 5828 current loss 0.700436, current_train_items 186528.
I0302 19:00:24.630415 22895071117440 run.py:483] Algo bellman_ford step 5829 current loss 0.932969, current_train_items 186560.
I0302 19:00:24.648386 22895071117440 run.py:483] Algo bellman_ford step 5830 current loss 0.328906, current_train_items 186592.
I0302 19:00:24.663952 22895071117440 run.py:483] Algo bellman_ford step 5831 current loss 0.504441, current_train_items 186624.
I0302 19:00:24.685032 22895071117440 run.py:483] Algo bellman_ford step 5832 current loss 0.676953, current_train_items 186656.
I0302 19:00:24.713561 22895071117440 run.py:483] Algo bellman_ford step 5833 current loss 0.749357, current_train_items 186688.
I0302 19:00:24.741024 22895071117440 run.py:483] Algo bellman_ford step 5834 current loss 0.616170, current_train_items 186720.
I0302 19:00:24.758955 22895071117440 run.py:483] Algo bellman_ford step 5835 current loss 0.295796, current_train_items 186752.
I0302 19:00:24.773712 22895071117440 run.py:483] Algo bellman_ford step 5836 current loss 0.479158, current_train_items 186784.
I0302 19:00:24.796841 22895071117440 run.py:483] Algo bellman_ford step 5837 current loss 0.910595, current_train_items 186816.
I0302 19:00:24.823739 22895071117440 run.py:483] Algo bellman_ford step 5838 current loss 0.771043, current_train_items 186848.
I0302 19:00:24.855732 22895071117440 run.py:483] Algo bellman_ford step 5839 current loss 1.547959, current_train_items 186880.
I0302 19:00:24.874003 22895071117440 run.py:483] Algo bellman_ford step 5840 current loss 0.342645, current_train_items 186912.
I0302 19:00:24.889700 22895071117440 run.py:483] Algo bellman_ford step 5841 current loss 0.514583, current_train_items 186944.
I0302 19:00:24.911182 22895071117440 run.py:483] Algo bellman_ford step 5842 current loss 0.773058, current_train_items 186976.
I0302 19:00:24.938189 22895071117440 run.py:483] Algo bellman_ford step 5843 current loss 0.771288, current_train_items 187008.
I0302 19:00:24.968127 22895071117440 run.py:483] Algo bellman_ford step 5844 current loss 0.905481, current_train_items 187040.
I0302 19:00:24.986128 22895071117440 run.py:483] Algo bellman_ford step 5845 current loss 0.369641, current_train_items 187072.
I0302 19:00:25.001571 22895071117440 run.py:483] Algo bellman_ford step 5846 current loss 0.456646, current_train_items 187104.
I0302 19:00:25.024470 22895071117440 run.py:483] Algo bellman_ford step 5847 current loss 0.772812, current_train_items 187136.
I0302 19:00:25.051517 22895071117440 run.py:483] Algo bellman_ford step 5848 current loss 0.669929, current_train_items 187168.
I0302 19:00:25.082570 22895071117440 run.py:483] Algo bellman_ford step 5849 current loss 0.915286, current_train_items 187200.
I0302 19:00:25.100178 22895071117440 run.py:483] Algo bellman_ford step 5850 current loss 0.255057, current_train_items 187232.
I0302 19:00:25.108115 22895071117440 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:00:25.108222 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:00:25.124674 22895071117440 run.py:483] Algo bellman_ford step 5851 current loss 0.471222, current_train_items 187264.
I0302 19:00:25.146622 22895071117440 run.py:483] Algo bellman_ford step 5852 current loss 0.647312, current_train_items 187296.
I0302 19:00:25.174660 22895071117440 run.py:483] Algo bellman_ford step 5853 current loss 0.661424, current_train_items 187328.
I0302 19:00:25.206645 22895071117440 run.py:483] Algo bellman_ford step 5854 current loss 0.834777, current_train_items 187360.
I0302 19:00:25.225305 22895071117440 run.py:483] Algo bellman_ford step 5855 current loss 0.363154, current_train_items 187392.
I0302 19:00:25.240646 22895071117440 run.py:483] Algo bellman_ford step 5856 current loss 0.642695, current_train_items 187424.
I0302 19:00:25.263103 22895071117440 run.py:483] Algo bellman_ford step 5857 current loss 0.713646, current_train_items 187456.
I0302 19:00:25.290678 22895071117440 run.py:483] Algo bellman_ford step 5858 current loss 0.614113, current_train_items 187488.
I0302 19:00:25.321520 22895071117440 run.py:483] Algo bellman_ford step 5859 current loss 0.763333, current_train_items 187520.
I0302 19:00:25.340156 22895071117440 run.py:483] Algo bellman_ford step 5860 current loss 0.273122, current_train_items 187552.
I0302 19:00:25.356141 22895071117440 run.py:483] Algo bellman_ford step 5861 current loss 0.420803, current_train_items 187584.
I0302 19:00:25.377034 22895071117440 run.py:483] Algo bellman_ford step 5862 current loss 0.651700, current_train_items 187616.
I0302 19:00:25.405690 22895071117440 run.py:483] Algo bellman_ford step 5863 current loss 0.706014, current_train_items 187648.
I0302 19:00:25.437283 22895071117440 run.py:483] Algo bellman_ford step 5864 current loss 0.920136, current_train_items 187680.
I0302 19:00:25.454976 22895071117440 run.py:483] Algo bellman_ford step 5865 current loss 0.385217, current_train_items 187712.
I0302 19:00:25.470915 22895071117440 run.py:483] Algo bellman_ford step 5866 current loss 0.505939, current_train_items 187744.
I0302 19:00:25.492906 22895071117440 run.py:483] Algo bellman_ford step 5867 current loss 0.658702, current_train_items 187776.
I0302 19:00:25.521073 22895071117440 run.py:483] Algo bellman_ford step 5868 current loss 0.802739, current_train_items 187808.
I0302 19:00:25.553437 22895071117440 run.py:483] Algo bellman_ford step 5869 current loss 0.947441, current_train_items 187840.
I0302 19:00:25.572034 22895071117440 run.py:483] Algo bellman_ford step 5870 current loss 0.287262, current_train_items 187872.
I0302 19:00:25.587788 22895071117440 run.py:483] Algo bellman_ford step 5871 current loss 0.467303, current_train_items 187904.
I0302 19:00:25.608320 22895071117440 run.py:483] Algo bellman_ford step 5872 current loss 0.586753, current_train_items 187936.
I0302 19:00:25.636212 22895071117440 run.py:483] Algo bellman_ford step 5873 current loss 0.667655, current_train_items 187968.
I0302 19:00:25.667548 22895071117440 run.py:483] Algo bellman_ford step 5874 current loss 0.851146, current_train_items 188000.
I0302 19:00:25.685889 22895071117440 run.py:483] Algo bellman_ford step 5875 current loss 0.315633, current_train_items 188032.
I0302 19:00:25.701582 22895071117440 run.py:483] Algo bellman_ford step 5876 current loss 0.506451, current_train_items 188064.
I0302 19:00:25.722484 22895071117440 run.py:483] Algo bellman_ford step 5877 current loss 0.626928, current_train_items 188096.
I0302 19:00:25.750529 22895071117440 run.py:483] Algo bellman_ford step 5878 current loss 0.698610, current_train_items 188128.
I0302 19:00:25.781996 22895071117440 run.py:483] Algo bellman_ford step 5879 current loss 0.885516, current_train_items 188160.
I0302 19:00:25.800005 22895071117440 run.py:483] Algo bellman_ford step 5880 current loss 0.297625, current_train_items 188192.
I0302 19:00:25.815164 22895071117440 run.py:483] Algo bellman_ford step 5881 current loss 0.506136, current_train_items 188224.
I0302 19:00:25.837180 22895071117440 run.py:483] Algo bellman_ford step 5882 current loss 0.645696, current_train_items 188256.
I0302 19:00:25.865249 22895071117440 run.py:483] Algo bellman_ford step 5883 current loss 0.644020, current_train_items 188288.
I0302 19:00:25.897420 22895071117440 run.py:483] Algo bellman_ford step 5884 current loss 0.718959, current_train_items 188320.
I0302 19:00:25.916332 22895071117440 run.py:483] Algo bellman_ford step 5885 current loss 0.317248, current_train_items 188352.
I0302 19:00:25.931839 22895071117440 run.py:483] Algo bellman_ford step 5886 current loss 0.475469, current_train_items 188384.
I0302 19:00:25.953320 22895071117440 run.py:483] Algo bellman_ford step 5887 current loss 0.592623, current_train_items 188416.
I0302 19:00:25.980841 22895071117440 run.py:483] Algo bellman_ford step 5888 current loss 0.624872, current_train_items 188448.
I0302 19:00:26.010610 22895071117440 run.py:483] Algo bellman_ford step 5889 current loss 0.742808, current_train_items 188480.
I0302 19:00:26.029377 22895071117440 run.py:483] Algo bellman_ford step 5890 current loss 0.366885, current_train_items 188512.
I0302 19:00:26.044981 22895071117440 run.py:483] Algo bellman_ford step 5891 current loss 0.626908, current_train_items 188544.
I0302 19:00:26.066517 22895071117440 run.py:483] Algo bellman_ford step 5892 current loss 0.696592, current_train_items 188576.
I0302 19:00:26.093635 22895071117440 run.py:483] Algo bellman_ford step 5893 current loss 0.594353, current_train_items 188608.
I0302 19:00:26.122539 22895071117440 run.py:483] Algo bellman_ford step 5894 current loss 0.657500, current_train_items 188640.
I0302 19:00:26.140514 22895071117440 run.py:483] Algo bellman_ford step 5895 current loss 0.293244, current_train_items 188672.
I0302 19:00:26.156154 22895071117440 run.py:483] Algo bellman_ford step 5896 current loss 0.519034, current_train_items 188704.
I0302 19:00:26.178619 22895071117440 run.py:483] Algo bellman_ford step 5897 current loss 0.641400, current_train_items 188736.
I0302 19:00:26.207451 22895071117440 run.py:483] Algo bellman_ford step 5898 current loss 0.731591, current_train_items 188768.
I0302 19:00:26.237871 22895071117440 run.py:483] Algo bellman_ford step 5899 current loss 0.791867, current_train_items 188800.
I0302 19:00:26.256193 22895071117440 run.py:483] Algo bellman_ford step 5900 current loss 0.299189, current_train_items 188832.
I0302 19:00:26.263924 22895071117440 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:00:26.264032 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:26.279694 22895071117440 run.py:483] Algo bellman_ford step 5901 current loss 0.441745, current_train_items 188864.
I0302 19:00:26.302299 22895071117440 run.py:483] Algo bellman_ford step 5902 current loss 0.569286, current_train_items 188896.
I0302 19:00:26.331373 22895071117440 run.py:483] Algo bellman_ford step 5903 current loss 0.663584, current_train_items 188928.
I0302 19:00:26.360795 22895071117440 run.py:483] Algo bellman_ford step 5904 current loss 0.759115, current_train_items 188960.
I0302 19:00:26.379186 22895071117440 run.py:483] Algo bellman_ford step 5905 current loss 0.316732, current_train_items 188992.
I0302 19:00:26.394971 22895071117440 run.py:483] Algo bellman_ford step 5906 current loss 0.508490, current_train_items 189024.
I0302 19:00:26.416455 22895071117440 run.py:483] Algo bellman_ford step 5907 current loss 0.562778, current_train_items 189056.
I0302 19:00:26.444877 22895071117440 run.py:483] Algo bellman_ford step 5908 current loss 0.578578, current_train_items 189088.
I0302 19:00:26.474080 22895071117440 run.py:483] Algo bellman_ford step 5909 current loss 0.827907, current_train_items 189120.
I0302 19:00:26.491995 22895071117440 run.py:483] Algo bellman_ford step 5910 current loss 0.346530, current_train_items 189152.
I0302 19:00:26.507546 22895071117440 run.py:483] Algo bellman_ford step 5911 current loss 0.674382, current_train_items 189184.
I0302 19:00:26.529436 22895071117440 run.py:483] Algo bellman_ford step 5912 current loss 0.672056, current_train_items 189216.
I0302 19:00:26.557174 22895071117440 run.py:483] Algo bellman_ford step 5913 current loss 0.640620, current_train_items 189248.
I0302 19:00:26.586889 22895071117440 run.py:483] Algo bellman_ford step 5914 current loss 0.732742, current_train_items 189280.
I0302 19:00:26.604710 22895071117440 run.py:483] Algo bellman_ford step 5915 current loss 0.351654, current_train_items 189312.
I0302 19:00:26.620151 22895071117440 run.py:483] Algo bellman_ford step 5916 current loss 0.434403, current_train_items 189344.
I0302 19:00:26.641522 22895071117440 run.py:483] Algo bellman_ford step 5917 current loss 0.571412, current_train_items 189376.
I0302 19:00:26.669214 22895071117440 run.py:483] Algo bellman_ford step 5918 current loss 0.682248, current_train_items 189408.
I0302 19:00:26.700354 22895071117440 run.py:483] Algo bellman_ford step 5919 current loss 0.693148, current_train_items 189440.
I0302 19:00:26.718245 22895071117440 run.py:483] Algo bellman_ford step 5920 current loss 0.325542, current_train_items 189472.
I0302 19:00:26.733559 22895071117440 run.py:483] Algo bellman_ford step 5921 current loss 0.473564, current_train_items 189504.
I0302 19:00:26.754451 22895071117440 run.py:483] Algo bellman_ford step 5922 current loss 0.567045, current_train_items 189536.
I0302 19:00:26.781228 22895071117440 run.py:483] Algo bellman_ford step 5923 current loss 0.598095, current_train_items 189568.
I0302 19:00:26.811651 22895071117440 run.py:483] Algo bellman_ford step 5924 current loss 0.774337, current_train_items 189600.
I0302 19:00:26.829331 22895071117440 run.py:483] Algo bellman_ford step 5925 current loss 0.265726, current_train_items 189632.
I0302 19:00:26.844311 22895071117440 run.py:483] Algo bellman_ford step 5926 current loss 0.503788, current_train_items 189664.
I0302 19:00:26.865122 22895071117440 run.py:483] Algo bellman_ford step 5927 current loss 0.583387, current_train_items 189696.
I0302 19:00:26.893575 22895071117440 run.py:483] Algo bellman_ford step 5928 current loss 0.752678, current_train_items 189728.
I0302 19:00:26.922842 22895071117440 run.py:483] Algo bellman_ford step 5929 current loss 0.754109, current_train_items 189760.
I0302 19:00:26.940891 22895071117440 run.py:483] Algo bellman_ford step 5930 current loss 0.315463, current_train_items 189792.
I0302 19:00:26.956640 22895071117440 run.py:483] Algo bellman_ford step 5931 current loss 0.460957, current_train_items 189824.
I0302 19:00:26.977915 22895071117440 run.py:483] Algo bellman_ford step 5932 current loss 0.699606, current_train_items 189856.
I0302 19:00:27.004726 22895071117440 run.py:483] Algo bellman_ford step 5933 current loss 0.660655, current_train_items 189888.
I0302 19:00:27.036343 22895071117440 run.py:483] Algo bellman_ford step 5934 current loss 0.837662, current_train_items 189920.
I0302 19:00:27.054141 22895071117440 run.py:483] Algo bellman_ford step 5935 current loss 0.360232, current_train_items 189952.
I0302 19:00:27.069224 22895071117440 run.py:483] Algo bellman_ford step 5936 current loss 0.481005, current_train_items 189984.
I0302 19:00:27.090723 22895071117440 run.py:483] Algo bellman_ford step 5937 current loss 0.792013, current_train_items 190016.
I0302 19:00:27.119505 22895071117440 run.py:483] Algo bellman_ford step 5938 current loss 0.749853, current_train_items 190048.
I0302 19:00:27.148624 22895071117440 run.py:483] Algo bellman_ford step 5939 current loss 0.818532, current_train_items 190080.
I0302 19:00:27.166782 22895071117440 run.py:483] Algo bellman_ford step 5940 current loss 0.331900, current_train_items 190112.
I0302 19:00:27.181868 22895071117440 run.py:483] Algo bellman_ford step 5941 current loss 0.454677, current_train_items 190144.
I0302 19:00:27.203763 22895071117440 run.py:483] Algo bellman_ford step 5942 current loss 0.648074, current_train_items 190176.
I0302 19:00:27.232085 22895071117440 run.py:483] Algo bellman_ford step 5943 current loss 0.681725, current_train_items 190208.
I0302 19:00:27.262852 22895071117440 run.py:483] Algo bellman_ford step 5944 current loss 0.806871, current_train_items 190240.
I0302 19:00:27.280680 22895071117440 run.py:483] Algo bellman_ford step 5945 current loss 0.235262, current_train_items 190272.
I0302 19:00:27.296236 22895071117440 run.py:483] Algo bellman_ford step 5946 current loss 0.520733, current_train_items 190304.
I0302 19:00:27.318054 22895071117440 run.py:483] Algo bellman_ford step 5947 current loss 0.709109, current_train_items 190336.
I0302 19:00:27.344972 22895071117440 run.py:483] Algo bellman_ford step 5948 current loss 0.717838, current_train_items 190368.
I0302 19:00:27.375580 22895071117440 run.py:483] Algo bellman_ford step 5949 current loss 0.838246, current_train_items 190400.
I0302 19:00:27.393295 22895071117440 run.py:483] Algo bellman_ford step 5950 current loss 0.252938, current_train_items 190432.
I0302 19:00:27.401387 22895071117440 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:00:27.401494 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:27.417436 22895071117440 run.py:483] Algo bellman_ford step 5951 current loss 0.530029, current_train_items 190464.
I0302 19:00:27.439849 22895071117440 run.py:483] Algo bellman_ford step 5952 current loss 0.743269, current_train_items 190496.
I0302 19:00:27.468770 22895071117440 run.py:483] Algo bellman_ford step 5953 current loss 0.786787, current_train_items 190528.
I0302 19:00:27.499845 22895071117440 run.py:483] Algo bellman_ford step 5954 current loss 0.765688, current_train_items 190560.
I0302 19:00:27.518957 22895071117440 run.py:483] Algo bellman_ford step 5955 current loss 0.311589, current_train_items 190592.
I0302 19:00:27.534716 22895071117440 run.py:483] Algo bellman_ford step 5956 current loss 0.494193, current_train_items 190624.
I0302 19:00:27.555437 22895071117440 run.py:483] Algo bellman_ford step 5957 current loss 0.607217, current_train_items 190656.
I0302 19:00:27.583562 22895071117440 run.py:483] Algo bellman_ford step 5958 current loss 0.707658, current_train_items 190688.
I0302 19:00:27.614397 22895071117440 run.py:483] Algo bellman_ford step 5959 current loss 0.842946, current_train_items 190720.
I0302 19:00:27.632799 22895071117440 run.py:483] Algo bellman_ford step 5960 current loss 0.237758, current_train_items 190752.
I0302 19:00:27.648508 22895071117440 run.py:483] Algo bellman_ford step 5961 current loss 0.525488, current_train_items 190784.
I0302 19:00:27.670620 22895071117440 run.py:483] Algo bellman_ford step 5962 current loss 0.602628, current_train_items 190816.
I0302 19:00:27.697222 22895071117440 run.py:483] Algo bellman_ford step 5963 current loss 0.590519, current_train_items 190848.
I0302 19:00:27.727573 22895071117440 run.py:483] Algo bellman_ford step 5964 current loss 0.784146, current_train_items 190880.
I0302 19:00:27.745553 22895071117440 run.py:483] Algo bellman_ford step 5965 current loss 0.339436, current_train_items 190912.
I0302 19:00:27.760754 22895071117440 run.py:483] Algo bellman_ford step 5966 current loss 0.521573, current_train_items 190944.
I0302 19:00:27.782864 22895071117440 run.py:483] Algo bellman_ford step 5967 current loss 0.593974, current_train_items 190976.
I0302 19:00:27.810939 22895071117440 run.py:483] Algo bellman_ford step 5968 current loss 0.670380, current_train_items 191008.
I0302 19:00:27.842687 22895071117440 run.py:483] Algo bellman_ford step 5969 current loss 0.730851, current_train_items 191040.
I0302 19:00:27.861214 22895071117440 run.py:483] Algo bellman_ford step 5970 current loss 0.314253, current_train_items 191072.
I0302 19:00:27.876835 22895071117440 run.py:483] Algo bellman_ford step 5971 current loss 0.527131, current_train_items 191104.
I0302 19:00:27.898151 22895071117440 run.py:483] Algo bellman_ford step 5972 current loss 0.537608, current_train_items 191136.
I0302 19:00:27.926706 22895071117440 run.py:483] Algo bellman_ford step 5973 current loss 0.682875, current_train_items 191168.
I0302 19:00:27.957174 22895071117440 run.py:483] Algo bellman_ford step 5974 current loss 0.726847, current_train_items 191200.
I0302 19:00:27.975633 22895071117440 run.py:483] Algo bellman_ford step 5975 current loss 0.385029, current_train_items 191232.
I0302 19:00:27.991217 22895071117440 run.py:483] Algo bellman_ford step 5976 current loss 0.447763, current_train_items 191264.
I0302 19:00:28.012229 22895071117440 run.py:483] Algo bellman_ford step 5977 current loss 0.658804, current_train_items 191296.
I0302 19:00:28.040806 22895071117440 run.py:483] Algo bellman_ford step 5978 current loss 0.834322, current_train_items 191328.
I0302 19:00:28.070330 22895071117440 run.py:483] Algo bellman_ford step 5979 current loss 0.744617, current_train_items 191360.
I0302 19:00:28.087939 22895071117440 run.py:483] Algo bellman_ford step 5980 current loss 0.258424, current_train_items 191392.
I0302 19:00:28.104111 22895071117440 run.py:483] Algo bellman_ford step 5981 current loss 0.504437, current_train_items 191424.
I0302 19:00:28.125077 22895071117440 run.py:483] Algo bellman_ford step 5982 current loss 0.556990, current_train_items 191456.
I0302 19:00:28.152690 22895071117440 run.py:483] Algo bellman_ford step 5983 current loss 0.718143, current_train_items 191488.
I0302 19:00:28.182884 22895071117440 run.py:483] Algo bellman_ford step 5984 current loss 1.041292, current_train_items 191520.
I0302 19:00:28.201399 22895071117440 run.py:483] Algo bellman_ford step 5985 current loss 0.307645, current_train_items 191552.
I0302 19:00:28.217594 22895071117440 run.py:483] Algo bellman_ford step 5986 current loss 0.522716, current_train_items 191584.
I0302 19:00:28.239175 22895071117440 run.py:483] Algo bellman_ford step 5987 current loss 0.621113, current_train_items 191616.
I0302 19:00:28.266104 22895071117440 run.py:483] Algo bellman_ford step 5988 current loss 0.677417, current_train_items 191648.
I0302 19:00:28.296470 22895071117440 run.py:483] Algo bellman_ford step 5989 current loss 0.729515, current_train_items 191680.
I0302 19:00:28.314828 22895071117440 run.py:483] Algo bellman_ford step 5990 current loss 0.271596, current_train_items 191712.
I0302 19:00:28.329862 22895071117440 run.py:483] Algo bellman_ford step 5991 current loss 0.453914, current_train_items 191744.
I0302 19:00:28.351413 22895071117440 run.py:483] Algo bellman_ford step 5992 current loss 0.654089, current_train_items 191776.
I0302 19:00:28.381223 22895071117440 run.py:483] Algo bellman_ford step 5993 current loss 0.733849, current_train_items 191808.
I0302 19:00:28.412014 22895071117440 run.py:483] Algo bellman_ford step 5994 current loss 0.743675, current_train_items 191840.
I0302 19:00:28.429742 22895071117440 run.py:483] Algo bellman_ford step 5995 current loss 0.319145, current_train_items 191872.
I0302 19:00:28.445014 22895071117440 run.py:483] Algo bellman_ford step 5996 current loss 0.418314, current_train_items 191904.
I0302 19:00:28.466841 22895071117440 run.py:483] Algo bellman_ford step 5997 current loss 0.629358, current_train_items 191936.
I0302 19:00:28.494387 22895071117440 run.py:483] Algo bellman_ford step 5998 current loss 0.730524, current_train_items 191968.
I0302 19:00:28.524251 22895071117440 run.py:483] Algo bellman_ford step 5999 current loss 0.702885, current_train_items 192000.
I0302 19:00:28.542779 22895071117440 run.py:483] Algo bellman_ford step 6000 current loss 0.290376, current_train_items 192032.
I0302 19:00:28.550614 22895071117440 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:00:28.550719 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:28.566749 22895071117440 run.py:483] Algo bellman_ford step 6001 current loss 0.490254, current_train_items 192064.
I0302 19:00:28.588578 22895071117440 run.py:483] Algo bellman_ford step 6002 current loss 0.616764, current_train_items 192096.
I0302 19:00:28.616049 22895071117440 run.py:483] Algo bellman_ford step 6003 current loss 0.579745, current_train_items 192128.
I0302 19:00:28.646781 22895071117440 run.py:483] Algo bellman_ford step 6004 current loss 0.712310, current_train_items 192160.
I0302 19:00:28.665077 22895071117440 run.py:483] Algo bellman_ford step 6005 current loss 0.227344, current_train_items 192192.
I0302 19:00:28.680393 22895071117440 run.py:483] Algo bellman_ford step 6006 current loss 0.426020, current_train_items 192224.
I0302 19:00:28.703138 22895071117440 run.py:483] Algo bellman_ford step 6007 current loss 0.670032, current_train_items 192256.
I0302 19:00:28.733255 22895071117440 run.py:483] Algo bellman_ford step 6008 current loss 0.728834, current_train_items 192288.
I0302 19:00:28.765425 22895071117440 run.py:483] Algo bellman_ford step 6009 current loss 0.759618, current_train_items 192320.
I0302 19:00:28.783347 22895071117440 run.py:483] Algo bellman_ford step 6010 current loss 0.284847, current_train_items 192352.
I0302 19:00:28.798641 22895071117440 run.py:483] Algo bellman_ford step 6011 current loss 0.471410, current_train_items 192384.
I0302 19:00:28.820652 22895071117440 run.py:483] Algo bellman_ford step 6012 current loss 0.569231, current_train_items 192416.
I0302 19:00:28.848499 22895071117440 run.py:483] Algo bellman_ford step 6013 current loss 0.631822, current_train_items 192448.
I0302 19:00:28.881111 22895071117440 run.py:483] Algo bellman_ford step 6014 current loss 0.761319, current_train_items 192480.
I0302 19:00:28.899331 22895071117440 run.py:483] Algo bellman_ford step 6015 current loss 0.315083, current_train_items 192512.
I0302 19:00:28.914946 22895071117440 run.py:483] Algo bellman_ford step 6016 current loss 0.439197, current_train_items 192544.
I0302 19:00:28.936312 22895071117440 run.py:483] Algo bellman_ford step 6017 current loss 0.564169, current_train_items 192576.
I0302 19:00:28.965090 22895071117440 run.py:483] Algo bellman_ford step 6018 current loss 0.709062, current_train_items 192608.
I0302 19:00:28.995668 22895071117440 run.py:483] Algo bellman_ford step 6019 current loss 0.802005, current_train_items 192640.
I0302 19:00:29.013787 22895071117440 run.py:483] Algo bellman_ford step 6020 current loss 0.320215, current_train_items 192672.
I0302 19:00:29.029549 22895071117440 run.py:483] Algo bellman_ford step 6021 current loss 0.467952, current_train_items 192704.
I0302 19:00:29.051757 22895071117440 run.py:483] Algo bellman_ford step 6022 current loss 0.645754, current_train_items 192736.
I0302 19:00:29.081650 22895071117440 run.py:483] Algo bellman_ford step 6023 current loss 0.797995, current_train_items 192768.
I0302 19:00:29.112928 22895071117440 run.py:483] Algo bellman_ford step 6024 current loss 0.704856, current_train_items 192800.
I0302 19:00:29.130847 22895071117440 run.py:483] Algo bellman_ford step 6025 current loss 0.280581, current_train_items 192832.
I0302 19:00:29.146442 22895071117440 run.py:483] Algo bellman_ford step 6026 current loss 0.602395, current_train_items 192864.
I0302 19:00:29.168140 22895071117440 run.py:483] Algo bellman_ford step 6027 current loss 0.628033, current_train_items 192896.
I0302 19:00:29.196320 22895071117440 run.py:483] Algo bellman_ford step 6028 current loss 0.776448, current_train_items 192928.
I0302 19:00:29.226980 22895071117440 run.py:483] Algo bellman_ford step 6029 current loss 0.720336, current_train_items 192960.
I0302 19:00:29.244964 22895071117440 run.py:483] Algo bellman_ford step 6030 current loss 0.265476, current_train_items 192992.
I0302 19:00:29.260113 22895071117440 run.py:483] Algo bellman_ford step 6031 current loss 0.414248, current_train_items 193024.
I0302 19:00:29.281753 22895071117440 run.py:483] Algo bellman_ford step 6032 current loss 0.537047, current_train_items 193056.
I0302 19:00:29.311922 22895071117440 run.py:483] Algo bellman_ford step 6033 current loss 0.812232, current_train_items 193088.
I0302 19:00:29.342464 22895071117440 run.py:483] Algo bellman_ford step 6034 current loss 0.800983, current_train_items 193120.
I0302 19:00:29.360072 22895071117440 run.py:483] Algo bellman_ford step 6035 current loss 0.259454, current_train_items 193152.
I0302 19:00:29.375345 22895071117440 run.py:483] Algo bellman_ford step 6036 current loss 0.424934, current_train_items 193184.
I0302 19:00:29.397226 22895071117440 run.py:483] Algo bellman_ford step 6037 current loss 0.556998, current_train_items 193216.
I0302 19:00:29.426202 22895071117440 run.py:483] Algo bellman_ford step 6038 current loss 0.684620, current_train_items 193248.
I0302 19:00:29.456228 22895071117440 run.py:483] Algo bellman_ford step 6039 current loss 0.698012, current_train_items 193280.
I0302 19:00:29.473866 22895071117440 run.py:483] Algo bellman_ford step 6040 current loss 0.411832, current_train_items 193312.
I0302 19:00:29.489304 22895071117440 run.py:483] Algo bellman_ford step 6041 current loss 0.468752, current_train_items 193344.
I0302 19:00:29.511774 22895071117440 run.py:483] Algo bellman_ford step 6042 current loss 0.610615, current_train_items 193376.
I0302 19:00:29.539771 22895071117440 run.py:483] Algo bellman_ford step 6043 current loss 0.643651, current_train_items 193408.
I0302 19:00:29.573208 22895071117440 run.py:483] Algo bellman_ford step 6044 current loss 0.828812, current_train_items 193440.
I0302 19:00:29.591211 22895071117440 run.py:483] Algo bellman_ford step 6045 current loss 0.265021, current_train_items 193472.
I0302 19:00:29.606920 22895071117440 run.py:483] Algo bellman_ford step 6046 current loss 0.461885, current_train_items 193504.
I0302 19:00:29.628379 22895071117440 run.py:483] Algo bellman_ford step 6047 current loss 0.651828, current_train_items 193536.
I0302 19:00:29.657984 22895071117440 run.py:483] Algo bellman_ford step 6048 current loss 0.800850, current_train_items 193568.
I0302 19:00:29.689510 22895071117440 run.py:483] Algo bellman_ford step 6049 current loss 0.791807, current_train_items 193600.
I0302 19:00:29.707361 22895071117440 run.py:483] Algo bellman_ford step 6050 current loss 0.258220, current_train_items 193632.
I0302 19:00:29.715113 22895071117440 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:00:29.715245 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:00:29.731150 22895071117440 run.py:483] Algo bellman_ford step 6051 current loss 0.476173, current_train_items 193664.
I0302 19:00:29.754154 22895071117440 run.py:483] Algo bellman_ford step 6052 current loss 0.600651, current_train_items 193696.
I0302 19:00:29.782604 22895071117440 run.py:483] Algo bellman_ford step 6053 current loss 0.623790, current_train_items 193728.
I0302 19:00:29.814740 22895071117440 run.py:483] Algo bellman_ford step 6054 current loss 0.684317, current_train_items 193760.
I0302 19:00:29.833203 22895071117440 run.py:483] Algo bellman_ford step 6055 current loss 0.328093, current_train_items 193792.
I0302 19:00:29.848371 22895071117440 run.py:483] Algo bellman_ford step 6056 current loss 0.488004, current_train_items 193824.
I0302 19:00:29.869611 22895071117440 run.py:483] Algo bellman_ford step 6057 current loss 0.631962, current_train_items 193856.
I0302 19:00:29.897856 22895071117440 run.py:483] Algo bellman_ford step 6058 current loss 0.677496, current_train_items 193888.
I0302 19:00:29.926418 22895071117440 run.py:483] Algo bellman_ford step 6059 current loss 0.573274, current_train_items 193920.
I0302 19:00:29.945115 22895071117440 run.py:483] Algo bellman_ford step 6060 current loss 0.292579, current_train_items 193952.
I0302 19:00:29.960819 22895071117440 run.py:483] Algo bellman_ford step 6061 current loss 0.484783, current_train_items 193984.
I0302 19:00:29.981009 22895071117440 run.py:483] Algo bellman_ford step 6062 current loss 0.524490, current_train_items 194016.
I0302 19:00:30.008124 22895071117440 run.py:483] Algo bellman_ford step 6063 current loss 0.639968, current_train_items 194048.
I0302 19:00:30.039889 22895071117440 run.py:483] Algo bellman_ford step 6064 current loss 0.775098, current_train_items 194080.
I0302 19:00:30.057956 22895071117440 run.py:483] Algo bellman_ford step 6065 current loss 0.279410, current_train_items 194112.
I0302 19:00:30.073309 22895071117440 run.py:483] Algo bellman_ford step 6066 current loss 0.424695, current_train_items 194144.
I0302 19:00:30.095884 22895071117440 run.py:483] Algo bellman_ford step 6067 current loss 0.636493, current_train_items 194176.
I0302 19:00:30.124676 22895071117440 run.py:483] Algo bellman_ford step 6068 current loss 0.705323, current_train_items 194208.
I0302 19:00:30.154957 22895071117440 run.py:483] Algo bellman_ford step 6069 current loss 0.740547, current_train_items 194240.
I0302 19:00:30.173532 22895071117440 run.py:483] Algo bellman_ford step 6070 current loss 0.324310, current_train_items 194272.
I0302 19:00:30.189131 22895071117440 run.py:483] Algo bellman_ford step 6071 current loss 0.557090, current_train_items 194304.
I0302 19:00:30.211022 22895071117440 run.py:483] Algo bellman_ford step 6072 current loss 0.640404, current_train_items 194336.
I0302 19:00:30.238348 22895071117440 run.py:483] Algo bellman_ford step 6073 current loss 0.622024, current_train_items 194368.
I0302 19:00:30.269522 22895071117440 run.py:483] Algo bellman_ford step 6074 current loss 0.849518, current_train_items 194400.
I0302 19:00:30.287979 22895071117440 run.py:483] Algo bellman_ford step 6075 current loss 0.312253, current_train_items 194432.
I0302 19:00:30.303651 22895071117440 run.py:483] Algo bellman_ford step 6076 current loss 0.432449, current_train_items 194464.
I0302 19:00:30.324208 22895071117440 run.py:483] Algo bellman_ford step 6077 current loss 0.595409, current_train_items 194496.
I0302 19:00:30.351841 22895071117440 run.py:483] Algo bellman_ford step 6078 current loss 0.734058, current_train_items 194528.
I0302 19:00:30.383605 22895071117440 run.py:483] Algo bellman_ford step 6079 current loss 0.842013, current_train_items 194560.
I0302 19:00:30.401458 22895071117440 run.py:483] Algo bellman_ford step 6080 current loss 0.285607, current_train_items 194592.
I0302 19:00:30.416970 22895071117440 run.py:483] Algo bellman_ford step 6081 current loss 0.437131, current_train_items 194624.
I0302 19:00:30.438774 22895071117440 run.py:483] Algo bellman_ford step 6082 current loss 0.665480, current_train_items 194656.
I0302 19:00:30.468772 22895071117440 run.py:483] Algo bellman_ford step 6083 current loss 0.903200, current_train_items 194688.
I0302 19:00:30.498153 22895071117440 run.py:483] Algo bellman_ford step 6084 current loss 0.906870, current_train_items 194720.
I0302 19:00:30.516692 22895071117440 run.py:483] Algo bellman_ford step 6085 current loss 0.274389, current_train_items 194752.
I0302 19:00:30.532549 22895071117440 run.py:483] Algo bellman_ford step 6086 current loss 0.511594, current_train_items 194784.
I0302 19:00:30.554055 22895071117440 run.py:483] Algo bellman_ford step 6087 current loss 0.723534, current_train_items 194816.
I0302 19:00:30.581923 22895071117440 run.py:483] Algo bellman_ford step 6088 current loss 0.774613, current_train_items 194848.
I0302 19:00:30.613236 22895071117440 run.py:483] Algo bellman_ford step 6089 current loss 0.934636, current_train_items 194880.
I0302 19:00:30.631729 22895071117440 run.py:483] Algo bellman_ford step 6090 current loss 0.422748, current_train_items 194912.
I0302 19:00:30.647437 22895071117440 run.py:483] Algo bellman_ford step 6091 current loss 0.549727, current_train_items 194944.
I0302 19:00:30.668693 22895071117440 run.py:483] Algo bellman_ford step 6092 current loss 0.651194, current_train_items 194976.
I0302 19:00:30.696574 22895071117440 run.py:483] Algo bellman_ford step 6093 current loss 0.705983, current_train_items 195008.
I0302 19:00:30.726448 22895071117440 run.py:483] Algo bellman_ford step 6094 current loss 0.882732, current_train_items 195040.
I0302 19:00:30.744782 22895071117440 run.py:483] Algo bellman_ford step 6095 current loss 0.250618, current_train_items 195072.
I0302 19:00:30.760226 22895071117440 run.py:483] Algo bellman_ford step 6096 current loss 0.434423, current_train_items 195104.
I0302 19:00:30.782214 22895071117440 run.py:483] Algo bellman_ford step 6097 current loss 0.644015, current_train_items 195136.
I0302 19:00:30.809314 22895071117440 run.py:483] Algo bellman_ford step 6098 current loss 0.663284, current_train_items 195168.
I0302 19:00:30.840154 22895071117440 run.py:483] Algo bellman_ford step 6099 current loss 0.842075, current_train_items 195200.
I0302 19:00:30.858488 22895071117440 run.py:483] Algo bellman_ford step 6100 current loss 0.181339, current_train_items 195232.
I0302 19:00:30.872242 22895071117440 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:00:30.872349 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:00:30.887733 22895071117440 run.py:483] Algo bellman_ford step 6101 current loss 0.453312, current_train_items 195264.
I0302 19:00:30.910856 22895071117440 run.py:483] Algo bellman_ford step 6102 current loss 0.626901, current_train_items 195296.
I0302 19:00:30.941189 22895071117440 run.py:483] Algo bellman_ford step 6103 current loss 0.844240, current_train_items 195328.
I0302 19:00:30.971970 22895071117440 run.py:483] Algo bellman_ford step 6104 current loss 0.692252, current_train_items 195360.
I0302 19:00:30.990338 22895071117440 run.py:483] Algo bellman_ford step 6105 current loss 0.351996, current_train_items 195392.
I0302 19:00:31.005183 22895071117440 run.py:483] Algo bellman_ford step 6106 current loss 0.492247, current_train_items 195424.
I0302 19:00:31.027197 22895071117440 run.py:483] Algo bellman_ford step 6107 current loss 0.655533, current_train_items 195456.
I0302 19:00:31.055762 22895071117440 run.py:483] Algo bellman_ford step 6108 current loss 0.643035, current_train_items 195488.
I0302 19:00:31.085836 22895071117440 run.py:483] Algo bellman_ford step 6109 current loss 0.772230, current_train_items 195520.
I0302 19:00:31.103867 22895071117440 run.py:483] Algo bellman_ford step 6110 current loss 0.272962, current_train_items 195552.
I0302 19:00:31.119141 22895071117440 run.py:483] Algo bellman_ford step 6111 current loss 0.418060, current_train_items 195584.
I0302 19:00:31.140757 22895071117440 run.py:483] Algo bellman_ford step 6112 current loss 0.676160, current_train_items 195616.
I0302 19:00:31.168652 22895071117440 run.py:483] Algo bellman_ford step 6113 current loss 0.693601, current_train_items 195648.
I0302 19:00:31.199565 22895071117440 run.py:483] Algo bellman_ford step 6114 current loss 0.734073, current_train_items 195680.
I0302 19:00:31.218094 22895071117440 run.py:483] Algo bellman_ford step 6115 current loss 0.268762, current_train_items 195712.
I0302 19:00:31.233426 22895071117440 run.py:483] Algo bellman_ford step 6116 current loss 0.500292, current_train_items 195744.
I0302 19:00:31.255259 22895071117440 run.py:483] Algo bellman_ford step 6117 current loss 0.577620, current_train_items 195776.
I0302 19:00:31.284783 22895071117440 run.py:483] Algo bellman_ford step 6118 current loss 0.727838, current_train_items 195808.
I0302 19:00:31.315324 22895071117440 run.py:483] Algo bellman_ford step 6119 current loss 0.758933, current_train_items 195840.
I0302 19:00:31.333240 22895071117440 run.py:483] Algo bellman_ford step 6120 current loss 0.364356, current_train_items 195872.
I0302 19:00:31.348829 22895071117440 run.py:483] Algo bellman_ford step 6121 current loss 0.472513, current_train_items 195904.
I0302 19:00:31.370573 22895071117440 run.py:483] Algo bellman_ford step 6122 current loss 0.694045, current_train_items 195936.
I0302 19:00:31.400086 22895071117440 run.py:483] Algo bellman_ford step 6123 current loss 0.739415, current_train_items 195968.
I0302 19:00:31.430906 22895071117440 run.py:483] Algo bellman_ford step 6124 current loss 0.904156, current_train_items 196000.
I0302 19:00:31.449141 22895071117440 run.py:483] Algo bellman_ford step 6125 current loss 0.247547, current_train_items 196032.
I0302 19:00:31.464007 22895071117440 run.py:483] Algo bellman_ford step 6126 current loss 0.472613, current_train_items 196064.
I0302 19:00:31.487678 22895071117440 run.py:483] Algo bellman_ford step 6127 current loss 0.699432, current_train_items 196096.
I0302 19:00:31.516206 22895071117440 run.py:483] Algo bellman_ford step 6128 current loss 0.621005, current_train_items 196128.
I0302 19:00:31.546543 22895071117440 run.py:483] Algo bellman_ford step 6129 current loss 0.689087, current_train_items 196160.
I0302 19:00:31.564615 22895071117440 run.py:483] Algo bellman_ford step 6130 current loss 0.304959, current_train_items 196192.
I0302 19:00:31.579841 22895071117440 run.py:483] Algo bellman_ford step 6131 current loss 0.401664, current_train_items 196224.
I0302 19:00:31.602532 22895071117440 run.py:483] Algo bellman_ford step 6132 current loss 0.562065, current_train_items 196256.
I0302 19:00:31.630009 22895071117440 run.py:483] Algo bellman_ford step 6133 current loss 0.650448, current_train_items 196288.
I0302 19:00:31.659802 22895071117440 run.py:483] Algo bellman_ford step 6134 current loss 0.623507, current_train_items 196320.
I0302 19:00:31.677718 22895071117440 run.py:483] Algo bellman_ford step 6135 current loss 0.279072, current_train_items 196352.
I0302 19:00:31.692709 22895071117440 run.py:483] Algo bellman_ford step 6136 current loss 0.498182, current_train_items 196384.
I0302 19:00:31.714910 22895071117440 run.py:483] Algo bellman_ford step 6137 current loss 0.733173, current_train_items 196416.
I0302 19:00:31.743968 22895071117440 run.py:483] Algo bellman_ford step 6138 current loss 0.719662, current_train_items 196448.
I0302 19:00:31.774743 22895071117440 run.py:483] Algo bellman_ford step 6139 current loss 0.732380, current_train_items 196480.
I0302 19:00:31.793057 22895071117440 run.py:483] Algo bellman_ford step 6140 current loss 0.328297, current_train_items 196512.
I0302 19:00:31.808351 22895071117440 run.py:483] Algo bellman_ford step 6141 current loss 0.367363, current_train_items 196544.
I0302 19:00:31.830689 22895071117440 run.py:483] Algo bellman_ford step 6142 current loss 0.564019, current_train_items 196576.
I0302 19:00:31.859506 22895071117440 run.py:483] Algo bellman_ford step 6143 current loss 0.617978, current_train_items 196608.
I0302 19:00:31.890268 22895071117440 run.py:483] Algo bellman_ford step 6144 current loss 0.849009, current_train_items 196640.
I0302 19:00:31.908205 22895071117440 run.py:483] Algo bellman_ford step 6145 current loss 0.268217, current_train_items 196672.
I0302 19:00:31.923687 22895071117440 run.py:483] Algo bellman_ford step 6146 current loss 0.528264, current_train_items 196704.
I0302 19:00:31.945056 22895071117440 run.py:483] Algo bellman_ford step 6147 current loss 0.557531, current_train_items 196736.
I0302 19:00:31.973308 22895071117440 run.py:483] Algo bellman_ford step 6148 current loss 0.664831, current_train_items 196768.
I0302 19:00:32.005225 22895071117440 run.py:483] Algo bellman_ford step 6149 current loss 0.886821, current_train_items 196800.
I0302 19:00:32.023285 22895071117440 run.py:483] Algo bellman_ford step 6150 current loss 0.305064, current_train_items 196832.
I0302 19:00:32.031242 22895071117440 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:00:32.031346 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:00:32.047399 22895071117440 run.py:483] Algo bellman_ford step 6151 current loss 0.473097, current_train_items 196864.
I0302 19:00:32.068418 22895071117440 run.py:483] Algo bellman_ford step 6152 current loss 0.512111, current_train_items 196896.
I0302 19:00:32.095735 22895071117440 run.py:483] Algo bellman_ford step 6153 current loss 0.625821, current_train_items 196928.
I0302 19:00:32.126754 22895071117440 run.py:483] Algo bellman_ford step 6154 current loss 0.704610, current_train_items 196960.
I0302 19:00:32.144999 22895071117440 run.py:483] Algo bellman_ford step 6155 current loss 0.254132, current_train_items 196992.
I0302 19:00:32.160433 22895071117440 run.py:483] Algo bellman_ford step 6156 current loss 0.411445, current_train_items 197024.
I0302 19:00:32.181987 22895071117440 run.py:483] Algo bellman_ford step 6157 current loss 0.489282, current_train_items 197056.
I0302 19:00:32.209714 22895071117440 run.py:483] Algo bellman_ford step 6158 current loss 0.626195, current_train_items 197088.
I0302 19:00:32.240180 22895071117440 run.py:483] Algo bellman_ford step 6159 current loss 0.797806, current_train_items 197120.
I0302 19:00:32.258372 22895071117440 run.py:483] Algo bellman_ford step 6160 current loss 0.429327, current_train_items 197152.
I0302 19:00:32.274437 22895071117440 run.py:483] Algo bellman_ford step 6161 current loss 0.534976, current_train_items 197184.
I0302 19:00:32.295711 22895071117440 run.py:483] Algo bellman_ford step 6162 current loss 0.717357, current_train_items 197216.
I0302 19:00:32.325346 22895071117440 run.py:483] Algo bellman_ford step 6163 current loss 0.731467, current_train_items 197248.
I0302 19:00:32.356525 22895071117440 run.py:483] Algo bellman_ford step 6164 current loss 0.758303, current_train_items 197280.
I0302 19:00:32.374736 22895071117440 run.py:483] Algo bellman_ford step 6165 current loss 0.237867, current_train_items 197312.
I0302 19:00:32.390265 22895071117440 run.py:483] Algo bellman_ford step 6166 current loss 0.495016, current_train_items 197344.
I0302 19:00:32.412179 22895071117440 run.py:483] Algo bellman_ford step 6167 current loss 0.760315, current_train_items 197376.
I0302 19:00:32.440878 22895071117440 run.py:483] Algo bellman_ford step 6168 current loss 0.818143, current_train_items 197408.
I0302 19:00:32.471916 22895071117440 run.py:483] Algo bellman_ford step 6169 current loss 0.865562, current_train_items 197440.
I0302 19:00:32.490774 22895071117440 run.py:483] Algo bellman_ford step 6170 current loss 0.314143, current_train_items 197472.
I0302 19:00:32.506272 22895071117440 run.py:483] Algo bellman_ford step 6171 current loss 0.446905, current_train_items 197504.
I0302 19:00:32.528139 22895071117440 run.py:483] Algo bellman_ford step 6172 current loss 0.571342, current_train_items 197536.
I0302 19:00:32.557686 22895071117440 run.py:483] Algo bellman_ford step 6173 current loss 0.762733, current_train_items 197568.
I0302 19:00:32.588456 22895071117440 run.py:483] Algo bellman_ford step 6174 current loss 0.822339, current_train_items 197600.
I0302 19:00:32.606876 22895071117440 run.py:483] Algo bellman_ford step 6175 current loss 0.271659, current_train_items 197632.
I0302 19:00:32.622758 22895071117440 run.py:483] Algo bellman_ford step 6176 current loss 0.698141, current_train_items 197664.
I0302 19:00:32.643695 22895071117440 run.py:483] Algo bellman_ford step 6177 current loss 0.617424, current_train_items 197696.
I0302 19:00:32.671893 22895071117440 run.py:483] Algo bellman_ford step 6178 current loss 0.685750, current_train_items 197728.
I0302 19:00:32.702684 22895071117440 run.py:483] Algo bellman_ford step 6179 current loss 0.888696, current_train_items 197760.
I0302 19:00:32.720601 22895071117440 run.py:483] Algo bellman_ford step 6180 current loss 0.320583, current_train_items 197792.
I0302 19:00:32.735993 22895071117440 run.py:483] Algo bellman_ford step 6181 current loss 0.481461, current_train_items 197824.
I0302 19:00:32.757442 22895071117440 run.py:483] Algo bellman_ford step 6182 current loss 0.664352, current_train_items 197856.
I0302 19:00:32.786030 22895071117440 run.py:483] Algo bellman_ford step 6183 current loss 0.701080, current_train_items 197888.
I0302 19:00:32.818979 22895071117440 run.py:483] Algo bellman_ford step 6184 current loss 0.826179, current_train_items 197920.
I0302 19:00:32.837244 22895071117440 run.py:483] Algo bellman_ford step 6185 current loss 0.348940, current_train_items 197952.
I0302 19:00:32.852982 22895071117440 run.py:483] Algo bellman_ford step 6186 current loss 0.495629, current_train_items 197984.
I0302 19:00:32.873835 22895071117440 run.py:483] Algo bellman_ford step 6187 current loss 0.667025, current_train_items 198016.
I0302 19:00:32.901496 22895071117440 run.py:483] Algo bellman_ford step 6188 current loss 0.592341, current_train_items 198048.
I0302 19:00:32.932828 22895071117440 run.py:483] Algo bellman_ford step 6189 current loss 0.906366, current_train_items 198080.
I0302 19:00:32.951171 22895071117440 run.py:483] Algo bellman_ford step 6190 current loss 0.375773, current_train_items 198112.
I0302 19:00:32.967024 22895071117440 run.py:483] Algo bellman_ford step 6191 current loss 0.517156, current_train_items 198144.
I0302 19:00:32.989115 22895071117440 run.py:483] Algo bellman_ford step 6192 current loss 0.666820, current_train_items 198176.
I0302 19:00:33.018272 22895071117440 run.py:483] Algo bellman_ford step 6193 current loss 0.722167, current_train_items 198208.
I0302 19:00:33.048123 22895071117440 run.py:483] Algo bellman_ford step 6194 current loss 0.768625, current_train_items 198240.
I0302 19:00:33.066215 22895071117440 run.py:483] Algo bellman_ford step 6195 current loss 0.377014, current_train_items 198272.
I0302 19:00:33.081025 22895071117440 run.py:483] Algo bellman_ford step 6196 current loss 0.454359, current_train_items 198304.
I0302 19:00:33.103792 22895071117440 run.py:483] Algo bellman_ford step 6197 current loss 0.681121, current_train_items 198336.
I0302 19:00:33.131211 22895071117440 run.py:483] Algo bellman_ford step 6198 current loss 0.653145, current_train_items 198368.
I0302 19:00:33.162882 22895071117440 run.py:483] Algo bellman_ford step 6199 current loss 0.820543, current_train_items 198400.
I0302 19:00:33.181447 22895071117440 run.py:483] Algo bellman_ford step 6200 current loss 0.301473, current_train_items 198432.
I0302 19:00:33.189422 22895071117440 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:00:33.189530 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:00:33.205668 22895071117440 run.py:483] Algo bellman_ford step 6201 current loss 0.446102, current_train_items 198464.
I0302 19:00:33.228672 22895071117440 run.py:483] Algo bellman_ford step 6202 current loss 0.595562, current_train_items 198496.
I0302 19:00:33.256492 22895071117440 run.py:483] Algo bellman_ford step 6203 current loss 0.596574, current_train_items 198528.
I0302 19:00:33.287199 22895071117440 run.py:483] Algo bellman_ford step 6204 current loss 0.680938, current_train_items 198560.
I0302 19:00:33.305582 22895071117440 run.py:483] Algo bellman_ford step 6205 current loss 0.340276, current_train_items 198592.
I0302 19:00:33.320317 22895071117440 run.py:483] Algo bellman_ford step 6206 current loss 0.463470, current_train_items 198624.
I0302 19:00:33.342248 22895071117440 run.py:483] Algo bellman_ford step 6207 current loss 0.581017, current_train_items 198656.
I0302 19:00:33.371066 22895071117440 run.py:483] Algo bellman_ford step 6208 current loss 0.734054, current_train_items 198688.
I0302 19:00:33.403119 22895071117440 run.py:483] Algo bellman_ford step 6209 current loss 0.830351, current_train_items 198720.
I0302 19:00:33.421254 22895071117440 run.py:483] Algo bellman_ford step 6210 current loss 0.278186, current_train_items 198752.
I0302 19:00:33.436670 22895071117440 run.py:483] Algo bellman_ford step 6211 current loss 0.449151, current_train_items 198784.
I0302 19:00:33.458241 22895071117440 run.py:483] Algo bellman_ford step 6212 current loss 0.657706, current_train_items 198816.
I0302 19:00:33.485163 22895071117440 run.py:483] Algo bellman_ford step 6213 current loss 0.644681, current_train_items 198848.
I0302 19:00:33.517417 22895071117440 run.py:483] Algo bellman_ford step 6214 current loss 0.764320, current_train_items 198880.
I0302 19:00:33.535305 22895071117440 run.py:483] Algo bellman_ford step 6215 current loss 0.272721, current_train_items 198912.
I0302 19:00:33.550708 22895071117440 run.py:483] Algo bellman_ford step 6216 current loss 0.463193, current_train_items 198944.
I0302 19:00:33.571828 22895071117440 run.py:483] Algo bellman_ford step 6217 current loss 0.577861, current_train_items 198976.
I0302 19:00:33.600058 22895071117440 run.py:483] Algo bellman_ford step 6218 current loss 0.705696, current_train_items 199008.
I0302 19:00:33.630244 22895071117440 run.py:483] Algo bellman_ford step 6219 current loss 0.905687, current_train_items 199040.
I0302 19:00:33.648313 22895071117440 run.py:483] Algo bellman_ford step 6220 current loss 0.266734, current_train_items 199072.
I0302 19:00:33.663093 22895071117440 run.py:483] Algo bellman_ford step 6221 current loss 0.510307, current_train_items 199104.
I0302 19:00:33.685955 22895071117440 run.py:483] Algo bellman_ford step 6222 current loss 0.768944, current_train_items 199136.
I0302 19:00:33.713506 22895071117440 run.py:483] Algo bellman_ford step 6223 current loss 0.696451, current_train_items 199168.
I0302 19:00:33.745176 22895071117440 run.py:483] Algo bellman_ford step 6224 current loss 1.021714, current_train_items 199200.
I0302 19:00:33.763489 22895071117440 run.py:483] Algo bellman_ford step 6225 current loss 0.324905, current_train_items 199232.
I0302 19:00:33.778828 22895071117440 run.py:483] Algo bellman_ford step 6226 current loss 0.469672, current_train_items 199264.
I0302 19:00:33.799776 22895071117440 run.py:483] Algo bellman_ford step 6227 current loss 0.610446, current_train_items 199296.
I0302 19:00:33.826686 22895071117440 run.py:483] Algo bellman_ford step 6228 current loss 0.736011, current_train_items 199328.
I0302 19:00:33.855997 22895071117440 run.py:483] Algo bellman_ford step 6229 current loss 0.787817, current_train_items 199360.
I0302 19:00:33.874158 22895071117440 run.py:483] Algo bellman_ford step 6230 current loss 0.295192, current_train_items 199392.
I0302 19:00:33.889658 22895071117440 run.py:483] Algo bellman_ford step 6231 current loss 0.486060, current_train_items 199424.
I0302 19:00:33.911908 22895071117440 run.py:483] Algo bellman_ford step 6232 current loss 0.641403, current_train_items 199456.
I0302 19:00:33.938976 22895071117440 run.py:483] Algo bellman_ford step 6233 current loss 0.747946, current_train_items 199488.
I0302 19:00:33.969371 22895071117440 run.py:483] Algo bellman_ford step 6234 current loss 0.712941, current_train_items 199520.
I0302 19:00:33.987008 22895071117440 run.py:483] Algo bellman_ford step 6235 current loss 0.270513, current_train_items 199552.
I0302 19:00:34.002481 22895071117440 run.py:483] Algo bellman_ford step 6236 current loss 0.465262, current_train_items 199584.
I0302 19:00:34.023358 22895071117440 run.py:483] Algo bellman_ford step 6237 current loss 0.595208, current_train_items 199616.
I0302 19:00:34.052258 22895071117440 run.py:483] Algo bellman_ford step 6238 current loss 0.611918, current_train_items 199648.
I0302 19:00:34.080695 22895071117440 run.py:483] Algo bellman_ford step 6239 current loss 0.815212, current_train_items 199680.
I0302 19:00:34.098406 22895071117440 run.py:483] Algo bellman_ford step 6240 current loss 0.292150, current_train_items 199712.
I0302 19:00:34.113839 22895071117440 run.py:483] Algo bellman_ford step 6241 current loss 0.491652, current_train_items 199744.
I0302 19:00:34.135785 22895071117440 run.py:483] Algo bellman_ford step 6242 current loss 0.668062, current_train_items 199776.
I0302 19:00:34.164812 22895071117440 run.py:483] Algo bellman_ford step 6243 current loss 0.742038, current_train_items 199808.
I0302 19:00:34.194752 22895071117440 run.py:483] Algo bellman_ford step 6244 current loss 0.752689, current_train_items 199840.
I0302 19:00:34.212813 22895071117440 run.py:483] Algo bellman_ford step 6245 current loss 0.338917, current_train_items 199872.
I0302 19:00:34.228238 22895071117440 run.py:483] Algo bellman_ford step 6246 current loss 0.546363, current_train_items 199904.
I0302 19:00:34.249764 22895071117440 run.py:483] Algo bellman_ford step 6247 current loss 0.523451, current_train_items 199936.
I0302 19:00:34.278810 22895071117440 run.py:483] Algo bellman_ford step 6248 current loss 0.693006, current_train_items 199968.
I0302 19:00:34.308087 22895071117440 run.py:483] Algo bellman_ford step 6249 current loss 0.695520, current_train_items 200000.
I0302 19:00:34.325887 22895071117440 run.py:483] Algo bellman_ford step 6250 current loss 0.279080, current_train_items 200032.
I0302 19:00:34.333711 22895071117440 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:00:34.333818 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:34.350059 22895071117440 run.py:483] Algo bellman_ford step 6251 current loss 0.526470, current_train_items 200064.
I0302 19:00:34.372262 22895071117440 run.py:483] Algo bellman_ford step 6252 current loss 0.599433, current_train_items 200096.
I0302 19:00:34.401712 22895071117440 run.py:483] Algo bellman_ford step 6253 current loss 0.812193, current_train_items 200128.
I0302 19:00:34.431908 22895071117440 run.py:483] Algo bellman_ford step 6254 current loss 0.761851, current_train_items 200160.
I0302 19:00:34.450219 22895071117440 run.py:483] Algo bellman_ford step 6255 current loss 0.267314, current_train_items 200192.
I0302 19:00:34.465407 22895071117440 run.py:483] Algo bellman_ford step 6256 current loss 0.421155, current_train_items 200224.
I0302 19:00:34.487646 22895071117440 run.py:483] Algo bellman_ford step 6257 current loss 0.671035, current_train_items 200256.
I0302 19:00:34.515367 22895071117440 run.py:483] Algo bellman_ford step 6258 current loss 0.800784, current_train_items 200288.
I0302 19:00:34.544987 22895071117440 run.py:483] Algo bellman_ford step 6259 current loss 1.047209, current_train_items 200320.
I0302 19:00:34.563550 22895071117440 run.py:483] Algo bellman_ford step 6260 current loss 0.374957, current_train_items 200352.
I0302 19:00:34.579393 22895071117440 run.py:483] Algo bellman_ford step 6261 current loss 0.413579, current_train_items 200384.
I0302 19:00:34.600961 22895071117440 run.py:483] Algo bellman_ford step 6262 current loss 0.599409, current_train_items 200416.
I0302 19:00:34.629205 22895071117440 run.py:483] Algo bellman_ford step 6263 current loss 0.638588, current_train_items 200448.
I0302 19:00:34.659842 22895071117440 run.py:483] Algo bellman_ford step 6264 current loss 0.737588, current_train_items 200480.
I0302 19:00:34.678267 22895071117440 run.py:483] Algo bellman_ford step 6265 current loss 0.302866, current_train_items 200512.
I0302 19:00:34.694297 22895071117440 run.py:483] Algo bellman_ford step 6266 current loss 0.534079, current_train_items 200544.
I0302 19:00:34.717087 22895071117440 run.py:483] Algo bellman_ford step 6267 current loss 0.646894, current_train_items 200576.
I0302 19:00:34.745304 22895071117440 run.py:483] Algo bellman_ford step 6268 current loss 0.580170, current_train_items 200608.
I0302 19:00:34.773302 22895071117440 run.py:483] Algo bellman_ford step 6269 current loss 0.630303, current_train_items 200640.
I0302 19:00:34.791795 22895071117440 run.py:483] Algo bellman_ford step 6270 current loss 0.275759, current_train_items 200672.
I0302 19:00:34.807652 22895071117440 run.py:483] Algo bellman_ford step 6271 current loss 0.466514, current_train_items 200704.
I0302 19:00:34.829165 22895071117440 run.py:483] Algo bellman_ford step 6272 current loss 0.662232, current_train_items 200736.
I0302 19:00:34.856389 22895071117440 run.py:483] Algo bellman_ford step 6273 current loss 0.637708, current_train_items 200768.
I0302 19:00:34.888731 22895071117440 run.py:483] Algo bellman_ford step 6274 current loss 0.812295, current_train_items 200800.
I0302 19:00:34.907284 22895071117440 run.py:483] Algo bellman_ford step 6275 current loss 0.336195, current_train_items 200832.
I0302 19:00:34.923167 22895071117440 run.py:483] Algo bellman_ford step 6276 current loss 0.452757, current_train_items 200864.
I0302 19:00:34.944775 22895071117440 run.py:483] Algo bellman_ford step 6277 current loss 0.636601, current_train_items 200896.
I0302 19:00:34.972248 22895071117440 run.py:483] Algo bellman_ford step 6278 current loss 0.665917, current_train_items 200928.
I0302 19:00:35.003873 22895071117440 run.py:483] Algo bellman_ford step 6279 current loss 0.773839, current_train_items 200960.
I0302 19:00:35.022294 22895071117440 run.py:483] Algo bellman_ford step 6280 current loss 0.286113, current_train_items 200992.
I0302 19:00:35.037445 22895071117440 run.py:483] Algo bellman_ford step 6281 current loss 0.413167, current_train_items 201024.
I0302 19:00:35.058629 22895071117440 run.py:483] Algo bellman_ford step 6282 current loss 0.749334, current_train_items 201056.
I0302 19:00:35.087073 22895071117440 run.py:483] Algo bellman_ford step 6283 current loss 0.857259, current_train_items 201088.
I0302 19:00:35.118611 22895071117440 run.py:483] Algo bellman_ford step 6284 current loss 0.904098, current_train_items 201120.
I0302 19:00:35.136836 22895071117440 run.py:483] Algo bellman_ford step 6285 current loss 0.308762, current_train_items 201152.
I0302 19:00:35.152694 22895071117440 run.py:483] Algo bellman_ford step 6286 current loss 0.467595, current_train_items 201184.
I0302 19:00:35.174369 22895071117440 run.py:483] Algo bellman_ford step 6287 current loss 0.623255, current_train_items 201216.
I0302 19:00:35.203117 22895071117440 run.py:483] Algo bellman_ford step 6288 current loss 0.780479, current_train_items 201248.
I0302 19:00:35.233987 22895071117440 run.py:483] Algo bellman_ford step 6289 current loss 0.896388, current_train_items 201280.
I0302 19:00:35.252616 22895071117440 run.py:483] Algo bellman_ford step 6290 current loss 0.321661, current_train_items 201312.
I0302 19:00:35.268459 22895071117440 run.py:483] Algo bellman_ford step 6291 current loss 0.449671, current_train_items 201344.
I0302 19:00:35.290041 22895071117440 run.py:483] Algo bellman_ford step 6292 current loss 0.606972, current_train_items 201376.
I0302 19:00:35.319384 22895071117440 run.py:483] Algo bellman_ford step 6293 current loss 0.976481, current_train_items 201408.
I0302 19:00:35.351787 22895071117440 run.py:483] Algo bellman_ford step 6294 current loss 1.095997, current_train_items 201440.
I0302 19:00:35.369815 22895071117440 run.py:483] Algo bellman_ford step 6295 current loss 0.257714, current_train_items 201472.
I0302 19:00:35.385867 22895071117440 run.py:483] Algo bellman_ford step 6296 current loss 0.506312, current_train_items 201504.
I0302 19:00:35.407224 22895071117440 run.py:483] Algo bellman_ford step 6297 current loss 0.529835, current_train_items 201536.
I0302 19:00:35.435963 22895071117440 run.py:483] Algo bellman_ford step 6298 current loss 0.702852, current_train_items 201568.
I0302 19:00:35.465878 22895071117440 run.py:483] Algo bellman_ford step 6299 current loss 0.687691, current_train_items 201600.
I0302 19:00:35.483962 22895071117440 run.py:483] Algo bellman_ford step 6300 current loss 0.298536, current_train_items 201632.
I0302 19:00:35.491625 22895071117440 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:00:35.491731 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:00:35.507963 22895071117440 run.py:483] Algo bellman_ford step 6301 current loss 0.487355, current_train_items 201664.
I0302 19:00:35.529749 22895071117440 run.py:483] Algo bellman_ford step 6302 current loss 0.611037, current_train_items 201696.
I0302 19:00:35.558382 22895071117440 run.py:483] Algo bellman_ford step 6303 current loss 0.653014, current_train_items 201728.
I0302 19:00:35.587418 22895071117440 run.py:483] Algo bellman_ford step 6304 current loss 0.603298, current_train_items 201760.
I0302 19:00:35.606060 22895071117440 run.py:483] Algo bellman_ford step 6305 current loss 0.293684, current_train_items 201792.
I0302 19:00:35.621102 22895071117440 run.py:483] Algo bellman_ford step 6306 current loss 0.418505, current_train_items 201824.
I0302 19:00:35.643060 22895071117440 run.py:483] Algo bellman_ford step 6307 current loss 0.628338, current_train_items 201856.
I0302 19:00:35.672080 22895071117440 run.py:483] Algo bellman_ford step 6308 current loss 0.670496, current_train_items 201888.
I0302 19:00:35.702351 22895071117440 run.py:483] Algo bellman_ford step 6309 current loss 0.825707, current_train_items 201920.
I0302 19:00:35.720457 22895071117440 run.py:483] Algo bellman_ford step 6310 current loss 0.317618, current_train_items 201952.
I0302 19:00:35.735662 22895071117440 run.py:483] Algo bellman_ford step 6311 current loss 0.461307, current_train_items 201984.
I0302 19:00:35.756139 22895071117440 run.py:483] Algo bellman_ford step 6312 current loss 0.532034, current_train_items 202016.
I0302 19:00:35.785380 22895071117440 run.py:483] Algo bellman_ford step 6313 current loss 0.678354, current_train_items 202048.
I0302 19:00:35.816743 22895071117440 run.py:483] Algo bellman_ford step 6314 current loss 0.790890, current_train_items 202080.
I0302 19:00:35.834847 22895071117440 run.py:483] Algo bellman_ford step 6315 current loss 0.267629, current_train_items 202112.
I0302 19:00:35.849935 22895071117440 run.py:483] Algo bellman_ford step 6316 current loss 0.441588, current_train_items 202144.
I0302 19:00:35.871366 22895071117440 run.py:483] Algo bellman_ford step 6317 current loss 0.605603, current_train_items 202176.
I0302 19:00:35.898981 22895071117440 run.py:483] Algo bellman_ford step 6318 current loss 0.584325, current_train_items 202208.
I0302 19:00:35.928957 22895071117440 run.py:483] Algo bellman_ford step 6319 current loss 0.660353, current_train_items 202240.
I0302 19:00:35.946955 22895071117440 run.py:483] Algo bellman_ford step 6320 current loss 0.301407, current_train_items 202272.
I0302 19:00:35.962609 22895071117440 run.py:483] Algo bellman_ford step 6321 current loss 0.450794, current_train_items 202304.
I0302 19:00:35.985524 22895071117440 run.py:483] Algo bellman_ford step 6322 current loss 0.598410, current_train_items 202336.
I0302 19:00:36.014906 22895071117440 run.py:483] Algo bellman_ford step 6323 current loss 0.779512, current_train_items 202368.
I0302 19:00:36.046283 22895071117440 run.py:483] Algo bellman_ford step 6324 current loss 0.788061, current_train_items 202400.
I0302 19:00:36.064256 22895071117440 run.py:483] Algo bellman_ford step 6325 current loss 0.324292, current_train_items 202432.
I0302 19:00:36.079322 22895071117440 run.py:483] Algo bellman_ford step 6326 current loss 0.491303, current_train_items 202464.
I0302 19:00:36.102145 22895071117440 run.py:483] Algo bellman_ford step 6327 current loss 0.639687, current_train_items 202496.
I0302 19:00:36.130297 22895071117440 run.py:483] Algo bellman_ford step 6328 current loss 0.781525, current_train_items 202528.
I0302 19:00:36.161393 22895071117440 run.py:483] Algo bellman_ford step 6329 current loss 0.768296, current_train_items 202560.
I0302 19:00:36.179342 22895071117440 run.py:483] Algo bellman_ford step 6330 current loss 0.278173, current_train_items 202592.
I0302 19:00:36.194698 22895071117440 run.py:483] Algo bellman_ford step 6331 current loss 0.481414, current_train_items 202624.
I0302 19:00:36.215870 22895071117440 run.py:483] Algo bellman_ford step 6332 current loss 0.615302, current_train_items 202656.
I0302 19:00:36.244145 22895071117440 run.py:483] Algo bellman_ford step 6333 current loss 0.660956, current_train_items 202688.
I0302 19:00:36.274206 22895071117440 run.py:483] Algo bellman_ford step 6334 current loss 0.790162, current_train_items 202720.
I0302 19:00:36.292202 22895071117440 run.py:483] Algo bellman_ford step 6335 current loss 0.286901, current_train_items 202752.
I0302 19:00:36.307764 22895071117440 run.py:483] Algo bellman_ford step 6336 current loss 0.419693, current_train_items 202784.
I0302 19:00:36.330109 22895071117440 run.py:483] Algo bellman_ford step 6337 current loss 0.637429, current_train_items 202816.
I0302 19:00:36.359215 22895071117440 run.py:483] Algo bellman_ford step 6338 current loss 0.963885, current_train_items 202848.
I0302 19:00:36.389689 22895071117440 run.py:483] Algo bellman_ford step 6339 current loss 0.848424, current_train_items 202880.
I0302 19:00:36.407512 22895071117440 run.py:483] Algo bellman_ford step 6340 current loss 0.354380, current_train_items 202912.
I0302 19:00:36.422499 22895071117440 run.py:483] Algo bellman_ford step 6341 current loss 0.493498, current_train_items 202944.
I0302 19:00:36.444825 22895071117440 run.py:483] Algo bellman_ford step 6342 current loss 0.633806, current_train_items 202976.
I0302 19:00:36.472604 22895071117440 run.py:483] Algo bellman_ford step 6343 current loss 0.673099, current_train_items 203008.
I0302 19:00:36.500921 22895071117440 run.py:483] Algo bellman_ford step 6344 current loss 0.788387, current_train_items 203040.
I0302 19:00:36.518716 22895071117440 run.py:483] Algo bellman_ford step 6345 current loss 0.277381, current_train_items 203072.
I0302 19:00:36.534270 22895071117440 run.py:483] Algo bellman_ford step 6346 current loss 0.576584, current_train_items 203104.
I0302 19:00:36.555870 22895071117440 run.py:483] Algo bellman_ford step 6347 current loss 0.648370, current_train_items 203136.
I0302 19:00:36.583602 22895071117440 run.py:483] Algo bellman_ford step 6348 current loss 0.741581, current_train_items 203168.
I0302 19:00:36.616469 22895071117440 run.py:483] Algo bellman_ford step 6349 current loss 0.685849, current_train_items 203200.
I0302 19:00:36.634335 22895071117440 run.py:483] Algo bellman_ford step 6350 current loss 0.401426, current_train_items 203232.
I0302 19:00:36.642124 22895071117440 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:00:36.642232 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:36.658707 22895071117440 run.py:483] Algo bellman_ford step 6351 current loss 0.550003, current_train_items 203264.
I0302 19:00:36.680272 22895071117440 run.py:483] Algo bellman_ford step 6352 current loss 0.682427, current_train_items 203296.
I0302 19:00:36.709158 22895071117440 run.py:483] Algo bellman_ford step 6353 current loss 0.594360, current_train_items 203328.
I0302 19:00:36.740244 22895071117440 run.py:483] Algo bellman_ford step 6354 current loss 0.793414, current_train_items 203360.
I0302 19:00:36.759051 22895071117440 run.py:483] Algo bellman_ford step 6355 current loss 0.341935, current_train_items 203392.
I0302 19:00:36.774466 22895071117440 run.py:483] Algo bellman_ford step 6356 current loss 0.550963, current_train_items 203424.
I0302 19:00:36.796875 22895071117440 run.py:483] Algo bellman_ford step 6357 current loss 0.722024, current_train_items 203456.
I0302 19:00:36.825114 22895071117440 run.py:483] Algo bellman_ford step 6358 current loss 0.761593, current_train_items 203488.
I0302 19:00:36.856577 22895071117440 run.py:483] Algo bellman_ford step 6359 current loss 0.700363, current_train_items 203520.
I0302 19:00:36.875005 22895071117440 run.py:483] Algo bellman_ford step 6360 current loss 0.245149, current_train_items 203552.
I0302 19:00:36.890917 22895071117440 run.py:483] Algo bellman_ford step 6361 current loss 0.476790, current_train_items 203584.
I0302 19:00:36.911859 22895071117440 run.py:483] Algo bellman_ford step 6362 current loss 0.644228, current_train_items 203616.
I0302 19:00:36.940228 22895071117440 run.py:483] Algo bellman_ford step 6363 current loss 0.564905, current_train_items 203648.
I0302 19:00:36.970504 22895071117440 run.py:483] Algo bellman_ford step 6364 current loss 0.726020, current_train_items 203680.
I0302 19:00:36.988631 22895071117440 run.py:483] Algo bellman_ford step 6365 current loss 0.256554, current_train_items 203712.
I0302 19:00:37.004171 22895071117440 run.py:483] Algo bellman_ford step 6366 current loss 0.415328, current_train_items 203744.
I0302 19:00:37.025630 22895071117440 run.py:483] Algo bellman_ford step 6367 current loss 0.678596, current_train_items 203776.
I0302 19:00:37.055139 22895071117440 run.py:483] Algo bellman_ford step 6368 current loss 0.931663, current_train_items 203808.
I0302 19:00:37.084368 22895071117440 run.py:483] Algo bellman_ford step 6369 current loss 0.729718, current_train_items 203840.
I0302 19:00:37.102565 22895071117440 run.py:483] Algo bellman_ford step 6370 current loss 0.335625, current_train_items 203872.
I0302 19:00:37.118404 22895071117440 run.py:483] Algo bellman_ford step 6371 current loss 0.523970, current_train_items 203904.
I0302 19:00:37.140572 22895071117440 run.py:483] Algo bellman_ford step 6372 current loss 0.557661, current_train_items 203936.
I0302 19:00:37.168269 22895071117440 run.py:483] Algo bellman_ford step 6373 current loss 0.671852, current_train_items 203968.
I0302 19:00:37.198880 22895071117440 run.py:483] Algo bellman_ford step 6374 current loss 0.807125, current_train_items 204000.
I0302 19:00:37.217443 22895071117440 run.py:483] Algo bellman_ford step 6375 current loss 0.289180, current_train_items 204032.
I0302 19:00:37.232797 22895071117440 run.py:483] Algo bellman_ford step 6376 current loss 0.496291, current_train_items 204064.
I0302 19:00:37.254996 22895071117440 run.py:483] Algo bellman_ford step 6377 current loss 0.694178, current_train_items 204096.
I0302 19:00:37.283143 22895071117440 run.py:483] Algo bellman_ford step 6378 current loss 0.533774, current_train_items 204128.
I0302 19:00:37.315740 22895071117440 run.py:483] Algo bellman_ford step 6379 current loss 0.861195, current_train_items 204160.
I0302 19:00:37.333447 22895071117440 run.py:483] Algo bellman_ford step 6380 current loss 0.280394, current_train_items 204192.
I0302 19:00:37.348640 22895071117440 run.py:483] Algo bellman_ford step 6381 current loss 0.500930, current_train_items 204224.
I0302 19:00:37.370630 22895071117440 run.py:483] Algo bellman_ford step 6382 current loss 0.538613, current_train_items 204256.
I0302 19:00:37.397931 22895071117440 run.py:483] Algo bellman_ford step 6383 current loss 0.858294, current_train_items 204288.
I0302 19:00:37.428345 22895071117440 run.py:483] Algo bellman_ford step 6384 current loss 0.981452, current_train_items 204320.
I0302 19:00:37.446948 22895071117440 run.py:483] Algo bellman_ford step 6385 current loss 0.342014, current_train_items 204352.
I0302 19:00:37.462171 22895071117440 run.py:483] Algo bellman_ford step 6386 current loss 0.504453, current_train_items 204384.
I0302 19:00:37.484223 22895071117440 run.py:483] Algo bellman_ford step 6387 current loss 0.675665, current_train_items 204416.
I0302 19:00:37.512121 22895071117440 run.py:483] Algo bellman_ford step 6388 current loss 0.655416, current_train_items 204448.
I0302 19:00:37.542376 22895071117440 run.py:483] Algo bellman_ford step 6389 current loss 0.696748, current_train_items 204480.
I0302 19:00:37.560917 22895071117440 run.py:483] Algo bellman_ford step 6390 current loss 0.297435, current_train_items 204512.
I0302 19:00:37.576842 22895071117440 run.py:483] Algo bellman_ford step 6391 current loss 0.489140, current_train_items 204544.
I0302 19:00:37.598330 22895071117440 run.py:483] Algo bellman_ford step 6392 current loss 0.654750, current_train_items 204576.
I0302 19:00:37.624988 22895071117440 run.py:483] Algo bellman_ford step 6393 current loss 0.673258, current_train_items 204608.
I0302 19:00:37.655992 22895071117440 run.py:483] Algo bellman_ford step 6394 current loss 0.927117, current_train_items 204640.
I0302 19:00:37.674284 22895071117440 run.py:483] Algo bellman_ford step 6395 current loss 0.382988, current_train_items 204672.
I0302 19:00:37.689555 22895071117440 run.py:483] Algo bellman_ford step 6396 current loss 0.441779, current_train_items 204704.
I0302 19:00:37.710606 22895071117440 run.py:483] Algo bellman_ford step 6397 current loss 0.656020, current_train_items 204736.
I0302 19:00:37.738005 22895071117440 run.py:483] Algo bellman_ford step 6398 current loss 0.713346, current_train_items 204768.
I0302 19:00:37.766808 22895071117440 run.py:483] Algo bellman_ford step 6399 current loss 0.721641, current_train_items 204800.
I0302 19:00:37.785424 22895071117440 run.py:483] Algo bellman_ford step 6400 current loss 0.310481, current_train_items 204832.
I0302 19:00:37.793378 22895071117440 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:00:37.793483 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:00:37.808797 22895071117440 run.py:483] Algo bellman_ford step 6401 current loss 0.457150, current_train_items 204864.
I0302 19:00:37.830561 22895071117440 run.py:483] Algo bellman_ford step 6402 current loss 0.602355, current_train_items 204896.
I0302 19:00:37.859209 22895071117440 run.py:483] Algo bellman_ford step 6403 current loss 0.681277, current_train_items 204928.
I0302 19:00:37.890486 22895071117440 run.py:483] Algo bellman_ford step 6404 current loss 0.760207, current_train_items 204960.
I0302 19:00:37.908836 22895071117440 run.py:483] Algo bellman_ford step 6405 current loss 0.296897, current_train_items 204992.
I0302 19:00:37.924018 22895071117440 run.py:483] Algo bellman_ford step 6406 current loss 0.488796, current_train_items 205024.
I0302 19:00:37.945294 22895071117440 run.py:483] Algo bellman_ford step 6407 current loss 0.560569, current_train_items 205056.
I0302 19:00:37.973717 22895071117440 run.py:483] Algo bellman_ford step 6408 current loss 0.645373, current_train_items 205088.
I0302 19:00:38.005825 22895071117440 run.py:483] Algo bellman_ford step 6409 current loss 0.778939, current_train_items 205120.
I0302 19:00:38.023865 22895071117440 run.py:483] Algo bellman_ford step 6410 current loss 0.248101, current_train_items 205152.
I0302 19:00:38.038972 22895071117440 run.py:483] Algo bellman_ford step 6411 current loss 0.463080, current_train_items 205184.
I0302 19:00:38.059922 22895071117440 run.py:483] Algo bellman_ford step 6412 current loss 0.597621, current_train_items 205216.
I0302 19:00:38.088344 22895071117440 run.py:483] Algo bellman_ford step 6413 current loss 0.770739, current_train_items 205248.
I0302 19:00:38.119347 22895071117440 run.py:483] Algo bellman_ford step 6414 current loss 0.692261, current_train_items 205280.
I0302 19:00:38.137242 22895071117440 run.py:483] Algo bellman_ford step 6415 current loss 0.252907, current_train_items 205312.
I0302 19:00:38.152479 22895071117440 run.py:483] Algo bellman_ford step 6416 current loss 0.411802, current_train_items 205344.
I0302 19:00:38.174903 22895071117440 run.py:483] Algo bellman_ford step 6417 current loss 0.587891, current_train_items 205376.
I0302 19:00:38.204599 22895071117440 run.py:483] Algo bellman_ford step 6418 current loss 0.744590, current_train_items 205408.
I0302 19:00:38.236169 22895071117440 run.py:483] Algo bellman_ford step 6419 current loss 0.896053, current_train_items 205440.
I0302 19:00:38.254439 22895071117440 run.py:483] Algo bellman_ford step 6420 current loss 0.299776, current_train_items 205472.
I0302 19:00:38.269844 22895071117440 run.py:483] Algo bellman_ford step 6421 current loss 0.462810, current_train_items 205504.
I0302 19:00:38.291522 22895071117440 run.py:483] Algo bellman_ford step 6422 current loss 0.619888, current_train_items 205536.
I0302 19:00:38.319782 22895071117440 run.py:483] Algo bellman_ford step 6423 current loss 0.662656, current_train_items 205568.
I0302 19:00:38.350040 22895071117440 run.py:483] Algo bellman_ford step 6424 current loss 0.776231, current_train_items 205600.
I0302 19:00:38.368059 22895071117440 run.py:483] Algo bellman_ford step 6425 current loss 0.342157, current_train_items 205632.
I0302 19:00:38.383781 22895071117440 run.py:483] Algo bellman_ford step 6426 current loss 0.499016, current_train_items 205664.
I0302 19:00:38.405608 22895071117440 run.py:483] Algo bellman_ford step 6427 current loss 0.596437, current_train_items 205696.
I0302 19:00:38.434289 22895071117440 run.py:483] Algo bellman_ford step 6428 current loss 0.701373, current_train_items 205728.
I0302 19:00:38.467300 22895071117440 run.py:483] Algo bellman_ford step 6429 current loss 0.913116, current_train_items 205760.
I0302 19:00:38.485643 22895071117440 run.py:483] Algo bellman_ford step 6430 current loss 0.320991, current_train_items 205792.
I0302 19:00:38.500488 22895071117440 run.py:483] Algo bellman_ford step 6431 current loss 0.534068, current_train_items 205824.
I0302 19:00:38.522531 22895071117440 run.py:483] Algo bellman_ford step 6432 current loss 0.718056, current_train_items 205856.
I0302 19:00:38.551053 22895071117440 run.py:483] Algo bellman_ford step 6433 current loss 0.629660, current_train_items 205888.
I0302 19:00:38.580841 22895071117440 run.py:483] Algo bellman_ford step 6434 current loss 0.752180, current_train_items 205920.
I0302 19:00:38.598806 22895071117440 run.py:483] Algo bellman_ford step 6435 current loss 0.316136, current_train_items 205952.
I0302 19:00:38.614087 22895071117440 run.py:483] Algo bellman_ford step 6436 current loss 0.532661, current_train_items 205984.
I0302 19:00:38.635478 22895071117440 run.py:483] Algo bellman_ford step 6437 current loss 0.590633, current_train_items 206016.
I0302 19:00:38.664585 22895071117440 run.py:483] Algo bellman_ford step 6438 current loss 0.739224, current_train_items 206048.
I0302 19:00:38.696190 22895071117440 run.py:483] Algo bellman_ford step 6439 current loss 0.738748, current_train_items 206080.
I0302 19:00:38.714144 22895071117440 run.py:483] Algo bellman_ford step 6440 current loss 0.264756, current_train_items 206112.
I0302 19:00:38.729779 22895071117440 run.py:483] Algo bellman_ford step 6441 current loss 0.560075, current_train_items 206144.
I0302 19:00:38.750524 22895071117440 run.py:483] Algo bellman_ford step 6442 current loss 0.599450, current_train_items 206176.
I0302 19:00:38.777736 22895071117440 run.py:483] Algo bellman_ford step 6443 current loss 0.653036, current_train_items 206208.
I0302 19:00:38.808990 22895071117440 run.py:483] Algo bellman_ford step 6444 current loss 0.694810, current_train_items 206240.
I0302 19:00:38.827214 22895071117440 run.py:483] Algo bellman_ford step 6445 current loss 0.256610, current_train_items 206272.
I0302 19:00:38.842313 22895071117440 run.py:483] Algo bellman_ford step 6446 current loss 0.490130, current_train_items 206304.
I0302 19:00:38.865010 22895071117440 run.py:483] Algo bellman_ford step 6447 current loss 0.623482, current_train_items 206336.
I0302 19:00:38.893692 22895071117440 run.py:483] Algo bellman_ford step 6448 current loss 0.728430, current_train_items 206368.
I0302 19:00:38.924452 22895071117440 run.py:483] Algo bellman_ford step 6449 current loss 0.807760, current_train_items 206400.
I0302 19:00:38.942623 22895071117440 run.py:483] Algo bellman_ford step 6450 current loss 0.308649, current_train_items 206432.
I0302 19:00:38.950527 22895071117440 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:00:38.950634 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:38.966706 22895071117440 run.py:483] Algo bellman_ford step 6451 current loss 0.467347, current_train_items 206464.
I0302 19:00:38.990057 22895071117440 run.py:483] Algo bellman_ford step 6452 current loss 0.654507, current_train_items 206496.
I0302 19:00:39.017543 22895071117440 run.py:483] Algo bellman_ford step 6453 current loss 0.625541, current_train_items 206528.
I0302 19:00:39.047161 22895071117440 run.py:483] Algo bellman_ford step 6454 current loss 0.621134, current_train_items 206560.
I0302 19:00:39.065297 22895071117440 run.py:483] Algo bellman_ford step 6455 current loss 0.367323, current_train_items 206592.
I0302 19:00:39.080647 22895071117440 run.py:483] Algo bellman_ford step 6456 current loss 0.416731, current_train_items 206624.
I0302 19:00:39.101595 22895071117440 run.py:483] Algo bellman_ford step 6457 current loss 0.630470, current_train_items 206656.
I0302 19:00:39.129643 22895071117440 run.py:483] Algo bellman_ford step 6458 current loss 0.636602, current_train_items 206688.
I0302 19:00:39.158935 22895071117440 run.py:483] Algo bellman_ford step 6459 current loss 0.675490, current_train_items 206720.
I0302 19:00:39.177364 22895071117440 run.py:483] Algo bellman_ford step 6460 current loss 0.358156, current_train_items 206752.
I0302 19:00:39.193098 22895071117440 run.py:483] Algo bellman_ford step 6461 current loss 0.475768, current_train_items 206784.
I0302 19:00:39.214362 22895071117440 run.py:483] Algo bellman_ford step 6462 current loss 0.625017, current_train_items 206816.
I0302 19:00:39.242176 22895071117440 run.py:483] Algo bellman_ford step 6463 current loss 0.645198, current_train_items 206848.
I0302 19:00:39.275101 22895071117440 run.py:483] Algo bellman_ford step 6464 current loss 0.804685, current_train_items 206880.
I0302 19:00:39.292698 22895071117440 run.py:483] Algo bellman_ford step 6465 current loss 0.212037, current_train_items 206912.
I0302 19:00:39.308149 22895071117440 run.py:483] Algo bellman_ford step 6466 current loss 0.436834, current_train_items 206944.
I0302 19:00:39.331043 22895071117440 run.py:483] Algo bellman_ford step 6467 current loss 0.760858, current_train_items 206976.
I0302 19:00:39.360543 22895071117440 run.py:483] Algo bellman_ford step 6468 current loss 0.775343, current_train_items 207008.
I0302 19:00:39.392426 22895071117440 run.py:483] Algo bellman_ford step 6469 current loss 0.762942, current_train_items 207040.
I0302 19:00:39.410986 22895071117440 run.py:483] Algo bellman_ford step 6470 current loss 0.366614, current_train_items 207072.
I0302 19:00:39.427164 22895071117440 run.py:483] Algo bellman_ford step 6471 current loss 0.478490, current_train_items 207104.
I0302 19:00:39.448037 22895071117440 run.py:483] Algo bellman_ford step 6472 current loss 0.543257, current_train_items 207136.
I0302 19:00:39.476350 22895071117440 run.py:483] Algo bellman_ford step 6473 current loss 0.831312, current_train_items 207168.
I0302 19:00:39.506045 22895071117440 run.py:483] Algo bellman_ford step 6474 current loss 0.765727, current_train_items 207200.
I0302 19:00:39.524541 22895071117440 run.py:483] Algo bellman_ford step 6475 current loss 0.326563, current_train_items 207232.
I0302 19:00:39.540372 22895071117440 run.py:483] Algo bellman_ford step 6476 current loss 0.534279, current_train_items 207264.
I0302 19:00:39.560866 22895071117440 run.py:483] Algo bellman_ford step 6477 current loss 0.525697, current_train_items 207296.
I0302 19:00:39.589918 22895071117440 run.py:483] Algo bellman_ford step 6478 current loss 0.655510, current_train_items 207328.
I0302 19:00:39.621606 22895071117440 run.py:483] Algo bellman_ford step 6479 current loss 0.754223, current_train_items 207360.
I0302 19:00:39.639652 22895071117440 run.py:483] Algo bellman_ford step 6480 current loss 0.356058, current_train_items 207392.
I0302 19:00:39.654702 22895071117440 run.py:483] Algo bellman_ford step 6481 current loss 0.458016, current_train_items 207424.
I0302 19:00:39.676401 22895071117440 run.py:483] Algo bellman_ford step 6482 current loss 0.609723, current_train_items 207456.
I0302 19:00:39.705583 22895071117440 run.py:483] Algo bellman_ford step 6483 current loss 0.728303, current_train_items 207488.
I0302 19:00:39.737288 22895071117440 run.py:483] Algo bellman_ford step 6484 current loss 0.742654, current_train_items 207520.
I0302 19:00:39.755745 22895071117440 run.py:483] Algo bellman_ford step 6485 current loss 0.361936, current_train_items 207552.
I0302 19:00:39.771626 22895071117440 run.py:483] Algo bellman_ford step 6486 current loss 0.449463, current_train_items 207584.
I0302 19:00:39.794675 22895071117440 run.py:483] Algo bellman_ford step 6487 current loss 0.686211, current_train_items 207616.
I0302 19:00:39.821746 22895071117440 run.py:483] Algo bellman_ford step 6488 current loss 0.700895, current_train_items 207648.
I0302 19:00:39.853241 22895071117440 run.py:483] Algo bellman_ford step 6489 current loss 0.700972, current_train_items 207680.
I0302 19:00:39.871893 22895071117440 run.py:483] Algo bellman_ford step 6490 current loss 0.362297, current_train_items 207712.
I0302 19:00:39.887481 22895071117440 run.py:483] Algo bellman_ford step 6491 current loss 0.439196, current_train_items 207744.
I0302 19:00:39.908576 22895071117440 run.py:483] Algo bellman_ford step 6492 current loss 0.645431, current_train_items 207776.
I0302 19:00:39.936714 22895071117440 run.py:483] Algo bellman_ford step 6493 current loss 0.655267, current_train_items 207808.
I0302 19:00:39.967161 22895071117440 run.py:483] Algo bellman_ford step 6494 current loss 0.858058, current_train_items 207840.
I0302 19:00:39.985201 22895071117440 run.py:483] Algo bellman_ford step 6495 current loss 0.288949, current_train_items 207872.
I0302 19:00:40.000219 22895071117440 run.py:483] Algo bellman_ford step 6496 current loss 0.472340, current_train_items 207904.
I0302 19:00:40.022759 22895071117440 run.py:483] Algo bellman_ford step 6497 current loss 0.694713, current_train_items 207936.
I0302 19:00:40.050594 22895071117440 run.py:483] Algo bellman_ford step 6498 current loss 0.630247, current_train_items 207968.
I0302 19:00:40.082930 22895071117440 run.py:483] Algo bellman_ford step 6499 current loss 0.950985, current_train_items 208000.
I0302 19:00:40.101185 22895071117440 run.py:483] Algo bellman_ford step 6500 current loss 0.370296, current_train_items 208032.
I0302 19:00:40.108984 22895071117440 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:00:40.109091 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:40.125161 22895071117440 run.py:483] Algo bellman_ford step 6501 current loss 0.435348, current_train_items 208064.
I0302 19:00:40.147944 22895071117440 run.py:483] Algo bellman_ford step 6502 current loss 0.638729, current_train_items 208096.
I0302 19:00:40.176237 22895071117440 run.py:483] Algo bellman_ford step 6503 current loss 0.702049, current_train_items 208128.
I0302 19:00:40.208606 22895071117440 run.py:483] Algo bellman_ford step 6504 current loss 0.748616, current_train_items 208160.
I0302 19:00:40.227128 22895071117440 run.py:483] Algo bellman_ford step 6505 current loss 0.283655, current_train_items 208192.
I0302 19:00:40.241919 22895071117440 run.py:483] Algo bellman_ford step 6506 current loss 0.334119, current_train_items 208224.
I0302 19:00:40.263754 22895071117440 run.py:483] Algo bellman_ford step 6507 current loss 0.649075, current_train_items 208256.
I0302 19:00:40.293093 22895071117440 run.py:483] Algo bellman_ford step 6508 current loss 0.700253, current_train_items 208288.
I0302 19:00:40.321691 22895071117440 run.py:483] Algo bellman_ford step 6509 current loss 0.875648, current_train_items 208320.
I0302 19:00:40.339607 22895071117440 run.py:483] Algo bellman_ford step 6510 current loss 0.301794, current_train_items 208352.
I0302 19:00:40.354998 22895071117440 run.py:483] Algo bellman_ford step 6511 current loss 0.476465, current_train_items 208384.
I0302 19:00:40.377636 22895071117440 run.py:483] Algo bellman_ford step 6512 current loss 0.676684, current_train_items 208416.
I0302 19:00:40.405729 22895071117440 run.py:483] Algo bellman_ford step 6513 current loss 0.604496, current_train_items 208448.
I0302 19:00:40.437642 22895071117440 run.py:483] Algo bellman_ford step 6514 current loss 0.822016, current_train_items 208480.
I0302 19:00:40.455411 22895071117440 run.py:483] Algo bellman_ford step 6515 current loss 0.288977, current_train_items 208512.
I0302 19:00:40.470630 22895071117440 run.py:483] Algo bellman_ford step 6516 current loss 0.536048, current_train_items 208544.
I0302 19:00:40.492823 22895071117440 run.py:483] Algo bellman_ford step 6517 current loss 0.607171, current_train_items 208576.
I0302 19:00:40.521217 22895071117440 run.py:483] Algo bellman_ford step 6518 current loss 0.578374, current_train_items 208608.
I0302 19:00:40.551468 22895071117440 run.py:483] Algo bellman_ford step 6519 current loss 0.735219, current_train_items 208640.
I0302 19:00:40.569563 22895071117440 run.py:483] Algo bellman_ford step 6520 current loss 0.329202, current_train_items 208672.
I0302 19:00:40.584937 22895071117440 run.py:483] Algo bellman_ford step 6521 current loss 0.475896, current_train_items 208704.
I0302 19:00:40.607386 22895071117440 run.py:483] Algo bellman_ford step 6522 current loss 0.662233, current_train_items 208736.
I0302 19:00:40.635171 22895071117440 run.py:483] Algo bellman_ford step 6523 current loss 0.671913, current_train_items 208768.
I0302 19:00:40.663943 22895071117440 run.py:483] Algo bellman_ford step 6524 current loss 0.700068, current_train_items 208800.
I0302 19:00:40.682219 22895071117440 run.py:483] Algo bellman_ford step 6525 current loss 0.251245, current_train_items 208832.
I0302 19:00:40.697355 22895071117440 run.py:483] Algo bellman_ford step 6526 current loss 0.430978, current_train_items 208864.
I0302 19:00:40.718595 22895071117440 run.py:483] Algo bellman_ford step 6527 current loss 0.566619, current_train_items 208896.
I0302 19:00:40.745850 22895071117440 run.py:483] Algo bellman_ford step 6528 current loss 0.625603, current_train_items 208928.
I0302 19:00:40.777059 22895071117440 run.py:483] Algo bellman_ford step 6529 current loss 0.774857, current_train_items 208960.
I0302 19:00:40.795040 22895071117440 run.py:483] Algo bellman_ford step 6530 current loss 0.299880, current_train_items 208992.
I0302 19:00:40.810539 22895071117440 run.py:483] Algo bellman_ford step 6531 current loss 0.491367, current_train_items 209024.
I0302 19:00:40.833731 22895071117440 run.py:483] Algo bellman_ford step 6532 current loss 0.668380, current_train_items 209056.
I0302 19:00:40.861477 22895071117440 run.py:483] Algo bellman_ford step 6533 current loss 0.704587, current_train_items 209088.
I0302 19:00:40.892409 22895071117440 run.py:483] Algo bellman_ford step 6534 current loss 0.774975, current_train_items 209120.
I0302 19:00:40.910305 22895071117440 run.py:483] Algo bellman_ford step 6535 current loss 0.256342, current_train_items 209152.
I0302 19:00:40.925723 22895071117440 run.py:483] Algo bellman_ford step 6536 current loss 0.472057, current_train_items 209184.
I0302 19:00:40.947071 22895071117440 run.py:483] Algo bellman_ford step 6537 current loss 0.565254, current_train_items 209216.
I0302 19:00:40.974644 22895071117440 run.py:483] Algo bellman_ford step 6538 current loss 0.701287, current_train_items 209248.
I0302 19:00:41.005829 22895071117440 run.py:483] Algo bellman_ford step 6539 current loss 0.757247, current_train_items 209280.
I0302 19:00:41.023769 22895071117440 run.py:483] Algo bellman_ford step 6540 current loss 0.314338, current_train_items 209312.
I0302 19:00:41.039139 22895071117440 run.py:483] Algo bellman_ford step 6541 current loss 0.532270, current_train_items 209344.
I0302 19:00:41.060623 22895071117440 run.py:483] Algo bellman_ford step 6542 current loss 0.626499, current_train_items 209376.
I0302 19:00:41.089286 22895071117440 run.py:483] Algo bellman_ford step 6543 current loss 0.769248, current_train_items 209408.
I0302 19:00:41.122075 22895071117440 run.py:483] Algo bellman_ford step 6544 current loss 0.796793, current_train_items 209440.
I0302 19:00:41.140274 22895071117440 run.py:483] Algo bellman_ford step 6545 current loss 0.255717, current_train_items 209472.
I0302 19:00:41.155369 22895071117440 run.py:483] Algo bellman_ford step 6546 current loss 0.425387, current_train_items 209504.
I0302 19:00:41.176787 22895071117440 run.py:483] Algo bellman_ford step 6547 current loss 0.529222, current_train_items 209536.
I0302 19:00:41.206126 22895071117440 run.py:483] Algo bellman_ford step 6548 current loss 0.727949, current_train_items 209568.
I0302 19:00:41.237795 22895071117440 run.py:483] Algo bellman_ford step 6549 current loss 0.756375, current_train_items 209600.
I0302 19:00:41.255934 22895071117440 run.py:483] Algo bellman_ford step 6550 current loss 0.276334, current_train_items 209632.
I0302 19:00:41.264361 22895071117440 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:00:41.264467 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:41.280787 22895071117440 run.py:483] Algo bellman_ford step 6551 current loss 0.450616, current_train_items 209664.
I0302 19:00:41.302638 22895071117440 run.py:483] Algo bellman_ford step 6552 current loss 0.551295, current_train_items 209696.
I0302 19:00:41.332681 22895071117440 run.py:483] Algo bellman_ford step 6553 current loss 0.643502, current_train_items 209728.
I0302 19:00:41.365649 22895071117440 run.py:483] Algo bellman_ford step 6554 current loss 0.876574, current_train_items 209760.
I0302 19:00:41.383966 22895071117440 run.py:483] Algo bellman_ford step 6555 current loss 0.352323, current_train_items 209792.
I0302 19:00:41.399202 22895071117440 run.py:483] Algo bellman_ford step 6556 current loss 0.493963, current_train_items 209824.
I0302 19:00:41.421115 22895071117440 run.py:483] Algo bellman_ford step 6557 current loss 0.649454, current_train_items 209856.
I0302 19:00:41.450166 22895071117440 run.py:483] Algo bellman_ford step 6558 current loss 0.699333, current_train_items 209888.
I0302 19:00:41.481412 22895071117440 run.py:483] Algo bellman_ford step 6559 current loss 0.754974, current_train_items 209920.
I0302 19:00:41.499917 22895071117440 run.py:483] Algo bellman_ford step 6560 current loss 0.275509, current_train_items 209952.
I0302 19:00:41.515155 22895071117440 run.py:483] Algo bellman_ford step 6561 current loss 0.437997, current_train_items 209984.
I0302 19:00:41.535807 22895071117440 run.py:483] Algo bellman_ford step 6562 current loss 0.534759, current_train_items 210016.
I0302 19:00:41.564099 22895071117440 run.py:483] Algo bellman_ford step 6563 current loss 0.661786, current_train_items 210048.
I0302 19:00:41.592621 22895071117440 run.py:483] Algo bellman_ford step 6564 current loss 0.759488, current_train_items 210080.
I0302 19:00:41.610583 22895071117440 run.py:483] Algo bellman_ford step 6565 current loss 0.284337, current_train_items 210112.
I0302 19:00:41.625530 22895071117440 run.py:483] Algo bellman_ford step 6566 current loss 0.421447, current_train_items 210144.
I0302 19:00:41.647367 22895071117440 run.py:483] Algo bellman_ford step 6567 current loss 0.605220, current_train_items 210176.
I0302 19:00:41.674176 22895071117440 run.py:483] Algo bellman_ford step 6568 current loss 0.679012, current_train_items 210208.
I0302 19:00:41.705227 22895071117440 run.py:483] Algo bellman_ford step 6569 current loss 0.685809, current_train_items 210240.
I0302 19:00:41.724028 22895071117440 run.py:483] Algo bellman_ford step 6570 current loss 0.317476, current_train_items 210272.
I0302 19:00:41.739680 22895071117440 run.py:483] Algo bellman_ford step 6571 current loss 0.540209, current_train_items 210304.
I0302 19:00:41.761104 22895071117440 run.py:483] Algo bellman_ford step 6572 current loss 0.524004, current_train_items 210336.
I0302 19:00:41.789130 22895071117440 run.py:483] Algo bellman_ford step 6573 current loss 0.548527, current_train_items 210368.
I0302 19:00:41.819290 22895071117440 run.py:483] Algo bellman_ford step 6574 current loss 0.756348, current_train_items 210400.
I0302 19:00:41.837553 22895071117440 run.py:483] Algo bellman_ford step 6575 current loss 0.300864, current_train_items 210432.
I0302 19:00:41.853330 22895071117440 run.py:483] Algo bellman_ford step 6576 current loss 0.439917, current_train_items 210464.
I0302 19:00:41.874442 22895071117440 run.py:483] Algo bellman_ford step 6577 current loss 0.589861, current_train_items 210496.
I0302 19:00:41.902995 22895071117440 run.py:483] Algo bellman_ford step 6578 current loss 0.661966, current_train_items 210528.
I0302 19:00:41.933873 22895071117440 run.py:483] Algo bellman_ford step 6579 current loss 0.749838, current_train_items 210560.
I0302 19:00:41.952206 22895071117440 run.py:483] Algo bellman_ford step 6580 current loss 0.346239, current_train_items 210592.
I0302 19:00:41.967549 22895071117440 run.py:483] Algo bellman_ford step 6581 current loss 0.417806, current_train_items 210624.
I0302 19:00:41.989393 22895071117440 run.py:483] Algo bellman_ford step 6582 current loss 0.595387, current_train_items 210656.
I0302 19:00:42.018776 22895071117440 run.py:483] Algo bellman_ford step 6583 current loss 0.684155, current_train_items 210688.
I0302 19:00:42.049084 22895071117440 run.py:483] Algo bellman_ford step 6584 current loss 0.712788, current_train_items 210720.
I0302 19:00:42.067538 22895071117440 run.py:483] Algo bellman_ford step 6585 current loss 0.342206, current_train_items 210752.
I0302 19:00:42.082676 22895071117440 run.py:483] Algo bellman_ford step 6586 current loss 0.439987, current_train_items 210784.
I0302 19:00:42.103161 22895071117440 run.py:483] Algo bellman_ford step 6587 current loss 0.565963, current_train_items 210816.
I0302 19:00:42.130804 22895071117440 run.py:483] Algo bellman_ford step 6588 current loss 0.711922, current_train_items 210848.
I0302 19:00:42.160033 22895071117440 run.py:483] Algo bellman_ford step 6589 current loss 0.741883, current_train_items 210880.
I0302 19:00:42.178444 22895071117440 run.py:483] Algo bellman_ford step 6590 current loss 0.308040, current_train_items 210912.
I0302 19:00:42.193843 22895071117440 run.py:483] Algo bellman_ford step 6591 current loss 0.461989, current_train_items 210944.
I0302 19:00:42.215867 22895071117440 run.py:483] Algo bellman_ford step 6592 current loss 0.542876, current_train_items 210976.
I0302 19:00:42.243798 22895071117440 run.py:483] Algo bellman_ford step 6593 current loss 0.653400, current_train_items 211008.
I0302 19:00:42.273693 22895071117440 run.py:483] Algo bellman_ford step 6594 current loss 0.847239, current_train_items 211040.
I0302 19:00:42.291748 22895071117440 run.py:483] Algo bellman_ford step 6595 current loss 0.286020, current_train_items 211072.
I0302 19:00:42.306964 22895071117440 run.py:483] Algo bellman_ford step 6596 current loss 0.427648, current_train_items 211104.
I0302 19:00:42.329336 22895071117440 run.py:483] Algo bellman_ford step 6597 current loss 0.599802, current_train_items 211136.
I0302 19:00:42.358257 22895071117440 run.py:483] Algo bellman_ford step 6598 current loss 0.723490, current_train_items 211168.
I0302 19:00:42.385678 22895071117440 run.py:483] Algo bellman_ford step 6599 current loss 0.769480, current_train_items 211200.
I0302 19:00:42.404246 22895071117440 run.py:483] Algo bellman_ford step 6600 current loss 0.253880, current_train_items 211232.
I0302 19:00:42.411998 22895071117440 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:00:42.412103 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:00:42.427963 22895071117440 run.py:483] Algo bellman_ford step 6601 current loss 0.482741, current_train_items 211264.
I0302 19:00:42.448633 22895071117440 run.py:483] Algo bellman_ford step 6602 current loss 0.591082, current_train_items 211296.
I0302 19:00:42.477312 22895071117440 run.py:483] Algo bellman_ford step 6603 current loss 0.735836, current_train_items 211328.
I0302 19:00:42.509049 22895071117440 run.py:483] Algo bellman_ford step 6604 current loss 0.749788, current_train_items 211360.
I0302 19:00:42.527343 22895071117440 run.py:483] Algo bellman_ford step 6605 current loss 0.278077, current_train_items 211392.
I0302 19:00:42.542486 22895071117440 run.py:483] Algo bellman_ford step 6606 current loss 0.428059, current_train_items 211424.
I0302 19:00:42.564265 22895071117440 run.py:483] Algo bellman_ford step 6607 current loss 0.608611, current_train_items 211456.
I0302 19:00:42.593181 22895071117440 run.py:483] Algo bellman_ford step 6608 current loss 0.543646, current_train_items 211488.
I0302 19:00:42.622254 22895071117440 run.py:483] Algo bellman_ford step 6609 current loss 0.652129, current_train_items 211520.
I0302 19:00:42.640578 22895071117440 run.py:483] Algo bellman_ford step 6610 current loss 0.267066, current_train_items 211552.
I0302 19:00:42.655674 22895071117440 run.py:483] Algo bellman_ford step 6611 current loss 0.432571, current_train_items 211584.
I0302 19:00:42.676431 22895071117440 run.py:483] Algo bellman_ford step 6612 current loss 0.574291, current_train_items 211616.
I0302 19:00:42.705019 22895071117440 run.py:483] Algo bellman_ford step 6613 current loss 0.740210, current_train_items 211648.
I0302 19:00:42.738524 22895071117440 run.py:483] Algo bellman_ford step 6614 current loss 0.853974, current_train_items 211680.
I0302 19:00:42.756449 22895071117440 run.py:483] Algo bellman_ford step 6615 current loss 0.295025, current_train_items 211712.
I0302 19:00:42.771625 22895071117440 run.py:483] Algo bellman_ford step 6616 current loss 0.446508, current_train_items 211744.
I0302 19:00:42.793770 22895071117440 run.py:483] Algo bellman_ford step 6617 current loss 0.698914, current_train_items 211776.
I0302 19:00:42.821830 22895071117440 run.py:483] Algo bellman_ford step 6618 current loss 0.685050, current_train_items 211808.
I0302 19:00:42.852990 22895071117440 run.py:483] Algo bellman_ford step 6619 current loss 0.855612, current_train_items 211840.
I0302 19:00:42.870873 22895071117440 run.py:483] Algo bellman_ford step 6620 current loss 0.383891, current_train_items 211872.
I0302 19:00:42.885804 22895071117440 run.py:483] Algo bellman_ford step 6621 current loss 0.426293, current_train_items 211904.
I0302 19:00:42.906260 22895071117440 run.py:483] Algo bellman_ford step 6622 current loss 0.534092, current_train_items 211936.
I0302 19:00:42.935330 22895071117440 run.py:483] Algo bellman_ford step 6623 current loss 0.608163, current_train_items 211968.
I0302 19:00:42.964832 22895071117440 run.py:483] Algo bellman_ford step 6624 current loss 0.735539, current_train_items 212000.
I0302 19:00:42.983057 22895071117440 run.py:483] Algo bellman_ford step 6625 current loss 0.328100, current_train_items 212032.
I0302 19:00:42.998436 22895071117440 run.py:483] Algo bellman_ford step 6626 current loss 0.452862, current_train_items 212064.
I0302 19:00:43.020036 22895071117440 run.py:483] Algo bellman_ford step 6627 current loss 0.616239, current_train_items 212096.
I0302 19:00:43.047259 22895071117440 run.py:483] Algo bellman_ford step 6628 current loss 0.724864, current_train_items 212128.
I0302 19:00:43.076512 22895071117440 run.py:483] Algo bellman_ford step 6629 current loss 0.712502, current_train_items 212160.
I0302 19:00:43.094111 22895071117440 run.py:483] Algo bellman_ford step 6630 current loss 0.376486, current_train_items 212192.
I0302 19:00:43.109475 22895071117440 run.py:483] Algo bellman_ford step 6631 current loss 0.519404, current_train_items 212224.
I0302 19:00:43.132145 22895071117440 run.py:483] Algo bellman_ford step 6632 current loss 0.669786, current_train_items 212256.
I0302 19:00:43.161324 22895071117440 run.py:483] Algo bellman_ford step 6633 current loss 0.710394, current_train_items 212288.
I0302 19:00:43.190995 22895071117440 run.py:483] Algo bellman_ford step 6634 current loss 0.821764, current_train_items 212320.
I0302 19:00:43.209306 22895071117440 run.py:483] Algo bellman_ford step 6635 current loss 0.294700, current_train_items 212352.
I0302 19:00:43.224722 22895071117440 run.py:483] Algo bellman_ford step 6636 current loss 0.557290, current_train_items 212384.
I0302 19:00:43.246426 22895071117440 run.py:483] Algo bellman_ford step 6637 current loss 0.610522, current_train_items 212416.
I0302 19:00:43.274655 22895071117440 run.py:483] Algo bellman_ford step 6638 current loss 0.654091, current_train_items 212448.
I0302 19:00:43.305848 22895071117440 run.py:483] Algo bellman_ford step 6639 current loss 0.844116, current_train_items 212480.
I0302 19:00:43.323804 22895071117440 run.py:483] Algo bellman_ford step 6640 current loss 0.296236, current_train_items 212512.
I0302 19:00:43.339300 22895071117440 run.py:483] Algo bellman_ford step 6641 current loss 0.464026, current_train_items 212544.
I0302 19:00:43.362125 22895071117440 run.py:483] Algo bellman_ford step 6642 current loss 0.778041, current_train_items 212576.
I0302 19:00:43.390184 22895071117440 run.py:483] Algo bellman_ford step 6643 current loss 0.799025, current_train_items 212608.
I0302 19:00:43.421659 22895071117440 run.py:483] Algo bellman_ford step 6644 current loss 0.846872, current_train_items 212640.
I0302 19:00:43.439700 22895071117440 run.py:483] Algo bellman_ford step 6645 current loss 0.324858, current_train_items 212672.
I0302 19:00:43.454941 22895071117440 run.py:483] Algo bellman_ford step 6646 current loss 0.436047, current_train_items 212704.
I0302 19:00:43.476416 22895071117440 run.py:483] Algo bellman_ford step 6647 current loss 0.791425, current_train_items 212736.
I0302 19:00:43.505019 22895071117440 run.py:483] Algo bellman_ford step 6648 current loss 0.807670, current_train_items 212768.
I0302 19:00:43.536769 22895071117440 run.py:483] Algo bellman_ford step 6649 current loss 0.918750, current_train_items 212800.
I0302 19:00:43.555111 22895071117440 run.py:483] Algo bellman_ford step 6650 current loss 0.395995, current_train_items 212832.
I0302 19:00:43.563215 22895071117440 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:00:43.563322 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:43.579787 22895071117440 run.py:483] Algo bellman_ford step 6651 current loss 0.548668, current_train_items 212864.
I0302 19:00:43.602613 22895071117440 run.py:483] Algo bellman_ford step 6652 current loss 0.633968, current_train_items 212896.
I0302 19:00:43.631902 22895071117440 run.py:483] Algo bellman_ford step 6653 current loss 0.710831, current_train_items 212928.
I0302 19:00:43.659067 22895071117440 run.py:483] Algo bellman_ford step 6654 current loss 0.544166, current_train_items 212960.
I0302 19:00:43.677403 22895071117440 run.py:483] Algo bellman_ford step 6655 current loss 0.325514, current_train_items 212992.
I0302 19:00:43.692650 22895071117440 run.py:483] Algo bellman_ford step 6656 current loss 0.570773, current_train_items 213024.
I0302 19:00:43.714206 22895071117440 run.py:483] Algo bellman_ford step 6657 current loss 0.546177, current_train_items 213056.
I0302 19:00:43.742975 22895071117440 run.py:483] Algo bellman_ford step 6658 current loss 0.711773, current_train_items 213088.
I0302 19:00:43.775336 22895071117440 run.py:483] Algo bellman_ford step 6659 current loss 0.751218, current_train_items 213120.
I0302 19:00:43.793933 22895071117440 run.py:483] Algo bellman_ford step 6660 current loss 0.311983, current_train_items 213152.
I0302 19:00:43.809810 22895071117440 run.py:483] Algo bellman_ford step 6661 current loss 0.502153, current_train_items 213184.
I0302 19:00:43.832812 22895071117440 run.py:483] Algo bellman_ford step 6662 current loss 0.682037, current_train_items 213216.
I0302 19:00:43.859957 22895071117440 run.py:483] Algo bellman_ford step 6663 current loss 0.616894, current_train_items 213248.
I0302 19:00:43.890436 22895071117440 run.py:483] Algo bellman_ford step 6664 current loss 0.707891, current_train_items 213280.
I0302 19:00:43.908857 22895071117440 run.py:483] Algo bellman_ford step 6665 current loss 0.247515, current_train_items 213312.
I0302 19:00:43.924159 22895071117440 run.py:483] Algo bellman_ford step 6666 current loss 0.500457, current_train_items 213344.
I0302 19:00:43.947874 22895071117440 run.py:483] Algo bellman_ford step 6667 current loss 0.641336, current_train_items 213376.
I0302 19:00:43.977522 22895071117440 run.py:483] Algo bellman_ford step 6668 current loss 0.657079, current_train_items 213408.
I0302 19:00:44.009889 22895071117440 run.py:483] Algo bellman_ford step 6669 current loss 0.783792, current_train_items 213440.
I0302 19:00:44.028622 22895071117440 run.py:483] Algo bellman_ford step 6670 current loss 0.292049, current_train_items 213472.
I0302 19:00:44.044263 22895071117440 run.py:483] Algo bellman_ford step 6671 current loss 0.420104, current_train_items 213504.
I0302 19:00:44.066093 22895071117440 run.py:483] Algo bellman_ford step 6672 current loss 0.562179, current_train_items 213536.
I0302 19:00:44.094848 22895071117440 run.py:483] Algo bellman_ford step 6673 current loss 0.719789, current_train_items 213568.
I0302 19:00:44.124463 22895071117440 run.py:483] Algo bellman_ford step 6674 current loss 0.692358, current_train_items 213600.
I0302 19:00:44.142748 22895071117440 run.py:483] Algo bellman_ford step 6675 current loss 0.401451, current_train_items 213632.
I0302 19:00:44.157806 22895071117440 run.py:483] Algo bellman_ford step 6676 current loss 0.449275, current_train_items 213664.
I0302 19:00:44.180045 22895071117440 run.py:483] Algo bellman_ford step 6677 current loss 0.636616, current_train_items 213696.
I0302 19:00:44.209480 22895071117440 run.py:483] Algo bellman_ford step 6678 current loss 0.737196, current_train_items 213728.
I0302 19:00:44.242083 22895071117440 run.py:483] Algo bellman_ford step 6679 current loss 0.873648, current_train_items 213760.
I0302 19:00:44.260501 22895071117440 run.py:483] Algo bellman_ford step 6680 current loss 0.330397, current_train_items 213792.
I0302 19:00:44.275831 22895071117440 run.py:483] Algo bellman_ford step 6681 current loss 0.503591, current_train_items 213824.
I0302 19:00:44.298502 22895071117440 run.py:483] Algo bellman_ford step 6682 current loss 0.617574, current_train_items 213856.
I0302 19:00:44.326407 22895071117440 run.py:483] Algo bellman_ford step 6683 current loss 0.768649, current_train_items 213888.
I0302 19:00:44.359920 22895071117440 run.py:483] Algo bellman_ford step 6684 current loss 1.049050, current_train_items 213920.
I0302 19:00:44.378364 22895071117440 run.py:483] Algo bellman_ford step 6685 current loss 0.305276, current_train_items 213952.
I0302 19:00:44.393972 22895071117440 run.py:483] Algo bellman_ford step 6686 current loss 0.502536, current_train_items 213984.
I0302 19:00:44.415693 22895071117440 run.py:483] Algo bellman_ford step 6687 current loss 0.705522, current_train_items 214016.
I0302 19:00:44.444775 22895071117440 run.py:483] Algo bellman_ford step 6688 current loss 0.717319, current_train_items 214048.
I0302 19:00:44.475224 22895071117440 run.py:483] Algo bellman_ford step 6689 current loss 0.781890, current_train_items 214080.
I0302 19:00:44.493510 22895071117440 run.py:483] Algo bellman_ford step 6690 current loss 0.307096, current_train_items 214112.
I0302 19:00:44.508809 22895071117440 run.py:483] Algo bellman_ford step 6691 current loss 0.436906, current_train_items 214144.
I0302 19:00:44.530853 22895071117440 run.py:483] Algo bellman_ford step 6692 current loss 0.700260, current_train_items 214176.
I0302 19:00:44.559350 22895071117440 run.py:483] Algo bellman_ford step 6693 current loss 0.673893, current_train_items 214208.
I0302 19:00:44.590572 22895071117440 run.py:483] Algo bellman_ford step 6694 current loss 0.670089, current_train_items 214240.
I0302 19:00:44.608388 22895071117440 run.py:483] Algo bellman_ford step 6695 current loss 0.237963, current_train_items 214272.
I0302 19:00:44.623878 22895071117440 run.py:483] Algo bellman_ford step 6696 current loss 0.520441, current_train_items 214304.
I0302 19:00:44.646175 22895071117440 run.py:483] Algo bellman_ford step 6697 current loss 0.746446, current_train_items 214336.
I0302 19:00:44.675435 22895071117440 run.py:483] Algo bellman_ford step 6698 current loss 0.740628, current_train_items 214368.
I0302 19:00:44.707113 22895071117440 run.py:483] Algo bellman_ford step 6699 current loss 0.841898, current_train_items 214400.
I0302 19:00:44.725190 22895071117440 run.py:483] Algo bellman_ford step 6700 current loss 0.452509, current_train_items 214432.
I0302 19:00:44.733018 22895071117440 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:00:44.733124 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:00:44.749574 22895071117440 run.py:483] Algo bellman_ford step 6701 current loss 0.633478, current_train_items 214464.
I0302 19:00:44.772026 22895071117440 run.py:483] Algo bellman_ford step 6702 current loss 0.798347, current_train_items 214496.
I0302 19:00:44.801130 22895071117440 run.py:483] Algo bellman_ford step 6703 current loss 0.753020, current_train_items 214528.
I0302 19:00:44.831559 22895071117440 run.py:483] Algo bellman_ford step 6704 current loss 0.904347, current_train_items 214560.
I0302 19:00:44.850329 22895071117440 run.py:483] Algo bellman_ford step 6705 current loss 0.238598, current_train_items 214592.
I0302 19:00:44.865776 22895071117440 run.py:483] Algo bellman_ford step 6706 current loss 0.551309, current_train_items 214624.
I0302 19:00:44.888853 22895071117440 run.py:483] Algo bellman_ford step 6707 current loss 0.737617, current_train_items 214656.
I0302 19:00:44.917356 22895071117440 run.py:483] Algo bellman_ford step 6708 current loss 0.810388, current_train_items 214688.
I0302 19:00:44.949520 22895071117440 run.py:483] Algo bellman_ford step 6709 current loss 1.022828, current_train_items 214720.
I0302 19:00:44.967657 22895071117440 run.py:483] Algo bellman_ford step 6710 current loss 0.354208, current_train_items 214752.
I0302 19:00:44.983208 22895071117440 run.py:483] Algo bellman_ford step 6711 current loss 0.486036, current_train_items 214784.
I0302 19:00:45.004346 22895071117440 run.py:483] Algo bellman_ford step 6712 current loss 0.719985, current_train_items 214816.
I0302 19:00:45.034597 22895071117440 run.py:483] Algo bellman_ford step 6713 current loss 0.835323, current_train_items 214848.
I0302 19:00:45.065571 22895071117440 run.py:483] Algo bellman_ford step 6714 current loss 0.723469, current_train_items 214880.
I0302 19:00:45.083764 22895071117440 run.py:483] Algo bellman_ford step 6715 current loss 0.300200, current_train_items 214912.
I0302 19:00:45.099058 22895071117440 run.py:483] Algo bellman_ford step 6716 current loss 0.463538, current_train_items 214944.
I0302 19:00:45.120963 22895071117440 run.py:483] Algo bellman_ford step 6717 current loss 0.583177, current_train_items 214976.
I0302 19:00:45.149854 22895071117440 run.py:483] Algo bellman_ford step 6718 current loss 0.597768, current_train_items 215008.
I0302 19:00:45.182443 22895071117440 run.py:483] Algo bellman_ford step 6719 current loss 0.716513, current_train_items 215040.
I0302 19:00:45.200207 22895071117440 run.py:483] Algo bellman_ford step 6720 current loss 0.244422, current_train_items 215072.
I0302 19:00:45.215511 22895071117440 run.py:483] Algo bellman_ford step 6721 current loss 0.455885, current_train_items 215104.
I0302 19:00:45.236509 22895071117440 run.py:483] Algo bellman_ford step 6722 current loss 0.587016, current_train_items 215136.
I0302 19:00:45.265971 22895071117440 run.py:483] Algo bellman_ford step 6723 current loss 0.714749, current_train_items 215168.
I0302 19:00:45.295797 22895071117440 run.py:483] Algo bellman_ford step 6724 current loss 0.744920, current_train_items 215200.
I0302 19:00:45.313424 22895071117440 run.py:483] Algo bellman_ford step 6725 current loss 0.235002, current_train_items 215232.
I0302 19:00:45.328759 22895071117440 run.py:483] Algo bellman_ford step 6726 current loss 0.455735, current_train_items 215264.
I0302 19:00:45.350235 22895071117440 run.py:483] Algo bellman_ford step 6727 current loss 0.530155, current_train_items 215296.
I0302 19:00:45.378516 22895071117440 run.py:483] Algo bellman_ford step 6728 current loss 0.630895, current_train_items 215328.
I0302 19:00:45.408101 22895071117440 run.py:483] Algo bellman_ford step 6729 current loss 0.677978, current_train_items 215360.
I0302 19:00:45.425963 22895071117440 run.py:483] Algo bellman_ford step 6730 current loss 0.305745, current_train_items 215392.
I0302 19:00:45.441152 22895071117440 run.py:483] Algo bellman_ford step 6731 current loss 0.502389, current_train_items 215424.
I0302 19:00:45.463248 22895071117440 run.py:483] Algo bellman_ford step 6732 current loss 0.674706, current_train_items 215456.
I0302 19:00:45.490274 22895071117440 run.py:483] Algo bellman_ford step 6733 current loss 0.625612, current_train_items 215488.
I0302 19:00:45.519304 22895071117440 run.py:483] Algo bellman_ford step 6734 current loss 0.673631, current_train_items 215520.
I0302 19:00:45.537180 22895071117440 run.py:483] Algo bellman_ford step 6735 current loss 0.298208, current_train_items 215552.
I0302 19:00:45.552657 22895071117440 run.py:483] Algo bellman_ford step 6736 current loss 0.541126, current_train_items 215584.
I0302 19:00:45.575274 22895071117440 run.py:483] Algo bellman_ford step 6737 current loss 0.628067, current_train_items 215616.
I0302 19:00:45.602924 22895071117440 run.py:483] Algo bellman_ford step 6738 current loss 0.656639, current_train_items 215648.
I0302 19:00:45.635368 22895071117440 run.py:483] Algo bellman_ford step 6739 current loss 0.838974, current_train_items 215680.
I0302 19:00:45.653161 22895071117440 run.py:483] Algo bellman_ford step 6740 current loss 0.317453, current_train_items 215712.
I0302 19:00:45.668865 22895071117440 run.py:483] Algo bellman_ford step 6741 current loss 0.460795, current_train_items 215744.
I0302 19:00:45.691650 22895071117440 run.py:483] Algo bellman_ford step 6742 current loss 0.572384, current_train_items 215776.
I0302 19:00:45.719297 22895071117440 run.py:483] Algo bellman_ford step 6743 current loss 0.641100, current_train_items 215808.
I0302 19:00:45.751157 22895071117440 run.py:483] Algo bellman_ford step 6744 current loss 0.870180, current_train_items 215840.
I0302 19:00:45.769457 22895071117440 run.py:483] Algo bellman_ford step 6745 current loss 0.272457, current_train_items 215872.
I0302 19:00:45.784859 22895071117440 run.py:483] Algo bellman_ford step 6746 current loss 0.468900, current_train_items 215904.
I0302 19:00:45.806732 22895071117440 run.py:483] Algo bellman_ford step 6747 current loss 0.523049, current_train_items 215936.
I0302 19:00:45.834957 22895071117440 run.py:483] Algo bellman_ford step 6748 current loss 0.558746, current_train_items 215968.
I0302 19:00:45.866304 22895071117440 run.py:483] Algo bellman_ford step 6749 current loss 0.747012, current_train_items 216000.
I0302 19:00:45.884098 22895071117440 run.py:483] Algo bellman_ford step 6750 current loss 0.349536, current_train_items 216032.
I0302 19:00:45.892080 22895071117440 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:00:45.892186 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:45.908139 22895071117440 run.py:483] Algo bellman_ford step 6751 current loss 0.437049, current_train_items 216064.
I0302 19:00:45.930784 22895071117440 run.py:483] Algo bellman_ford step 6752 current loss 0.627327, current_train_items 216096.
I0302 19:00:45.960873 22895071117440 run.py:483] Algo bellman_ford step 6753 current loss 0.717651, current_train_items 216128.
I0302 19:00:45.993106 22895071117440 run.py:483] Algo bellman_ford step 6754 current loss 0.728467, current_train_items 216160.
I0302 19:00:46.011765 22895071117440 run.py:483] Algo bellman_ford step 6755 current loss 0.302449, current_train_items 216192.
I0302 19:00:46.027631 22895071117440 run.py:483] Algo bellman_ford step 6756 current loss 0.538977, current_train_items 216224.
I0302 19:00:46.049660 22895071117440 run.py:483] Algo bellman_ford step 6757 current loss 0.520882, current_train_items 216256.
I0302 19:00:46.078550 22895071117440 run.py:483] Algo bellman_ford step 6758 current loss 0.733688, current_train_items 216288.
I0302 19:00:46.110913 22895071117440 run.py:483] Algo bellman_ford step 6759 current loss 0.906778, current_train_items 216320.
I0302 19:00:46.129500 22895071117440 run.py:483] Algo bellman_ford step 6760 current loss 0.292858, current_train_items 216352.
I0302 19:00:46.145464 22895071117440 run.py:483] Algo bellman_ford step 6761 current loss 0.385743, current_train_items 216384.
I0302 19:00:46.166960 22895071117440 run.py:483] Algo bellman_ford step 6762 current loss 0.529872, current_train_items 216416.
I0302 19:00:46.195828 22895071117440 run.py:483] Algo bellman_ford step 6763 current loss 0.639080, current_train_items 216448.
I0302 19:00:46.226222 22895071117440 run.py:483] Algo bellman_ford step 6764 current loss 0.762478, current_train_items 216480.
I0302 19:00:46.244432 22895071117440 run.py:483] Algo bellman_ford step 6765 current loss 0.252030, current_train_items 216512.
I0302 19:00:46.260002 22895071117440 run.py:483] Algo bellman_ford step 6766 current loss 0.500227, current_train_items 216544.
I0302 19:00:46.282127 22895071117440 run.py:483] Algo bellman_ford step 6767 current loss 0.559100, current_train_items 216576.
I0302 19:00:46.309682 22895071117440 run.py:483] Algo bellman_ford step 6768 current loss 0.627579, current_train_items 216608.
I0302 19:00:46.340043 22895071117440 run.py:483] Algo bellman_ford step 6769 current loss 0.740209, current_train_items 216640.
I0302 19:00:46.358734 22895071117440 run.py:483] Algo bellman_ford step 6770 current loss 0.264959, current_train_items 216672.
I0302 19:00:46.374234 22895071117440 run.py:483] Algo bellman_ford step 6771 current loss 0.519046, current_train_items 216704.
I0302 19:00:46.394860 22895071117440 run.py:483] Algo bellman_ford step 6772 current loss 0.525844, current_train_items 216736.
I0302 19:00:46.423732 22895071117440 run.py:483] Algo bellman_ford step 6773 current loss 0.615955, current_train_items 216768.
I0302 19:00:46.452980 22895071117440 run.py:483] Algo bellman_ford step 6774 current loss 0.739973, current_train_items 216800.
I0302 19:00:46.471101 22895071117440 run.py:483] Algo bellman_ford step 6775 current loss 0.299166, current_train_items 216832.
I0302 19:00:46.486849 22895071117440 run.py:483] Algo bellman_ford step 6776 current loss 0.472256, current_train_items 216864.
I0302 19:00:46.507842 22895071117440 run.py:483] Algo bellman_ford step 6777 current loss 0.609341, current_train_items 216896.
I0302 19:00:46.537615 22895071117440 run.py:483] Algo bellman_ford step 6778 current loss 0.780524, current_train_items 216928.
I0302 19:00:46.568001 22895071117440 run.py:483] Algo bellman_ford step 6779 current loss 0.851167, current_train_items 216960.
I0302 19:00:46.585727 22895071117440 run.py:483] Algo bellman_ford step 6780 current loss 0.328887, current_train_items 216992.
I0302 19:00:46.600952 22895071117440 run.py:483] Algo bellman_ford step 6781 current loss 0.520627, current_train_items 217024.
I0302 19:00:46.622163 22895071117440 run.py:483] Algo bellman_ford step 6782 current loss 0.704587, current_train_items 217056.
I0302 19:00:46.651628 22895071117440 run.py:483] Algo bellman_ford step 6783 current loss 0.857553, current_train_items 217088.
I0302 19:00:46.681695 22895071117440 run.py:483] Algo bellman_ford step 6784 current loss 1.010657, current_train_items 217120.
I0302 19:00:46.700285 22895071117440 run.py:483] Algo bellman_ford step 6785 current loss 0.274805, current_train_items 217152.
I0302 19:00:46.715663 22895071117440 run.py:483] Algo bellman_ford step 6786 current loss 0.447801, current_train_items 217184.
I0302 19:00:46.736542 22895071117440 run.py:483] Algo bellman_ford step 6787 current loss 0.497268, current_train_items 217216.
I0302 19:00:46.763811 22895071117440 run.py:483] Algo bellman_ford step 6788 current loss 0.641564, current_train_items 217248.
I0302 19:00:46.795443 22895071117440 run.py:483] Algo bellman_ford step 6789 current loss 0.859005, current_train_items 217280.
I0302 19:00:46.814015 22895071117440 run.py:483] Algo bellman_ford step 6790 current loss 0.395109, current_train_items 217312.
I0302 19:00:46.829568 22895071117440 run.py:483] Algo bellman_ford step 6791 current loss 0.489898, current_train_items 217344.
I0302 19:00:46.851423 22895071117440 run.py:483] Algo bellman_ford step 6792 current loss 0.565121, current_train_items 217376.
I0302 19:00:46.878754 22895071117440 run.py:483] Algo bellman_ford step 6793 current loss 0.724845, current_train_items 217408.
I0302 19:00:46.909414 22895071117440 run.py:483] Algo bellman_ford step 6794 current loss 0.862459, current_train_items 217440.
I0302 19:00:46.927539 22895071117440 run.py:483] Algo bellman_ford step 6795 current loss 0.278817, current_train_items 217472.
I0302 19:00:46.942728 22895071117440 run.py:483] Algo bellman_ford step 6796 current loss 0.555957, current_train_items 217504.
I0302 19:00:46.965109 22895071117440 run.py:483] Algo bellman_ford step 6797 current loss 0.760115, current_train_items 217536.
I0302 19:00:46.992864 22895071117440 run.py:483] Algo bellman_ford step 6798 current loss 0.688476, current_train_items 217568.
I0302 19:00:47.023423 22895071117440 run.py:483] Algo bellman_ford step 6799 current loss 0.763868, current_train_items 217600.
I0302 19:00:47.041840 22895071117440 run.py:483] Algo bellman_ford step 6800 current loss 0.364683, current_train_items 217632.
I0302 19:00:47.049643 22895071117440 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:00:47.049749 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:00:47.065531 22895071117440 run.py:483] Algo bellman_ford step 6801 current loss 0.456912, current_train_items 217664.
I0302 19:00:47.087706 22895071117440 run.py:483] Algo bellman_ford step 6802 current loss 0.554916, current_train_items 217696.
I0302 19:00:47.118238 22895071117440 run.py:483] Algo bellman_ford step 6803 current loss 0.653149, current_train_items 217728.
I0302 19:00:47.149814 22895071117440 run.py:483] Algo bellman_ford step 6804 current loss 0.748776, current_train_items 217760.
I0302 19:00:47.168284 22895071117440 run.py:483] Algo bellman_ford step 6805 current loss 0.249155, current_train_items 217792.
I0302 19:00:47.183892 22895071117440 run.py:483] Algo bellman_ford step 6806 current loss 0.366013, current_train_items 217824.
I0302 19:00:47.206398 22895071117440 run.py:483] Algo bellman_ford step 6807 current loss 0.654093, current_train_items 217856.
I0302 19:00:47.233966 22895071117440 run.py:483] Algo bellman_ford step 6808 current loss 0.580456, current_train_items 217888.
I0302 19:00:47.264808 22895071117440 run.py:483] Algo bellman_ford step 6809 current loss 0.665625, current_train_items 217920.
I0302 19:00:47.283202 22895071117440 run.py:483] Algo bellman_ford step 6810 current loss 0.421040, current_train_items 217952.
I0302 19:00:47.298076 22895071117440 run.py:483] Algo bellman_ford step 6811 current loss 0.375560, current_train_items 217984.
I0302 19:00:47.321030 22895071117440 run.py:483] Algo bellman_ford step 6812 current loss 0.660901, current_train_items 218016.
I0302 19:00:47.349110 22895071117440 run.py:483] Algo bellman_ford step 6813 current loss 0.677372, current_train_items 218048.
I0302 19:00:47.378207 22895071117440 run.py:483] Algo bellman_ford step 6814 current loss 0.706267, current_train_items 218080.
I0302 19:00:47.396355 22895071117440 run.py:483] Algo bellman_ford step 6815 current loss 0.309280, current_train_items 218112.
I0302 19:00:47.411593 22895071117440 run.py:483] Algo bellman_ford step 6816 current loss 0.419459, current_train_items 218144.
I0302 19:00:47.432163 22895071117440 run.py:483] Algo bellman_ford step 6817 current loss 0.534234, current_train_items 218176.
I0302 19:00:47.461036 22895071117440 run.py:483] Algo bellman_ford step 6818 current loss 0.623239, current_train_items 218208.
I0302 19:00:47.493934 22895071117440 run.py:483] Algo bellman_ford step 6819 current loss 0.834977, current_train_items 218240.
I0302 19:00:47.511822 22895071117440 run.py:483] Algo bellman_ford step 6820 current loss 0.245379, current_train_items 218272.
I0302 19:00:47.527073 22895071117440 run.py:483] Algo bellman_ford step 6821 current loss 0.435808, current_train_items 218304.
I0302 19:00:47.548491 22895071117440 run.py:483] Algo bellman_ford step 6822 current loss 0.674399, current_train_items 218336.
I0302 19:00:47.577232 22895071117440 run.py:483] Algo bellman_ford step 6823 current loss 0.807804, current_train_items 218368.
I0302 19:00:47.609544 22895071117440 run.py:483] Algo bellman_ford step 6824 current loss 0.846246, current_train_items 218400.
I0302 19:00:47.627771 22895071117440 run.py:483] Algo bellman_ford step 6825 current loss 0.275521, current_train_items 218432.
I0302 19:00:47.643262 22895071117440 run.py:483] Algo bellman_ford step 6826 current loss 0.469617, current_train_items 218464.
I0302 19:00:47.664871 22895071117440 run.py:483] Algo bellman_ford step 6827 current loss 0.648464, current_train_items 218496.
I0302 19:00:47.692792 22895071117440 run.py:483] Algo bellman_ford step 6828 current loss 0.864106, current_train_items 218528.
I0302 19:00:47.725843 22895071117440 run.py:483] Algo bellman_ford step 6829 current loss 1.159378, current_train_items 218560.
I0302 19:00:47.744089 22895071117440 run.py:483] Algo bellman_ford step 6830 current loss 0.354398, current_train_items 218592.
I0302 19:00:47.759215 22895071117440 run.py:483] Algo bellman_ford step 6831 current loss 0.421921, current_train_items 218624.
I0302 19:00:47.782470 22895071117440 run.py:483] Algo bellman_ford step 6832 current loss 0.595520, current_train_items 218656.
I0302 19:00:47.810983 22895071117440 run.py:483] Algo bellman_ford step 6833 current loss 0.600917, current_train_items 218688.
I0302 19:00:47.841824 22895071117440 run.py:483] Algo bellman_ford step 6834 current loss 0.748175, current_train_items 218720.
I0302 19:00:47.860169 22895071117440 run.py:483] Algo bellman_ford step 6835 current loss 0.324856, current_train_items 218752.
I0302 19:00:47.875664 22895071117440 run.py:483] Algo bellman_ford step 6836 current loss 0.580096, current_train_items 218784.
I0302 19:00:47.897346 22895071117440 run.py:483] Algo bellman_ford step 6837 current loss 0.656685, current_train_items 218816.
I0302 19:00:47.926628 22895071117440 run.py:483] Algo bellman_ford step 6838 current loss 0.711917, current_train_items 218848.
I0302 19:00:47.955706 22895071117440 run.py:483] Algo bellman_ford step 6839 current loss 0.648779, current_train_items 218880.
I0302 19:00:47.973398 22895071117440 run.py:483] Algo bellman_ford step 6840 current loss 0.292733, current_train_items 218912.
I0302 19:00:47.989124 22895071117440 run.py:483] Algo bellman_ford step 6841 current loss 0.524911, current_train_items 218944.
I0302 19:00:48.011642 22895071117440 run.py:483] Algo bellman_ford step 6842 current loss 0.709746, current_train_items 218976.
I0302 19:00:48.038924 22895071117440 run.py:483] Algo bellman_ford step 6843 current loss 0.738194, current_train_items 219008.
I0302 19:00:48.069151 22895071117440 run.py:483] Algo bellman_ford step 6844 current loss 0.951089, current_train_items 219040.
I0302 19:00:48.086942 22895071117440 run.py:483] Algo bellman_ford step 6845 current loss 0.339938, current_train_items 219072.
I0302 19:00:48.102814 22895071117440 run.py:483] Algo bellman_ford step 6846 current loss 0.422122, current_train_items 219104.
I0302 19:00:48.125456 22895071117440 run.py:483] Algo bellman_ford step 6847 current loss 0.616748, current_train_items 219136.
I0302 19:00:48.154580 22895071117440 run.py:483] Algo bellman_ford step 6848 current loss 0.668113, current_train_items 219168.
I0302 19:00:48.186277 22895071117440 run.py:483] Algo bellman_ford step 6849 current loss 0.735287, current_train_items 219200.
I0302 19:00:48.204267 22895071117440 run.py:483] Algo bellman_ford step 6850 current loss 0.335193, current_train_items 219232.
I0302 19:00:48.212076 22895071117440 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:00:48.212181 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:00:48.228387 22895071117440 run.py:483] Algo bellman_ford step 6851 current loss 0.483995, current_train_items 219264.
I0302 19:00:48.251081 22895071117440 run.py:483] Algo bellman_ford step 6852 current loss 0.683768, current_train_items 219296.
I0302 19:00:48.280270 22895071117440 run.py:483] Algo bellman_ford step 6853 current loss 0.643524, current_train_items 219328.
I0302 19:00:48.312301 22895071117440 run.py:483] Algo bellman_ford step 6854 current loss 0.769023, current_train_items 219360.
I0302 19:00:48.330911 22895071117440 run.py:483] Algo bellman_ford step 6855 current loss 0.273618, current_train_items 219392.
I0302 19:00:48.346032 22895071117440 run.py:483] Algo bellman_ford step 6856 current loss 0.427433, current_train_items 219424.
I0302 19:00:48.368233 22895071117440 run.py:483] Algo bellman_ford step 6857 current loss 0.681061, current_train_items 219456.
I0302 19:00:48.395951 22895071117440 run.py:483] Algo bellman_ford step 6858 current loss 0.648972, current_train_items 219488.
I0302 19:00:48.426925 22895071117440 run.py:483] Algo bellman_ford step 6859 current loss 0.733118, current_train_items 219520.
I0302 19:00:48.445319 22895071117440 run.py:483] Algo bellman_ford step 6860 current loss 0.365571, current_train_items 219552.
I0302 19:00:48.460921 22895071117440 run.py:483] Algo bellman_ford step 6861 current loss 0.477266, current_train_items 219584.
I0302 19:00:48.481880 22895071117440 run.py:483] Algo bellman_ford step 6862 current loss 0.590955, current_train_items 219616.
I0302 19:00:48.509832 22895071117440 run.py:483] Algo bellman_ford step 6863 current loss 0.577885, current_train_items 219648.
I0302 19:00:48.539184 22895071117440 run.py:483] Algo bellman_ford step 6864 current loss 0.788758, current_train_items 219680.
I0302 19:00:48.557574 22895071117440 run.py:483] Algo bellman_ford step 6865 current loss 0.298612, current_train_items 219712.
I0302 19:00:48.572896 22895071117440 run.py:483] Algo bellman_ford step 6866 current loss 0.537941, current_train_items 219744.
I0302 19:00:48.595625 22895071117440 run.py:483] Algo bellman_ford step 6867 current loss 0.659330, current_train_items 219776.
I0302 19:00:48.624294 22895071117440 run.py:483] Algo bellman_ford step 6868 current loss 0.757013, current_train_items 219808.
I0302 19:00:48.657698 22895071117440 run.py:483] Algo bellman_ford step 6869 current loss 0.769482, current_train_items 219840.
I0302 19:00:48.676472 22895071117440 run.py:483] Algo bellman_ford step 6870 current loss 0.324545, current_train_items 219872.
I0302 19:00:48.692411 22895071117440 run.py:483] Algo bellman_ford step 6871 current loss 0.445792, current_train_items 219904.
I0302 19:00:48.713881 22895071117440 run.py:483] Algo bellman_ford step 6872 current loss 0.603602, current_train_items 219936.
I0302 19:00:48.741013 22895071117440 run.py:483] Algo bellman_ford step 6873 current loss 0.572861, current_train_items 219968.
I0302 19:00:48.771770 22895071117440 run.py:483] Algo bellman_ford step 6874 current loss 0.799167, current_train_items 220000.
I0302 19:00:48.790328 22895071117440 run.py:483] Algo bellman_ford step 6875 current loss 0.299461, current_train_items 220032.
I0302 19:00:48.805629 22895071117440 run.py:483] Algo bellman_ford step 6876 current loss 0.450442, current_train_items 220064.
I0302 19:00:48.827186 22895071117440 run.py:483] Algo bellman_ford step 6877 current loss 0.649250, current_train_items 220096.
I0302 19:00:48.855266 22895071117440 run.py:483] Algo bellman_ford step 6878 current loss 0.673486, current_train_items 220128.
I0302 19:00:48.886547 22895071117440 run.py:483] Algo bellman_ford step 6879 current loss 0.811003, current_train_items 220160.
I0302 19:00:48.904595 22895071117440 run.py:483] Algo bellman_ford step 6880 current loss 0.381064, current_train_items 220192.
I0302 19:00:48.919570 22895071117440 run.py:483] Algo bellman_ford step 6881 current loss 0.547608, current_train_items 220224.
I0302 19:00:48.942111 22895071117440 run.py:483] Algo bellman_ford step 6882 current loss 0.634071, current_train_items 220256.
I0302 19:00:48.969325 22895071117440 run.py:483] Algo bellman_ford step 6883 current loss 0.605080, current_train_items 220288.
I0302 19:00:48.999654 22895071117440 run.py:483] Algo bellman_ford step 6884 current loss 0.689560, current_train_items 220320.
I0302 19:00:49.018132 22895071117440 run.py:483] Algo bellman_ford step 6885 current loss 0.302639, current_train_items 220352.
I0302 19:00:49.033954 22895071117440 run.py:483] Algo bellman_ford step 6886 current loss 0.458272, current_train_items 220384.
I0302 19:00:49.056033 22895071117440 run.py:483] Algo bellman_ford step 6887 current loss 0.752849, current_train_items 220416.
I0302 19:00:49.085647 22895071117440 run.py:483] Algo bellman_ford step 6888 current loss 0.844545, current_train_items 220448.
I0302 19:00:49.116790 22895071117440 run.py:483] Algo bellman_ford step 6889 current loss 0.819358, current_train_items 220480.
I0302 19:00:49.135158 22895071117440 run.py:483] Algo bellman_ford step 6890 current loss 0.315576, current_train_items 220512.
I0302 19:00:49.151105 22895071117440 run.py:483] Algo bellman_ford step 6891 current loss 0.497175, current_train_items 220544.
I0302 19:00:49.173008 22895071117440 run.py:483] Algo bellman_ford step 6892 current loss 0.645851, current_train_items 220576.
I0302 19:00:49.202011 22895071117440 run.py:483] Algo bellman_ford step 6893 current loss 0.782251, current_train_items 220608.
I0302 19:00:49.231731 22895071117440 run.py:483] Algo bellman_ford step 6894 current loss 1.009626, current_train_items 220640.
I0302 19:00:49.250074 22895071117440 run.py:483] Algo bellman_ford step 6895 current loss 0.346914, current_train_items 220672.
I0302 19:00:49.265341 22895071117440 run.py:483] Algo bellman_ford step 6896 current loss 0.474415, current_train_items 220704.
I0302 19:00:49.287654 22895071117440 run.py:483] Algo bellman_ford step 6897 current loss 0.763140, current_train_items 220736.
I0302 19:00:49.314785 22895071117440 run.py:483] Algo bellman_ford step 6898 current loss 0.525847, current_train_items 220768.
I0302 19:00:49.346580 22895071117440 run.py:483] Algo bellman_ford step 6899 current loss 0.741749, current_train_items 220800.
I0302 19:00:49.365155 22895071117440 run.py:483] Algo bellman_ford step 6900 current loss 0.325895, current_train_items 220832.
I0302 19:00:49.373044 22895071117440 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:00:49.373149 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:00:49.388923 22895071117440 run.py:483] Algo bellman_ford step 6901 current loss 0.409911, current_train_items 220864.
I0302 19:00:49.411129 22895071117440 run.py:483] Algo bellman_ford step 6902 current loss 0.682596, current_train_items 220896.
I0302 19:00:49.441317 22895071117440 run.py:483] Algo bellman_ford step 6903 current loss 0.702435, current_train_items 220928.
I0302 19:00:49.474075 22895071117440 run.py:483] Algo bellman_ford step 6904 current loss 0.787736, current_train_items 220960.
I0302 19:00:49.492358 22895071117440 run.py:483] Algo bellman_ford step 6905 current loss 0.365432, current_train_items 220992.
I0302 19:00:49.507370 22895071117440 run.py:483] Algo bellman_ford step 6906 current loss 0.437855, current_train_items 221024.
I0302 19:00:49.529225 22895071117440 run.py:483] Algo bellman_ford step 6907 current loss 0.622868, current_train_items 221056.
I0302 19:00:49.558959 22895071117440 run.py:483] Algo bellman_ford step 6908 current loss 0.815381, current_train_items 221088.
I0302 19:00:49.591005 22895071117440 run.py:483] Algo bellman_ford step 6909 current loss 0.859386, current_train_items 221120.
I0302 19:00:49.609026 22895071117440 run.py:483] Algo bellman_ford step 6910 current loss 0.278878, current_train_items 221152.
I0302 19:00:49.625037 22895071117440 run.py:483] Algo bellman_ford step 6911 current loss 0.514916, current_train_items 221184.
I0302 19:00:49.645782 22895071117440 run.py:483] Algo bellman_ford step 6912 current loss 0.569918, current_train_items 221216.
I0302 19:00:49.675036 22895071117440 run.py:483] Algo bellman_ford step 6913 current loss 0.625761, current_train_items 221248.
I0302 19:00:49.707457 22895071117440 run.py:483] Algo bellman_ford step 6914 current loss 0.786666, current_train_items 221280.
I0302 19:00:49.725538 22895071117440 run.py:483] Algo bellman_ford step 6915 current loss 0.360215, current_train_items 221312.
I0302 19:00:49.740489 22895071117440 run.py:483] Algo bellman_ford step 6916 current loss 0.442261, current_train_items 221344.
I0302 19:00:49.763319 22895071117440 run.py:483] Algo bellman_ford step 6917 current loss 0.652169, current_train_items 221376.
I0302 19:00:49.792631 22895071117440 run.py:483] Algo bellman_ford step 6918 current loss 0.693366, current_train_items 221408.
I0302 19:00:49.825081 22895071117440 run.py:483] Algo bellman_ford step 6919 current loss 0.690877, current_train_items 221440.
I0302 19:00:49.842962 22895071117440 run.py:483] Algo bellman_ford step 6920 current loss 0.257483, current_train_items 221472.
I0302 19:00:49.858251 22895071117440 run.py:483] Algo bellman_ford step 6921 current loss 0.487319, current_train_items 221504.
I0302 19:00:49.879628 22895071117440 run.py:483] Algo bellman_ford step 6922 current loss 0.606866, current_train_items 221536.
I0302 19:00:49.908755 22895071117440 run.py:483] Algo bellman_ford step 6923 current loss 0.774698, current_train_items 221568.
I0302 19:00:49.939840 22895071117440 run.py:483] Algo bellman_ford step 6924 current loss 0.697761, current_train_items 221600.
I0302 19:00:49.958324 22895071117440 run.py:483] Algo bellman_ford step 6925 current loss 0.274716, current_train_items 221632.
I0302 19:00:49.973500 22895071117440 run.py:483] Algo bellman_ford step 6926 current loss 0.387315, current_train_items 221664.
I0302 19:00:49.995841 22895071117440 run.py:483] Algo bellman_ford step 6927 current loss 0.710215, current_train_items 221696.
I0302 19:00:50.024666 22895071117440 run.py:483] Algo bellman_ford step 6928 current loss 0.735170, current_train_items 221728.
I0302 19:00:50.055050 22895071117440 run.py:483] Algo bellman_ford step 6929 current loss 0.919193, current_train_items 221760.
I0302 19:00:50.073436 22895071117440 run.py:483] Algo bellman_ford step 6930 current loss 0.300019, current_train_items 221792.
I0302 19:00:50.088981 22895071117440 run.py:483] Algo bellman_ford step 6931 current loss 0.475541, current_train_items 221824.
I0302 19:00:50.110211 22895071117440 run.py:483] Algo bellman_ford step 6932 current loss 0.610676, current_train_items 221856.
I0302 19:00:50.139103 22895071117440 run.py:483] Algo bellman_ford step 6933 current loss 0.707166, current_train_items 221888.
I0302 19:00:50.171434 22895071117440 run.py:483] Algo bellman_ford step 6934 current loss 0.844656, current_train_items 221920.
I0302 19:00:50.189367 22895071117440 run.py:483] Algo bellman_ford step 6935 current loss 0.334062, current_train_items 221952.
I0302 19:00:50.204513 22895071117440 run.py:483] Algo bellman_ford step 6936 current loss 0.468353, current_train_items 221984.
I0302 19:00:50.226493 22895071117440 run.py:483] Algo bellman_ford step 6937 current loss 0.544300, current_train_items 222016.
I0302 19:00:50.255258 22895071117440 run.py:483] Algo bellman_ford step 6938 current loss 0.629336, current_train_items 222048.
I0302 19:00:50.285019 22895071117440 run.py:483] Algo bellman_ford step 6939 current loss 0.684707, current_train_items 222080.
I0302 19:00:50.303195 22895071117440 run.py:483] Algo bellman_ford step 6940 current loss 0.286641, current_train_items 222112.
I0302 19:00:50.318913 22895071117440 run.py:483] Algo bellman_ford step 6941 current loss 0.550406, current_train_items 222144.
I0302 19:00:50.341317 22895071117440 run.py:483] Algo bellman_ford step 6942 current loss 0.663773, current_train_items 222176.
I0302 19:00:50.370002 22895071117440 run.py:483] Algo bellman_ford step 6943 current loss 0.641676, current_train_items 222208.
I0302 19:00:50.401916 22895071117440 run.py:483] Algo bellman_ford step 6944 current loss 0.832219, current_train_items 222240.
I0302 19:00:50.419823 22895071117440 run.py:483] Algo bellman_ford step 6945 current loss 0.307080, current_train_items 222272.
I0302 19:00:50.435321 22895071117440 run.py:483] Algo bellman_ford step 6946 current loss 0.503106, current_train_items 222304.
I0302 19:00:50.457425 22895071117440 run.py:483] Algo bellman_ford step 6947 current loss 0.667395, current_train_items 222336.
I0302 19:00:50.483812 22895071117440 run.py:483] Algo bellman_ford step 6948 current loss 0.588951, current_train_items 222368.
I0302 19:00:50.515878 22895071117440 run.py:483] Algo bellman_ford step 6949 current loss 0.867906, current_train_items 222400.
I0302 19:00:50.533691 22895071117440 run.py:483] Algo bellman_ford step 6950 current loss 0.300593, current_train_items 222432.
I0302 19:00:50.541623 22895071117440 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:00:50.541730 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:00:50.557854 22895071117440 run.py:483] Algo bellman_ford step 6951 current loss 0.492660, current_train_items 222464.
I0302 19:00:50.580745 22895071117440 run.py:483] Algo bellman_ford step 6952 current loss 0.693122, current_train_items 222496.
I0302 19:00:50.609092 22895071117440 run.py:483] Algo bellman_ford step 6953 current loss 0.665777, current_train_items 222528.
I0302 19:00:50.641111 22895071117440 run.py:483] Algo bellman_ford step 6954 current loss 0.784376, current_train_items 222560.
I0302 19:00:50.659852 22895071117440 run.py:483] Algo bellman_ford step 6955 current loss 0.305924, current_train_items 222592.
I0302 19:00:50.675272 22895071117440 run.py:483] Algo bellman_ford step 6956 current loss 0.437193, current_train_items 222624.
I0302 19:00:50.698310 22895071117440 run.py:483] Algo bellman_ford step 6957 current loss 0.717492, current_train_items 222656.
I0302 19:00:50.726673 22895071117440 run.py:483] Algo bellman_ford step 6958 current loss 0.737747, current_train_items 222688.
I0302 19:00:50.756062 22895071117440 run.py:483] Algo bellman_ford step 6959 current loss 0.708927, current_train_items 222720.
I0302 19:00:50.774394 22895071117440 run.py:483] Algo bellman_ford step 6960 current loss 0.291480, current_train_items 222752.
I0302 19:00:50.790496 22895071117440 run.py:483] Algo bellman_ford step 6961 current loss 0.470291, current_train_items 222784.
I0302 19:00:50.811715 22895071117440 run.py:483] Algo bellman_ford step 6962 current loss 0.578862, current_train_items 222816.
I0302 19:00:50.840420 22895071117440 run.py:483] Algo bellman_ford step 6963 current loss 0.780563, current_train_items 222848.
I0302 19:00:50.872631 22895071117440 run.py:483] Algo bellman_ford step 6964 current loss 0.774027, current_train_items 222880.
I0302 19:00:50.891120 22895071117440 run.py:483] Algo bellman_ford step 6965 current loss 0.350251, current_train_items 222912.
I0302 19:00:50.906547 22895071117440 run.py:483] Algo bellman_ford step 6966 current loss 0.504357, current_train_items 222944.
I0302 19:00:50.928184 22895071117440 run.py:483] Algo bellman_ford step 6967 current loss 0.549033, current_train_items 222976.
I0302 19:00:50.955764 22895071117440 run.py:483] Algo bellman_ford step 6968 current loss 0.655801, current_train_items 223008.
I0302 19:00:50.986620 22895071117440 run.py:483] Algo bellman_ford step 6969 current loss 0.782830, current_train_items 223040.
I0302 19:00:51.004800 22895071117440 run.py:483] Algo bellman_ford step 6970 current loss 0.236582, current_train_items 223072.
I0302 19:00:51.020532 22895071117440 run.py:483] Algo bellman_ford step 6971 current loss 0.427205, current_train_items 223104.
I0302 19:00:51.042137 22895071117440 run.py:483] Algo bellman_ford step 6972 current loss 0.622065, current_train_items 223136.
I0302 19:00:51.070402 22895071117440 run.py:483] Algo bellman_ford step 6973 current loss 0.592521, current_train_items 223168.
I0302 19:00:51.102070 22895071117440 run.py:483] Algo bellman_ford step 6974 current loss 0.761352, current_train_items 223200.
I0302 19:00:51.120453 22895071117440 run.py:483] Algo bellman_ford step 6975 current loss 0.346150, current_train_items 223232.
I0302 19:00:51.136241 22895071117440 run.py:483] Algo bellman_ford step 6976 current loss 0.467346, current_train_items 223264.
I0302 19:00:51.157643 22895071117440 run.py:483] Algo bellman_ford step 6977 current loss 0.609853, current_train_items 223296.
I0302 19:00:51.184870 22895071117440 run.py:483] Algo bellman_ford step 6978 current loss 0.695324, current_train_items 223328.
I0302 19:00:51.216397 22895071117440 run.py:483] Algo bellman_ford step 6979 current loss 0.712750, current_train_items 223360.
I0302 19:00:51.234550 22895071117440 run.py:483] Algo bellman_ford step 6980 current loss 0.338711, current_train_items 223392.
I0302 19:00:51.250387 22895071117440 run.py:483] Algo bellman_ford step 6981 current loss 0.472544, current_train_items 223424.
I0302 19:00:51.271306 22895071117440 run.py:483] Algo bellman_ford step 6982 current loss 0.518278, current_train_items 223456.
I0302 19:00:51.300662 22895071117440 run.py:483] Algo bellman_ford step 6983 current loss 0.742390, current_train_items 223488.
I0302 19:00:51.331419 22895071117440 run.py:483] Algo bellman_ford step 6984 current loss 0.766909, current_train_items 223520.
I0302 19:00:51.349940 22895071117440 run.py:483] Algo bellman_ford step 6985 current loss 0.266227, current_train_items 223552.
I0302 19:00:51.365424 22895071117440 run.py:483] Algo bellman_ford step 6986 current loss 0.404782, current_train_items 223584.
I0302 19:00:51.386846 22895071117440 run.py:483] Algo bellman_ford step 6987 current loss 0.573758, current_train_items 223616.
I0302 19:00:51.415213 22895071117440 run.py:483] Algo bellman_ford step 6988 current loss 0.718253, current_train_items 223648.
I0302 19:00:51.447957 22895071117440 run.py:483] Algo bellman_ford step 6989 current loss 0.898044, current_train_items 223680.
I0302 19:00:51.466685 22895071117440 run.py:483] Algo bellman_ford step 6990 current loss 0.267718, current_train_items 223712.
I0302 19:00:51.482146 22895071117440 run.py:483] Algo bellman_ford step 6991 current loss 0.604305, current_train_items 223744.
I0302 19:00:51.503710 22895071117440 run.py:483] Algo bellman_ford step 6992 current loss 0.774035, current_train_items 223776.
I0302 19:00:51.533090 22895071117440 run.py:483] Algo bellman_ford step 6993 current loss 0.768551, current_train_items 223808.
I0302 19:00:51.566830 22895071117440 run.py:483] Algo bellman_ford step 6994 current loss 0.822568, current_train_items 223840.
I0302 19:00:51.584719 22895071117440 run.py:483] Algo bellman_ford step 6995 current loss 0.330246, current_train_items 223872.
I0302 19:00:51.600342 22895071117440 run.py:483] Algo bellman_ford step 6996 current loss 0.443831, current_train_items 223904.
I0302 19:00:51.620840 22895071117440 run.py:483] Algo bellman_ford step 6997 current loss 0.545657, current_train_items 223936.
I0302 19:00:51.648628 22895071117440 run.py:483] Algo bellman_ford step 6998 current loss 0.613697, current_train_items 223968.
I0302 19:00:51.679568 22895071117440 run.py:483] Algo bellman_ford step 6999 current loss 0.889714, current_train_items 224000.
I0302 19:00:51.698074 22895071117440 run.py:483] Algo bellman_ford step 7000 current loss 0.302827, current_train_items 224032.
I0302 19:00:51.705772 22895071117440 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:00:51.705879 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:51.721667 22895071117440 run.py:483] Algo bellman_ford step 7001 current loss 0.460153, current_train_items 224064.
I0302 19:00:51.743176 22895071117440 run.py:483] Algo bellman_ford step 7002 current loss 0.670973, current_train_items 224096.
I0302 19:00:51.770238 22895071117440 run.py:483] Algo bellman_ford step 7003 current loss 0.610974, current_train_items 224128.
I0302 19:00:51.800605 22895071117440 run.py:483] Algo bellman_ford step 7004 current loss 0.790842, current_train_items 224160.
I0302 19:00:51.818852 22895071117440 run.py:483] Algo bellman_ford step 7005 current loss 0.289494, current_train_items 224192.
I0302 19:00:51.834337 22895071117440 run.py:483] Algo bellman_ford step 7006 current loss 0.464148, current_train_items 224224.
I0302 19:00:51.856849 22895071117440 run.py:483] Algo bellman_ford step 7007 current loss 0.588270, current_train_items 224256.
I0302 19:00:51.884042 22895071117440 run.py:483] Algo bellman_ford step 7008 current loss 0.616898, current_train_items 224288.
I0302 19:00:51.917068 22895071117440 run.py:483] Algo bellman_ford step 7009 current loss 0.793407, current_train_items 224320.
I0302 19:00:51.935300 22895071117440 run.py:483] Algo bellman_ford step 7010 current loss 0.322781, current_train_items 224352.
I0302 19:00:51.950772 22895071117440 run.py:483] Algo bellman_ford step 7011 current loss 0.467001, current_train_items 224384.
I0302 19:00:51.972492 22895071117440 run.py:483] Algo bellman_ford step 7012 current loss 0.601635, current_train_items 224416.
I0302 19:00:52.000230 22895071117440 run.py:483] Algo bellman_ford step 7013 current loss 0.695011, current_train_items 224448.
I0302 19:00:52.031638 22895071117440 run.py:483] Algo bellman_ford step 7014 current loss 0.727362, current_train_items 224480.
I0302 19:00:52.049715 22895071117440 run.py:483] Algo bellman_ford step 7015 current loss 0.273459, current_train_items 224512.
I0302 19:00:52.064881 22895071117440 run.py:483] Algo bellman_ford step 7016 current loss 0.454902, current_train_items 224544.
I0302 19:00:52.086652 22895071117440 run.py:483] Algo bellman_ford step 7017 current loss 0.528408, current_train_items 224576.
I0302 19:00:52.114415 22895071117440 run.py:483] Algo bellman_ford step 7018 current loss 0.608391, current_train_items 224608.
I0302 19:00:52.144514 22895071117440 run.py:483] Algo bellman_ford step 7019 current loss 0.695220, current_train_items 224640.
I0302 19:00:52.162153 22895071117440 run.py:483] Algo bellman_ford step 7020 current loss 0.293834, current_train_items 224672.
I0302 19:00:52.177462 22895071117440 run.py:483] Algo bellman_ford step 7021 current loss 0.569874, current_train_items 224704.
I0302 19:00:52.199008 22895071117440 run.py:483] Algo bellman_ford step 7022 current loss 0.544689, current_train_items 224736.
I0302 19:00:52.226711 22895071117440 run.py:483] Algo bellman_ford step 7023 current loss 0.580029, current_train_items 224768.
I0302 19:00:52.258273 22895071117440 run.py:483] Algo bellman_ford step 7024 current loss 0.701270, current_train_items 224800.
I0302 19:00:52.276520 22895071117440 run.py:483] Algo bellman_ford step 7025 current loss 0.357657, current_train_items 224832.
I0302 19:00:52.292176 22895071117440 run.py:483] Algo bellman_ford step 7026 current loss 0.478548, current_train_items 224864.
I0302 19:00:52.315272 22895071117440 run.py:483] Algo bellman_ford step 7027 current loss 0.869888, current_train_items 224896.
I0302 19:00:52.345667 22895071117440 run.py:483] Algo bellman_ford step 7028 current loss 0.865395, current_train_items 224928.
I0302 19:00:52.374499 22895071117440 run.py:483] Algo bellman_ford step 7029 current loss 0.733134, current_train_items 224960.
I0302 19:00:52.392782 22895071117440 run.py:483] Algo bellman_ford step 7030 current loss 0.375330, current_train_items 224992.
I0302 19:00:52.408142 22895071117440 run.py:483] Algo bellman_ford step 7031 current loss 0.409666, current_train_items 225024.
I0302 19:00:52.429713 22895071117440 run.py:483] Algo bellman_ford step 7032 current loss 0.606490, current_train_items 225056.
I0302 19:00:52.457590 22895071117440 run.py:483] Algo bellman_ford step 7033 current loss 0.759133, current_train_items 225088.
I0302 19:00:52.490543 22895071117440 run.py:483] Algo bellman_ford step 7034 current loss 1.009853, current_train_items 225120.
I0302 19:00:52.508419 22895071117440 run.py:483] Algo bellman_ford step 7035 current loss 0.232871, current_train_items 225152.
I0302 19:00:52.523869 22895071117440 run.py:483] Algo bellman_ford step 7036 current loss 0.453292, current_train_items 225184.
I0302 19:00:52.545345 22895071117440 run.py:483] Algo bellman_ford step 7037 current loss 0.571366, current_train_items 225216.
I0302 19:00:52.574297 22895071117440 run.py:483] Algo bellman_ford step 7038 current loss 0.665713, current_train_items 225248.
I0302 19:00:52.604558 22895071117440 run.py:483] Algo bellman_ford step 7039 current loss 0.711005, current_train_items 225280.
I0302 19:00:52.622665 22895071117440 run.py:483] Algo bellman_ford step 7040 current loss 0.288926, current_train_items 225312.
I0302 19:00:52.638313 22895071117440 run.py:483] Algo bellman_ford step 7041 current loss 0.466702, current_train_items 225344.
I0302 19:00:52.660048 22895071117440 run.py:483] Algo bellman_ford step 7042 current loss 0.575407, current_train_items 225376.
I0302 19:00:52.689084 22895071117440 run.py:483] Algo bellman_ford step 7043 current loss 0.697806, current_train_items 225408.
I0302 19:00:52.720448 22895071117440 run.py:483] Algo bellman_ford step 7044 current loss 0.648353, current_train_items 225440.
I0302 19:00:52.738646 22895071117440 run.py:483] Algo bellman_ford step 7045 current loss 0.345024, current_train_items 225472.
I0302 19:00:52.754250 22895071117440 run.py:483] Algo bellman_ford step 7046 current loss 0.489886, current_train_items 225504.
I0302 19:00:52.775671 22895071117440 run.py:483] Algo bellman_ford step 7047 current loss 0.668867, current_train_items 225536.
I0302 19:00:52.804711 22895071117440 run.py:483] Algo bellman_ford step 7048 current loss 0.609346, current_train_items 225568.
I0302 19:00:52.835260 22895071117440 run.py:483] Algo bellman_ford step 7049 current loss 0.688095, current_train_items 225600.
I0302 19:00:52.853229 22895071117440 run.py:483] Algo bellman_ford step 7050 current loss 0.261321, current_train_items 225632.
I0302 19:00:52.861269 22895071117440 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:00:52.861377 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:00:52.877051 22895071117440 run.py:483] Algo bellman_ford step 7051 current loss 0.450010, current_train_items 225664.
I0302 19:00:52.900071 22895071117440 run.py:483] Algo bellman_ford step 7052 current loss 0.706023, current_train_items 225696.
I0302 19:00:52.930216 22895071117440 run.py:483] Algo bellman_ford step 7053 current loss 0.844705, current_train_items 225728.
I0302 19:00:52.962804 22895071117440 run.py:483] Algo bellman_ford step 7054 current loss 0.727551, current_train_items 225760.
I0302 19:00:52.981145 22895071117440 run.py:483] Algo bellman_ford step 7055 current loss 0.337126, current_train_items 225792.
I0302 19:00:52.996631 22895071117440 run.py:483] Algo bellman_ford step 7056 current loss 0.426718, current_train_items 225824.
I0302 19:00:53.018963 22895071117440 run.py:483] Algo bellman_ford step 7057 current loss 0.611617, current_train_items 225856.
I0302 19:00:53.047308 22895071117440 run.py:483] Algo bellman_ford step 7058 current loss 0.682117, current_train_items 225888.
I0302 19:00:53.080111 22895071117440 run.py:483] Algo bellman_ford step 7059 current loss 0.753644, current_train_items 225920.
I0302 19:00:53.098447 22895071117440 run.py:483] Algo bellman_ford step 7060 current loss 0.292837, current_train_items 225952.
I0302 19:00:53.114152 22895071117440 run.py:483] Algo bellman_ford step 7061 current loss 0.460515, current_train_items 225984.
I0302 19:00:53.136453 22895071117440 run.py:483] Algo bellman_ford step 7062 current loss 0.638660, current_train_items 226016.
I0302 19:00:53.162534 22895071117440 run.py:483] Algo bellman_ford step 7063 current loss 0.605131, current_train_items 226048.
I0302 19:00:53.193262 22895071117440 run.py:483] Algo bellman_ford step 7064 current loss 0.786108, current_train_items 226080.
I0302 19:00:53.211475 22895071117440 run.py:483] Algo bellman_ford step 7065 current loss 0.305807, current_train_items 226112.
I0302 19:00:53.227211 22895071117440 run.py:483] Algo bellman_ford step 7066 current loss 0.442288, current_train_items 226144.
I0302 19:00:53.248978 22895071117440 run.py:483] Algo bellman_ford step 7067 current loss 0.526572, current_train_items 226176.
I0302 19:00:53.279344 22895071117440 run.py:483] Algo bellman_ford step 7068 current loss 0.715680, current_train_items 226208.
I0302 19:00:53.312257 22895071117440 run.py:483] Algo bellman_ford step 7069 current loss 0.796602, current_train_items 226240.
I0302 19:00:53.330962 22895071117440 run.py:483] Algo bellman_ford step 7070 current loss 0.304845, current_train_items 226272.
I0302 19:00:53.346545 22895071117440 run.py:483] Algo bellman_ford step 7071 current loss 0.537108, current_train_items 226304.
I0302 19:00:53.366514 22895071117440 run.py:483] Algo bellman_ford step 7072 current loss 0.589165, current_train_items 226336.
I0302 19:00:53.394014 22895071117440 run.py:483] Algo bellman_ford step 7073 current loss 0.560876, current_train_items 226368.
I0302 19:00:53.424623 22895071117440 run.py:483] Algo bellman_ford step 7074 current loss 0.710803, current_train_items 226400.
I0302 19:00:53.443567 22895071117440 run.py:483] Algo bellman_ford step 7075 current loss 0.257439, current_train_items 226432.
I0302 19:00:53.458534 22895071117440 run.py:483] Algo bellman_ford step 7076 current loss 0.426389, current_train_items 226464.
I0302 19:00:53.479488 22895071117440 run.py:483] Algo bellman_ford step 7077 current loss 0.674069, current_train_items 226496.
I0302 19:00:53.508469 22895071117440 run.py:483] Algo bellman_ford step 7078 current loss 0.628670, current_train_items 226528.
I0302 19:00:53.539950 22895071117440 run.py:483] Algo bellman_ford step 7079 current loss 0.841660, current_train_items 226560.
I0302 19:00:53.557937 22895071117440 run.py:483] Algo bellman_ford step 7080 current loss 0.220481, current_train_items 226592.
I0302 19:00:53.572920 22895071117440 run.py:483] Algo bellman_ford step 7081 current loss 0.402827, current_train_items 226624.
I0302 19:00:53.594463 22895071117440 run.py:483] Algo bellman_ford step 7082 current loss 0.646518, current_train_items 226656.
I0302 19:00:53.623193 22895071117440 run.py:483] Algo bellman_ford step 7083 current loss 0.704730, current_train_items 226688.
I0302 19:00:53.653704 22895071117440 run.py:483] Algo bellman_ford step 7084 current loss 0.694354, current_train_items 226720.
I0302 19:00:53.672879 22895071117440 run.py:483] Algo bellman_ford step 7085 current loss 0.269559, current_train_items 226752.
I0302 19:00:53.688271 22895071117440 run.py:483] Algo bellman_ford step 7086 current loss 0.449003, current_train_items 226784.
I0302 19:00:53.708932 22895071117440 run.py:483] Algo bellman_ford step 7087 current loss 0.510316, current_train_items 226816.
I0302 19:00:53.737550 22895071117440 run.py:483] Algo bellman_ford step 7088 current loss 0.643927, current_train_items 226848.
I0302 19:00:53.767618 22895071117440 run.py:483] Algo bellman_ford step 7089 current loss 0.665762, current_train_items 226880.
I0302 19:00:53.786245 22895071117440 run.py:483] Algo bellman_ford step 7090 current loss 0.273356, current_train_items 226912.
I0302 19:00:53.801597 22895071117440 run.py:483] Algo bellman_ford step 7091 current loss 0.410926, current_train_items 226944.
I0302 19:00:53.823725 22895071117440 run.py:483] Algo bellman_ford step 7092 current loss 0.672319, current_train_items 226976.
I0302 19:00:53.852984 22895071117440 run.py:483] Algo bellman_ford step 7093 current loss 0.737955, current_train_items 227008.
I0302 19:00:53.881852 22895071117440 run.py:483] Algo bellman_ford step 7094 current loss 0.664037, current_train_items 227040.
I0302 19:00:53.899994 22895071117440 run.py:483] Algo bellman_ford step 7095 current loss 0.339990, current_train_items 227072.
I0302 19:00:53.915356 22895071117440 run.py:483] Algo bellman_ford step 7096 current loss 0.498651, current_train_items 227104.
I0302 19:00:53.936041 22895071117440 run.py:483] Algo bellman_ford step 7097 current loss 0.544118, current_train_items 227136.
I0302 19:00:53.964808 22895071117440 run.py:483] Algo bellman_ford step 7098 current loss 0.774583, current_train_items 227168.
I0302 19:00:53.994790 22895071117440 run.py:483] Algo bellman_ford step 7099 current loss 0.691993, current_train_items 227200.
I0302 19:00:54.013252 22895071117440 run.py:483] Algo bellman_ford step 7100 current loss 0.314790, current_train_items 227232.
I0302 19:00:54.020971 22895071117440 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:00:54.021076 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:00:54.037367 22895071117440 run.py:483] Algo bellman_ford step 7101 current loss 0.467920, current_train_items 227264.
I0302 19:00:54.059592 22895071117440 run.py:483] Algo bellman_ford step 7102 current loss 0.587296, current_train_items 227296.
I0302 19:00:54.088732 22895071117440 run.py:483] Algo bellman_ford step 7103 current loss 0.679351, current_train_items 227328.
I0302 19:00:54.121699 22895071117440 run.py:483] Algo bellman_ford step 7104 current loss 0.888678, current_train_items 227360.
I0302 19:00:54.140381 22895071117440 run.py:483] Algo bellman_ford step 7105 current loss 0.336890, current_train_items 227392.
I0302 19:00:54.156083 22895071117440 run.py:483] Algo bellman_ford step 7106 current loss 0.460654, current_train_items 227424.
I0302 19:00:54.178092 22895071117440 run.py:483] Algo bellman_ford step 7107 current loss 0.650138, current_train_items 227456.
I0302 19:00:54.206443 22895071117440 run.py:483] Algo bellman_ford step 7108 current loss 0.756658, current_train_items 227488.
I0302 19:00:54.239434 22895071117440 run.py:483] Algo bellman_ford step 7109 current loss 0.927982, current_train_items 227520.
I0302 19:00:54.257612 22895071117440 run.py:483] Algo bellman_ford step 7110 current loss 0.322480, current_train_items 227552.
I0302 19:00:54.273198 22895071117440 run.py:483] Algo bellman_ford step 7111 current loss 0.481364, current_train_items 227584.
I0302 19:00:54.295364 22895071117440 run.py:483] Algo bellman_ford step 7112 current loss 0.629144, current_train_items 227616.
I0302 19:00:54.324680 22895071117440 run.py:483] Algo bellman_ford step 7113 current loss 0.893876, current_train_items 227648.
I0302 19:00:54.356223 22895071117440 run.py:483] Algo bellman_ford step 7114 current loss 0.935562, current_train_items 227680.
I0302 19:00:54.374534 22895071117440 run.py:483] Algo bellman_ford step 7115 current loss 0.365274, current_train_items 227712.
I0302 19:00:54.390112 22895071117440 run.py:483] Algo bellman_ford step 7116 current loss 0.564844, current_train_items 227744.
I0302 19:00:54.412293 22895071117440 run.py:483] Algo bellman_ford step 7117 current loss 0.627587, current_train_items 227776.
I0302 19:00:54.441387 22895071117440 run.py:483] Algo bellman_ford step 7118 current loss 0.691249, current_train_items 227808.
I0302 19:00:54.472556 22895071117440 run.py:483] Algo bellman_ford step 7119 current loss 0.686113, current_train_items 227840.
I0302 19:00:54.490615 22895071117440 run.py:483] Algo bellman_ford step 7120 current loss 0.308311, current_train_items 227872.
I0302 19:00:54.506133 22895071117440 run.py:483] Algo bellman_ford step 7121 current loss 0.492962, current_train_items 227904.
I0302 19:00:54.527814 22895071117440 run.py:483] Algo bellman_ford step 7122 current loss 0.584518, current_train_items 227936.
I0302 19:00:54.555917 22895071117440 run.py:483] Algo bellman_ford step 7123 current loss 0.643965, current_train_items 227968.
I0302 19:00:54.587255 22895071117440 run.py:483] Algo bellman_ford step 7124 current loss 0.839281, current_train_items 228000.
I0302 19:00:54.605171 22895071117440 run.py:483] Algo bellman_ford step 7125 current loss 0.325807, current_train_items 228032.
I0302 19:00:54.620005 22895071117440 run.py:483] Algo bellman_ford step 7126 current loss 0.420509, current_train_items 228064.
I0302 19:00:54.641380 22895071117440 run.py:483] Algo bellman_ford step 7127 current loss 0.538755, current_train_items 228096.
I0302 19:00:54.668956 22895071117440 run.py:483] Algo bellman_ford step 7128 current loss 0.603061, current_train_items 228128.
I0302 19:00:54.701432 22895071117440 run.py:483] Algo bellman_ford step 7129 current loss 0.769303, current_train_items 228160.
I0302 19:00:54.719304 22895071117440 run.py:483] Algo bellman_ford step 7130 current loss 0.373586, current_train_items 228192.
I0302 19:00:54.734890 22895071117440 run.py:483] Algo bellman_ford step 7131 current loss 0.435036, current_train_items 228224.
I0302 19:00:54.757526 22895071117440 run.py:483] Algo bellman_ford step 7132 current loss 0.538649, current_train_items 228256.
I0302 19:00:54.785808 22895071117440 run.py:483] Algo bellman_ford step 7133 current loss 0.755205, current_train_items 228288.
I0302 19:00:54.816068 22895071117440 run.py:483] Algo bellman_ford step 7134 current loss 0.771349, current_train_items 228320.
I0302 19:00:54.834269 22895071117440 run.py:483] Algo bellman_ford step 7135 current loss 0.262744, current_train_items 228352.
I0302 19:00:54.849287 22895071117440 run.py:483] Algo bellman_ford step 7136 current loss 0.401331, current_train_items 228384.
I0302 19:00:54.870521 22895071117440 run.py:483] Algo bellman_ford step 7137 current loss 0.611629, current_train_items 228416.
I0302 19:00:54.897614 22895071117440 run.py:483] Algo bellman_ford step 7138 current loss 0.599362, current_train_items 228448.
I0302 19:00:54.931051 22895071117440 run.py:483] Algo bellman_ford step 7139 current loss 0.768174, current_train_items 228480.
I0302 19:00:54.948798 22895071117440 run.py:483] Algo bellman_ford step 7140 current loss 0.340477, current_train_items 228512.
I0302 19:00:54.963998 22895071117440 run.py:483] Algo bellman_ford step 7141 current loss 0.558956, current_train_items 228544.
I0302 19:00:54.986275 22895071117440 run.py:483] Algo bellman_ford step 7142 current loss 0.576506, current_train_items 228576.
I0302 19:00:55.012887 22895071117440 run.py:483] Algo bellman_ford step 7143 current loss 0.577242, current_train_items 228608.
I0302 19:00:55.042821 22895071117440 run.py:483] Algo bellman_ford step 7144 current loss 0.783843, current_train_items 228640.
I0302 19:00:55.060815 22895071117440 run.py:483] Algo bellman_ford step 7145 current loss 0.313477, current_train_items 228672.
I0302 19:00:55.076526 22895071117440 run.py:483] Algo bellman_ford step 7146 current loss 0.480856, current_train_items 228704.
I0302 19:00:55.098694 22895071117440 run.py:483] Algo bellman_ford step 7147 current loss 0.674083, current_train_items 228736.
I0302 19:00:55.126060 22895071117440 run.py:483] Algo bellman_ford step 7148 current loss 0.618703, current_train_items 228768.
I0302 19:00:55.154432 22895071117440 run.py:483] Algo bellman_ford step 7149 current loss 0.705728, current_train_items 228800.
I0302 19:00:55.172326 22895071117440 run.py:483] Algo bellman_ford step 7150 current loss 0.282605, current_train_items 228832.
I0302 19:00:55.180254 22895071117440 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:00:55.180361 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:00:55.196782 22895071117440 run.py:483] Algo bellman_ford step 7151 current loss 0.443269, current_train_items 228864.
I0302 19:00:55.219555 22895071117440 run.py:483] Algo bellman_ford step 7152 current loss 0.688213, current_train_items 228896.
I0302 19:00:55.248292 22895071117440 run.py:483] Algo bellman_ford step 7153 current loss 0.758558, current_train_items 228928.
I0302 19:00:55.279282 22895071117440 run.py:483] Algo bellman_ford step 7154 current loss 0.619825, current_train_items 228960.
I0302 19:00:55.297780 22895071117440 run.py:483] Algo bellman_ford step 7155 current loss 0.288464, current_train_items 228992.
I0302 19:00:55.313431 22895071117440 run.py:483] Algo bellman_ford step 7156 current loss 0.525551, current_train_items 229024.
I0302 19:00:55.335152 22895071117440 run.py:483] Algo bellman_ford step 7157 current loss 0.617967, current_train_items 229056.
I0302 19:00:55.363170 22895071117440 run.py:483] Algo bellman_ford step 7158 current loss 0.628272, current_train_items 229088.
I0302 19:00:55.394600 22895071117440 run.py:483] Algo bellman_ford step 7159 current loss 0.718193, current_train_items 229120.
I0302 19:00:55.412835 22895071117440 run.py:483] Algo bellman_ford step 7160 current loss 0.312843, current_train_items 229152.
I0302 19:00:55.428808 22895071117440 run.py:483] Algo bellman_ford step 7161 current loss 0.587432, current_train_items 229184.
W0302 19:00:55.442992 22895071117440 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:01.034683 22895071117440 run.py:483] Algo bellman_ford step 7162 current loss 0.704736, current_train_items 229216.
I0302 19:01:01.064273 22895071117440 run.py:483] Algo bellman_ford step 7163 current loss 0.676244, current_train_items 229248.
I0302 19:01:01.096344 22895071117440 run.py:483] Algo bellman_ford step 7164 current loss 0.792826, current_train_items 229280.
I0302 19:01:01.114793 22895071117440 run.py:483] Algo bellman_ford step 7165 current loss 0.363440, current_train_items 229312.
I0302 19:01:01.130151 22895071117440 run.py:483] Algo bellman_ford step 7166 current loss 0.446183, current_train_items 229344.
I0302 19:01:01.151803 22895071117440 run.py:483] Algo bellman_ford step 7167 current loss 0.589365, current_train_items 229376.
I0302 19:01:01.180753 22895071117440 run.py:483] Algo bellman_ford step 7168 current loss 0.530216, current_train_items 229408.
I0302 19:01:01.209116 22895071117440 run.py:483] Algo bellman_ford step 7169 current loss 0.746866, current_train_items 229440.
I0302 19:01:01.227519 22895071117440 run.py:483] Algo bellman_ford step 7170 current loss 0.325213, current_train_items 229472.
I0302 19:01:01.243554 22895071117440 run.py:483] Algo bellman_ford step 7171 current loss 0.484376, current_train_items 229504.
I0302 19:01:01.264980 22895071117440 run.py:483] Algo bellman_ford step 7172 current loss 0.587646, current_train_items 229536.
I0302 19:01:01.293763 22895071117440 run.py:483] Algo bellman_ford step 7173 current loss 0.607195, current_train_items 229568.
I0302 19:01:01.325576 22895071117440 run.py:483] Algo bellman_ford step 7174 current loss 0.947036, current_train_items 229600.
I0302 19:01:01.344359 22895071117440 run.py:483] Algo bellman_ford step 7175 current loss 0.316663, current_train_items 229632.
I0302 19:01:01.359638 22895071117440 run.py:483] Algo bellman_ford step 7176 current loss 0.542700, current_train_items 229664.
I0302 19:01:01.381158 22895071117440 run.py:483] Algo bellman_ford step 7177 current loss 0.597349, current_train_items 229696.
I0302 19:01:01.411890 22895071117440 run.py:483] Algo bellman_ford step 7178 current loss 0.758067, current_train_items 229728.
I0302 19:01:01.444171 22895071117440 run.py:483] Algo bellman_ford step 7179 current loss 0.735545, current_train_items 229760.
I0302 19:01:01.462349 22895071117440 run.py:483] Algo bellman_ford step 7180 current loss 0.255094, current_train_items 229792.
I0302 19:01:01.477888 22895071117440 run.py:483] Algo bellman_ford step 7181 current loss 0.485664, current_train_items 229824.
I0302 19:01:01.500608 22895071117440 run.py:483] Algo bellman_ford step 7182 current loss 0.644030, current_train_items 229856.
I0302 19:01:01.528662 22895071117440 run.py:483] Algo bellman_ford step 7183 current loss 0.594843, current_train_items 229888.
I0302 19:01:01.562630 22895071117440 run.py:483] Algo bellman_ford step 7184 current loss 0.822425, current_train_items 229920.
I0302 19:01:01.580923 22895071117440 run.py:483] Algo bellman_ford step 7185 current loss 0.320441, current_train_items 229952.
I0302 19:01:01.596610 22895071117440 run.py:483] Algo bellman_ford step 7186 current loss 0.503922, current_train_items 229984.
I0302 19:01:01.619231 22895071117440 run.py:483] Algo bellman_ford step 7187 current loss 0.640733, current_train_items 230016.
I0302 19:01:01.648291 22895071117440 run.py:483] Algo bellman_ford step 7188 current loss 0.695640, current_train_items 230048.
I0302 19:01:01.681040 22895071117440 run.py:483] Algo bellman_ford step 7189 current loss 0.832276, current_train_items 230080.
I0302 19:01:01.699577 22895071117440 run.py:483] Algo bellman_ford step 7190 current loss 0.276474, current_train_items 230112.
I0302 19:01:01.715515 22895071117440 run.py:483] Algo bellman_ford step 7191 current loss 0.476866, current_train_items 230144.
I0302 19:01:01.737178 22895071117440 run.py:483] Algo bellman_ford step 7192 current loss 0.732953, current_train_items 230176.
I0302 19:01:01.765849 22895071117440 run.py:483] Algo bellman_ford step 7193 current loss 0.700561, current_train_items 230208.
I0302 19:01:01.796465 22895071117440 run.py:483] Algo bellman_ford step 7194 current loss 0.745588, current_train_items 230240.
I0302 19:01:01.814783 22895071117440 run.py:483] Algo bellman_ford step 7195 current loss 0.290377, current_train_items 230272.
I0302 19:01:01.829965 22895071117440 run.py:483] Algo bellman_ford step 7196 current loss 0.490573, current_train_items 230304.
I0302 19:01:01.851687 22895071117440 run.py:483] Algo bellman_ford step 7197 current loss 0.619624, current_train_items 230336.
I0302 19:01:01.880769 22895071117440 run.py:483] Algo bellman_ford step 7198 current loss 0.697771, current_train_items 230368.
I0302 19:01:01.911770 22895071117440 run.py:483] Algo bellman_ford step 7199 current loss 0.952035, current_train_items 230400.
I0302 19:01:01.930433 22895071117440 run.py:483] Algo bellman_ford step 7200 current loss 0.295252, current_train_items 230432.
I0302 19:01:01.939749 22895071117440 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:01.939858 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:01.955804 22895071117440 run.py:483] Algo bellman_ford step 7201 current loss 0.542252, current_train_items 230464.
I0302 19:01:01.977477 22895071117440 run.py:483] Algo bellman_ford step 7202 current loss 0.603798, current_train_items 230496.
I0302 19:01:02.006993 22895071117440 run.py:483] Algo bellman_ford step 7203 current loss 0.657666, current_train_items 230528.
I0302 19:01:02.038050 22895071117440 run.py:483] Algo bellman_ford step 7204 current loss 0.695888, current_train_items 230560.
I0302 19:01:02.056367 22895071117440 run.py:483] Algo bellman_ford step 7205 current loss 0.352192, current_train_items 230592.
I0302 19:01:02.071473 22895071117440 run.py:483] Algo bellman_ford step 7206 current loss 0.482786, current_train_items 230624.
I0302 19:01:02.093415 22895071117440 run.py:483] Algo bellman_ford step 7207 current loss 0.558020, current_train_items 230656.
I0302 19:01:02.121850 22895071117440 run.py:483] Algo bellman_ford step 7208 current loss 0.646085, current_train_items 230688.
I0302 19:01:02.153871 22895071117440 run.py:483] Algo bellman_ford step 7209 current loss 0.857864, current_train_items 230720.
I0302 19:01:02.171715 22895071117440 run.py:483] Algo bellman_ford step 7210 current loss 0.299381, current_train_items 230752.
I0302 19:01:02.186817 22895071117440 run.py:483] Algo bellman_ford step 7211 current loss 0.430679, current_train_items 230784.
I0302 19:01:02.207989 22895071117440 run.py:483] Algo bellman_ford step 7212 current loss 0.605293, current_train_items 230816.
I0302 19:01:02.236010 22895071117440 run.py:483] Algo bellman_ford step 7213 current loss 0.583772, current_train_items 230848.
I0302 19:01:02.265098 22895071117440 run.py:483] Algo bellman_ford step 7214 current loss 0.745201, current_train_items 230880.
I0302 19:01:02.282779 22895071117440 run.py:483] Algo bellman_ford step 7215 current loss 0.391434, current_train_items 230912.
I0302 19:01:02.298065 22895071117440 run.py:483] Algo bellman_ford step 7216 current loss 0.491561, current_train_items 230944.
I0302 19:01:02.319894 22895071117440 run.py:483] Algo bellman_ford step 7217 current loss 0.573676, current_train_items 230976.
I0302 19:01:02.348784 22895071117440 run.py:483] Algo bellman_ford step 7218 current loss 0.653499, current_train_items 231008.
I0302 19:01:02.379074 22895071117440 run.py:483] Algo bellman_ford step 7219 current loss 0.783485, current_train_items 231040.
I0302 19:01:02.396851 22895071117440 run.py:483] Algo bellman_ford step 7220 current loss 0.377691, current_train_items 231072.
I0302 19:01:02.412275 22895071117440 run.py:483] Algo bellman_ford step 7221 current loss 0.561389, current_train_items 231104.
I0302 19:01:02.434801 22895071117440 run.py:483] Algo bellman_ford step 7222 current loss 0.654539, current_train_items 231136.
I0302 19:01:02.463237 22895071117440 run.py:483] Algo bellman_ford step 7223 current loss 0.728719, current_train_items 231168.
I0302 19:01:02.494680 22895071117440 run.py:483] Algo bellman_ford step 7224 current loss 0.899095, current_train_items 231200.
I0302 19:01:02.512935 22895071117440 run.py:483] Algo bellman_ford step 7225 current loss 0.272937, current_train_items 231232.
I0302 19:01:02.528308 22895071117440 run.py:483] Algo bellman_ford step 7226 current loss 0.458412, current_train_items 231264.
I0302 19:01:02.551481 22895071117440 run.py:483] Algo bellman_ford step 7227 current loss 0.738744, current_train_items 231296.
I0302 19:01:02.579906 22895071117440 run.py:483] Algo bellman_ford step 7228 current loss 0.649814, current_train_items 231328.
I0302 19:01:02.609003 22895071117440 run.py:483] Algo bellman_ford step 7229 current loss 0.884498, current_train_items 231360.
I0302 19:01:02.626994 22895071117440 run.py:483] Algo bellman_ford step 7230 current loss 0.298986, current_train_items 231392.
I0302 19:01:02.642558 22895071117440 run.py:483] Algo bellman_ford step 7231 current loss 0.491775, current_train_items 231424.
I0302 19:01:02.662407 22895071117440 run.py:483] Algo bellman_ford step 7232 current loss 0.528844, current_train_items 231456.
I0302 19:01:02.691697 22895071117440 run.py:483] Algo bellman_ford step 7233 current loss 0.642807, current_train_items 231488.
I0302 19:01:02.721510 22895071117440 run.py:483] Algo bellman_ford step 7234 current loss 0.776022, current_train_items 231520.
I0302 19:01:02.739549 22895071117440 run.py:483] Algo bellman_ford step 7235 current loss 0.265144, current_train_items 231552.
I0302 19:01:02.755404 22895071117440 run.py:483] Algo bellman_ford step 7236 current loss 0.507226, current_train_items 231584.
I0302 19:01:02.778001 22895071117440 run.py:483] Algo bellman_ford step 7237 current loss 0.569119, current_train_items 231616.
I0302 19:01:02.808151 22895071117440 run.py:483] Algo bellman_ford step 7238 current loss 0.758166, current_train_items 231648.
I0302 19:01:02.839018 22895071117440 run.py:483] Algo bellman_ford step 7239 current loss 0.660061, current_train_items 231680.
I0302 19:01:02.856884 22895071117440 run.py:483] Algo bellman_ford step 7240 current loss 0.287441, current_train_items 231712.
I0302 19:01:02.872117 22895071117440 run.py:483] Algo bellman_ford step 7241 current loss 0.468800, current_train_items 231744.
I0302 19:01:02.894794 22895071117440 run.py:483] Algo bellman_ford step 7242 current loss 0.751496, current_train_items 231776.
I0302 19:01:02.923711 22895071117440 run.py:483] Algo bellman_ford step 7243 current loss 0.689847, current_train_items 231808.
I0302 19:01:02.956320 22895071117440 run.py:483] Algo bellman_ford step 7244 current loss 0.755020, current_train_items 231840.
I0302 19:01:02.974037 22895071117440 run.py:483] Algo bellman_ford step 7245 current loss 0.335037, current_train_items 231872.
I0302 19:01:02.989403 22895071117440 run.py:483] Algo bellman_ford step 7246 current loss 0.437936, current_train_items 231904.
I0302 19:01:03.011477 22895071117440 run.py:483] Algo bellman_ford step 7247 current loss 0.630718, current_train_items 231936.
I0302 19:01:03.041516 22895071117440 run.py:483] Algo bellman_ford step 7248 current loss 0.817145, current_train_items 231968.
I0302 19:01:03.072830 22895071117440 run.py:483] Algo bellman_ford step 7249 current loss 0.873630, current_train_items 232000.
I0302 19:01:03.091005 22895071117440 run.py:483] Algo bellman_ford step 7250 current loss 0.341029, current_train_items 232032.
I0302 19:01:03.099539 22895071117440 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:03.099646 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:01:03.115887 22895071117440 run.py:483] Algo bellman_ford step 7251 current loss 0.434170, current_train_items 232064.
I0302 19:01:03.138030 22895071117440 run.py:483] Algo bellman_ford step 7252 current loss 0.558161, current_train_items 232096.
I0302 19:01:03.165963 22895071117440 run.py:483] Algo bellman_ford step 7253 current loss 0.563668, current_train_items 232128.
I0302 19:01:03.197477 22895071117440 run.py:483] Algo bellman_ford step 7254 current loss 0.712802, current_train_items 232160.
I0302 19:01:03.216241 22895071117440 run.py:483] Algo bellman_ford step 7255 current loss 0.342471, current_train_items 232192.
I0302 19:01:03.231229 22895071117440 run.py:483] Algo bellman_ford step 7256 current loss 0.480852, current_train_items 232224.
I0302 19:01:03.253236 22895071117440 run.py:483] Algo bellman_ford step 7257 current loss 0.586229, current_train_items 232256.
I0302 19:01:03.284561 22895071117440 run.py:483] Algo bellman_ford step 7258 current loss 0.713991, current_train_items 232288.
I0302 19:01:03.315922 22895071117440 run.py:483] Algo bellman_ford step 7259 current loss 0.748818, current_train_items 232320.
I0302 19:01:03.334350 22895071117440 run.py:483] Algo bellman_ford step 7260 current loss 0.295340, current_train_items 232352.
I0302 19:01:03.350126 22895071117440 run.py:483] Algo bellman_ford step 7261 current loss 0.480416, current_train_items 232384.
I0302 19:01:03.372116 22895071117440 run.py:483] Algo bellman_ford step 7262 current loss 0.591524, current_train_items 232416.
I0302 19:01:03.401054 22895071117440 run.py:483] Algo bellman_ford step 7263 current loss 0.714438, current_train_items 232448.
I0302 19:01:03.431627 22895071117440 run.py:483] Algo bellman_ford step 7264 current loss 0.741926, current_train_items 232480.
I0302 19:01:03.449705 22895071117440 run.py:483] Algo bellman_ford step 7265 current loss 0.274482, current_train_items 232512.
I0302 19:01:03.464946 22895071117440 run.py:483] Algo bellman_ford step 7266 current loss 0.498123, current_train_items 232544.
I0302 19:01:03.486605 22895071117440 run.py:483] Algo bellman_ford step 7267 current loss 0.624489, current_train_items 232576.
I0302 19:01:03.515567 22895071117440 run.py:483] Algo bellman_ford step 7268 current loss 0.580776, current_train_items 232608.
I0302 19:01:03.545917 22895071117440 run.py:483] Algo bellman_ford step 7269 current loss 0.751751, current_train_items 232640.
I0302 19:01:03.564494 22895071117440 run.py:483] Algo bellman_ford step 7270 current loss 0.313515, current_train_items 232672.
I0302 19:01:03.580451 22895071117440 run.py:483] Algo bellman_ford step 7271 current loss 0.518233, current_train_items 232704.
I0302 19:01:03.602282 22895071117440 run.py:483] Algo bellman_ford step 7272 current loss 0.481245, current_train_items 232736.
I0302 19:01:03.630968 22895071117440 run.py:483] Algo bellman_ford step 7273 current loss 0.613126, current_train_items 232768.
I0302 19:01:03.661696 22895071117440 run.py:483] Algo bellman_ford step 7274 current loss 0.688894, current_train_items 232800.
I0302 19:01:03.680295 22895071117440 run.py:483] Algo bellman_ford step 7275 current loss 0.321970, current_train_items 232832.
I0302 19:01:03.695909 22895071117440 run.py:483] Algo bellman_ford step 7276 current loss 0.427311, current_train_items 232864.
I0302 19:01:03.717784 22895071117440 run.py:483] Algo bellman_ford step 7277 current loss 0.608382, current_train_items 232896.
I0302 19:01:03.746784 22895071117440 run.py:483] Algo bellman_ford step 7278 current loss 0.663587, current_train_items 232928.
I0302 19:01:03.777758 22895071117440 run.py:483] Algo bellman_ford step 7279 current loss 0.665341, current_train_items 232960.
I0302 19:01:03.795535 22895071117440 run.py:483] Algo bellman_ford step 7280 current loss 0.283481, current_train_items 232992.
I0302 19:01:03.811400 22895071117440 run.py:483] Algo bellman_ford step 7281 current loss 0.441611, current_train_items 233024.
I0302 19:01:03.832778 22895071117440 run.py:483] Algo bellman_ford step 7282 current loss 0.552621, current_train_items 233056.
I0302 19:01:03.861533 22895071117440 run.py:483] Algo bellman_ford step 7283 current loss 0.673339, current_train_items 233088.
I0302 19:01:03.891868 22895071117440 run.py:483] Algo bellman_ford step 7284 current loss 0.603574, current_train_items 233120.
I0302 19:01:03.910418 22895071117440 run.py:483] Algo bellman_ford step 7285 current loss 0.272718, current_train_items 233152.
I0302 19:01:03.927052 22895071117440 run.py:483] Algo bellman_ford step 7286 current loss 0.615173, current_train_items 233184.
I0302 19:01:03.948466 22895071117440 run.py:483] Algo bellman_ford step 7287 current loss 0.508770, current_train_items 233216.
I0302 19:01:03.977386 22895071117440 run.py:483] Algo bellman_ford step 7288 current loss 0.626461, current_train_items 233248.
I0302 19:01:04.007697 22895071117440 run.py:483] Algo bellman_ford step 7289 current loss 0.654150, current_train_items 233280.
I0302 19:01:04.026197 22895071117440 run.py:483] Algo bellman_ford step 7290 current loss 0.330280, current_train_items 233312.
I0302 19:01:04.042017 22895071117440 run.py:483] Algo bellman_ford step 7291 current loss 0.481404, current_train_items 233344.
I0302 19:01:04.062848 22895071117440 run.py:483] Algo bellman_ford step 7292 current loss 0.477105, current_train_items 233376.
I0302 19:01:04.092263 22895071117440 run.py:483] Algo bellman_ford step 7293 current loss 0.578740, current_train_items 233408.
I0302 19:01:04.121525 22895071117440 run.py:483] Algo bellman_ford step 7294 current loss 0.682264, current_train_items 233440.
I0302 19:01:04.139323 22895071117440 run.py:483] Algo bellman_ford step 7295 current loss 0.307680, current_train_items 233472.
I0302 19:01:04.154796 22895071117440 run.py:483] Algo bellman_ford step 7296 current loss 0.437645, current_train_items 233504.
I0302 19:01:04.176562 22895071117440 run.py:483] Algo bellman_ford step 7297 current loss 0.653772, current_train_items 233536.
I0302 19:01:04.205564 22895071117440 run.py:483] Algo bellman_ford step 7298 current loss 0.643500, current_train_items 233568.
I0302 19:01:04.236087 22895071117440 run.py:483] Algo bellman_ford step 7299 current loss 0.726503, current_train_items 233600.
I0302 19:01:04.254797 22895071117440 run.py:483] Algo bellman_ford step 7300 current loss 0.325109, current_train_items 233632.
I0302 19:01:04.262687 22895071117440 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:04.262791 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:01:04.279102 22895071117440 run.py:483] Algo bellman_ford step 7301 current loss 0.461485, current_train_items 233664.
I0302 19:01:04.301250 22895071117440 run.py:483] Algo bellman_ford step 7302 current loss 0.585223, current_train_items 233696.
I0302 19:01:04.330062 22895071117440 run.py:483] Algo bellman_ford step 7303 current loss 0.721399, current_train_items 233728.
I0302 19:01:04.361765 22895071117440 run.py:483] Algo bellman_ford step 7304 current loss 0.739968, current_train_items 233760.
I0302 19:01:04.380266 22895071117440 run.py:483] Algo bellman_ford step 7305 current loss 0.306174, current_train_items 233792.
I0302 19:01:04.395474 22895071117440 run.py:483] Algo bellman_ford step 7306 current loss 0.411582, current_train_items 233824.
I0302 19:01:04.417208 22895071117440 run.py:483] Algo bellman_ford step 7307 current loss 0.655859, current_train_items 233856.
I0302 19:01:04.444674 22895071117440 run.py:483] Algo bellman_ford step 7308 current loss 0.577141, current_train_items 233888.
I0302 19:01:04.476758 22895071117440 run.py:483] Algo bellman_ford step 7309 current loss 0.789290, current_train_items 233920.
I0302 19:01:04.494981 22895071117440 run.py:483] Algo bellman_ford step 7310 current loss 0.252123, current_train_items 233952.
I0302 19:01:04.510216 22895071117440 run.py:483] Algo bellman_ford step 7311 current loss 0.417929, current_train_items 233984.
I0302 19:01:04.533456 22895071117440 run.py:483] Algo bellman_ford step 7312 current loss 0.657063, current_train_items 234016.
I0302 19:01:04.562261 22895071117440 run.py:483] Algo bellman_ford step 7313 current loss 0.590952, current_train_items 234048.
I0302 19:01:04.594168 22895071117440 run.py:483] Algo bellman_ford step 7314 current loss 0.656555, current_train_items 234080.
I0302 19:01:04.612311 22895071117440 run.py:483] Algo bellman_ford step 7315 current loss 0.244825, current_train_items 234112.
I0302 19:01:04.627476 22895071117440 run.py:483] Algo bellman_ford step 7316 current loss 0.449755, current_train_items 234144.
I0302 19:01:04.650187 22895071117440 run.py:483] Algo bellman_ford step 7317 current loss 0.621492, current_train_items 234176.
I0302 19:01:04.679036 22895071117440 run.py:483] Algo bellman_ford step 7318 current loss 0.687909, current_train_items 234208.
I0302 19:01:04.711530 22895071117440 run.py:483] Algo bellman_ford step 7319 current loss 0.815425, current_train_items 234240.
I0302 19:01:04.729519 22895071117440 run.py:483] Algo bellman_ford step 7320 current loss 0.333622, current_train_items 234272.
I0302 19:01:04.745440 22895071117440 run.py:483] Algo bellman_ford step 7321 current loss 0.499102, current_train_items 234304.
I0302 19:01:04.768674 22895071117440 run.py:483] Algo bellman_ford step 7322 current loss 0.649045, current_train_items 234336.
I0302 19:01:04.796833 22895071117440 run.py:483] Algo bellman_ford step 7323 current loss 0.604937, current_train_items 234368.
I0302 19:01:04.829440 22895071117440 run.py:483] Algo bellman_ford step 7324 current loss 0.758739, current_train_items 234400.
I0302 19:01:04.847527 22895071117440 run.py:483] Algo bellman_ford step 7325 current loss 0.298300, current_train_items 234432.
I0302 19:01:04.863218 22895071117440 run.py:483] Algo bellman_ford step 7326 current loss 0.497181, current_train_items 234464.
I0302 19:01:04.885563 22895071117440 run.py:483] Algo bellman_ford step 7327 current loss 0.566067, current_train_items 234496.
I0302 19:01:04.913998 22895071117440 run.py:483] Algo bellman_ford step 7328 current loss 0.655050, current_train_items 234528.
I0302 19:01:04.945443 22895071117440 run.py:483] Algo bellman_ford step 7329 current loss 0.676077, current_train_items 234560.
I0302 19:01:04.963443 22895071117440 run.py:483] Algo bellman_ford step 7330 current loss 0.313200, current_train_items 234592.
I0302 19:01:04.979118 22895071117440 run.py:483] Algo bellman_ford step 7331 current loss 0.594719, current_train_items 234624.
I0302 19:01:05.002080 22895071117440 run.py:483] Algo bellman_ford step 7332 current loss 0.579658, current_train_items 234656.
I0302 19:01:05.031529 22895071117440 run.py:483] Algo bellman_ford step 7333 current loss 0.692916, current_train_items 234688.
I0302 19:01:05.061089 22895071117440 run.py:483] Algo bellman_ford step 7334 current loss 0.808907, current_train_items 234720.
I0302 19:01:05.078932 22895071117440 run.py:483] Algo bellman_ford step 7335 current loss 0.300081, current_train_items 234752.
I0302 19:01:05.093990 22895071117440 run.py:483] Algo bellman_ford step 7336 current loss 0.447407, current_train_items 234784.
I0302 19:01:05.116060 22895071117440 run.py:483] Algo bellman_ford step 7337 current loss 0.552777, current_train_items 234816.
I0302 19:01:05.145277 22895071117440 run.py:483] Algo bellman_ford step 7338 current loss 0.602060, current_train_items 234848.
I0302 19:01:05.178121 22895071117440 run.py:483] Algo bellman_ford step 7339 current loss 0.695332, current_train_items 234880.
I0302 19:01:05.196397 22895071117440 run.py:483] Algo bellman_ford step 7340 current loss 0.243918, current_train_items 234912.
I0302 19:01:05.211688 22895071117440 run.py:483] Algo bellman_ford step 7341 current loss 0.517793, current_train_items 234944.
I0302 19:01:05.234413 22895071117440 run.py:483] Algo bellman_ford step 7342 current loss 0.703538, current_train_items 234976.
I0302 19:01:05.263852 22895071117440 run.py:483] Algo bellman_ford step 7343 current loss 0.651612, current_train_items 235008.
I0302 19:01:05.294503 22895071117440 run.py:483] Algo bellman_ford step 7344 current loss 0.815571, current_train_items 235040.
I0302 19:01:05.312654 22895071117440 run.py:483] Algo bellman_ford step 7345 current loss 0.325160, current_train_items 235072.
I0302 19:01:05.327685 22895071117440 run.py:483] Algo bellman_ford step 7346 current loss 0.448628, current_train_items 235104.
I0302 19:01:05.350192 22895071117440 run.py:483] Algo bellman_ford step 7347 current loss 0.724008, current_train_items 235136.
I0302 19:01:05.379587 22895071117440 run.py:483] Algo bellman_ford step 7348 current loss 0.627115, current_train_items 235168.
I0302 19:01:05.407397 22895071117440 run.py:483] Algo bellman_ford step 7349 current loss 0.626488, current_train_items 235200.
I0302 19:01:05.425565 22895071117440 run.py:483] Algo bellman_ford step 7350 current loss 0.296001, current_train_items 235232.
I0302 19:01:05.433390 22895071117440 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:05.433521 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:01:05.450145 22895071117440 run.py:483] Algo bellman_ford step 7351 current loss 0.547252, current_train_items 235264.
I0302 19:01:05.473191 22895071117440 run.py:483] Algo bellman_ford step 7352 current loss 0.618355, current_train_items 235296.
I0302 19:01:05.502962 22895071117440 run.py:483] Algo bellman_ford step 7353 current loss 0.768399, current_train_items 235328.
I0302 19:01:05.533757 22895071117440 run.py:483] Algo bellman_ford step 7354 current loss 0.807189, current_train_items 235360.
I0302 19:01:05.552138 22895071117440 run.py:483] Algo bellman_ford step 7355 current loss 0.310891, current_train_items 235392.
I0302 19:01:05.567220 22895071117440 run.py:483] Algo bellman_ford step 7356 current loss 0.437763, current_train_items 235424.
I0302 19:01:05.589907 22895071117440 run.py:483] Algo bellman_ford step 7357 current loss 0.613011, current_train_items 235456.
I0302 19:01:05.618173 22895071117440 run.py:483] Algo bellman_ford step 7358 current loss 0.672795, current_train_items 235488.
I0302 19:01:05.648633 22895071117440 run.py:483] Algo bellman_ford step 7359 current loss 0.646302, current_train_items 235520.
I0302 19:01:05.666918 22895071117440 run.py:483] Algo bellman_ford step 7360 current loss 0.367234, current_train_items 235552.
I0302 19:01:05.683137 22895071117440 run.py:483] Algo bellman_ford step 7361 current loss 0.516686, current_train_items 235584.
I0302 19:01:05.703675 22895071117440 run.py:483] Algo bellman_ford step 7362 current loss 0.650775, current_train_items 235616.
I0302 19:01:05.734127 22895071117440 run.py:483] Algo bellman_ford step 7363 current loss 0.836496, current_train_items 235648.
I0302 19:01:05.763012 22895071117440 run.py:483] Algo bellman_ford step 7364 current loss 0.736109, current_train_items 235680.
I0302 19:01:05.780999 22895071117440 run.py:483] Algo bellman_ford step 7365 current loss 0.330087, current_train_items 235712.
I0302 19:01:05.796317 22895071117440 run.py:483] Algo bellman_ford step 7366 current loss 0.452814, current_train_items 235744.
I0302 19:01:05.818706 22895071117440 run.py:483] Algo bellman_ford step 7367 current loss 0.502235, current_train_items 235776.
I0302 19:01:05.849220 22895071117440 run.py:483] Algo bellman_ford step 7368 current loss 0.682273, current_train_items 235808.
I0302 19:01:05.878810 22895071117440 run.py:483] Algo bellman_ford step 7369 current loss 0.609508, current_train_items 235840.
I0302 19:01:05.896876 22895071117440 run.py:483] Algo bellman_ford step 7370 current loss 0.246254, current_train_items 235872.
I0302 19:01:05.912673 22895071117440 run.py:483] Algo bellman_ford step 7371 current loss 0.554488, current_train_items 235904.
I0302 19:01:05.933237 22895071117440 run.py:483] Algo bellman_ford step 7372 current loss 0.611492, current_train_items 235936.
I0302 19:01:05.961737 22895071117440 run.py:483] Algo bellman_ford step 7373 current loss 0.672099, current_train_items 235968.
I0302 19:01:05.991891 22895071117440 run.py:483] Algo bellman_ford step 7374 current loss 0.828974, current_train_items 236000.
I0302 19:01:06.010188 22895071117440 run.py:483] Algo bellman_ford step 7375 current loss 0.248426, current_train_items 236032.
I0302 19:01:06.026424 22895071117440 run.py:483] Algo bellman_ford step 7376 current loss 0.410137, current_train_items 236064.
I0302 19:01:06.048366 22895071117440 run.py:483] Algo bellman_ford step 7377 current loss 0.563791, current_train_items 236096.
I0302 19:01:06.076762 22895071117440 run.py:483] Algo bellman_ford step 7378 current loss 0.603791, current_train_items 236128.
I0302 19:01:06.106098 22895071117440 run.py:483] Algo bellman_ford step 7379 current loss 0.651058, current_train_items 236160.
I0302 19:01:06.123808 22895071117440 run.py:483] Algo bellman_ford step 7380 current loss 0.339448, current_train_items 236192.
I0302 19:01:06.138780 22895071117440 run.py:483] Algo bellman_ford step 7381 current loss 0.441930, current_train_items 236224.
I0302 19:01:06.161603 22895071117440 run.py:483] Algo bellman_ford step 7382 current loss 0.677673, current_train_items 236256.
I0302 19:01:06.189532 22895071117440 run.py:483] Algo bellman_ford step 7383 current loss 0.683709, current_train_items 236288.
I0302 19:01:06.219402 22895071117440 run.py:483] Algo bellman_ford step 7384 current loss 0.660839, current_train_items 236320.
I0302 19:01:06.237589 22895071117440 run.py:483] Algo bellman_ford step 7385 current loss 0.244889, current_train_items 236352.
I0302 19:01:06.253331 22895071117440 run.py:483] Algo bellman_ford step 7386 current loss 0.469369, current_train_items 236384.
I0302 19:01:06.275420 22895071117440 run.py:483] Algo bellman_ford step 7387 current loss 0.562447, current_train_items 236416.
I0302 19:01:06.304730 22895071117440 run.py:483] Algo bellman_ford step 7388 current loss 0.632391, current_train_items 236448.
I0302 19:01:06.334443 22895071117440 run.py:483] Algo bellman_ford step 7389 current loss 0.586067, current_train_items 236480.
I0302 19:01:06.352745 22895071117440 run.py:483] Algo bellman_ford step 7390 current loss 0.274933, current_train_items 236512.
I0302 19:01:06.368639 22895071117440 run.py:483] Algo bellman_ford step 7391 current loss 0.505488, current_train_items 236544.
I0302 19:01:06.390122 22895071117440 run.py:483] Algo bellman_ford step 7392 current loss 0.583922, current_train_items 236576.
I0302 19:01:06.418096 22895071117440 run.py:483] Algo bellman_ford step 7393 current loss 0.577252, current_train_items 236608.
I0302 19:01:06.449145 22895071117440 run.py:483] Algo bellman_ford step 7394 current loss 0.763474, current_train_items 236640.
I0302 19:01:06.466814 22895071117440 run.py:483] Algo bellman_ford step 7395 current loss 0.323629, current_train_items 236672.
I0302 19:01:06.482548 22895071117440 run.py:483] Algo bellman_ford step 7396 current loss 0.447672, current_train_items 236704.
I0302 19:01:06.504984 22895071117440 run.py:483] Algo bellman_ford step 7397 current loss 0.698446, current_train_items 236736.
I0302 19:01:06.532821 22895071117440 run.py:483] Algo bellman_ford step 7398 current loss 0.559593, current_train_items 236768.
I0302 19:01:06.561982 22895071117440 run.py:483] Algo bellman_ford step 7399 current loss 0.630654, current_train_items 236800.
I0302 19:01:06.579946 22895071117440 run.py:483] Algo bellman_ford step 7400 current loss 0.293618, current_train_items 236832.
I0302 19:01:06.587639 22895071117440 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:06.587746 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:06.604028 22895071117440 run.py:483] Algo bellman_ford step 7401 current loss 0.434143, current_train_items 236864.
I0302 19:01:06.627366 22895071117440 run.py:483] Algo bellman_ford step 7402 current loss 0.550630, current_train_items 236896.
I0302 19:01:06.657180 22895071117440 run.py:483] Algo bellman_ford step 7403 current loss 0.697564, current_train_items 236928.
I0302 19:01:06.689382 22895071117440 run.py:483] Algo bellman_ford step 7404 current loss 0.735163, current_train_items 236960.
I0302 19:01:06.708149 22895071117440 run.py:483] Algo bellman_ford step 7405 current loss 0.357062, current_train_items 236992.
I0302 19:01:06.723197 22895071117440 run.py:483] Algo bellman_ford step 7406 current loss 0.392567, current_train_items 237024.
I0302 19:01:06.745503 22895071117440 run.py:483] Algo bellman_ford step 7407 current loss 0.524181, current_train_items 237056.
I0302 19:01:06.775233 22895071117440 run.py:483] Algo bellman_ford step 7408 current loss 0.633673, current_train_items 237088.
I0302 19:01:06.804569 22895071117440 run.py:483] Algo bellman_ford step 7409 current loss 0.792275, current_train_items 237120.
I0302 19:01:06.822540 22895071117440 run.py:483] Algo bellman_ford step 7410 current loss 0.289007, current_train_items 237152.
I0302 19:01:06.838541 22895071117440 run.py:483] Algo bellman_ford step 7411 current loss 0.471589, current_train_items 237184.
I0302 19:01:06.861759 22895071117440 run.py:483] Algo bellman_ford step 7412 current loss 0.614535, current_train_items 237216.
I0302 19:01:06.891177 22895071117440 run.py:483] Algo bellman_ford step 7413 current loss 0.793308, current_train_items 237248.
I0302 19:01:06.920499 22895071117440 run.py:483] Algo bellman_ford step 7414 current loss 0.760080, current_train_items 237280.
I0302 19:01:06.938637 22895071117440 run.py:483] Algo bellman_ford step 7415 current loss 0.334755, current_train_items 237312.
I0302 19:01:06.953737 22895071117440 run.py:483] Algo bellman_ford step 7416 current loss 0.453071, current_train_items 237344.
I0302 19:01:06.975967 22895071117440 run.py:483] Algo bellman_ford step 7417 current loss 0.673835, current_train_items 237376.
I0302 19:01:07.004628 22895071117440 run.py:483] Algo bellman_ford step 7418 current loss 0.726246, current_train_items 237408.
I0302 19:01:07.034668 22895071117440 run.py:483] Algo bellman_ford step 7419 current loss 0.738737, current_train_items 237440.
I0302 19:01:07.052871 22895071117440 run.py:483] Algo bellman_ford step 7420 current loss 0.305443, current_train_items 237472.
I0302 19:01:07.068067 22895071117440 run.py:483] Algo bellman_ford step 7421 current loss 0.506744, current_train_items 237504.
I0302 19:01:07.090098 22895071117440 run.py:483] Algo bellman_ford step 7422 current loss 0.575546, current_train_items 237536.
I0302 19:01:07.118667 22895071117440 run.py:483] Algo bellman_ford step 7423 current loss 0.662588, current_train_items 237568.
I0302 19:01:07.149171 22895071117440 run.py:483] Algo bellman_ford step 7424 current loss 0.617268, current_train_items 237600.
I0302 19:01:07.167315 22895071117440 run.py:483] Algo bellman_ford step 7425 current loss 0.277633, current_train_items 237632.
I0302 19:01:07.182644 22895071117440 run.py:483] Algo bellman_ford step 7426 current loss 0.414732, current_train_items 237664.
I0302 19:01:07.205627 22895071117440 run.py:483] Algo bellman_ford step 7427 current loss 0.576197, current_train_items 237696.
I0302 19:01:07.234419 22895071117440 run.py:483] Algo bellman_ford step 7428 current loss 0.618850, current_train_items 237728.
I0302 19:01:07.265774 22895071117440 run.py:483] Algo bellman_ford step 7429 current loss 0.925436, current_train_items 237760.
I0302 19:01:07.284179 22895071117440 run.py:483] Algo bellman_ford step 7430 current loss 0.268381, current_train_items 237792.
I0302 19:01:07.299301 22895071117440 run.py:483] Algo bellman_ford step 7431 current loss 0.434238, current_train_items 237824.
I0302 19:01:07.322041 22895071117440 run.py:483] Algo bellman_ford step 7432 current loss 0.711369, current_train_items 237856.
I0302 19:01:07.351579 22895071117440 run.py:483] Algo bellman_ford step 7433 current loss 0.735286, current_train_items 237888.
I0302 19:01:07.380137 22895071117440 run.py:483] Algo bellman_ford step 7434 current loss 0.762056, current_train_items 237920.
I0302 19:01:07.398535 22895071117440 run.py:483] Algo bellman_ford step 7435 current loss 0.361523, current_train_items 237952.
I0302 19:01:07.413845 22895071117440 run.py:483] Algo bellman_ford step 7436 current loss 0.472332, current_train_items 237984.
I0302 19:01:07.436733 22895071117440 run.py:483] Algo bellman_ford step 7437 current loss 0.578117, current_train_items 238016.
I0302 19:01:07.465292 22895071117440 run.py:483] Algo bellman_ford step 7438 current loss 0.670110, current_train_items 238048.
I0302 19:01:07.497299 22895071117440 run.py:483] Algo bellman_ford step 7439 current loss 0.741838, current_train_items 238080.
I0302 19:01:07.515657 22895071117440 run.py:483] Algo bellman_ford step 7440 current loss 0.359919, current_train_items 238112.
I0302 19:01:07.531409 22895071117440 run.py:483] Algo bellman_ford step 7441 current loss 0.561241, current_train_items 238144.
I0302 19:01:07.553113 22895071117440 run.py:483] Algo bellman_ford step 7442 current loss 0.542726, current_train_items 238176.
I0302 19:01:07.582733 22895071117440 run.py:483] Algo bellman_ford step 7443 current loss 0.682401, current_train_items 238208.
I0302 19:01:07.614617 22895071117440 run.py:483] Algo bellman_ford step 7444 current loss 0.677190, current_train_items 238240.
I0302 19:01:07.632566 22895071117440 run.py:483] Algo bellman_ford step 7445 current loss 0.265356, current_train_items 238272.
I0302 19:01:07.647924 22895071117440 run.py:483] Algo bellman_ford step 7446 current loss 0.484421, current_train_items 238304.
I0302 19:01:07.669955 22895071117440 run.py:483] Algo bellman_ford step 7447 current loss 0.539285, current_train_items 238336.
I0302 19:01:07.698600 22895071117440 run.py:483] Algo bellman_ford step 7448 current loss 0.576588, current_train_items 238368.
I0302 19:01:07.731164 22895071117440 run.py:483] Algo bellman_ford step 7449 current loss 0.697009, current_train_items 238400.
I0302 19:01:07.749314 22895071117440 run.py:483] Algo bellman_ford step 7450 current loss 0.289651, current_train_items 238432.
I0302 19:01:07.757288 22895071117440 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:07.757395 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:07.773482 22895071117440 run.py:483] Algo bellman_ford step 7451 current loss 0.443962, current_train_items 238464.
I0302 19:01:07.797107 22895071117440 run.py:483] Algo bellman_ford step 7452 current loss 0.600126, current_train_items 238496.
I0302 19:01:07.825628 22895071117440 run.py:483] Algo bellman_ford step 7453 current loss 0.566991, current_train_items 238528.
I0302 19:01:07.856238 22895071117440 run.py:483] Algo bellman_ford step 7454 current loss 0.899839, current_train_items 238560.
I0302 19:01:07.875036 22895071117440 run.py:483] Algo bellman_ford step 7455 current loss 0.298156, current_train_items 238592.
I0302 19:01:07.890879 22895071117440 run.py:483] Algo bellman_ford step 7456 current loss 0.413550, current_train_items 238624.
I0302 19:01:07.912595 22895071117440 run.py:483] Algo bellman_ford step 7457 current loss 0.573211, current_train_items 238656.
I0302 19:01:07.942032 22895071117440 run.py:483] Algo bellman_ford step 7458 current loss 0.638537, current_train_items 238688.
I0302 19:01:07.972640 22895071117440 run.py:483] Algo bellman_ford step 7459 current loss 0.707911, current_train_items 238720.
I0302 19:01:07.991128 22895071117440 run.py:483] Algo bellman_ford step 7460 current loss 0.206430, current_train_items 238752.
I0302 19:01:08.007439 22895071117440 run.py:483] Algo bellman_ford step 7461 current loss 0.480332, current_train_items 238784.
I0302 19:01:08.028775 22895071117440 run.py:483] Algo bellman_ford step 7462 current loss 0.634256, current_train_items 238816.
I0302 19:01:08.057089 22895071117440 run.py:483] Algo bellman_ford step 7463 current loss 0.644063, current_train_items 238848.
I0302 19:01:08.087733 22895071117440 run.py:483] Algo bellman_ford step 7464 current loss 0.821105, current_train_items 238880.
I0302 19:01:08.105801 22895071117440 run.py:483] Algo bellman_ford step 7465 current loss 0.295655, current_train_items 238912.
I0302 19:01:08.120972 22895071117440 run.py:483] Algo bellman_ford step 7466 current loss 0.402952, current_train_items 238944.
I0302 19:01:08.142611 22895071117440 run.py:483] Algo bellman_ford step 7467 current loss 0.639349, current_train_items 238976.
I0302 19:01:08.172256 22895071117440 run.py:483] Algo bellman_ford step 7468 current loss 0.838315, current_train_items 239008.
I0302 19:01:08.201630 22895071117440 run.py:483] Algo bellman_ford step 7469 current loss 0.900045, current_train_items 239040.
I0302 19:01:08.220503 22895071117440 run.py:483] Algo bellman_ford step 7470 current loss 0.258115, current_train_items 239072.
I0302 19:01:08.236249 22895071117440 run.py:483] Algo bellman_ford step 7471 current loss 0.469590, current_train_items 239104.
I0302 19:01:08.257324 22895071117440 run.py:483] Algo bellman_ford step 7472 current loss 0.653795, current_train_items 239136.
I0302 19:01:08.286186 22895071117440 run.py:483] Algo bellman_ford step 7473 current loss 0.764946, current_train_items 239168.
I0302 19:01:08.318374 22895071117440 run.py:483] Algo bellman_ford step 7474 current loss 0.729951, current_train_items 239200.
I0302 19:01:08.336761 22895071117440 run.py:483] Algo bellman_ford step 7475 current loss 0.253790, current_train_items 239232.
I0302 19:01:08.352632 22895071117440 run.py:483] Algo bellman_ford step 7476 current loss 0.504558, current_train_items 239264.
I0302 19:01:08.373585 22895071117440 run.py:483] Algo bellman_ford step 7477 current loss 0.622755, current_train_items 239296.
I0302 19:01:08.402673 22895071117440 run.py:483] Algo bellman_ford step 7478 current loss 0.756238, current_train_items 239328.
I0302 19:01:08.436625 22895071117440 run.py:483] Algo bellman_ford step 7479 current loss 1.031545, current_train_items 239360.
I0302 19:01:08.454558 22895071117440 run.py:483] Algo bellman_ford step 7480 current loss 0.313445, current_train_items 239392.
I0302 19:01:08.470191 22895071117440 run.py:483] Algo bellman_ford step 7481 current loss 0.465947, current_train_items 239424.
I0302 19:01:08.493399 22895071117440 run.py:483] Algo bellman_ford step 7482 current loss 0.743125, current_train_items 239456.
I0302 19:01:08.523414 22895071117440 run.py:483] Algo bellman_ford step 7483 current loss 0.684246, current_train_items 239488.
I0302 19:01:08.555290 22895071117440 run.py:483] Algo bellman_ford step 7484 current loss 0.820555, current_train_items 239520.
I0302 19:01:08.574034 22895071117440 run.py:483] Algo bellman_ford step 7485 current loss 0.278325, current_train_items 239552.
I0302 19:01:08.589738 22895071117440 run.py:483] Algo bellman_ford step 7486 current loss 0.524660, current_train_items 239584.
I0302 19:01:08.610645 22895071117440 run.py:483] Algo bellman_ford step 7487 current loss 0.496611, current_train_items 239616.
I0302 19:01:08.639964 22895071117440 run.py:483] Algo bellman_ford step 7488 current loss 0.813896, current_train_items 239648.
I0302 19:01:08.672319 22895071117440 run.py:483] Algo bellman_ford step 7489 current loss 0.752836, current_train_items 239680.
I0302 19:01:08.690832 22895071117440 run.py:483] Algo bellman_ford step 7490 current loss 0.355834, current_train_items 239712.
I0302 19:01:08.706236 22895071117440 run.py:483] Algo bellman_ford step 7491 current loss 0.470288, current_train_items 239744.
I0302 19:01:08.727187 22895071117440 run.py:483] Algo bellman_ford step 7492 current loss 0.533679, current_train_items 239776.
I0302 19:01:08.755706 22895071117440 run.py:483] Algo bellman_ford step 7493 current loss 0.620461, current_train_items 239808.
I0302 19:01:08.786345 22895071117440 run.py:483] Algo bellman_ford step 7494 current loss 0.700481, current_train_items 239840.
I0302 19:01:08.804323 22895071117440 run.py:483] Algo bellman_ford step 7495 current loss 0.342032, current_train_items 239872.
I0302 19:01:08.819830 22895071117440 run.py:483] Algo bellman_ford step 7496 current loss 0.450631, current_train_items 239904.
I0302 19:01:08.842038 22895071117440 run.py:483] Algo bellman_ford step 7497 current loss 0.514280, current_train_items 239936.
I0302 19:01:08.870292 22895071117440 run.py:483] Algo bellman_ford step 7498 current loss 0.627672, current_train_items 239968.
I0302 19:01:08.900216 22895071117440 run.py:483] Algo bellman_ford step 7499 current loss 0.685553, current_train_items 240000.
I0302 19:01:08.918689 22895071117440 run.py:483] Algo bellman_ford step 7500 current loss 0.298722, current_train_items 240032.
I0302 19:01:08.926597 22895071117440 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:08.926702 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:08.942536 22895071117440 run.py:483] Algo bellman_ford step 7501 current loss 0.376282, current_train_items 240064.
I0302 19:01:08.964620 22895071117440 run.py:483] Algo bellman_ford step 7502 current loss 0.614613, current_train_items 240096.
I0302 19:01:08.993748 22895071117440 run.py:483] Algo bellman_ford step 7503 current loss 0.637943, current_train_items 240128.
I0302 19:01:09.025055 22895071117440 run.py:483] Algo bellman_ford step 7504 current loss 0.684249, current_train_items 240160.
I0302 19:01:09.043582 22895071117440 run.py:483] Algo bellman_ford step 7505 current loss 0.289423, current_train_items 240192.
I0302 19:01:09.058984 22895071117440 run.py:483] Algo bellman_ford step 7506 current loss 0.462045, current_train_items 240224.
I0302 19:01:09.080485 22895071117440 run.py:483] Algo bellman_ford step 7507 current loss 0.519908, current_train_items 240256.
I0302 19:01:09.109160 22895071117440 run.py:483] Algo bellman_ford step 7508 current loss 0.614992, current_train_items 240288.
I0302 19:01:09.139544 22895071117440 run.py:483] Algo bellman_ford step 7509 current loss 0.691769, current_train_items 240320.
I0302 19:01:09.157824 22895071117440 run.py:483] Algo bellman_ford step 7510 current loss 0.230943, current_train_items 240352.
I0302 19:01:09.173611 22895071117440 run.py:483] Algo bellman_ford step 7511 current loss 0.522505, current_train_items 240384.
I0302 19:01:09.194555 22895071117440 run.py:483] Algo bellman_ford step 7512 current loss 0.591077, current_train_items 240416.
I0302 19:01:09.223455 22895071117440 run.py:483] Algo bellman_ford step 7513 current loss 0.676096, current_train_items 240448.
I0302 19:01:09.253329 22895071117440 run.py:483] Algo bellman_ford step 7514 current loss 0.709941, current_train_items 240480.
I0302 19:01:09.271336 22895071117440 run.py:483] Algo bellman_ford step 7515 current loss 0.349685, current_train_items 240512.
I0302 19:01:09.286089 22895071117440 run.py:483] Algo bellman_ford step 7516 current loss 0.506162, current_train_items 240544.
I0302 19:01:09.308314 22895071117440 run.py:483] Algo bellman_ford step 7517 current loss 0.603876, current_train_items 240576.
I0302 19:01:09.337414 22895071117440 run.py:483] Algo bellman_ford step 7518 current loss 0.699170, current_train_items 240608.
I0302 19:01:09.368532 22895071117440 run.py:483] Algo bellman_ford step 7519 current loss 0.865401, current_train_items 240640.
I0302 19:01:09.386425 22895071117440 run.py:483] Algo bellman_ford step 7520 current loss 0.282825, current_train_items 240672.
I0302 19:01:09.401837 22895071117440 run.py:483] Algo bellman_ford step 7521 current loss 0.451191, current_train_items 240704.
I0302 19:01:09.423879 22895071117440 run.py:483] Algo bellman_ford step 7522 current loss 0.585424, current_train_items 240736.
I0302 19:01:09.452950 22895071117440 run.py:483] Algo bellman_ford step 7523 current loss 0.683374, current_train_items 240768.
I0302 19:01:09.481656 22895071117440 run.py:483] Algo bellman_ford step 7524 current loss 0.602108, current_train_items 240800.
I0302 19:01:09.499438 22895071117440 run.py:483] Algo bellman_ford step 7525 current loss 0.199806, current_train_items 240832.
I0302 19:01:09.514685 22895071117440 run.py:483] Algo bellman_ford step 7526 current loss 0.455674, current_train_items 240864.
I0302 19:01:09.537442 22895071117440 run.py:483] Algo bellman_ford step 7527 current loss 0.725963, current_train_items 240896.
I0302 19:01:09.565408 22895071117440 run.py:483] Algo bellman_ford step 7528 current loss 0.773405, current_train_items 240928.
I0302 19:01:09.596140 22895071117440 run.py:483] Algo bellman_ford step 7529 current loss 0.798109, current_train_items 240960.
I0302 19:01:09.614390 22895071117440 run.py:483] Algo bellman_ford step 7530 current loss 0.295149, current_train_items 240992.
I0302 19:01:09.629651 22895071117440 run.py:483] Algo bellman_ford step 7531 current loss 0.488290, current_train_items 241024.
I0302 19:01:09.650694 22895071117440 run.py:483] Algo bellman_ford step 7532 current loss 0.650211, current_train_items 241056.
I0302 19:01:09.680655 22895071117440 run.py:483] Algo bellman_ford step 7533 current loss 0.728764, current_train_items 241088.
I0302 19:01:09.712278 22895071117440 run.py:483] Algo bellman_ford step 7534 current loss 0.786362, current_train_items 241120.
I0302 19:01:09.730411 22895071117440 run.py:483] Algo bellman_ford step 7535 current loss 0.276577, current_train_items 241152.
I0302 19:01:09.745888 22895071117440 run.py:483] Algo bellman_ford step 7536 current loss 0.477449, current_train_items 241184.
I0302 19:01:09.767651 22895071117440 run.py:483] Algo bellman_ford step 7537 current loss 0.667434, current_train_items 241216.
I0302 19:01:09.797230 22895071117440 run.py:483] Algo bellman_ford step 7538 current loss 0.784490, current_train_items 241248.
I0302 19:01:09.827529 22895071117440 run.py:483] Algo bellman_ford step 7539 current loss 0.796503, current_train_items 241280.
I0302 19:01:09.845824 22895071117440 run.py:483] Algo bellman_ford step 7540 current loss 0.253106, current_train_items 241312.
I0302 19:01:09.861619 22895071117440 run.py:483] Algo bellman_ford step 7541 current loss 0.528527, current_train_items 241344.
I0302 19:01:09.884798 22895071117440 run.py:483] Algo bellman_ford step 7542 current loss 0.665113, current_train_items 241376.
I0302 19:01:09.914524 22895071117440 run.py:483] Algo bellman_ford step 7543 current loss 0.778537, current_train_items 241408.
I0302 19:01:09.945835 22895071117440 run.py:483] Algo bellman_ford step 7544 current loss 0.702555, current_train_items 241440.
I0302 19:01:09.963778 22895071117440 run.py:483] Algo bellman_ford step 7545 current loss 0.232240, current_train_items 241472.
I0302 19:01:09.979361 22895071117440 run.py:483] Algo bellman_ford step 7546 current loss 0.543159, current_train_items 241504.
I0302 19:01:10.002287 22895071117440 run.py:483] Algo bellman_ford step 7547 current loss 0.719952, current_train_items 241536.
I0302 19:01:10.031830 22895071117440 run.py:483] Algo bellman_ford step 7548 current loss 0.762141, current_train_items 241568.
I0302 19:01:10.063050 22895071117440 run.py:483] Algo bellman_ford step 7549 current loss 0.782779, current_train_items 241600.
I0302 19:01:10.081537 22895071117440 run.py:483] Algo bellman_ford step 7550 current loss 0.287799, current_train_items 241632.
I0302 19:01:10.089586 22895071117440 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:10.089698 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:10.106335 22895071117440 run.py:483] Algo bellman_ford step 7551 current loss 0.517809, current_train_items 241664.
I0302 19:01:10.128777 22895071117440 run.py:483] Algo bellman_ford step 7552 current loss 0.568264, current_train_items 241696.
I0302 19:01:10.158698 22895071117440 run.py:483] Algo bellman_ford step 7553 current loss 0.672354, current_train_items 241728.
I0302 19:01:10.190269 22895071117440 run.py:483] Algo bellman_ford step 7554 current loss 0.743016, current_train_items 241760.
I0302 19:01:10.208639 22895071117440 run.py:483] Algo bellman_ford step 7555 current loss 0.257761, current_train_items 241792.
I0302 19:01:10.224208 22895071117440 run.py:483] Algo bellman_ford step 7556 current loss 0.470218, current_train_items 241824.
I0302 19:01:10.246339 22895071117440 run.py:483] Algo bellman_ford step 7557 current loss 0.550476, current_train_items 241856.
I0302 19:01:10.276256 22895071117440 run.py:483] Algo bellman_ford step 7558 current loss 0.699311, current_train_items 241888.
I0302 19:01:10.306305 22895071117440 run.py:483] Algo bellman_ford step 7559 current loss 0.709504, current_train_items 241920.
I0302 19:01:10.324491 22895071117440 run.py:483] Algo bellman_ford step 7560 current loss 0.310307, current_train_items 241952.
I0302 19:01:10.340985 22895071117440 run.py:483] Algo bellman_ford step 7561 current loss 0.456375, current_train_items 241984.
I0302 19:01:10.362226 22895071117440 run.py:483] Algo bellman_ford step 7562 current loss 0.587333, current_train_items 242016.
I0302 19:01:10.391264 22895071117440 run.py:483] Algo bellman_ford step 7563 current loss 0.574468, current_train_items 242048.
I0302 19:01:10.422406 22895071117440 run.py:483] Algo bellman_ford step 7564 current loss 0.674845, current_train_items 242080.
I0302 19:01:10.440389 22895071117440 run.py:483] Algo bellman_ford step 7565 current loss 0.321877, current_train_items 242112.
I0302 19:01:10.455550 22895071117440 run.py:483] Algo bellman_ford step 7566 current loss 0.492162, current_train_items 242144.
I0302 19:01:10.477719 22895071117440 run.py:483] Algo bellman_ford step 7567 current loss 0.546587, current_train_items 242176.
I0302 19:01:10.505914 22895071117440 run.py:483] Algo bellman_ford step 7568 current loss 0.656542, current_train_items 242208.
I0302 19:01:10.536134 22895071117440 run.py:483] Algo bellman_ford step 7569 current loss 0.766747, current_train_items 242240.
I0302 19:01:10.554888 22895071117440 run.py:483] Algo bellman_ford step 7570 current loss 0.313472, current_train_items 242272.
I0302 19:01:10.570832 22895071117440 run.py:483] Algo bellman_ford step 7571 current loss 0.429861, current_train_items 242304.
I0302 19:01:10.592327 22895071117440 run.py:483] Algo bellman_ford step 7572 current loss 0.612998, current_train_items 242336.
I0302 19:01:10.620193 22895071117440 run.py:483] Algo bellman_ford step 7573 current loss 0.582903, current_train_items 242368.
I0302 19:01:10.648857 22895071117440 run.py:483] Algo bellman_ford step 7574 current loss 0.617975, current_train_items 242400.
I0302 19:01:10.667193 22895071117440 run.py:483] Algo bellman_ford step 7575 current loss 0.233304, current_train_items 242432.
I0302 19:01:10.683505 22895071117440 run.py:483] Algo bellman_ford step 7576 current loss 0.476236, current_train_items 242464.
I0302 19:01:10.704600 22895071117440 run.py:483] Algo bellman_ford step 7577 current loss 0.659292, current_train_items 242496.
I0302 19:01:10.733519 22895071117440 run.py:483] Algo bellman_ford step 7578 current loss 0.718960, current_train_items 242528.
I0302 19:01:10.765999 22895071117440 run.py:483] Algo bellman_ford step 7579 current loss 0.840611, current_train_items 242560.
I0302 19:01:10.784096 22895071117440 run.py:483] Algo bellman_ford step 7580 current loss 0.290266, current_train_items 242592.
I0302 19:01:10.799726 22895071117440 run.py:483] Algo bellman_ford step 7581 current loss 0.587451, current_train_items 242624.
I0302 19:01:10.822130 22895071117440 run.py:483] Algo bellman_ford step 7582 current loss 0.564377, current_train_items 242656.
I0302 19:01:10.850714 22895071117440 run.py:483] Algo bellman_ford step 7583 current loss 0.637863, current_train_items 242688.
I0302 19:01:10.880661 22895071117440 run.py:483] Algo bellman_ford step 7584 current loss 0.686804, current_train_items 242720.
I0302 19:01:10.899346 22895071117440 run.py:483] Algo bellman_ford step 7585 current loss 0.219008, current_train_items 242752.
I0302 19:01:10.914841 22895071117440 run.py:483] Algo bellman_ford step 7586 current loss 0.503025, current_train_items 242784.
I0302 19:01:10.936399 22895071117440 run.py:483] Algo bellman_ford step 7587 current loss 0.775780, current_train_items 242816.
I0302 19:01:10.962864 22895071117440 run.py:483] Algo bellman_ford step 7588 current loss 0.671230, current_train_items 242848.
I0302 19:01:10.994039 22895071117440 run.py:483] Algo bellman_ford step 7589 current loss 0.978409, current_train_items 242880.
I0302 19:01:11.012527 22895071117440 run.py:483] Algo bellman_ford step 7590 current loss 0.256630, current_train_items 242912.
I0302 19:01:11.027946 22895071117440 run.py:483] Algo bellman_ford step 7591 current loss 0.451156, current_train_items 242944.
I0302 19:01:11.050590 22895071117440 run.py:483] Algo bellman_ford step 7592 current loss 0.694978, current_train_items 242976.
I0302 19:01:11.078065 22895071117440 run.py:483] Algo bellman_ford step 7593 current loss 0.867122, current_train_items 243008.
I0302 19:01:11.108884 22895071117440 run.py:483] Algo bellman_ford step 7594 current loss 0.836205, current_train_items 243040.
I0302 19:01:11.126959 22895071117440 run.py:483] Algo bellman_ford step 7595 current loss 0.316851, current_train_items 243072.
I0302 19:01:11.142187 22895071117440 run.py:483] Algo bellman_ford step 7596 current loss 0.469522, current_train_items 243104.
I0302 19:01:11.164805 22895071117440 run.py:483] Algo bellman_ford step 7597 current loss 0.641011, current_train_items 243136.
I0302 19:01:11.194263 22895071117440 run.py:483] Algo bellman_ford step 7598 current loss 0.654667, current_train_items 243168.
I0302 19:01:11.222569 22895071117440 run.py:483] Algo bellman_ford step 7599 current loss 0.798521, current_train_items 243200.
I0302 19:01:11.241154 22895071117440 run.py:483] Algo bellman_ford step 7600 current loss 0.293764, current_train_items 243232.
I0302 19:01:11.248988 22895071117440 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:01:11.249094 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:11.265265 22895071117440 run.py:483] Algo bellman_ford step 7601 current loss 0.519192, current_train_items 243264.
I0302 19:01:11.289275 22895071117440 run.py:483] Algo bellman_ford step 7602 current loss 0.747838, current_train_items 243296.
I0302 19:01:11.318202 22895071117440 run.py:483] Algo bellman_ford step 7603 current loss 0.706762, current_train_items 243328.
I0302 19:01:11.350474 22895071117440 run.py:483] Algo bellman_ford step 7604 current loss 0.818172, current_train_items 243360.
I0302 19:01:11.369062 22895071117440 run.py:483] Algo bellman_ford step 7605 current loss 0.279439, current_train_items 243392.
I0302 19:01:11.383803 22895071117440 run.py:483] Algo bellman_ford step 7606 current loss 0.445051, current_train_items 243424.
I0302 19:01:11.405644 22895071117440 run.py:483] Algo bellman_ford step 7607 current loss 0.535772, current_train_items 243456.
I0302 19:01:11.435142 22895071117440 run.py:483] Algo bellman_ford step 7608 current loss 0.764948, current_train_items 243488.
I0302 19:01:11.467121 22895071117440 run.py:483] Algo bellman_ford step 7609 current loss 1.066616, current_train_items 243520.
I0302 19:01:11.485173 22895071117440 run.py:483] Algo bellman_ford step 7610 current loss 0.245281, current_train_items 243552.
I0302 19:01:11.501007 22895071117440 run.py:483] Algo bellman_ford step 7611 current loss 0.529803, current_train_items 243584.
I0302 19:01:11.523205 22895071117440 run.py:483] Algo bellman_ford step 7612 current loss 0.629352, current_train_items 243616.
I0302 19:01:11.549951 22895071117440 run.py:483] Algo bellman_ford step 7613 current loss 0.622541, current_train_items 243648.
I0302 19:01:11.580444 22895071117440 run.py:483] Algo bellman_ford step 7614 current loss 0.946878, current_train_items 243680.
I0302 19:01:11.598522 22895071117440 run.py:483] Algo bellman_ford step 7615 current loss 0.277349, current_train_items 243712.
I0302 19:01:11.614010 22895071117440 run.py:483] Algo bellman_ford step 7616 current loss 0.495709, current_train_items 243744.
I0302 19:01:11.635786 22895071117440 run.py:483] Algo bellman_ford step 7617 current loss 0.637727, current_train_items 243776.
I0302 19:01:11.663963 22895071117440 run.py:483] Algo bellman_ford step 7618 current loss 0.697136, current_train_items 243808.
I0302 19:01:11.695392 22895071117440 run.py:483] Algo bellman_ford step 7619 current loss 0.987030, current_train_items 243840.
I0302 19:01:11.713615 22895071117440 run.py:483] Algo bellman_ford step 7620 current loss 0.367205, current_train_items 243872.
I0302 19:01:11.729095 22895071117440 run.py:483] Algo bellman_ford step 7621 current loss 0.477530, current_train_items 243904.
I0302 19:01:11.751207 22895071117440 run.py:483] Algo bellman_ford step 7622 current loss 0.557744, current_train_items 243936.
I0302 19:01:11.781272 22895071117440 run.py:483] Algo bellman_ford step 7623 current loss 0.631600, current_train_items 243968.
I0302 19:01:11.811421 22895071117440 run.py:483] Algo bellman_ford step 7624 current loss 0.754699, current_train_items 244000.
I0302 19:01:11.829415 22895071117440 run.py:483] Algo bellman_ford step 7625 current loss 0.281512, current_train_items 244032.
I0302 19:01:11.844153 22895071117440 run.py:483] Algo bellman_ford step 7626 current loss 0.404172, current_train_items 244064.
I0302 19:01:11.866670 22895071117440 run.py:483] Algo bellman_ford step 7627 current loss 0.727061, current_train_items 244096.
I0302 19:01:11.896811 22895071117440 run.py:483] Algo bellman_ford step 7628 current loss 0.634771, current_train_items 244128.
I0302 19:01:11.927007 22895071117440 run.py:483] Algo bellman_ford step 7629 current loss 0.722117, current_train_items 244160.
I0302 19:01:11.944877 22895071117440 run.py:483] Algo bellman_ford step 7630 current loss 0.250786, current_train_items 244192.
I0302 19:01:11.960593 22895071117440 run.py:483] Algo bellman_ford step 7631 current loss 0.567032, current_train_items 244224.
I0302 19:01:11.982569 22895071117440 run.py:483] Algo bellman_ford step 7632 current loss 0.611114, current_train_items 244256.
I0302 19:01:12.012129 22895071117440 run.py:483] Algo bellman_ford step 7633 current loss 0.707505, current_train_items 244288.
I0302 19:01:12.043588 22895071117440 run.py:483] Algo bellman_ford step 7634 current loss 0.709904, current_train_items 244320.
I0302 19:01:12.061594 22895071117440 run.py:483] Algo bellman_ford step 7635 current loss 0.245522, current_train_items 244352.
I0302 19:01:12.076916 22895071117440 run.py:483] Algo bellman_ford step 7636 current loss 0.491515, current_train_items 244384.
I0302 19:01:12.099189 22895071117440 run.py:483] Algo bellman_ford step 7637 current loss 0.597210, current_train_items 244416.
I0302 19:01:12.126994 22895071117440 run.py:483] Algo bellman_ford step 7638 current loss 0.632342, current_train_items 244448.
I0302 19:01:12.160123 22895071117440 run.py:483] Algo bellman_ford step 7639 current loss 0.749038, current_train_items 244480.
I0302 19:01:12.178025 22895071117440 run.py:483] Algo bellman_ford step 7640 current loss 0.290443, current_train_items 244512.
I0302 19:01:12.193208 22895071117440 run.py:483] Algo bellman_ford step 7641 current loss 0.533689, current_train_items 244544.
I0302 19:01:12.214465 22895071117440 run.py:483] Algo bellman_ford step 7642 current loss 0.604842, current_train_items 244576.
I0302 19:01:12.245838 22895071117440 run.py:483] Algo bellman_ford step 7643 current loss 0.712891, current_train_items 244608.
I0302 19:01:12.278282 22895071117440 run.py:483] Algo bellman_ford step 7644 current loss 0.702693, current_train_items 244640.
I0302 19:01:12.296449 22895071117440 run.py:483] Algo bellman_ford step 7645 current loss 0.235041, current_train_items 244672.
I0302 19:01:12.311959 22895071117440 run.py:483] Algo bellman_ford step 7646 current loss 0.547919, current_train_items 244704.
I0302 19:01:12.334789 22895071117440 run.py:483] Algo bellman_ford step 7647 current loss 0.562842, current_train_items 244736.
I0302 19:01:12.365228 22895071117440 run.py:483] Algo bellman_ford step 7648 current loss 0.653812, current_train_items 244768.
I0302 19:01:12.398170 22895071117440 run.py:483] Algo bellman_ford step 7649 current loss 0.908164, current_train_items 244800.
I0302 19:01:12.416360 22895071117440 run.py:483] Algo bellman_ford step 7650 current loss 0.359620, current_train_items 244832.
I0302 19:01:12.424479 22895071117440 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:01:12.424587 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:12.440372 22895071117440 run.py:483] Algo bellman_ford step 7651 current loss 0.375310, current_train_items 244864.
I0302 19:01:12.463571 22895071117440 run.py:483] Algo bellman_ford step 7652 current loss 0.673559, current_train_items 244896.
I0302 19:01:12.492693 22895071117440 run.py:483] Algo bellman_ford step 7653 current loss 0.559730, current_train_items 244928.
I0302 19:01:12.524008 22895071117440 run.py:483] Algo bellman_ford step 7654 current loss 0.749479, current_train_items 244960.
I0302 19:01:12.542475 22895071117440 run.py:483] Algo bellman_ford step 7655 current loss 0.212956, current_train_items 244992.
I0302 19:01:12.557786 22895071117440 run.py:483] Algo bellman_ford step 7656 current loss 0.458367, current_train_items 245024.
I0302 19:01:12.580173 22895071117440 run.py:483] Algo bellman_ford step 7657 current loss 0.698090, current_train_items 245056.
I0302 19:01:12.609678 22895071117440 run.py:483] Algo bellman_ford step 7658 current loss 0.710368, current_train_items 245088.
I0302 19:01:12.641894 22895071117440 run.py:483] Algo bellman_ford step 7659 current loss 1.199334, current_train_items 245120.
I0302 19:01:12.660344 22895071117440 run.py:483] Algo bellman_ford step 7660 current loss 0.342806, current_train_items 245152.
I0302 19:01:12.676092 22895071117440 run.py:483] Algo bellman_ford step 7661 current loss 0.446283, current_train_items 245184.
I0302 19:01:12.697718 22895071117440 run.py:483] Algo bellman_ford step 7662 current loss 0.711367, current_train_items 245216.
I0302 19:01:12.725944 22895071117440 run.py:483] Algo bellman_ford step 7663 current loss 0.850825, current_train_items 245248.
I0302 19:01:12.756710 22895071117440 run.py:483] Algo bellman_ford step 7664 current loss 0.888137, current_train_items 245280.
I0302 19:01:12.774704 22895071117440 run.py:483] Algo bellman_ford step 7665 current loss 0.344711, current_train_items 245312.
I0302 19:01:12.790115 22895071117440 run.py:483] Algo bellman_ford step 7666 current loss 0.476996, current_train_items 245344.
I0302 19:01:12.811697 22895071117440 run.py:483] Algo bellman_ford step 7667 current loss 0.671144, current_train_items 245376.
I0302 19:01:12.842748 22895071117440 run.py:483] Algo bellman_ford step 7668 current loss 0.635683, current_train_items 245408.
I0302 19:01:12.874394 22895071117440 run.py:483] Algo bellman_ford step 7669 current loss 0.706829, current_train_items 245440.
I0302 19:01:12.892910 22895071117440 run.py:483] Algo bellman_ford step 7670 current loss 0.247525, current_train_items 245472.
I0302 19:01:12.909065 22895071117440 run.py:483] Algo bellman_ford step 7671 current loss 0.479797, current_train_items 245504.
I0302 19:01:12.930437 22895071117440 run.py:483] Algo bellman_ford step 7672 current loss 0.685399, current_train_items 245536.
I0302 19:01:12.959852 22895071117440 run.py:483] Algo bellman_ford step 7673 current loss 0.838501, current_train_items 245568.
I0302 19:01:12.991039 22895071117440 run.py:483] Algo bellman_ford step 7674 current loss 0.769393, current_train_items 245600.
I0302 19:01:13.009164 22895071117440 run.py:483] Algo bellman_ford step 7675 current loss 0.273909, current_train_items 245632.
I0302 19:01:13.025371 22895071117440 run.py:483] Algo bellman_ford step 7676 current loss 0.527078, current_train_items 245664.
I0302 19:01:13.047616 22895071117440 run.py:483] Algo bellman_ford step 7677 current loss 0.665956, current_train_items 245696.
I0302 19:01:13.076020 22895071117440 run.py:483] Algo bellman_ford step 7678 current loss 0.667537, current_train_items 245728.
I0302 19:01:13.108354 22895071117440 run.py:483] Algo bellman_ford step 7679 current loss 0.938062, current_train_items 245760.
I0302 19:01:13.126349 22895071117440 run.py:483] Algo bellman_ford step 7680 current loss 0.296996, current_train_items 245792.
I0302 19:01:13.141921 22895071117440 run.py:483] Algo bellman_ford step 7681 current loss 0.559278, current_train_items 245824.
I0302 19:01:13.163506 22895071117440 run.py:483] Algo bellman_ford step 7682 current loss 0.623370, current_train_items 245856.
I0302 19:01:13.192548 22895071117440 run.py:483] Algo bellman_ford step 7683 current loss 0.738529, current_train_items 245888.
I0302 19:01:13.224291 22895071117440 run.py:483] Algo bellman_ford step 7684 current loss 0.857054, current_train_items 245920.
I0302 19:01:13.243000 22895071117440 run.py:483] Algo bellman_ford step 7685 current loss 0.287843, current_train_items 245952.
I0302 19:01:13.258980 22895071117440 run.py:483] Algo bellman_ford step 7686 current loss 0.528769, current_train_items 245984.
I0302 19:01:13.280018 22895071117440 run.py:483] Algo bellman_ford step 7687 current loss 0.632335, current_train_items 246016.
I0302 19:01:13.309691 22895071117440 run.py:483] Algo bellman_ford step 7688 current loss 0.620498, current_train_items 246048.
I0302 19:01:13.341033 22895071117440 run.py:483] Algo bellman_ford step 7689 current loss 0.775118, current_train_items 246080.
I0302 19:01:13.359277 22895071117440 run.py:483] Algo bellman_ford step 7690 current loss 0.285781, current_train_items 246112.
I0302 19:01:13.375052 22895071117440 run.py:483] Algo bellman_ford step 7691 current loss 0.453564, current_train_items 246144.
I0302 19:01:13.396431 22895071117440 run.py:483] Algo bellman_ford step 7692 current loss 0.590970, current_train_items 246176.
I0302 19:01:13.426987 22895071117440 run.py:483] Algo bellman_ford step 7693 current loss 0.688890, current_train_items 246208.
I0302 19:01:13.457381 22895071117440 run.py:483] Algo bellman_ford step 7694 current loss 0.690105, current_train_items 246240.
I0302 19:01:13.475652 22895071117440 run.py:483] Algo bellman_ford step 7695 current loss 0.257074, current_train_items 246272.
I0302 19:01:13.491262 22895071117440 run.py:483] Algo bellman_ford step 7696 current loss 0.443071, current_train_items 246304.
I0302 19:01:13.513694 22895071117440 run.py:483] Algo bellman_ford step 7697 current loss 0.560858, current_train_items 246336.
I0302 19:01:13.543814 22895071117440 run.py:483] Algo bellman_ford step 7698 current loss 0.683958, current_train_items 246368.
I0302 19:01:13.576491 22895071117440 run.py:483] Algo bellman_ford step 7699 current loss 0.789533, current_train_items 246400.
I0302 19:01:13.595183 22895071117440 run.py:483] Algo bellman_ford step 7700 current loss 0.353900, current_train_items 246432.
I0302 19:01:13.602877 22895071117440 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:01:13.602990 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:01:13.619446 22895071117440 run.py:483] Algo bellman_ford step 7701 current loss 0.474998, current_train_items 246464.
I0302 19:01:13.641761 22895071117440 run.py:483] Algo bellman_ford step 7702 current loss 0.557344, current_train_items 246496.
I0302 19:01:13.670295 22895071117440 run.py:483] Algo bellman_ford step 7703 current loss 0.538116, current_train_items 246528.
I0302 19:01:13.701350 22895071117440 run.py:483] Algo bellman_ford step 7704 current loss 0.721036, current_train_items 246560.
I0302 19:01:13.719942 22895071117440 run.py:483] Algo bellman_ford step 7705 current loss 0.246223, current_train_items 246592.
I0302 19:01:13.735340 22895071117440 run.py:483] Algo bellman_ford step 7706 current loss 0.431349, current_train_items 246624.
I0302 19:01:13.758475 22895071117440 run.py:483] Algo bellman_ford step 7707 current loss 0.588350, current_train_items 246656.
I0302 19:01:13.786829 22895071117440 run.py:483] Algo bellman_ford step 7708 current loss 0.605402, current_train_items 246688.
I0302 19:01:13.816654 22895071117440 run.py:483] Algo bellman_ford step 7709 current loss 0.702730, current_train_items 246720.
I0302 19:01:13.834983 22895071117440 run.py:483] Algo bellman_ford step 7710 current loss 0.318471, current_train_items 246752.
I0302 19:01:13.850495 22895071117440 run.py:483] Algo bellman_ford step 7711 current loss 0.412863, current_train_items 246784.
I0302 19:01:13.872664 22895071117440 run.py:483] Algo bellman_ford step 7712 current loss 0.650349, current_train_items 246816.
I0302 19:01:13.900755 22895071117440 run.py:483] Algo bellman_ford step 7713 current loss 0.728590, current_train_items 246848.
I0302 19:01:13.931606 22895071117440 run.py:483] Algo bellman_ford step 7714 current loss 0.678150, current_train_items 246880.
I0302 19:01:13.949734 22895071117440 run.py:483] Algo bellman_ford step 7715 current loss 0.321346, current_train_items 246912.
I0302 19:01:13.964996 22895071117440 run.py:483] Algo bellman_ford step 7716 current loss 0.407713, current_train_items 246944.
I0302 19:01:13.987388 22895071117440 run.py:483] Algo bellman_ford step 7717 current loss 0.605012, current_train_items 246976.
I0302 19:01:14.016121 22895071117440 run.py:483] Algo bellman_ford step 7718 current loss 0.619337, current_train_items 247008.
I0302 19:01:14.045910 22895071117440 run.py:483] Algo bellman_ford step 7719 current loss 0.646980, current_train_items 247040.
I0302 19:01:14.064133 22895071117440 run.py:483] Algo bellman_ford step 7720 current loss 0.323324, current_train_items 247072.
I0302 19:01:14.079347 22895071117440 run.py:483] Algo bellman_ford step 7721 current loss 0.443311, current_train_items 247104.
I0302 19:01:14.102096 22895071117440 run.py:483] Algo bellman_ford step 7722 current loss 0.720590, current_train_items 247136.
I0302 19:01:14.132148 22895071117440 run.py:483] Algo bellman_ford step 7723 current loss 0.760253, current_train_items 247168.
I0302 19:01:14.164026 22895071117440 run.py:483] Algo bellman_ford step 7724 current loss 0.974892, current_train_items 247200.
I0302 19:01:14.182384 22895071117440 run.py:483] Algo bellman_ford step 7725 current loss 0.287479, current_train_items 247232.
I0302 19:01:14.197736 22895071117440 run.py:483] Algo bellman_ford step 7726 current loss 0.469530, current_train_items 247264.
I0302 19:01:14.218546 22895071117440 run.py:483] Algo bellman_ford step 7727 current loss 0.620669, current_train_items 247296.
I0302 19:01:14.247465 22895071117440 run.py:483] Algo bellman_ford step 7728 current loss 0.636868, current_train_items 247328.
I0302 19:01:14.277573 22895071117440 run.py:483] Algo bellman_ford step 7729 current loss 0.680557, current_train_items 247360.
I0302 19:01:14.295701 22895071117440 run.py:483] Algo bellman_ford step 7730 current loss 0.266472, current_train_items 247392.
I0302 19:01:14.310849 22895071117440 run.py:483] Algo bellman_ford step 7731 current loss 0.434046, current_train_items 247424.
I0302 19:01:14.332372 22895071117440 run.py:483] Algo bellman_ford step 7732 current loss 0.560499, current_train_items 247456.
I0302 19:01:14.361608 22895071117440 run.py:483] Algo bellman_ford step 7733 current loss 0.683601, current_train_items 247488.
I0302 19:01:14.391123 22895071117440 run.py:483] Algo bellman_ford step 7734 current loss 0.680697, current_train_items 247520.
I0302 19:01:14.409358 22895071117440 run.py:483] Algo bellman_ford step 7735 current loss 0.274471, current_train_items 247552.
I0302 19:01:14.424760 22895071117440 run.py:483] Algo bellman_ford step 7736 current loss 0.544181, current_train_items 247584.
I0302 19:01:14.446962 22895071117440 run.py:483] Algo bellman_ford step 7737 current loss 0.576187, current_train_items 247616.
I0302 19:01:14.475595 22895071117440 run.py:483] Algo bellman_ford step 7738 current loss 0.730546, current_train_items 247648.
I0302 19:01:14.506824 22895071117440 run.py:483] Algo bellman_ford step 7739 current loss 0.738393, current_train_items 247680.
I0302 19:01:14.524567 22895071117440 run.py:483] Algo bellman_ford step 7740 current loss 0.205800, current_train_items 247712.
I0302 19:01:14.540230 22895071117440 run.py:483] Algo bellman_ford step 7741 current loss 0.436739, current_train_items 247744.
I0302 19:01:14.563352 22895071117440 run.py:483] Algo bellman_ford step 7742 current loss 0.700778, current_train_items 247776.
I0302 19:01:14.592920 22895071117440 run.py:483] Algo bellman_ford step 7743 current loss 0.775428, current_train_items 247808.
I0302 19:01:14.624252 22895071117440 run.py:483] Algo bellman_ford step 7744 current loss 0.776192, current_train_items 247840.
I0302 19:01:14.642464 22895071117440 run.py:483] Algo bellman_ford step 7745 current loss 0.281086, current_train_items 247872.
I0302 19:01:14.657640 22895071117440 run.py:483] Algo bellman_ford step 7746 current loss 0.387057, current_train_items 247904.
I0302 19:01:14.679476 22895071117440 run.py:483] Algo bellman_ford step 7747 current loss 0.607596, current_train_items 247936.
I0302 19:01:14.709985 22895071117440 run.py:483] Algo bellman_ford step 7748 current loss 0.688973, current_train_items 247968.
I0302 19:01:14.741751 22895071117440 run.py:483] Algo bellman_ford step 7749 current loss 0.658711, current_train_items 248000.
I0302 19:01:14.760092 22895071117440 run.py:483] Algo bellman_ford step 7750 current loss 0.321453, current_train_items 248032.
I0302 19:01:14.767987 22895071117440 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.94140625, 'score': 0.94140625, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:01:14.768094 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.941, val scores are: bellman_ford: 0.941
I0302 19:01:14.784564 22895071117440 run.py:483] Algo bellman_ford step 7751 current loss 0.476363, current_train_items 248064.
I0302 19:01:14.807087 22895071117440 run.py:483] Algo bellman_ford step 7752 current loss 0.615879, current_train_items 248096.
I0302 19:01:14.836146 22895071117440 run.py:483] Algo bellman_ford step 7753 current loss 0.602732, current_train_items 248128.
I0302 19:01:14.869148 22895071117440 run.py:483] Algo bellman_ford step 7754 current loss 0.862613, current_train_items 248160.
I0302 19:01:14.887934 22895071117440 run.py:483] Algo bellman_ford step 7755 current loss 0.312461, current_train_items 248192.
I0302 19:01:14.902884 22895071117440 run.py:483] Algo bellman_ford step 7756 current loss 0.438398, current_train_items 248224.
I0302 19:01:14.924958 22895071117440 run.py:483] Algo bellman_ford step 7757 current loss 0.565762, current_train_items 248256.
I0302 19:01:14.952697 22895071117440 run.py:483] Algo bellman_ford step 7758 current loss 0.665286, current_train_items 248288.
I0302 19:01:14.983160 22895071117440 run.py:483] Algo bellman_ford step 7759 current loss 0.752266, current_train_items 248320.
I0302 19:01:15.001478 22895071117440 run.py:483] Algo bellman_ford step 7760 current loss 0.296776, current_train_items 248352.
I0302 19:01:15.017185 22895071117440 run.py:483] Algo bellman_ford step 7761 current loss 0.538838, current_train_items 248384.
I0302 19:01:15.039547 22895071117440 run.py:483] Algo bellman_ford step 7762 current loss 0.568138, current_train_items 248416.
I0302 19:01:15.068357 22895071117440 run.py:483] Algo bellman_ford step 7763 current loss 0.722910, current_train_items 248448.
I0302 19:01:15.098308 22895071117440 run.py:483] Algo bellman_ford step 7764 current loss 0.670946, current_train_items 248480.
I0302 19:01:15.116316 22895071117440 run.py:483] Algo bellman_ford step 7765 current loss 0.346559, current_train_items 248512.
I0302 19:01:15.131687 22895071117440 run.py:483] Algo bellman_ford step 7766 current loss 0.491042, current_train_items 248544.
I0302 19:01:15.153608 22895071117440 run.py:483] Algo bellman_ford step 7767 current loss 0.513874, current_train_items 248576.
I0302 19:01:15.182371 22895071117440 run.py:483] Algo bellman_ford step 7768 current loss 0.582632, current_train_items 248608.
I0302 19:01:15.213218 22895071117440 run.py:483] Algo bellman_ford step 7769 current loss 0.744845, current_train_items 248640.
I0302 19:01:15.231689 22895071117440 run.py:483] Algo bellman_ford step 7770 current loss 0.345972, current_train_items 248672.
I0302 19:01:15.247550 22895071117440 run.py:483] Algo bellman_ford step 7771 current loss 0.418034, current_train_items 248704.
I0302 19:01:15.269220 22895071117440 run.py:483] Algo bellman_ford step 7772 current loss 0.492823, current_train_items 248736.
I0302 19:01:15.298468 22895071117440 run.py:483] Algo bellman_ford step 7773 current loss 0.686818, current_train_items 248768.
I0302 19:01:15.329104 22895071117440 run.py:483] Algo bellman_ford step 7774 current loss 0.673296, current_train_items 248800.
I0302 19:01:15.347878 22895071117440 run.py:483] Algo bellman_ford step 7775 current loss 0.294288, current_train_items 248832.
I0302 19:01:15.363622 22895071117440 run.py:483] Algo bellman_ford step 7776 current loss 0.568276, current_train_items 248864.
I0302 19:01:15.386673 22895071117440 run.py:483] Algo bellman_ford step 7777 current loss 0.737789, current_train_items 248896.
I0302 19:01:15.415879 22895071117440 run.py:483] Algo bellman_ford step 7778 current loss 0.693239, current_train_items 248928.
I0302 19:01:15.448519 22895071117440 run.py:483] Algo bellman_ford step 7779 current loss 0.813595, current_train_items 248960.
I0302 19:01:15.466464 22895071117440 run.py:483] Algo bellman_ford step 7780 current loss 0.191809, current_train_items 248992.
I0302 19:01:15.482137 22895071117440 run.py:483] Algo bellman_ford step 7781 current loss 0.505457, current_train_items 249024.
I0302 19:01:15.503909 22895071117440 run.py:483] Algo bellman_ford step 7782 current loss 0.566572, current_train_items 249056.
I0302 19:01:15.533557 22895071117440 run.py:483] Algo bellman_ford step 7783 current loss 0.673212, current_train_items 249088.
I0302 19:01:15.565040 22895071117440 run.py:483] Algo bellman_ford step 7784 current loss 0.678313, current_train_items 249120.
I0302 19:01:15.583302 22895071117440 run.py:483] Algo bellman_ford step 7785 current loss 0.326915, current_train_items 249152.
I0302 19:01:15.599109 22895071117440 run.py:483] Algo bellman_ford step 7786 current loss 0.497334, current_train_items 249184.
I0302 19:01:15.620588 22895071117440 run.py:483] Algo bellman_ford step 7787 current loss 0.594991, current_train_items 249216.
I0302 19:01:15.649434 22895071117440 run.py:483] Algo bellman_ford step 7788 current loss 0.630840, current_train_items 249248.
I0302 19:01:15.678885 22895071117440 run.py:483] Algo bellman_ford step 7789 current loss 0.759307, current_train_items 249280.
I0302 19:01:15.697578 22895071117440 run.py:483] Algo bellman_ford step 7790 current loss 0.341598, current_train_items 249312.
I0302 19:01:15.712861 22895071117440 run.py:483] Algo bellman_ford step 7791 current loss 0.562110, current_train_items 249344.
I0302 19:01:15.734608 22895071117440 run.py:483] Algo bellman_ford step 7792 current loss 0.730441, current_train_items 249376.
I0302 19:01:15.762641 22895071117440 run.py:483] Algo bellman_ford step 7793 current loss 0.773089, current_train_items 249408.
I0302 19:01:15.793316 22895071117440 run.py:483] Algo bellman_ford step 7794 current loss 0.741058, current_train_items 249440.
I0302 19:01:15.811468 22895071117440 run.py:483] Algo bellman_ford step 7795 current loss 0.331107, current_train_items 249472.
I0302 19:01:15.826409 22895071117440 run.py:483] Algo bellman_ford step 7796 current loss 0.471760, current_train_items 249504.
I0302 19:01:15.847559 22895071117440 run.py:483] Algo bellman_ford step 7797 current loss 0.636972, current_train_items 249536.
I0302 19:01:15.875173 22895071117440 run.py:483] Algo bellman_ford step 7798 current loss 0.580454, current_train_items 249568.
I0302 19:01:15.907648 22895071117440 run.py:483] Algo bellman_ford step 7799 current loss 0.792811, current_train_items 249600.
I0302 19:01:15.926133 22895071117440 run.py:483] Algo bellman_ford step 7800 current loss 0.335000, current_train_items 249632.
I0302 19:01:15.934053 22895071117440 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:01:15.934161 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:15.950257 22895071117440 run.py:483] Algo bellman_ford step 7801 current loss 0.525211, current_train_items 249664.
I0302 19:01:15.972651 22895071117440 run.py:483] Algo bellman_ford step 7802 current loss 0.608034, current_train_items 249696.
I0302 19:01:16.003467 22895071117440 run.py:483] Algo bellman_ford step 7803 current loss 0.670973, current_train_items 249728.
I0302 19:01:16.036669 22895071117440 run.py:483] Algo bellman_ford step 7804 current loss 0.756589, current_train_items 249760.
I0302 19:01:16.055663 22895071117440 run.py:483] Algo bellman_ford step 7805 current loss 0.256320, current_train_items 249792.
I0302 19:01:16.071397 22895071117440 run.py:483] Algo bellman_ford step 7806 current loss 0.422916, current_train_items 249824.
I0302 19:01:16.092289 22895071117440 run.py:483] Algo bellman_ford step 7807 current loss 0.500972, current_train_items 249856.
I0302 19:01:16.119982 22895071117440 run.py:483] Algo bellman_ford step 7808 current loss 0.621811, current_train_items 249888.
I0302 19:01:16.151257 22895071117440 run.py:483] Algo bellman_ford step 7809 current loss 0.772535, current_train_items 249920.
I0302 19:01:16.169455 22895071117440 run.py:483] Algo bellman_ford step 7810 current loss 0.334688, current_train_items 249952.
I0302 19:01:16.185187 22895071117440 run.py:483] Algo bellman_ford step 7811 current loss 0.493968, current_train_items 249984.
I0302 19:01:16.207371 22895071117440 run.py:483] Algo bellman_ford step 7812 current loss 0.596172, current_train_items 250016.
I0302 19:01:16.237215 22895071117440 run.py:483] Algo bellman_ford step 7813 current loss 0.679905, current_train_items 250048.
I0302 19:01:16.268235 22895071117440 run.py:483] Algo bellman_ford step 7814 current loss 0.771984, current_train_items 250080.
I0302 19:01:16.286153 22895071117440 run.py:483] Algo bellman_ford step 7815 current loss 0.317672, current_train_items 250112.
I0302 19:01:16.301321 22895071117440 run.py:483] Algo bellman_ford step 7816 current loss 0.440962, current_train_items 250144.
I0302 19:01:16.323748 22895071117440 run.py:483] Algo bellman_ford step 7817 current loss 0.583541, current_train_items 250176.
I0302 19:01:16.351118 22895071117440 run.py:483] Algo bellman_ford step 7818 current loss 0.606547, current_train_items 250208.
I0302 19:01:16.381302 22895071117440 run.py:483] Algo bellman_ford step 7819 current loss 0.758043, current_train_items 250240.
I0302 19:01:16.399491 22895071117440 run.py:483] Algo bellman_ford step 7820 current loss 0.327150, current_train_items 250272.
I0302 19:01:16.415277 22895071117440 run.py:483] Algo bellman_ford step 7821 current loss 0.529644, current_train_items 250304.
I0302 19:01:16.438847 22895071117440 run.py:483] Algo bellman_ford step 7822 current loss 0.557232, current_train_items 250336.
I0302 19:01:16.469259 22895071117440 run.py:483] Algo bellman_ford step 7823 current loss 0.775643, current_train_items 250368.
I0302 19:01:16.499219 22895071117440 run.py:483] Algo bellman_ford step 7824 current loss 0.865279, current_train_items 250400.
I0302 19:01:16.517513 22895071117440 run.py:483] Algo bellman_ford step 7825 current loss 0.247322, current_train_items 250432.
I0302 19:01:16.533222 22895071117440 run.py:483] Algo bellman_ford step 7826 current loss 0.545891, current_train_items 250464.
I0302 19:01:16.555454 22895071117440 run.py:483] Algo bellman_ford step 7827 current loss 0.618893, current_train_items 250496.
I0302 19:01:16.584980 22895071117440 run.py:483] Algo bellman_ford step 7828 current loss 0.687683, current_train_items 250528.
I0302 19:01:16.615975 22895071117440 run.py:483] Algo bellman_ford step 7829 current loss 0.800879, current_train_items 250560.
I0302 19:01:16.634030 22895071117440 run.py:483] Algo bellman_ford step 7830 current loss 0.234005, current_train_items 250592.
I0302 19:01:16.649175 22895071117440 run.py:483] Algo bellman_ford step 7831 current loss 0.519388, current_train_items 250624.
I0302 19:01:16.671279 22895071117440 run.py:483] Algo bellman_ford step 7832 current loss 0.616132, current_train_items 250656.
I0302 19:01:16.701074 22895071117440 run.py:483] Algo bellman_ford step 7833 current loss 0.711349, current_train_items 250688.
I0302 19:01:16.729958 22895071117440 run.py:483] Algo bellman_ford step 7834 current loss 0.784088, current_train_items 250720.
I0302 19:01:16.748121 22895071117440 run.py:483] Algo bellman_ford step 7835 current loss 0.279194, current_train_items 250752.
I0302 19:01:16.763675 22895071117440 run.py:483] Algo bellman_ford step 7836 current loss 0.450325, current_train_items 250784.
I0302 19:01:16.785664 22895071117440 run.py:483] Algo bellman_ford step 7837 current loss 0.522800, current_train_items 250816.
I0302 19:01:16.815694 22895071117440 run.py:483] Algo bellman_ford step 7838 current loss 0.662479, current_train_items 250848.
I0302 19:01:16.847982 22895071117440 run.py:483] Algo bellman_ford step 7839 current loss 0.747839, current_train_items 250880.
I0302 19:01:16.865825 22895071117440 run.py:483] Algo bellman_ford step 7840 current loss 0.226256, current_train_items 250912.
I0302 19:01:16.881387 22895071117440 run.py:483] Algo bellman_ford step 7841 current loss 0.433989, current_train_items 250944.
I0302 19:01:16.903580 22895071117440 run.py:483] Algo bellman_ford step 7842 current loss 0.582528, current_train_items 250976.
I0302 19:01:16.933921 22895071117440 run.py:483] Algo bellman_ford step 7843 current loss 0.570733, current_train_items 251008.
I0302 19:01:16.966552 22895071117440 run.py:483] Algo bellman_ford step 7844 current loss 0.677181, current_train_items 251040.
I0302 19:01:16.984524 22895071117440 run.py:483] Algo bellman_ford step 7845 current loss 0.458319, current_train_items 251072.
I0302 19:01:16.999811 22895071117440 run.py:483] Algo bellman_ford step 7846 current loss 0.511916, current_train_items 251104.
I0302 19:01:17.022176 22895071117440 run.py:483] Algo bellman_ford step 7847 current loss 0.717642, current_train_items 251136.
I0302 19:01:17.051664 22895071117440 run.py:483] Algo bellman_ford step 7848 current loss 0.759384, current_train_items 251168.
I0302 19:01:17.083474 22895071117440 run.py:483] Algo bellman_ford step 7849 current loss 0.883065, current_train_items 251200.
I0302 19:01:17.101314 22895071117440 run.py:483] Algo bellman_ford step 7850 current loss 0.265613, current_train_items 251232.
I0302 19:01:17.109398 22895071117440 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:01:17.109505 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:17.125517 22895071117440 run.py:483] Algo bellman_ford step 7851 current loss 0.481077, current_train_items 251264.
I0302 19:01:17.147355 22895071117440 run.py:483] Algo bellman_ford step 7852 current loss 0.530057, current_train_items 251296.
I0302 19:01:17.177752 22895071117440 run.py:483] Algo bellman_ford step 7853 current loss 0.709890, current_train_items 251328.
I0302 19:01:17.211747 22895071117440 run.py:483] Algo bellman_ford step 7854 current loss 0.880966, current_train_items 251360.
I0302 19:01:17.230109 22895071117440 run.py:483] Algo bellman_ford step 7855 current loss 0.322029, current_train_items 251392.
I0302 19:01:17.246011 22895071117440 run.py:483] Algo bellman_ford step 7856 current loss 0.622481, current_train_items 251424.
I0302 19:01:17.268630 22895071117440 run.py:483] Algo bellman_ford step 7857 current loss 0.645006, current_train_items 251456.
I0302 19:01:17.297280 22895071117440 run.py:483] Algo bellman_ford step 7858 current loss 0.643416, current_train_items 251488.
I0302 19:01:17.329542 22895071117440 run.py:483] Algo bellman_ford step 7859 current loss 0.836912, current_train_items 251520.
I0302 19:01:17.347961 22895071117440 run.py:483] Algo bellman_ford step 7860 current loss 0.323567, current_train_items 251552.
I0302 19:01:17.363510 22895071117440 run.py:483] Algo bellman_ford step 7861 current loss 0.400392, current_train_items 251584.
I0302 19:01:17.385506 22895071117440 run.py:483] Algo bellman_ford step 7862 current loss 0.669070, current_train_items 251616.
I0302 19:01:17.414588 22895071117440 run.py:483] Algo bellman_ford step 7863 current loss 0.680718, current_train_items 251648.
I0302 19:01:17.444451 22895071117440 run.py:483] Algo bellman_ford step 7864 current loss 0.670655, current_train_items 251680.
I0302 19:01:17.462691 22895071117440 run.py:483] Algo bellman_ford step 7865 current loss 0.293388, current_train_items 251712.
I0302 19:01:17.478377 22895071117440 run.py:483] Algo bellman_ford step 7866 current loss 0.592824, current_train_items 251744.
I0302 19:01:17.501181 22895071117440 run.py:483] Algo bellman_ford step 7867 current loss 0.597527, current_train_items 251776.
I0302 19:01:17.529827 22895071117440 run.py:483] Algo bellman_ford step 7868 current loss 0.704901, current_train_items 251808.
I0302 19:01:17.559894 22895071117440 run.py:483] Algo bellman_ford step 7869 current loss 0.803993, current_train_items 251840.
I0302 19:01:17.578306 22895071117440 run.py:483] Algo bellman_ford step 7870 current loss 0.301091, current_train_items 251872.
I0302 19:01:17.594092 22895071117440 run.py:483] Algo bellman_ford step 7871 current loss 0.404902, current_train_items 251904.
I0302 19:01:17.616287 22895071117440 run.py:483] Algo bellman_ford step 7872 current loss 0.636570, current_train_items 251936.
I0302 19:01:17.644477 22895071117440 run.py:483] Algo bellman_ford step 7873 current loss 0.705460, current_train_items 251968.
I0302 19:01:17.675250 22895071117440 run.py:483] Algo bellman_ford step 7874 current loss 0.710631, current_train_items 252000.
I0302 19:01:17.693688 22895071117440 run.py:483] Algo bellman_ford step 7875 current loss 0.286997, current_train_items 252032.
I0302 19:01:17.709133 22895071117440 run.py:483] Algo bellman_ford step 7876 current loss 0.406094, current_train_items 252064.
I0302 19:01:17.730793 22895071117440 run.py:483] Algo bellman_ford step 7877 current loss 0.626525, current_train_items 252096.
I0302 19:01:17.759526 22895071117440 run.py:483] Algo bellman_ford step 7878 current loss 0.670405, current_train_items 252128.
I0302 19:01:17.790615 22895071117440 run.py:483] Algo bellman_ford step 7879 current loss 0.777142, current_train_items 252160.
I0302 19:01:17.808801 22895071117440 run.py:483] Algo bellman_ford step 7880 current loss 0.292315, current_train_items 252192.
I0302 19:01:17.823813 22895071117440 run.py:483] Algo bellman_ford step 7881 current loss 0.407749, current_train_items 252224.
I0302 19:01:17.845761 22895071117440 run.py:483] Algo bellman_ford step 7882 current loss 0.561099, current_train_items 252256.
I0302 19:01:17.875331 22895071117440 run.py:483] Algo bellman_ford step 7883 current loss 0.657249, current_train_items 252288.
I0302 19:01:17.906813 22895071117440 run.py:483] Algo bellman_ford step 7884 current loss 0.831241, current_train_items 252320.
I0302 19:01:17.925712 22895071117440 run.py:483] Algo bellman_ford step 7885 current loss 0.353038, current_train_items 252352.
I0302 19:01:17.941490 22895071117440 run.py:483] Algo bellman_ford step 7886 current loss 0.470405, current_train_items 252384.
I0302 19:01:17.962632 22895071117440 run.py:483] Algo bellman_ford step 7887 current loss 0.499706, current_train_items 252416.
I0302 19:01:17.989629 22895071117440 run.py:483] Algo bellman_ford step 7888 current loss 0.632661, current_train_items 252448.
I0302 19:01:18.020130 22895071117440 run.py:483] Algo bellman_ford step 7889 current loss 0.703662, current_train_items 252480.
I0302 19:01:18.038926 22895071117440 run.py:483] Algo bellman_ford step 7890 current loss 0.267928, current_train_items 252512.
I0302 19:01:18.054447 22895071117440 run.py:483] Algo bellman_ford step 7891 current loss 0.494990, current_train_items 252544.
I0302 19:01:18.076457 22895071117440 run.py:483] Algo bellman_ford step 7892 current loss 0.575972, current_train_items 252576.
I0302 19:01:18.105673 22895071117440 run.py:483] Algo bellman_ford step 7893 current loss 0.669430, current_train_items 252608.
I0302 19:01:18.136125 22895071117440 run.py:483] Algo bellman_ford step 7894 current loss 0.702729, current_train_items 252640.
I0302 19:01:18.153924 22895071117440 run.py:483] Algo bellman_ford step 7895 current loss 0.287119, current_train_items 252672.
I0302 19:01:18.168955 22895071117440 run.py:483] Algo bellman_ford step 7896 current loss 0.438917, current_train_items 252704.
I0302 19:01:18.191508 22895071117440 run.py:483] Algo bellman_ford step 7897 current loss 0.645664, current_train_items 252736.
I0302 19:01:18.221271 22895071117440 run.py:483] Algo bellman_ford step 7898 current loss 0.641719, current_train_items 252768.
I0302 19:01:18.253248 22895071117440 run.py:483] Algo bellman_ford step 7899 current loss 0.847438, current_train_items 252800.
I0302 19:01:18.271557 22895071117440 run.py:483] Algo bellman_ford step 7900 current loss 0.308176, current_train_items 252832.
I0302 19:01:18.279348 22895071117440 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:01:18.279456 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:18.295413 22895071117440 run.py:483] Algo bellman_ford step 7901 current loss 0.502066, current_train_items 252864.
I0302 19:01:18.318011 22895071117440 run.py:483] Algo bellman_ford step 7902 current loss 0.605946, current_train_items 252896.
I0302 19:01:18.348176 22895071117440 run.py:483] Algo bellman_ford step 7903 current loss 0.677672, current_train_items 252928.
I0302 19:01:18.382080 22895071117440 run.py:483] Algo bellman_ford step 7904 current loss 0.856251, current_train_items 252960.
I0302 19:01:18.400278 22895071117440 run.py:483] Algo bellman_ford step 7905 current loss 0.285307, current_train_items 252992.
I0302 19:01:18.416281 22895071117440 run.py:483] Algo bellman_ford step 7906 current loss 0.488579, current_train_items 253024.
I0302 19:01:18.438890 22895071117440 run.py:483] Algo bellman_ford step 7907 current loss 0.601509, current_train_items 253056.
I0302 19:01:18.467227 22895071117440 run.py:483] Algo bellman_ford step 7908 current loss 0.658475, current_train_items 253088.
I0302 19:01:18.496123 22895071117440 run.py:483] Algo bellman_ford step 7909 current loss 0.596718, current_train_items 253120.
I0302 19:01:18.514136 22895071117440 run.py:483] Algo bellman_ford step 7910 current loss 0.281958, current_train_items 253152.
I0302 19:01:18.530079 22895071117440 run.py:483] Algo bellman_ford step 7911 current loss 0.523196, current_train_items 253184.
I0302 19:01:18.552516 22895071117440 run.py:483] Algo bellman_ford step 7912 current loss 0.655010, current_train_items 253216.
I0302 19:01:18.581619 22895071117440 run.py:483] Algo bellman_ford step 7913 current loss 0.677700, current_train_items 253248.
I0302 19:01:18.612813 22895071117440 run.py:483] Algo bellman_ford step 7914 current loss 0.789216, current_train_items 253280.
I0302 19:01:18.630778 22895071117440 run.py:483] Algo bellman_ford step 7915 current loss 0.267288, current_train_items 253312.
I0302 19:01:18.646412 22895071117440 run.py:483] Algo bellman_ford step 7916 current loss 0.500630, current_train_items 253344.
I0302 19:01:18.668343 22895071117440 run.py:483] Algo bellman_ford step 7917 current loss 0.582686, current_train_items 253376.
I0302 19:01:18.697278 22895071117440 run.py:483] Algo bellman_ford step 7918 current loss 0.606426, current_train_items 253408.
I0302 19:01:18.729990 22895071117440 run.py:483] Algo bellman_ford step 7919 current loss 0.704498, current_train_items 253440.
I0302 19:01:18.748151 22895071117440 run.py:483] Algo bellman_ford step 7920 current loss 0.270695, current_train_items 253472.
I0302 19:01:18.763790 22895071117440 run.py:483] Algo bellman_ford step 7921 current loss 0.452310, current_train_items 253504.
I0302 19:01:18.786124 22895071117440 run.py:483] Algo bellman_ford step 7922 current loss 0.565691, current_train_items 253536.
I0302 19:01:18.814582 22895071117440 run.py:483] Algo bellman_ford step 7923 current loss 0.523838, current_train_items 253568.
I0302 19:01:18.845827 22895071117440 run.py:483] Algo bellman_ford step 7924 current loss 0.740951, current_train_items 253600.
I0302 19:01:18.863859 22895071117440 run.py:483] Algo bellman_ford step 7925 current loss 0.301348, current_train_items 253632.
I0302 19:01:18.879398 22895071117440 run.py:483] Algo bellman_ford step 7926 current loss 0.438879, current_train_items 253664.
I0302 19:01:18.901455 22895071117440 run.py:483] Algo bellman_ford step 7927 current loss 0.577462, current_train_items 253696.
I0302 19:01:18.931369 22895071117440 run.py:483] Algo bellman_ford step 7928 current loss 0.629078, current_train_items 253728.
I0302 19:01:18.961758 22895071117440 run.py:483] Algo bellman_ford step 7929 current loss 0.620284, current_train_items 253760.
I0302 19:01:18.979848 22895071117440 run.py:483] Algo bellman_ford step 7930 current loss 0.302347, current_train_items 253792.
I0302 19:01:18.995305 22895071117440 run.py:483] Algo bellman_ford step 7931 current loss 0.400458, current_train_items 253824.
I0302 19:01:19.016256 22895071117440 run.py:483] Algo bellman_ford step 7932 current loss 0.481874, current_train_items 253856.
I0302 19:01:19.045500 22895071117440 run.py:483] Algo bellman_ford step 7933 current loss 0.684824, current_train_items 253888.
I0302 19:01:19.076903 22895071117440 run.py:483] Algo bellman_ford step 7934 current loss 0.770355, current_train_items 253920.
I0302 19:01:19.094694 22895071117440 run.py:483] Algo bellman_ford step 7935 current loss 0.326374, current_train_items 253952.
I0302 19:01:19.109753 22895071117440 run.py:483] Algo bellman_ford step 7936 current loss 0.418624, current_train_items 253984.
I0302 19:01:19.132128 22895071117440 run.py:483] Algo bellman_ford step 7937 current loss 0.493561, current_train_items 254016.
I0302 19:01:19.160567 22895071117440 run.py:483] Algo bellman_ford step 7938 current loss 0.619958, current_train_items 254048.
I0302 19:01:19.192350 22895071117440 run.py:483] Algo bellman_ford step 7939 current loss 0.747743, current_train_items 254080.
I0302 19:01:19.210341 22895071117440 run.py:483] Algo bellman_ford step 7940 current loss 0.325468, current_train_items 254112.
I0302 19:01:19.225184 22895071117440 run.py:483] Algo bellman_ford step 7941 current loss 0.430668, current_train_items 254144.
I0302 19:01:19.247656 22895071117440 run.py:483] Algo bellman_ford step 7942 current loss 0.591747, current_train_items 254176.
I0302 19:01:19.277889 22895071117440 run.py:483] Algo bellman_ford step 7943 current loss 0.711074, current_train_items 254208.
I0302 19:01:19.308248 22895071117440 run.py:483] Algo bellman_ford step 7944 current loss 0.795379, current_train_items 254240.
I0302 19:01:19.326040 22895071117440 run.py:483] Algo bellman_ford step 7945 current loss 0.312951, current_train_items 254272.
I0302 19:01:19.340956 22895071117440 run.py:483] Algo bellman_ford step 7946 current loss 0.469384, current_train_items 254304.
I0302 19:01:19.362335 22895071117440 run.py:483] Algo bellman_ford step 7947 current loss 0.543013, current_train_items 254336.
I0302 19:01:19.389701 22895071117440 run.py:483] Algo bellman_ford step 7948 current loss 0.632177, current_train_items 254368.
I0302 19:01:19.421563 22895071117440 run.py:483] Algo bellman_ford step 7949 current loss 0.779708, current_train_items 254400.
I0302 19:01:19.439242 22895071117440 run.py:483] Algo bellman_ford step 7950 current loss 0.335479, current_train_items 254432.
I0302 19:01:19.447152 22895071117440 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:01:19.447258 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:19.463499 22895071117440 run.py:483] Algo bellman_ford step 7951 current loss 0.456077, current_train_items 254464.
I0302 19:01:19.486436 22895071117440 run.py:483] Algo bellman_ford step 7952 current loss 0.552244, current_train_items 254496.
I0302 19:01:19.516944 22895071117440 run.py:483] Algo bellman_ford step 7953 current loss 0.655586, current_train_items 254528.
I0302 19:01:19.547944 22895071117440 run.py:483] Algo bellman_ford step 7954 current loss 0.644726, current_train_items 254560.
I0302 19:01:19.566633 22895071117440 run.py:483] Algo bellman_ford step 7955 current loss 0.422562, current_train_items 254592.
I0302 19:01:19.582221 22895071117440 run.py:483] Algo bellman_ford step 7956 current loss 0.566828, current_train_items 254624.
I0302 19:01:19.604648 22895071117440 run.py:483] Algo bellman_ford step 7957 current loss 0.555685, current_train_items 254656.
I0302 19:01:19.633504 22895071117440 run.py:483] Algo bellman_ford step 7958 current loss 0.674436, current_train_items 254688.
I0302 19:01:19.664180 22895071117440 run.py:483] Algo bellman_ford step 7959 current loss 0.626817, current_train_items 254720.
I0302 19:01:19.682992 22895071117440 run.py:483] Algo bellman_ford step 7960 current loss 0.241075, current_train_items 254752.
I0302 19:01:19.699298 22895071117440 run.py:483] Algo bellman_ford step 7961 current loss 0.443676, current_train_items 254784.
I0302 19:01:19.721109 22895071117440 run.py:483] Algo bellman_ford step 7962 current loss 0.580178, current_train_items 254816.
I0302 19:01:19.749215 22895071117440 run.py:483] Algo bellman_ford step 7963 current loss 0.637525, current_train_items 254848.
I0302 19:01:19.781059 22895071117440 run.py:483] Algo bellman_ford step 7964 current loss 0.796198, current_train_items 254880.
I0302 19:01:19.798988 22895071117440 run.py:483] Algo bellman_ford step 7965 current loss 0.277180, current_train_items 254912.
I0302 19:01:19.814723 22895071117440 run.py:483] Algo bellman_ford step 7966 current loss 0.551476, current_train_items 254944.
I0302 19:01:19.837635 22895071117440 run.py:483] Algo bellman_ford step 7967 current loss 0.738516, current_train_items 254976.
I0302 19:01:19.866435 22895071117440 run.py:483] Algo bellman_ford step 7968 current loss 0.659761, current_train_items 255008.
I0302 19:01:19.896260 22895071117440 run.py:483] Algo bellman_ford step 7969 current loss 0.824500, current_train_items 255040.
I0302 19:01:19.914617 22895071117440 run.py:483] Algo bellman_ford step 7970 current loss 0.371690, current_train_items 255072.
I0302 19:01:19.930324 22895071117440 run.py:483] Algo bellman_ford step 7971 current loss 0.611374, current_train_items 255104.
I0302 19:01:19.952794 22895071117440 run.py:483] Algo bellman_ford step 7972 current loss 0.602639, current_train_items 255136.
I0302 19:01:19.980920 22895071117440 run.py:483] Algo bellman_ford step 7973 current loss 0.586745, current_train_items 255168.
I0302 19:01:20.013587 22895071117440 run.py:483] Algo bellman_ford step 7974 current loss 0.698678, current_train_items 255200.
I0302 19:01:20.031918 22895071117440 run.py:483] Algo bellman_ford step 7975 current loss 0.301720, current_train_items 255232.
I0302 19:01:20.047722 22895071117440 run.py:483] Algo bellman_ford step 7976 current loss 0.451117, current_train_items 255264.
I0302 19:01:20.070001 22895071117440 run.py:483] Algo bellman_ford step 7977 current loss 0.607809, current_train_items 255296.
I0302 19:01:20.099058 22895071117440 run.py:483] Algo bellman_ford step 7978 current loss 0.678110, current_train_items 255328.
I0302 19:01:20.128328 22895071117440 run.py:483] Algo bellman_ford step 7979 current loss 0.635998, current_train_items 255360.
I0302 19:01:20.146579 22895071117440 run.py:483] Algo bellman_ford step 7980 current loss 0.294236, current_train_items 255392.
I0302 19:01:20.161757 22895071117440 run.py:483] Algo bellman_ford step 7981 current loss 0.527918, current_train_items 255424.
I0302 19:01:20.183028 22895071117440 run.py:483] Algo bellman_ford step 7982 current loss 0.590052, current_train_items 255456.
I0302 19:01:20.211234 22895071117440 run.py:483] Algo bellman_ford step 7983 current loss 0.633445, current_train_items 255488.
I0302 19:01:20.243086 22895071117440 run.py:483] Algo bellman_ford step 7984 current loss 0.732699, current_train_items 255520.
I0302 19:01:20.261386 22895071117440 run.py:483] Algo bellman_ford step 7985 current loss 0.230893, current_train_items 255552.
I0302 19:01:20.277320 22895071117440 run.py:483] Algo bellman_ford step 7986 current loss 0.580413, current_train_items 255584.
I0302 19:01:20.298387 22895071117440 run.py:483] Algo bellman_ford step 7987 current loss 0.631190, current_train_items 255616.
I0302 19:01:20.328101 22895071117440 run.py:483] Algo bellman_ford step 7988 current loss 0.778862, current_train_items 255648.
I0302 19:01:20.358105 22895071117440 run.py:483] Algo bellman_ford step 7989 current loss 0.843457, current_train_items 255680.
I0302 19:01:20.376580 22895071117440 run.py:483] Algo bellman_ford step 7990 current loss 0.390150, current_train_items 255712.
I0302 19:01:20.392390 22895071117440 run.py:483] Algo bellman_ford step 7991 current loss 0.546816, current_train_items 255744.
I0302 19:01:20.414378 22895071117440 run.py:483] Algo bellman_ford step 7992 current loss 0.704005, current_train_items 255776.
I0302 19:01:20.444833 22895071117440 run.py:483] Algo bellman_ford step 7993 current loss 1.010823, current_train_items 255808.
I0302 19:01:20.477154 22895071117440 run.py:483] Algo bellman_ford step 7994 current loss 1.013798, current_train_items 255840.
I0302 19:01:20.495179 22895071117440 run.py:483] Algo bellman_ford step 7995 current loss 0.341223, current_train_items 255872.
I0302 19:01:20.510073 22895071117440 run.py:483] Algo bellman_ford step 7996 current loss 0.394544, current_train_items 255904.
I0302 19:01:20.531918 22895071117440 run.py:483] Algo bellman_ford step 7997 current loss 0.693278, current_train_items 255936.
I0302 19:01:20.560648 22895071117440 run.py:483] Algo bellman_ford step 7998 current loss 0.693418, current_train_items 255968.
I0302 19:01:20.589705 22895071117440 run.py:483] Algo bellman_ford step 7999 current loss 0.786873, current_train_items 256000.
I0302 19:01:20.607985 22895071117440 run.py:483] Algo bellman_ford step 8000 current loss 0.318545, current_train_items 256032.
I0302 19:01:20.615639 22895071117440 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:01:20.615744 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:20.632058 22895071117440 run.py:483] Algo bellman_ford step 8001 current loss 0.469738, current_train_items 256064.
I0302 19:01:20.654057 22895071117440 run.py:483] Algo bellman_ford step 8002 current loss 0.579584, current_train_items 256096.
I0302 19:01:20.684279 22895071117440 run.py:483] Algo bellman_ford step 8003 current loss 0.675981, current_train_items 256128.
I0302 19:01:20.714454 22895071117440 run.py:483] Algo bellman_ford step 8004 current loss 0.676835, current_train_items 256160.
I0302 19:01:20.732914 22895071117440 run.py:483] Algo bellman_ford step 8005 current loss 0.274099, current_train_items 256192.
I0302 19:01:20.748352 22895071117440 run.py:483] Algo bellman_ford step 8006 current loss 0.446809, current_train_items 256224.
I0302 19:01:20.770729 22895071117440 run.py:483] Algo bellman_ford step 8007 current loss 0.618135, current_train_items 256256.
I0302 19:01:20.801210 22895071117440 run.py:483] Algo bellman_ford step 8008 current loss 0.598037, current_train_items 256288.
I0302 19:01:20.832696 22895071117440 run.py:483] Algo bellman_ford step 8009 current loss 0.696015, current_train_items 256320.
I0302 19:01:20.850929 22895071117440 run.py:483] Algo bellman_ford step 8010 current loss 0.274011, current_train_items 256352.
I0302 19:01:20.866183 22895071117440 run.py:483] Algo bellman_ford step 8011 current loss 0.425445, current_train_items 256384.
I0302 19:01:20.888791 22895071117440 run.py:483] Algo bellman_ford step 8012 current loss 0.522858, current_train_items 256416.
I0302 19:01:20.917685 22895071117440 run.py:483] Algo bellman_ford step 8013 current loss 0.684539, current_train_items 256448.
I0302 19:01:20.946780 22895071117440 run.py:483] Algo bellman_ford step 8014 current loss 0.619105, current_train_items 256480.
I0302 19:01:20.965024 22895071117440 run.py:483] Algo bellman_ford step 8015 current loss 0.363639, current_train_items 256512.
I0302 19:01:20.980915 22895071117440 run.py:483] Algo bellman_ford step 8016 current loss 0.449722, current_train_items 256544.
I0302 19:01:21.002295 22895071117440 run.py:483] Algo bellman_ford step 8017 current loss 0.531182, current_train_items 256576.
I0302 19:01:21.031129 22895071117440 run.py:483] Algo bellman_ford step 8018 current loss 0.533855, current_train_items 256608.
I0302 19:01:21.060769 22895071117440 run.py:483] Algo bellman_ford step 8019 current loss 0.689248, current_train_items 256640.
I0302 19:01:21.078990 22895071117440 run.py:483] Algo bellman_ford step 8020 current loss 0.323363, current_train_items 256672.
I0302 19:01:21.094510 22895071117440 run.py:483] Algo bellman_ford step 8021 current loss 0.455369, current_train_items 256704.
I0302 19:01:21.115907 22895071117440 run.py:483] Algo bellman_ford step 8022 current loss 0.597453, current_train_items 256736.
I0302 19:01:21.144135 22895071117440 run.py:483] Algo bellman_ford step 8023 current loss 0.737309, current_train_items 256768.
I0302 19:01:21.176608 22895071117440 run.py:483] Algo bellman_ford step 8024 current loss 0.775991, current_train_items 256800.
I0302 19:01:21.194653 22895071117440 run.py:483] Algo bellman_ford step 8025 current loss 0.361357, current_train_items 256832.
I0302 19:01:21.210416 22895071117440 run.py:483] Algo bellman_ford step 8026 current loss 0.461012, current_train_items 256864.
I0302 19:01:21.232419 22895071117440 run.py:483] Algo bellman_ford step 8027 current loss 0.640995, current_train_items 256896.
I0302 19:01:21.260607 22895071117440 run.py:483] Algo bellman_ford step 8028 current loss 0.625967, current_train_items 256928.
I0302 19:01:21.291585 22895071117440 run.py:483] Algo bellman_ford step 8029 current loss 0.660201, current_train_items 256960.
I0302 19:01:21.309643 22895071117440 run.py:483] Algo bellman_ford step 8030 current loss 0.306101, current_train_items 256992.
I0302 19:01:21.324917 22895071117440 run.py:483] Algo bellman_ford step 8031 current loss 0.388980, current_train_items 257024.
I0302 19:01:21.347195 22895071117440 run.py:483] Algo bellman_ford step 8032 current loss 0.696915, current_train_items 257056.
I0302 19:01:21.376596 22895071117440 run.py:483] Algo bellman_ford step 8033 current loss 0.659376, current_train_items 257088.
I0302 19:01:21.408732 22895071117440 run.py:483] Algo bellman_ford step 8034 current loss 0.742747, current_train_items 257120.
I0302 19:01:21.427089 22895071117440 run.py:483] Algo bellman_ford step 8035 current loss 0.287000, current_train_items 257152.
I0302 19:01:21.442921 22895071117440 run.py:483] Algo bellman_ford step 8036 current loss 0.538421, current_train_items 257184.
I0302 19:01:21.465049 22895071117440 run.py:483] Algo bellman_ford step 8037 current loss 0.557357, current_train_items 257216.
I0302 19:01:21.493962 22895071117440 run.py:483] Algo bellman_ford step 8038 current loss 0.607524, current_train_items 257248.
I0302 19:01:21.526263 22895071117440 run.py:483] Algo bellman_ford step 8039 current loss 0.825763, current_train_items 257280.
I0302 19:01:21.544557 22895071117440 run.py:483] Algo bellman_ford step 8040 current loss 0.306107, current_train_items 257312.
I0302 19:01:21.560221 22895071117440 run.py:483] Algo bellman_ford step 8041 current loss 0.509282, current_train_items 257344.
I0302 19:01:21.582533 22895071117440 run.py:483] Algo bellman_ford step 8042 current loss 0.788900, current_train_items 257376.
I0302 19:01:21.611702 22895071117440 run.py:483] Algo bellman_ford step 8043 current loss 0.771095, current_train_items 257408.
I0302 19:01:21.643296 22895071117440 run.py:483] Algo bellman_ford step 8044 current loss 0.948332, current_train_items 257440.
I0302 19:01:21.661516 22895071117440 run.py:483] Algo bellman_ford step 8045 current loss 0.285571, current_train_items 257472.
I0302 19:01:21.677455 22895071117440 run.py:483] Algo bellman_ford step 8046 current loss 0.559063, current_train_items 257504.
I0302 19:01:21.699139 22895071117440 run.py:483] Algo bellman_ford step 8047 current loss 0.620809, current_train_items 257536.
I0302 19:01:21.728815 22895071117440 run.py:483] Algo bellman_ford step 8048 current loss 0.670198, current_train_items 257568.
I0302 19:01:21.758250 22895071117440 run.py:483] Algo bellman_ford step 8049 current loss 0.660403, current_train_items 257600.
I0302 19:01:21.776422 22895071117440 run.py:483] Algo bellman_ford step 8050 current loss 0.291111, current_train_items 257632.
I0302 19:01:21.784068 22895071117440 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:01:21.784175 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 19:01:21.800481 22895071117440 run.py:483] Algo bellman_ford step 8051 current loss 0.560698, current_train_items 257664.
I0302 19:01:21.822441 22895071117440 run.py:483] Algo bellman_ford step 8052 current loss 0.561045, current_train_items 257696.
I0302 19:01:21.851562 22895071117440 run.py:483] Algo bellman_ford step 8053 current loss 0.633072, current_train_items 257728.
I0302 19:01:21.883874 22895071117440 run.py:483] Algo bellman_ford step 8054 current loss 0.916712, current_train_items 257760.
I0302 19:01:21.902714 22895071117440 run.py:483] Algo bellman_ford step 8055 current loss 0.377077, current_train_items 257792.
I0302 19:01:21.917320 22895071117440 run.py:483] Algo bellman_ford step 8056 current loss 0.398229, current_train_items 257824.
I0302 19:01:21.938014 22895071117440 run.py:483] Algo bellman_ford step 8057 current loss 0.520598, current_train_items 257856.
I0302 19:01:21.967839 22895071117440 run.py:483] Algo bellman_ford step 8058 current loss 0.733706, current_train_items 257888.
I0302 19:01:22.000924 22895071117440 run.py:483] Algo bellman_ford step 8059 current loss 0.860721, current_train_items 257920.
I0302 19:01:22.019350 22895071117440 run.py:483] Algo bellman_ford step 8060 current loss 0.286236, current_train_items 257952.
I0302 19:01:22.034790 22895071117440 run.py:483] Algo bellman_ford step 8061 current loss 0.533032, current_train_items 257984.
I0302 19:01:22.057717 22895071117440 run.py:483] Algo bellman_ford step 8062 current loss 0.634869, current_train_items 258016.
I0302 19:01:22.086602 22895071117440 run.py:483] Algo bellman_ford step 8063 current loss 0.615471, current_train_items 258048.
I0302 19:01:22.116581 22895071117440 run.py:483] Algo bellman_ford step 8064 current loss 0.787478, current_train_items 258080.
I0302 19:01:22.134444 22895071117440 run.py:483] Algo bellman_ford step 8065 current loss 0.286083, current_train_items 258112.
I0302 19:01:22.149657 22895071117440 run.py:483] Algo bellman_ford step 8066 current loss 0.515248, current_train_items 258144.
I0302 19:01:22.171609 22895071117440 run.py:483] Algo bellman_ford step 8067 current loss 0.555444, current_train_items 258176.
I0302 19:01:22.200390 22895071117440 run.py:483] Algo bellman_ford step 8068 current loss 0.625386, current_train_items 258208.
I0302 19:01:22.230186 22895071117440 run.py:483] Algo bellman_ford step 8069 current loss 0.685125, current_train_items 258240.
I0302 19:01:22.248994 22895071117440 run.py:483] Algo bellman_ford step 8070 current loss 0.319458, current_train_items 258272.
I0302 19:01:22.265494 22895071117440 run.py:483] Algo bellman_ford step 8071 current loss 0.523108, current_train_items 258304.
I0302 19:01:22.287364 22895071117440 run.py:483] Algo bellman_ford step 8072 current loss 0.621124, current_train_items 258336.
I0302 19:01:22.316520 22895071117440 run.py:483] Algo bellman_ford step 8073 current loss 0.609663, current_train_items 258368.
I0302 19:01:22.348486 22895071117440 run.py:483] Algo bellman_ford step 8074 current loss 0.749343, current_train_items 258400.
I0302 19:01:22.367030 22895071117440 run.py:483] Algo bellman_ford step 8075 current loss 0.313243, current_train_items 258432.
I0302 19:01:22.382712 22895071117440 run.py:483] Algo bellman_ford step 8076 current loss 0.446176, current_train_items 258464.
I0302 19:01:22.404397 22895071117440 run.py:483] Algo bellman_ford step 8077 current loss 0.467314, current_train_items 258496.
I0302 19:01:22.432682 22895071117440 run.py:483] Algo bellman_ford step 8078 current loss 0.779962, current_train_items 258528.
I0302 19:01:22.461992 22895071117440 run.py:483] Algo bellman_ford step 8079 current loss 0.704007, current_train_items 258560.
I0302 19:01:22.480109 22895071117440 run.py:483] Algo bellman_ford step 8080 current loss 0.377574, current_train_items 258592.
I0302 19:01:22.495574 22895071117440 run.py:483] Algo bellman_ford step 8081 current loss 0.483840, current_train_items 258624.
I0302 19:01:22.517307 22895071117440 run.py:483] Algo bellman_ford step 8082 current loss 0.597343, current_train_items 258656.
I0302 19:01:22.546352 22895071117440 run.py:483] Algo bellman_ford step 8083 current loss 0.634381, current_train_items 258688.
I0302 19:01:22.580154 22895071117440 run.py:483] Algo bellman_ford step 8084 current loss 0.788969, current_train_items 258720.
I0302 19:01:22.598593 22895071117440 run.py:483] Algo bellman_ford step 8085 current loss 0.333612, current_train_items 258752.
I0302 19:01:22.614696 22895071117440 run.py:483] Algo bellman_ford step 8086 current loss 0.478007, current_train_items 258784.
I0302 19:01:22.635848 22895071117440 run.py:483] Algo bellman_ford step 8087 current loss 0.605711, current_train_items 258816.
I0302 19:01:22.666451 22895071117440 run.py:483] Algo bellman_ford step 8088 current loss 0.655679, current_train_items 258848.
I0302 19:01:22.696007 22895071117440 run.py:483] Algo bellman_ford step 8089 current loss 0.659193, current_train_items 258880.
I0302 19:01:22.714680 22895071117440 run.py:483] Algo bellman_ford step 8090 current loss 0.311903, current_train_items 258912.
I0302 19:01:22.729967 22895071117440 run.py:483] Algo bellman_ford step 8091 current loss 0.397473, current_train_items 258944.
I0302 19:01:22.752291 22895071117440 run.py:483] Algo bellman_ford step 8092 current loss 0.597510, current_train_items 258976.
I0302 19:01:22.780388 22895071117440 run.py:483] Algo bellman_ford step 8093 current loss 0.603044, current_train_items 259008.
I0302 19:01:22.812136 22895071117440 run.py:483] Algo bellman_ford step 8094 current loss 0.774631, current_train_items 259040.
I0302 19:01:22.830553 22895071117440 run.py:483] Algo bellman_ford step 8095 current loss 0.306820, current_train_items 259072.
I0302 19:01:22.845796 22895071117440 run.py:483] Algo bellman_ford step 8096 current loss 0.506788, current_train_items 259104.
I0302 19:01:22.868833 22895071117440 run.py:483] Algo bellman_ford step 8097 current loss 0.620225, current_train_items 259136.
I0302 19:01:22.898649 22895071117440 run.py:483] Algo bellman_ford step 8098 current loss 0.738619, current_train_items 259168.
I0302 19:01:22.929499 22895071117440 run.py:483] Algo bellman_ford step 8099 current loss 0.818207, current_train_items 259200.
I0302 19:01:22.947942 22895071117440 run.py:483] Algo bellman_ford step 8100 current loss 0.258259, current_train_items 259232.
I0302 19:01:22.955516 22895071117440 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:01:22.955621 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:01:22.971800 22895071117440 run.py:483] Algo bellman_ford step 8101 current loss 0.458721, current_train_items 259264.
I0302 19:01:22.995255 22895071117440 run.py:483] Algo bellman_ford step 8102 current loss 0.690341, current_train_items 259296.
I0302 19:01:23.024712 22895071117440 run.py:483] Algo bellman_ford step 8103 current loss 0.564397, current_train_items 259328.
I0302 19:01:23.055764 22895071117440 run.py:483] Algo bellman_ford step 8104 current loss 0.672785, current_train_items 259360.
I0302 19:01:23.074392 22895071117440 run.py:483] Algo bellman_ford step 8105 current loss 0.288054, current_train_items 259392.
I0302 19:01:23.090037 22895071117440 run.py:483] Algo bellman_ford step 8106 current loss 0.455828, current_train_items 259424.
I0302 19:01:23.112021 22895071117440 run.py:483] Algo bellman_ford step 8107 current loss 0.667039, current_train_items 259456.
I0302 19:01:23.140730 22895071117440 run.py:483] Algo bellman_ford step 8108 current loss 0.916235, current_train_items 259488.
I0302 19:01:23.172512 22895071117440 run.py:483] Algo bellman_ford step 8109 current loss 0.728162, current_train_items 259520.
I0302 19:01:23.190791 22895071117440 run.py:483] Algo bellman_ford step 8110 current loss 0.271428, current_train_items 259552.
I0302 19:01:23.205735 22895071117440 run.py:483] Algo bellman_ford step 8111 current loss 0.491651, current_train_items 259584.
I0302 19:01:23.227244 22895071117440 run.py:483] Algo bellman_ford step 8112 current loss 0.572068, current_train_items 259616.
I0302 19:01:23.254862 22895071117440 run.py:483] Algo bellman_ford step 8113 current loss 0.602318, current_train_items 259648.
I0302 19:01:23.288096 22895071117440 run.py:483] Algo bellman_ford step 8114 current loss 0.912342, current_train_items 259680.
I0302 19:01:23.306564 22895071117440 run.py:483] Algo bellman_ford step 8115 current loss 0.313797, current_train_items 259712.
I0302 19:01:23.322058 22895071117440 run.py:483] Algo bellman_ford step 8116 current loss 0.418454, current_train_items 259744.
I0302 19:01:23.344249 22895071117440 run.py:483] Algo bellman_ford step 8117 current loss 0.632866, current_train_items 259776.
I0302 19:01:23.373863 22895071117440 run.py:483] Algo bellman_ford step 8118 current loss 0.658074, current_train_items 259808.
I0302 19:01:23.404336 22895071117440 run.py:483] Algo bellman_ford step 8119 current loss 0.681817, current_train_items 259840.
I0302 19:01:23.422521 22895071117440 run.py:483] Algo bellman_ford step 8120 current loss 0.312476, current_train_items 259872.
I0302 19:01:23.438500 22895071117440 run.py:483] Algo bellman_ford step 8121 current loss 0.506887, current_train_items 259904.
I0302 19:01:23.460676 22895071117440 run.py:483] Algo bellman_ford step 8122 current loss 0.595691, current_train_items 259936.
I0302 19:01:23.490967 22895071117440 run.py:483] Algo bellman_ford step 8123 current loss 0.630041, current_train_items 259968.
I0302 19:01:23.520196 22895071117440 run.py:483] Algo bellman_ford step 8124 current loss 0.661966, current_train_items 260000.
I0302 19:01:23.538146 22895071117440 run.py:483] Algo bellman_ford step 8125 current loss 0.262029, current_train_items 260032.
I0302 19:01:23.553827 22895071117440 run.py:483] Algo bellman_ford step 8126 current loss 0.687180, current_train_items 260064.
I0302 19:01:23.576728 22895071117440 run.py:483] Algo bellman_ford step 8127 current loss 0.594378, current_train_items 260096.
I0302 19:01:23.606643 22895071117440 run.py:483] Algo bellman_ford step 8128 current loss 0.690485, current_train_items 260128.
I0302 19:01:23.640269 22895071117440 run.py:483] Algo bellman_ford step 8129 current loss 0.804422, current_train_items 260160.
I0302 19:01:23.658639 22895071117440 run.py:483] Algo bellman_ford step 8130 current loss 0.289663, current_train_items 260192.
I0302 19:01:23.673665 22895071117440 run.py:483] Algo bellman_ford step 8131 current loss 0.419438, current_train_items 260224.
I0302 19:01:23.695924 22895071117440 run.py:483] Algo bellman_ford step 8132 current loss 0.655423, current_train_items 260256.
I0302 19:01:23.725102 22895071117440 run.py:483] Algo bellman_ford step 8133 current loss 0.588508, current_train_items 260288.
I0302 19:01:23.756756 22895071117440 run.py:483] Algo bellman_ford step 8134 current loss 0.849731, current_train_items 260320.
I0302 19:01:23.774569 22895071117440 run.py:483] Algo bellman_ford step 8135 current loss 0.304834, current_train_items 260352.
I0302 19:01:23.789880 22895071117440 run.py:483] Algo bellman_ford step 8136 current loss 0.460055, current_train_items 260384.
I0302 19:01:23.811533 22895071117440 run.py:483] Algo bellman_ford step 8137 current loss 0.619520, current_train_items 260416.
I0302 19:01:23.840919 22895071117440 run.py:483] Algo bellman_ford step 8138 current loss 0.625483, current_train_items 260448.
I0302 19:01:23.873879 22895071117440 run.py:483] Algo bellman_ford step 8139 current loss 0.799105, current_train_items 260480.
I0302 19:01:23.891753 22895071117440 run.py:483] Algo bellman_ford step 8140 current loss 0.320815, current_train_items 260512.
I0302 19:01:23.906934 22895071117440 run.py:483] Algo bellman_ford step 8141 current loss 0.557502, current_train_items 260544.
I0302 19:01:23.929705 22895071117440 run.py:483] Algo bellman_ford step 8142 current loss 0.594442, current_train_items 260576.
I0302 19:01:23.959071 22895071117440 run.py:483] Algo bellman_ford step 8143 current loss 0.657986, current_train_items 260608.
I0302 19:01:23.990620 22895071117440 run.py:483] Algo bellman_ford step 8144 current loss 0.717398, current_train_items 260640.
I0302 19:01:24.008721 22895071117440 run.py:483] Algo bellman_ford step 8145 current loss 0.283394, current_train_items 260672.
I0302 19:01:24.024413 22895071117440 run.py:483] Algo bellman_ford step 8146 current loss 0.484797, current_train_items 260704.
I0302 19:01:24.046214 22895071117440 run.py:483] Algo bellman_ford step 8147 current loss 0.571992, current_train_items 260736.
I0302 19:01:24.075459 22895071117440 run.py:483] Algo bellman_ford step 8148 current loss 0.723975, current_train_items 260768.
I0302 19:01:24.105841 22895071117440 run.py:483] Algo bellman_ford step 8149 current loss 0.663489, current_train_items 260800.
I0302 19:01:24.124012 22895071117440 run.py:483] Algo bellman_ford step 8150 current loss 0.308039, current_train_items 260832.
I0302 19:01:24.131787 22895071117440 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.94140625, 'score': 0.94140625, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:01:24.131892 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.941, val scores are: bellman_ford: 0.941
I0302 19:01:24.147959 22895071117440 run.py:483] Algo bellman_ford step 8151 current loss 0.386214, current_train_items 260864.
I0302 19:01:24.170225 22895071117440 run.py:483] Algo bellman_ford step 8152 current loss 0.625614, current_train_items 260896.
I0302 19:01:24.198432 22895071117440 run.py:483] Algo bellman_ford step 8153 current loss 0.663716, current_train_items 260928.
I0302 19:01:24.231460 22895071117440 run.py:483] Algo bellman_ford step 8154 current loss 0.795214, current_train_items 260960.
I0302 19:01:24.250212 22895071117440 run.py:483] Algo bellman_ford step 8155 current loss 0.292776, current_train_items 260992.
I0302 19:01:24.265145 22895071117440 run.py:483] Algo bellman_ford step 8156 current loss 0.397184, current_train_items 261024.
I0302 19:01:24.287513 22895071117440 run.py:483] Algo bellman_ford step 8157 current loss 0.578614, current_train_items 261056.
I0302 19:01:24.316327 22895071117440 run.py:483] Algo bellman_ford step 8158 current loss 0.680881, current_train_items 261088.
I0302 19:01:24.346977 22895071117440 run.py:483] Algo bellman_ford step 8159 current loss 0.741584, current_train_items 261120.
I0302 19:01:24.365264 22895071117440 run.py:483] Algo bellman_ford step 8160 current loss 0.257092, current_train_items 261152.
I0302 19:01:24.381160 22895071117440 run.py:483] Algo bellman_ford step 8161 current loss 0.467970, current_train_items 261184.
I0302 19:01:24.402783 22895071117440 run.py:483] Algo bellman_ford step 8162 current loss 0.662626, current_train_items 261216.
I0302 19:01:24.432107 22895071117440 run.py:483] Algo bellman_ford step 8163 current loss 0.744788, current_train_items 261248.
I0302 19:01:24.460170 22895071117440 run.py:483] Algo bellman_ford step 8164 current loss 0.625915, current_train_items 261280.
I0302 19:01:24.478147 22895071117440 run.py:483] Algo bellman_ford step 8165 current loss 0.234236, current_train_items 261312.
I0302 19:01:24.493548 22895071117440 run.py:483] Algo bellman_ford step 8166 current loss 0.520432, current_train_items 261344.
I0302 19:01:24.515584 22895071117440 run.py:483] Algo bellman_ford step 8167 current loss 0.692293, current_train_items 261376.
I0302 19:01:24.544176 22895071117440 run.py:483] Algo bellman_ford step 8168 current loss 0.643191, current_train_items 261408.
I0302 19:01:24.576047 22895071117440 run.py:483] Algo bellman_ford step 8169 current loss 0.746456, current_train_items 261440.
I0302 19:01:24.594401 22895071117440 run.py:483] Algo bellman_ford step 8170 current loss 0.235375, current_train_items 261472.
I0302 19:01:24.610184 22895071117440 run.py:483] Algo bellman_ford step 8171 current loss 0.428014, current_train_items 261504.
I0302 19:01:24.632509 22895071117440 run.py:483] Algo bellman_ford step 8172 current loss 0.639899, current_train_items 261536.
I0302 19:01:24.661498 22895071117440 run.py:483] Algo bellman_ford step 8173 current loss 0.695170, current_train_items 261568.
I0302 19:01:24.691839 22895071117440 run.py:483] Algo bellman_ford step 8174 current loss 0.791406, current_train_items 261600.
I0302 19:01:24.710639 22895071117440 run.py:483] Algo bellman_ford step 8175 current loss 0.324240, current_train_items 261632.
I0302 19:01:24.726745 22895071117440 run.py:483] Algo bellman_ford step 8176 current loss 0.461664, current_train_items 261664.
I0302 19:01:24.748363 22895071117440 run.py:483] Algo bellman_ford step 8177 current loss 0.618432, current_train_items 261696.
I0302 19:01:24.778219 22895071117440 run.py:483] Algo bellman_ford step 8178 current loss 0.735343, current_train_items 261728.
I0302 19:01:24.807008 22895071117440 run.py:483] Algo bellman_ford step 8179 current loss 0.665721, current_train_items 261760.
I0302 19:01:24.825336 22895071117440 run.py:483] Algo bellman_ford step 8180 current loss 0.277514, current_train_items 261792.
I0302 19:01:24.840606 22895071117440 run.py:483] Algo bellman_ford step 8181 current loss 0.445810, current_train_items 261824.
I0302 19:01:24.862323 22895071117440 run.py:483] Algo bellman_ford step 8182 current loss 0.530806, current_train_items 261856.
I0302 19:01:24.891918 22895071117440 run.py:483] Algo bellman_ford step 8183 current loss 0.685119, current_train_items 261888.
I0302 19:01:24.925017 22895071117440 run.py:483] Algo bellman_ford step 8184 current loss 0.806317, current_train_items 261920.
I0302 19:01:24.943311 22895071117440 run.py:483] Algo bellman_ford step 8185 current loss 0.288764, current_train_items 261952.
I0302 19:01:24.958954 22895071117440 run.py:483] Algo bellman_ford step 8186 current loss 0.495498, current_train_items 261984.
I0302 19:01:24.980986 22895071117440 run.py:483] Algo bellman_ford step 8187 current loss 0.578667, current_train_items 262016.
I0302 19:01:25.009654 22895071117440 run.py:483] Algo bellman_ford step 8188 current loss 0.568386, current_train_items 262048.
I0302 19:01:25.040288 22895071117440 run.py:483] Algo bellman_ford step 8189 current loss 0.750827, current_train_items 262080.
I0302 19:01:25.059108 22895071117440 run.py:483] Algo bellman_ford step 8190 current loss 0.291046, current_train_items 262112.
I0302 19:01:25.074923 22895071117440 run.py:483] Algo bellman_ford step 8191 current loss 0.481413, current_train_items 262144.
I0302 19:01:25.096951 22895071117440 run.py:483] Algo bellman_ford step 8192 current loss 0.568853, current_train_items 262176.
I0302 19:01:25.125695 22895071117440 run.py:483] Algo bellman_ford step 8193 current loss 0.555584, current_train_items 262208.
I0302 19:01:25.156505 22895071117440 run.py:483] Algo bellman_ford step 8194 current loss 0.681093, current_train_items 262240.
I0302 19:01:25.174679 22895071117440 run.py:483] Algo bellman_ford step 8195 current loss 0.310690, current_train_items 262272.
I0302 19:01:25.190071 22895071117440 run.py:483] Algo bellman_ford step 8196 current loss 0.446924, current_train_items 262304.
I0302 19:01:25.211462 22895071117440 run.py:483] Algo bellman_ford step 8197 current loss 0.579802, current_train_items 262336.
I0302 19:01:25.239980 22895071117440 run.py:483] Algo bellman_ford step 8198 current loss 0.734033, current_train_items 262368.
I0302 19:01:25.270874 22895071117440 run.py:483] Algo bellman_ford step 8199 current loss 0.712865, current_train_items 262400.
I0302 19:01:25.289808 22895071117440 run.py:483] Algo bellman_ford step 8200 current loss 0.279785, current_train_items 262432.
I0302 19:01:25.297552 22895071117440 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:01:25.297659 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:25.313419 22895071117440 run.py:483] Algo bellman_ford step 8201 current loss 0.416856, current_train_items 262464.
I0302 19:01:25.336184 22895071117440 run.py:483] Algo bellman_ford step 8202 current loss 0.623362, current_train_items 262496.
I0302 19:01:25.364968 22895071117440 run.py:483] Algo bellman_ford step 8203 current loss 0.796009, current_train_items 262528.
I0302 19:01:25.397637 22895071117440 run.py:483] Algo bellman_ford step 8204 current loss 0.915757, current_train_items 262560.
I0302 19:01:25.416165 22895071117440 run.py:483] Algo bellman_ford step 8205 current loss 0.247365, current_train_items 262592.
I0302 19:01:25.431863 22895071117440 run.py:483] Algo bellman_ford step 8206 current loss 0.416536, current_train_items 262624.
I0302 19:01:25.454367 22895071117440 run.py:483] Algo bellman_ford step 8207 current loss 0.582750, current_train_items 262656.
I0302 19:01:25.483378 22895071117440 run.py:483] Algo bellman_ford step 8208 current loss 0.613310, current_train_items 262688.
I0302 19:01:25.511858 22895071117440 run.py:483] Algo bellman_ford step 8209 current loss 0.570315, current_train_items 262720.
I0302 19:01:25.529788 22895071117440 run.py:483] Algo bellman_ford step 8210 current loss 0.262324, current_train_items 262752.
I0302 19:01:25.545071 22895071117440 run.py:483] Algo bellman_ford step 8211 current loss 0.399413, current_train_items 262784.
I0302 19:01:25.566260 22895071117440 run.py:483] Algo bellman_ford step 8212 current loss 0.526014, current_train_items 262816.
I0302 19:01:25.595001 22895071117440 run.py:483] Algo bellman_ford step 8213 current loss 0.698610, current_train_items 262848.
I0302 19:01:25.625771 22895071117440 run.py:483] Algo bellman_ford step 8214 current loss 0.775692, current_train_items 262880.
I0302 19:01:25.643753 22895071117440 run.py:483] Algo bellman_ford step 8215 current loss 0.245127, current_train_items 262912.
I0302 19:01:25.658591 22895071117440 run.py:483] Algo bellman_ford step 8216 current loss 0.441954, current_train_items 262944.
I0302 19:01:25.679793 22895071117440 run.py:483] Algo bellman_ford step 8217 current loss 0.609629, current_train_items 262976.
I0302 19:01:25.709959 22895071117440 run.py:483] Algo bellman_ford step 8218 current loss 0.592602, current_train_items 263008.
I0302 19:01:25.741624 22895071117440 run.py:483] Algo bellman_ford step 8219 current loss 0.680504, current_train_items 263040.
I0302 19:01:25.759446 22895071117440 run.py:483] Algo bellman_ford step 8220 current loss 0.373972, current_train_items 263072.
I0302 19:01:25.774552 22895071117440 run.py:483] Algo bellman_ford step 8221 current loss 0.487324, current_train_items 263104.
I0302 19:01:25.797291 22895071117440 run.py:483] Algo bellman_ford step 8222 current loss 0.662577, current_train_items 263136.
I0302 19:01:25.826854 22895071117440 run.py:483] Algo bellman_ford step 8223 current loss 0.598108, current_train_items 263168.
I0302 19:01:25.855432 22895071117440 run.py:483] Algo bellman_ford step 8224 current loss 0.574577, current_train_items 263200.
I0302 19:01:25.873717 22895071117440 run.py:483] Algo bellman_ford step 8225 current loss 0.264231, current_train_items 263232.
I0302 19:01:25.889240 22895071117440 run.py:483] Algo bellman_ford step 8226 current loss 0.411563, current_train_items 263264.
I0302 19:01:25.912047 22895071117440 run.py:483] Algo bellman_ford step 8227 current loss 0.599014, current_train_items 263296.
I0302 19:01:25.940243 22895071117440 run.py:483] Algo bellman_ford step 8228 current loss 0.568567, current_train_items 263328.
I0302 19:01:25.971549 22895071117440 run.py:483] Algo bellman_ford step 8229 current loss 0.720315, current_train_items 263360.
I0302 19:01:25.989659 22895071117440 run.py:483] Algo bellman_ford step 8230 current loss 0.257786, current_train_items 263392.
I0302 19:01:26.004946 22895071117440 run.py:483] Algo bellman_ford step 8231 current loss 0.496882, current_train_items 263424.
I0302 19:01:26.027551 22895071117440 run.py:483] Algo bellman_ford step 8232 current loss 0.534560, current_train_items 263456.
I0302 19:01:26.056140 22895071117440 run.py:483] Algo bellman_ford step 8233 current loss 0.581463, current_train_items 263488.
I0302 19:01:26.086908 22895071117440 run.py:483] Algo bellman_ford step 8234 current loss 0.669760, current_train_items 263520.
I0302 19:01:26.104517 22895071117440 run.py:483] Algo bellman_ford step 8235 current loss 0.317296, current_train_items 263552.
I0302 19:01:26.120289 22895071117440 run.py:483] Algo bellman_ford step 8236 current loss 0.457841, current_train_items 263584.
I0302 19:01:26.142894 22895071117440 run.py:483] Algo bellman_ford step 8237 current loss 0.636390, current_train_items 263616.
I0302 19:01:26.172303 22895071117440 run.py:483] Algo bellman_ford step 8238 current loss 0.644872, current_train_items 263648.
I0302 19:01:26.202939 22895071117440 run.py:483] Algo bellman_ford step 8239 current loss 0.594603, current_train_items 263680.
I0302 19:01:26.220964 22895071117440 run.py:483] Algo bellman_ford step 8240 current loss 0.262511, current_train_items 263712.
I0302 19:01:26.236322 22895071117440 run.py:483] Algo bellman_ford step 8241 current loss 0.417120, current_train_items 263744.
I0302 19:01:26.257788 22895071117440 run.py:483] Algo bellman_ford step 8242 current loss 0.561424, current_train_items 263776.
I0302 19:01:26.286779 22895071117440 run.py:483] Algo bellman_ford step 8243 current loss 0.596347, current_train_items 263808.
I0302 19:01:26.319706 22895071117440 run.py:483] Algo bellman_ford step 8244 current loss 0.723490, current_train_items 263840.
I0302 19:01:26.338067 22895071117440 run.py:483] Algo bellman_ford step 8245 current loss 0.287880, current_train_items 263872.
I0302 19:01:26.353363 22895071117440 run.py:483] Algo bellman_ford step 8246 current loss 0.469228, current_train_items 263904.
I0302 19:01:26.374413 22895071117440 run.py:483] Algo bellman_ford step 8247 current loss 0.539233, current_train_items 263936.
I0302 19:01:26.403160 22895071117440 run.py:483] Algo bellman_ford step 8248 current loss 0.575096, current_train_items 263968.
I0302 19:01:26.434417 22895071117440 run.py:483] Algo bellman_ford step 8249 current loss 0.806878, current_train_items 264000.
I0302 19:01:26.452528 22895071117440 run.py:483] Algo bellman_ford step 8250 current loss 0.224538, current_train_items 264032.
I0302 19:01:26.460408 22895071117440 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:01:26.460513 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:26.476565 22895071117440 run.py:483] Algo bellman_ford step 8251 current loss 0.471213, current_train_items 264064.
I0302 19:01:26.499094 22895071117440 run.py:483] Algo bellman_ford step 8252 current loss 0.615111, current_train_items 264096.
I0302 19:01:26.529736 22895071117440 run.py:483] Algo bellman_ford step 8253 current loss 0.787093, current_train_items 264128.
I0302 19:01:26.560429 22895071117440 run.py:483] Algo bellman_ford step 8254 current loss 0.714611, current_train_items 264160.
I0302 19:01:26.578971 22895071117440 run.py:483] Algo bellman_ford step 8255 current loss 0.368627, current_train_items 264192.
I0302 19:01:26.594971 22895071117440 run.py:483] Algo bellman_ford step 8256 current loss 0.433836, current_train_items 264224.
I0302 19:01:26.617659 22895071117440 run.py:483] Algo bellman_ford step 8257 current loss 0.575623, current_train_items 264256.
I0302 19:01:26.647037 22895071117440 run.py:483] Algo bellman_ford step 8258 current loss 0.646692, current_train_items 264288.
I0302 19:01:26.679363 22895071117440 run.py:483] Algo bellman_ford step 8259 current loss 0.749784, current_train_items 264320.
I0302 19:01:26.697494 22895071117440 run.py:483] Algo bellman_ford step 8260 current loss 0.341102, current_train_items 264352.
I0302 19:01:26.713430 22895071117440 run.py:483] Algo bellman_ford step 8261 current loss 0.433022, current_train_items 264384.
I0302 19:01:26.736086 22895071117440 run.py:483] Algo bellman_ford step 8262 current loss 0.609046, current_train_items 264416.
I0302 19:01:26.765677 22895071117440 run.py:483] Algo bellman_ford step 8263 current loss 0.642246, current_train_items 264448.
I0302 19:01:26.797832 22895071117440 run.py:483] Algo bellman_ford step 8264 current loss 0.724370, current_train_items 264480.
I0302 19:01:26.815655 22895071117440 run.py:483] Algo bellman_ford step 8265 current loss 0.264502, current_train_items 264512.
I0302 19:01:26.830638 22895071117440 run.py:483] Algo bellman_ford step 8266 current loss 0.475388, current_train_items 264544.
I0302 19:01:26.852492 22895071117440 run.py:483] Algo bellman_ford step 8267 current loss 0.532975, current_train_items 264576.
I0302 19:01:26.882085 22895071117440 run.py:483] Algo bellman_ford step 8268 current loss 0.736915, current_train_items 264608.
I0302 19:01:26.911923 22895071117440 run.py:483] Algo bellman_ford step 8269 current loss 0.663921, current_train_items 264640.
I0302 19:01:26.930533 22895071117440 run.py:483] Algo bellman_ford step 8270 current loss 0.335519, current_train_items 264672.
I0302 19:01:26.945665 22895071117440 run.py:483] Algo bellman_ford step 8271 current loss 0.344833, current_train_items 264704.
I0302 19:01:26.967628 22895071117440 run.py:483] Algo bellman_ford step 8272 current loss 0.505172, current_train_items 264736.
I0302 19:01:26.995751 22895071117440 run.py:483] Algo bellman_ford step 8273 current loss 0.590691, current_train_items 264768.
I0302 19:01:27.027827 22895071117440 run.py:483] Algo bellman_ford step 8274 current loss 0.708678, current_train_items 264800.
I0302 19:01:27.046370 22895071117440 run.py:483] Algo bellman_ford step 8275 current loss 0.245480, current_train_items 264832.
I0302 19:01:27.062536 22895071117440 run.py:483] Algo bellman_ford step 8276 current loss 0.447652, current_train_items 264864.
I0302 19:01:27.084992 22895071117440 run.py:483] Algo bellman_ford step 8277 current loss 0.561991, current_train_items 264896.
I0302 19:01:27.113300 22895071117440 run.py:483] Algo bellman_ford step 8278 current loss 0.576985, current_train_items 264928.
I0302 19:01:27.143740 22895071117440 run.py:483] Algo bellman_ford step 8279 current loss 0.702953, current_train_items 264960.
I0302 19:01:27.161736 22895071117440 run.py:483] Algo bellman_ford step 8280 current loss 0.307394, current_train_items 264992.
I0302 19:01:27.177467 22895071117440 run.py:483] Algo bellman_ford step 8281 current loss 0.469949, current_train_items 265024.
I0302 19:01:27.199577 22895071117440 run.py:483] Algo bellman_ford step 8282 current loss 0.580339, current_train_items 265056.
I0302 19:01:27.228218 22895071117440 run.py:483] Algo bellman_ford step 8283 current loss 0.604772, current_train_items 265088.
I0302 19:01:27.258354 22895071117440 run.py:483] Algo bellman_ford step 8284 current loss 0.689380, current_train_items 265120.
I0302 19:01:27.276865 22895071117440 run.py:483] Algo bellman_ford step 8285 current loss 0.277045, current_train_items 265152.
I0302 19:01:27.292824 22895071117440 run.py:483] Algo bellman_ford step 8286 current loss 0.493907, current_train_items 265184.
I0302 19:01:27.314441 22895071117440 run.py:483] Algo bellman_ford step 8287 current loss 0.544148, current_train_items 265216.
I0302 19:01:27.344747 22895071117440 run.py:483] Algo bellman_ford step 8288 current loss 0.686162, current_train_items 265248.
I0302 19:01:27.376233 22895071117440 run.py:483] Algo bellman_ford step 8289 current loss 0.712837, current_train_items 265280.
I0302 19:01:27.394224 22895071117440 run.py:483] Algo bellman_ford step 8290 current loss 0.320908, current_train_items 265312.
I0302 19:01:27.410027 22895071117440 run.py:483] Algo bellman_ford step 8291 current loss 0.483965, current_train_items 265344.
I0302 19:01:27.432443 22895071117440 run.py:483] Algo bellman_ford step 8292 current loss 0.602730, current_train_items 265376.
I0302 19:01:27.460208 22895071117440 run.py:483] Algo bellman_ford step 8293 current loss 0.568337, current_train_items 265408.
I0302 19:01:27.492697 22895071117440 run.py:483] Algo bellman_ford step 8294 current loss 0.749116, current_train_items 265440.
I0302 19:01:27.510670 22895071117440 run.py:483] Algo bellman_ford step 8295 current loss 0.255810, current_train_items 265472.
I0302 19:01:27.526194 22895071117440 run.py:483] Algo bellman_ford step 8296 current loss 0.535885, current_train_items 265504.
I0302 19:01:27.549446 22895071117440 run.py:483] Algo bellman_ford step 8297 current loss 0.803646, current_train_items 265536.
I0302 19:01:27.578478 22895071117440 run.py:483] Algo bellman_ford step 8298 current loss 0.637937, current_train_items 265568.
I0302 19:01:27.608748 22895071117440 run.py:483] Algo bellman_ford step 8299 current loss 0.701201, current_train_items 265600.
I0302 19:01:27.627261 22895071117440 run.py:483] Algo bellman_ford step 8300 current loss 0.288911, current_train_items 265632.
I0302 19:01:27.635075 22895071117440 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:01:27.635182 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:01:27.650891 22895071117440 run.py:483] Algo bellman_ford step 8301 current loss 0.542298, current_train_items 265664.
I0302 19:01:27.674784 22895071117440 run.py:483] Algo bellman_ford step 8302 current loss 0.716371, current_train_items 265696.
I0302 19:01:27.704721 22895071117440 run.py:483] Algo bellman_ford step 8303 current loss 0.787340, current_train_items 265728.
I0302 19:01:27.736229 22895071117440 run.py:483] Algo bellman_ford step 8304 current loss 0.826072, current_train_items 265760.
I0302 19:01:27.755044 22895071117440 run.py:483] Algo bellman_ford step 8305 current loss 0.284311, current_train_items 265792.
I0302 19:01:27.770462 22895071117440 run.py:483] Algo bellman_ford step 8306 current loss 0.526039, current_train_items 265824.
I0302 19:01:27.792610 22895071117440 run.py:483] Algo bellman_ford step 8307 current loss 0.551653, current_train_items 265856.
I0302 19:01:27.819824 22895071117440 run.py:483] Algo bellman_ford step 8308 current loss 0.519312, current_train_items 265888.
I0302 19:01:27.852861 22895071117440 run.py:483] Algo bellman_ford step 8309 current loss 0.760238, current_train_items 265920.
I0302 19:01:27.871136 22895071117440 run.py:483] Algo bellman_ford step 8310 current loss 0.288417, current_train_items 265952.
I0302 19:01:27.886481 22895071117440 run.py:483] Algo bellman_ford step 8311 current loss 0.431051, current_train_items 265984.
I0302 19:01:27.908613 22895071117440 run.py:483] Algo bellman_ford step 8312 current loss 0.580350, current_train_items 266016.
I0302 19:01:27.936876 22895071117440 run.py:483] Algo bellman_ford step 8313 current loss 0.582697, current_train_items 266048.
I0302 19:01:27.966387 22895071117440 run.py:483] Algo bellman_ford step 8314 current loss 0.640881, current_train_items 266080.
I0302 19:01:27.984160 22895071117440 run.py:483] Algo bellman_ford step 8315 current loss 0.249505, current_train_items 266112.
I0302 19:01:27.999931 22895071117440 run.py:483] Algo bellman_ford step 8316 current loss 0.426049, current_train_items 266144.
I0302 19:01:28.021605 22895071117440 run.py:483] Algo bellman_ford step 8317 current loss 0.595443, current_train_items 266176.
I0302 19:01:28.049023 22895071117440 run.py:483] Algo bellman_ford step 8318 current loss 0.570965, current_train_items 266208.
I0302 19:01:28.079936 22895071117440 run.py:483] Algo bellman_ford step 8319 current loss 0.701954, current_train_items 266240.
I0302 19:01:28.097813 22895071117440 run.py:483] Algo bellman_ford step 8320 current loss 0.237610, current_train_items 266272.
I0302 19:01:28.113198 22895071117440 run.py:483] Algo bellman_ford step 8321 current loss 0.394537, current_train_items 266304.
I0302 19:01:28.136312 22895071117440 run.py:483] Algo bellman_ford step 8322 current loss 0.566765, current_train_items 266336.
I0302 19:01:28.165017 22895071117440 run.py:483] Algo bellman_ford step 8323 current loss 0.640824, current_train_items 266368.
I0302 19:01:28.197329 22895071117440 run.py:483] Algo bellman_ford step 8324 current loss 0.789752, current_train_items 266400.
I0302 19:01:28.215613 22895071117440 run.py:483] Algo bellman_ford step 8325 current loss 0.323730, current_train_items 266432.
I0302 19:01:28.230544 22895071117440 run.py:483] Algo bellman_ford step 8326 current loss 0.423035, current_train_items 266464.
I0302 19:01:28.252802 22895071117440 run.py:483] Algo bellman_ford step 8327 current loss 0.522961, current_train_items 266496.
I0302 19:01:28.280894 22895071117440 run.py:483] Algo bellman_ford step 8328 current loss 0.686977, current_train_items 266528.
I0302 19:01:28.312665 22895071117440 run.py:483] Algo bellman_ford step 8329 current loss 0.762676, current_train_items 266560.
I0302 19:01:28.330909 22895071117440 run.py:483] Algo bellman_ford step 8330 current loss 0.299835, current_train_items 266592.
I0302 19:01:28.346132 22895071117440 run.py:483] Algo bellman_ford step 8331 current loss 0.481247, current_train_items 266624.
I0302 19:01:28.367660 22895071117440 run.py:483] Algo bellman_ford step 8332 current loss 0.532743, current_train_items 266656.
I0302 19:01:28.397303 22895071117440 run.py:483] Algo bellman_ford step 8333 current loss 0.684293, current_train_items 266688.
I0302 19:01:28.427635 22895071117440 run.py:483] Algo bellman_ford step 8334 current loss 0.788421, current_train_items 266720.
I0302 19:01:28.445630 22895071117440 run.py:483] Algo bellman_ford step 8335 current loss 0.286884, current_train_items 266752.
I0302 19:01:28.460979 22895071117440 run.py:483] Algo bellman_ford step 8336 current loss 0.535122, current_train_items 266784.
I0302 19:01:28.484323 22895071117440 run.py:483] Algo bellman_ford step 8337 current loss 0.572241, current_train_items 266816.
I0302 19:01:28.513061 22895071117440 run.py:483] Algo bellman_ford step 8338 current loss 0.676987, current_train_items 266848.
I0302 19:01:28.543493 22895071117440 run.py:483] Algo bellman_ford step 8339 current loss 0.792331, current_train_items 266880.
I0302 19:01:28.561406 22895071117440 run.py:483] Algo bellman_ford step 8340 current loss 0.371014, current_train_items 266912.
I0302 19:01:28.576658 22895071117440 run.py:483] Algo bellman_ford step 8341 current loss 0.383744, current_train_items 266944.
I0302 19:01:28.598138 22895071117440 run.py:483] Algo bellman_ford step 8342 current loss 0.529035, current_train_items 266976.
I0302 19:01:28.626145 22895071117440 run.py:483] Algo bellman_ford step 8343 current loss 0.596025, current_train_items 267008.
I0302 19:01:28.658723 22895071117440 run.py:483] Algo bellman_ford step 8344 current loss 0.768223, current_train_items 267040.
I0302 19:01:28.676905 22895071117440 run.py:483] Algo bellman_ford step 8345 current loss 0.308448, current_train_items 267072.
I0302 19:01:28.692140 22895071117440 run.py:483] Algo bellman_ford step 8346 current loss 0.465924, current_train_items 267104.
I0302 19:01:28.714861 22895071117440 run.py:483] Algo bellman_ford step 8347 current loss 0.612100, current_train_items 267136.
I0302 19:01:28.744480 22895071117440 run.py:483] Algo bellman_ford step 8348 current loss 0.682715, current_train_items 267168.
I0302 19:01:28.776608 22895071117440 run.py:483] Algo bellman_ford step 8349 current loss 0.785842, current_train_items 267200.
I0302 19:01:28.794952 22895071117440 run.py:483] Algo bellman_ford step 8350 current loss 0.286974, current_train_items 267232.
I0302 19:01:28.803004 22895071117440 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:01:28.803112 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:28.819236 22895071117440 run.py:483] Algo bellman_ford step 8351 current loss 0.408667, current_train_items 267264.
I0302 19:01:28.841870 22895071117440 run.py:483] Algo bellman_ford step 8352 current loss 0.568237, current_train_items 267296.
I0302 19:01:28.871097 22895071117440 run.py:483] Algo bellman_ford step 8353 current loss 0.618194, current_train_items 267328.
I0302 19:01:28.904268 22895071117440 run.py:483] Algo bellman_ford step 8354 current loss 0.842663, current_train_items 267360.
I0302 19:01:28.922608 22895071117440 run.py:483] Algo bellman_ford step 8355 current loss 0.306682, current_train_items 267392.
I0302 19:01:28.937698 22895071117440 run.py:483] Algo bellman_ford step 8356 current loss 0.489408, current_train_items 267424.
I0302 19:01:28.959442 22895071117440 run.py:483] Algo bellman_ford step 8357 current loss 0.500111, current_train_items 267456.
I0302 19:01:28.988208 22895071117440 run.py:483] Algo bellman_ford step 8358 current loss 0.569861, current_train_items 267488.
I0302 19:01:29.017944 22895071117440 run.py:483] Algo bellman_ford step 8359 current loss 0.719743, current_train_items 267520.
I0302 19:01:29.036321 22895071117440 run.py:483] Algo bellman_ford step 8360 current loss 0.298595, current_train_items 267552.
I0302 19:01:29.051763 22895071117440 run.py:483] Algo bellman_ford step 8361 current loss 0.465349, current_train_items 267584.
I0302 19:01:29.073648 22895071117440 run.py:483] Algo bellman_ford step 8362 current loss 0.578445, current_train_items 267616.
I0302 19:01:29.101659 22895071117440 run.py:483] Algo bellman_ford step 8363 current loss 0.571513, current_train_items 267648.
I0302 19:01:29.132686 22895071117440 run.py:483] Algo bellman_ford step 8364 current loss 0.765811, current_train_items 267680.
I0302 19:01:29.150571 22895071117440 run.py:483] Algo bellman_ford step 8365 current loss 0.262268, current_train_items 267712.
I0302 19:01:29.166339 22895071117440 run.py:483] Algo bellman_ford step 8366 current loss 0.411339, current_train_items 267744.
I0302 19:01:29.187442 22895071117440 run.py:483] Algo bellman_ford step 8367 current loss 0.520054, current_train_items 267776.
I0302 19:01:29.216201 22895071117440 run.py:483] Algo bellman_ford step 8368 current loss 0.622846, current_train_items 267808.
I0302 19:01:29.249135 22895071117440 run.py:483] Algo bellman_ford step 8369 current loss 0.886799, current_train_items 267840.
I0302 19:01:29.267437 22895071117440 run.py:483] Algo bellman_ford step 8370 current loss 0.237384, current_train_items 267872.
I0302 19:01:29.283321 22895071117440 run.py:483] Algo bellman_ford step 8371 current loss 0.556517, current_train_items 267904.
I0302 19:01:29.304617 22895071117440 run.py:483] Algo bellman_ford step 8372 current loss 0.730189, current_train_items 267936.
I0302 19:01:29.333276 22895071117440 run.py:483] Algo bellman_ford step 8373 current loss 0.671043, current_train_items 267968.
I0302 19:01:29.364830 22895071117440 run.py:483] Algo bellman_ford step 8374 current loss 0.766715, current_train_items 268000.
I0302 19:01:29.383453 22895071117440 run.py:483] Algo bellman_ford step 8375 current loss 0.275897, current_train_items 268032.
I0302 19:01:29.398797 22895071117440 run.py:483] Algo bellman_ford step 8376 current loss 0.474155, current_train_items 268064.
I0302 19:01:29.419640 22895071117440 run.py:483] Algo bellman_ford step 8377 current loss 0.670108, current_train_items 268096.
I0302 19:01:29.448583 22895071117440 run.py:483] Algo bellman_ford step 8378 current loss 0.706336, current_train_items 268128.
I0302 19:01:29.479980 22895071117440 run.py:483] Algo bellman_ford step 8379 current loss 0.836623, current_train_items 268160.
I0302 19:01:29.498085 22895071117440 run.py:483] Algo bellman_ford step 8380 current loss 0.360873, current_train_items 268192.
I0302 19:01:29.513545 22895071117440 run.py:483] Algo bellman_ford step 8381 current loss 0.456828, current_train_items 268224.
I0302 19:01:29.534528 22895071117440 run.py:483] Algo bellman_ford step 8382 current loss 0.512768, current_train_items 268256.
I0302 19:01:29.563884 22895071117440 run.py:483] Algo bellman_ford step 8383 current loss 0.782591, current_train_items 268288.
I0302 19:01:29.593820 22895071117440 run.py:483] Algo bellman_ford step 8384 current loss 0.731473, current_train_items 268320.
I0302 19:01:29.612083 22895071117440 run.py:483] Algo bellman_ford step 8385 current loss 0.316333, current_train_items 268352.
I0302 19:01:29.627386 22895071117440 run.py:483] Algo bellman_ford step 8386 current loss 0.399592, current_train_items 268384.
I0302 19:01:29.648589 22895071117440 run.py:483] Algo bellman_ford step 8387 current loss 0.524410, current_train_items 268416.
I0302 19:01:29.677202 22895071117440 run.py:483] Algo bellman_ford step 8388 current loss 0.668098, current_train_items 268448.
I0302 19:01:29.707574 22895071117440 run.py:483] Algo bellman_ford step 8389 current loss 0.804913, current_train_items 268480.
I0302 19:01:29.726004 22895071117440 run.py:483] Algo bellman_ford step 8390 current loss 0.356774, current_train_items 268512.
I0302 19:01:29.741766 22895071117440 run.py:483] Algo bellman_ford step 8391 current loss 0.414289, current_train_items 268544.
I0302 19:01:29.762893 22895071117440 run.py:483] Algo bellman_ford step 8392 current loss 0.580774, current_train_items 268576.
I0302 19:01:29.792838 22895071117440 run.py:483] Algo bellman_ford step 8393 current loss 0.783382, current_train_items 268608.
I0302 19:01:29.822473 22895071117440 run.py:483] Algo bellman_ford step 8394 current loss 0.836702, current_train_items 268640.
I0302 19:01:29.840515 22895071117440 run.py:483] Algo bellman_ford step 8395 current loss 0.390927, current_train_items 268672.
I0302 19:01:29.855871 22895071117440 run.py:483] Algo bellman_ford step 8396 current loss 0.467914, current_train_items 268704.
I0302 19:01:29.877460 22895071117440 run.py:483] Algo bellman_ford step 8397 current loss 0.575318, current_train_items 268736.
I0302 19:01:29.906479 22895071117440 run.py:483] Algo bellman_ford step 8398 current loss 0.600885, current_train_items 268768.
I0302 19:01:29.938291 22895071117440 run.py:483] Algo bellman_ford step 8399 current loss 0.793752, current_train_items 268800.
I0302 19:01:29.956634 22895071117440 run.py:483] Algo bellman_ford step 8400 current loss 0.326690, current_train_items 268832.
I0302 19:01:29.976698 22895071117440 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:01:29.976804 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:29.993108 22895071117440 run.py:483] Algo bellman_ford step 8401 current loss 0.462418, current_train_items 268864.
I0302 19:01:30.016032 22895071117440 run.py:483] Algo bellman_ford step 8402 current loss 0.624199, current_train_items 268896.
I0302 19:01:30.045960 22895071117440 run.py:483] Algo bellman_ford step 8403 current loss 0.702361, current_train_items 268928.
I0302 19:01:30.079789 22895071117440 run.py:483] Algo bellman_ford step 8404 current loss 0.842419, current_train_items 268960.
I0302 19:01:30.098145 22895071117440 run.py:483] Algo bellman_ford step 8405 current loss 0.309756, current_train_items 268992.
I0302 19:01:30.113602 22895071117440 run.py:483] Algo bellman_ford step 8406 current loss 0.466570, current_train_items 269024.
I0302 19:01:30.135074 22895071117440 run.py:483] Algo bellman_ford step 8407 current loss 0.593503, current_train_items 269056.
I0302 19:01:30.163758 22895071117440 run.py:483] Algo bellman_ford step 8408 current loss 0.621392, current_train_items 269088.
I0302 19:01:30.195940 22895071117440 run.py:483] Algo bellman_ford step 8409 current loss 0.800840, current_train_items 269120.
I0302 19:01:30.213891 22895071117440 run.py:483] Algo bellman_ford step 8410 current loss 0.314323, current_train_items 269152.
I0302 19:01:30.229344 22895071117440 run.py:483] Algo bellman_ford step 8411 current loss 0.515656, current_train_items 269184.
I0302 19:01:30.252025 22895071117440 run.py:483] Algo bellman_ford step 8412 current loss 0.559953, current_train_items 269216.
I0302 19:01:30.282054 22895071117440 run.py:483] Algo bellman_ford step 8413 current loss 0.670680, current_train_items 269248.
I0302 19:01:30.313862 22895071117440 run.py:483] Algo bellman_ford step 8414 current loss 0.676699, current_train_items 269280.
I0302 19:01:30.332190 22895071117440 run.py:483] Algo bellman_ford step 8415 current loss 0.332094, current_train_items 269312.
I0302 19:01:30.347666 22895071117440 run.py:483] Algo bellman_ford step 8416 current loss 0.412454, current_train_items 269344.
I0302 19:01:30.369870 22895071117440 run.py:483] Algo bellman_ford step 8417 current loss 0.669122, current_train_items 269376.
I0302 19:01:30.399865 22895071117440 run.py:483] Algo bellman_ford step 8418 current loss 0.547501, current_train_items 269408.
I0302 19:01:30.434246 22895071117440 run.py:483] Algo bellman_ford step 8419 current loss 0.811682, current_train_items 269440.
I0302 19:01:30.452246 22895071117440 run.py:483] Algo bellman_ford step 8420 current loss 0.270790, current_train_items 269472.
I0302 19:01:30.467694 22895071117440 run.py:483] Algo bellman_ford step 8421 current loss 0.490580, current_train_items 269504.
I0302 19:01:30.491151 22895071117440 run.py:483] Algo bellman_ford step 8422 current loss 0.520872, current_train_items 269536.
I0302 19:01:30.520527 22895071117440 run.py:483] Algo bellman_ford step 8423 current loss 0.555365, current_train_items 269568.
I0302 19:01:30.552461 22895071117440 run.py:483] Algo bellman_ford step 8424 current loss 0.683356, current_train_items 269600.
I0302 19:01:30.570332 22895071117440 run.py:483] Algo bellman_ford step 8425 current loss 0.275350, current_train_items 269632.
I0302 19:01:30.585344 22895071117440 run.py:483] Algo bellman_ford step 8426 current loss 0.449188, current_train_items 269664.
I0302 19:01:30.607534 22895071117440 run.py:483] Algo bellman_ford step 8427 current loss 0.551421, current_train_items 269696.
I0302 19:01:30.636263 22895071117440 run.py:483] Algo bellman_ford step 8428 current loss 0.582703, current_train_items 269728.
I0302 19:01:30.667402 22895071117440 run.py:483] Algo bellman_ford step 8429 current loss 0.804222, current_train_items 269760.
I0302 19:01:30.685668 22895071117440 run.py:483] Algo bellman_ford step 8430 current loss 0.321477, current_train_items 269792.
I0302 19:01:30.701546 22895071117440 run.py:483] Algo bellman_ford step 8431 current loss 0.582830, current_train_items 269824.
I0302 19:01:30.723436 22895071117440 run.py:483] Algo bellman_ford step 8432 current loss 0.526408, current_train_items 269856.
I0302 19:01:30.753950 22895071117440 run.py:483] Algo bellman_ford step 8433 current loss 0.777088, current_train_items 269888.
I0302 19:01:30.783632 22895071117440 run.py:483] Algo bellman_ford step 8434 current loss 0.725984, current_train_items 269920.
I0302 19:01:30.801289 22895071117440 run.py:483] Algo bellman_ford step 8435 current loss 0.263299, current_train_items 269952.
I0302 19:01:30.816450 22895071117440 run.py:483] Algo bellman_ford step 8436 current loss 0.481517, current_train_items 269984.
I0302 19:01:30.838518 22895071117440 run.py:483] Algo bellman_ford step 8437 current loss 0.560261, current_train_items 270016.
I0302 19:01:30.867377 22895071117440 run.py:483] Algo bellman_ford step 8438 current loss 0.627846, current_train_items 270048.
I0302 19:01:30.897301 22895071117440 run.py:483] Algo bellman_ford step 8439 current loss 0.818358, current_train_items 270080.
I0302 19:01:30.915317 22895071117440 run.py:483] Algo bellman_ford step 8440 current loss 0.325788, current_train_items 270112.
I0302 19:01:30.930862 22895071117440 run.py:483] Algo bellman_ford step 8441 current loss 0.488854, current_train_items 270144.
I0302 19:01:30.953408 22895071117440 run.py:483] Algo bellman_ford step 8442 current loss 0.652360, current_train_items 270176.
I0302 19:01:30.982274 22895071117440 run.py:483] Algo bellman_ford step 8443 current loss 0.736367, current_train_items 270208.
I0302 19:01:31.013685 22895071117440 run.py:483] Algo bellman_ford step 8444 current loss 0.826823, current_train_items 270240.
I0302 19:01:31.031916 22895071117440 run.py:483] Algo bellman_ford step 8445 current loss 0.313346, current_train_items 270272.
I0302 19:01:31.047238 22895071117440 run.py:483] Algo bellman_ford step 8446 current loss 0.463860, current_train_items 270304.
I0302 19:01:31.069226 22895071117440 run.py:483] Algo bellman_ford step 8447 current loss 0.558163, current_train_items 270336.
I0302 19:01:31.098166 22895071117440 run.py:483] Algo bellman_ford step 8448 current loss 0.632570, current_train_items 270368.
I0302 19:01:31.129593 22895071117440 run.py:483] Algo bellman_ford step 8449 current loss 0.784741, current_train_items 270400.
I0302 19:01:31.147563 22895071117440 run.py:483] Algo bellman_ford step 8450 current loss 0.283028, current_train_items 270432.
I0302 19:01:31.155603 22895071117440 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:01:31.155710 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:01:31.171480 22895071117440 run.py:483] Algo bellman_ford step 8451 current loss 0.434811, current_train_items 270464.
I0302 19:01:31.194297 22895071117440 run.py:483] Algo bellman_ford step 8452 current loss 0.548657, current_train_items 270496.
I0302 19:01:31.224400 22895071117440 run.py:483] Algo bellman_ford step 8453 current loss 0.650433, current_train_items 270528.
I0302 19:01:31.257037 22895071117440 run.py:483] Algo bellman_ford step 8454 current loss 0.724164, current_train_items 270560.
I0302 19:01:31.275823 22895071117440 run.py:483] Algo bellman_ford step 8455 current loss 0.249217, current_train_items 270592.
I0302 19:01:31.291615 22895071117440 run.py:483] Algo bellman_ford step 8456 current loss 0.532603, current_train_items 270624.
I0302 19:01:31.314414 22895071117440 run.py:483] Algo bellman_ford step 8457 current loss 0.630898, current_train_items 270656.
I0302 19:01:31.342344 22895071117440 run.py:483] Algo bellman_ford step 8458 current loss 0.702582, current_train_items 270688.
I0302 19:01:31.371394 22895071117440 run.py:483] Algo bellman_ford step 8459 current loss 0.689812, current_train_items 270720.
I0302 19:01:31.390264 22895071117440 run.py:483] Algo bellman_ford step 8460 current loss 0.403189, current_train_items 270752.
I0302 19:01:31.405851 22895071117440 run.py:483] Algo bellman_ford step 8461 current loss 0.414344, current_train_items 270784.
I0302 19:01:31.426806 22895071117440 run.py:483] Algo bellman_ford step 8462 current loss 0.542343, current_train_items 270816.
I0302 19:01:31.454886 22895071117440 run.py:483] Algo bellman_ford step 8463 current loss 0.548913, current_train_items 270848.
I0302 19:01:31.486655 22895071117440 run.py:483] Algo bellman_ford step 8464 current loss 0.720374, current_train_items 270880.
I0302 19:01:31.504811 22895071117440 run.py:483] Algo bellman_ford step 8465 current loss 0.325835, current_train_items 270912.
I0302 19:01:31.520601 22895071117440 run.py:483] Algo bellman_ford step 8466 current loss 0.568102, current_train_items 270944.
I0302 19:01:31.542000 22895071117440 run.py:483] Algo bellman_ford step 8467 current loss 0.559172, current_train_items 270976.
I0302 19:01:31.570506 22895071117440 run.py:483] Algo bellman_ford step 8468 current loss 0.723592, current_train_items 271008.
I0302 19:01:31.600775 22895071117440 run.py:483] Algo bellman_ford step 8469 current loss 0.632171, current_train_items 271040.
I0302 19:01:31.619591 22895071117440 run.py:483] Algo bellman_ford step 8470 current loss 0.308307, current_train_items 271072.
I0302 19:01:31.635773 22895071117440 run.py:483] Algo bellman_ford step 8471 current loss 0.507231, current_train_items 271104.
I0302 19:01:31.657199 22895071117440 run.py:483] Algo bellman_ford step 8472 current loss 0.582504, current_train_items 271136.
I0302 19:01:31.683925 22895071117440 run.py:483] Algo bellman_ford step 8473 current loss 0.519615, current_train_items 271168.
I0302 19:01:31.715734 22895071117440 run.py:483] Algo bellman_ford step 8474 current loss 0.812991, current_train_items 271200.
I0302 19:01:31.734355 22895071117440 run.py:483] Algo bellman_ford step 8475 current loss 0.277641, current_train_items 271232.
I0302 19:01:31.750318 22895071117440 run.py:483] Algo bellman_ford step 8476 current loss 0.481684, current_train_items 271264.
I0302 19:01:31.773751 22895071117440 run.py:483] Algo bellman_ford step 8477 current loss 0.627541, current_train_items 271296.
I0302 19:01:31.802612 22895071117440 run.py:483] Algo bellman_ford step 8478 current loss 0.540211, current_train_items 271328.
I0302 19:01:31.834894 22895071117440 run.py:483] Algo bellman_ford step 8479 current loss 0.617904, current_train_items 271360.
I0302 19:01:31.852762 22895071117440 run.py:483] Algo bellman_ford step 8480 current loss 0.250495, current_train_items 271392.
I0302 19:01:31.868283 22895071117440 run.py:483] Algo bellman_ford step 8481 current loss 0.620389, current_train_items 271424.
I0302 19:01:31.890758 22895071117440 run.py:483] Algo bellman_ford step 8482 current loss 0.767139, current_train_items 271456.
I0302 19:01:31.919935 22895071117440 run.py:483] Algo bellman_ford step 8483 current loss 0.751526, current_train_items 271488.
I0302 19:01:31.949823 22895071117440 run.py:483] Algo bellman_ford step 8484 current loss 0.762957, current_train_items 271520.
I0302 19:01:31.968520 22895071117440 run.py:483] Algo bellman_ford step 8485 current loss 0.348210, current_train_items 271552.
I0302 19:01:31.984159 22895071117440 run.py:483] Algo bellman_ford step 8486 current loss 0.352831, current_train_items 271584.
I0302 19:01:32.005984 22895071117440 run.py:483] Algo bellman_ford step 8487 current loss 0.591635, current_train_items 271616.
I0302 19:01:32.035614 22895071117440 run.py:483] Algo bellman_ford step 8488 current loss 0.660739, current_train_items 271648.
I0302 19:01:32.067096 22895071117440 run.py:483] Algo bellman_ford step 8489 current loss 0.699612, current_train_items 271680.
I0302 19:01:32.085961 22895071117440 run.py:483] Algo bellman_ford step 8490 current loss 0.304630, current_train_items 271712.
I0302 19:01:32.101347 22895071117440 run.py:483] Algo bellman_ford step 8491 current loss 0.428654, current_train_items 271744.
I0302 19:01:32.123778 22895071117440 run.py:483] Algo bellman_ford step 8492 current loss 0.692071, current_train_items 271776.
I0302 19:01:32.152351 22895071117440 run.py:483] Algo bellman_ford step 8493 current loss 0.615109, current_train_items 271808.
I0302 19:01:32.182288 22895071117440 run.py:483] Algo bellman_ford step 8494 current loss 0.697033, current_train_items 271840.
I0302 19:01:32.200594 22895071117440 run.py:483] Algo bellman_ford step 8495 current loss 0.282221, current_train_items 271872.
I0302 19:01:32.216437 22895071117440 run.py:483] Algo bellman_ford step 8496 current loss 0.515696, current_train_items 271904.
I0302 19:01:32.238459 22895071117440 run.py:483] Algo bellman_ford step 8497 current loss 0.477354, current_train_items 271936.
I0302 19:01:32.267805 22895071117440 run.py:483] Algo bellman_ford step 8498 current loss 0.656514, current_train_items 271968.
I0302 19:01:32.297287 22895071117440 run.py:483] Algo bellman_ford step 8499 current loss 0.640305, current_train_items 272000.
I0302 19:01:32.315925 22895071117440 run.py:483] Algo bellman_ford step 8500 current loss 0.265952, current_train_items 272032.
I0302 19:01:32.323839 22895071117440 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:01:32.323974 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:01:32.340451 22895071117440 run.py:483] Algo bellman_ford step 8501 current loss 0.425371, current_train_items 272064.
I0302 19:01:32.363543 22895071117440 run.py:483] Algo bellman_ford step 8502 current loss 0.638680, current_train_items 272096.
I0302 19:01:32.392000 22895071117440 run.py:483] Algo bellman_ford step 8503 current loss 0.675949, current_train_items 272128.
I0302 19:01:32.422513 22895071117440 run.py:483] Algo bellman_ford step 8504 current loss 0.786948, current_train_items 272160.
I0302 19:01:32.441153 22895071117440 run.py:483] Algo bellman_ford step 8505 current loss 0.278533, current_train_items 272192.
I0302 19:01:32.455920 22895071117440 run.py:483] Algo bellman_ford step 8506 current loss 0.391623, current_train_items 272224.
I0302 19:01:32.476471 22895071117440 run.py:483] Algo bellman_ford step 8507 current loss 0.576297, current_train_items 272256.
I0302 19:01:32.506045 22895071117440 run.py:483] Algo bellman_ford step 8508 current loss 0.800381, current_train_items 272288.
I0302 19:01:32.538326 22895071117440 run.py:483] Algo bellman_ford step 8509 current loss 0.879236, current_train_items 272320.
I0302 19:01:32.556824 22895071117440 run.py:483] Algo bellman_ford step 8510 current loss 0.315149, current_train_items 272352.
I0302 19:01:32.571717 22895071117440 run.py:483] Algo bellman_ford step 8511 current loss 0.446274, current_train_items 272384.
I0302 19:01:32.594833 22895071117440 run.py:483] Algo bellman_ford step 8512 current loss 0.531993, current_train_items 272416.
I0302 19:01:32.623373 22895071117440 run.py:483] Algo bellman_ford step 8513 current loss 0.683654, current_train_items 272448.
I0302 19:01:32.654930 22895071117440 run.py:483] Algo bellman_ford step 8514 current loss 0.793105, current_train_items 272480.
I0302 19:01:32.673146 22895071117440 run.py:483] Algo bellman_ford step 8515 current loss 0.282000, current_train_items 272512.
I0302 19:01:32.688585 22895071117440 run.py:483] Algo bellman_ford step 8516 current loss 0.415940, current_train_items 272544.
I0302 19:01:32.710437 22895071117440 run.py:483] Algo bellman_ford step 8517 current loss 0.595286, current_train_items 272576.
I0302 19:01:32.740961 22895071117440 run.py:483] Algo bellman_ford step 8518 current loss 0.674664, current_train_items 272608.
I0302 19:01:32.771011 22895071117440 run.py:483] Algo bellman_ford step 8519 current loss 0.757275, current_train_items 272640.
I0302 19:01:32.789072 22895071117440 run.py:483] Algo bellman_ford step 8520 current loss 0.280478, current_train_items 272672.
I0302 19:01:32.804205 22895071117440 run.py:483] Algo bellman_ford step 8521 current loss 0.466191, current_train_items 272704.
I0302 19:01:32.826256 22895071117440 run.py:483] Algo bellman_ford step 8522 current loss 0.497396, current_train_items 272736.
I0302 19:01:32.855195 22895071117440 run.py:483] Algo bellman_ford step 8523 current loss 0.860728, current_train_items 272768.
I0302 19:01:32.887130 22895071117440 run.py:483] Algo bellman_ford step 8524 current loss 0.789977, current_train_items 272800.
I0302 19:01:32.905701 22895071117440 run.py:483] Algo bellman_ford step 8525 current loss 0.313127, current_train_items 272832.
I0302 19:01:32.921128 22895071117440 run.py:483] Algo bellman_ford step 8526 current loss 0.481221, current_train_items 272864.
I0302 19:01:32.942935 22895071117440 run.py:483] Algo bellman_ford step 8527 current loss 0.629197, current_train_items 272896.
I0302 19:01:32.971396 22895071117440 run.py:483] Algo bellman_ford step 8528 current loss 0.578580, current_train_items 272928.
I0302 19:01:33.003329 22895071117440 run.py:483] Algo bellman_ford step 8529 current loss 0.715590, current_train_items 272960.
I0302 19:01:33.021843 22895071117440 run.py:483] Algo bellman_ford step 8530 current loss 0.220728, current_train_items 272992.
I0302 19:01:33.037292 22895071117440 run.py:483] Algo bellman_ford step 8531 current loss 0.455189, current_train_items 273024.
I0302 19:01:33.058159 22895071117440 run.py:483] Algo bellman_ford step 8532 current loss 0.574660, current_train_items 273056.
I0302 19:01:33.086570 22895071117440 run.py:483] Algo bellman_ford step 8533 current loss 0.730846, current_train_items 273088.
I0302 19:01:33.116746 22895071117440 run.py:483] Algo bellman_ford step 8534 current loss 0.685873, current_train_items 273120.
I0302 19:01:33.134624 22895071117440 run.py:483] Algo bellman_ford step 8535 current loss 0.222829, current_train_items 273152.
I0302 19:01:33.150057 22895071117440 run.py:483] Algo bellman_ford step 8536 current loss 0.478469, current_train_items 273184.
I0302 19:01:33.172106 22895071117440 run.py:483] Algo bellman_ford step 8537 current loss 0.587306, current_train_items 273216.
I0302 19:01:33.201453 22895071117440 run.py:483] Algo bellman_ford step 8538 current loss 0.603159, current_train_items 273248.
I0302 19:01:33.234357 22895071117440 run.py:483] Algo bellman_ford step 8539 current loss 0.884006, current_train_items 273280.
I0302 19:01:33.252692 22895071117440 run.py:483] Algo bellman_ford step 8540 current loss 0.248830, current_train_items 273312.
I0302 19:01:33.268025 22895071117440 run.py:483] Algo bellman_ford step 8541 current loss 0.476560, current_train_items 273344.
I0302 19:01:33.290632 22895071117440 run.py:483] Algo bellman_ford step 8542 current loss 0.599367, current_train_items 273376.
I0302 19:01:33.320248 22895071117440 run.py:483] Algo bellman_ford step 8543 current loss 0.603943, current_train_items 273408.
I0302 19:01:33.351304 22895071117440 run.py:483] Algo bellman_ford step 8544 current loss 0.786078, current_train_items 273440.
I0302 19:01:33.369421 22895071117440 run.py:483] Algo bellman_ford step 8545 current loss 0.215476, current_train_items 273472.
I0302 19:01:33.384318 22895071117440 run.py:483] Algo bellman_ford step 8546 current loss 0.399369, current_train_items 273504.
I0302 19:01:33.406889 22895071117440 run.py:483] Algo bellman_ford step 8547 current loss 0.635057, current_train_items 273536.
I0302 19:01:33.435360 22895071117440 run.py:483] Algo bellman_ford step 8548 current loss 0.617775, current_train_items 273568.
I0302 19:01:33.467546 22895071117440 run.py:483] Algo bellman_ford step 8549 current loss 0.872702, current_train_items 273600.
I0302 19:01:33.485653 22895071117440 run.py:483] Algo bellman_ford step 8550 current loss 0.248103, current_train_items 273632.
I0302 19:01:33.493277 22895071117440 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:01:33.493385 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:33.510229 22895071117440 run.py:483] Algo bellman_ford step 8551 current loss 0.499220, current_train_items 273664.
I0302 19:01:33.533098 22895071117440 run.py:483] Algo bellman_ford step 8552 current loss 0.573097, current_train_items 273696.
I0302 19:01:33.563219 22895071117440 run.py:483] Algo bellman_ford step 8553 current loss 0.661231, current_train_items 273728.
I0302 19:01:33.592288 22895071117440 run.py:483] Algo bellman_ford step 8554 current loss 0.644492, current_train_items 273760.
I0302 19:01:33.611069 22895071117440 run.py:483] Algo bellman_ford step 8555 current loss 0.436010, current_train_items 273792.
I0302 19:01:33.626539 22895071117440 run.py:483] Algo bellman_ford step 8556 current loss 0.583481, current_train_items 273824.
I0302 19:01:33.648277 22895071117440 run.py:483] Algo bellman_ford step 8557 current loss 0.681870, current_train_items 273856.
I0302 19:01:33.677381 22895071117440 run.py:483] Algo bellman_ford step 8558 current loss 0.657466, current_train_items 273888.
I0302 19:01:33.709188 22895071117440 run.py:483] Algo bellman_ford step 8559 current loss 0.716090, current_train_items 273920.
I0302 19:01:33.727566 22895071117440 run.py:483] Algo bellman_ford step 8560 current loss 0.259875, current_train_items 273952.
I0302 19:01:33.743587 22895071117440 run.py:483] Algo bellman_ford step 8561 current loss 0.417144, current_train_items 273984.
I0302 19:01:33.765506 22895071117440 run.py:483] Algo bellman_ford step 8562 current loss 0.690519, current_train_items 274016.
I0302 19:01:33.795379 22895071117440 run.py:483] Algo bellman_ford step 8563 current loss 0.736079, current_train_items 274048.
I0302 19:01:33.825306 22895071117440 run.py:483] Algo bellman_ford step 8564 current loss 0.776449, current_train_items 274080.
I0302 19:01:33.843340 22895071117440 run.py:483] Algo bellman_ford step 8565 current loss 0.292398, current_train_items 274112.
I0302 19:01:33.859241 22895071117440 run.py:483] Algo bellman_ford step 8566 current loss 0.454102, current_train_items 274144.
I0302 19:01:33.882461 22895071117440 run.py:483] Algo bellman_ford step 8567 current loss 0.599538, current_train_items 274176.
I0302 19:01:33.911434 22895071117440 run.py:483] Algo bellman_ford step 8568 current loss 0.719081, current_train_items 274208.
I0302 19:01:33.943081 22895071117440 run.py:483] Algo bellman_ford step 8569 current loss 0.806080, current_train_items 274240.
I0302 19:01:33.961372 22895071117440 run.py:483] Algo bellman_ford step 8570 current loss 0.276571, current_train_items 274272.
I0302 19:01:33.977035 22895071117440 run.py:483] Algo bellman_ford step 8571 current loss 0.561504, current_train_items 274304.
I0302 19:01:33.998398 22895071117440 run.py:483] Algo bellman_ford step 8572 current loss 0.538878, current_train_items 274336.
I0302 19:01:34.027311 22895071117440 run.py:483] Algo bellman_ford step 8573 current loss 0.619953, current_train_items 274368.
I0302 19:01:34.057152 22895071117440 run.py:483] Algo bellman_ford step 8574 current loss 0.705163, current_train_items 274400.
I0302 19:01:34.075622 22895071117440 run.py:483] Algo bellman_ford step 8575 current loss 0.350518, current_train_items 274432.
I0302 19:01:34.091201 22895071117440 run.py:483] Algo bellman_ford step 8576 current loss 0.427077, current_train_items 274464.
I0302 19:01:34.113173 22895071117440 run.py:483] Algo bellman_ford step 8577 current loss 0.543650, current_train_items 274496.
I0302 19:01:34.142576 22895071117440 run.py:483] Algo bellman_ford step 8578 current loss 0.675212, current_train_items 274528.
I0302 19:01:34.173875 22895071117440 run.py:483] Algo bellman_ford step 8579 current loss 0.781449, current_train_items 274560.
I0302 19:01:34.192069 22895071117440 run.py:483] Algo bellman_ford step 8580 current loss 0.303160, current_train_items 274592.
I0302 19:01:34.207448 22895071117440 run.py:483] Algo bellman_ford step 8581 current loss 0.430093, current_train_items 274624.
I0302 19:01:34.228347 22895071117440 run.py:483] Algo bellman_ford step 8582 current loss 0.556715, current_train_items 274656.
I0302 19:01:34.257327 22895071117440 run.py:483] Algo bellman_ford step 8583 current loss 0.686673, current_train_items 274688.
I0302 19:01:34.289414 22895071117440 run.py:483] Algo bellman_ford step 8584 current loss 0.786364, current_train_items 274720.
I0302 19:01:34.307489 22895071117440 run.py:483] Algo bellman_ford step 8585 current loss 0.281797, current_train_items 274752.
I0302 19:01:34.323107 22895071117440 run.py:483] Algo bellman_ford step 8586 current loss 0.437737, current_train_items 274784.
I0302 19:01:34.346087 22895071117440 run.py:483] Algo bellman_ford step 8587 current loss 0.666669, current_train_items 274816.
I0302 19:01:34.375114 22895071117440 run.py:483] Algo bellman_ford step 8588 current loss 0.587010, current_train_items 274848.
I0302 19:01:34.405373 22895071117440 run.py:483] Algo bellman_ford step 8589 current loss 0.772862, current_train_items 274880.
I0302 19:01:34.423989 22895071117440 run.py:483] Algo bellman_ford step 8590 current loss 0.332875, current_train_items 274912.
I0302 19:01:34.438960 22895071117440 run.py:483] Algo bellman_ford step 8591 current loss 0.421724, current_train_items 274944.
I0302 19:01:34.460603 22895071117440 run.py:483] Algo bellman_ford step 8592 current loss 0.647163, current_train_items 274976.
I0302 19:01:34.490092 22895071117440 run.py:483] Algo bellman_ford step 8593 current loss 0.799223, current_train_items 275008.
I0302 19:01:34.521422 22895071117440 run.py:483] Algo bellman_ford step 8594 current loss 0.895329, current_train_items 275040.
I0302 19:01:34.539535 22895071117440 run.py:483] Algo bellman_ford step 8595 current loss 0.265278, current_train_items 275072.
I0302 19:01:34.554655 22895071117440 run.py:483] Algo bellman_ford step 8596 current loss 0.446996, current_train_items 275104.
I0302 19:01:34.576391 22895071117440 run.py:483] Algo bellman_ford step 8597 current loss 0.567743, current_train_items 275136.
I0302 19:01:34.603926 22895071117440 run.py:483] Algo bellman_ford step 8598 current loss 0.568748, current_train_items 275168.
I0302 19:01:34.635738 22895071117440 run.py:483] Algo bellman_ford step 8599 current loss 0.876956, current_train_items 275200.
I0302 19:01:34.654088 22895071117440 run.py:483] Algo bellman_ford step 8600 current loss 0.334907, current_train_items 275232.
I0302 19:01:34.661908 22895071117440 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:01:34.662018 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:01:34.678269 22895071117440 run.py:483] Algo bellman_ford step 8601 current loss 0.512198, current_train_items 275264.
I0302 19:01:34.701448 22895071117440 run.py:483] Algo bellman_ford step 8602 current loss 0.677786, current_train_items 275296.
I0302 19:01:34.731726 22895071117440 run.py:483] Algo bellman_ford step 8603 current loss 0.602011, current_train_items 275328.
I0302 19:01:34.763437 22895071117440 run.py:483] Algo bellman_ford step 8604 current loss 0.771829, current_train_items 275360.
I0302 19:01:34.782235 22895071117440 run.py:483] Algo bellman_ford step 8605 current loss 0.288531, current_train_items 275392.
I0302 19:01:34.797111 22895071117440 run.py:483] Algo bellman_ford step 8606 current loss 0.414469, current_train_items 275424.
I0302 19:01:34.818738 22895071117440 run.py:483] Algo bellman_ford step 8607 current loss 0.576422, current_train_items 275456.
I0302 19:01:34.847478 22895071117440 run.py:483] Algo bellman_ford step 8608 current loss 0.618478, current_train_items 275488.
I0302 19:01:34.880334 22895071117440 run.py:483] Algo bellman_ford step 8609 current loss 0.791408, current_train_items 275520.
I0302 19:01:34.898643 22895071117440 run.py:483] Algo bellman_ford step 8610 current loss 0.288257, current_train_items 275552.
I0302 19:01:34.914299 22895071117440 run.py:483] Algo bellman_ford step 8611 current loss 0.444298, current_train_items 275584.
I0302 19:01:34.937151 22895071117440 run.py:483] Algo bellman_ford step 8612 current loss 0.635059, current_train_items 275616.
I0302 19:01:34.966658 22895071117440 run.py:483] Algo bellman_ford step 8613 current loss 0.610909, current_train_items 275648.
I0302 19:01:34.995647 22895071117440 run.py:483] Algo bellman_ford step 8614 current loss 0.695034, current_train_items 275680.
I0302 19:01:35.013612 22895071117440 run.py:483] Algo bellman_ford step 8615 current loss 0.241339, current_train_items 275712.
I0302 19:01:35.029089 22895071117440 run.py:483] Algo bellman_ford step 8616 current loss 0.442520, current_train_items 275744.
I0302 19:01:35.051162 22895071117440 run.py:483] Algo bellman_ford step 8617 current loss 0.506078, current_train_items 275776.
I0302 19:01:35.080852 22895071117440 run.py:483] Algo bellman_ford step 8618 current loss 0.706048, current_train_items 275808.
I0302 19:01:35.112109 22895071117440 run.py:483] Algo bellman_ford step 8619 current loss 0.776315, current_train_items 275840.
I0302 19:01:35.130158 22895071117440 run.py:483] Algo bellman_ford step 8620 current loss 0.287333, current_train_items 275872.
I0302 19:01:35.145621 22895071117440 run.py:483] Algo bellman_ford step 8621 current loss 0.431465, current_train_items 275904.
I0302 19:01:35.168608 22895071117440 run.py:483] Algo bellman_ford step 8622 current loss 0.604733, current_train_items 275936.
I0302 19:01:35.198651 22895071117440 run.py:483] Algo bellman_ford step 8623 current loss 0.609645, current_train_items 275968.
I0302 19:01:35.229727 22895071117440 run.py:483] Algo bellman_ford step 8624 current loss 0.664902, current_train_items 276000.
I0302 19:01:35.248132 22895071117440 run.py:483] Algo bellman_ford step 8625 current loss 0.292837, current_train_items 276032.
I0302 19:01:35.263669 22895071117440 run.py:483] Algo bellman_ford step 8626 current loss 0.400204, current_train_items 276064.
I0302 19:01:35.285869 22895071117440 run.py:483] Algo bellman_ford step 8627 current loss 0.532115, current_train_items 276096.
I0302 19:01:35.316323 22895071117440 run.py:483] Algo bellman_ford step 8628 current loss 0.751017, current_train_items 276128.
I0302 19:01:35.346886 22895071117440 run.py:483] Algo bellman_ford step 8629 current loss 0.924478, current_train_items 276160.
I0302 19:01:35.365404 22895071117440 run.py:483] Algo bellman_ford step 8630 current loss 0.318700, current_train_items 276192.
I0302 19:01:35.380970 22895071117440 run.py:483] Algo bellman_ford step 8631 current loss 0.410830, current_train_items 276224.
I0302 19:01:35.403183 22895071117440 run.py:483] Algo bellman_ford step 8632 current loss 0.687431, current_train_items 276256.
I0302 19:01:35.432114 22895071117440 run.py:483] Algo bellman_ford step 8633 current loss 0.522907, current_train_items 276288.
I0302 19:01:35.463057 22895071117440 run.py:483] Algo bellman_ford step 8634 current loss 0.771928, current_train_items 276320.
I0302 19:01:35.481555 22895071117440 run.py:483] Algo bellman_ford step 8635 current loss 0.336333, current_train_items 276352.
I0302 19:01:35.496943 22895071117440 run.py:483] Algo bellman_ford step 8636 current loss 0.413560, current_train_items 276384.
I0302 19:01:35.519576 22895071117440 run.py:483] Algo bellman_ford step 8637 current loss 0.574765, current_train_items 276416.
I0302 19:01:35.548070 22895071117440 run.py:483] Algo bellman_ford step 8638 current loss 0.608529, current_train_items 276448.
I0302 19:01:35.579858 22895071117440 run.py:483] Algo bellman_ford step 8639 current loss 0.759627, current_train_items 276480.
I0302 19:01:35.598116 22895071117440 run.py:483] Algo bellman_ford step 8640 current loss 0.305977, current_train_items 276512.
I0302 19:01:35.613922 22895071117440 run.py:483] Algo bellman_ford step 8641 current loss 0.470876, current_train_items 276544.
I0302 19:01:35.636289 22895071117440 run.py:483] Algo bellman_ford step 8642 current loss 0.477872, current_train_items 276576.
I0302 19:01:35.665662 22895071117440 run.py:483] Algo bellman_ford step 8643 current loss 0.665622, current_train_items 276608.
I0302 19:01:35.697046 22895071117440 run.py:483] Algo bellman_ford step 8644 current loss 0.795453, current_train_items 276640.
I0302 19:01:35.715108 22895071117440 run.py:483] Algo bellman_ford step 8645 current loss 0.356438, current_train_items 276672.
I0302 19:01:35.730501 22895071117440 run.py:483] Algo bellman_ford step 8646 current loss 0.486104, current_train_items 276704.
I0302 19:01:35.752609 22895071117440 run.py:483] Algo bellman_ford step 8647 current loss 0.625111, current_train_items 276736.
I0302 19:01:35.782823 22895071117440 run.py:483] Algo bellman_ford step 8648 current loss 0.669711, current_train_items 276768.
I0302 19:01:35.813710 22895071117440 run.py:483] Algo bellman_ford step 8649 current loss 0.732983, current_train_items 276800.
I0302 19:01:35.831562 22895071117440 run.py:483] Algo bellman_ford step 8650 current loss 0.394966, current_train_items 276832.
I0302 19:01:35.839506 22895071117440 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:01:35.839611 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:35.855699 22895071117440 run.py:483] Algo bellman_ford step 8651 current loss 0.374660, current_train_items 276864.
I0302 19:01:35.878204 22895071117440 run.py:483] Algo bellman_ford step 8652 current loss 0.496103, current_train_items 276896.
I0302 19:01:35.907785 22895071117440 run.py:483] Algo bellman_ford step 8653 current loss 0.565040, current_train_items 276928.
I0302 19:01:35.940532 22895071117440 run.py:483] Algo bellman_ford step 8654 current loss 0.747972, current_train_items 276960.
I0302 19:01:35.959198 22895071117440 run.py:483] Algo bellman_ford step 8655 current loss 0.255440, current_train_items 276992.
I0302 19:01:35.974575 22895071117440 run.py:483] Algo bellman_ford step 8656 current loss 0.440814, current_train_items 277024.
I0302 19:01:35.998034 22895071117440 run.py:483] Algo bellman_ford step 8657 current loss 0.621511, current_train_items 277056.
I0302 19:01:36.027369 22895071117440 run.py:483] Algo bellman_ford step 8658 current loss 0.582345, current_train_items 277088.
I0302 19:01:36.058686 22895071117440 run.py:483] Algo bellman_ford step 8659 current loss 0.711832, current_train_items 277120.
I0302 19:01:36.077036 22895071117440 run.py:483] Algo bellman_ford step 8660 current loss 0.244320, current_train_items 277152.
I0302 19:01:36.092531 22895071117440 run.py:483] Algo bellman_ford step 8661 current loss 0.433208, current_train_items 277184.
I0302 19:01:36.113912 22895071117440 run.py:483] Algo bellman_ford step 8662 current loss 0.618104, current_train_items 277216.
I0302 19:01:36.142410 22895071117440 run.py:483] Algo bellman_ford step 8663 current loss 0.624623, current_train_items 277248.
I0302 19:01:36.174797 22895071117440 run.py:483] Algo bellman_ford step 8664 current loss 0.616404, current_train_items 277280.
I0302 19:01:36.193048 22895071117440 run.py:483] Algo bellman_ford step 8665 current loss 0.340469, current_train_items 277312.
I0302 19:01:36.207872 22895071117440 run.py:483] Algo bellman_ford step 8666 current loss 0.389539, current_train_items 277344.
I0302 19:01:36.230021 22895071117440 run.py:483] Algo bellman_ford step 8667 current loss 0.585154, current_train_items 277376.
I0302 19:01:36.258092 22895071117440 run.py:483] Algo bellman_ford step 8668 current loss 0.624371, current_train_items 277408.
I0302 19:01:36.288326 22895071117440 run.py:483] Algo bellman_ford step 8669 current loss 0.667062, current_train_items 277440.
I0302 19:01:36.306693 22895071117440 run.py:483] Algo bellman_ford step 8670 current loss 0.331596, current_train_items 277472.
I0302 19:01:36.322618 22895071117440 run.py:483] Algo bellman_ford step 8671 current loss 0.456604, current_train_items 277504.
I0302 19:01:36.344145 22895071117440 run.py:483] Algo bellman_ford step 8672 current loss 0.579520, current_train_items 277536.
I0302 19:01:36.373138 22895071117440 run.py:483] Algo bellman_ford step 8673 current loss 0.683856, current_train_items 277568.
I0302 19:01:36.402439 22895071117440 run.py:483] Algo bellman_ford step 8674 current loss 0.930867, current_train_items 277600.
I0302 19:01:36.420831 22895071117440 run.py:483] Algo bellman_ford step 8675 current loss 0.299424, current_train_items 277632.
I0302 19:01:36.436362 22895071117440 run.py:483] Algo bellman_ford step 8676 current loss 0.480643, current_train_items 277664.
I0302 19:01:36.458693 22895071117440 run.py:483] Algo bellman_ford step 8677 current loss 0.622344, current_train_items 277696.
I0302 19:01:36.487740 22895071117440 run.py:483] Algo bellman_ford step 8678 current loss 0.650832, current_train_items 277728.
I0302 19:01:36.518068 22895071117440 run.py:483] Algo bellman_ford step 8679 current loss 0.718438, current_train_items 277760.
I0302 19:01:36.536198 22895071117440 run.py:483] Algo bellman_ford step 8680 current loss 0.319792, current_train_items 277792.
I0302 19:01:36.551674 22895071117440 run.py:483] Algo bellman_ford step 8681 current loss 0.522436, current_train_items 277824.
I0302 19:01:36.573789 22895071117440 run.py:483] Algo bellman_ford step 8682 current loss 0.520702, current_train_items 277856.
I0302 19:01:36.602578 22895071117440 run.py:483] Algo bellman_ford step 8683 current loss 0.571299, current_train_items 277888.
I0302 19:01:36.633144 22895071117440 run.py:483] Algo bellman_ford step 8684 current loss 0.699935, current_train_items 277920.
I0302 19:01:36.651556 22895071117440 run.py:483] Algo bellman_ford step 8685 current loss 0.329897, current_train_items 277952.
I0302 19:01:36.666943 22895071117440 run.py:483] Algo bellman_ford step 8686 current loss 0.503080, current_train_items 277984.
I0302 19:01:36.689378 22895071117440 run.py:483] Algo bellman_ford step 8687 current loss 0.644997, current_train_items 278016.
I0302 19:01:36.718814 22895071117440 run.py:483] Algo bellman_ford step 8688 current loss 0.670083, current_train_items 278048.
I0302 19:01:36.749140 22895071117440 run.py:483] Algo bellman_ford step 8689 current loss 0.644414, current_train_items 278080.
I0302 19:01:36.767129 22895071117440 run.py:483] Algo bellman_ford step 8690 current loss 0.246147, current_train_items 278112.
I0302 19:01:36.782936 22895071117440 run.py:483] Algo bellman_ford step 8691 current loss 0.420509, current_train_items 278144.
I0302 19:01:36.804531 22895071117440 run.py:483] Algo bellman_ford step 8692 current loss 0.601565, current_train_items 278176.
I0302 19:01:36.833202 22895071117440 run.py:483] Algo bellman_ford step 8693 current loss 0.665905, current_train_items 278208.
I0302 19:01:36.863932 22895071117440 run.py:483] Algo bellman_ford step 8694 current loss 0.866205, current_train_items 278240.
I0302 19:01:36.882161 22895071117440 run.py:483] Algo bellman_ford step 8695 current loss 0.299785, current_train_items 278272.
I0302 19:01:36.897325 22895071117440 run.py:483] Algo bellman_ford step 8696 current loss 0.409028, current_train_items 278304.
I0302 19:01:36.919486 22895071117440 run.py:483] Algo bellman_ford step 8697 current loss 0.567243, current_train_items 278336.
I0302 19:01:36.948706 22895071117440 run.py:483] Algo bellman_ford step 8698 current loss 0.639008, current_train_items 278368.
I0302 19:01:36.981042 22895071117440 run.py:483] Algo bellman_ford step 8699 current loss 0.690371, current_train_items 278400.
I0302 19:01:36.999434 22895071117440 run.py:483] Algo bellman_ford step 8700 current loss 0.271669, current_train_items 278432.
I0302 19:01:37.007310 22895071117440 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:01:37.007414 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:01:37.023633 22895071117440 run.py:483] Algo bellman_ford step 8701 current loss 0.414189, current_train_items 278464.
I0302 19:01:37.046118 22895071117440 run.py:483] Algo bellman_ford step 8702 current loss 0.540170, current_train_items 278496.
I0302 19:01:37.075987 22895071117440 run.py:483] Algo bellman_ford step 8703 current loss 0.666374, current_train_items 278528.
I0302 19:01:37.109410 22895071117440 run.py:483] Algo bellman_ford step 8704 current loss 0.711365, current_train_items 278560.
I0302 19:01:37.128118 22895071117440 run.py:483] Algo bellman_ford step 8705 current loss 0.291197, current_train_items 278592.
I0302 19:01:37.143240 22895071117440 run.py:483] Algo bellman_ford step 8706 current loss 0.490075, current_train_items 278624.
I0302 19:01:37.165313 22895071117440 run.py:483] Algo bellman_ford step 8707 current loss 0.581277, current_train_items 278656.
I0302 19:01:37.195197 22895071117440 run.py:483] Algo bellman_ford step 8708 current loss 0.603253, current_train_items 278688.
I0302 19:01:37.226207 22895071117440 run.py:483] Algo bellman_ford step 8709 current loss 0.698852, current_train_items 278720.
I0302 19:01:37.244246 22895071117440 run.py:483] Algo bellman_ford step 8710 current loss 0.361290, current_train_items 278752.
I0302 19:01:37.259742 22895071117440 run.py:483] Algo bellman_ford step 8711 current loss 0.551222, current_train_items 278784.
I0302 19:01:37.281933 22895071117440 run.py:483] Algo bellman_ford step 8712 current loss 0.565021, current_train_items 278816.
I0302 19:01:37.313019 22895071117440 run.py:483] Algo bellman_ford step 8713 current loss 0.759358, current_train_items 278848.
I0302 19:01:37.342872 22895071117440 run.py:483] Algo bellman_ford step 8714 current loss 0.710797, current_train_items 278880.
I0302 19:01:37.361009 22895071117440 run.py:483] Algo bellman_ford step 8715 current loss 0.274178, current_train_items 278912.
I0302 19:01:37.376676 22895071117440 run.py:483] Algo bellman_ford step 8716 current loss 0.453388, current_train_items 278944.
I0302 19:01:37.398133 22895071117440 run.py:483] Algo bellman_ford step 8717 current loss 0.543644, current_train_items 278976.
I0302 19:01:37.426516 22895071117440 run.py:483] Algo bellman_ford step 8718 current loss 0.609261, current_train_items 279008.
I0302 19:01:37.456716 22895071117440 run.py:483] Algo bellman_ford step 8719 current loss 0.755970, current_train_items 279040.
I0302 19:01:37.474689 22895071117440 run.py:483] Algo bellman_ford step 8720 current loss 0.328602, current_train_items 279072.
I0302 19:01:37.490468 22895071117440 run.py:483] Algo bellman_ford step 8721 current loss 0.565339, current_train_items 279104.
I0302 19:01:37.513092 22895071117440 run.py:483] Algo bellman_ford step 8722 current loss 0.631609, current_train_items 279136.
I0302 19:01:37.541548 22895071117440 run.py:483] Algo bellman_ford step 8723 current loss 0.663396, current_train_items 279168.
I0302 19:01:37.572049 22895071117440 run.py:483] Algo bellman_ford step 8724 current loss 0.736979, current_train_items 279200.
I0302 19:01:37.590556 22895071117440 run.py:483] Algo bellman_ford step 8725 current loss 0.284813, current_train_items 279232.
I0302 19:01:37.606084 22895071117440 run.py:483] Algo bellman_ford step 8726 current loss 0.491358, current_train_items 279264.
I0302 19:01:37.628072 22895071117440 run.py:483] Algo bellman_ford step 8727 current loss 0.533320, current_train_items 279296.
I0302 19:01:37.656728 22895071117440 run.py:483] Algo bellman_ford step 8728 current loss 0.571474, current_train_items 279328.
I0302 19:01:37.687503 22895071117440 run.py:483] Algo bellman_ford step 8729 current loss 0.730883, current_train_items 279360.
I0302 19:01:37.705756 22895071117440 run.py:483] Algo bellman_ford step 8730 current loss 0.288311, current_train_items 279392.
I0302 19:01:37.720666 22895071117440 run.py:483] Algo bellman_ford step 8731 current loss 0.353619, current_train_items 279424.
I0302 19:01:37.743036 22895071117440 run.py:483] Algo bellman_ford step 8732 current loss 0.551788, current_train_items 279456.
I0302 19:01:37.772453 22895071117440 run.py:483] Algo bellman_ford step 8733 current loss 0.646349, current_train_items 279488.
I0302 19:01:37.805539 22895071117440 run.py:483] Algo bellman_ford step 8734 current loss 0.689403, current_train_items 279520.
I0302 19:01:37.823578 22895071117440 run.py:483] Algo bellman_ford step 8735 current loss 0.319670, current_train_items 279552.
I0302 19:01:37.838878 22895071117440 run.py:483] Algo bellman_ford step 8736 current loss 0.445786, current_train_items 279584.
I0302 19:01:37.862190 22895071117440 run.py:483] Algo bellman_ford step 8737 current loss 0.605877, current_train_items 279616.
I0302 19:01:37.891036 22895071117440 run.py:483] Algo bellman_ford step 8738 current loss 0.663897, current_train_items 279648.
I0302 19:01:37.923100 22895071117440 run.py:483] Algo bellman_ford step 8739 current loss 0.849731, current_train_items 279680.
I0302 19:01:37.941350 22895071117440 run.py:483] Algo bellman_ford step 8740 current loss 0.354867, current_train_items 279712.
I0302 19:01:37.956577 22895071117440 run.py:483] Algo bellman_ford step 8741 current loss 0.461705, current_train_items 279744.
I0302 19:01:37.978907 22895071117440 run.py:483] Algo bellman_ford step 8742 current loss 0.627449, current_train_items 279776.
I0302 19:01:38.008589 22895071117440 run.py:483] Algo bellman_ford step 8743 current loss 0.643526, current_train_items 279808.
I0302 19:01:38.039805 22895071117440 run.py:483] Algo bellman_ford step 8744 current loss 0.613837, current_train_items 279840.
I0302 19:01:38.057801 22895071117440 run.py:483] Algo bellman_ford step 8745 current loss 0.329621, current_train_items 279872.
I0302 19:01:38.073422 22895071117440 run.py:483] Algo bellman_ford step 8746 current loss 0.488678, current_train_items 279904.
I0302 19:01:38.095364 22895071117440 run.py:483] Algo bellman_ford step 8747 current loss 0.574731, current_train_items 279936.
I0302 19:01:38.125582 22895071117440 run.py:483] Algo bellman_ford step 8748 current loss 0.651068, current_train_items 279968.
I0302 19:01:38.157846 22895071117440 run.py:483] Algo bellman_ford step 8749 current loss 0.725739, current_train_items 280000.
I0302 19:01:38.175771 22895071117440 run.py:483] Algo bellman_ford step 8750 current loss 0.299039, current_train_items 280032.
I0302 19:01:38.183624 22895071117440 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:01:38.183729 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:01:38.199959 22895071117440 run.py:483] Algo bellman_ford step 8751 current loss 0.417753, current_train_items 280064.
I0302 19:01:38.221835 22895071117440 run.py:483] Algo bellman_ford step 8752 current loss 0.532929, current_train_items 280096.
I0302 19:01:38.251689 22895071117440 run.py:483] Algo bellman_ford step 8753 current loss 0.684850, current_train_items 280128.
I0302 19:01:38.283657 22895071117440 run.py:483] Algo bellman_ford step 8754 current loss 0.665665, current_train_items 280160.
I0302 19:01:38.302032 22895071117440 run.py:483] Algo bellman_ford step 8755 current loss 0.286091, current_train_items 280192.
I0302 19:01:38.317221 22895071117440 run.py:483] Algo bellman_ford step 8756 current loss 0.429217, current_train_items 280224.
I0302 19:01:38.340343 22895071117440 run.py:483] Algo bellman_ford step 8757 current loss 0.577274, current_train_items 280256.
I0302 19:01:38.368738 22895071117440 run.py:483] Algo bellman_ford step 8758 current loss 0.675369, current_train_items 280288.
I0302 19:01:38.400082 22895071117440 run.py:483] Algo bellman_ford step 8759 current loss 0.691340, current_train_items 280320.
I0302 19:01:38.418356 22895071117440 run.py:483] Algo bellman_ford step 8760 current loss 0.314467, current_train_items 280352.
I0302 19:01:38.434229 22895071117440 run.py:483] Algo bellman_ford step 8761 current loss 0.499662, current_train_items 280384.
I0302 19:01:38.456689 22895071117440 run.py:483] Algo bellman_ford step 8762 current loss 0.587498, current_train_items 280416.
I0302 19:01:38.484778 22895071117440 run.py:483] Algo bellman_ford step 8763 current loss 0.659260, current_train_items 280448.
I0302 19:01:38.517400 22895071117440 run.py:483] Algo bellman_ford step 8764 current loss 0.765119, current_train_items 280480.
I0302 19:01:38.535094 22895071117440 run.py:483] Algo bellman_ford step 8765 current loss 0.263199, current_train_items 280512.
I0302 19:01:38.550857 22895071117440 run.py:483] Algo bellman_ford step 8766 current loss 0.435583, current_train_items 280544.
I0302 19:01:38.571431 22895071117440 run.py:483] Algo bellman_ford step 8767 current loss 0.496399, current_train_items 280576.
I0302 19:01:38.599523 22895071117440 run.py:483] Algo bellman_ford step 8768 current loss 0.676973, current_train_items 280608.
I0302 19:01:38.628556 22895071117440 run.py:483] Algo bellman_ford step 8769 current loss 0.582287, current_train_items 280640.
I0302 19:01:38.646873 22895071117440 run.py:483] Algo bellman_ford step 8770 current loss 0.244852, current_train_items 280672.
I0302 19:01:38.663018 22895071117440 run.py:483] Algo bellman_ford step 8771 current loss 0.435625, current_train_items 280704.
I0302 19:01:38.685288 22895071117440 run.py:483] Algo bellman_ford step 8772 current loss 0.589245, current_train_items 280736.
I0302 19:01:38.714465 22895071117440 run.py:483] Algo bellman_ford step 8773 current loss 0.669889, current_train_items 280768.
I0302 19:01:38.744884 22895071117440 run.py:483] Algo bellman_ford step 8774 current loss 0.660356, current_train_items 280800.
I0302 19:01:38.763166 22895071117440 run.py:483] Algo bellman_ford step 8775 current loss 0.344802, current_train_items 280832.
I0302 19:01:38.778765 22895071117440 run.py:483] Algo bellman_ford step 8776 current loss 0.457431, current_train_items 280864.
I0302 19:01:38.800160 22895071117440 run.py:483] Algo bellman_ford step 8777 current loss 0.561122, current_train_items 280896.
I0302 19:01:38.828494 22895071117440 run.py:483] Algo bellman_ford step 8778 current loss 0.597436, current_train_items 280928.
I0302 19:01:38.860280 22895071117440 run.py:483] Algo bellman_ford step 8779 current loss 0.632490, current_train_items 280960.
I0302 19:01:38.878058 22895071117440 run.py:483] Algo bellman_ford step 8780 current loss 0.344802, current_train_items 280992.
I0302 19:01:38.893292 22895071117440 run.py:483] Algo bellman_ford step 8781 current loss 0.523370, current_train_items 281024.
I0302 19:01:38.915037 22895071117440 run.py:483] Algo bellman_ford step 8782 current loss 0.616522, current_train_items 281056.
I0302 19:01:38.944499 22895071117440 run.py:483] Algo bellman_ford step 8783 current loss 0.674414, current_train_items 281088.
I0302 19:01:38.975737 22895071117440 run.py:483] Algo bellman_ford step 8784 current loss 0.752313, current_train_items 281120.
I0302 19:01:38.994049 22895071117440 run.py:483] Algo bellman_ford step 8785 current loss 0.295718, current_train_items 281152.
I0302 19:01:39.009625 22895071117440 run.py:483] Algo bellman_ford step 8786 current loss 0.474629, current_train_items 281184.
I0302 19:01:39.031734 22895071117440 run.py:483] Algo bellman_ford step 8787 current loss 0.563954, current_train_items 281216.
I0302 19:01:39.060060 22895071117440 run.py:483] Algo bellman_ford step 8788 current loss 0.605688, current_train_items 281248.
I0302 19:01:39.091296 22895071117440 run.py:483] Algo bellman_ford step 8789 current loss 0.694621, current_train_items 281280.
I0302 19:01:39.109558 22895071117440 run.py:483] Algo bellman_ford step 8790 current loss 0.325902, current_train_items 281312.
I0302 19:01:39.125633 22895071117440 run.py:483] Algo bellman_ford step 8791 current loss 0.534054, current_train_items 281344.
I0302 19:01:39.147485 22895071117440 run.py:483] Algo bellman_ford step 8792 current loss 0.526022, current_train_items 281376.
I0302 19:01:39.176456 22895071117440 run.py:483] Algo bellman_ford step 8793 current loss 0.605837, current_train_items 281408.
I0302 19:01:39.208377 22895071117440 run.py:483] Algo bellman_ford step 8794 current loss 0.704557, current_train_items 281440.
I0302 19:01:39.226191 22895071117440 run.py:483] Algo bellman_ford step 8795 current loss 0.302495, current_train_items 281472.
I0302 19:01:39.241477 22895071117440 run.py:483] Algo bellman_ford step 8796 current loss 0.481283, current_train_items 281504.
I0302 19:01:39.263100 22895071117440 run.py:483] Algo bellman_ford step 8797 current loss 0.553974, current_train_items 281536.
I0302 19:01:39.292513 22895071117440 run.py:483] Algo bellman_ford step 8798 current loss 0.738756, current_train_items 281568.
I0302 19:01:39.325040 22895071117440 run.py:483] Algo bellman_ford step 8799 current loss 0.725101, current_train_items 281600.
I0302 19:01:39.343095 22895071117440 run.py:483] Algo bellman_ford step 8800 current loss 0.250417, current_train_items 281632.
I0302 19:01:39.350874 22895071117440 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:01:39.350991 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:01:39.366732 22895071117440 run.py:483] Algo bellman_ford step 8801 current loss 0.394402, current_train_items 281664.
I0302 19:01:39.388994 22895071117440 run.py:483] Algo bellman_ford step 8802 current loss 0.648993, current_train_items 281696.
I0302 19:01:39.419274 22895071117440 run.py:483] Algo bellman_ford step 8803 current loss 0.772148, current_train_items 281728.
I0302 19:01:39.450925 22895071117440 run.py:483] Algo bellman_ford step 8804 current loss 0.771433, current_train_items 281760.
I0302 19:01:39.469366 22895071117440 run.py:483] Algo bellman_ford step 8805 current loss 0.288484, current_train_items 281792.
I0302 19:01:39.484608 22895071117440 run.py:483] Algo bellman_ford step 8806 current loss 0.435534, current_train_items 281824.
I0302 19:01:39.506648 22895071117440 run.py:483] Algo bellman_ford step 8807 current loss 0.585302, current_train_items 281856.
I0302 19:01:39.535003 22895071117440 run.py:483] Algo bellman_ford step 8808 current loss 0.630699, current_train_items 281888.
I0302 19:01:39.568147 22895071117440 run.py:483] Algo bellman_ford step 8809 current loss 0.740319, current_train_items 281920.
I0302 19:01:39.586231 22895071117440 run.py:483] Algo bellman_ford step 8810 current loss 0.335648, current_train_items 281952.
I0302 19:01:39.601851 22895071117440 run.py:483] Algo bellman_ford step 8811 current loss 0.475230, current_train_items 281984.
I0302 19:01:39.623636 22895071117440 run.py:483] Algo bellman_ford step 8812 current loss 0.572711, current_train_items 282016.
I0302 19:01:39.653043 22895071117440 run.py:483] Algo bellman_ford step 8813 current loss 0.582643, current_train_items 282048.
I0302 19:01:39.684165 22895071117440 run.py:483] Algo bellman_ford step 8814 current loss 0.721731, current_train_items 282080.
I0302 19:01:39.701995 22895071117440 run.py:483] Algo bellman_ford step 8815 current loss 0.337532, current_train_items 282112.
I0302 19:01:39.717368 22895071117440 run.py:483] Algo bellman_ford step 8816 current loss 0.443294, current_train_items 282144.
I0302 19:01:39.739189 22895071117440 run.py:483] Algo bellman_ford step 8817 current loss 0.480204, current_train_items 282176.
I0302 19:01:39.767277 22895071117440 run.py:483] Algo bellman_ford step 8818 current loss 0.634605, current_train_items 282208.
I0302 19:01:39.802197 22895071117440 run.py:483] Algo bellman_ford step 8819 current loss 0.701714, current_train_items 282240.
I0302 19:01:39.820240 22895071117440 run.py:483] Algo bellman_ford step 8820 current loss 0.326246, current_train_items 282272.
I0302 19:01:39.835483 22895071117440 run.py:483] Algo bellman_ford step 8821 current loss 0.446466, current_train_items 282304.
I0302 19:01:39.857530 22895071117440 run.py:483] Algo bellman_ford step 8822 current loss 0.632001, current_train_items 282336.
I0302 19:01:39.887086 22895071117440 run.py:483] Algo bellman_ford step 8823 current loss 0.733553, current_train_items 282368.
I0302 19:01:39.917373 22895071117440 run.py:483] Algo bellman_ford step 8824 current loss 0.806526, current_train_items 282400.
I0302 19:01:39.935106 22895071117440 run.py:483] Algo bellman_ford step 8825 current loss 0.232552, current_train_items 282432.
I0302 19:01:39.950650 22895071117440 run.py:483] Algo bellman_ford step 8826 current loss 0.464386, current_train_items 282464.
I0302 19:01:39.972889 22895071117440 run.py:483] Algo bellman_ford step 8827 current loss 0.643547, current_train_items 282496.
I0302 19:01:40.002357 22895071117440 run.py:483] Algo bellman_ford step 8828 current loss 0.722443, current_train_items 282528.
I0302 19:01:40.032801 22895071117440 run.py:483] Algo bellman_ford step 8829 current loss 0.771384, current_train_items 282560.
I0302 19:01:40.051053 22895071117440 run.py:483] Algo bellman_ford step 8830 current loss 0.369247, current_train_items 282592.
I0302 19:01:40.066219 22895071117440 run.py:483] Algo bellman_ford step 8831 current loss 0.471662, current_train_items 282624.
I0302 19:01:40.089041 22895071117440 run.py:483] Algo bellman_ford step 8832 current loss 0.643459, current_train_items 282656.
I0302 19:01:40.116522 22895071117440 run.py:483] Algo bellman_ford step 8833 current loss 0.588258, current_train_items 282688.
I0302 19:01:40.146171 22895071117440 run.py:483] Algo bellman_ford step 8834 current loss 0.744007, current_train_items 282720.
I0302 19:01:40.164323 22895071117440 run.py:483] Algo bellman_ford step 8835 current loss 0.343597, current_train_items 282752.
I0302 19:01:40.179644 22895071117440 run.py:483] Algo bellman_ford step 8836 current loss 0.480433, current_train_items 282784.
I0302 19:01:40.202130 22895071117440 run.py:483] Algo bellman_ford step 8837 current loss 0.611399, current_train_items 282816.
I0302 19:01:40.231725 22895071117440 run.py:483] Algo bellman_ford step 8838 current loss 0.714860, current_train_items 282848.
I0302 19:01:40.262688 22895071117440 run.py:483] Algo bellman_ford step 8839 current loss 0.786657, current_train_items 282880.
I0302 19:01:40.280719 22895071117440 run.py:483] Algo bellman_ford step 8840 current loss 0.330081, current_train_items 282912.
I0302 19:01:40.296341 22895071117440 run.py:483] Algo bellman_ford step 8841 current loss 0.492112, current_train_items 282944.
I0302 19:01:40.318604 22895071117440 run.py:483] Algo bellman_ford step 8842 current loss 0.615462, current_train_items 282976.
I0302 19:01:40.347527 22895071117440 run.py:483] Algo bellman_ford step 8843 current loss 0.700667, current_train_items 283008.
I0302 19:01:40.380081 22895071117440 run.py:483] Algo bellman_ford step 8844 current loss 0.853668, current_train_items 283040.
I0302 19:01:40.398044 22895071117440 run.py:483] Algo bellman_ford step 8845 current loss 0.372090, current_train_items 283072.
I0302 19:01:40.413228 22895071117440 run.py:483] Algo bellman_ford step 8846 current loss 0.422443, current_train_items 283104.
I0302 19:01:40.435219 22895071117440 run.py:483] Algo bellman_ford step 8847 current loss 0.579083, current_train_items 283136.
I0302 19:01:40.462623 22895071117440 run.py:483] Algo bellman_ford step 8848 current loss 0.634657, current_train_items 283168.
I0302 19:01:40.493705 22895071117440 run.py:483] Algo bellman_ford step 8849 current loss 0.721297, current_train_items 283200.
I0302 19:01:40.511430 22895071117440 run.py:483] Algo bellman_ford step 8850 current loss 0.329080, current_train_items 283232.
I0302 19:01:40.519354 22895071117440 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:01:40.519459 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:40.535556 22895071117440 run.py:483] Algo bellman_ford step 8851 current loss 0.455232, current_train_items 283264.
I0302 19:01:40.558377 22895071117440 run.py:483] Algo bellman_ford step 8852 current loss 0.575637, current_train_items 283296.
I0302 19:01:40.588213 22895071117440 run.py:483] Algo bellman_ford step 8853 current loss 0.609503, current_train_items 283328.
I0302 19:01:40.620234 22895071117440 run.py:483] Algo bellman_ford step 8854 current loss 0.769403, current_train_items 283360.
I0302 19:01:40.638965 22895071117440 run.py:483] Algo bellman_ford step 8855 current loss 0.361236, current_train_items 283392.
I0302 19:01:40.654582 22895071117440 run.py:483] Algo bellman_ford step 8856 current loss 0.449789, current_train_items 283424.
I0302 19:01:40.676162 22895071117440 run.py:483] Algo bellman_ford step 8857 current loss 0.549838, current_train_items 283456.
I0302 19:01:40.706030 22895071117440 run.py:483] Algo bellman_ford step 8858 current loss 0.617685, current_train_items 283488.
I0302 19:01:40.735348 22895071117440 run.py:483] Algo bellman_ford step 8859 current loss 0.696636, current_train_items 283520.
I0302 19:01:40.753956 22895071117440 run.py:483] Algo bellman_ford step 8860 current loss 0.289714, current_train_items 283552.
I0302 19:01:40.770045 22895071117440 run.py:483] Algo bellman_ford step 8861 current loss 0.463262, current_train_items 283584.
I0302 19:01:40.792314 22895071117440 run.py:483] Algo bellman_ford step 8862 current loss 0.588309, current_train_items 283616.
I0302 19:01:40.820417 22895071117440 run.py:483] Algo bellman_ford step 8863 current loss 0.621304, current_train_items 283648.
I0302 19:01:40.850716 22895071117440 run.py:483] Algo bellman_ford step 8864 current loss 0.689028, current_train_items 283680.
I0302 19:01:40.869207 22895071117440 run.py:483] Algo bellman_ford step 8865 current loss 0.358722, current_train_items 283712.
I0302 19:01:40.884464 22895071117440 run.py:483] Algo bellman_ford step 8866 current loss 0.460294, current_train_items 283744.
I0302 19:01:40.905836 22895071117440 run.py:483] Algo bellman_ford step 8867 current loss 0.593388, current_train_items 283776.
I0302 19:01:40.935450 22895071117440 run.py:483] Algo bellman_ford step 8868 current loss 0.642647, current_train_items 283808.
I0302 19:01:40.966026 22895071117440 run.py:483] Algo bellman_ford step 8869 current loss 0.721342, current_train_items 283840.
I0302 19:01:40.984668 22895071117440 run.py:483] Algo bellman_ford step 8870 current loss 0.333208, current_train_items 283872.
I0302 19:01:41.000411 22895071117440 run.py:483] Algo bellman_ford step 8871 current loss 0.374986, current_train_items 283904.
I0302 19:01:41.022519 22895071117440 run.py:483] Algo bellman_ford step 8872 current loss 0.566380, current_train_items 283936.
I0302 19:01:41.051014 22895071117440 run.py:483] Algo bellman_ford step 8873 current loss 0.543690, current_train_items 283968.
I0302 19:01:41.083472 22895071117440 run.py:483] Algo bellman_ford step 8874 current loss 0.773540, current_train_items 284000.
I0302 19:01:41.101767 22895071117440 run.py:483] Algo bellman_ford step 8875 current loss 0.291061, current_train_items 284032.
I0302 19:01:41.117466 22895071117440 run.py:483] Algo bellman_ford step 8876 current loss 0.404186, current_train_items 284064.
I0302 19:01:41.138908 22895071117440 run.py:483] Algo bellman_ford step 8877 current loss 0.586442, current_train_items 284096.
I0302 19:01:41.167697 22895071117440 run.py:483] Algo bellman_ford step 8878 current loss 0.608228, current_train_items 284128.
I0302 19:01:41.199327 22895071117440 run.py:483] Algo bellman_ford step 8879 current loss 0.762856, current_train_items 284160.
I0302 19:01:41.217501 22895071117440 run.py:483] Algo bellman_ford step 8880 current loss 0.320125, current_train_items 284192.
I0302 19:01:41.232841 22895071117440 run.py:483] Algo bellman_ford step 8881 current loss 0.477620, current_train_items 284224.
I0302 19:01:41.254013 22895071117440 run.py:483] Algo bellman_ford step 8882 current loss 0.536564, current_train_items 284256.
I0302 19:01:41.283853 22895071117440 run.py:483] Algo bellman_ford step 8883 current loss 0.638007, current_train_items 284288.
I0302 19:01:41.314948 22895071117440 run.py:483] Algo bellman_ford step 8884 current loss 0.742161, current_train_items 284320.
I0302 19:01:41.333313 22895071117440 run.py:483] Algo bellman_ford step 8885 current loss 0.303366, current_train_items 284352.
I0302 19:01:41.349440 22895071117440 run.py:483] Algo bellman_ford step 8886 current loss 0.423401, current_train_items 284384.
I0302 19:01:41.371328 22895071117440 run.py:483] Algo bellman_ford step 8887 current loss 0.590973, current_train_items 284416.
I0302 19:01:41.399779 22895071117440 run.py:483] Algo bellman_ford step 8888 current loss 0.843019, current_train_items 284448.
I0302 19:01:41.431639 22895071117440 run.py:483] Algo bellman_ford step 8889 current loss 0.836647, current_train_items 284480.
I0302 19:01:41.450092 22895071117440 run.py:483] Algo bellman_ford step 8890 current loss 0.315369, current_train_items 284512.
I0302 19:01:41.465404 22895071117440 run.py:483] Algo bellman_ford step 8891 current loss 0.421317, current_train_items 284544.
I0302 19:01:41.487421 22895071117440 run.py:483] Algo bellman_ford step 8892 current loss 0.530860, current_train_items 284576.
I0302 19:01:41.517764 22895071117440 run.py:483] Algo bellman_ford step 8893 current loss 0.737703, current_train_items 284608.
I0302 19:01:41.548705 22895071117440 run.py:483] Algo bellman_ford step 8894 current loss 0.763294, current_train_items 284640.
I0302 19:01:41.567013 22895071117440 run.py:483] Algo bellman_ford step 8895 current loss 0.298951, current_train_items 284672.
I0302 19:01:41.582173 22895071117440 run.py:483] Algo bellman_ford step 8896 current loss 0.460057, current_train_items 284704.
I0302 19:01:41.603255 22895071117440 run.py:483] Algo bellman_ford step 8897 current loss 0.521016, current_train_items 284736.
I0302 19:01:41.632947 22895071117440 run.py:483] Algo bellman_ford step 8898 current loss 0.605211, current_train_items 284768.
I0302 19:01:41.665746 22895071117440 run.py:483] Algo bellman_ford step 8899 current loss 0.658275, current_train_items 284800.
I0302 19:01:41.684317 22895071117440 run.py:483] Algo bellman_ford step 8900 current loss 0.318662, current_train_items 284832.
I0302 19:01:41.692291 22895071117440 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:01:41.692398 22895071117440 run.py:519] Checkpointing best model, best avg val score was 0.942, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:01:41.720487 22895071117440 run.py:483] Algo bellman_ford step 8901 current loss 0.440082, current_train_items 284864.
I0302 19:01:41.742535 22895071117440 run.py:483] Algo bellman_ford step 8902 current loss 0.549224, current_train_items 284896.
I0302 19:01:41.771262 22895071117440 run.py:483] Algo bellman_ford step 8903 current loss 0.627283, current_train_items 284928.
I0302 19:01:41.804309 22895071117440 run.py:483] Algo bellman_ford step 8904 current loss 0.728259, current_train_items 284960.
I0302 19:01:41.822894 22895071117440 run.py:483] Algo bellman_ford step 8905 current loss 0.348997, current_train_items 284992.
I0302 19:01:41.838480 22895071117440 run.py:483] Algo bellman_ford step 8906 current loss 0.429523, current_train_items 285024.
I0302 19:01:41.860685 22895071117440 run.py:483] Algo bellman_ford step 8907 current loss 0.542861, current_train_items 285056.
I0302 19:01:41.889317 22895071117440 run.py:483] Algo bellman_ford step 8908 current loss 0.635091, current_train_items 285088.
I0302 19:01:41.919110 22895071117440 run.py:483] Algo bellman_ford step 8909 current loss 0.709251, current_train_items 285120.
I0302 19:01:41.937303 22895071117440 run.py:483] Algo bellman_ford step 8910 current loss 0.310765, current_train_items 285152.
I0302 19:01:41.952599 22895071117440 run.py:483] Algo bellman_ford step 8911 current loss 0.399883, current_train_items 285184.
I0302 19:01:41.975272 22895071117440 run.py:483] Algo bellman_ford step 8912 current loss 0.580694, current_train_items 285216.
I0302 19:01:42.003134 22895071117440 run.py:483] Algo bellman_ford step 8913 current loss 0.605877, current_train_items 285248.
I0302 19:01:42.035426 22895071117440 run.py:483] Algo bellman_ford step 8914 current loss 0.948999, current_train_items 285280.
I0302 19:01:42.053689 22895071117440 run.py:483] Algo bellman_ford step 8915 current loss 0.319561, current_train_items 285312.
I0302 19:01:42.068774 22895071117440 run.py:483] Algo bellman_ford step 8916 current loss 0.450780, current_train_items 285344.
I0302 19:01:42.091405 22895071117440 run.py:483] Algo bellman_ford step 8917 current loss 0.666958, current_train_items 285376.
I0302 19:01:42.120141 22895071117440 run.py:483] Algo bellman_ford step 8918 current loss 0.657799, current_train_items 285408.
I0302 19:01:42.150618 22895071117440 run.py:483] Algo bellman_ford step 8919 current loss 0.762961, current_train_items 285440.
I0302 19:01:42.168845 22895071117440 run.py:483] Algo bellman_ford step 8920 current loss 0.303382, current_train_items 285472.
I0302 19:01:42.184338 22895071117440 run.py:483] Algo bellman_ford step 8921 current loss 0.483302, current_train_items 285504.
I0302 19:01:42.207007 22895071117440 run.py:483] Algo bellman_ford step 8922 current loss 0.586755, current_train_items 285536.
I0302 19:01:42.235608 22895071117440 run.py:483] Algo bellman_ford step 8923 current loss 0.689792, current_train_items 285568.
I0302 19:01:42.267802 22895071117440 run.py:483] Algo bellman_ford step 8924 current loss 0.841410, current_train_items 285600.
I0302 19:01:42.285689 22895071117440 run.py:483] Algo bellman_ford step 8925 current loss 0.240808, current_train_items 285632.
I0302 19:01:42.300670 22895071117440 run.py:483] Algo bellman_ford step 8926 current loss 0.617006, current_train_items 285664.
I0302 19:01:42.322797 22895071117440 run.py:483] Algo bellman_ford step 8927 current loss 0.607521, current_train_items 285696.
I0302 19:01:42.350779 22895071117440 run.py:483] Algo bellman_ford step 8928 current loss 0.534475, current_train_items 285728.
I0302 19:01:42.381773 22895071117440 run.py:483] Algo bellman_ford step 8929 current loss 0.764613, current_train_items 285760.
I0302 19:01:42.399936 22895071117440 run.py:483] Algo bellman_ford step 8930 current loss 0.323274, current_train_items 285792.
I0302 19:01:42.415894 22895071117440 run.py:483] Algo bellman_ford step 8931 current loss 0.514912, current_train_items 285824.
I0302 19:01:42.437827 22895071117440 run.py:483] Algo bellman_ford step 8932 current loss 0.516573, current_train_items 285856.
I0302 19:01:42.468311 22895071117440 run.py:483] Algo bellman_ford step 8933 current loss 0.675737, current_train_items 285888.
I0302 19:01:42.499993 22895071117440 run.py:483] Algo bellman_ford step 8934 current loss 0.716250, current_train_items 285920.
I0302 19:01:42.518152 22895071117440 run.py:483] Algo bellman_ford step 8935 current loss 0.297978, current_train_items 285952.
I0302 19:01:42.533224 22895071117440 run.py:483] Algo bellman_ford step 8936 current loss 0.428697, current_train_items 285984.
I0302 19:01:42.555158 22895071117440 run.py:483] Algo bellman_ford step 8937 current loss 0.554953, current_train_items 286016.
I0302 19:01:42.583605 22895071117440 run.py:483] Algo bellman_ford step 8938 current loss 0.637216, current_train_items 286048.
I0302 19:01:42.613983 22895071117440 run.py:483] Algo bellman_ford step 8939 current loss 0.646873, current_train_items 286080.
I0302 19:01:42.632165 22895071117440 run.py:483] Algo bellman_ford step 8940 current loss 0.312967, current_train_items 286112.
I0302 19:01:42.647401 22895071117440 run.py:483] Algo bellman_ford step 8941 current loss 0.413463, current_train_items 286144.
I0302 19:01:42.669684 22895071117440 run.py:483] Algo bellman_ford step 8942 current loss 0.591966, current_train_items 286176.
I0302 19:01:42.699477 22895071117440 run.py:483] Algo bellman_ford step 8943 current loss 0.743361, current_train_items 286208.
I0302 19:01:42.730659 22895071117440 run.py:483] Algo bellman_ford step 8944 current loss 0.704422, current_train_items 286240.
I0302 19:01:42.748612 22895071117440 run.py:483] Algo bellman_ford step 8945 current loss 0.361410, current_train_items 286272.
I0302 19:01:42.763980 22895071117440 run.py:483] Algo bellman_ford step 8946 current loss 0.414875, current_train_items 286304.
I0302 19:01:42.785580 22895071117440 run.py:483] Algo bellman_ford step 8947 current loss 0.606669, current_train_items 286336.
I0302 19:01:42.815041 22895071117440 run.py:483] Algo bellman_ford step 8948 current loss 0.650197, current_train_items 286368.
I0302 19:01:42.845923 22895071117440 run.py:483] Algo bellman_ford step 8949 current loss 0.743171, current_train_items 286400.
I0302 19:01:42.863768 22895071117440 run.py:483] Algo bellman_ford step 8950 current loss 0.309660, current_train_items 286432.
I0302 19:01:42.871717 22895071117440 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:01:42.871822 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:01:42.888159 22895071117440 run.py:483] Algo bellman_ford step 8951 current loss 0.519981, current_train_items 286464.
I0302 19:01:42.910049 22895071117440 run.py:483] Algo bellman_ford step 8952 current loss 0.597398, current_train_items 286496.
I0302 19:01:42.942082 22895071117440 run.py:483] Algo bellman_ford step 8953 current loss 0.727330, current_train_items 286528.
I0302 19:01:42.975155 22895071117440 run.py:483] Algo bellman_ford step 8954 current loss 0.673447, current_train_items 286560.
I0302 19:01:42.993583 22895071117440 run.py:483] Algo bellman_ford step 8955 current loss 0.332469, current_train_items 286592.
I0302 19:01:43.009530 22895071117440 run.py:483] Algo bellman_ford step 8956 current loss 0.529791, current_train_items 286624.
I0302 19:01:43.032333 22895071117440 run.py:483] Algo bellman_ford step 8957 current loss 0.641035, current_train_items 286656.
I0302 19:01:43.060798 22895071117440 run.py:483] Algo bellman_ford step 8958 current loss 0.651093, current_train_items 286688.
I0302 19:01:43.093069 22895071117440 run.py:483] Algo bellman_ford step 8959 current loss 0.822827, current_train_items 286720.
I0302 19:01:43.111736 22895071117440 run.py:483] Algo bellman_ford step 8960 current loss 0.284869, current_train_items 286752.
I0302 19:01:43.127021 22895071117440 run.py:483] Algo bellman_ford step 8961 current loss 0.440924, current_train_items 286784.
I0302 19:01:43.149813 22895071117440 run.py:483] Algo bellman_ford step 8962 current loss 0.562570, current_train_items 286816.
I0302 19:01:43.179104 22895071117440 run.py:483] Algo bellman_ford step 8963 current loss 0.590845, current_train_items 286848.
I0302 19:01:43.208860 22895071117440 run.py:483] Algo bellman_ford step 8964 current loss 0.680417, current_train_items 286880.
I0302 19:01:43.227120 22895071117440 run.py:483] Algo bellman_ford step 8965 current loss 0.293383, current_train_items 286912.
I0302 19:01:43.242584 22895071117440 run.py:483] Algo bellman_ford step 8966 current loss 0.473950, current_train_items 286944.
I0302 19:01:43.264504 22895071117440 run.py:483] Algo bellman_ford step 8967 current loss 0.632849, current_train_items 286976.
I0302 19:01:43.293725 22895071117440 run.py:483] Algo bellman_ford step 8968 current loss 0.632808, current_train_items 287008.
I0302 19:01:43.324323 22895071117440 run.py:483] Algo bellman_ford step 8969 current loss 0.684383, current_train_items 287040.
I0302 19:01:43.342716 22895071117440 run.py:483] Algo bellman_ford step 8970 current loss 0.321482, current_train_items 287072.
I0302 19:01:43.358369 22895071117440 run.py:483] Algo bellman_ford step 8971 current loss 0.388123, current_train_items 287104.
I0302 19:01:43.380411 22895071117440 run.py:483] Algo bellman_ford step 8972 current loss 0.590089, current_train_items 287136.
I0302 19:01:43.408781 22895071117440 run.py:483] Algo bellman_ford step 8973 current loss 0.626971, current_train_items 287168.
I0302 19:01:43.439236 22895071117440 run.py:483] Algo bellman_ford step 8974 current loss 0.773173, current_train_items 287200.
I0302 19:01:43.457975 22895071117440 run.py:483] Algo bellman_ford step 8975 current loss 0.307344, current_train_items 287232.
I0302 19:01:43.473347 22895071117440 run.py:483] Algo bellman_ford step 8976 current loss 0.458108, current_train_items 287264.
I0302 19:01:43.494050 22895071117440 run.py:483] Algo bellman_ford step 8977 current loss 0.532839, current_train_items 287296.
I0302 19:01:43.523346 22895071117440 run.py:483] Algo bellman_ford step 8978 current loss 0.614989, current_train_items 287328.
I0302 19:01:43.554070 22895071117440 run.py:483] Algo bellman_ford step 8979 current loss 0.730245, current_train_items 287360.
I0302 19:01:43.572023 22895071117440 run.py:483] Algo bellman_ford step 8980 current loss 0.257058, current_train_items 287392.
I0302 19:01:43.587519 22895071117440 run.py:483] Algo bellman_ford step 8981 current loss 0.485976, current_train_items 287424.
I0302 19:01:43.609886 22895071117440 run.py:483] Algo bellman_ford step 8982 current loss 0.553477, current_train_items 287456.
I0302 19:01:43.638748 22895071117440 run.py:483] Algo bellman_ford step 8983 current loss 0.614882, current_train_items 287488.
I0302 19:01:43.669816 22895071117440 run.py:483] Algo bellman_ford step 8984 current loss 0.647493, current_train_items 287520.
I0302 19:01:43.688437 22895071117440 run.py:483] Algo bellman_ford step 8985 current loss 0.257630, current_train_items 287552.
I0302 19:01:43.704143 22895071117440 run.py:483] Algo bellman_ford step 8986 current loss 0.417695, current_train_items 287584.
I0302 19:01:43.725369 22895071117440 run.py:483] Algo bellman_ford step 8987 current loss 0.526020, current_train_items 287616.
I0302 19:01:43.752781 22895071117440 run.py:483] Algo bellman_ford step 8988 current loss 0.623592, current_train_items 287648.
I0302 19:01:43.785663 22895071117440 run.py:483] Algo bellman_ford step 8989 current loss 0.711735, current_train_items 287680.
I0302 19:01:43.803871 22895071117440 run.py:483] Algo bellman_ford step 8990 current loss 0.270878, current_train_items 287712.
I0302 19:01:43.819213 22895071117440 run.py:483] Algo bellman_ford step 8991 current loss 0.409958, current_train_items 287744.
I0302 19:01:43.840650 22895071117440 run.py:483] Algo bellman_ford step 8992 current loss 0.564987, current_train_items 287776.
I0302 19:01:43.869308 22895071117440 run.py:483] Algo bellman_ford step 8993 current loss 0.631578, current_train_items 287808.
I0302 19:01:43.899871 22895071117440 run.py:483] Algo bellman_ford step 8994 current loss 0.724316, current_train_items 287840.
I0302 19:01:43.917491 22895071117440 run.py:483] Algo bellman_ford step 8995 current loss 0.295935, current_train_items 287872.
I0302 19:01:43.932896 22895071117440 run.py:483] Algo bellman_ford step 8996 current loss 0.398987, current_train_items 287904.
I0302 19:01:43.955254 22895071117440 run.py:483] Algo bellman_ford step 8997 current loss 0.566904, current_train_items 287936.
I0302 19:01:43.985596 22895071117440 run.py:483] Algo bellman_ford step 8998 current loss 0.611035, current_train_items 287968.
I0302 19:01:44.015503 22895071117440 run.py:483] Algo bellman_ford step 8999 current loss 0.707287, current_train_items 288000.
I0302 19:01:44.034422 22895071117440 run.py:483] Algo bellman_ford step 9000 current loss 0.280357, current_train_items 288032.
I0302 19:01:44.042253 22895071117440 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:01:44.042357 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:44.058864 22895071117440 run.py:483] Algo bellman_ford step 9001 current loss 0.417384, current_train_items 288064.
I0302 19:01:44.080974 22895071117440 run.py:483] Algo bellman_ford step 9002 current loss 0.584947, current_train_items 288096.
I0302 19:01:44.110062 22895071117440 run.py:483] Algo bellman_ford step 9003 current loss 0.664984, current_train_items 288128.
I0302 19:01:44.141892 22895071117440 run.py:483] Algo bellman_ford step 9004 current loss 0.678003, current_train_items 288160.
I0302 19:01:44.160551 22895071117440 run.py:483] Algo bellman_ford step 9005 current loss 0.314088, current_train_items 288192.
I0302 19:01:44.176140 22895071117440 run.py:483] Algo bellman_ford step 9006 current loss 0.432913, current_train_items 288224.
I0302 19:01:44.197554 22895071117440 run.py:483] Algo bellman_ford step 9007 current loss 0.628297, current_train_items 288256.
I0302 19:01:44.227319 22895071117440 run.py:483] Algo bellman_ford step 9008 current loss 0.809738, current_train_items 288288.
I0302 19:01:44.258770 22895071117440 run.py:483] Algo bellman_ford step 9009 current loss 0.910264, current_train_items 288320.
I0302 19:01:44.277137 22895071117440 run.py:483] Algo bellman_ford step 9010 current loss 0.311600, current_train_items 288352.
I0302 19:01:44.292624 22895071117440 run.py:483] Algo bellman_ford step 9011 current loss 0.435467, current_train_items 288384.
I0302 19:01:44.314018 22895071117440 run.py:483] Algo bellman_ford step 9012 current loss 0.558490, current_train_items 288416.
I0302 19:01:44.342314 22895071117440 run.py:483] Algo bellman_ford step 9013 current loss 0.565875, current_train_items 288448.
I0302 19:01:44.375280 22895071117440 run.py:483] Algo bellman_ford step 9014 current loss 0.755477, current_train_items 288480.
I0302 19:01:44.393274 22895071117440 run.py:483] Algo bellman_ford step 9015 current loss 0.290735, current_train_items 288512.
I0302 19:01:44.408830 22895071117440 run.py:483] Algo bellman_ford step 9016 current loss 0.435428, current_train_items 288544.
I0302 19:01:44.431249 22895071117440 run.py:483] Algo bellman_ford step 9017 current loss 0.613901, current_train_items 288576.
I0302 19:01:44.460687 22895071117440 run.py:483] Algo bellman_ford step 9018 current loss 0.620138, current_train_items 288608.
I0302 19:01:44.492993 22895071117440 run.py:483] Algo bellman_ford step 9019 current loss 0.716524, current_train_items 288640.
I0302 19:01:44.510767 22895071117440 run.py:483] Algo bellman_ford step 9020 current loss 0.395937, current_train_items 288672.
I0302 19:01:44.526139 22895071117440 run.py:483] Algo bellman_ford step 9021 current loss 0.474712, current_train_items 288704.
I0302 19:01:44.549413 22895071117440 run.py:483] Algo bellman_ford step 9022 current loss 0.635300, current_train_items 288736.
I0302 19:01:44.579880 22895071117440 run.py:483] Algo bellman_ford step 9023 current loss 0.774007, current_train_items 288768.
I0302 19:01:44.610992 22895071117440 run.py:483] Algo bellman_ford step 9024 current loss 0.925567, current_train_items 288800.
I0302 19:01:44.629147 22895071117440 run.py:483] Algo bellman_ford step 9025 current loss 0.317908, current_train_items 288832.
I0302 19:01:44.644261 22895071117440 run.py:483] Algo bellman_ford step 9026 current loss 0.321678, current_train_items 288864.
I0302 19:01:44.666667 22895071117440 run.py:483] Algo bellman_ford step 9027 current loss 0.540021, current_train_items 288896.
I0302 19:01:44.696486 22895071117440 run.py:483] Algo bellman_ford step 9028 current loss 0.590724, current_train_items 288928.
I0302 19:01:44.727682 22895071117440 run.py:483] Algo bellman_ford step 9029 current loss 0.735708, current_train_items 288960.
I0302 19:01:44.746033 22895071117440 run.py:483] Algo bellman_ford step 9030 current loss 0.279439, current_train_items 288992.
I0302 19:01:44.761330 22895071117440 run.py:483] Algo bellman_ford step 9031 current loss 0.411034, current_train_items 289024.
I0302 19:01:44.784197 22895071117440 run.py:483] Algo bellman_ford step 9032 current loss 0.656908, current_train_items 289056.
I0302 19:01:44.814009 22895071117440 run.py:483] Algo bellman_ford step 9033 current loss 0.634535, current_train_items 289088.
I0302 19:01:44.847691 22895071117440 run.py:483] Algo bellman_ford step 9034 current loss 0.854833, current_train_items 289120.
I0302 19:01:44.865505 22895071117440 run.py:483] Algo bellman_ford step 9035 current loss 0.325005, current_train_items 289152.
I0302 19:01:44.880557 22895071117440 run.py:483] Algo bellman_ford step 9036 current loss 0.527085, current_train_items 289184.
I0302 19:01:44.903660 22895071117440 run.py:483] Algo bellman_ford step 9037 current loss 0.606890, current_train_items 289216.
I0302 19:01:44.932862 22895071117440 run.py:483] Algo bellman_ford step 9038 current loss 0.647454, current_train_items 289248.
I0302 19:01:44.964245 22895071117440 run.py:483] Algo bellman_ford step 9039 current loss 0.668540, current_train_items 289280.
I0302 19:01:44.982558 22895071117440 run.py:483] Algo bellman_ford step 9040 current loss 0.268847, current_train_items 289312.
I0302 19:01:44.998184 22895071117440 run.py:483] Algo bellman_ford step 9041 current loss 0.411118, current_train_items 289344.
I0302 19:01:45.019933 22895071117440 run.py:483] Algo bellman_ford step 9042 current loss 0.553021, current_train_items 289376.
I0302 19:01:45.048563 22895071117440 run.py:483] Algo bellman_ford step 9043 current loss 0.680843, current_train_items 289408.
I0302 19:01:45.080040 22895071117440 run.py:483] Algo bellman_ford step 9044 current loss 0.768250, current_train_items 289440.
I0302 19:01:45.098004 22895071117440 run.py:483] Algo bellman_ford step 9045 current loss 0.309572, current_train_items 289472.
I0302 19:01:45.113646 22895071117440 run.py:483] Algo bellman_ford step 9046 current loss 0.429295, current_train_items 289504.
I0302 19:01:45.134737 22895071117440 run.py:483] Algo bellman_ford step 9047 current loss 0.657912, current_train_items 289536.
I0302 19:01:45.164332 22895071117440 run.py:483] Algo bellman_ford step 9048 current loss 0.655162, current_train_items 289568.
I0302 19:01:45.195657 22895071117440 run.py:483] Algo bellman_ford step 9049 current loss 0.703822, current_train_items 289600.
I0302 19:01:45.213750 22895071117440 run.py:483] Algo bellman_ford step 9050 current loss 0.282398, current_train_items 289632.
I0302 19:01:45.221663 22895071117440 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:01:45.221768 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:45.237745 22895071117440 run.py:483] Algo bellman_ford step 9051 current loss 0.406152, current_train_items 289664.
I0302 19:01:45.260685 22895071117440 run.py:483] Algo bellman_ford step 9052 current loss 0.656162, current_train_items 289696.
I0302 19:01:45.290478 22895071117440 run.py:483] Algo bellman_ford step 9053 current loss 0.648437, current_train_items 289728.
I0302 19:01:45.320919 22895071117440 run.py:483] Algo bellman_ford step 9054 current loss 0.691355, current_train_items 289760.
I0302 19:01:45.339405 22895071117440 run.py:483] Algo bellman_ford step 9055 current loss 0.282303, current_train_items 289792.
I0302 19:01:45.354672 22895071117440 run.py:483] Algo bellman_ford step 9056 current loss 0.415319, current_train_items 289824.
I0302 19:01:45.376439 22895071117440 run.py:483] Algo bellman_ford step 9057 current loss 0.575018, current_train_items 289856.
I0302 19:01:45.403936 22895071117440 run.py:483] Algo bellman_ford step 9058 current loss 0.627052, current_train_items 289888.
I0302 19:01:45.433230 22895071117440 run.py:483] Algo bellman_ford step 9059 current loss 0.686716, current_train_items 289920.
I0302 19:01:45.451636 22895071117440 run.py:483] Algo bellman_ford step 9060 current loss 0.352131, current_train_items 289952.
I0302 19:01:45.467801 22895071117440 run.py:483] Algo bellman_ford step 9061 current loss 0.472257, current_train_items 289984.
I0302 19:01:45.491023 22895071117440 run.py:483] Algo bellman_ford step 9062 current loss 0.740117, current_train_items 290016.
I0302 19:01:45.518659 22895071117440 run.py:483] Algo bellman_ford step 9063 current loss 0.775126, current_train_items 290048.
I0302 19:01:45.550419 22895071117440 run.py:483] Algo bellman_ford step 9064 current loss 1.078820, current_train_items 290080.
I0302 19:01:45.568854 22895071117440 run.py:483] Algo bellman_ford step 9065 current loss 0.310203, current_train_items 290112.
I0302 19:01:45.584214 22895071117440 run.py:483] Algo bellman_ford step 9066 current loss 0.509947, current_train_items 290144.
I0302 19:01:45.604276 22895071117440 run.py:483] Algo bellman_ford step 9067 current loss 0.503351, current_train_items 290176.
I0302 19:01:45.633893 22895071117440 run.py:483] Algo bellman_ford step 9068 current loss 0.586213, current_train_items 290208.
I0302 19:01:45.665524 22895071117440 run.py:483] Algo bellman_ford step 9069 current loss 0.731635, current_train_items 290240.
I0302 19:01:45.684209 22895071117440 run.py:483] Algo bellman_ford step 9070 current loss 0.245209, current_train_items 290272.
I0302 19:01:45.700183 22895071117440 run.py:483] Algo bellman_ford step 9071 current loss 0.508528, current_train_items 290304.
I0302 19:01:45.723011 22895071117440 run.py:483] Algo bellman_ford step 9072 current loss 0.594783, current_train_items 290336.
I0302 19:01:45.752520 22895071117440 run.py:483] Algo bellman_ford step 9073 current loss 0.575074, current_train_items 290368.
I0302 19:01:45.783657 22895071117440 run.py:483] Algo bellman_ford step 9074 current loss 0.712566, current_train_items 290400.
I0302 19:01:45.802118 22895071117440 run.py:483] Algo bellman_ford step 9075 current loss 0.229267, current_train_items 290432.
I0302 19:01:45.817736 22895071117440 run.py:483] Algo bellman_ford step 9076 current loss 0.527402, current_train_items 290464.
I0302 19:01:45.839464 22895071117440 run.py:483] Algo bellman_ford step 9077 current loss 0.533878, current_train_items 290496.
I0302 19:01:45.867987 22895071117440 run.py:483] Algo bellman_ford step 9078 current loss 0.597225, current_train_items 290528.
I0302 19:01:45.900036 22895071117440 run.py:483] Algo bellman_ford step 9079 current loss 0.845390, current_train_items 290560.
I0302 19:01:45.918332 22895071117440 run.py:483] Algo bellman_ford step 9080 current loss 0.296500, current_train_items 290592.
I0302 19:01:45.933497 22895071117440 run.py:483] Algo bellman_ford step 9081 current loss 0.446074, current_train_items 290624.
I0302 19:01:45.954915 22895071117440 run.py:483] Algo bellman_ford step 9082 current loss 0.549103, current_train_items 290656.
I0302 19:01:45.983132 22895071117440 run.py:483] Algo bellman_ford step 9083 current loss 0.619594, current_train_items 290688.
I0302 19:01:46.013487 22895071117440 run.py:483] Algo bellman_ford step 9084 current loss 0.654478, current_train_items 290720.
I0302 19:01:46.032010 22895071117440 run.py:483] Algo bellman_ford step 9085 current loss 0.310403, current_train_items 290752.
I0302 19:01:46.047837 22895071117440 run.py:483] Algo bellman_ford step 9086 current loss 0.413759, current_train_items 290784.
I0302 19:01:46.070021 22895071117440 run.py:483] Algo bellman_ford step 9087 current loss 0.564521, current_train_items 290816.
I0302 19:01:46.098201 22895071117440 run.py:483] Algo bellman_ford step 9088 current loss 0.527790, current_train_items 290848.
I0302 19:01:46.130874 22895071117440 run.py:483] Algo bellman_ford step 9089 current loss 0.858023, current_train_items 290880.
I0302 19:01:46.149089 22895071117440 run.py:483] Algo bellman_ford step 9090 current loss 0.236379, current_train_items 290912.
I0302 19:01:46.164621 22895071117440 run.py:483] Algo bellman_ford step 9091 current loss 0.474292, current_train_items 290944.
I0302 19:01:46.186079 22895071117440 run.py:483] Algo bellman_ford step 9092 current loss 0.516268, current_train_items 290976.
I0302 19:01:46.213049 22895071117440 run.py:483] Algo bellman_ford step 9093 current loss 0.598021, current_train_items 291008.
I0302 19:01:46.245971 22895071117440 run.py:483] Algo bellman_ford step 9094 current loss 0.904269, current_train_items 291040.
I0302 19:01:46.263855 22895071117440 run.py:483] Algo bellman_ford step 9095 current loss 0.381614, current_train_items 291072.
I0302 19:01:46.279031 22895071117440 run.py:483] Algo bellman_ford step 9096 current loss 0.467920, current_train_items 291104.
I0302 19:01:46.300724 22895071117440 run.py:483] Algo bellman_ford step 9097 current loss 0.613500, current_train_items 291136.
I0302 19:01:46.330256 22895071117440 run.py:483] Algo bellman_ford step 9098 current loss 0.657432, current_train_items 291168.
I0302 19:01:46.360368 22895071117440 run.py:483] Algo bellman_ford step 9099 current loss 0.715632, current_train_items 291200.
I0302 19:01:46.378726 22895071117440 run.py:483] Algo bellman_ford step 9100 current loss 0.253438, current_train_items 291232.
I0302 19:01:46.386537 22895071117440 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:01:46.386645 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:01:46.402704 22895071117440 run.py:483] Algo bellman_ford step 9101 current loss 0.377468, current_train_items 291264.
I0302 19:01:46.426136 22895071117440 run.py:483] Algo bellman_ford step 9102 current loss 0.600254, current_train_items 291296.
I0302 19:01:46.455555 22895071117440 run.py:483] Algo bellman_ford step 9103 current loss 0.620858, current_train_items 291328.
I0302 19:01:46.488828 22895071117440 run.py:483] Algo bellman_ford step 9104 current loss 0.715965, current_train_items 291360.
I0302 19:01:46.507445 22895071117440 run.py:483] Algo bellman_ford step 9105 current loss 0.280656, current_train_items 291392.
I0302 19:01:46.523024 22895071117440 run.py:483] Algo bellman_ford step 9106 current loss 0.539009, current_train_items 291424.
I0302 19:01:46.545349 22895071117440 run.py:483] Algo bellman_ford step 9107 current loss 0.618454, current_train_items 291456.
I0302 19:01:46.574938 22895071117440 run.py:483] Algo bellman_ford step 9108 current loss 0.653298, current_train_items 291488.
I0302 19:01:46.606447 22895071117440 run.py:483] Algo bellman_ford step 9109 current loss 0.713989, current_train_items 291520.
I0302 19:01:46.624694 22895071117440 run.py:483] Algo bellman_ford step 9110 current loss 0.294945, current_train_items 291552.
I0302 19:01:46.639975 22895071117440 run.py:483] Algo bellman_ford step 9111 current loss 0.440062, current_train_items 291584.
I0302 19:01:46.661466 22895071117440 run.py:483] Algo bellman_ford step 9112 current loss 0.655882, current_train_items 291616.
I0302 19:01:46.692167 22895071117440 run.py:483] Algo bellman_ford step 9113 current loss 0.624703, current_train_items 291648.
I0302 19:01:46.724884 22895071117440 run.py:483] Algo bellman_ford step 9114 current loss 0.674793, current_train_items 291680.
I0302 19:01:46.743027 22895071117440 run.py:483] Algo bellman_ford step 9115 current loss 0.336887, current_train_items 291712.
I0302 19:01:46.758572 22895071117440 run.py:483] Algo bellman_ford step 9116 current loss 0.413242, current_train_items 291744.
I0302 19:01:46.780794 22895071117440 run.py:483] Algo bellman_ford step 9117 current loss 0.609420, current_train_items 291776.
I0302 19:01:46.809437 22895071117440 run.py:483] Algo bellman_ford step 9118 current loss 0.661689, current_train_items 291808.
I0302 19:01:46.842437 22895071117440 run.py:483] Algo bellman_ford step 9119 current loss 0.788208, current_train_items 291840.
I0302 19:01:46.860415 22895071117440 run.py:483] Algo bellman_ford step 9120 current loss 0.244307, current_train_items 291872.
I0302 19:01:46.875718 22895071117440 run.py:483] Algo bellman_ford step 9121 current loss 0.464842, current_train_items 291904.
I0302 19:01:46.897551 22895071117440 run.py:483] Algo bellman_ford step 9122 current loss 0.612168, current_train_items 291936.
I0302 19:01:46.928020 22895071117440 run.py:483] Algo bellman_ford step 9123 current loss 0.721151, current_train_items 291968.
I0302 19:01:46.961273 22895071117440 run.py:483] Algo bellman_ford step 9124 current loss 0.650875, current_train_items 292000.
I0302 19:01:46.979545 22895071117440 run.py:483] Algo bellman_ford step 9125 current loss 0.247476, current_train_items 292032.
I0302 19:01:46.994642 22895071117440 run.py:483] Algo bellman_ford step 9126 current loss 0.458069, current_train_items 292064.
I0302 19:01:47.015563 22895071117440 run.py:483] Algo bellman_ford step 9127 current loss 0.605055, current_train_items 292096.
I0302 19:01:47.045431 22895071117440 run.py:483] Algo bellman_ford step 9128 current loss 0.683522, current_train_items 292128.
I0302 19:01:47.076204 22895071117440 run.py:483] Algo bellman_ford step 9129 current loss 0.728799, current_train_items 292160.
I0302 19:01:47.094624 22895071117440 run.py:483] Algo bellman_ford step 9130 current loss 0.261495, current_train_items 292192.
I0302 19:01:47.109962 22895071117440 run.py:483] Algo bellman_ford step 9131 current loss 0.498724, current_train_items 292224.
I0302 19:01:47.132438 22895071117440 run.py:483] Algo bellman_ford step 9132 current loss 0.707055, current_train_items 292256.
I0302 19:01:47.161834 22895071117440 run.py:483] Algo bellman_ford step 9133 current loss 0.763834, current_train_items 292288.
I0302 19:01:47.191712 22895071117440 run.py:483] Algo bellman_ford step 9134 current loss 0.755184, current_train_items 292320.
I0302 19:01:47.209786 22895071117440 run.py:483] Algo bellman_ford step 9135 current loss 0.293489, current_train_items 292352.
I0302 19:01:47.224737 22895071117440 run.py:483] Algo bellman_ford step 9136 current loss 0.435154, current_train_items 292384.
I0302 19:01:47.245864 22895071117440 run.py:483] Algo bellman_ford step 9137 current loss 0.580133, current_train_items 292416.
I0302 19:01:47.274735 22895071117440 run.py:483] Algo bellman_ford step 9138 current loss 0.802078, current_train_items 292448.
I0302 19:01:47.304874 22895071117440 run.py:483] Algo bellman_ford step 9139 current loss 0.907787, current_train_items 292480.
I0302 19:01:47.323086 22895071117440 run.py:483] Algo bellman_ford step 9140 current loss 0.286917, current_train_items 292512.
I0302 19:01:47.338318 22895071117440 run.py:483] Algo bellman_ford step 9141 current loss 0.476305, current_train_items 292544.
I0302 19:01:47.359724 22895071117440 run.py:483] Algo bellman_ford step 9142 current loss 0.546474, current_train_items 292576.
I0302 19:01:47.388236 22895071117440 run.py:483] Algo bellman_ford step 9143 current loss 0.648698, current_train_items 292608.
I0302 19:01:47.420236 22895071117440 run.py:483] Algo bellman_ford step 9144 current loss 0.675901, current_train_items 292640.
I0302 19:01:47.438200 22895071117440 run.py:483] Algo bellman_ford step 9145 current loss 0.294213, current_train_items 292672.
I0302 19:01:47.453627 22895071117440 run.py:483] Algo bellman_ford step 9146 current loss 0.425144, current_train_items 292704.
I0302 19:01:47.475734 22895071117440 run.py:483] Algo bellman_ford step 9147 current loss 0.668518, current_train_items 292736.
I0302 19:01:47.504780 22895071117440 run.py:483] Algo bellman_ford step 9148 current loss 0.796828, current_train_items 292768.
I0302 19:01:47.532769 22895071117440 run.py:483] Algo bellman_ford step 9149 current loss 0.622053, current_train_items 292800.
I0302 19:01:47.550612 22895071117440 run.py:483] Algo bellman_ford step 9150 current loss 0.286380, current_train_items 292832.
I0302 19:01:47.558361 22895071117440 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:01:47.558472 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 19:01:47.574571 22895071117440 run.py:483] Algo bellman_ford step 9151 current loss 0.480968, current_train_items 292864.
I0302 19:01:47.596761 22895071117440 run.py:483] Algo bellman_ford step 9152 current loss 0.501607, current_train_items 292896.
I0302 19:01:47.626367 22895071117440 run.py:483] Algo bellman_ford step 9153 current loss 0.637451, current_train_items 292928.
I0302 19:01:47.655626 22895071117440 run.py:483] Algo bellman_ford step 9154 current loss 0.745514, current_train_items 292960.
I0302 19:01:47.673847 22895071117440 run.py:483] Algo bellman_ford step 9155 current loss 0.271883, current_train_items 292992.
I0302 19:01:47.689751 22895071117440 run.py:483] Algo bellman_ford step 9156 current loss 0.532990, current_train_items 293024.
I0302 19:01:47.712709 22895071117440 run.py:483] Algo bellman_ford step 9157 current loss 0.605398, current_train_items 293056.
I0302 19:01:47.741028 22895071117440 run.py:483] Algo bellman_ford step 9158 current loss 0.603506, current_train_items 293088.
I0302 19:01:47.772808 22895071117440 run.py:483] Algo bellman_ford step 9159 current loss 0.740352, current_train_items 293120.
I0302 19:01:47.791167 22895071117440 run.py:483] Algo bellman_ford step 9160 current loss 0.255112, current_train_items 293152.
I0302 19:01:47.807020 22895071117440 run.py:483] Algo bellman_ford step 9161 current loss 0.410820, current_train_items 293184.
I0302 19:01:47.828627 22895071117440 run.py:483] Algo bellman_ford step 9162 current loss 0.582723, current_train_items 293216.
I0302 19:01:47.856994 22895071117440 run.py:483] Algo bellman_ford step 9163 current loss 0.579872, current_train_items 293248.
I0302 19:01:47.889726 22895071117440 run.py:483] Algo bellman_ford step 9164 current loss 0.829072, current_train_items 293280.
I0302 19:01:47.907976 22895071117440 run.py:483] Algo bellman_ford step 9165 current loss 0.321870, current_train_items 293312.
I0302 19:01:47.923357 22895071117440 run.py:483] Algo bellman_ford step 9166 current loss 0.449100, current_train_items 293344.
I0302 19:01:47.945263 22895071117440 run.py:483] Algo bellman_ford step 9167 current loss 0.537256, current_train_items 293376.
I0302 19:01:47.974267 22895071117440 run.py:483] Algo bellman_ford step 9168 current loss 0.589615, current_train_items 293408.
I0302 19:01:48.004608 22895071117440 run.py:483] Algo bellman_ford step 9169 current loss 0.675905, current_train_items 293440.
I0302 19:01:48.022693 22895071117440 run.py:483] Algo bellman_ford step 9170 current loss 0.271629, current_train_items 293472.
I0302 19:01:48.038203 22895071117440 run.py:483] Algo bellman_ford step 9171 current loss 0.468858, current_train_items 293504.
I0302 19:01:48.061170 22895071117440 run.py:483] Algo bellman_ford step 9172 current loss 0.687275, current_train_items 293536.
I0302 19:01:48.090080 22895071117440 run.py:483] Algo bellman_ford step 9173 current loss 0.588784, current_train_items 293568.
I0302 19:01:48.121792 22895071117440 run.py:483] Algo bellman_ford step 9174 current loss 0.716680, current_train_items 293600.
I0302 19:01:48.139806 22895071117440 run.py:483] Algo bellman_ford step 9175 current loss 0.266099, current_train_items 293632.
I0302 19:01:48.155555 22895071117440 run.py:483] Algo bellman_ford step 9176 current loss 0.337572, current_train_items 293664.
I0302 19:01:48.176532 22895071117440 run.py:483] Algo bellman_ford step 9177 current loss 0.527838, current_train_items 293696.
I0302 19:01:48.203789 22895071117440 run.py:483] Algo bellman_ford step 9178 current loss 0.602267, current_train_items 293728.
I0302 19:01:48.235991 22895071117440 run.py:483] Algo bellman_ford step 9179 current loss 0.745919, current_train_items 293760.
I0302 19:01:48.253411 22895071117440 run.py:483] Algo bellman_ford step 9180 current loss 0.326854, current_train_items 293792.
I0302 19:01:48.268676 22895071117440 run.py:483] Algo bellman_ford step 9181 current loss 0.497842, current_train_items 293824.
I0302 19:01:48.291215 22895071117440 run.py:483] Algo bellman_ford step 9182 current loss 0.609677, current_train_items 293856.
I0302 19:01:48.318853 22895071117440 run.py:483] Algo bellman_ford step 9183 current loss 0.586755, current_train_items 293888.
I0302 19:01:48.349076 22895071117440 run.py:483] Algo bellman_ford step 9184 current loss 0.725226, current_train_items 293920.
I0302 19:01:48.367187 22895071117440 run.py:483] Algo bellman_ford step 9185 current loss 0.317526, current_train_items 293952.
I0302 19:01:48.382998 22895071117440 run.py:483] Algo bellman_ford step 9186 current loss 0.429561, current_train_items 293984.
I0302 19:01:48.404953 22895071117440 run.py:483] Algo bellman_ford step 9187 current loss 0.532728, current_train_items 294016.
I0302 19:01:48.434873 22895071117440 run.py:483] Algo bellman_ford step 9188 current loss 0.602935, current_train_items 294048.
I0302 19:01:48.467888 22895071117440 run.py:483] Algo bellman_ford step 9189 current loss 0.724229, current_train_items 294080.
I0302 19:01:48.486249 22895071117440 run.py:483] Algo bellman_ford step 9190 current loss 0.366938, current_train_items 294112.
I0302 19:01:48.501612 22895071117440 run.py:483] Algo bellman_ford step 9191 current loss 0.541692, current_train_items 294144.
I0302 19:01:48.523813 22895071117440 run.py:483] Algo bellman_ford step 9192 current loss 0.566581, current_train_items 294176.
I0302 19:01:48.552649 22895071117440 run.py:483] Algo bellman_ford step 9193 current loss 0.686829, current_train_items 294208.
I0302 19:01:48.582594 22895071117440 run.py:483] Algo bellman_ford step 9194 current loss 0.664039, current_train_items 294240.
I0302 19:01:48.600446 22895071117440 run.py:483] Algo bellman_ford step 9195 current loss 0.308521, current_train_items 294272.
I0302 19:01:48.616165 22895071117440 run.py:483] Algo bellman_ford step 9196 current loss 0.448490, current_train_items 294304.
I0302 19:01:48.638542 22895071117440 run.py:483] Algo bellman_ford step 9197 current loss 0.647811, current_train_items 294336.
I0302 19:01:48.666586 22895071117440 run.py:483] Algo bellman_ford step 9198 current loss 0.697796, current_train_items 294368.
I0302 19:01:48.696150 22895071117440 run.py:483] Algo bellman_ford step 9199 current loss 0.812301, current_train_items 294400.
I0302 19:01:48.714596 22895071117440 run.py:483] Algo bellman_ford step 9200 current loss 0.310008, current_train_items 294432.
I0302 19:01:48.722503 22895071117440 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:01:48.722606 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:01:48.738642 22895071117440 run.py:483] Algo bellman_ford step 9201 current loss 0.469365, current_train_items 294464.
I0302 19:01:48.760816 22895071117440 run.py:483] Algo bellman_ford step 9202 current loss 0.537646, current_train_items 294496.
I0302 19:01:48.791487 22895071117440 run.py:483] Algo bellman_ford step 9203 current loss 0.763267, current_train_items 294528.
I0302 19:01:48.824201 22895071117440 run.py:483] Algo bellman_ford step 9204 current loss 0.787152, current_train_items 294560.
I0302 19:01:48.842958 22895071117440 run.py:483] Algo bellman_ford step 9205 current loss 0.252304, current_train_items 294592.
I0302 19:01:48.858400 22895071117440 run.py:483] Algo bellman_ford step 9206 current loss 0.469382, current_train_items 294624.
I0302 19:01:48.880573 22895071117440 run.py:483] Algo bellman_ford step 9207 current loss 0.550450, current_train_items 294656.
I0302 19:01:48.910143 22895071117440 run.py:483] Algo bellman_ford step 9208 current loss 0.653537, current_train_items 294688.
I0302 19:01:48.938758 22895071117440 run.py:483] Algo bellman_ford step 9209 current loss 0.726084, current_train_items 294720.
I0302 19:01:48.957011 22895071117440 run.py:483] Algo bellman_ford step 9210 current loss 0.336333, current_train_items 294752.
I0302 19:01:48.972768 22895071117440 run.py:483] Algo bellman_ford step 9211 current loss 0.501937, current_train_items 294784.
I0302 19:01:48.994887 22895071117440 run.py:483] Algo bellman_ford step 9212 current loss 0.585671, current_train_items 294816.
I0302 19:01:49.023576 22895071117440 run.py:483] Algo bellman_ford step 9213 current loss 0.713685, current_train_items 294848.
I0302 19:01:49.053774 22895071117440 run.py:483] Algo bellman_ford step 9214 current loss 0.745947, current_train_items 294880.
I0302 19:01:49.071824 22895071117440 run.py:483] Algo bellman_ford step 9215 current loss 0.334461, current_train_items 294912.
I0302 19:01:49.087378 22895071117440 run.py:483] Algo bellman_ford step 9216 current loss 0.461878, current_train_items 294944.
I0302 19:01:49.109339 22895071117440 run.py:483] Algo bellman_ford step 9217 current loss 0.536980, current_train_items 294976.
I0302 19:01:49.138071 22895071117440 run.py:483] Algo bellman_ford step 9218 current loss 0.611381, current_train_items 295008.
I0302 19:01:49.169032 22895071117440 run.py:483] Algo bellman_ford step 9219 current loss 0.678129, current_train_items 295040.
I0302 19:01:49.186805 22895071117440 run.py:483] Algo bellman_ford step 9220 current loss 0.268296, current_train_items 295072.
I0302 19:01:49.202191 22895071117440 run.py:483] Algo bellman_ford step 9221 current loss 0.411944, current_train_items 295104.
I0302 19:01:49.224737 22895071117440 run.py:483] Algo bellman_ford step 9222 current loss 0.602896, current_train_items 295136.
I0302 19:01:49.253208 22895071117440 run.py:483] Algo bellman_ford step 9223 current loss 0.681198, current_train_items 295168.
I0302 19:01:49.284004 22895071117440 run.py:483] Algo bellman_ford step 9224 current loss 0.728651, current_train_items 295200.
I0302 19:01:49.302036 22895071117440 run.py:483] Algo bellman_ford step 9225 current loss 0.273736, current_train_items 295232.
I0302 19:01:49.317284 22895071117440 run.py:483] Algo bellman_ford step 9226 current loss 0.402925, current_train_items 295264.
I0302 19:01:49.339736 22895071117440 run.py:483] Algo bellman_ford step 9227 current loss 0.586037, current_train_items 295296.
I0302 19:01:49.368552 22895071117440 run.py:483] Algo bellman_ford step 9228 current loss 0.697389, current_train_items 295328.
I0302 19:01:49.397325 22895071117440 run.py:483] Algo bellman_ford step 9229 current loss 0.694251, current_train_items 295360.
I0302 19:01:49.415299 22895071117440 run.py:483] Algo bellman_ford step 9230 current loss 0.280645, current_train_items 295392.
I0302 19:01:49.431010 22895071117440 run.py:483] Algo bellman_ford step 9231 current loss 0.413148, current_train_items 295424.
I0302 19:01:49.453599 22895071117440 run.py:483] Algo bellman_ford step 9232 current loss 0.679368, current_train_items 295456.
I0302 19:01:49.484878 22895071117440 run.py:483] Algo bellman_ford step 9233 current loss 0.644166, current_train_items 295488.
I0302 19:01:49.516471 22895071117440 run.py:483] Algo bellman_ford step 9234 current loss 0.775274, current_train_items 295520.
I0302 19:01:49.534222 22895071117440 run.py:483] Algo bellman_ford step 9235 current loss 0.253498, current_train_items 295552.
I0302 19:01:49.549711 22895071117440 run.py:483] Algo bellman_ford step 9236 current loss 0.462435, current_train_items 295584.
I0302 19:01:49.571434 22895071117440 run.py:483] Algo bellman_ford step 9237 current loss 0.519219, current_train_items 295616.
I0302 19:01:49.601356 22895071117440 run.py:483] Algo bellman_ford step 9238 current loss 0.731599, current_train_items 295648.
I0302 19:01:49.632998 22895071117440 run.py:483] Algo bellman_ford step 9239 current loss 0.687452, current_train_items 295680.
I0302 19:01:49.651301 22895071117440 run.py:483] Algo bellman_ford step 9240 current loss 0.237438, current_train_items 295712.
I0302 19:01:49.667106 22895071117440 run.py:483] Algo bellman_ford step 9241 current loss 0.524961, current_train_items 295744.
I0302 19:01:49.690099 22895071117440 run.py:483] Algo bellman_ford step 9242 current loss 0.649290, current_train_items 295776.
I0302 19:01:49.719479 22895071117440 run.py:483] Algo bellman_ford step 9243 current loss 0.582344, current_train_items 295808.
I0302 19:01:49.749111 22895071117440 run.py:483] Algo bellman_ford step 9244 current loss 0.595719, current_train_items 295840.
I0302 19:01:49.767612 22895071117440 run.py:483] Algo bellman_ford step 9245 current loss 0.265427, current_train_items 295872.
I0302 19:01:49.783049 22895071117440 run.py:483] Algo bellman_ford step 9246 current loss 0.454705, current_train_items 295904.
I0302 19:01:49.804291 22895071117440 run.py:483] Algo bellman_ford step 9247 current loss 0.594655, current_train_items 295936.
I0302 19:01:49.833630 22895071117440 run.py:483] Algo bellman_ford step 9248 current loss 0.751209, current_train_items 295968.
I0302 19:01:49.864762 22895071117440 run.py:483] Algo bellman_ford step 9249 current loss 0.772010, current_train_items 296000.
I0302 19:01:49.883052 22895071117440 run.py:483] Algo bellman_ford step 9250 current loss 0.297505, current_train_items 296032.
I0302 19:01:49.890832 22895071117440 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:01:49.890948 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:49.907065 22895071117440 run.py:483] Algo bellman_ford step 9251 current loss 0.510997, current_train_items 296064.
I0302 19:01:49.929301 22895071117440 run.py:483] Algo bellman_ford step 9252 current loss 0.586955, current_train_items 296096.
I0302 19:01:49.958081 22895071117440 run.py:483] Algo bellman_ford step 9253 current loss 0.611841, current_train_items 296128.
I0302 19:01:49.987982 22895071117440 run.py:483] Algo bellman_ford step 9254 current loss 0.690512, current_train_items 296160.
I0302 19:01:50.006607 22895071117440 run.py:483] Algo bellman_ford step 9255 current loss 0.299007, current_train_items 296192.
I0302 19:01:50.021967 22895071117440 run.py:483] Algo bellman_ford step 9256 current loss 0.352464, current_train_items 296224.
I0302 19:01:50.044355 22895071117440 run.py:483] Algo bellman_ford step 9257 current loss 0.625703, current_train_items 296256.
I0302 19:01:50.073524 22895071117440 run.py:483] Algo bellman_ford step 9258 current loss 0.623327, current_train_items 296288.
I0302 19:01:50.104295 22895071117440 run.py:483] Algo bellman_ford step 9259 current loss 0.630871, current_train_items 296320.
I0302 19:01:50.122920 22895071117440 run.py:483] Algo bellman_ford step 9260 current loss 0.335193, current_train_items 296352.
I0302 19:01:50.139390 22895071117440 run.py:483] Algo bellman_ford step 9261 current loss 0.500701, current_train_items 296384.
I0302 19:01:50.160288 22895071117440 run.py:483] Algo bellman_ford step 9262 current loss 0.487122, current_train_items 296416.
I0302 19:01:50.189375 22895071117440 run.py:483] Algo bellman_ford step 9263 current loss 0.602375, current_train_items 296448.
I0302 19:01:50.220399 22895071117440 run.py:483] Algo bellman_ford step 9264 current loss 0.655712, current_train_items 296480.
I0302 19:01:50.238490 22895071117440 run.py:483] Algo bellman_ford step 9265 current loss 0.300328, current_train_items 296512.
I0302 19:01:50.253936 22895071117440 run.py:483] Algo bellman_ford step 9266 current loss 0.412630, current_train_items 296544.
I0302 19:01:50.274573 22895071117440 run.py:483] Algo bellman_ford step 9267 current loss 0.514953, current_train_items 296576.
I0302 19:01:50.303219 22895071117440 run.py:483] Algo bellman_ford step 9268 current loss 0.606622, current_train_items 296608.
I0302 19:01:50.333857 22895071117440 run.py:483] Algo bellman_ford step 9269 current loss 0.693587, current_train_items 296640.
I0302 19:01:50.352141 22895071117440 run.py:483] Algo bellman_ford step 9270 current loss 0.342360, current_train_items 296672.
I0302 19:01:50.367711 22895071117440 run.py:483] Algo bellman_ford step 9271 current loss 0.448671, current_train_items 296704.
I0302 19:01:50.389317 22895071117440 run.py:483] Algo bellman_ford step 9272 current loss 0.583959, current_train_items 296736.
I0302 19:01:50.419814 22895071117440 run.py:483] Algo bellman_ford step 9273 current loss 0.727350, current_train_items 296768.
I0302 19:01:50.451680 22895071117440 run.py:483] Algo bellman_ford step 9274 current loss 0.777665, current_train_items 296800.
I0302 19:01:50.470138 22895071117440 run.py:483] Algo bellman_ford step 9275 current loss 0.290163, current_train_items 296832.
I0302 19:01:50.485838 22895071117440 run.py:483] Algo bellman_ford step 9276 current loss 0.462092, current_train_items 296864.
I0302 19:01:50.507632 22895071117440 run.py:483] Algo bellman_ford step 9277 current loss 0.629561, current_train_items 296896.
I0302 19:01:50.537192 22895071117440 run.py:483] Algo bellman_ford step 9278 current loss 0.618492, current_train_items 296928.
I0302 19:01:50.568388 22895071117440 run.py:483] Algo bellman_ford step 9279 current loss 0.874312, current_train_items 296960.
I0302 19:01:50.586181 22895071117440 run.py:483] Algo bellman_ford step 9280 current loss 0.304291, current_train_items 296992.
I0302 19:01:50.601721 22895071117440 run.py:483] Algo bellman_ford step 9281 current loss 0.435317, current_train_items 297024.
I0302 19:01:50.623533 22895071117440 run.py:483] Algo bellman_ford step 9282 current loss 0.566675, current_train_items 297056.
I0302 19:01:50.650450 22895071117440 run.py:483] Algo bellman_ford step 9283 current loss 0.505651, current_train_items 297088.
I0302 19:01:50.680535 22895071117440 run.py:483] Algo bellman_ford step 9284 current loss 0.665936, current_train_items 297120.
I0302 19:01:50.698986 22895071117440 run.py:483] Algo bellman_ford step 9285 current loss 0.302000, current_train_items 297152.
I0302 19:01:50.714027 22895071117440 run.py:483] Algo bellman_ford step 9286 current loss 0.424787, current_train_items 297184.
I0302 19:01:50.736973 22895071117440 run.py:483] Algo bellman_ford step 9287 current loss 0.643464, current_train_items 297216.
I0302 19:01:50.764744 22895071117440 run.py:483] Algo bellman_ford step 9288 current loss 0.586594, current_train_items 297248.
I0302 19:01:50.797103 22895071117440 run.py:483] Algo bellman_ford step 9289 current loss 0.759917, current_train_items 297280.
I0302 19:01:50.815737 22895071117440 run.py:483] Algo bellman_ford step 9290 current loss 0.400754, current_train_items 297312.
I0302 19:01:50.831277 22895071117440 run.py:483] Algo bellman_ford step 9291 current loss 0.475837, current_train_items 297344.
I0302 19:01:50.853663 22895071117440 run.py:483] Algo bellman_ford step 9292 current loss 0.577691, current_train_items 297376.
I0302 19:01:50.881818 22895071117440 run.py:483] Algo bellman_ford step 9293 current loss 0.625243, current_train_items 297408.
I0302 19:01:50.913570 22895071117440 run.py:483] Algo bellman_ford step 9294 current loss 0.820928, current_train_items 297440.
I0302 19:01:50.931850 22895071117440 run.py:483] Algo bellman_ford step 9295 current loss 0.283024, current_train_items 297472.
I0302 19:01:50.947152 22895071117440 run.py:483] Algo bellman_ford step 9296 current loss 0.388463, current_train_items 297504.
I0302 19:01:50.968818 22895071117440 run.py:483] Algo bellman_ford step 9297 current loss 0.562534, current_train_items 297536.
I0302 19:01:50.998037 22895071117440 run.py:483] Algo bellman_ford step 9298 current loss 0.580008, current_train_items 297568.
I0302 19:01:51.029243 22895071117440 run.py:483] Algo bellman_ford step 9299 current loss 0.696908, current_train_items 297600.
I0302 19:01:51.047569 22895071117440 run.py:483] Algo bellman_ford step 9300 current loss 0.312481, current_train_items 297632.
I0302 19:01:51.055390 22895071117440 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:01:51.055495 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:51.071548 22895071117440 run.py:483] Algo bellman_ford step 9301 current loss 0.414662, current_train_items 297664.
I0302 19:01:51.093684 22895071117440 run.py:483] Algo bellman_ford step 9302 current loss 0.489058, current_train_items 297696.
I0302 19:01:51.123291 22895071117440 run.py:483] Algo bellman_ford step 9303 current loss 0.639129, current_train_items 297728.
I0302 19:01:51.156310 22895071117440 run.py:483] Algo bellman_ford step 9304 current loss 0.772151, current_train_items 297760.
I0302 19:01:51.175016 22895071117440 run.py:483] Algo bellman_ford step 9305 current loss 0.269042, current_train_items 297792.
I0302 19:01:51.190229 22895071117440 run.py:483] Algo bellman_ford step 9306 current loss 0.466226, current_train_items 297824.
I0302 19:01:51.212725 22895071117440 run.py:483] Algo bellman_ford step 9307 current loss 0.630278, current_train_items 297856.
I0302 19:01:51.241444 22895071117440 run.py:483] Algo bellman_ford step 9308 current loss 0.610989, current_train_items 297888.
I0302 19:01:51.272142 22895071117440 run.py:483] Algo bellman_ford step 9309 current loss 0.686711, current_train_items 297920.
I0302 19:01:51.290337 22895071117440 run.py:483] Algo bellman_ford step 9310 current loss 0.325188, current_train_items 297952.
I0302 19:01:51.305623 22895071117440 run.py:483] Algo bellman_ford step 9311 current loss 0.428676, current_train_items 297984.
I0302 19:01:51.327568 22895071117440 run.py:483] Algo bellman_ford step 9312 current loss 0.605680, current_train_items 298016.
I0302 19:01:51.355728 22895071117440 run.py:483] Algo bellman_ford step 9313 current loss 0.630584, current_train_items 298048.
I0302 19:01:51.386967 22895071117440 run.py:483] Algo bellman_ford step 9314 current loss 0.695179, current_train_items 298080.
I0302 19:01:51.404915 22895071117440 run.py:483] Algo bellman_ford step 9315 current loss 0.327886, current_train_items 298112.
I0302 19:01:51.419946 22895071117440 run.py:483] Algo bellman_ford step 9316 current loss 0.377208, current_train_items 298144.
I0302 19:01:51.442155 22895071117440 run.py:483] Algo bellman_ford step 9317 current loss 0.559419, current_train_items 298176.
I0302 19:01:51.471870 22895071117440 run.py:483] Algo bellman_ford step 9318 current loss 0.672217, current_train_items 298208.
I0302 19:01:51.503043 22895071117440 run.py:483] Algo bellman_ford step 9319 current loss 0.639944, current_train_items 298240.
I0302 19:01:51.521104 22895071117440 run.py:483] Algo bellman_ford step 9320 current loss 0.335694, current_train_items 298272.
I0302 19:01:51.536453 22895071117440 run.py:483] Algo bellman_ford step 9321 current loss 0.425815, current_train_items 298304.
I0302 19:01:51.557149 22895071117440 run.py:483] Algo bellman_ford step 9322 current loss 0.553324, current_train_items 298336.
I0302 19:01:51.585010 22895071117440 run.py:483] Algo bellman_ford step 9323 current loss 0.664079, current_train_items 298368.
I0302 19:01:51.618699 22895071117440 run.py:483] Algo bellman_ford step 9324 current loss 0.910772, current_train_items 298400.
I0302 19:01:51.637142 22895071117440 run.py:483] Algo bellman_ford step 9325 current loss 0.289346, current_train_items 298432.
I0302 19:01:51.652333 22895071117440 run.py:483] Algo bellman_ford step 9326 current loss 0.475326, current_train_items 298464.
I0302 19:01:51.674842 22895071117440 run.py:483] Algo bellman_ford step 9327 current loss 0.631961, current_train_items 298496.
I0302 19:01:51.705011 22895071117440 run.py:483] Algo bellman_ford step 9328 current loss 0.652929, current_train_items 298528.
I0302 19:01:51.733003 22895071117440 run.py:483] Algo bellman_ford step 9329 current loss 0.725313, current_train_items 298560.
I0302 19:01:51.751441 22895071117440 run.py:483] Algo bellman_ford step 9330 current loss 0.292050, current_train_items 298592.
I0302 19:01:51.766870 22895071117440 run.py:483] Algo bellman_ford step 9331 current loss 0.438798, current_train_items 298624.
I0302 19:01:51.788362 22895071117440 run.py:483] Algo bellman_ford step 9332 current loss 0.541187, current_train_items 298656.
I0302 19:01:51.816968 22895071117440 run.py:483] Algo bellman_ford step 9333 current loss 0.653787, current_train_items 298688.
I0302 19:01:51.847609 22895071117440 run.py:483] Algo bellman_ford step 9334 current loss 0.771953, current_train_items 298720.
I0302 19:01:51.865671 22895071117440 run.py:483] Algo bellman_ford step 9335 current loss 0.246577, current_train_items 298752.
I0302 19:01:51.881272 22895071117440 run.py:483] Algo bellman_ford step 9336 current loss 0.505037, current_train_items 298784.
I0302 19:01:51.903728 22895071117440 run.py:483] Algo bellman_ford step 9337 current loss 0.683799, current_train_items 298816.
I0302 19:01:51.932763 22895071117440 run.py:483] Algo bellman_ford step 9338 current loss 0.771868, current_train_items 298848.
I0302 19:01:51.964280 22895071117440 run.py:483] Algo bellman_ford step 9339 current loss 0.881928, current_train_items 298880.
I0302 19:01:51.982278 22895071117440 run.py:483] Algo bellman_ford step 9340 current loss 0.294251, current_train_items 298912.
I0302 19:01:51.997243 22895071117440 run.py:483] Algo bellman_ford step 9341 current loss 0.417097, current_train_items 298944.
I0302 19:01:52.019420 22895071117440 run.py:483] Algo bellman_ford step 9342 current loss 0.513144, current_train_items 298976.
I0302 19:01:52.048797 22895071117440 run.py:483] Algo bellman_ford step 9343 current loss 0.621104, current_train_items 299008.
I0302 19:01:52.080040 22895071117440 run.py:483] Algo bellman_ford step 9344 current loss 0.672567, current_train_items 299040.
I0302 19:01:52.098035 22895071117440 run.py:483] Algo bellman_ford step 9345 current loss 0.336752, current_train_items 299072.
I0302 19:01:52.113288 22895071117440 run.py:483] Algo bellman_ford step 9346 current loss 0.443087, current_train_items 299104.
I0302 19:01:52.135605 22895071117440 run.py:483] Algo bellman_ford step 9347 current loss 0.550766, current_train_items 299136.
I0302 19:01:52.165211 22895071117440 run.py:483] Algo bellman_ford step 9348 current loss 0.614455, current_train_items 299168.
I0302 19:01:52.195106 22895071117440 run.py:483] Algo bellman_ford step 9349 current loss 0.625017, current_train_items 299200.
I0302 19:01:52.212907 22895071117440 run.py:483] Algo bellman_ford step 9350 current loss 0.365796, current_train_items 299232.
I0302 19:01:52.220724 22895071117440 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:01:52.220829 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:52.236722 22895071117440 run.py:483] Algo bellman_ford step 9351 current loss 0.430253, current_train_items 299264.
I0302 19:01:52.259588 22895071117440 run.py:483] Algo bellman_ford step 9352 current loss 0.711484, current_train_items 299296.
I0302 19:01:52.289433 22895071117440 run.py:483] Algo bellman_ford step 9353 current loss 0.654264, current_train_items 299328.
I0302 19:01:52.322838 22895071117440 run.py:483] Algo bellman_ford step 9354 current loss 0.818621, current_train_items 299360.
I0302 19:01:52.341433 22895071117440 run.py:483] Algo bellman_ford step 9355 current loss 0.303487, current_train_items 299392.
I0302 19:01:52.356401 22895071117440 run.py:483] Algo bellman_ford step 9356 current loss 0.380287, current_train_items 299424.
I0302 19:01:52.378880 22895071117440 run.py:483] Algo bellman_ford step 9357 current loss 0.635745, current_train_items 299456.
I0302 19:01:52.407909 22895071117440 run.py:483] Algo bellman_ford step 9358 current loss 0.612506, current_train_items 299488.
I0302 19:01:52.438346 22895071117440 run.py:483] Algo bellman_ford step 9359 current loss 0.647197, current_train_items 299520.
I0302 19:01:52.456826 22895071117440 run.py:483] Algo bellman_ford step 9360 current loss 0.300653, current_train_items 299552.
I0302 19:01:52.472468 22895071117440 run.py:483] Algo bellman_ford step 9361 current loss 0.452469, current_train_items 299584.
I0302 19:01:52.493874 22895071117440 run.py:483] Algo bellman_ford step 9362 current loss 0.514410, current_train_items 299616.
I0302 19:01:52.523284 22895071117440 run.py:483] Algo bellman_ford step 9363 current loss 0.594659, current_train_items 299648.
I0302 19:01:52.555383 22895071117440 run.py:483] Algo bellman_ford step 9364 current loss 0.742795, current_train_items 299680.
I0302 19:01:52.573035 22895071117440 run.py:483] Algo bellman_ford step 9365 current loss 0.347032, current_train_items 299712.
I0302 19:01:52.588694 22895071117440 run.py:483] Algo bellman_ford step 9366 current loss 0.484111, current_train_items 299744.
I0302 19:01:52.612305 22895071117440 run.py:483] Algo bellman_ford step 9367 current loss 0.679911, current_train_items 299776.
I0302 19:01:52.641997 22895071117440 run.py:483] Algo bellman_ford step 9368 current loss 0.623369, current_train_items 299808.
I0302 19:01:52.672317 22895071117440 run.py:483] Algo bellman_ford step 9369 current loss 0.637851, current_train_items 299840.
I0302 19:01:52.690845 22895071117440 run.py:483] Algo bellman_ford step 9370 current loss 0.367368, current_train_items 299872.
I0302 19:01:52.706728 22895071117440 run.py:483] Algo bellman_ford step 9371 current loss 0.478198, current_train_items 299904.
I0302 19:01:52.728725 22895071117440 run.py:483] Algo bellman_ford step 9372 current loss 0.650301, current_train_items 299936.
I0302 19:01:52.758287 22895071117440 run.py:483] Algo bellman_ford step 9373 current loss 0.632352, current_train_items 299968.
I0302 19:01:52.789795 22895071117440 run.py:483] Algo bellman_ford step 9374 current loss 0.652911, current_train_items 300000.
I0302 19:01:52.807874 22895071117440 run.py:483] Algo bellman_ford step 9375 current loss 0.265362, current_train_items 300032.
I0302 19:01:52.823068 22895071117440 run.py:483] Algo bellman_ford step 9376 current loss 0.510280, current_train_items 300064.
I0302 19:01:52.844627 22895071117440 run.py:483] Algo bellman_ford step 9377 current loss 0.514498, current_train_items 300096.
I0302 19:01:52.872708 22895071117440 run.py:483] Algo bellman_ford step 9378 current loss 0.706927, current_train_items 300128.
I0302 19:01:52.903613 22895071117440 run.py:483] Algo bellman_ford step 9379 current loss 0.710889, current_train_items 300160.
I0302 19:01:52.921594 22895071117440 run.py:483] Algo bellman_ford step 9380 current loss 0.260547, current_train_items 300192.
I0302 19:01:52.936448 22895071117440 run.py:483] Algo bellman_ford step 9381 current loss 0.477319, current_train_items 300224.
I0302 19:01:52.958487 22895071117440 run.py:483] Algo bellman_ford step 9382 current loss 0.642380, current_train_items 300256.
I0302 19:01:52.986711 22895071117440 run.py:483] Algo bellman_ford step 9383 current loss 0.595984, current_train_items 300288.
I0302 19:01:53.017803 22895071117440 run.py:483] Algo bellman_ford step 9384 current loss 0.671747, current_train_items 300320.
I0302 19:01:53.036413 22895071117440 run.py:483] Algo bellman_ford step 9385 current loss 0.296959, current_train_items 300352.
I0302 19:01:53.052521 22895071117440 run.py:483] Algo bellman_ford step 9386 current loss 0.538439, current_train_items 300384.
I0302 19:01:53.074167 22895071117440 run.py:483] Algo bellman_ford step 9387 current loss 0.642674, current_train_items 300416.
I0302 19:01:53.102940 22895071117440 run.py:483] Algo bellman_ford step 9388 current loss 0.696518, current_train_items 300448.
I0302 19:01:53.135932 22895071117440 run.py:483] Algo bellman_ford step 9389 current loss 0.763446, current_train_items 300480.
I0302 19:01:53.154596 22895071117440 run.py:483] Algo bellman_ford step 9390 current loss 0.322167, current_train_items 300512.
I0302 19:01:53.170496 22895071117440 run.py:483] Algo bellman_ford step 9391 current loss 0.401310, current_train_items 300544.
I0302 19:01:53.192114 22895071117440 run.py:483] Algo bellman_ford step 9392 current loss 0.565676, current_train_items 300576.
I0302 19:01:53.220750 22895071117440 run.py:483] Algo bellman_ford step 9393 current loss 0.616775, current_train_items 300608.
I0302 19:01:53.251652 22895071117440 run.py:483] Algo bellman_ford step 9394 current loss 0.867064, current_train_items 300640.
I0302 19:01:53.269778 22895071117440 run.py:483] Algo bellman_ford step 9395 current loss 0.368969, current_train_items 300672.
I0302 19:01:53.285160 22895071117440 run.py:483] Algo bellman_ford step 9396 current loss 0.489904, current_train_items 300704.
I0302 19:01:53.308034 22895071117440 run.py:483] Algo bellman_ford step 9397 current loss 0.583974, current_train_items 300736.
I0302 19:01:53.336867 22895071117440 run.py:483] Algo bellman_ford step 9398 current loss 0.553439, current_train_items 300768.
I0302 19:01:53.368552 22895071117440 run.py:483] Algo bellman_ford step 9399 current loss 0.704634, current_train_items 300800.
I0302 19:01:53.387028 22895071117440 run.py:483] Algo bellman_ford step 9400 current loss 0.293761, current_train_items 300832.
I0302 19:01:53.394721 22895071117440 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:01:53.394826 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:53.411005 22895071117440 run.py:483] Algo bellman_ford step 9401 current loss 0.475728, current_train_items 300864.
I0302 19:01:53.434429 22895071117440 run.py:483] Algo bellman_ford step 9402 current loss 0.743713, current_train_items 300896.
I0302 19:01:53.463362 22895071117440 run.py:483] Algo bellman_ford step 9403 current loss 0.607256, current_train_items 300928.
I0302 19:01:53.495281 22895071117440 run.py:483] Algo bellman_ford step 9404 current loss 0.682261, current_train_items 300960.
I0302 19:01:53.513405 22895071117440 run.py:483] Algo bellman_ford step 9405 current loss 0.277368, current_train_items 300992.
I0302 19:01:53.528632 22895071117440 run.py:483] Algo bellman_ford step 9406 current loss 0.423786, current_train_items 301024.
I0302 19:01:53.552321 22895071117440 run.py:483] Algo bellman_ford step 9407 current loss 0.685745, current_train_items 301056.
I0302 19:01:53.580978 22895071117440 run.py:483] Algo bellman_ford step 9408 current loss 0.649541, current_train_items 301088.
I0302 19:01:53.613422 22895071117440 run.py:483] Algo bellman_ford step 9409 current loss 0.745847, current_train_items 301120.
I0302 19:01:53.631989 22895071117440 run.py:483] Algo bellman_ford step 9410 current loss 0.337151, current_train_items 301152.
I0302 19:01:53.647253 22895071117440 run.py:483] Algo bellman_ford step 9411 current loss 0.513681, current_train_items 301184.
I0302 19:01:53.669252 22895071117440 run.py:483] Algo bellman_ford step 9412 current loss 0.547914, current_train_items 301216.
I0302 19:01:53.698327 22895071117440 run.py:483] Algo bellman_ford step 9413 current loss 0.588262, current_train_items 301248.
I0302 19:01:53.728663 22895071117440 run.py:483] Algo bellman_ford step 9414 current loss 0.732123, current_train_items 301280.
I0302 19:01:53.747015 22895071117440 run.py:483] Algo bellman_ford step 9415 current loss 0.316534, current_train_items 301312.
I0302 19:01:53.762678 22895071117440 run.py:483] Algo bellman_ford step 9416 current loss 0.394992, current_train_items 301344.
I0302 19:01:53.784965 22895071117440 run.py:483] Algo bellman_ford step 9417 current loss 0.547455, current_train_items 301376.
I0302 19:01:53.813829 22895071117440 run.py:483] Algo bellman_ford step 9418 current loss 0.617288, current_train_items 301408.
I0302 19:01:53.843099 22895071117440 run.py:483] Algo bellman_ford step 9419 current loss 0.560037, current_train_items 301440.
I0302 19:01:53.861234 22895071117440 run.py:483] Algo bellman_ford step 9420 current loss 0.337334, current_train_items 301472.
I0302 19:01:53.877067 22895071117440 run.py:483] Algo bellman_ford step 9421 current loss 0.436361, current_train_items 301504.
I0302 19:01:53.899698 22895071117440 run.py:483] Algo bellman_ford step 9422 current loss 0.566064, current_train_items 301536.
I0302 19:01:53.928191 22895071117440 run.py:483] Algo bellman_ford step 9423 current loss 0.666634, current_train_items 301568.
I0302 19:01:53.959493 22895071117440 run.py:483] Algo bellman_ford step 9424 current loss 0.832917, current_train_items 301600.
I0302 19:01:53.977843 22895071117440 run.py:483] Algo bellman_ford step 9425 current loss 0.351077, current_train_items 301632.
I0302 19:01:53.992812 22895071117440 run.py:483] Algo bellman_ford step 9426 current loss 0.458135, current_train_items 301664.
I0302 19:01:54.014501 22895071117440 run.py:483] Algo bellman_ford step 9427 current loss 0.578769, current_train_items 301696.
I0302 19:01:54.044278 22895071117440 run.py:483] Algo bellman_ford step 9428 current loss 0.722303, current_train_items 301728.
I0302 19:01:54.072329 22895071117440 run.py:483] Algo bellman_ford step 9429 current loss 0.799796, current_train_items 301760.
I0302 19:01:54.090623 22895071117440 run.py:483] Algo bellman_ford step 9430 current loss 0.279402, current_train_items 301792.
I0302 19:01:54.105984 22895071117440 run.py:483] Algo bellman_ford step 9431 current loss 0.502742, current_train_items 301824.
I0302 19:01:54.127905 22895071117440 run.py:483] Algo bellman_ford step 9432 current loss 0.562877, current_train_items 301856.
I0302 19:01:54.156234 22895071117440 run.py:483] Algo bellman_ford step 9433 current loss 0.597871, current_train_items 301888.
I0302 19:01:54.187796 22895071117440 run.py:483] Algo bellman_ford step 9434 current loss 0.771666, current_train_items 301920.
I0302 19:01:54.205854 22895071117440 run.py:483] Algo bellman_ford step 9435 current loss 0.279392, current_train_items 301952.
I0302 19:01:54.221580 22895071117440 run.py:483] Algo bellman_ford step 9436 current loss 0.406469, current_train_items 301984.
I0302 19:01:54.243366 22895071117440 run.py:483] Algo bellman_ford step 9437 current loss 0.537949, current_train_items 302016.
I0302 19:01:54.272801 22895071117440 run.py:483] Algo bellman_ford step 9438 current loss 0.674612, current_train_items 302048.
I0302 19:01:54.302888 22895071117440 run.py:483] Algo bellman_ford step 9439 current loss 0.787444, current_train_items 302080.
I0302 19:01:54.321211 22895071117440 run.py:483] Algo bellman_ford step 9440 current loss 0.281999, current_train_items 302112.
I0302 19:01:54.336932 22895071117440 run.py:483] Algo bellman_ford step 9441 current loss 0.439886, current_train_items 302144.
I0302 19:01:54.359170 22895071117440 run.py:483] Algo bellman_ford step 9442 current loss 0.657350, current_train_items 302176.
I0302 19:01:54.388690 22895071117440 run.py:483] Algo bellman_ford step 9443 current loss 0.560170, current_train_items 302208.
I0302 19:01:54.421950 22895071117440 run.py:483] Algo bellman_ford step 9444 current loss 0.708820, current_train_items 302240.
I0302 19:01:54.440243 22895071117440 run.py:483] Algo bellman_ford step 9445 current loss 0.380125, current_train_items 302272.
I0302 19:01:54.455245 22895071117440 run.py:483] Algo bellman_ford step 9446 current loss 0.387388, current_train_items 302304.
I0302 19:01:54.477068 22895071117440 run.py:483] Algo bellman_ford step 9447 current loss 0.639644, current_train_items 302336.
I0302 19:01:54.505834 22895071117440 run.py:483] Algo bellman_ford step 9448 current loss 0.706362, current_train_items 302368.
I0302 19:01:54.535279 22895071117440 run.py:483] Algo bellman_ford step 9449 current loss 0.658724, current_train_items 302400.
I0302 19:01:54.553440 22895071117440 run.py:483] Algo bellman_ford step 9450 current loss 0.296792, current_train_items 302432.
I0302 19:01:54.561485 22895071117440 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:01:54.561591 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:54.577705 22895071117440 run.py:483] Algo bellman_ford step 9451 current loss 0.412751, current_train_items 302464.
I0302 19:01:54.599007 22895071117440 run.py:483] Algo bellman_ford step 9452 current loss 0.558296, current_train_items 302496.
I0302 19:01:54.628715 22895071117440 run.py:483] Algo bellman_ford step 9453 current loss 0.595275, current_train_items 302528.
I0302 19:01:54.660892 22895071117440 run.py:483] Algo bellman_ford step 9454 current loss 0.773521, current_train_items 302560.
I0302 19:01:54.679778 22895071117440 run.py:483] Algo bellman_ford step 9455 current loss 0.291847, current_train_items 302592.
I0302 19:01:54.695168 22895071117440 run.py:483] Algo bellman_ford step 9456 current loss 0.433173, current_train_items 302624.
I0302 19:01:54.716745 22895071117440 run.py:483] Algo bellman_ford step 9457 current loss 0.524591, current_train_items 302656.
I0302 19:01:54.745795 22895071117440 run.py:483] Algo bellman_ford step 9458 current loss 0.637146, current_train_items 302688.
I0302 19:01:54.775425 22895071117440 run.py:483] Algo bellman_ford step 9459 current loss 0.696410, current_train_items 302720.
I0302 19:01:54.794055 22895071117440 run.py:483] Algo bellman_ford step 9460 current loss 0.259333, current_train_items 302752.
I0302 19:01:54.809993 22895071117440 run.py:483] Algo bellman_ford step 9461 current loss 0.429032, current_train_items 302784.
I0302 19:01:54.831327 22895071117440 run.py:483] Algo bellman_ford step 9462 current loss 0.486064, current_train_items 302816.
I0302 19:01:54.861032 22895071117440 run.py:483] Algo bellman_ford step 9463 current loss 0.692541, current_train_items 302848.
I0302 19:01:54.891739 22895071117440 run.py:483] Algo bellman_ford step 9464 current loss 0.774967, current_train_items 302880.
I0302 19:01:54.909390 22895071117440 run.py:483] Algo bellman_ford step 9465 current loss 0.322111, current_train_items 302912.
I0302 19:01:54.925110 22895071117440 run.py:483] Algo bellman_ford step 9466 current loss 0.486729, current_train_items 302944.
I0302 19:01:54.948795 22895071117440 run.py:483] Algo bellman_ford step 9467 current loss 0.664012, current_train_items 302976.
I0302 19:01:54.978518 22895071117440 run.py:483] Algo bellman_ford step 9468 current loss 0.592463, current_train_items 303008.
I0302 19:01:55.010243 22895071117440 run.py:483] Algo bellman_ford step 9469 current loss 0.732378, current_train_items 303040.
I0302 19:01:55.028548 22895071117440 run.py:483] Algo bellman_ford step 9470 current loss 0.280875, current_train_items 303072.
I0302 19:01:55.043876 22895071117440 run.py:483] Algo bellman_ford step 9471 current loss 0.460068, current_train_items 303104.
I0302 19:01:55.065772 22895071117440 run.py:483] Algo bellman_ford step 9472 current loss 0.535483, current_train_items 303136.
I0302 19:01:55.095059 22895071117440 run.py:483] Algo bellman_ford step 9473 current loss 0.677784, current_train_items 303168.
I0302 19:01:55.125202 22895071117440 run.py:483] Algo bellman_ford step 9474 current loss 0.707817, current_train_items 303200.
I0302 19:01:55.143127 22895071117440 run.py:483] Algo bellman_ford step 9475 current loss 0.294594, current_train_items 303232.
I0302 19:01:55.158888 22895071117440 run.py:483] Algo bellman_ford step 9476 current loss 0.494064, current_train_items 303264.
I0302 19:01:55.180529 22895071117440 run.py:483] Algo bellman_ford step 9477 current loss 0.586993, current_train_items 303296.
I0302 19:01:55.208618 22895071117440 run.py:483] Algo bellman_ford step 9478 current loss 0.682091, current_train_items 303328.
I0302 19:01:55.238851 22895071117440 run.py:483] Algo bellman_ford step 9479 current loss 0.725501, current_train_items 303360.
I0302 19:01:55.256887 22895071117440 run.py:483] Algo bellman_ford step 9480 current loss 0.440950, current_train_items 303392.
I0302 19:01:55.272036 22895071117440 run.py:483] Algo bellman_ford step 9481 current loss 0.481917, current_train_items 303424.
I0302 19:01:55.295419 22895071117440 run.py:483] Algo bellman_ford step 9482 current loss 0.742418, current_train_items 303456.
I0302 19:01:55.323039 22895071117440 run.py:483] Algo bellman_ford step 9483 current loss 0.573242, current_train_items 303488.
I0302 19:01:55.352724 22895071117440 run.py:483] Algo bellman_ford step 9484 current loss 0.780881, current_train_items 303520.
I0302 19:01:55.371228 22895071117440 run.py:483] Algo bellman_ford step 9485 current loss 0.333347, current_train_items 303552.
I0302 19:01:55.387171 22895071117440 run.py:483] Algo bellman_ford step 9486 current loss 0.410563, current_train_items 303584.
I0302 19:01:55.410195 22895071117440 run.py:483] Algo bellman_ford step 9487 current loss 0.535102, current_train_items 303616.
I0302 19:01:55.440381 22895071117440 run.py:483] Algo bellman_ford step 9488 current loss 0.616551, current_train_items 303648.
I0302 19:01:55.470422 22895071117440 run.py:483] Algo bellman_ford step 9489 current loss 0.656913, current_train_items 303680.
I0302 19:01:55.488857 22895071117440 run.py:483] Algo bellman_ford step 9490 current loss 0.258569, current_train_items 303712.
I0302 19:01:55.504312 22895071117440 run.py:483] Algo bellman_ford step 9491 current loss 0.462860, current_train_items 303744.
I0302 19:01:55.526414 22895071117440 run.py:483] Algo bellman_ford step 9492 current loss 0.525427, current_train_items 303776.
I0302 19:01:55.554461 22895071117440 run.py:483] Algo bellman_ford step 9493 current loss 0.513527, current_train_items 303808.
I0302 19:01:55.585542 22895071117440 run.py:483] Algo bellman_ford step 9494 current loss 0.713958, current_train_items 303840.
I0302 19:01:55.603667 22895071117440 run.py:483] Algo bellman_ford step 9495 current loss 0.287992, current_train_items 303872.
I0302 19:01:55.619523 22895071117440 run.py:483] Algo bellman_ford step 9496 current loss 0.447583, current_train_items 303904.
I0302 19:01:55.641206 22895071117440 run.py:483] Algo bellman_ford step 9497 current loss 0.571258, current_train_items 303936.
I0302 19:01:55.669058 22895071117440 run.py:483] Algo bellman_ford step 9498 current loss 0.597691, current_train_items 303968.
I0302 19:01:55.700564 22895071117440 run.py:483] Algo bellman_ford step 9499 current loss 0.744985, current_train_items 304000.
I0302 19:01:55.719510 22895071117440 run.py:483] Algo bellman_ford step 9500 current loss 0.237047, current_train_items 304032.
I0302 19:01:55.727468 22895071117440 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:01:55.727572 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:55.743315 22895071117440 run.py:483] Algo bellman_ford step 9501 current loss 0.396125, current_train_items 304064.
I0302 19:01:55.765788 22895071117440 run.py:483] Algo bellman_ford step 9502 current loss 0.566130, current_train_items 304096.
I0302 19:01:55.794977 22895071117440 run.py:483] Algo bellman_ford step 9503 current loss 0.642627, current_train_items 304128.
I0302 19:01:55.825356 22895071117440 run.py:483] Algo bellman_ford step 9504 current loss 0.647912, current_train_items 304160.
I0302 19:01:55.844053 22895071117440 run.py:483] Algo bellman_ford step 9505 current loss 0.310636, current_train_items 304192.
I0302 19:01:55.859833 22895071117440 run.py:483] Algo bellman_ford step 9506 current loss 0.447330, current_train_items 304224.
I0302 19:01:55.882863 22895071117440 run.py:483] Algo bellman_ford step 9507 current loss 0.528889, current_train_items 304256.
I0302 19:01:55.910989 22895071117440 run.py:483] Algo bellman_ford step 9508 current loss 0.581370, current_train_items 304288.
I0302 19:01:55.944444 22895071117440 run.py:483] Algo bellman_ford step 9509 current loss 0.751403, current_train_items 304320.
I0302 19:01:55.962670 22895071117440 run.py:483] Algo bellman_ford step 9510 current loss 0.330279, current_train_items 304352.
I0302 19:01:55.978599 22895071117440 run.py:483] Algo bellman_ford step 9511 current loss 0.467271, current_train_items 304384.
I0302 19:01:56.000980 22895071117440 run.py:483] Algo bellman_ford step 9512 current loss 0.605299, current_train_items 304416.
I0302 19:01:56.030091 22895071117440 run.py:483] Algo bellman_ford step 9513 current loss 0.694952, current_train_items 304448.
I0302 19:01:56.059877 22895071117440 run.py:483] Algo bellman_ford step 9514 current loss 0.702718, current_train_items 304480.
I0302 19:01:56.078014 22895071117440 run.py:483] Algo bellman_ford step 9515 current loss 0.264383, current_train_items 304512.
I0302 19:01:56.092965 22895071117440 run.py:483] Algo bellman_ford step 9516 current loss 0.392439, current_train_items 304544.
I0302 19:01:56.115143 22895071117440 run.py:483] Algo bellman_ford step 9517 current loss 0.579225, current_train_items 304576.
I0302 19:01:56.144017 22895071117440 run.py:483] Algo bellman_ford step 9518 current loss 0.620757, current_train_items 304608.
I0302 19:01:56.173670 22895071117440 run.py:483] Algo bellman_ford step 9519 current loss 0.598025, current_train_items 304640.
I0302 19:01:56.191753 22895071117440 run.py:483] Algo bellman_ford step 9520 current loss 0.297743, current_train_items 304672.
I0302 19:01:56.206402 22895071117440 run.py:483] Algo bellman_ford step 9521 current loss 0.430858, current_train_items 304704.
I0302 19:01:56.228206 22895071117440 run.py:483] Algo bellman_ford step 9522 current loss 0.541441, current_train_items 304736.
I0302 19:01:56.255722 22895071117440 run.py:483] Algo bellman_ford step 9523 current loss 0.590241, current_train_items 304768.
I0302 19:01:56.285563 22895071117440 run.py:483] Algo bellman_ford step 9524 current loss 0.660848, current_train_items 304800.
I0302 19:01:56.303543 22895071117440 run.py:483] Algo bellman_ford step 9525 current loss 0.236479, current_train_items 304832.
I0302 19:01:56.318869 22895071117440 run.py:483] Algo bellman_ford step 9526 current loss 0.415462, current_train_items 304864.
I0302 19:01:56.341518 22895071117440 run.py:483] Algo bellman_ford step 9527 current loss 0.642709, current_train_items 304896.
I0302 19:01:56.371310 22895071117440 run.py:483] Algo bellman_ford step 9528 current loss 0.657774, current_train_items 304928.
I0302 19:01:56.403069 22895071117440 run.py:483] Algo bellman_ford step 9529 current loss 0.806600, current_train_items 304960.
I0302 19:01:56.420702 22895071117440 run.py:483] Algo bellman_ford step 9530 current loss 0.275072, current_train_items 304992.
I0302 19:01:56.435972 22895071117440 run.py:483] Algo bellman_ford step 9531 current loss 0.438400, current_train_items 305024.
I0302 19:01:56.457895 22895071117440 run.py:483] Algo bellman_ford step 9532 current loss 0.532512, current_train_items 305056.
I0302 19:01:56.488732 22895071117440 run.py:483] Algo bellman_ford step 9533 current loss 0.652230, current_train_items 305088.
I0302 19:01:56.519493 22895071117440 run.py:483] Algo bellman_ford step 9534 current loss 0.669726, current_train_items 305120.
I0302 19:01:56.537726 22895071117440 run.py:483] Algo bellman_ford step 9535 current loss 0.308420, current_train_items 305152.
I0302 19:01:56.553114 22895071117440 run.py:483] Algo bellman_ford step 9536 current loss 0.497587, current_train_items 305184.
I0302 19:01:56.575589 22895071117440 run.py:483] Algo bellman_ford step 9537 current loss 0.545340, current_train_items 305216.
I0302 19:01:56.605599 22895071117440 run.py:483] Algo bellman_ford step 9538 current loss 0.617139, current_train_items 305248.
I0302 19:01:56.634378 22895071117440 run.py:483] Algo bellman_ford step 9539 current loss 0.696938, current_train_items 305280.
I0302 19:01:56.652679 22895071117440 run.py:483] Algo bellman_ford step 9540 current loss 0.291784, current_train_items 305312.
I0302 19:01:56.667805 22895071117440 run.py:483] Algo bellman_ford step 9541 current loss 0.475712, current_train_items 305344.
I0302 19:01:56.689495 22895071117440 run.py:483] Algo bellman_ford step 9542 current loss 0.570190, current_train_items 305376.
I0302 19:01:56.720062 22895071117440 run.py:483] Algo bellman_ford step 9543 current loss 0.709549, current_train_items 305408.
I0302 19:01:56.751125 22895071117440 run.py:483] Algo bellman_ford step 9544 current loss 0.755277, current_train_items 305440.
I0302 19:01:56.769022 22895071117440 run.py:483] Algo bellman_ford step 9545 current loss 0.286306, current_train_items 305472.
I0302 19:01:56.783751 22895071117440 run.py:483] Algo bellman_ford step 9546 current loss 0.452471, current_train_items 305504.
I0302 19:01:56.803780 22895071117440 run.py:483] Algo bellman_ford step 9547 current loss 0.501472, current_train_items 305536.
I0302 19:01:56.832693 22895071117440 run.py:483] Algo bellman_ford step 9548 current loss 0.634830, current_train_items 305568.
I0302 19:01:56.864084 22895071117440 run.py:483] Algo bellman_ford step 9549 current loss 0.705718, current_train_items 305600.
I0302 19:01:56.882377 22895071117440 run.py:483] Algo bellman_ford step 9550 current loss 0.218840, current_train_items 305632.
I0302 19:01:56.890177 22895071117440 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:01:56.890284 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:56.906308 22895071117440 run.py:483] Algo bellman_ford step 9551 current loss 0.479984, current_train_items 305664.
I0302 19:01:56.929218 22895071117440 run.py:483] Algo bellman_ford step 9552 current loss 0.646132, current_train_items 305696.
I0302 19:01:56.959890 22895071117440 run.py:483] Algo bellman_ford step 9553 current loss 0.673261, current_train_items 305728.
I0302 19:01:56.990657 22895071117440 run.py:483] Algo bellman_ford step 9554 current loss 0.640367, current_train_items 305760.
I0302 19:01:57.009296 22895071117440 run.py:483] Algo bellman_ford step 9555 current loss 0.329330, current_train_items 305792.
I0302 19:01:57.024185 22895071117440 run.py:483] Algo bellman_ford step 9556 current loss 0.413292, current_train_items 305824.
I0302 19:01:57.046785 22895071117440 run.py:483] Algo bellman_ford step 9557 current loss 0.574335, current_train_items 305856.
I0302 19:01:57.075077 22895071117440 run.py:483] Algo bellman_ford step 9558 current loss 0.695878, current_train_items 305888.
I0302 19:01:57.105335 22895071117440 run.py:483] Algo bellman_ford step 9559 current loss 0.754964, current_train_items 305920.
I0302 19:01:57.123564 22895071117440 run.py:483] Algo bellman_ford step 9560 current loss 0.418107, current_train_items 305952.
I0302 19:01:57.139227 22895071117440 run.py:483] Algo bellman_ford step 9561 current loss 0.458071, current_train_items 305984.
I0302 19:01:57.160611 22895071117440 run.py:483] Algo bellman_ford step 9562 current loss 0.560060, current_train_items 306016.
I0302 19:01:57.189076 22895071117440 run.py:483] Algo bellman_ford step 9563 current loss 0.600425, current_train_items 306048.
I0302 19:01:57.219170 22895071117440 run.py:483] Algo bellman_ford step 9564 current loss 0.643946, current_train_items 306080.
I0302 19:01:57.237117 22895071117440 run.py:483] Algo bellman_ford step 9565 current loss 0.329560, current_train_items 306112.
I0302 19:01:57.252893 22895071117440 run.py:483] Algo bellman_ford step 9566 current loss 0.467786, current_train_items 306144.
I0302 19:01:57.276030 22895071117440 run.py:483] Algo bellman_ford step 9567 current loss 0.647440, current_train_items 306176.
I0302 19:01:57.305321 22895071117440 run.py:483] Algo bellman_ford step 9568 current loss 0.786495, current_train_items 306208.
I0302 19:01:57.336187 22895071117440 run.py:483] Algo bellman_ford step 9569 current loss 0.795215, current_train_items 306240.
I0302 19:01:57.354681 22895071117440 run.py:483] Algo bellman_ford step 9570 current loss 0.308357, current_train_items 306272.
I0302 19:01:57.370335 22895071117440 run.py:483] Algo bellman_ford step 9571 current loss 0.449951, current_train_items 306304.
I0302 19:01:57.392825 22895071117440 run.py:483] Algo bellman_ford step 9572 current loss 0.642896, current_train_items 306336.
I0302 19:01:57.421146 22895071117440 run.py:483] Algo bellman_ford step 9573 current loss 0.529795, current_train_items 306368.
I0302 19:01:57.452686 22895071117440 run.py:483] Algo bellman_ford step 9574 current loss 0.727930, current_train_items 306400.
I0302 19:01:57.471425 22895071117440 run.py:483] Algo bellman_ford step 9575 current loss 0.290226, current_train_items 306432.
I0302 19:01:57.487709 22895071117440 run.py:483] Algo bellman_ford step 9576 current loss 0.416380, current_train_items 306464.
I0302 19:01:57.510094 22895071117440 run.py:483] Algo bellman_ford step 9577 current loss 0.747777, current_train_items 306496.
I0302 19:01:57.538853 22895071117440 run.py:483] Algo bellman_ford step 9578 current loss 0.556858, current_train_items 306528.
I0302 19:01:57.569933 22895071117440 run.py:483] Algo bellman_ford step 9579 current loss 0.691231, current_train_items 306560.
I0302 19:01:57.587965 22895071117440 run.py:483] Algo bellman_ford step 9580 current loss 0.284402, current_train_items 306592.
I0302 19:01:57.603133 22895071117440 run.py:483] Algo bellman_ford step 9581 current loss 0.465734, current_train_items 306624.
I0302 19:01:57.626646 22895071117440 run.py:483] Algo bellman_ford step 9582 current loss 0.543690, current_train_items 306656.
I0302 19:01:57.656713 22895071117440 run.py:483] Algo bellman_ford step 9583 current loss 0.635127, current_train_items 306688.
I0302 19:01:57.688309 22895071117440 run.py:483] Algo bellman_ford step 9584 current loss 0.688997, current_train_items 306720.
I0302 19:01:57.706835 22895071117440 run.py:483] Algo bellman_ford step 9585 current loss 0.304020, current_train_items 306752.
I0302 19:01:57.722739 22895071117440 run.py:483] Algo bellman_ford step 9586 current loss 0.403840, current_train_items 306784.
I0302 19:01:57.744919 22895071117440 run.py:483] Algo bellman_ford step 9587 current loss 0.553386, current_train_items 306816.
I0302 19:01:57.774430 22895071117440 run.py:483] Algo bellman_ford step 9588 current loss 0.747952, current_train_items 306848.
I0302 19:01:57.806941 22895071117440 run.py:483] Algo bellman_ford step 9589 current loss 0.814920, current_train_items 306880.
I0302 19:01:57.825349 22895071117440 run.py:483] Algo bellman_ford step 9590 current loss 0.228764, current_train_items 306912.
I0302 19:01:57.841071 22895071117440 run.py:483] Algo bellman_ford step 9591 current loss 0.423182, current_train_items 306944.
I0302 19:01:57.863242 22895071117440 run.py:483] Algo bellman_ford step 9592 current loss 0.625112, current_train_items 306976.
I0302 19:01:57.892234 22895071117440 run.py:483] Algo bellman_ford step 9593 current loss 0.612992, current_train_items 307008.
I0302 19:01:57.922571 22895071117440 run.py:483] Algo bellman_ford step 9594 current loss 0.768532, current_train_items 307040.
I0302 19:01:57.941082 22895071117440 run.py:483] Algo bellman_ford step 9595 current loss 0.376953, current_train_items 307072.
I0302 19:01:57.956711 22895071117440 run.py:483] Algo bellman_ford step 9596 current loss 0.457737, current_train_items 307104.
I0302 19:01:57.979351 22895071117440 run.py:483] Algo bellman_ford step 9597 current loss 0.593700, current_train_items 307136.
I0302 19:01:58.007212 22895071117440 run.py:483] Algo bellman_ford step 9598 current loss 0.533915, current_train_items 307168.
I0302 19:01:58.038058 22895071117440 run.py:483] Algo bellman_ford step 9599 current loss 0.745443, current_train_items 307200.
I0302 19:01:58.056798 22895071117440 run.py:483] Algo bellman_ford step 9600 current loss 0.287378, current_train_items 307232.
I0302 19:01:58.064570 22895071117440 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:01:58.064676 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:01:58.081141 22895071117440 run.py:483] Algo bellman_ford step 9601 current loss 0.467295, current_train_items 307264.
I0302 19:01:58.104416 22895071117440 run.py:483] Algo bellman_ford step 9602 current loss 0.578212, current_train_items 307296.
I0302 19:01:58.133590 22895071117440 run.py:483] Algo bellman_ford step 9603 current loss 0.601827, current_train_items 307328.
I0302 19:01:58.166556 22895071117440 run.py:483] Algo bellman_ford step 9604 current loss 0.768441, current_train_items 307360.
I0302 19:01:58.185535 22895071117440 run.py:483] Algo bellman_ford step 9605 current loss 0.289985, current_train_items 307392.
I0302 19:01:58.200705 22895071117440 run.py:483] Algo bellman_ford step 9606 current loss 0.458088, current_train_items 307424.
I0302 19:01:58.222814 22895071117440 run.py:483] Algo bellman_ford step 9607 current loss 0.594494, current_train_items 307456.
I0302 19:01:58.251603 22895071117440 run.py:483] Algo bellman_ford step 9608 current loss 0.684821, current_train_items 307488.
I0302 19:01:58.282660 22895071117440 run.py:483] Algo bellman_ford step 9609 current loss 0.774156, current_train_items 307520.
I0302 19:01:58.300582 22895071117440 run.py:483] Algo bellman_ford step 9610 current loss 0.336298, current_train_items 307552.
I0302 19:01:58.316167 22895071117440 run.py:483] Algo bellman_ford step 9611 current loss 0.490785, current_train_items 307584.
I0302 19:01:58.338855 22895071117440 run.py:483] Algo bellman_ford step 9612 current loss 0.593925, current_train_items 307616.
I0302 19:01:58.367985 22895071117440 run.py:483] Algo bellman_ford step 9613 current loss 0.659791, current_train_items 307648.
I0302 19:01:58.398803 22895071117440 run.py:483] Algo bellman_ford step 9614 current loss 0.676687, current_train_items 307680.
I0302 19:01:58.417154 22895071117440 run.py:483] Algo bellman_ford step 9615 current loss 0.339047, current_train_items 307712.
I0302 19:01:58.432677 22895071117440 run.py:483] Algo bellman_ford step 9616 current loss 0.517010, current_train_items 307744.
I0302 19:01:58.455032 22895071117440 run.py:483] Algo bellman_ford step 9617 current loss 0.585093, current_train_items 307776.
I0302 19:01:58.483906 22895071117440 run.py:483] Algo bellman_ford step 9618 current loss 0.581918, current_train_items 307808.
I0302 19:01:58.515302 22895071117440 run.py:483] Algo bellman_ford step 9619 current loss 0.720480, current_train_items 307840.
I0302 19:01:58.533360 22895071117440 run.py:483] Algo bellman_ford step 9620 current loss 0.297485, current_train_items 307872.
I0302 19:01:58.548542 22895071117440 run.py:483] Algo bellman_ford step 9621 current loss 0.377100, current_train_items 307904.
I0302 19:01:58.571093 22895071117440 run.py:483] Algo bellman_ford step 9622 current loss 0.620087, current_train_items 307936.
I0302 19:01:58.600341 22895071117440 run.py:483] Algo bellman_ford step 9623 current loss 0.553023, current_train_items 307968.
I0302 19:01:58.632000 22895071117440 run.py:483] Algo bellman_ford step 9624 current loss 0.708609, current_train_items 308000.
I0302 19:01:58.650363 22895071117440 run.py:483] Algo bellman_ford step 9625 current loss 0.317087, current_train_items 308032.
I0302 19:01:58.665687 22895071117440 run.py:483] Algo bellman_ford step 9626 current loss 0.468044, current_train_items 308064.
I0302 19:01:58.687747 22895071117440 run.py:483] Algo bellman_ford step 9627 current loss 0.492349, current_train_items 308096.
I0302 19:01:58.716398 22895071117440 run.py:483] Algo bellman_ford step 9628 current loss 0.586419, current_train_items 308128.
I0302 19:01:58.748633 22895071117440 run.py:483] Algo bellman_ford step 9629 current loss 0.765710, current_train_items 308160.
I0302 19:01:58.766574 22895071117440 run.py:483] Algo bellman_ford step 9630 current loss 0.288629, current_train_items 308192.
I0302 19:01:58.782370 22895071117440 run.py:483] Algo bellman_ford step 9631 current loss 0.411012, current_train_items 308224.
I0302 19:01:58.805788 22895071117440 run.py:483] Algo bellman_ford step 9632 current loss 0.618346, current_train_items 308256.
I0302 19:01:58.833915 22895071117440 run.py:483] Algo bellman_ford step 9633 current loss 0.622887, current_train_items 308288.
I0302 19:01:58.865710 22895071117440 run.py:483] Algo bellman_ford step 9634 current loss 0.892278, current_train_items 308320.
I0302 19:01:58.883935 22895071117440 run.py:483] Algo bellman_ford step 9635 current loss 0.400968, current_train_items 308352.
I0302 19:01:58.899243 22895071117440 run.py:483] Algo bellman_ford step 9636 current loss 0.429068, current_train_items 308384.
I0302 19:01:58.921667 22895071117440 run.py:483] Algo bellman_ford step 9637 current loss 0.642244, current_train_items 308416.
I0302 19:01:58.951502 22895071117440 run.py:483] Algo bellman_ford step 9638 current loss 0.696203, current_train_items 308448.
I0302 19:01:58.982632 22895071117440 run.py:483] Algo bellman_ford step 9639 current loss 0.785320, current_train_items 308480.
I0302 19:01:59.001067 22895071117440 run.py:483] Algo bellman_ford step 9640 current loss 0.227193, current_train_items 308512.
I0302 19:01:59.016165 22895071117440 run.py:483] Algo bellman_ford step 9641 current loss 0.406105, current_train_items 308544.
I0302 19:01:59.038752 22895071117440 run.py:483] Algo bellman_ford step 9642 current loss 0.559062, current_train_items 308576.
I0302 19:01:59.069264 22895071117440 run.py:483] Algo bellman_ford step 9643 current loss 0.641842, current_train_items 308608.
I0302 19:01:59.098538 22895071117440 run.py:483] Algo bellman_ford step 9644 current loss 1.203346, current_train_items 308640.
I0302 19:01:59.116415 22895071117440 run.py:483] Algo bellman_ford step 9645 current loss 0.260809, current_train_items 308672.
I0302 19:01:59.131902 22895071117440 run.py:483] Algo bellman_ford step 9646 current loss 0.523567, current_train_items 308704.
I0302 19:01:59.153402 22895071117440 run.py:483] Algo bellman_ford step 9647 current loss 0.576517, current_train_items 308736.
I0302 19:01:59.183256 22895071117440 run.py:483] Algo bellman_ford step 9648 current loss 0.607236, current_train_items 308768.
I0302 19:01:59.214924 22895071117440 run.py:483] Algo bellman_ford step 9649 current loss 0.718875, current_train_items 308800.
I0302 19:01:59.232730 22895071117440 run.py:483] Algo bellman_ford step 9650 current loss 0.403515, current_train_items 308832.
I0302 19:01:59.240582 22895071117440 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:01:59.240711 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:59.256872 22895071117440 run.py:483] Algo bellman_ford step 9651 current loss 0.407067, current_train_items 308864.
I0302 19:01:59.281116 22895071117440 run.py:483] Algo bellman_ford step 9652 current loss 0.719929, current_train_items 308896.
I0302 19:01:59.310780 22895071117440 run.py:483] Algo bellman_ford step 9653 current loss 0.743704, current_train_items 308928.
I0302 19:01:59.343711 22895071117440 run.py:483] Algo bellman_ford step 9654 current loss 0.809066, current_train_items 308960.
I0302 19:01:59.362360 22895071117440 run.py:483] Algo bellman_ford step 9655 current loss 0.324369, current_train_items 308992.
I0302 19:01:59.378012 22895071117440 run.py:483] Algo bellman_ford step 9656 current loss 0.499980, current_train_items 309024.
I0302 19:01:59.400300 22895071117440 run.py:483] Algo bellman_ford step 9657 current loss 0.748439, current_train_items 309056.
I0302 19:01:59.429982 22895071117440 run.py:483] Algo bellman_ford step 9658 current loss 0.712042, current_train_items 309088.
I0302 19:01:59.459820 22895071117440 run.py:483] Algo bellman_ford step 9659 current loss 0.755441, current_train_items 309120.
I0302 19:01:59.478310 22895071117440 run.py:483] Algo bellman_ford step 9660 current loss 0.270594, current_train_items 309152.
I0302 19:01:59.494416 22895071117440 run.py:483] Algo bellman_ford step 9661 current loss 0.452163, current_train_items 309184.
I0302 19:01:59.516467 22895071117440 run.py:483] Algo bellman_ford step 9662 current loss 0.589393, current_train_items 309216.
I0302 19:01:59.545227 22895071117440 run.py:483] Algo bellman_ford step 9663 current loss 0.629532, current_train_items 309248.
I0302 19:01:59.576665 22895071117440 run.py:483] Algo bellman_ford step 9664 current loss 0.850291, current_train_items 309280.
I0302 19:01:59.594941 22895071117440 run.py:483] Algo bellman_ford step 9665 current loss 0.314602, current_train_items 309312.
I0302 19:01:59.610360 22895071117440 run.py:483] Algo bellman_ford step 9666 current loss 0.426894, current_train_items 309344.
I0302 19:01:59.631509 22895071117440 run.py:483] Algo bellman_ford step 9667 current loss 0.604594, current_train_items 309376.
I0302 19:01:59.660131 22895071117440 run.py:483] Algo bellman_ford step 9668 current loss 0.612417, current_train_items 309408.
I0302 19:01:59.691143 22895071117440 run.py:483] Algo bellman_ford step 9669 current loss 0.676808, current_train_items 309440.
I0302 19:01:59.709844 22895071117440 run.py:483] Algo bellman_ford step 9670 current loss 0.285194, current_train_items 309472.
I0302 19:01:59.725620 22895071117440 run.py:483] Algo bellman_ford step 9671 current loss 0.405193, current_train_items 309504.
I0302 19:01:59.747918 22895071117440 run.py:483] Algo bellman_ford step 9672 current loss 0.559284, current_train_items 309536.
I0302 19:01:59.776237 22895071117440 run.py:483] Algo bellman_ford step 9673 current loss 0.530870, current_train_items 309568.
I0302 19:01:59.807159 22895071117440 run.py:483] Algo bellman_ford step 9674 current loss 0.690660, current_train_items 309600.
I0302 19:01:59.825325 22895071117440 run.py:483] Algo bellman_ford step 9675 current loss 0.239160, current_train_items 309632.
I0302 19:01:59.841319 22895071117440 run.py:483] Algo bellman_ford step 9676 current loss 0.462134, current_train_items 309664.
I0302 19:01:59.863462 22895071117440 run.py:483] Algo bellman_ford step 9677 current loss 0.492513, current_train_items 309696.
I0302 19:01:59.891174 22895071117440 run.py:483] Algo bellman_ford step 9678 current loss 0.574474, current_train_items 309728.
I0302 19:01:59.922841 22895071117440 run.py:483] Algo bellman_ford step 9679 current loss 0.720788, current_train_items 309760.
I0302 19:01:59.941045 22895071117440 run.py:483] Algo bellman_ford step 9680 current loss 0.260522, current_train_items 309792.
I0302 19:01:59.956604 22895071117440 run.py:483] Algo bellman_ford step 9681 current loss 0.417158, current_train_items 309824.
I0302 19:01:59.978752 22895071117440 run.py:483] Algo bellman_ford step 9682 current loss 0.471539, current_train_items 309856.
I0302 19:02:00.006917 22895071117440 run.py:483] Algo bellman_ford step 9683 current loss 0.571174, current_train_items 309888.
I0302 19:02:00.036958 22895071117440 run.py:483] Algo bellman_ford step 9684 current loss 0.603350, current_train_items 309920.
I0302 19:02:00.055511 22895071117440 run.py:483] Algo bellman_ford step 9685 current loss 0.280302, current_train_items 309952.
I0302 19:02:00.071183 22895071117440 run.py:483] Algo bellman_ford step 9686 current loss 0.533487, current_train_items 309984.
I0302 19:02:00.093323 22895071117440 run.py:483] Algo bellman_ford step 9687 current loss 0.769919, current_train_items 310016.
I0302 19:02:00.121655 22895071117440 run.py:483] Algo bellman_ford step 9688 current loss 0.825546, current_train_items 310048.
I0302 19:02:00.153110 22895071117440 run.py:483] Algo bellman_ford step 9689 current loss 0.789056, current_train_items 310080.
I0302 19:02:00.171806 22895071117440 run.py:483] Algo bellman_ford step 9690 current loss 0.269641, current_train_items 310112.
I0302 19:02:00.187227 22895071117440 run.py:483] Algo bellman_ford step 9691 current loss 0.401911, current_train_items 310144.
I0302 19:02:00.209217 22895071117440 run.py:483] Algo bellman_ford step 9692 current loss 0.517358, current_train_items 310176.
I0302 19:02:00.237551 22895071117440 run.py:483] Algo bellman_ford step 9693 current loss 0.671161, current_train_items 310208.
I0302 19:02:00.268698 22895071117440 run.py:483] Algo bellman_ford step 9694 current loss 0.715525, current_train_items 310240.
I0302 19:02:00.286358 22895071117440 run.py:483] Algo bellman_ford step 9695 current loss 0.360418, current_train_items 310272.
I0302 19:02:00.301680 22895071117440 run.py:483] Algo bellman_ford step 9696 current loss 0.563460, current_train_items 310304.
I0302 19:02:00.323149 22895071117440 run.py:483] Algo bellman_ford step 9697 current loss 0.497596, current_train_items 310336.
I0302 19:02:00.354282 22895071117440 run.py:483] Algo bellman_ford step 9698 current loss 0.773955, current_train_items 310368.
I0302 19:02:00.386946 22895071117440 run.py:483] Algo bellman_ford step 9699 current loss 0.774924, current_train_items 310400.
I0302 19:02:00.405234 22895071117440 run.py:483] Algo bellman_ford step 9700 current loss 0.293563, current_train_items 310432.
I0302 19:02:00.413014 22895071117440 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:00.413120 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:00.429074 22895071117440 run.py:483] Algo bellman_ford step 9701 current loss 0.502201, current_train_items 310464.
I0302 19:02:00.452050 22895071117440 run.py:483] Algo bellman_ford step 9702 current loss 0.586673, current_train_items 310496.
I0302 19:02:00.481981 22895071117440 run.py:483] Algo bellman_ford step 9703 current loss 0.658475, current_train_items 310528.
I0302 19:02:00.513293 22895071117440 run.py:483] Algo bellman_ford step 9704 current loss 0.719997, current_train_items 310560.
I0302 19:02:00.532047 22895071117440 run.py:483] Algo bellman_ford step 9705 current loss 0.243743, current_train_items 310592.
I0302 19:02:00.547439 22895071117440 run.py:483] Algo bellman_ford step 9706 current loss 0.443975, current_train_items 310624.
I0302 19:02:00.570663 22895071117440 run.py:483] Algo bellman_ford step 9707 current loss 0.636489, current_train_items 310656.
I0302 19:02:00.600791 22895071117440 run.py:483] Algo bellman_ford step 9708 current loss 0.623089, current_train_items 310688.
I0302 19:02:00.630078 22895071117440 run.py:483] Algo bellman_ford step 9709 current loss 0.665189, current_train_items 310720.
I0302 19:02:00.648190 22895071117440 run.py:483] Algo bellman_ford step 9710 current loss 0.262098, current_train_items 310752.
I0302 19:02:00.663589 22895071117440 run.py:483] Algo bellman_ford step 9711 current loss 0.463810, current_train_items 310784.
I0302 19:02:00.685839 22895071117440 run.py:483] Algo bellman_ford step 9712 current loss 0.623200, current_train_items 310816.
I0302 19:02:00.715532 22895071117440 run.py:483] Algo bellman_ford step 9713 current loss 0.504759, current_train_items 310848.
I0302 19:02:00.747153 22895071117440 run.py:483] Algo bellman_ford step 9714 current loss 0.701215, current_train_items 310880.
I0302 19:02:00.765082 22895071117440 run.py:483] Algo bellman_ford step 9715 current loss 0.287968, current_train_items 310912.
I0302 19:02:00.780859 22895071117440 run.py:483] Algo bellman_ford step 9716 current loss 0.435079, current_train_items 310944.
I0302 19:02:00.801678 22895071117440 run.py:483] Algo bellman_ford step 9717 current loss 0.604787, current_train_items 310976.
I0302 19:02:00.831246 22895071117440 run.py:483] Algo bellman_ford step 9718 current loss 0.633799, current_train_items 311008.
I0302 19:02:00.859908 22895071117440 run.py:483] Algo bellman_ford step 9719 current loss 0.494246, current_train_items 311040.
I0302 19:02:00.877819 22895071117440 run.py:483] Algo bellman_ford step 9720 current loss 0.280104, current_train_items 311072.
I0302 19:02:00.892787 22895071117440 run.py:483] Algo bellman_ford step 9721 current loss 0.370961, current_train_items 311104.
I0302 19:02:00.914285 22895071117440 run.py:483] Algo bellman_ford step 9722 current loss 0.608714, current_train_items 311136.
I0302 19:02:00.943696 22895071117440 run.py:483] Algo bellman_ford step 9723 current loss 0.655124, current_train_items 311168.
I0302 19:02:00.974857 22895071117440 run.py:483] Algo bellman_ford step 9724 current loss 0.603580, current_train_items 311200.
I0302 19:02:00.992983 22895071117440 run.py:483] Algo bellman_ford step 9725 current loss 0.293342, current_train_items 311232.
I0302 19:02:01.008142 22895071117440 run.py:483] Algo bellman_ford step 9726 current loss 0.454043, current_train_items 311264.
I0302 19:02:01.029485 22895071117440 run.py:483] Algo bellman_ford step 9727 current loss 0.480659, current_train_items 311296.
I0302 19:02:01.059155 22895071117440 run.py:483] Algo bellman_ford step 9728 current loss 0.605080, current_train_items 311328.
I0302 19:02:01.090367 22895071117440 run.py:483] Algo bellman_ford step 9729 current loss 0.746279, current_train_items 311360.
I0302 19:02:01.108543 22895071117440 run.py:483] Algo bellman_ford step 9730 current loss 0.334536, current_train_items 311392.
I0302 19:02:01.124239 22895071117440 run.py:483] Algo bellman_ford step 9731 current loss 0.456013, current_train_items 311424.
I0302 19:02:01.145997 22895071117440 run.py:483] Algo bellman_ford step 9732 current loss 0.607197, current_train_items 311456.
I0302 19:02:01.174923 22895071117440 run.py:483] Algo bellman_ford step 9733 current loss 0.580807, current_train_items 311488.
I0302 19:02:01.205108 22895071117440 run.py:483] Algo bellman_ford step 9734 current loss 0.624681, current_train_items 311520.
I0302 19:02:01.223345 22895071117440 run.py:483] Algo bellman_ford step 9735 current loss 0.267267, current_train_items 311552.
I0302 19:02:01.238715 22895071117440 run.py:483] Algo bellman_ford step 9736 current loss 0.510596, current_train_items 311584.
I0302 19:02:01.260426 22895071117440 run.py:483] Algo bellman_ford step 9737 current loss 0.604221, current_train_items 311616.
I0302 19:02:01.291065 22895071117440 run.py:483] Algo bellman_ford step 9738 current loss 0.629366, current_train_items 311648.
I0302 19:02:01.323094 22895071117440 run.py:483] Algo bellman_ford step 9739 current loss 0.669241, current_train_items 311680.
I0302 19:02:01.341348 22895071117440 run.py:483] Algo bellman_ford step 9740 current loss 0.274270, current_train_items 311712.
I0302 19:02:01.357455 22895071117440 run.py:483] Algo bellman_ford step 9741 current loss 0.423669, current_train_items 311744.
I0302 19:02:01.381192 22895071117440 run.py:483] Algo bellman_ford step 9742 current loss 0.593408, current_train_items 311776.
I0302 19:02:01.410955 22895071117440 run.py:483] Algo bellman_ford step 9743 current loss 0.626934, current_train_items 311808.
I0302 19:02:01.442237 22895071117440 run.py:483] Algo bellman_ford step 9744 current loss 0.682689, current_train_items 311840.
I0302 19:02:01.460735 22895071117440 run.py:483] Algo bellman_ford step 9745 current loss 0.236222, current_train_items 311872.
I0302 19:02:01.476754 22895071117440 run.py:483] Algo bellman_ford step 9746 current loss 0.398613, current_train_items 311904.
I0302 19:02:01.498673 22895071117440 run.py:483] Algo bellman_ford step 9747 current loss 0.520243, current_train_items 311936.
I0302 19:02:01.525803 22895071117440 run.py:483] Algo bellman_ford step 9748 current loss 0.523336, current_train_items 311968.
I0302 19:02:01.555627 22895071117440 run.py:483] Algo bellman_ford step 9749 current loss 0.670224, current_train_items 312000.
I0302 19:02:01.573767 22895071117440 run.py:483] Algo bellman_ford step 9750 current loss 0.313171, current_train_items 312032.
I0302 19:02:01.581842 22895071117440 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:01.581959 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:01.597764 22895071117440 run.py:483] Algo bellman_ford step 9751 current loss 0.478782, current_train_items 312064.
I0302 19:02:01.621455 22895071117440 run.py:483] Algo bellman_ford step 9752 current loss 0.625776, current_train_items 312096.
I0302 19:02:01.650050 22895071117440 run.py:483] Algo bellman_ford step 9753 current loss 0.716830, current_train_items 312128.
I0302 19:02:01.680896 22895071117440 run.py:483] Algo bellman_ford step 9754 current loss 0.597527, current_train_items 312160.
I0302 19:02:01.699327 22895071117440 run.py:483] Algo bellman_ford step 9755 current loss 0.313243, current_train_items 312192.
I0302 19:02:01.715365 22895071117440 run.py:483] Algo bellman_ford step 9756 current loss 0.490647, current_train_items 312224.
I0302 19:02:01.736226 22895071117440 run.py:483] Algo bellman_ford step 9757 current loss 0.545530, current_train_items 312256.
I0302 19:02:01.766028 22895071117440 run.py:483] Algo bellman_ford step 9758 current loss 0.699170, current_train_items 312288.
I0302 19:02:01.796879 22895071117440 run.py:483] Algo bellman_ford step 9759 current loss 0.709979, current_train_items 312320.
I0302 19:02:01.815609 22895071117440 run.py:483] Algo bellman_ford step 9760 current loss 0.269717, current_train_items 312352.
I0302 19:02:01.831786 22895071117440 run.py:483] Algo bellman_ford step 9761 current loss 0.467666, current_train_items 312384.
I0302 19:02:01.854257 22895071117440 run.py:483] Algo bellman_ford step 9762 current loss 0.633770, current_train_items 312416.
I0302 19:02:01.884094 22895071117440 run.py:483] Algo bellman_ford step 9763 current loss 0.649457, current_train_items 312448.
I0302 19:02:01.912352 22895071117440 run.py:483] Algo bellman_ford step 9764 current loss 0.646902, current_train_items 312480.
I0302 19:02:01.930559 22895071117440 run.py:483] Algo bellman_ford step 9765 current loss 0.287430, current_train_items 312512.
I0302 19:02:01.945549 22895071117440 run.py:483] Algo bellman_ford step 9766 current loss 0.436930, current_train_items 312544.
I0302 19:02:01.968122 22895071117440 run.py:483] Algo bellman_ford step 9767 current loss 0.675725, current_train_items 312576.
I0302 19:02:01.996591 22895071117440 run.py:483] Algo bellman_ford step 9768 current loss 0.654855, current_train_items 312608.
I0302 19:02:02.027946 22895071117440 run.py:483] Algo bellman_ford step 9769 current loss 0.760754, current_train_items 312640.
I0302 19:02:02.046515 22895071117440 run.py:483] Algo bellman_ford step 9770 current loss 0.263404, current_train_items 312672.
I0302 19:02:02.062090 22895071117440 run.py:483] Algo bellman_ford step 9771 current loss 0.490069, current_train_items 312704.
I0302 19:02:02.083447 22895071117440 run.py:483] Algo bellman_ford step 9772 current loss 0.615839, current_train_items 312736.
I0302 19:02:02.113298 22895071117440 run.py:483] Algo bellman_ford step 9773 current loss 0.729793, current_train_items 312768.
I0302 19:02:02.144403 22895071117440 run.py:483] Algo bellman_ford step 9774 current loss 0.835596, current_train_items 312800.
I0302 19:02:02.162714 22895071117440 run.py:483] Algo bellman_ford step 9775 current loss 0.331027, current_train_items 312832.
I0302 19:02:02.178150 22895071117440 run.py:483] Algo bellman_ford step 9776 current loss 0.415417, current_train_items 312864.
I0302 19:02:02.200985 22895071117440 run.py:483] Algo bellman_ford step 9777 current loss 0.596408, current_train_items 312896.
I0302 19:02:02.230584 22895071117440 run.py:483] Algo bellman_ford step 9778 current loss 0.743056, current_train_items 312928.
I0302 19:02:02.261704 22895071117440 run.py:483] Algo bellman_ford step 9779 current loss 0.731796, current_train_items 312960.
I0302 19:02:02.279623 22895071117440 run.py:483] Algo bellman_ford step 9780 current loss 0.301734, current_train_items 312992.
I0302 19:02:02.295166 22895071117440 run.py:483] Algo bellman_ford step 9781 current loss 0.454828, current_train_items 313024.
I0302 19:02:02.316711 22895071117440 run.py:483] Algo bellman_ford step 9782 current loss 0.536044, current_train_items 313056.
I0302 19:02:02.347491 22895071117440 run.py:483] Algo bellman_ford step 9783 current loss 0.605940, current_train_items 313088.
I0302 19:02:02.376979 22895071117440 run.py:483] Algo bellman_ford step 9784 current loss 0.606658, current_train_items 313120.
I0302 19:02:02.395872 22895071117440 run.py:483] Algo bellman_ford step 9785 current loss 0.280441, current_train_items 313152.
I0302 19:02:02.411918 22895071117440 run.py:483] Algo bellman_ford step 9786 current loss 0.490479, current_train_items 313184.
I0302 19:02:02.433462 22895071117440 run.py:483] Algo bellman_ford step 9787 current loss 0.552526, current_train_items 313216.
I0302 19:02:02.461956 22895071117440 run.py:483] Algo bellman_ford step 9788 current loss 0.591277, current_train_items 313248.
I0302 19:02:02.492759 22895071117440 run.py:483] Algo bellman_ford step 9789 current loss 0.736536, current_train_items 313280.
I0302 19:02:02.511175 22895071117440 run.py:483] Algo bellman_ford step 9790 current loss 0.322036, current_train_items 313312.
I0302 19:02:02.526769 22895071117440 run.py:483] Algo bellman_ford step 9791 current loss 0.411234, current_train_items 313344.
I0302 19:02:02.548275 22895071117440 run.py:483] Algo bellman_ford step 9792 current loss 0.566981, current_train_items 313376.
I0302 19:02:02.576797 22895071117440 run.py:483] Algo bellman_ford step 9793 current loss 0.618588, current_train_items 313408.
I0302 19:02:02.605848 22895071117440 run.py:483] Algo bellman_ford step 9794 current loss 0.653228, current_train_items 313440.
I0302 19:02:02.624152 22895071117440 run.py:483] Algo bellman_ford step 9795 current loss 0.336438, current_train_items 313472.
I0302 19:02:02.639806 22895071117440 run.py:483] Algo bellman_ford step 9796 current loss 0.414405, current_train_items 313504.
I0302 19:02:02.661954 22895071117440 run.py:483] Algo bellman_ford step 9797 current loss 0.551773, current_train_items 313536.
I0302 19:02:02.690464 22895071117440 run.py:483] Algo bellman_ford step 9798 current loss 0.610615, current_train_items 313568.
I0302 19:02:02.722754 22895071117440 run.py:483] Algo bellman_ford step 9799 current loss 0.662070, current_train_items 313600.
I0302 19:02:02.741435 22895071117440 run.py:483] Algo bellman_ford step 9800 current loss 0.366134, current_train_items 313632.
I0302 19:02:02.748914 22895071117440 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:02.749019 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:02.764722 22895071117440 run.py:483] Algo bellman_ford step 9801 current loss 0.437333, current_train_items 313664.
I0302 19:02:02.787560 22895071117440 run.py:483] Algo bellman_ford step 9802 current loss 0.586543, current_train_items 313696.
I0302 19:02:02.818189 22895071117440 run.py:483] Algo bellman_ford step 9803 current loss 0.753300, current_train_items 313728.
I0302 19:02:02.848646 22895071117440 run.py:483] Algo bellman_ford step 9804 current loss 0.628595, current_train_items 313760.
I0302 19:02:02.867404 22895071117440 run.py:483] Algo bellman_ford step 9805 current loss 0.327082, current_train_items 313792.
I0302 19:02:02.882958 22895071117440 run.py:483] Algo bellman_ford step 9806 current loss 0.493686, current_train_items 313824.
I0302 19:02:02.904263 22895071117440 run.py:483] Algo bellman_ford step 9807 current loss 0.534681, current_train_items 313856.
I0302 19:02:02.933288 22895071117440 run.py:483] Algo bellman_ford step 9808 current loss 0.581581, current_train_items 313888.
I0302 19:02:02.966398 22895071117440 run.py:483] Algo bellman_ford step 9809 current loss 0.809272, current_train_items 313920.
I0302 19:02:02.984725 22895071117440 run.py:483] Algo bellman_ford step 9810 current loss 0.232953, current_train_items 313952.
I0302 19:02:02.999775 22895071117440 run.py:483] Algo bellman_ford step 9811 current loss 0.409924, current_train_items 313984.
I0302 19:02:03.022727 22895071117440 run.py:483] Algo bellman_ford step 9812 current loss 0.621309, current_train_items 314016.
I0302 19:02:03.049740 22895071117440 run.py:483] Algo bellman_ford step 9813 current loss 0.529679, current_train_items 314048.
I0302 19:02:03.081105 22895071117440 run.py:483] Algo bellman_ford step 9814 current loss 0.623457, current_train_items 314080.
I0302 19:02:03.099040 22895071117440 run.py:483] Algo bellman_ford step 9815 current loss 0.285913, current_train_items 314112.
I0302 19:02:03.114713 22895071117440 run.py:483] Algo bellman_ford step 9816 current loss 0.434217, current_train_items 314144.
I0302 19:02:03.136517 22895071117440 run.py:483] Algo bellman_ford step 9817 current loss 0.527029, current_train_items 314176.
I0302 19:02:03.166926 22895071117440 run.py:483] Algo bellman_ford step 9818 current loss 0.636062, current_train_items 314208.
I0302 19:02:03.197424 22895071117440 run.py:483] Algo bellman_ford step 9819 current loss 0.724378, current_train_items 314240.
I0302 19:02:03.215250 22895071117440 run.py:483] Algo bellman_ford step 9820 current loss 0.312151, current_train_items 314272.
I0302 19:02:03.230498 22895071117440 run.py:483] Algo bellman_ford step 9821 current loss 0.520893, current_train_items 314304.
I0302 19:02:03.252759 22895071117440 run.py:483] Algo bellman_ford step 9822 current loss 0.559069, current_train_items 314336.
I0302 19:02:03.281481 22895071117440 run.py:483] Algo bellman_ford step 9823 current loss 0.509475, current_train_items 314368.
I0302 19:02:03.312000 22895071117440 run.py:483] Algo bellman_ford step 9824 current loss 0.703771, current_train_items 314400.
I0302 19:02:03.329964 22895071117440 run.py:483] Algo bellman_ford step 9825 current loss 0.286671, current_train_items 314432.
I0302 19:02:03.345025 22895071117440 run.py:483] Algo bellman_ford step 9826 current loss 0.442235, current_train_items 314464.
I0302 19:02:03.367130 22895071117440 run.py:483] Algo bellman_ford step 9827 current loss 0.576655, current_train_items 314496.
I0302 19:02:03.395449 22895071117440 run.py:483] Algo bellman_ford step 9828 current loss 0.557540, current_train_items 314528.
I0302 19:02:03.427341 22895071117440 run.py:483] Algo bellman_ford step 9829 current loss 0.769503, current_train_items 314560.
I0302 19:02:03.445242 22895071117440 run.py:483] Algo bellman_ford step 9830 current loss 0.247377, current_train_items 314592.
I0302 19:02:03.461154 22895071117440 run.py:483] Algo bellman_ford step 9831 current loss 0.434753, current_train_items 314624.
I0302 19:02:03.483407 22895071117440 run.py:483] Algo bellman_ford step 9832 current loss 0.557799, current_train_items 314656.
I0302 19:02:03.513125 22895071117440 run.py:483] Algo bellman_ford step 9833 current loss 0.680135, current_train_items 314688.
I0302 19:02:03.542683 22895071117440 run.py:483] Algo bellman_ford step 9834 current loss 0.687713, current_train_items 314720.
I0302 19:02:03.560693 22895071117440 run.py:483] Algo bellman_ford step 9835 current loss 0.254910, current_train_items 314752.
I0302 19:02:03.576147 22895071117440 run.py:483] Algo bellman_ford step 9836 current loss 0.477740, current_train_items 314784.
I0302 19:02:03.597989 22895071117440 run.py:483] Algo bellman_ford step 9837 current loss 0.688152, current_train_items 314816.
I0302 19:02:03.627867 22895071117440 run.py:483] Algo bellman_ford step 9838 current loss 0.715519, current_train_items 314848.
I0302 19:02:03.660244 22895071117440 run.py:483] Algo bellman_ford step 9839 current loss 0.750016, current_train_items 314880.
I0302 19:02:03.678118 22895071117440 run.py:483] Algo bellman_ford step 9840 current loss 0.255450, current_train_items 314912.
I0302 19:02:03.693473 22895071117440 run.py:483] Algo bellman_ford step 9841 current loss 0.419603, current_train_items 314944.
I0302 19:02:03.715831 22895071117440 run.py:483] Algo bellman_ford step 9842 current loss 0.531424, current_train_items 314976.
I0302 19:02:03.744345 22895071117440 run.py:483] Algo bellman_ford step 9843 current loss 0.644134, current_train_items 315008.
I0302 19:02:03.772856 22895071117440 run.py:483] Algo bellman_ford step 9844 current loss 0.665815, current_train_items 315040.
I0302 19:02:03.791146 22895071117440 run.py:483] Algo bellman_ford step 9845 current loss 0.293572, current_train_items 315072.
I0302 19:02:03.806685 22895071117440 run.py:483] Algo bellman_ford step 9846 current loss 0.419289, current_train_items 315104.
I0302 19:02:03.829200 22895071117440 run.py:483] Algo bellman_ford step 9847 current loss 0.671509, current_train_items 315136.
I0302 19:02:03.857848 22895071117440 run.py:483] Algo bellman_ford step 9848 current loss 0.696516, current_train_items 315168.
I0302 19:02:03.889106 22895071117440 run.py:483] Algo bellman_ford step 9849 current loss 0.702868, current_train_items 315200.
I0302 19:02:03.907469 22895071117440 run.py:483] Algo bellman_ford step 9850 current loss 0.257835, current_train_items 315232.
I0302 19:02:03.915335 22895071117440 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:03.915440 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:02:03.932217 22895071117440 run.py:483] Algo bellman_ford step 9851 current loss 0.434347, current_train_items 315264.
I0302 19:02:03.956092 22895071117440 run.py:483] Algo bellman_ford step 9852 current loss 0.762586, current_train_items 315296.
I0302 19:02:03.984870 22895071117440 run.py:483] Algo bellman_ford step 9853 current loss 0.710559, current_train_items 315328.
I0302 19:02:04.016815 22895071117440 run.py:483] Algo bellman_ford step 9854 current loss 1.006855, current_train_items 315360.
I0302 19:02:04.035348 22895071117440 run.py:483] Algo bellman_ford step 9855 current loss 0.326609, current_train_items 315392.
I0302 19:02:04.050807 22895071117440 run.py:483] Algo bellman_ford step 9856 current loss 0.513227, current_train_items 315424.
I0302 19:02:04.073152 22895071117440 run.py:483] Algo bellman_ford step 9857 current loss 0.624387, current_train_items 315456.
I0302 19:02:04.101411 22895071117440 run.py:483] Algo bellman_ford step 9858 current loss 0.594457, current_train_items 315488.
I0302 19:02:04.132488 22895071117440 run.py:483] Algo bellman_ford step 9859 current loss 0.691717, current_train_items 315520.
I0302 19:02:04.151449 22895071117440 run.py:483] Algo bellman_ford step 9860 current loss 0.328957, current_train_items 315552.
I0302 19:02:04.167140 22895071117440 run.py:483] Algo bellman_ford step 9861 current loss 0.437745, current_train_items 315584.
I0302 19:02:04.189445 22895071117440 run.py:483] Algo bellman_ford step 9862 current loss 0.628986, current_train_items 315616.
I0302 19:02:04.218340 22895071117440 run.py:483] Algo bellman_ford step 9863 current loss 0.615448, current_train_items 315648.
I0302 19:02:04.248912 22895071117440 run.py:483] Algo bellman_ford step 9864 current loss 0.856836, current_train_items 315680.
I0302 19:02:04.267189 22895071117440 run.py:483] Algo bellman_ford step 9865 current loss 0.282722, current_train_items 315712.
I0302 19:02:04.282470 22895071117440 run.py:483] Algo bellman_ford step 9866 current loss 0.526278, current_train_items 315744.
I0302 19:02:04.304265 22895071117440 run.py:483] Algo bellman_ford step 9867 current loss 0.690795, current_train_items 315776.
I0302 19:02:04.333889 22895071117440 run.py:483] Algo bellman_ford step 9868 current loss 0.669059, current_train_items 315808.
I0302 19:02:04.364827 22895071117440 run.py:483] Algo bellman_ford step 9869 current loss 0.724928, current_train_items 315840.
I0302 19:02:04.383790 22895071117440 run.py:483] Algo bellman_ford step 9870 current loss 0.310050, current_train_items 315872.
I0302 19:02:04.400104 22895071117440 run.py:483] Algo bellman_ford step 9871 current loss 0.445598, current_train_items 315904.
I0302 19:02:04.422366 22895071117440 run.py:483] Algo bellman_ford step 9872 current loss 0.584390, current_train_items 315936.
I0302 19:02:04.451843 22895071117440 run.py:483] Algo bellman_ford step 9873 current loss 0.572389, current_train_items 315968.
I0302 19:02:04.482065 22895071117440 run.py:483] Algo bellman_ford step 9874 current loss 0.658801, current_train_items 316000.
I0302 19:02:04.500522 22895071117440 run.py:483] Algo bellman_ford step 9875 current loss 0.264513, current_train_items 316032.
I0302 19:02:04.516180 22895071117440 run.py:483] Algo bellman_ford step 9876 current loss 0.418206, current_train_items 316064.
I0302 19:02:04.536568 22895071117440 run.py:483] Algo bellman_ford step 9877 current loss 0.554450, current_train_items 316096.
I0302 19:02:04.565237 22895071117440 run.py:483] Algo bellman_ford step 9878 current loss 0.611463, current_train_items 316128.
I0302 19:02:04.596108 22895071117440 run.py:483] Algo bellman_ford step 9879 current loss 0.662055, current_train_items 316160.
I0302 19:02:04.613955 22895071117440 run.py:483] Algo bellman_ford step 9880 current loss 0.293812, current_train_items 316192.
I0302 19:02:04.629405 22895071117440 run.py:483] Algo bellman_ford step 9881 current loss 0.463878, current_train_items 316224.
I0302 19:02:04.651442 22895071117440 run.py:483] Algo bellman_ford step 9882 current loss 0.471669, current_train_items 316256.
I0302 19:02:04.680394 22895071117440 run.py:483] Algo bellman_ford step 9883 current loss 0.651143, current_train_items 316288.
I0302 19:02:04.711476 22895071117440 run.py:483] Algo bellman_ford step 9884 current loss 0.644612, current_train_items 316320.
I0302 19:02:04.730414 22895071117440 run.py:483] Algo bellman_ford step 9885 current loss 0.287620, current_train_items 316352.
I0302 19:02:04.746016 22895071117440 run.py:483] Algo bellman_ford step 9886 current loss 0.447210, current_train_items 316384.
I0302 19:02:04.768124 22895071117440 run.py:483] Algo bellman_ford step 9887 current loss 0.545929, current_train_items 316416.
I0302 19:02:04.796548 22895071117440 run.py:483] Algo bellman_ford step 9888 current loss 0.701604, current_train_items 316448.
I0302 19:02:04.826685 22895071117440 run.py:483] Algo bellman_ford step 9889 current loss 0.646898, current_train_items 316480.
I0302 19:02:04.845403 22895071117440 run.py:483] Algo bellman_ford step 9890 current loss 0.292078, current_train_items 316512.
I0302 19:02:04.860989 22895071117440 run.py:483] Algo bellman_ford step 9891 current loss 0.420766, current_train_items 316544.
I0302 19:02:04.882059 22895071117440 run.py:483] Algo bellman_ford step 9892 current loss 0.556464, current_train_items 316576.
I0302 19:02:04.911901 22895071117440 run.py:483] Algo bellman_ford step 9893 current loss 0.601711, current_train_items 316608.
I0302 19:02:04.944047 22895071117440 run.py:483] Algo bellman_ford step 9894 current loss 0.640561, current_train_items 316640.
I0302 19:02:04.962444 22895071117440 run.py:483] Algo bellman_ford step 9895 current loss 0.260297, current_train_items 316672.
I0302 19:02:04.977659 22895071117440 run.py:483] Algo bellman_ford step 9896 current loss 0.416814, current_train_items 316704.
I0302 19:02:05.000176 22895071117440 run.py:483] Algo bellman_ford step 9897 current loss 0.602502, current_train_items 316736.
I0302 19:02:05.029040 22895071117440 run.py:483] Algo bellman_ford step 9898 current loss 0.674030, current_train_items 316768.
I0302 19:02:05.062016 22895071117440 run.py:483] Algo bellman_ford step 9899 current loss 0.781916, current_train_items 316800.
I0302 19:02:05.080164 22895071117440 run.py:483] Algo bellman_ford step 9900 current loss 0.268027, current_train_items 316832.
I0302 19:02:05.087985 22895071117440 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:05.088091 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:02:05.104206 22895071117440 run.py:483] Algo bellman_ford step 9901 current loss 0.432582, current_train_items 316864.
I0302 19:02:05.126365 22895071117440 run.py:483] Algo bellman_ford step 9902 current loss 0.561227, current_train_items 316896.
I0302 19:02:05.155245 22895071117440 run.py:483] Algo bellman_ford step 9903 current loss 0.569852, current_train_items 316928.
I0302 19:02:05.187592 22895071117440 run.py:483] Algo bellman_ford step 9904 current loss 0.572534, current_train_items 316960.
I0302 19:02:05.205985 22895071117440 run.py:483] Algo bellman_ford step 9905 current loss 0.248209, current_train_items 316992.
I0302 19:02:05.221784 22895071117440 run.py:483] Algo bellman_ford step 9906 current loss 0.403851, current_train_items 317024.
I0302 19:02:05.244626 22895071117440 run.py:483] Algo bellman_ford step 9907 current loss 0.684231, current_train_items 317056.
I0302 19:02:05.273761 22895071117440 run.py:483] Algo bellman_ford step 9908 current loss 0.711983, current_train_items 317088.
I0302 19:02:05.303362 22895071117440 run.py:483] Algo bellman_ford step 9909 current loss 0.687730, current_train_items 317120.
I0302 19:02:05.321410 22895071117440 run.py:483] Algo bellman_ford step 9910 current loss 0.252390, current_train_items 317152.
I0302 19:02:05.337102 22895071117440 run.py:483] Algo bellman_ford step 9911 current loss 0.500060, current_train_items 317184.
I0302 19:02:05.359446 22895071117440 run.py:483] Algo bellman_ford step 9912 current loss 0.666893, current_train_items 317216.
I0302 19:02:05.388824 22895071117440 run.py:483] Algo bellman_ford step 9913 current loss 0.703576, current_train_items 317248.
I0302 19:02:05.419591 22895071117440 run.py:483] Algo bellman_ford step 9914 current loss 0.777189, current_train_items 317280.
I0302 19:02:05.437747 22895071117440 run.py:483] Algo bellman_ford step 9915 current loss 0.385782, current_train_items 317312.
I0302 19:02:05.453402 22895071117440 run.py:483] Algo bellman_ford step 9916 current loss 0.444485, current_train_items 317344.
I0302 19:02:05.475997 22895071117440 run.py:483] Algo bellman_ford step 9917 current loss 0.528371, current_train_items 317376.
I0302 19:02:05.505616 22895071117440 run.py:483] Algo bellman_ford step 9918 current loss 0.698167, current_train_items 317408.
I0302 19:02:05.535809 22895071117440 run.py:483] Algo bellman_ford step 9919 current loss 0.767542, current_train_items 317440.
I0302 19:02:05.553784 22895071117440 run.py:483] Algo bellman_ford step 9920 current loss 0.312623, current_train_items 317472.
I0302 19:02:05.569620 22895071117440 run.py:483] Algo bellman_ford step 9921 current loss 0.500688, current_train_items 317504.
I0302 19:02:05.591797 22895071117440 run.py:483] Algo bellman_ford step 9922 current loss 0.589190, current_train_items 317536.
I0302 19:02:05.619071 22895071117440 run.py:483] Algo bellman_ford step 9923 current loss 0.578745, current_train_items 317568.
I0302 19:02:05.652834 22895071117440 run.py:483] Algo bellman_ford step 9924 current loss 0.797513, current_train_items 317600.
I0302 19:02:05.670684 22895071117440 run.py:483] Algo bellman_ford step 9925 current loss 0.296943, current_train_items 317632.
I0302 19:02:05.685997 22895071117440 run.py:483] Algo bellman_ford step 9926 current loss 0.471028, current_train_items 317664.
I0302 19:02:05.708737 22895071117440 run.py:483] Algo bellman_ford step 9927 current loss 0.554567, current_train_items 317696.
I0302 19:02:05.739143 22895071117440 run.py:483] Algo bellman_ford step 9928 current loss 0.672727, current_train_items 317728.
I0302 19:02:05.767563 22895071117440 run.py:483] Algo bellman_ford step 9929 current loss 0.674366, current_train_items 317760.
I0302 19:02:05.785907 22895071117440 run.py:483] Algo bellman_ford step 9930 current loss 0.260259, current_train_items 317792.
I0302 19:02:05.800618 22895071117440 run.py:483] Algo bellman_ford step 9931 current loss 0.401717, current_train_items 317824.
I0302 19:02:05.823259 22895071117440 run.py:483] Algo bellman_ford step 9932 current loss 0.598598, current_train_items 317856.
I0302 19:02:05.851796 22895071117440 run.py:483] Algo bellman_ford step 9933 current loss 0.709514, current_train_items 317888.
I0302 19:02:05.881224 22895071117440 run.py:483] Algo bellman_ford step 9934 current loss 0.836656, current_train_items 317920.
I0302 19:02:05.899120 22895071117440 run.py:483] Algo bellman_ford step 9935 current loss 0.251167, current_train_items 317952.
I0302 19:02:05.914123 22895071117440 run.py:483] Algo bellman_ford step 9936 current loss 0.559064, current_train_items 317984.
I0302 19:02:05.936099 22895071117440 run.py:483] Algo bellman_ford step 9937 current loss 0.631916, current_train_items 318016.
I0302 19:02:05.964819 22895071117440 run.py:483] Algo bellman_ford step 9938 current loss 0.649282, current_train_items 318048.
I0302 19:02:05.996685 22895071117440 run.py:483] Algo bellman_ford step 9939 current loss 1.010757, current_train_items 318080.
I0302 19:02:06.014722 22895071117440 run.py:483] Algo bellman_ford step 9940 current loss 0.377450, current_train_items 318112.
I0302 19:02:06.029834 22895071117440 run.py:483] Algo bellman_ford step 9941 current loss 0.406483, current_train_items 318144.
I0302 19:02:06.051244 22895071117440 run.py:483] Algo bellman_ford step 9942 current loss 0.679055, current_train_items 318176.
I0302 19:02:06.080647 22895071117440 run.py:483] Algo bellman_ford step 9943 current loss 0.707414, current_train_items 318208.
I0302 19:02:06.110024 22895071117440 run.py:483] Algo bellman_ford step 9944 current loss 0.802297, current_train_items 318240.
I0302 19:02:06.128027 22895071117440 run.py:483] Algo bellman_ford step 9945 current loss 0.399436, current_train_items 318272.
I0302 19:02:06.143035 22895071117440 run.py:483] Algo bellman_ford step 9946 current loss 0.463560, current_train_items 318304.
I0302 19:02:06.165105 22895071117440 run.py:483] Algo bellman_ford step 9947 current loss 0.708442, current_train_items 318336.
I0302 19:02:06.193989 22895071117440 run.py:483] Algo bellman_ford step 9948 current loss 0.726652, current_train_items 318368.
I0302 19:02:06.225708 22895071117440 run.py:483] Algo bellman_ford step 9949 current loss 0.771289, current_train_items 318400.
I0302 19:02:06.243698 22895071117440 run.py:483] Algo bellman_ford step 9950 current loss 0.296655, current_train_items 318432.
I0302 19:02:06.251557 22895071117440 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:06.251662 22895071117440 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:02:06.268077 22895071117440 run.py:483] Algo bellman_ford step 9951 current loss 0.512697, current_train_items 318464.
I0302 19:02:06.290770 22895071117440 run.py:483] Algo bellman_ford step 9952 current loss 0.557496, current_train_items 318496.
I0302 19:02:06.321033 22895071117440 run.py:483] Algo bellman_ford step 9953 current loss 0.735255, current_train_items 318528.
I0302 19:02:06.353970 22895071117440 run.py:483] Algo bellman_ford step 9954 current loss 0.632730, current_train_items 318560.
I0302 19:02:06.372381 22895071117440 run.py:483] Algo bellman_ford step 9955 current loss 0.320847, current_train_items 318592.
I0302 19:02:06.388181 22895071117440 run.py:483] Algo bellman_ford step 9956 current loss 0.492773, current_train_items 318624.
I0302 19:02:06.410464 22895071117440 run.py:483] Algo bellman_ford step 9957 current loss 0.586018, current_train_items 318656.
I0302 19:02:06.439632 22895071117440 run.py:483] Algo bellman_ford step 9958 current loss 0.648704, current_train_items 318688.
I0302 19:02:06.471650 22895071117440 run.py:483] Algo bellman_ford step 9959 current loss 0.763329, current_train_items 318720.
I0302 19:02:06.490149 22895071117440 run.py:483] Algo bellman_ford step 9960 current loss 0.326433, current_train_items 318752.
I0302 19:02:06.505378 22895071117440 run.py:483] Algo bellman_ford step 9961 current loss 0.429199, current_train_items 318784.
I0302 19:02:06.528297 22895071117440 run.py:483] Algo bellman_ford step 9962 current loss 0.681878, current_train_items 318816.
I0302 19:02:06.557499 22895071117440 run.py:483] Algo bellman_ford step 9963 current loss 0.642852, current_train_items 318848.
I0302 19:02:06.588002 22895071117440 run.py:483] Algo bellman_ford step 9964 current loss 0.742951, current_train_items 318880.
I0302 19:02:06.606247 22895071117440 run.py:483] Algo bellman_ford step 9965 current loss 0.346501, current_train_items 318912.
I0302 19:02:06.621912 22895071117440 run.py:483] Algo bellman_ford step 9966 current loss 0.497726, current_train_items 318944.
I0302 19:02:06.644031 22895071117440 run.py:483] Algo bellman_ford step 9967 current loss 0.573570, current_train_items 318976.
I0302 19:02:06.672786 22895071117440 run.py:483] Algo bellman_ford step 9968 current loss 0.632174, current_train_items 319008.
I0302 19:02:06.703857 22895071117440 run.py:483] Algo bellman_ford step 9969 current loss 0.642565, current_train_items 319040.
I0302 19:02:06.722265 22895071117440 run.py:483] Algo bellman_ford step 9970 current loss 0.296923, current_train_items 319072.
I0302 19:02:06.737759 22895071117440 run.py:483] Algo bellman_ford step 9971 current loss 0.484864, current_train_items 319104.
I0302 19:02:06.759603 22895071117440 run.py:483] Algo bellman_ford step 9972 current loss 0.634724, current_train_items 319136.
I0302 19:02:06.787914 22895071117440 run.py:483] Algo bellman_ford step 9973 current loss 0.647215, current_train_items 319168.
I0302 19:02:06.818532 22895071117440 run.py:483] Algo bellman_ford step 9974 current loss 0.678818, current_train_items 319200.
I0302 19:02:06.837094 22895071117440 run.py:483] Algo bellman_ford step 9975 current loss 0.291403, current_train_items 319232.
I0302 19:02:06.853061 22895071117440 run.py:483] Algo bellman_ford step 9976 current loss 0.446642, current_train_items 319264.
I0302 19:02:06.875138 22895071117440 run.py:483] Algo bellman_ford step 9977 current loss 0.556437, current_train_items 319296.
I0302 19:02:06.904032 22895071117440 run.py:483] Algo bellman_ford step 9978 current loss 0.619165, current_train_items 319328.
I0302 19:02:06.933483 22895071117440 run.py:483] Algo bellman_ford step 9979 current loss 0.666848, current_train_items 319360.
I0302 19:02:06.951371 22895071117440 run.py:483] Algo bellman_ford step 9980 current loss 0.305915, current_train_items 319392.
I0302 19:02:06.966598 22895071117440 run.py:483] Algo bellman_ford step 9981 current loss 0.475001, current_train_items 319424.
I0302 19:02:06.989426 22895071117440 run.py:483] Algo bellman_ford step 9982 current loss 0.532589, current_train_items 319456.
I0302 19:02:07.019861 22895071117440 run.py:483] Algo bellman_ford step 9983 current loss 0.731909, current_train_items 319488.
I0302 19:02:07.051209 22895071117440 run.py:483] Algo bellman_ford step 9984 current loss 0.800109, current_train_items 319520.
I0302 19:02:07.069607 22895071117440 run.py:483] Algo bellman_ford step 9985 current loss 0.319664, current_train_items 319552.
I0302 19:02:07.085191 22895071117440 run.py:483] Algo bellman_ford step 9986 current loss 0.372406, current_train_items 319584.
I0302 19:02:07.106679 22895071117440 run.py:483] Algo bellman_ford step 9987 current loss 0.590592, current_train_items 319616.
I0302 19:02:07.135729 22895071117440 run.py:483] Algo bellman_ford step 9988 current loss 0.661969, current_train_items 319648.
I0302 19:02:07.166184 22895071117440 run.py:483] Algo bellman_ford step 9989 current loss 0.663201, current_train_items 319680.
I0302 19:02:07.184527 22895071117440 run.py:483] Algo bellman_ford step 9990 current loss 0.278937, current_train_items 319712.
I0302 19:02:07.200316 22895071117440 run.py:483] Algo bellman_ford step 9991 current loss 0.435316, current_train_items 319744.
I0302 19:02:07.221757 22895071117440 run.py:483] Algo bellman_ford step 9992 current loss 0.600243, current_train_items 319776.
I0302 19:02:07.249911 22895071117440 run.py:483] Algo bellman_ford step 9993 current loss 0.641637, current_train_items 319808.
I0302 19:02:07.279628 22895071117440 run.py:483] Algo bellman_ford step 9994 current loss 0.617103, current_train_items 319840.
I0302 19:02:07.298010 22895071117440 run.py:483] Algo bellman_ford step 9995 current loss 0.307549, current_train_items 319872.
I0302 19:02:07.313630 22895071117440 run.py:483] Algo bellman_ford step 9996 current loss 0.502190, current_train_items 319904.
I0302 19:02:07.335923 22895071117440 run.py:483] Algo bellman_ford step 9997 current loss 0.514482, current_train_items 319936.
I0302 19:02:07.364785 22895071117440 run.py:483] Algo bellman_ford step 9998 current loss 0.573069, current_train_items 319968.
I0302 19:02:07.393749 22895071117440 run.py:483] Algo bellman_ford step 9999 current loss 0.774320, current_train_items 320000.
I0302 19:02:07.398317 22895071117440 run.py:527] Restoring best model from checkpoint...
I0302 19:02:09.737726 22895071117440 run.py:542] (test) algo bellman_ford : {'pi': 0.59033203125, 'score': 0.59033203125, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:02:09.737949 22895071117440 run.py:544] Done!

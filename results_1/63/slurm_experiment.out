Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-04 19:24:58.439625: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-04 19:24:58.439908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-04 19:24:58.482582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-04 19:25:19.202689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0304 19:26:25.666300 23128000471168 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0304 19:26:25.672345 23128000471168 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0304 19:26:27.031074 23128000471168 run.py:307] Creating samplers for algo bellman_ford
W0304 19:26:27.031560 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.031859 23128000471168 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.247698 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.247962 23128000471168 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.497556 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.497808 23128000471168 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.841181 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.841414 23128000471168 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.258155 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:28.258409 23128000471168 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.787983 23128000471168 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0304 19:26:28.788226 23128000471168 samplers.py:112] Creating a dataset with 64 samples.
I0304 19:26:28.826359 23128000471168 run.py:166] Dataset not found in ./datasets_1/63/CLRS30_v1.0.0. Downloading...
I0304 19:26:45.909772 23128000471168 dataset_info.py:482] Load dataset info from ./datasets_1/63/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.912407 23128000471168 dataset_info.py:482] Load dataset info from ./datasets_1/63/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.913218 23128000471168 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/63/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0304 19:26:45.913295 23128000471168 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/63/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:27:02.317243 23128000471168 run.py:483] Algo bellman_ford step 0 current loss 3.700403, current_train_items 32.
I0304 19:27:05.303234 23128000471168 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.5322265625, 'score': 0.5322265625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0304 19:27:05.303476 23128000471168 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.532, val scores are: bellman_ford: 0.532
I0304 19:27:15.275861 23128000471168 run.py:483] Algo bellman_ford step 1 current loss 3.765024, current_train_items 64.
I0304 19:27:26.119619 23128000471168 run.py:483] Algo bellman_ford step 2 current loss 3.570317, current_train_items 96.
I0304 19:27:37.034440 23128000471168 run.py:483] Algo bellman_ford step 3 current loss 3.366226, current_train_items 128.
I0304 19:27:46.934114 23128000471168 run.py:483] Algo bellman_ford step 4 current loss 3.570996, current_train_items 160.
I0304 19:27:46.952995 23128000471168 run.py:483] Algo bellman_ford step 5 current loss 1.063726, current_train_items 192.
I0304 19:27:46.970087 23128000471168 run.py:483] Algo bellman_ford step 6 current loss 1.927523, current_train_items 224.
I0304 19:27:46.993370 23128000471168 run.py:483] Algo bellman_ford step 7 current loss 2.209085, current_train_items 256.
I0304 19:27:47.023012 23128000471168 run.py:483] Algo bellman_ford step 8 current loss 2.578653, current_train_items 288.
I0304 19:27:47.055064 23128000471168 run.py:483] Algo bellman_ford step 9 current loss 2.775190, current_train_items 320.
I0304 19:27:47.072933 23128000471168 run.py:483] Algo bellman_ford step 10 current loss 1.063293, current_train_items 352.
I0304 19:27:47.090134 23128000471168 run.py:483] Algo bellman_ford step 11 current loss 1.558971, current_train_items 384.
I0304 19:27:47.112709 23128000471168 run.py:483] Algo bellman_ford step 12 current loss 1.633751, current_train_items 416.
I0304 19:27:47.143785 23128000471168 run.py:483] Algo bellman_ford step 13 current loss 2.101719, current_train_items 448.
I0304 19:27:47.172265 23128000471168 run.py:483] Algo bellman_ford step 14 current loss 1.898079, current_train_items 480.
I0304 19:27:47.190867 23128000471168 run.py:483] Algo bellman_ford step 15 current loss 0.730744, current_train_items 512.
I0304 19:27:47.207134 23128000471168 run.py:483] Algo bellman_ford step 16 current loss 1.100456, current_train_items 544.
I0304 19:27:47.232148 23128000471168 run.py:483] Algo bellman_ford step 17 current loss 1.926517, current_train_items 576.
I0304 19:27:47.262065 23128000471168 run.py:483] Algo bellman_ford step 18 current loss 1.692377, current_train_items 608.
I0304 19:27:47.294702 23128000471168 run.py:483] Algo bellman_ford step 19 current loss 1.931018, current_train_items 640.
I0304 19:27:47.312392 23128000471168 run.py:483] Algo bellman_ford step 20 current loss 0.550177, current_train_items 672.
I0304 19:27:47.328476 23128000471168 run.py:483] Algo bellman_ford step 21 current loss 0.847197, current_train_items 704.
I0304 19:27:47.352282 23128000471168 run.py:483] Algo bellman_ford step 22 current loss 1.449344, current_train_items 736.
I0304 19:27:47.381806 23128000471168 run.py:483] Algo bellman_ford step 23 current loss 1.499374, current_train_items 768.
I0304 19:27:47.413225 23128000471168 run.py:483] Algo bellman_ford step 24 current loss 1.813325, current_train_items 800.
I0304 19:27:47.431688 23128000471168 run.py:483] Algo bellman_ford step 25 current loss 0.493736, current_train_items 832.
I0304 19:27:47.448123 23128000471168 run.py:483] Algo bellman_ford step 26 current loss 0.880637, current_train_items 864.
I0304 19:27:47.471925 23128000471168 run.py:483] Algo bellman_ford step 27 current loss 1.262411, current_train_items 896.
I0304 19:27:47.502465 23128000471168 run.py:483] Algo bellman_ford step 28 current loss 1.325311, current_train_items 928.
I0304 19:27:47.535376 23128000471168 run.py:483] Algo bellman_ford step 29 current loss 1.591496, current_train_items 960.
I0304 19:27:47.553524 23128000471168 run.py:483] Algo bellman_ford step 30 current loss 0.382445, current_train_items 992.
I0304 19:27:47.569128 23128000471168 run.py:483] Algo bellman_ford step 31 current loss 0.608653, current_train_items 1024.
I0304 19:27:47.592308 23128000471168 run.py:483] Algo bellman_ford step 32 current loss 1.060638, current_train_items 1056.
I0304 19:27:47.623186 23128000471168 run.py:483] Algo bellman_ford step 33 current loss 1.298377, current_train_items 1088.
I0304 19:27:47.654673 23128000471168 run.py:483] Algo bellman_ford step 34 current loss 1.367270, current_train_items 1120.
I0304 19:27:47.672760 23128000471168 run.py:483] Algo bellman_ford step 35 current loss 0.379131, current_train_items 1152.
I0304 19:27:47.688702 23128000471168 run.py:483] Algo bellman_ford step 36 current loss 0.569720, current_train_items 1184.
I0304 19:27:47.712524 23128000471168 run.py:483] Algo bellman_ford step 37 current loss 1.144480, current_train_items 1216.
I0304 19:27:47.742679 23128000471168 run.py:483] Algo bellman_ford step 38 current loss 1.215624, current_train_items 1248.
W0304 19:27:47.765872 23128000471168 samplers.py:155] Increasing hint lengh from 9 to 11
I0304 19:27:54.464728 23128000471168 run.py:483] Algo bellman_ford step 39 current loss 1.786194, current_train_items 1280.
I0304 19:27:54.485099 23128000471168 run.py:483] Algo bellman_ford step 40 current loss 0.329770, current_train_items 1312.
I0304 19:27:54.501814 23128000471168 run.py:483] Algo bellman_ford step 41 current loss 0.598361, current_train_items 1344.
I0304 19:27:54.525272 23128000471168 run.py:483] Algo bellman_ford step 42 current loss 0.896216, current_train_items 1376.
I0304 19:27:54.556632 23128000471168 run.py:483] Algo bellman_ford step 43 current loss 1.059775, current_train_items 1408.
I0304 19:27:54.589453 23128000471168 run.py:483] Algo bellman_ford step 44 current loss 1.191026, current_train_items 1440.
I0304 19:27:54.608854 23128000471168 run.py:483] Algo bellman_ford step 45 current loss 0.245393, current_train_items 1472.
I0304 19:27:54.625381 23128000471168 run.py:483] Algo bellman_ford step 46 current loss 0.613757, current_train_items 1504.
I0304 19:27:54.648252 23128000471168 run.py:483] Algo bellman_ford step 47 current loss 0.819918, current_train_items 1536.
I0304 19:27:54.676025 23128000471168 run.py:483] Algo bellman_ford step 48 current loss 0.679997, current_train_items 1568.
I0304 19:27:54.706001 23128000471168 run.py:483] Algo bellman_ford step 49 current loss 0.993832, current_train_items 1600.
I0304 19:27:54.725082 23128000471168 run.py:483] Algo bellman_ford step 50 current loss 0.266609, current_train_items 1632.
I0304 19:27:54.734145 23128000471168 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8369140625, 'score': 0.8369140625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0304 19:27:54.734265 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.532, current avg val score is 0.837, val scores are: bellman_ford: 0.837
I0304 19:27:54.763156 23128000471168 run.py:483] Algo bellman_ford step 51 current loss 0.466952, current_train_items 1664.
I0304 19:27:54.787074 23128000471168 run.py:483] Algo bellman_ford step 52 current loss 0.858956, current_train_items 1696.
I0304 19:27:54.817033 23128000471168 run.py:483] Algo bellman_ford step 53 current loss 0.841067, current_train_items 1728.
I0304 19:27:54.849261 23128000471168 run.py:483] Algo bellman_ford step 54 current loss 1.106900, current_train_items 1760.
I0304 19:27:54.868539 23128000471168 run.py:483] Algo bellman_ford step 55 current loss 0.266207, current_train_items 1792.
I0304 19:27:54.884697 23128000471168 run.py:483] Algo bellman_ford step 56 current loss 0.393746, current_train_items 1824.
I0304 19:27:54.907887 23128000471168 run.py:483] Algo bellman_ford step 57 current loss 0.763813, current_train_items 1856.
I0304 19:27:54.936203 23128000471168 run.py:483] Algo bellman_ford step 58 current loss 0.666606, current_train_items 1888.
I0304 19:27:54.969068 23128000471168 run.py:483] Algo bellman_ford step 59 current loss 1.025135, current_train_items 1920.
I0304 19:27:54.988178 23128000471168 run.py:483] Algo bellman_ford step 60 current loss 0.156853, current_train_items 1952.
W0304 19:27:54.997489 23128000471168 samplers.py:155] Increasing hint lengh from 6 to 7
I0304 19:28:01.487190 23128000471168 run.py:483] Algo bellman_ford step 61 current loss 0.469129, current_train_items 1984.
I0304 19:28:01.512015 23128000471168 run.py:483] Algo bellman_ford step 62 current loss 0.803405, current_train_items 2016.
I0304 19:28:01.542015 23128000471168 run.py:483] Algo bellman_ford step 63 current loss 0.896908, current_train_items 2048.
I0304 19:28:01.576392 23128000471168 run.py:483] Algo bellman_ford step 64 current loss 1.130343, current_train_items 2080.
I0304 19:28:01.596455 23128000471168 run.py:483] Algo bellman_ford step 65 current loss 0.162617, current_train_items 2112.
I0304 19:28:01.612809 23128000471168 run.py:483] Algo bellman_ford step 66 current loss 0.314237, current_train_items 2144.
I0304 19:28:01.637870 23128000471168 run.py:483] Algo bellman_ford step 67 current loss 0.727048, current_train_items 2176.
I0304 19:28:01.666938 23128000471168 run.py:483] Algo bellman_ford step 68 current loss 0.664056, current_train_items 2208.
I0304 19:28:01.699141 23128000471168 run.py:483] Algo bellman_ford step 69 current loss 0.801114, current_train_items 2240.
I0304 19:28:01.718057 23128000471168 run.py:483] Algo bellman_ford step 70 current loss 0.148366, current_train_items 2272.
I0304 19:28:01.734596 23128000471168 run.py:483] Algo bellman_ford step 71 current loss 0.378937, current_train_items 2304.
I0304 19:28:01.758350 23128000471168 run.py:483] Algo bellman_ford step 72 current loss 0.788146, current_train_items 2336.
I0304 19:28:01.788684 23128000471168 run.py:483] Algo bellman_ford step 73 current loss 0.764333, current_train_items 2368.
I0304 19:28:01.821446 23128000471168 run.py:483] Algo bellman_ford step 74 current loss 0.949367, current_train_items 2400.
I0304 19:28:01.840569 23128000471168 run.py:483] Algo bellman_ford step 75 current loss 0.062366, current_train_items 2432.
I0304 19:28:01.857534 23128000471168 run.py:483] Algo bellman_ford step 76 current loss 0.445035, current_train_items 2464.
I0304 19:28:01.881072 23128000471168 run.py:483] Algo bellman_ford step 77 current loss 0.751421, current_train_items 2496.
I0304 19:28:01.910250 23128000471168 run.py:483] Algo bellman_ford step 78 current loss 0.626508, current_train_items 2528.
I0304 19:28:01.939975 23128000471168 run.py:483] Algo bellman_ford step 79 current loss 0.762988, current_train_items 2560.
I0304 19:28:01.959670 23128000471168 run.py:483] Algo bellman_ford step 80 current loss 0.111614, current_train_items 2592.
I0304 19:28:01.975959 23128000471168 run.py:483] Algo bellman_ford step 81 current loss 0.346938, current_train_items 2624.
I0304 19:28:01.999533 23128000471168 run.py:483] Algo bellman_ford step 82 current loss 0.656397, current_train_items 2656.
I0304 19:28:02.029025 23128000471168 run.py:483] Algo bellman_ford step 83 current loss 0.625640, current_train_items 2688.
I0304 19:28:02.059925 23128000471168 run.py:483] Algo bellman_ford step 84 current loss 0.717436, current_train_items 2720.
I0304 19:28:02.078958 23128000471168 run.py:483] Algo bellman_ford step 85 current loss 0.137930, current_train_items 2752.
I0304 19:28:02.095516 23128000471168 run.py:483] Algo bellman_ford step 86 current loss 0.339505, current_train_items 2784.
I0304 19:28:02.120833 23128000471168 run.py:483] Algo bellman_ford step 87 current loss 0.692815, current_train_items 2816.
I0304 19:28:02.151010 23128000471168 run.py:483] Algo bellman_ford step 88 current loss 0.558624, current_train_items 2848.
I0304 19:28:02.183220 23128000471168 run.py:483] Algo bellman_ford step 89 current loss 0.695313, current_train_items 2880.
I0304 19:28:02.202624 23128000471168 run.py:483] Algo bellman_ford step 90 current loss 0.105918, current_train_items 2912.
I0304 19:28:02.219121 23128000471168 run.py:483] Algo bellman_ford step 91 current loss 0.366325, current_train_items 2944.
I0304 19:28:02.242619 23128000471168 run.py:483] Algo bellman_ford step 92 current loss 0.534059, current_train_items 2976.
I0304 19:28:02.273435 23128000471168 run.py:483] Algo bellman_ford step 93 current loss 0.659430, current_train_items 3008.
I0304 19:28:02.304913 23128000471168 run.py:483] Algo bellman_ford step 94 current loss 0.627233, current_train_items 3040.
I0304 19:28:02.324553 23128000471168 run.py:483] Algo bellman_ford step 95 current loss 0.086256, current_train_items 3072.
I0304 19:28:02.340721 23128000471168 run.py:483] Algo bellman_ford step 96 current loss 0.285066, current_train_items 3104.
I0304 19:28:02.364274 23128000471168 run.py:483] Algo bellman_ford step 97 current loss 0.511991, current_train_items 3136.
I0304 19:28:02.394043 23128000471168 run.py:483] Algo bellman_ford step 98 current loss 0.579016, current_train_items 3168.
I0304 19:28:02.426224 23128000471168 run.py:483] Algo bellman_ford step 99 current loss 0.768442, current_train_items 3200.
I0304 19:28:02.445294 23128000471168 run.py:483] Algo bellman_ford step 100 current loss 0.121767, current_train_items 3232.
I0304 19:28:02.454867 23128000471168 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0304 19:28:02.454981 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.837, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0304 19:28:02.486081 23128000471168 run.py:483] Algo bellman_ford step 101 current loss 0.366751, current_train_items 3264.
I0304 19:28:02.510119 23128000471168 run.py:483] Algo bellman_ford step 102 current loss 0.352796, current_train_items 3296.
I0304 19:28:02.539589 23128000471168 run.py:483] Algo bellman_ford step 103 current loss 0.522963, current_train_items 3328.
I0304 19:28:02.574321 23128000471168 run.py:483] Algo bellman_ford step 104 current loss 0.771542, current_train_items 3360.
I0304 19:28:02.593844 23128000471168 run.py:483] Algo bellman_ford step 105 current loss 0.061662, current_train_items 3392.
I0304 19:28:02.610431 23128000471168 run.py:483] Algo bellman_ford step 106 current loss 0.246152, current_train_items 3424.
I0304 19:28:02.634583 23128000471168 run.py:483] Algo bellman_ford step 107 current loss 0.652041, current_train_items 3456.
I0304 19:28:02.663796 23128000471168 run.py:483] Algo bellman_ford step 108 current loss 0.716535, current_train_items 3488.
I0304 19:28:02.694920 23128000471168 run.py:483] Algo bellman_ford step 109 current loss 0.610209, current_train_items 3520.
I0304 19:28:02.713927 23128000471168 run.py:483] Algo bellman_ford step 110 current loss 0.059750, current_train_items 3552.
I0304 19:28:02.730174 23128000471168 run.py:483] Algo bellman_ford step 111 current loss 0.215939, current_train_items 3584.
I0304 19:28:02.753228 23128000471168 run.py:483] Algo bellman_ford step 112 current loss 0.443237, current_train_items 3616.
I0304 19:28:02.783665 23128000471168 run.py:483] Algo bellman_ford step 113 current loss 0.683425, current_train_items 3648.
I0304 19:28:02.814702 23128000471168 run.py:483] Algo bellman_ford step 114 current loss 0.617162, current_train_items 3680.
I0304 19:28:02.833340 23128000471168 run.py:483] Algo bellman_ford step 115 current loss 0.051655, current_train_items 3712.
I0304 19:28:02.849825 23128000471168 run.py:483] Algo bellman_ford step 116 current loss 0.302329, current_train_items 3744.
I0304 19:28:02.874738 23128000471168 run.py:483] Algo bellman_ford step 117 current loss 0.770011, current_train_items 3776.
I0304 19:28:02.904336 23128000471168 run.py:483] Algo bellman_ford step 118 current loss 0.510696, current_train_items 3808.
I0304 19:28:02.934227 23128000471168 run.py:483] Algo bellman_ford step 119 current loss 0.468975, current_train_items 3840.
I0304 19:28:02.953367 23128000471168 run.py:483] Algo bellman_ford step 120 current loss 0.075287, current_train_items 3872.
I0304 19:28:02.969999 23128000471168 run.py:483] Algo bellman_ford step 121 current loss 0.360942, current_train_items 3904.
I0304 19:28:02.994253 23128000471168 run.py:483] Algo bellman_ford step 122 current loss 0.824603, current_train_items 3936.
I0304 19:28:03.024741 23128000471168 run.py:483] Algo bellman_ford step 123 current loss 0.950632, current_train_items 3968.
I0304 19:28:03.060374 23128000471168 run.py:483] Algo bellman_ford step 124 current loss 1.160586, current_train_items 4000.
I0304 19:28:03.079567 23128000471168 run.py:483] Algo bellman_ford step 125 current loss 0.122214, current_train_items 4032.
I0304 19:28:03.096321 23128000471168 run.py:483] Algo bellman_ford step 126 current loss 0.343055, current_train_items 4064.
I0304 19:28:03.120335 23128000471168 run.py:483] Algo bellman_ford step 127 current loss 0.619915, current_train_items 4096.
I0304 19:28:03.150195 23128000471168 run.py:483] Algo bellman_ford step 128 current loss 0.518169, current_train_items 4128.
I0304 19:28:03.182172 23128000471168 run.py:483] Algo bellman_ford step 129 current loss 0.508993, current_train_items 4160.
I0304 19:28:03.200917 23128000471168 run.py:483] Algo bellman_ford step 130 current loss 0.085200, current_train_items 4192.
I0304 19:28:03.217112 23128000471168 run.py:483] Algo bellman_ford step 131 current loss 0.197600, current_train_items 4224.
I0304 19:28:03.241436 23128000471168 run.py:483] Algo bellman_ford step 132 current loss 0.970803, current_train_items 4256.
I0304 19:28:03.271001 23128000471168 run.py:483] Algo bellman_ford step 133 current loss 0.841812, current_train_items 4288.
I0304 19:28:03.302520 23128000471168 run.py:483] Algo bellman_ford step 134 current loss 0.703699, current_train_items 4320.
I0304 19:28:03.321079 23128000471168 run.py:483] Algo bellman_ford step 135 current loss 0.043984, current_train_items 4352.
I0304 19:28:03.337383 23128000471168 run.py:483] Algo bellman_ford step 136 current loss 0.238493, current_train_items 4384.
I0304 19:28:03.361717 23128000471168 run.py:483] Algo bellman_ford step 137 current loss 0.442326, current_train_items 4416.
I0304 19:28:03.391385 23128000471168 run.py:483] Algo bellman_ford step 138 current loss 0.635259, current_train_items 4448.
I0304 19:28:03.423945 23128000471168 run.py:483] Algo bellman_ford step 139 current loss 0.608585, current_train_items 4480.
I0304 19:28:03.442834 23128000471168 run.py:483] Algo bellman_ford step 140 current loss 0.058355, current_train_items 4512.
I0304 19:28:03.459716 23128000471168 run.py:483] Algo bellman_ford step 141 current loss 0.252900, current_train_items 4544.
I0304 19:28:03.483428 23128000471168 run.py:483] Algo bellman_ford step 142 current loss 0.426820, current_train_items 4576.
I0304 19:28:03.513721 23128000471168 run.py:483] Algo bellman_ford step 143 current loss 0.667208, current_train_items 4608.
I0304 19:28:03.545258 23128000471168 run.py:483] Algo bellman_ford step 144 current loss 0.493455, current_train_items 4640.
I0304 19:28:03.564222 23128000471168 run.py:483] Algo bellman_ford step 145 current loss 0.047909, current_train_items 4672.
I0304 19:28:03.580490 23128000471168 run.py:483] Algo bellman_ford step 146 current loss 0.204491, current_train_items 4704.
I0304 19:28:03.603600 23128000471168 run.py:483] Algo bellman_ford step 147 current loss 0.389206, current_train_items 4736.
I0304 19:28:03.632954 23128000471168 run.py:483] Algo bellman_ford step 148 current loss 0.564249, current_train_items 4768.
I0304 19:28:03.663293 23128000471168 run.py:483] Algo bellman_ford step 149 current loss 0.547334, current_train_items 4800.
I0304 19:28:03.681997 23128000471168 run.py:483] Algo bellman_ford step 150 current loss 0.048623, current_train_items 4832.
I0304 19:28:03.690238 23128000471168 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0304 19:28:03.690353 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.878, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0304 19:28:03.719144 23128000471168 run.py:483] Algo bellman_ford step 151 current loss 0.121556, current_train_items 4864.
I0304 19:28:03.743426 23128000471168 run.py:483] Algo bellman_ford step 152 current loss 0.291484, current_train_items 4896.
I0304 19:28:03.773120 23128000471168 run.py:483] Algo bellman_ford step 153 current loss 0.371303, current_train_items 4928.
I0304 19:28:03.809334 23128000471168 run.py:483] Algo bellman_ford step 154 current loss 0.597441, current_train_items 4960.
I0304 19:28:03.829293 23128000471168 run.py:483] Algo bellman_ford step 155 current loss 0.073054, current_train_items 4992.
I0304 19:28:03.845990 23128000471168 run.py:483] Algo bellman_ford step 156 current loss 0.256830, current_train_items 5024.
I0304 19:28:03.870151 23128000471168 run.py:483] Algo bellman_ford step 157 current loss 0.361493, current_train_items 5056.
I0304 19:28:03.898865 23128000471168 run.py:483] Algo bellman_ford step 158 current loss 0.314933, current_train_items 5088.
I0304 19:28:03.930982 23128000471168 run.py:483] Algo bellman_ford step 159 current loss 0.402283, current_train_items 5120.
I0304 19:28:03.950099 23128000471168 run.py:483] Algo bellman_ford step 160 current loss 0.097470, current_train_items 5152.
I0304 19:28:03.966961 23128000471168 run.py:483] Algo bellman_ford step 161 current loss 0.151970, current_train_items 5184.
I0304 19:28:03.989968 23128000471168 run.py:483] Algo bellman_ford step 162 current loss 0.242554, current_train_items 5216.
I0304 19:28:04.019601 23128000471168 run.py:483] Algo bellman_ford step 163 current loss 0.349409, current_train_items 5248.
I0304 19:28:04.050878 23128000471168 run.py:483] Algo bellman_ford step 164 current loss 0.391196, current_train_items 5280.
I0304 19:28:04.070096 23128000471168 run.py:483] Algo bellman_ford step 165 current loss 0.041313, current_train_items 5312.
I0304 19:28:04.086825 23128000471168 run.py:483] Algo bellman_ford step 166 current loss 0.210650, current_train_items 5344.
I0304 19:28:04.110059 23128000471168 run.py:483] Algo bellman_ford step 167 current loss 0.236725, current_train_items 5376.
I0304 19:28:04.140073 23128000471168 run.py:483] Algo bellman_ford step 168 current loss 0.355463, current_train_items 5408.
I0304 19:28:04.171872 23128000471168 run.py:483] Algo bellman_ford step 169 current loss 0.364040, current_train_items 5440.
I0304 19:28:04.191346 23128000471168 run.py:483] Algo bellman_ford step 170 current loss 0.054266, current_train_items 5472.
I0304 19:28:04.208019 23128000471168 run.py:483] Algo bellman_ford step 171 current loss 0.109910, current_train_items 5504.
I0304 19:28:04.231038 23128000471168 run.py:483] Algo bellman_ford step 172 current loss 0.208905, current_train_items 5536.
I0304 19:28:04.260992 23128000471168 run.py:483] Algo bellman_ford step 173 current loss 0.357206, current_train_items 5568.
I0304 19:28:04.296302 23128000471168 run.py:483] Algo bellman_ford step 174 current loss 0.523955, current_train_items 5600.
I0304 19:28:04.315136 23128000471168 run.py:483] Algo bellman_ford step 175 current loss 0.036040, current_train_items 5632.
I0304 19:28:04.331527 23128000471168 run.py:483] Algo bellman_ford step 176 current loss 0.175075, current_train_items 5664.
I0304 19:28:04.355661 23128000471168 run.py:483] Algo bellman_ford step 177 current loss 0.323853, current_train_items 5696.
I0304 19:28:04.384116 23128000471168 run.py:483] Algo bellman_ford step 178 current loss 0.198827, current_train_items 5728.
I0304 19:28:04.414256 23128000471168 run.py:483] Algo bellman_ford step 179 current loss 0.397176, current_train_items 5760.
I0304 19:28:04.433934 23128000471168 run.py:483] Algo bellman_ford step 180 current loss 0.034018, current_train_items 5792.
I0304 19:28:04.450510 23128000471168 run.py:483] Algo bellman_ford step 181 current loss 0.104761, current_train_items 5824.
I0304 19:28:04.474984 23128000471168 run.py:483] Algo bellman_ford step 182 current loss 0.281500, current_train_items 5856.
I0304 19:28:04.504783 23128000471168 run.py:483] Algo bellman_ford step 183 current loss 0.342171, current_train_items 5888.
I0304 19:28:04.535354 23128000471168 run.py:483] Algo bellman_ford step 184 current loss 0.330624, current_train_items 5920.
I0304 19:28:04.554104 23128000471168 run.py:483] Algo bellman_ford step 185 current loss 0.020955, current_train_items 5952.
I0304 19:28:04.571264 23128000471168 run.py:483] Algo bellman_ford step 186 current loss 0.134915, current_train_items 5984.
I0304 19:28:04.594232 23128000471168 run.py:483] Algo bellman_ford step 187 current loss 0.197698, current_train_items 6016.
I0304 19:28:04.623569 23128000471168 run.py:483] Algo bellman_ford step 188 current loss 0.350645, current_train_items 6048.
I0304 19:28:04.657316 23128000471168 run.py:483] Algo bellman_ford step 189 current loss 0.416966, current_train_items 6080.
I0304 19:28:04.676279 23128000471168 run.py:483] Algo bellman_ford step 190 current loss 0.028932, current_train_items 6112.
I0304 19:28:04.693174 23128000471168 run.py:483] Algo bellman_ford step 191 current loss 0.282437, current_train_items 6144.
I0304 19:28:04.717357 23128000471168 run.py:483] Algo bellman_ford step 192 current loss 0.445481, current_train_items 6176.
I0304 19:28:04.747110 23128000471168 run.py:483] Algo bellman_ford step 193 current loss 0.312714, current_train_items 6208.
I0304 19:28:04.778703 23128000471168 run.py:483] Algo bellman_ford step 194 current loss 0.364516, current_train_items 6240.
I0304 19:28:04.798419 23128000471168 run.py:483] Algo bellman_ford step 195 current loss 0.038161, current_train_items 6272.
I0304 19:28:04.814885 23128000471168 run.py:483] Algo bellman_ford step 196 current loss 0.160016, current_train_items 6304.
I0304 19:28:04.838407 23128000471168 run.py:483] Algo bellman_ford step 197 current loss 0.252539, current_train_items 6336.
I0304 19:28:04.868678 23128000471168 run.py:483] Algo bellman_ford step 198 current loss 0.491068, current_train_items 6368.
I0304 19:28:04.901300 23128000471168 run.py:483] Algo bellman_ford step 199 current loss 0.392524, current_train_items 6400.
I0304 19:28:04.920480 23128000471168 run.py:483] Algo bellman_ford step 200 current loss 0.039392, current_train_items 6432.
I0304 19:28:04.927970 23128000471168 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0304 19:28:04.928084 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.922, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0304 19:28:04.960357 23128000471168 run.py:483] Algo bellman_ford step 201 current loss 0.108491, current_train_items 6464.
I0304 19:28:04.984707 23128000471168 run.py:483] Algo bellman_ford step 202 current loss 0.343599, current_train_items 6496.
I0304 19:28:05.016725 23128000471168 run.py:483] Algo bellman_ford step 203 current loss 0.345659, current_train_items 6528.
I0304 19:28:05.051934 23128000471168 run.py:483] Algo bellman_ford step 204 current loss 0.573195, current_train_items 6560.
I0304 19:28:05.071793 23128000471168 run.py:483] Algo bellman_ford step 205 current loss 0.075678, current_train_items 6592.
I0304 19:28:05.087693 23128000471168 run.py:483] Algo bellman_ford step 206 current loss 0.085158, current_train_items 6624.
I0304 19:28:05.111981 23128000471168 run.py:483] Algo bellman_ford step 207 current loss 0.306317, current_train_items 6656.
I0304 19:28:05.142241 23128000471168 run.py:483] Algo bellman_ford step 208 current loss 0.297128, current_train_items 6688.
I0304 19:28:05.173068 23128000471168 run.py:483] Algo bellman_ford step 209 current loss 0.351211, current_train_items 6720.
I0304 19:28:05.192125 23128000471168 run.py:483] Algo bellman_ford step 210 current loss 0.116089, current_train_items 6752.
I0304 19:28:05.208586 23128000471168 run.py:483] Algo bellman_ford step 211 current loss 0.110728, current_train_items 6784.
I0304 19:28:05.232878 23128000471168 run.py:483] Algo bellman_ford step 212 current loss 0.246809, current_train_items 6816.
I0304 19:28:05.262931 23128000471168 run.py:483] Algo bellman_ford step 213 current loss 0.237633, current_train_items 6848.
I0304 19:28:05.291419 23128000471168 run.py:483] Algo bellman_ford step 214 current loss 0.179337, current_train_items 6880.
I0304 19:28:05.310539 23128000471168 run.py:483] Algo bellman_ford step 215 current loss 0.032040, current_train_items 6912.
I0304 19:28:05.326668 23128000471168 run.py:483] Algo bellman_ford step 216 current loss 0.230477, current_train_items 6944.
I0304 19:28:05.349966 23128000471168 run.py:483] Algo bellman_ford step 217 current loss 0.297844, current_train_items 6976.
I0304 19:28:05.379155 23128000471168 run.py:483] Algo bellman_ford step 218 current loss 0.290571, current_train_items 7008.
I0304 19:28:05.410033 23128000471168 run.py:483] Algo bellman_ford step 219 current loss 0.324643, current_train_items 7040.
I0304 19:28:05.429232 23128000471168 run.py:483] Algo bellman_ford step 220 current loss 0.029074, current_train_items 7072.
I0304 19:28:05.445692 23128000471168 run.py:483] Algo bellman_ford step 221 current loss 0.129617, current_train_items 7104.
I0304 19:28:05.470210 23128000471168 run.py:483] Algo bellman_ford step 222 current loss 0.312562, current_train_items 7136.
I0304 19:28:05.499112 23128000471168 run.py:483] Algo bellman_ford step 223 current loss 0.288282, current_train_items 7168.
I0304 19:28:05.531574 23128000471168 run.py:483] Algo bellman_ford step 224 current loss 0.313437, current_train_items 7200.
I0304 19:28:05.550522 23128000471168 run.py:483] Algo bellman_ford step 225 current loss 0.029146, current_train_items 7232.
I0304 19:28:05.567232 23128000471168 run.py:483] Algo bellman_ford step 226 current loss 0.165401, current_train_items 7264.
I0304 19:28:05.590996 23128000471168 run.py:483] Algo bellman_ford step 227 current loss 0.399859, current_train_items 7296.
I0304 19:28:05.621394 23128000471168 run.py:483] Algo bellman_ford step 228 current loss 0.359704, current_train_items 7328.
I0304 19:28:05.654037 23128000471168 run.py:483] Algo bellman_ford step 229 current loss 0.402976, current_train_items 7360.
I0304 19:28:05.672611 23128000471168 run.py:483] Algo bellman_ford step 230 current loss 0.019065, current_train_items 7392.
I0304 19:28:05.688961 23128000471168 run.py:483] Algo bellman_ford step 231 current loss 0.097372, current_train_items 7424.
I0304 19:28:05.713344 23128000471168 run.py:483] Algo bellman_ford step 232 current loss 0.317496, current_train_items 7456.
I0304 19:28:05.743804 23128000471168 run.py:483] Algo bellman_ford step 233 current loss 0.285807, current_train_items 7488.
I0304 19:28:05.777035 23128000471168 run.py:483] Algo bellman_ford step 234 current loss 0.329123, current_train_items 7520.
I0304 19:28:05.796089 23128000471168 run.py:483] Algo bellman_ford step 235 current loss 0.043351, current_train_items 7552.
I0304 19:28:05.812215 23128000471168 run.py:483] Algo bellman_ford step 236 current loss 0.113860, current_train_items 7584.
I0304 19:28:05.835790 23128000471168 run.py:483] Algo bellman_ford step 237 current loss 0.331808, current_train_items 7616.
I0304 19:28:05.865607 23128000471168 run.py:483] Algo bellman_ford step 238 current loss 0.288029, current_train_items 7648.
I0304 19:28:05.897248 23128000471168 run.py:483] Algo bellman_ford step 239 current loss 0.333541, current_train_items 7680.
I0304 19:28:05.915976 23128000471168 run.py:483] Algo bellman_ford step 240 current loss 0.018184, current_train_items 7712.
I0304 19:28:05.932571 23128000471168 run.py:483] Algo bellman_ford step 241 current loss 0.122847, current_train_items 7744.
I0304 19:28:05.956967 23128000471168 run.py:483] Algo bellman_ford step 242 current loss 0.252644, current_train_items 7776.
I0304 19:28:05.986300 23128000471168 run.py:483] Algo bellman_ford step 243 current loss 0.296310, current_train_items 7808.
I0304 19:28:06.015909 23128000471168 run.py:483] Algo bellman_ford step 244 current loss 0.311280, current_train_items 7840.
I0304 19:28:06.035308 23128000471168 run.py:483] Algo bellman_ford step 245 current loss 0.056390, current_train_items 7872.
I0304 19:28:06.051812 23128000471168 run.py:483] Algo bellman_ford step 246 current loss 0.210912, current_train_items 7904.
I0304 19:28:06.075048 23128000471168 run.py:483] Algo bellman_ford step 247 current loss 0.462027, current_train_items 7936.
I0304 19:28:06.106027 23128000471168 run.py:483] Algo bellman_ford step 248 current loss 0.596486, current_train_items 7968.
I0304 19:28:06.138210 23128000471168 run.py:483] Algo bellman_ford step 249 current loss 0.434296, current_train_items 8000.
I0304 19:28:06.157396 23128000471168 run.py:483] Algo bellman_ford step 250 current loss 0.102967, current_train_items 8032.
I0304 19:28:06.166032 23128000471168 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0304 19:28:06.166142 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.932, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0304 19:28:06.183682 23128000471168 run.py:483] Algo bellman_ford step 251 current loss 0.390966, current_train_items 8064.
I0304 19:28:06.208705 23128000471168 run.py:483] Algo bellman_ford step 252 current loss 0.675522, current_train_items 8096.
I0304 19:28:06.239685 23128000471168 run.py:483] Algo bellman_ford step 253 current loss 0.544896, current_train_items 8128.
I0304 19:28:06.273254 23128000471168 run.py:483] Algo bellman_ford step 254 current loss 0.553896, current_train_items 8160.
I0304 19:28:06.293212 23128000471168 run.py:483] Algo bellman_ford step 255 current loss 0.103290, current_train_items 8192.
I0304 19:28:06.309169 23128000471168 run.py:483] Algo bellman_ford step 256 current loss 0.132188, current_train_items 8224.
I0304 19:28:06.332634 23128000471168 run.py:483] Algo bellman_ford step 257 current loss 0.296255, current_train_items 8256.
I0304 19:28:06.362490 23128000471168 run.py:483] Algo bellman_ford step 258 current loss 0.504926, current_train_items 8288.
I0304 19:28:06.392986 23128000471168 run.py:483] Algo bellman_ford step 259 current loss 0.540860, current_train_items 8320.
I0304 19:28:06.412654 23128000471168 run.py:483] Algo bellman_ford step 260 current loss 0.042727, current_train_items 8352.
I0304 19:28:06.429339 23128000471168 run.py:483] Algo bellman_ford step 261 current loss 0.139708, current_train_items 8384.
I0304 19:28:06.452634 23128000471168 run.py:483] Algo bellman_ford step 262 current loss 0.257891, current_train_items 8416.
I0304 19:28:06.482049 23128000471168 run.py:483] Algo bellman_ford step 263 current loss 0.340513, current_train_items 8448.
I0304 19:28:06.513307 23128000471168 run.py:483] Algo bellman_ford step 264 current loss 0.270847, current_train_items 8480.
I0304 19:28:06.532571 23128000471168 run.py:483] Algo bellman_ford step 265 current loss 0.048454, current_train_items 8512.
I0304 19:28:06.549278 23128000471168 run.py:483] Algo bellman_ford step 266 current loss 0.150571, current_train_items 8544.
I0304 19:28:06.573634 23128000471168 run.py:483] Algo bellman_ford step 267 current loss 0.353205, current_train_items 8576.
I0304 19:28:06.604792 23128000471168 run.py:483] Algo bellman_ford step 268 current loss 0.450729, current_train_items 8608.
I0304 19:28:06.635296 23128000471168 run.py:483] Algo bellman_ford step 269 current loss 0.307216, current_train_items 8640.
I0304 19:28:06.654940 23128000471168 run.py:483] Algo bellman_ford step 270 current loss 0.050187, current_train_items 8672.
I0304 19:28:06.670995 23128000471168 run.py:483] Algo bellman_ford step 271 current loss 0.097568, current_train_items 8704.
I0304 19:28:06.695145 23128000471168 run.py:483] Algo bellman_ford step 272 current loss 0.225638, current_train_items 8736.
I0304 19:28:06.724818 23128000471168 run.py:483] Algo bellman_ford step 273 current loss 0.265565, current_train_items 8768.
I0304 19:28:06.755602 23128000471168 run.py:483] Algo bellman_ford step 274 current loss 0.373733, current_train_items 8800.
I0304 19:28:06.775196 23128000471168 run.py:483] Algo bellman_ford step 275 current loss 0.039262, current_train_items 8832.
I0304 19:28:06.791678 23128000471168 run.py:483] Algo bellman_ford step 276 current loss 0.098708, current_train_items 8864.
I0304 19:28:06.816357 23128000471168 run.py:483] Algo bellman_ford step 277 current loss 0.263459, current_train_items 8896.
I0304 19:28:06.847437 23128000471168 run.py:483] Algo bellman_ford step 278 current loss 0.342215, current_train_items 8928.
I0304 19:28:06.878872 23128000471168 run.py:483] Algo bellman_ford step 279 current loss 0.305483, current_train_items 8960.
I0304 19:28:06.897871 23128000471168 run.py:483] Algo bellman_ford step 280 current loss 0.057727, current_train_items 8992.
I0304 19:28:06.914491 23128000471168 run.py:483] Algo bellman_ford step 281 current loss 0.146455, current_train_items 9024.
I0304 19:28:06.938870 23128000471168 run.py:483] Algo bellman_ford step 282 current loss 0.234854, current_train_items 9056.
I0304 19:28:06.968592 23128000471168 run.py:483] Algo bellman_ford step 283 current loss 0.265462, current_train_items 9088.
I0304 19:28:07.001510 23128000471168 run.py:483] Algo bellman_ford step 284 current loss 0.415067, current_train_items 9120.
I0304 19:28:07.020925 23128000471168 run.py:483] Algo bellman_ford step 285 current loss 0.055556, current_train_items 9152.
I0304 19:28:07.037573 23128000471168 run.py:483] Algo bellman_ford step 286 current loss 0.105568, current_train_items 9184.
I0304 19:28:07.060686 23128000471168 run.py:483] Algo bellman_ford step 287 current loss 0.260004, current_train_items 9216.
I0304 19:28:07.091184 23128000471168 run.py:483] Algo bellman_ford step 288 current loss 0.324280, current_train_items 9248.
I0304 19:28:07.124295 23128000471168 run.py:483] Algo bellman_ford step 289 current loss 0.324389, current_train_items 9280.
I0304 19:28:07.143631 23128000471168 run.py:483] Algo bellman_ford step 290 current loss 0.019877, current_train_items 9312.
I0304 19:28:07.160117 23128000471168 run.py:483] Algo bellman_ford step 291 current loss 0.075582, current_train_items 9344.
I0304 19:28:07.184028 23128000471168 run.py:483] Algo bellman_ford step 292 current loss 0.173746, current_train_items 9376.
I0304 19:28:07.213711 23128000471168 run.py:483] Algo bellman_ford step 293 current loss 0.271475, current_train_items 9408.
I0304 19:28:07.245409 23128000471168 run.py:483] Algo bellman_ford step 294 current loss 0.461487, current_train_items 9440.
I0304 19:28:07.264384 23128000471168 run.py:483] Algo bellman_ford step 295 current loss 0.091904, current_train_items 9472.
I0304 19:28:07.280521 23128000471168 run.py:483] Algo bellman_ford step 296 current loss 0.076817, current_train_items 9504.
I0304 19:28:07.304765 23128000471168 run.py:483] Algo bellman_ford step 297 current loss 0.208412, current_train_items 9536.
I0304 19:28:07.335627 23128000471168 run.py:483] Algo bellman_ford step 298 current loss 0.343432, current_train_items 9568.
I0304 19:28:07.367144 23128000471168 run.py:483] Algo bellman_ford step 299 current loss 0.374247, current_train_items 9600.
I0304 19:28:07.386464 23128000471168 run.py:483] Algo bellman_ford step 300 current loss 0.056400, current_train_items 9632.
I0304 19:28:07.394490 23128000471168 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0304 19:28:07.394598 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.932, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0304 19:28:07.423820 23128000471168 run.py:483] Algo bellman_ford step 301 current loss 0.095663, current_train_items 9664.
I0304 19:28:07.447719 23128000471168 run.py:483] Algo bellman_ford step 302 current loss 0.178489, current_train_items 9696.
I0304 19:28:07.477261 23128000471168 run.py:483] Algo bellman_ford step 303 current loss 0.154057, current_train_items 9728.
I0304 19:28:07.510138 23128000471168 run.py:483] Algo bellman_ford step 304 current loss 0.365755, current_train_items 9760.
I0304 19:28:07.529600 23128000471168 run.py:483] Algo bellman_ford step 305 current loss 0.018425, current_train_items 9792.
I0304 19:28:07.546371 23128000471168 run.py:483] Algo bellman_ford step 306 current loss 0.139589, current_train_items 9824.
I0304 19:28:07.571177 23128000471168 run.py:483] Algo bellman_ford step 307 current loss 0.248317, current_train_items 9856.
I0304 19:28:07.601181 23128000471168 run.py:483] Algo bellman_ford step 308 current loss 0.292027, current_train_items 9888.
I0304 19:28:07.633272 23128000471168 run.py:483] Algo bellman_ford step 309 current loss 0.318543, current_train_items 9920.
I0304 19:28:07.652514 23128000471168 run.py:483] Algo bellman_ford step 310 current loss 0.049694, current_train_items 9952.
I0304 19:28:07.669163 23128000471168 run.py:483] Algo bellman_ford step 311 current loss 0.084334, current_train_items 9984.
I0304 19:28:07.693049 23128000471168 run.py:483] Algo bellman_ford step 312 current loss 0.160718, current_train_items 10016.
I0304 19:28:07.723718 23128000471168 run.py:483] Algo bellman_ford step 313 current loss 0.239793, current_train_items 10048.
I0304 19:28:07.755552 23128000471168 run.py:483] Algo bellman_ford step 314 current loss 0.299522, current_train_items 10080.
I0304 19:28:07.774483 23128000471168 run.py:483] Algo bellman_ford step 315 current loss 0.036553, current_train_items 10112.
I0304 19:28:07.791277 23128000471168 run.py:483] Algo bellman_ford step 316 current loss 0.077430, current_train_items 10144.
I0304 19:28:07.814533 23128000471168 run.py:483] Algo bellman_ford step 317 current loss 0.152693, current_train_items 10176.
I0304 19:28:07.843274 23128000471168 run.py:483] Algo bellman_ford step 318 current loss 0.168422, current_train_items 10208.
I0304 19:28:07.874178 23128000471168 run.py:483] Algo bellman_ford step 319 current loss 0.310135, current_train_items 10240.
I0304 19:28:07.892962 23128000471168 run.py:483] Algo bellman_ford step 320 current loss 0.015911, current_train_items 10272.
I0304 19:28:07.909598 23128000471168 run.py:483] Algo bellman_ford step 321 current loss 0.084223, current_train_items 10304.
I0304 19:28:07.933440 23128000471168 run.py:483] Algo bellman_ford step 322 current loss 0.270165, current_train_items 10336.
I0304 19:28:07.962396 23128000471168 run.py:483] Algo bellman_ford step 323 current loss 0.165574, current_train_items 10368.
I0304 19:28:07.994256 23128000471168 run.py:483] Algo bellman_ford step 324 current loss 0.271074, current_train_items 10400.
I0304 19:28:08.013339 23128000471168 run.py:483] Algo bellman_ford step 325 current loss 0.057890, current_train_items 10432.
I0304 19:28:08.029470 23128000471168 run.py:483] Algo bellman_ford step 326 current loss 0.046757, current_train_items 10464.
I0304 19:28:08.053225 23128000471168 run.py:483] Algo bellman_ford step 327 current loss 0.182607, current_train_items 10496.
I0304 19:28:08.083776 23128000471168 run.py:483] Algo bellman_ford step 328 current loss 0.269151, current_train_items 10528.
I0304 19:28:08.116116 23128000471168 run.py:483] Algo bellman_ford step 329 current loss 0.298313, current_train_items 10560.
I0304 19:28:08.135240 23128000471168 run.py:483] Algo bellman_ford step 330 current loss 0.072851, current_train_items 10592.
I0304 19:28:08.151569 23128000471168 run.py:483] Algo bellman_ford step 331 current loss 0.138405, current_train_items 10624.
I0304 19:28:08.175492 23128000471168 run.py:483] Algo bellman_ford step 332 current loss 0.241307, current_train_items 10656.
I0304 19:28:08.206141 23128000471168 run.py:483] Algo bellman_ford step 333 current loss 0.242465, current_train_items 10688.
I0304 19:28:08.239371 23128000471168 run.py:483] Algo bellman_ford step 334 current loss 0.419943, current_train_items 10720.
I0304 19:28:08.258575 23128000471168 run.py:483] Algo bellman_ford step 335 current loss 0.112638, current_train_items 10752.
I0304 19:28:08.275431 23128000471168 run.py:483] Algo bellman_ford step 336 current loss 0.325314, current_train_items 10784.
I0304 19:28:08.300203 23128000471168 run.py:483] Algo bellman_ford step 337 current loss 0.386144, current_train_items 10816.
I0304 19:28:08.330159 23128000471168 run.py:483] Algo bellman_ford step 338 current loss 0.216276, current_train_items 10848.
I0304 19:28:08.362988 23128000471168 run.py:483] Algo bellman_ford step 339 current loss 0.358949, current_train_items 10880.
I0304 19:28:08.382043 23128000471168 run.py:483] Algo bellman_ford step 340 current loss 0.063752, current_train_items 10912.
I0304 19:28:08.398214 23128000471168 run.py:483] Algo bellman_ford step 341 current loss 0.262079, current_train_items 10944.
I0304 19:28:08.421907 23128000471168 run.py:483] Algo bellman_ford step 342 current loss 0.565166, current_train_items 10976.
I0304 19:28:08.450614 23128000471168 run.py:483] Algo bellman_ford step 343 current loss 0.378090, current_train_items 11008.
I0304 19:28:08.483042 23128000471168 run.py:483] Algo bellman_ford step 344 current loss 0.276365, current_train_items 11040.
I0304 19:28:08.502248 23128000471168 run.py:483] Algo bellman_ford step 345 current loss 0.067330, current_train_items 11072.
I0304 19:28:08.518941 23128000471168 run.py:483] Algo bellman_ford step 346 current loss 0.275124, current_train_items 11104.
I0304 19:28:08.543817 23128000471168 run.py:483] Algo bellman_ford step 347 current loss 0.569178, current_train_items 11136.
I0304 19:28:08.573545 23128000471168 run.py:483] Algo bellman_ford step 348 current loss 0.504335, current_train_items 11168.
I0304 19:28:08.606261 23128000471168 run.py:483] Algo bellman_ford step 349 current loss 0.374345, current_train_items 11200.
I0304 19:28:08.625546 23128000471168 run.py:483] Algo bellman_ford step 350 current loss 0.055699, current_train_items 11232.
I0304 19:28:08.633853 23128000471168 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0304 19:28:08.633960 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.935, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0304 19:28:08.650636 23128000471168 run.py:483] Algo bellman_ford step 351 current loss 0.176168, current_train_items 11264.
I0304 19:28:08.675287 23128000471168 run.py:483] Algo bellman_ford step 352 current loss 0.479421, current_train_items 11296.
I0304 19:28:08.705838 23128000471168 run.py:483] Algo bellman_ford step 353 current loss 0.526759, current_train_items 11328.
I0304 19:28:08.740334 23128000471168 run.py:483] Algo bellman_ford step 354 current loss 0.559503, current_train_items 11360.
I0304 19:28:08.759723 23128000471168 run.py:483] Algo bellman_ford step 355 current loss 0.070430, current_train_items 11392.
I0304 19:28:08.775831 23128000471168 run.py:483] Algo bellman_ford step 356 current loss 0.155694, current_train_items 11424.
I0304 19:28:08.799759 23128000471168 run.py:483] Algo bellman_ford step 357 current loss 0.203821, current_train_items 11456.
I0304 19:28:08.831207 23128000471168 run.py:483] Algo bellman_ford step 358 current loss 0.327241, current_train_items 11488.
I0304 19:28:08.864244 23128000471168 run.py:483] Algo bellman_ford step 359 current loss 0.339172, current_train_items 11520.
I0304 19:28:08.883586 23128000471168 run.py:483] Algo bellman_ford step 360 current loss 0.032216, current_train_items 11552.
I0304 19:28:08.900708 23128000471168 run.py:483] Algo bellman_ford step 361 current loss 0.106910, current_train_items 11584.
I0304 19:28:08.923368 23128000471168 run.py:483] Algo bellman_ford step 362 current loss 0.240252, current_train_items 11616.
I0304 19:28:08.953265 23128000471168 run.py:483] Algo bellman_ford step 363 current loss 0.286792, current_train_items 11648.
I0304 19:28:08.983620 23128000471168 run.py:483] Algo bellman_ford step 364 current loss 0.328042, current_train_items 11680.
I0304 19:28:09.003045 23128000471168 run.py:483] Algo bellman_ford step 365 current loss 0.029338, current_train_items 11712.
I0304 19:28:09.019568 23128000471168 run.py:483] Algo bellman_ford step 366 current loss 0.130134, current_train_items 11744.
I0304 19:28:09.042867 23128000471168 run.py:483] Algo bellman_ford step 367 current loss 0.208741, current_train_items 11776.
I0304 19:28:09.073436 23128000471168 run.py:483] Algo bellman_ford step 368 current loss 0.316601, current_train_items 11808.
I0304 19:28:09.103608 23128000471168 run.py:483] Algo bellman_ford step 369 current loss 0.190988, current_train_items 11840.
I0304 19:28:09.122995 23128000471168 run.py:483] Algo bellman_ford step 370 current loss 0.024597, current_train_items 11872.
I0304 19:28:09.139417 23128000471168 run.py:483] Algo bellman_ford step 371 current loss 0.080701, current_train_items 11904.
I0304 19:28:09.162621 23128000471168 run.py:483] Algo bellman_ford step 372 current loss 0.156737, current_train_items 11936.
I0304 19:28:09.193703 23128000471168 run.py:483] Algo bellman_ford step 373 current loss 0.302214, current_train_items 11968.
I0304 19:28:09.227552 23128000471168 run.py:483] Algo bellman_ford step 374 current loss 0.360949, current_train_items 12000.
I0304 19:28:09.246912 23128000471168 run.py:483] Algo bellman_ford step 375 current loss 0.035432, current_train_items 12032.
I0304 19:28:09.263064 23128000471168 run.py:483] Algo bellman_ford step 376 current loss 0.102150, current_train_items 12064.
I0304 19:28:09.287504 23128000471168 run.py:483] Algo bellman_ford step 377 current loss 0.269699, current_train_items 12096.
I0304 19:28:09.317089 23128000471168 run.py:483] Algo bellman_ford step 378 current loss 0.230718, current_train_items 12128.
I0304 19:28:09.350212 23128000471168 run.py:483] Algo bellman_ford step 379 current loss 0.305454, current_train_items 12160.
I0304 19:28:09.369016 23128000471168 run.py:483] Algo bellman_ford step 380 current loss 0.027849, current_train_items 12192.
I0304 19:28:09.386038 23128000471168 run.py:483] Algo bellman_ford step 381 current loss 0.157780, current_train_items 12224.
I0304 19:28:09.410189 23128000471168 run.py:483] Algo bellman_ford step 382 current loss 0.370572, current_train_items 12256.
I0304 19:28:09.440642 23128000471168 run.py:483] Algo bellman_ford step 383 current loss 0.328026, current_train_items 12288.
I0304 19:28:09.474159 23128000471168 run.py:483] Algo bellman_ford step 384 current loss 0.318958, current_train_items 12320.
I0304 19:28:09.493608 23128000471168 run.py:483] Algo bellman_ford step 385 current loss 0.045867, current_train_items 12352.
I0304 19:28:09.510356 23128000471168 run.py:483] Algo bellman_ford step 386 current loss 0.127089, current_train_items 12384.
I0304 19:28:09.533622 23128000471168 run.py:483] Algo bellman_ford step 387 current loss 0.425664, current_train_items 12416.
I0304 19:28:09.563170 23128000471168 run.py:483] Algo bellman_ford step 388 current loss 0.489520, current_train_items 12448.
I0304 19:28:09.596030 23128000471168 run.py:483] Algo bellman_ford step 389 current loss 0.425404, current_train_items 12480.
I0304 19:28:09.615465 23128000471168 run.py:483] Algo bellman_ford step 390 current loss 0.029357, current_train_items 12512.
I0304 19:28:09.631815 23128000471168 run.py:483] Algo bellman_ford step 391 current loss 0.088969, current_train_items 12544.
I0304 19:28:09.655962 23128000471168 run.py:483] Algo bellman_ford step 392 current loss 0.331811, current_train_items 12576.
I0304 19:28:09.687676 23128000471168 run.py:483] Algo bellman_ford step 393 current loss 0.590302, current_train_items 12608.
I0304 19:28:09.719372 23128000471168 run.py:483] Algo bellman_ford step 394 current loss 0.558693, current_train_items 12640.
I0304 19:28:09.738214 23128000471168 run.py:483] Algo bellman_ford step 395 current loss 0.029924, current_train_items 12672.
I0304 19:28:09.754711 23128000471168 run.py:483] Algo bellman_ford step 396 current loss 0.107839, current_train_items 12704.
I0304 19:28:09.779329 23128000471168 run.py:483] Algo bellman_ford step 397 current loss 0.277698, current_train_items 12736.
I0304 19:28:09.810383 23128000471168 run.py:483] Algo bellman_ford step 398 current loss 0.289886, current_train_items 12768.
I0304 19:28:09.840776 23128000471168 run.py:483] Algo bellman_ford step 399 current loss 0.300040, current_train_items 12800.
I0304 19:28:09.860028 23128000471168 run.py:483] Algo bellman_ford step 400 current loss 0.016390, current_train_items 12832.
I0304 19:28:09.867829 23128000471168 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0304 19:28:09.867969 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.935, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0304 19:28:09.901653 23128000471168 run.py:483] Algo bellman_ford step 401 current loss 0.112985, current_train_items 12864.
I0304 19:28:09.926187 23128000471168 run.py:483] Algo bellman_ford step 402 current loss 0.191851, current_train_items 12896.
I0304 19:28:09.957244 23128000471168 run.py:483] Algo bellman_ford step 403 current loss 0.252467, current_train_items 12928.
I0304 19:28:09.988648 23128000471168 run.py:483] Algo bellman_ford step 404 current loss 0.276331, current_train_items 12960.
I0304 19:28:10.008220 23128000471168 run.py:483] Algo bellman_ford step 405 current loss 0.066461, current_train_items 12992.
I0304 19:28:10.023784 23128000471168 run.py:483] Algo bellman_ford step 406 current loss 0.103368, current_train_items 13024.
I0304 19:28:10.047814 23128000471168 run.py:483] Algo bellman_ford step 407 current loss 0.238369, current_train_items 13056.
I0304 19:28:10.078961 23128000471168 run.py:483] Algo bellman_ford step 408 current loss 0.277922, current_train_items 13088.
I0304 19:28:10.111263 23128000471168 run.py:483] Algo bellman_ford step 409 current loss 0.243178, current_train_items 13120.
I0304 19:28:10.130552 23128000471168 run.py:483] Algo bellman_ford step 410 current loss 0.040337, current_train_items 13152.
I0304 19:28:10.146841 23128000471168 run.py:483] Algo bellman_ford step 411 current loss 0.057426, current_train_items 13184.
I0304 19:28:10.170920 23128000471168 run.py:483] Algo bellman_ford step 412 current loss 0.184899, current_train_items 13216.
I0304 19:28:10.201291 23128000471168 run.py:483] Algo bellman_ford step 413 current loss 0.213127, current_train_items 13248.
I0304 19:28:10.233973 23128000471168 run.py:483] Algo bellman_ford step 414 current loss 0.261242, current_train_items 13280.
I0304 19:28:10.253519 23128000471168 run.py:483] Algo bellman_ford step 415 current loss 0.042426, current_train_items 13312.
I0304 19:28:10.269781 23128000471168 run.py:483] Algo bellman_ford step 416 current loss 0.047601, current_train_items 13344.
I0304 19:28:10.294358 23128000471168 run.py:483] Algo bellman_ford step 417 current loss 0.177743, current_train_items 13376.
I0304 19:28:10.326384 23128000471168 run.py:483] Algo bellman_ford step 418 current loss 0.334535, current_train_items 13408.
I0304 19:28:10.358832 23128000471168 run.py:483] Algo bellman_ford step 419 current loss 0.344489, current_train_items 13440.
I0304 19:28:10.377685 23128000471168 run.py:483] Algo bellman_ford step 420 current loss 0.062127, current_train_items 13472.
I0304 19:28:10.394010 23128000471168 run.py:483] Algo bellman_ford step 421 current loss 0.090323, current_train_items 13504.
I0304 19:28:10.418508 23128000471168 run.py:483] Algo bellman_ford step 422 current loss 0.162637, current_train_items 13536.
I0304 19:28:10.448215 23128000471168 run.py:483] Algo bellman_ford step 423 current loss 0.233423, current_train_items 13568.
I0304 19:28:10.480306 23128000471168 run.py:483] Algo bellman_ford step 424 current loss 0.295517, current_train_items 13600.
I0304 19:28:10.499120 23128000471168 run.py:483] Algo bellman_ford step 425 current loss 0.081892, current_train_items 13632.
I0304 19:28:10.515220 23128000471168 run.py:483] Algo bellman_ford step 426 current loss 0.045919, current_train_items 13664.
I0304 19:28:10.538719 23128000471168 run.py:483] Algo bellman_ford step 427 current loss 0.251598, current_train_items 13696.
I0304 19:28:10.568259 23128000471168 run.py:483] Algo bellman_ford step 428 current loss 0.274622, current_train_items 13728.
I0304 19:28:10.600084 23128000471168 run.py:483] Algo bellman_ford step 429 current loss 0.250043, current_train_items 13760.
I0304 19:28:10.619009 23128000471168 run.py:483] Algo bellman_ford step 430 current loss 0.051099, current_train_items 13792.
I0304 19:28:10.635407 23128000471168 run.py:483] Algo bellman_ford step 431 current loss 0.093305, current_train_items 13824.
I0304 19:28:10.660028 23128000471168 run.py:483] Algo bellman_ford step 432 current loss 0.286153, current_train_items 13856.
I0304 19:28:10.688823 23128000471168 run.py:483] Algo bellman_ford step 433 current loss 0.197074, current_train_items 13888.
I0304 19:28:10.720435 23128000471168 run.py:483] Algo bellman_ford step 434 current loss 0.272385, current_train_items 13920.
I0304 19:28:10.739167 23128000471168 run.py:483] Algo bellman_ford step 435 current loss 0.029188, current_train_items 13952.
I0304 19:28:10.756174 23128000471168 run.py:483] Algo bellman_ford step 436 current loss 0.092020, current_train_items 13984.
I0304 19:28:10.779266 23128000471168 run.py:483] Algo bellman_ford step 437 current loss 0.180808, current_train_items 14016.
I0304 19:28:10.808480 23128000471168 run.py:483] Algo bellman_ford step 438 current loss 0.148372, current_train_items 14048.
I0304 19:28:10.840513 23128000471168 run.py:483] Algo bellman_ford step 439 current loss 0.237029, current_train_items 14080.
I0304 19:28:10.859556 23128000471168 run.py:483] Algo bellman_ford step 440 current loss 0.042860, current_train_items 14112.
I0304 19:28:10.875774 23128000471168 run.py:483] Algo bellman_ford step 441 current loss 0.091425, current_train_items 14144.
I0304 19:28:10.899155 23128000471168 run.py:483] Algo bellman_ford step 442 current loss 0.154657, current_train_items 14176.
I0304 19:28:10.928992 23128000471168 run.py:483] Algo bellman_ford step 443 current loss 0.239614, current_train_items 14208.
I0304 19:28:10.960086 23128000471168 run.py:483] Algo bellman_ford step 444 current loss 0.196642, current_train_items 14240.
I0304 19:28:10.978996 23128000471168 run.py:483] Algo bellman_ford step 445 current loss 0.030994, current_train_items 14272.
I0304 19:28:10.996280 23128000471168 run.py:483] Algo bellman_ford step 446 current loss 0.167624, current_train_items 14304.
I0304 19:28:11.020571 23128000471168 run.py:483] Algo bellman_ford step 447 current loss 0.180333, current_train_items 14336.
I0304 19:28:11.048928 23128000471168 run.py:483] Algo bellman_ford step 448 current loss 0.249744, current_train_items 14368.
I0304 19:28:11.079308 23128000471168 run.py:483] Algo bellman_ford step 449 current loss 0.241115, current_train_items 14400.
I0304 19:28:11.098201 23128000471168 run.py:483] Algo bellman_ford step 450 current loss 0.040279, current_train_items 14432.
I0304 19:28:11.106582 23128000471168 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0304 19:28:11.106691 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.945, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0304 19:28:11.136068 23128000471168 run.py:483] Algo bellman_ford step 451 current loss 0.092280, current_train_items 14464.
I0304 19:28:11.159479 23128000471168 run.py:483] Algo bellman_ford step 452 current loss 0.118701, current_train_items 14496.
I0304 19:28:11.190884 23128000471168 run.py:483] Algo bellman_ford step 453 current loss 0.220392, current_train_items 14528.
I0304 19:28:11.223306 23128000471168 run.py:483] Algo bellman_ford step 454 current loss 0.252610, current_train_items 14560.
I0304 19:28:11.242429 23128000471168 run.py:483] Algo bellman_ford step 455 current loss 0.011952, current_train_items 14592.
I0304 19:28:11.258975 23128000471168 run.py:483] Algo bellman_ford step 456 current loss 0.180359, current_train_items 14624.
I0304 19:28:11.282245 23128000471168 run.py:483] Algo bellman_ford step 457 current loss 0.185043, current_train_items 14656.
I0304 19:28:11.311470 23128000471168 run.py:483] Algo bellman_ford step 458 current loss 0.140885, current_train_items 14688.
I0304 19:28:11.342163 23128000471168 run.py:483] Algo bellman_ford step 459 current loss 0.231221, current_train_items 14720.
I0304 19:28:11.361181 23128000471168 run.py:483] Algo bellman_ford step 460 current loss 0.017893, current_train_items 14752.
I0304 19:28:11.377894 23128000471168 run.py:483] Algo bellman_ford step 461 current loss 0.083587, current_train_items 14784.
I0304 19:28:11.401779 23128000471168 run.py:483] Algo bellman_ford step 462 current loss 0.147688, current_train_items 14816.
I0304 19:28:11.432266 23128000471168 run.py:483] Algo bellman_ford step 463 current loss 0.195822, current_train_items 14848.
I0304 19:28:11.465190 23128000471168 run.py:483] Algo bellman_ford step 464 current loss 0.246987, current_train_items 14880.
I0304 19:28:11.483912 23128000471168 run.py:483] Algo bellman_ford step 465 current loss 0.027214, current_train_items 14912.
I0304 19:28:11.499858 23128000471168 run.py:483] Algo bellman_ford step 466 current loss 0.100611, current_train_items 14944.
I0304 19:28:11.523855 23128000471168 run.py:483] Algo bellman_ford step 467 current loss 0.200650, current_train_items 14976.
I0304 19:28:11.552309 23128000471168 run.py:483] Algo bellman_ford step 468 current loss 0.107317, current_train_items 15008.
I0304 19:28:11.585500 23128000471168 run.py:483] Algo bellman_ford step 469 current loss 0.306646, current_train_items 15040.
I0304 19:28:11.604422 23128000471168 run.py:483] Algo bellman_ford step 470 current loss 0.023558, current_train_items 15072.
I0304 19:28:11.620965 23128000471168 run.py:483] Algo bellman_ford step 471 current loss 0.058045, current_train_items 15104.
I0304 19:28:11.644786 23128000471168 run.py:483] Algo bellman_ford step 472 current loss 0.214823, current_train_items 15136.
I0304 19:28:11.674885 23128000471168 run.py:483] Algo bellman_ford step 473 current loss 0.258852, current_train_items 15168.
I0304 19:28:11.708291 23128000471168 run.py:483] Algo bellman_ford step 474 current loss 0.334932, current_train_items 15200.
I0304 19:28:11.727526 23128000471168 run.py:483] Algo bellman_ford step 475 current loss 0.016585, current_train_items 15232.
I0304 19:28:11.743741 23128000471168 run.py:483] Algo bellman_ford step 476 current loss 0.107185, current_train_items 15264.
I0304 19:28:11.768063 23128000471168 run.py:483] Algo bellman_ford step 477 current loss 0.201589, current_train_items 15296.
I0304 19:28:11.796366 23128000471168 run.py:483] Algo bellman_ford step 478 current loss 0.130720, current_train_items 15328.
I0304 19:28:11.829112 23128000471168 run.py:483] Algo bellman_ford step 479 current loss 0.359294, current_train_items 15360.
I0304 19:28:11.847743 23128000471168 run.py:483] Algo bellman_ford step 480 current loss 0.039524, current_train_items 15392.
I0304 19:28:11.864014 23128000471168 run.py:483] Algo bellman_ford step 481 current loss 0.095424, current_train_items 15424.
I0304 19:28:11.887193 23128000471168 run.py:483] Algo bellman_ford step 482 current loss 0.143275, current_train_items 15456.
I0304 19:28:11.917063 23128000471168 run.py:483] Algo bellman_ford step 483 current loss 0.154005, current_train_items 15488.
I0304 19:28:11.949396 23128000471168 run.py:483] Algo bellman_ford step 484 current loss 0.259850, current_train_items 15520.
I0304 19:28:11.968416 23128000471168 run.py:483] Algo bellman_ford step 485 current loss 0.019172, current_train_items 15552.
I0304 19:28:11.984975 23128000471168 run.py:483] Algo bellman_ford step 486 current loss 0.088919, current_train_items 15584.
I0304 19:28:12.008431 23128000471168 run.py:483] Algo bellman_ford step 487 current loss 0.142588, current_train_items 15616.
I0304 19:28:12.037889 23128000471168 run.py:483] Algo bellman_ford step 488 current loss 0.148748, current_train_items 15648.
I0304 19:28:12.068930 23128000471168 run.py:483] Algo bellman_ford step 489 current loss 0.211680, current_train_items 15680.
I0304 19:28:12.087963 23128000471168 run.py:483] Algo bellman_ford step 490 current loss 0.023435, current_train_items 15712.
I0304 19:28:12.104561 23128000471168 run.py:483] Algo bellman_ford step 491 current loss 0.046689, current_train_items 15744.
I0304 19:28:12.127918 23128000471168 run.py:483] Algo bellman_ford step 492 current loss 0.136289, current_train_items 15776.
I0304 19:28:12.157189 23128000471168 run.py:483] Algo bellman_ford step 493 current loss 0.150854, current_train_items 15808.
I0304 19:28:12.190230 23128000471168 run.py:483] Algo bellman_ford step 494 current loss 0.220903, current_train_items 15840.
I0304 19:28:12.208867 23128000471168 run.py:483] Algo bellman_ford step 495 current loss 0.023242, current_train_items 15872.
I0304 19:28:12.224925 23128000471168 run.py:483] Algo bellman_ford step 496 current loss 0.093801, current_train_items 15904.
I0304 19:28:12.249783 23128000471168 run.py:483] Algo bellman_ford step 497 current loss 0.196238, current_train_items 15936.
I0304 19:28:12.280236 23128000471168 run.py:483] Algo bellman_ford step 498 current loss 0.168599, current_train_items 15968.
I0304 19:28:12.311933 23128000471168 run.py:483] Algo bellman_ford step 499 current loss 0.182991, current_train_items 16000.
I0304 19:28:12.331487 23128000471168 run.py:483] Algo bellman_ford step 500 current loss 0.014767, current_train_items 16032.
I0304 19:28:12.339082 23128000471168 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0304 19:28:12.339188 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.947, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0304 19:28:12.355656 23128000471168 run.py:483] Algo bellman_ford step 501 current loss 0.053213, current_train_items 16064.
I0304 19:28:12.380190 23128000471168 run.py:483] Algo bellman_ford step 502 current loss 0.189201, current_train_items 16096.
I0304 19:28:12.411726 23128000471168 run.py:483] Algo bellman_ford step 503 current loss 0.255166, current_train_items 16128.
I0304 19:28:12.446382 23128000471168 run.py:483] Algo bellman_ford step 504 current loss 0.218838, current_train_items 16160.
I0304 19:28:12.465345 23128000471168 run.py:483] Algo bellman_ford step 505 current loss 0.014994, current_train_items 16192.
I0304 19:28:12.480849 23128000471168 run.py:483] Algo bellman_ford step 506 current loss 0.034292, current_train_items 16224.
I0304 19:28:12.503831 23128000471168 run.py:483] Algo bellman_ford step 507 current loss 0.083972, current_train_items 16256.
I0304 19:28:12.534240 23128000471168 run.py:483] Algo bellman_ford step 508 current loss 0.167857, current_train_items 16288.
I0304 19:28:12.566587 23128000471168 run.py:483] Algo bellman_ford step 509 current loss 0.230991, current_train_items 16320.
I0304 19:28:12.585726 23128000471168 run.py:483] Algo bellman_ford step 510 current loss 0.016214, current_train_items 16352.
I0304 19:28:12.602160 23128000471168 run.py:483] Algo bellman_ford step 511 current loss 0.070176, current_train_items 16384.
I0304 19:28:12.625799 23128000471168 run.py:483] Algo bellman_ford step 512 current loss 0.142103, current_train_items 16416.
I0304 19:28:12.655887 23128000471168 run.py:483] Algo bellman_ford step 513 current loss 0.195169, current_train_items 16448.
I0304 19:28:12.688326 23128000471168 run.py:483] Algo bellman_ford step 514 current loss 0.190513, current_train_items 16480.
I0304 19:28:12.706893 23128000471168 run.py:483] Algo bellman_ford step 515 current loss 0.008700, current_train_items 16512.
I0304 19:28:12.723000 23128000471168 run.py:483] Algo bellman_ford step 516 current loss 0.109373, current_train_items 16544.
I0304 19:28:12.747040 23128000471168 run.py:483] Algo bellman_ford step 517 current loss 0.133676, current_train_items 16576.
I0304 19:28:12.777863 23128000471168 run.py:483] Algo bellman_ford step 518 current loss 0.157620, current_train_items 16608.
I0304 19:28:12.809789 23128000471168 run.py:483] Algo bellman_ford step 519 current loss 0.169227, current_train_items 16640.
I0304 19:28:12.828528 23128000471168 run.py:483] Algo bellman_ford step 520 current loss 0.027743, current_train_items 16672.
I0304 19:28:12.845293 23128000471168 run.py:483] Algo bellman_ford step 521 current loss 0.120286, current_train_items 16704.
I0304 19:28:12.868692 23128000471168 run.py:483] Algo bellman_ford step 522 current loss 0.106025, current_train_items 16736.
I0304 19:28:12.899759 23128000471168 run.py:483] Algo bellman_ford step 523 current loss 0.131876, current_train_items 16768.
I0304 19:28:12.930975 23128000471168 run.py:483] Algo bellman_ford step 524 current loss 0.258440, current_train_items 16800.
I0304 19:28:12.949443 23128000471168 run.py:483] Algo bellman_ford step 525 current loss 0.022715, current_train_items 16832.
I0304 19:28:12.965955 23128000471168 run.py:483] Algo bellman_ford step 526 current loss 0.075622, current_train_items 16864.
I0304 19:28:12.989502 23128000471168 run.py:483] Algo bellman_ford step 527 current loss 0.120233, current_train_items 16896.
I0304 19:28:13.018518 23128000471168 run.py:483] Algo bellman_ford step 528 current loss 0.148262, current_train_items 16928.
I0304 19:28:13.050975 23128000471168 run.py:483] Algo bellman_ford step 529 current loss 0.260348, current_train_items 16960.
I0304 19:28:13.069819 23128000471168 run.py:483] Algo bellman_ford step 530 current loss 0.017738, current_train_items 16992.
I0304 19:28:13.086073 23128000471168 run.py:483] Algo bellman_ford step 531 current loss 0.039546, current_train_items 17024.
I0304 19:28:13.109060 23128000471168 run.py:483] Algo bellman_ford step 532 current loss 0.097122, current_train_items 17056.
I0304 19:28:13.139411 23128000471168 run.py:483] Algo bellman_ford step 533 current loss 0.188763, current_train_items 17088.
I0304 19:28:13.171798 23128000471168 run.py:483] Algo bellman_ford step 534 current loss 0.178592, current_train_items 17120.
I0304 19:28:13.190578 23128000471168 run.py:483] Algo bellman_ford step 535 current loss 0.009244, current_train_items 17152.
I0304 19:28:13.206460 23128000471168 run.py:483] Algo bellman_ford step 536 current loss 0.032737, current_train_items 17184.
I0304 19:28:13.229154 23128000471168 run.py:483] Algo bellman_ford step 537 current loss 0.085803, current_train_items 17216.
I0304 19:28:13.258347 23128000471168 run.py:483] Algo bellman_ford step 538 current loss 0.107095, current_train_items 17248.
I0304 19:28:13.289169 23128000471168 run.py:483] Algo bellman_ford step 539 current loss 0.140913, current_train_items 17280.
I0304 19:28:13.307734 23128000471168 run.py:483] Algo bellman_ford step 540 current loss 0.035852, current_train_items 17312.
I0304 19:28:13.323535 23128000471168 run.py:483] Algo bellman_ford step 541 current loss 0.054893, current_train_items 17344.
I0304 19:28:13.347861 23128000471168 run.py:483] Algo bellman_ford step 542 current loss 0.217403, current_train_items 17376.
I0304 19:28:13.377964 23128000471168 run.py:483] Algo bellman_ford step 543 current loss 0.240104, current_train_items 17408.
I0304 19:28:13.410464 23128000471168 run.py:483] Algo bellman_ford step 544 current loss 0.302133, current_train_items 17440.
I0304 19:28:13.429270 23128000471168 run.py:483] Algo bellman_ford step 545 current loss 0.009515, current_train_items 17472.
I0304 19:28:13.445733 23128000471168 run.py:483] Algo bellman_ford step 546 current loss 0.068087, current_train_items 17504.
I0304 19:28:13.469268 23128000471168 run.py:483] Algo bellman_ford step 547 current loss 0.172645, current_train_items 17536.
I0304 19:28:13.499676 23128000471168 run.py:483] Algo bellman_ford step 548 current loss 0.283397, current_train_items 17568.
I0304 19:28:13.529405 23128000471168 run.py:483] Algo bellman_ford step 549 current loss 0.179082, current_train_items 17600.
I0304 19:28:13.548300 23128000471168 run.py:483] Algo bellman_ford step 550 current loss 0.014653, current_train_items 17632.
I0304 19:28:13.556239 23128000471168 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0304 19:28:13.556349 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.947, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0304 19:28:13.585047 23128000471168 run.py:483] Algo bellman_ford step 551 current loss 0.033242, current_train_items 17664.
I0304 19:28:13.609043 23128000471168 run.py:483] Algo bellman_ford step 552 current loss 0.209373, current_train_items 17696.
I0304 19:28:13.641423 23128000471168 run.py:483] Algo bellman_ford step 553 current loss 0.529009, current_train_items 17728.
I0304 19:28:13.673877 23128000471168 run.py:483] Algo bellman_ford step 554 current loss 0.336824, current_train_items 17760.
I0304 19:28:13.693328 23128000471168 run.py:483] Algo bellman_ford step 555 current loss 0.048241, current_train_items 17792.
I0304 19:28:13.709410 23128000471168 run.py:483] Algo bellman_ford step 556 current loss 0.113427, current_train_items 17824.
I0304 19:28:13.734485 23128000471168 run.py:483] Algo bellman_ford step 557 current loss 0.517377, current_train_items 17856.
I0304 19:28:13.764241 23128000471168 run.py:483] Algo bellman_ford step 558 current loss 0.492345, current_train_items 17888.
I0304 19:28:13.797963 23128000471168 run.py:483] Algo bellman_ford step 559 current loss 0.373068, current_train_items 17920.
I0304 19:28:13.817614 23128000471168 run.py:483] Algo bellman_ford step 560 current loss 0.020726, current_train_items 17952.
I0304 19:28:13.834146 23128000471168 run.py:483] Algo bellman_ford step 561 current loss 0.102122, current_train_items 17984.
I0304 19:28:13.858232 23128000471168 run.py:483] Algo bellman_ford step 562 current loss 0.236205, current_train_items 18016.
I0304 19:28:13.887758 23128000471168 run.py:483] Algo bellman_ford step 563 current loss 0.233487, current_train_items 18048.
I0304 19:28:13.918056 23128000471168 run.py:483] Algo bellman_ford step 564 current loss 0.363353, current_train_items 18080.
I0304 19:28:13.937314 23128000471168 run.py:483] Algo bellman_ford step 565 current loss 0.021407, current_train_items 18112.
I0304 19:28:13.953867 23128000471168 run.py:483] Algo bellman_ford step 566 current loss 0.075090, current_train_items 18144.
I0304 19:28:13.978801 23128000471168 run.py:483] Algo bellman_ford step 567 current loss 0.213358, current_train_items 18176.
I0304 19:28:14.009178 23128000471168 run.py:483] Algo bellman_ford step 568 current loss 0.295680, current_train_items 18208.
I0304 19:28:14.040704 23128000471168 run.py:483] Algo bellman_ford step 569 current loss 0.224838, current_train_items 18240.
I0304 19:28:14.060033 23128000471168 run.py:483] Algo bellman_ford step 570 current loss 0.050811, current_train_items 18272.
I0304 19:28:14.076288 23128000471168 run.py:483] Algo bellman_ford step 571 current loss 0.132062, current_train_items 18304.
I0304 19:28:14.100536 23128000471168 run.py:483] Algo bellman_ford step 572 current loss 0.215770, current_train_items 18336.
I0304 19:28:14.131850 23128000471168 run.py:483] Algo bellman_ford step 573 current loss 0.267074, current_train_items 18368.
I0304 19:28:14.163083 23128000471168 run.py:483] Algo bellman_ford step 574 current loss 0.153199, current_train_items 18400.
I0304 19:28:14.182531 23128000471168 run.py:483] Algo bellman_ford step 575 current loss 0.025752, current_train_items 18432.
I0304 19:28:14.198799 23128000471168 run.py:483] Algo bellman_ford step 576 current loss 0.069483, current_train_items 18464.
I0304 19:28:14.222883 23128000471168 run.py:483] Algo bellman_ford step 577 current loss 0.199580, current_train_items 18496.
I0304 19:28:14.253491 23128000471168 run.py:483] Algo bellman_ford step 578 current loss 0.251861, current_train_items 18528.
I0304 19:28:14.286557 23128000471168 run.py:483] Algo bellman_ford step 579 current loss 0.186184, current_train_items 18560.
I0304 19:28:14.305866 23128000471168 run.py:483] Algo bellman_ford step 580 current loss 0.040115, current_train_items 18592.
I0304 19:28:14.322080 23128000471168 run.py:483] Algo bellman_ford step 581 current loss 0.077644, current_train_items 18624.
I0304 19:28:14.344588 23128000471168 run.py:483] Algo bellman_ford step 582 current loss 0.254906, current_train_items 18656.
I0304 19:28:14.373753 23128000471168 run.py:483] Algo bellman_ford step 583 current loss 0.191062, current_train_items 18688.
I0304 19:28:14.405517 23128000471168 run.py:483] Algo bellman_ford step 584 current loss 0.190550, current_train_items 18720.
I0304 19:28:14.424554 23128000471168 run.py:483] Algo bellman_ford step 585 current loss 0.058032, current_train_items 18752.
I0304 19:28:14.441047 23128000471168 run.py:483] Algo bellman_ford step 586 current loss 0.132476, current_train_items 18784.
I0304 19:28:14.464001 23128000471168 run.py:483] Algo bellman_ford step 587 current loss 0.141181, current_train_items 18816.
I0304 19:28:14.493422 23128000471168 run.py:483] Algo bellman_ford step 588 current loss 0.268014, current_train_items 18848.
I0304 19:28:14.526712 23128000471168 run.py:483] Algo bellman_ford step 589 current loss 0.335453, current_train_items 18880.
I0304 19:28:14.546021 23128000471168 run.py:483] Algo bellman_ford step 590 current loss 0.040951, current_train_items 18912.
I0304 19:28:14.562573 23128000471168 run.py:483] Algo bellman_ford step 591 current loss 0.079058, current_train_items 18944.
I0304 19:28:14.586898 23128000471168 run.py:483] Algo bellman_ford step 592 current loss 0.397999, current_train_items 18976.
I0304 19:28:14.617062 23128000471168 run.py:483] Algo bellman_ford step 593 current loss 0.325441, current_train_items 19008.
I0304 19:28:14.648628 23128000471168 run.py:483] Algo bellman_ford step 594 current loss 0.298771, current_train_items 19040.
I0304 19:28:14.667632 23128000471168 run.py:483] Algo bellman_ford step 595 current loss 0.023615, current_train_items 19072.
I0304 19:28:14.684205 23128000471168 run.py:483] Algo bellman_ford step 596 current loss 0.052890, current_train_items 19104.
I0304 19:28:14.708788 23128000471168 run.py:483] Algo bellman_ford step 597 current loss 0.252699, current_train_items 19136.
I0304 19:28:14.738743 23128000471168 run.py:483] Algo bellman_ford step 598 current loss 0.181888, current_train_items 19168.
I0304 19:28:14.770944 23128000471168 run.py:483] Algo bellman_ford step 599 current loss 0.227844, current_train_items 19200.
I0304 19:28:14.790386 23128000471168 run.py:483] Algo bellman_ford step 600 current loss 0.048628, current_train_items 19232.
I0304 19:28:14.798389 23128000471168 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0304 19:28:14.798498 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.949, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0304 19:28:14.815454 23128000471168 run.py:483] Algo bellman_ford step 601 current loss 0.101511, current_train_items 19264.
I0304 19:28:14.839562 23128000471168 run.py:483] Algo bellman_ford step 602 current loss 0.274037, current_train_items 19296.
I0304 19:28:14.869371 23128000471168 run.py:483] Algo bellman_ford step 603 current loss 0.206795, current_train_items 19328.
I0304 19:28:14.900889 23128000471168 run.py:483] Algo bellman_ford step 604 current loss 0.206418, current_train_items 19360.
I0304 19:28:14.920348 23128000471168 run.py:483] Algo bellman_ford step 605 current loss 0.044076, current_train_items 19392.
I0304 19:28:14.936167 23128000471168 run.py:483] Algo bellman_ford step 606 current loss 0.053691, current_train_items 19424.
I0304 19:28:14.960972 23128000471168 run.py:483] Algo bellman_ford step 607 current loss 0.263928, current_train_items 19456.
I0304 19:28:14.990150 23128000471168 run.py:483] Algo bellman_ford step 608 current loss 0.185018, current_train_items 19488.
I0304 19:28:15.022617 23128000471168 run.py:483] Algo bellman_ford step 609 current loss 0.255985, current_train_items 19520.
I0304 19:28:15.041409 23128000471168 run.py:483] Algo bellman_ford step 610 current loss 0.041528, current_train_items 19552.
I0304 19:28:15.058135 23128000471168 run.py:483] Algo bellman_ford step 611 current loss 0.046515, current_train_items 19584.
I0304 19:28:15.082373 23128000471168 run.py:483] Algo bellman_ford step 612 current loss 0.162335, current_train_items 19616.
I0304 19:28:15.110704 23128000471168 run.py:483] Algo bellman_ford step 613 current loss 0.172020, current_train_items 19648.
I0304 19:28:15.142204 23128000471168 run.py:483] Algo bellman_ford step 614 current loss 0.218033, current_train_items 19680.
I0304 19:28:15.161000 23128000471168 run.py:483] Algo bellman_ford step 615 current loss 0.020344, current_train_items 19712.
I0304 19:28:15.177809 23128000471168 run.py:483] Algo bellman_ford step 616 current loss 0.060955, current_train_items 19744.
I0304 19:28:15.201220 23128000471168 run.py:483] Algo bellman_ford step 617 current loss 0.176651, current_train_items 19776.
I0304 19:28:15.230655 23128000471168 run.py:483] Algo bellman_ford step 618 current loss 0.147948, current_train_items 19808.
I0304 19:28:15.263020 23128000471168 run.py:483] Algo bellman_ford step 619 current loss 0.207439, current_train_items 19840.
I0304 19:28:15.282085 23128000471168 run.py:483] Algo bellman_ford step 620 current loss 0.072297, current_train_items 19872.
I0304 19:28:15.298975 23128000471168 run.py:483] Algo bellman_ford step 621 current loss 0.141357, current_train_items 19904.
I0304 19:28:15.322969 23128000471168 run.py:483] Algo bellman_ford step 622 current loss 0.168208, current_train_items 19936.
I0304 19:28:15.352525 23128000471168 run.py:483] Algo bellman_ford step 623 current loss 0.218070, current_train_items 19968.
I0304 19:28:15.384207 23128000471168 run.py:483] Algo bellman_ford step 624 current loss 0.238029, current_train_items 20000.
I0304 19:28:15.403373 23128000471168 run.py:483] Algo bellman_ford step 625 current loss 0.040895, current_train_items 20032.
I0304 19:28:15.419466 23128000471168 run.py:483] Algo bellman_ford step 626 current loss 0.093774, current_train_items 20064.
I0304 19:28:15.444019 23128000471168 run.py:483] Algo bellman_ford step 627 current loss 0.221898, current_train_items 20096.
I0304 19:28:15.474646 23128000471168 run.py:483] Algo bellman_ford step 628 current loss 0.356210, current_train_items 20128.
I0304 19:28:15.507210 23128000471168 run.py:483] Algo bellman_ford step 629 current loss 0.289037, current_train_items 20160.
I0304 19:28:15.526257 23128000471168 run.py:483] Algo bellman_ford step 630 current loss 0.018799, current_train_items 20192.
I0304 19:28:15.542366 23128000471168 run.py:483] Algo bellman_ford step 631 current loss 0.108728, current_train_items 20224.
I0304 19:28:15.565841 23128000471168 run.py:483] Algo bellman_ford step 632 current loss 0.277456, current_train_items 20256.
I0304 19:28:15.595381 23128000471168 run.py:483] Algo bellman_ford step 633 current loss 0.318062, current_train_items 20288.
I0304 19:28:15.627251 23128000471168 run.py:483] Algo bellman_ford step 634 current loss 0.260039, current_train_items 20320.
I0304 19:28:15.646584 23128000471168 run.py:483] Algo bellman_ford step 635 current loss 0.033551, current_train_items 20352.
I0304 19:28:15.662844 23128000471168 run.py:483] Algo bellman_ford step 636 current loss 0.163147, current_train_items 20384.
I0304 19:28:15.686836 23128000471168 run.py:483] Algo bellman_ford step 637 current loss 0.153818, current_train_items 20416.
I0304 19:28:15.717744 23128000471168 run.py:483] Algo bellman_ford step 638 current loss 0.245162, current_train_items 20448.
I0304 19:28:15.747260 23128000471168 run.py:483] Algo bellman_ford step 639 current loss 0.154704, current_train_items 20480.
I0304 19:28:15.766113 23128000471168 run.py:483] Algo bellman_ford step 640 current loss 0.018693, current_train_items 20512.
I0304 19:28:15.782294 23128000471168 run.py:483] Algo bellman_ford step 641 current loss 0.072990, current_train_items 20544.
I0304 19:28:15.805360 23128000471168 run.py:483] Algo bellman_ford step 642 current loss 0.182479, current_train_items 20576.
I0304 19:28:15.834939 23128000471168 run.py:483] Algo bellman_ford step 643 current loss 0.150387, current_train_items 20608.
I0304 19:28:15.867249 23128000471168 run.py:483] Algo bellman_ford step 644 current loss 0.232329, current_train_items 20640.
I0304 19:28:15.886257 23128000471168 run.py:483] Algo bellman_ford step 645 current loss 0.028430, current_train_items 20672.
I0304 19:28:15.903667 23128000471168 run.py:483] Algo bellman_ford step 646 current loss 0.099296, current_train_items 20704.
I0304 19:28:15.927053 23128000471168 run.py:483] Algo bellman_ford step 647 current loss 0.099382, current_train_items 20736.
I0304 19:28:15.957721 23128000471168 run.py:483] Algo bellman_ford step 648 current loss 0.206084, current_train_items 20768.
I0304 19:28:15.990289 23128000471168 run.py:483] Algo bellman_ford step 649 current loss 0.201871, current_train_items 20800.
I0304 19:28:16.009423 23128000471168 run.py:483] Algo bellman_ford step 650 current loss 0.072428, current_train_items 20832.
I0304 19:28:16.017742 23128000471168 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.94140625, 'score': 0.94140625, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0304 19:28:16.017850 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.949, current avg val score is 0.941, val scores are: bellman_ford: 0.941
I0304 19:28:16.035264 23128000471168 run.py:483] Algo bellman_ford step 651 current loss 0.065305, current_train_items 20864.
I0304 19:28:16.058844 23128000471168 run.py:483] Algo bellman_ford step 652 current loss 0.144918, current_train_items 20896.
I0304 19:28:16.088139 23128000471168 run.py:483] Algo bellman_ford step 653 current loss 0.119960, current_train_items 20928.
I0304 19:28:16.119735 23128000471168 run.py:483] Algo bellman_ford step 654 current loss 0.182930, current_train_items 20960.
I0304 19:28:16.138865 23128000471168 run.py:483] Algo bellman_ford step 655 current loss 0.009772, current_train_items 20992.
I0304 19:28:16.154735 23128000471168 run.py:483] Algo bellman_ford step 656 current loss 0.087411, current_train_items 21024.
I0304 19:28:16.178825 23128000471168 run.py:483] Algo bellman_ford step 657 current loss 0.252269, current_train_items 21056.
I0304 19:28:16.209424 23128000471168 run.py:483] Algo bellman_ford step 658 current loss 0.259587, current_train_items 21088.
I0304 19:28:16.240647 23128000471168 run.py:483] Algo bellman_ford step 659 current loss 0.401719, current_train_items 21120.
I0304 19:28:16.259905 23128000471168 run.py:483] Algo bellman_ford step 660 current loss 0.052346, current_train_items 21152.
I0304 19:28:16.276153 23128000471168 run.py:483] Algo bellman_ford step 661 current loss 0.105931, current_train_items 21184.
I0304 19:28:16.299036 23128000471168 run.py:483] Algo bellman_ford step 662 current loss 0.085181, current_train_items 21216.
I0304 19:28:16.329102 23128000471168 run.py:483] Algo bellman_ford step 663 current loss 0.215694, current_train_items 21248.
I0304 19:28:16.363393 23128000471168 run.py:483] Algo bellman_ford step 664 current loss 0.260994, current_train_items 21280.
I0304 19:28:16.382392 23128000471168 run.py:483] Algo bellman_ford step 665 current loss 0.025255, current_train_items 21312.
I0304 19:28:16.398774 23128000471168 run.py:483] Algo bellman_ford step 666 current loss 0.064867, current_train_items 21344.
I0304 19:28:16.423041 23128000471168 run.py:483] Algo bellman_ford step 667 current loss 0.171851, current_train_items 21376.
I0304 19:28:16.452227 23128000471168 run.py:483] Algo bellman_ford step 668 current loss 0.180013, current_train_items 21408.
I0304 19:28:16.483179 23128000471168 run.py:483] Algo bellman_ford step 669 current loss 0.153300, current_train_items 21440.
I0304 19:28:16.502187 23128000471168 run.py:483] Algo bellman_ford step 670 current loss 0.012847, current_train_items 21472.
I0304 19:28:16.518331 23128000471168 run.py:483] Algo bellman_ford step 671 current loss 0.054405, current_train_items 21504.
I0304 19:28:16.542470 23128000471168 run.py:483] Algo bellman_ford step 672 current loss 0.124815, current_train_items 21536.
I0304 19:28:16.571621 23128000471168 run.py:483] Algo bellman_ford step 673 current loss 0.145275, current_train_items 21568.
I0304 19:28:16.604472 23128000471168 run.py:483] Algo bellman_ford step 674 current loss 0.218091, current_train_items 21600.
I0304 19:28:16.623630 23128000471168 run.py:483] Algo bellman_ford step 675 current loss 0.028757, current_train_items 21632.
I0304 19:28:16.639693 23128000471168 run.py:483] Algo bellman_ford step 676 current loss 0.072366, current_train_items 21664.
I0304 19:28:16.662716 23128000471168 run.py:483] Algo bellman_ford step 677 current loss 0.112108, current_train_items 21696.
I0304 19:28:16.692916 23128000471168 run.py:483] Algo bellman_ford step 678 current loss 0.176701, current_train_items 21728.
I0304 19:28:16.725439 23128000471168 run.py:483] Algo bellman_ford step 679 current loss 0.187265, current_train_items 21760.
I0304 19:28:16.744321 23128000471168 run.py:483] Algo bellman_ford step 680 current loss 0.030720, current_train_items 21792.
I0304 19:28:16.760727 23128000471168 run.py:483] Algo bellman_ford step 681 current loss 0.084459, current_train_items 21824.
I0304 19:28:16.784308 23128000471168 run.py:483] Algo bellman_ford step 682 current loss 0.151858, current_train_items 21856.
I0304 19:28:16.813306 23128000471168 run.py:483] Algo bellman_ford step 683 current loss 0.174262, current_train_items 21888.
I0304 19:28:16.845893 23128000471168 run.py:483] Algo bellman_ford step 684 current loss 0.190184, current_train_items 21920.
I0304 19:28:16.865137 23128000471168 run.py:483] Algo bellman_ford step 685 current loss 0.023566, current_train_items 21952.
I0304 19:28:16.881464 23128000471168 run.py:483] Algo bellman_ford step 686 current loss 0.086151, current_train_items 21984.
I0304 19:28:16.905225 23128000471168 run.py:483] Algo bellman_ford step 687 current loss 0.159709, current_train_items 22016.
I0304 19:28:16.933832 23128000471168 run.py:483] Algo bellman_ford step 688 current loss 0.139817, current_train_items 22048.
I0304 19:28:16.967526 23128000471168 run.py:483] Algo bellman_ford step 689 current loss 0.338580, current_train_items 22080.
I0304 19:28:16.986428 23128000471168 run.py:483] Algo bellman_ford step 690 current loss 0.057445, current_train_items 22112.
I0304 19:28:17.003465 23128000471168 run.py:483] Algo bellman_ford step 691 current loss 0.173289, current_train_items 22144.
I0304 19:28:17.027523 23128000471168 run.py:483] Algo bellman_ford step 692 current loss 0.180764, current_train_items 22176.
I0304 19:28:17.056612 23128000471168 run.py:483] Algo bellman_ford step 693 current loss 0.279976, current_train_items 22208.
I0304 19:28:17.089792 23128000471168 run.py:483] Algo bellman_ford step 694 current loss 0.293274, current_train_items 22240.
I0304 19:28:17.108686 23128000471168 run.py:483] Algo bellman_ford step 695 current loss 0.045522, current_train_items 22272.
I0304 19:28:17.124904 23128000471168 run.py:483] Algo bellman_ford step 696 current loss 0.082693, current_train_items 22304.
I0304 19:28:17.147226 23128000471168 run.py:483] Algo bellman_ford step 697 current loss 0.115595, current_train_items 22336.
I0304 19:28:17.176935 23128000471168 run.py:483] Algo bellman_ford step 698 current loss 0.139493, current_train_items 22368.
I0304 19:28:17.209941 23128000471168 run.py:483] Algo bellman_ford step 699 current loss 0.235177, current_train_items 22400.
I0304 19:28:17.229069 23128000471168 run.py:483] Algo bellman_ford step 700 current loss 0.015172, current_train_items 22432.
I0304 19:28:17.237138 23128000471168 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0304 19:28:17.237246 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.949, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:17.267255 23128000471168 run.py:483] Algo bellman_ford step 701 current loss 0.048258, current_train_items 22464.
I0304 19:28:17.291507 23128000471168 run.py:483] Algo bellman_ford step 702 current loss 0.221090, current_train_items 22496.
I0304 19:28:17.320463 23128000471168 run.py:483] Algo bellman_ford step 703 current loss 0.126535, current_train_items 22528.
I0304 19:28:17.352701 23128000471168 run.py:483] Algo bellman_ford step 704 current loss 0.148169, current_train_items 22560.
I0304 19:28:17.372256 23128000471168 run.py:483] Algo bellman_ford step 705 current loss 0.014438, current_train_items 22592.
I0304 19:28:17.388483 23128000471168 run.py:483] Algo bellman_ford step 706 current loss 0.056332, current_train_items 22624.
I0304 19:28:17.412428 23128000471168 run.py:483] Algo bellman_ford step 707 current loss 0.089373, current_train_items 22656.
I0304 19:28:17.442399 23128000471168 run.py:483] Algo bellman_ford step 708 current loss 0.148628, current_train_items 22688.
I0304 19:28:17.474885 23128000471168 run.py:483] Algo bellman_ford step 709 current loss 0.185640, current_train_items 22720.
I0304 19:28:17.494151 23128000471168 run.py:483] Algo bellman_ford step 710 current loss 0.011889, current_train_items 22752.
I0304 19:28:17.510589 23128000471168 run.py:483] Algo bellman_ford step 711 current loss 0.069892, current_train_items 22784.
I0304 19:28:17.535332 23128000471168 run.py:483] Algo bellman_ford step 712 current loss 0.198169, current_train_items 22816.
I0304 19:28:17.566303 23128000471168 run.py:483] Algo bellman_ford step 713 current loss 0.199353, current_train_items 22848.
I0304 19:28:17.597021 23128000471168 run.py:483] Algo bellman_ford step 714 current loss 0.169464, current_train_items 22880.
I0304 19:28:17.616459 23128000471168 run.py:483] Algo bellman_ford step 715 current loss 0.011876, current_train_items 22912.
I0304 19:28:17.633199 23128000471168 run.py:483] Algo bellman_ford step 716 current loss 0.187551, current_train_items 22944.
I0304 19:28:17.658339 23128000471168 run.py:483] Algo bellman_ford step 717 current loss 0.299146, current_train_items 22976.
I0304 19:28:17.687316 23128000471168 run.py:483] Algo bellman_ford step 718 current loss 0.104198, current_train_items 23008.
I0304 19:28:17.720012 23128000471168 run.py:483] Algo bellman_ford step 719 current loss 0.197971, current_train_items 23040.
I0304 19:28:17.738954 23128000471168 run.py:483] Algo bellman_ford step 720 current loss 0.044008, current_train_items 23072.
I0304 19:28:17.755824 23128000471168 run.py:483] Algo bellman_ford step 721 current loss 0.197273, current_train_items 23104.
I0304 19:28:17.779675 23128000471168 run.py:483] Algo bellman_ford step 722 current loss 0.144822, current_train_items 23136.
I0304 19:28:17.809237 23128000471168 run.py:483] Algo bellman_ford step 723 current loss 0.185421, current_train_items 23168.
I0304 19:28:17.840535 23128000471168 run.py:483] Algo bellman_ford step 724 current loss 0.198817, current_train_items 23200.
I0304 19:28:17.859637 23128000471168 run.py:483] Algo bellman_ford step 725 current loss 0.031696, current_train_items 23232.
I0304 19:28:17.876573 23128000471168 run.py:483] Algo bellman_ford step 726 current loss 0.151612, current_train_items 23264.
I0304 19:28:17.901064 23128000471168 run.py:483] Algo bellman_ford step 727 current loss 0.444007, current_train_items 23296.
I0304 19:28:17.930406 23128000471168 run.py:483] Algo bellman_ford step 728 current loss 0.306257, current_train_items 23328.
I0304 19:28:17.962269 23128000471168 run.py:483] Algo bellman_ford step 729 current loss 0.227939, current_train_items 23360.
I0304 19:28:17.981171 23128000471168 run.py:483] Algo bellman_ford step 730 current loss 0.013164, current_train_items 23392.
I0304 19:28:17.997703 23128000471168 run.py:483] Algo bellman_ford step 731 current loss 0.134551, current_train_items 23424.
I0304 19:28:18.021766 23128000471168 run.py:483] Algo bellman_ford step 732 current loss 0.520193, current_train_items 23456.
I0304 19:28:18.051996 23128000471168 run.py:483] Algo bellman_ford step 733 current loss 0.332098, current_train_items 23488.
I0304 19:28:18.085032 23128000471168 run.py:483] Algo bellman_ford step 734 current loss 0.278072, current_train_items 23520.
I0304 19:28:18.104264 23128000471168 run.py:483] Algo bellman_ford step 735 current loss 0.023894, current_train_items 23552.
I0304 19:28:18.120870 23128000471168 run.py:483] Algo bellman_ford step 736 current loss 0.055908, current_train_items 23584.
I0304 19:28:18.144693 23128000471168 run.py:483] Algo bellman_ford step 737 current loss 0.207304, current_train_items 23616.
I0304 19:28:18.174845 23128000471168 run.py:483] Algo bellman_ford step 738 current loss 0.303706, current_train_items 23648.
I0304 19:28:18.205124 23128000471168 run.py:483] Algo bellman_ford step 739 current loss 0.166187, current_train_items 23680.
I0304 19:28:18.224236 23128000471168 run.py:483] Algo bellman_ford step 740 current loss 0.018344, current_train_items 23712.
I0304 19:28:18.240738 23128000471168 run.py:483] Algo bellman_ford step 741 current loss 0.069538, current_train_items 23744.
I0304 19:28:18.264464 23128000471168 run.py:483] Algo bellman_ford step 742 current loss 0.210363, current_train_items 23776.
I0304 19:28:18.293367 23128000471168 run.py:483] Algo bellman_ford step 743 current loss 0.224592, current_train_items 23808.
I0304 19:28:18.327726 23128000471168 run.py:483] Algo bellman_ford step 744 current loss 0.275383, current_train_items 23840.
I0304 19:28:18.346931 23128000471168 run.py:483] Algo bellman_ford step 745 current loss 0.020784, current_train_items 23872.
I0304 19:28:18.363314 23128000471168 run.py:483] Algo bellman_ford step 746 current loss 0.112304, current_train_items 23904.
I0304 19:28:18.386911 23128000471168 run.py:483] Algo bellman_ford step 747 current loss 0.138241, current_train_items 23936.
I0304 19:28:18.417309 23128000471168 run.py:483] Algo bellman_ford step 748 current loss 0.301049, current_train_items 23968.
I0304 19:28:18.449167 23128000471168 run.py:483] Algo bellman_ford step 749 current loss 0.273902, current_train_items 24000.
I0304 19:28:18.468066 23128000471168 run.py:483] Algo bellman_ford step 750 current loss 0.010133, current_train_items 24032.
I0304 19:28:18.476098 23128000471168 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.9521484375, 'score': 0.9521484375, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0304 19:28:18.476206 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.961, current avg val score is 0.952, val scores are: bellman_ford: 0.952
I0304 19:28:18.493442 23128000471168 run.py:483] Algo bellman_ford step 751 current loss 0.060732, current_train_items 24064.
I0304 19:28:18.517010 23128000471168 run.py:483] Algo bellman_ford step 752 current loss 0.127385, current_train_items 24096.
I0304 19:28:18.546595 23128000471168 run.py:483] Algo bellman_ford step 753 current loss 0.149588, current_train_items 24128.
I0304 19:28:18.579548 23128000471168 run.py:483] Algo bellman_ford step 754 current loss 0.171086, current_train_items 24160.
I0304 19:28:18.598635 23128000471168 run.py:483] Algo bellman_ford step 755 current loss 0.042262, current_train_items 24192.
I0304 19:28:18.614382 23128000471168 run.py:483] Algo bellman_ford step 756 current loss 0.056020, current_train_items 24224.
I0304 19:28:18.639047 23128000471168 run.py:483] Algo bellman_ford step 757 current loss 0.105062, current_train_items 24256.
I0304 19:28:18.668584 23128000471168 run.py:483] Algo bellman_ford step 758 current loss 0.215193, current_train_items 24288.
I0304 19:28:18.699740 23128000471168 run.py:483] Algo bellman_ford step 759 current loss 0.171784, current_train_items 24320.
I0304 19:28:18.718822 23128000471168 run.py:483] Algo bellman_ford step 760 current loss 0.016301, current_train_items 24352.
I0304 19:28:18.735186 23128000471168 run.py:483] Algo bellman_ford step 761 current loss 0.063379, current_train_items 24384.
I0304 19:28:18.758892 23128000471168 run.py:483] Algo bellman_ford step 762 current loss 0.142156, current_train_items 24416.
I0304 19:28:18.787849 23128000471168 run.py:483] Algo bellman_ford step 763 current loss 0.118619, current_train_items 24448.
I0304 19:28:18.820300 23128000471168 run.py:483] Algo bellman_ford step 764 current loss 0.161281, current_train_items 24480.
I0304 19:28:18.839593 23128000471168 run.py:483] Algo bellman_ford step 765 current loss 0.018374, current_train_items 24512.
I0304 19:28:18.856470 23128000471168 run.py:483] Algo bellman_ford step 766 current loss 0.066005, current_train_items 24544.
I0304 19:28:18.880285 23128000471168 run.py:483] Algo bellman_ford step 767 current loss 0.121589, current_train_items 24576.
I0304 19:28:18.910298 23128000471168 run.py:483] Algo bellman_ford step 768 current loss 0.109614, current_train_items 24608.
I0304 19:28:18.943044 23128000471168 run.py:483] Algo bellman_ford step 769 current loss 0.133227, current_train_items 24640.
I0304 19:28:18.962321 23128000471168 run.py:483] Algo bellman_ford step 770 current loss 0.006267, current_train_items 24672.
I0304 19:28:18.978990 23128000471168 run.py:483] Algo bellman_ford step 771 current loss 0.075317, current_train_items 24704.
I0304 19:28:19.002423 23128000471168 run.py:483] Algo bellman_ford step 772 current loss 0.101741, current_train_items 24736.
I0304 19:28:19.032086 23128000471168 run.py:483] Algo bellman_ford step 773 current loss 0.105278, current_train_items 24768.
I0304 19:28:19.065214 23128000471168 run.py:483] Algo bellman_ford step 774 current loss 0.214293, current_train_items 24800.
I0304 19:28:19.084438 23128000471168 run.py:483] Algo bellman_ford step 775 current loss 0.009619, current_train_items 24832.
I0304 19:28:19.101593 23128000471168 run.py:483] Algo bellman_ford step 776 current loss 0.053302, current_train_items 24864.
I0304 19:28:19.125488 23128000471168 run.py:483] Algo bellman_ford step 777 current loss 0.082326, current_train_items 24896.
I0304 19:28:19.154591 23128000471168 run.py:483] Algo bellman_ford step 778 current loss 0.130587, current_train_items 24928.
I0304 19:28:19.186191 23128000471168 run.py:483] Algo bellman_ford step 779 current loss 0.175842, current_train_items 24960.
I0304 19:28:19.204979 23128000471168 run.py:483] Algo bellman_ford step 780 current loss 0.019312, current_train_items 24992.
I0304 19:28:19.221517 23128000471168 run.py:483] Algo bellman_ford step 781 current loss 0.053624, current_train_items 25024.
I0304 19:28:19.244554 23128000471168 run.py:483] Algo bellman_ford step 782 current loss 0.069438, current_train_items 25056.
I0304 19:28:19.273908 23128000471168 run.py:483] Algo bellman_ford step 783 current loss 0.136600, current_train_items 25088.
I0304 19:28:19.306585 23128000471168 run.py:483] Algo bellman_ford step 784 current loss 0.173667, current_train_items 25120.
I0304 19:28:19.325536 23128000471168 run.py:483] Algo bellman_ford step 785 current loss 0.014785, current_train_items 25152.
I0304 19:28:19.342308 23128000471168 run.py:483] Algo bellman_ford step 786 current loss 0.068621, current_train_items 25184.
I0304 19:28:19.365598 23128000471168 run.py:483] Algo bellman_ford step 787 current loss 0.093875, current_train_items 25216.
I0304 19:28:19.395654 23128000471168 run.py:483] Algo bellman_ford step 788 current loss 0.120064, current_train_items 25248.
I0304 19:28:19.425752 23128000471168 run.py:483] Algo bellman_ford step 789 current loss 0.221198, current_train_items 25280.
I0304 19:28:19.444937 23128000471168 run.py:483] Algo bellman_ford step 790 current loss 0.013381, current_train_items 25312.
I0304 19:28:19.461895 23128000471168 run.py:483] Algo bellman_ford step 791 current loss 0.077984, current_train_items 25344.
I0304 19:28:19.484280 23128000471168 run.py:483] Algo bellman_ford step 792 current loss 0.087734, current_train_items 25376.
I0304 19:28:19.514019 23128000471168 run.py:483] Algo bellman_ford step 793 current loss 0.161447, current_train_items 25408.
I0304 19:28:19.544703 23128000471168 run.py:483] Algo bellman_ford step 794 current loss 0.160096, current_train_items 25440.
I0304 19:28:19.564100 23128000471168 run.py:483] Algo bellman_ford step 795 current loss 0.013888, current_train_items 25472.
I0304 19:28:19.580147 23128000471168 run.py:483] Algo bellman_ford step 796 current loss 0.049349, current_train_items 25504.
I0304 19:28:19.603984 23128000471168 run.py:483] Algo bellman_ford step 797 current loss 0.098582, current_train_items 25536.
I0304 19:28:19.632670 23128000471168 run.py:483] Algo bellman_ford step 798 current loss 0.115553, current_train_items 25568.
I0304 19:28:19.664074 23128000471168 run.py:483] Algo bellman_ford step 799 current loss 0.137424, current_train_items 25600.
I0304 19:28:19.683393 23128000471168 run.py:483] Algo bellman_ford step 800 current loss 0.013160, current_train_items 25632.
I0304 19:28:19.691418 23128000471168 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0304 19:28:19.691527 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.961, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0304 19:28:19.708686 23128000471168 run.py:483] Algo bellman_ford step 801 current loss 0.093753, current_train_items 25664.
I0304 19:28:19.733939 23128000471168 run.py:483] Algo bellman_ford step 802 current loss 0.163696, current_train_items 25696.
I0304 19:28:19.764303 23128000471168 run.py:483] Algo bellman_ford step 803 current loss 0.172683, current_train_items 25728.
I0304 19:28:19.796208 23128000471168 run.py:483] Algo bellman_ford step 804 current loss 0.208543, current_train_items 25760.
I0304 19:28:19.815813 23128000471168 run.py:483] Algo bellman_ford step 805 current loss 0.007940, current_train_items 25792.
I0304 19:28:19.831757 23128000471168 run.py:483] Algo bellman_ford step 806 current loss 0.046485, current_train_items 25824.
I0304 19:28:19.856739 23128000471168 run.py:483] Algo bellman_ford step 807 current loss 0.145070, current_train_items 25856.
I0304 19:28:19.886723 23128000471168 run.py:483] Algo bellman_ford step 808 current loss 0.162656, current_train_items 25888.
I0304 19:28:19.918044 23128000471168 run.py:483] Algo bellman_ford step 809 current loss 0.247421, current_train_items 25920.
I0304 19:28:19.937248 23128000471168 run.py:483] Algo bellman_ford step 810 current loss 0.019454, current_train_items 25952.
I0304 19:28:19.954263 23128000471168 run.py:483] Algo bellman_ford step 811 current loss 0.114529, current_train_items 25984.
I0304 19:28:19.977636 23128000471168 run.py:483] Algo bellman_ford step 812 current loss 0.103646, current_train_items 26016.
I0304 19:28:20.006582 23128000471168 run.py:483] Algo bellman_ford step 813 current loss 0.161645, current_train_items 26048.
I0304 19:28:20.038298 23128000471168 run.py:483] Algo bellman_ford step 814 current loss 0.241165, current_train_items 26080.
I0304 19:28:20.057452 23128000471168 run.py:483] Algo bellman_ford step 815 current loss 0.019461, current_train_items 26112.
I0304 19:28:20.074200 23128000471168 run.py:483] Algo bellman_ford step 816 current loss 0.083154, current_train_items 26144.
I0304 19:28:20.098516 23128000471168 run.py:483] Algo bellman_ford step 817 current loss 0.070765, current_train_items 26176.
I0304 19:28:20.129452 23128000471168 run.py:483] Algo bellman_ford step 818 current loss 0.166152, current_train_items 26208.
I0304 19:28:20.163165 23128000471168 run.py:483] Algo bellman_ford step 819 current loss 0.176315, current_train_items 26240.
I0304 19:28:20.182560 23128000471168 run.py:483] Algo bellman_ford step 820 current loss 0.009592, current_train_items 26272.
I0304 19:28:20.198620 23128000471168 run.py:483] Algo bellman_ford step 821 current loss 0.076331, current_train_items 26304.
I0304 19:28:20.222229 23128000471168 run.py:483] Algo bellman_ford step 822 current loss 0.164855, current_train_items 26336.
I0304 19:28:20.252206 23128000471168 run.py:483] Algo bellman_ford step 823 current loss 0.158266, current_train_items 26368.
I0304 19:28:20.283659 23128000471168 run.py:483] Algo bellman_ford step 824 current loss 0.270658, current_train_items 26400.
I0304 19:28:20.302676 23128000471168 run.py:483] Algo bellman_ford step 825 current loss 0.029181, current_train_items 26432.
I0304 19:28:20.319522 23128000471168 run.py:483] Algo bellman_ford step 826 current loss 0.068568, current_train_items 26464.
I0304 19:28:20.343903 23128000471168 run.py:483] Algo bellman_ford step 827 current loss 0.117698, current_train_items 26496.
I0304 19:28:20.374159 23128000471168 run.py:483] Algo bellman_ford step 828 current loss 0.150399, current_train_items 26528.
I0304 19:28:20.405882 23128000471168 run.py:483] Algo bellman_ford step 829 current loss 0.148966, current_train_items 26560.
I0304 19:28:20.424742 23128000471168 run.py:483] Algo bellman_ford step 830 current loss 0.010736, current_train_items 26592.
I0304 19:28:20.441060 23128000471168 run.py:483] Algo bellman_ford step 831 current loss 0.053970, current_train_items 26624.
I0304 19:28:20.465439 23128000471168 run.py:483] Algo bellman_ford step 832 current loss 0.161844, current_train_items 26656.
I0304 19:28:20.495265 23128000471168 run.py:483] Algo bellman_ford step 833 current loss 0.161638, current_train_items 26688.
I0304 19:28:20.525994 23128000471168 run.py:483] Algo bellman_ford step 834 current loss 0.136036, current_train_items 26720.
I0304 19:28:20.545301 23128000471168 run.py:483] Algo bellman_ford step 835 current loss 0.009479, current_train_items 26752.
I0304 19:28:20.561549 23128000471168 run.py:483] Algo bellman_ford step 836 current loss 0.036437, current_train_items 26784.
I0304 19:28:20.585718 23128000471168 run.py:483] Algo bellman_ford step 837 current loss 0.160203, current_train_items 26816.
I0304 19:28:20.615678 23128000471168 run.py:483] Algo bellman_ford step 838 current loss 0.104709, current_train_items 26848.
I0304 19:28:20.648973 23128000471168 run.py:483] Algo bellman_ford step 839 current loss 0.244322, current_train_items 26880.
I0304 19:28:20.667847 23128000471168 run.py:483] Algo bellman_ford step 840 current loss 0.034423, current_train_items 26912.
I0304 19:28:20.684200 23128000471168 run.py:483] Algo bellman_ford step 841 current loss 0.049739, current_train_items 26944.
I0304 19:28:20.707996 23128000471168 run.py:483] Algo bellman_ford step 842 current loss 0.088541, current_train_items 26976.
I0304 19:28:20.736777 23128000471168 run.py:483] Algo bellman_ford step 843 current loss 0.108220, current_train_items 27008.
I0304 19:28:20.767842 23128000471168 run.py:483] Algo bellman_ford step 844 current loss 0.145756, current_train_items 27040.
I0304 19:28:20.786643 23128000471168 run.py:483] Algo bellman_ford step 845 current loss 0.012428, current_train_items 27072.
I0304 19:28:20.803444 23128000471168 run.py:483] Algo bellman_ford step 846 current loss 0.039830, current_train_items 27104.
I0304 19:28:20.828437 23128000471168 run.py:483] Algo bellman_ford step 847 current loss 0.113728, current_train_items 27136.
I0304 19:28:20.857692 23128000471168 run.py:483] Algo bellman_ford step 848 current loss 0.144411, current_train_items 27168.
I0304 19:28:20.890188 23128000471168 run.py:483] Algo bellman_ford step 849 current loss 0.144609, current_train_items 27200.
I0304 19:28:20.909069 23128000471168 run.py:483] Algo bellman_ford step 850 current loss 0.006673, current_train_items 27232.
I0304 19:28:20.917299 23128000471168 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0304 19:28:20.917409 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.961, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:28:20.949958 23128000471168 run.py:483] Algo bellman_ford step 851 current loss 0.025040, current_train_items 27264.
I0304 19:28:20.974249 23128000471168 run.py:483] Algo bellman_ford step 852 current loss 0.103552, current_train_items 27296.
I0304 19:28:21.005780 23128000471168 run.py:483] Algo bellman_ford step 853 current loss 0.118584, current_train_items 27328.
I0304 19:28:21.037704 23128000471168 run.py:483] Algo bellman_ford step 854 current loss 0.147336, current_train_items 27360.
I0304 19:28:21.057124 23128000471168 run.py:483] Algo bellman_ford step 855 current loss 0.043653, current_train_items 27392.
I0304 19:28:21.073332 23128000471168 run.py:483] Algo bellman_ford step 856 current loss 0.045066, current_train_items 27424.
I0304 19:28:21.096695 23128000471168 run.py:483] Algo bellman_ford step 857 current loss 0.146721, current_train_items 27456.
I0304 19:28:21.127736 23128000471168 run.py:483] Algo bellman_ford step 858 current loss 0.166394, current_train_items 27488.
I0304 19:28:21.158992 23128000471168 run.py:483] Algo bellman_ford step 859 current loss 0.164972, current_train_items 27520.
I0304 19:28:21.178445 23128000471168 run.py:483] Algo bellman_ford step 860 current loss 0.013674, current_train_items 27552.
I0304 19:28:21.195121 23128000471168 run.py:483] Algo bellman_ford step 861 current loss 0.112300, current_train_items 27584.
I0304 19:28:21.219021 23128000471168 run.py:483] Algo bellman_ford step 862 current loss 0.141064, current_train_items 27616.
I0304 19:28:21.248820 23128000471168 run.py:483] Algo bellman_ford step 863 current loss 0.215374, current_train_items 27648.
I0304 19:28:21.279954 23128000471168 run.py:483] Algo bellman_ford step 864 current loss 0.217840, current_train_items 27680.
I0304 19:28:21.298737 23128000471168 run.py:483] Algo bellman_ford step 865 current loss 0.025358, current_train_items 27712.
I0304 19:28:21.314764 23128000471168 run.py:483] Algo bellman_ford step 866 current loss 0.120224, current_train_items 27744.
I0304 19:28:21.339421 23128000471168 run.py:483] Algo bellman_ford step 867 current loss 0.303180, current_train_items 27776.
I0304 19:28:21.368977 23128000471168 run.py:483] Algo bellman_ford step 868 current loss 0.206828, current_train_items 27808.
I0304 19:28:21.401866 23128000471168 run.py:483] Algo bellman_ford step 869 current loss 0.203053, current_train_items 27840.
I0304 19:28:21.421323 23128000471168 run.py:483] Algo bellman_ford step 870 current loss 0.038054, current_train_items 27872.
I0304 19:28:21.438061 23128000471168 run.py:483] Algo bellman_ford step 871 current loss 0.060730, current_train_items 27904.
I0304 19:28:21.460724 23128000471168 run.py:483] Algo bellman_ford step 872 current loss 0.138961, current_train_items 27936.
I0304 19:28:21.491013 23128000471168 run.py:483] Algo bellman_ford step 873 current loss 0.250565, current_train_items 27968.
I0304 19:28:21.522217 23128000471168 run.py:483] Algo bellman_ford step 874 current loss 0.246131, current_train_items 28000.
I0304 19:28:21.541290 23128000471168 run.py:483] Algo bellman_ford step 875 current loss 0.008389, current_train_items 28032.
I0304 19:28:21.558098 23128000471168 run.py:483] Algo bellman_ford step 876 current loss 0.103383, current_train_items 28064.
I0304 19:28:21.583876 23128000471168 run.py:483] Algo bellman_ford step 877 current loss 0.271877, current_train_items 28096.
I0304 19:28:21.612740 23128000471168 run.py:483] Algo bellman_ford step 878 current loss 0.149454, current_train_items 28128.
I0304 19:28:21.646014 23128000471168 run.py:483] Algo bellman_ford step 879 current loss 0.169850, current_train_items 28160.
I0304 19:28:21.664926 23128000471168 run.py:483] Algo bellman_ford step 880 current loss 0.005153, current_train_items 28192.
I0304 19:28:21.681044 23128000471168 run.py:483] Algo bellman_ford step 881 current loss 0.023317, current_train_items 28224.
I0304 19:28:21.703968 23128000471168 run.py:483] Algo bellman_ford step 882 current loss 0.221265, current_train_items 28256.
I0304 19:28:21.733948 23128000471168 run.py:483] Algo bellman_ford step 883 current loss 0.382851, current_train_items 28288.
I0304 19:28:21.767137 23128000471168 run.py:483] Algo bellman_ford step 884 current loss 0.404645, current_train_items 28320.
I0304 19:28:21.786662 23128000471168 run.py:483] Algo bellman_ford step 885 current loss 0.015572, current_train_items 28352.
I0304 19:28:21.803088 23128000471168 run.py:483] Algo bellman_ford step 886 current loss 0.082538, current_train_items 28384.
I0304 19:28:21.827181 23128000471168 run.py:483] Algo bellman_ford step 887 current loss 0.116914, current_train_items 28416.
I0304 19:28:21.856525 23128000471168 run.py:483] Algo bellman_ford step 888 current loss 0.119221, current_train_items 28448.
I0304 19:28:21.889826 23128000471168 run.py:483] Algo bellman_ford step 889 current loss 0.192082, current_train_items 28480.
I0304 19:28:21.909349 23128000471168 run.py:483] Algo bellman_ford step 890 current loss 0.020724, current_train_items 28512.
I0304 19:28:21.926117 23128000471168 run.py:483] Algo bellman_ford step 891 current loss 0.065994, current_train_items 28544.
I0304 19:28:21.950348 23128000471168 run.py:483] Algo bellman_ford step 892 current loss 0.153771, current_train_items 28576.
I0304 19:28:21.981164 23128000471168 run.py:483] Algo bellman_ford step 893 current loss 0.194489, current_train_items 28608.
I0304 19:28:22.012357 23128000471168 run.py:483] Algo bellman_ford step 894 current loss 0.128988, current_train_items 28640.
I0304 19:28:22.031128 23128000471168 run.py:483] Algo bellman_ford step 895 current loss 0.029369, current_train_items 28672.
I0304 19:28:22.047633 23128000471168 run.py:483] Algo bellman_ford step 896 current loss 0.024859, current_train_items 28704.
I0304 19:28:22.071746 23128000471168 run.py:483] Algo bellman_ford step 897 current loss 0.157004, current_train_items 28736.
I0304 19:28:22.102114 23128000471168 run.py:483] Algo bellman_ford step 898 current loss 0.150007, current_train_items 28768.
I0304 19:28:22.135978 23128000471168 run.py:483] Algo bellman_ford step 899 current loss 0.211376, current_train_items 28800.
I0304 19:28:22.155441 23128000471168 run.py:483] Algo bellman_ford step 900 current loss 0.017308, current_train_items 28832.
I0304 19:28:22.163325 23128000471168 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0304 19:28:22.163430 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.963, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0304 19:28:22.192315 23128000471168 run.py:483] Algo bellman_ford step 901 current loss 0.101137, current_train_items 28864.
I0304 19:28:22.217039 23128000471168 run.py:483] Algo bellman_ford step 902 current loss 0.107862, current_train_items 28896.
I0304 19:28:22.248445 23128000471168 run.py:483] Algo bellman_ford step 903 current loss 0.107757, current_train_items 28928.
I0304 19:28:22.281058 23128000471168 run.py:483] Algo bellman_ford step 904 current loss 0.185435, current_train_items 28960.
I0304 19:28:22.300393 23128000471168 run.py:483] Algo bellman_ford step 905 current loss 0.014132, current_train_items 28992.
I0304 19:28:22.317055 23128000471168 run.py:483] Algo bellman_ford step 906 current loss 0.157498, current_train_items 29024.
I0304 19:28:22.341085 23128000471168 run.py:483] Algo bellman_ford step 907 current loss 0.125891, current_train_items 29056.
I0304 19:28:22.372371 23128000471168 run.py:483] Algo bellman_ford step 908 current loss 0.212156, current_train_items 29088.
I0304 19:28:22.405083 23128000471168 run.py:483] Algo bellman_ford step 909 current loss 0.190395, current_train_items 29120.
I0304 19:28:22.424049 23128000471168 run.py:483] Algo bellman_ford step 910 current loss 0.013094, current_train_items 29152.
I0304 19:28:22.441102 23128000471168 run.py:483] Algo bellman_ford step 911 current loss 0.108804, current_train_items 29184.
I0304 19:28:22.465954 23128000471168 run.py:483] Algo bellman_ford step 912 current loss 0.199212, current_train_items 29216.
I0304 19:28:22.494559 23128000471168 run.py:483] Algo bellman_ford step 913 current loss 0.136870, current_train_items 29248.
I0304 19:28:22.527713 23128000471168 run.py:483] Algo bellman_ford step 914 current loss 0.222027, current_train_items 29280.
I0304 19:28:22.546576 23128000471168 run.py:483] Algo bellman_ford step 915 current loss 0.009318, current_train_items 29312.
I0304 19:28:22.563380 23128000471168 run.py:483] Algo bellman_ford step 916 current loss 0.053463, current_train_items 29344.
I0304 19:28:22.587830 23128000471168 run.py:483] Algo bellman_ford step 917 current loss 0.150229, current_train_items 29376.
I0304 19:28:22.617813 23128000471168 run.py:483] Algo bellman_ford step 918 current loss 0.146781, current_train_items 29408.
I0304 19:28:22.651222 23128000471168 run.py:483] Algo bellman_ford step 919 current loss 0.227460, current_train_items 29440.
I0304 19:28:22.670329 23128000471168 run.py:483] Algo bellman_ford step 920 current loss 0.037299, current_train_items 29472.
I0304 19:28:22.687145 23128000471168 run.py:483] Algo bellman_ford step 921 current loss 0.061283, current_train_items 29504.
I0304 19:28:22.711410 23128000471168 run.py:483] Algo bellman_ford step 922 current loss 0.136958, current_train_items 29536.
I0304 19:28:22.740978 23128000471168 run.py:483] Algo bellman_ford step 923 current loss 0.090794, current_train_items 29568.
I0304 19:28:22.773250 23128000471168 run.py:483] Algo bellman_ford step 924 current loss 0.156489, current_train_items 29600.
I0304 19:28:22.792253 23128000471168 run.py:483] Algo bellman_ford step 925 current loss 0.028746, current_train_items 29632.
I0304 19:28:22.808376 23128000471168 run.py:483] Algo bellman_ford step 926 current loss 0.074182, current_train_items 29664.
I0304 19:28:22.832651 23128000471168 run.py:483] Algo bellman_ford step 927 current loss 0.244267, current_train_items 29696.
I0304 19:28:22.863990 23128000471168 run.py:483] Algo bellman_ford step 928 current loss 0.167811, current_train_items 29728.
I0304 19:28:22.894710 23128000471168 run.py:483] Algo bellman_ford step 929 current loss 0.144535, current_train_items 29760.
I0304 19:28:22.913669 23128000471168 run.py:483] Algo bellman_ford step 930 current loss 0.038894, current_train_items 29792.
I0304 19:28:22.930109 23128000471168 run.py:483] Algo bellman_ford step 931 current loss 0.061673, current_train_items 29824.
I0304 19:28:22.954059 23128000471168 run.py:483] Algo bellman_ford step 932 current loss 0.151985, current_train_items 29856.
I0304 19:28:22.983405 23128000471168 run.py:483] Algo bellman_ford step 933 current loss 0.111294, current_train_items 29888.
I0304 19:28:23.016376 23128000471168 run.py:483] Algo bellman_ford step 934 current loss 0.172468, current_train_items 29920.
I0304 19:28:23.035351 23128000471168 run.py:483] Algo bellman_ford step 935 current loss 0.014202, current_train_items 29952.
I0304 19:28:23.051858 23128000471168 run.py:483] Algo bellman_ford step 936 current loss 0.070906, current_train_items 29984.
I0304 19:28:23.075283 23128000471168 run.py:483] Algo bellman_ford step 937 current loss 0.110786, current_train_items 30016.
I0304 19:28:23.104746 23128000471168 run.py:483] Algo bellman_ford step 938 current loss 0.149773, current_train_items 30048.
I0304 19:28:23.136973 23128000471168 run.py:483] Algo bellman_ford step 939 current loss 0.223219, current_train_items 30080.
I0304 19:28:23.155950 23128000471168 run.py:483] Algo bellman_ford step 940 current loss 0.014405, current_train_items 30112.
I0304 19:28:23.172407 23128000471168 run.py:483] Algo bellman_ford step 941 current loss 0.087180, current_train_items 30144.
I0304 19:28:23.196972 23128000471168 run.py:483] Algo bellman_ford step 942 current loss 0.175069, current_train_items 30176.
I0304 19:28:23.226961 23128000471168 run.py:483] Algo bellman_ford step 943 current loss 0.193798, current_train_items 30208.
I0304 19:28:23.259926 23128000471168 run.py:483] Algo bellman_ford step 944 current loss 0.193366, current_train_items 30240.
I0304 19:28:23.279292 23128000471168 run.py:483] Algo bellman_ford step 945 current loss 0.013669, current_train_items 30272.
I0304 19:28:23.295256 23128000471168 run.py:483] Algo bellman_ford step 946 current loss 0.130786, current_train_items 30304.
I0304 19:28:23.320125 23128000471168 run.py:483] Algo bellman_ford step 947 current loss 0.303453, current_train_items 30336.
I0304 19:28:23.349722 23128000471168 run.py:483] Algo bellman_ford step 948 current loss 0.258121, current_train_items 30368.
I0304 19:28:23.381425 23128000471168 run.py:483] Algo bellman_ford step 949 current loss 0.203494, current_train_items 30400.
I0304 19:28:23.400384 23128000471168 run.py:483] Algo bellman_ford step 950 current loss 0.011963, current_train_items 30432.
I0304 19:28:23.408640 23128000471168 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0304 19:28:23.408747 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.965, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0304 19:28:23.425783 23128000471168 run.py:483] Algo bellman_ford step 951 current loss 0.188310, current_train_items 30464.
I0304 19:28:23.450118 23128000471168 run.py:483] Algo bellman_ford step 952 current loss 0.203768, current_train_items 30496.
I0304 19:28:23.480341 23128000471168 run.py:483] Algo bellman_ford step 953 current loss 0.264555, current_train_items 30528.
I0304 19:28:23.510319 23128000471168 run.py:483] Algo bellman_ford step 954 current loss 0.130455, current_train_items 30560.
I0304 19:28:23.529657 23128000471168 run.py:483] Algo bellman_ford step 955 current loss 0.029653, current_train_items 30592.
I0304 19:28:23.545905 23128000471168 run.py:483] Algo bellman_ford step 956 current loss 0.079721, current_train_items 30624.
I0304 19:28:23.570326 23128000471168 run.py:483] Algo bellman_ford step 957 current loss 0.129272, current_train_items 30656.
I0304 19:28:23.599081 23128000471168 run.py:483] Algo bellman_ford step 958 current loss 0.183404, current_train_items 30688.
I0304 19:28:23.631423 23128000471168 run.py:483] Algo bellman_ford step 959 current loss 0.214419, current_train_items 30720.
I0304 19:28:23.650726 23128000471168 run.py:483] Algo bellman_ford step 960 current loss 0.019559, current_train_items 30752.
I0304 19:28:23.667924 23128000471168 run.py:483] Algo bellman_ford step 961 current loss 0.076295, current_train_items 30784.
I0304 19:28:23.690338 23128000471168 run.py:483] Algo bellman_ford step 962 current loss 0.082075, current_train_items 30816.
I0304 19:28:23.718670 23128000471168 run.py:483] Algo bellman_ford step 963 current loss 0.166362, current_train_items 30848.
I0304 19:28:23.751392 23128000471168 run.py:483] Algo bellman_ford step 964 current loss 0.329056, current_train_items 30880.
I0304 19:28:23.770442 23128000471168 run.py:483] Algo bellman_ford step 965 current loss 0.025977, current_train_items 30912.
I0304 19:28:23.786686 23128000471168 run.py:483] Algo bellman_ford step 966 current loss 0.040881, current_train_items 30944.
I0304 19:28:23.811035 23128000471168 run.py:483] Algo bellman_ford step 967 current loss 0.136133, current_train_items 30976.
I0304 19:28:23.841513 23128000471168 run.py:483] Algo bellman_ford step 968 current loss 0.121056, current_train_items 31008.
I0304 19:28:23.873079 23128000471168 run.py:483] Algo bellman_ford step 969 current loss 0.145095, current_train_items 31040.
I0304 19:28:23.892313 23128000471168 run.py:483] Algo bellman_ford step 970 current loss 0.007211, current_train_items 31072.
I0304 19:28:23.908946 23128000471168 run.py:483] Algo bellman_ford step 971 current loss 0.035626, current_train_items 31104.
I0304 19:28:23.933549 23128000471168 run.py:483] Algo bellman_ford step 972 current loss 0.189661, current_train_items 31136.
I0304 19:28:23.964378 23128000471168 run.py:483] Algo bellman_ford step 973 current loss 0.254466, current_train_items 31168.
I0304 19:28:23.997913 23128000471168 run.py:483] Algo bellman_ford step 974 current loss 0.254664, current_train_items 31200.
I0304 19:28:24.017087 23128000471168 run.py:483] Algo bellman_ford step 975 current loss 0.028856, current_train_items 31232.
I0304 19:28:24.033535 23128000471168 run.py:483] Algo bellman_ford step 976 current loss 0.030521, current_train_items 31264.
I0304 19:28:24.057115 23128000471168 run.py:483] Algo bellman_ford step 977 current loss 0.113238, current_train_items 31296.
I0304 19:28:24.086623 23128000471168 run.py:483] Algo bellman_ford step 978 current loss 0.186783, current_train_items 31328.
I0304 19:28:24.118271 23128000471168 run.py:483] Algo bellman_ford step 979 current loss 0.231121, current_train_items 31360.
I0304 19:28:24.136788 23128000471168 run.py:483] Algo bellman_ford step 980 current loss 0.024612, current_train_items 31392.
I0304 19:28:24.153558 23128000471168 run.py:483] Algo bellman_ford step 981 current loss 0.065747, current_train_items 31424.
I0304 19:28:24.177121 23128000471168 run.py:483] Algo bellman_ford step 982 current loss 0.111545, current_train_items 31456.
I0304 19:28:24.206337 23128000471168 run.py:483] Algo bellman_ford step 983 current loss 0.132135, current_train_items 31488.
I0304 19:28:24.239158 23128000471168 run.py:483] Algo bellman_ford step 984 current loss 0.172001, current_train_items 31520.
I0304 19:28:24.258562 23128000471168 run.py:483] Algo bellman_ford step 985 current loss 0.022979, current_train_items 31552.
I0304 19:28:24.275465 23128000471168 run.py:483] Algo bellman_ford step 986 current loss 0.093331, current_train_items 31584.
I0304 19:28:24.298402 23128000471168 run.py:483] Algo bellman_ford step 987 current loss 0.076893, current_train_items 31616.
I0304 19:28:24.328560 23128000471168 run.py:483] Algo bellman_ford step 988 current loss 0.153295, current_train_items 31648.
I0304 19:28:24.359766 23128000471168 run.py:483] Algo bellman_ford step 989 current loss 0.118145, current_train_items 31680.
I0304 19:28:24.378981 23128000471168 run.py:483] Algo bellman_ford step 990 current loss 0.011766, current_train_items 31712.
I0304 19:28:24.395185 23128000471168 run.py:483] Algo bellman_ford step 991 current loss 0.047372, current_train_items 31744.
I0304 19:28:24.419066 23128000471168 run.py:483] Algo bellman_ford step 992 current loss 0.121350, current_train_items 31776.
I0304 19:28:24.447604 23128000471168 run.py:483] Algo bellman_ford step 993 current loss 0.100336, current_train_items 31808.
I0304 19:28:24.482113 23128000471168 run.py:483] Algo bellman_ford step 994 current loss 0.204418, current_train_items 31840.
I0304 19:28:24.501121 23128000471168 run.py:483] Algo bellman_ford step 995 current loss 0.023461, current_train_items 31872.
I0304 19:28:24.517477 23128000471168 run.py:483] Algo bellman_ford step 996 current loss 0.042423, current_train_items 31904.
I0304 19:28:24.541043 23128000471168 run.py:483] Algo bellman_ford step 997 current loss 0.156036, current_train_items 31936.
I0304 19:28:24.571517 23128000471168 run.py:483] Algo bellman_ford step 998 current loss 0.143795, current_train_items 31968.
I0304 19:28:24.603202 23128000471168 run.py:483] Algo bellman_ford step 999 current loss 0.134869, current_train_items 32000.
I0304 19:28:24.622415 23128000471168 run.py:483] Algo bellman_ford step 1000 current loss 0.015151, current_train_items 32032.
I0304 19:28:24.630190 23128000471168 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0304 19:28:24.630297 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.965, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:24.647349 23128000471168 run.py:483] Algo bellman_ford step 1001 current loss 0.072125, current_train_items 32064.
I0304 19:28:24.671439 23128000471168 run.py:483] Algo bellman_ford step 1002 current loss 0.122171, current_train_items 32096.
I0304 19:28:24.700097 23128000471168 run.py:483] Algo bellman_ford step 1003 current loss 0.195452, current_train_items 32128.
I0304 19:28:24.734781 23128000471168 run.py:483] Algo bellman_ford step 1004 current loss 0.270863, current_train_items 32160.
I0304 19:28:24.754262 23128000471168 run.py:483] Algo bellman_ford step 1005 current loss 0.091617, current_train_items 32192.
I0304 19:28:24.770233 23128000471168 run.py:483] Algo bellman_ford step 1006 current loss 0.144273, current_train_items 32224.
I0304 19:28:24.795337 23128000471168 run.py:483] Algo bellman_ford step 1007 current loss 0.241768, current_train_items 32256.
I0304 19:28:24.824609 23128000471168 run.py:483] Algo bellman_ford step 1008 current loss 0.148962, current_train_items 32288.
I0304 19:28:24.857710 23128000471168 run.py:483] Algo bellman_ford step 1009 current loss 0.145782, current_train_items 32320.
I0304 19:28:24.876654 23128000471168 run.py:483] Algo bellman_ford step 1010 current loss 0.018776, current_train_items 32352.
I0304 19:28:24.893058 23128000471168 run.py:483] Algo bellman_ford step 1011 current loss 0.087734, current_train_items 32384.
I0304 19:28:24.918024 23128000471168 run.py:483] Algo bellman_ford step 1012 current loss 0.254634, current_train_items 32416.
I0304 19:28:24.948228 23128000471168 run.py:483] Algo bellman_ford step 1013 current loss 0.195543, current_train_items 32448.
I0304 19:28:24.981094 23128000471168 run.py:483] Algo bellman_ford step 1014 current loss 0.125654, current_train_items 32480.
I0304 19:28:25.000286 23128000471168 run.py:483] Algo bellman_ford step 1015 current loss 0.014577, current_train_items 32512.
I0304 19:28:25.017044 23128000471168 run.py:483] Algo bellman_ford step 1016 current loss 0.117606, current_train_items 32544.
I0304 19:28:25.041165 23128000471168 run.py:483] Algo bellman_ford step 1017 current loss 0.135714, current_train_items 32576.
I0304 19:28:25.071321 23128000471168 run.py:483] Algo bellman_ford step 1018 current loss 0.094311, current_train_items 32608.
I0304 19:28:25.106534 23128000471168 run.py:483] Algo bellman_ford step 1019 current loss 0.221496, current_train_items 32640.
I0304 19:28:25.125330 23128000471168 run.py:483] Algo bellman_ford step 1020 current loss 0.005692, current_train_items 32672.
I0304 19:28:25.141441 23128000471168 run.py:483] Algo bellman_ford step 1021 current loss 0.038655, current_train_items 32704.
I0304 19:28:25.165023 23128000471168 run.py:483] Algo bellman_ford step 1022 current loss 0.106029, current_train_items 32736.
I0304 19:28:25.194227 23128000471168 run.py:483] Algo bellman_ford step 1023 current loss 0.140509, current_train_items 32768.
I0304 19:28:25.224679 23128000471168 run.py:483] Algo bellman_ford step 1024 current loss 0.105391, current_train_items 32800.
I0304 19:28:25.243916 23128000471168 run.py:483] Algo bellman_ford step 1025 current loss 0.025000, current_train_items 32832.
I0304 19:28:25.260944 23128000471168 run.py:483] Algo bellman_ford step 1026 current loss 0.070795, current_train_items 32864.
I0304 19:28:25.284650 23128000471168 run.py:483] Algo bellman_ford step 1027 current loss 0.075341, current_train_items 32896.
I0304 19:28:25.315305 23128000471168 run.py:483] Algo bellman_ford step 1028 current loss 0.103404, current_train_items 32928.
I0304 19:28:25.346855 23128000471168 run.py:483] Algo bellman_ford step 1029 current loss 0.130149, current_train_items 32960.
I0304 19:28:25.365851 23128000471168 run.py:483] Algo bellman_ford step 1030 current loss 0.013087, current_train_items 32992.
I0304 19:28:25.381777 23128000471168 run.py:483] Algo bellman_ford step 1031 current loss 0.044005, current_train_items 33024.
I0304 19:28:25.405148 23128000471168 run.py:483] Algo bellman_ford step 1032 current loss 0.195391, current_train_items 33056.
I0304 19:28:25.434045 23128000471168 run.py:483] Algo bellman_ford step 1033 current loss 0.259688, current_train_items 33088.
I0304 19:28:25.467401 23128000471168 run.py:483] Algo bellman_ford step 1034 current loss 0.204882, current_train_items 33120.
I0304 19:28:25.486358 23128000471168 run.py:483] Algo bellman_ford step 1035 current loss 0.030123, current_train_items 33152.
I0304 19:28:25.502760 23128000471168 run.py:483] Algo bellman_ford step 1036 current loss 0.060751, current_train_items 33184.
I0304 19:28:25.527467 23128000471168 run.py:483] Algo bellman_ford step 1037 current loss 0.200811, current_train_items 33216.
I0304 19:28:25.557672 23128000471168 run.py:483] Algo bellman_ford step 1038 current loss 0.362850, current_train_items 33248.
I0304 19:28:25.591502 23128000471168 run.py:483] Algo bellman_ford step 1039 current loss 0.343051, current_train_items 33280.
I0304 19:28:25.610463 23128000471168 run.py:483] Algo bellman_ford step 1040 current loss 0.034599, current_train_items 33312.
I0304 19:28:25.627529 23128000471168 run.py:483] Algo bellman_ford step 1041 current loss 0.072643, current_train_items 33344.
I0304 19:28:25.651692 23128000471168 run.py:483] Algo bellman_ford step 1042 current loss 0.146055, current_train_items 33376.
I0304 19:28:25.681257 23128000471168 run.py:483] Algo bellman_ford step 1043 current loss 0.195563, current_train_items 33408.
I0304 19:28:25.713332 23128000471168 run.py:483] Algo bellman_ford step 1044 current loss 0.238317, current_train_items 33440.
I0304 19:28:25.732059 23128000471168 run.py:483] Algo bellman_ford step 1045 current loss 0.008213, current_train_items 33472.
I0304 19:28:25.748073 23128000471168 run.py:483] Algo bellman_ford step 1046 current loss 0.032091, current_train_items 33504.
I0304 19:28:25.772665 23128000471168 run.py:483] Algo bellman_ford step 1047 current loss 0.094897, current_train_items 33536.
I0304 19:28:25.803373 23128000471168 run.py:483] Algo bellman_ford step 1048 current loss 0.115119, current_train_items 33568.
I0304 19:28:25.835883 23128000471168 run.py:483] Algo bellman_ford step 1049 current loss 0.170772, current_train_items 33600.
I0304 19:28:25.855020 23128000471168 run.py:483] Algo bellman_ford step 1050 current loss 0.019961, current_train_items 33632.
I0304 19:28:25.863074 23128000471168 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0304 19:28:25.863181 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.965, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:25.892193 23128000471168 run.py:483] Algo bellman_ford step 1051 current loss 0.040996, current_train_items 33664.
I0304 19:28:25.916697 23128000471168 run.py:483] Algo bellman_ford step 1052 current loss 0.132727, current_train_items 33696.
I0304 19:28:25.946399 23128000471168 run.py:483] Algo bellman_ford step 1053 current loss 0.137609, current_train_items 33728.
I0304 19:28:25.978937 23128000471168 run.py:483] Algo bellman_ford step 1054 current loss 0.196042, current_train_items 33760.
I0304 19:28:25.998857 23128000471168 run.py:483] Algo bellman_ford step 1055 current loss 0.025734, current_train_items 33792.
I0304 19:28:26.015361 23128000471168 run.py:483] Algo bellman_ford step 1056 current loss 0.048288, current_train_items 33824.
I0304 19:28:26.039521 23128000471168 run.py:483] Algo bellman_ford step 1057 current loss 0.118062, current_train_items 33856.
I0304 19:28:26.070129 23128000471168 run.py:483] Algo bellman_ford step 1058 current loss 0.106030, current_train_items 33888.
I0304 19:28:26.100247 23128000471168 run.py:483] Algo bellman_ford step 1059 current loss 0.109734, current_train_items 33920.
I0304 19:28:26.119321 23128000471168 run.py:483] Algo bellman_ford step 1060 current loss 0.013961, current_train_items 33952.
I0304 19:28:26.135797 23128000471168 run.py:483] Algo bellman_ford step 1061 current loss 0.037517, current_train_items 33984.
I0304 19:28:26.158092 23128000471168 run.py:483] Algo bellman_ford step 1062 current loss 0.054133, current_train_items 34016.
I0304 19:28:26.187267 23128000471168 run.py:483] Algo bellman_ford step 1063 current loss 0.153965, current_train_items 34048.
I0304 19:28:26.219847 23128000471168 run.py:483] Algo bellman_ford step 1064 current loss 0.142653, current_train_items 34080.
I0304 19:28:26.238558 23128000471168 run.py:483] Algo bellman_ford step 1065 current loss 0.016941, current_train_items 34112.
I0304 19:28:26.254923 23128000471168 run.py:483] Algo bellman_ford step 1066 current loss 0.047252, current_train_items 34144.
I0304 19:28:26.279036 23128000471168 run.py:483] Algo bellman_ford step 1067 current loss 0.106048, current_train_items 34176.
I0304 19:28:26.308952 23128000471168 run.py:483] Algo bellman_ford step 1068 current loss 0.073234, current_train_items 34208.
I0304 19:28:26.342400 23128000471168 run.py:483] Algo bellman_ford step 1069 current loss 0.169332, current_train_items 34240.
I0304 19:28:26.361242 23128000471168 run.py:483] Algo bellman_ford step 1070 current loss 0.010979, current_train_items 34272.
I0304 19:28:26.377996 23128000471168 run.py:483] Algo bellman_ford step 1071 current loss 0.106089, current_train_items 34304.
I0304 19:28:26.401581 23128000471168 run.py:483] Algo bellman_ford step 1072 current loss 0.155441, current_train_items 34336.
I0304 19:28:26.431308 23128000471168 run.py:483] Algo bellman_ford step 1073 current loss 0.091242, current_train_items 34368.
I0304 19:28:26.462452 23128000471168 run.py:483] Algo bellman_ford step 1074 current loss 0.090541, current_train_items 34400.
I0304 19:28:26.481311 23128000471168 run.py:483] Algo bellman_ford step 1075 current loss 0.011933, current_train_items 34432.
I0304 19:28:26.497994 23128000471168 run.py:483] Algo bellman_ford step 1076 current loss 0.074661, current_train_items 34464.
I0304 19:28:26.522063 23128000471168 run.py:483] Algo bellman_ford step 1077 current loss 0.114608, current_train_items 34496.
I0304 19:28:26.552165 23128000471168 run.py:483] Algo bellman_ford step 1078 current loss 0.174950, current_train_items 34528.
I0304 19:28:26.583765 23128000471168 run.py:483] Algo bellman_ford step 1079 current loss 0.135643, current_train_items 34560.
I0304 19:28:26.602345 23128000471168 run.py:483] Algo bellman_ford step 1080 current loss 0.009072, current_train_items 34592.
I0304 19:28:26.619016 23128000471168 run.py:483] Algo bellman_ford step 1081 current loss 0.047374, current_train_items 34624.
I0304 19:28:26.642108 23128000471168 run.py:483] Algo bellman_ford step 1082 current loss 0.186470, current_train_items 34656.
I0304 19:28:26.671139 23128000471168 run.py:483] Algo bellman_ford step 1083 current loss 0.149715, current_train_items 34688.
I0304 19:28:26.703909 23128000471168 run.py:483] Algo bellman_ford step 1084 current loss 0.128582, current_train_items 34720.
I0304 19:28:26.722863 23128000471168 run.py:483] Algo bellman_ford step 1085 current loss 0.006955, current_train_items 34752.
I0304 19:28:26.739404 23128000471168 run.py:483] Algo bellman_ford step 1086 current loss 0.047772, current_train_items 34784.
I0304 19:28:26.763403 23128000471168 run.py:483] Algo bellman_ford step 1087 current loss 0.188469, current_train_items 34816.
I0304 19:28:26.791819 23128000471168 run.py:483] Algo bellman_ford step 1088 current loss 0.139295, current_train_items 34848.
I0304 19:28:26.822196 23128000471168 run.py:483] Algo bellman_ford step 1089 current loss 0.165714, current_train_items 34880.
I0304 19:28:26.841198 23128000471168 run.py:483] Algo bellman_ford step 1090 current loss 0.027321, current_train_items 34912.
I0304 19:28:26.858165 23128000471168 run.py:483] Algo bellman_ford step 1091 current loss 0.137557, current_train_items 34944.
I0304 19:28:26.881064 23128000471168 run.py:483] Algo bellman_ford step 1092 current loss 0.171083, current_train_items 34976.
I0304 19:28:26.911375 23128000471168 run.py:483] Algo bellman_ford step 1093 current loss 0.300967, current_train_items 35008.
I0304 19:28:26.944361 23128000471168 run.py:483] Algo bellman_ford step 1094 current loss 0.202193, current_train_items 35040.
I0304 19:28:26.963028 23128000471168 run.py:483] Algo bellman_ford step 1095 current loss 0.007252, current_train_items 35072.
I0304 19:28:26.979251 23128000471168 run.py:483] Algo bellman_ford step 1096 current loss 0.050741, current_train_items 35104.
I0304 19:28:27.002914 23128000471168 run.py:483] Algo bellman_ford step 1097 current loss 0.109456, current_train_items 35136.
I0304 19:28:27.032804 23128000471168 run.py:483] Algo bellman_ford step 1098 current loss 0.181997, current_train_items 35168.
I0304 19:28:27.064115 23128000471168 run.py:483] Algo bellman_ford step 1099 current loss 0.124325, current_train_items 35200.
I0304 19:28:27.083450 23128000471168 run.py:483] Algo bellman_ford step 1100 current loss 0.004094, current_train_items 35232.
I0304 19:28:27.090882 23128000471168 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0304 19:28:27.090991 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.979, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:28:27.120512 23128000471168 run.py:483] Algo bellman_ford step 1101 current loss 0.047811, current_train_items 35264.
I0304 19:28:27.143934 23128000471168 run.py:483] Algo bellman_ford step 1102 current loss 0.125737, current_train_items 35296.
I0304 19:28:27.175257 23128000471168 run.py:483] Algo bellman_ford step 1103 current loss 0.150532, current_train_items 35328.
I0304 19:28:27.209585 23128000471168 run.py:483] Algo bellman_ford step 1104 current loss 0.164596, current_train_items 35360.
I0304 19:28:27.229027 23128000471168 run.py:483] Algo bellman_ford step 1105 current loss 0.007929, current_train_items 35392.
I0304 19:28:27.245064 23128000471168 run.py:483] Algo bellman_ford step 1106 current loss 0.020589, current_train_items 35424.
I0304 19:28:27.269399 23128000471168 run.py:483] Algo bellman_ford step 1107 current loss 0.141701, current_train_items 35456.
I0304 19:28:27.298943 23128000471168 run.py:483] Algo bellman_ford step 1108 current loss 0.156095, current_train_items 35488.
I0304 19:28:27.329338 23128000471168 run.py:483] Algo bellman_ford step 1109 current loss 0.129751, current_train_items 35520.
I0304 19:28:27.348154 23128000471168 run.py:483] Algo bellman_ford step 1110 current loss 0.013129, current_train_items 35552.
I0304 19:28:27.364632 23128000471168 run.py:483] Algo bellman_ford step 1111 current loss 0.055574, current_train_items 35584.
I0304 19:28:27.388646 23128000471168 run.py:483] Algo bellman_ford step 1112 current loss 0.110891, current_train_items 35616.
I0304 19:28:27.417138 23128000471168 run.py:483] Algo bellman_ford step 1113 current loss 0.126511, current_train_items 35648.
I0304 19:28:27.450503 23128000471168 run.py:483] Algo bellman_ford step 1114 current loss 0.142244, current_train_items 35680.
I0304 19:28:27.469204 23128000471168 run.py:483] Algo bellman_ford step 1115 current loss 0.012930, current_train_items 35712.
I0304 19:28:27.485172 23128000471168 run.py:483] Algo bellman_ford step 1116 current loss 0.033759, current_train_items 35744.
I0304 19:28:27.509511 23128000471168 run.py:483] Algo bellman_ford step 1117 current loss 0.125485, current_train_items 35776.
I0304 19:28:27.539960 23128000471168 run.py:483] Algo bellman_ford step 1118 current loss 0.078855, current_train_items 35808.
I0304 19:28:27.572823 23128000471168 run.py:483] Algo bellman_ford step 1119 current loss 0.213313, current_train_items 35840.
I0304 19:28:27.591789 23128000471168 run.py:483] Algo bellman_ford step 1120 current loss 0.011762, current_train_items 35872.
I0304 19:28:27.607572 23128000471168 run.py:483] Algo bellman_ford step 1121 current loss 0.034586, current_train_items 35904.
I0304 19:28:27.631993 23128000471168 run.py:483] Algo bellman_ford step 1122 current loss 0.180864, current_train_items 35936.
I0304 19:28:27.660915 23128000471168 run.py:483] Algo bellman_ford step 1123 current loss 0.157116, current_train_items 35968.
I0304 19:28:27.694886 23128000471168 run.py:483] Algo bellman_ford step 1124 current loss 0.249650, current_train_items 36000.
I0304 19:28:27.713569 23128000471168 run.py:483] Algo bellman_ford step 1125 current loss 0.006033, current_train_items 36032.
I0304 19:28:27.729557 23128000471168 run.py:483] Algo bellman_ford step 1126 current loss 0.058109, current_train_items 36064.
I0304 19:28:27.753081 23128000471168 run.py:483] Algo bellman_ford step 1127 current loss 0.150218, current_train_items 36096.
I0304 19:28:27.783179 23128000471168 run.py:483] Algo bellman_ford step 1128 current loss 0.171634, current_train_items 36128.
I0304 19:28:27.813783 23128000471168 run.py:483] Algo bellman_ford step 1129 current loss 0.148199, current_train_items 36160.
I0304 19:28:27.832773 23128000471168 run.py:483] Algo bellman_ford step 1130 current loss 0.008114, current_train_items 36192.
I0304 19:28:27.849063 23128000471168 run.py:483] Algo bellman_ford step 1131 current loss 0.024354, current_train_items 36224.
I0304 19:28:27.873315 23128000471168 run.py:483] Algo bellman_ford step 1132 current loss 0.182959, current_train_items 36256.
I0304 19:28:27.903584 23128000471168 run.py:483] Algo bellman_ford step 1133 current loss 0.186394, current_train_items 36288.
I0304 19:28:27.934694 23128000471168 run.py:483] Algo bellman_ford step 1134 current loss 0.135681, current_train_items 36320.
I0304 19:28:27.953681 23128000471168 run.py:483] Algo bellman_ford step 1135 current loss 0.017865, current_train_items 36352.
I0304 19:28:27.970014 23128000471168 run.py:483] Algo bellman_ford step 1136 current loss 0.035248, current_train_items 36384.
I0304 19:28:27.993191 23128000471168 run.py:483] Algo bellman_ford step 1137 current loss 0.064265, current_train_items 36416.
I0304 19:28:28.023960 23128000471168 run.py:483] Algo bellman_ford step 1138 current loss 0.104320, current_train_items 36448.
I0304 19:28:28.055926 23128000471168 run.py:483] Algo bellman_ford step 1139 current loss 0.184063, current_train_items 36480.
I0304 19:28:28.075323 23128000471168 run.py:483] Algo bellman_ford step 1140 current loss 0.008075, current_train_items 36512.
I0304 19:28:28.091840 23128000471168 run.py:483] Algo bellman_ford step 1141 current loss 0.020468, current_train_items 36544.
I0304 19:28:28.115191 23128000471168 run.py:483] Algo bellman_ford step 1142 current loss 0.121203, current_train_items 36576.
I0304 19:28:28.144260 23128000471168 run.py:483] Algo bellman_ford step 1143 current loss 0.156694, current_train_items 36608.
I0304 19:28:28.176332 23128000471168 run.py:483] Algo bellman_ford step 1144 current loss 0.076519, current_train_items 36640.
I0304 19:28:28.195605 23128000471168 run.py:483] Algo bellman_ford step 1145 current loss 0.029366, current_train_items 36672.
I0304 19:28:28.212138 23128000471168 run.py:483] Algo bellman_ford step 1146 current loss 0.059486, current_train_items 36704.
I0304 19:28:28.236243 23128000471168 run.py:483] Algo bellman_ford step 1147 current loss 0.088540, current_train_items 36736.
I0304 19:28:28.266134 23128000471168 run.py:483] Algo bellman_ford step 1148 current loss 0.129656, current_train_items 36768.
I0304 19:28:28.295821 23128000471168 run.py:483] Algo bellman_ford step 1149 current loss 0.112254, current_train_items 36800.
I0304 19:28:28.314766 23128000471168 run.py:483] Algo bellman_ford step 1150 current loss 0.021599, current_train_items 36832.
I0304 19:28:28.322853 23128000471168 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0304 19:28:28.322960 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.982, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:28:28.339700 23128000471168 run.py:483] Algo bellman_ford step 1151 current loss 0.029113, current_train_items 36864.
I0304 19:28:28.364344 23128000471168 run.py:483] Algo bellman_ford step 1152 current loss 0.130619, current_train_items 36896.
I0304 19:28:28.396001 23128000471168 run.py:483] Algo bellman_ford step 1153 current loss 0.113208, current_train_items 36928.
W0304 19:28:28.416856 23128000471168 samplers.py:155] Increasing hint lengh from 11 to 12
I0304 19:28:35.162106 23128000471168 run.py:483] Algo bellman_ford step 1154 current loss 0.131460, current_train_items 36960.
I0304 19:28:35.182898 23128000471168 run.py:483] Algo bellman_ford step 1155 current loss 0.006949, current_train_items 36992.
I0304 19:28:35.199163 23128000471168 run.py:483] Algo bellman_ford step 1156 current loss 0.072953, current_train_items 37024.
I0304 19:28:35.222816 23128000471168 run.py:483] Algo bellman_ford step 1157 current loss 0.105363, current_train_items 37056.
I0304 19:28:35.252619 23128000471168 run.py:483] Algo bellman_ford step 1158 current loss 0.115813, current_train_items 37088.
I0304 19:28:35.283343 23128000471168 run.py:483] Algo bellman_ford step 1159 current loss 0.153435, current_train_items 37120.
I0304 19:28:35.303493 23128000471168 run.py:483] Algo bellman_ford step 1160 current loss 0.018192, current_train_items 37152.
I0304 19:28:35.320172 23128000471168 run.py:483] Algo bellman_ford step 1161 current loss 0.043936, current_train_items 37184.
I0304 19:28:35.343074 23128000471168 run.py:483] Algo bellman_ford step 1162 current loss 0.070563, current_train_items 37216.
I0304 19:28:35.372802 23128000471168 run.py:483] Algo bellman_ford step 1163 current loss 0.093084, current_train_items 37248.
I0304 19:28:35.405896 23128000471168 run.py:483] Algo bellman_ford step 1164 current loss 0.181997, current_train_items 37280.
I0304 19:28:35.424945 23128000471168 run.py:483] Algo bellman_ford step 1165 current loss 0.010116, current_train_items 37312.
I0304 19:28:35.441374 23128000471168 run.py:483] Algo bellman_ford step 1166 current loss 0.036078, current_train_items 37344.
I0304 19:28:35.465157 23128000471168 run.py:483] Algo bellman_ford step 1167 current loss 0.164100, current_train_items 37376.
I0304 19:28:35.494902 23128000471168 run.py:483] Algo bellman_ford step 1168 current loss 0.106934, current_train_items 37408.
I0304 19:28:35.528394 23128000471168 run.py:483] Algo bellman_ford step 1169 current loss 0.170629, current_train_items 37440.
I0304 19:28:35.547955 23128000471168 run.py:483] Algo bellman_ford step 1170 current loss 0.014579, current_train_items 37472.
I0304 19:28:35.564935 23128000471168 run.py:483] Algo bellman_ford step 1171 current loss 0.083790, current_train_items 37504.
I0304 19:28:35.588196 23128000471168 run.py:483] Algo bellman_ford step 1172 current loss 0.123549, current_train_items 37536.
I0304 19:28:35.618670 23128000471168 run.py:483] Algo bellman_ford step 1173 current loss 0.132074, current_train_items 37568.
I0304 19:28:35.651206 23128000471168 run.py:483] Algo bellman_ford step 1174 current loss 0.152826, current_train_items 37600.
I0304 19:28:35.670725 23128000471168 run.py:483] Algo bellman_ford step 1175 current loss 0.008662, current_train_items 37632.
I0304 19:28:35.687325 23128000471168 run.py:483] Algo bellman_ford step 1176 current loss 0.078175, current_train_items 37664.
I0304 19:28:35.710598 23128000471168 run.py:483] Algo bellman_ford step 1177 current loss 0.137328, current_train_items 37696.
I0304 19:28:35.739839 23128000471168 run.py:483] Algo bellman_ford step 1178 current loss 0.117988, current_train_items 37728.
I0304 19:28:35.773907 23128000471168 run.py:483] Algo bellman_ford step 1179 current loss 0.181192, current_train_items 37760.
I0304 19:28:35.792795 23128000471168 run.py:483] Algo bellman_ford step 1180 current loss 0.013362, current_train_items 37792.
I0304 19:28:35.809602 23128000471168 run.py:483] Algo bellman_ford step 1181 current loss 0.147218, current_train_items 37824.
I0304 19:28:35.833149 23128000471168 run.py:483] Algo bellman_ford step 1182 current loss 0.091031, current_train_items 37856.
I0304 19:28:35.862039 23128000471168 run.py:483] Algo bellman_ford step 1183 current loss 0.124751, current_train_items 37888.
I0304 19:28:35.894275 23128000471168 run.py:483] Algo bellman_ford step 1184 current loss 0.169772, current_train_items 37920.
I0304 19:28:35.913966 23128000471168 run.py:483] Algo bellman_ford step 1185 current loss 0.032448, current_train_items 37952.
I0304 19:28:35.930324 23128000471168 run.py:483] Algo bellman_ford step 1186 current loss 0.085114, current_train_items 37984.
I0304 19:28:35.953914 23128000471168 run.py:483] Algo bellman_ford step 1187 current loss 0.168260, current_train_items 38016.
I0304 19:28:35.983234 23128000471168 run.py:483] Algo bellman_ford step 1188 current loss 0.083847, current_train_items 38048.
I0304 19:28:36.014384 23128000471168 run.py:483] Algo bellman_ford step 1189 current loss 0.120586, current_train_items 38080.
I0304 19:28:36.033735 23128000471168 run.py:483] Algo bellman_ford step 1190 current loss 0.011550, current_train_items 38112.
I0304 19:28:36.050630 23128000471168 run.py:483] Algo bellman_ford step 1191 current loss 0.087078, current_train_items 38144.
I0304 19:28:36.075027 23128000471168 run.py:483] Algo bellman_ford step 1192 current loss 0.147945, current_train_items 38176.
I0304 19:28:36.104789 23128000471168 run.py:483] Algo bellman_ford step 1193 current loss 0.103381, current_train_items 38208.
I0304 19:28:36.136822 23128000471168 run.py:483] Algo bellman_ford step 1194 current loss 0.133778, current_train_items 38240.
I0304 19:28:36.156213 23128000471168 run.py:483] Algo bellman_ford step 1195 current loss 0.011971, current_train_items 38272.
I0304 19:28:36.172665 23128000471168 run.py:483] Algo bellman_ford step 1196 current loss 0.050972, current_train_items 38304.
I0304 19:28:36.195800 23128000471168 run.py:483] Algo bellman_ford step 1197 current loss 0.230086, current_train_items 38336.
I0304 19:28:36.226765 23128000471168 run.py:483] Algo bellman_ford step 1198 current loss 0.183946, current_train_items 38368.
I0304 19:28:36.260069 23128000471168 run.py:483] Algo bellman_ford step 1199 current loss 0.197776, current_train_items 38400.
I0304 19:28:36.280085 23128000471168 run.py:483] Algo bellman_ford step 1200 current loss 0.040581, current_train_items 38432.
I0304 19:28:36.289550 23128000471168 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0304 19:28:36.289660 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.982, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:28:36.307133 23128000471168 run.py:483] Algo bellman_ford step 1201 current loss 0.145416, current_train_items 38464.
I0304 19:28:36.331099 23128000471168 run.py:483] Algo bellman_ford step 1202 current loss 0.143453, current_train_items 38496.
I0304 19:28:36.362110 23128000471168 run.py:483] Algo bellman_ford step 1203 current loss 0.232814, current_train_items 38528.
I0304 19:28:36.395570 23128000471168 run.py:483] Algo bellman_ford step 1204 current loss 0.217975, current_train_items 38560.
I0304 19:28:36.415009 23128000471168 run.py:483] Algo bellman_ford step 1205 current loss 0.013031, current_train_items 38592.
I0304 19:28:36.431119 23128000471168 run.py:483] Algo bellman_ford step 1206 current loss 0.029406, current_train_items 38624.
I0304 19:28:36.454166 23128000471168 run.py:483] Algo bellman_ford step 1207 current loss 0.080576, current_train_items 38656.
I0304 19:28:36.484204 23128000471168 run.py:483] Algo bellman_ford step 1208 current loss 0.198505, current_train_items 38688.
I0304 19:28:36.517856 23128000471168 run.py:483] Algo bellman_ford step 1209 current loss 0.167558, current_train_items 38720.
I0304 19:28:36.536976 23128000471168 run.py:483] Algo bellman_ford step 1210 current loss 0.004732, current_train_items 38752.
I0304 19:28:36.553540 23128000471168 run.py:483] Algo bellman_ford step 1211 current loss 0.045566, current_train_items 38784.
I0304 19:28:36.576582 23128000471168 run.py:483] Algo bellman_ford step 1212 current loss 0.066261, current_train_items 38816.
I0304 19:28:36.605515 23128000471168 run.py:483] Algo bellman_ford step 1213 current loss 0.066852, current_train_items 38848.
I0304 19:28:36.637871 23128000471168 run.py:483] Algo bellman_ford step 1214 current loss 0.111337, current_train_items 38880.
I0304 19:28:36.656994 23128000471168 run.py:483] Algo bellman_ford step 1215 current loss 0.005390, current_train_items 38912.
I0304 19:28:36.673209 23128000471168 run.py:483] Algo bellman_ford step 1216 current loss 0.014194, current_train_items 38944.
I0304 19:28:36.696838 23128000471168 run.py:483] Algo bellman_ford step 1217 current loss 0.101247, current_train_items 38976.
I0304 19:28:36.727439 23128000471168 run.py:483] Algo bellman_ford step 1218 current loss 0.136693, current_train_items 39008.
I0304 19:28:36.759989 23128000471168 run.py:483] Algo bellman_ford step 1219 current loss 0.109977, current_train_items 39040.
I0304 19:28:36.779479 23128000471168 run.py:483] Algo bellman_ford step 1220 current loss 0.010350, current_train_items 39072.
I0304 19:28:36.795585 23128000471168 run.py:483] Algo bellman_ford step 1221 current loss 0.032320, current_train_items 39104.
I0304 19:28:36.819859 23128000471168 run.py:483] Algo bellman_ford step 1222 current loss 0.112250, current_train_items 39136.
I0304 19:28:36.850042 23128000471168 run.py:483] Algo bellman_ford step 1223 current loss 0.096631, current_train_items 39168.
I0304 19:28:36.881900 23128000471168 run.py:483] Algo bellman_ford step 1224 current loss 0.100758, current_train_items 39200.
I0304 19:28:36.901405 23128000471168 run.py:483] Algo bellman_ford step 1225 current loss 0.005049, current_train_items 39232.
I0304 19:28:36.917802 23128000471168 run.py:483] Algo bellman_ford step 1226 current loss 0.076786, current_train_items 39264.
I0304 19:28:36.942169 23128000471168 run.py:483] Algo bellman_ford step 1227 current loss 0.080016, current_train_items 39296.
I0304 19:28:36.971692 23128000471168 run.py:483] Algo bellman_ford step 1228 current loss 0.118934, current_train_items 39328.
I0304 19:28:37.005522 23128000471168 run.py:483] Algo bellman_ford step 1229 current loss 0.228159, current_train_items 39360.
I0304 19:28:37.024888 23128000471168 run.py:483] Algo bellman_ford step 1230 current loss 0.005361, current_train_items 39392.
I0304 19:28:37.041279 23128000471168 run.py:483] Algo bellman_ford step 1231 current loss 0.101828, current_train_items 39424.
I0304 19:28:37.065335 23128000471168 run.py:483] Algo bellman_ford step 1232 current loss 0.163447, current_train_items 39456.
I0304 19:28:37.094860 23128000471168 run.py:483] Algo bellman_ford step 1233 current loss 0.160708, current_train_items 39488.
I0304 19:28:37.125764 23128000471168 run.py:483] Algo bellman_ford step 1234 current loss 0.141398, current_train_items 39520.
I0304 19:28:37.144846 23128000471168 run.py:483] Algo bellman_ford step 1235 current loss 0.006666, current_train_items 39552.
I0304 19:28:37.160702 23128000471168 run.py:483] Algo bellman_ford step 1236 current loss 0.017880, current_train_items 39584.
I0304 19:28:37.184056 23128000471168 run.py:483] Algo bellman_ford step 1237 current loss 0.141051, current_train_items 39616.
I0304 19:28:37.214001 23128000471168 run.py:483] Algo bellman_ford step 1238 current loss 0.303884, current_train_items 39648.
I0304 19:28:37.247320 23128000471168 run.py:483] Algo bellman_ford step 1239 current loss 0.190642, current_train_items 39680.
I0304 19:28:37.266652 23128000471168 run.py:483] Algo bellman_ford step 1240 current loss 0.013841, current_train_items 39712.
I0304 19:28:37.283051 23128000471168 run.py:483] Algo bellman_ford step 1241 current loss 0.066876, current_train_items 39744.
I0304 19:28:37.307277 23128000471168 run.py:483] Algo bellman_ford step 1242 current loss 0.126895, current_train_items 39776.
I0304 19:28:37.336980 23128000471168 run.py:483] Algo bellman_ford step 1243 current loss 0.129958, current_train_items 39808.
I0304 19:28:37.369635 23128000471168 run.py:483] Algo bellman_ford step 1244 current loss 0.210707, current_train_items 39840.
I0304 19:28:37.388538 23128000471168 run.py:483] Algo bellman_ford step 1245 current loss 0.010741, current_train_items 39872.
I0304 19:28:37.404812 23128000471168 run.py:483] Algo bellman_ford step 1246 current loss 0.041665, current_train_items 39904.
I0304 19:28:37.427802 23128000471168 run.py:483] Algo bellman_ford step 1247 current loss 0.141164, current_train_items 39936.
I0304 19:28:37.457886 23128000471168 run.py:483] Algo bellman_ford step 1248 current loss 0.155235, current_train_items 39968.
I0304 19:28:37.489658 23128000471168 run.py:483] Algo bellman_ford step 1249 current loss 0.156087, current_train_items 40000.
I0304 19:28:37.509025 23128000471168 run.py:483] Algo bellman_ford step 1250 current loss 0.017432, current_train_items 40032.
I0304 19:28:37.517709 23128000471168 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0304 19:28:37.517816 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.982, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:37.534732 23128000471168 run.py:483] Algo bellman_ford step 1251 current loss 0.073520, current_train_items 40064.
I0304 19:28:37.558658 23128000471168 run.py:483] Algo bellman_ford step 1252 current loss 0.089522, current_train_items 40096.
I0304 19:28:37.587224 23128000471168 run.py:483] Algo bellman_ford step 1253 current loss 0.123867, current_train_items 40128.
I0304 19:28:37.617336 23128000471168 run.py:483] Algo bellman_ford step 1254 current loss 0.133001, current_train_items 40160.
I0304 19:28:37.636851 23128000471168 run.py:483] Algo bellman_ford step 1255 current loss 0.007206, current_train_items 40192.
I0304 19:28:37.653324 23128000471168 run.py:483] Algo bellman_ford step 1256 current loss 0.045050, current_train_items 40224.
I0304 19:28:37.676679 23128000471168 run.py:483] Algo bellman_ford step 1257 current loss 0.138375, current_train_items 40256.
I0304 19:28:37.705901 23128000471168 run.py:483] Algo bellman_ford step 1258 current loss 0.133122, current_train_items 40288.
I0304 19:28:37.737677 23128000471168 run.py:483] Algo bellman_ford step 1259 current loss 0.191273, current_train_items 40320.
I0304 19:28:37.757248 23128000471168 run.py:483] Algo bellman_ford step 1260 current loss 0.053735, current_train_items 40352.
I0304 19:28:37.774285 23128000471168 run.py:483] Algo bellman_ford step 1261 current loss 0.042798, current_train_items 40384.
I0304 19:28:37.798255 23128000471168 run.py:483] Algo bellman_ford step 1262 current loss 0.109921, current_train_items 40416.
I0304 19:28:37.825628 23128000471168 run.py:483] Algo bellman_ford step 1263 current loss 0.097265, current_train_items 40448.
I0304 19:28:37.859893 23128000471168 run.py:483] Algo bellman_ford step 1264 current loss 0.144758, current_train_items 40480.
I0304 19:28:37.878771 23128000471168 run.py:483] Algo bellman_ford step 1265 current loss 0.004583, current_train_items 40512.
I0304 19:28:37.895048 23128000471168 run.py:483] Algo bellman_ford step 1266 current loss 0.042255, current_train_items 40544.
I0304 19:28:37.917823 23128000471168 run.py:483] Algo bellman_ford step 1267 current loss 0.068300, current_train_items 40576.
I0304 19:28:37.946987 23128000471168 run.py:483] Algo bellman_ford step 1268 current loss 0.091918, current_train_items 40608.
I0304 19:28:37.979813 23128000471168 run.py:483] Algo bellman_ford step 1269 current loss 0.110827, current_train_items 40640.
I0304 19:28:37.999401 23128000471168 run.py:483] Algo bellman_ford step 1270 current loss 0.005915, current_train_items 40672.
I0304 19:28:38.015743 23128000471168 run.py:483] Algo bellman_ford step 1271 current loss 0.031571, current_train_items 40704.
I0304 19:28:38.038168 23128000471168 run.py:483] Algo bellman_ford step 1272 current loss 0.091240, current_train_items 40736.
I0304 19:28:38.066869 23128000471168 run.py:483] Algo bellman_ford step 1273 current loss 0.110553, current_train_items 40768.
I0304 19:28:38.099598 23128000471168 run.py:483] Algo bellman_ford step 1274 current loss 0.150979, current_train_items 40800.
I0304 19:28:38.119254 23128000471168 run.py:483] Algo bellman_ford step 1275 current loss 0.007720, current_train_items 40832.
I0304 19:28:38.135818 23128000471168 run.py:483] Algo bellman_ford step 1276 current loss 0.066846, current_train_items 40864.
I0304 19:28:38.158295 23128000471168 run.py:483] Algo bellman_ford step 1277 current loss 0.124212, current_train_items 40896.
I0304 19:28:38.187930 23128000471168 run.py:483] Algo bellman_ford step 1278 current loss 0.121333, current_train_items 40928.
I0304 19:28:38.219761 23128000471168 run.py:483] Algo bellman_ford step 1279 current loss 0.180934, current_train_items 40960.
I0304 19:28:38.238973 23128000471168 run.py:483] Algo bellman_ford step 1280 current loss 0.010962, current_train_items 40992.
I0304 19:28:38.255169 23128000471168 run.py:483] Algo bellman_ford step 1281 current loss 0.038225, current_train_items 41024.
I0304 19:28:38.279145 23128000471168 run.py:483] Algo bellman_ford step 1282 current loss 0.141018, current_train_items 41056.
I0304 19:28:38.309602 23128000471168 run.py:483] Algo bellman_ford step 1283 current loss 0.216359, current_train_items 41088.
I0304 19:28:38.341018 23128000471168 run.py:483] Algo bellman_ford step 1284 current loss 0.153528, current_train_items 41120.
I0304 19:28:38.360677 23128000471168 run.py:483] Algo bellman_ford step 1285 current loss 0.071247, current_train_items 41152.
I0304 19:28:38.377382 23128000471168 run.py:483] Algo bellman_ford step 1286 current loss 0.034393, current_train_items 41184.
I0304 19:28:38.402100 23128000471168 run.py:483] Algo bellman_ford step 1287 current loss 0.112985, current_train_items 41216.
I0304 19:28:38.432133 23128000471168 run.py:483] Algo bellman_ford step 1288 current loss 0.170904, current_train_items 41248.
I0304 19:28:38.464631 23128000471168 run.py:483] Algo bellman_ford step 1289 current loss 0.189740, current_train_items 41280.
I0304 19:28:38.484367 23128000471168 run.py:483] Algo bellman_ford step 1290 current loss 0.007197, current_train_items 41312.
I0304 19:28:38.500890 23128000471168 run.py:483] Algo bellman_ford step 1291 current loss 0.055300, current_train_items 41344.
I0304 19:28:38.524117 23128000471168 run.py:483] Algo bellman_ford step 1292 current loss 0.079896, current_train_items 41376.
I0304 19:28:38.553572 23128000471168 run.py:483] Algo bellman_ford step 1293 current loss 0.090214, current_train_items 41408.
I0304 19:28:38.586980 23128000471168 run.py:483] Algo bellman_ford step 1294 current loss 0.165322, current_train_items 41440.
I0304 19:28:38.606097 23128000471168 run.py:483] Algo bellman_ford step 1295 current loss 0.027669, current_train_items 41472.
I0304 19:28:38.622807 23128000471168 run.py:483] Algo bellman_ford step 1296 current loss 0.032232, current_train_items 41504.
I0304 19:28:38.646687 23128000471168 run.py:483] Algo bellman_ford step 1297 current loss 0.074803, current_train_items 41536.
I0304 19:28:38.675254 23128000471168 run.py:483] Algo bellman_ford step 1298 current loss 0.085164, current_train_items 41568.
I0304 19:28:38.705452 23128000471168 run.py:483] Algo bellman_ford step 1299 current loss 0.115729, current_train_items 41600.
I0304 19:28:38.724782 23128000471168 run.py:483] Algo bellman_ford step 1300 current loss 0.013105, current_train_items 41632.
I0304 19:28:38.732620 23128000471168 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0304 19:28:38.732730 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.982, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:28:38.762285 23128000471168 run.py:483] Algo bellman_ford step 1301 current loss 0.040213, current_train_items 41664.
I0304 19:28:38.786957 23128000471168 run.py:483] Algo bellman_ford step 1302 current loss 0.131890, current_train_items 41696.
I0304 19:28:38.818138 23128000471168 run.py:483] Algo bellman_ford step 1303 current loss 0.127341, current_train_items 41728.
I0304 19:28:38.849426 23128000471168 run.py:483] Algo bellman_ford step 1304 current loss 0.104820, current_train_items 41760.
I0304 19:28:38.869417 23128000471168 run.py:483] Algo bellman_ford step 1305 current loss 0.007876, current_train_items 41792.
I0304 19:28:38.885638 23128000471168 run.py:483] Algo bellman_ford step 1306 current loss 0.051681, current_train_items 41824.
I0304 19:28:38.908684 23128000471168 run.py:483] Algo bellman_ford step 1307 current loss 0.122977, current_train_items 41856.
I0304 19:28:38.937954 23128000471168 run.py:483] Algo bellman_ford step 1308 current loss 0.174909, current_train_items 41888.
I0304 19:28:38.969798 23128000471168 run.py:483] Algo bellman_ford step 1309 current loss 0.155427, current_train_items 41920.
I0304 19:28:38.989167 23128000471168 run.py:483] Algo bellman_ford step 1310 current loss 0.005539, current_train_items 41952.
I0304 19:28:39.005532 23128000471168 run.py:483] Algo bellman_ford step 1311 current loss 0.080106, current_train_items 41984.
I0304 19:28:39.028813 23128000471168 run.py:483] Algo bellman_ford step 1312 current loss 0.061650, current_train_items 42016.
I0304 19:28:39.059256 23128000471168 run.py:483] Algo bellman_ford step 1313 current loss 0.133653, current_train_items 42048.
I0304 19:28:39.091838 23128000471168 run.py:483] Algo bellman_ford step 1314 current loss 0.119001, current_train_items 42080.
I0304 19:28:39.111000 23128000471168 run.py:483] Algo bellman_ford step 1315 current loss 0.005461, current_train_items 42112.
I0304 19:28:39.127392 23128000471168 run.py:483] Algo bellman_ford step 1316 current loss 0.021417, current_train_items 42144.
I0304 19:28:39.150937 23128000471168 run.py:483] Algo bellman_ford step 1317 current loss 0.167845, current_train_items 42176.
I0304 19:28:39.180833 23128000471168 run.py:483] Algo bellman_ford step 1318 current loss 0.103228, current_train_items 42208.
I0304 19:28:39.212747 23128000471168 run.py:483] Algo bellman_ford step 1319 current loss 0.198825, current_train_items 42240.
I0304 19:28:39.231889 23128000471168 run.py:483] Algo bellman_ford step 1320 current loss 0.018542, current_train_items 42272.
I0304 19:28:39.248398 23128000471168 run.py:483] Algo bellman_ford step 1321 current loss 0.036690, current_train_items 42304.
I0304 19:28:39.272422 23128000471168 run.py:483] Algo bellman_ford step 1322 current loss 0.095311, current_train_items 42336.
I0304 19:28:39.302779 23128000471168 run.py:483] Algo bellman_ford step 1323 current loss 0.078707, current_train_items 42368.
I0304 19:28:39.334329 23128000471168 run.py:483] Algo bellman_ford step 1324 current loss 0.140428, current_train_items 42400.
I0304 19:28:39.353651 23128000471168 run.py:483] Algo bellman_ford step 1325 current loss 0.034142, current_train_items 42432.
I0304 19:28:39.369758 23128000471168 run.py:483] Algo bellman_ford step 1326 current loss 0.033560, current_train_items 42464.
I0304 19:28:39.392575 23128000471168 run.py:483] Algo bellman_ford step 1327 current loss 0.088069, current_train_items 42496.
I0304 19:28:39.422403 23128000471168 run.py:483] Algo bellman_ford step 1328 current loss 0.120777, current_train_items 42528.
I0304 19:28:39.454844 23128000471168 run.py:483] Algo bellman_ford step 1329 current loss 0.132986, current_train_items 42560.
I0304 19:28:39.474090 23128000471168 run.py:483] Algo bellman_ford step 1330 current loss 0.006228, current_train_items 42592.
I0304 19:28:39.490362 23128000471168 run.py:483] Algo bellman_ford step 1331 current loss 0.071310, current_train_items 42624.
I0304 19:28:39.513258 23128000471168 run.py:483] Algo bellman_ford step 1332 current loss 0.141160, current_train_items 42656.
I0304 19:28:39.542633 23128000471168 run.py:483] Algo bellman_ford step 1333 current loss 0.164427, current_train_items 42688.
I0304 19:28:39.574046 23128000471168 run.py:483] Algo bellman_ford step 1334 current loss 0.117556, current_train_items 42720.
I0304 19:28:39.593131 23128000471168 run.py:483] Algo bellman_ford step 1335 current loss 0.008419, current_train_items 42752.
I0304 19:28:39.609054 23128000471168 run.py:483] Algo bellman_ford step 1336 current loss 0.046114, current_train_items 42784.
I0304 19:28:39.633321 23128000471168 run.py:483] Algo bellman_ford step 1337 current loss 0.075303, current_train_items 42816.
I0304 19:28:39.662756 23128000471168 run.py:483] Algo bellman_ford step 1338 current loss 0.168603, current_train_items 42848.
I0304 19:28:39.697329 23128000471168 run.py:483] Algo bellman_ford step 1339 current loss 0.161985, current_train_items 42880.
I0304 19:28:39.716281 23128000471168 run.py:483] Algo bellman_ford step 1340 current loss 0.038427, current_train_items 42912.
I0304 19:28:39.732602 23128000471168 run.py:483] Algo bellman_ford step 1341 current loss 0.025485, current_train_items 42944.
I0304 19:28:39.756193 23128000471168 run.py:483] Algo bellman_ford step 1342 current loss 0.118888, current_train_items 42976.
I0304 19:28:39.785758 23128000471168 run.py:483] Algo bellman_ford step 1343 current loss 0.177722, current_train_items 43008.
I0304 19:28:39.817856 23128000471168 run.py:483] Algo bellman_ford step 1344 current loss 0.176352, current_train_items 43040.
I0304 19:28:39.837108 23128000471168 run.py:483] Algo bellman_ford step 1345 current loss 0.005872, current_train_items 43072.
I0304 19:28:39.853672 23128000471168 run.py:483] Algo bellman_ford step 1346 current loss 0.089357, current_train_items 43104.
I0304 19:28:39.878140 23128000471168 run.py:483] Algo bellman_ford step 1347 current loss 0.233361, current_train_items 43136.
I0304 19:28:39.906842 23128000471168 run.py:483] Algo bellman_ford step 1348 current loss 0.197907, current_train_items 43168.
I0304 19:28:39.937891 23128000471168 run.py:483] Algo bellman_ford step 1349 current loss 0.126237, current_train_items 43200.
I0304 19:28:39.957323 23128000471168 run.py:483] Algo bellman_ford step 1350 current loss 0.052897, current_train_items 43232.
I0304 19:28:39.965723 23128000471168 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0304 19:28:39.965831 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:39.983078 23128000471168 run.py:483] Algo bellman_ford step 1351 current loss 0.042075, current_train_items 43264.
I0304 19:28:40.008149 23128000471168 run.py:483] Algo bellman_ford step 1352 current loss 0.072051, current_train_items 43296.
I0304 19:28:40.036680 23128000471168 run.py:483] Algo bellman_ford step 1353 current loss 0.106533, current_train_items 43328.
I0304 19:28:40.071639 23128000471168 run.py:483] Algo bellman_ford step 1354 current loss 0.173110, current_train_items 43360.
I0304 19:28:40.091618 23128000471168 run.py:483] Algo bellman_ford step 1355 current loss 0.016203, current_train_items 43392.
I0304 19:28:40.107733 23128000471168 run.py:483] Algo bellman_ford step 1356 current loss 0.080465, current_train_items 43424.
I0304 19:28:40.131072 23128000471168 run.py:483] Algo bellman_ford step 1357 current loss 0.091162, current_train_items 43456.
I0304 19:28:40.160812 23128000471168 run.py:483] Algo bellman_ford step 1358 current loss 0.097654, current_train_items 43488.
I0304 19:28:40.194242 23128000471168 run.py:483] Algo bellman_ford step 1359 current loss 0.232882, current_train_items 43520.
I0304 19:28:40.214104 23128000471168 run.py:483] Algo bellman_ford step 1360 current loss 0.004771, current_train_items 43552.
I0304 19:28:40.231428 23128000471168 run.py:483] Algo bellman_ford step 1361 current loss 0.129891, current_train_items 43584.
I0304 19:28:40.255257 23128000471168 run.py:483] Algo bellman_ford step 1362 current loss 0.171349, current_train_items 43616.
I0304 19:28:40.285233 23128000471168 run.py:483] Algo bellman_ford step 1363 current loss 0.156396, current_train_items 43648.
I0304 19:28:40.319045 23128000471168 run.py:483] Algo bellman_ford step 1364 current loss 0.190534, current_train_items 43680.
I0304 19:28:40.338880 23128000471168 run.py:483] Algo bellman_ford step 1365 current loss 0.005240, current_train_items 43712.
I0304 19:28:40.355036 23128000471168 run.py:483] Algo bellman_ford step 1366 current loss 0.046440, current_train_items 43744.
I0304 19:28:40.378233 23128000471168 run.py:483] Algo bellman_ford step 1367 current loss 0.070798, current_train_items 43776.
I0304 19:28:40.406244 23128000471168 run.py:483] Algo bellman_ford step 1368 current loss 0.135157, current_train_items 43808.
I0304 19:28:40.438328 23128000471168 run.py:483] Algo bellman_ford step 1369 current loss 0.164292, current_train_items 43840.
I0304 19:28:40.458091 23128000471168 run.py:483] Algo bellman_ford step 1370 current loss 0.004239, current_train_items 43872.
I0304 19:28:40.475134 23128000471168 run.py:483] Algo bellman_ford step 1371 current loss 0.082078, current_train_items 43904.
I0304 19:28:40.499063 23128000471168 run.py:483] Algo bellman_ford step 1372 current loss 0.111355, current_train_items 43936.
I0304 19:28:40.530175 23128000471168 run.py:483] Algo bellman_ford step 1373 current loss 0.145133, current_train_items 43968.
I0304 19:28:40.563310 23128000471168 run.py:483] Algo bellman_ford step 1374 current loss 0.115307, current_train_items 44000.
I0304 19:28:40.583066 23128000471168 run.py:483] Algo bellman_ford step 1375 current loss 0.020588, current_train_items 44032.
I0304 19:28:40.599126 23128000471168 run.py:483] Algo bellman_ford step 1376 current loss 0.043007, current_train_items 44064.
I0304 19:28:40.622724 23128000471168 run.py:483] Algo bellman_ford step 1377 current loss 0.161232, current_train_items 44096.
I0304 19:28:40.653376 23128000471168 run.py:483] Algo bellman_ford step 1378 current loss 0.127587, current_train_items 44128.
I0304 19:28:40.686015 23128000471168 run.py:483] Algo bellman_ford step 1379 current loss 0.108070, current_train_items 44160.
I0304 19:28:40.705521 23128000471168 run.py:483] Algo bellman_ford step 1380 current loss 0.039363, current_train_items 44192.
I0304 19:28:40.722135 23128000471168 run.py:483] Algo bellman_ford step 1381 current loss 0.049818, current_train_items 44224.
I0304 19:28:40.745533 23128000471168 run.py:483] Algo bellman_ford step 1382 current loss 0.115504, current_train_items 44256.
I0304 19:28:40.775551 23128000471168 run.py:483] Algo bellman_ford step 1383 current loss 0.195451, current_train_items 44288.
I0304 19:28:40.805745 23128000471168 run.py:483] Algo bellman_ford step 1384 current loss 0.196803, current_train_items 44320.
I0304 19:28:40.825768 23128000471168 run.py:483] Algo bellman_ford step 1385 current loss 0.006852, current_train_items 44352.
I0304 19:28:40.841957 23128000471168 run.py:483] Algo bellman_ford step 1386 current loss 0.025874, current_train_items 44384.
I0304 19:28:40.864396 23128000471168 run.py:483] Algo bellman_ford step 1387 current loss 0.084868, current_train_items 44416.
I0304 19:28:40.894680 23128000471168 run.py:483] Algo bellman_ford step 1388 current loss 0.154937, current_train_items 44448.
I0304 19:28:40.927402 23128000471168 run.py:483] Algo bellman_ford step 1389 current loss 0.192073, current_train_items 44480.
I0304 19:28:40.946972 23128000471168 run.py:483] Algo bellman_ford step 1390 current loss 0.013236, current_train_items 44512.
I0304 19:28:40.963202 23128000471168 run.py:483] Algo bellman_ford step 1391 current loss 0.050752, current_train_items 44544.
I0304 19:28:40.987194 23128000471168 run.py:483] Algo bellman_ford step 1392 current loss 0.084225, current_train_items 44576.
I0304 19:28:41.017030 23128000471168 run.py:483] Algo bellman_ford step 1393 current loss 0.164183, current_train_items 44608.
I0304 19:28:41.049471 23128000471168 run.py:483] Algo bellman_ford step 1394 current loss 0.164344, current_train_items 44640.
I0304 19:28:41.068694 23128000471168 run.py:483] Algo bellman_ford step 1395 current loss 0.006656, current_train_items 44672.
I0304 19:28:41.085134 23128000471168 run.py:483] Algo bellman_ford step 1396 current loss 0.040033, current_train_items 44704.
I0304 19:28:41.109379 23128000471168 run.py:483] Algo bellman_ford step 1397 current loss 0.118014, current_train_items 44736.
I0304 19:28:41.140158 23128000471168 run.py:483] Algo bellman_ford step 1398 current loss 0.215507, current_train_items 44768.
I0304 19:28:41.171452 23128000471168 run.py:483] Algo bellman_ford step 1399 current loss 0.189407, current_train_items 44800.
I0304 19:28:41.191597 23128000471168 run.py:483] Algo bellman_ford step 1400 current loss 0.020296, current_train_items 44832.
I0304 19:28:41.199609 23128000471168 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0304 19:28:41.199714 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:28:41.216528 23128000471168 run.py:483] Algo bellman_ford step 1401 current loss 0.048312, current_train_items 44864.
I0304 19:28:41.241118 23128000471168 run.py:483] Algo bellman_ford step 1402 current loss 0.159564, current_train_items 44896.
I0304 19:28:41.271878 23128000471168 run.py:483] Algo bellman_ford step 1403 current loss 0.202039, current_train_items 44928.
I0304 19:28:41.305530 23128000471168 run.py:483] Algo bellman_ford step 1404 current loss 0.162778, current_train_items 44960.
I0304 19:28:41.325310 23128000471168 run.py:483] Algo bellman_ford step 1405 current loss 0.008000, current_train_items 44992.
I0304 19:28:41.341854 23128000471168 run.py:483] Algo bellman_ford step 1406 current loss 0.054141, current_train_items 45024.
I0304 19:28:41.365635 23128000471168 run.py:483] Algo bellman_ford step 1407 current loss 0.091162, current_train_items 45056.
I0304 19:28:41.395607 23128000471168 run.py:483] Algo bellman_ford step 1408 current loss 0.128594, current_train_items 45088.
I0304 19:28:41.428327 23128000471168 run.py:483] Algo bellman_ford step 1409 current loss 0.132460, current_train_items 45120.
I0304 19:28:41.447427 23128000471168 run.py:483] Algo bellman_ford step 1410 current loss 0.006345, current_train_items 45152.
I0304 19:28:41.464082 23128000471168 run.py:483] Algo bellman_ford step 1411 current loss 0.025058, current_train_items 45184.
I0304 19:28:41.487581 23128000471168 run.py:483] Algo bellman_ford step 1412 current loss 0.069731, current_train_items 45216.
I0304 19:28:41.515793 23128000471168 run.py:483] Algo bellman_ford step 1413 current loss 0.058762, current_train_items 45248.
I0304 19:28:41.547358 23128000471168 run.py:483] Algo bellman_ford step 1414 current loss 0.111899, current_train_items 45280.
I0304 19:28:41.566721 23128000471168 run.py:483] Algo bellman_ford step 1415 current loss 0.014183, current_train_items 45312.
I0304 19:28:41.583505 23128000471168 run.py:483] Algo bellman_ford step 1416 current loss 0.034565, current_train_items 45344.
I0304 19:28:41.606632 23128000471168 run.py:483] Algo bellman_ford step 1417 current loss 0.040216, current_train_items 45376.
I0304 19:28:41.635909 23128000471168 run.py:483] Algo bellman_ford step 1418 current loss 0.146349, current_train_items 45408.
I0304 19:28:41.668869 23128000471168 run.py:483] Algo bellman_ford step 1419 current loss 0.168166, current_train_items 45440.
I0304 19:28:41.688415 23128000471168 run.py:483] Algo bellman_ford step 1420 current loss 0.035741, current_train_items 45472.
I0304 19:28:41.704787 23128000471168 run.py:483] Algo bellman_ford step 1421 current loss 0.042709, current_train_items 45504.
I0304 19:28:41.728918 23128000471168 run.py:483] Algo bellman_ford step 1422 current loss 0.108023, current_train_items 45536.
I0304 19:28:41.759946 23128000471168 run.py:483] Algo bellman_ford step 1423 current loss 0.110248, current_train_items 45568.
I0304 19:28:41.791402 23128000471168 run.py:483] Algo bellman_ford step 1424 current loss 0.091884, current_train_items 45600.
I0304 19:28:41.811200 23128000471168 run.py:483] Algo bellman_ford step 1425 current loss 0.005508, current_train_items 45632.
I0304 19:28:41.827961 23128000471168 run.py:483] Algo bellman_ford step 1426 current loss 0.141833, current_train_items 45664.
I0304 19:28:41.852446 23128000471168 run.py:483] Algo bellman_ford step 1427 current loss 0.099268, current_train_items 45696.
I0304 19:28:41.882439 23128000471168 run.py:483] Algo bellman_ford step 1428 current loss 0.084893, current_train_items 45728.
I0304 19:28:41.913232 23128000471168 run.py:483] Algo bellman_ford step 1429 current loss 0.135494, current_train_items 45760.
I0304 19:28:41.932214 23128000471168 run.py:483] Algo bellman_ford step 1430 current loss 0.022092, current_train_items 45792.
I0304 19:28:41.948301 23128000471168 run.py:483] Algo bellman_ford step 1431 current loss 0.024254, current_train_items 45824.
I0304 19:28:41.973110 23128000471168 run.py:483] Algo bellman_ford step 1432 current loss 0.160402, current_train_items 45856.
I0304 19:28:42.001782 23128000471168 run.py:483] Algo bellman_ford step 1433 current loss 0.106141, current_train_items 45888.
I0304 19:28:42.032743 23128000471168 run.py:483] Algo bellman_ford step 1434 current loss 0.103059, current_train_items 45920.
I0304 19:28:42.051979 23128000471168 run.py:483] Algo bellman_ford step 1435 current loss 0.004704, current_train_items 45952.
I0304 19:28:42.068185 23128000471168 run.py:483] Algo bellman_ford step 1436 current loss 0.049217, current_train_items 45984.
I0304 19:28:42.091785 23128000471168 run.py:483] Algo bellman_ford step 1437 current loss 0.106196, current_train_items 46016.
I0304 19:28:42.120858 23128000471168 run.py:483] Algo bellman_ford step 1438 current loss 0.134440, current_train_items 46048.
I0304 19:28:42.154294 23128000471168 run.py:483] Algo bellman_ford step 1439 current loss 0.194361, current_train_items 46080.
I0304 19:28:42.173533 23128000471168 run.py:483] Algo bellman_ford step 1440 current loss 0.005040, current_train_items 46112.
I0304 19:28:42.190082 23128000471168 run.py:483] Algo bellman_ford step 1441 current loss 0.048214, current_train_items 46144.
I0304 19:28:42.213245 23128000471168 run.py:483] Algo bellman_ford step 1442 current loss 0.052799, current_train_items 46176.
I0304 19:28:42.242892 23128000471168 run.py:483] Algo bellman_ford step 1443 current loss 0.132155, current_train_items 46208.
I0304 19:28:42.278894 23128000471168 run.py:483] Algo bellman_ford step 1444 current loss 0.154070, current_train_items 46240.
I0304 19:28:42.298204 23128000471168 run.py:483] Algo bellman_ford step 1445 current loss 0.008197, current_train_items 46272.
I0304 19:28:42.314039 23128000471168 run.py:483] Algo bellman_ford step 1446 current loss 0.073167, current_train_items 46304.
I0304 19:28:42.337996 23128000471168 run.py:483] Algo bellman_ford step 1447 current loss 0.076780, current_train_items 46336.
I0304 19:28:42.367515 23128000471168 run.py:483] Algo bellman_ford step 1448 current loss 0.158700, current_train_items 46368.
I0304 19:28:42.399377 23128000471168 run.py:483] Algo bellman_ford step 1449 current loss 0.107337, current_train_items 46400.
I0304 19:28:42.418880 23128000471168 run.py:483] Algo bellman_ford step 1450 current loss 0.008421, current_train_items 46432.
I0304 19:28:42.426773 23128000471168 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0304 19:28:42.426879 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0304 19:28:42.443391 23128000471168 run.py:483] Algo bellman_ford step 1451 current loss 0.064868, current_train_items 46464.
I0304 19:28:42.467175 23128000471168 run.py:483] Algo bellman_ford step 1452 current loss 0.089030, current_train_items 46496.
I0304 19:28:42.498447 23128000471168 run.py:483] Algo bellman_ford step 1453 current loss 0.213780, current_train_items 46528.
I0304 19:28:42.532355 23128000471168 run.py:483] Algo bellman_ford step 1454 current loss 0.164319, current_train_items 46560.
I0304 19:28:42.552082 23128000471168 run.py:483] Algo bellman_ford step 1455 current loss 0.018690, current_train_items 46592.
I0304 19:28:42.568223 23128000471168 run.py:483] Algo bellman_ford step 1456 current loss 0.038260, current_train_items 46624.
I0304 19:28:42.591274 23128000471168 run.py:483] Algo bellman_ford step 1457 current loss 0.069257, current_train_items 46656.
I0304 19:28:42.621737 23128000471168 run.py:483] Algo bellman_ford step 1458 current loss 0.236514, current_train_items 46688.
I0304 19:28:42.653828 23128000471168 run.py:483] Algo bellman_ford step 1459 current loss 0.172967, current_train_items 46720.
I0304 19:28:42.673567 23128000471168 run.py:483] Algo bellman_ford step 1460 current loss 0.011447, current_train_items 46752.
I0304 19:28:42.689641 23128000471168 run.py:483] Algo bellman_ford step 1461 current loss 0.046726, current_train_items 46784.
I0304 19:28:42.712429 23128000471168 run.py:483] Algo bellman_ford step 1462 current loss 0.141803, current_train_items 46816.
I0304 19:28:42.742221 23128000471168 run.py:483] Algo bellman_ford step 1463 current loss 0.212167, current_train_items 46848.
I0304 19:28:42.773442 23128000471168 run.py:483] Algo bellman_ford step 1464 current loss 0.151497, current_train_items 46880.
I0304 19:28:42.793047 23128000471168 run.py:483] Algo bellman_ford step 1465 current loss 0.037717, current_train_items 46912.
I0304 19:28:42.809653 23128000471168 run.py:483] Algo bellman_ford step 1466 current loss 0.050924, current_train_items 46944.
I0304 19:28:42.833249 23128000471168 run.py:483] Algo bellman_ford step 1467 current loss 0.114223, current_train_items 46976.
I0304 19:28:42.861259 23128000471168 run.py:483] Algo bellman_ford step 1468 current loss 0.122865, current_train_items 47008.
I0304 19:28:42.892191 23128000471168 run.py:483] Algo bellman_ford step 1469 current loss 0.102233, current_train_items 47040.
I0304 19:28:42.911968 23128000471168 run.py:483] Algo bellman_ford step 1470 current loss 0.006687, current_train_items 47072.
I0304 19:28:42.929016 23128000471168 run.py:483] Algo bellman_ford step 1471 current loss 0.048753, current_train_items 47104.
I0304 19:28:42.951965 23128000471168 run.py:483] Algo bellman_ford step 1472 current loss 0.075479, current_train_items 47136.
I0304 19:28:42.981742 23128000471168 run.py:483] Algo bellman_ford step 1473 current loss 0.199531, current_train_items 47168.
I0304 19:28:43.014879 23128000471168 run.py:483] Algo bellman_ford step 1474 current loss 0.134790, current_train_items 47200.
I0304 19:28:43.034836 23128000471168 run.py:483] Algo bellman_ford step 1475 current loss 0.015369, current_train_items 47232.
I0304 19:28:43.051280 23128000471168 run.py:483] Algo bellman_ford step 1476 current loss 0.026112, current_train_items 47264.
I0304 19:28:43.074679 23128000471168 run.py:483] Algo bellman_ford step 1477 current loss 0.104317, current_train_items 47296.
I0304 19:28:43.104335 23128000471168 run.py:483] Algo bellman_ford step 1478 current loss 0.274245, current_train_items 47328.
I0304 19:28:43.137768 23128000471168 run.py:483] Algo bellman_ford step 1479 current loss 0.202487, current_train_items 47360.
I0304 19:28:43.156833 23128000471168 run.py:483] Algo bellman_ford step 1480 current loss 0.005087, current_train_items 47392.
I0304 19:28:43.172822 23128000471168 run.py:483] Algo bellman_ford step 1481 current loss 0.019231, current_train_items 47424.
I0304 19:28:43.195592 23128000471168 run.py:483] Algo bellman_ford step 1482 current loss 0.121866, current_train_items 47456.
I0304 19:28:43.225936 23128000471168 run.py:483] Algo bellman_ford step 1483 current loss 0.188741, current_train_items 47488.
I0304 19:28:43.256697 23128000471168 run.py:483] Algo bellman_ford step 1484 current loss 0.146585, current_train_items 47520.
I0304 19:28:43.276159 23128000471168 run.py:483] Algo bellman_ford step 1485 current loss 0.017799, current_train_items 47552.
I0304 19:28:43.292144 23128000471168 run.py:483] Algo bellman_ford step 1486 current loss 0.032119, current_train_items 47584.
I0304 19:28:43.316329 23128000471168 run.py:483] Algo bellman_ford step 1487 current loss 0.116461, current_train_items 47616.
I0304 19:28:43.344241 23128000471168 run.py:483] Algo bellman_ford step 1488 current loss 0.127745, current_train_items 47648.
I0304 19:28:43.377469 23128000471168 run.py:483] Algo bellman_ford step 1489 current loss 0.163773, current_train_items 47680.
I0304 19:28:43.396913 23128000471168 run.py:483] Algo bellman_ford step 1490 current loss 0.005616, current_train_items 47712.
I0304 19:28:43.413517 23128000471168 run.py:483] Algo bellman_ford step 1491 current loss 0.044431, current_train_items 47744.
I0304 19:28:43.436888 23128000471168 run.py:483] Algo bellman_ford step 1492 current loss 0.047402, current_train_items 47776.
I0304 19:28:43.466452 23128000471168 run.py:483] Algo bellman_ford step 1493 current loss 0.135821, current_train_items 47808.
I0304 19:28:43.500121 23128000471168 run.py:483] Algo bellman_ford step 1494 current loss 0.140087, current_train_items 47840.
I0304 19:28:43.519450 23128000471168 run.py:483] Algo bellman_ford step 1495 current loss 0.005240, current_train_items 47872.
I0304 19:28:43.536089 23128000471168 run.py:483] Algo bellman_ford step 1496 current loss 0.073662, current_train_items 47904.
I0304 19:28:43.560069 23128000471168 run.py:483] Algo bellman_ford step 1497 current loss 0.122182, current_train_items 47936.
I0304 19:28:43.590450 23128000471168 run.py:483] Algo bellman_ford step 1498 current loss 0.155748, current_train_items 47968.
I0304 19:28:43.624349 23128000471168 run.py:483] Algo bellman_ford step 1499 current loss 0.145010, current_train_items 48000.
I0304 19:28:43.643796 23128000471168 run.py:483] Algo bellman_ford step 1500 current loss 0.006426, current_train_items 48032.
I0304 19:28:43.651570 23128000471168 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0304 19:28:43.651679 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:28:43.669086 23128000471168 run.py:483] Algo bellman_ford step 1501 current loss 0.038640, current_train_items 48064.
I0304 19:28:43.693994 23128000471168 run.py:483] Algo bellman_ford step 1502 current loss 0.101498, current_train_items 48096.
I0304 19:28:43.724244 23128000471168 run.py:483] Algo bellman_ford step 1503 current loss 0.124661, current_train_items 48128.
I0304 19:28:43.756597 23128000471168 run.py:483] Algo bellman_ford step 1504 current loss 0.097747, current_train_items 48160.
I0304 19:28:43.776044 23128000471168 run.py:483] Algo bellman_ford step 1505 current loss 0.007570, current_train_items 48192.
I0304 19:28:43.792251 23128000471168 run.py:483] Algo bellman_ford step 1506 current loss 0.044136, current_train_items 48224.
I0304 19:28:43.815283 23128000471168 run.py:483] Algo bellman_ford step 1507 current loss 0.071721, current_train_items 48256.
I0304 19:28:43.844763 23128000471168 run.py:483] Algo bellman_ford step 1508 current loss 0.117400, current_train_items 48288.
I0304 19:28:43.876889 23128000471168 run.py:483] Algo bellman_ford step 1509 current loss 0.126558, current_train_items 48320.
I0304 19:28:43.896342 23128000471168 run.py:483] Algo bellman_ford step 1510 current loss 0.023851, current_train_items 48352.
I0304 19:28:43.912862 23128000471168 run.py:483] Algo bellman_ford step 1511 current loss 0.045040, current_train_items 48384.
I0304 19:28:43.937038 23128000471168 run.py:483] Algo bellman_ford step 1512 current loss 0.139074, current_train_items 48416.
I0304 19:28:43.965234 23128000471168 run.py:483] Algo bellman_ford step 1513 current loss 0.075520, current_train_items 48448.
I0304 19:28:43.999558 23128000471168 run.py:483] Algo bellman_ford step 1514 current loss 0.230433, current_train_items 48480.
I0304 19:28:44.018889 23128000471168 run.py:483] Algo bellman_ford step 1515 current loss 0.006141, current_train_items 48512.
I0304 19:28:44.034912 23128000471168 run.py:483] Algo bellman_ford step 1516 current loss 0.015716, current_train_items 48544.
I0304 19:28:44.058593 23128000471168 run.py:483] Algo bellman_ford step 1517 current loss 0.112050, current_train_items 48576.
I0304 19:28:44.088744 23128000471168 run.py:483] Algo bellman_ford step 1518 current loss 0.160925, current_train_items 48608.
I0304 19:28:44.121070 23128000471168 run.py:483] Algo bellman_ford step 1519 current loss 0.193242, current_train_items 48640.
I0304 19:28:44.140726 23128000471168 run.py:483] Algo bellman_ford step 1520 current loss 0.007554, current_train_items 48672.
I0304 19:28:44.156810 23128000471168 run.py:483] Algo bellman_ford step 1521 current loss 0.027626, current_train_items 48704.
I0304 19:28:44.181052 23128000471168 run.py:483] Algo bellman_ford step 1522 current loss 0.088634, current_train_items 48736.
I0304 19:28:44.210717 23128000471168 run.py:483] Algo bellman_ford step 1523 current loss 0.151665, current_train_items 48768.
I0304 19:28:44.243314 23128000471168 run.py:483] Algo bellman_ford step 1524 current loss 0.148014, current_train_items 48800.
I0304 19:28:44.263012 23128000471168 run.py:483] Algo bellman_ford step 1525 current loss 0.007236, current_train_items 48832.
I0304 19:28:44.279009 23128000471168 run.py:483] Algo bellman_ford step 1526 current loss 0.021675, current_train_items 48864.
I0304 19:28:44.303737 23128000471168 run.py:483] Algo bellman_ford step 1527 current loss 0.130777, current_train_items 48896.
I0304 19:28:44.333799 23128000471168 run.py:483] Algo bellman_ford step 1528 current loss 0.156351, current_train_items 48928.
I0304 19:28:44.366543 23128000471168 run.py:483] Algo bellman_ford step 1529 current loss 0.183030, current_train_items 48960.
I0304 19:28:44.385619 23128000471168 run.py:483] Algo bellman_ford step 1530 current loss 0.015750, current_train_items 48992.
I0304 19:28:44.401733 23128000471168 run.py:483] Algo bellman_ford step 1531 current loss 0.035922, current_train_items 49024.
I0304 19:28:44.425418 23128000471168 run.py:483] Algo bellman_ford step 1532 current loss 0.078269, current_train_items 49056.
I0304 19:28:44.455749 23128000471168 run.py:483] Algo bellman_ford step 1533 current loss 0.183269, current_train_items 49088.
I0304 19:28:44.489300 23128000471168 run.py:483] Algo bellman_ford step 1534 current loss 0.175146, current_train_items 49120.
I0304 19:28:44.508716 23128000471168 run.py:483] Algo bellman_ford step 1535 current loss 0.028600, current_train_items 49152.
I0304 19:28:44.525365 23128000471168 run.py:483] Algo bellman_ford step 1536 current loss 0.052238, current_train_items 49184.
I0304 19:28:44.549107 23128000471168 run.py:483] Algo bellman_ford step 1537 current loss 0.153097, current_train_items 49216.
I0304 19:28:44.579312 23128000471168 run.py:483] Algo bellman_ford step 1538 current loss 0.157158, current_train_items 49248.
I0304 19:28:44.614167 23128000471168 run.py:483] Algo bellman_ford step 1539 current loss 0.340008, current_train_items 49280.
I0304 19:28:44.633680 23128000471168 run.py:483] Algo bellman_ford step 1540 current loss 0.011434, current_train_items 49312.
I0304 19:28:44.649647 23128000471168 run.py:483] Algo bellman_ford step 1541 current loss 0.048291, current_train_items 49344.
I0304 19:28:44.672996 23128000471168 run.py:483] Algo bellman_ford step 1542 current loss 0.082077, current_train_items 49376.
I0304 19:28:44.702809 23128000471168 run.py:483] Algo bellman_ford step 1543 current loss 0.135623, current_train_items 49408.
I0304 19:28:44.735302 23128000471168 run.py:483] Algo bellman_ford step 1544 current loss 0.174041, current_train_items 49440.
I0304 19:28:44.754828 23128000471168 run.py:483] Algo bellman_ford step 1545 current loss 0.030099, current_train_items 49472.
I0304 19:28:44.771160 23128000471168 run.py:483] Algo bellman_ford step 1546 current loss 0.048999, current_train_items 49504.
I0304 19:28:44.794917 23128000471168 run.py:483] Algo bellman_ford step 1547 current loss 0.087514, current_train_items 49536.
I0304 19:28:44.825845 23128000471168 run.py:483] Algo bellman_ford step 1548 current loss 0.079060, current_train_items 49568.
I0304 19:28:44.860072 23128000471168 run.py:483] Algo bellman_ford step 1549 current loss 0.227007, current_train_items 49600.
I0304 19:28:44.879321 23128000471168 run.py:483] Algo bellman_ford step 1550 current loss 0.038056, current_train_items 49632.
I0304 19:28:44.887163 23128000471168 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0304 19:28:44.887270 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:44.904803 23128000471168 run.py:483] Algo bellman_ford step 1551 current loss 0.055588, current_train_items 49664.
I0304 19:28:44.928757 23128000471168 run.py:483] Algo bellman_ford step 1552 current loss 0.062351, current_train_items 49696.
I0304 19:28:44.958965 23128000471168 run.py:483] Algo bellman_ford step 1553 current loss 0.080776, current_train_items 49728.
I0304 19:28:44.990255 23128000471168 run.py:483] Algo bellman_ford step 1554 current loss 0.088308, current_train_items 49760.
I0304 19:28:45.009432 23128000471168 run.py:483] Algo bellman_ford step 1555 current loss 0.004559, current_train_items 49792.
I0304 19:28:45.025416 23128000471168 run.py:483] Algo bellman_ford step 1556 current loss 0.034211, current_train_items 49824.
I0304 19:28:45.049312 23128000471168 run.py:483] Algo bellman_ford step 1557 current loss 0.086288, current_train_items 49856.
I0304 19:28:45.079715 23128000471168 run.py:483] Algo bellman_ford step 1558 current loss 0.104929, current_train_items 49888.
I0304 19:28:45.112620 23128000471168 run.py:483] Algo bellman_ford step 1559 current loss 0.097267, current_train_items 49920.
I0304 19:28:45.132223 23128000471168 run.py:483] Algo bellman_ford step 1560 current loss 0.007384, current_train_items 49952.
I0304 19:28:45.148750 23128000471168 run.py:483] Algo bellman_ford step 1561 current loss 0.021476, current_train_items 49984.
I0304 19:28:45.172402 23128000471168 run.py:483] Algo bellman_ford step 1562 current loss 0.083043, current_train_items 50016.
I0304 19:28:45.202789 23128000471168 run.py:483] Algo bellman_ford step 1563 current loss 0.087863, current_train_items 50048.
I0304 19:28:45.236424 23128000471168 run.py:483] Algo bellman_ford step 1564 current loss 0.127666, current_train_items 50080.
I0304 19:28:45.255627 23128000471168 run.py:483] Algo bellman_ford step 1565 current loss 0.006611, current_train_items 50112.
I0304 19:28:45.271952 23128000471168 run.py:483] Algo bellman_ford step 1566 current loss 0.037253, current_train_items 50144.
I0304 19:28:45.295662 23128000471168 run.py:483] Algo bellman_ford step 1567 current loss 0.096878, current_train_items 50176.
I0304 19:28:45.325150 23128000471168 run.py:483] Algo bellman_ford step 1568 current loss 0.096253, current_train_items 50208.
I0304 19:28:45.358893 23128000471168 run.py:483] Algo bellman_ford step 1569 current loss 0.114610, current_train_items 50240.
I0304 19:28:45.378341 23128000471168 run.py:483] Algo bellman_ford step 1570 current loss 0.010555, current_train_items 50272.
I0304 19:28:45.395037 23128000471168 run.py:483] Algo bellman_ford step 1571 current loss 0.026829, current_train_items 50304.
I0304 19:28:45.419281 23128000471168 run.py:483] Algo bellman_ford step 1572 current loss 0.084616, current_train_items 50336.
I0304 19:28:45.448823 23128000471168 run.py:483] Algo bellman_ford step 1573 current loss 0.081163, current_train_items 50368.
I0304 19:28:45.479093 23128000471168 run.py:483] Algo bellman_ford step 1574 current loss 0.113363, current_train_items 50400.
I0304 19:28:45.498835 23128000471168 run.py:483] Algo bellman_ford step 1575 current loss 0.006792, current_train_items 50432.
I0304 19:28:45.515327 23128000471168 run.py:483] Algo bellman_ford step 1576 current loss 0.031077, current_train_items 50464.
I0304 19:28:45.537575 23128000471168 run.py:483] Algo bellman_ford step 1577 current loss 0.058063, current_train_items 50496.
I0304 19:28:45.568622 23128000471168 run.py:483] Algo bellman_ford step 1578 current loss 0.134341, current_train_items 50528.
I0304 19:28:45.600798 23128000471168 run.py:483] Algo bellman_ford step 1579 current loss 0.078418, current_train_items 50560.
I0304 19:28:45.619963 23128000471168 run.py:483] Algo bellman_ford step 1580 current loss 0.032751, current_train_items 50592.
I0304 19:28:45.635916 23128000471168 run.py:483] Algo bellman_ford step 1581 current loss 0.101328, current_train_items 50624.
I0304 19:28:45.659145 23128000471168 run.py:483] Algo bellman_ford step 1582 current loss 0.096077, current_train_items 50656.
I0304 19:28:45.689215 23128000471168 run.py:483] Algo bellman_ford step 1583 current loss 0.072668, current_train_items 50688.
I0304 19:28:45.722445 23128000471168 run.py:483] Algo bellman_ford step 1584 current loss 0.138190, current_train_items 50720.
I0304 19:28:45.742051 23128000471168 run.py:483] Algo bellman_ford step 1585 current loss 0.033264, current_train_items 50752.
I0304 19:28:45.757975 23128000471168 run.py:483] Algo bellman_ford step 1586 current loss 0.032799, current_train_items 50784.
I0304 19:28:45.781331 23128000471168 run.py:483] Algo bellman_ford step 1587 current loss 0.116314, current_train_items 50816.
I0304 19:28:45.810841 23128000471168 run.py:483] Algo bellman_ford step 1588 current loss 0.134976, current_train_items 50848.
I0304 19:28:45.843048 23128000471168 run.py:483] Algo bellman_ford step 1589 current loss 0.115424, current_train_items 50880.
I0304 19:28:45.862473 23128000471168 run.py:483] Algo bellman_ford step 1590 current loss 0.015881, current_train_items 50912.
I0304 19:28:45.878962 23128000471168 run.py:483] Algo bellman_ford step 1591 current loss 0.046624, current_train_items 50944.
I0304 19:28:45.901827 23128000471168 run.py:483] Algo bellman_ford step 1592 current loss 0.136599, current_train_items 50976.
I0304 19:28:45.932477 23128000471168 run.py:483] Algo bellman_ford step 1593 current loss 0.116873, current_train_items 51008.
I0304 19:28:45.966060 23128000471168 run.py:483] Algo bellman_ford step 1594 current loss 0.101969, current_train_items 51040.
I0304 19:28:45.985438 23128000471168 run.py:483] Algo bellman_ford step 1595 current loss 0.005325, current_train_items 51072.
I0304 19:28:46.001288 23128000471168 run.py:483] Algo bellman_ford step 1596 current loss 0.031651, current_train_items 51104.
I0304 19:28:46.025315 23128000471168 run.py:483] Algo bellman_ford step 1597 current loss 0.148425, current_train_items 51136.
I0304 19:28:46.053840 23128000471168 run.py:483] Algo bellman_ford step 1598 current loss 0.156473, current_train_items 51168.
I0304 19:28:46.086341 23128000471168 run.py:483] Algo bellman_ford step 1599 current loss 0.148578, current_train_items 51200.
I0304 19:28:46.105746 23128000471168 run.py:483] Algo bellman_ford step 1600 current loss 0.012771, current_train_items 51232.
I0304 19:28:46.113306 23128000471168 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0304 19:28:46.113415 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:46.130206 23128000471168 run.py:483] Algo bellman_ford step 1601 current loss 0.036151, current_train_items 51264.
I0304 19:28:46.155278 23128000471168 run.py:483] Algo bellman_ford step 1602 current loss 0.060647, current_train_items 51296.
I0304 19:28:46.185353 23128000471168 run.py:483] Algo bellman_ford step 1603 current loss 0.177154, current_train_items 51328.
I0304 19:28:46.218538 23128000471168 run.py:483] Algo bellman_ford step 1604 current loss 0.160316, current_train_items 51360.
I0304 19:28:46.238025 23128000471168 run.py:483] Algo bellman_ford step 1605 current loss 0.015620, current_train_items 51392.
I0304 19:28:46.253661 23128000471168 run.py:483] Algo bellman_ford step 1606 current loss 0.073476, current_train_items 51424.
I0304 19:28:46.276824 23128000471168 run.py:483] Algo bellman_ford step 1607 current loss 0.071881, current_train_items 51456.
I0304 19:28:46.307543 23128000471168 run.py:483] Algo bellman_ford step 1608 current loss 0.212438, current_train_items 51488.
I0304 19:28:46.340594 23128000471168 run.py:483] Algo bellman_ford step 1609 current loss 0.308555, current_train_items 51520.
I0304 19:28:46.359842 23128000471168 run.py:483] Algo bellman_ford step 1610 current loss 0.062089, current_train_items 51552.
I0304 19:28:46.376381 23128000471168 run.py:483] Algo bellman_ford step 1611 current loss 0.024378, current_train_items 51584.
I0304 19:28:46.400206 23128000471168 run.py:483] Algo bellman_ford step 1612 current loss 0.060858, current_train_items 51616.
I0304 19:28:46.430249 23128000471168 run.py:483] Algo bellman_ford step 1613 current loss 0.164631, current_train_items 51648.
I0304 19:28:46.461959 23128000471168 run.py:483] Algo bellman_ford step 1614 current loss 0.175156, current_train_items 51680.
I0304 19:28:46.481239 23128000471168 run.py:483] Algo bellman_ford step 1615 current loss 0.007572, current_train_items 51712.
I0304 19:28:46.497793 23128000471168 run.py:483] Algo bellman_ford step 1616 current loss 0.029867, current_train_items 51744.
I0304 19:28:46.521477 23128000471168 run.py:483] Algo bellman_ford step 1617 current loss 0.115119, current_train_items 51776.
I0304 19:28:46.550298 23128000471168 run.py:483] Algo bellman_ford step 1618 current loss 0.073267, current_train_items 51808.
I0304 19:28:46.584217 23128000471168 run.py:483] Algo bellman_ford step 1619 current loss 0.118394, current_train_items 51840.
I0304 19:28:46.603455 23128000471168 run.py:483] Algo bellman_ford step 1620 current loss 0.014569, current_train_items 51872.
I0304 19:28:46.619548 23128000471168 run.py:483] Algo bellman_ford step 1621 current loss 0.033681, current_train_items 51904.
I0304 19:28:46.643338 23128000471168 run.py:483] Algo bellman_ford step 1622 current loss 0.100363, current_train_items 51936.
I0304 19:28:46.673416 23128000471168 run.py:483] Algo bellman_ford step 1623 current loss 0.104682, current_train_items 51968.
I0304 19:28:46.705887 23128000471168 run.py:483] Algo bellman_ford step 1624 current loss 0.103701, current_train_items 52000.
I0304 19:28:46.725235 23128000471168 run.py:483] Algo bellman_ford step 1625 current loss 0.004641, current_train_items 52032.
I0304 19:28:46.741683 23128000471168 run.py:483] Algo bellman_ford step 1626 current loss 0.055313, current_train_items 52064.
I0304 19:28:46.765277 23128000471168 run.py:483] Algo bellman_ford step 1627 current loss 0.098620, current_train_items 52096.
I0304 19:28:46.796402 23128000471168 run.py:483] Algo bellman_ford step 1628 current loss 0.119166, current_train_items 52128.
I0304 19:28:46.828248 23128000471168 run.py:483] Algo bellman_ford step 1629 current loss 0.170850, current_train_items 52160.
I0304 19:28:46.847469 23128000471168 run.py:483] Algo bellman_ford step 1630 current loss 0.018897, current_train_items 52192.
I0304 19:28:46.864252 23128000471168 run.py:483] Algo bellman_ford step 1631 current loss 0.040660, current_train_items 52224.
I0304 19:28:46.888432 23128000471168 run.py:483] Algo bellman_ford step 1632 current loss 0.112663, current_train_items 52256.
I0304 19:28:46.918053 23128000471168 run.py:483] Algo bellman_ford step 1633 current loss 0.079170, current_train_items 52288.
I0304 19:28:46.951098 23128000471168 run.py:483] Algo bellman_ford step 1634 current loss 0.161879, current_train_items 52320.
I0304 19:28:46.970394 23128000471168 run.py:483] Algo bellman_ford step 1635 current loss 0.004943, current_train_items 52352.
I0304 19:28:46.986735 23128000471168 run.py:483] Algo bellman_ford step 1636 current loss 0.033527, current_train_items 52384.
I0304 19:28:47.010634 23128000471168 run.py:483] Algo bellman_ford step 1637 current loss 0.100785, current_train_items 52416.
I0304 19:28:47.041044 23128000471168 run.py:483] Algo bellman_ford step 1638 current loss 0.123789, current_train_items 52448.
I0304 19:28:47.076233 23128000471168 run.py:483] Algo bellman_ford step 1639 current loss 0.127875, current_train_items 52480.
I0304 19:28:47.095194 23128000471168 run.py:483] Algo bellman_ford step 1640 current loss 0.046681, current_train_items 52512.
I0304 19:28:47.111150 23128000471168 run.py:483] Algo bellman_ford step 1641 current loss 0.020342, current_train_items 52544.
I0304 19:28:47.134839 23128000471168 run.py:483] Algo bellman_ford step 1642 current loss 0.080727, current_train_items 52576.
I0304 19:28:47.164492 23128000471168 run.py:483] Algo bellman_ford step 1643 current loss 0.112309, current_train_items 52608.
I0304 19:28:47.196191 23128000471168 run.py:483] Algo bellman_ford step 1644 current loss 0.169202, current_train_items 52640.
I0304 19:28:47.215253 23128000471168 run.py:483] Algo bellman_ford step 1645 current loss 0.018245, current_train_items 52672.
I0304 19:28:47.231517 23128000471168 run.py:483] Algo bellman_ford step 1646 current loss 0.022882, current_train_items 52704.
I0304 19:28:47.256013 23128000471168 run.py:483] Algo bellman_ford step 1647 current loss 0.081888, current_train_items 52736.
I0304 19:28:47.284639 23128000471168 run.py:483] Algo bellman_ford step 1648 current loss 0.059818, current_train_items 52768.
I0304 19:28:47.317783 23128000471168 run.py:483] Algo bellman_ford step 1649 current loss 0.153989, current_train_items 52800.
I0304 19:28:47.336953 23128000471168 run.py:483] Algo bellman_ford step 1650 current loss 0.021301, current_train_items 52832.
I0304 19:28:47.345071 23128000471168 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0304 19:28:47.345184 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:28:47.362820 23128000471168 run.py:483] Algo bellman_ford step 1651 current loss 0.049820, current_train_items 52864.
I0304 19:28:47.388025 23128000471168 run.py:483] Algo bellman_ford step 1652 current loss 0.109647, current_train_items 52896.
I0304 19:28:47.418208 23128000471168 run.py:483] Algo bellman_ford step 1653 current loss 0.075215, current_train_items 52928.
I0304 19:28:47.449547 23128000471168 run.py:483] Algo bellman_ford step 1654 current loss 0.106669, current_train_items 52960.
I0304 19:28:47.469238 23128000471168 run.py:483] Algo bellman_ford step 1655 current loss 0.004357, current_train_items 52992.
I0304 19:28:47.485428 23128000471168 run.py:483] Algo bellman_ford step 1656 current loss 0.037012, current_train_items 53024.
I0304 19:28:47.509403 23128000471168 run.py:483] Algo bellman_ford step 1657 current loss 0.063980, current_train_items 53056.
I0304 19:28:47.538505 23128000471168 run.py:483] Algo bellman_ford step 1658 current loss 0.085623, current_train_items 53088.
I0304 19:28:47.569308 23128000471168 run.py:483] Algo bellman_ford step 1659 current loss 0.132535, current_train_items 53120.
I0304 19:28:47.588946 23128000471168 run.py:483] Algo bellman_ford step 1660 current loss 0.011264, current_train_items 53152.
I0304 19:28:47.605503 23128000471168 run.py:483] Algo bellman_ford step 1661 current loss 0.040344, current_train_items 53184.
I0304 19:28:47.628299 23128000471168 run.py:483] Algo bellman_ford step 1662 current loss 0.090449, current_train_items 53216.
I0304 19:28:47.659549 23128000471168 run.py:483] Algo bellman_ford step 1663 current loss 0.119355, current_train_items 53248.
I0304 19:28:47.693535 23128000471168 run.py:483] Algo bellman_ford step 1664 current loss 0.171362, current_train_items 53280.
I0304 19:28:47.712953 23128000471168 run.py:483] Algo bellman_ford step 1665 current loss 0.032233, current_train_items 53312.
I0304 19:28:47.729547 23128000471168 run.py:483] Algo bellman_ford step 1666 current loss 0.040124, current_train_items 53344.
I0304 19:28:47.752903 23128000471168 run.py:483] Algo bellman_ford step 1667 current loss 0.044044, current_train_items 53376.
I0304 19:28:47.781917 23128000471168 run.py:483] Algo bellman_ford step 1668 current loss 0.085185, current_train_items 53408.
I0304 19:28:47.812883 23128000471168 run.py:483] Algo bellman_ford step 1669 current loss 0.172009, current_train_items 53440.
I0304 19:28:47.832513 23128000471168 run.py:483] Algo bellman_ford step 1670 current loss 0.006257, current_train_items 53472.
I0304 19:28:47.848815 23128000471168 run.py:483] Algo bellman_ford step 1671 current loss 0.029841, current_train_items 53504.
I0304 19:28:47.871878 23128000471168 run.py:483] Algo bellman_ford step 1672 current loss 0.069659, current_train_items 53536.
I0304 19:28:47.902498 23128000471168 run.py:483] Algo bellman_ford step 1673 current loss 0.104282, current_train_items 53568.
I0304 19:28:47.935425 23128000471168 run.py:483] Algo bellman_ford step 1674 current loss 0.111593, current_train_items 53600.
I0304 19:28:47.955177 23128000471168 run.py:483] Algo bellman_ford step 1675 current loss 0.004322, current_train_items 53632.
I0304 19:28:47.971818 23128000471168 run.py:483] Algo bellman_ford step 1676 current loss 0.077654, current_train_items 53664.
I0304 19:28:47.995084 23128000471168 run.py:483] Algo bellman_ford step 1677 current loss 0.071776, current_train_items 53696.
I0304 19:28:48.024760 23128000471168 run.py:483] Algo bellman_ford step 1678 current loss 0.132199, current_train_items 53728.
I0304 19:28:48.058484 23128000471168 run.py:483] Algo bellman_ford step 1679 current loss 0.119951, current_train_items 53760.
I0304 19:28:48.077882 23128000471168 run.py:483] Algo bellman_ford step 1680 current loss 0.010286, current_train_items 53792.
I0304 19:28:48.093964 23128000471168 run.py:483] Algo bellman_ford step 1681 current loss 0.015541, current_train_items 53824.
I0304 19:28:48.117031 23128000471168 run.py:483] Algo bellman_ford step 1682 current loss 0.062637, current_train_items 53856.
I0304 19:28:48.145839 23128000471168 run.py:483] Algo bellman_ford step 1683 current loss 0.078936, current_train_items 53888.
I0304 19:28:48.178261 23128000471168 run.py:483] Algo bellman_ford step 1684 current loss 0.096876, current_train_items 53920.
I0304 19:28:48.198127 23128000471168 run.py:483] Algo bellman_ford step 1685 current loss 0.013195, current_train_items 53952.
I0304 19:28:48.214502 23128000471168 run.py:483] Algo bellman_ford step 1686 current loss 0.057988, current_train_items 53984.
I0304 19:28:48.238194 23128000471168 run.py:483] Algo bellman_ford step 1687 current loss 0.071519, current_train_items 54016.
I0304 19:28:48.267933 23128000471168 run.py:483] Algo bellman_ford step 1688 current loss 0.120889, current_train_items 54048.
I0304 19:28:48.302282 23128000471168 run.py:483] Algo bellman_ford step 1689 current loss 0.133927, current_train_items 54080.
I0304 19:28:48.321988 23128000471168 run.py:483] Algo bellman_ford step 1690 current loss 0.006891, current_train_items 54112.
I0304 19:28:48.338938 23128000471168 run.py:483] Algo bellman_ford step 1691 current loss 0.035762, current_train_items 54144.
I0304 19:28:48.362645 23128000471168 run.py:483] Algo bellman_ford step 1692 current loss 0.078500, current_train_items 54176.
I0304 19:28:48.393424 23128000471168 run.py:483] Algo bellman_ford step 1693 current loss 0.127462, current_train_items 54208.
I0304 19:28:48.426317 23128000471168 run.py:483] Algo bellman_ford step 1694 current loss 0.119312, current_train_items 54240.
I0304 19:28:48.445581 23128000471168 run.py:483] Algo bellman_ford step 1695 current loss 0.017859, current_train_items 54272.
I0304 19:28:48.462278 23128000471168 run.py:483] Algo bellman_ford step 1696 current loss 0.050208, current_train_items 54304.
I0304 19:28:48.484364 23128000471168 run.py:483] Algo bellman_ford step 1697 current loss 0.114232, current_train_items 54336.
I0304 19:28:48.513191 23128000471168 run.py:483] Algo bellman_ford step 1698 current loss 0.145823, current_train_items 54368.
I0304 19:28:48.546436 23128000471168 run.py:483] Algo bellman_ford step 1699 current loss 0.215321, current_train_items 54400.
I0304 19:28:48.566481 23128000471168 run.py:483] Algo bellman_ford step 1700 current loss 0.013546, current_train_items 54432.
I0304 19:28:48.574219 23128000471168 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0304 19:28:48.574327 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:28:48.591637 23128000471168 run.py:483] Algo bellman_ford step 1701 current loss 0.052624, current_train_items 54464.
I0304 19:28:48.615571 23128000471168 run.py:483] Algo bellman_ford step 1702 current loss 0.098479, current_train_items 54496.
I0304 19:28:48.646157 23128000471168 run.py:483] Algo bellman_ford step 1703 current loss 0.104658, current_train_items 54528.
I0304 19:28:48.678852 23128000471168 run.py:483] Algo bellman_ford step 1704 current loss 0.090950, current_train_items 54560.
I0304 19:28:48.698781 23128000471168 run.py:483] Algo bellman_ford step 1705 current loss 0.010048, current_train_items 54592.
I0304 19:28:48.714841 23128000471168 run.py:483] Algo bellman_ford step 1706 current loss 0.021129, current_train_items 54624.
I0304 19:28:48.738952 23128000471168 run.py:483] Algo bellman_ford step 1707 current loss 0.110034, current_train_items 54656.
I0304 19:28:48.769285 23128000471168 run.py:483] Algo bellman_ford step 1708 current loss 0.129342, current_train_items 54688.
I0304 19:28:48.803320 23128000471168 run.py:483] Algo bellman_ford step 1709 current loss 0.095492, current_train_items 54720.
I0304 19:28:48.822721 23128000471168 run.py:483] Algo bellman_ford step 1710 current loss 0.013499, current_train_items 54752.
I0304 19:28:48.839266 23128000471168 run.py:483] Algo bellman_ford step 1711 current loss 0.023035, current_train_items 54784.
I0304 19:28:48.863671 23128000471168 run.py:483] Algo bellman_ford step 1712 current loss 0.145951, current_train_items 54816.
I0304 19:28:48.893243 23128000471168 run.py:483] Algo bellman_ford step 1713 current loss 0.128039, current_train_items 54848.
I0304 19:28:48.925495 23128000471168 run.py:483] Algo bellman_ford step 1714 current loss 0.157908, current_train_items 54880.
I0304 19:28:48.944890 23128000471168 run.py:483] Algo bellman_ford step 1715 current loss 0.006255, current_train_items 54912.
I0304 19:28:48.961169 23128000471168 run.py:483] Algo bellman_ford step 1716 current loss 0.016911, current_train_items 54944.
I0304 19:28:48.985195 23128000471168 run.py:483] Algo bellman_ford step 1717 current loss 0.113257, current_train_items 54976.
I0304 19:28:49.013668 23128000471168 run.py:483] Algo bellman_ford step 1718 current loss 0.165346, current_train_items 55008.
I0304 19:28:49.046054 23128000471168 run.py:483] Algo bellman_ford step 1719 current loss 0.163504, current_train_items 55040.
I0304 19:28:49.065480 23128000471168 run.py:483] Algo bellman_ford step 1720 current loss 0.029647, current_train_items 55072.
I0304 19:28:49.081685 23128000471168 run.py:483] Algo bellman_ford step 1721 current loss 0.061240, current_train_items 55104.
I0304 19:28:49.105264 23128000471168 run.py:483] Algo bellman_ford step 1722 current loss 0.072413, current_train_items 55136.
I0304 19:28:49.135218 23128000471168 run.py:483] Algo bellman_ford step 1723 current loss 0.106021, current_train_items 55168.
I0304 19:28:49.167446 23128000471168 run.py:483] Algo bellman_ford step 1724 current loss 0.122917, current_train_items 55200.
I0304 19:28:49.186576 23128000471168 run.py:483] Algo bellman_ford step 1725 current loss 0.003391, current_train_items 55232.
I0304 19:28:49.203166 23128000471168 run.py:483] Algo bellman_ford step 1726 current loss 0.099635, current_train_items 55264.
I0304 19:28:49.228415 23128000471168 run.py:483] Algo bellman_ford step 1727 current loss 0.137861, current_train_items 55296.
I0304 19:28:49.258169 23128000471168 run.py:483] Algo bellman_ford step 1728 current loss 0.119463, current_train_items 55328.
I0304 19:28:49.290456 23128000471168 run.py:483] Algo bellman_ford step 1729 current loss 0.118856, current_train_items 55360.
I0304 19:28:49.309774 23128000471168 run.py:483] Algo bellman_ford step 1730 current loss 0.007617, current_train_items 55392.
I0304 19:28:49.326185 23128000471168 run.py:483] Algo bellman_ford step 1731 current loss 0.020319, current_train_items 55424.
I0304 19:28:49.350048 23128000471168 run.py:483] Algo bellman_ford step 1732 current loss 0.151397, current_train_items 55456.
I0304 19:28:49.380213 23128000471168 run.py:483] Algo bellman_ford step 1733 current loss 0.255392, current_train_items 55488.
I0304 19:28:49.410836 23128000471168 run.py:483] Algo bellman_ford step 1734 current loss 0.179088, current_train_items 55520.
I0304 19:28:49.430654 23128000471168 run.py:483] Algo bellman_ford step 1735 current loss 0.007743, current_train_items 55552.
I0304 19:28:49.447178 23128000471168 run.py:483] Algo bellman_ford step 1736 current loss 0.057668, current_train_items 55584.
I0304 19:28:49.471573 23128000471168 run.py:483] Algo bellman_ford step 1737 current loss 0.091186, current_train_items 55616.
I0304 19:28:49.501371 23128000471168 run.py:483] Algo bellman_ford step 1738 current loss 0.102447, current_train_items 55648.
I0304 19:28:49.534493 23128000471168 run.py:483] Algo bellman_ford step 1739 current loss 0.110011, current_train_items 55680.
I0304 19:28:49.553879 23128000471168 run.py:483] Algo bellman_ford step 1740 current loss 0.005101, current_train_items 55712.
I0304 19:28:49.570343 23128000471168 run.py:483] Algo bellman_ford step 1741 current loss 0.030454, current_train_items 55744.
I0304 19:28:49.594354 23128000471168 run.py:483] Algo bellman_ford step 1742 current loss 0.061757, current_train_items 55776.
I0304 19:28:49.624993 23128000471168 run.py:483] Algo bellman_ford step 1743 current loss 0.073204, current_train_items 55808.
I0304 19:28:49.656218 23128000471168 run.py:483] Algo bellman_ford step 1744 current loss 0.128239, current_train_items 55840.
I0304 19:28:49.675616 23128000471168 run.py:483] Algo bellman_ford step 1745 current loss 0.003726, current_train_items 55872.
I0304 19:28:49.692227 23128000471168 run.py:483] Algo bellman_ford step 1746 current loss 0.031676, current_train_items 55904.
I0304 19:28:49.715794 23128000471168 run.py:483] Algo bellman_ford step 1747 current loss 0.076967, current_train_items 55936.
I0304 19:28:49.745708 23128000471168 run.py:483] Algo bellman_ford step 1748 current loss 0.074823, current_train_items 55968.
I0304 19:28:49.776818 23128000471168 run.py:483] Algo bellman_ford step 1749 current loss 0.067124, current_train_items 56000.
I0304 19:28:49.796610 23128000471168 run.py:483] Algo bellman_ford step 1750 current loss 0.017564, current_train_items 56032.
I0304 19:28:49.804634 23128000471168 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0304 19:28:49.804744 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.984, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:28:49.834096 23128000471168 run.py:483] Algo bellman_ford step 1751 current loss 0.044329, current_train_items 56064.
I0304 19:28:49.858522 23128000471168 run.py:483] Algo bellman_ford step 1752 current loss 0.062308, current_train_items 56096.
I0304 19:28:49.890311 23128000471168 run.py:483] Algo bellman_ford step 1753 current loss 0.096190, current_train_items 56128.
I0304 19:28:49.925106 23128000471168 run.py:483] Algo bellman_ford step 1754 current loss 0.121225, current_train_items 56160.
I0304 19:28:49.944929 23128000471168 run.py:483] Algo bellman_ford step 1755 current loss 0.003218, current_train_items 56192.
I0304 19:28:49.961265 23128000471168 run.py:483] Algo bellman_ford step 1756 current loss 0.032011, current_train_items 56224.
I0304 19:28:49.985489 23128000471168 run.py:483] Algo bellman_ford step 1757 current loss 0.074963, current_train_items 56256.
I0304 19:28:50.016622 23128000471168 run.py:483] Algo bellman_ford step 1758 current loss 0.088984, current_train_items 56288.
I0304 19:28:50.047050 23128000471168 run.py:483] Algo bellman_ford step 1759 current loss 0.076245, current_train_items 56320.
I0304 19:28:50.066566 23128000471168 run.py:483] Algo bellman_ford step 1760 current loss 0.002970, current_train_items 56352.
I0304 19:28:50.082952 23128000471168 run.py:483] Algo bellman_ford step 1761 current loss 0.028531, current_train_items 56384.
I0304 19:28:50.107701 23128000471168 run.py:483] Algo bellman_ford step 1762 current loss 0.105279, current_train_items 56416.
I0304 19:28:50.139401 23128000471168 run.py:483] Algo bellman_ford step 1763 current loss 0.081601, current_train_items 56448.
I0304 19:28:50.173776 23128000471168 run.py:483] Algo bellman_ford step 1764 current loss 0.121600, current_train_items 56480.
I0304 19:28:50.193259 23128000471168 run.py:483] Algo bellman_ford step 1765 current loss 0.010170, current_train_items 56512.
I0304 19:28:50.209552 23128000471168 run.py:483] Algo bellman_ford step 1766 current loss 0.009470, current_train_items 56544.
I0304 19:28:50.232905 23128000471168 run.py:483] Algo bellman_ford step 1767 current loss 0.065109, current_train_items 56576.
I0304 19:28:50.262763 23128000471168 run.py:483] Algo bellman_ford step 1768 current loss 0.064941, current_train_items 56608.
I0304 19:28:50.294107 23128000471168 run.py:483] Algo bellman_ford step 1769 current loss 0.117708, current_train_items 56640.
I0304 19:28:50.313984 23128000471168 run.py:483] Algo bellman_ford step 1770 current loss 0.015048, current_train_items 56672.
I0304 19:28:50.331015 23128000471168 run.py:483] Algo bellman_ford step 1771 current loss 0.033088, current_train_items 56704.
I0304 19:28:50.355032 23128000471168 run.py:483] Algo bellman_ford step 1772 current loss 0.073812, current_train_items 56736.
I0304 19:28:50.385571 23128000471168 run.py:483] Algo bellman_ford step 1773 current loss 0.080249, current_train_items 56768.
I0304 19:28:50.417023 23128000471168 run.py:483] Algo bellman_ford step 1774 current loss 0.157148, current_train_items 56800.
I0304 19:28:50.436806 23128000471168 run.py:483] Algo bellman_ford step 1775 current loss 0.010231, current_train_items 56832.
I0304 19:28:50.453480 23128000471168 run.py:483] Algo bellman_ford step 1776 current loss 0.051207, current_train_items 56864.
I0304 19:28:50.477330 23128000471168 run.py:483] Algo bellman_ford step 1777 current loss 0.108563, current_train_items 56896.
I0304 19:28:50.507338 23128000471168 run.py:483] Algo bellman_ford step 1778 current loss 0.111074, current_train_items 56928.
I0304 19:28:50.537897 23128000471168 run.py:483] Algo bellman_ford step 1779 current loss 0.116985, current_train_items 56960.
I0304 19:28:50.557051 23128000471168 run.py:483] Algo bellman_ford step 1780 current loss 0.010994, current_train_items 56992.
I0304 19:28:50.573444 23128000471168 run.py:483] Algo bellman_ford step 1781 current loss 0.078159, current_train_items 57024.
I0304 19:28:50.596861 23128000471168 run.py:483] Algo bellman_ford step 1782 current loss 0.106538, current_train_items 57056.
I0304 19:28:50.626989 23128000471168 run.py:483] Algo bellman_ford step 1783 current loss 0.185396, current_train_items 57088.
I0304 19:28:50.658554 23128000471168 run.py:483] Algo bellman_ford step 1784 current loss 0.129114, current_train_items 57120.
I0304 19:28:50.678182 23128000471168 run.py:483] Algo bellman_ford step 1785 current loss 0.010748, current_train_items 57152.
I0304 19:28:50.694957 23128000471168 run.py:483] Algo bellman_ford step 1786 current loss 0.041507, current_train_items 57184.
I0304 19:28:50.717917 23128000471168 run.py:483] Algo bellman_ford step 1787 current loss 0.139755, current_train_items 57216.
I0304 19:28:50.746832 23128000471168 run.py:483] Algo bellman_ford step 1788 current loss 0.110151, current_train_items 57248.
I0304 19:28:50.779151 23128000471168 run.py:483] Algo bellman_ford step 1789 current loss 0.121299, current_train_items 57280.
I0304 19:28:50.798982 23128000471168 run.py:483] Algo bellman_ford step 1790 current loss 0.005674, current_train_items 57312.
I0304 19:28:50.816021 23128000471168 run.py:483] Algo bellman_ford step 1791 current loss 0.051709, current_train_items 57344.
I0304 19:28:50.839616 23128000471168 run.py:483] Algo bellman_ford step 1792 current loss 0.104416, current_train_items 57376.
I0304 19:28:50.868278 23128000471168 run.py:483] Algo bellman_ford step 1793 current loss 0.174346, current_train_items 57408.
I0304 19:28:50.900236 23128000471168 run.py:483] Algo bellman_ford step 1794 current loss 0.165660, current_train_items 57440.
I0304 19:28:50.919578 23128000471168 run.py:483] Algo bellman_ford step 1795 current loss 0.008218, current_train_items 57472.
I0304 19:28:50.935832 23128000471168 run.py:483] Algo bellman_ford step 1796 current loss 0.051349, current_train_items 57504.
I0304 19:28:50.959977 23128000471168 run.py:483] Algo bellman_ford step 1797 current loss 0.075566, current_train_items 57536.
I0304 19:28:50.990226 23128000471168 run.py:483] Algo bellman_ford step 1798 current loss 0.097902, current_train_items 57568.
I0304 19:28:51.023488 23128000471168 run.py:483] Algo bellman_ford step 1799 current loss 0.105735, current_train_items 57600.
I0304 19:28:51.043171 23128000471168 run.py:483] Algo bellman_ford step 1800 current loss 0.009395, current_train_items 57632.
I0304 19:28:51.050866 23128000471168 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0304 19:28:51.050977 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:51.067836 23128000471168 run.py:483] Algo bellman_ford step 1801 current loss 0.014702, current_train_items 57664.
I0304 19:28:51.092857 23128000471168 run.py:483] Algo bellman_ford step 1802 current loss 0.101920, current_train_items 57696.
I0304 19:28:51.122623 23128000471168 run.py:483] Algo bellman_ford step 1803 current loss 0.081696, current_train_items 57728.
I0304 19:28:51.154670 23128000471168 run.py:483] Algo bellman_ford step 1804 current loss 0.171157, current_train_items 57760.
I0304 19:28:51.174301 23128000471168 run.py:483] Algo bellman_ford step 1805 current loss 0.003173, current_train_items 57792.
I0304 19:28:51.190437 23128000471168 run.py:483] Algo bellman_ford step 1806 current loss 0.012745, current_train_items 57824.
I0304 19:28:51.213689 23128000471168 run.py:483] Algo bellman_ford step 1807 current loss 0.062261, current_train_items 57856.
I0304 19:28:51.242674 23128000471168 run.py:483] Algo bellman_ford step 1808 current loss 0.103147, current_train_items 57888.
I0304 19:28:51.276274 23128000471168 run.py:483] Algo bellman_ford step 1809 current loss 0.163733, current_train_items 57920.
I0304 19:28:51.295693 23128000471168 run.py:483] Algo bellman_ford step 1810 current loss 0.003524, current_train_items 57952.
I0304 19:28:51.312047 23128000471168 run.py:483] Algo bellman_ford step 1811 current loss 0.044320, current_train_items 57984.
I0304 19:28:51.336252 23128000471168 run.py:483] Algo bellman_ford step 1812 current loss 0.057483, current_train_items 58016.
I0304 19:28:51.364448 23128000471168 run.py:483] Algo bellman_ford step 1813 current loss 0.075885, current_train_items 58048.
I0304 19:28:51.397207 23128000471168 run.py:483] Algo bellman_ford step 1814 current loss 0.138511, current_train_items 58080.
I0304 19:28:51.416810 23128000471168 run.py:483] Algo bellman_ford step 1815 current loss 0.004084, current_train_items 58112.
I0304 19:28:51.434077 23128000471168 run.py:483] Algo bellman_ford step 1816 current loss 0.106779, current_train_items 58144.
I0304 19:28:51.457625 23128000471168 run.py:483] Algo bellman_ford step 1817 current loss 0.062193, current_train_items 58176.
I0304 19:28:51.487461 23128000471168 run.py:483] Algo bellman_ford step 1818 current loss 0.075115, current_train_items 58208.
I0304 19:28:51.519385 23128000471168 run.py:483] Algo bellman_ford step 1819 current loss 0.116030, current_train_items 58240.
I0304 19:28:51.539194 23128000471168 run.py:483] Algo bellman_ford step 1820 current loss 0.006795, current_train_items 58272.
I0304 19:28:51.556163 23128000471168 run.py:483] Algo bellman_ford step 1821 current loss 0.044690, current_train_items 58304.
I0304 19:28:51.582109 23128000471168 run.py:483] Algo bellman_ford step 1822 current loss 0.072446, current_train_items 58336.
I0304 19:28:51.612277 23128000471168 run.py:483] Algo bellman_ford step 1823 current loss 0.059880, current_train_items 58368.
I0304 19:28:51.643534 23128000471168 run.py:483] Algo bellman_ford step 1824 current loss 0.087624, current_train_items 58400.
I0304 19:28:51.663064 23128000471168 run.py:483] Algo bellman_ford step 1825 current loss 0.003388, current_train_items 58432.
I0304 19:28:51.679668 23128000471168 run.py:483] Algo bellman_ford step 1826 current loss 0.040320, current_train_items 58464.
I0304 19:28:51.703250 23128000471168 run.py:483] Algo bellman_ford step 1827 current loss 0.051855, current_train_items 58496.
I0304 19:28:51.734219 23128000471168 run.py:483] Algo bellman_ford step 1828 current loss 0.077023, current_train_items 58528.
I0304 19:28:51.769602 23128000471168 run.py:483] Algo bellman_ford step 1829 current loss 0.122597, current_train_items 58560.
I0304 19:28:51.789273 23128000471168 run.py:483] Algo bellman_ford step 1830 current loss 0.006332, current_train_items 58592.
I0304 19:28:51.806049 23128000471168 run.py:483] Algo bellman_ford step 1831 current loss 0.081226, current_train_items 58624.
I0304 19:28:51.831120 23128000471168 run.py:483] Algo bellman_ford step 1832 current loss 0.078349, current_train_items 58656.
I0304 19:28:51.861341 23128000471168 run.py:483] Algo bellman_ford step 1833 current loss 0.123600, current_train_items 58688.
I0304 19:28:51.893493 23128000471168 run.py:483] Algo bellman_ford step 1834 current loss 0.090613, current_train_items 58720.
I0304 19:28:51.912789 23128000471168 run.py:483] Algo bellman_ford step 1835 current loss 0.005945, current_train_items 58752.
I0304 19:28:51.929402 23128000471168 run.py:483] Algo bellman_ford step 1836 current loss 0.037745, current_train_items 58784.
I0304 19:28:51.954392 23128000471168 run.py:483] Algo bellman_ford step 1837 current loss 0.136084, current_train_items 58816.
I0304 19:28:51.984022 23128000471168 run.py:483] Algo bellman_ford step 1838 current loss 0.220142, current_train_items 58848.
I0304 19:28:52.016712 23128000471168 run.py:483] Algo bellman_ford step 1839 current loss 0.214821, current_train_items 58880.
I0304 19:28:52.036080 23128000471168 run.py:483] Algo bellman_ford step 1840 current loss 0.009953, current_train_items 58912.
I0304 19:28:52.052664 23128000471168 run.py:483] Algo bellman_ford step 1841 current loss 0.035132, current_train_items 58944.
I0304 19:28:52.076958 23128000471168 run.py:483] Algo bellman_ford step 1842 current loss 0.092905, current_train_items 58976.
I0304 19:28:52.107415 23128000471168 run.py:483] Algo bellman_ford step 1843 current loss 0.092812, current_train_items 59008.
I0304 19:28:52.139795 23128000471168 run.py:483] Algo bellman_ford step 1844 current loss 0.081885, current_train_items 59040.
I0304 19:28:52.159523 23128000471168 run.py:483] Algo bellman_ford step 1845 current loss 0.008651, current_train_items 59072.
I0304 19:28:52.176027 23128000471168 run.py:483] Algo bellman_ford step 1846 current loss 0.028216, current_train_items 59104.
I0304 19:28:52.200121 23128000471168 run.py:483] Algo bellman_ford step 1847 current loss 0.137456, current_train_items 59136.
I0304 19:28:52.230387 23128000471168 run.py:483] Algo bellman_ford step 1848 current loss 0.064854, current_train_items 59168.
I0304 19:28:52.264181 23128000471168 run.py:483] Algo bellman_ford step 1849 current loss 0.133982, current_train_items 59200.
I0304 19:28:52.283430 23128000471168 run.py:483] Algo bellman_ford step 1850 current loss 0.011508, current_train_items 59232.
I0304 19:28:52.291566 23128000471168 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0304 19:28:52.291674 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:52.308861 23128000471168 run.py:483] Algo bellman_ford step 1851 current loss 0.022583, current_train_items 59264.
I0304 19:28:52.333828 23128000471168 run.py:483] Algo bellman_ford step 1852 current loss 0.102476, current_train_items 59296.
I0304 19:28:52.364193 23128000471168 run.py:483] Algo bellman_ford step 1853 current loss 0.107278, current_train_items 59328.
I0304 19:28:52.397322 23128000471168 run.py:483] Algo bellman_ford step 1854 current loss 0.121706, current_train_items 59360.
I0304 19:28:52.416922 23128000471168 run.py:483] Algo bellman_ford step 1855 current loss 0.017455, current_train_items 59392.
I0304 19:28:52.432728 23128000471168 run.py:483] Algo bellman_ford step 1856 current loss 0.040188, current_train_items 59424.
I0304 19:28:52.456806 23128000471168 run.py:483] Algo bellman_ford step 1857 current loss 0.115462, current_train_items 59456.
I0304 19:28:52.485790 23128000471168 run.py:483] Algo bellman_ford step 1858 current loss 0.145202, current_train_items 59488.
I0304 19:28:52.518583 23128000471168 run.py:483] Algo bellman_ford step 1859 current loss 0.110117, current_train_items 59520.
I0304 19:28:52.538460 23128000471168 run.py:483] Algo bellman_ford step 1860 current loss 0.009443, current_train_items 59552.
I0304 19:28:52.555474 23128000471168 run.py:483] Algo bellman_ford step 1861 current loss 0.030469, current_train_items 59584.
I0304 19:28:52.579154 23128000471168 run.py:483] Algo bellman_ford step 1862 current loss 0.111669, current_train_items 59616.
I0304 19:28:52.609444 23128000471168 run.py:483] Algo bellman_ford step 1863 current loss 0.115814, current_train_items 59648.
I0304 19:28:52.642633 23128000471168 run.py:483] Algo bellman_ford step 1864 current loss 0.271986, current_train_items 59680.
I0304 19:28:52.662533 23128000471168 run.py:483] Algo bellman_ford step 1865 current loss 0.003506, current_train_items 59712.
I0304 19:28:52.679103 23128000471168 run.py:483] Algo bellman_ford step 1866 current loss 0.029273, current_train_items 59744.
I0304 19:28:52.703900 23128000471168 run.py:483] Algo bellman_ford step 1867 current loss 0.094892, current_train_items 59776.
I0304 19:28:52.734293 23128000471168 run.py:483] Algo bellman_ford step 1868 current loss 0.154917, current_train_items 59808.
I0304 19:28:52.767148 23128000471168 run.py:483] Algo bellman_ford step 1869 current loss 0.141704, current_train_items 59840.
I0304 19:28:52.786802 23128000471168 run.py:483] Algo bellman_ford step 1870 current loss 0.005898, current_train_items 59872.
I0304 19:28:52.803346 23128000471168 run.py:483] Algo bellman_ford step 1871 current loss 0.011254, current_train_items 59904.
I0304 19:28:52.826496 23128000471168 run.py:483] Algo bellman_ford step 1872 current loss 0.019125, current_train_items 59936.
I0304 19:28:52.856509 23128000471168 run.py:483] Algo bellman_ford step 1873 current loss 0.095518, current_train_items 59968.
I0304 19:28:52.888892 23128000471168 run.py:483] Algo bellman_ford step 1874 current loss 0.097755, current_train_items 60000.
I0304 19:28:52.908832 23128000471168 run.py:483] Algo bellman_ford step 1875 current loss 0.018986, current_train_items 60032.
I0304 19:28:52.925253 23128000471168 run.py:483] Algo bellman_ford step 1876 current loss 0.022015, current_train_items 60064.
I0304 19:28:52.949227 23128000471168 run.py:483] Algo bellman_ford step 1877 current loss 0.068012, current_train_items 60096.
I0304 19:28:52.978248 23128000471168 run.py:483] Algo bellman_ford step 1878 current loss 0.054982, current_train_items 60128.
I0304 19:28:53.013019 23128000471168 run.py:483] Algo bellman_ford step 1879 current loss 0.119354, current_train_items 60160.
I0304 19:28:53.032656 23128000471168 run.py:483] Algo bellman_ford step 1880 current loss 0.004128, current_train_items 60192.
I0304 19:28:53.049379 23128000471168 run.py:483] Algo bellman_ford step 1881 current loss 0.027261, current_train_items 60224.
I0304 19:28:53.072906 23128000471168 run.py:483] Algo bellman_ford step 1882 current loss 0.074646, current_train_items 60256.
I0304 19:28:53.103119 23128000471168 run.py:483] Algo bellman_ford step 1883 current loss 0.085722, current_train_items 60288.
I0304 19:28:53.135751 23128000471168 run.py:483] Algo bellman_ford step 1884 current loss 0.146197, current_train_items 60320.
I0304 19:28:53.155111 23128000471168 run.py:483] Algo bellman_ford step 1885 current loss 0.047598, current_train_items 60352.
I0304 19:28:53.171965 23128000471168 run.py:483] Algo bellman_ford step 1886 current loss 0.022854, current_train_items 60384.
I0304 19:28:53.195859 23128000471168 run.py:483] Algo bellman_ford step 1887 current loss 0.097020, current_train_items 60416.
I0304 19:28:53.225039 23128000471168 run.py:483] Algo bellman_ford step 1888 current loss 0.065905, current_train_items 60448.
I0304 19:28:53.259404 23128000471168 run.py:483] Algo bellman_ford step 1889 current loss 0.110757, current_train_items 60480.
I0304 19:28:53.279246 23128000471168 run.py:483] Algo bellman_ford step 1890 current loss 0.004686, current_train_items 60512.
I0304 19:28:53.295928 23128000471168 run.py:483] Algo bellman_ford step 1891 current loss 0.035007, current_train_items 60544.
I0304 19:28:53.319071 23128000471168 run.py:483] Algo bellman_ford step 1892 current loss 0.063769, current_train_items 60576.
I0304 19:28:53.349813 23128000471168 run.py:483] Algo bellman_ford step 1893 current loss 0.093135, current_train_items 60608.
I0304 19:28:53.385049 23128000471168 run.py:483] Algo bellman_ford step 1894 current loss 0.211145, current_train_items 60640.
I0304 19:28:53.404185 23128000471168 run.py:483] Algo bellman_ford step 1895 current loss 0.027024, current_train_items 60672.
I0304 19:28:53.420553 23128000471168 run.py:483] Algo bellman_ford step 1896 current loss 0.025741, current_train_items 60704.
I0304 19:28:53.444482 23128000471168 run.py:483] Algo bellman_ford step 1897 current loss 0.090335, current_train_items 60736.
I0304 19:28:53.474885 23128000471168 run.py:483] Algo bellman_ford step 1898 current loss 0.093794, current_train_items 60768.
I0304 19:28:53.508227 23128000471168 run.py:483] Algo bellman_ford step 1899 current loss 0.113788, current_train_items 60800.
I0304 19:28:53.528108 23128000471168 run.py:483] Algo bellman_ford step 1900 current loss 0.006773, current_train_items 60832.
I0304 19:28:53.536189 23128000471168 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0304 19:28:53.536298 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:28:53.553207 23128000471168 run.py:483] Algo bellman_ford step 1901 current loss 0.028002, current_train_items 60864.
I0304 19:28:53.577148 23128000471168 run.py:483] Algo bellman_ford step 1902 current loss 0.069716, current_train_items 60896.
I0304 19:28:53.605726 23128000471168 run.py:483] Algo bellman_ford step 1903 current loss 0.068838, current_train_items 60928.
I0304 19:28:53.638201 23128000471168 run.py:483] Algo bellman_ford step 1904 current loss 0.130263, current_train_items 60960.
I0304 19:28:53.657771 23128000471168 run.py:483] Algo bellman_ford step 1905 current loss 0.004621, current_train_items 60992.
I0304 19:28:53.673730 23128000471168 run.py:483] Algo bellman_ford step 1906 current loss 0.038810, current_train_items 61024.
I0304 19:28:53.696918 23128000471168 run.py:483] Algo bellman_ford step 1907 current loss 0.056467, current_train_items 61056.
I0304 19:28:53.726481 23128000471168 run.py:483] Algo bellman_ford step 1908 current loss 0.078759, current_train_items 61088.
I0304 19:28:53.758240 23128000471168 run.py:483] Algo bellman_ford step 1909 current loss 0.106153, current_train_items 61120.
I0304 19:28:53.777641 23128000471168 run.py:483] Algo bellman_ford step 1910 current loss 0.009103, current_train_items 61152.
I0304 19:28:53.794323 23128000471168 run.py:483] Algo bellman_ford step 1911 current loss 0.017778, current_train_items 61184.
I0304 19:28:53.818840 23128000471168 run.py:483] Algo bellman_ford step 1912 current loss 0.095993, current_train_items 61216.
I0304 19:28:53.848336 23128000471168 run.py:483] Algo bellman_ford step 1913 current loss 0.069207, current_train_items 61248.
I0304 19:28:53.881216 23128000471168 run.py:483] Algo bellman_ford step 1914 current loss 0.106628, current_train_items 61280.
I0304 19:28:53.900318 23128000471168 run.py:483] Algo bellman_ford step 1915 current loss 0.003192, current_train_items 61312.
I0304 19:28:53.917200 23128000471168 run.py:483] Algo bellman_ford step 1916 current loss 0.041015, current_train_items 61344.
I0304 19:28:53.941251 23128000471168 run.py:483] Algo bellman_ford step 1917 current loss 0.099101, current_train_items 61376.
I0304 19:28:53.972035 23128000471168 run.py:483] Algo bellman_ford step 1918 current loss 0.069877, current_train_items 61408.
I0304 19:28:54.005763 23128000471168 run.py:483] Algo bellman_ford step 1919 current loss 0.139786, current_train_items 61440.
I0304 19:28:54.025350 23128000471168 run.py:483] Algo bellman_ford step 1920 current loss 0.016341, current_train_items 61472.
I0304 19:28:54.041896 23128000471168 run.py:483] Algo bellman_ford step 1921 current loss 0.037894, current_train_items 61504.
I0304 19:28:54.065652 23128000471168 run.py:483] Algo bellman_ford step 1922 current loss 0.172851, current_train_items 61536.
I0304 19:28:54.094576 23128000471168 run.py:483] Algo bellman_ford step 1923 current loss 0.124508, current_train_items 61568.
I0304 19:28:54.127145 23128000471168 run.py:483] Algo bellman_ford step 1924 current loss 0.176543, current_train_items 61600.
I0304 19:28:54.146423 23128000471168 run.py:483] Algo bellman_ford step 1925 current loss 0.003753, current_train_items 61632.
I0304 19:28:54.163168 23128000471168 run.py:483] Algo bellman_ford step 1926 current loss 0.046521, current_train_items 61664.
I0304 19:28:54.185884 23128000471168 run.py:483] Algo bellman_ford step 1927 current loss 0.056832, current_train_items 61696.
I0304 19:28:54.214869 23128000471168 run.py:483] Algo bellman_ford step 1928 current loss 0.141554, current_train_items 61728.
I0304 19:28:54.245855 23128000471168 run.py:483] Algo bellman_ford step 1929 current loss 0.224785, current_train_items 61760.
I0304 19:28:54.265135 23128000471168 run.py:483] Algo bellman_ford step 1930 current loss 0.013641, current_train_items 61792.
I0304 19:28:54.281061 23128000471168 run.py:483] Algo bellman_ford step 1931 current loss 0.062852, current_train_items 61824.
I0304 19:28:54.305786 23128000471168 run.py:483] Algo bellman_ford step 1932 current loss 0.129321, current_train_items 61856.
I0304 19:28:54.335692 23128000471168 run.py:483] Algo bellman_ford step 1933 current loss 0.212410, current_train_items 61888.
I0304 19:28:54.366392 23128000471168 run.py:483] Algo bellman_ford step 1934 current loss 0.128203, current_train_items 61920.
I0304 19:28:54.385702 23128000471168 run.py:483] Algo bellman_ford step 1935 current loss 0.032177, current_train_items 61952.
I0304 19:28:54.402144 23128000471168 run.py:483] Algo bellman_ford step 1936 current loss 0.051893, current_train_items 61984.
I0304 19:28:54.426464 23128000471168 run.py:483] Algo bellman_ford step 1937 current loss 0.074172, current_train_items 62016.
I0304 19:28:54.455371 23128000471168 run.py:483] Algo bellman_ford step 1938 current loss 0.106281, current_train_items 62048.
I0304 19:28:54.488818 23128000471168 run.py:483] Algo bellman_ford step 1939 current loss 0.223852, current_train_items 62080.
I0304 19:28:54.508539 23128000471168 run.py:483] Algo bellman_ford step 1940 current loss 0.025225, current_train_items 62112.
I0304 19:28:54.524969 23128000471168 run.py:483] Algo bellman_ford step 1941 current loss 0.019183, current_train_items 62144.
I0304 19:28:54.548701 23128000471168 run.py:483] Algo bellman_ford step 1942 current loss 0.080610, current_train_items 62176.
I0304 19:28:54.577353 23128000471168 run.py:483] Algo bellman_ford step 1943 current loss 0.089112, current_train_items 62208.
I0304 19:28:54.610161 23128000471168 run.py:483] Algo bellman_ford step 1944 current loss 0.155002, current_train_items 62240.
I0304 19:28:54.629502 23128000471168 run.py:483] Algo bellman_ford step 1945 current loss 0.013491, current_train_items 62272.
I0304 19:28:54.646132 23128000471168 run.py:483] Algo bellman_ford step 1946 current loss 0.069581, current_train_items 62304.
I0304 19:28:54.670250 23128000471168 run.py:483] Algo bellman_ford step 1947 current loss 0.047238, current_train_items 62336.
I0304 19:28:54.700836 23128000471168 run.py:483] Algo bellman_ford step 1948 current loss 0.056241, current_train_items 62368.
I0304 19:28:54.732246 23128000471168 run.py:483] Algo bellman_ford step 1949 current loss 0.100957, current_train_items 62400.
I0304 19:28:54.751738 23128000471168 run.py:483] Algo bellman_ford step 1950 current loss 0.022228, current_train_items 62432.
I0304 19:28:54.759925 23128000471168 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0304 19:28:54.760044 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:54.777412 23128000471168 run.py:483] Algo bellman_ford step 1951 current loss 0.064784, current_train_items 62464.
I0304 19:28:54.802017 23128000471168 run.py:483] Algo bellman_ford step 1952 current loss 0.081050, current_train_items 62496.
I0304 19:28:54.834348 23128000471168 run.py:483] Algo bellman_ford step 1953 current loss 0.142695, current_train_items 62528.
I0304 19:28:54.867250 23128000471168 run.py:483] Algo bellman_ford step 1954 current loss 0.122540, current_train_items 62560.
I0304 19:28:54.887549 23128000471168 run.py:483] Algo bellman_ford step 1955 current loss 0.018323, current_train_items 62592.
I0304 19:28:54.902987 23128000471168 run.py:483] Algo bellman_ford step 1956 current loss 0.021812, current_train_items 62624.
I0304 19:28:54.927322 23128000471168 run.py:483] Algo bellman_ford step 1957 current loss 0.119789, current_train_items 62656.
I0304 19:28:54.957918 23128000471168 run.py:483] Algo bellman_ford step 1958 current loss 0.099732, current_train_items 62688.
I0304 19:28:54.991345 23128000471168 run.py:483] Algo bellman_ford step 1959 current loss 0.094894, current_train_items 62720.
I0304 19:28:55.011343 23128000471168 run.py:483] Algo bellman_ford step 1960 current loss 0.003932, current_train_items 62752.
I0304 19:28:55.027977 23128000471168 run.py:483] Algo bellman_ford step 1961 current loss 0.066608, current_train_items 62784.
I0304 19:28:55.051029 23128000471168 run.py:483] Algo bellman_ford step 1962 current loss 0.093994, current_train_items 62816.
I0304 19:28:55.080186 23128000471168 run.py:483] Algo bellman_ford step 1963 current loss 0.104017, current_train_items 62848.
I0304 19:28:55.115033 23128000471168 run.py:483] Algo bellman_ford step 1964 current loss 0.191523, current_train_items 62880.
I0304 19:28:55.134781 23128000471168 run.py:483] Algo bellman_ford step 1965 current loss 0.003742, current_train_items 62912.
I0304 19:28:55.151093 23128000471168 run.py:483] Algo bellman_ford step 1966 current loss 0.071680, current_train_items 62944.
I0304 19:28:55.175260 23128000471168 run.py:483] Algo bellman_ford step 1967 current loss 0.120205, current_train_items 62976.
I0304 19:28:55.204887 23128000471168 run.py:483] Algo bellman_ford step 1968 current loss 0.085874, current_train_items 63008.
I0304 19:28:55.237237 23128000471168 run.py:483] Algo bellman_ford step 1969 current loss 0.088119, current_train_items 63040.
I0304 19:28:55.257134 23128000471168 run.py:483] Algo bellman_ford step 1970 current loss 0.004103, current_train_items 63072.
I0304 19:28:55.273723 23128000471168 run.py:483] Algo bellman_ford step 1971 current loss 0.057890, current_train_items 63104.
I0304 19:28:55.296961 23128000471168 run.py:483] Algo bellman_ford step 1972 current loss 0.082359, current_train_items 63136.
I0304 19:28:55.326663 23128000471168 run.py:483] Algo bellman_ford step 1973 current loss 0.107827, current_train_items 63168.
I0304 19:28:55.362374 23128000471168 run.py:483] Algo bellman_ford step 1974 current loss 0.221865, current_train_items 63200.
I0304 19:28:55.382328 23128000471168 run.py:483] Algo bellman_ford step 1975 current loss 0.006180, current_train_items 63232.
I0304 19:28:55.398437 23128000471168 run.py:483] Algo bellman_ford step 1976 current loss 0.033433, current_train_items 63264.
I0304 19:28:55.421009 23128000471168 run.py:483] Algo bellman_ford step 1977 current loss 0.059914, current_train_items 63296.
I0304 19:28:55.450274 23128000471168 run.py:483] Algo bellman_ford step 1978 current loss 0.122612, current_train_items 63328.
I0304 19:28:55.483700 23128000471168 run.py:483] Algo bellman_ford step 1979 current loss 0.202708, current_train_items 63360.
I0304 19:28:55.503237 23128000471168 run.py:483] Algo bellman_ford step 1980 current loss 0.006065, current_train_items 63392.
I0304 19:28:55.519953 23128000471168 run.py:483] Algo bellman_ford step 1981 current loss 0.027154, current_train_items 63424.
I0304 19:28:55.543792 23128000471168 run.py:483] Algo bellman_ford step 1982 current loss 0.061240, current_train_items 63456.
I0304 19:28:55.572881 23128000471168 run.py:483] Algo bellman_ford step 1983 current loss 0.050037, current_train_items 63488.
I0304 19:28:55.608317 23128000471168 run.py:483] Algo bellman_ford step 1984 current loss 0.147756, current_train_items 63520.
I0304 19:28:55.628295 23128000471168 run.py:483] Algo bellman_ford step 1985 current loss 0.003229, current_train_items 63552.
I0304 19:28:55.644826 23128000471168 run.py:483] Algo bellman_ford step 1986 current loss 0.058487, current_train_items 63584.
I0304 19:28:55.667356 23128000471168 run.py:483] Algo bellman_ford step 1987 current loss 0.083583, current_train_items 63616.
I0304 19:28:55.696396 23128000471168 run.py:483] Algo bellman_ford step 1988 current loss 0.073505, current_train_items 63648.
I0304 19:28:55.729676 23128000471168 run.py:483] Algo bellman_ford step 1989 current loss 0.101341, current_train_items 63680.
I0304 19:28:55.749223 23128000471168 run.py:483] Algo bellman_ford step 1990 current loss 0.007309, current_train_items 63712.
I0304 19:28:55.765687 23128000471168 run.py:483] Algo bellman_ford step 1991 current loss 0.048192, current_train_items 63744.
I0304 19:28:55.789295 23128000471168 run.py:483] Algo bellman_ford step 1992 current loss 0.090436, current_train_items 63776.
I0304 19:28:55.819313 23128000471168 run.py:483] Algo bellman_ford step 1993 current loss 0.135838, current_train_items 63808.
I0304 19:28:55.847839 23128000471168 run.py:483] Algo bellman_ford step 1994 current loss 0.090310, current_train_items 63840.
I0304 19:28:55.867551 23128000471168 run.py:483] Algo bellman_ford step 1995 current loss 0.029200, current_train_items 63872.
I0304 19:28:55.884563 23128000471168 run.py:483] Algo bellman_ford step 1996 current loss 0.029373, current_train_items 63904.
I0304 19:28:55.908139 23128000471168 run.py:483] Algo bellman_ford step 1997 current loss 0.049668, current_train_items 63936.
I0304 19:28:55.939100 23128000471168 run.py:483] Algo bellman_ford step 1998 current loss 0.123255, current_train_items 63968.
I0304 19:28:55.971220 23128000471168 run.py:483] Algo bellman_ford step 1999 current loss 0.101365, current_train_items 64000.
I0304 19:28:55.990773 23128000471168 run.py:483] Algo bellman_ford step 2000 current loss 0.005816, current_train_items 64032.
I0304 19:28:55.998754 23128000471168 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0304 19:28:55.998862 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:56.015861 23128000471168 run.py:483] Algo bellman_ford step 2001 current loss 0.053059, current_train_items 64064.
I0304 19:28:56.040950 23128000471168 run.py:483] Algo bellman_ford step 2002 current loss 0.217332, current_train_items 64096.
I0304 19:28:56.071577 23128000471168 run.py:483] Algo bellman_ford step 2003 current loss 0.235486, current_train_items 64128.
I0304 19:28:56.103932 23128000471168 run.py:483] Algo bellman_ford step 2004 current loss 0.101485, current_train_items 64160.
I0304 19:28:56.123735 23128000471168 run.py:483] Algo bellman_ford step 2005 current loss 0.005422, current_train_items 64192.
I0304 19:28:56.139896 23128000471168 run.py:483] Algo bellman_ford step 2006 current loss 0.032991, current_train_items 64224.
I0304 19:28:56.163847 23128000471168 run.py:483] Algo bellman_ford step 2007 current loss 0.054984, current_train_items 64256.
I0304 19:28:56.194360 23128000471168 run.py:483] Algo bellman_ford step 2008 current loss 0.206900, current_train_items 64288.
I0304 19:28:56.226580 23128000471168 run.py:483] Algo bellman_ford step 2009 current loss 0.181105, current_train_items 64320.
I0304 19:28:56.246094 23128000471168 run.py:483] Algo bellman_ford step 2010 current loss 0.048766, current_train_items 64352.
I0304 19:28:56.262648 23128000471168 run.py:483] Algo bellman_ford step 2011 current loss 0.023995, current_train_items 64384.
I0304 19:28:56.286553 23128000471168 run.py:483] Algo bellman_ford step 2012 current loss 0.076464, current_train_items 64416.
I0304 19:28:56.316046 23128000471168 run.py:483] Algo bellman_ford step 2013 current loss 0.119004, current_train_items 64448.
I0304 19:28:56.348423 23128000471168 run.py:483] Algo bellman_ford step 2014 current loss 0.100375, current_train_items 64480.
I0304 19:28:56.367795 23128000471168 run.py:483] Algo bellman_ford step 2015 current loss 0.013784, current_train_items 64512.
I0304 19:28:56.384959 23128000471168 run.py:483] Algo bellman_ford step 2016 current loss 0.028458, current_train_items 64544.
I0304 19:28:56.409430 23128000471168 run.py:483] Algo bellman_ford step 2017 current loss 0.137499, current_train_items 64576.
I0304 19:28:56.438139 23128000471168 run.py:483] Algo bellman_ford step 2018 current loss 0.084326, current_train_items 64608.
I0304 19:28:56.471926 23128000471168 run.py:483] Algo bellman_ford step 2019 current loss 0.143147, current_train_items 64640.
I0304 19:28:56.491296 23128000471168 run.py:483] Algo bellman_ford step 2020 current loss 0.004696, current_train_items 64672.
I0304 19:28:56.507654 23128000471168 run.py:483] Algo bellman_ford step 2021 current loss 0.012217, current_train_items 64704.
I0304 19:28:56.532145 23128000471168 run.py:483] Algo bellman_ford step 2022 current loss 0.158641, current_train_items 64736.
I0304 19:28:56.562022 23128000471168 run.py:483] Algo bellman_ford step 2023 current loss 0.079306, current_train_items 64768.
I0304 19:28:56.597364 23128000471168 run.py:483] Algo bellman_ford step 2024 current loss 0.121027, current_train_items 64800.
I0304 19:28:56.616913 23128000471168 run.py:483] Algo bellman_ford step 2025 current loss 0.029321, current_train_items 64832.
I0304 19:28:56.633496 23128000471168 run.py:483] Algo bellman_ford step 2026 current loss 0.038231, current_train_items 64864.
I0304 19:28:56.657574 23128000471168 run.py:483] Algo bellman_ford step 2027 current loss 0.100119, current_train_items 64896.
I0304 19:28:56.687095 23128000471168 run.py:483] Algo bellman_ford step 2028 current loss 0.070160, current_train_items 64928.
I0304 19:28:56.719296 23128000471168 run.py:483] Algo bellman_ford step 2029 current loss 0.088176, current_train_items 64960.
I0304 19:28:56.739089 23128000471168 run.py:483] Algo bellman_ford step 2030 current loss 0.008537, current_train_items 64992.
I0304 19:28:56.755825 23128000471168 run.py:483] Algo bellman_ford step 2031 current loss 0.064265, current_train_items 65024.
I0304 19:28:56.778868 23128000471168 run.py:483] Algo bellman_ford step 2032 current loss 0.063116, current_train_items 65056.
I0304 19:28:56.809664 23128000471168 run.py:483] Algo bellman_ford step 2033 current loss 0.124194, current_train_items 65088.
I0304 19:28:56.841600 23128000471168 run.py:483] Algo bellman_ford step 2034 current loss 0.080961, current_train_items 65120.
I0304 19:28:56.860969 23128000471168 run.py:483] Algo bellman_ford step 2035 current loss 0.004439, current_train_items 65152.
I0304 19:28:56.877079 23128000471168 run.py:483] Algo bellman_ford step 2036 current loss 0.013568, current_train_items 65184.
I0304 19:28:56.900077 23128000471168 run.py:483] Algo bellman_ford step 2037 current loss 0.112957, current_train_items 65216.
I0304 19:28:56.929976 23128000471168 run.py:483] Algo bellman_ford step 2038 current loss 0.124144, current_train_items 65248.
I0304 19:28:56.963201 23128000471168 run.py:483] Algo bellman_ford step 2039 current loss 0.084829, current_train_items 65280.
I0304 19:28:56.982592 23128000471168 run.py:483] Algo bellman_ford step 2040 current loss 0.014696, current_train_items 65312.
I0304 19:28:56.998777 23128000471168 run.py:483] Algo bellman_ford step 2041 current loss 0.062938, current_train_items 65344.
I0304 19:28:57.023664 23128000471168 run.py:483] Algo bellman_ford step 2042 current loss 0.144824, current_train_items 65376.
I0304 19:28:57.053794 23128000471168 run.py:483] Algo bellman_ford step 2043 current loss 0.128228, current_train_items 65408.
I0304 19:28:57.081415 23128000471168 run.py:483] Algo bellman_ford step 2044 current loss 0.053376, current_train_items 65440.
I0304 19:28:57.100807 23128000471168 run.py:483] Algo bellman_ford step 2045 current loss 0.009352, current_train_items 65472.
I0304 19:28:57.117927 23128000471168 run.py:483] Algo bellman_ford step 2046 current loss 0.042691, current_train_items 65504.
I0304 19:28:57.141815 23128000471168 run.py:483] Algo bellman_ford step 2047 current loss 0.097974, current_train_items 65536.
I0304 19:28:57.171372 23128000471168 run.py:483] Algo bellman_ford step 2048 current loss 0.135964, current_train_items 65568.
I0304 19:28:57.202071 23128000471168 run.py:483] Algo bellman_ford step 2049 current loss 0.096038, current_train_items 65600.
I0304 19:28:57.221782 23128000471168 run.py:483] Algo bellman_ford step 2050 current loss 0.011136, current_train_items 65632.
I0304 19:28:57.230029 23128000471168 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0304 19:28:57.230175 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:28:57.248127 23128000471168 run.py:483] Algo bellman_ford step 2051 current loss 0.037868, current_train_items 65664.
I0304 19:28:57.272174 23128000471168 run.py:483] Algo bellman_ford step 2052 current loss 0.076229, current_train_items 65696.
I0304 19:28:57.302162 23128000471168 run.py:483] Algo bellman_ford step 2053 current loss 0.114599, current_train_items 65728.
I0304 19:28:57.337740 23128000471168 run.py:483] Algo bellman_ford step 2054 current loss 0.197865, current_train_items 65760.
I0304 19:28:57.357335 23128000471168 run.py:483] Algo bellman_ford step 2055 current loss 0.004213, current_train_items 65792.
I0304 19:28:57.373348 23128000471168 run.py:483] Algo bellman_ford step 2056 current loss 0.015453, current_train_items 65824.
I0304 19:28:57.396923 23128000471168 run.py:483] Algo bellman_ford step 2057 current loss 0.065161, current_train_items 65856.
I0304 19:28:57.426714 23128000471168 run.py:483] Algo bellman_ford step 2058 current loss 0.087380, current_train_items 65888.
I0304 19:28:57.459773 23128000471168 run.py:483] Algo bellman_ford step 2059 current loss 0.124035, current_train_items 65920.
I0304 19:28:57.479715 23128000471168 run.py:483] Algo bellman_ford step 2060 current loss 0.007725, current_train_items 65952.
I0304 19:28:57.496425 23128000471168 run.py:483] Algo bellman_ford step 2061 current loss 0.051667, current_train_items 65984.
I0304 19:28:57.520015 23128000471168 run.py:483] Algo bellman_ford step 2062 current loss 0.098959, current_train_items 66016.
I0304 19:28:57.550433 23128000471168 run.py:483] Algo bellman_ford step 2063 current loss 0.059353, current_train_items 66048.
I0304 19:28:57.584698 23128000471168 run.py:483] Algo bellman_ford step 2064 current loss 0.090055, current_train_items 66080.
I0304 19:28:57.604269 23128000471168 run.py:483] Algo bellman_ford step 2065 current loss 0.003821, current_train_items 66112.
I0304 19:28:57.620754 23128000471168 run.py:483] Algo bellman_ford step 2066 current loss 0.031576, current_train_items 66144.
I0304 19:28:57.645018 23128000471168 run.py:483] Algo bellman_ford step 2067 current loss 0.108869, current_train_items 66176.
I0304 19:28:57.675887 23128000471168 run.py:483] Algo bellman_ford step 2068 current loss 0.228583, current_train_items 66208.
I0304 19:28:57.708446 23128000471168 run.py:483] Algo bellman_ford step 2069 current loss 0.241783, current_train_items 66240.
I0304 19:28:57.728352 23128000471168 run.py:483] Algo bellman_ford step 2070 current loss 0.002787, current_train_items 66272.
I0304 19:28:57.745251 23128000471168 run.py:483] Algo bellman_ford step 2071 current loss 0.016872, current_train_items 66304.
I0304 19:28:57.768702 23128000471168 run.py:483] Algo bellman_ford step 2072 current loss 0.074584, current_train_items 66336.
I0304 19:28:57.799142 23128000471168 run.py:483] Algo bellman_ford step 2073 current loss 0.142680, current_train_items 66368.
I0304 19:28:57.832037 23128000471168 run.py:483] Algo bellman_ford step 2074 current loss 0.135545, current_train_items 66400.
I0304 19:28:57.852111 23128000471168 run.py:483] Algo bellman_ford step 2075 current loss 0.009874, current_train_items 66432.
I0304 19:28:57.869192 23128000471168 run.py:483] Algo bellman_ford step 2076 current loss 0.040055, current_train_items 66464.
I0304 19:28:57.893178 23128000471168 run.py:483] Algo bellman_ford step 2077 current loss 0.080586, current_train_items 66496.
I0304 19:28:57.921043 23128000471168 run.py:483] Algo bellman_ford step 2078 current loss 0.066260, current_train_items 66528.
I0304 19:28:57.953477 23128000471168 run.py:483] Algo bellman_ford step 2079 current loss 0.105193, current_train_items 66560.
I0304 19:28:57.973019 23128000471168 run.py:483] Algo bellman_ford step 2080 current loss 0.006916, current_train_items 66592.
I0304 19:28:57.989568 23128000471168 run.py:483] Algo bellman_ford step 2081 current loss 0.034045, current_train_items 66624.
I0304 19:28:58.013351 23128000471168 run.py:483] Algo bellman_ford step 2082 current loss 0.090537, current_train_items 66656.
I0304 19:28:58.044692 23128000471168 run.py:483] Algo bellman_ford step 2083 current loss 0.156445, current_train_items 66688.
I0304 19:28:58.077775 23128000471168 run.py:483] Algo bellman_ford step 2084 current loss 0.125820, current_train_items 66720.
I0304 19:28:58.097913 23128000471168 run.py:483] Algo bellman_ford step 2085 current loss 0.004408, current_train_items 66752.
I0304 19:28:58.114575 23128000471168 run.py:483] Algo bellman_ford step 2086 current loss 0.088120, current_train_items 66784.
I0304 19:28:58.138369 23128000471168 run.py:483] Algo bellman_ford step 2087 current loss 0.168880, current_train_items 66816.
I0304 19:28:58.166260 23128000471168 run.py:483] Algo bellman_ford step 2088 current loss 0.131049, current_train_items 66848.
I0304 19:28:58.199385 23128000471168 run.py:483] Algo bellman_ford step 2089 current loss 0.163967, current_train_items 66880.
I0304 19:28:58.219252 23128000471168 run.py:483] Algo bellman_ford step 2090 current loss 0.005634, current_train_items 66912.
I0304 19:28:58.235857 23128000471168 run.py:483] Algo bellman_ford step 2091 current loss 0.033186, current_train_items 66944.
I0304 19:28:58.259233 23128000471168 run.py:483] Algo bellman_ford step 2092 current loss 0.057970, current_train_items 66976.
I0304 19:28:58.289087 23128000471168 run.py:483] Algo bellman_ford step 2093 current loss 0.104765, current_train_items 67008.
I0304 19:28:58.321833 23128000471168 run.py:483] Algo bellman_ford step 2094 current loss 0.207840, current_train_items 67040.
I0304 19:28:58.341212 23128000471168 run.py:483] Algo bellman_ford step 2095 current loss 0.014516, current_train_items 67072.
I0304 19:28:58.357726 23128000471168 run.py:483] Algo bellman_ford step 2096 current loss 0.024362, current_train_items 67104.
I0304 19:28:58.381239 23128000471168 run.py:483] Algo bellman_ford step 2097 current loss 0.066647, current_train_items 67136.
I0304 19:28:58.410341 23128000471168 run.py:483] Algo bellman_ford step 2098 current loss 0.081972, current_train_items 67168.
I0304 19:28:58.442564 23128000471168 run.py:483] Algo bellman_ford step 2099 current loss 0.119092, current_train_items 67200.
I0304 19:28:58.462618 23128000471168 run.py:483] Algo bellman_ford step 2100 current loss 0.004930, current_train_items 67232.
I0304 19:28:58.470350 23128000471168 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0304 19:28:58.470458 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:28:58.487278 23128000471168 run.py:483] Algo bellman_ford step 2101 current loss 0.021129, current_train_items 67264.
I0304 19:28:58.511134 23128000471168 run.py:483] Algo bellman_ford step 2102 current loss 0.079878, current_train_items 67296.
I0304 19:28:58.541707 23128000471168 run.py:483] Algo bellman_ford step 2103 current loss 0.065401, current_train_items 67328.
I0304 19:28:58.573754 23128000471168 run.py:483] Algo bellman_ford step 2104 current loss 0.083025, current_train_items 67360.
I0304 19:28:58.593651 23128000471168 run.py:483] Algo bellman_ford step 2105 current loss 0.005372, current_train_items 67392.
I0304 19:28:58.609105 23128000471168 run.py:483] Algo bellman_ford step 2106 current loss 0.016438, current_train_items 67424.
I0304 19:28:58.633236 23128000471168 run.py:483] Algo bellman_ford step 2107 current loss 0.083339, current_train_items 67456.
I0304 19:28:58.664522 23128000471168 run.py:483] Algo bellman_ford step 2108 current loss 0.154404, current_train_items 67488.
I0304 19:28:58.695720 23128000471168 run.py:483] Algo bellman_ford step 2109 current loss 0.091993, current_train_items 67520.
I0304 19:28:58.715397 23128000471168 run.py:483] Algo bellman_ford step 2110 current loss 0.013132, current_train_items 67552.
I0304 19:28:58.731881 23128000471168 run.py:483] Algo bellman_ford step 2111 current loss 0.023187, current_train_items 67584.
I0304 19:28:58.754947 23128000471168 run.py:483] Algo bellman_ford step 2112 current loss 0.097265, current_train_items 67616.
I0304 19:28:58.784185 23128000471168 run.py:483] Algo bellman_ford step 2113 current loss 0.057500, current_train_items 67648.
I0304 19:28:58.815467 23128000471168 run.py:483] Algo bellman_ford step 2114 current loss 0.086200, current_train_items 67680.
I0304 19:28:58.834647 23128000471168 run.py:483] Algo bellman_ford step 2115 current loss 0.020361, current_train_items 67712.
I0304 19:28:58.851014 23128000471168 run.py:483] Algo bellman_ford step 2116 current loss 0.061738, current_train_items 67744.
I0304 19:28:58.875593 23128000471168 run.py:483] Algo bellman_ford step 2117 current loss 0.076943, current_train_items 67776.
I0304 19:28:58.906021 23128000471168 run.py:483] Algo bellman_ford step 2118 current loss 0.104231, current_train_items 67808.
I0304 19:28:58.937606 23128000471168 run.py:483] Algo bellman_ford step 2119 current loss 0.077235, current_train_items 67840.
I0304 19:28:58.956889 23128000471168 run.py:483] Algo bellman_ford step 2120 current loss 0.004656, current_train_items 67872.
I0304 19:28:58.973094 23128000471168 run.py:483] Algo bellman_ford step 2121 current loss 0.032200, current_train_items 67904.
I0304 19:28:58.996586 23128000471168 run.py:483] Algo bellman_ford step 2122 current loss 0.036665, current_train_items 67936.
I0304 19:28:59.027313 23128000471168 run.py:483] Algo bellman_ford step 2123 current loss 0.070751, current_train_items 67968.
I0304 19:28:59.056311 23128000471168 run.py:483] Algo bellman_ford step 2124 current loss 0.075509, current_train_items 68000.
I0304 19:28:59.075745 23128000471168 run.py:483] Algo bellman_ford step 2125 current loss 0.010443, current_train_items 68032.
I0304 19:28:59.092454 23128000471168 run.py:483] Algo bellman_ford step 2126 current loss 0.074773, current_train_items 68064.
I0304 19:28:59.117682 23128000471168 run.py:483] Algo bellman_ford step 2127 current loss 0.043996, current_train_items 68096.
I0304 19:28:59.147223 23128000471168 run.py:483] Algo bellman_ford step 2128 current loss 0.086622, current_train_items 68128.
I0304 19:28:59.178973 23128000471168 run.py:483] Algo bellman_ford step 2129 current loss 0.069436, current_train_items 68160.
I0304 19:28:59.198254 23128000471168 run.py:483] Algo bellman_ford step 2130 current loss 0.002822, current_train_items 68192.
I0304 19:28:59.215137 23128000471168 run.py:483] Algo bellman_ford step 2131 current loss 0.024508, current_train_items 68224.
I0304 19:28:59.239410 23128000471168 run.py:483] Algo bellman_ford step 2132 current loss 0.090618, current_train_items 68256.
I0304 19:28:59.268679 23128000471168 run.py:483] Algo bellman_ford step 2133 current loss 0.120367, current_train_items 68288.
I0304 19:28:59.302071 23128000471168 run.py:483] Algo bellman_ford step 2134 current loss 0.098458, current_train_items 68320.
I0304 19:28:59.321325 23128000471168 run.py:483] Algo bellman_ford step 2135 current loss 0.004309, current_train_items 68352.
I0304 19:28:59.337887 23128000471168 run.py:483] Algo bellman_ford step 2136 current loss 0.047885, current_train_items 68384.
I0304 19:28:59.360929 23128000471168 run.py:483] Algo bellman_ford step 2137 current loss 0.059581, current_train_items 68416.
I0304 19:28:59.391028 23128000471168 run.py:483] Algo bellman_ford step 2138 current loss 0.106190, current_train_items 68448.
I0304 19:28:59.426172 23128000471168 run.py:483] Algo bellman_ford step 2139 current loss 0.125997, current_train_items 68480.
I0304 19:28:59.445278 23128000471168 run.py:483] Algo bellman_ford step 2140 current loss 0.028018, current_train_items 68512.
I0304 19:28:59.461557 23128000471168 run.py:483] Algo bellman_ford step 2141 current loss 0.036668, current_train_items 68544.
I0304 19:28:59.483528 23128000471168 run.py:483] Algo bellman_ford step 2142 current loss 0.020586, current_train_items 68576.
I0304 19:28:59.513305 23128000471168 run.py:483] Algo bellman_ford step 2143 current loss 0.057669, current_train_items 68608.
I0304 19:28:59.546043 23128000471168 run.py:483] Algo bellman_ford step 2144 current loss 0.088322, current_train_items 68640.
I0304 19:28:59.565116 23128000471168 run.py:483] Algo bellman_ford step 2145 current loss 0.006770, current_train_items 68672.
I0304 19:28:59.581856 23128000471168 run.py:483] Algo bellman_ford step 2146 current loss 0.080612, current_train_items 68704.
I0304 19:28:59.605961 23128000471168 run.py:483] Algo bellman_ford step 2147 current loss 0.087171, current_train_items 68736.
I0304 19:28:59.636245 23128000471168 run.py:483] Algo bellman_ford step 2148 current loss 0.076987, current_train_items 68768.
I0304 19:28:59.667892 23128000471168 run.py:483] Algo bellman_ford step 2149 current loss 0.086680, current_train_items 68800.
I0304 19:28:59.687164 23128000471168 run.py:483] Algo bellman_ford step 2150 current loss 0.005799, current_train_items 68832.
I0304 19:28:59.695255 23128000471168 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0304 19:28:59.695363 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:59.712462 23128000471168 run.py:483] Algo bellman_ford step 2151 current loss 0.012936, current_train_items 68864.
I0304 19:28:59.737204 23128000471168 run.py:483] Algo bellman_ford step 2152 current loss 0.106143, current_train_items 68896.
I0304 19:28:59.767208 23128000471168 run.py:483] Algo bellman_ford step 2153 current loss 0.164842, current_train_items 68928.
I0304 19:28:59.799704 23128000471168 run.py:483] Algo bellman_ford step 2154 current loss 0.139175, current_train_items 68960.
I0304 19:28:59.819730 23128000471168 run.py:483] Algo bellman_ford step 2155 current loss 0.008722, current_train_items 68992.
I0304 19:28:59.835823 23128000471168 run.py:483] Algo bellman_ford step 2156 current loss 0.025992, current_train_items 69024.
I0304 19:28:59.859829 23128000471168 run.py:483] Algo bellman_ford step 2157 current loss 0.122154, current_train_items 69056.
I0304 19:28:59.890194 23128000471168 run.py:483] Algo bellman_ford step 2158 current loss 0.197334, current_train_items 69088.
I0304 19:28:59.923131 23128000471168 run.py:483] Algo bellman_ford step 2159 current loss 0.266177, current_train_items 69120.
I0304 19:28:59.943463 23128000471168 run.py:483] Algo bellman_ford step 2160 current loss 0.013486, current_train_items 69152.
I0304 19:28:59.959816 23128000471168 run.py:483] Algo bellman_ford step 2161 current loss 0.014144, current_train_items 69184.
I0304 19:28:59.983657 23128000471168 run.py:483] Algo bellman_ford step 2162 current loss 0.116068, current_train_items 69216.
I0304 19:29:00.012649 23128000471168 run.py:483] Algo bellman_ford step 2163 current loss 0.098953, current_train_items 69248.
I0304 19:29:00.046173 23128000471168 run.py:483] Algo bellman_ford step 2164 current loss 0.182938, current_train_items 69280.
I0304 19:29:00.065524 23128000471168 run.py:483] Algo bellman_ford step 2165 current loss 0.003546, current_train_items 69312.
I0304 19:29:00.082481 23128000471168 run.py:483] Algo bellman_ford step 2166 current loss 0.073296, current_train_items 69344.
I0304 19:29:00.105997 23128000471168 run.py:483] Algo bellman_ford step 2167 current loss 0.076603, current_train_items 69376.
I0304 19:29:00.134791 23128000471168 run.py:483] Algo bellman_ford step 2168 current loss 0.071047, current_train_items 69408.
I0304 19:29:00.167418 23128000471168 run.py:483] Algo bellman_ford step 2169 current loss 0.102313, current_train_items 69440.
I0304 19:29:00.186895 23128000471168 run.py:483] Algo bellman_ford step 2170 current loss 0.003057, current_train_items 69472.
I0304 19:29:00.203530 23128000471168 run.py:483] Algo bellman_ford step 2171 current loss 0.041084, current_train_items 69504.
I0304 19:29:00.226892 23128000471168 run.py:483] Algo bellman_ford step 2172 current loss 0.112575, current_train_items 69536.
I0304 19:29:00.257978 23128000471168 run.py:483] Algo bellman_ford step 2173 current loss 0.120720, current_train_items 69568.
I0304 19:29:00.288959 23128000471168 run.py:483] Algo bellman_ford step 2174 current loss 0.119531, current_train_items 69600.
I0304 19:29:00.308953 23128000471168 run.py:483] Algo bellman_ford step 2175 current loss 0.011823, current_train_items 69632.
I0304 19:29:00.325052 23128000471168 run.py:483] Algo bellman_ford step 2176 current loss 0.025398, current_train_items 69664.
I0304 19:29:00.349157 23128000471168 run.py:483] Algo bellman_ford step 2177 current loss 0.140530, current_train_items 69696.
I0304 19:29:00.378347 23128000471168 run.py:483] Algo bellman_ford step 2178 current loss 0.132465, current_train_items 69728.
I0304 19:29:00.411371 23128000471168 run.py:483] Algo bellman_ford step 2179 current loss 0.087135, current_train_items 69760.
I0304 19:29:00.430647 23128000471168 run.py:483] Algo bellman_ford step 2180 current loss 0.040133, current_train_items 69792.
I0304 19:29:00.447461 23128000471168 run.py:483] Algo bellman_ford step 2181 current loss 0.035159, current_train_items 69824.
I0304 19:29:00.471294 23128000471168 run.py:483] Algo bellman_ford step 2182 current loss 0.082287, current_train_items 69856.
I0304 19:29:00.500731 23128000471168 run.py:483] Algo bellman_ford step 2183 current loss 0.134924, current_train_items 69888.
I0304 19:29:00.534910 23128000471168 run.py:483] Algo bellman_ford step 2184 current loss 0.185838, current_train_items 69920.
I0304 19:29:00.554938 23128000471168 run.py:483] Algo bellman_ford step 2185 current loss 0.008657, current_train_items 69952.
I0304 19:29:00.571120 23128000471168 run.py:483] Algo bellman_ford step 2186 current loss 0.016515, current_train_items 69984.
I0304 19:29:00.594630 23128000471168 run.py:483] Algo bellman_ford step 2187 current loss 0.080019, current_train_items 70016.
I0304 19:29:00.624809 23128000471168 run.py:483] Algo bellman_ford step 2188 current loss 0.052927, current_train_items 70048.
W0304 19:29:00.649245 23128000471168 samplers.py:155] Increasing hint lengh from 12 to 13
I0304 19:29:07.455566 23128000471168 run.py:483] Algo bellman_ford step 2189 current loss 0.139707, current_train_items 70080.
I0304 19:29:07.477042 23128000471168 run.py:483] Algo bellman_ford step 2190 current loss 0.005671, current_train_items 70112.
I0304 19:29:07.493990 23128000471168 run.py:483] Algo bellman_ford step 2191 current loss 0.053316, current_train_items 70144.
I0304 19:29:07.518321 23128000471168 run.py:483] Algo bellman_ford step 2192 current loss 0.067646, current_train_items 70176.
W0304 19:29:07.539882 23128000471168 samplers.py:155] Increasing hint lengh from 10 to 12
I0304 19:29:14.732031 23128000471168 run.py:483] Algo bellman_ford step 2193 current loss 0.171943, current_train_items 70208.
I0304 19:29:14.764786 23128000471168 run.py:483] Algo bellman_ford step 2194 current loss 0.113914, current_train_items 70240.
I0304 19:29:14.785990 23128000471168 run.py:483] Algo bellman_ford step 2195 current loss 0.029728, current_train_items 70272.
I0304 19:29:14.802996 23128000471168 run.py:483] Algo bellman_ford step 2196 current loss 0.021107, current_train_items 70304.
I0304 19:29:14.826674 23128000471168 run.py:483] Algo bellman_ford step 2197 current loss 0.052136, current_train_items 70336.
I0304 19:29:14.856565 23128000471168 run.py:483] Algo bellman_ford step 2198 current loss 0.071457, current_train_items 70368.
I0304 19:29:14.889540 23128000471168 run.py:483] Algo bellman_ford step 2199 current loss 0.099581, current_train_items 70400.
I0304 19:29:14.909802 23128000471168 run.py:483] Algo bellman_ford step 2200 current loss 0.018342, current_train_items 70432.
I0304 19:29:14.919079 23128000471168 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0304 19:29:14.919187 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:14.936604 23128000471168 run.py:483] Algo bellman_ford step 2201 current loss 0.033098, current_train_items 70464.
I0304 19:29:14.960703 23128000471168 run.py:483] Algo bellman_ford step 2202 current loss 0.056002, current_train_items 70496.
I0304 19:29:14.991193 23128000471168 run.py:483] Algo bellman_ford step 2203 current loss 0.077202, current_train_items 70528.
I0304 19:29:15.025137 23128000471168 run.py:483] Algo bellman_ford step 2204 current loss 0.072811, current_train_items 70560.
I0304 19:29:15.045613 23128000471168 run.py:483] Algo bellman_ford step 2205 current loss 0.002466, current_train_items 70592.
I0304 19:29:15.062255 23128000471168 run.py:483] Algo bellman_ford step 2206 current loss 0.032687, current_train_items 70624.
I0304 19:29:15.085951 23128000471168 run.py:483] Algo bellman_ford step 2207 current loss 0.050555, current_train_items 70656.
I0304 19:29:15.116517 23128000471168 run.py:483] Algo bellman_ford step 2208 current loss 0.130130, current_train_items 70688.
I0304 19:29:15.151232 23128000471168 run.py:483] Algo bellman_ford step 2209 current loss 0.107741, current_train_items 70720.
I0304 19:29:15.171904 23128000471168 run.py:483] Algo bellman_ford step 2210 current loss 0.009046, current_train_items 70752.
I0304 19:29:15.188394 23128000471168 run.py:483] Algo bellman_ford step 2211 current loss 0.025799, current_train_items 70784.
I0304 19:29:15.212638 23128000471168 run.py:483] Algo bellman_ford step 2212 current loss 0.120743, current_train_items 70816.
I0304 19:29:15.243884 23128000471168 run.py:483] Algo bellman_ford step 2213 current loss 0.162312, current_train_items 70848.
I0304 19:29:15.278297 23128000471168 run.py:483] Algo bellman_ford step 2214 current loss 0.122050, current_train_items 70880.
I0304 19:29:15.298410 23128000471168 run.py:483] Algo bellman_ford step 2215 current loss 0.010581, current_train_items 70912.
I0304 19:29:15.314870 23128000471168 run.py:483] Algo bellman_ford step 2216 current loss 0.017544, current_train_items 70944.
I0304 19:29:15.337527 23128000471168 run.py:483] Algo bellman_ford step 2217 current loss 0.076276, current_train_items 70976.
I0304 19:29:15.369083 23128000471168 run.py:483] Algo bellman_ford step 2218 current loss 0.319894, current_train_items 71008.
I0304 19:29:15.401557 23128000471168 run.py:483] Algo bellman_ford step 2219 current loss 0.224343, current_train_items 71040.
I0304 19:29:15.421318 23128000471168 run.py:483] Algo bellman_ford step 2220 current loss 0.014103, current_train_items 71072.
I0304 19:29:15.437588 23128000471168 run.py:483] Algo bellman_ford step 2221 current loss 0.043029, current_train_items 71104.
I0304 19:29:15.461333 23128000471168 run.py:483] Algo bellman_ford step 2222 current loss 0.097117, current_train_items 71136.
I0304 19:29:15.492526 23128000471168 run.py:483] Algo bellman_ford step 2223 current loss 0.058619, current_train_items 71168.
I0304 19:29:15.526958 23128000471168 run.py:483] Algo bellman_ford step 2224 current loss 0.172471, current_train_items 71200.
I0304 19:29:15.546837 23128000471168 run.py:483] Algo bellman_ford step 2225 current loss 0.024508, current_train_items 71232.
I0304 19:29:15.563390 23128000471168 run.py:483] Algo bellman_ford step 2226 current loss 0.050133, current_train_items 71264.
I0304 19:29:15.587183 23128000471168 run.py:483] Algo bellman_ford step 2227 current loss 0.071712, current_train_items 71296.
I0304 19:29:15.617892 23128000471168 run.py:483] Algo bellman_ford step 2228 current loss 0.104342, current_train_items 71328.
I0304 19:29:15.648988 23128000471168 run.py:483] Algo bellman_ford step 2229 current loss 0.109247, current_train_items 71360.
I0304 19:29:15.669001 23128000471168 run.py:483] Algo bellman_ford step 2230 current loss 0.006495, current_train_items 71392.
I0304 19:29:15.685700 23128000471168 run.py:483] Algo bellman_ford step 2231 current loss 0.053864, current_train_items 71424.
I0304 19:29:15.709460 23128000471168 run.py:483] Algo bellman_ford step 2232 current loss 0.073766, current_train_items 71456.
I0304 19:29:15.739435 23128000471168 run.py:483] Algo bellman_ford step 2233 current loss 0.082833, current_train_items 71488.
I0304 19:29:15.772123 23128000471168 run.py:483] Algo bellman_ford step 2234 current loss 0.098321, current_train_items 71520.
I0304 19:29:15.792157 23128000471168 run.py:483] Algo bellman_ford step 2235 current loss 0.006268, current_train_items 71552.
I0304 19:29:15.808911 23128000471168 run.py:483] Algo bellman_ford step 2236 current loss 0.042092, current_train_items 71584.
I0304 19:29:15.833465 23128000471168 run.py:483] Algo bellman_ford step 2237 current loss 0.068977, current_train_items 71616.
I0304 19:29:15.864356 23128000471168 run.py:483] Algo bellman_ford step 2238 current loss 0.067879, current_train_items 71648.
I0304 19:29:15.896604 23128000471168 run.py:483] Algo bellman_ford step 2239 current loss 0.137395, current_train_items 71680.
I0304 19:29:15.916309 23128000471168 run.py:483] Algo bellman_ford step 2240 current loss 0.056225, current_train_items 71712.
I0304 19:29:15.933084 23128000471168 run.py:483] Algo bellman_ford step 2241 current loss 0.043056, current_train_items 71744.
I0304 19:29:15.957617 23128000471168 run.py:483] Algo bellman_ford step 2242 current loss 0.069589, current_train_items 71776.
I0304 19:29:15.988763 23128000471168 run.py:483] Algo bellman_ford step 2243 current loss 0.092871, current_train_items 71808.
I0304 19:29:16.020537 23128000471168 run.py:483] Algo bellman_ford step 2244 current loss 0.075916, current_train_items 71840.
I0304 19:29:16.040339 23128000471168 run.py:483] Algo bellman_ford step 2245 current loss 0.006352, current_train_items 71872.
I0304 19:29:16.056986 23128000471168 run.py:483] Algo bellman_ford step 2246 current loss 0.066508, current_train_items 71904.
I0304 19:29:16.080366 23128000471168 run.py:483] Algo bellman_ford step 2247 current loss 0.137299, current_train_items 71936.
I0304 19:29:16.111872 23128000471168 run.py:483] Algo bellman_ford step 2248 current loss 0.089467, current_train_items 71968.
I0304 19:29:16.145368 23128000471168 run.py:483] Algo bellman_ford step 2249 current loss 0.103604, current_train_items 72000.
I0304 19:29:16.165351 23128000471168 run.py:483] Algo bellman_ford step 2250 current loss 0.006393, current_train_items 72032.
I0304 19:29:16.173540 23128000471168 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0304 19:29:16.173647 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:29:16.190664 23128000471168 run.py:483] Algo bellman_ford step 2251 current loss 0.029024, current_train_items 72064.
I0304 19:29:16.213863 23128000471168 run.py:483] Algo bellman_ford step 2252 current loss 0.059934, current_train_items 72096.
I0304 19:29:16.246652 23128000471168 run.py:483] Algo bellman_ford step 2253 current loss 0.162110, current_train_items 72128.
I0304 19:29:16.281404 23128000471168 run.py:483] Algo bellman_ford step 2254 current loss 0.162434, current_train_items 72160.
I0304 19:29:16.301687 23128000471168 run.py:483] Algo bellman_ford step 2255 current loss 0.004947, current_train_items 72192.
I0304 19:29:16.318125 23128000471168 run.py:483] Algo bellman_ford step 2256 current loss 0.024119, current_train_items 72224.
I0304 19:29:16.341731 23128000471168 run.py:483] Algo bellman_ford step 2257 current loss 0.082379, current_train_items 72256.
I0304 19:29:16.371546 23128000471168 run.py:483] Algo bellman_ford step 2258 current loss 0.107457, current_train_items 72288.
I0304 19:29:16.406022 23128000471168 run.py:483] Algo bellman_ford step 2259 current loss 0.126439, current_train_items 72320.
I0304 19:29:16.426840 23128000471168 run.py:483] Algo bellman_ford step 2260 current loss 0.012520, current_train_items 72352.
I0304 19:29:16.443942 23128000471168 run.py:483] Algo bellman_ford step 2261 current loss 0.021993, current_train_items 72384.
I0304 19:29:16.469227 23128000471168 run.py:483] Algo bellman_ford step 2262 current loss 0.061347, current_train_items 72416.
I0304 19:29:16.499249 23128000471168 run.py:483] Algo bellman_ford step 2263 current loss 0.063347, current_train_items 72448.
I0304 19:29:16.533223 23128000471168 run.py:483] Algo bellman_ford step 2264 current loss 0.125268, current_train_items 72480.
I0304 19:29:16.553579 23128000471168 run.py:483] Algo bellman_ford step 2265 current loss 0.050143, current_train_items 72512.
I0304 19:29:16.570047 23128000471168 run.py:483] Algo bellman_ford step 2266 current loss 0.055110, current_train_items 72544.
I0304 19:29:16.595210 23128000471168 run.py:483] Algo bellman_ford step 2267 current loss 0.077143, current_train_items 72576.
I0304 19:29:16.626132 23128000471168 run.py:483] Algo bellman_ford step 2268 current loss 0.099945, current_train_items 72608.
I0304 19:29:16.659065 23128000471168 run.py:483] Algo bellman_ford step 2269 current loss 0.085146, current_train_items 72640.
I0304 19:29:16.679412 23128000471168 run.py:483] Algo bellman_ford step 2270 current loss 0.006241, current_train_items 72672.
I0304 19:29:16.695935 23128000471168 run.py:483] Algo bellman_ford step 2271 current loss 0.023368, current_train_items 72704.
I0304 19:29:16.718671 23128000471168 run.py:483] Algo bellman_ford step 2272 current loss 0.085159, current_train_items 72736.
I0304 19:29:16.749991 23128000471168 run.py:483] Algo bellman_ford step 2273 current loss 0.082089, current_train_items 72768.
I0304 19:29:16.785940 23128000471168 run.py:483] Algo bellman_ford step 2274 current loss 0.138350, current_train_items 72800.
I0304 19:29:16.806554 23128000471168 run.py:483] Algo bellman_ford step 2275 current loss 0.004221, current_train_items 72832.
I0304 19:29:16.822909 23128000471168 run.py:483] Algo bellman_ford step 2276 current loss 0.017334, current_train_items 72864.
I0304 19:29:16.845137 23128000471168 run.py:483] Algo bellman_ford step 2277 current loss 0.041064, current_train_items 72896.
I0304 19:29:16.875059 23128000471168 run.py:483] Algo bellman_ford step 2278 current loss 0.127688, current_train_items 72928.
I0304 19:29:16.909905 23128000471168 run.py:483] Algo bellman_ford step 2279 current loss 0.093164, current_train_items 72960.
I0304 19:29:16.930032 23128000471168 run.py:483] Algo bellman_ford step 2280 current loss 0.002560, current_train_items 72992.
I0304 19:29:16.946183 23128000471168 run.py:483] Algo bellman_ford step 2281 current loss 0.054985, current_train_items 73024.
I0304 19:29:16.970751 23128000471168 run.py:483] Algo bellman_ford step 2282 current loss 0.051185, current_train_items 73056.
I0304 19:29:17.004366 23128000471168 run.py:483] Algo bellman_ford step 2283 current loss 0.172255, current_train_items 73088.
I0304 19:29:17.038978 23128000471168 run.py:483] Algo bellman_ford step 2284 current loss 0.098362, current_train_items 73120.
I0304 19:29:17.059194 23128000471168 run.py:483] Algo bellman_ford step 2285 current loss 0.007795, current_train_items 73152.
I0304 19:29:17.075725 23128000471168 run.py:483] Algo bellman_ford step 2286 current loss 0.028316, current_train_items 73184.
I0304 19:29:17.099190 23128000471168 run.py:483] Algo bellman_ford step 2287 current loss 0.077944, current_train_items 73216.
I0304 19:29:17.129769 23128000471168 run.py:483] Algo bellman_ford step 2288 current loss 0.081571, current_train_items 73248.
I0304 19:29:17.165696 23128000471168 run.py:483] Algo bellman_ford step 2289 current loss 0.164366, current_train_items 73280.
I0304 19:29:17.186062 23128000471168 run.py:483] Algo bellman_ford step 2290 current loss 0.002368, current_train_items 73312.
I0304 19:29:17.202084 23128000471168 run.py:483] Algo bellman_ford step 2291 current loss 0.022051, current_train_items 73344.
I0304 19:29:17.225355 23128000471168 run.py:483] Algo bellman_ford step 2292 current loss 0.055681, current_train_items 73376.
I0304 19:29:17.256815 23128000471168 run.py:483] Algo bellman_ford step 2293 current loss 0.083326, current_train_items 73408.
I0304 19:29:17.290959 23128000471168 run.py:483] Algo bellman_ford step 2294 current loss 0.122946, current_train_items 73440.
I0304 19:29:17.310862 23128000471168 run.py:483] Algo bellman_ford step 2295 current loss 0.008025, current_train_items 73472.
I0304 19:29:17.327245 23128000471168 run.py:483] Algo bellman_ford step 2296 current loss 0.046386, current_train_items 73504.
I0304 19:29:17.350547 23128000471168 run.py:483] Algo bellman_ford step 2297 current loss 0.060811, current_train_items 73536.
I0304 19:29:17.380769 23128000471168 run.py:483] Algo bellman_ford step 2298 current loss 0.133070, current_train_items 73568.
I0304 19:29:17.415036 23128000471168 run.py:483] Algo bellman_ford step 2299 current loss 0.117692, current_train_items 73600.
I0304 19:29:17.435581 23128000471168 run.py:483] Algo bellman_ford step 2300 current loss 0.009680, current_train_items 73632.
I0304 19:29:17.443288 23128000471168 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0304 19:29:17.443393 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:17.460874 23128000471168 run.py:483] Algo bellman_ford step 2301 current loss 0.046193, current_train_items 73664.
I0304 19:29:17.485430 23128000471168 run.py:483] Algo bellman_ford step 2302 current loss 0.076293, current_train_items 73696.
I0304 19:29:17.518032 23128000471168 run.py:483] Algo bellman_ford step 2303 current loss 0.097829, current_train_items 73728.
I0304 19:29:17.552510 23128000471168 run.py:483] Algo bellman_ford step 2304 current loss 0.090038, current_train_items 73760.
I0304 19:29:17.572951 23128000471168 run.py:483] Algo bellman_ford step 2305 current loss 0.024165, current_train_items 73792.
I0304 19:29:17.589679 23128000471168 run.py:483] Algo bellman_ford step 2306 current loss 0.102349, current_train_items 73824.
I0304 19:29:17.613159 23128000471168 run.py:483] Algo bellman_ford step 2307 current loss 0.102668, current_train_items 73856.
I0304 19:29:17.643217 23128000471168 run.py:483] Algo bellman_ford step 2308 current loss 0.065104, current_train_items 73888.
I0304 19:29:17.677345 23128000471168 run.py:483] Algo bellman_ford step 2309 current loss 0.090419, current_train_items 73920.
I0304 19:29:17.697660 23128000471168 run.py:483] Algo bellman_ford step 2310 current loss 0.005952, current_train_items 73952.
I0304 19:29:17.714139 23128000471168 run.py:483] Algo bellman_ford step 2311 current loss 0.021450, current_train_items 73984.
I0304 19:29:17.738145 23128000471168 run.py:483] Algo bellman_ford step 2312 current loss 0.092357, current_train_items 74016.
I0304 19:29:17.769252 23128000471168 run.py:483] Algo bellman_ford step 2313 current loss 0.092754, current_train_items 74048.
I0304 19:29:17.803167 23128000471168 run.py:483] Algo bellman_ford step 2314 current loss 0.093161, current_train_items 74080.
I0304 19:29:17.823334 23128000471168 run.py:483] Algo bellman_ford step 2315 current loss 0.017955, current_train_items 74112.
I0304 19:29:17.839700 23128000471168 run.py:483] Algo bellman_ford step 2316 current loss 0.049624, current_train_items 74144.
I0304 19:29:17.863074 23128000471168 run.py:483] Algo bellman_ford step 2317 current loss 0.083618, current_train_items 74176.
I0304 19:29:17.894757 23128000471168 run.py:483] Algo bellman_ford step 2318 current loss 0.072436, current_train_items 74208.
I0304 19:29:17.929517 23128000471168 run.py:483] Algo bellman_ford step 2319 current loss 0.123617, current_train_items 74240.
I0304 19:29:17.949486 23128000471168 run.py:483] Algo bellman_ford step 2320 current loss 0.009126, current_train_items 74272.
I0304 19:29:17.965693 23128000471168 run.py:483] Algo bellman_ford step 2321 current loss 0.030918, current_train_items 74304.
I0304 19:29:17.990359 23128000471168 run.py:483] Algo bellman_ford step 2322 current loss 0.126146, current_train_items 74336.
I0304 19:29:18.020638 23128000471168 run.py:483] Algo bellman_ford step 2323 current loss 0.100537, current_train_items 74368.
I0304 19:29:18.054740 23128000471168 run.py:483] Algo bellman_ford step 2324 current loss 0.154550, current_train_items 74400.
I0304 19:29:18.074920 23128000471168 run.py:483] Algo bellman_ford step 2325 current loss 0.006764, current_train_items 74432.
I0304 19:29:18.091194 23128000471168 run.py:483] Algo bellman_ford step 2326 current loss 0.031281, current_train_items 74464.
I0304 19:29:18.114280 23128000471168 run.py:483] Algo bellman_ford step 2327 current loss 0.106149, current_train_items 74496.
I0304 19:29:18.143674 23128000471168 run.py:483] Algo bellman_ford step 2328 current loss 0.062954, current_train_items 74528.
I0304 19:29:18.176177 23128000471168 run.py:483] Algo bellman_ford step 2329 current loss 0.134900, current_train_items 74560.
I0304 19:29:18.196466 23128000471168 run.py:483] Algo bellman_ford step 2330 current loss 0.010060, current_train_items 74592.
I0304 19:29:18.212446 23128000471168 run.py:483] Algo bellman_ford step 2331 current loss 0.033532, current_train_items 74624.
I0304 19:29:18.236569 23128000471168 run.py:483] Algo bellman_ford step 2332 current loss 0.096104, current_train_items 74656.
I0304 19:29:18.268158 23128000471168 run.py:483] Algo bellman_ford step 2333 current loss 0.102065, current_train_items 74688.
I0304 19:29:18.302796 23128000471168 run.py:483] Algo bellman_ford step 2334 current loss 0.086219, current_train_items 74720.
I0304 19:29:18.322907 23128000471168 run.py:483] Algo bellman_ford step 2335 current loss 0.006845, current_train_items 74752.
I0304 19:29:18.339623 23128000471168 run.py:483] Algo bellman_ford step 2336 current loss 0.025168, current_train_items 74784.
I0304 19:29:18.363602 23128000471168 run.py:483] Algo bellman_ford step 2337 current loss 0.038929, current_train_items 74816.
I0304 19:29:18.394235 23128000471168 run.py:483] Algo bellman_ford step 2338 current loss 0.092844, current_train_items 74848.
I0304 19:29:18.427346 23128000471168 run.py:483] Algo bellman_ford step 2339 current loss 0.088023, current_train_items 74880.
I0304 19:29:18.447520 23128000471168 run.py:483] Algo bellman_ford step 2340 current loss 0.007609, current_train_items 74912.
I0304 19:29:18.464204 23128000471168 run.py:483] Algo bellman_ford step 2341 current loss 0.021341, current_train_items 74944.
I0304 19:29:18.487207 23128000471168 run.py:483] Algo bellman_ford step 2342 current loss 0.072334, current_train_items 74976.
I0304 19:29:18.516915 23128000471168 run.py:483] Algo bellman_ford step 2343 current loss 0.075011, current_train_items 75008.
I0304 19:29:18.549407 23128000471168 run.py:483] Algo bellman_ford step 2344 current loss 0.110753, current_train_items 75040.
I0304 19:29:18.569469 23128000471168 run.py:483] Algo bellman_ford step 2345 current loss 0.017388, current_train_items 75072.
I0304 19:29:18.585896 23128000471168 run.py:483] Algo bellman_ford step 2346 current loss 0.023898, current_train_items 75104.
I0304 19:29:18.609987 23128000471168 run.py:483] Algo bellman_ford step 2347 current loss 0.033824, current_train_items 75136.
I0304 19:29:18.639507 23128000471168 run.py:483] Algo bellman_ford step 2348 current loss 0.055038, current_train_items 75168.
I0304 19:29:18.671886 23128000471168 run.py:483] Algo bellman_ford step 2349 current loss 0.092170, current_train_items 75200.
I0304 19:29:18.691830 23128000471168 run.py:483] Algo bellman_ford step 2350 current loss 0.008039, current_train_items 75232.
I0304 19:29:18.700177 23128000471168 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0304 19:29:18.700287 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:18.717398 23128000471168 run.py:483] Algo bellman_ford step 2351 current loss 0.020411, current_train_items 75264.
I0304 19:29:18.741564 23128000471168 run.py:483] Algo bellman_ford step 2352 current loss 0.104435, current_train_items 75296.
I0304 19:29:18.771493 23128000471168 run.py:483] Algo bellman_ford step 2353 current loss 0.058602, current_train_items 75328.
I0304 19:29:18.807137 23128000471168 run.py:483] Algo bellman_ford step 2354 current loss 0.100723, current_train_items 75360.
I0304 19:29:18.827439 23128000471168 run.py:483] Algo bellman_ford step 2355 current loss 0.056568, current_train_items 75392.
I0304 19:29:18.843301 23128000471168 run.py:483] Algo bellman_ford step 2356 current loss 0.022554, current_train_items 75424.
I0304 19:29:18.867445 23128000471168 run.py:483] Algo bellman_ford step 2357 current loss 0.110206, current_train_items 75456.
I0304 19:29:18.897081 23128000471168 run.py:483] Algo bellman_ford step 2358 current loss 0.149412, current_train_items 75488.
I0304 19:29:18.932485 23128000471168 run.py:483] Algo bellman_ford step 2359 current loss 0.143977, current_train_items 75520.
I0304 19:29:18.952513 23128000471168 run.py:483] Algo bellman_ford step 2360 current loss 0.008526, current_train_items 75552.
I0304 19:29:18.969707 23128000471168 run.py:483] Algo bellman_ford step 2361 current loss 0.028023, current_train_items 75584.
I0304 19:29:18.994732 23128000471168 run.py:483] Algo bellman_ford step 2362 current loss 0.203643, current_train_items 75616.
I0304 19:29:19.024835 23128000471168 run.py:483] Algo bellman_ford step 2363 current loss 0.146979, current_train_items 75648.
I0304 19:29:19.058406 23128000471168 run.py:483] Algo bellman_ford step 2364 current loss 0.159385, current_train_items 75680.
I0304 19:29:19.078324 23128000471168 run.py:483] Algo bellman_ford step 2365 current loss 0.003190, current_train_items 75712.
I0304 19:29:19.094996 23128000471168 run.py:483] Algo bellman_ford step 2366 current loss 0.016745, current_train_items 75744.
I0304 19:29:19.118731 23128000471168 run.py:483] Algo bellman_ford step 2367 current loss 0.134080, current_train_items 75776.
I0304 19:29:19.150536 23128000471168 run.py:483] Algo bellman_ford step 2368 current loss 0.120084, current_train_items 75808.
I0304 19:29:19.185256 23128000471168 run.py:483] Algo bellman_ford step 2369 current loss 0.120962, current_train_items 75840.
I0304 19:29:19.206099 23128000471168 run.py:483] Algo bellman_ford step 2370 current loss 0.003109, current_train_items 75872.
I0304 19:29:19.222503 23128000471168 run.py:483] Algo bellman_ford step 2371 current loss 0.045850, current_train_items 75904.
I0304 19:29:19.246241 23128000471168 run.py:483] Algo bellman_ford step 2372 current loss 0.084689, current_train_items 75936.
I0304 19:29:19.276077 23128000471168 run.py:483] Algo bellman_ford step 2373 current loss 0.099878, current_train_items 75968.
I0304 19:29:19.308965 23128000471168 run.py:483] Algo bellman_ford step 2374 current loss 0.080670, current_train_items 76000.
I0304 19:29:19.329093 23128000471168 run.py:483] Algo bellman_ford step 2375 current loss 0.003380, current_train_items 76032.
I0304 19:29:19.345979 23128000471168 run.py:483] Algo bellman_ford step 2376 current loss 0.012006, current_train_items 76064.
I0304 19:29:19.369708 23128000471168 run.py:483] Algo bellman_ford step 2377 current loss 0.103458, current_train_items 76096.
I0304 19:29:19.401619 23128000471168 run.py:483] Algo bellman_ford step 2378 current loss 0.095389, current_train_items 76128.
I0304 19:29:19.436753 23128000471168 run.py:483] Algo bellman_ford step 2379 current loss 0.105504, current_train_items 76160.
I0304 19:29:19.457088 23128000471168 run.py:483] Algo bellman_ford step 2380 current loss 0.035572, current_train_items 76192.
I0304 19:29:19.474190 23128000471168 run.py:483] Algo bellman_ford step 2381 current loss 0.059409, current_train_items 76224.
I0304 19:29:19.498328 23128000471168 run.py:483] Algo bellman_ford step 2382 current loss 0.088172, current_train_items 76256.
I0304 19:29:19.527870 23128000471168 run.py:483] Algo bellman_ford step 2383 current loss 0.086558, current_train_items 76288.
I0304 19:29:19.562228 23128000471168 run.py:483] Algo bellman_ford step 2384 current loss 0.110139, current_train_items 76320.
I0304 19:29:19.582703 23128000471168 run.py:483] Algo bellman_ford step 2385 current loss 0.005482, current_train_items 76352.
I0304 19:29:19.599031 23128000471168 run.py:483] Algo bellman_ford step 2386 current loss 0.018560, current_train_items 76384.
I0304 19:29:19.622099 23128000471168 run.py:483] Algo bellman_ford step 2387 current loss 0.070033, current_train_items 76416.
I0304 19:29:19.653287 23128000471168 run.py:483] Algo bellman_ford step 2388 current loss 0.100432, current_train_items 76448.
I0304 19:29:19.688690 23128000471168 run.py:483] Algo bellman_ford step 2389 current loss 0.119453, current_train_items 76480.
I0304 19:29:19.709054 23128000471168 run.py:483] Algo bellman_ford step 2390 current loss 0.004146, current_train_items 76512.
I0304 19:29:19.725156 23128000471168 run.py:483] Algo bellman_ford step 2391 current loss 0.050485, current_train_items 76544.
I0304 19:29:19.747907 23128000471168 run.py:483] Algo bellman_ford step 2392 current loss 0.054798, current_train_items 76576.
I0304 19:29:19.778608 23128000471168 run.py:483] Algo bellman_ford step 2393 current loss 0.076174, current_train_items 76608.
I0304 19:29:19.815209 23128000471168 run.py:483] Algo bellman_ford step 2394 current loss 0.105739, current_train_items 76640.
I0304 19:29:19.835165 23128000471168 run.py:483] Algo bellman_ford step 2395 current loss 0.011538, current_train_items 76672.
I0304 19:29:19.851402 23128000471168 run.py:483] Algo bellman_ford step 2396 current loss 0.009706, current_train_items 76704.
I0304 19:29:19.874307 23128000471168 run.py:483] Algo bellman_ford step 2397 current loss 0.054912, current_train_items 76736.
I0304 19:29:19.904569 23128000471168 run.py:483] Algo bellman_ford step 2398 current loss 0.157501, current_train_items 76768.
I0304 19:29:19.939508 23128000471168 run.py:483] Algo bellman_ford step 2399 current loss 0.127918, current_train_items 76800.
I0304 19:29:19.960068 23128000471168 run.py:483] Algo bellman_ford step 2400 current loss 0.003231, current_train_items 76832.
I0304 19:29:19.967977 23128000471168 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0304 19:29:19.968096 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:29:19.985187 23128000471168 run.py:483] Algo bellman_ford step 2401 current loss 0.017805, current_train_items 76864.
I0304 19:29:20.009449 23128000471168 run.py:483] Algo bellman_ford step 2402 current loss 0.066103, current_train_items 76896.
I0304 19:29:20.041254 23128000471168 run.py:483] Algo bellman_ford step 2403 current loss 0.122168, current_train_items 76928.
I0304 19:29:20.078146 23128000471168 run.py:483] Algo bellman_ford step 2404 current loss 0.136077, current_train_items 76960.
I0304 19:29:20.098836 23128000471168 run.py:483] Algo bellman_ford step 2405 current loss 0.008037, current_train_items 76992.
I0304 19:29:20.115024 23128000471168 run.py:483] Algo bellman_ford step 2406 current loss 0.046376, current_train_items 77024.
I0304 19:29:20.138844 23128000471168 run.py:483] Algo bellman_ford step 2407 current loss 0.061111, current_train_items 77056.
I0304 19:29:20.168569 23128000471168 run.py:483] Algo bellman_ford step 2408 current loss 0.103973, current_train_items 77088.
I0304 19:29:20.201701 23128000471168 run.py:483] Algo bellman_ford step 2409 current loss 0.093423, current_train_items 77120.
I0304 19:29:20.221535 23128000471168 run.py:483] Algo bellman_ford step 2410 current loss 0.002837, current_train_items 77152.
I0304 19:29:20.238093 23128000471168 run.py:483] Algo bellman_ford step 2411 current loss 0.025053, current_train_items 77184.
I0304 19:29:20.263203 23128000471168 run.py:483] Algo bellman_ford step 2412 current loss 0.098406, current_train_items 77216.
I0304 19:29:20.294590 23128000471168 run.py:483] Algo bellman_ford step 2413 current loss 0.071669, current_train_items 77248.
I0304 19:29:20.329469 23128000471168 run.py:483] Algo bellman_ford step 2414 current loss 0.091301, current_train_items 77280.
I0304 19:29:20.349479 23128000471168 run.py:483] Algo bellman_ford step 2415 current loss 0.002418, current_train_items 77312.
I0304 19:29:20.365241 23128000471168 run.py:483] Algo bellman_ford step 2416 current loss 0.052390, current_train_items 77344.
I0304 19:29:20.388931 23128000471168 run.py:483] Algo bellman_ford step 2417 current loss 0.084555, current_train_items 77376.
I0304 19:29:20.420911 23128000471168 run.py:483] Algo bellman_ford step 2418 current loss 0.109047, current_train_items 77408.
I0304 19:29:20.452870 23128000471168 run.py:483] Algo bellman_ford step 2419 current loss 0.063055, current_train_items 77440.
I0304 19:29:20.472577 23128000471168 run.py:483] Algo bellman_ford step 2420 current loss 0.008718, current_train_items 77472.
I0304 19:29:20.488817 23128000471168 run.py:483] Algo bellman_ford step 2421 current loss 0.059016, current_train_items 77504.
I0304 19:29:20.513383 23128000471168 run.py:483] Algo bellman_ford step 2422 current loss 0.125364, current_train_items 77536.
I0304 19:29:20.544586 23128000471168 run.py:483] Algo bellman_ford step 2423 current loss 0.098916, current_train_items 77568.
I0304 19:29:20.579176 23128000471168 run.py:483] Algo bellman_ford step 2424 current loss 0.115626, current_train_items 77600.
I0304 19:29:20.599119 23128000471168 run.py:483] Algo bellman_ford step 2425 current loss 0.025518, current_train_items 77632.
I0304 19:29:20.615401 23128000471168 run.py:483] Algo bellman_ford step 2426 current loss 0.091082, current_train_items 77664.
I0304 19:29:20.640243 23128000471168 run.py:483] Algo bellman_ford step 2427 current loss 0.094613, current_train_items 77696.
I0304 19:29:20.669895 23128000471168 run.py:483] Algo bellman_ford step 2428 current loss 0.098946, current_train_items 77728.
I0304 19:29:20.702964 23128000471168 run.py:483] Algo bellman_ford step 2429 current loss 0.087546, current_train_items 77760.
I0304 19:29:20.722977 23128000471168 run.py:483] Algo bellman_ford step 2430 current loss 0.018009, current_train_items 77792.
I0304 19:29:20.739547 23128000471168 run.py:483] Algo bellman_ford step 2431 current loss 0.025973, current_train_items 77824.
I0304 19:29:20.764361 23128000471168 run.py:483] Algo bellman_ford step 2432 current loss 0.069183, current_train_items 77856.
I0304 19:29:20.794696 23128000471168 run.py:483] Algo bellman_ford step 2433 current loss 0.057875, current_train_items 77888.
I0304 19:29:20.828686 23128000471168 run.py:483] Algo bellman_ford step 2434 current loss 0.072258, current_train_items 77920.
I0304 19:29:20.848360 23128000471168 run.py:483] Algo bellman_ford step 2435 current loss 0.007952, current_train_items 77952.
I0304 19:29:20.865053 23128000471168 run.py:483] Algo bellman_ford step 2436 current loss 0.041039, current_train_items 77984.
I0304 19:29:20.888792 23128000471168 run.py:483] Algo bellman_ford step 2437 current loss 0.087544, current_train_items 78016.
I0304 19:29:20.917948 23128000471168 run.py:483] Algo bellman_ford step 2438 current loss 0.114394, current_train_items 78048.
I0304 19:29:20.950986 23128000471168 run.py:483] Algo bellman_ford step 2439 current loss 0.061669, current_train_items 78080.
I0304 19:29:20.970983 23128000471168 run.py:483] Algo bellman_ford step 2440 current loss 0.026837, current_train_items 78112.
I0304 19:29:20.987264 23128000471168 run.py:483] Algo bellman_ford step 2441 current loss 0.027053, current_train_items 78144.
I0304 19:29:21.012070 23128000471168 run.py:483] Algo bellman_ford step 2442 current loss 0.156720, current_train_items 78176.
I0304 19:29:21.042667 23128000471168 run.py:483] Algo bellman_ford step 2443 current loss 0.150727, current_train_items 78208.
I0304 19:29:21.077811 23128000471168 run.py:483] Algo bellman_ford step 2444 current loss 0.126154, current_train_items 78240.
I0304 19:29:21.098175 23128000471168 run.py:483] Algo bellman_ford step 2445 current loss 0.021875, current_train_items 78272.
I0304 19:29:21.114534 23128000471168 run.py:483] Algo bellman_ford step 2446 current loss 0.030068, current_train_items 78304.
I0304 19:29:21.139386 23128000471168 run.py:483] Algo bellman_ford step 2447 current loss 0.078766, current_train_items 78336.
I0304 19:29:21.169063 23128000471168 run.py:483] Algo bellman_ford step 2448 current loss 0.125473, current_train_items 78368.
I0304 19:29:21.202314 23128000471168 run.py:483] Algo bellman_ford step 2449 current loss 0.180642, current_train_items 78400.
I0304 19:29:21.222228 23128000471168 run.py:483] Algo bellman_ford step 2450 current loss 0.003403, current_train_items 78432.
I0304 19:29:21.230327 23128000471168 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0304 19:29:21.230436 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:21.247532 23128000471168 run.py:483] Algo bellman_ford step 2451 current loss 0.034206, current_train_items 78464.
I0304 19:29:21.272318 23128000471168 run.py:483] Algo bellman_ford step 2452 current loss 0.065055, current_train_items 78496.
I0304 19:29:21.302872 23128000471168 run.py:483] Algo bellman_ford step 2453 current loss 0.060634, current_train_items 78528.
I0304 19:29:21.337656 23128000471168 run.py:483] Algo bellman_ford step 2454 current loss 0.107353, current_train_items 78560.
I0304 19:29:21.358154 23128000471168 run.py:483] Algo bellman_ford step 2455 current loss 0.033956, current_train_items 78592.
I0304 19:29:21.374397 23128000471168 run.py:483] Algo bellman_ford step 2456 current loss 0.030296, current_train_items 78624.
I0304 19:29:21.397477 23128000471168 run.py:483] Algo bellman_ford step 2457 current loss 0.039573, current_train_items 78656.
I0304 19:29:21.428499 23128000471168 run.py:483] Algo bellman_ford step 2458 current loss 0.091403, current_train_items 78688.
I0304 19:29:21.463989 23128000471168 run.py:483] Algo bellman_ford step 2459 current loss 0.224903, current_train_items 78720.
I0304 19:29:21.484697 23128000471168 run.py:483] Algo bellman_ford step 2460 current loss 0.004387, current_train_items 78752.
I0304 19:29:21.501748 23128000471168 run.py:483] Algo bellman_ford step 2461 current loss 0.040191, current_train_items 78784.
I0304 19:29:21.525557 23128000471168 run.py:483] Algo bellman_ford step 2462 current loss 0.096781, current_train_items 78816.
I0304 19:29:21.557422 23128000471168 run.py:483] Algo bellman_ford step 2463 current loss 0.116183, current_train_items 78848.
I0304 19:29:21.591374 23128000471168 run.py:483] Algo bellman_ford step 2464 current loss 0.116157, current_train_items 78880.
I0304 19:29:21.611114 23128000471168 run.py:483] Algo bellman_ford step 2465 current loss 0.018723, current_train_items 78912.
I0304 19:29:21.627821 23128000471168 run.py:483] Algo bellman_ford step 2466 current loss 0.039407, current_train_items 78944.
I0304 19:29:21.652050 23128000471168 run.py:483] Algo bellman_ford step 2467 current loss 0.055208, current_train_items 78976.
I0304 19:29:21.683931 23128000471168 run.py:483] Algo bellman_ford step 2468 current loss 0.082172, current_train_items 79008.
I0304 19:29:21.719695 23128000471168 run.py:483] Algo bellman_ford step 2469 current loss 0.133028, current_train_items 79040.
I0304 19:29:21.740037 23128000471168 run.py:483] Algo bellman_ford step 2470 current loss 0.008013, current_train_items 79072.
I0304 19:29:21.756562 23128000471168 run.py:483] Algo bellman_ford step 2471 current loss 0.072361, current_train_items 79104.
I0304 19:29:21.780805 23128000471168 run.py:483] Algo bellman_ford step 2472 current loss 0.149773, current_train_items 79136.
I0304 19:29:21.813090 23128000471168 run.py:483] Algo bellman_ford step 2473 current loss 0.096712, current_train_items 79168.
I0304 19:29:21.846212 23128000471168 run.py:483] Algo bellman_ford step 2474 current loss 0.057647, current_train_items 79200.
I0304 19:29:21.866829 23128000471168 run.py:483] Algo bellman_ford step 2475 current loss 0.047078, current_train_items 79232.
I0304 19:29:21.883478 23128000471168 run.py:483] Algo bellman_ford step 2476 current loss 0.053597, current_train_items 79264.
I0304 19:29:21.906079 23128000471168 run.py:483] Algo bellman_ford step 2477 current loss 0.032972, current_train_items 79296.
I0304 19:29:21.936588 23128000471168 run.py:483] Algo bellman_ford step 2478 current loss 0.114222, current_train_items 79328.
I0304 19:29:21.972054 23128000471168 run.py:483] Algo bellman_ford step 2479 current loss 0.177769, current_train_items 79360.
I0304 19:29:21.991896 23128000471168 run.py:483] Algo bellman_ford step 2480 current loss 0.004253, current_train_items 79392.
I0304 19:29:22.007997 23128000471168 run.py:483] Algo bellman_ford step 2481 current loss 0.032989, current_train_items 79424.
I0304 19:29:22.032641 23128000471168 run.py:483] Algo bellman_ford step 2482 current loss 0.078228, current_train_items 79456.
I0304 19:29:22.063801 23128000471168 run.py:483] Algo bellman_ford step 2483 current loss 0.153637, current_train_items 79488.
I0304 19:29:22.099276 23128000471168 run.py:483] Algo bellman_ford step 2484 current loss 0.104884, current_train_items 79520.
I0304 19:29:22.119805 23128000471168 run.py:483] Algo bellman_ford step 2485 current loss 0.004149, current_train_items 79552.
I0304 19:29:22.136482 23128000471168 run.py:483] Algo bellman_ford step 2486 current loss 0.024091, current_train_items 79584.
I0304 19:29:22.160991 23128000471168 run.py:483] Algo bellman_ford step 2487 current loss 0.098572, current_train_items 79616.
I0304 19:29:22.189436 23128000471168 run.py:483] Algo bellman_ford step 2488 current loss 0.032618, current_train_items 79648.
I0304 19:29:22.224086 23128000471168 run.py:483] Algo bellman_ford step 2489 current loss 0.128039, current_train_items 79680.
I0304 19:29:22.244609 23128000471168 run.py:483] Algo bellman_ford step 2490 current loss 0.024210, current_train_items 79712.
I0304 19:29:22.261098 23128000471168 run.py:483] Algo bellman_ford step 2491 current loss 0.026791, current_train_items 79744.
I0304 19:29:22.284880 23128000471168 run.py:483] Algo bellman_ford step 2492 current loss 0.056689, current_train_items 79776.
I0304 19:29:22.315630 23128000471168 run.py:483] Algo bellman_ford step 2493 current loss 0.076663, current_train_items 79808.
I0304 19:29:22.348896 23128000471168 run.py:483] Algo bellman_ford step 2494 current loss 0.086066, current_train_items 79840.
I0304 19:29:22.368719 23128000471168 run.py:483] Algo bellman_ford step 2495 current loss 0.005437, current_train_items 79872.
I0304 19:29:22.385538 23128000471168 run.py:483] Algo bellman_ford step 2496 current loss 0.019620, current_train_items 79904.
I0304 19:29:22.408962 23128000471168 run.py:483] Algo bellman_ford step 2497 current loss 0.052312, current_train_items 79936.
I0304 19:29:22.438760 23128000471168 run.py:483] Algo bellman_ford step 2498 current loss 0.101781, current_train_items 79968.
I0304 19:29:22.472960 23128000471168 run.py:483] Algo bellman_ford step 2499 current loss 0.084832, current_train_items 80000.
I0304 19:29:22.493574 23128000471168 run.py:483] Algo bellman_ford step 2500 current loss 0.003938, current_train_items 80032.
I0304 19:29:22.501295 23128000471168 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0304 19:29:22.501405 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:22.518707 23128000471168 run.py:483] Algo bellman_ford step 2501 current loss 0.023795, current_train_items 80064.
I0304 19:29:22.543393 23128000471168 run.py:483] Algo bellman_ford step 2502 current loss 0.101648, current_train_items 80096.
I0304 19:29:22.573541 23128000471168 run.py:483] Algo bellman_ford step 2503 current loss 0.050938, current_train_items 80128.
I0304 19:29:22.608747 23128000471168 run.py:483] Algo bellman_ford step 2504 current loss 0.076162, current_train_items 80160.
I0304 19:29:22.628972 23128000471168 run.py:483] Algo bellman_ford step 2505 current loss 0.004236, current_train_items 80192.
I0304 19:29:22.645206 23128000471168 run.py:483] Algo bellman_ford step 2506 current loss 0.018046, current_train_items 80224.
I0304 19:29:22.669380 23128000471168 run.py:483] Algo bellman_ford step 2507 current loss 0.060381, current_train_items 80256.
I0304 19:29:22.699221 23128000471168 run.py:483] Algo bellman_ford step 2508 current loss 0.149915, current_train_items 80288.
I0304 19:29:22.734321 23128000471168 run.py:483] Algo bellman_ford step 2509 current loss 0.114215, current_train_items 80320.
I0304 19:29:22.754146 23128000471168 run.py:483] Algo bellman_ford step 2510 current loss 0.013413, current_train_items 80352.
I0304 19:29:22.771054 23128000471168 run.py:483] Algo bellman_ford step 2511 current loss 0.083514, current_train_items 80384.
I0304 19:29:22.794671 23128000471168 run.py:483] Algo bellman_ford step 2512 current loss 0.085661, current_train_items 80416.
I0304 19:29:22.824926 23128000471168 run.py:483] Algo bellman_ford step 2513 current loss 0.142046, current_train_items 80448.
I0304 19:29:22.859632 23128000471168 run.py:483] Algo bellman_ford step 2514 current loss 0.094906, current_train_items 80480.
I0304 19:29:22.879651 23128000471168 run.py:483] Algo bellman_ford step 2515 current loss 0.005662, current_train_items 80512.
I0304 19:29:22.896233 23128000471168 run.py:483] Algo bellman_ford step 2516 current loss 0.036064, current_train_items 80544.
I0304 19:29:22.920099 23128000471168 run.py:483] Algo bellman_ford step 2517 current loss 0.191268, current_train_items 80576.
I0304 19:29:22.950270 23128000471168 run.py:483] Algo bellman_ford step 2518 current loss 0.116505, current_train_items 80608.
I0304 19:29:22.984322 23128000471168 run.py:483] Algo bellman_ford step 2519 current loss 0.112511, current_train_items 80640.
I0304 19:29:23.004509 23128000471168 run.py:483] Algo bellman_ford step 2520 current loss 0.003415, current_train_items 80672.
I0304 19:29:23.020973 23128000471168 run.py:483] Algo bellman_ford step 2521 current loss 0.035261, current_train_items 80704.
I0304 19:29:23.043393 23128000471168 run.py:483] Algo bellman_ford step 2522 current loss 0.039807, current_train_items 80736.
I0304 19:29:23.073514 23128000471168 run.py:483] Algo bellman_ford step 2523 current loss 0.060480, current_train_items 80768.
I0304 19:29:23.106650 23128000471168 run.py:483] Algo bellman_ford step 2524 current loss 0.105871, current_train_items 80800.
I0304 19:29:23.127023 23128000471168 run.py:483] Algo bellman_ford step 2525 current loss 0.005622, current_train_items 80832.
I0304 19:29:23.143413 23128000471168 run.py:483] Algo bellman_ford step 2526 current loss 0.018177, current_train_items 80864.
I0304 19:29:23.167331 23128000471168 run.py:483] Algo bellman_ford step 2527 current loss 0.112909, current_train_items 80896.
I0304 19:29:23.197160 23128000471168 run.py:483] Algo bellman_ford step 2528 current loss 0.075770, current_train_items 80928.
I0304 19:29:23.231930 23128000471168 run.py:483] Algo bellman_ford step 2529 current loss 0.096789, current_train_items 80960.
I0304 19:29:23.252120 23128000471168 run.py:483] Algo bellman_ford step 2530 current loss 0.003013, current_train_items 80992.
I0304 19:29:23.268107 23128000471168 run.py:483] Algo bellman_ford step 2531 current loss 0.029363, current_train_items 81024.
I0304 19:29:23.291985 23128000471168 run.py:483] Algo bellman_ford step 2532 current loss 0.101791, current_train_items 81056.
I0304 19:29:23.322683 23128000471168 run.py:483] Algo bellman_ford step 2533 current loss 0.091991, current_train_items 81088.
I0304 19:29:23.355973 23128000471168 run.py:483] Algo bellman_ford step 2534 current loss 0.096550, current_train_items 81120.
I0304 19:29:23.375892 23128000471168 run.py:483] Algo bellman_ford step 2535 current loss 0.002526, current_train_items 81152.
I0304 19:29:23.392038 23128000471168 run.py:483] Algo bellman_ford step 2536 current loss 0.021977, current_train_items 81184.
W0304 19:29:23.409002 23128000471168 samplers.py:155] Increasing hint lengh from 9 to 10
I0304 19:29:30.072517 23128000471168 run.py:483] Algo bellman_ford step 2537 current loss 0.145443, current_train_items 81216.
I0304 19:29:30.105752 23128000471168 run.py:483] Algo bellman_ford step 2538 current loss 0.084160, current_train_items 81248.
I0304 19:29:30.140686 23128000471168 run.py:483] Algo bellman_ford step 2539 current loss 0.110154, current_train_items 81280.
I0304 19:29:30.160926 23128000471168 run.py:483] Algo bellman_ford step 2540 current loss 0.025494, current_train_items 81312.
I0304 19:29:30.177133 23128000471168 run.py:483] Algo bellman_ford step 2541 current loss 0.011744, current_train_items 81344.
I0304 19:29:30.201426 23128000471168 run.py:483] Algo bellman_ford step 2542 current loss 0.118999, current_train_items 81376.
I0304 19:29:30.231326 23128000471168 run.py:483] Algo bellman_ford step 2543 current loss 0.133361, current_train_items 81408.
I0304 19:29:30.265095 23128000471168 run.py:483] Algo bellman_ford step 2544 current loss 0.157990, current_train_items 81440.
I0304 19:29:30.285127 23128000471168 run.py:483] Algo bellman_ford step 2545 current loss 0.007002, current_train_items 81472.
I0304 19:29:30.301576 23128000471168 run.py:483] Algo bellman_ford step 2546 current loss 0.041400, current_train_items 81504.
I0304 19:29:30.325748 23128000471168 run.py:483] Algo bellman_ford step 2547 current loss 0.037066, current_train_items 81536.
I0304 19:29:30.357637 23128000471168 run.py:483] Algo bellman_ford step 2548 current loss 0.087292, current_train_items 81568.
I0304 19:29:30.390785 23128000471168 run.py:483] Algo bellman_ford step 2549 current loss 0.105979, current_train_items 81600.
I0304 19:29:30.410932 23128000471168 run.py:483] Algo bellman_ford step 2550 current loss 0.003290, current_train_items 81632.
I0304 19:29:30.420834 23128000471168 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0304 19:29:30.420943 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.987, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:30.438272 23128000471168 run.py:483] Algo bellman_ford step 2551 current loss 0.012452, current_train_items 81664.
I0304 19:29:30.463367 23128000471168 run.py:483] Algo bellman_ford step 2552 current loss 0.050657, current_train_items 81696.
I0304 19:29:30.496001 23128000471168 run.py:483] Algo bellman_ford step 2553 current loss 0.100211, current_train_items 81728.
I0304 19:29:30.530333 23128000471168 run.py:483] Algo bellman_ford step 2554 current loss 0.118571, current_train_items 81760.
I0304 19:29:30.550854 23128000471168 run.py:483] Algo bellman_ford step 2555 current loss 0.002378, current_train_items 81792.
I0304 19:29:30.567501 23128000471168 run.py:483] Algo bellman_ford step 2556 current loss 0.044011, current_train_items 81824.
I0304 19:29:30.591809 23128000471168 run.py:483] Algo bellman_ford step 2557 current loss 0.056915, current_train_items 81856.
I0304 19:29:30.623779 23128000471168 run.py:483] Algo bellman_ford step 2558 current loss 0.077914, current_train_items 81888.
I0304 19:29:30.659272 23128000471168 run.py:483] Algo bellman_ford step 2559 current loss 0.096758, current_train_items 81920.
I0304 19:29:30.679597 23128000471168 run.py:483] Algo bellman_ford step 2560 current loss 0.015446, current_train_items 81952.
I0304 19:29:30.696537 23128000471168 run.py:483] Algo bellman_ford step 2561 current loss 0.015608, current_train_items 81984.
I0304 19:29:30.720711 23128000471168 run.py:483] Algo bellman_ford step 2562 current loss 0.076477, current_train_items 82016.
I0304 19:29:30.753240 23128000471168 run.py:483] Algo bellman_ford step 2563 current loss 0.100677, current_train_items 82048.
I0304 19:29:30.787751 23128000471168 run.py:483] Algo bellman_ford step 2564 current loss 0.065199, current_train_items 82080.
I0304 19:29:30.807479 23128000471168 run.py:483] Algo bellman_ford step 2565 current loss 0.002987, current_train_items 82112.
I0304 19:29:30.824379 23128000471168 run.py:483] Algo bellman_ford step 2566 current loss 0.032890, current_train_items 82144.
I0304 19:29:30.848387 23128000471168 run.py:483] Algo bellman_ford step 2567 current loss 0.051492, current_train_items 82176.
I0304 19:29:30.880504 23128000471168 run.py:483] Algo bellman_ford step 2568 current loss 0.091520, current_train_items 82208.
I0304 19:29:30.914200 23128000471168 run.py:483] Algo bellman_ford step 2569 current loss 0.086102, current_train_items 82240.
I0304 19:29:30.934427 23128000471168 run.py:483] Algo bellman_ford step 2570 current loss 0.003225, current_train_items 82272.
I0304 19:29:30.951088 23128000471168 run.py:483] Algo bellman_ford step 2571 current loss 0.020760, current_train_items 82304.
I0304 19:29:30.973766 23128000471168 run.py:483] Algo bellman_ford step 2572 current loss 0.051285, current_train_items 82336.
I0304 19:29:31.004957 23128000471168 run.py:483] Algo bellman_ford step 2573 current loss 0.069107, current_train_items 82368.
I0304 19:29:31.040378 23128000471168 run.py:483] Algo bellman_ford step 2574 current loss 0.092101, current_train_items 82400.
I0304 19:29:31.060603 23128000471168 run.py:483] Algo bellman_ford step 2575 current loss 0.014070, current_train_items 82432.
I0304 19:29:31.077177 23128000471168 run.py:483] Algo bellman_ford step 2576 current loss 0.058708, current_train_items 82464.
I0304 19:29:31.100587 23128000471168 run.py:483] Algo bellman_ford step 2577 current loss 0.090397, current_train_items 82496.
I0304 19:29:31.133196 23128000471168 run.py:483] Algo bellman_ford step 2578 current loss 0.118710, current_train_items 82528.
I0304 19:29:31.166487 23128000471168 run.py:483] Algo bellman_ford step 2579 current loss 0.128772, current_train_items 82560.
I0304 19:29:31.186512 23128000471168 run.py:483] Algo bellman_ford step 2580 current loss 0.005195, current_train_items 82592.
I0304 19:29:31.203027 23128000471168 run.py:483] Algo bellman_ford step 2581 current loss 0.017109, current_train_items 82624.
I0304 19:29:31.226724 23128000471168 run.py:483] Algo bellman_ford step 2582 current loss 0.062144, current_train_items 82656.
I0304 19:29:31.257514 23128000471168 run.py:483] Algo bellman_ford step 2583 current loss 0.079440, current_train_items 82688.
I0304 19:29:31.292509 23128000471168 run.py:483] Algo bellman_ford step 2584 current loss 0.116345, current_train_items 82720.
I0304 19:29:31.312664 23128000471168 run.py:483] Algo bellman_ford step 2585 current loss 0.005122, current_train_items 82752.
I0304 19:29:31.329336 23128000471168 run.py:483] Algo bellman_ford step 2586 current loss 0.035102, current_train_items 82784.
I0304 19:29:31.352931 23128000471168 run.py:483] Algo bellman_ford step 2587 current loss 0.045293, current_train_items 82816.
I0304 19:29:31.383761 23128000471168 run.py:483] Algo bellman_ford step 2588 current loss 0.098477, current_train_items 82848.
I0304 19:29:31.418155 23128000471168 run.py:483] Algo bellman_ford step 2589 current loss 0.162891, current_train_items 82880.
I0304 19:29:31.438231 23128000471168 run.py:483] Algo bellman_ford step 2590 current loss 0.003240, current_train_items 82912.
I0304 19:29:31.455033 23128000471168 run.py:483] Algo bellman_ford step 2591 current loss 0.020480, current_train_items 82944.
I0304 19:29:31.479438 23128000471168 run.py:483] Algo bellman_ford step 2592 current loss 0.078072, current_train_items 82976.
I0304 19:29:31.511733 23128000471168 run.py:483] Algo bellman_ford step 2593 current loss 0.105318, current_train_items 83008.
I0304 19:29:31.544152 23128000471168 run.py:483] Algo bellman_ford step 2594 current loss 0.116108, current_train_items 83040.
I0304 19:29:31.563751 23128000471168 run.py:483] Algo bellman_ford step 2595 current loss 0.008008, current_train_items 83072.
I0304 19:29:31.580493 23128000471168 run.py:483] Algo bellman_ford step 2596 current loss 0.049216, current_train_items 83104.
I0304 19:29:31.605920 23128000471168 run.py:483] Algo bellman_ford step 2597 current loss 0.066021, current_train_items 83136.
I0304 19:29:31.637446 23128000471168 run.py:483] Algo bellman_ford step 2598 current loss 0.071841, current_train_items 83168.
I0304 19:29:31.671089 23128000471168 run.py:483] Algo bellman_ford step 2599 current loss 0.111959, current_train_items 83200.
I0304 19:29:31.691444 23128000471168 run.py:483] Algo bellman_ford step 2600 current loss 0.007384, current_train_items 83232.
I0304 19:29:31.699639 23128000471168 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0304 19:29:31.699750 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.987, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:29:31.730056 23128000471168 run.py:483] Algo bellman_ford step 2601 current loss 0.021911, current_train_items 83264.
I0304 19:29:31.754302 23128000471168 run.py:483] Algo bellman_ford step 2602 current loss 0.038090, current_train_items 83296.
I0304 19:29:31.784735 23128000471168 run.py:483] Algo bellman_ford step 2603 current loss 0.064634, current_train_items 83328.
I0304 19:29:31.819432 23128000471168 run.py:483] Algo bellman_ford step 2604 current loss 0.088596, current_train_items 83360.
I0304 19:29:31.840011 23128000471168 run.py:483] Algo bellman_ford step 2605 current loss 0.029904, current_train_items 83392.
I0304 19:29:31.856288 23128000471168 run.py:483] Algo bellman_ford step 2606 current loss 0.009482, current_train_items 83424.
I0304 19:29:31.880775 23128000471168 run.py:483] Algo bellman_ford step 2607 current loss 0.026041, current_train_items 83456.
I0304 19:29:31.912874 23128000471168 run.py:483] Algo bellman_ford step 2608 current loss 0.076589, current_train_items 83488.
I0304 19:29:31.947738 23128000471168 run.py:483] Algo bellman_ford step 2609 current loss 0.113049, current_train_items 83520.
I0304 19:29:31.968027 23128000471168 run.py:483] Algo bellman_ford step 2610 current loss 0.008216, current_train_items 83552.
I0304 19:29:31.984429 23128000471168 run.py:483] Algo bellman_ford step 2611 current loss 0.015713, current_train_items 83584.
I0304 19:29:32.008266 23128000471168 run.py:483] Algo bellman_ford step 2612 current loss 0.047165, current_train_items 83616.
I0304 19:29:32.039484 23128000471168 run.py:483] Algo bellman_ford step 2613 current loss 0.063422, current_train_items 83648.
I0304 19:29:32.072440 23128000471168 run.py:483] Algo bellman_ford step 2614 current loss 0.090721, current_train_items 83680.
I0304 19:29:32.092466 23128000471168 run.py:483] Algo bellman_ford step 2615 current loss 0.003490, current_train_items 83712.
I0304 19:29:32.108665 23128000471168 run.py:483] Algo bellman_ford step 2616 current loss 0.024468, current_train_items 83744.
I0304 19:29:32.133659 23128000471168 run.py:483] Algo bellman_ford step 2617 current loss 0.067435, current_train_items 83776.
I0304 19:29:32.166095 23128000471168 run.py:483] Algo bellman_ford step 2618 current loss 0.098408, current_train_items 83808.
I0304 19:29:32.199134 23128000471168 run.py:483] Algo bellman_ford step 2619 current loss 0.073144, current_train_items 83840.
I0304 19:29:32.219382 23128000471168 run.py:483] Algo bellman_ford step 2620 current loss 0.005085, current_train_items 83872.
I0304 19:29:32.235910 23128000471168 run.py:483] Algo bellman_ford step 2621 current loss 0.016703, current_train_items 83904.
I0304 19:29:32.260364 23128000471168 run.py:483] Algo bellman_ford step 2622 current loss 0.061538, current_train_items 83936.
I0304 19:29:32.291667 23128000471168 run.py:483] Algo bellman_ford step 2623 current loss 0.062321, current_train_items 83968.
I0304 19:29:32.324867 23128000471168 run.py:483] Algo bellman_ford step 2624 current loss 0.067209, current_train_items 84000.
I0304 19:29:32.344933 23128000471168 run.py:483] Algo bellman_ford step 2625 current loss 0.014376, current_train_items 84032.
I0304 19:29:32.361307 23128000471168 run.py:483] Algo bellman_ford step 2626 current loss 0.024496, current_train_items 84064.
I0304 19:29:32.385806 23128000471168 run.py:483] Algo bellman_ford step 2627 current loss 0.048425, current_train_items 84096.
I0304 19:29:32.417394 23128000471168 run.py:483] Algo bellman_ford step 2628 current loss 0.123813, current_train_items 84128.
I0304 19:29:32.451629 23128000471168 run.py:483] Algo bellman_ford step 2629 current loss 0.111667, current_train_items 84160.
I0304 19:29:32.471799 23128000471168 run.py:483] Algo bellman_ford step 2630 current loss 0.006992, current_train_items 84192.
I0304 19:29:32.488221 23128000471168 run.py:483] Algo bellman_ford step 2631 current loss 0.043709, current_train_items 84224.
I0304 19:29:32.512144 23128000471168 run.py:483] Algo bellman_ford step 2632 current loss 0.055253, current_train_items 84256.
I0304 19:29:32.542622 23128000471168 run.py:483] Algo bellman_ford step 2633 current loss 0.042217, current_train_items 84288.
I0304 19:29:32.578742 23128000471168 run.py:483] Algo bellman_ford step 2634 current loss 0.171165, current_train_items 84320.
I0304 19:29:32.599129 23128000471168 run.py:483] Algo bellman_ford step 2635 current loss 0.012569, current_train_items 84352.
I0304 19:29:32.615392 23128000471168 run.py:483] Algo bellman_ford step 2636 current loss 0.081852, current_train_items 84384.
I0304 19:29:32.640242 23128000471168 run.py:483] Algo bellman_ford step 2637 current loss 0.090038, current_train_items 84416.
I0304 19:29:32.672260 23128000471168 run.py:483] Algo bellman_ford step 2638 current loss 0.062071, current_train_items 84448.
I0304 19:29:32.706732 23128000471168 run.py:483] Algo bellman_ford step 2639 current loss 0.111557, current_train_items 84480.
I0304 19:29:32.726745 23128000471168 run.py:483] Algo bellman_ford step 2640 current loss 0.005498, current_train_items 84512.
I0304 19:29:32.743093 23128000471168 run.py:483] Algo bellman_ford step 2641 current loss 0.022781, current_train_items 84544.
I0304 19:29:32.766384 23128000471168 run.py:483] Algo bellman_ford step 2642 current loss 0.103103, current_train_items 84576.
I0304 19:29:32.797517 23128000471168 run.py:483] Algo bellman_ford step 2643 current loss 0.085905, current_train_items 84608.
I0304 19:29:32.832616 23128000471168 run.py:483] Algo bellman_ford step 2644 current loss 0.089922, current_train_items 84640.
I0304 19:29:32.852739 23128000471168 run.py:483] Algo bellman_ford step 2645 current loss 0.010795, current_train_items 84672.
I0304 19:29:32.869204 23128000471168 run.py:483] Algo bellman_ford step 2646 current loss 0.031145, current_train_items 84704.
I0304 19:29:32.893352 23128000471168 run.py:483] Algo bellman_ford step 2647 current loss 0.076278, current_train_items 84736.
I0304 19:29:32.925264 23128000471168 run.py:483] Algo bellman_ford step 2648 current loss 0.093527, current_train_items 84768.
I0304 19:29:32.961242 23128000471168 run.py:483] Algo bellman_ford step 2649 current loss 0.091075, current_train_items 84800.
I0304 19:29:32.981782 23128000471168 run.py:483] Algo bellman_ford step 2650 current loss 0.006575, current_train_items 84832.
I0304 19:29:32.990002 23128000471168 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0304 19:29:32.990119 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:29:33.007121 23128000471168 run.py:483] Algo bellman_ford step 2651 current loss 0.018796, current_train_items 84864.
I0304 19:29:33.031604 23128000471168 run.py:483] Algo bellman_ford step 2652 current loss 0.077938, current_train_items 84896.
I0304 19:29:33.064218 23128000471168 run.py:483] Algo bellman_ford step 2653 current loss 0.093826, current_train_items 84928.
I0304 19:29:33.098311 23128000471168 run.py:483] Algo bellman_ford step 2654 current loss 0.059365, current_train_items 84960.
I0304 19:29:33.118494 23128000471168 run.py:483] Algo bellman_ford step 2655 current loss 0.004103, current_train_items 84992.
I0304 19:29:33.134794 23128000471168 run.py:483] Algo bellman_ford step 2656 current loss 0.028774, current_train_items 85024.
I0304 19:29:33.159072 23128000471168 run.py:483] Algo bellman_ford step 2657 current loss 0.062512, current_train_items 85056.
I0304 19:29:33.189638 23128000471168 run.py:483] Algo bellman_ford step 2658 current loss 0.048201, current_train_items 85088.
I0304 19:29:33.225688 23128000471168 run.py:483] Algo bellman_ford step 2659 current loss 0.123470, current_train_items 85120.
I0304 19:29:33.246086 23128000471168 run.py:483] Algo bellman_ford step 2660 current loss 0.003769, current_train_items 85152.
I0304 19:29:33.263459 23128000471168 run.py:483] Algo bellman_ford step 2661 current loss 0.024374, current_train_items 85184.
I0304 19:29:33.287278 23128000471168 run.py:483] Algo bellman_ford step 2662 current loss 0.061898, current_train_items 85216.
I0304 19:29:33.318895 23128000471168 run.py:483] Algo bellman_ford step 2663 current loss 0.110295, current_train_items 85248.
I0304 19:29:33.355365 23128000471168 run.py:483] Algo bellman_ford step 2664 current loss 0.134094, current_train_items 85280.
I0304 19:29:33.375446 23128000471168 run.py:483] Algo bellman_ford step 2665 current loss 0.009230, current_train_items 85312.
I0304 19:29:33.392168 23128000471168 run.py:483] Algo bellman_ford step 2666 current loss 0.053448, current_train_items 85344.
I0304 19:29:33.417358 23128000471168 run.py:483] Algo bellman_ford step 2667 current loss 0.046135, current_train_items 85376.
I0304 19:29:33.449503 23128000471168 run.py:483] Algo bellman_ford step 2668 current loss 0.172013, current_train_items 85408.
I0304 19:29:33.482820 23128000471168 run.py:483] Algo bellman_ford step 2669 current loss 0.067170, current_train_items 85440.
I0304 19:29:33.503201 23128000471168 run.py:483] Algo bellman_ford step 2670 current loss 0.003376, current_train_items 85472.
I0304 19:29:33.519894 23128000471168 run.py:483] Algo bellman_ford step 2671 current loss 0.013114, current_train_items 85504.
I0304 19:29:33.543968 23128000471168 run.py:483] Algo bellman_ford step 2672 current loss 0.057998, current_train_items 85536.
I0304 19:29:33.576538 23128000471168 run.py:483] Algo bellman_ford step 2673 current loss 0.099569, current_train_items 85568.
I0304 19:29:33.611974 23128000471168 run.py:483] Algo bellman_ford step 2674 current loss 0.083139, current_train_items 85600.
I0304 19:29:33.632382 23128000471168 run.py:483] Algo bellman_ford step 2675 current loss 0.006215, current_train_items 85632.
I0304 19:29:33.649020 23128000471168 run.py:483] Algo bellman_ford step 2676 current loss 0.023364, current_train_items 85664.
I0304 19:29:33.674185 23128000471168 run.py:483] Algo bellman_ford step 2677 current loss 0.086223, current_train_items 85696.
I0304 19:29:33.704283 23128000471168 run.py:483] Algo bellman_ford step 2678 current loss 0.045766, current_train_items 85728.
I0304 19:29:33.737313 23128000471168 run.py:483] Algo bellman_ford step 2679 current loss 0.057788, current_train_items 85760.
I0304 19:29:33.757373 23128000471168 run.py:483] Algo bellman_ford step 2680 current loss 0.002508, current_train_items 85792.
I0304 19:29:33.773868 23128000471168 run.py:483] Algo bellman_ford step 2681 current loss 0.023634, current_train_items 85824.
I0304 19:29:33.798270 23128000471168 run.py:483] Algo bellman_ford step 2682 current loss 0.049381, current_train_items 85856.
I0304 19:29:33.831000 23128000471168 run.py:483] Algo bellman_ford step 2683 current loss 0.070918, current_train_items 85888.
I0304 19:29:33.865305 23128000471168 run.py:483] Algo bellman_ford step 2684 current loss 0.109686, current_train_items 85920.
I0304 19:29:33.885886 23128000471168 run.py:483] Algo bellman_ford step 2685 current loss 0.003567, current_train_items 85952.
I0304 19:29:33.902534 23128000471168 run.py:483] Algo bellman_ford step 2686 current loss 0.043468, current_train_items 85984.
I0304 19:29:33.926676 23128000471168 run.py:483] Algo bellman_ford step 2687 current loss 0.051483, current_train_items 86016.
I0304 19:29:33.957939 23128000471168 run.py:483] Algo bellman_ford step 2688 current loss 0.083813, current_train_items 86048.
I0304 19:29:33.990870 23128000471168 run.py:483] Algo bellman_ford step 2689 current loss 0.074543, current_train_items 86080.
I0304 19:29:34.011486 23128000471168 run.py:483] Algo bellman_ford step 2690 current loss 0.023768, current_train_items 86112.
I0304 19:29:34.028120 23128000471168 run.py:483] Algo bellman_ford step 2691 current loss 0.017619, current_train_items 86144.
I0304 19:29:34.053135 23128000471168 run.py:483] Algo bellman_ford step 2692 current loss 0.053805, current_train_items 86176.
I0304 19:29:34.085458 23128000471168 run.py:483] Algo bellman_ford step 2693 current loss 0.072572, current_train_items 86208.
I0304 19:29:34.119560 23128000471168 run.py:483] Algo bellman_ford step 2694 current loss 0.082410, current_train_items 86240.
I0304 19:29:34.140247 23128000471168 run.py:483] Algo bellman_ford step 2695 current loss 0.002531, current_train_items 86272.
I0304 19:29:34.156543 23128000471168 run.py:483] Algo bellman_ford step 2696 current loss 0.008381, current_train_items 86304.
I0304 19:29:34.181228 23128000471168 run.py:483] Algo bellman_ford step 2697 current loss 0.102696, current_train_items 86336.
I0304 19:29:34.212096 23128000471168 run.py:483] Algo bellman_ford step 2698 current loss 0.081879, current_train_items 86368.
I0304 19:29:34.244610 23128000471168 run.py:483] Algo bellman_ford step 2699 current loss 0.096021, current_train_items 86400.
I0304 19:29:34.264965 23128000471168 run.py:483] Algo bellman_ford step 2700 current loss 0.012618, current_train_items 86432.
I0304 19:29:34.272995 23128000471168 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0304 19:29:34.273110 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:34.290304 23128000471168 run.py:483] Algo bellman_ford step 2701 current loss 0.028381, current_train_items 86464.
I0304 19:29:34.315343 23128000471168 run.py:483] Algo bellman_ford step 2702 current loss 0.041151, current_train_items 86496.
I0304 19:29:34.349622 23128000471168 run.py:483] Algo bellman_ford step 2703 current loss 0.046682, current_train_items 86528.
I0304 19:29:34.386169 23128000471168 run.py:483] Algo bellman_ford step 2704 current loss 0.131136, current_train_items 86560.
I0304 19:29:34.406512 23128000471168 run.py:483] Algo bellman_ford step 2705 current loss 0.003164, current_train_items 86592.
I0304 19:29:34.422609 23128000471168 run.py:483] Algo bellman_ford step 2706 current loss 0.088855, current_train_items 86624.
I0304 19:29:34.447226 23128000471168 run.py:483] Algo bellman_ford step 2707 current loss 0.068252, current_train_items 86656.
I0304 19:29:34.477468 23128000471168 run.py:483] Algo bellman_ford step 2708 current loss 0.069427, current_train_items 86688.
I0304 19:29:34.510123 23128000471168 run.py:483] Algo bellman_ford step 2709 current loss 0.081898, current_train_items 86720.
I0304 19:29:34.530449 23128000471168 run.py:483] Algo bellman_ford step 2710 current loss 0.004938, current_train_items 86752.
I0304 19:29:34.547413 23128000471168 run.py:483] Algo bellman_ford step 2711 current loss 0.014608, current_train_items 86784.
I0304 19:29:34.572275 23128000471168 run.py:483] Algo bellman_ford step 2712 current loss 0.048167, current_train_items 86816.
I0304 19:29:34.604716 23128000471168 run.py:483] Algo bellman_ford step 2713 current loss 0.062519, current_train_items 86848.
I0304 19:29:34.640986 23128000471168 run.py:483] Algo bellman_ford step 2714 current loss 0.119567, current_train_items 86880.
I0304 19:29:34.661194 23128000471168 run.py:483] Algo bellman_ford step 2715 current loss 0.021145, current_train_items 86912.
I0304 19:29:34.677321 23128000471168 run.py:483] Algo bellman_ford step 2716 current loss 0.006616, current_train_items 86944.
I0304 19:29:34.702938 23128000471168 run.py:483] Algo bellman_ford step 2717 current loss 0.095558, current_train_items 86976.
I0304 19:29:34.733111 23128000471168 run.py:483] Algo bellman_ford step 2718 current loss 0.067013, current_train_items 87008.
I0304 19:29:34.767348 23128000471168 run.py:483] Algo bellman_ford step 2719 current loss 0.081194, current_train_items 87040.
I0304 19:29:34.787386 23128000471168 run.py:483] Algo bellman_ford step 2720 current loss 0.004019, current_train_items 87072.
I0304 19:29:34.803819 23128000471168 run.py:483] Algo bellman_ford step 2721 current loss 0.081498, current_train_items 87104.
I0304 19:29:34.827917 23128000471168 run.py:483] Algo bellman_ford step 2722 current loss 0.055189, current_train_items 87136.
I0304 19:29:34.860669 23128000471168 run.py:483] Algo bellman_ford step 2723 current loss 0.067089, current_train_items 87168.
I0304 19:29:34.894744 23128000471168 run.py:483] Algo bellman_ford step 2724 current loss 0.131450, current_train_items 87200.
I0304 19:29:34.914641 23128000471168 run.py:483] Algo bellman_ford step 2725 current loss 0.006363, current_train_items 87232.
I0304 19:29:34.930956 23128000471168 run.py:483] Algo bellman_ford step 2726 current loss 0.039857, current_train_items 87264.
I0304 19:29:34.955830 23128000471168 run.py:483] Algo bellman_ford step 2727 current loss 0.071727, current_train_items 87296.
I0304 19:29:34.987225 23128000471168 run.py:483] Algo bellman_ford step 2728 current loss 0.085148, current_train_items 87328.
I0304 19:29:35.020761 23128000471168 run.py:483] Algo bellman_ford step 2729 current loss 0.074308, current_train_items 87360.
I0304 19:29:35.040978 23128000471168 run.py:483] Algo bellman_ford step 2730 current loss 0.004024, current_train_items 87392.
I0304 19:29:35.057430 23128000471168 run.py:483] Algo bellman_ford step 2731 current loss 0.019616, current_train_items 87424.
I0304 19:29:35.080604 23128000471168 run.py:483] Algo bellman_ford step 2732 current loss 0.031757, current_train_items 87456.
I0304 19:29:35.111288 23128000471168 run.py:483] Algo bellman_ford step 2733 current loss 0.062755, current_train_items 87488.
I0304 19:29:35.145030 23128000471168 run.py:483] Algo bellman_ford step 2734 current loss 0.061895, current_train_items 87520.
I0304 19:29:35.165309 23128000471168 run.py:483] Algo bellman_ford step 2735 current loss 0.025483, current_train_items 87552.
I0304 19:29:35.181350 23128000471168 run.py:483] Algo bellman_ford step 2736 current loss 0.014899, current_train_items 87584.
I0304 19:29:35.205399 23128000471168 run.py:483] Algo bellman_ford step 2737 current loss 0.063024, current_train_items 87616.
I0304 19:29:35.235546 23128000471168 run.py:483] Algo bellman_ford step 2738 current loss 0.044785, current_train_items 87648.
I0304 19:29:35.268481 23128000471168 run.py:483] Algo bellman_ford step 2739 current loss 0.065731, current_train_items 87680.
I0304 19:29:35.288530 23128000471168 run.py:483] Algo bellman_ford step 2740 current loss 0.003503, current_train_items 87712.
I0304 19:29:35.304811 23128000471168 run.py:483] Algo bellman_ford step 2741 current loss 0.018380, current_train_items 87744.
I0304 19:29:35.329343 23128000471168 run.py:483] Algo bellman_ford step 2742 current loss 0.065263, current_train_items 87776.
I0304 19:29:35.362162 23128000471168 run.py:483] Algo bellman_ford step 2743 current loss 0.108673, current_train_items 87808.
I0304 19:29:35.396038 23128000471168 run.py:483] Algo bellman_ford step 2744 current loss 0.093949, current_train_items 87840.
I0304 19:29:35.416063 23128000471168 run.py:483] Algo bellman_ford step 2745 current loss 0.003228, current_train_items 87872.
I0304 19:29:35.432193 23128000471168 run.py:483] Algo bellman_ford step 2746 current loss 0.028299, current_train_items 87904.
I0304 19:29:35.456475 23128000471168 run.py:483] Algo bellman_ford step 2747 current loss 0.056005, current_train_items 87936.
I0304 19:29:35.488615 23128000471168 run.py:483] Algo bellman_ford step 2748 current loss 0.054658, current_train_items 87968.
I0304 19:29:35.522355 23128000471168 run.py:483] Algo bellman_ford step 2749 current loss 0.085349, current_train_items 88000.
I0304 19:29:35.542602 23128000471168 run.py:483] Algo bellman_ford step 2750 current loss 0.005645, current_train_items 88032.
I0304 19:29:35.550875 23128000471168 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0304 19:29:35.550982 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:35.568368 23128000471168 run.py:483] Algo bellman_ford step 2751 current loss 0.028737, current_train_items 88064.
I0304 19:29:35.593071 23128000471168 run.py:483] Algo bellman_ford step 2752 current loss 0.082245, current_train_items 88096.
I0304 19:29:35.625475 23128000471168 run.py:483] Algo bellman_ford step 2753 current loss 0.122153, current_train_items 88128.
I0304 19:29:35.658664 23128000471168 run.py:483] Algo bellman_ford step 2754 current loss 0.081889, current_train_items 88160.
I0304 19:29:35.678951 23128000471168 run.py:483] Algo bellman_ford step 2755 current loss 0.005189, current_train_items 88192.
I0304 19:29:35.695517 23128000471168 run.py:483] Algo bellman_ford step 2756 current loss 0.048337, current_train_items 88224.
I0304 19:29:35.718919 23128000471168 run.py:483] Algo bellman_ford step 2757 current loss 0.045993, current_train_items 88256.
I0304 19:29:35.751195 23128000471168 run.py:483] Algo bellman_ford step 2758 current loss 0.120830, current_train_items 88288.
I0304 19:29:35.787319 23128000471168 run.py:483] Algo bellman_ford step 2759 current loss 0.158789, current_train_items 88320.
I0304 19:29:35.807638 23128000471168 run.py:483] Algo bellman_ford step 2760 current loss 0.003215, current_train_items 88352.
I0304 19:29:35.824695 23128000471168 run.py:483] Algo bellman_ford step 2761 current loss 0.035529, current_train_items 88384.
I0304 19:29:35.848867 23128000471168 run.py:483] Algo bellman_ford step 2762 current loss 0.183223, current_train_items 88416.
I0304 19:29:35.880443 23128000471168 run.py:483] Algo bellman_ford step 2763 current loss 0.166296, current_train_items 88448.
I0304 19:29:35.914472 23128000471168 run.py:483] Algo bellman_ford step 2764 current loss 0.141919, current_train_items 88480.
I0304 19:29:35.934606 23128000471168 run.py:483] Algo bellman_ford step 2765 current loss 0.020067, current_train_items 88512.
I0304 19:29:35.951529 23128000471168 run.py:483] Algo bellman_ford step 2766 current loss 0.040524, current_train_items 88544.
I0304 19:29:35.975640 23128000471168 run.py:483] Algo bellman_ford step 2767 current loss 0.067830, current_train_items 88576.
I0304 19:29:36.007974 23128000471168 run.py:483] Algo bellman_ford step 2768 current loss 0.140412, current_train_items 88608.
I0304 19:29:36.041859 23128000471168 run.py:483] Algo bellman_ford step 2769 current loss 0.124035, current_train_items 88640.
I0304 19:29:36.062089 23128000471168 run.py:483] Algo bellman_ford step 2770 current loss 0.002700, current_train_items 88672.
I0304 19:29:36.079298 23128000471168 run.py:483] Algo bellman_ford step 2771 current loss 0.024339, current_train_items 88704.
I0304 19:29:36.103502 23128000471168 run.py:483] Algo bellman_ford step 2772 current loss 0.060197, current_train_items 88736.
I0304 19:29:36.134476 23128000471168 run.py:483] Algo bellman_ford step 2773 current loss 0.080904, current_train_items 88768.
I0304 19:29:36.168350 23128000471168 run.py:483] Algo bellman_ford step 2774 current loss 0.090557, current_train_items 88800.
I0304 19:29:36.188361 23128000471168 run.py:483] Algo bellman_ford step 2775 current loss 0.002584, current_train_items 88832.
I0304 19:29:36.205505 23128000471168 run.py:483] Algo bellman_ford step 2776 current loss 0.067910, current_train_items 88864.
I0304 19:29:36.229187 23128000471168 run.py:483] Algo bellman_ford step 2777 current loss 0.055166, current_train_items 88896.
I0304 19:29:36.260416 23128000471168 run.py:483] Algo bellman_ford step 2778 current loss 0.116623, current_train_items 88928.
I0304 19:29:36.294868 23128000471168 run.py:483] Algo bellman_ford step 2779 current loss 0.135070, current_train_items 88960.
I0304 19:29:36.314920 23128000471168 run.py:483] Algo bellman_ford step 2780 current loss 0.006351, current_train_items 88992.
I0304 19:29:36.331438 23128000471168 run.py:483] Algo bellman_ford step 2781 current loss 0.028997, current_train_items 89024.
I0304 19:29:36.355971 23128000471168 run.py:483] Algo bellman_ford step 2782 current loss 0.090326, current_train_items 89056.
I0304 19:29:36.387378 23128000471168 run.py:483] Algo bellman_ford step 2783 current loss 0.063972, current_train_items 89088.
I0304 19:29:36.421745 23128000471168 run.py:483] Algo bellman_ford step 2784 current loss 0.059569, current_train_items 89120.
I0304 19:29:36.442135 23128000471168 run.py:483] Algo bellman_ford step 2785 current loss 0.002726, current_train_items 89152.
I0304 19:29:36.458591 23128000471168 run.py:483] Algo bellman_ford step 2786 current loss 0.032840, current_train_items 89184.
I0304 19:29:36.481327 23128000471168 run.py:483] Algo bellman_ford step 2787 current loss 0.031724, current_train_items 89216.
I0304 19:29:36.512474 23128000471168 run.py:483] Algo bellman_ford step 2788 current loss 0.099190, current_train_items 89248.
I0304 19:29:36.543702 23128000471168 run.py:483] Algo bellman_ford step 2789 current loss 0.102922, current_train_items 89280.
I0304 19:29:36.563655 23128000471168 run.py:483] Algo bellman_ford step 2790 current loss 0.002800, current_train_items 89312.
I0304 19:29:36.580157 23128000471168 run.py:483] Algo bellman_ford step 2791 current loss 0.010931, current_train_items 89344.
I0304 19:29:36.603687 23128000471168 run.py:483] Algo bellman_ford step 2792 current loss 0.029568, current_train_items 89376.
I0304 19:29:36.634508 23128000471168 run.py:483] Algo bellman_ford step 2793 current loss 0.083659, current_train_items 89408.
I0304 19:29:36.670481 23128000471168 run.py:483] Algo bellman_ford step 2794 current loss 0.088147, current_train_items 89440.
I0304 19:29:36.690498 23128000471168 run.py:483] Algo bellman_ford step 2795 current loss 0.003797, current_train_items 89472.
I0304 19:29:36.707355 23128000471168 run.py:483] Algo bellman_ford step 2796 current loss 0.041721, current_train_items 89504.
I0304 19:29:36.731052 23128000471168 run.py:483] Algo bellman_ford step 2797 current loss 0.085156, current_train_items 89536.
I0304 19:29:36.763427 23128000471168 run.py:483] Algo bellman_ford step 2798 current loss 0.078181, current_train_items 89568.
I0304 19:29:36.796915 23128000471168 run.py:483] Algo bellman_ford step 2799 current loss 0.084078, current_train_items 89600.
I0304 19:29:36.817401 23128000471168 run.py:483] Algo bellman_ford step 2800 current loss 0.003366, current_train_items 89632.
I0304 19:29:36.825390 23128000471168 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0304 19:29:36.825501 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:29:36.842660 23128000471168 run.py:483] Algo bellman_ford step 2801 current loss 0.020079, current_train_items 89664.
I0304 19:29:36.868303 23128000471168 run.py:483] Algo bellman_ford step 2802 current loss 0.095930, current_train_items 89696.
I0304 19:29:36.901386 23128000471168 run.py:483] Algo bellman_ford step 2803 current loss 0.067977, current_train_items 89728.
I0304 19:29:36.935695 23128000471168 run.py:483] Algo bellman_ford step 2804 current loss 0.084783, current_train_items 89760.
I0304 19:29:36.956399 23128000471168 run.py:483] Algo bellman_ford step 2805 current loss 0.005744, current_train_items 89792.
I0304 19:29:36.972931 23128000471168 run.py:483] Algo bellman_ford step 2806 current loss 0.070814, current_train_items 89824.
I0304 19:29:36.996625 23128000471168 run.py:483] Algo bellman_ford step 2807 current loss 0.050780, current_train_items 89856.
I0304 19:29:37.028880 23128000471168 run.py:483] Algo bellman_ford step 2808 current loss 0.077191, current_train_items 89888.
I0304 19:29:37.063293 23128000471168 run.py:483] Algo bellman_ford step 2809 current loss 0.105126, current_train_items 89920.
I0304 19:29:37.083685 23128000471168 run.py:483] Algo bellman_ford step 2810 current loss 0.003378, current_train_items 89952.
I0304 19:29:37.100758 23128000471168 run.py:483] Algo bellman_ford step 2811 current loss 0.061726, current_train_items 89984.
I0304 19:29:37.123980 23128000471168 run.py:483] Algo bellman_ford step 2812 current loss 0.038795, current_train_items 90016.
I0304 19:29:37.156338 23128000471168 run.py:483] Algo bellman_ford step 2813 current loss 0.097317, current_train_items 90048.
I0304 19:29:37.189483 23128000471168 run.py:483] Algo bellman_ford step 2814 current loss 0.082241, current_train_items 90080.
I0304 19:29:37.209510 23128000471168 run.py:483] Algo bellman_ford step 2815 current loss 0.004342, current_train_items 90112.
I0304 19:29:37.226123 23128000471168 run.py:483] Algo bellman_ford step 2816 current loss 0.038699, current_train_items 90144.
I0304 19:29:37.250951 23128000471168 run.py:483] Algo bellman_ford step 2817 current loss 0.084766, current_train_items 90176.
I0304 19:29:37.279495 23128000471168 run.py:483] Algo bellman_ford step 2818 current loss 0.085812, current_train_items 90208.
I0304 19:29:37.313347 23128000471168 run.py:483] Algo bellman_ford step 2819 current loss 0.112863, current_train_items 90240.
I0304 19:29:37.333208 23128000471168 run.py:483] Algo bellman_ford step 2820 current loss 0.008398, current_train_items 90272.
I0304 19:29:37.349574 23128000471168 run.py:483] Algo bellman_ford step 2821 current loss 0.023093, current_train_items 90304.
I0304 19:29:37.373910 23128000471168 run.py:483] Algo bellman_ford step 2822 current loss 0.067292, current_train_items 90336.
I0304 19:29:37.405277 23128000471168 run.py:483] Algo bellman_ford step 2823 current loss 0.144477, current_train_items 90368.
I0304 19:29:37.441194 23128000471168 run.py:483] Algo bellman_ford step 2824 current loss 0.105855, current_train_items 90400.
I0304 19:29:37.461379 23128000471168 run.py:483] Algo bellman_ford step 2825 current loss 0.005263, current_train_items 90432.
I0304 19:29:37.477739 23128000471168 run.py:483] Algo bellman_ford step 2826 current loss 0.038250, current_train_items 90464.
I0304 19:29:37.502297 23128000471168 run.py:483] Algo bellman_ford step 2827 current loss 0.121850, current_train_items 90496.
I0304 19:29:37.534027 23128000471168 run.py:483] Algo bellman_ford step 2828 current loss 0.069053, current_train_items 90528.
I0304 19:29:37.566918 23128000471168 run.py:483] Algo bellman_ford step 2829 current loss 0.084891, current_train_items 90560.
I0304 19:29:37.586697 23128000471168 run.py:483] Algo bellman_ford step 2830 current loss 0.016361, current_train_items 90592.
I0304 19:29:37.603400 23128000471168 run.py:483] Algo bellman_ford step 2831 current loss 0.084515, current_train_items 90624.
I0304 19:29:37.627364 23128000471168 run.py:483] Algo bellman_ford step 2832 current loss 0.098066, current_train_items 90656.
I0304 19:29:37.657966 23128000471168 run.py:483] Algo bellman_ford step 2833 current loss 0.056236, current_train_items 90688.
I0304 19:29:37.691774 23128000471168 run.py:483] Algo bellman_ford step 2834 current loss 0.103822, current_train_items 90720.
I0304 19:29:37.711845 23128000471168 run.py:483] Algo bellman_ford step 2835 current loss 0.066434, current_train_items 90752.
I0304 19:29:37.727909 23128000471168 run.py:483] Algo bellman_ford step 2836 current loss 0.023578, current_train_items 90784.
I0304 19:29:37.751355 23128000471168 run.py:483] Algo bellman_ford step 2837 current loss 0.064939, current_train_items 90816.
I0304 19:29:37.783196 23128000471168 run.py:483] Algo bellman_ford step 2838 current loss 0.078453, current_train_items 90848.
I0304 19:29:37.819453 23128000471168 run.py:483] Algo bellman_ford step 2839 current loss 0.088093, current_train_items 90880.
I0304 19:29:37.839413 23128000471168 run.py:483] Algo bellman_ford step 2840 current loss 0.007668, current_train_items 90912.
I0304 19:29:37.855509 23128000471168 run.py:483] Algo bellman_ford step 2841 current loss 0.043524, current_train_items 90944.
I0304 19:29:37.879907 23128000471168 run.py:483] Algo bellman_ford step 2842 current loss 0.114129, current_train_items 90976.
I0304 19:29:37.912369 23128000471168 run.py:483] Algo bellman_ford step 2843 current loss 0.099409, current_train_items 91008.
I0304 19:29:37.946952 23128000471168 run.py:483] Algo bellman_ford step 2844 current loss 0.088672, current_train_items 91040.
I0304 19:29:37.967445 23128000471168 run.py:483] Algo bellman_ford step 2845 current loss 0.002264, current_train_items 91072.
I0304 19:29:37.984327 23128000471168 run.py:483] Algo bellman_ford step 2846 current loss 0.035639, current_train_items 91104.
I0304 19:29:38.008064 23128000471168 run.py:483] Algo bellman_ford step 2847 current loss 0.048793, current_train_items 91136.
I0304 19:29:38.039490 23128000471168 run.py:483] Algo bellman_ford step 2848 current loss 0.082258, current_train_items 91168.
I0304 19:29:38.074718 23128000471168 run.py:483] Algo bellman_ford step 2849 current loss 0.152649, current_train_items 91200.
I0304 19:29:38.094701 23128000471168 run.py:483] Algo bellman_ford step 2850 current loss 0.005461, current_train_items 91232.
I0304 19:29:38.102837 23128000471168 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0304 19:29:38.102944 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:29:38.119699 23128000471168 run.py:483] Algo bellman_ford step 2851 current loss 0.026451, current_train_items 91264.
I0304 19:29:38.143863 23128000471168 run.py:483] Algo bellman_ford step 2852 current loss 0.048222, current_train_items 91296.
I0304 19:29:38.176189 23128000471168 run.py:483] Algo bellman_ford step 2853 current loss 0.073781, current_train_items 91328.
I0304 19:29:38.210212 23128000471168 run.py:483] Algo bellman_ford step 2854 current loss 0.107362, current_train_items 91360.
I0304 19:29:38.230772 23128000471168 run.py:483] Algo bellman_ford step 2855 current loss 0.004226, current_train_items 91392.
I0304 19:29:38.247097 23128000471168 run.py:483] Algo bellman_ford step 2856 current loss 0.042172, current_train_items 91424.
I0304 19:29:38.271913 23128000471168 run.py:483] Algo bellman_ford step 2857 current loss 0.110994, current_train_items 91456.
I0304 19:29:38.301466 23128000471168 run.py:483] Algo bellman_ford step 2858 current loss 0.082763, current_train_items 91488.
I0304 19:29:38.333770 23128000471168 run.py:483] Algo bellman_ford step 2859 current loss 0.083038, current_train_items 91520.
I0304 19:29:38.354120 23128000471168 run.py:483] Algo bellman_ford step 2860 current loss 0.003215, current_train_items 91552.
I0304 19:29:38.371086 23128000471168 run.py:483] Algo bellman_ford step 2861 current loss 0.017707, current_train_items 91584.
I0304 19:29:38.395019 23128000471168 run.py:483] Algo bellman_ford step 2862 current loss 0.062591, current_train_items 91616.
I0304 19:29:38.426944 23128000471168 run.py:483] Algo bellman_ford step 2863 current loss 0.085718, current_train_items 91648.
I0304 19:29:38.458130 23128000471168 run.py:483] Algo bellman_ford step 2864 current loss 0.104155, current_train_items 91680.
I0304 19:29:38.478679 23128000471168 run.py:483] Algo bellman_ford step 2865 current loss 0.004227, current_train_items 91712.
I0304 19:29:38.495149 23128000471168 run.py:483] Algo bellman_ford step 2866 current loss 0.035139, current_train_items 91744.
I0304 19:29:38.518899 23128000471168 run.py:483] Algo bellman_ford step 2867 current loss 0.070862, current_train_items 91776.
I0304 19:29:38.549733 23128000471168 run.py:483] Algo bellman_ford step 2868 current loss 0.045758, current_train_items 91808.
I0304 19:29:38.583976 23128000471168 run.py:483] Algo bellman_ford step 2869 current loss 0.124736, current_train_items 91840.
I0304 19:29:38.603974 23128000471168 run.py:483] Algo bellman_ford step 2870 current loss 0.002614, current_train_items 91872.
I0304 19:29:38.620558 23128000471168 run.py:483] Algo bellman_ford step 2871 current loss 0.032781, current_train_items 91904.
I0304 19:29:38.645210 23128000471168 run.py:483] Algo bellman_ford step 2872 current loss 0.061712, current_train_items 91936.
I0304 19:29:38.677371 23128000471168 run.py:483] Algo bellman_ford step 2873 current loss 0.063435, current_train_items 91968.
I0304 19:29:38.710268 23128000471168 run.py:483] Algo bellman_ford step 2874 current loss 0.089771, current_train_items 92000.
I0304 19:29:38.730601 23128000471168 run.py:483] Algo bellman_ford step 2875 current loss 0.003714, current_train_items 92032.
I0304 19:29:38.747190 23128000471168 run.py:483] Algo bellman_ford step 2876 current loss 0.022924, current_train_items 92064.
I0304 19:29:38.770567 23128000471168 run.py:483] Algo bellman_ford step 2877 current loss 0.031389, current_train_items 92096.
I0304 19:29:38.803135 23128000471168 run.py:483] Algo bellman_ford step 2878 current loss 0.151513, current_train_items 92128.
I0304 19:29:38.835958 23128000471168 run.py:483] Algo bellman_ford step 2879 current loss 0.067767, current_train_items 92160.
I0304 19:29:38.855890 23128000471168 run.py:483] Algo bellman_ford step 2880 current loss 0.031936, current_train_items 92192.
I0304 19:29:38.872281 23128000471168 run.py:483] Algo bellman_ford step 2881 current loss 0.014257, current_train_items 92224.
I0304 19:29:38.896496 23128000471168 run.py:483] Algo bellman_ford step 2882 current loss 0.066724, current_train_items 92256.
I0304 19:29:38.928144 23128000471168 run.py:483] Algo bellman_ford step 2883 current loss 0.086212, current_train_items 92288.
I0304 19:29:38.960312 23128000471168 run.py:483] Algo bellman_ford step 2884 current loss 0.104139, current_train_items 92320.
I0304 19:29:38.980436 23128000471168 run.py:483] Algo bellman_ford step 2885 current loss 0.003490, current_train_items 92352.
I0304 19:29:38.996984 23128000471168 run.py:483] Algo bellman_ford step 2886 current loss 0.030355, current_train_items 92384.
I0304 19:29:39.021078 23128000471168 run.py:483] Algo bellman_ford step 2887 current loss 0.057294, current_train_items 92416.
I0304 19:29:39.051780 23128000471168 run.py:483] Algo bellman_ford step 2888 current loss 0.063677, current_train_items 92448.
I0304 19:29:39.085852 23128000471168 run.py:483] Algo bellman_ford step 2889 current loss 0.120756, current_train_items 92480.
I0304 19:29:39.106052 23128000471168 run.py:483] Algo bellman_ford step 2890 current loss 0.006398, current_train_items 92512.
I0304 19:29:39.122816 23128000471168 run.py:483] Algo bellman_ford step 2891 current loss 0.022718, current_train_items 92544.
I0304 19:29:39.147865 23128000471168 run.py:483] Algo bellman_ford step 2892 current loss 0.046793, current_train_items 92576.
I0304 19:29:39.178296 23128000471168 run.py:483] Algo bellman_ford step 2893 current loss 0.055570, current_train_items 92608.
I0304 19:29:39.212461 23128000471168 run.py:483] Algo bellman_ford step 2894 current loss 0.096063, current_train_items 92640.
I0304 19:29:39.232649 23128000471168 run.py:483] Algo bellman_ford step 2895 current loss 0.006454, current_train_items 92672.
I0304 19:29:39.249738 23128000471168 run.py:483] Algo bellman_ford step 2896 current loss 0.049691, current_train_items 92704.
I0304 19:29:39.272509 23128000471168 run.py:483] Algo bellman_ford step 2897 current loss 0.085449, current_train_items 92736.
I0304 19:29:39.303223 23128000471168 run.py:483] Algo bellman_ford step 2898 current loss 0.044799, current_train_items 92768.
I0304 19:29:39.334444 23128000471168 run.py:483] Algo bellman_ford step 2899 current loss 0.059644, current_train_items 92800.
I0304 19:29:39.354343 23128000471168 run.py:483] Algo bellman_ford step 2900 current loss 0.003146, current_train_items 92832.
I0304 19:29:39.362199 23128000471168 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0304 19:29:39.362309 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:29:39.379130 23128000471168 run.py:483] Algo bellman_ford step 2901 current loss 0.010648, current_train_items 92864.
I0304 19:29:39.403252 23128000471168 run.py:483] Algo bellman_ford step 2902 current loss 0.066899, current_train_items 92896.
I0304 19:29:39.435061 23128000471168 run.py:483] Algo bellman_ford step 2903 current loss 0.087239, current_train_items 92928.
I0304 19:29:39.469686 23128000471168 run.py:483] Algo bellman_ford step 2904 current loss 0.087311, current_train_items 92960.
I0304 19:29:39.490035 23128000471168 run.py:483] Algo bellman_ford step 2905 current loss 0.007858, current_train_items 92992.
I0304 19:29:39.506367 23128000471168 run.py:483] Algo bellman_ford step 2906 current loss 0.036413, current_train_items 93024.
I0304 19:29:39.530839 23128000471168 run.py:483] Algo bellman_ford step 2907 current loss 0.073061, current_train_items 93056.
I0304 19:29:39.563615 23128000471168 run.py:483] Algo bellman_ford step 2908 current loss 0.084337, current_train_items 93088.
I0304 19:29:39.597984 23128000471168 run.py:483] Algo bellman_ford step 2909 current loss 0.093464, current_train_items 93120.
I0304 19:29:39.618043 23128000471168 run.py:483] Algo bellman_ford step 2910 current loss 0.007259, current_train_items 93152.
I0304 19:29:39.634976 23128000471168 run.py:483] Algo bellman_ford step 2911 current loss 0.043505, current_train_items 93184.
I0304 19:29:39.659184 23128000471168 run.py:483] Algo bellman_ford step 2912 current loss 0.041895, current_train_items 93216.
I0304 19:29:39.690279 23128000471168 run.py:483] Algo bellman_ford step 2913 current loss 0.042189, current_train_items 93248.
I0304 19:29:39.725427 23128000471168 run.py:483] Algo bellman_ford step 2914 current loss 0.109533, current_train_items 93280.
I0304 19:29:39.745416 23128000471168 run.py:483] Algo bellman_ford step 2915 current loss 0.005635, current_train_items 93312.
I0304 19:29:39.761868 23128000471168 run.py:483] Algo bellman_ford step 2916 current loss 0.015961, current_train_items 93344.
I0304 19:29:39.785815 23128000471168 run.py:483] Algo bellman_ford step 2917 current loss 0.040681, current_train_items 93376.
I0304 19:29:39.818433 23128000471168 run.py:483] Algo bellman_ford step 2918 current loss 0.081516, current_train_items 93408.
I0304 19:29:39.853996 23128000471168 run.py:483] Algo bellman_ford step 2919 current loss 0.072114, current_train_items 93440.
I0304 19:29:39.873995 23128000471168 run.py:483] Algo bellman_ford step 2920 current loss 0.002451, current_train_items 93472.
I0304 19:29:39.890672 23128000471168 run.py:483] Algo bellman_ford step 2921 current loss 0.015756, current_train_items 93504.
I0304 19:29:39.915230 23128000471168 run.py:483] Algo bellman_ford step 2922 current loss 0.052970, current_train_items 93536.
I0304 19:29:39.945718 23128000471168 run.py:483] Algo bellman_ford step 2923 current loss 0.089526, current_train_items 93568.
I0304 19:29:39.981324 23128000471168 run.py:483] Algo bellman_ford step 2924 current loss 0.092706, current_train_items 93600.
I0304 19:29:40.001346 23128000471168 run.py:483] Algo bellman_ford step 2925 current loss 0.037476, current_train_items 93632.
I0304 19:29:40.017642 23128000471168 run.py:483] Algo bellman_ford step 2926 current loss 0.040658, current_train_items 93664.
I0304 19:29:40.042534 23128000471168 run.py:483] Algo bellman_ford step 2927 current loss 0.057073, current_train_items 93696.
I0304 19:29:40.074585 23128000471168 run.py:483] Algo bellman_ford step 2928 current loss 0.100817, current_train_items 93728.
I0304 19:29:40.108846 23128000471168 run.py:483] Algo bellman_ford step 2929 current loss 0.084314, current_train_items 93760.
I0304 19:29:40.128700 23128000471168 run.py:483] Algo bellman_ford step 2930 current loss 0.003336, current_train_items 93792.
I0304 19:29:40.145050 23128000471168 run.py:483] Algo bellman_ford step 2931 current loss 0.027091, current_train_items 93824.
I0304 19:29:40.170649 23128000471168 run.py:483] Algo bellman_ford step 2932 current loss 0.104794, current_train_items 93856.
I0304 19:29:40.201263 23128000471168 run.py:483] Algo bellman_ford step 2933 current loss 0.077850, current_train_items 93888.
I0304 19:29:40.234497 23128000471168 run.py:483] Algo bellman_ford step 2934 current loss 0.093852, current_train_items 93920.
I0304 19:29:40.254633 23128000471168 run.py:483] Algo bellman_ford step 2935 current loss 0.007191, current_train_items 93952.
I0304 19:29:40.270845 23128000471168 run.py:483] Algo bellman_ford step 2936 current loss 0.030260, current_train_items 93984.
I0304 19:29:40.295177 23128000471168 run.py:483] Algo bellman_ford step 2937 current loss 0.131775, current_train_items 94016.
I0304 19:29:40.324877 23128000471168 run.py:483] Algo bellman_ford step 2938 current loss 0.073722, current_train_items 94048.
I0304 19:29:40.358858 23128000471168 run.py:483] Algo bellman_ford step 2939 current loss 0.072937, current_train_items 94080.
I0304 19:29:40.378634 23128000471168 run.py:483] Algo bellman_ford step 2940 current loss 0.027705, current_train_items 94112.
I0304 19:29:40.395280 23128000471168 run.py:483] Algo bellman_ford step 2941 current loss 0.040278, current_train_items 94144.
I0304 19:29:40.419963 23128000471168 run.py:483] Algo bellman_ford step 2942 current loss 0.074692, current_train_items 94176.
I0304 19:29:40.452531 23128000471168 run.py:483] Algo bellman_ford step 2943 current loss 0.105710, current_train_items 94208.
I0304 19:29:40.487873 23128000471168 run.py:483] Algo bellman_ford step 2944 current loss 0.078959, current_train_items 94240.
I0304 19:29:40.507702 23128000471168 run.py:483] Algo bellman_ford step 2945 current loss 0.051096, current_train_items 94272.
I0304 19:29:40.524420 23128000471168 run.py:483] Algo bellman_ford step 2946 current loss 0.018990, current_train_items 94304.
I0304 19:29:40.548651 23128000471168 run.py:483] Algo bellman_ford step 2947 current loss 0.067859, current_train_items 94336.
I0304 19:29:40.579905 23128000471168 run.py:483] Algo bellman_ford step 2948 current loss 0.136941, current_train_items 94368.
I0304 19:29:40.613825 23128000471168 run.py:483] Algo bellman_ford step 2949 current loss 0.127209, current_train_items 94400.
I0304 19:29:40.633517 23128000471168 run.py:483] Algo bellman_ford step 2950 current loss 0.005408, current_train_items 94432.
I0304 19:29:40.641629 23128000471168 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0304 19:29:40.641738 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:29:40.658864 23128000471168 run.py:483] Algo bellman_ford step 2951 current loss 0.053663, current_train_items 94464.
I0304 19:29:40.681719 23128000471168 run.py:483] Algo bellman_ford step 2952 current loss 0.019531, current_train_items 94496.
I0304 19:29:40.712947 23128000471168 run.py:483] Algo bellman_ford step 2953 current loss 0.081137, current_train_items 94528.
I0304 19:29:40.747466 23128000471168 run.py:483] Algo bellman_ford step 2954 current loss 0.106051, current_train_items 94560.
I0304 19:29:40.767644 23128000471168 run.py:483] Algo bellman_ford step 2955 current loss 0.002476, current_train_items 94592.
I0304 19:29:40.783551 23128000471168 run.py:483] Algo bellman_ford step 2956 current loss 0.028551, current_train_items 94624.
I0304 19:29:40.808162 23128000471168 run.py:483] Algo bellman_ford step 2957 current loss 0.064571, current_train_items 94656.
I0304 19:29:40.840558 23128000471168 run.py:483] Algo bellman_ford step 2958 current loss 0.070014, current_train_items 94688.
I0304 19:29:40.874770 23128000471168 run.py:483] Algo bellman_ford step 2959 current loss 0.131627, current_train_items 94720.
I0304 19:29:40.895197 23128000471168 run.py:483] Algo bellman_ford step 2960 current loss 0.015566, current_train_items 94752.
I0304 19:29:40.911992 23128000471168 run.py:483] Algo bellman_ford step 2961 current loss 0.026194, current_train_items 94784.
I0304 19:29:40.935420 23128000471168 run.py:483] Algo bellman_ford step 2962 current loss 0.050893, current_train_items 94816.
I0304 19:29:40.965829 23128000471168 run.py:483] Algo bellman_ford step 2963 current loss 0.067958, current_train_items 94848.
I0304 19:29:41.000760 23128000471168 run.py:483] Algo bellman_ford step 2964 current loss 0.105157, current_train_items 94880.
I0304 19:29:41.020406 23128000471168 run.py:483] Algo bellman_ford step 2965 current loss 0.003633, current_train_items 94912.
I0304 19:29:41.036778 23128000471168 run.py:483] Algo bellman_ford step 2966 current loss 0.023445, current_train_items 94944.
I0304 19:29:41.060744 23128000471168 run.py:483] Algo bellman_ford step 2967 current loss 0.088220, current_train_items 94976.
I0304 19:29:41.091760 23128000471168 run.py:483] Algo bellman_ford step 2968 current loss 0.063093, current_train_items 95008.
I0304 19:29:41.126216 23128000471168 run.py:483] Algo bellman_ford step 2969 current loss 0.075464, current_train_items 95040.
I0304 19:29:41.146622 23128000471168 run.py:483] Algo bellman_ford step 2970 current loss 0.005729, current_train_items 95072.
I0304 19:29:41.163032 23128000471168 run.py:483] Algo bellman_ford step 2971 current loss 0.034750, current_train_items 95104.
I0304 19:29:41.186786 23128000471168 run.py:483] Algo bellman_ford step 2972 current loss 0.057602, current_train_items 95136.
I0304 19:29:41.217331 23128000471168 run.py:483] Algo bellman_ford step 2973 current loss 0.055389, current_train_items 95168.
I0304 19:29:41.253816 23128000471168 run.py:483] Algo bellman_ford step 2974 current loss 0.142905, current_train_items 95200.
I0304 19:29:41.273873 23128000471168 run.py:483] Algo bellman_ford step 2975 current loss 0.004332, current_train_items 95232.
I0304 19:29:41.291055 23128000471168 run.py:483] Algo bellman_ford step 2976 current loss 0.024528, current_train_items 95264.
I0304 19:29:41.314685 23128000471168 run.py:483] Algo bellman_ford step 2977 current loss 0.083464, current_train_items 95296.
I0304 19:29:41.345796 23128000471168 run.py:483] Algo bellman_ford step 2978 current loss 0.073417, current_train_items 95328.
I0304 19:29:41.380974 23128000471168 run.py:483] Algo bellman_ford step 2979 current loss 0.087392, current_train_items 95360.
I0304 19:29:41.400747 23128000471168 run.py:483] Algo bellman_ford step 2980 current loss 0.003718, current_train_items 95392.
I0304 19:29:41.417060 23128000471168 run.py:483] Algo bellman_ford step 2981 current loss 0.029509, current_train_items 95424.
I0304 19:29:41.442015 23128000471168 run.py:483] Algo bellman_ford step 2982 current loss 0.069022, current_train_items 95456.
I0304 19:29:41.475020 23128000471168 run.py:483] Algo bellman_ford step 2983 current loss 0.057398, current_train_items 95488.
I0304 19:29:41.509022 23128000471168 run.py:483] Algo bellman_ford step 2984 current loss 0.071673, current_train_items 95520.
I0304 19:29:41.528995 23128000471168 run.py:483] Algo bellman_ford step 2985 current loss 0.002813, current_train_items 95552.
I0304 19:29:41.545937 23128000471168 run.py:483] Algo bellman_ford step 2986 current loss 0.061878, current_train_items 95584.
I0304 19:29:41.570658 23128000471168 run.py:483] Algo bellman_ford step 2987 current loss 0.068567, current_train_items 95616.
I0304 19:29:41.601942 23128000471168 run.py:483] Algo bellman_ford step 2988 current loss 0.053653, current_train_items 95648.
I0304 19:29:41.636389 23128000471168 run.py:483] Algo bellman_ford step 2989 current loss 0.075294, current_train_items 95680.
I0304 19:29:41.656416 23128000471168 run.py:483] Algo bellman_ford step 2990 current loss 0.017130, current_train_items 95712.
I0304 19:29:41.673693 23128000471168 run.py:483] Algo bellman_ford step 2991 current loss 0.035724, current_train_items 95744.
I0304 19:29:41.697986 23128000471168 run.py:483] Algo bellman_ford step 2992 current loss 0.048465, current_train_items 95776.
I0304 19:29:41.730034 23128000471168 run.py:483] Algo bellman_ford step 2993 current loss 0.083222, current_train_items 95808.
I0304 19:29:41.765486 23128000471168 run.py:483] Algo bellman_ford step 2994 current loss 0.110906, current_train_items 95840.
I0304 19:29:41.785141 23128000471168 run.py:483] Algo bellman_ford step 2995 current loss 0.051250, current_train_items 95872.
I0304 19:29:41.801716 23128000471168 run.py:483] Algo bellman_ford step 2996 current loss 0.059403, current_train_items 95904.
I0304 19:29:41.826824 23128000471168 run.py:483] Algo bellman_ford step 2997 current loss 0.084918, current_train_items 95936.
I0304 19:29:41.858908 23128000471168 run.py:483] Algo bellman_ford step 2998 current loss 0.062266, current_train_items 95968.
I0304 19:29:41.891776 23128000471168 run.py:483] Algo bellman_ford step 2999 current loss 0.102875, current_train_items 96000.
I0304 19:29:41.911919 23128000471168 run.py:483] Algo bellman_ford step 3000 current loss 0.002447, current_train_items 96032.
I0304 19:29:41.919871 23128000471168 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0304 19:29:41.919979 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:41.936918 23128000471168 run.py:483] Algo bellman_ford step 3001 current loss 0.049476, current_train_items 96064.
I0304 19:29:41.960999 23128000471168 run.py:483] Algo bellman_ford step 3002 current loss 0.045038, current_train_items 96096.
I0304 19:29:41.993255 23128000471168 run.py:483] Algo bellman_ford step 3003 current loss 0.094395, current_train_items 96128.
I0304 19:29:42.026881 23128000471168 run.py:483] Algo bellman_ford step 3004 current loss 0.096549, current_train_items 96160.
I0304 19:29:42.047305 23128000471168 run.py:483] Algo bellman_ford step 3005 current loss 0.004387, current_train_items 96192.
I0304 19:29:42.063570 23128000471168 run.py:483] Algo bellman_ford step 3006 current loss 0.020015, current_train_items 96224.
I0304 19:29:42.088631 23128000471168 run.py:483] Algo bellman_ford step 3007 current loss 0.054166, current_train_items 96256.
I0304 19:29:42.120640 23128000471168 run.py:483] Algo bellman_ford step 3008 current loss 0.086834, current_train_items 96288.
I0304 19:29:42.157318 23128000471168 run.py:483] Algo bellman_ford step 3009 current loss 0.130441, current_train_items 96320.
I0304 19:29:42.177022 23128000471168 run.py:483] Algo bellman_ford step 3010 current loss 0.002584, current_train_items 96352.
I0304 19:29:42.193567 23128000471168 run.py:483] Algo bellman_ford step 3011 current loss 0.034930, current_train_items 96384.
I0304 19:29:42.217610 23128000471168 run.py:483] Algo bellman_ford step 3012 current loss 0.054395, current_train_items 96416.
I0304 19:29:42.248398 23128000471168 run.py:483] Algo bellman_ford step 3013 current loss 0.101444, current_train_items 96448.
I0304 19:29:42.283486 23128000471168 run.py:483] Algo bellman_ford step 3014 current loss 0.092488, current_train_items 96480.
I0304 19:29:42.303359 23128000471168 run.py:483] Algo bellman_ford step 3015 current loss 0.006752, current_train_items 96512.
I0304 19:29:42.320199 23128000471168 run.py:483] Algo bellman_ford step 3016 current loss 0.033290, current_train_items 96544.
I0304 19:29:42.344130 23128000471168 run.py:483] Algo bellman_ford step 3017 current loss 0.102649, current_train_items 96576.
I0304 19:29:42.374821 23128000471168 run.py:483] Algo bellman_ford step 3018 current loss 0.087462, current_train_items 96608.
I0304 19:29:42.407436 23128000471168 run.py:483] Algo bellman_ford step 3019 current loss 0.106519, current_train_items 96640.
I0304 19:29:42.427154 23128000471168 run.py:483] Algo bellman_ford step 3020 current loss 0.008040, current_train_items 96672.
I0304 19:29:42.443190 23128000471168 run.py:483] Algo bellman_ford step 3021 current loss 0.017913, current_train_items 96704.
I0304 19:29:42.468716 23128000471168 run.py:483] Algo bellman_ford step 3022 current loss 0.082311, current_train_items 96736.
I0304 19:29:42.500251 23128000471168 run.py:483] Algo bellman_ford step 3023 current loss 0.117114, current_train_items 96768.
I0304 19:29:42.533840 23128000471168 run.py:483] Algo bellman_ford step 3024 current loss 0.080642, current_train_items 96800.
I0304 19:29:42.553735 23128000471168 run.py:483] Algo bellman_ford step 3025 current loss 0.007910, current_train_items 96832.
I0304 19:29:42.570035 23128000471168 run.py:483] Algo bellman_ford step 3026 current loss 0.028119, current_train_items 96864.
I0304 19:29:42.592875 23128000471168 run.py:483] Algo bellman_ford step 3027 current loss 0.052870, current_train_items 96896.
I0304 19:29:42.624066 23128000471168 run.py:483] Algo bellman_ford step 3028 current loss 0.072527, current_train_items 96928.
I0304 19:29:42.657116 23128000471168 run.py:483] Algo bellman_ford step 3029 current loss 0.099350, current_train_items 96960.
I0304 19:29:42.676959 23128000471168 run.py:483] Algo bellman_ford step 3030 current loss 0.003262, current_train_items 96992.
I0304 19:29:42.693094 23128000471168 run.py:483] Algo bellman_ford step 3031 current loss 0.053699, current_train_items 97024.
I0304 19:29:42.717827 23128000471168 run.py:483] Algo bellman_ford step 3032 current loss 0.067411, current_train_items 97056.
I0304 19:29:42.749813 23128000471168 run.py:483] Algo bellman_ford step 3033 current loss 0.069244, current_train_items 97088.
I0304 19:29:42.784546 23128000471168 run.py:483] Algo bellman_ford step 3034 current loss 0.062049, current_train_items 97120.
I0304 19:29:42.804431 23128000471168 run.py:483] Algo bellman_ford step 3035 current loss 0.027382, current_train_items 97152.
I0304 19:29:42.821181 23128000471168 run.py:483] Algo bellman_ford step 3036 current loss 0.017084, current_train_items 97184.
I0304 19:29:42.845528 23128000471168 run.py:483] Algo bellman_ford step 3037 current loss 0.043489, current_train_items 97216.
I0304 19:29:42.877243 23128000471168 run.py:483] Algo bellman_ford step 3038 current loss 0.088591, current_train_items 97248.
I0304 19:29:42.909608 23128000471168 run.py:483] Algo bellman_ford step 3039 current loss 0.125048, current_train_items 97280.
I0304 19:29:42.929329 23128000471168 run.py:483] Algo bellman_ford step 3040 current loss 0.003604, current_train_items 97312.
I0304 19:29:42.945688 23128000471168 run.py:483] Algo bellman_ford step 3041 current loss 0.018342, current_train_items 97344.
I0304 19:29:42.969535 23128000471168 run.py:483] Algo bellman_ford step 3042 current loss 0.060276, current_train_items 97376.
I0304 19:29:43.001021 23128000471168 run.py:483] Algo bellman_ford step 3043 current loss 0.074797, current_train_items 97408.
I0304 19:29:43.033711 23128000471168 run.py:483] Algo bellman_ford step 3044 current loss 0.096129, current_train_items 97440.
I0304 19:29:43.053575 23128000471168 run.py:483] Algo bellman_ford step 3045 current loss 0.004654, current_train_items 97472.
I0304 19:29:43.069983 23128000471168 run.py:483] Algo bellman_ford step 3046 current loss 0.058646, current_train_items 97504.
I0304 19:29:43.093904 23128000471168 run.py:483] Algo bellman_ford step 3047 current loss 0.101295, current_train_items 97536.
I0304 19:29:43.125897 23128000471168 run.py:483] Algo bellman_ford step 3048 current loss 0.066950, current_train_items 97568.
I0304 19:29:43.159739 23128000471168 run.py:483] Algo bellman_ford step 3049 current loss 0.085601, current_train_items 97600.
I0304 19:29:43.179568 23128000471168 run.py:483] Algo bellman_ford step 3050 current loss 0.002961, current_train_items 97632.
I0304 19:29:43.188134 23128000471168 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0304 19:29:43.188243 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:43.205181 23128000471168 run.py:483] Algo bellman_ford step 3051 current loss 0.017399, current_train_items 97664.
I0304 19:29:43.229996 23128000471168 run.py:483] Algo bellman_ford step 3052 current loss 0.055449, current_train_items 97696.
I0304 19:29:43.262211 23128000471168 run.py:483] Algo bellman_ford step 3053 current loss 0.065971, current_train_items 97728.
I0304 19:29:43.294752 23128000471168 run.py:483] Algo bellman_ford step 3054 current loss 0.083040, current_train_items 97760.
I0304 19:29:43.314766 23128000471168 run.py:483] Algo bellman_ford step 3055 current loss 0.002940, current_train_items 97792.
I0304 19:29:43.330869 23128000471168 run.py:483] Algo bellman_ford step 3056 current loss 0.076253, current_train_items 97824.
I0304 19:29:43.355250 23128000471168 run.py:483] Algo bellman_ford step 3057 current loss 0.084301, current_train_items 97856.
I0304 19:29:43.386631 23128000471168 run.py:483] Algo bellman_ford step 3058 current loss 0.063150, current_train_items 97888.
I0304 19:29:43.419643 23128000471168 run.py:483] Algo bellman_ford step 3059 current loss 0.089677, current_train_items 97920.
I0304 19:29:43.439634 23128000471168 run.py:483] Algo bellman_ford step 3060 current loss 0.007839, current_train_items 97952.
I0304 19:29:43.456389 23128000471168 run.py:483] Algo bellman_ford step 3061 current loss 0.037462, current_train_items 97984.
I0304 19:29:43.479674 23128000471168 run.py:483] Algo bellman_ford step 3062 current loss 0.043017, current_train_items 98016.
I0304 19:29:43.512164 23128000471168 run.py:483] Algo bellman_ford step 3063 current loss 0.096009, current_train_items 98048.
I0304 19:29:43.547056 23128000471168 run.py:483] Algo bellman_ford step 3064 current loss 0.065685, current_train_items 98080.
I0304 19:29:43.567101 23128000471168 run.py:483] Algo bellman_ford step 3065 current loss 0.005077, current_train_items 98112.
I0304 19:29:43.583411 23128000471168 run.py:483] Algo bellman_ford step 3066 current loss 0.028087, current_train_items 98144.
I0304 19:29:43.608232 23128000471168 run.py:483] Algo bellman_ford step 3067 current loss 0.042642, current_train_items 98176.
I0304 19:29:43.638894 23128000471168 run.py:483] Algo bellman_ford step 3068 current loss 0.069932, current_train_items 98208.
I0304 19:29:43.675183 23128000471168 run.py:483] Algo bellman_ford step 3069 current loss 0.090394, current_train_items 98240.
I0304 19:29:43.695735 23128000471168 run.py:483] Algo bellman_ford step 3070 current loss 0.003456, current_train_items 98272.
I0304 19:29:43.712362 23128000471168 run.py:483] Algo bellman_ford step 3071 current loss 0.058556, current_train_items 98304.
I0304 19:29:43.735718 23128000471168 run.py:483] Algo bellman_ford step 3072 current loss 0.046753, current_train_items 98336.
I0304 19:29:43.766424 23128000471168 run.py:483] Algo bellman_ford step 3073 current loss 0.064404, current_train_items 98368.
I0304 19:29:43.802141 23128000471168 run.py:483] Algo bellman_ford step 3074 current loss 0.153462, current_train_items 98400.
I0304 19:29:43.822324 23128000471168 run.py:483] Algo bellman_ford step 3075 current loss 0.004203, current_train_items 98432.
I0304 19:29:43.839184 23128000471168 run.py:483] Algo bellman_ford step 3076 current loss 0.036092, current_train_items 98464.
I0304 19:29:43.863000 23128000471168 run.py:483] Algo bellman_ford step 3077 current loss 0.068047, current_train_items 98496.
I0304 19:29:43.894983 23128000471168 run.py:483] Algo bellman_ford step 3078 current loss 0.110868, current_train_items 98528.
I0304 19:29:43.927225 23128000471168 run.py:483] Algo bellman_ford step 3079 current loss 0.073115, current_train_items 98560.
I0304 19:29:43.947115 23128000471168 run.py:483] Algo bellman_ford step 3080 current loss 0.008456, current_train_items 98592.
I0304 19:29:43.963752 23128000471168 run.py:483] Algo bellman_ford step 3081 current loss 0.039559, current_train_items 98624.
I0304 19:29:43.987469 23128000471168 run.py:483] Algo bellman_ford step 3082 current loss 0.061202, current_train_items 98656.
I0304 19:29:44.018857 23128000471168 run.py:483] Algo bellman_ford step 3083 current loss 0.107451, current_train_items 98688.
I0304 19:29:44.052910 23128000471168 run.py:483] Algo bellman_ford step 3084 current loss 0.129920, current_train_items 98720.
I0304 19:29:44.073047 23128000471168 run.py:483] Algo bellman_ford step 3085 current loss 0.024538, current_train_items 98752.
I0304 19:29:44.089829 23128000471168 run.py:483] Algo bellman_ford step 3086 current loss 0.012045, current_train_items 98784.
I0304 19:29:44.112681 23128000471168 run.py:483] Algo bellman_ford step 3087 current loss 0.031095, current_train_items 98816.
I0304 19:29:44.144164 23128000471168 run.py:483] Algo bellman_ford step 3088 current loss 0.087934, current_train_items 98848.
I0304 19:29:44.179366 23128000471168 run.py:483] Algo bellman_ford step 3089 current loss 0.133503, current_train_items 98880.
I0304 19:29:44.199442 23128000471168 run.py:483] Algo bellman_ford step 3090 current loss 0.004991, current_train_items 98912.
I0304 19:29:44.215894 23128000471168 run.py:483] Algo bellman_ford step 3091 current loss 0.013390, current_train_items 98944.
I0304 19:29:44.238991 23128000471168 run.py:483] Algo bellman_ford step 3092 current loss 0.082102, current_train_items 98976.
I0304 19:29:44.270400 23128000471168 run.py:483] Algo bellman_ford step 3093 current loss 0.148134, current_train_items 99008.
I0304 19:29:44.304120 23128000471168 run.py:483] Algo bellman_ford step 3094 current loss 0.082697, current_train_items 99040.
I0304 19:29:44.323997 23128000471168 run.py:483] Algo bellman_ford step 3095 current loss 0.005455, current_train_items 99072.
I0304 19:29:44.340980 23128000471168 run.py:483] Algo bellman_ford step 3096 current loss 0.029656, current_train_items 99104.
I0304 19:29:44.366078 23128000471168 run.py:483] Algo bellman_ford step 3097 current loss 0.100689, current_train_items 99136.
I0304 19:29:44.397050 23128000471168 run.py:483] Algo bellman_ford step 3098 current loss 0.105598, current_train_items 99168.
I0304 19:29:44.430444 23128000471168 run.py:483] Algo bellman_ford step 3099 current loss 0.078690, current_train_items 99200.
I0304 19:29:44.450637 23128000471168 run.py:483] Algo bellman_ford step 3100 current loss 0.004489, current_train_items 99232.
I0304 19:29:44.458240 23128000471168 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0304 19:29:44.458347 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:44.476440 23128000471168 run.py:483] Algo bellman_ford step 3101 current loss 0.033612, current_train_items 99264.
I0304 19:29:44.501093 23128000471168 run.py:483] Algo bellman_ford step 3102 current loss 0.057131, current_train_items 99296.
I0304 19:29:44.533813 23128000471168 run.py:483] Algo bellman_ford step 3103 current loss 0.039102, current_train_items 99328.
I0304 19:29:44.568612 23128000471168 run.py:483] Algo bellman_ford step 3104 current loss 0.078002, current_train_items 99360.
I0304 19:29:44.589035 23128000471168 run.py:483] Algo bellman_ford step 3105 current loss 0.033728, current_train_items 99392.
I0304 19:29:44.604982 23128000471168 run.py:483] Algo bellman_ford step 3106 current loss 0.004440, current_train_items 99424.
I0304 19:29:44.628999 23128000471168 run.py:483] Algo bellman_ford step 3107 current loss 0.035869, current_train_items 99456.
I0304 19:29:44.661047 23128000471168 run.py:483] Algo bellman_ford step 3108 current loss 0.063201, current_train_items 99488.
I0304 19:29:44.693654 23128000471168 run.py:483] Algo bellman_ford step 3109 current loss 0.089130, current_train_items 99520.
I0304 19:29:44.714251 23128000471168 run.py:483] Algo bellman_ford step 3110 current loss 0.005873, current_train_items 99552.
I0304 19:29:44.730710 23128000471168 run.py:483] Algo bellman_ford step 3111 current loss 0.025967, current_train_items 99584.
I0304 19:29:44.754540 23128000471168 run.py:483] Algo bellman_ford step 3112 current loss 0.037958, current_train_items 99616.
I0304 19:29:44.785856 23128000471168 run.py:483] Algo bellman_ford step 3113 current loss 0.055100, current_train_items 99648.
I0304 19:29:44.820065 23128000471168 run.py:483] Algo bellman_ford step 3114 current loss 0.052330, current_train_items 99680.
I0304 19:29:44.840137 23128000471168 run.py:483] Algo bellman_ford step 3115 current loss 0.002583, current_train_items 99712.
I0304 19:29:44.856793 23128000471168 run.py:483] Algo bellman_ford step 3116 current loss 0.018798, current_train_items 99744.
I0304 19:29:44.881227 23128000471168 run.py:483] Algo bellman_ford step 3117 current loss 0.052133, current_train_items 99776.
I0304 19:29:44.913022 23128000471168 run.py:483] Algo bellman_ford step 3118 current loss 0.103788, current_train_items 99808.
I0304 19:29:44.946259 23128000471168 run.py:483] Algo bellman_ford step 3119 current loss 0.102079, current_train_items 99840.
I0304 19:29:44.966272 23128000471168 run.py:483] Algo bellman_ford step 3120 current loss 0.002112, current_train_items 99872.
I0304 19:29:44.982670 23128000471168 run.py:483] Algo bellman_ford step 3121 current loss 0.040284, current_train_items 99904.
I0304 19:29:45.006703 23128000471168 run.py:483] Algo bellman_ford step 3122 current loss 0.034686, current_train_items 99936.
I0304 19:29:45.038598 23128000471168 run.py:483] Algo bellman_ford step 3123 current loss 0.069251, current_train_items 99968.
I0304 19:29:45.072599 23128000471168 run.py:483] Algo bellman_ford step 3124 current loss 0.103437, current_train_items 100000.
I0304 19:29:45.092412 23128000471168 run.py:483] Algo bellman_ford step 3125 current loss 0.006400, current_train_items 100032.
I0304 19:29:45.108934 23128000471168 run.py:483] Algo bellman_ford step 3126 current loss 0.032525, current_train_items 100064.
I0304 19:29:45.132063 23128000471168 run.py:483] Algo bellman_ford step 3127 current loss 0.094246, current_train_items 100096.
I0304 19:29:45.165041 23128000471168 run.py:483] Algo bellman_ford step 3128 current loss 0.136061, current_train_items 100128.
I0304 19:29:45.197413 23128000471168 run.py:483] Algo bellman_ford step 3129 current loss 0.103575, current_train_items 100160.
I0304 19:29:45.217355 23128000471168 run.py:483] Algo bellman_ford step 3130 current loss 0.004358, current_train_items 100192.
I0304 19:29:45.233789 23128000471168 run.py:483] Algo bellman_ford step 3131 current loss 0.021128, current_train_items 100224.
I0304 19:29:45.258057 23128000471168 run.py:483] Algo bellman_ford step 3132 current loss 0.081688, current_train_items 100256.
I0304 19:29:45.288739 23128000471168 run.py:483] Algo bellman_ford step 3133 current loss 0.130497, current_train_items 100288.
I0304 19:29:45.324060 23128000471168 run.py:483] Algo bellman_ford step 3134 current loss 0.145676, current_train_items 100320.
I0304 19:29:45.344136 23128000471168 run.py:483] Algo bellman_ford step 3135 current loss 0.014727, current_train_items 100352.
I0304 19:29:45.360463 23128000471168 run.py:483] Algo bellman_ford step 3136 current loss 0.021520, current_train_items 100384.
I0304 19:29:45.384722 23128000471168 run.py:483] Algo bellman_ford step 3137 current loss 0.055904, current_train_items 100416.
I0304 19:29:45.415158 23128000471168 run.py:483] Algo bellman_ford step 3138 current loss 0.050110, current_train_items 100448.
I0304 19:29:45.449901 23128000471168 run.py:483] Algo bellman_ford step 3139 current loss 0.111626, current_train_items 100480.
I0304 19:29:45.469692 23128000471168 run.py:483] Algo bellman_ford step 3140 current loss 0.005938, current_train_items 100512.
I0304 19:29:45.486696 23128000471168 run.py:483] Algo bellman_ford step 3141 current loss 0.071220, current_train_items 100544.
I0304 19:29:45.509382 23128000471168 run.py:483] Algo bellman_ford step 3142 current loss 0.051969, current_train_items 100576.
I0304 19:29:45.542197 23128000471168 run.py:483] Algo bellman_ford step 3143 current loss 0.079184, current_train_items 100608.
I0304 19:29:45.575413 23128000471168 run.py:483] Algo bellman_ford step 3144 current loss 0.072827, current_train_items 100640.
I0304 19:29:45.595283 23128000471168 run.py:483] Algo bellman_ford step 3145 current loss 0.003309, current_train_items 100672.
I0304 19:29:45.611632 23128000471168 run.py:483] Algo bellman_ford step 3146 current loss 0.018188, current_train_items 100704.
I0304 19:29:45.635074 23128000471168 run.py:483] Algo bellman_ford step 3147 current loss 0.025106, current_train_items 100736.
I0304 19:29:45.664754 23128000471168 run.py:483] Algo bellman_ford step 3148 current loss 0.042617, current_train_items 100768.
I0304 19:29:45.700754 23128000471168 run.py:483] Algo bellman_ford step 3149 current loss 0.096497, current_train_items 100800.
I0304 19:29:45.720449 23128000471168 run.py:483] Algo bellman_ford step 3150 current loss 0.004471, current_train_items 100832.
I0304 19:29:45.728310 23128000471168 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0304 19:29:45.728420 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:45.745651 23128000471168 run.py:483] Algo bellman_ford step 3151 current loss 0.034823, current_train_items 100864.
I0304 19:29:45.771200 23128000471168 run.py:483] Algo bellman_ford step 3152 current loss 0.094937, current_train_items 100896.
I0304 19:29:45.803197 23128000471168 run.py:483] Algo bellman_ford step 3153 current loss 0.067436, current_train_items 100928.
I0304 19:29:45.837988 23128000471168 run.py:483] Algo bellman_ford step 3154 current loss 0.088645, current_train_items 100960.
I0304 19:29:45.858261 23128000471168 run.py:483] Algo bellman_ford step 3155 current loss 0.010987, current_train_items 100992.
I0304 19:29:45.874394 23128000471168 run.py:483] Algo bellman_ford step 3156 current loss 0.025116, current_train_items 101024.
I0304 19:29:45.898866 23128000471168 run.py:483] Algo bellman_ford step 3157 current loss 0.056528, current_train_items 101056.
I0304 19:29:45.929761 23128000471168 run.py:483] Algo bellman_ford step 3158 current loss 0.107795, current_train_items 101088.
I0304 19:29:45.962887 23128000471168 run.py:483] Algo bellman_ford step 3159 current loss 0.087549, current_train_items 101120.
I0304 19:29:45.983211 23128000471168 run.py:483] Algo bellman_ford step 3160 current loss 0.004147, current_train_items 101152.
I0304 19:29:45.999508 23128000471168 run.py:483] Algo bellman_ford step 3161 current loss 0.018502, current_train_items 101184.
I0304 19:29:46.023083 23128000471168 run.py:483] Algo bellman_ford step 3162 current loss 0.042808, current_train_items 101216.
I0304 19:29:46.053950 23128000471168 run.py:483] Algo bellman_ford step 3163 current loss 0.089160, current_train_items 101248.
I0304 19:29:46.089010 23128000471168 run.py:483] Algo bellman_ford step 3164 current loss 0.161061, current_train_items 101280.
I0304 19:29:46.109055 23128000471168 run.py:483] Algo bellman_ford step 3165 current loss 0.007027, current_train_items 101312.
I0304 19:29:46.126090 23128000471168 run.py:483] Algo bellman_ford step 3166 current loss 0.099202, current_train_items 101344.
I0304 19:29:46.150457 23128000471168 run.py:483] Algo bellman_ford step 3167 current loss 0.065768, current_train_items 101376.
I0304 19:29:46.181900 23128000471168 run.py:483] Algo bellman_ford step 3168 current loss 0.072232, current_train_items 101408.
I0304 19:29:46.215665 23128000471168 run.py:483] Algo bellman_ford step 3169 current loss 0.075262, current_train_items 101440.
I0304 19:29:46.235975 23128000471168 run.py:483] Algo bellman_ford step 3170 current loss 0.008260, current_train_items 101472.
I0304 19:29:46.252980 23128000471168 run.py:483] Algo bellman_ford step 3171 current loss 0.093569, current_train_items 101504.
I0304 19:29:46.277032 23128000471168 run.py:483] Algo bellman_ford step 3172 current loss 0.044167, current_train_items 101536.
I0304 19:29:46.308711 23128000471168 run.py:483] Algo bellman_ford step 3173 current loss 0.086642, current_train_items 101568.
I0304 19:29:46.340642 23128000471168 run.py:483] Algo bellman_ford step 3174 current loss 0.068199, current_train_items 101600.
I0304 19:29:46.360819 23128000471168 run.py:483] Algo bellman_ford step 3175 current loss 0.005419, current_train_items 101632.
I0304 19:29:46.377514 23128000471168 run.py:483] Algo bellman_ford step 3176 current loss 0.038315, current_train_items 101664.
I0304 19:29:46.400917 23128000471168 run.py:483] Algo bellman_ford step 3177 current loss 0.062921, current_train_items 101696.
I0304 19:29:46.430318 23128000471168 run.py:483] Algo bellman_ford step 3178 current loss 0.049067, current_train_items 101728.
I0304 19:29:46.465130 23128000471168 run.py:483] Algo bellman_ford step 3179 current loss 0.088277, current_train_items 101760.
I0304 19:29:46.484944 23128000471168 run.py:483] Algo bellman_ford step 3180 current loss 0.010440, current_train_items 101792.
I0304 19:29:46.501563 23128000471168 run.py:483] Algo bellman_ford step 3181 current loss 0.019408, current_train_items 101824.
I0304 19:29:46.525814 23128000471168 run.py:483] Algo bellman_ford step 3182 current loss 0.037230, current_train_items 101856.
I0304 19:29:46.557474 23128000471168 run.py:483] Algo bellman_ford step 3183 current loss 0.033963, current_train_items 101888.
I0304 19:29:46.594087 23128000471168 run.py:483] Algo bellman_ford step 3184 current loss 0.083238, current_train_items 101920.
I0304 19:29:46.614651 23128000471168 run.py:483] Algo bellman_ford step 3185 current loss 0.005714, current_train_items 101952.
I0304 19:29:46.631189 23128000471168 run.py:483] Algo bellman_ford step 3186 current loss 0.028211, current_train_items 101984.
I0304 19:29:46.655663 23128000471168 run.py:483] Algo bellman_ford step 3187 current loss 0.060698, current_train_items 102016.
I0304 19:29:46.686876 23128000471168 run.py:483] Algo bellman_ford step 3188 current loss 0.043172, current_train_items 102048.
I0304 19:29:46.720629 23128000471168 run.py:483] Algo bellman_ford step 3189 current loss 0.068062, current_train_items 102080.
I0304 19:29:46.740994 23128000471168 run.py:483] Algo bellman_ford step 3190 current loss 0.022273, current_train_items 102112.
I0304 19:29:46.757720 23128000471168 run.py:483] Algo bellman_ford step 3191 current loss 0.011820, current_train_items 102144.
I0304 19:29:46.782016 23128000471168 run.py:483] Algo bellman_ford step 3192 current loss 0.097236, current_train_items 102176.
I0304 19:29:46.812868 23128000471168 run.py:483] Algo bellman_ford step 3193 current loss 0.068574, current_train_items 102208.
I0304 19:29:46.847292 23128000471168 run.py:483] Algo bellman_ford step 3194 current loss 0.063780, current_train_items 102240.
I0304 19:29:46.867442 23128000471168 run.py:483] Algo bellman_ford step 3195 current loss 0.003557, current_train_items 102272.
I0304 19:29:46.883489 23128000471168 run.py:483] Algo bellman_ford step 3196 current loss 0.022185, current_train_items 102304.
I0304 19:29:46.907822 23128000471168 run.py:483] Algo bellman_ford step 3197 current loss 0.111869, current_train_items 102336.
I0304 19:29:46.939701 23128000471168 run.py:483] Algo bellman_ford step 3198 current loss 0.093704, current_train_items 102368.
I0304 19:29:46.973585 23128000471168 run.py:483] Algo bellman_ford step 3199 current loss 0.111466, current_train_items 102400.
I0304 19:29:46.993828 23128000471168 run.py:483] Algo bellman_ford step 3200 current loss 0.017577, current_train_items 102432.
I0304 19:29:47.001729 23128000471168 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0304 19:29:47.001839 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:29:47.018399 23128000471168 run.py:483] Algo bellman_ford step 3201 current loss 0.022293, current_train_items 102464.
I0304 19:29:47.043051 23128000471168 run.py:483] Algo bellman_ford step 3202 current loss 0.042373, current_train_items 102496.
I0304 19:29:47.076645 23128000471168 run.py:483] Algo bellman_ford step 3203 current loss 0.182781, current_train_items 102528.
I0304 19:29:47.111360 23128000471168 run.py:483] Algo bellman_ford step 3204 current loss 0.124292, current_train_items 102560.
I0304 19:29:47.131903 23128000471168 run.py:483] Algo bellman_ford step 3205 current loss 0.003107, current_train_items 102592.
I0304 19:29:47.147830 23128000471168 run.py:483] Algo bellman_ford step 3206 current loss 0.051627, current_train_items 102624.
I0304 19:29:47.170773 23128000471168 run.py:483] Algo bellman_ford step 3207 current loss 0.054127, current_train_items 102656.
I0304 19:29:47.203534 23128000471168 run.py:483] Algo bellman_ford step 3208 current loss 0.128498, current_train_items 102688.
I0304 19:29:47.238465 23128000471168 run.py:483] Algo bellman_ford step 3209 current loss 0.102678, current_train_items 102720.
I0304 19:29:47.258471 23128000471168 run.py:483] Algo bellman_ford step 3210 current loss 0.041823, current_train_items 102752.
I0304 19:29:47.274608 23128000471168 run.py:483] Algo bellman_ford step 3211 current loss 0.006105, current_train_items 102784.
I0304 19:29:47.298519 23128000471168 run.py:483] Algo bellman_ford step 3212 current loss 0.049621, current_train_items 102816.
I0304 19:29:47.330338 23128000471168 run.py:483] Algo bellman_ford step 3213 current loss 0.229011, current_train_items 102848.
I0304 19:29:47.364237 23128000471168 run.py:483] Algo bellman_ford step 3214 current loss 0.191855, current_train_items 102880.
I0304 19:29:47.384515 23128000471168 run.py:483] Algo bellman_ford step 3215 current loss 0.028677, current_train_items 102912.
I0304 19:29:47.400831 23128000471168 run.py:483] Algo bellman_ford step 3216 current loss 0.029476, current_train_items 102944.
I0304 19:29:47.425637 23128000471168 run.py:483] Algo bellman_ford step 3217 current loss 0.084679, current_train_items 102976.
I0304 19:29:47.456521 23128000471168 run.py:483] Algo bellman_ford step 3218 current loss 0.144758, current_train_items 103008.
I0304 19:29:47.489825 23128000471168 run.py:483] Algo bellman_ford step 3219 current loss 0.082090, current_train_items 103040.
I0304 19:29:47.509948 23128000471168 run.py:483] Algo bellman_ford step 3220 current loss 0.005428, current_train_items 103072.
I0304 19:29:47.526470 23128000471168 run.py:483] Algo bellman_ford step 3221 current loss 0.031471, current_train_items 103104.
I0304 19:29:47.551109 23128000471168 run.py:483] Algo bellman_ford step 3222 current loss 0.095679, current_train_items 103136.
I0304 19:29:47.583313 23128000471168 run.py:483] Algo bellman_ford step 3223 current loss 0.065975, current_train_items 103168.
I0304 19:29:47.615882 23128000471168 run.py:483] Algo bellman_ford step 3224 current loss 0.068374, current_train_items 103200.
I0304 19:29:47.635802 23128000471168 run.py:483] Algo bellman_ford step 3225 current loss 0.016064, current_train_items 103232.
I0304 19:29:47.651565 23128000471168 run.py:483] Algo bellman_ford step 3226 current loss 0.025511, current_train_items 103264.
I0304 19:29:47.676087 23128000471168 run.py:483] Algo bellman_ford step 3227 current loss 0.034210, current_train_items 103296.
I0304 19:29:47.708130 23128000471168 run.py:483] Algo bellman_ford step 3228 current loss 0.090679, current_train_items 103328.
I0304 19:29:47.745916 23128000471168 run.py:483] Algo bellman_ford step 3229 current loss 0.124851, current_train_items 103360.
I0304 19:29:47.766122 23128000471168 run.py:483] Algo bellman_ford step 3230 current loss 0.008241, current_train_items 103392.
I0304 19:29:47.782652 23128000471168 run.py:483] Algo bellman_ford step 3231 current loss 0.039880, current_train_items 103424.
I0304 19:29:47.806301 23128000471168 run.py:483] Algo bellman_ford step 3232 current loss 0.066824, current_train_items 103456.
I0304 19:29:47.838366 23128000471168 run.py:483] Algo bellman_ford step 3233 current loss 0.064613, current_train_items 103488.
I0304 19:29:47.873443 23128000471168 run.py:483] Algo bellman_ford step 3234 current loss 0.072963, current_train_items 103520.
I0304 19:29:47.893338 23128000471168 run.py:483] Algo bellman_ford step 3235 current loss 0.019218, current_train_items 103552.
I0304 19:29:47.909905 23128000471168 run.py:483] Algo bellman_ford step 3236 current loss 0.022644, current_train_items 103584.
I0304 19:29:47.933526 23128000471168 run.py:483] Algo bellman_ford step 3237 current loss 0.057445, current_train_items 103616.
I0304 19:29:47.966104 23128000471168 run.py:483] Algo bellman_ford step 3238 current loss 0.057294, current_train_items 103648.
I0304 19:29:48.000133 23128000471168 run.py:483] Algo bellman_ford step 3239 current loss 0.074088, current_train_items 103680.
I0304 19:29:48.020231 23128000471168 run.py:483] Algo bellman_ford step 3240 current loss 0.015506, current_train_items 103712.
I0304 19:29:48.036890 23128000471168 run.py:483] Algo bellman_ford step 3241 current loss 0.022845, current_train_items 103744.
I0304 19:29:48.060910 23128000471168 run.py:483] Algo bellman_ford step 3242 current loss 0.046188, current_train_items 103776.
I0304 19:29:48.091598 23128000471168 run.py:483] Algo bellman_ford step 3243 current loss 0.061110, current_train_items 103808.
I0304 19:29:48.126494 23128000471168 run.py:483] Algo bellman_ford step 3244 current loss 0.096948, current_train_items 103840.
I0304 19:29:48.146623 23128000471168 run.py:483] Algo bellman_ford step 3245 current loss 0.007584, current_train_items 103872.
I0304 19:29:48.163409 23128000471168 run.py:483] Algo bellman_ford step 3246 current loss 0.012667, current_train_items 103904.
I0304 19:29:48.187330 23128000471168 run.py:483] Algo bellman_ford step 3247 current loss 0.070238, current_train_items 103936.
I0304 19:29:48.219383 23128000471168 run.py:483] Algo bellman_ford step 3248 current loss 0.044031, current_train_items 103968.
I0304 19:29:48.251991 23128000471168 run.py:483] Algo bellman_ford step 3249 current loss 0.033260, current_train_items 104000.
I0304 19:29:48.272286 23128000471168 run.py:483] Algo bellman_ford step 3250 current loss 0.008257, current_train_items 104032.
I0304 19:29:48.280427 23128000471168 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0304 19:29:48.280535 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:48.297535 23128000471168 run.py:483] Algo bellman_ford step 3251 current loss 0.064658, current_train_items 104064.
I0304 19:29:48.320519 23128000471168 run.py:483] Algo bellman_ford step 3252 current loss 0.023535, current_train_items 104096.
I0304 19:29:48.353851 23128000471168 run.py:483] Algo bellman_ford step 3253 current loss 0.074865, current_train_items 104128.
I0304 19:29:48.388645 23128000471168 run.py:483] Algo bellman_ford step 3254 current loss 0.060130, current_train_items 104160.
I0304 19:29:48.409420 23128000471168 run.py:483] Algo bellman_ford step 3255 current loss 0.014213, current_train_items 104192.
I0304 19:29:48.425501 23128000471168 run.py:483] Algo bellman_ford step 3256 current loss 0.031142, current_train_items 104224.
I0304 19:29:48.449316 23128000471168 run.py:483] Algo bellman_ford step 3257 current loss 0.028521, current_train_items 104256.
I0304 19:29:48.482145 23128000471168 run.py:483] Algo bellman_ford step 3258 current loss 0.114902, current_train_items 104288.
I0304 19:29:48.517700 23128000471168 run.py:483] Algo bellman_ford step 3259 current loss 0.080985, current_train_items 104320.
I0304 19:29:48.537860 23128000471168 run.py:483] Algo bellman_ford step 3260 current loss 0.003068, current_train_items 104352.
I0304 19:29:48.554430 23128000471168 run.py:483] Algo bellman_ford step 3261 current loss 0.008175, current_train_items 104384.
I0304 19:29:48.578045 23128000471168 run.py:483] Algo bellman_ford step 3262 current loss 0.057141, current_train_items 104416.
I0304 19:29:48.609976 23128000471168 run.py:483] Algo bellman_ford step 3263 current loss 0.091856, current_train_items 104448.
I0304 19:29:48.644699 23128000471168 run.py:483] Algo bellman_ford step 3264 current loss 0.072784, current_train_items 104480.
I0304 19:29:48.664653 23128000471168 run.py:483] Algo bellman_ford step 3265 current loss 0.030502, current_train_items 104512.
I0304 19:29:48.681337 23128000471168 run.py:483] Algo bellman_ford step 3266 current loss 0.061709, current_train_items 104544.
I0304 19:29:48.705224 23128000471168 run.py:483] Algo bellman_ford step 3267 current loss 0.138867, current_train_items 104576.
I0304 19:29:48.735400 23128000471168 run.py:483] Algo bellman_ford step 3268 current loss 0.152531, current_train_items 104608.
I0304 19:29:48.771540 23128000471168 run.py:483] Algo bellman_ford step 3269 current loss 0.226269, current_train_items 104640.
I0304 19:29:48.792096 23128000471168 run.py:483] Algo bellman_ford step 3270 current loss 0.005824, current_train_items 104672.
I0304 19:29:48.808751 23128000471168 run.py:483] Algo bellman_ford step 3271 current loss 0.033732, current_train_items 104704.
I0304 19:29:48.832937 23128000471168 run.py:483] Algo bellman_ford step 3272 current loss 0.059789, current_train_items 104736.
I0304 19:29:48.863463 23128000471168 run.py:483] Algo bellman_ford step 3273 current loss 0.101508, current_train_items 104768.
I0304 19:29:48.895569 23128000471168 run.py:483] Algo bellman_ford step 3274 current loss 0.042509, current_train_items 104800.
I0304 19:29:48.916054 23128000471168 run.py:483] Algo bellman_ford step 3275 current loss 0.004608, current_train_items 104832.
I0304 19:29:48.932397 23128000471168 run.py:483] Algo bellman_ford step 3276 current loss 0.019705, current_train_items 104864.
I0304 19:29:48.955824 23128000471168 run.py:483] Algo bellman_ford step 3277 current loss 0.045045, current_train_items 104896.
I0304 19:29:48.988434 23128000471168 run.py:483] Algo bellman_ford step 3278 current loss 0.101085, current_train_items 104928.
I0304 19:29:49.023231 23128000471168 run.py:483] Algo bellman_ford step 3279 current loss 0.101946, current_train_items 104960.
I0304 19:29:49.043294 23128000471168 run.py:483] Algo bellman_ford step 3280 current loss 0.004790, current_train_items 104992.
I0304 19:29:49.059514 23128000471168 run.py:483] Algo bellman_ford step 3281 current loss 0.014796, current_train_items 105024.
I0304 19:29:49.083154 23128000471168 run.py:483] Algo bellman_ford step 3282 current loss 0.043163, current_train_items 105056.
I0304 19:29:49.115412 23128000471168 run.py:483] Algo bellman_ford step 3283 current loss 0.100105, current_train_items 105088.
I0304 19:29:49.150088 23128000471168 run.py:483] Algo bellman_ford step 3284 current loss 0.089866, current_train_items 105120.
I0304 19:29:49.170517 23128000471168 run.py:483] Algo bellman_ford step 3285 current loss 0.004066, current_train_items 105152.
I0304 19:29:49.187391 23128000471168 run.py:483] Algo bellman_ford step 3286 current loss 0.030134, current_train_items 105184.
I0304 19:29:49.211764 23128000471168 run.py:483] Algo bellman_ford step 3287 current loss 0.065137, current_train_items 105216.
I0304 19:29:49.242115 23128000471168 run.py:483] Algo bellman_ford step 3288 current loss 0.024723, current_train_items 105248.
I0304 19:29:49.274565 23128000471168 run.py:483] Algo bellman_ford step 3289 current loss 0.081436, current_train_items 105280.
I0304 19:29:49.295210 23128000471168 run.py:483] Algo bellman_ford step 3290 current loss 0.007817, current_train_items 105312.
I0304 19:29:49.312269 23128000471168 run.py:483] Algo bellman_ford step 3291 current loss 0.015219, current_train_items 105344.
I0304 19:29:49.336141 23128000471168 run.py:483] Algo bellman_ford step 3292 current loss 0.054523, current_train_items 105376.
I0304 19:29:49.367931 23128000471168 run.py:483] Algo bellman_ford step 3293 current loss 0.090741, current_train_items 105408.
I0304 19:29:49.402258 23128000471168 run.py:483] Algo bellman_ford step 3294 current loss 0.075119, current_train_items 105440.
I0304 19:29:49.422316 23128000471168 run.py:483] Algo bellman_ford step 3295 current loss 0.035808, current_train_items 105472.
I0304 19:29:49.438431 23128000471168 run.py:483] Algo bellman_ford step 3296 current loss 0.005831, current_train_items 105504.
I0304 19:29:49.463539 23128000471168 run.py:483] Algo bellman_ford step 3297 current loss 0.144162, current_train_items 105536.
I0304 19:29:49.496897 23128000471168 run.py:483] Algo bellman_ford step 3298 current loss 0.237626, current_train_items 105568.
I0304 19:29:49.532189 23128000471168 run.py:483] Algo bellman_ford step 3299 current loss 0.157011, current_train_items 105600.
I0304 19:29:49.552576 23128000471168 run.py:483] Algo bellman_ford step 3300 current loss 0.015373, current_train_items 105632.
I0304 19:29:49.560533 23128000471168 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0304 19:29:49.560642 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:29:49.577632 23128000471168 run.py:483] Algo bellman_ford step 3301 current loss 0.030513, current_train_items 105664.
I0304 19:29:49.601758 23128000471168 run.py:483] Algo bellman_ford step 3302 current loss 0.090290, current_train_items 105696.
I0304 19:29:49.632683 23128000471168 run.py:483] Algo bellman_ford step 3303 current loss 0.067209, current_train_items 105728.
I0304 19:29:49.664974 23128000471168 run.py:483] Algo bellman_ford step 3304 current loss 0.040702, current_train_items 105760.
I0304 19:29:49.685642 23128000471168 run.py:483] Algo bellman_ford step 3305 current loss 0.004030, current_train_items 105792.
I0304 19:29:49.702084 23128000471168 run.py:483] Algo bellman_ford step 3306 current loss 0.027454, current_train_items 105824.
I0304 19:29:49.726647 23128000471168 run.py:483] Algo bellman_ford step 3307 current loss 0.038669, current_train_items 105856.
I0304 19:29:49.758188 23128000471168 run.py:483] Algo bellman_ford step 3308 current loss 0.045344, current_train_items 105888.
I0304 19:29:49.793632 23128000471168 run.py:483] Algo bellman_ford step 3309 current loss 0.105126, current_train_items 105920.
I0304 19:29:49.813582 23128000471168 run.py:483] Algo bellman_ford step 3310 current loss 0.020774, current_train_items 105952.
I0304 19:29:49.830258 23128000471168 run.py:483] Algo bellman_ford step 3311 current loss 0.048969, current_train_items 105984.
I0304 19:29:49.854316 23128000471168 run.py:483] Algo bellman_ford step 3312 current loss 0.042069, current_train_items 106016.
I0304 19:29:49.886869 23128000471168 run.py:483] Algo bellman_ford step 3313 current loss 0.088550, current_train_items 106048.
I0304 19:29:49.919452 23128000471168 run.py:483] Algo bellman_ford step 3314 current loss 0.090679, current_train_items 106080.
I0304 19:29:49.939096 23128000471168 run.py:483] Algo bellman_ford step 3315 current loss 0.002410, current_train_items 106112.
I0304 19:29:49.955847 23128000471168 run.py:483] Algo bellman_ford step 3316 current loss 0.017996, current_train_items 106144.
I0304 19:29:49.979629 23128000471168 run.py:483] Algo bellman_ford step 3317 current loss 0.032704, current_train_items 106176.
I0304 19:29:50.010925 23128000471168 run.py:483] Algo bellman_ford step 3318 current loss 0.049643, current_train_items 106208.
I0304 19:29:50.045471 23128000471168 run.py:483] Algo bellman_ford step 3319 current loss 0.112723, current_train_items 106240.
I0304 19:29:50.065357 23128000471168 run.py:483] Algo bellman_ford step 3320 current loss 0.003021, current_train_items 106272.
I0304 19:29:50.081830 23128000471168 run.py:483] Algo bellman_ford step 3321 current loss 0.015138, current_train_items 106304.
I0304 19:29:50.106614 23128000471168 run.py:483] Algo bellman_ford step 3322 current loss 0.042280, current_train_items 106336.
I0304 19:29:50.138369 23128000471168 run.py:483] Algo bellman_ford step 3323 current loss 0.052562, current_train_items 106368.
I0304 19:29:50.171600 23128000471168 run.py:483] Algo bellman_ford step 3324 current loss 0.062128, current_train_items 106400.
I0304 19:29:50.191294 23128000471168 run.py:483] Algo bellman_ford step 3325 current loss 0.002457, current_train_items 106432.
I0304 19:29:50.208057 23128000471168 run.py:483] Algo bellman_ford step 3326 current loss 0.025709, current_train_items 106464.
I0304 19:29:50.231688 23128000471168 run.py:483] Algo bellman_ford step 3327 current loss 0.034050, current_train_items 106496.
I0304 19:29:50.262459 23128000471168 run.py:483] Algo bellman_ford step 3328 current loss 0.035649, current_train_items 106528.
I0304 19:29:50.296422 23128000471168 run.py:483] Algo bellman_ford step 3329 current loss 0.075576, current_train_items 106560.
I0304 19:29:50.316430 23128000471168 run.py:483] Algo bellman_ford step 3330 current loss 0.090650, current_train_items 106592.
I0304 19:29:50.332736 23128000471168 run.py:483] Algo bellman_ford step 3331 current loss 0.026552, current_train_items 106624.
I0304 19:29:50.356515 23128000471168 run.py:483] Algo bellman_ford step 3332 current loss 0.047689, current_train_items 106656.
I0304 19:29:50.387446 23128000471168 run.py:483] Algo bellman_ford step 3333 current loss 0.043984, current_train_items 106688.
I0304 19:29:50.420017 23128000471168 run.py:483] Algo bellman_ford step 3334 current loss 0.047056, current_train_items 106720.
I0304 19:29:50.439548 23128000471168 run.py:483] Algo bellman_ford step 3335 current loss 0.003895, current_train_items 106752.
I0304 19:29:50.455744 23128000471168 run.py:483] Algo bellman_ford step 3336 current loss 0.033895, current_train_items 106784.
I0304 19:29:50.480149 23128000471168 run.py:483] Algo bellman_ford step 3337 current loss 0.062768, current_train_items 106816.
I0304 19:29:50.512531 23128000471168 run.py:483] Algo bellman_ford step 3338 current loss 0.059573, current_train_items 106848.
I0304 19:29:50.547848 23128000471168 run.py:483] Algo bellman_ford step 3339 current loss 0.050570, current_train_items 106880.
I0304 19:29:50.567517 23128000471168 run.py:483] Algo bellman_ford step 3340 current loss 0.002339, current_train_items 106912.
I0304 19:29:50.584392 23128000471168 run.py:483] Algo bellman_ford step 3341 current loss 0.019560, current_train_items 106944.
I0304 19:29:50.608619 23128000471168 run.py:483] Algo bellman_ford step 3342 current loss 0.047043, current_train_items 106976.
I0304 19:29:50.640807 23128000471168 run.py:483] Algo bellman_ford step 3343 current loss 0.096603, current_train_items 107008.
I0304 19:29:50.672775 23128000471168 run.py:483] Algo bellman_ford step 3344 current loss 0.066144, current_train_items 107040.
I0304 19:29:50.692796 23128000471168 run.py:483] Algo bellman_ford step 3345 current loss 0.019716, current_train_items 107072.
I0304 19:29:50.708907 23128000471168 run.py:483] Algo bellman_ford step 3346 current loss 0.015648, current_train_items 107104.
I0304 19:29:50.733453 23128000471168 run.py:483] Algo bellman_ford step 3347 current loss 0.033255, current_train_items 107136.
I0304 19:29:50.764242 23128000471168 run.py:483] Algo bellman_ford step 3348 current loss 0.043640, current_train_items 107168.
I0304 19:29:50.795959 23128000471168 run.py:483] Algo bellman_ford step 3349 current loss 0.074015, current_train_items 107200.
I0304 19:29:50.815968 23128000471168 run.py:483] Algo bellman_ford step 3350 current loss 0.017307, current_train_items 107232.
I0304 19:29:50.823924 23128000471168 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0304 19:29:50.824041 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:29:50.841858 23128000471168 run.py:483] Algo bellman_ford step 3351 current loss 0.033410, current_train_items 107264.
I0304 19:29:50.866831 23128000471168 run.py:483] Algo bellman_ford step 3352 current loss 0.069611, current_train_items 107296.
I0304 19:29:50.899463 23128000471168 run.py:483] Algo bellman_ford step 3353 current loss 0.050504, current_train_items 107328.
I0304 19:29:50.933845 23128000471168 run.py:483] Algo bellman_ford step 3354 current loss 0.068824, current_train_items 107360.
I0304 19:29:50.953741 23128000471168 run.py:483] Algo bellman_ford step 3355 current loss 0.002344, current_train_items 107392.
I0304 19:29:50.969700 23128000471168 run.py:483] Algo bellman_ford step 3356 current loss 0.040731, current_train_items 107424.
I0304 19:29:50.993890 23128000471168 run.py:483] Algo bellman_ford step 3357 current loss 0.050915, current_train_items 107456.
I0304 19:29:51.025240 23128000471168 run.py:483] Algo bellman_ford step 3358 current loss 0.055728, current_train_items 107488.
I0304 19:29:51.060416 23128000471168 run.py:483] Algo bellman_ford step 3359 current loss 0.079144, current_train_items 107520.
I0304 19:29:51.080555 23128000471168 run.py:483] Algo bellman_ford step 3360 current loss 0.003687, current_train_items 107552.
I0304 19:29:51.097038 23128000471168 run.py:483] Algo bellman_ford step 3361 current loss 0.011000, current_train_items 107584.
I0304 19:29:51.121386 23128000471168 run.py:483] Algo bellman_ford step 3362 current loss 0.112252, current_train_items 107616.
I0304 19:29:51.152365 23128000471168 run.py:483] Algo bellman_ford step 3363 current loss 0.068014, current_train_items 107648.
I0304 19:29:51.185351 23128000471168 run.py:483] Algo bellman_ford step 3364 current loss 0.119530, current_train_items 107680.
I0304 19:29:51.205088 23128000471168 run.py:483] Algo bellman_ford step 3365 current loss 0.024885, current_train_items 107712.
I0304 19:29:51.221104 23128000471168 run.py:483] Algo bellman_ford step 3366 current loss 0.020286, current_train_items 107744.
I0304 19:29:51.245707 23128000471168 run.py:483] Algo bellman_ford step 3367 current loss 0.096436, current_train_items 107776.
I0304 19:29:51.275938 23128000471168 run.py:483] Algo bellman_ford step 3368 current loss 0.072781, current_train_items 107808.
I0304 19:29:51.308851 23128000471168 run.py:483] Algo bellman_ford step 3369 current loss 0.075395, current_train_items 107840.
I0304 19:29:51.329095 23128000471168 run.py:483] Algo bellman_ford step 3370 current loss 0.006218, current_train_items 107872.
I0304 19:29:51.345899 23128000471168 run.py:483] Algo bellman_ford step 3371 current loss 0.039699, current_train_items 107904.
I0304 19:29:51.369636 23128000471168 run.py:483] Algo bellman_ford step 3372 current loss 0.089953, current_train_items 107936.
I0304 19:29:51.402511 23128000471168 run.py:483] Algo bellman_ford step 3373 current loss 0.167107, current_train_items 107968.
I0304 19:29:51.435566 23128000471168 run.py:483] Algo bellman_ford step 3374 current loss 0.222785, current_train_items 108000.
I0304 19:29:51.455753 23128000471168 run.py:483] Algo bellman_ford step 3375 current loss 0.010672, current_train_items 108032.
I0304 19:29:51.471986 23128000471168 run.py:483] Algo bellman_ford step 3376 current loss 0.021109, current_train_items 108064.
I0304 19:29:51.495258 23128000471168 run.py:483] Algo bellman_ford step 3377 current loss 0.106102, current_train_items 108096.
I0304 19:29:51.525528 23128000471168 run.py:483] Algo bellman_ford step 3378 current loss 0.068845, current_train_items 108128.
I0304 19:29:51.560318 23128000471168 run.py:483] Algo bellman_ford step 3379 current loss 0.100130, current_train_items 108160.
I0304 19:29:51.579996 23128000471168 run.py:483] Algo bellman_ford step 3380 current loss 0.005318, current_train_items 108192.
I0304 19:29:51.596781 23128000471168 run.py:483] Algo bellman_ford step 3381 current loss 0.053416, current_train_items 108224.
I0304 19:29:51.621807 23128000471168 run.py:483] Algo bellman_ford step 3382 current loss 0.057486, current_train_items 108256.
I0304 19:29:51.652289 23128000471168 run.py:483] Algo bellman_ford step 3383 current loss 0.182368, current_train_items 108288.
I0304 19:29:51.687679 23128000471168 run.py:483] Algo bellman_ford step 3384 current loss 0.208063, current_train_items 108320.
I0304 19:29:51.707807 23128000471168 run.py:483] Algo bellman_ford step 3385 current loss 0.004310, current_train_items 108352.
I0304 19:29:51.724656 23128000471168 run.py:483] Algo bellman_ford step 3386 current loss 0.018348, current_train_items 108384.
I0304 19:29:51.748410 23128000471168 run.py:483] Algo bellman_ford step 3387 current loss 0.049739, current_train_items 108416.
I0304 19:29:51.779394 23128000471168 run.py:483] Algo bellman_ford step 3388 current loss 0.105360, current_train_items 108448.
I0304 19:29:51.811592 23128000471168 run.py:483] Algo bellman_ford step 3389 current loss 0.219310, current_train_items 108480.
I0304 19:29:51.831693 23128000471168 run.py:483] Algo bellman_ford step 3390 current loss 0.008095, current_train_items 108512.
I0304 19:29:51.848129 23128000471168 run.py:483] Algo bellman_ford step 3391 current loss 0.014213, current_train_items 108544.
I0304 19:29:51.871983 23128000471168 run.py:483] Algo bellman_ford step 3392 current loss 0.045685, current_train_items 108576.
I0304 19:29:51.902569 23128000471168 run.py:483] Algo bellman_ford step 3393 current loss 0.052706, current_train_items 108608.
I0304 19:29:51.934592 23128000471168 run.py:483] Algo bellman_ford step 3394 current loss 0.053854, current_train_items 108640.
I0304 19:29:51.954508 23128000471168 run.py:483] Algo bellman_ford step 3395 current loss 0.003570, current_train_items 108672.
I0304 19:29:51.971134 23128000471168 run.py:483] Algo bellman_ford step 3396 current loss 0.021738, current_train_items 108704.
I0304 19:29:51.995147 23128000471168 run.py:483] Algo bellman_ford step 3397 current loss 0.046158, current_train_items 108736.
I0304 19:29:52.026895 23128000471168 run.py:483] Algo bellman_ford step 3398 current loss 0.089825, current_train_items 108768.
I0304 19:29:52.059796 23128000471168 run.py:483] Algo bellman_ford step 3399 current loss 0.078440, current_train_items 108800.
I0304 19:29:52.080201 23128000471168 run.py:483] Algo bellman_ford step 3400 current loss 0.002993, current_train_items 108832.
I0304 19:29:52.087896 23128000471168 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0304 19:29:52.088012 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:52.105492 23128000471168 run.py:483] Algo bellman_ford step 3401 current loss 0.022338, current_train_items 108864.
I0304 19:29:52.130441 23128000471168 run.py:483] Algo bellman_ford step 3402 current loss 0.041787, current_train_items 108896.
I0304 19:29:52.162701 23128000471168 run.py:483] Algo bellman_ford step 3403 current loss 0.109809, current_train_items 108928.
I0304 19:29:52.198308 23128000471168 run.py:483] Algo bellman_ford step 3404 current loss 0.127254, current_train_items 108960.
I0304 19:29:52.218593 23128000471168 run.py:483] Algo bellman_ford step 3405 current loss 0.001617, current_train_items 108992.
I0304 19:29:52.234455 23128000471168 run.py:483] Algo bellman_ford step 3406 current loss 0.040304, current_train_items 109024.
I0304 19:29:52.259016 23128000471168 run.py:483] Algo bellman_ford step 3407 current loss 0.072712, current_train_items 109056.
I0304 19:29:52.292232 23128000471168 run.py:483] Algo bellman_ford step 3408 current loss 0.086163, current_train_items 109088.
I0304 19:29:52.325795 23128000471168 run.py:483] Algo bellman_ford step 3409 current loss 0.068221, current_train_items 109120.
I0304 19:29:52.345646 23128000471168 run.py:483] Algo bellman_ford step 3410 current loss 0.001952, current_train_items 109152.
I0304 19:29:52.362457 23128000471168 run.py:483] Algo bellman_ford step 3411 current loss 0.048099, current_train_items 109184.
I0304 19:29:52.387400 23128000471168 run.py:483] Algo bellman_ford step 3412 current loss 0.052342, current_train_items 109216.
I0304 19:29:52.420340 23128000471168 run.py:483] Algo bellman_ford step 3413 current loss 0.092366, current_train_items 109248.
I0304 19:29:52.456791 23128000471168 run.py:483] Algo bellman_ford step 3414 current loss 0.091020, current_train_items 109280.
I0304 19:29:52.477084 23128000471168 run.py:483] Algo bellman_ford step 3415 current loss 0.010387, current_train_items 109312.
I0304 19:29:52.493603 23128000471168 run.py:483] Algo bellman_ford step 3416 current loss 0.020129, current_train_items 109344.
I0304 19:29:52.518215 23128000471168 run.py:483] Algo bellman_ford step 3417 current loss 0.046831, current_train_items 109376.
I0304 19:29:52.549912 23128000471168 run.py:483] Algo bellman_ford step 3418 current loss 0.136153, current_train_items 109408.
I0304 19:29:52.583200 23128000471168 run.py:483] Algo bellman_ford step 3419 current loss 0.093081, current_train_items 109440.
I0304 19:29:52.603434 23128000471168 run.py:483] Algo bellman_ford step 3420 current loss 0.006175, current_train_items 109472.
I0304 19:29:52.619706 23128000471168 run.py:483] Algo bellman_ford step 3421 current loss 0.021173, current_train_items 109504.
I0304 19:29:52.644369 23128000471168 run.py:483] Algo bellman_ford step 3422 current loss 0.041611, current_train_items 109536.
I0304 19:29:52.676305 23128000471168 run.py:483] Algo bellman_ford step 3423 current loss 0.039392, current_train_items 109568.
I0304 19:29:52.709947 23128000471168 run.py:483] Algo bellman_ford step 3424 current loss 0.085954, current_train_items 109600.
I0304 19:29:52.730140 23128000471168 run.py:483] Algo bellman_ford step 3425 current loss 0.004522, current_train_items 109632.
I0304 19:29:52.747103 23128000471168 run.py:483] Algo bellman_ford step 3426 current loss 0.023618, current_train_items 109664.
I0304 19:29:52.770953 23128000471168 run.py:483] Algo bellman_ford step 3427 current loss 0.039161, current_train_items 109696.
I0304 19:29:52.802973 23128000471168 run.py:483] Algo bellman_ford step 3428 current loss 0.061395, current_train_items 109728.
I0304 19:29:52.834301 23128000471168 run.py:483] Algo bellman_ford step 3429 current loss 0.057553, current_train_items 109760.
I0304 19:29:52.854552 23128000471168 run.py:483] Algo bellman_ford step 3430 current loss 0.004614, current_train_items 109792.
I0304 19:29:52.871270 23128000471168 run.py:483] Algo bellman_ford step 3431 current loss 0.004754, current_train_items 109824.
I0304 19:29:52.895260 23128000471168 run.py:483] Algo bellman_ford step 3432 current loss 0.034075, current_train_items 109856.
I0304 19:29:52.928798 23128000471168 run.py:483] Algo bellman_ford step 3433 current loss 0.099692, current_train_items 109888.
I0304 19:29:52.961859 23128000471168 run.py:483] Algo bellman_ford step 3434 current loss 0.141876, current_train_items 109920.
I0304 19:29:52.981857 23128000471168 run.py:483] Algo bellman_ford step 3435 current loss 0.002096, current_train_items 109952.
I0304 19:29:52.998331 23128000471168 run.py:483] Algo bellman_ford step 3436 current loss 0.029970, current_train_items 109984.
I0304 19:29:53.022836 23128000471168 run.py:483] Algo bellman_ford step 3437 current loss 0.040097, current_train_items 110016.
I0304 19:29:53.053377 23128000471168 run.py:483] Algo bellman_ford step 3438 current loss 0.039298, current_train_items 110048.
I0304 19:29:53.085540 23128000471168 run.py:483] Algo bellman_ford step 3439 current loss 0.056020, current_train_items 110080.
I0304 19:29:53.105421 23128000471168 run.py:483] Algo bellman_ford step 3440 current loss 0.024107, current_train_items 110112.
I0304 19:29:53.122202 23128000471168 run.py:483] Algo bellman_ford step 3441 current loss 0.025192, current_train_items 110144.
I0304 19:29:53.145809 23128000471168 run.py:483] Algo bellman_ford step 3442 current loss 0.057771, current_train_items 110176.
I0304 19:29:53.178286 23128000471168 run.py:483] Algo bellman_ford step 3443 current loss 0.107896, current_train_items 110208.
I0304 19:29:53.213195 23128000471168 run.py:483] Algo bellman_ford step 3444 current loss 0.112335, current_train_items 110240.
I0304 19:29:53.233450 23128000471168 run.py:483] Algo bellman_ford step 3445 current loss 0.002435, current_train_items 110272.
I0304 19:29:53.250217 23128000471168 run.py:483] Algo bellman_ford step 3446 current loss 0.025819, current_train_items 110304.
I0304 19:29:53.274582 23128000471168 run.py:483] Algo bellman_ford step 3447 current loss 0.063975, current_train_items 110336.
I0304 19:29:53.305894 23128000471168 run.py:483] Algo bellman_ford step 3448 current loss 0.031376, current_train_items 110368.
I0304 19:29:53.340136 23128000471168 run.py:483] Algo bellman_ford step 3449 current loss 0.062715, current_train_items 110400.
I0304 19:29:53.360419 23128000471168 run.py:483] Algo bellman_ford step 3450 current loss 0.008711, current_train_items 110432.
I0304 19:29:53.368679 23128000471168 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0304 19:29:53.368787 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:29:53.385840 23128000471168 run.py:483] Algo bellman_ford step 3451 current loss 0.022344, current_train_items 110464.
I0304 19:29:53.410951 23128000471168 run.py:483] Algo bellman_ford step 3452 current loss 0.058972, current_train_items 110496.
I0304 19:29:53.441921 23128000471168 run.py:483] Algo bellman_ford step 3453 current loss 0.060552, current_train_items 110528.
I0304 19:29:53.478474 23128000471168 run.py:483] Algo bellman_ford step 3454 current loss 0.088864, current_train_items 110560.
I0304 19:29:53.498899 23128000471168 run.py:483] Algo bellman_ford step 3455 current loss 0.002604, current_train_items 110592.
I0304 19:29:53.514810 23128000471168 run.py:483] Algo bellman_ford step 3456 current loss 0.036772, current_train_items 110624.
I0304 19:29:53.539052 23128000471168 run.py:483] Algo bellman_ford step 3457 current loss 0.054838, current_train_items 110656.
I0304 19:29:53.571815 23128000471168 run.py:483] Algo bellman_ford step 3458 current loss 0.085488, current_train_items 110688.
I0304 19:29:53.605104 23128000471168 run.py:483] Algo bellman_ford step 3459 current loss 0.095572, current_train_items 110720.
I0304 19:29:53.625469 23128000471168 run.py:483] Algo bellman_ford step 3460 current loss 0.017808, current_train_items 110752.
I0304 19:29:53.642308 23128000471168 run.py:483] Algo bellman_ford step 3461 current loss 0.071392, current_train_items 110784.
I0304 19:29:53.666234 23128000471168 run.py:483] Algo bellman_ford step 3462 current loss 0.069334, current_train_items 110816.
I0304 19:29:53.697377 23128000471168 run.py:483] Algo bellman_ford step 3463 current loss 0.064154, current_train_items 110848.
I0304 19:29:53.730514 23128000471168 run.py:483] Algo bellman_ford step 3464 current loss 0.117236, current_train_items 110880.
I0304 19:29:53.750585 23128000471168 run.py:483] Algo bellman_ford step 3465 current loss 0.028261, current_train_items 110912.
I0304 19:29:53.767391 23128000471168 run.py:483] Algo bellman_ford step 3466 current loss 0.033381, current_train_items 110944.
I0304 19:29:53.792180 23128000471168 run.py:483] Algo bellman_ford step 3467 current loss 0.077012, current_train_items 110976.
I0304 19:29:53.825160 23128000471168 run.py:483] Algo bellman_ford step 3468 current loss 0.103158, current_train_items 111008.
I0304 19:29:53.861390 23128000471168 run.py:483] Algo bellman_ford step 3469 current loss 0.122204, current_train_items 111040.
I0304 19:29:53.881890 23128000471168 run.py:483] Algo bellman_ford step 3470 current loss 0.002553, current_train_items 111072.
I0304 19:29:53.898945 23128000471168 run.py:483] Algo bellman_ford step 3471 current loss 0.015701, current_train_items 111104.
I0304 19:29:53.922699 23128000471168 run.py:483] Algo bellman_ford step 3472 current loss 0.078327, current_train_items 111136.
I0304 19:29:53.955335 23128000471168 run.py:483] Algo bellman_ford step 3473 current loss 0.077585, current_train_items 111168.
I0304 19:29:53.989034 23128000471168 run.py:483] Algo bellman_ford step 3474 current loss 0.072648, current_train_items 111200.
I0304 19:29:54.009317 23128000471168 run.py:483] Algo bellman_ford step 3475 current loss 0.002833, current_train_items 111232.
I0304 19:29:54.026043 23128000471168 run.py:483] Algo bellman_ford step 3476 current loss 0.039719, current_train_items 111264.
I0304 19:29:54.049258 23128000471168 run.py:483] Algo bellman_ford step 3477 current loss 0.066242, current_train_items 111296.
I0304 19:29:54.080168 23128000471168 run.py:483] Algo bellman_ford step 3478 current loss 0.049236, current_train_items 111328.
I0304 19:29:54.115293 23128000471168 run.py:483] Algo bellman_ford step 3479 current loss 0.125273, current_train_items 111360.
I0304 19:29:54.135221 23128000471168 run.py:483] Algo bellman_ford step 3480 current loss 0.004627, current_train_items 111392.
I0304 19:29:54.151499 23128000471168 run.py:483] Algo bellman_ford step 3481 current loss 0.028333, current_train_items 111424.
I0304 19:29:54.175685 23128000471168 run.py:483] Algo bellman_ford step 3482 current loss 0.035931, current_train_items 111456.
I0304 19:29:54.206568 23128000471168 run.py:483] Algo bellman_ford step 3483 current loss 0.120375, current_train_items 111488.
I0304 19:29:54.239873 23128000471168 run.py:483] Algo bellman_ford step 3484 current loss 0.095548, current_train_items 111520.
I0304 19:29:54.260322 23128000471168 run.py:483] Algo bellman_ford step 3485 current loss 0.003531, current_train_items 111552.
I0304 19:29:54.277017 23128000471168 run.py:483] Algo bellman_ford step 3486 current loss 0.024316, current_train_items 111584.
I0304 19:29:54.301755 23128000471168 run.py:483] Algo bellman_ford step 3487 current loss 0.069426, current_train_items 111616.
I0304 19:29:54.332730 23128000471168 run.py:483] Algo bellman_ford step 3488 current loss 0.081284, current_train_items 111648.
I0304 19:29:54.364891 23128000471168 run.py:483] Algo bellman_ford step 3489 current loss 0.105585, current_train_items 111680.
I0304 19:29:54.385201 23128000471168 run.py:483] Algo bellman_ford step 3490 current loss 0.003460, current_train_items 111712.
I0304 19:29:54.401567 23128000471168 run.py:483] Algo bellman_ford step 3491 current loss 0.024651, current_train_items 111744.
I0304 19:29:54.425385 23128000471168 run.py:483] Algo bellman_ford step 3492 current loss 0.042563, current_train_items 111776.
I0304 19:29:54.457915 23128000471168 run.py:483] Algo bellman_ford step 3493 current loss 0.077926, current_train_items 111808.
I0304 19:29:54.491133 23128000471168 run.py:483] Algo bellman_ford step 3494 current loss 0.077900, current_train_items 111840.
I0304 19:29:54.510913 23128000471168 run.py:483] Algo bellman_ford step 3495 current loss 0.001815, current_train_items 111872.
I0304 19:29:54.527540 23128000471168 run.py:483] Algo bellman_ford step 3496 current loss 0.055180, current_train_items 111904.
I0304 19:29:54.551326 23128000471168 run.py:483] Algo bellman_ford step 3497 current loss 0.058124, current_train_items 111936.
I0304 19:29:54.583151 23128000471168 run.py:483] Algo bellman_ford step 3498 current loss 0.090512, current_train_items 111968.
I0304 19:29:54.617808 23128000471168 run.py:483] Algo bellman_ford step 3499 current loss 0.068371, current_train_items 112000.
I0304 19:29:54.638336 23128000471168 run.py:483] Algo bellman_ford step 3500 current loss 0.003566, current_train_items 112032.
I0304 19:29:54.646243 23128000471168 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0304 19:29:54.646351 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:29:54.663105 23128000471168 run.py:483] Algo bellman_ford step 3501 current loss 0.024964, current_train_items 112064.
I0304 19:29:54.687703 23128000471168 run.py:483] Algo bellman_ford step 3502 current loss 0.192724, current_train_items 112096.
I0304 19:29:54.719612 23128000471168 run.py:483] Algo bellman_ford step 3503 current loss 0.141780, current_train_items 112128.
I0304 19:29:54.755553 23128000471168 run.py:483] Algo bellman_ford step 3504 current loss 0.179426, current_train_items 112160.
I0304 19:29:54.776245 23128000471168 run.py:483] Algo bellman_ford step 3505 current loss 0.032543, current_train_items 112192.
I0304 19:29:54.791821 23128000471168 run.py:483] Algo bellman_ford step 3506 current loss 0.025138, current_train_items 112224.
I0304 19:29:54.816593 23128000471168 run.py:483] Algo bellman_ford step 3507 current loss 0.042028, current_train_items 112256.
I0304 19:29:54.847180 23128000471168 run.py:483] Algo bellman_ford step 3508 current loss 0.094941, current_train_items 112288.
I0304 19:29:54.878363 23128000471168 run.py:483] Algo bellman_ford step 3509 current loss 0.094220, current_train_items 112320.
I0304 19:29:54.898690 23128000471168 run.py:483] Algo bellman_ford step 3510 current loss 0.015607, current_train_items 112352.
I0304 19:29:54.915076 23128000471168 run.py:483] Algo bellman_ford step 3511 current loss 0.020844, current_train_items 112384.
I0304 19:29:54.938604 23128000471168 run.py:483] Algo bellman_ford step 3512 current loss 0.030400, current_train_items 112416.
I0304 19:29:54.969328 23128000471168 run.py:483] Algo bellman_ford step 3513 current loss 0.039031, current_train_items 112448.
I0304 19:29:55.001944 23128000471168 run.py:483] Algo bellman_ford step 3514 current loss 0.069203, current_train_items 112480.
I0304 19:29:55.021723 23128000471168 run.py:483] Algo bellman_ford step 3515 current loss 0.003740, current_train_items 112512.
I0304 19:29:55.037686 23128000471168 run.py:483] Algo bellman_ford step 3516 current loss 0.006996, current_train_items 112544.
I0304 19:29:55.061752 23128000471168 run.py:483] Algo bellman_ford step 3517 current loss 0.038261, current_train_items 112576.
I0304 19:29:55.093544 23128000471168 run.py:483] Algo bellman_ford step 3518 current loss 0.055653, current_train_items 112608.
I0304 19:29:55.130111 23128000471168 run.py:483] Algo bellman_ford step 3519 current loss 0.095916, current_train_items 112640.
I0304 19:29:55.150019 23128000471168 run.py:483] Algo bellman_ford step 3520 current loss 0.005145, current_train_items 112672.
I0304 19:29:55.166321 23128000471168 run.py:483] Algo bellman_ford step 3521 current loss 0.024931, current_train_items 112704.
I0304 19:29:55.189396 23128000471168 run.py:483] Algo bellman_ford step 3522 current loss 0.025211, current_train_items 112736.
I0304 19:29:55.221138 23128000471168 run.py:483] Algo bellman_ford step 3523 current loss 0.081844, current_train_items 112768.
I0304 19:29:55.255010 23128000471168 run.py:483] Algo bellman_ford step 3524 current loss 0.153825, current_train_items 112800.
I0304 19:29:55.274705 23128000471168 run.py:483] Algo bellman_ford step 3525 current loss 0.063347, current_train_items 112832.
I0304 19:29:55.290915 23128000471168 run.py:483] Algo bellman_ford step 3526 current loss 0.019832, current_train_items 112864.
I0304 19:29:55.315169 23128000471168 run.py:483] Algo bellman_ford step 3527 current loss 0.053369, current_train_items 112896.
I0304 19:29:55.348119 23128000471168 run.py:483] Algo bellman_ford step 3528 current loss 0.073691, current_train_items 112928.
I0304 19:29:55.380950 23128000471168 run.py:483] Algo bellman_ford step 3529 current loss 0.075598, current_train_items 112960.
I0304 19:29:55.400810 23128000471168 run.py:483] Algo bellman_ford step 3530 current loss 0.004077, current_train_items 112992.
I0304 19:29:55.417500 23128000471168 run.py:483] Algo bellman_ford step 3531 current loss 0.016466, current_train_items 113024.
I0304 19:29:55.442733 23128000471168 run.py:483] Algo bellman_ford step 3532 current loss 0.048376, current_train_items 113056.
I0304 19:29:55.477119 23128000471168 run.py:483] Algo bellman_ford step 3533 current loss 0.077690, current_train_items 113088.
I0304 19:29:55.511312 23128000471168 run.py:483] Algo bellman_ford step 3534 current loss 0.099192, current_train_items 113120.
I0304 19:29:55.531250 23128000471168 run.py:483] Algo bellman_ford step 3535 current loss 0.017964, current_train_items 113152.
I0304 19:29:55.547811 23128000471168 run.py:483] Algo bellman_ford step 3536 current loss 0.046334, current_train_items 113184.
I0304 19:29:55.571699 23128000471168 run.py:483] Algo bellman_ford step 3537 current loss 0.053185, current_train_items 113216.
I0304 19:29:55.603938 23128000471168 run.py:483] Algo bellman_ford step 3538 current loss 0.102678, current_train_items 113248.
I0304 19:29:55.639749 23128000471168 run.py:483] Algo bellman_ford step 3539 current loss 0.058324, current_train_items 113280.
I0304 19:29:55.659538 23128000471168 run.py:483] Algo bellman_ford step 3540 current loss 0.004568, current_train_items 113312.
I0304 19:29:55.675995 23128000471168 run.py:483] Algo bellman_ford step 3541 current loss 0.011241, current_train_items 113344.
I0304 19:29:55.700503 23128000471168 run.py:483] Algo bellman_ford step 3542 current loss 0.081565, current_train_items 113376.
I0304 19:29:55.731459 23128000471168 run.py:483] Algo bellman_ford step 3543 current loss 0.066896, current_train_items 113408.
I0304 19:29:55.764820 23128000471168 run.py:483] Algo bellman_ford step 3544 current loss 0.067840, current_train_items 113440.
I0304 19:29:55.784606 23128000471168 run.py:483] Algo bellman_ford step 3545 current loss 0.004086, current_train_items 113472.
I0304 19:29:55.800816 23128000471168 run.py:483] Algo bellman_ford step 3546 current loss 0.007855, current_train_items 113504.
I0304 19:29:55.825040 23128000471168 run.py:483] Algo bellman_ford step 3547 current loss 0.106348, current_train_items 113536.
I0304 19:29:55.855767 23128000471168 run.py:483] Algo bellman_ford step 3548 current loss 0.062217, current_train_items 113568.
I0304 19:29:55.889312 23128000471168 run.py:483] Algo bellman_ford step 3549 current loss 0.095470, current_train_items 113600.
I0304 19:29:55.909272 23128000471168 run.py:483] Algo bellman_ford step 3550 current loss 0.007202, current_train_items 113632.
I0304 19:29:55.917225 23128000471168 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0304 19:29:55.917331 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:55.934838 23128000471168 run.py:483] Algo bellman_ford step 3551 current loss 0.038485, current_train_items 113664.
I0304 19:29:55.961376 23128000471168 run.py:483] Algo bellman_ford step 3552 current loss 0.083939, current_train_items 113696.
I0304 19:29:55.993255 23128000471168 run.py:483] Algo bellman_ford step 3553 current loss 0.048266, current_train_items 113728.
I0304 19:29:56.028053 23128000471168 run.py:483] Algo bellman_ford step 3554 current loss 0.062024, current_train_items 113760.
I0304 19:29:56.048706 23128000471168 run.py:483] Algo bellman_ford step 3555 current loss 0.012286, current_train_items 113792.
I0304 19:29:56.065053 23128000471168 run.py:483] Algo bellman_ford step 3556 current loss 0.011392, current_train_items 113824.
I0304 19:29:56.089495 23128000471168 run.py:483] Algo bellman_ford step 3557 current loss 0.059207, current_train_items 113856.
I0304 19:29:56.122041 23128000471168 run.py:483] Algo bellman_ford step 3558 current loss 0.052659, current_train_items 113888.
I0304 19:29:56.155478 23128000471168 run.py:483] Algo bellman_ford step 3559 current loss 0.085286, current_train_items 113920.
I0304 19:29:56.175707 23128000471168 run.py:483] Algo bellman_ford step 3560 current loss 0.002538, current_train_items 113952.
I0304 19:29:56.192478 23128000471168 run.py:483] Algo bellman_ford step 3561 current loss 0.023427, current_train_items 113984.
I0304 19:29:56.217489 23128000471168 run.py:483] Algo bellman_ford step 3562 current loss 0.089547, current_train_items 114016.
I0304 19:29:56.248483 23128000471168 run.py:483] Algo bellman_ford step 3563 current loss 0.074333, current_train_items 114048.
I0304 19:29:56.285237 23128000471168 run.py:483] Algo bellman_ford step 3564 current loss 0.097528, current_train_items 114080.
I0304 19:29:56.305526 23128000471168 run.py:483] Algo bellman_ford step 3565 current loss 0.002253, current_train_items 114112.
I0304 19:29:56.322149 23128000471168 run.py:483] Algo bellman_ford step 3566 current loss 0.014176, current_train_items 114144.
I0304 19:29:56.346366 23128000471168 run.py:483] Algo bellman_ford step 3567 current loss 0.041348, current_train_items 114176.
I0304 19:29:56.377716 23128000471168 run.py:483] Algo bellman_ford step 3568 current loss 0.040306, current_train_items 114208.
I0304 19:29:56.412187 23128000471168 run.py:483] Algo bellman_ford step 3569 current loss 0.074728, current_train_items 114240.
I0304 19:29:56.432578 23128000471168 run.py:483] Algo bellman_ford step 3570 current loss 0.002329, current_train_items 114272.
I0304 19:29:56.448909 23128000471168 run.py:483] Algo bellman_ford step 3571 current loss 0.014740, current_train_items 114304.
I0304 19:29:56.472771 23128000471168 run.py:483] Algo bellman_ford step 3572 current loss 0.072215, current_train_items 114336.
I0304 19:29:56.503268 23128000471168 run.py:483] Algo bellman_ford step 3573 current loss 0.054806, current_train_items 114368.
I0304 19:29:56.539235 23128000471168 run.py:483] Algo bellman_ford step 3574 current loss 0.065715, current_train_items 114400.
I0304 19:29:56.559978 23128000471168 run.py:483] Algo bellman_ford step 3575 current loss 0.003255, current_train_items 114432.
I0304 19:29:56.576797 23128000471168 run.py:483] Algo bellman_ford step 3576 current loss 0.013836, current_train_items 114464.
I0304 19:29:56.600807 23128000471168 run.py:483] Algo bellman_ford step 3577 current loss 0.081388, current_train_items 114496.
I0304 19:29:56.632369 23128000471168 run.py:483] Algo bellman_ford step 3578 current loss 0.101774, current_train_items 114528.
I0304 19:29:56.665162 23128000471168 run.py:483] Algo bellman_ford step 3579 current loss 0.075719, current_train_items 114560.
I0304 19:29:56.685329 23128000471168 run.py:483] Algo bellman_ford step 3580 current loss 0.003283, current_train_items 114592.
I0304 19:29:56.701848 23128000471168 run.py:483] Algo bellman_ford step 3581 current loss 0.089102, current_train_items 114624.
I0304 19:29:56.727240 23128000471168 run.py:483] Algo bellman_ford step 3582 current loss 0.073251, current_train_items 114656.
I0304 19:29:56.759600 23128000471168 run.py:483] Algo bellman_ford step 3583 current loss 0.078781, current_train_items 114688.
I0304 19:29:56.793776 23128000471168 run.py:483] Algo bellman_ford step 3584 current loss 0.074223, current_train_items 114720.
I0304 19:29:56.814279 23128000471168 run.py:483] Algo bellman_ford step 3585 current loss 0.001729, current_train_items 114752.
I0304 19:29:56.830914 23128000471168 run.py:483] Algo bellman_ford step 3586 current loss 0.046967, current_train_items 114784.
I0304 19:29:56.854740 23128000471168 run.py:483] Algo bellman_ford step 3587 current loss 0.076196, current_train_items 114816.
I0304 19:29:56.887415 23128000471168 run.py:483] Algo bellman_ford step 3588 current loss 0.080399, current_train_items 114848.
I0304 19:29:56.922613 23128000471168 run.py:483] Algo bellman_ford step 3589 current loss 0.061519, current_train_items 114880.
I0304 19:29:56.942728 23128000471168 run.py:483] Algo bellman_ford step 3590 current loss 0.003354, current_train_items 114912.
I0304 19:29:56.959083 23128000471168 run.py:483] Algo bellman_ford step 3591 current loss 0.017545, current_train_items 114944.
I0304 19:29:56.984750 23128000471168 run.py:483] Algo bellman_ford step 3592 current loss 0.056407, current_train_items 114976.
I0304 19:29:57.017738 23128000471168 run.py:483] Algo bellman_ford step 3593 current loss 0.324914, current_train_items 115008.
I0304 19:29:57.052650 23128000471168 run.py:483] Algo bellman_ford step 3594 current loss 0.145976, current_train_items 115040.
I0304 19:29:57.072733 23128000471168 run.py:483] Algo bellman_ford step 3595 current loss 0.004925, current_train_items 115072.
I0304 19:29:57.089663 23128000471168 run.py:483] Algo bellman_ford step 3596 current loss 0.071024, current_train_items 115104.
I0304 19:29:57.113517 23128000471168 run.py:483] Algo bellman_ford step 3597 current loss 0.025917, current_train_items 115136.
I0304 19:29:57.143963 23128000471168 run.py:483] Algo bellman_ford step 3598 current loss 0.051690, current_train_items 115168.
I0304 19:29:57.178466 23128000471168 run.py:483] Algo bellman_ford step 3599 current loss 0.098185, current_train_items 115200.
I0304 19:29:57.199222 23128000471168 run.py:483] Algo bellman_ford step 3600 current loss 0.002885, current_train_items 115232.
I0304 19:29:57.206922 23128000471168 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0304 19:29:57.207041 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:29:57.224582 23128000471168 run.py:483] Algo bellman_ford step 3601 current loss 0.028749, current_train_items 115264.
I0304 19:29:57.249628 23128000471168 run.py:483] Algo bellman_ford step 3602 current loss 0.038181, current_train_items 115296.
I0304 19:29:57.281153 23128000471168 run.py:483] Algo bellman_ford step 3603 current loss 0.061012, current_train_items 115328.
I0304 19:29:57.318432 23128000471168 run.py:483] Algo bellman_ford step 3604 current loss 0.098401, current_train_items 115360.
I0304 19:29:57.338587 23128000471168 run.py:483] Algo bellman_ford step 3605 current loss 0.002357, current_train_items 115392.
I0304 19:29:57.355215 23128000471168 run.py:483] Algo bellman_ford step 3606 current loss 0.040937, current_train_items 115424.
I0304 19:29:57.378396 23128000471168 run.py:483] Algo bellman_ford step 3607 current loss 0.024486, current_train_items 115456.
I0304 19:29:57.409265 23128000471168 run.py:483] Algo bellman_ford step 3608 current loss 0.056470, current_train_items 115488.
I0304 19:29:57.441857 23128000471168 run.py:483] Algo bellman_ford step 3609 current loss 0.062573, current_train_items 115520.
I0304 19:29:57.461936 23128000471168 run.py:483] Algo bellman_ford step 3610 current loss 0.002415, current_train_items 115552.
I0304 19:29:57.478689 23128000471168 run.py:483] Algo bellman_ford step 3611 current loss 0.032030, current_train_items 115584.
I0304 19:29:57.504235 23128000471168 run.py:483] Algo bellman_ford step 3612 current loss 0.107682, current_train_items 115616.
I0304 19:29:57.535251 23128000471168 run.py:483] Algo bellman_ford step 3613 current loss 0.080713, current_train_items 115648.
I0304 19:29:57.568048 23128000471168 run.py:483] Algo bellman_ford step 3614 current loss 0.077898, current_train_items 115680.
I0304 19:29:57.587987 23128000471168 run.py:483] Algo bellman_ford step 3615 current loss 0.021740, current_train_items 115712.
I0304 19:29:57.604825 23128000471168 run.py:483] Algo bellman_ford step 3616 current loss 0.028098, current_train_items 115744.
I0304 19:29:57.628937 23128000471168 run.py:483] Algo bellman_ford step 3617 current loss 0.048383, current_train_items 115776.
I0304 19:29:57.660984 23128000471168 run.py:483] Algo bellman_ford step 3618 current loss 0.090211, current_train_items 115808.
I0304 19:29:57.691996 23128000471168 run.py:483] Algo bellman_ford step 3619 current loss 0.082230, current_train_items 115840.
I0304 19:29:57.712169 23128000471168 run.py:483] Algo bellman_ford step 3620 current loss 0.002945, current_train_items 115872.
I0304 19:29:57.728682 23128000471168 run.py:483] Algo bellman_ford step 3621 current loss 0.017032, current_train_items 115904.
I0304 19:29:57.752670 23128000471168 run.py:483] Algo bellman_ford step 3622 current loss 0.081881, current_train_items 115936.
I0304 19:29:57.785053 23128000471168 run.py:483] Algo bellman_ford step 3623 current loss 0.183133, current_train_items 115968.
I0304 19:29:57.818562 23128000471168 run.py:483] Algo bellman_ford step 3624 current loss 0.088729, current_train_items 116000.
I0304 19:29:57.838353 23128000471168 run.py:483] Algo bellman_ford step 3625 current loss 0.004446, current_train_items 116032.
I0304 19:29:57.854970 23128000471168 run.py:483] Algo bellman_ford step 3626 current loss 0.016073, current_train_items 116064.
I0304 19:29:57.879742 23128000471168 run.py:483] Algo bellman_ford step 3627 current loss 0.060569, current_train_items 116096.
I0304 19:29:57.911901 23128000471168 run.py:483] Algo bellman_ford step 3628 current loss 0.082631, current_train_items 116128.
I0304 19:29:57.945537 23128000471168 run.py:483] Algo bellman_ford step 3629 current loss 0.074074, current_train_items 116160.
I0304 19:29:57.965873 23128000471168 run.py:483] Algo bellman_ford step 3630 current loss 0.006758, current_train_items 116192.
I0304 19:29:57.982339 23128000471168 run.py:483] Algo bellman_ford step 3631 current loss 0.030649, current_train_items 116224.
I0304 19:29:58.006711 23128000471168 run.py:483] Algo bellman_ford step 3632 current loss 0.036737, current_train_items 116256.
I0304 19:29:58.038257 23128000471168 run.py:483] Algo bellman_ford step 3633 current loss 0.163764, current_train_items 116288.
I0304 19:29:58.071833 23128000471168 run.py:483] Algo bellman_ford step 3634 current loss 0.086205, current_train_items 116320.
I0304 19:29:58.091740 23128000471168 run.py:483] Algo bellman_ford step 3635 current loss 0.005298, current_train_items 116352.
I0304 19:29:58.108222 23128000471168 run.py:483] Algo bellman_ford step 3636 current loss 0.044001, current_train_items 116384.
I0304 19:29:58.131645 23128000471168 run.py:483] Algo bellman_ford step 3637 current loss 0.044805, current_train_items 116416.
I0304 19:29:58.163618 23128000471168 run.py:483] Algo bellman_ford step 3638 current loss 0.050040, current_train_items 116448.
I0304 19:29:58.196866 23128000471168 run.py:483] Algo bellman_ford step 3639 current loss 0.077290, current_train_items 116480.
I0304 19:29:58.216998 23128000471168 run.py:483] Algo bellman_ford step 3640 current loss 0.005927, current_train_items 116512.
I0304 19:29:58.233186 23128000471168 run.py:483] Algo bellman_ford step 3641 current loss 0.020024, current_train_items 116544.
I0304 19:29:58.258176 23128000471168 run.py:483] Algo bellman_ford step 3642 current loss 0.139417, current_train_items 116576.
I0304 19:29:58.292444 23128000471168 run.py:483] Algo bellman_ford step 3643 current loss 0.096451, current_train_items 116608.
I0304 19:29:58.327312 23128000471168 run.py:483] Algo bellman_ford step 3644 current loss 0.076257, current_train_items 116640.
I0304 19:29:58.347120 23128000471168 run.py:483] Algo bellman_ford step 3645 current loss 0.002265, current_train_items 116672.
I0304 19:29:58.363600 23128000471168 run.py:483] Algo bellman_ford step 3646 current loss 0.027811, current_train_items 116704.
I0304 19:29:58.389486 23128000471168 run.py:483] Algo bellman_ford step 3647 current loss 0.077672, current_train_items 116736.
I0304 19:29:58.420630 23128000471168 run.py:483] Algo bellman_ford step 3648 current loss 0.067414, current_train_items 116768.
I0304 19:29:58.454942 23128000471168 run.py:483] Algo bellman_ford step 3649 current loss 0.117022, current_train_items 116800.
I0304 19:29:58.475039 23128000471168 run.py:483] Algo bellman_ford step 3650 current loss 0.002531, current_train_items 116832.
I0304 19:29:58.482998 23128000471168 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0304 19:29:58.483110 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:29:58.500431 23128000471168 run.py:483] Algo bellman_ford step 3651 current loss 0.015376, current_train_items 116864.
I0304 19:29:58.523925 23128000471168 run.py:483] Algo bellman_ford step 3652 current loss 0.070139, current_train_items 116896.
I0304 19:29:58.555137 23128000471168 run.py:483] Algo bellman_ford step 3653 current loss 0.044905, current_train_items 116928.
I0304 19:29:58.589091 23128000471168 run.py:483] Algo bellman_ford step 3654 current loss 0.092098, current_train_items 116960.
I0304 19:29:58.609122 23128000471168 run.py:483] Algo bellman_ford step 3655 current loss 0.003697, current_train_items 116992.
I0304 19:29:58.624874 23128000471168 run.py:483] Algo bellman_ford step 3656 current loss 0.027154, current_train_items 117024.
I0304 19:29:58.649454 23128000471168 run.py:483] Algo bellman_ford step 3657 current loss 0.049817, current_train_items 117056.
I0304 19:29:58.680704 23128000471168 run.py:483] Algo bellman_ford step 3658 current loss 0.083258, current_train_items 117088.
I0304 19:29:58.714536 23128000471168 run.py:483] Algo bellman_ford step 3659 current loss 0.066594, current_train_items 117120.
I0304 19:29:58.735616 23128000471168 run.py:483] Algo bellman_ford step 3660 current loss 0.001616, current_train_items 117152.
I0304 19:29:58.752430 23128000471168 run.py:483] Algo bellman_ford step 3661 current loss 0.016594, current_train_items 117184.
I0304 19:29:58.775875 23128000471168 run.py:483] Algo bellman_ford step 3662 current loss 0.038010, current_train_items 117216.
I0304 19:29:58.808060 23128000471168 run.py:483] Algo bellman_ford step 3663 current loss 0.092882, current_train_items 117248.
I0304 19:29:58.843309 23128000471168 run.py:483] Algo bellman_ford step 3664 current loss 0.070037, current_train_items 117280.
I0304 19:29:58.863194 23128000471168 run.py:483] Algo bellman_ford step 3665 current loss 0.012982, current_train_items 117312.
I0304 19:29:58.879459 23128000471168 run.py:483] Algo bellman_ford step 3666 current loss 0.014772, current_train_items 117344.
I0304 19:29:58.904167 23128000471168 run.py:483] Algo bellman_ford step 3667 current loss 0.083723, current_train_items 117376.
I0304 19:29:58.935998 23128000471168 run.py:483] Algo bellman_ford step 3668 current loss 0.125724, current_train_items 117408.
I0304 19:29:58.971376 23128000471168 run.py:483] Algo bellman_ford step 3669 current loss 0.114455, current_train_items 117440.
I0304 19:29:58.991490 23128000471168 run.py:483] Algo bellman_ford step 3670 current loss 0.003842, current_train_items 117472.
I0304 19:29:59.008414 23128000471168 run.py:483] Algo bellman_ford step 3671 current loss 0.133004, current_train_items 117504.
I0304 19:29:59.031845 23128000471168 run.py:483] Algo bellman_ford step 3672 current loss 0.055398, current_train_items 117536.
I0304 19:29:59.063069 23128000471168 run.py:483] Algo bellman_ford step 3673 current loss 0.053726, current_train_items 117568.
I0304 19:29:59.098147 23128000471168 run.py:483] Algo bellman_ford step 3674 current loss 0.121300, current_train_items 117600.
I0304 19:29:59.118262 23128000471168 run.py:483] Algo bellman_ford step 3675 current loss 0.001880, current_train_items 117632.
I0304 19:29:59.134508 23128000471168 run.py:483] Algo bellman_ford step 3676 current loss 0.011571, current_train_items 117664.
I0304 19:29:59.158075 23128000471168 run.py:483] Algo bellman_ford step 3677 current loss 0.044782, current_train_items 117696.
I0304 19:29:59.189644 23128000471168 run.py:483] Algo bellman_ford step 3678 current loss 0.087897, current_train_items 117728.
I0304 19:29:59.223884 23128000471168 run.py:483] Algo bellman_ford step 3679 current loss 0.106654, current_train_items 117760.
I0304 19:29:59.243527 23128000471168 run.py:483] Algo bellman_ford step 3680 current loss 0.001405, current_train_items 117792.
I0304 19:29:59.260229 23128000471168 run.py:483] Algo bellman_ford step 3681 current loss 0.014350, current_train_items 117824.
I0304 19:29:59.284888 23128000471168 run.py:483] Algo bellman_ford step 3682 current loss 0.040500, current_train_items 117856.
I0304 19:29:59.315768 23128000471168 run.py:483] Algo bellman_ford step 3683 current loss 0.062028, current_train_items 117888.
I0304 19:29:59.349673 23128000471168 run.py:483] Algo bellman_ford step 3684 current loss 0.103434, current_train_items 117920.
I0304 19:29:59.369816 23128000471168 run.py:483] Algo bellman_ford step 3685 current loss 0.001782, current_train_items 117952.
I0304 19:29:59.386375 23128000471168 run.py:483] Algo bellman_ford step 3686 current loss 0.024355, current_train_items 117984.
I0304 19:29:59.409811 23128000471168 run.py:483] Algo bellman_ford step 3687 current loss 0.034861, current_train_items 118016.
I0304 19:29:59.442814 23128000471168 run.py:483] Algo bellman_ford step 3688 current loss 0.065521, current_train_items 118048.
I0304 19:29:59.478374 23128000471168 run.py:483] Algo bellman_ford step 3689 current loss 0.053607, current_train_items 118080.
I0304 19:29:59.498757 23128000471168 run.py:483] Algo bellman_ford step 3690 current loss 0.010162, current_train_items 118112.
I0304 19:29:59.515265 23128000471168 run.py:483] Algo bellman_ford step 3691 current loss 0.009208, current_train_items 118144.
I0304 19:29:59.537606 23128000471168 run.py:483] Algo bellman_ford step 3692 current loss 0.033862, current_train_items 118176.
I0304 19:29:59.570404 23128000471168 run.py:483] Algo bellman_ford step 3693 current loss 0.047383, current_train_items 118208.
I0304 19:29:59.603570 23128000471168 run.py:483] Algo bellman_ford step 3694 current loss 0.054555, current_train_items 118240.
I0304 19:29:59.623491 23128000471168 run.py:483] Algo bellman_ford step 3695 current loss 0.002757, current_train_items 118272.
I0304 19:29:59.640622 23128000471168 run.py:483] Algo bellman_ford step 3696 current loss 0.052981, current_train_items 118304.
I0304 19:29:59.664650 23128000471168 run.py:483] Algo bellman_ford step 3697 current loss 0.058859, current_train_items 118336.
I0304 19:29:59.696916 23128000471168 run.py:483] Algo bellman_ford step 3698 current loss 0.051057, current_train_items 118368.
I0304 19:29:59.729292 23128000471168 run.py:483] Algo bellman_ford step 3699 current loss 0.084120, current_train_items 118400.
I0304 19:29:59.749616 23128000471168 run.py:483] Algo bellman_ford step 3700 current loss 0.002328, current_train_items 118432.
I0304 19:29:59.757527 23128000471168 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0304 19:29:59.757636 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:59.774674 23128000471168 run.py:483] Algo bellman_ford step 3701 current loss 0.023926, current_train_items 118464.
I0304 19:29:59.799447 23128000471168 run.py:483] Algo bellman_ford step 3702 current loss 0.122347, current_train_items 118496.
I0304 19:29:59.832845 23128000471168 run.py:483] Algo bellman_ford step 3703 current loss 0.075746, current_train_items 118528.
I0304 19:29:59.866800 23128000471168 run.py:483] Algo bellman_ford step 3704 current loss 0.054873, current_train_items 118560.
I0304 19:29:59.886809 23128000471168 run.py:483] Algo bellman_ford step 3705 current loss 0.002112, current_train_items 118592.
I0304 19:29:59.903310 23128000471168 run.py:483] Algo bellman_ford step 3706 current loss 0.031469, current_train_items 118624.
I0304 19:29:59.928113 23128000471168 run.py:483] Algo bellman_ford step 3707 current loss 0.080433, current_train_items 118656.
I0304 19:29:59.958530 23128000471168 run.py:483] Algo bellman_ford step 3708 current loss 0.056939, current_train_items 118688.
I0304 19:29:59.992517 23128000471168 run.py:483] Algo bellman_ford step 3709 current loss 0.084240, current_train_items 118720.
I0304 19:30:00.012999 23128000471168 run.py:483] Algo bellman_ford step 3710 current loss 0.010228, current_train_items 118752.
I0304 19:30:00.029639 23128000471168 run.py:483] Algo bellman_ford step 3711 current loss 0.034102, current_train_items 118784.
I0304 19:30:00.053751 23128000471168 run.py:483] Algo bellman_ford step 3712 current loss 0.073185, current_train_items 118816.
I0304 19:30:00.086568 23128000471168 run.py:483] Algo bellman_ford step 3713 current loss 0.067838, current_train_items 118848.
I0304 19:30:00.121229 23128000471168 run.py:483] Algo bellman_ford step 3714 current loss 0.096063, current_train_items 118880.
I0304 19:30:00.140971 23128000471168 run.py:483] Algo bellman_ford step 3715 current loss 0.005522, current_train_items 118912.
I0304 19:30:00.157330 23128000471168 run.py:483] Algo bellman_ford step 3716 current loss 0.008650, current_train_items 118944.
I0304 19:30:00.181233 23128000471168 run.py:483] Algo bellman_ford step 3717 current loss 0.065910, current_train_items 118976.
I0304 19:30:00.212988 23128000471168 run.py:483] Algo bellman_ford step 3718 current loss 0.070040, current_train_items 119008.
I0304 19:30:00.245572 23128000471168 run.py:483] Algo bellman_ford step 3719 current loss 0.101565, current_train_items 119040.
I0304 19:30:00.265544 23128000471168 run.py:483] Algo bellman_ford step 3720 current loss 0.005863, current_train_items 119072.
I0304 19:30:00.282244 23128000471168 run.py:483] Algo bellman_ford step 3721 current loss 0.040915, current_train_items 119104.
I0304 19:30:00.306149 23128000471168 run.py:483] Algo bellman_ford step 3722 current loss 0.022766, current_train_items 119136.
I0304 19:30:00.337814 23128000471168 run.py:483] Algo bellman_ford step 3723 current loss 0.049038, current_train_items 119168.
I0304 19:30:00.371141 23128000471168 run.py:483] Algo bellman_ford step 3724 current loss 0.075692, current_train_items 119200.
I0304 19:30:00.390790 23128000471168 run.py:483] Algo bellman_ford step 3725 current loss 0.001971, current_train_items 119232.
I0304 19:30:00.407801 23128000471168 run.py:483] Algo bellman_ford step 3726 current loss 0.010003, current_train_items 119264.
I0304 19:30:00.432856 23128000471168 run.py:483] Algo bellman_ford step 3727 current loss 0.064050, current_train_items 119296.
I0304 19:30:00.464503 23128000471168 run.py:483] Algo bellman_ford step 3728 current loss 0.053844, current_train_items 119328.
I0304 19:30:00.498085 23128000471168 run.py:483] Algo bellman_ford step 3729 current loss 0.057087, current_train_items 119360.
I0304 19:30:00.517935 23128000471168 run.py:483] Algo bellman_ford step 3730 current loss 0.006907, current_train_items 119392.
I0304 19:30:00.534366 23128000471168 run.py:483] Algo bellman_ford step 3731 current loss 0.037510, current_train_items 119424.
I0304 19:30:00.558485 23128000471168 run.py:483] Algo bellman_ford step 3732 current loss 0.045655, current_train_items 119456.
I0304 19:30:00.588076 23128000471168 run.py:483] Algo bellman_ford step 3733 current loss 0.046503, current_train_items 119488.
I0304 19:30:00.622627 23128000471168 run.py:483] Algo bellman_ford step 3734 current loss 0.110645, current_train_items 119520.
I0304 19:30:00.642467 23128000471168 run.py:483] Algo bellman_ford step 3735 current loss 0.007466, current_train_items 119552.
I0304 19:30:00.658655 23128000471168 run.py:483] Algo bellman_ford step 3736 current loss 0.036152, current_train_items 119584.
I0304 19:30:00.682025 23128000471168 run.py:483] Algo bellman_ford step 3737 current loss 0.034064, current_train_items 119616.
I0304 19:30:00.713927 23128000471168 run.py:483] Algo bellman_ford step 3738 current loss 0.069644, current_train_items 119648.
I0304 19:30:00.747538 23128000471168 run.py:483] Algo bellman_ford step 3739 current loss 0.070942, current_train_items 119680.
I0304 19:30:00.767545 23128000471168 run.py:483] Algo bellman_ford step 3740 current loss 0.006361, current_train_items 119712.
I0304 19:30:00.783849 23128000471168 run.py:483] Algo bellman_ford step 3741 current loss 0.019068, current_train_items 119744.
I0304 19:30:00.807375 23128000471168 run.py:483] Algo bellman_ford step 3742 current loss 0.075947, current_train_items 119776.
I0304 19:30:00.838523 23128000471168 run.py:483] Algo bellman_ford step 3743 current loss 0.110578, current_train_items 119808.
I0304 19:30:00.870710 23128000471168 run.py:483] Algo bellman_ford step 3744 current loss 0.084157, current_train_items 119840.
I0304 19:30:00.890458 23128000471168 run.py:483] Algo bellman_ford step 3745 current loss 0.011909, current_train_items 119872.
I0304 19:30:00.906563 23128000471168 run.py:483] Algo bellman_ford step 3746 current loss 0.018534, current_train_items 119904.
I0304 19:30:00.931141 23128000471168 run.py:483] Algo bellman_ford step 3747 current loss 0.052214, current_train_items 119936.
I0304 19:30:00.963196 23128000471168 run.py:483] Algo bellman_ford step 3748 current loss 0.086532, current_train_items 119968.
I0304 19:30:00.998255 23128000471168 run.py:483] Algo bellman_ford step 3749 current loss 0.141450, current_train_items 120000.
I0304 19:30:01.018360 23128000471168 run.py:483] Algo bellman_ford step 3750 current loss 0.005641, current_train_items 120032.
I0304 19:30:01.026765 23128000471168 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0304 19:30:01.026903 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:01.044347 23128000471168 run.py:483] Algo bellman_ford step 3751 current loss 0.016638, current_train_items 120064.
I0304 19:30:01.069275 23128000471168 run.py:483] Algo bellman_ford step 3752 current loss 0.040929, current_train_items 120096.
I0304 19:30:01.101485 23128000471168 run.py:483] Algo bellman_ford step 3753 current loss 0.066454, current_train_items 120128.
I0304 19:30:01.137890 23128000471168 run.py:483] Algo bellman_ford step 3754 current loss 0.084657, current_train_items 120160.
I0304 19:30:01.157972 23128000471168 run.py:483] Algo bellman_ford step 3755 current loss 0.010341, current_train_items 120192.
I0304 19:30:01.174228 23128000471168 run.py:483] Algo bellman_ford step 3756 current loss 0.033478, current_train_items 120224.
I0304 19:30:01.198881 23128000471168 run.py:483] Algo bellman_ford step 3757 current loss 0.058660, current_train_items 120256.
I0304 19:30:01.230773 23128000471168 run.py:483] Algo bellman_ford step 3758 current loss 0.081059, current_train_items 120288.
I0304 19:30:01.263023 23128000471168 run.py:483] Algo bellman_ford step 3759 current loss 0.056267, current_train_items 120320.
I0304 19:30:01.283230 23128000471168 run.py:483] Algo bellman_ford step 3760 current loss 0.004279, current_train_items 120352.
I0304 19:30:01.299995 23128000471168 run.py:483] Algo bellman_ford step 3761 current loss 0.008528, current_train_items 120384.
I0304 19:30:01.323336 23128000471168 run.py:483] Algo bellman_ford step 3762 current loss 0.040546, current_train_items 120416.
I0304 19:30:01.354710 23128000471168 run.py:483] Algo bellman_ford step 3763 current loss 0.056864, current_train_items 120448.
I0304 19:30:01.389041 23128000471168 run.py:483] Algo bellman_ford step 3764 current loss 0.068446, current_train_items 120480.
I0304 19:30:01.408977 23128000471168 run.py:483] Algo bellman_ford step 3765 current loss 0.002528, current_train_items 120512.
I0304 19:30:01.425882 23128000471168 run.py:483] Algo bellman_ford step 3766 current loss 0.024092, current_train_items 120544.
I0304 19:30:01.450376 23128000471168 run.py:483] Algo bellman_ford step 3767 current loss 0.093490, current_train_items 120576.
I0304 19:30:01.482959 23128000471168 run.py:483] Algo bellman_ford step 3768 current loss 0.079349, current_train_items 120608.
I0304 19:30:01.515938 23128000471168 run.py:483] Algo bellman_ford step 3769 current loss 0.066519, current_train_items 120640.
I0304 19:30:01.536569 23128000471168 run.py:483] Algo bellman_ford step 3770 current loss 0.002471, current_train_items 120672.
I0304 19:30:01.553294 23128000471168 run.py:483] Algo bellman_ford step 3771 current loss 0.046976, current_train_items 120704.
I0304 19:30:01.576703 23128000471168 run.py:483] Algo bellman_ford step 3772 current loss 0.038189, current_train_items 120736.
I0304 19:30:01.607825 23128000471168 run.py:483] Algo bellman_ford step 3773 current loss 0.056605, current_train_items 120768.
I0304 19:30:01.638818 23128000471168 run.py:483] Algo bellman_ford step 3774 current loss 0.060911, current_train_items 120800.
I0304 19:30:01.659125 23128000471168 run.py:483] Algo bellman_ford step 3775 current loss 0.002596, current_train_items 120832.
I0304 19:30:01.676185 23128000471168 run.py:483] Algo bellman_ford step 3776 current loss 0.019349, current_train_items 120864.
I0304 19:30:01.699685 23128000471168 run.py:483] Algo bellman_ford step 3777 current loss 0.049274, current_train_items 120896.
I0304 19:30:01.731960 23128000471168 run.py:483] Algo bellman_ford step 3778 current loss 0.052013, current_train_items 120928.
I0304 19:30:01.766136 23128000471168 run.py:483] Algo bellman_ford step 3779 current loss 0.053990, current_train_items 120960.
I0304 19:30:01.785939 23128000471168 run.py:483] Algo bellman_ford step 3780 current loss 0.002303, current_train_items 120992.
I0304 19:30:01.802555 23128000471168 run.py:483] Algo bellman_ford step 3781 current loss 0.030273, current_train_items 121024.
I0304 19:30:01.826526 23128000471168 run.py:483] Algo bellman_ford step 3782 current loss 0.064848, current_train_items 121056.
I0304 19:30:01.856671 23128000471168 run.py:483] Algo bellman_ford step 3783 current loss 0.046807, current_train_items 121088.
I0304 19:30:01.893507 23128000471168 run.py:483] Algo bellman_ford step 3784 current loss 0.097932, current_train_items 121120.
I0304 19:30:01.913857 23128000471168 run.py:483] Algo bellman_ford step 3785 current loss 0.004189, current_train_items 121152.
I0304 19:30:01.930307 23128000471168 run.py:483] Algo bellman_ford step 3786 current loss 0.024546, current_train_items 121184.
I0304 19:30:01.954621 23128000471168 run.py:483] Algo bellman_ford step 3787 current loss 0.042151, current_train_items 121216.
I0304 19:30:01.986248 23128000471168 run.py:483] Algo bellman_ford step 3788 current loss 0.045515, current_train_items 121248.
I0304 19:30:02.020481 23128000471168 run.py:483] Algo bellman_ford step 3789 current loss 0.070949, current_train_items 121280.
I0304 19:30:02.040753 23128000471168 run.py:483] Algo bellman_ford step 3790 current loss 0.004707, current_train_items 121312.
I0304 19:30:02.057449 23128000471168 run.py:483] Algo bellman_ford step 3791 current loss 0.009187, current_train_items 121344.
I0304 19:30:02.081104 23128000471168 run.py:483] Algo bellman_ford step 3792 current loss 0.067496, current_train_items 121376.
I0304 19:30:02.113602 23128000471168 run.py:483] Algo bellman_ford step 3793 current loss 0.064441, current_train_items 121408.
I0304 19:30:02.146968 23128000471168 run.py:483] Algo bellman_ford step 3794 current loss 0.044073, current_train_items 121440.
I0304 19:30:02.167269 23128000471168 run.py:483] Algo bellman_ford step 3795 current loss 0.008051, current_train_items 121472.
I0304 19:30:02.183997 23128000471168 run.py:483] Algo bellman_ford step 3796 current loss 0.027737, current_train_items 121504.
I0304 19:30:02.208821 23128000471168 run.py:483] Algo bellman_ford step 3797 current loss 0.060289, current_train_items 121536.
I0304 19:30:02.241729 23128000471168 run.py:483] Algo bellman_ford step 3798 current loss 0.125525, current_train_items 121568.
I0304 19:30:02.274535 23128000471168 run.py:483] Algo bellman_ford step 3799 current loss 0.107580, current_train_items 121600.
I0304 19:30:02.295020 23128000471168 run.py:483] Algo bellman_ford step 3800 current loss 0.002717, current_train_items 121632.
I0304 19:30:02.302831 23128000471168 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0304 19:30:02.302939 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:30:02.320100 23128000471168 run.py:483] Algo bellman_ford step 3801 current loss 0.018898, current_train_items 121664.
I0304 19:30:02.345783 23128000471168 run.py:483] Algo bellman_ford step 3802 current loss 0.028150, current_train_items 121696.
I0304 19:30:02.378012 23128000471168 run.py:483] Algo bellman_ford step 3803 current loss 0.060343, current_train_items 121728.
I0304 19:30:02.410010 23128000471168 run.py:483] Algo bellman_ford step 3804 current loss 0.050459, current_train_items 121760.
I0304 19:30:02.430213 23128000471168 run.py:483] Algo bellman_ford step 3805 current loss 0.002432, current_train_items 121792.
I0304 19:30:02.446553 23128000471168 run.py:483] Algo bellman_ford step 3806 current loss 0.032002, current_train_items 121824.
I0304 19:30:02.470824 23128000471168 run.py:483] Algo bellman_ford step 3807 current loss 0.055400, current_train_items 121856.
I0304 19:30:02.502704 23128000471168 run.py:483] Algo bellman_ford step 3808 current loss 0.087835, current_train_items 121888.
I0304 19:30:02.538356 23128000471168 run.py:483] Algo bellman_ford step 3809 current loss 0.053074, current_train_items 121920.
I0304 19:30:02.558195 23128000471168 run.py:483] Algo bellman_ford step 3810 current loss 0.015367, current_train_items 121952.
I0304 19:30:02.575066 23128000471168 run.py:483] Algo bellman_ford step 3811 current loss 0.005538, current_train_items 121984.
I0304 19:30:02.599560 23128000471168 run.py:483] Algo bellman_ford step 3812 current loss 0.078554, current_train_items 122016.
I0304 19:30:02.633418 23128000471168 run.py:483] Algo bellman_ford step 3813 current loss 0.100287, current_train_items 122048.
I0304 19:30:02.669190 23128000471168 run.py:483] Algo bellman_ford step 3814 current loss 0.167292, current_train_items 122080.
I0304 19:30:02.689607 23128000471168 run.py:483] Algo bellman_ford step 3815 current loss 0.010041, current_train_items 122112.
I0304 19:30:02.706500 23128000471168 run.py:483] Algo bellman_ford step 3816 current loss 0.060999, current_train_items 122144.
I0304 19:30:02.730810 23128000471168 run.py:483] Algo bellman_ford step 3817 current loss 0.049979, current_train_items 122176.
I0304 19:30:02.762748 23128000471168 run.py:483] Algo bellman_ford step 3818 current loss 0.060808, current_train_items 122208.
I0304 19:30:02.796805 23128000471168 run.py:483] Algo bellman_ford step 3819 current loss 0.132168, current_train_items 122240.
I0304 19:30:02.816786 23128000471168 run.py:483] Algo bellman_ford step 3820 current loss 0.002933, current_train_items 122272.
I0304 19:30:02.832941 23128000471168 run.py:483] Algo bellman_ford step 3821 current loss 0.015854, current_train_items 122304.
I0304 19:30:02.857633 23128000471168 run.py:483] Algo bellman_ford step 3822 current loss 0.059380, current_train_items 122336.
I0304 19:30:02.889726 23128000471168 run.py:483] Algo bellman_ford step 3823 current loss 0.041010, current_train_items 122368.
I0304 19:30:02.924313 23128000471168 run.py:483] Algo bellman_ford step 3824 current loss 0.063651, current_train_items 122400.
I0304 19:30:02.944695 23128000471168 run.py:483] Algo bellman_ford step 3825 current loss 0.007190, current_train_items 122432.
I0304 19:30:02.960844 23128000471168 run.py:483] Algo bellman_ford step 3826 current loss 0.013456, current_train_items 122464.
I0304 19:30:02.985384 23128000471168 run.py:483] Algo bellman_ford step 3827 current loss 0.037955, current_train_items 122496.
I0304 19:30:03.017869 23128000471168 run.py:483] Algo bellman_ford step 3828 current loss 0.057221, current_train_items 122528.
I0304 19:30:03.050661 23128000471168 run.py:483] Algo bellman_ford step 3829 current loss 0.162056, current_train_items 122560.
I0304 19:30:03.070960 23128000471168 run.py:483] Algo bellman_ford step 3830 current loss 0.002360, current_train_items 122592.
I0304 19:30:03.087799 23128000471168 run.py:483] Algo bellman_ford step 3831 current loss 0.032833, current_train_items 122624.
I0304 19:30:03.111186 23128000471168 run.py:483] Algo bellman_ford step 3832 current loss 0.037740, current_train_items 122656.
I0304 19:30:03.144175 23128000471168 run.py:483] Algo bellman_ford step 3833 current loss 0.047267, current_train_items 122688.
I0304 19:30:03.177450 23128000471168 run.py:483] Algo bellman_ford step 3834 current loss 0.056827, current_train_items 122720.
I0304 19:30:03.197376 23128000471168 run.py:483] Algo bellman_ford step 3835 current loss 0.015761, current_train_items 122752.
I0304 19:30:03.213289 23128000471168 run.py:483] Algo bellman_ford step 3836 current loss 0.009385, current_train_items 122784.
I0304 19:30:03.238424 23128000471168 run.py:483] Algo bellman_ford step 3837 current loss 0.088700, current_train_items 122816.
I0304 19:30:03.270255 23128000471168 run.py:483] Algo bellman_ford step 3838 current loss 0.111605, current_train_items 122848.
I0304 19:30:03.304316 23128000471168 run.py:483] Algo bellman_ford step 3839 current loss 0.069606, current_train_items 122880.
I0304 19:30:03.324483 23128000471168 run.py:483] Algo bellman_ford step 3840 current loss 0.032550, current_train_items 122912.
I0304 19:30:03.340605 23128000471168 run.py:483] Algo bellman_ford step 3841 current loss 0.011432, current_train_items 122944.
I0304 19:30:03.364251 23128000471168 run.py:483] Algo bellman_ford step 3842 current loss 0.049599, current_train_items 122976.
I0304 19:30:03.396288 23128000471168 run.py:483] Algo bellman_ford step 3843 current loss 0.054483, current_train_items 123008.
I0304 19:30:03.431234 23128000471168 run.py:483] Algo bellman_ford step 3844 current loss 0.144199, current_train_items 123040.
I0304 19:30:03.450994 23128000471168 run.py:483] Algo bellman_ford step 3845 current loss 0.006016, current_train_items 123072.
I0304 19:30:03.467398 23128000471168 run.py:483] Algo bellman_ford step 3846 current loss 0.028379, current_train_items 123104.
I0304 19:30:03.493124 23128000471168 run.py:483] Algo bellman_ford step 3847 current loss 0.079056, current_train_items 123136.
I0304 19:30:03.524138 23128000471168 run.py:483] Algo bellman_ford step 3848 current loss 0.054672, current_train_items 123168.
I0304 19:30:03.557942 23128000471168 run.py:483] Algo bellman_ford step 3849 current loss 0.080407, current_train_items 123200.
I0304 19:30:03.578290 23128000471168 run.py:483] Algo bellman_ford step 3850 current loss 0.008160, current_train_items 123232.
I0304 19:30:03.586132 23128000471168 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0304 19:30:03.586243 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:03.602993 23128000471168 run.py:483] Algo bellman_ford step 3851 current loss 0.006617, current_train_items 123264.
I0304 19:30:03.626984 23128000471168 run.py:483] Algo bellman_ford step 3852 current loss 0.042619, current_train_items 123296.
I0304 19:30:03.659493 23128000471168 run.py:483] Algo bellman_ford step 3853 current loss 0.075170, current_train_items 123328.
I0304 19:30:03.695513 23128000471168 run.py:483] Algo bellman_ford step 3854 current loss 0.080322, current_train_items 123360.
I0304 19:30:03.715728 23128000471168 run.py:483] Algo bellman_ford step 3855 current loss 0.014467, current_train_items 123392.
I0304 19:30:03.731761 23128000471168 run.py:483] Algo bellman_ford step 3856 current loss 0.011093, current_train_items 123424.
I0304 19:30:03.755153 23128000471168 run.py:483] Algo bellman_ford step 3857 current loss 0.036058, current_train_items 123456.
I0304 19:30:03.787249 23128000471168 run.py:483] Algo bellman_ford step 3858 current loss 0.098225, current_train_items 123488.
I0304 19:30:03.823384 23128000471168 run.py:483] Algo bellman_ford step 3859 current loss 0.109892, current_train_items 123520.
I0304 19:30:03.843776 23128000471168 run.py:483] Algo bellman_ford step 3860 current loss 0.004581, current_train_items 123552.
I0304 19:30:03.860698 23128000471168 run.py:483] Algo bellman_ford step 3861 current loss 0.059733, current_train_items 123584.
I0304 19:30:03.884480 23128000471168 run.py:483] Algo bellman_ford step 3862 current loss 0.073192, current_train_items 123616.
I0304 19:30:03.916119 23128000471168 run.py:483] Algo bellman_ford step 3863 current loss 0.088789, current_train_items 123648.
I0304 19:30:03.952300 23128000471168 run.py:483] Algo bellman_ford step 3864 current loss 0.097511, current_train_items 123680.
I0304 19:30:03.971925 23128000471168 run.py:483] Algo bellman_ford step 3865 current loss 0.004772, current_train_items 123712.
I0304 19:30:03.988369 23128000471168 run.py:483] Algo bellman_ford step 3866 current loss 0.028530, current_train_items 123744.
I0304 19:30:04.012391 23128000471168 run.py:483] Algo bellman_ford step 3867 current loss 0.059454, current_train_items 123776.
I0304 19:30:04.043660 23128000471168 run.py:483] Algo bellman_ford step 3868 current loss 0.061107, current_train_items 123808.
I0304 19:30:04.077663 23128000471168 run.py:483] Algo bellman_ford step 3869 current loss 0.111918, current_train_items 123840.
I0304 19:30:04.098659 23128000471168 run.py:483] Algo bellman_ford step 3870 current loss 0.007319, current_train_items 123872.
I0304 19:30:04.114921 23128000471168 run.py:483] Algo bellman_ford step 3871 current loss 0.016534, current_train_items 123904.
I0304 19:30:04.139323 23128000471168 run.py:483] Algo bellman_ford step 3872 current loss 0.026087, current_train_items 123936.
I0304 19:30:04.169390 23128000471168 run.py:483] Algo bellman_ford step 3873 current loss 0.029052, current_train_items 123968.
I0304 19:30:04.202326 23128000471168 run.py:483] Algo bellman_ford step 3874 current loss 0.085903, current_train_items 124000.
I0304 19:30:04.222832 23128000471168 run.py:483] Algo bellman_ford step 3875 current loss 0.004503, current_train_items 124032.
I0304 19:30:04.239766 23128000471168 run.py:483] Algo bellman_ford step 3876 current loss 0.040394, current_train_items 124064.
I0304 19:30:04.264626 23128000471168 run.py:483] Algo bellman_ford step 3877 current loss 0.034507, current_train_items 124096.
I0304 19:30:04.296352 23128000471168 run.py:483] Algo bellman_ford step 3878 current loss 0.039476, current_train_items 124128.
I0304 19:30:04.330238 23128000471168 run.py:483] Algo bellman_ford step 3879 current loss 0.076712, current_train_items 124160.
I0304 19:30:04.350679 23128000471168 run.py:483] Algo bellman_ford step 3880 current loss 0.023197, current_train_items 124192.
I0304 19:30:04.366997 23128000471168 run.py:483] Algo bellman_ford step 3881 current loss 0.020988, current_train_items 124224.
I0304 19:30:04.391516 23128000471168 run.py:483] Algo bellman_ford step 3882 current loss 0.045614, current_train_items 124256.
I0304 19:30:04.423048 23128000471168 run.py:483] Algo bellman_ford step 3883 current loss 0.060192, current_train_items 124288.
I0304 19:30:04.456309 23128000471168 run.py:483] Algo bellman_ford step 3884 current loss 0.079754, current_train_items 124320.
I0304 19:30:04.477041 23128000471168 run.py:483] Algo bellman_ford step 3885 current loss 0.002724, current_train_items 124352.
I0304 19:30:04.494033 23128000471168 run.py:483] Algo bellman_ford step 3886 current loss 0.013976, current_train_items 124384.
I0304 19:30:04.518011 23128000471168 run.py:483] Algo bellman_ford step 3887 current loss 0.056424, current_train_items 124416.
I0304 19:30:04.549342 23128000471168 run.py:483] Algo bellman_ford step 3888 current loss 0.067038, current_train_items 124448.
I0304 19:30:04.584300 23128000471168 run.py:483] Algo bellman_ford step 3889 current loss 0.079162, current_train_items 124480.
I0304 19:30:04.604690 23128000471168 run.py:483] Algo bellman_ford step 3890 current loss 0.002383, current_train_items 124512.
I0304 19:30:04.621420 23128000471168 run.py:483] Algo bellman_ford step 3891 current loss 0.023082, current_train_items 124544.
I0304 19:30:04.646027 23128000471168 run.py:483] Algo bellman_ford step 3892 current loss 0.044042, current_train_items 124576.
I0304 19:30:04.675756 23128000471168 run.py:483] Algo bellman_ford step 3893 current loss 0.027678, current_train_items 124608.
I0304 19:30:04.710070 23128000471168 run.py:483] Algo bellman_ford step 3894 current loss 0.045186, current_train_items 124640.
I0304 19:30:04.730269 23128000471168 run.py:483] Algo bellman_ford step 3895 current loss 0.002688, current_train_items 124672.
I0304 19:30:04.746973 23128000471168 run.py:483] Algo bellman_ford step 3896 current loss 0.047697, current_train_items 124704.
I0304 19:30:04.772126 23128000471168 run.py:483] Algo bellman_ford step 3897 current loss 0.061356, current_train_items 124736.
I0304 19:30:04.804123 23128000471168 run.py:483] Algo bellman_ford step 3898 current loss 0.062553, current_train_items 124768.
I0304 19:30:04.838112 23128000471168 run.py:483] Algo bellman_ford step 3899 current loss 0.079940, current_train_items 124800.
I0304 19:30:04.858955 23128000471168 run.py:483] Algo bellman_ford step 3900 current loss 0.003362, current_train_items 124832.
I0304 19:30:04.866951 23128000471168 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0304 19:30:04.867070 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:04.884550 23128000471168 run.py:483] Algo bellman_ford step 3901 current loss 0.044770, current_train_items 124864.
I0304 19:30:04.908853 23128000471168 run.py:483] Algo bellman_ford step 3902 current loss 0.099702, current_train_items 124896.
I0304 19:30:04.941023 23128000471168 run.py:483] Algo bellman_ford step 3903 current loss 0.213307, current_train_items 124928.
I0304 19:30:04.974937 23128000471168 run.py:483] Algo bellman_ford step 3904 current loss 0.099793, current_train_items 124960.
I0304 19:30:04.995386 23128000471168 run.py:483] Algo bellman_ford step 3905 current loss 0.002701, current_train_items 124992.
I0304 19:30:05.011491 23128000471168 run.py:483] Algo bellman_ford step 3906 current loss 0.030602, current_train_items 125024.
I0304 19:30:05.034583 23128000471168 run.py:483] Algo bellman_ford step 3907 current loss 0.045494, current_train_items 125056.
I0304 19:30:05.066240 23128000471168 run.py:483] Algo bellman_ford step 3908 current loss 0.059463, current_train_items 125088.
I0304 19:30:05.100429 23128000471168 run.py:483] Algo bellman_ford step 3909 current loss 0.087760, current_train_items 125120.
I0304 19:30:05.120147 23128000471168 run.py:483] Algo bellman_ford step 3910 current loss 0.002064, current_train_items 125152.
I0304 19:30:05.136402 23128000471168 run.py:483] Algo bellman_ford step 3911 current loss 0.006125, current_train_items 125184.
I0304 19:30:05.160987 23128000471168 run.py:483] Algo bellman_ford step 3912 current loss 0.028015, current_train_items 125216.
I0304 19:30:05.191991 23128000471168 run.py:483] Algo bellman_ford step 3913 current loss 0.037140, current_train_items 125248.
I0304 19:30:05.223024 23128000471168 run.py:483] Algo bellman_ford step 3914 current loss 0.047979, current_train_items 125280.
I0304 19:30:05.242710 23128000471168 run.py:483] Algo bellman_ford step 3915 current loss 0.008889, current_train_items 125312.
I0304 19:30:05.258992 23128000471168 run.py:483] Algo bellman_ford step 3916 current loss 0.006778, current_train_items 125344.
I0304 19:30:05.284312 23128000471168 run.py:483] Algo bellman_ford step 3917 current loss 0.066085, current_train_items 125376.
I0304 19:30:05.314874 23128000471168 run.py:483] Algo bellman_ford step 3918 current loss 0.065364, current_train_items 125408.
I0304 19:30:05.348463 23128000471168 run.py:483] Algo bellman_ford step 3919 current loss 0.059054, current_train_items 125440.
I0304 19:30:05.368350 23128000471168 run.py:483] Algo bellman_ford step 3920 current loss 0.001187, current_train_items 125472.
I0304 19:30:05.384977 23128000471168 run.py:483] Algo bellman_ford step 3921 current loss 0.006777, current_train_items 125504.
I0304 19:30:05.410269 23128000471168 run.py:483] Algo bellman_ford step 3922 current loss 0.022984, current_train_items 125536.
I0304 19:30:05.442498 23128000471168 run.py:483] Algo bellman_ford step 3923 current loss 0.098473, current_train_items 125568.
I0304 19:30:05.478691 23128000471168 run.py:483] Algo bellman_ford step 3924 current loss 0.076175, current_train_items 125600.
I0304 19:30:05.498960 23128000471168 run.py:483] Algo bellman_ford step 3925 current loss 0.001762, current_train_items 125632.
I0304 19:30:05.515674 23128000471168 run.py:483] Algo bellman_ford step 3926 current loss 0.036736, current_train_items 125664.
I0304 19:30:05.539736 23128000471168 run.py:483] Algo bellman_ford step 3927 current loss 0.061776, current_train_items 125696.
I0304 19:30:05.570749 23128000471168 run.py:483] Algo bellman_ford step 3928 current loss 0.121398, current_train_items 125728.
I0304 19:30:05.604234 23128000471168 run.py:483] Algo bellman_ford step 3929 current loss 0.096515, current_train_items 125760.
I0304 19:30:05.623868 23128000471168 run.py:483] Algo bellman_ford step 3930 current loss 0.001387, current_train_items 125792.
I0304 19:30:05.640035 23128000471168 run.py:483] Algo bellman_ford step 3931 current loss 0.041442, current_train_items 125824.
I0304 19:30:05.664159 23128000471168 run.py:483] Algo bellman_ford step 3932 current loss 0.070595, current_train_items 125856.
I0304 19:30:05.696888 23128000471168 run.py:483] Algo bellman_ford step 3933 current loss 0.067203, current_train_items 125888.
I0304 19:30:05.730881 23128000471168 run.py:483] Algo bellman_ford step 3934 current loss 0.089222, current_train_items 125920.
I0304 19:30:05.751346 23128000471168 run.py:483] Algo bellman_ford step 3935 current loss 0.018665, current_train_items 125952.
I0304 19:30:05.768223 23128000471168 run.py:483] Algo bellman_ford step 3936 current loss 0.027689, current_train_items 125984.
I0304 19:30:05.792171 23128000471168 run.py:483] Algo bellman_ford step 3937 current loss 0.035552, current_train_items 126016.
I0304 19:30:05.823901 23128000471168 run.py:483] Algo bellman_ford step 3938 current loss 0.029486, current_train_items 126048.
I0304 19:30:05.858827 23128000471168 run.py:483] Algo bellman_ford step 3939 current loss 0.076727, current_train_items 126080.
I0304 19:30:05.878789 23128000471168 run.py:483] Algo bellman_ford step 3940 current loss 0.002668, current_train_items 126112.
I0304 19:30:05.895149 23128000471168 run.py:483] Algo bellman_ford step 3941 current loss 0.028770, current_train_items 126144.
I0304 19:30:05.919360 23128000471168 run.py:483] Algo bellman_ford step 3942 current loss 0.061512, current_train_items 126176.
I0304 19:30:05.949944 23128000471168 run.py:483] Algo bellman_ford step 3943 current loss 0.042741, current_train_items 126208.
I0304 19:30:05.984506 23128000471168 run.py:483] Algo bellman_ford step 3944 current loss 0.094787, current_train_items 126240.
I0304 19:30:06.004336 23128000471168 run.py:483] Algo bellman_ford step 3945 current loss 0.009318, current_train_items 126272.
I0304 19:30:06.021054 23128000471168 run.py:483] Algo bellman_ford step 3946 current loss 0.013918, current_train_items 126304.
I0304 19:30:06.045203 23128000471168 run.py:483] Algo bellman_ford step 3947 current loss 0.057507, current_train_items 126336.
I0304 19:30:06.075051 23128000471168 run.py:483] Algo bellman_ford step 3948 current loss 0.083944, current_train_items 126368.
I0304 19:30:06.109546 23128000471168 run.py:483] Algo bellman_ford step 3949 current loss 0.058335, current_train_items 126400.
I0304 19:30:06.129476 23128000471168 run.py:483] Algo bellman_ford step 3950 current loss 0.002462, current_train_items 126432.
I0304 19:30:06.137547 23128000471168 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0304 19:30:06.137656 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:06.155187 23128000471168 run.py:483] Algo bellman_ford step 3951 current loss 0.021657, current_train_items 126464.
I0304 19:30:06.180398 23128000471168 run.py:483] Algo bellman_ford step 3952 current loss 0.044574, current_train_items 126496.
I0304 19:30:06.212095 23128000471168 run.py:483] Algo bellman_ford step 3953 current loss 0.034962, current_train_items 126528.
I0304 19:30:06.244750 23128000471168 run.py:483] Algo bellman_ford step 3954 current loss 0.054316, current_train_items 126560.
I0304 19:30:06.265212 23128000471168 run.py:483] Algo bellman_ford step 3955 current loss 0.003480, current_train_items 126592.
I0304 19:30:06.281416 23128000471168 run.py:483] Algo bellman_ford step 3956 current loss 0.019303, current_train_items 126624.
I0304 19:30:06.306178 23128000471168 run.py:483] Algo bellman_ford step 3957 current loss 0.036123, current_train_items 126656.
I0304 19:30:06.338735 23128000471168 run.py:483] Algo bellman_ford step 3958 current loss 0.069590, current_train_items 126688.
I0304 19:30:06.374828 23128000471168 run.py:483] Algo bellman_ford step 3959 current loss 0.072476, current_train_items 126720.
I0304 19:30:06.395230 23128000471168 run.py:483] Algo bellman_ford step 3960 current loss 0.012186, current_train_items 126752.
I0304 19:30:06.411545 23128000471168 run.py:483] Algo bellman_ford step 3961 current loss 0.023330, current_train_items 126784.
I0304 19:30:06.434311 23128000471168 run.py:483] Algo bellman_ford step 3962 current loss 0.027268, current_train_items 126816.
I0304 19:30:06.466428 23128000471168 run.py:483] Algo bellman_ford step 3963 current loss 0.064576, current_train_items 126848.
I0304 19:30:06.501095 23128000471168 run.py:483] Algo bellman_ford step 3964 current loss 0.085336, current_train_items 126880.
I0304 19:30:06.520951 23128000471168 run.py:483] Algo bellman_ford step 3965 current loss 0.004315, current_train_items 126912.
I0304 19:30:06.537473 23128000471168 run.py:483] Algo bellman_ford step 3966 current loss 0.028037, current_train_items 126944.
I0304 19:30:06.563110 23128000471168 run.py:483] Algo bellman_ford step 3967 current loss 0.039952, current_train_items 126976.
I0304 19:30:06.595139 23128000471168 run.py:483] Algo bellman_ford step 3968 current loss 0.061777, current_train_items 127008.
I0304 19:30:06.628297 23128000471168 run.py:483] Algo bellman_ford step 3969 current loss 0.062163, current_train_items 127040.
I0304 19:30:06.648617 23128000471168 run.py:483] Algo bellman_ford step 3970 current loss 0.002474, current_train_items 127072.
I0304 19:30:06.665155 23128000471168 run.py:483] Algo bellman_ford step 3971 current loss 0.011193, current_train_items 127104.
I0304 19:30:06.689844 23128000471168 run.py:483] Algo bellman_ford step 3972 current loss 0.037041, current_train_items 127136.
I0304 19:30:06.722701 23128000471168 run.py:483] Algo bellman_ford step 3973 current loss 0.084263, current_train_items 127168.
I0304 19:30:06.755427 23128000471168 run.py:483] Algo bellman_ford step 3974 current loss 0.069843, current_train_items 127200.
I0304 19:30:06.775842 23128000471168 run.py:483] Algo bellman_ford step 3975 current loss 0.002015, current_train_items 127232.
I0304 19:30:06.792659 23128000471168 run.py:483] Algo bellman_ford step 3976 current loss 0.041517, current_train_items 127264.
I0304 19:30:06.816475 23128000471168 run.py:483] Algo bellman_ford step 3977 current loss 0.037487, current_train_items 127296.
I0304 19:30:06.848830 23128000471168 run.py:483] Algo bellman_ford step 3978 current loss 0.069073, current_train_items 127328.
I0304 19:30:06.882393 23128000471168 run.py:483] Algo bellman_ford step 3979 current loss 0.063432, current_train_items 127360.
I0304 19:30:06.902255 23128000471168 run.py:483] Algo bellman_ford step 3980 current loss 0.001956, current_train_items 127392.
I0304 19:30:06.918590 23128000471168 run.py:483] Algo bellman_ford step 3981 current loss 0.023451, current_train_items 127424.
I0304 19:30:06.942553 23128000471168 run.py:483] Algo bellman_ford step 3982 current loss 0.050150, current_train_items 127456.
I0304 19:30:06.974776 23128000471168 run.py:483] Algo bellman_ford step 3983 current loss 0.073511, current_train_items 127488.
I0304 19:30:07.007515 23128000471168 run.py:483] Algo bellman_ford step 3984 current loss 0.057464, current_train_items 127520.
I0304 19:30:07.028210 23128000471168 run.py:483] Algo bellman_ford step 3985 current loss 0.002633, current_train_items 127552.
I0304 19:30:07.045238 23128000471168 run.py:483] Algo bellman_ford step 3986 current loss 0.044048, current_train_items 127584.
I0304 19:30:07.069202 23128000471168 run.py:483] Algo bellman_ford step 3987 current loss 0.067287, current_train_items 127616.
I0304 19:30:07.101366 23128000471168 run.py:483] Algo bellman_ford step 3988 current loss 0.093291, current_train_items 127648.
I0304 19:30:07.135985 23128000471168 run.py:483] Algo bellman_ford step 3989 current loss 0.079611, current_train_items 127680.
I0304 19:30:07.156761 23128000471168 run.py:483] Algo bellman_ford step 3990 current loss 0.012782, current_train_items 127712.
I0304 19:30:07.173786 23128000471168 run.py:483] Algo bellman_ford step 3991 current loss 0.039665, current_train_items 127744.
I0304 19:30:07.197172 23128000471168 run.py:483] Algo bellman_ford step 3992 current loss 0.053528, current_train_items 127776.
I0304 19:30:07.228242 23128000471168 run.py:483] Algo bellman_ford step 3993 current loss 0.052233, current_train_items 127808.
I0304 19:30:07.262495 23128000471168 run.py:483] Algo bellman_ford step 3994 current loss 0.102584, current_train_items 127840.
I0304 19:30:07.282727 23128000471168 run.py:483] Algo bellman_ford step 3995 current loss 0.007539, current_train_items 127872.
I0304 19:30:07.299362 23128000471168 run.py:483] Algo bellman_ford step 3996 current loss 0.019681, current_train_items 127904.
I0304 19:30:07.324101 23128000471168 run.py:483] Algo bellman_ford step 3997 current loss 0.060220, current_train_items 127936.
I0304 19:30:07.355182 23128000471168 run.py:483] Algo bellman_ford step 3998 current loss 0.081771, current_train_items 127968.
I0304 19:30:07.390233 23128000471168 run.py:483] Algo bellman_ford step 3999 current loss 0.052525, current_train_items 128000.
I0304 19:30:07.410886 23128000471168 run.py:483] Algo bellman_ford step 4000 current loss 0.005011, current_train_items 128032.
I0304 19:30:07.418522 23128000471168 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0304 19:30:07.418632 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:30:07.436022 23128000471168 run.py:483] Algo bellman_ford step 4001 current loss 0.029806, current_train_items 128064.
I0304 19:30:07.460773 23128000471168 run.py:483] Algo bellman_ford step 4002 current loss 0.062961, current_train_items 128096.
I0304 19:30:07.492485 23128000471168 run.py:483] Algo bellman_ford step 4003 current loss 0.084717, current_train_items 128128.
I0304 19:30:07.525992 23128000471168 run.py:483] Algo bellman_ford step 4004 current loss 0.099907, current_train_items 128160.
I0304 19:30:07.546720 23128000471168 run.py:483] Algo bellman_ford step 4005 current loss 0.002507, current_train_items 128192.
I0304 19:30:07.563071 23128000471168 run.py:483] Algo bellman_ford step 4006 current loss 0.024416, current_train_items 128224.
I0304 19:30:07.587602 23128000471168 run.py:483] Algo bellman_ford step 4007 current loss 0.180703, current_train_items 128256.
I0304 19:30:07.620064 23128000471168 run.py:483] Algo bellman_ford step 4008 current loss 0.079303, current_train_items 128288.
I0304 19:30:07.652590 23128000471168 run.py:483] Algo bellman_ford step 4009 current loss 0.105833, current_train_items 128320.
I0304 19:30:07.673156 23128000471168 run.py:483] Algo bellman_ford step 4010 current loss 0.004526, current_train_items 128352.
I0304 19:30:07.689572 23128000471168 run.py:483] Algo bellman_ford step 4011 current loss 0.017166, current_train_items 128384.
I0304 19:30:07.712887 23128000471168 run.py:483] Algo bellman_ford step 4012 current loss 0.041829, current_train_items 128416.
I0304 19:30:07.744771 23128000471168 run.py:483] Algo bellman_ford step 4013 current loss 0.104370, current_train_items 128448.
I0304 19:30:07.779190 23128000471168 run.py:483] Algo bellman_ford step 4014 current loss 0.107994, current_train_items 128480.
I0304 19:30:07.799487 23128000471168 run.py:483] Algo bellman_ford step 4015 current loss 0.003101, current_train_items 128512.
I0304 19:30:07.815927 23128000471168 run.py:483] Algo bellman_ford step 4016 current loss 0.025996, current_train_items 128544.
I0304 19:30:07.840219 23128000471168 run.py:483] Algo bellman_ford step 4017 current loss 0.039693, current_train_items 128576.
I0304 19:30:07.871792 23128000471168 run.py:483] Algo bellman_ford step 4018 current loss 0.076400, current_train_items 128608.
I0304 19:30:07.905054 23128000471168 run.py:483] Algo bellman_ford step 4019 current loss 0.065823, current_train_items 128640.
I0304 19:30:07.925190 23128000471168 run.py:483] Algo bellman_ford step 4020 current loss 0.002584, current_train_items 128672.
I0304 19:30:07.941455 23128000471168 run.py:483] Algo bellman_ford step 4021 current loss 0.017331, current_train_items 128704.
I0304 19:30:07.966298 23128000471168 run.py:483] Algo bellman_ford step 4022 current loss 0.039854, current_train_items 128736.
I0304 19:30:07.997128 23128000471168 run.py:483] Algo bellman_ford step 4023 current loss 0.048287, current_train_items 128768.
I0304 19:30:08.034267 23128000471168 run.py:483] Algo bellman_ford step 4024 current loss 0.167381, current_train_items 128800.
I0304 19:30:08.054551 23128000471168 run.py:483] Algo bellman_ford step 4025 current loss 0.002551, current_train_items 128832.
I0304 19:30:08.071362 23128000471168 run.py:483] Algo bellman_ford step 4026 current loss 0.014536, current_train_items 128864.
I0304 19:30:08.095189 23128000471168 run.py:483] Algo bellman_ford step 4027 current loss 0.022950, current_train_items 128896.
I0304 19:30:08.126181 23128000471168 run.py:483] Algo bellman_ford step 4028 current loss 0.056539, current_train_items 128928.
I0304 19:30:08.159342 23128000471168 run.py:483] Algo bellman_ford step 4029 current loss 0.051848, current_train_items 128960.
I0304 19:30:08.179489 23128000471168 run.py:483] Algo bellman_ford step 4030 current loss 0.002794, current_train_items 128992.
I0304 19:30:08.195654 23128000471168 run.py:483] Algo bellman_ford step 4031 current loss 0.027462, current_train_items 129024.
I0304 19:30:08.220122 23128000471168 run.py:483] Algo bellman_ford step 4032 current loss 0.040418, current_train_items 129056.
I0304 19:30:08.252434 23128000471168 run.py:483] Algo bellman_ford step 4033 current loss 0.062565, current_train_items 129088.
I0304 19:30:08.285674 23128000471168 run.py:483] Algo bellman_ford step 4034 current loss 0.085513, current_train_items 129120.
I0304 19:30:08.305898 23128000471168 run.py:483] Algo bellman_ford step 4035 current loss 0.002555, current_train_items 129152.
I0304 19:30:08.322407 23128000471168 run.py:483] Algo bellman_ford step 4036 current loss 0.028188, current_train_items 129184.
I0304 19:30:08.346931 23128000471168 run.py:483] Algo bellman_ford step 4037 current loss 0.054399, current_train_items 129216.
I0304 19:30:08.379017 23128000471168 run.py:483] Algo bellman_ford step 4038 current loss 0.065617, current_train_items 129248.
I0304 19:30:08.413618 23128000471168 run.py:483] Algo bellman_ford step 4039 current loss 0.081380, current_train_items 129280.
I0304 19:30:08.433564 23128000471168 run.py:483] Algo bellman_ford step 4040 current loss 0.004420, current_train_items 129312.
I0304 19:30:08.450316 23128000471168 run.py:483] Algo bellman_ford step 4041 current loss 0.016164, current_train_items 129344.
I0304 19:30:08.474186 23128000471168 run.py:483] Algo bellman_ford step 4042 current loss 0.090967, current_train_items 129376.
I0304 19:30:08.506859 23128000471168 run.py:483] Algo bellman_ford step 4043 current loss 0.155872, current_train_items 129408.
I0304 19:30:08.539676 23128000471168 run.py:483] Algo bellman_ford step 4044 current loss 0.063707, current_train_items 129440.
I0304 19:30:08.559774 23128000471168 run.py:483] Algo bellman_ford step 4045 current loss 0.009094, current_train_items 129472.
I0304 19:30:08.576556 23128000471168 run.py:483] Algo bellman_ford step 4046 current loss 0.059808, current_train_items 129504.
I0304 19:30:08.600437 23128000471168 run.py:483] Algo bellman_ford step 4047 current loss 0.092542, current_train_items 129536.
I0304 19:30:08.631191 23128000471168 run.py:483] Algo bellman_ford step 4048 current loss 0.077694, current_train_items 129568.
I0304 19:30:08.666248 23128000471168 run.py:483] Algo bellman_ford step 4049 current loss 0.076836, current_train_items 129600.
I0304 19:30:08.686069 23128000471168 run.py:483] Algo bellman_ford step 4050 current loss 0.003130, current_train_items 129632.
I0304 19:30:08.694158 23128000471168 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0304 19:30:08.694266 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:30:08.711655 23128000471168 run.py:483] Algo bellman_ford step 4051 current loss 0.019842, current_train_items 129664.
I0304 19:30:08.735556 23128000471168 run.py:483] Algo bellman_ford step 4052 current loss 0.078404, current_train_items 129696.
I0304 19:30:08.767420 23128000471168 run.py:483] Algo bellman_ford step 4053 current loss 0.138173, current_train_items 129728.
I0304 19:30:08.803463 23128000471168 run.py:483] Algo bellman_ford step 4054 current loss 0.165416, current_train_items 129760.
I0304 19:30:08.823654 23128000471168 run.py:483] Algo bellman_ford step 4055 current loss 0.002853, current_train_items 129792.
I0304 19:30:08.839699 23128000471168 run.py:483] Algo bellman_ford step 4056 current loss 0.013406, current_train_items 129824.
I0304 19:30:08.863981 23128000471168 run.py:483] Algo bellman_ford step 4057 current loss 0.056494, current_train_items 129856.
I0304 19:30:08.894737 23128000471168 run.py:483] Algo bellman_ford step 4058 current loss 0.061887, current_train_items 129888.
I0304 19:30:08.926979 23128000471168 run.py:483] Algo bellman_ford step 4059 current loss 0.072994, current_train_items 129920.
I0304 19:30:08.947275 23128000471168 run.py:483] Algo bellman_ford step 4060 current loss 0.004973, current_train_items 129952.
I0304 19:30:08.964468 23128000471168 run.py:483] Algo bellman_ford step 4061 current loss 0.022075, current_train_items 129984.
I0304 19:30:08.987563 23128000471168 run.py:483] Algo bellman_ford step 4062 current loss 0.042056, current_train_items 130016.
I0304 19:30:09.017625 23128000471168 run.py:483] Algo bellman_ford step 4063 current loss 0.046298, current_train_items 130048.
I0304 19:30:09.052660 23128000471168 run.py:483] Algo bellman_ford step 4064 current loss 0.098785, current_train_items 130080.
I0304 19:30:09.072558 23128000471168 run.py:483] Algo bellman_ford step 4065 current loss 0.001461, current_train_items 130112.
I0304 19:30:09.088582 23128000471168 run.py:483] Algo bellman_ford step 4066 current loss 0.087543, current_train_items 130144.
I0304 19:30:09.113726 23128000471168 run.py:483] Algo bellman_ford step 4067 current loss 0.058598, current_train_items 130176.
I0304 19:30:09.144916 23128000471168 run.py:483] Algo bellman_ford step 4068 current loss 0.045213, current_train_items 130208.
I0304 19:30:09.179999 23128000471168 run.py:483] Algo bellman_ford step 4069 current loss 0.077500, current_train_items 130240.
I0304 19:30:09.200126 23128000471168 run.py:483] Algo bellman_ford step 4070 current loss 0.003943, current_train_items 130272.
I0304 19:30:09.216801 23128000471168 run.py:483] Algo bellman_ford step 4071 current loss 0.081727, current_train_items 130304.
I0304 19:30:09.240353 23128000471168 run.py:483] Algo bellman_ford step 4072 current loss 0.069704, current_train_items 130336.
I0304 19:30:09.270677 23128000471168 run.py:483] Algo bellman_ford step 4073 current loss 0.074044, current_train_items 130368.
I0304 19:30:09.303691 23128000471168 run.py:483] Algo bellman_ford step 4074 current loss 0.060851, current_train_items 130400.
I0304 19:30:09.323972 23128000471168 run.py:483] Algo bellman_ford step 4075 current loss 0.008682, current_train_items 130432.
I0304 19:30:09.341218 23128000471168 run.py:483] Algo bellman_ford step 4076 current loss 0.028526, current_train_items 130464.
I0304 19:30:09.365689 23128000471168 run.py:483] Algo bellman_ford step 4077 current loss 0.070601, current_train_items 130496.
I0304 19:30:09.396625 23128000471168 run.py:483] Algo bellman_ford step 4078 current loss 0.045058, current_train_items 130528.
I0304 19:30:09.429784 23128000471168 run.py:483] Algo bellman_ford step 4079 current loss 0.081683, current_train_items 130560.
I0304 19:30:09.449681 23128000471168 run.py:483] Algo bellman_ford step 4080 current loss 0.002319, current_train_items 130592.
I0304 19:30:09.466434 23128000471168 run.py:483] Algo bellman_ford step 4081 current loss 0.016693, current_train_items 130624.
I0304 19:30:09.491377 23128000471168 run.py:483] Algo bellman_ford step 4082 current loss 0.094134, current_train_items 130656.
I0304 19:30:09.524056 23128000471168 run.py:483] Algo bellman_ford step 4083 current loss 0.127809, current_train_items 130688.
I0304 19:30:09.555460 23128000471168 run.py:483] Algo bellman_ford step 4084 current loss 0.073273, current_train_items 130720.
I0304 19:30:09.576102 23128000471168 run.py:483] Algo bellman_ford step 4085 current loss 0.025505, current_train_items 130752.
I0304 19:30:09.592743 23128000471168 run.py:483] Algo bellman_ford step 4086 current loss 0.020745, current_train_items 130784.
I0304 19:30:09.615941 23128000471168 run.py:483] Algo bellman_ford step 4087 current loss 0.088033, current_train_items 130816.
I0304 19:30:09.646958 23128000471168 run.py:483] Algo bellman_ford step 4088 current loss 0.052108, current_train_items 130848.
I0304 19:30:09.679062 23128000471168 run.py:483] Algo bellman_ford step 4089 current loss 0.055597, current_train_items 130880.
I0304 19:30:09.699337 23128000471168 run.py:483] Algo bellman_ford step 4090 current loss 0.004680, current_train_items 130912.
I0304 19:30:09.715835 23128000471168 run.py:483] Algo bellman_ford step 4091 current loss 0.016502, current_train_items 130944.
I0304 19:30:09.740239 23128000471168 run.py:483] Algo bellman_ford step 4092 current loss 0.086745, current_train_items 130976.
I0304 19:30:09.772423 23128000471168 run.py:483] Algo bellman_ford step 4093 current loss 0.102757, current_train_items 131008.
I0304 19:30:09.806397 23128000471168 run.py:483] Algo bellman_ford step 4094 current loss 0.078502, current_train_items 131040.
I0304 19:30:09.826197 23128000471168 run.py:483] Algo bellman_ford step 4095 current loss 0.004672, current_train_items 131072.
I0304 19:30:09.842917 23128000471168 run.py:483] Algo bellman_ford step 4096 current loss 0.055529, current_train_items 131104.
I0304 19:30:09.867341 23128000471168 run.py:483] Algo bellman_ford step 4097 current loss 0.129973, current_train_items 131136.
I0304 19:30:09.898666 23128000471168 run.py:483] Algo bellman_ford step 4098 current loss 0.146099, current_train_items 131168.
I0304 19:30:09.933038 23128000471168 run.py:483] Algo bellman_ford step 4099 current loss 0.244642, current_train_items 131200.
I0304 19:30:09.953497 23128000471168 run.py:483] Algo bellman_ford step 4100 current loss 0.006685, current_train_items 131232.
I0304 19:30:09.961281 23128000471168 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0304 19:30:09.961389 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:30:09.977961 23128000471168 run.py:483] Algo bellman_ford step 4101 current loss 0.014012, current_train_items 131264.
I0304 19:30:10.000687 23128000471168 run.py:483] Algo bellman_ford step 4102 current loss 0.032946, current_train_items 131296.
I0304 19:30:10.033367 23128000471168 run.py:483] Algo bellman_ford step 4103 current loss 0.099491, current_train_items 131328.
I0304 19:30:10.068839 23128000471168 run.py:483] Algo bellman_ford step 4104 current loss 0.089155, current_train_items 131360.
I0304 19:30:10.088739 23128000471168 run.py:483] Algo bellman_ford step 4105 current loss 0.006520, current_train_items 131392.
I0304 19:30:10.105154 23128000471168 run.py:483] Algo bellman_ford step 4106 current loss 0.041026, current_train_items 131424.
I0304 19:30:10.129771 23128000471168 run.py:483] Algo bellman_ford step 4107 current loss 0.074234, current_train_items 131456.
I0304 19:30:10.161161 23128000471168 run.py:483] Algo bellman_ford step 4108 current loss 0.100752, current_train_items 131488.
I0304 19:30:10.196976 23128000471168 run.py:483] Algo bellman_ford step 4109 current loss 0.123429, current_train_items 131520.
I0304 19:30:10.216681 23128000471168 run.py:483] Algo bellman_ford step 4110 current loss 0.010870, current_train_items 131552.
I0304 19:30:10.233577 23128000471168 run.py:483] Algo bellman_ford step 4111 current loss 0.020281, current_train_items 131584.
I0304 19:30:10.256708 23128000471168 run.py:483] Algo bellman_ford step 4112 current loss 0.070922, current_train_items 131616.
I0304 19:30:10.289123 23128000471168 run.py:483] Algo bellman_ford step 4113 current loss 0.056940, current_train_items 131648.
I0304 19:30:10.323797 23128000471168 run.py:483] Algo bellman_ford step 4114 current loss 0.072863, current_train_items 131680.
I0304 19:30:10.343627 23128000471168 run.py:483] Algo bellman_ford step 4115 current loss 0.005431, current_train_items 131712.
I0304 19:30:10.360324 23128000471168 run.py:483] Algo bellman_ford step 4116 current loss 0.036772, current_train_items 131744.
I0304 19:30:10.385457 23128000471168 run.py:483] Algo bellman_ford step 4117 current loss 0.048643, current_train_items 131776.
I0304 19:30:10.416453 23128000471168 run.py:483] Algo bellman_ford step 4118 current loss 0.039238, current_train_items 131808.
I0304 19:30:10.450393 23128000471168 run.py:483] Algo bellman_ford step 4119 current loss 0.077047, current_train_items 131840.
I0304 19:30:10.470085 23128000471168 run.py:483] Algo bellman_ford step 4120 current loss 0.011050, current_train_items 131872.
I0304 19:30:10.486588 23128000471168 run.py:483] Algo bellman_ford step 4121 current loss 0.005305, current_train_items 131904.
I0304 19:30:10.511669 23128000471168 run.py:483] Algo bellman_ford step 4122 current loss 0.036169, current_train_items 131936.
I0304 19:30:10.544167 23128000471168 run.py:483] Algo bellman_ford step 4123 current loss 0.093392, current_train_items 131968.
I0304 19:30:10.578118 23128000471168 run.py:483] Algo bellman_ford step 4124 current loss 0.068142, current_train_items 132000.
I0304 19:30:10.597826 23128000471168 run.py:483] Algo bellman_ford step 4125 current loss 0.003028, current_train_items 132032.
I0304 19:30:10.614308 23128000471168 run.py:483] Algo bellman_ford step 4126 current loss 0.057706, current_train_items 132064.
I0304 19:30:10.639467 23128000471168 run.py:483] Algo bellman_ford step 4127 current loss 0.060829, current_train_items 132096.
I0304 19:30:10.670404 23128000471168 run.py:483] Algo bellman_ford step 4128 current loss 0.035881, current_train_items 132128.
I0304 19:30:10.703063 23128000471168 run.py:483] Algo bellman_ford step 4129 current loss 0.064223, current_train_items 132160.
I0304 19:30:10.722897 23128000471168 run.py:483] Algo bellman_ford step 4130 current loss 0.003114, current_train_items 132192.
I0304 19:30:10.739517 23128000471168 run.py:483] Algo bellman_ford step 4131 current loss 0.033385, current_train_items 132224.
I0304 19:30:10.763755 23128000471168 run.py:483] Algo bellman_ford step 4132 current loss 0.057540, current_train_items 132256.
I0304 19:30:10.795962 23128000471168 run.py:483] Algo bellman_ford step 4133 current loss 0.071918, current_train_items 132288.
I0304 19:30:10.831067 23128000471168 run.py:483] Algo bellman_ford step 4134 current loss 0.056785, current_train_items 132320.
I0304 19:30:10.851018 23128000471168 run.py:483] Algo bellman_ford step 4135 current loss 0.003141, current_train_items 132352.
I0304 19:30:10.867549 23128000471168 run.py:483] Algo bellman_ford step 4136 current loss 0.014977, current_train_items 132384.
I0304 19:30:10.891499 23128000471168 run.py:483] Algo bellman_ford step 4137 current loss 0.048430, current_train_items 132416.
I0304 19:30:10.922551 23128000471168 run.py:483] Algo bellman_ford step 4138 current loss 0.041390, current_train_items 132448.
I0304 19:30:10.957165 23128000471168 run.py:483] Algo bellman_ford step 4139 current loss 0.077037, current_train_items 132480.
I0304 19:30:10.977159 23128000471168 run.py:483] Algo bellman_ford step 4140 current loss 0.002181, current_train_items 132512.
I0304 19:30:10.993200 23128000471168 run.py:483] Algo bellman_ford step 4141 current loss 0.010373, current_train_items 132544.
I0304 19:30:11.018543 23128000471168 run.py:483] Algo bellman_ford step 4142 current loss 0.055188, current_train_items 132576.
I0304 19:30:11.049376 23128000471168 run.py:483] Algo bellman_ford step 4143 current loss 0.058578, current_train_items 132608.
I0304 19:30:11.082612 23128000471168 run.py:483] Algo bellman_ford step 4144 current loss 0.075835, current_train_items 132640.
I0304 19:30:11.102509 23128000471168 run.py:483] Algo bellman_ford step 4145 current loss 0.002078, current_train_items 132672.
I0304 19:30:11.118673 23128000471168 run.py:483] Algo bellman_ford step 4146 current loss 0.006685, current_train_items 132704.
I0304 19:30:11.142482 23128000471168 run.py:483] Algo bellman_ford step 4147 current loss 0.065878, current_train_items 132736.
I0304 19:30:11.172368 23128000471168 run.py:483] Algo bellman_ford step 4148 current loss 0.038463, current_train_items 132768.
I0304 19:30:11.205398 23128000471168 run.py:483] Algo bellman_ford step 4149 current loss 0.074393, current_train_items 132800.
I0304 19:30:11.225349 23128000471168 run.py:483] Algo bellman_ford step 4150 current loss 0.002416, current_train_items 132832.
I0304 19:30:11.233574 23128000471168 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0304 19:30:11.233682 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:11.250922 23128000471168 run.py:483] Algo bellman_ford step 4151 current loss 0.043641, current_train_items 132864.
I0304 19:30:11.275998 23128000471168 run.py:483] Algo bellman_ford step 4152 current loss 0.061558, current_train_items 132896.
I0304 19:30:11.307994 23128000471168 run.py:483] Algo bellman_ford step 4153 current loss 0.050702, current_train_items 132928.
I0304 19:30:11.343724 23128000471168 run.py:483] Algo bellman_ford step 4154 current loss 0.072462, current_train_items 132960.
I0304 19:30:11.364241 23128000471168 run.py:483] Algo bellman_ford step 4155 current loss 0.007771, current_train_items 132992.
I0304 19:30:11.380966 23128000471168 run.py:483] Algo bellman_ford step 4156 current loss 0.024063, current_train_items 133024.
I0304 19:30:11.404427 23128000471168 run.py:483] Algo bellman_ford step 4157 current loss 0.046869, current_train_items 133056.
I0304 19:30:11.435590 23128000471168 run.py:483] Algo bellman_ford step 4158 current loss 0.047022, current_train_items 133088.
I0304 19:30:11.469635 23128000471168 run.py:483] Algo bellman_ford step 4159 current loss 0.070620, current_train_items 133120.
I0304 19:30:11.490502 23128000471168 run.py:483] Algo bellman_ford step 4160 current loss 0.003438, current_train_items 133152.
I0304 19:30:11.507302 23128000471168 run.py:483] Algo bellman_ford step 4161 current loss 0.047051, current_train_items 133184.
I0304 19:30:11.529962 23128000471168 run.py:483] Algo bellman_ford step 4162 current loss 0.094765, current_train_items 133216.
I0304 19:30:11.562022 23128000471168 run.py:483] Algo bellman_ford step 4163 current loss 0.059252, current_train_items 133248.
I0304 19:30:11.596791 23128000471168 run.py:483] Algo bellman_ford step 4164 current loss 0.050890, current_train_items 133280.
I0304 19:30:11.616833 23128000471168 run.py:483] Algo bellman_ford step 4165 current loss 0.005335, current_train_items 133312.
I0304 19:30:11.633196 23128000471168 run.py:483] Algo bellman_ford step 4166 current loss 0.012898, current_train_items 133344.
I0304 19:30:11.658230 23128000471168 run.py:483] Algo bellman_ford step 4167 current loss 0.096833, current_train_items 133376.
I0304 19:30:11.689647 23128000471168 run.py:483] Algo bellman_ford step 4168 current loss 0.047795, current_train_items 133408.
I0304 19:30:11.722932 23128000471168 run.py:483] Algo bellman_ford step 4169 current loss 0.065864, current_train_items 133440.
I0304 19:30:11.743241 23128000471168 run.py:483] Algo bellman_ford step 4170 current loss 0.001712, current_train_items 133472.
I0304 19:30:11.759313 23128000471168 run.py:483] Algo bellman_ford step 4171 current loss 0.006743, current_train_items 133504.
I0304 19:30:11.783085 23128000471168 run.py:483] Algo bellman_ford step 4172 current loss 0.050566, current_train_items 133536.
I0304 19:30:11.814435 23128000471168 run.py:483] Algo bellman_ford step 4173 current loss 0.093556, current_train_items 133568.
I0304 19:30:11.850472 23128000471168 run.py:483] Algo bellman_ford step 4174 current loss 0.083621, current_train_items 133600.
I0304 19:30:11.870938 23128000471168 run.py:483] Algo bellman_ford step 4175 current loss 0.004942, current_train_items 133632.
I0304 19:30:11.888146 23128000471168 run.py:483] Algo bellman_ford step 4176 current loss 0.012573, current_train_items 133664.
I0304 19:30:11.912499 23128000471168 run.py:483] Algo bellman_ford step 4177 current loss 0.043470, current_train_items 133696.
I0304 19:30:11.944450 23128000471168 run.py:483] Algo bellman_ford step 4178 current loss 0.087060, current_train_items 133728.
I0304 19:30:11.978995 23128000471168 run.py:483] Algo bellman_ford step 4179 current loss 0.136904, current_train_items 133760.
I0304 19:30:11.999221 23128000471168 run.py:483] Algo bellman_ford step 4180 current loss 0.006161, current_train_items 133792.
I0304 19:30:12.015972 23128000471168 run.py:483] Algo bellman_ford step 4181 current loss 0.026973, current_train_items 133824.
I0304 19:30:12.040944 23128000471168 run.py:483] Algo bellman_ford step 4182 current loss 0.058824, current_train_items 133856.
I0304 19:30:12.071044 23128000471168 run.py:483] Algo bellman_ford step 4183 current loss 0.039833, current_train_items 133888.
I0304 19:30:12.105768 23128000471168 run.py:483] Algo bellman_ford step 4184 current loss 0.095431, current_train_items 133920.
I0304 19:30:12.126137 23128000471168 run.py:483] Algo bellman_ford step 4185 current loss 0.008815, current_train_items 133952.
I0304 19:30:12.142593 23128000471168 run.py:483] Algo bellman_ford step 4186 current loss 0.048598, current_train_items 133984.
I0304 19:30:12.166357 23128000471168 run.py:483] Algo bellman_ford step 4187 current loss 0.039904, current_train_items 134016.
I0304 19:30:12.197615 23128000471168 run.py:483] Algo bellman_ford step 4188 current loss 0.046891, current_train_items 134048.
I0304 19:30:12.230827 23128000471168 run.py:483] Algo bellman_ford step 4189 current loss 0.059591, current_train_items 134080.
I0304 19:30:12.251260 23128000471168 run.py:483] Algo bellman_ford step 4190 current loss 0.005012, current_train_items 134112.
I0304 19:30:12.268367 23128000471168 run.py:483] Algo bellman_ford step 4191 current loss 0.022853, current_train_items 134144.
I0304 19:30:12.290333 23128000471168 run.py:483] Algo bellman_ford step 4192 current loss 0.044881, current_train_items 134176.
I0304 19:30:12.322557 23128000471168 run.py:483] Algo bellman_ford step 4193 current loss 0.089275, current_train_items 134208.
I0304 19:30:12.358911 23128000471168 run.py:483] Algo bellman_ford step 4194 current loss 0.093626, current_train_items 134240.
I0304 19:30:12.378794 23128000471168 run.py:483] Algo bellman_ford step 4195 current loss 0.002356, current_train_items 134272.
I0304 19:30:12.395104 23128000471168 run.py:483] Algo bellman_ford step 4196 current loss 0.018607, current_train_items 134304.
I0304 19:30:12.419859 23128000471168 run.py:483] Algo bellman_ford step 4197 current loss 0.060197, current_train_items 134336.
I0304 19:30:12.450895 23128000471168 run.py:483] Algo bellman_ford step 4198 current loss 0.034174, current_train_items 134368.
I0304 19:30:12.484714 23128000471168 run.py:483] Algo bellman_ford step 4199 current loss 0.055290, current_train_items 134400.
I0304 19:30:12.504858 23128000471168 run.py:483] Algo bellman_ford step 4200 current loss 0.002196, current_train_items 134432.
I0304 19:30:12.512513 23128000471168 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0304 19:30:12.512623 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:12.529902 23128000471168 run.py:483] Algo bellman_ford step 4201 current loss 0.025268, current_train_items 134464.
I0304 19:30:12.554228 23128000471168 run.py:483] Algo bellman_ford step 4202 current loss 0.042885, current_train_items 134496.
I0304 19:30:12.585933 23128000471168 run.py:483] Algo bellman_ford step 4203 current loss 0.080612, current_train_items 134528.
I0304 19:30:12.619908 23128000471168 run.py:483] Algo bellman_ford step 4204 current loss 0.092967, current_train_items 134560.
I0304 19:30:12.640027 23128000471168 run.py:483] Algo bellman_ford step 4205 current loss 0.001507, current_train_items 134592.
I0304 19:30:12.656380 23128000471168 run.py:483] Algo bellman_ford step 4206 current loss 0.022071, current_train_items 134624.
I0304 19:30:12.680619 23128000471168 run.py:483] Algo bellman_ford step 4207 current loss 0.054251, current_train_items 134656.
I0304 19:30:12.712577 23128000471168 run.py:483] Algo bellman_ford step 4208 current loss 0.072164, current_train_items 134688.
I0304 19:30:12.747264 23128000471168 run.py:483] Algo bellman_ford step 4209 current loss 0.088664, current_train_items 134720.
I0304 19:30:12.767108 23128000471168 run.py:483] Algo bellman_ford step 4210 current loss 0.010253, current_train_items 134752.
I0304 19:30:12.783591 23128000471168 run.py:483] Algo bellman_ford step 4211 current loss 0.031828, current_train_items 134784.
I0304 19:30:12.807778 23128000471168 run.py:483] Algo bellman_ford step 4212 current loss 0.059880, current_train_items 134816.
I0304 19:30:12.839474 23128000471168 run.py:483] Algo bellman_ford step 4213 current loss 0.055461, current_train_items 134848.
I0304 19:30:12.873121 23128000471168 run.py:483] Algo bellman_ford step 4214 current loss 0.095864, current_train_items 134880.
I0304 19:30:12.893008 23128000471168 run.py:483] Algo bellman_ford step 4215 current loss 0.008816, current_train_items 134912.
I0304 19:30:12.909538 23128000471168 run.py:483] Algo bellman_ford step 4216 current loss 0.033704, current_train_items 134944.
I0304 19:30:12.933057 23128000471168 run.py:483] Algo bellman_ford step 4217 current loss 0.034921, current_train_items 134976.
I0304 19:30:12.963929 23128000471168 run.py:483] Algo bellman_ford step 4218 current loss 0.114471, current_train_items 135008.
I0304 19:30:12.997631 23128000471168 run.py:483] Algo bellman_ford step 4219 current loss 0.070161, current_train_items 135040.
I0304 19:30:13.017563 23128000471168 run.py:483] Algo bellman_ford step 4220 current loss 0.003818, current_train_items 135072.
I0304 19:30:13.034141 23128000471168 run.py:483] Algo bellman_ford step 4221 current loss 0.039606, current_train_items 135104.
I0304 19:30:13.058339 23128000471168 run.py:483] Algo bellman_ford step 4222 current loss 0.046720, current_train_items 135136.
I0304 19:30:13.090575 23128000471168 run.py:483] Algo bellman_ford step 4223 current loss 0.052006, current_train_items 135168.
I0304 19:30:13.126260 23128000471168 run.py:483] Algo bellman_ford step 4224 current loss 0.081629, current_train_items 135200.
I0304 19:30:13.145944 23128000471168 run.py:483] Algo bellman_ford step 4225 current loss 0.001825, current_train_items 135232.
I0304 19:30:13.162194 23128000471168 run.py:483] Algo bellman_ford step 4226 current loss 0.018577, current_train_items 135264.
I0304 19:30:13.185670 23128000471168 run.py:483] Algo bellman_ford step 4227 current loss 0.090008, current_train_items 135296.
I0304 19:30:13.216017 23128000471168 run.py:483] Algo bellman_ford step 4228 current loss 0.063272, current_train_items 135328.
I0304 19:30:13.248266 23128000471168 run.py:483] Algo bellman_ford step 4229 current loss 0.056674, current_train_items 135360.
I0304 19:30:13.267887 23128000471168 run.py:483] Algo bellman_ford step 4230 current loss 0.001720, current_train_items 135392.
I0304 19:30:13.284071 23128000471168 run.py:483] Algo bellman_ford step 4231 current loss 0.024827, current_train_items 135424.
I0304 19:30:13.308037 23128000471168 run.py:483] Algo bellman_ford step 4232 current loss 0.055582, current_train_items 135456.
I0304 19:30:13.339414 23128000471168 run.py:483] Algo bellman_ford step 4233 current loss 0.051917, current_train_items 135488.
I0304 19:30:13.372779 23128000471168 run.py:483] Algo bellman_ford step 4234 current loss 0.056606, current_train_items 135520.
I0304 19:30:13.392699 23128000471168 run.py:483] Algo bellman_ford step 4235 current loss 0.002439, current_train_items 135552.
I0304 19:30:13.409756 23128000471168 run.py:483] Algo bellman_ford step 4236 current loss 0.027999, current_train_items 135584.
I0304 19:30:13.434262 23128000471168 run.py:483] Algo bellman_ford step 4237 current loss 0.067915, current_train_items 135616.
I0304 19:30:13.465881 23128000471168 run.py:483] Algo bellman_ford step 4238 current loss 0.049709, current_train_items 135648.
I0304 19:30:13.501474 23128000471168 run.py:483] Algo bellman_ford step 4239 current loss 0.093116, current_train_items 135680.
I0304 19:30:13.521420 23128000471168 run.py:483] Algo bellman_ford step 4240 current loss 0.024696, current_train_items 135712.
I0304 19:30:13.537794 23128000471168 run.py:483] Algo bellman_ford step 4241 current loss 0.008963, current_train_items 135744.
I0304 19:30:13.561356 23128000471168 run.py:483] Algo bellman_ford step 4242 current loss 0.030530, current_train_items 135776.
I0304 19:30:13.593610 23128000471168 run.py:483] Algo bellman_ford step 4243 current loss 0.076400, current_train_items 135808.
I0304 19:30:13.626553 23128000471168 run.py:483] Algo bellman_ford step 4244 current loss 0.052036, current_train_items 135840.
I0304 19:30:13.646178 23128000471168 run.py:483] Algo bellman_ford step 4245 current loss 0.002739, current_train_items 135872.
I0304 19:30:13.662233 23128000471168 run.py:483] Algo bellman_ford step 4246 current loss 0.005651, current_train_items 135904.
I0304 19:30:13.687436 23128000471168 run.py:483] Algo bellman_ford step 4247 current loss 0.032179, current_train_items 135936.
I0304 19:30:13.719338 23128000471168 run.py:483] Algo bellman_ford step 4248 current loss 0.072837, current_train_items 135968.
I0304 19:30:13.754732 23128000471168 run.py:483] Algo bellman_ford step 4249 current loss 0.072049, current_train_items 136000.
I0304 19:30:13.774762 23128000471168 run.py:483] Algo bellman_ford step 4250 current loss 0.003533, current_train_items 136032.
I0304 19:30:13.782817 23128000471168 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0304 19:30:13.782926 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:13.799834 23128000471168 run.py:483] Algo bellman_ford step 4251 current loss 0.024014, current_train_items 136064.
I0304 19:30:13.824455 23128000471168 run.py:483] Algo bellman_ford step 4252 current loss 0.025329, current_train_items 136096.
I0304 19:30:13.856167 23128000471168 run.py:483] Algo bellman_ford step 4253 current loss 0.095385, current_train_items 136128.
I0304 19:30:13.890650 23128000471168 run.py:483] Algo bellman_ford step 4254 current loss 0.067157, current_train_items 136160.
I0304 19:30:13.910527 23128000471168 run.py:483] Algo bellman_ford step 4255 current loss 0.002040, current_train_items 136192.
I0304 19:30:13.926582 23128000471168 run.py:483] Algo bellman_ford step 4256 current loss 0.011203, current_train_items 136224.
I0304 19:30:13.950028 23128000471168 run.py:483] Algo bellman_ford step 4257 current loss 0.032002, current_train_items 136256.
I0304 19:30:13.981613 23128000471168 run.py:483] Algo bellman_ford step 4258 current loss 0.077257, current_train_items 136288.
I0304 19:30:14.015958 23128000471168 run.py:483] Algo bellman_ford step 4259 current loss 0.114693, current_train_items 136320.
I0304 19:30:14.036211 23128000471168 run.py:483] Algo bellman_ford step 4260 current loss 0.003860, current_train_items 136352.
I0304 19:30:14.053245 23128000471168 run.py:483] Algo bellman_ford step 4261 current loss 0.024651, current_train_items 136384.
I0304 19:30:14.076845 23128000471168 run.py:483] Algo bellman_ford step 4262 current loss 0.036566, current_train_items 136416.
I0304 19:30:14.108461 23128000471168 run.py:483] Algo bellman_ford step 4263 current loss 0.060462, current_train_items 136448.
I0304 19:30:14.142661 23128000471168 run.py:483] Algo bellman_ford step 4264 current loss 0.048210, current_train_items 136480.
I0304 19:30:14.162518 23128000471168 run.py:483] Algo bellman_ford step 4265 current loss 0.005298, current_train_items 136512.
I0304 19:30:14.179001 23128000471168 run.py:483] Algo bellman_ford step 4266 current loss 0.039892, current_train_items 136544.
I0304 19:30:14.202795 23128000471168 run.py:483] Algo bellman_ford step 4267 current loss 0.033104, current_train_items 136576.
I0304 19:30:14.233948 23128000471168 run.py:483] Algo bellman_ford step 4268 current loss 0.075958, current_train_items 136608.
I0304 19:30:14.267055 23128000471168 run.py:483] Algo bellman_ford step 4269 current loss 0.082367, current_train_items 136640.
I0304 19:30:14.287717 23128000471168 run.py:483] Algo bellman_ford step 4270 current loss 0.002675, current_train_items 136672.
I0304 19:30:14.304047 23128000471168 run.py:483] Algo bellman_ford step 4271 current loss 0.020768, current_train_items 136704.
I0304 19:30:14.326882 23128000471168 run.py:483] Algo bellman_ford step 4272 current loss 0.007627, current_train_items 136736.
I0304 19:30:14.357873 23128000471168 run.py:483] Algo bellman_ford step 4273 current loss 0.072231, current_train_items 136768.
I0304 19:30:14.390798 23128000471168 run.py:483] Algo bellman_ford step 4274 current loss 0.053606, current_train_items 136800.
I0304 19:30:14.411373 23128000471168 run.py:483] Algo bellman_ford step 4275 current loss 0.002603, current_train_items 136832.
I0304 19:30:14.427901 23128000471168 run.py:483] Algo bellman_ford step 4276 current loss 0.016708, current_train_items 136864.
I0304 19:30:14.451296 23128000471168 run.py:483] Algo bellman_ford step 4277 current loss 0.055463, current_train_items 136896.
I0304 19:30:14.484660 23128000471168 run.py:483] Algo bellman_ford step 4278 current loss 0.095696, current_train_items 136928.
I0304 19:30:14.519292 23128000471168 run.py:483] Algo bellman_ford step 4279 current loss 0.051393, current_train_items 136960.
I0304 19:30:14.539110 23128000471168 run.py:483] Algo bellman_ford step 4280 current loss 0.018043, current_train_items 136992.
I0304 19:30:14.555389 23128000471168 run.py:483] Algo bellman_ford step 4281 current loss 0.021302, current_train_items 137024.
I0304 19:30:14.579761 23128000471168 run.py:483] Algo bellman_ford step 4282 current loss 0.014998, current_train_items 137056.
I0304 19:30:14.609972 23128000471168 run.py:483] Algo bellman_ford step 4283 current loss 0.034734, current_train_items 137088.
I0304 19:30:14.645403 23128000471168 run.py:483] Algo bellman_ford step 4284 current loss 0.082215, current_train_items 137120.
I0304 19:30:14.665488 23128000471168 run.py:483] Algo bellman_ford step 4285 current loss 0.001503, current_train_items 137152.
I0304 19:30:14.682008 23128000471168 run.py:483] Algo bellman_ford step 4286 current loss 0.003290, current_train_items 137184.
I0304 19:30:14.705552 23128000471168 run.py:483] Algo bellman_ford step 4287 current loss 0.049033, current_train_items 137216.
I0304 19:30:14.738487 23128000471168 run.py:483] Algo bellman_ford step 4288 current loss 0.076883, current_train_items 137248.
I0304 19:30:14.772813 23128000471168 run.py:483] Algo bellman_ford step 4289 current loss 0.089642, current_train_items 137280.
I0304 19:30:14.792831 23128000471168 run.py:483] Algo bellman_ford step 4290 current loss 0.003211, current_train_items 137312.
I0304 19:30:14.809835 23128000471168 run.py:483] Algo bellman_ford step 4291 current loss 0.026994, current_train_items 137344.
I0304 19:30:14.834683 23128000471168 run.py:483] Algo bellman_ford step 4292 current loss 0.073384, current_train_items 137376.
I0304 19:30:14.867677 23128000471168 run.py:483] Algo bellman_ford step 4293 current loss 0.108917, current_train_items 137408.
I0304 19:30:14.903447 23128000471168 run.py:483] Algo bellman_ford step 4294 current loss 0.096281, current_train_items 137440.
I0304 19:30:14.923312 23128000471168 run.py:483] Algo bellman_ford step 4295 current loss 0.001827, current_train_items 137472.
I0304 19:30:14.939597 23128000471168 run.py:483] Algo bellman_ford step 4296 current loss 0.007914, current_train_items 137504.
I0304 19:30:14.963069 23128000471168 run.py:483] Algo bellman_ford step 4297 current loss 0.063329, current_train_items 137536.
I0304 19:30:14.995350 23128000471168 run.py:483] Algo bellman_ford step 4298 current loss 0.067848, current_train_items 137568.
I0304 19:30:15.025717 23128000471168 run.py:483] Algo bellman_ford step 4299 current loss 0.044516, current_train_items 137600.
I0304 19:30:15.045937 23128000471168 run.py:483] Algo bellman_ford step 4300 current loss 0.001653, current_train_items 137632.
I0304 19:30:15.053751 23128000471168 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0304 19:30:15.053859 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:15.070816 23128000471168 run.py:483] Algo bellman_ford step 4301 current loss 0.009270, current_train_items 137664.
I0304 19:30:15.095258 23128000471168 run.py:483] Algo bellman_ford step 4302 current loss 0.049224, current_train_items 137696.
I0304 19:30:15.128140 23128000471168 run.py:483] Algo bellman_ford step 4303 current loss 0.290835, current_train_items 137728.
I0304 19:30:15.164644 23128000471168 run.py:483] Algo bellman_ford step 4304 current loss 0.040102, current_train_items 137760.
I0304 19:30:15.184993 23128000471168 run.py:483] Algo bellman_ford step 4305 current loss 0.001953, current_train_items 137792.
I0304 19:30:15.201106 23128000471168 run.py:483] Algo bellman_ford step 4306 current loss 0.021720, current_train_items 137824.
I0304 19:30:15.226556 23128000471168 run.py:483] Algo bellman_ford step 4307 current loss 0.065243, current_train_items 137856.
I0304 19:30:15.259298 23128000471168 run.py:483] Algo bellman_ford step 4308 current loss 0.102216, current_train_items 137888.
I0304 19:30:15.295362 23128000471168 run.py:483] Algo bellman_ford step 4309 current loss 0.096942, current_train_items 137920.
I0304 19:30:15.315412 23128000471168 run.py:483] Algo bellman_ford step 4310 current loss 0.007985, current_train_items 137952.
I0304 19:30:15.331867 23128000471168 run.py:483] Algo bellman_ford step 4311 current loss 0.037916, current_train_items 137984.
I0304 19:30:15.354733 23128000471168 run.py:483] Algo bellman_ford step 4312 current loss 0.032560, current_train_items 138016.
I0304 19:30:15.386461 23128000471168 run.py:483] Algo bellman_ford step 4313 current loss 0.037644, current_train_items 138048.
I0304 19:30:15.420830 23128000471168 run.py:483] Algo bellman_ford step 4314 current loss 0.068909, current_train_items 138080.
I0304 19:30:15.440653 23128000471168 run.py:483] Algo bellman_ford step 4315 current loss 0.002261, current_train_items 138112.
I0304 19:30:15.456860 23128000471168 run.py:483] Algo bellman_ford step 4316 current loss 0.060313, current_train_items 138144.
I0304 19:30:15.481340 23128000471168 run.py:483] Algo bellman_ford step 4317 current loss 0.052399, current_train_items 138176.
I0304 19:30:15.511882 23128000471168 run.py:483] Algo bellman_ford step 4318 current loss 0.067295, current_train_items 138208.
I0304 19:30:15.545596 23128000471168 run.py:483] Algo bellman_ford step 4319 current loss 0.055439, current_train_items 138240.
I0304 19:30:15.565316 23128000471168 run.py:483] Algo bellman_ford step 4320 current loss 0.003582, current_train_items 138272.
I0304 19:30:15.581715 23128000471168 run.py:483] Algo bellman_ford step 4321 current loss 0.014035, current_train_items 138304.
I0304 19:30:15.605774 23128000471168 run.py:483] Algo bellman_ford step 4322 current loss 0.045258, current_train_items 138336.
I0304 19:30:15.636131 23128000471168 run.py:483] Algo bellman_ford step 4323 current loss 0.068420, current_train_items 138368.
I0304 19:30:15.670714 23128000471168 run.py:483] Algo bellman_ford step 4324 current loss 0.075832, current_train_items 138400.
I0304 19:30:15.690637 23128000471168 run.py:483] Algo bellman_ford step 4325 current loss 0.004332, current_train_items 138432.
I0304 19:30:15.707265 23128000471168 run.py:483] Algo bellman_ford step 4326 current loss 0.011968, current_train_items 138464.
I0304 19:30:15.731339 23128000471168 run.py:483] Algo bellman_ford step 4327 current loss 0.100316, current_train_items 138496.
I0304 19:30:15.764092 23128000471168 run.py:483] Algo bellman_ford step 4328 current loss 0.087206, current_train_items 138528.
I0304 19:30:15.796903 23128000471168 run.py:483] Algo bellman_ford step 4329 current loss 0.086130, current_train_items 138560.
I0304 19:30:15.816982 23128000471168 run.py:483] Algo bellman_ford step 4330 current loss 0.003548, current_train_items 138592.
I0304 19:30:15.833899 23128000471168 run.py:483] Algo bellman_ford step 4331 current loss 0.012063, current_train_items 138624.
I0304 19:30:15.858426 23128000471168 run.py:483] Algo bellman_ford step 4332 current loss 0.051291, current_train_items 138656.
I0304 19:30:15.889181 23128000471168 run.py:483] Algo bellman_ford step 4333 current loss 0.087835, current_train_items 138688.
I0304 19:30:15.921205 23128000471168 run.py:483] Algo bellman_ford step 4334 current loss 0.052776, current_train_items 138720.
I0304 19:30:15.941192 23128000471168 run.py:483] Algo bellman_ford step 4335 current loss 0.009520, current_train_items 138752.
I0304 19:30:15.957556 23128000471168 run.py:483] Algo bellman_ford step 4336 current loss 0.042615, current_train_items 138784.
I0304 19:30:15.981871 23128000471168 run.py:483] Algo bellman_ford step 4337 current loss 0.066281, current_train_items 138816.
I0304 19:30:16.014580 23128000471168 run.py:483] Algo bellman_ford step 4338 current loss 0.076044, current_train_items 138848.
I0304 19:30:16.047621 23128000471168 run.py:483] Algo bellman_ford step 4339 current loss 0.100588, current_train_items 138880.
I0304 19:30:16.067542 23128000471168 run.py:483] Algo bellman_ford step 4340 current loss 0.014142, current_train_items 138912.
I0304 19:30:16.083784 23128000471168 run.py:483] Algo bellman_ford step 4341 current loss 0.011040, current_train_items 138944.
I0304 19:30:16.107720 23128000471168 run.py:483] Algo bellman_ford step 4342 current loss 0.080450, current_train_items 138976.
I0304 19:30:16.139314 23128000471168 run.py:483] Algo bellman_ford step 4343 current loss 0.124768, current_train_items 139008.
I0304 19:30:16.173817 23128000471168 run.py:483] Algo bellman_ford step 4344 current loss 0.151855, current_train_items 139040.
I0304 19:30:16.193759 23128000471168 run.py:483] Algo bellman_ford step 4345 current loss 0.013014, current_train_items 139072.
I0304 19:30:16.210904 23128000471168 run.py:483] Algo bellman_ford step 4346 current loss 0.015726, current_train_items 139104.
I0304 19:30:16.235001 23128000471168 run.py:483] Algo bellman_ford step 4347 current loss 0.041630, current_train_items 139136.
I0304 19:30:16.266572 23128000471168 run.py:483] Algo bellman_ford step 4348 current loss 0.038977, current_train_items 139168.
I0304 19:30:16.299642 23128000471168 run.py:483] Algo bellman_ford step 4349 current loss 0.111311, current_train_items 139200.
I0304 19:30:16.319486 23128000471168 run.py:483] Algo bellman_ford step 4350 current loss 0.009339, current_train_items 139232.
I0304 19:30:16.327892 23128000471168 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0304 19:30:16.328000 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:16.345342 23128000471168 run.py:483] Algo bellman_ford step 4351 current loss 0.031918, current_train_items 139264.
I0304 19:30:16.370612 23128000471168 run.py:483] Algo bellman_ford step 4352 current loss 0.063933, current_train_items 139296.
I0304 19:30:16.402070 23128000471168 run.py:483] Algo bellman_ford step 4353 current loss 0.072080, current_train_items 139328.
I0304 19:30:16.437013 23128000471168 run.py:483] Algo bellman_ford step 4354 current loss 0.069151, current_train_items 139360.
I0304 19:30:16.457141 23128000471168 run.py:483] Algo bellman_ford step 4355 current loss 0.002208, current_train_items 139392.
I0304 19:30:16.473601 23128000471168 run.py:483] Algo bellman_ford step 4356 current loss 0.007037, current_train_items 139424.
I0304 19:30:16.498151 23128000471168 run.py:483] Algo bellman_ford step 4357 current loss 0.048111, current_train_items 139456.
I0304 19:30:16.527995 23128000471168 run.py:483] Algo bellman_ford step 4358 current loss 0.071662, current_train_items 139488.
I0304 19:30:16.560592 23128000471168 run.py:483] Algo bellman_ford step 4359 current loss 0.052436, current_train_items 139520.
I0304 19:30:16.580802 23128000471168 run.py:483] Algo bellman_ford step 4360 current loss 0.001855, current_train_items 139552.
I0304 19:30:16.597867 23128000471168 run.py:483] Algo bellman_ford step 4361 current loss 0.069577, current_train_items 139584.
I0304 19:30:16.620679 23128000471168 run.py:483] Algo bellman_ford step 4362 current loss 0.046483, current_train_items 139616.
I0304 19:30:16.653452 23128000471168 run.py:483] Algo bellman_ford step 4363 current loss 0.052256, current_train_items 139648.
I0304 19:30:16.688540 23128000471168 run.py:483] Algo bellman_ford step 4364 current loss 0.078140, current_train_items 139680.
I0304 19:30:16.708633 23128000471168 run.py:483] Algo bellman_ford step 4365 current loss 0.002842, current_train_items 139712.
I0304 19:30:16.725181 23128000471168 run.py:483] Algo bellman_ford step 4366 current loss 0.017535, current_train_items 139744.
I0304 19:30:16.749072 23128000471168 run.py:483] Algo bellman_ford step 4367 current loss 0.042938, current_train_items 139776.
I0304 19:30:16.779557 23128000471168 run.py:483] Algo bellman_ford step 4368 current loss 0.072027, current_train_items 139808.
I0304 19:30:16.813918 23128000471168 run.py:483] Algo bellman_ford step 4369 current loss 0.050274, current_train_items 139840.
I0304 19:30:16.833872 23128000471168 run.py:483] Algo bellman_ford step 4370 current loss 0.002496, current_train_items 139872.
I0304 19:30:16.850371 23128000471168 run.py:483] Algo bellman_ford step 4371 current loss 0.032500, current_train_items 139904.
I0304 19:30:16.873603 23128000471168 run.py:483] Algo bellman_ford step 4372 current loss 0.030671, current_train_items 139936.
I0304 19:30:16.904568 23128000471168 run.py:483] Algo bellman_ford step 4373 current loss 0.059480, current_train_items 139968.
I0304 19:30:16.937937 23128000471168 run.py:483] Algo bellman_ford step 4374 current loss 0.120995, current_train_items 140000.
I0304 19:30:16.958277 23128000471168 run.py:483] Algo bellman_ford step 4375 current loss 0.005415, current_train_items 140032.
I0304 19:30:16.974930 23128000471168 run.py:483] Algo bellman_ford step 4376 current loss 0.007334, current_train_items 140064.
I0304 19:30:16.998189 23128000471168 run.py:483] Algo bellman_ford step 4377 current loss 0.037582, current_train_items 140096.
I0304 19:30:17.029087 23128000471168 run.py:483] Algo bellman_ford step 4378 current loss 0.046560, current_train_items 140128.
I0304 19:30:17.062911 23128000471168 run.py:483] Algo bellman_ford step 4379 current loss 0.047673, current_train_items 140160.
I0304 19:30:17.082937 23128000471168 run.py:483] Algo bellman_ford step 4380 current loss 0.001758, current_train_items 140192.
I0304 19:30:17.099297 23128000471168 run.py:483] Algo bellman_ford step 4381 current loss 0.043203, current_train_items 140224.
I0304 19:30:17.123478 23128000471168 run.py:483] Algo bellman_ford step 4382 current loss 0.059639, current_train_items 140256.
I0304 19:30:17.155119 23128000471168 run.py:483] Algo bellman_ford step 4383 current loss 0.060062, current_train_items 140288.
I0304 19:30:17.189036 23128000471168 run.py:483] Algo bellman_ford step 4384 current loss 0.049380, current_train_items 140320.
I0304 19:30:17.209517 23128000471168 run.py:483] Algo bellman_ford step 4385 current loss 0.005821, current_train_items 140352.
I0304 19:30:17.225914 23128000471168 run.py:483] Algo bellman_ford step 4386 current loss 0.027754, current_train_items 140384.
I0304 19:30:17.249806 23128000471168 run.py:483] Algo bellman_ford step 4387 current loss 0.041303, current_train_items 140416.
I0304 19:30:17.281206 23128000471168 run.py:483] Algo bellman_ford step 4388 current loss 0.047853, current_train_items 140448.
I0304 19:30:17.313491 23128000471168 run.py:483] Algo bellman_ford step 4389 current loss 0.050434, current_train_items 140480.
I0304 19:30:17.334216 23128000471168 run.py:483] Algo bellman_ford step 4390 current loss 0.003263, current_train_items 140512.
I0304 19:30:17.350584 23128000471168 run.py:483] Algo bellman_ford step 4391 current loss 0.008765, current_train_items 140544.
I0304 19:30:17.374039 23128000471168 run.py:483] Algo bellman_ford step 4392 current loss 0.056357, current_train_items 140576.
I0304 19:30:17.405695 23128000471168 run.py:483] Algo bellman_ford step 4393 current loss 0.089510, current_train_items 140608.
I0304 19:30:17.439888 23128000471168 run.py:483] Algo bellman_ford step 4394 current loss 0.123006, current_train_items 140640.
I0304 19:30:17.459619 23128000471168 run.py:483] Algo bellman_ford step 4395 current loss 0.006472, current_train_items 140672.
I0304 19:30:17.476382 23128000471168 run.py:483] Algo bellman_ford step 4396 current loss 0.055634, current_train_items 140704.
I0304 19:30:17.500545 23128000471168 run.py:483] Algo bellman_ford step 4397 current loss 0.062361, current_train_items 140736.
I0304 19:30:17.532037 23128000471168 run.py:483] Algo bellman_ford step 4398 current loss 0.065124, current_train_items 140768.
I0304 19:30:17.565441 23128000471168 run.py:483] Algo bellman_ford step 4399 current loss 0.110910, current_train_items 140800.
I0304 19:30:17.585616 23128000471168 run.py:483] Algo bellman_ford step 4400 current loss 0.002327, current_train_items 140832.
I0304 19:30:17.593148 23128000471168 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0304 19:30:17.593257 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:30:17.610513 23128000471168 run.py:483] Algo bellman_ford step 4401 current loss 0.022526, current_train_items 140864.
I0304 19:30:17.634902 23128000471168 run.py:483] Algo bellman_ford step 4402 current loss 0.068845, current_train_items 140896.
I0304 19:30:17.666489 23128000471168 run.py:483] Algo bellman_ford step 4403 current loss 0.053453, current_train_items 140928.
I0304 19:30:17.704760 23128000471168 run.py:483] Algo bellman_ford step 4404 current loss 0.199252, current_train_items 140960.
I0304 19:30:17.725001 23128000471168 run.py:483] Algo bellman_ford step 4405 current loss 0.007200, current_train_items 140992.
I0304 19:30:17.741346 23128000471168 run.py:483] Algo bellman_ford step 4406 current loss 0.028002, current_train_items 141024.
I0304 19:30:17.765439 23128000471168 run.py:483] Algo bellman_ford step 4407 current loss 0.047390, current_train_items 141056.
I0304 19:30:17.796817 23128000471168 run.py:483] Algo bellman_ford step 4408 current loss 0.056541, current_train_items 141088.
I0304 19:30:17.832565 23128000471168 run.py:483] Algo bellman_ford step 4409 current loss 0.080751, current_train_items 141120.
I0304 19:30:17.852530 23128000471168 run.py:483] Algo bellman_ford step 4410 current loss 0.003170, current_train_items 141152.
I0304 19:30:17.869074 23128000471168 run.py:483] Algo bellman_ford step 4411 current loss 0.017228, current_train_items 141184.
I0304 19:30:17.893328 23128000471168 run.py:483] Algo bellman_ford step 4412 current loss 0.033719, current_train_items 141216.
I0304 19:30:17.925992 23128000471168 run.py:483] Algo bellman_ford step 4413 current loss 0.072014, current_train_items 141248.
I0304 19:30:17.959161 23128000471168 run.py:483] Algo bellman_ford step 4414 current loss 0.038867, current_train_items 141280.
I0304 19:30:17.979094 23128000471168 run.py:483] Algo bellman_ford step 4415 current loss 0.001821, current_train_items 141312.
I0304 19:30:17.996045 23128000471168 run.py:483] Algo bellman_ford step 4416 current loss 0.046477, current_train_items 141344.
I0304 19:30:18.019918 23128000471168 run.py:483] Algo bellman_ford step 4417 current loss 0.058048, current_train_items 141376.
I0304 19:30:18.051431 23128000471168 run.py:483] Algo bellman_ford step 4418 current loss 0.066005, current_train_items 141408.
I0304 19:30:18.085116 23128000471168 run.py:483] Algo bellman_ford step 4419 current loss 0.054817, current_train_items 141440.
I0304 19:30:18.104923 23128000471168 run.py:483] Algo bellman_ford step 4420 current loss 0.006789, current_train_items 141472.
I0304 19:30:18.121153 23128000471168 run.py:483] Algo bellman_ford step 4421 current loss 0.014723, current_train_items 141504.
I0304 19:30:18.145156 23128000471168 run.py:483] Algo bellman_ford step 4422 current loss 0.051656, current_train_items 141536.
I0304 19:30:18.176481 23128000471168 run.py:483] Algo bellman_ford step 4423 current loss 0.029825, current_train_items 141568.
I0304 19:30:18.211037 23128000471168 run.py:483] Algo bellman_ford step 4424 current loss 0.068379, current_train_items 141600.
I0304 19:30:18.231081 23128000471168 run.py:483] Algo bellman_ford step 4425 current loss 0.013152, current_train_items 141632.
I0304 19:30:18.246669 23128000471168 run.py:483] Algo bellman_ford step 4426 current loss 0.008922, current_train_items 141664.
I0304 19:30:18.271315 23128000471168 run.py:483] Algo bellman_ford step 4427 current loss 0.067893, current_train_items 141696.
I0304 19:30:18.302995 23128000471168 run.py:483] Algo bellman_ford step 4428 current loss 0.028972, current_train_items 141728.
I0304 19:30:18.336593 23128000471168 run.py:483] Algo bellman_ford step 4429 current loss 0.037305, current_train_items 141760.
I0304 19:30:18.356222 23128000471168 run.py:483] Algo bellman_ford step 4430 current loss 0.001850, current_train_items 141792.
I0304 19:30:18.372416 23128000471168 run.py:483] Algo bellman_ford step 4431 current loss 0.017821, current_train_items 141824.
I0304 19:30:18.396706 23128000471168 run.py:483] Algo bellman_ford step 4432 current loss 0.050866, current_train_items 141856.
I0304 19:30:18.427165 23128000471168 run.py:483] Algo bellman_ford step 4433 current loss 0.033631, current_train_items 141888.
I0304 19:30:18.460783 23128000471168 run.py:483] Algo bellman_ford step 4434 current loss 0.069182, current_train_items 141920.
I0304 19:30:18.480732 23128000471168 run.py:483] Algo bellman_ford step 4435 current loss 0.002588, current_train_items 141952.
I0304 19:30:18.497323 23128000471168 run.py:483] Algo bellman_ford step 4436 current loss 0.040864, current_train_items 141984.
I0304 19:30:18.522000 23128000471168 run.py:483] Algo bellman_ford step 4437 current loss 0.055241, current_train_items 142016.
I0304 19:30:18.552396 23128000471168 run.py:483] Algo bellman_ford step 4438 current loss 0.070097, current_train_items 142048.
I0304 19:30:18.587298 23128000471168 run.py:483] Algo bellman_ford step 4439 current loss 0.070603, current_train_items 142080.
I0304 19:30:18.607153 23128000471168 run.py:483] Algo bellman_ford step 4440 current loss 0.002980, current_train_items 142112.
I0304 19:30:18.623810 23128000471168 run.py:483] Algo bellman_ford step 4441 current loss 0.023137, current_train_items 142144.
I0304 19:30:18.647608 23128000471168 run.py:483] Algo bellman_ford step 4442 current loss 0.047683, current_train_items 142176.
I0304 19:30:18.678658 23128000471168 run.py:483] Algo bellman_ford step 4443 current loss 0.052510, current_train_items 142208.
I0304 19:30:18.713711 23128000471168 run.py:483] Algo bellman_ford step 4444 current loss 0.063648, current_train_items 142240.
I0304 19:30:18.733703 23128000471168 run.py:483] Algo bellman_ford step 4445 current loss 0.006910, current_train_items 142272.
I0304 19:30:18.749859 23128000471168 run.py:483] Algo bellman_ford step 4446 current loss 0.026345, current_train_items 142304.
I0304 19:30:18.774064 23128000471168 run.py:483] Algo bellman_ford step 4447 current loss 0.037029, current_train_items 142336.
I0304 19:30:18.805922 23128000471168 run.py:483] Algo bellman_ford step 4448 current loss 0.123458, current_train_items 142368.
I0304 19:30:18.839944 23128000471168 run.py:483] Algo bellman_ford step 4449 current loss 0.112425, current_train_items 142400.
I0304 19:30:18.859828 23128000471168 run.py:483] Algo bellman_ford step 4450 current loss 0.043775, current_train_items 142432.
I0304 19:30:18.868091 23128000471168 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0304 19:30:18.868199 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:18.884772 23128000471168 run.py:483] Algo bellman_ford step 4451 current loss 0.005034, current_train_items 142464.
I0304 19:30:18.909519 23128000471168 run.py:483] Algo bellman_ford step 4452 current loss 0.029512, current_train_items 142496.
I0304 19:30:18.939969 23128000471168 run.py:483] Algo bellman_ford step 4453 current loss 0.052288, current_train_items 142528.
I0304 19:30:18.973316 23128000471168 run.py:483] Algo bellman_ford step 4454 current loss 0.064917, current_train_items 142560.
I0304 19:30:18.993439 23128000471168 run.py:483] Algo bellman_ford step 4455 current loss 0.047549, current_train_items 142592.
I0304 19:30:19.009582 23128000471168 run.py:483] Algo bellman_ford step 4456 current loss 0.018959, current_train_items 142624.
I0304 19:30:19.033947 23128000471168 run.py:483] Algo bellman_ford step 4457 current loss 0.127061, current_train_items 142656.
I0304 19:30:19.064324 23128000471168 run.py:483] Algo bellman_ford step 4458 current loss 0.048441, current_train_items 142688.
I0304 19:30:19.101253 23128000471168 run.py:483] Algo bellman_ford step 4459 current loss 0.072988, current_train_items 142720.
I0304 19:30:19.122063 23128000471168 run.py:483] Algo bellman_ford step 4460 current loss 0.012845, current_train_items 142752.
I0304 19:30:19.138786 23128000471168 run.py:483] Algo bellman_ford step 4461 current loss 0.022444, current_train_items 142784.
I0304 19:30:19.163453 23128000471168 run.py:483] Algo bellman_ford step 4462 current loss 0.058086, current_train_items 142816.
I0304 19:30:19.193453 23128000471168 run.py:483] Algo bellman_ford step 4463 current loss 0.037921, current_train_items 142848.
I0304 19:30:19.227898 23128000471168 run.py:483] Algo bellman_ford step 4464 current loss 0.074621, current_train_items 142880.
I0304 19:30:19.247908 23128000471168 run.py:483] Algo bellman_ford step 4465 current loss 0.003453, current_train_items 142912.
I0304 19:30:19.264505 23128000471168 run.py:483] Algo bellman_ford step 4466 current loss 0.019781, current_train_items 142944.
I0304 19:30:19.288387 23128000471168 run.py:483] Algo bellman_ford step 4467 current loss 0.039791, current_train_items 142976.
I0304 19:30:19.321808 23128000471168 run.py:483] Algo bellman_ford step 4468 current loss 0.037891, current_train_items 143008.
I0304 19:30:19.356460 23128000471168 run.py:483] Algo bellman_ford step 4469 current loss 0.075256, current_train_items 143040.
I0304 19:30:19.376889 23128000471168 run.py:483] Algo bellman_ford step 4470 current loss 0.010839, current_train_items 143072.
I0304 19:30:19.393277 23128000471168 run.py:483] Algo bellman_ford step 4471 current loss 0.003624, current_train_items 143104.
I0304 19:30:19.416942 23128000471168 run.py:483] Algo bellman_ford step 4472 current loss 0.044089, current_train_items 143136.
I0304 19:30:19.448229 23128000471168 run.py:483] Algo bellman_ford step 4473 current loss 0.044033, current_train_items 143168.
I0304 19:30:19.484640 23128000471168 run.py:483] Algo bellman_ford step 4474 current loss 0.086073, current_train_items 143200.
I0304 19:30:19.504991 23128000471168 run.py:483] Algo bellman_ford step 4475 current loss 0.002401, current_train_items 143232.
I0304 19:30:19.521469 23128000471168 run.py:483] Algo bellman_ford step 4476 current loss 0.030734, current_train_items 143264.
I0304 19:30:19.544891 23128000471168 run.py:483] Algo bellman_ford step 4477 current loss 0.044025, current_train_items 143296.
I0304 19:30:19.576298 23128000471168 run.py:483] Algo bellman_ford step 4478 current loss 0.044647, current_train_items 143328.
I0304 19:30:19.612288 23128000471168 run.py:483] Algo bellman_ford step 4479 current loss 0.094709, current_train_items 143360.
I0304 19:30:19.632056 23128000471168 run.py:483] Algo bellman_ford step 4480 current loss 0.002098, current_train_items 143392.
I0304 19:30:19.648449 23128000471168 run.py:483] Algo bellman_ford step 4481 current loss 0.023759, current_train_items 143424.
I0304 19:30:19.672563 23128000471168 run.py:483] Algo bellman_ford step 4482 current loss 0.032982, current_train_items 143456.
I0304 19:30:19.704448 23128000471168 run.py:483] Algo bellman_ford step 4483 current loss 0.064470, current_train_items 143488.
I0304 19:30:19.735978 23128000471168 run.py:483] Algo bellman_ford step 4484 current loss 0.070376, current_train_items 143520.
I0304 19:30:19.756322 23128000471168 run.py:483] Algo bellman_ford step 4485 current loss 0.003878, current_train_items 143552.
I0304 19:30:19.773088 23128000471168 run.py:483] Algo bellman_ford step 4486 current loss 0.010740, current_train_items 143584.
I0304 19:30:19.798097 23128000471168 run.py:483] Algo bellman_ford step 4487 current loss 0.108450, current_train_items 143616.
I0304 19:30:19.830014 23128000471168 run.py:483] Algo bellman_ford step 4488 current loss 0.112844, current_train_items 143648.
I0304 19:30:19.863357 23128000471168 run.py:483] Algo bellman_ford step 4489 current loss 0.139596, current_train_items 143680.
I0304 19:30:19.883727 23128000471168 run.py:483] Algo bellman_ford step 4490 current loss 0.005419, current_train_items 143712.
I0304 19:30:19.899997 23128000471168 run.py:483] Algo bellman_ford step 4491 current loss 0.016069, current_train_items 143744.
I0304 19:30:19.924271 23128000471168 run.py:483] Algo bellman_ford step 4492 current loss 0.032313, current_train_items 143776.
I0304 19:30:19.955955 23128000471168 run.py:483] Algo bellman_ford step 4493 current loss 0.078802, current_train_items 143808.
I0304 19:30:19.990638 23128000471168 run.py:483] Algo bellman_ford step 4494 current loss 0.110146, current_train_items 143840.
I0304 19:30:20.010617 23128000471168 run.py:483] Algo bellman_ford step 4495 current loss 0.015748, current_train_items 143872.
I0304 19:30:20.027444 23128000471168 run.py:483] Algo bellman_ford step 4496 current loss 0.028628, current_train_items 143904.
I0304 19:30:20.051560 23128000471168 run.py:483] Algo bellman_ford step 4497 current loss 0.038240, current_train_items 143936.
I0304 19:30:20.081726 23128000471168 run.py:483] Algo bellman_ford step 4498 current loss 0.030721, current_train_items 143968.
I0304 19:30:20.113406 23128000471168 run.py:483] Algo bellman_ford step 4499 current loss 0.051353, current_train_items 144000.
I0304 19:30:20.133911 23128000471168 run.py:483] Algo bellman_ford step 4500 current loss 0.004864, current_train_items 144032.
I0304 19:30:20.141769 23128000471168 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0304 19:30:20.141877 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:20.158935 23128000471168 run.py:483] Algo bellman_ford step 4501 current loss 0.025303, current_train_items 144064.
I0304 19:30:20.183952 23128000471168 run.py:483] Algo bellman_ford step 4502 current loss 0.039463, current_train_items 144096.
I0304 19:30:20.215866 23128000471168 run.py:483] Algo bellman_ford step 4503 current loss 0.072605, current_train_items 144128.
I0304 19:30:20.251394 23128000471168 run.py:483] Algo bellman_ford step 4504 current loss 0.040754, current_train_items 144160.
I0304 19:30:20.271948 23128000471168 run.py:483] Algo bellman_ford step 4505 current loss 0.003780, current_train_items 144192.
I0304 19:30:20.288359 23128000471168 run.py:483] Algo bellman_ford step 4506 current loss 0.075996, current_train_items 144224.
I0304 19:30:20.312240 23128000471168 run.py:483] Algo bellman_ford step 4507 current loss 0.025517, current_train_items 144256.
I0304 19:30:20.343540 23128000471168 run.py:483] Algo bellman_ford step 4508 current loss 0.044961, current_train_items 144288.
I0304 19:30:20.376722 23128000471168 run.py:483] Algo bellman_ford step 4509 current loss 0.055622, current_train_items 144320.
I0304 19:30:20.396733 23128000471168 run.py:483] Algo bellman_ford step 4510 current loss 0.057932, current_train_items 144352.
I0304 19:30:20.413417 23128000471168 run.py:483] Algo bellman_ford step 4511 current loss 0.048196, current_train_items 144384.
I0304 19:30:20.438509 23128000471168 run.py:483] Algo bellman_ford step 4512 current loss 0.045046, current_train_items 144416.
I0304 19:30:20.470239 23128000471168 run.py:483] Algo bellman_ford step 4513 current loss 0.075403, current_train_items 144448.
I0304 19:30:20.503696 23128000471168 run.py:483] Algo bellman_ford step 4514 current loss 0.042507, current_train_items 144480.
I0304 19:30:20.523503 23128000471168 run.py:483] Algo bellman_ford step 4515 current loss 0.002110, current_train_items 144512.
I0304 19:30:20.540479 23128000471168 run.py:483] Algo bellman_ford step 4516 current loss 0.014474, current_train_items 144544.
I0304 19:30:20.563742 23128000471168 run.py:483] Algo bellman_ford step 4517 current loss 0.061318, current_train_items 144576.
I0304 19:30:20.596397 23128000471168 run.py:483] Algo bellman_ford step 4518 current loss 0.067941, current_train_items 144608.
I0304 19:30:20.633576 23128000471168 run.py:483] Algo bellman_ford step 4519 current loss 0.069963, current_train_items 144640.
I0304 19:30:20.653682 23128000471168 run.py:483] Algo bellman_ford step 4520 current loss 0.002119, current_train_items 144672.
I0304 19:30:20.670441 23128000471168 run.py:483] Algo bellman_ford step 4521 current loss 0.009115, current_train_items 144704.
I0304 19:30:20.694338 23128000471168 run.py:483] Algo bellman_ford step 4522 current loss 0.030633, current_train_items 144736.
I0304 19:30:20.727693 23128000471168 run.py:483] Algo bellman_ford step 4523 current loss 0.077190, current_train_items 144768.
I0304 19:30:20.761345 23128000471168 run.py:483] Algo bellman_ford step 4524 current loss 0.057388, current_train_items 144800.
I0304 19:30:20.781597 23128000471168 run.py:483] Algo bellman_ford step 4525 current loss 0.001813, current_train_items 144832.
I0304 19:30:20.798060 23128000471168 run.py:483] Algo bellman_ford step 4526 current loss 0.012190, current_train_items 144864.
I0304 19:30:20.823402 23128000471168 run.py:483] Algo bellman_ford step 4527 current loss 0.049842, current_train_items 144896.
I0304 19:30:20.855407 23128000471168 run.py:483] Algo bellman_ford step 4528 current loss 0.030115, current_train_items 144928.
I0304 19:30:20.889631 23128000471168 run.py:483] Algo bellman_ford step 4529 current loss 0.064009, current_train_items 144960.
I0304 19:30:20.909953 23128000471168 run.py:483] Algo bellman_ford step 4530 current loss 0.012801, current_train_items 144992.
I0304 19:30:20.926220 23128000471168 run.py:483] Algo bellman_ford step 4531 current loss 0.006294, current_train_items 145024.
I0304 19:30:20.949801 23128000471168 run.py:483] Algo bellman_ford step 4532 current loss 0.033395, current_train_items 145056.
I0304 19:30:20.979918 23128000471168 run.py:483] Algo bellman_ford step 4533 current loss 0.055962, current_train_items 145088.
I0304 19:30:21.015403 23128000471168 run.py:483] Algo bellman_ford step 4534 current loss 0.141497, current_train_items 145120.
I0304 19:30:21.035559 23128000471168 run.py:483] Algo bellman_ford step 4535 current loss 0.002855, current_train_items 145152.
I0304 19:30:21.051959 23128000471168 run.py:483] Algo bellman_ford step 4536 current loss 0.009852, current_train_items 145184.
I0304 19:30:21.076430 23128000471168 run.py:483] Algo bellman_ford step 4537 current loss 0.047343, current_train_items 145216.
I0304 19:30:21.107508 23128000471168 run.py:483] Algo bellman_ford step 4538 current loss 0.032008, current_train_items 145248.
I0304 19:30:21.143148 23128000471168 run.py:483] Algo bellman_ford step 4539 current loss 0.106421, current_train_items 145280.
I0304 19:30:21.163350 23128000471168 run.py:483] Algo bellman_ford step 4540 current loss 0.007521, current_train_items 145312.
I0304 19:30:21.179665 23128000471168 run.py:483] Algo bellman_ford step 4541 current loss 0.014279, current_train_items 145344.
I0304 19:30:21.203058 23128000471168 run.py:483] Algo bellman_ford step 4542 current loss 0.040872, current_train_items 145376.
I0304 19:30:21.234304 23128000471168 run.py:483] Algo bellman_ford step 4543 current loss 0.041404, current_train_items 145408.
I0304 19:30:21.269389 23128000471168 run.py:483] Algo bellman_ford step 4544 current loss 0.111172, current_train_items 145440.
I0304 19:30:21.289612 23128000471168 run.py:483] Algo bellman_ford step 4545 current loss 0.004642, current_train_items 145472.
I0304 19:30:21.305927 23128000471168 run.py:483] Algo bellman_ford step 4546 current loss 0.009731, current_train_items 145504.
I0304 19:30:21.329469 23128000471168 run.py:483] Algo bellman_ford step 4547 current loss 0.028443, current_train_items 145536.
I0304 19:30:21.359837 23128000471168 run.py:483] Algo bellman_ford step 4548 current loss 0.048034, current_train_items 145568.
I0304 19:30:21.394884 23128000471168 run.py:483] Algo bellman_ford step 4549 current loss 0.065668, current_train_items 145600.
I0304 19:30:21.414983 23128000471168 run.py:483] Algo bellman_ford step 4550 current loss 0.003614, current_train_items 145632.
I0304 19:30:21.423134 23128000471168 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0304 19:30:21.423244 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:21.440677 23128000471168 run.py:483] Algo bellman_ford step 4551 current loss 0.013601, current_train_items 145664.
I0304 19:30:21.465024 23128000471168 run.py:483] Algo bellman_ford step 4552 current loss 0.033556, current_train_items 145696.
I0304 19:30:21.497372 23128000471168 run.py:483] Algo bellman_ford step 4553 current loss 0.106137, current_train_items 145728.
I0304 19:30:21.531657 23128000471168 run.py:483] Algo bellman_ford step 4554 current loss 0.061398, current_train_items 145760.
I0304 19:30:21.552047 23128000471168 run.py:483] Algo bellman_ford step 4555 current loss 0.027186, current_train_items 145792.
I0304 19:30:21.567904 23128000471168 run.py:483] Algo bellman_ford step 4556 current loss 0.035727, current_train_items 145824.
I0304 19:30:21.591424 23128000471168 run.py:483] Algo bellman_ford step 4557 current loss 0.068655, current_train_items 145856.
I0304 19:30:21.623855 23128000471168 run.py:483] Algo bellman_ford step 4558 current loss 0.045457, current_train_items 145888.
I0304 19:30:21.659133 23128000471168 run.py:483] Algo bellman_ford step 4559 current loss 0.061293, current_train_items 145920.
I0304 19:30:21.679534 23128000471168 run.py:483] Algo bellman_ford step 4560 current loss 0.002057, current_train_items 145952.
I0304 19:30:21.696316 23128000471168 run.py:483] Algo bellman_ford step 4561 current loss 0.020902, current_train_items 145984.
I0304 19:30:21.720052 23128000471168 run.py:483] Algo bellman_ford step 4562 current loss 0.050834, current_train_items 146016.
I0304 19:30:21.750915 23128000471168 run.py:483] Algo bellman_ford step 4563 current loss 0.137516, current_train_items 146048.
I0304 19:30:21.785933 23128000471168 run.py:483] Algo bellman_ford step 4564 current loss 0.092193, current_train_items 146080.
I0304 19:30:21.805890 23128000471168 run.py:483] Algo bellman_ford step 4565 current loss 0.006982, current_train_items 146112.
I0304 19:30:21.822415 23128000471168 run.py:483] Algo bellman_ford step 4566 current loss 0.050792, current_train_items 146144.
I0304 19:30:21.846945 23128000471168 run.py:483] Algo bellman_ford step 4567 current loss 0.049445, current_train_items 146176.
I0304 19:30:21.879226 23128000471168 run.py:483] Algo bellman_ford step 4568 current loss 0.075139, current_train_items 146208.
I0304 19:30:21.912323 23128000471168 run.py:483] Algo bellman_ford step 4569 current loss 0.058732, current_train_items 146240.
I0304 19:30:21.932281 23128000471168 run.py:483] Algo bellman_ford step 4570 current loss 0.004226, current_train_items 146272.
I0304 19:30:21.948686 23128000471168 run.py:483] Algo bellman_ford step 4571 current loss 0.033491, current_train_items 146304.
I0304 19:30:21.972933 23128000471168 run.py:483] Algo bellman_ford step 4572 current loss 0.046119, current_train_items 146336.
I0304 19:30:22.003842 23128000471168 run.py:483] Algo bellman_ford step 4573 current loss 0.039924, current_train_items 146368.
I0304 19:30:22.036813 23128000471168 run.py:483] Algo bellman_ford step 4574 current loss 0.056744, current_train_items 146400.
I0304 19:30:22.057085 23128000471168 run.py:483] Algo bellman_ford step 4575 current loss 0.003251, current_train_items 146432.
I0304 19:30:22.073587 23128000471168 run.py:483] Algo bellman_ford step 4576 current loss 0.035692, current_train_items 146464.
I0304 19:30:22.097167 23128000471168 run.py:483] Algo bellman_ford step 4577 current loss 0.090758, current_train_items 146496.
I0304 19:30:22.129180 23128000471168 run.py:483] Algo bellman_ford step 4578 current loss 0.132788, current_train_items 146528.
I0304 19:30:22.163774 23128000471168 run.py:483] Algo bellman_ford step 4579 current loss 0.100308, current_train_items 146560.
I0304 19:30:22.183411 23128000471168 run.py:483] Algo bellman_ford step 4580 current loss 0.013657, current_train_items 146592.
I0304 19:30:22.200160 23128000471168 run.py:483] Algo bellman_ford step 4581 current loss 0.050605, current_train_items 146624.
I0304 19:30:22.223738 23128000471168 run.py:483] Algo bellman_ford step 4582 current loss 0.138053, current_train_items 146656.
I0304 19:30:22.255226 23128000471168 run.py:483] Algo bellman_ford step 4583 current loss 0.140906, current_train_items 146688.
I0304 19:30:22.290025 23128000471168 run.py:483] Algo bellman_ford step 4584 current loss 0.276431, current_train_items 146720.
I0304 19:30:22.310443 23128000471168 run.py:483] Algo bellman_ford step 4585 current loss 0.005592, current_train_items 146752.
I0304 19:30:22.327255 23128000471168 run.py:483] Algo bellman_ford step 4586 current loss 0.006896, current_train_items 146784.
I0304 19:30:22.350398 23128000471168 run.py:483] Algo bellman_ford step 4587 current loss 0.055404, current_train_items 146816.
I0304 19:30:22.382025 23128000471168 run.py:483] Algo bellman_ford step 4588 current loss 0.071862, current_train_items 146848.
I0304 19:30:22.415093 23128000471168 run.py:483] Algo bellman_ford step 4589 current loss 0.106278, current_train_items 146880.
I0304 19:30:22.435379 23128000471168 run.py:483] Algo bellman_ford step 4590 current loss 0.020359, current_train_items 146912.
I0304 19:30:22.452086 23128000471168 run.py:483] Algo bellman_ford step 4591 current loss 0.021743, current_train_items 146944.
I0304 19:30:22.475493 23128000471168 run.py:483] Algo bellman_ford step 4592 current loss 0.032520, current_train_items 146976.
I0304 19:30:22.507718 23128000471168 run.py:483] Algo bellman_ford step 4593 current loss 0.054255, current_train_items 147008.
I0304 19:30:22.542489 23128000471168 run.py:483] Algo bellman_ford step 4594 current loss 0.090225, current_train_items 147040.
I0304 19:30:22.562417 23128000471168 run.py:483] Algo bellman_ford step 4595 current loss 0.004952, current_train_items 147072.
I0304 19:30:22.578812 23128000471168 run.py:483] Algo bellman_ford step 4596 current loss 0.012888, current_train_items 147104.
I0304 19:30:22.604717 23128000471168 run.py:483] Algo bellman_ford step 4597 current loss 0.070614, current_train_items 147136.
I0304 19:30:22.635946 23128000471168 run.py:483] Algo bellman_ford step 4598 current loss 0.048685, current_train_items 147168.
I0304 19:30:22.668470 23128000471168 run.py:483] Algo bellman_ford step 4599 current loss 0.075381, current_train_items 147200.
I0304 19:30:22.688867 23128000471168 run.py:483] Algo bellman_ford step 4600 current loss 0.003380, current_train_items 147232.
I0304 19:30:22.696880 23128000471168 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0304 19:30:22.696989 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:22.714548 23128000471168 run.py:483] Algo bellman_ford step 4601 current loss 0.029678, current_train_items 147264.
I0304 19:30:22.739940 23128000471168 run.py:483] Algo bellman_ford step 4602 current loss 0.052110, current_train_items 147296.
I0304 19:30:22.771810 23128000471168 run.py:483] Algo bellman_ford step 4603 current loss 0.059017, current_train_items 147328.
I0304 19:30:22.806782 23128000471168 run.py:483] Algo bellman_ford step 4604 current loss 0.103688, current_train_items 147360.
I0304 19:30:22.826660 23128000471168 run.py:483] Algo bellman_ford step 4605 current loss 0.002395, current_train_items 147392.
I0304 19:30:22.843169 23128000471168 run.py:483] Algo bellman_ford step 4606 current loss 0.025614, current_train_items 147424.
I0304 19:30:22.867037 23128000471168 run.py:483] Algo bellman_ford step 4607 current loss 0.039760, current_train_items 147456.
I0304 19:30:22.899539 23128000471168 run.py:483] Algo bellman_ford step 4608 current loss 0.022759, current_train_items 147488.
I0304 19:30:22.931763 23128000471168 run.py:483] Algo bellman_ford step 4609 current loss 0.067831, current_train_items 147520.
I0304 19:30:22.951502 23128000471168 run.py:483] Algo bellman_ford step 4610 current loss 0.002789, current_train_items 147552.
I0304 19:30:22.967904 23128000471168 run.py:483] Algo bellman_ford step 4611 current loss 0.032135, current_train_items 147584.
I0304 19:30:22.992453 23128000471168 run.py:483] Algo bellman_ford step 4612 current loss 0.060025, current_train_items 147616.
I0304 19:30:23.024894 23128000471168 run.py:483] Algo bellman_ford step 4613 current loss 0.061686, current_train_items 147648.
I0304 19:30:23.059741 23128000471168 run.py:483] Algo bellman_ford step 4614 current loss 0.035200, current_train_items 147680.
I0304 19:30:23.079692 23128000471168 run.py:483] Algo bellman_ford step 4615 current loss 0.011897, current_train_items 147712.
I0304 19:30:23.096261 23128000471168 run.py:483] Algo bellman_ford step 4616 current loss 0.029194, current_train_items 147744.
I0304 19:30:23.120483 23128000471168 run.py:483] Algo bellman_ford step 4617 current loss 0.035223, current_train_items 147776.
I0304 19:30:23.151195 23128000471168 run.py:483] Algo bellman_ford step 4618 current loss 0.034291, current_train_items 147808.
I0304 19:30:23.184948 23128000471168 run.py:483] Algo bellman_ford step 4619 current loss 0.056913, current_train_items 147840.
I0304 19:30:23.204659 23128000471168 run.py:483] Algo bellman_ford step 4620 current loss 0.017865, current_train_items 147872.
I0304 19:30:23.220796 23128000471168 run.py:483] Algo bellman_ford step 4621 current loss 0.032938, current_train_items 147904.
I0304 19:30:23.245063 23128000471168 run.py:483] Algo bellman_ford step 4622 current loss 0.039754, current_train_items 147936.
I0304 19:30:23.276588 23128000471168 run.py:483] Algo bellman_ford step 4623 current loss 0.069780, current_train_items 147968.
I0304 19:30:23.309812 23128000471168 run.py:483] Algo bellman_ford step 4624 current loss 0.180515, current_train_items 148000.
I0304 19:30:23.329644 23128000471168 run.py:483] Algo bellman_ford step 4625 current loss 0.003534, current_train_items 148032.
I0304 19:30:23.346225 23128000471168 run.py:483] Algo bellman_ford step 4626 current loss 0.017263, current_train_items 148064.
I0304 19:30:23.370707 23128000471168 run.py:483] Algo bellman_ford step 4627 current loss 0.064665, current_train_items 148096.
I0304 19:30:23.403091 23128000471168 run.py:483] Algo bellman_ford step 4628 current loss 0.096612, current_train_items 148128.
I0304 19:30:23.438755 23128000471168 run.py:483] Algo bellman_ford step 4629 current loss 0.072073, current_train_items 148160.
I0304 19:30:23.458562 23128000471168 run.py:483] Algo bellman_ford step 4630 current loss 0.001487, current_train_items 148192.
I0304 19:30:23.475144 23128000471168 run.py:483] Algo bellman_ford step 4631 current loss 0.029741, current_train_items 148224.
I0304 19:30:23.500399 23128000471168 run.py:483] Algo bellman_ford step 4632 current loss 0.062649, current_train_items 148256.
I0304 19:30:23.530445 23128000471168 run.py:483] Algo bellman_ford step 4633 current loss 0.041148, current_train_items 148288.
I0304 19:30:23.564761 23128000471168 run.py:483] Algo bellman_ford step 4634 current loss 0.058986, current_train_items 148320.
I0304 19:30:23.584798 23128000471168 run.py:483] Algo bellman_ford step 4635 current loss 0.001879, current_train_items 148352.
I0304 19:30:23.601827 23128000471168 run.py:483] Algo bellman_ford step 4636 current loss 0.006514, current_train_items 148384.
I0304 19:30:23.626430 23128000471168 run.py:483] Algo bellman_ford step 4637 current loss 0.061385, current_train_items 148416.
I0304 19:30:23.659229 23128000471168 run.py:483] Algo bellman_ford step 4638 current loss 0.069894, current_train_items 148448.
I0304 19:30:23.693515 23128000471168 run.py:483] Algo bellman_ford step 4639 current loss 0.073586, current_train_items 148480.
I0304 19:30:23.713540 23128000471168 run.py:483] Algo bellman_ford step 4640 current loss 0.009974, current_train_items 148512.
I0304 19:30:23.729904 23128000471168 run.py:483] Algo bellman_ford step 4641 current loss 0.040268, current_train_items 148544.
I0304 19:30:23.754330 23128000471168 run.py:483] Algo bellman_ford step 4642 current loss 0.069607, current_train_items 148576.
I0304 19:30:23.786310 23128000471168 run.py:483] Algo bellman_ford step 4643 current loss 0.105564, current_train_items 148608.
I0304 19:30:23.821131 23128000471168 run.py:483] Algo bellman_ford step 4644 current loss 0.081769, current_train_items 148640.
I0304 19:30:23.841094 23128000471168 run.py:483] Algo bellman_ford step 4645 current loss 0.004168, current_train_items 148672.
I0304 19:30:23.857704 23128000471168 run.py:483] Algo bellman_ford step 4646 current loss 0.005350, current_train_items 148704.
I0304 19:30:23.880658 23128000471168 run.py:483] Algo bellman_ford step 4647 current loss 0.030879, current_train_items 148736.
I0304 19:30:23.910683 23128000471168 run.py:483] Algo bellman_ford step 4648 current loss 0.051519, current_train_items 148768.
I0304 19:30:23.942730 23128000471168 run.py:483] Algo bellman_ford step 4649 current loss 0.052116, current_train_items 148800.
I0304 19:30:23.962598 23128000471168 run.py:483] Algo bellman_ford step 4650 current loss 0.002470, current_train_items 148832.
I0304 19:30:23.970580 23128000471168 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0304 19:30:23.970688 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:23.987972 23128000471168 run.py:483] Algo bellman_ford step 4651 current loss 0.016700, current_train_items 148864.
I0304 19:30:24.012937 23128000471168 run.py:483] Algo bellman_ford step 4652 current loss 0.033080, current_train_items 148896.
I0304 19:30:24.045983 23128000471168 run.py:483] Algo bellman_ford step 4653 current loss 0.058688, current_train_items 148928.
I0304 19:30:24.080543 23128000471168 run.py:483] Algo bellman_ford step 4654 current loss 0.065925, current_train_items 148960.
I0304 19:30:24.101017 23128000471168 run.py:483] Algo bellman_ford step 4655 current loss 0.002663, current_train_items 148992.
I0304 19:30:24.117217 23128000471168 run.py:483] Algo bellman_ford step 4656 current loss 0.013598, current_train_items 149024.
I0304 19:30:24.141684 23128000471168 run.py:483] Algo bellman_ford step 4657 current loss 0.114486, current_train_items 149056.
I0304 19:30:24.172937 23128000471168 run.py:483] Algo bellman_ford step 4658 current loss 0.163391, current_train_items 149088.
I0304 19:30:24.205987 23128000471168 run.py:483] Algo bellman_ford step 4659 current loss 0.169337, current_train_items 149120.
I0304 19:30:24.226667 23128000471168 run.py:483] Algo bellman_ford step 4660 current loss 0.002824, current_train_items 149152.
I0304 19:30:24.243374 23128000471168 run.py:483] Algo bellman_ford step 4661 current loss 0.035989, current_train_items 149184.
I0304 19:30:24.268203 23128000471168 run.py:483] Algo bellman_ford step 4662 current loss 0.070750, current_train_items 149216.
I0304 19:30:24.299126 23128000471168 run.py:483] Algo bellman_ford step 4663 current loss 0.083850, current_train_items 149248.
I0304 19:30:24.336883 23128000471168 run.py:483] Algo bellman_ford step 4664 current loss 0.097080, current_train_items 149280.
I0304 19:30:24.357027 23128000471168 run.py:483] Algo bellman_ford step 4665 current loss 0.006780, current_train_items 149312.
I0304 19:30:24.373273 23128000471168 run.py:483] Algo bellman_ford step 4666 current loss 0.020573, current_train_items 149344.
I0304 19:30:24.396940 23128000471168 run.py:483] Algo bellman_ford step 4667 current loss 0.067509, current_train_items 149376.
I0304 19:30:24.429089 23128000471168 run.py:483] Algo bellman_ford step 4668 current loss 0.093246, current_train_items 149408.
I0304 19:30:24.463588 23128000471168 run.py:483] Algo bellman_ford step 4669 current loss 0.108918, current_train_items 149440.
I0304 19:30:24.483894 23128000471168 run.py:483] Algo bellman_ford step 4670 current loss 0.002633, current_train_items 149472.
I0304 19:30:24.500097 23128000471168 run.py:483] Algo bellman_ford step 4671 current loss 0.019106, current_train_items 149504.
I0304 19:30:24.523151 23128000471168 run.py:483] Algo bellman_ford step 4672 current loss 0.055917, current_train_items 149536.
I0304 19:30:24.553214 23128000471168 run.py:483] Algo bellman_ford step 4673 current loss 0.124710, current_train_items 149568.
I0304 19:30:24.586650 23128000471168 run.py:483] Algo bellman_ford step 4674 current loss 0.074884, current_train_items 149600.
I0304 19:30:24.607156 23128000471168 run.py:483] Algo bellman_ford step 4675 current loss 0.006614, current_train_items 149632.
I0304 19:30:24.624180 23128000471168 run.py:483] Algo bellman_ford step 4676 current loss 0.028184, current_train_items 149664.
I0304 19:30:24.648386 23128000471168 run.py:483] Algo bellman_ford step 4677 current loss 0.083451, current_train_items 149696.
I0304 19:30:24.681430 23128000471168 run.py:483] Algo bellman_ford step 4678 current loss 0.077787, current_train_items 149728.
I0304 19:30:24.715542 23128000471168 run.py:483] Algo bellman_ford step 4679 current loss 0.086497, current_train_items 149760.
I0304 19:30:24.735305 23128000471168 run.py:483] Algo bellman_ford step 4680 current loss 0.001788, current_train_items 149792.
I0304 19:30:24.751499 23128000471168 run.py:483] Algo bellman_ford step 4681 current loss 0.010727, current_train_items 149824.
I0304 19:30:24.775297 23128000471168 run.py:483] Algo bellman_ford step 4682 current loss 0.054374, current_train_items 149856.
I0304 19:30:24.806530 23128000471168 run.py:483] Algo bellman_ford step 4683 current loss 0.039248, current_train_items 149888.
I0304 19:30:24.841695 23128000471168 run.py:483] Algo bellman_ford step 4684 current loss 0.070231, current_train_items 149920.
I0304 19:30:24.862245 23128000471168 run.py:483] Algo bellman_ford step 4685 current loss 0.006042, current_train_items 149952.
I0304 19:30:24.878507 23128000471168 run.py:483] Algo bellman_ford step 4686 current loss 0.008307, current_train_items 149984.
I0304 19:30:24.902366 23128000471168 run.py:483] Algo bellman_ford step 4687 current loss 0.061257, current_train_items 150016.
I0304 19:30:24.934486 23128000471168 run.py:483] Algo bellman_ford step 4688 current loss 0.038957, current_train_items 150048.
I0304 19:30:24.970063 23128000471168 run.py:483] Algo bellman_ford step 4689 current loss 0.074482, current_train_items 150080.
I0304 19:30:24.990348 23128000471168 run.py:483] Algo bellman_ford step 4690 current loss 0.002140, current_train_items 150112.
I0304 19:30:25.006844 23128000471168 run.py:483] Algo bellman_ford step 4691 current loss 0.010096, current_train_items 150144.
I0304 19:30:25.030175 23128000471168 run.py:483] Algo bellman_ford step 4692 current loss 0.037863, current_train_items 150176.
I0304 19:30:25.061827 23128000471168 run.py:483] Algo bellman_ford step 4693 current loss 0.085126, current_train_items 150208.
I0304 19:30:25.095273 23128000471168 run.py:483] Algo bellman_ford step 4694 current loss 0.079881, current_train_items 150240.
I0304 19:30:25.115154 23128000471168 run.py:483] Algo bellman_ford step 4695 current loss 0.001558, current_train_items 150272.
I0304 19:30:25.131606 23128000471168 run.py:483] Algo bellman_ford step 4696 current loss 0.011445, current_train_items 150304.
I0304 19:30:25.156601 23128000471168 run.py:483] Algo bellman_ford step 4697 current loss 0.064077, current_train_items 150336.
I0304 19:30:25.187689 23128000471168 run.py:483] Algo bellman_ford step 4698 current loss 0.074238, current_train_items 150368.
I0304 19:30:25.220263 23128000471168 run.py:483] Algo bellman_ford step 4699 current loss 0.095498, current_train_items 150400.
I0304 19:30:25.240444 23128000471168 run.py:483] Algo bellman_ford step 4700 current loss 0.002443, current_train_items 150432.
I0304 19:30:25.248237 23128000471168 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0304 19:30:25.248346 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:30:25.265024 23128000471168 run.py:483] Algo bellman_ford step 4701 current loss 0.017212, current_train_items 150464.
I0304 19:30:25.290096 23128000471168 run.py:483] Algo bellman_ford step 4702 current loss 0.067398, current_train_items 150496.
I0304 19:30:25.323462 23128000471168 run.py:483] Algo bellman_ford step 4703 current loss 0.054706, current_train_items 150528.
I0304 19:30:25.359197 23128000471168 run.py:483] Algo bellman_ford step 4704 current loss 0.058462, current_train_items 150560.
I0304 19:30:25.379700 23128000471168 run.py:483] Algo bellman_ford step 4705 current loss 0.012487, current_train_items 150592.
I0304 19:30:25.395617 23128000471168 run.py:483] Algo bellman_ford step 4706 current loss 0.014871, current_train_items 150624.
I0304 19:30:25.420140 23128000471168 run.py:483] Algo bellman_ford step 4707 current loss 0.032465, current_train_items 150656.
I0304 19:30:25.452919 23128000471168 run.py:483] Algo bellman_ford step 4708 current loss 0.080686, current_train_items 150688.
I0304 19:30:25.487365 23128000471168 run.py:483] Algo bellman_ford step 4709 current loss 0.065020, current_train_items 150720.
I0304 19:30:25.507516 23128000471168 run.py:483] Algo bellman_ford step 4710 current loss 0.013904, current_train_items 150752.
I0304 19:30:25.524023 23128000471168 run.py:483] Algo bellman_ford step 4711 current loss 0.013447, current_train_items 150784.
I0304 19:30:25.549111 23128000471168 run.py:483] Algo bellman_ford step 4712 current loss 0.045270, current_train_items 150816.
I0304 19:30:25.580809 23128000471168 run.py:483] Algo bellman_ford step 4713 current loss 0.074005, current_train_items 150848.
I0304 19:30:25.613774 23128000471168 run.py:483] Algo bellman_ford step 4714 current loss 0.065124, current_train_items 150880.
I0304 19:30:25.633911 23128000471168 run.py:483] Algo bellman_ford step 4715 current loss 0.002186, current_train_items 150912.
I0304 19:30:25.650404 23128000471168 run.py:483] Algo bellman_ford step 4716 current loss 0.028470, current_train_items 150944.
I0304 19:30:25.673408 23128000471168 run.py:483] Algo bellman_ford step 4717 current loss 0.019992, current_train_items 150976.
I0304 19:30:25.705224 23128000471168 run.py:483] Algo bellman_ford step 4718 current loss 0.029858, current_train_items 151008.
I0304 19:30:25.741467 23128000471168 run.py:483] Algo bellman_ford step 4719 current loss 0.093225, current_train_items 151040.
I0304 19:30:25.761433 23128000471168 run.py:483] Algo bellman_ford step 4720 current loss 0.002771, current_train_items 151072.
I0304 19:30:25.777641 23128000471168 run.py:483] Algo bellman_ford step 4721 current loss 0.013825, current_train_items 151104.
I0304 19:30:25.802885 23128000471168 run.py:483] Algo bellman_ford step 4722 current loss 0.115650, current_train_items 151136.
I0304 19:30:25.833325 23128000471168 run.py:483] Algo bellman_ford step 4723 current loss 0.080647, current_train_items 151168.
I0304 19:30:25.867918 23128000471168 run.py:483] Algo bellman_ford step 4724 current loss 0.081676, current_train_items 151200.
I0304 19:30:25.887570 23128000471168 run.py:483] Algo bellman_ford step 4725 current loss 0.002221, current_train_items 151232.
I0304 19:30:25.904559 23128000471168 run.py:483] Algo bellman_ford step 4726 current loss 0.028873, current_train_items 151264.
I0304 19:30:25.929746 23128000471168 run.py:483] Algo bellman_ford step 4727 current loss 0.070035, current_train_items 151296.
I0304 19:30:25.959629 23128000471168 run.py:483] Algo bellman_ford step 4728 current loss 0.047619, current_train_items 151328.
I0304 19:30:25.990124 23128000471168 run.py:483] Algo bellman_ford step 4729 current loss 0.073015, current_train_items 151360.
I0304 19:30:26.009996 23128000471168 run.py:483] Algo bellman_ford step 4730 current loss 0.004618, current_train_items 151392.
I0304 19:30:26.026374 23128000471168 run.py:483] Algo bellman_ford step 4731 current loss 0.005544, current_train_items 151424.
I0304 19:30:26.050087 23128000471168 run.py:483] Algo bellman_ford step 4732 current loss 0.044316, current_train_items 151456.
I0304 19:30:26.081383 23128000471168 run.py:483] Algo bellman_ford step 4733 current loss 0.049307, current_train_items 151488.
I0304 19:30:26.115348 23128000471168 run.py:483] Algo bellman_ford step 4734 current loss 0.100373, current_train_items 151520.
I0304 19:30:26.135327 23128000471168 run.py:483] Algo bellman_ford step 4735 current loss 0.002226, current_train_items 151552.
I0304 19:30:26.152069 23128000471168 run.py:483] Algo bellman_ford step 4736 current loss 0.028242, current_train_items 151584.
I0304 19:30:26.175716 23128000471168 run.py:483] Algo bellman_ford step 4737 current loss 0.064759, current_train_items 151616.
I0304 19:30:26.206798 23128000471168 run.py:483] Algo bellman_ford step 4738 current loss 0.071440, current_train_items 151648.
I0304 19:30:26.240363 23128000471168 run.py:483] Algo bellman_ford step 4739 current loss 0.080622, current_train_items 151680.
I0304 19:30:26.260520 23128000471168 run.py:483] Algo bellman_ford step 4740 current loss 0.019506, current_train_items 151712.
I0304 19:30:26.276846 23128000471168 run.py:483] Algo bellman_ford step 4741 current loss 0.029623, current_train_items 151744.
I0304 19:30:26.300403 23128000471168 run.py:483] Algo bellman_ford step 4742 current loss 0.053301, current_train_items 151776.
I0304 19:30:26.332432 23128000471168 run.py:483] Algo bellman_ford step 4743 current loss 0.060871, current_train_items 151808.
I0304 19:30:26.365050 23128000471168 run.py:483] Algo bellman_ford step 4744 current loss 0.064658, current_train_items 151840.
I0304 19:30:26.385250 23128000471168 run.py:483] Algo bellman_ford step 4745 current loss 0.008903, current_train_items 151872.
I0304 19:30:26.402150 23128000471168 run.py:483] Algo bellman_ford step 4746 current loss 0.035283, current_train_items 151904.
I0304 19:30:26.427749 23128000471168 run.py:483] Algo bellman_ford step 4747 current loss 0.039219, current_train_items 151936.
I0304 19:30:26.459085 23128000471168 run.py:483] Algo bellman_ford step 4748 current loss 0.058580, current_train_items 151968.
I0304 19:30:26.492714 23128000471168 run.py:483] Algo bellman_ford step 4749 current loss 0.072911, current_train_items 152000.
I0304 19:30:26.512715 23128000471168 run.py:483] Algo bellman_ford step 4750 current loss 0.016262, current_train_items 152032.
I0304 19:30:26.520888 23128000471168 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0304 19:30:26.520997 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.991, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:30:26.551027 23128000471168 run.py:483] Algo bellman_ford step 4751 current loss 0.013888, current_train_items 152064.
I0304 19:30:26.575894 23128000471168 run.py:483] Algo bellman_ford step 4752 current loss 0.030015, current_train_items 152096.
I0304 19:30:26.609538 23128000471168 run.py:483] Algo bellman_ford step 4753 current loss 0.054676, current_train_items 152128.
I0304 19:30:26.643908 23128000471168 run.py:483] Algo bellman_ford step 4754 current loss 0.045008, current_train_items 152160.
I0304 19:30:26.664480 23128000471168 run.py:483] Algo bellman_ford step 4755 current loss 0.002807, current_train_items 152192.
I0304 19:30:26.680841 23128000471168 run.py:483] Algo bellman_ford step 4756 current loss 0.003492, current_train_items 152224.
I0304 19:30:26.705733 23128000471168 run.py:483] Algo bellman_ford step 4757 current loss 0.052314, current_train_items 152256.
I0304 19:30:26.737544 23128000471168 run.py:483] Algo bellman_ford step 4758 current loss 0.060570, current_train_items 152288.
I0304 19:30:26.772112 23128000471168 run.py:483] Algo bellman_ford step 4759 current loss 0.057817, current_train_items 152320.
I0304 19:30:26.792350 23128000471168 run.py:483] Algo bellman_ford step 4760 current loss 0.002171, current_train_items 152352.
I0304 19:30:26.809298 23128000471168 run.py:483] Algo bellman_ford step 4761 current loss 0.012840, current_train_items 152384.
I0304 19:30:26.833561 23128000471168 run.py:483] Algo bellman_ford step 4762 current loss 0.054847, current_train_items 152416.
I0304 19:30:26.864417 23128000471168 run.py:483] Algo bellman_ford step 4763 current loss 0.051533, current_train_items 152448.
I0304 19:30:26.899122 23128000471168 run.py:483] Algo bellman_ford step 4764 current loss 0.059794, current_train_items 152480.
I0304 19:30:26.919209 23128000471168 run.py:483] Algo bellman_ford step 4765 current loss 0.006404, current_train_items 152512.
I0304 19:30:26.935199 23128000471168 run.py:483] Algo bellman_ford step 4766 current loss 0.010572, current_train_items 152544.
I0304 19:30:26.960340 23128000471168 run.py:483] Algo bellman_ford step 4767 current loss 0.073634, current_train_items 152576.
I0304 19:30:26.990843 23128000471168 run.py:483] Algo bellman_ford step 4768 current loss 0.042409, current_train_items 152608.
I0304 19:30:27.023679 23128000471168 run.py:483] Algo bellman_ford step 4769 current loss 0.055347, current_train_items 152640.
I0304 19:30:27.044140 23128000471168 run.py:483] Algo bellman_ford step 4770 current loss 0.003661, current_train_items 152672.
I0304 19:30:27.060760 23128000471168 run.py:483] Algo bellman_ford step 4771 current loss 0.004390, current_train_items 152704.
I0304 19:30:27.083564 23128000471168 run.py:483] Algo bellman_ford step 4772 current loss 0.030761, current_train_items 152736.
I0304 19:30:27.114862 23128000471168 run.py:483] Algo bellman_ford step 4773 current loss 0.079964, current_train_items 152768.
I0304 19:30:27.149433 23128000471168 run.py:483] Algo bellman_ford step 4774 current loss 0.126462, current_train_items 152800.
I0304 19:30:27.169420 23128000471168 run.py:483] Algo bellman_ford step 4775 current loss 0.002318, current_train_items 152832.
I0304 19:30:27.186020 23128000471168 run.py:483] Algo bellman_ford step 4776 current loss 0.033782, current_train_items 152864.
I0304 19:30:27.209732 23128000471168 run.py:483] Algo bellman_ford step 4777 current loss 0.039994, current_train_items 152896.
I0304 19:30:27.241864 23128000471168 run.py:483] Algo bellman_ford step 4778 current loss 0.079483, current_train_items 152928.
I0304 19:30:27.274457 23128000471168 run.py:483] Algo bellman_ford step 4779 current loss 0.044529, current_train_items 152960.
I0304 19:30:27.294316 23128000471168 run.py:483] Algo bellman_ford step 4780 current loss 0.002024, current_train_items 152992.
I0304 19:30:27.310822 23128000471168 run.py:483] Algo bellman_ford step 4781 current loss 0.008927, current_train_items 153024.
I0304 19:30:27.335121 23128000471168 run.py:483] Algo bellman_ford step 4782 current loss 0.038375, current_train_items 153056.
I0304 19:30:27.367153 23128000471168 run.py:483] Algo bellman_ford step 4783 current loss 0.047965, current_train_items 153088.
I0304 19:30:27.402308 23128000471168 run.py:483] Algo bellman_ford step 4784 current loss 0.076226, current_train_items 153120.
I0304 19:30:27.422571 23128000471168 run.py:483] Algo bellman_ford step 4785 current loss 0.021314, current_train_items 153152.
I0304 19:30:27.439421 23128000471168 run.py:483] Algo bellman_ford step 4786 current loss 0.020608, current_train_items 153184.
I0304 19:30:27.463857 23128000471168 run.py:483] Algo bellman_ford step 4787 current loss 0.098504, current_train_items 153216.
I0304 19:30:27.495069 23128000471168 run.py:483] Algo bellman_ford step 4788 current loss 0.103606, current_train_items 153248.
I0304 19:30:27.527836 23128000471168 run.py:483] Algo bellman_ford step 4789 current loss 0.086112, current_train_items 153280.
I0304 19:30:27.547636 23128000471168 run.py:483] Algo bellman_ford step 4790 current loss 0.001626, current_train_items 153312.
I0304 19:30:27.564277 23128000471168 run.py:483] Algo bellman_ford step 4791 current loss 0.017449, current_train_items 153344.
I0304 19:30:27.588405 23128000471168 run.py:483] Algo bellman_ford step 4792 current loss 0.088951, current_train_items 153376.
I0304 19:30:27.619547 23128000471168 run.py:483] Algo bellman_ford step 4793 current loss 0.176296, current_train_items 153408.
I0304 19:30:27.655714 23128000471168 run.py:483] Algo bellman_ford step 4794 current loss 0.083696, current_train_items 153440.
I0304 19:30:27.675373 23128000471168 run.py:483] Algo bellman_ford step 4795 current loss 0.001629, current_train_items 153472.
I0304 19:30:27.691833 23128000471168 run.py:483] Algo bellman_ford step 4796 current loss 0.005003, current_train_items 153504.
I0304 19:30:27.716303 23128000471168 run.py:483] Algo bellman_ford step 4797 current loss 0.039574, current_train_items 153536.
I0304 19:30:27.747470 23128000471168 run.py:483] Algo bellman_ford step 4798 current loss 0.090191, current_train_items 153568.
I0304 19:30:27.780482 23128000471168 run.py:483] Algo bellman_ford step 4799 current loss 0.104021, current_train_items 153600.
I0304 19:30:27.800551 23128000471168 run.py:483] Algo bellman_ford step 4800 current loss 0.001336, current_train_items 153632.
I0304 19:30:27.808319 23128000471168 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0304 19:30:27.808427 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:27.825656 23128000471168 run.py:483] Algo bellman_ford step 4801 current loss 0.046472, current_train_items 153664.
I0304 19:30:27.850576 23128000471168 run.py:483] Algo bellman_ford step 4802 current loss 0.056551, current_train_items 153696.
I0304 19:30:27.881919 23128000471168 run.py:483] Algo bellman_ford step 4803 current loss 0.031492, current_train_items 153728.
I0304 19:30:27.918561 23128000471168 run.py:483] Algo bellman_ford step 4804 current loss 0.076669, current_train_items 153760.
I0304 19:30:27.938559 23128000471168 run.py:483] Algo bellman_ford step 4805 current loss 0.018679, current_train_items 153792.
I0304 19:30:27.955220 23128000471168 run.py:483] Algo bellman_ford step 4806 current loss 0.012672, current_train_items 153824.
I0304 19:30:27.979614 23128000471168 run.py:483] Algo bellman_ford step 4807 current loss 0.040410, current_train_items 153856.
I0304 19:30:28.011319 23128000471168 run.py:483] Algo bellman_ford step 4808 current loss 0.103209, current_train_items 153888.
I0304 19:30:28.045405 23128000471168 run.py:483] Algo bellman_ford step 4809 current loss 0.083741, current_train_items 153920.
I0304 19:30:28.065433 23128000471168 run.py:483] Algo bellman_ford step 4810 current loss 0.001562, current_train_items 153952.
I0304 19:30:28.081866 23128000471168 run.py:483] Algo bellman_ford step 4811 current loss 0.012566, current_train_items 153984.
I0304 19:30:28.105586 23128000471168 run.py:483] Algo bellman_ford step 4812 current loss 0.061994, current_train_items 154016.
I0304 19:30:28.136407 23128000471168 run.py:483] Algo bellman_ford step 4813 current loss 0.079570, current_train_items 154048.
I0304 19:30:28.171876 23128000471168 run.py:483] Algo bellman_ford step 4814 current loss 0.093448, current_train_items 154080.
I0304 19:30:28.191764 23128000471168 run.py:483] Algo bellman_ford step 4815 current loss 0.002857, current_train_items 154112.
I0304 19:30:28.208272 23128000471168 run.py:483] Algo bellman_ford step 4816 current loss 0.067564, current_train_items 154144.
I0304 19:30:28.233517 23128000471168 run.py:483] Algo bellman_ford step 4817 current loss 0.075155, current_train_items 154176.
I0304 19:30:28.265201 23128000471168 run.py:483] Algo bellman_ford step 4818 current loss 0.052257, current_train_items 154208.
I0304 19:30:28.295388 23128000471168 run.py:483] Algo bellman_ford step 4819 current loss 0.044005, current_train_items 154240.
I0304 19:30:28.315452 23128000471168 run.py:483] Algo bellman_ford step 4820 current loss 0.004232, current_train_items 154272.
I0304 19:30:28.331794 23128000471168 run.py:483] Algo bellman_ford step 4821 current loss 0.034885, current_train_items 154304.
I0304 19:30:28.356149 23128000471168 run.py:483] Algo bellman_ford step 4822 current loss 0.073833, current_train_items 154336.
I0304 19:30:28.387380 23128000471168 run.py:483] Algo bellman_ford step 4823 current loss 0.048505, current_train_items 154368.
I0304 19:30:28.422277 23128000471168 run.py:483] Algo bellman_ford step 4824 current loss 0.102873, current_train_items 154400.
I0304 19:30:28.442600 23128000471168 run.py:483] Algo bellman_ford step 4825 current loss 0.006152, current_train_items 154432.
I0304 19:30:28.459672 23128000471168 run.py:483] Algo bellman_ford step 4826 current loss 0.020706, current_train_items 154464.
I0304 19:30:28.483547 23128000471168 run.py:483] Algo bellman_ford step 4827 current loss 0.028644, current_train_items 154496.
I0304 19:30:28.515168 23128000471168 run.py:483] Algo bellman_ford step 4828 current loss 0.068054, current_train_items 154528.
I0304 19:30:28.549529 23128000471168 run.py:483] Algo bellman_ford step 4829 current loss 0.139020, current_train_items 154560.
I0304 19:30:28.569341 23128000471168 run.py:483] Algo bellman_ford step 4830 current loss 0.004435, current_train_items 154592.
I0304 19:30:28.586192 23128000471168 run.py:483] Algo bellman_ford step 4831 current loss 0.030576, current_train_items 154624.
I0304 19:30:28.611198 23128000471168 run.py:483] Algo bellman_ford step 4832 current loss 0.028312, current_train_items 154656.
I0304 19:30:28.643238 23128000471168 run.py:483] Algo bellman_ford step 4833 current loss 0.055485, current_train_items 154688.
I0304 19:30:28.676685 23128000471168 run.py:483] Algo bellman_ford step 4834 current loss 0.061465, current_train_items 154720.
I0304 19:30:28.696476 23128000471168 run.py:483] Algo bellman_ford step 4835 current loss 0.004335, current_train_items 154752.
I0304 19:30:28.712865 23128000471168 run.py:483] Algo bellman_ford step 4836 current loss 0.009149, current_train_items 154784.
I0304 19:30:28.735877 23128000471168 run.py:483] Algo bellman_ford step 4837 current loss 0.041652, current_train_items 154816.
I0304 19:30:28.767191 23128000471168 run.py:483] Algo bellman_ford step 4838 current loss 0.044310, current_train_items 154848.
I0304 19:30:28.798908 23128000471168 run.py:483] Algo bellman_ford step 4839 current loss 0.056943, current_train_items 154880.
I0304 19:30:28.819133 23128000471168 run.py:483] Algo bellman_ford step 4840 current loss 0.003905, current_train_items 154912.
I0304 19:30:28.835477 23128000471168 run.py:483] Algo bellman_ford step 4841 current loss 0.013376, current_train_items 154944.
I0304 19:30:28.859758 23128000471168 run.py:483] Algo bellman_ford step 4842 current loss 0.033580, current_train_items 154976.
I0304 19:30:28.890611 23128000471168 run.py:483] Algo bellman_ford step 4843 current loss 0.056128, current_train_items 155008.
I0304 19:30:28.925556 23128000471168 run.py:483] Algo bellman_ford step 4844 current loss 0.077481, current_train_items 155040.
I0304 19:30:28.945586 23128000471168 run.py:483] Algo bellman_ford step 4845 current loss 0.004282, current_train_items 155072.
I0304 19:30:28.962263 23128000471168 run.py:483] Algo bellman_ford step 4846 current loss 0.012335, current_train_items 155104.
I0304 19:30:28.985688 23128000471168 run.py:483] Algo bellman_ford step 4847 current loss 0.035077, current_train_items 155136.
I0304 19:30:29.016378 23128000471168 run.py:483] Algo bellman_ford step 4848 current loss 0.063556, current_train_items 155168.
I0304 19:30:29.050181 23128000471168 run.py:483] Algo bellman_ford step 4849 current loss 0.094368, current_train_items 155200.
I0304 19:30:29.070413 23128000471168 run.py:483] Algo bellman_ford step 4850 current loss 0.003577, current_train_items 155232.
I0304 19:30:29.078415 23128000471168 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0304 19:30:29.078522 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:29.095929 23128000471168 run.py:483] Algo bellman_ford step 4851 current loss 0.029387, current_train_items 155264.
I0304 19:30:29.119614 23128000471168 run.py:483] Algo bellman_ford step 4852 current loss 0.048941, current_train_items 155296.
I0304 19:30:29.150796 23128000471168 run.py:483] Algo bellman_ford step 4853 current loss 0.066365, current_train_items 155328.
I0304 19:30:29.184761 23128000471168 run.py:483] Algo bellman_ford step 4854 current loss 0.051691, current_train_items 155360.
I0304 19:30:29.204859 23128000471168 run.py:483] Algo bellman_ford step 4855 current loss 0.005006, current_train_items 155392.
I0304 19:30:29.221525 23128000471168 run.py:483] Algo bellman_ford step 4856 current loss 0.019959, current_train_items 155424.
I0304 19:30:29.246538 23128000471168 run.py:483] Algo bellman_ford step 4857 current loss 0.033165, current_train_items 155456.
I0304 19:30:29.278854 23128000471168 run.py:483] Algo bellman_ford step 4858 current loss 0.051074, current_train_items 155488.
I0304 19:30:29.314566 23128000471168 run.py:483] Algo bellman_ford step 4859 current loss 0.071765, current_train_items 155520.
I0304 19:30:29.334491 23128000471168 run.py:483] Algo bellman_ford step 4860 current loss 0.004302, current_train_items 155552.
I0304 19:30:29.351976 23128000471168 run.py:483] Algo bellman_ford step 4861 current loss 0.009968, current_train_items 155584.
I0304 19:30:29.375643 23128000471168 run.py:483] Algo bellman_ford step 4862 current loss 0.039799, current_train_items 155616.
I0304 19:30:29.407638 23128000471168 run.py:483] Algo bellman_ford step 4863 current loss 0.035302, current_train_items 155648.
I0304 19:30:29.440359 23128000471168 run.py:483] Algo bellman_ford step 4864 current loss 0.030571, current_train_items 155680.
I0304 19:30:29.460154 23128000471168 run.py:483] Algo bellman_ford step 4865 current loss 0.003836, current_train_items 155712.
I0304 19:30:29.476312 23128000471168 run.py:483] Algo bellman_ford step 4866 current loss 0.028788, current_train_items 155744.
I0304 19:30:29.500774 23128000471168 run.py:483] Algo bellman_ford step 4867 current loss 0.029495, current_train_items 155776.
I0304 19:30:29.533456 23128000471168 run.py:483] Algo bellman_ford step 4868 current loss 0.057790, current_train_items 155808.
I0304 19:30:29.566081 23128000471168 run.py:483] Algo bellman_ford step 4869 current loss 0.037167, current_train_items 155840.
I0304 19:30:29.586674 23128000471168 run.py:483] Algo bellman_ford step 4870 current loss 0.002383, current_train_items 155872.
I0304 19:30:29.603329 23128000471168 run.py:483] Algo bellman_ford step 4871 current loss 0.062445, current_train_items 155904.
I0304 19:30:29.627097 23128000471168 run.py:483] Algo bellman_ford step 4872 current loss 0.054717, current_train_items 155936.
I0304 19:30:29.656835 23128000471168 run.py:483] Algo bellman_ford step 4873 current loss 0.045116, current_train_items 155968.
I0304 19:30:29.689328 23128000471168 run.py:483] Algo bellman_ford step 4874 current loss 0.060838, current_train_items 156000.
I0304 19:30:29.709598 23128000471168 run.py:483] Algo bellman_ford step 4875 current loss 0.002452, current_train_items 156032.
I0304 19:30:29.725878 23128000471168 run.py:483] Algo bellman_ford step 4876 current loss 0.025552, current_train_items 156064.
I0304 19:30:29.749227 23128000471168 run.py:483] Algo bellman_ford step 4877 current loss 0.020264, current_train_items 156096.
I0304 19:30:29.779628 23128000471168 run.py:483] Algo bellman_ford step 4878 current loss 0.032230, current_train_items 156128.
I0304 19:30:29.813459 23128000471168 run.py:483] Algo bellman_ford step 4879 current loss 0.070435, current_train_items 156160.
I0304 19:30:29.833361 23128000471168 run.py:483] Algo bellman_ford step 4880 current loss 0.004082, current_train_items 156192.
I0304 19:30:29.849813 23128000471168 run.py:483] Algo bellman_ford step 4881 current loss 0.031217, current_train_items 156224.
I0304 19:30:29.874281 23128000471168 run.py:483] Algo bellman_ford step 4882 current loss 0.041722, current_train_items 156256.
I0304 19:30:29.906296 23128000471168 run.py:483] Algo bellman_ford step 4883 current loss 0.061315, current_train_items 156288.
I0304 19:30:29.940739 23128000471168 run.py:483] Algo bellman_ford step 4884 current loss 0.097598, current_train_items 156320.
I0304 19:30:29.960790 23128000471168 run.py:483] Algo bellman_ford step 4885 current loss 0.004454, current_train_items 156352.
I0304 19:30:29.977622 23128000471168 run.py:483] Algo bellman_ford step 4886 current loss 0.029782, current_train_items 156384.
I0304 19:30:30.001421 23128000471168 run.py:483] Algo bellman_ford step 4887 current loss 0.056277, current_train_items 156416.
I0304 19:30:30.032244 23128000471168 run.py:483] Algo bellman_ford step 4888 current loss 0.043939, current_train_items 156448.
I0304 19:30:30.065352 23128000471168 run.py:483] Algo bellman_ford step 4889 current loss 0.082772, current_train_items 156480.
I0304 19:30:30.085554 23128000471168 run.py:483] Algo bellman_ford step 4890 current loss 0.001801, current_train_items 156512.
I0304 19:30:30.102101 23128000471168 run.py:483] Algo bellman_ford step 4891 current loss 0.015956, current_train_items 156544.
I0304 19:30:30.125874 23128000471168 run.py:483] Algo bellman_ford step 4892 current loss 0.031701, current_train_items 156576.
I0304 19:30:30.156079 23128000471168 run.py:483] Algo bellman_ford step 4893 current loss 0.064064, current_train_items 156608.
I0304 19:30:30.188695 23128000471168 run.py:483] Algo bellman_ford step 4894 current loss 0.088867, current_train_items 156640.
I0304 19:30:30.208372 23128000471168 run.py:483] Algo bellman_ford step 4895 current loss 0.050416, current_train_items 156672.
I0304 19:30:30.224952 23128000471168 run.py:483] Algo bellman_ford step 4896 current loss 0.009205, current_train_items 156704.
I0304 19:30:30.248382 23128000471168 run.py:483] Algo bellman_ford step 4897 current loss 0.024422, current_train_items 156736.
I0304 19:30:30.278987 23128000471168 run.py:483] Algo bellman_ford step 4898 current loss 0.041754, current_train_items 156768.
I0304 19:30:30.313564 23128000471168 run.py:483] Algo bellman_ford step 4899 current loss 0.058489, current_train_items 156800.
I0304 19:30:30.333817 23128000471168 run.py:483] Algo bellman_ford step 4900 current loss 0.008022, current_train_items 156832.
I0304 19:30:30.341742 23128000471168 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0304 19:30:30.341882 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:30.358526 23128000471168 run.py:483] Algo bellman_ford step 4901 current loss 0.016129, current_train_items 156864.
I0304 19:30:30.383275 23128000471168 run.py:483] Algo bellman_ford step 4902 current loss 0.046718, current_train_items 156896.
I0304 19:30:30.416910 23128000471168 run.py:483] Algo bellman_ford step 4903 current loss 0.046351, current_train_items 156928.
I0304 19:30:30.450517 23128000471168 run.py:483] Algo bellman_ford step 4904 current loss 0.058745, current_train_items 156960.
I0304 19:30:30.470750 23128000471168 run.py:483] Algo bellman_ford step 4905 current loss 0.007571, current_train_items 156992.
I0304 19:30:30.486903 23128000471168 run.py:483] Algo bellman_ford step 4906 current loss 0.032545, current_train_items 157024.
I0304 19:30:30.511594 23128000471168 run.py:483] Algo bellman_ford step 4907 current loss 0.045063, current_train_items 157056.
I0304 19:30:30.543368 23128000471168 run.py:483] Algo bellman_ford step 4908 current loss 0.061573, current_train_items 157088.
I0304 19:30:30.576883 23128000471168 run.py:483] Algo bellman_ford step 4909 current loss 0.067130, current_train_items 157120.
I0304 19:30:30.596716 23128000471168 run.py:483] Algo bellman_ford step 4910 current loss 0.005720, current_train_items 157152.
I0304 19:30:30.613526 23128000471168 run.py:483] Algo bellman_ford step 4911 current loss 0.025962, current_train_items 157184.
I0304 19:30:30.637511 23128000471168 run.py:483] Algo bellman_ford step 4912 current loss 0.042024, current_train_items 157216.
I0304 19:30:30.669940 23128000471168 run.py:483] Algo bellman_ford step 4913 current loss 0.092668, current_train_items 157248.
I0304 19:30:30.703298 23128000471168 run.py:483] Algo bellman_ford step 4914 current loss 0.074581, current_train_items 157280.
I0304 19:30:30.723339 23128000471168 run.py:483] Algo bellman_ford step 4915 current loss 0.005460, current_train_items 157312.
I0304 19:30:30.739547 23128000471168 run.py:483] Algo bellman_ford step 4916 current loss 0.031483, current_train_items 157344.
I0304 19:30:30.763772 23128000471168 run.py:483] Algo bellman_ford step 4917 current loss 0.045351, current_train_items 157376.
I0304 19:30:30.793920 23128000471168 run.py:483] Algo bellman_ford step 4918 current loss 0.041441, current_train_items 157408.
I0304 19:30:30.827615 23128000471168 run.py:483] Algo bellman_ford step 4919 current loss 0.064657, current_train_items 157440.
I0304 19:30:30.847490 23128000471168 run.py:483] Algo bellman_ford step 4920 current loss 0.003539, current_train_items 157472.
I0304 19:30:30.863648 23128000471168 run.py:483] Algo bellman_ford step 4921 current loss 0.011990, current_train_items 157504.
I0304 19:30:30.888010 23128000471168 run.py:483] Algo bellman_ford step 4922 current loss 0.048827, current_train_items 157536.
I0304 19:30:30.919264 23128000471168 run.py:483] Algo bellman_ford step 4923 current loss 0.035570, current_train_items 157568.
I0304 19:30:30.953017 23128000471168 run.py:483] Algo bellman_ford step 4924 current loss 0.063476, current_train_items 157600.
I0304 19:30:30.973181 23128000471168 run.py:483] Algo bellman_ford step 4925 current loss 0.004999, current_train_items 157632.
I0304 19:30:30.989758 23128000471168 run.py:483] Algo bellman_ford step 4926 current loss 0.019716, current_train_items 157664.
I0304 19:30:31.014506 23128000471168 run.py:483] Algo bellman_ford step 4927 current loss 0.036969, current_train_items 157696.
I0304 19:30:31.044863 23128000471168 run.py:483] Algo bellman_ford step 4928 current loss 0.065710, current_train_items 157728.
I0304 19:30:31.078690 23128000471168 run.py:483] Algo bellman_ford step 4929 current loss 0.049529, current_train_items 157760.
I0304 19:30:31.098424 23128000471168 run.py:483] Algo bellman_ford step 4930 current loss 0.002332, current_train_items 157792.
I0304 19:30:31.114567 23128000471168 run.py:483] Algo bellman_ford step 4931 current loss 0.005766, current_train_items 157824.
I0304 19:30:31.138467 23128000471168 run.py:483] Algo bellman_ford step 4932 current loss 0.030694, current_train_items 157856.
I0304 19:30:31.169328 23128000471168 run.py:483] Algo bellman_ford step 4933 current loss 0.051160, current_train_items 157888.
I0304 19:30:31.203056 23128000471168 run.py:483] Algo bellman_ford step 4934 current loss 0.073709, current_train_items 157920.
I0304 19:30:31.222768 23128000471168 run.py:483] Algo bellman_ford step 4935 current loss 0.013453, current_train_items 157952.
I0304 19:30:31.239170 23128000471168 run.py:483] Algo bellman_ford step 4936 current loss 0.017966, current_train_items 157984.
I0304 19:30:31.264581 23128000471168 run.py:483] Algo bellman_ford step 4937 current loss 0.051008, current_train_items 158016.
I0304 19:30:31.295729 23128000471168 run.py:483] Algo bellman_ford step 4938 current loss 0.061513, current_train_items 158048.
I0304 19:30:31.331375 23128000471168 run.py:483] Algo bellman_ford step 4939 current loss 0.096887, current_train_items 158080.
I0304 19:30:31.351327 23128000471168 run.py:483] Algo bellman_ford step 4940 current loss 0.001854, current_train_items 158112.
I0304 19:30:31.367372 23128000471168 run.py:483] Algo bellman_ford step 4941 current loss 0.054571, current_train_items 158144.
I0304 19:30:31.391256 23128000471168 run.py:483] Algo bellman_ford step 4942 current loss 0.051414, current_train_items 158176.
I0304 19:30:31.423646 23128000471168 run.py:483] Algo bellman_ford step 4943 current loss 0.081335, current_train_items 158208.
I0304 19:30:31.455646 23128000471168 run.py:483] Algo bellman_ford step 4944 current loss 0.054469, current_train_items 158240.
I0304 19:30:31.475441 23128000471168 run.py:483] Algo bellman_ford step 4945 current loss 0.002094, current_train_items 158272.
I0304 19:30:31.491878 23128000471168 run.py:483] Algo bellman_ford step 4946 current loss 0.027817, current_train_items 158304.
I0304 19:30:31.515863 23128000471168 run.py:483] Algo bellman_ford step 4947 current loss 0.051512, current_train_items 158336.
I0304 19:30:31.547446 23128000471168 run.py:483] Algo bellman_ford step 4948 current loss 0.071152, current_train_items 158368.
I0304 19:30:31.582996 23128000471168 run.py:483] Algo bellman_ford step 4949 current loss 0.048588, current_train_items 158400.
I0304 19:30:31.602851 23128000471168 run.py:483] Algo bellman_ford step 4950 current loss 0.002367, current_train_items 158432.
I0304 19:30:31.610806 23128000471168 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0304 19:30:31.610915 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:31.627700 23128000471168 run.py:483] Algo bellman_ford step 4951 current loss 0.005069, current_train_items 158464.
I0304 19:30:31.651800 23128000471168 run.py:483] Algo bellman_ford step 4952 current loss 0.024945, current_train_items 158496.
I0304 19:30:31.683977 23128000471168 run.py:483] Algo bellman_ford step 4953 current loss 0.058014, current_train_items 158528.
I0304 19:30:31.720076 23128000471168 run.py:483] Algo bellman_ford step 4954 current loss 0.043372, current_train_items 158560.
I0304 19:30:31.740535 23128000471168 run.py:483] Algo bellman_ford step 4955 current loss 0.007045, current_train_items 158592.
I0304 19:30:31.756948 23128000471168 run.py:483] Algo bellman_ford step 4956 current loss 0.008810, current_train_items 158624.
I0304 19:30:31.782441 23128000471168 run.py:483] Algo bellman_ford step 4957 current loss 0.039210, current_train_items 158656.
I0304 19:30:31.814615 23128000471168 run.py:483] Algo bellman_ford step 4958 current loss 0.061901, current_train_items 158688.
I0304 19:30:31.846162 23128000471168 run.py:483] Algo bellman_ford step 4959 current loss 0.054791, current_train_items 158720.
I0304 19:30:31.866225 23128000471168 run.py:483] Algo bellman_ford step 4960 current loss 0.006641, current_train_items 158752.
I0304 19:30:31.883000 23128000471168 run.py:483] Algo bellman_ford step 4961 current loss 0.008744, current_train_items 158784.
I0304 19:30:31.907102 23128000471168 run.py:483] Algo bellman_ford step 4962 current loss 0.029465, current_train_items 158816.
I0304 19:30:31.937307 23128000471168 run.py:483] Algo bellman_ford step 4963 current loss 0.035564, current_train_items 158848.
I0304 19:30:31.970883 23128000471168 run.py:483] Algo bellman_ford step 4964 current loss 0.073380, current_train_items 158880.
I0304 19:30:31.990598 23128000471168 run.py:483] Algo bellman_ford step 4965 current loss 0.001475, current_train_items 158912.
I0304 19:30:32.007286 23128000471168 run.py:483] Algo bellman_ford step 4966 current loss 0.026606, current_train_items 158944.
I0304 19:30:32.032237 23128000471168 run.py:483] Algo bellman_ford step 4967 current loss 0.049212, current_train_items 158976.
I0304 19:30:32.064528 23128000471168 run.py:483] Algo bellman_ford step 4968 current loss 0.055956, current_train_items 159008.
I0304 19:30:32.096451 23128000471168 run.py:483] Algo bellman_ford step 4969 current loss 0.062916, current_train_items 159040.
I0304 19:30:32.116717 23128000471168 run.py:483] Algo bellman_ford step 4970 current loss 0.002690, current_train_items 159072.
I0304 19:30:32.133131 23128000471168 run.py:483] Algo bellman_ford step 4971 current loss 0.041130, current_train_items 159104.
I0304 19:30:32.157400 23128000471168 run.py:483] Algo bellman_ford step 4972 current loss 0.070028, current_train_items 159136.
I0304 19:30:32.189682 23128000471168 run.py:483] Algo bellman_ford step 4973 current loss 0.053456, current_train_items 159168.
I0304 19:30:32.223028 23128000471168 run.py:483] Algo bellman_ford step 4974 current loss 0.062957, current_train_items 159200.
I0304 19:30:32.243113 23128000471168 run.py:483] Algo bellman_ford step 4975 current loss 0.002758, current_train_items 159232.
I0304 19:30:32.260184 23128000471168 run.py:483] Algo bellman_ford step 4976 current loss 0.021329, current_train_items 159264.
I0304 19:30:32.284736 23128000471168 run.py:483] Algo bellman_ford step 4977 current loss 0.112132, current_train_items 159296.
I0304 19:30:32.316788 23128000471168 run.py:483] Algo bellman_ford step 4978 current loss 0.211629, current_train_items 159328.
I0304 19:30:32.351573 23128000471168 run.py:483] Algo bellman_ford step 4979 current loss 0.076536, current_train_items 159360.
I0304 19:30:32.371504 23128000471168 run.py:483] Algo bellman_ford step 4980 current loss 0.008175, current_train_items 159392.
I0304 19:30:32.388108 23128000471168 run.py:483] Algo bellman_ford step 4981 current loss 0.006958, current_train_items 159424.
I0304 19:30:32.413349 23128000471168 run.py:483] Algo bellman_ford step 4982 current loss 0.035217, current_train_items 159456.
I0304 19:30:32.443083 23128000471168 run.py:483] Algo bellman_ford step 4983 current loss 0.074453, current_train_items 159488.
I0304 19:30:32.476425 23128000471168 run.py:483] Algo bellman_ford step 4984 current loss 0.104851, current_train_items 159520.
I0304 19:30:32.496703 23128000471168 run.py:483] Algo bellman_ford step 4985 current loss 0.004511, current_train_items 159552.
I0304 19:30:32.513428 23128000471168 run.py:483] Algo bellman_ford step 4986 current loss 0.038534, current_train_items 159584.
I0304 19:30:32.535903 23128000471168 run.py:483] Algo bellman_ford step 4987 current loss 0.085143, current_train_items 159616.
I0304 19:30:32.566508 23128000471168 run.py:483] Algo bellman_ford step 4988 current loss 0.056894, current_train_items 159648.
I0304 19:30:32.602581 23128000471168 run.py:483] Algo bellman_ford step 4989 current loss 0.087598, current_train_items 159680.
I0304 19:30:32.622635 23128000471168 run.py:483] Algo bellman_ford step 4990 current loss 0.002824, current_train_items 159712.
I0304 19:30:32.638936 23128000471168 run.py:483] Algo bellman_ford step 4991 current loss 0.025339, current_train_items 159744.
I0304 19:30:32.663235 23128000471168 run.py:483] Algo bellman_ford step 4992 current loss 0.083248, current_train_items 159776.
I0304 19:30:32.694560 23128000471168 run.py:483] Algo bellman_ford step 4993 current loss 0.065169, current_train_items 159808.
I0304 19:30:32.728851 23128000471168 run.py:483] Algo bellman_ford step 4994 current loss 0.072808, current_train_items 159840.
I0304 19:30:32.748456 23128000471168 run.py:483] Algo bellman_ford step 4995 current loss 0.003810, current_train_items 159872.
I0304 19:30:32.765154 23128000471168 run.py:483] Algo bellman_ford step 4996 current loss 0.025230, current_train_items 159904.
I0304 19:30:32.788941 23128000471168 run.py:483] Algo bellman_ford step 4997 current loss 0.046926, current_train_items 159936.
I0304 19:30:32.820741 23128000471168 run.py:483] Algo bellman_ford step 4998 current loss 0.056690, current_train_items 159968.
I0304 19:30:32.855173 23128000471168 run.py:483] Algo bellman_ford step 4999 current loss 0.086838, current_train_items 160000.
I0304 19:30:32.875345 23128000471168 run.py:483] Algo bellman_ford step 5000 current loss 0.005739, current_train_items 160032.
I0304 19:30:32.883194 23128000471168 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0304 19:30:32.883301 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:32.900645 23128000471168 run.py:483] Algo bellman_ford step 5001 current loss 0.049590, current_train_items 160064.
I0304 19:30:32.925215 23128000471168 run.py:483] Algo bellman_ford step 5002 current loss 0.074218, current_train_items 160096.
I0304 19:30:32.956119 23128000471168 run.py:483] Algo bellman_ford step 5003 current loss 0.019758, current_train_items 160128.
I0304 19:30:32.989597 23128000471168 run.py:483] Algo bellman_ford step 5004 current loss 0.087303, current_train_items 160160.
I0304 19:30:33.009831 23128000471168 run.py:483] Algo bellman_ford step 5005 current loss 0.003580, current_train_items 160192.
I0304 19:30:33.026108 23128000471168 run.py:483] Algo bellman_ford step 5006 current loss 0.033565, current_train_items 160224.
I0304 19:30:33.050803 23128000471168 run.py:483] Algo bellman_ford step 5007 current loss 0.061019, current_train_items 160256.
I0304 19:30:33.084277 23128000471168 run.py:483] Algo bellman_ford step 5008 current loss 0.072669, current_train_items 160288.
I0304 19:30:33.116481 23128000471168 run.py:483] Algo bellman_ford step 5009 current loss 0.043332, current_train_items 160320.
I0304 19:30:33.136182 23128000471168 run.py:483] Algo bellman_ford step 5010 current loss 0.009684, current_train_items 160352.
I0304 19:30:33.152294 23128000471168 run.py:483] Algo bellman_ford step 5011 current loss 0.009310, current_train_items 160384.
I0304 19:30:33.176473 23128000471168 run.py:483] Algo bellman_ford step 5012 current loss 0.027799, current_train_items 160416.
I0304 19:30:33.207329 23128000471168 run.py:483] Algo bellman_ford step 5013 current loss 0.052185, current_train_items 160448.
I0304 19:30:33.242500 23128000471168 run.py:483] Algo bellman_ford step 5014 current loss 0.056204, current_train_items 160480.
I0304 19:30:33.262791 23128000471168 run.py:483] Algo bellman_ford step 5015 current loss 0.027658, current_train_items 160512.
I0304 19:30:33.279865 23128000471168 run.py:483] Algo bellman_ford step 5016 current loss 0.028558, current_train_items 160544.
I0304 19:30:33.303937 23128000471168 run.py:483] Algo bellman_ford step 5017 current loss 0.049815, current_train_items 160576.
I0304 19:30:33.335715 23128000471168 run.py:483] Algo bellman_ford step 5018 current loss 0.044475, current_train_items 160608.
I0304 19:30:33.371225 23128000471168 run.py:483] Algo bellman_ford step 5019 current loss 0.228058, current_train_items 160640.
I0304 19:30:33.390941 23128000471168 run.py:483] Algo bellman_ford step 5020 current loss 0.005548, current_train_items 160672.
I0304 19:30:33.407201 23128000471168 run.py:483] Algo bellman_ford step 5021 current loss 0.017500, current_train_items 160704.
I0304 19:30:33.432368 23128000471168 run.py:483] Algo bellman_ford step 5022 current loss 0.042513, current_train_items 160736.
I0304 19:30:33.464089 23128000471168 run.py:483] Algo bellman_ford step 5023 current loss 0.075631, current_train_items 160768.
I0304 19:30:33.497967 23128000471168 run.py:483] Algo bellman_ford step 5024 current loss 0.059945, current_train_items 160800.
I0304 19:30:33.517731 23128000471168 run.py:483] Algo bellman_ford step 5025 current loss 0.001481, current_train_items 160832.
I0304 19:30:33.533791 23128000471168 run.py:483] Algo bellman_ford step 5026 current loss 0.013537, current_train_items 160864.
I0304 19:30:33.557010 23128000471168 run.py:483] Algo bellman_ford step 5027 current loss 0.069901, current_train_items 160896.
I0304 19:30:33.587258 23128000471168 run.py:483] Algo bellman_ford step 5028 current loss 0.064906, current_train_items 160928.
I0304 19:30:33.620582 23128000471168 run.py:483] Algo bellman_ford step 5029 current loss 0.076072, current_train_items 160960.
I0304 19:30:33.640649 23128000471168 run.py:483] Algo bellman_ford step 5030 current loss 0.002093, current_train_items 160992.
I0304 19:30:33.657088 23128000471168 run.py:483] Algo bellman_ford step 5031 current loss 0.026163, current_train_items 161024.
I0304 19:30:33.681800 23128000471168 run.py:483] Algo bellman_ford step 5032 current loss 0.058226, current_train_items 161056.
I0304 19:30:33.712018 23128000471168 run.py:483] Algo bellman_ford step 5033 current loss 0.041938, current_train_items 161088.
I0304 19:30:33.746665 23128000471168 run.py:483] Algo bellman_ford step 5034 current loss 0.091565, current_train_items 161120.
I0304 19:30:33.766574 23128000471168 run.py:483] Algo bellman_ford step 5035 current loss 0.005557, current_train_items 161152.
I0304 19:30:33.783481 23128000471168 run.py:483] Algo bellman_ford step 5036 current loss 0.009527, current_train_items 161184.
I0304 19:30:33.807713 23128000471168 run.py:483] Algo bellman_ford step 5037 current loss 0.070650, current_train_items 161216.
I0304 19:30:33.840349 23128000471168 run.py:483] Algo bellman_ford step 5038 current loss 0.089169, current_train_items 161248.
I0304 19:30:33.874187 23128000471168 run.py:483] Algo bellman_ford step 5039 current loss 0.050550, current_train_items 161280.
I0304 19:30:33.893989 23128000471168 run.py:483] Algo bellman_ford step 5040 current loss 0.002475, current_train_items 161312.
I0304 19:30:33.909903 23128000471168 run.py:483] Algo bellman_ford step 5041 current loss 0.006252, current_train_items 161344.
I0304 19:30:33.934987 23128000471168 run.py:483] Algo bellman_ford step 5042 current loss 0.071249, current_train_items 161376.
I0304 19:30:33.965267 23128000471168 run.py:483] Algo bellman_ford step 5043 current loss 0.047569, current_train_items 161408.
I0304 19:30:33.999067 23128000471168 run.py:483] Algo bellman_ford step 5044 current loss 0.081285, current_train_items 161440.
I0304 19:30:34.018901 23128000471168 run.py:483] Algo bellman_ford step 5045 current loss 0.004528, current_train_items 161472.
I0304 19:30:34.035307 23128000471168 run.py:483] Algo bellman_ford step 5046 current loss 0.052335, current_train_items 161504.
I0304 19:30:34.058325 23128000471168 run.py:483] Algo bellman_ford step 5047 current loss 0.023746, current_train_items 161536.
I0304 19:30:34.089405 23128000471168 run.py:483] Algo bellman_ford step 5048 current loss 0.083426, current_train_items 161568.
I0304 19:30:34.123488 23128000471168 run.py:483] Algo bellman_ford step 5049 current loss 0.080486, current_train_items 161600.
I0304 19:30:34.143198 23128000471168 run.py:483] Algo bellman_ford step 5050 current loss 0.001935, current_train_items 161632.
I0304 19:30:34.151361 23128000471168 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0304 19:30:34.151469 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:34.168423 23128000471168 run.py:483] Algo bellman_ford step 5051 current loss 0.011118, current_train_items 161664.
I0304 19:30:34.193853 23128000471168 run.py:483] Algo bellman_ford step 5052 current loss 0.112053, current_train_items 161696.
I0304 19:30:34.225923 23128000471168 run.py:483] Algo bellman_ford step 5053 current loss 0.113107, current_train_items 161728.
I0304 19:30:34.258794 23128000471168 run.py:483] Algo bellman_ford step 5054 current loss 0.105457, current_train_items 161760.
I0304 19:30:34.279771 23128000471168 run.py:483] Algo bellman_ford step 5055 current loss 0.012387, current_train_items 161792.
I0304 19:30:34.296879 23128000471168 run.py:483] Algo bellman_ford step 5056 current loss 0.022287, current_train_items 161824.
I0304 19:30:34.322329 23128000471168 run.py:483] Algo bellman_ford step 5057 current loss 0.040156, current_train_items 161856.
I0304 19:30:34.353239 23128000471168 run.py:483] Algo bellman_ford step 5058 current loss 0.068098, current_train_items 161888.
I0304 19:30:34.387536 23128000471168 run.py:483] Algo bellman_ford step 5059 current loss 0.057156, current_train_items 161920.
I0304 19:30:34.407722 23128000471168 run.py:483] Algo bellman_ford step 5060 current loss 0.052377, current_train_items 161952.
I0304 19:30:34.424287 23128000471168 run.py:483] Algo bellman_ford step 5061 current loss 0.027866, current_train_items 161984.
I0304 19:30:34.446709 23128000471168 run.py:483] Algo bellman_ford step 5062 current loss 0.028961, current_train_items 162016.
I0304 19:30:34.478128 23128000471168 run.py:483] Algo bellman_ford step 5063 current loss 0.070756, current_train_items 162048.
I0304 19:30:34.509788 23128000471168 run.py:483] Algo bellman_ford step 5064 current loss 0.092714, current_train_items 162080.
I0304 19:30:34.529607 23128000471168 run.py:483] Algo bellman_ford step 5065 current loss 0.015139, current_train_items 162112.
I0304 19:30:34.546234 23128000471168 run.py:483] Algo bellman_ford step 5066 current loss 0.023372, current_train_items 162144.
I0304 19:30:34.571468 23128000471168 run.py:483] Algo bellman_ford step 5067 current loss 0.065808, current_train_items 162176.
I0304 19:30:34.602632 23128000471168 run.py:483] Algo bellman_ford step 5068 current loss 0.078410, current_train_items 162208.
I0304 19:30:34.637485 23128000471168 run.py:483] Algo bellman_ford step 5069 current loss 0.118014, current_train_items 162240.
I0304 19:30:34.658011 23128000471168 run.py:483] Algo bellman_ford step 5070 current loss 0.002874, current_train_items 162272.
I0304 19:30:34.674959 23128000471168 run.py:483] Algo bellman_ford step 5071 current loss 0.018090, current_train_items 162304.
I0304 19:30:34.697577 23128000471168 run.py:483] Algo bellman_ford step 5072 current loss 0.050705, current_train_items 162336.
I0304 19:30:34.729744 23128000471168 run.py:483] Algo bellman_ford step 5073 current loss 0.089009, current_train_items 162368.
I0304 19:30:34.761191 23128000471168 run.py:483] Algo bellman_ford step 5074 current loss 0.044624, current_train_items 162400.
I0304 19:30:34.781458 23128000471168 run.py:483] Algo bellman_ford step 5075 current loss 0.003148, current_train_items 162432.
I0304 19:30:34.797903 23128000471168 run.py:483] Algo bellman_ford step 5076 current loss 0.011979, current_train_items 162464.
I0304 19:30:34.821377 23128000471168 run.py:483] Algo bellman_ford step 5077 current loss 0.023773, current_train_items 162496.
I0304 19:30:34.852741 23128000471168 run.py:483] Algo bellman_ford step 5078 current loss 0.090403, current_train_items 162528.
I0304 19:30:34.885746 23128000471168 run.py:483] Algo bellman_ford step 5079 current loss 0.081747, current_train_items 162560.
I0304 19:30:34.905591 23128000471168 run.py:483] Algo bellman_ford step 5080 current loss 0.004670, current_train_items 162592.
I0304 19:30:34.921650 23128000471168 run.py:483] Algo bellman_ford step 5081 current loss 0.013329, current_train_items 162624.
I0304 19:30:34.945532 23128000471168 run.py:483] Algo bellman_ford step 5082 current loss 0.023147, current_train_items 162656.
I0304 19:30:34.977272 23128000471168 run.py:483] Algo bellman_ford step 5083 current loss 0.049485, current_train_items 162688.
I0304 19:30:35.010905 23128000471168 run.py:483] Algo bellman_ford step 5084 current loss 0.047097, current_train_items 162720.
I0304 19:30:35.031383 23128000471168 run.py:483] Algo bellman_ford step 5085 current loss 0.002855, current_train_items 162752.
I0304 19:30:35.047883 23128000471168 run.py:483] Algo bellman_ford step 5086 current loss 0.023365, current_train_items 162784.
I0304 19:30:35.071989 23128000471168 run.py:483] Algo bellman_ford step 5087 current loss 0.040161, current_train_items 162816.
I0304 19:30:35.103453 23128000471168 run.py:483] Algo bellman_ford step 5088 current loss 0.040333, current_train_items 162848.
I0304 19:30:35.138188 23128000471168 run.py:483] Algo bellman_ford step 5089 current loss 0.068724, current_train_items 162880.
I0304 19:30:35.158167 23128000471168 run.py:483] Algo bellman_ford step 5090 current loss 0.002478, current_train_items 162912.
I0304 19:30:35.174949 23128000471168 run.py:483] Algo bellman_ford step 5091 current loss 0.028085, current_train_items 162944.
I0304 19:30:35.197435 23128000471168 run.py:483] Algo bellman_ford step 5092 current loss 0.021900, current_train_items 162976.
I0304 19:30:35.227895 23128000471168 run.py:483] Algo bellman_ford step 5093 current loss 0.034961, current_train_items 163008.
I0304 19:30:35.260665 23128000471168 run.py:483] Algo bellman_ford step 5094 current loss 0.052591, current_train_items 163040.
I0304 19:30:35.280681 23128000471168 run.py:483] Algo bellman_ford step 5095 current loss 0.001552, current_train_items 163072.
I0304 19:30:35.297201 23128000471168 run.py:483] Algo bellman_ford step 5096 current loss 0.008158, current_train_items 163104.
I0304 19:30:35.322197 23128000471168 run.py:483] Algo bellman_ford step 5097 current loss 0.043371, current_train_items 163136.
I0304 19:30:35.354652 23128000471168 run.py:483] Algo bellman_ford step 5098 current loss 0.053202, current_train_items 163168.
I0304 19:30:35.389612 23128000471168 run.py:483] Algo bellman_ford step 5099 current loss 0.053530, current_train_items 163200.
I0304 19:30:35.410047 23128000471168 run.py:483] Algo bellman_ford step 5100 current loss 0.002344, current_train_items 163232.
I0304 19:30:35.417717 23128000471168 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0304 19:30:35.417824 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:35.434804 23128000471168 run.py:483] Algo bellman_ford step 5101 current loss 0.039303, current_train_items 163264.
I0304 19:30:35.457800 23128000471168 run.py:483] Algo bellman_ford step 5102 current loss 0.011379, current_train_items 163296.
I0304 19:30:35.490551 23128000471168 run.py:483] Algo bellman_ford step 5103 current loss 0.030509, current_train_items 163328.
I0304 19:30:35.525539 23128000471168 run.py:483] Algo bellman_ford step 5104 current loss 0.066501, current_train_items 163360.
I0304 19:30:35.546088 23128000471168 run.py:483] Algo bellman_ford step 5105 current loss 0.001833, current_train_items 163392.
I0304 19:30:35.562627 23128000471168 run.py:483] Algo bellman_ford step 5106 current loss 0.025055, current_train_items 163424.
I0304 19:30:35.587079 23128000471168 run.py:483] Algo bellman_ford step 5107 current loss 0.048078, current_train_items 163456.
I0304 19:30:35.617375 23128000471168 run.py:483] Algo bellman_ford step 5108 current loss 0.024743, current_train_items 163488.
I0304 19:30:35.651933 23128000471168 run.py:483] Algo bellman_ford step 5109 current loss 0.057849, current_train_items 163520.
I0304 19:30:35.672210 23128000471168 run.py:483] Algo bellman_ford step 5110 current loss 0.001378, current_train_items 163552.
I0304 19:30:35.688959 23128000471168 run.py:483] Algo bellman_ford step 5111 current loss 0.020053, current_train_items 163584.
I0304 19:30:35.712735 23128000471168 run.py:483] Algo bellman_ford step 5112 current loss 0.017580, current_train_items 163616.
I0304 19:30:35.743686 23128000471168 run.py:483] Algo bellman_ford step 5113 current loss 0.040073, current_train_items 163648.
I0304 19:30:35.776913 23128000471168 run.py:483] Algo bellman_ford step 5114 current loss 0.043476, current_train_items 163680.
I0304 19:30:35.797262 23128000471168 run.py:483] Algo bellman_ford step 5115 current loss 0.001622, current_train_items 163712.
I0304 19:30:35.813902 23128000471168 run.py:483] Algo bellman_ford step 5116 current loss 0.011031, current_train_items 163744.
I0304 19:30:35.838926 23128000471168 run.py:483] Algo bellman_ford step 5117 current loss 0.064313, current_train_items 163776.
I0304 19:30:35.869843 23128000471168 run.py:483] Algo bellman_ford step 5118 current loss 0.034011, current_train_items 163808.
I0304 19:30:35.904405 23128000471168 run.py:483] Algo bellman_ford step 5119 current loss 0.048423, current_train_items 163840.
I0304 19:30:35.924674 23128000471168 run.py:483] Algo bellman_ford step 5120 current loss 0.002960, current_train_items 163872.
I0304 19:30:35.941091 23128000471168 run.py:483] Algo bellman_ford step 5121 current loss 0.011032, current_train_items 163904.
I0304 19:30:35.964900 23128000471168 run.py:483] Algo bellman_ford step 5122 current loss 0.054419, current_train_items 163936.
I0304 19:30:35.995445 23128000471168 run.py:483] Algo bellman_ford step 5123 current loss 0.022506, current_train_items 163968.
I0304 19:30:36.029515 23128000471168 run.py:483] Algo bellman_ford step 5124 current loss 0.080039, current_train_items 164000.
I0304 19:30:36.049769 23128000471168 run.py:483] Algo bellman_ford step 5125 current loss 0.001333, current_train_items 164032.
I0304 19:30:36.066128 23128000471168 run.py:483] Algo bellman_ford step 5126 current loss 0.027107, current_train_items 164064.
I0304 19:30:36.090491 23128000471168 run.py:483] Algo bellman_ford step 5127 current loss 0.054156, current_train_items 164096.
I0304 19:30:36.122216 23128000471168 run.py:483] Algo bellman_ford step 5128 current loss 0.034601, current_train_items 164128.
I0304 19:30:36.157535 23128000471168 run.py:483] Algo bellman_ford step 5129 current loss 0.047031, current_train_items 164160.
I0304 19:30:36.177223 23128000471168 run.py:483] Algo bellman_ford step 5130 current loss 0.001339, current_train_items 164192.
I0304 19:30:36.193890 23128000471168 run.py:483] Algo bellman_ford step 5131 current loss 0.021762, current_train_items 164224.
I0304 19:30:36.217159 23128000471168 run.py:483] Algo bellman_ford step 5132 current loss 0.058283, current_train_items 164256.
I0304 19:30:36.249691 23128000471168 run.py:483] Algo bellman_ford step 5133 current loss 0.053181, current_train_items 164288.
I0304 19:30:36.282911 23128000471168 run.py:483] Algo bellman_ford step 5134 current loss 0.042019, current_train_items 164320.
I0304 19:30:36.303424 23128000471168 run.py:483] Algo bellman_ford step 5135 current loss 0.004085, current_train_items 164352.
I0304 19:30:36.319751 23128000471168 run.py:483] Algo bellman_ford step 5136 current loss 0.011430, current_train_items 164384.
I0304 19:30:36.344175 23128000471168 run.py:483] Algo bellman_ford step 5137 current loss 0.035040, current_train_items 164416.
I0304 19:30:36.375762 23128000471168 run.py:483] Algo bellman_ford step 5138 current loss 0.046351, current_train_items 164448.
I0304 19:30:36.410111 23128000471168 run.py:483] Algo bellman_ford step 5139 current loss 0.033058, current_train_items 164480.
I0304 19:30:36.430352 23128000471168 run.py:483] Algo bellman_ford step 5140 current loss 0.001805, current_train_items 164512.
I0304 19:30:36.447104 23128000471168 run.py:483] Algo bellman_ford step 5141 current loss 0.010907, current_train_items 164544.
I0304 19:30:36.471980 23128000471168 run.py:483] Algo bellman_ford step 5142 current loss 0.032698, current_train_items 164576.
I0304 19:30:36.503770 23128000471168 run.py:483] Algo bellman_ford step 5143 current loss 0.045278, current_train_items 164608.
I0304 19:30:36.537690 23128000471168 run.py:483] Algo bellman_ford step 5144 current loss 0.057682, current_train_items 164640.
I0304 19:30:36.557896 23128000471168 run.py:483] Algo bellman_ford step 5145 current loss 0.002854, current_train_items 164672.
I0304 19:30:36.574180 23128000471168 run.py:483] Algo bellman_ford step 5146 current loss 0.012802, current_train_items 164704.
I0304 19:30:36.597422 23128000471168 run.py:483] Algo bellman_ford step 5147 current loss 0.011248, current_train_items 164736.
I0304 19:30:36.628265 23128000471168 run.py:483] Algo bellman_ford step 5148 current loss 0.055492, current_train_items 164768.
I0304 19:30:36.661387 23128000471168 run.py:483] Algo bellman_ford step 5149 current loss 0.066445, current_train_items 164800.
I0304 19:30:36.681259 23128000471168 run.py:483] Algo bellman_ford step 5150 current loss 0.003168, current_train_items 164832.
I0304 19:30:36.689600 23128000471168 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0304 19:30:36.689709 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:36.706837 23128000471168 run.py:483] Algo bellman_ford step 5151 current loss 0.017650, current_train_items 164864.
I0304 19:30:36.731014 23128000471168 run.py:483] Algo bellman_ford step 5152 current loss 0.018875, current_train_items 164896.
I0304 19:30:36.761620 23128000471168 run.py:483] Algo bellman_ford step 5153 current loss 0.065005, current_train_items 164928.
I0304 19:30:36.794983 23128000471168 run.py:483] Algo bellman_ford step 5154 current loss 0.101955, current_train_items 164960.
I0304 19:30:36.815251 23128000471168 run.py:483] Algo bellman_ford step 5155 current loss 0.006321, current_train_items 164992.
I0304 19:30:36.831463 23128000471168 run.py:483] Algo bellman_ford step 5156 current loss 0.004175, current_train_items 165024.
I0304 19:30:36.856277 23128000471168 run.py:483] Algo bellman_ford step 5157 current loss 0.037569, current_train_items 165056.
I0304 19:30:36.889226 23128000471168 run.py:483] Algo bellman_ford step 5158 current loss 0.071618, current_train_items 165088.
I0304 19:30:36.922192 23128000471168 run.py:483] Algo bellman_ford step 5159 current loss 0.045727, current_train_items 165120.
I0304 19:30:36.942474 23128000471168 run.py:483] Algo bellman_ford step 5160 current loss 0.002128, current_train_items 165152.
I0304 19:30:36.958906 23128000471168 run.py:483] Algo bellman_ford step 5161 current loss 0.012643, current_train_items 165184.
I0304 19:30:36.983648 23128000471168 run.py:483] Algo bellman_ford step 5162 current loss 0.113797, current_train_items 165216.
I0304 19:30:37.016328 23128000471168 run.py:483] Algo bellman_ford step 5163 current loss 0.080527, current_train_items 165248.
I0304 19:30:37.048959 23128000471168 run.py:483] Algo bellman_ford step 5164 current loss 0.068915, current_train_items 165280.
I0304 19:30:37.068654 23128000471168 run.py:483] Algo bellman_ford step 5165 current loss 0.003166, current_train_items 165312.
I0304 19:30:37.084968 23128000471168 run.py:483] Algo bellman_ford step 5166 current loss 0.009949, current_train_items 165344.
I0304 19:30:37.108022 23128000471168 run.py:483] Algo bellman_ford step 5167 current loss 0.067824, current_train_items 165376.
I0304 19:30:37.139664 23128000471168 run.py:483] Algo bellman_ford step 5168 current loss 0.054310, current_train_items 165408.
I0304 19:30:37.174640 23128000471168 run.py:483] Algo bellman_ford step 5169 current loss 0.098063, current_train_items 165440.
I0304 19:30:37.194900 23128000471168 run.py:483] Algo bellman_ford step 5170 current loss 0.003426, current_train_items 165472.
I0304 19:30:37.211497 23128000471168 run.py:483] Algo bellman_ford step 5171 current loss 0.025571, current_train_items 165504.
I0304 19:30:37.235473 23128000471168 run.py:483] Algo bellman_ford step 5172 current loss 0.053370, current_train_items 165536.
I0304 19:30:37.267864 23128000471168 run.py:483] Algo bellman_ford step 5173 current loss 0.062974, current_train_items 165568.
I0304 19:30:37.301081 23128000471168 run.py:483] Algo bellman_ford step 5174 current loss 0.054858, current_train_items 165600.
I0304 19:30:37.320995 23128000471168 run.py:483] Algo bellman_ford step 5175 current loss 0.004525, current_train_items 165632.
I0304 19:30:37.337656 23128000471168 run.py:483] Algo bellman_ford step 5176 current loss 0.015514, current_train_items 165664.
I0304 19:30:37.361735 23128000471168 run.py:483] Algo bellman_ford step 5177 current loss 0.050031, current_train_items 165696.
I0304 19:30:37.392875 23128000471168 run.py:483] Algo bellman_ford step 5178 current loss 0.062915, current_train_items 165728.
I0304 19:30:37.425258 23128000471168 run.py:483] Algo bellman_ford step 5179 current loss 0.040398, current_train_items 165760.
I0304 19:30:37.445119 23128000471168 run.py:483] Algo bellman_ford step 5180 current loss 0.001433, current_train_items 165792.
I0304 19:30:37.461502 23128000471168 run.py:483] Algo bellman_ford step 5181 current loss 0.010245, current_train_items 165824.
I0304 19:30:37.485432 23128000471168 run.py:483] Algo bellman_ford step 5182 current loss 0.029325, current_train_items 165856.
I0304 19:30:37.516739 23128000471168 run.py:483] Algo bellman_ford step 5183 current loss 0.057924, current_train_items 165888.
I0304 19:30:37.549273 23128000471168 run.py:483] Algo bellman_ford step 5184 current loss 0.040576, current_train_items 165920.
I0304 19:30:37.569818 23128000471168 run.py:483] Algo bellman_ford step 5185 current loss 0.019445, current_train_items 165952.
I0304 19:30:37.586724 23128000471168 run.py:483] Algo bellman_ford step 5186 current loss 0.033118, current_train_items 165984.
I0304 19:30:37.609440 23128000471168 run.py:483] Algo bellman_ford step 5187 current loss 0.078494, current_train_items 166016.
I0304 19:30:37.641481 23128000471168 run.py:483] Algo bellman_ford step 5188 current loss 0.116864, current_train_items 166048.
I0304 19:30:37.674313 23128000471168 run.py:483] Algo bellman_ford step 5189 current loss 0.078792, current_train_items 166080.
I0304 19:30:37.694455 23128000471168 run.py:483] Algo bellman_ford step 5190 current loss 0.002762, current_train_items 166112.
I0304 19:30:37.710937 23128000471168 run.py:483] Algo bellman_ford step 5191 current loss 0.023461, current_train_items 166144.
I0304 19:30:37.735136 23128000471168 run.py:483] Algo bellman_ford step 5192 current loss 0.106562, current_train_items 166176.
I0304 19:30:37.765691 23128000471168 run.py:483] Algo bellman_ford step 5193 current loss 0.064701, current_train_items 166208.
I0304 19:30:37.799118 23128000471168 run.py:483] Algo bellman_ford step 5194 current loss 0.082752, current_train_items 166240.
I0304 19:30:37.819065 23128000471168 run.py:483] Algo bellman_ford step 5195 current loss 0.001688, current_train_items 166272.
I0304 19:30:37.835613 23128000471168 run.py:483] Algo bellman_ford step 5196 current loss 0.013126, current_train_items 166304.
I0304 19:30:37.859922 23128000471168 run.py:483] Algo bellman_ford step 5197 current loss 0.048441, current_train_items 166336.
I0304 19:30:37.892339 23128000471168 run.py:483] Algo bellman_ford step 5198 current loss 0.077222, current_train_items 166368.
I0304 19:30:37.925313 23128000471168 run.py:483] Algo bellman_ford step 5199 current loss 0.075661, current_train_items 166400.
I0304 19:30:37.945394 23128000471168 run.py:483] Algo bellman_ford step 5200 current loss 0.002912, current_train_items 166432.
I0304 19:30:37.953405 23128000471168 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0304 19:30:37.953514 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:37.970777 23128000471168 run.py:483] Algo bellman_ford step 5201 current loss 0.013260, current_train_items 166464.
I0304 19:30:37.996076 23128000471168 run.py:483] Algo bellman_ford step 5202 current loss 0.028399, current_train_items 166496.
I0304 19:30:38.029240 23128000471168 run.py:483] Algo bellman_ford step 5203 current loss 0.069639, current_train_items 166528.
I0304 19:30:38.064509 23128000471168 run.py:483] Algo bellman_ford step 5204 current loss 0.058907, current_train_items 166560.
I0304 19:30:38.084708 23128000471168 run.py:483] Algo bellman_ford step 5205 current loss 0.002418, current_train_items 166592.
I0304 19:30:38.100670 23128000471168 run.py:483] Algo bellman_ford step 5206 current loss 0.016971, current_train_items 166624.
I0304 19:30:38.124769 23128000471168 run.py:483] Algo bellman_ford step 5207 current loss 0.039681, current_train_items 166656.
I0304 19:30:38.154995 23128000471168 run.py:483] Algo bellman_ford step 5208 current loss 0.066513, current_train_items 166688.
I0304 19:30:38.191281 23128000471168 run.py:483] Algo bellman_ford step 5209 current loss 0.074296, current_train_items 166720.
I0304 19:30:38.211209 23128000471168 run.py:483] Algo bellman_ford step 5210 current loss 0.004770, current_train_items 166752.
I0304 19:30:38.227227 23128000471168 run.py:483] Algo bellman_ford step 5211 current loss 0.012848, current_train_items 166784.
I0304 19:30:38.252037 23128000471168 run.py:483] Algo bellman_ford step 5212 current loss 0.046264, current_train_items 166816.
I0304 19:30:38.285282 23128000471168 run.py:483] Algo bellman_ford step 5213 current loss 0.074687, current_train_items 166848.
I0304 19:30:38.318526 23128000471168 run.py:483] Algo bellman_ford step 5214 current loss 0.049779, current_train_items 166880.
I0304 19:30:38.338076 23128000471168 run.py:483] Algo bellman_ford step 5215 current loss 0.001738, current_train_items 166912.
I0304 19:30:38.354798 23128000471168 run.py:483] Algo bellman_ford step 5216 current loss 0.022597, current_train_items 166944.
I0304 19:30:38.378934 23128000471168 run.py:483] Algo bellman_ford step 5217 current loss 0.024009, current_train_items 166976.
I0304 19:30:38.409704 23128000471168 run.py:483] Algo bellman_ford step 5218 current loss 0.049296, current_train_items 167008.
I0304 19:30:38.443251 23128000471168 run.py:483] Algo bellman_ford step 5219 current loss 0.062097, current_train_items 167040.
I0304 19:30:38.463085 23128000471168 run.py:483] Algo bellman_ford step 5220 current loss 0.004403, current_train_items 167072.
I0304 19:30:38.479312 23128000471168 run.py:483] Algo bellman_ford step 5221 current loss 0.013178, current_train_items 167104.
I0304 19:30:38.502245 23128000471168 run.py:483] Algo bellman_ford step 5222 current loss 0.048613, current_train_items 167136.
I0304 19:30:38.534551 23128000471168 run.py:483] Algo bellman_ford step 5223 current loss 0.137012, current_train_items 167168.
I0304 19:30:38.567820 23128000471168 run.py:483] Algo bellman_ford step 5224 current loss 0.080007, current_train_items 167200.
I0304 19:30:38.587661 23128000471168 run.py:483] Algo bellman_ford step 5225 current loss 0.004600, current_train_items 167232.
I0304 19:30:38.603897 23128000471168 run.py:483] Algo bellman_ford step 5226 current loss 0.019425, current_train_items 167264.
I0304 19:30:38.628643 23128000471168 run.py:483] Algo bellman_ford step 5227 current loss 0.043018, current_train_items 167296.
I0304 19:30:38.661212 23128000471168 run.py:483] Algo bellman_ford step 5228 current loss 0.091379, current_train_items 167328.
I0304 19:30:38.696236 23128000471168 run.py:483] Algo bellman_ford step 5229 current loss 0.080948, current_train_items 167360.
I0304 19:30:38.715993 23128000471168 run.py:483] Algo bellman_ford step 5230 current loss 0.001951, current_train_items 167392.
I0304 19:30:38.732566 23128000471168 run.py:483] Algo bellman_ford step 5231 current loss 0.018812, current_train_items 167424.
I0304 19:30:38.757027 23128000471168 run.py:483] Algo bellman_ford step 5232 current loss 0.030138, current_train_items 167456.
I0304 19:30:38.788031 23128000471168 run.py:483] Algo bellman_ford step 5233 current loss 0.037907, current_train_items 167488.
I0304 19:30:38.823644 23128000471168 run.py:483] Algo bellman_ford step 5234 current loss 0.045357, current_train_items 167520.
I0304 19:30:38.843920 23128000471168 run.py:483] Algo bellman_ford step 5235 current loss 0.001836, current_train_items 167552.
I0304 19:30:38.860092 23128000471168 run.py:483] Algo bellman_ford step 5236 current loss 0.006365, current_train_items 167584.
I0304 19:30:38.884117 23128000471168 run.py:483] Algo bellman_ford step 5237 current loss 0.023407, current_train_items 167616.
I0304 19:30:38.917275 23128000471168 run.py:483] Algo bellman_ford step 5238 current loss 0.054191, current_train_items 167648.
I0304 19:30:38.951981 23128000471168 run.py:483] Algo bellman_ford step 5239 current loss 0.060946, current_train_items 167680.
I0304 19:30:38.972001 23128000471168 run.py:483] Algo bellman_ford step 5240 current loss 0.008897, current_train_items 167712.
I0304 19:30:38.988440 23128000471168 run.py:483] Algo bellman_ford step 5241 current loss 0.024150, current_train_items 167744.
I0304 19:30:39.012828 23128000471168 run.py:483] Algo bellman_ford step 5242 current loss 0.051004, current_train_items 167776.
I0304 19:30:39.044488 23128000471168 run.py:483] Algo bellman_ford step 5243 current loss 0.036971, current_train_items 167808.
I0304 19:30:39.076016 23128000471168 run.py:483] Algo bellman_ford step 5244 current loss 0.026687, current_train_items 167840.
I0304 19:30:39.095704 23128000471168 run.py:483] Algo bellman_ford step 5245 current loss 0.008561, current_train_items 167872.
I0304 19:30:39.112181 23128000471168 run.py:483] Algo bellman_ford step 5246 current loss 0.012559, current_train_items 167904.
I0304 19:30:39.137162 23128000471168 run.py:483] Algo bellman_ford step 5247 current loss 0.035156, current_train_items 167936.
I0304 19:30:39.169264 23128000471168 run.py:483] Algo bellman_ford step 5248 current loss 0.047841, current_train_items 167968.
I0304 19:30:39.203352 23128000471168 run.py:483] Algo bellman_ford step 5249 current loss 0.086072, current_train_items 168000.
I0304 19:30:39.223464 23128000471168 run.py:483] Algo bellman_ford step 5250 current loss 0.001652, current_train_items 168032.
I0304 19:30:39.231428 23128000471168 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0304 19:30:39.231538 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:39.248572 23128000471168 run.py:483] Algo bellman_ford step 5251 current loss 0.019917, current_train_items 168064.
I0304 19:30:39.273690 23128000471168 run.py:483] Algo bellman_ford step 5252 current loss 0.082569, current_train_items 168096.
I0304 19:30:39.306228 23128000471168 run.py:483] Algo bellman_ford step 5253 current loss 0.033676, current_train_items 168128.
I0304 19:30:39.338292 23128000471168 run.py:483] Algo bellman_ford step 5254 current loss 0.039231, current_train_items 168160.
I0304 19:30:39.358609 23128000471168 run.py:483] Algo bellman_ford step 5255 current loss 0.019162, current_train_items 168192.
I0304 19:30:39.374876 23128000471168 run.py:483] Algo bellman_ford step 5256 current loss 0.004483, current_train_items 168224.
I0304 19:30:39.398362 23128000471168 run.py:483] Algo bellman_ford step 5257 current loss 0.058072, current_train_items 168256.
I0304 19:30:39.430545 23128000471168 run.py:483] Algo bellman_ford step 5258 current loss 0.052055, current_train_items 168288.
I0304 19:30:39.462725 23128000471168 run.py:483] Algo bellman_ford step 5259 current loss 0.039885, current_train_items 168320.
I0304 19:30:39.483212 23128000471168 run.py:483] Algo bellman_ford step 5260 current loss 0.002277, current_train_items 168352.
I0304 19:30:39.500020 23128000471168 run.py:483] Algo bellman_ford step 5261 current loss 0.018114, current_train_items 168384.
I0304 19:30:39.523949 23128000471168 run.py:483] Algo bellman_ford step 5262 current loss 0.042708, current_train_items 168416.
I0304 19:30:39.556870 23128000471168 run.py:483] Algo bellman_ford step 5263 current loss 0.041271, current_train_items 168448.
I0304 19:30:39.589577 23128000471168 run.py:483] Algo bellman_ford step 5264 current loss 0.044122, current_train_items 168480.
I0304 19:30:39.609814 23128000471168 run.py:483] Algo bellman_ford step 5265 current loss 0.004630, current_train_items 168512.
I0304 19:30:39.626331 23128000471168 run.py:483] Algo bellman_ford step 5266 current loss 0.023496, current_train_items 168544.
I0304 19:30:39.651663 23128000471168 run.py:483] Algo bellman_ford step 5267 current loss 0.044325, current_train_items 168576.
I0304 19:30:39.682759 23128000471168 run.py:483] Algo bellman_ford step 5268 current loss 0.040269, current_train_items 168608.
I0304 19:30:39.715521 23128000471168 run.py:483] Algo bellman_ford step 5269 current loss 0.063268, current_train_items 168640.
I0304 19:30:39.735926 23128000471168 run.py:483] Algo bellman_ford step 5270 current loss 0.001818, current_train_items 168672.
I0304 19:30:39.752292 23128000471168 run.py:483] Algo bellman_ford step 5271 current loss 0.021288, current_train_items 168704.
I0304 19:30:39.775442 23128000471168 run.py:483] Algo bellman_ford step 5272 current loss 0.027650, current_train_items 168736.
I0304 19:30:39.807098 23128000471168 run.py:483] Algo bellman_ford step 5273 current loss 0.031878, current_train_items 168768.
I0304 19:30:39.840351 23128000471168 run.py:483] Algo bellman_ford step 5274 current loss 0.042121, current_train_items 168800.
I0304 19:30:39.860553 23128000471168 run.py:483] Algo bellman_ford step 5275 current loss 0.002967, current_train_items 168832.
I0304 19:30:39.877045 23128000471168 run.py:483] Algo bellman_ford step 5276 current loss 0.008472, current_train_items 168864.
I0304 19:30:39.900729 23128000471168 run.py:483] Algo bellman_ford step 5277 current loss 0.038515, current_train_items 168896.
I0304 19:30:39.932746 23128000471168 run.py:483] Algo bellman_ford step 5278 current loss 0.048086, current_train_items 168928.
I0304 19:30:39.967216 23128000471168 run.py:483] Algo bellman_ford step 5279 current loss 0.052976, current_train_items 168960.
I0304 19:30:39.987247 23128000471168 run.py:483] Algo bellman_ford step 5280 current loss 0.001989, current_train_items 168992.
I0304 19:30:40.003893 23128000471168 run.py:483] Algo bellman_ford step 5281 current loss 0.036337, current_train_items 169024.
I0304 19:30:40.029381 23128000471168 run.py:483] Algo bellman_ford step 5282 current loss 0.047528, current_train_items 169056.
I0304 19:30:40.061278 23128000471168 run.py:483] Algo bellman_ford step 5283 current loss 0.059186, current_train_items 169088.
I0304 19:30:40.095082 23128000471168 run.py:483] Algo bellman_ford step 5284 current loss 0.061718, current_train_items 169120.
I0304 19:30:40.115444 23128000471168 run.py:483] Algo bellman_ford step 5285 current loss 0.004114, current_train_items 169152.
I0304 19:30:40.132188 23128000471168 run.py:483] Algo bellman_ford step 5286 current loss 0.009829, current_train_items 169184.
I0304 19:30:40.157528 23128000471168 run.py:483] Algo bellman_ford step 5287 current loss 0.061047, current_train_items 169216.
I0304 19:30:40.188989 23128000471168 run.py:483] Algo bellman_ford step 5288 current loss 0.075264, current_train_items 169248.
I0304 19:30:40.223750 23128000471168 run.py:483] Algo bellman_ford step 5289 current loss 0.063456, current_train_items 169280.
I0304 19:30:40.244174 23128000471168 run.py:483] Algo bellman_ford step 5290 current loss 0.003740, current_train_items 169312.
I0304 19:30:40.261315 23128000471168 run.py:483] Algo bellman_ford step 5291 current loss 0.008636, current_train_items 169344.
I0304 19:30:40.285015 23128000471168 run.py:483] Algo bellman_ford step 5292 current loss 0.054451, current_train_items 169376.
I0304 19:30:40.316761 23128000471168 run.py:483] Algo bellman_ford step 5293 current loss 0.048300, current_train_items 169408.
I0304 19:30:40.350260 23128000471168 run.py:483] Algo bellman_ford step 5294 current loss 0.043081, current_train_items 169440.
I0304 19:30:40.370138 23128000471168 run.py:483] Algo bellman_ford step 5295 current loss 0.002998, current_train_items 169472.
I0304 19:30:40.386574 23128000471168 run.py:483] Algo bellman_ford step 5296 current loss 0.012068, current_train_items 169504.
I0304 19:30:40.410374 23128000471168 run.py:483] Algo bellman_ford step 5297 current loss 0.038931, current_train_items 169536.
I0304 19:30:40.443083 23128000471168 run.py:483] Algo bellman_ford step 5298 current loss 0.051973, current_train_items 169568.
I0304 19:30:40.479454 23128000471168 run.py:483] Algo bellman_ford step 5299 current loss 0.068379, current_train_items 169600.
I0304 19:30:40.499867 23128000471168 run.py:483] Algo bellman_ford step 5300 current loss 0.015013, current_train_items 169632.
I0304 19:30:40.507562 23128000471168 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0304 19:30:40.507672 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:40.524437 23128000471168 run.py:483] Algo bellman_ford step 5301 current loss 0.014103, current_train_items 169664.
I0304 19:30:40.549647 23128000471168 run.py:483] Algo bellman_ford step 5302 current loss 0.116814, current_train_items 169696.
I0304 19:30:40.582578 23128000471168 run.py:483] Algo bellman_ford step 5303 current loss 0.090438, current_train_items 169728.
I0304 19:30:40.617426 23128000471168 run.py:483] Algo bellman_ford step 5304 current loss 0.059555, current_train_items 169760.
I0304 19:30:40.637656 23128000471168 run.py:483] Algo bellman_ford step 5305 current loss 0.004866, current_train_items 169792.
I0304 19:30:40.654383 23128000471168 run.py:483] Algo bellman_ford step 5306 current loss 0.020753, current_train_items 169824.
I0304 19:30:40.679470 23128000471168 run.py:483] Algo bellman_ford step 5307 current loss 0.051662, current_train_items 169856.
I0304 19:30:40.711932 23128000471168 run.py:483] Algo bellman_ford step 5308 current loss 0.062312, current_train_items 169888.
I0304 19:30:40.744207 23128000471168 run.py:483] Algo bellman_ford step 5309 current loss 0.062203, current_train_items 169920.
I0304 19:30:40.764412 23128000471168 run.py:483] Algo bellman_ford step 5310 current loss 0.002323, current_train_items 169952.
I0304 19:30:40.780450 23128000471168 run.py:483] Algo bellman_ford step 5311 current loss 0.004812, current_train_items 169984.
I0304 19:30:40.804500 23128000471168 run.py:483] Algo bellman_ford step 5312 current loss 0.038141, current_train_items 170016.
I0304 19:30:40.836177 23128000471168 run.py:483] Algo bellman_ford step 5313 current loss 0.103695, current_train_items 170048.
I0304 19:30:40.871067 23128000471168 run.py:483] Algo bellman_ford step 5314 current loss 0.120691, current_train_items 170080.
I0304 19:30:40.891074 23128000471168 run.py:483] Algo bellman_ford step 5315 current loss 0.006428, current_train_items 170112.
I0304 19:30:40.907624 23128000471168 run.py:483] Algo bellman_ford step 5316 current loss 0.023013, current_train_items 170144.
I0304 19:30:40.931949 23128000471168 run.py:483] Algo bellman_ford step 5317 current loss 0.063772, current_train_items 170176.
I0304 19:30:40.962085 23128000471168 run.py:483] Algo bellman_ford step 5318 current loss 0.046506, current_train_items 170208.
I0304 19:30:40.996904 23128000471168 run.py:483] Algo bellman_ford step 5319 current loss 0.086877, current_train_items 170240.
I0304 19:30:41.016672 23128000471168 run.py:483] Algo bellman_ford step 5320 current loss 0.008845, current_train_items 170272.
I0304 19:30:41.033025 23128000471168 run.py:483] Algo bellman_ford step 5321 current loss 0.048026, current_train_items 170304.
I0304 19:30:41.057862 23128000471168 run.py:483] Algo bellman_ford step 5322 current loss 0.096137, current_train_items 170336.
I0304 19:30:41.088180 23128000471168 run.py:483] Algo bellman_ford step 5323 current loss 0.071571, current_train_items 170368.
I0304 19:30:41.123316 23128000471168 run.py:483] Algo bellman_ford step 5324 current loss 0.066220, current_train_items 170400.
I0304 19:30:41.143265 23128000471168 run.py:483] Algo bellman_ford step 5325 current loss 0.001875, current_train_items 170432.
I0304 19:30:41.159523 23128000471168 run.py:483] Algo bellman_ford step 5326 current loss 0.035343, current_train_items 170464.
I0304 19:30:41.182655 23128000471168 run.py:483] Algo bellman_ford step 5327 current loss 0.057846, current_train_items 170496.
I0304 19:30:41.215427 23128000471168 run.py:483] Algo bellman_ford step 5328 current loss 0.042015, current_train_items 170528.
I0304 19:30:41.248637 23128000471168 run.py:483] Algo bellman_ford step 5329 current loss 0.129221, current_train_items 170560.
I0304 19:30:41.268488 23128000471168 run.py:483] Algo bellman_ford step 5330 current loss 0.036027, current_train_items 170592.
I0304 19:30:41.284932 23128000471168 run.py:483] Algo bellman_ford step 5331 current loss 0.019849, current_train_items 170624.
I0304 19:30:41.309274 23128000471168 run.py:483] Algo bellman_ford step 5332 current loss 0.033478, current_train_items 170656.
I0304 19:30:41.339430 23128000471168 run.py:483] Algo bellman_ford step 5333 current loss 0.084043, current_train_items 170688.
I0304 19:30:41.372146 23128000471168 run.py:483] Algo bellman_ford step 5334 current loss 0.074872, current_train_items 170720.
I0304 19:30:41.392306 23128000471168 run.py:483] Algo bellman_ford step 5335 current loss 0.008708, current_train_items 170752.
I0304 19:30:41.409009 23128000471168 run.py:483] Algo bellman_ford step 5336 current loss 0.027120, current_train_items 170784.
I0304 19:30:41.432739 23128000471168 run.py:483] Algo bellman_ford step 5337 current loss 0.025022, current_train_items 170816.
I0304 19:30:41.463313 23128000471168 run.py:483] Algo bellman_ford step 5338 current loss 0.066831, current_train_items 170848.
I0304 19:30:41.497892 23128000471168 run.py:483] Algo bellman_ford step 5339 current loss 0.071647, current_train_items 170880.
I0304 19:30:41.517636 23128000471168 run.py:483] Algo bellman_ford step 5340 current loss 0.001853, current_train_items 170912.
I0304 19:30:41.534369 23128000471168 run.py:483] Algo bellman_ford step 5341 current loss 0.031989, current_train_items 170944.
I0304 19:30:41.560058 23128000471168 run.py:483] Algo bellman_ford step 5342 current loss 0.036013, current_train_items 170976.
I0304 19:30:41.591795 23128000471168 run.py:483] Algo bellman_ford step 5343 current loss 0.080447, current_train_items 171008.
I0304 19:30:41.625329 23128000471168 run.py:483] Algo bellman_ford step 5344 current loss 0.060486, current_train_items 171040.
I0304 19:30:41.645011 23128000471168 run.py:483] Algo bellman_ford step 5345 current loss 0.001415, current_train_items 171072.
I0304 19:30:41.660854 23128000471168 run.py:483] Algo bellman_ford step 5346 current loss 0.015410, current_train_items 171104.
I0304 19:30:41.684834 23128000471168 run.py:483] Algo bellman_ford step 5347 current loss 0.053416, current_train_items 171136.
I0304 19:30:41.717272 23128000471168 run.py:483] Algo bellman_ford step 5348 current loss 0.066474, current_train_items 171168.
I0304 19:30:41.751190 23128000471168 run.py:483] Algo bellman_ford step 5349 current loss 0.056057, current_train_items 171200.
I0304 19:30:41.771243 23128000471168 run.py:483] Algo bellman_ford step 5350 current loss 0.002279, current_train_items 171232.
I0304 19:30:41.779341 23128000471168 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0304 19:30:41.779448 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:41.796091 23128000471168 run.py:483] Algo bellman_ford step 5351 current loss 0.006207, current_train_items 171264.
I0304 19:30:41.821077 23128000471168 run.py:483] Algo bellman_ford step 5352 current loss 0.049963, current_train_items 171296.
I0304 19:30:41.852221 23128000471168 run.py:483] Algo bellman_ford step 5353 current loss 0.056437, current_train_items 171328.
I0304 19:30:41.886267 23128000471168 run.py:483] Algo bellman_ford step 5354 current loss 0.110688, current_train_items 171360.
I0304 19:30:41.906577 23128000471168 run.py:483] Algo bellman_ford step 5355 current loss 0.002334, current_train_items 171392.
I0304 19:30:41.922889 23128000471168 run.py:483] Algo bellman_ford step 5356 current loss 0.045226, current_train_items 171424.
I0304 19:30:41.947098 23128000471168 run.py:483] Algo bellman_ford step 5357 current loss 0.080817, current_train_items 171456.
I0304 19:30:41.977410 23128000471168 run.py:483] Algo bellman_ford step 5358 current loss 0.028789, current_train_items 171488.
I0304 19:30:42.010891 23128000471168 run.py:483] Algo bellman_ford step 5359 current loss 0.071191, current_train_items 171520.
I0304 19:30:42.031356 23128000471168 run.py:483] Algo bellman_ford step 5360 current loss 0.023215, current_train_items 171552.
I0304 19:30:42.047840 23128000471168 run.py:483] Algo bellman_ford step 5361 current loss 0.037582, current_train_items 171584.
I0304 19:30:42.072059 23128000471168 run.py:483] Algo bellman_ford step 5362 current loss 0.090274, current_train_items 171616.
I0304 19:30:42.103539 23128000471168 run.py:483] Algo bellman_ford step 5363 current loss 0.058273, current_train_items 171648.
I0304 19:30:42.137162 23128000471168 run.py:483] Algo bellman_ford step 5364 current loss 0.072068, current_train_items 171680.
I0304 19:30:42.157649 23128000471168 run.py:483] Algo bellman_ford step 5365 current loss 0.002785, current_train_items 171712.
I0304 19:30:42.174182 23128000471168 run.py:483] Algo bellman_ford step 5366 current loss 0.028749, current_train_items 171744.
I0304 19:30:42.198438 23128000471168 run.py:483] Algo bellman_ford step 5367 current loss 0.025932, current_train_items 171776.
I0304 19:30:42.229599 23128000471168 run.py:483] Algo bellman_ford step 5368 current loss 0.037428, current_train_items 171808.
I0304 19:30:42.262462 23128000471168 run.py:483] Algo bellman_ford step 5369 current loss 0.046900, current_train_items 171840.
I0304 19:30:42.282999 23128000471168 run.py:483] Algo bellman_ford step 5370 current loss 0.003239, current_train_items 171872.
I0304 19:30:42.299502 23128000471168 run.py:483] Algo bellman_ford step 5371 current loss 0.005545, current_train_items 171904.
I0304 19:30:42.323334 23128000471168 run.py:483] Algo bellman_ford step 5372 current loss 0.020486, current_train_items 171936.
I0304 19:30:42.354540 23128000471168 run.py:483] Algo bellman_ford step 5373 current loss 0.041399, current_train_items 171968.
I0304 19:30:42.386301 23128000471168 run.py:483] Algo bellman_ford step 5374 current loss 0.063750, current_train_items 172000.
I0304 19:30:42.406378 23128000471168 run.py:483] Algo bellman_ford step 5375 current loss 0.001367, current_train_items 172032.
I0304 19:30:42.423172 23128000471168 run.py:483] Algo bellman_ford step 5376 current loss 0.033661, current_train_items 172064.
I0304 19:30:42.446816 23128000471168 run.py:483] Algo bellman_ford step 5377 current loss 0.058289, current_train_items 172096.
I0304 19:30:42.477320 23128000471168 run.py:483] Algo bellman_ford step 5378 current loss 0.026694, current_train_items 172128.
I0304 19:30:42.508442 23128000471168 run.py:483] Algo bellman_ford step 5379 current loss 0.056768, current_train_items 172160.
I0304 19:30:42.528232 23128000471168 run.py:483] Algo bellman_ford step 5380 current loss 0.008859, current_train_items 172192.
I0304 19:30:42.544873 23128000471168 run.py:483] Algo bellman_ford step 5381 current loss 0.008198, current_train_items 172224.
I0304 19:30:42.569558 23128000471168 run.py:483] Algo bellman_ford step 5382 current loss 0.061990, current_train_items 172256.
I0304 19:30:42.601246 23128000471168 run.py:483] Algo bellman_ford step 5383 current loss 0.111518, current_train_items 172288.
I0304 19:30:42.633815 23128000471168 run.py:483] Algo bellman_ford step 5384 current loss 0.059803, current_train_items 172320.
I0304 19:30:42.653960 23128000471168 run.py:483] Algo bellman_ford step 5385 current loss 0.001450, current_train_items 172352.
I0304 19:30:42.670161 23128000471168 run.py:483] Algo bellman_ford step 5386 current loss 0.014998, current_train_items 172384.
I0304 19:30:42.693513 23128000471168 run.py:483] Algo bellman_ford step 5387 current loss 0.019127, current_train_items 172416.
I0304 19:30:42.724195 23128000471168 run.py:483] Algo bellman_ford step 5388 current loss 0.025521, current_train_items 172448.
I0304 19:30:42.758455 23128000471168 run.py:483] Algo bellman_ford step 5389 current loss 0.075863, current_train_items 172480.
I0304 19:30:42.778810 23128000471168 run.py:483] Algo bellman_ford step 5390 current loss 0.003416, current_train_items 172512.
I0304 19:30:42.795947 23128000471168 run.py:483] Algo bellman_ford step 5391 current loss 0.022094, current_train_items 172544.
I0304 19:30:42.819034 23128000471168 run.py:483] Algo bellman_ford step 5392 current loss 0.039094, current_train_items 172576.
I0304 19:30:42.851641 23128000471168 run.py:483] Algo bellman_ford step 5393 current loss 0.037914, current_train_items 172608.
I0304 19:30:42.886424 23128000471168 run.py:483] Algo bellman_ford step 5394 current loss 0.055273, current_train_items 172640.
I0304 19:30:42.906412 23128000471168 run.py:483] Algo bellman_ford step 5395 current loss 0.001822, current_train_items 172672.
I0304 19:30:42.923247 23128000471168 run.py:483] Algo bellman_ford step 5396 current loss 0.028301, current_train_items 172704.
I0304 19:30:42.947502 23128000471168 run.py:483] Algo bellman_ford step 5397 current loss 0.041929, current_train_items 172736.
I0304 19:30:42.979178 23128000471168 run.py:483] Algo bellman_ford step 5398 current loss 0.040446, current_train_items 172768.
I0304 19:30:43.013871 23128000471168 run.py:483] Algo bellman_ford step 5399 current loss 0.133640, current_train_items 172800.
I0304 19:30:43.034173 23128000471168 run.py:483] Algo bellman_ford step 5400 current loss 0.001164, current_train_items 172832.
I0304 19:30:43.041755 23128000471168 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0304 19:30:43.041866 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:43.059496 23128000471168 run.py:483] Algo bellman_ford step 5401 current loss 0.031829, current_train_items 172864.
I0304 19:30:43.084778 23128000471168 run.py:483] Algo bellman_ford step 5402 current loss 0.045775, current_train_items 172896.
I0304 19:30:43.118316 23128000471168 run.py:483] Algo bellman_ford step 5403 current loss 0.074909, current_train_items 172928.
I0304 19:30:43.153721 23128000471168 run.py:483] Algo bellman_ford step 5404 current loss 0.048550, current_train_items 172960.
I0304 19:30:43.174311 23128000471168 run.py:483] Algo bellman_ford step 5405 current loss 0.001466, current_train_items 172992.
I0304 19:30:43.190243 23128000471168 run.py:483] Algo bellman_ford step 5406 current loss 0.023205, current_train_items 173024.
I0304 19:30:43.213973 23128000471168 run.py:483] Algo bellman_ford step 5407 current loss 0.014343, current_train_items 173056.
I0304 19:30:43.244374 23128000471168 run.py:483] Algo bellman_ford step 5408 current loss 0.047456, current_train_items 173088.
I0304 19:30:43.277621 23128000471168 run.py:483] Algo bellman_ford step 5409 current loss 0.045144, current_train_items 173120.
I0304 19:30:43.298052 23128000471168 run.py:483] Algo bellman_ford step 5410 current loss 0.003685, current_train_items 173152.
I0304 19:30:43.315108 23128000471168 run.py:483] Algo bellman_ford step 5411 current loss 0.018276, current_train_items 173184.
I0304 19:30:43.339855 23128000471168 run.py:483] Algo bellman_ford step 5412 current loss 0.054056, current_train_items 173216.
I0304 19:30:43.371652 23128000471168 run.py:483] Algo bellman_ford step 5413 current loss 0.030898, current_train_items 173248.
I0304 19:30:43.406497 23128000471168 run.py:483] Algo bellman_ford step 5414 current loss 0.066335, current_train_items 173280.
I0304 19:30:43.426995 23128000471168 run.py:483] Algo bellman_ford step 5415 current loss 0.014851, current_train_items 173312.
I0304 19:30:43.443705 23128000471168 run.py:483] Algo bellman_ford step 5416 current loss 0.015706, current_train_items 173344.
I0304 19:30:43.468425 23128000471168 run.py:483] Algo bellman_ford step 5417 current loss 0.055825, current_train_items 173376.
I0304 19:30:43.501327 23128000471168 run.py:483] Algo bellman_ford step 5418 current loss 0.035890, current_train_items 173408.
I0304 19:30:43.537135 23128000471168 run.py:483] Algo bellman_ford step 5419 current loss 0.075113, current_train_items 173440.
I0304 19:30:43.557222 23128000471168 run.py:483] Algo bellman_ford step 5420 current loss 0.004340, current_train_items 173472.
I0304 19:30:43.573835 23128000471168 run.py:483] Algo bellman_ford step 5421 current loss 0.035985, current_train_items 173504.
I0304 19:30:43.597703 23128000471168 run.py:483] Algo bellman_ford step 5422 current loss 0.023980, current_train_items 173536.
I0304 19:30:43.629060 23128000471168 run.py:483] Algo bellman_ford step 5423 current loss 0.030222, current_train_items 173568.
I0304 19:30:43.661205 23128000471168 run.py:483] Algo bellman_ford step 5424 current loss 0.070168, current_train_items 173600.
I0304 19:30:43.681560 23128000471168 run.py:483] Algo bellman_ford step 5425 current loss 0.002283, current_train_items 173632.
I0304 19:30:43.697686 23128000471168 run.py:483] Algo bellman_ford step 5426 current loss 0.008091, current_train_items 173664.
I0304 19:30:43.722492 23128000471168 run.py:483] Algo bellman_ford step 5427 current loss 0.046216, current_train_items 173696.
I0304 19:30:43.753930 23128000471168 run.py:483] Algo bellman_ford step 5428 current loss 0.080396, current_train_items 173728.
I0304 19:30:43.788689 23128000471168 run.py:483] Algo bellman_ford step 5429 current loss 0.038058, current_train_items 173760.
I0304 19:30:43.808545 23128000471168 run.py:483] Algo bellman_ford step 5430 current loss 0.001795, current_train_items 173792.
I0304 19:30:43.825167 23128000471168 run.py:483] Algo bellman_ford step 5431 current loss 0.058937, current_train_items 173824.
I0304 19:30:43.850019 23128000471168 run.py:483] Algo bellman_ford step 5432 current loss 0.056375, current_train_items 173856.
I0304 19:30:43.884030 23128000471168 run.py:483] Algo bellman_ford step 5433 current loss 0.079309, current_train_items 173888.
I0304 19:30:43.920260 23128000471168 run.py:483] Algo bellman_ford step 5434 current loss 0.089059, current_train_items 173920.
I0304 19:30:43.940014 23128000471168 run.py:483] Algo bellman_ford step 5435 current loss 0.002331, current_train_items 173952.
I0304 19:30:43.956450 23128000471168 run.py:483] Algo bellman_ford step 5436 current loss 0.051636, current_train_items 173984.
I0304 19:30:43.980856 23128000471168 run.py:483] Algo bellman_ford step 5437 current loss 0.065295, current_train_items 174016.
I0304 19:30:44.011599 23128000471168 run.py:483] Algo bellman_ford step 5438 current loss 0.104218, current_train_items 174048.
I0304 19:30:44.049672 23128000471168 run.py:483] Algo bellman_ford step 5439 current loss 0.123803, current_train_items 174080.
I0304 19:30:44.069935 23128000471168 run.py:483] Algo bellman_ford step 5440 current loss 0.008644, current_train_items 174112.
I0304 19:30:44.086172 23128000471168 run.py:483] Algo bellman_ford step 5441 current loss 0.007434, current_train_items 174144.
I0304 19:30:44.109819 23128000471168 run.py:483] Algo bellman_ford step 5442 current loss 0.031637, current_train_items 174176.
I0304 19:30:44.140814 23128000471168 run.py:483] Algo bellman_ford step 5443 current loss 0.036391, current_train_items 174208.
I0304 19:30:44.175094 23128000471168 run.py:483] Algo bellman_ford step 5444 current loss 0.069466, current_train_items 174240.
I0304 19:30:44.195214 23128000471168 run.py:483] Algo bellman_ford step 5445 current loss 0.001603, current_train_items 174272.
I0304 19:30:44.211595 23128000471168 run.py:483] Algo bellman_ford step 5446 current loss 0.018874, current_train_items 174304.
I0304 19:30:44.236722 23128000471168 run.py:483] Algo bellman_ford step 5447 current loss 0.069580, current_train_items 174336.
I0304 19:30:44.269224 23128000471168 run.py:483] Algo bellman_ford step 5448 current loss 0.084773, current_train_items 174368.
I0304 19:30:44.303418 23128000471168 run.py:483] Algo bellman_ford step 5449 current loss 0.058284, current_train_items 174400.
I0304 19:30:44.323446 23128000471168 run.py:483] Algo bellman_ford step 5450 current loss 0.003194, current_train_items 174432.
I0304 19:30:44.331874 23128000471168 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0304 19:30:44.331980 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:44.348662 23128000471168 run.py:483] Algo bellman_ford step 5451 current loss 0.063266, current_train_items 174464.
I0304 19:30:44.373163 23128000471168 run.py:483] Algo bellman_ford step 5452 current loss 0.054700, current_train_items 174496.
I0304 19:30:44.404150 23128000471168 run.py:483] Algo bellman_ford step 5453 current loss 0.062641, current_train_items 174528.
I0304 19:30:44.438417 23128000471168 run.py:483] Algo bellman_ford step 5454 current loss 0.067266, current_train_items 174560.
I0304 19:30:44.458595 23128000471168 run.py:483] Algo bellman_ford step 5455 current loss 0.002335, current_train_items 174592.
I0304 19:30:44.475084 23128000471168 run.py:483] Algo bellman_ford step 5456 current loss 0.010969, current_train_items 174624.
I0304 19:30:44.499664 23128000471168 run.py:483] Algo bellman_ford step 5457 current loss 0.092586, current_train_items 174656.
I0304 19:30:44.532187 23128000471168 run.py:483] Algo bellman_ford step 5458 current loss 0.112099, current_train_items 174688.
I0304 19:30:44.563601 23128000471168 run.py:483] Algo bellman_ford step 5459 current loss 0.112436, current_train_items 174720.
I0304 19:30:44.583625 23128000471168 run.py:483] Algo bellman_ford step 5460 current loss 0.002863, current_train_items 174752.
I0304 19:30:44.600738 23128000471168 run.py:483] Algo bellman_ford step 5461 current loss 0.016134, current_train_items 174784.
I0304 19:30:44.625039 23128000471168 run.py:483] Algo bellman_ford step 5462 current loss 0.026403, current_train_items 174816.
I0304 19:30:44.655980 23128000471168 run.py:483] Algo bellman_ford step 5463 current loss 0.046020, current_train_items 174848.
I0304 19:30:44.690188 23128000471168 run.py:483] Algo bellman_ford step 5464 current loss 0.070309, current_train_items 174880.
I0304 19:30:44.710575 23128000471168 run.py:483] Algo bellman_ford step 5465 current loss 0.002342, current_train_items 174912.
I0304 19:30:44.726784 23128000471168 run.py:483] Algo bellman_ford step 5466 current loss 0.015100, current_train_items 174944.
I0304 19:30:44.750738 23128000471168 run.py:483] Algo bellman_ford step 5467 current loss 0.020631, current_train_items 174976.
I0304 19:30:44.781852 23128000471168 run.py:483] Algo bellman_ford step 5468 current loss 0.051495, current_train_items 175008.
I0304 19:30:44.815448 23128000471168 run.py:483] Algo bellman_ford step 5469 current loss 0.108056, current_train_items 175040.
I0304 19:30:44.835697 23128000471168 run.py:483] Algo bellman_ford step 5470 current loss 0.002626, current_train_items 175072.
I0304 19:30:44.852400 23128000471168 run.py:483] Algo bellman_ford step 5471 current loss 0.039382, current_train_items 175104.
I0304 19:30:44.875828 23128000471168 run.py:483] Algo bellman_ford step 5472 current loss 0.038575, current_train_items 175136.
I0304 19:30:44.906815 23128000471168 run.py:483] Algo bellman_ford step 5473 current loss 0.026662, current_train_items 175168.
I0304 19:30:44.941545 23128000471168 run.py:483] Algo bellman_ford step 5474 current loss 0.057686, current_train_items 175200.
I0304 19:30:44.961685 23128000471168 run.py:483] Algo bellman_ford step 5475 current loss 0.001607, current_train_items 175232.
I0304 19:30:44.978257 23128000471168 run.py:483] Algo bellman_ford step 5476 current loss 0.022220, current_train_items 175264.
I0304 19:30:45.001817 23128000471168 run.py:483] Algo bellman_ford step 5477 current loss 0.086466, current_train_items 175296.
I0304 19:30:45.033748 23128000471168 run.py:483] Algo bellman_ford step 5478 current loss 0.096045, current_train_items 175328.
I0304 19:30:45.067508 23128000471168 run.py:483] Algo bellman_ford step 5479 current loss 0.090635, current_train_items 175360.
I0304 19:30:45.087489 23128000471168 run.py:483] Algo bellman_ford step 5480 current loss 0.006227, current_train_items 175392.
I0304 19:30:45.104449 23128000471168 run.py:483] Algo bellman_ford step 5481 current loss 0.037994, current_train_items 175424.
I0304 19:30:45.129837 23128000471168 run.py:483] Algo bellman_ford step 5482 current loss 0.100304, current_train_items 175456.
I0304 19:30:45.162238 23128000471168 run.py:483] Algo bellman_ford step 5483 current loss 0.130081, current_train_items 175488.
I0304 19:30:45.197842 23128000471168 run.py:483] Algo bellman_ford step 5484 current loss 0.136337, current_train_items 175520.
I0304 19:30:45.218152 23128000471168 run.py:483] Algo bellman_ford step 5485 current loss 0.004385, current_train_items 175552.
I0304 19:30:45.234480 23128000471168 run.py:483] Algo bellman_ford step 5486 current loss 0.042115, current_train_items 175584.
I0304 19:30:45.257665 23128000471168 run.py:483] Algo bellman_ford step 5487 current loss 0.081096, current_train_items 175616.
I0304 19:30:45.290524 23128000471168 run.py:483] Algo bellman_ford step 5488 current loss 0.063663, current_train_items 175648.
I0304 19:30:45.323395 23128000471168 run.py:483] Algo bellman_ford step 5489 current loss 0.058419, current_train_items 175680.
I0304 19:30:45.343714 23128000471168 run.py:483] Algo bellman_ford step 5490 current loss 0.003896, current_train_items 175712.
I0304 19:30:45.360375 23128000471168 run.py:483] Algo bellman_ford step 5491 current loss 0.038245, current_train_items 175744.
I0304 19:30:45.384466 23128000471168 run.py:483] Algo bellman_ford step 5492 current loss 0.040545, current_train_items 175776.
I0304 19:30:45.415841 23128000471168 run.py:483] Algo bellman_ford step 5493 current loss 0.048682, current_train_items 175808.
I0304 19:30:45.449442 23128000471168 run.py:483] Algo bellman_ford step 5494 current loss 0.085086, current_train_items 175840.
I0304 19:30:45.469634 23128000471168 run.py:483] Algo bellman_ford step 5495 current loss 0.064668, current_train_items 175872.
I0304 19:30:45.486323 23128000471168 run.py:483] Algo bellman_ford step 5496 current loss 0.010169, current_train_items 175904.
I0304 19:30:45.510734 23128000471168 run.py:483] Algo bellman_ford step 5497 current loss 0.088541, current_train_items 175936.
I0304 19:30:45.542487 23128000471168 run.py:483] Algo bellman_ford step 5498 current loss 0.106951, current_train_items 175968.
I0304 19:30:45.577082 23128000471168 run.py:483] Algo bellman_ford step 5499 current loss 0.147224, current_train_items 176000.
I0304 19:30:45.597721 23128000471168 run.py:483] Algo bellman_ford step 5500 current loss 0.006388, current_train_items 176032.
I0304 19:30:45.605563 23128000471168 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0304 19:30:45.605671 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:45.623536 23128000471168 run.py:483] Algo bellman_ford step 5501 current loss 0.087076, current_train_items 176064.
I0304 19:30:45.648797 23128000471168 run.py:483] Algo bellman_ford step 5502 current loss 0.070863, current_train_items 176096.
I0304 19:30:45.680662 23128000471168 run.py:483] Algo bellman_ford step 5503 current loss 0.064684, current_train_items 176128.
I0304 19:30:45.715459 23128000471168 run.py:483] Algo bellman_ford step 5504 current loss 0.103720, current_train_items 176160.
I0304 19:30:45.735620 23128000471168 run.py:483] Algo bellman_ford step 5505 current loss 0.005473, current_train_items 176192.
I0304 19:30:45.752033 23128000471168 run.py:483] Algo bellman_ford step 5506 current loss 0.028272, current_train_items 176224.
I0304 19:30:45.776115 23128000471168 run.py:483] Algo bellman_ford step 5507 current loss 0.013004, current_train_items 176256.
I0304 19:30:45.806825 23128000471168 run.py:483] Algo bellman_ford step 5508 current loss 0.068224, current_train_items 176288.
I0304 19:30:45.840304 23128000471168 run.py:483] Algo bellman_ford step 5509 current loss 0.033677, current_train_items 176320.
I0304 19:30:45.860370 23128000471168 run.py:483] Algo bellman_ford step 5510 current loss 0.003450, current_train_items 176352.
I0304 19:30:45.877053 23128000471168 run.py:483] Algo bellman_ford step 5511 current loss 0.020260, current_train_items 176384.
I0304 19:30:45.902598 23128000471168 run.py:483] Algo bellman_ford step 5512 current loss 0.050633, current_train_items 176416.
I0304 19:30:45.933458 23128000471168 run.py:483] Algo bellman_ford step 5513 current loss 0.037934, current_train_items 176448.
I0304 19:30:45.966116 23128000471168 run.py:483] Algo bellman_ford step 5514 current loss 0.053442, current_train_items 176480.
I0304 19:30:45.986366 23128000471168 run.py:483] Algo bellman_ford step 5515 current loss 0.003127, current_train_items 176512.
I0304 19:30:46.002527 23128000471168 run.py:483] Algo bellman_ford step 5516 current loss 0.006345, current_train_items 176544.
I0304 19:30:46.027227 23128000471168 run.py:483] Algo bellman_ford step 5517 current loss 0.049797, current_train_items 176576.
I0304 19:30:46.058564 23128000471168 run.py:483] Algo bellman_ford step 5518 current loss 0.048062, current_train_items 176608.
I0304 19:30:46.094629 23128000471168 run.py:483] Algo bellman_ford step 5519 current loss 0.072361, current_train_items 176640.
I0304 19:30:46.114943 23128000471168 run.py:483] Algo bellman_ford step 5520 current loss 0.015129, current_train_items 176672.
I0304 19:30:46.131711 23128000471168 run.py:483] Algo bellman_ford step 5521 current loss 0.008998, current_train_items 176704.
I0304 19:30:46.155573 23128000471168 run.py:483] Algo bellman_ford step 5522 current loss 0.014853, current_train_items 176736.
I0304 19:30:46.188225 23128000471168 run.py:483] Algo bellman_ford step 5523 current loss 0.044283, current_train_items 176768.
I0304 19:30:46.223450 23128000471168 run.py:483] Algo bellman_ford step 5524 current loss 0.085542, current_train_items 176800.
I0304 19:30:46.243334 23128000471168 run.py:483] Algo bellman_ford step 5525 current loss 0.002724, current_train_items 176832.
I0304 19:30:46.259486 23128000471168 run.py:483] Algo bellman_ford step 5526 current loss 0.016233, current_train_items 176864.
I0304 19:30:46.285218 23128000471168 run.py:483] Algo bellman_ford step 5527 current loss 0.030967, current_train_items 176896.
I0304 19:30:46.315974 23128000471168 run.py:483] Algo bellman_ford step 5528 current loss 0.039747, current_train_items 176928.
I0304 19:30:46.348554 23128000471168 run.py:483] Algo bellman_ford step 5529 current loss 0.031656, current_train_items 176960.
I0304 19:30:46.368765 23128000471168 run.py:483] Algo bellman_ford step 5530 current loss 0.001675, current_train_items 176992.
I0304 19:30:46.385576 23128000471168 run.py:483] Algo bellman_ford step 5531 current loss 0.025083, current_train_items 177024.
I0304 19:30:46.410773 23128000471168 run.py:483] Algo bellman_ford step 5532 current loss 0.042630, current_train_items 177056.
I0304 19:30:46.441718 23128000471168 run.py:483] Algo bellman_ford step 5533 current loss 0.057927, current_train_items 177088.
I0304 19:30:46.477820 23128000471168 run.py:483] Algo bellman_ford step 5534 current loss 0.077265, current_train_items 177120.
I0304 19:30:46.497977 23128000471168 run.py:483] Algo bellman_ford step 5535 current loss 0.007066, current_train_items 177152.
I0304 19:30:46.514812 23128000471168 run.py:483] Algo bellman_ford step 5536 current loss 0.059990, current_train_items 177184.
I0304 19:30:46.538356 23128000471168 run.py:483] Algo bellman_ford step 5537 current loss 0.076134, current_train_items 177216.
I0304 19:30:46.569239 23128000471168 run.py:483] Algo bellman_ford step 5538 current loss 0.108568, current_train_items 177248.
I0304 19:30:46.604888 23128000471168 run.py:483] Algo bellman_ford step 5539 current loss 0.067490, current_train_items 177280.
I0304 19:30:46.625298 23128000471168 run.py:483] Algo bellman_ford step 5540 current loss 0.001336, current_train_items 177312.
I0304 19:30:46.642160 23128000471168 run.py:483] Algo bellman_ford step 5541 current loss 0.024142, current_train_items 177344.
I0304 19:30:46.667362 23128000471168 run.py:483] Algo bellman_ford step 5542 current loss 0.053093, current_train_items 177376.
I0304 19:30:46.698619 23128000471168 run.py:483] Algo bellman_ford step 5543 current loss 0.045964, current_train_items 177408.
I0304 19:30:46.731937 23128000471168 run.py:483] Algo bellman_ford step 5544 current loss 0.067722, current_train_items 177440.
I0304 19:30:46.752128 23128000471168 run.py:483] Algo bellman_ford step 5545 current loss 0.002995, current_train_items 177472.
I0304 19:30:46.769079 23128000471168 run.py:483] Algo bellman_ford step 5546 current loss 0.033134, current_train_items 177504.
I0304 19:30:46.793321 23128000471168 run.py:483] Algo bellman_ford step 5547 current loss 0.024928, current_train_items 177536.
I0304 19:30:46.824682 23128000471168 run.py:483] Algo bellman_ford step 5548 current loss 0.035798, current_train_items 177568.
I0304 19:30:46.857633 23128000471168 run.py:483] Algo bellman_ford step 5549 current loss 0.072190, current_train_items 177600.
I0304 19:30:46.877566 23128000471168 run.py:483] Algo bellman_ford step 5550 current loss 0.002207, current_train_items 177632.
I0304 19:30:46.885520 23128000471168 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0304 19:30:46.885626 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:30:46.902720 23128000471168 run.py:483] Algo bellman_ford step 5551 current loss 0.009828, current_train_items 177664.
I0304 19:30:46.928678 23128000471168 run.py:483] Algo bellman_ford step 5552 current loss 0.076315, current_train_items 177696.
I0304 19:30:46.960569 23128000471168 run.py:483] Algo bellman_ford step 5553 current loss 0.083349, current_train_items 177728.
I0304 19:30:46.997521 23128000471168 run.py:483] Algo bellman_ford step 5554 current loss 0.115108, current_train_items 177760.
I0304 19:30:47.018160 23128000471168 run.py:483] Algo bellman_ford step 5555 current loss 0.006180, current_train_items 177792.
I0304 19:30:47.034330 23128000471168 run.py:483] Algo bellman_ford step 5556 current loss 0.015296, current_train_items 177824.
I0304 19:30:47.059117 23128000471168 run.py:483] Algo bellman_ford step 5557 current loss 0.055265, current_train_items 177856.
I0304 19:30:47.089955 23128000471168 run.py:483] Algo bellman_ford step 5558 current loss 0.059604, current_train_items 177888.
I0304 19:30:47.124662 23128000471168 run.py:483] Algo bellman_ford step 5559 current loss 0.068662, current_train_items 177920.
I0304 19:30:47.145564 23128000471168 run.py:483] Algo bellman_ford step 5560 current loss 0.011019, current_train_items 177952.
I0304 19:30:47.161986 23128000471168 run.py:483] Algo bellman_ford step 5561 current loss 0.021566, current_train_items 177984.
I0304 19:30:47.185545 23128000471168 run.py:483] Algo bellman_ford step 5562 current loss 0.046050, current_train_items 178016.
I0304 19:30:47.217839 23128000471168 run.py:483] Algo bellman_ford step 5563 current loss 0.064945, current_train_items 178048.
I0304 19:30:47.252758 23128000471168 run.py:483] Algo bellman_ford step 5564 current loss 0.081928, current_train_items 178080.
I0304 19:30:47.272832 23128000471168 run.py:483] Algo bellman_ford step 5565 current loss 0.003084, current_train_items 178112.
I0304 19:30:47.289322 23128000471168 run.py:483] Algo bellman_ford step 5566 current loss 0.016023, current_train_items 178144.
I0304 19:30:47.313584 23128000471168 run.py:483] Algo bellman_ford step 5567 current loss 0.087554, current_train_items 178176.
I0304 19:30:47.346081 23128000471168 run.py:483] Algo bellman_ford step 5568 current loss 0.061355, current_train_items 178208.
I0304 19:30:47.380605 23128000471168 run.py:483] Algo bellman_ford step 5569 current loss 0.070489, current_train_items 178240.
I0304 19:30:47.401188 23128000471168 run.py:483] Algo bellman_ford step 5570 current loss 0.008744, current_train_items 178272.
I0304 19:30:47.417553 23128000471168 run.py:483] Algo bellman_ford step 5571 current loss 0.008558, current_train_items 178304.
I0304 19:30:47.441241 23128000471168 run.py:483] Algo bellman_ford step 5572 current loss 0.027763, current_train_items 178336.
I0304 19:30:47.472865 23128000471168 run.py:483] Algo bellman_ford step 5573 current loss 0.045164, current_train_items 178368.
I0304 19:30:47.506924 23128000471168 run.py:483] Algo bellman_ford step 5574 current loss 0.062790, current_train_items 178400.
I0304 19:30:47.527567 23128000471168 run.py:483] Algo bellman_ford step 5575 current loss 0.002227, current_train_items 178432.
I0304 19:30:47.544313 23128000471168 run.py:483] Algo bellman_ford step 5576 current loss 0.031918, current_train_items 178464.
I0304 19:30:47.567562 23128000471168 run.py:483] Algo bellman_ford step 5577 current loss 0.040755, current_train_items 178496.
I0304 19:30:47.599038 23128000471168 run.py:483] Algo bellman_ford step 5578 current loss 0.045400, current_train_items 178528.
I0304 19:30:47.634654 23128000471168 run.py:483] Algo bellman_ford step 5579 current loss 0.058799, current_train_items 178560.
I0304 19:30:47.654632 23128000471168 run.py:483] Algo bellman_ford step 5580 current loss 0.002444, current_train_items 178592.
I0304 19:30:47.670644 23128000471168 run.py:483] Algo bellman_ford step 5581 current loss 0.054917, current_train_items 178624.
I0304 19:30:47.695722 23128000471168 run.py:483] Algo bellman_ford step 5582 current loss 0.035482, current_train_items 178656.
I0304 19:30:47.727318 23128000471168 run.py:483] Algo bellman_ford step 5583 current loss 0.058697, current_train_items 178688.
I0304 19:30:47.761564 23128000471168 run.py:483] Algo bellman_ford step 5584 current loss 0.054309, current_train_items 178720.
I0304 19:30:47.781832 23128000471168 run.py:483] Algo bellman_ford step 5585 current loss 0.011113, current_train_items 178752.
I0304 19:30:47.798594 23128000471168 run.py:483] Algo bellman_ford step 5586 current loss 0.014936, current_train_items 178784.
I0304 19:30:47.823799 23128000471168 run.py:483] Algo bellman_ford step 5587 current loss 0.079665, current_train_items 178816.
I0304 19:30:47.856159 23128000471168 run.py:483] Algo bellman_ford step 5588 current loss 0.057630, current_train_items 178848.
I0304 19:30:47.888184 23128000471168 run.py:483] Algo bellman_ford step 5589 current loss 0.068637, current_train_items 178880.
I0304 19:30:47.908440 23128000471168 run.py:483] Algo bellman_ford step 5590 current loss 0.002226, current_train_items 178912.
I0304 19:30:47.924898 23128000471168 run.py:483] Algo bellman_ford step 5591 current loss 0.015266, current_train_items 178944.
I0304 19:30:47.947648 23128000471168 run.py:483] Algo bellman_ford step 5592 current loss 0.026183, current_train_items 178976.
I0304 19:30:47.979168 23128000471168 run.py:483] Algo bellman_ford step 5593 current loss 0.082872, current_train_items 179008.
I0304 19:30:48.014058 23128000471168 run.py:483] Algo bellman_ford step 5594 current loss 0.064255, current_train_items 179040.
I0304 19:30:48.033739 23128000471168 run.py:483] Algo bellman_ford step 5595 current loss 0.015443, current_train_items 179072.
I0304 19:30:48.049963 23128000471168 run.py:483] Algo bellman_ford step 5596 current loss 0.010442, current_train_items 179104.
I0304 19:30:48.073913 23128000471168 run.py:483] Algo bellman_ford step 5597 current loss 0.052786, current_train_items 179136.
I0304 19:30:48.105512 23128000471168 run.py:483] Algo bellman_ford step 5598 current loss 0.100001, current_train_items 179168.
I0304 19:30:48.135775 23128000471168 run.py:483] Algo bellman_ford step 5599 current loss 0.071977, current_train_items 179200.
I0304 19:30:48.156280 23128000471168 run.py:483] Algo bellman_ford step 5600 current loss 0.002236, current_train_items 179232.
I0304 19:30:48.164060 23128000471168 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0304 19:30:48.164169 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:48.181307 23128000471168 run.py:483] Algo bellman_ford step 5601 current loss 0.017540, current_train_items 179264.
I0304 19:30:48.206523 23128000471168 run.py:483] Algo bellman_ford step 5602 current loss 0.018802, current_train_items 179296.
I0304 19:30:48.238461 23128000471168 run.py:483] Algo bellman_ford step 5603 current loss 0.070323, current_train_items 179328.
I0304 19:30:48.271495 23128000471168 run.py:483] Algo bellman_ford step 5604 current loss 0.038761, current_train_items 179360.
I0304 19:30:48.292262 23128000471168 run.py:483] Algo bellman_ford step 5605 current loss 0.001931, current_train_items 179392.
I0304 19:30:48.308403 23128000471168 run.py:483] Algo bellman_ford step 5606 current loss 0.016289, current_train_items 179424.
I0304 19:30:48.333061 23128000471168 run.py:483] Algo bellman_ford step 5607 current loss 0.041040, current_train_items 179456.
I0304 19:30:48.363398 23128000471168 run.py:483] Algo bellman_ford step 5608 current loss 0.102229, current_train_items 179488.
I0304 19:30:48.398047 23128000471168 run.py:483] Algo bellman_ford step 5609 current loss 0.100856, current_train_items 179520.
I0304 19:30:48.418716 23128000471168 run.py:483] Algo bellman_ford step 5610 current loss 0.002265, current_train_items 179552.
I0304 19:30:48.435202 23128000471168 run.py:483] Algo bellman_ford step 5611 current loss 0.016033, current_train_items 179584.
I0304 19:30:48.459069 23128000471168 run.py:483] Algo bellman_ford step 5612 current loss 0.047703, current_train_items 179616.
I0304 19:30:48.491750 23128000471168 run.py:483] Algo bellman_ford step 5613 current loss 0.050600, current_train_items 179648.
I0304 19:30:48.524463 23128000471168 run.py:483] Algo bellman_ford step 5614 current loss 0.054618, current_train_items 179680.
I0304 19:30:48.544618 23128000471168 run.py:483] Algo bellman_ford step 5615 current loss 0.004764, current_train_items 179712.
I0304 19:30:48.561388 23128000471168 run.py:483] Algo bellman_ford step 5616 current loss 0.014497, current_train_items 179744.
I0304 19:30:48.585850 23128000471168 run.py:483] Algo bellman_ford step 5617 current loss 0.013340, current_train_items 179776.
I0304 19:30:48.616866 23128000471168 run.py:483] Algo bellman_ford step 5618 current loss 0.041067, current_train_items 179808.
I0304 19:30:48.652259 23128000471168 run.py:483] Algo bellman_ford step 5619 current loss 0.092413, current_train_items 179840.
I0304 19:30:48.672153 23128000471168 run.py:483] Algo bellman_ford step 5620 current loss 0.005317, current_train_items 179872.
I0304 19:30:48.688656 23128000471168 run.py:483] Algo bellman_ford step 5621 current loss 0.016027, current_train_items 179904.
I0304 19:30:48.712591 23128000471168 run.py:483] Algo bellman_ford step 5622 current loss 0.034957, current_train_items 179936.
I0304 19:30:48.743976 23128000471168 run.py:483] Algo bellman_ford step 5623 current loss 0.063003, current_train_items 179968.
I0304 19:30:48.778873 23128000471168 run.py:483] Algo bellman_ford step 5624 current loss 0.064508, current_train_items 180000.
I0304 19:30:48.799051 23128000471168 run.py:483] Algo bellman_ford step 5625 current loss 0.001212, current_train_items 180032.
I0304 19:30:48.815932 23128000471168 run.py:483] Algo bellman_ford step 5626 current loss 0.021680, current_train_items 180064.
I0304 19:30:48.840606 23128000471168 run.py:483] Algo bellman_ford step 5627 current loss 0.031606, current_train_items 180096.
I0304 19:30:48.872677 23128000471168 run.py:483] Algo bellman_ford step 5628 current loss 0.076517, current_train_items 180128.
I0304 19:30:48.907326 23128000471168 run.py:483] Algo bellman_ford step 5629 current loss 0.055700, current_train_items 180160.
I0304 19:30:48.927417 23128000471168 run.py:483] Algo bellman_ford step 5630 current loss 0.002359, current_train_items 180192.
I0304 19:30:48.943700 23128000471168 run.py:483] Algo bellman_ford step 5631 current loss 0.004404, current_train_items 180224.
I0304 19:30:48.968660 23128000471168 run.py:483] Algo bellman_ford step 5632 current loss 0.045206, current_train_items 180256.
I0304 19:30:48.999829 23128000471168 run.py:483] Algo bellman_ford step 5633 current loss 0.047575, current_train_items 180288.
I0304 19:30:49.034461 23128000471168 run.py:483] Algo bellman_ford step 5634 current loss 0.092850, current_train_items 180320.
I0304 19:30:49.054667 23128000471168 run.py:483] Algo bellman_ford step 5635 current loss 0.006359, current_train_items 180352.
I0304 19:30:49.071292 23128000471168 run.py:483] Algo bellman_ford step 5636 current loss 0.012657, current_train_items 180384.
I0304 19:30:49.095831 23128000471168 run.py:483] Algo bellman_ford step 5637 current loss 0.040470, current_train_items 180416.
I0304 19:30:49.126220 23128000471168 run.py:483] Algo bellman_ford step 5638 current loss 0.058937, current_train_items 180448.
I0304 19:30:49.158722 23128000471168 run.py:483] Algo bellman_ford step 5639 current loss 0.094757, current_train_items 180480.
I0304 19:30:49.178813 23128000471168 run.py:483] Algo bellman_ford step 5640 current loss 0.001959, current_train_items 180512.
I0304 19:30:49.195195 23128000471168 run.py:483] Algo bellman_ford step 5641 current loss 0.008614, current_train_items 180544.
I0304 19:30:49.219479 23128000471168 run.py:483] Algo bellman_ford step 5642 current loss 0.048911, current_train_items 180576.
I0304 19:30:49.251150 23128000471168 run.py:483] Algo bellman_ford step 5643 current loss 0.063071, current_train_items 180608.
I0304 19:30:49.284553 23128000471168 run.py:483] Algo bellman_ford step 5644 current loss 0.062046, current_train_items 180640.
I0304 19:30:49.304943 23128000471168 run.py:483] Algo bellman_ford step 5645 current loss 0.002026, current_train_items 180672.
I0304 19:30:49.322059 23128000471168 run.py:483] Algo bellman_ford step 5646 current loss 0.034976, current_train_items 180704.
I0304 19:30:49.345769 23128000471168 run.py:483] Algo bellman_ford step 5647 current loss 0.037632, current_train_items 180736.
I0304 19:30:49.377333 23128000471168 run.py:483] Algo bellman_ford step 5648 current loss 0.065037, current_train_items 180768.
I0304 19:30:49.412815 23128000471168 run.py:483] Algo bellman_ford step 5649 current loss 0.060843, current_train_items 180800.
I0304 19:30:49.433074 23128000471168 run.py:483] Algo bellman_ford step 5650 current loss 0.018400, current_train_items 180832.
I0304 19:30:49.441046 23128000471168 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0304 19:30:49.441154 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:49.458320 23128000471168 run.py:483] Algo bellman_ford step 5651 current loss 0.013718, current_train_items 180864.
I0304 19:30:49.482698 23128000471168 run.py:483] Algo bellman_ford step 5652 current loss 0.037071, current_train_items 180896.
I0304 19:30:49.516549 23128000471168 run.py:483] Algo bellman_ford step 5653 current loss 0.073108, current_train_items 180928.
I0304 19:30:49.552256 23128000471168 run.py:483] Algo bellman_ford step 5654 current loss 0.062577, current_train_items 180960.
I0304 19:30:49.572823 23128000471168 run.py:483] Algo bellman_ford step 5655 current loss 0.002835, current_train_items 180992.
I0304 19:30:49.589302 23128000471168 run.py:483] Algo bellman_ford step 5656 current loss 0.008051, current_train_items 181024.
I0304 19:30:49.612824 23128000471168 run.py:483] Algo bellman_ford step 5657 current loss 0.028411, current_train_items 181056.
I0304 19:30:49.645460 23128000471168 run.py:483] Algo bellman_ford step 5658 current loss 0.051781, current_train_items 181088.
I0304 19:30:49.679840 23128000471168 run.py:483] Algo bellman_ford step 5659 current loss 0.071869, current_train_items 181120.
I0304 19:30:49.700384 23128000471168 run.py:483] Algo bellman_ford step 5660 current loss 0.027941, current_train_items 181152.
I0304 19:30:49.716739 23128000471168 run.py:483] Algo bellman_ford step 5661 current loss 0.020124, current_train_items 181184.
I0304 19:30:49.740368 23128000471168 run.py:483] Algo bellman_ford step 5662 current loss 0.055925, current_train_items 181216.
I0304 19:30:49.774308 23128000471168 run.py:483] Algo bellman_ford step 5663 current loss 0.083691, current_train_items 181248.
I0304 19:30:49.808712 23128000471168 run.py:483] Algo bellman_ford step 5664 current loss 0.065620, current_train_items 181280.
I0304 19:30:49.829066 23128000471168 run.py:483] Algo bellman_ford step 5665 current loss 0.010656, current_train_items 181312.
I0304 19:30:49.845936 23128000471168 run.py:483] Algo bellman_ford step 5666 current loss 0.031101, current_train_items 181344.
I0304 19:30:49.869877 23128000471168 run.py:483] Algo bellman_ford step 5667 current loss 0.043430, current_train_items 181376.
I0304 19:30:49.902444 23128000471168 run.py:483] Algo bellman_ford step 5668 current loss 0.073457, current_train_items 181408.
I0304 19:30:49.935596 23128000471168 run.py:483] Algo bellman_ford step 5669 current loss 0.037300, current_train_items 181440.
I0304 19:30:49.955758 23128000471168 run.py:483] Algo bellman_ford step 5670 current loss 0.002205, current_train_items 181472.
I0304 19:30:49.972282 23128000471168 run.py:483] Algo bellman_ford step 5671 current loss 0.006659, current_train_items 181504.
I0304 19:30:49.996157 23128000471168 run.py:483] Algo bellman_ford step 5672 current loss 0.047237, current_train_items 181536.
I0304 19:30:50.027711 23128000471168 run.py:483] Algo bellman_ford step 5673 current loss 0.075704, current_train_items 181568.
I0304 19:30:50.061089 23128000471168 run.py:483] Algo bellman_ford step 5674 current loss 0.048933, current_train_items 181600.
I0304 19:30:50.081480 23128000471168 run.py:483] Algo bellman_ford step 5675 current loss 0.003439, current_train_items 181632.
I0304 19:30:50.097839 23128000471168 run.py:483] Algo bellman_ford step 5676 current loss 0.016609, current_train_items 181664.
I0304 19:30:50.122136 23128000471168 run.py:483] Algo bellman_ford step 5677 current loss 0.048381, current_train_items 181696.
I0304 19:30:50.155338 23128000471168 run.py:483] Algo bellman_ford step 5678 current loss 0.109760, current_train_items 181728.
I0304 19:30:50.189579 23128000471168 run.py:483] Algo bellman_ford step 5679 current loss 0.108855, current_train_items 181760.
I0304 19:30:50.209842 23128000471168 run.py:483] Algo bellman_ford step 5680 current loss 0.004046, current_train_items 181792.
I0304 19:30:50.226711 23128000471168 run.py:483] Algo bellman_ford step 5681 current loss 0.012553, current_train_items 181824.
I0304 19:30:50.250787 23128000471168 run.py:483] Algo bellman_ford step 5682 current loss 0.067860, current_train_items 181856.
I0304 19:30:50.282297 23128000471168 run.py:483] Algo bellman_ford step 5683 current loss 0.060849, current_train_items 181888.
I0304 19:30:50.315155 23128000471168 run.py:483] Algo bellman_ford step 5684 current loss 0.081825, current_train_items 181920.
I0304 19:30:50.335570 23128000471168 run.py:483] Algo bellman_ford step 5685 current loss 0.006019, current_train_items 181952.
I0304 19:30:50.352156 23128000471168 run.py:483] Algo bellman_ford step 5686 current loss 0.030816, current_train_items 181984.
I0304 19:30:50.376423 23128000471168 run.py:483] Algo bellman_ford step 5687 current loss 0.074932, current_train_items 182016.
I0304 19:30:50.408713 23128000471168 run.py:483] Algo bellman_ford step 5688 current loss 0.115234, current_train_items 182048.
I0304 19:30:50.441836 23128000471168 run.py:483] Algo bellman_ford step 5689 current loss 0.076513, current_train_items 182080.
I0304 19:30:50.462612 23128000471168 run.py:483] Algo bellman_ford step 5690 current loss 0.002393, current_train_items 182112.
I0304 19:30:50.479750 23128000471168 run.py:483] Algo bellman_ford step 5691 current loss 0.014575, current_train_items 182144.
I0304 19:30:50.503464 23128000471168 run.py:483] Algo bellman_ford step 5692 current loss 0.039200, current_train_items 182176.
I0304 19:30:50.534995 23128000471168 run.py:483] Algo bellman_ford step 5693 current loss 0.050572, current_train_items 182208.
I0304 19:30:50.567848 23128000471168 run.py:483] Algo bellman_ford step 5694 current loss 0.066154, current_train_items 182240.
I0304 19:30:50.587868 23128000471168 run.py:483] Algo bellman_ford step 5695 current loss 0.002146, current_train_items 182272.
I0304 19:30:50.604855 23128000471168 run.py:483] Algo bellman_ford step 5696 current loss 0.014485, current_train_items 182304.
I0304 19:30:50.629283 23128000471168 run.py:483] Algo bellman_ford step 5697 current loss 0.079804, current_train_items 182336.
I0304 19:30:50.662240 23128000471168 run.py:483] Algo bellman_ford step 5698 current loss 0.074376, current_train_items 182368.
I0304 19:30:50.697022 23128000471168 run.py:483] Algo bellman_ford step 5699 current loss 0.064968, current_train_items 182400.
I0304 19:30:50.717210 23128000471168 run.py:483] Algo bellman_ford step 5700 current loss 0.010829, current_train_items 182432.
I0304 19:30:50.724885 23128000471168 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0304 19:30:50.724993 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:50.741708 23128000471168 run.py:483] Algo bellman_ford step 5701 current loss 0.010538, current_train_items 182464.
I0304 19:30:50.765883 23128000471168 run.py:483] Algo bellman_ford step 5702 current loss 0.019117, current_train_items 182496.
I0304 19:30:50.798430 23128000471168 run.py:483] Algo bellman_ford step 5703 current loss 0.031523, current_train_items 182528.
I0304 19:30:50.834349 23128000471168 run.py:483] Algo bellman_ford step 5704 current loss 0.049426, current_train_items 182560.
I0304 19:30:50.855254 23128000471168 run.py:483] Algo bellman_ford step 5705 current loss 0.001805, current_train_items 182592.
I0304 19:30:50.871317 23128000471168 run.py:483] Algo bellman_ford step 5706 current loss 0.009895, current_train_items 182624.
I0304 19:30:50.896328 23128000471168 run.py:483] Algo bellman_ford step 5707 current loss 0.042331, current_train_items 182656.
I0304 19:30:50.927343 23128000471168 run.py:483] Algo bellman_ford step 5708 current loss 0.053969, current_train_items 182688.
I0304 19:30:50.961875 23128000471168 run.py:483] Algo bellman_ford step 5709 current loss 0.058522, current_train_items 182720.
I0304 19:30:50.982235 23128000471168 run.py:483] Algo bellman_ford step 5710 current loss 0.001711, current_train_items 182752.
I0304 19:30:50.998524 23128000471168 run.py:483] Algo bellman_ford step 5711 current loss 0.009295, current_train_items 182784.
I0304 19:30:51.022405 23128000471168 run.py:483] Algo bellman_ford step 5712 current loss 0.048026, current_train_items 182816.
I0304 19:30:51.053273 23128000471168 run.py:483] Algo bellman_ford step 5713 current loss 0.045445, current_train_items 182848.
I0304 19:30:51.086776 23128000471168 run.py:483] Algo bellman_ford step 5714 current loss 0.040352, current_train_items 182880.
I0304 19:30:51.106762 23128000471168 run.py:483] Algo bellman_ford step 5715 current loss 0.001949, current_train_items 182912.
I0304 19:30:51.123131 23128000471168 run.py:483] Algo bellman_ford step 5716 current loss 0.015845, current_train_items 182944.
I0304 19:30:51.148001 23128000471168 run.py:483] Algo bellman_ford step 5717 current loss 0.040255, current_train_items 182976.
I0304 19:30:51.179339 23128000471168 run.py:483] Algo bellman_ford step 5718 current loss 0.052559, current_train_items 183008.
I0304 19:30:51.213919 23128000471168 run.py:483] Algo bellman_ford step 5719 current loss 0.071462, current_train_items 183040.
I0304 19:30:51.234102 23128000471168 run.py:483] Algo bellman_ford step 5720 current loss 0.002967, current_train_items 183072.
I0304 19:30:51.250610 23128000471168 run.py:483] Algo bellman_ford step 5721 current loss 0.004876, current_train_items 183104.
I0304 19:30:51.274204 23128000471168 run.py:483] Algo bellman_ford step 5722 current loss 0.056188, current_train_items 183136.
I0304 19:30:51.305615 23128000471168 run.py:483] Algo bellman_ford step 5723 current loss 0.097419, current_train_items 183168.
I0304 19:30:51.339033 23128000471168 run.py:483] Algo bellman_ford step 5724 current loss 0.100919, current_train_items 183200.
I0304 19:30:51.359230 23128000471168 run.py:483] Algo bellman_ford step 5725 current loss 0.002477, current_train_items 183232.
I0304 19:30:51.375656 23128000471168 run.py:483] Algo bellman_ford step 5726 current loss 0.011580, current_train_items 183264.
I0304 19:30:51.400618 23128000471168 run.py:483] Algo bellman_ford step 5727 current loss 0.063360, current_train_items 183296.
I0304 19:30:51.431512 23128000471168 run.py:483] Algo bellman_ford step 5728 current loss 0.059342, current_train_items 183328.
I0304 19:30:51.465672 23128000471168 run.py:483] Algo bellman_ford step 5729 current loss 0.043775, current_train_items 183360.
I0304 19:30:51.485491 23128000471168 run.py:483] Algo bellman_ford step 5730 current loss 0.002007, current_train_items 183392.
I0304 19:30:51.502259 23128000471168 run.py:483] Algo bellman_ford step 5731 current loss 0.012857, current_train_items 183424.
I0304 19:30:51.526950 23128000471168 run.py:483] Algo bellman_ford step 5732 current loss 0.034730, current_train_items 183456.
I0304 19:30:51.558241 23128000471168 run.py:483] Algo bellman_ford step 5733 current loss 0.088905, current_train_items 183488.
I0304 19:30:51.593129 23128000471168 run.py:483] Algo bellman_ford step 5734 current loss 0.082587, current_train_items 183520.
I0304 19:30:51.613493 23128000471168 run.py:483] Algo bellman_ford step 5735 current loss 0.002022, current_train_items 183552.
I0304 19:30:51.630436 23128000471168 run.py:483] Algo bellman_ford step 5736 current loss 0.021540, current_train_items 183584.
I0304 19:30:51.654656 23128000471168 run.py:483] Algo bellman_ford step 5737 current loss 0.059211, current_train_items 183616.
I0304 19:30:51.685584 23128000471168 run.py:483] Algo bellman_ford step 5738 current loss 0.060704, current_train_items 183648.
I0304 19:30:51.719896 23128000471168 run.py:483] Algo bellman_ford step 5739 current loss 0.055099, current_train_items 183680.
I0304 19:30:51.739868 23128000471168 run.py:483] Algo bellman_ford step 5740 current loss 0.028239, current_train_items 183712.
I0304 19:30:51.756739 23128000471168 run.py:483] Algo bellman_ford step 5741 current loss 0.012694, current_train_items 183744.
I0304 19:30:51.781757 23128000471168 run.py:483] Algo bellman_ford step 5742 current loss 0.040996, current_train_items 183776.
I0304 19:30:51.814036 23128000471168 run.py:483] Algo bellman_ford step 5743 current loss 0.054576, current_train_items 183808.
I0304 19:30:51.848960 23128000471168 run.py:483] Algo bellman_ford step 5744 current loss 0.071596, current_train_items 183840.
I0304 19:30:51.869001 23128000471168 run.py:483] Algo bellman_ford step 5745 current loss 0.026311, current_train_items 183872.
I0304 19:30:51.885922 23128000471168 run.py:483] Algo bellman_ford step 5746 current loss 0.021735, current_train_items 183904.
I0304 19:30:51.910462 23128000471168 run.py:483] Algo bellman_ford step 5747 current loss 0.048618, current_train_items 183936.
I0304 19:30:51.942379 23128000471168 run.py:483] Algo bellman_ford step 5748 current loss 0.053911, current_train_items 183968.
I0304 19:30:51.976299 23128000471168 run.py:483] Algo bellman_ford step 5749 current loss 0.062749, current_train_items 184000.
I0304 19:30:51.996399 23128000471168 run.py:483] Algo bellman_ford step 5750 current loss 0.003861, current_train_items 184032.
I0304 19:30:52.004460 23128000471168 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0304 19:30:52.004570 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:52.022096 23128000471168 run.py:483] Algo bellman_ford step 5751 current loss 0.013266, current_train_items 184064.
I0304 19:30:52.046810 23128000471168 run.py:483] Algo bellman_ford step 5752 current loss 0.069404, current_train_items 184096.
I0304 19:30:52.078236 23128000471168 run.py:483] Algo bellman_ford step 5753 current loss 0.072553, current_train_items 184128.
I0304 19:30:52.113866 23128000471168 run.py:483] Algo bellman_ford step 5754 current loss 0.108864, current_train_items 184160.
I0304 19:30:52.134608 23128000471168 run.py:483] Algo bellman_ford step 5755 current loss 0.004370, current_train_items 184192.
I0304 19:30:52.150599 23128000471168 run.py:483] Algo bellman_ford step 5756 current loss 0.019460, current_train_items 184224.
I0304 19:30:52.175413 23128000471168 run.py:483] Algo bellman_ford step 5757 current loss 0.061794, current_train_items 184256.
I0304 19:30:52.207983 23128000471168 run.py:483] Algo bellman_ford step 5758 current loss 0.087017, current_train_items 184288.
I0304 19:30:52.241971 23128000471168 run.py:483] Algo bellman_ford step 5759 current loss 0.031767, current_train_items 184320.
I0304 19:30:52.262048 23128000471168 run.py:483] Algo bellman_ford step 5760 current loss 0.022258, current_train_items 184352.
I0304 19:30:52.279111 23128000471168 run.py:483] Algo bellman_ford step 5761 current loss 0.018652, current_train_items 184384.
I0304 19:30:52.303146 23128000471168 run.py:483] Algo bellman_ford step 5762 current loss 0.030342, current_train_items 184416.
I0304 19:30:52.334071 23128000471168 run.py:483] Algo bellman_ford step 5763 current loss 0.055220, current_train_items 184448.
I0304 19:30:52.369016 23128000471168 run.py:483] Algo bellman_ford step 5764 current loss 0.054749, current_train_items 184480.
I0304 19:30:52.389694 23128000471168 run.py:483] Algo bellman_ford step 5765 current loss 0.004102, current_train_items 184512.
I0304 19:30:52.406308 23128000471168 run.py:483] Algo bellman_ford step 5766 current loss 0.011673, current_train_items 184544.
I0304 19:30:52.430512 23128000471168 run.py:483] Algo bellman_ford step 5767 current loss 0.025416, current_train_items 184576.
I0304 19:30:52.463286 23128000471168 run.py:483] Algo bellman_ford step 5768 current loss 0.031469, current_train_items 184608.
I0304 19:30:52.497271 23128000471168 run.py:483] Algo bellman_ford step 5769 current loss 0.074567, current_train_items 184640.
I0304 19:30:52.517798 23128000471168 run.py:483] Algo bellman_ford step 5770 current loss 0.002253, current_train_items 184672.
I0304 19:30:52.534374 23128000471168 run.py:483] Algo bellman_ford step 5771 current loss 0.025223, current_train_items 184704.
I0304 19:30:52.557384 23128000471168 run.py:483] Algo bellman_ford step 5772 current loss 0.034107, current_train_items 184736.
I0304 19:30:52.588398 23128000471168 run.py:483] Algo bellman_ford step 5773 current loss 0.060372, current_train_items 184768.
I0304 19:30:52.621663 23128000471168 run.py:483] Algo bellman_ford step 5774 current loss 0.094041, current_train_items 184800.
I0304 19:30:52.641808 23128000471168 run.py:483] Algo bellman_ford step 5775 current loss 0.002091, current_train_items 184832.
I0304 19:30:52.659034 23128000471168 run.py:483] Algo bellman_ford step 5776 current loss 0.064058, current_train_items 184864.
I0304 19:30:52.682493 23128000471168 run.py:483] Algo bellman_ford step 5777 current loss 0.051815, current_train_items 184896.
I0304 19:30:52.714282 23128000471168 run.py:483] Algo bellman_ford step 5778 current loss 0.055858, current_train_items 184928.
I0304 19:30:52.748985 23128000471168 run.py:483] Algo bellman_ford step 5779 current loss 0.093528, current_train_items 184960.
I0304 19:30:52.768816 23128000471168 run.py:483] Algo bellman_ford step 5780 current loss 0.005963, current_train_items 184992.
I0304 19:30:52.785311 23128000471168 run.py:483] Algo bellman_ford step 5781 current loss 0.014700, current_train_items 185024.
I0304 19:30:52.809998 23128000471168 run.py:483] Algo bellman_ford step 5782 current loss 0.042369, current_train_items 185056.
I0304 19:30:52.840777 23128000471168 run.py:483] Algo bellman_ford step 5783 current loss 0.030131, current_train_items 185088.
I0304 19:30:52.873046 23128000471168 run.py:483] Algo bellman_ford step 5784 current loss 0.081827, current_train_items 185120.
I0304 19:30:52.893317 23128000471168 run.py:483] Algo bellman_ford step 5785 current loss 0.003957, current_train_items 185152.
I0304 19:30:52.909595 23128000471168 run.py:483] Algo bellman_ford step 5786 current loss 0.028155, current_train_items 185184.
I0304 19:30:52.932689 23128000471168 run.py:483] Algo bellman_ford step 5787 current loss 0.041892, current_train_items 185216.
I0304 19:30:52.965278 23128000471168 run.py:483] Algo bellman_ford step 5788 current loss 0.040316, current_train_items 185248.
I0304 19:30:52.998137 23128000471168 run.py:483] Algo bellman_ford step 5789 current loss 0.099284, current_train_items 185280.
I0304 19:30:53.018422 23128000471168 run.py:483] Algo bellman_ford step 5790 current loss 0.008209, current_train_items 185312.
I0304 19:30:53.035351 23128000471168 run.py:483] Algo bellman_ford step 5791 current loss 0.034510, current_train_items 185344.
I0304 19:30:53.058462 23128000471168 run.py:483] Algo bellman_ford step 5792 current loss 0.039584, current_train_items 185376.
I0304 19:30:53.089890 23128000471168 run.py:483] Algo bellman_ford step 5793 current loss 0.046732, current_train_items 185408.
I0304 19:30:53.124213 23128000471168 run.py:483] Algo bellman_ford step 5794 current loss 0.052392, current_train_items 185440.
I0304 19:30:53.144423 23128000471168 run.py:483] Algo bellman_ford step 5795 current loss 0.003116, current_train_items 185472.
I0304 19:30:53.160792 23128000471168 run.py:483] Algo bellman_ford step 5796 current loss 0.007567, current_train_items 185504.
I0304 19:30:53.185533 23128000471168 run.py:483] Algo bellman_ford step 5797 current loss 0.027326, current_train_items 185536.
I0304 19:30:53.217225 23128000471168 run.py:483] Algo bellman_ford step 5798 current loss 0.046390, current_train_items 185568.
I0304 19:30:53.252042 23128000471168 run.py:483] Algo bellman_ford step 5799 current loss 0.071361, current_train_items 185600.
I0304 19:30:53.272079 23128000471168 run.py:483] Algo bellman_ford step 5800 current loss 0.001937, current_train_items 185632.
I0304 19:30:53.279921 23128000471168 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0304 19:30:53.280038 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:53.297338 23128000471168 run.py:483] Algo bellman_ford step 5801 current loss 0.031321, current_train_items 185664.
I0304 19:30:53.320979 23128000471168 run.py:483] Algo bellman_ford step 5802 current loss 0.021832, current_train_items 185696.
I0304 19:30:53.353611 23128000471168 run.py:483] Algo bellman_ford step 5803 current loss 0.045475, current_train_items 185728.
I0304 19:30:53.389462 23128000471168 run.py:483] Algo bellman_ford step 5804 current loss 0.040473, current_train_items 185760.
I0304 19:30:53.409916 23128000471168 run.py:483] Algo bellman_ford step 5805 current loss 0.002799, current_train_items 185792.
I0304 19:30:53.426339 23128000471168 run.py:483] Algo bellman_ford step 5806 current loss 0.008705, current_train_items 185824.
I0304 19:30:53.451439 23128000471168 run.py:483] Algo bellman_ford step 5807 current loss 0.044305, current_train_items 185856.
I0304 19:30:53.483794 23128000471168 run.py:483] Algo bellman_ford step 5808 current loss 0.048341, current_train_items 185888.
I0304 19:30:53.517629 23128000471168 run.py:483] Algo bellman_ford step 5809 current loss 0.083821, current_train_items 185920.
I0304 19:30:53.538048 23128000471168 run.py:483] Algo bellman_ford step 5810 current loss 0.003380, current_train_items 185952.
I0304 19:30:53.554434 23128000471168 run.py:483] Algo bellman_ford step 5811 current loss 0.036323, current_train_items 185984.
I0304 19:30:53.579245 23128000471168 run.py:483] Algo bellman_ford step 5812 current loss 0.034900, current_train_items 186016.
I0304 19:30:53.611784 23128000471168 run.py:483] Algo bellman_ford step 5813 current loss 0.056938, current_train_items 186048.
I0304 19:30:53.646781 23128000471168 run.py:483] Algo bellman_ford step 5814 current loss 0.064712, current_train_items 186080.
I0304 19:30:53.666933 23128000471168 run.py:483] Algo bellman_ford step 5815 current loss 0.002302, current_train_items 186112.
I0304 19:30:53.683587 23128000471168 run.py:483] Algo bellman_ford step 5816 current loss 0.021216, current_train_items 186144.
I0304 19:30:53.708213 23128000471168 run.py:483] Algo bellman_ford step 5817 current loss 0.060222, current_train_items 186176.
I0304 19:30:53.739088 23128000471168 run.py:483] Algo bellman_ford step 5818 current loss 0.031515, current_train_items 186208.
I0304 19:30:53.773514 23128000471168 run.py:483] Algo bellman_ford step 5819 current loss 0.061775, current_train_items 186240.
I0304 19:30:53.793553 23128000471168 run.py:483] Algo bellman_ford step 5820 current loss 0.001670, current_train_items 186272.
I0304 19:30:53.810091 23128000471168 run.py:483] Algo bellman_ford step 5821 current loss 0.004380, current_train_items 186304.
I0304 19:30:53.834567 23128000471168 run.py:483] Algo bellman_ford step 5822 current loss 0.059614, current_train_items 186336.
I0304 19:30:53.866346 23128000471168 run.py:483] Algo bellman_ford step 5823 current loss 0.080489, current_train_items 186368.
I0304 19:30:53.900231 23128000471168 run.py:483] Algo bellman_ford step 5824 current loss 0.099397, current_train_items 186400.
I0304 19:30:53.920143 23128000471168 run.py:483] Algo bellman_ford step 5825 current loss 0.006260, current_train_items 186432.
I0304 19:30:53.936743 23128000471168 run.py:483] Algo bellman_ford step 5826 current loss 0.011465, current_train_items 186464.
I0304 19:30:53.961237 23128000471168 run.py:483] Algo bellman_ford step 5827 current loss 0.060764, current_train_items 186496.
I0304 19:30:53.994026 23128000471168 run.py:483] Algo bellman_ford step 5828 current loss 0.074995, current_train_items 186528.
I0304 19:30:54.029386 23128000471168 run.py:483] Algo bellman_ford step 5829 current loss 0.111561, current_train_items 186560.
I0304 19:30:54.049700 23128000471168 run.py:483] Algo bellman_ford step 5830 current loss 0.003318, current_train_items 186592.
I0304 19:30:54.066439 23128000471168 run.py:483] Algo bellman_ford step 5831 current loss 0.028168, current_train_items 186624.
I0304 19:30:54.090293 23128000471168 run.py:483] Algo bellman_ford step 5832 current loss 0.039140, current_train_items 186656.
I0304 19:30:54.121807 23128000471168 run.py:483] Algo bellman_ford step 5833 current loss 0.049222, current_train_items 186688.
I0304 19:30:54.152620 23128000471168 run.py:483] Algo bellman_ford step 5834 current loss 0.058374, current_train_items 186720.
I0304 19:30:54.172777 23128000471168 run.py:483] Algo bellman_ford step 5835 current loss 0.005925, current_train_items 186752.
I0304 19:30:54.188572 23128000471168 run.py:483] Algo bellman_ford step 5836 current loss 0.011739, current_train_items 186784.
I0304 19:30:54.214443 23128000471168 run.py:483] Algo bellman_ford step 5837 current loss 0.081025, current_train_items 186816.
I0304 19:30:54.244333 23128000471168 run.py:483] Algo bellman_ford step 5838 current loss 0.027191, current_train_items 186848.
I0304 19:30:54.279843 23128000471168 run.py:483] Algo bellman_ford step 5839 current loss 0.054302, current_train_items 186880.
I0304 19:30:54.299945 23128000471168 run.py:483] Algo bellman_ford step 5840 current loss 0.002737, current_train_items 186912.
I0304 19:30:54.316833 23128000471168 run.py:483] Algo bellman_ford step 5841 current loss 0.019764, current_train_items 186944.
I0304 19:30:54.340770 23128000471168 run.py:483] Algo bellman_ford step 5842 current loss 0.059919, current_train_items 186976.
I0304 19:30:54.370956 23128000471168 run.py:483] Algo bellman_ford step 5843 current loss 0.049307, current_train_items 187008.
I0304 19:30:54.404424 23128000471168 run.py:483] Algo bellman_ford step 5844 current loss 0.059700, current_train_items 187040.
I0304 19:30:54.425024 23128000471168 run.py:483] Algo bellman_ford step 5845 current loss 0.009137, current_train_items 187072.
I0304 19:30:54.441597 23128000471168 run.py:483] Algo bellman_ford step 5846 current loss 0.011091, current_train_items 187104.
I0304 19:30:54.466982 23128000471168 run.py:483] Algo bellman_ford step 5847 current loss 0.030097, current_train_items 187136.
I0304 19:30:54.497067 23128000471168 run.py:483] Algo bellman_ford step 5848 current loss 0.044650, current_train_items 187168.
I0304 19:30:54.531696 23128000471168 run.py:483] Algo bellman_ford step 5849 current loss 0.042562, current_train_items 187200.
I0304 19:30:54.551765 23128000471168 run.py:483] Algo bellman_ford step 5850 current loss 0.008262, current_train_items 187232.
I0304 19:30:54.559845 23128000471168 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0304 19:30:54.559952 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.994, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:30:54.589089 23128000471168 run.py:483] Algo bellman_ford step 5851 current loss 0.004755, current_train_items 187264.
I0304 19:30:54.612872 23128000471168 run.py:483] Algo bellman_ford step 5852 current loss 0.062451, current_train_items 187296.
I0304 19:30:54.643931 23128000471168 run.py:483] Algo bellman_ford step 5853 current loss 0.026801, current_train_items 187328.
I0304 19:30:54.678875 23128000471168 run.py:483] Algo bellman_ford step 5854 current loss 0.056936, current_train_items 187360.
I0304 19:30:54.698958 23128000471168 run.py:483] Algo bellman_ford step 5855 current loss 0.001933, current_train_items 187392.
I0304 19:30:54.715170 23128000471168 run.py:483] Algo bellman_ford step 5856 current loss 0.014822, current_train_items 187424.
I0304 19:30:54.739682 23128000471168 run.py:483] Algo bellman_ford step 5857 current loss 0.038675, current_train_items 187456.
I0304 19:30:54.770096 23128000471168 run.py:483] Algo bellman_ford step 5858 current loss 0.027728, current_train_items 187488.
I0304 19:30:54.804031 23128000471168 run.py:483] Algo bellman_ford step 5859 current loss 0.061972, current_train_items 187520.
I0304 19:30:54.824354 23128000471168 run.py:483] Algo bellman_ford step 5860 current loss 0.002032, current_train_items 187552.
I0304 19:30:54.841155 23128000471168 run.py:483] Algo bellman_ford step 5861 current loss 0.008580, current_train_items 187584.
I0304 19:30:54.864276 23128000471168 run.py:483] Algo bellman_ford step 5862 current loss 0.037821, current_train_items 187616.
I0304 19:30:54.895949 23128000471168 run.py:483] Algo bellman_ford step 5863 current loss 0.037484, current_train_items 187648.
I0304 19:30:54.930358 23128000471168 run.py:483] Algo bellman_ford step 5864 current loss 0.070613, current_train_items 187680.
I0304 19:30:54.950458 23128000471168 run.py:483] Algo bellman_ford step 5865 current loss 0.001782, current_train_items 187712.
I0304 19:30:54.967555 23128000471168 run.py:483] Algo bellman_ford step 5866 current loss 0.023588, current_train_items 187744.
I0304 19:30:54.991663 23128000471168 run.py:483] Algo bellman_ford step 5867 current loss 0.028189, current_train_items 187776.
I0304 19:30:55.022520 23128000471168 run.py:483] Algo bellman_ford step 5868 current loss 0.069981, current_train_items 187808.
I0304 19:30:55.057875 23128000471168 run.py:483] Algo bellman_ford step 5869 current loss 0.047331, current_train_items 187840.
I0304 19:30:55.078374 23128000471168 run.py:483] Algo bellman_ford step 5870 current loss 0.001919, current_train_items 187872.
I0304 19:30:55.094952 23128000471168 run.py:483] Algo bellman_ford step 5871 current loss 0.011589, current_train_items 187904.
I0304 19:30:55.117732 23128000471168 run.py:483] Algo bellman_ford step 5872 current loss 0.027274, current_train_items 187936.
I0304 19:30:55.148639 23128000471168 run.py:483] Algo bellman_ford step 5873 current loss 0.047209, current_train_items 187968.
I0304 19:30:55.182741 23128000471168 run.py:483] Algo bellman_ford step 5874 current loss 0.079026, current_train_items 188000.
I0304 19:30:55.202849 23128000471168 run.py:483] Algo bellman_ford step 5875 current loss 0.002143, current_train_items 188032.
I0304 19:30:55.219392 23128000471168 run.py:483] Algo bellman_ford step 5876 current loss 0.023661, current_train_items 188064.
I0304 19:30:55.242259 23128000471168 run.py:483] Algo bellman_ford step 5877 current loss 0.025608, current_train_items 188096.
I0304 19:30:55.273291 23128000471168 run.py:483] Algo bellman_ford step 5878 current loss 0.042997, current_train_items 188128.
I0304 19:30:55.307725 23128000471168 run.py:483] Algo bellman_ford step 5879 current loss 0.038261, current_train_items 188160.
I0304 19:30:55.327722 23128000471168 run.py:483] Algo bellman_ford step 5880 current loss 0.007450, current_train_items 188192.
I0304 19:30:55.343873 23128000471168 run.py:483] Algo bellman_ford step 5881 current loss 0.026989, current_train_items 188224.
I0304 19:30:55.368387 23128000471168 run.py:483] Algo bellman_ford step 5882 current loss 0.104259, current_train_items 188256.
I0304 19:30:55.399460 23128000471168 run.py:483] Algo bellman_ford step 5883 current loss 0.049712, current_train_items 188288.
I0304 19:30:55.434187 23128000471168 run.py:483] Algo bellman_ford step 5884 current loss 0.170142, current_train_items 188320.
I0304 19:30:55.454543 23128000471168 run.py:483] Algo bellman_ford step 5885 current loss 0.002975, current_train_items 188352.
I0304 19:30:55.470961 23128000471168 run.py:483] Algo bellman_ford step 5886 current loss 0.017606, current_train_items 188384.
I0304 19:30:55.494381 23128000471168 run.py:483] Algo bellman_ford step 5887 current loss 0.034094, current_train_items 188416.
I0304 19:30:55.525163 23128000471168 run.py:483] Algo bellman_ford step 5888 current loss 0.041980, current_train_items 188448.
I0304 19:30:55.557476 23128000471168 run.py:483] Algo bellman_ford step 5889 current loss 0.136780, current_train_items 188480.
I0304 19:30:55.577651 23128000471168 run.py:483] Algo bellman_ford step 5890 current loss 0.002833, current_train_items 188512.
I0304 19:30:55.594212 23128000471168 run.py:483] Algo bellman_ford step 5891 current loss 0.055941, current_train_items 188544.
I0304 19:30:55.617950 23128000471168 run.py:483] Algo bellman_ford step 5892 current loss 0.040476, current_train_items 188576.
I0304 19:30:55.648232 23128000471168 run.py:483] Algo bellman_ford step 5893 current loss 0.021691, current_train_items 188608.
I0304 19:30:55.679627 23128000471168 run.py:483] Algo bellman_ford step 5894 current loss 0.053933, current_train_items 188640.
I0304 19:30:55.699451 23128000471168 run.py:483] Algo bellman_ford step 5895 current loss 0.002909, current_train_items 188672.
I0304 19:30:55.716103 23128000471168 run.py:483] Algo bellman_ford step 5896 current loss 0.049599, current_train_items 188704.
I0304 19:30:55.740820 23128000471168 run.py:483] Algo bellman_ford step 5897 current loss 0.063463, current_train_items 188736.
I0304 19:30:55.772660 23128000471168 run.py:483] Algo bellman_ford step 5898 current loss 0.036967, current_train_items 188768.
I0304 19:30:55.806104 23128000471168 run.py:483] Algo bellman_ford step 5899 current loss 0.036401, current_train_items 188800.
I0304 19:30:55.826389 23128000471168 run.py:483] Algo bellman_ford step 5900 current loss 0.002499, current_train_items 188832.
I0304 19:30:55.834334 23128000471168 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0304 19:30:55.834441 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:30:55.851331 23128000471168 run.py:483] Algo bellman_ford step 5901 current loss 0.021210, current_train_items 188864.
I0304 19:30:55.876763 23128000471168 run.py:483] Algo bellman_ford step 5902 current loss 0.024991, current_train_items 188896.
I0304 19:30:55.909133 23128000471168 run.py:483] Algo bellman_ford step 5903 current loss 0.065062, current_train_items 188928.
I0304 19:30:55.941872 23128000471168 run.py:483] Algo bellman_ford step 5904 current loss 0.050880, current_train_items 188960.
I0304 19:30:55.962172 23128000471168 run.py:483] Algo bellman_ford step 5905 current loss 0.004666, current_train_items 188992.
I0304 19:30:55.978784 23128000471168 run.py:483] Algo bellman_ford step 5906 current loss 0.013319, current_train_items 189024.
I0304 19:30:56.002559 23128000471168 run.py:483] Algo bellman_ford step 5907 current loss 0.051400, current_train_items 189056.
I0304 19:30:56.034291 23128000471168 run.py:483] Algo bellman_ford step 5908 current loss 0.083793, current_train_items 189088.
I0304 19:30:56.066969 23128000471168 run.py:483] Algo bellman_ford step 5909 current loss 0.139892, current_train_items 189120.
I0304 19:30:56.087213 23128000471168 run.py:483] Algo bellman_ford step 5910 current loss 0.004019, current_train_items 189152.
I0304 19:30:56.103900 23128000471168 run.py:483] Algo bellman_ford step 5911 current loss 0.032809, current_train_items 189184.
I0304 19:30:56.128188 23128000471168 run.py:483] Algo bellman_ford step 5912 current loss 0.064862, current_train_items 189216.
I0304 19:30:56.158969 23128000471168 run.py:483] Algo bellman_ford step 5913 current loss 0.061489, current_train_items 189248.
I0304 19:30:56.192112 23128000471168 run.py:483] Algo bellman_ford step 5914 current loss 0.120595, current_train_items 189280.
I0304 19:30:56.212403 23128000471168 run.py:483] Algo bellman_ford step 5915 current loss 0.004083, current_train_items 189312.
I0304 19:30:56.228967 23128000471168 run.py:483] Algo bellman_ford step 5916 current loss 0.014666, current_train_items 189344.
I0304 19:30:56.252944 23128000471168 run.py:483] Algo bellman_ford step 5917 current loss 0.032377, current_train_items 189376.
I0304 19:30:56.283841 23128000471168 run.py:483] Algo bellman_ford step 5918 current loss 0.027693, current_train_items 189408.
I0304 19:30:56.318312 23128000471168 run.py:483] Algo bellman_ford step 5919 current loss 0.034794, current_train_items 189440.
I0304 19:30:56.338581 23128000471168 run.py:483] Algo bellman_ford step 5920 current loss 0.003142, current_train_items 189472.
I0304 19:30:56.355087 23128000471168 run.py:483] Algo bellman_ford step 5921 current loss 0.013725, current_train_items 189504.
I0304 19:30:56.378777 23128000471168 run.py:483] Algo bellman_ford step 5922 current loss 0.032398, current_train_items 189536.
I0304 19:30:56.408485 23128000471168 run.py:483] Algo bellman_ford step 5923 current loss 0.026985, current_train_items 189568.
I0304 19:30:56.442188 23128000471168 run.py:483] Algo bellman_ford step 5924 current loss 0.056595, current_train_items 189600.
I0304 19:30:56.462233 23128000471168 run.py:483] Algo bellman_ford step 5925 current loss 0.001342, current_train_items 189632.
I0304 19:30:56.478646 23128000471168 run.py:483] Algo bellman_ford step 5926 current loss 0.015001, current_train_items 189664.
I0304 19:30:56.502098 23128000471168 run.py:483] Algo bellman_ford step 5927 current loss 0.023562, current_train_items 189696.
I0304 19:30:56.533881 23128000471168 run.py:483] Algo bellman_ford step 5928 current loss 0.047536, current_train_items 189728.
I0304 19:30:56.566629 23128000471168 run.py:483] Algo bellman_ford step 5929 current loss 0.031754, current_train_items 189760.
I0304 19:30:56.586970 23128000471168 run.py:483] Algo bellman_ford step 5930 current loss 0.001380, current_train_items 189792.
I0304 19:30:56.603544 23128000471168 run.py:483] Algo bellman_ford step 5931 current loss 0.009552, current_train_items 189824.
I0304 19:30:56.627452 23128000471168 run.py:483] Algo bellman_ford step 5932 current loss 0.051347, current_train_items 189856.
I0304 19:30:56.657827 23128000471168 run.py:483] Algo bellman_ford step 5933 current loss 0.028146, current_train_items 189888.
I0304 19:30:56.692842 23128000471168 run.py:483] Algo bellman_ford step 5934 current loss 0.056241, current_train_items 189920.
I0304 19:30:56.713192 23128000471168 run.py:483] Algo bellman_ford step 5935 current loss 0.007075, current_train_items 189952.
I0304 19:30:56.729212 23128000471168 run.py:483] Algo bellman_ford step 5936 current loss 0.034336, current_train_items 189984.
I0304 19:30:56.753217 23128000471168 run.py:483] Algo bellman_ford step 5937 current loss 0.072095, current_train_items 190016.
I0304 19:30:56.785240 23128000471168 run.py:483] Algo bellman_ford step 5938 current loss 0.063642, current_train_items 190048.
I0304 19:30:56.817629 23128000471168 run.py:483] Algo bellman_ford step 5939 current loss 0.068526, current_train_items 190080.
I0304 19:30:56.837831 23128000471168 run.py:483] Algo bellman_ford step 5940 current loss 0.001571, current_train_items 190112.
I0304 19:30:56.854202 23128000471168 run.py:483] Algo bellman_ford step 5941 current loss 0.017049, current_train_items 190144.
I0304 19:30:56.878561 23128000471168 run.py:483] Algo bellman_ford step 5942 current loss 0.031558, current_train_items 190176.
I0304 19:30:56.910175 23128000471168 run.py:483] Algo bellman_ford step 5943 current loss 0.044094, current_train_items 190208.
I0304 19:30:56.944402 23128000471168 run.py:483] Algo bellman_ford step 5944 current loss 0.075997, current_train_items 190240.
I0304 19:30:56.964576 23128000471168 run.py:483] Algo bellman_ford step 5945 current loss 0.001958, current_train_items 190272.
I0304 19:30:56.981248 23128000471168 run.py:483] Algo bellman_ford step 5946 current loss 0.013226, current_train_items 190304.
I0304 19:30:57.005741 23128000471168 run.py:483] Algo bellman_ford step 5947 current loss 0.032231, current_train_items 190336.
I0304 19:30:57.035991 23128000471168 run.py:483] Algo bellman_ford step 5948 current loss 0.031549, current_train_items 190368.
I0304 19:30:57.070296 23128000471168 run.py:483] Algo bellman_ford step 5949 current loss 0.067353, current_train_items 190400.
I0304 19:30:57.090040 23128000471168 run.py:483] Algo bellman_ford step 5950 current loss 0.002200, current_train_items 190432.
I0304 19:30:57.098033 23128000471168 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0304 19:30:57.098144 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:30:57.115232 23128000471168 run.py:483] Algo bellman_ford step 5951 current loss 0.005751, current_train_items 190464.
I0304 19:30:57.140345 23128000471168 run.py:483] Algo bellman_ford step 5952 current loss 0.024078, current_train_items 190496.
I0304 19:30:57.172499 23128000471168 run.py:483] Algo bellman_ford step 5953 current loss 0.036752, current_train_items 190528.
I0304 19:30:57.206990 23128000471168 run.py:483] Algo bellman_ford step 5954 current loss 0.036226, current_train_items 190560.
I0304 19:30:57.227693 23128000471168 run.py:483] Algo bellman_ford step 5955 current loss 0.002303, current_train_items 190592.
I0304 19:30:57.244384 23128000471168 run.py:483] Algo bellman_ford step 5956 current loss 0.009861, current_train_items 190624.
I0304 19:30:57.268114 23128000471168 run.py:483] Algo bellman_ford step 5957 current loss 0.037779, current_train_items 190656.
I0304 19:30:57.299284 23128000471168 run.py:483] Algo bellman_ford step 5958 current loss 0.014090, current_train_items 190688.
I0304 19:30:57.332896 23128000471168 run.py:483] Algo bellman_ford step 5959 current loss 0.048136, current_train_items 190720.
I0304 19:30:57.353863 23128000471168 run.py:483] Algo bellman_ford step 5960 current loss 0.003184, current_train_items 190752.
I0304 19:30:57.370705 23128000471168 run.py:483] Algo bellman_ford step 5961 current loss 0.016269, current_train_items 190784.
I0304 19:30:57.394983 23128000471168 run.py:483] Algo bellman_ford step 5962 current loss 0.039604, current_train_items 190816.
I0304 19:30:57.424998 23128000471168 run.py:483] Algo bellman_ford step 5963 current loss 0.042734, current_train_items 190848.
I0304 19:30:57.458635 23128000471168 run.py:483] Algo bellman_ford step 5964 current loss 0.066494, current_train_items 190880.
I0304 19:30:57.478728 23128000471168 run.py:483] Algo bellman_ford step 5965 current loss 0.017066, current_train_items 190912.
I0304 19:30:57.495022 23128000471168 run.py:483] Algo bellman_ford step 5966 current loss 0.005103, current_train_items 190944.
I0304 19:30:57.519916 23128000471168 run.py:483] Algo bellman_ford step 5967 current loss 0.048045, current_train_items 190976.
I0304 19:30:57.550878 23128000471168 run.py:483] Algo bellman_ford step 5968 current loss 0.090673, current_train_items 191008.
I0304 19:30:57.585541 23128000471168 run.py:483] Algo bellman_ford step 5969 current loss 0.088498, current_train_items 191040.
I0304 19:30:57.606097 23128000471168 run.py:483] Algo bellman_ford step 5970 current loss 0.002755, current_train_items 191072.
I0304 19:30:57.622797 23128000471168 run.py:483] Algo bellman_ford step 5971 current loss 0.015595, current_train_items 191104.
I0304 19:30:57.646517 23128000471168 run.py:483] Algo bellman_ford step 5972 current loss 0.025860, current_train_items 191136.
I0304 19:30:57.678501 23128000471168 run.py:483] Algo bellman_ford step 5973 current loss 0.087169, current_train_items 191168.
I0304 19:30:57.711880 23128000471168 run.py:483] Algo bellman_ford step 5974 current loss 0.070433, current_train_items 191200.
I0304 19:30:57.732316 23128000471168 run.py:483] Algo bellman_ford step 5975 current loss 0.005118, current_train_items 191232.
I0304 19:30:57.748575 23128000471168 run.py:483] Algo bellman_ford step 5976 current loss 0.004672, current_train_items 191264.
I0304 19:30:57.771864 23128000471168 run.py:483] Algo bellman_ford step 5977 current loss 0.051053, current_train_items 191296.
I0304 19:30:57.803652 23128000471168 run.py:483] Algo bellman_ford step 5978 current loss 0.095321, current_train_items 191328.
I0304 19:30:57.836184 23128000471168 run.py:483] Algo bellman_ford step 5979 current loss 0.038494, current_train_items 191360.
I0304 19:30:57.856063 23128000471168 run.py:483] Algo bellman_ford step 5980 current loss 0.002915, current_train_items 191392.
I0304 19:30:57.873172 23128000471168 run.py:483] Algo bellman_ford step 5981 current loss 0.032682, current_train_items 191424.
I0304 19:30:57.896766 23128000471168 run.py:483] Algo bellman_ford step 5982 current loss 0.015541, current_train_items 191456.
I0304 19:30:57.927964 23128000471168 run.py:483] Algo bellman_ford step 5983 current loss 0.028031, current_train_items 191488.
I0304 19:30:57.960863 23128000471168 run.py:483] Algo bellman_ford step 5984 current loss 0.079509, current_train_items 191520.
I0304 19:30:57.981258 23128000471168 run.py:483] Algo bellman_ford step 5985 current loss 0.006826, current_train_items 191552.
I0304 19:30:57.998410 23128000471168 run.py:483] Algo bellman_ford step 5986 current loss 0.018602, current_train_items 191584.
I0304 19:30:58.022415 23128000471168 run.py:483] Algo bellman_ford step 5987 current loss 0.036259, current_train_items 191616.
I0304 19:30:58.052882 23128000471168 run.py:483] Algo bellman_ford step 5988 current loss 0.041997, current_train_items 191648.
I0304 19:30:58.086374 23128000471168 run.py:483] Algo bellman_ford step 5989 current loss 0.060942, current_train_items 191680.
I0304 19:30:58.106919 23128000471168 run.py:483] Algo bellman_ford step 5990 current loss 0.002004, current_train_items 191712.
I0304 19:30:58.122911 23128000471168 run.py:483] Algo bellman_ford step 5991 current loss 0.028374, current_train_items 191744.
I0304 19:30:58.147040 23128000471168 run.py:483] Algo bellman_ford step 5992 current loss 0.050994, current_train_items 191776.
I0304 19:30:58.180334 23128000471168 run.py:483] Algo bellman_ford step 5993 current loss 0.091116, current_train_items 191808.
I0304 19:30:58.214028 23128000471168 run.py:483] Algo bellman_ford step 5994 current loss 0.064003, current_train_items 191840.
I0304 19:30:58.233968 23128000471168 run.py:483] Algo bellman_ford step 5995 current loss 0.002962, current_train_items 191872.
I0304 19:30:58.250326 23128000471168 run.py:483] Algo bellman_ford step 5996 current loss 0.009070, current_train_items 191904.
I0304 19:30:58.274858 23128000471168 run.py:483] Algo bellman_ford step 5997 current loss 0.023893, current_train_items 191936.
I0304 19:30:58.306250 23128000471168 run.py:483] Algo bellman_ford step 5998 current loss 0.032650, current_train_items 191968.
I0304 19:30:58.339179 23128000471168 run.py:483] Algo bellman_ford step 5999 current loss 0.037919, current_train_items 192000.
I0304 19:30:58.359985 23128000471168 run.py:483] Algo bellman_ford step 6000 current loss 0.009486, current_train_items 192032.
I0304 19:30:58.367772 23128000471168 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0304 19:30:58.367882 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:58.385095 23128000471168 run.py:483] Algo bellman_ford step 6001 current loss 0.017186, current_train_items 192064.
I0304 19:30:58.409636 23128000471168 run.py:483] Algo bellman_ford step 6002 current loss 0.054408, current_train_items 192096.
I0304 19:30:58.440186 23128000471168 run.py:483] Algo bellman_ford step 6003 current loss 0.065525, current_train_items 192128.
I0304 19:30:58.473945 23128000471168 run.py:483] Algo bellman_ford step 6004 current loss 0.050521, current_train_items 192160.
I0304 19:30:58.494529 23128000471168 run.py:483] Algo bellman_ford step 6005 current loss 0.002664, current_train_items 192192.
I0304 19:30:58.510672 23128000471168 run.py:483] Algo bellman_ford step 6006 current loss 0.016094, current_train_items 192224.
I0304 19:30:58.535671 23128000471168 run.py:483] Algo bellman_ford step 6007 current loss 0.069892, current_train_items 192256.
I0304 19:30:58.568900 23128000471168 run.py:483] Algo bellman_ford step 6008 current loss 0.117092, current_train_items 192288.
I0304 19:30:58.604222 23128000471168 run.py:483] Algo bellman_ford step 6009 current loss 0.049708, current_train_items 192320.
I0304 19:30:58.624019 23128000471168 run.py:483] Algo bellman_ford step 6010 current loss 0.002183, current_train_items 192352.
I0304 19:30:58.640709 23128000471168 run.py:483] Algo bellman_ford step 6011 current loss 0.015056, current_train_items 192384.
I0304 19:30:58.664977 23128000471168 run.py:483] Algo bellman_ford step 6012 current loss 0.023665, current_train_items 192416.
I0304 19:30:58.695515 23128000471168 run.py:483] Algo bellman_ford step 6013 current loss 0.029599, current_train_items 192448.
I0304 19:30:58.731118 23128000471168 run.py:483] Algo bellman_ford step 6014 current loss 0.059180, current_train_items 192480.
I0304 19:30:58.751335 23128000471168 run.py:483] Algo bellman_ford step 6015 current loss 0.006787, current_train_items 192512.
I0304 19:30:58.768040 23128000471168 run.py:483] Algo bellman_ford step 6016 current loss 0.008892, current_train_items 192544.
I0304 19:30:58.791801 23128000471168 run.py:483] Algo bellman_ford step 6017 current loss 0.025696, current_train_items 192576.
I0304 19:30:58.823806 23128000471168 run.py:483] Algo bellman_ford step 6018 current loss 0.081500, current_train_items 192608.
I0304 19:30:58.857797 23128000471168 run.py:483] Algo bellman_ford step 6019 current loss 0.061590, current_train_items 192640.
I0304 19:30:58.878161 23128000471168 run.py:483] Algo bellman_ford step 6020 current loss 0.001927, current_train_items 192672.
I0304 19:30:58.894984 23128000471168 run.py:483] Algo bellman_ford step 6021 current loss 0.006002, current_train_items 192704.
I0304 19:30:58.919628 23128000471168 run.py:483] Algo bellman_ford step 6022 current loss 0.030979, current_train_items 192736.
I0304 19:30:58.952585 23128000471168 run.py:483] Algo bellman_ford step 6023 current loss 0.088513, current_train_items 192768.
I0304 19:30:58.986816 23128000471168 run.py:483] Algo bellman_ford step 6024 current loss 0.080226, current_train_items 192800.
I0304 19:30:59.006613 23128000471168 run.py:483] Algo bellman_ford step 6025 current loss 0.003414, current_train_items 192832.
I0304 19:30:59.023257 23128000471168 run.py:483] Algo bellman_ford step 6026 current loss 0.018964, current_train_items 192864.
I0304 19:30:59.047354 23128000471168 run.py:483] Algo bellman_ford step 6027 current loss 0.028589, current_train_items 192896.
I0304 19:30:59.079085 23128000471168 run.py:483] Algo bellman_ford step 6028 current loss 0.131017, current_train_items 192928.
I0304 19:30:59.112810 23128000471168 run.py:483] Algo bellman_ford step 6029 current loss 0.228637, current_train_items 192960.
I0304 19:30:59.132677 23128000471168 run.py:483] Algo bellman_ford step 6030 current loss 0.003354, current_train_items 192992.
I0304 19:30:59.148914 23128000471168 run.py:483] Algo bellman_ford step 6031 current loss 0.036303, current_train_items 193024.
I0304 19:30:59.172967 23128000471168 run.py:483] Algo bellman_ford step 6032 current loss 0.024819, current_train_items 193056.
I0304 19:30:59.205863 23128000471168 run.py:483] Algo bellman_ford step 6033 current loss 0.050686, current_train_items 193088.
I0304 19:30:59.239464 23128000471168 run.py:483] Algo bellman_ford step 6034 current loss 0.051241, current_train_items 193120.
I0304 19:30:59.259256 23128000471168 run.py:483] Algo bellman_ford step 6035 current loss 0.002493, current_train_items 193152.
I0304 19:30:59.275647 23128000471168 run.py:483] Algo bellman_ford step 6036 current loss 0.024011, current_train_items 193184.
I0304 19:30:59.300088 23128000471168 run.py:483] Algo bellman_ford step 6037 current loss 0.028171, current_train_items 193216.
I0304 19:30:59.332301 23128000471168 run.py:483] Algo bellman_ford step 6038 current loss 0.112915, current_train_items 193248.
I0304 19:30:59.365239 23128000471168 run.py:483] Algo bellman_ford step 6039 current loss 0.044855, current_train_items 193280.
I0304 19:30:59.385090 23128000471168 run.py:483] Algo bellman_ford step 6040 current loss 0.003911, current_train_items 193312.
I0304 19:30:59.401587 23128000471168 run.py:483] Algo bellman_ford step 6041 current loss 0.037785, current_train_items 193344.
I0304 19:30:59.426717 23128000471168 run.py:483] Algo bellman_ford step 6042 current loss 0.047543, current_train_items 193376.
I0304 19:30:59.458223 23128000471168 run.py:483] Algo bellman_ford step 6043 current loss 0.069136, current_train_items 193408.
I0304 19:30:59.495174 23128000471168 run.py:483] Algo bellman_ford step 6044 current loss 0.077874, current_train_items 193440.
I0304 19:30:59.515389 23128000471168 run.py:483] Algo bellman_ford step 6045 current loss 0.002750, current_train_items 193472.
I0304 19:30:59.532118 23128000471168 run.py:483] Algo bellman_ford step 6046 current loss 0.022210, current_train_items 193504.
I0304 19:30:59.556056 23128000471168 run.py:483] Algo bellman_ford step 6047 current loss 0.042332, current_train_items 193536.
I0304 19:30:59.588829 23128000471168 run.py:483] Algo bellman_ford step 6048 current loss 0.049546, current_train_items 193568.
I0304 19:30:59.623989 23128000471168 run.py:483] Algo bellman_ford step 6049 current loss 0.035833, current_train_items 193600.
I0304 19:30:59.643917 23128000471168 run.py:483] Algo bellman_ford step 6050 current loss 0.004002, current_train_items 193632.
I0304 19:30:59.652014 23128000471168 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0304 19:30:59.652151 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:59.669215 23128000471168 run.py:483] Algo bellman_ford step 6051 current loss 0.009318, current_train_items 193664.
I0304 19:30:59.694258 23128000471168 run.py:483] Algo bellman_ford step 6052 current loss 0.044902, current_train_items 193696.
I0304 19:30:59.725687 23128000471168 run.py:483] Algo bellman_ford step 6053 current loss 0.033771, current_train_items 193728.
I0304 19:30:59.760818 23128000471168 run.py:483] Algo bellman_ford step 6054 current loss 0.044846, current_train_items 193760.
I0304 19:30:59.781180 23128000471168 run.py:483] Algo bellman_ford step 6055 current loss 0.020609, current_train_items 193792.
I0304 19:30:59.797157 23128000471168 run.py:483] Algo bellman_ford step 6056 current loss 0.029974, current_train_items 193824.
I0304 19:30:59.820488 23128000471168 run.py:483] Algo bellman_ford step 6057 current loss 0.070204, current_train_items 193856.
I0304 19:30:59.851636 23128000471168 run.py:483] Algo bellman_ford step 6058 current loss 0.043596, current_train_items 193888.
I0304 19:30:59.882768 23128000471168 run.py:483] Algo bellman_ford step 6059 current loss 0.038881, current_train_items 193920.
I0304 19:30:59.903234 23128000471168 run.py:483] Algo bellman_ford step 6060 current loss 0.002264, current_train_items 193952.
I0304 19:30:59.919834 23128000471168 run.py:483] Algo bellman_ford step 6061 current loss 0.016213, current_train_items 193984.
I0304 19:30:59.942476 23128000471168 run.py:483] Algo bellman_ford step 6062 current loss 0.034819, current_train_items 194016.
I0304 19:30:59.972449 23128000471168 run.py:483] Algo bellman_ford step 6063 current loss 0.047952, current_train_items 194048.
I0304 19:31:00.006605 23128000471168 run.py:483] Algo bellman_ford step 6064 current loss 0.096508, current_train_items 194080.
I0304 19:31:00.026488 23128000471168 run.py:483] Algo bellman_ford step 6065 current loss 0.004868, current_train_items 194112.
I0304 19:31:00.043072 23128000471168 run.py:483] Algo bellman_ford step 6066 current loss 0.023948, current_train_items 194144.
I0304 19:31:00.067658 23128000471168 run.py:483] Algo bellman_ford step 6067 current loss 0.048483, current_train_items 194176.
I0304 19:31:00.099484 23128000471168 run.py:483] Algo bellman_ford step 6068 current loss 0.075484, current_train_items 194208.
I0304 19:31:00.132392 23128000471168 run.py:483] Algo bellman_ford step 6069 current loss 0.061798, current_train_items 194240.
I0304 19:31:00.152654 23128000471168 run.py:483] Algo bellman_ford step 6070 current loss 0.003574, current_train_items 194272.
I0304 19:31:00.169021 23128000471168 run.py:483] Algo bellman_ford step 6071 current loss 0.012104, current_train_items 194304.
I0304 19:31:00.192847 23128000471168 run.py:483] Algo bellman_ford step 6072 current loss 0.042450, current_train_items 194336.
I0304 19:31:00.223653 23128000471168 run.py:483] Algo bellman_ford step 6073 current loss 0.081673, current_train_items 194368.
I0304 19:31:00.257796 23128000471168 run.py:483] Algo bellman_ford step 6074 current loss 0.046417, current_train_items 194400.
I0304 19:31:00.277865 23128000471168 run.py:483] Algo bellman_ford step 6075 current loss 0.002862, current_train_items 194432.
I0304 19:31:00.294427 23128000471168 run.py:483] Algo bellman_ford step 6076 current loss 0.020852, current_train_items 194464.
I0304 19:31:00.316965 23128000471168 run.py:483] Algo bellman_ford step 6077 current loss 0.021204, current_train_items 194496.
I0304 19:31:00.347830 23128000471168 run.py:483] Algo bellman_ford step 6078 current loss 0.067372, current_train_items 194528.
I0304 19:31:00.382506 23128000471168 run.py:483] Algo bellman_ford step 6079 current loss 0.117296, current_train_items 194560.
I0304 19:31:00.402306 23128000471168 run.py:483] Algo bellman_ford step 6080 current loss 0.004520, current_train_items 194592.
I0304 19:31:00.418842 23128000471168 run.py:483] Algo bellman_ford step 6081 current loss 0.005940, current_train_items 194624.
I0304 19:31:00.442506 23128000471168 run.py:483] Algo bellman_ford step 6082 current loss 0.020579, current_train_items 194656.
I0304 19:31:00.475039 23128000471168 run.py:483] Algo bellman_ford step 6083 current loss 0.051822, current_train_items 194688.
I0304 19:31:00.507770 23128000471168 run.py:483] Algo bellman_ford step 6084 current loss 0.053680, current_train_items 194720.
I0304 19:31:00.527896 23128000471168 run.py:483] Algo bellman_ford step 6085 current loss 0.006104, current_train_items 194752.
I0304 19:31:00.544649 23128000471168 run.py:483] Algo bellman_ford step 6086 current loss 0.016799, current_train_items 194784.
I0304 19:31:00.568063 23128000471168 run.py:483] Algo bellman_ford step 6087 current loss 0.054049, current_train_items 194816.
I0304 19:31:00.598668 23128000471168 run.py:483] Algo bellman_ford step 6088 current loss 0.056565, current_train_items 194848.
I0304 19:31:00.633141 23128000471168 run.py:483] Algo bellman_ford step 6089 current loss 0.126451, current_train_items 194880.
I0304 19:31:00.653353 23128000471168 run.py:483] Algo bellman_ford step 6090 current loss 0.009655, current_train_items 194912.
I0304 19:31:00.670019 23128000471168 run.py:483] Algo bellman_ford step 6091 current loss 0.015600, current_train_items 194944.
I0304 19:31:00.693463 23128000471168 run.py:483] Algo bellman_ford step 6092 current loss 0.083890, current_train_items 194976.
I0304 19:31:00.724219 23128000471168 run.py:483] Algo bellman_ford step 6093 current loss 0.080325, current_train_items 195008.
I0304 19:31:00.756716 23128000471168 run.py:483] Algo bellman_ford step 6094 current loss 0.110096, current_train_items 195040.
I0304 19:31:00.776611 23128000471168 run.py:483] Algo bellman_ford step 6095 current loss 0.001843, current_train_items 195072.
I0304 19:31:00.793245 23128000471168 run.py:483] Algo bellman_ford step 6096 current loss 0.017203, current_train_items 195104.
I0304 19:31:00.817566 23128000471168 run.py:483] Algo bellman_ford step 6097 current loss 0.045816, current_train_items 195136.
I0304 19:31:00.848060 23128000471168 run.py:483] Algo bellman_ford step 6098 current loss 0.044090, current_train_items 195168.
I0304 19:31:00.882167 23128000471168 run.py:483] Algo bellman_ford step 6099 current loss 0.065799, current_train_items 195200.
I0304 19:31:00.902393 23128000471168 run.py:483] Algo bellman_ford step 6100 current loss 0.001326, current_train_items 195232.
I0304 19:31:00.910149 23128000471168 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0304 19:31:00.910257 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:00.926835 23128000471168 run.py:483] Algo bellman_ford step 6101 current loss 0.009514, current_train_items 195264.
I0304 19:31:00.952234 23128000471168 run.py:483] Algo bellman_ford step 6102 current loss 0.025663, current_train_items 195296.
I0304 19:31:00.985713 23128000471168 run.py:483] Algo bellman_ford step 6103 current loss 0.047340, current_train_items 195328.
I0304 19:31:01.019707 23128000471168 run.py:483] Algo bellman_ford step 6104 current loss 0.048356, current_train_items 195360.
I0304 19:31:01.040361 23128000471168 run.py:483] Algo bellman_ford step 6105 current loss 0.001271, current_train_items 195392.
I0304 19:31:01.056065 23128000471168 run.py:483] Algo bellman_ford step 6106 current loss 0.024075, current_train_items 195424.
I0304 19:31:01.080350 23128000471168 run.py:483] Algo bellman_ford step 6107 current loss 0.046811, current_train_items 195456.
I0304 19:31:01.111962 23128000471168 run.py:483] Algo bellman_ford step 6108 current loss 0.072231, current_train_items 195488.
I0304 19:31:01.145416 23128000471168 run.py:483] Algo bellman_ford step 6109 current loss 0.056600, current_train_items 195520.
I0304 19:31:01.165673 23128000471168 run.py:483] Algo bellman_ford step 6110 current loss 0.002652, current_train_items 195552.
I0304 19:31:01.181985 23128000471168 run.py:483] Algo bellman_ford step 6111 current loss 0.023486, current_train_items 195584.
I0304 19:31:01.206210 23128000471168 run.py:483] Algo bellman_ford step 6112 current loss 0.042669, current_train_items 195616.
I0304 19:31:01.237404 23128000471168 run.py:483] Algo bellman_ford step 6113 current loss 0.053723, current_train_items 195648.
I0304 19:31:01.271615 23128000471168 run.py:483] Algo bellman_ford step 6114 current loss 0.067273, current_train_items 195680.
I0304 19:31:01.292233 23128000471168 run.py:483] Algo bellman_ford step 6115 current loss 0.021756, current_train_items 195712.
I0304 19:31:01.308701 23128000471168 run.py:483] Algo bellman_ford step 6116 current loss 0.022369, current_train_items 195744.
I0304 19:31:01.332973 23128000471168 run.py:483] Algo bellman_ford step 6117 current loss 0.032232, current_train_items 195776.
I0304 19:31:01.365345 23128000471168 run.py:483] Algo bellman_ford step 6118 current loss 0.042206, current_train_items 195808.
I0304 19:31:01.398733 23128000471168 run.py:483] Algo bellman_ford step 6119 current loss 0.049639, current_train_items 195840.
I0304 19:31:01.418900 23128000471168 run.py:483] Algo bellman_ford step 6120 current loss 0.012554, current_train_items 195872.
I0304 19:31:01.435450 23128000471168 run.py:483] Algo bellman_ford step 6121 current loss 0.008630, current_train_items 195904.
I0304 19:31:01.459406 23128000471168 run.py:483] Algo bellman_ford step 6122 current loss 0.026987, current_train_items 195936.
I0304 19:31:01.492053 23128000471168 run.py:483] Algo bellman_ford step 6123 current loss 0.035966, current_train_items 195968.
I0304 19:31:01.526068 23128000471168 run.py:483] Algo bellman_ford step 6124 current loss 0.087267, current_train_items 196000.
I0304 19:31:01.546251 23128000471168 run.py:483] Algo bellman_ford step 6125 current loss 0.001688, current_train_items 196032.
I0304 19:31:01.562170 23128000471168 run.py:483] Algo bellman_ford step 6126 current loss 0.053988, current_train_items 196064.
I0304 19:31:01.588166 23128000471168 run.py:483] Algo bellman_ford step 6127 current loss 0.064643, current_train_items 196096.
I0304 19:31:01.619869 23128000471168 run.py:483] Algo bellman_ford step 6128 current loss 0.050411, current_train_items 196128.
I0304 19:31:01.653136 23128000471168 run.py:483] Algo bellman_ford step 6129 current loss 0.064499, current_train_items 196160.
I0304 19:31:01.673466 23128000471168 run.py:483] Algo bellman_ford step 6130 current loss 0.004318, current_train_items 196192.
I0304 19:31:01.689711 23128000471168 run.py:483] Algo bellman_ford step 6131 current loss 0.010276, current_train_items 196224.
I0304 19:31:01.714495 23128000471168 run.py:483] Algo bellman_ford step 6132 current loss 0.067707, current_train_items 196256.
I0304 19:31:01.745222 23128000471168 run.py:483] Algo bellman_ford step 6133 current loss 0.040304, current_train_items 196288.
I0304 19:31:01.778064 23128000471168 run.py:483] Algo bellman_ford step 6134 current loss 0.054824, current_train_items 196320.
I0304 19:31:01.797931 23128000471168 run.py:483] Algo bellman_ford step 6135 current loss 0.013373, current_train_items 196352.
I0304 19:31:01.814042 23128000471168 run.py:483] Algo bellman_ford step 6136 current loss 0.019693, current_train_items 196384.
I0304 19:31:01.838655 23128000471168 run.py:483] Algo bellman_ford step 6137 current loss 0.032250, current_train_items 196416.
I0304 19:31:01.871179 23128000471168 run.py:483] Algo bellman_ford step 6138 current loss 0.051355, current_train_items 196448.
I0304 19:31:01.905285 23128000471168 run.py:483] Algo bellman_ford step 6139 current loss 0.040980, current_train_items 196480.
I0304 19:31:01.925405 23128000471168 run.py:483] Algo bellman_ford step 6140 current loss 0.003269, current_train_items 196512.
I0304 19:31:01.941850 23128000471168 run.py:483] Algo bellman_ford step 6141 current loss 0.012677, current_train_items 196544.
I0304 19:31:01.966654 23128000471168 run.py:483] Algo bellman_ford step 6142 current loss 0.026072, current_train_items 196576.
I0304 19:31:01.998733 23128000471168 run.py:483] Algo bellman_ford step 6143 current loss 0.039167, current_train_items 196608.
I0304 19:31:02.032082 23128000471168 run.py:483] Algo bellman_ford step 6144 current loss 0.046239, current_train_items 196640.
I0304 19:31:02.052089 23128000471168 run.py:483] Algo bellman_ford step 6145 current loss 0.019513, current_train_items 196672.
I0304 19:31:02.068731 23128000471168 run.py:483] Algo bellman_ford step 6146 current loss 0.012944, current_train_items 196704.
I0304 19:31:02.092665 23128000471168 run.py:483] Algo bellman_ford step 6147 current loss 0.023444, current_train_items 196736.
I0304 19:31:02.123898 23128000471168 run.py:483] Algo bellman_ford step 6148 current loss 0.040016, current_train_items 196768.
I0304 19:31:02.159197 23128000471168 run.py:483] Algo bellman_ford step 6149 current loss 0.079631, current_train_items 196800.
I0304 19:31:02.179888 23128000471168 run.py:483] Algo bellman_ford step 6150 current loss 0.001624, current_train_items 196832.
I0304 19:31:02.188051 23128000471168 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0304 19:31:02.188160 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:02.205256 23128000471168 run.py:483] Algo bellman_ford step 6151 current loss 0.008923, current_train_items 196864.
I0304 19:31:02.228734 23128000471168 run.py:483] Algo bellman_ford step 6152 current loss 0.017097, current_train_items 196896.
I0304 19:31:02.259128 23128000471168 run.py:483] Algo bellman_ford step 6153 current loss 0.120832, current_train_items 196928.
I0304 19:31:02.292986 23128000471168 run.py:483] Algo bellman_ford step 6154 current loss 0.060112, current_train_items 196960.
I0304 19:31:02.312906 23128000471168 run.py:483] Algo bellman_ford step 6155 current loss 0.002389, current_train_items 196992.
I0304 19:31:02.329071 23128000471168 run.py:483] Algo bellman_ford step 6156 current loss 0.085718, current_train_items 197024.
I0304 19:31:02.352861 23128000471168 run.py:483] Algo bellman_ford step 6157 current loss 0.033569, current_train_items 197056.
I0304 19:31:02.383516 23128000471168 run.py:483] Algo bellman_ford step 6158 current loss 0.081325, current_train_items 197088.
I0304 19:31:02.416672 23128000471168 run.py:483] Algo bellman_ford step 6159 current loss 0.047008, current_train_items 197120.
I0304 19:31:02.436804 23128000471168 run.py:483] Algo bellman_ford step 6160 current loss 0.016839, current_train_items 197152.
I0304 19:31:02.453658 23128000471168 run.py:483] Algo bellman_ford step 6161 current loss 0.019049, current_train_items 197184.
I0304 19:31:02.477321 23128000471168 run.py:483] Algo bellman_ford step 6162 current loss 0.073488, current_train_items 197216.
I0304 19:31:02.509970 23128000471168 run.py:483] Algo bellman_ford step 6163 current loss 0.071290, current_train_items 197248.
I0304 19:31:02.544241 23128000471168 run.py:483] Algo bellman_ford step 6164 current loss 0.102057, current_train_items 197280.
I0304 19:31:02.564028 23128000471168 run.py:483] Algo bellman_ford step 6165 current loss 0.004137, current_train_items 197312.
I0304 19:31:02.580632 23128000471168 run.py:483] Algo bellman_ford step 6166 current loss 0.012160, current_train_items 197344.
I0304 19:31:02.604873 23128000471168 run.py:483] Algo bellman_ford step 6167 current loss 0.052510, current_train_items 197376.
I0304 19:31:02.637012 23128000471168 run.py:483] Algo bellman_ford step 6168 current loss 0.029653, current_train_items 197408.
I0304 19:31:02.670976 23128000471168 run.py:483] Algo bellman_ford step 6169 current loss 0.049188, current_train_items 197440.
I0304 19:31:02.691365 23128000471168 run.py:483] Algo bellman_ford step 6170 current loss 0.004137, current_train_items 197472.
I0304 19:31:02.707549 23128000471168 run.py:483] Algo bellman_ford step 6171 current loss 0.021095, current_train_items 197504.
I0304 19:31:02.731482 23128000471168 run.py:483] Algo bellman_ford step 6172 current loss 0.024673, current_train_items 197536.
I0304 19:31:02.764053 23128000471168 run.py:483] Algo bellman_ford step 6173 current loss 0.046781, current_train_items 197568.
I0304 19:31:02.797763 23128000471168 run.py:483] Algo bellman_ford step 6174 current loss 0.046483, current_train_items 197600.
I0304 19:31:02.817853 23128000471168 run.py:483] Algo bellman_ford step 6175 current loss 0.003784, current_train_items 197632.
I0304 19:31:02.834674 23128000471168 run.py:483] Algo bellman_ford step 6176 current loss 0.017430, current_train_items 197664.
I0304 19:31:02.858031 23128000471168 run.py:483] Algo bellman_ford step 6177 current loss 0.056168, current_train_items 197696.
I0304 19:31:02.889534 23128000471168 run.py:483] Algo bellman_ford step 6178 current loss 0.026815, current_train_items 197728.
I0304 19:31:02.923421 23128000471168 run.py:483] Algo bellman_ford step 6179 current loss 0.057593, current_train_items 197760.
I0304 19:31:02.943118 23128000471168 run.py:483] Algo bellman_ford step 6180 current loss 0.003087, current_train_items 197792.
I0304 19:31:02.959585 23128000471168 run.py:483] Algo bellman_ford step 6181 current loss 0.016382, current_train_items 197824.
I0304 19:31:02.983426 23128000471168 run.py:483] Algo bellman_ford step 6182 current loss 0.041913, current_train_items 197856.
I0304 19:31:03.014784 23128000471168 run.py:483] Algo bellman_ford step 6183 current loss 0.128496, current_train_items 197888.
I0304 19:31:03.050812 23128000471168 run.py:483] Algo bellman_ford step 6184 current loss 0.073111, current_train_items 197920.
I0304 19:31:03.071266 23128000471168 run.py:483] Algo bellman_ford step 6185 current loss 0.003854, current_train_items 197952.
I0304 19:31:03.087718 23128000471168 run.py:483] Algo bellman_ford step 6186 current loss 0.042340, current_train_items 197984.
I0304 19:31:03.110501 23128000471168 run.py:483] Algo bellman_ford step 6187 current loss 0.046768, current_train_items 198016.
I0304 19:31:03.141205 23128000471168 run.py:483] Algo bellman_ford step 6188 current loss 0.028798, current_train_items 198048.
I0304 19:31:03.175382 23128000471168 run.py:483] Algo bellman_ford step 6189 current loss 0.055780, current_train_items 198080.
I0304 19:31:03.195844 23128000471168 run.py:483] Algo bellman_ford step 6190 current loss 0.002459, current_train_items 198112.
I0304 19:31:03.212509 23128000471168 run.py:483] Algo bellman_ford step 6191 current loss 0.016776, current_train_items 198144.
I0304 19:31:03.236623 23128000471168 run.py:483] Algo bellman_ford step 6192 current loss 0.026949, current_train_items 198176.
I0304 19:31:03.268519 23128000471168 run.py:483] Algo bellman_ford step 6193 current loss 0.055182, current_train_items 198208.
I0304 19:31:03.301443 23128000471168 run.py:483] Algo bellman_ford step 6194 current loss 0.055456, current_train_items 198240.
I0304 19:31:03.321149 23128000471168 run.py:483] Algo bellman_ford step 6195 current loss 0.002356, current_train_items 198272.
I0304 19:31:03.336916 23128000471168 run.py:483] Algo bellman_ford step 6196 current loss 0.038572, current_train_items 198304.
I0304 19:31:03.361678 23128000471168 run.py:483] Algo bellman_ford step 6197 current loss 0.072628, current_train_items 198336.
I0304 19:31:03.391795 23128000471168 run.py:483] Algo bellman_ford step 6198 current loss 0.022123, current_train_items 198368.
I0304 19:31:03.426716 23128000471168 run.py:483] Algo bellman_ford step 6199 current loss 0.112504, current_train_items 198400.
I0304 19:31:03.447105 23128000471168 run.py:483] Algo bellman_ford step 6200 current loss 0.002529, current_train_items 198432.
I0304 19:31:03.455039 23128000471168 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0304 19:31:03.455149 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:03.472533 23128000471168 run.py:483] Algo bellman_ford step 6201 current loss 0.026246, current_train_items 198464.
I0304 19:31:03.497944 23128000471168 run.py:483] Algo bellman_ford step 6202 current loss 0.049159, current_train_items 198496.
I0304 19:31:03.528321 23128000471168 run.py:483] Algo bellman_ford step 6203 current loss 0.033311, current_train_items 198528.
I0304 19:31:03.561643 23128000471168 run.py:483] Algo bellman_ford step 6204 current loss 0.046457, current_train_items 198560.
I0304 19:31:03.582331 23128000471168 run.py:483] Algo bellman_ford step 6205 current loss 0.001791, current_train_items 198592.
I0304 19:31:03.597869 23128000471168 run.py:483] Algo bellman_ford step 6206 current loss 0.004776, current_train_items 198624.
I0304 19:31:03.622105 23128000471168 run.py:483] Algo bellman_ford step 6207 current loss 0.043810, current_train_items 198656.
I0304 19:31:03.653701 23128000471168 run.py:483] Algo bellman_ford step 6208 current loss 0.062467, current_train_items 198688.
I0304 19:31:03.688693 23128000471168 run.py:483] Algo bellman_ford step 6209 current loss 0.087260, current_train_items 198720.
I0304 19:31:03.708477 23128000471168 run.py:483] Algo bellman_ford step 6210 current loss 0.003045, current_train_items 198752.
I0304 19:31:03.724968 23128000471168 run.py:483] Algo bellman_ford step 6211 current loss 0.021085, current_train_items 198784.
I0304 19:31:03.748802 23128000471168 run.py:483] Algo bellman_ford step 6212 current loss 0.059209, current_train_items 198816.
I0304 19:31:03.778782 23128000471168 run.py:483] Algo bellman_ford step 6213 current loss 0.051835, current_train_items 198848.
I0304 19:31:03.814137 23128000471168 run.py:483] Algo bellman_ford step 6214 current loss 0.069218, current_train_items 198880.
I0304 19:31:03.833765 23128000471168 run.py:483] Algo bellman_ford step 6215 current loss 0.006337, current_train_items 198912.
I0304 19:31:03.850107 23128000471168 run.py:483] Algo bellman_ford step 6216 current loss 0.025807, current_train_items 198944.
I0304 19:31:03.873589 23128000471168 run.py:483] Algo bellman_ford step 6217 current loss 0.057547, current_train_items 198976.
I0304 19:31:03.904942 23128000471168 run.py:483] Algo bellman_ford step 6218 current loss 0.073316, current_train_items 199008.
I0304 19:31:03.938088 23128000471168 run.py:483] Algo bellman_ford step 6219 current loss 0.097993, current_train_items 199040.
I0304 19:31:03.958308 23128000471168 run.py:483] Algo bellman_ford step 6220 current loss 0.014546, current_train_items 199072.
I0304 19:31:03.974203 23128000471168 run.py:483] Algo bellman_ford step 6221 current loss 0.033777, current_train_items 199104.
I0304 19:31:03.998837 23128000471168 run.py:483] Algo bellman_ford step 6222 current loss 0.040243, current_train_items 199136.
I0304 19:31:04.029126 23128000471168 run.py:483] Algo bellman_ford step 6223 current loss 0.070471, current_train_items 199168.
I0304 19:31:04.063680 23128000471168 run.py:483] Algo bellman_ford step 6224 current loss 0.113664, current_train_items 199200.
I0304 19:31:04.083873 23128000471168 run.py:483] Algo bellman_ford step 6225 current loss 0.003215, current_train_items 199232.
I0304 19:31:04.100224 23128000471168 run.py:483] Algo bellman_ford step 6226 current loss 0.020976, current_train_items 199264.
I0304 19:31:04.123185 23128000471168 run.py:483] Algo bellman_ford step 6227 current loss 0.012244, current_train_items 199296.
I0304 19:31:04.153415 23128000471168 run.py:483] Algo bellman_ford step 6228 current loss 0.061226, current_train_items 199328.
I0304 19:31:04.185264 23128000471168 run.py:483] Algo bellman_ford step 6229 current loss 0.047008, current_train_items 199360.
I0304 19:31:04.205177 23128000471168 run.py:483] Algo bellman_ford step 6230 current loss 0.002130, current_train_items 199392.
I0304 19:31:04.221749 23128000471168 run.py:483] Algo bellman_ford step 6231 current loss 0.042598, current_train_items 199424.
I0304 19:31:04.246149 23128000471168 run.py:483] Algo bellman_ford step 6232 current loss 0.052063, current_train_items 199456.
I0304 19:31:04.276679 23128000471168 run.py:483] Algo bellman_ford step 6233 current loss 0.029743, current_train_items 199488.
I0304 19:31:04.310168 23128000471168 run.py:483] Algo bellman_ford step 6234 current loss 0.069461, current_train_items 199520.
I0304 19:31:04.329947 23128000471168 run.py:483] Algo bellman_ford step 6235 current loss 0.002763, current_train_items 199552.
I0304 19:31:04.346377 23128000471168 run.py:483] Algo bellman_ford step 6236 current loss 0.019482, current_train_items 199584.
I0304 19:31:04.369647 23128000471168 run.py:483] Algo bellman_ford step 6237 current loss 0.037340, current_train_items 199616.
I0304 19:31:04.401776 23128000471168 run.py:483] Algo bellman_ford step 6238 current loss 0.028136, current_train_items 199648.
I0304 19:31:04.433065 23128000471168 run.py:483] Algo bellman_ford step 6239 current loss 0.044256, current_train_items 199680.
I0304 19:31:04.452955 23128000471168 run.py:483] Algo bellman_ford step 6240 current loss 0.002321, current_train_items 199712.
I0304 19:31:04.469454 23128000471168 run.py:483] Algo bellman_ford step 6241 current loss 0.036683, current_train_items 199744.
I0304 19:31:04.493979 23128000471168 run.py:483] Algo bellman_ford step 6242 current loss 0.048470, current_train_items 199776.
I0304 19:31:04.525878 23128000471168 run.py:483] Algo bellman_ford step 6243 current loss 0.046903, current_train_items 199808.
I0304 19:31:04.558306 23128000471168 run.py:483] Algo bellman_ford step 6244 current loss 0.040386, current_train_items 199840.
I0304 19:31:04.578434 23128000471168 run.py:483] Algo bellman_ford step 6245 current loss 0.005587, current_train_items 199872.
I0304 19:31:04.594815 23128000471168 run.py:483] Algo bellman_ford step 6246 current loss 0.049666, current_train_items 199904.
I0304 19:31:04.618113 23128000471168 run.py:483] Algo bellman_ford step 6247 current loss 0.021616, current_train_items 199936.
I0304 19:31:04.650775 23128000471168 run.py:483] Algo bellman_ford step 6248 current loss 0.037947, current_train_items 199968.
I0304 19:31:04.682775 23128000471168 run.py:483] Algo bellman_ford step 6249 current loss 0.049221, current_train_items 200000.
I0304 19:31:04.702378 23128000471168 run.py:483] Algo bellman_ford step 6250 current loss 0.001674, current_train_items 200032.
I0304 19:31:04.710567 23128000471168 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0304 19:31:04.710675 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:04.728322 23128000471168 run.py:483] Algo bellman_ford step 6251 current loss 0.033038, current_train_items 200064.
I0304 19:31:04.752919 23128000471168 run.py:483] Algo bellman_ford step 6252 current loss 0.040053, current_train_items 200096.
I0304 19:31:04.785755 23128000471168 run.py:483] Algo bellman_ford step 6253 current loss 0.070524, current_train_items 200128.
I0304 19:31:04.818251 23128000471168 run.py:483] Algo bellman_ford step 6254 current loss 0.046434, current_train_items 200160.
I0304 19:31:04.838239 23128000471168 run.py:483] Algo bellman_ford step 6255 current loss 0.026671, current_train_items 200192.
I0304 19:31:04.854330 23128000471168 run.py:483] Algo bellman_ford step 6256 current loss 0.016923, current_train_items 200224.
I0304 19:31:04.878811 23128000471168 run.py:483] Algo bellman_ford step 6257 current loss 0.027947, current_train_items 200256.
I0304 19:31:04.909592 23128000471168 run.py:483] Algo bellman_ford step 6258 current loss 0.046484, current_train_items 200288.
I0304 19:31:04.942444 23128000471168 run.py:483] Algo bellman_ford step 6259 current loss 0.067316, current_train_items 200320.
I0304 19:31:04.962833 23128000471168 run.py:483] Algo bellman_ford step 6260 current loss 0.003534, current_train_items 200352.
I0304 19:31:04.979726 23128000471168 run.py:483] Algo bellman_ford step 6261 current loss 0.013797, current_train_items 200384.
I0304 19:31:05.003314 23128000471168 run.py:483] Algo bellman_ford step 6262 current loss 0.024597, current_train_items 200416.
I0304 19:31:05.034861 23128000471168 run.py:483] Algo bellman_ford step 6263 current loss 0.068328, current_train_items 200448.
I0304 19:31:05.069042 23128000471168 run.py:483] Algo bellman_ford step 6264 current loss 0.071937, current_train_items 200480.
I0304 19:31:05.089171 23128000471168 run.py:483] Algo bellman_ford step 6265 current loss 0.009770, current_train_items 200512.
I0304 19:31:05.106335 23128000471168 run.py:483] Algo bellman_ford step 6266 current loss 0.050859, current_train_items 200544.
I0304 19:31:05.131277 23128000471168 run.py:483] Algo bellman_ford step 6267 current loss 0.067332, current_train_items 200576.
I0304 19:31:05.162161 23128000471168 run.py:483] Algo bellman_ford step 6268 current loss 0.039132, current_train_items 200608.
I0304 19:31:05.193337 23128000471168 run.py:483] Algo bellman_ford step 6269 current loss 0.061460, current_train_items 200640.
I0304 19:31:05.213410 23128000471168 run.py:483] Algo bellman_ford step 6270 current loss 0.001989, current_train_items 200672.
I0304 19:31:05.230235 23128000471168 run.py:483] Algo bellman_ford step 6271 current loss 0.050326, current_train_items 200704.
I0304 19:31:05.253782 23128000471168 run.py:483] Algo bellman_ford step 6272 current loss 0.026514, current_train_items 200736.
I0304 19:31:05.284127 23128000471168 run.py:483] Algo bellman_ford step 6273 current loss 0.043857, current_train_items 200768.
I0304 19:31:05.319339 23128000471168 run.py:483] Algo bellman_ford step 6274 current loss 0.078260, current_train_items 200800.
I0304 19:31:05.339660 23128000471168 run.py:483] Algo bellman_ford step 6275 current loss 0.002608, current_train_items 200832.
I0304 19:31:05.356461 23128000471168 run.py:483] Algo bellman_ford step 6276 current loss 0.019381, current_train_items 200864.
I0304 19:31:05.380112 23128000471168 run.py:483] Algo bellman_ford step 6277 current loss 0.052437, current_train_items 200896.
I0304 19:31:05.411133 23128000471168 run.py:483] Algo bellman_ford step 6278 current loss 0.060685, current_train_items 200928.
I0304 19:31:05.445882 23128000471168 run.py:483] Algo bellman_ford step 6279 current loss 0.066676, current_train_items 200960.
I0304 19:31:05.465824 23128000471168 run.py:483] Algo bellman_ford step 6280 current loss 0.001570, current_train_items 200992.
I0304 19:31:05.482141 23128000471168 run.py:483] Algo bellman_ford step 6281 current loss 0.005725, current_train_items 201024.
I0304 19:31:05.505890 23128000471168 run.py:483] Algo bellman_ford step 6282 current loss 0.116430, current_train_items 201056.
I0304 19:31:05.537453 23128000471168 run.py:483] Algo bellman_ford step 6283 current loss 0.072715, current_train_items 201088.
I0304 19:31:05.571714 23128000471168 run.py:483] Algo bellman_ford step 6284 current loss 0.066009, current_train_items 201120.
I0304 19:31:05.592132 23128000471168 run.py:483] Algo bellman_ford step 6285 current loss 0.002566, current_train_items 201152.
I0304 19:31:05.608888 23128000471168 run.py:483] Algo bellman_ford step 6286 current loss 0.014694, current_train_items 201184.
I0304 19:31:05.632955 23128000471168 run.py:483] Algo bellman_ford step 6287 current loss 0.029585, current_train_items 201216.
I0304 19:31:05.664872 23128000471168 run.py:483] Algo bellman_ford step 6288 current loss 0.042835, current_train_items 201248.
I0304 19:31:05.698697 23128000471168 run.py:483] Algo bellman_ford step 6289 current loss 0.069416, current_train_items 201280.
I0304 19:31:05.718647 23128000471168 run.py:483] Algo bellman_ford step 6290 current loss 0.002457, current_train_items 201312.
I0304 19:31:05.735413 23128000471168 run.py:483] Algo bellman_ford step 6291 current loss 0.061008, current_train_items 201344.
I0304 19:31:05.759385 23128000471168 run.py:483] Algo bellman_ford step 6292 current loss 0.033540, current_train_items 201376.
I0304 19:31:05.791577 23128000471168 run.py:483] Algo bellman_ford step 6293 current loss 0.089218, current_train_items 201408.
I0304 19:31:05.826881 23128000471168 run.py:483] Algo bellman_ford step 6294 current loss 0.064141, current_train_items 201440.
I0304 19:31:05.846758 23128000471168 run.py:483] Algo bellman_ford step 6295 current loss 0.002441, current_train_items 201472.
I0304 19:31:05.863823 23128000471168 run.py:483] Algo bellman_ford step 6296 current loss 0.028894, current_train_items 201504.
I0304 19:31:05.887742 23128000471168 run.py:483] Algo bellman_ford step 6297 current loss 0.009674, current_train_items 201536.
I0304 19:31:05.919324 23128000471168 run.py:483] Algo bellman_ford step 6298 current loss 0.050821, current_train_items 201568.
I0304 19:31:05.952769 23128000471168 run.py:483] Algo bellman_ford step 6299 current loss 0.059852, current_train_items 201600.
I0304 19:31:05.972732 23128000471168 run.py:483] Algo bellman_ford step 6300 current loss 0.001348, current_train_items 201632.
I0304 19:31:05.980416 23128000471168 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0304 19:31:05.980522 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:05.997795 23128000471168 run.py:483] Algo bellman_ford step 6301 current loss 0.020022, current_train_items 201664.
I0304 19:31:06.021729 23128000471168 run.py:483] Algo bellman_ford step 6302 current loss 0.040604, current_train_items 201696.
I0304 19:31:06.053085 23128000471168 run.py:483] Algo bellman_ford step 6303 current loss 0.039007, current_train_items 201728.
I0304 19:31:06.084848 23128000471168 run.py:483] Algo bellman_ford step 6304 current loss 0.041695, current_train_items 201760.
I0304 19:31:06.104927 23128000471168 run.py:483] Algo bellman_ford step 6305 current loss 0.011823, current_train_items 201792.
I0304 19:31:06.120852 23128000471168 run.py:483] Algo bellman_ford step 6306 current loss 0.012974, current_train_items 201824.
I0304 19:31:06.145077 23128000471168 run.py:483] Algo bellman_ford step 6307 current loss 0.043969, current_train_items 201856.
I0304 19:31:06.177106 23128000471168 run.py:483] Algo bellman_ford step 6308 current loss 0.036936, current_train_items 201888.
I0304 19:31:06.210047 23128000471168 run.py:483] Algo bellman_ford step 6309 current loss 0.120615, current_train_items 201920.
I0304 19:31:06.229966 23128000471168 run.py:483] Algo bellman_ford step 6310 current loss 0.004521, current_train_items 201952.
I0304 19:31:06.246225 23128000471168 run.py:483] Algo bellman_ford step 6311 current loss 0.005425, current_train_items 201984.
I0304 19:31:06.268890 23128000471168 run.py:483] Algo bellman_ford step 6312 current loss 0.053938, current_train_items 202016.
I0304 19:31:06.301415 23128000471168 run.py:483] Algo bellman_ford step 6313 current loss 0.129760, current_train_items 202048.
I0304 19:31:06.335710 23128000471168 run.py:483] Algo bellman_ford step 6314 current loss 0.143735, current_train_items 202080.
I0304 19:31:06.356049 23128000471168 run.py:483] Algo bellman_ford step 6315 current loss 0.003763, current_train_items 202112.
I0304 19:31:06.372137 23128000471168 run.py:483] Algo bellman_ford step 6316 current loss 0.013552, current_train_items 202144.
I0304 19:31:06.395725 23128000471168 run.py:483] Algo bellman_ford step 6317 current loss 0.044709, current_train_items 202176.
I0304 19:31:06.426358 23128000471168 run.py:483] Algo bellman_ford step 6318 current loss 0.021942, current_train_items 202208.
I0304 19:31:06.459715 23128000471168 run.py:483] Algo bellman_ford step 6319 current loss 0.073789, current_train_items 202240.
I0304 19:31:06.479587 23128000471168 run.py:483] Algo bellman_ford step 6320 current loss 0.014975, current_train_items 202272.
I0304 19:31:06.496122 23128000471168 run.py:483] Algo bellman_ford step 6321 current loss 0.023861, current_train_items 202304.
I0304 19:31:06.520981 23128000471168 run.py:483] Algo bellman_ford step 6322 current loss 0.025412, current_train_items 202336.
I0304 19:31:06.553098 23128000471168 run.py:483] Algo bellman_ford step 6323 current loss 0.056940, current_train_items 202368.
I0304 19:31:06.587456 23128000471168 run.py:483] Algo bellman_ford step 6324 current loss 0.072301, current_train_items 202400.
I0304 19:31:06.607439 23128000471168 run.py:483] Algo bellman_ford step 6325 current loss 0.007289, current_train_items 202432.
I0304 19:31:06.623745 23128000471168 run.py:483] Algo bellman_ford step 6326 current loss 0.017013, current_train_items 202464.
I0304 19:31:06.648644 23128000471168 run.py:483] Algo bellman_ford step 6327 current loss 0.084364, current_train_items 202496.
I0304 19:31:06.679711 23128000471168 run.py:483] Algo bellman_ford step 6328 current loss 0.067103, current_train_items 202528.
I0304 19:31:06.713886 23128000471168 run.py:483] Algo bellman_ford step 6329 current loss 0.061374, current_train_items 202560.
I0304 19:31:06.733796 23128000471168 run.py:483] Algo bellman_ford step 6330 current loss 0.004334, current_train_items 202592.
I0304 19:31:06.750338 23128000471168 run.py:483] Algo bellman_ford step 6331 current loss 0.032050, current_train_items 202624.
I0304 19:31:06.773732 23128000471168 run.py:483] Algo bellman_ford step 6332 current loss 0.044249, current_train_items 202656.
I0304 19:31:06.804966 23128000471168 run.py:483] Algo bellman_ford step 6333 current loss 0.023138, current_train_items 202688.
I0304 19:31:06.837774 23128000471168 run.py:483] Algo bellman_ford step 6334 current loss 0.046836, current_train_items 202720.
I0304 19:31:06.857902 23128000471168 run.py:483] Algo bellman_ford step 6335 current loss 0.002494, current_train_items 202752.
I0304 19:31:06.874434 23128000471168 run.py:483] Algo bellman_ford step 6336 current loss 0.012881, current_train_items 202784.
I0304 19:31:06.898745 23128000471168 run.py:483] Algo bellman_ford step 6337 current loss 0.035541, current_train_items 202816.
I0304 19:31:06.930978 23128000471168 run.py:483] Algo bellman_ford step 6338 current loss 0.047083, current_train_items 202848.
I0304 19:31:06.964148 23128000471168 run.py:483] Algo bellman_ford step 6339 current loss 0.047695, current_train_items 202880.
I0304 19:31:06.983633 23128000471168 run.py:483] Algo bellman_ford step 6340 current loss 0.004231, current_train_items 202912.
I0304 19:31:06.999621 23128000471168 run.py:483] Algo bellman_ford step 6341 current loss 0.009862, current_train_items 202944.
I0304 19:31:07.024437 23128000471168 run.py:483] Algo bellman_ford step 6342 current loss 0.054651, current_train_items 202976.
I0304 19:31:07.055630 23128000471168 run.py:483] Algo bellman_ford step 6343 current loss 0.027446, current_train_items 203008.
I0304 19:31:07.086914 23128000471168 run.py:483] Algo bellman_ford step 6344 current loss 0.055542, current_train_items 203040.
I0304 19:31:07.106672 23128000471168 run.py:483] Algo bellman_ford step 6345 current loss 0.001225, current_train_items 203072.
I0304 19:31:07.123299 23128000471168 run.py:483] Algo bellman_ford step 6346 current loss 0.041057, current_train_items 203104.
I0304 19:31:07.147216 23128000471168 run.py:483] Algo bellman_ford step 6347 current loss 0.047745, current_train_items 203136.
I0304 19:31:07.177929 23128000471168 run.py:483] Algo bellman_ford step 6348 current loss 0.061795, current_train_items 203168.
I0304 19:31:07.213578 23128000471168 run.py:483] Algo bellman_ford step 6349 current loss 0.052404, current_train_items 203200.
I0304 19:31:07.233210 23128000471168 run.py:483] Algo bellman_ford step 6350 current loss 0.020673, current_train_items 203232.
I0304 19:31:07.241438 23128000471168 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0304 19:31:07.241545 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:07.259027 23128000471168 run.py:483] Algo bellman_ford step 6351 current loss 0.012763, current_train_items 203264.
I0304 19:31:07.283021 23128000471168 run.py:483] Algo bellman_ford step 6352 current loss 0.051463, current_train_items 203296.
I0304 19:31:07.315127 23128000471168 run.py:483] Algo bellman_ford step 6353 current loss 0.073015, current_train_items 203328.
I0304 19:31:07.349207 23128000471168 run.py:483] Algo bellman_ford step 6354 current loss 0.085804, current_train_items 203360.
I0304 19:31:07.369862 23128000471168 run.py:483] Algo bellman_ford step 6355 current loss 0.002902, current_train_items 203392.
I0304 19:31:07.386414 23128000471168 run.py:483] Algo bellman_ford step 6356 current loss 0.011116, current_train_items 203424.
I0304 19:31:07.411260 23128000471168 run.py:483] Algo bellman_ford step 6357 current loss 0.045289, current_train_items 203456.
I0304 19:31:07.442466 23128000471168 run.py:483] Algo bellman_ford step 6358 current loss 0.098533, current_train_items 203488.
I0304 19:31:07.476779 23128000471168 run.py:483] Algo bellman_ford step 6359 current loss 0.066973, current_train_items 203520.
I0304 19:31:07.497213 23128000471168 run.py:483] Algo bellman_ford step 6360 current loss 0.004872, current_train_items 203552.
I0304 19:31:07.514232 23128000471168 run.py:483] Algo bellman_ford step 6361 current loss 0.010500, current_train_items 203584.
I0304 19:31:07.537344 23128000471168 run.py:483] Algo bellman_ford step 6362 current loss 0.040417, current_train_items 203616.
I0304 19:31:07.568810 23128000471168 run.py:483] Algo bellman_ford step 6363 current loss 0.045200, current_train_items 203648.
I0304 19:31:07.601974 23128000471168 run.py:483] Algo bellman_ford step 6364 current loss 0.055731, current_train_items 203680.
I0304 19:31:07.622064 23128000471168 run.py:483] Algo bellman_ford step 6365 current loss 0.002913, current_train_items 203712.
I0304 19:31:07.638719 23128000471168 run.py:483] Algo bellman_ford step 6366 current loss 0.014365, current_train_items 203744.
I0304 19:31:07.662904 23128000471168 run.py:483] Algo bellman_ford step 6367 current loss 0.064861, current_train_items 203776.
I0304 19:31:07.695616 23128000471168 run.py:483] Algo bellman_ford step 6368 current loss 0.066409, current_train_items 203808.
I0304 19:31:07.727736 23128000471168 run.py:483] Algo bellman_ford step 6369 current loss 0.030251, current_train_items 203840.
I0304 19:31:07.748152 23128000471168 run.py:483] Algo bellman_ford step 6370 current loss 0.002879, current_train_items 203872.
I0304 19:31:07.765048 23128000471168 run.py:483] Algo bellman_ford step 6371 current loss 0.008816, current_train_items 203904.
I0304 19:31:07.789599 23128000471168 run.py:483] Algo bellman_ford step 6372 current loss 0.188114, current_train_items 203936.
I0304 19:31:07.820600 23128000471168 run.py:483] Algo bellman_ford step 6373 current loss 0.196418, current_train_items 203968.
I0304 19:31:07.854185 23128000471168 run.py:483] Algo bellman_ford step 6374 current loss 0.125000, current_train_items 204000.
I0304 19:31:07.874720 23128000471168 run.py:483] Algo bellman_ford step 6375 current loss 0.003843, current_train_items 204032.
I0304 19:31:07.891062 23128000471168 run.py:483] Algo bellman_ford step 6376 current loss 0.003334, current_train_items 204064.
I0304 19:31:07.915825 23128000471168 run.py:483] Algo bellman_ford step 6377 current loss 0.035918, current_train_items 204096.
I0304 19:31:07.947720 23128000471168 run.py:483] Algo bellman_ford step 6378 current loss 0.067800, current_train_items 204128.
I0304 19:31:07.983417 23128000471168 run.py:483] Algo bellman_ford step 6379 current loss 0.090727, current_train_items 204160.
I0304 19:31:08.003439 23128000471168 run.py:483] Algo bellman_ford step 6380 current loss 0.005517, current_train_items 204192.
I0304 19:31:08.019776 23128000471168 run.py:483] Algo bellman_ford step 6381 current loss 0.057642, current_train_items 204224.
I0304 19:31:08.043779 23128000471168 run.py:483] Algo bellman_ford step 6382 current loss 0.028469, current_train_items 204256.
I0304 19:31:08.074355 23128000471168 run.py:483] Algo bellman_ford step 6383 current loss 0.036508, current_train_items 204288.
I0304 19:31:08.107883 23128000471168 run.py:483] Algo bellman_ford step 6384 current loss 0.047935, current_train_items 204320.
I0304 19:31:08.128280 23128000471168 run.py:483] Algo bellman_ford step 6385 current loss 0.001656, current_train_items 204352.
I0304 19:31:08.144417 23128000471168 run.py:483] Algo bellman_ford step 6386 current loss 0.010129, current_train_items 204384.
I0304 19:31:08.168723 23128000471168 run.py:483] Algo bellman_ford step 6387 current loss 0.080643, current_train_items 204416.
I0304 19:31:08.199702 23128000471168 run.py:483] Algo bellman_ford step 6388 current loss 0.044803, current_train_items 204448.
I0304 19:31:08.233550 23128000471168 run.py:483] Algo bellman_ford step 6389 current loss 0.037752, current_train_items 204480.
I0304 19:31:08.253618 23128000471168 run.py:483] Algo bellman_ford step 6390 current loss 0.001480, current_train_items 204512.
I0304 19:31:08.270520 23128000471168 run.py:483] Algo bellman_ford step 6391 current loss 0.014416, current_train_items 204544.
I0304 19:31:08.293983 23128000471168 run.py:483] Algo bellman_ford step 6392 current loss 0.044284, current_train_items 204576.
I0304 19:31:08.323858 23128000471168 run.py:483] Algo bellman_ford step 6393 current loss 0.043462, current_train_items 204608.
I0304 19:31:08.357707 23128000471168 run.py:483] Algo bellman_ford step 6394 current loss 0.054234, current_train_items 204640.
I0304 19:31:08.378433 23128000471168 run.py:483] Algo bellman_ford step 6395 current loss 0.002331, current_train_items 204672.
I0304 19:31:08.394801 23128000471168 run.py:483] Algo bellman_ford step 6396 current loss 0.014415, current_train_items 204704.
I0304 19:31:08.418538 23128000471168 run.py:483] Algo bellman_ford step 6397 current loss 0.044895, current_train_items 204736.
I0304 19:31:08.449252 23128000471168 run.py:483] Algo bellman_ford step 6398 current loss 0.051706, current_train_items 204768.
I0304 19:31:08.481343 23128000471168 run.py:483] Algo bellman_ford step 6399 current loss 0.062859, current_train_items 204800.
I0304 19:31:08.501952 23128000471168 run.py:483] Algo bellman_ford step 6400 current loss 0.001388, current_train_items 204832.
I0304 19:31:08.509554 23128000471168 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9970703125, 'score': 0.9970703125, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0304 19:31:08.509663 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.996, current avg val score is 0.997, val scores are: bellman_ford: 0.997
I0304 19:31:08.538637 23128000471168 run.py:483] Algo bellman_ford step 6401 current loss 0.008974, current_train_items 204864.
I0304 19:31:08.562625 23128000471168 run.py:483] Algo bellman_ford step 6402 current loss 0.012885, current_train_items 204896.
I0304 19:31:08.594661 23128000471168 run.py:483] Algo bellman_ford step 6403 current loss 0.075808, current_train_items 204928.
I0304 19:31:08.628745 23128000471168 run.py:483] Algo bellman_ford step 6404 current loss 0.066248, current_train_items 204960.
I0304 19:31:08.649507 23128000471168 run.py:483] Algo bellman_ford step 6405 current loss 0.002950, current_train_items 204992.
I0304 19:31:08.665653 23128000471168 run.py:483] Algo bellman_ford step 6406 current loss 0.018536, current_train_items 205024.
I0304 19:31:08.689638 23128000471168 run.py:483] Algo bellman_ford step 6407 current loss 0.020126, current_train_items 205056.
I0304 19:31:08.721210 23128000471168 run.py:483] Algo bellman_ford step 6408 current loss 0.045381, current_train_items 205088.
I0304 19:31:08.756484 23128000471168 run.py:483] Algo bellman_ford step 6409 current loss 0.050485, current_train_items 205120.
I0304 19:31:08.776691 23128000471168 run.py:483] Algo bellman_ford step 6410 current loss 0.012413, current_train_items 205152.
I0304 19:31:08.793080 23128000471168 run.py:483] Algo bellman_ford step 6411 current loss 0.014406, current_train_items 205184.
I0304 19:31:08.816289 23128000471168 run.py:483] Algo bellman_ford step 6412 current loss 0.031330, current_train_items 205216.
I0304 19:31:08.847739 23128000471168 run.py:483] Algo bellman_ford step 6413 current loss 0.035611, current_train_items 205248.
I0304 19:31:08.881609 23128000471168 run.py:483] Algo bellman_ford step 6414 current loss 0.033116, current_train_items 205280.
I0304 19:31:08.901772 23128000471168 run.py:483] Algo bellman_ford step 6415 current loss 0.001406, current_train_items 205312.
I0304 19:31:08.918200 23128000471168 run.py:483] Algo bellman_ford step 6416 current loss 0.012753, current_train_items 205344.
I0304 19:31:08.943079 23128000471168 run.py:483] Algo bellman_ford step 6417 current loss 0.043353, current_train_items 205376.
I0304 19:31:08.975963 23128000471168 run.py:483] Algo bellman_ford step 6418 current loss 0.023484, current_train_items 205408.
I0304 19:31:09.010383 23128000471168 run.py:483] Algo bellman_ford step 6419 current loss 0.073057, current_train_items 205440.
I0304 19:31:09.030673 23128000471168 run.py:483] Algo bellman_ford step 6420 current loss 0.001714, current_train_items 205472.
I0304 19:31:09.047276 23128000471168 run.py:483] Algo bellman_ford step 6421 current loss 0.010344, current_train_items 205504.
I0304 19:31:09.071156 23128000471168 run.py:483] Algo bellman_ford step 6422 current loss 0.045410, current_train_items 205536.
I0304 19:31:09.102566 23128000471168 run.py:483] Algo bellman_ford step 6423 current loss 0.077296, current_train_items 205568.
I0304 19:31:09.136311 23128000471168 run.py:483] Algo bellman_ford step 6424 current loss 0.047379, current_train_items 205600.
I0304 19:31:09.156391 23128000471168 run.py:483] Algo bellman_ford step 6425 current loss 0.001428, current_train_items 205632.
I0304 19:31:09.173092 23128000471168 run.py:483] Algo bellman_ford step 6426 current loss 0.007712, current_train_items 205664.
I0304 19:31:09.197479 23128000471168 run.py:483] Algo bellman_ford step 6427 current loss 0.105398, current_train_items 205696.
I0304 19:31:09.229758 23128000471168 run.py:483] Algo bellman_ford step 6428 current loss 0.081810, current_train_items 205728.
I0304 19:31:09.265842 23128000471168 run.py:483] Algo bellman_ford step 6429 current loss 0.113117, current_train_items 205760.
I0304 19:31:09.286227 23128000471168 run.py:483] Algo bellman_ford step 6430 current loss 0.002329, current_train_items 205792.
I0304 19:31:09.302220 23128000471168 run.py:483] Algo bellman_ford step 6431 current loss 0.055901, current_train_items 205824.
I0304 19:31:09.326368 23128000471168 run.py:483] Algo bellman_ford step 6432 current loss 0.026461, current_train_items 205856.
I0304 19:31:09.358132 23128000471168 run.py:483] Algo bellman_ford step 6433 current loss 0.068833, current_train_items 205888.
I0304 19:31:09.391322 23128000471168 run.py:483] Algo bellman_ford step 6434 current loss 0.097123, current_train_items 205920.
I0304 19:31:09.411585 23128000471168 run.py:483] Algo bellman_ford step 6435 current loss 0.001772, current_train_items 205952.
I0304 19:31:09.427914 23128000471168 run.py:483] Algo bellman_ford step 6436 current loss 0.018630, current_train_items 205984.
I0304 19:31:09.451744 23128000471168 run.py:483] Algo bellman_ford step 6437 current loss 0.030870, current_train_items 206016.
I0304 19:31:09.484293 23128000471168 run.py:483] Algo bellman_ford step 6438 current loss 0.066124, current_train_items 206048.
I0304 19:31:09.519155 23128000471168 run.py:483] Algo bellman_ford step 6439 current loss 0.082629, current_train_items 206080.
I0304 19:31:09.539356 23128000471168 run.py:483] Algo bellman_ford step 6440 current loss 0.002590, current_train_items 206112.
I0304 19:31:09.556025 23128000471168 run.py:483] Algo bellman_ford step 6441 current loss 0.008908, current_train_items 206144.
I0304 19:31:09.579423 23128000471168 run.py:483] Algo bellman_ford step 6442 current loss 0.021629, current_train_items 206176.
I0304 19:31:09.609569 23128000471168 run.py:483] Algo bellman_ford step 6443 current loss 0.066518, current_train_items 206208.
I0304 19:31:09.644105 23128000471168 run.py:483] Algo bellman_ford step 6444 current loss 0.045393, current_train_items 206240.
I0304 19:31:09.664025 23128000471168 run.py:483] Algo bellman_ford step 6445 current loss 0.011630, current_train_items 206272.
I0304 19:31:09.680191 23128000471168 run.py:483] Algo bellman_ford step 6446 current loss 0.020406, current_train_items 206304.
I0304 19:31:09.705342 23128000471168 run.py:483] Algo bellman_ford step 6447 current loss 0.030553, current_train_items 206336.
I0304 19:31:09.737412 23128000471168 run.py:483] Algo bellman_ford step 6448 current loss 0.035218, current_train_items 206368.
I0304 19:31:09.771266 23128000471168 run.py:483] Algo bellman_ford step 6449 current loss 0.037129, current_train_items 206400.
I0304 19:31:09.791377 23128000471168 run.py:483] Algo bellman_ford step 6450 current loss 0.006344, current_train_items 206432.
I0304 19:31:09.799460 23128000471168 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0304 19:31:09.799568 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:09.816858 23128000471168 run.py:483] Algo bellman_ford step 6451 current loss 0.011803, current_train_items 206464.
I0304 19:31:09.842296 23128000471168 run.py:483] Algo bellman_ford step 6452 current loss 0.042488, current_train_items 206496.
I0304 19:31:09.873193 23128000471168 run.py:483] Algo bellman_ford step 6453 current loss 0.066926, current_train_items 206528.
I0304 19:31:09.905513 23128000471168 run.py:483] Algo bellman_ford step 6454 current loss 0.064566, current_train_items 206560.
I0304 19:31:09.925846 23128000471168 run.py:483] Algo bellman_ford step 6455 current loss 0.033434, current_train_items 206592.
I0304 19:31:09.942538 23128000471168 run.py:483] Algo bellman_ford step 6456 current loss 0.008230, current_train_items 206624.
I0304 19:31:09.965979 23128000471168 run.py:483] Algo bellman_ford step 6457 current loss 0.037481, current_train_items 206656.
I0304 19:31:09.997479 23128000471168 run.py:483] Algo bellman_ford step 6458 current loss 0.035038, current_train_items 206688.
I0304 19:31:10.029788 23128000471168 run.py:483] Algo bellman_ford step 6459 current loss 0.062002, current_train_items 206720.
I0304 19:31:10.050128 23128000471168 run.py:483] Algo bellman_ford step 6460 current loss 0.030877, current_train_items 206752.
I0304 19:31:10.066679 23128000471168 run.py:483] Algo bellman_ford step 6461 current loss 0.003708, current_train_items 206784.
I0304 19:31:10.090301 23128000471168 run.py:483] Algo bellman_ford step 6462 current loss 0.097767, current_train_items 206816.
I0304 19:31:10.121209 23128000471168 run.py:483] Algo bellman_ford step 6463 current loss 0.024466, current_train_items 206848.
I0304 19:31:10.156978 23128000471168 run.py:483] Algo bellman_ford step 6464 current loss 0.137339, current_train_items 206880.
I0304 19:31:10.176807 23128000471168 run.py:483] Algo bellman_ford step 6465 current loss 0.002884, current_train_items 206912.
I0304 19:31:10.193444 23128000471168 run.py:483] Algo bellman_ford step 6466 current loss 0.011284, current_train_items 206944.
I0304 19:31:10.219037 23128000471168 run.py:483] Algo bellman_ford step 6467 current loss 0.095201, current_train_items 206976.
I0304 19:31:10.251587 23128000471168 run.py:483] Algo bellman_ford step 6468 current loss 0.052103, current_train_items 207008.
I0304 19:31:10.287009 23128000471168 run.py:483] Algo bellman_ford step 6469 current loss 0.057605, current_train_items 207040.
I0304 19:31:10.307571 23128000471168 run.py:483] Algo bellman_ford step 6470 current loss 0.003476, current_train_items 207072.
I0304 19:31:10.324688 23128000471168 run.py:483] Algo bellman_ford step 6471 current loss 0.015400, current_train_items 207104.
I0304 19:31:10.348109 23128000471168 run.py:483] Algo bellman_ford step 6472 current loss 0.026848, current_train_items 207136.
I0304 19:31:10.379783 23128000471168 run.py:483] Algo bellman_ford step 6473 current loss 0.136865, current_train_items 207168.
I0304 19:31:10.412550 23128000471168 run.py:483] Algo bellman_ford step 6474 current loss 0.078700, current_train_items 207200.
I0304 19:31:10.432676 23128000471168 run.py:483] Algo bellman_ford step 6475 current loss 0.019111, current_train_items 207232.
I0304 19:31:10.449493 23128000471168 run.py:483] Algo bellman_ford step 6476 current loss 0.044588, current_train_items 207264.
I0304 19:31:10.472358 23128000471168 run.py:483] Algo bellman_ford step 6477 current loss 0.046120, current_train_items 207296.
I0304 19:31:10.504066 23128000471168 run.py:483] Algo bellman_ford step 6478 current loss 0.046988, current_train_items 207328.
I0304 19:31:10.539429 23128000471168 run.py:483] Algo bellman_ford step 6479 current loss 0.058787, current_train_items 207360.
I0304 19:31:10.559729 23128000471168 run.py:483] Algo bellman_ford step 6480 current loss 0.003177, current_train_items 207392.
I0304 19:31:10.575892 23128000471168 run.py:483] Algo bellman_ford step 6481 current loss 0.082491, current_train_items 207424.
I0304 19:31:10.600001 23128000471168 run.py:483] Algo bellman_ford step 6482 current loss 0.033227, current_train_items 207456.
I0304 19:31:10.632545 23128000471168 run.py:483] Algo bellman_ford step 6483 current loss 0.046738, current_train_items 207488.
I0304 19:31:10.667292 23128000471168 run.py:483] Algo bellman_ford step 6484 current loss 0.060279, current_train_items 207520.
I0304 19:31:10.688183 23128000471168 run.py:483] Algo bellman_ford step 6485 current loss 0.001804, current_train_items 207552.
I0304 19:31:10.704906 23128000471168 run.py:483] Algo bellman_ford step 6486 current loss 0.051193, current_train_items 207584.
I0304 19:31:10.730331 23128000471168 run.py:483] Algo bellman_ford step 6487 current loss 0.030601, current_train_items 207616.
I0304 19:31:10.760950 23128000471168 run.py:483] Algo bellman_ford step 6488 current loss 0.026197, current_train_items 207648.
I0304 19:31:10.795510 23128000471168 run.py:483] Algo bellman_ford step 6489 current loss 0.042531, current_train_items 207680.
I0304 19:31:10.815872 23128000471168 run.py:483] Algo bellman_ford step 6490 current loss 0.003085, current_train_items 207712.
I0304 19:31:10.832290 23128000471168 run.py:483] Algo bellman_ford step 6491 current loss 0.009237, current_train_items 207744.
I0304 19:31:10.855574 23128000471168 run.py:483] Algo bellman_ford step 6492 current loss 0.031854, current_train_items 207776.
I0304 19:31:10.887167 23128000471168 run.py:483] Algo bellman_ford step 6493 current loss 0.043528, current_train_items 207808.
I0304 19:31:10.920950 23128000471168 run.py:483] Algo bellman_ford step 6494 current loss 0.076284, current_train_items 207840.
I0304 19:31:10.941105 23128000471168 run.py:483] Algo bellman_ford step 6495 current loss 0.002611, current_train_items 207872.
I0304 19:31:10.957132 23128000471168 run.py:483] Algo bellman_ford step 6496 current loss 0.019061, current_train_items 207904.
I0304 19:31:10.981798 23128000471168 run.py:483] Algo bellman_ford step 6497 current loss 0.030740, current_train_items 207936.
I0304 19:31:11.012716 23128000471168 run.py:483] Algo bellman_ford step 6498 current loss 0.031261, current_train_items 207968.
I0304 19:31:11.048056 23128000471168 run.py:483] Algo bellman_ford step 6499 current loss 0.096080, current_train_items 208000.
I0304 19:31:11.068794 23128000471168 run.py:483] Algo bellman_ford step 6500 current loss 0.003026, current_train_items 208032.
I0304 19:31:11.076973 23128000471168 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0304 19:31:11.077089 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:11.094091 23128000471168 run.py:483] Algo bellman_ford step 6501 current loss 0.026341, current_train_items 208064.
I0304 19:31:11.119580 23128000471168 run.py:483] Algo bellman_ford step 6502 current loss 0.059442, current_train_items 208096.
I0304 19:31:11.150974 23128000471168 run.py:483] Algo bellman_ford step 6503 current loss 0.038175, current_train_items 208128.
I0304 19:31:11.185784 23128000471168 run.py:483] Algo bellman_ford step 6504 current loss 0.062204, current_train_items 208160.
I0304 19:31:11.205928 23128000471168 run.py:483] Algo bellman_ford step 6505 current loss 0.003105, current_train_items 208192.
I0304 19:31:11.221488 23128000471168 run.py:483] Algo bellman_ford step 6506 current loss 0.006000, current_train_items 208224.
I0304 19:31:11.245538 23128000471168 run.py:483] Algo bellman_ford step 6507 current loss 0.071785, current_train_items 208256.
I0304 19:31:11.277975 23128000471168 run.py:483] Algo bellman_ford step 6508 current loss 0.073505, current_train_items 208288.
I0304 19:31:11.310014 23128000471168 run.py:483] Algo bellman_ford step 6509 current loss 0.055093, current_train_items 208320.
I0304 19:31:11.329805 23128000471168 run.py:483] Algo bellman_ford step 6510 current loss 0.003313, current_train_items 208352.
I0304 19:31:11.346393 23128000471168 run.py:483] Algo bellman_ford step 6511 current loss 0.024904, current_train_items 208384.
I0304 19:31:11.370970 23128000471168 run.py:483] Algo bellman_ford step 6512 current loss 0.049148, current_train_items 208416.
I0304 19:31:11.401977 23128000471168 run.py:483] Algo bellman_ford step 6513 current loss 0.043603, current_train_items 208448.
I0304 19:31:11.437037 23128000471168 run.py:483] Algo bellman_ford step 6514 current loss 0.057457, current_train_items 208480.
I0304 19:31:11.456941 23128000471168 run.py:483] Algo bellman_ford step 6515 current loss 0.003424, current_train_items 208512.
I0304 19:31:11.473057 23128000471168 run.py:483] Algo bellman_ford step 6516 current loss 0.019573, current_train_items 208544.
I0304 19:31:11.497327 23128000471168 run.py:483] Algo bellman_ford step 6517 current loss 0.058834, current_train_items 208576.
I0304 19:31:11.528574 23128000471168 run.py:483] Algo bellman_ford step 6518 current loss 0.089118, current_train_items 208608.
I0304 19:31:11.561771 23128000471168 run.py:483] Algo bellman_ford step 6519 current loss 0.074369, current_train_items 208640.
I0304 19:31:11.581521 23128000471168 run.py:483] Algo bellman_ford step 6520 current loss 0.018040, current_train_items 208672.
I0304 19:31:11.597974 23128000471168 run.py:483] Algo bellman_ford step 6521 current loss 0.009797, current_train_items 208704.
I0304 19:31:11.622938 23128000471168 run.py:483] Algo bellman_ford step 6522 current loss 0.073640, current_train_items 208736.
I0304 19:31:11.653880 23128000471168 run.py:483] Algo bellman_ford step 6523 current loss 0.036698, current_train_items 208768.
I0304 19:31:11.685145 23128000471168 run.py:483] Algo bellman_ford step 6524 current loss 0.080696, current_train_items 208800.
I0304 19:31:11.705157 23128000471168 run.py:483] Algo bellman_ford step 6525 current loss 0.002508, current_train_items 208832.
I0304 19:31:11.721352 23128000471168 run.py:483] Algo bellman_ford step 6526 current loss 0.010097, current_train_items 208864.
I0304 19:31:11.744929 23128000471168 run.py:483] Algo bellman_ford step 6527 current loss 0.043046, current_train_items 208896.
I0304 19:31:11.775458 23128000471168 run.py:483] Algo bellman_ford step 6528 current loss 0.041711, current_train_items 208928.
I0304 19:31:11.809544 23128000471168 run.py:483] Algo bellman_ford step 6529 current loss 0.080519, current_train_items 208960.
I0304 19:31:11.829365 23128000471168 run.py:483] Algo bellman_ford step 6530 current loss 0.001804, current_train_items 208992.
I0304 19:31:11.845903 23128000471168 run.py:483] Algo bellman_ford step 6531 current loss 0.016531, current_train_items 209024.
I0304 19:31:11.871475 23128000471168 run.py:483] Algo bellman_ford step 6532 current loss 0.045069, current_train_items 209056.
I0304 19:31:11.901984 23128000471168 run.py:483] Algo bellman_ford step 6533 current loss 0.052904, current_train_items 209088.
I0304 19:31:11.935507 23128000471168 run.py:483] Algo bellman_ford step 6534 current loss 0.096016, current_train_items 209120.
I0304 19:31:11.955132 23128000471168 run.py:483] Algo bellman_ford step 6535 current loss 0.002307, current_train_items 209152.
I0304 19:31:11.971760 23128000471168 run.py:483] Algo bellman_ford step 6536 current loss 0.031140, current_train_items 209184.
I0304 19:31:11.995399 23128000471168 run.py:483] Algo bellman_ford step 6537 current loss 0.034822, current_train_items 209216.
I0304 19:31:12.026230 23128000471168 run.py:483] Algo bellman_ford step 6538 current loss 0.045834, current_train_items 209248.
I0304 19:31:12.059840 23128000471168 run.py:483] Algo bellman_ford step 6539 current loss 0.038731, current_train_items 209280.
I0304 19:31:12.079634 23128000471168 run.py:483] Algo bellman_ford step 6540 current loss 0.002233, current_train_items 209312.
I0304 19:31:12.096032 23128000471168 run.py:483] Algo bellman_ford step 6541 current loss 0.012667, current_train_items 209344.
I0304 19:31:12.119845 23128000471168 run.py:483] Algo bellman_ford step 6542 current loss 0.033325, current_train_items 209376.
I0304 19:31:12.151936 23128000471168 run.py:483] Algo bellman_ford step 6543 current loss 0.064936, current_train_items 209408.
I0304 19:31:12.187482 23128000471168 run.py:483] Algo bellman_ford step 6544 current loss 0.067541, current_train_items 209440.
I0304 19:31:12.207751 23128000471168 run.py:483] Algo bellman_ford step 6545 current loss 0.002032, current_train_items 209472.
I0304 19:31:12.223871 23128000471168 run.py:483] Algo bellman_ford step 6546 current loss 0.035907, current_train_items 209504.
I0304 19:31:12.247344 23128000471168 run.py:483] Algo bellman_ford step 6547 current loss 0.016270, current_train_items 209536.
I0304 19:31:12.280009 23128000471168 run.py:483] Algo bellman_ford step 6548 current loss 0.055689, current_train_items 209568.
I0304 19:31:12.314623 23128000471168 run.py:483] Algo bellman_ford step 6549 current loss 0.086890, current_train_items 209600.
I0304 19:31:12.334585 23128000471168 run.py:483] Algo bellman_ford step 6550 current loss 0.020086, current_train_items 209632.
I0304 19:31:12.342422 23128000471168 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0304 19:31:12.342530 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:12.360047 23128000471168 run.py:483] Algo bellman_ford step 6551 current loss 0.008230, current_train_items 209664.
I0304 19:31:12.384698 23128000471168 run.py:483] Algo bellman_ford step 6552 current loss 0.036226, current_train_items 209696.
I0304 19:31:12.417510 23128000471168 run.py:483] Algo bellman_ford step 6553 current loss 0.059587, current_train_items 209728.
I0304 19:31:12.453500 23128000471168 run.py:483] Algo bellman_ford step 6554 current loss 0.141735, current_train_items 209760.
I0304 19:31:12.473997 23128000471168 run.py:483] Algo bellman_ford step 6555 current loss 0.013386, current_train_items 209792.
I0304 19:31:12.490314 23128000471168 run.py:483] Algo bellman_ford step 6556 current loss 0.039977, current_train_items 209824.
I0304 19:31:12.514629 23128000471168 run.py:483] Algo bellman_ford step 6557 current loss 0.033945, current_train_items 209856.
I0304 19:31:12.547142 23128000471168 run.py:483] Algo bellman_ford step 6558 current loss 0.054044, current_train_items 209888.
I0304 19:31:12.581683 23128000471168 run.py:483] Algo bellman_ford step 6559 current loss 0.146856, current_train_items 209920.
I0304 19:31:12.602363 23128000471168 run.py:483] Algo bellman_ford step 6560 current loss 0.011093, current_train_items 209952.
I0304 19:31:12.618494 23128000471168 run.py:483] Algo bellman_ford step 6561 current loss 0.008932, current_train_items 209984.
I0304 19:31:12.641743 23128000471168 run.py:483] Algo bellman_ford step 6562 current loss 0.045514, current_train_items 210016.
I0304 19:31:12.672891 23128000471168 run.py:483] Algo bellman_ford step 6563 current loss 0.119225, current_train_items 210048.
I0304 19:31:12.704528 23128000471168 run.py:483] Algo bellman_ford step 6564 current loss 0.046354, current_train_items 210080.
I0304 19:31:12.724895 23128000471168 run.py:483] Algo bellman_ford step 6565 current loss 0.004462, current_train_items 210112.
I0304 19:31:12.741022 23128000471168 run.py:483] Algo bellman_ford step 6566 current loss 0.007169, current_train_items 210144.
I0304 19:31:12.765575 23128000471168 run.py:483] Algo bellman_ford step 6567 current loss 0.042039, current_train_items 210176.
I0304 19:31:12.795554 23128000471168 run.py:483] Algo bellman_ford step 6568 current loss 0.064511, current_train_items 210208.
I0304 19:31:12.829828 23128000471168 run.py:483] Algo bellman_ford step 6569 current loss 0.154140, current_train_items 210240.
I0304 19:31:12.850409 23128000471168 run.py:483] Algo bellman_ford step 6570 current loss 0.003716, current_train_items 210272.
I0304 19:31:12.867013 23128000471168 run.py:483] Algo bellman_ford step 6571 current loss 0.033912, current_train_items 210304.
I0304 19:31:12.890708 23128000471168 run.py:483] Algo bellman_ford step 6572 current loss 0.086898, current_train_items 210336.
I0304 19:31:12.921766 23128000471168 run.py:483] Algo bellman_ford step 6573 current loss 0.040387, current_train_items 210368.
I0304 19:31:12.955350 23128000471168 run.py:483] Algo bellman_ford step 6574 current loss 0.083776, current_train_items 210400.
I0304 19:31:12.975753 23128000471168 run.py:483] Algo bellman_ford step 6575 current loss 0.041193, current_train_items 210432.
I0304 19:31:12.992691 23128000471168 run.py:483] Algo bellman_ford step 6576 current loss 0.026795, current_train_items 210464.
I0304 19:31:13.016254 23128000471168 run.py:483] Algo bellman_ford step 6577 current loss 0.023078, current_train_items 210496.
I0304 19:31:13.047775 23128000471168 run.py:483] Algo bellman_ford step 6578 current loss 0.064639, current_train_items 210528.
I0304 19:31:13.081636 23128000471168 run.py:483] Algo bellman_ford step 6579 current loss 0.053048, current_train_items 210560.
I0304 19:31:13.101727 23128000471168 run.py:483] Algo bellman_ford step 6580 current loss 0.039345, current_train_items 210592.
I0304 19:31:13.118193 23128000471168 run.py:483] Algo bellman_ford step 6581 current loss 0.005066, current_train_items 210624.
I0304 19:31:13.142424 23128000471168 run.py:483] Algo bellman_ford step 6582 current loss 0.037372, current_train_items 210656.
I0304 19:31:13.175247 23128000471168 run.py:483] Algo bellman_ford step 6583 current loss 0.038308, current_train_items 210688.
I0304 19:31:13.208596 23128000471168 run.py:483] Algo bellman_ford step 6584 current loss 0.051975, current_train_items 210720.
I0304 19:31:13.228947 23128000471168 run.py:483] Algo bellman_ford step 6585 current loss 0.043228, current_train_items 210752.
I0304 19:31:13.245055 23128000471168 run.py:483] Algo bellman_ford step 6586 current loss 0.025763, current_train_items 210784.
I0304 19:31:13.267803 23128000471168 run.py:483] Algo bellman_ford step 6587 current loss 0.037543, current_train_items 210816.
I0304 19:31:13.298436 23128000471168 run.py:483] Algo bellman_ford step 6588 current loss 0.074934, current_train_items 210848.
I0304 19:31:13.330953 23128000471168 run.py:483] Algo bellman_ford step 6589 current loss 0.114231, current_train_items 210880.
I0304 19:31:13.351313 23128000471168 run.py:483] Algo bellman_ford step 6590 current loss 0.021276, current_train_items 210912.
I0304 19:31:13.367602 23128000471168 run.py:483] Algo bellman_ford step 6591 current loss 0.011116, current_train_items 210944.
I0304 19:31:13.391941 23128000471168 run.py:483] Algo bellman_ford step 6592 current loss 0.038741, current_train_items 210976.
I0304 19:31:13.423183 23128000471168 run.py:483] Algo bellman_ford step 6593 current loss 0.073650, current_train_items 211008.
I0304 19:31:13.456303 23128000471168 run.py:483] Algo bellman_ford step 6594 current loss 0.130893, current_train_items 211040.
I0304 19:31:13.476241 23128000471168 run.py:483] Algo bellman_ford step 6595 current loss 0.012353, current_train_items 211072.
I0304 19:31:13.492478 23128000471168 run.py:483] Algo bellman_ford step 6596 current loss 0.013765, current_train_items 211104.
I0304 19:31:13.517315 23128000471168 run.py:483] Algo bellman_ford step 6597 current loss 0.050783, current_train_items 211136.
I0304 19:31:13.549447 23128000471168 run.py:483] Algo bellman_ford step 6598 current loss 0.034009, current_train_items 211168.
I0304 19:31:13.580055 23128000471168 run.py:483] Algo bellman_ford step 6599 current loss 0.066411, current_train_items 211200.
I0304 19:31:13.600570 23128000471168 run.py:483] Algo bellman_ford step 6600 current loss 0.007223, current_train_items 211232.
I0304 19:31:13.608337 23128000471168 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0304 19:31:13.608448 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:31:13.625706 23128000471168 run.py:483] Algo bellman_ford step 6601 current loss 0.010878, current_train_items 211264.
I0304 19:31:13.648910 23128000471168 run.py:483] Algo bellman_ford step 6602 current loss 0.031564, current_train_items 211296.
I0304 19:31:13.680926 23128000471168 run.py:483] Algo bellman_ford step 6603 current loss 0.078470, current_train_items 211328.
I0304 19:31:13.715051 23128000471168 run.py:483] Algo bellman_ford step 6604 current loss 0.045786, current_train_items 211360.
I0304 19:31:13.735194 23128000471168 run.py:483] Algo bellman_ford step 6605 current loss 0.002225, current_train_items 211392.
I0304 19:31:13.751100 23128000471168 run.py:483] Algo bellman_ford step 6606 current loss 0.023369, current_train_items 211424.
I0304 19:31:13.775240 23128000471168 run.py:483] Algo bellman_ford step 6607 current loss 0.046465, current_train_items 211456.
I0304 19:31:13.807009 23128000471168 run.py:483] Algo bellman_ford step 6608 current loss 0.035221, current_train_items 211488.
I0304 19:31:13.838888 23128000471168 run.py:483] Algo bellman_ford step 6609 current loss 0.028818, current_train_items 211520.
I0304 19:31:13.858871 23128000471168 run.py:483] Algo bellman_ford step 6610 current loss 0.003495, current_train_items 211552.
I0304 19:31:13.875067 23128000471168 run.py:483] Algo bellman_ford step 6611 current loss 0.026352, current_train_items 211584.
I0304 19:31:13.898223 23128000471168 run.py:483] Algo bellman_ford step 6612 current loss 0.040251, current_train_items 211616.
I0304 19:31:13.929960 23128000471168 run.py:483] Algo bellman_ford step 6613 current loss 0.062716, current_train_items 211648.
I0304 19:31:13.966097 23128000471168 run.py:483] Algo bellman_ford step 6614 current loss 0.058195, current_train_items 211680.
I0304 19:31:13.985961 23128000471168 run.py:483] Algo bellman_ford step 6615 current loss 0.002801, current_train_items 211712.
I0304 19:31:14.002130 23128000471168 run.py:483] Algo bellman_ford step 6616 current loss 0.004830, current_train_items 211744.
I0304 19:31:14.026204 23128000471168 run.py:483] Algo bellman_ford step 6617 current loss 0.038358, current_train_items 211776.
I0304 19:31:14.057225 23128000471168 run.py:483] Algo bellman_ford step 6618 current loss 0.095773, current_train_items 211808.
I0304 19:31:14.091148 23128000471168 run.py:483] Algo bellman_ford step 6619 current loss 0.073580, current_train_items 211840.
I0304 19:31:14.110959 23128000471168 run.py:483] Algo bellman_ford step 6620 current loss 0.003701, current_train_items 211872.
I0304 19:31:14.126985 23128000471168 run.py:483] Algo bellman_ford step 6621 current loss 0.022130, current_train_items 211904.
I0304 19:31:14.149615 23128000471168 run.py:483] Algo bellman_ford step 6622 current loss 0.052562, current_train_items 211936.
I0304 19:31:14.181171 23128000471168 run.py:483] Algo bellman_ford step 6623 current loss 0.081204, current_train_items 211968.
I0304 19:31:14.213741 23128000471168 run.py:483] Algo bellman_ford step 6624 current loss 0.080143, current_train_items 212000.
I0304 19:31:14.233655 23128000471168 run.py:483] Algo bellman_ford step 6625 current loss 0.001473, current_train_items 212032.
I0304 19:31:14.250142 23128000471168 run.py:483] Algo bellman_ford step 6626 current loss 0.007573, current_train_items 212064.
I0304 19:31:14.274097 23128000471168 run.py:483] Algo bellman_ford step 6627 current loss 0.069188, current_train_items 212096.
I0304 19:31:14.304182 23128000471168 run.py:483] Algo bellman_ford step 6628 current loss 0.055574, current_train_items 212128.
I0304 19:31:14.336430 23128000471168 run.py:483] Algo bellman_ford step 6629 current loss 0.057487, current_train_items 212160.
I0304 19:31:14.356112 23128000471168 run.py:483] Algo bellman_ford step 6630 current loss 0.009722, current_train_items 212192.
I0304 19:31:14.372616 23128000471168 run.py:483] Algo bellman_ford step 6631 current loss 0.032301, current_train_items 212224.
I0304 19:31:14.397282 23128000471168 run.py:483] Algo bellman_ford step 6632 current loss 0.067359, current_train_items 212256.
I0304 19:31:14.429262 23128000471168 run.py:483] Algo bellman_ford step 6633 current loss 0.093533, current_train_items 212288.
I0304 19:31:14.461508 23128000471168 run.py:483] Algo bellman_ford step 6634 current loss 0.056227, current_train_items 212320.
I0304 19:31:14.481575 23128000471168 run.py:483] Algo bellman_ford step 6635 current loss 0.002557, current_train_items 212352.
I0304 19:31:14.498096 23128000471168 run.py:483] Algo bellman_ford step 6636 current loss 0.053616, current_train_items 212384.
I0304 19:31:14.521948 23128000471168 run.py:483] Algo bellman_ford step 6637 current loss 0.021916, current_train_items 212416.
I0304 19:31:14.553215 23128000471168 run.py:483] Algo bellman_ford step 6638 current loss 0.098146, current_train_items 212448.
I0304 19:31:14.587098 23128000471168 run.py:483] Algo bellman_ford step 6639 current loss 0.095512, current_train_items 212480.
I0304 19:31:14.606813 23128000471168 run.py:483] Algo bellman_ford step 6640 current loss 0.003500, current_train_items 212512.
I0304 19:31:14.623307 23128000471168 run.py:483] Algo bellman_ford step 6641 current loss 0.007975, current_train_items 212544.
I0304 19:31:14.648318 23128000471168 run.py:483] Algo bellman_ford step 6642 current loss 0.081267, current_train_items 212576.
I0304 19:31:14.678992 23128000471168 run.py:483] Algo bellman_ford step 6643 current loss 0.050148, current_train_items 212608.
I0304 19:31:14.713486 23128000471168 run.py:483] Algo bellman_ford step 6644 current loss 0.055065, current_train_items 212640.
I0304 19:31:14.733374 23128000471168 run.py:483] Algo bellman_ford step 6645 current loss 0.002675, current_train_items 212672.
I0304 19:31:14.749621 23128000471168 run.py:483] Algo bellman_ford step 6646 current loss 0.005041, current_train_items 212704.
I0304 19:31:14.773388 23128000471168 run.py:483] Algo bellman_ford step 6647 current loss 0.041089, current_train_items 212736.
I0304 19:31:14.805049 23128000471168 run.py:483] Algo bellman_ford step 6648 current loss 0.057432, current_train_items 212768.
I0304 19:31:14.839369 23128000471168 run.py:483] Algo bellman_ford step 6649 current loss 0.080378, current_train_items 212800.
I0304 19:31:14.859270 23128000471168 run.py:483] Algo bellman_ford step 6650 current loss 0.002432, current_train_items 212832.
I0304 19:31:14.867035 23128000471168 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0304 19:31:14.867143 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:31:14.884609 23128000471168 run.py:483] Algo bellman_ford step 6651 current loss 0.014053, current_train_items 212864.
I0304 19:31:14.909908 23128000471168 run.py:483] Algo bellman_ford step 6652 current loss 0.075717, current_train_items 212896.
I0304 19:31:14.942310 23128000471168 run.py:483] Algo bellman_ford step 6653 current loss 0.047875, current_train_items 212928.
I0304 19:31:14.972849 23128000471168 run.py:483] Algo bellman_ford step 6654 current loss 0.026558, current_train_items 212960.
I0304 19:31:14.992780 23128000471168 run.py:483] Algo bellman_ford step 6655 current loss 0.001768, current_train_items 212992.
I0304 19:31:15.008720 23128000471168 run.py:483] Algo bellman_ford step 6656 current loss 0.011956, current_train_items 213024.
I0304 19:31:15.032413 23128000471168 run.py:483] Algo bellman_ford step 6657 current loss 0.054563, current_train_items 213056.
I0304 19:31:15.064276 23128000471168 run.py:483] Algo bellman_ford step 6658 current loss 0.042664, current_train_items 213088.
I0304 19:31:15.099942 23128000471168 run.py:483] Algo bellman_ford step 6659 current loss 0.117573, current_train_items 213120.
I0304 19:31:15.120528 23128000471168 run.py:483] Algo bellman_ford step 6660 current loss 0.005039, current_train_items 213152.
I0304 19:31:15.137282 23128000471168 run.py:483] Algo bellman_ford step 6661 current loss 0.028655, current_train_items 213184.
I0304 19:31:15.162319 23128000471168 run.py:483] Algo bellman_ford step 6662 current loss 0.066778, current_train_items 213216.
I0304 19:31:15.192562 23128000471168 run.py:483] Algo bellman_ford step 6663 current loss 0.039512, current_train_items 213248.
I0304 19:31:15.226140 23128000471168 run.py:483] Algo bellman_ford step 6664 current loss 0.035836, current_train_items 213280.
I0304 19:31:15.246095 23128000471168 run.py:483] Algo bellman_ford step 6665 current loss 0.001978, current_train_items 213312.
I0304 19:31:15.262423 23128000471168 run.py:483] Algo bellman_ford step 6666 current loss 0.005707, current_train_items 213344.
I0304 19:31:15.288293 23128000471168 run.py:483] Algo bellman_ford step 6667 current loss 0.055010, current_train_items 213376.
I0304 19:31:15.321030 23128000471168 run.py:483] Algo bellman_ford step 6668 current loss 0.070813, current_train_items 213408.
I0304 19:31:15.356174 23128000471168 run.py:483] Algo bellman_ford step 6669 current loss 0.052836, current_train_items 213440.
I0304 19:31:15.376422 23128000471168 run.py:483] Algo bellman_ford step 6670 current loss 0.030466, current_train_items 213472.
I0304 19:31:15.392804 23128000471168 run.py:483] Algo bellman_ford step 6671 current loss 0.014621, current_train_items 213504.
I0304 19:31:15.416848 23128000471168 run.py:483] Algo bellman_ford step 6672 current loss 0.060514, current_train_items 213536.
I0304 19:31:15.448777 23128000471168 run.py:483] Algo bellman_ford step 6673 current loss 0.050365, current_train_items 213568.
I0304 19:31:15.481088 23128000471168 run.py:483] Algo bellman_ford step 6674 current loss 0.071218, current_train_items 213600.
I0304 19:31:15.501240 23128000471168 run.py:483] Algo bellman_ford step 6675 current loss 0.011274, current_train_items 213632.
I0304 19:31:15.517119 23128000471168 run.py:483] Algo bellman_ford step 6676 current loss 0.013218, current_train_items 213664.
I0304 19:31:15.541716 23128000471168 run.py:483] Algo bellman_ford step 6677 current loss 0.042098, current_train_items 213696.
I0304 19:31:15.574075 23128000471168 run.py:483] Algo bellman_ford step 6678 current loss 0.046368, current_train_items 213728.
I0304 19:31:15.609677 23128000471168 run.py:483] Algo bellman_ford step 6679 current loss 0.059681, current_train_items 213760.
I0304 19:31:15.629673 23128000471168 run.py:483] Algo bellman_ford step 6680 current loss 0.001686, current_train_items 213792.
I0304 19:31:15.646124 23128000471168 run.py:483] Algo bellman_ford step 6681 current loss 0.016182, current_train_items 213824.
I0304 19:31:15.671255 23128000471168 run.py:483] Algo bellman_ford step 6682 current loss 0.056491, current_train_items 213856.
I0304 19:31:15.702226 23128000471168 run.py:483] Algo bellman_ford step 6683 current loss 0.096604, current_train_items 213888.
I0304 19:31:15.738632 23128000471168 run.py:483] Algo bellman_ford step 6684 current loss 0.086824, current_train_items 213920.
I0304 19:31:15.758808 23128000471168 run.py:483] Algo bellman_ford step 6685 current loss 0.004101, current_train_items 213952.
I0304 19:31:15.775376 23128000471168 run.py:483] Algo bellman_ford step 6686 current loss 0.079431, current_train_items 213984.
I0304 19:31:15.799168 23128000471168 run.py:483] Algo bellman_ford step 6687 current loss 0.049627, current_train_items 214016.
I0304 19:31:15.831274 23128000471168 run.py:483] Algo bellman_ford step 6688 current loss 0.037326, current_train_items 214048.
I0304 19:31:15.864547 23128000471168 run.py:483] Algo bellman_ford step 6689 current loss 0.043196, current_train_items 214080.
I0304 19:31:15.884809 23128000471168 run.py:483] Algo bellman_ford step 6690 current loss 0.003347, current_train_items 214112.
I0304 19:31:15.900972 23128000471168 run.py:483] Algo bellman_ford step 6691 current loss 0.003668, current_train_items 214144.
I0304 19:31:15.925212 23128000471168 run.py:483] Algo bellman_ford step 6692 current loss 0.039130, current_train_items 214176.
I0304 19:31:15.956761 23128000471168 run.py:483] Algo bellman_ford step 6693 current loss 0.079649, current_train_items 214208.
I0304 19:31:15.990956 23128000471168 run.py:483] Algo bellman_ford step 6694 current loss 0.041104, current_train_items 214240.
I0304 19:31:16.010618 23128000471168 run.py:483] Algo bellman_ford step 6695 current loss 0.003095, current_train_items 214272.
I0304 19:31:16.027163 23128000471168 run.py:483] Algo bellman_ford step 6696 current loss 0.022652, current_train_items 214304.
I0304 19:31:16.051546 23128000471168 run.py:483] Algo bellman_ford step 6697 current loss 0.053270, current_train_items 214336.
I0304 19:31:16.083590 23128000471168 run.py:483] Algo bellman_ford step 6698 current loss 0.074688, current_train_items 214368.
I0304 19:31:16.118231 23128000471168 run.py:483] Algo bellman_ford step 6699 current loss 0.084216, current_train_items 214400.
I0304 19:31:16.138047 23128000471168 run.py:483] Algo bellman_ford step 6700 current loss 0.007160, current_train_items 214432.
I0304 19:31:16.145701 23128000471168 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0304 19:31:16.145808 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:31:16.163331 23128000471168 run.py:483] Algo bellman_ford step 6701 current loss 0.030506, current_train_items 214464.
I0304 19:31:16.187870 23128000471168 run.py:483] Algo bellman_ford step 6702 current loss 0.027124, current_train_items 214496.
I0304 19:31:16.220382 23128000471168 run.py:483] Algo bellman_ford step 6703 current loss 0.075350, current_train_items 214528.
I0304 19:31:16.253929 23128000471168 run.py:483] Algo bellman_ford step 6704 current loss 0.075315, current_train_items 214560.
I0304 19:31:16.274562 23128000471168 run.py:483] Algo bellman_ford step 6705 current loss 0.017197, current_train_items 214592.
I0304 19:31:16.290708 23128000471168 run.py:483] Algo bellman_ford step 6706 current loss 0.010860, current_train_items 214624.
I0304 19:31:16.316304 23128000471168 run.py:483] Algo bellman_ford step 6707 current loss 0.062310, current_train_items 214656.
I0304 19:31:16.347882 23128000471168 run.py:483] Algo bellman_ford step 6708 current loss 0.064443, current_train_items 214688.
I0304 19:31:16.382944 23128000471168 run.py:483] Algo bellman_ford step 6709 current loss 0.079268, current_train_items 214720.
I0304 19:31:16.402875 23128000471168 run.py:483] Algo bellman_ford step 6710 current loss 0.003265, current_train_items 214752.
I0304 19:31:16.419420 23128000471168 run.py:483] Algo bellman_ford step 6711 current loss 0.018569, current_train_items 214784.
I0304 19:31:16.443058 23128000471168 run.py:483] Algo bellman_ford step 6712 current loss 0.085844, current_train_items 214816.
I0304 19:31:16.476132 23128000471168 run.py:483] Algo bellman_ford step 6713 current loss 0.066032, current_train_items 214848.
I0304 19:31:16.509795 23128000471168 run.py:483] Algo bellman_ford step 6714 current loss 0.047669, current_train_items 214880.
I0304 19:31:16.529553 23128000471168 run.py:483] Algo bellman_ford step 6715 current loss 0.025311, current_train_items 214912.
I0304 19:31:16.545985 23128000471168 run.py:483] Algo bellman_ford step 6716 current loss 0.006302, current_train_items 214944.
I0304 19:31:16.570337 23128000471168 run.py:483] Algo bellman_ford step 6717 current loss 0.034968, current_train_items 214976.
I0304 19:31:16.602342 23128000471168 run.py:483] Algo bellman_ford step 6718 current loss 0.058409, current_train_items 215008.
I0304 19:31:16.637698 23128000471168 run.py:483] Algo bellman_ford step 6719 current loss 0.104979, current_train_items 215040.
I0304 19:31:16.657494 23128000471168 run.py:483] Algo bellman_ford step 6720 current loss 0.001505, current_train_items 215072.
I0304 19:31:16.673879 23128000471168 run.py:483] Algo bellman_ford step 6721 current loss 0.005691, current_train_items 215104.
I0304 19:31:16.697131 23128000471168 run.py:483] Algo bellman_ford step 6722 current loss 0.025111, current_train_items 215136.
I0304 19:31:16.729809 23128000471168 run.py:483] Algo bellman_ford step 6723 current loss 0.054617, current_train_items 215168.
I0304 19:31:16.762257 23128000471168 run.py:483] Algo bellman_ford step 6724 current loss 0.064910, current_train_items 215200.
I0304 19:31:16.782163 23128000471168 run.py:483] Algo bellman_ford step 6725 current loss 0.001363, current_train_items 215232.
I0304 19:31:16.798578 23128000471168 run.py:483] Algo bellman_ford step 6726 current loss 0.018207, current_train_items 215264.
I0304 19:31:16.822375 23128000471168 run.py:483] Algo bellman_ford step 6727 current loss 0.011537, current_train_items 215296.
I0304 19:31:16.853652 23128000471168 run.py:483] Algo bellman_ford step 6728 current loss 0.021707, current_train_items 215328.
I0304 19:31:16.885966 23128000471168 run.py:483] Algo bellman_ford step 6729 current loss 0.082974, current_train_items 215360.
I0304 19:31:16.906044 23128000471168 run.py:483] Algo bellman_ford step 6730 current loss 0.002072, current_train_items 215392.
I0304 19:31:16.922152 23128000471168 run.py:483] Algo bellman_ford step 6731 current loss 0.003596, current_train_items 215424.
I0304 19:31:16.946903 23128000471168 run.py:483] Algo bellman_ford step 6732 current loss 0.057603, current_train_items 215456.
I0304 19:31:16.976519 23128000471168 run.py:483] Algo bellman_ford step 6733 current loss 0.029762, current_train_items 215488.
I0304 19:31:17.008522 23128000471168 run.py:483] Algo bellman_ford step 6734 current loss 0.049832, current_train_items 215520.
I0304 19:31:17.028475 23128000471168 run.py:483] Algo bellman_ford step 6735 current loss 0.001793, current_train_items 215552.
I0304 19:31:17.045108 23128000471168 run.py:483] Algo bellman_ford step 6736 current loss 0.008509, current_train_items 215584.
I0304 19:31:17.069935 23128000471168 run.py:483] Algo bellman_ford step 6737 current loss 0.036711, current_train_items 215616.
I0304 19:31:17.100404 23128000471168 run.py:483] Algo bellman_ford step 6738 current loss 0.022157, current_train_items 215648.
I0304 19:31:17.135592 23128000471168 run.py:483] Algo bellman_ford step 6739 current loss 0.055447, current_train_items 215680.
I0304 19:31:17.155369 23128000471168 run.py:483] Algo bellman_ford step 6740 current loss 0.032934, current_train_items 215712.
I0304 19:31:17.172040 23128000471168 run.py:483] Algo bellman_ford step 6741 current loss 0.006497, current_train_items 215744.
I0304 19:31:17.196669 23128000471168 run.py:483] Algo bellman_ford step 6742 current loss 0.042921, current_train_items 215776.
I0304 19:31:17.227036 23128000471168 run.py:483] Algo bellman_ford step 6743 current loss 0.016303, current_train_items 215808.
I0304 19:31:17.261684 23128000471168 run.py:483] Algo bellman_ford step 6744 current loss 0.053021, current_train_items 215840.
I0304 19:31:17.281725 23128000471168 run.py:483] Algo bellman_ford step 6745 current loss 0.001845, current_train_items 215872.
I0304 19:31:17.298185 23128000471168 run.py:483] Algo bellman_ford step 6746 current loss 0.028758, current_train_items 215904.
I0304 19:31:17.322217 23128000471168 run.py:483] Algo bellman_ford step 6747 current loss 0.025254, current_train_items 215936.
I0304 19:31:17.353182 23128000471168 run.py:483] Algo bellman_ford step 6748 current loss 0.049923, current_train_items 215968.
I0304 19:31:17.387350 23128000471168 run.py:483] Algo bellman_ford step 6749 current loss 0.081830, current_train_items 216000.
I0304 19:31:17.407052 23128000471168 run.py:483] Algo bellman_ford step 6750 current loss 0.003454, current_train_items 216032.
I0304 19:31:17.415010 23128000471168 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0304 19:31:17.415117 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:31:17.432657 23128000471168 run.py:483] Algo bellman_ford step 6751 current loss 0.014455, current_train_items 216064.
I0304 19:31:17.457924 23128000471168 run.py:483] Algo bellman_ford step 6752 current loss 0.041602, current_train_items 216096.
I0304 19:31:17.491196 23128000471168 run.py:483] Algo bellman_ford step 6753 current loss 0.042614, current_train_items 216128.
I0304 19:31:17.526592 23128000471168 run.py:483] Algo bellman_ford step 6754 current loss 0.059401, current_train_items 216160.
I0304 19:31:17.547165 23128000471168 run.py:483] Algo bellman_ford step 6755 current loss 0.048800, current_train_items 216192.
I0304 19:31:17.563915 23128000471168 run.py:483] Algo bellman_ford step 6756 current loss 0.019353, current_train_items 216224.
I0304 19:31:17.587839 23128000471168 run.py:483] Algo bellman_ford step 6757 current loss 0.027487, current_train_items 216256.
I0304 19:31:17.619957 23128000471168 run.py:483] Algo bellman_ford step 6758 current loss 0.051608, current_train_items 216288.
I0304 19:31:17.655372 23128000471168 run.py:483] Algo bellman_ford step 6759 current loss 0.085340, current_train_items 216320.
I0304 19:31:17.675790 23128000471168 run.py:483] Algo bellman_ford step 6760 current loss 0.011190, current_train_items 216352.
I0304 19:31:17.692509 23128000471168 run.py:483] Algo bellman_ford step 6761 current loss 0.006784, current_train_items 216384.
I0304 19:31:17.716393 23128000471168 run.py:483] Algo bellman_ford step 6762 current loss 0.063665, current_train_items 216416.
I0304 19:31:17.748365 23128000471168 run.py:483] Algo bellman_ford step 6763 current loss 0.075468, current_train_items 216448.
I0304 19:31:17.781710 23128000471168 run.py:483] Algo bellman_ford step 6764 current loss 0.049171, current_train_items 216480.
I0304 19:31:17.801672 23128000471168 run.py:483] Algo bellman_ford step 6765 current loss 0.005598, current_train_items 216512.
I0304 19:31:17.818451 23128000471168 run.py:483] Algo bellman_ford step 6766 current loss 0.052077, current_train_items 216544.
I0304 19:31:17.842531 23128000471168 run.py:483] Algo bellman_ford step 6767 current loss 0.041041, current_train_items 216576.
I0304 19:31:17.873399 23128000471168 run.py:483] Algo bellman_ford step 6768 current loss 0.025414, current_train_items 216608.
I0304 19:31:17.906969 23128000471168 run.py:483] Algo bellman_ford step 6769 current loss 0.063188, current_train_items 216640.
I0304 19:31:17.927204 23128000471168 run.py:483] Algo bellman_ford step 6770 current loss 0.002211, current_train_items 216672.
I0304 19:31:17.943558 23128000471168 run.py:483] Algo bellman_ford step 6771 current loss 0.023021, current_train_items 216704.
I0304 19:31:17.966705 23128000471168 run.py:483] Algo bellman_ford step 6772 current loss 0.025285, current_train_items 216736.
I0304 19:31:17.998745 23128000471168 run.py:483] Algo bellman_ford step 6773 current loss 0.072350, current_train_items 216768.
I0304 19:31:18.031131 23128000471168 run.py:483] Algo bellman_ford step 6774 current loss 0.058609, current_train_items 216800.
I0304 19:31:18.051309 23128000471168 run.py:483] Algo bellman_ford step 6775 current loss 0.004532, current_train_items 216832.
I0304 19:31:18.067918 23128000471168 run.py:483] Algo bellman_ford step 6776 current loss 0.019415, current_train_items 216864.
I0304 19:31:18.091362 23128000471168 run.py:483] Algo bellman_ford step 6777 current loss 0.029173, current_train_items 216896.
I0304 19:31:18.124510 23128000471168 run.py:483] Algo bellman_ford step 6778 current loss 0.079844, current_train_items 216928.
I0304 19:31:18.157570 23128000471168 run.py:483] Algo bellman_ford step 6779 current loss 0.036713, current_train_items 216960.
I0304 19:31:18.177741 23128000471168 run.py:483] Algo bellman_ford step 6780 current loss 0.001793, current_train_items 216992.
I0304 19:31:18.193926 23128000471168 run.py:483] Algo bellman_ford step 6781 current loss 0.010910, current_train_items 217024.
I0304 19:31:18.217554 23128000471168 run.py:483] Algo bellman_ford step 6782 current loss 0.036760, current_train_items 217056.
I0304 19:31:18.250580 23128000471168 run.py:483] Algo bellman_ford step 6783 current loss 0.045028, current_train_items 217088.
I0304 19:31:18.283833 23128000471168 run.py:483] Algo bellman_ford step 6784 current loss 0.050983, current_train_items 217120.
I0304 19:31:18.304426 23128000471168 run.py:483] Algo bellman_ford step 6785 current loss 0.003123, current_train_items 217152.
I0304 19:31:18.320592 23128000471168 run.py:483] Algo bellman_ford step 6786 current loss 0.006638, current_train_items 217184.
I0304 19:31:18.343925 23128000471168 run.py:483] Algo bellman_ford step 6787 current loss 0.069915, current_train_items 217216.
I0304 19:31:18.374557 23128000471168 run.py:483] Algo bellman_ford step 6788 current loss 0.051547, current_train_items 217248.
I0304 19:31:18.409086 23128000471168 run.py:483] Algo bellman_ford step 6789 current loss 0.155903, current_train_items 217280.
I0304 19:31:18.429681 23128000471168 run.py:483] Algo bellman_ford step 6790 current loss 0.004480, current_train_items 217312.
I0304 19:31:18.446079 23128000471168 run.py:483] Algo bellman_ford step 6791 current loss 0.008592, current_train_items 217344.
I0304 19:31:18.470476 23128000471168 run.py:483] Algo bellman_ford step 6792 current loss 0.015375, current_train_items 217376.
I0304 19:31:18.501409 23128000471168 run.py:483] Algo bellman_ford step 6793 current loss 0.047134, current_train_items 217408.
I0304 19:31:18.535495 23128000471168 run.py:483] Algo bellman_ford step 6794 current loss 0.087710, current_train_items 217440.
I0304 19:31:18.555670 23128000471168 run.py:483] Algo bellman_ford step 6795 current loss 0.003440, current_train_items 217472.
I0304 19:31:18.572028 23128000471168 run.py:483] Algo bellman_ford step 6796 current loss 0.014138, current_train_items 217504.
I0304 19:31:18.597401 23128000471168 run.py:483] Algo bellman_ford step 6797 current loss 0.031285, current_train_items 217536.
I0304 19:31:18.628424 23128000471168 run.py:483] Algo bellman_ford step 6798 current loss 0.043144, current_train_items 217568.
I0304 19:31:18.661921 23128000471168 run.py:483] Algo bellman_ford step 6799 current loss 0.043620, current_train_items 217600.
I0304 19:31:18.682598 23128000471168 run.py:483] Algo bellman_ford step 6800 current loss 0.036442, current_train_items 217632.
I0304 19:31:18.690540 23128000471168 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0304 19:31:18.690650 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:31:18.707560 23128000471168 run.py:483] Algo bellman_ford step 6801 current loss 0.014665, current_train_items 217664.
I0304 19:31:18.732491 23128000471168 run.py:483] Algo bellman_ford step 6802 current loss 0.031553, current_train_items 217696.
I0304 19:31:18.766016 23128000471168 run.py:483] Algo bellman_ford step 6803 current loss 0.055509, current_train_items 217728.
I0304 19:31:18.800639 23128000471168 run.py:483] Algo bellman_ford step 6804 current loss 0.055769, current_train_items 217760.
I0304 19:31:18.821303 23128000471168 run.py:483] Algo bellman_ford step 6805 current loss 0.001319, current_train_items 217792.
I0304 19:31:18.837632 23128000471168 run.py:483] Algo bellman_ford step 6806 current loss 0.004649, current_train_items 217824.
I0304 19:31:18.862437 23128000471168 run.py:483] Algo bellman_ford step 6807 current loss 0.032792, current_train_items 217856.
I0304 19:31:18.892763 23128000471168 run.py:483] Algo bellman_ford step 6808 current loss 0.023717, current_train_items 217888.
I0304 19:31:18.926524 23128000471168 run.py:483] Algo bellman_ford step 6809 current loss 0.048307, current_train_items 217920.
I0304 19:31:18.946763 23128000471168 run.py:483] Algo bellman_ford step 6810 current loss 0.007111, current_train_items 217952.
I0304 19:31:18.962684 23128000471168 run.py:483] Algo bellman_ford step 6811 current loss 0.026745, current_train_items 217984.
I0304 19:31:18.988168 23128000471168 run.py:483] Algo bellman_ford step 6812 current loss 0.036195, current_train_items 218016.
I0304 19:31:19.019488 23128000471168 run.py:483] Algo bellman_ford step 6813 current loss 0.054649, current_train_items 218048.
I0304 19:31:19.051680 23128000471168 run.py:483] Algo bellman_ford step 6814 current loss 0.055203, current_train_items 218080.
I0304 19:31:19.071761 23128000471168 run.py:483] Algo bellman_ford step 6815 current loss 0.002894, current_train_items 218112.
I0304 19:31:19.088205 23128000471168 run.py:483] Algo bellman_ford step 6816 current loss 0.008587, current_train_items 218144.
I0304 19:31:19.111404 23128000471168 run.py:483] Algo bellman_ford step 6817 current loss 0.033553, current_train_items 218176.
I0304 19:31:19.143342 23128000471168 run.py:483] Algo bellman_ford step 6818 current loss 0.044135, current_train_items 218208.
I0304 19:31:19.178974 23128000471168 run.py:483] Algo bellman_ford step 6819 current loss 0.068088, current_train_items 218240.
I0304 19:31:19.199111 23128000471168 run.py:483] Algo bellman_ford step 6820 current loss 0.008909, current_train_items 218272.
I0304 19:31:19.215507 23128000471168 run.py:483] Algo bellman_ford step 6821 current loss 0.005931, current_train_items 218304.
I0304 19:31:19.239486 23128000471168 run.py:483] Algo bellman_ford step 6822 current loss 0.028907, current_train_items 218336.
I0304 19:31:19.271551 23128000471168 run.py:483] Algo bellman_ford step 6823 current loss 0.038566, current_train_items 218368.
I0304 19:31:19.306628 23128000471168 run.py:483] Algo bellman_ford step 6824 current loss 0.063959, current_train_items 218400.
I0304 19:31:19.326546 23128000471168 run.py:483] Algo bellman_ford step 6825 current loss 0.001274, current_train_items 218432.
I0304 19:31:19.343075 23128000471168 run.py:483] Algo bellman_ford step 6826 current loss 0.017168, current_train_items 218464.
I0304 19:31:19.367149 23128000471168 run.py:483] Algo bellman_ford step 6827 current loss 0.018107, current_train_items 218496.
I0304 19:31:19.398280 23128000471168 run.py:483] Algo bellman_ford step 6828 current loss 0.022997, current_train_items 218528.
I0304 19:31:19.434468 23128000471168 run.py:483] Algo bellman_ford step 6829 current loss 0.052281, current_train_items 218560.
I0304 19:31:19.454629 23128000471168 run.py:483] Algo bellman_ford step 6830 current loss 0.002126, current_train_items 218592.
I0304 19:31:19.470774 23128000471168 run.py:483] Algo bellman_ford step 6831 current loss 0.011783, current_train_items 218624.
I0304 19:31:19.496867 23128000471168 run.py:483] Algo bellman_ford step 6832 current loss 0.033500, current_train_items 218656.
I0304 19:31:19.528375 23128000471168 run.py:483] Algo bellman_ford step 6833 current loss 0.033547, current_train_items 218688.
I0304 19:31:19.563033 23128000471168 run.py:483] Algo bellman_ford step 6834 current loss 0.035904, current_train_items 218720.
I0304 19:31:19.582957 23128000471168 run.py:483] Algo bellman_ford step 6835 current loss 0.004930, current_train_items 218752.
I0304 19:31:19.599476 23128000471168 run.py:483] Algo bellman_ford step 6836 current loss 0.010913, current_train_items 218784.
I0304 19:31:19.623394 23128000471168 run.py:483] Algo bellman_ford step 6837 current loss 0.024737, current_train_items 218816.
I0304 19:31:19.656184 23128000471168 run.py:483] Algo bellman_ford step 6838 current loss 0.042375, current_train_items 218848.
I0304 19:31:19.688403 23128000471168 run.py:483] Algo bellman_ford step 6839 current loss 0.032962, current_train_items 218880.
I0304 19:31:19.708532 23128000471168 run.py:483] Algo bellman_ford step 6840 current loss 0.002740, current_train_items 218912.
I0304 19:31:19.725394 23128000471168 run.py:483] Algo bellman_ford step 6841 current loss 0.038130, current_train_items 218944.
I0304 19:31:19.750450 23128000471168 run.py:483] Algo bellman_ford step 6842 current loss 0.018489, current_train_items 218976.
I0304 19:31:19.781330 23128000471168 run.py:483] Algo bellman_ford step 6843 current loss 0.019375, current_train_items 219008.
I0304 19:31:19.814965 23128000471168 run.py:483] Algo bellman_ford step 6844 current loss 0.047358, current_train_items 219040.
I0304 19:31:19.835044 23128000471168 run.py:483] Algo bellman_ford step 6845 current loss 0.002159, current_train_items 219072.
I0304 19:31:19.851953 23128000471168 run.py:483] Algo bellman_ford step 6846 current loss 0.028408, current_train_items 219104.
I0304 19:31:19.876855 23128000471168 run.py:483] Algo bellman_ford step 6847 current loss 0.048502, current_train_items 219136.
I0304 19:31:19.909170 23128000471168 run.py:483] Algo bellman_ford step 6848 current loss 0.028936, current_train_items 219168.
I0304 19:31:19.943988 23128000471168 run.py:483] Algo bellman_ford step 6849 current loss 0.058488, current_train_items 219200.
I0304 19:31:19.964287 23128000471168 run.py:483] Algo bellman_ford step 6850 current loss 0.001601, current_train_items 219232.
I0304 19:31:19.972393 23128000471168 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0304 19:31:19.972501 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:31:19.989854 23128000471168 run.py:483] Algo bellman_ford step 6851 current loss 0.027760, current_train_items 219264.
I0304 19:31:20.014890 23128000471168 run.py:483] Algo bellman_ford step 6852 current loss 0.039961, current_train_items 219296.
I0304 19:31:20.047323 23128000471168 run.py:483] Algo bellman_ford step 6853 current loss 0.062271, current_train_items 219328.
I0304 19:31:20.082199 23128000471168 run.py:483] Algo bellman_ford step 6854 current loss 0.077132, current_train_items 219360.
I0304 19:31:20.102594 23128000471168 run.py:483] Algo bellman_ford step 6855 current loss 0.001829, current_train_items 219392.
I0304 19:31:20.118560 23128000471168 run.py:483] Algo bellman_ford step 6856 current loss 0.006397, current_train_items 219424.
I0304 19:31:20.143052 23128000471168 run.py:483] Algo bellman_ford step 6857 current loss 0.065205, current_train_items 219456.
I0304 19:31:20.174036 23128000471168 run.py:483] Algo bellman_ford step 6858 current loss 0.099029, current_train_items 219488.
I0304 19:31:20.208076 23128000471168 run.py:483] Algo bellman_ford step 6859 current loss 0.126257, current_train_items 219520.
I0304 19:31:20.228455 23128000471168 run.py:483] Algo bellman_ford step 6860 current loss 0.003638, current_train_items 219552.
I0304 19:31:20.244950 23128000471168 run.py:483] Algo bellman_ford step 6861 current loss 0.008769, current_train_items 219584.
I0304 19:31:20.268266 23128000471168 run.py:483] Algo bellman_ford step 6862 current loss 0.020579, current_train_items 219616.
I0304 19:31:20.299257 23128000471168 run.py:483] Algo bellman_ford step 6863 current loss 0.057081, current_train_items 219648.
I0304 19:31:20.331687 23128000471168 run.py:483] Algo bellman_ford step 6864 current loss 0.066909, current_train_items 219680.
I0304 19:31:20.352015 23128000471168 run.py:483] Algo bellman_ford step 6865 current loss 0.051955, current_train_items 219712.
I0304 19:31:20.368400 23128000471168 run.py:483] Algo bellman_ford step 6866 current loss 0.019091, current_train_items 219744.
I0304 19:31:20.393920 23128000471168 run.py:483] Algo bellman_ford step 6867 current loss 0.030277, current_train_items 219776.
I0304 19:31:20.425908 23128000471168 run.py:483] Algo bellman_ford step 6868 current loss 0.030159, current_train_items 219808.
I0304 19:31:20.461969 23128000471168 run.py:483] Algo bellman_ford step 6869 current loss 0.070257, current_train_items 219840.
I0304 19:31:20.482743 23128000471168 run.py:483] Algo bellman_ford step 6870 current loss 0.011315, current_train_items 219872.
I0304 19:31:20.499490 23128000471168 run.py:483] Algo bellman_ford step 6871 current loss 0.013674, current_train_items 219904.
I0304 19:31:20.523304 23128000471168 run.py:483] Algo bellman_ford step 6872 current loss 0.023302, current_train_items 219936.
I0304 19:31:20.554047 23128000471168 run.py:483] Algo bellman_ford step 6873 current loss 0.061816, current_train_items 219968.
I0304 19:31:20.587615 23128000471168 run.py:483] Algo bellman_ford step 6874 current loss 0.044277, current_train_items 220000.
I0304 19:31:20.608031 23128000471168 run.py:483] Algo bellman_ford step 6875 current loss 0.004333, current_train_items 220032.
I0304 19:31:20.624246 23128000471168 run.py:483] Algo bellman_ford step 6876 current loss 0.007501, current_train_items 220064.
I0304 19:31:20.648569 23128000471168 run.py:483] Algo bellman_ford step 6877 current loss 0.064143, current_train_items 220096.
I0304 19:31:20.680236 23128000471168 run.py:483] Algo bellman_ford step 6878 current loss 0.021317, current_train_items 220128.
I0304 19:31:20.714569 23128000471168 run.py:483] Algo bellman_ford step 6879 current loss 0.064596, current_train_items 220160.
I0304 19:31:20.734789 23128000471168 run.py:483] Algo bellman_ford step 6880 current loss 0.012277, current_train_items 220192.
I0304 19:31:20.750908 23128000471168 run.py:483] Algo bellman_ford step 6881 current loss 0.007382, current_train_items 220224.
I0304 19:31:20.775780 23128000471168 run.py:483] Algo bellman_ford step 6882 current loss 0.041364, current_train_items 220256.
I0304 19:31:20.806170 23128000471168 run.py:483] Algo bellman_ford step 6883 current loss 0.060792, current_train_items 220288.
I0304 19:31:20.839693 23128000471168 run.py:483] Algo bellman_ford step 6884 current loss 0.127306, current_train_items 220320.
I0304 19:31:20.860507 23128000471168 run.py:483] Algo bellman_ford step 6885 current loss 0.001380, current_train_items 220352.
I0304 19:31:20.877241 23128000471168 run.py:483] Algo bellman_ford step 6886 current loss 0.015244, current_train_items 220384.
I0304 19:31:20.901657 23128000471168 run.py:483] Algo bellman_ford step 6887 current loss 0.024405, current_train_items 220416.
I0304 19:31:20.934363 23128000471168 run.py:483] Algo bellman_ford step 6888 current loss 0.060132, current_train_items 220448.
I0304 19:31:20.968388 23128000471168 run.py:483] Algo bellman_ford step 6889 current loss 0.028223, current_train_items 220480.
I0304 19:31:20.988969 23128000471168 run.py:483] Algo bellman_ford step 6890 current loss 0.025679, current_train_items 220512.
I0304 19:31:21.006113 23128000471168 run.py:483] Algo bellman_ford step 6891 current loss 0.032911, current_train_items 220544.
I0304 19:31:21.030427 23128000471168 run.py:483] Algo bellman_ford step 6892 current loss 0.030609, current_train_items 220576.
I0304 19:31:21.062538 23128000471168 run.py:483] Algo bellman_ford step 6893 current loss 0.053261, current_train_items 220608.
I0304 19:31:21.096249 23128000471168 run.py:483] Algo bellman_ford step 6894 current loss 0.056728, current_train_items 220640.
I0304 19:31:21.116662 23128000471168 run.py:483] Algo bellman_ford step 6895 current loss 0.001652, current_train_items 220672.
I0304 19:31:21.132921 23128000471168 run.py:483] Algo bellman_ford step 6896 current loss 0.004252, current_train_items 220704.
I0304 19:31:21.157872 23128000471168 run.py:483] Algo bellman_ford step 6897 current loss 0.071743, current_train_items 220736.
I0304 19:31:21.188352 23128000471168 run.py:483] Algo bellman_ford step 6898 current loss 0.028723, current_train_items 220768.
I0304 19:31:21.223391 23128000471168 run.py:483] Algo bellman_ford step 6899 current loss 0.104570, current_train_items 220800.
I0304 19:31:21.244292 23128000471168 run.py:483] Algo bellman_ford step 6900 current loss 0.002480, current_train_items 220832.
I0304 19:31:21.252097 23128000471168 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0304 19:31:21.252208 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:21.269039 23128000471168 run.py:483] Algo bellman_ford step 6901 current loss 0.008902, current_train_items 220864.
I0304 19:31:21.293331 23128000471168 run.py:483] Algo bellman_ford step 6902 current loss 0.030922, current_train_items 220896.
I0304 19:31:21.326653 23128000471168 run.py:483] Algo bellman_ford step 6903 current loss 0.040448, current_train_items 220928.
I0304 19:31:21.362309 23128000471168 run.py:483] Algo bellman_ford step 6904 current loss 0.072911, current_train_items 220960.
I0304 19:31:21.382317 23128000471168 run.py:483] Algo bellman_ford step 6905 current loss 0.001459, current_train_items 220992.
I0304 19:31:21.398172 23128000471168 run.py:483] Algo bellman_ford step 6906 current loss 0.010278, current_train_items 221024.
I0304 19:31:21.422236 23128000471168 run.py:483] Algo bellman_ford step 6907 current loss 0.026820, current_train_items 221056.
I0304 19:31:21.454990 23128000471168 run.py:483] Algo bellman_ford step 6908 current loss 0.053978, current_train_items 221088.
I0304 19:31:21.489907 23128000471168 run.py:483] Algo bellman_ford step 6909 current loss 0.093965, current_train_items 221120.
I0304 19:31:21.509875 23128000471168 run.py:483] Algo bellman_ford step 6910 current loss 0.019140, current_train_items 221152.
I0304 19:31:21.526855 23128000471168 run.py:483] Algo bellman_ford step 6911 current loss 0.022326, current_train_items 221184.
I0304 19:31:21.549904 23128000471168 run.py:483] Algo bellman_ford step 6912 current loss 0.048956, current_train_items 221216.
I0304 19:31:21.582513 23128000471168 run.py:483] Algo bellman_ford step 6913 current loss 0.033688, current_train_items 221248.
I0304 19:31:21.617761 23128000471168 run.py:483] Algo bellman_ford step 6914 current loss 0.053821, current_train_items 221280.
I0304 19:31:21.637321 23128000471168 run.py:483] Algo bellman_ford step 6915 current loss 0.006593, current_train_items 221312.
I0304 19:31:21.653267 23128000471168 run.py:483] Algo bellman_ford step 6916 current loss 0.002316, current_train_items 221344.
I0304 19:31:21.678515 23128000471168 run.py:483] Algo bellman_ford step 6917 current loss 0.055283, current_train_items 221376.
I0304 19:31:21.710588 23128000471168 run.py:483] Algo bellman_ford step 6918 current loss 0.067861, current_train_items 221408.
I0304 19:31:21.745935 23128000471168 run.py:483] Algo bellman_ford step 6919 current loss 0.055371, current_train_items 221440.
I0304 19:31:21.766024 23128000471168 run.py:483] Algo bellman_ford step 6920 current loss 0.001971, current_train_items 221472.
I0304 19:31:21.782396 23128000471168 run.py:483] Algo bellman_ford step 6921 current loss 0.003409, current_train_items 221504.
I0304 19:31:21.806117 23128000471168 run.py:483] Algo bellman_ford step 6922 current loss 0.029760, current_train_items 221536.
I0304 19:31:21.837914 23128000471168 run.py:483] Algo bellman_ford step 6923 current loss 0.112583, current_train_items 221568.
I0304 19:31:21.872174 23128000471168 run.py:483] Algo bellman_ford step 6924 current loss 0.082873, current_train_items 221600.
I0304 19:31:21.892195 23128000471168 run.py:483] Algo bellman_ford step 6925 current loss 0.001503, current_train_items 221632.
I0304 19:31:21.908408 23128000471168 run.py:483] Algo bellman_ford step 6926 current loss 0.013144, current_train_items 221664.
I0304 19:31:21.933025 23128000471168 run.py:483] Algo bellman_ford step 6927 current loss 0.047205, current_train_items 221696.
I0304 19:31:21.964842 23128000471168 run.py:483] Algo bellman_ford step 6928 current loss 0.056150, current_train_items 221728.
I0304 19:31:21.998284 23128000471168 run.py:483] Algo bellman_ford step 6929 current loss 0.037867, current_train_items 221760.
I0304 19:31:22.018229 23128000471168 run.py:483] Algo bellman_ford step 6930 current loss 0.001667, current_train_items 221792.
I0304 19:31:22.035148 23128000471168 run.py:483] Algo bellman_ford step 6931 current loss 0.014169, current_train_items 221824.
I0304 19:31:22.058686 23128000471168 run.py:483] Algo bellman_ford step 6932 current loss 0.040618, current_train_items 221856.
I0304 19:31:22.090387 23128000471168 run.py:483] Algo bellman_ford step 6933 current loss 0.042482, current_train_items 221888.
I0304 19:31:22.125609 23128000471168 run.py:483] Algo bellman_ford step 6934 current loss 0.074064, current_train_items 221920.
I0304 19:31:22.145646 23128000471168 run.py:483] Algo bellman_ford step 6935 current loss 0.002053, current_train_items 221952.
I0304 19:31:22.161920 23128000471168 run.py:483] Algo bellman_ford step 6936 current loss 0.011409, current_train_items 221984.
I0304 19:31:22.185940 23128000471168 run.py:483] Algo bellman_ford step 6937 current loss 0.041319, current_train_items 222016.
I0304 19:31:22.217562 23128000471168 run.py:483] Algo bellman_ford step 6938 current loss 0.041292, current_train_items 222048.
I0304 19:31:22.250248 23128000471168 run.py:483] Algo bellman_ford step 6939 current loss 0.046185, current_train_items 222080.
I0304 19:31:22.270052 23128000471168 run.py:483] Algo bellman_ford step 6940 current loss 0.004073, current_train_items 222112.
I0304 19:31:22.286751 23128000471168 run.py:483] Algo bellman_ford step 6941 current loss 0.027328, current_train_items 222144.
I0304 19:31:22.311488 23128000471168 run.py:483] Algo bellman_ford step 6942 current loss 0.091494, current_train_items 222176.
I0304 19:31:22.343199 23128000471168 run.py:483] Algo bellman_ford step 6943 current loss 0.107284, current_train_items 222208.
I0304 19:31:22.378256 23128000471168 run.py:483] Algo bellman_ford step 6944 current loss 0.090472, current_train_items 222240.
I0304 19:31:22.397990 23128000471168 run.py:483] Algo bellman_ford step 6945 current loss 0.012151, current_train_items 222272.
I0304 19:31:22.414564 23128000471168 run.py:483] Algo bellman_ford step 6946 current loss 0.009396, current_train_items 222304.
I0304 19:31:22.438616 23128000471168 run.py:483] Algo bellman_ford step 6947 current loss 0.082198, current_train_items 222336.
I0304 19:31:22.468399 23128000471168 run.py:483] Algo bellman_ford step 6948 current loss 0.064422, current_train_items 222368.
I0304 19:31:22.502916 23128000471168 run.py:483] Algo bellman_ford step 6949 current loss 0.097622, current_train_items 222400.
I0304 19:31:22.522778 23128000471168 run.py:483] Algo bellman_ford step 6950 current loss 0.007337, current_train_items 222432.
I0304 19:31:22.530716 23128000471168 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0304 19:31:22.530823 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:31:22.548073 23128000471168 run.py:483] Algo bellman_ford step 6951 current loss 0.021747, current_train_items 222464.
I0304 19:31:22.573052 23128000471168 run.py:483] Algo bellman_ford step 6952 current loss 0.037716, current_train_items 222496.
I0304 19:31:22.604683 23128000471168 run.py:483] Algo bellman_ford step 6953 current loss 0.058989, current_train_items 222528.
I0304 19:31:22.639691 23128000471168 run.py:483] Algo bellman_ford step 6954 current loss 0.092821, current_train_items 222560.
I0304 19:31:22.659913 23128000471168 run.py:483] Algo bellman_ford step 6955 current loss 0.002550, current_train_items 222592.
I0304 19:31:22.676206 23128000471168 run.py:483] Algo bellman_ford step 6956 current loss 0.010933, current_train_items 222624.
I0304 19:31:22.701767 23128000471168 run.py:483] Algo bellman_ford step 6957 current loss 0.081857, current_train_items 222656.
I0304 19:31:22.732894 23128000471168 run.py:483] Algo bellman_ford step 6958 current loss 0.034588, current_train_items 222688.
I0304 19:31:22.765348 23128000471168 run.py:483] Algo bellman_ford step 6959 current loss 0.058459, current_train_items 222720.
I0304 19:31:22.785630 23128000471168 run.py:483] Algo bellman_ford step 6960 current loss 0.001784, current_train_items 222752.
I0304 19:31:22.802428 23128000471168 run.py:483] Algo bellman_ford step 6961 current loss 0.039821, current_train_items 222784.
I0304 19:31:22.825848 23128000471168 run.py:483] Algo bellman_ford step 6962 current loss 0.027427, current_train_items 222816.
I0304 19:31:22.857247 23128000471168 run.py:483] Algo bellman_ford step 6963 current loss 0.052985, current_train_items 222848.
I0304 19:31:22.892815 23128000471168 run.py:483] Algo bellman_ford step 6964 current loss 0.046679, current_train_items 222880.
I0304 19:31:22.912859 23128000471168 run.py:483] Algo bellman_ford step 6965 current loss 0.002261, current_train_items 222912.
I0304 19:31:22.929384 23128000471168 run.py:483] Algo bellman_ford step 6966 current loss 0.014060, current_train_items 222944.
I0304 19:31:22.953631 23128000471168 run.py:483] Algo bellman_ford step 6967 current loss 0.014073, current_train_items 222976.
I0304 19:31:22.984237 23128000471168 run.py:483] Algo bellman_ford step 6968 current loss 0.038452, current_train_items 223008.
I0304 19:31:23.018052 23128000471168 run.py:483] Algo bellman_ford step 6969 current loss 0.032763, current_train_items 223040.
I0304 19:31:23.038114 23128000471168 run.py:483] Algo bellman_ford step 6970 current loss 0.002364, current_train_items 223072.
I0304 19:31:23.054865 23128000471168 run.py:483] Algo bellman_ford step 6971 current loss 0.019588, current_train_items 223104.
I0304 19:31:23.078885 23128000471168 run.py:483] Algo bellman_ford step 6972 current loss 0.024345, current_train_items 223136.
I0304 19:31:23.110120 23128000471168 run.py:483] Algo bellman_ford step 6973 current loss 0.036169, current_train_items 223168.
I0304 19:31:23.144489 23128000471168 run.py:483] Algo bellman_ford step 6974 current loss 0.035517, current_train_items 223200.
I0304 19:31:23.164455 23128000471168 run.py:483] Algo bellman_ford step 6975 current loss 0.001941, current_train_items 223232.
I0304 19:31:23.181188 23128000471168 run.py:483] Algo bellman_ford step 6976 current loss 0.015866, current_train_items 223264.
I0304 19:31:23.204740 23128000471168 run.py:483] Algo bellman_ford step 6977 current loss 0.025351, current_train_items 223296.
I0304 19:31:23.235357 23128000471168 run.py:483] Algo bellman_ford step 6978 current loss 0.040749, current_train_items 223328.
I0304 19:31:23.270127 23128000471168 run.py:483] Algo bellman_ford step 6979 current loss 0.024245, current_train_items 223360.
I0304 19:31:23.289761 23128000471168 run.py:483] Algo bellman_ford step 6980 current loss 0.025489, current_train_items 223392.
I0304 19:31:23.306870 23128000471168 run.py:483] Algo bellman_ford step 6981 current loss 0.004603, current_train_items 223424.
I0304 19:31:23.329872 23128000471168 run.py:483] Algo bellman_ford step 6982 current loss 0.032123, current_train_items 223456.
I0304 19:31:23.362127 23128000471168 run.py:483] Algo bellman_ford step 6983 current loss 0.048226, current_train_items 223488.
I0304 19:31:23.396071 23128000471168 run.py:483] Algo bellman_ford step 6984 current loss 0.079491, current_train_items 223520.
I0304 19:31:23.416241 23128000471168 run.py:483] Algo bellman_ford step 6985 current loss 0.002758, current_train_items 223552.
I0304 19:31:23.432604 23128000471168 run.py:483] Algo bellman_ford step 6986 current loss 0.011735, current_train_items 223584.
I0304 19:31:23.456193 23128000471168 run.py:483] Algo bellman_ford step 6987 current loss 0.025287, current_train_items 223616.
I0304 19:31:23.487744 23128000471168 run.py:483] Algo bellman_ford step 6988 current loss 0.073181, current_train_items 223648.
I0304 19:31:23.523893 23128000471168 run.py:483] Algo bellman_ford step 6989 current loss 0.076737, current_train_items 223680.
I0304 19:31:23.544084 23128000471168 run.py:483] Algo bellman_ford step 6990 current loss 0.002014, current_train_items 223712.
I0304 19:31:23.560464 23128000471168 run.py:483] Algo bellman_ford step 6991 current loss 0.016599, current_train_items 223744.
I0304 19:31:23.584307 23128000471168 run.py:483] Algo bellman_ford step 6992 current loss 0.053761, current_train_items 223776.
I0304 19:31:23.616626 23128000471168 run.py:483] Algo bellman_ford step 6993 current loss 0.083202, current_train_items 223808.
I0304 19:31:23.653160 23128000471168 run.py:483] Algo bellman_ford step 6994 current loss 0.051183, current_train_items 223840.
I0304 19:31:23.673025 23128000471168 run.py:483] Algo bellman_ford step 6995 current loss 0.003247, current_train_items 223872.
I0304 19:31:23.689731 23128000471168 run.py:483] Algo bellman_ford step 6996 current loss 0.004631, current_train_items 223904.
I0304 19:31:23.712729 23128000471168 run.py:483] Algo bellman_ford step 6997 current loss 0.034009, current_train_items 223936.
I0304 19:31:23.743358 23128000471168 run.py:483] Algo bellman_ford step 6998 current loss 0.024907, current_train_items 223968.
I0304 19:31:23.777393 23128000471168 run.py:483] Algo bellman_ford step 6999 current loss 0.117996, current_train_items 224000.
I0304 19:31:23.797935 23128000471168 run.py:483] Algo bellman_ford step 7000 current loss 0.002892, current_train_items 224032.
I0304 19:31:23.805854 23128000471168 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0304 19:31:23.805960 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:23.822794 23128000471168 run.py:483] Algo bellman_ford step 7001 current loss 0.022286, current_train_items 224064.
I0304 19:31:23.846508 23128000471168 run.py:483] Algo bellman_ford step 7002 current loss 0.018947, current_train_items 224096.
I0304 19:31:23.876906 23128000471168 run.py:483] Algo bellman_ford step 7003 current loss 0.038724, current_train_items 224128.
I0304 19:31:23.909809 23128000471168 run.py:483] Algo bellman_ford step 7004 current loss 0.057039, current_train_items 224160.
I0304 19:31:23.929795 23128000471168 run.py:483] Algo bellman_ford step 7005 current loss 0.002163, current_train_items 224192.
I0304 19:31:23.946271 23128000471168 run.py:483] Algo bellman_ford step 7006 current loss 0.046253, current_train_items 224224.
I0304 19:31:23.971045 23128000471168 run.py:483] Algo bellman_ford step 7007 current loss 0.076800, current_train_items 224256.
I0304 19:31:24.001072 23128000471168 run.py:483] Algo bellman_ford step 7008 current loss 0.032727, current_train_items 224288.
I0304 19:31:24.037376 23128000471168 run.py:483] Algo bellman_ford step 7009 current loss 0.070356, current_train_items 224320.
I0304 19:31:24.057321 23128000471168 run.py:483] Algo bellman_ford step 7010 current loss 0.003827, current_train_items 224352.
I0304 19:31:24.073965 23128000471168 run.py:483] Algo bellman_ford step 7011 current loss 0.006987, current_train_items 224384.
I0304 19:31:24.097713 23128000471168 run.py:483] Algo bellman_ford step 7012 current loss 0.034947, current_train_items 224416.
I0304 19:31:24.128593 23128000471168 run.py:483] Algo bellman_ford step 7013 current loss 0.042389, current_train_items 224448.
I0304 19:31:24.162865 23128000471168 run.py:483] Algo bellman_ford step 7014 current loss 0.084121, current_train_items 224480.
I0304 19:31:24.182623 23128000471168 run.py:483] Algo bellman_ford step 7015 current loss 0.002165, current_train_items 224512.
I0304 19:31:24.198985 23128000471168 run.py:483] Algo bellman_ford step 7016 current loss 0.004937, current_train_items 224544.
I0304 19:31:24.223220 23128000471168 run.py:483] Algo bellman_ford step 7017 current loss 0.023585, current_train_items 224576.
I0304 19:31:24.253911 23128000471168 run.py:483] Algo bellman_ford step 7018 current loss 0.028731, current_train_items 224608.
I0304 19:31:24.286731 23128000471168 run.py:483] Algo bellman_ford step 7019 current loss 0.030124, current_train_items 224640.
I0304 19:31:24.306377 23128000471168 run.py:483] Algo bellman_ford step 7020 current loss 0.002543, current_train_items 224672.
I0304 19:31:24.322640 23128000471168 run.py:483] Algo bellman_ford step 7021 current loss 0.023400, current_train_items 224704.
I0304 19:31:24.346339 23128000471168 run.py:483] Algo bellman_ford step 7022 current loss 0.028835, current_train_items 224736.
I0304 19:31:24.376864 23128000471168 run.py:483] Algo bellman_ford step 7023 current loss 0.030583, current_train_items 224768.
I0304 19:31:24.411108 23128000471168 run.py:483] Algo bellman_ford step 7024 current loss 0.044007, current_train_items 224800.
I0304 19:31:24.430870 23128000471168 run.py:483] Algo bellman_ford step 7025 current loss 0.001878, current_train_items 224832.
I0304 19:31:24.447822 23128000471168 run.py:483] Algo bellman_ford step 7026 current loss 0.014815, current_train_items 224864.
I0304 19:31:24.473289 23128000471168 run.py:483] Algo bellman_ford step 7027 current loss 0.055243, current_train_items 224896.
I0304 19:31:24.506372 23128000471168 run.py:483] Algo bellman_ford step 7028 current loss 0.050622, current_train_items 224928.
I0304 19:31:24.538196 23128000471168 run.py:483] Algo bellman_ford step 7029 current loss 0.029108, current_train_items 224960.
I0304 19:31:24.558034 23128000471168 run.py:483] Algo bellman_ford step 7030 current loss 0.001964, current_train_items 224992.
I0304 19:31:24.574485 23128000471168 run.py:483] Algo bellman_ford step 7031 current loss 0.010911, current_train_items 225024.
I0304 19:31:24.598583 23128000471168 run.py:483] Algo bellman_ford step 7032 current loss 0.027136, current_train_items 225056.
I0304 19:31:24.629466 23128000471168 run.py:483] Algo bellman_ford step 7033 current loss 0.027687, current_train_items 225088.
I0304 19:31:24.665089 23128000471168 run.py:483] Algo bellman_ford step 7034 current loss 0.036796, current_train_items 225120.
I0304 19:31:24.685474 23128000471168 run.py:483] Algo bellman_ford step 7035 current loss 0.002621, current_train_items 225152.
I0304 19:31:24.701887 23128000471168 run.py:483] Algo bellman_ford step 7036 current loss 0.016524, current_train_items 225184.
I0304 19:31:24.726053 23128000471168 run.py:483] Algo bellman_ford step 7037 current loss 0.051707, current_train_items 225216.
I0304 19:31:24.758044 23128000471168 run.py:483] Algo bellman_ford step 7038 current loss 0.064362, current_train_items 225248.
I0304 19:31:24.791199 23128000471168 run.py:483] Algo bellman_ford step 7039 current loss 0.044408, current_train_items 225280.
I0304 19:31:24.811218 23128000471168 run.py:483] Algo bellman_ford step 7040 current loss 0.004130, current_train_items 225312.
I0304 19:31:24.827661 23128000471168 run.py:483] Algo bellman_ford step 7041 current loss 0.023996, current_train_items 225344.
I0304 19:31:24.851641 23128000471168 run.py:483] Algo bellman_ford step 7042 current loss 0.028936, current_train_items 225376.
I0304 19:31:24.883561 23128000471168 run.py:483] Algo bellman_ford step 7043 current loss 0.049905, current_train_items 225408.
I0304 19:31:24.917674 23128000471168 run.py:483] Algo bellman_ford step 7044 current loss 0.044381, current_train_items 225440.
I0304 19:31:24.937622 23128000471168 run.py:483] Algo bellman_ford step 7045 current loss 0.001490, current_train_items 225472.
I0304 19:31:24.954287 23128000471168 run.py:483] Algo bellman_ford step 7046 current loss 0.020592, current_train_items 225504.
I0304 19:31:24.977916 23128000471168 run.py:483] Algo bellman_ford step 7047 current loss 0.089318, current_train_items 225536.
I0304 19:31:25.010011 23128000471168 run.py:483] Algo bellman_ford step 7048 current loss 0.078432, current_train_items 225568.
I0304 19:31:25.043509 23128000471168 run.py:483] Algo bellman_ford step 7049 current loss 0.061067, current_train_items 225600.
I0304 19:31:25.063359 23128000471168 run.py:483] Algo bellman_ford step 7050 current loss 0.001487, current_train_items 225632.
I0304 19:31:25.071704 23128000471168 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0304 19:31:25.071814 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:25.088542 23128000471168 run.py:483] Algo bellman_ford step 7051 current loss 0.004443, current_train_items 225664.
I0304 19:31:25.113552 23128000471168 run.py:483] Algo bellman_ford step 7052 current loss 0.131380, current_train_items 225696.
I0304 19:31:25.146972 23128000471168 run.py:483] Algo bellman_ford step 7053 current loss 0.093576, current_train_items 225728.
I0304 19:31:25.182433 23128000471168 run.py:483] Algo bellman_ford step 7054 current loss 0.091293, current_train_items 225760.
I0304 19:31:25.202457 23128000471168 run.py:483] Algo bellman_ford step 7055 current loss 0.003346, current_train_items 225792.
I0304 19:31:25.218664 23128000471168 run.py:483] Algo bellman_ford step 7056 current loss 0.021007, current_train_items 225824.
I0304 19:31:25.243517 23128000471168 run.py:483] Algo bellman_ford step 7057 current loss 0.068704, current_train_items 225856.
I0304 19:31:25.274794 23128000471168 run.py:483] Algo bellman_ford step 7058 current loss 0.073849, current_train_items 225888.
I0304 19:31:25.310462 23128000471168 run.py:483] Algo bellman_ford step 7059 current loss 0.105475, current_train_items 225920.
I0304 19:31:25.330738 23128000471168 run.py:483] Algo bellman_ford step 7060 current loss 0.006321, current_train_items 225952.
I0304 19:31:25.347347 23128000471168 run.py:483] Algo bellman_ford step 7061 current loss 0.024554, current_train_items 225984.
I0304 19:31:25.372507 23128000471168 run.py:483] Algo bellman_ford step 7062 current loss 0.031039, current_train_items 226016.
I0304 19:31:25.401662 23128000471168 run.py:483] Algo bellman_ford step 7063 current loss 0.033690, current_train_items 226048.
I0304 19:31:25.435227 23128000471168 run.py:483] Algo bellman_ford step 7064 current loss 0.030909, current_train_items 226080.
I0304 19:31:25.455203 23128000471168 run.py:483] Algo bellman_ford step 7065 current loss 0.002839, current_train_items 226112.
I0304 19:31:25.472027 23128000471168 run.py:483] Algo bellman_ford step 7066 current loss 0.021140, current_train_items 226144.
I0304 19:31:25.495697 23128000471168 run.py:483] Algo bellman_ford step 7067 current loss 0.017377, current_train_items 226176.
I0304 19:31:25.529098 23128000471168 run.py:483] Algo bellman_ford step 7068 current loss 0.063252, current_train_items 226208.
I0304 19:31:25.564671 23128000471168 run.py:483] Algo bellman_ford step 7069 current loss 0.048937, current_train_items 226240.
I0304 19:31:25.584925 23128000471168 run.py:483] Algo bellman_ford step 7070 current loss 0.034046, current_train_items 226272.
I0304 19:31:25.601322 23128000471168 run.py:483] Algo bellman_ford step 7071 current loss 0.017049, current_train_items 226304.
I0304 19:31:25.623544 23128000471168 run.py:483] Algo bellman_ford step 7072 current loss 0.041713, current_train_items 226336.
I0304 19:31:25.654566 23128000471168 run.py:483] Algo bellman_ford step 7073 current loss 0.039329, current_train_items 226368.
I0304 19:31:25.688203 23128000471168 run.py:483] Algo bellman_ford step 7074 current loss 0.051730, current_train_items 226400.
I0304 19:31:25.708418 23128000471168 run.py:483] Algo bellman_ford step 7075 current loss 0.003262, current_train_items 226432.
I0304 19:31:25.724300 23128000471168 run.py:483] Algo bellman_ford step 7076 current loss 0.007631, current_train_items 226464.
I0304 19:31:25.747953 23128000471168 run.py:483] Algo bellman_ford step 7077 current loss 0.042753, current_train_items 226496.
I0304 19:31:25.779701 23128000471168 run.py:483] Algo bellman_ford step 7078 current loss 0.035513, current_train_items 226528.
I0304 19:31:25.814519 23128000471168 run.py:483] Algo bellman_ford step 7079 current loss 0.061707, current_train_items 226560.
I0304 19:31:25.834512 23128000471168 run.py:483] Algo bellman_ford step 7080 current loss 0.005255, current_train_items 226592.
I0304 19:31:25.850617 23128000471168 run.py:483] Algo bellman_ford step 7081 current loss 0.006298, current_train_items 226624.
I0304 19:31:25.874347 23128000471168 run.py:483] Algo bellman_ford step 7082 current loss 0.057218, current_train_items 226656.
I0304 19:31:25.906158 23128000471168 run.py:483] Algo bellman_ford step 7083 current loss 0.092244, current_train_items 226688.
I0304 19:31:25.939625 23128000471168 run.py:483] Algo bellman_ford step 7084 current loss 0.076802, current_train_items 226720.
I0304 19:31:25.959791 23128000471168 run.py:483] Algo bellman_ford step 7085 current loss 0.002631, current_train_items 226752.
I0304 19:31:25.976099 23128000471168 run.py:483] Algo bellman_ford step 7086 current loss 0.010161, current_train_items 226784.
I0304 19:31:25.998779 23128000471168 run.py:483] Algo bellman_ford step 7087 current loss 0.010609, current_train_items 226816.
I0304 19:31:26.030399 23128000471168 run.py:483] Algo bellman_ford step 7088 current loss 0.096525, current_train_items 226848.
I0304 19:31:26.063813 23128000471168 run.py:483] Algo bellman_ford step 7089 current loss 0.046254, current_train_items 226880.
I0304 19:31:26.084247 23128000471168 run.py:483] Algo bellman_ford step 7090 current loss 0.009788, current_train_items 226912.
I0304 19:31:26.100524 23128000471168 run.py:483] Algo bellman_ford step 7091 current loss 0.022417, current_train_items 226944.
I0304 19:31:26.124615 23128000471168 run.py:483] Algo bellman_ford step 7092 current loss 0.018486, current_train_items 226976.
I0304 19:31:26.157054 23128000471168 run.py:483] Algo bellman_ford step 7093 current loss 0.049087, current_train_items 227008.
I0304 19:31:26.189224 23128000471168 run.py:483] Algo bellman_ford step 7094 current loss 0.062610, current_train_items 227040.
I0304 19:31:26.209142 23128000471168 run.py:483] Algo bellman_ford step 7095 current loss 0.004285, current_train_items 227072.
I0304 19:31:26.225484 23128000471168 run.py:483] Algo bellman_ford step 7096 current loss 0.012771, current_train_items 227104.
I0304 19:31:26.248320 23128000471168 run.py:483] Algo bellman_ford step 7097 current loss 0.054025, current_train_items 227136.
I0304 19:31:26.280396 23128000471168 run.py:483] Algo bellman_ford step 7098 current loss 0.038154, current_train_items 227168.
I0304 19:31:26.313107 23128000471168 run.py:483] Algo bellman_ford step 7099 current loss 0.049187, current_train_items 227200.
I0304 19:31:26.333489 23128000471168 run.py:483] Algo bellman_ford step 7100 current loss 0.001657, current_train_items 227232.
I0304 19:31:26.341075 23128000471168 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0304 19:31:26.341186 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:26.358331 23128000471168 run.py:483] Algo bellman_ford step 7101 current loss 0.017653, current_train_items 227264.
I0304 19:31:26.382719 23128000471168 run.py:483] Algo bellman_ford step 7102 current loss 0.016289, current_train_items 227296.
I0304 19:31:26.415049 23128000471168 run.py:483] Algo bellman_ford step 7103 current loss 0.056113, current_train_items 227328.
I0304 19:31:26.450327 23128000471168 run.py:483] Algo bellman_ford step 7104 current loss 0.098933, current_train_items 227360.
I0304 19:31:26.470474 23128000471168 run.py:483] Algo bellman_ford step 7105 current loss 0.013373, current_train_items 227392.
I0304 19:31:26.486829 23128000471168 run.py:483] Algo bellman_ford step 7106 current loss 0.023031, current_train_items 227424.
I0304 19:31:26.511302 23128000471168 run.py:483] Algo bellman_ford step 7107 current loss 0.021519, current_train_items 227456.
I0304 19:31:26.542978 23128000471168 run.py:483] Algo bellman_ford step 7108 current loss 0.074708, current_train_items 227488.
I0304 19:31:26.578883 23128000471168 run.py:483] Algo bellman_ford step 7109 current loss 0.056929, current_train_items 227520.
I0304 19:31:26.598867 23128000471168 run.py:483] Algo bellman_ford step 7110 current loss 0.001973, current_train_items 227552.
I0304 19:31:26.615469 23128000471168 run.py:483] Algo bellman_ford step 7111 current loss 0.027150, current_train_items 227584.
I0304 19:31:26.639750 23128000471168 run.py:483] Algo bellman_ford step 7112 current loss 0.023432, current_train_items 227616.
I0304 19:31:26.671791 23128000471168 run.py:483] Algo bellman_ford step 7113 current loss 0.050886, current_train_items 227648.
I0304 19:31:26.705813 23128000471168 run.py:483] Algo bellman_ford step 7114 current loss 0.032128, current_train_items 227680.
I0304 19:31:26.725759 23128000471168 run.py:483] Algo bellman_ford step 7115 current loss 0.002221, current_train_items 227712.
I0304 19:31:26.742255 23128000471168 run.py:483] Algo bellman_ford step 7116 current loss 0.025441, current_train_items 227744.
I0304 19:31:26.766612 23128000471168 run.py:483] Algo bellman_ford step 7117 current loss 0.041488, current_train_items 227776.
I0304 19:31:26.798493 23128000471168 run.py:483] Algo bellman_ford step 7118 current loss 0.055891, current_train_items 227808.
I0304 19:31:26.832325 23128000471168 run.py:483] Algo bellman_ford step 7119 current loss 0.048589, current_train_items 227840.
I0304 19:31:26.852399 23128000471168 run.py:483] Algo bellman_ford step 7120 current loss 0.024195, current_train_items 227872.
I0304 19:31:26.868945 23128000471168 run.py:483] Algo bellman_ford step 7121 current loss 0.024899, current_train_items 227904.
I0304 19:31:26.892866 23128000471168 run.py:483] Algo bellman_ford step 7122 current loss 0.031459, current_train_items 227936.
I0304 19:31:26.923862 23128000471168 run.py:483] Algo bellman_ford step 7123 current loss 0.050825, current_train_items 227968.
I0304 19:31:26.958423 23128000471168 run.py:483] Algo bellman_ford step 7124 current loss 0.134723, current_train_items 228000.
I0304 19:31:26.978105 23128000471168 run.py:483] Algo bellman_ford step 7125 current loss 0.004095, current_train_items 228032.
I0304 19:31:26.993912 23128000471168 run.py:483] Algo bellman_ford step 7126 current loss 0.007664, current_train_items 228064.
I0304 19:31:27.017068 23128000471168 run.py:483] Algo bellman_ford step 7127 current loss 0.039681, current_train_items 228096.
I0304 19:31:27.047723 23128000471168 run.py:483] Algo bellman_ford step 7128 current loss 0.088323, current_train_items 228128.
I0304 19:31:27.083147 23128000471168 run.py:483] Algo bellman_ford step 7129 current loss 0.154601, current_train_items 228160.
I0304 19:31:27.102982 23128000471168 run.py:483] Algo bellman_ford step 7130 current loss 0.006070, current_train_items 228192.
I0304 19:31:27.119697 23128000471168 run.py:483] Algo bellman_ford step 7131 current loss 0.037602, current_train_items 228224.
I0304 19:31:27.144132 23128000471168 run.py:483] Algo bellman_ford step 7132 current loss 0.051121, current_train_items 228256.
I0304 19:31:27.175508 23128000471168 run.py:483] Algo bellman_ford step 7133 current loss 0.054104, current_train_items 228288.
I0304 19:31:27.208625 23128000471168 run.py:483] Algo bellman_ford step 7134 current loss 0.167747, current_train_items 228320.
I0304 19:31:27.228440 23128000471168 run.py:483] Algo bellman_ford step 7135 current loss 0.035480, current_train_items 228352.
I0304 19:31:27.244503 23128000471168 run.py:483] Algo bellman_ford step 7136 current loss 0.018243, current_train_items 228384.
I0304 19:31:27.267670 23128000471168 run.py:483] Algo bellman_ford step 7137 current loss 0.032419, current_train_items 228416.
I0304 19:31:27.297638 23128000471168 run.py:483] Algo bellman_ford step 7138 current loss 0.081181, current_train_items 228448.
I0304 19:31:27.333846 23128000471168 run.py:483] Algo bellman_ford step 7139 current loss 0.113823, current_train_items 228480.
I0304 19:31:27.353889 23128000471168 run.py:483] Algo bellman_ford step 7140 current loss 0.005918, current_train_items 228512.
I0304 19:31:27.369993 23128000471168 run.py:483] Algo bellman_ford step 7141 current loss 0.020743, current_train_items 228544.
I0304 19:31:27.394760 23128000471168 run.py:483] Algo bellman_ford step 7142 current loss 0.056400, current_train_items 228576.
I0304 19:31:27.424370 23128000471168 run.py:483] Algo bellman_ford step 7143 current loss 0.036232, current_train_items 228608.
I0304 19:31:27.457864 23128000471168 run.py:483] Algo bellman_ford step 7144 current loss 0.067214, current_train_items 228640.
I0304 19:31:27.477725 23128000471168 run.py:483] Algo bellman_ford step 7145 current loss 0.005722, current_train_items 228672.
I0304 19:31:27.494506 23128000471168 run.py:483] Algo bellman_ford step 7146 current loss 0.014692, current_train_items 228704.
I0304 19:31:27.519066 23128000471168 run.py:483] Algo bellman_ford step 7147 current loss 0.057836, current_train_items 228736.
I0304 19:31:27.549244 23128000471168 run.py:483] Algo bellman_ford step 7148 current loss 0.048890, current_train_items 228768.
I0304 19:31:27.580647 23128000471168 run.py:483] Algo bellman_ford step 7149 current loss 0.077023, current_train_items 228800.
I0304 19:31:27.600372 23128000471168 run.py:483] Algo bellman_ford step 7150 current loss 0.001630, current_train_items 228832.
I0304 19:31:27.608359 23128000471168 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0304 19:31:27.608468 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:27.625928 23128000471168 run.py:483] Algo bellman_ford step 7151 current loss 0.055642, current_train_items 228864.
I0304 19:31:27.651507 23128000471168 run.py:483] Algo bellman_ford step 7152 current loss 0.055772, current_train_items 228896.
I0304 19:31:27.683645 23128000471168 run.py:483] Algo bellman_ford step 7153 current loss 0.054356, current_train_items 228928.
I0304 19:31:27.717228 23128000471168 run.py:483] Algo bellman_ford step 7154 current loss 0.046400, current_train_items 228960.
I0304 19:31:27.737380 23128000471168 run.py:483] Algo bellman_ford step 7155 current loss 0.008300, current_train_items 228992.
I0304 19:31:27.753746 23128000471168 run.py:483] Algo bellman_ford step 7156 current loss 0.022718, current_train_items 229024.
I0304 19:31:27.777761 23128000471168 run.py:483] Algo bellman_ford step 7157 current loss 0.048590, current_train_items 229056.
I0304 19:31:27.809123 23128000471168 run.py:483] Algo bellman_ford step 7158 current loss 0.051663, current_train_items 229088.
I0304 19:31:27.843358 23128000471168 run.py:483] Algo bellman_ford step 7159 current loss 0.085248, current_train_items 229120.
I0304 19:31:27.863825 23128000471168 run.py:483] Algo bellman_ford step 7160 current loss 0.002237, current_train_items 229152.
I0304 19:31:27.880655 23128000471168 run.py:483] Algo bellman_ford step 7161 current loss 0.036184, current_train_items 229184.
W0304 19:31:27.895496 23128000471168 samplers.py:155] Increasing hint lengh from 10 to 11
I0304 19:31:34.869798 23128000471168 run.py:483] Algo bellman_ford step 7162 current loss 0.062222, current_train_items 229216.
I0304 19:31:34.902861 23128000471168 run.py:483] Algo bellman_ford step 7163 current loss 0.029263, current_train_items 229248.
I0304 19:31:34.938644 23128000471168 run.py:483] Algo bellman_ford step 7164 current loss 0.043745, current_train_items 229280.
I0304 19:31:34.959305 23128000471168 run.py:483] Algo bellman_ford step 7165 current loss 0.003011, current_train_items 229312.
I0304 19:31:34.975945 23128000471168 run.py:483] Algo bellman_ford step 7166 current loss 0.020734, current_train_items 229344.
I0304 19:31:35.000343 23128000471168 run.py:483] Algo bellman_ford step 7167 current loss 0.041389, current_train_items 229376.
I0304 19:31:35.032598 23128000471168 run.py:483] Algo bellman_ford step 7168 current loss 0.059079, current_train_items 229408.
I0304 19:31:35.064267 23128000471168 run.py:483] Algo bellman_ford step 7169 current loss 0.056862, current_train_items 229440.
I0304 19:31:35.085133 23128000471168 run.py:483] Algo bellman_ford step 7170 current loss 0.003019, current_train_items 229472.
I0304 19:31:35.102192 23128000471168 run.py:483] Algo bellman_ford step 7171 current loss 0.014324, current_train_items 229504.
I0304 19:31:35.126390 23128000471168 run.py:483] Algo bellman_ford step 7172 current loss 0.042272, current_train_items 229536.
I0304 19:31:35.158510 23128000471168 run.py:483] Algo bellman_ford step 7173 current loss 0.034761, current_train_items 229568.
I0304 19:31:35.193563 23128000471168 run.py:483] Algo bellman_ford step 7174 current loss 0.054932, current_train_items 229600.
I0304 19:31:35.214316 23128000471168 run.py:483] Algo bellman_ford step 7175 current loss 0.002869, current_train_items 229632.
I0304 19:31:35.230797 23128000471168 run.py:483] Algo bellman_ford step 7176 current loss 0.011778, current_train_items 229664.
I0304 19:31:35.255210 23128000471168 run.py:483] Algo bellman_ford step 7177 current loss 0.020200, current_train_items 229696.
I0304 19:31:35.289564 23128000471168 run.py:483] Algo bellman_ford step 7178 current loss 0.057031, current_train_items 229728.
I0304 19:31:35.324728 23128000471168 run.py:483] Algo bellman_ford step 7179 current loss 0.048848, current_train_items 229760.
I0304 19:31:35.344971 23128000471168 run.py:483] Algo bellman_ford step 7180 current loss 0.002956, current_train_items 229792.
I0304 19:31:35.361480 23128000471168 run.py:483] Algo bellman_ford step 7181 current loss 0.038883, current_train_items 229824.
I0304 19:31:35.386723 23128000471168 run.py:483] Algo bellman_ford step 7182 current loss 0.074456, current_train_items 229856.
I0304 19:31:35.418677 23128000471168 run.py:483] Algo bellman_ford step 7183 current loss 0.042721, current_train_items 229888.
I0304 19:31:35.456020 23128000471168 run.py:483] Algo bellman_ford step 7184 current loss 0.070192, current_train_items 229920.
I0304 19:31:35.476433 23128000471168 run.py:483] Algo bellman_ford step 7185 current loss 0.001904, current_train_items 229952.
I0304 19:31:35.493017 23128000471168 run.py:483] Algo bellman_ford step 7186 current loss 0.019733, current_train_items 229984.
I0304 19:31:35.518198 23128000471168 run.py:483] Algo bellman_ford step 7187 current loss 0.059337, current_train_items 230016.
I0304 19:31:35.550498 23128000471168 run.py:483] Algo bellman_ford step 7188 current loss 0.047127, current_train_items 230048.
I0304 19:31:35.586143 23128000471168 run.py:483] Algo bellman_ford step 7189 current loss 0.052880, current_train_items 230080.
I0304 19:31:35.606708 23128000471168 run.py:483] Algo bellman_ford step 7190 current loss 0.002481, current_train_items 230112.
I0304 19:31:35.623798 23128000471168 run.py:483] Algo bellman_ford step 7191 current loss 0.046407, current_train_items 230144.
I0304 19:31:35.648400 23128000471168 run.py:483] Algo bellman_ford step 7192 current loss 0.047002, current_train_items 230176.
I0304 19:31:35.680868 23128000471168 run.py:483] Algo bellman_ford step 7193 current loss 0.049693, current_train_items 230208.
I0304 19:31:35.714493 23128000471168 run.py:483] Algo bellman_ford step 7194 current loss 0.054838, current_train_items 230240.
I0304 19:31:35.734619 23128000471168 run.py:483] Algo bellman_ford step 7195 current loss 0.002887, current_train_items 230272.
I0304 19:31:35.750938 23128000471168 run.py:483] Algo bellman_ford step 7196 current loss 0.015358, current_train_items 230304.
I0304 19:31:35.775326 23128000471168 run.py:483] Algo bellman_ford step 7197 current loss 0.059948, current_train_items 230336.
I0304 19:31:35.808136 23128000471168 run.py:483] Algo bellman_ford step 7198 current loss 0.155842, current_train_items 230368.
I0304 19:31:35.842294 23128000471168 run.py:483] Algo bellman_ford step 7199 current loss 0.128804, current_train_items 230400.
I0304 19:31:35.863108 23128000471168 run.py:483] Algo bellman_ford step 7200 current loss 0.012450, current_train_items 230432.
I0304 19:31:35.872537 23128000471168 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0304 19:31:35.872686 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:35.889956 23128000471168 run.py:483] Algo bellman_ford step 7201 current loss 0.016838, current_train_items 230464.
I0304 19:31:35.914572 23128000471168 run.py:483] Algo bellman_ford step 7202 current loss 0.028580, current_train_items 230496.
I0304 19:31:35.948469 23128000471168 run.py:483] Algo bellman_ford step 7203 current loss 0.069211, current_train_items 230528.
I0304 19:31:35.982738 23128000471168 run.py:483] Algo bellman_ford step 7204 current loss 0.097623, current_train_items 230560.
I0304 19:31:36.003365 23128000471168 run.py:483] Algo bellman_ford step 7205 current loss 0.006026, current_train_items 230592.
I0304 19:31:36.019406 23128000471168 run.py:483] Algo bellman_ford step 7206 current loss 0.024791, current_train_items 230624.
I0304 19:31:36.044318 23128000471168 run.py:483] Algo bellman_ford step 7207 current loss 0.040624, current_train_items 230656.
I0304 19:31:36.076614 23128000471168 run.py:483] Algo bellman_ford step 7208 current loss 0.037940, current_train_items 230688.
I0304 19:31:36.111901 23128000471168 run.py:483] Algo bellman_ford step 7209 current loss 0.075264, current_train_items 230720.
I0304 19:31:36.132364 23128000471168 run.py:483] Algo bellman_ford step 7210 current loss 0.001925, current_train_items 230752.
I0304 19:31:36.148704 23128000471168 run.py:483] Algo bellman_ford step 7211 current loss 0.005816, current_train_items 230784.
I0304 19:31:36.172757 23128000471168 run.py:483] Algo bellman_ford step 7212 current loss 0.049276, current_train_items 230816.
I0304 19:31:36.204803 23128000471168 run.py:483] Algo bellman_ford step 7213 current loss 0.035954, current_train_items 230848.
I0304 19:31:36.237138 23128000471168 run.py:483] Algo bellman_ford step 7214 current loss 0.042817, current_train_items 230880.
I0304 19:31:36.257506 23128000471168 run.py:483] Algo bellman_ford step 7215 current loss 0.003759, current_train_items 230912.
I0304 19:31:36.274038 23128000471168 run.py:483] Algo bellman_ford step 7216 current loss 0.007313, current_train_items 230944.
I0304 19:31:36.298837 23128000471168 run.py:483] Algo bellman_ford step 7217 current loss 0.036023, current_train_items 230976.
I0304 19:31:36.331066 23128000471168 run.py:483] Algo bellman_ford step 7218 current loss 0.080821, current_train_items 231008.
I0304 19:31:36.364471 23128000471168 run.py:483] Algo bellman_ford step 7219 current loss 0.063544, current_train_items 231040.
I0304 19:31:36.384715 23128000471168 run.py:483] Algo bellman_ford step 7220 current loss 0.025033, current_train_items 231072.
I0304 19:31:36.401286 23128000471168 run.py:483] Algo bellman_ford step 7221 current loss 0.011025, current_train_items 231104.
I0304 19:31:36.426648 23128000471168 run.py:483] Algo bellman_ford step 7222 current loss 0.046438, current_train_items 231136.
I0304 19:31:36.458910 23128000471168 run.py:483] Algo bellman_ford step 7223 current loss 0.088228, current_train_items 231168.
I0304 19:31:36.493979 23128000471168 run.py:483] Algo bellman_ford step 7224 current loss 0.081052, current_train_items 231200.
I0304 19:31:36.514465 23128000471168 run.py:483] Algo bellman_ford step 7225 current loss 0.002278, current_train_items 231232.
I0304 19:31:36.531054 23128000471168 run.py:483] Algo bellman_ford step 7226 current loss 0.018696, current_train_items 231264.
I0304 19:31:36.556608 23128000471168 run.py:483] Algo bellman_ford step 7227 current loss 0.053069, current_train_items 231296.
I0304 19:31:36.588889 23128000471168 run.py:483] Algo bellman_ford step 7228 current loss 0.038035, current_train_items 231328.
I0304 19:31:36.621595 23128000471168 run.py:483] Algo bellman_ford step 7229 current loss 0.063201, current_train_items 231360.
I0304 19:31:36.641496 23128000471168 run.py:483] Algo bellman_ford step 7230 current loss 0.002935, current_train_items 231392.
I0304 19:31:36.658176 23128000471168 run.py:483] Algo bellman_ford step 7231 current loss 0.026949, current_train_items 231424.
I0304 19:31:36.680479 23128000471168 run.py:483] Algo bellman_ford step 7232 current loss 0.023067, current_train_items 231456.
I0304 19:31:36.713483 23128000471168 run.py:483] Algo bellman_ford step 7233 current loss 0.053359, current_train_items 231488.
I0304 19:31:36.746530 23128000471168 run.py:483] Algo bellman_ford step 7234 current loss 0.051686, current_train_items 231520.
I0304 19:31:36.766587 23128000471168 run.py:483] Algo bellman_ford step 7235 current loss 0.001323, current_train_items 231552.
I0304 19:31:36.783343 23128000471168 run.py:483] Algo bellman_ford step 7236 current loss 0.018399, current_train_items 231584.
I0304 19:31:36.808556 23128000471168 run.py:483] Algo bellman_ford step 7237 current loss 0.020455, current_train_items 231616.
I0304 19:31:36.842418 23128000471168 run.py:483] Algo bellman_ford step 7238 current loss 0.102901, current_train_items 231648.
I0304 19:31:36.876648 23128000471168 run.py:483] Algo bellman_ford step 7239 current loss 0.059716, current_train_items 231680.
I0304 19:31:36.896523 23128000471168 run.py:483] Algo bellman_ford step 7240 current loss 0.008228, current_train_items 231712.
I0304 19:31:36.912919 23128000471168 run.py:483] Algo bellman_ford step 7241 current loss 0.006974, current_train_items 231744.
I0304 19:31:36.938662 23128000471168 run.py:483] Algo bellman_ford step 7242 current loss 0.072501, current_train_items 231776.
I0304 19:31:36.971216 23128000471168 run.py:483] Algo bellman_ford step 7243 current loss 0.073953, current_train_items 231808.
I0304 19:31:37.006981 23128000471168 run.py:483] Algo bellman_ford step 7244 current loss 0.064344, current_train_items 231840.
I0304 19:31:37.026931 23128000471168 run.py:483] Algo bellman_ford step 7245 current loss 0.002117, current_train_items 231872.
I0304 19:31:37.043479 23128000471168 run.py:483] Algo bellman_ford step 7246 current loss 0.004467, current_train_items 231904.
I0304 19:31:37.068475 23128000471168 run.py:483] Algo bellman_ford step 7247 current loss 0.026545, current_train_items 231936.
I0304 19:31:37.102237 23128000471168 run.py:483] Algo bellman_ford step 7248 current loss 0.100665, current_train_items 231968.
I0304 19:31:37.137273 23128000471168 run.py:483] Algo bellman_ford step 7249 current loss 0.054682, current_train_items 232000.
I0304 19:31:37.157555 23128000471168 run.py:483] Algo bellman_ford step 7250 current loss 0.011144, current_train_items 232032.
I0304 19:31:37.166034 23128000471168 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0304 19:31:37.166144 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:37.183461 23128000471168 run.py:483] Algo bellman_ford step 7251 current loss 0.026575, current_train_items 232064.
I0304 19:31:37.208287 23128000471168 run.py:483] Algo bellman_ford step 7252 current loss 0.038574, current_train_items 232096.
I0304 19:31:37.240073 23128000471168 run.py:483] Algo bellman_ford step 7253 current loss 0.069880, current_train_items 232128.
I0304 19:31:37.274735 23128000471168 run.py:483] Algo bellman_ford step 7254 current loss 0.060357, current_train_items 232160.
I0304 19:31:37.295330 23128000471168 run.py:483] Algo bellman_ford step 7255 current loss 0.001874, current_train_items 232192.
I0304 19:31:37.311268 23128000471168 run.py:483] Algo bellman_ford step 7256 current loss 0.020238, current_train_items 232224.
I0304 19:31:37.335509 23128000471168 run.py:483] Algo bellman_ford step 7257 current loss 0.052032, current_train_items 232256.
I0304 19:31:37.370301 23128000471168 run.py:483] Algo bellman_ford step 7258 current loss 0.072628, current_train_items 232288.
I0304 19:31:37.405000 23128000471168 run.py:483] Algo bellman_ford step 7259 current loss 0.070531, current_train_items 232320.
I0304 19:31:37.425508 23128000471168 run.py:483] Algo bellman_ford step 7260 current loss 0.002443, current_train_items 232352.
I0304 19:31:37.442134 23128000471168 run.py:483] Algo bellman_ford step 7261 current loss 0.009935, current_train_items 232384.
I0304 19:31:37.466349 23128000471168 run.py:483] Algo bellman_ford step 7262 current loss 0.030166, current_train_items 232416.
I0304 19:31:37.498920 23128000471168 run.py:483] Algo bellman_ford step 7263 current loss 0.046704, current_train_items 232448.
I0304 19:31:37.533213 23128000471168 run.py:483] Algo bellman_ford step 7264 current loss 0.057217, current_train_items 232480.
I0304 19:31:37.553104 23128000471168 run.py:483] Algo bellman_ford step 7265 current loss 0.003714, current_train_items 232512.
I0304 19:31:37.569648 23128000471168 run.py:483] Algo bellman_ford step 7266 current loss 0.059228, current_train_items 232544.
I0304 19:31:37.594116 23128000471168 run.py:483] Algo bellman_ford step 7267 current loss 0.034483, current_train_items 232576.
I0304 19:31:37.626379 23128000471168 run.py:483] Algo bellman_ford step 7268 current loss 0.061941, current_train_items 232608.
I0304 19:31:37.659564 23128000471168 run.py:483] Algo bellman_ford step 7269 current loss 0.066792, current_train_items 232640.
I0304 19:31:37.679830 23128000471168 run.py:483] Algo bellman_ford step 7270 current loss 0.003825, current_train_items 232672.
I0304 19:31:37.696737 23128000471168 run.py:483] Algo bellman_ford step 7271 current loss 0.031533, current_train_items 232704.
I0304 19:31:37.721198 23128000471168 run.py:483] Algo bellman_ford step 7272 current loss 0.019354, current_train_items 232736.
I0304 19:31:37.753305 23128000471168 run.py:483] Algo bellman_ford step 7273 current loss 0.034552, current_train_items 232768.
I0304 19:31:37.787391 23128000471168 run.py:483] Algo bellman_ford step 7274 current loss 0.110563, current_train_items 232800.
I0304 19:31:37.807889 23128000471168 run.py:483] Algo bellman_ford step 7275 current loss 0.003730, current_train_items 232832.
I0304 19:31:37.824441 23128000471168 run.py:483] Algo bellman_ford step 7276 current loss 0.015055, current_train_items 232864.
I0304 19:31:37.848975 23128000471168 run.py:483] Algo bellman_ford step 7277 current loss 0.040339, current_train_items 232896.
I0304 19:31:37.882163 23128000471168 run.py:483] Algo bellman_ford step 7278 current loss 0.072668, current_train_items 232928.
I0304 19:31:37.916849 23128000471168 run.py:483] Algo bellman_ford step 7279 current loss 0.049310, current_train_items 232960.
I0304 19:31:37.936943 23128000471168 run.py:483] Algo bellman_ford step 7280 current loss 0.017017, current_train_items 232992.
I0304 19:31:37.953778 23128000471168 run.py:483] Algo bellman_ford step 7281 current loss 0.027540, current_train_items 233024.
I0304 19:31:37.977551 23128000471168 run.py:483] Algo bellman_ford step 7282 current loss 0.026781, current_train_items 233056.
I0304 19:31:38.010089 23128000471168 run.py:483] Algo bellman_ford step 7283 current loss 0.060304, current_train_items 233088.
I0304 19:31:38.043544 23128000471168 run.py:483] Algo bellman_ford step 7284 current loss 0.046751, current_train_items 233120.
I0304 19:31:38.064136 23128000471168 run.py:483] Algo bellman_ford step 7285 current loss 0.004586, current_train_items 233152.
I0304 19:31:38.081601 23128000471168 run.py:483] Algo bellman_ford step 7286 current loss 0.024069, current_train_items 233184.
I0304 19:31:38.105560 23128000471168 run.py:483] Algo bellman_ford step 7287 current loss 0.032562, current_train_items 233216.
I0304 19:31:38.137998 23128000471168 run.py:483] Algo bellman_ford step 7288 current loss 0.033987, current_train_items 233248.
I0304 19:31:38.171217 23128000471168 run.py:483] Algo bellman_ford step 7289 current loss 0.046870, current_train_items 233280.
I0304 19:31:38.191862 23128000471168 run.py:483] Algo bellman_ford step 7290 current loss 0.034503, current_train_items 233312.
I0304 19:31:38.208561 23128000471168 run.py:483] Algo bellman_ford step 7291 current loss 0.030802, current_train_items 233344.
I0304 19:31:38.231791 23128000471168 run.py:483] Algo bellman_ford step 7292 current loss 0.009626, current_train_items 233376.
I0304 19:31:38.264837 23128000471168 run.py:483] Algo bellman_ford step 7293 current loss 0.041631, current_train_items 233408.
I0304 19:31:38.297314 23128000471168 run.py:483] Algo bellman_ford step 7294 current loss 0.049691, current_train_items 233440.
I0304 19:31:38.317259 23128000471168 run.py:483] Algo bellman_ford step 7295 current loss 0.002838, current_train_items 233472.
I0304 19:31:38.333885 23128000471168 run.py:483] Algo bellman_ford step 7296 current loss 0.007198, current_train_items 233504.
I0304 19:31:38.358406 23128000471168 run.py:483] Algo bellman_ford step 7297 current loss 0.069251, current_train_items 233536.
I0304 19:31:38.391135 23128000471168 run.py:483] Algo bellman_ford step 7298 current loss 0.045488, current_train_items 233568.
I0304 19:31:38.425292 23128000471168 run.py:483] Algo bellman_ford step 7299 current loss 0.088165, current_train_items 233600.
I0304 19:31:38.445985 23128000471168 run.py:483] Algo bellman_ford step 7300 current loss 0.005400, current_train_items 233632.
I0304 19:31:38.453922 23128000471168 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0304 19:31:38.454038 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:38.471602 23128000471168 run.py:483] Algo bellman_ford step 7301 current loss 0.013058, current_train_items 233664.
I0304 19:31:38.496211 23128000471168 run.py:483] Algo bellman_ford step 7302 current loss 0.028167, current_train_items 233696.
I0304 19:31:38.528207 23128000471168 run.py:483] Algo bellman_ford step 7303 current loss 0.049372, current_train_items 233728.
I0304 19:31:38.563426 23128000471168 run.py:483] Algo bellman_ford step 7304 current loss 0.056457, current_train_items 233760.
I0304 19:31:38.583650 23128000471168 run.py:483] Algo bellman_ford step 7305 current loss 0.002083, current_train_items 233792.
I0304 19:31:38.599749 23128000471168 run.py:483] Algo bellman_ford step 7306 current loss 0.006609, current_train_items 233824.
I0304 19:31:38.623517 23128000471168 run.py:483] Algo bellman_ford step 7307 current loss 0.056318, current_train_items 233856.
I0304 19:31:38.654663 23128000471168 run.py:483] Algo bellman_ford step 7308 current loss 0.040598, current_train_items 233888.
I0304 19:31:38.689716 23128000471168 run.py:483] Algo bellman_ford step 7309 current loss 0.081065, current_train_items 233920.
I0304 19:31:38.710177 23128000471168 run.py:483] Algo bellman_ford step 7310 current loss 0.002192, current_train_items 233952.
I0304 19:31:38.726368 23128000471168 run.py:483] Algo bellman_ford step 7311 current loss 0.008354, current_train_items 233984.
I0304 19:31:38.752243 23128000471168 run.py:483] Algo bellman_ford step 7312 current loss 0.042121, current_train_items 234016.
I0304 19:31:38.784560 23128000471168 run.py:483] Algo bellman_ford step 7313 current loss 0.049996, current_train_items 234048.
I0304 19:31:38.819231 23128000471168 run.py:483] Algo bellman_ford step 7314 current loss 0.037515, current_train_items 234080.
I0304 19:31:38.839158 23128000471168 run.py:483] Algo bellman_ford step 7315 current loss 0.003548, current_train_items 234112.
I0304 19:31:38.855496 23128000471168 run.py:483] Algo bellman_ford step 7316 current loss 0.004151, current_train_items 234144.
I0304 19:31:38.880410 23128000471168 run.py:483] Algo bellman_ford step 7317 current loss 0.061917, current_train_items 234176.
I0304 19:31:38.912527 23128000471168 run.py:483] Algo bellman_ford step 7318 current loss 0.049454, current_train_items 234208.
I0304 19:31:38.947940 23128000471168 run.py:483] Algo bellman_ford step 7319 current loss 0.067877, current_train_items 234240.
I0304 19:31:38.967841 23128000471168 run.py:483] Algo bellman_ford step 7320 current loss 0.002231, current_train_items 234272.
I0304 19:31:38.984825 23128000471168 run.py:483] Algo bellman_ford step 7321 current loss 0.021105, current_train_items 234304.
I0304 19:31:39.010479 23128000471168 run.py:483] Algo bellman_ford step 7322 current loss 0.067004, current_train_items 234336.
I0304 19:31:39.042152 23128000471168 run.py:483] Algo bellman_ford step 7323 current loss 0.015725, current_train_items 234368.
I0304 19:31:39.077321 23128000471168 run.py:483] Algo bellman_ford step 7324 current loss 0.072893, current_train_items 234400.
I0304 19:31:39.097586 23128000471168 run.py:483] Algo bellman_ford step 7325 current loss 0.002477, current_train_items 234432.
I0304 19:31:39.114509 23128000471168 run.py:483] Algo bellman_ford step 7326 current loss 0.017774, current_train_items 234464.
I0304 19:31:39.138907 23128000471168 run.py:483] Algo bellman_ford step 7327 current loss 0.021398, current_train_items 234496.
I0304 19:31:39.170820 23128000471168 run.py:483] Algo bellman_ford step 7328 current loss 0.052791, current_train_items 234528.
I0304 19:31:39.205319 23128000471168 run.py:483] Algo bellman_ford step 7329 current loss 0.063318, current_train_items 234560.
I0304 19:31:39.225013 23128000471168 run.py:483] Algo bellman_ford step 7330 current loss 0.001965, current_train_items 234592.
I0304 19:31:39.241816 23128000471168 run.py:483] Algo bellman_ford step 7331 current loss 0.030599, current_train_items 234624.
I0304 19:31:39.267165 23128000471168 run.py:483] Algo bellman_ford step 7332 current loss 0.073418, current_train_items 234656.
I0304 19:31:39.300091 23128000471168 run.py:483] Algo bellman_ford step 7333 current loss 0.055120, current_train_items 234688.
I0304 19:31:39.332718 23128000471168 run.py:483] Algo bellman_ford step 7334 current loss 0.065170, current_train_items 234720.
I0304 19:31:39.352698 23128000471168 run.py:483] Algo bellman_ford step 7335 current loss 0.005183, current_train_items 234752.
I0304 19:31:39.368946 23128000471168 run.py:483] Algo bellman_ford step 7336 current loss 0.030321, current_train_items 234784.
I0304 19:31:39.393169 23128000471168 run.py:483] Algo bellman_ford step 7337 current loss 0.041333, current_train_items 234816.
I0304 19:31:39.425506 23128000471168 run.py:483] Algo bellman_ford step 7338 current loss 0.051367, current_train_items 234848.
I0304 19:31:39.461283 23128000471168 run.py:483] Algo bellman_ford step 7339 current loss 0.057975, current_train_items 234880.
I0304 19:31:39.481153 23128000471168 run.py:483] Algo bellman_ford step 7340 current loss 0.005038, current_train_items 234912.
I0304 19:31:39.497513 23128000471168 run.py:483] Algo bellman_ford step 7341 current loss 0.010677, current_train_items 234944.
I0304 19:31:39.522697 23128000471168 run.py:483] Algo bellman_ford step 7342 current loss 0.048622, current_train_items 234976.
I0304 19:31:39.555628 23128000471168 run.py:483] Algo bellman_ford step 7343 current loss 0.066860, current_train_items 235008.
I0304 19:31:39.589307 23128000471168 run.py:483] Algo bellman_ford step 7344 current loss 0.064709, current_train_items 235040.
I0304 19:31:39.609357 23128000471168 run.py:483] Algo bellman_ford step 7345 current loss 0.011399, current_train_items 235072.
I0304 19:31:39.625496 23128000471168 run.py:483] Algo bellman_ford step 7346 current loss 0.009408, current_train_items 235104.
I0304 19:31:39.650298 23128000471168 run.py:483] Algo bellman_ford step 7347 current loss 0.036148, current_train_items 235136.
I0304 19:31:39.682742 23128000471168 run.py:483] Algo bellman_ford step 7348 current loss 0.051225, current_train_items 235168.
I0304 19:31:39.713596 23128000471168 run.py:483] Algo bellman_ford step 7349 current loss 0.026356, current_train_items 235200.
I0304 19:31:39.733945 23128000471168 run.py:483] Algo bellman_ford step 7350 current loss 0.002487, current_train_items 235232.
I0304 19:31:39.742125 23128000471168 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0304 19:31:39.742234 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:39.759833 23128000471168 run.py:483] Algo bellman_ford step 7351 current loss 0.013068, current_train_items 235264.
I0304 19:31:39.785582 23128000471168 run.py:483] Algo bellman_ford step 7352 current loss 0.037793, current_train_items 235296.
I0304 19:31:39.819260 23128000471168 run.py:483] Algo bellman_ford step 7353 current loss 0.042203, current_train_items 235328.
I0304 19:31:39.853196 23128000471168 run.py:483] Algo bellman_ford step 7354 current loss 0.082253, current_train_items 235360.
I0304 19:31:39.873731 23128000471168 run.py:483] Algo bellman_ford step 7355 current loss 0.007061, current_train_items 235392.
I0304 19:31:39.889764 23128000471168 run.py:483] Algo bellman_ford step 7356 current loss 0.008554, current_train_items 235424.
I0304 19:31:39.915543 23128000471168 run.py:483] Algo bellman_ford step 7357 current loss 0.055214, current_train_items 235456.
I0304 19:31:39.947555 23128000471168 run.py:483] Algo bellman_ford step 7358 current loss 0.047314, current_train_items 235488.
I0304 19:31:39.981926 23128000471168 run.py:483] Algo bellman_ford step 7359 current loss 0.080526, current_train_items 235520.
I0304 19:31:40.002360 23128000471168 run.py:483] Algo bellman_ford step 7360 current loss 0.004418, current_train_items 235552.
I0304 19:31:40.019745 23128000471168 run.py:483] Algo bellman_ford step 7361 current loss 0.026749, current_train_items 235584.
I0304 19:31:40.043036 23128000471168 run.py:483] Algo bellman_ford step 7362 current loss 0.036824, current_train_items 235616.
I0304 19:31:40.077575 23128000471168 run.py:483] Algo bellman_ford step 7363 current loss 0.105854, current_train_items 235648.
I0304 19:31:40.110218 23128000471168 run.py:483] Algo bellman_ford step 7364 current loss 0.058259, current_train_items 235680.
I0304 19:31:40.130270 23128000471168 run.py:483] Algo bellman_ford step 7365 current loss 0.038508, current_train_items 235712.
I0304 19:31:40.146819 23128000471168 run.py:483] Algo bellman_ford step 7366 current loss 0.018502, current_train_items 235744.
I0304 19:31:40.171882 23128000471168 run.py:483] Algo bellman_ford step 7367 current loss 0.018600, current_train_items 235776.
I0304 19:31:40.206578 23128000471168 run.py:483] Algo bellman_ford step 7368 current loss 0.035210, current_train_items 235808.
I0304 19:31:40.239555 23128000471168 run.py:483] Algo bellman_ford step 7369 current loss 0.026860, current_train_items 235840.
I0304 19:31:40.260177 23128000471168 run.py:483] Algo bellman_ford step 7370 current loss 0.003023, current_train_items 235872.
I0304 19:31:40.276916 23128000471168 run.py:483] Algo bellman_ford step 7371 current loss 0.025646, current_train_items 235904.
I0304 19:31:40.300745 23128000471168 run.py:483] Algo bellman_ford step 7372 current loss 0.018990, current_train_items 235936.
I0304 19:31:40.332933 23128000471168 run.py:483] Algo bellman_ford step 7373 current loss 0.025405, current_train_items 235968.
I0304 19:31:40.366764 23128000471168 run.py:483] Algo bellman_ford step 7374 current loss 0.054178, current_train_items 236000.
I0304 19:31:40.387100 23128000471168 run.py:483] Algo bellman_ford step 7375 current loss 0.003297, current_train_items 236032.
I0304 19:31:40.404543 23128000471168 run.py:483] Algo bellman_ford step 7376 current loss 0.044429, current_train_items 236064.
I0304 19:31:40.429192 23128000471168 run.py:483] Algo bellman_ford step 7377 current loss 0.024484, current_train_items 236096.
I0304 19:31:40.461437 23128000471168 run.py:483] Algo bellman_ford step 7378 current loss 0.058176, current_train_items 236128.
I0304 19:31:40.494151 23128000471168 run.py:483] Algo bellman_ford step 7379 current loss 0.061433, current_train_items 236160.
I0304 19:31:40.514242 23128000471168 run.py:483] Algo bellman_ford step 7380 current loss 0.002997, current_train_items 236192.
I0304 19:31:40.530372 23128000471168 run.py:483] Algo bellman_ford step 7381 current loss 0.027488, current_train_items 236224.
I0304 19:31:40.555996 23128000471168 run.py:483] Algo bellman_ford step 7382 current loss 0.045453, current_train_items 236256.
I0304 19:31:40.587667 23128000471168 run.py:483] Algo bellman_ford step 7383 current loss 0.043727, current_train_items 236288.
I0304 19:31:40.620691 23128000471168 run.py:483] Algo bellman_ford step 7384 current loss 0.047776, current_train_items 236320.
I0304 19:31:40.641108 23128000471168 run.py:483] Algo bellman_ford step 7385 current loss 0.001840, current_train_items 236352.
I0304 19:31:40.657800 23128000471168 run.py:483] Algo bellman_ford step 7386 current loss 0.017367, current_train_items 236384.
I0304 19:31:40.682392 23128000471168 run.py:483] Algo bellman_ford step 7387 current loss 0.043946, current_train_items 236416.
I0304 19:31:40.715749 23128000471168 run.py:483] Algo bellman_ford step 7388 current loss 0.042761, current_train_items 236448.
I0304 19:31:40.749104 23128000471168 run.py:483] Algo bellman_ford step 7389 current loss 0.043676, current_train_items 236480.
I0304 19:31:40.769689 23128000471168 run.py:483] Algo bellman_ford step 7390 current loss 0.002201, current_train_items 236512.
I0304 19:31:40.786672 23128000471168 run.py:483] Algo bellman_ford step 7391 current loss 0.020330, current_train_items 236544.
I0304 19:31:40.811046 23128000471168 run.py:483] Algo bellman_ford step 7392 current loss 0.042335, current_train_items 236576.
I0304 19:31:40.842489 23128000471168 run.py:483] Algo bellman_ford step 7393 current loss 0.039048, current_train_items 236608.
I0304 19:31:40.876904 23128000471168 run.py:483] Algo bellman_ford step 7394 current loss 0.073450, current_train_items 236640.
I0304 19:31:40.897102 23128000471168 run.py:483] Algo bellman_ford step 7395 current loss 0.003500, current_train_items 236672.
I0304 19:31:40.913929 23128000471168 run.py:483] Algo bellman_ford step 7396 current loss 0.021761, current_train_items 236704.
I0304 19:31:40.939102 23128000471168 run.py:483] Algo bellman_ford step 7397 current loss 0.045484, current_train_items 236736.
I0304 19:31:40.970814 23128000471168 run.py:483] Algo bellman_ford step 7398 current loss 0.053580, current_train_items 236768.
I0304 19:31:41.003396 23128000471168 run.py:483] Algo bellman_ford step 7399 current loss 0.044057, current_train_items 236800.
I0304 19:31:41.023742 23128000471168 run.py:483] Algo bellman_ford step 7400 current loss 0.003731, current_train_items 236832.
I0304 19:31:41.031676 23128000471168 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0304 19:31:41.031785 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:41.049469 23128000471168 run.py:483] Algo bellman_ford step 7401 current loss 0.042897, current_train_items 236864.
I0304 19:31:41.075415 23128000471168 run.py:483] Algo bellman_ford step 7402 current loss 0.057884, current_train_items 236896.
I0304 19:31:41.108691 23128000471168 run.py:483] Algo bellman_ford step 7403 current loss 0.047379, current_train_items 236928.
I0304 19:31:41.143431 23128000471168 run.py:483] Algo bellman_ford step 7404 current loss 0.057788, current_train_items 236960.
I0304 19:31:41.164252 23128000471168 run.py:483] Algo bellman_ford step 7405 current loss 0.003286, current_train_items 236992.
I0304 19:31:41.180157 23128000471168 run.py:483] Algo bellman_ford step 7406 current loss 0.005455, current_train_items 237024.
I0304 19:31:41.205095 23128000471168 run.py:483] Algo bellman_ford step 7407 current loss 0.025395, current_train_items 237056.
I0304 19:31:41.237868 23128000471168 run.py:483] Algo bellman_ford step 7408 current loss 0.069975, current_train_items 237088.
I0304 19:31:41.270395 23128000471168 run.py:483] Algo bellman_ford step 7409 current loss 0.080452, current_train_items 237120.
I0304 19:31:41.290405 23128000471168 run.py:483] Algo bellman_ford step 7410 current loss 0.002098, current_train_items 237152.
I0304 19:31:41.307553 23128000471168 run.py:483] Algo bellman_ford step 7411 current loss 0.008398, current_train_items 237184.
I0304 19:31:41.333300 23128000471168 run.py:483] Algo bellman_ford step 7412 current loss 0.058143, current_train_items 237216.
I0304 19:31:41.365948 23128000471168 run.py:483] Algo bellman_ford step 7413 current loss 0.049436, current_train_items 237248.
I0304 19:31:41.398684 23128000471168 run.py:483] Algo bellman_ford step 7414 current loss 0.047183, current_train_items 237280.
I0304 19:31:41.418725 23128000471168 run.py:483] Algo bellman_ford step 7415 current loss 0.003385, current_train_items 237312.
I0304 19:31:41.434998 23128000471168 run.py:483] Algo bellman_ford step 7416 current loss 0.004138, current_train_items 237344.
I0304 19:31:41.459741 23128000471168 run.py:483] Algo bellman_ford step 7417 current loss 0.041167, current_train_items 237376.
I0304 19:31:41.491739 23128000471168 run.py:483] Algo bellman_ford step 7418 current loss 0.059171, current_train_items 237408.
I0304 19:31:41.525169 23128000471168 run.py:483] Algo bellman_ford step 7419 current loss 0.066174, current_train_items 237440.
I0304 19:31:41.545070 23128000471168 run.py:483] Algo bellman_ford step 7420 current loss 0.005925, current_train_items 237472.
I0304 19:31:41.561387 23128000471168 run.py:483] Algo bellman_ford step 7421 current loss 0.006666, current_train_items 237504.
I0304 19:31:41.585855 23128000471168 run.py:483] Algo bellman_ford step 7422 current loss 0.039332, current_train_items 237536.
I0304 19:31:41.617561 23128000471168 run.py:483] Algo bellman_ford step 7423 current loss 0.054372, current_train_items 237568.
I0304 19:31:41.651195 23128000471168 run.py:483] Algo bellman_ford step 7424 current loss 0.036834, current_train_items 237600.
I0304 19:31:41.671378 23128000471168 run.py:483] Algo bellman_ford step 7425 current loss 0.003177, current_train_items 237632.
I0304 19:31:41.687804 23128000471168 run.py:483] Algo bellman_ford step 7426 current loss 0.006748, current_train_items 237664.
I0304 19:31:41.712764 23128000471168 run.py:483] Algo bellman_ford step 7427 current loss 0.053792, current_train_items 237696.
I0304 19:31:41.744575 23128000471168 run.py:483] Algo bellman_ford step 7428 current loss 0.035115, current_train_items 237728.
I0304 19:31:41.779100 23128000471168 run.py:483] Algo bellman_ford step 7429 current loss 0.058685, current_train_items 237760.
I0304 19:31:41.799042 23128000471168 run.py:483] Algo bellman_ford step 7430 current loss 0.003297, current_train_items 237792.
I0304 19:31:41.815216 23128000471168 run.py:483] Algo bellman_ford step 7431 current loss 0.007696, current_train_items 237824.
I0304 19:31:41.840650 23128000471168 run.py:483] Algo bellman_ford step 7432 current loss 0.073689, current_train_items 237856.
I0304 19:31:41.873217 23128000471168 run.py:483] Algo bellman_ford step 7433 current loss 0.073031, current_train_items 237888.
I0304 19:31:41.904891 23128000471168 run.py:483] Algo bellman_ford step 7434 current loss 0.058225, current_train_items 237920.
I0304 19:31:41.925267 23128000471168 run.py:483] Algo bellman_ford step 7435 current loss 0.003308, current_train_items 237952.
I0304 19:31:41.941675 23128000471168 run.py:483] Algo bellman_ford step 7436 current loss 0.017031, current_train_items 237984.
I0304 19:31:41.967072 23128000471168 run.py:483] Algo bellman_ford step 7437 current loss 0.128295, current_train_items 238016.
I0304 19:31:41.998926 23128000471168 run.py:483] Algo bellman_ford step 7438 current loss 0.081694, current_train_items 238048.
I0304 19:31:42.033541 23128000471168 run.py:483] Algo bellman_ford step 7439 current loss 0.107059, current_train_items 238080.
I0304 19:31:42.053951 23128000471168 run.py:483] Algo bellman_ford step 7440 current loss 0.005653, current_train_items 238112.
I0304 19:31:42.070874 23128000471168 run.py:483] Algo bellman_ford step 7441 current loss 0.016507, current_train_items 238144.
I0304 19:31:42.094801 23128000471168 run.py:483] Algo bellman_ford step 7442 current loss 0.016644, current_train_items 238176.
I0304 19:31:42.127918 23128000471168 run.py:483] Algo bellman_ford step 7443 current loss 0.041354, current_train_items 238208.
I0304 19:31:42.162705 23128000471168 run.py:483] Algo bellman_ford step 7444 current loss 0.083950, current_train_items 238240.
I0304 19:31:42.182785 23128000471168 run.py:483] Algo bellman_ford step 7445 current loss 0.002478, current_train_items 238272.
I0304 19:31:42.199278 23128000471168 run.py:483] Algo bellman_ford step 7446 current loss 0.012346, current_train_items 238304.
I0304 19:31:42.223861 23128000471168 run.py:483] Algo bellman_ford step 7447 current loss 0.025017, current_train_items 238336.
I0304 19:31:42.255621 23128000471168 run.py:483] Algo bellman_ford step 7448 current loss 0.030241, current_train_items 238368.
I0304 19:31:42.291185 23128000471168 run.py:483] Algo bellman_ford step 7449 current loss 0.055388, current_train_items 238400.
I0304 19:31:42.311036 23128000471168 run.py:483] Algo bellman_ford step 7450 current loss 0.001534, current_train_items 238432.
I0304 19:31:42.319054 23128000471168 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0304 19:31:42.319165 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:42.336551 23128000471168 run.py:483] Algo bellman_ford step 7451 current loss 0.006680, current_train_items 238464.
I0304 19:31:42.362716 23128000471168 run.py:483] Algo bellman_ford step 7452 current loss 0.032451, current_train_items 238496.
I0304 19:31:42.394634 23128000471168 run.py:483] Algo bellman_ford step 7453 current loss 0.026618, current_train_items 238528.
I0304 19:31:42.427889 23128000471168 run.py:483] Algo bellman_ford step 7454 current loss 0.038831, current_train_items 238560.
I0304 19:31:42.448031 23128000471168 run.py:483] Algo bellman_ford step 7455 current loss 0.012705, current_train_items 238592.
I0304 19:31:42.464755 23128000471168 run.py:483] Algo bellman_ford step 7456 current loss 0.029942, current_train_items 238624.
I0304 19:31:42.489130 23128000471168 run.py:483] Algo bellman_ford step 7457 current loss 0.029580, current_train_items 238656.
I0304 19:31:42.522172 23128000471168 run.py:483] Algo bellman_ford step 7458 current loss 0.051453, current_train_items 238688.
I0304 19:31:42.555706 23128000471168 run.py:483] Algo bellman_ford step 7459 current loss 0.056554, current_train_items 238720.
I0304 19:31:42.575990 23128000471168 run.py:483] Algo bellman_ford step 7460 current loss 0.001829, current_train_items 238752.
I0304 19:31:42.593137 23128000471168 run.py:483] Algo bellman_ford step 7461 current loss 0.019432, current_train_items 238784.
I0304 19:31:42.616699 23128000471168 run.py:483] Algo bellman_ford step 7462 current loss 0.060792, current_train_items 238816.
I0304 19:31:42.648643 23128000471168 run.py:483] Algo bellman_ford step 7463 current loss 0.022175, current_train_items 238848.
I0304 19:31:42.682590 23128000471168 run.py:483] Algo bellman_ford step 7464 current loss 0.071382, current_train_items 238880.
I0304 19:31:42.702523 23128000471168 run.py:483] Algo bellman_ford step 7465 current loss 0.002132, current_train_items 238912.
I0304 19:31:42.718816 23128000471168 run.py:483] Algo bellman_ford step 7466 current loss 0.003649, current_train_items 238944.
I0304 19:31:42.742692 23128000471168 run.py:483] Algo bellman_ford step 7467 current loss 0.088101, current_train_items 238976.
I0304 19:31:42.775760 23128000471168 run.py:483] Algo bellman_ford step 7468 current loss 0.138903, current_train_items 239008.
I0304 19:31:42.808421 23128000471168 run.py:483] Algo bellman_ford step 7469 current loss 0.095481, current_train_items 239040.
I0304 19:31:42.828949 23128000471168 run.py:483] Algo bellman_ford step 7470 current loss 0.001817, current_train_items 239072.
I0304 19:31:42.845656 23128000471168 run.py:483] Algo bellman_ford step 7471 current loss 0.010038, current_train_items 239104.
I0304 19:31:42.869178 23128000471168 run.py:483] Algo bellman_ford step 7472 current loss 0.045329, current_train_items 239136.
I0304 19:31:42.901567 23128000471168 run.py:483] Algo bellman_ford step 7473 current loss 0.056444, current_train_items 239168.
I0304 19:31:42.936786 23128000471168 run.py:483] Algo bellman_ford step 7474 current loss 0.051308, current_train_items 239200.
I0304 19:31:42.957227 23128000471168 run.py:483] Algo bellman_ford step 7475 current loss 0.001495, current_train_items 239232.
I0304 19:31:42.974111 23128000471168 run.py:483] Algo bellman_ford step 7476 current loss 0.007057, current_train_items 239264.
I0304 19:31:42.997385 23128000471168 run.py:483] Algo bellman_ford step 7477 current loss 0.014988, current_train_items 239296.
I0304 19:31:43.029862 23128000471168 run.py:483] Algo bellman_ford step 7478 current loss 0.028030, current_train_items 239328.
I0304 19:31:43.066362 23128000471168 run.py:483] Algo bellman_ford step 7479 current loss 0.099468, current_train_items 239360.
I0304 19:31:43.086160 23128000471168 run.py:483] Algo bellman_ford step 7480 current loss 0.001668, current_train_items 239392.
I0304 19:31:43.102850 23128000471168 run.py:483] Algo bellman_ford step 7481 current loss 0.011411, current_train_items 239424.
I0304 19:31:43.128262 23128000471168 run.py:483] Algo bellman_ford step 7482 current loss 0.037427, current_train_items 239456.
I0304 19:31:43.161448 23128000471168 run.py:483] Algo bellman_ford step 7483 current loss 0.059599, current_train_items 239488.
I0304 19:31:43.196038 23128000471168 run.py:483] Algo bellman_ford step 7484 current loss 0.057107, current_train_items 239520.
I0304 19:31:43.216187 23128000471168 run.py:483] Algo bellman_ford step 7485 current loss 0.001453, current_train_items 239552.
I0304 19:31:43.232809 23128000471168 run.py:483] Algo bellman_ford step 7486 current loss 0.013832, current_train_items 239584.
I0304 19:31:43.256331 23128000471168 run.py:483] Algo bellman_ford step 7487 current loss 0.027767, current_train_items 239616.
I0304 19:31:43.289269 23128000471168 run.py:483] Algo bellman_ford step 7488 current loss 0.027566, current_train_items 239648.
I0304 19:31:43.324614 23128000471168 run.py:483] Algo bellman_ford step 7489 current loss 0.088847, current_train_items 239680.
I0304 19:31:43.345164 23128000471168 run.py:483] Algo bellman_ford step 7490 current loss 0.001111, current_train_items 239712.
I0304 19:31:43.361434 23128000471168 run.py:483] Algo bellman_ford step 7491 current loss 0.060007, current_train_items 239744.
I0304 19:31:43.384673 23128000471168 run.py:483] Algo bellman_ford step 7492 current loss 0.012965, current_train_items 239776.
I0304 19:31:43.416821 23128000471168 run.py:483] Algo bellman_ford step 7493 current loss 0.029844, current_train_items 239808.
I0304 19:31:43.450111 23128000471168 run.py:483] Algo bellman_ford step 7494 current loss 0.066979, current_train_items 239840.
I0304 19:31:43.470247 23128000471168 run.py:483] Algo bellman_ford step 7495 current loss 0.002164, current_train_items 239872.
I0304 19:31:43.486916 23128000471168 run.py:483] Algo bellman_ford step 7496 current loss 0.023392, current_train_items 239904.
I0304 19:31:43.511539 23128000471168 run.py:483] Algo bellman_ford step 7497 current loss 0.053191, current_train_items 239936.
I0304 19:31:43.543073 23128000471168 run.py:483] Algo bellman_ford step 7498 current loss 0.033776, current_train_items 239968.
I0304 19:31:43.576589 23128000471168 run.py:483] Algo bellman_ford step 7499 current loss 0.044443, current_train_items 240000.
I0304 19:31:43.596639 23128000471168 run.py:483] Algo bellman_ford step 7500 current loss 0.004919, current_train_items 240032.
I0304 19:31:43.604383 23128000471168 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0304 19:31:43.604492 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:43.621461 23128000471168 run.py:483] Algo bellman_ford step 7501 current loss 0.009129, current_train_items 240064.
I0304 19:31:43.646137 23128000471168 run.py:483] Algo bellman_ford step 7502 current loss 0.090482, current_train_items 240096.
I0304 19:31:43.678931 23128000471168 run.py:483] Algo bellman_ford step 7503 current loss 0.035408, current_train_items 240128.
I0304 19:31:43.712849 23128000471168 run.py:483] Algo bellman_ford step 7504 current loss 0.061971, current_train_items 240160.
I0304 19:31:43.733093 23128000471168 run.py:483] Algo bellman_ford step 7505 current loss 0.032023, current_train_items 240192.
I0304 19:31:43.749372 23128000471168 run.py:483] Algo bellman_ford step 7506 current loss 0.007521, current_train_items 240224.
I0304 19:31:43.773423 23128000471168 run.py:483] Algo bellman_ford step 7507 current loss 0.038155, current_train_items 240256.
I0304 19:31:43.805416 23128000471168 run.py:483] Algo bellman_ford step 7508 current loss 0.023939, current_train_items 240288.
I0304 19:31:43.838328 23128000471168 run.py:483] Algo bellman_ford step 7509 current loss 0.059725, current_train_items 240320.
I0304 19:31:43.858262 23128000471168 run.py:483] Algo bellman_ford step 7510 current loss 0.007068, current_train_items 240352.
I0304 19:31:43.875132 23128000471168 run.py:483] Algo bellman_ford step 7511 current loss 0.027174, current_train_items 240384.
I0304 19:31:43.898997 23128000471168 run.py:483] Algo bellman_ford step 7512 current loss 0.035063, current_train_items 240416.
I0304 19:31:43.930703 23128000471168 run.py:483] Algo bellman_ford step 7513 current loss 0.025878, current_train_items 240448.
I0304 19:31:43.963904 23128000471168 run.py:483] Algo bellman_ford step 7514 current loss 0.051159, current_train_items 240480.
I0304 19:31:43.983875 23128000471168 run.py:483] Algo bellman_ford step 7515 current loss 0.005128, current_train_items 240512.
I0304 19:31:43.999701 23128000471168 run.py:483] Algo bellman_ford step 7516 current loss 0.027160, current_train_items 240544.
I0304 19:31:44.024467 23128000471168 run.py:483] Algo bellman_ford step 7517 current loss 0.061633, current_train_items 240576.
I0304 19:31:44.056950 23128000471168 run.py:483] Algo bellman_ford step 7518 current loss 0.055516, current_train_items 240608.
I0304 19:31:44.091364 23128000471168 run.py:483] Algo bellman_ford step 7519 current loss 0.064482, current_train_items 240640.
I0304 19:31:44.111530 23128000471168 run.py:483] Algo bellman_ford step 7520 current loss 0.005171, current_train_items 240672.
I0304 19:31:44.127960 23128000471168 run.py:483] Algo bellman_ford step 7521 current loss 0.019343, current_train_items 240704.
I0304 19:31:44.152309 23128000471168 run.py:483] Algo bellman_ford step 7522 current loss 0.040764, current_train_items 240736.
I0304 19:31:44.184754 23128000471168 run.py:483] Algo bellman_ford step 7523 current loss 0.046761, current_train_items 240768.
I0304 19:31:44.216087 23128000471168 run.py:483] Algo bellman_ford step 7524 current loss 0.030602, current_train_items 240800.
I0304 19:31:44.236215 23128000471168 run.py:483] Algo bellman_ford step 7525 current loss 0.007879, current_train_items 240832.
I0304 19:31:44.252587 23128000471168 run.py:483] Algo bellman_ford step 7526 current loss 0.026004, current_train_items 240864.
I0304 19:31:44.277903 23128000471168 run.py:483] Algo bellman_ford step 7527 current loss 0.036751, current_train_items 240896.
I0304 19:31:44.309140 23128000471168 run.py:483] Algo bellman_ford step 7528 current loss 0.053047, current_train_items 240928.
I0304 19:31:44.342861 23128000471168 run.py:483] Algo bellman_ford step 7529 current loss 0.089220, current_train_items 240960.
I0304 19:31:44.362941 23128000471168 run.py:483] Algo bellman_ford step 7530 current loss 0.006894, current_train_items 240992.
I0304 19:31:44.379261 23128000471168 run.py:483] Algo bellman_ford step 7531 current loss 0.010409, current_train_items 241024.
I0304 19:31:44.402958 23128000471168 run.py:483] Algo bellman_ford step 7532 current loss 0.035425, current_train_items 241056.
I0304 19:31:44.436372 23128000471168 run.py:483] Algo bellman_ford step 7533 current loss 0.091982, current_train_items 241088.
I0304 19:31:44.470812 23128000471168 run.py:483] Algo bellman_ford step 7534 current loss 0.078806, current_train_items 241120.
I0304 19:31:44.491101 23128000471168 run.py:483] Algo bellman_ford step 7535 current loss 0.002504, current_train_items 241152.
I0304 19:31:44.507652 23128000471168 run.py:483] Algo bellman_ford step 7536 current loss 0.010633, current_train_items 241184.
I0304 19:31:44.532163 23128000471168 run.py:483] Algo bellman_ford step 7537 current loss 0.040550, current_train_items 241216.
I0304 19:31:44.564727 23128000471168 run.py:483] Algo bellman_ford step 7538 current loss 0.036956, current_train_items 241248.
I0304 19:31:44.598056 23128000471168 run.py:483] Algo bellman_ford step 7539 current loss 0.033562, current_train_items 241280.
I0304 19:31:44.618678 23128000471168 run.py:483] Algo bellman_ford step 7540 current loss 0.002841, current_train_items 241312.
I0304 19:31:44.635570 23128000471168 run.py:483] Algo bellman_ford step 7541 current loss 0.015353, current_train_items 241344.
I0304 19:31:44.661028 23128000471168 run.py:483] Algo bellman_ford step 7542 current loss 0.055194, current_train_items 241376.
I0304 19:31:44.694446 23128000471168 run.py:483] Algo bellman_ford step 7543 current loss 0.033412, current_train_items 241408.
I0304 19:31:44.728476 23128000471168 run.py:483] Algo bellman_ford step 7544 current loss 0.045291, current_train_items 241440.
I0304 19:31:44.748076 23128000471168 run.py:483] Algo bellman_ford step 7545 current loss 0.002107, current_train_items 241472.
I0304 19:31:44.764537 23128000471168 run.py:483] Algo bellman_ford step 7546 current loss 0.009590, current_train_items 241504.
I0304 19:31:44.789625 23128000471168 run.py:483] Algo bellman_ford step 7547 current loss 0.033314, current_train_items 241536.
I0304 19:31:44.822669 23128000471168 run.py:483] Algo bellman_ford step 7548 current loss 0.035929, current_train_items 241568.
I0304 19:31:44.856535 23128000471168 run.py:483] Algo bellman_ford step 7549 current loss 0.038399, current_train_items 241600.
I0304 19:31:44.876977 23128000471168 run.py:483] Algo bellman_ford step 7550 current loss 0.010594, current_train_items 241632.
I0304 19:31:44.885319 23128000471168 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0304 19:31:44.885431 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:31:44.903000 23128000471168 run.py:483] Algo bellman_ford step 7551 current loss 0.014389, current_train_items 241664.
I0304 19:31:44.928229 23128000471168 run.py:483] Algo bellman_ford step 7552 current loss 0.011252, current_train_items 241696.
I0304 19:31:44.961620 23128000471168 run.py:483] Algo bellman_ford step 7553 current loss 0.063748, current_train_items 241728.
I0304 19:31:44.996168 23128000471168 run.py:483] Algo bellman_ford step 7554 current loss 0.069313, current_train_items 241760.
I0304 19:31:45.016427 23128000471168 run.py:483] Algo bellman_ford step 7555 current loss 0.001522, current_train_items 241792.
I0304 19:31:45.032839 23128000471168 run.py:483] Algo bellman_ford step 7556 current loss 0.038044, current_train_items 241824.
I0304 19:31:45.057554 23128000471168 run.py:483] Algo bellman_ford step 7557 current loss 0.018618, current_train_items 241856.
I0304 19:31:45.091270 23128000471168 run.py:483] Algo bellman_ford step 7558 current loss 0.069493, current_train_items 241888.
I0304 19:31:45.124410 23128000471168 run.py:483] Algo bellman_ford step 7559 current loss 0.040949, current_train_items 241920.
I0304 19:31:45.144928 23128000471168 run.py:483] Algo bellman_ford step 7560 current loss 0.001905, current_train_items 241952.
I0304 19:31:45.162307 23128000471168 run.py:483] Algo bellman_ford step 7561 current loss 0.044135, current_train_items 241984.
I0304 19:31:45.186273 23128000471168 run.py:483] Algo bellman_ford step 7562 current loss 0.029698, current_train_items 242016.
I0304 19:31:45.219153 23128000471168 run.py:483] Algo bellman_ford step 7563 current loss 0.031377, current_train_items 242048.
I0304 19:31:45.253539 23128000471168 run.py:483] Algo bellman_ford step 7564 current loss 0.047431, current_train_items 242080.
I0304 19:31:45.273808 23128000471168 run.py:483] Algo bellman_ford step 7565 current loss 0.002170, current_train_items 242112.
I0304 19:31:45.290135 23128000471168 run.py:483] Algo bellman_ford step 7566 current loss 0.037581, current_train_items 242144.
I0304 19:31:45.314955 23128000471168 run.py:483] Algo bellman_ford step 7567 current loss 0.046171, current_train_items 242176.
I0304 19:31:45.347001 23128000471168 run.py:483] Algo bellman_ford step 7568 current loss 0.078212, current_train_items 242208.
I0304 19:31:45.380270 23128000471168 run.py:483] Algo bellman_ford step 7569 current loss 0.086370, current_train_items 242240.
I0304 19:31:45.400731 23128000471168 run.py:483] Algo bellman_ford step 7570 current loss 0.002384, current_train_items 242272.
I0304 19:31:45.417618 23128000471168 run.py:483] Algo bellman_ford step 7571 current loss 0.008768, current_train_items 242304.
I0304 19:31:45.441753 23128000471168 run.py:483] Algo bellman_ford step 7572 current loss 0.015205, current_train_items 242336.
I0304 19:31:45.473141 23128000471168 run.py:483] Algo bellman_ford step 7573 current loss 0.033636, current_train_items 242368.
I0304 19:31:45.504805 23128000471168 run.py:483] Algo bellman_ford step 7574 current loss 0.032363, current_train_items 242400.
I0304 19:31:45.525345 23128000471168 run.py:483] Algo bellman_ford step 7575 current loss 0.001069, current_train_items 242432.
I0304 19:31:45.542553 23128000471168 run.py:483] Algo bellman_ford step 7576 current loss 0.029713, current_train_items 242464.
I0304 19:31:45.566025 23128000471168 run.py:483] Algo bellman_ford step 7577 current loss 0.071713, current_train_items 242496.
I0304 19:31:45.598707 23128000471168 run.py:483] Algo bellman_ford step 7578 current loss 0.039380, current_train_items 242528.
I0304 19:31:45.633989 23128000471168 run.py:483] Algo bellman_ford step 7579 current loss 0.067068, current_train_items 242560.
I0304 19:31:45.654326 23128000471168 run.py:483] Algo bellman_ford step 7580 current loss 0.001349, current_train_items 242592.
I0304 19:31:45.671072 23128000471168 run.py:483] Algo bellman_ford step 7581 current loss 0.025158, current_train_items 242624.
I0304 19:31:45.696036 23128000471168 run.py:483] Algo bellman_ford step 7582 current loss 0.039225, current_train_items 242656.
I0304 19:31:45.728318 23128000471168 run.py:483] Algo bellman_ford step 7583 current loss 0.054072, current_train_items 242688.
I0304 19:31:45.761667 23128000471168 run.py:483] Algo bellman_ford step 7584 current loss 0.039492, current_train_items 242720.
I0304 19:31:45.782091 23128000471168 run.py:483] Algo bellman_ford step 7585 current loss 0.001819, current_train_items 242752.
I0304 19:31:45.798715 23128000471168 run.py:483] Algo bellman_ford step 7586 current loss 0.026220, current_train_items 242784.
I0304 19:31:45.822766 23128000471168 run.py:483] Algo bellman_ford step 7587 current loss 0.034717, current_train_items 242816.
I0304 19:31:45.853011 23128000471168 run.py:483] Algo bellman_ford step 7588 current loss 0.018565, current_train_items 242848.
I0304 19:31:45.887387 23128000471168 run.py:483] Algo bellman_ford step 7589 current loss 0.063068, current_train_items 242880.
I0304 19:31:45.908011 23128000471168 run.py:483] Algo bellman_ford step 7590 current loss 0.012812, current_train_items 242912.
I0304 19:31:45.924363 23128000471168 run.py:483] Algo bellman_ford step 7591 current loss 0.010784, current_train_items 242944.
I0304 19:31:45.950137 23128000471168 run.py:483] Algo bellman_ford step 7592 current loss 0.058984, current_train_items 242976.
I0304 19:31:45.980939 23128000471168 run.py:483] Algo bellman_ford step 7593 current loss 0.046488, current_train_items 243008.
I0304 19:31:46.015150 23128000471168 run.py:483] Algo bellman_ford step 7594 current loss 0.056741, current_train_items 243040.
I0304 19:31:46.035343 23128000471168 run.py:483] Algo bellman_ford step 7595 current loss 0.002965, current_train_items 243072.
I0304 19:31:46.051694 23128000471168 run.py:483] Algo bellman_ford step 7596 current loss 0.018340, current_train_items 243104.
I0304 19:31:46.076689 23128000471168 run.py:483] Algo bellman_ford step 7597 current loss 0.056276, current_train_items 243136.
I0304 19:31:46.109906 23128000471168 run.py:483] Algo bellman_ford step 7598 current loss 0.035466, current_train_items 243168.
I0304 19:31:46.141070 23128000471168 run.py:483] Algo bellman_ford step 7599 current loss 0.036777, current_train_items 243200.
I0304 19:31:46.161546 23128000471168 run.py:483] Algo bellman_ford step 7600 current loss 0.001940, current_train_items 243232.
I0304 19:31:46.169592 23128000471168 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0304 19:31:46.169702 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:46.187055 23128000471168 run.py:483] Algo bellman_ford step 7601 current loss 0.009606, current_train_items 243264.
I0304 19:31:46.213313 23128000471168 run.py:483] Algo bellman_ford step 7602 current loss 0.119535, current_train_items 243296.
I0304 19:31:46.245435 23128000471168 run.py:483] Algo bellman_ford step 7603 current loss 0.032233, current_train_items 243328.
I0304 19:31:46.280364 23128000471168 run.py:483] Algo bellman_ford step 7604 current loss 0.158290, current_train_items 243360.
I0304 19:31:46.300593 23128000471168 run.py:483] Algo bellman_ford step 7605 current loss 0.037382, current_train_items 243392.
I0304 19:31:46.316249 23128000471168 run.py:483] Algo bellman_ford step 7606 current loss 0.006979, current_train_items 243424.
I0304 19:31:46.340192 23128000471168 run.py:483] Algo bellman_ford step 7607 current loss 0.024413, current_train_items 243456.
I0304 19:31:46.372796 23128000471168 run.py:483] Algo bellman_ford step 7608 current loss 0.052974, current_train_items 243488.
I0304 19:31:46.407674 23128000471168 run.py:483] Algo bellman_ford step 7609 current loss 0.069496, current_train_items 243520.
I0304 19:31:46.427449 23128000471168 run.py:483] Algo bellman_ford step 7610 current loss 0.001253, current_train_items 243552.
I0304 19:31:46.444364 23128000471168 run.py:483] Algo bellman_ford step 7611 current loss 0.006802, current_train_items 243584.
I0304 19:31:46.468724 23128000471168 run.py:483] Algo bellman_ford step 7612 current loss 0.043442, current_train_items 243616.
I0304 19:31:46.498756 23128000471168 run.py:483] Algo bellman_ford step 7613 current loss 0.028370, current_train_items 243648.
I0304 19:31:46.531948 23128000471168 run.py:483] Algo bellman_ford step 7614 current loss 0.074794, current_train_items 243680.
I0304 19:31:46.552065 23128000471168 run.py:483] Algo bellman_ford step 7615 current loss 0.001599, current_train_items 243712.
I0304 19:31:46.568747 23128000471168 run.py:483] Algo bellman_ford step 7616 current loss 0.005885, current_train_items 243744.
I0304 19:31:46.592841 23128000471168 run.py:483] Algo bellman_ford step 7617 current loss 0.034383, current_train_items 243776.
I0304 19:31:46.624546 23128000471168 run.py:483] Algo bellman_ford step 7618 current loss 0.034126, current_train_items 243808.
I0304 19:31:46.658698 23128000471168 run.py:483] Algo bellman_ford step 7619 current loss 0.068000, current_train_items 243840.
I0304 19:31:46.678558 23128000471168 run.py:483] Algo bellman_ford step 7620 current loss 0.002449, current_train_items 243872.
I0304 19:31:46.695199 23128000471168 run.py:483] Algo bellman_ford step 7621 current loss 0.011858, current_train_items 243904.
I0304 19:31:46.719744 23128000471168 run.py:483] Algo bellman_ford step 7622 current loss 0.026791, current_train_items 243936.
I0304 19:31:46.753262 23128000471168 run.py:483] Algo bellman_ford step 7623 current loss 0.064829, current_train_items 243968.
I0304 19:31:46.786495 23128000471168 run.py:483] Algo bellman_ford step 7624 current loss 0.067985, current_train_items 244000.
I0304 19:31:46.806434 23128000471168 run.py:483] Algo bellman_ford step 7625 current loss 0.001580, current_train_items 244032.
I0304 19:31:46.822244 23128000471168 run.py:483] Algo bellman_ford step 7626 current loss 0.003674, current_train_items 244064.
I0304 19:31:46.847559 23128000471168 run.py:483] Algo bellman_ford step 7627 current loss 0.046194, current_train_items 244096.
I0304 19:31:46.880408 23128000471168 run.py:483] Algo bellman_ford step 7628 current loss 0.043684, current_train_items 244128.
I0304 19:31:46.913756 23128000471168 run.py:483] Algo bellman_ford step 7629 current loss 0.053886, current_train_items 244160.
I0304 19:31:46.933592 23128000471168 run.py:483] Algo bellman_ford step 7630 current loss 0.022959, current_train_items 244192.
I0304 19:31:46.950286 23128000471168 run.py:483] Algo bellman_ford step 7631 current loss 0.008228, current_train_items 244224.
I0304 19:31:46.974457 23128000471168 run.py:483] Algo bellman_ford step 7632 current loss 0.026924, current_train_items 244256.
I0304 19:31:47.007555 23128000471168 run.py:483] Algo bellman_ford step 7633 current loss 0.022073, current_train_items 244288.
I0304 19:31:47.041804 23128000471168 run.py:483] Algo bellman_ford step 7634 current loss 0.052467, current_train_items 244320.
I0304 19:31:47.061923 23128000471168 run.py:483] Algo bellman_ford step 7635 current loss 0.006171, current_train_items 244352.
I0304 19:31:47.078263 23128000471168 run.py:483] Algo bellman_ford step 7636 current loss 0.007775, current_train_items 244384.
I0304 19:31:47.102604 23128000471168 run.py:483] Algo bellman_ford step 7637 current loss 0.020078, current_train_items 244416.
I0304 19:31:47.133914 23128000471168 run.py:483] Algo bellman_ford step 7638 current loss 0.023906, current_train_items 244448.
I0304 19:31:47.169983 23128000471168 run.py:483] Algo bellman_ford step 7639 current loss 0.038354, current_train_items 244480.
I0304 19:31:47.189736 23128000471168 run.py:483] Algo bellman_ford step 7640 current loss 0.001502, current_train_items 244512.
I0304 19:31:47.206211 23128000471168 run.py:483] Algo bellman_ford step 7641 current loss 0.018720, current_train_items 244544.
I0304 19:31:47.229615 23128000471168 run.py:483] Algo bellman_ford step 7642 current loss 0.026710, current_train_items 244576.
I0304 19:31:47.264099 23128000471168 run.py:483] Algo bellman_ford step 7643 current loss 0.052738, current_train_items 244608.
I0304 19:31:47.299851 23128000471168 run.py:483] Algo bellman_ford step 7644 current loss 0.039987, current_train_items 244640.
I0304 19:31:47.319662 23128000471168 run.py:483] Algo bellman_ford step 7645 current loss 0.006456, current_train_items 244672.
I0304 19:31:47.336152 23128000471168 run.py:483] Algo bellman_ford step 7646 current loss 0.021369, current_train_items 244704.
I0304 19:31:47.361295 23128000471168 run.py:483] Algo bellman_ford step 7647 current loss 0.032525, current_train_items 244736.
I0304 19:31:47.395229 23128000471168 run.py:483] Algo bellman_ford step 7648 current loss 0.055170, current_train_items 244768.
I0304 19:31:47.430843 23128000471168 run.py:483] Algo bellman_ford step 7649 current loss 0.094462, current_train_items 244800.
I0304 19:31:47.450979 23128000471168 run.py:483] Algo bellman_ford step 7650 current loss 0.002342, current_train_items 244832.
I0304 19:31:47.458942 23128000471168 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0304 19:31:47.459063 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:47.475826 23128000471168 run.py:483] Algo bellman_ford step 7651 current loss 0.010479, current_train_items 244864.
I0304 19:31:47.501634 23128000471168 run.py:483] Algo bellman_ford step 7652 current loss 0.057468, current_train_items 244896.
I0304 19:31:47.534279 23128000471168 run.py:483] Algo bellman_ford step 7653 current loss 0.031092, current_train_items 244928.
I0304 19:31:47.568712 23128000471168 run.py:483] Algo bellman_ford step 7654 current loss 0.072075, current_train_items 244960.
I0304 19:31:47.588907 23128000471168 run.py:483] Algo bellman_ford step 7655 current loss 0.005509, current_train_items 244992.
I0304 19:31:47.605116 23128000471168 run.py:483] Algo bellman_ford step 7656 current loss 0.016514, current_train_items 245024.
I0304 19:31:47.629635 23128000471168 run.py:483] Algo bellman_ford step 7657 current loss 0.028574, current_train_items 245056.
I0304 19:31:47.662960 23128000471168 run.py:483] Algo bellman_ford step 7658 current loss 0.029230, current_train_items 245088.
I0304 19:31:47.697963 23128000471168 run.py:483] Algo bellman_ford step 7659 current loss 0.099707, current_train_items 245120.
I0304 19:31:47.718485 23128000471168 run.py:483] Algo bellman_ford step 7660 current loss 0.001806, current_train_items 245152.
I0304 19:31:47.735054 23128000471168 run.py:483] Algo bellman_ford step 7661 current loss 0.006407, current_train_items 245184.
I0304 19:31:47.759063 23128000471168 run.py:483] Algo bellman_ford step 7662 current loss 0.027630, current_train_items 245216.
I0304 19:31:47.790939 23128000471168 run.py:483] Algo bellman_ford step 7663 current loss 0.034798, current_train_items 245248.
I0304 19:31:47.824820 23128000471168 run.py:483] Algo bellman_ford step 7664 current loss 0.043279, current_train_items 245280.
I0304 19:31:47.844622 23128000471168 run.py:483] Algo bellman_ford step 7665 current loss 0.019999, current_train_items 245312.
I0304 19:31:47.861150 23128000471168 run.py:483] Algo bellman_ford step 7666 current loss 0.029585, current_train_items 245344.
I0304 19:31:47.885353 23128000471168 run.py:483] Algo bellman_ford step 7667 current loss 0.053238, current_train_items 245376.
I0304 19:31:47.919726 23128000471168 run.py:483] Algo bellman_ford step 7668 current loss 0.053991, current_train_items 245408.
I0304 19:31:47.954737 23128000471168 run.py:483] Algo bellman_ford step 7669 current loss 0.051256, current_train_items 245440.
I0304 19:31:47.974864 23128000471168 run.py:483] Algo bellman_ford step 7670 current loss 0.026780, current_train_items 245472.
I0304 19:31:47.991809 23128000471168 run.py:483] Algo bellman_ford step 7671 current loss 0.035353, current_train_items 245504.
I0304 19:31:48.015706 23128000471168 run.py:483] Algo bellman_ford step 7672 current loss 0.044440, current_train_items 245536.
I0304 19:31:48.048437 23128000471168 run.py:483] Algo bellman_ford step 7673 current loss 0.038350, current_train_items 245568.
I0304 19:31:48.082809 23128000471168 run.py:483] Algo bellman_ford step 7674 current loss 0.059993, current_train_items 245600.
I0304 19:31:48.103132 23128000471168 run.py:483] Algo bellman_ford step 7675 current loss 0.002355, current_train_items 245632.
I0304 19:31:48.120282 23128000471168 run.py:483] Algo bellman_ford step 7676 current loss 0.028419, current_train_items 245664.
I0304 19:31:48.145011 23128000471168 run.py:483] Algo bellman_ford step 7677 current loss 0.047190, current_train_items 245696.
I0304 19:31:48.176993 23128000471168 run.py:483] Algo bellman_ford step 7678 current loss 0.032372, current_train_items 245728.
I0304 19:31:48.212113 23128000471168 run.py:483] Algo bellman_ford step 7679 current loss 0.036410, current_train_items 245760.
I0304 19:31:48.231909 23128000471168 run.py:483] Algo bellman_ford step 7680 current loss 0.001849, current_train_items 245792.
I0304 19:31:48.248583 23128000471168 run.py:483] Algo bellman_ford step 7681 current loss 0.026503, current_train_items 245824.
I0304 19:31:48.272841 23128000471168 run.py:483] Algo bellman_ford step 7682 current loss 0.042757, current_train_items 245856.
I0304 19:31:48.305107 23128000471168 run.py:483] Algo bellman_ford step 7683 current loss 0.028311, current_train_items 245888.
I0304 19:31:48.340296 23128000471168 run.py:483] Algo bellman_ford step 7684 current loss 0.061548, current_train_items 245920.
I0304 19:31:48.360848 23128000471168 run.py:483] Algo bellman_ford step 7685 current loss 0.009328, current_train_items 245952.
I0304 19:31:48.377733 23128000471168 run.py:483] Algo bellman_ford step 7686 current loss 0.052668, current_train_items 245984.
I0304 19:31:48.400981 23128000471168 run.py:483] Algo bellman_ford step 7687 current loss 0.182441, current_train_items 246016.
I0304 19:31:48.434470 23128000471168 run.py:483] Algo bellman_ford step 7688 current loss 0.043317, current_train_items 246048.
I0304 19:31:48.468575 23128000471168 run.py:483] Algo bellman_ford step 7689 current loss 0.059789, current_train_items 246080.
I0304 19:31:48.488908 23128000471168 run.py:483] Algo bellman_ford step 7690 current loss 0.012265, current_train_items 246112.
I0304 19:31:48.505551 23128000471168 run.py:483] Algo bellman_ford step 7691 current loss 0.025849, current_train_items 246144.
I0304 19:31:48.529512 23128000471168 run.py:483] Algo bellman_ford step 7692 current loss 0.051229, current_train_items 246176.
I0304 19:31:48.563268 23128000471168 run.py:483] Algo bellman_ford step 7693 current loss 0.155315, current_train_items 246208.
I0304 19:31:48.596455 23128000471168 run.py:483] Algo bellman_ford step 7694 current loss 0.204084, current_train_items 246240.
I0304 19:31:48.616418 23128000471168 run.py:483] Algo bellman_ford step 7695 current loss 0.004145, current_train_items 246272.
I0304 19:31:48.633159 23128000471168 run.py:483] Algo bellman_ford step 7696 current loss 0.093131, current_train_items 246304.
I0304 19:31:48.658657 23128000471168 run.py:483] Algo bellman_ford step 7697 current loss 0.037132, current_train_items 246336.
I0304 19:31:48.691861 23128000471168 run.py:483] Algo bellman_ford step 7698 current loss 0.075579, current_train_items 246368.
I0304 19:31:48.727467 23128000471168 run.py:483] Algo bellman_ford step 7699 current loss 0.074439, current_train_items 246400.
I0304 19:31:48.747898 23128000471168 run.py:483] Algo bellman_ford step 7700 current loss 0.005381, current_train_items 246432.
I0304 19:31:48.755662 23128000471168 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0304 19:31:48.755769 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:31:48.773248 23128000471168 run.py:483] Algo bellman_ford step 7701 current loss 0.041098, current_train_items 246464.
I0304 19:31:48.798795 23128000471168 run.py:483] Algo bellman_ford step 7702 current loss 0.126106, current_train_items 246496.
I0304 19:31:48.830891 23128000471168 run.py:483] Algo bellman_ford step 7703 current loss 0.055885, current_train_items 246528.
I0304 19:31:48.864995 23128000471168 run.py:483] Algo bellman_ford step 7704 current loss 0.186995, current_train_items 246560.
I0304 19:31:48.885593 23128000471168 run.py:483] Algo bellman_ford step 7705 current loss 0.005908, current_train_items 246592.
I0304 19:31:48.901837 23128000471168 run.py:483] Algo bellman_ford step 7706 current loss 0.025051, current_train_items 246624.
I0304 19:31:48.927714 23128000471168 run.py:483] Algo bellman_ford step 7707 current loss 0.022537, current_train_items 246656.
I0304 19:31:48.959506 23128000471168 run.py:483] Algo bellman_ford step 7708 current loss 0.039503, current_train_items 246688.
I0304 19:31:48.992758 23128000471168 run.py:483] Algo bellman_ford step 7709 current loss 0.039680, current_train_items 246720.
I0304 19:31:49.013272 23128000471168 run.py:483] Algo bellman_ford step 7710 current loss 0.005771, current_train_items 246752.
I0304 19:31:49.030055 23128000471168 run.py:483] Algo bellman_ford step 7711 current loss 0.012103, current_train_items 246784.
I0304 19:31:49.054763 23128000471168 run.py:483] Algo bellman_ford step 7712 current loss 0.046620, current_train_items 246816.
I0304 19:31:49.086444 23128000471168 run.py:483] Algo bellman_ford step 7713 current loss 0.080613, current_train_items 246848.
I0304 19:31:49.120344 23128000471168 run.py:483] Algo bellman_ford step 7714 current loss 0.066981, current_train_items 246880.
I0304 19:31:49.140418 23128000471168 run.py:483] Algo bellman_ford step 7715 current loss 0.036385, current_train_items 246912.
I0304 19:31:49.156790 23128000471168 run.py:483] Algo bellman_ford step 7716 current loss 0.004258, current_train_items 246944.
I0304 19:31:49.181651 23128000471168 run.py:483] Algo bellman_ford step 7717 current loss 0.049173, current_train_items 246976.
I0304 19:31:49.213985 23128000471168 run.py:483] Algo bellman_ford step 7718 current loss 0.043373, current_train_items 247008.
I0304 19:31:49.247074 23128000471168 run.py:483] Algo bellman_ford step 7719 current loss 0.066070, current_train_items 247040.
I0304 19:31:49.267568 23128000471168 run.py:483] Algo bellman_ford step 7720 current loss 0.003267, current_train_items 247072.
I0304 19:31:49.283984 23128000471168 run.py:483] Algo bellman_ford step 7721 current loss 0.021664, current_train_items 247104.
I0304 19:31:49.309350 23128000471168 run.py:483] Algo bellman_ford step 7722 current loss 0.021711, current_train_items 247136.
I0304 19:31:49.342682 23128000471168 run.py:483] Algo bellman_ford step 7723 current loss 0.046402, current_train_items 247168.
I0304 19:31:49.377440 23128000471168 run.py:483] Algo bellman_ford step 7724 current loss 0.074489, current_train_items 247200.
I0304 19:31:49.397684 23128000471168 run.py:483] Algo bellman_ford step 7725 current loss 0.002708, current_train_items 247232.
I0304 19:31:49.414177 23128000471168 run.py:483] Algo bellman_ford step 7726 current loss 0.014725, current_train_items 247264.
I0304 19:31:49.437696 23128000471168 run.py:483] Algo bellman_ford step 7727 current loss 0.017659, current_train_items 247296.
I0304 19:31:49.469831 23128000471168 run.py:483] Algo bellman_ford step 7728 current loss 0.032910, current_train_items 247328.
I0304 19:31:49.503164 23128000471168 run.py:483] Algo bellman_ford step 7729 current loss 0.064747, current_train_items 247360.
I0304 19:31:49.523542 23128000471168 run.py:483] Algo bellman_ford step 7730 current loss 0.018263, current_train_items 247392.
I0304 19:31:49.539807 23128000471168 run.py:483] Algo bellman_ford step 7731 current loss 0.012528, current_train_items 247424.
I0304 19:31:49.563861 23128000471168 run.py:483] Algo bellman_ford step 7732 current loss 0.030185, current_train_items 247456.
I0304 19:31:49.596082 23128000471168 run.py:483] Algo bellman_ford step 7733 current loss 0.047274, current_train_items 247488.
I0304 19:31:49.628634 23128000471168 run.py:483] Algo bellman_ford step 7734 current loss 0.064199, current_train_items 247520.
I0304 19:31:49.648730 23128000471168 run.py:483] Algo bellman_ford step 7735 current loss 0.002538, current_train_items 247552.
I0304 19:31:49.665473 23128000471168 run.py:483] Algo bellman_ford step 7736 current loss 0.013543, current_train_items 247584.
I0304 19:31:49.690617 23128000471168 run.py:483] Algo bellman_ford step 7737 current loss 0.042823, current_train_items 247616.
I0304 19:31:49.722752 23128000471168 run.py:483] Algo bellman_ford step 7738 current loss 0.064246, current_train_items 247648.
I0304 19:31:49.757494 23128000471168 run.py:483] Algo bellman_ford step 7739 current loss 0.071360, current_train_items 247680.
I0304 19:31:49.777279 23128000471168 run.py:483] Algo bellman_ford step 7740 current loss 0.008185, current_train_items 247712.
I0304 19:31:49.794252 23128000471168 run.py:483] Algo bellman_ford step 7741 current loss 0.018468, current_train_items 247744.
I0304 19:31:49.819892 23128000471168 run.py:483] Algo bellman_ford step 7742 current loss 0.060573, current_train_items 247776.
I0304 19:31:49.853026 23128000471168 run.py:483] Algo bellman_ford step 7743 current loss 0.054660, current_train_items 247808.
I0304 19:31:49.887856 23128000471168 run.py:483] Algo bellman_ford step 7744 current loss 0.100859, current_train_items 247840.
I0304 19:31:49.907786 23128000471168 run.py:483] Algo bellman_ford step 7745 current loss 0.041968, current_train_items 247872.
I0304 19:31:49.924140 23128000471168 run.py:483] Algo bellman_ford step 7746 current loss 0.011618, current_train_items 247904.
I0304 19:31:49.948557 23128000471168 run.py:483] Algo bellman_ford step 7747 current loss 0.033159, current_train_items 247936.
I0304 19:31:49.982387 23128000471168 run.py:483] Algo bellman_ford step 7748 current loss 0.124515, current_train_items 247968.
I0304 19:31:50.017473 23128000471168 run.py:483] Algo bellman_ford step 7749 current loss 0.045381, current_train_items 248000.
I0304 19:31:50.037899 23128000471168 run.py:483] Algo bellman_ford step 7750 current loss 0.006959, current_train_items 248032.
I0304 19:31:50.045845 23128000471168 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0304 19:31:50.045953 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:50.063446 23128000471168 run.py:483] Algo bellman_ford step 7751 current loss 0.020166, current_train_items 248064.
I0304 19:31:50.088205 23128000471168 run.py:483] Algo bellman_ford step 7752 current loss 0.066144, current_train_items 248096.
I0304 19:31:50.120940 23128000471168 run.py:483] Algo bellman_ford step 7753 current loss 0.051705, current_train_items 248128.
I0304 19:31:50.156879 23128000471168 run.py:483] Algo bellman_ford step 7754 current loss 0.132145, current_train_items 248160.
I0304 19:31:50.177225 23128000471168 run.py:483] Algo bellman_ford step 7755 current loss 0.002462, current_train_items 248192.
I0304 19:31:50.193027 23128000471168 run.py:483] Algo bellman_ford step 7756 current loss 0.017417, current_train_items 248224.
I0304 19:31:50.217627 23128000471168 run.py:483] Algo bellman_ford step 7757 current loss 0.032949, current_train_items 248256.
I0304 19:31:50.249226 23128000471168 run.py:483] Algo bellman_ford step 7758 current loss 0.078156, current_train_items 248288.
I0304 19:31:50.282909 23128000471168 run.py:483] Algo bellman_ford step 7759 current loss 0.062469, current_train_items 248320.
I0304 19:31:50.303325 23128000471168 run.py:483] Algo bellman_ford step 7760 current loss 0.002760, current_train_items 248352.
I0304 19:31:50.320009 23128000471168 run.py:483] Algo bellman_ford step 7761 current loss 0.023677, current_train_items 248384.
I0304 19:31:50.344506 23128000471168 run.py:483] Algo bellman_ford step 7762 current loss 0.050007, current_train_items 248416.
I0304 19:31:50.376479 23128000471168 run.py:483] Algo bellman_ford step 7763 current loss 0.043390, current_train_items 248448.
I0304 19:31:50.409245 23128000471168 run.py:483] Algo bellman_ford step 7764 current loss 0.056733, current_train_items 248480.
I0304 19:31:50.429496 23128000471168 run.py:483] Algo bellman_ford step 7765 current loss 0.003053, current_train_items 248512.
I0304 19:31:50.446052 23128000471168 run.py:483] Algo bellman_ford step 7766 current loss 0.016890, current_train_items 248544.
I0304 19:31:50.470021 23128000471168 run.py:483] Algo bellman_ford step 7767 current loss 0.026756, current_train_items 248576.
I0304 19:31:50.502673 23128000471168 run.py:483] Algo bellman_ford step 7768 current loss 0.056509, current_train_items 248608.
I0304 19:31:50.536358 23128000471168 run.py:483] Algo bellman_ford step 7769 current loss 0.061736, current_train_items 248640.
I0304 19:31:50.556569 23128000471168 run.py:483] Algo bellman_ford step 7770 current loss 0.003918, current_train_items 248672.
I0304 19:31:50.573308 23128000471168 run.py:483] Algo bellman_ford step 7771 current loss 0.055603, current_train_items 248704.
I0304 19:31:50.597870 23128000471168 run.py:483] Algo bellman_ford step 7772 current loss 0.032760, current_train_items 248736.
I0304 19:31:50.630566 23128000471168 run.py:483] Algo bellman_ford step 7773 current loss 0.034210, current_train_items 248768.
I0304 19:31:50.664013 23128000471168 run.py:483] Algo bellman_ford step 7774 current loss 0.079275, current_train_items 248800.
I0304 19:31:50.684337 23128000471168 run.py:483] Algo bellman_ford step 7775 current loss 0.003409, current_train_items 248832.
I0304 19:31:50.701219 23128000471168 run.py:483] Algo bellman_ford step 7776 current loss 0.046103, current_train_items 248864.
I0304 19:31:50.726325 23128000471168 run.py:483] Algo bellman_ford step 7777 current loss 0.045565, current_train_items 248896.
I0304 19:31:50.759084 23128000471168 run.py:483] Algo bellman_ford step 7778 current loss 0.056110, current_train_items 248928.
I0304 19:31:50.794899 23128000471168 run.py:483] Algo bellman_ford step 7779 current loss 0.098707, current_train_items 248960.
I0304 19:31:50.814559 23128000471168 run.py:483] Algo bellman_ford step 7780 current loss 0.002265, current_train_items 248992.
I0304 19:31:50.831312 23128000471168 run.py:483] Algo bellman_ford step 7781 current loss 0.012209, current_train_items 249024.
I0304 19:31:50.855878 23128000471168 run.py:483] Algo bellman_ford step 7782 current loss 0.053794, current_train_items 249056.
I0304 19:31:50.889176 23128000471168 run.py:483] Algo bellman_ford step 7783 current loss 0.081590, current_train_items 249088.
I0304 19:31:50.923415 23128000471168 run.py:483] Algo bellman_ford step 7784 current loss 0.068806, current_train_items 249120.
I0304 19:31:50.943684 23128000471168 run.py:483] Algo bellman_ford step 7785 current loss 0.004662, current_train_items 249152.
I0304 19:31:50.960413 23128000471168 run.py:483] Algo bellman_ford step 7786 current loss 0.021822, current_train_items 249184.
I0304 19:31:50.984253 23128000471168 run.py:483] Algo bellman_ford step 7787 current loss 0.034093, current_train_items 249216.
I0304 19:31:51.016414 23128000471168 run.py:483] Algo bellman_ford step 7788 current loss 0.042906, current_train_items 249248.
I0304 19:31:51.048228 23128000471168 run.py:483] Algo bellman_ford step 7789 current loss 0.052041, current_train_items 249280.
I0304 19:31:51.068598 23128000471168 run.py:483] Algo bellman_ford step 7790 current loss 0.001825, current_train_items 249312.
I0304 19:31:51.084831 23128000471168 run.py:483] Algo bellman_ford step 7791 current loss 0.017682, current_train_items 249344.
I0304 19:31:51.108999 23128000471168 run.py:483] Algo bellman_ford step 7792 current loss 0.040917, current_train_items 249376.
I0304 19:31:51.140595 23128000471168 run.py:483] Algo bellman_ford step 7793 current loss 0.048535, current_train_items 249408.
I0304 19:31:51.173954 23128000471168 run.py:483] Algo bellman_ford step 7794 current loss 0.029101, current_train_items 249440.
I0304 19:31:51.193868 23128000471168 run.py:483] Algo bellman_ford step 7795 current loss 0.011403, current_train_items 249472.
I0304 19:31:51.209944 23128000471168 run.py:483] Algo bellman_ford step 7796 current loss 0.013512, current_train_items 249504.
I0304 19:31:51.233502 23128000471168 run.py:483] Algo bellman_ford step 7797 current loss 0.025017, current_train_items 249536.
I0304 19:31:51.265150 23128000471168 run.py:483] Algo bellman_ford step 7798 current loss 0.045637, current_train_items 249568.
I0304 19:31:51.300253 23128000471168 run.py:483] Algo bellman_ford step 7799 current loss 0.096657, current_train_items 249600.
I0304 19:31:51.320432 23128000471168 run.py:483] Algo bellman_ford step 7800 current loss 0.001962, current_train_items 249632.
I0304 19:31:51.328129 23128000471168 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0304 19:31:51.328238 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:51.345358 23128000471168 run.py:483] Algo bellman_ford step 7801 current loss 0.024258, current_train_items 249664.
I0304 19:31:51.370185 23128000471168 run.py:483] Algo bellman_ford step 7802 current loss 0.029422, current_train_items 249696.
I0304 19:31:51.404017 23128000471168 run.py:483] Algo bellman_ford step 7803 current loss 0.068300, current_train_items 249728.
I0304 19:31:51.440495 23128000471168 run.py:483] Algo bellman_ford step 7804 current loss 0.082443, current_train_items 249760.
I0304 19:31:51.460848 23128000471168 run.py:483] Algo bellman_ford step 7805 current loss 0.006215, current_train_items 249792.
I0304 19:31:51.477355 23128000471168 run.py:483] Algo bellman_ford step 7806 current loss 0.002665, current_train_items 249824.
I0304 19:31:51.500889 23128000471168 run.py:483] Algo bellman_ford step 7807 current loss 0.010652, current_train_items 249856.
I0304 19:31:51.532104 23128000471168 run.py:483] Algo bellman_ford step 7808 current loss 0.026674, current_train_items 249888.
I0304 19:31:51.566291 23128000471168 run.py:483] Algo bellman_ford step 7809 current loss 0.112710, current_train_items 249920.
I0304 19:31:51.586715 23128000471168 run.py:483] Algo bellman_ford step 7810 current loss 0.001159, current_train_items 249952.
I0304 19:31:51.603533 23128000471168 run.py:483] Algo bellman_ford step 7811 current loss 0.004099, current_train_items 249984.
I0304 19:31:51.628178 23128000471168 run.py:483] Algo bellman_ford step 7812 current loss 0.050481, current_train_items 250016.
I0304 19:31:51.661306 23128000471168 run.py:483] Algo bellman_ford step 7813 current loss 0.053008, current_train_items 250048.
I0304 19:31:51.695617 23128000471168 run.py:483] Algo bellman_ford step 7814 current loss 0.068218, current_train_items 250080.
I0304 19:31:51.715456 23128000471168 run.py:483] Algo bellman_ford step 7815 current loss 0.010128, current_train_items 250112.
I0304 19:31:51.732069 23128000471168 run.py:483] Algo bellman_ford step 7816 current loss 0.015990, current_train_items 250144.
I0304 19:31:51.756755 23128000471168 run.py:483] Algo bellman_ford step 7817 current loss 0.023461, current_train_items 250176.
I0304 19:31:51.787663 23128000471168 run.py:483] Algo bellman_ford step 7818 current loss 0.065253, current_train_items 250208.
I0304 19:31:51.821121 23128000471168 run.py:483] Algo bellman_ford step 7819 current loss 0.043500, current_train_items 250240.
I0304 19:31:51.841225 23128000471168 run.py:483] Algo bellman_ford step 7820 current loss 0.001508, current_train_items 250272.
I0304 19:31:51.857965 23128000471168 run.py:483] Algo bellman_ford step 7821 current loss 0.015980, current_train_items 250304.
I0304 19:31:51.883589 23128000471168 run.py:483] Algo bellman_ford step 7822 current loss 0.047610, current_train_items 250336.
I0304 19:31:51.917680 23128000471168 run.py:483] Algo bellman_ford step 7823 current loss 0.225313, current_train_items 250368.
I0304 19:31:51.950707 23128000471168 run.py:483] Algo bellman_ford step 7824 current loss 0.265991, current_train_items 250400.
I0304 19:31:51.970459 23128000471168 run.py:483] Algo bellman_ford step 7825 current loss 0.002732, current_train_items 250432.
I0304 19:31:51.987284 23128000471168 run.py:483] Algo bellman_ford step 7826 current loss 0.032527, current_train_items 250464.
I0304 19:31:52.011937 23128000471168 run.py:483] Algo bellman_ford step 7827 current loss 0.022708, current_train_items 250496.
I0304 19:31:52.044869 23128000471168 run.py:483] Algo bellman_ford step 7828 current loss 0.058332, current_train_items 250528.
I0304 19:31:52.079105 23128000471168 run.py:483] Algo bellman_ford step 7829 current loss 0.088863, current_train_items 250560.
I0304 19:31:52.098996 23128000471168 run.py:483] Algo bellman_ford step 7830 current loss 0.004623, current_train_items 250592.
I0304 19:31:52.115181 23128000471168 run.py:483] Algo bellman_ford step 7831 current loss 0.053429, current_train_items 250624.
I0304 19:31:52.139630 23128000471168 run.py:483] Algo bellman_ford step 7832 current loss 0.030837, current_train_items 250656.
I0304 19:31:52.172605 23128000471168 run.py:483] Algo bellman_ford step 7833 current loss 0.060557, current_train_items 250688.
I0304 19:31:52.205120 23128000471168 run.py:483] Algo bellman_ford step 7834 current loss 0.079363, current_train_items 250720.
I0304 19:31:52.224921 23128000471168 run.py:483] Algo bellman_ford step 7835 current loss 0.004910, current_train_items 250752.
I0304 19:31:52.241658 23128000471168 run.py:483] Algo bellman_ford step 7836 current loss 0.012178, current_train_items 250784.
I0304 19:31:52.266022 23128000471168 run.py:483] Algo bellman_ford step 7837 current loss 0.024383, current_train_items 250816.
I0304 19:31:52.299958 23128000471168 run.py:483] Algo bellman_ford step 7838 current loss 0.033311, current_train_items 250848.
I0304 19:31:52.335219 23128000471168 run.py:483] Algo bellman_ford step 7839 current loss 0.172265, current_train_items 250880.
I0304 19:31:52.355103 23128000471168 run.py:483] Algo bellman_ford step 7840 current loss 0.002966, current_train_items 250912.
I0304 19:31:52.371853 23128000471168 run.py:483] Algo bellman_ford step 7841 current loss 0.022598, current_train_items 250944.
I0304 19:31:52.396241 23128000471168 run.py:483] Algo bellman_ford step 7842 current loss 0.019819, current_train_items 250976.
I0304 19:31:52.429819 23128000471168 run.py:483] Algo bellman_ford step 7843 current loss 0.046414, current_train_items 251008.
I0304 19:31:52.464985 23128000471168 run.py:483] Algo bellman_ford step 7844 current loss 0.103458, current_train_items 251040.
I0304 19:31:52.485205 23128000471168 run.py:483] Algo bellman_ford step 7845 current loss 0.003068, current_train_items 251072.
I0304 19:31:52.501662 23128000471168 run.py:483] Algo bellman_ford step 7846 current loss 0.010279, current_train_items 251104.
I0304 19:31:52.526437 23128000471168 run.py:483] Algo bellman_ford step 7847 current loss 0.044887, current_train_items 251136.
I0304 19:31:52.559297 23128000471168 run.py:483] Algo bellman_ford step 7848 current loss 0.064948, current_train_items 251168.
I0304 19:31:52.594035 23128000471168 run.py:483] Algo bellman_ford step 7849 current loss 0.061996, current_train_items 251200.
I0304 19:31:52.614259 23128000471168 run.py:483] Algo bellman_ford step 7850 current loss 0.003234, current_train_items 251232.
I0304 19:31:52.622231 23128000471168 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0304 19:31:52.622344 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:31:52.639612 23128000471168 run.py:483] Algo bellman_ford step 7851 current loss 0.045030, current_train_items 251264.
I0304 19:31:52.664468 23128000471168 run.py:483] Algo bellman_ford step 7852 current loss 0.019133, current_train_items 251296.
I0304 19:31:52.698181 23128000471168 run.py:483] Algo bellman_ford step 7853 current loss 0.047462, current_train_items 251328.
I0304 19:31:52.734893 23128000471168 run.py:483] Algo bellman_ford step 7854 current loss 0.066023, current_train_items 251360.
I0304 19:31:52.754834 23128000471168 run.py:483] Algo bellman_ford step 7855 current loss 0.002461, current_train_items 251392.
I0304 19:31:52.771533 23128000471168 run.py:483] Algo bellman_ford step 7856 current loss 0.016741, current_train_items 251424.
I0304 19:31:52.796670 23128000471168 run.py:483] Algo bellman_ford step 7857 current loss 0.043545, current_train_items 251456.
I0304 19:31:52.829013 23128000471168 run.py:483] Algo bellman_ford step 7858 current loss 0.020010, current_train_items 251488.
I0304 19:31:52.864147 23128000471168 run.py:483] Algo bellman_ford step 7859 current loss 0.061988, current_train_items 251520.
I0304 19:31:52.884371 23128000471168 run.py:483] Algo bellman_ford step 7860 current loss 0.010414, current_train_items 251552.
I0304 19:31:52.900756 23128000471168 run.py:483] Algo bellman_ford step 7861 current loss 0.007393, current_train_items 251584.
I0304 19:31:52.925361 23128000471168 run.py:483] Algo bellman_ford step 7862 current loss 0.053649, current_train_items 251616.
I0304 19:31:52.957565 23128000471168 run.py:483] Algo bellman_ford step 7863 current loss 0.086868, current_train_items 251648.
I0304 19:31:52.990202 23128000471168 run.py:483] Algo bellman_ford step 7864 current loss 0.032439, current_train_items 251680.
I0304 19:31:53.010735 23128000471168 run.py:483] Algo bellman_ford step 7865 current loss 0.003063, current_train_items 251712.
I0304 19:31:53.027700 23128000471168 run.py:483] Algo bellman_ford step 7866 current loss 0.018412, current_train_items 251744.
I0304 19:31:53.052867 23128000471168 run.py:483] Algo bellman_ford step 7867 current loss 0.049360, current_train_items 251776.
I0304 19:31:53.085049 23128000471168 run.py:483] Algo bellman_ford step 7868 current loss 0.040182, current_train_items 251808.
I0304 19:31:53.118136 23128000471168 run.py:483] Algo bellman_ford step 7869 current loss 0.039268, current_train_items 251840.
I0304 19:31:53.138434 23128000471168 run.py:483] Algo bellman_ford step 7870 current loss 0.003430, current_train_items 251872.
I0304 19:31:53.155050 23128000471168 run.py:483] Algo bellman_ford step 7871 current loss 0.012113, current_train_items 251904.
I0304 19:31:53.179566 23128000471168 run.py:483] Algo bellman_ford step 7872 current loss 0.034236, current_train_items 251936.
I0304 19:31:53.211161 23128000471168 run.py:483] Algo bellman_ford step 7873 current loss 0.057853, current_train_items 251968.
I0304 19:31:53.244574 23128000471168 run.py:483] Algo bellman_ford step 7874 current loss 0.056152, current_train_items 252000.
I0304 19:31:53.264869 23128000471168 run.py:483] Algo bellman_ford step 7875 current loss 0.008420, current_train_items 252032.
I0304 19:31:53.281285 23128000471168 run.py:483] Algo bellman_ford step 7876 current loss 0.037630, current_train_items 252064.
I0304 19:31:53.305346 23128000471168 run.py:483] Algo bellman_ford step 7877 current loss 0.071532, current_train_items 252096.
I0304 19:31:53.337975 23128000471168 run.py:483] Algo bellman_ford step 7878 current loss 0.062436, current_train_items 252128.
I0304 19:31:53.372336 23128000471168 run.py:483] Algo bellman_ford step 7879 current loss 0.103263, current_train_items 252160.
I0304 19:31:53.391999 23128000471168 run.py:483] Algo bellman_ford step 7880 current loss 0.003947, current_train_items 252192.
I0304 19:31:53.408094 23128000471168 run.py:483] Algo bellman_ford step 7881 current loss 0.016621, current_train_items 252224.
I0304 19:31:53.432027 23128000471168 run.py:483] Algo bellman_ford step 7882 current loss 0.033670, current_train_items 252256.
I0304 19:31:53.464946 23128000471168 run.py:483] Algo bellman_ford step 7883 current loss 0.045856, current_train_items 252288.
I0304 19:31:53.499366 23128000471168 run.py:483] Algo bellman_ford step 7884 current loss 0.084463, current_train_items 252320.
I0304 19:31:53.519995 23128000471168 run.py:483] Algo bellman_ford step 7885 current loss 0.002691, current_train_items 252352.
I0304 19:31:53.536681 23128000471168 run.py:483] Algo bellman_ford step 7886 current loss 0.014564, current_train_items 252384.
I0304 19:31:53.559835 23128000471168 run.py:483] Algo bellman_ford step 7887 current loss 0.067450, current_train_items 252416.
I0304 19:31:53.590725 23128000471168 run.py:483] Algo bellman_ford step 7888 current loss 0.022799, current_train_items 252448.
I0304 19:31:53.624332 23128000471168 run.py:483] Algo bellman_ford step 7889 current loss 0.048275, current_train_items 252480.
I0304 19:31:53.644931 23128000471168 run.py:483] Algo bellman_ford step 7890 current loss 0.002751, current_train_items 252512.
I0304 19:31:53.661413 23128000471168 run.py:483] Algo bellman_ford step 7891 current loss 0.003355, current_train_items 252544.
I0304 19:31:53.686101 23128000471168 run.py:483] Algo bellman_ford step 7892 current loss 0.041256, current_train_items 252576.
I0304 19:31:53.718948 23128000471168 run.py:483] Algo bellman_ford step 7893 current loss 0.032206, current_train_items 252608.
I0304 19:31:53.752426 23128000471168 run.py:483] Algo bellman_ford step 7894 current loss 0.044803, current_train_items 252640.
I0304 19:31:53.772662 23128000471168 run.py:483] Algo bellman_ford step 7895 current loss 0.004463, current_train_items 252672.
I0304 19:31:53.788852 23128000471168 run.py:483] Algo bellman_ford step 7896 current loss 0.019940, current_train_items 252704.
I0304 19:31:53.813828 23128000471168 run.py:483] Algo bellman_ford step 7897 current loss 0.073115, current_train_items 252736.
I0304 19:31:53.847010 23128000471168 run.py:483] Algo bellman_ford step 7898 current loss 0.036713, current_train_items 252768.
I0304 19:31:53.881280 23128000471168 run.py:483] Algo bellman_ford step 7899 current loss 0.072522, current_train_items 252800.
I0304 19:31:53.901418 23128000471168 run.py:483] Algo bellman_ford step 7900 current loss 0.004970, current_train_items 252832.
I0304 19:31:53.909433 23128000471168 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0304 19:31:53.909548 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:53.926631 23128000471168 run.py:483] Algo bellman_ford step 7901 current loss 0.017986, current_train_items 252864.
I0304 19:31:53.952185 23128000471168 run.py:483] Algo bellman_ford step 7902 current loss 0.037635, current_train_items 252896.
I0304 19:31:53.986142 23128000471168 run.py:483] Algo bellman_ford step 7903 current loss 0.028238, current_train_items 252928.
I0304 19:31:54.022977 23128000471168 run.py:483] Algo bellman_ford step 7904 current loss 0.069508, current_train_items 252960.
I0304 19:31:54.043190 23128000471168 run.py:483] Algo bellman_ford step 7905 current loss 0.002916, current_train_items 252992.
I0304 19:31:54.059997 23128000471168 run.py:483] Algo bellman_ford step 7906 current loss 0.024176, current_train_items 253024.
I0304 19:31:54.085052 23128000471168 run.py:483] Algo bellman_ford step 7907 current loss 0.035860, current_train_items 253056.
I0304 19:31:54.117104 23128000471168 run.py:483] Algo bellman_ford step 7908 current loss 0.032519, current_train_items 253088.
I0304 19:31:54.148785 23128000471168 run.py:483] Algo bellman_ford step 7909 current loss 0.063987, current_train_items 253120.
I0304 19:31:54.168539 23128000471168 run.py:483] Algo bellman_ford step 7910 current loss 0.010681, current_train_items 253152.
I0304 19:31:54.185728 23128000471168 run.py:483] Algo bellman_ford step 7911 current loss 0.018889, current_train_items 253184.
I0304 19:31:54.210671 23128000471168 run.py:483] Algo bellman_ford step 7912 current loss 0.047834, current_train_items 253216.
I0304 19:31:54.243200 23128000471168 run.py:483] Algo bellman_ford step 7913 current loss 0.028096, current_train_items 253248.
I0304 19:31:54.277254 23128000471168 run.py:483] Algo bellman_ford step 7914 current loss 0.064305, current_train_items 253280.
I0304 19:31:54.297306 23128000471168 run.py:483] Algo bellman_ford step 7915 current loss 0.011268, current_train_items 253312.
I0304 19:31:54.313995 23128000471168 run.py:483] Algo bellman_ford step 7916 current loss 0.023113, current_train_items 253344.
I0304 19:31:54.338407 23128000471168 run.py:483] Algo bellman_ford step 7917 current loss 0.048392, current_train_items 253376.
I0304 19:31:54.370936 23128000471168 run.py:483] Algo bellman_ford step 7918 current loss 0.034132, current_train_items 253408.
I0304 19:31:54.406517 23128000471168 run.py:483] Algo bellman_ford step 7919 current loss 0.041295, current_train_items 253440.
I0304 19:31:54.426733 23128000471168 run.py:483] Algo bellman_ford step 7920 current loss 0.001491, current_train_items 253472.
I0304 19:31:54.443403 23128000471168 run.py:483] Algo bellman_ford step 7921 current loss 0.005498, current_train_items 253504.
I0304 19:31:54.468094 23128000471168 run.py:483] Algo bellman_ford step 7922 current loss 0.037411, current_train_items 253536.
I0304 19:31:54.499682 23128000471168 run.py:483] Algo bellman_ford step 7923 current loss 0.049453, current_train_items 253568.
I0304 19:31:54.533802 23128000471168 run.py:483] Algo bellman_ford step 7924 current loss 0.088573, current_train_items 253600.
I0304 19:31:54.553797 23128000471168 run.py:483] Algo bellman_ford step 7925 current loss 0.012190, current_train_items 253632.
I0304 19:31:54.570407 23128000471168 run.py:483] Algo bellman_ford step 7926 current loss 0.009843, current_train_items 253664.
I0304 19:31:54.594663 23128000471168 run.py:483] Algo bellman_ford step 7927 current loss 0.059518, current_train_items 253696.
I0304 19:31:54.628047 23128000471168 run.py:483] Algo bellman_ford step 7928 current loss 0.091219, current_train_items 253728.
I0304 19:31:54.661449 23128000471168 run.py:483] Algo bellman_ford step 7929 current loss 0.053776, current_train_items 253760.
I0304 19:31:54.681353 23128000471168 run.py:483] Algo bellman_ford step 7930 current loss 0.003754, current_train_items 253792.
I0304 19:31:54.697755 23128000471168 run.py:483] Algo bellman_ford step 7931 current loss 0.022604, current_train_items 253824.
I0304 19:31:54.721261 23128000471168 run.py:483] Algo bellman_ford step 7932 current loss 0.045704, current_train_items 253856.
I0304 19:31:54.754229 23128000471168 run.py:483] Algo bellman_ford step 7933 current loss 0.048131, current_train_items 253888.
I0304 19:31:54.788679 23128000471168 run.py:483] Algo bellman_ford step 7934 current loss 0.069869, current_train_items 253920.
I0304 19:31:54.808795 23128000471168 run.py:483] Algo bellman_ford step 7935 current loss 0.003819, current_train_items 253952.
I0304 19:31:54.824943 23128000471168 run.py:483] Algo bellman_ford step 7936 current loss 0.024290, current_train_items 253984.
I0304 19:31:54.849818 23128000471168 run.py:483] Algo bellman_ford step 7937 current loss 0.039095, current_train_items 254016.
I0304 19:31:54.881382 23128000471168 run.py:483] Algo bellman_ford step 7938 current loss 0.040275, current_train_items 254048.
I0304 19:31:54.916229 23128000471168 run.py:483] Algo bellman_ford step 7939 current loss 0.068168, current_train_items 254080.
I0304 19:31:54.936321 23128000471168 run.py:483] Algo bellman_ford step 7940 current loss 0.003269, current_train_items 254112.
I0304 19:31:54.952212 23128000471168 run.py:483] Algo bellman_ford step 7941 current loss 0.004573, current_train_items 254144.
I0304 19:31:54.977768 23128000471168 run.py:483] Algo bellman_ford step 7942 current loss 0.074496, current_train_items 254176.
I0304 19:31:55.011482 23128000471168 run.py:483] Algo bellman_ford step 7943 current loss 0.132964, current_train_items 254208.
I0304 19:31:55.044968 23128000471168 run.py:483] Algo bellman_ford step 7944 current loss 0.100758, current_train_items 254240.
I0304 19:31:55.064784 23128000471168 run.py:483] Algo bellman_ford step 7945 current loss 0.003591, current_train_items 254272.
I0304 19:31:55.080786 23128000471168 run.py:483] Algo bellman_ford step 7946 current loss 0.033724, current_train_items 254304.
I0304 19:31:55.104443 23128000471168 run.py:483] Algo bellman_ford step 7947 current loss 0.060444, current_train_items 254336.
I0304 19:31:55.135182 23128000471168 run.py:483] Algo bellman_ford step 7948 current loss 0.014735, current_train_items 254368.
I0304 19:31:55.170260 23128000471168 run.py:483] Algo bellman_ford step 7949 current loss 0.068634, current_train_items 254400.
I0304 19:31:55.189991 23128000471168 run.py:483] Algo bellman_ford step 7950 current loss 0.002798, current_train_items 254432.
I0304 19:31:55.198077 23128000471168 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0304 19:31:55.198185 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:55.215545 23128000471168 run.py:483] Algo bellman_ford step 7951 current loss 0.017238, current_train_items 254464.
I0304 19:31:55.240847 23128000471168 run.py:483] Algo bellman_ford step 7952 current loss 0.026990, current_train_items 254496.
I0304 19:31:55.274615 23128000471168 run.py:483] Algo bellman_ford step 7953 current loss 0.026770, current_train_items 254528.
I0304 19:31:55.308597 23128000471168 run.py:483] Algo bellman_ford step 7954 current loss 0.034495, current_train_items 254560.
I0304 19:31:55.329202 23128000471168 run.py:483] Algo bellman_ford step 7955 current loss 0.010472, current_train_items 254592.
I0304 19:31:55.345581 23128000471168 run.py:483] Algo bellman_ford step 7956 current loss 0.030643, current_train_items 254624.
I0304 19:31:55.370113 23128000471168 run.py:483] Algo bellman_ford step 7957 current loss 0.037364, current_train_items 254656.
I0304 19:31:55.402479 23128000471168 run.py:483] Algo bellman_ford step 7958 current loss 0.039331, current_train_items 254688.
I0304 19:31:55.436523 23128000471168 run.py:483] Algo bellman_ford step 7959 current loss 0.038132, current_train_items 254720.
I0304 19:31:55.457248 23128000471168 run.py:483] Algo bellman_ford step 7960 current loss 0.002457, current_train_items 254752.
I0304 19:31:55.474283 23128000471168 run.py:483] Algo bellman_ford step 7961 current loss 0.018273, current_train_items 254784.
I0304 19:31:55.498374 23128000471168 run.py:483] Algo bellman_ford step 7962 current loss 0.042067, current_train_items 254816.
I0304 19:31:55.529665 23128000471168 run.py:483] Algo bellman_ford step 7963 current loss 0.029270, current_train_items 254848.
I0304 19:31:55.564107 23128000471168 run.py:483] Algo bellman_ford step 7964 current loss 0.033369, current_train_items 254880.
I0304 19:31:55.584382 23128000471168 run.py:483] Algo bellman_ford step 7965 current loss 0.002791, current_train_items 254912.
I0304 19:31:55.601108 23128000471168 run.py:483] Algo bellman_ford step 7966 current loss 0.024566, current_train_items 254944.
I0304 19:31:55.626931 23128000471168 run.py:483] Algo bellman_ford step 7967 current loss 0.054821, current_train_items 254976.
I0304 19:31:55.659121 23128000471168 run.py:483] Algo bellman_ford step 7968 current loss 0.044418, current_train_items 255008.
I0304 19:31:55.691972 23128000471168 run.py:483] Algo bellman_ford step 7969 current loss 0.049612, current_train_items 255040.
I0304 19:31:55.712424 23128000471168 run.py:483] Algo bellman_ford step 7970 current loss 0.002913, current_train_items 255072.
I0304 19:31:55.729051 23128000471168 run.py:483] Algo bellman_ford step 7971 current loss 0.014393, current_train_items 255104.
I0304 19:31:55.754074 23128000471168 run.py:483] Algo bellman_ford step 7972 current loss 0.035635, current_train_items 255136.
I0304 19:31:55.785994 23128000471168 run.py:483] Algo bellman_ford step 7973 current loss 0.037675, current_train_items 255168.
I0304 19:31:55.821347 23128000471168 run.py:483] Algo bellman_ford step 7974 current loss 0.061241, current_train_items 255200.
I0304 19:31:55.842127 23128000471168 run.py:483] Algo bellman_ford step 7975 current loss 0.005513, current_train_items 255232.
I0304 19:31:55.858914 23128000471168 run.py:483] Algo bellman_ford step 7976 current loss 0.025097, current_train_items 255264.
I0304 19:31:55.883479 23128000471168 run.py:483] Algo bellman_ford step 7977 current loss 0.034736, current_train_items 255296.
I0304 19:31:55.916100 23128000471168 run.py:483] Algo bellman_ford step 7978 current loss 0.038414, current_train_items 255328.
I0304 19:31:55.948506 23128000471168 run.py:483] Algo bellman_ford step 7979 current loss 0.043211, current_train_items 255360.
I0304 19:31:55.968849 23128000471168 run.py:483] Algo bellman_ford step 7980 current loss 0.001401, current_train_items 255392.
I0304 19:31:55.985169 23128000471168 run.py:483] Algo bellman_ford step 7981 current loss 0.019882, current_train_items 255424.
I0304 19:31:56.009120 23128000471168 run.py:483] Algo bellman_ford step 7982 current loss 0.020545, current_train_items 255456.
I0304 19:31:56.040873 23128000471168 run.py:483] Algo bellman_ford step 7983 current loss 0.025455, current_train_items 255488.
I0304 19:31:56.075813 23128000471168 run.py:483] Algo bellman_ford step 7984 current loss 0.044427, current_train_items 255520.
I0304 19:31:56.096282 23128000471168 run.py:483] Algo bellman_ford step 7985 current loss 0.001936, current_train_items 255552.
I0304 19:31:56.113072 23128000471168 run.py:483] Algo bellman_ford step 7986 current loss 0.019049, current_train_items 255584.
I0304 19:31:56.136670 23128000471168 run.py:483] Algo bellman_ford step 7987 current loss 0.042686, current_train_items 255616.
I0304 19:31:56.169687 23128000471168 run.py:483] Algo bellman_ford step 7988 current loss 0.028776, current_train_items 255648.
I0304 19:31:56.203513 23128000471168 run.py:483] Algo bellman_ford step 7989 current loss 0.044924, current_train_items 255680.
I0304 19:31:56.223849 23128000471168 run.py:483] Algo bellman_ford step 7990 current loss 0.001802, current_train_items 255712.
I0304 19:31:56.240498 23128000471168 run.py:483] Algo bellman_ford step 7991 current loss 0.026763, current_train_items 255744.
I0304 19:31:56.264896 23128000471168 run.py:483] Algo bellman_ford step 7992 current loss 0.041071, current_train_items 255776.
I0304 19:31:56.299161 23128000471168 run.py:483] Algo bellman_ford step 7993 current loss 0.032038, current_train_items 255808.
I0304 19:31:56.334108 23128000471168 run.py:483] Algo bellman_ford step 7994 current loss 0.026698, current_train_items 255840.
I0304 19:31:56.353785 23128000471168 run.py:483] Algo bellman_ford step 7995 current loss 0.002234, current_train_items 255872.
I0304 19:31:56.369739 23128000471168 run.py:483] Algo bellman_ford step 7996 current loss 0.008076, current_train_items 255904.
I0304 19:31:56.394093 23128000471168 run.py:483] Algo bellman_ford step 7997 current loss 0.034107, current_train_items 255936.
I0304 19:31:56.426052 23128000471168 run.py:483] Algo bellman_ford step 7998 current loss 0.013967, current_train_items 255968.
I0304 19:31:56.458375 23128000471168 run.py:483] Algo bellman_ford step 7999 current loss 0.041995, current_train_items 256000.
I0304 19:31:56.478569 23128000471168 run.py:483] Algo bellman_ford step 8000 current loss 0.001883, current_train_items 256032.
I0304 19:31:56.486584 23128000471168 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0304 19:31:56.486694 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:56.504102 23128000471168 run.py:483] Algo bellman_ford step 8001 current loss 0.005296, current_train_items 256064.
I0304 19:31:56.528448 23128000471168 run.py:483] Algo bellman_ford step 8002 current loss 0.031337, current_train_items 256096.
I0304 19:31:56.561905 23128000471168 run.py:483] Algo bellman_ford step 8003 current loss 0.033414, current_train_items 256128.
I0304 19:31:56.595229 23128000471168 run.py:483] Algo bellman_ford step 8004 current loss 0.051071, current_train_items 256160.
I0304 19:31:56.615612 23128000471168 run.py:483] Algo bellman_ford step 8005 current loss 0.001432, current_train_items 256192.
I0304 19:31:56.631925 23128000471168 run.py:483] Algo bellman_ford step 8006 current loss 0.010890, current_train_items 256224.
I0304 19:31:56.656393 23128000471168 run.py:483] Algo bellman_ford step 8007 current loss 0.032231, current_train_items 256256.
I0304 19:31:56.689915 23128000471168 run.py:483] Algo bellman_ford step 8008 current loss 0.046469, current_train_items 256288.
I0304 19:31:56.724668 23128000471168 run.py:483] Algo bellman_ford step 8009 current loss 0.057226, current_train_items 256320.
I0304 19:31:56.744628 23128000471168 run.py:483] Algo bellman_ford step 8010 current loss 0.000919, current_train_items 256352.
I0304 19:31:56.760909 23128000471168 run.py:483] Algo bellman_ford step 8011 current loss 0.016720, current_train_items 256384.
I0304 19:31:56.786077 23128000471168 run.py:483] Algo bellman_ford step 8012 current loss 0.034547, current_train_items 256416.
I0304 19:31:56.818166 23128000471168 run.py:483] Algo bellman_ford step 8013 current loss 0.056407, current_train_items 256448.
I0304 19:31:56.850454 23128000471168 run.py:483] Algo bellman_ford step 8014 current loss 0.074491, current_train_items 256480.
I0304 19:31:56.870736 23128000471168 run.py:483] Algo bellman_ford step 8015 current loss 0.028997, current_train_items 256512.
I0304 19:31:56.887744 23128000471168 run.py:483] Algo bellman_ford step 8016 current loss 0.029685, current_train_items 256544.
I0304 19:31:56.911612 23128000471168 run.py:483] Algo bellman_ford step 8017 current loss 0.037564, current_train_items 256576.
I0304 19:31:56.944212 23128000471168 run.py:483] Algo bellman_ford step 8018 current loss 0.045105, current_train_items 256608.
I0304 19:31:56.976674 23128000471168 run.py:483] Algo bellman_ford step 8019 current loss 0.029895, current_train_items 256640.
I0304 19:31:56.996663 23128000471168 run.py:483] Algo bellman_ford step 8020 current loss 0.001036, current_train_items 256672.
I0304 19:31:57.013097 23128000471168 run.py:483] Algo bellman_ford step 8021 current loss 0.004349, current_train_items 256704.
I0304 19:31:57.037356 23128000471168 run.py:483] Algo bellman_ford step 8022 current loss 0.021108, current_train_items 256736.
I0304 19:31:57.069360 23128000471168 run.py:483] Algo bellman_ford step 8023 current loss 0.041936, current_train_items 256768.
I0304 19:31:57.104845 23128000471168 run.py:483] Algo bellman_ford step 8024 current loss 0.055822, current_train_items 256800.
I0304 19:31:57.124922 23128000471168 run.py:483] Algo bellman_ford step 8025 current loss 0.018679, current_train_items 256832.
I0304 19:31:57.141642 23128000471168 run.py:483] Algo bellman_ford step 8026 current loss 0.008037, current_train_items 256864.
I0304 19:31:57.166251 23128000471168 run.py:483] Algo bellman_ford step 8027 current loss 0.035214, current_train_items 256896.
I0304 19:31:57.197522 23128000471168 run.py:483] Algo bellman_ford step 8028 current loss 0.022957, current_train_items 256928.
I0304 19:31:57.231734 23128000471168 run.py:483] Algo bellman_ford step 8029 current loss 0.067238, current_train_items 256960.
I0304 19:31:57.251559 23128000471168 run.py:483] Algo bellman_ford step 8030 current loss 0.000991, current_train_items 256992.
I0304 19:31:57.267827 23128000471168 run.py:483] Algo bellman_ford step 8031 current loss 0.016219, current_train_items 257024.
I0304 19:31:57.292969 23128000471168 run.py:483] Algo bellman_ford step 8032 current loss 0.030811, current_train_items 257056.
I0304 19:31:57.325970 23128000471168 run.py:483] Algo bellman_ford step 8033 current loss 0.046238, current_train_items 257088.
I0304 19:31:57.360859 23128000471168 run.py:483] Algo bellman_ford step 8034 current loss 0.044839, current_train_items 257120.
I0304 19:31:57.380944 23128000471168 run.py:483] Algo bellman_ford step 8035 current loss 0.001597, current_train_items 257152.
I0304 19:31:57.397821 23128000471168 run.py:483] Algo bellman_ford step 8036 current loss 0.027492, current_train_items 257184.
I0304 19:31:57.422550 23128000471168 run.py:483] Algo bellman_ford step 8037 current loss 0.037031, current_train_items 257216.
I0304 19:31:57.454586 23128000471168 run.py:483] Algo bellman_ford step 8038 current loss 0.032213, current_train_items 257248.
I0304 19:31:57.490265 23128000471168 run.py:483] Algo bellman_ford step 8039 current loss 0.065021, current_train_items 257280.
I0304 19:31:57.510118 23128000471168 run.py:483] Algo bellman_ford step 8040 current loss 0.001869, current_train_items 257312.
I0304 19:31:57.526769 23128000471168 run.py:483] Algo bellman_ford step 8041 current loss 0.025376, current_train_items 257344.
I0304 19:31:57.551326 23128000471168 run.py:483] Algo bellman_ford step 8042 current loss 0.066525, current_train_items 257376.
I0304 19:31:57.584089 23128000471168 run.py:483] Algo bellman_ford step 8043 current loss 0.073658, current_train_items 257408.
I0304 19:31:57.618424 23128000471168 run.py:483] Algo bellman_ford step 8044 current loss 0.074418, current_train_items 257440.
I0304 19:31:57.638628 23128000471168 run.py:483] Algo bellman_ford step 8045 current loss 0.002672, current_train_items 257472.
I0304 19:31:57.655839 23128000471168 run.py:483] Algo bellman_ford step 8046 current loss 0.041931, current_train_items 257504.
I0304 19:31:57.679902 23128000471168 run.py:483] Algo bellman_ford step 8047 current loss 0.028791, current_train_items 257536.
I0304 19:31:57.712669 23128000471168 run.py:483] Algo bellman_ford step 8048 current loss 0.073350, current_train_items 257568.
I0304 19:31:57.745150 23128000471168 run.py:483] Algo bellman_ford step 8049 current loss 0.048454, current_train_items 257600.
I0304 19:31:57.765399 23128000471168 run.py:483] Algo bellman_ford step 8050 current loss 0.002270, current_train_items 257632.
I0304 19:31:57.773548 23128000471168 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0304 19:31:57.773656 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:57.790958 23128000471168 run.py:483] Algo bellman_ford step 8051 current loss 0.023914, current_train_items 257664.
I0304 19:31:57.815818 23128000471168 run.py:483] Algo bellman_ford step 8052 current loss 0.017792, current_train_items 257696.
I0304 19:31:57.848385 23128000471168 run.py:483] Algo bellman_ford step 8053 current loss 0.038275, current_train_items 257728.
I0304 19:31:57.883819 23128000471168 run.py:483] Algo bellman_ford step 8054 current loss 0.185909, current_train_items 257760.
I0304 19:31:57.903961 23128000471168 run.py:483] Algo bellman_ford step 8055 current loss 0.004833, current_train_items 257792.
I0304 19:31:57.919580 23128000471168 run.py:483] Algo bellman_ford step 8056 current loss 0.004512, current_train_items 257824.
I0304 19:31:57.942858 23128000471168 run.py:483] Algo bellman_ford step 8057 current loss 0.040594, current_train_items 257856.
I0304 19:31:57.976472 23128000471168 run.py:483] Algo bellman_ford step 8058 current loss 0.034098, current_train_items 257888.
I0304 19:31:58.012706 23128000471168 run.py:483] Algo bellman_ford step 8059 current loss 0.051985, current_train_items 257920.
I0304 19:31:58.033082 23128000471168 run.py:483] Algo bellman_ford step 8060 current loss 0.008351, current_train_items 257952.
I0304 19:31:58.049323 23128000471168 run.py:483] Algo bellman_ford step 8061 current loss 0.047928, current_train_items 257984.
I0304 19:31:58.074755 23128000471168 run.py:483] Algo bellman_ford step 8062 current loss 0.043630, current_train_items 258016.
I0304 19:31:58.107306 23128000471168 run.py:483] Algo bellman_ford step 8063 current loss 0.040938, current_train_items 258048.
I0304 19:31:58.140273 23128000471168 run.py:483] Algo bellman_ford step 8064 current loss 0.040184, current_train_items 258080.
I0304 19:31:58.160166 23128000471168 run.py:483] Algo bellman_ford step 8065 current loss 0.002735, current_train_items 258112.
I0304 19:31:58.176575 23128000471168 run.py:483] Algo bellman_ford step 8066 current loss 0.028665, current_train_items 258144.
I0304 19:31:58.201221 23128000471168 run.py:483] Algo bellman_ford step 8067 current loss 0.050295, current_train_items 258176.
I0304 19:31:58.233536 23128000471168 run.py:483] Algo bellman_ford step 8068 current loss 0.065058, current_train_items 258208.
I0304 19:31:58.266448 23128000471168 run.py:483] Algo bellman_ford step 8069 current loss 0.040062, current_train_items 258240.
I0304 19:31:58.286900 23128000471168 run.py:483] Algo bellman_ford step 8070 current loss 0.001997, current_train_items 258272.
I0304 19:31:58.304235 23128000471168 run.py:483] Algo bellman_ford step 8071 current loss 0.037011, current_train_items 258304.
I0304 19:31:58.328625 23128000471168 run.py:483] Algo bellman_ford step 8072 current loss 0.027604, current_train_items 258336.
I0304 19:31:58.361201 23128000471168 run.py:483] Algo bellman_ford step 8073 current loss 0.043370, current_train_items 258368.
I0304 19:31:58.396468 23128000471168 run.py:483] Algo bellman_ford step 8074 current loss 0.043667, current_train_items 258400.
I0304 19:31:58.417298 23128000471168 run.py:483] Algo bellman_ford step 8075 current loss 0.002000, current_train_items 258432.
I0304 19:31:58.433923 23128000471168 run.py:483] Algo bellman_ford step 8076 current loss 0.041729, current_train_items 258464.
I0304 19:31:58.457755 23128000471168 run.py:483] Algo bellman_ford step 8077 current loss 0.007231, current_train_items 258496.
I0304 19:31:58.489589 23128000471168 run.py:483] Algo bellman_ford step 8078 current loss 0.057782, current_train_items 258528.
I0304 19:31:58.522078 23128000471168 run.py:483] Algo bellman_ford step 8079 current loss 0.039606, current_train_items 258560.
I0304 19:31:58.542075 23128000471168 run.py:483] Algo bellman_ford step 8080 current loss 0.006589, current_train_items 258592.
I0304 19:31:58.558605 23128000471168 run.py:483] Algo bellman_ford step 8081 current loss 0.011055, current_train_items 258624.
I0304 19:31:58.582874 23128000471168 run.py:483] Algo bellman_ford step 8082 current loss 0.046817, current_train_items 258656.
I0304 19:31:58.615117 23128000471168 run.py:483] Algo bellman_ford step 8083 current loss 0.047738, current_train_items 258688.
I0304 19:31:58.652272 23128000471168 run.py:483] Algo bellman_ford step 8084 current loss 0.046271, current_train_items 258720.
I0304 19:31:58.672537 23128000471168 run.py:483] Algo bellman_ford step 8085 current loss 0.002778, current_train_items 258752.
I0304 19:31:58.689675 23128000471168 run.py:483] Algo bellman_ford step 8086 current loss 0.043063, current_train_items 258784.
I0304 19:31:58.713516 23128000471168 run.py:483] Algo bellman_ford step 8087 current loss 0.033730, current_train_items 258816.
I0304 19:31:58.746975 23128000471168 run.py:483] Algo bellman_ford step 8088 current loss 0.062940, current_train_items 258848.
I0304 19:31:58.779826 23128000471168 run.py:483] Algo bellman_ford step 8089 current loss 0.048477, current_train_items 258880.
I0304 19:31:58.800513 23128000471168 run.py:483] Algo bellman_ford step 8090 current loss 0.005833, current_train_items 258912.
I0304 19:31:58.816590 23128000471168 run.py:483] Algo bellman_ford step 8091 current loss 0.008894, current_train_items 258944.
I0304 19:31:58.841258 23128000471168 run.py:483] Algo bellman_ford step 8092 current loss 0.017264, current_train_items 258976.
I0304 19:31:58.873012 23128000471168 run.py:483] Algo bellman_ford step 8093 current loss 0.035984, current_train_items 259008.
I0304 19:31:58.907601 23128000471168 run.py:483] Algo bellman_ford step 8094 current loss 0.040903, current_train_items 259040.
I0304 19:31:58.927745 23128000471168 run.py:483] Algo bellman_ford step 8095 current loss 0.010488, current_train_items 259072.
I0304 19:31:58.943985 23128000471168 run.py:483] Algo bellman_ford step 8096 current loss 0.014435, current_train_items 259104.
I0304 19:31:58.969826 23128000471168 run.py:483] Algo bellman_ford step 8097 current loss 0.024104, current_train_items 259136.
I0304 19:31:59.002915 23128000471168 run.py:483] Algo bellman_ford step 8098 current loss 0.037642, current_train_items 259168.
I0304 19:31:59.036586 23128000471168 run.py:483] Algo bellman_ford step 8099 current loss 0.076397, current_train_items 259200.
I0304 19:31:59.057035 23128000471168 run.py:483] Algo bellman_ford step 8100 current loss 0.002979, current_train_items 259232.
I0304 19:31:59.065123 23128000471168 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0304 19:31:59.065229 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:59.082436 23128000471168 run.py:483] Algo bellman_ford step 8101 current loss 0.009290, current_train_items 259264.
I0304 19:31:59.108591 23128000471168 run.py:483] Algo bellman_ford step 8102 current loss 0.050297, current_train_items 259296.
I0304 19:31:59.141285 23128000471168 run.py:483] Algo bellman_ford step 8103 current loss 0.025496, current_train_items 259328.
I0304 19:31:59.175408 23128000471168 run.py:483] Algo bellman_ford step 8104 current loss 0.045807, current_train_items 259360.
I0304 19:31:59.196071 23128000471168 run.py:483] Algo bellman_ford step 8105 current loss 0.002202, current_train_items 259392.
I0304 19:31:59.212464 23128000471168 run.py:483] Algo bellman_ford step 8106 current loss 0.034728, current_train_items 259424.
I0304 19:31:59.237051 23128000471168 run.py:483] Algo bellman_ford step 8107 current loss 0.046165, current_train_items 259456.
I0304 19:31:59.269184 23128000471168 run.py:483] Algo bellman_ford step 8108 current loss 0.129649, current_train_items 259488.
I0304 19:31:59.304028 23128000471168 run.py:483] Algo bellman_ford step 8109 current loss 0.050653, current_train_items 259520.
I0304 19:31:59.324347 23128000471168 run.py:483] Algo bellman_ford step 8110 current loss 0.005614, current_train_items 259552.
I0304 19:31:59.340318 23128000471168 run.py:483] Algo bellman_ford step 8111 current loss 0.026884, current_train_items 259584.
I0304 19:31:59.364405 23128000471168 run.py:483] Algo bellman_ford step 8112 current loss 0.045222, current_train_items 259616.
I0304 19:31:59.395362 23128000471168 run.py:483] Algo bellman_ford step 8113 current loss 0.039286, current_train_items 259648.
I0304 19:31:59.431275 23128000471168 run.py:483] Algo bellman_ford step 8114 current loss 0.064818, current_train_items 259680.
I0304 19:31:59.451551 23128000471168 run.py:483] Algo bellman_ford step 8115 current loss 0.002689, current_train_items 259712.
I0304 19:31:59.468147 23128000471168 run.py:483] Algo bellman_ford step 8116 current loss 0.006802, current_train_items 259744.
I0304 19:31:59.493380 23128000471168 run.py:483] Algo bellman_ford step 8117 current loss 0.028131, current_train_items 259776.
I0304 19:31:59.526594 23128000471168 run.py:483] Algo bellman_ford step 8118 current loss 0.039199, current_train_items 259808.
I0304 19:31:59.560292 23128000471168 run.py:483] Algo bellman_ford step 8119 current loss 0.071783, current_train_items 259840.
I0304 19:31:59.580437 23128000471168 run.py:483] Algo bellman_ford step 8120 current loss 0.004045, current_train_items 259872.
I0304 19:31:59.596999 23128000471168 run.py:483] Algo bellman_ford step 8121 current loss 0.004689, current_train_items 259904.
I0304 19:31:59.621759 23128000471168 run.py:483] Algo bellman_ford step 8122 current loss 0.039668, current_train_items 259936.
I0304 19:31:59.655338 23128000471168 run.py:483] Algo bellman_ford step 8123 current loss 0.046140, current_train_items 259968.
I0304 19:31:59.687557 23128000471168 run.py:483] Algo bellman_ford step 8124 current loss 0.034410, current_train_items 260000.
I0304 19:31:59.707798 23128000471168 run.py:483] Algo bellman_ford step 8125 current loss 0.001369, current_train_items 260032.
I0304 19:31:59.724513 23128000471168 run.py:483] Algo bellman_ford step 8126 current loss 0.015210, current_train_items 260064.
I0304 19:31:59.749860 23128000471168 run.py:483] Algo bellman_ford step 8127 current loss 0.025668, current_train_items 260096.
I0304 19:31:59.782891 23128000471168 run.py:483] Algo bellman_ford step 8128 current loss 0.038133, current_train_items 260128.
I0304 19:31:59.819533 23128000471168 run.py:483] Algo bellman_ford step 8129 current loss 0.085812, current_train_items 260160.
I0304 19:31:59.839724 23128000471168 run.py:483] Algo bellman_ford step 8130 current loss 0.005431, current_train_items 260192.
I0304 19:31:59.855872 23128000471168 run.py:483] Algo bellman_ford step 8131 current loss 0.018320, current_train_items 260224.
I0304 19:31:59.880557 23128000471168 run.py:483] Algo bellman_ford step 8132 current loss 0.028524, current_train_items 260256.
I0304 19:31:59.912682 23128000471168 run.py:483] Algo bellman_ford step 8133 current loss 0.037459, current_train_items 260288.
I0304 19:31:59.947115 23128000471168 run.py:483] Algo bellman_ford step 8134 current loss 0.046757, current_train_items 260320.
I0304 19:31:59.967299 23128000471168 run.py:483] Algo bellman_ford step 8135 current loss 0.001539, current_train_items 260352.
I0304 19:31:59.983398 23128000471168 run.py:483] Algo bellman_ford step 8136 current loss 0.014062, current_train_items 260384.
I0304 19:32:00.007518 23128000471168 run.py:483] Algo bellman_ford step 8137 current loss 0.051279, current_train_items 260416.
I0304 19:32:00.040194 23128000471168 run.py:483] Algo bellman_ford step 8138 current loss 0.062184, current_train_items 260448.
I0304 19:32:00.075868 23128000471168 run.py:483] Algo bellman_ford step 8139 current loss 0.086695, current_train_items 260480.
I0304 19:32:00.095631 23128000471168 run.py:483] Algo bellman_ford step 8140 current loss 0.001341, current_train_items 260512.
I0304 19:32:00.111862 23128000471168 run.py:483] Algo bellman_ford step 8141 current loss 0.035529, current_train_items 260544.
I0304 19:32:00.136779 23128000471168 run.py:483] Algo bellman_ford step 8142 current loss 0.059991, current_train_items 260576.
I0304 19:32:00.169656 23128000471168 run.py:483] Algo bellman_ford step 8143 current loss 0.072210, current_train_items 260608.
I0304 19:32:00.204674 23128000471168 run.py:483] Algo bellman_ford step 8144 current loss 0.064243, current_train_items 260640.
I0304 19:32:00.224735 23128000471168 run.py:483] Algo bellman_ford step 8145 current loss 0.002709, current_train_items 260672.
I0304 19:32:00.241555 23128000471168 run.py:483] Algo bellman_ford step 8146 current loss 0.013789, current_train_items 260704.
I0304 19:32:00.266176 23128000471168 run.py:483] Algo bellman_ford step 8147 current loss 0.117991, current_train_items 260736.
I0304 19:32:00.298887 23128000471168 run.py:483] Algo bellman_ford step 8148 current loss 0.123509, current_train_items 260768.
I0304 19:32:00.332278 23128000471168 run.py:483] Algo bellman_ford step 8149 current loss 0.074551, current_train_items 260800.
I0304 19:32:00.352391 23128000471168 run.py:483] Algo bellman_ford step 8150 current loss 0.002737, current_train_items 260832.
I0304 19:32:00.360374 23128000471168 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0304 19:32:00.360482 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:00.377438 23128000471168 run.py:483] Algo bellman_ford step 8151 current loss 0.010289, current_train_items 260864.
I0304 19:32:00.402825 23128000471168 run.py:483] Algo bellman_ford step 8152 current loss 0.036474, current_train_items 260896.
I0304 19:32:00.434268 23128000471168 run.py:483] Algo bellman_ford step 8153 current loss 0.012322, current_train_items 260928.
I0304 19:32:00.469561 23128000471168 run.py:483] Algo bellman_ford step 8154 current loss 0.083473, current_train_items 260960.
I0304 19:32:00.489939 23128000471168 run.py:483] Algo bellman_ford step 8155 current loss 0.002486, current_train_items 260992.
I0304 19:32:00.505764 23128000471168 run.py:483] Algo bellman_ford step 8156 current loss 0.053197, current_train_items 261024.
I0304 19:32:00.530806 23128000471168 run.py:483] Algo bellman_ford step 8157 current loss 0.014879, current_train_items 261056.
I0304 19:32:00.562724 23128000471168 run.py:483] Algo bellman_ford step 8158 current loss 0.051968, current_train_items 261088.
I0304 19:32:00.596041 23128000471168 run.py:483] Algo bellman_ford step 8159 current loss 0.092939, current_train_items 261120.
I0304 19:32:00.615951 23128000471168 run.py:483] Algo bellman_ford step 8160 current loss 0.001716, current_train_items 261152.
I0304 19:32:00.632644 23128000471168 run.py:483] Algo bellman_ford step 8161 current loss 0.008745, current_train_items 261184.
I0304 19:32:00.656785 23128000471168 run.py:483] Algo bellman_ford step 8162 current loss 0.059009, current_train_items 261216.
I0304 19:32:00.689461 23128000471168 run.py:483] Algo bellman_ford step 8163 current loss 0.040948, current_train_items 261248.
I0304 19:32:00.720181 23128000471168 run.py:483] Algo bellman_ford step 8164 current loss 0.017502, current_train_items 261280.
I0304 19:32:00.740202 23128000471168 run.py:483] Algo bellman_ford step 8165 current loss 0.003014, current_train_items 261312.
I0304 19:32:00.756691 23128000471168 run.py:483] Algo bellman_ford step 8166 current loss 0.030931, current_train_items 261344.
I0304 19:32:00.780811 23128000471168 run.py:483] Algo bellman_ford step 8167 current loss 0.081515, current_train_items 261376.
I0304 19:32:00.813037 23128000471168 run.py:483] Algo bellman_ford step 8168 current loss 0.039540, current_train_items 261408.
I0304 19:32:00.847945 23128000471168 run.py:483] Algo bellman_ford step 8169 current loss 0.062074, current_train_items 261440.
I0304 19:32:00.868095 23128000471168 run.py:483] Algo bellman_ford step 8170 current loss 0.001988, current_train_items 261472.
I0304 19:32:00.884658 23128000471168 run.py:483] Algo bellman_ford step 8171 current loss 0.005080, current_train_items 261504.
I0304 19:32:00.909142 23128000471168 run.py:483] Algo bellman_ford step 8172 current loss 0.058733, current_train_items 261536.
I0304 19:32:00.941859 23128000471168 run.py:483] Algo bellman_ford step 8173 current loss 0.085440, current_train_items 261568.
I0304 19:32:00.974718 23128000471168 run.py:483] Algo bellman_ford step 8174 current loss 0.058922, current_train_items 261600.
I0304 19:32:00.995184 23128000471168 run.py:483] Algo bellman_ford step 8175 current loss 0.005610, current_train_items 261632.
I0304 19:32:01.012130 23128000471168 run.py:483] Algo bellman_ford step 8176 current loss 0.023095, current_train_items 261664.
I0304 19:32:01.035979 23128000471168 run.py:483] Algo bellman_ford step 8177 current loss 0.065889, current_train_items 261696.
I0304 19:32:01.069074 23128000471168 run.py:483] Algo bellman_ford step 8178 current loss 0.053040, current_train_items 261728.
I0304 19:32:01.100744 23128000471168 run.py:483] Algo bellman_ford step 8179 current loss 0.047227, current_train_items 261760.
I0304 19:32:01.120489 23128000471168 run.py:483] Algo bellman_ford step 8180 current loss 0.001785, current_train_items 261792.
I0304 19:32:01.136828 23128000471168 run.py:483] Algo bellman_ford step 8181 current loss 0.013593, current_train_items 261824.
I0304 19:32:01.160941 23128000471168 run.py:483] Algo bellman_ford step 8182 current loss 0.018762, current_train_items 261856.
I0304 19:32:01.193599 23128000471168 run.py:483] Algo bellman_ford step 8183 current loss 0.036542, current_train_items 261888.
I0304 19:32:01.229608 23128000471168 run.py:483] Algo bellman_ford step 8184 current loss 0.100921, current_train_items 261920.
I0304 19:32:01.249757 23128000471168 run.py:483] Algo bellman_ford step 8185 current loss 0.002756, current_train_items 261952.
I0304 19:32:01.266388 23128000471168 run.py:483] Algo bellman_ford step 8186 current loss 0.011190, current_train_items 261984.
I0304 19:32:01.290621 23128000471168 run.py:483] Algo bellman_ford step 8187 current loss 0.055461, current_train_items 262016.
I0304 19:32:01.322782 23128000471168 run.py:483] Algo bellman_ford step 8188 current loss 0.064458, current_train_items 262048.
I0304 19:32:01.356350 23128000471168 run.py:483] Algo bellman_ford step 8189 current loss 0.058455, current_train_items 262080.
I0304 19:32:01.376623 23128000471168 run.py:483] Algo bellman_ford step 8190 current loss 0.012541, current_train_items 262112.
I0304 19:32:01.393686 23128000471168 run.py:483] Algo bellman_ford step 8191 current loss 0.020743, current_train_items 262144.
I0304 19:32:01.417703 23128000471168 run.py:483] Algo bellman_ford step 8192 current loss 0.033958, current_train_items 262176.
I0304 19:32:01.449930 23128000471168 run.py:483] Algo bellman_ford step 8193 current loss 0.042364, current_train_items 262208.
I0304 19:32:01.483993 23128000471168 run.py:483] Algo bellman_ford step 8194 current loss 0.065761, current_train_items 262240.
I0304 19:32:01.503448 23128000471168 run.py:483] Algo bellman_ford step 8195 current loss 0.002807, current_train_items 262272.
I0304 19:32:01.519799 23128000471168 run.py:483] Algo bellman_ford step 8196 current loss 0.005023, current_train_items 262304.
I0304 19:32:01.543305 23128000471168 run.py:483] Algo bellman_ford step 8197 current loss 0.054309, current_train_items 262336.
I0304 19:32:01.575643 23128000471168 run.py:483] Algo bellman_ford step 8198 current loss 0.041726, current_train_items 262368.
I0304 19:32:01.609546 23128000471168 run.py:483] Algo bellman_ford step 8199 current loss 0.092989, current_train_items 262400.
I0304 19:32:01.630164 23128000471168 run.py:483] Algo bellman_ford step 8200 current loss 0.002285, current_train_items 262432.
I0304 19:32:01.637825 23128000471168 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0304 19:32:01.637934 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:01.654646 23128000471168 run.py:483] Algo bellman_ford step 8201 current loss 0.004237, current_train_items 262464.
I0304 19:32:01.679641 23128000471168 run.py:483] Algo bellman_ford step 8202 current loss 0.025289, current_train_items 262496.
I0304 19:32:01.712153 23128000471168 run.py:483] Algo bellman_ford step 8203 current loss 0.032596, current_train_items 262528.
I0304 19:32:01.746995 23128000471168 run.py:483] Algo bellman_ford step 8204 current loss 0.070487, current_train_items 262560.
I0304 19:32:01.766853 23128000471168 run.py:483] Algo bellman_ford step 8205 current loss 0.001346, current_train_items 262592.
I0304 19:32:01.783403 23128000471168 run.py:483] Algo bellman_ford step 8206 current loss 0.005402, current_train_items 262624.
I0304 19:32:01.808420 23128000471168 run.py:483] Algo bellman_ford step 8207 current loss 0.050297, current_train_items 262656.
I0304 19:32:01.840492 23128000471168 run.py:483] Algo bellman_ford step 8208 current loss 0.035832, current_train_items 262688.
I0304 19:32:01.871653 23128000471168 run.py:483] Algo bellman_ford step 8209 current loss 0.024298, current_train_items 262720.
I0304 19:32:01.891446 23128000471168 run.py:483] Algo bellman_ford step 8210 current loss 0.001465, current_train_items 262752.
I0304 19:32:01.907659 23128000471168 run.py:483] Algo bellman_ford step 8211 current loss 0.052842, current_train_items 262784.
I0304 19:32:01.931568 23128000471168 run.py:483] Algo bellman_ford step 8212 current loss 0.039366, current_train_items 262816.
I0304 19:32:01.963557 23128000471168 run.py:483] Algo bellman_ford step 8213 current loss 0.063929, current_train_items 262848.
I0304 19:32:01.997025 23128000471168 run.py:483] Algo bellman_ford step 8214 current loss 0.081026, current_train_items 262880.
I0304 19:32:02.017150 23128000471168 run.py:483] Algo bellman_ford step 8215 current loss 0.001723, current_train_items 262912.
I0304 19:32:02.032993 23128000471168 run.py:483] Algo bellman_ford step 8216 current loss 0.002473, current_train_items 262944.
I0304 19:32:02.056181 23128000471168 run.py:483] Algo bellman_ford step 8217 current loss 0.039108, current_train_items 262976.
I0304 19:32:02.089306 23128000471168 run.py:483] Algo bellman_ford step 8218 current loss 0.053794, current_train_items 263008.
I0304 19:32:02.123860 23128000471168 run.py:483] Algo bellman_ford step 8219 current loss 0.050072, current_train_items 263040.
I0304 19:32:02.143397 23128000471168 run.py:483] Algo bellman_ford step 8220 current loss 0.001123, current_train_items 263072.
I0304 19:32:02.159582 23128000471168 run.py:483] Algo bellman_ford step 8221 current loss 0.032353, current_train_items 263104.
I0304 19:32:02.184564 23128000471168 run.py:483] Algo bellman_ford step 8222 current loss 0.039373, current_train_items 263136.
I0304 19:32:02.217104 23128000471168 run.py:483] Algo bellman_ford step 8223 current loss 0.064613, current_train_items 263168.
I0304 19:32:02.248447 23128000471168 run.py:483] Algo bellman_ford step 8224 current loss 0.046953, current_train_items 263200.
I0304 19:32:02.268502 23128000471168 run.py:483] Algo bellman_ford step 8225 current loss 0.002883, current_train_items 263232.
I0304 19:32:02.285443 23128000471168 run.py:483] Algo bellman_ford step 8226 current loss 0.013619, current_train_items 263264.
I0304 19:32:02.310298 23128000471168 run.py:483] Algo bellman_ford step 8227 current loss 0.018471, current_train_items 263296.
I0304 19:32:02.341241 23128000471168 run.py:483] Algo bellman_ford step 8228 current loss 0.042607, current_train_items 263328.
I0304 19:32:02.375033 23128000471168 run.py:483] Algo bellman_ford step 8229 current loss 0.030869, current_train_items 263360.
I0304 19:32:02.394924 23128000471168 run.py:483] Algo bellman_ford step 8230 current loss 0.001463, current_train_items 263392.
I0304 19:32:02.411272 23128000471168 run.py:483] Algo bellman_ford step 8231 current loss 0.020459, current_train_items 263424.
I0304 19:32:02.436322 23128000471168 run.py:483] Algo bellman_ford step 8232 current loss 0.029141, current_train_items 263456.
I0304 19:32:02.468106 23128000471168 run.py:483] Algo bellman_ford step 8233 current loss 0.036571, current_train_items 263488.
I0304 19:32:02.501215 23128000471168 run.py:483] Algo bellman_ford step 8234 current loss 0.050842, current_train_items 263520.
I0304 19:32:02.520822 23128000471168 run.py:483] Algo bellman_ford step 8235 current loss 0.002387, current_train_items 263552.
I0304 19:32:02.537856 23128000471168 run.py:483] Algo bellman_ford step 8236 current loss 0.009982, current_train_items 263584.
I0304 19:32:02.563129 23128000471168 run.py:483] Algo bellman_ford step 8237 current loss 0.026799, current_train_items 263616.
I0304 19:32:02.595655 23128000471168 run.py:483] Algo bellman_ford step 8238 current loss 0.027751, current_train_items 263648.
I0304 19:32:02.629194 23128000471168 run.py:483] Algo bellman_ford step 8239 current loss 0.038824, current_train_items 263680.
I0304 19:32:02.648992 23128000471168 run.py:483] Algo bellman_ford step 8240 current loss 0.001424, current_train_items 263712.
I0304 19:32:02.665327 23128000471168 run.py:483] Algo bellman_ford step 8241 current loss 0.014186, current_train_items 263744.
I0304 19:32:02.689011 23128000471168 run.py:483] Algo bellman_ford step 8242 current loss 0.043909, current_train_items 263776.
I0304 19:32:02.720923 23128000471168 run.py:483] Algo bellman_ford step 8243 current loss 0.050351, current_train_items 263808.
I0304 19:32:02.756595 23128000471168 run.py:483] Algo bellman_ford step 8244 current loss 0.053415, current_train_items 263840.
I0304 19:32:02.776765 23128000471168 run.py:483] Algo bellman_ford step 8245 current loss 0.023463, current_train_items 263872.
I0304 19:32:02.793081 23128000471168 run.py:483] Algo bellman_ford step 8246 current loss 0.007000, current_train_items 263904.
I0304 19:32:02.817049 23128000471168 run.py:483] Algo bellman_ford step 8247 current loss 0.018931, current_train_items 263936.
I0304 19:32:02.848663 23128000471168 run.py:483] Algo bellman_ford step 8248 current loss 0.028634, current_train_items 263968.
I0304 19:32:02.882885 23128000471168 run.py:483] Algo bellman_ford step 8249 current loss 0.049650, current_train_items 264000.
I0304 19:32:02.902925 23128000471168 run.py:483] Algo bellman_ford step 8250 current loss 0.001269, current_train_items 264032.
I0304 19:32:02.910842 23128000471168 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0304 19:32:02.910950 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:02.927978 23128000471168 run.py:483] Algo bellman_ford step 8251 current loss 0.015086, current_train_items 264064.
I0304 19:32:02.953305 23128000471168 run.py:483] Algo bellman_ford step 8252 current loss 0.044092, current_train_items 264096.
I0304 19:32:02.987519 23128000471168 run.py:483] Algo bellman_ford step 8253 current loss 0.031877, current_train_items 264128.
I0304 19:32:03.020819 23128000471168 run.py:483] Algo bellman_ford step 8254 current loss 0.028379, current_train_items 264160.
I0304 19:32:03.041383 23128000471168 run.py:483] Algo bellman_ford step 8255 current loss 0.028359, current_train_items 264192.
I0304 19:32:03.058052 23128000471168 run.py:483] Algo bellman_ford step 8256 current loss 0.019816, current_train_items 264224.
I0304 19:32:03.083034 23128000471168 run.py:483] Algo bellman_ford step 8257 current loss 0.024461, current_train_items 264256.
I0304 19:32:03.116081 23128000471168 run.py:483] Algo bellman_ford step 8258 current loss 0.074315, current_train_items 264288.
I0304 19:32:03.151555 23128000471168 run.py:483] Algo bellman_ford step 8259 current loss 0.078112, current_train_items 264320.
I0304 19:32:03.171830 23128000471168 run.py:483] Algo bellman_ford step 8260 current loss 0.003841, current_train_items 264352.
I0304 19:32:03.188561 23128000471168 run.py:483] Algo bellman_ford step 8261 current loss 0.013094, current_train_items 264384.
I0304 19:32:03.213632 23128000471168 run.py:483] Algo bellman_ford step 8262 current loss 0.041979, current_train_items 264416.
I0304 19:32:03.246707 23128000471168 run.py:483] Algo bellman_ford step 8263 current loss 0.029010, current_train_items 264448.
I0304 19:32:03.282359 23128000471168 run.py:483] Algo bellman_ford step 8264 current loss 0.052366, current_train_items 264480.
I0304 19:32:03.302552 23128000471168 run.py:483] Algo bellman_ford step 8265 current loss 0.002960, current_train_items 264512.
I0304 19:32:03.318967 23128000471168 run.py:483] Algo bellman_ford step 8266 current loss 0.015144, current_train_items 264544.
I0304 19:32:03.343319 23128000471168 run.py:483] Algo bellman_ford step 8267 current loss 0.030008, current_train_items 264576.
I0304 19:32:03.376551 23128000471168 run.py:483] Algo bellman_ford step 8268 current loss 0.035480, current_train_items 264608.
I0304 19:32:03.409070 23128000471168 run.py:483] Algo bellman_ford step 8269 current loss 0.078189, current_train_items 264640.
I0304 19:32:03.429848 23128000471168 run.py:483] Algo bellman_ford step 8270 current loss 0.002762, current_train_items 264672.
I0304 19:32:03.445902 23128000471168 run.py:483] Algo bellman_ford step 8271 current loss 0.007638, current_train_items 264704.
I0304 19:32:03.470206 23128000471168 run.py:483] Algo bellman_ford step 8272 current loss 0.041690, current_train_items 264736.
I0304 19:32:03.501783 23128000471168 run.py:483] Algo bellman_ford step 8273 current loss 0.025303, current_train_items 264768.
I0304 19:32:03.536711 23128000471168 run.py:483] Algo bellman_ford step 8274 current loss 0.066074, current_train_items 264800.
I0304 19:32:03.557031 23128000471168 run.py:483] Algo bellman_ford step 8275 current loss 0.003259, current_train_items 264832.
I0304 19:32:03.574145 23128000471168 run.py:483] Algo bellman_ford step 8276 current loss 0.035021, current_train_items 264864.
I0304 19:32:03.599076 23128000471168 run.py:483] Algo bellman_ford step 8277 current loss 0.023082, current_train_items 264896.
I0304 19:32:03.631313 23128000471168 run.py:483] Algo bellman_ford step 8278 current loss 0.082151, current_train_items 264928.
I0304 19:32:03.664848 23128000471168 run.py:483] Algo bellman_ford step 8279 current loss 0.055239, current_train_items 264960.
I0304 19:32:03.684658 23128000471168 run.py:483] Algo bellman_ford step 8280 current loss 0.005391, current_train_items 264992.
I0304 19:32:03.701633 23128000471168 run.py:483] Algo bellman_ford step 8281 current loss 0.016582, current_train_items 265024.
I0304 19:32:03.725743 23128000471168 run.py:483] Algo bellman_ford step 8282 current loss 0.022950, current_train_items 265056.
I0304 19:32:03.757570 23128000471168 run.py:483] Algo bellman_ford step 8283 current loss 0.042439, current_train_items 265088.
I0304 19:32:03.790965 23128000471168 run.py:483] Algo bellman_ford step 8284 current loss 0.054832, current_train_items 265120.
I0304 19:32:03.811396 23128000471168 run.py:483] Algo bellman_ford step 8285 current loss 0.004282, current_train_items 265152.
I0304 19:32:03.828151 23128000471168 run.py:483] Algo bellman_ford step 8286 current loss 0.027226, current_train_items 265184.
I0304 19:32:03.852233 23128000471168 run.py:483] Algo bellman_ford step 8287 current loss 0.032875, current_train_items 265216.
I0304 19:32:03.886310 23128000471168 run.py:483] Algo bellman_ford step 8288 current loss 0.047875, current_train_items 265248.
I0304 19:32:03.920632 23128000471168 run.py:483] Algo bellman_ford step 8289 current loss 0.031685, current_train_items 265280.
I0304 19:32:03.941034 23128000471168 run.py:483] Algo bellman_ford step 8290 current loss 0.002525, current_train_items 265312.
I0304 19:32:03.957650 23128000471168 run.py:483] Algo bellman_ford step 8291 current loss 0.011710, current_train_items 265344.
I0304 19:32:03.982866 23128000471168 run.py:483] Algo bellman_ford step 8292 current loss 0.041130, current_train_items 265376.
I0304 19:32:04.014286 23128000471168 run.py:483] Algo bellman_ford step 8293 current loss 0.044789, current_train_items 265408.
I0304 19:32:04.049941 23128000471168 run.py:483] Algo bellman_ford step 8294 current loss 0.053307, current_train_items 265440.
I0304 19:32:04.070315 23128000471168 run.py:483] Algo bellman_ford step 8295 current loss 0.013822, current_train_items 265472.
I0304 19:32:04.086884 23128000471168 run.py:483] Algo bellman_ford step 8296 current loss 0.026464, current_train_items 265504.
I0304 19:32:04.112791 23128000471168 run.py:483] Algo bellman_ford step 8297 current loss 0.075608, current_train_items 265536.
I0304 19:32:04.145137 23128000471168 run.py:483] Algo bellman_ford step 8298 current loss 0.097032, current_train_items 265568.
I0304 19:32:04.178618 23128000471168 run.py:483] Algo bellman_ford step 8299 current loss 0.083601, current_train_items 265600.
I0304 19:32:04.198967 23128000471168 run.py:483] Algo bellman_ford step 8300 current loss 0.013534, current_train_items 265632.
I0304 19:32:04.206596 23128000471168 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0304 19:32:04.206704 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:04.223418 23128000471168 run.py:483] Algo bellman_ford step 8301 current loss 0.013199, current_train_items 265664.
I0304 19:32:04.249876 23128000471168 run.py:483] Algo bellman_ford step 8302 current loss 0.069003, current_train_items 265696.
I0304 19:32:04.283254 23128000471168 run.py:483] Algo bellman_ford step 8303 current loss 0.056998, current_train_items 265728.
I0304 19:32:04.317578 23128000471168 run.py:483] Algo bellman_ford step 8304 current loss 0.070954, current_train_items 265760.
I0304 19:32:04.337975 23128000471168 run.py:483] Algo bellman_ford step 8305 current loss 0.004366, current_train_items 265792.
I0304 19:32:04.353914 23128000471168 run.py:483] Algo bellman_ford step 8306 current loss 0.013946, current_train_items 265824.
I0304 19:32:04.378442 23128000471168 run.py:483] Algo bellman_ford step 8307 current loss 0.060773, current_train_items 265856.
I0304 19:32:04.408558 23128000471168 run.py:483] Algo bellman_ford step 8308 current loss 0.031932, current_train_items 265888.
I0304 19:32:04.445096 23128000471168 run.py:483] Algo bellman_ford step 8309 current loss 0.102559, current_train_items 265920.
I0304 19:32:04.465325 23128000471168 run.py:483] Algo bellman_ford step 8310 current loss 0.003126, current_train_items 265952.
I0304 19:32:04.481597 23128000471168 run.py:483] Algo bellman_ford step 8311 current loss 0.009393, current_train_items 265984.
I0304 19:32:04.506293 23128000471168 run.py:483] Algo bellman_ford step 8312 current loss 0.069477, current_train_items 266016.
I0304 19:32:04.537775 23128000471168 run.py:483] Algo bellman_ford step 8313 current loss 0.042542, current_train_items 266048.
I0304 19:32:04.569881 23128000471168 run.py:483] Algo bellman_ford step 8314 current loss 0.058814, current_train_items 266080.
I0304 19:32:04.590211 23128000471168 run.py:483] Algo bellman_ford step 8315 current loss 0.005186, current_train_items 266112.
I0304 19:32:04.606938 23128000471168 run.py:483] Algo bellman_ford step 8316 current loss 0.029470, current_train_items 266144.
I0304 19:32:04.631404 23128000471168 run.py:483] Algo bellman_ford step 8317 current loss 0.043840, current_train_items 266176.
I0304 19:32:04.662129 23128000471168 run.py:483] Algo bellman_ford step 8318 current loss 0.022771, current_train_items 266208.
I0304 19:32:04.696370 23128000471168 run.py:483] Algo bellman_ford step 8319 current loss 0.073764, current_train_items 266240.
I0304 19:32:04.716484 23128000471168 run.py:483] Algo bellman_ford step 8320 current loss 0.005504, current_train_items 266272.
I0304 19:32:04.733009 23128000471168 run.py:483] Algo bellman_ford step 8321 current loss 0.021572, current_train_items 266304.
I0304 19:32:04.758391 23128000471168 run.py:483] Algo bellman_ford step 8322 current loss 0.035114, current_train_items 266336.
I0304 19:32:04.790383 23128000471168 run.py:483] Algo bellman_ford step 8323 current loss 0.062567, current_train_items 266368.
I0304 19:32:04.826178 23128000471168 run.py:483] Algo bellman_ford step 8324 current loss 0.051185, current_train_items 266400.
I0304 19:32:04.846014 23128000471168 run.py:483] Algo bellman_ford step 8325 current loss 0.003657, current_train_items 266432.
I0304 19:32:04.861999 23128000471168 run.py:483] Algo bellman_ford step 8326 current loss 0.008403, current_train_items 266464.
I0304 19:32:04.886715 23128000471168 run.py:483] Algo bellman_ford step 8327 current loss 0.033586, current_train_items 266496.
I0304 19:32:04.918097 23128000471168 run.py:483] Algo bellman_ford step 8328 current loss 0.132194, current_train_items 266528.
I0304 19:32:04.952895 23128000471168 run.py:483] Algo bellman_ford step 8329 current loss 0.076314, current_train_items 266560.
I0304 19:32:04.973054 23128000471168 run.py:483] Algo bellman_ford step 8330 current loss 0.004935, current_train_items 266592.
I0304 19:32:04.989334 23128000471168 run.py:483] Algo bellman_ford step 8331 current loss 0.035963, current_train_items 266624.
I0304 19:32:05.013594 23128000471168 run.py:483] Algo bellman_ford step 8332 current loss 0.010348, current_train_items 266656.
I0304 19:32:05.046635 23128000471168 run.py:483] Algo bellman_ford step 8333 current loss 0.051626, current_train_items 266688.
I0304 19:32:05.080201 23128000471168 run.py:483] Algo bellman_ford step 8334 current loss 0.057175, current_train_items 266720.
I0304 19:32:05.100276 23128000471168 run.py:483] Algo bellman_ford step 8335 current loss 0.023996, current_train_items 266752.
I0304 19:32:05.116691 23128000471168 run.py:483] Algo bellman_ford step 8336 current loss 0.018073, current_train_items 266784.
I0304 19:32:05.142367 23128000471168 run.py:483] Algo bellman_ford step 8337 current loss 0.020484, current_train_items 266816.
I0304 19:32:05.174640 23128000471168 run.py:483] Algo bellman_ford step 8338 current loss 0.087777, current_train_items 266848.
I0304 19:32:05.207647 23128000471168 run.py:483] Algo bellman_ford step 8339 current loss 0.061583, current_train_items 266880.
I0304 19:32:05.227651 23128000471168 run.py:483] Algo bellman_ford step 8340 current loss 0.002890, current_train_items 266912.
I0304 19:32:05.243901 23128000471168 run.py:483] Algo bellman_ford step 8341 current loss 0.005474, current_train_items 266944.
I0304 19:32:05.268116 23128000471168 run.py:483] Algo bellman_ford step 8342 current loss 0.043108, current_train_items 266976.
I0304 19:32:05.299698 23128000471168 run.py:483] Algo bellman_ford step 8343 current loss 0.067977, current_train_items 267008.
I0304 19:32:05.334904 23128000471168 run.py:483] Algo bellman_ford step 8344 current loss 0.062425, current_train_items 267040.
I0304 19:32:05.354912 23128000471168 run.py:483] Algo bellman_ford step 8345 current loss 0.002456, current_train_items 267072.
I0304 19:32:05.371757 23128000471168 run.py:483] Algo bellman_ford step 8346 current loss 0.004841, current_train_items 267104.
I0304 19:32:05.396703 23128000471168 run.py:483] Algo bellman_ford step 8347 current loss 0.063339, current_train_items 267136.
I0304 19:32:05.429503 23128000471168 run.py:483] Algo bellman_ford step 8348 current loss 0.053683, current_train_items 267168.
I0304 19:32:05.464505 23128000471168 run.py:483] Algo bellman_ford step 8349 current loss 0.058886, current_train_items 267200.
I0304 19:32:05.484888 23128000471168 run.py:483] Algo bellman_ford step 8350 current loss 0.003128, current_train_items 267232.
I0304 19:32:05.492958 23128000471168 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0304 19:32:05.493103 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:05.510318 23128000471168 run.py:483] Algo bellman_ford step 8351 current loss 0.013405, current_train_items 267264.
I0304 19:32:05.535768 23128000471168 run.py:483] Algo bellman_ford step 8352 current loss 0.031289, current_train_items 267296.
I0304 19:32:05.568282 23128000471168 run.py:483] Algo bellman_ford step 8353 current loss 0.023090, current_train_items 267328.
I0304 19:32:05.604039 23128000471168 run.py:483] Algo bellman_ford step 8354 current loss 0.081501, current_train_items 267360.
I0304 19:32:05.624443 23128000471168 run.py:483] Algo bellman_ford step 8355 current loss 0.002870, current_train_items 267392.
I0304 19:32:05.640389 23128000471168 run.py:483] Algo bellman_ford step 8356 current loss 0.033170, current_train_items 267424.
I0304 19:32:05.664493 23128000471168 run.py:483] Algo bellman_ford step 8357 current loss 0.031191, current_train_items 267456.
I0304 19:32:05.696838 23128000471168 run.py:483] Algo bellman_ford step 8358 current loss 0.041256, current_train_items 267488.
I0304 19:32:05.729881 23128000471168 run.py:483] Algo bellman_ford step 8359 current loss 0.059151, current_train_items 267520.
I0304 19:32:05.750195 23128000471168 run.py:483] Algo bellman_ford step 8360 current loss 0.002301, current_train_items 267552.
I0304 19:32:05.766508 23128000471168 run.py:483] Algo bellman_ford step 8361 current loss 0.004900, current_train_items 267584.
I0304 19:32:05.791240 23128000471168 run.py:483] Algo bellman_ford step 8362 current loss 0.025990, current_train_items 267616.
I0304 19:32:05.822659 23128000471168 run.py:483] Algo bellman_ford step 8363 current loss 0.055042, current_train_items 267648.
I0304 19:32:05.857180 23128000471168 run.py:483] Algo bellman_ford step 8364 current loss 0.129802, current_train_items 267680.
I0304 19:32:05.877105 23128000471168 run.py:483] Algo bellman_ford step 8365 current loss 0.003280, current_train_items 267712.
I0304 19:32:05.893863 23128000471168 run.py:483] Algo bellman_ford step 8366 current loss 0.005631, current_train_items 267744.
I0304 19:32:05.917446 23128000471168 run.py:483] Algo bellman_ford step 8367 current loss 0.021257, current_train_items 267776.
I0304 19:32:05.950339 23128000471168 run.py:483] Algo bellman_ford step 8368 current loss 0.049030, current_train_items 267808.
I0304 19:32:05.986452 23128000471168 run.py:483] Algo bellman_ford step 8369 current loss 0.052155, current_train_items 267840.
I0304 19:32:06.006784 23128000471168 run.py:483] Algo bellman_ford step 8370 current loss 0.001912, current_train_items 267872.
I0304 19:32:06.023620 23128000471168 run.py:483] Algo bellman_ford step 8371 current loss 0.023458, current_train_items 267904.
I0304 19:32:06.047417 23128000471168 run.py:483] Algo bellman_ford step 8372 current loss 0.081258, current_train_items 267936.
I0304 19:32:06.079452 23128000471168 run.py:483] Algo bellman_ford step 8373 current loss 0.028499, current_train_items 267968.
I0304 19:32:06.113895 23128000471168 run.py:483] Algo bellman_ford step 8374 current loss 0.030599, current_train_items 268000.
I0304 19:32:06.134792 23128000471168 run.py:483] Algo bellman_ford step 8375 current loss 0.001365, current_train_items 268032.
I0304 19:32:06.151292 23128000471168 run.py:483] Algo bellman_ford step 8376 current loss 0.004542, current_train_items 268064.
I0304 19:32:06.174684 23128000471168 run.py:483] Algo bellman_ford step 8377 current loss 0.041236, current_train_items 268096.
I0304 19:32:06.206837 23128000471168 run.py:483] Algo bellman_ford step 8378 current loss 0.054744, current_train_items 268128.
I0304 19:32:06.240988 23128000471168 run.py:483] Algo bellman_ford step 8379 current loss 0.064901, current_train_items 268160.
I0304 19:32:06.261531 23128000471168 run.py:483] Algo bellman_ford step 8380 current loss 0.020805, current_train_items 268192.
I0304 19:32:06.278059 23128000471168 run.py:483] Algo bellman_ford step 8381 current loss 0.007948, current_train_items 268224.
I0304 19:32:06.301510 23128000471168 run.py:483] Algo bellman_ford step 8382 current loss 0.052410, current_train_items 268256.
I0304 19:32:06.334486 23128000471168 run.py:483] Algo bellman_ford step 8383 current loss 0.043934, current_train_items 268288.
I0304 19:32:06.367322 23128000471168 run.py:483] Algo bellman_ford step 8384 current loss 0.034310, current_train_items 268320.
I0304 19:32:06.387488 23128000471168 run.py:483] Algo bellman_ford step 8385 current loss 0.003079, current_train_items 268352.
I0304 19:32:06.403664 23128000471168 run.py:483] Algo bellman_ford step 8386 current loss 0.007421, current_train_items 268384.
I0304 19:32:06.427229 23128000471168 run.py:483] Algo bellman_ford step 8387 current loss 0.040279, current_train_items 268416.
I0304 19:32:06.459535 23128000471168 run.py:483] Algo bellman_ford step 8388 current loss 0.036370, current_train_items 268448.
I0304 19:32:06.493483 23128000471168 run.py:483] Algo bellman_ford step 8389 current loss 0.045760, current_train_items 268480.
I0304 19:32:06.513816 23128000471168 run.py:483] Algo bellman_ford step 8390 current loss 0.002131, current_train_items 268512.
I0304 19:32:06.530480 23128000471168 run.py:483] Algo bellman_ford step 8391 current loss 0.003514, current_train_items 268544.
I0304 19:32:06.553827 23128000471168 run.py:483] Algo bellman_ford step 8392 current loss 0.027932, current_train_items 268576.
I0304 19:32:06.587590 23128000471168 run.py:483] Algo bellman_ford step 8393 current loss 0.062671, current_train_items 268608.
I0304 19:32:06.620320 23128000471168 run.py:483] Algo bellman_ford step 8394 current loss 0.038542, current_train_items 268640.
I0304 19:32:06.640191 23128000471168 run.py:483] Algo bellman_ford step 8395 current loss 0.002673, current_train_items 268672.
I0304 19:32:06.657050 23128000471168 run.py:483] Algo bellman_ford step 8396 current loss 0.012145, current_train_items 268704.
I0304 19:32:06.681042 23128000471168 run.py:483] Algo bellman_ford step 8397 current loss 0.036382, current_train_items 268736.
I0304 19:32:06.714312 23128000471168 run.py:483] Algo bellman_ford step 8398 current loss 0.095661, current_train_items 268768.
I0304 19:32:06.748925 23128000471168 run.py:483] Algo bellman_ford step 8399 current loss 0.097460, current_train_items 268800.
I0304 19:32:06.768959 23128000471168 run.py:483] Algo bellman_ford step 8400 current loss 0.002035, current_train_items 268832.
I0304 19:32:06.776648 23128000471168 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0304 19:32:06.776758 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:06.794059 23128000471168 run.py:483] Algo bellman_ford step 8401 current loss 0.028791, current_train_items 268864.
I0304 19:32:06.819940 23128000471168 run.py:483] Algo bellman_ford step 8402 current loss 0.037663, current_train_items 268896.
I0304 19:32:06.853423 23128000471168 run.py:483] Algo bellman_ford step 8403 current loss 0.035259, current_train_items 268928.
I0304 19:32:06.890255 23128000471168 run.py:483] Algo bellman_ford step 8404 current loss 0.104518, current_train_items 268960.
I0304 19:32:06.910614 23128000471168 run.py:483] Algo bellman_ford step 8405 current loss 0.002822, current_train_items 268992.
I0304 19:32:06.926990 23128000471168 run.py:483] Algo bellman_ford step 8406 current loss 0.024349, current_train_items 269024.
I0304 19:32:06.950850 23128000471168 run.py:483] Algo bellman_ford step 8407 current loss 0.049640, current_train_items 269056.
I0304 19:32:06.982448 23128000471168 run.py:483] Algo bellman_ford step 8408 current loss 0.064604, current_train_items 269088.
I0304 19:32:07.017807 23128000471168 run.py:483] Algo bellman_ford step 8409 current loss 0.080893, current_train_items 269120.
I0304 19:32:07.038055 23128000471168 run.py:483] Algo bellman_ford step 8410 current loss 0.002925, current_train_items 269152.
I0304 19:32:07.054476 23128000471168 run.py:483] Algo bellman_ford step 8411 current loss 0.009373, current_train_items 269184.
I0304 19:32:07.079564 23128000471168 run.py:483] Algo bellman_ford step 8412 current loss 0.055515, current_train_items 269216.
I0304 19:32:07.113308 23128000471168 run.py:483] Algo bellman_ford step 8413 current loss 0.094225, current_train_items 269248.
I0304 19:32:07.148083 23128000471168 run.py:483] Algo bellman_ford step 8414 current loss 0.091235, current_train_items 269280.
I0304 19:32:07.168341 23128000471168 run.py:483] Algo bellman_ford step 8415 current loss 0.003165, current_train_items 269312.
I0304 19:32:07.184964 23128000471168 run.py:483] Algo bellman_ford step 8416 current loss 0.008263, current_train_items 269344.
I0304 19:32:07.209351 23128000471168 run.py:483] Algo bellman_ford step 8417 current loss 0.085381, current_train_items 269376.
I0304 19:32:07.242530 23128000471168 run.py:483] Algo bellman_ford step 8418 current loss 0.024638, current_train_items 269408.
I0304 19:32:07.279773 23128000471168 run.py:483] Algo bellman_ford step 8419 current loss 0.105050, current_train_items 269440.
I0304 19:32:07.300009 23128000471168 run.py:483] Algo bellman_ford step 8420 current loss 0.004246, current_train_items 269472.
I0304 19:32:07.316434 23128000471168 run.py:483] Algo bellman_ford step 8421 current loss 0.025778, current_train_items 269504.
I0304 19:32:07.341946 23128000471168 run.py:483] Algo bellman_ford step 8422 current loss 0.047113, current_train_items 269536.
I0304 19:32:07.374950 23128000471168 run.py:483] Algo bellman_ford step 8423 current loss 0.026117, current_train_items 269568.
I0304 19:32:07.410301 23128000471168 run.py:483] Algo bellman_ford step 8424 current loss 0.028112, current_train_items 269600.
I0304 19:32:07.430810 23128000471168 run.py:483] Algo bellman_ford step 8425 current loss 0.003256, current_train_items 269632.
I0304 19:32:07.446846 23128000471168 run.py:483] Algo bellman_ford step 8426 current loss 0.049059, current_train_items 269664.
I0304 19:32:07.471673 23128000471168 run.py:483] Algo bellman_ford step 8427 current loss 0.056880, current_train_items 269696.
I0304 19:32:07.503440 23128000471168 run.py:483] Algo bellman_ford step 8428 current loss 0.053415, current_train_items 269728.
I0304 19:32:07.537451 23128000471168 run.py:483] Algo bellman_ford step 8429 current loss 0.048365, current_train_items 269760.
I0304 19:32:07.557861 23128000471168 run.py:483] Algo bellman_ford step 8430 current loss 0.005987, current_train_items 269792.
I0304 19:32:07.574690 23128000471168 run.py:483] Algo bellman_ford step 8431 current loss 0.048193, current_train_items 269824.
I0304 19:32:07.598995 23128000471168 run.py:483] Algo bellman_ford step 8432 current loss 0.040960, current_train_items 269856.
I0304 19:32:07.632800 23128000471168 run.py:483] Algo bellman_ford step 8433 current loss 0.093112, current_train_items 269888.
I0304 19:32:07.665736 23128000471168 run.py:483] Algo bellman_ford step 8434 current loss 0.037252, current_train_items 269920.
I0304 19:32:07.685670 23128000471168 run.py:483] Algo bellman_ford step 8435 current loss 0.002475, current_train_items 269952.
I0304 19:32:07.701832 23128000471168 run.py:483] Algo bellman_ford step 8436 current loss 0.027855, current_train_items 269984.
I0304 19:32:07.726179 23128000471168 run.py:483] Algo bellman_ford step 8437 current loss 0.047971, current_train_items 270016.
I0304 19:32:07.758857 23128000471168 run.py:483] Algo bellman_ford step 8438 current loss 0.086013, current_train_items 270048.
I0304 19:32:07.791223 23128000471168 run.py:483] Algo bellman_ford step 8439 current loss 0.080665, current_train_items 270080.
I0304 19:32:07.811470 23128000471168 run.py:483] Algo bellman_ford step 8440 current loss 0.002934, current_train_items 270112.
I0304 19:32:07.828001 23128000471168 run.py:483] Algo bellman_ford step 8441 current loss 0.015010, current_train_items 270144.
I0304 19:32:07.852939 23128000471168 run.py:483] Algo bellman_ford step 8442 current loss 0.013932, current_train_items 270176.
I0304 19:32:07.885581 23128000471168 run.py:483] Algo bellman_ford step 8443 current loss 0.034483, current_train_items 270208.
I0304 19:32:07.920553 23128000471168 run.py:483] Algo bellman_ford step 8444 current loss 0.053961, current_train_items 270240.
I0304 19:32:07.940612 23128000471168 run.py:483] Algo bellman_ford step 8445 current loss 0.002541, current_train_items 270272.
I0304 19:32:07.956955 23128000471168 run.py:483] Algo bellman_ford step 8446 current loss 0.009450, current_train_items 270304.
I0304 19:32:07.981567 23128000471168 run.py:483] Algo bellman_ford step 8447 current loss 0.040829, current_train_items 270336.
I0304 19:32:08.013652 23128000471168 run.py:483] Algo bellman_ford step 8448 current loss 0.028055, current_train_items 270368.
I0304 19:32:08.048302 23128000471168 run.py:483] Algo bellman_ford step 8449 current loss 0.075557, current_train_items 270400.
I0304 19:32:08.068716 23128000471168 run.py:483] Algo bellman_ford step 8450 current loss 0.001825, current_train_items 270432.
I0304 19:32:08.076901 23128000471168 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0304 19:32:08.077015 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:08.093801 23128000471168 run.py:483] Algo bellman_ford step 8451 current loss 0.012720, current_train_items 270464.
I0304 19:32:08.119271 23128000471168 run.py:483] Algo bellman_ford step 8452 current loss 0.026790, current_train_items 270496.
I0304 19:32:08.153164 23128000471168 run.py:483] Algo bellman_ford step 8453 current loss 0.067072, current_train_items 270528.
I0304 19:32:08.188453 23128000471168 run.py:483] Algo bellman_ford step 8454 current loss 0.066629, current_train_items 270560.
I0304 19:32:08.208862 23128000471168 run.py:483] Algo bellman_ford step 8455 current loss 0.004901, current_train_items 270592.
I0304 19:32:08.225632 23128000471168 run.py:483] Algo bellman_ford step 8456 current loss 0.013669, current_train_items 270624.
I0304 19:32:08.250870 23128000471168 run.py:483] Algo bellman_ford step 8457 current loss 0.067213, current_train_items 270656.
I0304 19:32:08.282210 23128000471168 run.py:483] Algo bellman_ford step 8458 current loss 0.050252, current_train_items 270688.
I0304 19:32:08.314249 23128000471168 run.py:483] Algo bellman_ford step 8459 current loss 0.060216, current_train_items 270720.
I0304 19:32:08.334846 23128000471168 run.py:483] Algo bellman_ford step 8460 current loss 0.002899, current_train_items 270752.
I0304 19:32:08.351208 23128000471168 run.py:483] Algo bellman_ford step 8461 current loss 0.003922, current_train_items 270784.
I0304 19:32:08.374639 23128000471168 run.py:483] Algo bellman_ford step 8462 current loss 0.027354, current_train_items 270816.
I0304 19:32:08.405882 23128000471168 run.py:483] Algo bellman_ford step 8463 current loss 0.038928, current_train_items 270848.
I0304 19:32:08.440878 23128000471168 run.py:483] Algo bellman_ford step 8464 current loss 0.058736, current_train_items 270880.
I0304 19:32:08.460744 23128000471168 run.py:483] Algo bellman_ford step 8465 current loss 0.002087, current_train_items 270912.
I0304 19:32:08.477608 23128000471168 run.py:483] Algo bellman_ford step 8466 current loss 0.023778, current_train_items 270944.
I0304 19:32:08.501592 23128000471168 run.py:483] Algo bellman_ford step 8467 current loss 0.024701, current_train_items 270976.
I0304 19:32:08.533380 23128000471168 run.py:483] Algo bellman_ford step 8468 current loss 0.030598, current_train_items 271008.
I0304 19:32:08.566782 23128000471168 run.py:483] Algo bellman_ford step 8469 current loss 0.031157, current_train_items 271040.
I0304 19:32:08.587463 23128000471168 run.py:483] Algo bellman_ford step 8470 current loss 0.002504, current_train_items 271072.
I0304 19:32:08.604408 23128000471168 run.py:483] Algo bellman_ford step 8471 current loss 0.036861, current_train_items 271104.
I0304 19:32:08.628137 23128000471168 run.py:483] Algo bellman_ford step 8472 current loss 0.012123, current_train_items 271136.
I0304 19:32:08.658324 23128000471168 run.py:483] Algo bellman_ford step 8473 current loss 0.021820, current_train_items 271168.
I0304 19:32:08.693069 23128000471168 run.py:483] Algo bellman_ford step 8474 current loss 0.075089, current_train_items 271200.
I0304 19:32:08.713751 23128000471168 run.py:483] Algo bellman_ford step 8475 current loss 0.002251, current_train_items 271232.
I0304 19:32:08.730517 23128000471168 run.py:483] Algo bellman_ford step 8476 current loss 0.020712, current_train_items 271264.
I0304 19:32:08.755871 23128000471168 run.py:483] Algo bellman_ford step 8477 current loss 0.056604, current_train_items 271296.
I0304 19:32:08.788400 23128000471168 run.py:483] Algo bellman_ford step 8478 current loss 0.039811, current_train_items 271328.
I0304 19:32:08.823848 23128000471168 run.py:483] Algo bellman_ford step 8479 current loss 0.048744, current_train_items 271360.
I0304 19:32:08.843551 23128000471168 run.py:483] Algo bellman_ford step 8480 current loss 0.003540, current_train_items 271392.
I0304 19:32:08.860149 23128000471168 run.py:483] Algo bellman_ford step 8481 current loss 0.053521, current_train_items 271424.
I0304 19:32:08.884750 23128000471168 run.py:483] Algo bellman_ford step 8482 current loss 0.067943, current_train_items 271456.
I0304 19:32:08.917318 23128000471168 run.py:483] Algo bellman_ford step 8483 current loss 0.050313, current_train_items 271488.
I0304 19:32:08.950539 23128000471168 run.py:483] Algo bellman_ford step 8484 current loss 0.050481, current_train_items 271520.
I0304 19:32:08.971124 23128000471168 run.py:483] Algo bellman_ford step 8485 current loss 0.004092, current_train_items 271552.
I0304 19:32:08.987522 23128000471168 run.py:483] Algo bellman_ford step 8486 current loss 0.008676, current_train_items 271584.
I0304 19:32:09.011612 23128000471168 run.py:483] Algo bellman_ford step 8487 current loss 0.039168, current_train_items 271616.
I0304 19:32:09.044714 23128000471168 run.py:483] Algo bellman_ford step 8488 current loss 0.108937, current_train_items 271648.
I0304 19:32:09.079276 23128000471168 run.py:483] Algo bellman_ford step 8489 current loss 0.128668, current_train_items 271680.
I0304 19:32:09.100180 23128000471168 run.py:483] Algo bellman_ford step 8490 current loss 0.027855, current_train_items 271712.
I0304 19:32:09.116591 23128000471168 run.py:483] Algo bellman_ford step 8491 current loss 0.018851, current_train_items 271744.
I0304 19:32:09.141531 23128000471168 run.py:483] Algo bellman_ford step 8492 current loss 0.042412, current_train_items 271776.
I0304 19:32:09.173585 23128000471168 run.py:483] Algo bellman_ford step 8493 current loss 0.040928, current_train_items 271808.
I0304 19:32:09.206461 23128000471168 run.py:483] Algo bellman_ford step 8494 current loss 0.112217, current_train_items 271840.
I0304 19:32:09.227079 23128000471168 run.py:483] Algo bellman_ford step 8495 current loss 0.006978, current_train_items 271872.
I0304 19:32:09.243856 23128000471168 run.py:483] Algo bellman_ford step 8496 current loss 0.052122, current_train_items 271904.
I0304 19:32:09.268455 23128000471168 run.py:483] Algo bellman_ford step 8497 current loss 0.031610, current_train_items 271936.
I0304 19:32:09.301062 23128000471168 run.py:483] Algo bellman_ford step 8498 current loss 0.092635, current_train_items 271968.
I0304 19:32:09.334102 23128000471168 run.py:483] Algo bellman_ford step 8499 current loss 0.051827, current_train_items 272000.
I0304 19:32:09.354584 23128000471168 run.py:483] Algo bellman_ford step 8500 current loss 0.003362, current_train_items 272032.
I0304 19:32:09.362959 23128000471168 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0304 19:32:09.363129 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:09.380663 23128000471168 run.py:483] Algo bellman_ford step 8501 current loss 0.035130, current_train_items 272064.
I0304 19:32:09.406659 23128000471168 run.py:483] Algo bellman_ford step 8502 current loss 0.027492, current_train_items 272096.
I0304 19:32:09.438813 23128000471168 run.py:483] Algo bellman_ford step 8503 current loss 0.033647, current_train_items 272128.
I0304 19:32:09.472731 23128000471168 run.py:483] Algo bellman_ford step 8504 current loss 0.059367, current_train_items 272160.
I0304 19:32:09.493207 23128000471168 run.py:483] Algo bellman_ford step 8505 current loss 0.002186, current_train_items 272192.
I0304 19:32:09.508941 23128000471168 run.py:483] Algo bellman_ford step 8506 current loss 0.006746, current_train_items 272224.
I0304 19:32:09.532488 23128000471168 run.py:483] Algo bellman_ford step 8507 current loss 0.042654, current_train_items 272256.
I0304 19:32:09.565418 23128000471168 run.py:483] Algo bellman_ford step 8508 current loss 0.061374, current_train_items 272288.
I0304 19:32:09.601471 23128000471168 run.py:483] Algo bellman_ford step 8509 current loss 0.062757, current_train_items 272320.
I0304 19:32:09.621840 23128000471168 run.py:483] Algo bellman_ford step 8510 current loss 0.003115, current_train_items 272352.
I0304 19:32:09.637903 23128000471168 run.py:483] Algo bellman_ford step 8511 current loss 0.049027, current_train_items 272384.
I0304 19:32:09.663219 23128000471168 run.py:483] Algo bellman_ford step 8512 current loss 0.024293, current_train_items 272416.
I0304 19:32:09.695277 23128000471168 run.py:483] Algo bellman_ford step 8513 current loss 0.045915, current_train_items 272448.
I0304 19:32:09.730240 23128000471168 run.py:483] Algo bellman_ford step 8514 current loss 0.074246, current_train_items 272480.
I0304 19:32:09.750250 23128000471168 run.py:483] Algo bellman_ford step 8515 current loss 0.001548, current_train_items 272512.
I0304 19:32:09.766600 23128000471168 run.py:483] Algo bellman_ford step 8516 current loss 0.008722, current_train_items 272544.
I0304 19:32:09.791252 23128000471168 run.py:483] Algo bellman_ford step 8517 current loss 0.117481, current_train_items 272576.
I0304 19:32:09.825122 23128000471168 run.py:483] Algo bellman_ford step 8518 current loss 0.054419, current_train_items 272608.
I0304 19:32:09.857991 23128000471168 run.py:483] Algo bellman_ford step 8519 current loss 0.071918, current_train_items 272640.
I0304 19:32:09.878323 23128000471168 run.py:483] Algo bellman_ford step 8520 current loss 0.001947, current_train_items 272672.
I0304 19:32:09.894698 23128000471168 run.py:483] Algo bellman_ford step 8521 current loss 0.005345, current_train_items 272704.
I0304 19:32:09.919260 23128000471168 run.py:483] Algo bellman_ford step 8522 current loss 0.044260, current_train_items 272736.
I0304 19:32:09.951663 23128000471168 run.py:483] Algo bellman_ford step 8523 current loss 0.053484, current_train_items 272768.
I0304 19:32:09.986736 23128000471168 run.py:483] Algo bellman_ford step 8524 current loss 0.084119, current_train_items 272800.
I0304 19:32:10.007435 23128000471168 run.py:483] Algo bellman_ford step 8525 current loss 0.004120, current_train_items 272832.
I0304 19:32:10.024252 23128000471168 run.py:483] Algo bellman_ford step 8526 current loss 0.013556, current_train_items 272864.
I0304 19:32:10.048821 23128000471168 run.py:483] Algo bellman_ford step 8527 current loss 0.056303, current_train_items 272896.
I0304 19:32:10.080641 23128000471168 run.py:483] Algo bellman_ford step 8528 current loss 0.017193, current_train_items 272928.
I0304 19:32:10.115470 23128000471168 run.py:483] Algo bellman_ford step 8529 current loss 0.094462, current_train_items 272960.
I0304 19:32:10.135964 23128000471168 run.py:483] Algo bellman_ford step 8530 current loss 0.002153, current_train_items 272992.
I0304 19:32:10.152901 23128000471168 run.py:483] Algo bellman_ford step 8531 current loss 0.010949, current_train_items 273024.
I0304 19:32:10.176250 23128000471168 run.py:483] Algo bellman_ford step 8532 current loss 0.013257, current_train_items 273056.
I0304 19:32:10.207729 23128000471168 run.py:483] Algo bellman_ford step 8533 current loss 0.032937, current_train_items 273088.
I0304 19:32:10.241099 23128000471168 run.py:483] Algo bellman_ford step 8534 current loss 0.054140, current_train_items 273120.
I0304 19:32:10.261359 23128000471168 run.py:483] Algo bellman_ford step 8535 current loss 0.001301, current_train_items 273152.
I0304 19:32:10.277863 23128000471168 run.py:483] Algo bellman_ford step 8536 current loss 0.017388, current_train_items 273184.
I0304 19:32:10.302332 23128000471168 run.py:483] Algo bellman_ford step 8537 current loss 0.033596, current_train_items 273216.
I0304 19:32:10.334656 23128000471168 run.py:483] Algo bellman_ford step 8538 current loss 0.031364, current_train_items 273248.
I0304 19:32:10.370766 23128000471168 run.py:483] Algo bellman_ford step 8539 current loss 0.062469, current_train_items 273280.
I0304 19:32:10.390906 23128000471168 run.py:483] Algo bellman_ford step 8540 current loss 0.023870, current_train_items 273312.
I0304 19:32:10.407334 23128000471168 run.py:483] Algo bellman_ford step 8541 current loss 0.060560, current_train_items 273344.
I0304 19:32:10.432093 23128000471168 run.py:483] Algo bellman_ford step 8542 current loss 0.016714, current_train_items 273376.
I0304 19:32:10.465307 23128000471168 run.py:483] Algo bellman_ford step 8543 current loss 0.044985, current_train_items 273408.
I0304 19:32:10.499450 23128000471168 run.py:483] Algo bellman_ford step 8544 current loss 0.054062, current_train_items 273440.
I0304 19:32:10.519579 23128000471168 run.py:483] Algo bellman_ford step 8545 current loss 0.001715, current_train_items 273472.
I0304 19:32:10.535655 23128000471168 run.py:483] Algo bellman_ford step 8546 current loss 0.030245, current_train_items 273504.
I0304 19:32:10.560401 23128000471168 run.py:483] Algo bellman_ford step 8547 current loss 0.080075, current_train_items 273536.
I0304 19:32:10.593152 23128000471168 run.py:483] Algo bellman_ford step 8548 current loss 0.050375, current_train_items 273568.
I0304 19:32:10.627695 23128000471168 run.py:483] Algo bellman_ford step 8549 current loss 0.066636, current_train_items 273600.
I0304 19:32:10.647857 23128000471168 run.py:483] Algo bellman_ford step 8550 current loss 0.003033, current_train_items 273632.
I0304 19:32:10.656022 23128000471168 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0304 19:32:10.656131 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:32:10.673573 23128000471168 run.py:483] Algo bellman_ford step 8551 current loss 0.021187, current_train_items 273664.
I0304 19:32:10.699038 23128000471168 run.py:483] Algo bellman_ford step 8552 current loss 0.036870, current_train_items 273696.
I0304 19:32:10.731736 23128000471168 run.py:483] Algo bellman_ford step 8553 current loss 0.059392, current_train_items 273728.
I0304 19:32:10.763942 23128000471168 run.py:483] Algo bellman_ford step 8554 current loss 0.029378, current_train_items 273760.
I0304 19:32:10.784095 23128000471168 run.py:483] Algo bellman_ford step 8555 current loss 0.003904, current_train_items 273792.
I0304 19:32:10.800508 23128000471168 run.py:483] Algo bellman_ford step 8556 current loss 0.020562, current_train_items 273824.
I0304 19:32:10.824614 23128000471168 run.py:483] Algo bellman_ford step 8557 current loss 0.017331, current_train_items 273856.
I0304 19:32:10.857017 23128000471168 run.py:483] Algo bellman_ford step 8558 current loss 0.096125, current_train_items 273888.
I0304 19:32:10.891883 23128000471168 run.py:483] Algo bellman_ford step 8559 current loss 0.085744, current_train_items 273920.
I0304 19:32:10.912083 23128000471168 run.py:483] Algo bellman_ford step 8560 current loss 0.002551, current_train_items 273952.
I0304 19:32:10.928993 23128000471168 run.py:483] Algo bellman_ford step 8561 current loss 0.023220, current_train_items 273984.
I0304 19:32:10.953619 23128000471168 run.py:483] Algo bellman_ford step 8562 current loss 0.051256, current_train_items 274016.
I0304 19:32:10.986701 23128000471168 run.py:483] Algo bellman_ford step 8563 current loss 0.057927, current_train_items 274048.
I0304 19:32:11.019743 23128000471168 run.py:483] Algo bellman_ford step 8564 current loss 0.061715, current_train_items 274080.
I0304 19:32:11.039478 23128000471168 run.py:483] Algo bellman_ford step 8565 current loss 0.001772, current_train_items 274112.
I0304 19:32:11.056577 23128000471168 run.py:483] Algo bellman_ford step 8566 current loss 0.025008, current_train_items 274144.
I0304 19:32:11.082280 23128000471168 run.py:483] Algo bellman_ford step 8567 current loss 0.038322, current_train_items 274176.
I0304 19:32:11.114593 23128000471168 run.py:483] Algo bellman_ford step 8568 current loss 0.053267, current_train_items 274208.
I0304 19:32:11.149350 23128000471168 run.py:483] Algo bellman_ford step 8569 current loss 0.087932, current_train_items 274240.
I0304 19:32:11.169672 23128000471168 run.py:483] Algo bellman_ford step 8570 current loss 0.003015, current_train_items 274272.
I0304 19:32:11.186414 23128000471168 run.py:483] Algo bellman_ford step 8571 current loss 0.038623, current_train_items 274304.
I0304 19:32:11.210107 23128000471168 run.py:483] Algo bellman_ford step 8572 current loss 0.034073, current_train_items 274336.
I0304 19:32:11.242086 23128000471168 run.py:483] Algo bellman_ford step 8573 current loss 0.029143, current_train_items 274368.
I0304 19:32:11.274862 23128000471168 run.py:483] Algo bellman_ford step 8574 current loss 0.051323, current_train_items 274400.
I0304 19:32:11.295259 23128000471168 run.py:483] Algo bellman_ford step 8575 current loss 0.001364, current_train_items 274432.
I0304 19:32:11.311815 23128000471168 run.py:483] Algo bellman_ford step 8576 current loss 0.011738, current_train_items 274464.
I0304 19:32:11.336342 23128000471168 run.py:483] Algo bellman_ford step 8577 current loss 0.018839, current_train_items 274496.
I0304 19:32:11.368952 23128000471168 run.py:483] Algo bellman_ford step 8578 current loss 0.048473, current_train_items 274528.
I0304 19:32:11.403825 23128000471168 run.py:483] Algo bellman_ford step 8579 current loss 0.058340, current_train_items 274560.
I0304 19:32:11.423758 23128000471168 run.py:483] Algo bellman_ford step 8580 current loss 0.003793, current_train_items 274592.
I0304 19:32:11.440234 23128000471168 run.py:483] Algo bellman_ford step 8581 current loss 0.002973, current_train_items 274624.
I0304 19:32:11.463521 23128000471168 run.py:483] Algo bellman_ford step 8582 current loss 0.071527, current_train_items 274656.
I0304 19:32:11.495815 23128000471168 run.py:483] Algo bellman_ford step 8583 current loss 0.067828, current_train_items 274688.
I0304 19:32:11.531027 23128000471168 run.py:483] Algo bellman_ford step 8584 current loss 0.065208, current_train_items 274720.
I0304 19:32:11.550986 23128000471168 run.py:483] Algo bellman_ford step 8585 current loss 0.003075, current_train_items 274752.
I0304 19:32:11.567677 23128000471168 run.py:483] Algo bellman_ford step 8586 current loss 0.006976, current_train_items 274784.
I0304 19:32:11.592925 23128000471168 run.py:483] Algo bellman_ford step 8587 current loss 0.066266, current_train_items 274816.
I0304 19:32:11.624892 23128000471168 run.py:483] Algo bellman_ford step 8588 current loss 0.047918, current_train_items 274848.
I0304 19:32:11.657899 23128000471168 run.py:483] Algo bellman_ford step 8589 current loss 0.045438, current_train_items 274880.
I0304 19:32:11.678294 23128000471168 run.py:483] Algo bellman_ford step 8590 current loss 0.019128, current_train_items 274912.
I0304 19:32:11.694185 23128000471168 run.py:483] Algo bellman_ford step 8591 current loss 0.011695, current_train_items 274944.
I0304 19:32:11.718313 23128000471168 run.py:483] Algo bellman_ford step 8592 current loss 0.035007, current_train_items 274976.
I0304 19:32:11.751109 23128000471168 run.py:483] Algo bellman_ford step 8593 current loss 0.074463, current_train_items 275008.
I0304 19:32:11.785490 23128000471168 run.py:483] Algo bellman_ford step 8594 current loss 0.136148, current_train_items 275040.
I0304 19:32:11.805170 23128000471168 run.py:483] Algo bellman_ford step 8595 current loss 0.002529, current_train_items 275072.
I0304 19:32:11.821625 23128000471168 run.py:483] Algo bellman_ford step 8596 current loss 0.013668, current_train_items 275104.
I0304 19:32:11.846302 23128000471168 run.py:483] Algo bellman_ford step 8597 current loss 0.054399, current_train_items 275136.
I0304 19:32:11.877378 23128000471168 run.py:483] Algo bellman_ford step 8598 current loss 0.031464, current_train_items 275168.
I0304 19:32:11.912273 23128000471168 run.py:483] Algo bellman_ford step 8599 current loss 0.077748, current_train_items 275200.
I0304 19:32:11.932813 23128000471168 run.py:483] Algo bellman_ford step 8600 current loss 0.010419, current_train_items 275232.
I0304 19:32:11.940945 23128000471168 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0304 19:32:11.941061 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:32:11.958420 23128000471168 run.py:483] Algo bellman_ford step 8601 current loss 0.024808, current_train_items 275264.
I0304 19:32:11.984185 23128000471168 run.py:483] Algo bellman_ford step 8602 current loss 0.041548, current_train_items 275296.
I0304 19:32:12.017792 23128000471168 run.py:483] Algo bellman_ford step 8603 current loss 0.046266, current_train_items 275328.
I0304 19:32:12.052880 23128000471168 run.py:483] Algo bellman_ford step 8604 current loss 0.046839, current_train_items 275360.
I0304 19:32:12.073548 23128000471168 run.py:483] Algo bellman_ford step 8605 current loss 0.002637, current_train_items 275392.
I0304 19:32:12.089200 23128000471168 run.py:483] Algo bellman_ford step 8606 current loss 0.009549, current_train_items 275424.
I0304 19:32:12.113509 23128000471168 run.py:483] Algo bellman_ford step 8607 current loss 0.033133, current_train_items 275456.
I0304 19:32:12.146012 23128000471168 run.py:483] Algo bellman_ford step 8608 current loss 0.028066, current_train_items 275488.
I0304 19:32:12.182486 23128000471168 run.py:483] Algo bellman_ford step 8609 current loss 0.062759, current_train_items 275520.
I0304 19:32:12.202893 23128000471168 run.py:483] Algo bellman_ford step 8610 current loss 0.022648, current_train_items 275552.
I0304 19:32:12.219433 23128000471168 run.py:483] Algo bellman_ford step 8611 current loss 0.004802, current_train_items 275584.
I0304 19:32:12.244529 23128000471168 run.py:483] Algo bellman_ford step 8612 current loss 0.049111, current_train_items 275616.
I0304 19:32:12.277371 23128000471168 run.py:483] Algo bellman_ford step 8613 current loss 0.056843, current_train_items 275648.
I0304 19:32:12.310057 23128000471168 run.py:483] Algo bellman_ford step 8614 current loss 0.028439, current_train_items 275680.
I0304 19:32:12.330204 23128000471168 run.py:483] Algo bellman_ford step 8615 current loss 0.001877, current_train_items 275712.
I0304 19:32:12.346663 23128000471168 run.py:483] Algo bellman_ford step 8616 current loss 0.017154, current_train_items 275744.
I0304 19:32:12.371342 23128000471168 run.py:483] Algo bellman_ford step 8617 current loss 0.021259, current_train_items 275776.
I0304 19:32:12.404510 23128000471168 run.py:483] Algo bellman_ford step 8618 current loss 0.030805, current_train_items 275808.
I0304 19:32:12.438844 23128000471168 run.py:483] Algo bellman_ford step 8619 current loss 0.058372, current_train_items 275840.
I0304 19:32:12.459244 23128000471168 run.py:483] Algo bellman_ford step 8620 current loss 0.001232, current_train_items 275872.
I0304 19:32:12.475747 23128000471168 run.py:483] Algo bellman_ford step 8621 current loss 0.021676, current_train_items 275904.
I0304 19:32:12.501152 23128000471168 run.py:483] Algo bellman_ford step 8622 current loss 0.028403, current_train_items 275936.
I0304 19:32:12.534357 23128000471168 run.py:483] Algo bellman_ford step 8623 current loss 0.020701, current_train_items 275968.
I0304 19:32:12.568502 23128000471168 run.py:483] Algo bellman_ford step 8624 current loss 0.023101, current_train_items 276000.
I0304 19:32:12.588675 23128000471168 run.py:483] Algo bellman_ford step 8625 current loss 0.004012, current_train_items 276032.
I0304 19:32:12.605172 23128000471168 run.py:483] Algo bellman_ford step 8626 current loss 0.010480, current_train_items 276064.
I0304 19:32:12.630025 23128000471168 run.py:483] Algo bellman_ford step 8627 current loss 0.026921, current_train_items 276096.
I0304 19:32:12.663845 23128000471168 run.py:483] Algo bellman_ford step 8628 current loss 0.033489, current_train_items 276128.
I0304 19:32:12.697604 23128000471168 run.py:483] Algo bellman_ford step 8629 current loss 0.055993, current_train_items 276160.
I0304 19:32:12.717980 23128000471168 run.py:483] Algo bellman_ford step 8630 current loss 0.001457, current_train_items 276192.
I0304 19:32:12.734535 23128000471168 run.py:483] Algo bellman_ford step 8631 current loss 0.016000, current_train_items 276224.
I0304 19:32:12.759025 23128000471168 run.py:483] Algo bellman_ford step 8632 current loss 0.027275, current_train_items 276256.
I0304 19:32:12.791337 23128000471168 run.py:483] Algo bellman_ford step 8633 current loss 0.019136, current_train_items 276288.
I0304 19:32:12.825325 23128000471168 run.py:483] Algo bellman_ford step 8634 current loss 0.049751, current_train_items 276320.
I0304 19:32:12.845911 23128000471168 run.py:483] Algo bellman_ford step 8635 current loss 0.009285, current_train_items 276352.
I0304 19:32:12.862265 23128000471168 run.py:483] Algo bellman_ford step 8636 current loss 0.006010, current_train_items 276384.
I0304 19:32:12.887821 23128000471168 run.py:483] Algo bellman_ford step 8637 current loss 0.023318, current_train_items 276416.
I0304 19:32:12.919630 23128000471168 run.py:483] Algo bellman_ford step 8638 current loss 0.048214, current_train_items 276448.
I0304 19:32:12.954405 23128000471168 run.py:483] Algo bellman_ford step 8639 current loss 0.040186, current_train_items 276480.
I0304 19:32:12.974675 23128000471168 run.py:483] Algo bellman_ford step 8640 current loss 0.009993, current_train_items 276512.
I0304 19:32:12.991721 23128000471168 run.py:483] Algo bellman_ford step 8641 current loss 0.039992, current_train_items 276544.
I0304 19:32:13.016559 23128000471168 run.py:483] Algo bellman_ford step 8642 current loss 0.042239, current_train_items 276576.
I0304 19:32:13.049355 23128000471168 run.py:483] Algo bellman_ford step 8643 current loss 0.053003, current_train_items 276608.
I0304 19:32:13.084164 23128000471168 run.py:483] Algo bellman_ford step 8644 current loss 0.053781, current_train_items 276640.
I0304 19:32:13.104322 23128000471168 run.py:483] Algo bellman_ford step 8645 current loss 0.001542, current_train_items 276672.
I0304 19:32:13.120817 23128000471168 run.py:483] Algo bellman_ford step 8646 current loss 0.048668, current_train_items 276704.
I0304 19:32:13.145782 23128000471168 run.py:483] Algo bellman_ford step 8647 current loss 0.049741, current_train_items 276736.
I0304 19:32:13.179025 23128000471168 run.py:483] Algo bellman_ford step 8648 current loss 0.078404, current_train_items 276768.
I0304 19:32:13.212855 23128000471168 run.py:483] Algo bellman_ford step 8649 current loss 0.045812, current_train_items 276800.
I0304 19:32:13.232849 23128000471168 run.py:483] Algo bellman_ford step 8650 current loss 0.002343, current_train_items 276832.
I0304 19:32:13.240900 23128000471168 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0304 19:32:13.241018 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:13.258374 23128000471168 run.py:483] Algo bellman_ford step 8651 current loss 0.016878, current_train_items 276864.
I0304 19:32:13.283273 23128000471168 run.py:483] Algo bellman_ford step 8652 current loss 0.074859, current_train_items 276896.
I0304 19:32:13.316558 23128000471168 run.py:483] Algo bellman_ford step 8653 current loss 0.055544, current_train_items 276928.
I0304 19:32:13.352013 23128000471168 run.py:483] Algo bellman_ford step 8654 current loss 0.090149, current_train_items 276960.
I0304 19:32:13.372177 23128000471168 run.py:483] Algo bellman_ford step 8655 current loss 0.001983, current_train_items 276992.
I0304 19:32:13.388313 23128000471168 run.py:483] Algo bellman_ford step 8656 current loss 0.008010, current_train_items 277024.
I0304 19:32:13.414137 23128000471168 run.py:483] Algo bellman_ford step 8657 current loss 0.059622, current_train_items 277056.
I0304 19:32:13.447054 23128000471168 run.py:483] Algo bellman_ford step 8658 current loss 0.054755, current_train_items 277088.
I0304 19:32:13.481112 23128000471168 run.py:483] Algo bellman_ford step 8659 current loss 0.100173, current_train_items 277120.
I0304 19:32:13.501459 23128000471168 run.py:483] Algo bellman_ford step 8660 current loss 0.002378, current_train_items 277152.
I0304 19:32:13.517914 23128000471168 run.py:483] Algo bellman_ford step 8661 current loss 0.009407, current_train_items 277184.
I0304 19:32:13.542012 23128000471168 run.py:483] Algo bellman_ford step 8662 current loss 0.035141, current_train_items 277216.
I0304 19:32:13.574145 23128000471168 run.py:483] Algo bellman_ford step 8663 current loss 0.063334, current_train_items 277248.
I0304 19:32:13.609393 23128000471168 run.py:483] Algo bellman_ford step 8664 current loss 0.051307, current_train_items 277280.
I0304 19:32:13.629361 23128000471168 run.py:483] Algo bellman_ford step 8665 current loss 0.002249, current_train_items 277312.
I0304 19:32:13.645353 23128000471168 run.py:483] Algo bellman_ford step 8666 current loss 0.025805, current_train_items 277344.
I0304 19:32:13.670350 23128000471168 run.py:483] Algo bellman_ford step 8667 current loss 0.014666, current_train_items 277376.
I0304 19:32:13.701950 23128000471168 run.py:483] Algo bellman_ford step 8668 current loss 0.025538, current_train_items 277408.
I0304 19:32:13.735305 23128000471168 run.py:483] Algo bellman_ford step 8669 current loss 0.030447, current_train_items 277440.
I0304 19:32:13.755693 23128000471168 run.py:483] Algo bellman_ford step 8670 current loss 0.002478, current_train_items 277472.
I0304 19:32:13.772626 23128000471168 run.py:483] Algo bellman_ford step 8671 current loss 0.016888, current_train_items 277504.
I0304 19:32:13.796462 23128000471168 run.py:483] Algo bellman_ford step 8672 current loss 0.020681, current_train_items 277536.
I0304 19:32:13.828700 23128000471168 run.py:483] Algo bellman_ford step 8673 current loss 0.045376, current_train_items 277568.
I0304 19:32:13.861149 23128000471168 run.py:483] Algo bellman_ford step 8674 current loss 0.082160, current_train_items 277600.
I0304 19:32:13.881659 23128000471168 run.py:483] Algo bellman_ford step 8675 current loss 0.003972, current_train_items 277632.
I0304 19:32:13.898108 23128000471168 run.py:483] Algo bellman_ford step 8676 current loss 0.015729, current_train_items 277664.
I0304 19:32:13.922468 23128000471168 run.py:483] Algo bellman_ford step 8677 current loss 0.032922, current_train_items 277696.
I0304 19:32:13.954870 23128000471168 run.py:483] Algo bellman_ford step 8678 current loss 0.029610, current_train_items 277728.
I0304 19:32:13.988224 23128000471168 run.py:483] Algo bellman_ford step 8679 current loss 0.045169, current_train_items 277760.
I0304 19:32:14.008153 23128000471168 run.py:483] Algo bellman_ford step 8680 current loss 0.002936, current_train_items 277792.
I0304 19:32:14.024800 23128000471168 run.py:483] Algo bellman_ford step 8681 current loss 0.006440, current_train_items 277824.
I0304 19:32:14.049520 23128000471168 run.py:483] Algo bellman_ford step 8682 current loss 0.020241, current_train_items 277856.
I0304 19:32:14.081598 23128000471168 run.py:483] Algo bellman_ford step 8683 current loss 0.032613, current_train_items 277888.
I0304 19:32:14.114971 23128000471168 run.py:483] Algo bellman_ford step 8684 current loss 0.056696, current_train_items 277920.
I0304 19:32:14.135526 23128000471168 run.py:483] Algo bellman_ford step 8685 current loss 0.001593, current_train_items 277952.
I0304 19:32:14.151898 23128000471168 run.py:483] Algo bellman_ford step 8686 current loss 0.024541, current_train_items 277984.
I0304 19:32:14.176800 23128000471168 run.py:483] Algo bellman_ford step 8687 current loss 0.042056, current_train_items 278016.
I0304 19:32:14.209523 23128000471168 run.py:483] Algo bellman_ford step 8688 current loss 0.029135, current_train_items 278048.
I0304 19:32:14.242499 23128000471168 run.py:483] Algo bellman_ford step 8689 current loss 0.067887, current_train_items 278080.
I0304 19:32:14.262462 23128000471168 run.py:483] Algo bellman_ford step 8690 current loss 0.003320, current_train_items 278112.
I0304 19:32:14.279357 23128000471168 run.py:483] Algo bellman_ford step 8691 current loss 0.033546, current_train_items 278144.
I0304 19:32:14.303595 23128000471168 run.py:483] Algo bellman_ford step 8692 current loss 0.034449, current_train_items 278176.
I0304 19:32:14.335690 23128000471168 run.py:483] Algo bellman_ford step 8693 current loss 0.020391, current_train_items 278208.
I0304 19:32:14.369535 23128000471168 run.py:483] Algo bellman_ford step 8694 current loss 0.059391, current_train_items 278240.
I0304 19:32:14.389698 23128000471168 run.py:483] Algo bellman_ford step 8695 current loss 0.002480, current_train_items 278272.
I0304 19:32:14.405980 23128000471168 run.py:483] Algo bellman_ford step 8696 current loss 0.008466, current_train_items 278304.
I0304 19:32:14.430456 23128000471168 run.py:483] Algo bellman_ford step 8697 current loss 0.063411, current_train_items 278336.
I0304 19:32:14.463137 23128000471168 run.py:483] Algo bellman_ford step 8698 current loss 0.027335, current_train_items 278368.
I0304 19:32:14.499140 23128000471168 run.py:483] Algo bellman_ford step 8699 current loss 0.103080, current_train_items 278400.
I0304 19:32:14.519323 23128000471168 run.py:483] Algo bellman_ford step 8700 current loss 0.001371, current_train_items 278432.
I0304 19:32:14.527096 23128000471168 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0304 19:32:14.527204 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:14.544443 23128000471168 run.py:483] Algo bellman_ford step 8701 current loss 0.006741, current_train_items 278464.
I0304 19:32:14.569078 23128000471168 run.py:483] Algo bellman_ford step 8702 current loss 0.022753, current_train_items 278496.
I0304 19:32:14.602581 23128000471168 run.py:483] Algo bellman_ford step 8703 current loss 0.074736, current_train_items 278528.
I0304 19:32:14.638364 23128000471168 run.py:483] Algo bellman_ford step 8704 current loss 0.071613, current_train_items 278560.
I0304 19:32:14.658818 23128000471168 run.py:483] Algo bellman_ford step 8705 current loss 0.004339, current_train_items 278592.
I0304 19:32:14.674857 23128000471168 run.py:483] Algo bellman_ford step 8706 current loss 0.017952, current_train_items 278624.
I0304 19:32:14.699352 23128000471168 run.py:483] Algo bellman_ford step 8707 current loss 0.024188, current_train_items 278656.
I0304 19:32:14.732258 23128000471168 run.py:483] Algo bellman_ford step 8708 current loss 0.045506, current_train_items 278688.
I0304 19:32:14.766572 23128000471168 run.py:483] Algo bellman_ford step 8709 current loss 0.055450, current_train_items 278720.
I0304 19:32:14.786545 23128000471168 run.py:483] Algo bellman_ford step 8710 current loss 0.023689, current_train_items 278752.
I0304 19:32:14.803185 23128000471168 run.py:483] Algo bellman_ford step 8711 current loss 0.009252, current_train_items 278784.
I0304 19:32:14.827849 23128000471168 run.py:483] Algo bellman_ford step 8712 current loss 0.026584, current_train_items 278816.
I0304 19:32:14.862385 23128000471168 run.py:483] Algo bellman_ford step 8713 current loss 0.063409, current_train_items 278848.
I0304 19:32:14.895436 23128000471168 run.py:483] Algo bellman_ford step 8714 current loss 0.030771, current_train_items 278880.
I0304 19:32:14.915284 23128000471168 run.py:483] Algo bellman_ford step 8715 current loss 0.003914, current_train_items 278912.
I0304 19:32:14.931952 23128000471168 run.py:483] Algo bellman_ford step 8716 current loss 0.022008, current_train_items 278944.
I0304 19:32:14.955850 23128000471168 run.py:483] Algo bellman_ford step 8717 current loss 0.012661, current_train_items 278976.
I0304 19:32:14.987309 23128000471168 run.py:483] Algo bellman_ford step 8718 current loss 0.020972, current_train_items 279008.
I0304 19:32:15.020417 23128000471168 run.py:483] Algo bellman_ford step 8719 current loss 0.049544, current_train_items 279040.
I0304 19:32:15.040311 23128000471168 run.py:483] Algo bellman_ford step 8720 current loss 0.001467, current_train_items 279072.
I0304 19:32:15.057314 23128000471168 run.py:483] Algo bellman_ford step 8721 current loss 0.019427, current_train_items 279104.
I0304 19:32:15.082479 23128000471168 run.py:483] Algo bellman_ford step 8722 current loss 0.033994, current_train_items 279136.
I0304 19:32:15.114148 23128000471168 run.py:483] Algo bellman_ford step 8723 current loss 0.037899, current_train_items 279168.
I0304 19:32:15.147582 23128000471168 run.py:483] Algo bellman_ford step 8724 current loss 0.030249, current_train_items 279200.
I0304 19:32:15.167747 23128000471168 run.py:483] Algo bellman_ford step 8725 current loss 0.004732, current_train_items 279232.
I0304 19:32:15.184342 23128000471168 run.py:483] Algo bellman_ford step 8726 current loss 0.029236, current_train_items 279264.
I0304 19:32:15.209289 23128000471168 run.py:483] Algo bellman_ford step 8727 current loss 0.021721, current_train_items 279296.
I0304 19:32:15.241316 23128000471168 run.py:483] Algo bellman_ford step 8728 current loss 0.050366, current_train_items 279328.
I0304 19:32:15.275120 23128000471168 run.py:483] Algo bellman_ford step 8729 current loss 0.054023, current_train_items 279360.
I0304 19:32:15.295191 23128000471168 run.py:483] Algo bellman_ford step 8730 current loss 0.004586, current_train_items 279392.
I0304 19:32:15.311197 23128000471168 run.py:483] Algo bellman_ford step 8731 current loss 0.002224, current_train_items 279424.
I0304 19:32:15.335700 23128000471168 run.py:483] Algo bellman_ford step 8732 current loss 0.053737, current_train_items 279456.
I0304 19:32:15.368664 23128000471168 run.py:483] Algo bellman_ford step 8733 current loss 0.059181, current_train_items 279488.
I0304 19:32:15.404280 23128000471168 run.py:483] Algo bellman_ford step 8734 current loss 0.049264, current_train_items 279520.
I0304 19:32:15.423830 23128000471168 run.py:483] Algo bellman_ford step 8735 current loss 0.002135, current_train_items 279552.
I0304 19:32:15.440200 23128000471168 run.py:483] Algo bellman_ford step 8736 current loss 0.007407, current_train_items 279584.
I0304 19:32:15.465806 23128000471168 run.py:483] Algo bellman_ford step 8737 current loss 0.060756, current_train_items 279616.
I0304 19:32:15.497831 23128000471168 run.py:483] Algo bellman_ford step 8738 current loss 0.045385, current_train_items 279648.
I0304 19:32:15.532538 23128000471168 run.py:483] Algo bellman_ford step 8739 current loss 0.080959, current_train_items 279680.
I0304 19:32:15.552506 23128000471168 run.py:483] Algo bellman_ford step 8740 current loss 0.001637, current_train_items 279712.
I0304 19:32:15.568795 23128000471168 run.py:483] Algo bellman_ford step 8741 current loss 0.004870, current_train_items 279744.
I0304 19:32:15.593414 23128000471168 run.py:483] Algo bellman_ford step 8742 current loss 0.026470, current_train_items 279776.
I0304 19:32:15.626343 23128000471168 run.py:483] Algo bellman_ford step 8743 current loss 0.038835, current_train_items 279808.
I0304 19:32:15.660807 23128000471168 run.py:483] Algo bellman_ford step 8744 current loss 0.035121, current_train_items 279840.
I0304 19:32:15.680711 23128000471168 run.py:483] Algo bellman_ford step 8745 current loss 0.007764, current_train_items 279872.
I0304 19:32:15.697434 23128000471168 run.py:483] Algo bellman_ford step 8746 current loss 0.037691, current_train_items 279904.
I0304 19:32:15.721449 23128000471168 run.py:483] Algo bellman_ford step 8747 current loss 0.054654, current_train_items 279936.
I0304 19:32:15.755214 23128000471168 run.py:483] Algo bellman_ford step 8748 current loss 0.033149, current_train_items 279968.
I0304 19:32:15.790533 23128000471168 run.py:483] Algo bellman_ford step 8749 current loss 0.030088, current_train_items 280000.
I0304 19:32:15.810278 23128000471168 run.py:483] Algo bellman_ford step 8750 current loss 0.002232, current_train_items 280032.
I0304 19:32:15.818070 23128000471168 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0304 19:32:15.818179 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:15.835367 23128000471168 run.py:483] Algo bellman_ford step 8751 current loss 0.014123, current_train_items 280064.
I0304 19:32:15.859782 23128000471168 run.py:483] Algo bellman_ford step 8752 current loss 0.037920, current_train_items 280096.
I0304 19:32:15.893325 23128000471168 run.py:483] Algo bellman_ford step 8753 current loss 0.026506, current_train_items 280128.
I0304 19:32:15.927778 23128000471168 run.py:483] Algo bellman_ford step 8754 current loss 0.031646, current_train_items 280160.
I0304 19:32:15.947983 23128000471168 run.py:483] Algo bellman_ford step 8755 current loss 0.025784, current_train_items 280192.
I0304 19:32:15.964051 23128000471168 run.py:483] Algo bellman_ford step 8756 current loss 0.012553, current_train_items 280224.
I0304 19:32:15.989547 23128000471168 run.py:483] Algo bellman_ford step 8757 current loss 0.038809, current_train_items 280256.
I0304 19:32:16.021948 23128000471168 run.py:483] Algo bellman_ford step 8758 current loss 0.016208, current_train_items 280288.
I0304 19:32:16.056293 23128000471168 run.py:483] Algo bellman_ford step 8759 current loss 0.051981, current_train_items 280320.
I0304 19:32:16.076709 23128000471168 run.py:483] Algo bellman_ford step 8760 current loss 0.004832, current_train_items 280352.
I0304 19:32:16.093428 23128000471168 run.py:483] Algo bellman_ford step 8761 current loss 0.005354, current_train_items 280384.
I0304 19:32:16.117967 23128000471168 run.py:483] Algo bellman_ford step 8762 current loss 0.023109, current_train_items 280416.
I0304 19:32:16.149665 23128000471168 run.py:483] Algo bellman_ford step 8763 current loss 0.031841, current_train_items 280448.
I0304 19:32:16.185370 23128000471168 run.py:483] Algo bellman_ford step 8764 current loss 0.063945, current_train_items 280480.
I0304 19:32:16.205121 23128000471168 run.py:483] Algo bellman_ford step 8765 current loss 0.002677, current_train_items 280512.
I0304 19:32:16.221963 23128000471168 run.py:483] Algo bellman_ford step 8766 current loss 0.011579, current_train_items 280544.
I0304 19:32:16.245314 23128000471168 run.py:483] Algo bellman_ford step 8767 current loss 0.025008, current_train_items 280576.
I0304 19:32:16.277129 23128000471168 run.py:483] Algo bellman_ford step 8768 current loss 0.051810, current_train_items 280608.
I0304 19:32:16.309333 23128000471168 run.py:483] Algo bellman_ford step 8769 current loss 0.037360, current_train_items 280640.
I0304 19:32:16.329760 23128000471168 run.py:483] Algo bellman_ford step 8770 current loss 0.003499, current_train_items 280672.
I0304 19:32:16.346625 23128000471168 run.py:483] Algo bellman_ford step 8771 current loss 0.005875, current_train_items 280704.
I0304 19:32:16.371444 23128000471168 run.py:483] Algo bellman_ford step 8772 current loss 0.023936, current_train_items 280736.
I0304 19:32:16.404243 23128000471168 run.py:483] Algo bellman_ford step 8773 current loss 0.037589, current_train_items 280768.
I0304 19:32:16.437588 23128000471168 run.py:483] Algo bellman_ford step 8774 current loss 0.053988, current_train_items 280800.
I0304 19:32:16.457705 23128000471168 run.py:483] Algo bellman_ford step 8775 current loss 0.009963, current_train_items 280832.
I0304 19:32:16.474311 23128000471168 run.py:483] Algo bellman_ford step 8776 current loss 0.025352, current_train_items 280864.
I0304 19:32:16.497951 23128000471168 run.py:483] Algo bellman_ford step 8777 current loss 0.056178, current_train_items 280896.
I0304 19:32:16.530011 23128000471168 run.py:483] Algo bellman_ford step 8778 current loss 0.055768, current_train_items 280928.
I0304 19:32:16.564906 23128000471168 run.py:483] Algo bellman_ford step 8779 current loss 0.028260, current_train_items 280960.
I0304 19:32:16.584653 23128000471168 run.py:483] Algo bellman_ford step 8780 current loss 0.001830, current_train_items 280992.
I0304 19:32:16.601072 23128000471168 run.py:483] Algo bellman_ford step 8781 current loss 0.070444, current_train_items 281024.
I0304 19:32:16.625529 23128000471168 run.py:483] Algo bellman_ford step 8782 current loss 0.046951, current_train_items 281056.
I0304 19:32:16.658538 23128000471168 run.py:483] Algo bellman_ford step 8783 current loss 0.044700, current_train_items 281088.
I0304 19:32:16.693411 23128000471168 run.py:483] Algo bellman_ford step 8784 current loss 0.073089, current_train_items 281120.
I0304 19:32:16.713572 23128000471168 run.py:483] Algo bellman_ford step 8785 current loss 0.001398, current_train_items 281152.
I0304 19:32:16.730138 23128000471168 run.py:483] Algo bellman_ford step 8786 current loss 0.038428, current_train_items 281184.
I0304 19:32:16.754718 23128000471168 run.py:483] Algo bellman_ford step 8787 current loss 0.082469, current_train_items 281216.
I0304 19:32:16.786848 23128000471168 run.py:483] Algo bellman_ford step 8788 current loss 0.073886, current_train_items 281248.
I0304 19:32:16.820981 23128000471168 run.py:483] Algo bellman_ford step 8789 current loss 0.059483, current_train_items 281280.
I0304 19:32:16.841072 23128000471168 run.py:483] Algo bellman_ford step 8790 current loss 0.034891, current_train_items 281312.
I0304 19:32:16.858122 23128000471168 run.py:483] Algo bellman_ford step 8791 current loss 0.015655, current_train_items 281344.
I0304 19:32:16.882247 23128000471168 run.py:483] Algo bellman_ford step 8792 current loss 0.027899, current_train_items 281376.
I0304 19:32:16.914740 23128000471168 run.py:483] Algo bellman_ford step 8793 current loss 0.048230, current_train_items 281408.
I0304 19:32:16.950138 23128000471168 run.py:483] Algo bellman_ford step 8794 current loss 0.110912, current_train_items 281440.
I0304 19:32:16.969996 23128000471168 run.py:483] Algo bellman_ford step 8795 current loss 0.012603, current_train_items 281472.
I0304 19:32:16.986568 23128000471168 run.py:483] Algo bellman_ford step 8796 current loss 0.023767, current_train_items 281504.
I0304 19:32:17.011045 23128000471168 run.py:483] Algo bellman_ford step 8797 current loss 0.026611, current_train_items 281536.
I0304 19:32:17.043715 23128000471168 run.py:483] Algo bellman_ford step 8798 current loss 0.055690, current_train_items 281568.
I0304 19:32:17.079248 23128000471168 run.py:483] Algo bellman_ford step 8799 current loss 0.100639, current_train_items 281600.
I0304 19:32:17.099236 23128000471168 run.py:483] Algo bellman_ford step 8800 current loss 0.003234, current_train_items 281632.
I0304 19:32:17.107418 23128000471168 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0304 19:32:17.107527 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:17.124337 23128000471168 run.py:483] Algo bellman_ford step 8801 current loss 0.029228, current_train_items 281664.
I0304 19:32:17.149518 23128000471168 run.py:483] Algo bellman_ford step 8802 current loss 0.053965, current_train_items 281696.
I0304 19:32:17.183393 23128000471168 run.py:483] Algo bellman_ford step 8803 current loss 0.048558, current_train_items 281728.
I0304 19:32:17.217980 23128000471168 run.py:483] Algo bellman_ford step 8804 current loss 0.035916, current_train_items 281760.
I0304 19:32:17.238227 23128000471168 run.py:483] Algo bellman_ford step 8805 current loss 0.001898, current_train_items 281792.
I0304 19:32:17.254476 23128000471168 run.py:483] Algo bellman_ford step 8806 current loss 0.048955, current_train_items 281824.
I0304 19:32:17.279108 23128000471168 run.py:483] Algo bellman_ford step 8807 current loss 0.049420, current_train_items 281856.
I0304 19:32:17.311392 23128000471168 run.py:483] Algo bellman_ford step 8808 current loss 0.099928, current_train_items 281888.
I0304 19:32:17.347727 23128000471168 run.py:483] Algo bellman_ford step 8809 current loss 0.090574, current_train_items 281920.
I0304 19:32:17.367567 23128000471168 run.py:483] Algo bellman_ford step 8810 current loss 0.002585, current_train_items 281952.
I0304 19:32:17.384195 23128000471168 run.py:483] Algo bellman_ford step 8811 current loss 0.009790, current_train_items 281984.
I0304 19:32:17.408541 23128000471168 run.py:483] Algo bellman_ford step 8812 current loss 0.035785, current_train_items 282016.
I0304 19:32:17.441553 23128000471168 run.py:483] Algo bellman_ford step 8813 current loss 0.040862, current_train_items 282048.
I0304 19:32:17.475645 23128000471168 run.py:483] Algo bellman_ford step 8814 current loss 0.060392, current_train_items 282080.
I0304 19:32:17.495346 23128000471168 run.py:483] Algo bellman_ford step 8815 current loss 0.003070, current_train_items 282112.
I0304 19:32:17.511935 23128000471168 run.py:483] Algo bellman_ford step 8816 current loss 0.040049, current_train_items 282144.
I0304 19:32:17.536443 23128000471168 run.py:483] Algo bellman_ford step 8817 current loss 0.022083, current_train_items 282176.
I0304 19:32:17.567949 23128000471168 run.py:483] Algo bellman_ford step 8818 current loss 0.026727, current_train_items 282208.
I0304 19:32:17.605209 23128000471168 run.py:483] Algo bellman_ford step 8819 current loss 0.051082, current_train_items 282240.
I0304 19:32:17.625107 23128000471168 run.py:483] Algo bellman_ford step 8820 current loss 0.002194, current_train_items 282272.
I0304 19:32:17.641508 23128000471168 run.py:483] Algo bellman_ford step 8821 current loss 0.013650, current_train_items 282304.
I0304 19:32:17.666049 23128000471168 run.py:483] Algo bellman_ford step 8822 current loss 0.025959, current_train_items 282336.
I0304 19:32:17.699184 23128000471168 run.py:483] Algo bellman_ford step 8823 current loss 0.041110, current_train_items 282368.
I0304 19:32:17.732393 23128000471168 run.py:483] Algo bellman_ford step 8824 current loss 0.061141, current_train_items 282400.
I0304 19:32:17.752229 23128000471168 run.py:483] Algo bellman_ford step 8825 current loss 0.003010, current_train_items 282432.
I0304 19:32:17.768817 23128000471168 run.py:483] Algo bellman_ford step 8826 current loss 0.011234, current_train_items 282464.
I0304 19:32:17.793846 23128000471168 run.py:483] Algo bellman_ford step 8827 current loss 0.031600, current_train_items 282496.
I0304 19:32:17.826539 23128000471168 run.py:483] Algo bellman_ford step 8828 current loss 0.044454, current_train_items 282528.
I0304 19:32:17.860633 23128000471168 run.py:483] Algo bellman_ford step 8829 current loss 0.025130, current_train_items 282560.
I0304 19:32:17.880334 23128000471168 run.py:483] Algo bellman_ford step 8830 current loss 0.002809, current_train_items 282592.
I0304 19:32:17.896601 23128000471168 run.py:483] Algo bellman_ford step 8831 current loss 0.007480, current_train_items 282624.
I0304 19:32:17.921520 23128000471168 run.py:483] Algo bellman_ford step 8832 current loss 0.061194, current_train_items 282656.
I0304 19:32:17.952541 23128000471168 run.py:483] Algo bellman_ford step 8833 current loss 0.036162, current_train_items 282688.
I0304 19:32:17.985821 23128000471168 run.py:483] Algo bellman_ford step 8834 current loss 0.046720, current_train_items 282720.
I0304 19:32:18.005999 23128000471168 run.py:483] Algo bellman_ford step 8835 current loss 0.003580, current_train_items 282752.
I0304 19:32:18.022224 23128000471168 run.py:483] Algo bellman_ford step 8836 current loss 0.017838, current_train_items 282784.
I0304 19:32:18.047316 23128000471168 run.py:483] Algo bellman_ford step 8837 current loss 0.048461, current_train_items 282816.
I0304 19:32:18.079833 23128000471168 run.py:483] Algo bellman_ford step 8838 current loss 0.073045, current_train_items 282848.
I0304 19:32:18.114212 23128000471168 run.py:483] Algo bellman_ford step 8839 current loss 0.067893, current_train_items 282880.
I0304 19:32:18.134406 23128000471168 run.py:483] Algo bellman_ford step 8840 current loss 0.003540, current_train_items 282912.
I0304 19:32:18.151202 23128000471168 run.py:483] Algo bellman_ford step 8841 current loss 0.010141, current_train_items 282944.
I0304 19:32:18.175983 23128000471168 run.py:483] Algo bellman_ford step 8842 current loss 0.031794, current_train_items 282976.
I0304 19:32:18.208230 23128000471168 run.py:483] Algo bellman_ford step 8843 current loss 0.070639, current_train_items 283008.
I0304 19:32:18.244103 23128000471168 run.py:483] Algo bellman_ford step 8844 current loss 0.078203, current_train_items 283040.
I0304 19:32:18.264317 23128000471168 run.py:483] Algo bellman_ford step 8845 current loss 0.004042, current_train_items 283072.
I0304 19:32:18.280607 23128000471168 run.py:483] Algo bellman_ford step 8846 current loss 0.035431, current_train_items 283104.
I0304 19:32:18.304916 23128000471168 run.py:483] Algo bellman_ford step 8847 current loss 0.035511, current_train_items 283136.
I0304 19:32:18.335908 23128000471168 run.py:483] Algo bellman_ford step 8848 current loss 0.019006, current_train_items 283168.
I0304 19:32:18.370055 23128000471168 run.py:483] Algo bellman_ford step 8849 current loss 0.043678, current_train_items 283200.
I0304 19:32:18.389773 23128000471168 run.py:483] Algo bellman_ford step 8850 current loss 0.001606, current_train_items 283232.
I0304 19:32:18.398055 23128000471168 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0304 19:32:18.398164 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:18.415381 23128000471168 run.py:483] Algo bellman_ford step 8851 current loss 0.013231, current_train_items 283264.
I0304 19:32:18.440703 23128000471168 run.py:483] Algo bellman_ford step 8852 current loss 0.040284, current_train_items 283296.
I0304 19:32:18.473554 23128000471168 run.py:483] Algo bellman_ford step 8853 current loss 0.062045, current_train_items 283328.
I0304 19:32:18.508387 23128000471168 run.py:483] Algo bellman_ford step 8854 current loss 0.048957, current_train_items 283360.
I0304 19:32:18.528735 23128000471168 run.py:483] Algo bellman_ford step 8855 current loss 0.002470, current_train_items 283392.
I0304 19:32:18.545186 23128000471168 run.py:483] Algo bellman_ford step 8856 current loss 0.039335, current_train_items 283424.
I0304 19:32:18.569431 23128000471168 run.py:483] Algo bellman_ford step 8857 current loss 0.047732, current_train_items 283456.
I0304 19:32:18.602717 23128000471168 run.py:483] Algo bellman_ford step 8858 current loss 0.038553, current_train_items 283488.
I0304 19:32:18.635208 23128000471168 run.py:483] Algo bellman_ford step 8859 current loss 0.030291, current_train_items 283520.
I0304 19:32:18.655439 23128000471168 run.py:483] Algo bellman_ford step 8860 current loss 0.001618, current_train_items 283552.
I0304 19:32:18.672568 23128000471168 run.py:483] Algo bellman_ford step 8861 current loss 0.005602, current_train_items 283584.
I0304 19:32:18.696910 23128000471168 run.py:483] Algo bellman_ford step 8862 current loss 0.027505, current_train_items 283616.
I0304 19:32:18.728447 23128000471168 run.py:483] Algo bellman_ford step 8863 current loss 0.020112, current_train_items 283648.
I0304 19:32:18.761548 23128000471168 run.py:483] Algo bellman_ford step 8864 current loss 0.046052, current_train_items 283680.
I0304 19:32:18.781728 23128000471168 run.py:483] Algo bellman_ford step 8865 current loss 0.016127, current_train_items 283712.
I0304 19:32:18.798187 23128000471168 run.py:483] Algo bellman_ford step 8866 current loss 0.011922, current_train_items 283744.
I0304 19:32:18.821537 23128000471168 run.py:483] Algo bellman_ford step 8867 current loss 0.029203, current_train_items 283776.
I0304 19:32:18.854232 23128000471168 run.py:483] Algo bellman_ford step 8868 current loss 0.062710, current_train_items 283808.
I0304 19:32:18.887957 23128000471168 run.py:483] Algo bellman_ford step 8869 current loss 0.058375, current_train_items 283840.
I0304 19:32:18.908342 23128000471168 run.py:483] Algo bellman_ford step 8870 current loss 0.004128, current_train_items 283872.
I0304 19:32:18.924861 23128000471168 run.py:483] Algo bellman_ford step 8871 current loss 0.002805, current_train_items 283904.
I0304 19:32:18.949293 23128000471168 run.py:483] Algo bellman_ford step 8872 current loss 0.023078, current_train_items 283936.
I0304 19:32:18.981485 23128000471168 run.py:483] Algo bellman_ford step 8873 current loss 0.026438, current_train_items 283968.
I0304 19:32:19.016699 23128000471168 run.py:483] Algo bellman_ford step 8874 current loss 0.113454, current_train_items 284000.
I0304 19:32:19.036965 23128000471168 run.py:483] Algo bellman_ford step 8875 current loss 0.001220, current_train_items 284032.
I0304 19:32:19.053632 23128000471168 run.py:483] Algo bellman_ford step 8876 current loss 0.012214, current_train_items 284064.
I0304 19:32:19.077250 23128000471168 run.py:483] Algo bellman_ford step 8877 current loss 0.036894, current_train_items 284096.
I0304 19:32:19.109045 23128000471168 run.py:483] Algo bellman_ford step 8878 current loss 0.029705, current_train_items 284128.
I0304 19:32:19.143805 23128000471168 run.py:483] Algo bellman_ford step 8879 current loss 0.085836, current_train_items 284160.
I0304 19:32:19.163819 23128000471168 run.py:483] Algo bellman_ford step 8880 current loss 0.001629, current_train_items 284192.
I0304 19:32:19.180345 23128000471168 run.py:483] Algo bellman_ford step 8881 current loss 0.018489, current_train_items 284224.
I0304 19:32:19.203962 23128000471168 run.py:483] Algo bellman_ford step 8882 current loss 0.026058, current_train_items 284256.
I0304 19:32:19.237213 23128000471168 run.py:483] Algo bellman_ford step 8883 current loss 0.031816, current_train_items 284288.
I0304 19:32:19.271250 23128000471168 run.py:483] Algo bellman_ford step 8884 current loss 0.054133, current_train_items 284320.
I0304 19:32:19.291677 23128000471168 run.py:483] Algo bellman_ford step 8885 current loss 0.004671, current_train_items 284352.
I0304 19:32:19.308962 23128000471168 run.py:483] Algo bellman_ford step 8886 current loss 0.018683, current_train_items 284384.
I0304 19:32:19.333273 23128000471168 run.py:483] Algo bellman_ford step 8887 current loss 0.036572, current_train_items 284416.
I0304 19:32:19.364990 23128000471168 run.py:483] Algo bellman_ford step 8888 current loss 0.056648, current_train_items 284448.
I0304 19:32:19.399604 23128000471168 run.py:483] Algo bellman_ford step 8889 current loss 0.055791, current_train_items 284480.
I0304 19:32:19.420061 23128000471168 run.py:483] Algo bellman_ford step 8890 current loss 0.001421, current_train_items 284512.
I0304 19:32:19.436213 23128000471168 run.py:483] Algo bellman_ford step 8891 current loss 0.020542, current_train_items 284544.
I0304 19:32:19.460569 23128000471168 run.py:483] Algo bellman_ford step 8892 current loss 0.027510, current_train_items 284576.
I0304 19:32:19.494143 23128000471168 run.py:483] Algo bellman_ford step 8893 current loss 0.048225, current_train_items 284608.
I0304 19:32:19.528194 23128000471168 run.py:483] Algo bellman_ford step 8894 current loss 0.044129, current_train_items 284640.
I0304 19:32:19.547992 23128000471168 run.py:483] Algo bellman_ford step 8895 current loss 0.002019, current_train_items 284672.
I0304 19:32:19.564399 23128000471168 run.py:483] Algo bellman_ford step 8896 current loss 0.013846, current_train_items 284704.
I0304 19:32:19.588202 23128000471168 run.py:483] Algo bellman_ford step 8897 current loss 0.022421, current_train_items 284736.
I0304 19:32:19.621546 23128000471168 run.py:483] Algo bellman_ford step 8898 current loss 0.054239, current_train_items 284768.
I0304 19:32:19.657436 23128000471168 run.py:483] Algo bellman_ford step 8899 current loss 0.045460, current_train_items 284800.
I0304 19:32:19.677667 23128000471168 run.py:483] Algo bellman_ford step 8900 current loss 0.002085, current_train_items 284832.
I0304 19:32:19.685464 23128000471168 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.998046875, 'score': 0.998046875, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0304 19:32:19.685572 23128000471168 run.py:519] Checkpointing best model, best avg val score was 0.997, current avg val score is 0.998, val scores are: bellman_ford: 0.998
I0304 19:32:19.715611 23128000471168 run.py:483] Algo bellman_ford step 8901 current loss 0.011622, current_train_items 284864.
I0304 19:32:19.741048 23128000471168 run.py:483] Algo bellman_ford step 8902 current loss 0.017366, current_train_items 284896.
I0304 19:32:19.773307 23128000471168 run.py:483] Algo bellman_ford step 8903 current loss 0.049730, current_train_items 284928.
I0304 19:32:19.809644 23128000471168 run.py:483] Algo bellman_ford step 8904 current loss 0.045499, current_train_items 284960.
I0304 19:32:19.829949 23128000471168 run.py:483] Algo bellman_ford step 8905 current loss 0.001555, current_train_items 284992.
I0304 19:32:19.846416 23128000471168 run.py:483] Algo bellman_ford step 8906 current loss 0.016837, current_train_items 285024.
I0304 19:32:19.870854 23128000471168 run.py:483] Algo bellman_ford step 8907 current loss 0.049911, current_train_items 285056.
I0304 19:32:19.903486 23128000471168 run.py:483] Algo bellman_ford step 8908 current loss 0.040335, current_train_items 285088.
I0304 19:32:19.936186 23128000471168 run.py:483] Algo bellman_ford step 8909 current loss 0.028463, current_train_items 285120.
I0304 19:32:19.956801 23128000471168 run.py:483] Algo bellman_ford step 8910 current loss 0.001991, current_train_items 285152.
I0304 19:32:19.973209 23128000471168 run.py:483] Algo bellman_ford step 8911 current loss 0.023528, current_train_items 285184.
I0304 19:32:19.998355 23128000471168 run.py:483] Algo bellman_ford step 8912 current loss 0.050096, current_train_items 285216.
I0304 19:32:20.030010 23128000471168 run.py:483] Algo bellman_ford step 8913 current loss 0.054244, current_train_items 285248.
I0304 19:32:20.065782 23128000471168 run.py:483] Algo bellman_ford step 8914 current loss 0.049165, current_train_items 285280.
I0304 19:32:20.086127 23128000471168 run.py:483] Algo bellman_ford step 8915 current loss 0.001497, current_train_items 285312.
I0304 19:32:20.102339 23128000471168 run.py:483] Algo bellman_ford step 8916 current loss 0.008541, current_train_items 285344.
I0304 19:32:20.127354 23128000471168 run.py:483] Algo bellman_ford step 8917 current loss 0.052369, current_train_items 285376.
I0304 19:32:20.159969 23128000471168 run.py:483] Algo bellman_ford step 8918 current loss 0.018105, current_train_items 285408.
I0304 19:32:20.193971 23128000471168 run.py:483] Algo bellman_ford step 8919 current loss 0.054886, current_train_items 285440.
I0304 19:32:20.214289 23128000471168 run.py:483] Algo bellman_ford step 8920 current loss 0.002399, current_train_items 285472.
I0304 19:32:20.231085 23128000471168 run.py:483] Algo bellman_ford step 8921 current loss 0.039469, current_train_items 285504.
I0304 19:32:20.256314 23128000471168 run.py:483] Algo bellman_ford step 8922 current loss 0.031331, current_train_items 285536.
I0304 19:32:20.288457 23128000471168 run.py:483] Algo bellman_ford step 8923 current loss 0.043264, current_train_items 285568.
I0304 19:32:20.323001 23128000471168 run.py:483] Algo bellman_ford step 8924 current loss 0.063096, current_train_items 285600.
I0304 19:32:20.343278 23128000471168 run.py:483] Algo bellman_ford step 8925 current loss 0.003602, current_train_items 285632.
I0304 19:32:20.359391 23128000471168 run.py:483] Algo bellman_ford step 8926 current loss 0.017441, current_train_items 285664.
I0304 19:32:20.384308 23128000471168 run.py:483] Algo bellman_ford step 8927 current loss 0.061283, current_train_items 285696.
I0304 19:32:20.415639 23128000471168 run.py:483] Algo bellman_ford step 8928 current loss 0.025848, current_train_items 285728.
I0304 19:32:20.449966 23128000471168 run.py:483] Algo bellman_ford step 8929 current loss 0.052317, current_train_items 285760.
I0304 19:32:20.470032 23128000471168 run.py:483] Algo bellman_ford step 8930 current loss 0.001614, current_train_items 285792.
I0304 19:32:20.487046 23128000471168 run.py:483] Algo bellman_ford step 8931 current loss 0.015286, current_train_items 285824.
I0304 19:32:20.511747 23128000471168 run.py:483] Algo bellman_ford step 8932 current loss 0.035813, current_train_items 285856.
I0304 19:32:20.545531 23128000471168 run.py:483] Algo bellman_ford step 8933 current loss 0.068728, current_train_items 285888.
I0304 19:32:20.580432 23128000471168 run.py:483] Algo bellman_ford step 8934 current loss 0.047889, current_train_items 285920.
I0304 19:32:20.600908 23128000471168 run.py:483] Algo bellman_ford step 8935 current loss 0.002507, current_train_items 285952.
I0304 19:32:20.617226 23128000471168 run.py:483] Algo bellman_ford step 8936 current loss 0.022035, current_train_items 285984.
I0304 19:32:20.641895 23128000471168 run.py:483] Algo bellman_ford step 8937 current loss 0.029902, current_train_items 286016.
I0304 19:32:20.673889 23128000471168 run.py:483] Algo bellman_ford step 8938 current loss 0.034966, current_train_items 286048.
I0304 19:32:20.707161 23128000471168 run.py:483] Algo bellman_ford step 8939 current loss 0.039267, current_train_items 286080.
I0304 19:32:20.727267 23128000471168 run.py:483] Algo bellman_ford step 8940 current loss 0.004429, current_train_items 286112.
I0304 19:32:20.743544 23128000471168 run.py:483] Algo bellman_ford step 8941 current loss 0.008540, current_train_items 286144.
I0304 19:32:20.768591 23128000471168 run.py:483] Algo bellman_ford step 8942 current loss 0.030107, current_train_items 286176.
I0304 19:32:20.802314 23128000471168 run.py:483] Algo bellman_ford step 8943 current loss 0.049465, current_train_items 286208.
I0304 19:32:20.836871 23128000471168 run.py:483] Algo bellman_ford step 8944 current loss 0.036110, current_train_items 286240.
I0304 19:32:20.857142 23128000471168 run.py:483] Algo bellman_ford step 8945 current loss 0.004854, current_train_items 286272.
I0304 19:32:20.873555 23128000471168 run.py:483] Algo bellman_ford step 8946 current loss 0.012610, current_train_items 286304.
I0304 19:32:20.897754 23128000471168 run.py:483] Algo bellman_ford step 8947 current loss 0.043001, current_train_items 286336.
I0304 19:32:20.931230 23128000471168 run.py:483] Algo bellman_ford step 8948 current loss 0.051992, current_train_items 286368.
I0304 19:32:20.965159 23128000471168 run.py:483] Algo bellman_ford step 8949 current loss 0.056812, current_train_items 286400.
I0304 19:32:20.985294 23128000471168 run.py:483] Algo bellman_ford step 8950 current loss 0.001918, current_train_items 286432.
I0304 19:32:20.993501 23128000471168 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0304 19:32:20.993611 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:21.011046 23128000471168 run.py:483] Algo bellman_ford step 8951 current loss 0.030204, current_train_items 286464.
I0304 19:32:21.035689 23128000471168 run.py:483] Algo bellman_ford step 8952 current loss 0.024830, current_train_items 286496.
I0304 19:32:21.071101 23128000471168 run.py:483] Algo bellman_ford step 8953 current loss 0.048268, current_train_items 286528.
I0304 19:32:21.106702 23128000471168 run.py:483] Algo bellman_ford step 8954 current loss 0.048633, current_train_items 286560.
I0304 19:32:21.126837 23128000471168 run.py:483] Algo bellman_ford step 8955 current loss 0.002557, current_train_items 286592.
I0304 19:32:21.143623 23128000471168 run.py:483] Algo bellman_ford step 8956 current loss 0.027537, current_train_items 286624.
I0304 19:32:21.168394 23128000471168 run.py:483] Algo bellman_ford step 8957 current loss 0.035643, current_train_items 286656.
I0304 19:32:21.200536 23128000471168 run.py:483] Algo bellman_ford step 8958 current loss 0.036319, current_train_items 286688.
I0304 19:32:21.235421 23128000471168 run.py:483] Algo bellman_ford step 8959 current loss 0.078240, current_train_items 286720.
I0304 19:32:21.255743 23128000471168 run.py:483] Algo bellman_ford step 8960 current loss 0.004108, current_train_items 286752.
I0304 19:32:21.272041 23128000471168 run.py:483] Algo bellman_ford step 8961 current loss 0.013136, current_train_items 286784.
I0304 19:32:21.297169 23128000471168 run.py:483] Algo bellman_ford step 8962 current loss 0.037462, current_train_items 286816.
I0304 19:32:21.329583 23128000471168 run.py:483] Algo bellman_ford step 8963 current loss 0.038940, current_train_items 286848.
I0304 19:32:21.362372 23128000471168 run.py:483] Algo bellman_ford step 8964 current loss 0.032927, current_train_items 286880.
I0304 19:32:21.382592 23128000471168 run.py:483] Algo bellman_ford step 8965 current loss 0.003037, current_train_items 286912.
I0304 19:32:21.399233 23128000471168 run.py:483] Algo bellman_ford step 8966 current loss 0.013946, current_train_items 286944.
I0304 19:32:21.423577 23128000471168 run.py:483] Algo bellman_ford step 8967 current loss 0.037862, current_train_items 286976.
I0304 19:32:21.456042 23128000471168 run.py:483] Algo bellman_ford step 8968 current loss 0.038122, current_train_items 287008.
I0304 19:32:21.490071 23128000471168 run.py:483] Algo bellman_ford step 8969 current loss 0.044549, current_train_items 287040.
I0304 19:32:21.510237 23128000471168 run.py:483] Algo bellman_ford step 8970 current loss 0.001456, current_train_items 287072.
I0304 19:32:21.526738 23128000471168 run.py:483] Algo bellman_ford step 8971 current loss 0.004894, current_train_items 287104.
I0304 19:32:21.551424 23128000471168 run.py:483] Algo bellman_ford step 8972 current loss 0.045933, current_train_items 287136.
I0304 19:32:21.582967 23128000471168 run.py:483] Algo bellman_ford step 8973 current loss 0.017513, current_train_items 287168.
I0304 19:32:21.616220 23128000471168 run.py:483] Algo bellman_ford step 8974 current loss 0.049123, current_train_items 287200.
I0304 19:32:21.636464 23128000471168 run.py:483] Algo bellman_ford step 8975 current loss 0.032266, current_train_items 287232.
I0304 19:32:21.653092 23128000471168 run.py:483] Algo bellman_ford step 8976 current loss 0.007522, current_train_items 287264.
I0304 19:32:21.676353 23128000471168 run.py:483] Algo bellman_ford step 8977 current loss 0.010398, current_train_items 287296.
I0304 19:32:21.708872 23128000471168 run.py:483] Algo bellman_ford step 8978 current loss 0.015885, current_train_items 287328.
I0304 19:32:21.742507 23128000471168 run.py:483] Algo bellman_ford step 8979 current loss 0.036692, current_train_items 287360.
I0304 19:32:21.762609 23128000471168 run.py:483] Algo bellman_ford step 8980 current loss 0.001088, current_train_items 287392.
I0304 19:32:21.779136 23128000471168 run.py:483] Algo bellman_ford step 8981 current loss 0.010231, current_train_items 287424.
I0304 19:32:21.803437 23128000471168 run.py:483] Algo bellman_ford step 8982 current loss 0.020417, current_train_items 287456.
I0304 19:32:21.835956 23128000471168 run.py:483] Algo bellman_ford step 8983 current loss 0.021256, current_train_items 287488.
I0304 19:32:21.870273 23128000471168 run.py:483] Algo bellman_ford step 8984 current loss 0.047935, current_train_items 287520.
I0304 19:32:21.890693 23128000471168 run.py:483] Algo bellman_ford step 8985 current loss 0.040974, current_train_items 287552.
I0304 19:32:21.907279 23128000471168 run.py:483] Algo bellman_ford step 8986 current loss 0.003286, current_train_items 287584.
I0304 19:32:21.930554 23128000471168 run.py:483] Algo bellman_ford step 8987 current loss 0.019949, current_train_items 287616.
I0304 19:32:21.961284 23128000471168 run.py:483] Algo bellman_ford step 8988 current loss 0.023359, current_train_items 287648.
I0304 19:32:21.997074 23128000471168 run.py:483] Algo bellman_ford step 8989 current loss 0.054182, current_train_items 287680.
I0304 19:32:22.017240 23128000471168 run.py:483] Algo bellman_ford step 8990 current loss 0.014871, current_train_items 287712.
I0304 19:32:22.033518 23128000471168 run.py:483] Algo bellman_ford step 8991 current loss 0.019590, current_train_items 287744.
I0304 19:32:22.057506 23128000471168 run.py:483] Algo bellman_ford step 8992 current loss 0.040153, current_train_items 287776.
I0304 19:32:22.089642 23128000471168 run.py:483] Algo bellman_ford step 8993 current loss 0.041292, current_train_items 287808.
I0304 19:32:22.123394 23128000471168 run.py:483] Algo bellman_ford step 8994 current loss 0.073506, current_train_items 287840.
I0304 19:32:22.142991 23128000471168 run.py:483] Algo bellman_ford step 8995 current loss 0.003959, current_train_items 287872.
I0304 19:32:22.159518 23128000471168 run.py:483] Algo bellman_ford step 8996 current loss 0.038601, current_train_items 287904.
I0304 19:32:22.184358 23128000471168 run.py:483] Algo bellman_ford step 8997 current loss 0.047455, current_train_items 287936.
I0304 19:32:22.218033 23128000471168 run.py:483] Algo bellman_ford step 8998 current loss 0.044483, current_train_items 287968.
I0304 19:32:22.251175 23128000471168 run.py:483] Algo bellman_ford step 8999 current loss 0.056803, current_train_items 288000.
I0304 19:32:22.271462 23128000471168 run.py:483] Algo bellman_ford step 9000 current loss 0.002976, current_train_items 288032.
I0304 19:32:22.279131 23128000471168 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.998046875, 'score': 0.998046875, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0304 19:32:22.279242 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.998, val scores are: bellman_ford: 0.998
I0304 19:32:22.296622 23128000471168 run.py:483] Algo bellman_ford step 9001 current loss 0.021200, current_train_items 288064.
I0304 19:32:22.321137 23128000471168 run.py:483] Algo bellman_ford step 9002 current loss 0.024933, current_train_items 288096.
I0304 19:32:22.353478 23128000471168 run.py:483] Algo bellman_ford step 9003 current loss 0.078634, current_train_items 288128.
I0304 19:32:22.388237 23128000471168 run.py:483] Algo bellman_ford step 9004 current loss 0.067433, current_train_items 288160.
I0304 19:32:22.408962 23128000471168 run.py:483] Algo bellman_ford step 9005 current loss 0.007211, current_train_items 288192.
I0304 19:32:22.425338 23128000471168 run.py:483] Algo bellman_ford step 9006 current loss 0.018035, current_train_items 288224.
I0304 19:32:22.449023 23128000471168 run.py:483] Algo bellman_ford step 9007 current loss 0.021025, current_train_items 288256.
I0304 19:32:22.482455 23128000471168 run.py:483] Algo bellman_ford step 9008 current loss 0.035067, current_train_items 288288.
I0304 19:32:22.517328 23128000471168 run.py:483] Algo bellman_ford step 9009 current loss 0.043488, current_train_items 288320.
I0304 19:32:22.537765 23128000471168 run.py:483] Algo bellman_ford step 9010 current loss 0.046302, current_train_items 288352.
I0304 19:32:22.554318 23128000471168 run.py:483] Algo bellman_ford step 9011 current loss 0.028116, current_train_items 288384.
I0304 19:32:22.578397 23128000471168 run.py:483] Algo bellman_ford step 9012 current loss 0.012672, current_train_items 288416.
I0304 19:32:22.610224 23128000471168 run.py:483] Algo bellman_ford step 9013 current loss 0.035917, current_train_items 288448.
I0304 19:32:22.646467 23128000471168 run.py:483] Algo bellman_ford step 9014 current loss 0.053176, current_train_items 288480.
I0304 19:32:22.666593 23128000471168 run.py:483] Algo bellman_ford step 9015 current loss 0.002386, current_train_items 288512.
I0304 19:32:22.683580 23128000471168 run.py:483] Algo bellman_ford step 9016 current loss 0.040545, current_train_items 288544.
I0304 19:32:22.708471 23128000471168 run.py:483] Algo bellman_ford step 9017 current loss 0.032237, current_train_items 288576.
I0304 19:32:22.741304 23128000471168 run.py:483] Algo bellman_ford step 9018 current loss 0.053204, current_train_items 288608.
I0304 19:32:22.776711 23128000471168 run.py:483] Algo bellman_ford step 9019 current loss 0.094725, current_train_items 288640.
I0304 19:32:22.796910 23128000471168 run.py:483] Algo bellman_ford step 9020 current loss 0.002066, current_train_items 288672.
I0304 19:32:22.813407 23128000471168 run.py:483] Algo bellman_ford step 9021 current loss 0.006739, current_train_items 288704.
I0304 19:32:22.839325 23128000471168 run.py:483] Algo bellman_ford step 9022 current loss 0.038363, current_train_items 288736.
I0304 19:32:22.872970 23128000471168 run.py:483] Algo bellman_ford step 9023 current loss 0.047145, current_train_items 288768.
I0304 19:32:22.907472 23128000471168 run.py:483] Algo bellman_ford step 9024 current loss 0.048057, current_train_items 288800.
I0304 19:32:22.927495 23128000471168 run.py:483] Algo bellman_ford step 9025 current loss 0.001563, current_train_items 288832.
I0304 19:32:22.943623 23128000471168 run.py:483] Algo bellman_ford step 9026 current loss 0.009133, current_train_items 288864.
I0304 19:32:22.968765 23128000471168 run.py:483] Algo bellman_ford step 9027 current loss 0.046890, current_train_items 288896.
I0304 19:32:23.001975 23128000471168 run.py:483] Algo bellman_ford step 9028 current loss 0.038948, current_train_items 288928.
I0304 19:32:23.036805 23128000471168 run.py:483] Algo bellman_ford step 9029 current loss 0.063193, current_train_items 288960.
I0304 19:32:23.057100 23128000471168 run.py:483] Algo bellman_ford step 9030 current loss 0.003805, current_train_items 288992.
I0304 19:32:23.073577 23128000471168 run.py:483] Algo bellman_ford step 9031 current loss 0.031439, current_train_items 289024.
I0304 19:32:23.098812 23128000471168 run.py:483] Algo bellman_ford step 9032 current loss 0.056570, current_train_items 289056.
I0304 19:32:23.131753 23128000471168 run.py:483] Algo bellman_ford step 9033 current loss 0.036362, current_train_items 289088.
I0304 19:32:23.168287 23128000471168 run.py:483] Algo bellman_ford step 9034 current loss 0.110292, current_train_items 289120.
I0304 19:32:23.188410 23128000471168 run.py:483] Algo bellman_ford step 9035 current loss 0.001818, current_train_items 289152.
I0304 19:32:23.204597 23128000471168 run.py:483] Algo bellman_ford step 9036 current loss 0.040375, current_train_items 289184.
I0304 19:32:23.230248 23128000471168 run.py:483] Algo bellman_ford step 9037 current loss 0.054036, current_train_items 289216.
I0304 19:32:23.263140 23128000471168 run.py:483] Algo bellman_ford step 9038 current loss 0.040394, current_train_items 289248.
I0304 19:32:23.297318 23128000471168 run.py:483] Algo bellman_ford step 9039 current loss 0.054556, current_train_items 289280.
I0304 19:32:23.317312 23128000471168 run.py:483] Algo bellman_ford step 9040 current loss 0.005005, current_train_items 289312.
I0304 19:32:23.334136 23128000471168 run.py:483] Algo bellman_ford step 9041 current loss 0.010290, current_train_items 289344.
I0304 19:32:23.358492 23128000471168 run.py:483] Algo bellman_ford step 9042 current loss 0.013363, current_train_items 289376.
I0304 19:32:23.391036 23128000471168 run.py:483] Algo bellman_ford step 9043 current loss 0.054903, current_train_items 289408.
I0304 19:32:23.425672 23128000471168 run.py:483] Algo bellman_ford step 9044 current loss 0.035542, current_train_items 289440.
I0304 19:32:23.446273 23128000471168 run.py:483] Algo bellman_ford step 9045 current loss 0.003210, current_train_items 289472.
I0304 19:32:23.463063 23128000471168 run.py:483] Algo bellman_ford step 9046 current loss 0.015302, current_train_items 289504.
I0304 19:32:23.486602 23128000471168 run.py:483] Algo bellman_ford step 9047 current loss 0.042678, current_train_items 289536.
I0304 19:32:23.519218 23128000471168 run.py:483] Algo bellman_ford step 9048 current loss 0.067055, current_train_items 289568.
I0304 19:32:23.553766 23128000471168 run.py:483] Algo bellman_ford step 9049 current loss 0.077902, current_train_items 289600.
I0304 19:32:23.573645 23128000471168 run.py:483] Algo bellman_ford step 9050 current loss 0.002085, current_train_items 289632.
I0304 19:32:23.581959 23128000471168 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0304 19:32:23.582078 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:23.599094 23128000471168 run.py:483] Algo bellman_ford step 9051 current loss 0.015634, current_train_items 289664.
I0304 19:32:23.624678 23128000471168 run.py:483] Algo bellman_ford step 9052 current loss 0.032354, current_train_items 289696.
I0304 19:32:23.658341 23128000471168 run.py:483] Algo bellman_ford step 9053 current loss 0.038566, current_train_items 289728.
I0304 19:32:23.691770 23128000471168 run.py:483] Algo bellman_ford step 9054 current loss 0.067975, current_train_items 289760.
I0304 19:32:23.712391 23128000471168 run.py:483] Algo bellman_ford step 9055 current loss 0.001404, current_train_items 289792.
I0304 19:32:23.728540 23128000471168 run.py:483] Algo bellman_ford step 9056 current loss 0.041244, current_train_items 289824.
I0304 19:32:23.752860 23128000471168 run.py:483] Algo bellman_ford step 9057 current loss 0.059225, current_train_items 289856.
I0304 19:32:23.784048 23128000471168 run.py:483] Algo bellman_ford step 9058 current loss 0.076529, current_train_items 289888.
I0304 19:32:23.816679 23128000471168 run.py:483] Algo bellman_ford step 9059 current loss 0.039669, current_train_items 289920.
I0304 19:32:23.837147 23128000471168 run.py:483] Algo bellman_ford step 9060 current loss 0.007239, current_train_items 289952.
I0304 19:32:23.854242 23128000471168 run.py:483] Algo bellman_ford step 9061 current loss 0.008244, current_train_items 289984.
I0304 19:32:23.879785 23128000471168 run.py:483] Algo bellman_ford step 9062 current loss 0.045768, current_train_items 290016.
I0304 19:32:23.911343 23128000471168 run.py:483] Algo bellman_ford step 9063 current loss 0.018738, current_train_items 290048.
I0304 19:32:23.946399 23128000471168 run.py:483] Algo bellman_ford step 9064 current loss 0.079027, current_train_items 290080.
I0304 19:32:23.966608 23128000471168 run.py:483] Algo bellman_ford step 9065 current loss 0.002113, current_train_items 290112.
I0304 19:32:23.983182 23128000471168 run.py:483] Algo bellman_ford step 9066 current loss 0.010360, current_train_items 290144.
I0304 19:32:24.006151 23128000471168 run.py:483] Algo bellman_ford step 9067 current loss 0.048137, current_train_items 290176.
I0304 19:32:24.038887 23128000471168 run.py:483] Algo bellman_ford step 9068 current loss 0.037692, current_train_items 290208.
I0304 19:32:24.073634 23128000471168 run.py:483] Algo bellman_ford step 9069 current loss 0.066483, current_train_items 290240.
I0304 19:32:24.094044 23128000471168 run.py:483] Algo bellman_ford step 9070 current loss 0.001345, current_train_items 290272.
I0304 19:32:24.110968 23128000471168 run.py:483] Algo bellman_ford step 9071 current loss 0.028232, current_train_items 290304.
I0304 19:32:24.135957 23128000471168 run.py:483] Algo bellman_ford step 9072 current loss 0.048426, current_train_items 290336.
I0304 19:32:24.169634 23128000471168 run.py:483] Algo bellman_ford step 9073 current loss 0.040508, current_train_items 290368.
I0304 19:32:24.203722 23128000471168 run.py:483] Algo bellman_ford step 9074 current loss 0.045182, current_train_items 290400.
I0304 19:32:24.223939 23128000471168 run.py:483] Algo bellman_ford step 9075 current loss 0.005064, current_train_items 290432.
I0304 19:32:24.240618 23128000471168 run.py:483] Algo bellman_ford step 9076 current loss 0.024979, current_train_items 290464.
I0304 19:32:24.265122 23128000471168 run.py:483] Algo bellman_ford step 9077 current loss 0.043606, current_train_items 290496.
I0304 19:32:24.297684 23128000471168 run.py:483] Algo bellman_ford step 9078 current loss 0.028197, current_train_items 290528.
I0304 19:32:24.332659 23128000471168 run.py:483] Algo bellman_ford step 9079 current loss 0.051741, current_train_items 290560.
I0304 19:32:24.353022 23128000471168 run.py:483] Algo bellman_ford step 9080 current loss 0.001259, current_train_items 290592.
I0304 19:32:24.369226 23128000471168 run.py:483] Algo bellman_ford step 9081 current loss 0.023631, current_train_items 290624.
I0304 19:32:24.393564 23128000471168 run.py:483] Algo bellman_ford step 9082 current loss 0.015568, current_train_items 290656.
I0304 19:32:24.425855 23128000471168 run.py:483] Algo bellman_ford step 9083 current loss 0.032898, current_train_items 290688.
I0304 19:32:24.459334 23128000471168 run.py:483] Algo bellman_ford step 9084 current loss 0.039591, current_train_items 290720.
I0304 19:32:24.479761 23128000471168 run.py:483] Algo bellman_ford step 9085 current loss 0.001233, current_train_items 290752.
I0304 19:32:24.496341 23128000471168 run.py:483] Algo bellman_ford step 9086 current loss 0.026196, current_train_items 290784.
I0304 19:32:24.521208 23128000471168 run.py:483] Algo bellman_ford step 9087 current loss 0.028404, current_train_items 290816.
I0304 19:32:24.553170 23128000471168 run.py:483] Algo bellman_ford step 9088 current loss 0.020952, current_train_items 290848.
I0304 19:32:24.589107 23128000471168 run.py:483] Algo bellman_ford step 9089 current loss 0.070024, current_train_items 290880.
I0304 19:32:24.609505 23128000471168 run.py:483] Algo bellman_ford step 9090 current loss 0.001225, current_train_items 290912.
I0304 19:32:24.626021 23128000471168 run.py:483] Algo bellman_ford step 9091 current loss 0.023951, current_train_items 290944.
I0304 19:32:24.650448 23128000471168 run.py:483] Algo bellman_ford step 9092 current loss 0.032588, current_train_items 290976.
I0304 19:32:24.681015 23128000471168 run.py:483] Algo bellman_ford step 9093 current loss 0.036690, current_train_items 291008.
I0304 19:32:24.717290 23128000471168 run.py:483] Algo bellman_ford step 9094 current loss 0.107926, current_train_items 291040.
I0304 19:32:24.737597 23128000471168 run.py:483] Algo bellman_ford step 9095 current loss 0.004014, current_train_items 291072.
I0304 19:32:24.754226 23128000471168 run.py:483] Algo bellman_ford step 9096 current loss 0.016221, current_train_items 291104.
I0304 19:32:24.778829 23128000471168 run.py:483] Algo bellman_ford step 9097 current loss 0.038954, current_train_items 291136.
I0304 19:32:24.811503 23128000471168 run.py:483] Algo bellman_ford step 9098 current loss 0.044565, current_train_items 291168.
I0304 19:32:24.845227 23128000471168 run.py:483] Algo bellman_ford step 9099 current loss 0.036845, current_train_items 291200.
I0304 19:32:24.865413 23128000471168 run.py:483] Algo bellman_ford step 9100 current loss 0.001333, current_train_items 291232.
I0304 19:32:24.873125 23128000471168 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0304 19:32:24.873234 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:24.890627 23128000471168 run.py:483] Algo bellman_ford step 9101 current loss 0.003573, current_train_items 291264.
I0304 19:32:24.916270 23128000471168 run.py:483] Algo bellman_ford step 9102 current loss 0.082335, current_train_items 291296.
I0304 19:32:24.949215 23128000471168 run.py:483] Algo bellman_ford step 9103 current loss 0.022199, current_train_items 291328.
I0304 19:32:24.984989 23128000471168 run.py:483] Algo bellman_ford step 9104 current loss 0.043449, current_train_items 291360.
I0304 19:32:25.005269 23128000471168 run.py:483] Algo bellman_ford step 9105 current loss 0.015803, current_train_items 291392.
I0304 19:32:25.021745 23128000471168 run.py:483] Algo bellman_ford step 9106 current loss 0.012234, current_train_items 291424.
I0304 19:32:25.046709 23128000471168 run.py:483] Algo bellman_ford step 9107 current loss 0.039548, current_train_items 291456.
I0304 19:32:25.079377 23128000471168 run.py:483] Algo bellman_ford step 9108 current loss 0.065037, current_train_items 291488.
I0304 19:32:25.113636 23128000471168 run.py:483] Algo bellman_ford step 9109 current loss 0.079642, current_train_items 291520.
I0304 19:32:25.133565 23128000471168 run.py:483] Algo bellman_ford step 9110 current loss 0.001687, current_train_items 291552.
I0304 19:32:25.150247 23128000471168 run.py:483] Algo bellman_ford step 9111 current loss 0.019453, current_train_items 291584.
I0304 19:32:25.174146 23128000471168 run.py:483] Algo bellman_ford step 9112 current loss 0.057891, current_train_items 291616.
I0304 19:32:25.207807 23128000471168 run.py:483] Algo bellman_ford step 9113 current loss 0.042015, current_train_items 291648.
I0304 19:32:25.243196 23128000471168 run.py:483] Algo bellman_ford step 9114 current loss 0.075389, current_train_items 291680.
I0304 19:32:25.263246 23128000471168 run.py:483] Algo bellman_ford step 9115 current loss 0.055130, current_train_items 291712.
I0304 19:32:25.280043 23128000471168 run.py:483] Algo bellman_ford step 9116 current loss 0.004873, current_train_items 291744.
I0304 19:32:25.304528 23128000471168 run.py:483] Algo bellman_ford step 9117 current loss 0.034400, current_train_items 291776.
I0304 19:32:25.336360 23128000471168 run.py:483] Algo bellman_ford step 9118 current loss 0.026727, current_train_items 291808.
I0304 19:32:25.371743 23128000471168 run.py:483] Algo bellman_ford step 9119 current loss 0.066836, current_train_items 291840.
I0304 19:32:25.391556 23128000471168 run.py:483] Algo bellman_ford step 9120 current loss 0.004226, current_train_items 291872.
I0304 19:32:25.408149 23128000471168 run.py:483] Algo bellman_ford step 9121 current loss 0.015268, current_train_items 291904.
I0304 19:32:25.432475 23128000471168 run.py:483] Algo bellman_ford step 9122 current loss 0.035875, current_train_items 291936.
I0304 19:32:25.466615 23128000471168 run.py:483] Algo bellman_ford step 9123 current loss 0.065587, current_train_items 291968.
I0304 19:32:25.502423 23128000471168 run.py:483] Algo bellman_ford step 9124 current loss 0.051367, current_train_items 292000.
I0304 19:32:25.522294 23128000471168 run.py:483] Algo bellman_ford step 9125 current loss 0.006106, current_train_items 292032.
I0304 19:32:25.538460 23128000471168 run.py:483] Algo bellman_ford step 9126 current loss 0.024142, current_train_items 292064.
I0304 19:32:25.561905 23128000471168 run.py:483] Algo bellman_ford step 9127 current loss 0.045276, current_train_items 292096.
I0304 19:32:25.595436 23128000471168 run.py:483] Algo bellman_ford step 9128 current loss 0.054412, current_train_items 292128.
I0304 19:32:25.628823 23128000471168 run.py:483] Algo bellman_ford step 9129 current loss 0.111623, current_train_items 292160.
I0304 19:32:25.648833 23128000471168 run.py:483] Algo bellman_ford step 9130 current loss 0.002770, current_train_items 292192.
I0304 19:32:25.665201 23128000471168 run.py:483] Algo bellman_ford step 9131 current loss 0.005484, current_train_items 292224.
I0304 19:32:25.690514 23128000471168 run.py:483] Algo bellman_ford step 9132 current loss 0.096374, current_train_items 292256.
I0304 19:32:25.723340 23128000471168 run.py:483] Algo bellman_ford step 9133 current loss 0.127518, current_train_items 292288.
I0304 19:32:25.756363 23128000471168 run.py:483] Algo bellman_ford step 9134 current loss 0.049785, current_train_items 292320.
I0304 19:32:25.776236 23128000471168 run.py:483] Algo bellman_ford step 9135 current loss 0.001411, current_train_items 292352.
I0304 19:32:25.792248 23128000471168 run.py:483] Algo bellman_ford step 9136 current loss 0.003137, current_train_items 292384.
I0304 19:32:25.815851 23128000471168 run.py:483] Algo bellman_ford step 9137 current loss 0.018659, current_train_items 292416.
I0304 19:32:25.847988 23128000471168 run.py:483] Algo bellman_ford step 9138 current loss 0.024830, current_train_items 292448.
I0304 19:32:25.880898 23128000471168 run.py:483] Algo bellman_ford step 9139 current loss 0.043123, current_train_items 292480.
I0304 19:32:25.900915 23128000471168 run.py:483] Algo bellman_ford step 9140 current loss 0.001370, current_train_items 292512.
I0304 19:32:25.917290 23128000471168 run.py:483] Algo bellman_ford step 9141 current loss 0.004897, current_train_items 292544.
I0304 19:32:25.941400 23128000471168 run.py:483] Algo bellman_ford step 9142 current loss 0.015186, current_train_items 292576.
I0304 19:32:25.973296 23128000471168 run.py:483] Algo bellman_ford step 9143 current loss 0.035635, current_train_items 292608.
I0304 19:32:26.008535 23128000471168 run.py:483] Algo bellman_ford step 9144 current loss 0.067754, current_train_items 292640.
I0304 19:32:26.028195 23128000471168 run.py:483] Algo bellman_ford step 9145 current loss 0.002380, current_train_items 292672.
I0304 19:32:26.044699 23128000471168 run.py:483] Algo bellman_ford step 9146 current loss 0.023451, current_train_items 292704.
I0304 19:32:26.069170 23128000471168 run.py:483] Algo bellman_ford step 9147 current loss 0.024576, current_train_items 292736.
I0304 19:32:26.101517 23128000471168 run.py:483] Algo bellman_ford step 9148 current loss 0.048603, current_train_items 292768.
I0304 19:32:26.132366 23128000471168 run.py:483] Algo bellman_ford step 9149 current loss 0.036046, current_train_items 292800.
I0304 19:32:26.152302 23128000471168 run.py:483] Algo bellman_ford step 9150 current loss 0.001888, current_train_items 292832.
I0304 19:32:26.160474 23128000471168 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0304 19:32:26.160585 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:26.177832 23128000471168 run.py:483] Algo bellman_ford step 9151 current loss 0.005915, current_train_items 292864.
I0304 19:32:26.202736 23128000471168 run.py:483] Algo bellman_ford step 9152 current loss 0.034683, current_train_items 292896.
I0304 19:32:26.236078 23128000471168 run.py:483] Algo bellman_ford step 9153 current loss 0.039376, current_train_items 292928.
I0304 19:32:26.269057 23128000471168 run.py:483] Algo bellman_ford step 9154 current loss 0.062023, current_train_items 292960.
I0304 19:32:26.289568 23128000471168 run.py:483] Algo bellman_ford step 9155 current loss 0.002528, current_train_items 292992.
I0304 19:32:26.306478 23128000471168 run.py:483] Algo bellman_ford step 9156 current loss 0.011506, current_train_items 293024.
I0304 19:32:26.331963 23128000471168 run.py:483] Algo bellman_ford step 9157 current loss 0.040289, current_train_items 293056.
I0304 19:32:26.363828 23128000471168 run.py:483] Algo bellman_ford step 9158 current loss 0.058024, current_train_items 293088.
I0304 19:32:26.399016 23128000471168 run.py:483] Algo bellman_ford step 9159 current loss 0.070895, current_train_items 293120.
I0304 19:32:26.419559 23128000471168 run.py:483] Algo bellman_ford step 9160 current loss 0.004028, current_train_items 293152.
I0304 19:32:26.436364 23128000471168 run.py:483] Algo bellman_ford step 9161 current loss 0.009649, current_train_items 293184.
I0304 19:32:26.460654 23128000471168 run.py:483] Algo bellman_ford step 9162 current loss 0.038973, current_train_items 293216.
I0304 19:32:26.492856 23128000471168 run.py:483] Algo bellman_ford step 9163 current loss 0.033544, current_train_items 293248.
I0304 19:32:26.528874 23128000471168 run.py:483] Algo bellman_ford step 9164 current loss 0.045201, current_train_items 293280.
I0304 19:32:26.549155 23128000471168 run.py:483] Algo bellman_ford step 9165 current loss 0.001790, current_train_items 293312.
I0304 19:32:26.566001 23128000471168 run.py:483] Algo bellman_ford step 9166 current loss 0.008817, current_train_items 293344.
I0304 19:32:26.590708 23128000471168 run.py:483] Algo bellman_ford step 9167 current loss 0.040678, current_train_items 293376.
I0304 19:32:26.623771 23128000471168 run.py:483] Algo bellman_ford step 9168 current loss 0.046959, current_train_items 293408.
I0304 19:32:26.657165 23128000471168 run.py:483] Algo bellman_ford step 9169 current loss 0.038546, current_train_items 293440.
I0304 19:32:26.677601 23128000471168 run.py:483] Algo bellman_ford step 9170 current loss 0.001644, current_train_items 293472.
I0304 19:32:26.694296 23128000471168 run.py:483] Algo bellman_ford step 9171 current loss 0.009324, current_train_items 293504.
I0304 19:32:26.720299 23128000471168 run.py:483] Algo bellman_ford step 9172 current loss 0.101817, current_train_items 293536.
I0304 19:32:26.753184 23128000471168 run.py:483] Algo bellman_ford step 9173 current loss 0.083556, current_train_items 293568.
I0304 19:32:26.788789 23128000471168 run.py:483] Algo bellman_ford step 9174 current loss 0.077677, current_train_items 293600.
I0304 19:32:26.809139 23128000471168 run.py:483] Algo bellman_ford step 9175 current loss 0.010461, current_train_items 293632.
I0304 19:32:26.826069 23128000471168 run.py:483] Algo bellman_ford step 9176 current loss 0.006295, current_train_items 293664.
I0304 19:32:26.849320 23128000471168 run.py:483] Algo bellman_ford step 9177 current loss 0.037012, current_train_items 293696.
I0304 19:32:26.880625 23128000471168 run.py:483] Algo bellman_ford step 9178 current loss 0.060220, current_train_items 293728.
I0304 19:32:26.916358 23128000471168 run.py:483] Algo bellman_ford step 9179 current loss 0.107067, current_train_items 293760.
I0304 19:32:26.936528 23128000471168 run.py:483] Algo bellman_ford step 9180 current loss 0.001646, current_train_items 293792.
I0304 19:32:26.953131 23128000471168 run.py:483] Algo bellman_ford step 9181 current loss 0.045980, current_train_items 293824.
I0304 19:32:26.978689 23128000471168 run.py:483] Algo bellman_ford step 9182 current loss 0.052426, current_train_items 293856.
I0304 19:32:27.010221 23128000471168 run.py:483] Algo bellman_ford step 9183 current loss 0.055687, current_train_items 293888.
I0304 19:32:27.043849 23128000471168 run.py:483] Algo bellman_ford step 9184 current loss 0.071801, current_train_items 293920.
I0304 19:32:27.064507 23128000471168 run.py:483] Algo bellman_ford step 9185 current loss 0.017778, current_train_items 293952.
I0304 19:32:27.081382 23128000471168 run.py:483] Algo bellman_ford step 9186 current loss 0.009478, current_train_items 293984.
I0304 19:32:27.106149 23128000471168 run.py:483] Algo bellman_ford step 9187 current loss 0.011897, current_train_items 294016.
I0304 19:32:27.139808 23128000471168 run.py:483] Algo bellman_ford step 9188 current loss 0.035098, current_train_items 294048.
I0304 19:32:27.176619 23128000471168 run.py:483] Algo bellman_ford step 9189 current loss 0.037640, current_train_items 294080.
I0304 19:32:27.196991 23128000471168 run.py:483] Algo bellman_ford step 9190 current loss 0.002032, current_train_items 294112.
I0304 19:32:27.213463 23128000471168 run.py:483] Algo bellman_ford step 9191 current loss 0.030775, current_train_items 294144.
I0304 19:32:27.238349 23128000471168 run.py:483] Algo bellman_ford step 9192 current loss 0.022409, current_train_items 294176.
I0304 19:32:27.270927 23128000471168 run.py:483] Algo bellman_ford step 9193 current loss 0.045430, current_train_items 294208.
I0304 19:32:27.304513 23128000471168 run.py:483] Algo bellman_ford step 9194 current loss 0.031988, current_train_items 294240.
I0304 19:32:27.324693 23128000471168 run.py:483] Algo bellman_ford step 9195 current loss 0.017873, current_train_items 294272.
I0304 19:32:27.341678 23128000471168 run.py:483] Algo bellman_ford step 9196 current loss 0.012744, current_train_items 294304.
I0304 19:32:27.366401 23128000471168 run.py:483] Algo bellman_ford step 9197 current loss 0.045384, current_train_items 294336.
I0304 19:32:27.398296 23128000471168 run.py:483] Algo bellman_ford step 9198 current loss 0.017542, current_train_items 294368.
I0304 19:32:27.431391 23128000471168 run.py:483] Algo bellman_ford step 9199 current loss 0.059502, current_train_items 294400.
I0304 19:32:27.452218 23128000471168 run.py:483] Algo bellman_ford step 9200 current loss 0.002363, current_train_items 294432.
I0304 19:32:27.459886 23128000471168 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0304 19:32:27.459996 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:27.477142 23128000471168 run.py:483] Algo bellman_ford step 9201 current loss 0.011334, current_train_items 294464.
I0304 19:32:27.502080 23128000471168 run.py:483] Algo bellman_ford step 9202 current loss 0.060997, current_train_items 294496.
I0304 19:32:27.536231 23128000471168 run.py:483] Algo bellman_ford step 9203 current loss 0.038901, current_train_items 294528.
I0304 19:32:27.571549 23128000471168 run.py:483] Algo bellman_ford step 9204 current loss 0.050976, current_train_items 294560.
I0304 19:32:27.591996 23128000471168 run.py:483] Algo bellman_ford step 9205 current loss 0.015130, current_train_items 294592.
I0304 19:32:27.608664 23128000471168 run.py:483] Algo bellman_ford step 9206 current loss 0.018244, current_train_items 294624.
I0304 19:32:27.633115 23128000471168 run.py:483] Algo bellman_ford step 9207 current loss 0.028800, current_train_items 294656.
I0304 19:32:27.665703 23128000471168 run.py:483] Algo bellman_ford step 9208 current loss 0.093761, current_train_items 294688.
I0304 19:32:27.697682 23128000471168 run.py:483] Algo bellman_ford step 9209 current loss 0.035037, current_train_items 294720.
I0304 19:32:27.717885 23128000471168 run.py:483] Algo bellman_ford step 9210 current loss 0.001767, current_train_items 294752.
I0304 19:32:27.734836 23128000471168 run.py:483] Algo bellman_ford step 9211 current loss 0.016643, current_train_items 294784.
I0304 19:32:27.759026 23128000471168 run.py:483] Algo bellman_ford step 9212 current loss 0.051032, current_train_items 294816.
I0304 19:32:27.791141 23128000471168 run.py:483] Algo bellman_ford step 9213 current loss 0.084777, current_train_items 294848.
I0304 19:32:27.824141 23128000471168 run.py:483] Algo bellman_ford step 9214 current loss 0.094960, current_train_items 294880.
I0304 19:32:27.844504 23128000471168 run.py:483] Algo bellman_ford step 9215 current loss 0.023925, current_train_items 294912.
I0304 19:32:27.861201 23128000471168 run.py:483] Algo bellman_ford step 9216 current loss 0.013355, current_train_items 294944.
I0304 19:32:27.885237 23128000471168 run.py:483] Algo bellman_ford step 9217 current loss 0.022222, current_train_items 294976.
I0304 19:32:27.917255 23128000471168 run.py:483] Algo bellman_ford step 9218 current loss 0.047841, current_train_items 295008.
I0304 19:32:27.951290 23128000471168 run.py:483] Algo bellman_ford step 9219 current loss 0.061294, current_train_items 295040.
I0304 19:32:27.971399 23128000471168 run.py:483] Algo bellman_ford step 9220 current loss 0.025947, current_train_items 295072.
I0304 19:32:27.987713 23128000471168 run.py:483] Algo bellman_ford step 9221 current loss 0.049208, current_train_items 295104.
I0304 19:32:28.012603 23128000471168 run.py:483] Algo bellman_ford step 9222 current loss 0.087702, current_train_items 295136.
I0304 19:32:28.044373 23128000471168 run.py:483] Algo bellman_ford step 9223 current loss 0.028976, current_train_items 295168.
I0304 19:32:28.078500 23128000471168 run.py:483] Algo bellman_ford step 9224 current loss 0.036801, current_train_items 295200.
I0304 19:32:28.098478 23128000471168 run.py:483] Algo bellman_ford step 9225 current loss 0.002307, current_train_items 295232.
I0304 19:32:28.115062 23128000471168 run.py:483] Algo bellman_ford step 9226 current loss 0.005526, current_train_items 295264.
I0304 19:32:28.139727 23128000471168 run.py:483] Algo bellman_ford step 9227 current loss 0.041236, current_train_items 295296.
I0304 19:32:28.172093 23128000471168 run.py:483] Algo bellman_ford step 9228 current loss 0.037143, current_train_items 295328.
I0304 19:32:28.203662 23128000471168 run.py:483] Algo bellman_ford step 9229 current loss 0.025025, current_train_items 295360.
I0304 19:32:28.224156 23128000471168 run.py:483] Algo bellman_ford step 9230 current loss 0.002229, current_train_items 295392.
I0304 19:32:28.240963 23128000471168 run.py:483] Algo bellman_ford step 9231 current loss 0.021103, current_train_items 295424.
I0304 19:32:28.265985 23128000471168 run.py:483] Algo bellman_ford step 9232 current loss 0.017608, current_train_items 295456.
I0304 19:32:28.300352 23128000471168 run.py:483] Algo bellman_ford step 9233 current loss 0.055814, current_train_items 295488.
I0304 19:32:28.334953 23128000471168 run.py:483] Algo bellman_ford step 9234 current loss 0.040291, current_train_items 295520.
I0304 19:32:28.354476 23128000471168 run.py:483] Algo bellman_ford step 9235 current loss 0.001679, current_train_items 295552.
I0304 19:32:28.371144 23128000471168 run.py:483] Algo bellman_ford step 9236 current loss 0.022736, current_train_items 295584.
I0304 19:32:28.395643 23128000471168 run.py:483] Algo bellman_ford step 9237 current loss 0.017370, current_train_items 295616.
I0304 19:32:28.429308 23128000471168 run.py:483] Algo bellman_ford step 9238 current loss 0.034562, current_train_items 295648.
I0304 19:32:28.463863 23128000471168 run.py:483] Algo bellman_ford step 9239 current loss 0.047952, current_train_items 295680.
I0304 19:32:28.483941 23128000471168 run.py:483] Algo bellman_ford step 9240 current loss 0.001927, current_train_items 295712.
I0304 19:32:28.500939 23128000471168 run.py:483] Algo bellman_ford step 9241 current loss 0.018990, current_train_items 295744.
I0304 19:32:28.526103 23128000471168 run.py:483] Algo bellman_ford step 9242 current loss 0.033784, current_train_items 295776.
I0304 19:32:28.558508 23128000471168 run.py:483] Algo bellman_ford step 9243 current loss 0.033342, current_train_items 295808.
I0304 19:32:28.591540 23128000471168 run.py:483] Algo bellman_ford step 9244 current loss 0.054317, current_train_items 295840.
I0304 19:32:28.611573 23128000471168 run.py:483] Algo bellman_ford step 9245 current loss 0.002773, current_train_items 295872.
I0304 19:32:28.628117 23128000471168 run.py:483] Algo bellman_ford step 9246 current loss 0.010603, current_train_items 295904.
I0304 19:32:28.651756 23128000471168 run.py:483] Algo bellman_ford step 9247 current loss 0.011497, current_train_items 295936.
I0304 19:32:28.684550 23128000471168 run.py:483] Algo bellman_ford step 9248 current loss 0.032357, current_train_items 295968.
I0304 19:32:28.718994 23128000471168 run.py:483] Algo bellman_ford step 9249 current loss 0.047861, current_train_items 296000.
I0304 19:32:28.739423 23128000471168 run.py:483] Algo bellman_ford step 9250 current loss 0.003142, current_train_items 296032.
I0304 19:32:28.747694 23128000471168 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0304 19:32:28.747803 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:28.764829 23128000471168 run.py:483] Algo bellman_ford step 9251 current loss 0.015034, current_train_items 296064.
I0304 19:32:28.790128 23128000471168 run.py:483] Algo bellman_ford step 9252 current loss 0.033295, current_train_items 296096.
I0304 19:32:28.822520 23128000471168 run.py:483] Algo bellman_ford step 9253 current loss 0.019428, current_train_items 296128.
I0304 19:32:28.855122 23128000471168 run.py:483] Algo bellman_ford step 9254 current loss 0.028070, current_train_items 296160.
I0304 19:32:28.875420 23128000471168 run.py:483] Algo bellman_ford step 9255 current loss 0.001623, current_train_items 296192.
I0304 19:32:28.891564 23128000471168 run.py:483] Algo bellman_ford step 9256 current loss 0.024727, current_train_items 296224.
I0304 19:32:28.916135 23128000471168 run.py:483] Algo bellman_ford step 9257 current loss 0.024699, current_train_items 296256.
I0304 19:32:28.949253 23128000471168 run.py:483] Algo bellman_ford step 9258 current loss 0.041998, current_train_items 296288.
I0304 19:32:28.983415 23128000471168 run.py:483] Algo bellman_ford step 9259 current loss 0.033556, current_train_items 296320.
I0304 19:32:29.004221 23128000471168 run.py:483] Algo bellman_ford step 9260 current loss 0.003418, current_train_items 296352.
I0304 19:32:29.021628 23128000471168 run.py:483] Algo bellman_ford step 9261 current loss 0.015031, current_train_items 296384.
I0304 19:32:29.045029 23128000471168 run.py:483] Algo bellman_ford step 9262 current loss 0.024583, current_train_items 296416.
I0304 19:32:29.077392 23128000471168 run.py:483] Algo bellman_ford step 9263 current loss 0.060853, current_train_items 296448.
I0304 19:32:29.111863 23128000471168 run.py:483] Algo bellman_ford step 9264 current loss 0.059052, current_train_items 296480.
I0304 19:32:29.132245 23128000471168 run.py:483] Algo bellman_ford step 9265 current loss 0.002833, current_train_items 296512.
I0304 19:32:29.148836 23128000471168 run.py:483] Algo bellman_ford step 9266 current loss 0.009554, current_train_items 296544.
I0304 19:32:29.172194 23128000471168 run.py:483] Algo bellman_ford step 9267 current loss 0.027894, current_train_items 296576.
I0304 19:32:29.204537 23128000471168 run.py:483] Algo bellman_ford step 9268 current loss 0.045357, current_train_items 296608.
I0304 19:32:29.238412 23128000471168 run.py:483] Algo bellman_ford step 9269 current loss 0.066182, current_train_items 296640.
I0304 19:32:29.258710 23128000471168 run.py:483] Algo bellman_ford step 9270 current loss 0.001515, current_train_items 296672.
I0304 19:32:29.275242 23128000471168 run.py:483] Algo bellman_ford step 9271 current loss 0.017357, current_train_items 296704.
I0304 19:32:29.299486 23128000471168 run.py:483] Algo bellman_ford step 9272 current loss 0.021247, current_train_items 296736.
I0304 19:32:29.333548 23128000471168 run.py:483] Algo bellman_ford step 9273 current loss 0.047824, current_train_items 296768.
I0304 19:32:29.368627 23128000471168 run.py:483] Algo bellman_ford step 9274 current loss 0.046497, current_train_items 296800.
I0304 19:32:29.389249 23128000471168 run.py:483] Algo bellman_ford step 9275 current loss 0.011562, current_train_items 296832.
I0304 19:32:29.406151 23128000471168 run.py:483] Algo bellman_ford step 9276 current loss 0.013352, current_train_items 296864.
I0304 19:32:29.430475 23128000471168 run.py:483] Algo bellman_ford step 9277 current loss 0.056378, current_train_items 296896.
I0304 19:32:29.463415 23128000471168 run.py:483] Algo bellman_ford step 9278 current loss 0.053590, current_train_items 296928.
I0304 19:32:29.497948 23128000471168 run.py:483] Algo bellman_ford step 9279 current loss 0.087735, current_train_items 296960.
I0304 19:32:29.518014 23128000471168 run.py:483] Algo bellman_ford step 9280 current loss 0.002075, current_train_items 296992.
I0304 19:32:29.534716 23128000471168 run.py:483] Algo bellman_ford step 9281 current loss 0.027805, current_train_items 297024.
I0304 19:32:29.559001 23128000471168 run.py:483] Algo bellman_ford step 9282 current loss 0.012954, current_train_items 297056.
I0304 19:32:29.589997 23128000471168 run.py:483] Algo bellman_ford step 9283 current loss 0.021577, current_train_items 297088.
I0304 19:32:29.623449 23128000471168 run.py:483] Algo bellman_ford step 9284 current loss 0.054811, current_train_items 297120.
I0304 19:32:29.643744 23128000471168 run.py:483] Algo bellman_ford step 9285 current loss 0.003473, current_train_items 297152.
I0304 19:32:29.659742 23128000471168 run.py:483] Algo bellman_ford step 9286 current loss 0.006401, current_train_items 297184.
I0304 19:32:29.685732 23128000471168 run.py:483] Algo bellman_ford step 9287 current loss 0.047687, current_train_items 297216.
I0304 19:32:29.717586 23128000471168 run.py:483] Algo bellman_ford step 9288 current loss 0.029151, current_train_items 297248.
I0304 19:32:29.753242 23128000471168 run.py:483] Algo bellman_ford step 9289 current loss 0.035653, current_train_items 297280.
I0304 19:32:29.773541 23128000471168 run.py:483] Algo bellman_ford step 9290 current loss 0.001423, current_train_items 297312.
I0304 19:32:29.790317 23128000471168 run.py:483] Algo bellman_ford step 9291 current loss 0.022726, current_train_items 297344.
I0304 19:32:29.815374 23128000471168 run.py:483] Algo bellman_ford step 9292 current loss 0.060361, current_train_items 297376.
I0304 19:32:29.847009 23128000471168 run.py:483] Algo bellman_ford step 9293 current loss 0.042030, current_train_items 297408.
I0304 19:32:29.882139 23128000471168 run.py:483] Algo bellman_ford step 9294 current loss 0.068247, current_train_items 297440.
I0304 19:32:29.902524 23128000471168 run.py:483] Algo bellman_ford step 9295 current loss 0.004170, current_train_items 297472.
I0304 19:32:29.919084 23128000471168 run.py:483] Algo bellman_ford step 9296 current loss 0.004450, current_train_items 297504.
I0304 19:32:29.943628 23128000471168 run.py:483] Algo bellman_ford step 9297 current loss 0.036321, current_train_items 297536.
I0304 19:32:29.976459 23128000471168 run.py:483] Algo bellman_ford step 9298 current loss 0.039023, current_train_items 297568.
I0304 19:32:30.011303 23128000471168 run.py:483] Algo bellman_ford step 9299 current loss 0.035153, current_train_items 297600.
I0304 19:32:30.032134 23128000471168 run.py:483] Algo bellman_ford step 9300 current loss 0.001889, current_train_items 297632.
I0304 19:32:30.040045 23128000471168 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0304 19:32:30.040154 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:30.057670 23128000471168 run.py:483] Algo bellman_ford step 9301 current loss 0.021014, current_train_items 297664.
I0304 19:32:30.082698 23128000471168 run.py:483] Algo bellman_ford step 9302 current loss 0.026981, current_train_items 297696.
I0304 19:32:30.115838 23128000471168 run.py:483] Algo bellman_ford step 9303 current loss 0.025851, current_train_items 297728.
I0304 19:32:30.151285 23128000471168 run.py:483] Algo bellman_ford step 9304 current loss 0.099758, current_train_items 297760.
I0304 19:32:30.171842 23128000471168 run.py:483] Algo bellman_ford step 9305 current loss 0.001902, current_train_items 297792.
I0304 19:32:30.188585 23128000471168 run.py:483] Algo bellman_ford step 9306 current loss 0.023678, current_train_items 297824.
I0304 19:32:30.213261 23128000471168 run.py:483] Algo bellman_ford step 9307 current loss 0.028479, current_train_items 297856.
I0304 19:32:30.245562 23128000471168 run.py:483] Algo bellman_ford step 9308 current loss 0.084883, current_train_items 297888.
I0304 19:32:30.279213 23128000471168 run.py:483] Algo bellman_ford step 9309 current loss 0.045223, current_train_items 297920.
I0304 19:32:30.299439 23128000471168 run.py:483] Algo bellman_ford step 9310 current loss 0.002533, current_train_items 297952.
I0304 19:32:30.315853 23128000471168 run.py:483] Algo bellman_ford step 9311 current loss 0.005144, current_train_items 297984.
I0304 19:32:30.340105 23128000471168 run.py:483] Algo bellman_ford step 9312 current loss 0.034651, current_train_items 298016.
I0304 19:32:30.371462 23128000471168 run.py:483] Algo bellman_ford step 9313 current loss 0.030597, current_train_items 298048.
I0304 19:32:30.405983 23128000471168 run.py:483] Algo bellman_ford step 9314 current loss 0.106533, current_train_items 298080.
I0304 19:32:30.426163 23128000471168 run.py:483] Algo bellman_ford step 9315 current loss 0.005971, current_train_items 298112.
I0304 19:32:30.442305 23128000471168 run.py:483] Algo bellman_ford step 9316 current loss 0.004025, current_train_items 298144.
I0304 19:32:30.466908 23128000471168 run.py:483] Algo bellman_ford step 9317 current loss 0.029580, current_train_items 298176.
I0304 19:32:30.499801 23128000471168 run.py:483] Algo bellman_ford step 9318 current loss 0.043930, current_train_items 298208.
I0304 19:32:30.533834 23128000471168 run.py:483] Algo bellman_ford step 9319 current loss 0.031769, current_train_items 298240.
I0304 19:32:30.553872 23128000471168 run.py:483] Algo bellman_ford step 9320 current loss 0.002675, current_train_items 298272.
I0304 19:32:30.570452 23128000471168 run.py:483] Algo bellman_ford step 9321 current loss 0.019960, current_train_items 298304.
I0304 19:32:30.593549 23128000471168 run.py:483] Algo bellman_ford step 9322 current loss 0.018432, current_train_items 298336.
I0304 19:32:30.624950 23128000471168 run.py:483] Algo bellman_ford step 9323 current loss 0.033499, current_train_items 298368.
I0304 19:32:30.661953 23128000471168 run.py:483] Algo bellman_ford step 9324 current loss 0.042461, current_train_items 298400.
I0304 19:32:30.681852 23128000471168 run.py:483] Algo bellman_ford step 9325 current loss 0.001264, current_train_items 298432.
I0304 19:32:30.698263 23128000471168 run.py:483] Algo bellman_ford step 9326 current loss 0.030248, current_train_items 298464.
I0304 19:32:30.723432 23128000471168 run.py:483] Algo bellman_ford step 9327 current loss 0.031147, current_train_items 298496.
I0304 19:32:30.756570 23128000471168 run.py:483] Algo bellman_ford step 9328 current loss 0.041392, current_train_items 298528.
I0304 19:32:30.788170 23128000471168 run.py:483] Algo bellman_ford step 9329 current loss 0.028147, current_train_items 298560.
I0304 19:32:30.808162 23128000471168 run.py:483] Algo bellman_ford step 9330 current loss 0.002512, current_train_items 298592.
I0304 19:32:30.824775 23128000471168 run.py:483] Algo bellman_ford step 9331 current loss 0.022008, current_train_items 298624.
I0304 19:32:30.848731 23128000471168 run.py:483] Algo bellman_ford step 9332 current loss 0.058623, current_train_items 298656.
I0304 19:32:30.880943 23128000471168 run.py:483] Algo bellman_ford step 9333 current loss 0.035568, current_train_items 298688.
I0304 19:32:30.914314 23128000471168 run.py:483] Algo bellman_ford step 9334 current loss 0.090505, current_train_items 298720.
I0304 19:32:30.934232 23128000471168 run.py:483] Algo bellman_ford step 9335 current loss 0.001379, current_train_items 298752.
I0304 19:32:30.950694 23128000471168 run.py:483] Algo bellman_ford step 9336 current loss 0.035874, current_train_items 298784.
I0304 19:32:30.976053 23128000471168 run.py:483] Algo bellman_ford step 9337 current loss 0.020830, current_train_items 298816.
I0304 19:32:31.008405 23128000471168 run.py:483] Algo bellman_ford step 9338 current loss 0.038059, current_train_items 298848.
I0304 19:32:31.043502 23128000471168 run.py:483] Algo bellman_ford step 9339 current loss 0.063880, current_train_items 298880.
I0304 19:32:31.063819 23128000471168 run.py:483] Algo bellman_ford step 9340 current loss 0.004742, current_train_items 298912.
I0304 19:32:31.079938 23128000471168 run.py:483] Algo bellman_ford step 9341 current loss 0.012254, current_train_items 298944.
I0304 19:32:31.104935 23128000471168 run.py:483] Algo bellman_ford step 9342 current loss 0.042464, current_train_items 298976.
I0304 19:32:31.137398 23128000471168 run.py:483] Algo bellman_ford step 9343 current loss 0.039448, current_train_items 299008.
I0304 19:32:31.172022 23128000471168 run.py:483] Algo bellman_ford step 9344 current loss 0.038302, current_train_items 299040.
I0304 19:32:31.192033 23128000471168 run.py:483] Algo bellman_ford step 9345 current loss 0.003096, current_train_items 299072.
I0304 19:32:31.208538 23128000471168 run.py:483] Algo bellman_ford step 9346 current loss 0.008620, current_train_items 299104.
I0304 19:32:31.233407 23128000471168 run.py:483] Algo bellman_ford step 9347 current loss 0.022707, current_train_items 299136.
I0304 19:32:31.266126 23128000471168 run.py:483] Algo bellman_ford step 9348 current loss 0.038146, current_train_items 299168.
I0304 19:32:31.299163 23128000471168 run.py:483] Algo bellman_ford step 9349 current loss 0.060989, current_train_items 299200.
I0304 19:32:31.319209 23128000471168 run.py:483] Algo bellman_ford step 9350 current loss 0.026234, current_train_items 299232.
I0304 19:32:31.327397 23128000471168 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0304 19:32:31.327507 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:31.344582 23128000471168 run.py:483] Algo bellman_ford step 9351 current loss 0.002507, current_train_items 299264.
I0304 19:32:31.370383 23128000471168 run.py:483] Algo bellman_ford step 9352 current loss 0.032158, current_train_items 299296.
I0304 19:32:31.403696 23128000471168 run.py:483] Algo bellman_ford step 9353 current loss 0.030029, current_train_items 299328.
I0304 19:32:31.439500 23128000471168 run.py:483] Algo bellman_ford step 9354 current loss 0.109597, current_train_items 299360.
I0304 19:32:31.459638 23128000471168 run.py:483] Algo bellman_ford step 9355 current loss 0.040580, current_train_items 299392.
I0304 19:32:31.475447 23128000471168 run.py:483] Algo bellman_ford step 9356 current loss 0.029931, current_train_items 299424.
I0304 19:32:31.500120 23128000471168 run.py:483] Algo bellman_ford step 9357 current loss 0.093208, current_train_items 299456.
I0304 19:32:31.532869 23128000471168 run.py:483] Algo bellman_ford step 9358 current loss 0.030867, current_train_items 299488.
I0304 19:32:31.566475 23128000471168 run.py:483] Algo bellman_ford step 9359 current loss 0.026775, current_train_items 299520.
I0304 19:32:31.587124 23128000471168 run.py:483] Algo bellman_ford step 9360 current loss 0.020877, current_train_items 299552.
I0304 19:32:31.603709 23128000471168 run.py:483] Algo bellman_ford step 9361 current loss 0.011078, current_train_items 299584.
I0304 19:32:31.627475 23128000471168 run.py:483] Algo bellman_ford step 9362 current loss 0.043256, current_train_items 299616.
I0304 19:32:31.660555 23128000471168 run.py:483] Algo bellman_ford step 9363 current loss 0.042797, current_train_items 299648.
I0304 19:32:31.695413 23128000471168 run.py:483] Algo bellman_ford step 9364 current loss 0.042911, current_train_items 299680.
I0304 19:32:31.715134 23128000471168 run.py:483] Algo bellman_ford step 9365 current loss 0.002633, current_train_items 299712.
I0304 19:32:31.731928 23128000471168 run.py:483] Algo bellman_ford step 9366 current loss 0.021091, current_train_items 299744.
I0304 19:32:31.757916 23128000471168 run.py:483] Algo bellman_ford step 9367 current loss 0.033744, current_train_items 299776.
I0304 19:32:31.791017 23128000471168 run.py:483] Algo bellman_ford step 9368 current loss 0.039706, current_train_items 299808.
I0304 19:32:31.824400 23128000471168 run.py:483] Algo bellman_ford step 9369 current loss 0.049029, current_train_items 299840.
I0304 19:32:31.844878 23128000471168 run.py:483] Algo bellman_ford step 9370 current loss 0.011663, current_train_items 299872.
I0304 19:32:31.861641 23128000471168 run.py:483] Algo bellman_ford step 9371 current loss 0.022400, current_train_items 299904.
I0304 19:32:31.885980 23128000471168 run.py:483] Algo bellman_ford step 9372 current loss 0.018499, current_train_items 299936.
I0304 19:32:31.919064 23128000471168 run.py:483] Algo bellman_ford step 9373 current loss 0.054812, current_train_items 299968.
I0304 19:32:31.953358 23128000471168 run.py:483] Algo bellman_ford step 9374 current loss 0.035687, current_train_items 300000.
I0304 19:32:31.973489 23128000471168 run.py:483] Algo bellman_ford step 9375 current loss 0.015063, current_train_items 300032.
I0304 19:32:31.989652 23128000471168 run.py:483] Algo bellman_ford step 9376 current loss 0.021205, current_train_items 300064.
I0304 19:32:32.013641 23128000471168 run.py:483] Algo bellman_ford step 9377 current loss 0.025768, current_train_items 300096.
I0304 19:32:32.045253 23128000471168 run.py:483] Algo bellman_ford step 9378 current loss 0.029974, current_train_items 300128.
I0304 19:32:32.079176 23128000471168 run.py:483] Algo bellman_ford step 9379 current loss 0.066955, current_train_items 300160.
I0304 19:32:32.099098 23128000471168 run.py:483] Algo bellman_ford step 9380 current loss 0.001496, current_train_items 300192.
I0304 19:32:32.115085 23128000471168 run.py:483] Algo bellman_ford step 9381 current loss 0.030654, current_train_items 300224.
I0304 19:32:32.139738 23128000471168 run.py:483] Algo bellman_ford step 9382 current loss 0.024792, current_train_items 300256.
I0304 19:32:32.171231 23128000471168 run.py:483] Algo bellman_ford step 9383 current loss 0.038447, current_train_items 300288.
I0304 19:32:32.205086 23128000471168 run.py:483] Algo bellman_ford step 9384 current loss 0.041264, current_train_items 300320.
I0304 19:32:32.225336 23128000471168 run.py:483] Algo bellman_ford step 9385 current loss 0.002204, current_train_items 300352.
I0304 19:32:32.242538 23128000471168 run.py:483] Algo bellman_ford step 9386 current loss 0.033068, current_train_items 300384.
I0304 19:32:32.266701 23128000471168 run.py:483] Algo bellman_ford step 9387 current loss 0.044644, current_train_items 300416.
I0304 19:32:32.298909 23128000471168 run.py:483] Algo bellman_ford step 9388 current loss 0.036098, current_train_items 300448.
I0304 19:32:32.334796 23128000471168 run.py:483] Algo bellman_ford step 9389 current loss 0.042759, current_train_items 300480.
I0304 19:32:32.355213 23128000471168 run.py:483] Algo bellman_ford step 9390 current loss 0.003203, current_train_items 300512.
I0304 19:32:32.372114 23128000471168 run.py:483] Algo bellman_ford step 9391 current loss 0.007209, current_train_items 300544.
I0304 19:32:32.396183 23128000471168 run.py:483] Algo bellman_ford step 9392 current loss 0.017049, current_train_items 300576.
I0304 19:32:32.428254 23128000471168 run.py:483] Algo bellman_ford step 9393 current loss 0.052284, current_train_items 300608.
I0304 19:32:32.461653 23128000471168 run.py:483] Algo bellman_ford step 9394 current loss 0.059616, current_train_items 300640.
I0304 19:32:32.481723 23128000471168 run.py:483] Algo bellman_ford step 9395 current loss 0.005140, current_train_items 300672.
I0304 19:32:32.498396 23128000471168 run.py:483] Algo bellman_ford step 9396 current loss 0.020502, current_train_items 300704.
I0304 19:32:32.523629 23128000471168 run.py:483] Algo bellman_ford step 9397 current loss 0.049704, current_train_items 300736.
I0304 19:32:32.555619 23128000471168 run.py:483] Algo bellman_ford step 9398 current loss 0.078129, current_train_items 300768.
I0304 19:32:32.589878 23128000471168 run.py:483] Algo bellman_ford step 9399 current loss 0.069715, current_train_items 300800.
I0304 19:32:32.610128 23128000471168 run.py:483] Algo bellman_ford step 9400 current loss 0.007212, current_train_items 300832.
I0304 19:32:32.618159 23128000471168 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0304 19:32:32.618268 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:32:32.635715 23128000471168 run.py:483] Algo bellman_ford step 9401 current loss 0.020485, current_train_items 300864.
I0304 19:32:32.661467 23128000471168 run.py:483] Algo bellman_ford step 9402 current loss 0.039824, current_train_items 300896.
I0304 19:32:32.693823 23128000471168 run.py:483] Algo bellman_ford step 9403 current loss 0.020610, current_train_items 300928.
I0304 19:32:32.729136 23128000471168 run.py:483] Algo bellman_ford step 9404 current loss 0.053473, current_train_items 300960.
I0304 19:32:32.749345 23128000471168 run.py:483] Algo bellman_ford step 9405 current loss 0.001629, current_train_items 300992.
I0304 19:32:32.765553 23128000471168 run.py:483] Algo bellman_ford step 9406 current loss 0.012066, current_train_items 301024.
I0304 19:32:32.791492 23128000471168 run.py:483] Algo bellman_ford step 9407 current loss 0.045147, current_train_items 301056.
I0304 19:32:32.823509 23128000471168 run.py:483] Algo bellman_ford step 9408 current loss 0.019928, current_train_items 301088.
I0304 19:32:32.858611 23128000471168 run.py:483] Algo bellman_ford step 9409 current loss 0.079844, current_train_items 301120.
I0304 19:32:32.878537 23128000471168 run.py:483] Algo bellman_ford step 9410 current loss 0.003369, current_train_items 301152.
I0304 19:32:32.894920 23128000471168 run.py:483] Algo bellman_ford step 9411 current loss 0.009308, current_train_items 301184.
I0304 19:32:32.919677 23128000471168 run.py:483] Algo bellman_ford step 9412 current loss 0.022649, current_train_items 301216.
I0304 19:32:32.952076 23128000471168 run.py:483] Algo bellman_ford step 9413 current loss 0.025526, current_train_items 301248.
I0304 19:32:32.985786 23128000471168 run.py:483] Algo bellman_ford step 9414 current loss 0.053800, current_train_items 301280.
I0304 19:32:33.005572 23128000471168 run.py:483] Algo bellman_ford step 9415 current loss 0.009895, current_train_items 301312.
I0304 19:32:33.022301 23128000471168 run.py:483] Algo bellman_ford step 9416 current loss 0.011614, current_train_items 301344.
I0304 19:32:33.047405 23128000471168 run.py:483] Algo bellman_ford step 9417 current loss 0.044529, current_train_items 301376.
I0304 19:32:33.079576 23128000471168 run.py:483] Algo bellman_ford step 9418 current loss 0.035964, current_train_items 301408.
I0304 19:32:33.111549 23128000471168 run.py:483] Algo bellman_ford step 9419 current loss 0.021487, current_train_items 301440.
I0304 19:32:33.131255 23128000471168 run.py:483] Algo bellman_ford step 9420 current loss 0.016325, current_train_items 301472.
I0304 19:32:33.147810 23128000471168 run.py:483] Algo bellman_ford step 9421 current loss 0.014887, current_train_items 301504.
I0304 19:32:33.173199 23128000471168 run.py:483] Algo bellman_ford step 9422 current loss 0.017585, current_train_items 301536.
I0304 19:32:33.205399 23128000471168 run.py:483] Algo bellman_ford step 9423 current loss 0.025456, current_train_items 301568.
I0304 19:32:33.239334 23128000471168 run.py:483] Algo bellman_ford step 9424 current loss 0.049181, current_train_items 301600.
I0304 19:32:33.259399 23128000471168 run.py:483] Algo bellman_ford step 9425 current loss 0.014241, current_train_items 301632.
I0304 19:32:33.275476 23128000471168 run.py:483] Algo bellman_ford step 9426 current loss 0.023934, current_train_items 301664.
I0304 19:32:33.299435 23128000471168 run.py:483] Algo bellman_ford step 9427 current loss 0.024571, current_train_items 301696.
I0304 19:32:33.332796 23128000471168 run.py:483] Algo bellman_ford step 9428 current loss 0.034497, current_train_items 301728.
I0304 19:32:33.364168 23128000471168 run.py:483] Algo bellman_ford step 9429 current loss 0.146750, current_train_items 301760.
I0304 19:32:33.383921 23128000471168 run.py:483] Algo bellman_ford step 9430 current loss 0.006640, current_train_items 301792.
I0304 19:32:33.400292 23128000471168 run.py:483] Algo bellman_ford step 9431 current loss 0.018287, current_train_items 301824.
I0304 19:32:33.424748 23128000471168 run.py:483] Algo bellman_ford step 9432 current loss 0.042462, current_train_items 301856.
I0304 19:32:33.456263 23128000471168 run.py:483] Algo bellman_ford step 9433 current loss 0.045830, current_train_items 301888.
I0304 19:32:33.490771 23128000471168 run.py:483] Algo bellman_ford step 9434 current loss 0.061896, current_train_items 301920.
I0304 19:32:33.510463 23128000471168 run.py:483] Algo bellman_ford step 9435 current loss 0.001735, current_train_items 301952.
I0304 19:32:33.527118 23128000471168 run.py:483] Algo bellman_ford step 9436 current loss 0.028559, current_train_items 301984.
I0304 19:32:33.551499 23128000471168 run.py:483] Algo bellman_ford step 9437 current loss 0.009335, current_train_items 302016.
I0304 19:32:33.584174 23128000471168 run.py:483] Algo bellman_ford step 9438 current loss 0.034353, current_train_items 302048.
I0304 19:32:33.617293 23128000471168 run.py:483] Algo bellman_ford step 9439 current loss 0.056556, current_train_items 302080.
I0304 19:32:33.637421 23128000471168 run.py:483] Algo bellman_ford step 9440 current loss 0.030286, current_train_items 302112.
I0304 19:32:33.654264 23128000471168 run.py:483] Algo bellman_ford step 9441 current loss 0.010591, current_train_items 302144.
I0304 19:32:33.678560 23128000471168 run.py:483] Algo bellman_ford step 9442 current loss 0.016912, current_train_items 302176.
I0304 19:32:33.711547 23128000471168 run.py:483] Algo bellman_ford step 9443 current loss 0.022674, current_train_items 302208.
I0304 19:32:33.748181 23128000471168 run.py:483] Algo bellman_ford step 9444 current loss 0.069931, current_train_items 302240.
I0304 19:32:33.767913 23128000471168 run.py:483] Algo bellman_ford step 9445 current loss 0.045061, current_train_items 302272.
I0304 19:32:33.784260 23128000471168 run.py:483] Algo bellman_ford step 9446 current loss 0.003966, current_train_items 302304.
I0304 19:32:33.808430 23128000471168 run.py:483] Algo bellman_ford step 9447 current loss 0.020230, current_train_items 302336.
I0304 19:32:33.840361 23128000471168 run.py:483] Algo bellman_ford step 9448 current loss 0.053070, current_train_items 302368.
I0304 19:32:33.872840 23128000471168 run.py:483] Algo bellman_ford step 9449 current loss 0.049854, current_train_items 302400.
I0304 19:32:33.892836 23128000471168 run.py:483] Algo bellman_ford step 9450 current loss 0.002393, current_train_items 302432.
I0304 19:32:33.901127 23128000471168 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0304 19:32:33.901236 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:32:33.918619 23128000471168 run.py:483] Algo bellman_ford step 9451 current loss 0.004572, current_train_items 302464.
I0304 19:32:33.942836 23128000471168 run.py:483] Algo bellman_ford step 9452 current loss 0.036583, current_train_items 302496.
I0304 19:32:33.975962 23128000471168 run.py:483] Algo bellman_ford step 9453 current loss 0.034976, current_train_items 302528.
I0304 19:32:34.010851 23128000471168 run.py:483] Algo bellman_ford step 9454 current loss 0.045349, current_train_items 302560.
I0304 19:32:34.031574 23128000471168 run.py:483] Algo bellman_ford step 9455 current loss 0.001651, current_train_items 302592.
I0304 19:32:34.048282 23128000471168 run.py:483] Algo bellman_ford step 9456 current loss 0.005048, current_train_items 302624.
I0304 19:32:34.072801 23128000471168 run.py:483] Algo bellman_ford step 9457 current loss 0.025550, current_train_items 302656.
I0304 19:32:34.105201 23128000471168 run.py:483] Algo bellman_ford step 9458 current loss 0.030815, current_train_items 302688.
I0304 19:32:34.138063 23128000471168 run.py:483] Algo bellman_ford step 9459 current loss 0.053826, current_train_items 302720.
I0304 19:32:34.159104 23128000471168 run.py:483] Algo bellman_ford step 9460 current loss 0.002251, current_train_items 302752.
I0304 19:32:34.175969 23128000471168 run.py:483] Algo bellman_ford step 9461 current loss 0.054734, current_train_items 302784.
I0304 19:32:34.199904 23128000471168 run.py:483] Algo bellman_ford step 9462 current loss 0.021721, current_train_items 302816.
I0304 19:32:34.233087 23128000471168 run.py:483] Algo bellman_ford step 9463 current loss 0.030604, current_train_items 302848.
I0304 19:32:34.266744 23128000471168 run.py:483] Algo bellman_ford step 9464 current loss 0.035092, current_train_items 302880.
I0304 19:32:34.286912 23128000471168 run.py:483] Algo bellman_ford step 9465 current loss 0.003529, current_train_items 302912.
I0304 19:32:34.303945 23128000471168 run.py:483] Algo bellman_ford step 9466 current loss 0.058500, current_train_items 302944.
I0304 19:32:34.329920 23128000471168 run.py:483] Algo bellman_ford step 9467 current loss 0.080392, current_train_items 302976.
I0304 19:32:34.363175 23128000471168 run.py:483] Algo bellman_ford step 9468 current loss 0.063497, current_train_items 303008.
I0304 19:32:34.398428 23128000471168 run.py:483] Algo bellman_ford step 9469 current loss 0.107507, current_train_items 303040.
I0304 19:32:34.418849 23128000471168 run.py:483] Algo bellman_ford step 9470 current loss 0.005515, current_train_items 303072.
I0304 19:32:34.435104 23128000471168 run.py:483] Algo bellman_ford step 9471 current loss 0.010745, current_train_items 303104.
I0304 19:32:34.459347 23128000471168 run.py:483] Algo bellman_ford step 9472 current loss 0.027828, current_train_items 303136.
I0304 19:32:34.492344 23128000471168 run.py:483] Algo bellman_ford step 9473 current loss 0.031484, current_train_items 303168.
I0304 19:32:34.525585 23128000471168 run.py:483] Algo bellman_ford step 9474 current loss 0.055284, current_train_items 303200.
I0304 19:32:34.545906 23128000471168 run.py:483] Algo bellman_ford step 9475 current loss 0.003635, current_train_items 303232.
I0304 19:32:34.562704 23128000471168 run.py:483] Algo bellman_ford step 9476 current loss 0.036087, current_train_items 303264.
I0304 19:32:34.586556 23128000471168 run.py:483] Algo bellman_ford step 9477 current loss 0.029189, current_train_items 303296.
I0304 19:32:34.618226 23128000471168 run.py:483] Algo bellman_ford step 9478 current loss 0.036366, current_train_items 303328.
I0304 19:32:34.652061 23128000471168 run.py:483] Algo bellman_ford step 9479 current loss 0.071926, current_train_items 303360.
I0304 19:32:34.671916 23128000471168 run.py:483] Algo bellman_ford step 9480 current loss 0.002966, current_train_items 303392.
I0304 19:32:34.688305 23128000471168 run.py:483] Algo bellman_ford step 9481 current loss 0.010415, current_train_items 303424.
I0304 19:32:34.714255 23128000471168 run.py:483] Algo bellman_ford step 9482 current loss 0.055025, current_train_items 303456.
I0304 19:32:34.745551 23128000471168 run.py:483] Algo bellman_ford step 9483 current loss 0.057073, current_train_items 303488.
I0304 19:32:34.778534 23128000471168 run.py:483] Algo bellman_ford step 9484 current loss 0.064587, current_train_items 303520.
I0304 19:32:34.798922 23128000471168 run.py:483] Algo bellman_ford step 9485 current loss 0.001247, current_train_items 303552.
I0304 19:32:34.815846 23128000471168 run.py:483] Algo bellman_ford step 9486 current loss 0.004403, current_train_items 303584.
I0304 19:32:34.841619 23128000471168 run.py:483] Algo bellman_ford step 9487 current loss 0.038603, current_train_items 303616.
I0304 19:32:34.875187 23128000471168 run.py:483] Algo bellman_ford step 9488 current loss 0.032421, current_train_items 303648.
I0304 19:32:34.908182 23128000471168 run.py:483] Algo bellman_ford step 9489 current loss 0.039734, current_train_items 303680.
I0304 19:32:34.928755 23128000471168 run.py:483] Algo bellman_ford step 9490 current loss 0.002191, current_train_items 303712.
I0304 19:32:34.945202 23128000471168 run.py:483] Algo bellman_ford step 9491 current loss 0.012046, current_train_items 303744.
I0304 19:32:34.970180 23128000471168 run.py:483] Algo bellman_ford step 9492 current loss 0.015400, current_train_items 303776.
I0304 19:32:35.001762 23128000471168 run.py:483] Algo bellman_ford step 9493 current loss 0.025495, current_train_items 303808.
I0304 19:32:35.035840 23128000471168 run.py:483] Algo bellman_ford step 9494 current loss 0.077078, current_train_items 303840.
I0304 19:32:35.056200 23128000471168 run.py:483] Algo bellman_ford step 9495 current loss 0.007203, current_train_items 303872.
I0304 19:32:35.073195 23128000471168 run.py:483] Algo bellman_ford step 9496 current loss 0.020116, current_train_items 303904.
I0304 19:32:35.097879 23128000471168 run.py:483] Algo bellman_ford step 9497 current loss 0.018050, current_train_items 303936.
I0304 19:32:35.129496 23128000471168 run.py:483] Algo bellman_ford step 9498 current loss 0.043098, current_train_items 303968.
I0304 19:32:35.164463 23128000471168 run.py:483] Algo bellman_ford step 9499 current loss 0.044908, current_train_items 304000.
I0304 19:32:35.185266 23128000471168 run.py:483] Algo bellman_ford step 9500 current loss 0.001492, current_train_items 304032.
I0304 19:32:35.192995 23128000471168 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0304 19:32:35.193113 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:35.210034 23128000471168 run.py:483] Algo bellman_ford step 9501 current loss 0.021987, current_train_items 304064.
I0304 19:32:35.235407 23128000471168 run.py:483] Algo bellman_ford step 9502 current loss 0.027711, current_train_items 304096.
I0304 19:32:35.268407 23128000471168 run.py:483] Algo bellman_ford step 9503 current loss 0.022109, current_train_items 304128.
I0304 19:32:35.301851 23128000471168 run.py:483] Algo bellman_ford step 9504 current loss 0.036088, current_train_items 304160.
I0304 19:32:35.322235 23128000471168 run.py:483] Algo bellman_ford step 9505 current loss 0.003763, current_train_items 304192.
I0304 19:32:35.338794 23128000471168 run.py:483] Algo bellman_ford step 9506 current loss 0.006586, current_train_items 304224.
I0304 19:32:35.364192 23128000471168 run.py:483] Algo bellman_ford step 9507 current loss 0.029929, current_train_items 304256.
I0304 19:32:35.395928 23128000471168 run.py:483] Algo bellman_ford step 9508 current loss 0.064908, current_train_items 304288.
I0304 19:32:35.432353 23128000471168 run.py:483] Algo bellman_ford step 9509 current loss 0.115885, current_train_items 304320.
I0304 19:32:35.452636 23128000471168 run.py:483] Algo bellman_ford step 9510 current loss 0.001584, current_train_items 304352.
I0304 19:32:35.469606 23128000471168 run.py:483] Algo bellman_ford step 9511 current loss 0.008630, current_train_items 304384.
I0304 19:32:35.494982 23128000471168 run.py:483] Algo bellman_ford step 9512 current loss 0.030339, current_train_items 304416.
I0304 19:32:35.527625 23128000471168 run.py:483] Algo bellman_ford step 9513 current loss 0.033261, current_train_items 304448.
I0304 19:32:35.560718 23128000471168 run.py:483] Algo bellman_ford step 9514 current loss 0.039101, current_train_items 304480.
I0304 19:32:35.580615 23128000471168 run.py:483] Algo bellman_ford step 9515 current loss 0.005709, current_train_items 304512.
I0304 19:32:35.596971 23128000471168 run.py:483] Algo bellman_ford step 9516 current loss 0.016916, current_train_items 304544.
I0304 19:32:35.621811 23128000471168 run.py:483] Algo bellman_ford step 9517 current loss 0.016455, current_train_items 304576.
I0304 19:32:35.654675 23128000471168 run.py:483] Algo bellman_ford step 9518 current loss 0.037842, current_train_items 304608.
I0304 19:32:35.687434 23128000471168 run.py:483] Algo bellman_ford step 9519 current loss 0.030346, current_train_items 304640.
I0304 19:32:35.707525 23128000471168 run.py:483] Algo bellman_ford step 9520 current loss 0.001126, current_train_items 304672.
I0304 19:32:35.723361 23128000471168 run.py:483] Algo bellman_ford step 9521 current loss 0.003898, current_train_items 304704.
I0304 19:32:35.747509 23128000471168 run.py:483] Algo bellman_ford step 9522 current loss 0.024091, current_train_items 304736.
I0304 19:32:35.778773 23128000471168 run.py:483] Algo bellman_ford step 9523 current loss 0.029749, current_train_items 304768.
I0304 19:32:35.812042 23128000471168 run.py:483] Algo bellman_ford step 9524 current loss 0.039415, current_train_items 304800.
I0304 19:32:35.831990 23128000471168 run.py:483] Algo bellman_ford step 9525 current loss 0.002595, current_train_items 304832.
I0304 19:32:35.848507 23128000471168 run.py:483] Algo bellman_ford step 9526 current loss 0.004584, current_train_items 304864.
I0304 19:32:35.873498 23128000471168 run.py:483] Algo bellman_ford step 9527 current loss 0.064064, current_train_items 304896.
I0304 19:32:35.906642 23128000471168 run.py:483] Algo bellman_ford step 9528 current loss 0.058946, current_train_items 304928.
I0304 19:32:35.942430 23128000471168 run.py:483] Algo bellman_ford step 9529 current loss 0.036511, current_train_items 304960.
I0304 19:32:35.962305 23128000471168 run.py:483] Algo bellman_ford step 9530 current loss 0.001573, current_train_items 304992.
I0304 19:32:35.978709 23128000471168 run.py:483] Algo bellman_ford step 9531 current loss 0.018244, current_train_items 305024.
I0304 19:32:36.002970 23128000471168 run.py:483] Algo bellman_ford step 9532 current loss 0.025338, current_train_items 305056.
I0304 19:32:36.036979 23128000471168 run.py:483] Algo bellman_ford step 9533 current loss 0.099481, current_train_items 305088.
I0304 19:32:36.071099 23128000471168 run.py:483] Algo bellman_ford step 9534 current loss 0.059871, current_train_items 305120.
I0304 19:32:36.091559 23128000471168 run.py:483] Algo bellman_ford step 9535 current loss 0.010239, current_train_items 305152.
I0304 19:32:36.108047 23128000471168 run.py:483] Algo bellman_ford step 9536 current loss 0.016514, current_train_items 305184.
I0304 19:32:36.132753 23128000471168 run.py:483] Algo bellman_ford step 9537 current loss 0.032156, current_train_items 305216.
I0304 19:32:36.166672 23128000471168 run.py:483] Algo bellman_ford step 9538 current loss 0.068794, current_train_items 305248.
I0304 19:32:36.198538 23128000471168 run.py:483] Algo bellman_ford step 9539 current loss 0.041137, current_train_items 305280.
I0304 19:32:36.218682 23128000471168 run.py:483] Algo bellman_ford step 9540 current loss 0.001151, current_train_items 305312.
I0304 19:32:36.234917 23128000471168 run.py:483] Algo bellman_ford step 9541 current loss 0.041091, current_train_items 305344.
I0304 19:32:36.259302 23128000471168 run.py:483] Algo bellman_ford step 9542 current loss 0.028552, current_train_items 305376.
I0304 19:32:36.293410 23128000471168 run.py:483] Algo bellman_ford step 9543 current loss 0.035781, current_train_items 305408.
I0304 19:32:36.327991 23128000471168 run.py:483] Algo bellman_ford step 9544 current loss 0.044308, current_train_items 305440.
I0304 19:32:36.348178 23128000471168 run.py:483] Algo bellman_ford step 9545 current loss 0.001559, current_train_items 305472.
I0304 19:32:36.364066 23128000471168 run.py:483] Algo bellman_ford step 9546 current loss 0.036205, current_train_items 305504.
I0304 19:32:36.386691 23128000471168 run.py:483] Algo bellman_ford step 9547 current loss 0.017106, current_train_items 305536.
I0304 19:32:36.418852 23128000471168 run.py:483] Algo bellman_ford step 9548 current loss 0.041197, current_train_items 305568.
I0304 19:32:36.453937 23128000471168 run.py:483] Algo bellman_ford step 9549 current loss 0.038380, current_train_items 305600.
I0304 19:32:36.474143 23128000471168 run.py:483] Algo bellman_ford step 9550 current loss 0.001594, current_train_items 305632.
I0304 19:32:36.482852 23128000471168 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0304 19:32:36.482964 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0304 19:32:36.500558 23128000471168 run.py:483] Algo bellman_ford step 9551 current loss 0.018648, current_train_items 305664.
I0304 19:32:36.525873 23128000471168 run.py:483] Algo bellman_ford step 9552 current loss 0.048592, current_train_items 305696.
I0304 19:32:36.560088 23128000471168 run.py:483] Algo bellman_ford step 9553 current loss 0.057199, current_train_items 305728.
I0304 19:32:36.593354 23128000471168 run.py:483] Algo bellman_ford step 9554 current loss 0.039052, current_train_items 305760.
I0304 19:32:36.613507 23128000471168 run.py:483] Algo bellman_ford step 9555 current loss 0.001894, current_train_items 305792.
I0304 19:32:36.629266 23128000471168 run.py:483] Algo bellman_ford step 9556 current loss 0.013515, current_train_items 305824.
I0304 19:32:36.654257 23128000471168 run.py:483] Algo bellman_ford step 9557 current loss 0.038743, current_train_items 305856.
I0304 19:32:36.686428 23128000471168 run.py:483] Algo bellman_ford step 9558 current loss 0.040768, current_train_items 305888.
I0304 19:32:36.719481 23128000471168 run.py:483] Algo bellman_ford step 9559 current loss 0.060509, current_train_items 305920.
I0304 19:32:36.740104 23128000471168 run.py:483] Algo bellman_ford step 9560 current loss 0.003987, current_train_items 305952.
I0304 19:32:36.756661 23128000471168 run.py:483] Algo bellman_ford step 9561 current loss 0.005325, current_train_items 305984.
I0304 19:32:36.780694 23128000471168 run.py:483] Algo bellman_ford step 9562 current loss 0.016685, current_train_items 306016.
I0304 19:32:36.812621 23128000471168 run.py:483] Algo bellman_ford step 9563 current loss 0.023654, current_train_items 306048.
I0304 19:32:36.846257 23128000471168 run.py:483] Algo bellman_ford step 9564 current loss 0.048903, current_train_items 306080.
I0304 19:32:36.866146 23128000471168 run.py:483] Algo bellman_ford step 9565 current loss 0.024878, current_train_items 306112.
I0304 19:32:36.883107 23128000471168 run.py:483] Algo bellman_ford step 9566 current loss 0.010980, current_train_items 306144.
I0304 19:32:36.908473 23128000471168 run.py:483] Algo bellman_ford step 9567 current loss 0.083577, current_train_items 306176.
I0304 19:32:36.941650 23128000471168 run.py:483] Algo bellman_ford step 9568 current loss 0.062135, current_train_items 306208.
I0304 19:32:36.975350 23128000471168 run.py:483] Algo bellman_ford step 9569 current loss 0.057648, current_train_items 306240.
I0304 19:32:36.995766 23128000471168 run.py:483] Algo bellman_ford step 9570 current loss 0.004012, current_train_items 306272.
I0304 19:32:37.012447 23128000471168 run.py:483] Algo bellman_ford step 9571 current loss 0.011480, current_train_items 306304.
I0304 19:32:37.037661 23128000471168 run.py:483] Algo bellman_ford step 9572 current loss 0.136525, current_train_items 306336.
I0304 19:32:37.069362 23128000471168 run.py:483] Algo bellman_ford step 9573 current loss 0.077545, current_train_items 306368.
I0304 19:32:37.103723 23128000471168 run.py:483] Algo bellman_ford step 9574 current loss 0.129484, current_train_items 306400.
I0304 19:32:37.124104 23128000471168 run.py:483] Algo bellman_ford step 9575 current loss 0.002283, current_train_items 306432.
I0304 19:32:37.141453 23128000471168 run.py:483] Algo bellman_ford step 9576 current loss 0.038983, current_train_items 306464.
I0304 19:32:37.166397 23128000471168 run.py:483] Algo bellman_ford step 9577 current loss 0.046978, current_train_items 306496.
I0304 19:32:37.198209 23128000471168 run.py:483] Algo bellman_ford step 9578 current loss 0.021317, current_train_items 306528.
I0304 19:32:37.232415 23128000471168 run.py:483] Algo bellman_ford step 9579 current loss 0.060595, current_train_items 306560.
I0304 19:32:37.252453 23128000471168 run.py:483] Algo bellman_ford step 9580 current loss 0.002111, current_train_items 306592.
I0304 19:32:37.268731 23128000471168 run.py:483] Algo bellman_ford step 9581 current loss 0.033663, current_train_items 306624.
I0304 19:32:37.294429 23128000471168 run.py:483] Algo bellman_ford step 9582 current loss 0.028282, current_train_items 306656.
I0304 19:32:37.327808 23128000471168 run.py:483] Algo bellman_ford step 9583 current loss 0.036294, current_train_items 306688.
I0304 19:32:37.362968 23128000471168 run.py:483] Algo bellman_ford step 9584 current loss 0.049400, current_train_items 306720.
I0304 19:32:37.383239 23128000471168 run.py:483] Algo bellman_ford step 9585 current loss 0.022611, current_train_items 306752.
I0304 19:32:37.399876 23128000471168 run.py:483] Algo bellman_ford step 9586 current loss 0.030550, current_train_items 306784.
I0304 19:32:37.424293 23128000471168 run.py:483] Algo bellman_ford step 9587 current loss 0.034538, current_train_items 306816.
I0304 19:32:37.457715 23128000471168 run.py:483] Algo bellman_ford step 9588 current loss 0.048603, current_train_items 306848.
I0304 19:32:37.492893 23128000471168 run.py:483] Algo bellman_ford step 9589 current loss 0.092261, current_train_items 306880.
I0304 19:32:37.513136 23128000471168 run.py:483] Algo bellman_ford step 9590 current loss 0.003635, current_train_items 306912.
I0304 19:32:37.529637 23128000471168 run.py:483] Algo bellman_ford step 9591 current loss 0.012237, current_train_items 306944.
I0304 19:32:37.554309 23128000471168 run.py:483] Algo bellman_ford step 9592 current loss 0.046773, current_train_items 306976.
I0304 19:32:37.586530 23128000471168 run.py:483] Algo bellman_ford step 9593 current loss 0.063229, current_train_items 307008.
I0304 19:32:37.619771 23128000471168 run.py:483] Algo bellman_ford step 9594 current loss 0.037516, current_train_items 307040.
I0304 19:32:37.640100 23128000471168 run.py:483] Algo bellman_ford step 9595 current loss 0.002607, current_train_items 307072.
I0304 19:32:37.656861 23128000471168 run.py:483] Algo bellman_ford step 9596 current loss 0.027020, current_train_items 307104.
I0304 19:32:37.681940 23128000471168 run.py:483] Algo bellman_ford step 9597 current loss 0.047284, current_train_items 307136.
I0304 19:32:37.713358 23128000471168 run.py:483] Algo bellman_ford step 9598 current loss 0.032077, current_train_items 307168.
I0304 19:32:37.747240 23128000471168 run.py:483] Algo bellman_ford step 9599 current loss 0.042785, current_train_items 307200.
I0304 19:32:37.768058 23128000471168 run.py:483] Algo bellman_ford step 9600 current loss 0.003418, current_train_items 307232.
I0304 19:32:37.775792 23128000471168 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0304 19:32:37.775902 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:37.793539 23128000471168 run.py:483] Algo bellman_ford step 9601 current loss 0.019807, current_train_items 307264.
I0304 19:32:37.819374 23128000471168 run.py:483] Algo bellman_ford step 9602 current loss 0.031017, current_train_items 307296.
I0304 19:32:37.851714 23128000471168 run.py:483] Algo bellman_ford step 9603 current loss 0.039739, current_train_items 307328.
I0304 19:32:37.887516 23128000471168 run.py:483] Algo bellman_ford step 9604 current loss 0.047120, current_train_items 307360.
I0304 19:32:37.908153 23128000471168 run.py:483] Algo bellman_ford step 9605 current loss 0.004846, current_train_items 307392.
I0304 19:32:37.924315 23128000471168 run.py:483] Algo bellman_ford step 9606 current loss 0.007156, current_train_items 307424.
I0304 19:32:37.948945 23128000471168 run.py:483] Algo bellman_ford step 9607 current loss 0.027832, current_train_items 307456.
I0304 19:32:37.981019 23128000471168 run.py:483] Algo bellman_ford step 9608 current loss 0.030140, current_train_items 307488.
I0304 19:32:38.015203 23128000471168 run.py:483] Algo bellman_ford step 9609 current loss 0.055680, current_train_items 307520.
I0304 19:32:38.035280 23128000471168 run.py:483] Algo bellman_ford step 9610 current loss 0.014309, current_train_items 307552.
I0304 19:32:38.051870 23128000471168 run.py:483] Algo bellman_ford step 9611 current loss 0.015544, current_train_items 307584.
I0304 19:32:38.077310 23128000471168 run.py:483] Algo bellman_ford step 9612 current loss 0.037304, current_train_items 307616.
I0304 19:32:38.109745 23128000471168 run.py:483] Algo bellman_ford step 9613 current loss 0.047270, current_train_items 307648.
I0304 19:32:38.143890 23128000471168 run.py:483] Algo bellman_ford step 9614 current loss 0.053277, current_train_items 307680.
I0304 19:32:38.164475 23128000471168 run.py:483] Algo bellman_ford step 9615 current loss 0.002574, current_train_items 307712.
I0304 19:32:38.181067 23128000471168 run.py:483] Algo bellman_ford step 9616 current loss 0.017263, current_train_items 307744.
I0304 19:32:38.205703 23128000471168 run.py:483] Algo bellman_ford step 9617 current loss 0.044564, current_train_items 307776.
I0304 19:32:38.238314 23128000471168 run.py:483] Algo bellman_ford step 9618 current loss 0.033037, current_train_items 307808.
I0304 19:32:38.272758 23128000471168 run.py:483] Algo bellman_ford step 9619 current loss 0.039280, current_train_items 307840.
I0304 19:32:38.292822 23128000471168 run.py:483] Algo bellman_ford step 9620 current loss 0.001668, current_train_items 307872.
I0304 19:32:38.309121 23128000471168 run.py:483] Algo bellman_ford step 9621 current loss 0.012415, current_train_items 307904.
I0304 19:32:38.334399 23128000471168 run.py:483] Algo bellman_ford step 9622 current loss 0.034671, current_train_items 307936.
I0304 19:32:38.366990 23128000471168 run.py:483] Algo bellman_ford step 9623 current loss 0.047901, current_train_items 307968.
I0304 19:32:38.401481 23128000471168 run.py:483] Algo bellman_ford step 9624 current loss 0.079110, current_train_items 308000.
I0304 19:32:38.422089 23128000471168 run.py:483] Algo bellman_ford step 9625 current loss 0.004457, current_train_items 308032.
I0304 19:32:38.438530 23128000471168 run.py:483] Algo bellman_ford step 9626 current loss 0.006157, current_train_items 308064.
I0304 19:32:38.463040 23128000471168 run.py:483] Algo bellman_ford step 9627 current loss 0.032699, current_train_items 308096.
I0304 19:32:38.494881 23128000471168 run.py:483] Algo bellman_ford step 9628 current loss 0.026770, current_train_items 308128.
I0304 19:32:38.530307 23128000471168 run.py:483] Algo bellman_ford step 9629 current loss 0.051100, current_train_items 308160.
I0304 19:32:38.550876 23128000471168 run.py:483] Algo bellman_ford step 9630 current loss 0.027668, current_train_items 308192.
I0304 19:32:38.567739 23128000471168 run.py:483] Algo bellman_ford step 9631 current loss 0.007807, current_train_items 308224.
I0304 19:32:38.593356 23128000471168 run.py:483] Algo bellman_ford step 9632 current loss 0.114768, current_train_items 308256.
I0304 19:32:38.625092 23128000471168 run.py:483] Algo bellman_ford step 9633 current loss 0.046991, current_train_items 308288.
I0304 19:32:38.660075 23128000471168 run.py:483] Algo bellman_ford step 9634 current loss 0.102529, current_train_items 308320.
I0304 19:32:38.680268 23128000471168 run.py:483] Algo bellman_ford step 9635 current loss 0.002000, current_train_items 308352.
I0304 19:32:38.696779 23128000471168 run.py:483] Algo bellman_ford step 9636 current loss 0.004938, current_train_items 308384.
I0304 19:32:38.721823 23128000471168 run.py:483] Algo bellman_ford step 9637 current loss 0.020425, current_train_items 308416.
I0304 19:32:38.755223 23128000471168 run.py:483] Algo bellman_ford step 9638 current loss 0.029673, current_train_items 308448.
I0304 19:32:38.789470 23128000471168 run.py:483] Algo bellman_ford step 9639 current loss 0.104292, current_train_items 308480.
I0304 19:32:38.809630 23128000471168 run.py:483] Algo bellman_ford step 9640 current loss 0.001350, current_train_items 308512.
I0304 19:32:38.825760 23128000471168 run.py:483] Algo bellman_ford step 9641 current loss 0.004366, current_train_items 308544.
I0304 19:32:38.851109 23128000471168 run.py:483] Algo bellman_ford step 9642 current loss 0.043569, current_train_items 308576.
I0304 19:32:38.885215 23128000471168 run.py:483] Algo bellman_ford step 9643 current loss 0.028388, current_train_items 308608.
I0304 19:32:38.917707 23128000471168 run.py:483] Algo bellman_ford step 9644 current loss 0.039568, current_train_items 308640.
I0304 19:32:38.937789 23128000471168 run.py:483] Algo bellman_ford step 9645 current loss 0.021994, current_train_items 308672.
I0304 19:32:38.954405 23128000471168 run.py:483] Algo bellman_ford step 9646 current loss 0.028400, current_train_items 308704.
I0304 19:32:38.978560 23128000471168 run.py:483] Algo bellman_ford step 9647 current loss 0.013299, current_train_items 308736.
I0304 19:32:39.011796 23128000471168 run.py:483] Algo bellman_ford step 9648 current loss 0.026517, current_train_items 308768.
I0304 19:32:39.046743 23128000471168 run.py:483] Algo bellman_ford step 9649 current loss 0.035893, current_train_items 308800.
I0304 19:32:39.066835 23128000471168 run.py:483] Algo bellman_ford step 9650 current loss 0.001954, current_train_items 308832.
I0304 19:32:39.074977 23128000471168 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0304 19:32:39.075136 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:39.092461 23128000471168 run.py:483] Algo bellman_ford step 9651 current loss 0.021223, current_train_items 308864.
I0304 19:32:39.119089 23128000471168 run.py:483] Algo bellman_ford step 9652 current loss 0.054574, current_train_items 308896.
I0304 19:32:39.151955 23128000471168 run.py:483] Algo bellman_ford step 9653 current loss 0.116431, current_train_items 308928.
I0304 19:32:39.188221 23128000471168 run.py:483] Algo bellman_ford step 9654 current loss 0.051807, current_train_items 308960.
I0304 19:32:39.208773 23128000471168 run.py:483] Algo bellman_ford step 9655 current loss 0.002456, current_train_items 308992.
I0304 19:32:39.225141 23128000471168 run.py:483] Algo bellman_ford step 9656 current loss 0.013055, current_train_items 309024.
I0304 19:32:39.249805 23128000471168 run.py:483] Algo bellman_ford step 9657 current loss 0.027712, current_train_items 309056.
I0304 19:32:39.283483 23128000471168 run.py:483] Algo bellman_ford step 9658 current loss 0.069576, current_train_items 309088.
I0304 19:32:39.316495 23128000471168 run.py:483] Algo bellman_ford step 9659 current loss 0.039138, current_train_items 309120.
I0304 19:32:39.336927 23128000471168 run.py:483] Algo bellman_ford step 9660 current loss 0.002936, current_train_items 309152.
I0304 19:32:39.353929 23128000471168 run.py:483] Algo bellman_ford step 9661 current loss 0.007361, current_train_items 309184.
I0304 19:32:39.378315 23128000471168 run.py:483] Algo bellman_ford step 9662 current loss 0.102195, current_train_items 309216.
I0304 19:32:39.410881 23128000471168 run.py:483] Algo bellman_ford step 9663 current loss 0.047458, current_train_items 309248.
I0304 19:32:39.445483 23128000471168 run.py:483] Algo bellman_ford step 9664 current loss 0.072127, current_train_items 309280.
I0304 19:32:39.465833 23128000471168 run.py:483] Algo bellman_ford step 9665 current loss 0.004508, current_train_items 309312.
I0304 19:32:39.482470 23128000471168 run.py:483] Algo bellman_ford step 9666 current loss 0.014406, current_train_items 309344.
I0304 19:32:39.506522 23128000471168 run.py:483] Algo bellman_ford step 9667 current loss 0.038090, current_train_items 309376.
I0304 19:32:39.538797 23128000471168 run.py:483] Algo bellman_ford step 9668 current loss 0.058759, current_train_items 309408.
I0304 19:32:39.572914 23128000471168 run.py:483] Algo bellman_ford step 9669 current loss 0.067517, current_train_items 309440.
I0304 19:32:39.593631 23128000471168 run.py:483] Algo bellman_ford step 9670 current loss 0.001219, current_train_items 309472.
I0304 19:32:39.610428 23128000471168 run.py:483] Algo bellman_ford step 9671 current loss 0.014439, current_train_items 309504.
I0304 19:32:39.634871 23128000471168 run.py:483] Algo bellman_ford step 9672 current loss 0.021016, current_train_items 309536.
I0304 19:32:39.666789 23128000471168 run.py:483] Algo bellman_ford step 9673 current loss 0.022190, current_train_items 309568.
I0304 19:32:39.701134 23128000471168 run.py:483] Algo bellman_ford step 9674 current loss 0.049982, current_train_items 309600.
I0304 19:32:39.721513 23128000471168 run.py:483] Algo bellman_ford step 9675 current loss 0.001285, current_train_items 309632.
I0304 19:32:39.738508 23128000471168 run.py:483] Algo bellman_ford step 9676 current loss 0.011897, current_train_items 309664.
I0304 19:32:39.763145 23128000471168 run.py:483] Algo bellman_ford step 9677 current loss 0.029737, current_train_items 309696.
I0304 19:32:39.794413 23128000471168 run.py:483] Algo bellman_ford step 9678 current loss 0.034250, current_train_items 309728.
I0304 19:32:39.829433 23128000471168 run.py:483] Algo bellman_ford step 9679 current loss 0.051313, current_train_items 309760.
I0304 19:32:39.849784 23128000471168 run.py:483] Algo bellman_ford step 9680 current loss 0.001567, current_train_items 309792.
I0304 19:32:39.866573 23128000471168 run.py:483] Algo bellman_ford step 9681 current loss 0.018246, current_train_items 309824.
I0304 19:32:39.891439 23128000471168 run.py:483] Algo bellman_ford step 9682 current loss 0.008681, current_train_items 309856.
I0304 19:32:39.923318 23128000471168 run.py:483] Algo bellman_ford step 9683 current loss 0.032869, current_train_items 309888.
I0304 19:32:39.956563 23128000471168 run.py:483] Algo bellman_ford step 9684 current loss 0.032459, current_train_items 309920.
I0304 19:32:39.976962 23128000471168 run.py:483] Algo bellman_ford step 9685 current loss 0.001015, current_train_items 309952.
I0304 19:32:39.993576 23128000471168 run.py:483] Algo bellman_ford step 9686 current loss 0.009955, current_train_items 309984.
I0304 19:32:40.018161 23128000471168 run.py:483] Algo bellman_ford step 9687 current loss 0.040019, current_train_items 310016.
I0304 19:32:40.050625 23128000471168 run.py:483] Algo bellman_ford step 9688 current loss 0.055361, current_train_items 310048.
I0304 19:32:40.085078 23128000471168 run.py:483] Algo bellman_ford step 9689 current loss 0.053773, current_train_items 310080.
I0304 19:32:40.105531 23128000471168 run.py:483] Algo bellman_ford step 9690 current loss 0.008938, current_train_items 310112.
I0304 19:32:40.121998 23128000471168 run.py:483] Algo bellman_ford step 9691 current loss 0.002647, current_train_items 310144.
I0304 19:32:40.146456 23128000471168 run.py:483] Algo bellman_ford step 9692 current loss 0.017542, current_train_items 310176.
I0304 19:32:40.178443 23128000471168 run.py:483] Algo bellman_ford step 9693 current loss 0.054137, current_train_items 310208.
I0304 19:32:40.212590 23128000471168 run.py:483] Algo bellman_ford step 9694 current loss 0.030905, current_train_items 310240.
I0304 19:32:40.232602 23128000471168 run.py:483] Algo bellman_ford step 9695 current loss 0.001195, current_train_items 310272.
I0304 19:32:40.249089 23128000471168 run.py:483] Algo bellman_ford step 9696 current loss 0.041582, current_train_items 310304.
I0304 19:32:40.272943 23128000471168 run.py:483] Algo bellman_ford step 9697 current loss 0.012719, current_train_items 310336.
I0304 19:32:40.307923 23128000471168 run.py:483] Algo bellman_ford step 9698 current loss 0.040254, current_train_items 310368.
I0304 19:32:40.343916 23128000471168 run.py:483] Algo bellman_ford step 9699 current loss 0.041446, current_train_items 310400.
I0304 19:32:40.364688 23128000471168 run.py:483] Algo bellman_ford step 9700 current loss 0.006740, current_train_items 310432.
I0304 19:32:40.372669 23128000471168 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0304 19:32:40.372777 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:40.389842 23128000471168 run.py:483] Algo bellman_ford step 9701 current loss 0.007197, current_train_items 310464.
I0304 19:32:40.415582 23128000471168 run.py:483] Algo bellman_ford step 9702 current loss 0.045462, current_train_items 310496.
I0304 19:32:40.449722 23128000471168 run.py:483] Algo bellman_ford step 9703 current loss 0.049992, current_train_items 310528.
I0304 19:32:40.483772 23128000471168 run.py:483] Algo bellman_ford step 9704 current loss 0.042872, current_train_items 310560.
I0304 19:32:40.504291 23128000471168 run.py:483] Algo bellman_ford step 9705 current loss 0.002173, current_train_items 310592.
I0304 19:32:40.520610 23128000471168 run.py:483] Algo bellman_ford step 9706 current loss 0.024597, current_train_items 310624.
I0304 19:32:40.546486 23128000471168 run.py:483] Algo bellman_ford step 9707 current loss 0.021621, current_train_items 310656.
I0304 19:32:40.579822 23128000471168 run.py:483] Algo bellman_ford step 9708 current loss 0.049704, current_train_items 310688.
I0304 19:32:40.612110 23128000471168 run.py:483] Algo bellman_ford step 9709 current loss 0.043989, current_train_items 310720.
I0304 19:32:40.632302 23128000471168 run.py:483] Algo bellman_ford step 9710 current loss 0.001827, current_train_items 310752.
I0304 19:32:40.648837 23128000471168 run.py:483] Algo bellman_ford step 9711 current loss 0.007805, current_train_items 310784.
I0304 19:32:40.673719 23128000471168 run.py:483] Algo bellman_ford step 9712 current loss 0.031092, current_train_items 310816.
I0304 19:32:40.706789 23128000471168 run.py:483] Algo bellman_ford step 9713 current loss 0.039228, current_train_items 310848.
I0304 19:32:40.741823 23128000471168 run.py:483] Algo bellman_ford step 9714 current loss 0.047474, current_train_items 310880.
I0304 19:32:40.762166 23128000471168 run.py:483] Algo bellman_ford step 9715 current loss 0.002469, current_train_items 310912.
I0304 19:32:40.779121 23128000471168 run.py:483] Algo bellman_ford step 9716 current loss 0.028791, current_train_items 310944.
I0304 19:32:40.802542 23128000471168 run.py:483] Algo bellman_ford step 9717 current loss 0.016334, current_train_items 310976.
I0304 19:32:40.835505 23128000471168 run.py:483] Algo bellman_ford step 9718 current loss 0.063436, current_train_items 311008.
I0304 19:32:40.866981 23128000471168 run.py:483] Algo bellman_ford step 9719 current loss 0.096415, current_train_items 311040.
I0304 19:32:40.887392 23128000471168 run.py:483] Algo bellman_ford step 9720 current loss 0.011575, current_train_items 311072.
I0304 19:32:40.903461 23128000471168 run.py:483] Algo bellman_ford step 9721 current loss 0.004105, current_train_items 311104.
I0304 19:32:40.927468 23128000471168 run.py:483] Algo bellman_ford step 9722 current loss 0.029378, current_train_items 311136.
I0304 19:32:40.960266 23128000471168 run.py:483] Algo bellman_ford step 9723 current loss 0.033873, current_train_items 311168.
I0304 19:32:40.994817 23128000471168 run.py:483] Algo bellman_ford step 9724 current loss 0.058645, current_train_items 311200.
I0304 19:32:41.015126 23128000471168 run.py:483] Algo bellman_ford step 9725 current loss 0.004617, current_train_items 311232.
I0304 19:32:41.031296 23128000471168 run.py:483] Algo bellman_ford step 9726 current loss 0.011648, current_train_items 311264.
I0304 19:32:41.055147 23128000471168 run.py:483] Algo bellman_ford step 9727 current loss 0.009928, current_train_items 311296.
I0304 19:32:41.088392 23128000471168 run.py:483] Algo bellman_ford step 9728 current loss 0.029975, current_train_items 311328.
I0304 19:32:41.123516 23128000471168 run.py:483] Algo bellman_ford step 9729 current loss 0.068021, current_train_items 311360.
I0304 19:32:41.144174 23128000471168 run.py:483] Algo bellman_ford step 9730 current loss 0.015253, current_train_items 311392.
I0304 19:32:41.160882 23128000471168 run.py:483] Algo bellman_ford step 9731 current loss 0.018933, current_train_items 311424.
I0304 19:32:41.185125 23128000471168 run.py:483] Algo bellman_ford step 9732 current loss 0.040294, current_train_items 311456.
I0304 19:32:41.217251 23128000471168 run.py:483] Algo bellman_ford step 9733 current loss 0.108305, current_train_items 311488.
I0304 19:32:41.250662 23128000471168 run.py:483] Algo bellman_ford step 9734 current loss 0.076203, current_train_items 311520.
I0304 19:32:41.270914 23128000471168 run.py:483] Algo bellman_ford step 9735 current loss 0.005555, current_train_items 311552.
I0304 19:32:41.287374 23128000471168 run.py:483] Algo bellman_ford step 9736 current loss 0.021854, current_train_items 311584.
I0304 19:32:41.311917 23128000471168 run.py:483] Algo bellman_ford step 9737 current loss 0.029173, current_train_items 311616.
I0304 19:32:41.346089 23128000471168 run.py:483] Algo bellman_ford step 9738 current loss 0.052315, current_train_items 311648.
I0304 19:32:41.381197 23128000471168 run.py:483] Algo bellman_ford step 9739 current loss 0.048838, current_train_items 311680.
I0304 19:32:41.401286 23128000471168 run.py:483] Algo bellman_ford step 9740 current loss 0.002671, current_train_items 311712.
I0304 19:32:41.418571 23128000471168 run.py:483] Algo bellman_ford step 9741 current loss 0.011991, current_train_items 311744.
I0304 19:32:41.444975 23128000471168 run.py:483] Algo bellman_ford step 9742 current loss 0.037489, current_train_items 311776.
I0304 19:32:41.478222 23128000471168 run.py:483] Algo bellman_ford step 9743 current loss 0.030037, current_train_items 311808.
I0304 19:32:41.512744 23128000471168 run.py:483] Algo bellman_ford step 9744 current loss 0.039893, current_train_items 311840.
I0304 19:32:41.533074 23128000471168 run.py:483] Algo bellman_ford step 9745 current loss 0.008071, current_train_items 311872.
I0304 19:32:41.550251 23128000471168 run.py:483] Algo bellman_ford step 9746 current loss 0.009461, current_train_items 311904.
I0304 19:32:41.574556 23128000471168 run.py:483] Algo bellman_ford step 9747 current loss 0.018059, current_train_items 311936.
I0304 19:32:41.605152 23128000471168 run.py:483] Algo bellman_ford step 9748 current loss 0.026766, current_train_items 311968.
I0304 19:32:41.638106 23128000471168 run.py:483] Algo bellman_ford step 9749 current loss 0.037529, current_train_items 312000.
I0304 19:32:41.657948 23128000471168 run.py:483] Algo bellman_ford step 9750 current loss 0.001210, current_train_items 312032.
I0304 19:32:41.666299 23128000471168 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0304 19:32:41.666408 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:41.683416 23128000471168 run.py:483] Algo bellman_ford step 9751 current loss 0.012078, current_train_items 312064.
I0304 19:32:41.709799 23128000471168 run.py:483] Algo bellman_ford step 9752 current loss 0.037534, current_train_items 312096.
I0304 19:32:41.742117 23128000471168 run.py:483] Algo bellman_ford step 9753 current loss 0.027827, current_train_items 312128.
I0304 19:32:41.775693 23128000471168 run.py:483] Algo bellman_ford step 9754 current loss 0.050984, current_train_items 312160.
I0304 19:32:41.796075 23128000471168 run.py:483] Algo bellman_ford step 9755 current loss 0.007567, current_train_items 312192.
I0304 19:32:41.812987 23128000471168 run.py:483] Algo bellman_ford step 9756 current loss 0.019700, current_train_items 312224.
I0304 19:32:41.836873 23128000471168 run.py:483] Algo bellman_ford step 9757 current loss 0.018474, current_train_items 312256.
I0304 19:32:41.870234 23128000471168 run.py:483] Algo bellman_ford step 9758 current loss 0.057281, current_train_items 312288.
I0304 19:32:41.904200 23128000471168 run.py:483] Algo bellman_ford step 9759 current loss 0.048209, current_train_items 312320.
I0304 19:32:41.924901 23128000471168 run.py:483] Algo bellman_ford step 9760 current loss 0.004127, current_train_items 312352.
I0304 19:32:41.942277 23128000471168 run.py:483] Algo bellman_ford step 9761 current loss 0.018603, current_train_items 312384.
I0304 19:32:41.967092 23128000471168 run.py:483] Algo bellman_ford step 9762 current loss 0.027465, current_train_items 312416.
I0304 19:32:42.000305 23128000471168 run.py:483] Algo bellman_ford step 9763 current loss 0.063941, current_train_items 312448.
I0304 19:32:42.032076 23128000471168 run.py:483] Algo bellman_ford step 9764 current loss 0.042186, current_train_items 312480.
I0304 19:32:42.052150 23128000471168 run.py:483] Algo bellman_ford step 9765 current loss 0.013874, current_train_items 312512.
I0304 19:32:42.068434 23128000471168 run.py:483] Algo bellman_ford step 9766 current loss 0.013634, current_train_items 312544.
I0304 19:32:42.093611 23128000471168 run.py:483] Algo bellman_ford step 9767 current loss 0.052390, current_train_items 312576.
I0304 19:32:42.125927 23128000471168 run.py:483] Algo bellman_ford step 9768 current loss 0.047667, current_train_items 312608.
I0304 19:32:42.160486 23128000471168 run.py:483] Algo bellman_ford step 9769 current loss 0.049504, current_train_items 312640.
I0304 19:32:42.181273 23128000471168 run.py:483] Algo bellman_ford step 9770 current loss 0.001555, current_train_items 312672.
I0304 19:32:42.197805 23128000471168 run.py:483] Algo bellman_ford step 9771 current loss 0.018650, current_train_items 312704.
I0304 19:32:42.221770 23128000471168 run.py:483] Algo bellman_ford step 9772 current loss 0.041175, current_train_items 312736.
I0304 19:32:42.255540 23128000471168 run.py:483] Algo bellman_ford step 9773 current loss 0.051896, current_train_items 312768.
I0304 19:32:42.289573 23128000471168 run.py:483] Algo bellman_ford step 9774 current loss 0.064852, current_train_items 312800.
I0304 19:32:42.309857 23128000471168 run.py:483] Algo bellman_ford step 9775 current loss 0.001704, current_train_items 312832.
I0304 19:32:42.326737 23128000471168 run.py:483] Algo bellman_ford step 9776 current loss 0.003555, current_train_items 312864.
I0304 19:32:42.351845 23128000471168 run.py:483] Algo bellman_ford step 9777 current loss 0.028695, current_train_items 312896.
I0304 19:32:42.385063 23128000471168 run.py:483] Algo bellman_ford step 9778 current loss 0.033478, current_train_items 312928.
I0304 19:32:42.419403 23128000471168 run.py:483] Algo bellman_ford step 9779 current loss 0.028641, current_train_items 312960.
I0304 19:32:42.439669 23128000471168 run.py:483] Algo bellman_ford step 9780 current loss 0.003111, current_train_items 312992.
I0304 19:32:42.456340 23128000471168 run.py:483] Algo bellman_ford step 9781 current loss 0.004341, current_train_items 313024.
I0304 19:32:42.480276 23128000471168 run.py:483] Algo bellman_ford step 9782 current loss 0.019681, current_train_items 313056.
I0304 19:32:42.514553 23128000471168 run.py:483] Algo bellman_ford step 9783 current loss 0.070514, current_train_items 313088.
I0304 19:32:42.547243 23128000471168 run.py:483] Algo bellman_ford step 9784 current loss 0.030723, current_train_items 313120.
I0304 19:32:42.568055 23128000471168 run.py:483] Algo bellman_ford step 9785 current loss 0.002943, current_train_items 313152.
I0304 19:32:42.585097 23128000471168 run.py:483] Algo bellman_ford step 9786 current loss 0.022549, current_train_items 313184.
I0304 19:32:42.609156 23128000471168 run.py:483] Algo bellman_ford step 9787 current loss 0.022396, current_train_items 313216.
I0304 19:32:42.641273 23128000471168 run.py:483] Algo bellman_ford step 9788 current loss 0.025295, current_train_items 313248.
I0304 19:32:42.675193 23128000471168 run.py:483] Algo bellman_ford step 9789 current loss 0.046387, current_train_items 313280.
I0304 19:32:42.695651 23128000471168 run.py:483] Algo bellman_ford step 9790 current loss 0.007081, current_train_items 313312.
I0304 19:32:42.712088 23128000471168 run.py:483] Algo bellman_ford step 9791 current loss 0.021380, current_train_items 313344.
I0304 19:32:42.735907 23128000471168 run.py:483] Algo bellman_ford step 9792 current loss 0.035081, current_train_items 313376.
I0304 19:32:42.768239 23128000471168 run.py:483] Algo bellman_ford step 9793 current loss 0.053443, current_train_items 313408.
I0304 19:32:42.800648 23128000471168 run.py:483] Algo bellman_ford step 9794 current loss 0.033867, current_train_items 313440.
I0304 19:32:42.821110 23128000471168 run.py:483] Algo bellman_ford step 9795 current loss 0.004101, current_train_items 313472.
I0304 19:32:42.838032 23128000471168 run.py:483] Algo bellman_ford step 9796 current loss 0.011835, current_train_items 313504.
I0304 19:32:42.862523 23128000471168 run.py:483] Algo bellman_ford step 9797 current loss 0.088335, current_train_items 313536.
I0304 19:32:42.894430 23128000471168 run.py:483] Algo bellman_ford step 9798 current loss 0.044024, current_train_items 313568.
I0304 19:32:42.927684 23128000471168 run.py:483] Algo bellman_ford step 9799 current loss 0.114645, current_train_items 313600.
I0304 19:32:42.948080 23128000471168 run.py:483] Algo bellman_ford step 9800 current loss 0.025780, current_train_items 313632.
I0304 19:32:42.956073 23128000471168 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0304 19:32:42.956182 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:32:42.973063 23128000471168 run.py:483] Algo bellman_ford step 9801 current loss 0.025728, current_train_items 313664.
I0304 19:32:42.998282 23128000471168 run.py:483] Algo bellman_ford step 9802 current loss 0.045762, current_train_items 313696.
I0304 19:32:43.031813 23128000471168 run.py:483] Algo bellman_ford step 9803 current loss 0.131646, current_train_items 313728.
I0304 19:32:43.065052 23128000471168 run.py:483] Algo bellman_ford step 9804 current loss 0.111302, current_train_items 313760.
I0304 19:32:43.085657 23128000471168 run.py:483] Algo bellman_ford step 9805 current loss 0.003414, current_train_items 313792.
I0304 19:32:43.102077 23128000471168 run.py:483] Algo bellman_ford step 9806 current loss 0.010971, current_train_items 313824.
I0304 19:32:43.126109 23128000471168 run.py:483] Algo bellman_ford step 9807 current loss 0.038890, current_train_items 313856.
I0304 19:32:43.159046 23128000471168 run.py:483] Algo bellman_ford step 9808 current loss 0.024196, current_train_items 313888.
I0304 19:32:43.195266 23128000471168 run.py:483] Algo bellman_ford step 9809 current loss 0.045509, current_train_items 313920.
I0304 19:32:43.215328 23128000471168 run.py:483] Algo bellman_ford step 9810 current loss 0.002257, current_train_items 313952.
I0304 19:32:43.231533 23128000471168 run.py:483] Algo bellman_ford step 9811 current loss 0.019590, current_train_items 313984.
I0304 19:32:43.257077 23128000471168 run.py:483] Algo bellman_ford step 9812 current loss 0.057803, current_train_items 314016.
I0304 19:32:43.287460 23128000471168 run.py:483] Algo bellman_ford step 9813 current loss 0.030007, current_train_items 314048.
I0304 19:32:43.321918 23128000471168 run.py:483] Algo bellman_ford step 9814 current loss 0.047892, current_train_items 314080.
I0304 19:32:43.341962 23128000471168 run.py:483] Algo bellman_ford step 9815 current loss 0.002158, current_train_items 314112.
I0304 19:32:43.358768 23128000471168 run.py:483] Algo bellman_ford step 9816 current loss 0.023357, current_train_items 314144.
I0304 19:32:43.382839 23128000471168 run.py:483] Algo bellman_ford step 9817 current loss 0.020331, current_train_items 314176.
I0304 19:32:43.416734 23128000471168 run.py:483] Algo bellman_ford step 9818 current loss 0.046082, current_train_items 314208.
I0304 19:32:43.450291 23128000471168 run.py:483] Algo bellman_ford step 9819 current loss 0.048772, current_train_items 314240.
I0304 19:32:43.470541 23128000471168 run.py:483] Algo bellman_ford step 9820 current loss 0.001899, current_train_items 314272.
I0304 19:32:43.486911 23128000471168 run.py:483] Algo bellman_ford step 9821 current loss 0.043134, current_train_items 314304.
I0304 19:32:43.511925 23128000471168 run.py:483] Algo bellman_ford step 9822 current loss 0.046093, current_train_items 314336.
I0304 19:32:43.543927 23128000471168 run.py:483] Algo bellman_ford step 9823 current loss 0.038345, current_train_items 314368.
I0304 19:32:43.577568 23128000471168 run.py:483] Algo bellman_ford step 9824 current loss 0.045805, current_train_items 314400.
I0304 19:32:43.597963 23128000471168 run.py:483] Algo bellman_ford step 9825 current loss 0.001786, current_train_items 314432.
I0304 19:32:43.614330 23128000471168 run.py:483] Algo bellman_ford step 9826 current loss 0.019591, current_train_items 314464.
I0304 19:32:43.638660 23128000471168 run.py:483] Algo bellman_ford step 9827 current loss 0.043345, current_train_items 314496.
I0304 19:32:43.670045 23128000471168 run.py:483] Algo bellman_ford step 9828 current loss 0.055961, current_train_items 314528.
I0304 19:32:43.705222 23128000471168 run.py:483] Algo bellman_ford step 9829 current loss 0.040296, current_train_items 314560.
I0304 19:32:43.724880 23128000471168 run.py:483] Algo bellman_ford step 9830 current loss 0.001597, current_train_items 314592.
I0304 19:32:43.741883 23128000471168 run.py:483] Algo bellman_ford step 9831 current loss 0.021848, current_train_items 314624.
I0304 19:32:43.766276 23128000471168 run.py:483] Algo bellman_ford step 9832 current loss 0.064390, current_train_items 314656.
I0304 19:32:43.799349 23128000471168 run.py:483] Algo bellman_ford step 9833 current loss 0.121260, current_train_items 314688.
I0304 19:32:43.831963 23128000471168 run.py:483] Algo bellman_ford step 9834 current loss 0.074172, current_train_items 314720.
I0304 19:32:43.852052 23128000471168 run.py:483] Algo bellman_ford step 9835 current loss 0.008693, current_train_items 314752.
I0304 19:32:43.868594 23128000471168 run.py:483] Algo bellman_ford step 9836 current loss 0.007170, current_train_items 314784.
I0304 19:32:43.893265 23128000471168 run.py:483] Algo bellman_ford step 9837 current loss 0.021522, current_train_items 314816.
I0304 19:32:43.926662 23128000471168 run.py:483] Algo bellman_ford step 9838 current loss 0.022926, current_train_items 314848.
I0304 19:32:43.962016 23128000471168 run.py:483] Algo bellman_ford step 9839 current loss 0.053901, current_train_items 314880.
I0304 19:32:43.982330 23128000471168 run.py:483] Algo bellman_ford step 9840 current loss 0.010336, current_train_items 314912.
I0304 19:32:43.998756 23128000471168 run.py:483] Algo bellman_ford step 9841 current loss 0.017509, current_train_items 314944.
I0304 19:32:44.023816 23128000471168 run.py:483] Algo bellman_ford step 9842 current loss 0.040276, current_train_items 314976.
I0304 19:32:44.056068 23128000471168 run.py:483] Algo bellman_ford step 9843 current loss 0.021961, current_train_items 315008.
I0304 19:32:44.088343 23128000471168 run.py:483] Algo bellman_ford step 9844 current loss 0.046254, current_train_items 315040.
I0304 19:32:44.108591 23128000471168 run.py:483] Algo bellman_ford step 9845 current loss 0.001821, current_train_items 315072.
I0304 19:32:44.125310 23128000471168 run.py:483] Algo bellman_ford step 9846 current loss 0.016057, current_train_items 315104.
I0304 19:32:44.150512 23128000471168 run.py:483] Algo bellman_ford step 9847 current loss 0.024081, current_train_items 315136.
I0304 19:32:44.182796 23128000471168 run.py:483] Algo bellman_ford step 9848 current loss 0.067070, current_train_items 315168.
I0304 19:32:44.216824 23128000471168 run.py:483] Algo bellman_ford step 9849 current loss 0.043632, current_train_items 315200.
I0304 19:32:44.237307 23128000471168 run.py:483] Algo bellman_ford step 9850 current loss 0.002387, current_train_items 315232.
I0304 19:32:44.245581 23128000471168 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0304 19:32:44.245688 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:44.263546 23128000471168 run.py:483] Algo bellman_ford step 9851 current loss 0.033835, current_train_items 315264.
I0304 19:32:44.289838 23128000471168 run.py:483] Algo bellman_ford step 9852 current loss 0.045541, current_train_items 315296.
I0304 19:32:44.321953 23128000471168 run.py:483] Algo bellman_ford step 9853 current loss 0.051029, current_train_items 315328.
I0304 19:32:44.356575 23128000471168 run.py:483] Algo bellman_ford step 9854 current loss 0.043212, current_train_items 315360.
I0304 19:32:44.376546 23128000471168 run.py:483] Algo bellman_ford step 9855 current loss 0.001712, current_train_items 315392.
I0304 19:32:44.393121 23128000471168 run.py:483] Algo bellman_ford step 9856 current loss 0.015531, current_train_items 315424.
I0304 19:32:44.418209 23128000471168 run.py:483] Algo bellman_ford step 9857 current loss 0.055525, current_train_items 315456.
I0304 19:32:44.450042 23128000471168 run.py:483] Algo bellman_ford step 9858 current loss 0.051458, current_train_items 315488.
I0304 19:32:44.483882 23128000471168 run.py:483] Algo bellman_ford step 9859 current loss 0.088811, current_train_items 315520.
I0304 19:32:44.504736 23128000471168 run.py:483] Algo bellman_ford step 9860 current loss 0.003093, current_train_items 315552.
I0304 19:32:44.521410 23128000471168 run.py:483] Algo bellman_ford step 9861 current loss 0.016050, current_train_items 315584.
I0304 19:32:44.546363 23128000471168 run.py:483] Algo bellman_ford step 9862 current loss 0.029998, current_train_items 315616.
I0304 19:32:44.578827 23128000471168 run.py:483] Algo bellman_ford step 9863 current loss 0.084113, current_train_items 315648.
I0304 19:32:44.613031 23128000471168 run.py:483] Algo bellman_ford step 9864 current loss 0.065633, current_train_items 315680.
I0304 19:32:44.633078 23128000471168 run.py:483] Algo bellman_ford step 9865 current loss 0.001161, current_train_items 315712.
I0304 19:32:44.649344 23128000471168 run.py:483] Algo bellman_ford step 9866 current loss 0.004888, current_train_items 315744.
I0304 19:32:44.673845 23128000471168 run.py:483] Algo bellman_ford step 9867 current loss 0.028260, current_train_items 315776.
I0304 19:32:44.706680 23128000471168 run.py:483] Algo bellman_ford step 9868 current loss 0.040683, current_train_items 315808.
I0304 19:32:44.741087 23128000471168 run.py:483] Algo bellman_ford step 9869 current loss 0.047376, current_train_items 315840.
I0304 19:32:44.761439 23128000471168 run.py:483] Algo bellman_ford step 9870 current loss 0.003655, current_train_items 315872.
I0304 19:32:44.778639 23128000471168 run.py:483] Algo bellman_ford step 9871 current loss 0.025105, current_train_items 315904.
I0304 19:32:44.803212 23128000471168 run.py:483] Algo bellman_ford step 9872 current loss 0.043513, current_train_items 315936.
I0304 19:32:44.836102 23128000471168 run.py:483] Algo bellman_ford step 9873 current loss 0.037554, current_train_items 315968.
I0304 19:32:44.869065 23128000471168 run.py:483] Algo bellman_ford step 9874 current loss 0.083706, current_train_items 316000.
I0304 19:32:44.889475 23128000471168 run.py:483] Algo bellman_ford step 9875 current loss 0.001252, current_train_items 316032.
I0304 19:32:44.906096 23128000471168 run.py:483] Algo bellman_ford step 9876 current loss 0.010327, current_train_items 316064.
I0304 19:32:44.928791 23128000471168 run.py:483] Algo bellman_ford step 9877 current loss 0.032543, current_train_items 316096.
I0304 19:32:44.960795 23128000471168 run.py:483] Algo bellman_ford step 9878 current loss 0.024224, current_train_items 316128.
I0304 19:32:44.995078 23128000471168 run.py:483] Algo bellman_ford step 9879 current loss 0.045105, current_train_items 316160.
I0304 19:32:45.014990 23128000471168 run.py:483] Algo bellman_ford step 9880 current loss 0.001891, current_train_items 316192.
I0304 19:32:45.031537 23128000471168 run.py:483] Algo bellman_ford step 9881 current loss 0.028217, current_train_items 316224.
I0304 19:32:45.056285 23128000471168 run.py:483] Algo bellman_ford step 9882 current loss 0.015884, current_train_items 316256.
I0304 19:32:45.088975 23128000471168 run.py:483] Algo bellman_ford step 9883 current loss 0.031036, current_train_items 316288.
I0304 19:32:45.122986 23128000471168 run.py:483] Algo bellman_ford step 9884 current loss 0.059752, current_train_items 316320.
I0304 19:32:45.143550 23128000471168 run.py:483] Algo bellman_ford step 9885 current loss 0.001071, current_train_items 316352.
I0304 19:32:45.160171 23128000471168 run.py:483] Algo bellman_ford step 9886 current loss 0.004287, current_train_items 316384.
I0304 19:32:45.184410 23128000471168 run.py:483] Algo bellman_ford step 9887 current loss 0.027079, current_train_items 316416.
I0304 19:32:45.216466 23128000471168 run.py:483] Algo bellman_ford step 9888 current loss 0.031216, current_train_items 316448.
I0304 19:32:45.249442 23128000471168 run.py:483] Algo bellman_ford step 9889 current loss 0.046971, current_train_items 316480.
I0304 19:32:45.269944 23128000471168 run.py:483] Algo bellman_ford step 9890 current loss 0.003634, current_train_items 316512.
I0304 19:32:45.286453 23128000471168 run.py:483] Algo bellman_ford step 9891 current loss 0.020782, current_train_items 316544.
I0304 19:32:45.310297 23128000471168 run.py:483] Algo bellman_ford step 9892 current loss 0.027518, current_train_items 316576.
I0304 19:32:45.343281 23128000471168 run.py:483] Algo bellman_ford step 9893 current loss 0.030515, current_train_items 316608.
I0304 19:32:45.378356 23128000471168 run.py:483] Algo bellman_ford step 9894 current loss 0.059853, current_train_items 316640.
I0304 19:32:45.398389 23128000471168 run.py:483] Algo bellman_ford step 9895 current loss 0.001127, current_train_items 316672.
I0304 19:32:45.414839 23128000471168 run.py:483] Algo bellman_ford step 9896 current loss 0.005690, current_train_items 316704.
I0304 19:32:45.439640 23128000471168 run.py:483] Algo bellman_ford step 9897 current loss 0.081774, current_train_items 316736.
I0304 19:32:45.472213 23128000471168 run.py:483] Algo bellman_ford step 9898 current loss 0.088178, current_train_items 316768.
I0304 19:32:45.507844 23128000471168 run.py:483] Algo bellman_ford step 9899 current loss 0.054478, current_train_items 316800.
I0304 19:32:45.528490 23128000471168 run.py:483] Algo bellman_ford step 9900 current loss 0.001241, current_train_items 316832.
I0304 19:32:45.536458 23128000471168 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0304 19:32:45.536567 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:45.553851 23128000471168 run.py:483] Algo bellman_ford step 9901 current loss 0.008665, current_train_items 316864.
I0304 19:32:45.578670 23128000471168 run.py:483] Algo bellman_ford step 9902 current loss 0.021707, current_train_items 316896.
I0304 19:32:45.610892 23128000471168 run.py:483] Algo bellman_ford step 9903 current loss 0.021392, current_train_items 316928.
I0304 19:32:45.645890 23128000471168 run.py:483] Algo bellman_ford step 9904 current loss 0.029830, current_train_items 316960.
I0304 19:32:45.666402 23128000471168 run.py:483] Algo bellman_ford step 9905 current loss 0.001595, current_train_items 316992.
I0304 19:32:45.683120 23128000471168 run.py:483] Algo bellman_ford step 9906 current loss 0.030219, current_train_items 317024.
I0304 19:32:45.708474 23128000471168 run.py:483] Algo bellman_ford step 9907 current loss 0.032006, current_train_items 317056.
I0304 19:32:45.741027 23128000471168 run.py:483] Algo bellman_ford step 9908 current loss 0.019637, current_train_items 317088.
I0304 19:32:45.773472 23128000471168 run.py:483] Algo bellman_ford step 9909 current loss 0.039552, current_train_items 317120.
I0304 19:32:45.793477 23128000471168 run.py:483] Algo bellman_ford step 9910 current loss 0.006028, current_train_items 317152.
I0304 19:32:45.810312 23128000471168 run.py:483] Algo bellman_ford step 9911 current loss 0.006784, current_train_items 317184.
I0304 19:32:45.834881 23128000471168 run.py:483] Algo bellman_ford step 9912 current loss 0.051418, current_train_items 317216.
I0304 19:32:45.868079 23128000471168 run.py:483] Algo bellman_ford step 9913 current loss 0.045645, current_train_items 317248.
I0304 19:32:45.901605 23128000471168 run.py:483] Algo bellman_ford step 9914 current loss 0.050925, current_train_items 317280.
I0304 19:32:45.921672 23128000471168 run.py:483] Algo bellman_ford step 9915 current loss 0.014732, current_train_items 317312.
I0304 19:32:45.938530 23128000471168 run.py:483] Algo bellman_ford step 9916 current loss 0.005122, current_train_items 317344.
I0304 19:32:45.963443 23128000471168 run.py:483] Algo bellman_ford step 9917 current loss 0.028110, current_train_items 317376.
I0304 19:32:45.995849 23128000471168 run.py:483] Algo bellman_ford step 9918 current loss 0.059799, current_train_items 317408.
I0304 19:32:46.029076 23128000471168 run.py:483] Algo bellman_ford step 9919 current loss 0.044238, current_train_items 317440.
I0304 19:32:46.049129 23128000471168 run.py:483] Algo bellman_ford step 9920 current loss 0.001346, current_train_items 317472.
I0304 19:32:46.065943 23128000471168 run.py:483] Algo bellman_ford step 9921 current loss 0.012024, current_train_items 317504.
I0304 19:32:46.090652 23128000471168 run.py:483] Algo bellman_ford step 9922 current loss 0.050887, current_train_items 317536.
I0304 19:32:46.121193 23128000471168 run.py:483] Algo bellman_ford step 9923 current loss 0.041639, current_train_items 317568.
I0304 19:32:46.157779 23128000471168 run.py:483] Algo bellman_ford step 9924 current loss 0.047567, current_train_items 317600.
I0304 19:32:46.177467 23128000471168 run.py:483] Algo bellman_ford step 9925 current loss 0.008784, current_train_items 317632.
I0304 19:32:46.193803 23128000471168 run.py:483] Algo bellman_ford step 9926 current loss 0.024453, current_train_items 317664.
I0304 19:32:46.219108 23128000471168 run.py:483] Algo bellman_ford step 9927 current loss 0.024772, current_train_items 317696.
I0304 19:32:46.252746 23128000471168 run.py:483] Algo bellman_ford step 9928 current loss 0.038974, current_train_items 317728.
I0304 19:32:46.284270 23128000471168 run.py:483] Algo bellman_ford step 9929 current loss 0.059864, current_train_items 317760.
I0304 19:32:46.304526 23128000471168 run.py:483] Algo bellman_ford step 9930 current loss 0.001142, current_train_items 317792.
I0304 19:32:46.320394 23128000471168 run.py:483] Algo bellman_ford step 9931 current loss 0.009689, current_train_items 317824.
I0304 19:32:46.345069 23128000471168 run.py:483] Algo bellman_ford step 9932 current loss 0.026998, current_train_items 317856.
I0304 19:32:46.376773 23128000471168 run.py:483] Algo bellman_ford step 9933 current loss 0.033914, current_train_items 317888.
I0304 19:32:46.409392 23128000471168 run.py:483] Algo bellman_ford step 9934 current loss 0.065890, current_train_items 317920.
I0304 19:32:46.429309 23128000471168 run.py:483] Algo bellman_ford step 9935 current loss 0.002567, current_train_items 317952.
I0304 19:32:46.445414 23128000471168 run.py:483] Algo bellman_ford step 9936 current loss 0.007738, current_train_items 317984.
I0304 19:32:46.469720 23128000471168 run.py:483] Algo bellman_ford step 9937 current loss 0.023875, current_train_items 318016.
I0304 19:32:46.501574 23128000471168 run.py:483] Algo bellman_ford step 9938 current loss 0.022783, current_train_items 318048.
I0304 19:32:46.535897 23128000471168 run.py:483] Algo bellman_ford step 9939 current loss 0.068054, current_train_items 318080.
I0304 19:32:46.555913 23128000471168 run.py:483] Algo bellman_ford step 9940 current loss 0.004712, current_train_items 318112.
I0304 19:32:46.572237 23128000471168 run.py:483] Algo bellman_ford step 9941 current loss 0.006406, current_train_items 318144.
I0304 19:32:46.596165 23128000471168 run.py:483] Algo bellman_ford step 9942 current loss 0.025649, current_train_items 318176.
I0304 19:32:46.628997 23128000471168 run.py:483] Algo bellman_ford step 9943 current loss 0.025612, current_train_items 318208.
I0304 19:32:46.661396 23128000471168 run.py:483] Algo bellman_ford step 9944 current loss 0.077990, current_train_items 318240.
I0304 19:32:46.681048 23128000471168 run.py:483] Algo bellman_ford step 9945 current loss 0.007259, current_train_items 318272.
I0304 19:32:46.697134 23128000471168 run.py:483] Algo bellman_ford step 9946 current loss 0.057624, current_train_items 318304.
I0304 19:32:46.721696 23128000471168 run.py:483] Algo bellman_ford step 9947 current loss 0.016811, current_train_items 318336.
I0304 19:32:46.754222 23128000471168 run.py:483] Algo bellman_ford step 9948 current loss 0.030681, current_train_items 318368.
I0304 19:32:46.789373 23128000471168 run.py:483] Algo bellman_ford step 9949 current loss 0.042230, current_train_items 318400.
I0304 19:32:46.809521 23128000471168 run.py:483] Algo bellman_ford step 9950 current loss 0.001063, current_train_items 318432.
I0304 19:32:46.817716 23128000471168 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0304 19:32:46.817827 23128000471168 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:32:46.835273 23128000471168 run.py:483] Algo bellman_ford step 9951 current loss 0.011041, current_train_items 318464.
I0304 19:32:46.860177 23128000471168 run.py:483] Algo bellman_ford step 9952 current loss 0.013149, current_train_items 318496.
I0304 19:32:46.893958 23128000471168 run.py:483] Algo bellman_ford step 9953 current loss 0.037684, current_train_items 318528.
I0304 19:32:46.929324 23128000471168 run.py:483] Algo bellman_ford step 9954 current loss 0.069203, current_train_items 318560.
I0304 19:32:46.949603 23128000471168 run.py:483] Algo bellman_ford step 9955 current loss 0.003699, current_train_items 318592.
I0304 19:32:46.966300 23128000471168 run.py:483] Algo bellman_ford step 9956 current loss 0.010960, current_train_items 318624.
I0304 19:32:46.991344 23128000471168 run.py:483] Algo bellman_ford step 9957 current loss 0.019994, current_train_items 318656.
I0304 19:32:47.024010 23128000471168 run.py:483] Algo bellman_ford step 9958 current loss 0.045040, current_train_items 318688.
I0304 19:32:47.058933 23128000471168 run.py:483] Algo bellman_ford step 9959 current loss 0.042618, current_train_items 318720.
I0304 19:32:47.079355 23128000471168 run.py:483] Algo bellman_ford step 9960 current loss 0.000824, current_train_items 318752.
I0304 19:32:47.095543 23128000471168 run.py:483] Algo bellman_ford step 9961 current loss 0.016594, current_train_items 318784.
I0304 19:32:47.120422 23128000471168 run.py:483] Algo bellman_ford step 9962 current loss 0.015215, current_train_items 318816.
I0304 19:32:47.152926 23128000471168 run.py:483] Algo bellman_ford step 9963 current loss 0.031551, current_train_items 318848.
I0304 19:32:47.186579 23128000471168 run.py:483] Algo bellman_ford step 9964 current loss 0.062944, current_train_items 318880.
I0304 19:32:47.206262 23128000471168 run.py:483] Algo bellman_ford step 9965 current loss 0.002532, current_train_items 318912.
I0304 19:32:47.222991 23128000471168 run.py:483] Algo bellman_ford step 9966 current loss 0.012145, current_train_items 318944.
I0304 19:32:47.247658 23128000471168 run.py:483] Algo bellman_ford step 9967 current loss 0.029967, current_train_items 318976.
I0304 19:32:47.279866 23128000471168 run.py:483] Algo bellman_ford step 9968 current loss 0.036627, current_train_items 319008.
I0304 19:32:47.313964 23128000471168 run.py:483] Algo bellman_ford step 9969 current loss 0.046353, current_train_items 319040.
I0304 19:32:47.334166 23128000471168 run.py:483] Algo bellman_ford step 9970 current loss 0.001416, current_train_items 319072.
I0304 19:32:47.350613 23128000471168 run.py:483] Algo bellman_ford step 9971 current loss 0.016391, current_train_items 319104.
I0304 19:32:47.374611 23128000471168 run.py:483] Algo bellman_ford step 9972 current loss 0.024026, current_train_items 319136.
I0304 19:32:47.406188 23128000471168 run.py:483] Algo bellman_ford step 9973 current loss 0.043787, current_train_items 319168.
I0304 19:32:47.439851 23128000471168 run.py:483] Algo bellman_ford step 9974 current loss 0.041776, current_train_items 319200.
I0304 19:32:47.460041 23128000471168 run.py:483] Algo bellman_ford step 9975 current loss 0.000856, current_train_items 319232.
I0304 19:32:47.477124 23128000471168 run.py:483] Algo bellman_ford step 9976 current loss 0.018911, current_train_items 319264.
I0304 19:32:47.501403 23128000471168 run.py:483] Algo bellman_ford step 9977 current loss 0.038399, current_train_items 319296.
I0304 19:32:47.533583 23128000471168 run.py:483] Algo bellman_ford step 9978 current loss 0.018780, current_train_items 319328.
I0304 19:32:47.566165 23128000471168 run.py:483] Algo bellman_ford step 9979 current loss 0.020884, current_train_items 319360.
I0304 19:32:47.586242 23128000471168 run.py:483] Algo bellman_ford step 9980 current loss 0.001139, current_train_items 319392.
I0304 19:32:47.602658 23128000471168 run.py:483] Algo bellman_ford step 9981 current loss 0.005154, current_train_items 319424.
I0304 19:32:47.627918 23128000471168 run.py:483] Algo bellman_ford step 9982 current loss 0.039340, current_train_items 319456.
I0304 19:32:47.661593 23128000471168 run.py:483] Algo bellman_ford step 9983 current loss 0.026497, current_train_items 319488.
I0304 19:32:47.695973 23128000471168 run.py:483] Algo bellman_ford step 9984 current loss 0.052358, current_train_items 319520.
I0304 19:32:47.716464 23128000471168 run.py:483] Algo bellman_ford step 9985 current loss 0.001792, current_train_items 319552.
I0304 19:32:47.732773 23128000471168 run.py:483] Algo bellman_ford step 9986 current loss 0.002387, current_train_items 319584.
I0304 19:32:47.756720 23128000471168 run.py:483] Algo bellman_ford step 9987 current loss 0.021781, current_train_items 319616.
I0304 19:32:47.789514 23128000471168 run.py:483] Algo bellman_ford step 9988 current loss 0.035766, current_train_items 319648.
I0304 19:32:47.822892 23128000471168 run.py:483] Algo bellman_ford step 9989 current loss 0.050698, current_train_items 319680.
I0304 19:32:47.843247 23128000471168 run.py:483] Algo bellman_ford step 9990 current loss 0.032936, current_train_items 319712.
I0304 19:32:47.859906 23128000471168 run.py:483] Algo bellman_ford step 9991 current loss 0.005216, current_train_items 319744.
I0304 19:32:47.884079 23128000471168 run.py:483] Algo bellman_ford step 9992 current loss 0.045048, current_train_items 319776.
I0304 19:32:47.915681 23128000471168 run.py:483] Algo bellman_ford step 9993 current loss 0.027123, current_train_items 319808.
I0304 19:32:47.948946 23128000471168 run.py:483] Algo bellman_ford step 9994 current loss 0.036228, current_train_items 319840.
I0304 19:32:47.968984 23128000471168 run.py:483] Algo bellman_ford step 9995 current loss 0.002153, current_train_items 319872.
I0304 19:32:47.985637 23128000471168 run.py:483] Algo bellman_ford step 9996 current loss 0.046775, current_train_items 319904.
I0304 19:32:48.009937 23128000471168 run.py:483] Algo bellman_ford step 9997 current loss 0.023084, current_train_items 319936.
I0304 19:32:48.042280 23128000471168 run.py:483] Algo bellman_ford step 9998 current loss 0.029783, current_train_items 319968.
I0304 19:32:48.074025 23128000471168 run.py:483] Algo bellman_ford step 9999 current loss 0.047076, current_train_items 320000.
I0304 19:32:48.080120 23128000471168 run.py:527] Restoring best model from checkpoint...
I0304 19:32:50.709366 23128000471168 run.py:542] (test) algo bellman_ford : {'pi': 0.96044921875, 'score': 0.96044921875, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0304 19:32:50.709595 23128000471168 run.py:544] Done!

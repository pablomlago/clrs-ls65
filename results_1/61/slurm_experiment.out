Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-04 19:24:58.438971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-04 19:24:58.439185: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-04 19:24:58.482664: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-04 19:25:19.186880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0304 19:26:25.666260 23118544486528 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0304 19:26:25.670038 23118544486528 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0304 19:26:27.025449 23118544486528 run.py:307] Creating samplers for algo bellman_ford
W0304 19:26:27.025909 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.026179 23118544486528 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.243657 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.243920 23118544486528 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.491585 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.491841 23118544486528 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:27.834535 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:27.834777 23118544486528 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.256969 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0304 19:26:28.257216 23118544486528 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0304 19:26:28.792714 23118544486528 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0304 19:26:28.792984 23118544486528 samplers.py:112] Creating a dataset with 64 samples.
I0304 19:26:28.831605 23118544486528 run.py:166] Dataset not found in ./datasets_1/61/CLRS30_v1.0.0. Downloading...
I0304 19:26:45.018563 23118544486528 dataset_info.py:482] Load dataset info from ./datasets_1/61/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.020898 23118544486528 dataset_info.py:482] Load dataset info from ./datasets_1/61/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:26:45.021665 23118544486528 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/61/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0304 19:26:45.021751 23118544486528 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/61/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0304 19:27:00.740290 23118544486528 run.py:483] Algo bellman_ford step 0 current loss 3.759103, current_train_items 32.
I0304 19:27:03.515547 23118544486528 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.5322265625, 'score': 0.5322265625, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0304 19:27:03.515724 23118544486528 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.532, val scores are: bellman_ford: 0.532
I0304 19:27:13.768395 23118544486528 run.py:483] Algo bellman_ford step 1 current loss 3.835244, current_train_items 64.
I0304 19:27:24.876363 23118544486528 run.py:483] Algo bellman_ford step 2 current loss 3.686968, current_train_items 96.
I0304 19:27:35.827133 23118544486528 run.py:483] Algo bellman_ford step 3 current loss 3.475085, current_train_items 128.
I0304 19:27:45.912860 23118544486528 run.py:483] Algo bellman_ford step 4 current loss 3.654037, current_train_items 160.
I0304 19:27:45.931829 23118544486528 run.py:483] Algo bellman_ford step 5 current loss 1.121786, current_train_items 192.
I0304 19:27:45.949188 23118544486528 run.py:483] Algo bellman_ford step 6 current loss 1.946368, current_train_items 224.
I0304 19:27:45.971961 23118544486528 run.py:483] Algo bellman_ford step 7 current loss 2.263535, current_train_items 256.
I0304 19:27:46.001533 23118544486528 run.py:483] Algo bellman_ford step 8 current loss 2.629200, current_train_items 288.
I0304 19:27:46.033843 23118544486528 run.py:483] Algo bellman_ford step 9 current loss 2.819334, current_train_items 320.
I0304 19:27:46.052123 23118544486528 run.py:483] Algo bellman_ford step 10 current loss 1.106376, current_train_items 352.
I0304 19:27:46.069240 23118544486528 run.py:483] Algo bellman_ford step 11 current loss 1.590095, current_train_items 384.
I0304 19:27:46.091464 23118544486528 run.py:483] Algo bellman_ford step 12 current loss 1.684052, current_train_items 416.
I0304 19:27:46.122626 23118544486528 run.py:483] Algo bellman_ford step 13 current loss 2.163620, current_train_items 448.
I0304 19:27:46.150866 23118544486528 run.py:483] Algo bellman_ford step 14 current loss 1.927310, current_train_items 480.
I0304 19:27:46.169004 23118544486528 run.py:483] Algo bellman_ford step 15 current loss 0.771427, current_train_items 512.
I0304 19:27:46.185159 23118544486528 run.py:483] Algo bellman_ford step 16 current loss 1.132644, current_train_items 544.
I0304 19:27:46.209967 23118544486528 run.py:483] Algo bellman_ford step 17 current loss 1.948315, current_train_items 576.
I0304 19:27:46.239459 23118544486528 run.py:483] Algo bellman_ford step 18 current loss 1.717103, current_train_items 608.
I0304 19:27:46.272310 23118544486528 run.py:483] Algo bellman_ford step 19 current loss 1.959532, current_train_items 640.
I0304 19:27:46.290188 23118544486528 run.py:483] Algo bellman_ford step 20 current loss 0.587039, current_train_items 672.
I0304 19:27:46.306085 23118544486528 run.py:483] Algo bellman_ford step 21 current loss 0.872644, current_train_items 704.
I0304 19:27:46.330211 23118544486528 run.py:483] Algo bellman_ford step 22 current loss 1.418692, current_train_items 736.
I0304 19:27:46.359729 23118544486528 run.py:483] Algo bellman_ford step 23 current loss 1.477947, current_train_items 768.
I0304 19:27:46.391094 23118544486528 run.py:483] Algo bellman_ford step 24 current loss 1.765892, current_train_items 800.
I0304 19:27:46.408845 23118544486528 run.py:483] Algo bellman_ford step 25 current loss 0.523515, current_train_items 832.
I0304 19:27:46.425204 23118544486528 run.py:483] Algo bellman_ford step 26 current loss 0.842493, current_train_items 864.
I0304 19:27:46.449167 23118544486528 run.py:483] Algo bellman_ford step 27 current loss 1.235453, current_train_items 896.
I0304 19:27:46.479901 23118544486528 run.py:483] Algo bellman_ford step 28 current loss 1.275829, current_train_items 928.
I0304 19:27:46.512881 23118544486528 run.py:483] Algo bellman_ford step 29 current loss 1.674550, current_train_items 960.
I0304 19:27:46.530631 23118544486528 run.py:483] Algo bellman_ford step 30 current loss 0.404996, current_train_items 992.
I0304 19:27:46.546188 23118544486528 run.py:483] Algo bellman_ford step 31 current loss 0.632116, current_train_items 1024.
I0304 19:27:46.569370 23118544486528 run.py:483] Algo bellman_ford step 32 current loss 1.080394, current_train_items 1056.
I0304 19:27:46.600257 23118544486528 run.py:483] Algo bellman_ford step 33 current loss 1.239446, current_train_items 1088.
I0304 19:27:46.631843 23118544486528 run.py:483] Algo bellman_ford step 34 current loss 1.326820, current_train_items 1120.
I0304 19:27:46.649509 23118544486528 run.py:483] Algo bellman_ford step 35 current loss 0.402841, current_train_items 1152.
I0304 19:27:46.665418 23118544486528 run.py:483] Algo bellman_ford step 36 current loss 0.512574, current_train_items 1184.
I0304 19:27:46.688995 23118544486528 run.py:483] Algo bellman_ford step 37 current loss 0.946720, current_train_items 1216.
I0304 19:27:46.718955 23118544486528 run.py:483] Algo bellman_ford step 38 current loss 1.026858, current_train_items 1248.
W0304 19:27:46.742219 23118544486528 samplers.py:155] Increasing hint lengh from 9 to 11
I0304 19:27:53.450702 23118544486528 run.py:483] Algo bellman_ford step 39 current loss 1.566299, current_train_items 1280.
I0304 19:27:53.470542 23118544486528 run.py:483] Algo bellman_ford step 40 current loss 0.427550, current_train_items 1312.
I0304 19:27:53.487520 23118544486528 run.py:483] Algo bellman_ford step 41 current loss 0.536588, current_train_items 1344.
I0304 19:27:53.511063 23118544486528 run.py:483] Algo bellman_ford step 42 current loss 0.795980, current_train_items 1376.
I0304 19:27:53.542155 23118544486528 run.py:483] Algo bellman_ford step 43 current loss 0.970885, current_train_items 1408.
I0304 19:27:53.574986 23118544486528 run.py:483] Algo bellman_ford step 44 current loss 1.163924, current_train_items 1440.
I0304 19:27:53.594398 23118544486528 run.py:483] Algo bellman_ford step 45 current loss 0.279081, current_train_items 1472.
I0304 19:27:53.610886 23118544486528 run.py:483] Algo bellman_ford step 46 current loss 0.668202, current_train_items 1504.
I0304 19:27:53.633475 23118544486528 run.py:483] Algo bellman_ford step 47 current loss 0.800444, current_train_items 1536.
I0304 19:27:53.661709 23118544486528 run.py:483] Algo bellman_ford step 48 current loss 0.677179, current_train_items 1568.
I0304 19:27:53.691509 23118544486528 run.py:483] Algo bellman_ford step 49 current loss 1.081884, current_train_items 1600.
I0304 19:27:53.710232 23118544486528 run.py:483] Algo bellman_ford step 50 current loss 0.244767, current_train_items 1632.
I0304 19:27:53.719473 23118544486528 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8447265625, 'score': 0.8447265625, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0304 19:27:53.719589 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.532, current avg val score is 0.845, val scores are: bellman_ford: 0.845
I0304 19:27:53.747993 23118544486528 run.py:483] Algo bellman_ford step 51 current loss 0.454300, current_train_items 1664.
I0304 19:27:53.771821 23118544486528 run.py:483] Algo bellman_ford step 52 current loss 0.822953, current_train_items 1696.
I0304 19:27:53.801043 23118544486528 run.py:483] Algo bellman_ford step 53 current loss 0.791504, current_train_items 1728.
I0304 19:27:53.833847 23118544486528 run.py:483] Algo bellman_ford step 54 current loss 1.025727, current_train_items 1760.
I0304 19:27:53.852889 23118544486528 run.py:483] Algo bellman_ford step 55 current loss 0.277801, current_train_items 1792.
I0304 19:27:53.869001 23118544486528 run.py:483] Algo bellman_ford step 56 current loss 0.380547, current_train_items 1824.
I0304 19:27:53.892070 23118544486528 run.py:483] Algo bellman_ford step 57 current loss 0.818087, current_train_items 1856.
I0304 19:27:53.920284 23118544486528 run.py:483] Algo bellman_ford step 58 current loss 0.611348, current_train_items 1888.
I0304 19:27:53.953258 23118544486528 run.py:483] Algo bellman_ford step 59 current loss 0.967356, current_train_items 1920.
I0304 19:27:53.971859 23118544486528 run.py:483] Algo bellman_ford step 60 current loss 0.184440, current_train_items 1952.
W0304 19:27:53.981153 23118544486528 samplers.py:155] Increasing hint lengh from 6 to 7
I0304 19:28:00.384186 23118544486528 run.py:483] Algo bellman_ford step 61 current loss 0.458207, current_train_items 1984.
I0304 19:28:00.408763 23118544486528 run.py:483] Algo bellman_ford step 62 current loss 0.710860, current_train_items 2016.
I0304 19:28:00.438956 23118544486528 run.py:483] Algo bellman_ford step 63 current loss 0.860694, current_train_items 2048.
I0304 19:28:00.474148 23118544486528 run.py:483] Algo bellman_ford step 64 current loss 1.061528, current_train_items 2080.
I0304 19:28:00.493844 23118544486528 run.py:483] Algo bellman_ford step 65 current loss 0.192439, current_train_items 2112.
I0304 19:28:00.510135 23118544486528 run.py:483] Algo bellman_ford step 66 current loss 0.323752, current_train_items 2144.
I0304 19:28:00.535153 23118544486528 run.py:483] Algo bellman_ford step 67 current loss 0.711729, current_train_items 2176.
I0304 19:28:00.564279 23118544486528 run.py:483] Algo bellman_ford step 68 current loss 0.633131, current_train_items 2208.
I0304 19:28:00.597110 23118544486528 run.py:483] Algo bellman_ford step 69 current loss 0.846452, current_train_items 2240.
I0304 19:28:00.616004 23118544486528 run.py:483] Algo bellman_ford step 70 current loss 0.152564, current_train_items 2272.
I0304 19:28:00.632740 23118544486528 run.py:483] Algo bellman_ford step 71 current loss 0.318259, current_train_items 2304.
I0304 19:28:00.656767 23118544486528 run.py:483] Algo bellman_ford step 72 current loss 0.663847, current_train_items 2336.
I0304 19:28:00.687061 23118544486528 run.py:483] Algo bellman_ford step 73 current loss 0.668299, current_train_items 2368.
I0304 19:28:00.720343 23118544486528 run.py:483] Algo bellman_ford step 74 current loss 0.793335, current_train_items 2400.
I0304 19:28:00.739403 23118544486528 run.py:483] Algo bellman_ford step 75 current loss 0.112573, current_train_items 2432.
I0304 19:28:00.756287 23118544486528 run.py:483] Algo bellman_ford step 76 current loss 0.343463, current_train_items 2464.
I0304 19:28:00.779820 23118544486528 run.py:483] Algo bellman_ford step 77 current loss 0.664850, current_train_items 2496.
I0304 19:28:00.808507 23118544486528 run.py:483] Algo bellman_ford step 78 current loss 0.574281, current_train_items 2528.
I0304 19:28:00.838730 23118544486528 run.py:483] Algo bellman_ford step 79 current loss 0.647701, current_train_items 2560.
I0304 19:28:00.857750 23118544486528 run.py:483] Algo bellman_ford step 80 current loss 0.136947, current_train_items 2592.
I0304 19:28:00.874040 23118544486528 run.py:483] Algo bellman_ford step 81 current loss 0.386735, current_train_items 2624.
I0304 19:28:00.897650 23118544486528 run.py:483] Algo bellman_ford step 82 current loss 0.694209, current_train_items 2656.
I0304 19:28:00.927220 23118544486528 run.py:483] Algo bellman_ford step 83 current loss 0.618118, current_train_items 2688.
I0304 19:28:00.957883 23118544486528 run.py:483] Algo bellman_ford step 84 current loss 0.650780, current_train_items 2720.
I0304 19:28:00.976500 23118544486528 run.py:483] Algo bellman_ford step 85 current loss 0.148294, current_train_items 2752.
I0304 19:28:00.993047 23118544486528 run.py:483] Algo bellman_ford step 86 current loss 0.357458, current_train_items 2784.
I0304 19:28:01.018299 23118544486528 run.py:483] Algo bellman_ford step 87 current loss 0.663703, current_train_items 2816.
I0304 19:28:01.048172 23118544486528 run.py:483] Algo bellman_ford step 88 current loss 0.542665, current_train_items 2848.
I0304 19:28:01.080742 23118544486528 run.py:483] Algo bellman_ford step 89 current loss 0.748151, current_train_items 2880.
I0304 19:28:01.099426 23118544486528 run.py:483] Algo bellman_ford step 90 current loss 0.141061, current_train_items 2912.
I0304 19:28:01.116076 23118544486528 run.py:483] Algo bellman_ford step 91 current loss 0.341429, current_train_items 2944.
I0304 19:28:01.139573 23118544486528 run.py:483] Algo bellman_ford step 92 current loss 0.483498, current_train_items 2976.
I0304 19:28:01.170604 23118544486528 run.py:483] Algo bellman_ford step 93 current loss 0.642113, current_train_items 3008.
I0304 19:28:01.202501 23118544486528 run.py:483] Algo bellman_ford step 94 current loss 0.606221, current_train_items 3040.
I0304 19:28:01.221832 23118544486528 run.py:483] Algo bellman_ford step 95 current loss 0.096503, current_train_items 3072.
I0304 19:28:01.237875 23118544486528 run.py:483] Algo bellman_ford step 96 current loss 0.275476, current_train_items 3104.
I0304 19:28:01.261859 23118544486528 run.py:483] Algo bellman_ford step 97 current loss 0.454919, current_train_items 3136.
I0304 19:28:01.292143 23118544486528 run.py:483] Algo bellman_ford step 98 current loss 0.537432, current_train_items 3168.
I0304 19:28:01.324451 23118544486528 run.py:483] Algo bellman_ford step 99 current loss 0.691937, current_train_items 3200.
I0304 19:28:01.343443 23118544486528 run.py:483] Algo bellman_ford step 100 current loss 0.131987, current_train_items 3232.
I0304 19:28:01.352612 23118544486528 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0304 19:28:01.352731 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.845, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0304 19:28:01.382397 23118544486528 run.py:483] Algo bellman_ford step 101 current loss 0.368308, current_train_items 3264.
I0304 19:28:01.406274 23118544486528 run.py:483] Algo bellman_ford step 102 current loss 0.373148, current_train_items 3296.
I0304 19:28:01.435653 23118544486528 run.py:483] Algo bellman_ford step 103 current loss 0.518012, current_train_items 3328.
I0304 19:28:01.470175 23118544486528 run.py:483] Algo bellman_ford step 104 current loss 0.721844, current_train_items 3360.
I0304 19:28:01.489608 23118544486528 run.py:483] Algo bellman_ford step 105 current loss 0.084102, current_train_items 3392.
I0304 19:28:01.506095 23118544486528 run.py:483] Algo bellman_ford step 106 current loss 0.267149, current_train_items 3424.
I0304 19:28:01.529923 23118544486528 run.py:483] Algo bellman_ford step 107 current loss 0.602837, current_train_items 3456.
I0304 19:28:01.559069 23118544486528 run.py:483] Algo bellman_ford step 108 current loss 0.580686, current_train_items 3488.
I0304 19:28:01.589903 23118544486528 run.py:483] Algo bellman_ford step 109 current loss 0.554658, current_train_items 3520.
I0304 19:28:01.609018 23118544486528 run.py:483] Algo bellman_ford step 110 current loss 0.091507, current_train_items 3552.
I0304 19:28:01.625360 23118544486528 run.py:483] Algo bellman_ford step 111 current loss 0.219135, current_train_items 3584.
I0304 19:28:01.648655 23118544486528 run.py:483] Algo bellman_ford step 112 current loss 0.337071, current_train_items 3616.
I0304 19:28:01.678576 23118544486528 run.py:483] Algo bellman_ford step 113 current loss 0.465080, current_train_items 3648.
I0304 19:28:01.710241 23118544486528 run.py:483] Algo bellman_ford step 114 current loss 0.580283, current_train_items 3680.
I0304 19:28:01.729047 23118544486528 run.py:483] Algo bellman_ford step 115 current loss 0.078033, current_train_items 3712.
I0304 19:28:01.745606 23118544486528 run.py:483] Algo bellman_ford step 116 current loss 0.298600, current_train_items 3744.
I0304 19:28:01.770186 23118544486528 run.py:483] Algo bellman_ford step 117 current loss 0.517636, current_train_items 3776.
I0304 19:28:01.799580 23118544486528 run.py:483] Algo bellman_ford step 118 current loss 0.503167, current_train_items 3808.
I0304 19:28:01.829242 23118544486528 run.py:483] Algo bellman_ford step 119 current loss 0.462784, current_train_items 3840.
I0304 19:28:01.848271 23118544486528 run.py:483] Algo bellman_ford step 120 current loss 0.126081, current_train_items 3872.
I0304 19:28:01.864917 23118544486528 run.py:483] Algo bellman_ford step 121 current loss 0.247845, current_train_items 3904.
I0304 19:28:01.889047 23118544486528 run.py:483] Algo bellman_ford step 122 current loss 0.431031, current_train_items 3936.
I0304 19:28:01.920112 23118544486528 run.py:483] Algo bellman_ford step 123 current loss 0.472038, current_train_items 3968.
I0304 19:28:01.955824 23118544486528 run.py:483] Algo bellman_ford step 124 current loss 0.734039, current_train_items 4000.
I0304 19:28:01.974888 23118544486528 run.py:483] Algo bellman_ford step 125 current loss 0.186970, current_train_items 4032.
I0304 19:28:01.991568 23118544486528 run.py:483] Algo bellman_ford step 126 current loss 0.330680, current_train_items 4064.
I0304 19:28:02.015794 23118544486528 run.py:483] Algo bellman_ford step 127 current loss 0.362086, current_train_items 4096.
I0304 19:28:02.045721 23118544486528 run.py:483] Algo bellman_ford step 128 current loss 0.466680, current_train_items 4128.
I0304 19:28:02.078179 23118544486528 run.py:483] Algo bellman_ford step 129 current loss 0.548161, current_train_items 4160.
I0304 19:28:02.096812 23118544486528 run.py:483] Algo bellman_ford step 130 current loss 0.119243, current_train_items 4192.
I0304 19:28:02.113082 23118544486528 run.py:483] Algo bellman_ford step 131 current loss 0.196758, current_train_items 4224.
I0304 19:28:02.137499 23118544486528 run.py:483] Algo bellman_ford step 132 current loss 0.446823, current_train_items 4256.
I0304 19:28:02.167610 23118544486528 run.py:483] Algo bellman_ford step 133 current loss 0.514986, current_train_items 4288.
I0304 19:28:02.199166 23118544486528 run.py:483] Algo bellman_ford step 134 current loss 0.497728, current_train_items 4320.
I0304 19:28:02.217718 23118544486528 run.py:483] Algo bellman_ford step 135 current loss 0.065860, current_train_items 4352.
I0304 19:28:02.234036 23118544486528 run.py:483] Algo bellman_ford step 136 current loss 0.233061, current_train_items 4384.
I0304 19:28:02.258030 23118544486528 run.py:483] Algo bellman_ford step 137 current loss 0.399808, current_train_items 4416.
I0304 19:28:02.288175 23118544486528 run.py:483] Algo bellman_ford step 138 current loss 0.447880, current_train_items 4448.
I0304 19:28:02.321263 23118544486528 run.py:483] Algo bellman_ford step 139 current loss 0.546075, current_train_items 4480.
I0304 19:28:02.339988 23118544486528 run.py:483] Algo bellman_ford step 140 current loss 0.082148, current_train_items 4512.
I0304 19:28:02.356715 23118544486528 run.py:483] Algo bellman_ford step 141 current loss 0.275918, current_train_items 4544.
I0304 19:28:02.380503 23118544486528 run.py:483] Algo bellman_ford step 142 current loss 0.390898, current_train_items 4576.
I0304 19:28:02.411027 23118544486528 run.py:483] Algo bellman_ford step 143 current loss 0.537910, current_train_items 4608.
I0304 19:28:02.442653 23118544486528 run.py:483] Algo bellman_ford step 144 current loss 0.480936, current_train_items 4640.
I0304 19:28:02.461459 23118544486528 run.py:483] Algo bellman_ford step 145 current loss 0.065546, current_train_items 4672.
I0304 19:28:02.477711 23118544486528 run.py:483] Algo bellman_ford step 146 current loss 0.205954, current_train_items 4704.
I0304 19:28:02.500759 23118544486528 run.py:483] Algo bellman_ford step 147 current loss 0.313819, current_train_items 4736.
I0304 19:28:02.530261 23118544486528 run.py:483] Algo bellman_ford step 148 current loss 0.531196, current_train_items 4768.
I0304 19:28:02.560643 23118544486528 run.py:483] Algo bellman_ford step 149 current loss 0.554678, current_train_items 4800.
I0304 19:28:02.579556 23118544486528 run.py:483] Algo bellman_ford step 150 current loss 0.082694, current_train_items 4832.
I0304 19:28:02.587811 23118544486528 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0304 19:28:02.587924 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.894, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0304 19:28:02.616493 23118544486528 run.py:483] Algo bellman_ford step 151 current loss 0.199562, current_train_items 4864.
I0304 19:28:02.640521 23118544486528 run.py:483] Algo bellman_ford step 152 current loss 0.318217, current_train_items 4896.
I0304 19:28:02.669958 23118544486528 run.py:483] Algo bellman_ford step 153 current loss 0.445336, current_train_items 4928.
I0304 19:28:02.706064 23118544486528 run.py:483] Algo bellman_ford step 154 current loss 0.838695, current_train_items 4960.
I0304 19:28:02.725291 23118544486528 run.py:483] Algo bellman_ford step 155 current loss 0.122989, current_train_items 4992.
I0304 19:28:02.741886 23118544486528 run.py:483] Algo bellman_ford step 156 current loss 0.256686, current_train_items 5024.
I0304 19:28:02.765605 23118544486528 run.py:483] Algo bellman_ford step 157 current loss 0.354295, current_train_items 5056.
I0304 19:28:02.793881 23118544486528 run.py:483] Algo bellman_ford step 158 current loss 0.364128, current_train_items 5088.
I0304 19:28:02.826591 23118544486528 run.py:483] Algo bellman_ford step 159 current loss 0.401808, current_train_items 5120.
I0304 19:28:02.845755 23118544486528 run.py:483] Algo bellman_ford step 160 current loss 0.077301, current_train_items 5152.
I0304 19:28:02.862262 23118544486528 run.py:483] Algo bellman_ford step 161 current loss 0.142992, current_train_items 5184.
I0304 19:28:02.885242 23118544486528 run.py:483] Algo bellman_ford step 162 current loss 0.252103, current_train_items 5216.
I0304 19:28:02.915044 23118544486528 run.py:483] Algo bellman_ford step 163 current loss 0.410546, current_train_items 5248.
I0304 19:28:02.946472 23118544486528 run.py:483] Algo bellman_ford step 164 current loss 0.420791, current_train_items 5280.
I0304 19:28:02.965716 23118544486528 run.py:483] Algo bellman_ford step 165 current loss 0.071645, current_train_items 5312.
I0304 19:28:02.982448 23118544486528 run.py:483] Algo bellman_ford step 166 current loss 0.217437, current_train_items 5344.
I0304 19:28:03.005614 23118544486528 run.py:483] Algo bellman_ford step 167 current loss 0.366413, current_train_items 5376.
I0304 19:28:03.036105 23118544486528 run.py:483] Algo bellman_ford step 168 current loss 0.440549, current_train_items 5408.
I0304 19:28:03.068695 23118544486528 run.py:483] Algo bellman_ford step 169 current loss 0.418101, current_train_items 5440.
I0304 19:28:03.088154 23118544486528 run.py:483] Algo bellman_ford step 170 current loss 0.097522, current_train_items 5472.
I0304 19:28:03.104663 23118544486528 run.py:483] Algo bellman_ford step 171 current loss 0.177478, current_train_items 5504.
I0304 19:28:03.127433 23118544486528 run.py:483] Algo bellman_ford step 172 current loss 0.284537, current_train_items 5536.
I0304 19:28:03.157861 23118544486528 run.py:483] Algo bellman_ford step 173 current loss 0.399334, current_train_items 5568.
I0304 19:28:03.193578 23118544486528 run.py:483] Algo bellman_ford step 174 current loss 0.594692, current_train_items 5600.
I0304 19:28:03.212193 23118544486528 run.py:483] Algo bellman_ford step 175 current loss 0.065078, current_train_items 5632.
I0304 19:28:03.228640 23118544486528 run.py:483] Algo bellman_ford step 176 current loss 0.188194, current_train_items 5664.
I0304 19:28:03.252882 23118544486528 run.py:483] Algo bellman_ford step 177 current loss 0.386910, current_train_items 5696.
I0304 19:28:03.281122 23118544486528 run.py:483] Algo bellman_ford step 178 current loss 0.231670, current_train_items 5728.
I0304 19:28:03.311825 23118544486528 run.py:483] Algo bellman_ford step 179 current loss 0.402333, current_train_items 5760.
I0304 19:28:03.331400 23118544486528 run.py:483] Algo bellman_ford step 180 current loss 0.063511, current_train_items 5792.
I0304 19:28:03.348147 23118544486528 run.py:483] Algo bellman_ford step 181 current loss 0.169544, current_train_items 5824.
I0304 19:28:03.372919 23118544486528 run.py:483] Algo bellman_ford step 182 current loss 0.322324, current_train_items 5856.
I0304 19:28:03.402837 23118544486528 run.py:483] Algo bellman_ford step 183 current loss 0.408094, current_train_items 5888.
I0304 19:28:03.433137 23118544486528 run.py:483] Algo bellman_ford step 184 current loss 0.409797, current_train_items 5920.
I0304 19:28:03.451931 23118544486528 run.py:483] Algo bellman_ford step 185 current loss 0.041442, current_train_items 5952.
I0304 19:28:03.468820 23118544486528 run.py:483] Algo bellman_ford step 186 current loss 0.202843, current_train_items 5984.
I0304 19:28:03.491613 23118544486528 run.py:483] Algo bellman_ford step 187 current loss 0.251193, current_train_items 6016.
I0304 19:28:03.521146 23118544486528 run.py:483] Algo bellman_ford step 188 current loss 0.404236, current_train_items 6048.
I0304 19:28:03.554742 23118544486528 run.py:483] Algo bellman_ford step 189 current loss 0.446466, current_train_items 6080.
I0304 19:28:03.573629 23118544486528 run.py:483] Algo bellman_ford step 190 current loss 0.061481, current_train_items 6112.
I0304 19:28:03.590269 23118544486528 run.py:483] Algo bellman_ford step 191 current loss 0.244578, current_train_items 6144.
I0304 19:28:03.614807 23118544486528 run.py:483] Algo bellman_ford step 192 current loss 0.448089, current_train_items 6176.
I0304 19:28:03.644335 23118544486528 run.py:483] Algo bellman_ford step 193 current loss 0.406415, current_train_items 6208.
I0304 19:28:03.676095 23118544486528 run.py:483] Algo bellman_ford step 194 current loss 0.449042, current_train_items 6240.
I0304 19:28:03.695390 23118544486528 run.py:483] Algo bellman_ford step 195 current loss 0.065334, current_train_items 6272.
I0304 19:28:03.711892 23118544486528 run.py:483] Algo bellman_ford step 196 current loss 0.196151, current_train_items 6304.
I0304 19:28:03.735394 23118544486528 run.py:483] Algo bellman_ford step 197 current loss 0.381347, current_train_items 6336.
I0304 19:28:03.765947 23118544486528 run.py:483] Algo bellman_ford step 198 current loss 0.547321, current_train_items 6368.
I0304 19:28:03.798516 23118544486528 run.py:483] Algo bellman_ford step 199 current loss 0.434659, current_train_items 6400.
I0304 19:28:03.817574 23118544486528 run.py:483] Algo bellman_ford step 200 current loss 0.094072, current_train_items 6432.
I0304 19:28:03.825301 23118544486528 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0304 19:28:03.825408 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.923, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0304 19:28:03.854912 23118544486528 run.py:483] Algo bellman_ford step 201 current loss 0.190387, current_train_items 6464.
I0304 19:28:03.879148 23118544486528 run.py:483] Algo bellman_ford step 202 current loss 0.482808, current_train_items 6496.
I0304 19:28:03.910826 23118544486528 run.py:483] Algo bellman_ford step 203 current loss 0.409126, current_train_items 6528.
I0304 19:28:03.946642 23118544486528 run.py:483] Algo bellman_ford step 204 current loss 0.636191, current_train_items 6560.
I0304 19:28:03.965653 23118544486528 run.py:483] Algo bellman_ford step 205 current loss 0.134200, current_train_items 6592.
I0304 19:28:03.981164 23118544486528 run.py:483] Algo bellman_ford step 206 current loss 0.126175, current_train_items 6624.
I0304 19:28:04.005393 23118544486528 run.py:483] Algo bellman_ford step 207 current loss 0.432856, current_train_items 6656.
I0304 19:28:04.035597 23118544486528 run.py:483] Algo bellman_ford step 208 current loss 0.348543, current_train_items 6688.
I0304 19:28:04.066993 23118544486528 run.py:483] Algo bellman_ford step 209 current loss 0.469874, current_train_items 6720.
I0304 19:28:04.085713 23118544486528 run.py:483] Algo bellman_ford step 210 current loss 0.122762, current_train_items 6752.
I0304 19:28:04.102196 23118544486528 run.py:483] Algo bellman_ford step 211 current loss 0.207291, current_train_items 6784.
I0304 19:28:04.126484 23118544486528 run.py:483] Algo bellman_ford step 212 current loss 0.323743, current_train_items 6816.
I0304 19:28:04.157076 23118544486528 run.py:483] Algo bellman_ford step 213 current loss 0.329758, current_train_items 6848.
I0304 19:28:04.185272 23118544486528 run.py:483] Algo bellman_ford step 214 current loss 0.363053, current_train_items 6880.
I0304 19:28:04.203944 23118544486528 run.py:483] Algo bellman_ford step 215 current loss 0.062980, current_train_items 6912.
I0304 19:28:04.220136 23118544486528 run.py:483] Algo bellman_ford step 216 current loss 0.180420, current_train_items 6944.
I0304 19:28:04.243575 23118544486528 run.py:483] Algo bellman_ford step 217 current loss 0.209050, current_train_items 6976.
I0304 19:28:04.272777 23118544486528 run.py:483] Algo bellman_ford step 218 current loss 0.301134, current_train_items 7008.
I0304 19:28:04.303580 23118544486528 run.py:483] Algo bellman_ford step 219 current loss 0.368451, current_train_items 7040.
I0304 19:28:04.322144 23118544486528 run.py:483] Algo bellman_ford step 220 current loss 0.076226, current_train_items 7072.
I0304 19:28:04.338433 23118544486528 run.py:483] Algo bellman_ford step 221 current loss 0.165684, current_train_items 7104.
I0304 19:28:04.362960 23118544486528 run.py:483] Algo bellman_ford step 222 current loss 0.297770, current_train_items 7136.
I0304 19:28:04.391893 23118544486528 run.py:483] Algo bellman_ford step 223 current loss 0.311943, current_train_items 7168.
I0304 19:28:04.424222 23118544486528 run.py:483] Algo bellman_ford step 224 current loss 0.375072, current_train_items 7200.
I0304 19:28:04.442953 23118544486528 run.py:483] Algo bellman_ford step 225 current loss 0.067777, current_train_items 7232.
I0304 19:28:04.459477 23118544486528 run.py:483] Algo bellman_ford step 226 current loss 0.168137, current_train_items 7264.
I0304 19:28:04.483109 23118544486528 run.py:483] Algo bellman_ford step 227 current loss 0.362135, current_train_items 7296.
I0304 19:28:04.513470 23118544486528 run.py:483] Algo bellman_ford step 228 current loss 0.248245, current_train_items 7328.
I0304 19:28:04.545958 23118544486528 run.py:483] Algo bellman_ford step 229 current loss 0.350018, current_train_items 7360.
I0304 19:28:04.564336 23118544486528 run.py:483] Algo bellman_ford step 230 current loss 0.049549, current_train_items 7392.
I0304 19:28:04.580689 23118544486528 run.py:483] Algo bellman_ford step 231 current loss 0.165498, current_train_items 7424.
I0304 19:28:04.604605 23118544486528 run.py:483] Algo bellman_ford step 232 current loss 0.328479, current_train_items 7456.
I0304 19:28:04.634718 23118544486528 run.py:483] Algo bellman_ford step 233 current loss 0.486795, current_train_items 7488.
I0304 19:28:04.667876 23118544486528 run.py:483] Algo bellman_ford step 234 current loss 0.473667, current_train_items 7520.
I0304 19:28:04.686728 23118544486528 run.py:483] Algo bellman_ford step 235 current loss 0.139752, current_train_items 7552.
I0304 19:28:04.703010 23118544486528 run.py:483] Algo bellman_ford step 236 current loss 0.192663, current_train_items 7584.
I0304 19:28:04.726744 23118544486528 run.py:483] Algo bellman_ford step 237 current loss 0.303329, current_train_items 7616.
I0304 19:28:04.756374 23118544486528 run.py:483] Algo bellman_ford step 238 current loss 0.323266, current_train_items 7648.
I0304 19:28:04.787967 23118544486528 run.py:483] Algo bellman_ford step 239 current loss 0.341798, current_train_items 7680.
I0304 19:28:04.806671 23118544486528 run.py:483] Algo bellman_ford step 240 current loss 0.049065, current_train_items 7712.
I0304 19:28:04.823288 23118544486528 run.py:483] Algo bellman_ford step 241 current loss 0.214535, current_train_items 7744.
I0304 19:28:04.846947 23118544486528 run.py:483] Algo bellman_ford step 242 current loss 0.335300, current_train_items 7776.
I0304 19:28:04.876042 23118544486528 run.py:483] Algo bellman_ford step 243 current loss 0.389430, current_train_items 7808.
I0304 19:28:04.905320 23118544486528 run.py:483] Algo bellman_ford step 244 current loss 0.325008, current_train_items 7840.
I0304 19:28:04.924071 23118544486528 run.py:483] Algo bellman_ford step 245 current loss 0.080462, current_train_items 7872.
I0304 19:28:04.940380 23118544486528 run.py:483] Algo bellman_ford step 246 current loss 0.130295, current_train_items 7904.
I0304 19:28:04.963432 23118544486528 run.py:483] Algo bellman_ford step 247 current loss 0.249257, current_train_items 7936.
I0304 19:28:04.993821 23118544486528 run.py:483] Algo bellman_ford step 248 current loss 0.352702, current_train_items 7968.
I0304 19:28:05.025955 23118544486528 run.py:483] Algo bellman_ford step 249 current loss 0.303059, current_train_items 8000.
I0304 19:28:05.044507 23118544486528 run.py:483] Algo bellman_ford step 250 current loss 0.059069, current_train_items 8032.
I0304 19:28:05.052514 23118544486528 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0304 19:28:05.052621 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0304 19:28:05.070082 23118544486528 run.py:483] Algo bellman_ford step 251 current loss 0.186278, current_train_items 8064.
I0304 19:28:05.094591 23118544486528 run.py:483] Algo bellman_ford step 252 current loss 0.284085, current_train_items 8096.
I0304 19:28:05.125803 23118544486528 run.py:483] Algo bellman_ford step 253 current loss 0.399075, current_train_items 8128.
I0304 19:28:05.158858 23118544486528 run.py:483] Algo bellman_ford step 254 current loss 0.448209, current_train_items 8160.
I0304 19:28:05.178198 23118544486528 run.py:483] Algo bellman_ford step 255 current loss 0.102910, current_train_items 8192.
I0304 19:28:05.194033 23118544486528 run.py:483] Algo bellman_ford step 256 current loss 0.155443, current_train_items 8224.
I0304 19:28:05.217285 23118544486528 run.py:483] Algo bellman_ford step 257 current loss 0.243848, current_train_items 8256.
I0304 19:28:05.247440 23118544486528 run.py:483] Algo bellman_ford step 258 current loss 0.297108, current_train_items 8288.
I0304 19:28:05.278084 23118544486528 run.py:483] Algo bellman_ford step 259 current loss 0.338247, current_train_items 8320.
I0304 19:28:05.297317 23118544486528 run.py:483] Algo bellman_ford step 260 current loss 0.052275, current_train_items 8352.
I0304 19:28:05.313562 23118544486528 run.py:483] Algo bellman_ford step 261 current loss 0.123775, current_train_items 8384.
I0304 19:28:05.336813 23118544486528 run.py:483] Algo bellman_ford step 262 current loss 0.321567, current_train_items 8416.
I0304 19:28:05.366230 23118544486528 run.py:483] Algo bellman_ford step 263 current loss 0.320943, current_train_items 8448.
I0304 19:28:05.397142 23118544486528 run.py:483] Algo bellman_ford step 264 current loss 0.350405, current_train_items 8480.
I0304 19:28:05.415834 23118544486528 run.py:483] Algo bellman_ford step 265 current loss 0.097420, current_train_items 8512.
I0304 19:28:05.432394 23118544486528 run.py:483] Algo bellman_ford step 266 current loss 0.226990, current_train_items 8544.
I0304 19:28:05.456958 23118544486528 run.py:483] Algo bellman_ford step 267 current loss 0.369780, current_train_items 8576.
I0304 19:28:05.487741 23118544486528 run.py:483] Algo bellman_ford step 268 current loss 0.424476, current_train_items 8608.
I0304 19:28:05.518595 23118544486528 run.py:483] Algo bellman_ford step 269 current loss 0.350727, current_train_items 8640.
I0304 19:28:05.537967 23118544486528 run.py:483] Algo bellman_ford step 270 current loss 0.054705, current_train_items 8672.
I0304 19:28:05.553959 23118544486528 run.py:483] Algo bellman_ford step 271 current loss 0.121469, current_train_items 8704.
I0304 19:28:05.578505 23118544486528 run.py:483] Algo bellman_ford step 272 current loss 0.264100, current_train_items 8736.
I0304 19:28:05.608285 23118544486528 run.py:483] Algo bellman_ford step 273 current loss 0.299429, current_train_items 8768.
I0304 19:28:05.639482 23118544486528 run.py:483] Algo bellman_ford step 274 current loss 0.387488, current_train_items 8800.
I0304 19:28:05.658612 23118544486528 run.py:483] Algo bellman_ford step 275 current loss 0.039871, current_train_items 8832.
I0304 19:28:05.675067 23118544486528 run.py:483] Algo bellman_ford step 276 current loss 0.129159, current_train_items 8864.
I0304 19:28:05.699755 23118544486528 run.py:483] Algo bellman_ford step 277 current loss 0.289456, current_train_items 8896.
I0304 19:28:05.730322 23118544486528 run.py:483] Algo bellman_ford step 278 current loss 0.385468, current_train_items 8928.
I0304 19:28:05.761816 23118544486528 run.py:483] Algo bellman_ford step 279 current loss 0.335199, current_train_items 8960.
I0304 19:28:05.780636 23118544486528 run.py:483] Algo bellman_ford step 280 current loss 0.088251, current_train_items 8992.
I0304 19:28:05.797329 23118544486528 run.py:483] Algo bellman_ford step 281 current loss 0.170009, current_train_items 9024.
I0304 19:28:05.821562 23118544486528 run.py:483] Algo bellman_ford step 282 current loss 0.252415, current_train_items 9056.
I0304 19:28:05.851065 23118544486528 run.py:483] Algo bellman_ford step 283 current loss 0.359173, current_train_items 9088.
I0304 19:28:05.884241 23118544486528 run.py:483] Algo bellman_ford step 284 current loss 0.524613, current_train_items 9120.
I0304 19:28:05.903295 23118544486528 run.py:483] Algo bellman_ford step 285 current loss 0.056088, current_train_items 9152.
I0304 19:28:05.920026 23118544486528 run.py:483] Algo bellman_ford step 286 current loss 0.141123, current_train_items 9184.
I0304 19:28:05.943277 23118544486528 run.py:483] Algo bellman_ford step 287 current loss 0.242506, current_train_items 9216.
I0304 19:28:05.973778 23118544486528 run.py:483] Algo bellman_ford step 288 current loss 0.282195, current_train_items 9248.
I0304 19:28:06.006666 23118544486528 run.py:483] Algo bellman_ford step 289 current loss 0.332693, current_train_items 9280.
I0304 19:28:06.025729 23118544486528 run.py:483] Algo bellman_ford step 290 current loss 0.026822, current_train_items 9312.
I0304 19:28:06.042098 23118544486528 run.py:483] Algo bellman_ford step 291 current loss 0.102512, current_train_items 9344.
I0304 19:28:06.065628 23118544486528 run.py:483] Algo bellman_ford step 292 current loss 0.193653, current_train_items 9376.
I0304 19:28:06.094965 23118544486528 run.py:483] Algo bellman_ford step 293 current loss 0.274332, current_train_items 9408.
I0304 19:28:06.126509 23118544486528 run.py:483] Algo bellman_ford step 294 current loss 0.320830, current_train_items 9440.
I0304 19:28:06.145310 23118544486528 run.py:483] Algo bellman_ford step 295 current loss 0.059060, current_train_items 9472.
I0304 19:28:06.161417 23118544486528 run.py:483] Algo bellman_ford step 296 current loss 0.128971, current_train_items 9504.
I0304 19:28:06.185646 23118544486528 run.py:483] Algo bellman_ford step 297 current loss 0.317534, current_train_items 9536.
I0304 19:28:06.216521 23118544486528 run.py:483] Algo bellman_ford step 298 current loss 0.323057, current_train_items 9568.
I0304 19:28:06.248105 23118544486528 run.py:483] Algo bellman_ford step 299 current loss 0.358381, current_train_items 9600.
I0304 19:28:06.267570 23118544486528 run.py:483] Algo bellman_ford step 300 current loss 0.066527, current_train_items 9632.
I0304 19:28:06.275520 23118544486528 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0304 19:28:06.275630 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0304 19:28:06.292712 23118544486528 run.py:483] Algo bellman_ford step 301 current loss 0.174577, current_train_items 9664.
I0304 19:28:06.316073 23118544486528 run.py:483] Algo bellman_ford step 302 current loss 0.229598, current_train_items 9696.
I0304 19:28:06.345419 23118544486528 run.py:483] Algo bellman_ford step 303 current loss 0.176816, current_train_items 9728.
I0304 19:28:06.378826 23118544486528 run.py:483] Algo bellman_ford step 304 current loss 0.461464, current_train_items 9760.
I0304 19:28:06.397902 23118544486528 run.py:483] Algo bellman_ford step 305 current loss 0.051691, current_train_items 9792.
I0304 19:28:06.414646 23118544486528 run.py:483] Algo bellman_ford step 306 current loss 0.181034, current_train_items 9824.
I0304 19:28:06.439338 23118544486528 run.py:483] Algo bellman_ford step 307 current loss 0.228210, current_train_items 9856.
I0304 19:28:06.470061 23118544486528 run.py:483] Algo bellman_ford step 308 current loss 0.326510, current_train_items 9888.
I0304 19:28:06.501878 23118544486528 run.py:483] Algo bellman_ford step 309 current loss 0.459550, current_train_items 9920.
I0304 19:28:06.521116 23118544486528 run.py:483] Algo bellman_ford step 310 current loss 0.096480, current_train_items 9952.
I0304 19:28:06.537342 23118544486528 run.py:483] Algo bellman_ford step 311 current loss 0.116467, current_train_items 9984.
I0304 19:28:06.560981 23118544486528 run.py:483] Algo bellman_ford step 312 current loss 0.191596, current_train_items 10016.
I0304 19:28:06.591743 23118544486528 run.py:483] Algo bellman_ford step 313 current loss 0.254930, current_train_items 10048.
I0304 19:28:06.623887 23118544486528 run.py:483] Algo bellman_ford step 314 current loss 0.342019, current_train_items 10080.
I0304 19:28:06.642512 23118544486528 run.py:483] Algo bellman_ford step 315 current loss 0.045167, current_train_items 10112.
I0304 19:28:06.659152 23118544486528 run.py:483] Algo bellman_ford step 316 current loss 0.091364, current_train_items 10144.
I0304 19:28:06.682079 23118544486528 run.py:483] Algo bellman_ford step 317 current loss 0.184878, current_train_items 10176.
I0304 19:28:06.710720 23118544486528 run.py:483] Algo bellman_ford step 318 current loss 0.172867, current_train_items 10208.
I0304 19:28:06.741863 23118544486528 run.py:483] Algo bellman_ford step 319 current loss 0.308336, current_train_items 10240.
I0304 19:28:06.760672 23118544486528 run.py:483] Algo bellman_ford step 320 current loss 0.032960, current_train_items 10272.
I0304 19:28:06.777189 23118544486528 run.py:483] Algo bellman_ford step 321 current loss 0.126426, current_train_items 10304.
I0304 19:28:06.801042 23118544486528 run.py:483] Algo bellman_ford step 322 current loss 0.244995, current_train_items 10336.
I0304 19:28:06.830094 23118544486528 run.py:483] Algo bellman_ford step 323 current loss 0.158213, current_train_items 10368.
I0304 19:28:06.862382 23118544486528 run.py:483] Algo bellman_ford step 324 current loss 0.255360, current_train_items 10400.
I0304 19:28:06.881704 23118544486528 run.py:483] Algo bellman_ford step 325 current loss 0.056689, current_train_items 10432.
I0304 19:28:06.897710 23118544486528 run.py:483] Algo bellman_ford step 326 current loss 0.070020, current_train_items 10464.
I0304 19:28:06.921408 23118544486528 run.py:483] Algo bellman_ford step 327 current loss 0.230391, current_train_items 10496.
I0304 19:28:06.951887 23118544486528 run.py:483] Algo bellman_ford step 328 current loss 0.283328, current_train_items 10528.
I0304 19:28:06.984448 23118544486528 run.py:483] Algo bellman_ford step 329 current loss 0.292284, current_train_items 10560.
I0304 19:28:07.003247 23118544486528 run.py:483] Algo bellman_ford step 330 current loss 0.052040, current_train_items 10592.
I0304 19:28:07.019619 23118544486528 run.py:483] Algo bellman_ford step 331 current loss 0.113004, current_train_items 10624.
I0304 19:28:07.043555 23118544486528 run.py:483] Algo bellman_ford step 332 current loss 0.308770, current_train_items 10656.
I0304 19:28:07.073840 23118544486528 run.py:483] Algo bellman_ford step 333 current loss 0.277045, current_train_items 10688.
I0304 19:28:07.107150 23118544486528 run.py:483] Algo bellman_ford step 334 current loss 0.396347, current_train_items 10720.
I0304 19:28:07.126060 23118544486528 run.py:483] Algo bellman_ford step 335 current loss 0.037767, current_train_items 10752.
I0304 19:28:07.142795 23118544486528 run.py:483] Algo bellman_ford step 336 current loss 0.160978, current_train_items 10784.
I0304 19:28:07.167963 23118544486528 run.py:483] Algo bellman_ford step 337 current loss 0.286033, current_train_items 10816.
I0304 19:28:07.197929 23118544486528 run.py:483] Algo bellman_ford step 338 current loss 0.322360, current_train_items 10848.
I0304 19:28:07.231235 23118544486528 run.py:483] Algo bellman_ford step 339 current loss 0.279688, current_train_items 10880.
I0304 19:28:07.250154 23118544486528 run.py:483] Algo bellman_ford step 340 current loss 0.039647, current_train_items 10912.
I0304 19:28:07.266447 23118544486528 run.py:483] Algo bellman_ford step 341 current loss 0.189312, current_train_items 10944.
I0304 19:28:07.289739 23118544486528 run.py:483] Algo bellman_ford step 342 current loss 0.404256, current_train_items 10976.
I0304 19:28:07.318598 23118544486528 run.py:483] Algo bellman_ford step 343 current loss 0.302963, current_train_items 11008.
I0304 19:28:07.351251 23118544486528 run.py:483] Algo bellman_ford step 344 current loss 0.331093, current_train_items 11040.
I0304 19:28:07.370233 23118544486528 run.py:483] Algo bellman_ford step 345 current loss 0.041842, current_train_items 11072.
I0304 19:28:07.386512 23118544486528 run.py:483] Algo bellman_ford step 346 current loss 0.204703, current_train_items 11104.
I0304 19:28:07.411459 23118544486528 run.py:483] Algo bellman_ford step 347 current loss 0.363663, current_train_items 11136.
I0304 19:28:07.441296 23118544486528 run.py:483] Algo bellman_ford step 348 current loss 0.382248, current_train_items 11168.
I0304 19:28:07.474288 23118544486528 run.py:483] Algo bellman_ford step 349 current loss 0.362893, current_train_items 11200.
I0304 19:28:07.493271 23118544486528 run.py:483] Algo bellman_ford step 350 current loss 0.081365, current_train_items 11232.
I0304 19:28:07.501290 23118544486528 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0304 19:28:07.501397 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0304 19:28:07.518007 23118544486528 run.py:483] Algo bellman_ford step 351 current loss 0.237404, current_train_items 11264.
I0304 19:28:07.543205 23118544486528 run.py:483] Algo bellman_ford step 352 current loss 0.289794, current_train_items 11296.
I0304 19:28:07.573632 23118544486528 run.py:483] Algo bellman_ford step 353 current loss 0.257699, current_train_items 11328.
I0304 19:28:07.607781 23118544486528 run.py:483] Algo bellman_ford step 354 current loss 0.379028, current_train_items 11360.
I0304 19:28:07.627120 23118544486528 run.py:483] Algo bellman_ford step 355 current loss 0.063006, current_train_items 11392.
I0304 19:28:07.643500 23118544486528 run.py:483] Algo bellman_ford step 356 current loss 0.134539, current_train_items 11424.
I0304 19:28:07.667647 23118544486528 run.py:483] Algo bellman_ford step 357 current loss 0.199698, current_train_items 11456.
I0304 19:28:07.699074 23118544486528 run.py:483] Algo bellman_ford step 358 current loss 0.375436, current_train_items 11488.
I0304 19:28:07.732226 23118544486528 run.py:483] Algo bellman_ford step 359 current loss 0.306965, current_train_items 11520.
I0304 19:28:07.751761 23118544486528 run.py:483] Algo bellman_ford step 360 current loss 0.045545, current_train_items 11552.
I0304 19:28:07.768653 23118544486528 run.py:483] Algo bellman_ford step 361 current loss 0.273923, current_train_items 11584.
I0304 19:28:07.791455 23118544486528 run.py:483] Algo bellman_ford step 362 current loss 0.283398, current_train_items 11616.
I0304 19:28:07.821267 23118544486528 run.py:483] Algo bellman_ford step 363 current loss 0.246865, current_train_items 11648.
I0304 19:28:07.851737 23118544486528 run.py:483] Algo bellman_ford step 364 current loss 0.261422, current_train_items 11680.
I0304 19:28:07.870799 23118544486528 run.py:483] Algo bellman_ford step 365 current loss 0.078315, current_train_items 11712.
I0304 19:28:07.887202 23118544486528 run.py:483] Algo bellman_ford step 366 current loss 0.164850, current_train_items 11744.
I0304 19:28:07.910451 23118544486528 run.py:483] Algo bellman_ford step 367 current loss 0.185377, current_train_items 11776.
I0304 19:28:07.941042 23118544486528 run.py:483] Algo bellman_ford step 368 current loss 0.282631, current_train_items 11808.
I0304 19:28:07.971074 23118544486528 run.py:483] Algo bellman_ford step 369 current loss 0.268087, current_train_items 11840.
I0304 19:28:07.990156 23118544486528 run.py:483] Algo bellman_ford step 370 current loss 0.051560, current_train_items 11872.
I0304 19:28:08.006286 23118544486528 run.py:483] Algo bellman_ford step 371 current loss 0.105923, current_train_items 11904.
I0304 19:28:08.029108 23118544486528 run.py:483] Algo bellman_ford step 372 current loss 0.201125, current_train_items 11936.
I0304 19:28:08.059960 23118544486528 run.py:483] Algo bellman_ford step 373 current loss 0.343282, current_train_items 11968.
I0304 19:28:08.094028 23118544486528 run.py:483] Algo bellman_ford step 374 current loss 0.646771, current_train_items 12000.
I0304 19:28:08.113399 23118544486528 run.py:483] Algo bellman_ford step 375 current loss 0.035128, current_train_items 12032.
I0304 19:28:08.129344 23118544486528 run.py:483] Algo bellman_ford step 376 current loss 0.105230, current_train_items 12064.
I0304 19:28:08.153807 23118544486528 run.py:483] Algo bellman_ford step 377 current loss 0.237106, current_train_items 12096.
I0304 19:28:08.183673 23118544486528 run.py:483] Algo bellman_ford step 378 current loss 0.205412, current_train_items 12128.
I0304 19:28:08.216950 23118544486528 run.py:483] Algo bellman_ford step 379 current loss 0.396332, current_train_items 12160.
I0304 19:28:08.235604 23118544486528 run.py:483] Algo bellman_ford step 380 current loss 0.065412, current_train_items 12192.
I0304 19:28:08.252288 23118544486528 run.py:483] Algo bellman_ford step 381 current loss 0.132134, current_train_items 12224.
I0304 19:28:08.276629 23118544486528 run.py:483] Algo bellman_ford step 382 current loss 0.160321, current_train_items 12256.
I0304 19:28:08.307102 23118544486528 run.py:483] Algo bellman_ford step 383 current loss 0.258970, current_train_items 12288.
I0304 19:28:08.340321 23118544486528 run.py:483] Algo bellman_ford step 384 current loss 0.359625, current_train_items 12320.
I0304 19:28:08.359889 23118544486528 run.py:483] Algo bellman_ford step 385 current loss 0.068347, current_train_items 12352.
I0304 19:28:08.376499 23118544486528 run.py:483] Algo bellman_ford step 386 current loss 0.102034, current_train_items 12384.
I0304 19:28:08.399952 23118544486528 run.py:483] Algo bellman_ford step 387 current loss 0.251056, current_train_items 12416.
I0304 19:28:08.429263 23118544486528 run.py:483] Algo bellman_ford step 388 current loss 0.264440, current_train_items 12448.
I0304 19:28:08.462355 23118544486528 run.py:483] Algo bellman_ford step 389 current loss 0.300603, current_train_items 12480.
I0304 19:28:08.481385 23118544486528 run.py:483] Algo bellman_ford step 390 current loss 0.035706, current_train_items 12512.
I0304 19:28:08.497578 23118544486528 run.py:483] Algo bellman_ford step 391 current loss 0.078352, current_train_items 12544.
I0304 19:28:08.521474 23118544486528 run.py:483] Algo bellman_ford step 392 current loss 0.196651, current_train_items 12576.
I0304 19:28:08.553534 23118544486528 run.py:483] Algo bellman_ford step 393 current loss 0.254146, current_train_items 12608.
I0304 19:28:08.585269 23118544486528 run.py:483] Algo bellman_ford step 394 current loss 0.291123, current_train_items 12640.
I0304 19:28:08.603957 23118544486528 run.py:483] Algo bellman_ford step 395 current loss 0.032521, current_train_items 12672.
I0304 19:28:08.620384 23118544486528 run.py:483] Algo bellman_ford step 396 current loss 0.113972, current_train_items 12704.
I0304 19:28:08.644841 23118544486528 run.py:483] Algo bellman_ford step 397 current loss 0.248528, current_train_items 12736.
I0304 19:28:08.675980 23118544486528 run.py:483] Algo bellman_ford step 398 current loss 0.237393, current_train_items 12768.
I0304 19:28:08.706084 23118544486528 run.py:483] Algo bellman_ford step 399 current loss 0.247019, current_train_items 12800.
I0304 19:28:08.725351 23118544486528 run.py:483] Algo bellman_ford step 400 current loss 0.024347, current_train_items 12832.
I0304 19:28:08.733001 23118544486528 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.958984375, 'score': 0.958984375, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0304 19:28:08.733123 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.959, val scores are: bellman_ford: 0.959
I0304 19:28:08.763071 23118544486528 run.py:483] Algo bellman_ford step 401 current loss 0.112060, current_train_items 12864.
I0304 19:28:08.787693 23118544486528 run.py:483] Algo bellman_ford step 402 current loss 0.171621, current_train_items 12896.
I0304 19:28:08.819052 23118544486528 run.py:483] Algo bellman_ford step 403 current loss 0.233294, current_train_items 12928.
I0304 19:28:08.850305 23118544486528 run.py:483] Algo bellman_ford step 404 current loss 0.287010, current_train_items 12960.
I0304 19:28:08.870127 23118544486528 run.py:483] Algo bellman_ford step 405 current loss 0.102465, current_train_items 12992.
I0304 19:28:08.885816 23118544486528 run.py:483] Algo bellman_ford step 406 current loss 0.106547, current_train_items 13024.
I0304 19:28:08.909395 23118544486528 run.py:483] Algo bellman_ford step 407 current loss 0.223931, current_train_items 13056.
I0304 19:28:08.940519 23118544486528 run.py:483] Algo bellman_ford step 408 current loss 0.289071, current_train_items 13088.
I0304 19:28:08.973418 23118544486528 run.py:483] Algo bellman_ford step 409 current loss 0.294996, current_train_items 13120.
I0304 19:28:08.991996 23118544486528 run.py:483] Algo bellman_ford step 410 current loss 0.044328, current_train_items 13152.
I0304 19:28:09.008197 23118544486528 run.py:483] Algo bellman_ford step 411 current loss 0.069441, current_train_items 13184.
I0304 19:28:09.031867 23118544486528 run.py:483] Algo bellman_ford step 412 current loss 0.167353, current_train_items 13216.
I0304 19:28:09.062107 23118544486528 run.py:483] Algo bellman_ford step 413 current loss 0.186382, current_train_items 13248.
I0304 19:28:09.095192 23118544486528 run.py:483] Algo bellman_ford step 414 current loss 0.258600, current_train_items 13280.
I0304 19:28:09.114193 23118544486528 run.py:483] Algo bellman_ford step 415 current loss 0.046580, current_train_items 13312.
I0304 19:28:09.130435 23118544486528 run.py:483] Algo bellman_ford step 416 current loss 0.065486, current_train_items 13344.
I0304 19:28:09.154864 23118544486528 run.py:483] Algo bellman_ford step 417 current loss 0.195952, current_train_items 13376.
I0304 19:28:09.186978 23118544486528 run.py:483] Algo bellman_ford step 418 current loss 0.215548, current_train_items 13408.
I0304 19:28:09.219575 23118544486528 run.py:483] Algo bellman_ford step 419 current loss 0.270830, current_train_items 13440.
I0304 19:28:09.238091 23118544486528 run.py:483] Algo bellman_ford step 420 current loss 0.045488, current_train_items 13472.
I0304 19:28:09.254265 23118544486528 run.py:483] Algo bellman_ford step 421 current loss 0.074705, current_train_items 13504.
I0304 19:28:09.278958 23118544486528 run.py:483] Algo bellman_ford step 422 current loss 0.151119, current_train_items 13536.
I0304 19:28:09.308192 23118544486528 run.py:483] Algo bellman_ford step 423 current loss 0.267985, current_train_items 13568.
I0304 19:28:09.340315 23118544486528 run.py:483] Algo bellman_ford step 424 current loss 0.290207, current_train_items 13600.
I0304 19:28:09.359081 23118544486528 run.py:483] Algo bellman_ford step 425 current loss 0.106539, current_train_items 13632.
I0304 19:28:09.375069 23118544486528 run.py:483] Algo bellman_ford step 426 current loss 0.047281, current_train_items 13664.
I0304 19:28:09.398375 23118544486528 run.py:483] Algo bellman_ford step 427 current loss 0.271603, current_train_items 13696.
I0304 19:28:09.427666 23118544486528 run.py:483] Algo bellman_ford step 428 current loss 0.279326, current_train_items 13728.
I0304 19:28:09.460433 23118544486528 run.py:483] Algo bellman_ford step 429 current loss 0.244372, current_train_items 13760.
I0304 19:28:09.479595 23118544486528 run.py:483] Algo bellman_ford step 430 current loss 0.056639, current_train_items 13792.
I0304 19:28:09.495838 23118544486528 run.py:483] Algo bellman_ford step 431 current loss 0.141776, current_train_items 13824.
I0304 19:28:09.519767 23118544486528 run.py:483] Algo bellman_ford step 432 current loss 0.378116, current_train_items 13856.
I0304 19:28:09.548677 23118544486528 run.py:483] Algo bellman_ford step 433 current loss 0.202873, current_train_items 13888.
I0304 19:28:09.580721 23118544486528 run.py:483] Algo bellman_ford step 434 current loss 0.286724, current_train_items 13920.
I0304 19:28:09.599911 23118544486528 run.py:483] Algo bellman_ford step 435 current loss 0.041626, current_train_items 13952.
I0304 19:28:09.616608 23118544486528 run.py:483] Algo bellman_ford step 436 current loss 0.102140, current_train_items 13984.
I0304 19:28:09.639894 23118544486528 run.py:483] Algo bellman_ford step 437 current loss 0.147186, current_train_items 14016.
I0304 19:28:09.668932 23118544486528 run.py:483] Algo bellman_ford step 438 current loss 0.215074, current_train_items 14048.
I0304 19:28:09.701166 23118544486528 run.py:483] Algo bellman_ford step 439 current loss 0.352895, current_train_items 14080.
I0304 19:28:09.719647 23118544486528 run.py:483] Algo bellman_ford step 440 current loss 0.069524, current_train_items 14112.
I0304 19:28:09.735646 23118544486528 run.py:483] Algo bellman_ford step 441 current loss 0.106592, current_train_items 14144.
I0304 19:28:09.758738 23118544486528 run.py:483] Algo bellman_ford step 442 current loss 0.180803, current_train_items 14176.
I0304 19:28:09.788859 23118544486528 run.py:483] Algo bellman_ford step 443 current loss 0.295981, current_train_items 14208.
I0304 19:28:09.820282 23118544486528 run.py:483] Algo bellman_ford step 444 current loss 0.236760, current_train_items 14240.
I0304 19:28:09.839293 23118544486528 run.py:483] Algo bellman_ford step 445 current loss 0.043084, current_train_items 14272.
I0304 19:28:09.856492 23118544486528 run.py:483] Algo bellman_ford step 446 current loss 0.220983, current_train_items 14304.
I0304 19:28:09.880952 23118544486528 run.py:483] Algo bellman_ford step 447 current loss 0.243719, current_train_items 14336.
I0304 19:28:09.909600 23118544486528 run.py:483] Algo bellman_ford step 448 current loss 0.207517, current_train_items 14368.
I0304 19:28:09.939912 23118544486528 run.py:483] Algo bellman_ford step 449 current loss 0.218124, current_train_items 14400.
I0304 19:28:09.958702 23118544486528 run.py:483] Algo bellman_ford step 450 current loss 0.058547, current_train_items 14432.
I0304 19:28:09.966986 23118544486528 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0304 19:28:09.967091 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.959, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:09.997863 23118544486528 run.py:483] Algo bellman_ford step 451 current loss 0.103762, current_train_items 14464.
I0304 19:28:10.021126 23118544486528 run.py:483] Algo bellman_ford step 452 current loss 0.141108, current_train_items 14496.
I0304 19:28:10.052410 23118544486528 run.py:483] Algo bellman_ford step 453 current loss 0.289099, current_train_items 14528.
I0304 19:28:10.085641 23118544486528 run.py:483] Algo bellman_ford step 454 current loss 0.267252, current_train_items 14560.
I0304 19:28:10.104967 23118544486528 run.py:483] Algo bellman_ford step 455 current loss 0.030569, current_train_items 14592.
I0304 19:28:10.121266 23118544486528 run.py:483] Algo bellman_ford step 456 current loss 0.258906, current_train_items 14624.
I0304 19:28:10.145020 23118544486528 run.py:483] Algo bellman_ford step 457 current loss 0.279032, current_train_items 14656.
I0304 19:28:10.174592 23118544486528 run.py:483] Algo bellman_ford step 458 current loss 0.188149, current_train_items 14688.
I0304 19:28:10.205735 23118544486528 run.py:483] Algo bellman_ford step 459 current loss 0.310184, current_train_items 14720.
I0304 19:28:10.224609 23118544486528 run.py:483] Algo bellman_ford step 460 current loss 0.060821, current_train_items 14752.
I0304 19:28:10.241321 23118544486528 run.py:483] Algo bellman_ford step 461 current loss 0.127969, current_train_items 14784.
I0304 19:28:10.265277 23118544486528 run.py:483] Algo bellman_ford step 462 current loss 0.227183, current_train_items 14816.
I0304 19:28:10.295390 23118544486528 run.py:483] Algo bellman_ford step 463 current loss 0.273891, current_train_items 14848.
I0304 19:28:10.328754 23118544486528 run.py:483] Algo bellman_ford step 464 current loss 0.373055, current_train_items 14880.
I0304 19:28:10.347709 23118544486528 run.py:483] Algo bellman_ford step 465 current loss 0.055591, current_train_items 14912.
I0304 19:28:10.363510 23118544486528 run.py:483] Algo bellman_ford step 466 current loss 0.117571, current_train_items 14944.
I0304 19:28:10.387647 23118544486528 run.py:483] Algo bellman_ford step 467 current loss 0.270080, current_train_items 14976.
I0304 19:28:10.416220 23118544486528 run.py:483] Algo bellman_ford step 468 current loss 0.412387, current_train_items 15008.
I0304 19:28:10.449551 23118544486528 run.py:483] Algo bellman_ford step 469 current loss 0.446413, current_train_items 15040.
I0304 19:28:10.468495 23118544486528 run.py:483] Algo bellman_ford step 470 current loss 0.089521, current_train_items 15072.
I0304 19:28:10.485044 23118544486528 run.py:483] Algo bellman_ford step 471 current loss 0.141548, current_train_items 15104.
I0304 19:28:10.508939 23118544486528 run.py:483] Algo bellman_ford step 472 current loss 0.217869, current_train_items 15136.
I0304 19:28:10.538990 23118544486528 run.py:483] Algo bellman_ford step 473 current loss 0.204157, current_train_items 15168.
I0304 19:28:10.572026 23118544486528 run.py:483] Algo bellman_ford step 474 current loss 0.320811, current_train_items 15200.
I0304 19:28:10.591487 23118544486528 run.py:483] Algo bellman_ford step 475 current loss 0.058719, current_train_items 15232.
I0304 19:28:10.607643 23118544486528 run.py:483] Algo bellman_ford step 476 current loss 0.147002, current_train_items 15264.
I0304 19:28:10.631832 23118544486528 run.py:483] Algo bellman_ford step 477 current loss 0.229464, current_train_items 15296.
I0304 19:28:10.660441 23118544486528 run.py:483] Algo bellman_ford step 478 current loss 0.160545, current_train_items 15328.
I0304 19:28:10.693463 23118544486528 run.py:483] Algo bellman_ford step 479 current loss 0.434905, current_train_items 15360.
I0304 19:28:10.712054 23118544486528 run.py:483] Algo bellman_ford step 480 current loss 0.040113, current_train_items 15392.
I0304 19:28:10.728538 23118544486528 run.py:483] Algo bellman_ford step 481 current loss 0.089082, current_train_items 15424.
I0304 19:28:10.751566 23118544486528 run.py:483] Algo bellman_ford step 482 current loss 0.162659, current_train_items 15456.
I0304 19:28:10.782339 23118544486528 run.py:483] Algo bellman_ford step 483 current loss 0.185615, current_train_items 15488.
I0304 19:28:10.814937 23118544486528 run.py:483] Algo bellman_ford step 484 current loss 0.245692, current_train_items 15520.
I0304 19:28:10.833847 23118544486528 run.py:483] Algo bellman_ford step 485 current loss 0.041119, current_train_items 15552.
I0304 19:28:10.850227 23118544486528 run.py:483] Algo bellman_ford step 486 current loss 0.093037, current_train_items 15584.
I0304 19:28:10.873827 23118544486528 run.py:483] Algo bellman_ford step 487 current loss 0.220608, current_train_items 15616.
I0304 19:28:10.903571 23118544486528 run.py:483] Algo bellman_ford step 488 current loss 0.184888, current_train_items 15648.
I0304 19:28:10.935025 23118544486528 run.py:483] Algo bellman_ford step 489 current loss 0.235155, current_train_items 15680.
I0304 19:28:10.953981 23118544486528 run.py:483] Algo bellman_ford step 490 current loss 0.024654, current_train_items 15712.
I0304 19:28:10.970535 23118544486528 run.py:483] Algo bellman_ford step 491 current loss 0.076084, current_train_items 15744.
I0304 19:28:10.993986 23118544486528 run.py:483] Algo bellman_ford step 492 current loss 0.144922, current_train_items 15776.
I0304 19:28:11.023894 23118544486528 run.py:483] Algo bellman_ford step 493 current loss 0.173504, current_train_items 15808.
I0304 19:28:11.057256 23118544486528 run.py:483] Algo bellman_ford step 494 current loss 0.285051, current_train_items 15840.
I0304 19:28:11.076038 23118544486528 run.py:483] Algo bellman_ford step 495 current loss 0.028840, current_train_items 15872.
I0304 19:28:11.092218 23118544486528 run.py:483] Algo bellman_ford step 496 current loss 0.090607, current_train_items 15904.
I0304 19:28:11.117031 23118544486528 run.py:483] Algo bellman_ford step 497 current loss 0.250765, current_train_items 15936.
I0304 19:28:11.147454 23118544486528 run.py:483] Algo bellman_ford step 498 current loss 0.213416, current_train_items 15968.
I0304 19:28:11.179527 23118544486528 run.py:483] Algo bellman_ford step 499 current loss 0.204725, current_train_items 16000.
I0304 19:28:11.198452 23118544486528 run.py:483] Algo bellman_ford step 500 current loss 0.035925, current_train_items 16032.
I0304 19:28:11.206286 23118544486528 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0304 19:28:11.206390 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0304 19:28:11.222714 23118544486528 run.py:483] Algo bellman_ford step 501 current loss 0.106859, current_train_items 16064.
I0304 19:28:11.246923 23118544486528 run.py:483] Algo bellman_ford step 502 current loss 0.269001, current_train_items 16096.
I0304 19:28:11.278413 23118544486528 run.py:483] Algo bellman_ford step 503 current loss 0.246705, current_train_items 16128.
I0304 19:28:11.313239 23118544486528 run.py:483] Algo bellman_ford step 504 current loss 0.274386, current_train_items 16160.
I0304 19:28:11.332591 23118544486528 run.py:483] Algo bellman_ford step 505 current loss 0.035536, current_train_items 16192.
I0304 19:28:11.348060 23118544486528 run.py:483] Algo bellman_ford step 506 current loss 0.066695, current_train_items 16224.
I0304 19:28:11.371206 23118544486528 run.py:483] Algo bellman_ford step 507 current loss 0.169609, current_train_items 16256.
I0304 19:28:11.401891 23118544486528 run.py:483] Algo bellman_ford step 508 current loss 0.224827, current_train_items 16288.
I0304 19:28:11.435300 23118544486528 run.py:483] Algo bellman_ford step 509 current loss 0.277585, current_train_items 16320.
I0304 19:28:11.454344 23118544486528 run.py:483] Algo bellman_ford step 510 current loss 0.034709, current_train_items 16352.
I0304 19:28:11.470815 23118544486528 run.py:483] Algo bellman_ford step 511 current loss 0.102104, current_train_items 16384.
I0304 19:28:11.494493 23118544486528 run.py:483] Algo bellman_ford step 512 current loss 0.215067, current_train_items 16416.
I0304 19:28:11.524676 23118544486528 run.py:483] Algo bellman_ford step 513 current loss 0.220459, current_train_items 16448.
I0304 19:28:11.557180 23118544486528 run.py:483] Algo bellman_ford step 514 current loss 0.207091, current_train_items 16480.
I0304 19:28:11.575751 23118544486528 run.py:483] Algo bellman_ford step 515 current loss 0.027480, current_train_items 16512.
I0304 19:28:11.591847 23118544486528 run.py:483] Algo bellman_ford step 516 current loss 0.132436, current_train_items 16544.
I0304 19:28:11.615890 23118544486528 run.py:483] Algo bellman_ford step 517 current loss 0.210059, current_train_items 16576.
I0304 19:28:11.647005 23118544486528 run.py:483] Algo bellman_ford step 518 current loss 0.219614, current_train_items 16608.
I0304 19:28:11.679474 23118544486528 run.py:483] Algo bellman_ford step 519 current loss 0.254866, current_train_items 16640.
I0304 19:28:11.698067 23118544486528 run.py:483] Algo bellman_ford step 520 current loss 0.040775, current_train_items 16672.
I0304 19:28:11.714610 23118544486528 run.py:483] Algo bellman_ford step 521 current loss 0.092149, current_train_items 16704.
I0304 19:28:11.738091 23118544486528 run.py:483] Algo bellman_ford step 522 current loss 0.136089, current_train_items 16736.
I0304 19:28:11.768936 23118544486528 run.py:483] Algo bellman_ford step 523 current loss 0.175321, current_train_items 16768.
I0304 19:28:11.800765 23118544486528 run.py:483] Algo bellman_ford step 524 current loss 0.268375, current_train_items 16800.
I0304 19:28:11.819350 23118544486528 run.py:483] Algo bellman_ford step 525 current loss 0.033171, current_train_items 16832.
I0304 19:28:11.835891 23118544486528 run.py:483] Algo bellman_ford step 526 current loss 0.084184, current_train_items 16864.
I0304 19:28:11.859740 23118544486528 run.py:483] Algo bellman_ford step 527 current loss 0.154093, current_train_items 16896.
I0304 19:28:11.888845 23118544486528 run.py:483] Algo bellman_ford step 528 current loss 0.202234, current_train_items 16928.
I0304 19:28:11.921468 23118544486528 run.py:483] Algo bellman_ford step 529 current loss 0.267431, current_train_items 16960.
I0304 19:28:11.940361 23118544486528 run.py:483] Algo bellman_ford step 530 current loss 0.047116, current_train_items 16992.
I0304 19:28:11.956723 23118544486528 run.py:483] Algo bellman_ford step 531 current loss 0.076042, current_train_items 17024.
I0304 19:28:11.980050 23118544486528 run.py:483] Algo bellman_ford step 532 current loss 0.120478, current_train_items 17056.
I0304 19:28:12.010363 23118544486528 run.py:483] Algo bellman_ford step 533 current loss 0.245738, current_train_items 17088.
I0304 19:28:12.042876 23118544486528 run.py:483] Algo bellman_ford step 534 current loss 0.225228, current_train_items 17120.
I0304 19:28:12.061574 23118544486528 run.py:483] Algo bellman_ford step 535 current loss 0.029985, current_train_items 17152.
I0304 19:28:12.077719 23118544486528 run.py:483] Algo bellman_ford step 536 current loss 0.057593, current_train_items 17184.
I0304 19:28:12.100487 23118544486528 run.py:483] Algo bellman_ford step 537 current loss 0.107854, current_train_items 17216.
I0304 19:28:12.129580 23118544486528 run.py:483] Algo bellman_ford step 538 current loss 0.137603, current_train_items 17248.
I0304 19:28:12.160996 23118544486528 run.py:483] Algo bellman_ford step 539 current loss 0.176130, current_train_items 17280.
I0304 19:28:12.179579 23118544486528 run.py:483] Algo bellman_ford step 540 current loss 0.049865, current_train_items 17312.
I0304 19:28:12.195460 23118544486528 run.py:483] Algo bellman_ford step 541 current loss 0.070357, current_train_items 17344.
I0304 19:28:12.219728 23118544486528 run.py:483] Algo bellman_ford step 542 current loss 0.169478, current_train_items 17376.
I0304 19:28:12.250347 23118544486528 run.py:483] Algo bellman_ford step 543 current loss 0.268753, current_train_items 17408.
I0304 19:28:12.282998 23118544486528 run.py:483] Algo bellman_ford step 544 current loss 0.348405, current_train_items 17440.
I0304 19:28:12.302221 23118544486528 run.py:483] Algo bellman_ford step 545 current loss 0.030139, current_train_items 17472.
I0304 19:28:12.318544 23118544486528 run.py:483] Algo bellman_ford step 546 current loss 0.075240, current_train_items 17504.
I0304 19:28:12.342275 23118544486528 run.py:483] Algo bellman_ford step 547 current loss 0.178049, current_train_items 17536.
I0304 19:28:12.372820 23118544486528 run.py:483] Algo bellman_ford step 548 current loss 0.387009, current_train_items 17568.
I0304 19:28:12.402739 23118544486528 run.py:483] Algo bellman_ford step 549 current loss 0.233556, current_train_items 17600.
I0304 19:28:12.421610 23118544486528 run.py:483] Algo bellman_ford step 550 current loss 0.028595, current_train_items 17632.
I0304 19:28:12.429833 23118544486528 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0304 19:28:12.429939 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:28:12.446694 23118544486528 run.py:483] Algo bellman_ford step 551 current loss 0.051024, current_train_items 17664.
I0304 19:28:12.470182 23118544486528 run.py:483] Algo bellman_ford step 552 current loss 0.199572, current_train_items 17696.
I0304 19:28:12.501840 23118544486528 run.py:483] Algo bellman_ford step 553 current loss 0.453038, current_train_items 17728.
I0304 19:28:12.533842 23118544486528 run.py:483] Algo bellman_ford step 554 current loss 0.356544, current_train_items 17760.
I0304 19:28:12.552715 23118544486528 run.py:483] Algo bellman_ford step 555 current loss 0.063432, current_train_items 17792.
I0304 19:28:12.568667 23118544486528 run.py:483] Algo bellman_ford step 556 current loss 0.114181, current_train_items 17824.
I0304 19:28:12.593759 23118544486528 run.py:483] Algo bellman_ford step 557 current loss 0.345126, current_train_items 17856.
I0304 19:28:12.623206 23118544486528 run.py:483] Algo bellman_ford step 558 current loss 0.290120, current_train_items 17888.
I0304 19:28:12.657111 23118544486528 run.py:483] Algo bellman_ford step 559 current loss 0.272753, current_train_items 17920.
I0304 19:28:12.676317 23118544486528 run.py:483] Algo bellman_ford step 560 current loss 0.050720, current_train_items 17952.
I0304 19:28:12.692836 23118544486528 run.py:483] Algo bellman_ford step 561 current loss 0.107940, current_train_items 17984.
I0304 19:28:12.716611 23118544486528 run.py:483] Algo bellman_ford step 562 current loss 0.244644, current_train_items 18016.
I0304 19:28:12.745880 23118544486528 run.py:483] Algo bellman_ford step 563 current loss 0.223162, current_train_items 18048.
I0304 19:28:12.776097 23118544486528 run.py:483] Algo bellman_ford step 564 current loss 0.271398, current_train_items 18080.
I0304 19:28:12.794757 23118544486528 run.py:483] Algo bellman_ford step 565 current loss 0.025938, current_train_items 18112.
I0304 19:28:12.811254 23118544486528 run.py:483] Algo bellman_ford step 566 current loss 0.081703, current_train_items 18144.
I0304 19:28:12.835703 23118544486528 run.py:483] Algo bellman_ford step 567 current loss 0.253696, current_train_items 18176.
I0304 19:28:12.866024 23118544486528 run.py:483] Algo bellman_ford step 568 current loss 0.384476, current_train_items 18208.
I0304 19:28:12.897376 23118544486528 run.py:483] Algo bellman_ford step 569 current loss 0.235541, current_train_items 18240.
I0304 19:28:12.916223 23118544486528 run.py:483] Algo bellman_ford step 570 current loss 0.114513, current_train_items 18272.
I0304 19:28:12.932420 23118544486528 run.py:483] Algo bellman_ford step 571 current loss 0.154414, current_train_items 18304.
I0304 19:28:12.956348 23118544486528 run.py:483] Algo bellman_ford step 572 current loss 0.245404, current_train_items 18336.
I0304 19:28:12.987396 23118544486528 run.py:483] Algo bellman_ford step 573 current loss 0.237915, current_train_items 18368.
I0304 19:28:13.018464 23118544486528 run.py:483] Algo bellman_ford step 574 current loss 0.193130, current_train_items 18400.
I0304 19:28:13.037644 23118544486528 run.py:483] Algo bellman_ford step 575 current loss 0.029955, current_train_items 18432.
I0304 19:28:13.053673 23118544486528 run.py:483] Algo bellman_ford step 576 current loss 0.063574, current_train_items 18464.
I0304 19:28:13.077390 23118544486528 run.py:483] Algo bellman_ford step 577 current loss 0.159412, current_train_items 18496.
I0304 19:28:13.107855 23118544486528 run.py:483] Algo bellman_ford step 578 current loss 0.212310, current_train_items 18528.
I0304 19:28:13.141069 23118544486528 run.py:483] Algo bellman_ford step 579 current loss 0.266453, current_train_items 18560.
I0304 19:28:13.159781 23118544486528 run.py:483] Algo bellman_ford step 580 current loss 0.044339, current_train_items 18592.
I0304 19:28:13.175963 23118544486528 run.py:483] Algo bellman_ford step 581 current loss 0.077230, current_train_items 18624.
I0304 19:28:13.198737 23118544486528 run.py:483] Algo bellman_ford step 582 current loss 0.126069, current_train_items 18656.
I0304 19:28:13.227419 23118544486528 run.py:483] Algo bellman_ford step 583 current loss 0.103885, current_train_items 18688.
I0304 19:28:13.259295 23118544486528 run.py:483] Algo bellman_ford step 584 current loss 0.194655, current_train_items 18720.
I0304 19:28:13.278182 23118544486528 run.py:483] Algo bellman_ford step 585 current loss 0.027264, current_train_items 18752.
I0304 19:28:13.294526 23118544486528 run.py:483] Algo bellman_ford step 586 current loss 0.065406, current_train_items 18784.
I0304 19:28:13.316678 23118544486528 run.py:483] Algo bellman_ford step 587 current loss 0.088016, current_train_items 18816.
I0304 19:28:13.345868 23118544486528 run.py:483] Algo bellman_ford step 588 current loss 0.225064, current_train_items 18848.
I0304 19:28:13.379354 23118544486528 run.py:483] Algo bellman_ford step 589 current loss 0.385122, current_train_items 18880.
I0304 19:28:13.398581 23118544486528 run.py:483] Algo bellman_ford step 590 current loss 0.077650, current_train_items 18912.
I0304 19:28:13.415052 23118544486528 run.py:483] Algo bellman_ford step 591 current loss 0.138555, current_train_items 18944.
I0304 19:28:13.438981 23118544486528 run.py:483] Algo bellman_ford step 592 current loss 0.152931, current_train_items 18976.
I0304 19:28:13.468824 23118544486528 run.py:483] Algo bellman_ford step 593 current loss 0.163523, current_train_items 19008.
I0304 19:28:13.500582 23118544486528 run.py:483] Algo bellman_ford step 594 current loss 0.250960, current_train_items 19040.
I0304 19:28:13.519287 23118544486528 run.py:483] Algo bellman_ford step 595 current loss 0.021828, current_train_items 19072.
I0304 19:28:13.535636 23118544486528 run.py:483] Algo bellman_ford step 596 current loss 0.075317, current_train_items 19104.
I0304 19:28:13.559905 23118544486528 run.py:483] Algo bellman_ford step 597 current loss 0.194451, current_train_items 19136.
I0304 19:28:13.589800 23118544486528 run.py:483] Algo bellman_ford step 598 current loss 0.162155, current_train_items 19168.
I0304 19:28:13.622196 23118544486528 run.py:483] Algo bellman_ford step 599 current loss 0.272528, current_train_items 19200.
I0304 19:28:13.641158 23118544486528 run.py:483] Algo bellman_ford step 600 current loss 0.052031, current_train_items 19232.
I0304 19:28:13.648983 23118544486528 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0304 19:28:13.649087 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0304 19:28:13.665745 23118544486528 run.py:483] Algo bellman_ford step 601 current loss 0.071180, current_train_items 19264.
I0304 19:28:13.689870 23118544486528 run.py:483] Algo bellman_ford step 602 current loss 0.155689, current_train_items 19296.
I0304 19:28:13.719739 23118544486528 run.py:483] Algo bellman_ford step 603 current loss 0.164171, current_train_items 19328.
I0304 19:28:13.750815 23118544486528 run.py:483] Algo bellman_ford step 604 current loss 0.182333, current_train_items 19360.
I0304 19:28:13.770008 23118544486528 run.py:483] Algo bellman_ford step 605 current loss 0.044580, current_train_items 19392.
I0304 19:28:13.786042 23118544486528 run.py:483] Algo bellman_ford step 606 current loss 0.059861, current_train_items 19424.
I0304 19:28:13.810791 23118544486528 run.py:483] Algo bellman_ford step 607 current loss 0.149262, current_train_items 19456.
I0304 19:28:13.839818 23118544486528 run.py:483] Algo bellman_ford step 608 current loss 0.183075, current_train_items 19488.
I0304 19:28:13.871940 23118544486528 run.py:483] Algo bellman_ford step 609 current loss 0.201985, current_train_items 19520.
I0304 19:28:13.890645 23118544486528 run.py:483] Algo bellman_ford step 610 current loss 0.035918, current_train_items 19552.
I0304 19:28:13.907285 23118544486528 run.py:483] Algo bellman_ford step 611 current loss 0.064641, current_train_items 19584.
I0304 19:28:13.931082 23118544486528 run.py:483] Algo bellman_ford step 612 current loss 0.157298, current_train_items 19616.
I0304 19:28:13.959324 23118544486528 run.py:483] Algo bellman_ford step 613 current loss 0.140116, current_train_items 19648.
I0304 19:28:13.991577 23118544486528 run.py:483] Algo bellman_ford step 614 current loss 0.186972, current_train_items 19680.
I0304 19:28:14.010063 23118544486528 run.py:483] Algo bellman_ford step 615 current loss 0.019497, current_train_items 19712.
I0304 19:28:14.026828 23118544486528 run.py:483] Algo bellman_ford step 616 current loss 0.064330, current_train_items 19744.
I0304 19:28:14.050191 23118544486528 run.py:483] Algo bellman_ford step 617 current loss 0.126571, current_train_items 19776.
I0304 19:28:14.079527 23118544486528 run.py:483] Algo bellman_ford step 618 current loss 0.143523, current_train_items 19808.
I0304 19:28:14.111964 23118544486528 run.py:483] Algo bellman_ford step 619 current loss 0.193914, current_train_items 19840.
I0304 19:28:14.131162 23118544486528 run.py:483] Algo bellman_ford step 620 current loss 0.061370, current_train_items 19872.
I0304 19:28:14.148013 23118544486528 run.py:483] Algo bellman_ford step 621 current loss 0.142668, current_train_items 19904.
I0304 19:28:14.172416 23118544486528 run.py:483] Algo bellman_ford step 622 current loss 0.157676, current_train_items 19936.
I0304 19:28:14.202232 23118544486528 run.py:483] Algo bellman_ford step 623 current loss 0.174855, current_train_items 19968.
I0304 19:28:14.233866 23118544486528 run.py:483] Algo bellman_ford step 624 current loss 0.249979, current_train_items 20000.
I0304 19:28:14.252555 23118544486528 run.py:483] Algo bellman_ford step 625 current loss 0.032358, current_train_items 20032.
I0304 19:28:14.268857 23118544486528 run.py:483] Algo bellman_ford step 626 current loss 0.073220, current_train_items 20064.
I0304 19:28:14.293282 23118544486528 run.py:483] Algo bellman_ford step 627 current loss 0.137331, current_train_items 20096.
I0304 19:28:14.324398 23118544486528 run.py:483] Algo bellman_ford step 628 current loss 0.184661, current_train_items 20128.
I0304 19:28:14.356931 23118544486528 run.py:483] Algo bellman_ford step 629 current loss 0.253411, current_train_items 20160.
I0304 19:28:14.375932 23118544486528 run.py:483] Algo bellman_ford step 630 current loss 0.021593, current_train_items 20192.
I0304 19:28:14.392126 23118544486528 run.py:483] Algo bellman_ford step 631 current loss 0.090568, current_train_items 20224.
I0304 19:28:14.415672 23118544486528 run.py:483] Algo bellman_ford step 632 current loss 0.219409, current_train_items 20256.
I0304 19:28:14.445711 23118544486528 run.py:483] Algo bellman_ford step 633 current loss 0.185879, current_train_items 20288.
I0304 19:28:14.477329 23118544486528 run.py:483] Algo bellman_ford step 634 current loss 0.196810, current_train_items 20320.
I0304 19:28:14.496090 23118544486528 run.py:483] Algo bellman_ford step 635 current loss 0.039571, current_train_items 20352.
I0304 19:28:14.512264 23118544486528 run.py:483] Algo bellman_ford step 636 current loss 0.066990, current_train_items 20384.
I0304 19:28:14.536154 23118544486528 run.py:483] Algo bellman_ford step 637 current loss 0.129888, current_train_items 20416.
I0304 19:28:14.566865 23118544486528 run.py:483] Algo bellman_ford step 638 current loss 0.212439, current_train_items 20448.
I0304 19:28:14.596316 23118544486528 run.py:483] Algo bellman_ford step 639 current loss 0.155797, current_train_items 20480.
I0304 19:28:14.615336 23118544486528 run.py:483] Algo bellman_ford step 640 current loss 0.019658, current_train_items 20512.
I0304 19:28:14.631509 23118544486528 run.py:483] Algo bellman_ford step 641 current loss 0.073054, current_train_items 20544.
I0304 19:28:14.654371 23118544486528 run.py:483] Algo bellman_ford step 642 current loss 0.191553, current_train_items 20576.
I0304 19:28:14.683637 23118544486528 run.py:483] Algo bellman_ford step 643 current loss 0.177229, current_train_items 20608.
I0304 19:28:14.716390 23118544486528 run.py:483] Algo bellman_ford step 644 current loss 0.344011, current_train_items 20640.
I0304 19:28:14.735566 23118544486528 run.py:483] Algo bellman_ford step 645 current loss 0.023165, current_train_items 20672.
I0304 19:28:14.752872 23118544486528 run.py:483] Algo bellman_ford step 646 current loss 0.088621, current_train_items 20704.
I0304 19:28:14.776264 23118544486528 run.py:483] Algo bellman_ford step 647 current loss 0.080209, current_train_items 20736.
I0304 19:28:14.807294 23118544486528 run.py:483] Algo bellman_ford step 648 current loss 0.197793, current_train_items 20768.
I0304 19:28:14.839911 23118544486528 run.py:483] Algo bellman_ford step 649 current loss 0.184636, current_train_items 20800.
I0304 19:28:14.858691 23118544486528 run.py:483] Algo bellman_ford step 650 current loss 0.096681, current_train_items 20832.
I0304 19:28:14.866615 23118544486528 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0304 19:28:14.866729 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0304 19:28:14.883918 23118544486528 run.py:483] Algo bellman_ford step 651 current loss 0.096121, current_train_items 20864.
I0304 19:28:14.907851 23118544486528 run.py:483] Algo bellman_ford step 652 current loss 0.147585, current_train_items 20896.
I0304 19:28:14.936837 23118544486528 run.py:483] Algo bellman_ford step 653 current loss 0.137254, current_train_items 20928.
I0304 19:28:14.968836 23118544486528 run.py:483] Algo bellman_ford step 654 current loss 0.169882, current_train_items 20960.
I0304 19:28:14.988121 23118544486528 run.py:483] Algo bellman_ford step 655 current loss 0.020744, current_train_items 20992.
I0304 19:28:15.004401 23118544486528 run.py:483] Algo bellman_ford step 656 current loss 0.065420, current_train_items 21024.
I0304 19:28:15.029027 23118544486528 run.py:483] Algo bellman_ford step 657 current loss 0.202898, current_train_items 21056.
I0304 19:28:15.059280 23118544486528 run.py:483] Algo bellman_ford step 658 current loss 0.167673, current_train_items 21088.
I0304 19:28:15.090713 23118544486528 run.py:483] Algo bellman_ford step 659 current loss 0.206484, current_train_items 21120.
I0304 19:28:15.110168 23118544486528 run.py:483] Algo bellman_ford step 660 current loss 0.030844, current_train_items 21152.
I0304 19:28:15.126593 23118544486528 run.py:483] Algo bellman_ford step 661 current loss 0.077289, current_train_items 21184.
I0304 19:28:15.149755 23118544486528 run.py:483] Algo bellman_ford step 662 current loss 0.096252, current_train_items 21216.
I0304 19:28:15.179817 23118544486528 run.py:483] Algo bellman_ford step 663 current loss 0.197674, current_train_items 21248.
I0304 19:28:15.214363 23118544486528 run.py:483] Algo bellman_ford step 664 current loss 0.252594, current_train_items 21280.
I0304 19:28:15.233429 23118544486528 run.py:483] Algo bellman_ford step 665 current loss 0.050051, current_train_items 21312.
I0304 19:28:15.249766 23118544486528 run.py:483] Algo bellman_ford step 666 current loss 0.096045, current_train_items 21344.
I0304 19:28:15.274500 23118544486528 run.py:483] Algo bellman_ford step 667 current loss 0.213141, current_train_items 21376.
I0304 19:28:15.304112 23118544486528 run.py:483] Algo bellman_ford step 668 current loss 0.235479, current_train_items 21408.
I0304 19:28:15.335608 23118544486528 run.py:483] Algo bellman_ford step 669 current loss 0.224061, current_train_items 21440.
I0304 19:28:15.355134 23118544486528 run.py:483] Algo bellman_ford step 670 current loss 0.022928, current_train_items 21472.
I0304 19:28:15.371328 23118544486528 run.py:483] Algo bellman_ford step 671 current loss 0.057121, current_train_items 21504.
I0304 19:28:15.395783 23118544486528 run.py:483] Algo bellman_ford step 672 current loss 0.099978, current_train_items 21536.
I0304 19:28:15.425215 23118544486528 run.py:483] Algo bellman_ford step 673 current loss 0.153721, current_train_items 21568.
I0304 19:28:15.458530 23118544486528 run.py:483] Algo bellman_ford step 674 current loss 0.211277, current_train_items 21600.
I0304 19:28:15.477836 23118544486528 run.py:483] Algo bellman_ford step 675 current loss 0.037092, current_train_items 21632.
I0304 19:28:15.494115 23118544486528 run.py:483] Algo bellman_ford step 676 current loss 0.078626, current_train_items 21664.
I0304 19:28:15.517610 23118544486528 run.py:483] Algo bellman_ford step 677 current loss 0.122687, current_train_items 21696.
I0304 19:28:15.548410 23118544486528 run.py:483] Algo bellman_ford step 678 current loss 0.179863, current_train_items 21728.
I0304 19:28:15.581423 23118544486528 run.py:483] Algo bellman_ford step 679 current loss 0.202935, current_train_items 21760.
I0304 19:28:15.600509 23118544486528 run.py:483] Algo bellman_ford step 680 current loss 0.048450, current_train_items 21792.
I0304 19:28:15.616748 23118544486528 run.py:483] Algo bellman_ford step 681 current loss 0.051920, current_train_items 21824.
I0304 19:28:15.640915 23118544486528 run.py:483] Algo bellman_ford step 682 current loss 0.154535, current_train_items 21856.
I0304 19:28:15.670181 23118544486528 run.py:483] Algo bellman_ford step 683 current loss 0.169403, current_train_items 21888.
I0304 19:28:15.703131 23118544486528 run.py:483] Algo bellman_ford step 684 current loss 0.196958, current_train_items 21920.
I0304 19:28:15.722216 23118544486528 run.py:483] Algo bellman_ford step 685 current loss 0.034997, current_train_items 21952.
I0304 19:28:15.738592 23118544486528 run.py:483] Algo bellman_ford step 686 current loss 0.056761, current_train_items 21984.
I0304 19:28:15.762580 23118544486528 run.py:483] Algo bellman_ford step 687 current loss 0.105176, current_train_items 22016.
I0304 19:28:15.791240 23118544486528 run.py:483] Algo bellman_ford step 688 current loss 0.110740, current_train_items 22048.
I0304 19:28:15.825248 23118544486528 run.py:483] Algo bellman_ford step 689 current loss 0.231727, current_train_items 22080.
I0304 19:28:15.844167 23118544486528 run.py:483] Algo bellman_ford step 690 current loss 0.032118, current_train_items 22112.
I0304 19:28:15.860983 23118544486528 run.py:483] Algo bellman_ford step 691 current loss 0.130655, current_train_items 22144.
I0304 19:28:15.885374 23118544486528 run.py:483] Algo bellman_ford step 692 current loss 0.138574, current_train_items 22176.
I0304 19:28:15.914494 23118544486528 run.py:483] Algo bellman_ford step 693 current loss 0.167608, current_train_items 22208.
I0304 19:28:15.948159 23118544486528 run.py:483] Algo bellman_ford step 694 current loss 0.292658, current_train_items 22240.
I0304 19:28:15.966918 23118544486528 run.py:483] Algo bellman_ford step 695 current loss 0.030728, current_train_items 22272.
I0304 19:28:15.982950 23118544486528 run.py:483] Algo bellman_ford step 696 current loss 0.052443, current_train_items 22304.
I0304 19:28:16.005407 23118544486528 run.py:483] Algo bellman_ford step 697 current loss 0.117920, current_train_items 22336.
I0304 19:28:16.035125 23118544486528 run.py:483] Algo bellman_ford step 698 current loss 0.186884, current_train_items 22368.
I0304 19:28:16.068658 23118544486528 run.py:483] Algo bellman_ford step 699 current loss 0.221944, current_train_items 22400.
I0304 19:28:16.087812 23118544486528 run.py:483] Algo bellman_ford step 700 current loss 0.018510, current_train_items 22432.
I0304 19:28:16.095557 23118544486528 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0304 19:28:16.095663 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0304 19:28:16.112131 23118544486528 run.py:483] Algo bellman_ford step 701 current loss 0.044601, current_train_items 22464.
I0304 19:28:16.136414 23118544486528 run.py:483] Algo bellman_ford step 702 current loss 0.136964, current_train_items 22496.
I0304 19:28:16.165274 23118544486528 run.py:483] Algo bellman_ford step 703 current loss 0.120839, current_train_items 22528.
I0304 19:28:16.197005 23118544486528 run.py:483] Algo bellman_ford step 704 current loss 0.200021, current_train_items 22560.
I0304 19:28:16.216289 23118544486528 run.py:483] Algo bellman_ford step 705 current loss 0.035921, current_train_items 22592.
I0304 19:28:16.232240 23118544486528 run.py:483] Algo bellman_ford step 706 current loss 0.049006, current_train_items 22624.
I0304 19:28:16.255861 23118544486528 run.py:483] Algo bellman_ford step 707 current loss 0.124902, current_train_items 22656.
I0304 19:28:16.285382 23118544486528 run.py:483] Algo bellman_ford step 708 current loss 0.180645, current_train_items 22688.
I0304 19:28:16.318205 23118544486528 run.py:483] Algo bellman_ford step 709 current loss 0.201655, current_train_items 22720.
I0304 19:28:16.337092 23118544486528 run.py:483] Algo bellman_ford step 710 current loss 0.014443, current_train_items 22752.
I0304 19:28:16.353419 23118544486528 run.py:483] Algo bellman_ford step 711 current loss 0.095160, current_train_items 22784.
I0304 19:28:16.377672 23118544486528 run.py:483] Algo bellman_ford step 712 current loss 0.314164, current_train_items 22816.
I0304 19:28:16.408575 23118544486528 run.py:483] Algo bellman_ford step 713 current loss 0.298256, current_train_items 22848.
I0304 19:28:16.439148 23118544486528 run.py:483] Algo bellman_ford step 714 current loss 0.169382, current_train_items 22880.
I0304 19:28:16.458242 23118544486528 run.py:483] Algo bellman_ford step 715 current loss 0.014804, current_train_items 22912.
I0304 19:28:16.474946 23118544486528 run.py:483] Algo bellman_ford step 716 current loss 0.215307, current_train_items 22944.
I0304 19:28:16.500042 23118544486528 run.py:483] Algo bellman_ford step 717 current loss 0.373908, current_train_items 22976.
I0304 19:28:16.529072 23118544486528 run.py:483] Algo bellman_ford step 718 current loss 0.164249, current_train_items 23008.
I0304 19:28:16.561428 23118544486528 run.py:483] Algo bellman_ford step 719 current loss 0.237850, current_train_items 23040.
I0304 19:28:16.580466 23118544486528 run.py:483] Algo bellman_ford step 720 current loss 0.048319, current_train_items 23072.
I0304 19:28:16.597581 23118544486528 run.py:483] Algo bellman_ford step 721 current loss 0.253849, current_train_items 23104.
I0304 19:28:16.621320 23118544486528 run.py:483] Algo bellman_ford step 722 current loss 0.337275, current_train_items 23136.
I0304 19:28:16.650666 23118544486528 run.py:483] Algo bellman_ford step 723 current loss 0.299462, current_train_items 23168.
I0304 19:28:16.682228 23118544486528 run.py:483] Algo bellman_ford step 724 current loss 0.316394, current_train_items 23200.
I0304 19:28:16.701204 23118544486528 run.py:483] Algo bellman_ford step 725 current loss 0.085688, current_train_items 23232.
I0304 19:28:16.717887 23118544486528 run.py:483] Algo bellman_ford step 726 current loss 0.186622, current_train_items 23264.
I0304 19:28:16.742449 23118544486528 run.py:483] Algo bellman_ford step 727 current loss 0.479407, current_train_items 23296.
I0304 19:28:16.771916 23118544486528 run.py:483] Algo bellman_ford step 728 current loss 0.325726, current_train_items 23328.
I0304 19:28:16.804077 23118544486528 run.py:483] Algo bellman_ford step 729 current loss 0.278645, current_train_items 23360.
I0304 19:28:16.822813 23118544486528 run.py:483] Algo bellman_ford step 730 current loss 0.024690, current_train_items 23392.
I0304 19:28:16.839199 23118544486528 run.py:483] Algo bellman_ford step 731 current loss 0.111295, current_train_items 23424.
I0304 19:28:16.863054 23118544486528 run.py:483] Algo bellman_ford step 732 current loss 0.355615, current_train_items 23456.
I0304 19:28:16.893356 23118544486528 run.py:483] Algo bellman_ford step 733 current loss 0.359654, current_train_items 23488.
I0304 19:28:16.926107 23118544486528 run.py:483] Algo bellman_ford step 734 current loss 0.260373, current_train_items 23520.
I0304 19:28:16.944840 23118544486528 run.py:483] Algo bellman_ford step 735 current loss 0.039243, current_train_items 23552.
I0304 19:28:16.961558 23118544486528 run.py:483] Algo bellman_ford step 736 current loss 0.065515, current_train_items 23584.
I0304 19:28:16.985114 23118544486528 run.py:483] Algo bellman_ford step 737 current loss 0.246953, current_train_items 23616.
I0304 19:28:17.014953 23118544486528 run.py:483] Algo bellman_ford step 738 current loss 0.448868, current_train_items 23648.
I0304 19:28:17.045914 23118544486528 run.py:483] Algo bellman_ford step 739 current loss 0.263614, current_train_items 23680.
I0304 19:28:17.064813 23118544486528 run.py:483] Algo bellman_ford step 740 current loss 0.022096, current_train_items 23712.
I0304 19:28:17.081294 23118544486528 run.py:483] Algo bellman_ford step 741 current loss 0.063208, current_train_items 23744.
I0304 19:28:17.104556 23118544486528 run.py:483] Algo bellman_ford step 742 current loss 0.169714, current_train_items 23776.
I0304 19:28:17.133766 23118544486528 run.py:483] Algo bellman_ford step 743 current loss 0.181032, current_train_items 23808.
I0304 19:28:17.167774 23118544486528 run.py:483] Algo bellman_ford step 744 current loss 0.238978, current_train_items 23840.
I0304 19:28:17.186867 23118544486528 run.py:483] Algo bellman_ford step 745 current loss 0.021965, current_train_items 23872.
I0304 19:28:17.203114 23118544486528 run.py:483] Algo bellman_ford step 746 current loss 0.133908, current_train_items 23904.
I0304 19:28:17.226877 23118544486528 run.py:483] Algo bellman_ford step 747 current loss 0.171747, current_train_items 23936.
I0304 19:28:17.257368 23118544486528 run.py:483] Algo bellman_ford step 748 current loss 0.243478, current_train_items 23968.
I0304 19:28:17.289531 23118544486528 run.py:483] Algo bellman_ford step 749 current loss 0.201546, current_train_items 24000.
I0304 19:28:17.308049 23118544486528 run.py:483] Algo bellman_ford step 750 current loss 0.020642, current_train_items 24032.
I0304 19:28:17.315931 23118544486528 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0304 19:28:17.316038 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0304 19:28:17.333165 23118544486528 run.py:483] Algo bellman_ford step 751 current loss 0.245476, current_train_items 24064.
I0304 19:28:17.356534 23118544486528 run.py:483] Algo bellman_ford step 752 current loss 0.252292, current_train_items 24096.
I0304 19:28:17.385843 23118544486528 run.py:483] Algo bellman_ford step 753 current loss 0.260232, current_train_items 24128.
I0304 19:28:17.418655 23118544486528 run.py:483] Algo bellman_ford step 754 current loss 0.211000, current_train_items 24160.
I0304 19:28:17.437610 23118544486528 run.py:483] Algo bellman_ford step 755 current loss 0.050477, current_train_items 24192.
I0304 19:28:17.453603 23118544486528 run.py:483] Algo bellman_ford step 756 current loss 0.146222, current_train_items 24224.
I0304 19:28:17.478364 23118544486528 run.py:483] Algo bellman_ford step 757 current loss 0.285046, current_train_items 24256.
I0304 19:28:17.507874 23118544486528 run.py:483] Algo bellman_ford step 758 current loss 0.474137, current_train_items 24288.
I0304 19:28:17.539356 23118544486528 run.py:483] Algo bellman_ford step 759 current loss 0.327178, current_train_items 24320.
I0304 19:28:17.558175 23118544486528 run.py:483] Algo bellman_ford step 760 current loss 0.021370, current_train_items 24352.
I0304 19:28:17.574447 23118544486528 run.py:483] Algo bellman_ford step 761 current loss 0.104827, current_train_items 24384.
I0304 19:28:17.598271 23118544486528 run.py:483] Algo bellman_ford step 762 current loss 0.204229, current_train_items 24416.
I0304 19:28:17.627051 23118544486528 run.py:483] Algo bellman_ford step 763 current loss 0.156043, current_train_items 24448.
I0304 19:28:17.659523 23118544486528 run.py:483] Algo bellman_ford step 764 current loss 0.197179, current_train_items 24480.
I0304 19:28:17.678328 23118544486528 run.py:483] Algo bellman_ford step 765 current loss 0.032739, current_train_items 24512.
I0304 19:28:17.695073 23118544486528 run.py:483] Algo bellman_ford step 766 current loss 0.119159, current_train_items 24544.
I0304 19:28:17.719074 23118544486528 run.py:483] Algo bellman_ford step 767 current loss 0.151605, current_train_items 24576.
I0304 19:28:17.748838 23118544486528 run.py:483] Algo bellman_ford step 768 current loss 0.146030, current_train_items 24608.
I0304 19:28:17.781592 23118544486528 run.py:483] Algo bellman_ford step 769 current loss 0.215672, current_train_items 24640.
I0304 19:28:17.800575 23118544486528 run.py:483] Algo bellman_ford step 770 current loss 0.033461, current_train_items 24672.
I0304 19:28:17.817165 23118544486528 run.py:483] Algo bellman_ford step 771 current loss 0.125785, current_train_items 24704.
I0304 19:28:17.840434 23118544486528 run.py:483] Algo bellman_ford step 772 current loss 0.225510, current_train_items 24736.
I0304 19:28:17.869840 23118544486528 run.py:483] Algo bellman_ford step 773 current loss 0.145718, current_train_items 24768.
I0304 19:28:17.902779 23118544486528 run.py:483] Algo bellman_ford step 774 current loss 0.265712, current_train_items 24800.
I0304 19:28:17.922044 23118544486528 run.py:483] Algo bellman_ford step 775 current loss 0.028898, current_train_items 24832.
I0304 19:28:17.938902 23118544486528 run.py:483] Algo bellman_ford step 776 current loss 0.062840, current_train_items 24864.
I0304 19:28:17.962572 23118544486528 run.py:483] Algo bellman_ford step 777 current loss 0.160684, current_train_items 24896.
I0304 19:28:17.991209 23118544486528 run.py:483] Algo bellman_ford step 778 current loss 0.273470, current_train_items 24928.
I0304 19:28:18.023092 23118544486528 run.py:483] Algo bellman_ford step 779 current loss 0.230583, current_train_items 24960.
I0304 19:28:18.042048 23118544486528 run.py:483] Algo bellman_ford step 780 current loss 0.024732, current_train_items 24992.
I0304 19:28:18.058544 23118544486528 run.py:483] Algo bellman_ford step 781 current loss 0.074268, current_train_items 25024.
I0304 19:28:18.081655 23118544486528 run.py:483] Algo bellman_ford step 782 current loss 0.119538, current_train_items 25056.
I0304 19:28:18.111252 23118544486528 run.py:483] Algo bellman_ford step 783 current loss 0.189771, current_train_items 25088.
I0304 19:28:18.144558 23118544486528 run.py:483] Algo bellman_ford step 784 current loss 0.203575, current_train_items 25120.
I0304 19:28:18.163279 23118544486528 run.py:483] Algo bellman_ford step 785 current loss 0.021366, current_train_items 25152.
I0304 19:28:18.179947 23118544486528 run.py:483] Algo bellman_ford step 786 current loss 0.088050, current_train_items 25184.
I0304 19:28:18.203159 23118544486528 run.py:483] Algo bellman_ford step 787 current loss 0.134393, current_train_items 25216.
I0304 19:28:18.233057 23118544486528 run.py:483] Algo bellman_ford step 788 current loss 0.164426, current_train_items 25248.
I0304 19:28:18.263326 23118544486528 run.py:483] Algo bellman_ford step 789 current loss 0.248083, current_train_items 25280.
I0304 19:28:18.282509 23118544486528 run.py:483] Algo bellman_ford step 790 current loss 0.023782, current_train_items 25312.
I0304 19:28:18.299497 23118544486528 run.py:483] Algo bellman_ford step 791 current loss 0.092737, current_train_items 25344.
I0304 19:28:18.321590 23118544486528 run.py:483] Algo bellman_ford step 792 current loss 0.119155, current_train_items 25376.
I0304 19:28:18.351091 23118544486528 run.py:483] Algo bellman_ford step 793 current loss 0.166101, current_train_items 25408.
I0304 19:28:18.381802 23118544486528 run.py:483] Algo bellman_ford step 794 current loss 0.177766, current_train_items 25440.
I0304 19:28:18.400616 23118544486528 run.py:483] Algo bellman_ford step 795 current loss 0.031851, current_train_items 25472.
I0304 19:28:18.416632 23118544486528 run.py:483] Algo bellman_ford step 796 current loss 0.077906, current_train_items 25504.
I0304 19:28:18.440567 23118544486528 run.py:483] Algo bellman_ford step 797 current loss 0.208428, current_train_items 25536.
I0304 19:28:18.469239 23118544486528 run.py:483] Algo bellman_ford step 798 current loss 0.228786, current_train_items 25568.
I0304 19:28:18.500773 23118544486528 run.py:483] Algo bellman_ford step 799 current loss 0.183740, current_train_items 25600.
I0304 19:28:18.519621 23118544486528 run.py:483] Algo bellman_ford step 800 current loss 0.016001, current_train_items 25632.
I0304 19:28:18.527495 23118544486528 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0304 19:28:18.527602 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0304 19:28:18.544500 23118544486528 run.py:483] Algo bellman_ford step 801 current loss 0.096317, current_train_items 25664.
I0304 19:28:18.569292 23118544486528 run.py:483] Algo bellman_ford step 802 current loss 0.322607, current_train_items 25696.
I0304 19:28:18.599642 23118544486528 run.py:483] Algo bellman_ford step 803 current loss 0.259560, current_train_items 25728.
I0304 19:28:18.631487 23118544486528 run.py:483] Algo bellman_ford step 804 current loss 0.241175, current_train_items 25760.
I0304 19:28:18.650900 23118544486528 run.py:483] Algo bellman_ford step 805 current loss 0.026712, current_train_items 25792.
I0304 19:28:18.666836 23118544486528 run.py:483] Algo bellman_ford step 806 current loss 0.116699, current_train_items 25824.
I0304 19:28:18.691144 23118544486528 run.py:483] Algo bellman_ford step 807 current loss 0.172151, current_train_items 25856.
I0304 19:28:18.720625 23118544486528 run.py:483] Algo bellman_ford step 808 current loss 0.163021, current_train_items 25888.
I0304 19:28:18.751491 23118544486528 run.py:483] Algo bellman_ford step 809 current loss 0.198124, current_train_items 25920.
I0304 19:28:18.770439 23118544486528 run.py:483] Algo bellman_ford step 810 current loss 0.040747, current_train_items 25952.
I0304 19:28:18.787388 23118544486528 run.py:483] Algo bellman_ford step 811 current loss 0.126051, current_train_items 25984.
I0304 19:28:18.810514 23118544486528 run.py:483] Algo bellman_ford step 812 current loss 0.139299, current_train_items 26016.
I0304 19:28:18.839443 23118544486528 run.py:483] Algo bellman_ford step 813 current loss 0.123223, current_train_items 26048.
I0304 19:28:18.871288 23118544486528 run.py:483] Algo bellman_ford step 814 current loss 0.188217, current_train_items 26080.
I0304 19:28:18.890237 23118544486528 run.py:483] Algo bellman_ford step 815 current loss 0.023414, current_train_items 26112.
I0304 19:28:18.906897 23118544486528 run.py:483] Algo bellman_ford step 816 current loss 0.082622, current_train_items 26144.
I0304 19:28:18.930917 23118544486528 run.py:483] Algo bellman_ford step 817 current loss 0.074315, current_train_items 26176.
I0304 19:28:18.961319 23118544486528 run.py:483] Algo bellman_ford step 818 current loss 0.212432, current_train_items 26208.
I0304 19:28:18.995114 23118544486528 run.py:483] Algo bellman_ford step 819 current loss 0.225230, current_train_items 26240.
I0304 19:28:19.013963 23118544486528 run.py:483] Algo bellman_ford step 820 current loss 0.019562, current_train_items 26272.
I0304 19:28:19.030040 23118544486528 run.py:483] Algo bellman_ford step 821 current loss 0.052998, current_train_items 26304.
I0304 19:28:19.053168 23118544486528 run.py:483] Algo bellman_ford step 822 current loss 0.099896, current_train_items 26336.
I0304 19:28:19.082770 23118544486528 run.py:483] Algo bellman_ford step 823 current loss 0.191450, current_train_items 26368.
I0304 19:28:19.114277 23118544486528 run.py:483] Algo bellman_ford step 824 current loss 0.281979, current_train_items 26400.
I0304 19:28:19.133266 23118544486528 run.py:483] Algo bellman_ford step 825 current loss 0.044441, current_train_items 26432.
I0304 19:28:19.149971 23118544486528 run.py:483] Algo bellman_ford step 826 current loss 0.081073, current_train_items 26464.
I0304 19:28:19.174182 23118544486528 run.py:483] Algo bellman_ford step 827 current loss 0.139015, current_train_items 26496.
I0304 19:28:19.204029 23118544486528 run.py:483] Algo bellman_ford step 828 current loss 0.192377, current_train_items 26528.
I0304 19:28:19.235843 23118544486528 run.py:483] Algo bellman_ford step 829 current loss 0.154541, current_train_items 26560.
I0304 19:28:19.254385 23118544486528 run.py:483] Algo bellman_ford step 830 current loss 0.015001, current_train_items 26592.
I0304 19:28:19.270922 23118544486528 run.py:483] Algo bellman_ford step 831 current loss 0.069753, current_train_items 26624.
I0304 19:28:19.295085 23118544486528 run.py:483] Algo bellman_ford step 832 current loss 0.169216, current_train_items 26656.
I0304 19:28:19.324660 23118544486528 run.py:483] Algo bellman_ford step 833 current loss 0.162787, current_train_items 26688.
I0304 19:28:19.355431 23118544486528 run.py:483] Algo bellman_ford step 834 current loss 0.144905, current_train_items 26720.
I0304 19:28:19.374454 23118544486528 run.py:483] Algo bellman_ford step 835 current loss 0.018899, current_train_items 26752.
I0304 19:28:19.391008 23118544486528 run.py:483] Algo bellman_ford step 836 current loss 0.060903, current_train_items 26784.
I0304 19:28:19.414887 23118544486528 run.py:483] Algo bellman_ford step 837 current loss 0.153491, current_train_items 26816.
I0304 19:28:19.444514 23118544486528 run.py:483] Algo bellman_ford step 838 current loss 0.108264, current_train_items 26848.
I0304 19:28:19.477937 23118544486528 run.py:483] Algo bellman_ford step 839 current loss 0.221919, current_train_items 26880.
I0304 19:28:19.496294 23118544486528 run.py:483] Algo bellman_ford step 840 current loss 0.043489, current_train_items 26912.
I0304 19:28:19.512705 23118544486528 run.py:483] Algo bellman_ford step 841 current loss 0.059973, current_train_items 26944.
I0304 19:28:19.536632 23118544486528 run.py:483] Algo bellman_ford step 842 current loss 0.096522, current_train_items 26976.
I0304 19:28:19.565211 23118544486528 run.py:483] Algo bellman_ford step 843 current loss 0.142229, current_train_items 27008.
I0304 19:28:19.596242 23118544486528 run.py:483] Algo bellman_ford step 844 current loss 0.141271, current_train_items 27040.
I0304 19:28:19.614757 23118544486528 run.py:483] Algo bellman_ford step 845 current loss 0.038899, current_train_items 27072.
I0304 19:28:19.631340 23118544486528 run.py:483] Algo bellman_ford step 846 current loss 0.051608, current_train_items 27104.
I0304 19:28:19.656421 23118544486528 run.py:483] Algo bellman_ford step 847 current loss 0.199827, current_train_items 27136.
I0304 19:28:19.686184 23118544486528 run.py:483] Algo bellman_ford step 848 current loss 0.239501, current_train_items 27168.
I0304 19:28:19.718869 23118544486528 run.py:483] Algo bellman_ford step 849 current loss 0.239279, current_train_items 27200.
I0304 19:28:19.737699 23118544486528 run.py:483] Algo bellman_ford step 850 current loss 0.019170, current_train_items 27232.
I0304 19:28:19.745958 23118544486528 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.9501953125, 'score': 0.9501953125, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0304 19:28:19.746066 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.967, current avg val score is 0.950, val scores are: bellman_ford: 0.950
I0304 19:28:19.762547 23118544486528 run.py:483] Algo bellman_ford step 851 current loss 0.052869, current_train_items 27264.
I0304 19:28:19.786728 23118544486528 run.py:483] Algo bellman_ford step 852 current loss 0.195539, current_train_items 27296.
I0304 19:28:19.818287 23118544486528 run.py:483] Algo bellman_ford step 853 current loss 0.244653, current_train_items 27328.
I0304 19:28:19.850020 23118544486528 run.py:483] Algo bellman_ford step 854 current loss 0.261079, current_train_items 27360.
I0304 19:28:19.868987 23118544486528 run.py:483] Algo bellman_ford step 855 current loss 0.053516, current_train_items 27392.
I0304 19:28:19.885616 23118544486528 run.py:483] Algo bellman_ford step 856 current loss 0.074236, current_train_items 27424.
I0304 19:28:19.908910 23118544486528 run.py:483] Algo bellman_ford step 857 current loss 0.199438, current_train_items 27456.
I0304 19:28:19.939855 23118544486528 run.py:483] Algo bellman_ford step 858 current loss 0.305383, current_train_items 27488.
I0304 19:28:19.971266 23118544486528 run.py:483] Algo bellman_ford step 859 current loss 0.243200, current_train_items 27520.
I0304 19:28:19.990267 23118544486528 run.py:483] Algo bellman_ford step 860 current loss 0.015674, current_train_items 27552.
I0304 19:28:20.006986 23118544486528 run.py:483] Algo bellman_ford step 861 current loss 0.090710, current_train_items 27584.
I0304 19:28:20.030342 23118544486528 run.py:483] Algo bellman_ford step 862 current loss 0.177987, current_train_items 27616.
I0304 19:28:20.059914 23118544486528 run.py:483] Algo bellman_ford step 863 current loss 0.260114, current_train_items 27648.
I0304 19:28:20.091220 23118544486528 run.py:483] Algo bellman_ford step 864 current loss 0.243692, current_train_items 27680.
I0304 19:28:20.109570 23118544486528 run.py:483] Algo bellman_ford step 865 current loss 0.015273, current_train_items 27712.
I0304 19:28:20.125627 23118544486528 run.py:483] Algo bellman_ford step 866 current loss 0.086274, current_train_items 27744.
I0304 19:28:20.150416 23118544486528 run.py:483] Algo bellman_ford step 867 current loss 0.229013, current_train_items 27776.
I0304 19:28:20.180243 23118544486528 run.py:483] Algo bellman_ford step 868 current loss 0.157219, current_train_items 27808.
I0304 19:28:20.213536 23118544486528 run.py:483] Algo bellman_ford step 869 current loss 0.179048, current_train_items 27840.
I0304 19:28:20.232645 23118544486528 run.py:483] Algo bellman_ford step 870 current loss 0.097479, current_train_items 27872.
I0304 19:28:20.249369 23118544486528 run.py:483] Algo bellman_ford step 871 current loss 0.079086, current_train_items 27904.
I0304 19:28:20.271903 23118544486528 run.py:483] Algo bellman_ford step 872 current loss 0.152307, current_train_items 27936.
I0304 19:28:20.302379 23118544486528 run.py:483] Algo bellman_ford step 873 current loss 0.209774, current_train_items 27968.
I0304 19:28:20.333869 23118544486528 run.py:483] Algo bellman_ford step 874 current loss 0.204443, current_train_items 28000.
I0304 19:28:20.352818 23118544486528 run.py:483] Algo bellman_ford step 875 current loss 0.015241, current_train_items 28032.
I0304 19:28:20.369518 23118544486528 run.py:483] Algo bellman_ford step 876 current loss 0.120106, current_train_items 28064.
I0304 19:28:20.394916 23118544486528 run.py:483] Algo bellman_ford step 877 current loss 0.246239, current_train_items 28096.
I0304 19:28:20.423655 23118544486528 run.py:483] Algo bellman_ford step 878 current loss 0.142658, current_train_items 28128.
I0304 19:28:20.456543 23118544486528 run.py:483] Algo bellman_ford step 879 current loss 0.196085, current_train_items 28160.
I0304 19:28:20.475488 23118544486528 run.py:483] Algo bellman_ford step 880 current loss 0.014704, current_train_items 28192.
I0304 19:28:20.491657 23118544486528 run.py:483] Algo bellman_ford step 881 current loss 0.027576, current_train_items 28224.
I0304 19:28:20.514679 23118544486528 run.py:483] Algo bellman_ford step 882 current loss 0.173011, current_train_items 28256.
I0304 19:28:20.544567 23118544486528 run.py:483] Algo bellman_ford step 883 current loss 0.195254, current_train_items 28288.
I0304 19:28:20.577833 23118544486528 run.py:483] Algo bellman_ford step 884 current loss 0.178224, current_train_items 28320.
I0304 19:28:20.596982 23118544486528 run.py:483] Algo bellman_ford step 885 current loss 0.028009, current_train_items 28352.
I0304 19:28:20.613638 23118544486528 run.py:483] Algo bellman_ford step 886 current loss 0.136133, current_train_items 28384.
I0304 19:28:20.637420 23118544486528 run.py:483] Algo bellman_ford step 887 current loss 0.192592, current_train_items 28416.
I0304 19:28:20.667130 23118544486528 run.py:483] Algo bellman_ford step 888 current loss 0.191944, current_train_items 28448.
I0304 19:28:20.700352 23118544486528 run.py:483] Algo bellman_ford step 889 current loss 0.230798, current_train_items 28480.
I0304 19:28:20.719502 23118544486528 run.py:483] Algo bellman_ford step 890 current loss 0.036894, current_train_items 28512.
I0304 19:28:20.735986 23118544486528 run.py:483] Algo bellman_ford step 891 current loss 0.084216, current_train_items 28544.
I0304 19:28:20.759783 23118544486528 run.py:483] Algo bellman_ford step 892 current loss 0.337711, current_train_items 28576.
I0304 19:28:20.790587 23118544486528 run.py:483] Algo bellman_ford step 893 current loss 0.369990, current_train_items 28608.
I0304 19:28:20.822185 23118544486528 run.py:483] Algo bellman_ford step 894 current loss 0.247819, current_train_items 28640.
I0304 19:28:20.840927 23118544486528 run.py:483] Algo bellman_ford step 895 current loss 0.026680, current_train_items 28672.
I0304 19:28:20.857097 23118544486528 run.py:483] Algo bellman_ford step 896 current loss 0.060872, current_train_items 28704.
I0304 19:28:20.881426 23118544486528 run.py:483] Algo bellman_ford step 897 current loss 0.228701, current_train_items 28736.
I0304 19:28:20.912257 23118544486528 run.py:483] Algo bellman_ford step 898 current loss 0.262826, current_train_items 28768.
I0304 19:28:20.946276 23118544486528 run.py:483] Algo bellman_ford step 899 current loss 0.261227, current_train_items 28800.
I0304 19:28:20.965224 23118544486528 run.py:483] Algo bellman_ford step 900 current loss 0.017334, current_train_items 28832.
I0304 19:28:20.972951 23118544486528 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0304 19:28:20.973059 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.967, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:28:21.003582 23118544486528 run.py:483] Algo bellman_ford step 901 current loss 0.077723, current_train_items 28864.
I0304 19:28:21.028149 23118544486528 run.py:483] Algo bellman_ford step 902 current loss 0.155051, current_train_items 28896.
I0304 19:28:21.059204 23118544486528 run.py:483] Algo bellman_ford step 903 current loss 0.174079, current_train_items 28928.
I0304 19:28:21.091696 23118544486528 run.py:483] Algo bellman_ford step 904 current loss 0.231483, current_train_items 28960.
I0304 19:28:21.111040 23118544486528 run.py:483] Algo bellman_ford step 905 current loss 0.015838, current_train_items 28992.
I0304 19:28:21.127544 23118544486528 run.py:483] Algo bellman_ford step 906 current loss 0.124226, current_train_items 29024.
I0304 19:28:21.151700 23118544486528 run.py:483] Algo bellman_ford step 907 current loss 0.144611, current_train_items 29056.
I0304 19:28:21.182454 23118544486528 run.py:483] Algo bellman_ford step 908 current loss 0.300189, current_train_items 29088.
I0304 19:28:21.215337 23118544486528 run.py:483] Algo bellman_ford step 909 current loss 0.176875, current_train_items 29120.
I0304 19:28:21.234155 23118544486528 run.py:483] Algo bellman_ford step 910 current loss 0.027580, current_train_items 29152.
I0304 19:28:21.251415 23118544486528 run.py:483] Algo bellman_ford step 911 current loss 0.129167, current_train_items 29184.
I0304 19:28:21.275784 23118544486528 run.py:483] Algo bellman_ford step 912 current loss 0.308048, current_train_items 29216.
I0304 19:28:21.304074 23118544486528 run.py:483] Algo bellman_ford step 913 current loss 0.258674, current_train_items 29248.
I0304 19:28:21.337170 23118544486528 run.py:483] Algo bellman_ford step 914 current loss 0.396703, current_train_items 29280.
I0304 19:28:21.355669 23118544486528 run.py:483] Algo bellman_ford step 915 current loss 0.010444, current_train_items 29312.
I0304 19:28:21.372333 23118544486528 run.py:483] Algo bellman_ford step 916 current loss 0.065923, current_train_items 29344.
I0304 19:28:21.396499 23118544486528 run.py:483] Algo bellman_ford step 917 current loss 0.162049, current_train_items 29376.
I0304 19:28:21.426548 23118544486528 run.py:483] Algo bellman_ford step 918 current loss 0.184922, current_train_items 29408.
I0304 19:28:21.459862 23118544486528 run.py:483] Algo bellman_ford step 919 current loss 0.194262, current_train_items 29440.
I0304 19:28:21.478766 23118544486528 run.py:483] Algo bellman_ford step 920 current loss 0.034079, current_train_items 29472.
I0304 19:28:21.495618 23118544486528 run.py:483] Algo bellman_ford step 921 current loss 0.059225, current_train_items 29504.
I0304 19:28:21.519976 23118544486528 run.py:483] Algo bellman_ford step 922 current loss 0.219473, current_train_items 29536.
I0304 19:28:21.549564 23118544486528 run.py:483] Algo bellman_ford step 923 current loss 0.243475, current_train_items 29568.
I0304 19:28:21.581602 23118544486528 run.py:483] Algo bellman_ford step 924 current loss 0.276627, current_train_items 29600.
I0304 19:28:21.600304 23118544486528 run.py:483] Algo bellman_ford step 925 current loss 0.027980, current_train_items 29632.
I0304 19:28:21.616720 23118544486528 run.py:483] Algo bellman_ford step 926 current loss 0.080785, current_train_items 29664.
I0304 19:28:21.640790 23118544486528 run.py:483] Algo bellman_ford step 927 current loss 0.186768, current_train_items 29696.
I0304 19:28:21.671628 23118544486528 run.py:483] Algo bellman_ford step 928 current loss 0.263137, current_train_items 29728.
I0304 19:28:21.702472 23118544486528 run.py:483] Algo bellman_ford step 929 current loss 0.189725, current_train_items 29760.
I0304 19:28:21.721397 23118544486528 run.py:483] Algo bellman_ford step 930 current loss 0.038562, current_train_items 29792.
I0304 19:28:21.737873 23118544486528 run.py:483] Algo bellman_ford step 931 current loss 0.055882, current_train_items 29824.
I0304 19:28:21.761709 23118544486528 run.py:483] Algo bellman_ford step 932 current loss 0.165407, current_train_items 29856.
I0304 19:28:21.790975 23118544486528 run.py:483] Algo bellman_ford step 933 current loss 0.144721, current_train_items 29888.
I0304 19:28:21.823916 23118544486528 run.py:483] Algo bellman_ford step 934 current loss 0.198471, current_train_items 29920.
I0304 19:28:21.842598 23118544486528 run.py:483] Algo bellman_ford step 935 current loss 0.028625, current_train_items 29952.
I0304 19:28:21.859281 23118544486528 run.py:483] Algo bellman_ford step 936 current loss 0.060271, current_train_items 29984.
I0304 19:28:21.882239 23118544486528 run.py:483] Algo bellman_ford step 937 current loss 0.084585, current_train_items 30016.
I0304 19:28:21.911525 23118544486528 run.py:483] Algo bellman_ford step 938 current loss 0.152210, current_train_items 30048.
I0304 19:28:21.943884 23118544486528 run.py:483] Algo bellman_ford step 939 current loss 0.233285, current_train_items 30080.
I0304 19:28:21.962475 23118544486528 run.py:483] Algo bellman_ford step 940 current loss 0.021097, current_train_items 30112.
I0304 19:28:21.979153 23118544486528 run.py:483] Algo bellman_ford step 941 current loss 0.102481, current_train_items 30144.
I0304 19:28:22.003345 23118544486528 run.py:483] Algo bellman_ford step 942 current loss 0.116208, current_train_items 30176.
I0304 19:28:22.033691 23118544486528 run.py:483] Algo bellman_ford step 943 current loss 0.100076, current_train_items 30208.
I0304 19:28:22.066737 23118544486528 run.py:483] Algo bellman_ford step 944 current loss 0.218369, current_train_items 30240.
I0304 19:28:22.085317 23118544486528 run.py:483] Algo bellman_ford step 945 current loss 0.019313, current_train_items 30272.
I0304 19:28:22.101328 23118544486528 run.py:483] Algo bellman_ford step 946 current loss 0.120019, current_train_items 30304.
I0304 19:28:22.126520 23118544486528 run.py:483] Algo bellman_ford step 947 current loss 0.216770, current_train_items 30336.
I0304 19:28:22.155731 23118544486528 run.py:483] Algo bellman_ford step 948 current loss 0.167436, current_train_items 30368.
I0304 19:28:22.187583 23118544486528 run.py:483] Algo bellman_ford step 949 current loss 0.196552, current_train_items 30400.
I0304 19:28:22.206368 23118544486528 run.py:483] Algo bellman_ford step 950 current loss 0.015119, current_train_items 30432.
I0304 19:28:22.214277 23118544486528 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9521484375, 'score': 0.9521484375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0304 19:28:22.214381 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.969, current avg val score is 0.952, val scores are: bellman_ford: 0.952
I0304 19:28:22.231456 23118544486528 run.py:483] Algo bellman_ford step 951 current loss 0.264972, current_train_items 30464.
I0304 19:28:22.255535 23118544486528 run.py:483] Algo bellman_ford step 952 current loss 0.222900, current_train_items 30496.
I0304 19:28:22.285539 23118544486528 run.py:483] Algo bellman_ford step 953 current loss 0.190632, current_train_items 30528.
I0304 19:28:22.315859 23118544486528 run.py:483] Algo bellman_ford step 954 current loss 0.147493, current_train_items 30560.
I0304 19:28:22.335198 23118544486528 run.py:483] Algo bellman_ford step 955 current loss 0.035136, current_train_items 30592.
I0304 19:28:22.351351 23118544486528 run.py:483] Algo bellman_ford step 956 current loss 0.076416, current_train_items 30624.
I0304 19:28:22.375490 23118544486528 run.py:483] Algo bellman_ford step 957 current loss 0.162572, current_train_items 30656.
I0304 19:28:22.404026 23118544486528 run.py:483] Algo bellman_ford step 958 current loss 0.187035, current_train_items 30688.
I0304 19:28:22.436818 23118544486528 run.py:483] Algo bellman_ford step 959 current loss 0.212153, current_train_items 30720.
I0304 19:28:22.455970 23118544486528 run.py:483] Algo bellman_ford step 960 current loss 0.037516, current_train_items 30752.
I0304 19:28:22.473030 23118544486528 run.py:483] Algo bellman_ford step 961 current loss 0.116750, current_train_items 30784.
I0304 19:28:22.495643 23118544486528 run.py:483] Algo bellman_ford step 962 current loss 0.066932, current_train_items 30816.
I0304 19:28:22.523971 23118544486528 run.py:483] Algo bellman_ford step 963 current loss 0.120652, current_train_items 30848.
I0304 19:28:22.556672 23118544486528 run.py:483] Algo bellman_ford step 964 current loss 0.171052, current_train_items 30880.
I0304 19:28:22.575612 23118544486528 run.py:483] Algo bellman_ford step 965 current loss 0.034030, current_train_items 30912.
I0304 19:28:22.591910 23118544486528 run.py:483] Algo bellman_ford step 966 current loss 0.093050, current_train_items 30944.
I0304 19:28:22.616288 23118544486528 run.py:483] Algo bellman_ford step 967 current loss 0.199249, current_train_items 30976.
I0304 19:28:22.646924 23118544486528 run.py:483] Algo bellman_ford step 968 current loss 0.177800, current_train_items 31008.
I0304 19:28:22.678973 23118544486528 run.py:483] Algo bellman_ford step 969 current loss 0.198464, current_train_items 31040.
I0304 19:28:22.697968 23118544486528 run.py:483] Algo bellman_ford step 970 current loss 0.014505, current_train_items 31072.
I0304 19:28:22.714596 23118544486528 run.py:483] Algo bellman_ford step 971 current loss 0.049260, current_train_items 31104.
I0304 19:28:22.738893 23118544486528 run.py:483] Algo bellman_ford step 972 current loss 0.180548, current_train_items 31136.
I0304 19:28:22.769703 23118544486528 run.py:483] Algo bellman_ford step 973 current loss 0.319838, current_train_items 31168.
I0304 19:28:22.803393 23118544486528 run.py:483] Algo bellman_ford step 974 current loss 0.271562, current_train_items 31200.
I0304 19:28:22.822575 23118544486528 run.py:483] Algo bellman_ford step 975 current loss 0.021783, current_train_items 31232.
I0304 19:28:22.838831 23118544486528 run.py:483] Algo bellman_ford step 976 current loss 0.042126, current_train_items 31264.
I0304 19:28:22.862578 23118544486528 run.py:483] Algo bellman_ford step 977 current loss 0.173562, current_train_items 31296.
I0304 19:28:22.893096 23118544486528 run.py:483] Algo bellman_ford step 978 current loss 0.285603, current_train_items 31328.
I0304 19:28:22.925182 23118544486528 run.py:483] Algo bellman_ford step 979 current loss 0.271515, current_train_items 31360.
I0304 19:28:22.943801 23118544486528 run.py:483] Algo bellman_ford step 980 current loss 0.050231, current_train_items 31392.
I0304 19:28:22.960374 23118544486528 run.py:483] Algo bellman_ford step 981 current loss 0.068077, current_train_items 31424.
I0304 19:28:22.983926 23118544486528 run.py:483] Algo bellman_ford step 982 current loss 0.121194, current_train_items 31456.
I0304 19:28:23.013226 23118544486528 run.py:483] Algo bellman_ford step 983 current loss 0.125138, current_train_items 31488.
I0304 19:28:23.046425 23118544486528 run.py:483] Algo bellman_ford step 984 current loss 0.164755, current_train_items 31520.
I0304 19:28:23.066111 23118544486528 run.py:483] Algo bellman_ford step 985 current loss 0.024234, current_train_items 31552.
I0304 19:28:23.082935 23118544486528 run.py:483] Algo bellman_ford step 986 current loss 0.115464, current_train_items 31584.
I0304 19:28:23.105952 23118544486528 run.py:483] Algo bellman_ford step 987 current loss 0.112008, current_train_items 31616.
I0304 19:28:23.135890 23118544486528 run.py:483] Algo bellman_ford step 988 current loss 0.153519, current_train_items 31648.
I0304 19:28:23.167556 23118544486528 run.py:483] Algo bellman_ford step 989 current loss 0.126849, current_train_items 31680.
I0304 19:28:23.186731 23118544486528 run.py:483] Algo bellman_ford step 990 current loss 0.016191, current_train_items 31712.
I0304 19:28:23.202971 23118544486528 run.py:483] Algo bellman_ford step 991 current loss 0.064560, current_train_items 31744.
I0304 19:28:23.226835 23118544486528 run.py:483] Algo bellman_ford step 992 current loss 0.155170, current_train_items 31776.
I0304 19:28:23.255259 23118544486528 run.py:483] Algo bellman_ford step 993 current loss 0.137084, current_train_items 31808.
I0304 19:28:23.289830 23118544486528 run.py:483] Algo bellman_ford step 994 current loss 0.203908, current_train_items 31840.
I0304 19:28:23.308442 23118544486528 run.py:483] Algo bellman_ford step 995 current loss 0.026851, current_train_items 31872.
I0304 19:28:23.324735 23118544486528 run.py:483] Algo bellman_ford step 996 current loss 0.045399, current_train_items 31904.
I0304 19:28:23.348494 23118544486528 run.py:483] Algo bellman_ford step 997 current loss 0.134453, current_train_items 31936.
I0304 19:28:23.379469 23118544486528 run.py:483] Algo bellman_ford step 998 current loss 0.122885, current_train_items 31968.
I0304 19:28:23.411346 23118544486528 run.py:483] Algo bellman_ford step 999 current loss 0.162115, current_train_items 32000.
I0304 19:28:23.430567 23118544486528 run.py:483] Algo bellman_ford step 1000 current loss 0.030555, current_train_items 32032.
I0304 19:28:23.438213 23118544486528 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0304 19:28:23.438319 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.969, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:28:23.467566 23118544486528 run.py:483] Algo bellman_ford step 1001 current loss 0.083998, current_train_items 32064.
I0304 19:28:23.491465 23118544486528 run.py:483] Algo bellman_ford step 1002 current loss 0.108297, current_train_items 32096.
I0304 19:28:23.520617 23118544486528 run.py:483] Algo bellman_ford step 1003 current loss 0.140510, current_train_items 32128.
I0304 19:28:23.555547 23118544486528 run.py:483] Algo bellman_ford step 1004 current loss 0.179090, current_train_items 32160.
I0304 19:28:23.575066 23118544486528 run.py:483] Algo bellman_ford step 1005 current loss 0.054417, current_train_items 32192.
I0304 19:28:23.590996 23118544486528 run.py:483] Algo bellman_ford step 1006 current loss 0.074250, current_train_items 32224.
I0304 19:28:23.615799 23118544486528 run.py:483] Algo bellman_ford step 1007 current loss 0.110729, current_train_items 32256.
I0304 19:28:23.644792 23118544486528 run.py:483] Algo bellman_ford step 1008 current loss 0.093332, current_train_items 32288.
I0304 19:28:23.677925 23118544486528 run.py:483] Algo bellman_ford step 1009 current loss 0.164512, current_train_items 32320.
I0304 19:28:23.696697 23118544486528 run.py:483] Algo bellman_ford step 1010 current loss 0.027688, current_train_items 32352.
I0304 19:28:23.713157 23118544486528 run.py:483] Algo bellman_ford step 1011 current loss 0.058922, current_train_items 32384.
I0304 19:28:23.738318 23118544486528 run.py:483] Algo bellman_ford step 1012 current loss 0.131816, current_train_items 32416.
I0304 19:28:23.768513 23118544486528 run.py:483] Algo bellman_ford step 1013 current loss 0.221086, current_train_items 32448.
I0304 19:28:23.801225 23118544486528 run.py:483] Algo bellman_ford step 1014 current loss 0.179309, current_train_items 32480.
I0304 19:28:23.819992 23118544486528 run.py:483] Algo bellman_ford step 1015 current loss 0.013173, current_train_items 32512.
I0304 19:28:23.836657 23118544486528 run.py:483] Algo bellman_ford step 1016 current loss 0.071670, current_train_items 32544.
I0304 19:28:23.861054 23118544486528 run.py:483] Algo bellman_ford step 1017 current loss 0.138798, current_train_items 32576.
I0304 19:28:23.891319 23118544486528 run.py:483] Algo bellman_ford step 1018 current loss 0.142979, current_train_items 32608.
I0304 19:28:23.926429 23118544486528 run.py:483] Algo bellman_ford step 1019 current loss 0.275597, current_train_items 32640.
I0304 19:28:23.944924 23118544486528 run.py:483] Algo bellman_ford step 1020 current loss 0.016532, current_train_items 32672.
I0304 19:28:23.961174 23118544486528 run.py:483] Algo bellman_ford step 1021 current loss 0.049922, current_train_items 32704.
I0304 19:28:23.984631 23118544486528 run.py:483] Algo bellman_ford step 1022 current loss 0.107057, current_train_items 32736.
I0304 19:28:24.014142 23118544486528 run.py:483] Algo bellman_ford step 1023 current loss 0.114094, current_train_items 32768.
I0304 19:28:24.045220 23118544486528 run.py:483] Algo bellman_ford step 1024 current loss 0.140046, current_train_items 32800.
I0304 19:28:24.064067 23118544486528 run.py:483] Algo bellman_ford step 1025 current loss 0.049869, current_train_items 32832.
I0304 19:28:24.081127 23118544486528 run.py:483] Algo bellman_ford step 1026 current loss 0.059434, current_train_items 32864.
I0304 19:28:24.104850 23118544486528 run.py:483] Algo bellman_ford step 1027 current loss 0.092637, current_train_items 32896.
I0304 19:28:24.135655 23118544486528 run.py:483] Algo bellman_ford step 1028 current loss 0.129363, current_train_items 32928.
I0304 19:28:24.167307 23118544486528 run.py:483] Algo bellman_ford step 1029 current loss 0.165819, current_train_items 32960.
I0304 19:28:24.186136 23118544486528 run.py:483] Algo bellman_ford step 1030 current loss 0.021656, current_train_items 32992.
I0304 19:28:24.202036 23118544486528 run.py:483] Algo bellman_ford step 1031 current loss 0.036734, current_train_items 33024.
I0304 19:28:24.225206 23118544486528 run.py:483] Algo bellman_ford step 1032 current loss 0.080059, current_train_items 33056.
I0304 19:28:24.254509 23118544486528 run.py:483] Algo bellman_ford step 1033 current loss 0.124185, current_train_items 33088.
I0304 19:28:24.287869 23118544486528 run.py:483] Algo bellman_ford step 1034 current loss 0.161599, current_train_items 33120.
I0304 19:28:24.306716 23118544486528 run.py:483] Algo bellman_ford step 1035 current loss 0.020792, current_train_items 33152.
I0304 19:28:24.323422 23118544486528 run.py:483] Algo bellman_ford step 1036 current loss 0.048656, current_train_items 33184.
I0304 19:28:24.347907 23118544486528 run.py:483] Algo bellman_ford step 1037 current loss 0.095895, current_train_items 33216.
I0304 19:28:24.378193 23118544486528 run.py:483] Algo bellman_ford step 1038 current loss 0.235201, current_train_items 33248.
I0304 19:28:24.412017 23118544486528 run.py:483] Algo bellman_ford step 1039 current loss 0.203306, current_train_items 33280.
I0304 19:28:24.430802 23118544486528 run.py:483] Algo bellman_ford step 1040 current loss 0.020188, current_train_items 33312.
I0304 19:28:24.448107 23118544486528 run.py:483] Algo bellman_ford step 1041 current loss 0.055841, current_train_items 33344.
I0304 19:28:24.472519 23118544486528 run.py:483] Algo bellman_ford step 1042 current loss 0.180530, current_train_items 33376.
I0304 19:28:24.502640 23118544486528 run.py:483] Algo bellman_ford step 1043 current loss 0.132006, current_train_items 33408.
I0304 19:28:24.534502 23118544486528 run.py:483] Algo bellman_ford step 1044 current loss 0.134520, current_train_items 33440.
I0304 19:28:24.553069 23118544486528 run.py:483] Algo bellman_ford step 1045 current loss 0.010862, current_train_items 33472.
I0304 19:28:24.569084 23118544486528 run.py:483] Algo bellman_ford step 1046 current loss 0.046835, current_train_items 33504.
I0304 19:28:24.593618 23118544486528 run.py:483] Algo bellman_ford step 1047 current loss 0.103811, current_train_items 33536.
I0304 19:28:24.624355 23118544486528 run.py:483] Algo bellman_ford step 1048 current loss 0.137191, current_train_items 33568.
I0304 19:28:24.657190 23118544486528 run.py:483] Algo bellman_ford step 1049 current loss 0.173250, current_train_items 33600.
I0304 19:28:24.675832 23118544486528 run.py:483] Algo bellman_ford step 1050 current loss 0.020985, current_train_items 33632.
I0304 19:28:24.683809 23118544486528 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0304 19:28:24.683917 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.973, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:28:24.712824 23118544486528 run.py:483] Algo bellman_ford step 1051 current loss 0.061380, current_train_items 33664.
I0304 19:28:24.737580 23118544486528 run.py:483] Algo bellman_ford step 1052 current loss 0.198793, current_train_items 33696.
I0304 19:28:24.766932 23118544486528 run.py:483] Algo bellman_ford step 1053 current loss 0.190711, current_train_items 33728.
I0304 19:28:24.799714 23118544486528 run.py:483] Algo bellman_ford step 1054 current loss 0.223286, current_train_items 33760.
I0304 19:28:24.818886 23118544486528 run.py:483] Algo bellman_ford step 1055 current loss 0.032462, current_train_items 33792.
I0304 19:28:24.835518 23118544486528 run.py:483] Algo bellman_ford step 1056 current loss 0.045743, current_train_items 33824.
I0304 19:28:24.859790 23118544486528 run.py:483] Algo bellman_ford step 1057 current loss 0.135339, current_train_items 33856.
I0304 19:28:24.890775 23118544486528 run.py:483] Algo bellman_ford step 1058 current loss 0.179041, current_train_items 33888.
I0304 19:28:24.921611 23118544486528 run.py:483] Algo bellman_ford step 1059 current loss 0.155409, current_train_items 33920.
I0304 19:28:24.940422 23118544486528 run.py:483] Algo bellman_ford step 1060 current loss 0.017006, current_train_items 33952.
I0304 19:28:24.956748 23118544486528 run.py:483] Algo bellman_ford step 1061 current loss 0.036046, current_train_items 33984.
I0304 19:28:24.978964 23118544486528 run.py:483] Algo bellman_ford step 1062 current loss 0.056882, current_train_items 34016.
I0304 19:28:25.008228 23118544486528 run.py:483] Algo bellman_ford step 1063 current loss 0.162032, current_train_items 34048.
I0304 19:28:25.041138 23118544486528 run.py:483] Algo bellman_ford step 1064 current loss 0.166831, current_train_items 34080.
I0304 19:28:25.059760 23118544486528 run.py:483] Algo bellman_ford step 1065 current loss 0.014238, current_train_items 34112.
I0304 19:28:25.076005 23118544486528 run.py:483] Algo bellman_ford step 1066 current loss 0.081165, current_train_items 34144.
I0304 19:28:25.100221 23118544486528 run.py:483] Algo bellman_ford step 1067 current loss 0.121363, current_train_items 34176.
I0304 19:28:25.130212 23118544486528 run.py:483] Algo bellman_ford step 1068 current loss 0.099542, current_train_items 34208.
I0304 19:28:25.164039 23118544486528 run.py:483] Algo bellman_ford step 1069 current loss 0.176782, current_train_items 34240.
I0304 19:28:25.182691 23118544486528 run.py:483] Algo bellman_ford step 1070 current loss 0.013744, current_train_items 34272.
I0304 19:28:25.199680 23118544486528 run.py:483] Algo bellman_ford step 1071 current loss 0.083098, current_train_items 34304.
I0304 19:28:25.223590 23118544486528 run.py:483] Algo bellman_ford step 1072 current loss 0.112137, current_train_items 34336.
I0304 19:28:25.253258 23118544486528 run.py:483] Algo bellman_ford step 1073 current loss 0.094807, current_train_items 34368.
I0304 19:28:25.284457 23118544486528 run.py:483] Algo bellman_ford step 1074 current loss 0.111840, current_train_items 34400.
I0304 19:28:25.303274 23118544486528 run.py:483] Algo bellman_ford step 1075 current loss 0.010743, current_train_items 34432.
I0304 19:28:25.320284 23118544486528 run.py:483] Algo bellman_ford step 1076 current loss 0.094309, current_train_items 34464.
I0304 19:28:25.344050 23118544486528 run.py:483] Algo bellman_ford step 1077 current loss 0.155014, current_train_items 34496.
I0304 19:28:25.374312 23118544486528 run.py:483] Algo bellman_ford step 1078 current loss 0.178587, current_train_items 34528.
I0304 19:28:25.406059 23118544486528 run.py:483] Algo bellman_ford step 1079 current loss 0.157146, current_train_items 34560.
I0304 19:28:25.424660 23118544486528 run.py:483] Algo bellman_ford step 1080 current loss 0.025946, current_train_items 34592.
I0304 19:28:25.441413 23118544486528 run.py:483] Algo bellman_ford step 1081 current loss 0.077774, current_train_items 34624.
I0304 19:28:25.464788 23118544486528 run.py:483] Algo bellman_ford step 1082 current loss 0.360271, current_train_items 34656.
I0304 19:28:25.494180 23118544486528 run.py:483] Algo bellman_ford step 1083 current loss 0.208563, current_train_items 34688.
I0304 19:28:25.527196 23118544486528 run.py:483] Algo bellman_ford step 1084 current loss 0.179893, current_train_items 34720.
I0304 19:28:25.546133 23118544486528 run.py:483] Algo bellman_ford step 1085 current loss 0.018105, current_train_items 34752.
I0304 19:28:25.562651 23118544486528 run.py:483] Algo bellman_ford step 1086 current loss 0.085160, current_train_items 34784.
I0304 19:28:25.586444 23118544486528 run.py:483] Algo bellman_ford step 1087 current loss 0.258060, current_train_items 34816.
I0304 19:28:25.614879 23118544486528 run.py:483] Algo bellman_ford step 1088 current loss 0.245726, current_train_items 34848.
I0304 19:28:25.645357 23118544486528 run.py:483] Algo bellman_ford step 1089 current loss 0.213144, current_train_items 34880.
I0304 19:28:25.664386 23118544486528 run.py:483] Algo bellman_ford step 1090 current loss 0.042860, current_train_items 34912.
I0304 19:28:25.681118 23118544486528 run.py:483] Algo bellman_ford step 1091 current loss 0.151035, current_train_items 34944.
I0304 19:28:25.704269 23118544486528 run.py:483] Algo bellman_ford step 1092 current loss 0.184104, current_train_items 34976.
I0304 19:28:25.734708 23118544486528 run.py:483] Algo bellman_ford step 1093 current loss 0.328392, current_train_items 35008.
I0304 19:28:25.767871 23118544486528 run.py:483] Algo bellman_ford step 1094 current loss 0.270889, current_train_items 35040.
I0304 19:28:25.786901 23118544486528 run.py:483] Algo bellman_ford step 1095 current loss 0.012963, current_train_items 35072.
I0304 19:28:25.803204 23118544486528 run.py:483] Algo bellman_ford step 1096 current loss 0.067536, current_train_items 35104.
I0304 19:28:25.827273 23118544486528 run.py:483] Algo bellman_ford step 1097 current loss 0.155992, current_train_items 35136.
I0304 19:28:25.857392 23118544486528 run.py:483] Algo bellman_ford step 1098 current loss 0.227245, current_train_items 35168.
I0304 19:28:25.889097 23118544486528 run.py:483] Algo bellman_ford step 1099 current loss 0.234794, current_train_items 35200.
I0304 19:28:25.908002 23118544486528 run.py:483] Algo bellman_ford step 1100 current loss 0.010219, current_train_items 35232.
I0304 19:28:25.915709 23118544486528 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0304 19:28:25.915815 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.974, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:28:25.945553 23118544486528 run.py:483] Algo bellman_ford step 1101 current loss 0.044892, current_train_items 35264.
I0304 19:28:25.969355 23118544486528 run.py:483] Algo bellman_ford step 1102 current loss 0.089493, current_train_items 35296.
I0304 19:28:26.001075 23118544486528 run.py:483] Algo bellman_ford step 1103 current loss 0.118802, current_train_items 35328.
I0304 19:28:26.035393 23118544486528 run.py:483] Algo bellman_ford step 1104 current loss 0.202221, current_train_items 35360.
I0304 19:28:26.054479 23118544486528 run.py:483] Algo bellman_ford step 1105 current loss 0.021455, current_train_items 35392.
I0304 19:28:26.070499 23118544486528 run.py:483] Algo bellman_ford step 1106 current loss 0.026852, current_train_items 35424.
I0304 19:28:26.094809 23118544486528 run.py:483] Algo bellman_ford step 1107 current loss 0.128862, current_train_items 35456.
I0304 19:28:26.123911 23118544486528 run.py:483] Algo bellman_ford step 1108 current loss 0.110162, current_train_items 35488.
I0304 19:28:26.154518 23118544486528 run.py:483] Algo bellman_ford step 1109 current loss 0.105606, current_train_items 35520.
I0304 19:28:26.173638 23118544486528 run.py:483] Algo bellman_ford step 1110 current loss 0.031297, current_train_items 35552.
I0304 19:28:26.190128 23118544486528 run.py:483] Algo bellman_ford step 1111 current loss 0.047708, current_train_items 35584.
I0304 19:28:26.213847 23118544486528 run.py:483] Algo bellman_ford step 1112 current loss 0.127900, current_train_items 35616.
I0304 19:28:26.242556 23118544486528 run.py:483] Algo bellman_ford step 1113 current loss 0.099252, current_train_items 35648.
I0304 19:28:26.276441 23118544486528 run.py:483] Algo bellman_ford step 1114 current loss 0.121858, current_train_items 35680.
I0304 19:28:26.295463 23118544486528 run.py:483] Algo bellman_ford step 1115 current loss 0.017477, current_train_items 35712.
I0304 19:28:26.311476 23118544486528 run.py:483] Algo bellman_ford step 1116 current loss 0.054784, current_train_items 35744.
I0304 19:28:26.335971 23118544486528 run.py:483] Algo bellman_ford step 1117 current loss 0.114482, current_train_items 35776.
I0304 19:28:26.366483 23118544486528 run.py:483] Algo bellman_ford step 1118 current loss 0.092740, current_train_items 35808.
I0304 19:28:26.399541 23118544486528 run.py:483] Algo bellman_ford step 1119 current loss 0.195590, current_train_items 35840.
I0304 19:28:26.418705 23118544486528 run.py:483] Algo bellman_ford step 1120 current loss 0.018584, current_train_items 35872.
I0304 19:28:26.434547 23118544486528 run.py:483] Algo bellman_ford step 1121 current loss 0.048278, current_train_items 35904.
I0304 19:28:26.458623 23118544486528 run.py:483] Algo bellman_ford step 1122 current loss 0.112309, current_train_items 35936.
I0304 19:28:26.487649 23118544486528 run.py:483] Algo bellman_ford step 1123 current loss 0.085347, current_train_items 35968.
I0304 19:28:26.522125 23118544486528 run.py:483] Algo bellman_ford step 1124 current loss 0.171259, current_train_items 36000.
I0304 19:28:26.541038 23118544486528 run.py:483] Algo bellman_ford step 1125 current loss 0.013518, current_train_items 36032.
I0304 19:28:26.557094 23118544486528 run.py:483] Algo bellman_ford step 1126 current loss 0.069214, current_train_items 36064.
I0304 19:28:26.580842 23118544486528 run.py:483] Algo bellman_ford step 1127 current loss 0.220164, current_train_items 36096.
I0304 19:28:26.610998 23118544486528 run.py:483] Algo bellman_ford step 1128 current loss 0.183208, current_train_items 36128.
I0304 19:28:26.641592 23118544486528 run.py:483] Algo bellman_ford step 1129 current loss 0.176534, current_train_items 36160.
I0304 19:28:26.660538 23118544486528 run.py:483] Algo bellman_ford step 1130 current loss 0.016049, current_train_items 36192.
I0304 19:28:26.677033 23118544486528 run.py:483] Algo bellman_ford step 1131 current loss 0.034834, current_train_items 36224.
I0304 19:28:26.701351 23118544486528 run.py:483] Algo bellman_ford step 1132 current loss 0.175542, current_train_items 36256.
I0304 19:28:26.731306 23118544486528 run.py:483] Algo bellman_ford step 1133 current loss 0.199281, current_train_items 36288.
I0304 19:28:26.763019 23118544486528 run.py:483] Algo bellman_ford step 1134 current loss 0.167915, current_train_items 36320.
I0304 19:28:26.781937 23118544486528 run.py:483] Algo bellman_ford step 1135 current loss 0.022606, current_train_items 36352.
I0304 19:28:26.798276 23118544486528 run.py:483] Algo bellman_ford step 1136 current loss 0.039305, current_train_items 36384.
I0304 19:28:26.821505 23118544486528 run.py:483] Algo bellman_ford step 1137 current loss 0.103929, current_train_items 36416.
I0304 19:28:26.852502 23118544486528 run.py:483] Algo bellman_ford step 1138 current loss 0.120437, current_train_items 36448.
I0304 19:28:26.884945 23118544486528 run.py:483] Algo bellman_ford step 1139 current loss 0.130937, current_train_items 36480.
I0304 19:28:26.904107 23118544486528 run.py:483] Algo bellman_ford step 1140 current loss 0.012003, current_train_items 36512.
I0304 19:28:26.920844 23118544486528 run.py:483] Algo bellman_ford step 1141 current loss 0.032350, current_train_items 36544.
I0304 19:28:26.943858 23118544486528 run.py:483] Algo bellman_ford step 1142 current loss 0.144930, current_train_items 36576.
I0304 19:28:26.973153 23118544486528 run.py:483] Algo bellman_ford step 1143 current loss 0.160424, current_train_items 36608.
I0304 19:28:27.005166 23118544486528 run.py:483] Algo bellman_ford step 1144 current loss 0.138613, current_train_items 36640.
I0304 19:28:27.024133 23118544486528 run.py:483] Algo bellman_ford step 1145 current loss 0.033180, current_train_items 36672.
I0304 19:28:27.040702 23118544486528 run.py:483] Algo bellman_ford step 1146 current loss 0.075673, current_train_items 36704.
I0304 19:28:27.064819 23118544486528 run.py:483] Algo bellman_ford step 1147 current loss 0.094931, current_train_items 36736.
I0304 19:28:27.095038 23118544486528 run.py:483] Algo bellman_ford step 1148 current loss 0.119205, current_train_items 36768.
I0304 19:28:27.125342 23118544486528 run.py:483] Algo bellman_ford step 1149 current loss 0.144490, current_train_items 36800.
I0304 19:28:27.144348 23118544486528 run.py:483] Algo bellman_ford step 1150 current loss 0.057368, current_train_items 36832.
I0304 19:28:27.153180 23118544486528 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0304 19:28:27.153285 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:28:27.170005 23118544486528 run.py:483] Algo bellman_ford step 1151 current loss 0.032581, current_train_items 36864.
I0304 19:28:27.194485 23118544486528 run.py:483] Algo bellman_ford step 1152 current loss 0.127695, current_train_items 36896.
I0304 19:28:27.226024 23118544486528 run.py:483] Algo bellman_ford step 1153 current loss 0.129912, current_train_items 36928.
W0304 19:28:27.247024 23118544486528 samplers.py:155] Increasing hint lengh from 11 to 12
I0304 19:28:34.001959 23118544486528 run.py:483] Algo bellman_ford step 1154 current loss 0.136813, current_train_items 36960.
I0304 19:28:34.022584 23118544486528 run.py:483] Algo bellman_ford step 1155 current loss 0.012322, current_train_items 36992.
I0304 19:28:34.038802 23118544486528 run.py:483] Algo bellman_ford step 1156 current loss 0.065932, current_train_items 37024.
I0304 19:28:34.062791 23118544486528 run.py:483] Algo bellman_ford step 1157 current loss 0.079179, current_train_items 37056.
I0304 19:28:34.092723 23118544486528 run.py:483] Algo bellman_ford step 1158 current loss 0.122590, current_train_items 37088.
I0304 19:28:34.123521 23118544486528 run.py:483] Algo bellman_ford step 1159 current loss 0.142221, current_train_items 37120.
I0304 19:28:34.143183 23118544486528 run.py:483] Algo bellman_ford step 1160 current loss 0.025137, current_train_items 37152.
I0304 19:28:34.159751 23118544486528 run.py:483] Algo bellman_ford step 1161 current loss 0.042307, current_train_items 37184.
I0304 19:28:34.182284 23118544486528 run.py:483] Algo bellman_ford step 1162 current loss 0.060372, current_train_items 37216.
I0304 19:28:34.211672 23118544486528 run.py:483] Algo bellman_ford step 1163 current loss 0.139913, current_train_items 37248.
I0304 19:28:34.244294 23118544486528 run.py:483] Algo bellman_ford step 1164 current loss 0.203868, current_train_items 37280.
I0304 19:28:34.263250 23118544486528 run.py:483] Algo bellman_ford step 1165 current loss 0.013438, current_train_items 37312.
I0304 19:28:34.279555 23118544486528 run.py:483] Algo bellman_ford step 1166 current loss 0.089669, current_train_items 37344.
I0304 19:28:34.303377 23118544486528 run.py:483] Algo bellman_ford step 1167 current loss 0.207544, current_train_items 37376.
I0304 19:28:34.333172 23118544486528 run.py:483] Algo bellman_ford step 1168 current loss 0.136334, current_train_items 37408.
I0304 19:28:34.367569 23118544486528 run.py:483] Algo bellman_ford step 1169 current loss 0.232040, current_train_items 37440.
I0304 19:28:34.387161 23118544486528 run.py:483] Algo bellman_ford step 1170 current loss 0.022892, current_train_items 37472.
I0304 19:28:34.404042 23118544486528 run.py:483] Algo bellman_ford step 1171 current loss 0.085809, current_train_items 37504.
I0304 19:28:34.427479 23118544486528 run.py:483] Algo bellman_ford step 1172 current loss 0.110931, current_train_items 37536.
I0304 19:28:34.458031 23118544486528 run.py:483] Algo bellman_ford step 1173 current loss 0.151794, current_train_items 37568.
I0304 19:28:34.491015 23118544486528 run.py:483] Algo bellman_ford step 1174 current loss 0.163982, current_train_items 37600.
I0304 19:28:34.510407 23118544486528 run.py:483] Algo bellman_ford step 1175 current loss 0.012226, current_train_items 37632.
I0304 19:28:34.526935 23118544486528 run.py:483] Algo bellman_ford step 1176 current loss 0.064661, current_train_items 37664.
I0304 19:28:34.549861 23118544486528 run.py:483] Algo bellman_ford step 1177 current loss 0.139471, current_train_items 37696.
I0304 19:28:34.579394 23118544486528 run.py:483] Algo bellman_ford step 1178 current loss 0.119453, current_train_items 37728.
I0304 19:28:34.613570 23118544486528 run.py:483] Algo bellman_ford step 1179 current loss 0.182475, current_train_items 37760.
I0304 19:28:34.632478 23118544486528 run.py:483] Algo bellman_ford step 1180 current loss 0.018267, current_train_items 37792.
I0304 19:28:34.649072 23118544486528 run.py:483] Algo bellman_ford step 1181 current loss 0.134513, current_train_items 37824.
I0304 19:28:34.673031 23118544486528 run.py:483] Algo bellman_ford step 1182 current loss 0.094821, current_train_items 37856.
I0304 19:28:34.701844 23118544486528 run.py:483] Algo bellman_ford step 1183 current loss 0.158071, current_train_items 37888.
I0304 19:28:34.734411 23118544486528 run.py:483] Algo bellman_ford step 1184 current loss 0.224874, current_train_items 37920.
I0304 19:28:34.753880 23118544486528 run.py:483] Algo bellman_ford step 1185 current loss 0.037017, current_train_items 37952.
I0304 19:28:34.770124 23118544486528 run.py:483] Algo bellman_ford step 1186 current loss 0.062776, current_train_items 37984.
I0304 19:28:34.793267 23118544486528 run.py:483] Algo bellman_ford step 1187 current loss 0.116149, current_train_items 38016.
I0304 19:28:34.822729 23118544486528 run.py:483] Algo bellman_ford step 1188 current loss 0.100670, current_train_items 38048.
I0304 19:28:34.854033 23118544486528 run.py:483] Algo bellman_ford step 1189 current loss 0.219951, current_train_items 38080.
I0304 19:28:34.873366 23118544486528 run.py:483] Algo bellman_ford step 1190 current loss 0.120126, current_train_items 38112.
I0304 19:28:34.889930 23118544486528 run.py:483] Algo bellman_ford step 1191 current loss 0.086375, current_train_items 38144.
I0304 19:28:34.914263 23118544486528 run.py:483] Algo bellman_ford step 1192 current loss 0.193449, current_train_items 38176.
I0304 19:28:34.944295 23118544486528 run.py:483] Algo bellman_ford step 1193 current loss 0.148054, current_train_items 38208.
I0304 19:28:34.976474 23118544486528 run.py:483] Algo bellman_ford step 1194 current loss 0.168059, current_train_items 38240.
I0304 19:28:34.995618 23118544486528 run.py:483] Algo bellman_ford step 1195 current loss 0.022680, current_train_items 38272.
I0304 19:28:35.012125 23118544486528 run.py:483] Algo bellman_ford step 1196 current loss 0.078775, current_train_items 38304.
I0304 19:28:35.035282 23118544486528 run.py:483] Algo bellman_ford step 1197 current loss 0.129457, current_train_items 38336.
I0304 19:28:35.066123 23118544486528 run.py:483] Algo bellman_ford step 1198 current loss 0.121876, current_train_items 38368.
I0304 19:28:35.100332 23118544486528 run.py:483] Algo bellman_ford step 1199 current loss 0.188092, current_train_items 38400.
I0304 19:28:35.119810 23118544486528 run.py:483] Algo bellman_ford step 1200 current loss 0.026523, current_train_items 38432.
I0304 19:28:35.129212 23118544486528 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0304 19:28:35.129320 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0304 19:28:35.146543 23118544486528 run.py:483] Algo bellman_ford step 1201 current loss 0.079472, current_train_items 38464.
I0304 19:28:35.170641 23118544486528 run.py:483] Algo bellman_ford step 1202 current loss 0.087149, current_train_items 38496.
I0304 19:28:35.202659 23118544486528 run.py:483] Algo bellman_ford step 1203 current loss 0.116554, current_train_items 38528.
I0304 19:28:35.236420 23118544486528 run.py:483] Algo bellman_ford step 1204 current loss 0.209314, current_train_items 38560.
I0304 19:28:35.255707 23118544486528 run.py:483] Algo bellman_ford step 1205 current loss 0.017796, current_train_items 38592.
I0304 19:28:35.272100 23118544486528 run.py:483] Algo bellman_ford step 1206 current loss 0.056246, current_train_items 38624.
I0304 19:28:35.295261 23118544486528 run.py:483] Algo bellman_ford step 1207 current loss 0.121665, current_train_items 38656.
I0304 19:28:35.325853 23118544486528 run.py:483] Algo bellman_ford step 1208 current loss 0.108472, current_train_items 38688.
I0304 19:28:35.359626 23118544486528 run.py:483] Algo bellman_ford step 1209 current loss 0.118433, current_train_items 38720.
I0304 19:28:35.378814 23118544486528 run.py:483] Algo bellman_ford step 1210 current loss 0.010487, current_train_items 38752.
I0304 19:28:35.395519 23118544486528 run.py:483] Algo bellman_ford step 1211 current loss 0.083360, current_train_items 38784.
I0304 19:28:35.418421 23118544486528 run.py:483] Algo bellman_ford step 1212 current loss 0.138125, current_train_items 38816.
I0304 19:28:35.447314 23118544486528 run.py:483] Algo bellman_ford step 1213 current loss 0.095654, current_train_items 38848.
I0304 19:28:35.480052 23118544486528 run.py:483] Algo bellman_ford step 1214 current loss 0.174119, current_train_items 38880.
I0304 19:28:35.499904 23118544486528 run.py:483] Algo bellman_ford step 1215 current loss 0.022738, current_train_items 38912.
I0304 19:28:35.516122 23118544486528 run.py:483] Algo bellman_ford step 1216 current loss 0.053654, current_train_items 38944.
I0304 19:28:35.539773 23118544486528 run.py:483] Algo bellman_ford step 1217 current loss 0.195303, current_train_items 38976.
I0304 19:28:35.570515 23118544486528 run.py:483] Algo bellman_ford step 1218 current loss 0.217471, current_train_items 39008.
I0304 19:28:35.603709 23118544486528 run.py:483] Algo bellman_ford step 1219 current loss 0.157085, current_train_items 39040.
I0304 19:28:35.623652 23118544486528 run.py:483] Algo bellman_ford step 1220 current loss 0.026598, current_train_items 39072.
I0304 19:28:35.640023 23118544486528 run.py:483] Algo bellman_ford step 1221 current loss 0.070050, current_train_items 39104.
I0304 19:28:35.664515 23118544486528 run.py:483] Algo bellman_ford step 1222 current loss 0.220513, current_train_items 39136.
I0304 19:28:35.695476 23118544486528 run.py:483] Algo bellman_ford step 1223 current loss 0.219935, current_train_items 39168.
I0304 19:28:35.728159 23118544486528 run.py:483] Algo bellman_ford step 1224 current loss 0.174292, current_train_items 39200.
I0304 19:28:35.747495 23118544486528 run.py:483] Algo bellman_ford step 1225 current loss 0.016872, current_train_items 39232.
I0304 19:28:35.763881 23118544486528 run.py:483] Algo bellman_ford step 1226 current loss 0.118480, current_train_items 39264.
I0304 19:28:35.788501 23118544486528 run.py:483] Algo bellman_ford step 1227 current loss 0.160165, current_train_items 39296.
I0304 19:28:35.818297 23118544486528 run.py:483] Algo bellman_ford step 1228 current loss 0.174244, current_train_items 39328.
I0304 19:28:35.852428 23118544486528 run.py:483] Algo bellman_ford step 1229 current loss 0.188770, current_train_items 39360.
I0304 19:28:35.871890 23118544486528 run.py:483] Algo bellman_ford step 1230 current loss 0.010747, current_train_items 39392.
I0304 19:28:35.888443 23118544486528 run.py:483] Algo bellman_ford step 1231 current loss 0.102208, current_train_items 39424.
I0304 19:28:35.913511 23118544486528 run.py:483] Algo bellman_ford step 1232 current loss 0.165185, current_train_items 39456.
I0304 19:28:35.943600 23118544486528 run.py:483] Algo bellman_ford step 1233 current loss 0.151579, current_train_items 39488.
I0304 19:28:35.974727 23118544486528 run.py:483] Algo bellman_ford step 1234 current loss 0.161670, current_train_items 39520.
I0304 19:28:35.993764 23118544486528 run.py:483] Algo bellman_ford step 1235 current loss 0.011459, current_train_items 39552.
I0304 19:28:36.009693 23118544486528 run.py:483] Algo bellman_ford step 1236 current loss 0.047910, current_train_items 39584.
I0304 19:28:36.033771 23118544486528 run.py:483] Algo bellman_ford step 1237 current loss 0.112164, current_train_items 39616.
I0304 19:28:36.063793 23118544486528 run.py:483] Algo bellman_ford step 1238 current loss 0.118897, current_train_items 39648.
I0304 19:28:36.097567 23118544486528 run.py:483] Algo bellman_ford step 1239 current loss 0.100421, current_train_items 39680.
I0304 19:28:36.116812 23118544486528 run.py:483] Algo bellman_ford step 1240 current loss 0.024510, current_train_items 39712.
I0304 19:28:36.133481 23118544486528 run.py:483] Algo bellman_ford step 1241 current loss 0.051069, current_train_items 39744.
I0304 19:28:36.157909 23118544486528 run.py:483] Algo bellman_ford step 1242 current loss 0.067900, current_train_items 39776.
I0304 19:28:36.187862 23118544486528 run.py:483] Algo bellman_ford step 1243 current loss 0.105704, current_train_items 39808.
I0304 19:28:36.220882 23118544486528 run.py:483] Algo bellman_ford step 1244 current loss 0.163178, current_train_items 39840.
I0304 19:28:36.239885 23118544486528 run.py:483] Algo bellman_ford step 1245 current loss 0.011130, current_train_items 39872.
I0304 19:28:36.256129 23118544486528 run.py:483] Algo bellman_ford step 1246 current loss 0.039533, current_train_items 39904.
I0304 19:28:36.279328 23118544486528 run.py:483] Algo bellman_ford step 1247 current loss 0.068643, current_train_items 39936.
I0304 19:28:36.310228 23118544486528 run.py:483] Algo bellman_ford step 1248 current loss 0.125900, current_train_items 39968.
I0304 19:28:36.342274 23118544486528 run.py:483] Algo bellman_ford step 1249 current loss 0.134701, current_train_items 40000.
I0304 19:28:36.361638 23118544486528 run.py:483] Algo bellman_ford step 1250 current loss 0.010811, current_train_items 40032.
I0304 19:28:36.369760 23118544486528 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0304 19:28:36.369869 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:28:36.386988 23118544486528 run.py:483] Algo bellman_ford step 1251 current loss 0.035854, current_train_items 40064.
I0304 19:28:36.411398 23118544486528 run.py:483] Algo bellman_ford step 1252 current loss 0.067039, current_train_items 40096.
I0304 19:28:36.440222 23118544486528 run.py:483] Algo bellman_ford step 1253 current loss 0.094203, current_train_items 40128.
I0304 19:28:36.470525 23118544486528 run.py:483] Algo bellman_ford step 1254 current loss 0.105781, current_train_items 40160.
I0304 19:28:36.490283 23118544486528 run.py:483] Algo bellman_ford step 1255 current loss 0.009874, current_train_items 40192.
I0304 19:28:36.506876 23118544486528 run.py:483] Algo bellman_ford step 1256 current loss 0.073849, current_train_items 40224.
I0304 19:28:36.530535 23118544486528 run.py:483] Algo bellman_ford step 1257 current loss 0.062479, current_train_items 40256.
I0304 19:28:36.559908 23118544486528 run.py:483] Algo bellman_ford step 1258 current loss 0.078106, current_train_items 40288.
I0304 19:28:36.592253 23118544486528 run.py:483] Algo bellman_ford step 1259 current loss 0.121983, current_train_items 40320.
I0304 19:28:36.612020 23118544486528 run.py:483] Algo bellman_ford step 1260 current loss 0.017691, current_train_items 40352.
I0304 19:28:36.629164 23118544486528 run.py:483] Algo bellman_ford step 1261 current loss 0.044187, current_train_items 40384.
I0304 19:28:36.653116 23118544486528 run.py:483] Algo bellman_ford step 1262 current loss 0.097729, current_train_items 40416.
I0304 19:28:36.680587 23118544486528 run.py:483] Algo bellman_ford step 1263 current loss 0.148992, current_train_items 40448.
I0304 19:28:36.715289 23118544486528 run.py:483] Algo bellman_ford step 1264 current loss 0.172791, current_train_items 40480.
I0304 19:28:36.734355 23118544486528 run.py:483] Algo bellman_ford step 1265 current loss 0.011571, current_train_items 40512.
I0304 19:28:36.750731 23118544486528 run.py:483] Algo bellman_ford step 1266 current loss 0.055117, current_train_items 40544.
I0304 19:28:36.773667 23118544486528 run.py:483] Algo bellman_ford step 1267 current loss 0.101744, current_train_items 40576.
I0304 19:28:36.803060 23118544486528 run.py:483] Algo bellman_ford step 1268 current loss 0.103176, current_train_items 40608.
I0304 19:28:36.836381 23118544486528 run.py:483] Algo bellman_ford step 1269 current loss 0.144788, current_train_items 40640.
I0304 19:28:36.856354 23118544486528 run.py:483] Algo bellman_ford step 1270 current loss 0.009628, current_train_items 40672.
I0304 19:28:36.872571 23118544486528 run.py:483] Algo bellman_ford step 1271 current loss 0.049876, current_train_items 40704.
I0304 19:28:36.894869 23118544486528 run.py:483] Algo bellman_ford step 1272 current loss 0.107071, current_train_items 40736.
I0304 19:28:36.923473 23118544486528 run.py:483] Algo bellman_ford step 1273 current loss 0.066287, current_train_items 40768.
I0304 19:28:36.956831 23118544486528 run.py:483] Algo bellman_ford step 1274 current loss 0.144435, current_train_items 40800.
I0304 19:28:36.976408 23118544486528 run.py:483] Algo bellman_ford step 1275 current loss 0.010319, current_train_items 40832.
I0304 19:28:36.993037 23118544486528 run.py:483] Algo bellman_ford step 1276 current loss 0.041585, current_train_items 40864.
I0304 19:28:37.015716 23118544486528 run.py:483] Algo bellman_ford step 1277 current loss 0.069303, current_train_items 40896.
I0304 19:28:37.045363 23118544486528 run.py:483] Algo bellman_ford step 1278 current loss 0.095313, current_train_items 40928.
I0304 19:28:37.077430 23118544486528 run.py:483] Algo bellman_ford step 1279 current loss 0.149756, current_train_items 40960.
I0304 19:28:37.097037 23118544486528 run.py:483] Algo bellman_ford step 1280 current loss 0.011156, current_train_items 40992.
I0304 19:28:37.113341 23118544486528 run.py:483] Algo bellman_ford step 1281 current loss 0.063594, current_train_items 41024.
I0304 19:28:37.138047 23118544486528 run.py:483] Algo bellman_ford step 1282 current loss 0.089843, current_train_items 41056.
I0304 19:28:37.168745 23118544486528 run.py:483] Algo bellman_ford step 1283 current loss 0.117750, current_train_items 41088.
I0304 19:28:37.200291 23118544486528 run.py:483] Algo bellman_ford step 1284 current loss 0.131447, current_train_items 41120.
I0304 19:28:37.219915 23118544486528 run.py:483] Algo bellman_ford step 1285 current loss 0.057279, current_train_items 41152.
I0304 19:28:37.236590 23118544486528 run.py:483] Algo bellman_ford step 1286 current loss 0.030927, current_train_items 41184.
I0304 19:28:37.261411 23118544486528 run.py:483] Algo bellman_ford step 1287 current loss 0.157335, current_train_items 41216.
I0304 19:28:37.291364 23118544486528 run.py:483] Algo bellman_ford step 1288 current loss 0.150573, current_train_items 41248.
I0304 19:28:37.324819 23118544486528 run.py:483] Algo bellman_ford step 1289 current loss 0.175708, current_train_items 41280.
I0304 19:28:37.344775 23118544486528 run.py:483] Algo bellman_ford step 1290 current loss 0.017904, current_train_items 41312.
I0304 19:28:37.361392 23118544486528 run.py:483] Algo bellman_ford step 1291 current loss 0.049465, current_train_items 41344.
I0304 19:28:37.384547 23118544486528 run.py:483] Algo bellman_ford step 1292 current loss 0.100408, current_train_items 41376.
I0304 19:28:37.414140 23118544486528 run.py:483] Algo bellman_ford step 1293 current loss 0.106856, current_train_items 41408.
I0304 19:28:37.447736 23118544486528 run.py:483] Algo bellman_ford step 1294 current loss 0.168868, current_train_items 41440.
I0304 19:28:37.466860 23118544486528 run.py:483] Algo bellman_ford step 1295 current loss 0.033909, current_train_items 41472.
I0304 19:28:37.483522 23118544486528 run.py:483] Algo bellman_ford step 1296 current loss 0.032475, current_train_items 41504.
I0304 19:28:37.508178 23118544486528 run.py:483] Algo bellman_ford step 1297 current loss 0.079290, current_train_items 41536.
I0304 19:28:37.536487 23118544486528 run.py:483] Algo bellman_ford step 1298 current loss 0.084159, current_train_items 41568.
I0304 19:28:37.567035 23118544486528 run.py:483] Algo bellman_ford step 1299 current loss 0.127691, current_train_items 41600.
I0304 19:28:37.586087 23118544486528 run.py:483] Algo bellman_ford step 1300 current loss 0.012866, current_train_items 41632.
I0304 19:28:37.593769 23118544486528 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0304 19:28:37.593873 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:37.611110 23118544486528 run.py:483] Algo bellman_ford step 1301 current loss 0.063220, current_train_items 41664.
I0304 19:28:37.636089 23118544486528 run.py:483] Algo bellman_ford step 1302 current loss 0.111622, current_train_items 41696.
I0304 19:28:37.667231 23118544486528 run.py:483] Algo bellman_ford step 1303 current loss 0.133758, current_train_items 41728.
I0304 19:28:37.698805 23118544486528 run.py:483] Algo bellman_ford step 1304 current loss 0.119796, current_train_items 41760.
I0304 19:28:37.718208 23118544486528 run.py:483] Algo bellman_ford step 1305 current loss 0.009118, current_train_items 41792.
I0304 19:28:37.734267 23118544486528 run.py:483] Algo bellman_ford step 1306 current loss 0.055721, current_train_items 41824.
I0304 19:28:37.757466 23118544486528 run.py:483] Algo bellman_ford step 1307 current loss 0.104137, current_train_items 41856.
I0304 19:28:37.786584 23118544486528 run.py:483] Algo bellman_ford step 1308 current loss 0.103925, current_train_items 41888.
I0304 19:28:37.818875 23118544486528 run.py:483] Algo bellman_ford step 1309 current loss 0.176142, current_train_items 41920.
I0304 19:28:37.838657 23118544486528 run.py:483] Algo bellman_ford step 1310 current loss 0.013018, current_train_items 41952.
I0304 19:28:37.855111 23118544486528 run.py:483] Algo bellman_ford step 1311 current loss 0.081542, current_train_items 41984.
I0304 19:28:37.878259 23118544486528 run.py:483] Algo bellman_ford step 1312 current loss 0.085897, current_train_items 42016.
I0304 19:28:37.909041 23118544486528 run.py:483] Algo bellman_ford step 1313 current loss 0.169516, current_train_items 42048.
I0304 19:28:37.942328 23118544486528 run.py:483] Algo bellman_ford step 1314 current loss 0.117015, current_train_items 42080.
I0304 19:28:37.961503 23118544486528 run.py:483] Algo bellman_ford step 1315 current loss 0.007328, current_train_items 42112.
I0304 19:28:37.977602 23118544486528 run.py:483] Algo bellman_ford step 1316 current loss 0.027439, current_train_items 42144.
I0304 19:28:38.001388 23118544486528 run.py:483] Algo bellman_ford step 1317 current loss 0.154456, current_train_items 42176.
I0304 19:28:38.031186 23118544486528 run.py:483] Algo bellman_ford step 1318 current loss 0.103019, current_train_items 42208.
I0304 19:28:38.063516 23118544486528 run.py:483] Algo bellman_ford step 1319 current loss 0.252252, current_train_items 42240.
I0304 19:28:38.082666 23118544486528 run.py:483] Algo bellman_ford step 1320 current loss 0.017685, current_train_items 42272.
I0304 19:28:38.099100 23118544486528 run.py:483] Algo bellman_ford step 1321 current loss 0.050076, current_train_items 42304.
I0304 19:28:38.123229 23118544486528 run.py:483] Algo bellman_ford step 1322 current loss 0.111204, current_train_items 42336.
I0304 19:28:38.153454 23118544486528 run.py:483] Algo bellman_ford step 1323 current loss 0.118410, current_train_items 42368.
I0304 19:28:38.185408 23118544486528 run.py:483] Algo bellman_ford step 1324 current loss 0.179507, current_train_items 42400.
I0304 19:28:38.205064 23118544486528 run.py:483] Algo bellman_ford step 1325 current loss 0.048055, current_train_items 42432.
I0304 19:28:38.221181 23118544486528 run.py:483] Algo bellman_ford step 1326 current loss 0.046722, current_train_items 42464.
I0304 19:28:38.244073 23118544486528 run.py:483] Algo bellman_ford step 1327 current loss 0.094603, current_train_items 42496.
I0304 19:28:38.273990 23118544486528 run.py:483] Algo bellman_ford step 1328 current loss 0.134403, current_train_items 42528.
I0304 19:28:38.306892 23118544486528 run.py:483] Algo bellman_ford step 1329 current loss 0.130320, current_train_items 42560.
I0304 19:28:38.325978 23118544486528 run.py:483] Algo bellman_ford step 1330 current loss 0.018710, current_train_items 42592.
I0304 19:28:38.342297 23118544486528 run.py:483] Algo bellman_ford step 1331 current loss 0.092313, current_train_items 42624.
I0304 19:28:38.365409 23118544486528 run.py:483] Algo bellman_ford step 1332 current loss 0.140585, current_train_items 42656.
I0304 19:28:38.394710 23118544486528 run.py:483] Algo bellman_ford step 1333 current loss 0.201725, current_train_items 42688.
I0304 19:28:38.426424 23118544486528 run.py:483] Algo bellman_ford step 1334 current loss 0.149349, current_train_items 42720.
I0304 19:28:38.445970 23118544486528 run.py:483] Algo bellman_ford step 1335 current loss 0.016782, current_train_items 42752.
I0304 19:28:38.462211 23118544486528 run.py:483] Algo bellman_ford step 1336 current loss 0.058685, current_train_items 42784.
I0304 19:28:38.487077 23118544486528 run.py:483] Algo bellman_ford step 1337 current loss 0.124465, current_train_items 42816.
I0304 19:28:38.517287 23118544486528 run.py:483] Algo bellman_ford step 1338 current loss 0.226567, current_train_items 42848.
I0304 19:28:38.552158 23118544486528 run.py:483] Algo bellman_ford step 1339 current loss 0.156049, current_train_items 42880.
I0304 19:28:38.571421 23118544486528 run.py:483] Algo bellman_ford step 1340 current loss 0.017553, current_train_items 42912.
I0304 19:28:38.587820 23118544486528 run.py:483] Algo bellman_ford step 1341 current loss 0.054560, current_train_items 42944.
I0304 19:28:38.611676 23118544486528 run.py:483] Algo bellman_ford step 1342 current loss 0.212228, current_train_items 42976.
I0304 19:28:38.641603 23118544486528 run.py:483] Algo bellman_ford step 1343 current loss 0.272414, current_train_items 43008.
I0304 19:28:38.674414 23118544486528 run.py:483] Algo bellman_ford step 1344 current loss 0.334186, current_train_items 43040.
I0304 19:28:38.694102 23118544486528 run.py:483] Algo bellman_ford step 1345 current loss 0.022312, current_train_items 43072.
I0304 19:28:38.710330 23118544486528 run.py:483] Algo bellman_ford step 1346 current loss 0.065814, current_train_items 43104.
I0304 19:28:38.734977 23118544486528 run.py:483] Algo bellman_ford step 1347 current loss 0.177301, current_train_items 43136.
I0304 19:28:38.763459 23118544486528 run.py:483] Algo bellman_ford step 1348 current loss 0.139430, current_train_items 43168.
I0304 19:28:38.794884 23118544486528 run.py:483] Algo bellman_ford step 1349 current loss 0.139711, current_train_items 43200.
I0304 19:28:38.814373 23118544486528 run.py:483] Algo bellman_ford step 1350 current loss 0.041941, current_train_items 43232.
I0304 19:28:38.822430 23118544486528 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.953125, 'score': 0.953125, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0304 19:28:38.822536 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.953, val scores are: bellman_ford: 0.953
I0304 19:28:38.839557 23118544486528 run.py:483] Algo bellman_ford step 1351 current loss 0.042037, current_train_items 43264.
I0304 19:28:38.864731 23118544486528 run.py:483] Algo bellman_ford step 1352 current loss 0.075067, current_train_items 43296.
I0304 19:28:38.893902 23118544486528 run.py:483] Algo bellman_ford step 1353 current loss 0.138307, current_train_items 43328.
I0304 19:28:38.928351 23118544486528 run.py:483] Algo bellman_ford step 1354 current loss 0.181134, current_train_items 43360.
I0304 19:28:38.947663 23118544486528 run.py:483] Algo bellman_ford step 1355 current loss 0.012015, current_train_items 43392.
I0304 19:28:38.963545 23118544486528 run.py:483] Algo bellman_ford step 1356 current loss 0.092213, current_train_items 43424.
I0304 19:28:38.986628 23118544486528 run.py:483] Algo bellman_ford step 1357 current loss 0.075149, current_train_items 43456.
I0304 19:28:39.016425 23118544486528 run.py:483] Algo bellman_ford step 1358 current loss 0.119843, current_train_items 43488.
I0304 19:28:39.050309 23118544486528 run.py:483] Algo bellman_ford step 1359 current loss 0.204789, current_train_items 43520.
I0304 19:28:39.070076 23118544486528 run.py:483] Algo bellman_ford step 1360 current loss 0.017177, current_train_items 43552.
I0304 19:28:39.087180 23118544486528 run.py:483] Algo bellman_ford step 1361 current loss 0.095340, current_train_items 43584.
I0304 19:28:39.111247 23118544486528 run.py:483] Algo bellman_ford step 1362 current loss 0.095360, current_train_items 43616.
I0304 19:28:39.141165 23118544486528 run.py:483] Algo bellman_ford step 1363 current loss 0.126192, current_train_items 43648.
I0304 19:28:39.175097 23118544486528 run.py:483] Algo bellman_ford step 1364 current loss 0.148083, current_train_items 43680.
I0304 19:28:39.194357 23118544486528 run.py:483] Algo bellman_ford step 1365 current loss 0.013704, current_train_items 43712.
I0304 19:28:39.210383 23118544486528 run.py:483] Algo bellman_ford step 1366 current loss 0.058381, current_train_items 43744.
I0304 19:28:39.233724 23118544486528 run.py:483] Algo bellman_ford step 1367 current loss 0.077534, current_train_items 43776.
I0304 19:28:39.262083 23118544486528 run.py:483] Algo bellman_ford step 1368 current loss 0.076789, current_train_items 43808.
I0304 19:28:39.294997 23118544486528 run.py:483] Algo bellman_ford step 1369 current loss 0.163656, current_train_items 43840.
I0304 19:28:39.314273 23118544486528 run.py:483] Algo bellman_ford step 1370 current loss 0.012983, current_train_items 43872.
I0304 19:28:39.330976 23118544486528 run.py:483] Algo bellman_ford step 1371 current loss 0.097353, current_train_items 43904.
I0304 19:28:39.355061 23118544486528 run.py:483] Algo bellman_ford step 1372 current loss 0.115871, current_train_items 43936.
I0304 19:28:39.386082 23118544486528 run.py:483] Algo bellman_ford step 1373 current loss 0.123665, current_train_items 43968.
I0304 19:28:39.419551 23118544486528 run.py:483] Algo bellman_ford step 1374 current loss 0.136821, current_train_items 44000.
I0304 19:28:39.438873 23118544486528 run.py:483] Algo bellman_ford step 1375 current loss 0.017206, current_train_items 44032.
I0304 19:28:39.454823 23118544486528 run.py:483] Algo bellman_ford step 1376 current loss 0.049307, current_train_items 44064.
I0304 19:28:39.478260 23118544486528 run.py:483] Algo bellman_ford step 1377 current loss 0.146428, current_train_items 44096.
I0304 19:28:39.508959 23118544486528 run.py:483] Algo bellman_ford step 1378 current loss 0.108809, current_train_items 44128.
I0304 19:28:39.541879 23118544486528 run.py:483] Algo bellman_ford step 1379 current loss 0.110146, current_train_items 44160.
I0304 19:28:39.561128 23118544486528 run.py:483] Algo bellman_ford step 1380 current loss 0.039769, current_train_items 44192.
I0304 19:28:39.577719 23118544486528 run.py:483] Algo bellman_ford step 1381 current loss 0.044488, current_train_items 44224.
I0304 19:28:39.601100 23118544486528 run.py:483] Algo bellman_ford step 1382 current loss 0.086598, current_train_items 44256.
I0304 19:28:39.631278 23118544486528 run.py:483] Algo bellman_ford step 1383 current loss 0.136126, current_train_items 44288.
I0304 19:28:39.661777 23118544486528 run.py:483] Algo bellman_ford step 1384 current loss 0.141645, current_train_items 44320.
I0304 19:28:39.681326 23118544486528 run.py:483] Algo bellman_ford step 1385 current loss 0.010635, current_train_items 44352.
I0304 19:28:39.697678 23118544486528 run.py:483] Algo bellman_ford step 1386 current loss 0.022500, current_train_items 44384.
I0304 19:28:39.720374 23118544486528 run.py:483] Algo bellman_ford step 1387 current loss 0.070766, current_train_items 44416.
I0304 19:28:39.750535 23118544486528 run.py:483] Algo bellman_ford step 1388 current loss 0.150824, current_train_items 44448.
I0304 19:28:39.783220 23118544486528 run.py:483] Algo bellman_ford step 1389 current loss 0.162530, current_train_items 44480.
I0304 19:28:39.802922 23118544486528 run.py:483] Algo bellman_ford step 1390 current loss 0.012944, current_train_items 44512.
I0304 19:28:39.818949 23118544486528 run.py:483] Algo bellman_ford step 1391 current loss 0.051617, current_train_items 44544.
I0304 19:28:39.843206 23118544486528 run.py:483] Algo bellman_ford step 1392 current loss 0.099151, current_train_items 44576.
I0304 19:28:39.872962 23118544486528 run.py:483] Algo bellman_ford step 1393 current loss 0.144570, current_train_items 44608.
I0304 19:28:39.905499 23118544486528 run.py:483] Algo bellman_ford step 1394 current loss 0.162212, current_train_items 44640.
I0304 19:28:39.924563 23118544486528 run.py:483] Algo bellman_ford step 1395 current loss 0.008498, current_train_items 44672.
I0304 19:28:39.940635 23118544486528 run.py:483] Algo bellman_ford step 1396 current loss 0.039823, current_train_items 44704.
I0304 19:28:39.964342 23118544486528 run.py:483] Algo bellman_ford step 1397 current loss 0.129358, current_train_items 44736.
I0304 19:28:39.995199 23118544486528 run.py:483] Algo bellman_ford step 1398 current loss 0.135120, current_train_items 44768.
I0304 19:28:40.026900 23118544486528 run.py:483] Algo bellman_ford step 1399 current loss 0.128731, current_train_items 44800.
I0304 19:28:40.046756 23118544486528 run.py:483] Algo bellman_ford step 1400 current loss 0.021494, current_train_items 44832.
I0304 19:28:40.054535 23118544486528 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0304 19:28:40.054642 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:40.071195 23118544486528 run.py:483] Algo bellman_ford step 1401 current loss 0.080495, current_train_items 44864.
I0304 19:28:40.095898 23118544486528 run.py:483] Algo bellman_ford step 1402 current loss 0.151494, current_train_items 44896.
I0304 19:28:40.126976 23118544486528 run.py:483] Algo bellman_ford step 1403 current loss 0.169257, current_train_items 44928.
I0304 19:28:40.160706 23118544486528 run.py:483] Algo bellman_ford step 1404 current loss 0.154545, current_train_items 44960.
I0304 19:28:40.179918 23118544486528 run.py:483] Algo bellman_ford step 1405 current loss 0.011351, current_train_items 44992.
I0304 19:28:40.196376 23118544486528 run.py:483] Algo bellman_ford step 1406 current loss 0.074594, current_train_items 45024.
I0304 19:28:40.220026 23118544486528 run.py:483] Algo bellman_ford step 1407 current loss 0.087332, current_train_items 45056.
I0304 19:28:40.250238 23118544486528 run.py:483] Algo bellman_ford step 1408 current loss 0.143844, current_train_items 45088.
I0304 19:28:40.282828 23118544486528 run.py:483] Algo bellman_ford step 1409 current loss 0.218070, current_train_items 45120.
I0304 19:28:40.302031 23118544486528 run.py:483] Algo bellman_ford step 1410 current loss 0.013931, current_train_items 45152.
I0304 19:28:40.318527 23118544486528 run.py:483] Algo bellman_ford step 1411 current loss 0.043110, current_train_items 45184.
I0304 19:28:40.342249 23118544486528 run.py:483] Algo bellman_ford step 1412 current loss 0.069044, current_train_items 45216.
I0304 19:28:40.370158 23118544486528 run.py:483] Algo bellman_ford step 1413 current loss 0.086770, current_train_items 45248.
I0304 19:28:40.402186 23118544486528 run.py:483] Algo bellman_ford step 1414 current loss 0.120289, current_train_items 45280.
I0304 19:28:40.421169 23118544486528 run.py:483] Algo bellman_ford step 1415 current loss 0.023583, current_train_items 45312.
I0304 19:28:40.437782 23118544486528 run.py:483] Algo bellman_ford step 1416 current loss 0.055797, current_train_items 45344.
I0304 19:28:40.460769 23118544486528 run.py:483] Algo bellman_ford step 1417 current loss 0.042450, current_train_items 45376.
I0304 19:28:40.489588 23118544486528 run.py:483] Algo bellman_ford step 1418 current loss 0.137399, current_train_items 45408.
I0304 19:28:40.522758 23118544486528 run.py:483] Algo bellman_ford step 1419 current loss 0.157239, current_train_items 45440.
I0304 19:28:40.541937 23118544486528 run.py:483] Algo bellman_ford step 1420 current loss 0.044387, current_train_items 45472.
I0304 19:28:40.558235 23118544486528 run.py:483] Algo bellman_ford step 1421 current loss 0.047116, current_train_items 45504.
I0304 19:28:40.582407 23118544486528 run.py:483] Algo bellman_ford step 1422 current loss 0.098774, current_train_items 45536.
I0304 19:28:40.613440 23118544486528 run.py:483] Algo bellman_ford step 1423 current loss 0.113444, current_train_items 45568.
I0304 19:28:40.644990 23118544486528 run.py:483] Algo bellman_ford step 1424 current loss 0.102838, current_train_items 45600.
I0304 19:28:40.663875 23118544486528 run.py:483] Algo bellman_ford step 1425 current loss 0.014032, current_train_items 45632.
I0304 19:28:40.680664 23118544486528 run.py:483] Algo bellman_ford step 1426 current loss 0.161214, current_train_items 45664.
I0304 19:28:40.705107 23118544486528 run.py:483] Algo bellman_ford step 1427 current loss 0.104748, current_train_items 45696.
I0304 19:28:40.735588 23118544486528 run.py:483] Algo bellman_ford step 1428 current loss 0.087933, current_train_items 45728.
I0304 19:28:40.766425 23118544486528 run.py:483] Algo bellman_ford step 1429 current loss 0.114920, current_train_items 45760.
I0304 19:28:40.785331 23118544486528 run.py:483] Algo bellman_ford step 1430 current loss 0.014105, current_train_items 45792.
I0304 19:28:40.801411 23118544486528 run.py:483] Algo bellman_ford step 1431 current loss 0.036956, current_train_items 45824.
I0304 19:28:40.826442 23118544486528 run.py:483] Algo bellman_ford step 1432 current loss 0.161356, current_train_items 45856.
I0304 19:28:40.855865 23118544486528 run.py:483] Algo bellman_ford step 1433 current loss 0.100296, current_train_items 45888.
I0304 19:28:40.886858 23118544486528 run.py:483] Algo bellman_ford step 1434 current loss 0.108062, current_train_items 45920.
I0304 19:28:40.905835 23118544486528 run.py:483] Algo bellman_ford step 1435 current loss 0.027558, current_train_items 45952.
I0304 19:28:40.922029 23118544486528 run.py:483] Algo bellman_ford step 1436 current loss 0.064966, current_train_items 45984.
I0304 19:28:40.945393 23118544486528 run.py:483] Algo bellman_ford step 1437 current loss 0.124372, current_train_items 46016.
I0304 19:28:40.974452 23118544486528 run.py:483] Algo bellman_ford step 1438 current loss 0.161249, current_train_items 46048.
I0304 19:28:41.008239 23118544486528 run.py:483] Algo bellman_ford step 1439 current loss 0.188924, current_train_items 46080.
I0304 19:28:41.027541 23118544486528 run.py:483] Algo bellman_ford step 1440 current loss 0.010017, current_train_items 46112.
I0304 19:28:41.043901 23118544486528 run.py:483] Algo bellman_ford step 1441 current loss 0.053099, current_train_items 46144.
I0304 19:28:41.067217 23118544486528 run.py:483] Algo bellman_ford step 1442 current loss 0.064014, current_train_items 46176.
I0304 19:28:41.096877 23118544486528 run.py:483] Algo bellman_ford step 1443 current loss 0.151996, current_train_items 46208.
I0304 19:28:41.133150 23118544486528 run.py:483] Algo bellman_ford step 1444 current loss 0.176059, current_train_items 46240.
I0304 19:28:41.152619 23118544486528 run.py:483] Algo bellman_ford step 1445 current loss 0.011456, current_train_items 46272.
I0304 19:28:41.168347 23118544486528 run.py:483] Algo bellman_ford step 1446 current loss 0.105524, current_train_items 46304.
I0304 19:28:41.192322 23118544486528 run.py:483] Algo bellman_ford step 1447 current loss 0.108059, current_train_items 46336.
I0304 19:28:41.221945 23118544486528 run.py:483] Algo bellman_ford step 1448 current loss 0.267648, current_train_items 46368.
I0304 19:28:41.253631 23118544486528 run.py:483] Algo bellman_ford step 1449 current loss 0.141701, current_train_items 46400.
I0304 19:28:41.273059 23118544486528 run.py:483] Algo bellman_ford step 1450 current loss 0.008777, current_train_items 46432.
I0304 19:28:41.281401 23118544486528 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0304 19:28:41.281510 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:28:41.297946 23118544486528 run.py:483] Algo bellman_ford step 1451 current loss 0.059486, current_train_items 46464.
I0304 19:28:41.321577 23118544486528 run.py:483] Algo bellman_ford step 1452 current loss 0.061897, current_train_items 46496.
I0304 19:28:41.353233 23118544486528 run.py:483] Algo bellman_ford step 1453 current loss 0.129769, current_train_items 46528.
I0304 19:28:41.387526 23118544486528 run.py:483] Algo bellman_ford step 1454 current loss 0.154205, current_train_items 46560.
I0304 19:28:41.407289 23118544486528 run.py:483] Algo bellman_ford step 1455 current loss 0.017979, current_train_items 46592.
I0304 19:28:41.423511 23118544486528 run.py:483] Algo bellman_ford step 1456 current loss 0.072305, current_train_items 46624.
I0304 19:28:41.446336 23118544486528 run.py:483] Algo bellman_ford step 1457 current loss 0.135256, current_train_items 46656.
I0304 19:28:41.476832 23118544486528 run.py:483] Algo bellman_ford step 1458 current loss 0.265184, current_train_items 46688.
I0304 19:28:41.509102 23118544486528 run.py:483] Algo bellman_ford step 1459 current loss 0.168238, current_train_items 46720.
I0304 19:28:41.528675 23118544486528 run.py:483] Algo bellman_ford step 1460 current loss 0.020843, current_train_items 46752.
I0304 19:28:41.544749 23118544486528 run.py:483] Algo bellman_ford step 1461 current loss 0.072705, current_train_items 46784.
I0304 19:28:41.567484 23118544486528 run.py:483] Algo bellman_ford step 1462 current loss 0.187791, current_train_items 46816.
I0304 19:28:41.597159 23118544486528 run.py:483] Algo bellman_ford step 1463 current loss 0.243570, current_train_items 46848.
I0304 19:28:41.628643 23118544486528 run.py:483] Algo bellman_ford step 1464 current loss 0.184597, current_train_items 46880.
I0304 19:28:41.648140 23118544486528 run.py:483] Algo bellman_ford step 1465 current loss 0.021384, current_train_items 46912.
I0304 19:28:41.664710 23118544486528 run.py:483] Algo bellman_ford step 1466 current loss 0.073121, current_train_items 46944.
I0304 19:28:41.688324 23118544486528 run.py:483] Algo bellman_ford step 1467 current loss 0.110606, current_train_items 46976.
I0304 19:28:41.716533 23118544486528 run.py:483] Algo bellman_ford step 1468 current loss 0.132410, current_train_items 47008.
I0304 19:28:41.747671 23118544486528 run.py:483] Algo bellman_ford step 1469 current loss 0.150246, current_train_items 47040.
I0304 19:28:41.767277 23118544486528 run.py:483] Algo bellman_ford step 1470 current loss 0.011219, current_train_items 47072.
I0304 19:28:41.784308 23118544486528 run.py:483] Algo bellman_ford step 1471 current loss 0.073350, current_train_items 47104.
I0304 19:28:41.807422 23118544486528 run.py:483] Algo bellman_ford step 1472 current loss 0.064286, current_train_items 47136.
I0304 19:28:41.837643 23118544486528 run.py:483] Algo bellman_ford step 1473 current loss 0.185649, current_train_items 47168.
I0304 19:28:41.870899 23118544486528 run.py:483] Algo bellman_ford step 1474 current loss 0.179159, current_train_items 47200.
I0304 19:28:41.890484 23118544486528 run.py:483] Algo bellman_ford step 1475 current loss 0.019047, current_train_items 47232.
I0304 19:28:41.906728 23118544486528 run.py:483] Algo bellman_ford step 1476 current loss 0.021355, current_train_items 47264.
I0304 19:28:41.930019 23118544486528 run.py:483] Algo bellman_ford step 1477 current loss 0.109523, current_train_items 47296.
I0304 19:28:41.959650 23118544486528 run.py:483] Algo bellman_ford step 1478 current loss 0.264580, current_train_items 47328.
I0304 19:28:41.993643 23118544486528 run.py:483] Algo bellman_ford step 1479 current loss 0.208774, current_train_items 47360.
I0304 19:28:42.012908 23118544486528 run.py:483] Algo bellman_ford step 1480 current loss 0.008482, current_train_items 47392.
I0304 19:28:42.029269 23118544486528 run.py:483] Algo bellman_ford step 1481 current loss 0.031656, current_train_items 47424.
I0304 19:28:42.052389 23118544486528 run.py:483] Algo bellman_ford step 1482 current loss 0.132200, current_train_items 47456.
I0304 19:28:42.082238 23118544486528 run.py:483] Algo bellman_ford step 1483 current loss 0.201617, current_train_items 47488.
I0304 19:28:42.113041 23118544486528 run.py:483] Algo bellman_ford step 1484 current loss 0.121687, current_train_items 47520.
I0304 19:28:42.132876 23118544486528 run.py:483] Algo bellman_ford step 1485 current loss 0.024124, current_train_items 47552.
I0304 19:28:42.148949 23118544486528 run.py:483] Algo bellman_ford step 1486 current loss 0.057578, current_train_items 47584.
I0304 19:28:42.173284 23118544486528 run.py:483] Algo bellman_ford step 1487 current loss 0.159998, current_train_items 47616.
I0304 19:28:42.202090 23118544486528 run.py:483] Algo bellman_ford step 1488 current loss 0.206775, current_train_items 47648.
I0304 19:28:42.235774 23118544486528 run.py:483] Algo bellman_ford step 1489 current loss 0.277727, current_train_items 47680.
I0304 19:28:42.255184 23118544486528 run.py:483] Algo bellman_ford step 1490 current loss 0.008625, current_train_items 47712.
I0304 19:28:42.271567 23118544486528 run.py:483] Algo bellman_ford step 1491 current loss 0.053869, current_train_items 47744.
I0304 19:28:42.295115 23118544486528 run.py:483] Algo bellman_ford step 1492 current loss 0.047793, current_train_items 47776.
I0304 19:28:42.324662 23118544486528 run.py:483] Algo bellman_ford step 1493 current loss 0.134073, current_train_items 47808.
I0304 19:28:42.358227 23118544486528 run.py:483] Algo bellman_ford step 1494 current loss 0.170357, current_train_items 47840.
I0304 19:28:42.377277 23118544486528 run.py:483] Algo bellman_ford step 1495 current loss 0.010853, current_train_items 47872.
I0304 19:28:42.393828 23118544486528 run.py:483] Algo bellman_ford step 1496 current loss 0.076072, current_train_items 47904.
I0304 19:28:42.418090 23118544486528 run.py:483] Algo bellman_ford step 1497 current loss 0.105172, current_train_items 47936.
I0304 19:28:42.448498 23118544486528 run.py:483] Algo bellman_ford step 1498 current loss 0.164005, current_train_items 47968.
I0304 19:28:42.483271 23118544486528 run.py:483] Algo bellman_ford step 1499 current loss 0.126397, current_train_items 48000.
I0304 19:28:42.502555 23118544486528 run.py:483] Algo bellman_ford step 1500 current loss 0.008301, current_train_items 48032.
I0304 19:28:42.510675 23118544486528 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0304 19:28:42.510791 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:28:42.527824 23118544486528 run.py:483] Algo bellman_ford step 1501 current loss 0.080389, current_train_items 48064.
I0304 19:28:42.552506 23118544486528 run.py:483] Algo bellman_ford step 1502 current loss 0.160099, current_train_items 48096.
I0304 19:28:42.582797 23118544486528 run.py:483] Algo bellman_ford step 1503 current loss 0.115417, current_train_items 48128.
I0304 19:28:42.615471 23118544486528 run.py:483] Algo bellman_ford step 1504 current loss 0.114308, current_train_items 48160.
I0304 19:28:42.634689 23118544486528 run.py:483] Algo bellman_ford step 1505 current loss 0.009916, current_train_items 48192.
I0304 19:28:42.650583 23118544486528 run.py:483] Algo bellman_ford step 1506 current loss 0.057866, current_train_items 48224.
I0304 19:28:42.673748 23118544486528 run.py:483] Algo bellman_ford step 1507 current loss 0.075652, current_train_items 48256.
I0304 19:28:42.703535 23118544486528 run.py:483] Algo bellman_ford step 1508 current loss 0.124179, current_train_items 48288.
I0304 19:28:42.736300 23118544486528 run.py:483] Algo bellman_ford step 1509 current loss 0.100426, current_train_items 48320.
I0304 19:28:42.755624 23118544486528 run.py:483] Algo bellman_ford step 1510 current loss 0.031293, current_train_items 48352.
I0304 19:28:42.772087 23118544486528 run.py:483] Algo bellman_ford step 1511 current loss 0.060050, current_train_items 48384.
I0304 19:28:42.795960 23118544486528 run.py:483] Algo bellman_ford step 1512 current loss 0.124662, current_train_items 48416.
I0304 19:28:42.824660 23118544486528 run.py:483] Algo bellman_ford step 1513 current loss 0.078225, current_train_items 48448.
I0304 19:28:42.858700 23118544486528 run.py:483] Algo bellman_ford step 1514 current loss 0.219601, current_train_items 48480.
I0304 19:28:42.878055 23118544486528 run.py:483] Algo bellman_ford step 1515 current loss 0.008886, current_train_items 48512.
I0304 19:28:42.894184 23118544486528 run.py:483] Algo bellman_ford step 1516 current loss 0.029994, current_train_items 48544.
I0304 19:28:42.917899 23118544486528 run.py:483] Algo bellman_ford step 1517 current loss 0.127106, current_train_items 48576.
I0304 19:28:42.948413 23118544486528 run.py:483] Algo bellman_ford step 1518 current loss 0.119390, current_train_items 48608.
I0304 19:28:42.980650 23118544486528 run.py:483] Algo bellman_ford step 1519 current loss 0.246744, current_train_items 48640.
I0304 19:28:42.999926 23118544486528 run.py:483] Algo bellman_ford step 1520 current loss 0.010278, current_train_items 48672.
I0304 19:28:43.015976 23118544486528 run.py:483] Algo bellman_ford step 1521 current loss 0.080519, current_train_items 48704.
I0304 19:28:43.040277 23118544486528 run.py:483] Algo bellman_ford step 1522 current loss 0.160572, current_train_items 48736.
I0304 19:28:43.070158 23118544486528 run.py:483] Algo bellman_ford step 1523 current loss 0.145140, current_train_items 48768.
I0304 19:28:43.102607 23118544486528 run.py:483] Algo bellman_ford step 1524 current loss 0.185361, current_train_items 48800.
I0304 19:28:43.122122 23118544486528 run.py:483] Algo bellman_ford step 1525 current loss 0.013033, current_train_items 48832.
I0304 19:28:43.138267 23118544486528 run.py:483] Algo bellman_ford step 1526 current loss 0.050600, current_train_items 48864.
I0304 19:28:43.162751 23118544486528 run.py:483] Algo bellman_ford step 1527 current loss 0.178744, current_train_items 48896.
I0304 19:28:43.192991 23118544486528 run.py:483] Algo bellman_ford step 1528 current loss 0.156829, current_train_items 48928.
I0304 19:28:43.226121 23118544486528 run.py:483] Algo bellman_ford step 1529 current loss 0.143894, current_train_items 48960.
I0304 19:28:43.245504 23118544486528 run.py:483] Algo bellman_ford step 1530 current loss 0.016741, current_train_items 48992.
I0304 19:28:43.261579 23118544486528 run.py:483] Algo bellman_ford step 1531 current loss 0.073027, current_train_items 49024.
I0304 19:28:43.285044 23118544486528 run.py:483] Algo bellman_ford step 1532 current loss 0.101230, current_train_items 49056.
I0304 19:28:43.315073 23118544486528 run.py:483] Algo bellman_ford step 1533 current loss 0.386187, current_train_items 49088.
I0304 19:28:43.348879 23118544486528 run.py:483] Algo bellman_ford step 1534 current loss 0.339392, current_train_items 49120.
I0304 19:28:43.368257 23118544486528 run.py:483] Algo bellman_ford step 1535 current loss 0.032890, current_train_items 49152.
I0304 19:28:43.384903 23118544486528 run.py:483] Algo bellman_ford step 1536 current loss 0.036427, current_train_items 49184.
I0304 19:28:43.408655 23118544486528 run.py:483] Algo bellman_ford step 1537 current loss 0.098600, current_train_items 49216.
I0304 19:28:43.438984 23118544486528 run.py:483] Algo bellman_ford step 1538 current loss 0.151058, current_train_items 49248.
I0304 19:28:43.474476 23118544486528 run.py:483] Algo bellman_ford step 1539 current loss 0.213901, current_train_items 49280.
I0304 19:28:43.494031 23118544486528 run.py:483] Algo bellman_ford step 1540 current loss 0.015692, current_train_items 49312.
I0304 19:28:43.510186 23118544486528 run.py:483] Algo bellman_ford step 1541 current loss 0.053307, current_train_items 49344.
I0304 19:28:43.533849 23118544486528 run.py:483] Algo bellman_ford step 1542 current loss 0.100564, current_train_items 49376.
I0304 19:28:43.563373 23118544486528 run.py:483] Algo bellman_ford step 1543 current loss 0.106852, current_train_items 49408.
I0304 19:28:43.596158 23118544486528 run.py:483] Algo bellman_ford step 1544 current loss 0.160746, current_train_items 49440.
I0304 19:28:43.615524 23118544486528 run.py:483] Algo bellman_ford step 1545 current loss 0.025761, current_train_items 49472.
I0304 19:28:43.631715 23118544486528 run.py:483] Algo bellman_ford step 1546 current loss 0.038586, current_train_items 49504.
I0304 19:28:43.655332 23118544486528 run.py:483] Algo bellman_ford step 1547 current loss 0.123315, current_train_items 49536.
I0304 19:28:43.686200 23118544486528 run.py:483] Algo bellman_ford step 1548 current loss 0.122743, current_train_items 49568.
I0304 19:28:43.720365 23118544486528 run.py:483] Algo bellman_ford step 1549 current loss 0.229671, current_train_items 49600.
I0304 19:28:43.739960 23118544486528 run.py:483] Algo bellman_ford step 1550 current loss 0.020494, current_train_items 49632.
I0304 19:28:43.748018 23118544486528 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0304 19:28:43.748122 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:28:43.765550 23118544486528 run.py:483] Algo bellman_ford step 1551 current loss 0.084038, current_train_items 49664.
I0304 19:28:43.788966 23118544486528 run.py:483] Algo bellman_ford step 1552 current loss 0.070162, current_train_items 49696.
I0304 19:28:43.819025 23118544486528 run.py:483] Algo bellman_ford step 1553 current loss 0.114798, current_train_items 49728.
I0304 19:28:43.851016 23118544486528 run.py:483] Algo bellman_ford step 1554 current loss 0.118931, current_train_items 49760.
I0304 19:28:43.870276 23118544486528 run.py:483] Algo bellman_ford step 1555 current loss 0.006675, current_train_items 49792.
I0304 19:28:43.886271 23118544486528 run.py:483] Algo bellman_ford step 1556 current loss 0.058619, current_train_items 49824.
I0304 19:28:43.910565 23118544486528 run.py:483] Algo bellman_ford step 1557 current loss 0.161780, current_train_items 49856.
I0304 19:28:43.940734 23118544486528 run.py:483] Algo bellman_ford step 1558 current loss 0.115635, current_train_items 49888.
I0304 19:28:43.973727 23118544486528 run.py:483] Algo bellman_ford step 1559 current loss 0.138810, current_train_items 49920.
I0304 19:28:43.993131 23118544486528 run.py:483] Algo bellman_ford step 1560 current loss 0.014409, current_train_items 49952.
I0304 19:28:44.009564 23118544486528 run.py:483] Algo bellman_ford step 1561 current loss 0.041842, current_train_items 49984.
I0304 19:28:44.033010 23118544486528 run.py:483] Algo bellman_ford step 1562 current loss 0.092949, current_train_items 50016.
I0304 19:28:44.063838 23118544486528 run.py:483] Algo bellman_ford step 1563 current loss 0.085554, current_train_items 50048.
I0304 19:28:44.097923 23118544486528 run.py:483] Algo bellman_ford step 1564 current loss 0.132549, current_train_items 50080.
I0304 19:28:44.116966 23118544486528 run.py:483] Algo bellman_ford step 1565 current loss 0.011505, current_train_items 50112.
I0304 19:28:44.133413 23118544486528 run.py:483] Algo bellman_ford step 1566 current loss 0.079732, current_train_items 50144.
I0304 19:28:44.157411 23118544486528 run.py:483] Algo bellman_ford step 1567 current loss 0.139382, current_train_items 50176.
I0304 19:28:44.187265 23118544486528 run.py:483] Algo bellman_ford step 1568 current loss 0.111317, current_train_items 50208.
I0304 19:28:44.221471 23118544486528 run.py:483] Algo bellman_ford step 1569 current loss 0.136710, current_train_items 50240.
I0304 19:28:44.240635 23118544486528 run.py:483] Algo bellman_ford step 1570 current loss 0.014118, current_train_items 50272.
I0304 19:28:44.257285 23118544486528 run.py:483] Algo bellman_ford step 1571 current loss 0.030379, current_train_items 50304.
I0304 19:28:44.281919 23118544486528 run.py:483] Algo bellman_ford step 1572 current loss 0.155259, current_train_items 50336.
I0304 19:28:44.311664 23118544486528 run.py:483] Algo bellman_ford step 1573 current loss 0.110972, current_train_items 50368.
I0304 19:28:44.342395 23118544486528 run.py:483] Algo bellman_ford step 1574 current loss 0.129288, current_train_items 50400.
I0304 19:28:44.362247 23118544486528 run.py:483] Algo bellman_ford step 1575 current loss 0.024356, current_train_items 50432.
I0304 19:28:44.378713 23118544486528 run.py:483] Algo bellman_ford step 1576 current loss 0.060850, current_train_items 50464.
I0304 19:28:44.401427 23118544486528 run.py:483] Algo bellman_ford step 1577 current loss 0.099427, current_train_items 50496.
I0304 19:28:44.432905 23118544486528 run.py:483] Algo bellman_ford step 1578 current loss 0.260643, current_train_items 50528.
I0304 19:28:44.465709 23118544486528 run.py:483] Algo bellman_ford step 1579 current loss 0.083394, current_train_items 50560.
I0304 19:28:44.485081 23118544486528 run.py:483] Algo bellman_ford step 1580 current loss 0.077783, current_train_items 50592.
I0304 19:28:44.501161 23118544486528 run.py:483] Algo bellman_ford step 1581 current loss 0.126227, current_train_items 50624.
I0304 19:28:44.524651 23118544486528 run.py:483] Algo bellman_ford step 1582 current loss 0.178859, current_train_items 50656.
I0304 19:28:44.554810 23118544486528 run.py:483] Algo bellman_ford step 1583 current loss 0.167269, current_train_items 50688.
I0304 19:28:44.588211 23118544486528 run.py:483] Algo bellman_ford step 1584 current loss 0.250898, current_train_items 50720.
I0304 19:28:44.607699 23118544486528 run.py:483] Algo bellman_ford step 1585 current loss 0.008235, current_train_items 50752.
I0304 19:28:44.623726 23118544486528 run.py:483] Algo bellman_ford step 1586 current loss 0.106279, current_train_items 50784.
I0304 19:28:44.647633 23118544486528 run.py:483] Algo bellman_ford step 1587 current loss 0.189755, current_train_items 50816.
I0304 19:28:44.677197 23118544486528 run.py:483] Algo bellman_ford step 1588 current loss 0.155722, current_train_items 50848.
I0304 19:28:44.709554 23118544486528 run.py:483] Algo bellman_ford step 1589 current loss 0.158291, current_train_items 50880.
I0304 19:28:44.728986 23118544486528 run.py:483] Algo bellman_ford step 1590 current loss 0.049780, current_train_items 50912.
I0304 19:28:44.745551 23118544486528 run.py:483] Algo bellman_ford step 1591 current loss 0.027023, current_train_items 50944.
I0304 19:28:44.768242 23118544486528 run.py:483] Algo bellman_ford step 1592 current loss 0.107011, current_train_items 50976.
I0304 19:28:44.799373 23118544486528 run.py:483] Algo bellman_ford step 1593 current loss 0.189840, current_train_items 51008.
I0304 19:28:44.833233 23118544486528 run.py:483] Algo bellman_ford step 1594 current loss 0.133358, current_train_items 51040.
I0304 19:28:44.852758 23118544486528 run.py:483] Algo bellman_ford step 1595 current loss 0.013818, current_train_items 51072.
I0304 19:28:44.868702 23118544486528 run.py:483] Algo bellman_ford step 1596 current loss 0.074402, current_train_items 51104.
I0304 19:28:44.893023 23118544486528 run.py:483] Algo bellman_ford step 1597 current loss 0.158383, current_train_items 51136.
I0304 19:28:44.921616 23118544486528 run.py:483] Algo bellman_ford step 1598 current loss 0.104862, current_train_items 51168.
I0304 19:28:44.954633 23118544486528 run.py:483] Algo bellman_ford step 1599 current loss 0.140533, current_train_items 51200.
I0304 19:28:44.974220 23118544486528 run.py:483] Algo bellman_ford step 1600 current loss 0.018139, current_train_items 51232.
I0304 19:28:44.982031 23118544486528 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.958984375, 'score': 0.958984375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0304 19:28:44.982136 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.959, val scores are: bellman_ford: 0.959
I0304 19:28:44.999061 23118544486528 run.py:483] Algo bellman_ford step 1601 current loss 0.054076, current_train_items 51264.
I0304 19:28:45.023889 23118544486528 run.py:483] Algo bellman_ford step 1602 current loss 0.132972, current_train_items 51296.
I0304 19:28:45.054005 23118544486528 run.py:483] Algo bellman_ford step 1603 current loss 0.210882, current_train_items 51328.
I0304 19:28:45.087491 23118544486528 run.py:483] Algo bellman_ford step 1604 current loss 0.178868, current_train_items 51360.
I0304 19:28:45.107599 23118544486528 run.py:483] Algo bellman_ford step 1605 current loss 0.019622, current_train_items 51392.
I0304 19:28:45.123226 23118544486528 run.py:483] Algo bellman_ford step 1606 current loss 0.082373, current_train_items 51424.
I0304 19:28:45.146500 23118544486528 run.py:483] Algo bellman_ford step 1607 current loss 0.150731, current_train_items 51456.
I0304 19:28:45.177215 23118544486528 run.py:483] Algo bellman_ford step 1608 current loss 0.253175, current_train_items 51488.
I0304 19:28:45.210888 23118544486528 run.py:483] Algo bellman_ford step 1609 current loss 0.203060, current_train_items 51520.
I0304 19:28:45.229981 23118544486528 run.py:483] Algo bellman_ford step 1610 current loss 0.024480, current_train_items 51552.
I0304 19:28:45.246422 23118544486528 run.py:483] Algo bellman_ford step 1611 current loss 0.038791, current_train_items 51584.
I0304 19:28:45.269748 23118544486528 run.py:483] Algo bellman_ford step 1612 current loss 0.096732, current_train_items 51616.
I0304 19:28:45.300107 23118544486528 run.py:483] Algo bellman_ford step 1613 current loss 0.157835, current_train_items 51648.
I0304 19:28:45.331902 23118544486528 run.py:483] Algo bellman_ford step 1614 current loss 0.151639, current_train_items 51680.
I0304 19:28:45.350766 23118544486528 run.py:483] Algo bellman_ford step 1615 current loss 0.009498, current_train_items 51712.
I0304 19:28:45.367265 23118544486528 run.py:483] Algo bellman_ford step 1616 current loss 0.034889, current_train_items 51744.
I0304 19:28:45.391396 23118544486528 run.py:483] Algo bellman_ford step 1617 current loss 0.087514, current_train_items 51776.
I0304 19:28:45.420285 23118544486528 run.py:483] Algo bellman_ford step 1618 current loss 0.087132, current_train_items 51808.
I0304 19:28:45.454397 23118544486528 run.py:483] Algo bellman_ford step 1619 current loss 0.140395, current_train_items 51840.
I0304 19:28:45.473486 23118544486528 run.py:483] Algo bellman_ford step 1620 current loss 0.019996, current_train_items 51872.
I0304 19:28:45.489595 23118544486528 run.py:483] Algo bellman_ford step 1621 current loss 0.076268, current_train_items 51904.
I0304 19:28:45.513529 23118544486528 run.py:483] Algo bellman_ford step 1622 current loss 0.099791, current_train_items 51936.
I0304 19:28:45.543885 23118544486528 run.py:483] Algo bellman_ford step 1623 current loss 0.186643, current_train_items 51968.
I0304 19:28:45.576501 23118544486528 run.py:483] Algo bellman_ford step 1624 current loss 0.120714, current_train_items 52000.
I0304 19:28:45.595984 23118544486528 run.py:483] Algo bellman_ford step 1625 current loss 0.014443, current_train_items 52032.
I0304 19:28:45.612434 23118544486528 run.py:483] Algo bellman_ford step 1626 current loss 0.124314, current_train_items 52064.
I0304 19:28:45.635808 23118544486528 run.py:483] Algo bellman_ford step 1627 current loss 0.126756, current_train_items 52096.
I0304 19:28:45.666968 23118544486528 run.py:483] Algo bellman_ford step 1628 current loss 0.179771, current_train_items 52128.
I0304 19:28:45.698808 23118544486528 run.py:483] Algo bellman_ford step 1629 current loss 0.135594, current_train_items 52160.
I0304 19:28:45.718075 23118544486528 run.py:483] Algo bellman_ford step 1630 current loss 0.015377, current_train_items 52192.
I0304 19:28:45.735011 23118544486528 run.py:483] Algo bellman_ford step 1631 current loss 0.038732, current_train_items 52224.
I0304 19:28:45.759200 23118544486528 run.py:483] Algo bellman_ford step 1632 current loss 0.126601, current_train_items 52256.
I0304 19:28:45.788427 23118544486528 run.py:483] Algo bellman_ford step 1633 current loss 0.115882, current_train_items 52288.
I0304 19:28:45.821795 23118544486528 run.py:483] Algo bellman_ford step 1634 current loss 0.125258, current_train_items 52320.
I0304 19:28:45.841058 23118544486528 run.py:483] Algo bellman_ford step 1635 current loss 0.010774, current_train_items 52352.
I0304 19:28:45.857357 23118544486528 run.py:483] Algo bellman_ford step 1636 current loss 0.029864, current_train_items 52384.
I0304 19:28:45.881235 23118544486528 run.py:483] Algo bellman_ford step 1637 current loss 0.127229, current_train_items 52416.
I0304 19:28:45.911637 23118544486528 run.py:483] Algo bellman_ford step 1638 current loss 0.165028, current_train_items 52448.
I0304 19:28:45.946713 23118544486528 run.py:483] Algo bellman_ford step 1639 current loss 0.129682, current_train_items 52480.
I0304 19:28:45.965538 23118544486528 run.py:483] Algo bellman_ford step 1640 current loss 0.010897, current_train_items 52512.
I0304 19:28:45.981508 23118544486528 run.py:483] Algo bellman_ford step 1641 current loss 0.020722, current_train_items 52544.
I0304 19:28:46.005383 23118544486528 run.py:483] Algo bellman_ford step 1642 current loss 0.092035, current_train_items 52576.
I0304 19:28:46.035135 23118544486528 run.py:483] Algo bellman_ford step 1643 current loss 0.160816, current_train_items 52608.
I0304 19:28:46.067071 23118544486528 run.py:483] Algo bellman_ford step 1644 current loss 0.177717, current_train_items 52640.
I0304 19:28:46.085930 23118544486528 run.py:483] Algo bellman_ford step 1645 current loss 0.024057, current_train_items 52672.
I0304 19:28:46.102231 23118544486528 run.py:483] Algo bellman_ford step 1646 current loss 0.020732, current_train_items 52704.
I0304 19:28:46.126736 23118544486528 run.py:483] Algo bellman_ford step 1647 current loss 0.102706, current_train_items 52736.
I0304 19:28:46.155169 23118544486528 run.py:483] Algo bellman_ford step 1648 current loss 0.068399, current_train_items 52768.
I0304 19:28:46.188942 23118544486528 run.py:483] Algo bellman_ford step 1649 current loss 0.134979, current_train_items 52800.
I0304 19:28:46.208196 23118544486528 run.py:483] Algo bellman_ford step 1650 current loss 0.011768, current_train_items 52832.
I0304 19:28:46.216053 23118544486528 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0304 19:28:46.216159 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:28:46.234077 23118544486528 run.py:483] Algo bellman_ford step 1651 current loss 0.041135, current_train_items 52864.
I0304 19:28:46.258911 23118544486528 run.py:483] Algo bellman_ford step 1652 current loss 0.180692, current_train_items 52896.
I0304 19:28:46.288552 23118544486528 run.py:483] Algo bellman_ford step 1653 current loss 0.101628, current_train_items 52928.
I0304 19:28:46.319998 23118544486528 run.py:483] Algo bellman_ford step 1654 current loss 0.255280, current_train_items 52960.
I0304 19:28:46.339497 23118544486528 run.py:483] Algo bellman_ford step 1655 current loss 0.007466, current_train_items 52992.
I0304 19:28:46.355560 23118544486528 run.py:483] Algo bellman_ford step 1656 current loss 0.035675, current_train_items 53024.
I0304 19:28:46.379051 23118544486528 run.py:483] Algo bellman_ford step 1657 current loss 0.070018, current_train_items 53056.
I0304 19:28:46.407610 23118544486528 run.py:483] Algo bellman_ford step 1658 current loss 0.114125, current_train_items 53088.
I0304 19:28:46.438923 23118544486528 run.py:483] Algo bellman_ford step 1659 current loss 0.140959, current_train_items 53120.
I0304 19:28:46.458072 23118544486528 run.py:483] Algo bellman_ford step 1660 current loss 0.014856, current_train_items 53152.
I0304 19:28:46.474362 23118544486528 run.py:483] Algo bellman_ford step 1661 current loss 0.048896, current_train_items 53184.
I0304 19:28:46.496956 23118544486528 run.py:483] Algo bellman_ford step 1662 current loss 0.094451, current_train_items 53216.
I0304 19:28:46.527895 23118544486528 run.py:483] Algo bellman_ford step 1663 current loss 0.141612, current_train_items 53248.
I0304 19:28:46.561535 23118544486528 run.py:483] Algo bellman_ford step 1664 current loss 0.208983, current_train_items 53280.
I0304 19:28:46.580568 23118544486528 run.py:483] Algo bellman_ford step 1665 current loss 0.039036, current_train_items 53312.
I0304 19:28:46.596850 23118544486528 run.py:483] Algo bellman_ford step 1666 current loss 0.034680, current_train_items 53344.
I0304 19:28:46.619996 23118544486528 run.py:483] Algo bellman_ford step 1667 current loss 0.071554, current_train_items 53376.
I0304 19:28:46.649157 23118544486528 run.py:483] Algo bellman_ford step 1668 current loss 0.086200, current_train_items 53408.
I0304 19:28:46.680393 23118544486528 run.py:483] Algo bellman_ford step 1669 current loss 0.166603, current_train_items 53440.
I0304 19:28:46.699649 23118544486528 run.py:483] Algo bellman_ford step 1670 current loss 0.013055, current_train_items 53472.
I0304 19:28:46.715813 23118544486528 run.py:483] Algo bellman_ford step 1671 current loss 0.066857, current_train_items 53504.
I0304 19:28:46.738506 23118544486528 run.py:483] Algo bellman_ford step 1672 current loss 0.075312, current_train_items 53536.
I0304 19:28:46.768912 23118544486528 run.py:483] Algo bellman_ford step 1673 current loss 0.146317, current_train_items 53568.
I0304 19:28:46.801950 23118544486528 run.py:483] Algo bellman_ford step 1674 current loss 0.101095, current_train_items 53600.
I0304 19:28:46.821216 23118544486528 run.py:483] Algo bellman_ford step 1675 current loss 0.012324, current_train_items 53632.
I0304 19:28:46.837675 23118544486528 run.py:483] Algo bellman_ford step 1676 current loss 0.116321, current_train_items 53664.
I0304 19:28:46.860934 23118544486528 run.py:483] Algo bellman_ford step 1677 current loss 0.098527, current_train_items 53696.
I0304 19:28:46.890583 23118544486528 run.py:483] Algo bellman_ford step 1678 current loss 0.125904, current_train_items 53728.
I0304 19:28:46.924343 23118544486528 run.py:483] Algo bellman_ford step 1679 current loss 0.137719, current_train_items 53760.
I0304 19:28:46.943399 23118544486528 run.py:483] Algo bellman_ford step 1680 current loss 0.031918, current_train_items 53792.
I0304 19:28:46.959699 23118544486528 run.py:483] Algo bellman_ford step 1681 current loss 0.036287, current_train_items 53824.
I0304 19:28:46.982344 23118544486528 run.py:483] Algo bellman_ford step 1682 current loss 0.124211, current_train_items 53856.
I0304 19:28:47.010915 23118544486528 run.py:483] Algo bellman_ford step 1683 current loss 0.108697, current_train_items 53888.
I0304 19:28:47.043659 23118544486528 run.py:483] Algo bellman_ford step 1684 current loss 0.112313, current_train_items 53920.
I0304 19:28:47.063291 23118544486528 run.py:483] Algo bellman_ford step 1685 current loss 0.028855, current_train_items 53952.
I0304 19:28:47.079531 23118544486528 run.py:483] Algo bellman_ford step 1686 current loss 0.105124, current_train_items 53984.
I0304 19:28:47.102771 23118544486528 run.py:483] Algo bellman_ford step 1687 current loss 0.080663, current_train_items 54016.
I0304 19:28:47.132297 23118544486528 run.py:483] Algo bellman_ford step 1688 current loss 0.120573, current_train_items 54048.
I0304 19:28:47.166743 23118544486528 run.py:483] Algo bellman_ford step 1689 current loss 0.131359, current_train_items 54080.
I0304 19:28:47.185905 23118544486528 run.py:483] Algo bellman_ford step 1690 current loss 0.010678, current_train_items 54112.
I0304 19:28:47.202566 23118544486528 run.py:483] Algo bellman_ford step 1691 current loss 0.081097, current_train_items 54144.
I0304 19:28:47.225954 23118544486528 run.py:483] Algo bellman_ford step 1692 current loss 0.083531, current_train_items 54176.
I0304 19:28:47.256492 23118544486528 run.py:483] Algo bellman_ford step 1693 current loss 0.129184, current_train_items 54208.
I0304 19:28:47.289940 23118544486528 run.py:483] Algo bellman_ford step 1694 current loss 0.175254, current_train_items 54240.
I0304 19:28:47.309065 23118544486528 run.py:483] Algo bellman_ford step 1695 current loss 0.028083, current_train_items 54272.
I0304 19:28:47.325744 23118544486528 run.py:483] Algo bellman_ford step 1696 current loss 0.047704, current_train_items 54304.
I0304 19:28:47.347634 23118544486528 run.py:483] Algo bellman_ford step 1697 current loss 0.048151, current_train_items 54336.
I0304 19:28:47.376042 23118544486528 run.py:483] Algo bellman_ford step 1698 current loss 0.100384, current_train_items 54368.
I0304 19:28:47.409321 23118544486528 run.py:483] Algo bellman_ford step 1699 current loss 0.165167, current_train_items 54400.
I0304 19:28:47.428967 23118544486528 run.py:483] Algo bellman_ford step 1700 current loss 0.010926, current_train_items 54432.
I0304 19:28:47.436914 23118544486528 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0304 19:28:47.437019 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:28:47.454113 23118544486528 run.py:483] Algo bellman_ford step 1701 current loss 0.092795, current_train_items 54464.
I0304 19:28:47.477628 23118544486528 run.py:483] Algo bellman_ford step 1702 current loss 0.061785, current_train_items 54496.
I0304 19:28:47.507827 23118544486528 run.py:483] Algo bellman_ford step 1703 current loss 0.079913, current_train_items 54528.
I0304 19:28:47.540231 23118544486528 run.py:483] Algo bellman_ford step 1704 current loss 0.124457, current_train_items 54560.
I0304 19:28:47.560052 23118544486528 run.py:483] Algo bellman_ford step 1705 current loss 0.009437, current_train_items 54592.
I0304 19:28:47.575752 23118544486528 run.py:483] Algo bellman_ford step 1706 current loss 0.025092, current_train_items 54624.
I0304 19:28:47.599978 23118544486528 run.py:483] Algo bellman_ford step 1707 current loss 0.110751, current_train_items 54656.
I0304 19:28:47.629703 23118544486528 run.py:483] Algo bellman_ford step 1708 current loss 0.131246, current_train_items 54688.
I0304 19:28:47.663910 23118544486528 run.py:483] Algo bellman_ford step 1709 current loss 0.136023, current_train_items 54720.
I0304 19:28:47.683134 23118544486528 run.py:483] Algo bellman_ford step 1710 current loss 0.027897, current_train_items 54752.
I0304 19:28:47.699532 23118544486528 run.py:483] Algo bellman_ford step 1711 current loss 0.041344, current_train_items 54784.
I0304 19:28:47.723805 23118544486528 run.py:483] Algo bellman_ford step 1712 current loss 0.125203, current_train_items 54816.
I0304 19:28:47.753092 23118544486528 run.py:483] Algo bellman_ford step 1713 current loss 0.140420, current_train_items 54848.
I0304 19:28:47.785670 23118544486528 run.py:483] Algo bellman_ford step 1714 current loss 0.145893, current_train_items 54880.
I0304 19:28:47.804665 23118544486528 run.py:483] Algo bellman_ford step 1715 current loss 0.013822, current_train_items 54912.
I0304 19:28:47.820589 23118544486528 run.py:483] Algo bellman_ford step 1716 current loss 0.032364, current_train_items 54944.
I0304 19:28:47.844899 23118544486528 run.py:483] Algo bellman_ford step 1717 current loss 0.077480, current_train_items 54976.
I0304 19:28:47.873540 23118544486528 run.py:483] Algo bellman_ford step 1718 current loss 0.129609, current_train_items 55008.
I0304 19:28:47.906095 23118544486528 run.py:483] Algo bellman_ford step 1719 current loss 0.179152, current_train_items 55040.
I0304 19:28:47.925228 23118544486528 run.py:483] Algo bellman_ford step 1720 current loss 0.051132, current_train_items 55072.
I0304 19:28:47.941426 23118544486528 run.py:483] Algo bellman_ford step 1721 current loss 0.032056, current_train_items 55104.
I0304 19:28:47.964581 23118544486528 run.py:483] Algo bellman_ford step 1722 current loss 0.053190, current_train_items 55136.
I0304 19:28:47.994374 23118544486528 run.py:483] Algo bellman_ford step 1723 current loss 0.107059, current_train_items 55168.
I0304 19:28:48.026762 23118544486528 run.py:483] Algo bellman_ford step 1724 current loss 0.129509, current_train_items 55200.
I0304 19:28:48.045794 23118544486528 run.py:483] Algo bellman_ford step 1725 current loss 0.011791, current_train_items 55232.
I0304 19:28:48.062179 23118544486528 run.py:483] Algo bellman_ford step 1726 current loss 0.097115, current_train_items 55264.
I0304 19:28:48.087463 23118544486528 run.py:483] Algo bellman_ford step 1727 current loss 0.090729, current_train_items 55296.
I0304 19:28:48.117322 23118544486528 run.py:483] Algo bellman_ford step 1728 current loss 0.090475, current_train_items 55328.
I0304 19:28:48.149867 23118544486528 run.py:483] Algo bellman_ford step 1729 current loss 0.126462, current_train_items 55360.
I0304 19:28:48.169268 23118544486528 run.py:483] Algo bellman_ford step 1730 current loss 0.012705, current_train_items 55392.
I0304 19:28:48.185549 23118544486528 run.py:483] Algo bellman_ford step 1731 current loss 0.026516, current_train_items 55424.
I0304 19:28:48.209340 23118544486528 run.py:483] Algo bellman_ford step 1732 current loss 0.082458, current_train_items 55456.
I0304 19:28:48.238908 23118544486528 run.py:483] Algo bellman_ford step 1733 current loss 0.107809, current_train_items 55488.
I0304 19:28:48.269446 23118544486528 run.py:483] Algo bellman_ford step 1734 current loss 0.088519, current_train_items 55520.
I0304 19:28:48.288846 23118544486528 run.py:483] Algo bellman_ford step 1735 current loss 0.012112, current_train_items 55552.
I0304 19:28:48.305333 23118544486528 run.py:483] Algo bellman_ford step 1736 current loss 0.034346, current_train_items 55584.
I0304 19:28:48.329288 23118544486528 run.py:483] Algo bellman_ford step 1737 current loss 0.131192, current_train_items 55616.
I0304 19:28:48.359088 23118544486528 run.py:483] Algo bellman_ford step 1738 current loss 0.091803, current_train_items 55648.
I0304 19:28:48.392038 23118544486528 run.py:483] Algo bellman_ford step 1739 current loss 0.108199, current_train_items 55680.
I0304 19:28:48.410950 23118544486528 run.py:483] Algo bellman_ford step 1740 current loss 0.011259, current_train_items 55712.
I0304 19:28:48.427288 23118544486528 run.py:483] Algo bellman_ford step 1741 current loss 0.057220, current_train_items 55744.
I0304 19:28:48.451256 23118544486528 run.py:483] Algo bellman_ford step 1742 current loss 0.105275, current_train_items 55776.
I0304 19:28:48.481938 23118544486528 run.py:483] Algo bellman_ford step 1743 current loss 0.100076, current_train_items 55808.
I0304 19:28:48.513616 23118544486528 run.py:483] Algo bellman_ford step 1744 current loss 0.137425, current_train_items 55840.
I0304 19:28:48.532839 23118544486528 run.py:483] Algo bellman_ford step 1745 current loss 0.009272, current_train_items 55872.
I0304 19:28:48.549297 23118544486528 run.py:483] Algo bellman_ford step 1746 current loss 0.074612, current_train_items 55904.
I0304 19:28:48.572637 23118544486528 run.py:483] Algo bellman_ford step 1747 current loss 0.098084, current_train_items 55936.
I0304 19:28:48.602583 23118544486528 run.py:483] Algo bellman_ford step 1748 current loss 0.104373, current_train_items 55968.
I0304 19:28:48.633760 23118544486528 run.py:483] Algo bellman_ford step 1749 current loss 0.091494, current_train_items 56000.
I0304 19:28:48.652894 23118544486528 run.py:483] Algo bellman_ford step 1750 current loss 0.067598, current_train_items 56032.
I0304 19:28:48.660887 23118544486528 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0304 19:28:48.660991 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:48.678008 23118544486528 run.py:483] Algo bellman_ford step 1751 current loss 0.071016, current_train_items 56064.
I0304 19:28:48.702051 23118544486528 run.py:483] Algo bellman_ford step 1752 current loss 0.105068, current_train_items 56096.
I0304 19:28:48.733516 23118544486528 run.py:483] Algo bellman_ford step 1753 current loss 0.145930, current_train_items 56128.
I0304 19:28:48.767884 23118544486528 run.py:483] Algo bellman_ford step 1754 current loss 0.175561, current_train_items 56160.
I0304 19:28:48.787259 23118544486528 run.py:483] Algo bellman_ford step 1755 current loss 0.011393, current_train_items 56192.
I0304 19:28:48.803879 23118544486528 run.py:483] Algo bellman_ford step 1756 current loss 0.057468, current_train_items 56224.
I0304 19:28:48.828527 23118544486528 run.py:483] Algo bellman_ford step 1757 current loss 0.205037, current_train_items 56256.
I0304 19:28:48.859126 23118544486528 run.py:483] Algo bellman_ford step 1758 current loss 0.122452, current_train_items 56288.
I0304 19:28:48.889539 23118544486528 run.py:483] Algo bellman_ford step 1759 current loss 0.085670, current_train_items 56320.
I0304 19:28:48.908859 23118544486528 run.py:483] Algo bellman_ford step 1760 current loss 0.008605, current_train_items 56352.
I0304 19:28:48.925304 23118544486528 run.py:483] Algo bellman_ford step 1761 current loss 0.044881, current_train_items 56384.
I0304 19:28:48.949707 23118544486528 run.py:483] Algo bellman_ford step 1762 current loss 0.361400, current_train_items 56416.
I0304 19:28:48.981366 23118544486528 run.py:483] Algo bellman_ford step 1763 current loss 0.250921, current_train_items 56448.
I0304 19:28:49.015826 23118544486528 run.py:483] Algo bellman_ford step 1764 current loss 0.196134, current_train_items 56480.
I0304 19:28:49.034919 23118544486528 run.py:483] Algo bellman_ford step 1765 current loss 0.012485, current_train_items 56512.
I0304 19:28:49.051179 23118544486528 run.py:483] Algo bellman_ford step 1766 current loss 0.029281, current_train_items 56544.
I0304 19:28:49.074466 23118544486528 run.py:483] Algo bellman_ford step 1767 current loss 0.093690, current_train_items 56576.
I0304 19:28:49.104420 23118544486528 run.py:483] Algo bellman_ford step 1768 current loss 0.099643, current_train_items 56608.
I0304 19:28:49.136037 23118544486528 run.py:483] Algo bellman_ford step 1769 current loss 0.159098, current_train_items 56640.
I0304 19:28:49.155611 23118544486528 run.py:483] Algo bellman_ford step 1770 current loss 0.030216, current_train_items 56672.
I0304 19:28:49.172091 23118544486528 run.py:483] Algo bellman_ford step 1771 current loss 0.070800, current_train_items 56704.
I0304 19:28:49.196439 23118544486528 run.py:483] Algo bellman_ford step 1772 current loss 0.114977, current_train_items 56736.
I0304 19:28:49.227339 23118544486528 run.py:483] Algo bellman_ford step 1773 current loss 0.123789, current_train_items 56768.
I0304 19:28:49.259167 23118544486528 run.py:483] Algo bellman_ford step 1774 current loss 0.144937, current_train_items 56800.
I0304 19:28:49.278462 23118544486528 run.py:483] Algo bellman_ford step 1775 current loss 0.012423, current_train_items 56832.
I0304 19:28:49.295118 23118544486528 run.py:483] Algo bellman_ford step 1776 current loss 0.068463, current_train_items 56864.
I0304 19:28:49.319342 23118544486528 run.py:483] Algo bellman_ford step 1777 current loss 0.128466, current_train_items 56896.
I0304 19:28:49.349059 23118544486528 run.py:483] Algo bellman_ford step 1778 current loss 0.101484, current_train_items 56928.
I0304 19:28:49.379703 23118544486528 run.py:483] Algo bellman_ford step 1779 current loss 0.113964, current_train_items 56960.
I0304 19:28:49.398844 23118544486528 run.py:483] Algo bellman_ford step 1780 current loss 0.018228, current_train_items 56992.
I0304 19:28:49.414995 23118544486528 run.py:483] Algo bellman_ford step 1781 current loss 0.050845, current_train_items 57024.
I0304 19:28:49.438549 23118544486528 run.py:483] Algo bellman_ford step 1782 current loss 0.133242, current_train_items 57056.
I0304 19:28:49.468940 23118544486528 run.py:483] Algo bellman_ford step 1783 current loss 0.172919, current_train_items 57088.
I0304 19:28:49.500894 23118544486528 run.py:483] Algo bellman_ford step 1784 current loss 0.112653, current_train_items 57120.
I0304 19:28:49.520669 23118544486528 run.py:483] Algo bellman_ford step 1785 current loss 0.008875, current_train_items 57152.
I0304 19:28:49.537098 23118544486528 run.py:483] Algo bellman_ford step 1786 current loss 0.029759, current_train_items 57184.
I0304 19:28:49.559767 23118544486528 run.py:483] Algo bellman_ford step 1787 current loss 0.119279, current_train_items 57216.
I0304 19:28:49.588654 23118544486528 run.py:483] Algo bellman_ford step 1788 current loss 0.077591, current_train_items 57248.
I0304 19:28:49.621668 23118544486528 run.py:483] Algo bellman_ford step 1789 current loss 0.114470, current_train_items 57280.
I0304 19:28:49.641070 23118544486528 run.py:483] Algo bellman_ford step 1790 current loss 0.011709, current_train_items 57312.
I0304 19:28:49.658039 23118544486528 run.py:483] Algo bellman_ford step 1791 current loss 0.040780, current_train_items 57344.
I0304 19:28:49.681942 23118544486528 run.py:483] Algo bellman_ford step 1792 current loss 0.069945, current_train_items 57376.
I0304 19:28:49.710662 23118544486528 run.py:483] Algo bellman_ford step 1793 current loss 0.116972, current_train_items 57408.
I0304 19:28:49.742981 23118544486528 run.py:483] Algo bellman_ford step 1794 current loss 0.130520, current_train_items 57440.
I0304 19:28:49.761921 23118544486528 run.py:483] Algo bellman_ford step 1795 current loss 0.012332, current_train_items 57472.
I0304 19:28:49.778335 23118544486528 run.py:483] Algo bellman_ford step 1796 current loss 0.058805, current_train_items 57504.
I0304 19:28:49.802592 23118544486528 run.py:483] Algo bellman_ford step 1797 current loss 0.089519, current_train_items 57536.
I0304 19:28:49.833034 23118544486528 run.py:483] Algo bellman_ford step 1798 current loss 0.137392, current_train_items 57568.
I0304 19:28:49.866554 23118544486528 run.py:483] Algo bellman_ford step 1799 current loss 0.127774, current_train_items 57600.
I0304 19:28:49.886392 23118544486528 run.py:483] Algo bellman_ford step 1800 current loss 0.021751, current_train_items 57632.
I0304 19:28:49.894059 23118544486528 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0304 19:28:49.894163 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0304 19:28:49.910734 23118544486528 run.py:483] Algo bellman_ford step 1801 current loss 0.055848, current_train_items 57664.
I0304 19:28:49.935114 23118544486528 run.py:483] Algo bellman_ford step 1802 current loss 0.113403, current_train_items 57696.
I0304 19:28:49.964428 23118544486528 run.py:483] Algo bellman_ford step 1803 current loss 0.138347, current_train_items 57728.
I0304 19:28:49.996938 23118544486528 run.py:483] Algo bellman_ford step 1804 current loss 0.200906, current_train_items 57760.
I0304 19:28:50.016458 23118544486528 run.py:483] Algo bellman_ford step 1805 current loss 0.010128, current_train_items 57792.
I0304 19:28:50.032280 23118544486528 run.py:483] Algo bellman_ford step 1806 current loss 0.021642, current_train_items 57824.
I0304 19:28:50.055398 23118544486528 run.py:483] Algo bellman_ford step 1807 current loss 0.069440, current_train_items 57856.
I0304 19:28:50.083723 23118544486528 run.py:483] Algo bellman_ford step 1808 current loss 0.087709, current_train_items 57888.
I0304 19:28:50.117313 23118544486528 run.py:483] Algo bellman_ford step 1809 current loss 0.163696, current_train_items 57920.
I0304 19:28:50.136245 23118544486528 run.py:483] Algo bellman_ford step 1810 current loss 0.007000, current_train_items 57952.
I0304 19:28:50.152384 23118544486528 run.py:483] Algo bellman_ford step 1811 current loss 0.043596, current_train_items 57984.
I0304 19:28:50.176109 23118544486528 run.py:483] Algo bellman_ford step 1812 current loss 0.092564, current_train_items 58016.
I0304 19:28:50.203493 23118544486528 run.py:483] Algo bellman_ford step 1813 current loss 0.081395, current_train_items 58048.
I0304 19:28:50.235814 23118544486528 run.py:483] Algo bellman_ford step 1814 current loss 0.127391, current_train_items 58080.
I0304 19:28:50.254691 23118544486528 run.py:483] Algo bellman_ford step 1815 current loss 0.007818, current_train_items 58112.
I0304 19:28:50.271355 23118544486528 run.py:483] Algo bellman_ford step 1816 current loss 0.087725, current_train_items 58144.
I0304 19:28:50.294513 23118544486528 run.py:483] Algo bellman_ford step 1817 current loss 0.073011, current_train_items 58176.
I0304 19:28:50.324207 23118544486528 run.py:483] Algo bellman_ford step 1818 current loss 0.086219, current_train_items 58208.
I0304 19:28:50.356675 23118544486528 run.py:483] Algo bellman_ford step 1819 current loss 0.128896, current_train_items 58240.
I0304 19:28:50.375706 23118544486528 run.py:483] Algo bellman_ford step 1820 current loss 0.030859, current_train_items 58272.
I0304 19:28:50.393007 23118544486528 run.py:483] Algo bellman_ford step 1821 current loss 0.048944, current_train_items 58304.
I0304 19:28:50.418447 23118544486528 run.py:483] Algo bellman_ford step 1822 current loss 0.113345, current_train_items 58336.
I0304 19:28:50.448666 23118544486528 run.py:483] Algo bellman_ford step 1823 current loss 0.071761, current_train_items 58368.
I0304 19:28:50.479825 23118544486528 run.py:483] Algo bellman_ford step 1824 current loss 0.109103, current_train_items 58400.
I0304 19:28:50.499361 23118544486528 run.py:483] Algo bellman_ford step 1825 current loss 0.007802, current_train_items 58432.
I0304 19:28:50.515766 23118544486528 run.py:483] Algo bellman_ford step 1826 current loss 0.057757, current_train_items 58464.
I0304 19:28:50.538874 23118544486528 run.py:483] Algo bellman_ford step 1827 current loss 0.074959, current_train_items 58496.
I0304 19:28:50.570143 23118544486528 run.py:483] Algo bellman_ford step 1828 current loss 0.132789, current_train_items 58528.
I0304 19:28:50.605116 23118544486528 run.py:483] Algo bellman_ford step 1829 current loss 0.157815, current_train_items 58560.
I0304 19:28:50.624169 23118544486528 run.py:483] Algo bellman_ford step 1830 current loss 0.008622, current_train_items 58592.
I0304 19:28:50.640660 23118544486528 run.py:483] Algo bellman_ford step 1831 current loss 0.072554, current_train_items 58624.
I0304 19:28:50.665804 23118544486528 run.py:483] Algo bellman_ford step 1832 current loss 0.078040, current_train_items 58656.
I0304 19:28:50.695827 23118544486528 run.py:483] Algo bellman_ford step 1833 current loss 0.147437, current_train_items 58688.
I0304 19:28:50.727520 23118544486528 run.py:483] Algo bellman_ford step 1834 current loss 0.120695, current_train_items 58720.
I0304 19:28:50.746425 23118544486528 run.py:483] Algo bellman_ford step 1835 current loss 0.011193, current_train_items 58752.
I0304 19:28:50.763132 23118544486528 run.py:483] Algo bellman_ford step 1836 current loss 0.049293, current_train_items 58784.
I0304 19:28:50.788054 23118544486528 run.py:483] Algo bellman_ford step 1837 current loss 0.108932, current_train_items 58816.
I0304 19:28:50.817544 23118544486528 run.py:483] Algo bellman_ford step 1838 current loss 0.137597, current_train_items 58848.
I0304 19:28:50.850265 23118544486528 run.py:483] Algo bellman_ford step 1839 current loss 0.139450, current_train_items 58880.
I0304 19:28:50.869284 23118544486528 run.py:483] Algo bellman_ford step 1840 current loss 0.008464, current_train_items 58912.
I0304 19:28:50.885645 23118544486528 run.py:483] Algo bellman_ford step 1841 current loss 0.033010, current_train_items 58944.
I0304 19:28:50.909759 23118544486528 run.py:483] Algo bellman_ford step 1842 current loss 0.082036, current_train_items 58976.
I0304 19:28:50.940166 23118544486528 run.py:483] Algo bellman_ford step 1843 current loss 0.089061, current_train_items 59008.
I0304 19:28:50.972689 23118544486528 run.py:483] Algo bellman_ford step 1844 current loss 0.110187, current_train_items 59040.
I0304 19:28:50.991824 23118544486528 run.py:483] Algo bellman_ford step 1845 current loss 0.013152, current_train_items 59072.
I0304 19:28:51.007982 23118544486528 run.py:483] Algo bellman_ford step 1846 current loss 0.067247, current_train_items 59104.
I0304 19:28:51.031931 23118544486528 run.py:483] Algo bellman_ford step 1847 current loss 0.135913, current_train_items 59136.
I0304 19:28:51.062241 23118544486528 run.py:483] Algo bellman_ford step 1848 current loss 0.109825, current_train_items 59168.
I0304 19:28:51.096309 23118544486528 run.py:483] Algo bellman_ford step 1849 current loss 0.135439, current_train_items 59200.
I0304 19:28:51.115596 23118544486528 run.py:483] Algo bellman_ford step 1850 current loss 0.014816, current_train_items 59232.
I0304 19:28:51.123494 23118544486528 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0304 19:28:51.123600 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0304 19:28:51.140500 23118544486528 run.py:483] Algo bellman_ford step 1851 current loss 0.029328, current_train_items 59264.
I0304 19:28:51.164659 23118544486528 run.py:483] Algo bellman_ford step 1852 current loss 0.155622, current_train_items 59296.
I0304 19:28:51.195165 23118544486528 run.py:483] Algo bellman_ford step 1853 current loss 0.133413, current_train_items 59328.
I0304 19:28:51.227996 23118544486528 run.py:483] Algo bellman_ford step 1854 current loss 0.131012, current_train_items 59360.
I0304 19:28:51.247321 23118544486528 run.py:483] Algo bellman_ford step 1855 current loss 0.040430, current_train_items 59392.
I0304 19:28:51.262776 23118544486528 run.py:483] Algo bellman_ford step 1856 current loss 0.032345, current_train_items 59424.
I0304 19:28:51.286669 23118544486528 run.py:483] Algo bellman_ford step 1857 current loss 0.166343, current_train_items 59456.
I0304 19:28:51.315453 23118544486528 run.py:483] Algo bellman_ford step 1858 current loss 0.124670, current_train_items 59488.
I0304 19:28:51.348147 23118544486528 run.py:483] Algo bellman_ford step 1859 current loss 0.215828, current_train_items 59520.
I0304 19:28:51.367665 23118544486528 run.py:483] Algo bellman_ford step 1860 current loss 0.017355, current_train_items 59552.
I0304 19:28:51.384323 23118544486528 run.py:483] Algo bellman_ford step 1861 current loss 0.042041, current_train_items 59584.
I0304 19:28:51.407455 23118544486528 run.py:483] Algo bellman_ford step 1862 current loss 0.120323, current_train_items 59616.
I0304 19:28:51.436956 23118544486528 run.py:483] Algo bellman_ford step 1863 current loss 0.099429, current_train_items 59648.
I0304 19:28:51.470255 23118544486528 run.py:483] Algo bellman_ford step 1864 current loss 0.209179, current_train_items 59680.
I0304 19:28:51.489380 23118544486528 run.py:483] Algo bellman_ford step 1865 current loss 0.011858, current_train_items 59712.
I0304 19:28:51.505757 23118544486528 run.py:483] Algo bellman_ford step 1866 current loss 0.038349, current_train_items 59744.
I0304 19:28:51.530407 23118544486528 run.py:483] Algo bellman_ford step 1867 current loss 0.103097, current_train_items 59776.
I0304 19:28:51.560386 23118544486528 run.py:483] Algo bellman_ford step 1868 current loss 0.154471, current_train_items 59808.
I0304 19:28:51.592644 23118544486528 run.py:483] Algo bellman_ford step 1869 current loss 0.125498, current_train_items 59840.
I0304 19:28:51.611935 23118544486528 run.py:483] Algo bellman_ford step 1870 current loss 0.006480, current_train_items 59872.
I0304 19:28:51.628097 23118544486528 run.py:483] Algo bellman_ford step 1871 current loss 0.026416, current_train_items 59904.
I0304 19:28:51.650921 23118544486528 run.py:483] Algo bellman_ford step 1872 current loss 0.031809, current_train_items 59936.
I0304 19:28:51.680823 23118544486528 run.py:483] Algo bellman_ford step 1873 current loss 0.111645, current_train_items 59968.
I0304 19:28:51.713187 23118544486528 run.py:483] Algo bellman_ford step 1874 current loss 0.108340, current_train_items 60000.
I0304 19:28:51.732523 23118544486528 run.py:483] Algo bellman_ford step 1875 current loss 0.034605, current_train_items 60032.
I0304 19:28:51.748753 23118544486528 run.py:483] Algo bellman_ford step 1876 current loss 0.023247, current_train_items 60064.
I0304 19:28:51.772272 23118544486528 run.py:483] Algo bellman_ford step 1877 current loss 0.082937, current_train_items 60096.
I0304 19:28:51.801380 23118544486528 run.py:483] Algo bellman_ford step 1878 current loss 0.067940, current_train_items 60128.
I0304 19:28:51.836371 23118544486528 run.py:483] Algo bellman_ford step 1879 current loss 0.149503, current_train_items 60160.
I0304 19:28:51.855304 23118544486528 run.py:483] Algo bellman_ford step 1880 current loss 0.007432, current_train_items 60192.
I0304 19:28:51.871858 23118544486528 run.py:483] Algo bellman_ford step 1881 current loss 0.049764, current_train_items 60224.
I0304 19:28:51.894868 23118544486528 run.py:483] Algo bellman_ford step 1882 current loss 0.086265, current_train_items 60256.
I0304 19:28:51.925375 23118544486528 run.py:483] Algo bellman_ford step 1883 current loss 0.108129, current_train_items 60288.
I0304 19:28:51.958065 23118544486528 run.py:483] Algo bellman_ford step 1884 current loss 0.157177, current_train_items 60320.
I0304 19:28:51.977453 23118544486528 run.py:483] Algo bellman_ford step 1885 current loss 0.042053, current_train_items 60352.
I0304 19:28:51.994054 23118544486528 run.py:483] Algo bellman_ford step 1886 current loss 0.026613, current_train_items 60384.
I0304 19:28:52.017477 23118544486528 run.py:483] Algo bellman_ford step 1887 current loss 0.125086, current_train_items 60416.
I0304 19:28:52.046633 23118544486528 run.py:483] Algo bellman_ford step 1888 current loss 0.085028, current_train_items 60448.
I0304 19:28:52.081234 23118544486528 run.py:483] Algo bellman_ford step 1889 current loss 0.163431, current_train_items 60480.
I0304 19:28:52.100263 23118544486528 run.py:483] Algo bellman_ford step 1890 current loss 0.006961, current_train_items 60512.
I0304 19:28:52.116859 23118544486528 run.py:483] Algo bellman_ford step 1891 current loss 0.051846, current_train_items 60544.
I0304 19:28:52.139586 23118544486528 run.py:483] Algo bellman_ford step 1892 current loss 0.075982, current_train_items 60576.
I0304 19:28:52.170136 23118544486528 run.py:483] Algo bellman_ford step 1893 current loss 0.114472, current_train_items 60608.
I0304 19:28:52.205466 23118544486528 run.py:483] Algo bellman_ford step 1894 current loss 0.164712, current_train_items 60640.
I0304 19:28:52.224518 23118544486528 run.py:483] Algo bellman_ford step 1895 current loss 0.029577, current_train_items 60672.
I0304 19:28:52.240780 23118544486528 run.py:483] Algo bellman_ford step 1896 current loss 0.024672, current_train_items 60704.
I0304 19:28:52.264370 23118544486528 run.py:483] Algo bellman_ford step 1897 current loss 0.111247, current_train_items 60736.
I0304 19:28:52.294474 23118544486528 run.py:483] Algo bellman_ford step 1898 current loss 0.107062, current_train_items 60768.
I0304 19:28:52.327315 23118544486528 run.py:483] Algo bellman_ford step 1899 current loss 0.117049, current_train_items 60800.
I0304 19:28:52.346745 23118544486528 run.py:483] Algo bellman_ford step 1900 current loss 0.010909, current_train_items 60832.
I0304 19:28:52.354702 23118544486528 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0304 19:28:52.354812 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:28:52.371503 23118544486528 run.py:483] Algo bellman_ford step 1901 current loss 0.020309, current_train_items 60864.
I0304 19:28:52.394987 23118544486528 run.py:483] Algo bellman_ford step 1902 current loss 0.051904, current_train_items 60896.
I0304 19:28:52.423270 23118544486528 run.py:483] Algo bellman_ford step 1903 current loss 0.106704, current_train_items 60928.
I0304 19:28:52.456258 23118544486528 run.py:483] Algo bellman_ford step 1904 current loss 0.105379, current_train_items 60960.
I0304 19:28:52.475716 23118544486528 run.py:483] Algo bellman_ford step 1905 current loss 0.009521, current_train_items 60992.
I0304 19:28:52.491536 23118544486528 run.py:483] Algo bellman_ford step 1906 current loss 0.031179, current_train_items 61024.
I0304 19:28:52.514083 23118544486528 run.py:483] Algo bellman_ford step 1907 current loss 0.070268, current_train_items 61056.
I0304 19:28:52.543468 23118544486528 run.py:483] Algo bellman_ford step 1908 current loss 0.093026, current_train_items 61088.
I0304 19:28:52.575417 23118544486528 run.py:483] Algo bellman_ford step 1909 current loss 0.093853, current_train_items 61120.
I0304 19:28:52.594337 23118544486528 run.py:483] Algo bellman_ford step 1910 current loss 0.011898, current_train_items 61152.
I0304 19:28:52.610715 23118544486528 run.py:483] Algo bellman_ford step 1911 current loss 0.037158, current_train_items 61184.
I0304 19:28:52.635126 23118544486528 run.py:483] Algo bellman_ford step 1912 current loss 0.083975, current_train_items 61216.
I0304 19:28:52.664484 23118544486528 run.py:483] Algo bellman_ford step 1913 current loss 0.083856, current_train_items 61248.
I0304 19:28:52.697624 23118544486528 run.py:483] Algo bellman_ford step 1914 current loss 0.086233, current_train_items 61280.
I0304 19:28:52.716432 23118544486528 run.py:483] Algo bellman_ford step 1915 current loss 0.005359, current_train_items 61312.
I0304 19:28:52.733142 23118544486528 run.py:483] Algo bellman_ford step 1916 current loss 0.021118, current_train_items 61344.
I0304 19:28:52.756992 23118544486528 run.py:483] Algo bellman_ford step 1917 current loss 0.066371, current_train_items 61376.
I0304 19:28:52.787287 23118544486528 run.py:483] Algo bellman_ford step 1918 current loss 0.071356, current_train_items 61408.
I0304 19:28:52.821164 23118544486528 run.py:483] Algo bellman_ford step 1919 current loss 0.142291, current_train_items 61440.
I0304 19:28:52.840251 23118544486528 run.py:483] Algo bellman_ford step 1920 current loss 0.014843, current_train_items 61472.
I0304 19:28:52.856719 23118544486528 run.py:483] Algo bellman_ford step 1921 current loss 0.079847, current_train_items 61504.
I0304 19:28:52.879894 23118544486528 run.py:483] Algo bellman_ford step 1922 current loss 0.086651, current_train_items 61536.
I0304 19:28:52.908860 23118544486528 run.py:483] Algo bellman_ford step 1923 current loss 0.075145, current_train_items 61568.
I0304 19:28:52.941601 23118544486528 run.py:483] Algo bellman_ford step 1924 current loss 0.106116, current_train_items 61600.
I0304 19:28:52.960943 23118544486528 run.py:483] Algo bellman_ford step 1925 current loss 0.007650, current_train_items 61632.
I0304 19:28:52.977305 23118544486528 run.py:483] Algo bellman_ford step 1926 current loss 0.094387, current_train_items 61664.
I0304 19:28:53.000048 23118544486528 run.py:483] Algo bellman_ford step 1927 current loss 0.041420, current_train_items 61696.
I0304 19:28:53.028944 23118544486528 run.py:483] Algo bellman_ford step 1928 current loss 0.074281, current_train_items 61728.
I0304 19:28:53.059873 23118544486528 run.py:483] Algo bellman_ford step 1929 current loss 0.116740, current_train_items 61760.
I0304 19:28:53.079016 23118544486528 run.py:483] Algo bellman_ford step 1930 current loss 0.011308, current_train_items 61792.
I0304 19:28:53.094815 23118544486528 run.py:483] Algo bellman_ford step 1931 current loss 0.046428, current_train_items 61824.
I0304 19:28:53.119577 23118544486528 run.py:483] Algo bellman_ford step 1932 current loss 0.072053, current_train_items 61856.
I0304 19:28:53.149219 23118544486528 run.py:483] Algo bellman_ford step 1933 current loss 0.135999, current_train_items 61888.
I0304 19:28:53.180283 23118544486528 run.py:483] Algo bellman_ford step 1934 current loss 0.092641, current_train_items 61920.
I0304 19:28:53.199122 23118544486528 run.py:483] Algo bellman_ford step 1935 current loss 0.014297, current_train_items 61952.
I0304 19:28:53.215416 23118544486528 run.py:483] Algo bellman_ford step 1936 current loss 0.052206, current_train_items 61984.
I0304 19:28:53.239613 23118544486528 run.py:483] Algo bellman_ford step 1937 current loss 0.102836, current_train_items 62016.
I0304 19:28:53.268358 23118544486528 run.py:483] Algo bellman_ford step 1938 current loss 0.135287, current_train_items 62048.
I0304 19:28:53.301719 23118544486528 run.py:483] Algo bellman_ford step 1939 current loss 0.137410, current_train_items 62080.
I0304 19:28:53.320838 23118544486528 run.py:483] Algo bellman_ford step 1940 current loss 0.039718, current_train_items 62112.
I0304 19:28:53.337321 23118544486528 run.py:483] Algo bellman_ford step 1941 current loss 0.022225, current_train_items 62144.
I0304 19:28:53.360536 23118544486528 run.py:483] Algo bellman_ford step 1942 current loss 0.085369, current_train_items 62176.
I0304 19:28:53.389449 23118544486528 run.py:483] Algo bellman_ford step 1943 current loss 0.084996, current_train_items 62208.
I0304 19:28:53.422227 23118544486528 run.py:483] Algo bellman_ford step 1944 current loss 0.134397, current_train_items 62240.
I0304 19:28:53.441270 23118544486528 run.py:483] Algo bellman_ford step 1945 current loss 0.014766, current_train_items 62272.
I0304 19:28:53.457608 23118544486528 run.py:483] Algo bellman_ford step 1946 current loss 0.134448, current_train_items 62304.
I0304 19:28:53.481379 23118544486528 run.py:483] Algo bellman_ford step 1947 current loss 0.079686, current_train_items 62336.
I0304 19:28:53.512006 23118544486528 run.py:483] Algo bellman_ford step 1948 current loss 0.106341, current_train_items 62368.
I0304 19:28:53.543627 23118544486528 run.py:483] Algo bellman_ford step 1949 current loss 0.087313, current_train_items 62400.
I0304 19:28:53.562710 23118544486528 run.py:483] Algo bellman_ford step 1950 current loss 0.018911, current_train_items 62432.
I0304 19:28:53.570676 23118544486528 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0304 19:28:53.570788 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0304 19:28:53.587946 23118544486528 run.py:483] Algo bellman_ford step 1951 current loss 0.061314, current_train_items 62464.
I0304 19:28:53.612483 23118544486528 run.py:483] Algo bellman_ford step 1952 current loss 0.127120, current_train_items 62496.
I0304 19:28:53.644948 23118544486528 run.py:483] Algo bellman_ford step 1953 current loss 0.162826, current_train_items 62528.
I0304 19:28:53.677572 23118544486528 run.py:483] Algo bellman_ford step 1954 current loss 0.137910, current_train_items 62560.
I0304 19:28:53.697023 23118544486528 run.py:483] Algo bellman_ford step 1955 current loss 0.012186, current_train_items 62592.
I0304 19:28:53.712260 23118544486528 run.py:483] Algo bellman_ford step 1956 current loss 0.017992, current_train_items 62624.
I0304 19:28:53.736314 23118544486528 run.py:483] Algo bellman_ford step 1957 current loss 0.188453, current_train_items 62656.
I0304 19:28:53.766789 23118544486528 run.py:483] Algo bellman_ford step 1958 current loss 0.139055, current_train_items 62688.
I0304 19:28:53.800164 23118544486528 run.py:483] Algo bellman_ford step 1959 current loss 0.133478, current_train_items 62720.
I0304 19:28:53.819859 23118544486528 run.py:483] Algo bellman_ford step 1960 current loss 0.010305, current_train_items 62752.
I0304 19:28:53.836226 23118544486528 run.py:483] Algo bellman_ford step 1961 current loss 0.052108, current_train_items 62784.
I0304 19:28:53.859302 23118544486528 run.py:483] Algo bellman_ford step 1962 current loss 0.126352, current_train_items 62816.
I0304 19:28:53.887846 23118544486528 run.py:483] Algo bellman_ford step 1963 current loss 0.100556, current_train_items 62848.
I0304 19:28:53.923587 23118544486528 run.py:483] Algo bellman_ford step 1964 current loss 0.127150, current_train_items 62880.
I0304 19:28:53.942768 23118544486528 run.py:483] Algo bellman_ford step 1965 current loss 0.009522, current_train_items 62912.
I0304 19:28:53.959154 23118544486528 run.py:483] Algo bellman_ford step 1966 current loss 0.066939, current_train_items 62944.
I0304 19:28:53.982579 23118544486528 run.py:483] Algo bellman_ford step 1967 current loss 0.157095, current_train_items 62976.
I0304 19:28:54.012060 23118544486528 run.py:483] Algo bellman_ford step 1968 current loss 0.228131, current_train_items 63008.
I0304 19:28:54.044050 23118544486528 run.py:483] Algo bellman_ford step 1969 current loss 0.323032, current_train_items 63040.
I0304 19:28:54.063153 23118544486528 run.py:483] Algo bellman_ford step 1970 current loss 0.009366, current_train_items 63072.
I0304 19:28:54.079332 23118544486528 run.py:483] Algo bellman_ford step 1971 current loss 0.079890, current_train_items 63104.
I0304 19:28:54.102058 23118544486528 run.py:483] Algo bellman_ford step 1972 current loss 0.078623, current_train_items 63136.
I0304 19:28:54.131593 23118544486528 run.py:483] Algo bellman_ford step 1973 current loss 0.140893, current_train_items 63168.
I0304 19:28:54.167512 23118544486528 run.py:483] Algo bellman_ford step 1974 current loss 0.253034, current_train_items 63200.
I0304 19:28:54.186718 23118544486528 run.py:483] Algo bellman_ford step 1975 current loss 0.006904, current_train_items 63232.
I0304 19:28:54.202759 23118544486528 run.py:483] Algo bellman_ford step 1976 current loss 0.058791, current_train_items 63264.
I0304 19:28:54.225054 23118544486528 run.py:483] Algo bellman_ford step 1977 current loss 0.089539, current_train_items 63296.
I0304 19:28:54.253946 23118544486528 run.py:483] Algo bellman_ford step 1978 current loss 0.166316, current_train_items 63328.
I0304 19:28:54.287500 23118544486528 run.py:483] Algo bellman_ford step 1979 current loss 0.186796, current_train_items 63360.
I0304 19:28:54.306414 23118544486528 run.py:483] Algo bellman_ford step 1980 current loss 0.010210, current_train_items 63392.
I0304 19:28:54.323034 23118544486528 run.py:483] Algo bellman_ford step 1981 current loss 0.024508, current_train_items 63424.
I0304 19:28:54.346510 23118544486528 run.py:483] Algo bellman_ford step 1982 current loss 0.152848, current_train_items 63456.
I0304 19:28:54.375155 23118544486528 run.py:483] Algo bellman_ford step 1983 current loss 0.081438, current_train_items 63488.
I0304 19:28:54.410523 23118544486528 run.py:483] Algo bellman_ford step 1984 current loss 0.166875, current_train_items 63520.
I0304 19:28:54.429862 23118544486528 run.py:483] Algo bellman_ford step 1985 current loss 0.007555, current_train_items 63552.
I0304 19:28:54.446158 23118544486528 run.py:483] Algo bellman_ford step 1986 current loss 0.058826, current_train_items 63584.
I0304 19:28:54.468485 23118544486528 run.py:483] Algo bellman_ford step 1987 current loss 0.105993, current_train_items 63616.
I0304 19:28:54.497153 23118544486528 run.py:483] Algo bellman_ford step 1988 current loss 0.134423, current_train_items 63648.
I0304 19:28:54.530599 23118544486528 run.py:483] Algo bellman_ford step 1989 current loss 0.119370, current_train_items 63680.
I0304 19:28:54.549752 23118544486528 run.py:483] Algo bellman_ford step 1990 current loss 0.021380, current_train_items 63712.
I0304 19:28:54.566154 23118544486528 run.py:483] Algo bellman_ford step 1991 current loss 0.042340, current_train_items 63744.
I0304 19:28:54.589645 23118544486528 run.py:483] Algo bellman_ford step 1992 current loss 0.137554, current_train_items 63776.
I0304 19:28:54.619463 23118544486528 run.py:483] Algo bellman_ford step 1993 current loss 0.262592, current_train_items 63808.
I0304 19:28:54.648123 23118544486528 run.py:483] Algo bellman_ford step 1994 current loss 0.082967, current_train_items 63840.
I0304 19:28:54.667414 23118544486528 run.py:483] Algo bellman_ford step 1995 current loss 0.059987, current_train_items 63872.
I0304 19:28:54.684238 23118544486528 run.py:483] Algo bellman_ford step 1996 current loss 0.046750, current_train_items 63904.
I0304 19:28:54.707445 23118544486528 run.py:483] Algo bellman_ford step 1997 current loss 0.109786, current_train_items 63936.
I0304 19:28:54.738422 23118544486528 run.py:483] Algo bellman_ford step 1998 current loss 0.144114, current_train_items 63968.
I0304 19:28:54.770430 23118544486528 run.py:483] Algo bellman_ford step 1999 current loss 0.146586, current_train_items 64000.
I0304 19:28:54.789531 23118544486528 run.py:483] Algo bellman_ford step 2000 current loss 0.010804, current_train_items 64032.
I0304 19:28:54.797024 23118544486528 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0304 19:28:54.797131 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:28:54.813754 23118544486528 run.py:483] Algo bellman_ford step 2001 current loss 0.042886, current_train_items 64064.
I0304 19:28:54.838346 23118544486528 run.py:483] Algo bellman_ford step 2002 current loss 0.125874, current_train_items 64096.
I0304 19:28:54.868876 23118544486528 run.py:483] Algo bellman_ford step 2003 current loss 0.122404, current_train_items 64128.
I0304 19:28:54.901261 23118544486528 run.py:483] Algo bellman_ford step 2004 current loss 0.138522, current_train_items 64160.
I0304 19:28:54.920424 23118544486528 run.py:483] Algo bellman_ford step 2005 current loss 0.012971, current_train_items 64192.
I0304 19:28:54.936448 23118544486528 run.py:483] Algo bellman_ford step 2006 current loss 0.037729, current_train_items 64224.
I0304 19:28:54.959961 23118544486528 run.py:483] Algo bellman_ford step 2007 current loss 0.059750, current_train_items 64256.
I0304 19:28:54.990507 23118544486528 run.py:483] Algo bellman_ford step 2008 current loss 0.134462, current_train_items 64288.
I0304 19:28:55.022465 23118544486528 run.py:483] Algo bellman_ford step 2009 current loss 0.108507, current_train_items 64320.
I0304 19:28:55.041568 23118544486528 run.py:483] Algo bellman_ford step 2010 current loss 0.024009, current_train_items 64352.
I0304 19:28:55.058285 23118544486528 run.py:483] Algo bellman_ford step 2011 current loss 0.028159, current_train_items 64384.
I0304 19:28:55.082015 23118544486528 run.py:483] Algo bellman_ford step 2012 current loss 0.075426, current_train_items 64416.
I0304 19:28:55.111299 23118544486528 run.py:483] Algo bellman_ford step 2013 current loss 0.101734, current_train_items 64448.
I0304 19:28:55.143658 23118544486528 run.py:483] Algo bellman_ford step 2014 current loss 0.099612, current_train_items 64480.
I0304 19:28:55.162944 23118544486528 run.py:483] Algo bellman_ford step 2015 current loss 0.019411, current_train_items 64512.
I0304 19:28:55.179634 23118544486528 run.py:483] Algo bellman_ford step 2016 current loss 0.042539, current_train_items 64544.
I0304 19:28:55.203509 23118544486528 run.py:483] Algo bellman_ford step 2017 current loss 0.096062, current_train_items 64576.
I0304 19:28:55.231191 23118544486528 run.py:483] Algo bellman_ford step 2018 current loss 0.076428, current_train_items 64608.
I0304 19:28:55.264641 23118544486528 run.py:483] Algo bellman_ford step 2019 current loss 0.122103, current_train_items 64640.
I0304 19:28:55.283552 23118544486528 run.py:483] Algo bellman_ford step 2020 current loss 0.010065, current_train_items 64672.
I0304 19:28:55.299717 23118544486528 run.py:483] Algo bellman_ford step 2021 current loss 0.019056, current_train_items 64704.
I0304 19:28:55.323429 23118544486528 run.py:483] Algo bellman_ford step 2022 current loss 0.111825, current_train_items 64736.
I0304 19:28:55.353008 23118544486528 run.py:483] Algo bellman_ford step 2023 current loss 0.098832, current_train_items 64768.
I0304 19:28:55.388615 23118544486528 run.py:483] Algo bellman_ford step 2024 current loss 0.089758, current_train_items 64800.
I0304 19:28:55.408120 23118544486528 run.py:483] Algo bellman_ford step 2025 current loss 0.036579, current_train_items 64832.
I0304 19:28:55.424160 23118544486528 run.py:483] Algo bellman_ford step 2026 current loss 0.068266, current_train_items 64864.
I0304 19:28:55.448382 23118544486528 run.py:483] Algo bellman_ford step 2027 current loss 0.121928, current_train_items 64896.
I0304 19:28:55.477833 23118544486528 run.py:483] Algo bellman_ford step 2028 current loss 0.070976, current_train_items 64928.
I0304 19:28:55.510199 23118544486528 run.py:483] Algo bellman_ford step 2029 current loss 0.113615, current_train_items 64960.
I0304 19:28:55.529282 23118544486528 run.py:483] Algo bellman_ford step 2030 current loss 0.023806, current_train_items 64992.
I0304 19:28:55.545988 23118544486528 run.py:483] Algo bellman_ford step 2031 current loss 0.075049, current_train_items 65024.
I0304 19:28:55.568996 23118544486528 run.py:483] Algo bellman_ford step 2032 current loss 0.082743, current_train_items 65056.
I0304 19:28:55.599700 23118544486528 run.py:483] Algo bellman_ford step 2033 current loss 0.125514, current_train_items 65088.
I0304 19:28:55.631326 23118544486528 run.py:483] Algo bellman_ford step 2034 current loss 0.120200, current_train_items 65120.
I0304 19:28:55.650180 23118544486528 run.py:483] Algo bellman_ford step 2035 current loss 0.013299, current_train_items 65152.
I0304 19:28:55.666166 23118544486528 run.py:483] Algo bellman_ford step 2036 current loss 0.028986, current_train_items 65184.
I0304 19:28:55.689017 23118544486528 run.py:483] Algo bellman_ford step 2037 current loss 0.134424, current_train_items 65216.
I0304 19:28:55.719055 23118544486528 run.py:483] Algo bellman_ford step 2038 current loss 0.159527, current_train_items 65248.
I0304 19:28:55.751967 23118544486528 run.py:483] Algo bellman_ford step 2039 current loss 0.091135, current_train_items 65280.
I0304 19:28:55.771081 23118544486528 run.py:483] Algo bellman_ford step 2040 current loss 0.037869, current_train_items 65312.
I0304 19:28:55.787074 23118544486528 run.py:483] Algo bellman_ford step 2041 current loss 0.069272, current_train_items 65344.
I0304 19:28:55.811676 23118544486528 run.py:483] Algo bellman_ford step 2042 current loss 0.126773, current_train_items 65376.
I0304 19:28:55.841841 23118544486528 run.py:483] Algo bellman_ford step 2043 current loss 0.085401, current_train_items 65408.
I0304 19:28:55.869242 23118544486528 run.py:483] Algo bellman_ford step 2044 current loss 0.052511, current_train_items 65440.
I0304 19:28:55.888325 23118544486528 run.py:483] Algo bellman_ford step 2045 current loss 0.010793, current_train_items 65472.
I0304 19:28:55.905094 23118544486528 run.py:483] Algo bellman_ford step 2046 current loss 0.052875, current_train_items 65504.
I0304 19:28:55.928905 23118544486528 run.py:483] Algo bellman_ford step 2047 current loss 0.127545, current_train_items 65536.
I0304 19:28:55.958485 23118544486528 run.py:483] Algo bellman_ford step 2048 current loss 0.104850, current_train_items 65568.
I0304 19:28:55.988942 23118544486528 run.py:483] Algo bellman_ford step 2049 current loss 0.074220, current_train_items 65600.
I0304 19:28:56.007987 23118544486528 run.py:483] Algo bellman_ford step 2050 current loss 0.017578, current_train_items 65632.
I0304 19:28:56.016221 23118544486528 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0304 19:28:56.016349 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0304 19:28:56.033714 23118544486528 run.py:483] Algo bellman_ford step 2051 current loss 0.049559, current_train_items 65664.
I0304 19:28:56.057071 23118544486528 run.py:483] Algo bellman_ford step 2052 current loss 0.131181, current_train_items 65696.
I0304 19:28:56.087028 23118544486528 run.py:483] Algo bellman_ford step 2053 current loss 0.164428, current_train_items 65728.
I0304 19:28:56.122958 23118544486528 run.py:483] Algo bellman_ford step 2054 current loss 0.216493, current_train_items 65760.
I0304 19:28:56.142202 23118544486528 run.py:483] Algo bellman_ford step 2055 current loss 0.006067, current_train_items 65792.
I0304 19:28:56.157991 23118544486528 run.py:483] Algo bellman_ford step 2056 current loss 0.031880, current_train_items 65824.
I0304 19:28:56.181278 23118544486528 run.py:483] Algo bellman_ford step 2057 current loss 0.115898, current_train_items 65856.
I0304 19:28:56.210567 23118544486528 run.py:483] Algo bellman_ford step 2058 current loss 0.148061, current_train_items 65888.
I0304 19:28:56.243776 23118544486528 run.py:483] Algo bellman_ford step 2059 current loss 0.220953, current_train_items 65920.
I0304 19:28:56.263192 23118544486528 run.py:483] Algo bellman_ford step 2060 current loss 0.010522, current_train_items 65952.
I0304 19:28:56.279675 23118544486528 run.py:483] Algo bellman_ford step 2061 current loss 0.047975, current_train_items 65984.
I0304 19:28:56.303263 23118544486528 run.py:483] Algo bellman_ford step 2062 current loss 0.115535, current_train_items 66016.
I0304 19:28:56.333346 23118544486528 run.py:483] Algo bellman_ford step 2063 current loss 0.111531, current_train_items 66048.
I0304 19:28:56.367718 23118544486528 run.py:483] Algo bellman_ford step 2064 current loss 0.110761, current_train_items 66080.
I0304 19:28:56.386741 23118544486528 run.py:483] Algo bellman_ford step 2065 current loss 0.008322, current_train_items 66112.
I0304 19:28:56.403259 23118544486528 run.py:483] Algo bellman_ford step 2066 current loss 0.043934, current_train_items 66144.
I0304 19:28:56.427223 23118544486528 run.py:483] Algo bellman_ford step 2067 current loss 0.085498, current_train_items 66176.
I0304 19:28:56.457548 23118544486528 run.py:483] Algo bellman_ford step 2068 current loss 0.142277, current_train_items 66208.
I0304 19:28:56.490066 23118544486528 run.py:483] Algo bellman_ford step 2069 current loss 0.105664, current_train_items 66240.
I0304 19:28:56.509456 23118544486528 run.py:483] Algo bellman_ford step 2070 current loss 0.009593, current_train_items 66272.
I0304 19:28:56.526136 23118544486528 run.py:483] Algo bellman_ford step 2071 current loss 0.032455, current_train_items 66304.
I0304 19:28:56.549540 23118544486528 run.py:483] Algo bellman_ford step 2072 current loss 0.135751, current_train_items 66336.
I0304 19:28:56.579704 23118544486528 run.py:483] Algo bellman_ford step 2073 current loss 0.222864, current_train_items 66368.
I0304 19:28:56.612478 23118544486528 run.py:483] Algo bellman_ford step 2074 current loss 0.209693, current_train_items 66400.
I0304 19:28:56.631716 23118544486528 run.py:483] Algo bellman_ford step 2075 current loss 0.015119, current_train_items 66432.
I0304 19:28:56.648465 23118544486528 run.py:483] Algo bellman_ford step 2076 current loss 0.049284, current_train_items 66464.
I0304 19:28:56.672258 23118544486528 run.py:483] Algo bellman_ford step 2077 current loss 0.107884, current_train_items 66496.
I0304 19:28:56.699950 23118544486528 run.py:483] Algo bellman_ford step 2078 current loss 0.084815, current_train_items 66528.
I0304 19:28:56.731948 23118544486528 run.py:483] Algo bellman_ford step 2079 current loss 0.099459, current_train_items 66560.
I0304 19:28:56.751244 23118544486528 run.py:483] Algo bellman_ford step 2080 current loss 0.007611, current_train_items 66592.
I0304 19:28:56.767476 23118544486528 run.py:483] Algo bellman_ford step 2081 current loss 0.036042, current_train_items 66624.
I0304 19:28:56.790829 23118544486528 run.py:483] Algo bellman_ford step 2082 current loss 0.086602, current_train_items 66656.
I0304 19:28:56.822183 23118544486528 run.py:483] Algo bellman_ford step 2083 current loss 0.139671, current_train_items 66688.
I0304 19:28:56.855388 23118544486528 run.py:483] Algo bellman_ford step 2084 current loss 0.131517, current_train_items 66720.
I0304 19:28:56.874908 23118544486528 run.py:483] Algo bellman_ford step 2085 current loss 0.007588, current_train_items 66752.
I0304 19:28:56.891192 23118544486528 run.py:483] Algo bellman_ford step 2086 current loss 0.060290, current_train_items 66784.
I0304 19:28:56.914839 23118544486528 run.py:483] Algo bellman_ford step 2087 current loss 0.105999, current_train_items 66816.
I0304 19:28:56.942488 23118544486528 run.py:483] Algo bellman_ford step 2088 current loss 0.079088, current_train_items 66848.
I0304 19:28:56.975586 23118544486528 run.py:483] Algo bellman_ford step 2089 current loss 0.144679, current_train_items 66880.
I0304 19:28:56.994877 23118544486528 run.py:483] Algo bellman_ford step 2090 current loss 0.011654, current_train_items 66912.
I0304 19:28:57.011312 23118544486528 run.py:483] Algo bellman_ford step 2091 current loss 0.057006, current_train_items 66944.
I0304 19:28:57.034247 23118544486528 run.py:483] Algo bellman_ford step 2092 current loss 0.097046, current_train_items 66976.
I0304 19:28:57.064217 23118544486528 run.py:483] Algo bellman_ford step 2093 current loss 0.127084, current_train_items 67008.
I0304 19:28:57.096788 23118544486528 run.py:483] Algo bellman_ford step 2094 current loss 0.222016, current_train_items 67040.
I0304 19:28:57.115816 23118544486528 run.py:483] Algo bellman_ford step 2095 current loss 0.035038, current_train_items 67072.
I0304 19:28:57.132186 23118544486528 run.py:483] Algo bellman_ford step 2096 current loss 0.040652, current_train_items 67104.
I0304 19:28:57.155453 23118544486528 run.py:483] Algo bellman_ford step 2097 current loss 0.108754, current_train_items 67136.
I0304 19:28:57.184444 23118544486528 run.py:483] Algo bellman_ford step 2098 current loss 0.102970, current_train_items 67168.
I0304 19:28:57.216825 23118544486528 run.py:483] Algo bellman_ford step 2099 current loss 0.150609, current_train_items 67200.
I0304 19:28:57.236403 23118544486528 run.py:483] Algo bellman_ford step 2100 current loss 0.010110, current_train_items 67232.
I0304 19:28:57.244365 23118544486528 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0304 19:28:57.244471 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:28:57.261013 23118544486528 run.py:483] Algo bellman_ford step 2101 current loss 0.032990, current_train_items 67264.
I0304 19:28:57.284783 23118544486528 run.py:483] Algo bellman_ford step 2102 current loss 0.117967, current_train_items 67296.
I0304 19:28:57.314983 23118544486528 run.py:483] Algo bellman_ford step 2103 current loss 0.081210, current_train_items 67328.
I0304 19:28:57.347991 23118544486528 run.py:483] Algo bellman_ford step 2104 current loss 0.097753, current_train_items 67360.
I0304 19:28:57.367719 23118544486528 run.py:483] Algo bellman_ford step 2105 current loss 0.011455, current_train_items 67392.
I0304 19:28:57.382923 23118544486528 run.py:483] Algo bellman_ford step 2106 current loss 0.016594, current_train_items 67424.
I0304 19:28:57.407430 23118544486528 run.py:483] Algo bellman_ford step 2107 current loss 0.084987, current_train_items 67456.
I0304 19:28:57.438744 23118544486528 run.py:483] Algo bellman_ford step 2108 current loss 0.174609, current_train_items 67488.
I0304 19:28:57.470311 23118544486528 run.py:483] Algo bellman_ford step 2109 current loss 0.098688, current_train_items 67520.
I0304 19:28:57.489669 23118544486528 run.py:483] Algo bellman_ford step 2110 current loss 0.018352, current_train_items 67552.
I0304 19:28:57.505929 23118544486528 run.py:483] Algo bellman_ford step 2111 current loss 0.019804, current_train_items 67584.
I0304 19:28:57.528892 23118544486528 run.py:483] Algo bellman_ford step 2112 current loss 0.084664, current_train_items 67616.
I0304 19:28:57.557963 23118544486528 run.py:483] Algo bellman_ford step 2113 current loss 0.083812, current_train_items 67648.
I0304 19:28:57.589184 23118544486528 run.py:483] Algo bellman_ford step 2114 current loss 0.114327, current_train_items 67680.
I0304 19:28:57.608503 23118544486528 run.py:483] Algo bellman_ford step 2115 current loss 0.018094, current_train_items 67712.
I0304 19:28:57.624661 23118544486528 run.py:483] Algo bellman_ford step 2116 current loss 0.043098, current_train_items 67744.
I0304 19:28:57.649016 23118544486528 run.py:483] Algo bellman_ford step 2117 current loss 0.092926, current_train_items 67776.
I0304 19:28:57.679939 23118544486528 run.py:483] Algo bellman_ford step 2118 current loss 0.137431, current_train_items 67808.
I0304 19:28:57.711978 23118544486528 run.py:483] Algo bellman_ford step 2119 current loss 0.109223, current_train_items 67840.
I0304 19:28:57.730793 23118544486528 run.py:483] Algo bellman_ford step 2120 current loss 0.009640, current_train_items 67872.
I0304 19:28:57.746784 23118544486528 run.py:483] Algo bellman_ford step 2121 current loss 0.055916, current_train_items 67904.
I0304 19:28:57.770356 23118544486528 run.py:483] Algo bellman_ford step 2122 current loss 0.052670, current_train_items 67936.
I0304 19:28:57.800706 23118544486528 run.py:483] Algo bellman_ford step 2123 current loss 0.074450, current_train_items 67968.
I0304 19:28:57.829798 23118544486528 run.py:483] Algo bellman_ford step 2124 current loss 0.100035, current_train_items 68000.
I0304 19:28:57.849017 23118544486528 run.py:483] Algo bellman_ford step 2125 current loss 0.007705, current_train_items 68032.
I0304 19:28:57.865667 23118544486528 run.py:483] Algo bellman_ford step 2126 current loss 0.066359, current_train_items 68064.
I0304 19:28:57.890827 23118544486528 run.py:483] Algo bellman_ford step 2127 current loss 0.062978, current_train_items 68096.
I0304 19:28:57.920504 23118544486528 run.py:483] Algo bellman_ford step 2128 current loss 0.116651, current_train_items 68128.
I0304 19:28:57.952608 23118544486528 run.py:483] Algo bellman_ford step 2129 current loss 0.090607, current_train_items 68160.
I0304 19:28:57.971714 23118544486528 run.py:483] Algo bellman_ford step 2130 current loss 0.007470, current_train_items 68192.
I0304 19:28:57.988567 23118544486528 run.py:483] Algo bellman_ford step 2131 current loss 0.045085, current_train_items 68224.
I0304 19:28:58.012794 23118544486528 run.py:483] Algo bellman_ford step 2132 current loss 0.088050, current_train_items 68256.
I0304 19:28:58.042063 23118544486528 run.py:483] Algo bellman_ford step 2133 current loss 0.102827, current_train_items 68288.
I0304 19:28:58.076114 23118544486528 run.py:483] Algo bellman_ford step 2134 current loss 0.111452, current_train_items 68320.
I0304 19:28:58.095248 23118544486528 run.py:483] Algo bellman_ford step 2135 current loss 0.007184, current_train_items 68352.
I0304 19:28:58.111681 23118544486528 run.py:483] Algo bellman_ford step 2136 current loss 0.034034, current_train_items 68384.
I0304 19:28:58.135209 23118544486528 run.py:483] Algo bellman_ford step 2137 current loss 0.051168, current_train_items 68416.
I0304 19:28:58.165158 23118544486528 run.py:483] Algo bellman_ford step 2138 current loss 0.096883, current_train_items 68448.
I0304 19:28:58.200910 23118544486528 run.py:483] Algo bellman_ford step 2139 current loss 0.147863, current_train_items 68480.
I0304 19:28:58.219949 23118544486528 run.py:483] Algo bellman_ford step 2140 current loss 0.026709, current_train_items 68512.
I0304 19:28:58.236205 23118544486528 run.py:483] Algo bellman_ford step 2141 current loss 0.019410, current_train_items 68544.
I0304 19:28:58.258229 23118544486528 run.py:483] Algo bellman_ford step 2142 current loss 0.027432, current_train_items 68576.
I0304 19:28:58.288274 23118544486528 run.py:483] Algo bellman_ford step 2143 current loss 0.083526, current_train_items 68608.
I0304 19:28:58.320919 23118544486528 run.py:483] Algo bellman_ford step 2144 current loss 0.119373, current_train_items 68640.
I0304 19:28:58.339946 23118544486528 run.py:483] Algo bellman_ford step 2145 current loss 0.022781, current_train_items 68672.
I0304 19:28:58.356542 23118544486528 run.py:483] Algo bellman_ford step 2146 current loss 0.053066, current_train_items 68704.
I0304 19:28:58.380642 23118544486528 run.py:483] Algo bellman_ford step 2147 current loss 0.128595, current_train_items 68736.
I0304 19:28:58.411248 23118544486528 run.py:483] Algo bellman_ford step 2148 current loss 0.111358, current_train_items 68768.
I0304 19:28:58.442778 23118544486528 run.py:483] Algo bellman_ford step 2149 current loss 0.104765, current_train_items 68800.
I0304 19:28:58.461636 23118544486528 run.py:483] Algo bellman_ford step 2150 current loss 0.009874, current_train_items 68832.
I0304 19:28:58.469653 23118544486528 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.953125, 'score': 0.953125, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0304 19:28:58.469765 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.953, val scores are: bellman_ford: 0.953
I0304 19:28:58.486704 23118544486528 run.py:483] Algo bellman_ford step 2151 current loss 0.054117, current_train_items 68864.
I0304 19:28:58.510920 23118544486528 run.py:483] Algo bellman_ford step 2152 current loss 0.134185, current_train_items 68896.
I0304 19:28:58.540763 23118544486528 run.py:483] Algo bellman_ford step 2153 current loss 0.098567, current_train_items 68928.
I0304 19:28:58.573050 23118544486528 run.py:483] Algo bellman_ford step 2154 current loss 0.143646, current_train_items 68960.
I0304 19:28:58.592655 23118544486528 run.py:483] Algo bellman_ford step 2155 current loss 0.007479, current_train_items 68992.
I0304 19:28:58.608957 23118544486528 run.py:483] Algo bellman_ford step 2156 current loss 0.033660, current_train_items 69024.
I0304 19:28:58.633012 23118544486528 run.py:483] Algo bellman_ford step 2157 current loss 0.051976, current_train_items 69056.
I0304 19:28:58.663048 23118544486528 run.py:483] Algo bellman_ford step 2158 current loss 0.074434, current_train_items 69088.
I0304 19:28:58.696018 23118544486528 run.py:483] Algo bellman_ford step 2159 current loss 0.156694, current_train_items 69120.
I0304 19:28:58.715623 23118544486528 run.py:483] Algo bellman_ford step 2160 current loss 0.025560, current_train_items 69152.
I0304 19:28:58.731890 23118544486528 run.py:483] Algo bellman_ford step 2161 current loss 0.017284, current_train_items 69184.
I0304 19:28:58.755557 23118544486528 run.py:483] Algo bellman_ford step 2162 current loss 0.156707, current_train_items 69216.
I0304 19:28:58.784082 23118544486528 run.py:483] Algo bellman_ford step 2163 current loss 0.109613, current_train_items 69248.
I0304 19:28:58.817631 23118544486528 run.py:483] Algo bellman_ford step 2164 current loss 0.114158, current_train_items 69280.
I0304 19:28:58.837516 23118544486528 run.py:483] Algo bellman_ford step 2165 current loss 0.007704, current_train_items 69312.
I0304 19:28:58.854291 23118544486528 run.py:483] Algo bellman_ford step 2166 current loss 0.113018, current_train_items 69344.
I0304 19:28:58.877280 23118544486528 run.py:483] Algo bellman_ford step 2167 current loss 0.097707, current_train_items 69376.
I0304 19:28:58.906039 23118544486528 run.py:483] Algo bellman_ford step 2168 current loss 0.077569, current_train_items 69408.
I0304 19:28:58.939269 23118544486528 run.py:483] Algo bellman_ford step 2169 current loss 0.117083, current_train_items 69440.
I0304 19:28:58.958523 23118544486528 run.py:483] Algo bellman_ford step 2170 current loss 0.021403, current_train_items 69472.
I0304 19:28:58.974643 23118544486528 run.py:483] Algo bellman_ford step 2171 current loss 0.052881, current_train_items 69504.
I0304 19:28:58.997707 23118544486528 run.py:483] Algo bellman_ford step 2172 current loss 0.172309, current_train_items 69536.
I0304 19:28:59.028476 23118544486528 run.py:483] Algo bellman_ford step 2173 current loss 0.284098, current_train_items 69568.
I0304 19:28:59.059534 23118544486528 run.py:483] Algo bellman_ford step 2174 current loss 0.258997, current_train_items 69600.
I0304 19:28:59.078839 23118544486528 run.py:483] Algo bellman_ford step 2175 current loss 0.023376, current_train_items 69632.
I0304 19:28:59.094990 23118544486528 run.py:483] Algo bellman_ford step 2176 current loss 0.033252, current_train_items 69664.
I0304 19:28:59.119130 23118544486528 run.py:483] Algo bellman_ford step 2177 current loss 0.130051, current_train_items 69696.
I0304 19:28:59.148075 23118544486528 run.py:483] Algo bellman_ford step 2178 current loss 0.176439, current_train_items 69728.
I0304 19:28:59.181668 23118544486528 run.py:483] Algo bellman_ford step 2179 current loss 0.196546, current_train_items 69760.
I0304 19:28:59.200824 23118544486528 run.py:483] Algo bellman_ford step 2180 current loss 0.022380, current_train_items 69792.
I0304 19:28:59.217180 23118544486528 run.py:483] Algo bellman_ford step 2181 current loss 0.039301, current_train_items 69824.
I0304 19:28:59.240844 23118544486528 run.py:483] Algo bellman_ford step 2182 current loss 0.045306, current_train_items 69856.
I0304 19:28:59.270089 23118544486528 run.py:483] Algo bellman_ford step 2183 current loss 0.098157, current_train_items 69888.
I0304 19:28:59.304143 23118544486528 run.py:483] Algo bellman_ford step 2184 current loss 0.188920, current_train_items 69920.
I0304 19:28:59.323740 23118544486528 run.py:483] Algo bellman_ford step 2185 current loss 0.011092, current_train_items 69952.
I0304 19:28:59.339881 23118544486528 run.py:483] Algo bellman_ford step 2186 current loss 0.038225, current_train_items 69984.
I0304 19:28:59.363724 23118544486528 run.py:483] Algo bellman_ford step 2187 current loss 0.062497, current_train_items 70016.
I0304 19:28:59.393776 23118544486528 run.py:483] Algo bellman_ford step 2188 current loss 0.053210, current_train_items 70048.
W0304 19:28:59.418423 23118544486528 samplers.py:155] Increasing hint lengh from 12 to 13
I0304 19:29:06.190851 23118544486528 run.py:483] Algo bellman_ford step 2189 current loss 0.171298, current_train_items 70080.
I0304 19:29:06.211487 23118544486528 run.py:483] Algo bellman_ford step 2190 current loss 0.008451, current_train_items 70112.
I0304 19:29:06.228401 23118544486528 run.py:483] Algo bellman_ford step 2191 current loss 0.064867, current_train_items 70144.
I0304 19:29:06.252226 23118544486528 run.py:483] Algo bellman_ford step 2192 current loss 0.096562, current_train_items 70176.
W0304 19:29:06.273744 23118544486528 samplers.py:155] Increasing hint lengh from 10 to 12
I0304 19:29:13.294682 23118544486528 run.py:483] Algo bellman_ford step 2193 current loss 0.136768, current_train_items 70208.
I0304 19:29:13.327236 23118544486528 run.py:483] Algo bellman_ford step 2194 current loss 0.144696, current_train_items 70240.
I0304 19:29:13.347564 23118544486528 run.py:483] Algo bellman_ford step 2195 current loss 0.032121, current_train_items 70272.
I0304 19:29:13.364678 23118544486528 run.py:483] Algo bellman_ford step 2196 current loss 0.028629, current_train_items 70304.
I0304 19:29:13.388084 23118544486528 run.py:483] Algo bellman_ford step 2197 current loss 0.061556, current_train_items 70336.
I0304 19:29:13.417183 23118544486528 run.py:483] Algo bellman_ford step 2198 current loss 0.097025, current_train_items 70368.
I0304 19:29:13.450407 23118544486528 run.py:483] Algo bellman_ford step 2199 current loss 0.097264, current_train_items 70400.
I0304 19:29:13.470577 23118544486528 run.py:483] Algo bellman_ford step 2200 current loss 0.025916, current_train_items 70432.
I0304 19:29:13.479775 23118544486528 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0304 19:29:13.479882 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:13.497184 23118544486528 run.py:483] Algo bellman_ford step 2201 current loss 0.049200, current_train_items 70464.
I0304 19:29:13.521244 23118544486528 run.py:483] Algo bellman_ford step 2202 current loss 0.055271, current_train_items 70496.
I0304 19:29:13.552118 23118544486528 run.py:483] Algo bellman_ford step 2203 current loss 0.100208, current_train_items 70528.
I0304 19:29:13.586555 23118544486528 run.py:483] Algo bellman_ford step 2204 current loss 0.073185, current_train_items 70560.
I0304 19:29:13.606767 23118544486528 run.py:483] Algo bellman_ford step 2205 current loss 0.008004, current_train_items 70592.
I0304 19:29:13.623188 23118544486528 run.py:483] Algo bellman_ford step 2206 current loss 0.040174, current_train_items 70624.
I0304 19:29:13.646574 23118544486528 run.py:483] Algo bellman_ford step 2207 current loss 0.057826, current_train_items 70656.
I0304 19:29:13.677272 23118544486528 run.py:483] Algo bellman_ford step 2208 current loss 0.109064, current_train_items 70688.
I0304 19:29:13.712110 23118544486528 run.py:483] Algo bellman_ford step 2209 current loss 0.098388, current_train_items 70720.
I0304 19:29:13.731808 23118544486528 run.py:483] Algo bellman_ford step 2210 current loss 0.036625, current_train_items 70752.
I0304 19:29:13.748311 23118544486528 run.py:483] Algo bellman_ford step 2211 current loss 0.023566, current_train_items 70784.
I0304 19:29:13.772866 23118544486528 run.py:483] Algo bellman_ford step 2212 current loss 0.093092, current_train_items 70816.
I0304 19:29:13.803967 23118544486528 run.py:483] Algo bellman_ford step 2213 current loss 0.100014, current_train_items 70848.
I0304 19:29:13.838335 23118544486528 run.py:483] Algo bellman_ford step 2214 current loss 0.100535, current_train_items 70880.
I0304 19:29:13.858322 23118544486528 run.py:483] Algo bellman_ford step 2215 current loss 0.014066, current_train_items 70912.
I0304 19:29:13.874761 23118544486528 run.py:483] Algo bellman_ford step 2216 current loss 0.013060, current_train_items 70944.
I0304 19:29:13.897475 23118544486528 run.py:483] Algo bellman_ford step 2217 current loss 0.068564, current_train_items 70976.
I0304 19:29:13.929308 23118544486528 run.py:483] Algo bellman_ford step 2218 current loss 0.109500, current_train_items 71008.
I0304 19:29:13.962194 23118544486528 run.py:483] Algo bellman_ford step 2219 current loss 0.090520, current_train_items 71040.
I0304 19:29:13.982133 23118544486528 run.py:483] Algo bellman_ford step 2220 current loss 0.011248, current_train_items 71072.
I0304 19:29:13.998473 23118544486528 run.py:483] Algo bellman_ford step 2221 current loss 0.054920, current_train_items 71104.
I0304 19:29:14.021909 23118544486528 run.py:483] Algo bellman_ford step 2222 current loss 0.120030, current_train_items 71136.
I0304 19:29:14.053436 23118544486528 run.py:483] Algo bellman_ford step 2223 current loss 0.123733, current_train_items 71168.
I0304 19:29:14.088157 23118544486528 run.py:483] Algo bellman_ford step 2224 current loss 0.166383, current_train_items 71200.
I0304 19:29:14.108184 23118544486528 run.py:483] Algo bellman_ford step 2225 current loss 0.016752, current_train_items 71232.
I0304 19:29:14.124765 23118544486528 run.py:483] Algo bellman_ford step 2226 current loss 0.035914, current_train_items 71264.
I0304 19:29:14.149305 23118544486528 run.py:483] Algo bellman_ford step 2227 current loss 0.145108, current_train_items 71296.
I0304 19:29:14.180116 23118544486528 run.py:483] Algo bellman_ford step 2228 current loss 0.181779, current_train_items 71328.
I0304 19:29:14.211615 23118544486528 run.py:483] Algo bellman_ford step 2229 current loss 0.179054, current_train_items 71360.
I0304 19:29:14.231805 23118544486528 run.py:483] Algo bellman_ford step 2230 current loss 0.010189, current_train_items 71392.
I0304 19:29:14.248441 23118544486528 run.py:483] Algo bellman_ford step 2231 current loss 0.048194, current_train_items 71424.
I0304 19:29:14.272009 23118544486528 run.py:483] Algo bellman_ford step 2232 current loss 0.093369, current_train_items 71456.
I0304 19:29:14.302539 23118544486528 run.py:483] Algo bellman_ford step 2233 current loss 0.125868, current_train_items 71488.
I0304 19:29:14.335734 23118544486528 run.py:483] Algo bellman_ford step 2234 current loss 0.105526, current_train_items 71520.
I0304 19:29:14.355778 23118544486528 run.py:483] Algo bellman_ford step 2235 current loss 0.006555, current_train_items 71552.
I0304 19:29:14.372455 23118544486528 run.py:483] Algo bellman_ford step 2236 current loss 0.053548, current_train_items 71584.
I0304 19:29:14.397598 23118544486528 run.py:483] Algo bellman_ford step 2237 current loss 0.059837, current_train_items 71616.
I0304 19:29:14.428913 23118544486528 run.py:483] Algo bellman_ford step 2238 current loss 0.075023, current_train_items 71648.
I0304 19:29:14.461733 23118544486528 run.py:483] Algo bellman_ford step 2239 current loss 0.107560, current_train_items 71680.
I0304 19:29:14.481641 23118544486528 run.py:483] Algo bellman_ford step 2240 current loss 0.025441, current_train_items 71712.
I0304 19:29:14.498822 23118544486528 run.py:483] Algo bellman_ford step 2241 current loss 0.032506, current_train_items 71744.
I0304 19:29:14.523664 23118544486528 run.py:483] Algo bellman_ford step 2242 current loss 0.104160, current_train_items 71776.
I0304 19:29:14.555047 23118544486528 run.py:483] Algo bellman_ford step 2243 current loss 0.128191, current_train_items 71808.
I0304 19:29:14.587152 23118544486528 run.py:483] Algo bellman_ford step 2244 current loss 0.096286, current_train_items 71840.
I0304 19:29:14.607143 23118544486528 run.py:483] Algo bellman_ford step 2245 current loss 0.020842, current_train_items 71872.
I0304 19:29:14.623883 23118544486528 run.py:483] Algo bellman_ford step 2246 current loss 0.074585, current_train_items 71904.
I0304 19:29:14.647056 23118544486528 run.py:483] Algo bellman_ford step 2247 current loss 0.079282, current_train_items 71936.
I0304 19:29:14.678883 23118544486528 run.py:483] Algo bellman_ford step 2248 current loss 0.121165, current_train_items 71968.
I0304 19:29:14.713041 23118544486528 run.py:483] Algo bellman_ford step 2249 current loss 0.112241, current_train_items 72000.
I0304 19:29:14.732794 23118544486528 run.py:483] Algo bellman_ford step 2250 current loss 0.010238, current_train_items 72032.
I0304 19:29:14.741290 23118544486528 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0304 19:29:14.741394 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0304 19:29:14.758253 23118544486528 run.py:483] Algo bellman_ford step 2251 current loss 0.070576, current_train_items 72064.
I0304 19:29:14.781429 23118544486528 run.py:483] Algo bellman_ford step 2252 current loss 0.039215, current_train_items 72096.
I0304 19:29:14.814406 23118544486528 run.py:483] Algo bellman_ford step 2253 current loss 0.192378, current_train_items 72128.
I0304 19:29:14.848917 23118544486528 run.py:483] Algo bellman_ford step 2254 current loss 0.110258, current_train_items 72160.
I0304 19:29:14.868982 23118544486528 run.py:483] Algo bellman_ford step 2255 current loss 0.015213, current_train_items 72192.
I0304 19:29:14.885279 23118544486528 run.py:483] Algo bellman_ford step 2256 current loss 0.042303, current_train_items 72224.
I0304 19:29:14.909010 23118544486528 run.py:483] Algo bellman_ford step 2257 current loss 0.097765, current_train_items 72256.
I0304 19:29:14.939271 23118544486528 run.py:483] Algo bellman_ford step 2258 current loss 0.140852, current_train_items 72288.
I0304 19:29:14.973915 23118544486528 run.py:483] Algo bellman_ford step 2259 current loss 0.128373, current_train_items 72320.
I0304 19:29:14.994004 23118544486528 run.py:483] Algo bellman_ford step 2260 current loss 0.013586, current_train_items 72352.
I0304 19:29:15.011052 23118544486528 run.py:483] Algo bellman_ford step 2261 current loss 0.035071, current_train_items 72384.
I0304 19:29:15.035932 23118544486528 run.py:483] Algo bellman_ford step 2262 current loss 0.127819, current_train_items 72416.
I0304 19:29:15.065944 23118544486528 run.py:483] Algo bellman_ford step 2263 current loss 0.076537, current_train_items 72448.
I0304 19:29:15.100067 23118544486528 run.py:483] Algo bellman_ford step 2264 current loss 0.178329, current_train_items 72480.
I0304 19:29:15.119956 23118544486528 run.py:483] Algo bellman_ford step 2265 current loss 0.024170, current_train_items 72512.
I0304 19:29:15.136326 23118544486528 run.py:483] Algo bellman_ford step 2266 current loss 0.069152, current_train_items 72544.
I0304 19:29:15.160956 23118544486528 run.py:483] Algo bellman_ford step 2267 current loss 0.077484, current_train_items 72576.
I0304 19:29:15.191989 23118544486528 run.py:483] Algo bellman_ford step 2268 current loss 0.153302, current_train_items 72608.
I0304 19:29:15.225058 23118544486528 run.py:483] Algo bellman_ford step 2269 current loss 0.129280, current_train_items 72640.
I0304 19:29:15.245095 23118544486528 run.py:483] Algo bellman_ford step 2270 current loss 0.009790, current_train_items 72672.
I0304 19:29:15.261364 23118544486528 run.py:483] Algo bellman_ford step 2271 current loss 0.024842, current_train_items 72704.
I0304 19:29:15.283981 23118544486528 run.py:483] Algo bellman_ford step 2272 current loss 0.118829, current_train_items 72736.
I0304 19:29:15.315478 23118544486528 run.py:483] Algo bellman_ford step 2273 current loss 0.133904, current_train_items 72768.
I0304 19:29:15.351985 23118544486528 run.py:483] Algo bellman_ford step 2274 current loss 0.153081, current_train_items 72800.
I0304 19:29:15.372241 23118544486528 run.py:483] Algo bellman_ford step 2275 current loss 0.008776, current_train_items 72832.
I0304 19:29:15.388805 23118544486528 run.py:483] Algo bellman_ford step 2276 current loss 0.020298, current_train_items 72864.
I0304 19:29:15.410951 23118544486528 run.py:483] Algo bellman_ford step 2277 current loss 0.047660, current_train_items 72896.
I0304 19:29:15.440922 23118544486528 run.py:483] Algo bellman_ford step 2278 current loss 0.150833, current_train_items 72928.
I0304 19:29:15.475687 23118544486528 run.py:483] Algo bellman_ford step 2279 current loss 0.161055, current_train_items 72960.
I0304 19:29:15.495735 23118544486528 run.py:483] Algo bellman_ford step 2280 current loss 0.006878, current_train_items 72992.
I0304 19:29:15.511853 23118544486528 run.py:483] Algo bellman_ford step 2281 current loss 0.058185, current_train_items 73024.
I0304 19:29:15.536387 23118544486528 run.py:483] Algo bellman_ford step 2282 current loss 0.058129, current_train_items 73056.
I0304 19:29:15.570076 23118544486528 run.py:483] Algo bellman_ford step 2283 current loss 0.148538, current_train_items 73088.
I0304 19:29:15.604902 23118544486528 run.py:483] Algo bellman_ford step 2284 current loss 0.107400, current_train_items 73120.
I0304 19:29:15.625178 23118544486528 run.py:483] Algo bellman_ford step 2285 current loss 0.012284, current_train_items 73152.
I0304 19:29:15.641738 23118544486528 run.py:483] Algo bellman_ford step 2286 current loss 0.041751, current_train_items 73184.
I0304 19:29:15.664900 23118544486528 run.py:483] Algo bellman_ford step 2287 current loss 0.059222, current_train_items 73216.
I0304 19:29:15.695345 23118544486528 run.py:483] Algo bellman_ford step 2288 current loss 0.071938, current_train_items 73248.
I0304 19:29:15.731539 23118544486528 run.py:483] Algo bellman_ford step 2289 current loss 0.112767, current_train_items 73280.
I0304 19:29:15.751653 23118544486528 run.py:483] Algo bellman_ford step 2290 current loss 0.005049, current_train_items 73312.
I0304 19:29:15.767573 23118544486528 run.py:483] Algo bellman_ford step 2291 current loss 0.015098, current_train_items 73344.
I0304 19:29:15.790796 23118544486528 run.py:483] Algo bellman_ford step 2292 current loss 0.077325, current_train_items 73376.
I0304 19:29:15.822334 23118544486528 run.py:483] Algo bellman_ford step 2293 current loss 0.086005, current_train_items 73408.
I0304 19:29:15.857016 23118544486528 run.py:483] Algo bellman_ford step 2294 current loss 0.140273, current_train_items 73440.
I0304 19:29:15.876459 23118544486528 run.py:483] Algo bellman_ford step 2295 current loss 0.023382, current_train_items 73472.
I0304 19:29:15.892873 23118544486528 run.py:483] Algo bellman_ford step 2296 current loss 0.065212, current_train_items 73504.
I0304 19:29:15.915798 23118544486528 run.py:483] Algo bellman_ford step 2297 current loss 0.036940, current_train_items 73536.
I0304 19:29:15.945718 23118544486528 run.py:483] Algo bellman_ford step 2298 current loss 0.099043, current_train_items 73568.
I0304 19:29:15.980059 23118544486528 run.py:483] Algo bellman_ford step 2299 current loss 0.126081, current_train_items 73600.
I0304 19:29:16.000406 23118544486528 run.py:483] Algo bellman_ford step 2300 current loss 0.011603, current_train_items 73632.
I0304 19:29:16.008325 23118544486528 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0304 19:29:16.008430 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:16.025514 23118544486528 run.py:483] Algo bellman_ford step 2301 current loss 0.044333, current_train_items 73664.
I0304 19:29:16.049842 23118544486528 run.py:483] Algo bellman_ford step 2302 current loss 0.080513, current_train_items 73696.
I0304 19:29:16.082047 23118544486528 run.py:483] Algo bellman_ford step 2303 current loss 0.083543, current_train_items 73728.
I0304 19:29:16.116405 23118544486528 run.py:483] Algo bellman_ford step 2304 current loss 0.162559, current_train_items 73760.
I0304 19:29:16.136506 23118544486528 run.py:483] Algo bellman_ford step 2305 current loss 0.080380, current_train_items 73792.
I0304 19:29:16.153116 23118544486528 run.py:483] Algo bellman_ford step 2306 current loss 0.051276, current_train_items 73824.
I0304 19:29:16.177000 23118544486528 run.py:483] Algo bellman_ford step 2307 current loss 0.069726, current_train_items 73856.
I0304 19:29:16.207035 23118544486528 run.py:483] Algo bellman_ford step 2308 current loss 0.086163, current_train_items 73888.
I0304 19:29:16.241981 23118544486528 run.py:483] Algo bellman_ford step 2309 current loss 0.101072, current_train_items 73920.
I0304 19:29:16.261662 23118544486528 run.py:483] Algo bellman_ford step 2310 current loss 0.008755, current_train_items 73952.
I0304 19:29:16.278363 23118544486528 run.py:483] Algo bellman_ford step 2311 current loss 0.025469, current_train_items 73984.
I0304 19:29:16.301895 23118544486528 run.py:483] Algo bellman_ford step 2312 current loss 0.078966, current_train_items 74016.
I0304 19:29:16.333201 23118544486528 run.py:483] Algo bellman_ford step 2313 current loss 0.100572, current_train_items 74048.
I0304 19:29:16.367496 23118544486528 run.py:483] Algo bellman_ford step 2314 current loss 0.072256, current_train_items 74080.
I0304 19:29:16.387555 23118544486528 run.py:483] Algo bellman_ford step 2315 current loss 0.015896, current_train_items 74112.
I0304 19:29:16.403834 23118544486528 run.py:483] Algo bellman_ford step 2316 current loss 0.076475, current_train_items 74144.
I0304 19:29:16.427038 23118544486528 run.py:483] Algo bellman_ford step 2317 current loss 0.050901, current_train_items 74176.
I0304 19:29:16.458673 23118544486528 run.py:483] Algo bellman_ford step 2318 current loss 0.059837, current_train_items 74208.
I0304 19:29:16.494097 23118544486528 run.py:483] Algo bellman_ford step 2319 current loss 0.126450, current_train_items 74240.
I0304 19:29:16.514018 23118544486528 run.py:483] Algo bellman_ford step 2320 current loss 0.008889, current_train_items 74272.
I0304 19:29:16.530215 23118544486528 run.py:483] Algo bellman_ford step 2321 current loss 0.015507, current_train_items 74304.
I0304 19:29:16.554507 23118544486528 run.py:483] Algo bellman_ford step 2322 current loss 0.103890, current_train_items 74336.
I0304 19:29:16.585106 23118544486528 run.py:483] Algo bellman_ford step 2323 current loss 0.118164, current_train_items 74368.
I0304 19:29:16.619697 23118544486528 run.py:483] Algo bellman_ford step 2324 current loss 0.146799, current_train_items 74400.
I0304 19:29:16.640084 23118544486528 run.py:483] Algo bellman_ford step 2325 current loss 0.014129, current_train_items 74432.
I0304 19:29:16.656329 23118544486528 run.py:483] Algo bellman_ford step 2326 current loss 0.038276, current_train_items 74464.
I0304 19:29:16.679279 23118544486528 run.py:483] Algo bellman_ford step 2327 current loss 0.156988, current_train_items 74496.
I0304 19:29:16.708815 23118544486528 run.py:483] Algo bellman_ford step 2328 current loss 0.093963, current_train_items 74528.
I0304 19:29:16.740995 23118544486528 run.py:483] Algo bellman_ford step 2329 current loss 0.158314, current_train_items 74560.
I0304 19:29:16.760905 23118544486528 run.py:483] Algo bellman_ford step 2330 current loss 0.014000, current_train_items 74592.
I0304 19:29:16.776922 23118544486528 run.py:483] Algo bellman_ford step 2331 current loss 0.039803, current_train_items 74624.
I0304 19:29:16.801339 23118544486528 run.py:483] Algo bellman_ford step 2332 current loss 0.134016, current_train_items 74656.
I0304 19:29:16.832774 23118544486528 run.py:483] Algo bellman_ford step 2333 current loss 0.108992, current_train_items 74688.
I0304 19:29:16.867887 23118544486528 run.py:483] Algo bellman_ford step 2334 current loss 0.125216, current_train_items 74720.
I0304 19:29:16.887374 23118544486528 run.py:483] Algo bellman_ford step 2335 current loss 0.011357, current_train_items 74752.
I0304 19:29:16.904093 23118544486528 run.py:483] Algo bellman_ford step 2336 current loss 0.038248, current_train_items 74784.
I0304 19:29:16.928076 23118544486528 run.py:483] Algo bellman_ford step 2337 current loss 0.051000, current_train_items 74816.
I0304 19:29:16.958754 23118544486528 run.py:483] Algo bellman_ford step 2338 current loss 0.083591, current_train_items 74848.
I0304 19:29:16.992211 23118544486528 run.py:483] Algo bellman_ford step 2339 current loss 0.115852, current_train_items 74880.
I0304 19:29:17.011944 23118544486528 run.py:483] Algo bellman_ford step 2340 current loss 0.010577, current_train_items 74912.
I0304 19:29:17.028492 23118544486528 run.py:483] Algo bellman_ford step 2341 current loss 0.024529, current_train_items 74944.
I0304 19:29:17.051515 23118544486528 run.py:483] Algo bellman_ford step 2342 current loss 0.074347, current_train_items 74976.
I0304 19:29:17.082013 23118544486528 run.py:483] Algo bellman_ford step 2343 current loss 0.108092, current_train_items 75008.
I0304 19:29:17.114528 23118544486528 run.py:483] Algo bellman_ford step 2344 current loss 0.104891, current_train_items 75040.
I0304 19:29:17.134344 23118544486528 run.py:483] Algo bellman_ford step 2345 current loss 0.019589, current_train_items 75072.
I0304 19:29:17.150555 23118544486528 run.py:483] Algo bellman_ford step 2346 current loss 0.066087, current_train_items 75104.
I0304 19:29:17.174424 23118544486528 run.py:483] Algo bellman_ford step 2347 current loss 0.040037, current_train_items 75136.
I0304 19:29:17.204076 23118544486528 run.py:483] Algo bellman_ford step 2348 current loss 0.069519, current_train_items 75168.
I0304 19:29:17.236748 23118544486528 run.py:483] Algo bellman_ford step 2349 current loss 0.103160, current_train_items 75200.
I0304 19:29:17.256402 23118544486528 run.py:483] Algo bellman_ford step 2350 current loss 0.008193, current_train_items 75232.
I0304 19:29:17.264819 23118544486528 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0304 19:29:17.264927 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:29:17.281879 23118544486528 run.py:483] Algo bellman_ford step 2351 current loss 0.027106, current_train_items 75264.
I0304 19:29:17.305962 23118544486528 run.py:483] Algo bellman_ford step 2352 current loss 0.124517, current_train_items 75296.
I0304 19:29:17.335349 23118544486528 run.py:483] Algo bellman_ford step 2353 current loss 0.079724, current_train_items 75328.
I0304 19:29:17.371228 23118544486528 run.py:483] Algo bellman_ford step 2354 current loss 0.115721, current_train_items 75360.
I0304 19:29:17.391667 23118544486528 run.py:483] Algo bellman_ford step 2355 current loss 0.025021, current_train_items 75392.
I0304 19:29:17.408047 23118544486528 run.py:483] Algo bellman_ford step 2356 current loss 0.027391, current_train_items 75424.
I0304 19:29:17.432442 23118544486528 run.py:483] Algo bellman_ford step 2357 current loss 0.091782, current_train_items 75456.
I0304 19:29:17.462013 23118544486528 run.py:483] Algo bellman_ford step 2358 current loss 0.108099, current_train_items 75488.
I0304 19:29:17.497638 23118544486528 run.py:483] Algo bellman_ford step 2359 current loss 0.127956, current_train_items 75520.
I0304 19:29:17.517568 23118544486528 run.py:483] Algo bellman_ford step 2360 current loss 0.010610, current_train_items 75552.
I0304 19:29:17.534703 23118544486528 run.py:483] Algo bellman_ford step 2361 current loss 0.068846, current_train_items 75584.
I0304 19:29:17.559585 23118544486528 run.py:483] Algo bellman_ford step 2362 current loss 0.068282, current_train_items 75616.
I0304 19:29:17.589982 23118544486528 run.py:483] Algo bellman_ford step 2363 current loss 0.090016, current_train_items 75648.
I0304 19:29:17.624049 23118544486528 run.py:483] Algo bellman_ford step 2364 current loss 0.094505, current_train_items 75680.
I0304 19:29:17.643788 23118544486528 run.py:483] Algo bellman_ford step 2365 current loss 0.004893, current_train_items 75712.
I0304 19:29:17.660281 23118544486528 run.py:483] Algo bellman_ford step 2366 current loss 0.027946, current_train_items 75744.
I0304 19:29:17.684103 23118544486528 run.py:483] Algo bellman_ford step 2367 current loss 0.191652, current_train_items 75776.
I0304 19:29:17.716086 23118544486528 run.py:483] Algo bellman_ford step 2368 current loss 0.215078, current_train_items 75808.
I0304 19:29:17.751497 23118544486528 run.py:483] Algo bellman_ford step 2369 current loss 0.153853, current_train_items 75840.
I0304 19:29:17.771914 23118544486528 run.py:483] Algo bellman_ford step 2370 current loss 0.006661, current_train_items 75872.
I0304 19:29:17.788230 23118544486528 run.py:483] Algo bellman_ford step 2371 current loss 0.091044, current_train_items 75904.
I0304 19:29:17.811797 23118544486528 run.py:483] Algo bellman_ford step 2372 current loss 0.086138, current_train_items 75936.
I0304 19:29:17.841764 23118544486528 run.py:483] Algo bellman_ford step 2373 current loss 0.103247, current_train_items 75968.
I0304 19:29:17.874878 23118544486528 run.py:483] Algo bellman_ford step 2374 current loss 0.136358, current_train_items 76000.
I0304 19:29:17.894635 23118544486528 run.py:483] Algo bellman_ford step 2375 current loss 0.004502, current_train_items 76032.
I0304 19:29:17.911147 23118544486528 run.py:483] Algo bellman_ford step 2376 current loss 0.024443, current_train_items 76064.
I0304 19:29:17.935033 23118544486528 run.py:483] Algo bellman_ford step 2377 current loss 0.098903, current_train_items 76096.
I0304 19:29:17.966798 23118544486528 run.py:483] Algo bellman_ford step 2378 current loss 0.093238, current_train_items 76128.
I0304 19:29:18.002058 23118544486528 run.py:483] Algo bellman_ford step 2379 current loss 0.114608, current_train_items 76160.
I0304 19:29:18.021980 23118544486528 run.py:483] Algo bellman_ford step 2380 current loss 0.019726, current_train_items 76192.
I0304 19:29:18.038823 23118544486528 run.py:483] Algo bellman_ford step 2381 current loss 0.049903, current_train_items 76224.
I0304 19:29:18.063080 23118544486528 run.py:483] Algo bellman_ford step 2382 current loss 0.115404, current_train_items 76256.
I0304 19:29:18.092916 23118544486528 run.py:483] Algo bellman_ford step 2383 current loss 0.114909, current_train_items 76288.
I0304 19:29:18.127556 23118544486528 run.py:483] Algo bellman_ford step 2384 current loss 0.148930, current_train_items 76320.
I0304 19:29:18.147621 23118544486528 run.py:483] Algo bellman_ford step 2385 current loss 0.006684, current_train_items 76352.
I0304 19:29:18.164173 23118544486528 run.py:483] Algo bellman_ford step 2386 current loss 0.023761, current_train_items 76384.
I0304 19:29:18.187183 23118544486528 run.py:483] Algo bellman_ford step 2387 current loss 0.056308, current_train_items 76416.
I0304 19:29:18.218002 23118544486528 run.py:483] Algo bellman_ford step 2388 current loss 0.085979, current_train_items 76448.
I0304 19:29:18.253748 23118544486528 run.py:483] Algo bellman_ford step 2389 current loss 0.143517, current_train_items 76480.
I0304 19:29:18.273909 23118544486528 run.py:483] Algo bellman_ford step 2390 current loss 0.006411, current_train_items 76512.
I0304 19:29:18.290124 23118544486528 run.py:483] Algo bellman_ford step 2391 current loss 0.042502, current_train_items 76544.
I0304 19:29:18.312510 23118544486528 run.py:483] Algo bellman_ford step 2392 current loss 0.073334, current_train_items 76576.
I0304 19:29:18.343012 23118544486528 run.py:483] Algo bellman_ford step 2393 current loss 0.107725, current_train_items 76608.
I0304 19:29:18.379830 23118544486528 run.py:483] Algo bellman_ford step 2394 current loss 0.102209, current_train_items 76640.
I0304 19:29:18.399737 23118544486528 run.py:483] Algo bellman_ford step 2395 current loss 0.010853, current_train_items 76672.
I0304 19:29:18.416043 23118544486528 run.py:483] Algo bellman_ford step 2396 current loss 0.013595, current_train_items 76704.
I0304 19:29:18.438857 23118544486528 run.py:483] Algo bellman_ford step 2397 current loss 0.042941, current_train_items 76736.
I0304 19:29:18.469637 23118544486528 run.py:483] Algo bellman_ford step 2398 current loss 0.131174, current_train_items 76768.
I0304 19:29:18.504817 23118544486528 run.py:483] Algo bellman_ford step 2399 current loss 0.145842, current_train_items 76800.
I0304 19:29:18.525346 23118544486528 run.py:483] Algo bellman_ford step 2400 current loss 0.007184, current_train_items 76832.
I0304 19:29:18.533086 23118544486528 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0304 19:29:18.533193 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:18.549854 23118544486528 run.py:483] Algo bellman_ford step 2401 current loss 0.034189, current_train_items 76864.
I0304 19:29:18.574237 23118544486528 run.py:483] Algo bellman_ford step 2402 current loss 0.092369, current_train_items 76896.
I0304 19:29:18.606276 23118544486528 run.py:483] Algo bellman_ford step 2403 current loss 0.128824, current_train_items 76928.
I0304 19:29:18.643768 23118544486528 run.py:483] Algo bellman_ford step 2404 current loss 0.107365, current_train_items 76960.
I0304 19:29:18.664157 23118544486528 run.py:483] Algo bellman_ford step 2405 current loss 0.008843, current_train_items 76992.
I0304 19:29:18.680317 23118544486528 run.py:483] Algo bellman_ford step 2406 current loss 0.033501, current_train_items 77024.
I0304 19:29:18.704213 23118544486528 run.py:483] Algo bellman_ford step 2407 current loss 0.081475, current_train_items 77056.
I0304 19:29:18.734010 23118544486528 run.py:483] Algo bellman_ford step 2408 current loss 0.090466, current_train_items 77088.
I0304 19:29:18.767860 23118544486528 run.py:483] Algo bellman_ford step 2409 current loss 0.171191, current_train_items 77120.
I0304 19:29:18.787440 23118544486528 run.py:483] Algo bellman_ford step 2410 current loss 0.004839, current_train_items 77152.
I0304 19:29:18.803974 23118544486528 run.py:483] Algo bellman_ford step 2411 current loss 0.019124, current_train_items 77184.
I0304 19:29:18.829230 23118544486528 run.py:483] Algo bellman_ford step 2412 current loss 0.057395, current_train_items 77216.
I0304 19:29:18.861021 23118544486528 run.py:483] Algo bellman_ford step 2413 current loss 0.097228, current_train_items 77248.
I0304 19:29:18.896127 23118544486528 run.py:483] Algo bellman_ford step 2414 current loss 0.091073, current_train_items 77280.
I0304 19:29:18.916347 23118544486528 run.py:483] Algo bellman_ford step 2415 current loss 0.007992, current_train_items 77312.
I0304 19:29:18.932227 23118544486528 run.py:483] Algo bellman_ford step 2416 current loss 0.040710, current_train_items 77344.
I0304 19:29:18.956295 23118544486528 run.py:483] Algo bellman_ford step 2417 current loss 0.077213, current_train_items 77376.
I0304 19:29:18.988495 23118544486528 run.py:483] Algo bellman_ford step 2418 current loss 0.129803, current_train_items 77408.
I0304 19:29:19.021020 23118544486528 run.py:483] Algo bellman_ford step 2419 current loss 0.077758, current_train_items 77440.
I0304 19:29:19.040689 23118544486528 run.py:483] Algo bellman_ford step 2420 current loss 0.013003, current_train_items 77472.
I0304 19:29:19.056813 23118544486528 run.py:483] Algo bellman_ford step 2421 current loss 0.029258, current_train_items 77504.
I0304 19:29:19.081436 23118544486528 run.py:483] Algo bellman_ford step 2422 current loss 0.081946, current_train_items 77536.
I0304 19:29:19.112885 23118544486528 run.py:483] Algo bellman_ford step 2423 current loss 0.079843, current_train_items 77568.
I0304 19:29:19.147957 23118544486528 run.py:483] Algo bellman_ford step 2424 current loss 0.153370, current_train_items 77600.
I0304 19:29:19.167682 23118544486528 run.py:483] Algo bellman_ford step 2425 current loss 0.018539, current_train_items 77632.
I0304 19:29:19.183830 23118544486528 run.py:483] Algo bellman_ford step 2426 current loss 0.072515, current_train_items 77664.
I0304 19:29:19.208787 23118544486528 run.py:483] Algo bellman_ford step 2427 current loss 0.154579, current_train_items 77696.
I0304 19:29:19.238919 23118544486528 run.py:483] Algo bellman_ford step 2428 current loss 0.155662, current_train_items 77728.
I0304 19:29:19.272671 23118544486528 run.py:483] Algo bellman_ford step 2429 current loss 0.127077, current_train_items 77760.
I0304 19:29:19.292539 23118544486528 run.py:483] Algo bellman_ford step 2430 current loss 0.017874, current_train_items 77792.
I0304 19:29:19.309159 23118544486528 run.py:483] Algo bellman_ford step 2431 current loss 0.028268, current_train_items 77824.
I0304 19:29:19.333692 23118544486528 run.py:483] Algo bellman_ford step 2432 current loss 0.046455, current_train_items 77856.
I0304 19:29:19.364769 23118544486528 run.py:483] Algo bellman_ford step 2433 current loss 0.059493, current_train_items 77888.
I0304 19:29:19.398963 23118544486528 run.py:483] Algo bellman_ford step 2434 current loss 0.089242, current_train_items 77920.
I0304 19:29:19.418552 23118544486528 run.py:483] Algo bellman_ford step 2435 current loss 0.015168, current_train_items 77952.
I0304 19:29:19.435174 23118544486528 run.py:483] Algo bellman_ford step 2436 current loss 0.027075, current_train_items 77984.
I0304 19:29:19.458824 23118544486528 run.py:483] Algo bellman_ford step 2437 current loss 0.076248, current_train_items 78016.
I0304 19:29:19.488618 23118544486528 run.py:483] Algo bellman_ford step 2438 current loss 0.095044, current_train_items 78048.
I0304 19:29:19.521695 23118544486528 run.py:483] Algo bellman_ford step 2439 current loss 0.086046, current_train_items 78080.
I0304 19:29:19.541676 23118544486528 run.py:483] Algo bellman_ford step 2440 current loss 0.026048, current_train_items 78112.
I0304 19:29:19.558187 23118544486528 run.py:483] Algo bellman_ford step 2441 current loss 0.042845, current_train_items 78144.
I0304 19:29:19.583271 23118544486528 run.py:483] Algo bellman_ford step 2442 current loss 0.099290, current_train_items 78176.
I0304 19:29:19.614320 23118544486528 run.py:483] Algo bellman_ford step 2443 current loss 0.072833, current_train_items 78208.
I0304 19:29:19.649674 23118544486528 run.py:483] Algo bellman_ford step 2444 current loss 0.125195, current_train_items 78240.
I0304 19:29:19.669946 23118544486528 run.py:483] Algo bellman_ford step 2445 current loss 0.056253, current_train_items 78272.
I0304 19:29:19.686490 23118544486528 run.py:483] Algo bellman_ford step 2446 current loss 0.057058, current_train_items 78304.
I0304 19:29:19.711390 23118544486528 run.py:483] Algo bellman_ford step 2447 current loss 0.093052, current_train_items 78336.
I0304 19:29:19.741296 23118544486528 run.py:483] Algo bellman_ford step 2448 current loss 0.072551, current_train_items 78368.
I0304 19:29:19.774741 23118544486528 run.py:483] Algo bellman_ford step 2449 current loss 0.103411, current_train_items 78400.
I0304 19:29:19.794602 23118544486528 run.py:483] Algo bellman_ford step 2450 current loss 0.011388, current_train_items 78432.
I0304 19:29:19.802475 23118544486528 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0304 19:29:19.802581 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:29:19.819755 23118544486528 run.py:483] Algo bellman_ford step 2451 current loss 0.020444, current_train_items 78464.
I0304 19:29:19.844047 23118544486528 run.py:483] Algo bellman_ford step 2452 current loss 0.065642, current_train_items 78496.
I0304 19:29:19.874858 23118544486528 run.py:483] Algo bellman_ford step 2453 current loss 0.060358, current_train_items 78528.
I0304 19:29:19.909765 23118544486528 run.py:483] Algo bellman_ford step 2454 current loss 0.085874, current_train_items 78560.
I0304 19:29:19.929846 23118544486528 run.py:483] Algo bellman_ford step 2455 current loss 0.013035, current_train_items 78592.
I0304 19:29:19.945963 23118544486528 run.py:483] Algo bellman_ford step 2456 current loss 0.035348, current_train_items 78624.
I0304 19:29:19.969138 23118544486528 run.py:483] Algo bellman_ford step 2457 current loss 0.050479, current_train_items 78656.
I0304 19:29:20.000133 23118544486528 run.py:483] Algo bellman_ford step 2458 current loss 0.088980, current_train_items 78688.
I0304 19:29:20.035853 23118544486528 run.py:483] Algo bellman_ford step 2459 current loss 0.201202, current_train_items 78720.
I0304 19:29:20.055896 23118544486528 run.py:483] Algo bellman_ford step 2460 current loss 0.007104, current_train_items 78752.
I0304 19:29:20.072756 23118544486528 run.py:483] Algo bellman_ford step 2461 current loss 0.044810, current_train_items 78784.
I0304 19:29:20.095898 23118544486528 run.py:483] Algo bellman_ford step 2462 current loss 0.055350, current_train_items 78816.
I0304 19:29:20.127490 23118544486528 run.py:483] Algo bellman_ford step 2463 current loss 0.117620, current_train_items 78848.
I0304 19:29:20.162028 23118544486528 run.py:483] Algo bellman_ford step 2464 current loss 0.125485, current_train_items 78880.
I0304 19:29:20.181512 23118544486528 run.py:483] Algo bellman_ford step 2465 current loss 0.008551, current_train_items 78912.
I0304 19:29:20.197793 23118544486528 run.py:483] Algo bellman_ford step 2466 current loss 0.046964, current_train_items 78944.
I0304 19:29:20.221755 23118544486528 run.py:483] Algo bellman_ford step 2467 current loss 0.056888, current_train_items 78976.
I0304 19:29:20.253206 23118544486528 run.py:483] Algo bellman_ford step 2468 current loss 0.101305, current_train_items 79008.
I0304 19:29:20.288867 23118544486528 run.py:483] Algo bellman_ford step 2469 current loss 0.133760, current_train_items 79040.
I0304 19:29:20.309127 23118544486528 run.py:483] Algo bellman_ford step 2470 current loss 0.013777, current_train_items 79072.
I0304 19:29:20.325580 23118544486528 run.py:483] Algo bellman_ford step 2471 current loss 0.092199, current_train_items 79104.
I0304 19:29:20.349671 23118544486528 run.py:483] Algo bellman_ford step 2472 current loss 0.091108, current_train_items 79136.
I0304 19:29:20.381400 23118544486528 run.py:483] Algo bellman_ford step 2473 current loss 0.077334, current_train_items 79168.
I0304 19:29:20.414620 23118544486528 run.py:483] Algo bellman_ford step 2474 current loss 0.049348, current_train_items 79200.
I0304 19:29:20.434845 23118544486528 run.py:483] Algo bellman_ford step 2475 current loss 0.039616, current_train_items 79232.
I0304 19:29:20.451288 23118544486528 run.py:483] Algo bellman_ford step 2476 current loss 0.046188, current_train_items 79264.
I0304 19:29:20.473643 23118544486528 run.py:483] Algo bellman_ford step 2477 current loss 0.052147, current_train_items 79296.
I0304 19:29:20.503758 23118544486528 run.py:483] Algo bellman_ford step 2478 current loss 0.169858, current_train_items 79328.
I0304 19:29:20.539028 23118544486528 run.py:483] Algo bellman_ford step 2479 current loss 0.150278, current_train_items 79360.
I0304 19:29:20.558912 23118544486528 run.py:483] Algo bellman_ford step 2480 current loss 0.007142, current_train_items 79392.
I0304 19:29:20.575306 23118544486528 run.py:483] Algo bellman_ford step 2481 current loss 0.074810, current_train_items 79424.
I0304 19:29:20.599894 23118544486528 run.py:483] Algo bellman_ford step 2482 current loss 0.066604, current_train_items 79456.
I0304 19:29:20.631107 23118544486528 run.py:483] Algo bellman_ford step 2483 current loss 0.127007, current_train_items 79488.
I0304 19:29:20.666517 23118544486528 run.py:483] Algo bellman_ford step 2484 current loss 0.132210, current_train_items 79520.
I0304 19:29:20.686742 23118544486528 run.py:483] Algo bellman_ford step 2485 current loss 0.014145, current_train_items 79552.
I0304 19:29:20.703319 23118544486528 run.py:483] Algo bellman_ford step 2486 current loss 0.033778, current_train_items 79584.
I0304 19:29:20.727863 23118544486528 run.py:483] Algo bellman_ford step 2487 current loss 0.075062, current_train_items 79616.
I0304 19:29:20.756241 23118544486528 run.py:483] Algo bellman_ford step 2488 current loss 0.037680, current_train_items 79648.
I0304 19:29:20.791021 23118544486528 run.py:483] Algo bellman_ford step 2489 current loss 0.101132, current_train_items 79680.
I0304 19:29:20.811285 23118544486528 run.py:483] Algo bellman_ford step 2490 current loss 0.014168, current_train_items 79712.
I0304 19:29:20.827722 23118544486528 run.py:483] Algo bellman_ford step 2491 current loss 0.024975, current_train_items 79744.
I0304 19:29:20.851153 23118544486528 run.py:483] Algo bellman_ford step 2492 current loss 0.071070, current_train_items 79776.
I0304 19:29:20.881646 23118544486528 run.py:483] Algo bellman_ford step 2493 current loss 0.072798, current_train_items 79808.
I0304 19:29:20.914504 23118544486528 run.py:483] Algo bellman_ford step 2494 current loss 0.096608, current_train_items 79840.
I0304 19:29:20.934078 23118544486528 run.py:483] Algo bellman_ford step 2495 current loss 0.010696, current_train_items 79872.
I0304 19:29:20.950879 23118544486528 run.py:483] Algo bellman_ford step 2496 current loss 0.037228, current_train_items 79904.
I0304 19:29:20.974373 23118544486528 run.py:483] Algo bellman_ford step 2497 current loss 0.059221, current_train_items 79936.
I0304 19:29:21.003670 23118544486528 run.py:483] Algo bellman_ford step 2498 current loss 0.097210, current_train_items 79968.
I0304 19:29:21.037552 23118544486528 run.py:483] Algo bellman_ford step 2499 current loss 0.085340, current_train_items 80000.
I0304 19:29:21.057657 23118544486528 run.py:483] Algo bellman_ford step 2500 current loss 0.007553, current_train_items 80032.
I0304 19:29:21.065384 23118544486528 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0304 19:29:21.065493 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:21.083031 23118544486528 run.py:483] Algo bellman_ford step 2501 current loss 0.034132, current_train_items 80064.
I0304 19:29:21.107766 23118544486528 run.py:483] Algo bellman_ford step 2502 current loss 0.109160, current_train_items 80096.
I0304 19:29:21.137998 23118544486528 run.py:483] Algo bellman_ford step 2503 current loss 0.066293, current_train_items 80128.
I0304 19:29:21.173290 23118544486528 run.py:483] Algo bellman_ford step 2504 current loss 0.084418, current_train_items 80160.
I0304 19:29:21.193658 23118544486528 run.py:483] Algo bellman_ford step 2505 current loss 0.013938, current_train_items 80192.
I0304 19:29:21.209884 23118544486528 run.py:483] Algo bellman_ford step 2506 current loss 0.037451, current_train_items 80224.
I0304 19:29:21.234328 23118544486528 run.py:483] Algo bellman_ford step 2507 current loss 0.047780, current_train_items 80256.
I0304 19:29:21.264462 23118544486528 run.py:483] Algo bellman_ford step 2508 current loss 0.063578, current_train_items 80288.
I0304 19:29:21.299450 23118544486528 run.py:483] Algo bellman_ford step 2509 current loss 0.082830, current_train_items 80320.
I0304 19:29:21.319015 23118544486528 run.py:483] Algo bellman_ford step 2510 current loss 0.019115, current_train_items 80352.
I0304 19:29:21.335918 23118544486528 run.py:483] Algo bellman_ford step 2511 current loss 0.042410, current_train_items 80384.
I0304 19:29:21.359879 23118544486528 run.py:483] Algo bellman_ford step 2512 current loss 0.058320, current_train_items 80416.
I0304 19:29:21.390228 23118544486528 run.py:483] Algo bellman_ford step 2513 current loss 0.085439, current_train_items 80448.
I0304 19:29:21.425347 23118544486528 run.py:483] Algo bellman_ford step 2514 current loss 0.101358, current_train_items 80480.
I0304 19:29:21.444981 23118544486528 run.py:483] Algo bellman_ford step 2515 current loss 0.007673, current_train_items 80512.
I0304 19:29:21.461463 23118544486528 run.py:483] Algo bellman_ford step 2516 current loss 0.033661, current_train_items 80544.
I0304 19:29:21.485732 23118544486528 run.py:483] Algo bellman_ford step 2517 current loss 0.127808, current_train_items 80576.
I0304 19:29:21.515945 23118544486528 run.py:483] Algo bellman_ford step 2518 current loss 0.100925, current_train_items 80608.
I0304 19:29:21.550760 23118544486528 run.py:483] Algo bellman_ford step 2519 current loss 0.172948, current_train_items 80640.
I0304 19:29:21.570387 23118544486528 run.py:483] Algo bellman_ford step 2520 current loss 0.006050, current_train_items 80672.
I0304 19:29:21.586836 23118544486528 run.py:483] Algo bellman_ford step 2521 current loss 0.056996, current_train_items 80704.
I0304 19:29:21.609588 23118544486528 run.py:483] Algo bellman_ford step 2522 current loss 0.025379, current_train_items 80736.
I0304 19:29:21.639647 23118544486528 run.py:483] Algo bellman_ford step 2523 current loss 0.039705, current_train_items 80768.
I0304 19:29:21.673355 23118544486528 run.py:483] Algo bellman_ford step 2524 current loss 0.092287, current_train_items 80800.
I0304 19:29:21.693232 23118544486528 run.py:483] Algo bellman_ford step 2525 current loss 0.008078, current_train_items 80832.
I0304 19:29:21.709932 23118544486528 run.py:483] Algo bellman_ford step 2526 current loss 0.033164, current_train_items 80864.
I0304 19:29:21.733970 23118544486528 run.py:483] Algo bellman_ford step 2527 current loss 0.086334, current_train_items 80896.
I0304 19:29:21.763665 23118544486528 run.py:483] Algo bellman_ford step 2528 current loss 0.045262, current_train_items 80928.
I0304 19:29:21.798707 23118544486528 run.py:483] Algo bellman_ford step 2529 current loss 0.180474, current_train_items 80960.
I0304 19:29:21.818711 23118544486528 run.py:483] Algo bellman_ford step 2530 current loss 0.010340, current_train_items 80992.
I0304 19:29:21.834833 23118544486528 run.py:483] Algo bellman_ford step 2531 current loss 0.065347, current_train_items 81024.
I0304 19:29:21.858731 23118544486528 run.py:483] Algo bellman_ford step 2532 current loss 0.119112, current_train_items 81056.
I0304 19:29:21.889367 23118544486528 run.py:483] Algo bellman_ford step 2533 current loss 0.100222, current_train_items 81088.
I0304 19:29:21.923032 23118544486528 run.py:483] Algo bellman_ford step 2534 current loss 0.073543, current_train_items 81120.
I0304 19:29:21.943162 23118544486528 run.py:483] Algo bellman_ford step 2535 current loss 0.007658, current_train_items 81152.
I0304 19:29:21.959177 23118544486528 run.py:483] Algo bellman_ford step 2536 current loss 0.034209, current_train_items 81184.
W0304 19:29:21.976313 23118544486528 samplers.py:155] Increasing hint lengh from 9 to 10
I0304 19:29:28.704666 23118544486528 run.py:483] Algo bellman_ford step 2537 current loss 0.130328, current_train_items 81216.
I0304 19:29:28.737670 23118544486528 run.py:483] Algo bellman_ford step 2538 current loss 0.094072, current_train_items 81248.
I0304 19:29:28.772830 23118544486528 run.py:483] Algo bellman_ford step 2539 current loss 0.100691, current_train_items 81280.
I0304 19:29:28.793003 23118544486528 run.py:483] Algo bellman_ford step 2540 current loss 0.021391, current_train_items 81312.
I0304 19:29:28.809123 23118544486528 run.py:483] Algo bellman_ford step 2541 current loss 0.020855, current_train_items 81344.
I0304 19:29:28.833235 23118544486528 run.py:483] Algo bellman_ford step 2542 current loss 0.219562, current_train_items 81376.
I0304 19:29:28.863282 23118544486528 run.py:483] Algo bellman_ford step 2543 current loss 0.206805, current_train_items 81408.
I0304 19:29:28.897198 23118544486528 run.py:483] Algo bellman_ford step 2544 current loss 0.200056, current_train_items 81440.
I0304 19:29:28.917094 23118544486528 run.py:483] Algo bellman_ford step 2545 current loss 0.014236, current_train_items 81472.
I0304 19:29:28.933447 23118544486528 run.py:483] Algo bellman_ford step 2546 current loss 0.024709, current_train_items 81504.
I0304 19:29:28.957649 23118544486528 run.py:483] Algo bellman_ford step 2547 current loss 0.061382, current_train_items 81536.
I0304 19:29:28.989698 23118544486528 run.py:483] Algo bellman_ford step 2548 current loss 0.081758, current_train_items 81568.
I0304 19:29:29.022778 23118544486528 run.py:483] Algo bellman_ford step 2549 current loss 0.100403, current_train_items 81600.
I0304 19:29:29.042614 23118544486528 run.py:483] Algo bellman_ford step 2550 current loss 0.009451, current_train_items 81632.
I0304 19:29:29.052202 23118544486528 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0304 19:29:29.052310 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0304 19:29:29.069434 23118544486528 run.py:483] Algo bellman_ford step 2551 current loss 0.026354, current_train_items 81664.
I0304 19:29:29.094694 23118544486528 run.py:483] Algo bellman_ford step 2552 current loss 0.079436, current_train_items 81696.
I0304 19:29:29.127320 23118544486528 run.py:483] Algo bellman_ford step 2553 current loss 0.130594, current_train_items 81728.
I0304 19:29:29.162438 23118544486528 run.py:483] Algo bellman_ford step 2554 current loss 0.131932, current_train_items 81760.
I0304 19:29:29.182758 23118544486528 run.py:483] Algo bellman_ford step 2555 current loss 0.008476, current_train_items 81792.
I0304 19:29:29.199289 23118544486528 run.py:483] Algo bellman_ford step 2556 current loss 0.037763, current_train_items 81824.
I0304 19:29:29.223639 23118544486528 run.py:483] Algo bellman_ford step 2557 current loss 0.071482, current_train_items 81856.
I0304 19:29:29.255606 23118544486528 run.py:483] Algo bellman_ford step 2558 current loss 0.103251, current_train_items 81888.
I0304 19:29:29.291337 23118544486528 run.py:483] Algo bellman_ford step 2559 current loss 0.104788, current_train_items 81920.
I0304 19:29:29.311379 23118544486528 run.py:483] Algo bellman_ford step 2560 current loss 0.021224, current_train_items 81952.
I0304 19:29:29.328191 23118544486528 run.py:483] Algo bellman_ford step 2561 current loss 0.020786, current_train_items 81984.
I0304 19:29:29.351848 23118544486528 run.py:483] Algo bellman_ford step 2562 current loss 0.060457, current_train_items 82016.
I0304 19:29:29.384801 23118544486528 run.py:483] Algo bellman_ford step 2563 current loss 0.107164, current_train_items 82048.
I0304 19:29:29.419183 23118544486528 run.py:483] Algo bellman_ford step 2564 current loss 0.093228, current_train_items 82080.
I0304 19:29:29.439261 23118544486528 run.py:483] Algo bellman_ford step 2565 current loss 0.007594, current_train_items 82112.
I0304 19:29:29.455829 23118544486528 run.py:483] Algo bellman_ford step 2566 current loss 0.038524, current_train_items 82144.
I0304 19:29:29.479992 23118544486528 run.py:483] Algo bellman_ford step 2567 current loss 0.054086, current_train_items 82176.
I0304 19:29:29.512152 23118544486528 run.py:483] Algo bellman_ford step 2568 current loss 0.088507, current_train_items 82208.
I0304 19:29:29.545933 23118544486528 run.py:483] Algo bellman_ford step 2569 current loss 0.091622, current_train_items 82240.
I0304 19:29:29.565864 23118544486528 run.py:483] Algo bellman_ford step 2570 current loss 0.008308, current_train_items 82272.
I0304 19:29:29.582298 23118544486528 run.py:483] Algo bellman_ford step 2571 current loss 0.023229, current_train_items 82304.
I0304 19:29:29.605211 23118544486528 run.py:483] Algo bellman_ford step 2572 current loss 0.033242, current_train_items 82336.
I0304 19:29:29.636780 23118544486528 run.py:483] Algo bellman_ford step 2573 current loss 0.053060, current_train_items 82368.
I0304 19:29:29.672186 23118544486528 run.py:483] Algo bellman_ford step 2574 current loss 0.098316, current_train_items 82400.
I0304 19:29:29.692339 23118544486528 run.py:483] Algo bellman_ford step 2575 current loss 0.018573, current_train_items 82432.
I0304 19:29:29.708537 23118544486528 run.py:483] Algo bellman_ford step 2576 current loss 0.043640, current_train_items 82464.
I0304 19:29:29.732058 23118544486528 run.py:483] Algo bellman_ford step 2577 current loss 0.037216, current_train_items 82496.
I0304 19:29:29.764435 23118544486528 run.py:483] Algo bellman_ford step 2578 current loss 0.050500, current_train_items 82528.
I0304 19:29:29.797297 23118544486528 run.py:483] Algo bellman_ford step 2579 current loss 0.082099, current_train_items 82560.
I0304 19:29:29.817068 23118544486528 run.py:483] Algo bellman_ford step 2580 current loss 0.009320, current_train_items 82592.
I0304 19:29:29.833723 23118544486528 run.py:483] Algo bellman_ford step 2581 current loss 0.018414, current_train_items 82624.
I0304 19:29:29.857432 23118544486528 run.py:483] Algo bellman_ford step 2582 current loss 0.066541, current_train_items 82656.
I0304 19:29:29.888326 23118544486528 run.py:483] Algo bellman_ford step 2583 current loss 0.051960, current_train_items 82688.
I0304 19:29:29.923504 23118544486528 run.py:483] Algo bellman_ford step 2584 current loss 0.068878, current_train_items 82720.
I0304 19:29:29.943382 23118544486528 run.py:483] Algo bellman_ford step 2585 current loss 0.008514, current_train_items 82752.
I0304 19:29:29.959889 23118544486528 run.py:483] Algo bellman_ford step 2586 current loss 0.024916, current_train_items 82784.
I0304 19:29:29.983668 23118544486528 run.py:483] Algo bellman_ford step 2587 current loss 0.037563, current_train_items 82816.
I0304 19:29:30.014525 23118544486528 run.py:483] Algo bellman_ford step 2588 current loss 0.058538, current_train_items 82848.
I0304 19:29:30.049323 23118544486528 run.py:483] Algo bellman_ford step 2589 current loss 0.095173, current_train_items 82880.
I0304 19:29:30.069326 23118544486528 run.py:483] Algo bellman_ford step 2590 current loss 0.008041, current_train_items 82912.
I0304 19:29:30.086070 23118544486528 run.py:483] Algo bellman_ford step 2591 current loss 0.038761, current_train_items 82944.
I0304 19:29:30.110544 23118544486528 run.py:483] Algo bellman_ford step 2592 current loss 0.104487, current_train_items 82976.
I0304 19:29:30.143167 23118544486528 run.py:483] Algo bellman_ford step 2593 current loss 0.092431, current_train_items 83008.
I0304 19:29:30.175676 23118544486528 run.py:483] Algo bellman_ford step 2594 current loss 0.116279, current_train_items 83040.
I0304 19:29:30.195244 23118544486528 run.py:483] Algo bellman_ford step 2595 current loss 0.014409, current_train_items 83072.
I0304 19:29:30.211994 23118544486528 run.py:483] Algo bellman_ford step 2596 current loss 0.058476, current_train_items 83104.
I0304 19:29:30.237305 23118544486528 run.py:483] Algo bellman_ford step 2597 current loss 0.083222, current_train_items 83136.
I0304 19:29:30.268919 23118544486528 run.py:483] Algo bellman_ford step 2598 current loss 0.084283, current_train_items 83168.
I0304 19:29:30.302834 23118544486528 run.py:483] Algo bellman_ford step 2599 current loss 0.099623, current_train_items 83200.
I0304 19:29:30.323119 23118544486528 run.py:483] Algo bellman_ford step 2600 current loss 0.013526, current_train_items 83232.
I0304 19:29:30.331041 23118544486528 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0304 19:29:30.331145 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:30.348038 23118544486528 run.py:483] Algo bellman_ford step 2601 current loss 0.071926, current_train_items 83264.
I0304 19:29:30.372048 23118544486528 run.py:483] Algo bellman_ford step 2602 current loss 0.095599, current_train_items 83296.
I0304 19:29:30.401925 23118544486528 run.py:483] Algo bellman_ford step 2603 current loss 0.116536, current_train_items 83328.
I0304 19:29:30.436255 23118544486528 run.py:483] Algo bellman_ford step 2604 current loss 0.084367, current_train_items 83360.
I0304 19:29:30.456101 23118544486528 run.py:483] Algo bellman_ford step 2605 current loss 0.019007, current_train_items 83392.
I0304 19:29:30.472198 23118544486528 run.py:483] Algo bellman_ford step 2606 current loss 0.019931, current_train_items 83424.
I0304 19:29:30.496308 23118544486528 run.py:483] Algo bellman_ford step 2607 current loss 0.092551, current_train_items 83456.
I0304 19:29:30.528379 23118544486528 run.py:483] Algo bellman_ford step 2608 current loss 0.122046, current_train_items 83488.
I0304 19:29:30.563087 23118544486528 run.py:483] Algo bellman_ford step 2609 current loss 0.196975, current_train_items 83520.
I0304 19:29:30.582661 23118544486528 run.py:483] Algo bellman_ford step 2610 current loss 0.026517, current_train_items 83552.
I0304 19:29:30.598947 23118544486528 run.py:483] Algo bellman_ford step 2611 current loss 0.023166, current_train_items 83584.
I0304 19:29:30.622648 23118544486528 run.py:483] Algo bellman_ford step 2612 current loss 0.052744, current_train_items 83616.
I0304 19:29:30.653433 23118544486528 run.py:483] Algo bellman_ford step 2613 current loss 0.061869, current_train_items 83648.
I0304 19:29:30.686379 23118544486528 run.py:483] Algo bellman_ford step 2614 current loss 0.087715, current_train_items 83680.
I0304 19:29:30.706125 23118544486528 run.py:483] Algo bellman_ford step 2615 current loss 0.006975, current_train_items 83712.
I0304 19:29:30.722305 23118544486528 run.py:483] Algo bellman_ford step 2616 current loss 0.029428, current_train_items 83744.
I0304 19:29:30.747601 23118544486528 run.py:483] Algo bellman_ford step 2617 current loss 0.075834, current_train_items 83776.
I0304 19:29:30.779596 23118544486528 run.py:483] Algo bellman_ford step 2618 current loss 0.065254, current_train_items 83808.
I0304 19:29:30.812212 23118544486528 run.py:483] Algo bellman_ford step 2619 current loss 0.075742, current_train_items 83840.
I0304 19:29:30.831872 23118544486528 run.py:483] Algo bellman_ford step 2620 current loss 0.005763, current_train_items 83872.
I0304 19:29:30.848411 23118544486528 run.py:483] Algo bellman_ford step 2621 current loss 0.022824, current_train_items 83904.
I0304 19:29:30.872631 23118544486528 run.py:483] Algo bellman_ford step 2622 current loss 0.082803, current_train_items 83936.
I0304 19:29:30.903579 23118544486528 run.py:483] Algo bellman_ford step 2623 current loss 0.058866, current_train_items 83968.
I0304 19:29:30.936439 23118544486528 run.py:483] Algo bellman_ford step 2624 current loss 0.098244, current_train_items 84000.
I0304 19:29:30.956322 23118544486528 run.py:483] Algo bellman_ford step 2625 current loss 0.014958, current_train_items 84032.
I0304 19:29:30.972618 23118544486528 run.py:483] Algo bellman_ford step 2626 current loss 0.020244, current_train_items 84064.
I0304 19:29:30.996758 23118544486528 run.py:483] Algo bellman_ford step 2627 current loss 0.046064, current_train_items 84096.
I0304 19:29:31.028347 23118544486528 run.py:483] Algo bellman_ford step 2628 current loss 0.104479, current_train_items 84128.
I0304 19:29:31.062167 23118544486528 run.py:483] Algo bellman_ford step 2629 current loss 0.086061, current_train_items 84160.
I0304 19:29:31.082013 23118544486528 run.py:483] Algo bellman_ford step 2630 current loss 0.010539, current_train_items 84192.
I0304 19:29:31.098530 23118544486528 run.py:483] Algo bellman_ford step 2631 current loss 0.033472, current_train_items 84224.
I0304 19:29:31.122067 23118544486528 run.py:483] Algo bellman_ford step 2632 current loss 0.054689, current_train_items 84256.
I0304 19:29:31.152639 23118544486528 run.py:483] Algo bellman_ford step 2633 current loss 0.040906, current_train_items 84288.
I0304 19:29:31.188422 23118544486528 run.py:483] Algo bellman_ford step 2634 current loss 0.142024, current_train_items 84320.
I0304 19:29:31.208596 23118544486528 run.py:483] Algo bellman_ford step 2635 current loss 0.020309, current_train_items 84352.
I0304 19:29:31.224788 23118544486528 run.py:483] Algo bellman_ford step 2636 current loss 0.099301, current_train_items 84384.
I0304 19:29:31.249145 23118544486528 run.py:483] Algo bellman_ford step 2637 current loss 0.117694, current_train_items 84416.
I0304 19:29:31.281200 23118544486528 run.py:483] Algo bellman_ford step 2638 current loss 0.100302, current_train_items 84448.
I0304 19:29:31.315834 23118544486528 run.py:483] Algo bellman_ford step 2639 current loss 0.117874, current_train_items 84480.
I0304 19:29:31.335484 23118544486528 run.py:483] Algo bellman_ford step 2640 current loss 0.006304, current_train_items 84512.
I0304 19:29:31.351644 23118544486528 run.py:483] Algo bellman_ford step 2641 current loss 0.036091, current_train_items 84544.
I0304 19:29:31.374565 23118544486528 run.py:483] Algo bellman_ford step 2642 current loss 0.100941, current_train_items 84576.
I0304 19:29:31.405563 23118544486528 run.py:483] Algo bellman_ford step 2643 current loss 0.089059, current_train_items 84608.
I0304 19:29:31.440560 23118544486528 run.py:483] Algo bellman_ford step 2644 current loss 0.081946, current_train_items 84640.
I0304 19:29:31.460255 23118544486528 run.py:483] Algo bellman_ford step 2645 current loss 0.009606, current_train_items 84672.
I0304 19:29:31.476406 23118544486528 run.py:483] Algo bellman_ford step 2646 current loss 0.069126, current_train_items 84704.
I0304 19:29:31.500218 23118544486528 run.py:483] Algo bellman_ford step 2647 current loss 0.148121, current_train_items 84736.
I0304 19:29:31.531833 23118544486528 run.py:483] Algo bellman_ford step 2648 current loss 0.163854, current_train_items 84768.
I0304 19:29:31.567866 23118544486528 run.py:483] Algo bellman_ford step 2649 current loss 0.089115, current_train_items 84800.
I0304 19:29:31.587532 23118544486528 run.py:483] Algo bellman_ford step 2650 current loss 0.010090, current_train_items 84832.
I0304 19:29:31.595726 23118544486528 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0304 19:29:31.595834 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0304 19:29:31.612459 23118544486528 run.py:483] Algo bellman_ford step 2651 current loss 0.036346, current_train_items 84864.
I0304 19:29:31.636938 23118544486528 run.py:483] Algo bellman_ford step 2652 current loss 0.069748, current_train_items 84896.
I0304 19:29:31.669505 23118544486528 run.py:483] Algo bellman_ford step 2653 current loss 0.187477, current_train_items 84928.
I0304 19:29:31.703850 23118544486528 run.py:483] Algo bellman_ford step 2654 current loss 0.137612, current_train_items 84960.
I0304 19:29:31.723601 23118544486528 run.py:483] Algo bellman_ford step 2655 current loss 0.005058, current_train_items 84992.
I0304 19:29:31.739946 23118544486528 run.py:483] Algo bellman_ford step 2656 current loss 0.028264, current_train_items 85024.
I0304 19:29:31.763654 23118544486528 run.py:483] Algo bellman_ford step 2657 current loss 0.048140, current_train_items 85056.
I0304 19:29:31.794309 23118544486528 run.py:483] Algo bellman_ford step 2658 current loss 0.054994, current_train_items 85088.
I0304 19:29:31.830883 23118544486528 run.py:483] Algo bellman_ford step 2659 current loss 0.121104, current_train_items 85120.
I0304 19:29:31.851034 23118544486528 run.py:483] Algo bellman_ford step 2660 current loss 0.005492, current_train_items 85152.
I0304 19:29:31.868080 23118544486528 run.py:483] Algo bellman_ford step 2661 current loss 0.028307, current_train_items 85184.
I0304 19:29:31.892046 23118544486528 run.py:483] Algo bellman_ford step 2662 current loss 0.062238, current_train_items 85216.
I0304 19:29:31.923521 23118544486528 run.py:483] Algo bellman_ford step 2663 current loss 0.087900, current_train_items 85248.
I0304 19:29:31.960436 23118544486528 run.py:483] Algo bellman_ford step 2664 current loss 0.124606, current_train_items 85280.
I0304 19:29:31.980192 23118544486528 run.py:483] Algo bellman_ford step 2665 current loss 0.009465, current_train_items 85312.
I0304 19:29:31.996666 23118544486528 run.py:483] Algo bellman_ford step 2666 current loss 0.058992, current_train_items 85344.
I0304 19:29:32.021738 23118544486528 run.py:483] Algo bellman_ford step 2667 current loss 0.079290, current_train_items 85376.
I0304 19:29:32.054405 23118544486528 run.py:483] Algo bellman_ford step 2668 current loss 0.166150, current_train_items 85408.
I0304 19:29:32.087971 23118544486528 run.py:483] Algo bellman_ford step 2669 current loss 0.056074, current_train_items 85440.
I0304 19:29:32.107736 23118544486528 run.py:483] Algo bellman_ford step 2670 current loss 0.007726, current_train_items 85472.
I0304 19:29:32.124386 23118544486528 run.py:483] Algo bellman_ford step 2671 current loss 0.034922, current_train_items 85504.
I0304 19:29:32.148282 23118544486528 run.py:483] Algo bellman_ford step 2672 current loss 0.106615, current_train_items 85536.
I0304 19:29:32.181121 23118544486528 run.py:483] Algo bellman_ford step 2673 current loss 0.105174, current_train_items 85568.
I0304 19:29:32.217101 23118544486528 run.py:483] Algo bellman_ford step 2674 current loss 0.071854, current_train_items 85600.
I0304 19:29:32.237153 23118544486528 run.py:483] Algo bellman_ford step 2675 current loss 0.009723, current_train_items 85632.
I0304 19:29:32.253648 23118544486528 run.py:483] Algo bellman_ford step 2676 current loss 0.037869, current_train_items 85664.
I0304 19:29:32.278764 23118544486528 run.py:483] Algo bellman_ford step 2677 current loss 0.145047, current_train_items 85696.
I0304 19:29:32.308761 23118544486528 run.py:483] Algo bellman_ford step 2678 current loss 0.083841, current_train_items 85728.
I0304 19:29:32.341922 23118544486528 run.py:483] Algo bellman_ford step 2679 current loss 0.101660, current_train_items 85760.
I0304 19:29:32.361995 23118544486528 run.py:483] Algo bellman_ford step 2680 current loss 0.008799, current_train_items 85792.
I0304 19:29:32.378334 23118544486528 run.py:483] Algo bellman_ford step 2681 current loss 0.027350, current_train_items 85824.
I0304 19:29:32.402771 23118544486528 run.py:483] Algo bellman_ford step 2682 current loss 0.042209, current_train_items 85856.
I0304 19:29:32.435559 23118544486528 run.py:483] Algo bellman_ford step 2683 current loss 0.070260, current_train_items 85888.
I0304 19:29:32.470144 23118544486528 run.py:483] Algo bellman_ford step 2684 current loss 0.107235, current_train_items 85920.
I0304 19:29:32.489901 23118544486528 run.py:483] Algo bellman_ford step 2685 current loss 0.010405, current_train_items 85952.
I0304 19:29:32.506239 23118544486528 run.py:483] Algo bellman_ford step 2686 current loss 0.037781, current_train_items 85984.
I0304 19:29:32.530563 23118544486528 run.py:483] Algo bellman_ford step 2687 current loss 0.047433, current_train_items 86016.
I0304 19:29:32.561821 23118544486528 run.py:483] Algo bellman_ford step 2688 current loss 0.100715, current_train_items 86048.
I0304 19:29:32.594769 23118544486528 run.py:483] Algo bellman_ford step 2689 current loss 0.082563, current_train_items 86080.
I0304 19:29:32.614987 23118544486528 run.py:483] Algo bellman_ford step 2690 current loss 0.021463, current_train_items 86112.
I0304 19:29:32.631521 23118544486528 run.py:483] Algo bellman_ford step 2691 current loss 0.030998, current_train_items 86144.
I0304 19:29:32.656676 23118544486528 run.py:483] Algo bellman_ford step 2692 current loss 0.067529, current_train_items 86176.
I0304 19:29:32.689400 23118544486528 run.py:483] Algo bellman_ford step 2693 current loss 0.065038, current_train_items 86208.
I0304 19:29:32.723672 23118544486528 run.py:483] Algo bellman_ford step 2694 current loss 0.092762, current_train_items 86240.
I0304 19:29:32.743787 23118544486528 run.py:483] Algo bellman_ford step 2695 current loss 0.007890, current_train_items 86272.
I0304 19:29:32.759979 23118544486528 run.py:483] Algo bellman_ford step 2696 current loss 0.011255, current_train_items 86304.
I0304 19:29:32.784624 23118544486528 run.py:483] Algo bellman_ford step 2697 current loss 0.174621, current_train_items 86336.
I0304 19:29:32.815758 23118544486528 run.py:483] Algo bellman_ford step 2698 current loss 0.116923, current_train_items 86368.
I0304 19:29:32.848100 23118544486528 run.py:483] Algo bellman_ford step 2699 current loss 0.143868, current_train_items 86400.
I0304 19:29:32.867956 23118544486528 run.py:483] Algo bellman_ford step 2700 current loss 0.013401, current_train_items 86432.
I0304 19:29:32.875663 23118544486528 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0304 19:29:32.875775 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:29:32.893003 23118544486528 run.py:483] Algo bellman_ford step 2701 current loss 0.032839, current_train_items 86464.
I0304 19:29:32.917751 23118544486528 run.py:483] Algo bellman_ford step 2702 current loss 0.043894, current_train_items 86496.
I0304 19:29:32.951481 23118544486528 run.py:483] Algo bellman_ford step 2703 current loss 0.090605, current_train_items 86528.
I0304 19:29:32.988275 23118544486528 run.py:483] Algo bellman_ford step 2704 current loss 0.109080, current_train_items 86560.
I0304 19:29:33.008196 23118544486528 run.py:483] Algo bellman_ford step 2705 current loss 0.007423, current_train_items 86592.
I0304 19:29:33.024173 23118544486528 run.py:483] Algo bellman_ford step 2706 current loss 0.060065, current_train_items 86624.
I0304 19:29:33.048624 23118544486528 run.py:483] Algo bellman_ford step 2707 current loss 0.090669, current_train_items 86656.
I0304 19:29:33.078806 23118544486528 run.py:483] Algo bellman_ford step 2708 current loss 0.118967, current_train_items 86688.
I0304 19:29:33.111232 23118544486528 run.py:483] Algo bellman_ford step 2709 current loss 0.076429, current_train_items 86720.
I0304 19:29:33.131587 23118544486528 run.py:483] Algo bellman_ford step 2710 current loss 0.011627, current_train_items 86752.
I0304 19:29:33.148727 23118544486528 run.py:483] Algo bellman_ford step 2711 current loss 0.036627, current_train_items 86784.
I0304 19:29:33.173387 23118544486528 run.py:483] Algo bellman_ford step 2712 current loss 0.113464, current_train_items 86816.
I0304 19:29:33.205793 23118544486528 run.py:483] Algo bellman_ford step 2713 current loss 0.151179, current_train_items 86848.
I0304 19:29:33.242422 23118544486528 run.py:483] Algo bellman_ford step 2714 current loss 0.181596, current_train_items 86880.
I0304 19:29:33.262462 23118544486528 run.py:483] Algo bellman_ford step 2715 current loss 0.019105, current_train_items 86912.
I0304 19:29:33.278395 23118544486528 run.py:483] Algo bellman_ford step 2716 current loss 0.008598, current_train_items 86944.
I0304 19:29:33.303380 23118544486528 run.py:483] Algo bellman_ford step 2717 current loss 0.144793, current_train_items 86976.
I0304 19:29:33.333560 23118544486528 run.py:483] Algo bellman_ford step 2718 current loss 0.126469, current_train_items 87008.
I0304 19:29:33.367883 23118544486528 run.py:483] Algo bellman_ford step 2719 current loss 0.120285, current_train_items 87040.
I0304 19:29:33.387723 23118544486528 run.py:483] Algo bellman_ford step 2720 current loss 0.007179, current_train_items 87072.
I0304 19:29:33.404035 23118544486528 run.py:483] Algo bellman_ford step 2721 current loss 0.080862, current_train_items 87104.
I0304 19:29:33.427747 23118544486528 run.py:483] Algo bellman_ford step 2722 current loss 0.066595, current_train_items 87136.
I0304 19:29:33.460387 23118544486528 run.py:483] Algo bellman_ford step 2723 current loss 0.069168, current_train_items 87168.
I0304 19:29:33.494366 23118544486528 run.py:483] Algo bellman_ford step 2724 current loss 0.090115, current_train_items 87200.
I0304 19:29:33.514067 23118544486528 run.py:483] Algo bellman_ford step 2725 current loss 0.006136, current_train_items 87232.
I0304 19:29:33.530294 23118544486528 run.py:483] Algo bellman_ford step 2726 current loss 0.048093, current_train_items 87264.
I0304 19:29:33.554640 23118544486528 run.py:483] Algo bellman_ford step 2727 current loss 0.075822, current_train_items 87296.
I0304 19:29:33.586301 23118544486528 run.py:483] Algo bellman_ford step 2728 current loss 0.106445, current_train_items 87328.
I0304 19:29:33.620307 23118544486528 run.py:483] Algo bellman_ford step 2729 current loss 0.095212, current_train_items 87360.
I0304 19:29:33.640414 23118544486528 run.py:483] Algo bellman_ford step 2730 current loss 0.007140, current_train_items 87392.
I0304 19:29:33.656708 23118544486528 run.py:483] Algo bellman_ford step 2731 current loss 0.018713, current_train_items 87424.
I0304 19:29:33.679675 23118544486528 run.py:483] Algo bellman_ford step 2732 current loss 0.025290, current_train_items 87456.
I0304 19:29:33.709935 23118544486528 run.py:483] Algo bellman_ford step 2733 current loss 0.070093, current_train_items 87488.
I0304 19:29:33.744034 23118544486528 run.py:483] Algo bellman_ford step 2734 current loss 0.100956, current_train_items 87520.
I0304 19:29:33.763791 23118544486528 run.py:483] Algo bellman_ford step 2735 current loss 0.019576, current_train_items 87552.
I0304 19:29:33.779645 23118544486528 run.py:483] Algo bellman_ford step 2736 current loss 0.017246, current_train_items 87584.
I0304 19:29:33.803363 23118544486528 run.py:483] Algo bellman_ford step 2737 current loss 0.039078, current_train_items 87616.
I0304 19:29:33.834320 23118544486528 run.py:483] Algo bellman_ford step 2738 current loss 0.070098, current_train_items 87648.
I0304 19:29:33.867107 23118544486528 run.py:483] Algo bellman_ford step 2739 current loss 0.099333, current_train_items 87680.
I0304 19:29:33.887037 23118544486528 run.py:483] Algo bellman_ford step 2740 current loss 0.006103, current_train_items 87712.
I0304 19:29:33.903150 23118544486528 run.py:483] Algo bellman_ford step 2741 current loss 0.026211, current_train_items 87744.
I0304 19:29:33.927264 23118544486528 run.py:483] Algo bellman_ford step 2742 current loss 0.038664, current_train_items 87776.
I0304 19:29:33.960029 23118544486528 run.py:483] Algo bellman_ford step 2743 current loss 0.130553, current_train_items 87808.
I0304 19:29:33.993444 23118544486528 run.py:483] Algo bellman_ford step 2744 current loss 0.121779, current_train_items 87840.
I0304 19:29:34.012968 23118544486528 run.py:483] Algo bellman_ford step 2745 current loss 0.005883, current_train_items 87872.
I0304 19:29:34.029145 23118544486528 run.py:483] Algo bellman_ford step 2746 current loss 0.040971, current_train_items 87904.
I0304 19:29:34.053191 23118544486528 run.py:483] Algo bellman_ford step 2747 current loss 0.091364, current_train_items 87936.
I0304 19:29:34.085484 23118544486528 run.py:483] Algo bellman_ford step 2748 current loss 0.070019, current_train_items 87968.
I0304 19:29:34.119113 23118544486528 run.py:483] Algo bellman_ford step 2749 current loss 0.085534, current_train_items 88000.
I0304 19:29:34.139365 23118544486528 run.py:483] Algo bellman_ford step 2750 current loss 0.009415, current_train_items 88032.
I0304 19:29:34.147697 23118544486528 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0304 19:29:34.147831 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:34.164854 23118544486528 run.py:483] Algo bellman_ford step 2751 current loss 0.024075, current_train_items 88064.
I0304 19:29:34.189587 23118544486528 run.py:483] Algo bellman_ford step 2752 current loss 0.091138, current_train_items 88096.
I0304 19:29:34.222216 23118544486528 run.py:483] Algo bellman_ford step 2753 current loss 0.098220, current_train_items 88128.
I0304 19:29:34.255526 23118544486528 run.py:483] Algo bellman_ford step 2754 current loss 0.086419, current_train_items 88160.
I0304 19:29:34.275193 23118544486528 run.py:483] Algo bellman_ford step 2755 current loss 0.010186, current_train_items 88192.
I0304 19:29:34.291629 23118544486528 run.py:483] Algo bellman_ford step 2756 current loss 0.044680, current_train_items 88224.
I0304 19:29:34.315203 23118544486528 run.py:483] Algo bellman_ford step 2757 current loss 0.047183, current_train_items 88256.
I0304 19:29:34.347518 23118544486528 run.py:483] Algo bellman_ford step 2758 current loss 0.097449, current_train_items 88288.
I0304 19:29:34.384124 23118544486528 run.py:483] Algo bellman_ford step 2759 current loss 0.140410, current_train_items 88320.
I0304 19:29:34.404156 23118544486528 run.py:483] Algo bellman_ford step 2760 current loss 0.004577, current_train_items 88352.
I0304 19:29:34.421023 23118544486528 run.py:483] Algo bellman_ford step 2761 current loss 0.077599, current_train_items 88384.
I0304 19:29:34.445752 23118544486528 run.py:483] Algo bellman_ford step 2762 current loss 0.223849, current_train_items 88416.
I0304 19:29:34.477669 23118544486528 run.py:483] Algo bellman_ford step 2763 current loss 0.254773, current_train_items 88448.
I0304 19:29:34.511809 23118544486528 run.py:483] Algo bellman_ford step 2764 current loss 0.209893, current_train_items 88480.
I0304 19:29:34.531378 23118544486528 run.py:483] Algo bellman_ford step 2765 current loss 0.008684, current_train_items 88512.
I0304 19:29:34.548379 23118544486528 run.py:483] Algo bellman_ford step 2766 current loss 0.037019, current_train_items 88544.
I0304 19:29:34.572615 23118544486528 run.py:483] Algo bellman_ford step 2767 current loss 0.062168, current_train_items 88576.
I0304 19:29:34.605339 23118544486528 run.py:483] Algo bellman_ford step 2768 current loss 0.206766, current_train_items 88608.
I0304 19:29:34.639367 23118544486528 run.py:483] Algo bellman_ford step 2769 current loss 0.194998, current_train_items 88640.
I0304 19:29:34.659103 23118544486528 run.py:483] Algo bellman_ford step 2770 current loss 0.006233, current_train_items 88672.
I0304 19:29:34.675668 23118544486528 run.py:483] Algo bellman_ford step 2771 current loss 0.037295, current_train_items 88704.
I0304 19:29:34.700022 23118544486528 run.py:483] Algo bellman_ford step 2772 current loss 0.070137, current_train_items 88736.
I0304 19:29:34.731153 23118544486528 run.py:483] Algo bellman_ford step 2773 current loss 0.064239, current_train_items 88768.
I0304 19:29:34.765379 23118544486528 run.py:483] Algo bellman_ford step 2774 current loss 0.080055, current_train_items 88800.
I0304 19:29:34.785474 23118544486528 run.py:483] Algo bellman_ford step 2775 current loss 0.004923, current_train_items 88832.
I0304 19:29:34.802492 23118544486528 run.py:483] Algo bellman_ford step 2776 current loss 0.092008, current_train_items 88864.
I0304 19:29:34.826388 23118544486528 run.py:483] Algo bellman_ford step 2777 current loss 0.068432, current_train_items 88896.
I0304 19:29:34.858005 23118544486528 run.py:483] Algo bellman_ford step 2778 current loss 0.092019, current_train_items 88928.
I0304 19:29:34.892570 23118544486528 run.py:483] Algo bellman_ford step 2779 current loss 0.097486, current_train_items 88960.
I0304 19:29:34.912373 23118544486528 run.py:483] Algo bellman_ford step 2780 current loss 0.008737, current_train_items 88992.
I0304 19:29:34.928678 23118544486528 run.py:483] Algo bellman_ford step 2781 current loss 0.042154, current_train_items 89024.
I0304 19:29:34.953765 23118544486528 run.py:483] Algo bellman_ford step 2782 current loss 0.146699, current_train_items 89056.
I0304 19:29:34.985232 23118544486528 run.py:483] Algo bellman_ford step 2783 current loss 0.128258, current_train_items 89088.
I0304 19:29:35.019954 23118544486528 run.py:483] Algo bellman_ford step 2784 current loss 0.114656, current_train_items 89120.
I0304 19:29:35.040245 23118544486528 run.py:483] Algo bellman_ford step 2785 current loss 0.007642, current_train_items 89152.
I0304 19:29:35.056754 23118544486528 run.py:483] Algo bellman_ford step 2786 current loss 0.086014, current_train_items 89184.
I0304 19:29:35.079585 23118544486528 run.py:483] Algo bellman_ford step 2787 current loss 0.053433, current_train_items 89216.
I0304 19:29:35.110732 23118544486528 run.py:483] Algo bellman_ford step 2788 current loss 0.126657, current_train_items 89248.
I0304 19:29:35.141904 23118544486528 run.py:483] Algo bellman_ford step 2789 current loss 0.142638, current_train_items 89280.
I0304 19:29:35.162065 23118544486528 run.py:483] Algo bellman_ford step 2790 current loss 0.008436, current_train_items 89312.
I0304 19:29:35.178396 23118544486528 run.py:483] Algo bellman_ford step 2791 current loss 0.015261, current_train_items 89344.
I0304 19:29:35.202169 23118544486528 run.py:483] Algo bellman_ford step 2792 current loss 0.050534, current_train_items 89376.
I0304 19:29:35.233011 23118544486528 run.py:483] Algo bellman_ford step 2793 current loss 0.125767, current_train_items 89408.
I0304 19:29:35.269204 23118544486528 run.py:483] Algo bellman_ford step 2794 current loss 0.107187, current_train_items 89440.
I0304 19:29:35.289342 23118544486528 run.py:483] Algo bellman_ford step 2795 current loss 0.014116, current_train_items 89472.
I0304 19:29:35.306241 23118544486528 run.py:483] Algo bellman_ford step 2796 current loss 0.042611, current_train_items 89504.
I0304 19:29:35.330147 23118544486528 run.py:483] Algo bellman_ford step 2797 current loss 0.073762, current_train_items 89536.
I0304 19:29:35.362515 23118544486528 run.py:483] Algo bellman_ford step 2798 current loss 0.101583, current_train_items 89568.
I0304 19:29:35.396439 23118544486528 run.py:483] Algo bellman_ford step 2799 current loss 0.108276, current_train_items 89600.
I0304 19:29:35.416482 23118544486528 run.py:483] Algo bellman_ford step 2800 current loss 0.005839, current_train_items 89632.
I0304 19:29:35.424326 23118544486528 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0304 19:29:35.424432 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:29:35.441508 23118544486528 run.py:483] Algo bellman_ford step 2801 current loss 0.020416, current_train_items 89664.
I0304 19:29:35.466638 23118544486528 run.py:483] Algo bellman_ford step 2802 current loss 0.133866, current_train_items 89696.
I0304 19:29:35.498832 23118544486528 run.py:483] Algo bellman_ford step 2803 current loss 0.117133, current_train_items 89728.
I0304 19:29:35.533138 23118544486528 run.py:483] Algo bellman_ford step 2804 current loss 0.096221, current_train_items 89760.
I0304 19:29:35.553187 23118544486528 run.py:483] Algo bellman_ford step 2805 current loss 0.014468, current_train_items 89792.
I0304 19:29:35.569598 23118544486528 run.py:483] Algo bellman_ford step 2806 current loss 0.058046, current_train_items 89824.
I0304 19:29:35.593013 23118544486528 run.py:483] Algo bellman_ford step 2807 current loss 0.072711, current_train_items 89856.
I0304 19:29:35.625158 23118544486528 run.py:483] Algo bellman_ford step 2808 current loss 0.119058, current_train_items 89888.
I0304 19:29:35.660168 23118544486528 run.py:483] Algo bellman_ford step 2809 current loss 0.118922, current_train_items 89920.
I0304 19:29:35.680197 23118544486528 run.py:483] Algo bellman_ford step 2810 current loss 0.009124, current_train_items 89952.
I0304 19:29:35.696991 23118544486528 run.py:483] Algo bellman_ford step 2811 current loss 0.077514, current_train_items 89984.
I0304 19:29:35.720091 23118544486528 run.py:483] Algo bellman_ford step 2812 current loss 0.045008, current_train_items 90016.
I0304 19:29:35.752507 23118544486528 run.py:483] Algo bellman_ford step 2813 current loss 0.138408, current_train_items 90048.
I0304 19:29:35.786181 23118544486528 run.py:483] Algo bellman_ford step 2814 current loss 0.098664, current_train_items 90080.
I0304 19:29:35.805760 23118544486528 run.py:483] Algo bellman_ford step 2815 current loss 0.008850, current_train_items 90112.
I0304 19:29:35.822385 23118544486528 run.py:483] Algo bellman_ford step 2816 current loss 0.049117, current_train_items 90144.
I0304 19:29:35.846729 23118544486528 run.py:483] Algo bellman_ford step 2817 current loss 0.084103, current_train_items 90176.
I0304 19:29:35.875141 23118544486528 run.py:483] Algo bellman_ford step 2818 current loss 0.093737, current_train_items 90208.
I0304 19:29:35.909216 23118544486528 run.py:483] Algo bellman_ford step 2819 current loss 0.119716, current_train_items 90240.
I0304 19:29:35.928645 23118544486528 run.py:483] Algo bellman_ford step 2820 current loss 0.008778, current_train_items 90272.
I0304 19:29:35.945067 23118544486528 run.py:483] Algo bellman_ford step 2821 current loss 0.044366, current_train_items 90304.
I0304 19:29:35.969266 23118544486528 run.py:483] Algo bellman_ford step 2822 current loss 0.068241, current_train_items 90336.
I0304 19:29:36.000787 23118544486528 run.py:483] Algo bellman_ford step 2823 current loss 0.149093, current_train_items 90368.
I0304 19:29:36.037286 23118544486528 run.py:483] Algo bellman_ford step 2824 current loss 0.135555, current_train_items 90400.
I0304 19:29:36.057289 23118544486528 run.py:483] Algo bellman_ford step 2825 current loss 0.007640, current_train_items 90432.
I0304 19:29:36.073347 23118544486528 run.py:483] Algo bellman_ford step 2826 current loss 0.049802, current_train_items 90464.
I0304 19:29:36.097587 23118544486528 run.py:483] Algo bellman_ford step 2827 current loss 0.109873, current_train_items 90496.
I0304 19:29:36.129963 23118544486528 run.py:483] Algo bellman_ford step 2828 current loss 0.094244, current_train_items 90528.
I0304 19:29:36.162543 23118544486528 run.py:483] Algo bellman_ford step 2829 current loss 0.082013, current_train_items 90560.
I0304 19:29:36.182180 23118544486528 run.py:483] Algo bellman_ford step 2830 current loss 0.011050, current_train_items 90592.
I0304 19:29:36.198860 23118544486528 run.py:483] Algo bellman_ford step 2831 current loss 0.048744, current_train_items 90624.
I0304 19:29:36.222673 23118544486528 run.py:483] Algo bellman_ford step 2832 current loss 0.068387, current_train_items 90656.
I0304 19:29:36.253206 23118544486528 run.py:483] Algo bellman_ford step 2833 current loss 0.056511, current_train_items 90688.
I0304 19:29:36.287354 23118544486528 run.py:483] Algo bellman_ford step 2834 current loss 0.111093, current_train_items 90720.
I0304 19:29:36.306981 23118544486528 run.py:483] Algo bellman_ford step 2835 current loss 0.037412, current_train_items 90752.
I0304 19:29:36.322880 23118544486528 run.py:483] Algo bellman_ford step 2836 current loss 0.035081, current_train_items 90784.
I0304 19:29:36.346528 23118544486528 run.py:483] Algo bellman_ford step 2837 current loss 0.057515, current_train_items 90816.
I0304 19:29:36.378226 23118544486528 run.py:483] Algo bellman_ford step 2838 current loss 0.089715, current_train_items 90848.
I0304 19:29:36.414648 23118544486528 run.py:483] Algo bellman_ford step 2839 current loss 0.124295, current_train_items 90880.
I0304 19:29:36.434424 23118544486528 run.py:483] Algo bellman_ford step 2840 current loss 0.011167, current_train_items 90912.
I0304 19:29:36.450444 23118544486528 run.py:483] Algo bellman_ford step 2841 current loss 0.027027, current_train_items 90944.
I0304 19:29:36.474793 23118544486528 run.py:483] Algo bellman_ford step 2842 current loss 0.126075, current_train_items 90976.
I0304 19:29:36.507234 23118544486528 run.py:483] Algo bellman_ford step 2843 current loss 0.157352, current_train_items 91008.
I0304 19:29:36.541915 23118544486528 run.py:483] Algo bellman_ford step 2844 current loss 0.112000, current_train_items 91040.
I0304 19:29:36.561978 23118544486528 run.py:483] Algo bellman_ford step 2845 current loss 0.005867, current_train_items 91072.
I0304 19:29:36.578789 23118544486528 run.py:483] Algo bellman_ford step 2846 current loss 0.067189, current_train_items 91104.
I0304 19:29:36.602483 23118544486528 run.py:483] Algo bellman_ford step 2847 current loss 0.137017, current_train_items 91136.
I0304 19:29:36.634105 23118544486528 run.py:483] Algo bellman_ford step 2848 current loss 0.115455, current_train_items 91168.
I0304 19:29:36.669883 23118544486528 run.py:483] Algo bellman_ford step 2849 current loss 0.126215, current_train_items 91200.
I0304 19:29:36.689637 23118544486528 run.py:483] Algo bellman_ford step 2850 current loss 0.009259, current_train_items 91232.
I0304 19:29:36.697532 23118544486528 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0304 19:29:36.697637 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0304 19:29:36.714073 23118544486528 run.py:483] Algo bellman_ford step 2851 current loss 0.025868, current_train_items 91264.
I0304 19:29:36.738234 23118544486528 run.py:483] Algo bellman_ford step 2852 current loss 0.040168, current_train_items 91296.
I0304 19:29:36.770728 23118544486528 run.py:483] Algo bellman_ford step 2853 current loss 0.164934, current_train_items 91328.
I0304 19:29:36.804939 23118544486528 run.py:483] Algo bellman_ford step 2854 current loss 0.164629, current_train_items 91360.
I0304 19:29:36.824655 23118544486528 run.py:483] Algo bellman_ford step 2855 current loss 0.007696, current_train_items 91392.
I0304 19:29:36.840784 23118544486528 run.py:483] Algo bellman_ford step 2856 current loss 0.059248, current_train_items 91424.
I0304 19:29:36.865079 23118544486528 run.py:483] Algo bellman_ford step 2857 current loss 0.069816, current_train_items 91456.
I0304 19:29:36.895126 23118544486528 run.py:483] Algo bellman_ford step 2858 current loss 0.051516, current_train_items 91488.
I0304 19:29:36.927694 23118544486528 run.py:483] Algo bellman_ford step 2859 current loss 0.059671, current_train_items 91520.
I0304 19:29:36.947787 23118544486528 run.py:483] Algo bellman_ford step 2860 current loss 0.005838, current_train_items 91552.
I0304 19:29:36.964223 23118544486528 run.py:483] Algo bellman_ford step 2861 current loss 0.039561, current_train_items 91584.
I0304 19:29:36.988149 23118544486528 run.py:483] Algo bellman_ford step 2862 current loss 0.034454, current_train_items 91616.
I0304 19:29:37.020210 23118544486528 run.py:483] Algo bellman_ford step 2863 current loss 0.065668, current_train_items 91648.
I0304 19:29:37.051382 23118544486528 run.py:483] Algo bellman_ford step 2864 current loss 0.068227, current_train_items 91680.
I0304 19:29:37.071187 23118544486528 run.py:483] Algo bellman_ford step 2865 current loss 0.010477, current_train_items 91712.
I0304 19:29:37.087535 23118544486528 run.py:483] Algo bellman_ford step 2866 current loss 0.032229, current_train_items 91744.
I0304 19:29:37.111635 23118544486528 run.py:483] Algo bellman_ford step 2867 current loss 0.044225, current_train_items 91776.
I0304 19:29:37.142416 23118544486528 run.py:483] Algo bellman_ford step 2868 current loss 0.046546, current_train_items 91808.
I0304 19:29:37.177146 23118544486528 run.py:483] Algo bellman_ford step 2869 current loss 0.118645, current_train_items 91840.
I0304 19:29:37.197071 23118544486528 run.py:483] Algo bellman_ford step 2870 current loss 0.006162, current_train_items 91872.
I0304 19:29:37.213473 23118544486528 run.py:483] Algo bellman_ford step 2871 current loss 0.035637, current_train_items 91904.
I0304 19:29:37.237703 23118544486528 run.py:483] Algo bellman_ford step 2872 current loss 0.061057, current_train_items 91936.
I0304 19:29:37.270009 23118544486528 run.py:483] Algo bellman_ford step 2873 current loss 0.062027, current_train_items 91968.
I0304 19:29:37.303216 23118544486528 run.py:483] Algo bellman_ford step 2874 current loss 0.082465, current_train_items 92000.
I0304 19:29:37.323410 23118544486528 run.py:483] Algo bellman_ford step 2875 current loss 0.004228, current_train_items 92032.
I0304 19:29:37.339832 23118544486528 run.py:483] Algo bellman_ford step 2876 current loss 0.039221, current_train_items 92064.
I0304 19:29:37.363132 23118544486528 run.py:483] Algo bellman_ford step 2877 current loss 0.035147, current_train_items 92096.
I0304 19:29:37.395944 23118544486528 run.py:483] Algo bellman_ford step 2878 current loss 0.117538, current_train_items 92128.
I0304 19:29:37.429677 23118544486528 run.py:483] Algo bellman_ford step 2879 current loss 0.071782, current_train_items 92160.
I0304 19:29:37.449708 23118544486528 run.py:483] Algo bellman_ford step 2880 current loss 0.059704, current_train_items 92192.
I0304 19:29:37.465957 23118544486528 run.py:483] Algo bellman_ford step 2881 current loss 0.024714, current_train_items 92224.
I0304 19:29:37.490016 23118544486528 run.py:483] Algo bellman_ford step 2882 current loss 0.060595, current_train_items 92256.
I0304 19:29:37.521587 23118544486528 run.py:483] Algo bellman_ford step 2883 current loss 0.065164, current_train_items 92288.
I0304 19:29:37.553884 23118544486528 run.py:483] Algo bellman_ford step 2884 current loss 0.079888, current_train_items 92320.
I0304 19:29:37.573796 23118544486528 run.py:483] Algo bellman_ford step 2885 current loss 0.008525, current_train_items 92352.
I0304 19:29:37.590162 23118544486528 run.py:483] Algo bellman_ford step 2886 current loss 0.064907, current_train_items 92384.
I0304 19:29:37.614226 23118544486528 run.py:483] Algo bellman_ford step 2887 current loss 0.088958, current_train_items 92416.
I0304 19:29:37.644734 23118544486528 run.py:483] Algo bellman_ford step 2888 current loss 0.071994, current_train_items 92448.
I0304 19:29:37.679168 23118544486528 run.py:483] Algo bellman_ford step 2889 current loss 0.119057, current_train_items 92480.
I0304 19:29:37.699213 23118544486528 run.py:483] Algo bellman_ford step 2890 current loss 0.007740, current_train_items 92512.
I0304 19:29:37.715679 23118544486528 run.py:483] Algo bellman_ford step 2891 current loss 0.060045, current_train_items 92544.
I0304 19:29:37.740794 23118544486528 run.py:483] Algo bellman_ford step 2892 current loss 0.124606, current_train_items 92576.
I0304 19:29:37.771048 23118544486528 run.py:483] Algo bellman_ford step 2893 current loss 0.103393, current_train_items 92608.
I0304 19:29:37.805506 23118544486528 run.py:483] Algo bellman_ford step 2894 current loss 0.146042, current_train_items 92640.
I0304 19:29:37.825200 23118544486528 run.py:483] Algo bellman_ford step 2895 current loss 0.007297, current_train_items 92672.
I0304 19:29:37.842125 23118544486528 run.py:483] Algo bellman_ford step 2896 current loss 0.086635, current_train_items 92704.
I0304 19:29:37.864721 23118544486528 run.py:483] Algo bellman_ford step 2897 current loss 0.076162, current_train_items 92736.
I0304 19:29:37.895314 23118544486528 run.py:483] Algo bellman_ford step 2898 current loss 0.049641, current_train_items 92768.
I0304 19:29:37.926887 23118544486528 run.py:483] Algo bellman_ford step 2899 current loss 0.060903, current_train_items 92800.
I0304 19:29:37.946559 23118544486528 run.py:483] Algo bellman_ford step 2900 current loss 0.005394, current_train_items 92832.
I0304 19:29:37.954159 23118544486528 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0304 19:29:37.954266 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:37.970701 23118544486528 run.py:483] Algo bellman_ford step 2901 current loss 0.018459, current_train_items 92864.
I0304 19:29:37.994892 23118544486528 run.py:483] Algo bellman_ford step 2902 current loss 0.141992, current_train_items 92896.
I0304 19:29:38.026702 23118544486528 run.py:483] Algo bellman_ford step 2903 current loss 0.092260, current_train_items 92928.
I0304 19:29:38.061968 23118544486528 run.py:483] Algo bellman_ford step 2904 current loss 0.138458, current_train_items 92960.
I0304 19:29:38.082041 23118544486528 run.py:483] Algo bellman_ford step 2905 current loss 0.020818, current_train_items 92992.
I0304 19:29:38.098188 23118544486528 run.py:483] Algo bellman_ford step 2906 current loss 0.040534, current_train_items 93024.
I0304 19:29:38.123048 23118544486528 run.py:483] Algo bellman_ford step 2907 current loss 0.100275, current_train_items 93056.
I0304 19:29:38.156247 23118544486528 run.py:483] Algo bellman_ford step 2908 current loss 0.186802, current_train_items 93088.
I0304 19:29:38.191179 23118544486528 run.py:483] Algo bellman_ford step 2909 current loss 0.148492, current_train_items 93120.
I0304 19:29:38.211073 23118544486528 run.py:483] Algo bellman_ford step 2910 current loss 0.027894, current_train_items 93152.
I0304 19:29:38.228042 23118544486528 run.py:483] Algo bellman_ford step 2911 current loss 0.050565, current_train_items 93184.
I0304 19:29:38.252338 23118544486528 run.py:483] Algo bellman_ford step 2912 current loss 0.103353, current_train_items 93216.
I0304 19:29:38.283508 23118544486528 run.py:483] Algo bellman_ford step 2913 current loss 0.139631, current_train_items 93248.
I0304 19:29:38.318753 23118544486528 run.py:483] Algo bellman_ford step 2914 current loss 0.229822, current_train_items 93280.
I0304 19:29:38.338483 23118544486528 run.py:483] Algo bellman_ford step 2915 current loss 0.009786, current_train_items 93312.
I0304 19:29:38.355024 23118544486528 run.py:483] Algo bellman_ford step 2916 current loss 0.020860, current_train_items 93344.
I0304 19:29:38.378821 23118544486528 run.py:483] Algo bellman_ford step 2917 current loss 0.045019, current_train_items 93376.
I0304 19:29:38.411588 23118544486528 run.py:483] Algo bellman_ford step 2918 current loss 0.113386, current_train_items 93408.
I0304 19:29:38.447796 23118544486528 run.py:483] Algo bellman_ford step 2919 current loss 0.159396, current_train_items 93440.
I0304 19:29:38.467620 23118544486528 run.py:483] Algo bellman_ford step 2920 current loss 0.008725, current_train_items 93472.
I0304 19:29:38.484200 23118544486528 run.py:483] Algo bellman_ford step 2921 current loss 0.025816, current_train_items 93504.
I0304 19:29:38.509233 23118544486528 run.py:483] Algo bellman_ford step 2922 current loss 0.072784, current_train_items 93536.
I0304 19:29:38.539883 23118544486528 run.py:483] Algo bellman_ford step 2923 current loss 0.072899, current_train_items 93568.
I0304 19:29:38.576339 23118544486528 run.py:483] Algo bellman_ford step 2924 current loss 0.122644, current_train_items 93600.
I0304 19:29:38.596300 23118544486528 run.py:483] Algo bellman_ford step 2925 current loss 0.051701, current_train_items 93632.
I0304 19:29:38.612597 23118544486528 run.py:483] Algo bellman_ford step 2926 current loss 0.045399, current_train_items 93664.
I0304 19:29:38.637739 23118544486528 run.py:483] Algo bellman_ford step 2927 current loss 0.132607, current_train_items 93696.
I0304 19:29:38.670004 23118544486528 run.py:483] Algo bellman_ford step 2928 current loss 0.098184, current_train_items 93728.
I0304 19:29:38.704597 23118544486528 run.py:483] Algo bellman_ford step 2929 current loss 0.089322, current_train_items 93760.
I0304 19:29:38.724442 23118544486528 run.py:483] Algo bellman_ford step 2930 current loss 0.007852, current_train_items 93792.
I0304 19:29:38.740642 23118544486528 run.py:483] Algo bellman_ford step 2931 current loss 0.039416, current_train_items 93824.
I0304 19:29:38.766869 23118544486528 run.py:483] Algo bellman_ford step 2932 current loss 0.095754, current_train_items 93856.
I0304 19:29:38.797306 23118544486528 run.py:483] Algo bellman_ford step 2933 current loss 0.118001, current_train_items 93888.
I0304 19:29:38.831041 23118544486528 run.py:483] Algo bellman_ford step 2934 current loss 0.112999, current_train_items 93920.
I0304 19:29:38.851307 23118544486528 run.py:483] Algo bellman_ford step 2935 current loss 0.009818, current_train_items 93952.
I0304 19:29:38.867622 23118544486528 run.py:483] Algo bellman_ford step 2936 current loss 0.028838, current_train_items 93984.
I0304 19:29:38.892391 23118544486528 run.py:483] Algo bellman_ford step 2937 current loss 0.093205, current_train_items 94016.
I0304 19:29:38.922223 23118544486528 run.py:483] Algo bellman_ford step 2938 current loss 0.070542, current_train_items 94048.
I0304 19:29:38.956561 23118544486528 run.py:483] Algo bellman_ford step 2939 current loss 0.112778, current_train_items 94080.
I0304 19:29:38.976160 23118544486528 run.py:483] Algo bellman_ford step 2940 current loss 0.050027, current_train_items 94112.
I0304 19:29:38.992817 23118544486528 run.py:483] Algo bellman_ford step 2941 current loss 0.049717, current_train_items 94144.
I0304 19:29:39.017910 23118544486528 run.py:483] Algo bellman_ford step 2942 current loss 0.101089, current_train_items 94176.
I0304 19:29:39.050660 23118544486528 run.py:483] Algo bellman_ford step 2943 current loss 0.102663, current_train_items 94208.
I0304 19:29:39.086896 23118544486528 run.py:483] Algo bellman_ford step 2944 current loss 0.145525, current_train_items 94240.
I0304 19:29:39.106553 23118544486528 run.py:483] Algo bellman_ford step 2945 current loss 0.029397, current_train_items 94272.
I0304 19:29:39.123308 23118544486528 run.py:483] Algo bellman_ford step 2946 current loss 0.020963, current_train_items 94304.
I0304 19:29:39.147495 23118544486528 run.py:483] Algo bellman_ford step 2947 current loss 0.147069, current_train_items 94336.
I0304 19:29:39.178644 23118544486528 run.py:483] Algo bellman_ford step 2948 current loss 0.188402, current_train_items 94368.
I0304 19:29:39.212605 23118544486528 run.py:483] Algo bellman_ford step 2949 current loss 0.095392, current_train_items 94400.
I0304 19:29:39.232510 23118544486528 run.py:483] Algo bellman_ford step 2950 current loss 0.007466, current_train_items 94432.
I0304 19:29:39.240423 23118544486528 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0304 19:29:39.240528 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0304 19:29:39.257584 23118544486528 run.py:483] Algo bellman_ford step 2951 current loss 0.058161, current_train_items 94464.
I0304 19:29:39.280359 23118544486528 run.py:483] Algo bellman_ford step 2952 current loss 0.085187, current_train_items 94496.
I0304 19:29:39.311618 23118544486528 run.py:483] Algo bellman_ford step 2953 current loss 0.125167, current_train_items 94528.
I0304 19:29:39.346616 23118544486528 run.py:483] Algo bellman_ford step 2954 current loss 0.103489, current_train_items 94560.
I0304 19:29:39.366627 23118544486528 run.py:483] Algo bellman_ford step 2955 current loss 0.006961, current_train_items 94592.
I0304 19:29:39.382438 23118544486528 run.py:483] Algo bellman_ford step 2956 current loss 0.065633, current_train_items 94624.
I0304 19:29:39.407358 23118544486528 run.py:483] Algo bellman_ford step 2957 current loss 0.127756, current_train_items 94656.
I0304 19:29:39.440211 23118544486528 run.py:483] Algo bellman_ford step 2958 current loss 0.072772, current_train_items 94688.
I0304 19:29:39.474598 23118544486528 run.py:483] Algo bellman_ford step 2959 current loss 0.115714, current_train_items 94720.
I0304 19:29:39.494443 23118544486528 run.py:483] Algo bellman_ford step 2960 current loss 0.019646, current_train_items 94752.
I0304 19:29:39.511194 23118544486528 run.py:483] Algo bellman_ford step 2961 current loss 0.023243, current_train_items 94784.
I0304 19:29:39.535018 23118544486528 run.py:483] Algo bellman_ford step 2962 current loss 0.141049, current_train_items 94816.
I0304 19:29:39.565718 23118544486528 run.py:483] Algo bellman_ford step 2963 current loss 0.103001, current_train_items 94848.
I0304 19:29:39.600883 23118544486528 run.py:483] Algo bellman_ford step 2964 current loss 0.112691, current_train_items 94880.
I0304 19:29:39.620463 23118544486528 run.py:483] Algo bellman_ford step 2965 current loss 0.004172, current_train_items 94912.
I0304 19:29:39.637014 23118544486528 run.py:483] Algo bellman_ford step 2966 current loss 0.033794, current_train_items 94944.
I0304 19:29:39.661226 23118544486528 run.py:483] Algo bellman_ford step 2967 current loss 0.104031, current_train_items 94976.
I0304 19:29:39.692625 23118544486528 run.py:483] Algo bellman_ford step 2968 current loss 0.104216, current_train_items 95008.
I0304 19:29:39.727632 23118544486528 run.py:483] Algo bellman_ford step 2969 current loss 0.134263, current_train_items 95040.
I0304 19:29:39.748426 23118544486528 run.py:483] Algo bellman_ford step 2970 current loss 0.009455, current_train_items 95072.
I0304 19:29:39.764817 23118544486528 run.py:483] Algo bellman_ford step 2971 current loss 0.025777, current_train_items 95104.
I0304 19:29:39.788932 23118544486528 run.py:483] Algo bellman_ford step 2972 current loss 0.124845, current_train_items 95136.
I0304 19:29:39.819378 23118544486528 run.py:483] Algo bellman_ford step 2973 current loss 0.087302, current_train_items 95168.
I0304 19:29:39.856492 23118544486528 run.py:483] Algo bellman_ford step 2974 current loss 0.192204, current_train_items 95200.
I0304 19:29:39.876474 23118544486528 run.py:483] Algo bellman_ford step 2975 current loss 0.011433, current_train_items 95232.
I0304 19:29:39.893584 23118544486528 run.py:483] Algo bellman_ford step 2976 current loss 0.043554, current_train_items 95264.
I0304 19:29:39.917451 23118544486528 run.py:483] Algo bellman_ford step 2977 current loss 0.150325, current_train_items 95296.
I0304 19:29:39.948170 23118544486528 run.py:483] Algo bellman_ford step 2978 current loss 0.184617, current_train_items 95328.
I0304 19:29:39.983720 23118544486528 run.py:483] Algo bellman_ford step 2979 current loss 0.187928, current_train_items 95360.
I0304 19:29:40.003241 23118544486528 run.py:483] Algo bellman_ford step 2980 current loss 0.005860, current_train_items 95392.
I0304 19:29:40.019443 23118544486528 run.py:483] Algo bellman_ford step 2981 current loss 0.044389, current_train_items 95424.
I0304 19:29:40.044223 23118544486528 run.py:483] Algo bellman_ford step 2982 current loss 0.138735, current_train_items 95456.
I0304 19:29:40.077987 23118544486528 run.py:483] Algo bellman_ford step 2983 current loss 0.113252, current_train_items 95488.
I0304 19:29:40.112214 23118544486528 run.py:483] Algo bellman_ford step 2984 current loss 0.120398, current_train_items 95520.
I0304 19:29:40.132043 23118544486528 run.py:483] Algo bellman_ford step 2985 current loss 0.008281, current_train_items 95552.
I0304 19:29:40.149178 23118544486528 run.py:483] Algo bellman_ford step 2986 current loss 0.069629, current_train_items 95584.
I0304 19:29:40.174049 23118544486528 run.py:483] Algo bellman_ford step 2987 current loss 0.152927, current_train_items 95616.
I0304 19:29:40.205346 23118544486528 run.py:483] Algo bellman_ford step 2988 current loss 0.077804, current_train_items 95648.
I0304 19:29:40.240214 23118544486528 run.py:483] Algo bellman_ford step 2989 current loss 0.114824, current_train_items 95680.
I0304 19:29:40.260402 23118544486528 run.py:483] Algo bellman_ford step 2990 current loss 0.017986, current_train_items 95712.
I0304 19:29:40.277495 23118544486528 run.py:483] Algo bellman_ford step 2991 current loss 0.053540, current_train_items 95744.
I0304 19:29:40.302421 23118544486528 run.py:483] Algo bellman_ford step 2992 current loss 0.100811, current_train_items 95776.
I0304 19:29:40.334642 23118544486528 run.py:483] Algo bellman_ford step 2993 current loss 0.165556, current_train_items 95808.
I0304 19:29:40.370926 23118544486528 run.py:483] Algo bellman_ford step 2994 current loss 0.175410, current_train_items 95840.
I0304 19:29:40.390825 23118544486528 run.py:483] Algo bellman_ford step 2995 current loss 0.013772, current_train_items 95872.
I0304 19:29:40.407643 23118544486528 run.py:483] Algo bellman_ford step 2996 current loss 0.073924, current_train_items 95904.
I0304 19:29:40.433351 23118544486528 run.py:483] Algo bellman_ford step 2997 current loss 0.097555, current_train_items 95936.
I0304 19:29:40.465427 23118544486528 run.py:483] Algo bellman_ford step 2998 current loss 0.084502, current_train_items 95968.
I0304 19:29:40.498896 23118544486528 run.py:483] Algo bellman_ford step 2999 current loss 0.140275, current_train_items 96000.
I0304 19:29:40.518877 23118544486528 run.py:483] Algo bellman_ford step 3000 current loss 0.007902, current_train_items 96032.
I0304 19:29:40.526769 23118544486528 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0304 19:29:40.526873 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:40.544154 23118544486528 run.py:483] Algo bellman_ford step 3001 current loss 0.061783, current_train_items 96064.
I0304 19:29:40.568064 23118544486528 run.py:483] Algo bellman_ford step 3002 current loss 0.054603, current_train_items 96096.
I0304 19:29:40.599803 23118544486528 run.py:483] Algo bellman_ford step 3003 current loss 0.156565, current_train_items 96128.
I0304 19:29:40.633138 23118544486528 run.py:483] Algo bellman_ford step 3004 current loss 0.123172, current_train_items 96160.
I0304 19:29:40.653187 23118544486528 run.py:483] Algo bellman_ford step 3005 current loss 0.008050, current_train_items 96192.
I0304 19:29:40.669274 23118544486528 run.py:483] Algo bellman_ford step 3006 current loss 0.021868, current_train_items 96224.
I0304 19:29:40.694640 23118544486528 run.py:483] Algo bellman_ford step 3007 current loss 0.088343, current_train_items 96256.
I0304 19:29:40.726922 23118544486528 run.py:483] Algo bellman_ford step 3008 current loss 0.144349, current_train_items 96288.
I0304 19:29:40.763729 23118544486528 run.py:483] Algo bellman_ford step 3009 current loss 0.159804, current_train_items 96320.
I0304 19:29:40.783854 23118544486528 run.py:483] Algo bellman_ford step 3010 current loss 0.006481, current_train_items 96352.
I0304 19:29:40.800234 23118544486528 run.py:483] Algo bellman_ford step 3011 current loss 0.034717, current_train_items 96384.
I0304 19:29:40.824707 23118544486528 run.py:483] Algo bellman_ford step 3012 current loss 0.099635, current_train_items 96416.
I0304 19:29:40.856077 23118544486528 run.py:483] Algo bellman_ford step 3013 current loss 0.126448, current_train_items 96448.
I0304 19:29:40.891443 23118544486528 run.py:483] Algo bellman_ford step 3014 current loss 0.126499, current_train_items 96480.
I0304 19:29:40.911351 23118544486528 run.py:483] Algo bellman_ford step 3015 current loss 0.016501, current_train_items 96512.
I0304 19:29:40.928293 23118544486528 run.py:483] Algo bellman_ford step 3016 current loss 0.034896, current_train_items 96544.
I0304 19:29:40.952249 23118544486528 run.py:483] Algo bellman_ford step 3017 current loss 0.101408, current_train_items 96576.
I0304 19:29:40.983057 23118544486528 run.py:483] Algo bellman_ford step 3018 current loss 0.110518, current_train_items 96608.
I0304 19:29:41.015989 23118544486528 run.py:483] Algo bellman_ford step 3019 current loss 0.135651, current_train_items 96640.
I0304 19:29:41.035550 23118544486528 run.py:483] Algo bellman_ford step 3020 current loss 0.009236, current_train_items 96672.
I0304 19:29:41.051505 23118544486528 run.py:483] Algo bellman_ford step 3021 current loss 0.026600, current_train_items 96704.
I0304 19:29:41.076565 23118544486528 run.py:483] Algo bellman_ford step 3022 current loss 0.077220, current_train_items 96736.
I0304 19:29:41.108040 23118544486528 run.py:483] Algo bellman_ford step 3023 current loss 0.094249, current_train_items 96768.
I0304 19:29:41.142130 23118544486528 run.py:483] Algo bellman_ford step 3024 current loss 0.078157, current_train_items 96800.
I0304 19:29:41.161596 23118544486528 run.py:483] Algo bellman_ford step 3025 current loss 0.006838, current_train_items 96832.
I0304 19:29:41.177812 23118544486528 run.py:483] Algo bellman_ford step 3026 current loss 0.023508, current_train_items 96864.
I0304 19:29:41.200534 23118544486528 run.py:483] Algo bellman_ford step 3027 current loss 0.054495, current_train_items 96896.
I0304 19:29:41.232044 23118544486528 run.py:483] Algo bellman_ford step 3028 current loss 0.089336, current_train_items 96928.
I0304 19:29:41.265090 23118544486528 run.py:483] Algo bellman_ford step 3029 current loss 0.055047, current_train_items 96960.
I0304 19:29:41.284999 23118544486528 run.py:483] Algo bellman_ford step 3030 current loss 0.006800, current_train_items 96992.
I0304 19:29:41.301106 23118544486528 run.py:483] Algo bellman_ford step 3031 current loss 0.050180, current_train_items 97024.
I0304 19:29:41.326330 23118544486528 run.py:483] Algo bellman_ford step 3032 current loss 0.049814, current_train_items 97056.
I0304 19:29:41.358715 23118544486528 run.py:483] Algo bellman_ford step 3033 current loss 0.061197, current_train_items 97088.
I0304 19:29:41.394047 23118544486528 run.py:483] Algo bellman_ford step 3034 current loss 0.073363, current_train_items 97120.
I0304 19:29:41.413752 23118544486528 run.py:483] Algo bellman_ford step 3035 current loss 0.144151, current_train_items 97152.
I0304 19:29:41.430718 23118544486528 run.py:483] Algo bellman_ford step 3036 current loss 0.054532, current_train_items 97184.
I0304 19:29:41.454922 23118544486528 run.py:483] Algo bellman_ford step 3037 current loss 0.050306, current_train_items 97216.
I0304 19:29:41.486618 23118544486528 run.py:483] Algo bellman_ford step 3038 current loss 0.079320, current_train_items 97248.
I0304 19:29:41.519396 23118544486528 run.py:483] Algo bellman_ford step 3039 current loss 0.122119, current_train_items 97280.
I0304 19:29:41.539563 23118544486528 run.py:483] Algo bellman_ford step 3040 current loss 0.006948, current_train_items 97312.
I0304 19:29:41.555901 23118544486528 run.py:483] Algo bellman_ford step 3041 current loss 0.032919, current_train_items 97344.
I0304 19:29:41.580121 23118544486528 run.py:483] Algo bellman_ford step 3042 current loss 0.074830, current_train_items 97376.
I0304 19:29:41.611400 23118544486528 run.py:483] Algo bellman_ford step 3043 current loss 0.110645, current_train_items 97408.
I0304 19:29:41.644772 23118544486528 run.py:483] Algo bellman_ford step 3044 current loss 0.152635, current_train_items 97440.
I0304 19:29:41.664325 23118544486528 run.py:483] Algo bellman_ford step 3045 current loss 0.023726, current_train_items 97472.
I0304 19:29:41.680697 23118544486528 run.py:483] Algo bellman_ford step 3046 current loss 0.098277, current_train_items 97504.
I0304 19:29:41.704436 23118544486528 run.py:483] Algo bellman_ford step 3047 current loss 0.102663, current_train_items 97536.
I0304 19:29:41.736500 23118544486528 run.py:483] Algo bellman_ford step 3048 current loss 0.097090, current_train_items 97568.
I0304 19:29:41.771056 23118544486528 run.py:483] Algo bellman_ford step 3049 current loss 0.098173, current_train_items 97600.
I0304 19:29:41.790855 23118544486528 run.py:483] Algo bellman_ford step 3050 current loss 0.008846, current_train_items 97632.
I0304 19:29:41.798860 23118544486528 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0304 19:29:41.798964 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:41.815646 23118544486528 run.py:483] Algo bellman_ford step 3051 current loss 0.036462, current_train_items 97664.
I0304 19:29:41.840193 23118544486528 run.py:483] Algo bellman_ford step 3052 current loss 0.077583, current_train_items 97696.
I0304 19:29:41.872701 23118544486528 run.py:483] Algo bellman_ford step 3053 current loss 0.086580, current_train_items 97728.
I0304 19:29:41.905235 23118544486528 run.py:483] Algo bellman_ford step 3054 current loss 0.106532, current_train_items 97760.
I0304 19:29:41.925121 23118544486528 run.py:483] Algo bellman_ford step 3055 current loss 0.007181, current_train_items 97792.
I0304 19:29:41.941250 23118544486528 run.py:483] Algo bellman_ford step 3056 current loss 0.059168, current_train_items 97824.
I0304 19:29:41.965311 23118544486528 run.py:483] Algo bellman_ford step 3057 current loss 0.069503, current_train_items 97856.
I0304 19:29:41.996844 23118544486528 run.py:483] Algo bellman_ford step 3058 current loss 0.064592, current_train_items 97888.
I0304 19:29:42.030017 23118544486528 run.py:483] Algo bellman_ford step 3059 current loss 0.118637, current_train_items 97920.
I0304 19:29:42.049937 23118544486528 run.py:483] Algo bellman_ford step 3060 current loss 0.007714, current_train_items 97952.
I0304 19:29:42.066576 23118544486528 run.py:483] Algo bellman_ford step 3061 current loss 0.047448, current_train_items 97984.
I0304 19:29:42.090190 23118544486528 run.py:483] Algo bellman_ford step 3062 current loss 0.055901, current_train_items 98016.
I0304 19:29:42.122665 23118544486528 run.py:483] Algo bellman_ford step 3063 current loss 0.119611, current_train_items 98048.
I0304 19:29:42.157747 23118544486528 run.py:483] Algo bellman_ford step 3064 current loss 0.127592, current_train_items 98080.
I0304 19:29:42.177348 23118544486528 run.py:483] Algo bellman_ford step 3065 current loss 0.024623, current_train_items 98112.
I0304 19:29:42.194059 23118544486528 run.py:483] Algo bellman_ford step 3066 current loss 0.045742, current_train_items 98144.
I0304 19:29:42.219502 23118544486528 run.py:483] Algo bellman_ford step 3067 current loss 0.074713, current_train_items 98176.
I0304 19:29:42.250419 23118544486528 run.py:483] Algo bellman_ford step 3068 current loss 0.095722, current_train_items 98208.
I0304 19:29:42.287107 23118544486528 run.py:483] Algo bellman_ford step 3069 current loss 0.089074, current_train_items 98240.
I0304 19:29:42.307342 23118544486528 run.py:483] Algo bellman_ford step 3070 current loss 0.007672, current_train_items 98272.
I0304 19:29:42.323875 23118544486528 run.py:483] Algo bellman_ford step 3071 current loss 0.069182, current_train_items 98304.
I0304 19:29:42.347285 23118544486528 run.py:483] Algo bellman_ford step 3072 current loss 0.111789, current_train_items 98336.
I0304 19:29:42.377985 23118544486528 run.py:483] Algo bellman_ford step 3073 current loss 0.089533, current_train_items 98368.
I0304 19:29:42.414043 23118544486528 run.py:483] Algo bellman_ford step 3074 current loss 0.110086, current_train_items 98400.
I0304 19:29:42.434193 23118544486528 run.py:483] Algo bellman_ford step 3075 current loss 0.008516, current_train_items 98432.
I0304 19:29:42.451050 23118544486528 run.py:483] Algo bellman_ford step 3076 current loss 0.053942, current_train_items 98464.
I0304 19:29:42.475133 23118544486528 run.py:483] Algo bellman_ford step 3077 current loss 0.156392, current_train_items 98496.
I0304 19:29:42.506954 23118544486528 run.py:483] Algo bellman_ford step 3078 current loss 0.176973, current_train_items 98528.
I0304 19:29:42.540319 23118544486528 run.py:483] Algo bellman_ford step 3079 current loss 0.104541, current_train_items 98560.
I0304 19:29:42.559958 23118544486528 run.py:483] Algo bellman_ford step 3080 current loss 0.011868, current_train_items 98592.
I0304 19:29:42.576514 23118544486528 run.py:483] Algo bellman_ford step 3081 current loss 0.039305, current_train_items 98624.
I0304 19:29:42.600419 23118544486528 run.py:483] Algo bellman_ford step 3082 current loss 0.070139, current_train_items 98656.
I0304 19:29:42.631610 23118544486528 run.py:483] Algo bellman_ford step 3083 current loss 0.100937, current_train_items 98688.
I0304 19:29:42.666193 23118544486528 run.py:483] Algo bellman_ford step 3084 current loss 0.106873, current_train_items 98720.
I0304 19:29:42.686198 23118544486528 run.py:483] Algo bellman_ford step 3085 current loss 0.032072, current_train_items 98752.
I0304 19:29:42.702866 23118544486528 run.py:483] Algo bellman_ford step 3086 current loss 0.022316, current_train_items 98784.
I0304 19:29:42.725403 23118544486528 run.py:483] Algo bellman_ford step 3087 current loss 0.080105, current_train_items 98816.
I0304 19:29:42.756861 23118544486528 run.py:483] Algo bellman_ford step 3088 current loss 0.161282, current_train_items 98848.
I0304 19:29:42.792300 23118544486528 run.py:483] Algo bellman_ford step 3089 current loss 0.159834, current_train_items 98880.
I0304 19:29:42.812139 23118544486528 run.py:483] Algo bellman_ford step 3090 current loss 0.005057, current_train_items 98912.
I0304 19:29:42.828465 23118544486528 run.py:483] Algo bellman_ford step 3091 current loss 0.012915, current_train_items 98944.
I0304 19:29:42.851381 23118544486528 run.py:483] Algo bellman_ford step 3092 current loss 0.090090, current_train_items 98976.
I0304 19:29:42.883044 23118544486528 run.py:483] Algo bellman_ford step 3093 current loss 0.223737, current_train_items 99008.
I0304 19:29:42.917202 23118544486528 run.py:483] Algo bellman_ford step 3094 current loss 0.094053, current_train_items 99040.
I0304 19:29:42.936649 23118544486528 run.py:483] Algo bellman_ford step 3095 current loss 0.010625, current_train_items 99072.
I0304 19:29:42.953565 23118544486528 run.py:483] Algo bellman_ford step 3096 current loss 0.063478, current_train_items 99104.
I0304 19:29:42.979187 23118544486528 run.py:483] Algo bellman_ford step 3097 current loss 0.122192, current_train_items 99136.
I0304 19:29:43.010265 23118544486528 run.py:483] Algo bellman_ford step 3098 current loss 0.163554, current_train_items 99168.
I0304 19:29:43.043706 23118544486528 run.py:483] Algo bellman_ford step 3099 current loss 0.117237, current_train_items 99200.
I0304 19:29:43.064099 23118544486528 run.py:483] Algo bellman_ford step 3100 current loss 0.009249, current_train_items 99232.
I0304 19:29:43.071681 23118544486528 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0304 19:29:43.071796 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:29:43.089679 23118544486528 run.py:483] Algo bellman_ford step 3101 current loss 0.074209, current_train_items 99264.
I0304 19:29:43.114340 23118544486528 run.py:483] Algo bellman_ford step 3102 current loss 0.102468, current_train_items 99296.
I0304 19:29:43.146490 23118544486528 run.py:483] Algo bellman_ford step 3103 current loss 0.098617, current_train_items 99328.
I0304 19:29:43.181833 23118544486528 run.py:483] Algo bellman_ford step 3104 current loss 0.109042, current_train_items 99360.
I0304 19:29:43.202055 23118544486528 run.py:483] Algo bellman_ford step 3105 current loss 0.042334, current_train_items 99392.
I0304 19:29:43.217798 23118544486528 run.py:483] Algo bellman_ford step 3106 current loss 0.010566, current_train_items 99424.
I0304 19:29:43.241896 23118544486528 run.py:483] Algo bellman_ford step 3107 current loss 0.041082, current_train_items 99456.
I0304 19:29:43.274348 23118544486528 run.py:483] Algo bellman_ford step 3108 current loss 0.110705, current_train_items 99488.
I0304 19:29:43.307136 23118544486528 run.py:483] Algo bellman_ford step 3109 current loss 0.102720, current_train_items 99520.
I0304 19:29:43.326779 23118544486528 run.py:483] Algo bellman_ford step 3110 current loss 0.009562, current_train_items 99552.
I0304 19:29:43.343213 23118544486528 run.py:483] Algo bellman_ford step 3111 current loss 0.038820, current_train_items 99584.
I0304 19:29:43.367146 23118544486528 run.py:483] Algo bellman_ford step 3112 current loss 0.144689, current_train_items 99616.
I0304 19:29:43.398355 23118544486528 run.py:483] Algo bellman_ford step 3113 current loss 0.064843, current_train_items 99648.
I0304 19:29:43.433036 23118544486528 run.py:483] Algo bellman_ford step 3114 current loss 0.086492, current_train_items 99680.
I0304 19:29:43.453012 23118544486528 run.py:483] Algo bellman_ford step 3115 current loss 0.009591, current_train_items 99712.
I0304 19:29:43.469698 23118544486528 run.py:483] Algo bellman_ford step 3116 current loss 0.028877, current_train_items 99744.
I0304 19:29:43.493836 23118544486528 run.py:483] Algo bellman_ford step 3117 current loss 0.069692, current_train_items 99776.
I0304 19:29:43.525978 23118544486528 run.py:483] Algo bellman_ford step 3118 current loss 0.107582, current_train_items 99808.
I0304 19:29:43.559848 23118544486528 run.py:483] Algo bellman_ford step 3119 current loss 0.086048, current_train_items 99840.
I0304 19:29:43.579496 23118544486528 run.py:483] Algo bellman_ford step 3120 current loss 0.004816, current_train_items 99872.
I0304 19:29:43.595841 23118544486528 run.py:483] Algo bellman_ford step 3121 current loss 0.034339, current_train_items 99904.
I0304 19:29:43.620449 23118544486528 run.py:483] Algo bellman_ford step 3122 current loss 0.037941, current_train_items 99936.
I0304 19:29:43.652267 23118544486528 run.py:483] Algo bellman_ford step 3123 current loss 0.066429, current_train_items 99968.
I0304 19:29:43.686415 23118544486528 run.py:483] Algo bellman_ford step 3124 current loss 0.094665, current_train_items 100000.
I0304 19:29:43.706023 23118544486528 run.py:483] Algo bellman_ford step 3125 current loss 0.008212, current_train_items 100032.
I0304 19:29:43.722413 23118544486528 run.py:483] Algo bellman_ford step 3126 current loss 0.025296, current_train_items 100064.
I0304 19:29:43.745539 23118544486528 run.py:483] Algo bellman_ford step 3127 current loss 0.040249, current_train_items 100096.
I0304 19:29:43.778111 23118544486528 run.py:483] Algo bellman_ford step 3128 current loss 0.069595, current_train_items 100128.
I0304 19:29:43.810641 23118544486528 run.py:483] Algo bellman_ford step 3129 current loss 0.063261, current_train_items 100160.
I0304 19:29:43.830718 23118544486528 run.py:483] Algo bellman_ford step 3130 current loss 0.005845, current_train_items 100192.
I0304 19:29:43.847113 23118544486528 run.py:483] Algo bellman_ford step 3131 current loss 0.038868, current_train_items 100224.
I0304 19:29:43.871649 23118544486528 run.py:483] Algo bellman_ford step 3132 current loss 0.048206, current_train_items 100256.
I0304 19:29:43.902150 23118544486528 run.py:483] Algo bellman_ford step 3133 current loss 0.099970, current_train_items 100288.
I0304 19:29:43.938062 23118544486528 run.py:483] Algo bellman_ford step 3134 current loss 0.096062, current_train_items 100320.
I0304 19:29:43.957844 23118544486528 run.py:483] Algo bellman_ford step 3135 current loss 0.023549, current_train_items 100352.
I0304 19:29:43.974125 23118544486528 run.py:483] Algo bellman_ford step 3136 current loss 0.023872, current_train_items 100384.
I0304 19:29:43.998546 23118544486528 run.py:483] Algo bellman_ford step 3137 current loss 0.058207, current_train_items 100416.
I0304 19:29:44.029255 23118544486528 run.py:483] Algo bellman_ford step 3138 current loss 0.059122, current_train_items 100448.
I0304 19:29:44.063783 23118544486528 run.py:483] Algo bellman_ford step 3139 current loss 0.087594, current_train_items 100480.
I0304 19:29:44.083555 23118544486528 run.py:483] Algo bellman_ford step 3140 current loss 0.009794, current_train_items 100512.
I0304 19:29:44.100452 23118544486528 run.py:483] Algo bellman_ford step 3141 current loss 0.059685, current_train_items 100544.
I0304 19:29:44.123447 23118544486528 run.py:483] Algo bellman_ford step 3142 current loss 0.057688, current_train_items 100576.
I0304 19:29:44.155971 23118544486528 run.py:483] Algo bellman_ford step 3143 current loss 0.071289, current_train_items 100608.
I0304 19:29:44.189451 23118544486528 run.py:483] Algo bellman_ford step 3144 current loss 0.079210, current_train_items 100640.
I0304 19:29:44.209380 23118544486528 run.py:483] Algo bellman_ford step 3145 current loss 0.009264, current_train_items 100672.
I0304 19:29:44.225764 23118544486528 run.py:483] Algo bellman_ford step 3146 current loss 0.026758, current_train_items 100704.
I0304 19:29:44.249557 23118544486528 run.py:483] Algo bellman_ford step 3147 current loss 0.032615, current_train_items 100736.
I0304 19:29:44.279203 23118544486528 run.py:483] Algo bellman_ford step 3148 current loss 0.046897, current_train_items 100768.
I0304 19:29:44.315296 23118544486528 run.py:483] Algo bellman_ford step 3149 current loss 0.086207, current_train_items 100800.
I0304 19:29:44.334852 23118544486528 run.py:483] Algo bellman_ford step 3150 current loss 0.008954, current_train_items 100832.
I0304 19:29:44.342856 23118544486528 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0304 19:29:44.342962 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:29:44.360051 23118544486528 run.py:483] Algo bellman_ford step 3151 current loss 0.037591, current_train_items 100864.
I0304 19:29:44.385716 23118544486528 run.py:483] Algo bellman_ford step 3152 current loss 0.115554, current_train_items 100896.
I0304 19:29:44.417473 23118544486528 run.py:483] Algo bellman_ford step 3153 current loss 0.076881, current_train_items 100928.
I0304 19:29:44.452633 23118544486528 run.py:483] Algo bellman_ford step 3154 current loss 0.090363, current_train_items 100960.
I0304 19:29:44.472483 23118544486528 run.py:483] Algo bellman_ford step 3155 current loss 0.009914, current_train_items 100992.
I0304 19:29:44.488575 23118544486528 run.py:483] Algo bellman_ford step 3156 current loss 0.028704, current_train_items 101024.
I0304 19:29:44.513125 23118544486528 run.py:483] Algo bellman_ford step 3157 current loss 0.078152, current_train_items 101056.
I0304 19:29:44.543697 23118544486528 run.py:483] Algo bellman_ford step 3158 current loss 0.081490, current_train_items 101088.
I0304 19:29:44.577153 23118544486528 run.py:483] Algo bellman_ford step 3159 current loss 0.091397, current_train_items 101120.
I0304 19:29:44.597322 23118544486528 run.py:483] Algo bellman_ford step 3160 current loss 0.005202, current_train_items 101152.
I0304 19:29:44.613424 23118544486528 run.py:483] Algo bellman_ford step 3161 current loss 0.014921, current_train_items 101184.
I0304 19:29:44.637129 23118544486528 run.py:483] Algo bellman_ford step 3162 current loss 0.037150, current_train_items 101216.
I0304 19:29:44.667722 23118544486528 run.py:483] Algo bellman_ford step 3163 current loss 0.061716, current_train_items 101248.
I0304 19:29:44.703264 23118544486528 run.py:483] Algo bellman_ford step 3164 current loss 0.117003, current_train_items 101280.
I0304 19:29:44.723357 23118544486528 run.py:483] Algo bellman_ford step 3165 current loss 0.009034, current_train_items 101312.
I0304 19:29:44.740393 23118544486528 run.py:483] Algo bellman_ford step 3166 current loss 0.063804, current_train_items 101344.
I0304 19:29:44.764942 23118544486528 run.py:483] Algo bellman_ford step 3167 current loss 0.065440, current_train_items 101376.
I0304 19:29:44.796460 23118544486528 run.py:483] Algo bellman_ford step 3168 current loss 0.060918, current_train_items 101408.
I0304 19:29:44.830462 23118544486528 run.py:483] Algo bellman_ford step 3169 current loss 0.130324, current_train_items 101440.
I0304 19:29:44.850494 23118544486528 run.py:483] Algo bellman_ford step 3170 current loss 0.033433, current_train_items 101472.
I0304 19:29:44.867351 23118544486528 run.py:483] Algo bellman_ford step 3171 current loss 0.084660, current_train_items 101504.
I0304 19:29:44.891915 23118544486528 run.py:483] Algo bellman_ford step 3172 current loss 0.054371, current_train_items 101536.
I0304 19:29:44.923386 23118544486528 run.py:483] Algo bellman_ford step 3173 current loss 0.060716, current_train_items 101568.
I0304 19:29:44.955562 23118544486528 run.py:483] Algo bellman_ford step 3174 current loss 0.094429, current_train_items 101600.
I0304 19:29:44.975526 23118544486528 run.py:483] Algo bellman_ford step 3175 current loss 0.009119, current_train_items 101632.
I0304 19:29:44.991967 23118544486528 run.py:483] Algo bellman_ford step 3176 current loss 0.059666, current_train_items 101664.
I0304 19:29:45.015378 23118544486528 run.py:483] Algo bellman_ford step 3177 current loss 0.068512, current_train_items 101696.
I0304 19:29:45.045463 23118544486528 run.py:483] Algo bellman_ford step 3178 current loss 0.037940, current_train_items 101728.
I0304 19:29:45.080224 23118544486528 run.py:483] Algo bellman_ford step 3179 current loss 0.116275, current_train_items 101760.
I0304 19:29:45.099909 23118544486528 run.py:483] Algo bellman_ford step 3180 current loss 0.009451, current_train_items 101792.
I0304 19:29:45.116537 23118544486528 run.py:483] Algo bellman_ford step 3181 current loss 0.019778, current_train_items 101824.
I0304 19:29:45.141417 23118544486528 run.py:483] Algo bellman_ford step 3182 current loss 0.054756, current_train_items 101856.
I0304 19:29:45.173460 23118544486528 run.py:483] Algo bellman_ford step 3183 current loss 0.069630, current_train_items 101888.
I0304 19:29:45.210488 23118544486528 run.py:483] Algo bellman_ford step 3184 current loss 0.084046, current_train_items 101920.
I0304 19:29:45.230674 23118544486528 run.py:483] Algo bellman_ford step 3185 current loss 0.028840, current_train_items 101952.
I0304 19:29:45.247243 23118544486528 run.py:483] Algo bellman_ford step 3186 current loss 0.055637, current_train_items 101984.
I0304 19:29:45.271792 23118544486528 run.py:483] Algo bellman_ford step 3187 current loss 0.115778, current_train_items 102016.
I0304 19:29:45.303097 23118544486528 run.py:483] Algo bellman_ford step 3188 current loss 0.070477, current_train_items 102048.
I0304 19:29:45.337228 23118544486528 run.py:483] Algo bellman_ford step 3189 current loss 0.073862, current_train_items 102080.
I0304 19:29:45.357447 23118544486528 run.py:483] Algo bellman_ford step 3190 current loss 0.024331, current_train_items 102112.
I0304 19:29:45.374197 23118544486528 run.py:483] Algo bellman_ford step 3191 current loss 0.021126, current_train_items 102144.
I0304 19:29:45.398682 23118544486528 run.py:483] Algo bellman_ford step 3192 current loss 0.093334, current_train_items 102176.
I0304 19:29:45.429809 23118544486528 run.py:483] Algo bellman_ford step 3193 current loss 0.071882, current_train_items 102208.
I0304 19:29:45.464216 23118544486528 run.py:483] Algo bellman_ford step 3194 current loss 0.082987, current_train_items 102240.
I0304 19:29:45.484185 23118544486528 run.py:483] Algo bellman_ford step 3195 current loss 0.024199, current_train_items 102272.
I0304 19:29:45.500557 23118544486528 run.py:483] Algo bellman_ford step 3196 current loss 0.008897, current_train_items 102304.
I0304 19:29:45.524424 23118544486528 run.py:483] Algo bellman_ford step 3197 current loss 0.098571, current_train_items 102336.
I0304 19:29:45.556448 23118544486528 run.py:483] Algo bellman_ford step 3198 current loss 0.092850, current_train_items 102368.
I0304 19:29:45.590892 23118544486528 run.py:483] Algo bellman_ford step 3199 current loss 0.112510, current_train_items 102400.
I0304 19:29:45.611383 23118544486528 run.py:483] Algo bellman_ford step 3200 current loss 0.019782, current_train_items 102432.
I0304 19:29:45.619300 23118544486528 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0304 19:29:45.619404 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0304 19:29:45.635984 23118544486528 run.py:483] Algo bellman_ford step 3201 current loss 0.024758, current_train_items 102464.
I0304 19:29:45.660304 23118544486528 run.py:483] Algo bellman_ford step 3202 current loss 0.069517, current_train_items 102496.
I0304 19:29:45.694104 23118544486528 run.py:483] Algo bellman_ford step 3203 current loss 0.219294, current_train_items 102528.
I0304 19:29:45.728791 23118544486528 run.py:483] Algo bellman_ford step 3204 current loss 0.146201, current_train_items 102560.
I0304 19:29:45.748810 23118544486528 run.py:483] Algo bellman_ford step 3205 current loss 0.011506, current_train_items 102592.
I0304 19:29:45.764613 23118544486528 run.py:483] Algo bellman_ford step 3206 current loss 0.039364, current_train_items 102624.
I0304 19:29:45.787862 23118544486528 run.py:483] Algo bellman_ford step 3207 current loss 0.048056, current_train_items 102656.
I0304 19:29:45.820535 23118544486528 run.py:483] Algo bellman_ford step 3208 current loss 0.117967, current_train_items 102688.
I0304 19:29:45.855309 23118544486528 run.py:483] Algo bellman_ford step 3209 current loss 0.214315, current_train_items 102720.
I0304 19:29:45.875024 23118544486528 run.py:483] Algo bellman_ford step 3210 current loss 0.036976, current_train_items 102752.
I0304 19:29:45.891068 23118544486528 run.py:483] Algo bellman_ford step 3211 current loss 0.019381, current_train_items 102784.
I0304 19:29:45.915146 23118544486528 run.py:483] Algo bellman_ford step 3212 current loss 0.085483, current_train_items 102816.
I0304 19:29:45.946681 23118544486528 run.py:483] Algo bellman_ford step 3213 current loss 0.129036, current_train_items 102848.
I0304 19:29:45.980848 23118544486528 run.py:483] Algo bellman_ford step 3214 current loss 0.101160, current_train_items 102880.
I0304 19:29:46.000921 23118544486528 run.py:483] Algo bellman_ford step 3215 current loss 0.017316, current_train_items 102912.
I0304 19:29:46.017225 23118544486528 run.py:483] Algo bellman_ford step 3216 current loss 0.031420, current_train_items 102944.
I0304 19:29:46.042610 23118544486528 run.py:483] Algo bellman_ford step 3217 current loss 0.053660, current_train_items 102976.
I0304 19:29:46.072892 23118544486528 run.py:483] Algo bellman_ford step 3218 current loss 0.065386, current_train_items 103008.
I0304 19:29:46.106753 23118544486528 run.py:483] Algo bellman_ford step 3219 current loss 0.071196, current_train_items 103040.
I0304 19:29:46.126541 23118544486528 run.py:483] Algo bellman_ford step 3220 current loss 0.013836, current_train_items 103072.
I0304 19:29:46.142899 23118544486528 run.py:483] Algo bellman_ford step 3221 current loss 0.040438, current_train_items 103104.
I0304 19:29:46.167667 23118544486528 run.py:483] Algo bellman_ford step 3222 current loss 0.077907, current_train_items 103136.
I0304 19:29:46.199775 23118544486528 run.py:483] Algo bellman_ford step 3223 current loss 0.065773, current_train_items 103168.
I0304 19:29:46.232901 23118544486528 run.py:483] Algo bellman_ford step 3224 current loss 0.085810, current_train_items 103200.
I0304 19:29:46.252543 23118544486528 run.py:483] Algo bellman_ford step 3225 current loss 0.007836, current_train_items 103232.
I0304 19:29:46.268220 23118544486528 run.py:483] Algo bellman_ford step 3226 current loss 0.022660, current_train_items 103264.
I0304 19:29:46.292881 23118544486528 run.py:483] Algo bellman_ford step 3227 current loss 0.031195, current_train_items 103296.
I0304 19:29:46.325504 23118544486528 run.py:483] Algo bellman_ford step 3228 current loss 0.081620, current_train_items 103328.
I0304 19:29:46.363591 23118544486528 run.py:483] Algo bellman_ford step 3229 current loss 0.146086, current_train_items 103360.
I0304 19:29:46.383364 23118544486528 run.py:483] Algo bellman_ford step 3230 current loss 0.008424, current_train_items 103392.
I0304 19:29:46.399991 23118544486528 run.py:483] Algo bellman_ford step 3231 current loss 0.061150, current_train_items 103424.
I0304 19:29:46.423401 23118544486528 run.py:483] Algo bellman_ford step 3232 current loss 0.051653, current_train_items 103456.
I0304 19:29:46.455302 23118544486528 run.py:483] Algo bellman_ford step 3233 current loss 0.042018, current_train_items 103488.
I0304 19:29:46.490926 23118544486528 run.py:483] Algo bellman_ford step 3234 current loss 0.087341, current_train_items 103520.
I0304 19:29:46.510820 23118544486528 run.py:483] Algo bellman_ford step 3235 current loss 0.019507, current_train_items 103552.
I0304 19:29:46.527413 23118544486528 run.py:483] Algo bellman_ford step 3236 current loss 0.035561, current_train_items 103584.
I0304 19:29:46.551117 23118544486528 run.py:483] Algo bellman_ford step 3237 current loss 0.039609, current_train_items 103616.
I0304 19:29:46.584445 23118544486528 run.py:483] Algo bellman_ford step 3238 current loss 0.085284, current_train_items 103648.
I0304 19:29:46.618148 23118544486528 run.py:483] Algo bellman_ford step 3239 current loss 0.075941, current_train_items 103680.
I0304 19:29:46.637927 23118544486528 run.py:483] Algo bellman_ford step 3240 current loss 0.014119, current_train_items 103712.
I0304 19:29:46.654533 23118544486528 run.py:483] Algo bellman_ford step 3241 current loss 0.060018, current_train_items 103744.
I0304 19:29:46.678886 23118544486528 run.py:483] Algo bellman_ford step 3242 current loss 0.117316, current_train_items 103776.
I0304 19:29:46.709331 23118544486528 run.py:483] Algo bellman_ford step 3243 current loss 0.093058, current_train_items 103808.
I0304 19:29:46.744251 23118544486528 run.py:483] Algo bellman_ford step 3244 current loss 0.111323, current_train_items 103840.
I0304 19:29:46.764165 23118544486528 run.py:483] Algo bellman_ford step 3245 current loss 0.010333, current_train_items 103872.
I0304 19:29:46.780735 23118544486528 run.py:483] Algo bellman_ford step 3246 current loss 0.030344, current_train_items 103904.
I0304 19:29:46.805162 23118544486528 run.py:483] Algo bellman_ford step 3247 current loss 0.060789, current_train_items 103936.
I0304 19:29:46.837377 23118544486528 run.py:483] Algo bellman_ford step 3248 current loss 0.074573, current_train_items 103968.
I0304 19:29:46.870400 23118544486528 run.py:483] Algo bellman_ford step 3249 current loss 0.071868, current_train_items 104000.
I0304 19:29:46.890243 23118544486528 run.py:483] Algo bellman_ford step 3250 current loss 0.010791, current_train_items 104032.
I0304 19:29:46.898005 23118544486528 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0304 19:29:46.898110 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:29:46.914971 23118544486528 run.py:483] Algo bellman_ford step 3251 current loss 0.029360, current_train_items 104064.
I0304 19:29:46.937727 23118544486528 run.py:483] Algo bellman_ford step 3252 current loss 0.029476, current_train_items 104096.
I0304 19:29:46.971412 23118544486528 run.py:483] Algo bellman_ford step 3253 current loss 0.105680, current_train_items 104128.
I0304 19:29:47.005777 23118544486528 run.py:483] Algo bellman_ford step 3254 current loss 0.087487, current_train_items 104160.
I0304 19:29:47.025811 23118544486528 run.py:483] Algo bellman_ford step 3255 current loss 0.005530, current_train_items 104192.
I0304 19:29:47.041680 23118544486528 run.py:483] Algo bellman_ford step 3256 current loss 0.048002, current_train_items 104224.
I0304 19:29:47.065066 23118544486528 run.py:483] Algo bellman_ford step 3257 current loss 0.071176, current_train_items 104256.
I0304 19:29:47.097678 23118544486528 run.py:483] Algo bellman_ford step 3258 current loss 0.129351, current_train_items 104288.
I0304 19:29:47.133982 23118544486528 run.py:483] Algo bellman_ford step 3259 current loss 0.081098, current_train_items 104320.
I0304 19:29:47.153790 23118544486528 run.py:483] Algo bellman_ford step 3260 current loss 0.006774, current_train_items 104352.
I0304 19:29:47.170206 23118544486528 run.py:483] Algo bellman_ford step 3261 current loss 0.016276, current_train_items 104384.
I0304 19:29:47.193554 23118544486528 run.py:483] Algo bellman_ford step 3262 current loss 0.059860, current_train_items 104416.
I0304 19:29:47.225294 23118544486528 run.py:483] Algo bellman_ford step 3263 current loss 0.113619, current_train_items 104448.
I0304 19:29:47.259946 23118544486528 run.py:483] Algo bellman_ford step 3264 current loss 0.089767, current_train_items 104480.
I0304 19:29:47.279632 23118544486528 run.py:483] Algo bellman_ford step 3265 current loss 0.036147, current_train_items 104512.
I0304 19:29:47.296109 23118544486528 run.py:483] Algo bellman_ford step 3266 current loss 0.027092, current_train_items 104544.
I0304 19:29:47.319530 23118544486528 run.py:483] Algo bellman_ford step 3267 current loss 0.068596, current_train_items 104576.
I0304 19:29:47.349207 23118544486528 run.py:483] Algo bellman_ford step 3268 current loss 0.079683, current_train_items 104608.
I0304 19:29:47.385803 23118544486528 run.py:483] Algo bellman_ford step 3269 current loss 0.139220, current_train_items 104640.
I0304 19:29:47.405591 23118544486528 run.py:483] Algo bellman_ford step 3270 current loss 0.006553, current_train_items 104672.
I0304 19:29:47.422151 23118544486528 run.py:483] Algo bellman_ford step 3271 current loss 0.023081, current_train_items 104704.
I0304 19:29:47.445960 23118544486528 run.py:483] Algo bellman_ford step 3272 current loss 0.049977, current_train_items 104736.
I0304 19:29:47.476444 23118544486528 run.py:483] Algo bellman_ford step 3273 current loss 0.071923, current_train_items 104768.
I0304 19:29:47.508908 23118544486528 run.py:483] Algo bellman_ford step 3274 current loss 0.047130, current_train_items 104800.
I0304 19:29:47.528965 23118544486528 run.py:483] Algo bellman_ford step 3275 current loss 0.014728, current_train_items 104832.
I0304 19:29:47.545120 23118544486528 run.py:483] Algo bellman_ford step 3276 current loss 0.026204, current_train_items 104864.
I0304 19:29:47.568320 23118544486528 run.py:483] Algo bellman_ford step 3277 current loss 0.029265, current_train_items 104896.
I0304 19:29:47.600910 23118544486528 run.py:483] Algo bellman_ford step 3278 current loss 0.070240, current_train_items 104928.
I0304 19:29:47.636317 23118544486528 run.py:483] Algo bellman_ford step 3279 current loss 0.091195, current_train_items 104960.
I0304 19:29:47.656245 23118544486528 run.py:483] Algo bellman_ford step 3280 current loss 0.011748, current_train_items 104992.
I0304 19:29:47.672476 23118544486528 run.py:483] Algo bellman_ford step 3281 current loss 0.016484, current_train_items 105024.
I0304 19:29:47.696328 23118544486528 run.py:483] Algo bellman_ford step 3282 current loss 0.062406, current_train_items 105056.
I0304 19:29:47.728070 23118544486528 run.py:483] Algo bellman_ford step 3283 current loss 0.081636, current_train_items 105088.
I0304 19:29:47.762490 23118544486528 run.py:483] Algo bellman_ford step 3284 current loss 0.079702, current_train_items 105120.
I0304 19:29:47.782585 23118544486528 run.py:483] Algo bellman_ford step 3285 current loss 0.007247, current_train_items 105152.
I0304 19:29:47.799375 23118544486528 run.py:483] Algo bellman_ford step 3286 current loss 0.023818, current_train_items 105184.
I0304 19:29:47.823530 23118544486528 run.py:483] Algo bellman_ford step 3287 current loss 0.067259, current_train_items 105216.
I0304 19:29:47.853667 23118544486528 run.py:483] Algo bellman_ford step 3288 current loss 0.046429, current_train_items 105248.
I0304 19:29:47.886728 23118544486528 run.py:483] Algo bellman_ford step 3289 current loss 0.108173, current_train_items 105280.
I0304 19:29:47.906786 23118544486528 run.py:483] Algo bellman_ford step 3290 current loss 0.010467, current_train_items 105312.
I0304 19:29:47.923749 23118544486528 run.py:483] Algo bellman_ford step 3291 current loss 0.021877, current_train_items 105344.
I0304 19:29:47.947663 23118544486528 run.py:483] Algo bellman_ford step 3292 current loss 0.065926, current_train_items 105376.
I0304 19:29:47.979500 23118544486528 run.py:483] Algo bellman_ford step 3293 current loss 0.110941, current_train_items 105408.
I0304 19:29:48.014201 23118544486528 run.py:483] Algo bellman_ford step 3294 current loss 0.087001, current_train_items 105440.
I0304 19:29:48.033972 23118544486528 run.py:483] Algo bellman_ford step 3295 current loss 0.008688, current_train_items 105472.
I0304 19:29:48.050076 23118544486528 run.py:483] Algo bellman_ford step 3296 current loss 0.005359, current_train_items 105504.
I0304 19:29:48.075247 23118544486528 run.py:483] Algo bellman_ford step 3297 current loss 0.071825, current_train_items 105536.
I0304 19:29:48.108560 23118544486528 run.py:483] Algo bellman_ford step 3298 current loss 0.079175, current_train_items 105568.
I0304 19:29:48.143849 23118544486528 run.py:483] Algo bellman_ford step 3299 current loss 0.096819, current_train_items 105600.
I0304 19:29:48.164028 23118544486528 run.py:483] Algo bellman_ford step 3300 current loss 0.026563, current_train_items 105632.
I0304 19:29:48.171653 23118544486528 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0304 19:29:48.171766 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:29:48.188657 23118544486528 run.py:483] Algo bellman_ford step 3301 current loss 0.028870, current_train_items 105664.
I0304 19:29:48.212888 23118544486528 run.py:483] Algo bellman_ford step 3302 current loss 0.039634, current_train_items 105696.
I0304 19:29:48.243705 23118544486528 run.py:483] Algo bellman_ford step 3303 current loss 0.084808, current_train_items 105728.
I0304 19:29:48.276262 23118544486528 run.py:483] Algo bellman_ford step 3304 current loss 0.071335, current_train_items 105760.
I0304 19:29:48.296218 23118544486528 run.py:483] Algo bellman_ford step 3305 current loss 0.005447, current_train_items 105792.
I0304 19:29:48.312522 23118544486528 run.py:483] Algo bellman_ford step 3306 current loss 0.028240, current_train_items 105824.
I0304 19:29:48.337195 23118544486528 run.py:483] Algo bellman_ford step 3307 current loss 0.038817, current_train_items 105856.
I0304 19:29:48.369032 23118544486528 run.py:483] Algo bellman_ford step 3308 current loss 0.055539, current_train_items 105888.
I0304 19:29:48.404935 23118544486528 run.py:483] Algo bellman_ford step 3309 current loss 0.112171, current_train_items 105920.
I0304 19:29:48.424541 23118544486528 run.py:483] Algo bellman_ford step 3310 current loss 0.041653, current_train_items 105952.
I0304 19:29:48.441174 23118544486528 run.py:483] Algo bellman_ford step 3311 current loss 0.061966, current_train_items 105984.
I0304 19:29:48.465735 23118544486528 run.py:483] Algo bellman_ford step 3312 current loss 0.048352, current_train_items 106016.
I0304 19:29:48.498030 23118544486528 run.py:483] Algo bellman_ford step 3313 current loss 0.061898, current_train_items 106048.
I0304 19:29:48.530622 23118544486528 run.py:483] Algo bellman_ford step 3314 current loss 0.086387, current_train_items 106080.
I0304 19:29:48.550454 23118544486528 run.py:483] Algo bellman_ford step 3315 current loss 0.005139, current_train_items 106112.
I0304 19:29:48.567272 23118544486528 run.py:483] Algo bellman_ford step 3316 current loss 0.020644, current_train_items 106144.
I0304 19:29:48.591056 23118544486528 run.py:483] Algo bellman_ford step 3317 current loss 0.026527, current_train_items 106176.
I0304 19:29:48.622370 23118544486528 run.py:483] Algo bellman_ford step 3318 current loss 0.037927, current_train_items 106208.
I0304 19:29:48.657103 23118544486528 run.py:483] Algo bellman_ford step 3319 current loss 0.142109, current_train_items 106240.
I0304 19:29:48.677059 23118544486528 run.py:483] Algo bellman_ford step 3320 current loss 0.004959, current_train_items 106272.
I0304 19:29:48.693705 23118544486528 run.py:483] Algo bellman_ford step 3321 current loss 0.013849, current_train_items 106304.
I0304 19:29:48.718801 23118544486528 run.py:483] Algo bellman_ford step 3322 current loss 0.053781, current_train_items 106336.
I0304 19:29:48.750932 23118544486528 run.py:483] Algo bellman_ford step 3323 current loss 0.092040, current_train_items 106368.
I0304 19:29:48.784259 23118544486528 run.py:483] Algo bellman_ford step 3324 current loss 0.074071, current_train_items 106400.
I0304 19:29:48.804061 23118544486528 run.py:483] Algo bellman_ford step 3325 current loss 0.004303, current_train_items 106432.
I0304 19:29:48.820640 23118544486528 run.py:483] Algo bellman_ford step 3326 current loss 0.038381, current_train_items 106464.
I0304 19:29:48.844479 23118544486528 run.py:483] Algo bellman_ford step 3327 current loss 0.043588, current_train_items 106496.
I0304 19:29:48.875373 23118544486528 run.py:483] Algo bellman_ford step 3328 current loss 0.056579, current_train_items 106528.
I0304 19:29:48.909573 23118544486528 run.py:483] Algo bellman_ford step 3329 current loss 0.070179, current_train_items 106560.
I0304 19:29:48.929614 23118544486528 run.py:483] Algo bellman_ford step 3330 current loss 0.014866, current_train_items 106592.
I0304 19:29:48.945841 23118544486528 run.py:483] Algo bellman_ford step 3331 current loss 0.032940, current_train_items 106624.
I0304 19:29:48.969506 23118544486528 run.py:483] Algo bellman_ford step 3332 current loss 0.044412, current_train_items 106656.
I0304 19:29:49.000094 23118544486528 run.py:483] Algo bellman_ford step 3333 current loss 0.037169, current_train_items 106688.
I0304 19:29:49.033129 23118544486528 run.py:483] Algo bellman_ford step 3334 current loss 0.100598, current_train_items 106720.
I0304 19:29:49.052603 23118544486528 run.py:483] Algo bellman_ford step 3335 current loss 0.007820, current_train_items 106752.
I0304 19:29:49.068567 23118544486528 run.py:483] Algo bellman_ford step 3336 current loss 0.024285, current_train_items 106784.
I0304 19:29:49.093063 23118544486528 run.py:483] Algo bellman_ford step 3337 current loss 0.061700, current_train_items 106816.
I0304 19:29:49.126148 23118544486528 run.py:483] Algo bellman_ford step 3338 current loss 0.082330, current_train_items 106848.
I0304 19:29:49.162064 23118544486528 run.py:483] Algo bellman_ford step 3339 current loss 0.050384, current_train_items 106880.
I0304 19:29:49.181432 23118544486528 run.py:483] Algo bellman_ford step 3340 current loss 0.003062, current_train_items 106912.
I0304 19:29:49.198316 23118544486528 run.py:483] Algo bellman_ford step 3341 current loss 0.021033, current_train_items 106944.
I0304 19:29:49.222652 23118544486528 run.py:483] Algo bellman_ford step 3342 current loss 0.072912, current_train_items 106976.
I0304 19:29:49.255205 23118544486528 run.py:483] Algo bellman_ford step 3343 current loss 0.159046, current_train_items 107008.
I0304 19:29:49.287411 23118544486528 run.py:483] Algo bellman_ford step 3344 current loss 0.098264, current_train_items 107040.
I0304 19:29:49.307290 23118544486528 run.py:483] Algo bellman_ford step 3345 current loss 0.017012, current_train_items 107072.
I0304 19:29:49.323383 23118544486528 run.py:483] Algo bellman_ford step 3346 current loss 0.017233, current_train_items 107104.
I0304 19:29:49.347427 23118544486528 run.py:483] Algo bellman_ford step 3347 current loss 0.052328, current_train_items 107136.
I0304 19:29:49.378418 23118544486528 run.py:483] Algo bellman_ford step 3348 current loss 0.058289, current_train_items 107168.
I0304 19:29:49.410441 23118544486528 run.py:483] Algo bellman_ford step 3349 current loss 0.102937, current_train_items 107200.
I0304 19:29:49.430408 23118544486528 run.py:483] Algo bellman_ford step 3350 current loss 0.036764, current_train_items 107232.
I0304 19:29:49.438487 23118544486528 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0304 19:29:49.438592 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:29:49.456044 23118544486528 run.py:483] Algo bellman_ford step 3351 current loss 0.046220, current_train_items 107264.
I0304 19:29:49.481134 23118544486528 run.py:483] Algo bellman_ford step 3352 current loss 0.072626, current_train_items 107296.
I0304 19:29:49.513862 23118544486528 run.py:483] Algo bellman_ford step 3353 current loss 0.048548, current_train_items 107328.
I0304 19:29:49.548425 23118544486528 run.py:483] Algo bellman_ford step 3354 current loss 0.089488, current_train_items 107360.
I0304 19:29:49.568095 23118544486528 run.py:483] Algo bellman_ford step 3355 current loss 0.005281, current_train_items 107392.
I0304 19:29:49.583991 23118544486528 run.py:483] Algo bellman_ford step 3356 current loss 0.081141, current_train_items 107424.
I0304 19:29:49.608398 23118544486528 run.py:483] Algo bellman_ford step 3357 current loss 0.073241, current_train_items 107456.
I0304 19:29:49.639651 23118544486528 run.py:483] Algo bellman_ford step 3358 current loss 0.067462, current_train_items 107488.
I0304 19:29:49.675175 23118544486528 run.py:483] Algo bellman_ford step 3359 current loss 0.083154, current_train_items 107520.
I0304 19:29:49.695322 23118544486528 run.py:483] Algo bellman_ford step 3360 current loss 0.011059, current_train_items 107552.
I0304 19:29:49.711619 23118544486528 run.py:483] Algo bellman_ford step 3361 current loss 0.032236, current_train_items 107584.
I0304 19:29:49.735891 23118544486528 run.py:483] Algo bellman_ford step 3362 current loss 0.080467, current_train_items 107616.
I0304 19:29:49.766891 23118544486528 run.py:483] Algo bellman_ford step 3363 current loss 0.076824, current_train_items 107648.
I0304 19:29:49.800346 23118544486528 run.py:483] Algo bellman_ford step 3364 current loss 0.102126, current_train_items 107680.
I0304 19:29:49.819820 23118544486528 run.py:483] Algo bellman_ford step 3365 current loss 0.017509, current_train_items 107712.
I0304 19:29:49.835776 23118544486528 run.py:483] Algo bellman_ford step 3366 current loss 0.013342, current_train_items 107744.
I0304 19:29:49.860358 23118544486528 run.py:483] Algo bellman_ford step 3367 current loss 0.095222, current_train_items 107776.
I0304 19:29:49.890436 23118544486528 run.py:483] Algo bellman_ford step 3368 current loss 0.124163, current_train_items 107808.
I0304 19:29:49.923431 23118544486528 run.py:483] Algo bellman_ford step 3369 current loss 0.103408, current_train_items 107840.
I0304 19:29:49.943457 23118544486528 run.py:483] Algo bellman_ford step 3370 current loss 0.006642, current_train_items 107872.
I0304 19:29:49.959989 23118544486528 run.py:483] Algo bellman_ford step 3371 current loss 0.032809, current_train_items 107904.
I0304 19:29:49.983955 23118544486528 run.py:483] Algo bellman_ford step 3372 current loss 0.102336, current_train_items 107936.
I0304 19:29:50.016816 23118544486528 run.py:483] Algo bellman_ford step 3373 current loss 0.112579, current_train_items 107968.
I0304 19:29:50.050057 23118544486528 run.py:483] Algo bellman_ford step 3374 current loss 0.148023, current_train_items 108000.
I0304 19:29:50.069749 23118544486528 run.py:483] Algo bellman_ford step 3375 current loss 0.009856, current_train_items 108032.
I0304 19:29:50.085823 23118544486528 run.py:483] Algo bellman_ford step 3376 current loss 0.020004, current_train_items 108064.
I0304 19:29:50.108941 23118544486528 run.py:483] Algo bellman_ford step 3377 current loss 0.074398, current_train_items 108096.
I0304 19:29:50.139444 23118544486528 run.py:483] Algo bellman_ford step 3378 current loss 0.057956, current_train_items 108128.
I0304 19:29:50.174589 23118544486528 run.py:483] Algo bellman_ford step 3379 current loss 0.116573, current_train_items 108160.
I0304 19:29:50.194176 23118544486528 run.py:483] Algo bellman_ford step 3380 current loss 0.007123, current_train_items 108192.
I0304 19:29:50.210779 23118544486528 run.py:483] Algo bellman_ford step 3381 current loss 0.077069, current_train_items 108224.
I0304 19:29:50.235847 23118544486528 run.py:483] Algo bellman_ford step 3382 current loss 0.075065, current_train_items 108256.
I0304 19:29:50.266329 23118544486528 run.py:483] Algo bellman_ford step 3383 current loss 0.061625, current_train_items 108288.
I0304 19:29:50.301957 23118544486528 run.py:483] Algo bellman_ford step 3384 current loss 0.128828, current_train_items 108320.
I0304 19:29:50.321939 23118544486528 run.py:483] Algo bellman_ford step 3385 current loss 0.005021, current_train_items 108352.
I0304 19:29:50.339035 23118544486528 run.py:483] Algo bellman_ford step 3386 current loss 0.029230, current_train_items 108384.
I0304 19:29:50.363170 23118544486528 run.py:483] Algo bellman_ford step 3387 current loss 0.092315, current_train_items 108416.
I0304 19:29:50.393835 23118544486528 run.py:483] Algo bellman_ford step 3388 current loss 0.104125, current_train_items 108448.
I0304 19:29:50.426194 23118544486528 run.py:483] Algo bellman_ford step 3389 current loss 0.096669, current_train_items 108480.
I0304 19:29:50.446048 23118544486528 run.py:483] Algo bellman_ford step 3390 current loss 0.020901, current_train_items 108512.
I0304 19:29:50.462386 23118544486528 run.py:483] Algo bellman_ford step 3391 current loss 0.020545, current_train_items 108544.
I0304 19:29:50.486152 23118544486528 run.py:483] Algo bellman_ford step 3392 current loss 0.051202, current_train_items 108576.
I0304 19:29:50.516953 23118544486528 run.py:483] Algo bellman_ford step 3393 current loss 0.090448, current_train_items 108608.
I0304 19:29:50.549046 23118544486528 run.py:483] Algo bellman_ford step 3394 current loss 0.069523, current_train_items 108640.
I0304 19:29:50.568586 23118544486528 run.py:483] Algo bellman_ford step 3395 current loss 0.007216, current_train_items 108672.
I0304 19:29:50.585173 23118544486528 run.py:483] Algo bellman_ford step 3396 current loss 0.025615, current_train_items 108704.
I0304 19:29:50.609157 23118544486528 run.py:483] Algo bellman_ford step 3397 current loss 0.042419, current_train_items 108736.
I0304 19:29:50.640955 23118544486528 run.py:483] Algo bellman_ford step 3398 current loss 0.110415, current_train_items 108768.
I0304 19:29:50.673803 23118544486528 run.py:483] Algo bellman_ford step 3399 current loss 0.063325, current_train_items 108800.
I0304 19:29:50.693807 23118544486528 run.py:483] Algo bellman_ford step 3400 current loss 0.006230, current_train_items 108832.
I0304 19:29:50.701286 23118544486528 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0304 19:29:50.701391 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:50.718930 23118544486528 run.py:483] Algo bellman_ford step 3401 current loss 0.030269, current_train_items 108864.
I0304 19:29:50.743699 23118544486528 run.py:483] Algo bellman_ford step 3402 current loss 0.048451, current_train_items 108896.
I0304 19:29:50.775487 23118544486528 run.py:483] Algo bellman_ford step 3403 current loss 0.082625, current_train_items 108928.
I0304 19:29:50.811404 23118544486528 run.py:483] Algo bellman_ford step 3404 current loss 0.130056, current_train_items 108960.
I0304 19:29:50.831271 23118544486528 run.py:483] Algo bellman_ford step 3405 current loss 0.004416, current_train_items 108992.
I0304 19:29:50.847049 23118544486528 run.py:483] Algo bellman_ford step 3406 current loss 0.033751, current_train_items 109024.
I0304 19:29:50.871664 23118544486528 run.py:483] Algo bellman_ford step 3407 current loss 0.075232, current_train_items 109056.
I0304 19:29:50.904971 23118544486528 run.py:483] Algo bellman_ford step 3408 current loss 0.081654, current_train_items 109088.
I0304 19:29:50.938380 23118544486528 run.py:483] Algo bellman_ford step 3409 current loss 0.060916, current_train_items 109120.
I0304 19:29:50.958176 23118544486528 run.py:483] Algo bellman_ford step 3410 current loss 0.004629, current_train_items 109152.
I0304 19:29:50.975036 23118544486528 run.py:483] Algo bellman_ford step 3411 current loss 0.088940, current_train_items 109184.
I0304 19:29:51.000003 23118544486528 run.py:483] Algo bellman_ford step 3412 current loss 0.142214, current_train_items 109216.
I0304 19:29:51.032790 23118544486528 run.py:483] Algo bellman_ford step 3413 current loss 0.208104, current_train_items 109248.
I0304 19:29:51.069560 23118544486528 run.py:483] Algo bellman_ford step 3414 current loss 0.200537, current_train_items 109280.
I0304 19:29:51.089591 23118544486528 run.py:483] Algo bellman_ford step 3415 current loss 0.009247, current_train_items 109312.
I0304 19:29:51.106140 23118544486528 run.py:483] Algo bellman_ford step 3416 current loss 0.019883, current_train_items 109344.
I0304 19:29:51.130505 23118544486528 run.py:483] Algo bellman_ford step 3417 current loss 0.053853, current_train_items 109376.
I0304 19:29:51.162024 23118544486528 run.py:483] Algo bellman_ford step 3418 current loss 0.229398, current_train_items 109408.
I0304 19:29:51.195640 23118544486528 run.py:483] Algo bellman_ford step 3419 current loss 0.111080, current_train_items 109440.
I0304 19:29:51.215449 23118544486528 run.py:483] Algo bellman_ford step 3420 current loss 0.010719, current_train_items 109472.
I0304 19:29:51.231573 23118544486528 run.py:483] Algo bellman_ford step 3421 current loss 0.043567, current_train_items 109504.
I0304 19:29:51.255840 23118544486528 run.py:483] Algo bellman_ford step 3422 current loss 0.059723, current_train_items 109536.
I0304 19:29:51.287662 23118544486528 run.py:483] Algo bellman_ford step 3423 current loss 0.049789, current_train_items 109568.
I0304 19:29:51.321548 23118544486528 run.py:483] Algo bellman_ford step 3424 current loss 0.102908, current_train_items 109600.
I0304 19:29:51.341493 23118544486528 run.py:483] Algo bellman_ford step 3425 current loss 0.012991, current_train_items 109632.
I0304 19:29:51.358554 23118544486528 run.py:483] Algo bellman_ford step 3426 current loss 0.037189, current_train_items 109664.
I0304 19:29:51.382553 23118544486528 run.py:483] Algo bellman_ford step 3427 current loss 0.057698, current_train_items 109696.
I0304 19:29:51.414535 23118544486528 run.py:483] Algo bellman_ford step 3428 current loss 0.087644, current_train_items 109728.
I0304 19:29:51.445957 23118544486528 run.py:483] Algo bellman_ford step 3429 current loss 0.105377, current_train_items 109760.
I0304 19:29:51.465954 23118544486528 run.py:483] Algo bellman_ford step 3430 current loss 0.009323, current_train_items 109792.
I0304 19:29:51.482620 23118544486528 run.py:483] Algo bellman_ford step 3431 current loss 0.009856, current_train_items 109824.
I0304 19:29:51.506436 23118544486528 run.py:483] Algo bellman_ford step 3432 current loss 0.046508, current_train_items 109856.
I0304 19:29:51.539700 23118544486528 run.py:483] Algo bellman_ford step 3433 current loss 0.143566, current_train_items 109888.
I0304 19:29:51.572715 23118544486528 run.py:483] Algo bellman_ford step 3434 current loss 0.139093, current_train_items 109920.
I0304 19:29:51.592294 23118544486528 run.py:483] Algo bellman_ford step 3435 current loss 0.006317, current_train_items 109952.
I0304 19:29:51.608778 23118544486528 run.py:483] Algo bellman_ford step 3436 current loss 0.036779, current_train_items 109984.
I0304 19:29:51.633072 23118544486528 run.py:483] Algo bellman_ford step 3437 current loss 0.053579, current_train_items 110016.
I0304 19:29:51.664200 23118544486528 run.py:483] Algo bellman_ford step 3438 current loss 0.080735, current_train_items 110048.
I0304 19:29:51.696466 23118544486528 run.py:483] Algo bellman_ford step 3439 current loss 0.078534, current_train_items 110080.
I0304 19:29:51.716439 23118544486528 run.py:483] Algo bellman_ford step 3440 current loss 0.048916, current_train_items 110112.
I0304 19:29:51.733091 23118544486528 run.py:483] Algo bellman_ford step 3441 current loss 0.038378, current_train_items 110144.
I0304 19:29:51.757010 23118544486528 run.py:483] Algo bellman_ford step 3442 current loss 0.052755, current_train_items 110176.
I0304 19:29:51.789181 23118544486528 run.py:483] Algo bellman_ford step 3443 current loss 0.119039, current_train_items 110208.
I0304 19:29:51.824288 23118544486528 run.py:483] Algo bellman_ford step 3444 current loss 0.102002, current_train_items 110240.
I0304 19:29:51.843870 23118544486528 run.py:483] Algo bellman_ford step 3445 current loss 0.004335, current_train_items 110272.
I0304 19:29:51.860670 23118544486528 run.py:483] Algo bellman_ford step 3446 current loss 0.029360, current_train_items 110304.
I0304 19:29:51.885034 23118544486528 run.py:483] Algo bellman_ford step 3447 current loss 0.054250, current_train_items 110336.
I0304 19:29:51.916446 23118544486528 run.py:483] Algo bellman_ford step 3448 current loss 0.037358, current_train_items 110368.
I0304 19:29:51.950560 23118544486528 run.py:483] Algo bellman_ford step 3449 current loss 0.075096, current_train_items 110400.
I0304 19:29:51.970307 23118544486528 run.py:483] Algo bellman_ford step 3450 current loss 0.015004, current_train_items 110432.
I0304 19:29:51.978652 23118544486528 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0304 19:29:51.978765 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0304 19:29:51.995638 23118544486528 run.py:483] Algo bellman_ford step 3451 current loss 0.016786, current_train_items 110464.
I0304 19:29:52.020575 23118544486528 run.py:483] Algo bellman_ford step 3452 current loss 0.053283, current_train_items 110496.
I0304 19:29:52.051101 23118544486528 run.py:483] Algo bellman_ford step 3453 current loss 0.065566, current_train_items 110528.
I0304 19:29:52.087562 23118544486528 run.py:483] Algo bellman_ford step 3454 current loss 0.095556, current_train_items 110560.
I0304 19:29:52.107750 23118544486528 run.py:483] Algo bellman_ford step 3455 current loss 0.003776, current_train_items 110592.
I0304 19:29:52.123632 23118544486528 run.py:483] Algo bellman_ford step 3456 current loss 0.059490, current_train_items 110624.
I0304 19:29:52.148047 23118544486528 run.py:483] Algo bellman_ford step 3457 current loss 0.039612, current_train_items 110656.
I0304 19:29:52.180711 23118544486528 run.py:483] Algo bellman_ford step 3458 current loss 0.068669, current_train_items 110688.
I0304 19:29:52.213911 23118544486528 run.py:483] Algo bellman_ford step 3459 current loss 0.053871, current_train_items 110720.
I0304 19:29:52.234172 23118544486528 run.py:483] Algo bellman_ford step 3460 current loss 0.012088, current_train_items 110752.
I0304 19:29:52.250933 23118544486528 run.py:483] Algo bellman_ford step 3461 current loss 0.064376, current_train_items 110784.
I0304 19:29:52.274644 23118544486528 run.py:483] Algo bellman_ford step 3462 current loss 0.088325, current_train_items 110816.
I0304 19:29:52.305986 23118544486528 run.py:483] Algo bellman_ford step 3463 current loss 0.061649, current_train_items 110848.
I0304 19:29:52.339244 23118544486528 run.py:483] Algo bellman_ford step 3464 current loss 0.112785, current_train_items 110880.
I0304 19:29:52.358912 23118544486528 run.py:483] Algo bellman_ford step 3465 current loss 0.019475, current_train_items 110912.
I0304 19:29:52.375549 23118544486528 run.py:483] Algo bellman_ford step 3466 current loss 0.028361, current_train_items 110944.
I0304 19:29:52.400429 23118544486528 run.py:483] Algo bellman_ford step 3467 current loss 0.096361, current_train_items 110976.
I0304 19:29:52.433588 23118544486528 run.py:483] Algo bellman_ford step 3468 current loss 0.109925, current_train_items 111008.
I0304 19:29:52.469647 23118544486528 run.py:483] Algo bellman_ford step 3469 current loss 0.114685, current_train_items 111040.
I0304 19:29:52.489619 23118544486528 run.py:483] Algo bellman_ford step 3470 current loss 0.018499, current_train_items 111072.
I0304 19:29:52.506566 23118544486528 run.py:483] Algo bellman_ford step 3471 current loss 0.021500, current_train_items 111104.
I0304 19:29:52.530120 23118544486528 run.py:483] Algo bellman_ford step 3472 current loss 0.071638, current_train_items 111136.
I0304 19:29:52.562930 23118544486528 run.py:483] Algo bellman_ford step 3473 current loss 0.101286, current_train_items 111168.
I0304 19:29:52.597315 23118544486528 run.py:483] Algo bellman_ford step 3474 current loss 0.088929, current_train_items 111200.
I0304 19:29:52.617547 23118544486528 run.py:483] Algo bellman_ford step 3475 current loss 0.014424, current_train_items 111232.
I0304 19:29:52.633867 23118544486528 run.py:483] Algo bellman_ford step 3476 current loss 0.035930, current_train_items 111264.
I0304 19:29:52.657418 23118544486528 run.py:483] Algo bellman_ford step 3477 current loss 0.068049, current_train_items 111296.
I0304 19:29:52.687988 23118544486528 run.py:483] Algo bellman_ford step 3478 current loss 0.046450, current_train_items 111328.
I0304 19:29:52.723098 23118544486528 run.py:483] Algo bellman_ford step 3479 current loss 0.131849, current_train_items 111360.
I0304 19:29:52.742842 23118544486528 run.py:483] Algo bellman_ford step 3480 current loss 0.008311, current_train_items 111392.
I0304 19:29:52.759116 23118544486528 run.py:483] Algo bellman_ford step 3481 current loss 0.026508, current_train_items 111424.
I0304 19:29:52.783223 23118544486528 run.py:483] Algo bellman_ford step 3482 current loss 0.057904, current_train_items 111456.
I0304 19:29:52.814083 23118544486528 run.py:483] Algo bellman_ford step 3483 current loss 0.079497, current_train_items 111488.
I0304 19:29:52.847718 23118544486528 run.py:483] Algo bellman_ford step 3484 current loss 0.082387, current_train_items 111520.
I0304 19:29:52.867989 23118544486528 run.py:483] Algo bellman_ford step 3485 current loss 0.006356, current_train_items 111552.
I0304 19:29:52.884635 23118544486528 run.py:483] Algo bellman_ford step 3486 current loss 0.051140, current_train_items 111584.
I0304 19:29:52.909162 23118544486528 run.py:483] Algo bellman_ford step 3487 current loss 0.155163, current_train_items 111616.
I0304 19:29:52.939782 23118544486528 run.py:483] Algo bellman_ford step 3488 current loss 0.095321, current_train_items 111648.
I0304 19:29:52.972414 23118544486528 run.py:483] Algo bellman_ford step 3489 current loss 0.112690, current_train_items 111680.
I0304 19:29:52.992554 23118544486528 run.py:483] Algo bellman_ford step 3490 current loss 0.007290, current_train_items 111712.
I0304 19:29:53.008734 23118544486528 run.py:483] Algo bellman_ford step 3491 current loss 0.011636, current_train_items 111744.
I0304 19:29:53.032420 23118544486528 run.py:483] Algo bellman_ford step 3492 current loss 0.104992, current_train_items 111776.
I0304 19:29:53.064906 23118544486528 run.py:483] Algo bellman_ford step 3493 current loss 0.097126, current_train_items 111808.
I0304 19:29:53.098535 23118544486528 run.py:483] Algo bellman_ford step 3494 current loss 0.092058, current_train_items 111840.
I0304 19:29:53.118001 23118544486528 run.py:483] Algo bellman_ford step 3495 current loss 0.005784, current_train_items 111872.
I0304 19:29:53.134416 23118544486528 run.py:483] Algo bellman_ford step 3496 current loss 0.043118, current_train_items 111904.
I0304 19:29:53.158100 23118544486528 run.py:483] Algo bellman_ford step 3497 current loss 0.078953, current_train_items 111936.
I0304 19:29:53.189506 23118544486528 run.py:483] Algo bellman_ford step 3498 current loss 0.093486, current_train_items 111968.
I0304 19:29:53.224377 23118544486528 run.py:483] Algo bellman_ford step 3499 current loss 0.096471, current_train_items 112000.
I0304 19:29:53.244374 23118544486528 run.py:483] Algo bellman_ford step 3500 current loss 0.006206, current_train_items 112032.
I0304 19:29:53.251870 23118544486528 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0304 19:29:53.251974 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0304 19:29:53.268389 23118544486528 run.py:483] Algo bellman_ford step 3501 current loss 0.018310, current_train_items 112064.
I0304 19:29:53.293260 23118544486528 run.py:483] Algo bellman_ford step 3502 current loss 0.077360, current_train_items 112096.
I0304 19:29:53.325533 23118544486528 run.py:483] Algo bellman_ford step 3503 current loss 0.145688, current_train_items 112128.
I0304 19:29:53.361605 23118544486528 run.py:483] Algo bellman_ford step 3504 current loss 0.144327, current_train_items 112160.
I0304 19:29:53.382080 23118544486528 run.py:483] Algo bellman_ford step 3505 current loss 0.017480, current_train_items 112192.
I0304 19:29:53.397510 23118544486528 run.py:483] Algo bellman_ford step 3506 current loss 0.024735, current_train_items 112224.
I0304 19:29:53.422430 23118544486528 run.py:483] Algo bellman_ford step 3507 current loss 0.053196, current_train_items 112256.
I0304 19:29:53.453243 23118544486528 run.py:483] Algo bellman_ford step 3508 current loss 0.102626, current_train_items 112288.
I0304 19:29:53.484480 23118544486528 run.py:483] Algo bellman_ford step 3509 current loss 0.087646, current_train_items 112320.
I0304 19:29:53.504359 23118544486528 run.py:483] Algo bellman_ford step 3510 current loss 0.014903, current_train_items 112352.
I0304 19:29:53.520587 23118544486528 run.py:483] Algo bellman_ford step 3511 current loss 0.021903, current_train_items 112384.
I0304 19:29:53.544361 23118544486528 run.py:483] Algo bellman_ford step 3512 current loss 0.035096, current_train_items 112416.
I0304 19:29:53.574800 23118544486528 run.py:483] Algo bellman_ford step 3513 current loss 0.046958, current_train_items 112448.
I0304 19:29:53.607649 23118544486528 run.py:483] Algo bellman_ford step 3514 current loss 0.055753, current_train_items 112480.
I0304 19:29:53.627643 23118544486528 run.py:483] Algo bellman_ford step 3515 current loss 0.005870, current_train_items 112512.
I0304 19:29:53.643608 23118544486528 run.py:483] Algo bellman_ford step 3516 current loss 0.022359, current_train_items 112544.
I0304 19:29:53.668136 23118544486528 run.py:483] Algo bellman_ford step 3517 current loss 0.152510, current_train_items 112576.
I0304 19:29:53.700222 23118544486528 run.py:483] Algo bellman_ford step 3518 current loss 0.268487, current_train_items 112608.
I0304 19:29:53.736587 23118544486528 run.py:483] Algo bellman_ford step 3519 current loss 0.192598, current_train_items 112640.
I0304 19:29:53.756427 23118544486528 run.py:483] Algo bellman_ford step 3520 current loss 0.017287, current_train_items 112672.
I0304 19:29:53.772711 23118544486528 run.py:483] Algo bellman_ford step 3521 current loss 0.031473, current_train_items 112704.
I0304 19:29:53.795854 23118544486528 run.py:483] Algo bellman_ford step 3522 current loss 0.032342, current_train_items 112736.
I0304 19:29:53.827102 23118544486528 run.py:483] Algo bellman_ford step 3523 current loss 0.116191, current_train_items 112768.
I0304 19:29:53.861330 23118544486528 run.py:483] Algo bellman_ford step 3524 current loss 0.207232, current_train_items 112800.
I0304 19:29:53.880951 23118544486528 run.py:483] Algo bellman_ford step 3525 current loss 0.103329, current_train_items 112832.
I0304 19:29:53.897160 23118544486528 run.py:483] Algo bellman_ford step 3526 current loss 0.029439, current_train_items 112864.
I0304 19:29:53.921744 23118544486528 run.py:483] Algo bellman_ford step 3527 current loss 0.048508, current_train_items 112896.
I0304 19:29:53.954853 23118544486528 run.py:483] Algo bellman_ford step 3528 current loss 0.066819, current_train_items 112928.
I0304 19:29:53.987823 23118544486528 run.py:483] Algo bellman_ford step 3529 current loss 0.068954, current_train_items 112960.
I0304 19:29:54.007449 23118544486528 run.py:483] Algo bellman_ford step 3530 current loss 0.008609, current_train_items 112992.
I0304 19:29:54.023998 23118544486528 run.py:483] Algo bellman_ford step 3531 current loss 0.025321, current_train_items 113024.
I0304 19:29:54.049314 23118544486528 run.py:483] Algo bellman_ford step 3532 current loss 0.066602, current_train_items 113056.
I0304 19:29:54.083800 23118544486528 run.py:483] Algo bellman_ford step 3533 current loss 0.082431, current_train_items 113088.
I0304 19:29:54.118681 23118544486528 run.py:483] Algo bellman_ford step 3534 current loss 0.114037, current_train_items 113120.
I0304 19:29:54.138528 23118544486528 run.py:483] Algo bellman_ford step 3535 current loss 0.025129, current_train_items 113152.
I0304 19:29:54.155160 23118544486528 run.py:483] Algo bellman_ford step 3536 current loss 0.068425, current_train_items 113184.
I0304 19:29:54.179502 23118544486528 run.py:483] Algo bellman_ford step 3537 current loss 0.076091, current_train_items 113216.
I0304 19:29:54.211803 23118544486528 run.py:483] Algo bellman_ford step 3538 current loss 0.111558, current_train_items 113248.
I0304 19:29:54.247902 23118544486528 run.py:483] Algo bellman_ford step 3539 current loss 0.070925, current_train_items 113280.
I0304 19:29:54.267473 23118544486528 run.py:483] Algo bellman_ford step 3540 current loss 0.008661, current_train_items 113312.
I0304 19:29:54.283741 23118544486528 run.py:483] Algo bellman_ford step 3541 current loss 0.027135, current_train_items 113344.
I0304 19:29:54.309272 23118544486528 run.py:483] Algo bellman_ford step 3542 current loss 0.096195, current_train_items 113376.
I0304 19:29:54.340248 23118544486528 run.py:483] Algo bellman_ford step 3543 current loss 0.094764, current_train_items 113408.
I0304 19:29:54.373856 23118544486528 run.py:483] Algo bellman_ford step 3544 current loss 0.107556, current_train_items 113440.
I0304 19:29:54.393377 23118544486528 run.py:483] Algo bellman_ford step 3545 current loss 0.005644, current_train_items 113472.
I0304 19:29:54.409614 23118544486528 run.py:483] Algo bellman_ford step 3546 current loss 0.015007, current_train_items 113504.
I0304 19:29:54.434178 23118544486528 run.py:483] Algo bellman_ford step 3547 current loss 0.101077, current_train_items 113536.
I0304 19:29:54.464587 23118544486528 run.py:483] Algo bellman_ford step 3548 current loss 0.102926, current_train_items 113568.
I0304 19:29:54.499483 23118544486528 run.py:483] Algo bellman_ford step 3549 current loss 0.156845, current_train_items 113600.
I0304 19:29:54.519200 23118544486528 run.py:483] Algo bellman_ford step 3550 current loss 0.014484, current_train_items 113632.
I0304 19:29:54.527105 23118544486528 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0304 19:29:54.527215 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:29:54.544535 23118544486528 run.py:483] Algo bellman_ford step 3551 current loss 0.040084, current_train_items 113664.
I0304 19:29:54.570536 23118544486528 run.py:483] Algo bellman_ford step 3552 current loss 0.050691, current_train_items 113696.
I0304 19:29:54.602955 23118544486528 run.py:483] Algo bellman_ford step 3553 current loss 0.042285, current_train_items 113728.
I0304 19:29:54.637696 23118544486528 run.py:483] Algo bellman_ford step 3554 current loss 0.068628, current_train_items 113760.
I0304 19:29:54.657743 23118544486528 run.py:483] Algo bellman_ford step 3555 current loss 0.009768, current_train_items 113792.
I0304 19:29:54.673912 23118544486528 run.py:483] Algo bellman_ford step 3556 current loss 0.039399, current_train_items 113824.
I0304 19:29:54.697988 23118544486528 run.py:483] Algo bellman_ford step 3557 current loss 0.094996, current_train_items 113856.
I0304 19:29:54.730113 23118544486528 run.py:483] Algo bellman_ford step 3558 current loss 0.078466, current_train_items 113888.
I0304 19:29:54.763482 23118544486528 run.py:483] Algo bellman_ford step 3559 current loss 0.065896, current_train_items 113920.
I0304 19:29:54.783313 23118544486528 run.py:483] Algo bellman_ford step 3560 current loss 0.009609, current_train_items 113952.
I0304 19:29:54.799670 23118544486528 run.py:483] Algo bellman_ford step 3561 current loss 0.030138, current_train_items 113984.
I0304 19:29:54.824855 23118544486528 run.py:483] Algo bellman_ford step 3562 current loss 0.239020, current_train_items 114016.
I0304 19:29:54.855319 23118544486528 run.py:483] Algo bellman_ford step 3563 current loss 0.100042, current_train_items 114048.
I0304 19:29:54.892578 23118544486528 run.py:483] Algo bellman_ford step 3564 current loss 0.140934, current_train_items 114080.
I0304 19:29:54.912297 23118544486528 run.py:483] Algo bellman_ford step 3565 current loss 0.005846, current_train_items 114112.
I0304 19:29:54.928850 23118544486528 run.py:483] Algo bellman_ford step 3566 current loss 0.041479, current_train_items 114144.
I0304 19:29:54.952885 23118544486528 run.py:483] Algo bellman_ford step 3567 current loss 0.087119, current_train_items 114176.
I0304 19:29:54.984027 23118544486528 run.py:483] Algo bellman_ford step 3568 current loss 0.085217, current_train_items 114208.
I0304 19:29:55.018325 23118544486528 run.py:483] Algo bellman_ford step 3569 current loss 0.122316, current_train_items 114240.
I0304 19:29:55.037990 23118544486528 run.py:483] Algo bellman_ford step 3570 current loss 0.004943, current_train_items 114272.
I0304 19:29:55.054091 23118544486528 run.py:483] Algo bellman_ford step 3571 current loss 0.013312, current_train_items 114304.
I0304 19:29:55.077944 23118544486528 run.py:483] Algo bellman_ford step 3572 current loss 0.102092, current_train_items 114336.
I0304 19:29:55.108443 23118544486528 run.py:483] Algo bellman_ford step 3573 current loss 0.084956, current_train_items 114368.
I0304 19:29:55.144438 23118544486528 run.py:483] Algo bellman_ford step 3574 current loss 0.110222, current_train_items 114400.
I0304 19:29:55.164175 23118544486528 run.py:483] Algo bellman_ford step 3575 current loss 0.006439, current_train_items 114432.
I0304 19:29:55.180989 23118544486528 run.py:483] Algo bellman_ford step 3576 current loss 0.028060, current_train_items 114464.
I0304 19:29:55.205517 23118544486528 run.py:483] Algo bellman_ford step 3577 current loss 0.086935, current_train_items 114496.
I0304 19:29:55.236989 23118544486528 run.py:483] Algo bellman_ford step 3578 current loss 0.052483, current_train_items 114528.
I0304 19:29:55.270299 23118544486528 run.py:483] Algo bellman_ford step 3579 current loss 0.079226, current_train_items 114560.
I0304 19:29:55.290143 23118544486528 run.py:483] Algo bellman_ford step 3580 current loss 0.006536, current_train_items 114592.
I0304 19:29:55.306589 23118544486528 run.py:483] Algo bellman_ford step 3581 current loss 0.066207, current_train_items 114624.
I0304 19:29:55.331939 23118544486528 run.py:483] Algo bellman_ford step 3582 current loss 0.077850, current_train_items 114656.
I0304 19:29:55.364164 23118544486528 run.py:483] Algo bellman_ford step 3583 current loss 0.043324, current_train_items 114688.
I0304 19:29:55.398782 23118544486528 run.py:483] Algo bellman_ford step 3584 current loss 0.081425, current_train_items 114720.
I0304 19:29:55.418691 23118544486528 run.py:483] Algo bellman_ford step 3585 current loss 0.004613, current_train_items 114752.
I0304 19:29:55.435209 23118544486528 run.py:483] Algo bellman_ford step 3586 current loss 0.080063, current_train_items 114784.
I0304 19:29:55.458945 23118544486528 run.py:483] Algo bellman_ford step 3587 current loss 0.050854, current_train_items 114816.
I0304 19:29:55.491349 23118544486528 run.py:483] Algo bellman_ford step 3588 current loss 0.062415, current_train_items 114848.
I0304 19:29:55.526238 23118544486528 run.py:483] Algo bellman_ford step 3589 current loss 0.080602, current_train_items 114880.
I0304 19:29:55.545867 23118544486528 run.py:483] Algo bellman_ford step 3590 current loss 0.008960, current_train_items 114912.
I0304 19:29:55.562341 23118544486528 run.py:483] Algo bellman_ford step 3591 current loss 0.029755, current_train_items 114944.
I0304 19:29:55.587566 23118544486528 run.py:483] Algo bellman_ford step 3592 current loss 0.096789, current_train_items 114976.
I0304 19:29:55.620465 23118544486528 run.py:483] Algo bellman_ford step 3593 current loss 0.074387, current_train_items 115008.
I0304 19:29:55.655172 23118544486528 run.py:483] Algo bellman_ford step 3594 current loss 0.082102, current_train_items 115040.
I0304 19:29:55.674849 23118544486528 run.py:483] Algo bellman_ford step 3595 current loss 0.008499, current_train_items 115072.
I0304 19:29:55.691495 23118544486528 run.py:483] Algo bellman_ford step 3596 current loss 0.035150, current_train_items 115104.
I0304 19:29:55.715577 23118544486528 run.py:483] Algo bellman_ford step 3597 current loss 0.033427, current_train_items 115136.
I0304 19:29:55.745935 23118544486528 run.py:483] Algo bellman_ford step 3598 current loss 0.050600, current_train_items 115168.
I0304 19:29:55.780778 23118544486528 run.py:483] Algo bellman_ford step 3599 current loss 0.079867, current_train_items 115200.
I0304 19:29:55.800906 23118544486528 run.py:483] Algo bellman_ford step 3600 current loss 0.006033, current_train_items 115232.
I0304 19:29:55.808890 23118544486528 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0304 19:29:55.809025 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:29:55.826390 23118544486528 run.py:483] Algo bellman_ford step 3601 current loss 0.029250, current_train_items 115264.
I0304 19:29:55.852229 23118544486528 run.py:483] Algo bellman_ford step 3602 current loss 0.046275, current_train_items 115296.
I0304 19:29:55.883566 23118544486528 run.py:483] Algo bellman_ford step 3603 current loss 0.074823, current_train_items 115328.
I0304 19:29:55.920420 23118544486528 run.py:483] Algo bellman_ford step 3604 current loss 0.130196, current_train_items 115360.
I0304 19:29:55.940253 23118544486528 run.py:483] Algo bellman_ford step 3605 current loss 0.007453, current_train_items 115392.
I0304 19:29:55.956923 23118544486528 run.py:483] Algo bellman_ford step 3606 current loss 0.038453, current_train_items 115424.
I0304 19:29:55.980554 23118544486528 run.py:483] Algo bellman_ford step 3607 current loss 0.041729, current_train_items 115456.
I0304 19:29:56.011057 23118544486528 run.py:483] Algo bellman_ford step 3608 current loss 0.048125, current_train_items 115488.
I0304 19:29:56.043640 23118544486528 run.py:483] Algo bellman_ford step 3609 current loss 0.060487, current_train_items 115520.
I0304 19:29:56.063578 23118544486528 run.py:483] Algo bellman_ford step 3610 current loss 0.004574, current_train_items 115552.
I0304 19:29:56.080282 23118544486528 run.py:483] Algo bellman_ford step 3611 current loss 0.024950, current_train_items 115584.
I0304 19:29:56.105411 23118544486528 run.py:483] Algo bellman_ford step 3612 current loss 0.079752, current_train_items 115616.
I0304 19:29:56.136575 23118544486528 run.py:483] Algo bellman_ford step 3613 current loss 0.058163, current_train_items 115648.
I0304 19:29:56.169162 23118544486528 run.py:483] Algo bellman_ford step 3614 current loss 0.065357, current_train_items 115680.
I0304 19:29:56.188791 23118544486528 run.py:483] Algo bellman_ford step 3615 current loss 0.012513, current_train_items 115712.
I0304 19:29:56.205564 23118544486528 run.py:483] Algo bellman_ford step 3616 current loss 0.029139, current_train_items 115744.
I0304 19:29:56.229499 23118544486528 run.py:483] Algo bellman_ford step 3617 current loss 0.063755, current_train_items 115776.
I0304 19:29:56.261551 23118544486528 run.py:483] Algo bellman_ford step 3618 current loss 0.078250, current_train_items 115808.
I0304 19:29:56.292486 23118544486528 run.py:483] Algo bellman_ford step 3619 current loss 0.068567, current_train_items 115840.
I0304 19:29:56.312153 23118544486528 run.py:483] Algo bellman_ford step 3620 current loss 0.007041, current_train_items 115872.
I0304 19:29:56.328644 23118544486528 run.py:483] Algo bellman_ford step 3621 current loss 0.042793, current_train_items 115904.
I0304 19:29:56.352916 23118544486528 run.py:483] Algo bellman_ford step 3622 current loss 0.059325, current_train_items 115936.
I0304 19:29:56.385349 23118544486528 run.py:483] Algo bellman_ford step 3623 current loss 0.070419, current_train_items 115968.
I0304 19:29:56.418838 23118544486528 run.py:483] Algo bellman_ford step 3624 current loss 0.073018, current_train_items 116000.
I0304 19:29:56.438298 23118544486528 run.py:483] Algo bellman_ford step 3625 current loss 0.007580, current_train_items 116032.
I0304 19:29:56.454817 23118544486528 run.py:483] Algo bellman_ford step 3626 current loss 0.048923, current_train_items 116064.
I0304 19:29:56.479389 23118544486528 run.py:483] Algo bellman_ford step 3627 current loss 0.071089, current_train_items 116096.
I0304 19:29:56.511368 23118544486528 run.py:483] Algo bellman_ford step 3628 current loss 0.126831, current_train_items 116128.
I0304 19:29:56.544837 23118544486528 run.py:483] Algo bellman_ford step 3629 current loss 0.067621, current_train_items 116160.
I0304 19:29:56.564620 23118544486528 run.py:483] Algo bellman_ford step 3630 current loss 0.014306, current_train_items 116192.
I0304 19:29:56.580896 23118544486528 run.py:483] Algo bellman_ford step 3631 current loss 0.050399, current_train_items 116224.
I0304 19:29:56.605340 23118544486528 run.py:483] Algo bellman_ford step 3632 current loss 0.079044, current_train_items 116256.
I0304 19:29:56.637184 23118544486528 run.py:483] Algo bellman_ford step 3633 current loss 0.061450, current_train_items 116288.
I0304 19:29:56.670465 23118544486528 run.py:483] Algo bellman_ford step 3634 current loss 0.119046, current_train_items 116320.
I0304 19:29:56.690093 23118544486528 run.py:483] Algo bellman_ford step 3635 current loss 0.005993, current_train_items 116352.
I0304 19:29:56.706460 23118544486528 run.py:483] Algo bellman_ford step 3636 current loss 0.046689, current_train_items 116384.
I0304 19:29:56.729591 23118544486528 run.py:483] Algo bellman_ford step 3637 current loss 0.083278, current_train_items 116416.
I0304 19:29:56.761914 23118544486528 run.py:483] Algo bellman_ford step 3638 current loss 0.051667, current_train_items 116448.
I0304 19:29:56.795711 23118544486528 run.py:483] Algo bellman_ford step 3639 current loss 0.101463, current_train_items 116480.
I0304 19:29:56.815528 23118544486528 run.py:483] Algo bellman_ford step 3640 current loss 0.011286, current_train_items 116512.
I0304 19:29:56.831773 23118544486528 run.py:483] Algo bellman_ford step 3641 current loss 0.025723, current_train_items 116544.
I0304 19:29:56.856785 23118544486528 run.py:483] Algo bellman_ford step 3642 current loss 0.199662, current_train_items 116576.
I0304 19:29:56.891217 23118544486528 run.py:483] Algo bellman_ford step 3643 current loss 0.118297, current_train_items 116608.
I0304 19:29:56.926251 23118544486528 run.py:483] Algo bellman_ford step 3644 current loss 0.122798, current_train_items 116640.
I0304 19:29:56.945874 23118544486528 run.py:483] Algo bellman_ford step 3645 current loss 0.004976, current_train_items 116672.
I0304 19:29:56.962313 23118544486528 run.py:483] Algo bellman_ford step 3646 current loss 0.037244, current_train_items 116704.
I0304 19:29:56.988348 23118544486528 run.py:483] Algo bellman_ford step 3647 current loss 0.096867, current_train_items 116736.
I0304 19:29:57.019532 23118544486528 run.py:483] Algo bellman_ford step 3648 current loss 0.084064, current_train_items 116768.
I0304 19:29:57.053871 23118544486528 run.py:483] Algo bellman_ford step 3649 current loss 0.103078, current_train_items 116800.
I0304 19:29:57.073380 23118544486528 run.py:483] Algo bellman_ford step 3650 current loss 0.006016, current_train_items 116832.
I0304 19:29:57.081620 23118544486528 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0304 19:29:57.081735 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0304 19:29:57.099344 23118544486528 run.py:483] Algo bellman_ford step 3651 current loss 0.020615, current_train_items 116864.
I0304 19:29:57.122634 23118544486528 run.py:483] Algo bellman_ford step 3652 current loss 0.113678, current_train_items 116896.
I0304 19:29:57.153614 23118544486528 run.py:483] Algo bellman_ford step 3653 current loss 0.072700, current_train_items 116928.
I0304 19:29:57.187147 23118544486528 run.py:483] Algo bellman_ford step 3654 current loss 0.100348, current_train_items 116960.
I0304 19:29:57.207116 23118544486528 run.py:483] Algo bellman_ford step 3655 current loss 0.003864, current_train_items 116992.
I0304 19:29:57.222811 23118544486528 run.py:483] Algo bellman_ford step 3656 current loss 0.034033, current_train_items 117024.
I0304 19:29:57.247269 23118544486528 run.py:483] Algo bellman_ford step 3657 current loss 0.072922, current_train_items 117056.
I0304 19:29:57.278098 23118544486528 run.py:483] Algo bellman_ford step 3658 current loss 0.110474, current_train_items 117088.
I0304 19:29:57.312844 23118544486528 run.py:483] Algo bellman_ford step 3659 current loss 0.113627, current_train_items 117120.
I0304 19:29:57.332851 23118544486528 run.py:483] Algo bellman_ford step 3660 current loss 0.007155, current_train_items 117152.
I0304 19:29:57.349470 23118544486528 run.py:483] Algo bellman_ford step 3661 current loss 0.015276, current_train_items 117184.
I0304 19:29:57.372730 23118544486528 run.py:483] Algo bellman_ford step 3662 current loss 0.026585, current_train_items 117216.
I0304 19:29:57.405113 23118544486528 run.py:483] Algo bellman_ford step 3663 current loss 0.082843, current_train_items 117248.
I0304 19:29:57.440469 23118544486528 run.py:483] Algo bellman_ford step 3664 current loss 0.106680, current_train_items 117280.
I0304 19:29:57.460110 23118544486528 run.py:483] Algo bellman_ford step 3665 current loss 0.037837, current_train_items 117312.
I0304 19:29:57.476381 23118544486528 run.py:483] Algo bellman_ford step 3666 current loss 0.020813, current_train_items 117344.
I0304 19:29:57.500838 23118544486528 run.py:483] Algo bellman_ford step 3667 current loss 0.079919, current_train_items 117376.
I0304 19:29:57.532522 23118544486528 run.py:483] Algo bellman_ford step 3668 current loss 0.135919, current_train_items 117408.
I0304 19:29:57.568058 23118544486528 run.py:483] Algo bellman_ford step 3669 current loss 0.129906, current_train_items 117440.
I0304 19:29:57.588004 23118544486528 run.py:483] Algo bellman_ford step 3670 current loss 0.011014, current_train_items 117472.
I0304 19:29:57.604696 23118544486528 run.py:483] Algo bellman_ford step 3671 current loss 0.061480, current_train_items 117504.
I0304 19:29:57.628057 23118544486528 run.py:483] Algo bellman_ford step 3672 current loss 0.060658, current_train_items 117536.
I0304 19:29:57.659212 23118544486528 run.py:483] Algo bellman_ford step 3673 current loss 0.086899, current_train_items 117568.
I0304 19:29:57.694338 23118544486528 run.py:483] Algo bellman_ford step 3674 current loss 0.130727, current_train_items 117600.
I0304 19:29:57.714308 23118544486528 run.py:483] Algo bellman_ford step 3675 current loss 0.005921, current_train_items 117632.
I0304 19:29:57.730380 23118544486528 run.py:483] Algo bellman_ford step 3676 current loss 0.015704, current_train_items 117664.
I0304 19:29:57.753941 23118544486528 run.py:483] Algo bellman_ford step 3677 current loss 0.041246, current_train_items 117696.
I0304 19:29:57.785535 23118544486528 run.py:483] Algo bellman_ford step 3678 current loss 0.109798, current_train_items 117728.
I0304 19:29:57.819615 23118544486528 run.py:483] Algo bellman_ford step 3679 current loss 0.171674, current_train_items 117760.
I0304 19:29:57.839049 23118544486528 run.py:483] Algo bellman_ford step 3680 current loss 0.004159, current_train_items 117792.
I0304 19:29:57.855635 23118544486528 run.py:483] Algo bellman_ford step 3681 current loss 0.016124, current_train_items 117824.
I0304 19:29:57.880190 23118544486528 run.py:483] Algo bellman_ford step 3682 current loss 0.078499, current_train_items 117856.
I0304 19:29:57.911417 23118544486528 run.py:483] Algo bellman_ford step 3683 current loss 0.099801, current_train_items 117888.
I0304 19:29:57.945416 23118544486528 run.py:483] Algo bellman_ford step 3684 current loss 0.080318, current_train_items 117920.
I0304 19:29:57.965456 23118544486528 run.py:483] Algo bellman_ford step 3685 current loss 0.004278, current_train_items 117952.
I0304 19:29:57.981853 23118544486528 run.py:483] Algo bellman_ford step 3686 current loss 0.035105, current_train_items 117984.
I0304 19:29:58.005296 23118544486528 run.py:483] Algo bellman_ford step 3687 current loss 0.060527, current_train_items 118016.
I0304 19:29:58.038077 23118544486528 run.py:483] Algo bellman_ford step 3688 current loss 0.130069, current_train_items 118048.
I0304 19:29:58.073690 23118544486528 run.py:483] Algo bellman_ford step 3689 current loss 0.077108, current_train_items 118080.
I0304 19:29:58.093678 23118544486528 run.py:483] Algo bellman_ford step 3690 current loss 0.008191, current_train_items 118112.
I0304 19:29:58.110016 23118544486528 run.py:483] Algo bellman_ford step 3691 current loss 0.016535, current_train_items 118144.
I0304 19:29:58.132357 23118544486528 run.py:483] Algo bellman_ford step 3692 current loss 0.052457, current_train_items 118176.
I0304 19:29:58.165405 23118544486528 run.py:483] Algo bellman_ford step 3693 current loss 0.081894, current_train_items 118208.
I0304 19:29:58.198486 23118544486528 run.py:483] Algo bellman_ford step 3694 current loss 0.066476, current_train_items 118240.
I0304 19:29:58.218148 23118544486528 run.py:483] Algo bellman_ford step 3695 current loss 0.009073, current_train_items 118272.
I0304 19:29:58.235164 23118544486528 run.py:483] Algo bellman_ford step 3696 current loss 0.037043, current_train_items 118304.
I0304 19:29:58.259490 23118544486528 run.py:483] Algo bellman_ford step 3697 current loss 0.054712, current_train_items 118336.
I0304 19:29:58.291792 23118544486528 run.py:483] Algo bellman_ford step 3698 current loss 0.092322, current_train_items 118368.
I0304 19:29:58.324459 23118544486528 run.py:483] Algo bellman_ford step 3699 current loss 0.109281, current_train_items 118400.
I0304 19:29:58.344809 23118544486528 run.py:483] Algo bellman_ford step 3700 current loss 0.005956, current_train_items 118432.
I0304 19:29:58.352640 23118544486528 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0304 19:29:58.352754 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0304 19:29:58.369519 23118544486528 run.py:483] Algo bellman_ford step 3701 current loss 0.031988, current_train_items 118464.
I0304 19:29:58.394860 23118544486528 run.py:483] Algo bellman_ford step 3702 current loss 0.122221, current_train_items 118496.
I0304 19:29:58.428552 23118544486528 run.py:483] Algo bellman_ford step 3703 current loss 0.142425, current_train_items 118528.
I0304 19:29:58.462507 23118544486528 run.py:483] Algo bellman_ford step 3704 current loss 0.110786, current_train_items 118560.
I0304 19:29:58.482339 23118544486528 run.py:483] Algo bellman_ford step 3705 current loss 0.003548, current_train_items 118592.
I0304 19:29:58.498668 23118544486528 run.py:483] Algo bellman_ford step 3706 current loss 0.027437, current_train_items 118624.
I0304 19:29:58.523007 23118544486528 run.py:483] Algo bellman_ford step 3707 current loss 0.121328, current_train_items 118656.
I0304 19:29:58.553589 23118544486528 run.py:483] Algo bellman_ford step 3708 current loss 0.058310, current_train_items 118688.
I0304 19:29:58.587828 23118544486528 run.py:483] Algo bellman_ford step 3709 current loss 0.073320, current_train_items 118720.
I0304 19:29:58.607721 23118544486528 run.py:483] Algo bellman_ford step 3710 current loss 0.016042, current_train_items 118752.
I0304 19:29:58.624354 23118544486528 run.py:483] Algo bellman_ford step 3711 current loss 0.026555, current_train_items 118784.
I0304 19:29:58.648536 23118544486528 run.py:483] Algo bellman_ford step 3712 current loss 0.055716, current_train_items 118816.
I0304 19:29:58.681378 23118544486528 run.py:483] Algo bellman_ford step 3713 current loss 0.062145, current_train_items 118848.
I0304 19:29:58.716449 23118544486528 run.py:483] Algo bellman_ford step 3714 current loss 0.099214, current_train_items 118880.
I0304 19:29:58.736482 23118544486528 run.py:483] Algo bellman_ford step 3715 current loss 0.006482, current_train_items 118912.
I0304 19:29:58.753226 23118544486528 run.py:483] Algo bellman_ford step 3716 current loss 0.016350, current_train_items 118944.
I0304 19:29:58.777210 23118544486528 run.py:483] Algo bellman_ford step 3717 current loss 0.062581, current_train_items 118976.
I0304 19:29:58.808835 23118544486528 run.py:483] Algo bellman_ford step 3718 current loss 0.060413, current_train_items 119008.
I0304 19:29:58.841871 23118544486528 run.py:483] Algo bellman_ford step 3719 current loss 0.056653, current_train_items 119040.
I0304 19:29:58.861769 23118544486528 run.py:483] Algo bellman_ford step 3720 current loss 0.018739, current_train_items 119072.
I0304 19:29:58.878321 23118544486528 run.py:483] Algo bellman_ford step 3721 current loss 0.035158, current_train_items 119104.
I0304 19:29:58.902130 23118544486528 run.py:483] Algo bellman_ford step 3722 current loss 0.024581, current_train_items 119136.
I0304 19:29:58.933386 23118544486528 run.py:483] Algo bellman_ford step 3723 current loss 0.072386, current_train_items 119168.
I0304 19:29:58.967028 23118544486528 run.py:483] Algo bellman_ford step 3724 current loss 0.072311, current_train_items 119200.
I0304 19:29:58.986597 23118544486528 run.py:483] Algo bellman_ford step 3725 current loss 0.003605, current_train_items 119232.
I0304 19:29:59.003348 23118544486528 run.py:483] Algo bellman_ford step 3726 current loss 0.015629, current_train_items 119264.
I0304 19:29:59.028073 23118544486528 run.py:483] Algo bellman_ford step 3727 current loss 0.074215, current_train_items 119296.
I0304 19:29:59.060039 23118544486528 run.py:483] Algo bellman_ford step 3728 current loss 0.064866, current_train_items 119328.
I0304 19:29:59.094165 23118544486528 run.py:483] Algo bellman_ford step 3729 current loss 0.054920, current_train_items 119360.
I0304 19:29:59.113805 23118544486528 run.py:483] Algo bellman_ford step 3730 current loss 0.018253, current_train_items 119392.
I0304 19:29:59.130017 23118544486528 run.py:483] Algo bellman_ford step 3731 current loss 0.040538, current_train_items 119424.
I0304 19:29:59.154625 23118544486528 run.py:483] Algo bellman_ford step 3732 current loss 0.058602, current_train_items 119456.
I0304 19:29:59.184322 23118544486528 run.py:483] Algo bellman_ford step 3733 current loss 0.088384, current_train_items 119488.
I0304 19:29:59.219334 23118544486528 run.py:483] Algo bellman_ford step 3734 current loss 0.097846, current_train_items 119520.
I0304 19:29:59.239073 23118544486528 run.py:483] Algo bellman_ford step 3735 current loss 0.017994, current_train_items 119552.
I0304 19:29:59.255279 23118544486528 run.py:483] Algo bellman_ford step 3736 current loss 0.032350, current_train_items 119584.
I0304 19:29:59.278747 23118544486528 run.py:483] Algo bellman_ford step 3737 current loss 0.025712, current_train_items 119616.
I0304 19:29:59.310691 23118544486528 run.py:483] Algo bellman_ford step 3738 current loss 0.080925, current_train_items 119648.
I0304 19:29:59.344479 23118544486528 run.py:483] Algo bellman_ford step 3739 current loss 0.086298, current_train_items 119680.
I0304 19:29:59.364373 23118544486528 run.py:483] Algo bellman_ford step 3740 current loss 0.008114, current_train_items 119712.
I0304 19:29:59.380576 23118544486528 run.py:483] Algo bellman_ford step 3741 current loss 0.018863, current_train_items 119744.
I0304 19:29:59.404588 23118544486528 run.py:483] Algo bellman_ford step 3742 current loss 0.095636, current_train_items 119776.
I0304 19:29:59.436343 23118544486528 run.py:483] Algo bellman_ford step 3743 current loss 0.147317, current_train_items 119808.
I0304 19:29:59.468845 23118544486528 run.py:483] Algo bellman_ford step 3744 current loss 0.091312, current_train_items 119840.
I0304 19:29:59.488664 23118544486528 run.py:483] Algo bellman_ford step 3745 current loss 0.046667, current_train_items 119872.
I0304 19:29:59.504773 23118544486528 run.py:483] Algo bellman_ford step 3746 current loss 0.031748, current_train_items 119904.
I0304 19:29:59.529558 23118544486528 run.py:483] Algo bellman_ford step 3747 current loss 0.068517, current_train_items 119936.
I0304 19:29:59.561794 23118544486528 run.py:483] Algo bellman_ford step 3748 current loss 0.090404, current_train_items 119968.
I0304 19:29:59.597214 23118544486528 run.py:483] Algo bellman_ford step 3749 current loss 0.181799, current_train_items 120000.
I0304 19:29:59.617453 23118544486528 run.py:483] Algo bellman_ford step 3750 current loss 0.008094, current_train_items 120032.
I0304 19:29:59.625349 23118544486528 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0304 19:29:59.625456 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:29:59.642706 23118544486528 run.py:483] Algo bellman_ford step 3751 current loss 0.028898, current_train_items 120064.
I0304 19:29:59.667377 23118544486528 run.py:483] Algo bellman_ford step 3752 current loss 0.058844, current_train_items 120096.
I0304 19:29:59.699218 23118544486528 run.py:483] Algo bellman_ford step 3753 current loss 0.073000, current_train_items 120128.
I0304 19:29:59.735489 23118544486528 run.py:483] Algo bellman_ford step 3754 current loss 0.098162, current_train_items 120160.
I0304 19:29:59.755224 23118544486528 run.py:483] Algo bellman_ford step 3755 current loss 0.015559, current_train_items 120192.
I0304 19:29:59.771320 23118544486528 run.py:483] Algo bellman_ford step 3756 current loss 0.037868, current_train_items 120224.
I0304 19:29:59.795711 23118544486528 run.py:483] Algo bellman_ford step 3757 current loss 0.072682, current_train_items 120256.
I0304 19:29:59.827539 23118544486528 run.py:483] Algo bellman_ford step 3758 current loss 0.067115, current_train_items 120288.
I0304 19:29:59.859998 23118544486528 run.py:483] Algo bellman_ford step 3759 current loss 0.066880, current_train_items 120320.
I0304 19:29:59.880058 23118544486528 run.py:483] Algo bellman_ford step 3760 current loss 0.007687, current_train_items 120352.
I0304 19:29:59.896728 23118544486528 run.py:483] Algo bellman_ford step 3761 current loss 0.017805, current_train_items 120384.
I0304 19:29:59.920281 23118544486528 run.py:483] Algo bellman_ford step 3762 current loss 0.040003, current_train_items 120416.
I0304 19:29:59.951358 23118544486528 run.py:483] Algo bellman_ford step 3763 current loss 0.056993, current_train_items 120448.
I0304 19:29:59.985455 23118544486528 run.py:483] Algo bellman_ford step 3764 current loss 0.060832, current_train_items 120480.
I0304 19:30:00.004967 23118544486528 run.py:483] Algo bellman_ford step 3765 current loss 0.008535, current_train_items 120512.
I0304 19:30:00.021832 23118544486528 run.py:483] Algo bellman_ford step 3766 current loss 0.026118, current_train_items 120544.
I0304 19:30:00.046248 23118544486528 run.py:483] Algo bellman_ford step 3767 current loss 0.124103, current_train_items 120576.
I0304 19:30:00.078693 23118544486528 run.py:483] Algo bellman_ford step 3768 current loss 0.109503, current_train_items 120608.
I0304 19:30:00.111599 23118544486528 run.py:483] Algo bellman_ford step 3769 current loss 0.088957, current_train_items 120640.
I0304 19:30:00.131607 23118544486528 run.py:483] Algo bellman_ford step 3770 current loss 0.010689, current_train_items 120672.
I0304 19:30:00.148250 23118544486528 run.py:483] Algo bellman_ford step 3771 current loss 0.041141, current_train_items 120704.
I0304 19:30:00.171764 23118544486528 run.py:483] Algo bellman_ford step 3772 current loss 0.084891, current_train_items 120736.
I0304 19:30:00.202448 23118544486528 run.py:483] Algo bellman_ford step 3773 current loss 0.083126, current_train_items 120768.
I0304 19:30:00.233610 23118544486528 run.py:483] Algo bellman_ford step 3774 current loss 0.066102, current_train_items 120800.
I0304 19:30:00.253670 23118544486528 run.py:483] Algo bellman_ford step 3775 current loss 0.011125, current_train_items 120832.
I0304 19:30:00.270870 23118544486528 run.py:483] Algo bellman_ford step 3776 current loss 0.040674, current_train_items 120864.
I0304 19:30:00.294107 23118544486528 run.py:483] Algo bellman_ford step 3777 current loss 0.049265, current_train_items 120896.
I0304 19:30:00.326601 23118544486528 run.py:483] Algo bellman_ford step 3778 current loss 0.069148, current_train_items 120928.
I0304 19:30:00.360835 23118544486528 run.py:483] Algo bellman_ford step 3779 current loss 0.112212, current_train_items 120960.
I0304 19:30:00.380511 23118544486528 run.py:483] Algo bellman_ford step 3780 current loss 0.006717, current_train_items 120992.
I0304 19:30:00.397034 23118544486528 run.py:483] Algo bellman_ford step 3781 current loss 0.039971, current_train_items 121024.
I0304 19:30:00.421022 23118544486528 run.py:483] Algo bellman_ford step 3782 current loss 0.051539, current_train_items 121056.
I0304 19:30:00.450908 23118544486528 run.py:483] Algo bellman_ford step 3783 current loss 0.048056, current_train_items 121088.
I0304 19:30:00.487664 23118544486528 run.py:483] Algo bellman_ford step 3784 current loss 0.104523, current_train_items 121120.
I0304 19:30:00.507357 23118544486528 run.py:483] Algo bellman_ford step 3785 current loss 0.008514, current_train_items 121152.
I0304 19:30:00.523780 23118544486528 run.py:483] Algo bellman_ford step 3786 current loss 0.045113, current_train_items 121184.
I0304 19:30:00.547888 23118544486528 run.py:483] Algo bellman_ford step 3787 current loss 0.055599, current_train_items 121216.
I0304 19:30:00.579694 23118544486528 run.py:483] Algo bellman_ford step 3788 current loss 0.058559, current_train_items 121248.
I0304 19:30:00.613569 23118544486528 run.py:483] Algo bellman_ford step 3789 current loss 0.085229, current_train_items 121280.
I0304 19:30:00.633796 23118544486528 run.py:483] Algo bellman_ford step 3790 current loss 0.008369, current_train_items 121312.
I0304 19:30:00.650316 23118544486528 run.py:483] Algo bellman_ford step 3791 current loss 0.015878, current_train_items 121344.
I0304 19:30:00.673675 23118544486528 run.py:483] Algo bellman_ford step 3792 current loss 0.062752, current_train_items 121376.
I0304 19:30:00.706244 23118544486528 run.py:483] Algo bellman_ford step 3793 current loss 0.086655, current_train_items 121408.
I0304 19:30:00.739555 23118544486528 run.py:483] Algo bellman_ford step 3794 current loss 0.065168, current_train_items 121440.
I0304 19:30:00.759278 23118544486528 run.py:483] Algo bellman_ford step 3795 current loss 0.014631, current_train_items 121472.
I0304 19:30:00.775959 23118544486528 run.py:483] Algo bellman_ford step 3796 current loss 0.038332, current_train_items 121504.
I0304 19:30:00.800371 23118544486528 run.py:483] Algo bellman_ford step 3797 current loss 0.069440, current_train_items 121536.
I0304 19:30:00.832857 23118544486528 run.py:483] Algo bellman_ford step 3798 current loss 0.137154, current_train_items 121568.
I0304 19:30:00.866052 23118544486528 run.py:483] Algo bellman_ford step 3799 current loss 0.111052, current_train_items 121600.
I0304 19:30:00.886042 23118544486528 run.py:483] Algo bellman_ford step 3800 current loss 0.005952, current_train_items 121632.
I0304 19:30:00.893537 23118544486528 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0304 19:30:00.893645 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:00.910670 23118544486528 run.py:483] Algo bellman_ford step 3801 current loss 0.027937, current_train_items 121664.
I0304 19:30:00.935961 23118544486528 run.py:483] Algo bellman_ford step 3802 current loss 0.048887, current_train_items 121696.
I0304 19:30:00.967642 23118544486528 run.py:483] Algo bellman_ford step 3803 current loss 0.093722, current_train_items 121728.
I0304 19:30:00.999741 23118544486528 run.py:483] Algo bellman_ford step 3804 current loss 0.057728, current_train_items 121760.
I0304 19:30:01.019750 23118544486528 run.py:483] Algo bellman_ford step 3805 current loss 0.005098, current_train_items 121792.
I0304 19:30:01.036035 23118544486528 run.py:483] Algo bellman_ford step 3806 current loss 0.045586, current_train_items 121824.
I0304 19:30:01.060364 23118544486528 run.py:483] Algo bellman_ford step 3807 current loss 0.128393, current_train_items 121856.
I0304 19:30:01.092627 23118544486528 run.py:483] Algo bellman_ford step 3808 current loss 0.114625, current_train_items 121888.
I0304 19:30:01.128111 23118544486528 run.py:483] Algo bellman_ford step 3809 current loss 0.059768, current_train_items 121920.
I0304 19:30:01.148152 23118544486528 run.py:483] Algo bellman_ford step 3810 current loss 0.022561, current_train_items 121952.
I0304 19:30:01.164975 23118544486528 run.py:483] Algo bellman_ford step 3811 current loss 0.008846, current_train_items 121984.
I0304 19:30:01.189605 23118544486528 run.py:483] Algo bellman_ford step 3812 current loss 0.037752, current_train_items 122016.
I0304 19:30:01.223330 23118544486528 run.py:483] Algo bellman_ford step 3813 current loss 0.072041, current_train_items 122048.
I0304 19:30:01.259185 23118544486528 run.py:483] Algo bellman_ford step 3814 current loss 0.113970, current_train_items 122080.
I0304 19:30:01.279129 23118544486528 run.py:483] Algo bellman_ford step 3815 current loss 0.027475, current_train_items 122112.
I0304 19:30:01.296005 23118544486528 run.py:483] Algo bellman_ford step 3816 current loss 0.040366, current_train_items 122144.
I0304 19:30:01.320800 23118544486528 run.py:483] Algo bellman_ford step 3817 current loss 0.082077, current_train_items 122176.
I0304 19:30:01.352437 23118544486528 run.py:483] Algo bellman_ford step 3818 current loss 0.100148, current_train_items 122208.
I0304 19:30:01.386594 23118544486528 run.py:483] Algo bellman_ford step 3819 current loss 0.146634, current_train_items 122240.
I0304 19:30:01.406507 23118544486528 run.py:483] Algo bellman_ford step 3820 current loss 0.005141, current_train_items 122272.
I0304 19:30:01.422732 23118544486528 run.py:483] Algo bellman_ford step 3821 current loss 0.018688, current_train_items 122304.
I0304 19:30:01.446992 23118544486528 run.py:483] Algo bellman_ford step 3822 current loss 0.059508, current_train_items 122336.
I0304 19:30:01.479337 23118544486528 run.py:483] Algo bellman_ford step 3823 current loss 0.086468, current_train_items 122368.
I0304 19:30:01.513900 23118544486528 run.py:483] Algo bellman_ford step 3824 current loss 0.089905, current_train_items 122400.
I0304 19:30:01.534123 23118544486528 run.py:483] Algo bellman_ford step 3825 current loss 0.028587, current_train_items 122432.
I0304 19:30:01.550164 23118544486528 run.py:483] Algo bellman_ford step 3826 current loss 0.014652, current_train_items 122464.
I0304 19:30:01.574799 23118544486528 run.py:483] Algo bellman_ford step 3827 current loss 0.110862, current_train_items 122496.
I0304 19:30:01.607129 23118544486528 run.py:483] Algo bellman_ford step 3828 current loss 0.097985, current_train_items 122528.
I0304 19:30:01.639897 23118544486528 run.py:483] Algo bellman_ford step 3829 current loss 0.092303, current_train_items 122560.
I0304 19:30:01.659612 23118544486528 run.py:483] Algo bellman_ford step 3830 current loss 0.004506, current_train_items 122592.
I0304 19:30:01.676143 23118544486528 run.py:483] Algo bellman_ford step 3831 current loss 0.050787, current_train_items 122624.
I0304 19:30:01.699382 23118544486528 run.py:483] Algo bellman_ford step 3832 current loss 0.050134, current_train_items 122656.
I0304 19:30:01.732043 23118544486528 run.py:483] Algo bellman_ford step 3833 current loss 0.092609, current_train_items 122688.
I0304 19:30:01.765272 23118544486528 run.py:483] Algo bellman_ford step 3834 current loss 0.072694, current_train_items 122720.
I0304 19:30:01.785025 23118544486528 run.py:483] Algo bellman_ford step 3835 current loss 0.015492, current_train_items 122752.
I0304 19:30:01.801172 23118544486528 run.py:483] Algo bellman_ford step 3836 current loss 0.019610, current_train_items 122784.
I0304 19:30:01.826657 23118544486528 run.py:483] Algo bellman_ford step 3837 current loss 0.070446, current_train_items 122816.
I0304 19:30:01.858779 23118544486528 run.py:483] Algo bellman_ford step 3838 current loss 0.104526, current_train_items 122848.
I0304 19:30:01.892919 23118544486528 run.py:483] Algo bellman_ford step 3839 current loss 0.101667, current_train_items 122880.
I0304 19:30:01.912923 23118544486528 run.py:483] Algo bellman_ford step 3840 current loss 0.020080, current_train_items 122912.
I0304 19:30:01.928989 23118544486528 run.py:483] Algo bellman_ford step 3841 current loss 0.044806, current_train_items 122944.
I0304 19:30:01.952265 23118544486528 run.py:483] Algo bellman_ford step 3842 current loss 0.087558, current_train_items 122976.
I0304 19:30:01.983897 23118544486528 run.py:483] Algo bellman_ford step 3843 current loss 0.063917, current_train_items 123008.
I0304 19:30:02.019122 23118544486528 run.py:483] Algo bellman_ford step 3844 current loss 0.155012, current_train_items 123040.
I0304 19:30:02.039222 23118544486528 run.py:483] Algo bellman_ford step 3845 current loss 0.005665, current_train_items 123072.
I0304 19:30:02.055700 23118544486528 run.py:483] Algo bellman_ford step 3846 current loss 0.044120, current_train_items 123104.
I0304 19:30:02.081386 23118544486528 run.py:483] Algo bellman_ford step 3847 current loss 0.079218, current_train_items 123136.
I0304 19:30:02.112333 23118544486528 run.py:483] Algo bellman_ford step 3848 current loss 0.054910, current_train_items 123168.
I0304 19:30:02.146250 23118544486528 run.py:483] Algo bellman_ford step 3849 current loss 0.111588, current_train_items 123200.
I0304 19:30:02.166469 23118544486528 run.py:483] Algo bellman_ford step 3850 current loss 0.017987, current_train_items 123232.
I0304 19:30:02.174489 23118544486528 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0304 19:30:02.174593 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:02.191062 23118544486528 run.py:483] Algo bellman_ford step 3851 current loss 0.015996, current_train_items 123264.
I0304 19:30:02.214589 23118544486528 run.py:483] Algo bellman_ford step 3852 current loss 0.046611, current_train_items 123296.
I0304 19:30:02.247209 23118544486528 run.py:483] Algo bellman_ford step 3853 current loss 0.134607, current_train_items 123328.
I0304 19:30:02.283474 23118544486528 run.py:483] Algo bellman_ford step 3854 current loss 0.176495, current_train_items 123360.
I0304 19:30:02.303627 23118544486528 run.py:483] Algo bellman_ford step 3855 current loss 0.007643, current_train_items 123392.
I0304 19:30:02.319495 23118544486528 run.py:483] Algo bellman_ford step 3856 current loss 0.010326, current_train_items 123424.
I0304 19:30:02.342668 23118544486528 run.py:483] Algo bellman_ford step 3857 current loss 0.076649, current_train_items 123456.
I0304 19:30:02.374863 23118544486528 run.py:483] Algo bellman_ford step 3858 current loss 0.137276, current_train_items 123488.
I0304 19:30:02.411432 23118544486528 run.py:483] Algo bellman_ford step 3859 current loss 0.161981, current_train_items 123520.
I0304 19:30:02.431350 23118544486528 run.py:483] Algo bellman_ford step 3860 current loss 0.006251, current_train_items 123552.
I0304 19:30:02.448012 23118544486528 run.py:483] Algo bellman_ford step 3861 current loss 0.051303, current_train_items 123584.
I0304 19:30:02.471766 23118544486528 run.py:483] Algo bellman_ford step 3862 current loss 0.041825, current_train_items 123616.
I0304 19:30:02.503765 23118544486528 run.py:483] Algo bellman_ford step 3863 current loss 0.103535, current_train_items 123648.
I0304 19:30:02.540222 23118544486528 run.py:483] Algo bellman_ford step 3864 current loss 0.118893, current_train_items 123680.
I0304 19:30:02.559598 23118544486528 run.py:483] Algo bellman_ford step 3865 current loss 0.006603, current_train_items 123712.
I0304 19:30:02.575790 23118544486528 run.py:483] Algo bellman_ford step 3866 current loss 0.029059, current_train_items 123744.
I0304 19:30:02.599879 23118544486528 run.py:483] Algo bellman_ford step 3867 current loss 0.039841, current_train_items 123776.
I0304 19:30:02.630922 23118544486528 run.py:483] Algo bellman_ford step 3868 current loss 0.052702, current_train_items 123808.
I0304 19:30:02.664649 23118544486528 run.py:483] Algo bellman_ford step 3869 current loss 0.075004, current_train_items 123840.
I0304 19:30:02.684917 23118544486528 run.py:483] Algo bellman_ford step 3870 current loss 0.012855, current_train_items 123872.
I0304 19:30:02.700903 23118544486528 run.py:483] Algo bellman_ford step 3871 current loss 0.023809, current_train_items 123904.
I0304 19:30:02.724987 23118544486528 run.py:483] Algo bellman_ford step 3872 current loss 0.043448, current_train_items 123936.
I0304 19:30:02.755354 23118544486528 run.py:483] Algo bellman_ford step 3873 current loss 0.046999, current_train_items 123968.
I0304 19:30:02.788740 23118544486528 run.py:483] Algo bellman_ford step 3874 current loss 0.085792, current_train_items 124000.
I0304 19:30:02.809102 23118544486528 run.py:483] Algo bellman_ford step 3875 current loss 0.010236, current_train_items 124032.
I0304 19:30:02.825943 23118544486528 run.py:483] Algo bellman_ford step 3876 current loss 0.037538, current_train_items 124064.
I0304 19:30:02.850370 23118544486528 run.py:483] Algo bellman_ford step 3877 current loss 0.039447, current_train_items 124096.
I0304 19:30:02.882461 23118544486528 run.py:483] Algo bellman_ford step 3878 current loss 0.076686, current_train_items 124128.
I0304 19:30:02.916787 23118544486528 run.py:483] Algo bellman_ford step 3879 current loss 0.088950, current_train_items 124160.
I0304 19:30:02.936876 23118544486528 run.py:483] Algo bellman_ford step 3880 current loss 0.058373, current_train_items 124192.
I0304 19:30:02.953173 23118544486528 run.py:483] Algo bellman_ford step 3881 current loss 0.038001, current_train_items 124224.
I0304 19:30:02.977913 23118544486528 run.py:483] Algo bellman_ford step 3882 current loss 0.045049, current_train_items 124256.
I0304 19:30:03.009444 23118544486528 run.py:483] Algo bellman_ford step 3883 current loss 0.071466, current_train_items 124288.
I0304 19:30:03.042459 23118544486528 run.py:483] Algo bellman_ford step 3884 current loss 0.129067, current_train_items 124320.
I0304 19:30:03.062635 23118544486528 run.py:483] Algo bellman_ford step 3885 current loss 0.009155, current_train_items 124352.
I0304 19:30:03.079472 23118544486528 run.py:483] Algo bellman_ford step 3886 current loss 0.017950, current_train_items 124384.
I0304 19:30:03.103423 23118544486528 run.py:483] Algo bellman_ford step 3887 current loss 0.040393, current_train_items 124416.
I0304 19:30:03.134748 23118544486528 run.py:483] Algo bellman_ford step 3888 current loss 0.044914, current_train_items 124448.
I0304 19:30:03.169896 23118544486528 run.py:483] Algo bellman_ford step 3889 current loss 0.086702, current_train_items 124480.
I0304 19:30:03.189995 23118544486528 run.py:483] Algo bellman_ford step 3890 current loss 0.004628, current_train_items 124512.
I0304 19:30:03.206576 23118544486528 run.py:483] Algo bellman_ford step 3891 current loss 0.024270, current_train_items 124544.
I0304 19:30:03.231296 23118544486528 run.py:483] Algo bellman_ford step 3892 current loss 0.041859, current_train_items 124576.
I0304 19:30:03.260909 23118544486528 run.py:483] Algo bellman_ford step 3893 current loss 0.027746, current_train_items 124608.
I0304 19:30:03.295857 23118544486528 run.py:483] Algo bellman_ford step 3894 current loss 0.051459, current_train_items 124640.
I0304 19:30:03.315315 23118544486528 run.py:483] Algo bellman_ford step 3895 current loss 0.006492, current_train_items 124672.
I0304 19:30:03.331984 23118544486528 run.py:483] Algo bellman_ford step 3896 current loss 0.027371, current_train_items 124704.
I0304 19:30:03.357510 23118544486528 run.py:483] Algo bellman_ford step 3897 current loss 0.046205, current_train_items 124736.
I0304 19:30:03.389533 23118544486528 run.py:483] Algo bellman_ford step 3898 current loss 0.048770, current_train_items 124768.
I0304 19:30:03.423603 23118544486528 run.py:483] Algo bellman_ford step 3899 current loss 0.113152, current_train_items 124800.
I0304 19:30:03.443842 23118544486528 run.py:483] Algo bellman_ford step 3900 current loss 0.003774, current_train_items 124832.
I0304 19:30:03.451333 23118544486528 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0304 19:30:03.451438 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.990, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:03.481346 23118544486528 run.py:483] Algo bellman_ford step 3901 current loss 0.036747, current_train_items 124864.
I0304 19:30:03.505795 23118544486528 run.py:483] Algo bellman_ford step 3902 current loss 0.072330, current_train_items 124896.
I0304 19:30:03.538196 23118544486528 run.py:483] Algo bellman_ford step 3903 current loss 0.126565, current_train_items 124928.
I0304 19:30:03.572667 23118544486528 run.py:483] Algo bellman_ford step 3904 current loss 0.061065, current_train_items 124960.
I0304 19:30:03.593379 23118544486528 run.py:483] Algo bellman_ford step 3905 current loss 0.005480, current_train_items 124992.
I0304 19:30:03.609502 23118544486528 run.py:483] Algo bellman_ford step 3906 current loss 0.053188, current_train_items 125024.
I0304 19:30:03.632717 23118544486528 run.py:483] Algo bellman_ford step 3907 current loss 0.047932, current_train_items 125056.
I0304 19:30:03.664860 23118544486528 run.py:483] Algo bellman_ford step 3908 current loss 0.071771, current_train_items 125088.
I0304 19:30:03.699278 23118544486528 run.py:483] Algo bellman_ford step 3909 current loss 0.097758, current_train_items 125120.
I0304 19:30:03.718927 23118544486528 run.py:483] Algo bellman_ford step 3910 current loss 0.003637, current_train_items 125152.
I0304 19:30:03.735084 23118544486528 run.py:483] Algo bellman_ford step 3911 current loss 0.017072, current_train_items 125184.
I0304 19:30:03.759346 23118544486528 run.py:483] Algo bellman_ford step 3912 current loss 0.109977, current_train_items 125216.
I0304 19:30:03.790532 23118544486528 run.py:483] Algo bellman_ford step 3913 current loss 0.087602, current_train_items 125248.
I0304 19:30:03.821764 23118544486528 run.py:483] Algo bellman_ford step 3914 current loss 0.112645, current_train_items 125280.
I0304 19:30:03.841414 23118544486528 run.py:483] Algo bellman_ford step 3915 current loss 0.063503, current_train_items 125312.
I0304 19:30:03.857902 23118544486528 run.py:483] Algo bellman_ford step 3916 current loss 0.013169, current_train_items 125344.
I0304 19:30:03.884066 23118544486528 run.py:483] Algo bellman_ford step 3917 current loss 0.103284, current_train_items 125376.
I0304 19:30:03.915054 23118544486528 run.py:483] Algo bellman_ford step 3918 current loss 0.102776, current_train_items 125408.
I0304 19:30:03.949161 23118544486528 run.py:483] Algo bellman_ford step 3919 current loss 0.073278, current_train_items 125440.
I0304 19:30:03.968981 23118544486528 run.py:483] Algo bellman_ford step 3920 current loss 0.004551, current_train_items 125472.
I0304 19:30:03.985471 23118544486528 run.py:483] Algo bellman_ford step 3921 current loss 0.011923, current_train_items 125504.
I0304 19:30:04.010585 23118544486528 run.py:483] Algo bellman_ford step 3922 current loss 0.052819, current_train_items 125536.
I0304 19:30:04.042781 23118544486528 run.py:483] Algo bellman_ford step 3923 current loss 0.142086, current_train_items 125568.
I0304 19:30:04.079462 23118544486528 run.py:483] Algo bellman_ford step 3924 current loss 0.083539, current_train_items 125600.
I0304 19:30:04.099231 23118544486528 run.py:483] Algo bellman_ford step 3925 current loss 0.008105, current_train_items 125632.
I0304 19:30:04.115729 23118544486528 run.py:483] Algo bellman_ford step 3926 current loss 0.037244, current_train_items 125664.
I0304 19:30:04.139966 23118544486528 run.py:483] Algo bellman_ford step 3927 current loss 0.055654, current_train_items 125696.
I0304 19:30:04.171543 23118544486528 run.py:483] Algo bellman_ford step 3928 current loss 0.085845, current_train_items 125728.
I0304 19:30:04.205646 23118544486528 run.py:483] Algo bellman_ford step 3929 current loss 0.060189, current_train_items 125760.
I0304 19:30:04.225276 23118544486528 run.py:483] Algo bellman_ford step 3930 current loss 0.006207, current_train_items 125792.
I0304 19:30:04.241447 23118544486528 run.py:483] Algo bellman_ford step 3931 current loss 0.033975, current_train_items 125824.
I0304 19:30:04.265790 23118544486528 run.py:483] Algo bellman_ford step 3932 current loss 0.077110, current_train_items 125856.
I0304 19:30:04.298859 23118544486528 run.py:483] Algo bellman_ford step 3933 current loss 0.074759, current_train_items 125888.
I0304 19:30:04.333062 23118544486528 run.py:483] Algo bellman_ford step 3934 current loss 0.083602, current_train_items 125920.
I0304 19:30:04.352933 23118544486528 run.py:483] Algo bellman_ford step 3935 current loss 0.020976, current_train_items 125952.
I0304 19:30:04.369695 23118544486528 run.py:483] Algo bellman_ford step 3936 current loss 0.016299, current_train_items 125984.
I0304 19:30:04.393484 23118544486528 run.py:483] Algo bellman_ford step 3937 current loss 0.051043, current_train_items 126016.
I0304 19:30:04.425606 23118544486528 run.py:483] Algo bellman_ford step 3938 current loss 0.042071, current_train_items 126048.
I0304 19:30:04.461034 23118544486528 run.py:483] Algo bellman_ford step 3939 current loss 0.090470, current_train_items 126080.
I0304 19:30:04.481094 23118544486528 run.py:483] Algo bellman_ford step 3940 current loss 0.004730, current_train_items 126112.
I0304 19:30:04.497395 23118544486528 run.py:483] Algo bellman_ford step 3941 current loss 0.037225, current_train_items 126144.
I0304 19:30:04.521803 23118544486528 run.py:483] Algo bellman_ford step 3942 current loss 0.057660, current_train_items 126176.
I0304 19:30:04.552213 23118544486528 run.py:483] Algo bellman_ford step 3943 current loss 0.038618, current_train_items 126208.
I0304 19:30:04.587169 23118544486528 run.py:483] Algo bellman_ford step 3944 current loss 0.129536, current_train_items 126240.
I0304 19:30:04.606964 23118544486528 run.py:483] Algo bellman_ford step 3945 current loss 0.007100, current_train_items 126272.
I0304 19:30:04.623649 23118544486528 run.py:483] Algo bellman_ford step 3946 current loss 0.023686, current_train_items 126304.
I0304 19:30:04.647453 23118544486528 run.py:483] Algo bellman_ford step 3947 current loss 0.083680, current_train_items 126336.
I0304 19:30:04.677081 23118544486528 run.py:483] Algo bellman_ford step 3948 current loss 0.090884, current_train_items 126368.
I0304 19:30:04.712036 23118544486528 run.py:483] Algo bellman_ford step 3949 current loss 0.075029, current_train_items 126400.
I0304 19:30:04.732066 23118544486528 run.py:483] Algo bellman_ford step 3950 current loss 0.006010, current_train_items 126432.
I0304 19:30:04.739955 23118544486528 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0304 19:30:04.740059 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:04.757395 23118544486528 run.py:483] Algo bellman_ford step 3951 current loss 0.022680, current_train_items 126464.
I0304 19:30:04.782642 23118544486528 run.py:483] Algo bellman_ford step 3952 current loss 0.061234, current_train_items 126496.
I0304 19:30:04.813601 23118544486528 run.py:483] Algo bellman_ford step 3953 current loss 0.053748, current_train_items 126528.
I0304 19:30:04.845895 23118544486528 run.py:483] Algo bellman_ford step 3954 current loss 0.048477, current_train_items 126560.
I0304 19:30:04.865456 23118544486528 run.py:483] Algo bellman_ford step 3955 current loss 0.004608, current_train_items 126592.
I0304 19:30:04.881524 23118544486528 run.py:483] Algo bellman_ford step 3956 current loss 0.042210, current_train_items 126624.
I0304 19:30:04.905938 23118544486528 run.py:483] Algo bellman_ford step 3957 current loss 0.065900, current_train_items 126656.
I0304 19:30:04.938412 23118544486528 run.py:483] Algo bellman_ford step 3958 current loss 0.073616, current_train_items 126688.
I0304 19:30:04.974487 23118544486528 run.py:483] Algo bellman_ford step 3959 current loss 0.089732, current_train_items 126720.
I0304 19:30:04.994499 23118544486528 run.py:483] Algo bellman_ford step 3960 current loss 0.032771, current_train_items 126752.
I0304 19:30:05.010699 23118544486528 run.py:483] Algo bellman_ford step 3961 current loss 0.033597, current_train_items 126784.
I0304 19:30:05.033439 23118544486528 run.py:483] Algo bellman_ford step 3962 current loss 0.033076, current_train_items 126816.
I0304 19:30:05.065764 23118544486528 run.py:483] Algo bellman_ford step 3963 current loss 0.067258, current_train_items 126848.
I0304 19:30:05.100308 23118544486528 run.py:483] Algo bellman_ford step 3964 current loss 0.081564, current_train_items 126880.
I0304 19:30:05.119796 23118544486528 run.py:483] Algo bellman_ford step 3965 current loss 0.007272, current_train_items 126912.
I0304 19:30:05.136208 23118544486528 run.py:483] Algo bellman_ford step 3966 current loss 0.049396, current_train_items 126944.
I0304 19:30:05.161627 23118544486528 run.py:483] Algo bellman_ford step 3967 current loss 0.058141, current_train_items 126976.
I0304 19:30:05.193242 23118544486528 run.py:483] Algo bellman_ford step 3968 current loss 0.063237, current_train_items 127008.
I0304 19:30:05.226423 23118544486528 run.py:483] Algo bellman_ford step 3969 current loss 0.067782, current_train_items 127040.
I0304 19:30:05.246327 23118544486528 run.py:483] Algo bellman_ford step 3970 current loss 0.005526, current_train_items 127072.
I0304 19:30:05.262658 23118544486528 run.py:483] Algo bellman_ford step 3971 current loss 0.033968, current_train_items 127104.
I0304 19:30:05.286895 23118544486528 run.py:483] Algo bellman_ford step 3972 current loss 0.040823, current_train_items 127136.
I0304 19:30:05.319177 23118544486528 run.py:483] Algo bellman_ford step 3973 current loss 0.090618, current_train_items 127168.
I0304 19:30:05.351723 23118544486528 run.py:483] Algo bellman_ford step 3974 current loss 0.058131, current_train_items 127200.
I0304 19:30:05.371662 23118544486528 run.py:483] Algo bellman_ford step 3975 current loss 0.006345, current_train_items 127232.
I0304 19:30:05.388465 23118544486528 run.py:483] Algo bellman_ford step 3976 current loss 0.040071, current_train_items 127264.
I0304 19:30:05.412360 23118544486528 run.py:483] Algo bellman_ford step 3977 current loss 0.048175, current_train_items 127296.
I0304 19:30:05.444563 23118544486528 run.py:483] Algo bellman_ford step 3978 current loss 0.087401, current_train_items 127328.
I0304 19:30:05.478263 23118544486528 run.py:483] Algo bellman_ford step 3979 current loss 0.086102, current_train_items 127360.
I0304 19:30:05.497700 23118544486528 run.py:483] Algo bellman_ford step 3980 current loss 0.003803, current_train_items 127392.
I0304 19:30:05.514019 23118544486528 run.py:483] Algo bellman_ford step 3981 current loss 0.029644, current_train_items 127424.
I0304 19:30:05.537630 23118544486528 run.py:483] Algo bellman_ford step 3982 current loss 0.046565, current_train_items 127456.
I0304 19:30:05.569577 23118544486528 run.py:483] Algo bellman_ford step 3983 current loss 0.078006, current_train_items 127488.
I0304 19:30:05.602431 23118544486528 run.py:483] Algo bellman_ford step 3984 current loss 0.090991, current_train_items 127520.
I0304 19:30:05.622495 23118544486528 run.py:483] Algo bellman_ford step 3985 current loss 0.007344, current_train_items 127552.
I0304 19:30:05.639113 23118544486528 run.py:483] Algo bellman_ford step 3986 current loss 0.020430, current_train_items 127584.
I0304 19:30:05.662564 23118544486528 run.py:483] Algo bellman_ford step 3987 current loss 0.046950, current_train_items 127616.
I0304 19:30:05.694802 23118544486528 run.py:483] Algo bellman_ford step 3988 current loss 0.089949, current_train_items 127648.
I0304 19:30:05.729473 23118544486528 run.py:483] Algo bellman_ford step 3989 current loss 0.081312, current_train_items 127680.
I0304 19:30:05.749671 23118544486528 run.py:483] Algo bellman_ford step 3990 current loss 0.012218, current_train_items 127712.
I0304 19:30:05.766453 23118544486528 run.py:483] Algo bellman_ford step 3991 current loss 0.036316, current_train_items 127744.
I0304 19:30:05.789809 23118544486528 run.py:483] Algo bellman_ford step 3992 current loss 0.041012, current_train_items 127776.
I0304 19:30:05.820971 23118544486528 run.py:483] Algo bellman_ford step 3993 current loss 0.060822, current_train_items 127808.
I0304 19:30:05.855034 23118544486528 run.py:483] Algo bellman_ford step 3994 current loss 0.162880, current_train_items 127840.
I0304 19:30:05.874570 23118544486528 run.py:483] Algo bellman_ford step 3995 current loss 0.005791, current_train_items 127872.
I0304 19:30:05.891178 23118544486528 run.py:483] Algo bellman_ford step 3996 current loss 0.019072, current_train_items 127904.
I0304 19:30:05.915393 23118544486528 run.py:483] Algo bellman_ford step 3997 current loss 0.044546, current_train_items 127936.
I0304 19:30:05.946454 23118544486528 run.py:483] Algo bellman_ford step 3998 current loss 0.050285, current_train_items 127968.
I0304 19:30:05.981444 23118544486528 run.py:483] Algo bellman_ford step 3999 current loss 0.084682, current_train_items 128000.
I0304 19:30:06.001703 23118544486528 run.py:483] Algo bellman_ford step 4000 current loss 0.008354, current_train_items 128032.
I0304 19:30:06.009351 23118544486528 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0304 19:30:06.009456 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:06.026421 23118544486528 run.py:483] Algo bellman_ford step 4001 current loss 0.023633, current_train_items 128064.
I0304 19:30:06.051349 23118544486528 run.py:483] Algo bellman_ford step 4002 current loss 0.061566, current_train_items 128096.
I0304 19:30:06.082671 23118544486528 run.py:483] Algo bellman_ford step 4003 current loss 0.084682, current_train_items 128128.
I0304 19:30:06.116110 23118544486528 run.py:483] Algo bellman_ford step 4004 current loss 0.104826, current_train_items 128160.
I0304 19:30:06.135966 23118544486528 run.py:483] Algo bellman_ford step 4005 current loss 0.004523, current_train_items 128192.
I0304 19:30:06.152398 23118544486528 run.py:483] Algo bellman_ford step 4006 current loss 0.054693, current_train_items 128224.
I0304 19:30:06.176531 23118544486528 run.py:483] Algo bellman_ford step 4007 current loss 0.255528, current_train_items 128256.
I0304 19:30:06.209104 23118544486528 run.py:483] Algo bellman_ford step 4008 current loss 0.119528, current_train_items 128288.
I0304 19:30:06.241715 23118544486528 run.py:483] Algo bellman_ford step 4009 current loss 0.140159, current_train_items 128320.
I0304 19:30:06.261767 23118544486528 run.py:483] Algo bellman_ford step 4010 current loss 0.023866, current_train_items 128352.
I0304 19:30:06.278103 23118544486528 run.py:483] Algo bellman_ford step 4011 current loss 0.024838, current_train_items 128384.
I0304 19:30:06.301179 23118544486528 run.py:483] Algo bellman_ford step 4012 current loss 0.100613, current_train_items 128416.
I0304 19:30:06.332914 23118544486528 run.py:483] Algo bellman_ford step 4013 current loss 0.154634, current_train_items 128448.
I0304 19:30:06.367689 23118544486528 run.py:483] Algo bellman_ford step 4014 current loss 0.204515, current_train_items 128480.
I0304 19:30:06.387908 23118544486528 run.py:483] Algo bellman_ford step 4015 current loss 0.006143, current_train_items 128512.
I0304 19:30:06.404595 23118544486528 run.py:483] Algo bellman_ford step 4016 current loss 0.057122, current_train_items 128544.
I0304 19:30:06.428681 23118544486528 run.py:483] Algo bellman_ford step 4017 current loss 0.040565, current_train_items 128576.
I0304 19:30:06.460044 23118544486528 run.py:483] Algo bellman_ford step 4018 current loss 0.113034, current_train_items 128608.
I0304 19:30:06.493262 23118544486528 run.py:483] Algo bellman_ford step 4019 current loss 0.083838, current_train_items 128640.
I0304 19:30:06.513080 23118544486528 run.py:483] Algo bellman_ford step 4020 current loss 0.002812, current_train_items 128672.
I0304 19:30:06.529358 23118544486528 run.py:483] Algo bellman_ford step 4021 current loss 0.031260, current_train_items 128704.
I0304 19:30:06.554283 23118544486528 run.py:483] Algo bellman_ford step 4022 current loss 0.104605, current_train_items 128736.
I0304 19:30:06.585368 23118544486528 run.py:483] Algo bellman_ford step 4023 current loss 0.083325, current_train_items 128768.
I0304 19:30:06.622211 23118544486528 run.py:483] Algo bellman_ford step 4024 current loss 0.208603, current_train_items 128800.
I0304 19:30:06.642177 23118544486528 run.py:483] Algo bellman_ford step 4025 current loss 0.005934, current_train_items 128832.
I0304 19:30:06.658859 23118544486528 run.py:483] Algo bellman_ford step 4026 current loss 0.017845, current_train_items 128864.
I0304 19:30:06.682536 23118544486528 run.py:483] Algo bellman_ford step 4027 current loss 0.030260, current_train_items 128896.
I0304 19:30:06.713213 23118544486528 run.py:483] Algo bellman_ford step 4028 current loss 0.066961, current_train_items 128928.
I0304 19:30:06.746879 23118544486528 run.py:483] Algo bellman_ford step 4029 current loss 0.099069, current_train_items 128960.
I0304 19:30:06.766360 23118544486528 run.py:483] Algo bellman_ford step 4030 current loss 0.006370, current_train_items 128992.
I0304 19:30:06.782463 23118544486528 run.py:483] Algo bellman_ford step 4031 current loss 0.022850, current_train_items 129024.
I0304 19:30:06.807147 23118544486528 run.py:483] Algo bellman_ford step 4032 current loss 0.058257, current_train_items 129056.
I0304 19:30:06.840170 23118544486528 run.py:483] Algo bellman_ford step 4033 current loss 0.077645, current_train_items 129088.
I0304 19:30:06.873698 23118544486528 run.py:483] Algo bellman_ford step 4034 current loss 0.088339, current_train_items 129120.
I0304 19:30:06.893468 23118544486528 run.py:483] Algo bellman_ford step 4035 current loss 0.003984, current_train_items 129152.
I0304 19:30:06.909716 23118544486528 run.py:483] Algo bellman_ford step 4036 current loss 0.024765, current_train_items 129184.
I0304 19:30:06.933744 23118544486528 run.py:483] Algo bellman_ford step 4037 current loss 0.051046, current_train_items 129216.
I0304 19:30:06.965929 23118544486528 run.py:483] Algo bellman_ford step 4038 current loss 0.094791, current_train_items 129248.
I0304 19:30:07.000440 23118544486528 run.py:483] Algo bellman_ford step 4039 current loss 0.083471, current_train_items 129280.
I0304 19:30:07.020535 23118544486528 run.py:483] Algo bellman_ford step 4040 current loss 0.009463, current_train_items 129312.
I0304 19:30:07.037068 23118544486528 run.py:483] Algo bellman_ford step 4041 current loss 0.020568, current_train_items 129344.
I0304 19:30:07.060947 23118544486528 run.py:483] Algo bellman_ford step 4042 current loss 0.082416, current_train_items 129376.
I0304 19:30:07.093670 23118544486528 run.py:483] Algo bellman_ford step 4043 current loss 0.105590, current_train_items 129408.
I0304 19:30:07.126604 23118544486528 run.py:483] Algo bellman_ford step 4044 current loss 0.078462, current_train_items 129440.
I0304 19:30:07.146256 23118544486528 run.py:483] Algo bellman_ford step 4045 current loss 0.009681, current_train_items 129472.
I0304 19:30:07.163301 23118544486528 run.py:483] Algo bellman_ford step 4046 current loss 0.069933, current_train_items 129504.
I0304 19:30:07.187168 23118544486528 run.py:483] Algo bellman_ford step 4047 current loss 0.058387, current_train_items 129536.
I0304 19:30:07.217823 23118544486528 run.py:483] Algo bellman_ford step 4048 current loss 0.034956, current_train_items 129568.
I0304 19:30:07.253356 23118544486528 run.py:483] Algo bellman_ford step 4049 current loss 0.095338, current_train_items 129600.
I0304 19:30:07.273181 23118544486528 run.py:483] Algo bellman_ford step 4050 current loss 0.010144, current_train_items 129632.
I0304 19:30:07.281126 23118544486528 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0304 19:30:07.281232 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:07.298417 23118544486528 run.py:483] Algo bellman_ford step 4051 current loss 0.023483, current_train_items 129664.
I0304 19:30:07.322420 23118544486528 run.py:483] Algo bellman_ford step 4052 current loss 0.047876, current_train_items 129696.
I0304 19:30:07.354371 23118544486528 run.py:483] Algo bellman_ford step 4053 current loss 0.075363, current_train_items 129728.
I0304 19:30:07.390500 23118544486528 run.py:483] Algo bellman_ford step 4054 current loss 0.125447, current_train_items 129760.
I0304 19:30:07.410313 23118544486528 run.py:483] Algo bellman_ford step 4055 current loss 0.007588, current_train_items 129792.
I0304 19:30:07.426696 23118544486528 run.py:483] Algo bellman_ford step 4056 current loss 0.014021, current_train_items 129824.
I0304 19:30:07.450637 23118544486528 run.py:483] Algo bellman_ford step 4057 current loss 0.065798, current_train_items 129856.
I0304 19:30:07.481129 23118544486528 run.py:483] Algo bellman_ford step 4058 current loss 0.070049, current_train_items 129888.
I0304 19:30:07.514016 23118544486528 run.py:483] Algo bellman_ford step 4059 current loss 0.065781, current_train_items 129920.
I0304 19:30:07.534351 23118544486528 run.py:483] Algo bellman_ford step 4060 current loss 0.017495, current_train_items 129952.
I0304 19:30:07.551181 23118544486528 run.py:483] Algo bellman_ford step 4061 current loss 0.028593, current_train_items 129984.
I0304 19:30:07.574289 23118544486528 run.py:483] Algo bellman_ford step 4062 current loss 0.048399, current_train_items 130016.
I0304 19:30:07.604524 23118544486528 run.py:483] Algo bellman_ford step 4063 current loss 0.047762, current_train_items 130048.
I0304 19:30:07.640500 23118544486528 run.py:483] Algo bellman_ford step 4064 current loss 0.082475, current_train_items 130080.
I0304 19:30:07.660115 23118544486528 run.py:483] Algo bellman_ford step 4065 current loss 0.005636, current_train_items 130112.
I0304 19:30:07.676133 23118544486528 run.py:483] Algo bellman_ford step 4066 current loss 0.128757, current_train_items 130144.
I0304 19:30:07.701556 23118544486528 run.py:483] Algo bellman_ford step 4067 current loss 0.219869, current_train_items 130176.
I0304 19:30:07.732916 23118544486528 run.py:483] Algo bellman_ford step 4068 current loss 0.092079, current_train_items 130208.
I0304 19:30:07.768474 23118544486528 run.py:483] Algo bellman_ford step 4069 current loss 0.116797, current_train_items 130240.
I0304 19:30:07.788509 23118544486528 run.py:483] Algo bellman_ford step 4070 current loss 0.007481, current_train_items 130272.
I0304 19:30:07.804973 23118544486528 run.py:483] Algo bellman_ford step 4071 current loss 0.036382, current_train_items 130304.
I0304 19:30:07.829078 23118544486528 run.py:483] Algo bellman_ford step 4072 current loss 0.114910, current_train_items 130336.
I0304 19:30:07.859110 23118544486528 run.py:483] Algo bellman_ford step 4073 current loss 0.093165, current_train_items 130368.
I0304 19:30:07.892281 23118544486528 run.py:483] Algo bellman_ford step 4074 current loss 0.097524, current_train_items 130400.
I0304 19:30:07.912609 23118544486528 run.py:483] Algo bellman_ford step 4075 current loss 0.007360, current_train_items 130432.
I0304 19:30:07.929906 23118544486528 run.py:483] Algo bellman_ford step 4076 current loss 0.025151, current_train_items 130464.
I0304 19:30:07.954635 23118544486528 run.py:483] Algo bellman_ford step 4077 current loss 0.060720, current_train_items 130496.
I0304 19:30:07.985528 23118544486528 run.py:483] Algo bellman_ford step 4078 current loss 0.044477, current_train_items 130528.
I0304 19:30:08.018945 23118544486528 run.py:483] Algo bellman_ford step 4079 current loss 0.076858, current_train_items 130560.
I0304 19:30:08.038466 23118544486528 run.py:483] Algo bellman_ford step 4080 current loss 0.003677, current_train_items 130592.
I0304 19:30:08.055245 23118544486528 run.py:483] Algo bellman_ford step 4081 current loss 0.012330, current_train_items 130624.
I0304 19:30:08.080710 23118544486528 run.py:483] Algo bellman_ford step 4082 current loss 0.070259, current_train_items 130656.
I0304 19:30:08.113970 23118544486528 run.py:483] Algo bellman_ford step 4083 current loss 0.071533, current_train_items 130688.
I0304 19:30:08.145784 23118544486528 run.py:483] Algo bellman_ford step 4084 current loss 0.049539, current_train_items 130720.
I0304 19:30:08.166317 23118544486528 run.py:483] Algo bellman_ford step 4085 current loss 0.009607, current_train_items 130752.
I0304 19:30:08.182883 23118544486528 run.py:483] Algo bellman_ford step 4086 current loss 0.025647, current_train_items 130784.
I0304 19:30:08.206058 23118544486528 run.py:483] Algo bellman_ford step 4087 current loss 0.053834, current_train_items 130816.
I0304 19:30:08.236966 23118544486528 run.py:483] Algo bellman_ford step 4088 current loss 0.039201, current_train_items 130848.
I0304 19:30:08.269218 23118544486528 run.py:483] Algo bellman_ford step 4089 current loss 0.046590, current_train_items 130880.
I0304 19:30:08.288985 23118544486528 run.py:483] Algo bellman_ford step 4090 current loss 0.006047, current_train_items 130912.
I0304 19:30:08.305376 23118544486528 run.py:483] Algo bellman_ford step 4091 current loss 0.018568, current_train_items 130944.
I0304 19:30:08.330033 23118544486528 run.py:483] Algo bellman_ford step 4092 current loss 0.048822, current_train_items 130976.
I0304 19:30:08.362004 23118544486528 run.py:483] Algo bellman_ford step 4093 current loss 0.069708, current_train_items 131008.
I0304 19:30:08.396717 23118544486528 run.py:483] Algo bellman_ford step 4094 current loss 0.068150, current_train_items 131040.
I0304 19:30:08.416184 23118544486528 run.py:483] Algo bellman_ford step 4095 current loss 0.005145, current_train_items 131072.
I0304 19:30:08.432852 23118544486528 run.py:483] Algo bellman_ford step 4096 current loss 0.017018, current_train_items 131104.
I0304 19:30:08.457810 23118544486528 run.py:483] Algo bellman_ford step 4097 current loss 0.076287, current_train_items 131136.
I0304 19:30:08.489642 23118544486528 run.py:483] Algo bellman_ford step 4098 current loss 0.073625, current_train_items 131168.
I0304 19:30:08.524933 23118544486528 run.py:483] Algo bellman_ford step 4099 current loss 0.067975, current_train_items 131200.
I0304 19:30:08.545535 23118544486528 run.py:483] Algo bellman_ford step 4100 current loss 0.004771, current_train_items 131232.
I0304 19:30:08.553146 23118544486528 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0304 19:30:08.553252 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:30:08.569681 23118544486528 run.py:483] Algo bellman_ford step 4101 current loss 0.016348, current_train_items 131264.
I0304 19:30:08.592478 23118544486528 run.py:483] Algo bellman_ford step 4102 current loss 0.093465, current_train_items 131296.
I0304 19:30:08.625250 23118544486528 run.py:483] Algo bellman_ford step 4103 current loss 0.156319, current_train_items 131328.
I0304 19:30:08.661398 23118544486528 run.py:483] Algo bellman_ford step 4104 current loss 0.145586, current_train_items 131360.
I0304 19:30:08.681036 23118544486528 run.py:483] Algo bellman_ford step 4105 current loss 0.003803, current_train_items 131392.
I0304 19:30:08.697432 23118544486528 run.py:483] Algo bellman_ford step 4106 current loss 0.040936, current_train_items 131424.
I0304 19:30:08.722437 23118544486528 run.py:483] Algo bellman_ford step 4107 current loss 0.077571, current_train_items 131456.
I0304 19:30:08.753903 23118544486528 run.py:483] Algo bellman_ford step 4108 current loss 0.091025, current_train_items 131488.
I0304 19:30:08.790327 23118544486528 run.py:483] Algo bellman_ford step 4109 current loss 0.089209, current_train_items 131520.
I0304 19:30:08.810048 23118544486528 run.py:483] Algo bellman_ford step 4110 current loss 0.021058, current_train_items 131552.
I0304 19:30:08.826857 23118544486528 run.py:483] Algo bellman_ford step 4111 current loss 0.045083, current_train_items 131584.
I0304 19:30:08.850201 23118544486528 run.py:483] Algo bellman_ford step 4112 current loss 0.084185, current_train_items 131616.
I0304 19:30:08.882571 23118544486528 run.py:483] Algo bellman_ford step 4113 current loss 0.111246, current_train_items 131648.
I0304 19:30:08.917332 23118544486528 run.py:483] Algo bellman_ford step 4114 current loss 0.099512, current_train_items 131680.
I0304 19:30:08.936878 23118544486528 run.py:483] Algo bellman_ford step 4115 current loss 0.004415, current_train_items 131712.
I0304 19:30:08.953682 23118544486528 run.py:483] Algo bellman_ford step 4116 current loss 0.025263, current_train_items 131744.
I0304 19:30:08.979615 23118544486528 run.py:483] Algo bellman_ford step 4117 current loss 0.125174, current_train_items 131776.
I0304 19:30:09.010213 23118544486528 run.py:483] Algo bellman_ford step 4118 current loss 0.070551, current_train_items 131808.
I0304 19:30:09.044505 23118544486528 run.py:483] Algo bellman_ford step 4119 current loss 0.112718, current_train_items 131840.
I0304 19:30:09.064256 23118544486528 run.py:483] Algo bellman_ford step 4120 current loss 0.009621, current_train_items 131872.
I0304 19:30:09.080636 23118544486528 run.py:483] Algo bellman_ford step 4121 current loss 0.012856, current_train_items 131904.
I0304 19:30:09.105634 23118544486528 run.py:483] Algo bellman_ford step 4122 current loss 0.071214, current_train_items 131936.
I0304 19:30:09.138147 23118544486528 run.py:483] Algo bellman_ford step 4123 current loss 0.190896, current_train_items 131968.
I0304 19:30:09.172337 23118544486528 run.py:483] Algo bellman_ford step 4124 current loss 0.161793, current_train_items 132000.
I0304 19:30:09.191992 23118544486528 run.py:483] Algo bellman_ford step 4125 current loss 0.004031, current_train_items 132032.
I0304 19:30:09.208480 23118544486528 run.py:483] Algo bellman_ford step 4126 current loss 0.045827, current_train_items 132064.
I0304 19:30:09.233458 23118544486528 run.py:483] Algo bellman_ford step 4127 current loss 0.053026, current_train_items 132096.
I0304 19:30:09.264461 23118544486528 run.py:483] Algo bellman_ford step 4128 current loss 0.067009, current_train_items 132128.
I0304 19:30:09.297006 23118544486528 run.py:483] Algo bellman_ford step 4129 current loss 0.071352, current_train_items 132160.
I0304 19:30:09.316838 23118544486528 run.py:483] Algo bellman_ford step 4130 current loss 0.005434, current_train_items 132192.
I0304 19:30:09.333402 23118544486528 run.py:483] Algo bellman_ford step 4131 current loss 0.024037, current_train_items 132224.
I0304 19:30:09.357788 23118544486528 run.py:483] Algo bellman_ford step 4132 current loss 0.079937, current_train_items 132256.
I0304 19:30:09.390027 23118544486528 run.py:483] Algo bellman_ford step 4133 current loss 0.098618, current_train_items 132288.
I0304 19:30:09.425639 23118544486528 run.py:483] Algo bellman_ford step 4134 current loss 0.079377, current_train_items 132320.
I0304 19:30:09.445351 23118544486528 run.py:483] Algo bellman_ford step 4135 current loss 0.008340, current_train_items 132352.
I0304 19:30:09.461577 23118544486528 run.py:483] Algo bellman_ford step 4136 current loss 0.016943, current_train_items 132384.
I0304 19:30:09.485512 23118544486528 run.py:483] Algo bellman_ford step 4137 current loss 0.022759, current_train_items 132416.
I0304 19:30:09.516406 23118544486528 run.py:483] Algo bellman_ford step 4138 current loss 0.084086, current_train_items 132448.
I0304 19:30:09.551577 23118544486528 run.py:483] Algo bellman_ford step 4139 current loss 0.138562, current_train_items 132480.
I0304 19:30:09.571396 23118544486528 run.py:483] Algo bellman_ford step 4140 current loss 0.004889, current_train_items 132512.
I0304 19:30:09.587575 23118544486528 run.py:483] Algo bellman_ford step 4141 current loss 0.010063, current_train_items 132544.
I0304 19:30:09.613313 23118544486528 run.py:483] Algo bellman_ford step 4142 current loss 0.043554, current_train_items 132576.
I0304 19:30:09.644278 23118544486528 run.py:483] Algo bellman_ford step 4143 current loss 0.070212, current_train_items 132608.
I0304 19:30:09.678244 23118544486528 run.py:483] Algo bellman_ford step 4144 current loss 0.098684, current_train_items 132640.
I0304 19:30:09.698075 23118544486528 run.py:483] Algo bellman_ford step 4145 current loss 0.002043, current_train_items 132672.
I0304 19:30:09.714336 23118544486528 run.py:483] Algo bellman_ford step 4146 current loss 0.014228, current_train_items 132704.
I0304 19:30:09.738224 23118544486528 run.py:483] Algo bellman_ford step 4147 current loss 0.074819, current_train_items 132736.
I0304 19:30:09.768797 23118544486528 run.py:483] Algo bellman_ford step 4148 current loss 0.029781, current_train_items 132768.
I0304 19:30:09.802855 23118544486528 run.py:483] Algo bellman_ford step 4149 current loss 0.084734, current_train_items 132800.
I0304 19:30:09.822989 23118544486528 run.py:483] Algo bellman_ford step 4150 current loss 0.006930, current_train_items 132832.
I0304 19:30:09.831096 23118544486528 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0304 19:30:09.831207 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:09.848361 23118544486528 run.py:483] Algo bellman_ford step 4151 current loss 0.046442, current_train_items 132864.
I0304 19:30:09.873479 23118544486528 run.py:483] Algo bellman_ford step 4152 current loss 0.100282, current_train_items 132896.
I0304 19:30:09.905691 23118544486528 run.py:483] Algo bellman_ford step 4153 current loss 0.067906, current_train_items 132928.
I0304 19:30:09.941587 23118544486528 run.py:483] Algo bellman_ford step 4154 current loss 0.086428, current_train_items 132960.
I0304 19:30:09.961543 23118544486528 run.py:483] Algo bellman_ford step 4155 current loss 0.004635, current_train_items 132992.
I0304 19:30:09.978178 23118544486528 run.py:483] Algo bellman_ford step 4156 current loss 0.035994, current_train_items 133024.
I0304 19:30:10.001831 23118544486528 run.py:483] Algo bellman_ford step 4157 current loss 0.064211, current_train_items 133056.
I0304 19:30:10.032905 23118544486528 run.py:483] Algo bellman_ford step 4158 current loss 0.067363, current_train_items 133088.
I0304 19:30:10.067701 23118544486528 run.py:483] Algo bellman_ford step 4159 current loss 0.096367, current_train_items 133120.
I0304 19:30:10.087883 23118544486528 run.py:483] Algo bellman_ford step 4160 current loss 0.007024, current_train_items 133152.
I0304 19:30:10.104411 23118544486528 run.py:483] Algo bellman_ford step 4161 current loss 0.037960, current_train_items 133184.
I0304 19:30:10.127205 23118544486528 run.py:483] Algo bellman_ford step 4162 current loss 0.074179, current_train_items 133216.
I0304 19:30:10.159016 23118544486528 run.py:483] Algo bellman_ford step 4163 current loss 0.101176, current_train_items 133248.
I0304 19:30:10.193804 23118544486528 run.py:483] Algo bellman_ford step 4164 current loss 0.105037, current_train_items 133280.
I0304 19:30:10.213657 23118544486528 run.py:483] Algo bellman_ford step 4165 current loss 0.007855, current_train_items 133312.
I0304 19:30:10.230098 23118544486528 run.py:483] Algo bellman_ford step 4166 current loss 0.022925, current_train_items 133344.
I0304 19:30:10.255502 23118544486528 run.py:483] Algo bellman_ford step 4167 current loss 0.107258, current_train_items 133376.
I0304 19:30:10.286873 23118544486528 run.py:483] Algo bellman_ford step 4168 current loss 0.062521, current_train_items 133408.
I0304 19:30:10.320290 23118544486528 run.py:483] Algo bellman_ford step 4169 current loss 0.073413, current_train_items 133440.
I0304 19:30:10.340458 23118544486528 run.py:483] Algo bellman_ford step 4170 current loss 0.005734, current_train_items 133472.
I0304 19:30:10.356424 23118544486528 run.py:483] Algo bellman_ford step 4171 current loss 0.019070, current_train_items 133504.
I0304 19:30:10.379982 23118544486528 run.py:483] Algo bellman_ford step 4172 current loss 0.031965, current_train_items 133536.
I0304 19:30:10.410983 23118544486528 run.py:483] Algo bellman_ford step 4173 current loss 0.103300, current_train_items 133568.
I0304 19:30:10.447153 23118544486528 run.py:483] Algo bellman_ford step 4174 current loss 0.110850, current_train_items 133600.
I0304 19:30:10.467078 23118544486528 run.py:483] Algo bellman_ford step 4175 current loss 0.006147, current_train_items 133632.
I0304 19:30:10.484311 23118544486528 run.py:483] Algo bellman_ford step 4176 current loss 0.024326, current_train_items 133664.
I0304 19:30:10.508251 23118544486528 run.py:483] Algo bellman_ford step 4177 current loss 0.156259, current_train_items 133696.
I0304 19:30:10.540038 23118544486528 run.py:483] Algo bellman_ford step 4178 current loss 0.076923, current_train_items 133728.
I0304 19:30:10.574895 23118544486528 run.py:483] Algo bellman_ford step 4179 current loss 0.103177, current_train_items 133760.
I0304 19:30:10.594744 23118544486528 run.py:483] Algo bellman_ford step 4180 current loss 0.009007, current_train_items 133792.
I0304 19:30:10.611147 23118544486528 run.py:483] Algo bellman_ford step 4181 current loss 0.038864, current_train_items 133824.
I0304 19:30:10.636160 23118544486528 run.py:483] Algo bellman_ford step 4182 current loss 0.055200, current_train_items 133856.
I0304 19:30:10.666290 23118544486528 run.py:483] Algo bellman_ford step 4183 current loss 0.072597, current_train_items 133888.
I0304 19:30:10.701185 23118544486528 run.py:483] Algo bellman_ford step 4184 current loss 0.097550, current_train_items 133920.
I0304 19:30:10.721336 23118544486528 run.py:483] Algo bellman_ford step 4185 current loss 0.007267, current_train_items 133952.
I0304 19:30:10.737636 23118544486528 run.py:483] Algo bellman_ford step 4186 current loss 0.051461, current_train_items 133984.
I0304 19:30:10.761492 23118544486528 run.py:483] Algo bellman_ford step 4187 current loss 0.035752, current_train_items 134016.
I0304 19:30:10.792500 23118544486528 run.py:483] Algo bellman_ford step 4188 current loss 0.061907, current_train_items 134048.
I0304 19:30:10.825882 23118544486528 run.py:483] Algo bellman_ford step 4189 current loss 0.068389, current_train_items 134080.
I0304 19:30:10.846669 23118544486528 run.py:483] Algo bellman_ford step 4190 current loss 0.013762, current_train_items 134112.
I0304 19:30:10.863673 23118544486528 run.py:483] Algo bellman_ford step 4191 current loss 0.019903, current_train_items 134144.
I0304 19:30:10.885583 23118544486528 run.py:483] Algo bellman_ford step 4192 current loss 0.023441, current_train_items 134176.
I0304 19:30:10.918220 23118544486528 run.py:483] Algo bellman_ford step 4193 current loss 0.054831, current_train_items 134208.
I0304 19:30:10.954813 23118544486528 run.py:483] Algo bellman_ford step 4194 current loss 0.083229, current_train_items 134240.
I0304 19:30:10.974703 23118544486528 run.py:483] Algo bellman_ford step 4195 current loss 0.007093, current_train_items 134272.
I0304 19:30:10.991012 23118544486528 run.py:483] Algo bellman_ford step 4196 current loss 0.017472, current_train_items 134304.
I0304 19:30:11.015574 23118544486528 run.py:483] Algo bellman_ford step 4197 current loss 0.025439, current_train_items 134336.
I0304 19:30:11.046327 23118544486528 run.py:483] Algo bellman_ford step 4198 current loss 0.046490, current_train_items 134368.
I0304 19:30:11.080695 23118544486528 run.py:483] Algo bellman_ford step 4199 current loss 0.070618, current_train_items 134400.
I0304 19:30:11.100301 23118544486528 run.py:483] Algo bellman_ford step 4200 current loss 0.002558, current_train_items 134432.
I0304 19:30:11.107893 23118544486528 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0304 19:30:11.108000 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:11.125201 23118544486528 run.py:483] Algo bellman_ford step 4201 current loss 0.020780, current_train_items 134464.
I0304 19:30:11.149238 23118544486528 run.py:483] Algo bellman_ford step 4202 current loss 0.021827, current_train_items 134496.
I0304 19:30:11.180505 23118544486528 run.py:483] Algo bellman_ford step 4203 current loss 0.051311, current_train_items 134528.
I0304 19:30:11.214455 23118544486528 run.py:483] Algo bellman_ford step 4204 current loss 0.064894, current_train_items 134560.
I0304 19:30:11.234396 23118544486528 run.py:483] Algo bellman_ford step 4205 current loss 0.005428, current_train_items 134592.
I0304 19:30:11.250654 23118544486528 run.py:483] Algo bellman_ford step 4206 current loss 0.028343, current_train_items 134624.
I0304 19:30:11.275052 23118544486528 run.py:483] Algo bellman_ford step 4207 current loss 0.072687, current_train_items 134656.
I0304 19:30:11.307356 23118544486528 run.py:483] Algo bellman_ford step 4208 current loss 0.055868, current_train_items 134688.
I0304 19:30:11.343027 23118544486528 run.py:483] Algo bellman_ford step 4209 current loss 0.111710, current_train_items 134720.
I0304 19:30:11.362695 23118544486528 run.py:483] Algo bellman_ford step 4210 current loss 0.037904, current_train_items 134752.
I0304 19:30:11.379267 23118544486528 run.py:483] Algo bellman_ford step 4211 current loss 0.022737, current_train_items 134784.
I0304 19:30:11.403930 23118544486528 run.py:483] Algo bellman_ford step 4212 current loss 0.057129, current_train_items 134816.
I0304 19:30:11.435624 23118544486528 run.py:483] Algo bellman_ford step 4213 current loss 0.067575, current_train_items 134848.
I0304 19:30:11.469738 23118544486528 run.py:483] Algo bellman_ford step 4214 current loss 0.146315, current_train_items 134880.
I0304 19:30:11.489903 23118544486528 run.py:483] Algo bellman_ford step 4215 current loss 0.014761, current_train_items 134912.
I0304 19:30:11.506328 23118544486528 run.py:483] Algo bellman_ford step 4216 current loss 0.030000, current_train_items 134944.
I0304 19:30:11.530351 23118544486528 run.py:483] Algo bellman_ford step 4217 current loss 0.031173, current_train_items 134976.
I0304 19:30:11.561637 23118544486528 run.py:483] Algo bellman_ford step 4218 current loss 0.106202, current_train_items 135008.
I0304 19:30:11.596038 23118544486528 run.py:483] Algo bellman_ford step 4219 current loss 0.041713, current_train_items 135040.
I0304 19:30:11.615989 23118544486528 run.py:483] Algo bellman_ford step 4220 current loss 0.007151, current_train_items 135072.
I0304 19:30:11.632586 23118544486528 run.py:483] Algo bellman_ford step 4221 current loss 0.027145, current_train_items 135104.
I0304 19:30:11.657407 23118544486528 run.py:483] Algo bellman_ford step 4222 current loss 0.048155, current_train_items 135136.
I0304 19:30:11.689809 23118544486528 run.py:483] Algo bellman_ford step 4223 current loss 0.042834, current_train_items 135168.
I0304 19:30:11.725661 23118544486528 run.py:483] Algo bellman_ford step 4224 current loss 0.100041, current_train_items 135200.
I0304 19:30:11.745495 23118544486528 run.py:483] Algo bellman_ford step 4225 current loss 0.008295, current_train_items 135232.
I0304 19:30:11.761802 23118544486528 run.py:483] Algo bellman_ford step 4226 current loss 0.014958, current_train_items 135264.
I0304 19:30:11.785513 23118544486528 run.py:483] Algo bellman_ford step 4227 current loss 0.086773, current_train_items 135296.
I0304 19:30:11.815932 23118544486528 run.py:483] Algo bellman_ford step 4228 current loss 0.046861, current_train_items 135328.
I0304 19:30:11.848034 23118544486528 run.py:483] Algo bellman_ford step 4229 current loss 0.056173, current_train_items 135360.
I0304 19:30:11.867661 23118544486528 run.py:483] Algo bellman_ford step 4230 current loss 0.005823, current_train_items 135392.
I0304 19:30:11.884216 23118544486528 run.py:483] Algo bellman_ford step 4231 current loss 0.033715, current_train_items 135424.
I0304 19:30:11.909316 23118544486528 run.py:483] Algo bellman_ford step 4232 current loss 0.078101, current_train_items 135456.
I0304 19:30:11.940917 23118544486528 run.py:483] Algo bellman_ford step 4233 current loss 0.097120, current_train_items 135488.
I0304 19:30:11.974736 23118544486528 run.py:483] Algo bellman_ford step 4234 current loss 0.097364, current_train_items 135520.
I0304 19:30:11.994862 23118544486528 run.py:483] Algo bellman_ford step 4235 current loss 0.006040, current_train_items 135552.
I0304 19:30:12.012004 23118544486528 run.py:483] Algo bellman_ford step 4236 current loss 0.035787, current_train_items 135584.
I0304 19:30:12.036296 23118544486528 run.py:483] Algo bellman_ford step 4237 current loss 0.062482, current_train_items 135616.
I0304 19:30:12.068037 23118544486528 run.py:483] Algo bellman_ford step 4238 current loss 0.076242, current_train_items 135648.
I0304 19:30:12.103946 23118544486528 run.py:483] Algo bellman_ford step 4239 current loss 0.157750, current_train_items 135680.
I0304 19:30:12.124007 23118544486528 run.py:483] Algo bellman_ford step 4240 current loss 0.014066, current_train_items 135712.
I0304 19:30:12.140335 23118544486528 run.py:483] Algo bellman_ford step 4241 current loss 0.015219, current_train_items 135744.
I0304 19:30:12.164007 23118544486528 run.py:483] Algo bellman_ford step 4242 current loss 0.067193, current_train_items 135776.
I0304 19:30:12.196521 23118544486528 run.py:483] Algo bellman_ford step 4243 current loss 0.090768, current_train_items 135808.
I0304 19:30:12.229971 23118544486528 run.py:483] Algo bellman_ford step 4244 current loss 0.058279, current_train_items 135840.
I0304 19:30:12.249647 23118544486528 run.py:483] Algo bellman_ford step 4245 current loss 0.008052, current_train_items 135872.
I0304 19:30:12.265866 23118544486528 run.py:483] Algo bellman_ford step 4246 current loss 0.021297, current_train_items 135904.
I0304 19:30:12.291528 23118544486528 run.py:483] Algo bellman_ford step 4247 current loss 0.075639, current_train_items 135936.
I0304 19:30:12.323806 23118544486528 run.py:483] Algo bellman_ford step 4248 current loss 0.122532, current_train_items 135968.
I0304 19:30:12.359745 23118544486528 run.py:483] Algo bellman_ford step 4249 current loss 0.146245, current_train_items 136000.
I0304 19:30:12.379894 23118544486528 run.py:483] Algo bellman_ford step 4250 current loss 0.010946, current_train_items 136032.
I0304 19:30:12.388150 23118544486528 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0304 19:30:12.388253 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:30:12.405262 23118544486528 run.py:483] Algo bellman_ford step 4251 current loss 0.026877, current_train_items 136064.
I0304 19:30:12.429968 23118544486528 run.py:483] Algo bellman_ford step 4252 current loss 0.036728, current_train_items 136096.
I0304 19:30:12.462008 23118544486528 run.py:483] Algo bellman_ford step 4253 current loss 0.139418, current_train_items 136128.
I0304 19:30:12.496236 23118544486528 run.py:483] Algo bellman_ford step 4254 current loss 0.084678, current_train_items 136160.
I0304 19:30:12.515981 23118544486528 run.py:483] Algo bellman_ford step 4255 current loss 0.007505, current_train_items 136192.
I0304 19:30:12.532327 23118544486528 run.py:483] Algo bellman_ford step 4256 current loss 0.011347, current_train_items 136224.
I0304 19:30:12.555817 23118544486528 run.py:483] Algo bellman_ford step 4257 current loss 0.037459, current_train_items 136256.
I0304 19:30:12.587662 23118544486528 run.py:483] Algo bellman_ford step 4258 current loss 0.084882, current_train_items 136288.
I0304 19:30:12.622659 23118544486528 run.py:483] Algo bellman_ford step 4259 current loss 0.113449, current_train_items 136320.
I0304 19:30:12.642633 23118544486528 run.py:483] Algo bellman_ford step 4260 current loss 0.005215, current_train_items 136352.
I0304 19:30:12.659547 23118544486528 run.py:483] Algo bellman_ford step 4261 current loss 0.032155, current_train_items 136384.
I0304 19:30:12.683382 23118544486528 run.py:483] Algo bellman_ford step 4262 current loss 0.061106, current_train_items 136416.
I0304 19:30:12.714949 23118544486528 run.py:483] Algo bellman_ford step 4263 current loss 0.091423, current_train_items 136448.
I0304 19:30:12.749286 23118544486528 run.py:483] Algo bellman_ford step 4264 current loss 0.063471, current_train_items 136480.
I0304 19:30:12.769278 23118544486528 run.py:483] Algo bellman_ford step 4265 current loss 0.016985, current_train_items 136512.
I0304 19:30:12.785727 23118544486528 run.py:483] Algo bellman_ford step 4266 current loss 0.037305, current_train_items 136544.
I0304 19:30:12.809636 23118544486528 run.py:483] Algo bellman_ford step 4267 current loss 0.029000, current_train_items 136576.
I0304 19:30:12.840934 23118544486528 run.py:483] Algo bellman_ford step 4268 current loss 0.065629, current_train_items 136608.
I0304 19:30:12.874428 23118544486528 run.py:483] Algo bellman_ford step 4269 current loss 0.076626, current_train_items 136640.
I0304 19:30:12.894584 23118544486528 run.py:483] Algo bellman_ford step 4270 current loss 0.005970, current_train_items 136672.
I0304 19:30:12.910818 23118544486528 run.py:483] Algo bellman_ford step 4271 current loss 0.018413, current_train_items 136704.
I0304 19:30:12.933661 23118544486528 run.py:483] Algo bellman_ford step 4272 current loss 0.016107, current_train_items 136736.
I0304 19:30:12.965179 23118544486528 run.py:483] Algo bellman_ford step 4273 current loss 0.080877, current_train_items 136768.
I0304 19:30:12.998527 23118544486528 run.py:483] Algo bellman_ford step 4274 current loss 0.078204, current_train_items 136800.
I0304 19:30:13.018347 23118544486528 run.py:483] Algo bellman_ford step 4275 current loss 0.005994, current_train_items 136832.
I0304 19:30:13.034741 23118544486528 run.py:483] Algo bellman_ford step 4276 current loss 0.015205, current_train_items 136864.
I0304 19:30:13.058572 23118544486528 run.py:483] Algo bellman_ford step 4277 current loss 0.071141, current_train_items 136896.
I0304 19:30:13.092117 23118544486528 run.py:483] Algo bellman_ford step 4278 current loss 0.077585, current_train_items 136928.
I0304 19:30:13.126993 23118544486528 run.py:483] Algo bellman_ford step 4279 current loss 0.078164, current_train_items 136960.
I0304 19:30:13.147027 23118544486528 run.py:483] Algo bellman_ford step 4280 current loss 0.014795, current_train_items 136992.
I0304 19:30:13.163289 23118544486528 run.py:483] Algo bellman_ford step 4281 current loss 0.020222, current_train_items 137024.
I0304 19:30:13.187476 23118544486528 run.py:483] Algo bellman_ford step 4282 current loss 0.022065, current_train_items 137056.
I0304 19:30:13.218072 23118544486528 run.py:483] Algo bellman_ford step 4283 current loss 0.035833, current_train_items 137088.
I0304 19:30:13.253913 23118544486528 run.py:483] Algo bellman_ford step 4284 current loss 0.102276, current_train_items 137120.
I0304 19:30:13.274109 23118544486528 run.py:483] Algo bellman_ford step 4285 current loss 0.005566, current_train_items 137152.
I0304 19:30:13.290504 23118544486528 run.py:483] Algo bellman_ford step 4286 current loss 0.005838, current_train_items 137184.
I0304 19:30:13.314749 23118544486528 run.py:483] Algo bellman_ford step 4287 current loss 0.077462, current_train_items 137216.
I0304 19:30:13.347747 23118544486528 run.py:483] Algo bellman_ford step 4288 current loss 0.085896, current_train_items 137248.
I0304 19:30:13.382497 23118544486528 run.py:483] Algo bellman_ford step 4289 current loss 0.153716, current_train_items 137280.
I0304 19:30:13.402441 23118544486528 run.py:483] Algo bellman_ford step 4290 current loss 0.007892, current_train_items 137312.
I0304 19:30:13.419535 23118544486528 run.py:483] Algo bellman_ford step 4291 current loss 0.045064, current_train_items 137344.
I0304 19:30:13.444775 23118544486528 run.py:483] Algo bellman_ford step 4292 current loss 0.046425, current_train_items 137376.
I0304 19:30:13.478087 23118544486528 run.py:483] Algo bellman_ford step 4293 current loss 0.074377, current_train_items 137408.
I0304 19:30:13.514178 23118544486528 run.py:483] Algo bellman_ford step 4294 current loss 0.065336, current_train_items 137440.
I0304 19:30:13.534099 23118544486528 run.py:483] Algo bellman_ford step 4295 current loss 0.003809, current_train_items 137472.
I0304 19:30:13.550413 23118544486528 run.py:483] Algo bellman_ford step 4296 current loss 0.012096, current_train_items 137504.
I0304 19:30:13.574306 23118544486528 run.py:483] Algo bellman_ford step 4297 current loss 0.072307, current_train_items 137536.
I0304 19:30:13.606228 23118544486528 run.py:483] Algo bellman_ford step 4298 current loss 0.070740, current_train_items 137568.
I0304 19:30:13.636959 23118544486528 run.py:483] Algo bellman_ford step 4299 current loss 0.035081, current_train_items 137600.
I0304 19:30:13.657450 23118544486528 run.py:483] Algo bellman_ford step 4300 current loss 0.005136, current_train_items 137632.
I0304 19:30:13.665206 23118544486528 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0304 19:30:13.665311 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:13.682183 23118544486528 run.py:483] Algo bellman_ford step 4301 current loss 0.029465, current_train_items 137664.
I0304 19:30:13.706966 23118544486528 run.py:483] Algo bellman_ford step 4302 current loss 0.050890, current_train_items 137696.
I0304 19:30:13.740007 23118544486528 run.py:483] Algo bellman_ford step 4303 current loss 0.284059, current_train_items 137728.
I0304 19:30:13.777075 23118544486528 run.py:483] Algo bellman_ford step 4304 current loss 0.057731, current_train_items 137760.
I0304 19:30:13.797483 23118544486528 run.py:483] Algo bellman_ford step 4305 current loss 0.006846, current_train_items 137792.
I0304 19:30:13.813628 23118544486528 run.py:483] Algo bellman_ford step 4306 current loss 0.015992, current_train_items 137824.
I0304 19:30:13.839112 23118544486528 run.py:483] Algo bellman_ford step 4307 current loss 0.081381, current_train_items 137856.
I0304 19:30:13.872056 23118544486528 run.py:483] Algo bellman_ford step 4308 current loss 0.083894, current_train_items 137888.
I0304 19:30:13.908610 23118544486528 run.py:483] Algo bellman_ford step 4309 current loss 0.102196, current_train_items 137920.
I0304 19:30:13.928446 23118544486528 run.py:483] Algo bellman_ford step 4310 current loss 0.011466, current_train_items 137952.
I0304 19:30:13.944889 23118544486528 run.py:483] Algo bellman_ford step 4311 current loss 0.039713, current_train_items 137984.
I0304 19:30:13.967843 23118544486528 run.py:483] Algo bellman_ford step 4312 current loss 0.039827, current_train_items 138016.
I0304 19:30:13.999943 23118544486528 run.py:483] Algo bellman_ford step 4313 current loss 0.044088, current_train_items 138048.
I0304 19:30:14.034702 23118544486528 run.py:483] Algo bellman_ford step 4314 current loss 0.106459, current_train_items 138080.
I0304 19:30:14.054396 23118544486528 run.py:483] Algo bellman_ford step 4315 current loss 0.005357, current_train_items 138112.
I0304 19:30:14.070552 23118544486528 run.py:483] Algo bellman_ford step 4316 current loss 0.057015, current_train_items 138144.
I0304 19:30:14.095330 23118544486528 run.py:483] Algo bellman_ford step 4317 current loss 0.104126, current_train_items 138176.
I0304 19:30:14.126082 23118544486528 run.py:483] Algo bellman_ford step 4318 current loss 0.108642, current_train_items 138208.
I0304 19:30:14.160367 23118544486528 run.py:483] Algo bellman_ford step 4319 current loss 0.065830, current_train_items 138240.
I0304 19:30:14.180078 23118544486528 run.py:483] Algo bellman_ford step 4320 current loss 0.005596, current_train_items 138272.
I0304 19:30:14.196434 23118544486528 run.py:483] Algo bellman_ford step 4321 current loss 0.024132, current_train_items 138304.
I0304 19:30:14.220924 23118544486528 run.py:483] Algo bellman_ford step 4322 current loss 0.073409, current_train_items 138336.
I0304 19:30:14.251497 23118544486528 run.py:483] Algo bellman_ford step 4323 current loss 0.129938, current_train_items 138368.
I0304 19:30:14.286487 23118544486528 run.py:483] Algo bellman_ford step 4324 current loss 0.104322, current_train_items 138400.
I0304 19:30:14.306097 23118544486528 run.py:483] Algo bellman_ford step 4325 current loss 0.004854, current_train_items 138432.
I0304 19:30:14.323160 23118544486528 run.py:483] Algo bellman_ford step 4326 current loss 0.011292, current_train_items 138464.
I0304 19:30:14.347549 23118544486528 run.py:483] Algo bellman_ford step 4327 current loss 0.107694, current_train_items 138496.
I0304 19:30:14.380578 23118544486528 run.py:483] Algo bellman_ford step 4328 current loss 0.131196, current_train_items 138528.
I0304 19:30:14.413836 23118544486528 run.py:483] Algo bellman_ford step 4329 current loss 0.079358, current_train_items 138560.
I0304 19:30:14.434187 23118544486528 run.py:483] Algo bellman_ford step 4330 current loss 0.005599, current_train_items 138592.
I0304 19:30:14.450964 23118544486528 run.py:483] Algo bellman_ford step 4331 current loss 0.013530, current_train_items 138624.
I0304 19:30:14.475775 23118544486528 run.py:483] Algo bellman_ford step 4332 current loss 0.048613, current_train_items 138656.
I0304 19:30:14.506798 23118544486528 run.py:483] Algo bellman_ford step 4333 current loss 0.103450, current_train_items 138688.
I0304 19:30:14.539720 23118544486528 run.py:483] Algo bellman_ford step 4334 current loss 0.089152, current_train_items 138720.
I0304 19:30:14.559676 23118544486528 run.py:483] Algo bellman_ford step 4335 current loss 0.006663, current_train_items 138752.
I0304 19:30:14.576083 23118544486528 run.py:483] Algo bellman_ford step 4336 current loss 0.021740, current_train_items 138784.
I0304 19:30:14.600419 23118544486528 run.py:483] Algo bellman_ford step 4337 current loss 0.084356, current_train_items 138816.
I0304 19:30:14.633751 23118544486528 run.py:483] Algo bellman_ford step 4338 current loss 0.074255, current_train_items 138848.
I0304 19:30:14.666812 23118544486528 run.py:483] Algo bellman_ford step 4339 current loss 0.087867, current_train_items 138880.
I0304 19:30:14.686829 23118544486528 run.py:483] Algo bellman_ford step 4340 current loss 0.012603, current_train_items 138912.
I0304 19:30:14.702927 23118544486528 run.py:483] Algo bellman_ford step 4341 current loss 0.009354, current_train_items 138944.
I0304 19:30:14.727082 23118544486528 run.py:483] Algo bellman_ford step 4342 current loss 0.043393, current_train_items 138976.
I0304 19:30:14.759146 23118544486528 run.py:483] Algo bellman_ford step 4343 current loss 0.084490, current_train_items 139008.
I0304 19:30:14.794731 23118544486528 run.py:483] Algo bellman_ford step 4344 current loss 0.083611, current_train_items 139040.
I0304 19:30:14.814391 23118544486528 run.py:483] Algo bellman_ford step 4345 current loss 0.010090, current_train_items 139072.
I0304 19:30:14.831140 23118544486528 run.py:483] Algo bellman_ford step 4346 current loss 0.041652, current_train_items 139104.
I0304 19:30:14.855345 23118544486528 run.py:483] Algo bellman_ford step 4347 current loss 0.059699, current_train_items 139136.
I0304 19:30:14.887327 23118544486528 run.py:483] Algo bellman_ford step 4348 current loss 0.050437, current_train_items 139168.
I0304 19:30:14.920943 23118544486528 run.py:483] Algo bellman_ford step 4349 current loss 0.094982, current_train_items 139200.
I0304 19:30:14.940841 23118544486528 run.py:483] Algo bellman_ford step 4350 current loss 0.008730, current_train_items 139232.
I0304 19:30:14.948692 23118544486528 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0304 19:30:14.948798 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:14.965900 23118544486528 run.py:483] Algo bellman_ford step 4351 current loss 0.030332, current_train_items 139264.
I0304 19:30:14.990970 23118544486528 run.py:483] Algo bellman_ford step 4352 current loss 0.096271, current_train_items 139296.
I0304 19:30:15.022372 23118544486528 run.py:483] Algo bellman_ford step 4353 current loss 0.097344, current_train_items 139328.
I0304 19:30:15.057664 23118544486528 run.py:483] Algo bellman_ford step 4354 current loss 0.119738, current_train_items 139360.
I0304 19:30:15.077459 23118544486528 run.py:483] Algo bellman_ford step 4355 current loss 0.006796, current_train_items 139392.
I0304 19:30:15.093590 23118544486528 run.py:483] Algo bellman_ford step 4356 current loss 0.017931, current_train_items 139424.
I0304 19:30:15.118079 23118544486528 run.py:483] Algo bellman_ford step 4357 current loss 0.052236, current_train_items 139456.
I0304 19:30:15.147778 23118544486528 run.py:483] Algo bellman_ford step 4358 current loss 0.097560, current_train_items 139488.
I0304 19:30:15.180771 23118544486528 run.py:483] Algo bellman_ford step 4359 current loss 0.082348, current_train_items 139520.
I0304 19:30:15.201000 23118544486528 run.py:483] Algo bellman_ford step 4360 current loss 0.005198, current_train_items 139552.
I0304 19:30:15.218385 23118544486528 run.py:483] Algo bellman_ford step 4361 current loss 0.062444, current_train_items 139584.
I0304 19:30:15.241245 23118544486528 run.py:483] Algo bellman_ford step 4362 current loss 0.058612, current_train_items 139616.
I0304 19:30:15.273749 23118544486528 run.py:483] Algo bellman_ford step 4363 current loss 0.067809, current_train_items 139648.
I0304 19:30:15.309152 23118544486528 run.py:483] Algo bellman_ford step 4364 current loss 0.116484, current_train_items 139680.
I0304 19:30:15.328583 23118544486528 run.py:483] Algo bellman_ford step 4365 current loss 0.008251, current_train_items 139712.
I0304 19:30:15.345050 23118544486528 run.py:483] Algo bellman_ford step 4366 current loss 0.021196, current_train_items 139744.
I0304 19:30:15.369060 23118544486528 run.py:483] Algo bellman_ford step 4367 current loss 0.047340, current_train_items 139776.
I0304 19:30:15.399594 23118544486528 run.py:483] Algo bellman_ford step 4368 current loss 0.071130, current_train_items 139808.
I0304 19:30:15.434596 23118544486528 run.py:483] Algo bellman_ford step 4369 current loss 0.081970, current_train_items 139840.
I0304 19:30:15.454235 23118544486528 run.py:483] Algo bellman_ford step 4370 current loss 0.004444, current_train_items 139872.
I0304 19:30:15.470535 23118544486528 run.py:483] Algo bellman_ford step 4371 current loss 0.036423, current_train_items 139904.
I0304 19:30:15.493836 23118544486528 run.py:483] Algo bellman_ford step 4372 current loss 0.060849, current_train_items 139936.
I0304 19:30:15.524494 23118544486528 run.py:483] Algo bellman_ford step 4373 current loss 0.052070, current_train_items 139968.
I0304 19:30:15.558273 23118544486528 run.py:483] Algo bellman_ford step 4374 current loss 0.104101, current_train_items 140000.
I0304 19:30:15.578377 23118544486528 run.py:483] Algo bellman_ford step 4375 current loss 0.017498, current_train_items 140032.
I0304 19:30:15.594643 23118544486528 run.py:483] Algo bellman_ford step 4376 current loss 0.012222, current_train_items 140064.
I0304 19:30:15.617974 23118544486528 run.py:483] Algo bellman_ford step 4377 current loss 0.047639, current_train_items 140096.
I0304 19:30:15.648623 23118544486528 run.py:483] Algo bellman_ford step 4378 current loss 0.067601, current_train_items 140128.
I0304 19:30:15.683322 23118544486528 run.py:483] Algo bellman_ford step 4379 current loss 0.051980, current_train_items 140160.
I0304 19:30:15.702735 23118544486528 run.py:483] Algo bellman_ford step 4380 current loss 0.007594, current_train_items 140192.
I0304 19:30:15.718990 23118544486528 run.py:483] Algo bellman_ford step 4381 current loss 0.058420, current_train_items 140224.
I0304 19:30:15.743094 23118544486528 run.py:483] Algo bellman_ford step 4382 current loss 0.076430, current_train_items 140256.
I0304 19:30:15.774368 23118544486528 run.py:483] Algo bellman_ford step 4383 current loss 0.095825, current_train_items 140288.
I0304 19:30:15.808885 23118544486528 run.py:483] Algo bellman_ford step 4384 current loss 0.088210, current_train_items 140320.
I0304 19:30:15.828778 23118544486528 run.py:483] Algo bellman_ford step 4385 current loss 0.014837, current_train_items 140352.
I0304 19:30:15.844887 23118544486528 run.py:483] Algo bellman_ford step 4386 current loss 0.026851, current_train_items 140384.
I0304 19:30:15.869009 23118544486528 run.py:483] Algo bellman_ford step 4387 current loss 0.081628, current_train_items 140416.
I0304 19:30:15.900575 23118544486528 run.py:483] Algo bellman_ford step 4388 current loss 0.079890, current_train_items 140448.
I0304 19:30:15.932884 23118544486528 run.py:483] Algo bellman_ford step 4389 current loss 0.063816, current_train_items 140480.
I0304 19:30:15.952869 23118544486528 run.py:483] Algo bellman_ford step 4390 current loss 0.007030, current_train_items 140512.
I0304 19:30:15.969266 23118544486528 run.py:483] Algo bellman_ford step 4391 current loss 0.012013, current_train_items 140544.
I0304 19:30:15.992832 23118544486528 run.py:483] Algo bellman_ford step 4392 current loss 0.065078, current_train_items 140576.
I0304 19:30:16.024265 23118544486528 run.py:483] Algo bellman_ford step 4393 current loss 0.121740, current_train_items 140608.
I0304 19:30:16.058694 23118544486528 run.py:483] Algo bellman_ford step 4394 current loss 0.156156, current_train_items 140640.
I0304 19:30:16.078540 23118544486528 run.py:483] Algo bellman_ford step 4395 current loss 0.020120, current_train_items 140672.
I0304 19:30:16.095477 23118544486528 run.py:483] Algo bellman_ford step 4396 current loss 0.063889, current_train_items 140704.
I0304 19:30:16.119757 23118544486528 run.py:483] Algo bellman_ford step 4397 current loss 0.059446, current_train_items 140736.
I0304 19:30:16.151234 23118544486528 run.py:483] Algo bellman_ford step 4398 current loss 0.071452, current_train_items 140768.
I0304 19:30:16.184765 23118544486528 run.py:483] Algo bellman_ford step 4399 current loss 0.152407, current_train_items 140800.
I0304 19:30:16.205046 23118544486528 run.py:483] Algo bellman_ford step 4400 current loss 0.005693, current_train_items 140832.
I0304 19:30:16.212873 23118544486528 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0304 19:30:16.212980 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0304 19:30:16.230209 23118544486528 run.py:483] Algo bellman_ford step 4401 current loss 0.025391, current_train_items 140864.
I0304 19:30:16.254749 23118544486528 run.py:483] Algo bellman_ford step 4402 current loss 0.095366, current_train_items 140896.
I0304 19:30:16.286169 23118544486528 run.py:483] Algo bellman_ford step 4403 current loss 0.060447, current_train_items 140928.
I0304 19:30:16.324645 23118544486528 run.py:483] Algo bellman_ford step 4404 current loss 0.204180, current_train_items 140960.
I0304 19:30:16.344637 23118544486528 run.py:483] Algo bellman_ford step 4405 current loss 0.010501, current_train_items 140992.
I0304 19:30:16.360945 23118544486528 run.py:483] Algo bellman_ford step 4406 current loss 0.054984, current_train_items 141024.
I0304 19:30:16.385172 23118544486528 run.py:483] Algo bellman_ford step 4407 current loss 0.068277, current_train_items 141056.
I0304 19:30:16.416491 23118544486528 run.py:483] Algo bellman_ford step 4408 current loss 0.114509, current_train_items 141088.
I0304 19:30:16.452494 23118544486528 run.py:483] Algo bellman_ford step 4409 current loss 0.111412, current_train_items 141120.
I0304 19:30:16.472311 23118544486528 run.py:483] Algo bellman_ford step 4410 current loss 0.005231, current_train_items 141152.
I0304 19:30:16.488797 23118544486528 run.py:483] Algo bellman_ford step 4411 current loss 0.019819, current_train_items 141184.
I0304 19:30:16.513379 23118544486528 run.py:483] Algo bellman_ford step 4412 current loss 0.047704, current_train_items 141216.
I0304 19:30:16.546330 23118544486528 run.py:483] Algo bellman_ford step 4413 current loss 0.080929, current_train_items 141248.
I0304 19:30:16.580067 23118544486528 run.py:483] Algo bellman_ford step 4414 current loss 0.042244, current_train_items 141280.
I0304 19:30:16.599628 23118544486528 run.py:483] Algo bellman_ford step 4415 current loss 0.005168, current_train_items 141312.
I0304 19:30:16.616410 23118544486528 run.py:483] Algo bellman_ford step 4416 current loss 0.043454, current_train_items 141344.
I0304 19:30:16.640557 23118544486528 run.py:483] Algo bellman_ford step 4417 current loss 0.088404, current_train_items 141376.
I0304 19:30:16.672084 23118544486528 run.py:483] Algo bellman_ford step 4418 current loss 0.063945, current_train_items 141408.
I0304 19:30:16.706202 23118544486528 run.py:483] Algo bellman_ford step 4419 current loss 0.073224, current_train_items 141440.
I0304 19:30:16.725743 23118544486528 run.py:483] Algo bellman_ford step 4420 current loss 0.018108, current_train_items 141472.
I0304 19:30:16.741827 23118544486528 run.py:483] Algo bellman_ford step 4421 current loss 0.015552, current_train_items 141504.
I0304 19:30:16.765871 23118544486528 run.py:483] Algo bellman_ford step 4422 current loss 0.080824, current_train_items 141536.
I0304 19:30:16.797432 23118544486528 run.py:483] Algo bellman_ford step 4423 current loss 0.043689, current_train_items 141568.
I0304 19:30:16.832481 23118544486528 run.py:483] Algo bellman_ford step 4424 current loss 0.075249, current_train_items 141600.
I0304 19:30:16.852468 23118544486528 run.py:483] Algo bellman_ford step 4425 current loss 0.011574, current_train_items 141632.
I0304 19:30:16.868267 23118544486528 run.py:483] Algo bellman_ford step 4426 current loss 0.030719, current_train_items 141664.
I0304 19:30:16.892781 23118544486528 run.py:483] Algo bellman_ford step 4427 current loss 0.089626, current_train_items 141696.
I0304 19:30:16.924534 23118544486528 run.py:483] Algo bellman_ford step 4428 current loss 0.048743, current_train_items 141728.
I0304 19:30:16.958636 23118544486528 run.py:483] Algo bellman_ford step 4429 current loss 0.053217, current_train_items 141760.
I0304 19:30:16.978368 23118544486528 run.py:483] Algo bellman_ford step 4430 current loss 0.004525, current_train_items 141792.
I0304 19:30:16.994383 23118544486528 run.py:483] Algo bellman_ford step 4431 current loss 0.036965, current_train_items 141824.
I0304 19:30:17.018769 23118544486528 run.py:483] Algo bellman_ford step 4432 current loss 0.082865, current_train_items 141856.
I0304 19:30:17.049381 23118544486528 run.py:483] Algo bellman_ford step 4433 current loss 0.087228, current_train_items 141888.
I0304 19:30:17.083702 23118544486528 run.py:483] Algo bellman_ford step 4434 current loss 0.191823, current_train_items 141920.
I0304 19:30:17.103646 23118544486528 run.py:483] Algo bellman_ford step 4435 current loss 0.005715, current_train_items 141952.
I0304 19:30:17.120126 23118544486528 run.py:483] Algo bellman_ford step 4436 current loss 0.055192, current_train_items 141984.
I0304 19:30:17.145285 23118544486528 run.py:483] Algo bellman_ford step 4437 current loss 0.052356, current_train_items 142016.
I0304 19:30:17.176099 23118544486528 run.py:483] Algo bellman_ford step 4438 current loss 0.091702, current_train_items 142048.
I0304 19:30:17.211327 23118544486528 run.py:483] Algo bellman_ford step 4439 current loss 0.249003, current_train_items 142080.
I0304 19:30:17.230973 23118544486528 run.py:483] Algo bellman_ford step 4440 current loss 0.005739, current_train_items 142112.
I0304 19:30:17.247614 23118544486528 run.py:483] Algo bellman_ford step 4441 current loss 0.109963, current_train_items 142144.
I0304 19:30:17.271559 23118544486528 run.py:483] Algo bellman_ford step 4442 current loss 0.061759, current_train_items 142176.
I0304 19:30:17.302736 23118544486528 run.py:483] Algo bellman_ford step 4443 current loss 0.066426, current_train_items 142208.
I0304 19:30:17.337662 23118544486528 run.py:483] Algo bellman_ford step 4444 current loss 0.087647, current_train_items 142240.
I0304 19:30:17.357418 23118544486528 run.py:483] Algo bellman_ford step 4445 current loss 0.012004, current_train_items 142272.
I0304 19:30:17.373596 23118544486528 run.py:483] Algo bellman_ford step 4446 current loss 0.041101, current_train_items 142304.
I0304 19:30:17.398470 23118544486528 run.py:483] Algo bellman_ford step 4447 current loss 0.098497, current_train_items 142336.
I0304 19:30:17.430599 23118544486528 run.py:483] Algo bellman_ford step 4448 current loss 0.132724, current_train_items 142368.
I0304 19:30:17.464606 23118544486528 run.py:483] Algo bellman_ford step 4449 current loss 0.088703, current_train_items 142400.
I0304 19:30:17.484519 23118544486528 run.py:483] Algo bellman_ford step 4450 current loss 0.043165, current_train_items 142432.
I0304 19:30:17.492694 23118544486528 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0304 19:30:17.492800 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:17.509264 23118544486528 run.py:483] Algo bellman_ford step 4451 current loss 0.017907, current_train_items 142464.
I0304 19:30:17.534384 23118544486528 run.py:483] Algo bellman_ford step 4452 current loss 0.050599, current_train_items 142496.
I0304 19:30:17.565328 23118544486528 run.py:483] Algo bellman_ford step 4453 current loss 0.067435, current_train_items 142528.
I0304 19:30:17.598594 23118544486528 run.py:483] Algo bellman_ford step 4454 current loss 0.057681, current_train_items 142560.
I0304 19:30:17.618592 23118544486528 run.py:483] Algo bellman_ford step 4455 current loss 0.014156, current_train_items 142592.
I0304 19:30:17.634557 23118544486528 run.py:483] Algo bellman_ford step 4456 current loss 0.030137, current_train_items 142624.
I0304 19:30:17.658978 23118544486528 run.py:483] Algo bellman_ford step 4457 current loss 0.130537, current_train_items 142656.
I0304 19:30:17.689041 23118544486528 run.py:483] Algo bellman_ford step 4458 current loss 0.137144, current_train_items 142688.
I0304 19:30:17.726456 23118544486528 run.py:483] Algo bellman_ford step 4459 current loss 0.139651, current_train_items 142720.
I0304 19:30:17.746640 23118544486528 run.py:483] Algo bellman_ford step 4460 current loss 0.013779, current_train_items 142752.
I0304 19:30:17.763572 23118544486528 run.py:483] Algo bellman_ford step 4461 current loss 0.033250, current_train_items 142784.
I0304 19:30:17.788290 23118544486528 run.py:483] Algo bellman_ford step 4462 current loss 0.078240, current_train_items 142816.
I0304 19:30:17.818403 23118544486528 run.py:483] Algo bellman_ford step 4463 current loss 0.063725, current_train_items 142848.
I0304 19:30:17.852915 23118544486528 run.py:483] Algo bellman_ford step 4464 current loss 0.126396, current_train_items 142880.
I0304 19:30:17.872948 23118544486528 run.py:483] Algo bellman_ford step 4465 current loss 0.005365, current_train_items 142912.
I0304 19:30:17.889514 23118544486528 run.py:483] Algo bellman_ford step 4466 current loss 0.019079, current_train_items 142944.
I0304 19:30:17.913814 23118544486528 run.py:483] Algo bellman_ford step 4467 current loss 0.053256, current_train_items 142976.
I0304 19:30:17.947159 23118544486528 run.py:483] Algo bellman_ford step 4468 current loss 0.066681, current_train_items 143008.
I0304 19:30:17.981811 23118544486528 run.py:483] Algo bellman_ford step 4469 current loss 0.083341, current_train_items 143040.
I0304 19:30:18.002067 23118544486528 run.py:483] Algo bellman_ford step 4470 current loss 0.006701, current_train_items 143072.
I0304 19:30:18.018289 23118544486528 run.py:483] Algo bellman_ford step 4471 current loss 0.011346, current_train_items 143104.
I0304 19:30:18.042203 23118544486528 run.py:483] Algo bellman_ford step 4472 current loss 0.073383, current_train_items 143136.
I0304 19:30:18.073787 23118544486528 run.py:483] Algo bellman_ford step 4473 current loss 0.090663, current_train_items 143168.
I0304 19:30:18.110694 23118544486528 run.py:483] Algo bellman_ford step 4474 current loss 0.105499, current_train_items 143200.
I0304 19:30:18.131103 23118544486528 run.py:483] Algo bellman_ford step 4475 current loss 0.014204, current_train_items 143232.
I0304 19:30:18.147459 23118544486528 run.py:483] Algo bellman_ford step 4476 current loss 0.047610, current_train_items 143264.
I0304 19:30:18.171362 23118544486528 run.py:483] Algo bellman_ford step 4477 current loss 0.050282, current_train_items 143296.
I0304 19:30:18.202970 23118544486528 run.py:483] Algo bellman_ford step 4478 current loss 0.067461, current_train_items 143328.
I0304 19:30:18.238955 23118544486528 run.py:483] Algo bellman_ford step 4479 current loss 0.108485, current_train_items 143360.
I0304 19:30:18.259063 23118544486528 run.py:483] Algo bellman_ford step 4480 current loss 0.003719, current_train_items 143392.
I0304 19:30:18.275437 23118544486528 run.py:483] Algo bellman_ford step 4481 current loss 0.034449, current_train_items 143424.
I0304 19:30:18.299605 23118544486528 run.py:483] Algo bellman_ford step 4482 current loss 0.048504, current_train_items 143456.
I0304 19:30:18.331510 23118544486528 run.py:483] Algo bellman_ford step 4483 current loss 0.094788, current_train_items 143488.
I0304 19:30:18.363425 23118544486528 run.py:483] Algo bellman_ford step 4484 current loss 0.078027, current_train_items 143520.
I0304 19:30:18.383541 23118544486528 run.py:483] Algo bellman_ford step 4485 current loss 0.006214, current_train_items 143552.
I0304 19:30:18.400485 23118544486528 run.py:483] Algo bellman_ford step 4486 current loss 0.008550, current_train_items 143584.
I0304 19:30:18.425649 23118544486528 run.py:483] Algo bellman_ford step 4487 current loss 0.083485, current_train_items 143616.
I0304 19:30:18.457824 23118544486528 run.py:483] Algo bellman_ford step 4488 current loss 0.064868, current_train_items 143648.
I0304 19:30:18.491752 23118544486528 run.py:483] Algo bellman_ford step 4489 current loss 0.072267, current_train_items 143680.
I0304 19:30:18.512047 23118544486528 run.py:483] Algo bellman_ford step 4490 current loss 0.007411, current_train_items 143712.
I0304 19:30:18.528088 23118544486528 run.py:483] Algo bellman_ford step 4491 current loss 0.015908, current_train_items 143744.
I0304 19:30:18.552858 23118544486528 run.py:483] Algo bellman_ford step 4492 current loss 0.035841, current_train_items 143776.
I0304 19:30:18.584566 23118544486528 run.py:483] Algo bellman_ford step 4493 current loss 0.079773, current_train_items 143808.
I0304 19:30:18.619466 23118544486528 run.py:483] Algo bellman_ford step 4494 current loss 0.111560, current_train_items 143840.
I0304 19:30:18.639442 23118544486528 run.py:483] Algo bellman_ford step 4495 current loss 0.006330, current_train_items 143872.
I0304 19:30:18.656193 23118544486528 run.py:483] Algo bellman_ford step 4496 current loss 0.035629, current_train_items 143904.
I0304 19:30:18.680825 23118544486528 run.py:483] Algo bellman_ford step 4497 current loss 0.040919, current_train_items 143936.
I0304 19:30:18.711016 23118544486528 run.py:483] Algo bellman_ford step 4498 current loss 0.045671, current_train_items 143968.
I0304 19:30:18.743130 23118544486528 run.py:483] Algo bellman_ford step 4499 current loss 0.044879, current_train_items 144000.
I0304 19:30:18.763301 23118544486528 run.py:483] Algo bellman_ford step 4500 current loss 0.004278, current_train_items 144032.
I0304 19:30:18.770908 23118544486528 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0304 19:30:18.771013 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:18.788174 23118544486528 run.py:483] Algo bellman_ford step 4501 current loss 0.050148, current_train_items 144064.
I0304 19:30:18.813405 23118544486528 run.py:483] Algo bellman_ford step 4502 current loss 0.051459, current_train_items 144096.
I0304 19:30:18.845553 23118544486528 run.py:483] Algo bellman_ford step 4503 current loss 0.081948, current_train_items 144128.
I0304 19:30:18.880216 23118544486528 run.py:483] Algo bellman_ford step 4504 current loss 0.046375, current_train_items 144160.
I0304 19:30:18.900156 23118544486528 run.py:483] Algo bellman_ford step 4505 current loss 0.005916, current_train_items 144192.
I0304 19:30:18.916411 23118544486528 run.py:483] Algo bellman_ford step 4506 current loss 0.031651, current_train_items 144224.
I0304 19:30:18.940626 23118544486528 run.py:483] Algo bellman_ford step 4507 current loss 0.050538, current_train_items 144256.
I0304 19:30:18.971786 23118544486528 run.py:483] Algo bellman_ford step 4508 current loss 0.058558, current_train_items 144288.
I0304 19:30:19.004937 23118544486528 run.py:483] Algo bellman_ford step 4509 current loss 0.059014, current_train_items 144320.
I0304 19:30:19.024657 23118544486528 run.py:483] Algo bellman_ford step 4510 current loss 0.056344, current_train_items 144352.
I0304 19:30:19.041370 23118544486528 run.py:483] Algo bellman_ford step 4511 current loss 0.070155, current_train_items 144384.
I0304 19:30:19.065823 23118544486528 run.py:483] Algo bellman_ford step 4512 current loss 0.046352, current_train_items 144416.
I0304 19:30:19.097172 23118544486528 run.py:483] Algo bellman_ford step 4513 current loss 0.089383, current_train_items 144448.
I0304 19:30:19.130947 23118544486528 run.py:483] Algo bellman_ford step 4514 current loss 0.047365, current_train_items 144480.
I0304 19:30:19.150705 23118544486528 run.py:483] Algo bellman_ford step 4515 current loss 0.005418, current_train_items 144512.
I0304 19:30:19.167310 23118544486528 run.py:483] Algo bellman_ford step 4516 current loss 0.016651, current_train_items 144544.
I0304 19:30:19.190416 23118544486528 run.py:483] Algo bellman_ford step 4517 current loss 0.062390, current_train_items 144576.
I0304 19:30:19.222985 23118544486528 run.py:483] Algo bellman_ford step 4518 current loss 0.033373, current_train_items 144608.
I0304 19:30:19.260552 23118544486528 run.py:483] Algo bellman_ford step 4519 current loss 0.072398, current_train_items 144640.
I0304 19:30:19.280666 23118544486528 run.py:483] Algo bellman_ford step 4520 current loss 0.006628, current_train_items 144672.
I0304 19:30:19.297287 23118544486528 run.py:483] Algo bellman_ford step 4521 current loss 0.014368, current_train_items 144704.
I0304 19:30:19.321346 23118544486528 run.py:483] Algo bellman_ford step 4522 current loss 0.034667, current_train_items 144736.
I0304 19:30:19.355035 23118544486528 run.py:483] Algo bellman_ford step 4523 current loss 0.061816, current_train_items 144768.
I0304 19:30:19.389135 23118544486528 run.py:483] Algo bellman_ford step 4524 current loss 0.064302, current_train_items 144800.
I0304 19:30:19.408900 23118544486528 run.py:483] Algo bellman_ford step 4525 current loss 0.004555, current_train_items 144832.
I0304 19:30:19.425342 23118544486528 run.py:483] Algo bellman_ford step 4526 current loss 0.014635, current_train_items 144864.
I0304 19:30:19.450486 23118544486528 run.py:483] Algo bellman_ford step 4527 current loss 0.067305, current_train_items 144896.
I0304 19:30:19.482262 23118544486528 run.py:483] Algo bellman_ford step 4528 current loss 0.057414, current_train_items 144928.
I0304 19:30:19.516959 23118544486528 run.py:483] Algo bellman_ford step 4529 current loss 0.085679, current_train_items 144960.
I0304 19:30:19.537125 23118544486528 run.py:483] Algo bellman_ford step 4530 current loss 0.026240, current_train_items 144992.
I0304 19:30:19.553268 23118544486528 run.py:483] Algo bellman_ford step 4531 current loss 0.012880, current_train_items 145024.
I0304 19:30:19.576634 23118544486528 run.py:483] Algo bellman_ford step 4532 current loss 0.030686, current_train_items 145056.
I0304 19:30:19.606425 23118544486528 run.py:483] Algo bellman_ford step 4533 current loss 0.075928, current_train_items 145088.
I0304 19:30:19.642138 23118544486528 run.py:483] Algo bellman_ford step 4534 current loss 0.187076, current_train_items 145120.
I0304 19:30:19.661823 23118544486528 run.py:483] Algo bellman_ford step 4535 current loss 0.005966, current_train_items 145152.
I0304 19:30:19.678377 23118544486528 run.py:483] Algo bellman_ford step 4536 current loss 0.016974, current_train_items 145184.
I0304 19:30:19.703047 23118544486528 run.py:483] Algo bellman_ford step 4537 current loss 0.048033, current_train_items 145216.
I0304 19:30:19.734547 23118544486528 run.py:483] Algo bellman_ford step 4538 current loss 0.029841, current_train_items 145248.
I0304 19:30:19.770358 23118544486528 run.py:483] Algo bellman_ford step 4539 current loss 0.105709, current_train_items 145280.
I0304 19:30:19.790097 23118544486528 run.py:483] Algo bellman_ford step 4540 current loss 0.012115, current_train_items 145312.
I0304 19:30:19.806676 23118544486528 run.py:483] Algo bellman_ford step 4541 current loss 0.027203, current_train_items 145344.
I0304 19:30:19.830193 23118544486528 run.py:483] Algo bellman_ford step 4542 current loss 0.037810, current_train_items 145376.
I0304 19:30:19.861195 23118544486528 run.py:483] Algo bellman_ford step 4543 current loss 0.053648, current_train_items 145408.
I0304 19:30:19.896171 23118544486528 run.py:483] Algo bellman_ford step 4544 current loss 0.074672, current_train_items 145440.
I0304 19:30:19.916157 23118544486528 run.py:483] Algo bellman_ford step 4545 current loss 0.004284, current_train_items 145472.
I0304 19:30:19.932455 23118544486528 run.py:483] Algo bellman_ford step 4546 current loss 0.019812, current_train_items 145504.
I0304 19:30:19.956024 23118544486528 run.py:483] Algo bellman_ford step 4547 current loss 0.023254, current_train_items 145536.
I0304 19:30:19.986394 23118544486528 run.py:483] Algo bellman_ford step 4548 current loss 0.046850, current_train_items 145568.
I0304 19:30:20.021604 23118544486528 run.py:483] Algo bellman_ford step 4549 current loss 0.061460, current_train_items 145600.
I0304 19:30:20.041306 23118544486528 run.py:483] Algo bellman_ford step 4550 current loss 0.006738, current_train_items 145632.
I0304 19:30:20.049303 23118544486528 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0304 19:30:20.049408 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:20.066707 23118544486528 run.py:483] Algo bellman_ford step 4551 current loss 0.026894, current_train_items 145664.
I0304 19:30:20.091326 23118544486528 run.py:483] Algo bellman_ford step 4552 current loss 0.037236, current_train_items 145696.
I0304 19:30:20.123749 23118544486528 run.py:483] Algo bellman_ford step 4553 current loss 0.085134, current_train_items 145728.
I0304 19:30:20.158793 23118544486528 run.py:483] Algo bellman_ford step 4554 current loss 0.046850, current_train_items 145760.
I0304 19:30:20.179376 23118544486528 run.py:483] Algo bellman_ford step 4555 current loss 0.012934, current_train_items 145792.
I0304 19:30:20.195136 23118544486528 run.py:483] Algo bellman_ford step 4556 current loss 0.033096, current_train_items 145824.
I0304 19:30:20.218822 23118544486528 run.py:483] Algo bellman_ford step 4557 current loss 0.050558, current_train_items 145856.
I0304 19:30:20.251464 23118544486528 run.py:483] Algo bellman_ford step 4558 current loss 0.027989, current_train_items 145888.
I0304 19:30:20.287605 23118544486528 run.py:483] Algo bellman_ford step 4559 current loss 0.097621, current_train_items 145920.
I0304 19:30:20.307832 23118544486528 run.py:483] Algo bellman_ford step 4560 current loss 0.004989, current_train_items 145952.
I0304 19:30:20.324546 23118544486528 run.py:483] Algo bellman_ford step 4561 current loss 0.023068, current_train_items 145984.
I0304 19:30:20.347913 23118544486528 run.py:483] Algo bellman_ford step 4562 current loss 0.036067, current_train_items 146016.
I0304 19:30:20.378891 23118544486528 run.py:483] Algo bellman_ford step 4563 current loss 0.049782, current_train_items 146048.
I0304 19:30:20.414574 23118544486528 run.py:483] Algo bellman_ford step 4564 current loss 0.066074, current_train_items 146080.
I0304 19:30:20.434483 23118544486528 run.py:483] Algo bellman_ford step 4565 current loss 0.004687, current_train_items 146112.
I0304 19:30:20.450887 23118544486528 run.py:483] Algo bellman_ford step 4566 current loss 0.019232, current_train_items 146144.
I0304 19:30:20.475236 23118544486528 run.py:483] Algo bellman_ford step 4567 current loss 0.048459, current_train_items 146176.
I0304 19:30:20.507654 23118544486528 run.py:483] Algo bellman_ford step 4568 current loss 0.063936, current_train_items 146208.
I0304 19:30:20.540943 23118544486528 run.py:483] Algo bellman_ford step 4569 current loss 0.076139, current_train_items 146240.
I0304 19:30:20.561148 23118544486528 run.py:483] Algo bellman_ford step 4570 current loss 0.009007, current_train_items 146272.
I0304 19:30:20.577574 23118544486528 run.py:483] Algo bellman_ford step 4571 current loss 0.012728, current_train_items 146304.
I0304 19:30:20.601525 23118544486528 run.py:483] Algo bellman_ford step 4572 current loss 0.157771, current_train_items 146336.
I0304 19:30:20.632368 23118544486528 run.py:483] Algo bellman_ford step 4573 current loss 0.075166, current_train_items 146368.
I0304 19:30:20.665274 23118544486528 run.py:483] Algo bellman_ford step 4574 current loss 0.091729, current_train_items 146400.
I0304 19:30:20.686142 23118544486528 run.py:483] Algo bellman_ford step 4575 current loss 0.011050, current_train_items 146432.
I0304 19:30:20.702495 23118544486528 run.py:483] Algo bellman_ford step 4576 current loss 0.018939, current_train_items 146464.
I0304 19:30:20.726980 23118544486528 run.py:483] Algo bellman_ford step 4577 current loss 0.077035, current_train_items 146496.
I0304 19:30:20.759037 23118544486528 run.py:483] Algo bellman_ford step 4578 current loss 0.084854, current_train_items 146528.
I0304 19:30:20.793392 23118544486528 run.py:483] Algo bellman_ford step 4579 current loss 0.131095, current_train_items 146560.
I0304 19:30:20.813312 23118544486528 run.py:483] Algo bellman_ford step 4580 current loss 0.010151, current_train_items 146592.
I0304 19:30:20.830289 23118544486528 run.py:483] Algo bellman_ford step 4581 current loss 0.024999, current_train_items 146624.
I0304 19:30:20.854269 23118544486528 run.py:483] Algo bellman_ford step 4582 current loss 0.060711, current_train_items 146656.
I0304 19:30:20.885891 23118544486528 run.py:483] Algo bellman_ford step 4583 current loss 0.046029, current_train_items 146688.
I0304 19:30:20.920368 23118544486528 run.py:483] Algo bellman_ford step 4584 current loss 0.097387, current_train_items 146720.
I0304 19:30:20.940665 23118544486528 run.py:483] Algo bellman_ford step 4585 current loss 0.013486, current_train_items 146752.
I0304 19:30:20.957622 23118544486528 run.py:483] Algo bellman_ford step 4586 current loss 0.012751, current_train_items 146784.
I0304 19:30:20.981166 23118544486528 run.py:483] Algo bellman_ford step 4587 current loss 0.043044, current_train_items 146816.
I0304 19:30:21.012728 23118544486528 run.py:483] Algo bellman_ford step 4588 current loss 0.067235, current_train_items 146848.
I0304 19:30:21.045884 23118544486528 run.py:483] Algo bellman_ford step 4589 current loss 0.093365, current_train_items 146880.
I0304 19:30:21.066448 23118544486528 run.py:483] Algo bellman_ford step 4590 current loss 0.010172, current_train_items 146912.
I0304 19:30:21.082882 23118544486528 run.py:483] Algo bellman_ford step 4591 current loss 0.022933, current_train_items 146944.
I0304 19:30:21.106517 23118544486528 run.py:483] Algo bellman_ford step 4592 current loss 0.031636, current_train_items 146976.
I0304 19:30:21.138893 23118544486528 run.py:483] Algo bellman_ford step 4593 current loss 0.043436, current_train_items 147008.
I0304 19:30:21.174242 23118544486528 run.py:483] Algo bellman_ford step 4594 current loss 0.113288, current_train_items 147040.
I0304 19:30:21.194477 23118544486528 run.py:483] Algo bellman_ford step 4595 current loss 0.005760, current_train_items 147072.
I0304 19:30:21.210990 23118544486528 run.py:483] Algo bellman_ford step 4596 current loss 0.013900, current_train_items 147104.
I0304 19:30:21.237404 23118544486528 run.py:483] Algo bellman_ford step 4597 current loss 0.092224, current_train_items 147136.
I0304 19:30:21.268587 23118544486528 run.py:483] Algo bellman_ford step 4598 current loss 0.045629, current_train_items 147168.
I0304 19:30:21.301353 23118544486528 run.py:483] Algo bellman_ford step 4599 current loss 0.072813, current_train_items 147200.
I0304 19:30:21.321959 23118544486528 run.py:483] Algo bellman_ford step 4600 current loss 0.006052, current_train_items 147232.
I0304 19:30:21.329982 23118544486528 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0304 19:30:21.330087 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0304 19:30:21.347401 23118544486528 run.py:483] Algo bellman_ford step 4601 current loss 0.042913, current_train_items 147264.
I0304 19:30:21.372835 23118544486528 run.py:483] Algo bellman_ford step 4602 current loss 0.044523, current_train_items 147296.
I0304 19:30:21.404971 23118544486528 run.py:483] Algo bellman_ford step 4603 current loss 0.095777, current_train_items 147328.
I0304 19:30:21.439936 23118544486528 run.py:483] Algo bellman_ford step 4604 current loss 0.106548, current_train_items 147360.
I0304 19:30:21.459667 23118544486528 run.py:483] Algo bellman_ford step 4605 current loss 0.006978, current_train_items 147392.
I0304 19:30:21.476013 23118544486528 run.py:483] Algo bellman_ford step 4606 current loss 0.040948, current_train_items 147424.
I0304 19:30:21.499932 23118544486528 run.py:483] Algo bellman_ford step 4607 current loss 0.075682, current_train_items 147456.
I0304 19:30:21.532656 23118544486528 run.py:483] Algo bellman_ford step 4608 current loss 0.056558, current_train_items 147488.
I0304 19:30:21.564847 23118544486528 run.py:483] Algo bellman_ford step 4609 current loss 0.060304, current_train_items 147520.
I0304 19:30:21.584807 23118544486528 run.py:483] Algo bellman_ford step 4610 current loss 0.003276, current_train_items 147552.
I0304 19:30:21.601064 23118544486528 run.py:483] Algo bellman_ford step 4611 current loss 0.033251, current_train_items 147584.
I0304 19:30:21.625848 23118544486528 run.py:483] Algo bellman_ford step 4612 current loss 0.060835, current_train_items 147616.
I0304 19:30:21.658481 23118544486528 run.py:483] Algo bellman_ford step 4613 current loss 0.112401, current_train_items 147648.
I0304 19:30:21.693731 23118544486528 run.py:483] Algo bellman_ford step 4614 current loss 0.048685, current_train_items 147680.
I0304 19:30:21.713704 23118544486528 run.py:483] Algo bellman_ford step 4615 current loss 0.014223, current_train_items 147712.
I0304 19:30:21.730132 23118544486528 run.py:483] Algo bellman_ford step 4616 current loss 0.047247, current_train_items 147744.
I0304 19:30:21.754845 23118544486528 run.py:483] Algo bellman_ford step 4617 current loss 0.036677, current_train_items 147776.
I0304 19:30:21.785600 23118544486528 run.py:483] Algo bellman_ford step 4618 current loss 0.046280, current_train_items 147808.
I0304 19:30:21.820311 23118544486528 run.py:483] Algo bellman_ford step 4619 current loss 0.069292, current_train_items 147840.
I0304 19:30:21.839916 23118544486528 run.py:483] Algo bellman_ford step 4620 current loss 0.029263, current_train_items 147872.
I0304 19:30:21.856026 23118544486528 run.py:483] Algo bellman_ford step 4621 current loss 0.019956, current_train_items 147904.
I0304 19:30:21.880532 23118544486528 run.py:483] Algo bellman_ford step 4622 current loss 0.069403, current_train_items 147936.
I0304 19:30:21.912340 23118544486528 run.py:483] Algo bellman_ford step 4623 current loss 0.061856, current_train_items 147968.
I0304 19:30:21.945552 23118544486528 run.py:483] Algo bellman_ford step 4624 current loss 0.229453, current_train_items 148000.
I0304 19:30:21.965174 23118544486528 run.py:483] Algo bellman_ford step 4625 current loss 0.008527, current_train_items 148032.
I0304 19:30:21.981565 23118544486528 run.py:483] Algo bellman_ford step 4626 current loss 0.032496, current_train_items 148064.
I0304 19:30:22.006428 23118544486528 run.py:483] Algo bellman_ford step 4627 current loss 0.059969, current_train_items 148096.
I0304 19:30:22.038882 23118544486528 run.py:483] Algo bellman_ford step 4628 current loss 0.060880, current_train_items 148128.
I0304 19:30:22.074913 23118544486528 run.py:483] Algo bellman_ford step 4629 current loss 0.088474, current_train_items 148160.
I0304 19:30:22.094537 23118544486528 run.py:483] Algo bellman_ford step 4630 current loss 0.006118, current_train_items 148192.
I0304 19:30:22.111118 23118544486528 run.py:483] Algo bellman_ford step 4631 current loss 0.041809, current_train_items 148224.
I0304 19:30:22.136471 23118544486528 run.py:483] Algo bellman_ford step 4632 current loss 0.084736, current_train_items 148256.
I0304 19:30:22.166505 23118544486528 run.py:483] Algo bellman_ford step 4633 current loss 0.046625, current_train_items 148288.
I0304 19:30:22.201171 23118544486528 run.py:483] Algo bellman_ford step 4634 current loss 0.069847, current_train_items 148320.
I0304 19:30:22.221213 23118544486528 run.py:483] Algo bellman_ford step 4635 current loss 0.006471, current_train_items 148352.
I0304 19:30:22.238376 23118544486528 run.py:483] Algo bellman_ford step 4636 current loss 0.015469, current_train_items 148384.
I0304 19:30:22.262808 23118544486528 run.py:483] Algo bellman_ford step 4637 current loss 0.106813, current_train_items 148416.
I0304 19:30:22.295891 23118544486528 run.py:483] Algo bellman_ford step 4638 current loss 0.108422, current_train_items 148448.
I0304 19:30:22.330549 23118544486528 run.py:483] Algo bellman_ford step 4639 current loss 0.087789, current_train_items 148480.
I0304 19:30:22.350165 23118544486528 run.py:483] Algo bellman_ford step 4640 current loss 0.011674, current_train_items 148512.
I0304 19:30:22.366483 23118544486528 run.py:483] Algo bellman_ford step 4641 current loss 0.040184, current_train_items 148544.
I0304 19:30:22.390873 23118544486528 run.py:483] Algo bellman_ford step 4642 current loss 0.172617, current_train_items 148576.
I0304 19:30:22.422766 23118544486528 run.py:483] Algo bellman_ford step 4643 current loss 0.151532, current_train_items 148608.
I0304 19:30:22.457622 23118544486528 run.py:483] Algo bellman_ford step 4644 current loss 0.127393, current_train_items 148640.
I0304 19:30:22.477359 23118544486528 run.py:483] Algo bellman_ford step 4645 current loss 0.009303, current_train_items 148672.
I0304 19:30:22.493789 23118544486528 run.py:483] Algo bellman_ford step 4646 current loss 0.013766, current_train_items 148704.
I0304 19:30:22.517272 23118544486528 run.py:483] Algo bellman_ford step 4647 current loss 0.047204, current_train_items 148736.
I0304 19:30:22.547150 23118544486528 run.py:483] Algo bellman_ford step 4648 current loss 0.071882, current_train_items 148768.
I0304 19:30:22.579280 23118544486528 run.py:483] Algo bellman_ford step 4649 current loss 0.050762, current_train_items 148800.
I0304 19:30:22.599193 23118544486528 run.py:483] Algo bellman_ford step 4650 current loss 0.005864, current_train_items 148832.
I0304 19:30:22.607213 23118544486528 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0304 19:30:22.607318 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:22.624515 23118544486528 run.py:483] Algo bellman_ford step 4651 current loss 0.021575, current_train_items 148864.
I0304 19:30:22.649117 23118544486528 run.py:483] Algo bellman_ford step 4652 current loss 0.037139, current_train_items 148896.
I0304 19:30:22.682044 23118544486528 run.py:483] Algo bellman_ford step 4653 current loss 0.104619, current_train_items 148928.
I0304 19:30:22.716924 23118544486528 run.py:483] Algo bellman_ford step 4654 current loss 0.101186, current_train_items 148960.
I0304 19:30:22.737334 23118544486528 run.py:483] Algo bellman_ford step 4655 current loss 0.009871, current_train_items 148992.
I0304 19:30:22.753597 23118544486528 run.py:483] Algo bellman_ford step 4656 current loss 0.026086, current_train_items 149024.
I0304 19:30:22.778422 23118544486528 run.py:483] Algo bellman_ford step 4657 current loss 0.085176, current_train_items 149056.
I0304 19:30:22.809873 23118544486528 run.py:483] Algo bellman_ford step 4658 current loss 0.137546, current_train_items 149088.
I0304 19:30:22.842941 23118544486528 run.py:483] Algo bellman_ford step 4659 current loss 0.163526, current_train_items 149120.
I0304 19:30:22.863022 23118544486528 run.py:483] Algo bellman_ford step 4660 current loss 0.008710, current_train_items 149152.
I0304 19:30:22.879531 23118544486528 run.py:483] Algo bellman_ford step 4661 current loss 0.042310, current_train_items 149184.
I0304 19:30:22.904674 23118544486528 run.py:483] Algo bellman_ford step 4662 current loss 0.069156, current_train_items 149216.
I0304 19:30:22.935793 23118544486528 run.py:483] Algo bellman_ford step 4663 current loss 0.053361, current_train_items 149248.
I0304 19:30:22.973527 23118544486528 run.py:483] Algo bellman_ford step 4664 current loss 0.130171, current_train_items 149280.
I0304 19:30:22.993743 23118544486528 run.py:483] Algo bellman_ford step 4665 current loss 0.010591, current_train_items 149312.
I0304 19:30:23.009913 23118544486528 run.py:483] Algo bellman_ford step 4666 current loss 0.014304, current_train_items 149344.
I0304 19:30:23.033677 23118544486528 run.py:483] Algo bellman_ford step 4667 current loss 0.034941, current_train_items 149376.
I0304 19:30:23.066358 23118544486528 run.py:483] Algo bellman_ford step 4668 current loss 0.044665, current_train_items 149408.
I0304 19:30:23.101646 23118544486528 run.py:483] Algo bellman_ford step 4669 current loss 0.070287, current_train_items 149440.
I0304 19:30:23.121755 23118544486528 run.py:483] Algo bellman_ford step 4670 current loss 0.004629, current_train_items 149472.
I0304 19:30:23.137820 23118544486528 run.py:483] Algo bellman_ford step 4671 current loss 0.014504, current_train_items 149504.
I0304 19:30:23.161475 23118544486528 run.py:483] Algo bellman_ford step 4672 current loss 0.029139, current_train_items 149536.
I0304 19:30:23.191875 23118544486528 run.py:483] Algo bellman_ford step 4673 current loss 0.059378, current_train_items 149568.
I0304 19:30:23.225733 23118544486528 run.py:483] Algo bellman_ford step 4674 current loss 0.034248, current_train_items 149600.
I0304 19:30:23.245734 23118544486528 run.py:483] Algo bellman_ford step 4675 current loss 0.039145, current_train_items 149632.
I0304 19:30:23.262928 23118544486528 run.py:483] Algo bellman_ford step 4676 current loss 0.030533, current_train_items 149664.
I0304 19:30:23.287549 23118544486528 run.py:483] Algo bellman_ford step 4677 current loss 0.080203, current_train_items 149696.
I0304 19:30:23.320613 23118544486528 run.py:483] Algo bellman_ford step 4678 current loss 0.082389, current_train_items 149728.
I0304 19:30:23.355066 23118544486528 run.py:483] Algo bellman_ford step 4679 current loss 0.084674, current_train_items 149760.
I0304 19:30:23.374848 23118544486528 run.py:483] Algo bellman_ford step 4680 current loss 0.003876, current_train_items 149792.
I0304 19:30:23.390901 23118544486528 run.py:483] Algo bellman_ford step 4681 current loss 0.017248, current_train_items 149824.
I0304 19:30:23.415386 23118544486528 run.py:483] Algo bellman_ford step 4682 current loss 0.100660, current_train_items 149856.
I0304 19:30:23.446561 23118544486528 run.py:483] Algo bellman_ford step 4683 current loss 0.119993, current_train_items 149888.
I0304 19:30:23.482501 23118544486528 run.py:483] Algo bellman_ford step 4684 current loss 0.070107, current_train_items 149920.
I0304 19:30:23.502835 23118544486528 run.py:483] Algo bellman_ford step 4685 current loss 0.009371, current_train_items 149952.
I0304 19:30:23.518897 23118544486528 run.py:483] Algo bellman_ford step 4686 current loss 0.010498, current_train_items 149984.
I0304 19:30:23.543029 23118544486528 run.py:483] Algo bellman_ford step 4687 current loss 0.113713, current_train_items 150016.
I0304 19:30:23.575487 23118544486528 run.py:483] Algo bellman_ford step 4688 current loss 0.131815, current_train_items 150048.
I0304 19:30:23.611128 23118544486528 run.py:483] Algo bellman_ford step 4689 current loss 0.161936, current_train_items 150080.
I0304 19:30:23.631405 23118544486528 run.py:483] Algo bellman_ford step 4690 current loss 0.010177, current_train_items 150112.
I0304 19:30:23.647937 23118544486528 run.py:483] Algo bellman_ford step 4691 current loss 0.012032, current_train_items 150144.
I0304 19:30:23.671747 23118544486528 run.py:483] Algo bellman_ford step 4692 current loss 0.054884, current_train_items 150176.
I0304 19:30:23.703790 23118544486528 run.py:483] Algo bellman_ford step 4693 current loss 0.152616, current_train_items 150208.
I0304 19:30:23.737396 23118544486528 run.py:483] Algo bellman_ford step 4694 current loss 0.143500, current_train_items 150240.
I0304 19:30:23.757093 23118544486528 run.py:483] Algo bellman_ford step 4695 current loss 0.003366, current_train_items 150272.
I0304 19:30:23.773448 23118544486528 run.py:483] Algo bellman_ford step 4696 current loss 0.021510, current_train_items 150304.
I0304 19:30:23.798182 23118544486528 run.py:483] Algo bellman_ford step 4697 current loss 0.054436, current_train_items 150336.
I0304 19:30:23.829926 23118544486528 run.py:483] Algo bellman_ford step 4698 current loss 0.044694, current_train_items 150368.
I0304 19:30:23.862722 23118544486528 run.py:483] Algo bellman_ford step 4699 current loss 0.052770, current_train_items 150400.
I0304 19:30:23.883045 23118544486528 run.py:483] Algo bellman_ford step 4700 current loss 0.004011, current_train_items 150432.
I0304 19:30:23.891129 23118544486528 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0304 19:30:23.891239 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:30:23.908002 23118544486528 run.py:483] Algo bellman_ford step 4701 current loss 0.023982, current_train_items 150464.
I0304 19:30:23.932751 23118544486528 run.py:483] Algo bellman_ford step 4702 current loss 0.082401, current_train_items 150496.
I0304 19:30:23.965901 23118544486528 run.py:483] Algo bellman_ford step 4703 current loss 0.050214, current_train_items 150528.
I0304 19:30:24.001712 23118544486528 run.py:483] Algo bellman_ford step 4704 current loss 0.059182, current_train_items 150560.
I0304 19:30:24.021524 23118544486528 run.py:483] Algo bellman_ford step 4705 current loss 0.010939, current_train_items 150592.
I0304 19:30:24.037263 23118544486528 run.py:483] Algo bellman_ford step 4706 current loss 0.017707, current_train_items 150624.
I0304 19:30:24.061812 23118544486528 run.py:483] Algo bellman_ford step 4707 current loss 0.040800, current_train_items 150656.
I0304 19:30:24.094206 23118544486528 run.py:483] Algo bellman_ford step 4708 current loss 0.093403, current_train_items 150688.
I0304 19:30:24.128443 23118544486528 run.py:483] Algo bellman_ford step 4709 current loss 0.047337, current_train_items 150720.
I0304 19:30:24.147902 23118544486528 run.py:483] Algo bellman_ford step 4710 current loss 0.007208, current_train_items 150752.
I0304 19:30:24.164299 23118544486528 run.py:483] Algo bellman_ford step 4711 current loss 0.016474, current_train_items 150784.
I0304 19:30:24.189106 23118544486528 run.py:483] Algo bellman_ford step 4712 current loss 0.065940, current_train_items 150816.
I0304 19:30:24.220888 23118544486528 run.py:483] Algo bellman_ford step 4713 current loss 0.080194, current_train_items 150848.
I0304 19:30:24.253665 23118544486528 run.py:483] Algo bellman_ford step 4714 current loss 0.081905, current_train_items 150880.
I0304 19:30:24.273051 23118544486528 run.py:483] Algo bellman_ford step 4715 current loss 0.003624, current_train_items 150912.
I0304 19:30:24.289701 23118544486528 run.py:483] Algo bellman_ford step 4716 current loss 0.040739, current_train_items 150944.
I0304 19:30:24.312826 23118544486528 run.py:483] Algo bellman_ford step 4717 current loss 0.025161, current_train_items 150976.
I0304 19:30:24.344249 23118544486528 run.py:483] Algo bellman_ford step 4718 current loss 0.041613, current_train_items 151008.
I0304 19:30:24.380448 23118544486528 run.py:483] Algo bellman_ford step 4719 current loss 0.122064, current_train_items 151040.
I0304 19:30:24.400099 23118544486528 run.py:483] Algo bellman_ford step 4720 current loss 0.005845, current_train_items 151072.
I0304 19:30:24.416289 23118544486528 run.py:483] Algo bellman_ford step 4721 current loss 0.017356, current_train_items 151104.
I0304 19:30:24.441290 23118544486528 run.py:483] Algo bellman_ford step 4722 current loss 0.097581, current_train_items 151136.
I0304 19:30:24.471504 23118544486528 run.py:483] Algo bellman_ford step 4723 current loss 0.077839, current_train_items 151168.
I0304 19:30:24.506225 23118544486528 run.py:483] Algo bellman_ford step 4724 current loss 0.108539, current_train_items 151200.
I0304 19:30:24.525606 23118544486528 run.py:483] Algo bellman_ford step 4725 current loss 0.002366, current_train_items 151232.
I0304 19:30:24.542300 23118544486528 run.py:483] Algo bellman_ford step 4726 current loss 0.031354, current_train_items 151264.
I0304 19:30:24.567438 23118544486528 run.py:483] Algo bellman_ford step 4727 current loss 0.092835, current_train_items 151296.
I0304 19:30:24.597337 23118544486528 run.py:483] Algo bellman_ford step 4728 current loss 0.058372, current_train_items 151328.
I0304 19:30:24.627546 23118544486528 run.py:483] Algo bellman_ford step 4729 current loss 0.073960, current_train_items 151360.
I0304 19:30:24.646923 23118544486528 run.py:483] Algo bellman_ford step 4730 current loss 0.009642, current_train_items 151392.
I0304 19:30:24.662927 23118544486528 run.py:483] Algo bellman_ford step 4731 current loss 0.006604, current_train_items 151424.
I0304 19:30:24.686200 23118544486528 run.py:483] Algo bellman_ford step 4732 current loss 0.044070, current_train_items 151456.
I0304 19:30:24.717460 23118544486528 run.py:483] Algo bellman_ford step 4733 current loss 0.054896, current_train_items 151488.
I0304 19:30:24.751848 23118544486528 run.py:483] Algo bellman_ford step 4734 current loss 0.097259, current_train_items 151520.
I0304 19:30:24.771330 23118544486528 run.py:483] Algo bellman_ford step 4735 current loss 0.003482, current_train_items 151552.
I0304 19:30:24.787948 23118544486528 run.py:483] Algo bellman_ford step 4736 current loss 0.017189, current_train_items 151584.
I0304 19:30:24.811691 23118544486528 run.py:483] Algo bellman_ford step 4737 current loss 0.041547, current_train_items 151616.
I0304 19:30:24.842599 23118544486528 run.py:483] Algo bellman_ford step 4738 current loss 0.087456, current_train_items 151648.
I0304 19:30:24.875667 23118544486528 run.py:483] Algo bellman_ford step 4739 current loss 0.070841, current_train_items 151680.
I0304 19:30:24.895365 23118544486528 run.py:483] Algo bellman_ford step 4740 current loss 0.043843, current_train_items 151712.
I0304 19:30:24.911427 23118544486528 run.py:483] Algo bellman_ford step 4741 current loss 0.043001, current_train_items 151744.
I0304 19:30:24.934669 23118544486528 run.py:483] Algo bellman_ford step 4742 current loss 0.083781, current_train_items 151776.
I0304 19:30:24.966830 23118544486528 run.py:483] Algo bellman_ford step 4743 current loss 0.063913, current_train_items 151808.
I0304 19:30:24.999179 23118544486528 run.py:483] Algo bellman_ford step 4744 current loss 0.090891, current_train_items 151840.
I0304 19:30:25.018663 23118544486528 run.py:483] Algo bellman_ford step 4745 current loss 0.012412, current_train_items 151872.
I0304 19:30:25.035398 23118544486528 run.py:483] Algo bellman_ford step 4746 current loss 0.053453, current_train_items 151904.
I0304 19:30:25.060853 23118544486528 run.py:483] Algo bellman_ford step 4747 current loss 0.054264, current_train_items 151936.
I0304 19:30:25.092290 23118544486528 run.py:483] Algo bellman_ford step 4748 current loss 0.092160, current_train_items 151968.
I0304 19:30:25.125907 23118544486528 run.py:483] Algo bellman_ford step 4749 current loss 0.087428, current_train_items 152000.
I0304 19:30:25.145634 23118544486528 run.py:483] Algo bellman_ford step 4750 current loss 0.024957, current_train_items 152032.
I0304 19:30:25.153511 23118544486528 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0304 19:30:25.153638 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0304 19:30:25.170569 23118544486528 run.py:483] Algo bellman_ford step 4751 current loss 0.022280, current_train_items 152064.
I0304 19:30:25.195436 23118544486528 run.py:483] Algo bellman_ford step 4752 current loss 0.027377, current_train_items 152096.
I0304 19:30:25.229012 23118544486528 run.py:483] Algo bellman_ford step 4753 current loss 0.092609, current_train_items 152128.
I0304 19:30:25.262823 23118544486528 run.py:483] Algo bellman_ford step 4754 current loss 0.054134, current_train_items 152160.
I0304 19:30:25.282662 23118544486528 run.py:483] Algo bellman_ford step 4755 current loss 0.005711, current_train_items 152192.
I0304 19:30:25.298720 23118544486528 run.py:483] Algo bellman_ford step 4756 current loss 0.008396, current_train_items 152224.
I0304 19:30:25.323600 23118544486528 run.py:483] Algo bellman_ford step 4757 current loss 0.089817, current_train_items 152256.
I0304 19:30:25.355323 23118544486528 run.py:483] Algo bellman_ford step 4758 current loss 0.086477, current_train_items 152288.
I0304 19:30:25.390101 23118544486528 run.py:483] Algo bellman_ford step 4759 current loss 0.104779, current_train_items 152320.
I0304 19:30:25.410460 23118544486528 run.py:483] Algo bellman_ford step 4760 current loss 0.004367, current_train_items 152352.
I0304 19:30:25.426975 23118544486528 run.py:483] Algo bellman_ford step 4761 current loss 0.018371, current_train_items 152384.
I0304 19:30:25.451187 23118544486528 run.py:483] Algo bellman_ford step 4762 current loss 0.089094, current_train_items 152416.
I0304 19:30:25.482285 23118544486528 run.py:483] Algo bellman_ford step 4763 current loss 0.061502, current_train_items 152448.
I0304 19:30:25.516941 23118544486528 run.py:483] Algo bellman_ford step 4764 current loss 0.105047, current_train_items 152480.
I0304 19:30:25.536473 23118544486528 run.py:483] Algo bellman_ford step 4765 current loss 0.014453, current_train_items 152512.
I0304 19:30:25.552484 23118544486528 run.py:483] Algo bellman_ford step 4766 current loss 0.021395, current_train_items 152544.
I0304 19:30:25.577391 23118544486528 run.py:483] Algo bellman_ford step 4767 current loss 0.073298, current_train_items 152576.
I0304 19:30:25.607864 23118544486528 run.py:483] Algo bellman_ford step 4768 current loss 0.044427, current_train_items 152608.
I0304 19:30:25.640880 23118544486528 run.py:483] Algo bellman_ford step 4769 current loss 0.088475, current_train_items 152640.
I0304 19:30:25.661147 23118544486528 run.py:483] Algo bellman_ford step 4770 current loss 0.005840, current_train_items 152672.
I0304 19:30:25.677345 23118544486528 run.py:483] Algo bellman_ford step 4771 current loss 0.009155, current_train_items 152704.
I0304 19:30:25.700358 23118544486528 run.py:483] Algo bellman_ford step 4772 current loss 0.042160, current_train_items 152736.
I0304 19:30:25.731615 23118544486528 run.py:483] Algo bellman_ford step 4773 current loss 0.093353, current_train_items 152768.
I0304 19:30:25.766271 23118544486528 run.py:483] Algo bellman_ford step 4774 current loss 0.086178, current_train_items 152800.
I0304 19:30:25.786164 23118544486528 run.py:483] Algo bellman_ford step 4775 current loss 0.005675, current_train_items 152832.
I0304 19:30:25.802893 23118544486528 run.py:483] Algo bellman_ford step 4776 current loss 0.026889, current_train_items 152864.
I0304 19:30:25.826633 23118544486528 run.py:483] Algo bellman_ford step 4777 current loss 0.034673, current_train_items 152896.
I0304 19:30:25.858178 23118544486528 run.py:483] Algo bellman_ford step 4778 current loss 0.070250, current_train_items 152928.
I0304 19:30:25.890751 23118544486528 run.py:483] Algo bellman_ford step 4779 current loss 0.067606, current_train_items 152960.
I0304 19:30:25.910401 23118544486528 run.py:483] Algo bellman_ford step 4780 current loss 0.007064, current_train_items 152992.
I0304 19:30:25.926858 23118544486528 run.py:483] Algo bellman_ford step 4781 current loss 0.014638, current_train_items 153024.
I0304 19:30:25.951659 23118544486528 run.py:483] Algo bellman_ford step 4782 current loss 0.063078, current_train_items 153056.
I0304 19:30:25.983710 23118544486528 run.py:483] Algo bellman_ford step 4783 current loss 0.063260, current_train_items 153088.
I0304 19:30:26.018564 23118544486528 run.py:483] Algo bellman_ford step 4784 current loss 0.079051, current_train_items 153120.
I0304 19:30:26.038822 23118544486528 run.py:483] Algo bellman_ford step 4785 current loss 0.018205, current_train_items 153152.
I0304 19:30:26.055612 23118544486528 run.py:483] Algo bellman_ford step 4786 current loss 0.018997, current_train_items 153184.
I0304 19:30:26.080197 23118544486528 run.py:483] Algo bellman_ford step 4787 current loss 0.088633, current_train_items 153216.
I0304 19:30:26.111255 23118544486528 run.py:483] Algo bellman_ford step 4788 current loss 0.121944, current_train_items 153248.
I0304 19:30:26.144017 23118544486528 run.py:483] Algo bellman_ford step 4789 current loss 0.081671, current_train_items 153280.
I0304 19:30:26.163769 23118544486528 run.py:483] Algo bellman_ford step 4790 current loss 0.003984, current_train_items 153312.
I0304 19:30:26.180139 23118544486528 run.py:483] Algo bellman_ford step 4791 current loss 0.037081, current_train_items 153344.
I0304 19:30:26.204040 23118544486528 run.py:483] Algo bellman_ford step 4792 current loss 0.126499, current_train_items 153376.
I0304 19:30:26.235269 23118544486528 run.py:483] Algo bellman_ford step 4793 current loss 0.191024, current_train_items 153408.
I0304 19:30:26.271470 23118544486528 run.py:483] Algo bellman_ford step 4794 current loss 0.084139, current_train_items 153440.
I0304 19:30:26.290969 23118544486528 run.py:483] Algo bellman_ford step 4795 current loss 0.003702, current_train_items 153472.
I0304 19:30:26.307378 23118544486528 run.py:483] Algo bellman_ford step 4796 current loss 0.008188, current_train_items 153504.
I0304 19:30:26.331986 23118544486528 run.py:483] Algo bellman_ford step 4797 current loss 0.062689, current_train_items 153536.
I0304 19:30:26.363046 23118544486528 run.py:483] Algo bellman_ford step 4798 current loss 0.132243, current_train_items 153568.
I0304 19:30:26.396449 23118544486528 run.py:483] Algo bellman_ford step 4799 current loss 0.142212, current_train_items 153600.
I0304 19:30:26.416354 23118544486528 run.py:483] Algo bellman_ford step 4800 current loss 0.006086, current_train_items 153632.
I0304 19:30:26.424003 23118544486528 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0304 19:30:26.424110 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:26.441153 23118544486528 run.py:483] Algo bellman_ford step 4801 current loss 0.042008, current_train_items 153664.
I0304 19:30:26.465524 23118544486528 run.py:483] Algo bellman_ford step 4802 current loss 0.059296, current_train_items 153696.
I0304 19:30:26.496929 23118544486528 run.py:483] Algo bellman_ford step 4803 current loss 0.030595, current_train_items 153728.
I0304 19:30:26.534077 23118544486528 run.py:483] Algo bellman_ford step 4804 current loss 0.081901, current_train_items 153760.
I0304 19:30:26.553944 23118544486528 run.py:483] Algo bellman_ford step 4805 current loss 0.038342, current_train_items 153792.
I0304 19:30:26.570484 23118544486528 run.py:483] Algo bellman_ford step 4806 current loss 0.017456, current_train_items 153824.
I0304 19:30:26.594768 23118544486528 run.py:483] Algo bellman_ford step 4807 current loss 0.034738, current_train_items 153856.
I0304 19:30:26.626634 23118544486528 run.py:483] Algo bellman_ford step 4808 current loss 0.086427, current_train_items 153888.
I0304 19:30:26.660735 23118544486528 run.py:483] Algo bellman_ford step 4809 current loss 0.100807, current_train_items 153920.
I0304 19:30:26.680610 23118544486528 run.py:483] Algo bellman_ford step 4810 current loss 0.005800, current_train_items 153952.
I0304 19:30:26.696966 23118544486528 run.py:483] Algo bellman_ford step 4811 current loss 0.019086, current_train_items 153984.
I0304 19:30:26.720983 23118544486528 run.py:483] Algo bellman_ford step 4812 current loss 0.019040, current_train_items 154016.
I0304 19:30:26.751837 23118544486528 run.py:483] Algo bellman_ford step 4813 current loss 0.058258, current_train_items 154048.
I0304 19:30:26.787075 23118544486528 run.py:483] Algo bellman_ford step 4814 current loss 0.117951, current_train_items 154080.
I0304 19:30:26.806807 23118544486528 run.py:483] Algo bellman_ford step 4815 current loss 0.009903, current_train_items 154112.
I0304 19:30:26.823288 23118544486528 run.py:483] Algo bellman_ford step 4816 current loss 0.037258, current_train_items 154144.
I0304 19:30:26.848501 23118544486528 run.py:483] Algo bellman_ford step 4817 current loss 0.070921, current_train_items 154176.
I0304 19:30:26.880066 23118544486528 run.py:483] Algo bellman_ford step 4818 current loss 0.045325, current_train_items 154208.
I0304 19:30:26.910577 23118544486528 run.py:483] Algo bellman_ford step 4819 current loss 0.058296, current_train_items 154240.
I0304 19:30:26.930301 23118544486528 run.py:483] Algo bellman_ford step 4820 current loss 0.006436, current_train_items 154272.
I0304 19:30:26.946561 23118544486528 run.py:483] Algo bellman_ford step 4821 current loss 0.023499, current_train_items 154304.
I0304 19:30:26.971006 23118544486528 run.py:483] Algo bellman_ford step 4822 current loss 0.052490, current_train_items 154336.
I0304 19:30:27.002118 23118544486528 run.py:483] Algo bellman_ford step 4823 current loss 0.057444, current_train_items 154368.
I0304 19:30:27.037110 23118544486528 run.py:483] Algo bellman_ford step 4824 current loss 0.065670, current_train_items 154400.
I0304 19:30:27.057167 23118544486528 run.py:483] Algo bellman_ford step 4825 current loss 0.012633, current_train_items 154432.
I0304 19:30:27.074152 23118544486528 run.py:483] Algo bellman_ford step 4826 current loss 0.016791, current_train_items 154464.
I0304 19:30:27.097661 23118544486528 run.py:483] Algo bellman_ford step 4827 current loss 0.042564, current_train_items 154496.
I0304 19:30:27.129502 23118544486528 run.py:483] Algo bellman_ford step 4828 current loss 0.068917, current_train_items 154528.
I0304 19:30:27.164192 23118544486528 run.py:483] Algo bellman_ford step 4829 current loss 0.083438, current_train_items 154560.
I0304 19:30:27.183799 23118544486528 run.py:483] Algo bellman_ford step 4830 current loss 0.003385, current_train_items 154592.
I0304 19:30:27.200271 23118544486528 run.py:483] Algo bellman_ford step 4831 current loss 0.028636, current_train_items 154624.
I0304 19:30:27.224968 23118544486528 run.py:483] Algo bellman_ford step 4832 current loss 0.029709, current_train_items 154656.
I0304 19:30:27.256922 23118544486528 run.py:483] Algo bellman_ford step 4833 current loss 0.077547, current_train_items 154688.
I0304 19:30:27.290944 23118544486528 run.py:483] Algo bellman_ford step 4834 current loss 0.070830, current_train_items 154720.
I0304 19:30:27.310279 23118544486528 run.py:483] Algo bellman_ford step 4835 current loss 0.003398, current_train_items 154752.
I0304 19:30:27.326577 23118544486528 run.py:483] Algo bellman_ford step 4836 current loss 0.015212, current_train_items 154784.
I0304 19:30:27.349677 23118544486528 run.py:483] Algo bellman_ford step 4837 current loss 0.059085, current_train_items 154816.
I0304 19:30:27.381092 23118544486528 run.py:483] Algo bellman_ford step 4838 current loss 0.120157, current_train_items 154848.
I0304 19:30:27.413005 23118544486528 run.py:483] Algo bellman_ford step 4839 current loss 0.068468, current_train_items 154880.
I0304 19:30:27.432975 23118544486528 run.py:483] Algo bellman_ford step 4840 current loss 0.006964, current_train_items 154912.
I0304 19:30:27.449166 23118544486528 run.py:483] Algo bellman_ford step 4841 current loss 0.032409, current_train_items 154944.
I0304 19:30:27.473104 23118544486528 run.py:483] Algo bellman_ford step 4842 current loss 0.048840, current_train_items 154976.
I0304 19:30:27.504695 23118544486528 run.py:483] Algo bellman_ford step 4843 current loss 0.119861, current_train_items 155008.
I0304 19:30:27.540470 23118544486528 run.py:483] Algo bellman_ford step 4844 current loss 0.087719, current_train_items 155040.
I0304 19:30:27.560221 23118544486528 run.py:483] Algo bellman_ford step 4845 current loss 0.007072, current_train_items 155072.
I0304 19:30:27.576890 23118544486528 run.py:483] Algo bellman_ford step 4846 current loss 0.015701, current_train_items 155104.
I0304 19:30:27.600500 23118544486528 run.py:483] Algo bellman_ford step 4847 current loss 0.037071, current_train_items 155136.
I0304 19:30:27.631848 23118544486528 run.py:483] Algo bellman_ford step 4848 current loss 0.057435, current_train_items 155168.
I0304 19:30:27.665694 23118544486528 run.py:483] Algo bellman_ford step 4849 current loss 0.088613, current_train_items 155200.
I0304 19:30:27.685639 23118544486528 run.py:483] Algo bellman_ford step 4850 current loss 0.004868, current_train_items 155232.
I0304 19:30:27.693485 23118544486528 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0304 19:30:27.693591 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:27.710886 23118544486528 run.py:483] Algo bellman_ford step 4851 current loss 0.034290, current_train_items 155264.
I0304 19:30:27.734653 23118544486528 run.py:483] Algo bellman_ford step 4852 current loss 0.061849, current_train_items 155296.
I0304 19:30:27.765964 23118544486528 run.py:483] Algo bellman_ford step 4853 current loss 0.078413, current_train_items 155328.
I0304 19:30:27.800220 23118544486528 run.py:483] Algo bellman_ford step 4854 current loss 0.068049, current_train_items 155360.
I0304 19:30:27.820083 23118544486528 run.py:483] Algo bellman_ford step 4855 current loss 0.005330, current_train_items 155392.
I0304 19:30:27.836709 23118544486528 run.py:483] Algo bellman_ford step 4856 current loss 0.035322, current_train_items 155424.
I0304 19:30:27.862177 23118544486528 run.py:483] Algo bellman_ford step 4857 current loss 0.054470, current_train_items 155456.
I0304 19:30:27.895075 23118544486528 run.py:483] Algo bellman_ford step 4858 current loss 0.065012, current_train_items 155488.
I0304 19:30:27.931507 23118544486528 run.py:483] Algo bellman_ford step 4859 current loss 0.105226, current_train_items 155520.
I0304 19:30:27.951240 23118544486528 run.py:483] Algo bellman_ford step 4860 current loss 0.009235, current_train_items 155552.
I0304 19:30:27.968538 23118544486528 run.py:483] Algo bellman_ford step 4861 current loss 0.026832, current_train_items 155584.
I0304 19:30:27.992427 23118544486528 run.py:483] Algo bellman_ford step 4862 current loss 0.050034, current_train_items 155616.
I0304 19:30:28.024740 23118544486528 run.py:483] Algo bellman_ford step 4863 current loss 0.055448, current_train_items 155648.
I0304 19:30:28.057890 23118544486528 run.py:483] Algo bellman_ford step 4864 current loss 0.041469, current_train_items 155680.
I0304 19:30:28.077545 23118544486528 run.py:483] Algo bellman_ford step 4865 current loss 0.004003, current_train_items 155712.
I0304 19:30:28.093652 23118544486528 run.py:483] Algo bellman_ford step 4866 current loss 0.043651, current_train_items 155744.
I0304 19:30:28.118359 23118544486528 run.py:483] Algo bellman_ford step 4867 current loss 0.043473, current_train_items 155776.
I0304 19:30:28.151356 23118544486528 run.py:483] Algo bellman_ford step 4868 current loss 0.087883, current_train_items 155808.
I0304 19:30:28.184522 23118544486528 run.py:483] Algo bellman_ford step 4869 current loss 0.040082, current_train_items 155840.
I0304 19:30:28.205078 23118544486528 run.py:483] Algo bellman_ford step 4870 current loss 0.005124, current_train_items 155872.
I0304 19:30:28.221765 23118544486528 run.py:483] Algo bellman_ford step 4871 current loss 0.092941, current_train_items 155904.
I0304 19:30:28.245628 23118544486528 run.py:483] Algo bellman_ford step 4872 current loss 0.044870, current_train_items 155936.
I0304 19:30:28.275284 23118544486528 run.py:483] Algo bellman_ford step 4873 current loss 0.062417, current_train_items 155968.
I0304 19:30:28.308148 23118544486528 run.py:483] Algo bellman_ford step 4874 current loss 0.076842, current_train_items 156000.
I0304 19:30:28.328341 23118544486528 run.py:483] Algo bellman_ford step 4875 current loss 0.005698, current_train_items 156032.
I0304 19:30:28.344717 23118544486528 run.py:483] Algo bellman_ford step 4876 current loss 0.018194, current_train_items 156064.
I0304 19:30:28.367851 23118544486528 run.py:483] Algo bellman_ford step 4877 current loss 0.013352, current_train_items 156096.
I0304 19:30:28.398342 23118544486528 run.py:483] Algo bellman_ford step 4878 current loss 0.025613, current_train_items 156128.
I0304 19:30:28.432845 23118544486528 run.py:483] Algo bellman_ford step 4879 current loss 0.066856, current_train_items 156160.
I0304 19:30:28.452877 23118544486528 run.py:483] Algo bellman_ford step 4880 current loss 0.007666, current_train_items 156192.
I0304 19:30:28.469236 23118544486528 run.py:483] Algo bellman_ford step 4881 current loss 0.017822, current_train_items 156224.
I0304 19:30:28.493906 23118544486528 run.py:483] Algo bellman_ford step 4882 current loss 0.050091, current_train_items 156256.
I0304 19:30:28.525959 23118544486528 run.py:483] Algo bellman_ford step 4883 current loss 0.071187, current_train_items 156288.
I0304 19:30:28.560703 23118544486528 run.py:483] Algo bellman_ford step 4884 current loss 0.070068, current_train_items 156320.
I0304 19:30:28.580591 23118544486528 run.py:483] Algo bellman_ford step 4885 current loss 0.003350, current_train_items 156352.
I0304 19:30:28.597376 23118544486528 run.py:483] Algo bellman_ford step 4886 current loss 0.023495, current_train_items 156384.
I0304 19:30:28.621397 23118544486528 run.py:483] Algo bellman_ford step 4887 current loss 0.036325, current_train_items 156416.
I0304 19:30:28.652888 23118544486528 run.py:483] Algo bellman_ford step 4888 current loss 0.028821, current_train_items 156448.
I0304 19:30:28.686515 23118544486528 run.py:483] Algo bellman_ford step 4889 current loss 0.049693, current_train_items 156480.
I0304 19:30:28.706590 23118544486528 run.py:483] Algo bellman_ford step 4890 current loss 0.004702, current_train_items 156512.
I0304 19:30:28.722931 23118544486528 run.py:483] Algo bellman_ford step 4891 current loss 0.039148, current_train_items 156544.
I0304 19:30:28.746623 23118544486528 run.py:483] Algo bellman_ford step 4892 current loss 0.024975, current_train_items 156576.
I0304 19:30:28.777142 23118544486528 run.py:483] Algo bellman_ford step 4893 current loss 0.070438, current_train_items 156608.
I0304 19:30:28.810081 23118544486528 run.py:483] Algo bellman_ford step 4894 current loss 0.089481, current_train_items 156640.
I0304 19:30:28.829805 23118544486528 run.py:483] Algo bellman_ford step 4895 current loss 0.045166, current_train_items 156672.
I0304 19:30:28.846360 23118544486528 run.py:483] Algo bellman_ford step 4896 current loss 0.011214, current_train_items 156704.
I0304 19:30:28.869593 23118544486528 run.py:483] Algo bellman_ford step 4897 current loss 0.022246, current_train_items 156736.
I0304 19:30:28.900626 23118544486528 run.py:483] Algo bellman_ford step 4898 current loss 0.079497, current_train_items 156768.
I0304 19:30:28.935543 23118544486528 run.py:483] Algo bellman_ford step 4899 current loss 0.142895, current_train_items 156800.
I0304 19:30:28.955401 23118544486528 run.py:483] Algo bellman_ford step 4900 current loss 0.008483, current_train_items 156832.
I0304 19:30:28.962989 23118544486528 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0304 19:30:28.963094 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:30:28.979662 23118544486528 run.py:483] Algo bellman_ford step 4901 current loss 0.026427, current_train_items 156864.
I0304 19:30:29.004819 23118544486528 run.py:483] Algo bellman_ford step 4902 current loss 0.055007, current_train_items 156896.
I0304 19:30:29.038709 23118544486528 run.py:483] Algo bellman_ford step 4903 current loss 0.048562, current_train_items 156928.
I0304 19:30:29.072279 23118544486528 run.py:483] Algo bellman_ford step 4904 current loss 0.053457, current_train_items 156960.
I0304 19:30:29.092454 23118544486528 run.py:483] Algo bellman_ford step 4905 current loss 0.012994, current_train_items 156992.
I0304 19:30:29.108470 23118544486528 run.py:483] Algo bellman_ford step 4906 current loss 0.037591, current_train_items 157024.
I0304 19:30:29.133010 23118544486528 run.py:483] Algo bellman_ford step 4907 current loss 0.056203, current_train_items 157056.
I0304 19:30:29.164994 23118544486528 run.py:483] Algo bellman_ford step 4908 current loss 0.046513, current_train_items 157088.
I0304 19:30:29.198253 23118544486528 run.py:483] Algo bellman_ford step 4909 current loss 0.056172, current_train_items 157120.
I0304 19:30:29.218014 23118544486528 run.py:483] Algo bellman_ford step 4910 current loss 0.005203, current_train_items 157152.
I0304 19:30:29.234559 23118544486528 run.py:483] Algo bellman_ford step 4911 current loss 0.025401, current_train_items 157184.
I0304 19:30:29.258702 23118544486528 run.py:483] Algo bellman_ford step 4912 current loss 0.042137, current_train_items 157216.
I0304 19:30:29.291378 23118544486528 run.py:483] Algo bellman_ford step 4913 current loss 0.126939, current_train_items 157248.
I0304 19:30:29.325026 23118544486528 run.py:483] Algo bellman_ford step 4914 current loss 0.075085, current_train_items 157280.
I0304 19:30:29.344630 23118544486528 run.py:483] Algo bellman_ford step 4915 current loss 0.010733, current_train_items 157312.
I0304 19:30:29.360706 23118544486528 run.py:483] Algo bellman_ford step 4916 current loss 0.028775, current_train_items 157344.
I0304 19:30:29.385025 23118544486528 run.py:483] Algo bellman_ford step 4917 current loss 0.043245, current_train_items 157376.
I0304 19:30:29.415523 23118544486528 run.py:483] Algo bellman_ford step 4918 current loss 0.043994, current_train_items 157408.
I0304 19:30:29.449584 23118544486528 run.py:483] Algo bellman_ford step 4919 current loss 0.063136, current_train_items 157440.
I0304 19:30:29.469534 23118544486528 run.py:483] Algo bellman_ford step 4920 current loss 0.004700, current_train_items 157472.
I0304 19:30:29.485241 23118544486528 run.py:483] Algo bellman_ford step 4921 current loss 0.010827, current_train_items 157504.
I0304 19:30:29.510051 23118544486528 run.py:483] Algo bellman_ford step 4922 current loss 0.064789, current_train_items 157536.
I0304 19:30:29.541431 23118544486528 run.py:483] Algo bellman_ford step 4923 current loss 0.050465, current_train_items 157568.
I0304 19:30:29.575505 23118544486528 run.py:483] Algo bellman_ford step 4924 current loss 0.099315, current_train_items 157600.
I0304 19:30:29.595474 23118544486528 run.py:483] Algo bellman_ford step 4925 current loss 0.015147, current_train_items 157632.
I0304 19:30:29.611947 23118544486528 run.py:483] Algo bellman_ford step 4926 current loss 0.017117, current_train_items 157664.
I0304 19:30:29.636737 23118544486528 run.py:483] Algo bellman_ford step 4927 current loss 0.047975, current_train_items 157696.
I0304 19:30:29.667059 23118544486528 run.py:483] Algo bellman_ford step 4928 current loss 0.070018, current_train_items 157728.
I0304 19:30:29.701159 23118544486528 run.py:483] Algo bellman_ford step 4929 current loss 0.063955, current_train_items 157760.
I0304 19:30:29.721369 23118544486528 run.py:483] Algo bellman_ford step 4930 current loss 0.005133, current_train_items 157792.
I0304 19:30:29.737421 23118544486528 run.py:483] Algo bellman_ford step 4931 current loss 0.010710, current_train_items 157824.
I0304 19:30:29.761566 23118544486528 run.py:483] Algo bellman_ford step 4932 current loss 0.025147, current_train_items 157856.
I0304 19:30:29.792815 23118544486528 run.py:483] Algo bellman_ford step 4933 current loss 0.061201, current_train_items 157888.
I0304 19:30:29.826728 23118544486528 run.py:483] Algo bellman_ford step 4934 current loss 0.092696, current_train_items 157920.
I0304 19:30:29.846659 23118544486528 run.py:483] Algo bellman_ford step 4935 current loss 0.010144, current_train_items 157952.
I0304 19:30:29.863043 23118544486528 run.py:483] Algo bellman_ford step 4936 current loss 0.010625, current_train_items 157984.
I0304 19:30:29.889051 23118544486528 run.py:483] Algo bellman_ford step 4937 current loss 0.033470, current_train_items 158016.
I0304 19:30:29.920289 23118544486528 run.py:483] Algo bellman_ford step 4938 current loss 0.038612, current_train_items 158048.
I0304 19:30:29.956445 23118544486528 run.py:483] Algo bellman_ford step 4939 current loss 0.054983, current_train_items 158080.
I0304 19:30:29.975923 23118544486528 run.py:483] Algo bellman_ford step 4940 current loss 0.003346, current_train_items 158112.
I0304 19:30:29.991941 23118544486528 run.py:483] Algo bellman_ford step 4941 current loss 0.012914, current_train_items 158144.
I0304 19:30:30.016063 23118544486528 run.py:483] Algo bellman_ford step 4942 current loss 0.045871, current_train_items 158176.
I0304 19:30:30.048838 23118544486528 run.py:483] Algo bellman_ford step 4943 current loss 0.061533, current_train_items 158208.
I0304 19:30:30.081037 23118544486528 run.py:483] Algo bellman_ford step 4944 current loss 0.046888, current_train_items 158240.
I0304 19:30:30.100634 23118544486528 run.py:483] Algo bellman_ford step 4945 current loss 0.006851, current_train_items 158272.
I0304 19:30:30.116992 23118544486528 run.py:483] Algo bellman_ford step 4946 current loss 0.027074, current_train_items 158304.
I0304 19:30:30.140722 23118544486528 run.py:483] Algo bellman_ford step 4947 current loss 0.055311, current_train_items 158336.
I0304 19:30:30.172976 23118544486528 run.py:483] Algo bellman_ford step 4948 current loss 0.066759, current_train_items 158368.
I0304 19:30:30.208370 23118544486528 run.py:483] Algo bellman_ford step 4949 current loss 0.048301, current_train_items 158400.
I0304 19:30:30.228067 23118544486528 run.py:483] Algo bellman_ford step 4950 current loss 0.004347, current_train_items 158432.
I0304 19:30:30.236055 23118544486528 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0304 19:30:30.236159 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:30.252729 23118544486528 run.py:483] Algo bellman_ford step 4951 current loss 0.006133, current_train_items 158464.
I0304 19:30:30.276737 23118544486528 run.py:483] Algo bellman_ford step 4952 current loss 0.068244, current_train_items 158496.
I0304 19:30:30.308788 23118544486528 run.py:483] Algo bellman_ford step 4953 current loss 0.062032, current_train_items 158528.
I0304 19:30:30.345347 23118544486528 run.py:483] Algo bellman_ford step 4954 current loss 0.048759, current_train_items 158560.
I0304 19:30:30.365824 23118544486528 run.py:483] Algo bellman_ford step 4955 current loss 0.009264, current_train_items 158592.
I0304 19:30:30.382340 23118544486528 run.py:483] Algo bellman_ford step 4956 current loss 0.010847, current_train_items 158624.
I0304 19:30:30.407960 23118544486528 run.py:483] Algo bellman_ford step 4957 current loss 0.074702, current_train_items 158656.
I0304 19:30:30.440104 23118544486528 run.py:483] Algo bellman_ford step 4958 current loss 0.129357, current_train_items 158688.
I0304 19:30:30.471761 23118544486528 run.py:483] Algo bellman_ford step 4959 current loss 0.082201, current_train_items 158720.
I0304 19:30:30.491796 23118544486528 run.py:483] Algo bellman_ford step 4960 current loss 0.014487, current_train_items 158752.
I0304 19:30:30.508406 23118544486528 run.py:483] Algo bellman_ford step 4961 current loss 0.006980, current_train_items 158784.
I0304 19:30:30.532499 23118544486528 run.py:483] Algo bellman_ford step 4962 current loss 0.056509, current_train_items 158816.
I0304 19:30:30.562849 23118544486528 run.py:483] Algo bellman_ford step 4963 current loss 0.065138, current_train_items 158848.
I0304 19:30:30.596794 23118544486528 run.py:483] Algo bellman_ford step 4964 current loss 0.072677, current_train_items 158880.
I0304 19:30:30.616147 23118544486528 run.py:483] Algo bellman_ford step 4965 current loss 0.002907, current_train_items 158912.
I0304 19:30:30.632624 23118544486528 run.py:483] Algo bellman_ford step 4966 current loss 0.028830, current_train_items 158944.
I0304 19:30:30.657641 23118544486528 run.py:483] Algo bellman_ford step 4967 current loss 0.068055, current_train_items 158976.
I0304 19:30:30.690029 23118544486528 run.py:483] Algo bellman_ford step 4968 current loss 0.053960, current_train_items 159008.
I0304 19:30:30.722260 23118544486528 run.py:483] Algo bellman_ford step 4969 current loss 0.101183, current_train_items 159040.
I0304 19:30:30.742823 23118544486528 run.py:483] Algo bellman_ford step 4970 current loss 0.005385, current_train_items 159072.
I0304 19:30:30.759088 23118544486528 run.py:483] Algo bellman_ford step 4971 current loss 0.075547, current_train_items 159104.
I0304 19:30:30.783245 23118544486528 run.py:483] Algo bellman_ford step 4972 current loss 0.067646, current_train_items 159136.
I0304 19:30:30.815566 23118544486528 run.py:483] Algo bellman_ford step 4973 current loss 0.103680, current_train_items 159168.
I0304 19:30:30.849522 23118544486528 run.py:483] Algo bellman_ford step 4974 current loss 0.124428, current_train_items 159200.
I0304 19:30:30.869755 23118544486528 run.py:483] Algo bellman_ford step 4975 current loss 0.005746, current_train_items 159232.
I0304 19:30:30.886639 23118544486528 run.py:483] Algo bellman_ford step 4976 current loss 0.015863, current_train_items 159264.
I0304 19:30:30.911480 23118544486528 run.py:483] Algo bellman_ford step 4977 current loss 0.091413, current_train_items 159296.
I0304 19:30:30.944015 23118544486528 run.py:483] Algo bellman_ford step 4978 current loss 0.136264, current_train_items 159328.
I0304 19:30:30.979118 23118544486528 run.py:483] Algo bellman_ford step 4979 current loss 0.080241, current_train_items 159360.
I0304 19:30:30.998848 23118544486528 run.py:483] Algo bellman_ford step 4980 current loss 0.021069, current_train_items 159392.
I0304 19:30:31.015345 23118544486528 run.py:483] Algo bellman_ford step 4981 current loss 0.014944, current_train_items 159424.
I0304 19:30:31.040890 23118544486528 run.py:483] Algo bellman_ford step 4982 current loss 0.069653, current_train_items 159456.
I0304 19:30:31.070616 23118544486528 run.py:483] Algo bellman_ford step 4983 current loss 0.041989, current_train_items 159488.
I0304 19:30:31.104354 23118544486528 run.py:483] Algo bellman_ford step 4984 current loss 0.062024, current_train_items 159520.
I0304 19:30:31.124418 23118544486528 run.py:483] Algo bellman_ford step 4985 current loss 0.007422, current_train_items 159552.
I0304 19:30:31.140983 23118544486528 run.py:483] Algo bellman_ford step 4986 current loss 0.040644, current_train_items 159584.
I0304 19:30:31.163902 23118544486528 run.py:483] Algo bellman_ford step 4987 current loss 0.084537, current_train_items 159616.
I0304 19:30:31.194771 23118544486528 run.py:483] Algo bellman_ford step 4988 current loss 0.063773, current_train_items 159648.
I0304 19:30:31.231422 23118544486528 run.py:483] Algo bellman_ford step 4989 current loss 0.105849, current_train_items 159680.
I0304 19:30:31.251297 23118544486528 run.py:483] Algo bellman_ford step 4990 current loss 0.012965, current_train_items 159712.
I0304 19:30:31.267520 23118544486528 run.py:483] Algo bellman_ford step 4991 current loss 0.010423, current_train_items 159744.
I0304 19:30:31.292017 23118544486528 run.py:483] Algo bellman_ford step 4992 current loss 0.116479, current_train_items 159776.
I0304 19:30:31.323205 23118544486528 run.py:483] Algo bellman_ford step 4993 current loss 0.133490, current_train_items 159808.
I0304 19:30:31.357659 23118544486528 run.py:483] Algo bellman_ford step 4994 current loss 0.172263, current_train_items 159840.
I0304 19:30:31.377341 23118544486528 run.py:483] Algo bellman_ford step 4995 current loss 0.007937, current_train_items 159872.
I0304 19:30:31.394123 23118544486528 run.py:483] Algo bellman_ford step 4996 current loss 0.010433, current_train_items 159904.
I0304 19:30:31.418271 23118544486528 run.py:483] Algo bellman_ford step 4997 current loss 0.033140, current_train_items 159936.
I0304 19:30:31.450355 23118544486528 run.py:483] Algo bellman_ford step 4998 current loss 0.082051, current_train_items 159968.
I0304 19:30:31.485296 23118544486528 run.py:483] Algo bellman_ford step 4999 current loss 0.097580, current_train_items 160000.
I0304 19:30:31.505494 23118544486528 run.py:483] Algo bellman_ford step 5000 current loss 0.008730, current_train_items 160032.
I0304 19:30:31.513136 23118544486528 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0304 19:30:31.513241 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:31.530592 23118544486528 run.py:483] Algo bellman_ford step 5001 current loss 0.051365, current_train_items 160064.
I0304 19:30:31.554931 23118544486528 run.py:483] Algo bellman_ford step 5002 current loss 0.069647, current_train_items 160096.
I0304 19:30:31.585345 23118544486528 run.py:483] Algo bellman_ford step 5003 current loss 0.026819, current_train_items 160128.
I0304 19:30:31.619032 23118544486528 run.py:483] Algo bellman_ford step 5004 current loss 0.071363, current_train_items 160160.
I0304 19:30:31.639121 23118544486528 run.py:483] Algo bellman_ford step 5005 current loss 0.006456, current_train_items 160192.
I0304 19:30:31.655343 23118544486528 run.py:483] Algo bellman_ford step 5006 current loss 0.013569, current_train_items 160224.
I0304 19:30:31.679954 23118544486528 run.py:483] Algo bellman_ford step 5007 current loss 0.057445, current_train_items 160256.
I0304 19:30:31.713713 23118544486528 run.py:483] Algo bellman_ford step 5008 current loss 0.070428, current_train_items 160288.
I0304 19:30:31.746152 23118544486528 run.py:483] Algo bellman_ford step 5009 current loss 0.051683, current_train_items 160320.
I0304 19:30:31.766039 23118544486528 run.py:483] Algo bellman_ford step 5010 current loss 0.008033, current_train_items 160352.
I0304 19:30:31.782086 23118544486528 run.py:483] Algo bellman_ford step 5011 current loss 0.018320, current_train_items 160384.
I0304 19:30:31.806834 23118544486528 run.py:483] Algo bellman_ford step 5012 current loss 0.040616, current_train_items 160416.
I0304 19:30:31.837780 23118544486528 run.py:483] Algo bellman_ford step 5013 current loss 0.065130, current_train_items 160448.
I0304 19:30:31.872845 23118544486528 run.py:483] Algo bellman_ford step 5014 current loss 0.083895, current_train_items 160480.
I0304 19:30:31.892922 23118544486528 run.py:483] Algo bellman_ford step 5015 current loss 0.030248, current_train_items 160512.
I0304 19:30:31.909936 23118544486528 run.py:483] Algo bellman_ford step 5016 current loss 0.078938, current_train_items 160544.
I0304 19:30:31.933650 23118544486528 run.py:483] Algo bellman_ford step 5017 current loss 0.034132, current_train_items 160576.
I0304 19:30:31.965725 23118544486528 run.py:483] Algo bellman_ford step 5018 current loss 0.138210, current_train_items 160608.
I0304 19:30:32.001850 23118544486528 run.py:483] Algo bellman_ford step 5019 current loss 0.172230, current_train_items 160640.
I0304 19:30:32.021861 23118544486528 run.py:483] Algo bellman_ford step 5020 current loss 0.003703, current_train_items 160672.
I0304 19:30:32.037992 23118544486528 run.py:483] Algo bellman_ford step 5021 current loss 0.011587, current_train_items 160704.
I0304 19:30:32.063153 23118544486528 run.py:483] Algo bellman_ford step 5022 current loss 0.041816, current_train_items 160736.
I0304 19:30:32.095104 23118544486528 run.py:483] Algo bellman_ford step 5023 current loss 0.099572, current_train_items 160768.
I0304 19:30:32.129848 23118544486528 run.py:483] Algo bellman_ford step 5024 current loss 0.050638, current_train_items 160800.
I0304 19:30:32.149541 23118544486528 run.py:483] Algo bellman_ford step 5025 current loss 0.004659, current_train_items 160832.
I0304 19:30:32.165520 23118544486528 run.py:483] Algo bellman_ford step 5026 current loss 0.012411, current_train_items 160864.
I0304 19:30:32.188777 23118544486528 run.py:483] Algo bellman_ford step 5027 current loss 0.059357, current_train_items 160896.
I0304 19:30:32.219296 23118544486528 run.py:483] Algo bellman_ford step 5028 current loss 0.067138, current_train_items 160928.
I0304 19:30:32.253314 23118544486528 run.py:483] Algo bellman_ford step 5029 current loss 0.071921, current_train_items 160960.
I0304 19:30:32.273559 23118544486528 run.py:483] Algo bellman_ford step 5030 current loss 0.022303, current_train_items 160992.
I0304 19:30:32.290018 23118544486528 run.py:483] Algo bellman_ford step 5031 current loss 0.024828, current_train_items 161024.
I0304 19:30:32.314911 23118544486528 run.py:483] Algo bellman_ford step 5032 current loss 0.044125, current_train_items 161056.
I0304 19:30:32.345131 23118544486528 run.py:483] Algo bellman_ford step 5033 current loss 0.045246, current_train_items 161088.
I0304 19:30:32.380006 23118544486528 run.py:483] Algo bellman_ford step 5034 current loss 0.129423, current_train_items 161120.
I0304 19:30:32.399926 23118544486528 run.py:483] Algo bellman_ford step 5035 current loss 0.016869, current_train_items 161152.
I0304 19:30:32.416489 23118544486528 run.py:483] Algo bellman_ford step 5036 current loss 0.007175, current_train_items 161184.
I0304 19:30:32.440902 23118544486528 run.py:483] Algo bellman_ford step 5037 current loss 0.055336, current_train_items 161216.
I0304 19:30:32.473861 23118544486528 run.py:483] Algo bellman_ford step 5038 current loss 0.096330, current_train_items 161248.
I0304 19:30:32.508131 23118544486528 run.py:483] Algo bellman_ford step 5039 current loss 0.050349, current_train_items 161280.
I0304 19:30:32.528176 23118544486528 run.py:483] Algo bellman_ford step 5040 current loss 0.004058, current_train_items 161312.
I0304 19:30:32.543941 23118544486528 run.py:483] Algo bellman_ford step 5041 current loss 0.004449, current_train_items 161344.
I0304 19:30:32.569056 23118544486528 run.py:483] Algo bellman_ford step 5042 current loss 0.077588, current_train_items 161376.
I0304 19:30:32.599678 23118544486528 run.py:483] Algo bellman_ford step 5043 current loss 0.044127, current_train_items 161408.
I0304 19:30:32.633370 23118544486528 run.py:483] Algo bellman_ford step 5044 current loss 0.068295, current_train_items 161440.
I0304 19:30:32.653113 23118544486528 run.py:483] Algo bellman_ford step 5045 current loss 0.005401, current_train_items 161472.
I0304 19:30:32.669466 23118544486528 run.py:483] Algo bellman_ford step 5046 current loss 0.043664, current_train_items 161504.
I0304 19:30:32.692576 23118544486528 run.py:483] Algo bellman_ford step 5047 current loss 0.062186, current_train_items 161536.
I0304 19:30:32.723970 23118544486528 run.py:483] Algo bellman_ford step 5048 current loss 0.096829, current_train_items 161568.
I0304 19:30:32.758624 23118544486528 run.py:483] Algo bellman_ford step 5049 current loss 0.122688, current_train_items 161600.
I0304 19:30:32.778388 23118544486528 run.py:483] Algo bellman_ford step 5050 current loss 0.004010, current_train_items 161632.
I0304 19:30:32.786350 23118544486528 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0304 19:30:32.786455 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:32.803310 23118544486528 run.py:483] Algo bellman_ford step 5051 current loss 0.010787, current_train_items 161664.
I0304 19:30:32.828831 23118544486528 run.py:483] Algo bellman_ford step 5052 current loss 0.145088, current_train_items 161696.
I0304 19:30:32.861080 23118544486528 run.py:483] Algo bellman_ford step 5053 current loss 0.172538, current_train_items 161728.
I0304 19:30:32.893925 23118544486528 run.py:483] Algo bellman_ford step 5054 current loss 0.152890, current_train_items 161760.
I0304 19:30:32.914148 23118544486528 run.py:483] Algo bellman_ford step 5055 current loss 0.005862, current_train_items 161792.
I0304 19:30:32.931607 23118544486528 run.py:483] Algo bellman_ford step 5056 current loss 0.035468, current_train_items 161824.
I0304 19:30:32.957127 23118544486528 run.py:483] Algo bellman_ford step 5057 current loss 0.043637, current_train_items 161856.
I0304 19:30:32.988281 23118544486528 run.py:483] Algo bellman_ford step 5058 current loss 0.079504, current_train_items 161888.
I0304 19:30:33.022832 23118544486528 run.py:483] Algo bellman_ford step 5059 current loss 0.059992, current_train_items 161920.
I0304 19:30:33.043475 23118544486528 run.py:483] Algo bellman_ford step 5060 current loss 0.057542, current_train_items 161952.
I0304 19:30:33.059892 23118544486528 run.py:483] Algo bellman_ford step 5061 current loss 0.028577, current_train_items 161984.
I0304 19:30:33.082286 23118544486528 run.py:483] Algo bellman_ford step 5062 current loss 0.017697, current_train_items 162016.
I0304 19:30:33.113437 23118544486528 run.py:483] Algo bellman_ford step 5063 current loss 0.081171, current_train_items 162048.
I0304 19:30:33.146021 23118544486528 run.py:483] Algo bellman_ford step 5064 current loss 0.081368, current_train_items 162080.
I0304 19:30:33.165728 23118544486528 run.py:483] Algo bellman_ford step 5065 current loss 0.006317, current_train_items 162112.
I0304 19:30:33.182108 23118544486528 run.py:483] Algo bellman_ford step 5066 current loss 0.021866, current_train_items 162144.
I0304 19:30:33.207168 23118544486528 run.py:483] Algo bellman_ford step 5067 current loss 0.059200, current_train_items 162176.
I0304 19:30:33.238743 23118544486528 run.py:483] Algo bellman_ford step 5068 current loss 0.093536, current_train_items 162208.
I0304 19:30:33.274012 23118544486528 run.py:483] Algo bellman_ford step 5069 current loss 0.104686, current_train_items 162240.
I0304 19:30:33.293934 23118544486528 run.py:483] Algo bellman_ford step 5070 current loss 0.005335, current_train_items 162272.
I0304 19:30:33.311040 23118544486528 run.py:483] Algo bellman_ford step 5071 current loss 0.023830, current_train_items 162304.
I0304 19:30:33.333434 23118544486528 run.py:483] Algo bellman_ford step 5072 current loss 0.044488, current_train_items 162336.
I0304 19:30:33.365632 23118544486528 run.py:483] Algo bellman_ford step 5073 current loss 0.103504, current_train_items 162368.
I0304 19:30:33.397122 23118544486528 run.py:483] Algo bellman_ford step 5074 current loss 0.058699, current_train_items 162400.
I0304 19:30:33.417195 23118544486528 run.py:483] Algo bellman_ford step 5075 current loss 0.004691, current_train_items 162432.
I0304 19:30:33.433563 23118544486528 run.py:483] Algo bellman_ford step 5076 current loss 0.026890, current_train_items 162464.
I0304 19:30:33.457678 23118544486528 run.py:483] Algo bellman_ford step 5077 current loss 0.031237, current_train_items 162496.
I0304 19:30:33.489175 23118544486528 run.py:483] Algo bellman_ford step 5078 current loss 0.087586, current_train_items 162528.
I0304 19:30:33.522323 23118544486528 run.py:483] Algo bellman_ford step 5079 current loss 0.095647, current_train_items 162560.
I0304 19:30:33.542162 23118544486528 run.py:483] Algo bellman_ford step 5080 current loss 0.008858, current_train_items 162592.
I0304 19:30:33.558525 23118544486528 run.py:483] Algo bellman_ford step 5081 current loss 0.011951, current_train_items 162624.
I0304 19:30:33.582616 23118544486528 run.py:483] Algo bellman_ford step 5082 current loss 0.029724, current_train_items 162656.
I0304 19:30:33.614492 23118544486528 run.py:483] Algo bellman_ford step 5083 current loss 0.057258, current_train_items 162688.
I0304 19:30:33.648629 23118544486528 run.py:483] Algo bellman_ford step 5084 current loss 0.055243, current_train_items 162720.
I0304 19:30:33.668813 23118544486528 run.py:483] Algo bellman_ford step 5085 current loss 0.005556, current_train_items 162752.
I0304 19:30:33.685093 23118544486528 run.py:483] Algo bellman_ford step 5086 current loss 0.034577, current_train_items 162784.
I0304 19:30:33.709548 23118544486528 run.py:483] Algo bellman_ford step 5087 current loss 0.054624, current_train_items 162816.
I0304 19:30:33.741368 23118544486528 run.py:483] Algo bellman_ford step 5088 current loss 0.049656, current_train_items 162848.
I0304 19:30:33.776549 23118544486528 run.py:483] Algo bellman_ford step 5089 current loss 0.063348, current_train_items 162880.
I0304 19:30:33.796381 23118544486528 run.py:483] Algo bellman_ford step 5090 current loss 0.006465, current_train_items 162912.
I0304 19:30:33.813102 23118544486528 run.py:483] Algo bellman_ford step 5091 current loss 0.027452, current_train_items 162944.
I0304 19:30:33.835982 23118544486528 run.py:483] Algo bellman_ford step 5092 current loss 0.030862, current_train_items 162976.
I0304 19:30:33.866504 23118544486528 run.py:483] Algo bellman_ford step 5093 current loss 0.046173, current_train_items 163008.
I0304 19:30:33.899698 23118544486528 run.py:483] Algo bellman_ford step 5094 current loss 0.056216, current_train_items 163040.
I0304 19:30:33.919443 23118544486528 run.py:483] Algo bellman_ford step 5095 current loss 0.005870, current_train_items 163072.
I0304 19:30:33.935869 23118544486528 run.py:483] Algo bellman_ford step 5096 current loss 0.019979, current_train_items 163104.
I0304 19:30:33.960978 23118544486528 run.py:483] Algo bellman_ford step 5097 current loss 0.072897, current_train_items 163136.
I0304 19:30:33.993404 23118544486528 run.py:483] Algo bellman_ford step 5098 current loss 0.053185, current_train_items 163168.
I0304 19:30:34.028973 23118544486528 run.py:483] Algo bellman_ford step 5099 current loss 0.080442, current_train_items 163200.
I0304 19:30:34.048894 23118544486528 run.py:483] Algo bellman_ford step 5100 current loss 0.008059, current_train_items 163232.
I0304 19:30:34.056388 23118544486528 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0304 19:30:34.056497 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:30:34.073202 23118544486528 run.py:483] Algo bellman_ford step 5101 current loss 0.063223, current_train_items 163264.
I0304 19:30:34.096065 23118544486528 run.py:483] Algo bellman_ford step 5102 current loss 0.016160, current_train_items 163296.
I0304 19:30:34.128545 23118544486528 run.py:483] Algo bellman_ford step 5103 current loss 0.048636, current_train_items 163328.
I0304 19:30:34.163516 23118544486528 run.py:483] Algo bellman_ford step 5104 current loss 0.085862, current_train_items 163360.
I0304 19:30:34.183614 23118544486528 run.py:483] Algo bellman_ford step 5105 current loss 0.005644, current_train_items 163392.
I0304 19:30:34.200218 23118544486528 run.py:483] Algo bellman_ford step 5106 current loss 0.036577, current_train_items 163424.
I0304 19:30:34.224233 23118544486528 run.py:483] Algo bellman_ford step 5107 current loss 0.054998, current_train_items 163456.
I0304 19:30:34.254512 23118544486528 run.py:483] Algo bellman_ford step 5108 current loss 0.030461, current_train_items 163488.
I0304 19:30:34.289527 23118544486528 run.py:483] Algo bellman_ford step 5109 current loss 0.075361, current_train_items 163520.
I0304 19:30:34.309401 23118544486528 run.py:483] Algo bellman_ford step 5110 current loss 0.004220, current_train_items 163552.
I0304 19:30:34.325808 23118544486528 run.py:483] Algo bellman_ford step 5111 current loss 0.015650, current_train_items 163584.
I0304 19:30:34.349765 23118544486528 run.py:483] Algo bellman_ford step 5112 current loss 0.020833, current_train_items 163616.
I0304 19:30:34.380498 23118544486528 run.py:483] Algo bellman_ford step 5113 current loss 0.050793, current_train_items 163648.
I0304 19:30:34.414209 23118544486528 run.py:483] Algo bellman_ford step 5114 current loss 0.063281, current_train_items 163680.
I0304 19:30:34.434357 23118544486528 run.py:483] Algo bellman_ford step 5115 current loss 0.005474, current_train_items 163712.
I0304 19:30:34.450966 23118544486528 run.py:483] Algo bellman_ford step 5116 current loss 0.023205, current_train_items 163744.
I0304 19:30:34.475814 23118544486528 run.py:483] Algo bellman_ford step 5117 current loss 0.082755, current_train_items 163776.
I0304 19:30:34.506464 23118544486528 run.py:483] Algo bellman_ford step 5118 current loss 0.060986, current_train_items 163808.
I0304 19:30:34.541191 23118544486528 run.py:483] Algo bellman_ford step 5119 current loss 0.070104, current_train_items 163840.
I0304 19:30:34.561085 23118544486528 run.py:483] Algo bellman_ford step 5120 current loss 0.007573, current_train_items 163872.
I0304 19:30:34.577073 23118544486528 run.py:483] Algo bellman_ford step 5121 current loss 0.014373, current_train_items 163904.
I0304 19:30:34.601229 23118544486528 run.py:483] Algo bellman_ford step 5122 current loss 0.046954, current_train_items 163936.
I0304 19:30:34.631568 23118544486528 run.py:483] Algo bellman_ford step 5123 current loss 0.029934, current_train_items 163968.
I0304 19:30:34.665697 23118544486528 run.py:483] Algo bellman_ford step 5124 current loss 0.075447, current_train_items 164000.
I0304 19:30:34.685464 23118544486528 run.py:483] Algo bellman_ford step 5125 current loss 0.003887, current_train_items 164032.
I0304 19:30:34.701791 23118544486528 run.py:483] Algo bellman_ford step 5126 current loss 0.054798, current_train_items 164064.
I0304 19:30:34.726023 23118544486528 run.py:483] Algo bellman_ford step 5127 current loss 0.064130, current_train_items 164096.
I0304 19:30:34.757729 23118544486528 run.py:483] Algo bellman_ford step 5128 current loss 0.042646, current_train_items 164128.
I0304 19:30:34.793396 23118544486528 run.py:483] Algo bellman_ford step 5129 current loss 0.059498, current_train_items 164160.
I0304 19:30:34.813032 23118544486528 run.py:483] Algo bellman_ford step 5130 current loss 0.002468, current_train_items 164192.
I0304 19:30:34.829588 23118544486528 run.py:483] Algo bellman_ford step 5131 current loss 0.019870, current_train_items 164224.
I0304 19:30:34.853219 23118544486528 run.py:483] Algo bellman_ford step 5132 current loss 0.117274, current_train_items 164256.
I0304 19:30:34.885260 23118544486528 run.py:483] Algo bellman_ford step 5133 current loss 0.058511, current_train_items 164288.
I0304 19:30:34.918942 23118544486528 run.py:483] Algo bellman_ford step 5134 current loss 0.048950, current_train_items 164320.
I0304 19:30:34.939035 23118544486528 run.py:483] Algo bellman_ford step 5135 current loss 0.011270, current_train_items 164352.
I0304 19:30:34.955639 23118544486528 run.py:483] Algo bellman_ford step 5136 current loss 0.015015, current_train_items 164384.
I0304 19:30:34.979644 23118544486528 run.py:483] Algo bellman_ford step 5137 current loss 0.053041, current_train_items 164416.
I0304 19:30:35.010942 23118544486528 run.py:483] Algo bellman_ford step 5138 current loss 0.062658, current_train_items 164448.
I0304 19:30:35.045264 23118544486528 run.py:483] Algo bellman_ford step 5139 current loss 0.041833, current_train_items 164480.
I0304 19:30:35.065682 23118544486528 run.py:483] Algo bellman_ford step 5140 current loss 0.003528, current_train_items 164512.
I0304 19:30:35.082496 23118544486528 run.py:483] Algo bellman_ford step 5141 current loss 0.017715, current_train_items 164544.
I0304 19:30:35.107547 23118544486528 run.py:483] Algo bellman_ford step 5142 current loss 0.062035, current_train_items 164576.
I0304 19:30:35.138937 23118544486528 run.py:483] Algo bellman_ford step 5143 current loss 0.077839, current_train_items 164608.
I0304 19:30:35.172737 23118544486528 run.py:483] Algo bellman_ford step 5144 current loss 0.077385, current_train_items 164640.
I0304 19:30:35.192830 23118544486528 run.py:483] Algo bellman_ford step 5145 current loss 0.007192, current_train_items 164672.
I0304 19:30:35.209291 23118544486528 run.py:483] Algo bellman_ford step 5146 current loss 0.064367, current_train_items 164704.
I0304 19:30:35.231951 23118544486528 run.py:483] Algo bellman_ford step 5147 current loss 0.039320, current_train_items 164736.
I0304 19:30:35.262464 23118544486528 run.py:483] Algo bellman_ford step 5148 current loss 0.051365, current_train_items 164768.
I0304 19:30:35.295969 23118544486528 run.py:483] Algo bellman_ford step 5149 current loss 0.053315, current_train_items 164800.
I0304 19:30:35.316006 23118544486528 run.py:483] Algo bellman_ford step 5150 current loss 0.008785, current_train_items 164832.
I0304 19:30:35.324205 23118544486528 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0304 19:30:35.324314 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:35.341534 23118544486528 run.py:483] Algo bellman_ford step 5151 current loss 0.021610, current_train_items 164864.
I0304 19:30:35.365749 23118544486528 run.py:483] Algo bellman_ford step 5152 current loss 0.054160, current_train_items 164896.
I0304 19:30:35.396479 23118544486528 run.py:483] Algo bellman_ford step 5153 current loss 0.099177, current_train_items 164928.
I0304 19:30:35.429823 23118544486528 run.py:483] Algo bellman_ford step 5154 current loss 0.094167, current_train_items 164960.
I0304 19:30:35.450273 23118544486528 run.py:483] Algo bellman_ford step 5155 current loss 0.009722, current_train_items 164992.
I0304 19:30:35.466477 23118544486528 run.py:483] Algo bellman_ford step 5156 current loss 0.012665, current_train_items 165024.
I0304 19:30:35.491298 23118544486528 run.py:483] Algo bellman_ford step 5157 current loss 0.073477, current_train_items 165056.
I0304 19:30:35.524498 23118544486528 run.py:483] Algo bellman_ford step 5158 current loss 0.234537, current_train_items 165088.
I0304 19:30:35.557974 23118544486528 run.py:483] Algo bellman_ford step 5159 current loss 0.112638, current_train_items 165120.
I0304 19:30:35.578040 23118544486528 run.py:483] Algo bellman_ford step 5160 current loss 0.004319, current_train_items 165152.
I0304 19:30:35.594304 23118544486528 run.py:483] Algo bellman_ford step 5161 current loss 0.016990, current_train_items 165184.
I0304 19:30:35.619060 23118544486528 run.py:483] Algo bellman_ford step 5162 current loss 0.057426, current_train_items 165216.
I0304 19:30:35.652176 23118544486528 run.py:483] Algo bellman_ford step 5163 current loss 0.059458, current_train_items 165248.
I0304 19:30:35.684978 23118544486528 run.py:483] Algo bellman_ford step 5164 current loss 0.061547, current_train_items 165280.
I0304 19:30:35.704419 23118544486528 run.py:483] Algo bellman_ford step 5165 current loss 0.007249, current_train_items 165312.
I0304 19:30:35.720637 23118544486528 run.py:483] Algo bellman_ford step 5166 current loss 0.018636, current_train_items 165344.
I0304 19:30:35.743945 23118544486528 run.py:483] Algo bellman_ford step 5167 current loss 0.086283, current_train_items 165376.
I0304 19:30:35.775931 23118544486528 run.py:483] Algo bellman_ford step 5168 current loss 0.066033, current_train_items 165408.
I0304 19:30:35.811069 23118544486528 run.py:483] Algo bellman_ford step 5169 current loss 0.073211, current_train_items 165440.
I0304 19:30:35.831408 23118544486528 run.py:483] Algo bellman_ford step 5170 current loss 0.010136, current_train_items 165472.
I0304 19:30:35.848084 23118544486528 run.py:483] Algo bellman_ford step 5171 current loss 0.047521, current_train_items 165504.
I0304 19:30:35.872504 23118544486528 run.py:483] Algo bellman_ford step 5172 current loss 0.034689, current_train_items 165536.
I0304 19:30:35.904923 23118544486528 run.py:483] Algo bellman_ford step 5173 current loss 0.063785, current_train_items 165568.
I0304 19:30:35.938134 23118544486528 run.py:483] Algo bellman_ford step 5174 current loss 0.060392, current_train_items 165600.
I0304 19:30:35.958694 23118544486528 run.py:483] Algo bellman_ford step 5175 current loss 0.007371, current_train_items 165632.
I0304 19:30:35.975373 23118544486528 run.py:483] Algo bellman_ford step 5176 current loss 0.016866, current_train_items 165664.
I0304 19:30:35.999508 23118544486528 run.py:483] Algo bellman_ford step 5177 current loss 0.081322, current_train_items 165696.
I0304 19:30:36.030626 23118544486528 run.py:483] Algo bellman_ford step 5178 current loss 0.083991, current_train_items 165728.
I0304 19:30:36.063764 23118544486528 run.py:483] Algo bellman_ford step 5179 current loss 0.067450, current_train_items 165760.
I0304 19:30:36.083240 23118544486528 run.py:483] Algo bellman_ford step 5180 current loss 0.005112, current_train_items 165792.
I0304 19:30:36.099495 23118544486528 run.py:483] Algo bellman_ford step 5181 current loss 0.018665, current_train_items 165824.
I0304 19:30:36.123726 23118544486528 run.py:483] Algo bellman_ford step 5182 current loss 0.053895, current_train_items 165856.
I0304 19:30:36.155240 23118544486528 run.py:483] Algo bellman_ford step 5183 current loss 0.063333, current_train_items 165888.
I0304 19:30:36.187646 23118544486528 run.py:483] Algo bellman_ford step 5184 current loss 0.050457, current_train_items 165920.
I0304 19:30:36.207988 23118544486528 run.py:483] Algo bellman_ford step 5185 current loss 0.024927, current_train_items 165952.
I0304 19:30:36.225046 23118544486528 run.py:483] Algo bellman_ford step 5186 current loss 0.019924, current_train_items 165984.
I0304 19:30:36.247888 23118544486528 run.py:483] Algo bellman_ford step 5187 current loss 0.015227, current_train_items 166016.
I0304 19:30:36.280207 23118544486528 run.py:483] Algo bellman_ford step 5188 current loss 0.037780, current_train_items 166048.
I0304 19:30:36.313152 23118544486528 run.py:483] Algo bellman_ford step 5189 current loss 0.058797, current_train_items 166080.
I0304 19:30:36.333655 23118544486528 run.py:483] Algo bellman_ford step 5190 current loss 0.007934, current_train_items 166112.
I0304 19:30:36.350170 23118544486528 run.py:483] Algo bellman_ford step 5191 current loss 0.022171, current_train_items 166144.
I0304 19:30:36.374702 23118544486528 run.py:483] Algo bellman_ford step 5192 current loss 0.039667, current_train_items 166176.
I0304 19:30:36.405268 23118544486528 run.py:483] Algo bellman_ford step 5193 current loss 0.037211, current_train_items 166208.
I0304 19:30:36.439298 23118544486528 run.py:483] Algo bellman_ford step 5194 current loss 0.053870, current_train_items 166240.
I0304 19:30:36.459078 23118544486528 run.py:483] Algo bellman_ford step 5195 current loss 0.003126, current_train_items 166272.
I0304 19:30:36.475533 23118544486528 run.py:483] Algo bellman_ford step 5196 current loss 0.022082, current_train_items 166304.
I0304 19:30:36.499962 23118544486528 run.py:483] Algo bellman_ford step 5197 current loss 0.044400, current_train_items 166336.
I0304 19:30:36.532806 23118544486528 run.py:483] Algo bellman_ford step 5198 current loss 0.070964, current_train_items 166368.
I0304 19:30:36.566107 23118544486528 run.py:483] Algo bellman_ford step 5199 current loss 0.042815, current_train_items 166400.
I0304 19:30:36.585937 23118544486528 run.py:483] Algo bellman_ford step 5200 current loss 0.003502, current_train_items 166432.
I0304 19:30:36.593914 23118544486528 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0304 19:30:36.594021 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:30:36.611128 23118544486528 run.py:483] Algo bellman_ford step 5201 current loss 0.018321, current_train_items 166464.
I0304 19:30:36.636353 23118544486528 run.py:483] Algo bellman_ford step 5202 current loss 0.027443, current_train_items 166496.
I0304 19:30:36.669312 23118544486528 run.py:483] Algo bellman_ford step 5203 current loss 0.055613, current_train_items 166528.
I0304 19:30:36.705092 23118544486528 run.py:483] Algo bellman_ford step 5204 current loss 0.042131, current_train_items 166560.
I0304 19:30:36.725485 23118544486528 run.py:483] Algo bellman_ford step 5205 current loss 0.006381, current_train_items 166592.
I0304 19:30:36.741487 23118544486528 run.py:483] Algo bellman_ford step 5206 current loss 0.026001, current_train_items 166624.
I0304 19:30:36.765909 23118544486528 run.py:483] Algo bellman_ford step 5207 current loss 0.051300, current_train_items 166656.
I0304 19:30:36.796428 23118544486528 run.py:483] Algo bellman_ford step 5208 current loss 0.040256, current_train_items 166688.
I0304 19:30:36.832960 23118544486528 run.py:483] Algo bellman_ford step 5209 current loss 0.054014, current_train_items 166720.
I0304 19:30:36.853052 23118544486528 run.py:483] Algo bellman_ford step 5210 current loss 0.024072, current_train_items 166752.
I0304 19:30:36.868973 23118544486528 run.py:483] Algo bellman_ford step 5211 current loss 0.025582, current_train_items 166784.
I0304 19:30:36.893652 23118544486528 run.py:483] Algo bellman_ford step 5212 current loss 0.028479, current_train_items 166816.
I0304 19:30:36.927135 23118544486528 run.py:483] Algo bellman_ford step 5213 current loss 0.086326, current_train_items 166848.
I0304 19:30:36.961168 23118544486528 run.py:483] Algo bellman_ford step 5214 current loss 0.067025, current_train_items 166880.
I0304 19:30:36.980679 23118544486528 run.py:483] Algo bellman_ford step 5215 current loss 0.003162, current_train_items 166912.
I0304 19:30:36.997284 23118544486528 run.py:483] Algo bellman_ford step 5216 current loss 0.019587, current_train_items 166944.
I0304 19:30:37.021352 23118544486528 run.py:483] Algo bellman_ford step 5217 current loss 0.029217, current_train_items 166976.
I0304 19:30:37.052365 23118544486528 run.py:483] Algo bellman_ford step 5218 current loss 0.047934, current_train_items 167008.
I0304 19:30:37.086901 23118544486528 run.py:483] Algo bellman_ford step 5219 current loss 0.053047, current_train_items 167040.
I0304 19:30:37.106718 23118544486528 run.py:483] Algo bellman_ford step 5220 current loss 0.006238, current_train_items 167072.
I0304 19:30:37.122873 23118544486528 run.py:483] Algo bellman_ford step 5221 current loss 0.013148, current_train_items 167104.
I0304 19:30:37.145903 23118544486528 run.py:483] Algo bellman_ford step 5222 current loss 0.015234, current_train_items 167136.
I0304 19:30:37.178413 23118544486528 run.py:483] Algo bellman_ford step 5223 current loss 0.056983, current_train_items 167168.
I0304 19:30:37.211913 23118544486528 run.py:483] Algo bellman_ford step 5224 current loss 0.056010, current_train_items 167200.
I0304 19:30:37.231809 23118544486528 run.py:483] Algo bellman_ford step 5225 current loss 0.006241, current_train_items 167232.
I0304 19:30:37.247882 23118544486528 run.py:483] Algo bellman_ford step 5226 current loss 0.014988, current_train_items 167264.
I0304 19:30:37.272511 23118544486528 run.py:483] Algo bellman_ford step 5227 current loss 0.066000, current_train_items 167296.
I0304 19:30:37.305515 23118544486528 run.py:483] Algo bellman_ford step 5228 current loss 0.052355, current_train_items 167328.
I0304 19:30:37.341122 23118544486528 run.py:483] Algo bellman_ford step 5229 current loss 0.069983, current_train_items 167360.
I0304 19:30:37.361052 23118544486528 run.py:483] Algo bellman_ford step 5230 current loss 0.004114, current_train_items 167392.
I0304 19:30:37.377518 23118544486528 run.py:483] Algo bellman_ford step 5231 current loss 0.032120, current_train_items 167424.
I0304 19:30:37.402136 23118544486528 run.py:483] Algo bellman_ford step 5232 current loss 0.032761, current_train_items 167456.
I0304 19:30:37.433540 23118544486528 run.py:483] Algo bellman_ford step 5233 current loss 0.041295, current_train_items 167488.
I0304 19:30:37.469130 23118544486528 run.py:483] Algo bellman_ford step 5234 current loss 0.065507, current_train_items 167520.
I0304 19:30:37.489092 23118544486528 run.py:483] Algo bellman_ford step 5235 current loss 0.003408, current_train_items 167552.
I0304 19:30:37.505251 23118544486528 run.py:483] Algo bellman_ford step 5236 current loss 0.008352, current_train_items 167584.
I0304 19:30:37.529568 23118544486528 run.py:483] Algo bellman_ford step 5237 current loss 0.044579, current_train_items 167616.
I0304 19:30:37.562727 23118544486528 run.py:483] Algo bellman_ford step 5238 current loss 0.068354, current_train_items 167648.
I0304 19:30:37.597783 23118544486528 run.py:483] Algo bellman_ford step 5239 current loss 0.060867, current_train_items 167680.
I0304 19:30:37.617794 23118544486528 run.py:483] Algo bellman_ford step 5240 current loss 0.015433, current_train_items 167712.
I0304 19:30:37.634155 23118544486528 run.py:483] Algo bellman_ford step 5241 current loss 0.047826, current_train_items 167744.
I0304 19:30:37.658476 23118544486528 run.py:483] Algo bellman_ford step 5242 current loss 0.033091, current_train_items 167776.
I0304 19:30:37.690201 23118544486528 run.py:483] Algo bellman_ford step 5243 current loss 0.055071, current_train_items 167808.
I0304 19:30:37.721814 23118544486528 run.py:483] Algo bellman_ford step 5244 current loss 0.037151, current_train_items 167840.
I0304 19:30:37.741456 23118544486528 run.py:483] Algo bellman_ford step 5245 current loss 0.011455, current_train_items 167872.
I0304 19:30:37.757859 23118544486528 run.py:483] Algo bellman_ford step 5246 current loss 0.059525, current_train_items 167904.
I0304 19:30:37.782787 23118544486528 run.py:483] Algo bellman_ford step 5247 current loss 0.065013, current_train_items 167936.
I0304 19:30:37.814959 23118544486528 run.py:483] Algo bellman_ford step 5248 current loss 0.052857, current_train_items 167968.
I0304 19:30:37.849627 23118544486528 run.py:483] Algo bellman_ford step 5249 current loss 0.068685, current_train_items 168000.
I0304 19:30:37.869169 23118544486528 run.py:483] Algo bellman_ford step 5250 current loss 0.005760, current_train_items 168032.
I0304 19:30:37.877266 23118544486528 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0304 19:30:37.877374 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:30:37.894295 23118544486528 run.py:483] Algo bellman_ford step 5251 current loss 0.016501, current_train_items 168064.
I0304 19:30:37.919295 23118544486528 run.py:483] Algo bellman_ford step 5252 current loss 0.191729, current_train_items 168096.
I0304 19:30:37.952235 23118544486528 run.py:483] Algo bellman_ford step 5253 current loss 0.134908, current_train_items 168128.
I0304 19:30:37.984225 23118544486528 run.py:483] Algo bellman_ford step 5254 current loss 0.081630, current_train_items 168160.
I0304 19:30:38.004297 23118544486528 run.py:483] Algo bellman_ford step 5255 current loss 0.035542, current_train_items 168192.
I0304 19:30:38.020554 23118544486528 run.py:483] Algo bellman_ford step 5256 current loss 0.009395, current_train_items 168224.
I0304 19:30:38.043930 23118544486528 run.py:483] Algo bellman_ford step 5257 current loss 0.054783, current_train_items 168256.
I0304 19:30:38.076547 23118544486528 run.py:483] Algo bellman_ford step 5258 current loss 0.073053, current_train_items 168288.
I0304 19:30:38.108926 23118544486528 run.py:483] Algo bellman_ford step 5259 current loss 0.038395, current_train_items 168320.
I0304 19:30:38.128911 23118544486528 run.py:483] Algo bellman_ford step 5260 current loss 0.005632, current_train_items 168352.
I0304 19:30:38.145453 23118544486528 run.py:483] Algo bellman_ford step 5261 current loss 0.019698, current_train_items 168384.
I0304 19:30:38.169127 23118544486528 run.py:483] Algo bellman_ford step 5262 current loss 0.069425, current_train_items 168416.
I0304 19:30:38.202132 23118544486528 run.py:483] Algo bellman_ford step 5263 current loss 0.044624, current_train_items 168448.
I0304 19:30:38.235107 23118544486528 run.py:483] Algo bellman_ford step 5264 current loss 0.047829, current_train_items 168480.
I0304 19:30:38.255033 23118544486528 run.py:483] Algo bellman_ford step 5265 current loss 0.010277, current_train_items 168512.
I0304 19:30:38.271516 23118544486528 run.py:483] Algo bellman_ford step 5266 current loss 0.023431, current_train_items 168544.
I0304 19:30:38.296841 23118544486528 run.py:483] Algo bellman_ford step 5267 current loss 0.067442, current_train_items 168576.
I0304 19:30:38.328026 23118544486528 run.py:483] Algo bellman_ford step 5268 current loss 0.050183, current_train_items 168608.
I0304 19:30:38.360391 23118544486528 run.py:483] Algo bellman_ford step 5269 current loss 0.043150, current_train_items 168640.
I0304 19:30:38.380428 23118544486528 run.py:483] Algo bellman_ford step 5270 current loss 0.005211, current_train_items 168672.
I0304 19:30:38.396749 23118544486528 run.py:483] Algo bellman_ford step 5271 current loss 0.015767, current_train_items 168704.
I0304 19:30:38.420580 23118544486528 run.py:483] Algo bellman_ford step 5272 current loss 0.047069, current_train_items 168736.
I0304 19:30:38.452036 23118544486528 run.py:483] Algo bellman_ford step 5273 current loss 0.060115, current_train_items 168768.
I0304 19:30:38.485402 23118544486528 run.py:483] Algo bellman_ford step 5274 current loss 0.037438, current_train_items 168800.
I0304 19:30:38.505592 23118544486528 run.py:483] Algo bellman_ford step 5275 current loss 0.003438, current_train_items 168832.
I0304 19:30:38.522023 23118544486528 run.py:483] Algo bellman_ford step 5276 current loss 0.011557, current_train_items 168864.
I0304 19:30:38.545819 23118544486528 run.py:483] Algo bellman_ford step 5277 current loss 0.042901, current_train_items 168896.
I0304 19:30:38.577680 23118544486528 run.py:483] Algo bellman_ford step 5278 current loss 0.059613, current_train_items 168928.
I0304 19:30:38.612434 23118544486528 run.py:483] Algo bellman_ford step 5279 current loss 0.057612, current_train_items 168960.
I0304 19:30:38.632236 23118544486528 run.py:483] Algo bellman_ford step 5280 current loss 0.002593, current_train_items 168992.
I0304 19:30:38.648881 23118544486528 run.py:483] Algo bellman_ford step 5281 current loss 0.035766, current_train_items 169024.
I0304 19:30:38.674787 23118544486528 run.py:483] Algo bellman_ford step 5282 current loss 0.047934, current_train_items 169056.
I0304 19:30:38.706787 23118544486528 run.py:483] Algo bellman_ford step 5283 current loss 0.066608, current_train_items 169088.
I0304 19:30:38.740550 23118544486528 run.py:483] Algo bellman_ford step 5284 current loss 0.073166, current_train_items 169120.
I0304 19:30:38.761152 23118544486528 run.py:483] Algo bellman_ford step 5285 current loss 0.004293, current_train_items 169152.
I0304 19:30:38.777674 23118544486528 run.py:483] Algo bellman_ford step 5286 current loss 0.015397, current_train_items 169184.
I0304 19:30:38.802857 23118544486528 run.py:483] Algo bellman_ford step 5287 current loss 0.065064, current_train_items 169216.
I0304 19:30:38.834383 23118544486528 run.py:483] Algo bellman_ford step 5288 current loss 0.094193, current_train_items 169248.
I0304 19:30:38.869089 23118544486528 run.py:483] Algo bellman_ford step 5289 current loss 0.061446, current_train_items 169280.
I0304 19:30:38.889210 23118544486528 run.py:483] Algo bellman_ford step 5290 current loss 0.004206, current_train_items 169312.
I0304 19:30:38.906140 23118544486528 run.py:483] Algo bellman_ford step 5291 current loss 0.016057, current_train_items 169344.
I0304 19:30:38.930121 23118544486528 run.py:483] Algo bellman_ford step 5292 current loss 0.038661, current_train_items 169376.
I0304 19:30:38.962080 23118544486528 run.py:483] Algo bellman_ford step 5293 current loss 0.053135, current_train_items 169408.
I0304 19:30:38.995209 23118544486528 run.py:483] Algo bellman_ford step 5294 current loss 0.048346, current_train_items 169440.
I0304 19:30:39.014955 23118544486528 run.py:483] Algo bellman_ford step 5295 current loss 0.004467, current_train_items 169472.
I0304 19:30:39.031339 23118544486528 run.py:483] Algo bellman_ford step 5296 current loss 0.017222, current_train_items 169504.
I0304 19:30:39.055154 23118544486528 run.py:483] Algo bellman_ford step 5297 current loss 0.047619, current_train_items 169536.
I0304 19:30:39.087837 23118544486528 run.py:483] Algo bellman_ford step 5298 current loss 0.075736, current_train_items 169568.
I0304 19:30:39.124032 23118544486528 run.py:483] Algo bellman_ford step 5299 current loss 0.086628, current_train_items 169600.
I0304 19:30:39.144483 23118544486528 run.py:483] Algo bellman_ford step 5300 current loss 0.004717, current_train_items 169632.
I0304 19:30:39.152218 23118544486528 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0304 19:30:39.152322 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:30:39.168940 23118544486528 run.py:483] Algo bellman_ford step 5301 current loss 0.016597, current_train_items 169664.
I0304 19:30:39.194158 23118544486528 run.py:483] Algo bellman_ford step 5302 current loss 0.126890, current_train_items 169696.
I0304 19:30:39.226926 23118544486528 run.py:483] Algo bellman_ford step 5303 current loss 0.072953, current_train_items 169728.
I0304 19:30:39.261973 23118544486528 run.py:483] Algo bellman_ford step 5304 current loss 0.064248, current_train_items 169760.
I0304 19:30:39.282353 23118544486528 run.py:483] Algo bellman_ford step 5305 current loss 0.009245, current_train_items 169792.
I0304 19:30:39.298572 23118544486528 run.py:483] Algo bellman_ford step 5306 current loss 0.037554, current_train_items 169824.
I0304 19:30:39.324501 23118544486528 run.py:483] Algo bellman_ford step 5307 current loss 0.054903, current_train_items 169856.
I0304 19:30:39.357414 23118544486528 run.py:483] Algo bellman_ford step 5308 current loss 0.040354, current_train_items 169888.
I0304 19:30:39.390425 23118544486528 run.py:483] Algo bellman_ford step 5309 current loss 0.048873, current_train_items 169920.
I0304 19:30:39.410494 23118544486528 run.py:483] Algo bellman_ford step 5310 current loss 0.010312, current_train_items 169952.
I0304 19:30:39.426386 23118544486528 run.py:483] Algo bellman_ford step 5311 current loss 0.006951, current_train_items 169984.
I0304 19:30:39.450380 23118544486528 run.py:483] Algo bellman_ford step 5312 current loss 0.076521, current_train_items 170016.
I0304 19:30:39.482078 23118544486528 run.py:483] Algo bellman_ford step 5313 current loss 0.166125, current_train_items 170048.
I0304 19:30:39.517150 23118544486528 run.py:483] Algo bellman_ford step 5314 current loss 0.265354, current_train_items 170080.
I0304 19:30:39.537351 23118544486528 run.py:483] Algo bellman_ford step 5315 current loss 0.019329, current_train_items 170112.
I0304 19:30:39.553663 23118544486528 run.py:483] Algo bellman_ford step 5316 current loss 0.017679, current_train_items 170144.
I0304 19:30:39.577958 23118544486528 run.py:483] Algo bellman_ford step 5317 current loss 0.122085, current_train_items 170176.
I0304 19:30:39.608604 23118544486528 run.py:483] Algo bellman_ford step 5318 current loss 0.091951, current_train_items 170208.
I0304 19:30:39.643318 23118544486528 run.py:483] Algo bellman_ford step 5319 current loss 0.132887, current_train_items 170240.
I0304 19:30:39.663191 23118544486528 run.py:483] Algo bellman_ford step 5320 current loss 0.026593, current_train_items 170272.
I0304 19:30:39.680178 23118544486528 run.py:483] Algo bellman_ford step 5321 current loss 0.072219, current_train_items 170304.
I0304 19:30:39.705454 23118544486528 run.py:483] Algo bellman_ford step 5322 current loss 0.074965, current_train_items 170336.
I0304 19:30:39.735893 23118544486528 run.py:483] Algo bellman_ford step 5323 current loss 0.045094, current_train_items 170368.
I0304 19:30:39.771327 23118544486528 run.py:483] Algo bellman_ford step 5324 current loss 0.078447, current_train_items 170400.
I0304 19:30:39.791386 23118544486528 run.py:483] Algo bellman_ford step 5325 current loss 0.005732, current_train_items 170432.
I0304 19:30:39.807793 23118544486528 run.py:483] Algo bellman_ford step 5326 current loss 0.025541, current_train_items 170464.
I0304 19:30:39.831644 23118544486528 run.py:483] Algo bellman_ford step 5327 current loss 0.048387, current_train_items 170496.
I0304 19:30:39.864672 23118544486528 run.py:483] Algo bellman_ford step 5328 current loss 0.061378, current_train_items 170528.
I0304 19:30:39.898401 23118544486528 run.py:483] Algo bellman_ford step 5329 current loss 0.110398, current_train_items 170560.
I0304 19:30:39.918218 23118544486528 run.py:483] Algo bellman_ford step 5330 current loss 0.036337, current_train_items 170592.
I0304 19:30:39.934837 23118544486528 run.py:483] Algo bellman_ford step 5331 current loss 0.015843, current_train_items 170624.
I0304 19:30:39.959079 23118544486528 run.py:483] Algo bellman_ford step 5332 current loss 0.044749, current_train_items 170656.
I0304 19:30:39.989501 23118544486528 run.py:483] Algo bellman_ford step 5333 current loss 0.084201, current_train_items 170688.
I0304 19:30:40.022586 23118544486528 run.py:483] Algo bellman_ford step 5334 current loss 0.104836, current_train_items 170720.
I0304 19:30:40.042476 23118544486528 run.py:483] Algo bellman_ford step 5335 current loss 0.017996, current_train_items 170752.
I0304 19:30:40.059132 23118544486528 run.py:483] Algo bellman_ford step 5336 current loss 0.034285, current_train_items 170784.
I0304 19:30:40.082996 23118544486528 run.py:483] Algo bellman_ford step 5337 current loss 0.032725, current_train_items 170816.
I0304 19:30:40.114016 23118544486528 run.py:483] Algo bellman_ford step 5338 current loss 0.090524, current_train_items 170848.
I0304 19:30:40.149236 23118544486528 run.py:483] Algo bellman_ford step 5339 current loss 0.166951, current_train_items 170880.
I0304 19:30:40.168970 23118544486528 run.py:483] Algo bellman_ford step 5340 current loss 0.003231, current_train_items 170912.
I0304 19:30:40.185502 23118544486528 run.py:483] Algo bellman_ford step 5341 current loss 0.045551, current_train_items 170944.
I0304 19:30:40.211267 23118544486528 run.py:483] Algo bellman_ford step 5342 current loss 0.055431, current_train_items 170976.
I0304 19:30:40.243495 23118544486528 run.py:483] Algo bellman_ford step 5343 current loss 0.100825, current_train_items 171008.
I0304 19:30:40.277778 23118544486528 run.py:483] Algo bellman_ford step 5344 current loss 0.075185, current_train_items 171040.
I0304 19:30:40.297555 23118544486528 run.py:483] Algo bellman_ford step 5345 current loss 0.002668, current_train_items 171072.
I0304 19:30:40.313498 23118544486528 run.py:483] Algo bellman_ford step 5346 current loss 0.023641, current_train_items 171104.
I0304 19:30:40.337651 23118544486528 run.py:483] Algo bellman_ford step 5347 current loss 0.028903, current_train_items 171136.
I0304 19:30:40.370645 23118544486528 run.py:483] Algo bellman_ford step 5348 current loss 0.082778, current_train_items 171168.
I0304 19:30:40.405427 23118544486528 run.py:483] Algo bellman_ford step 5349 current loss 0.080497, current_train_items 171200.
I0304 19:30:40.425899 23118544486528 run.py:483] Algo bellman_ford step 5350 current loss 0.008350, current_train_items 171232.
I0304 19:30:40.434041 23118544486528 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0304 19:30:40.434148 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:40.450851 23118544486528 run.py:483] Algo bellman_ford step 5351 current loss 0.013350, current_train_items 171264.
I0304 19:30:40.475968 23118544486528 run.py:483] Algo bellman_ford step 5352 current loss 0.127435, current_train_items 171296.
I0304 19:30:40.506939 23118544486528 run.py:483] Algo bellman_ford step 5353 current loss 0.118176, current_train_items 171328.
I0304 19:30:40.541001 23118544486528 run.py:483] Algo bellman_ford step 5354 current loss 0.065078, current_train_items 171360.
I0304 19:30:40.561379 23118544486528 run.py:483] Algo bellman_ford step 5355 current loss 0.004170, current_train_items 171392.
I0304 19:30:40.577911 23118544486528 run.py:483] Algo bellman_ford step 5356 current loss 0.027569, current_train_items 171424.
I0304 19:30:40.602043 23118544486528 run.py:483] Algo bellman_ford step 5357 current loss 0.120835, current_train_items 171456.
I0304 19:30:40.632942 23118544486528 run.py:483] Algo bellman_ford step 5358 current loss 0.107973, current_train_items 171488.
I0304 19:30:40.666863 23118544486528 run.py:483] Algo bellman_ford step 5359 current loss 0.095361, current_train_items 171520.
I0304 19:30:40.687249 23118544486528 run.py:483] Algo bellman_ford step 5360 current loss 0.005244, current_train_items 171552.
I0304 19:30:40.703804 23118544486528 run.py:483] Algo bellman_ford step 5361 current loss 0.041998, current_train_items 171584.
I0304 19:30:40.728209 23118544486528 run.py:483] Algo bellman_ford step 5362 current loss 0.081429, current_train_items 171616.
I0304 19:30:40.760028 23118544486528 run.py:483] Algo bellman_ford step 5363 current loss 0.056232, current_train_items 171648.
I0304 19:30:40.794268 23118544486528 run.py:483] Algo bellman_ford step 5364 current loss 0.065729, current_train_items 171680.
I0304 19:30:40.814318 23118544486528 run.py:483] Algo bellman_ford step 5365 current loss 0.007216, current_train_items 171712.
I0304 19:30:40.830820 23118544486528 run.py:483] Algo bellman_ford step 5366 current loss 0.028473, current_train_items 171744.
I0304 19:30:40.854843 23118544486528 run.py:483] Algo bellman_ford step 5367 current loss 0.051462, current_train_items 171776.
I0304 19:30:40.886396 23118544486528 run.py:483] Algo bellman_ford step 5368 current loss 0.081374, current_train_items 171808.
I0304 19:30:40.919386 23118544486528 run.py:483] Algo bellman_ford step 5369 current loss 0.049931, current_train_items 171840.
I0304 19:30:40.939467 23118544486528 run.py:483] Algo bellman_ford step 5370 current loss 0.005176, current_train_items 171872.
I0304 19:30:40.955913 23118544486528 run.py:483] Algo bellman_ford step 5371 current loss 0.010565, current_train_items 171904.
I0304 19:30:40.979724 23118544486528 run.py:483] Algo bellman_ford step 5372 current loss 0.026189, current_train_items 171936.
I0304 19:30:41.011347 23118544486528 run.py:483] Algo bellman_ford step 5373 current loss 0.069555, current_train_items 171968.
I0304 19:30:41.043159 23118544486528 run.py:483] Algo bellman_ford step 5374 current loss 0.069896, current_train_items 172000.
I0304 19:30:41.063241 23118544486528 run.py:483] Algo bellman_ford step 5375 current loss 0.006575, current_train_items 172032.
I0304 19:30:41.080017 23118544486528 run.py:483] Algo bellman_ford step 5376 current loss 0.011598, current_train_items 172064.
I0304 19:30:41.104134 23118544486528 run.py:483] Algo bellman_ford step 5377 current loss 0.059290, current_train_items 172096.
I0304 19:30:41.134898 23118544486528 run.py:483] Algo bellman_ford step 5378 current loss 0.046926, current_train_items 172128.
I0304 19:30:41.165889 23118544486528 run.py:483] Algo bellman_ford step 5379 current loss 0.051142, current_train_items 172160.
I0304 19:30:41.185592 23118544486528 run.py:483] Algo bellman_ford step 5380 current loss 0.016531, current_train_items 172192.
I0304 19:30:41.202479 23118544486528 run.py:483] Algo bellman_ford step 5381 current loss 0.019962, current_train_items 172224.
I0304 19:30:41.227343 23118544486528 run.py:483] Algo bellman_ford step 5382 current loss 0.068910, current_train_items 172256.
I0304 19:30:41.259089 23118544486528 run.py:483] Algo bellman_ford step 5383 current loss 0.089385, current_train_items 172288.
I0304 19:30:41.292021 23118544486528 run.py:483] Algo bellman_ford step 5384 current loss 0.056843, current_train_items 172320.
I0304 19:30:41.312118 23118544486528 run.py:483] Algo bellman_ford step 5385 current loss 0.004764, current_train_items 172352.
I0304 19:30:41.328584 23118544486528 run.py:483] Algo bellman_ford step 5386 current loss 0.012871, current_train_items 172384.
I0304 19:30:41.352223 23118544486528 run.py:483] Algo bellman_ford step 5387 current loss 0.023890, current_train_items 172416.
I0304 19:30:41.382786 23118544486528 run.py:483] Algo bellman_ford step 5388 current loss 0.023826, current_train_items 172448.
I0304 19:30:41.417224 23118544486528 run.py:483] Algo bellman_ford step 5389 current loss 0.118629, current_train_items 172480.
I0304 19:30:41.437744 23118544486528 run.py:483] Algo bellman_ford step 5390 current loss 0.013503, current_train_items 172512.
I0304 19:30:41.454751 23118544486528 run.py:483] Algo bellman_ford step 5391 current loss 0.014025, current_train_items 172544.
I0304 19:30:41.478280 23118544486528 run.py:483] Algo bellman_ford step 5392 current loss 0.034859, current_train_items 172576.
I0304 19:30:41.510867 23118544486528 run.py:483] Algo bellman_ford step 5393 current loss 0.069364, current_train_items 172608.
I0304 19:30:41.546618 23118544486528 run.py:483] Algo bellman_ford step 5394 current loss 0.068396, current_train_items 172640.
I0304 19:30:41.566426 23118544486528 run.py:483] Algo bellman_ford step 5395 current loss 0.005062, current_train_items 172672.
I0304 19:30:41.583434 23118544486528 run.py:483] Algo bellman_ford step 5396 current loss 0.049695, current_train_items 172704.
I0304 19:30:41.608126 23118544486528 run.py:483] Algo bellman_ford step 5397 current loss 0.045693, current_train_items 172736.
I0304 19:30:41.639464 23118544486528 run.py:483] Algo bellman_ford step 5398 current loss 0.054574, current_train_items 172768.
I0304 19:30:41.674774 23118544486528 run.py:483] Algo bellman_ford step 5399 current loss 0.119350, current_train_items 172800.
I0304 19:30:41.695248 23118544486528 run.py:483] Algo bellman_ford step 5400 current loss 0.002130, current_train_items 172832.
I0304 19:30:41.703269 23118544486528 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0304 19:30:41.703378 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:41.720977 23118544486528 run.py:483] Algo bellman_ford step 5401 current loss 0.034823, current_train_items 172864.
I0304 19:30:41.746328 23118544486528 run.py:483] Algo bellman_ford step 5402 current loss 0.057429, current_train_items 172896.
I0304 19:30:41.779625 23118544486528 run.py:483] Algo bellman_ford step 5403 current loss 0.109945, current_train_items 172928.
I0304 19:30:41.814978 23118544486528 run.py:483] Algo bellman_ford step 5404 current loss 0.058119, current_train_items 172960.
I0304 19:30:41.835675 23118544486528 run.py:483] Algo bellman_ford step 5405 current loss 0.002955, current_train_items 172992.
I0304 19:30:41.851494 23118544486528 run.py:483] Algo bellman_ford step 5406 current loss 0.019904, current_train_items 173024.
I0304 19:30:41.875171 23118544486528 run.py:483] Algo bellman_ford step 5407 current loss 0.011627, current_train_items 173056.
I0304 19:30:41.904985 23118544486528 run.py:483] Algo bellman_ford step 5408 current loss 0.154197, current_train_items 173088.
I0304 19:30:41.938289 23118544486528 run.py:483] Algo bellman_ford step 5409 current loss 0.168687, current_train_items 173120.
I0304 19:30:41.958027 23118544486528 run.py:483] Algo bellman_ford step 5410 current loss 0.005403, current_train_items 173152.
I0304 19:30:41.975206 23118544486528 run.py:483] Algo bellman_ford step 5411 current loss 0.031090, current_train_items 173184.
I0304 19:30:42.000047 23118544486528 run.py:483] Algo bellman_ford step 5412 current loss 0.142637, current_train_items 173216.
I0304 19:30:42.032085 23118544486528 run.py:483] Algo bellman_ford step 5413 current loss 0.077501, current_train_items 173248.
I0304 19:30:42.066871 23118544486528 run.py:483] Algo bellman_ford step 5414 current loss 0.072078, current_train_items 173280.
I0304 19:30:42.086808 23118544486528 run.py:483] Algo bellman_ford step 5415 current loss 0.018624, current_train_items 173312.
I0304 19:30:42.103718 23118544486528 run.py:483] Algo bellman_ford step 5416 current loss 0.047703, current_train_items 173344.
I0304 19:30:42.128654 23118544486528 run.py:483] Algo bellman_ford step 5417 current loss 0.067099, current_train_items 173376.
I0304 19:30:42.161817 23118544486528 run.py:483] Algo bellman_ford step 5418 current loss 0.043489, current_train_items 173408.
I0304 19:30:42.197559 23118544486528 run.py:483] Algo bellman_ford step 5419 current loss 0.102874, current_train_items 173440.
I0304 19:30:42.217533 23118544486528 run.py:483] Algo bellman_ford step 5420 current loss 0.008023, current_train_items 173472.
I0304 19:30:42.234150 23118544486528 run.py:483] Algo bellman_ford step 5421 current loss 0.043774, current_train_items 173504.
I0304 19:30:42.258359 23118544486528 run.py:483] Algo bellman_ford step 5422 current loss 0.029910, current_train_items 173536.
I0304 19:30:42.289222 23118544486528 run.py:483] Algo bellman_ford step 5423 current loss 0.031746, current_train_items 173568.
I0304 19:30:42.321658 23118544486528 run.py:483] Algo bellman_ford step 5424 current loss 0.074014, current_train_items 173600.
I0304 19:30:42.341805 23118544486528 run.py:483] Algo bellman_ford step 5425 current loss 0.005340, current_train_items 173632.
I0304 19:30:42.357941 23118544486528 run.py:483] Algo bellman_ford step 5426 current loss 0.016012, current_train_items 173664.
I0304 19:30:42.382551 23118544486528 run.py:483] Algo bellman_ford step 5427 current loss 0.066821, current_train_items 173696.
I0304 19:30:42.414189 23118544486528 run.py:483] Algo bellman_ford step 5428 current loss 0.065762, current_train_items 173728.
I0304 19:30:42.449440 23118544486528 run.py:483] Algo bellman_ford step 5429 current loss 0.065312, current_train_items 173760.
I0304 19:30:42.469149 23118544486528 run.py:483] Algo bellman_ford step 5430 current loss 0.005182, current_train_items 173792.
I0304 19:30:42.485774 23118544486528 run.py:483] Algo bellman_ford step 5431 current loss 0.030476, current_train_items 173824.
I0304 19:30:42.510283 23118544486528 run.py:483] Algo bellman_ford step 5432 current loss 0.039901, current_train_items 173856.
I0304 19:30:42.544502 23118544486528 run.py:483] Algo bellman_ford step 5433 current loss 0.098997, current_train_items 173888.
I0304 19:30:42.580887 23118544486528 run.py:483] Algo bellman_ford step 5434 current loss 0.111337, current_train_items 173920.
I0304 19:30:42.600905 23118544486528 run.py:483] Algo bellman_ford step 5435 current loss 0.003000, current_train_items 173952.
I0304 19:30:42.617369 23118544486528 run.py:483] Algo bellman_ford step 5436 current loss 0.033624, current_train_items 173984.
I0304 19:30:42.642209 23118544486528 run.py:483] Algo bellman_ford step 5437 current loss 0.071765, current_train_items 174016.
I0304 19:30:42.673124 23118544486528 run.py:483] Algo bellman_ford step 5438 current loss 0.110847, current_train_items 174048.
I0304 19:30:42.711142 23118544486528 run.py:483] Algo bellman_ford step 5439 current loss 0.162776, current_train_items 174080.
I0304 19:30:42.731162 23118544486528 run.py:483] Algo bellman_ford step 5440 current loss 0.010973, current_train_items 174112.
I0304 19:30:42.747366 23118544486528 run.py:483] Algo bellman_ford step 5441 current loss 0.016680, current_train_items 174144.
I0304 19:30:42.771000 23118544486528 run.py:483] Algo bellman_ford step 5442 current loss 0.073649, current_train_items 174176.
I0304 19:30:42.802207 23118544486528 run.py:483] Algo bellman_ford step 5443 current loss 0.079840, current_train_items 174208.
I0304 19:30:42.836652 23118544486528 run.py:483] Algo bellman_ford step 5444 current loss 0.063136, current_train_items 174240.
I0304 19:30:42.856714 23118544486528 run.py:483] Algo bellman_ford step 5445 current loss 0.003857, current_train_items 174272.
I0304 19:30:42.873090 23118544486528 run.py:483] Algo bellman_ford step 5446 current loss 0.054659, current_train_items 174304.
I0304 19:30:42.898079 23118544486528 run.py:483] Algo bellman_ford step 5447 current loss 0.065211, current_train_items 174336.
I0304 19:30:42.930553 23118544486528 run.py:483] Algo bellman_ford step 5448 current loss 0.077254, current_train_items 174368.
I0304 19:30:42.964502 23118544486528 run.py:483] Algo bellman_ford step 5449 current loss 0.040445, current_train_items 174400.
I0304 19:30:42.984405 23118544486528 run.py:483] Algo bellman_ford step 5450 current loss 0.005668, current_train_items 174432.
I0304 19:30:42.992371 23118544486528 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0304 19:30:42.992477 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:43.009232 23118544486528 run.py:483] Algo bellman_ford step 5451 current loss 0.021850, current_train_items 174464.
I0304 19:30:43.033819 23118544486528 run.py:483] Algo bellman_ford step 5452 current loss 0.081358, current_train_items 174496.
I0304 19:30:43.064529 23118544486528 run.py:483] Algo bellman_ford step 5453 current loss 0.036672, current_train_items 174528.
I0304 19:30:43.098357 23118544486528 run.py:483] Algo bellman_ford step 5454 current loss 0.083825, current_train_items 174560.
I0304 19:30:43.118700 23118544486528 run.py:483] Algo bellman_ford step 5455 current loss 0.005319, current_train_items 174592.
I0304 19:30:43.135382 23118544486528 run.py:483] Algo bellman_ford step 5456 current loss 0.005862, current_train_items 174624.
I0304 19:30:43.159949 23118544486528 run.py:483] Algo bellman_ford step 5457 current loss 0.047753, current_train_items 174656.
I0304 19:30:43.192100 23118544486528 run.py:483] Algo bellman_ford step 5458 current loss 0.065281, current_train_items 174688.
I0304 19:30:43.223613 23118544486528 run.py:483] Algo bellman_ford step 5459 current loss 0.045998, current_train_items 174720.
I0304 19:30:43.243751 23118544486528 run.py:483] Algo bellman_ford step 5460 current loss 0.005697, current_train_items 174752.
I0304 19:30:43.260751 23118544486528 run.py:483] Algo bellman_ford step 5461 current loss 0.027670, current_train_items 174784.
I0304 19:30:43.284897 23118544486528 run.py:483] Algo bellman_ford step 5462 current loss 0.016952, current_train_items 174816.
I0304 19:30:43.316115 23118544486528 run.py:483] Algo bellman_ford step 5463 current loss 0.056777, current_train_items 174848.
I0304 19:30:43.350798 23118544486528 run.py:483] Algo bellman_ford step 5464 current loss 0.094440, current_train_items 174880.
I0304 19:30:43.370823 23118544486528 run.py:483] Algo bellman_ford step 5465 current loss 0.005150, current_train_items 174912.
I0304 19:30:43.386993 23118544486528 run.py:483] Algo bellman_ford step 5466 current loss 0.032996, current_train_items 174944.
I0304 19:30:43.410679 23118544486528 run.py:483] Algo bellman_ford step 5467 current loss 0.065793, current_train_items 174976.
I0304 19:30:43.441432 23118544486528 run.py:483] Algo bellman_ford step 5468 current loss 0.067016, current_train_items 175008.
I0304 19:30:43.475886 23118544486528 run.py:483] Algo bellman_ford step 5469 current loss 0.110477, current_train_items 175040.
I0304 19:30:43.496308 23118544486528 run.py:483] Algo bellman_ford step 5470 current loss 0.009991, current_train_items 175072.
I0304 19:30:43.512925 23118544486528 run.py:483] Algo bellman_ford step 5471 current loss 0.045813, current_train_items 175104.
I0304 19:30:43.536325 23118544486528 run.py:483] Algo bellman_ford step 5472 current loss 0.038006, current_train_items 175136.
I0304 19:30:43.567635 23118544486528 run.py:483] Algo bellman_ford step 5473 current loss 0.043528, current_train_items 175168.
I0304 19:30:43.602647 23118544486528 run.py:483] Algo bellman_ford step 5474 current loss 0.073185, current_train_items 175200.
I0304 19:30:43.622673 23118544486528 run.py:483] Algo bellman_ford step 5475 current loss 0.002715, current_train_items 175232.
I0304 19:30:43.639194 23118544486528 run.py:483] Algo bellman_ford step 5476 current loss 0.028960, current_train_items 175264.
I0304 19:30:43.662859 23118544486528 run.py:483] Algo bellman_ford step 5477 current loss 0.085389, current_train_items 175296.
I0304 19:30:43.694841 23118544486528 run.py:483] Algo bellman_ford step 5478 current loss 0.090039, current_train_items 175328.
I0304 19:30:43.728631 23118544486528 run.py:483] Algo bellman_ford step 5479 current loss 0.138319, current_train_items 175360.
I0304 19:30:43.748488 23118544486528 run.py:483] Algo bellman_ford step 5480 current loss 0.024826, current_train_items 175392.
I0304 19:30:43.765537 23118544486528 run.py:483] Algo bellman_ford step 5481 current loss 0.058012, current_train_items 175424.
I0304 19:30:43.791273 23118544486528 run.py:483] Algo bellman_ford step 5482 current loss 0.116214, current_train_items 175456.
I0304 19:30:43.823642 23118544486528 run.py:483] Algo bellman_ford step 5483 current loss 0.137769, current_train_items 175488.
I0304 19:30:43.859570 23118544486528 run.py:483] Algo bellman_ford step 5484 current loss 0.164265, current_train_items 175520.
I0304 19:30:43.879754 23118544486528 run.py:483] Algo bellman_ford step 5485 current loss 0.006320, current_train_items 175552.
I0304 19:30:43.895995 23118544486528 run.py:483] Algo bellman_ford step 5486 current loss 0.030998, current_train_items 175584.
I0304 19:30:43.919804 23118544486528 run.py:483] Algo bellman_ford step 5487 current loss 0.066783, current_train_items 175616.
I0304 19:30:43.952891 23118544486528 run.py:483] Algo bellman_ford step 5488 current loss 0.058348, current_train_items 175648.
I0304 19:30:43.985940 23118544486528 run.py:483] Algo bellman_ford step 5489 current loss 0.056141, current_train_items 175680.
I0304 19:30:44.005931 23118544486528 run.py:483] Algo bellman_ford step 5490 current loss 0.006540, current_train_items 175712.
I0304 19:30:44.022444 23118544486528 run.py:483] Algo bellman_ford step 5491 current loss 0.035924, current_train_items 175744.
I0304 19:30:44.046738 23118544486528 run.py:483] Algo bellman_ford step 5492 current loss 0.043143, current_train_items 175776.
I0304 19:30:44.078234 23118544486528 run.py:483] Algo bellman_ford step 5493 current loss 0.041327, current_train_items 175808.
I0304 19:30:44.111997 23118544486528 run.py:483] Algo bellman_ford step 5494 current loss 0.074903, current_train_items 175840.
I0304 19:30:44.131696 23118544486528 run.py:483] Algo bellman_ford step 5495 current loss 0.007192, current_train_items 175872.
I0304 19:30:44.148392 23118544486528 run.py:483] Algo bellman_ford step 5496 current loss 0.010596, current_train_items 175904.
I0304 19:30:44.172895 23118544486528 run.py:483] Algo bellman_ford step 5497 current loss 0.111543, current_train_items 175936.
I0304 19:30:44.204848 23118544486528 run.py:483] Algo bellman_ford step 5498 current loss 0.098592, current_train_items 175968.
I0304 19:30:44.240639 23118544486528 run.py:483] Algo bellman_ford step 5499 current loss 0.102227, current_train_items 176000.
I0304 19:30:44.260869 23118544486528 run.py:483] Algo bellman_ford step 5500 current loss 0.010778, current_train_items 176032.
I0304 19:30:44.268535 23118544486528 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0304 19:30:44.268641 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:44.286115 23118544486528 run.py:483] Algo bellman_ford step 5501 current loss 0.068312, current_train_items 176064.
I0304 19:30:44.311289 23118544486528 run.py:483] Algo bellman_ford step 5502 current loss 0.070409, current_train_items 176096.
I0304 19:30:44.343129 23118544486528 run.py:483] Algo bellman_ford step 5503 current loss 0.057628, current_train_items 176128.
I0304 19:30:44.378508 23118544486528 run.py:483] Algo bellman_ford step 5504 current loss 0.072199, current_train_items 176160.
I0304 19:30:44.398760 23118544486528 run.py:483] Algo bellman_ford step 5505 current loss 0.014945, current_train_items 176192.
I0304 19:30:44.415248 23118544486528 run.py:483] Algo bellman_ford step 5506 current loss 0.022031, current_train_items 176224.
I0304 19:30:44.439244 23118544486528 run.py:483] Algo bellman_ford step 5507 current loss 0.045701, current_train_items 176256.
I0304 19:30:44.469669 23118544486528 run.py:483] Algo bellman_ford step 5508 current loss 0.079165, current_train_items 176288.
I0304 19:30:44.503498 23118544486528 run.py:483] Algo bellman_ford step 5509 current loss 0.065337, current_train_items 176320.
I0304 19:30:44.523817 23118544486528 run.py:483] Algo bellman_ford step 5510 current loss 0.006724, current_train_items 176352.
I0304 19:30:44.540702 23118544486528 run.py:483] Algo bellman_ford step 5511 current loss 0.026705, current_train_items 176384.
I0304 19:30:44.566026 23118544486528 run.py:483] Algo bellman_ford step 5512 current loss 0.122962, current_train_items 176416.
I0304 19:30:44.596997 23118544486528 run.py:483] Algo bellman_ford step 5513 current loss 0.070139, current_train_items 176448.
I0304 19:30:44.629288 23118544486528 run.py:483] Algo bellman_ford step 5514 current loss 0.086152, current_train_items 176480.
I0304 19:30:44.649466 23118544486528 run.py:483] Algo bellman_ford step 5515 current loss 0.007582, current_train_items 176512.
I0304 19:30:44.665762 23118544486528 run.py:483] Algo bellman_ford step 5516 current loss 0.016044, current_train_items 176544.
I0304 19:30:44.690313 23118544486528 run.py:483] Algo bellman_ford step 5517 current loss 0.039332, current_train_items 176576.
I0304 19:30:44.721464 23118544486528 run.py:483] Algo bellman_ford step 5518 current loss 0.058372, current_train_items 176608.
I0304 19:30:44.757464 23118544486528 run.py:483] Algo bellman_ford step 5519 current loss 0.125971, current_train_items 176640.
I0304 19:30:44.777386 23118544486528 run.py:483] Algo bellman_ford step 5520 current loss 0.013634, current_train_items 176672.
I0304 19:30:44.794156 23118544486528 run.py:483] Algo bellman_ford step 5521 current loss 0.016620, current_train_items 176704.
I0304 19:30:44.817922 23118544486528 run.py:483] Algo bellman_ford step 5522 current loss 0.030845, current_train_items 176736.
I0304 19:30:44.850653 23118544486528 run.py:483] Algo bellman_ford step 5523 current loss 0.061554, current_train_items 176768.
I0304 19:30:44.886223 23118544486528 run.py:483] Algo bellman_ford step 5524 current loss 0.080837, current_train_items 176800.
I0304 19:30:44.905995 23118544486528 run.py:483] Algo bellman_ford step 5525 current loss 0.010536, current_train_items 176832.
I0304 19:30:44.922065 23118544486528 run.py:483] Algo bellman_ford step 5526 current loss 0.019137, current_train_items 176864.
I0304 19:30:44.947470 23118544486528 run.py:483] Algo bellman_ford step 5527 current loss 0.033174, current_train_items 176896.
I0304 19:30:44.978232 23118544486528 run.py:483] Algo bellman_ford step 5528 current loss 0.063293, current_train_items 176928.
I0304 19:30:45.011558 23118544486528 run.py:483] Algo bellman_ford step 5529 current loss 0.039895, current_train_items 176960.
I0304 19:30:45.031779 23118544486528 run.py:483] Algo bellman_ford step 5530 current loss 0.004848, current_train_items 176992.
I0304 19:30:45.048352 23118544486528 run.py:483] Algo bellman_ford step 5531 current loss 0.016169, current_train_items 177024.
I0304 19:30:45.073020 23118544486528 run.py:483] Algo bellman_ford step 5532 current loss 0.050255, current_train_items 177056.
I0304 19:30:45.104163 23118544486528 run.py:483] Algo bellman_ford step 5533 current loss 0.049954, current_train_items 177088.
I0304 19:30:45.140353 23118544486528 run.py:483] Algo bellman_ford step 5534 current loss 0.097500, current_train_items 177120.
I0304 19:30:45.159986 23118544486528 run.py:483] Algo bellman_ford step 5535 current loss 0.006453, current_train_items 177152.
I0304 19:30:45.176674 23118544486528 run.py:483] Algo bellman_ford step 5536 current loss 0.074826, current_train_items 177184.
I0304 19:30:45.200196 23118544486528 run.py:483] Algo bellman_ford step 5537 current loss 0.056045, current_train_items 177216.
I0304 19:30:45.230999 23118544486528 run.py:483] Algo bellman_ford step 5538 current loss 0.055228, current_train_items 177248.
I0304 19:30:45.267177 23118544486528 run.py:483] Algo bellman_ford step 5539 current loss 0.048528, current_train_items 177280.
I0304 19:30:45.287033 23118544486528 run.py:483] Algo bellman_ford step 5540 current loss 0.003532, current_train_items 177312.
I0304 19:30:45.303893 23118544486528 run.py:483] Algo bellman_ford step 5541 current loss 0.022992, current_train_items 177344.
I0304 19:30:45.328999 23118544486528 run.py:483] Algo bellman_ford step 5542 current loss 0.058926, current_train_items 177376.
I0304 19:30:45.360331 23118544486528 run.py:483] Algo bellman_ford step 5543 current loss 0.041236, current_train_items 177408.
I0304 19:30:45.394533 23118544486528 run.py:483] Algo bellman_ford step 5544 current loss 0.124441, current_train_items 177440.
I0304 19:30:45.414488 23118544486528 run.py:483] Algo bellman_ford step 5545 current loss 0.006171, current_train_items 177472.
I0304 19:30:45.431540 23118544486528 run.py:483] Algo bellman_ford step 5546 current loss 0.045667, current_train_items 177504.
I0304 19:30:45.455803 23118544486528 run.py:483] Algo bellman_ford step 5547 current loss 0.027875, current_train_items 177536.
I0304 19:30:45.487337 23118544486528 run.py:483] Algo bellman_ford step 5548 current loss 0.032544, current_train_items 177568.
I0304 19:30:45.520291 23118544486528 run.py:483] Algo bellman_ford step 5549 current loss 0.086303, current_train_items 177600.
I0304 19:30:45.540356 23118544486528 run.py:483] Algo bellman_ford step 5550 current loss 0.003884, current_train_items 177632.
I0304 19:30:45.548370 23118544486528 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0304 19:30:45.548474 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:45.565552 23118544486528 run.py:483] Algo bellman_ford step 5551 current loss 0.012401, current_train_items 177664.
I0304 19:30:45.591449 23118544486528 run.py:483] Algo bellman_ford step 5552 current loss 0.057799, current_train_items 177696.
I0304 19:30:45.622915 23118544486528 run.py:483] Algo bellman_ford step 5553 current loss 0.041834, current_train_items 177728.
I0304 19:30:45.659719 23118544486528 run.py:483] Algo bellman_ford step 5554 current loss 0.090375, current_train_items 177760.
I0304 19:30:45.680159 23118544486528 run.py:483] Algo bellman_ford step 5555 current loss 0.007985, current_train_items 177792.
I0304 19:30:45.696482 23118544486528 run.py:483] Algo bellman_ford step 5556 current loss 0.054295, current_train_items 177824.
I0304 19:30:45.721316 23118544486528 run.py:483] Algo bellman_ford step 5557 current loss 0.093725, current_train_items 177856.
I0304 19:30:45.752296 23118544486528 run.py:483] Algo bellman_ford step 5558 current loss 0.104938, current_train_items 177888.
I0304 19:30:45.787118 23118544486528 run.py:483] Algo bellman_ford step 5559 current loss 0.067786, current_train_items 177920.
I0304 19:30:45.807535 23118544486528 run.py:483] Algo bellman_ford step 5560 current loss 0.038599, current_train_items 177952.
I0304 19:30:45.823926 23118544486528 run.py:483] Algo bellman_ford step 5561 current loss 0.032773, current_train_items 177984.
I0304 19:30:45.847982 23118544486528 run.py:483] Algo bellman_ford step 5562 current loss 0.073126, current_train_items 178016.
I0304 19:30:45.880299 23118544486528 run.py:483] Algo bellman_ford step 5563 current loss 0.061101, current_train_items 178048.
I0304 19:30:45.915871 23118544486528 run.py:483] Algo bellman_ford step 5564 current loss 0.082435, current_train_items 178080.
I0304 19:30:45.935714 23118544486528 run.py:483] Algo bellman_ford step 5565 current loss 0.006636, current_train_items 178112.
I0304 19:30:45.952136 23118544486528 run.py:483] Algo bellman_ford step 5566 current loss 0.059620, current_train_items 178144.
I0304 19:30:45.976844 23118544486528 run.py:483] Algo bellman_ford step 5567 current loss 0.052820, current_train_items 178176.
I0304 19:30:46.009802 23118544486528 run.py:483] Algo bellman_ford step 5568 current loss 0.084308, current_train_items 178208.
I0304 19:30:46.044553 23118544486528 run.py:483] Algo bellman_ford step 5569 current loss 0.117954, current_train_items 178240.
I0304 19:30:46.064926 23118544486528 run.py:483] Algo bellman_ford step 5570 current loss 0.020132, current_train_items 178272.
I0304 19:30:46.081234 23118544486528 run.py:483] Algo bellman_ford step 5571 current loss 0.031639, current_train_items 178304.
I0304 19:30:46.105130 23118544486528 run.py:483] Algo bellman_ford step 5572 current loss 0.052557, current_train_items 178336.
I0304 19:30:46.137094 23118544486528 run.py:483] Algo bellman_ford step 5573 current loss 0.106188, current_train_items 178368.
I0304 19:30:46.171721 23118544486528 run.py:483] Algo bellman_ford step 5574 current loss 0.112287, current_train_items 178400.
I0304 19:30:46.191994 23118544486528 run.py:483] Algo bellman_ford step 5575 current loss 0.004918, current_train_items 178432.
I0304 19:30:46.208389 23118544486528 run.py:483] Algo bellman_ford step 5576 current loss 0.019021, current_train_items 178464.
I0304 19:30:46.231542 23118544486528 run.py:483] Algo bellman_ford step 5577 current loss 0.039382, current_train_items 178496.
I0304 19:30:46.263547 23118544486528 run.py:483] Algo bellman_ford step 5578 current loss 0.059374, current_train_items 178528.
I0304 19:30:46.299717 23118544486528 run.py:483] Algo bellman_ford step 5579 current loss 0.074171, current_train_items 178560.
I0304 19:30:46.320019 23118544486528 run.py:483] Algo bellman_ford step 5580 current loss 0.005709, current_train_items 178592.
I0304 19:30:46.336082 23118544486528 run.py:483] Algo bellman_ford step 5581 current loss 0.022267, current_train_items 178624.
I0304 19:30:46.361157 23118544486528 run.py:483] Algo bellman_ford step 5582 current loss 0.036630, current_train_items 178656.
I0304 19:30:46.392464 23118544486528 run.py:483] Algo bellman_ford step 5583 current loss 0.055520, current_train_items 178688.
I0304 19:30:46.426922 23118544486528 run.py:483] Algo bellman_ford step 5584 current loss 0.081386, current_train_items 178720.
I0304 19:30:46.447060 23118544486528 run.py:483] Algo bellman_ford step 5585 current loss 0.006819, current_train_items 178752.
I0304 19:30:46.463655 23118544486528 run.py:483] Algo bellman_ford step 5586 current loss 0.017065, current_train_items 178784.
I0304 19:30:46.489120 23118544486528 run.py:483] Algo bellman_ford step 5587 current loss 0.057408, current_train_items 178816.
I0304 19:30:46.521750 23118544486528 run.py:483] Algo bellman_ford step 5588 current loss 0.066125, current_train_items 178848.
I0304 19:30:46.553919 23118544486528 run.py:483] Algo bellman_ford step 5589 current loss 0.074444, current_train_items 178880.
I0304 19:30:46.574157 23118544486528 run.py:483] Algo bellman_ford step 5590 current loss 0.005193, current_train_items 178912.
I0304 19:30:46.590766 23118544486528 run.py:483] Algo bellman_ford step 5591 current loss 0.022983, current_train_items 178944.
I0304 19:30:46.614400 23118544486528 run.py:483] Algo bellman_ford step 5592 current loss 0.071463, current_train_items 178976.
I0304 19:30:46.646263 23118544486528 run.py:483] Algo bellman_ford step 5593 current loss 0.064625, current_train_items 179008.
I0304 19:30:46.681494 23118544486528 run.py:483] Algo bellman_ford step 5594 current loss 0.043670, current_train_items 179040.
I0304 19:30:46.701574 23118544486528 run.py:483] Algo bellman_ford step 5595 current loss 0.027377, current_train_items 179072.
I0304 19:30:46.718007 23118544486528 run.py:483] Algo bellman_ford step 5596 current loss 0.023503, current_train_items 179104.
I0304 19:30:46.742470 23118544486528 run.py:483] Algo bellman_ford step 5597 current loss 0.056359, current_train_items 179136.
I0304 19:30:46.774580 23118544486528 run.py:483] Algo bellman_ford step 5598 current loss 0.089779, current_train_items 179168.
I0304 19:30:46.804826 23118544486528 run.py:483] Algo bellman_ford step 5599 current loss 0.055319, current_train_items 179200.
I0304 19:30:46.825237 23118544486528 run.py:483] Algo bellman_ford step 5600 current loss 0.006903, current_train_items 179232.
I0304 19:30:46.832726 23118544486528 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0304 19:30:46.832832 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:46.849811 23118544486528 run.py:483] Algo bellman_ford step 5601 current loss 0.021102, current_train_items 179264.
I0304 19:30:46.874700 23118544486528 run.py:483] Algo bellman_ford step 5602 current loss 0.074521, current_train_items 179296.
I0304 19:30:46.906457 23118544486528 run.py:483] Algo bellman_ford step 5603 current loss 0.073344, current_train_items 179328.
I0304 19:30:46.939455 23118544486528 run.py:483] Algo bellman_ford step 5604 current loss 0.048037, current_train_items 179360.
I0304 19:30:46.959920 23118544486528 run.py:483] Algo bellman_ford step 5605 current loss 0.004420, current_train_items 179392.
I0304 19:30:46.975867 23118544486528 run.py:483] Algo bellman_ford step 5606 current loss 0.052438, current_train_items 179424.
I0304 19:30:47.000196 23118544486528 run.py:483] Algo bellman_ford step 5607 current loss 0.043836, current_train_items 179456.
I0304 19:30:47.030987 23118544486528 run.py:483] Algo bellman_ford step 5608 current loss 0.034267, current_train_items 179488.
I0304 19:30:47.065606 23118544486528 run.py:483] Algo bellman_ford step 5609 current loss 0.120298, current_train_items 179520.
I0304 19:30:47.085556 23118544486528 run.py:483] Algo bellman_ford step 5610 current loss 0.005744, current_train_items 179552.
I0304 19:30:47.102269 23118544486528 run.py:483] Algo bellman_ford step 5611 current loss 0.019655, current_train_items 179584.
I0304 19:30:47.126250 23118544486528 run.py:483] Algo bellman_ford step 5612 current loss 0.060245, current_train_items 179616.
I0304 19:30:47.158656 23118544486528 run.py:483] Algo bellman_ford step 5613 current loss 0.053071, current_train_items 179648.
I0304 19:30:47.191622 23118544486528 run.py:483] Algo bellman_ford step 5614 current loss 0.057252, current_train_items 179680.
I0304 19:30:47.211624 23118544486528 run.py:483] Algo bellman_ford step 5615 current loss 0.006862, current_train_items 179712.
I0304 19:30:47.228574 23118544486528 run.py:483] Algo bellman_ford step 5616 current loss 0.021523, current_train_items 179744.
I0304 19:30:47.252985 23118544486528 run.py:483] Algo bellman_ford step 5617 current loss 0.022647, current_train_items 179776.
I0304 19:30:47.283917 23118544486528 run.py:483] Algo bellman_ford step 5618 current loss 0.039571, current_train_items 179808.
I0304 19:30:47.319387 23118544486528 run.py:483] Algo bellman_ford step 5619 current loss 0.111580, current_train_items 179840.
I0304 19:30:47.339070 23118544486528 run.py:483] Algo bellman_ford step 5620 current loss 0.005330, current_train_items 179872.
I0304 19:30:47.355589 23118544486528 run.py:483] Algo bellman_ford step 5621 current loss 0.019363, current_train_items 179904.
I0304 19:30:47.379431 23118544486528 run.py:483] Algo bellman_ford step 5622 current loss 0.047328, current_train_items 179936.
I0304 19:30:47.410620 23118544486528 run.py:483] Algo bellman_ford step 5623 current loss 0.084336, current_train_items 179968.
I0304 19:30:47.445401 23118544486528 run.py:483] Algo bellman_ford step 5624 current loss 0.065226, current_train_items 180000.
I0304 19:30:47.465009 23118544486528 run.py:483] Algo bellman_ford step 5625 current loss 0.002785, current_train_items 180032.
I0304 19:30:47.481828 23118544486528 run.py:483] Algo bellman_ford step 5626 current loss 0.039255, current_train_items 180064.
I0304 19:30:47.506320 23118544486528 run.py:483] Algo bellman_ford step 5627 current loss 0.049238, current_train_items 180096.
I0304 19:30:47.538433 23118544486528 run.py:483] Algo bellman_ford step 5628 current loss 0.087827, current_train_items 180128.
I0304 19:30:47.573328 23118544486528 run.py:483] Algo bellman_ford step 5629 current loss 0.061292, current_train_items 180160.
I0304 19:30:47.592971 23118544486528 run.py:483] Algo bellman_ford step 5630 current loss 0.006904, current_train_items 180192.
I0304 19:30:47.609270 23118544486528 run.py:483] Algo bellman_ford step 5631 current loss 0.009308, current_train_items 180224.
I0304 19:30:47.634054 23118544486528 run.py:483] Algo bellman_ford step 5632 current loss 0.072690, current_train_items 180256.
I0304 19:30:47.665813 23118544486528 run.py:483] Algo bellman_ford step 5633 current loss 0.046306, current_train_items 180288.
I0304 19:30:47.700399 23118544486528 run.py:483] Algo bellman_ford step 5634 current loss 0.096592, current_train_items 180320.
I0304 19:30:47.720231 23118544486528 run.py:483] Algo bellman_ford step 5635 current loss 0.009462, current_train_items 180352.
I0304 19:30:47.736799 23118544486528 run.py:483] Algo bellman_ford step 5636 current loss 0.017090, current_train_items 180384.
I0304 19:30:47.760721 23118544486528 run.py:483] Algo bellman_ford step 5637 current loss 0.029684, current_train_items 180416.
I0304 19:30:47.791147 23118544486528 run.py:483] Algo bellman_ford step 5638 current loss 0.079626, current_train_items 180448.
I0304 19:30:47.823874 23118544486528 run.py:483] Algo bellman_ford step 5639 current loss 0.119967, current_train_items 180480.
I0304 19:30:47.843626 23118544486528 run.py:483] Algo bellman_ford step 5640 current loss 0.005020, current_train_items 180512.
I0304 19:30:47.859929 23118544486528 run.py:483] Algo bellman_ford step 5641 current loss 0.020453, current_train_items 180544.
I0304 19:30:47.883856 23118544486528 run.py:483] Algo bellman_ford step 5642 current loss 0.050781, current_train_items 180576.
I0304 19:30:47.915859 23118544486528 run.py:483] Algo bellman_ford step 5643 current loss 0.058216, current_train_items 180608.
I0304 19:30:47.949579 23118544486528 run.py:483] Algo bellman_ford step 5644 current loss 0.056813, current_train_items 180640.
I0304 19:30:47.969152 23118544486528 run.py:483] Algo bellman_ford step 5645 current loss 0.004056, current_train_items 180672.
I0304 19:30:47.986321 23118544486528 run.py:483] Algo bellman_ford step 5646 current loss 0.024306, current_train_items 180704.
I0304 19:30:48.010148 23118544486528 run.py:483] Algo bellman_ford step 5647 current loss 0.038591, current_train_items 180736.
I0304 19:30:48.041500 23118544486528 run.py:483] Algo bellman_ford step 5648 current loss 0.060421, current_train_items 180768.
I0304 19:30:48.077332 23118544486528 run.py:483] Algo bellman_ford step 5649 current loss 0.082130, current_train_items 180800.
I0304 19:30:48.097332 23118544486528 run.py:483] Algo bellman_ford step 5650 current loss 0.023944, current_train_items 180832.
I0304 19:30:48.105600 23118544486528 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0304 19:30:48.105717 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:30:48.122845 23118544486528 run.py:483] Algo bellman_ford step 5651 current loss 0.012587, current_train_items 180864.
I0304 19:30:48.147198 23118544486528 run.py:483] Algo bellman_ford step 5652 current loss 0.056884, current_train_items 180896.
I0304 19:30:48.180950 23118544486528 run.py:483] Algo bellman_ford step 5653 current loss 0.070194, current_train_items 180928.
I0304 19:30:48.217079 23118544486528 run.py:483] Algo bellman_ford step 5654 current loss 0.082284, current_train_items 180960.
I0304 19:30:48.236957 23118544486528 run.py:483] Algo bellman_ford step 5655 current loss 0.004629, current_train_items 180992.
I0304 19:30:48.253460 23118544486528 run.py:483] Algo bellman_ford step 5656 current loss 0.010820, current_train_items 181024.
I0304 19:30:48.276910 23118544486528 run.py:483] Algo bellman_ford step 5657 current loss 0.031305, current_train_items 181056.
I0304 19:30:48.309934 23118544486528 run.py:483] Algo bellman_ford step 5658 current loss 0.047907, current_train_items 181088.
I0304 19:30:48.344484 23118544486528 run.py:483] Algo bellman_ford step 5659 current loss 0.074951, current_train_items 181120.
I0304 19:30:48.364728 23118544486528 run.py:483] Algo bellman_ford step 5660 current loss 0.017139, current_train_items 181152.
I0304 19:30:48.380975 23118544486528 run.py:483] Algo bellman_ford step 5661 current loss 0.013676, current_train_items 181184.
I0304 19:30:48.404542 23118544486528 run.py:483] Algo bellman_ford step 5662 current loss 0.054387, current_train_items 181216.
I0304 19:30:48.438277 23118544486528 run.py:483] Algo bellman_ford step 5663 current loss 0.074762, current_train_items 181248.
I0304 19:30:48.473318 23118544486528 run.py:483] Algo bellman_ford step 5664 current loss 0.041798, current_train_items 181280.
I0304 19:30:48.493360 23118544486528 run.py:483] Algo bellman_ford step 5665 current loss 0.013470, current_train_items 181312.
I0304 19:30:48.510238 23118544486528 run.py:483] Algo bellman_ford step 5666 current loss 0.015956, current_train_items 181344.
I0304 19:30:48.533879 23118544486528 run.py:483] Algo bellman_ford step 5667 current loss 0.058982, current_train_items 181376.
I0304 19:30:48.566820 23118544486528 run.py:483] Algo bellman_ford step 5668 current loss 0.094228, current_train_items 181408.
I0304 19:30:48.600260 23118544486528 run.py:483] Algo bellman_ford step 5669 current loss 0.063360, current_train_items 181440.
I0304 19:30:48.620491 23118544486528 run.py:483] Algo bellman_ford step 5670 current loss 0.003856, current_train_items 181472.
I0304 19:30:48.637103 23118544486528 run.py:483] Algo bellman_ford step 5671 current loss 0.010375, current_train_items 181504.
I0304 19:30:48.660634 23118544486528 run.py:483] Algo bellman_ford step 5672 current loss 0.050181, current_train_items 181536.
I0304 19:30:48.692108 23118544486528 run.py:483] Algo bellman_ford step 5673 current loss 0.062815, current_train_items 181568.
I0304 19:30:48.725772 23118544486528 run.py:483] Algo bellman_ford step 5674 current loss 0.071736, current_train_items 181600.
I0304 19:30:48.745913 23118544486528 run.py:483] Algo bellman_ford step 5675 current loss 0.003909, current_train_items 181632.
I0304 19:30:48.762355 23118544486528 run.py:483] Algo bellman_ford step 5676 current loss 0.015973, current_train_items 181664.
I0304 19:30:48.786540 23118544486528 run.py:483] Algo bellman_ford step 5677 current loss 0.062646, current_train_items 181696.
I0304 19:30:48.819857 23118544486528 run.py:483] Algo bellman_ford step 5678 current loss 0.154278, current_train_items 181728.
I0304 19:30:48.854238 23118544486528 run.py:483] Algo bellman_ford step 5679 current loss 0.143843, current_train_items 181760.
I0304 19:30:48.874124 23118544486528 run.py:483] Algo bellman_ford step 5680 current loss 0.005391, current_train_items 181792.
I0304 19:30:48.890706 23118544486528 run.py:483] Algo bellman_ford step 5681 current loss 0.018797, current_train_items 181824.
I0304 19:30:48.915000 23118544486528 run.py:483] Algo bellman_ford step 5682 current loss 0.067559, current_train_items 181856.
I0304 19:30:48.946538 23118544486528 run.py:483] Algo bellman_ford step 5683 current loss 0.103127, current_train_items 181888.
I0304 19:30:48.979673 23118544486528 run.py:483] Algo bellman_ford step 5684 current loss 0.082291, current_train_items 181920.
I0304 19:30:48.999742 23118544486528 run.py:483] Algo bellman_ford step 5685 current loss 0.009790, current_train_items 181952.
I0304 19:30:49.016219 23118544486528 run.py:483] Algo bellman_ford step 5686 current loss 0.020701, current_train_items 181984.
I0304 19:30:49.040788 23118544486528 run.py:483] Algo bellman_ford step 5687 current loss 0.070251, current_train_items 182016.
I0304 19:30:49.073214 23118544486528 run.py:483] Algo bellman_ford step 5688 current loss 0.118558, current_train_items 182048.
I0304 19:30:49.106716 23118544486528 run.py:483] Algo bellman_ford step 5689 current loss 0.083162, current_train_items 182080.
I0304 19:30:49.126766 23118544486528 run.py:483] Algo bellman_ford step 5690 current loss 0.004472, current_train_items 182112.
I0304 19:30:49.143902 23118544486528 run.py:483] Algo bellman_ford step 5691 current loss 0.032393, current_train_items 182144.
I0304 19:30:49.168328 23118544486528 run.py:483] Algo bellman_ford step 5692 current loss 0.054294, current_train_items 182176.
I0304 19:30:49.200022 23118544486528 run.py:483] Algo bellman_ford step 5693 current loss 0.042079, current_train_items 182208.
I0304 19:30:49.233222 23118544486528 run.py:483] Algo bellman_ford step 5694 current loss 0.074454, current_train_items 182240.
I0304 19:30:49.253064 23118544486528 run.py:483] Algo bellman_ford step 5695 current loss 0.004522, current_train_items 182272.
I0304 19:30:49.269773 23118544486528 run.py:483] Algo bellman_ford step 5696 current loss 0.028727, current_train_items 182304.
I0304 19:30:49.293760 23118544486528 run.py:483] Algo bellman_ford step 5697 current loss 0.063618, current_train_items 182336.
I0304 19:30:49.326487 23118544486528 run.py:483] Algo bellman_ford step 5698 current loss 0.070211, current_train_items 182368.
I0304 19:30:49.361524 23118544486528 run.py:483] Algo bellman_ford step 5699 current loss 0.086172, current_train_items 182400.
I0304 19:30:49.381596 23118544486528 run.py:483] Algo bellman_ford step 5700 current loss 0.008443, current_train_items 182432.
I0304 19:30:49.389570 23118544486528 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0304 19:30:49.389679 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:49.406333 23118544486528 run.py:483] Algo bellman_ford step 5701 current loss 0.021471, current_train_items 182464.
I0304 19:30:49.429860 23118544486528 run.py:483] Algo bellman_ford step 5702 current loss 0.022098, current_train_items 182496.
I0304 19:30:49.461966 23118544486528 run.py:483] Algo bellman_ford step 5703 current loss 0.032293, current_train_items 182528.
I0304 19:30:49.497662 23118544486528 run.py:483] Algo bellman_ford step 5704 current loss 0.059615, current_train_items 182560.
I0304 19:30:49.518294 23118544486528 run.py:483] Algo bellman_ford step 5705 current loss 0.004243, current_train_items 182592.
I0304 19:30:49.534390 23118544486528 run.py:483] Algo bellman_ford step 5706 current loss 0.009757, current_train_items 182624.
I0304 19:30:49.559096 23118544486528 run.py:483] Algo bellman_ford step 5707 current loss 0.043429, current_train_items 182656.
I0304 19:30:49.590452 23118544486528 run.py:483] Algo bellman_ford step 5708 current loss 0.053680, current_train_items 182688.
I0304 19:30:49.625025 23118544486528 run.py:483] Algo bellman_ford step 5709 current loss 0.060022, current_train_items 182720.
I0304 19:30:49.644864 23118544486528 run.py:483] Algo bellman_ford step 5710 current loss 0.004268, current_train_items 182752.
I0304 19:30:49.661158 23118544486528 run.py:483] Algo bellman_ford step 5711 current loss 0.011442, current_train_items 182784.
I0304 19:30:49.684605 23118544486528 run.py:483] Algo bellman_ford step 5712 current loss 0.061733, current_train_items 182816.
I0304 19:30:49.715500 23118544486528 run.py:483] Algo bellman_ford step 5713 current loss 0.072755, current_train_items 182848.
I0304 19:30:49.749356 23118544486528 run.py:483] Algo bellman_ford step 5714 current loss 0.048558, current_train_items 182880.
I0304 19:30:49.769197 23118544486528 run.py:483] Algo bellman_ford step 5715 current loss 0.008471, current_train_items 182912.
I0304 19:30:49.785591 23118544486528 run.py:483] Algo bellman_ford step 5716 current loss 0.016575, current_train_items 182944.
I0304 19:30:49.810262 23118544486528 run.py:483] Algo bellman_ford step 5717 current loss 0.038485, current_train_items 182976.
I0304 19:30:49.841672 23118544486528 run.py:483] Algo bellman_ford step 5718 current loss 0.087646, current_train_items 183008.
I0304 19:30:49.876851 23118544486528 run.py:483] Algo bellman_ford step 5719 current loss 0.064622, current_train_items 183040.
I0304 19:30:49.896957 23118544486528 run.py:483] Algo bellman_ford step 5720 current loss 0.004538, current_train_items 183072.
I0304 19:30:49.913384 23118544486528 run.py:483] Algo bellman_ford step 5721 current loss 0.021337, current_train_items 183104.
I0304 19:30:49.937368 23118544486528 run.py:483] Algo bellman_ford step 5722 current loss 0.073784, current_train_items 183136.
I0304 19:30:49.968436 23118544486528 run.py:483] Algo bellman_ford step 5723 current loss 0.085557, current_train_items 183168.
I0304 19:30:50.001748 23118544486528 run.py:483] Algo bellman_ford step 5724 current loss 0.089335, current_train_items 183200.
I0304 19:30:50.021769 23118544486528 run.py:483] Algo bellman_ford step 5725 current loss 0.004373, current_train_items 183232.
I0304 19:30:50.038390 23118544486528 run.py:483] Algo bellman_ford step 5726 current loss 0.029444, current_train_items 183264.
I0304 19:30:50.063617 23118544486528 run.py:483] Algo bellman_ford step 5727 current loss 0.083632, current_train_items 183296.
I0304 19:30:50.094321 23118544486528 run.py:483] Algo bellman_ford step 5728 current loss 0.053203, current_train_items 183328.
I0304 19:30:50.128546 23118544486528 run.py:483] Algo bellman_ford step 5729 current loss 0.093932, current_train_items 183360.
I0304 19:30:50.148282 23118544486528 run.py:483] Algo bellman_ford step 5730 current loss 0.003890, current_train_items 183392.
I0304 19:30:50.165465 23118544486528 run.py:483] Algo bellman_ford step 5731 current loss 0.007939, current_train_items 183424.
I0304 19:30:50.190211 23118544486528 run.py:483] Algo bellman_ford step 5732 current loss 0.099403, current_train_items 183456.
I0304 19:30:50.221522 23118544486528 run.py:483] Algo bellman_ford step 5733 current loss 0.114000, current_train_items 183488.
I0304 19:30:50.256947 23118544486528 run.py:483] Algo bellman_ford step 5734 current loss 0.090512, current_train_items 183520.
I0304 19:30:50.276839 23118544486528 run.py:483] Algo bellman_ford step 5735 current loss 0.005131, current_train_items 183552.
I0304 19:30:50.293580 23118544486528 run.py:483] Algo bellman_ford step 5736 current loss 0.020884, current_train_items 183584.
I0304 19:30:50.317902 23118544486528 run.py:483] Algo bellman_ford step 5737 current loss 0.050183, current_train_items 183616.
I0304 19:30:50.348958 23118544486528 run.py:483] Algo bellman_ford step 5738 current loss 0.043673, current_train_items 183648.
I0304 19:30:50.383625 23118544486528 run.py:483] Algo bellman_ford step 5739 current loss 0.096836, current_train_items 183680.
I0304 19:30:50.403658 23118544486528 run.py:483] Algo bellman_ford step 5740 current loss 0.038431, current_train_items 183712.
I0304 19:30:50.420467 23118544486528 run.py:483] Algo bellman_ford step 5741 current loss 0.016298, current_train_items 183744.
I0304 19:30:50.445591 23118544486528 run.py:483] Algo bellman_ford step 5742 current loss 0.051569, current_train_items 183776.
I0304 19:30:50.478229 23118544486528 run.py:483] Algo bellman_ford step 5743 current loss 0.063137, current_train_items 183808.
I0304 19:30:50.513434 23118544486528 run.py:483] Algo bellman_ford step 5744 current loss 0.086879, current_train_items 183840.
I0304 19:30:50.533405 23118544486528 run.py:483] Algo bellman_ford step 5745 current loss 0.051604, current_train_items 183872.
I0304 19:30:50.550409 23118544486528 run.py:483] Algo bellman_ford step 5746 current loss 0.018669, current_train_items 183904.
I0304 19:30:50.575382 23118544486528 run.py:483] Algo bellman_ford step 5747 current loss 0.060609, current_train_items 183936.
I0304 19:30:50.607119 23118544486528 run.py:483] Algo bellman_ford step 5748 current loss 0.082216, current_train_items 183968.
I0304 19:30:50.641344 23118544486528 run.py:483] Algo bellman_ford step 5749 current loss 0.051627, current_train_items 184000.
I0304 19:30:50.661447 23118544486528 run.py:483] Algo bellman_ford step 5750 current loss 0.007143, current_train_items 184032.
I0304 19:30:50.669474 23118544486528 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0304 19:30:50.669578 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:30:50.687095 23118544486528 run.py:483] Algo bellman_ford step 5751 current loss 0.010392, current_train_items 184064.
I0304 19:30:50.711404 23118544486528 run.py:483] Algo bellman_ford step 5752 current loss 0.082258, current_train_items 184096.
I0304 19:30:50.742311 23118544486528 run.py:483] Algo bellman_ford step 5753 current loss 0.061237, current_train_items 184128.
I0304 19:30:50.778172 23118544486528 run.py:483] Algo bellman_ford step 5754 current loss 0.089237, current_train_items 184160.
I0304 19:30:50.798747 23118544486528 run.py:483] Algo bellman_ford step 5755 current loss 0.006549, current_train_items 184192.
I0304 19:30:50.814910 23118544486528 run.py:483] Algo bellman_ford step 5756 current loss 0.021430, current_train_items 184224.
I0304 19:30:50.839705 23118544486528 run.py:483] Algo bellman_ford step 5757 current loss 0.063848, current_train_items 184256.
I0304 19:30:50.872484 23118544486528 run.py:483] Algo bellman_ford step 5758 current loss 0.055844, current_train_items 184288.
I0304 19:30:50.907409 23118544486528 run.py:483] Algo bellman_ford step 5759 current loss 0.038936, current_train_items 184320.
I0304 19:30:50.927264 23118544486528 run.py:483] Algo bellman_ford step 5760 current loss 0.012866, current_train_items 184352.
I0304 19:30:50.944067 23118544486528 run.py:483] Algo bellman_ford step 5761 current loss 0.030202, current_train_items 184384.
I0304 19:30:50.968221 23118544486528 run.py:483] Algo bellman_ford step 5762 current loss 0.034708, current_train_items 184416.
I0304 19:30:50.998950 23118544486528 run.py:483] Algo bellman_ford step 5763 current loss 0.061988, current_train_items 184448.
I0304 19:30:51.034088 23118544486528 run.py:483] Algo bellman_ford step 5764 current loss 0.050427, current_train_items 184480.
I0304 19:30:51.054437 23118544486528 run.py:483] Algo bellman_ford step 5765 current loss 0.014196, current_train_items 184512.
I0304 19:30:51.070904 23118544486528 run.py:483] Algo bellman_ford step 5766 current loss 0.014580, current_train_items 184544.
I0304 19:30:51.095171 23118544486528 run.py:483] Algo bellman_ford step 5767 current loss 0.022328, current_train_items 184576.
I0304 19:30:51.127862 23118544486528 run.py:483] Algo bellman_ford step 5768 current loss 0.048081, current_train_items 184608.
I0304 19:30:51.161789 23118544486528 run.py:483] Algo bellman_ford step 5769 current loss 0.068938, current_train_items 184640.
I0304 19:30:51.181946 23118544486528 run.py:483] Algo bellman_ford step 5770 current loss 0.005034, current_train_items 184672.
I0304 19:30:51.198622 23118544486528 run.py:483] Algo bellman_ford step 5771 current loss 0.034469, current_train_items 184704.
I0304 19:30:51.221379 23118544486528 run.py:483] Algo bellman_ford step 5772 current loss 0.031243, current_train_items 184736.
I0304 19:30:51.252634 23118544486528 run.py:483] Algo bellman_ford step 5773 current loss 0.061043, current_train_items 184768.
I0304 19:30:51.285829 23118544486528 run.py:483] Algo bellman_ford step 5774 current loss 0.060078, current_train_items 184800.
I0304 19:30:51.305869 23118544486528 run.py:483] Algo bellman_ford step 5775 current loss 0.005343, current_train_items 184832.
I0304 19:30:51.322668 23118544486528 run.py:483] Algo bellman_ford step 5776 current loss 0.049517, current_train_items 184864.
I0304 19:30:51.345828 23118544486528 run.py:483] Algo bellman_ford step 5777 current loss 0.042758, current_train_items 184896.
I0304 19:30:51.377680 23118544486528 run.py:483] Algo bellman_ford step 5778 current loss 0.048804, current_train_items 184928.
I0304 19:30:51.412764 23118544486528 run.py:483] Algo bellman_ford step 5779 current loss 0.131895, current_train_items 184960.
I0304 19:30:51.432603 23118544486528 run.py:483] Algo bellman_ford step 5780 current loss 0.013505, current_train_items 184992.
I0304 19:30:51.448935 23118544486528 run.py:483] Algo bellman_ford step 5781 current loss 0.033397, current_train_items 185024.
I0304 19:30:51.473771 23118544486528 run.py:483] Algo bellman_ford step 5782 current loss 0.040612, current_train_items 185056.
I0304 19:30:51.504546 23118544486528 run.py:483] Algo bellman_ford step 5783 current loss 0.029460, current_train_items 185088.
I0304 19:30:51.536757 23118544486528 run.py:483] Algo bellman_ford step 5784 current loss 0.088406, current_train_items 185120.
I0304 19:30:51.557028 23118544486528 run.py:483] Algo bellman_ford step 5785 current loss 0.006962, current_train_items 185152.
I0304 19:30:51.573442 23118544486528 run.py:483] Algo bellman_ford step 5786 current loss 0.050336, current_train_items 185184.
I0304 19:30:51.596313 23118544486528 run.py:483] Algo bellman_ford step 5787 current loss 0.040141, current_train_items 185216.
I0304 19:30:51.628484 23118544486528 run.py:483] Algo bellman_ford step 5788 current loss 0.064664, current_train_items 185248.
I0304 19:30:51.661394 23118544486528 run.py:483] Algo bellman_ford step 5789 current loss 0.104375, current_train_items 185280.
I0304 19:30:51.681394 23118544486528 run.py:483] Algo bellman_ford step 5790 current loss 0.010800, current_train_items 185312.
I0304 19:30:51.698336 23118544486528 run.py:483] Algo bellman_ford step 5791 current loss 0.055173, current_train_items 185344.
I0304 19:30:51.722039 23118544486528 run.py:483] Algo bellman_ford step 5792 current loss 0.059781, current_train_items 185376.
I0304 19:30:51.754006 23118544486528 run.py:483] Algo bellman_ford step 5793 current loss 0.067995, current_train_items 185408.
I0304 19:30:51.788220 23118544486528 run.py:483] Algo bellman_ford step 5794 current loss 0.067103, current_train_items 185440.
I0304 19:30:51.808349 23118544486528 run.py:483] Algo bellman_ford step 5795 current loss 0.005426, current_train_items 185472.
I0304 19:30:51.824674 23118544486528 run.py:483] Algo bellman_ford step 5796 current loss 0.012600, current_train_items 185504.
I0304 19:30:51.849159 23118544486528 run.py:483] Algo bellman_ford step 5797 current loss 0.039371, current_train_items 185536.
I0304 19:30:51.880750 23118544486528 run.py:483] Algo bellman_ford step 5798 current loss 0.049484, current_train_items 185568.
I0304 19:30:51.915549 23118544486528 run.py:483] Algo bellman_ford step 5799 current loss 0.092190, current_train_items 185600.
I0304 19:30:51.935778 23118544486528 run.py:483] Algo bellman_ford step 5800 current loss 0.007580, current_train_items 185632.
I0304 19:30:51.943589 23118544486528 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0304 19:30:51.943704 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:30:51.960833 23118544486528 run.py:483] Algo bellman_ford step 5801 current loss 0.045205, current_train_items 185664.
I0304 19:30:51.984694 23118544486528 run.py:483] Algo bellman_ford step 5802 current loss 0.020754, current_train_items 185696.
I0304 19:30:52.016712 23118544486528 run.py:483] Algo bellman_ford step 5803 current loss 0.063269, current_train_items 185728.
I0304 19:30:52.052609 23118544486528 run.py:483] Algo bellman_ford step 5804 current loss 0.067321, current_train_items 185760.
I0304 19:30:52.072594 23118544486528 run.py:483] Algo bellman_ford step 5805 current loss 0.007128, current_train_items 185792.
I0304 19:30:52.088972 23118544486528 run.py:483] Algo bellman_ford step 5806 current loss 0.022513, current_train_items 185824.
I0304 19:30:52.113984 23118544486528 run.py:483] Algo bellman_ford step 5807 current loss 0.065098, current_train_items 185856.
I0304 19:30:52.146064 23118544486528 run.py:483] Algo bellman_ford step 5808 current loss 0.084364, current_train_items 185888.
I0304 19:30:52.180399 23118544486528 run.py:483] Algo bellman_ford step 5809 current loss 0.077441, current_train_items 185920.
I0304 19:30:52.200369 23118544486528 run.py:483] Algo bellman_ford step 5810 current loss 0.007391, current_train_items 185952.
I0304 19:30:52.216604 23118544486528 run.py:483] Algo bellman_ford step 5811 current loss 0.019431, current_train_items 185984.
I0304 19:30:52.241333 23118544486528 run.py:483] Algo bellman_ford step 5812 current loss 0.108074, current_train_items 186016.
I0304 19:30:52.274141 23118544486528 run.py:483] Algo bellman_ford step 5813 current loss 0.127245, current_train_items 186048.
I0304 19:30:52.309733 23118544486528 run.py:483] Algo bellman_ford step 5814 current loss 0.143235, current_train_items 186080.
I0304 19:30:52.329868 23118544486528 run.py:483] Algo bellman_ford step 5815 current loss 0.008023, current_train_items 186112.
I0304 19:30:52.346472 23118544486528 run.py:483] Algo bellman_ford step 5816 current loss 0.029692, current_train_items 186144.
I0304 19:30:52.370704 23118544486528 run.py:483] Algo bellman_ford step 5817 current loss 0.073722, current_train_items 186176.
I0304 19:30:52.401491 23118544486528 run.py:483] Algo bellman_ford step 5818 current loss 0.052960, current_train_items 186208.
I0304 19:30:52.436310 23118544486528 run.py:483] Algo bellman_ford step 5819 current loss 0.097073, current_train_items 186240.
I0304 19:30:52.456048 23118544486528 run.py:483] Algo bellman_ford step 5820 current loss 0.004349, current_train_items 186272.
I0304 19:30:52.472398 23118544486528 run.py:483] Algo bellman_ford step 5821 current loss 0.006442, current_train_items 186304.
I0304 19:30:52.497055 23118544486528 run.py:483] Algo bellman_ford step 5822 current loss 0.069164, current_train_items 186336.
I0304 19:30:52.528736 23118544486528 run.py:483] Algo bellman_ford step 5823 current loss 0.104174, current_train_items 186368.
I0304 19:30:52.562293 23118544486528 run.py:483] Algo bellman_ford step 5824 current loss 0.076225, current_train_items 186400.
I0304 19:30:52.582370 23118544486528 run.py:483] Algo bellman_ford step 5825 current loss 0.046432, current_train_items 186432.
I0304 19:30:52.598928 23118544486528 run.py:483] Algo bellman_ford step 5826 current loss 0.027411, current_train_items 186464.
I0304 19:30:52.624136 23118544486528 run.py:483] Algo bellman_ford step 5827 current loss 0.051654, current_train_items 186496.
I0304 19:30:52.656437 23118544486528 run.py:483] Algo bellman_ford step 5828 current loss 0.081948, current_train_items 186528.
I0304 19:30:52.692029 23118544486528 run.py:483] Algo bellman_ford step 5829 current loss 0.087532, current_train_items 186560.
I0304 19:30:52.712010 23118544486528 run.py:483] Algo bellman_ford step 5830 current loss 0.014460, current_train_items 186592.
I0304 19:30:52.728860 23118544486528 run.py:483] Algo bellman_ford step 5831 current loss 0.046966, current_train_items 186624.
I0304 19:30:52.752739 23118544486528 run.py:483] Algo bellman_ford step 5832 current loss 0.114504, current_train_items 186656.
I0304 19:30:52.784943 23118544486528 run.py:483] Algo bellman_ford step 5833 current loss 0.077935, current_train_items 186688.
I0304 19:30:52.815878 23118544486528 run.py:483] Algo bellman_ford step 5834 current loss 0.071550, current_train_items 186720.
I0304 19:30:52.835668 23118544486528 run.py:483] Algo bellman_ford step 5835 current loss 0.027467, current_train_items 186752.
I0304 19:30:52.851503 23118544486528 run.py:483] Algo bellman_ford step 5836 current loss 0.013434, current_train_items 186784.
I0304 19:30:52.877143 23118544486528 run.py:483] Algo bellman_ford step 5837 current loss 0.125244, current_train_items 186816.
I0304 19:30:52.907378 23118544486528 run.py:483] Algo bellman_ford step 5838 current loss 0.086665, current_train_items 186848.
I0304 19:30:52.943374 23118544486528 run.py:483] Algo bellman_ford step 5839 current loss 0.128560, current_train_items 186880.
I0304 19:30:52.963364 23118544486528 run.py:483] Algo bellman_ford step 5840 current loss 0.013179, current_train_items 186912.
I0304 19:30:52.980166 23118544486528 run.py:483] Algo bellman_ford step 5841 current loss 0.026132, current_train_items 186944.
I0304 19:30:53.004247 23118544486528 run.py:483] Algo bellman_ford step 5842 current loss 0.046916, current_train_items 186976.
I0304 19:30:53.034988 23118544486528 run.py:483] Algo bellman_ford step 5843 current loss 0.051344, current_train_items 187008.
I0304 19:30:53.068768 23118544486528 run.py:483] Algo bellman_ford step 5844 current loss 0.065745, current_train_items 187040.
I0304 19:30:53.089132 23118544486528 run.py:483] Algo bellman_ford step 5845 current loss 0.006924, current_train_items 187072.
I0304 19:30:53.105722 23118544486528 run.py:483] Algo bellman_ford step 5846 current loss 0.010883, current_train_items 187104.
I0304 19:30:53.131701 23118544486528 run.py:483] Algo bellman_ford step 5847 current loss 0.083478, current_train_items 187136.
I0304 19:30:53.162139 23118544486528 run.py:483] Algo bellman_ford step 5848 current loss 0.053872, current_train_items 187168.
I0304 19:30:53.196576 23118544486528 run.py:483] Algo bellman_ford step 5849 current loss 0.069649, current_train_items 187200.
I0304 19:30:53.216119 23118544486528 run.py:483] Algo bellman_ford step 5850 current loss 0.007809, current_train_items 187232.
I0304 19:30:53.224022 23118544486528 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0304 19:30:53.224126 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:30:53.241438 23118544486528 run.py:483] Algo bellman_ford step 5851 current loss 0.009593, current_train_items 187264.
I0304 19:30:53.265913 23118544486528 run.py:483] Algo bellman_ford step 5852 current loss 0.167975, current_train_items 187296.
I0304 19:30:53.297066 23118544486528 run.py:483] Algo bellman_ford step 5853 current loss 0.068620, current_train_items 187328.
I0304 19:30:53.332184 23118544486528 run.py:483] Algo bellman_ford step 5854 current loss 0.152447, current_train_items 187360.
I0304 19:30:53.352239 23118544486528 run.py:483] Algo bellman_ford step 5855 current loss 0.005909, current_train_items 187392.
I0304 19:30:53.368451 23118544486528 run.py:483] Algo bellman_ford step 5856 current loss 0.024323, current_train_items 187424.
I0304 19:30:53.393343 23118544486528 run.py:483] Algo bellman_ford step 5857 current loss 0.042566, current_train_items 187456.
I0304 19:30:53.423969 23118544486528 run.py:483] Algo bellman_ford step 5858 current loss 0.048722, current_train_items 187488.
I0304 19:30:53.458146 23118544486528 run.py:483] Algo bellman_ford step 5859 current loss 0.094617, current_train_items 187520.
I0304 19:30:53.479013 23118544486528 run.py:483] Algo bellman_ford step 5860 current loss 0.005389, current_train_items 187552.
I0304 19:30:53.495741 23118544486528 run.py:483] Algo bellman_ford step 5861 current loss 0.019550, current_train_items 187584.
I0304 19:30:53.518913 23118544486528 run.py:483] Algo bellman_ford step 5862 current loss 0.032639, current_train_items 187616.
I0304 19:30:53.550955 23118544486528 run.py:483] Algo bellman_ford step 5863 current loss 0.041399, current_train_items 187648.
I0304 19:30:53.585772 23118544486528 run.py:483] Algo bellman_ford step 5864 current loss 0.104386, current_train_items 187680.
I0304 19:30:53.605749 23118544486528 run.py:483] Algo bellman_ford step 5865 current loss 0.004020, current_train_items 187712.
I0304 19:30:53.622779 23118544486528 run.py:483] Algo bellman_ford step 5866 current loss 0.038723, current_train_items 187744.
I0304 19:30:53.647189 23118544486528 run.py:483] Algo bellman_ford step 5867 current loss 0.043976, current_train_items 187776.
I0304 19:30:53.678210 23118544486528 run.py:483] Algo bellman_ford step 5868 current loss 0.106530, current_train_items 187808.
I0304 19:30:53.714472 23118544486528 run.py:483] Algo bellman_ford step 5869 current loss 0.106552, current_train_items 187840.
I0304 19:30:53.734903 23118544486528 run.py:483] Algo bellman_ford step 5870 current loss 0.006375, current_train_items 187872.
I0304 19:30:53.751466 23118544486528 run.py:483] Algo bellman_ford step 5871 current loss 0.015503, current_train_items 187904.
I0304 19:30:53.774305 23118544486528 run.py:483] Algo bellman_ford step 5872 current loss 0.030792, current_train_items 187936.
I0304 19:30:53.805997 23118544486528 run.py:483] Algo bellman_ford step 5873 current loss 0.057380, current_train_items 187968.
I0304 19:30:53.840610 23118544486528 run.py:483] Algo bellman_ford step 5874 current loss 0.078705, current_train_items 188000.
I0304 19:30:53.860768 23118544486528 run.py:483] Algo bellman_ford step 5875 current loss 0.004984, current_train_items 188032.
I0304 19:30:53.877254 23118544486528 run.py:483] Algo bellman_ford step 5876 current loss 0.018106, current_train_items 188064.
I0304 19:30:53.900580 23118544486528 run.py:483] Algo bellman_ford step 5877 current loss 0.017454, current_train_items 188096.
I0304 19:30:53.931977 23118544486528 run.py:483] Algo bellman_ford step 5878 current loss 0.037508, current_train_items 188128.
I0304 19:30:53.966755 23118544486528 run.py:483] Algo bellman_ford step 5879 current loss 0.038620, current_train_items 188160.
I0304 19:30:53.986921 23118544486528 run.py:483] Algo bellman_ford step 5880 current loss 0.015211, current_train_items 188192.
I0304 19:30:54.003226 23118544486528 run.py:483] Algo bellman_ford step 5881 current loss 0.016376, current_train_items 188224.
I0304 19:30:54.027762 23118544486528 run.py:483] Algo bellman_ford step 5882 current loss 0.082526, current_train_items 188256.
I0304 19:30:54.059120 23118544486528 run.py:483] Algo bellman_ford step 5883 current loss 0.029571, current_train_items 188288.
I0304 19:30:54.094291 23118544486528 run.py:483] Algo bellman_ford step 5884 current loss 0.125325, current_train_items 188320.
I0304 19:30:54.115274 23118544486528 run.py:483] Algo bellman_ford step 5885 current loss 0.004421, current_train_items 188352.
I0304 19:30:54.131887 23118544486528 run.py:483] Algo bellman_ford step 5886 current loss 0.016630, current_train_items 188384.
I0304 19:30:54.155501 23118544486528 run.py:483] Algo bellman_ford step 5887 current loss 0.035735, current_train_items 188416.
I0304 19:30:54.186378 23118544486528 run.py:483] Algo bellman_ford step 5888 current loss 0.053249, current_train_items 188448.
I0304 19:30:54.219013 23118544486528 run.py:483] Algo bellman_ford step 5889 current loss 0.137438, current_train_items 188480.
I0304 19:30:54.239563 23118544486528 run.py:483] Algo bellman_ford step 5890 current loss 0.006810, current_train_items 188512.
I0304 19:30:54.256176 23118544486528 run.py:483] Algo bellman_ford step 5891 current loss 0.032628, current_train_items 188544.
I0304 19:30:54.280052 23118544486528 run.py:483] Algo bellman_ford step 5892 current loss 0.060222, current_train_items 188576.
I0304 19:30:54.310469 23118544486528 run.py:483] Algo bellman_ford step 5893 current loss 0.028831, current_train_items 188608.
I0304 19:30:54.342623 23118544486528 run.py:483] Algo bellman_ford step 5894 current loss 0.045835, current_train_items 188640.
I0304 19:30:54.362389 23118544486528 run.py:483] Algo bellman_ford step 5895 current loss 0.004031, current_train_items 188672.
I0304 19:30:54.378935 23118544486528 run.py:483] Algo bellman_ford step 5896 current loss 0.054443, current_train_items 188704.
I0304 19:30:54.403949 23118544486528 run.py:483] Algo bellman_ford step 5897 current loss 0.064900, current_train_items 188736.
I0304 19:30:54.436286 23118544486528 run.py:483] Algo bellman_ford step 5898 current loss 0.067584, current_train_items 188768.
I0304 19:30:54.470295 23118544486528 run.py:483] Algo bellman_ford step 5899 current loss 0.061876, current_train_items 188800.
I0304 19:30:54.490564 23118544486528 run.py:483] Algo bellman_ford step 5900 current loss 0.008157, current_train_items 188832.
I0304 19:30:54.498299 23118544486528 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0304 19:30:54.498404 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.991, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:30:54.549063 23118544486528 run.py:483] Algo bellman_ford step 5901 current loss 0.018987, current_train_items 188864.
I0304 19:30:54.574462 23118544486528 run.py:483] Algo bellman_ford step 5902 current loss 0.020343, current_train_items 188896.
I0304 19:30:54.607187 23118544486528 run.py:483] Algo bellman_ford step 5903 current loss 0.047892, current_train_items 188928.
I0304 19:30:54.639932 23118544486528 run.py:483] Algo bellman_ford step 5904 current loss 0.047635, current_train_items 188960.
I0304 19:30:54.660339 23118544486528 run.py:483] Algo bellman_ford step 5905 current loss 0.007076, current_train_items 188992.
I0304 19:30:54.677209 23118544486528 run.py:483] Algo bellman_ford step 5906 current loss 0.027437, current_train_items 189024.
I0304 19:30:54.701288 23118544486528 run.py:483] Algo bellman_ford step 5907 current loss 0.048088, current_train_items 189056.
I0304 19:30:54.732889 23118544486528 run.py:483] Algo bellman_ford step 5908 current loss 0.050449, current_train_items 189088.
I0304 19:30:54.765909 23118544486528 run.py:483] Algo bellman_ford step 5909 current loss 0.100110, current_train_items 189120.
I0304 19:30:54.786217 23118544486528 run.py:483] Algo bellman_ford step 5910 current loss 0.005263, current_train_items 189152.
I0304 19:30:54.802834 23118544486528 run.py:483] Algo bellman_ford step 5911 current loss 0.020383, current_train_items 189184.
I0304 19:30:54.827316 23118544486528 run.py:483] Algo bellman_ford step 5912 current loss 0.094805, current_train_items 189216.
I0304 19:30:54.858224 23118544486528 run.py:483] Algo bellman_ford step 5913 current loss 0.064904, current_train_items 189248.
I0304 19:30:54.891535 23118544486528 run.py:483] Algo bellman_ford step 5914 current loss 0.083883, current_train_items 189280.
I0304 19:30:54.911505 23118544486528 run.py:483] Algo bellman_ford step 5915 current loss 0.012033, current_train_items 189312.
I0304 19:30:54.928083 23118544486528 run.py:483] Algo bellman_ford step 5916 current loss 0.027829, current_train_items 189344.
I0304 19:30:54.952290 23118544486528 run.py:483] Algo bellman_ford step 5917 current loss 0.059260, current_train_items 189376.
I0304 19:30:54.983215 23118544486528 run.py:483] Algo bellman_ford step 5918 current loss 0.029667, current_train_items 189408.
I0304 19:30:55.018268 23118544486528 run.py:483] Algo bellman_ford step 5919 current loss 0.091338, current_train_items 189440.
I0304 19:30:55.038306 23118544486528 run.py:483] Algo bellman_ford step 5920 current loss 0.009832, current_train_items 189472.
I0304 19:30:55.054883 23118544486528 run.py:483] Algo bellman_ford step 5921 current loss 0.022686, current_train_items 189504.
I0304 19:30:55.078549 23118544486528 run.py:483] Algo bellman_ford step 5922 current loss 0.042456, current_train_items 189536.
I0304 19:30:55.108785 23118544486528 run.py:483] Algo bellman_ford step 5923 current loss 0.040089, current_train_items 189568.
I0304 19:30:55.142703 23118544486528 run.py:483] Algo bellman_ford step 5924 current loss 0.065728, current_train_items 189600.
I0304 19:30:55.162737 23118544486528 run.py:483] Algo bellman_ford step 5925 current loss 0.005066, current_train_items 189632.
I0304 19:30:55.178858 23118544486528 run.py:483] Algo bellman_ford step 5926 current loss 0.031946, current_train_items 189664.
I0304 19:30:55.202680 23118544486528 run.py:483] Algo bellman_ford step 5927 current loss 0.032125, current_train_items 189696.
I0304 19:30:55.234771 23118544486528 run.py:483] Algo bellman_ford step 5928 current loss 0.050235, current_train_items 189728.
I0304 19:30:55.267894 23118544486528 run.py:483] Algo bellman_ford step 5929 current loss 0.032154, current_train_items 189760.
I0304 19:30:55.287848 23118544486528 run.py:483] Algo bellman_ford step 5930 current loss 0.003792, current_train_items 189792.
I0304 19:30:55.304512 23118544486528 run.py:483] Algo bellman_ford step 5931 current loss 0.026745, current_train_items 189824.
I0304 19:30:55.328708 23118544486528 run.py:483] Algo bellman_ford step 5932 current loss 0.035179, current_train_items 189856.
I0304 19:30:55.359700 23118544486528 run.py:483] Algo bellman_ford step 5933 current loss 0.046622, current_train_items 189888.
I0304 19:30:55.394847 23118544486528 run.py:483] Algo bellman_ford step 5934 current loss 0.069768, current_train_items 189920.
I0304 19:30:55.414555 23118544486528 run.py:483] Algo bellman_ford step 5935 current loss 0.008778, current_train_items 189952.
I0304 19:30:55.430616 23118544486528 run.py:483] Algo bellman_ford step 5936 current loss 0.019877, current_train_items 189984.
I0304 19:30:55.454915 23118544486528 run.py:483] Algo bellman_ford step 5937 current loss 0.088475, current_train_items 190016.
I0304 19:30:55.487272 23118544486528 run.py:483] Algo bellman_ford step 5938 current loss 0.066585, current_train_items 190048.
I0304 19:30:55.519885 23118544486528 run.py:483] Algo bellman_ford step 5939 current loss 0.081437, current_train_items 190080.
I0304 19:30:55.539799 23118544486528 run.py:483] Algo bellman_ford step 5940 current loss 0.005349, current_train_items 190112.
I0304 19:30:55.555971 23118544486528 run.py:483] Algo bellman_ford step 5941 current loss 0.028920, current_train_items 190144.
I0304 19:30:55.580457 23118544486528 run.py:483] Algo bellman_ford step 5942 current loss 0.033045, current_train_items 190176.
I0304 19:30:55.612740 23118544486528 run.py:483] Algo bellman_ford step 5943 current loss 0.040362, current_train_items 190208.
I0304 19:30:55.647224 23118544486528 run.py:483] Algo bellman_ford step 5944 current loss 0.078475, current_train_items 190240.
I0304 19:30:55.667021 23118544486528 run.py:483] Algo bellman_ford step 5945 current loss 0.004823, current_train_items 190272.
I0304 19:30:55.683924 23118544486528 run.py:483] Algo bellman_ford step 5946 current loss 0.017354, current_train_items 190304.
I0304 19:30:55.708192 23118544486528 run.py:483] Algo bellman_ford step 5947 current loss 0.029446, current_train_items 190336.
I0304 19:30:55.738415 23118544486528 run.py:483] Algo bellman_ford step 5948 current loss 0.038369, current_train_items 190368.
I0304 19:30:55.772655 23118544486528 run.py:483] Algo bellman_ford step 5949 current loss 0.113870, current_train_items 190400.
I0304 19:30:55.792905 23118544486528 run.py:483] Algo bellman_ford step 5950 current loss 0.003827, current_train_items 190432.
I0304 19:30:55.800947 23118544486528 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0304 19:30:55.801051 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:30:55.818164 23118544486528 run.py:483] Algo bellman_ford step 5951 current loss 0.016224, current_train_items 190464.
I0304 19:30:55.843066 23118544486528 run.py:483] Algo bellman_ford step 5952 current loss 0.053062, current_train_items 190496.
I0304 19:30:55.874676 23118544486528 run.py:483] Algo bellman_ford step 5953 current loss 0.023949, current_train_items 190528.
I0304 19:30:55.908997 23118544486528 run.py:483] Algo bellman_ford step 5954 current loss 0.079490, current_train_items 190560.
I0304 19:30:55.929489 23118544486528 run.py:483] Algo bellman_ford step 5955 current loss 0.005485, current_train_items 190592.
I0304 19:30:55.946083 23118544486528 run.py:483] Algo bellman_ford step 5956 current loss 0.020703, current_train_items 190624.
I0304 19:30:55.969214 23118544486528 run.py:483] Algo bellman_ford step 5957 current loss 0.038263, current_train_items 190656.
I0304 19:30:56.000480 23118544486528 run.py:483] Algo bellman_ford step 5958 current loss 0.025804, current_train_items 190688.
I0304 19:30:56.034285 23118544486528 run.py:483] Algo bellman_ford step 5959 current loss 0.070344, current_train_items 190720.
I0304 19:30:56.054630 23118544486528 run.py:483] Algo bellman_ford step 5960 current loss 0.005496, current_train_items 190752.
I0304 19:30:56.071181 23118544486528 run.py:483] Algo bellman_ford step 5961 current loss 0.024043, current_train_items 190784.
I0304 19:30:56.095554 23118544486528 run.py:483] Algo bellman_ford step 5962 current loss 0.062420, current_train_items 190816.
I0304 19:30:56.125470 23118544486528 run.py:483] Algo bellman_ford step 5963 current loss 0.040213, current_train_items 190848.
I0304 19:30:56.159089 23118544486528 run.py:483] Algo bellman_ford step 5964 current loss 0.098400, current_train_items 190880.
I0304 19:30:56.178717 23118544486528 run.py:483] Algo bellman_ford step 5965 current loss 0.043720, current_train_items 190912.
I0304 19:30:56.194929 23118544486528 run.py:483] Algo bellman_ford step 5966 current loss 0.019066, current_train_items 190944.
I0304 19:30:56.219242 23118544486528 run.py:483] Algo bellman_ford step 5967 current loss 0.035322, current_train_items 190976.
I0304 19:30:56.250806 23118544486528 run.py:483] Algo bellman_ford step 5968 current loss 0.054841, current_train_items 191008.
I0304 19:30:56.285442 23118544486528 run.py:483] Algo bellman_ford step 5969 current loss 0.075043, current_train_items 191040.
I0304 19:30:56.305466 23118544486528 run.py:483] Algo bellman_ford step 5970 current loss 0.005149, current_train_items 191072.
I0304 19:30:56.321872 23118544486528 run.py:483] Algo bellman_ford step 5971 current loss 0.017603, current_train_items 191104.
I0304 19:30:56.345520 23118544486528 run.py:483] Algo bellman_ford step 5972 current loss 0.037823, current_train_items 191136.
I0304 19:30:56.377431 23118544486528 run.py:483] Algo bellman_ford step 5973 current loss 0.064591, current_train_items 191168.
I0304 19:30:56.410946 23118544486528 run.py:483] Algo bellman_ford step 5974 current loss 0.067909, current_train_items 191200.
I0304 19:30:56.430982 23118544486528 run.py:483] Algo bellman_ford step 5975 current loss 0.005556, current_train_items 191232.
I0304 19:30:56.447151 23118544486528 run.py:483] Algo bellman_ford step 5976 current loss 0.011941, current_train_items 191264.
I0304 19:30:56.470604 23118544486528 run.py:483] Algo bellman_ford step 5977 current loss 0.055714, current_train_items 191296.
I0304 19:30:56.502180 23118544486528 run.py:483] Algo bellman_ford step 5978 current loss 0.103362, current_train_items 191328.
I0304 19:30:56.534829 23118544486528 run.py:483] Algo bellman_ford step 5979 current loss 0.046956, current_train_items 191360.
I0304 19:30:56.554159 23118544486528 run.py:483] Algo bellman_ford step 5980 current loss 0.005072, current_train_items 191392.
I0304 19:30:56.571117 23118544486528 run.py:483] Algo bellman_ford step 5981 current loss 0.041364, current_train_items 191424.
I0304 19:30:56.594437 23118544486528 run.py:483] Algo bellman_ford step 5982 current loss 0.024820, current_train_items 191456.
I0304 19:30:56.625257 23118544486528 run.py:483] Algo bellman_ford step 5983 current loss 0.042767, current_train_items 191488.
I0304 19:30:56.658377 23118544486528 run.py:483] Algo bellman_ford step 5984 current loss 0.080412, current_train_items 191520.
I0304 19:30:56.678323 23118544486528 run.py:483] Algo bellman_ford step 5985 current loss 0.009178, current_train_items 191552.
I0304 19:30:56.695219 23118544486528 run.py:483] Algo bellman_ford step 5986 current loss 0.012494, current_train_items 191584.
I0304 19:30:56.718769 23118544486528 run.py:483] Algo bellman_ford step 5987 current loss 0.044064, current_train_items 191616.
I0304 19:30:56.749191 23118544486528 run.py:483] Algo bellman_ford step 5988 current loss 0.059288, current_train_items 191648.
I0304 19:30:56.782605 23118544486528 run.py:483] Algo bellman_ford step 5989 current loss 0.056641, current_train_items 191680.
I0304 19:30:56.802616 23118544486528 run.py:483] Algo bellman_ford step 5990 current loss 0.003847, current_train_items 191712.
I0304 19:30:56.818836 23118544486528 run.py:483] Algo bellman_ford step 5991 current loss 0.033010, current_train_items 191744.
I0304 19:30:56.842725 23118544486528 run.py:483] Algo bellman_ford step 5992 current loss 0.058303, current_train_items 191776.
I0304 19:30:56.875613 23118544486528 run.py:483] Algo bellman_ford step 5993 current loss 0.058011, current_train_items 191808.
I0304 19:30:56.909398 23118544486528 run.py:483] Algo bellman_ford step 5994 current loss 0.060326, current_train_items 191840.
I0304 19:30:56.929231 23118544486528 run.py:483] Algo bellman_ford step 5995 current loss 0.005360, current_train_items 191872.
I0304 19:30:56.945644 23118544486528 run.py:483] Algo bellman_ford step 5996 current loss 0.009848, current_train_items 191904.
I0304 19:30:56.969929 23118544486528 run.py:483] Algo bellman_ford step 5997 current loss 0.035049, current_train_items 191936.
I0304 19:30:57.001107 23118544486528 run.py:483] Algo bellman_ford step 5998 current loss 0.029396, current_train_items 191968.
I0304 19:30:57.034015 23118544486528 run.py:483] Algo bellman_ford step 5999 current loss 0.028178, current_train_items 192000.
I0304 19:30:57.054256 23118544486528 run.py:483] Algo bellman_ford step 6000 current loss 0.014537, current_train_items 192032.
I0304 19:30:57.061863 23118544486528 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0304 19:30:57.061969 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:57.079027 23118544486528 run.py:483] Algo bellman_ford step 6001 current loss 0.028115, current_train_items 192064.
I0304 19:30:57.103915 23118544486528 run.py:483] Algo bellman_ford step 6002 current loss 0.054902, current_train_items 192096.
I0304 19:30:57.134376 23118544486528 run.py:483] Algo bellman_ford step 6003 current loss 0.063511, current_train_items 192128.
I0304 19:30:57.168244 23118544486528 run.py:483] Algo bellman_ford step 6004 current loss 0.060466, current_train_items 192160.
I0304 19:30:57.188454 23118544486528 run.py:483] Algo bellman_ford step 6005 current loss 0.006733, current_train_items 192192.
I0304 19:30:57.204535 23118544486528 run.py:483] Algo bellman_ford step 6006 current loss 0.016710, current_train_items 192224.
I0304 19:30:57.229807 23118544486528 run.py:483] Algo bellman_ford step 6007 current loss 0.112446, current_train_items 192256.
I0304 19:30:57.263586 23118544486528 run.py:483] Algo bellman_ford step 6008 current loss 0.093274, current_train_items 192288.
I0304 19:30:57.299170 23118544486528 run.py:483] Algo bellman_ford step 6009 current loss 0.049863, current_train_items 192320.
I0304 19:30:57.318987 23118544486528 run.py:483] Algo bellman_ford step 6010 current loss 0.004287, current_train_items 192352.
I0304 19:30:57.335401 23118544486528 run.py:483] Algo bellman_ford step 6011 current loss 0.027500, current_train_items 192384.
I0304 19:30:57.360065 23118544486528 run.py:483] Algo bellman_ford step 6012 current loss 0.043666, current_train_items 192416.
I0304 19:30:57.390847 23118544486528 run.py:483] Algo bellman_ford step 6013 current loss 0.045596, current_train_items 192448.
I0304 19:30:57.427122 23118544486528 run.py:483] Algo bellman_ford step 6014 current loss 0.071010, current_train_items 192480.
I0304 19:30:57.447247 23118544486528 run.py:483] Algo bellman_ford step 6015 current loss 0.012343, current_train_items 192512.
I0304 19:30:57.463962 23118544486528 run.py:483] Algo bellman_ford step 6016 current loss 0.017241, current_train_items 192544.
I0304 19:30:57.487977 23118544486528 run.py:483] Algo bellman_ford step 6017 current loss 0.066485, current_train_items 192576.
I0304 19:30:57.520061 23118544486528 run.py:483] Algo bellman_ford step 6018 current loss 0.173119, current_train_items 192608.
I0304 19:30:57.553968 23118544486528 run.py:483] Algo bellman_ford step 6019 current loss 0.125457, current_train_items 192640.
I0304 19:30:57.573592 23118544486528 run.py:483] Algo bellman_ford step 6020 current loss 0.005381, current_train_items 192672.
I0304 19:30:57.590514 23118544486528 run.py:483] Algo bellman_ford step 6021 current loss 0.011675, current_train_items 192704.
I0304 19:30:57.614741 23118544486528 run.py:483] Algo bellman_ford step 6022 current loss 0.058888, current_train_items 192736.
I0304 19:30:57.647612 23118544486528 run.py:483] Algo bellman_ford step 6023 current loss 0.095414, current_train_items 192768.
I0304 19:30:57.682086 23118544486528 run.py:483] Algo bellman_ford step 6024 current loss 0.125244, current_train_items 192800.
I0304 19:30:57.701717 23118544486528 run.py:483] Algo bellman_ford step 6025 current loss 0.004209, current_train_items 192832.
I0304 19:30:57.718294 23118544486528 run.py:483] Algo bellman_ford step 6026 current loss 0.033419, current_train_items 192864.
I0304 19:30:57.742444 23118544486528 run.py:483] Algo bellman_ford step 6027 current loss 0.053968, current_train_items 192896.
I0304 19:30:57.774070 23118544486528 run.py:483] Algo bellman_ford step 6028 current loss 0.141883, current_train_items 192928.
I0304 19:30:57.808420 23118544486528 run.py:483] Algo bellman_ford step 6029 current loss 0.294712, current_train_items 192960.
I0304 19:30:57.827883 23118544486528 run.py:483] Algo bellman_ford step 6030 current loss 0.006041, current_train_items 192992.
I0304 19:30:57.844116 23118544486528 run.py:483] Algo bellman_ford step 6031 current loss 0.047929, current_train_items 193024.
I0304 19:30:57.868238 23118544486528 run.py:483] Algo bellman_ford step 6032 current loss 0.035654, current_train_items 193056.
I0304 19:30:57.901319 23118544486528 run.py:483] Algo bellman_ford step 6033 current loss 0.076111, current_train_items 193088.
I0304 19:30:57.935294 23118544486528 run.py:483] Algo bellman_ford step 6034 current loss 0.051919, current_train_items 193120.
I0304 19:30:57.954950 23118544486528 run.py:483] Algo bellman_ford step 6035 current loss 0.004211, current_train_items 193152.
I0304 19:30:57.971465 23118544486528 run.py:483] Algo bellman_ford step 6036 current loss 0.023350, current_train_items 193184.
I0304 19:30:57.995907 23118544486528 run.py:483] Algo bellman_ford step 6037 current loss 0.028407, current_train_items 193216.
I0304 19:30:58.027996 23118544486528 run.py:483] Algo bellman_ford step 6038 current loss 0.083569, current_train_items 193248.
I0304 19:30:58.061350 23118544486528 run.py:483] Algo bellman_ford step 6039 current loss 0.043673, current_train_items 193280.
I0304 19:30:58.081162 23118544486528 run.py:483] Algo bellman_ford step 6040 current loss 0.006119, current_train_items 193312.
I0304 19:30:58.097648 23118544486528 run.py:483] Algo bellman_ford step 6041 current loss 0.038571, current_train_items 193344.
I0304 19:30:58.123072 23118544486528 run.py:483] Algo bellman_ford step 6042 current loss 0.057546, current_train_items 193376.
I0304 19:30:58.154390 23118544486528 run.py:483] Algo bellman_ford step 6043 current loss 0.063626, current_train_items 193408.
I0304 19:30:58.191473 23118544486528 run.py:483] Algo bellman_ford step 6044 current loss 0.077125, current_train_items 193440.
I0304 19:30:58.211596 23118544486528 run.py:483] Algo bellman_ford step 6045 current loss 0.004487, current_train_items 193472.
I0304 19:30:58.228312 23118544486528 run.py:483] Algo bellman_ford step 6046 current loss 0.037670, current_train_items 193504.
I0304 19:30:58.252238 23118544486528 run.py:483] Algo bellman_ford step 6047 current loss 0.052894, current_train_items 193536.
I0304 19:30:58.285332 23118544486528 run.py:483] Algo bellman_ford step 6048 current loss 0.073036, current_train_items 193568.
I0304 19:30:58.320140 23118544486528 run.py:483] Algo bellman_ford step 6049 current loss 0.055046, current_train_items 193600.
I0304 19:30:58.339960 23118544486528 run.py:483] Algo bellman_ford step 6050 current loss 0.010414, current_train_items 193632.
I0304 19:30:58.347985 23118544486528 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0304 19:30:58.348114 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:30:58.364993 23118544486528 run.py:483] Algo bellman_ford step 6051 current loss 0.023557, current_train_items 193664.
I0304 19:30:58.390405 23118544486528 run.py:483] Algo bellman_ford step 6052 current loss 0.129099, current_train_items 193696.
I0304 19:30:58.422348 23118544486528 run.py:483] Algo bellman_ford step 6053 current loss 0.056942, current_train_items 193728.
I0304 19:30:58.457829 23118544486528 run.py:483] Algo bellman_ford step 6054 current loss 0.057625, current_train_items 193760.
I0304 19:30:58.478146 23118544486528 run.py:483] Algo bellman_ford step 6055 current loss 0.009811, current_train_items 193792.
I0304 19:30:58.494043 23118544486528 run.py:483] Algo bellman_ford step 6056 current loss 0.051441, current_train_items 193824.
I0304 19:30:58.517523 23118544486528 run.py:483] Algo bellman_ford step 6057 current loss 0.076516, current_train_items 193856.
I0304 19:30:58.548727 23118544486528 run.py:483] Algo bellman_ford step 6058 current loss 0.064809, current_train_items 193888.
I0304 19:30:58.580506 23118544486528 run.py:483] Algo bellman_ford step 6059 current loss 0.076520, current_train_items 193920.
I0304 19:30:58.600768 23118544486528 run.py:483] Algo bellman_ford step 6060 current loss 0.006153, current_train_items 193952.
I0304 19:30:58.617290 23118544486528 run.py:483] Algo bellman_ford step 6061 current loss 0.016266, current_train_items 193984.
I0304 19:30:58.640033 23118544486528 run.py:483] Algo bellman_ford step 6062 current loss 0.031029, current_train_items 194016.
I0304 19:30:58.670536 23118544486528 run.py:483] Algo bellman_ford step 6063 current loss 0.050953, current_train_items 194048.
I0304 19:30:58.705332 23118544486528 run.py:483] Algo bellman_ford step 6064 current loss 0.096431, current_train_items 194080.
I0304 19:30:58.725506 23118544486528 run.py:483] Algo bellman_ford step 6065 current loss 0.007771, current_train_items 194112.
I0304 19:30:58.742175 23118544486528 run.py:483] Algo bellman_ford step 6066 current loss 0.029267, current_train_items 194144.
I0304 19:30:58.767275 23118544486528 run.py:483] Algo bellman_ford step 6067 current loss 0.067249, current_train_items 194176.
I0304 19:30:58.799850 23118544486528 run.py:483] Algo bellman_ford step 6068 current loss 0.101885, current_train_items 194208.
I0304 19:30:58.832992 23118544486528 run.py:483] Algo bellman_ford step 6069 current loss 0.074465, current_train_items 194240.
I0304 19:30:58.853263 23118544486528 run.py:483] Algo bellman_ford step 6070 current loss 0.003998, current_train_items 194272.
I0304 19:30:58.869575 23118544486528 run.py:483] Algo bellman_ford step 6071 current loss 0.009782, current_train_items 194304.
I0304 19:30:58.893714 23118544486528 run.py:483] Algo bellman_ford step 6072 current loss 0.054951, current_train_items 194336.
I0304 19:30:58.924721 23118544486528 run.py:483] Algo bellman_ford step 6073 current loss 0.110611, current_train_items 194368.
I0304 19:30:58.959530 23118544486528 run.py:483] Algo bellman_ford step 6074 current loss 0.095729, current_train_items 194400.
I0304 19:30:58.979463 23118544486528 run.py:483] Algo bellman_ford step 6075 current loss 0.005197, current_train_items 194432.
I0304 19:30:58.996031 23118544486528 run.py:483] Algo bellman_ford step 6076 current loss 0.032228, current_train_items 194464.
I0304 19:30:59.019099 23118544486528 run.py:483] Algo bellman_ford step 6077 current loss 0.045886, current_train_items 194496.
I0304 19:30:59.050625 23118544486528 run.py:483] Algo bellman_ford step 6078 current loss 0.036902, current_train_items 194528.
I0304 19:30:59.085408 23118544486528 run.py:483] Algo bellman_ford step 6079 current loss 0.088606, current_train_items 194560.
I0304 19:30:59.105334 23118544486528 run.py:483] Algo bellman_ford step 6080 current loss 0.006766, current_train_items 194592.
I0304 19:30:59.121975 23118544486528 run.py:483] Algo bellman_ford step 6081 current loss 0.009122, current_train_items 194624.
I0304 19:30:59.146146 23118544486528 run.py:483] Algo bellman_ford step 6082 current loss 0.040488, current_train_items 194656.
I0304 19:30:59.179152 23118544486528 run.py:483] Algo bellman_ford step 6083 current loss 0.053447, current_train_items 194688.
I0304 19:30:59.212260 23118544486528 run.py:483] Algo bellman_ford step 6084 current loss 0.061049, current_train_items 194720.
I0304 19:30:59.232753 23118544486528 run.py:483] Algo bellman_ford step 6085 current loss 0.006847, current_train_items 194752.
I0304 19:30:59.249494 23118544486528 run.py:483] Algo bellman_ford step 6086 current loss 0.044122, current_train_items 194784.
I0304 19:30:59.273661 23118544486528 run.py:483] Algo bellman_ford step 6087 current loss 0.025351, current_train_items 194816.
I0304 19:30:59.304374 23118544486528 run.py:483] Algo bellman_ford step 6088 current loss 0.040868, current_train_items 194848.
I0304 19:30:59.339105 23118544486528 run.py:483] Algo bellman_ford step 6089 current loss 0.077961, current_train_items 194880.
I0304 19:30:59.359493 23118544486528 run.py:483] Algo bellman_ford step 6090 current loss 0.024019, current_train_items 194912.
I0304 19:30:59.376286 23118544486528 run.py:483] Algo bellman_ford step 6091 current loss 0.012087, current_train_items 194944.
I0304 19:30:59.400056 23118544486528 run.py:483] Algo bellman_ford step 6092 current loss 0.039613, current_train_items 194976.
I0304 19:30:59.431227 23118544486528 run.py:483] Algo bellman_ford step 6093 current loss 0.056762, current_train_items 195008.
I0304 19:30:59.464741 23118544486528 run.py:483] Algo bellman_ford step 6094 current loss 0.046956, current_train_items 195040.
I0304 19:30:59.485001 23118544486528 run.py:483] Algo bellman_ford step 6095 current loss 0.005317, current_train_items 195072.
I0304 19:30:59.501495 23118544486528 run.py:483] Algo bellman_ford step 6096 current loss 0.014142, current_train_items 195104.
I0304 19:30:59.526161 23118544486528 run.py:483] Algo bellman_ford step 6097 current loss 0.049552, current_train_items 195136.
I0304 19:30:59.557002 23118544486528 run.py:483] Algo bellman_ford step 6098 current loss 0.037779, current_train_items 195168.
I0304 19:30:59.591187 23118544486528 run.py:483] Algo bellman_ford step 6099 current loss 0.090571, current_train_items 195200.
I0304 19:30:59.611045 23118544486528 run.py:483] Algo bellman_ford step 6100 current loss 0.002425, current_train_items 195232.
I0304 19:30:59.618774 23118544486528 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0304 19:30:59.618880 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:30:59.635565 23118544486528 run.py:483] Algo bellman_ford step 6101 current loss 0.012977, current_train_items 195264.
I0304 19:30:59.660964 23118544486528 run.py:483] Algo bellman_ford step 6102 current loss 0.035286, current_train_items 195296.
I0304 19:30:59.693840 23118544486528 run.py:483] Algo bellman_ford step 6103 current loss 0.057368, current_train_items 195328.
I0304 19:30:59.727488 23118544486528 run.py:483] Algo bellman_ford step 6104 current loss 0.050866, current_train_items 195360.
I0304 19:30:59.747604 23118544486528 run.py:483] Algo bellman_ford step 6105 current loss 0.004017, current_train_items 195392.
I0304 19:30:59.763288 23118544486528 run.py:483] Algo bellman_ford step 6106 current loss 0.021147, current_train_items 195424.
I0304 19:30:59.787662 23118544486528 run.py:483] Algo bellman_ford step 6107 current loss 0.063066, current_train_items 195456.
I0304 19:30:59.819214 23118544486528 run.py:483] Algo bellman_ford step 6108 current loss 0.070817, current_train_items 195488.
I0304 19:30:59.852573 23118544486528 run.py:483] Algo bellman_ford step 6109 current loss 0.076391, current_train_items 195520.
I0304 19:30:59.872395 23118544486528 run.py:483] Algo bellman_ford step 6110 current loss 0.004422, current_train_items 195552.
I0304 19:30:59.888751 23118544486528 run.py:483] Algo bellman_ford step 6111 current loss 0.022311, current_train_items 195584.
I0304 19:30:59.912756 23118544486528 run.py:483] Algo bellman_ford step 6112 current loss 0.051988, current_train_items 195616.
I0304 19:30:59.944008 23118544486528 run.py:483] Algo bellman_ford step 6113 current loss 0.085029, current_train_items 195648.
I0304 19:30:59.978073 23118544486528 run.py:483] Algo bellman_ford step 6114 current loss 0.089860, current_train_items 195680.
I0304 19:30:59.998092 23118544486528 run.py:483] Algo bellman_ford step 6115 current loss 0.028148, current_train_items 195712.
I0304 19:31:00.014437 23118544486528 run.py:483] Algo bellman_ford step 6116 current loss 0.039894, current_train_items 195744.
I0304 19:31:00.038724 23118544486528 run.py:483] Algo bellman_ford step 6117 current loss 0.045166, current_train_items 195776.
I0304 19:31:00.071235 23118544486528 run.py:483] Algo bellman_ford step 6118 current loss 0.048831, current_train_items 195808.
I0304 19:31:00.104725 23118544486528 run.py:483] Algo bellman_ford step 6119 current loss 0.058246, current_train_items 195840.
I0304 19:31:00.124393 23118544486528 run.py:483] Algo bellman_ford step 6120 current loss 0.020688, current_train_items 195872.
I0304 19:31:00.140966 23118544486528 run.py:483] Algo bellman_ford step 6121 current loss 0.019982, current_train_items 195904.
I0304 19:31:00.164620 23118544486528 run.py:483] Algo bellman_ford step 6122 current loss 0.045965, current_train_items 195936.
I0304 19:31:00.197151 23118544486528 run.py:483] Algo bellman_ford step 6123 current loss 0.055011, current_train_items 195968.
I0304 19:31:00.231280 23118544486528 run.py:483] Algo bellman_ford step 6124 current loss 0.074405, current_train_items 196000.
I0304 19:31:00.251002 23118544486528 run.py:483] Algo bellman_ford step 6125 current loss 0.003426, current_train_items 196032.
I0304 19:31:00.266884 23118544486528 run.py:483] Algo bellman_ford step 6126 current loss 0.028824, current_train_items 196064.
I0304 19:31:00.292858 23118544486528 run.py:483] Algo bellman_ford step 6127 current loss 0.060255, current_train_items 196096.
I0304 19:31:00.324282 23118544486528 run.py:483] Algo bellman_ford step 6128 current loss 0.045320, current_train_items 196128.
I0304 19:31:00.357794 23118544486528 run.py:483] Algo bellman_ford step 6129 current loss 0.078268, current_train_items 196160.
I0304 19:31:00.377802 23118544486528 run.py:483] Algo bellman_ford step 6130 current loss 0.003219, current_train_items 196192.
I0304 19:31:00.394163 23118544486528 run.py:483] Algo bellman_ford step 6131 current loss 0.022751, current_train_items 196224.
I0304 19:31:00.419066 23118544486528 run.py:483] Algo bellman_ford step 6132 current loss 0.038819, current_train_items 196256.
I0304 19:31:00.449504 23118544486528 run.py:483] Algo bellman_ford step 6133 current loss 0.051535, current_train_items 196288.
I0304 19:31:00.482347 23118544486528 run.py:483] Algo bellman_ford step 6134 current loss 0.046862, current_train_items 196320.
I0304 19:31:00.501760 23118544486528 run.py:483] Algo bellman_ford step 6135 current loss 0.031224, current_train_items 196352.
I0304 19:31:00.517741 23118544486528 run.py:483] Algo bellman_ford step 6136 current loss 0.020896, current_train_items 196384.
I0304 19:31:00.542097 23118544486528 run.py:483] Algo bellman_ford step 6137 current loss 0.116471, current_train_items 196416.
I0304 19:31:00.574535 23118544486528 run.py:483] Algo bellman_ford step 6138 current loss 0.091247, current_train_items 196448.
I0304 19:31:00.608760 23118544486528 run.py:483] Algo bellman_ford step 6139 current loss 0.121663, current_train_items 196480.
I0304 19:31:00.628453 23118544486528 run.py:483] Algo bellman_ford step 6140 current loss 0.006232, current_train_items 196512.
I0304 19:31:00.644741 23118544486528 run.py:483] Algo bellman_ford step 6141 current loss 0.024410, current_train_items 196544.
I0304 19:31:00.669368 23118544486528 run.py:483] Algo bellman_ford step 6142 current loss 0.033733, current_train_items 196576.
I0304 19:31:00.701072 23118544486528 run.py:483] Algo bellman_ford step 6143 current loss 0.056410, current_train_items 196608.
I0304 19:31:00.734748 23118544486528 run.py:483] Algo bellman_ford step 6144 current loss 0.043879, current_train_items 196640.
I0304 19:31:00.754278 23118544486528 run.py:483] Algo bellman_ford step 6145 current loss 0.011947, current_train_items 196672.
I0304 19:31:00.770830 23118544486528 run.py:483] Algo bellman_ford step 6146 current loss 0.013801, current_train_items 196704.
I0304 19:31:00.794456 23118544486528 run.py:483] Algo bellman_ford step 6147 current loss 0.037145, current_train_items 196736.
I0304 19:31:00.825957 23118544486528 run.py:483] Algo bellman_ford step 6148 current loss 0.038125, current_train_items 196768.
I0304 19:31:00.860973 23118544486528 run.py:483] Algo bellman_ford step 6149 current loss 0.117507, current_train_items 196800.
I0304 19:31:00.880778 23118544486528 run.py:483] Algo bellman_ford step 6150 current loss 0.005556, current_train_items 196832.
I0304 19:31:00.888816 23118544486528 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0304 19:31:00.888921 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:00.905807 23118544486528 run.py:483] Algo bellman_ford step 6151 current loss 0.010946, current_train_items 196864.
I0304 19:31:00.929850 23118544486528 run.py:483] Algo bellman_ford step 6152 current loss 0.024519, current_train_items 196896.
I0304 19:31:00.960335 23118544486528 run.py:483] Algo bellman_ford step 6153 current loss 0.065012, current_train_items 196928.
I0304 19:31:00.994491 23118544486528 run.py:483] Algo bellman_ford step 6154 current loss 0.067254, current_train_items 196960.
I0304 19:31:01.014729 23118544486528 run.py:483] Algo bellman_ford step 6155 current loss 0.008010, current_train_items 196992.
I0304 19:31:01.030985 23118544486528 run.py:483] Algo bellman_ford step 6156 current loss 0.060894, current_train_items 197024.
I0304 19:31:01.054877 23118544486528 run.py:483] Algo bellman_ford step 6157 current loss 0.032334, current_train_items 197056.
I0304 19:31:01.085599 23118544486528 run.py:483] Algo bellman_ford step 6158 current loss 0.096321, current_train_items 197088.
I0304 19:31:01.119351 23118544486528 run.py:483] Algo bellman_ford step 6159 current loss 0.042946, current_train_items 197120.
I0304 19:31:01.139170 23118544486528 run.py:483] Algo bellman_ford step 6160 current loss 0.200863, current_train_items 197152.
I0304 19:31:01.156045 23118544486528 run.py:483] Algo bellman_ford step 6161 current loss 0.020619, current_train_items 197184.
I0304 19:31:01.179789 23118544486528 run.py:483] Algo bellman_ford step 6162 current loss 0.085597, current_train_items 197216.
I0304 19:31:01.212857 23118544486528 run.py:483] Algo bellman_ford step 6163 current loss 0.079792, current_train_items 197248.
I0304 19:31:01.248108 23118544486528 run.py:483] Algo bellman_ford step 6164 current loss 0.115815, current_train_items 197280.
I0304 19:31:01.268100 23118544486528 run.py:483] Algo bellman_ford step 6165 current loss 0.009899, current_train_items 197312.
I0304 19:31:01.285069 23118544486528 run.py:483] Algo bellman_ford step 6166 current loss 0.016553, current_train_items 197344.
I0304 19:31:01.309625 23118544486528 run.py:483] Algo bellman_ford step 6167 current loss 0.087492, current_train_items 197376.
I0304 19:31:01.341526 23118544486528 run.py:483] Algo bellman_ford step 6168 current loss 0.058084, current_train_items 197408.
I0304 19:31:01.376112 23118544486528 run.py:483] Algo bellman_ford step 6169 current loss 0.079194, current_train_items 197440.
I0304 19:31:01.396871 23118544486528 run.py:483] Algo bellman_ford step 6170 current loss 0.008535, current_train_items 197472.
I0304 19:31:01.413246 23118544486528 run.py:483] Algo bellman_ford step 6171 current loss 0.024695, current_train_items 197504.
I0304 19:31:01.437863 23118544486528 run.py:483] Algo bellman_ford step 6172 current loss 0.035814, current_train_items 197536.
I0304 19:31:01.470762 23118544486528 run.py:483] Algo bellman_ford step 6173 current loss 0.064380, current_train_items 197568.
I0304 19:31:01.504977 23118544486528 run.py:483] Algo bellman_ford step 6174 current loss 0.068243, current_train_items 197600.
I0304 19:31:01.525090 23118544486528 run.py:483] Algo bellman_ford step 6175 current loss 0.005193, current_train_items 197632.
I0304 19:31:01.541869 23118544486528 run.py:483] Algo bellman_ford step 6176 current loss 0.017769, current_train_items 197664.
I0304 19:31:01.565512 23118544486528 run.py:483] Algo bellman_ford step 6177 current loss 0.051218, current_train_items 197696.
I0304 19:31:01.597272 23118544486528 run.py:483] Algo bellman_ford step 6178 current loss 0.040904, current_train_items 197728.
I0304 19:31:01.631806 23118544486528 run.py:483] Algo bellman_ford step 6179 current loss 0.083919, current_train_items 197760.
I0304 19:31:01.651817 23118544486528 run.py:483] Algo bellman_ford step 6180 current loss 0.006679, current_train_items 197792.
I0304 19:31:01.668356 23118544486528 run.py:483] Algo bellman_ford step 6181 current loss 0.029261, current_train_items 197824.
I0304 19:31:01.692128 23118544486528 run.py:483] Algo bellman_ford step 6182 current loss 0.032834, current_train_items 197856.
I0304 19:31:01.724076 23118544486528 run.py:483] Algo bellman_ford step 6183 current loss 0.113443, current_train_items 197888.
I0304 19:31:01.760834 23118544486528 run.py:483] Algo bellman_ford step 6184 current loss 0.092552, current_train_items 197920.
I0304 19:31:01.781636 23118544486528 run.py:483] Algo bellman_ford step 6185 current loss 0.005708, current_train_items 197952.
I0304 19:31:01.798212 23118544486528 run.py:483] Algo bellman_ford step 6186 current loss 0.045927, current_train_items 197984.
I0304 19:31:01.821538 23118544486528 run.py:483] Algo bellman_ford step 6187 current loss 0.038149, current_train_items 198016.
I0304 19:31:01.852387 23118544486528 run.py:483] Algo bellman_ford step 6188 current loss 0.031188, current_train_items 198048.
I0304 19:31:01.887313 23118544486528 run.py:483] Algo bellman_ford step 6189 current loss 0.047533, current_train_items 198080.
I0304 19:31:01.907629 23118544486528 run.py:483] Algo bellman_ford step 6190 current loss 0.006524, current_train_items 198112.
I0304 19:31:01.924378 23118544486528 run.py:483] Algo bellman_ford step 6191 current loss 0.039398, current_train_items 198144.
I0304 19:31:01.949113 23118544486528 run.py:483] Algo bellman_ford step 6192 current loss 0.030490, current_train_items 198176.
I0304 19:31:01.981306 23118544486528 run.py:483] Algo bellman_ford step 6193 current loss 0.082597, current_train_items 198208.
I0304 19:31:02.014449 23118544486528 run.py:483] Algo bellman_ford step 6194 current loss 0.057118, current_train_items 198240.
I0304 19:31:02.034111 23118544486528 run.py:483] Algo bellman_ford step 6195 current loss 0.004018, current_train_items 198272.
I0304 19:31:02.050173 23118544486528 run.py:483] Algo bellman_ford step 6196 current loss 0.017419, current_train_items 198304.
I0304 19:31:02.075464 23118544486528 run.py:483] Algo bellman_ford step 6197 current loss 0.052827, current_train_items 198336.
I0304 19:31:02.106388 23118544486528 run.py:483] Algo bellman_ford step 6198 current loss 0.031526, current_train_items 198368.
I0304 19:31:02.141372 23118544486528 run.py:483] Algo bellman_ford step 6199 current loss 0.098706, current_train_items 198400.
I0304 19:31:02.161791 23118544486528 run.py:483] Algo bellman_ford step 6200 current loss 0.008509, current_train_items 198432.
I0304 19:31:02.169524 23118544486528 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0304 19:31:02.169628 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0304 19:31:02.186734 23118544486528 run.py:483] Algo bellman_ford step 6201 current loss 0.021219, current_train_items 198464.
I0304 19:31:02.212420 23118544486528 run.py:483] Algo bellman_ford step 6202 current loss 0.089687, current_train_items 198496.
I0304 19:31:02.243519 23118544486528 run.py:483] Algo bellman_ford step 6203 current loss 0.044948, current_train_items 198528.
I0304 19:31:02.277017 23118544486528 run.py:483] Algo bellman_ford step 6204 current loss 0.079503, current_train_items 198560.
I0304 19:31:02.297267 23118544486528 run.py:483] Algo bellman_ford step 6205 current loss 0.004526, current_train_items 198592.
I0304 19:31:02.312811 23118544486528 run.py:483] Algo bellman_ford step 6206 current loss 0.016292, current_train_items 198624.
I0304 19:31:02.337377 23118544486528 run.py:483] Algo bellman_ford step 6207 current loss 0.065867, current_train_items 198656.
I0304 19:31:02.368874 23118544486528 run.py:483] Algo bellman_ford step 6208 current loss 0.100239, current_train_items 198688.
I0304 19:31:02.404336 23118544486528 run.py:483] Algo bellman_ford step 6209 current loss 0.096274, current_train_items 198720.
I0304 19:31:02.424399 23118544486528 run.py:483] Algo bellman_ford step 6210 current loss 0.006283, current_train_items 198752.
I0304 19:31:02.440980 23118544486528 run.py:483] Algo bellman_ford step 6211 current loss 0.017494, current_train_items 198784.
I0304 19:31:02.465124 23118544486528 run.py:483] Algo bellman_ford step 6212 current loss 0.054295, current_train_items 198816.
I0304 19:31:02.495468 23118544486528 run.py:483] Algo bellman_ford step 6213 current loss 0.068312, current_train_items 198848.
I0304 19:31:02.531441 23118544486528 run.py:483] Algo bellman_ford step 6214 current loss 0.068342, current_train_items 198880.
I0304 19:31:02.551453 23118544486528 run.py:483] Algo bellman_ford step 6215 current loss 0.008652, current_train_items 198912.
I0304 19:31:02.567968 23118544486528 run.py:483] Algo bellman_ford step 6216 current loss 0.022451, current_train_items 198944.
I0304 19:31:02.591474 23118544486528 run.py:483] Algo bellman_ford step 6217 current loss 0.035722, current_train_items 198976.
I0304 19:31:02.623180 23118544486528 run.py:483] Algo bellman_ford step 6218 current loss 0.060455, current_train_items 199008.
I0304 19:31:02.656406 23118544486528 run.py:483] Algo bellman_ford step 6219 current loss 0.071513, current_train_items 199040.
I0304 19:31:02.676333 23118544486528 run.py:483] Algo bellman_ford step 6220 current loss 0.007788, current_train_items 199072.
I0304 19:31:02.692242 23118544486528 run.py:483] Algo bellman_ford step 6221 current loss 0.031255, current_train_items 199104.
I0304 19:31:02.717214 23118544486528 run.py:483] Algo bellman_ford step 6222 current loss 0.062693, current_train_items 199136.
I0304 19:31:02.747809 23118544486528 run.py:483] Algo bellman_ford step 6223 current loss 0.071283, current_train_items 199168.
I0304 19:31:02.782979 23118544486528 run.py:483] Algo bellman_ford step 6224 current loss 0.098537, current_train_items 199200.
I0304 19:31:02.802834 23118544486528 run.py:483] Algo bellman_ford step 6225 current loss 0.005391, current_train_items 199232.
I0304 19:31:02.819232 23118544486528 run.py:483] Algo bellman_ford step 6226 current loss 0.031437, current_train_items 199264.
I0304 19:31:02.842497 23118544486528 run.py:483] Algo bellman_ford step 6227 current loss 0.025526, current_train_items 199296.
I0304 19:31:02.872861 23118544486528 run.py:483] Algo bellman_ford step 6228 current loss 0.059795, current_train_items 199328.
I0304 19:31:02.905029 23118544486528 run.py:483] Algo bellman_ford step 6229 current loss 0.072687, current_train_items 199360.
I0304 19:31:02.924840 23118544486528 run.py:483] Algo bellman_ford step 6230 current loss 0.004619, current_train_items 199392.
I0304 19:31:02.941327 23118544486528 run.py:483] Algo bellman_ford step 6231 current loss 0.026075, current_train_items 199424.
I0304 19:31:02.965738 23118544486528 run.py:483] Algo bellman_ford step 6232 current loss 0.053962, current_train_items 199456.
I0304 19:31:02.996518 23118544486528 run.py:483] Algo bellman_ford step 6233 current loss 0.076090, current_train_items 199488.
I0304 19:31:03.030151 23118544486528 run.py:483] Algo bellman_ford step 6234 current loss 0.102119, current_train_items 199520.
I0304 19:31:03.049778 23118544486528 run.py:483] Algo bellman_ford step 6235 current loss 0.004690, current_train_items 199552.
I0304 19:31:03.066290 23118544486528 run.py:483] Algo bellman_ford step 6236 current loss 0.022692, current_train_items 199584.
I0304 19:31:03.089638 23118544486528 run.py:483] Algo bellman_ford step 6237 current loss 0.044828, current_train_items 199616.
I0304 19:31:03.121918 23118544486528 run.py:483] Algo bellman_ford step 6238 current loss 0.055355, current_train_items 199648.
I0304 19:31:03.153815 23118544486528 run.py:483] Algo bellman_ford step 6239 current loss 0.099171, current_train_items 199680.
I0304 19:31:03.173520 23118544486528 run.py:483] Algo bellman_ford step 6240 current loss 0.006595, current_train_items 199712.
I0304 19:31:03.189965 23118544486528 run.py:483] Algo bellman_ford step 6241 current loss 0.055663, current_train_items 199744.
I0304 19:31:03.214448 23118544486528 run.py:483] Algo bellman_ford step 6242 current loss 0.080521, current_train_items 199776.
I0304 19:31:03.246433 23118544486528 run.py:483] Algo bellman_ford step 6243 current loss 0.064952, current_train_items 199808.
I0304 19:31:03.279513 23118544486528 run.py:483] Algo bellman_ford step 6244 current loss 0.049056, current_train_items 199840.
I0304 19:31:03.299402 23118544486528 run.py:483] Algo bellman_ford step 6245 current loss 0.010542, current_train_items 199872.
I0304 19:31:03.315867 23118544486528 run.py:483] Algo bellman_ford step 6246 current loss 0.066663, current_train_items 199904.
I0304 19:31:03.339576 23118544486528 run.py:483] Algo bellman_ford step 6247 current loss 0.037876, current_train_items 199936.
I0304 19:31:03.372270 23118544486528 run.py:483] Algo bellman_ford step 6248 current loss 0.073151, current_train_items 199968.
I0304 19:31:03.405279 23118544486528 run.py:483] Algo bellman_ford step 6249 current loss 0.102738, current_train_items 200000.
I0304 19:31:03.425248 23118544486528 run.py:483] Algo bellman_ford step 6250 current loss 0.006205, current_train_items 200032.
I0304 19:31:03.433117 23118544486528 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0304 19:31:03.433223 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:03.450435 23118544486528 run.py:483] Algo bellman_ford step 6251 current loss 0.037212, current_train_items 200064.
I0304 19:31:03.475078 23118544486528 run.py:483] Algo bellman_ford step 6252 current loss 0.080699, current_train_items 200096.
I0304 19:31:03.508260 23118544486528 run.py:483] Algo bellman_ford step 6253 current loss 0.132913, current_train_items 200128.
I0304 19:31:03.541167 23118544486528 run.py:483] Algo bellman_ford step 6254 current loss 0.068869, current_train_items 200160.
I0304 19:31:03.560923 23118544486528 run.py:483] Algo bellman_ford step 6255 current loss 0.042248, current_train_items 200192.
I0304 19:31:03.576953 23118544486528 run.py:483] Algo bellman_ford step 6256 current loss 0.028848, current_train_items 200224.
I0304 19:31:03.601681 23118544486528 run.py:483] Algo bellman_ford step 6257 current loss 0.050991, current_train_items 200256.
I0304 19:31:03.632662 23118544486528 run.py:483] Algo bellman_ford step 6258 current loss 0.117492, current_train_items 200288.
I0304 19:31:03.665941 23118544486528 run.py:483] Algo bellman_ford step 6259 current loss 0.100364, current_train_items 200320.
I0304 19:31:03.686272 23118544486528 run.py:483] Algo bellman_ford step 6260 current loss 0.022300, current_train_items 200352.
I0304 19:31:03.702985 23118544486528 run.py:483] Algo bellman_ford step 6261 current loss 0.023914, current_train_items 200384.
I0304 19:31:03.726879 23118544486528 run.py:483] Algo bellman_ford step 6262 current loss 0.018581, current_train_items 200416.
I0304 19:31:03.758376 23118544486528 run.py:483] Algo bellman_ford step 6263 current loss 0.097912, current_train_items 200448.
I0304 19:31:03.792902 23118544486528 run.py:483] Algo bellman_ford step 6264 current loss 0.108993, current_train_items 200480.
I0304 19:31:03.813063 23118544486528 run.py:483] Algo bellman_ford step 6265 current loss 0.010948, current_train_items 200512.
I0304 19:31:03.830321 23118544486528 run.py:483] Algo bellman_ford step 6266 current loss 0.085631, current_train_items 200544.
I0304 19:31:03.855436 23118544486528 run.py:483] Algo bellman_ford step 6267 current loss 0.052134, current_train_items 200576.
I0304 19:31:03.886950 23118544486528 run.py:483] Algo bellman_ford step 6268 current loss 0.043968, current_train_items 200608.
I0304 19:31:03.917747 23118544486528 run.py:483] Algo bellman_ford step 6269 current loss 0.053121, current_train_items 200640.
I0304 19:31:03.937538 23118544486528 run.py:483] Algo bellman_ford step 6270 current loss 0.004312, current_train_items 200672.
I0304 19:31:03.954147 23118544486528 run.py:483] Algo bellman_ford step 6271 current loss 0.054523, current_train_items 200704.
I0304 19:31:03.978096 23118544486528 run.py:483] Algo bellman_ford step 6272 current loss 0.030954, current_train_items 200736.
I0304 19:31:04.009101 23118544486528 run.py:483] Algo bellman_ford step 6273 current loss 0.046365, current_train_items 200768.
I0304 19:31:04.044485 23118544486528 run.py:483] Algo bellman_ford step 6274 current loss 0.080881, current_train_items 200800.
I0304 19:31:04.064990 23118544486528 run.py:483] Algo bellman_ford step 6275 current loss 0.004035, current_train_items 200832.
I0304 19:31:04.081754 23118544486528 run.py:483] Algo bellman_ford step 6276 current loss 0.028000, current_train_items 200864.
I0304 19:31:04.105572 23118544486528 run.py:483] Algo bellman_ford step 6277 current loss 0.059990, current_train_items 200896.
I0304 19:31:04.136447 23118544486528 run.py:483] Algo bellman_ford step 6278 current loss 0.049891, current_train_items 200928.
I0304 19:31:04.171547 23118544486528 run.py:483] Algo bellman_ford step 6279 current loss 0.068090, current_train_items 200960.
I0304 19:31:04.191433 23118544486528 run.py:483] Algo bellman_ford step 6280 current loss 0.004477, current_train_items 200992.
I0304 19:31:04.207940 23118544486528 run.py:483] Algo bellman_ford step 6281 current loss 0.015287, current_train_items 201024.
I0304 19:31:04.231941 23118544486528 run.py:483] Algo bellman_ford step 6282 current loss 0.067973, current_train_items 201056.
I0304 19:31:04.263944 23118544486528 run.py:483] Algo bellman_ford step 6283 current loss 0.080824, current_train_items 201088.
I0304 19:31:04.299209 23118544486528 run.py:483] Algo bellman_ford step 6284 current loss 0.100330, current_train_items 201120.
I0304 19:31:04.319414 23118544486528 run.py:483] Algo bellman_ford step 6285 current loss 0.006880, current_train_items 201152.
I0304 19:31:04.336083 23118544486528 run.py:483] Algo bellman_ford step 6286 current loss 0.020697, current_train_items 201184.
I0304 19:31:04.360450 23118544486528 run.py:483] Algo bellman_ford step 6287 current loss 0.033274, current_train_items 201216.
I0304 19:31:04.392441 23118544486528 run.py:483] Algo bellman_ford step 6288 current loss 0.054407, current_train_items 201248.
I0304 19:31:04.427045 23118544486528 run.py:483] Algo bellman_ford step 6289 current loss 0.058154, current_train_items 201280.
I0304 19:31:04.447002 23118544486528 run.py:483] Algo bellman_ford step 6290 current loss 0.005768, current_train_items 201312.
I0304 19:31:04.463695 23118544486528 run.py:483] Algo bellman_ford step 6291 current loss 0.041115, current_train_items 201344.
I0304 19:31:04.487512 23118544486528 run.py:483] Algo bellman_ford step 6292 current loss 0.037954, current_train_items 201376.
I0304 19:31:04.520048 23118544486528 run.py:483] Algo bellman_ford step 6293 current loss 0.172387, current_train_items 201408.
I0304 19:31:04.555939 23118544486528 run.py:483] Algo bellman_ford step 6294 current loss 0.074236, current_train_items 201440.
I0304 19:31:04.575696 23118544486528 run.py:483] Algo bellman_ford step 6295 current loss 0.005581, current_train_items 201472.
I0304 19:31:04.592899 23118544486528 run.py:483] Algo bellman_ford step 6296 current loss 0.043669, current_train_items 201504.
I0304 19:31:04.616487 23118544486528 run.py:483] Algo bellman_ford step 6297 current loss 0.021187, current_train_items 201536.
I0304 19:31:04.648565 23118544486528 run.py:483] Algo bellman_ford step 6298 current loss 0.071586, current_train_items 201568.
I0304 19:31:04.682441 23118544486528 run.py:483] Algo bellman_ford step 6299 current loss 0.081521, current_train_items 201600.
I0304 19:31:04.702452 23118544486528 run.py:483] Algo bellman_ford step 6300 current loss 0.006922, current_train_items 201632.
I0304 19:31:04.710397 23118544486528 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0304 19:31:04.710503 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:04.727607 23118544486528 run.py:483] Algo bellman_ford step 6301 current loss 0.020099, current_train_items 201664.
I0304 19:31:04.751734 23118544486528 run.py:483] Algo bellman_ford step 6302 current loss 0.066898, current_train_items 201696.
I0304 19:31:04.782939 23118544486528 run.py:483] Algo bellman_ford step 6303 current loss 0.052135, current_train_items 201728.
I0304 19:31:04.814651 23118544486528 run.py:483] Algo bellman_ford step 6304 current loss 0.069715, current_train_items 201760.
I0304 19:31:04.834666 23118544486528 run.py:483] Algo bellman_ford step 6305 current loss 0.019699, current_train_items 201792.
I0304 19:31:04.850476 23118544486528 run.py:483] Algo bellman_ford step 6306 current loss 0.019051, current_train_items 201824.
I0304 19:31:04.875000 23118544486528 run.py:483] Algo bellman_ford step 6307 current loss 0.064235, current_train_items 201856.
I0304 19:31:04.907111 23118544486528 run.py:483] Algo bellman_ford step 6308 current loss 0.062892, current_train_items 201888.
I0304 19:31:04.940522 23118544486528 run.py:483] Algo bellman_ford step 6309 current loss 0.107145, current_train_items 201920.
I0304 19:31:04.960216 23118544486528 run.py:483] Algo bellman_ford step 6310 current loss 0.009174, current_train_items 201952.
I0304 19:31:04.976451 23118544486528 run.py:483] Algo bellman_ford step 6311 current loss 0.011946, current_train_items 201984.
I0304 19:31:04.999143 23118544486528 run.py:483] Algo bellman_ford step 6312 current loss 0.057078, current_train_items 202016.
I0304 19:31:05.031879 23118544486528 run.py:483] Algo bellman_ford step 6313 current loss 0.135531, current_train_items 202048.
I0304 19:31:05.066213 23118544486528 run.py:483] Algo bellman_ford step 6314 current loss 0.135026, current_train_items 202080.
I0304 19:31:05.086126 23118544486528 run.py:483] Algo bellman_ford step 6315 current loss 0.008377, current_train_items 202112.
I0304 19:31:05.102282 23118544486528 run.py:483] Algo bellman_ford step 6316 current loss 0.012541, current_train_items 202144.
I0304 19:31:05.125973 23118544486528 run.py:483] Algo bellman_ford step 6317 current loss 0.034544, current_train_items 202176.
I0304 19:31:05.156517 23118544486528 run.py:483] Algo bellman_ford step 6318 current loss 0.022526, current_train_items 202208.
I0304 19:31:05.190017 23118544486528 run.py:483] Algo bellman_ford step 6319 current loss 0.082242, current_train_items 202240.
I0304 19:31:05.209678 23118544486528 run.py:483] Algo bellman_ford step 6320 current loss 0.009307, current_train_items 202272.
I0304 19:31:05.226289 23118544486528 run.py:483] Algo bellman_ford step 6321 current loss 0.032760, current_train_items 202304.
I0304 19:31:05.251298 23118544486528 run.py:483] Algo bellman_ford step 6322 current loss 0.031121, current_train_items 202336.
I0304 19:31:05.283694 23118544486528 run.py:483] Algo bellman_ford step 6323 current loss 0.039976, current_train_items 202368.
I0304 19:31:05.318351 23118544486528 run.py:483] Algo bellman_ford step 6324 current loss 0.064190, current_train_items 202400.
I0304 19:31:05.338040 23118544486528 run.py:483] Algo bellman_ford step 6325 current loss 0.006999, current_train_items 202432.
I0304 19:31:05.354151 23118544486528 run.py:483] Algo bellman_ford step 6326 current loss 0.015070, current_train_items 202464.
I0304 19:31:05.379053 23118544486528 run.py:483] Algo bellman_ford step 6327 current loss 0.046775, current_train_items 202496.
I0304 19:31:05.410227 23118544486528 run.py:483] Algo bellman_ford step 6328 current loss 0.067687, current_train_items 202528.
I0304 19:31:05.444847 23118544486528 run.py:483] Algo bellman_ford step 6329 current loss 0.053168, current_train_items 202560.
I0304 19:31:05.464340 23118544486528 run.py:483] Algo bellman_ford step 6330 current loss 0.004529, current_train_items 202592.
I0304 19:31:05.480739 23118544486528 run.py:483] Algo bellman_ford step 6331 current loss 0.039570, current_train_items 202624.
I0304 19:31:05.504182 23118544486528 run.py:483] Algo bellman_ford step 6332 current loss 0.054001, current_train_items 202656.
I0304 19:31:05.535870 23118544486528 run.py:483] Algo bellman_ford step 6333 current loss 0.028356, current_train_items 202688.
I0304 19:31:05.568884 23118544486528 run.py:483] Algo bellman_ford step 6334 current loss 0.040402, current_train_items 202720.
I0304 19:31:05.588595 23118544486528 run.py:483] Algo bellman_ford step 6335 current loss 0.004119, current_train_items 202752.
I0304 19:31:05.605207 23118544486528 run.py:483] Algo bellman_ford step 6336 current loss 0.015550, current_train_items 202784.
I0304 19:31:05.629929 23118544486528 run.py:483] Algo bellman_ford step 6337 current loss 0.034095, current_train_items 202816.
I0304 19:31:05.662340 23118544486528 run.py:483] Algo bellman_ford step 6338 current loss 0.051639, current_train_items 202848.
I0304 19:31:05.695615 23118544486528 run.py:483] Algo bellman_ford step 6339 current loss 0.061236, current_train_items 202880.
I0304 19:31:05.715489 23118544486528 run.py:483] Algo bellman_ford step 6340 current loss 0.003861, current_train_items 202912.
I0304 19:31:05.731522 23118544486528 run.py:483] Algo bellman_ford step 6341 current loss 0.011036, current_train_items 202944.
I0304 19:31:05.756196 23118544486528 run.py:483] Algo bellman_ford step 6342 current loss 0.058763, current_train_items 202976.
I0304 19:31:05.786977 23118544486528 run.py:483] Algo bellman_ford step 6343 current loss 0.028576, current_train_items 203008.
I0304 19:31:05.818650 23118544486528 run.py:483] Algo bellman_ford step 6344 current loss 0.074849, current_train_items 203040.
I0304 19:31:05.838456 23118544486528 run.py:483] Algo bellman_ford step 6345 current loss 0.005227, current_train_items 203072.
I0304 19:31:05.855097 23118544486528 run.py:483] Algo bellman_ford step 6346 current loss 0.043930, current_train_items 203104.
I0304 19:31:05.878916 23118544486528 run.py:483] Algo bellman_ford step 6347 current loss 0.059221, current_train_items 203136.
I0304 19:31:05.909716 23118544486528 run.py:483] Algo bellman_ford step 6348 current loss 0.082934, current_train_items 203168.
I0304 19:31:05.945507 23118544486528 run.py:483] Algo bellman_ford step 6349 current loss 0.055362, current_train_items 203200.
I0304 19:31:05.965211 23118544486528 run.py:483] Algo bellman_ford step 6350 current loss 0.029364, current_train_items 203232.
I0304 19:31:05.973219 23118544486528 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0304 19:31:05.973325 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:31:05.990736 23118544486528 run.py:483] Algo bellman_ford step 6351 current loss 0.014182, current_train_items 203264.
I0304 19:31:06.014649 23118544486528 run.py:483] Algo bellman_ford step 6352 current loss 0.069225, current_train_items 203296.
I0304 19:31:06.046834 23118544486528 run.py:483] Algo bellman_ford step 6353 current loss 0.089925, current_train_items 203328.
I0304 19:31:06.081012 23118544486528 run.py:483] Algo bellman_ford step 6354 current loss 0.087084, current_train_items 203360.
I0304 19:31:06.101137 23118544486528 run.py:483] Algo bellman_ford step 6355 current loss 0.005039, current_train_items 203392.
I0304 19:31:06.117503 23118544486528 run.py:483] Algo bellman_ford step 6356 current loss 0.015343, current_train_items 203424.
I0304 19:31:06.142743 23118544486528 run.py:483] Algo bellman_ford step 6357 current loss 0.045585, current_train_items 203456.
I0304 19:31:06.173987 23118544486528 run.py:483] Algo bellman_ford step 6358 current loss 0.064114, current_train_items 203488.
I0304 19:31:06.208214 23118544486528 run.py:483] Algo bellman_ford step 6359 current loss 0.089719, current_train_items 203520.
I0304 19:31:06.228043 23118544486528 run.py:483] Algo bellman_ford step 6360 current loss 0.006882, current_train_items 203552.
I0304 19:31:06.244906 23118544486528 run.py:483] Algo bellman_ford step 6361 current loss 0.014867, current_train_items 203584.
I0304 19:31:06.267940 23118544486528 run.py:483] Algo bellman_ford step 6362 current loss 0.048210, current_train_items 203616.
I0304 19:31:06.299409 23118544486528 run.py:483] Algo bellman_ford step 6363 current loss 0.037412, current_train_items 203648.
I0304 19:31:06.332502 23118544486528 run.py:483] Algo bellman_ford step 6364 current loss 0.076659, current_train_items 203680.
I0304 19:31:06.352238 23118544486528 run.py:483] Algo bellman_ford step 6365 current loss 0.003091, current_train_items 203712.
I0304 19:31:06.369066 23118544486528 run.py:483] Algo bellman_ford step 6366 current loss 0.025132, current_train_items 203744.
I0304 19:31:06.392860 23118544486528 run.py:483] Algo bellman_ford step 6367 current loss 0.065472, current_train_items 203776.
I0304 19:31:06.425547 23118544486528 run.py:483] Algo bellman_ford step 6368 current loss 0.082087, current_train_items 203808.
I0304 19:31:06.458152 23118544486528 run.py:483] Algo bellman_ford step 6369 current loss 0.041239, current_train_items 203840.
I0304 19:31:06.478410 23118544486528 run.py:483] Algo bellman_ford step 6370 current loss 0.003222, current_train_items 203872.
I0304 19:31:06.495062 23118544486528 run.py:483] Algo bellman_ford step 6371 current loss 0.013968, current_train_items 203904.
I0304 19:31:06.519818 23118544486528 run.py:483] Algo bellman_ford step 6372 current loss 0.176992, current_train_items 203936.
I0304 19:31:06.551337 23118544486528 run.py:483] Algo bellman_ford step 6373 current loss 0.194772, current_train_items 203968.
I0304 19:31:06.585602 23118544486528 run.py:483] Algo bellman_ford step 6374 current loss 0.122328, current_train_items 204000.
I0304 19:31:06.605858 23118544486528 run.py:483] Algo bellman_ford step 6375 current loss 0.003557, current_train_items 204032.
I0304 19:31:06.622087 23118544486528 run.py:483] Algo bellman_ford step 6376 current loss 0.006620, current_train_items 204064.
I0304 19:31:06.646406 23118544486528 run.py:483] Algo bellman_ford step 6377 current loss 0.042767, current_train_items 204096.
I0304 19:31:06.677991 23118544486528 run.py:483] Algo bellman_ford step 6378 current loss 0.089462, current_train_items 204128.
I0304 19:31:06.713709 23118544486528 run.py:483] Algo bellman_ford step 6379 current loss 0.107969, current_train_items 204160.
I0304 19:31:06.733596 23118544486528 run.py:483] Algo bellman_ford step 6380 current loss 0.015502, current_train_items 204192.
I0304 19:31:06.749984 23118544486528 run.py:483] Algo bellman_ford step 6381 current loss 0.077733, current_train_items 204224.
I0304 19:31:06.774062 23118544486528 run.py:483] Algo bellman_ford step 6382 current loss 0.030850, current_train_items 204256.
I0304 19:31:06.804928 23118544486528 run.py:483] Algo bellman_ford step 6383 current loss 0.043075, current_train_items 204288.
I0304 19:31:06.838976 23118544486528 run.py:483] Algo bellman_ford step 6384 current loss 0.071685, current_train_items 204320.
I0304 19:31:06.859318 23118544486528 run.py:483] Algo bellman_ford step 6385 current loss 0.009872, current_train_items 204352.
I0304 19:31:06.875362 23118544486528 run.py:483] Algo bellman_ford step 6386 current loss 0.020709, current_train_items 204384.
I0304 19:31:06.899803 23118544486528 run.py:483] Algo bellman_ford step 6387 current loss 0.064737, current_train_items 204416.
I0304 19:31:06.931111 23118544486528 run.py:483] Algo bellman_ford step 6388 current loss 0.046117, current_train_items 204448.
I0304 19:31:06.965152 23118544486528 run.py:483] Algo bellman_ford step 6389 current loss 0.036829, current_train_items 204480.
I0304 19:31:06.985383 23118544486528 run.py:483] Algo bellman_ford step 6390 current loss 0.004839, current_train_items 204512.
I0304 19:31:07.002155 23118544486528 run.py:483] Algo bellman_ford step 6391 current loss 0.018051, current_train_items 204544.
I0304 19:31:07.025814 23118544486528 run.py:483] Algo bellman_ford step 6392 current loss 0.054650, current_train_items 204576.
I0304 19:31:07.055805 23118544486528 run.py:483] Algo bellman_ford step 6393 current loss 0.047995, current_train_items 204608.
I0304 19:31:07.090338 23118544486528 run.py:483] Algo bellman_ford step 6394 current loss 0.120052, current_train_items 204640.
I0304 19:31:07.110660 23118544486528 run.py:483] Algo bellman_ford step 6395 current loss 0.005935, current_train_items 204672.
I0304 19:31:07.127064 23118544486528 run.py:483] Algo bellman_ford step 6396 current loss 0.012829, current_train_items 204704.
I0304 19:31:07.150571 23118544486528 run.py:483] Algo bellman_ford step 6397 current loss 0.035788, current_train_items 204736.
I0304 19:31:07.181499 23118544486528 run.py:483] Algo bellman_ford step 6398 current loss 0.065436, current_train_items 204768.
I0304 19:31:07.213889 23118544486528 run.py:483] Algo bellman_ford step 6399 current loss 0.091108, current_train_items 204800.
I0304 19:31:07.234132 23118544486528 run.py:483] Algo bellman_ford step 6400 current loss 0.004391, current_train_items 204832.
I0304 19:31:07.241753 23118544486528 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0304 19:31:07.241858 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:07.258240 23118544486528 run.py:483] Algo bellman_ford step 6401 current loss 0.010772, current_train_items 204864.
I0304 19:31:07.282387 23118544486528 run.py:483] Algo bellman_ford step 6402 current loss 0.019422, current_train_items 204896.
I0304 19:31:07.314169 23118544486528 run.py:483] Algo bellman_ford step 6403 current loss 0.084971, current_train_items 204928.
I0304 19:31:07.348427 23118544486528 run.py:483] Algo bellman_ford step 6404 current loss 0.057684, current_train_items 204960.
I0304 19:31:07.368862 23118544486528 run.py:483] Algo bellman_ford step 6405 current loss 0.010912, current_train_items 204992.
I0304 19:31:07.384861 23118544486528 run.py:483] Algo bellman_ford step 6406 current loss 0.030411, current_train_items 205024.
I0304 19:31:07.408909 23118544486528 run.py:483] Algo bellman_ford step 6407 current loss 0.033461, current_train_items 205056.
I0304 19:31:07.440417 23118544486528 run.py:483] Algo bellman_ford step 6408 current loss 0.051830, current_train_items 205088.
I0304 19:31:07.475788 23118544486528 run.py:483] Algo bellman_ford step 6409 current loss 0.062097, current_train_items 205120.
I0304 19:31:07.495555 23118544486528 run.py:483] Algo bellman_ford step 6410 current loss 0.012784, current_train_items 205152.
I0304 19:31:07.511744 23118544486528 run.py:483] Algo bellman_ford step 6411 current loss 0.017462, current_train_items 205184.
I0304 19:31:07.535343 23118544486528 run.py:483] Algo bellman_ford step 6412 current loss 0.047009, current_train_items 205216.
I0304 19:31:07.566881 23118544486528 run.py:483] Algo bellman_ford step 6413 current loss 0.054305, current_train_items 205248.
I0304 19:31:07.600860 23118544486528 run.py:483] Algo bellman_ford step 6414 current loss 0.059912, current_train_items 205280.
I0304 19:31:07.620712 23118544486528 run.py:483] Algo bellman_ford step 6415 current loss 0.003932, current_train_items 205312.
I0304 19:31:07.637105 23118544486528 run.py:483] Algo bellman_ford step 6416 current loss 0.032940, current_train_items 205344.
I0304 19:31:07.662143 23118544486528 run.py:483] Algo bellman_ford step 6417 current loss 0.038062, current_train_items 205376.
I0304 19:31:07.695004 23118544486528 run.py:483] Algo bellman_ford step 6418 current loss 0.051515, current_train_items 205408.
I0304 19:31:07.729633 23118544486528 run.py:483] Algo bellman_ford step 6419 current loss 0.117346, current_train_items 205440.
I0304 19:31:07.749807 23118544486528 run.py:483] Algo bellman_ford step 6420 current loss 0.004932, current_train_items 205472.
I0304 19:31:07.766401 23118544486528 run.py:483] Algo bellman_ford step 6421 current loss 0.031543, current_train_items 205504.
I0304 19:31:07.790735 23118544486528 run.py:483] Algo bellman_ford step 6422 current loss 0.039384, current_train_items 205536.
I0304 19:31:07.822121 23118544486528 run.py:483] Algo bellman_ford step 6423 current loss 0.052163, current_train_items 205568.
I0304 19:31:07.855668 23118544486528 run.py:483] Algo bellman_ford step 6424 current loss 0.048492, current_train_items 205600.
I0304 19:31:07.875497 23118544486528 run.py:483] Algo bellman_ford step 6425 current loss 0.003857, current_train_items 205632.
I0304 19:31:07.892280 23118544486528 run.py:483] Algo bellman_ford step 6426 current loss 0.019512, current_train_items 205664.
I0304 19:31:07.916867 23118544486528 run.py:483] Algo bellman_ford step 6427 current loss 0.030068, current_train_items 205696.
I0304 19:31:07.948873 23118544486528 run.py:483] Algo bellman_ford step 6428 current loss 0.042398, current_train_items 205728.
I0304 19:31:07.985070 23118544486528 run.py:483] Algo bellman_ford step 6429 current loss 0.092831, current_train_items 205760.
I0304 19:31:08.005024 23118544486528 run.py:483] Algo bellman_ford step 6430 current loss 0.005326, current_train_items 205792.
I0304 19:31:08.021067 23118544486528 run.py:483] Algo bellman_ford step 6431 current loss 0.056628, current_train_items 205824.
I0304 19:31:08.045758 23118544486528 run.py:483] Algo bellman_ford step 6432 current loss 0.043577, current_train_items 205856.
I0304 19:31:08.077797 23118544486528 run.py:483] Algo bellman_ford step 6433 current loss 0.046955, current_train_items 205888.
I0304 19:31:08.110905 23118544486528 run.py:483] Algo bellman_ford step 6434 current loss 0.042967, current_train_items 205920.
I0304 19:31:08.130443 23118544486528 run.py:483] Algo bellman_ford step 6435 current loss 0.003410, current_train_items 205952.
I0304 19:31:08.146998 23118544486528 run.py:483] Algo bellman_ford step 6436 current loss 0.009879, current_train_items 205984.
I0304 19:31:08.171142 23118544486528 run.py:483] Algo bellman_ford step 6437 current loss 0.045735, current_train_items 206016.
I0304 19:31:08.204090 23118544486528 run.py:483] Algo bellman_ford step 6438 current loss 0.070200, current_train_items 206048.
I0304 19:31:08.238907 23118544486528 run.py:483] Algo bellman_ford step 6439 current loss 0.071574, current_train_items 206080.
I0304 19:31:08.258572 23118544486528 run.py:483] Algo bellman_ford step 6440 current loss 0.005950, current_train_items 206112.
I0304 19:31:08.275205 23118544486528 run.py:483] Algo bellman_ford step 6441 current loss 0.015479, current_train_items 206144.
I0304 19:31:08.298328 23118544486528 run.py:483] Algo bellman_ford step 6442 current loss 0.016912, current_train_items 206176.
I0304 19:31:08.328606 23118544486528 run.py:483] Algo bellman_ford step 6443 current loss 0.073402, current_train_items 206208.
I0304 19:31:08.363518 23118544486528 run.py:483] Algo bellman_ford step 6444 current loss 0.073941, current_train_items 206240.
I0304 19:31:08.383367 23118544486528 run.py:483] Algo bellman_ford step 6445 current loss 0.010644, current_train_items 206272.
I0304 19:31:08.399470 23118544486528 run.py:483] Algo bellman_ford step 6446 current loss 0.016598, current_train_items 206304.
I0304 19:31:08.424240 23118544486528 run.py:483] Algo bellman_ford step 6447 current loss 0.022374, current_train_items 206336.
I0304 19:31:08.455859 23118544486528 run.py:483] Algo bellman_ford step 6448 current loss 0.049735, current_train_items 206368.
I0304 19:31:08.490150 23118544486528 run.py:483] Algo bellman_ford step 6449 current loss 0.042926, current_train_items 206400.
I0304 19:31:08.509965 23118544486528 run.py:483] Algo bellman_ford step 6450 current loss 0.005130, current_train_items 206432.
I0304 19:31:08.518009 23118544486528 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0304 19:31:08.518115 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:08.535164 23118544486528 run.py:483] Algo bellman_ford step 6451 current loss 0.008397, current_train_items 206464.
I0304 19:31:08.561019 23118544486528 run.py:483] Algo bellman_ford step 6452 current loss 0.070736, current_train_items 206496.
I0304 19:31:08.591777 23118544486528 run.py:483] Algo bellman_ford step 6453 current loss 0.039774, current_train_items 206528.
I0304 19:31:08.624376 23118544486528 run.py:483] Algo bellman_ford step 6454 current loss 0.034287, current_train_items 206560.
I0304 19:31:08.644336 23118544486528 run.py:483] Algo bellman_ford step 6455 current loss 0.038935, current_train_items 206592.
I0304 19:31:08.660501 23118544486528 run.py:483] Algo bellman_ford step 6456 current loss 0.005773, current_train_items 206624.
I0304 19:31:08.684070 23118544486528 run.py:483] Algo bellman_ford step 6457 current loss 0.033060, current_train_items 206656.
I0304 19:31:08.715292 23118544486528 run.py:483] Algo bellman_ford step 6458 current loss 0.045583, current_train_items 206688.
I0304 19:31:08.748010 23118544486528 run.py:483] Algo bellman_ford step 6459 current loss 0.077956, current_train_items 206720.
I0304 19:31:08.768134 23118544486528 run.py:483] Algo bellman_ford step 6460 current loss 0.087221, current_train_items 206752.
I0304 19:31:08.784612 23118544486528 run.py:483] Algo bellman_ford step 6461 current loss 0.007847, current_train_items 206784.
I0304 19:31:08.808284 23118544486528 run.py:483] Algo bellman_ford step 6462 current loss 0.056835, current_train_items 206816.
I0304 19:31:08.839494 23118544486528 run.py:483] Algo bellman_ford step 6463 current loss 0.019827, current_train_items 206848.
I0304 19:31:08.875844 23118544486528 run.py:483] Algo bellman_ford step 6464 current loss 0.118307, current_train_items 206880.
I0304 19:31:08.895593 23118544486528 run.py:483] Algo bellman_ford step 6465 current loss 0.003521, current_train_items 206912.
I0304 19:31:08.912283 23118544486528 run.py:483] Algo bellman_ford step 6466 current loss 0.010203, current_train_items 206944.
I0304 19:31:08.937956 23118544486528 run.py:483] Algo bellman_ford step 6467 current loss 0.064095, current_train_items 206976.
I0304 19:31:08.970545 23118544486528 run.py:483] Algo bellman_ford step 6468 current loss 0.048175, current_train_items 207008.
I0304 19:31:09.005848 23118544486528 run.py:483] Algo bellman_ford step 6469 current loss 0.058968, current_train_items 207040.
I0304 19:31:09.026415 23118544486528 run.py:483] Algo bellman_ford step 6470 current loss 0.004780, current_train_items 207072.
I0304 19:31:09.043453 23118544486528 run.py:483] Algo bellman_ford step 6471 current loss 0.016032, current_train_items 207104.
I0304 19:31:09.066797 23118544486528 run.py:483] Algo bellman_ford step 6472 current loss 0.040153, current_train_items 207136.
I0304 19:31:09.098562 23118544486528 run.py:483] Algo bellman_ford step 6473 current loss 0.060079, current_train_items 207168.
I0304 19:31:09.131522 23118544486528 run.py:483] Algo bellman_ford step 6474 current loss 0.056715, current_train_items 207200.
I0304 19:31:09.151456 23118544486528 run.py:483] Algo bellman_ford step 6475 current loss 0.022527, current_train_items 207232.
I0304 19:31:09.168386 23118544486528 run.py:483] Algo bellman_ford step 6476 current loss 0.034473, current_train_items 207264.
I0304 19:31:09.191404 23118544486528 run.py:483] Algo bellman_ford step 6477 current loss 0.044359, current_train_items 207296.
I0304 19:31:09.223006 23118544486528 run.py:483] Algo bellman_ford step 6478 current loss 0.052277, current_train_items 207328.
I0304 19:31:09.257912 23118544486528 run.py:483] Algo bellman_ford step 6479 current loss 0.057100, current_train_items 207360.
I0304 19:31:09.277916 23118544486528 run.py:483] Algo bellman_ford step 6480 current loss 0.006301, current_train_items 207392.
I0304 19:31:09.294000 23118544486528 run.py:483] Algo bellman_ford step 6481 current loss 0.052135, current_train_items 207424.
I0304 19:31:09.318064 23118544486528 run.py:483] Algo bellman_ford step 6482 current loss 0.026252, current_train_items 207456.
I0304 19:31:09.350400 23118544486528 run.py:483] Algo bellman_ford step 6483 current loss 0.049955, current_train_items 207488.
I0304 19:31:09.385343 23118544486528 run.py:483] Algo bellman_ford step 6484 current loss 0.095927, current_train_items 207520.
I0304 19:31:09.405573 23118544486528 run.py:483] Algo bellman_ford step 6485 current loss 0.007796, current_train_items 207552.
I0304 19:31:09.422344 23118544486528 run.py:483] Algo bellman_ford step 6486 current loss 0.057881, current_train_items 207584.
I0304 19:31:09.447679 23118544486528 run.py:483] Algo bellman_ford step 6487 current loss 0.043125, current_train_items 207616.
I0304 19:31:09.478205 23118544486528 run.py:483] Algo bellman_ford step 6488 current loss 0.040741, current_train_items 207648.
I0304 19:31:09.512677 23118544486528 run.py:483] Algo bellman_ford step 6489 current loss 0.064705, current_train_items 207680.
I0304 19:31:09.533107 23118544486528 run.py:483] Algo bellman_ford step 6490 current loss 0.004858, current_train_items 207712.
I0304 19:31:09.549491 23118544486528 run.py:483] Algo bellman_ford step 6491 current loss 0.015567, current_train_items 207744.
I0304 19:31:09.572785 23118544486528 run.py:483] Algo bellman_ford step 6492 current loss 0.028276, current_train_items 207776.
I0304 19:31:09.604247 23118544486528 run.py:483] Algo bellman_ford step 6493 current loss 0.038258, current_train_items 207808.
I0304 19:31:09.638197 23118544486528 run.py:483] Algo bellman_ford step 6494 current loss 0.087128, current_train_items 207840.
I0304 19:31:09.658030 23118544486528 run.py:483] Algo bellman_ford step 6495 current loss 0.004152, current_train_items 207872.
I0304 19:31:09.674024 23118544486528 run.py:483] Algo bellman_ford step 6496 current loss 0.017997, current_train_items 207904.
I0304 19:31:09.698719 23118544486528 run.py:483] Algo bellman_ford step 6497 current loss 0.030854, current_train_items 207936.
I0304 19:31:09.729560 23118544486528 run.py:483] Algo bellman_ford step 6498 current loss 0.034633, current_train_items 207968.
I0304 19:31:09.765308 23118544486528 run.py:483] Algo bellman_ford step 6499 current loss 0.082507, current_train_items 208000.
I0304 19:31:09.785524 23118544486528 run.py:483] Algo bellman_ford step 6500 current loss 0.004693, current_train_items 208032.
I0304 19:31:09.793231 23118544486528 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0304 19:31:09.793336 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:09.810573 23118544486528 run.py:483] Algo bellman_ford step 6501 current loss 0.020679, current_train_items 208064.
I0304 19:31:09.835891 23118544486528 run.py:483] Algo bellman_ford step 6502 current loss 0.042640, current_train_items 208096.
I0304 19:31:09.867520 23118544486528 run.py:483] Algo bellman_ford step 6503 current loss 0.036189, current_train_items 208128.
I0304 19:31:09.902796 23118544486528 run.py:483] Algo bellman_ford step 6504 current loss 0.073301, current_train_items 208160.
I0304 19:31:09.923097 23118544486528 run.py:483] Algo bellman_ford step 6505 current loss 0.005435, current_train_items 208192.
I0304 19:31:09.938704 23118544486528 run.py:483] Algo bellman_ford step 6506 current loss 0.006206, current_train_items 208224.
I0304 19:31:09.963464 23118544486528 run.py:483] Algo bellman_ford step 6507 current loss 0.032632, current_train_items 208256.
I0304 19:31:09.996336 23118544486528 run.py:483] Algo bellman_ford step 6508 current loss 0.080086, current_train_items 208288.
I0304 19:31:10.028399 23118544486528 run.py:483] Algo bellman_ford step 6509 current loss 0.068044, current_train_items 208320.
I0304 19:31:10.048087 23118544486528 run.py:483] Algo bellman_ford step 6510 current loss 0.004663, current_train_items 208352.
I0304 19:31:10.064504 23118544486528 run.py:483] Algo bellman_ford step 6511 current loss 0.049840, current_train_items 208384.
I0304 19:31:10.089844 23118544486528 run.py:483] Algo bellman_ford step 6512 current loss 0.040918, current_train_items 208416.
I0304 19:31:10.121292 23118544486528 run.py:483] Algo bellman_ford step 6513 current loss 0.040931, current_train_items 208448.
I0304 19:31:10.156699 23118544486528 run.py:483] Algo bellman_ford step 6514 current loss 0.064207, current_train_items 208480.
I0304 19:31:10.176356 23118544486528 run.py:483] Algo bellman_ford step 6515 current loss 0.005027, current_train_items 208512.
I0304 19:31:10.192772 23118544486528 run.py:483] Algo bellman_ford step 6516 current loss 0.024544, current_train_items 208544.
I0304 19:31:10.217214 23118544486528 run.py:483] Algo bellman_ford step 6517 current loss 0.032158, current_train_items 208576.
I0304 19:31:10.248722 23118544486528 run.py:483] Algo bellman_ford step 6518 current loss 0.068501, current_train_items 208608.
I0304 19:31:10.282019 23118544486528 run.py:483] Algo bellman_ford step 6519 current loss 0.055611, current_train_items 208640.
I0304 19:31:10.301696 23118544486528 run.py:483] Algo bellman_ford step 6520 current loss 0.004037, current_train_items 208672.
I0304 19:31:10.318083 23118544486528 run.py:483] Algo bellman_ford step 6521 current loss 0.009559, current_train_items 208704.
I0304 19:31:10.342877 23118544486528 run.py:483] Algo bellman_ford step 6522 current loss 0.055384, current_train_items 208736.
I0304 19:31:10.373934 23118544486528 run.py:483] Algo bellman_ford step 6523 current loss 0.055262, current_train_items 208768.
I0304 19:31:10.405980 23118544486528 run.py:483] Algo bellman_ford step 6524 current loss 0.066047, current_train_items 208800.
I0304 19:31:10.426063 23118544486528 run.py:483] Algo bellman_ford step 6525 current loss 0.004002, current_train_items 208832.
I0304 19:31:10.442178 23118544486528 run.py:483] Algo bellman_ford step 6526 current loss 0.014027, current_train_items 208864.
I0304 19:31:10.466211 23118544486528 run.py:483] Algo bellman_ford step 6527 current loss 0.040356, current_train_items 208896.
I0304 19:31:10.496882 23118544486528 run.py:483] Algo bellman_ford step 6528 current loss 0.060858, current_train_items 208928.
I0304 19:31:10.531358 23118544486528 run.py:483] Algo bellman_ford step 6529 current loss 0.065195, current_train_items 208960.
I0304 19:31:10.550945 23118544486528 run.py:483] Algo bellman_ford step 6530 current loss 0.004339, current_train_items 208992.
I0304 19:31:10.567552 23118544486528 run.py:483] Algo bellman_ford step 6531 current loss 0.025422, current_train_items 209024.
I0304 19:31:10.593335 23118544486528 run.py:483] Algo bellman_ford step 6532 current loss 0.053169, current_train_items 209056.
I0304 19:31:10.623809 23118544486528 run.py:483] Algo bellman_ford step 6533 current loss 0.046065, current_train_items 209088.
I0304 19:31:10.658041 23118544486528 run.py:483] Algo bellman_ford step 6534 current loss 0.076559, current_train_items 209120.
I0304 19:31:10.678186 23118544486528 run.py:483] Algo bellman_ford step 6535 current loss 0.003660, current_train_items 209152.
I0304 19:31:10.695058 23118544486528 run.py:483] Algo bellman_ford step 6536 current loss 0.042685, current_train_items 209184.
I0304 19:31:10.718953 23118544486528 run.py:483] Algo bellman_ford step 6537 current loss 0.042077, current_train_items 209216.
I0304 19:31:10.749924 23118544486528 run.py:483] Algo bellman_ford step 6538 current loss 0.048525, current_train_items 209248.
I0304 19:31:10.784333 23118544486528 run.py:483] Algo bellman_ford step 6539 current loss 0.040958, current_train_items 209280.
I0304 19:31:10.804169 23118544486528 run.py:483] Algo bellman_ford step 6540 current loss 0.004079, current_train_items 209312.
I0304 19:31:10.820608 23118544486528 run.py:483] Algo bellman_ford step 6541 current loss 0.023976, current_train_items 209344.
I0304 19:31:10.844539 23118544486528 run.py:483] Algo bellman_ford step 6542 current loss 0.044835, current_train_items 209376.
I0304 19:31:10.876751 23118544486528 run.py:483] Algo bellman_ford step 6543 current loss 0.042218, current_train_items 209408.
I0304 19:31:10.912815 23118544486528 run.py:483] Algo bellman_ford step 6544 current loss 0.133375, current_train_items 209440.
I0304 19:31:10.932944 23118544486528 run.py:483] Algo bellman_ford step 6545 current loss 0.004070, current_train_items 209472.
I0304 19:31:10.949297 23118544486528 run.py:483] Algo bellman_ford step 6546 current loss 0.024509, current_train_items 209504.
I0304 19:31:10.973106 23118544486528 run.py:483] Algo bellman_ford step 6547 current loss 0.016825, current_train_items 209536.
I0304 19:31:11.005715 23118544486528 run.py:483] Algo bellman_ford step 6548 current loss 0.079887, current_train_items 209568.
I0304 19:31:11.040868 23118544486528 run.py:483] Algo bellman_ford step 6549 current loss 0.096046, current_train_items 209600.
I0304 19:31:11.060799 23118544486528 run.py:483] Algo bellman_ford step 6550 current loss 0.009335, current_train_items 209632.
I0304 19:31:11.069121 23118544486528 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0304 19:31:11.069227 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:11.086807 23118544486528 run.py:483] Algo bellman_ford step 6551 current loss 0.011458, current_train_items 209664.
I0304 19:31:11.111979 23118544486528 run.py:483] Algo bellman_ford step 6552 current loss 0.033963, current_train_items 209696.
I0304 19:31:11.144980 23118544486528 run.py:483] Algo bellman_ford step 6553 current loss 0.040664, current_train_items 209728.
I0304 19:31:11.180893 23118544486528 run.py:483] Algo bellman_ford step 6554 current loss 0.112238, current_train_items 209760.
I0304 19:31:11.200822 23118544486528 run.py:483] Algo bellman_ford step 6555 current loss 0.007910, current_train_items 209792.
I0304 19:31:11.216845 23118544486528 run.py:483] Algo bellman_ford step 6556 current loss 0.016589, current_train_items 209824.
I0304 19:31:11.241466 23118544486528 run.py:483] Algo bellman_ford step 6557 current loss 0.034890, current_train_items 209856.
I0304 19:31:11.273913 23118544486528 run.py:483] Algo bellman_ford step 6558 current loss 0.089248, current_train_items 209888.
I0304 19:31:11.308558 23118544486528 run.py:483] Algo bellman_ford step 6559 current loss 0.150468, current_train_items 209920.
I0304 19:31:11.328944 23118544486528 run.py:483] Algo bellman_ford step 6560 current loss 0.007921, current_train_items 209952.
I0304 19:31:11.344989 23118544486528 run.py:483] Algo bellman_ford step 6561 current loss 0.014862, current_train_items 209984.
I0304 19:31:11.368377 23118544486528 run.py:483] Algo bellman_ford step 6562 current loss 0.058206, current_train_items 210016.
I0304 19:31:11.399433 23118544486528 run.py:483] Algo bellman_ford step 6563 current loss 0.051159, current_train_items 210048.
I0304 19:31:11.431429 23118544486528 run.py:483] Algo bellman_ford step 6564 current loss 0.035121, current_train_items 210080.
I0304 19:31:11.451754 23118544486528 run.py:483] Algo bellman_ford step 6565 current loss 0.006496, current_train_items 210112.
I0304 19:31:11.467843 23118544486528 run.py:483] Algo bellman_ford step 6566 current loss 0.007832, current_train_items 210144.
I0304 19:31:11.492091 23118544486528 run.py:483] Algo bellman_ford step 6567 current loss 0.061735, current_train_items 210176.
I0304 19:31:11.521943 23118544486528 run.py:483] Algo bellman_ford step 6568 current loss 0.064736, current_train_items 210208.
I0304 19:31:11.556205 23118544486528 run.py:483] Algo bellman_ford step 6569 current loss 0.097874, current_train_items 210240.
I0304 19:31:11.576297 23118544486528 run.py:483] Algo bellman_ford step 6570 current loss 0.004595, current_train_items 210272.
I0304 19:31:11.592769 23118544486528 run.py:483] Algo bellman_ford step 6571 current loss 0.032999, current_train_items 210304.
I0304 19:31:11.616453 23118544486528 run.py:483] Algo bellman_ford step 6572 current loss 0.078629, current_train_items 210336.
I0304 19:31:11.647970 23118544486528 run.py:483] Algo bellman_ford step 6573 current loss 0.044989, current_train_items 210368.
I0304 19:31:11.681840 23118544486528 run.py:483] Algo bellman_ford step 6574 current loss 0.091505, current_train_items 210400.
I0304 19:31:11.702654 23118544486528 run.py:483] Algo bellman_ford step 6575 current loss 0.012363, current_train_items 210432.
I0304 19:31:11.719214 23118544486528 run.py:483] Algo bellman_ford step 6576 current loss 0.027683, current_train_items 210464.
I0304 19:31:11.742433 23118544486528 run.py:483] Algo bellman_ford step 6577 current loss 0.024959, current_train_items 210496.
I0304 19:31:11.773746 23118544486528 run.py:483] Algo bellman_ford step 6578 current loss 0.056298, current_train_items 210528.
I0304 19:31:11.808134 23118544486528 run.py:483] Algo bellman_ford step 6579 current loss 0.072616, current_train_items 210560.
I0304 19:31:11.828085 23118544486528 run.py:483] Algo bellman_ford step 6580 current loss 0.018160, current_train_items 210592.
I0304 19:31:11.844540 23118544486528 run.py:483] Algo bellman_ford step 6581 current loss 0.008969, current_train_items 210624.
I0304 19:31:11.868787 23118544486528 run.py:483] Algo bellman_ford step 6582 current loss 0.052671, current_train_items 210656.
I0304 19:31:11.901524 23118544486528 run.py:483] Algo bellman_ford step 6583 current loss 0.055688, current_train_items 210688.
I0304 19:31:11.935005 23118544486528 run.py:483] Algo bellman_ford step 6584 current loss 0.051842, current_train_items 210720.
I0304 19:31:11.955061 23118544486528 run.py:483] Algo bellman_ford step 6585 current loss 0.037509, current_train_items 210752.
I0304 19:31:11.971080 23118544486528 run.py:483] Algo bellman_ford step 6586 current loss 0.017819, current_train_items 210784.
I0304 19:31:11.993750 23118544486528 run.py:483] Algo bellman_ford step 6587 current loss 0.029556, current_train_items 210816.
I0304 19:31:12.024302 23118544486528 run.py:483] Algo bellman_ford step 6588 current loss 0.048466, current_train_items 210848.
I0304 19:31:12.056829 23118544486528 run.py:483] Algo bellman_ford step 6589 current loss 0.111923, current_train_items 210880.
I0304 19:31:12.077042 23118544486528 run.py:483] Algo bellman_ford step 6590 current loss 0.024310, current_train_items 210912.
I0304 19:31:12.093234 23118544486528 run.py:483] Algo bellman_ford step 6591 current loss 0.008665, current_train_items 210944.
I0304 19:31:12.118037 23118544486528 run.py:483] Algo bellman_ford step 6592 current loss 0.040149, current_train_items 210976.
I0304 19:31:12.149251 23118544486528 run.py:483] Algo bellman_ford step 6593 current loss 0.111583, current_train_items 211008.
I0304 19:31:12.182404 23118544486528 run.py:483] Algo bellman_ford step 6594 current loss 0.171348, current_train_items 211040.
I0304 19:31:12.202188 23118544486528 run.py:483] Algo bellman_ford step 6595 current loss 0.006051, current_train_items 211072.
I0304 19:31:12.218374 23118544486528 run.py:483] Algo bellman_ford step 6596 current loss 0.015003, current_train_items 211104.
I0304 19:31:12.243505 23118544486528 run.py:483] Algo bellman_ford step 6597 current loss 0.048726, current_train_items 211136.
I0304 19:31:12.275660 23118544486528 run.py:483] Algo bellman_ford step 6598 current loss 0.038003, current_train_items 211168.
I0304 19:31:12.306245 23118544486528 run.py:483] Algo bellman_ford step 6599 current loss 0.074681, current_train_items 211200.
I0304 19:31:12.326297 23118544486528 run.py:483] Algo bellman_ford step 6600 current loss 0.006653, current_train_items 211232.
I0304 19:31:12.334031 23118544486528 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0304 19:31:12.334140 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0304 19:31:12.351044 23118544486528 run.py:483] Algo bellman_ford step 6601 current loss 0.017066, current_train_items 211264.
I0304 19:31:12.374366 23118544486528 run.py:483] Algo bellman_ford step 6602 current loss 0.023727, current_train_items 211296.
I0304 19:31:12.406171 23118544486528 run.py:483] Algo bellman_ford step 6603 current loss 0.062292, current_train_items 211328.
I0304 19:31:12.440919 23118544486528 run.py:483] Algo bellman_ford step 6604 current loss 0.085475, current_train_items 211360.
I0304 19:31:12.460681 23118544486528 run.py:483] Algo bellman_ford step 6605 current loss 0.004786, current_train_items 211392.
I0304 19:31:12.476578 23118544486528 run.py:483] Algo bellman_ford step 6606 current loss 0.038374, current_train_items 211424.
I0304 19:31:12.500775 23118544486528 run.py:483] Algo bellman_ford step 6607 current loss 0.050334, current_train_items 211456.
I0304 19:31:12.532798 23118544486528 run.py:483] Algo bellman_ford step 6608 current loss 0.044545, current_train_items 211488.
I0304 19:31:12.565124 23118544486528 run.py:483] Algo bellman_ford step 6609 current loss 0.031218, current_train_items 211520.
I0304 19:31:12.585131 23118544486528 run.py:483] Algo bellman_ford step 6610 current loss 0.003379, current_train_items 211552.
I0304 19:31:12.601665 23118544486528 run.py:483] Algo bellman_ford step 6611 current loss 0.042323, current_train_items 211584.
I0304 19:31:12.624861 23118544486528 run.py:483] Algo bellman_ford step 6612 current loss 0.061406, current_train_items 211616.
I0304 19:31:12.656594 23118544486528 run.py:483] Algo bellman_ford step 6613 current loss 0.077382, current_train_items 211648.
I0304 19:31:12.693024 23118544486528 run.py:483] Algo bellman_ford step 6614 current loss 0.068584, current_train_items 211680.
I0304 19:31:12.712957 23118544486528 run.py:483] Algo bellman_ford step 6615 current loss 0.004546, current_train_items 211712.
I0304 19:31:12.729122 23118544486528 run.py:483] Algo bellman_ford step 6616 current loss 0.006983, current_train_items 211744.
I0304 19:31:12.753299 23118544486528 run.py:483] Algo bellman_ford step 6617 current loss 0.108055, current_train_items 211776.
I0304 19:31:12.784484 23118544486528 run.py:483] Algo bellman_ford step 6618 current loss 0.187994, current_train_items 211808.
I0304 19:31:12.819015 23118544486528 run.py:483] Algo bellman_ford step 6619 current loss 0.145740, current_train_items 211840.
I0304 19:31:12.839035 23118544486528 run.py:483] Algo bellman_ford step 6620 current loss 0.052381, current_train_items 211872.
I0304 19:31:12.855205 23118544486528 run.py:483] Algo bellman_ford step 6621 current loss 0.029318, current_train_items 211904.
I0304 19:31:12.878273 23118544486528 run.py:483] Algo bellman_ford step 6622 current loss 0.043531, current_train_items 211936.
I0304 19:31:12.910116 23118544486528 run.py:483] Algo bellman_ford step 6623 current loss 0.076161, current_train_items 211968.
I0304 19:31:12.942746 23118544486528 run.py:483] Algo bellman_ford step 6624 current loss 0.096732, current_train_items 212000.
I0304 19:31:12.962428 23118544486528 run.py:483] Algo bellman_ford step 6625 current loss 0.006794, current_train_items 212032.
I0304 19:31:12.979420 23118544486528 run.py:483] Algo bellman_ford step 6626 current loss 0.029947, current_train_items 212064.
I0304 19:31:13.003712 23118544486528 run.py:483] Algo bellman_ford step 6627 current loss 0.090400, current_train_items 212096.
I0304 19:31:13.034133 23118544486528 run.py:483] Algo bellman_ford step 6628 current loss 0.114210, current_train_items 212128.
I0304 19:31:13.066419 23118544486528 run.py:483] Algo bellman_ford step 6629 current loss 0.065370, current_train_items 212160.
I0304 19:31:13.086221 23118544486528 run.py:483] Algo bellman_ford step 6630 current loss 0.010161, current_train_items 212192.
I0304 19:31:13.102672 23118544486528 run.py:483] Algo bellman_ford step 6631 current loss 0.048172, current_train_items 212224.
I0304 19:31:13.127746 23118544486528 run.py:483] Algo bellman_ford step 6632 current loss 0.131533, current_train_items 212256.
I0304 19:31:13.160366 23118544486528 run.py:483] Algo bellman_ford step 6633 current loss 0.141781, current_train_items 212288.
I0304 19:31:13.193176 23118544486528 run.py:483] Algo bellman_ford step 6634 current loss 0.102396, current_train_items 212320.
I0304 19:31:13.213075 23118544486528 run.py:483] Algo bellman_ford step 6635 current loss 0.006441, current_train_items 212352.
I0304 19:31:13.229553 23118544486528 run.py:483] Algo bellman_ford step 6636 current loss 0.041726, current_train_items 212384.
I0304 19:31:13.253774 23118544486528 run.py:483] Algo bellman_ford step 6637 current loss 0.046480, current_train_items 212416.
I0304 19:31:13.285352 23118544486528 run.py:483] Algo bellman_ford step 6638 current loss 0.133062, current_train_items 212448.
I0304 19:31:13.319782 23118544486528 run.py:483] Algo bellman_ford step 6639 current loss 0.077090, current_train_items 212480.
I0304 19:31:13.339451 23118544486528 run.py:483] Algo bellman_ford step 6640 current loss 0.002823, current_train_items 212512.
I0304 19:31:13.356051 23118544486528 run.py:483] Algo bellman_ford step 6641 current loss 0.012244, current_train_items 212544.
I0304 19:31:13.381318 23118544486528 run.py:483] Algo bellman_ford step 6642 current loss 0.066605, current_train_items 212576.
I0304 19:31:13.412164 23118544486528 run.py:483] Algo bellman_ford step 6643 current loss 0.083362, current_train_items 212608.
I0304 19:31:13.447164 23118544486528 run.py:483] Algo bellman_ford step 6644 current loss 0.057593, current_train_items 212640.
I0304 19:31:13.466707 23118544486528 run.py:483] Algo bellman_ford step 6645 current loss 0.003597, current_train_items 212672.
I0304 19:31:13.483272 23118544486528 run.py:483] Algo bellman_ford step 6646 current loss 0.007324, current_train_items 212704.
I0304 19:31:13.507317 23118544486528 run.py:483] Algo bellman_ford step 6647 current loss 0.043350, current_train_items 212736.
I0304 19:31:13.539295 23118544486528 run.py:483] Algo bellman_ford step 6648 current loss 0.110366, current_train_items 212768.
I0304 19:31:13.574123 23118544486528 run.py:483] Algo bellman_ford step 6649 current loss 0.075392, current_train_items 212800.
I0304 19:31:13.593963 23118544486528 run.py:483] Algo bellman_ford step 6650 current loss 0.004858, current_train_items 212832.
I0304 19:31:13.601864 23118544486528 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0304 19:31:13.601970 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:31:13.619433 23118544486528 run.py:483] Algo bellman_ford step 6651 current loss 0.033475, current_train_items 212864.
I0304 19:31:13.644729 23118544486528 run.py:483] Algo bellman_ford step 6652 current loss 0.095533, current_train_items 212896.
I0304 19:31:13.677476 23118544486528 run.py:483] Algo bellman_ford step 6653 current loss 0.056169, current_train_items 212928.
I0304 19:31:13.707627 23118544486528 run.py:483] Algo bellman_ford step 6654 current loss 0.054661, current_train_items 212960.
I0304 19:31:13.727843 23118544486528 run.py:483] Algo bellman_ford step 6655 current loss 0.003407, current_train_items 212992.
I0304 19:31:13.743852 23118544486528 run.py:483] Algo bellman_ford step 6656 current loss 0.025737, current_train_items 213024.
I0304 19:31:13.768043 23118544486528 run.py:483] Algo bellman_ford step 6657 current loss 0.046550, current_train_items 213056.
I0304 19:31:13.799831 23118544486528 run.py:483] Algo bellman_ford step 6658 current loss 0.052107, current_train_items 213088.
I0304 19:31:13.835326 23118544486528 run.py:483] Algo bellman_ford step 6659 current loss 0.063374, current_train_items 213120.
I0304 19:31:13.855808 23118544486528 run.py:483] Algo bellman_ford step 6660 current loss 0.005543, current_train_items 213152.
I0304 19:31:13.872730 23118544486528 run.py:483] Algo bellman_ford step 6661 current loss 0.034504, current_train_items 213184.
I0304 19:31:13.897979 23118544486528 run.py:483] Algo bellman_ford step 6662 current loss 0.080699, current_train_items 213216.
I0304 19:31:13.928703 23118544486528 run.py:483] Algo bellman_ford step 6663 current loss 0.051239, current_train_items 213248.
I0304 19:31:13.962556 23118544486528 run.py:483] Algo bellman_ford step 6664 current loss 0.039858, current_train_items 213280.
I0304 19:31:13.982618 23118544486528 run.py:483] Algo bellman_ford step 6665 current loss 0.003998, current_train_items 213312.
I0304 19:31:13.999044 23118544486528 run.py:483] Algo bellman_ford step 6666 current loss 0.011140, current_train_items 213344.
I0304 19:31:14.025190 23118544486528 run.py:483] Algo bellman_ford step 6667 current loss 0.104684, current_train_items 213376.
I0304 19:31:14.058056 23118544486528 run.py:483] Algo bellman_ford step 6668 current loss 0.071399, current_train_items 213408.
I0304 19:31:14.093741 23118544486528 run.py:483] Algo bellman_ford step 6669 current loss 0.092737, current_train_items 213440.
I0304 19:31:14.113953 23118544486528 run.py:483] Algo bellman_ford step 6670 current loss 0.015957, current_train_items 213472.
I0304 19:31:14.130357 23118544486528 run.py:483] Algo bellman_ford step 6671 current loss 0.020353, current_train_items 213504.
I0304 19:31:14.154977 23118544486528 run.py:483] Algo bellman_ford step 6672 current loss 0.037489, current_train_items 213536.
I0304 19:31:14.186985 23118544486528 run.py:483] Algo bellman_ford step 6673 current loss 0.076488, current_train_items 213568.
I0304 19:31:14.219611 23118544486528 run.py:483] Algo bellman_ford step 6674 current loss 0.076876, current_train_items 213600.
I0304 19:31:14.239649 23118544486528 run.py:483] Algo bellman_ford step 6675 current loss 0.010373, current_train_items 213632.
I0304 19:31:14.255619 23118544486528 run.py:483] Algo bellman_ford step 6676 current loss 0.010874, current_train_items 213664.
I0304 19:31:14.280485 23118544486528 run.py:483] Algo bellman_ford step 6677 current loss 0.037023, current_train_items 213696.
I0304 19:31:14.313442 23118544486528 run.py:483] Algo bellman_ford step 6678 current loss 0.073538, current_train_items 213728.
I0304 19:31:14.349359 23118544486528 run.py:483] Algo bellman_ford step 6679 current loss 0.076045, current_train_items 213760.
I0304 19:31:14.369149 23118544486528 run.py:483] Algo bellman_ford step 6680 current loss 0.003793, current_train_items 213792.
I0304 19:31:14.385571 23118544486528 run.py:483] Algo bellman_ford step 6681 current loss 0.013864, current_train_items 213824.
I0304 19:31:14.411093 23118544486528 run.py:483] Algo bellman_ford step 6682 current loss 0.038570, current_train_items 213856.
I0304 19:31:14.442611 23118544486528 run.py:483] Algo bellman_ford step 6683 current loss 0.086477, current_train_items 213888.
I0304 19:31:14.479628 23118544486528 run.py:483] Algo bellman_ford step 6684 current loss 0.077083, current_train_items 213920.
I0304 19:31:14.499914 23118544486528 run.py:483] Algo bellman_ford step 6685 current loss 0.005965, current_train_items 213952.
I0304 19:31:14.516171 23118544486528 run.py:483] Algo bellman_ford step 6686 current loss 0.085640, current_train_items 213984.
I0304 19:31:14.540268 23118544486528 run.py:483] Algo bellman_ford step 6687 current loss 0.048002, current_train_items 214016.
I0304 19:31:14.572535 23118544486528 run.py:483] Algo bellman_ford step 6688 current loss 0.034975, current_train_items 214048.
I0304 19:31:14.606385 23118544486528 run.py:483] Algo bellman_ford step 6689 current loss 0.042989, current_train_items 214080.
I0304 19:31:14.626915 23118544486528 run.py:483] Algo bellman_ford step 6690 current loss 0.004255, current_train_items 214112.
I0304 19:31:14.643033 23118544486528 run.py:483] Algo bellman_ford step 6691 current loss 0.005082, current_train_items 214144.
I0304 19:31:14.667611 23118544486528 run.py:483] Algo bellman_ford step 6692 current loss 0.042986, current_train_items 214176.
I0304 19:31:14.699644 23118544486528 run.py:483] Algo bellman_ford step 6693 current loss 0.060126, current_train_items 214208.
I0304 19:31:14.734945 23118544486528 run.py:483] Algo bellman_ford step 6694 current loss 0.072821, current_train_items 214240.
I0304 19:31:14.754899 23118544486528 run.py:483] Algo bellman_ford step 6695 current loss 0.003016, current_train_items 214272.
I0304 19:31:14.771469 23118544486528 run.py:483] Algo bellman_ford step 6696 current loss 0.016988, current_train_items 214304.
I0304 19:31:14.795888 23118544486528 run.py:483] Algo bellman_ford step 6697 current loss 0.063206, current_train_items 214336.
I0304 19:31:14.828124 23118544486528 run.py:483] Algo bellman_ford step 6698 current loss 0.075247, current_train_items 214368.
I0304 19:31:14.863466 23118544486528 run.py:483] Algo bellman_ford step 6699 current loss 0.099938, current_train_items 214400.
I0304 19:31:14.883347 23118544486528 run.py:483] Algo bellman_ford step 6700 current loss 0.013914, current_train_items 214432.
I0304 19:31:14.890948 23118544486528 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0304 19:31:14.891057 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:14.908506 23118544486528 run.py:483] Algo bellman_ford step 6701 current loss 0.041986, current_train_items 214464.
I0304 19:31:14.933313 23118544486528 run.py:483] Algo bellman_ford step 6702 current loss 0.069869, current_train_items 214496.
I0304 19:31:14.965469 23118544486528 run.py:483] Algo bellman_ford step 6703 current loss 0.101621, current_train_items 214528.
I0304 19:31:14.999397 23118544486528 run.py:483] Algo bellman_ford step 6704 current loss 0.073529, current_train_items 214560.
I0304 19:31:15.019653 23118544486528 run.py:483] Algo bellman_ford step 6705 current loss 0.008878, current_train_items 214592.
I0304 19:31:15.035930 23118544486528 run.py:483] Algo bellman_ford step 6706 current loss 0.017428, current_train_items 214624.
I0304 19:31:15.061625 23118544486528 run.py:483] Algo bellman_ford step 6707 current loss 0.078742, current_train_items 214656.
I0304 19:31:15.093883 23118544486528 run.py:483] Algo bellman_ford step 6708 current loss 0.054767, current_train_items 214688.
I0304 19:31:15.129183 23118544486528 run.py:483] Algo bellman_ford step 6709 current loss 0.092975, current_train_items 214720.
I0304 19:31:15.149151 23118544486528 run.py:483] Algo bellman_ford step 6710 current loss 0.004695, current_train_items 214752.
I0304 19:31:15.165714 23118544486528 run.py:483] Algo bellman_ford step 6711 current loss 0.025396, current_train_items 214784.
I0304 19:31:15.189213 23118544486528 run.py:483] Algo bellman_ford step 6712 current loss 0.080254, current_train_items 214816.
I0304 19:31:15.222470 23118544486528 run.py:483] Algo bellman_ford step 6713 current loss 0.078126, current_train_items 214848.
I0304 19:31:15.256341 23118544486528 run.py:483] Algo bellman_ford step 6714 current loss 0.058380, current_train_items 214880.
I0304 19:31:15.276141 23118544486528 run.py:483] Algo bellman_ford step 6715 current loss 0.013268, current_train_items 214912.
I0304 19:31:15.292560 23118544486528 run.py:483] Algo bellman_ford step 6716 current loss 0.010039, current_train_items 214944.
I0304 19:31:15.317101 23118544486528 run.py:483] Algo bellman_ford step 6717 current loss 0.043628, current_train_items 214976.
I0304 19:31:15.349155 23118544486528 run.py:483] Algo bellman_ford step 6718 current loss 0.056951, current_train_items 215008.
I0304 19:31:15.385194 23118544486528 run.py:483] Algo bellman_ford step 6719 current loss 0.113545, current_train_items 215040.
I0304 19:31:15.405001 23118544486528 run.py:483] Algo bellman_ford step 6720 current loss 0.006240, current_train_items 215072.
I0304 19:31:15.421300 23118544486528 run.py:483] Algo bellman_ford step 6721 current loss 0.025816, current_train_items 215104.
I0304 19:31:15.444616 23118544486528 run.py:483] Algo bellman_ford step 6722 current loss 0.049060, current_train_items 215136.
I0304 19:31:15.477190 23118544486528 run.py:483] Algo bellman_ford step 6723 current loss 0.056993, current_train_items 215168.
I0304 19:31:15.510194 23118544486528 run.py:483] Algo bellman_ford step 6724 current loss 0.079857, current_train_items 215200.
I0304 19:31:15.530160 23118544486528 run.py:483] Algo bellman_ford step 6725 current loss 0.005328, current_train_items 215232.
I0304 19:31:15.546658 23118544486528 run.py:483] Algo bellman_ford step 6726 current loss 0.042854, current_train_items 215264.
I0304 19:31:15.570198 23118544486528 run.py:483] Algo bellman_ford step 6727 current loss 0.023027, current_train_items 215296.
I0304 19:31:15.601869 23118544486528 run.py:483] Algo bellman_ford step 6728 current loss 0.036779, current_train_items 215328.
I0304 19:31:15.634573 23118544486528 run.py:483] Algo bellman_ford step 6729 current loss 0.056408, current_train_items 215360.
I0304 19:31:15.654329 23118544486528 run.py:483] Algo bellman_ford step 6730 current loss 0.009073, current_train_items 215392.
I0304 19:31:15.670548 23118544486528 run.py:483] Algo bellman_ford step 6731 current loss 0.008657, current_train_items 215424.
I0304 19:31:15.695244 23118544486528 run.py:483] Algo bellman_ford step 6732 current loss 0.071166, current_train_items 215456.
I0304 19:31:15.725500 23118544486528 run.py:483] Algo bellman_ford step 6733 current loss 0.049896, current_train_items 215488.
I0304 19:31:15.757872 23118544486528 run.py:483] Algo bellman_ford step 6734 current loss 0.063269, current_train_items 215520.
I0304 19:31:15.778065 23118544486528 run.py:483] Algo bellman_ford step 6735 current loss 0.008825, current_train_items 215552.
I0304 19:31:15.794630 23118544486528 run.py:483] Algo bellman_ford step 6736 current loss 0.033240, current_train_items 215584.
I0304 19:31:15.819713 23118544486528 run.py:483] Algo bellman_ford step 6737 current loss 0.051021, current_train_items 215616.
I0304 19:31:15.850393 23118544486528 run.py:483] Algo bellman_ford step 6738 current loss 0.041680, current_train_items 215648.
I0304 19:31:15.886106 23118544486528 run.py:483] Algo bellman_ford step 6739 current loss 0.098657, current_train_items 215680.
I0304 19:31:15.905740 23118544486528 run.py:483] Algo bellman_ford step 6740 current loss 0.015464, current_train_items 215712.
I0304 19:31:15.922282 23118544486528 run.py:483] Algo bellman_ford step 6741 current loss 0.017068, current_train_items 215744.
I0304 19:31:15.947021 23118544486528 run.py:483] Algo bellman_ford step 6742 current loss 0.067787, current_train_items 215776.
I0304 19:31:15.977737 23118544486528 run.py:483] Algo bellman_ford step 6743 current loss 0.035438, current_train_items 215808.
I0304 19:31:16.012654 23118544486528 run.py:483] Algo bellman_ford step 6744 current loss 0.059804, current_train_items 215840.
I0304 19:31:16.032931 23118544486528 run.py:483] Algo bellman_ford step 6745 current loss 0.006314, current_train_items 215872.
I0304 19:31:16.049437 23118544486528 run.py:483] Algo bellman_ford step 6746 current loss 0.037836, current_train_items 215904.
I0304 19:31:16.073767 23118544486528 run.py:483] Algo bellman_ford step 6747 current loss 0.022519, current_train_items 215936.
I0304 19:31:16.104782 23118544486528 run.py:483] Algo bellman_ford step 6748 current loss 0.048467, current_train_items 215968.
I0304 19:31:16.139474 23118544486528 run.py:483] Algo bellman_ford step 6749 current loss 0.074679, current_train_items 216000.
I0304 19:31:16.159371 23118544486528 run.py:483] Algo bellman_ford step 6750 current loss 0.005683, current_train_items 216032.
I0304 19:31:16.167393 23118544486528 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0304 19:31:16.167498 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:16.184463 23118544486528 run.py:483] Algo bellman_ford step 6751 current loss 0.015683, current_train_items 216064.
I0304 19:31:16.209325 23118544486528 run.py:483] Algo bellman_ford step 6752 current loss 0.052833, current_train_items 216096.
I0304 19:31:16.242356 23118544486528 run.py:483] Algo bellman_ford step 6753 current loss 0.044600, current_train_items 216128.
I0304 19:31:16.277484 23118544486528 run.py:483] Algo bellman_ford step 6754 current loss 0.065640, current_train_items 216160.
I0304 19:31:16.297768 23118544486528 run.py:483] Algo bellman_ford step 6755 current loss 0.016225, current_train_items 216192.
I0304 19:31:16.314352 23118544486528 run.py:483] Algo bellman_ford step 6756 current loss 0.029804, current_train_items 216224.
I0304 19:31:16.338126 23118544486528 run.py:483] Algo bellman_ford step 6757 current loss 0.020614, current_train_items 216256.
I0304 19:31:16.370149 23118544486528 run.py:483] Algo bellman_ford step 6758 current loss 0.080097, current_train_items 216288.
I0304 19:31:16.405625 23118544486528 run.py:483] Algo bellman_ford step 6759 current loss 0.102213, current_train_items 216320.
I0304 19:31:16.425758 23118544486528 run.py:483] Algo bellman_ford step 6760 current loss 0.008420, current_train_items 216352.
I0304 19:31:16.442875 23118544486528 run.py:483] Algo bellman_ford step 6761 current loss 0.009415, current_train_items 216384.
I0304 19:31:16.466867 23118544486528 run.py:483] Algo bellman_ford step 6762 current loss 0.022107, current_train_items 216416.
I0304 19:31:16.498886 23118544486528 run.py:483] Algo bellman_ford step 6763 current loss 0.049364, current_train_items 216448.
I0304 19:31:16.532131 23118544486528 run.py:483] Algo bellman_ford step 6764 current loss 0.063531, current_train_items 216480.
I0304 19:31:16.551954 23118544486528 run.py:483] Algo bellman_ford step 6765 current loss 0.004278, current_train_items 216512.
I0304 19:31:16.568606 23118544486528 run.py:483] Algo bellman_ford step 6766 current loss 0.040547, current_train_items 216544.
I0304 19:31:16.592560 23118544486528 run.py:483] Algo bellman_ford step 6767 current loss 0.028463, current_train_items 216576.
I0304 19:31:16.623278 23118544486528 run.py:483] Algo bellman_ford step 6768 current loss 0.043501, current_train_items 216608.
I0304 19:31:16.656947 23118544486528 run.py:483] Algo bellman_ford step 6769 current loss 0.114023, current_train_items 216640.
I0304 19:31:16.676862 23118544486528 run.py:483] Algo bellman_ford step 6770 current loss 0.002449, current_train_items 216672.
I0304 19:31:16.693152 23118544486528 run.py:483] Algo bellman_ford step 6771 current loss 0.015426, current_train_items 216704.
I0304 19:31:16.715934 23118544486528 run.py:483] Algo bellman_ford step 6772 current loss 0.024650, current_train_items 216736.
I0304 19:31:16.748273 23118544486528 run.py:483] Algo bellman_ford step 6773 current loss 0.081915, current_train_items 216768.
I0304 19:31:16.780782 23118544486528 run.py:483] Algo bellman_ford step 6774 current loss 0.057749, current_train_items 216800.
I0304 19:31:16.800620 23118544486528 run.py:483] Algo bellman_ford step 6775 current loss 0.005567, current_train_items 216832.
I0304 19:31:16.817324 23118544486528 run.py:483] Algo bellman_ford step 6776 current loss 0.029971, current_train_items 216864.
I0304 19:31:16.840544 23118544486528 run.py:483] Algo bellman_ford step 6777 current loss 0.027874, current_train_items 216896.
I0304 19:31:16.873613 23118544486528 run.py:483] Algo bellman_ford step 6778 current loss 0.123668, current_train_items 216928.
I0304 19:31:16.906972 23118544486528 run.py:483] Algo bellman_ford step 6779 current loss 0.035218, current_train_items 216960.
I0304 19:31:16.926969 23118544486528 run.py:483] Algo bellman_ford step 6780 current loss 0.004560, current_train_items 216992.
I0304 19:31:16.943269 23118544486528 run.py:483] Algo bellman_ford step 6781 current loss 0.009411, current_train_items 217024.
I0304 19:31:16.966514 23118544486528 run.py:483] Algo bellman_ford step 6782 current loss 0.047903, current_train_items 217056.
I0304 19:31:16.999152 23118544486528 run.py:483] Algo bellman_ford step 6783 current loss 0.043894, current_train_items 217088.
I0304 19:31:17.032415 23118544486528 run.py:483] Algo bellman_ford step 6784 current loss 0.089726, current_train_items 217120.
I0304 19:31:17.052460 23118544486528 run.py:483] Algo bellman_ford step 6785 current loss 0.005080, current_train_items 217152.
I0304 19:31:17.068626 23118544486528 run.py:483] Algo bellman_ford step 6786 current loss 0.017281, current_train_items 217184.
I0304 19:31:17.092149 23118544486528 run.py:483] Algo bellman_ford step 6787 current loss 0.041510, current_train_items 217216.
I0304 19:31:17.122467 23118544486528 run.py:483] Algo bellman_ford step 6788 current loss 0.031453, current_train_items 217248.
I0304 19:31:17.156943 23118544486528 run.py:483] Algo bellman_ford step 6789 current loss 0.091525, current_train_items 217280.
I0304 19:31:17.176953 23118544486528 run.py:483] Algo bellman_ford step 6790 current loss 0.008885, current_train_items 217312.
I0304 19:31:17.193276 23118544486528 run.py:483] Algo bellman_ford step 6791 current loss 0.011004, current_train_items 217344.
I0304 19:31:17.217360 23118544486528 run.py:483] Algo bellman_ford step 6792 current loss 0.020084, current_train_items 217376.
I0304 19:31:17.248241 23118544486528 run.py:483] Algo bellman_ford step 6793 current loss 0.058078, current_train_items 217408.
I0304 19:31:17.281976 23118544486528 run.py:483] Algo bellman_ford step 6794 current loss 0.099498, current_train_items 217440.
I0304 19:31:17.301658 23118544486528 run.py:483] Algo bellman_ford step 6795 current loss 0.011221, current_train_items 217472.
I0304 19:31:17.318003 23118544486528 run.py:483] Algo bellman_ford step 6796 current loss 0.015363, current_train_items 217504.
I0304 19:31:17.342821 23118544486528 run.py:483] Algo bellman_ford step 6797 current loss 0.026570, current_train_items 217536.
I0304 19:31:17.373521 23118544486528 run.py:483] Algo bellman_ford step 6798 current loss 0.047391, current_train_items 217568.
I0304 19:31:17.407453 23118544486528 run.py:483] Algo bellman_ford step 6799 current loss 0.055444, current_train_items 217600.
I0304 19:31:17.427429 23118544486528 run.py:483] Algo bellman_ford step 6800 current loss 0.013126, current_train_items 217632.
I0304 19:31:17.435410 23118544486528 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0304 19:31:17.435518 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:17.452557 23118544486528 run.py:483] Algo bellman_ford step 6801 current loss 0.021141, current_train_items 217664.
I0304 19:31:17.477500 23118544486528 run.py:483] Algo bellman_ford step 6802 current loss 0.042387, current_train_items 217696.
I0304 19:31:17.510649 23118544486528 run.py:483] Algo bellman_ford step 6803 current loss 0.062436, current_train_items 217728.
I0304 19:31:17.545604 23118544486528 run.py:483] Algo bellman_ford step 6804 current loss 0.070125, current_train_items 217760.
I0304 19:31:17.565710 23118544486528 run.py:483] Algo bellman_ford step 6805 current loss 0.003872, current_train_items 217792.
I0304 19:31:17.582054 23118544486528 run.py:483] Algo bellman_ford step 6806 current loss 0.008656, current_train_items 217824.
I0304 19:31:17.606628 23118544486528 run.py:483] Algo bellman_ford step 6807 current loss 0.023327, current_train_items 217856.
I0304 19:31:17.637147 23118544486528 run.py:483] Algo bellman_ford step 6808 current loss 0.025215, current_train_items 217888.
I0304 19:31:17.671392 23118544486528 run.py:483] Algo bellman_ford step 6809 current loss 0.057248, current_train_items 217920.
I0304 19:31:17.691561 23118544486528 run.py:483] Algo bellman_ford step 6810 current loss 0.010858, current_train_items 217952.
I0304 19:31:17.707588 23118544486528 run.py:483] Algo bellman_ford step 6811 current loss 0.025554, current_train_items 217984.
I0304 19:31:17.732732 23118544486528 run.py:483] Algo bellman_ford step 6812 current loss 0.060838, current_train_items 218016.
I0304 19:31:17.764110 23118544486528 run.py:483] Algo bellman_ford step 6813 current loss 0.072565, current_train_items 218048.
I0304 19:31:17.796201 23118544486528 run.py:483] Algo bellman_ford step 6814 current loss 0.092178, current_train_items 218080.
I0304 19:31:17.815960 23118544486528 run.py:483] Algo bellman_ford step 6815 current loss 0.009101, current_train_items 218112.
I0304 19:31:17.832309 23118544486528 run.py:483] Algo bellman_ford step 6816 current loss 0.015438, current_train_items 218144.
I0304 19:31:17.855363 23118544486528 run.py:483] Algo bellman_ford step 6817 current loss 0.050185, current_train_items 218176.
I0304 19:31:17.887541 23118544486528 run.py:483] Algo bellman_ford step 6818 current loss 0.075155, current_train_items 218208.
I0304 19:31:17.923223 23118544486528 run.py:483] Algo bellman_ford step 6819 current loss 0.106379, current_train_items 218240.
I0304 19:31:17.943135 23118544486528 run.py:483] Algo bellman_ford step 6820 current loss 0.026258, current_train_items 218272.
I0304 19:31:17.959373 23118544486528 run.py:483] Algo bellman_ford step 6821 current loss 0.013911, current_train_items 218304.
I0304 19:31:17.983165 23118544486528 run.py:483] Algo bellman_ford step 6822 current loss 0.062287, current_train_items 218336.
I0304 19:31:18.015081 23118544486528 run.py:483] Algo bellman_ford step 6823 current loss 0.073714, current_train_items 218368.
I0304 19:31:18.050538 23118544486528 run.py:483] Algo bellman_ford step 6824 current loss 0.116060, current_train_items 218400.
I0304 19:31:18.070436 23118544486528 run.py:483] Algo bellman_ford step 6825 current loss 0.008137, current_train_items 218432.
I0304 19:31:18.087030 23118544486528 run.py:483] Algo bellman_ford step 6826 current loss 0.042121, current_train_items 218464.
I0304 19:31:18.111257 23118544486528 run.py:483] Algo bellman_ford step 6827 current loss 0.026335, current_train_items 218496.
I0304 19:31:18.142438 23118544486528 run.py:483] Algo bellman_ford step 6828 current loss 0.054189, current_train_items 218528.
I0304 19:31:18.178809 23118544486528 run.py:483] Algo bellman_ford step 6829 current loss 0.086395, current_train_items 218560.
I0304 19:31:18.198632 23118544486528 run.py:483] Algo bellman_ford step 6830 current loss 0.007105, current_train_items 218592.
I0304 19:31:18.214836 23118544486528 run.py:483] Algo bellman_ford step 6831 current loss 0.013543, current_train_items 218624.
I0304 19:31:18.240991 23118544486528 run.py:483] Algo bellman_ford step 6832 current loss 0.073273, current_train_items 218656.
I0304 19:31:18.272240 23118544486528 run.py:483] Algo bellman_ford step 6833 current loss 0.067391, current_train_items 218688.
I0304 19:31:18.306484 23118544486528 run.py:483] Algo bellman_ford step 6834 current loss 0.068889, current_train_items 218720.
I0304 19:31:18.326548 23118544486528 run.py:483] Algo bellman_ford step 6835 current loss 0.007022, current_train_items 218752.
I0304 19:31:18.343129 23118544486528 run.py:483] Algo bellman_ford step 6836 current loss 0.030185, current_train_items 218784.
I0304 19:31:18.367337 23118544486528 run.py:483] Algo bellman_ford step 6837 current loss 0.052478, current_train_items 218816.
I0304 19:31:18.399894 23118544486528 run.py:483] Algo bellman_ford step 6838 current loss 0.108884, current_train_items 218848.
I0304 19:31:18.432028 23118544486528 run.py:483] Algo bellman_ford step 6839 current loss 0.149121, current_train_items 218880.
I0304 19:31:18.451858 23118544486528 run.py:483] Algo bellman_ford step 6840 current loss 0.003238, current_train_items 218912.
I0304 19:31:18.468948 23118544486528 run.py:483] Algo bellman_ford step 6841 current loss 0.042574, current_train_items 218944.
I0304 19:31:18.493882 23118544486528 run.py:483] Algo bellman_ford step 6842 current loss 0.039597, current_train_items 218976.
I0304 19:31:18.524690 23118544486528 run.py:483] Algo bellman_ford step 6843 current loss 0.033692, current_train_items 219008.
I0304 19:31:18.558363 23118544486528 run.py:483] Algo bellman_ford step 6844 current loss 0.077192, current_train_items 219040.
I0304 19:31:18.577957 23118544486528 run.py:483] Algo bellman_ford step 6845 current loss 0.004485, current_train_items 219072.
I0304 19:31:18.594870 23118544486528 run.py:483] Algo bellman_ford step 6846 current loss 0.022366, current_train_items 219104.
I0304 19:31:18.620335 23118544486528 run.py:483] Algo bellman_ford step 6847 current loss 0.049209, current_train_items 219136.
I0304 19:31:18.652431 23118544486528 run.py:483] Algo bellman_ford step 6848 current loss 0.061621, current_train_items 219168.
I0304 19:31:18.687517 23118544486528 run.py:483] Algo bellman_ford step 6849 current loss 0.073656, current_train_items 219200.
I0304 19:31:18.707570 23118544486528 run.py:483] Algo bellman_ford step 6850 current loss 0.006177, current_train_items 219232.
I0304 19:31:18.716133 23118544486528 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0304 19:31:18.716240 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:18.733411 23118544486528 run.py:483] Algo bellman_ford step 6851 current loss 0.030626, current_train_items 219264.
I0304 19:31:18.758440 23118544486528 run.py:483] Algo bellman_ford step 6852 current loss 0.028197, current_train_items 219296.
I0304 19:31:18.790426 23118544486528 run.py:483] Algo bellman_ford step 6853 current loss 0.045613, current_train_items 219328.
I0304 19:31:18.825374 23118544486528 run.py:483] Algo bellman_ford step 6854 current loss 0.074419, current_train_items 219360.
I0304 19:31:18.845316 23118544486528 run.py:483] Algo bellman_ford step 6855 current loss 0.003993, current_train_items 219392.
I0304 19:31:18.861266 23118544486528 run.py:483] Algo bellman_ford step 6856 current loss 0.007165, current_train_items 219424.
I0304 19:31:18.885856 23118544486528 run.py:483] Algo bellman_ford step 6857 current loss 0.039979, current_train_items 219456.
I0304 19:31:18.917060 23118544486528 run.py:483] Algo bellman_ford step 6858 current loss 0.049249, current_train_items 219488.
I0304 19:31:18.951148 23118544486528 run.py:483] Algo bellman_ford step 6859 current loss 0.063912, current_train_items 219520.
I0304 19:31:18.971332 23118544486528 run.py:483] Algo bellman_ford step 6860 current loss 0.009126, current_train_items 219552.
I0304 19:31:18.987724 23118544486528 run.py:483] Algo bellman_ford step 6861 current loss 0.025920, current_train_items 219584.
I0304 19:31:19.011173 23118544486528 run.py:483] Algo bellman_ford step 6862 current loss 0.036561, current_train_items 219616.
I0304 19:31:19.042128 23118544486528 run.py:483] Algo bellman_ford step 6863 current loss 0.098777, current_train_items 219648.
I0304 19:31:19.074662 23118544486528 run.py:483] Algo bellman_ford step 6864 current loss 0.076133, current_train_items 219680.
I0304 19:31:19.094797 23118544486528 run.py:483] Algo bellman_ford step 6865 current loss 0.012204, current_train_items 219712.
I0304 19:31:19.111245 23118544486528 run.py:483] Algo bellman_ford step 6866 current loss 0.009344, current_train_items 219744.
I0304 19:31:19.136445 23118544486528 run.py:483] Algo bellman_ford step 6867 current loss 0.118109, current_train_items 219776.
I0304 19:31:19.168584 23118544486528 run.py:483] Algo bellman_ford step 6868 current loss 0.116514, current_train_items 219808.
I0304 19:31:19.204968 23118544486528 run.py:483] Algo bellman_ford step 6869 current loss 0.182334, current_train_items 219840.
I0304 19:31:19.225182 23118544486528 run.py:483] Algo bellman_ford step 6870 current loss 0.035585, current_train_items 219872.
I0304 19:31:19.241906 23118544486528 run.py:483] Algo bellman_ford step 6871 current loss 0.022049, current_train_items 219904.
I0304 19:31:19.265760 23118544486528 run.py:483] Algo bellman_ford step 6872 current loss 0.029741, current_train_items 219936.
I0304 19:31:19.296471 23118544486528 run.py:483] Algo bellman_ford step 6873 current loss 0.077995, current_train_items 219968.
I0304 19:31:19.330524 23118544486528 run.py:483] Algo bellman_ford step 6874 current loss 0.112334, current_train_items 220000.
I0304 19:31:19.351038 23118544486528 run.py:483] Algo bellman_ford step 6875 current loss 0.029093, current_train_items 220032.
I0304 19:31:19.367149 23118544486528 run.py:483] Algo bellman_ford step 6876 current loss 0.016272, current_train_items 220064.
I0304 19:31:19.391019 23118544486528 run.py:483] Algo bellman_ford step 6877 current loss 0.046557, current_train_items 220096.
I0304 19:31:19.422626 23118544486528 run.py:483] Algo bellman_ford step 6878 current loss 0.033589, current_train_items 220128.
I0304 19:31:19.457544 23118544486528 run.py:483] Algo bellman_ford step 6879 current loss 0.090410, current_train_items 220160.
I0304 19:31:19.477330 23118544486528 run.py:483] Algo bellman_ford step 6880 current loss 0.025393, current_train_items 220192.
I0304 19:31:19.493429 23118544486528 run.py:483] Algo bellman_ford step 6881 current loss 0.019175, current_train_items 220224.
I0304 19:31:19.518245 23118544486528 run.py:483] Algo bellman_ford step 6882 current loss 0.022750, current_train_items 220256.
I0304 19:31:19.548709 23118544486528 run.py:483] Algo bellman_ford step 6883 current loss 0.037475, current_train_items 220288.
I0304 19:31:19.582180 23118544486528 run.py:483] Algo bellman_ford step 6884 current loss 0.119659, current_train_items 220320.
I0304 19:31:19.602625 23118544486528 run.py:483] Algo bellman_ford step 6885 current loss 0.005304, current_train_items 220352.
I0304 19:31:19.619244 23118544486528 run.py:483] Algo bellman_ford step 6886 current loss 0.034856, current_train_items 220384.
I0304 19:31:19.643523 23118544486528 run.py:483] Algo bellman_ford step 6887 current loss 0.051266, current_train_items 220416.
I0304 19:31:19.676374 23118544486528 run.py:483] Algo bellman_ford step 6888 current loss 0.080950, current_train_items 220448.
I0304 19:31:19.710623 23118544486528 run.py:483] Algo bellman_ford step 6889 current loss 0.038336, current_train_items 220480.
I0304 19:31:19.730546 23118544486528 run.py:483] Algo bellman_ford step 6890 current loss 0.119380, current_train_items 220512.
I0304 19:31:19.747429 23118544486528 run.py:483] Algo bellman_ford step 6891 current loss 0.042955, current_train_items 220544.
I0304 19:31:19.771478 23118544486528 run.py:483] Algo bellman_ford step 6892 current loss 0.071570, current_train_items 220576.
I0304 19:31:19.804092 23118544486528 run.py:483] Algo bellman_ford step 6893 current loss 0.040654, current_train_items 220608.
I0304 19:31:19.837437 23118544486528 run.py:483] Algo bellman_ford step 6894 current loss 0.066005, current_train_items 220640.
I0304 19:31:19.857741 23118544486528 run.py:483] Algo bellman_ford step 6895 current loss 0.004644, current_train_items 220672.
I0304 19:31:19.874058 23118544486528 run.py:483] Algo bellman_ford step 6896 current loss 0.012177, current_train_items 220704.
I0304 19:31:19.898919 23118544486528 run.py:483] Algo bellman_ford step 6897 current loss 0.039878, current_train_items 220736.
I0304 19:31:19.929558 23118544486528 run.py:483] Algo bellman_ford step 6898 current loss 0.035966, current_train_items 220768.
I0304 19:31:19.964348 23118544486528 run.py:483] Algo bellman_ford step 6899 current loss 0.068737, current_train_items 220800.
I0304 19:31:19.984784 23118544486528 run.py:483] Algo bellman_ford step 6900 current loss 0.014394, current_train_items 220832.
I0304 19:31:19.992472 23118544486528 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0304 19:31:19.992577 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:20.009277 23118544486528 run.py:483] Algo bellman_ford step 6901 current loss 0.005599, current_train_items 220864.
I0304 19:31:20.033654 23118544486528 run.py:483] Algo bellman_ford step 6902 current loss 0.042065, current_train_items 220896.
I0304 19:31:20.066953 23118544486528 run.py:483] Algo bellman_ford step 6903 current loss 0.080074, current_train_items 220928.
I0304 19:31:20.103003 23118544486528 run.py:483] Algo bellman_ford step 6904 current loss 0.064486, current_train_items 220960.
I0304 19:31:20.123056 23118544486528 run.py:483] Algo bellman_ford step 6905 current loss 0.005917, current_train_items 220992.
I0304 19:31:20.138893 23118544486528 run.py:483] Algo bellman_ford step 6906 current loss 0.008632, current_train_items 221024.
I0304 19:31:20.163633 23118544486528 run.py:483] Algo bellman_ford step 6907 current loss 0.041997, current_train_items 221056.
I0304 19:31:20.196254 23118544486528 run.py:483] Algo bellman_ford step 6908 current loss 0.124509, current_train_items 221088.
I0304 19:31:20.231770 23118544486528 run.py:483] Algo bellman_ford step 6909 current loss 0.094397, current_train_items 221120.
I0304 19:31:20.251517 23118544486528 run.py:483] Algo bellman_ford step 6910 current loss 0.020051, current_train_items 221152.
I0304 19:31:20.268603 23118544486528 run.py:483] Algo bellman_ford step 6911 current loss 0.034352, current_train_items 221184.
I0304 19:31:20.291574 23118544486528 run.py:483] Algo bellman_ford step 6912 current loss 0.045527, current_train_items 221216.
I0304 19:31:20.324651 23118544486528 run.py:483] Algo bellman_ford step 6913 current loss 0.046443, current_train_items 221248.
I0304 19:31:20.360560 23118544486528 run.py:483] Algo bellman_ford step 6914 current loss 0.068385, current_train_items 221280.
I0304 19:31:20.380152 23118544486528 run.py:483] Algo bellman_ford step 6915 current loss 0.010417, current_train_items 221312.
I0304 19:31:20.396149 23118544486528 run.py:483] Algo bellman_ford step 6916 current loss 0.005494, current_train_items 221344.
I0304 19:31:20.421528 23118544486528 run.py:483] Algo bellman_ford step 6917 current loss 0.088678, current_train_items 221376.
I0304 19:31:20.454088 23118544486528 run.py:483] Algo bellman_ford step 6918 current loss 0.054850, current_train_items 221408.
I0304 19:31:20.490002 23118544486528 run.py:483] Algo bellman_ford step 6919 current loss 0.063369, current_train_items 221440.
I0304 19:31:20.510051 23118544486528 run.py:483] Algo bellman_ford step 6920 current loss 0.009386, current_train_items 221472.
I0304 19:31:20.526423 23118544486528 run.py:483] Algo bellman_ford step 6921 current loss 0.009537, current_train_items 221504.
I0304 19:31:20.550263 23118544486528 run.py:483] Algo bellman_ford step 6922 current loss 0.043264, current_train_items 221536.
I0304 19:31:20.582278 23118544486528 run.py:483] Algo bellman_ford step 6923 current loss 0.045579, current_train_items 221568.
I0304 19:31:20.616544 23118544486528 run.py:483] Algo bellman_ford step 6924 current loss 0.032686, current_train_items 221600.
I0304 19:31:20.636676 23118544486528 run.py:483] Algo bellman_ford step 6925 current loss 0.004210, current_train_items 221632.
I0304 19:31:20.652928 23118544486528 run.py:483] Algo bellman_ford step 6926 current loss 0.025499, current_train_items 221664.
I0304 19:31:20.677465 23118544486528 run.py:483] Algo bellman_ford step 6927 current loss 0.051647, current_train_items 221696.
I0304 19:31:20.709598 23118544486528 run.py:483] Algo bellman_ford step 6928 current loss 0.053889, current_train_items 221728.
I0304 19:31:20.743302 23118544486528 run.py:483] Algo bellman_ford step 6929 current loss 0.048215, current_train_items 221760.
I0304 19:31:20.763350 23118544486528 run.py:483] Algo bellman_ford step 6930 current loss 0.003901, current_train_items 221792.
I0304 19:31:20.779992 23118544486528 run.py:483] Algo bellman_ford step 6931 current loss 0.026170, current_train_items 221824.
I0304 19:31:20.803642 23118544486528 run.py:483] Algo bellman_ford step 6932 current loss 0.044772, current_train_items 221856.
I0304 19:31:20.837140 23118544486528 run.py:483] Algo bellman_ford step 6933 current loss 0.033507, current_train_items 221888.
I0304 19:31:20.872412 23118544486528 run.py:483] Algo bellman_ford step 6934 current loss 0.118613, current_train_items 221920.
I0304 19:31:20.892484 23118544486528 run.py:483] Algo bellman_ford step 6935 current loss 0.004869, current_train_items 221952.
I0304 19:31:20.908753 23118544486528 run.py:483] Algo bellman_ford step 6936 current loss 0.010530, current_train_items 221984.
I0304 19:31:20.932817 23118544486528 run.py:483] Algo bellman_ford step 6937 current loss 0.034113, current_train_items 222016.
I0304 19:31:20.964760 23118544486528 run.py:483] Algo bellman_ford step 6938 current loss 0.030612, current_train_items 222048.
I0304 19:31:20.998106 23118544486528 run.py:483] Algo bellman_ford step 6939 current loss 0.052533, current_train_items 222080.
I0304 19:31:21.017930 23118544486528 run.py:483] Algo bellman_ford step 6940 current loss 0.012633, current_train_items 222112.
I0304 19:31:21.034725 23118544486528 run.py:483] Algo bellman_ford step 6941 current loss 0.026483, current_train_items 222144.
I0304 19:31:21.059848 23118544486528 run.py:483] Algo bellman_ford step 6942 current loss 0.065832, current_train_items 222176.
I0304 19:31:21.091928 23118544486528 run.py:483] Algo bellman_ford step 6943 current loss 0.081874, current_train_items 222208.
I0304 19:31:21.127145 23118544486528 run.py:483] Algo bellman_ford step 6944 current loss 0.093063, current_train_items 222240.
I0304 19:31:21.146825 23118544486528 run.py:483] Algo bellman_ford step 6945 current loss 0.008145, current_train_items 222272.
I0304 19:31:21.163384 23118544486528 run.py:483] Algo bellman_ford step 6946 current loss 0.015786, current_train_items 222304.
I0304 19:31:21.188276 23118544486528 run.py:483] Algo bellman_ford step 6947 current loss 0.109008, current_train_items 222336.
I0304 19:31:21.218078 23118544486528 run.py:483] Algo bellman_ford step 6948 current loss 0.101925, current_train_items 222368.
I0304 19:31:21.253035 23118544486528 run.py:483] Algo bellman_ford step 6949 current loss 0.118205, current_train_items 222400.
I0304 19:31:21.273124 23118544486528 run.py:483] Algo bellman_ford step 6950 current loss 0.009859, current_train_items 222432.
I0304 19:31:21.281003 23118544486528 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0304 19:31:21.281109 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:21.298249 23118544486528 run.py:483] Algo bellman_ford step 6951 current loss 0.028531, current_train_items 222464.
I0304 19:31:21.323529 23118544486528 run.py:483] Algo bellman_ford step 6952 current loss 0.069479, current_train_items 222496.
I0304 19:31:21.354996 23118544486528 run.py:483] Algo bellman_ford step 6953 current loss 0.071117, current_train_items 222528.
I0304 19:31:21.390551 23118544486528 run.py:483] Algo bellman_ford step 6954 current loss 0.137219, current_train_items 222560.
I0304 19:31:21.410815 23118544486528 run.py:483] Algo bellman_ford step 6955 current loss 0.004700, current_train_items 222592.
I0304 19:31:21.426997 23118544486528 run.py:483] Algo bellman_ford step 6956 current loss 0.013584, current_train_items 222624.
I0304 19:31:21.452821 23118544486528 run.py:483] Algo bellman_ford step 6957 current loss 0.097022, current_train_items 222656.
I0304 19:31:21.484370 23118544486528 run.py:483] Algo bellman_ford step 6958 current loss 0.053056, current_train_items 222688.
I0304 19:31:21.516832 23118544486528 run.py:483] Algo bellman_ford step 6959 current loss 0.049498, current_train_items 222720.
I0304 19:31:21.536796 23118544486528 run.py:483] Algo bellman_ford step 6960 current loss 0.004025, current_train_items 222752.
I0304 19:31:21.553624 23118544486528 run.py:483] Algo bellman_ford step 6961 current loss 0.028250, current_train_items 222784.
I0304 19:31:21.577599 23118544486528 run.py:483] Algo bellman_ford step 6962 current loss 0.032675, current_train_items 222816.
I0304 19:31:21.609634 23118544486528 run.py:483] Algo bellman_ford step 6963 current loss 0.072593, current_train_items 222848.
I0304 19:31:21.645184 23118544486528 run.py:483] Algo bellman_ford step 6964 current loss 0.067668, current_train_items 222880.
I0304 19:31:21.665275 23118544486528 run.py:483] Algo bellman_ford step 6965 current loss 0.006018, current_train_items 222912.
I0304 19:31:21.682430 23118544486528 run.py:483] Algo bellman_ford step 6966 current loss 0.034305, current_train_items 222944.
I0304 19:31:21.706746 23118544486528 run.py:483] Algo bellman_ford step 6967 current loss 0.014103, current_train_items 222976.
I0304 19:31:21.737453 23118544486528 run.py:483] Algo bellman_ford step 6968 current loss 0.039966, current_train_items 223008.
I0304 19:31:21.771533 23118544486528 run.py:483] Algo bellman_ford step 6969 current loss 0.050468, current_train_items 223040.
I0304 19:31:21.791811 23118544486528 run.py:483] Algo bellman_ford step 6970 current loss 0.003964, current_train_items 223072.
I0304 19:31:21.808317 23118544486528 run.py:483] Algo bellman_ford step 6971 current loss 0.019872, current_train_items 223104.
I0304 19:31:21.832340 23118544486528 run.py:483] Algo bellman_ford step 6972 current loss 0.028457, current_train_items 223136.
I0304 19:31:21.863713 23118544486528 run.py:483] Algo bellman_ford step 6973 current loss 0.040828, current_train_items 223168.
I0304 19:31:21.898349 23118544486528 run.py:483] Algo bellman_ford step 6974 current loss 0.044970, current_train_items 223200.
I0304 19:31:21.918616 23118544486528 run.py:483] Algo bellman_ford step 6975 current loss 0.003564, current_train_items 223232.
I0304 19:31:21.935488 23118544486528 run.py:483] Algo bellman_ford step 6976 current loss 0.014597, current_train_items 223264.
I0304 19:31:21.959259 23118544486528 run.py:483] Algo bellman_ford step 6977 current loss 0.030073, current_train_items 223296.
I0304 19:31:21.990077 23118544486528 run.py:483] Algo bellman_ford step 6978 current loss 0.033968, current_train_items 223328.
I0304 19:31:22.025072 23118544486528 run.py:483] Algo bellman_ford step 6979 current loss 0.045918, current_train_items 223360.
I0304 19:31:22.045008 23118544486528 run.py:483] Algo bellman_ford step 6980 current loss 0.019543, current_train_items 223392.
I0304 19:31:22.062077 23118544486528 run.py:483] Algo bellman_ford step 6981 current loss 0.008656, current_train_items 223424.
I0304 19:31:22.085717 23118544486528 run.py:483] Algo bellman_ford step 6982 current loss 0.039059, current_train_items 223456.
I0304 19:31:22.118275 23118544486528 run.py:483] Algo bellman_ford step 6983 current loss 0.069025, current_train_items 223488.
I0304 19:31:22.152439 23118544486528 run.py:483] Algo bellman_ford step 6984 current loss 0.069584, current_train_items 223520.
I0304 19:31:22.172660 23118544486528 run.py:483] Algo bellman_ford step 6985 current loss 0.004279, current_train_items 223552.
I0304 19:31:22.189262 23118544486528 run.py:483] Algo bellman_ford step 6986 current loss 0.012319, current_train_items 223584.
I0304 19:31:22.213221 23118544486528 run.py:483] Algo bellman_ford step 6987 current loss 0.035877, current_train_items 223616.
I0304 19:31:22.244721 23118544486528 run.py:483] Algo bellman_ford step 6988 current loss 0.039773, current_train_items 223648.
I0304 19:31:22.280864 23118544486528 run.py:483] Algo bellman_ford step 6989 current loss 0.067690, current_train_items 223680.
I0304 19:31:22.301058 23118544486528 run.py:483] Algo bellman_ford step 6990 current loss 0.005694, current_train_items 223712.
I0304 19:31:22.317368 23118544486528 run.py:483] Algo bellman_ford step 6991 current loss 0.017131, current_train_items 223744.
I0304 19:31:22.341434 23118544486528 run.py:483] Algo bellman_ford step 6992 current loss 0.041229, current_train_items 223776.
I0304 19:31:22.374078 23118544486528 run.py:483] Algo bellman_ford step 6993 current loss 0.089121, current_train_items 223808.
I0304 19:31:22.410732 23118544486528 run.py:483] Algo bellman_ford step 6994 current loss 0.085347, current_train_items 223840.
I0304 19:31:22.430674 23118544486528 run.py:483] Algo bellman_ford step 6995 current loss 0.004164, current_train_items 223872.
I0304 19:31:22.447386 23118544486528 run.py:483] Algo bellman_ford step 6996 current loss 0.010607, current_train_items 223904.
I0304 19:31:22.470169 23118544486528 run.py:483] Algo bellman_ford step 6997 current loss 0.087625, current_train_items 223936.
I0304 19:31:22.501332 23118544486528 run.py:483] Algo bellman_ford step 6998 current loss 0.047006, current_train_items 223968.
I0304 19:31:22.535482 23118544486528 run.py:483] Algo bellman_ford step 6999 current loss 0.130553, current_train_items 224000.
I0304 19:31:22.555897 23118544486528 run.py:483] Algo bellman_ford step 7000 current loss 0.004378, current_train_items 224032.
I0304 19:31:22.563469 23118544486528 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0304 19:31:22.563574 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:22.580246 23118544486528 run.py:483] Algo bellman_ford step 7001 current loss 0.032512, current_train_items 224064.
I0304 19:31:22.604212 23118544486528 run.py:483] Algo bellman_ford step 7002 current loss 0.033539, current_train_items 224096.
I0304 19:31:22.634834 23118544486528 run.py:483] Algo bellman_ford step 7003 current loss 0.048577, current_train_items 224128.
I0304 19:31:22.668176 23118544486528 run.py:483] Algo bellman_ford step 7004 current loss 0.066471, current_train_items 224160.
I0304 19:31:22.688144 23118544486528 run.py:483] Algo bellman_ford step 7005 current loss 0.003994, current_train_items 224192.
I0304 19:31:22.704309 23118544486528 run.py:483] Algo bellman_ford step 7006 current loss 0.024935, current_train_items 224224.
I0304 19:31:22.729780 23118544486528 run.py:483] Algo bellman_ford step 7007 current loss 0.060227, current_train_items 224256.
I0304 19:31:22.759913 23118544486528 run.py:483] Algo bellman_ford step 7008 current loss 0.048060, current_train_items 224288.
I0304 19:31:22.796158 23118544486528 run.py:483] Algo bellman_ford step 7009 current loss 0.070461, current_train_items 224320.
I0304 19:31:22.816357 23118544486528 run.py:483] Algo bellman_ford step 7010 current loss 0.005085, current_train_items 224352.
I0304 19:31:22.833101 23118544486528 run.py:483] Algo bellman_ford step 7011 current loss 0.008369, current_train_items 224384.
I0304 19:31:22.856848 23118544486528 run.py:483] Algo bellman_ford step 7012 current loss 0.070307, current_train_items 224416.
I0304 19:31:22.887919 23118544486528 run.py:483] Algo bellman_ford step 7013 current loss 0.049698, current_train_items 224448.
I0304 19:31:22.922224 23118544486528 run.py:483] Algo bellman_ford step 7014 current loss 0.061737, current_train_items 224480.
I0304 19:31:22.941972 23118544486528 run.py:483] Algo bellman_ford step 7015 current loss 0.004150, current_train_items 224512.
I0304 19:31:22.958286 23118544486528 run.py:483] Algo bellman_ford step 7016 current loss 0.017722, current_train_items 224544.
I0304 19:31:22.982478 23118544486528 run.py:483] Algo bellman_ford step 7017 current loss 0.056813, current_train_items 224576.
I0304 19:31:23.013085 23118544486528 run.py:483] Algo bellman_ford step 7018 current loss 0.066799, current_train_items 224608.
I0304 19:31:23.046205 23118544486528 run.py:483] Algo bellman_ford step 7019 current loss 0.061801, current_train_items 224640.
I0304 19:31:23.065751 23118544486528 run.py:483] Algo bellman_ford step 7020 current loss 0.002243, current_train_items 224672.
I0304 19:31:23.081983 23118544486528 run.py:483] Algo bellman_ford step 7021 current loss 0.044760, current_train_items 224704.
I0304 19:31:23.105730 23118544486528 run.py:483] Algo bellman_ford step 7022 current loss 0.043913, current_train_items 224736.
I0304 19:31:23.136888 23118544486528 run.py:483] Algo bellman_ford step 7023 current loss 0.020290, current_train_items 224768.
I0304 19:31:23.171524 23118544486528 run.py:483] Algo bellman_ford step 7024 current loss 0.054133, current_train_items 224800.
I0304 19:31:23.191222 23118544486528 run.py:483] Algo bellman_ford step 7025 current loss 0.003416, current_train_items 224832.
I0304 19:31:23.207952 23118544486528 run.py:483] Algo bellman_ford step 7026 current loss 0.012180, current_train_items 224864.
I0304 19:31:23.233609 23118544486528 run.py:483] Algo bellman_ford step 7027 current loss 0.082403, current_train_items 224896.
I0304 19:31:23.267235 23118544486528 run.py:483] Algo bellman_ford step 7028 current loss 0.081844, current_train_items 224928.
I0304 19:31:23.299446 23118544486528 run.py:483] Algo bellman_ford step 7029 current loss 0.075696, current_train_items 224960.
I0304 19:31:23.319325 23118544486528 run.py:483] Algo bellman_ford step 7030 current loss 0.003504, current_train_items 224992.
I0304 19:31:23.336110 23118544486528 run.py:483] Algo bellman_ford step 7031 current loss 0.012104, current_train_items 225024.
I0304 19:31:23.359787 23118544486528 run.py:483] Algo bellman_ford step 7032 current loss 0.045424, current_train_items 225056.
I0304 19:31:23.390832 23118544486528 run.py:483] Algo bellman_ford step 7033 current loss 0.076493, current_train_items 225088.
I0304 19:31:23.426570 23118544486528 run.py:483] Algo bellman_ford step 7034 current loss 0.078136, current_train_items 225120.
I0304 19:31:23.446370 23118544486528 run.py:483] Algo bellman_ford step 7035 current loss 0.005724, current_train_items 225152.
I0304 19:31:23.462844 23118544486528 run.py:483] Algo bellman_ford step 7036 current loss 0.030003, current_train_items 225184.
I0304 19:31:23.486822 23118544486528 run.py:483] Algo bellman_ford step 7037 current loss 0.059603, current_train_items 225216.
I0304 19:31:23.519176 23118544486528 run.py:483] Algo bellman_ford step 7038 current loss 0.097705, current_train_items 225248.
I0304 19:31:23.552553 23118544486528 run.py:483] Algo bellman_ford step 7039 current loss 0.063798, current_train_items 225280.
I0304 19:31:23.572429 23118544486528 run.py:483] Algo bellman_ford step 7040 current loss 0.035058, current_train_items 225312.
I0304 19:31:23.588853 23118544486528 run.py:483] Algo bellman_ford step 7041 current loss 0.030545, current_train_items 225344.
I0304 19:31:23.612983 23118544486528 run.py:483] Algo bellman_ford step 7042 current loss 0.032218, current_train_items 225376.
I0304 19:31:23.645017 23118544486528 run.py:483] Algo bellman_ford step 7043 current loss 0.061970, current_train_items 225408.
I0304 19:31:23.679364 23118544486528 run.py:483] Algo bellman_ford step 7044 current loss 0.053589, current_train_items 225440.
I0304 19:31:23.699266 23118544486528 run.py:483] Algo bellman_ford step 7045 current loss 0.005458, current_train_items 225472.
I0304 19:31:23.715786 23118544486528 run.py:483] Algo bellman_ford step 7046 current loss 0.020558, current_train_items 225504.
I0304 19:31:23.739453 23118544486528 run.py:483] Algo bellman_ford step 7047 current loss 0.100728, current_train_items 225536.
I0304 19:31:23.771869 23118544486528 run.py:483] Algo bellman_ford step 7048 current loss 0.079954, current_train_items 225568.
I0304 19:31:23.805510 23118544486528 run.py:483] Algo bellman_ford step 7049 current loss 0.073125, current_train_items 225600.
I0304 19:31:23.825625 23118544486528 run.py:483] Algo bellman_ford step 7050 current loss 0.003544, current_train_items 225632.
I0304 19:31:23.833902 23118544486528 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0304 19:31:23.834009 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:23.850783 23118544486528 run.py:483] Algo bellman_ford step 7051 current loss 0.009585, current_train_items 225664.
I0304 19:31:23.875920 23118544486528 run.py:483] Algo bellman_ford step 7052 current loss 0.099432, current_train_items 225696.
I0304 19:31:23.909742 23118544486528 run.py:483] Algo bellman_ford step 7053 current loss 0.081452, current_train_items 225728.
I0304 19:31:23.945450 23118544486528 run.py:483] Algo bellman_ford step 7054 current loss 0.102811, current_train_items 225760.
I0304 19:31:23.965551 23118544486528 run.py:483] Algo bellman_ford step 7055 current loss 0.020504, current_train_items 225792.
I0304 19:31:23.981746 23118544486528 run.py:483] Algo bellman_ford step 7056 current loss 0.020513, current_train_items 225824.
I0304 19:31:24.007048 23118544486528 run.py:483] Algo bellman_ford step 7057 current loss 0.069683, current_train_items 225856.
I0304 19:31:24.039044 23118544486528 run.py:483] Algo bellman_ford step 7058 current loss 0.068307, current_train_items 225888.
I0304 19:31:24.075109 23118544486528 run.py:483] Algo bellman_ford step 7059 current loss 0.107063, current_train_items 225920.
I0304 19:31:24.095061 23118544486528 run.py:483] Algo bellman_ford step 7060 current loss 0.008895, current_train_items 225952.
I0304 19:31:24.111551 23118544486528 run.py:483] Algo bellman_ford step 7061 current loss 0.016980, current_train_items 225984.
I0304 19:31:24.136328 23118544486528 run.py:483] Algo bellman_ford step 7062 current loss 0.053364, current_train_items 226016.
I0304 19:31:24.165674 23118544486528 run.py:483] Algo bellman_ford step 7063 current loss 0.056215, current_train_items 226048.
I0304 19:31:24.199723 23118544486528 run.py:483] Algo bellman_ford step 7064 current loss 0.048180, current_train_items 226080.
I0304 19:31:24.219679 23118544486528 run.py:483] Algo bellman_ford step 7065 current loss 0.004579, current_train_items 226112.
I0304 19:31:24.236538 23118544486528 run.py:483] Algo bellman_ford step 7066 current loss 0.037102, current_train_items 226144.
I0304 19:31:24.260582 23118544486528 run.py:483] Algo bellman_ford step 7067 current loss 0.024927, current_train_items 226176.
I0304 19:31:24.294324 23118544486528 run.py:483] Algo bellman_ford step 7068 current loss 0.060708, current_train_items 226208.
I0304 19:31:24.330234 23118544486528 run.py:483] Algo bellman_ford step 7069 current loss 0.055743, current_train_items 226240.
I0304 19:31:24.350280 23118544486528 run.py:483] Algo bellman_ford step 7070 current loss 0.046265, current_train_items 226272.
I0304 19:31:24.366569 23118544486528 run.py:483] Algo bellman_ford step 7071 current loss 0.024292, current_train_items 226304.
I0304 19:31:24.389697 23118544486528 run.py:483] Algo bellman_ford step 7072 current loss 0.076526, current_train_items 226336.
I0304 19:31:24.420785 23118544486528 run.py:483] Algo bellman_ford step 7073 current loss 0.032620, current_train_items 226368.
I0304 19:31:24.454452 23118544486528 run.py:483] Algo bellman_ford step 7074 current loss 0.081422, current_train_items 226400.
I0304 19:31:24.474760 23118544486528 run.py:483] Algo bellman_ford step 7075 current loss 0.008865, current_train_items 226432.
I0304 19:31:24.490599 23118544486528 run.py:483] Algo bellman_ford step 7076 current loss 0.030870, current_train_items 226464.
I0304 19:31:24.514365 23118544486528 run.py:483] Algo bellman_ford step 7077 current loss 0.041471, current_train_items 226496.
I0304 19:31:24.546499 23118544486528 run.py:483] Algo bellman_ford step 7078 current loss 0.058284, current_train_items 226528.
I0304 19:31:24.581382 23118544486528 run.py:483] Algo bellman_ford step 7079 current loss 0.097064, current_train_items 226560.
I0304 19:31:24.601388 23118544486528 run.py:483] Algo bellman_ford step 7080 current loss 0.006679, current_train_items 226592.
I0304 19:31:24.617760 23118544486528 run.py:483] Algo bellman_ford step 7081 current loss 0.020287, current_train_items 226624.
I0304 19:31:24.641692 23118544486528 run.py:483] Algo bellman_ford step 7082 current loss 0.080438, current_train_items 226656.
I0304 19:31:24.673415 23118544486528 run.py:483] Algo bellman_ford step 7083 current loss 0.079827, current_train_items 226688.
I0304 19:31:24.706725 23118544486528 run.py:483] Algo bellman_ford step 7084 current loss 0.090060, current_train_items 226720.
I0304 19:31:24.727076 23118544486528 run.py:483] Algo bellman_ford step 7085 current loss 0.004172, current_train_items 226752.
I0304 19:31:24.743296 23118544486528 run.py:483] Algo bellman_ford step 7086 current loss 0.018062, current_train_items 226784.
I0304 19:31:24.766582 23118544486528 run.py:483] Algo bellman_ford step 7087 current loss 0.053292, current_train_items 226816.
I0304 19:31:24.798302 23118544486528 run.py:483] Algo bellman_ford step 7088 current loss 0.144235, current_train_items 226848.
I0304 19:31:24.832208 23118544486528 run.py:483] Algo bellman_ford step 7089 current loss 0.057882, current_train_items 226880.
I0304 19:31:24.852676 23118544486528 run.py:483] Algo bellman_ford step 7090 current loss 0.012422, current_train_items 226912.
I0304 19:31:24.869099 23118544486528 run.py:483] Algo bellman_ford step 7091 current loss 0.041375, current_train_items 226944.
I0304 19:31:24.893395 23118544486528 run.py:483] Algo bellman_ford step 7092 current loss 0.051613, current_train_items 226976.
I0304 19:31:24.926076 23118544486528 run.py:483] Algo bellman_ford step 7093 current loss 0.098116, current_train_items 227008.
I0304 19:31:24.958476 23118544486528 run.py:483] Algo bellman_ford step 7094 current loss 0.066057, current_train_items 227040.
I0304 19:31:24.978799 23118544486528 run.py:483] Algo bellman_ford step 7095 current loss 0.016461, current_train_items 227072.
I0304 19:31:24.995137 23118544486528 run.py:483] Algo bellman_ford step 7096 current loss 0.025874, current_train_items 227104.
I0304 19:31:25.017995 23118544486528 run.py:483] Algo bellman_ford step 7097 current loss 0.051697, current_train_items 227136.
I0304 19:31:25.050555 23118544486528 run.py:483] Algo bellman_ford step 7098 current loss 0.060752, current_train_items 227168.
I0304 19:31:25.084459 23118544486528 run.py:483] Algo bellman_ford step 7099 current loss 0.069837, current_train_items 227200.
I0304 19:31:25.105117 23118544486528 run.py:483] Algo bellman_ford step 7100 current loss 0.003937, current_train_items 227232.
I0304 19:31:25.112741 23118544486528 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0304 19:31:25.112847 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:25.129846 23118544486528 run.py:483] Algo bellman_ford step 7101 current loss 0.039542, current_train_items 227264.
I0304 19:31:25.154386 23118544486528 run.py:483] Algo bellman_ford step 7102 current loss 0.026495, current_train_items 227296.
I0304 19:31:25.186535 23118544486528 run.py:483] Algo bellman_ford step 7103 current loss 0.055851, current_train_items 227328.
I0304 19:31:25.222571 23118544486528 run.py:483] Algo bellman_ford step 7104 current loss 0.080375, current_train_items 227360.
I0304 19:31:25.242711 23118544486528 run.py:483] Algo bellman_ford step 7105 current loss 0.040881, current_train_items 227392.
I0304 19:31:25.259288 23118544486528 run.py:483] Algo bellman_ford step 7106 current loss 0.035642, current_train_items 227424.
I0304 19:31:25.284033 23118544486528 run.py:483] Algo bellman_ford step 7107 current loss 0.030215, current_train_items 227456.
I0304 19:31:25.315404 23118544486528 run.py:483] Algo bellman_ford step 7108 current loss 0.079666, current_train_items 227488.
I0304 19:31:25.351488 23118544486528 run.py:483] Algo bellman_ford step 7109 current loss 0.072839, current_train_items 227520.
I0304 19:31:25.371374 23118544486528 run.py:483] Algo bellman_ford step 7110 current loss 0.007897, current_train_items 227552.
I0304 19:31:25.388017 23118544486528 run.py:483] Algo bellman_ford step 7111 current loss 0.019674, current_train_items 227584.
I0304 19:31:25.412120 23118544486528 run.py:483] Algo bellman_ford step 7112 current loss 0.043759, current_train_items 227616.
I0304 19:31:25.444526 23118544486528 run.py:483] Algo bellman_ford step 7113 current loss 0.097108, current_train_items 227648.
I0304 19:31:25.478962 23118544486528 run.py:483] Algo bellman_ford step 7114 current loss 0.067888, current_train_items 227680.
I0304 19:31:25.498887 23118544486528 run.py:483] Algo bellman_ford step 7115 current loss 0.003497, current_train_items 227712.
I0304 19:31:25.515364 23118544486528 run.py:483] Algo bellman_ford step 7116 current loss 0.025758, current_train_items 227744.
I0304 19:31:25.539738 23118544486528 run.py:483] Algo bellman_ford step 7117 current loss 0.032564, current_train_items 227776.
I0304 19:31:25.571595 23118544486528 run.py:483] Algo bellman_ford step 7118 current loss 0.075276, current_train_items 227808.
I0304 19:31:25.605880 23118544486528 run.py:483] Algo bellman_ford step 7119 current loss 0.037113, current_train_items 227840.
I0304 19:31:25.625324 23118544486528 run.py:483] Algo bellman_ford step 7120 current loss 0.008920, current_train_items 227872.
I0304 19:31:25.641959 23118544486528 run.py:483] Algo bellman_ford step 7121 current loss 0.040662, current_train_items 227904.
I0304 19:31:25.666003 23118544486528 run.py:483] Algo bellman_ford step 7122 current loss 0.068318, current_train_items 227936.
I0304 19:31:25.697355 23118544486528 run.py:483] Algo bellman_ford step 7123 current loss 0.041084, current_train_items 227968.
I0304 19:31:25.732106 23118544486528 run.py:483] Algo bellman_ford step 7124 current loss 0.081700, current_train_items 228000.
I0304 19:31:25.751580 23118544486528 run.py:483] Algo bellman_ford step 7125 current loss 0.005589, current_train_items 228032.
I0304 19:31:25.767404 23118544486528 run.py:483] Algo bellman_ford step 7126 current loss 0.007980, current_train_items 228064.
I0304 19:31:25.790735 23118544486528 run.py:483] Algo bellman_ford step 7127 current loss 0.049545, current_train_items 228096.
I0304 19:31:25.821804 23118544486528 run.py:483] Algo bellman_ford step 7128 current loss 0.073193, current_train_items 228128.
I0304 19:31:25.857083 23118544486528 run.py:483] Algo bellman_ford step 7129 current loss 0.073778, current_train_items 228160.
I0304 19:31:25.876865 23118544486528 run.py:483] Algo bellman_ford step 7130 current loss 0.016079, current_train_items 228192.
I0304 19:31:25.893785 23118544486528 run.py:483] Algo bellman_ford step 7131 current loss 0.024352, current_train_items 228224.
I0304 19:31:25.918135 23118544486528 run.py:483] Algo bellman_ford step 7132 current loss 0.034841, current_train_items 228256.
I0304 19:31:25.949488 23118544486528 run.py:483] Algo bellman_ford step 7133 current loss 0.057566, current_train_items 228288.
I0304 19:31:25.983047 23118544486528 run.py:483] Algo bellman_ford step 7134 current loss 0.064008, current_train_items 228320.
I0304 19:31:26.002775 23118544486528 run.py:483] Algo bellman_ford step 7135 current loss 0.023815, current_train_items 228352.
I0304 19:31:26.018821 23118544486528 run.py:483] Algo bellman_ford step 7136 current loss 0.020529, current_train_items 228384.
I0304 19:31:26.041980 23118544486528 run.py:483] Algo bellman_ford step 7137 current loss 0.050892, current_train_items 228416.
I0304 19:31:26.071922 23118544486528 run.py:483] Algo bellman_ford step 7138 current loss 0.041808, current_train_items 228448.
I0304 19:31:26.108235 23118544486528 run.py:483] Algo bellman_ford step 7139 current loss 0.072481, current_train_items 228480.
I0304 19:31:26.127862 23118544486528 run.py:483] Algo bellman_ford step 7140 current loss 0.006391, current_train_items 228512.
I0304 19:31:26.144220 23118544486528 run.py:483] Algo bellman_ford step 7141 current loss 0.025395, current_train_items 228544.
I0304 19:31:26.168667 23118544486528 run.py:483] Algo bellman_ford step 7142 current loss 0.045465, current_train_items 228576.
I0304 19:31:26.198295 23118544486528 run.py:483] Algo bellman_ford step 7143 current loss 0.032120, current_train_items 228608.
I0304 19:31:26.231407 23118544486528 run.py:483] Algo bellman_ford step 7144 current loss 0.074254, current_train_items 228640.
I0304 19:31:26.251261 23118544486528 run.py:483] Algo bellman_ford step 7145 current loss 0.009530, current_train_items 228672.
I0304 19:31:26.267932 23118544486528 run.py:483] Algo bellman_ford step 7146 current loss 0.013329, current_train_items 228704.
I0304 19:31:26.292376 23118544486528 run.py:483] Algo bellman_ford step 7147 current loss 0.078692, current_train_items 228736.
I0304 19:31:26.322772 23118544486528 run.py:483] Algo bellman_ford step 7148 current loss 0.036257, current_train_items 228768.
I0304 19:31:26.354297 23118544486528 run.py:483] Algo bellman_ford step 7149 current loss 0.079083, current_train_items 228800.
I0304 19:31:26.373772 23118544486528 run.py:483] Algo bellman_ford step 7150 current loss 0.004960, current_train_items 228832.
I0304 19:31:26.381630 23118544486528 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0304 19:31:26.381743 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:26.399143 23118544486528 run.py:483] Algo bellman_ford step 7151 current loss 0.043836, current_train_items 228864.
I0304 19:31:26.424667 23118544486528 run.py:483] Algo bellman_ford step 7152 current loss 0.035935, current_train_items 228896.
I0304 19:31:26.456872 23118544486528 run.py:483] Algo bellman_ford step 7153 current loss 0.081258, current_train_items 228928.
I0304 19:31:26.490390 23118544486528 run.py:483] Algo bellman_ford step 7154 current loss 0.053553, current_train_items 228960.
I0304 19:31:26.510372 23118544486528 run.py:483] Algo bellman_ford step 7155 current loss 0.012320, current_train_items 228992.
I0304 19:31:26.526746 23118544486528 run.py:483] Algo bellman_ford step 7156 current loss 0.040313, current_train_items 229024.
I0304 19:31:26.550870 23118544486528 run.py:483] Algo bellman_ford step 7157 current loss 0.066803, current_train_items 229056.
I0304 19:31:26.582595 23118544486528 run.py:483] Algo bellman_ford step 7158 current loss 0.029884, current_train_items 229088.
I0304 19:31:26.617064 23118544486528 run.py:483] Algo bellman_ford step 7159 current loss 0.104326, current_train_items 229120.
I0304 19:31:26.636985 23118544486528 run.py:483] Algo bellman_ford step 7160 current loss 0.009695, current_train_items 229152.
I0304 19:31:26.653825 23118544486528 run.py:483] Algo bellman_ford step 7161 current loss 0.047195, current_train_items 229184.
W0304 19:31:26.668604 23118544486528 samplers.py:155] Increasing hint lengh from 10 to 11
I0304 19:31:33.691280 23118544486528 run.py:483] Algo bellman_ford step 7162 current loss 0.044592, current_train_items 229216.
I0304 19:31:33.724432 23118544486528 run.py:483] Algo bellman_ford step 7163 current loss 0.035433, current_train_items 229248.
I0304 19:31:33.759945 23118544486528 run.py:483] Algo bellman_ford step 7164 current loss 0.100795, current_train_items 229280.
I0304 19:31:33.779978 23118544486528 run.py:483] Algo bellman_ford step 7165 current loss 0.005348, current_train_items 229312.
I0304 19:31:33.796322 23118544486528 run.py:483] Algo bellman_ford step 7166 current loss 0.036262, current_train_items 229344.
I0304 19:31:33.820474 23118544486528 run.py:483] Algo bellman_ford step 7167 current loss 0.043289, current_train_items 229376.
I0304 19:31:33.852444 23118544486528 run.py:483] Algo bellman_ford step 7168 current loss 0.043094, current_train_items 229408.
I0304 19:31:33.884400 23118544486528 run.py:483] Algo bellman_ford step 7169 current loss 0.076572, current_train_items 229440.
I0304 19:31:33.904648 23118544486528 run.py:483] Algo bellman_ford step 7170 current loss 0.006918, current_train_items 229472.
I0304 19:31:33.921468 23118544486528 run.py:483] Algo bellman_ford step 7171 current loss 0.011723, current_train_items 229504.
I0304 19:31:33.945211 23118544486528 run.py:483] Algo bellman_ford step 7172 current loss 0.050190, current_train_items 229536.
I0304 19:31:33.977261 23118544486528 run.py:483] Algo bellman_ford step 7173 current loss 0.044614, current_train_items 229568.
I0304 19:31:34.012552 23118544486528 run.py:483] Algo bellman_ford step 7174 current loss 0.081959, current_train_items 229600.
I0304 19:31:34.032602 23118544486528 run.py:483] Algo bellman_ford step 7175 current loss 0.005332, current_train_items 229632.
I0304 19:31:34.048711 23118544486528 run.py:483] Algo bellman_ford step 7176 current loss 0.022825, current_train_items 229664.
I0304 19:31:34.072716 23118544486528 run.py:483] Algo bellman_ford step 7177 current loss 0.034175, current_train_items 229696.
I0304 19:31:34.106512 23118544486528 run.py:483] Algo bellman_ford step 7178 current loss 0.052687, current_train_items 229728.
I0304 19:31:34.141741 23118544486528 run.py:483] Algo bellman_ford step 7179 current loss 0.047540, current_train_items 229760.
I0304 19:31:34.161404 23118544486528 run.py:483] Algo bellman_ford step 7180 current loss 0.007364, current_train_items 229792.
I0304 19:31:34.177707 23118544486528 run.py:483] Algo bellman_ford step 7181 current loss 0.036218, current_train_items 229824.
I0304 19:31:34.202644 23118544486528 run.py:483] Algo bellman_ford step 7182 current loss 0.072671, current_train_items 229856.
I0304 19:31:34.234209 23118544486528 run.py:483] Algo bellman_ford step 7183 current loss 0.091168, current_train_items 229888.
I0304 19:31:34.271504 23118544486528 run.py:483] Algo bellman_ford step 7184 current loss 0.072238, current_train_items 229920.
I0304 19:31:34.291355 23118544486528 run.py:483] Algo bellman_ford step 7185 current loss 0.004821, current_train_items 229952.
I0304 19:31:34.307720 23118544486528 run.py:483] Algo bellman_ford step 7186 current loss 0.024627, current_train_items 229984.
I0304 19:31:34.332813 23118544486528 run.py:483] Algo bellman_ford step 7187 current loss 0.054250, current_train_items 230016.
I0304 19:31:34.364984 23118544486528 run.py:483] Algo bellman_ford step 7188 current loss 0.064539, current_train_items 230048.
I0304 19:31:34.400781 23118544486528 run.py:483] Algo bellman_ford step 7189 current loss 0.059871, current_train_items 230080.
I0304 19:31:34.420624 23118544486528 run.py:483] Algo bellman_ford step 7190 current loss 0.005508, current_train_items 230112.
I0304 19:31:34.437294 23118544486528 run.py:483] Algo bellman_ford step 7191 current loss 0.039105, current_train_items 230144.
I0304 19:31:34.461294 23118544486528 run.py:483] Algo bellman_ford step 7192 current loss 0.041321, current_train_items 230176.
I0304 19:31:34.493525 23118544486528 run.py:483] Algo bellman_ford step 7193 current loss 0.044440, current_train_items 230208.
I0304 19:31:34.527181 23118544486528 run.py:483] Algo bellman_ford step 7194 current loss 0.074950, current_train_items 230240.
I0304 19:31:34.546935 23118544486528 run.py:483] Algo bellman_ford step 7195 current loss 0.004852, current_train_items 230272.
I0304 19:31:34.563147 23118544486528 run.py:483] Algo bellman_ford step 7196 current loss 0.044848, current_train_items 230304.
I0304 19:31:34.587215 23118544486528 run.py:483] Algo bellman_ford step 7197 current loss 0.084939, current_train_items 230336.
I0304 19:31:34.619616 23118544486528 run.py:483] Algo bellman_ford step 7198 current loss 0.170301, current_train_items 230368.
I0304 19:31:34.654039 23118544486528 run.py:483] Algo bellman_ford step 7199 current loss 0.133399, current_train_items 230400.
I0304 19:31:34.674095 23118544486528 run.py:483] Algo bellman_ford step 7200 current loss 0.012405, current_train_items 230432.
I0304 19:31:34.683393 23118544486528 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0304 19:31:34.683534 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:31:34.700592 23118544486528 run.py:483] Algo bellman_ford step 7201 current loss 0.025406, current_train_items 230464.
I0304 19:31:34.724741 23118544486528 run.py:483] Algo bellman_ford step 7202 current loss 0.022932, current_train_items 230496.
I0304 19:31:34.757441 23118544486528 run.py:483] Algo bellman_ford step 7203 current loss 0.102295, current_train_items 230528.
I0304 19:31:34.791369 23118544486528 run.py:483] Algo bellman_ford step 7204 current loss 0.093128, current_train_items 230560.
I0304 19:31:34.811291 23118544486528 run.py:483] Algo bellman_ford step 7205 current loss 0.011126, current_train_items 230592.
I0304 19:31:34.827237 23118544486528 run.py:483] Algo bellman_ford step 7206 current loss 0.041697, current_train_items 230624.
I0304 19:31:34.851887 23118544486528 run.py:483] Algo bellman_ford step 7207 current loss 0.039995, current_train_items 230656.
I0304 19:31:34.883317 23118544486528 run.py:483] Algo bellman_ford step 7208 current loss 0.052972, current_train_items 230688.
I0304 19:31:34.918539 23118544486528 run.py:483] Algo bellman_ford step 7209 current loss 0.069998, current_train_items 230720.
I0304 19:31:34.938330 23118544486528 run.py:483] Algo bellman_ford step 7210 current loss 0.006900, current_train_items 230752.
I0304 19:31:34.954414 23118544486528 run.py:483] Algo bellman_ford step 7211 current loss 0.013990, current_train_items 230784.
I0304 19:31:34.978146 23118544486528 run.py:483] Algo bellman_ford step 7212 current loss 0.050928, current_train_items 230816.
I0304 19:31:35.009691 23118544486528 run.py:483] Algo bellman_ford step 7213 current loss 0.047204, current_train_items 230848.
I0304 19:31:35.041702 23118544486528 run.py:483] Algo bellman_ford step 7214 current loss 0.067131, current_train_items 230880.
I0304 19:31:35.061273 23118544486528 run.py:483] Algo bellman_ford step 7215 current loss 0.007938, current_train_items 230912.
I0304 19:31:35.077856 23118544486528 run.py:483] Algo bellman_ford step 7216 current loss 0.007609, current_train_items 230944.
I0304 19:31:35.102339 23118544486528 run.py:483] Algo bellman_ford step 7217 current loss 0.057807, current_train_items 230976.
I0304 19:31:35.134415 23118544486528 run.py:483] Algo bellman_ford step 7218 current loss 0.069526, current_train_items 231008.
I0304 19:31:35.167904 23118544486528 run.py:483] Algo bellman_ford step 7219 current loss 0.084870, current_train_items 231040.
I0304 19:31:35.187614 23118544486528 run.py:483] Algo bellman_ford step 7220 current loss 0.021986, current_train_items 231072.
I0304 19:31:35.204065 23118544486528 run.py:483] Algo bellman_ford step 7221 current loss 0.020646, current_train_items 231104.
I0304 19:31:35.229648 23118544486528 run.py:483] Algo bellman_ford step 7222 current loss 0.037441, current_train_items 231136.
I0304 19:31:35.261719 23118544486528 run.py:483] Algo bellman_ford step 7223 current loss 0.059029, current_train_items 231168.
I0304 19:31:35.296531 23118544486528 run.py:483] Algo bellman_ford step 7224 current loss 0.087664, current_train_items 231200.
I0304 19:31:35.316812 23118544486528 run.py:483] Algo bellman_ford step 7225 current loss 0.005850, current_train_items 231232.
I0304 19:31:35.333240 23118544486528 run.py:483] Algo bellman_ford step 7226 current loss 0.018542, current_train_items 231264.
I0304 19:31:35.359558 23118544486528 run.py:483] Algo bellman_ford step 7227 current loss 0.042259, current_train_items 231296.
I0304 19:31:35.391308 23118544486528 run.py:483] Algo bellman_ford step 7228 current loss 0.028118, current_train_items 231328.
I0304 19:31:35.423522 23118544486528 run.py:483] Algo bellman_ford step 7229 current loss 0.058103, current_train_items 231360.
I0304 19:31:35.443054 23118544486528 run.py:483] Algo bellman_ford step 7230 current loss 0.004693, current_train_items 231392.
I0304 19:31:35.459525 23118544486528 run.py:483] Algo bellman_ford step 7231 current loss 0.009952, current_train_items 231424.
I0304 19:31:35.481561 23118544486528 run.py:483] Algo bellman_ford step 7232 current loss 0.012428, current_train_items 231456.
I0304 19:31:35.514404 23118544486528 run.py:483] Algo bellman_ford step 7233 current loss 0.037381, current_train_items 231488.
I0304 19:31:35.547422 23118544486528 run.py:483] Algo bellman_ford step 7234 current loss 0.035026, current_train_items 231520.
I0304 19:31:35.567082 23118544486528 run.py:483] Algo bellman_ford step 7235 current loss 0.004533, current_train_items 231552.
I0304 19:31:35.583731 23118544486528 run.py:483] Algo bellman_ford step 7236 current loss 0.013526, current_train_items 231584.
I0304 19:31:35.609038 23118544486528 run.py:483] Algo bellman_ford step 7237 current loss 0.031966, current_train_items 231616.
I0304 19:31:35.642376 23118544486528 run.py:483] Algo bellman_ford step 7238 current loss 0.086197, current_train_items 231648.
I0304 19:31:35.676470 23118544486528 run.py:483] Algo bellman_ford step 7239 current loss 0.038036, current_train_items 231680.
I0304 19:31:35.696549 23118544486528 run.py:483] Algo bellman_ford step 7240 current loss 0.012898, current_train_items 231712.
I0304 19:31:35.712794 23118544486528 run.py:483] Algo bellman_ford step 7241 current loss 0.009228, current_train_items 231744.
I0304 19:31:35.737963 23118544486528 run.py:483] Algo bellman_ford step 7242 current loss 0.043873, current_train_items 231776.
I0304 19:31:35.770088 23118544486528 run.py:483] Algo bellman_ford step 7243 current loss 0.057561, current_train_items 231808.
I0304 19:31:35.806149 23118544486528 run.py:483] Algo bellman_ford step 7244 current loss 0.048307, current_train_items 231840.
I0304 19:31:35.825577 23118544486528 run.py:483] Algo bellman_ford step 7245 current loss 0.002619, current_train_items 231872.
I0304 19:31:35.841875 23118544486528 run.py:483] Algo bellman_ford step 7246 current loss 0.006873, current_train_items 231904.
I0304 19:31:35.866571 23118544486528 run.py:483] Algo bellman_ford step 7247 current loss 0.022477, current_train_items 231936.
I0304 19:31:35.899904 23118544486528 run.py:483] Algo bellman_ford step 7248 current loss 0.063379, current_train_items 231968.
I0304 19:31:35.934558 23118544486528 run.py:483] Algo bellman_ford step 7249 current loss 0.076409, current_train_items 232000.
I0304 19:31:35.954329 23118544486528 run.py:483] Algo bellman_ford step 7250 current loss 0.021227, current_train_items 232032.
I0304 19:31:35.962288 23118544486528 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0304 19:31:35.962394 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:35.979541 23118544486528 run.py:483] Algo bellman_ford step 7251 current loss 0.049164, current_train_items 232064.
I0304 19:31:36.004120 23118544486528 run.py:483] Algo bellman_ford step 7252 current loss 0.040416, current_train_items 232096.
I0304 19:31:36.035457 23118544486528 run.py:483] Algo bellman_ford step 7253 current loss 0.038107, current_train_items 232128.
I0304 19:31:36.070139 23118544486528 run.py:483] Algo bellman_ford step 7254 current loss 0.068426, current_train_items 232160.
I0304 19:31:36.090213 23118544486528 run.py:483] Algo bellman_ford step 7255 current loss 0.002767, current_train_items 232192.
I0304 19:31:36.105901 23118544486528 run.py:483] Algo bellman_ford step 7256 current loss 0.024349, current_train_items 232224.
I0304 19:31:36.130279 23118544486528 run.py:483] Algo bellman_ford step 7257 current loss 0.070705, current_train_items 232256.
I0304 19:31:36.165296 23118544486528 run.py:483] Algo bellman_ford step 7258 current loss 0.097993, current_train_items 232288.
I0304 19:31:36.199980 23118544486528 run.py:483] Algo bellman_ford step 7259 current loss 0.102820, current_train_items 232320.
I0304 19:31:36.220186 23118544486528 run.py:483] Algo bellman_ford step 7260 current loss 0.002879, current_train_items 232352.
I0304 19:31:36.236643 23118544486528 run.py:483] Algo bellman_ford step 7261 current loss 0.014113, current_train_items 232384.
I0304 19:31:36.260971 23118544486528 run.py:483] Algo bellman_ford step 7262 current loss 0.041743, current_train_items 232416.
I0304 19:31:36.293552 23118544486528 run.py:483] Algo bellman_ford step 7263 current loss 0.080734, current_train_items 232448.
I0304 19:31:36.327677 23118544486528 run.py:483] Algo bellman_ford step 7264 current loss 0.103309, current_train_items 232480.
I0304 19:31:36.347325 23118544486528 run.py:483] Algo bellman_ford step 7265 current loss 0.009406, current_train_items 232512.
I0304 19:31:36.363670 23118544486528 run.py:483] Algo bellman_ford step 7266 current loss 0.024869, current_train_items 232544.
I0304 19:31:36.387813 23118544486528 run.py:483] Algo bellman_ford step 7267 current loss 0.037319, current_train_items 232576.
I0304 19:31:36.419786 23118544486528 run.py:483] Algo bellman_ford step 7268 current loss 0.073794, current_train_items 232608.
I0304 19:31:36.453327 23118544486528 run.py:483] Algo bellman_ford step 7269 current loss 0.061634, current_train_items 232640.
I0304 19:31:36.473519 23118544486528 run.py:483] Algo bellman_ford step 7270 current loss 0.006156, current_train_items 232672.
I0304 19:31:36.490549 23118544486528 run.py:483] Algo bellman_ford step 7271 current loss 0.035358, current_train_items 232704.
I0304 19:31:36.514728 23118544486528 run.py:483] Algo bellman_ford step 7272 current loss 0.024094, current_train_items 232736.
I0304 19:31:36.546463 23118544486528 run.py:483] Algo bellman_ford step 7273 current loss 0.039346, current_train_items 232768.
I0304 19:31:36.580382 23118544486528 run.py:483] Algo bellman_ford step 7274 current loss 0.136797, current_train_items 232800.
I0304 19:31:36.600557 23118544486528 run.py:483] Algo bellman_ford step 7275 current loss 0.002928, current_train_items 232832.
I0304 19:31:36.616868 23118544486528 run.py:483] Algo bellman_ford step 7276 current loss 0.016158, current_train_items 232864.
I0304 19:31:36.641057 23118544486528 run.py:483] Algo bellman_ford step 7277 current loss 0.040543, current_train_items 232896.
I0304 19:31:36.673613 23118544486528 run.py:483] Algo bellman_ford step 7278 current loss 0.054667, current_train_items 232928.
I0304 19:31:36.707757 23118544486528 run.py:483] Algo bellman_ford step 7279 current loss 0.050511, current_train_items 232960.
I0304 19:31:36.727556 23118544486528 run.py:483] Algo bellman_ford step 7280 current loss 0.021258, current_train_items 232992.
I0304 19:31:36.744215 23118544486528 run.py:483] Algo bellman_ford step 7281 current loss 0.028658, current_train_items 233024.
I0304 19:31:36.768216 23118544486528 run.py:483] Algo bellman_ford step 7282 current loss 0.018679, current_train_items 233056.
I0304 19:31:36.800476 23118544486528 run.py:483] Algo bellman_ford step 7283 current loss 0.057124, current_train_items 233088.
I0304 19:31:36.834448 23118544486528 run.py:483] Algo bellman_ford step 7284 current loss 0.054690, current_train_items 233120.
I0304 19:31:36.854406 23118544486528 run.py:483] Algo bellman_ford step 7285 current loss 0.003899, current_train_items 233152.
I0304 19:31:36.871764 23118544486528 run.py:483] Algo bellman_ford step 7286 current loss 0.045904, current_train_items 233184.
I0304 19:31:36.895666 23118544486528 run.py:483] Algo bellman_ford step 7287 current loss 0.040292, current_train_items 233216.
I0304 19:31:36.927785 23118544486528 run.py:483] Algo bellman_ford step 7288 current loss 0.030471, current_train_items 233248.
I0304 19:31:36.961340 23118544486528 run.py:483] Algo bellman_ford step 7289 current loss 0.047972, current_train_items 233280.
I0304 19:31:36.981410 23118544486528 run.py:483] Algo bellman_ford step 7290 current loss 0.019824, current_train_items 233312.
I0304 19:31:36.997905 23118544486528 run.py:483] Algo bellman_ford step 7291 current loss 0.024769, current_train_items 233344.
I0304 19:31:37.021290 23118544486528 run.py:483] Algo bellman_ford step 7292 current loss 0.018832, current_train_items 233376.
I0304 19:31:37.053739 23118544486528 run.py:483] Algo bellman_ford step 7293 current loss 0.046922, current_train_items 233408.
I0304 19:31:37.086282 23118544486528 run.py:483] Algo bellman_ford step 7294 current loss 0.068399, current_train_items 233440.
I0304 19:31:37.106011 23118544486528 run.py:483] Algo bellman_ford step 7295 current loss 0.007939, current_train_items 233472.
I0304 19:31:37.122455 23118544486528 run.py:483] Algo bellman_ford step 7296 current loss 0.009948, current_train_items 233504.
I0304 19:31:37.146730 23118544486528 run.py:483] Algo bellman_ford step 7297 current loss 0.071207, current_train_items 233536.
I0304 19:31:37.179014 23118544486528 run.py:483] Algo bellman_ford step 7298 current loss 0.048110, current_train_items 233568.
I0304 19:31:37.212623 23118544486528 run.py:483] Algo bellman_ford step 7299 current loss 0.079751, current_train_items 233600.
I0304 19:31:37.232825 23118544486528 run.py:483] Algo bellman_ford step 7300 current loss 0.008068, current_train_items 233632.
I0304 19:31:37.240596 23118544486528 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0304 19:31:37.240710 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:37.258427 23118544486528 run.py:483] Algo bellman_ford step 7301 current loss 0.012232, current_train_items 233664.
I0304 19:31:37.282781 23118544486528 run.py:483] Algo bellman_ford step 7302 current loss 0.033181, current_train_items 233696.
I0304 19:31:37.315034 23118544486528 run.py:483] Algo bellman_ford step 7303 current loss 0.044567, current_train_items 233728.
I0304 19:31:37.349965 23118544486528 run.py:483] Algo bellman_ford step 7304 current loss 0.048562, current_train_items 233760.
I0304 19:31:37.369936 23118544486528 run.py:483] Algo bellman_ford step 7305 current loss 0.003507, current_train_items 233792.
I0304 19:31:37.385821 23118544486528 run.py:483] Algo bellman_ford step 7306 current loss 0.016845, current_train_items 233824.
I0304 19:31:37.409824 23118544486528 run.py:483] Algo bellman_ford step 7307 current loss 0.055107, current_train_items 233856.
I0304 19:31:37.440892 23118544486528 run.py:483] Algo bellman_ford step 7308 current loss 0.038706, current_train_items 233888.
I0304 19:31:37.476717 23118544486528 run.py:483] Algo bellman_ford step 7309 current loss 0.066732, current_train_items 233920.
I0304 19:31:37.496944 23118544486528 run.py:483] Algo bellman_ford step 7310 current loss 0.005337, current_train_items 233952.
I0304 19:31:37.513142 23118544486528 run.py:483] Algo bellman_ford step 7311 current loss 0.006623, current_train_items 233984.
I0304 19:31:37.538738 23118544486528 run.py:483] Algo bellman_ford step 7312 current loss 0.062512, current_train_items 234016.
I0304 19:31:37.571118 23118544486528 run.py:483] Algo bellman_ford step 7313 current loss 0.084222, current_train_items 234048.
I0304 19:31:37.606549 23118544486528 run.py:483] Algo bellman_ford step 7314 current loss 0.038222, current_train_items 234080.
I0304 19:31:37.626744 23118544486528 run.py:483] Algo bellman_ford step 7315 current loss 0.004361, current_train_items 234112.
I0304 19:31:37.642983 23118544486528 run.py:483] Algo bellman_ford step 7316 current loss 0.011154, current_train_items 234144.
I0304 19:31:37.668376 23118544486528 run.py:483] Algo bellman_ford step 7317 current loss 0.079001, current_train_items 234176.
I0304 19:31:37.700417 23118544486528 run.py:483] Algo bellman_ford step 7318 current loss 0.105209, current_train_items 234208.
I0304 19:31:37.736383 23118544486528 run.py:483] Algo bellman_ford step 7319 current loss 0.121887, current_train_items 234240.
I0304 19:31:37.756386 23118544486528 run.py:483] Algo bellman_ford step 7320 current loss 0.004063, current_train_items 234272.
I0304 19:31:37.773334 23118544486528 run.py:483] Algo bellman_ford step 7321 current loss 0.016491, current_train_items 234304.
I0304 19:31:37.799150 23118544486528 run.py:483] Algo bellman_ford step 7322 current loss 0.059237, current_train_items 234336.
I0304 19:31:37.830680 23118544486528 run.py:483] Algo bellman_ford step 7323 current loss 0.024146, current_train_items 234368.
I0304 19:31:37.866657 23118544486528 run.py:483] Algo bellman_ford step 7324 current loss 0.115022, current_train_items 234400.
I0304 19:31:37.886291 23118544486528 run.py:483] Algo bellman_ford step 7325 current loss 0.002718, current_train_items 234432.
I0304 19:31:37.903069 23118544486528 run.py:483] Algo bellman_ford step 7326 current loss 0.024934, current_train_items 234464.
I0304 19:31:37.927892 23118544486528 run.py:483] Algo bellman_ford step 7327 current loss 0.027665, current_train_items 234496.
I0304 19:31:37.959851 23118544486528 run.py:483] Algo bellman_ford step 7328 current loss 0.050895, current_train_items 234528.
I0304 19:31:37.995010 23118544486528 run.py:483] Algo bellman_ford step 7329 current loss 0.060593, current_train_items 234560.
I0304 19:31:38.014482 23118544486528 run.py:483] Algo bellman_ford step 7330 current loss 0.003113, current_train_items 234592.
I0304 19:31:38.031138 23118544486528 run.py:483] Algo bellman_ford step 7331 current loss 0.054333, current_train_items 234624.
I0304 19:31:38.056510 23118544486528 run.py:483] Algo bellman_ford step 7332 current loss 0.049757, current_train_items 234656.
I0304 19:31:38.089221 23118544486528 run.py:483] Algo bellman_ford step 7333 current loss 0.042851, current_train_items 234688.
I0304 19:31:38.121807 23118544486528 run.py:483] Algo bellman_ford step 7334 current loss 0.045455, current_train_items 234720.
I0304 19:31:38.141438 23118544486528 run.py:483] Algo bellman_ford step 7335 current loss 0.003248, current_train_items 234752.
I0304 19:31:38.157564 23118544486528 run.py:483] Algo bellman_ford step 7336 current loss 0.016829, current_train_items 234784.
I0304 19:31:38.182289 23118544486528 run.py:483] Algo bellman_ford step 7337 current loss 0.049781, current_train_items 234816.
I0304 19:31:38.214749 23118544486528 run.py:483] Algo bellman_ford step 7338 current loss 0.055798, current_train_items 234848.
I0304 19:31:38.250746 23118544486528 run.py:483] Algo bellman_ford step 7339 current loss 0.063917, current_train_items 234880.
I0304 19:31:38.270581 23118544486528 run.py:483] Algo bellman_ford step 7340 current loss 0.004782, current_train_items 234912.
I0304 19:31:38.286971 23118544486528 run.py:483] Algo bellman_ford step 7341 current loss 0.014738, current_train_items 234944.
I0304 19:31:38.312582 23118544486528 run.py:483] Algo bellman_ford step 7342 current loss 0.041483, current_train_items 234976.
I0304 19:31:38.345608 23118544486528 run.py:483] Algo bellman_ford step 7343 current loss 0.047619, current_train_items 235008.
I0304 19:31:38.379741 23118544486528 run.py:483] Algo bellman_ford step 7344 current loss 0.078131, current_train_items 235040.
I0304 19:31:38.399848 23118544486528 run.py:483] Algo bellman_ford step 7345 current loss 0.010732, current_train_items 235072.
I0304 19:31:38.415873 23118544486528 run.py:483] Algo bellman_ford step 7346 current loss 0.009932, current_train_items 235104.
I0304 19:31:38.441003 23118544486528 run.py:483] Algo bellman_ford step 7347 current loss 0.063071, current_train_items 235136.
I0304 19:31:38.473596 23118544486528 run.py:483] Algo bellman_ford step 7348 current loss 0.043686, current_train_items 235168.
I0304 19:31:38.504456 23118544486528 run.py:483] Algo bellman_ford step 7349 current loss 0.028944, current_train_items 235200.
I0304 19:31:38.524108 23118544486528 run.py:483] Algo bellman_ford step 7350 current loss 0.005784, current_train_items 235232.
I0304 19:31:38.532521 23118544486528 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0304 19:31:38.532629 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:31:38.550269 23118544486528 run.py:483] Algo bellman_ford step 7351 current loss 0.018847, current_train_items 235264.
I0304 19:31:38.576185 23118544486528 run.py:483] Algo bellman_ford step 7352 current loss 0.033720, current_train_items 235296.
I0304 19:31:38.609282 23118544486528 run.py:483] Algo bellman_ford step 7353 current loss 0.048017, current_train_items 235328.
I0304 19:31:38.643240 23118544486528 run.py:483] Algo bellman_ford step 7354 current loss 0.046208, current_train_items 235360.
I0304 19:31:38.663663 23118544486528 run.py:483] Algo bellman_ford step 7355 current loss 0.005730, current_train_items 235392.
I0304 19:31:38.679682 23118544486528 run.py:483] Algo bellman_ford step 7356 current loss 0.020671, current_train_items 235424.
I0304 19:31:38.705307 23118544486528 run.py:483] Algo bellman_ford step 7357 current loss 0.026759, current_train_items 235456.
I0304 19:31:38.737488 23118544486528 run.py:483] Algo bellman_ford step 7358 current loss 0.044802, current_train_items 235488.
I0304 19:31:38.771694 23118544486528 run.py:483] Algo bellman_ford step 7359 current loss 0.067766, current_train_items 235520.
I0304 19:31:38.791824 23118544486528 run.py:483] Algo bellman_ford step 7360 current loss 0.003681, current_train_items 235552.
I0304 19:31:38.808793 23118544486528 run.py:483] Algo bellman_ford step 7361 current loss 0.024647, current_train_items 235584.
I0304 19:31:38.832150 23118544486528 run.py:483] Algo bellman_ford step 7362 current loss 0.017660, current_train_items 235616.
I0304 19:31:38.866493 23118544486528 run.py:483] Algo bellman_ford step 7363 current loss 0.084813, current_train_items 235648.
I0304 19:31:38.898958 23118544486528 run.py:483] Algo bellman_ford step 7364 current loss 0.076622, current_train_items 235680.
I0304 19:31:38.918900 23118544486528 run.py:483] Algo bellman_ford step 7365 current loss 0.098940, current_train_items 235712.
I0304 19:31:38.935270 23118544486528 run.py:483] Algo bellman_ford step 7366 current loss 0.018597, current_train_items 235744.
I0304 19:31:38.960487 23118544486528 run.py:483] Algo bellman_ford step 7367 current loss 0.030679, current_train_items 235776.
I0304 19:31:38.994809 23118544486528 run.py:483] Algo bellman_ford step 7368 current loss 0.090087, current_train_items 235808.
I0304 19:31:39.027874 23118544486528 run.py:483] Algo bellman_ford step 7369 current loss 0.044973, current_train_items 235840.
I0304 19:31:39.047772 23118544486528 run.py:483] Algo bellman_ford step 7370 current loss 0.007324, current_train_items 235872.
I0304 19:31:39.064313 23118544486528 run.py:483] Algo bellman_ford step 7371 current loss 0.020394, current_train_items 235904.
I0304 19:31:39.087513 23118544486528 run.py:483] Algo bellman_ford step 7372 current loss 0.031616, current_train_items 235936.
I0304 19:31:39.119468 23118544486528 run.py:483] Algo bellman_ford step 7373 current loss 0.033134, current_train_items 235968.
I0304 19:31:39.153107 23118544486528 run.py:483] Algo bellman_ford step 7374 current loss 0.082318, current_train_items 236000.
I0304 19:31:39.173505 23118544486528 run.py:483] Algo bellman_ford step 7375 current loss 0.003102, current_train_items 236032.
I0304 19:31:39.190508 23118544486528 run.py:483] Algo bellman_ford step 7376 current loss 0.035485, current_train_items 236064.
I0304 19:31:39.215221 23118544486528 run.py:483] Algo bellman_ford step 7377 current loss 0.027436, current_train_items 236096.
I0304 19:31:39.247462 23118544486528 run.py:483] Algo bellman_ford step 7378 current loss 0.061318, current_train_items 236128.
I0304 19:31:39.280566 23118544486528 run.py:483] Algo bellman_ford step 7379 current loss 0.086383, current_train_items 236160.
I0304 19:31:39.300043 23118544486528 run.py:483] Algo bellman_ford step 7380 current loss 0.003491, current_train_items 236192.
I0304 19:31:39.316024 23118544486528 run.py:483] Algo bellman_ford step 7381 current loss 0.030222, current_train_items 236224.
I0304 19:31:39.341444 23118544486528 run.py:483] Algo bellman_ford step 7382 current loss 0.037976, current_train_items 236256.
I0304 19:31:39.373075 23118544486528 run.py:483] Algo bellman_ford step 7383 current loss 0.036548, current_train_items 236288.
I0304 19:31:39.405991 23118544486528 run.py:483] Algo bellman_ford step 7384 current loss 0.049226, current_train_items 236320.
I0304 19:31:39.425689 23118544486528 run.py:483] Algo bellman_ford step 7385 current loss 0.003811, current_train_items 236352.
I0304 19:31:39.442203 23118544486528 run.py:483] Algo bellman_ford step 7386 current loss 0.018817, current_train_items 236384.
I0304 19:31:39.466701 23118544486528 run.py:483] Algo bellman_ford step 7387 current loss 0.017261, current_train_items 236416.
I0304 19:31:39.499643 23118544486528 run.py:483] Algo bellman_ford step 7388 current loss 0.041651, current_train_items 236448.
I0304 19:31:39.532691 23118544486528 run.py:483] Algo bellman_ford step 7389 current loss 0.049036, current_train_items 236480.
I0304 19:31:39.553242 23118544486528 run.py:483] Algo bellman_ford step 7390 current loss 0.003374, current_train_items 236512.
I0304 19:31:39.570020 23118544486528 run.py:483] Algo bellman_ford step 7391 current loss 0.012776, current_train_items 236544.
I0304 19:31:39.594297 23118544486528 run.py:483] Algo bellman_ford step 7392 current loss 0.070145, current_train_items 236576.
I0304 19:31:39.625369 23118544486528 run.py:483] Algo bellman_ford step 7393 current loss 0.084205, current_train_items 236608.
I0304 19:31:39.659618 23118544486528 run.py:483] Algo bellman_ford step 7394 current loss 0.136927, current_train_items 236640.
I0304 19:31:39.679559 23118544486528 run.py:483] Algo bellman_ford step 7395 current loss 0.001962, current_train_items 236672.
I0304 19:31:39.696563 23118544486528 run.py:483] Algo bellman_ford step 7396 current loss 0.024118, current_train_items 236704.
I0304 19:31:39.721658 23118544486528 run.py:483] Algo bellman_ford step 7397 current loss 0.031086, current_train_items 236736.
I0304 19:31:39.753229 23118544486528 run.py:483] Algo bellman_ford step 7398 current loss 0.097419, current_train_items 236768.
I0304 19:31:39.786434 23118544486528 run.py:483] Algo bellman_ford step 7399 current loss 0.077591, current_train_items 236800.
I0304 19:31:39.806561 23118544486528 run.py:483] Algo bellman_ford step 7400 current loss 0.003690, current_train_items 236832.
I0304 19:31:39.814647 23118544486528 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0304 19:31:39.814763 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:39.832100 23118544486528 run.py:483] Algo bellman_ford step 7401 current loss 0.010102, current_train_items 236864.
I0304 19:31:39.857892 23118544486528 run.py:483] Algo bellman_ford step 7402 current loss 0.048072, current_train_items 236896.
I0304 19:31:39.891015 23118544486528 run.py:483] Algo bellman_ford step 7403 current loss 0.058819, current_train_items 236928.
I0304 19:31:39.925888 23118544486528 run.py:483] Algo bellman_ford step 7404 current loss 0.061120, current_train_items 236960.
I0304 19:31:39.946169 23118544486528 run.py:483] Algo bellman_ford step 7405 current loss 0.004114, current_train_items 236992.
I0304 19:31:39.961916 23118544486528 run.py:483] Algo bellman_ford step 7406 current loss 0.011771, current_train_items 237024.
I0304 19:31:39.986422 23118544486528 run.py:483] Algo bellman_ford step 7407 current loss 0.054204, current_train_items 237056.
I0304 19:31:40.019212 23118544486528 run.py:483] Algo bellman_ford step 7408 current loss 0.100442, current_train_items 237088.
I0304 19:31:40.051499 23118544486528 run.py:483] Algo bellman_ford step 7409 current loss 0.094282, current_train_items 237120.
I0304 19:31:40.071204 23118544486528 run.py:483] Algo bellman_ford step 7410 current loss 0.004946, current_train_items 237152.
I0304 19:31:40.088152 23118544486528 run.py:483] Algo bellman_ford step 7411 current loss 0.014170, current_train_items 237184.
I0304 19:31:40.113638 23118544486528 run.py:483] Algo bellman_ford step 7412 current loss 0.068940, current_train_items 237216.
I0304 19:31:40.146490 23118544486528 run.py:483] Algo bellman_ford step 7413 current loss 0.117327, current_train_items 237248.
I0304 19:31:40.179124 23118544486528 run.py:483] Algo bellman_ford step 7414 current loss 0.085838, current_train_items 237280.
I0304 19:31:40.199091 23118544486528 run.py:483] Algo bellman_ford step 7415 current loss 0.008183, current_train_items 237312.
I0304 19:31:40.215211 23118544486528 run.py:483] Algo bellman_ford step 7416 current loss 0.007264, current_train_items 237344.
I0304 19:31:40.239697 23118544486528 run.py:483] Algo bellman_ford step 7417 current loss 0.073875, current_train_items 237376.
I0304 19:31:40.271723 23118544486528 run.py:483] Algo bellman_ford step 7418 current loss 0.067969, current_train_items 237408.
I0304 19:31:40.304914 23118544486528 run.py:483] Algo bellman_ford step 7419 current loss 0.097923, current_train_items 237440.
I0304 19:31:40.324451 23118544486528 run.py:483] Algo bellman_ford step 7420 current loss 0.008199, current_train_items 237472.
I0304 19:31:40.340581 23118544486528 run.py:483] Algo bellman_ford step 7421 current loss 0.010136, current_train_items 237504.
I0304 19:31:40.364798 23118544486528 run.py:483] Algo bellman_ford step 7422 current loss 0.067589, current_train_items 237536.
I0304 19:31:40.396449 23118544486528 run.py:483] Algo bellman_ford step 7423 current loss 0.143437, current_train_items 237568.
I0304 19:31:40.429909 23118544486528 run.py:483] Algo bellman_ford step 7424 current loss 0.044690, current_train_items 237600.
I0304 19:31:40.449549 23118544486528 run.py:483] Algo bellman_ford step 7425 current loss 0.007207, current_train_items 237632.
I0304 19:31:40.465943 23118544486528 run.py:483] Algo bellman_ford step 7426 current loss 0.013020, current_train_items 237664.
I0304 19:31:40.491557 23118544486528 run.py:483] Algo bellman_ford step 7427 current loss 0.038583, current_train_items 237696.
I0304 19:31:40.523344 23118544486528 run.py:483] Algo bellman_ford step 7428 current loss 0.044553, current_train_items 237728.
I0304 19:31:40.557823 23118544486528 run.py:483] Algo bellman_ford step 7429 current loss 0.086650, current_train_items 237760.
I0304 19:31:40.577682 23118544486528 run.py:483] Algo bellman_ford step 7430 current loss 0.004585, current_train_items 237792.
I0304 19:31:40.593789 23118544486528 run.py:483] Algo bellman_ford step 7431 current loss 0.022200, current_train_items 237824.
I0304 19:31:40.619033 23118544486528 run.py:483] Algo bellman_ford step 7432 current loss 0.050013, current_train_items 237856.
I0304 19:31:40.651492 23118544486528 run.py:483] Algo bellman_ford step 7433 current loss 0.032707, current_train_items 237888.
I0304 19:31:40.683026 23118544486528 run.py:483] Algo bellman_ford step 7434 current loss 0.048020, current_train_items 237920.
I0304 19:31:40.702772 23118544486528 run.py:483] Algo bellman_ford step 7435 current loss 0.003908, current_train_items 237952.
I0304 19:31:40.719180 23118544486528 run.py:483] Algo bellman_ford step 7436 current loss 0.011024, current_train_items 237984.
I0304 19:31:40.744492 23118544486528 run.py:483] Algo bellman_ford step 7437 current loss 0.071429, current_train_items 238016.
I0304 19:31:40.775906 23118544486528 run.py:483] Algo bellman_ford step 7438 current loss 0.134057, current_train_items 238048.
I0304 19:31:40.810984 23118544486528 run.py:483] Algo bellman_ford step 7439 current loss 0.104690, current_train_items 238080.
I0304 19:31:40.830651 23118544486528 run.py:483] Algo bellman_ford step 7440 current loss 0.005702, current_train_items 238112.
I0304 19:31:40.847552 23118544486528 run.py:483] Algo bellman_ford step 7441 current loss 0.032766, current_train_items 238144.
I0304 19:31:40.871353 23118544486528 run.py:483] Algo bellman_ford step 7442 current loss 0.020257, current_train_items 238176.
I0304 19:31:40.904239 23118544486528 run.py:483] Algo bellman_ford step 7443 current loss 0.061513, current_train_items 238208.
I0304 19:31:40.939267 23118544486528 run.py:483] Algo bellman_ford step 7444 current loss 0.090113, current_train_items 238240.
I0304 19:31:40.959108 23118544486528 run.py:483] Algo bellman_ford step 7445 current loss 0.005774, current_train_items 238272.
I0304 19:31:40.975410 23118544486528 run.py:483] Algo bellman_ford step 7446 current loss 0.011050, current_train_items 238304.
I0304 19:31:40.999842 23118544486528 run.py:483] Algo bellman_ford step 7447 current loss 0.030650, current_train_items 238336.
I0304 19:31:41.031397 23118544486528 run.py:483] Algo bellman_ford step 7448 current loss 0.043111, current_train_items 238368.
I0304 19:31:41.066942 23118544486528 run.py:483] Algo bellman_ford step 7449 current loss 0.085273, current_train_items 238400.
I0304 19:31:41.086703 23118544486528 run.py:483] Algo bellman_ford step 7450 current loss 0.004128, current_train_items 238432.
I0304 19:31:41.094793 23118544486528 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0304 19:31:41.094899 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:31:41.111899 23118544486528 run.py:483] Algo bellman_ford step 7451 current loss 0.014164, current_train_items 238464.
I0304 19:31:41.137839 23118544486528 run.py:483] Algo bellman_ford step 7452 current loss 0.039576, current_train_items 238496.
I0304 19:31:41.169435 23118544486528 run.py:483] Algo bellman_ford step 7453 current loss 0.038455, current_train_items 238528.
I0304 19:31:41.202804 23118544486528 run.py:483] Algo bellman_ford step 7454 current loss 0.057898, current_train_items 238560.
I0304 19:31:41.223334 23118544486528 run.py:483] Algo bellman_ford step 7455 current loss 0.020790, current_train_items 238592.
I0304 19:31:41.239905 23118544486528 run.py:483] Algo bellman_ford step 7456 current loss 0.021750, current_train_items 238624.
I0304 19:31:41.264058 23118544486528 run.py:483] Algo bellman_ford step 7457 current loss 0.027055, current_train_items 238656.
I0304 19:31:41.296787 23118544486528 run.py:483] Algo bellman_ford step 7458 current loss 0.068703, current_train_items 238688.
I0304 19:31:41.330330 23118544486528 run.py:483] Algo bellman_ford step 7459 current loss 0.063804, current_train_items 238720.
I0304 19:31:41.350350 23118544486528 run.py:483] Algo bellman_ford step 7460 current loss 0.004738, current_train_items 238752.
I0304 19:31:41.367407 23118544486528 run.py:483] Algo bellman_ford step 7461 current loss 0.014884, current_train_items 238784.
I0304 19:31:41.390813 23118544486528 run.py:483] Algo bellman_ford step 7462 current loss 0.061752, current_train_items 238816.
I0304 19:31:41.422604 23118544486528 run.py:483] Algo bellman_ford step 7463 current loss 0.029696, current_train_items 238848.
I0304 19:31:41.456556 23118544486528 run.py:483] Algo bellman_ford step 7464 current loss 0.074465, current_train_items 238880.
I0304 19:31:41.476268 23118544486528 run.py:483] Algo bellman_ford step 7465 current loss 0.003797, current_train_items 238912.
I0304 19:31:41.492761 23118544486528 run.py:483] Algo bellman_ford step 7466 current loss 0.008645, current_train_items 238944.
I0304 19:31:41.516586 23118544486528 run.py:483] Algo bellman_ford step 7467 current loss 0.055505, current_train_items 238976.
I0304 19:31:41.549667 23118544486528 run.py:483] Algo bellman_ford step 7468 current loss 0.107355, current_train_items 239008.
I0304 19:31:41.582172 23118544486528 run.py:483] Algo bellman_ford step 7469 current loss 0.060928, current_train_items 239040.
I0304 19:31:41.602254 23118544486528 run.py:483] Algo bellman_ford step 7470 current loss 0.007590, current_train_items 239072.
I0304 19:31:41.618669 23118544486528 run.py:483] Algo bellman_ford step 7471 current loss 0.015882, current_train_items 239104.
I0304 19:31:41.642171 23118544486528 run.py:483] Algo bellman_ford step 7472 current loss 0.058915, current_train_items 239136.
I0304 19:31:41.674439 23118544486528 run.py:483] Algo bellman_ford step 7473 current loss 0.082424, current_train_items 239168.
I0304 19:31:41.709877 23118544486528 run.py:483] Algo bellman_ford step 7474 current loss 0.042191, current_train_items 239200.
I0304 19:31:41.729932 23118544486528 run.py:483] Algo bellman_ford step 7475 current loss 0.004938, current_train_items 239232.
I0304 19:31:41.746572 23118544486528 run.py:483] Algo bellman_ford step 7476 current loss 0.018200, current_train_items 239264.
I0304 19:31:41.769873 23118544486528 run.py:483] Algo bellman_ford step 7477 current loss 0.031942, current_train_items 239296.
I0304 19:31:41.802263 23118544486528 run.py:483] Algo bellman_ford step 7478 current loss 0.031721, current_train_items 239328.
I0304 19:31:41.839424 23118544486528 run.py:483] Algo bellman_ford step 7479 current loss 0.115055, current_train_items 239360.
I0304 19:31:41.858934 23118544486528 run.py:483] Algo bellman_ford step 7480 current loss 0.005449, current_train_items 239392.
I0304 19:31:41.875579 23118544486528 run.py:483] Algo bellman_ford step 7481 current loss 0.019864, current_train_items 239424.
I0304 19:31:41.900987 23118544486528 run.py:483] Algo bellman_ford step 7482 current loss 0.072477, current_train_items 239456.
I0304 19:31:41.934346 23118544486528 run.py:483] Algo bellman_ford step 7483 current loss 0.058636, current_train_items 239488.
I0304 19:31:41.969239 23118544486528 run.py:483] Algo bellman_ford step 7484 current loss 0.084890, current_train_items 239520.
I0304 19:31:41.989289 23118544486528 run.py:483] Algo bellman_ford step 7485 current loss 0.006183, current_train_items 239552.
I0304 19:31:42.005716 23118544486528 run.py:483] Algo bellman_ford step 7486 current loss 0.011055, current_train_items 239584.
I0304 19:31:42.029073 23118544486528 run.py:483] Algo bellman_ford step 7487 current loss 0.057782, current_train_items 239616.
I0304 19:31:42.061458 23118544486528 run.py:483] Algo bellman_ford step 7488 current loss 0.049825, current_train_items 239648.
I0304 19:31:42.097548 23118544486528 run.py:483] Algo bellman_ford step 7489 current loss 0.080173, current_train_items 239680.
I0304 19:31:42.117401 23118544486528 run.py:483] Algo bellman_ford step 7490 current loss 0.004844, current_train_items 239712.
I0304 19:31:42.133472 23118544486528 run.py:483] Algo bellman_ford step 7491 current loss 0.089266, current_train_items 239744.
I0304 19:31:42.157202 23118544486528 run.py:483] Algo bellman_ford step 7492 current loss 0.013875, current_train_items 239776.
I0304 19:31:42.189197 23118544486528 run.py:483] Algo bellman_ford step 7493 current loss 0.025485, current_train_items 239808.
I0304 19:31:42.222596 23118544486528 run.py:483] Algo bellman_ford step 7494 current loss 0.075695, current_train_items 239840.
I0304 19:31:42.242177 23118544486528 run.py:483] Algo bellman_ford step 7495 current loss 0.006544, current_train_items 239872.
I0304 19:31:42.258780 23118544486528 run.py:483] Algo bellman_ford step 7496 current loss 0.015521, current_train_items 239904.
I0304 19:31:42.283323 23118544486528 run.py:483] Algo bellman_ford step 7497 current loss 0.057419, current_train_items 239936.
I0304 19:31:42.314991 23118544486528 run.py:483] Algo bellman_ford step 7498 current loss 0.042363, current_train_items 239968.
I0304 19:31:42.348477 23118544486528 run.py:483] Algo bellman_ford step 7499 current loss 0.051336, current_train_items 240000.
I0304 19:31:42.368762 23118544486528 run.py:483] Algo bellman_ford step 7500 current loss 0.016137, current_train_items 240032.
I0304 19:31:42.376553 23118544486528 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0304 19:31:42.376661 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:42.393745 23118544486528 run.py:483] Algo bellman_ford step 7501 current loss 0.007964, current_train_items 240064.
I0304 19:31:42.418361 23118544486528 run.py:483] Algo bellman_ford step 7502 current loss 0.089009, current_train_items 240096.
I0304 19:31:42.450877 23118544486528 run.py:483] Algo bellman_ford step 7503 current loss 0.041266, current_train_items 240128.
I0304 19:31:42.485140 23118544486528 run.py:483] Algo bellman_ford step 7504 current loss 0.061286, current_train_items 240160.
I0304 19:31:42.505273 23118544486528 run.py:483] Algo bellman_ford step 7505 current loss 0.035888, current_train_items 240192.
I0304 19:31:42.521424 23118544486528 run.py:483] Algo bellman_ford step 7506 current loss 0.009079, current_train_items 240224.
I0304 19:31:42.545296 23118544486528 run.py:483] Algo bellman_ford step 7507 current loss 0.048703, current_train_items 240256.
I0304 19:31:42.577421 23118544486528 run.py:483] Algo bellman_ford step 7508 current loss 0.023896, current_train_items 240288.
I0304 19:31:42.610855 23118544486528 run.py:483] Algo bellman_ford step 7509 current loss 0.039010, current_train_items 240320.
I0304 19:31:42.630677 23118544486528 run.py:483] Algo bellman_ford step 7510 current loss 0.005899, current_train_items 240352.
I0304 19:31:42.647618 23118544486528 run.py:483] Algo bellman_ford step 7511 current loss 0.028796, current_train_items 240384.
I0304 19:31:42.671005 23118544486528 run.py:483] Algo bellman_ford step 7512 current loss 0.034275, current_train_items 240416.
I0304 19:31:42.702648 23118544486528 run.py:483] Algo bellman_ford step 7513 current loss 0.039284, current_train_items 240448.
I0304 19:31:42.735564 23118544486528 run.py:483] Algo bellman_ford step 7514 current loss 0.056224, current_train_items 240480.
I0304 19:31:42.755670 23118544486528 run.py:483] Algo bellman_ford step 7515 current loss 0.005873, current_train_items 240512.
I0304 19:31:42.771446 23118544486528 run.py:483] Algo bellman_ford step 7516 current loss 0.013653, current_train_items 240544.
I0304 19:31:42.796306 23118544486528 run.py:483] Algo bellman_ford step 7517 current loss 0.070321, current_train_items 240576.
I0304 19:31:42.828989 23118544486528 run.py:483] Algo bellman_ford step 7518 current loss 0.059482, current_train_items 240608.
I0304 19:31:42.863289 23118544486528 run.py:483] Algo bellman_ford step 7519 current loss 0.055470, current_train_items 240640.
I0304 19:31:42.882928 23118544486528 run.py:483] Algo bellman_ford step 7520 current loss 0.005228, current_train_items 240672.
I0304 19:31:42.899223 23118544486528 run.py:483] Algo bellman_ford step 7521 current loss 0.014130, current_train_items 240704.
I0304 19:31:42.923587 23118544486528 run.py:483] Algo bellman_ford step 7522 current loss 0.036761, current_train_items 240736.
I0304 19:31:42.955883 23118544486528 run.py:483] Algo bellman_ford step 7523 current loss 0.041064, current_train_items 240768.
I0304 19:31:42.987222 23118544486528 run.py:483] Algo bellman_ford step 7524 current loss 0.026939, current_train_items 240800.
I0304 19:31:43.007116 23118544486528 run.py:483] Algo bellman_ford step 7525 current loss 0.018683, current_train_items 240832.
I0304 19:31:43.023410 23118544486528 run.py:483] Algo bellman_ford step 7526 current loss 0.031864, current_train_items 240864.
I0304 19:31:43.049177 23118544486528 run.py:483] Algo bellman_ford step 7527 current loss 0.060227, current_train_items 240896.
I0304 19:31:43.080518 23118544486528 run.py:483] Algo bellman_ford step 7528 current loss 0.035070, current_train_items 240928.
I0304 19:31:43.114403 23118544486528 run.py:483] Algo bellman_ford step 7529 current loss 0.069414, current_train_items 240960.
I0304 19:31:43.134569 23118544486528 run.py:483] Algo bellman_ford step 7530 current loss 0.004439, current_train_items 240992.
I0304 19:31:43.150839 23118544486528 run.py:483] Algo bellman_ford step 7531 current loss 0.007933, current_train_items 241024.
I0304 19:31:43.174857 23118544486528 run.py:483] Algo bellman_ford step 7532 current loss 0.042561, current_train_items 241056.
I0304 19:31:43.208107 23118544486528 run.py:483] Algo bellman_ford step 7533 current loss 0.059785, current_train_items 241088.
I0304 19:31:43.243175 23118544486528 run.py:483] Algo bellman_ford step 7534 current loss 0.063583, current_train_items 241120.
I0304 19:31:43.262863 23118544486528 run.py:483] Algo bellman_ford step 7535 current loss 0.008424, current_train_items 241152.
I0304 19:31:43.279337 23118544486528 run.py:483] Algo bellman_ford step 7536 current loss 0.014088, current_train_items 241184.
I0304 19:31:43.303555 23118544486528 run.py:483] Algo bellman_ford step 7537 current loss 0.063886, current_train_items 241216.
I0304 19:31:43.336262 23118544486528 run.py:483] Algo bellman_ford step 7538 current loss 0.066692, current_train_items 241248.
I0304 19:31:43.369987 23118544486528 run.py:483] Algo bellman_ford step 7539 current loss 0.036795, current_train_items 241280.
I0304 19:31:43.389932 23118544486528 run.py:483] Algo bellman_ford step 7540 current loss 0.006820, current_train_items 241312.
I0304 19:31:43.406749 23118544486528 run.py:483] Algo bellman_ford step 7541 current loss 0.021969, current_train_items 241344.
I0304 19:31:43.432247 23118544486528 run.py:483] Algo bellman_ford step 7542 current loss 0.122643, current_train_items 241376.
I0304 19:31:43.465857 23118544486528 run.py:483] Algo bellman_ford step 7543 current loss 0.113498, current_train_items 241408.
I0304 19:31:43.499920 23118544486528 run.py:483] Algo bellman_ford step 7544 current loss 0.074735, current_train_items 241440.
I0304 19:31:43.519352 23118544486528 run.py:483] Algo bellman_ford step 7545 current loss 0.003646, current_train_items 241472.
I0304 19:31:43.535636 23118544486528 run.py:483] Algo bellman_ford step 7546 current loss 0.013119, current_train_items 241504.
I0304 19:31:43.560662 23118544486528 run.py:483] Algo bellman_ford step 7547 current loss 0.045781, current_train_items 241536.
I0304 19:31:43.593588 23118544486528 run.py:483] Algo bellman_ford step 7548 current loss 0.052885, current_train_items 241568.
I0304 19:31:43.627762 23118544486528 run.py:483] Algo bellman_ford step 7549 current loss 0.050865, current_train_items 241600.
I0304 19:31:43.647599 23118544486528 run.py:483] Algo bellman_ford step 7550 current loss 0.017009, current_train_items 241632.
I0304 19:31:43.656014 23118544486528 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0304 19:31:43.656119 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0304 19:31:43.673572 23118544486528 run.py:483] Algo bellman_ford step 7551 current loss 0.044396, current_train_items 241664.
I0304 19:31:43.698843 23118544486528 run.py:483] Algo bellman_ford step 7552 current loss 0.017536, current_train_items 241696.
I0304 19:31:43.731696 23118544486528 run.py:483] Algo bellman_ford step 7553 current loss 0.049403, current_train_items 241728.
I0304 19:31:43.766034 23118544486528 run.py:483] Algo bellman_ford step 7554 current loss 0.080909, current_train_items 241760.
I0304 19:31:43.786060 23118544486528 run.py:483] Algo bellman_ford step 7555 current loss 0.005924, current_train_items 241792.
I0304 19:31:43.802503 23118544486528 run.py:483] Algo bellman_ford step 7556 current loss 0.050395, current_train_items 241824.
I0304 19:31:43.827254 23118544486528 run.py:483] Algo bellman_ford step 7557 current loss 0.028535, current_train_items 241856.
I0304 19:31:43.860860 23118544486528 run.py:483] Algo bellman_ford step 7558 current loss 0.040758, current_train_items 241888.
I0304 19:31:43.894048 23118544486528 run.py:483] Algo bellman_ford step 7559 current loss 0.043951, current_train_items 241920.
I0304 19:31:43.914107 23118544486528 run.py:483] Algo bellman_ford step 7560 current loss 0.003818, current_train_items 241952.
I0304 19:31:43.931232 23118544486528 run.py:483] Algo bellman_ford step 7561 current loss 0.062300, current_train_items 241984.
I0304 19:31:43.955069 23118544486528 run.py:483] Algo bellman_ford step 7562 current loss 0.043900, current_train_items 242016.
I0304 19:31:43.987970 23118544486528 run.py:483] Algo bellman_ford step 7563 current loss 0.042446, current_train_items 242048.
I0304 19:31:44.022048 23118544486528 run.py:483] Algo bellman_ford step 7564 current loss 0.051479, current_train_items 242080.
I0304 19:31:44.041661 23118544486528 run.py:483] Algo bellman_ford step 7565 current loss 0.005424, current_train_items 242112.
I0304 19:31:44.057872 23118544486528 run.py:483] Algo bellman_ford step 7566 current loss 0.020852, current_train_items 242144.
I0304 19:31:44.082867 23118544486528 run.py:483] Algo bellman_ford step 7567 current loss 0.023692, current_train_items 242176.
I0304 19:31:44.114749 23118544486528 run.py:483] Algo bellman_ford step 7568 current loss 0.055477, current_train_items 242208.
I0304 19:31:44.148501 23118544486528 run.py:483] Algo bellman_ford step 7569 current loss 0.091499, current_train_items 242240.
I0304 19:31:44.168664 23118544486528 run.py:483] Algo bellman_ford step 7570 current loss 0.008517, current_train_items 242272.
I0304 19:31:44.185264 23118544486528 run.py:483] Algo bellman_ford step 7571 current loss 0.019906, current_train_items 242304.
I0304 19:31:44.208917 23118544486528 run.py:483] Algo bellman_ford step 7572 current loss 0.025735, current_train_items 242336.
I0304 19:31:44.240235 23118544486528 run.py:483] Algo bellman_ford step 7573 current loss 0.044821, current_train_items 242368.
I0304 19:31:44.271866 23118544486528 run.py:483] Algo bellman_ford step 7574 current loss 0.031178, current_train_items 242400.
I0304 19:31:44.292109 23118544486528 run.py:483] Algo bellman_ford step 7575 current loss 0.003170, current_train_items 242432.
I0304 19:31:44.309257 23118544486528 run.py:483] Algo bellman_ford step 7576 current loss 0.030549, current_train_items 242464.
I0304 19:31:44.332749 23118544486528 run.py:483] Algo bellman_ford step 7577 current loss 0.080123, current_train_items 242496.
I0304 19:31:44.365503 23118544486528 run.py:483] Algo bellman_ford step 7578 current loss 0.082285, current_train_items 242528.
I0304 19:31:44.401108 23118544486528 run.py:483] Algo bellman_ford step 7579 current loss 0.101926, current_train_items 242560.
I0304 19:31:44.420885 23118544486528 run.py:483] Algo bellman_ford step 7580 current loss 0.005630, current_train_items 242592.
I0304 19:31:44.437442 23118544486528 run.py:483] Algo bellman_ford step 7581 current loss 0.049421, current_train_items 242624.
I0304 19:31:44.462444 23118544486528 run.py:483] Algo bellman_ford step 7582 current loss 0.054686, current_train_items 242656.
I0304 19:31:44.494499 23118544486528 run.py:483] Algo bellman_ford step 7583 current loss 0.093448, current_train_items 242688.
I0304 19:31:44.527631 23118544486528 run.py:483] Algo bellman_ford step 7584 current loss 0.060917, current_train_items 242720.
I0304 19:31:44.547791 23118544486528 run.py:483] Algo bellman_ford step 7585 current loss 0.002474, current_train_items 242752.
I0304 19:31:44.563909 23118544486528 run.py:483] Algo bellman_ford step 7586 current loss 0.040363, current_train_items 242784.
I0304 19:31:44.588470 23118544486528 run.py:483] Algo bellman_ford step 7587 current loss 0.074548, current_train_items 242816.
I0304 19:31:44.618710 23118544486528 run.py:483] Algo bellman_ford step 7588 current loss 0.078344, current_train_items 242848.
I0304 19:31:44.653255 23118544486528 run.py:483] Algo bellman_ford step 7589 current loss 0.129078, current_train_items 242880.
I0304 19:31:44.673013 23118544486528 run.py:483] Algo bellman_ford step 7590 current loss 0.024090, current_train_items 242912.
I0304 19:31:44.689209 23118544486528 run.py:483] Algo bellman_ford step 7591 current loss 0.010387, current_train_items 242944.
I0304 19:31:44.714295 23118544486528 run.py:483] Algo bellman_ford step 7592 current loss 0.043780, current_train_items 242976.
I0304 19:31:44.744741 23118544486528 run.py:483] Algo bellman_ford step 7593 current loss 0.019404, current_train_items 243008.
I0304 19:31:44.779218 23118544486528 run.py:483] Algo bellman_ford step 7594 current loss 0.103968, current_train_items 243040.
I0304 19:31:44.798972 23118544486528 run.py:483] Algo bellman_ford step 7595 current loss 0.012007, current_train_items 243072.
I0304 19:31:44.815252 23118544486528 run.py:483] Algo bellman_ford step 7596 current loss 0.010910, current_train_items 243104.
I0304 19:31:44.840176 23118544486528 run.py:483] Algo bellman_ford step 7597 current loss 0.046373, current_train_items 243136.
I0304 19:31:44.872857 23118544486528 run.py:483] Algo bellman_ford step 7598 current loss 0.056328, current_train_items 243168.
I0304 19:31:44.904632 23118544486528 run.py:483] Algo bellman_ford step 7599 current loss 0.043283, current_train_items 243200.
I0304 19:31:44.924460 23118544486528 run.py:483] Algo bellman_ford step 7600 current loss 0.022175, current_train_items 243232.
I0304 19:31:44.932153 23118544486528 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0304 19:31:44.932263 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0304 19:31:44.949268 23118544486528 run.py:483] Algo bellman_ford step 7601 current loss 0.034575, current_train_items 243264.
I0304 19:31:44.975649 23118544486528 run.py:483] Algo bellman_ford step 7602 current loss 0.140457, current_train_items 243296.
I0304 19:31:45.008292 23118544486528 run.py:483] Algo bellman_ford step 7603 current loss 0.050964, current_train_items 243328.
I0304 19:31:45.043577 23118544486528 run.py:483] Algo bellman_ford step 7604 current loss 0.117289, current_train_items 243360.
I0304 19:31:45.063414 23118544486528 run.py:483] Algo bellman_ford step 7605 current loss 0.043929, current_train_items 243392.
I0304 19:31:45.078833 23118544486528 run.py:483] Algo bellman_ford step 7606 current loss 0.012407, current_train_items 243424.
I0304 19:31:45.102852 23118544486528 run.py:483] Algo bellman_ford step 7607 current loss 0.030089, current_train_items 243456.
I0304 19:31:45.135427 23118544486528 run.py:483] Algo bellman_ford step 7608 current loss 0.052773, current_train_items 243488.
I0304 19:31:45.170506 23118544486528 run.py:483] Algo bellman_ford step 7609 current loss 0.086954, current_train_items 243520.
I0304 19:31:45.190701 23118544486528 run.py:483] Algo bellman_ford step 7610 current loss 0.004374, current_train_items 243552.
I0304 19:31:45.207590 23118544486528 run.py:483] Algo bellman_ford step 7611 current loss 0.010804, current_train_items 243584.
I0304 19:31:45.231876 23118544486528 run.py:483] Algo bellman_ford step 7612 current loss 0.030942, current_train_items 243616.
I0304 19:31:45.261756 23118544486528 run.py:483] Algo bellman_ford step 7613 current loss 0.084623, current_train_items 243648.
I0304 19:31:45.295264 23118544486528 run.py:483] Algo bellman_ford step 7614 current loss 0.112299, current_train_items 243680.
I0304 19:31:45.314788 23118544486528 run.py:483] Algo bellman_ford step 7615 current loss 0.003576, current_train_items 243712.
I0304 19:31:45.331307 23118544486528 run.py:483] Algo bellman_ford step 7616 current loss 0.008206, current_train_items 243744.
I0304 19:31:45.355434 23118544486528 run.py:483] Algo bellman_ford step 7617 current loss 0.036056, current_train_items 243776.
I0304 19:31:45.386718 23118544486528 run.py:483] Algo bellman_ford step 7618 current loss 0.053783, current_train_items 243808.
I0304 19:31:45.421118 23118544486528 run.py:483] Algo bellman_ford step 7619 current loss 0.094224, current_train_items 243840.
I0304 19:31:45.441055 23118544486528 run.py:483] Algo bellman_ford step 7620 current loss 0.008540, current_train_items 243872.
I0304 19:31:45.457746 23118544486528 run.py:483] Algo bellman_ford step 7621 current loss 0.011257, current_train_items 243904.
I0304 19:31:45.482278 23118544486528 run.py:483] Algo bellman_ford step 7622 current loss 0.079852, current_train_items 243936.
I0304 19:31:45.515451 23118544486528 run.py:483] Algo bellman_ford step 7623 current loss 0.116835, current_train_items 243968.
I0304 19:31:45.549119 23118544486528 run.py:483] Algo bellman_ford step 7624 current loss 0.089747, current_train_items 244000.
I0304 19:31:45.569047 23118544486528 run.py:483] Algo bellman_ford step 7625 current loss 0.004123, current_train_items 244032.
I0304 19:31:45.584709 23118544486528 run.py:483] Algo bellman_ford step 7626 current loss 0.023739, current_train_items 244064.
I0304 19:31:45.610148 23118544486528 run.py:483] Algo bellman_ford step 7627 current loss 0.058199, current_train_items 244096.
I0304 19:31:45.643081 23118544486528 run.py:483] Algo bellman_ford step 7628 current loss 0.050571, current_train_items 244128.
I0304 19:31:45.676701 23118544486528 run.py:483] Algo bellman_ford step 7629 current loss 0.047266, current_train_items 244160.
I0304 19:31:45.696313 23118544486528 run.py:483] Algo bellman_ford step 7630 current loss 0.032711, current_train_items 244192.
I0304 19:31:45.712956 23118544486528 run.py:483] Algo bellman_ford step 7631 current loss 0.020239, current_train_items 244224.
I0304 19:31:45.737010 23118544486528 run.py:483] Algo bellman_ford step 7632 current loss 0.036906, current_train_items 244256.
I0304 19:31:45.769877 23118544486528 run.py:483] Algo bellman_ford step 7633 current loss 0.026377, current_train_items 244288.
I0304 19:31:45.804237 23118544486528 run.py:483] Algo bellman_ford step 7634 current loss 0.084290, current_train_items 244320.
I0304 19:31:45.824057 23118544486528 run.py:483] Algo bellman_ford step 7635 current loss 0.015176, current_train_items 244352.
I0304 19:31:45.840405 23118544486528 run.py:483] Algo bellman_ford step 7636 current loss 0.021102, current_train_items 244384.
I0304 19:31:45.864928 23118544486528 run.py:483] Algo bellman_ford step 7637 current loss 0.030209, current_train_items 244416.
I0304 19:31:45.895711 23118544486528 run.py:483] Algo bellman_ford step 7638 current loss 0.041210, current_train_items 244448.
I0304 19:31:45.932388 23118544486528 run.py:483] Algo bellman_ford step 7639 current loss 0.056870, current_train_items 244480.
I0304 19:31:45.952377 23118544486528 run.py:483] Algo bellman_ford step 7640 current loss 0.004892, current_train_items 244512.
I0304 19:31:45.968602 23118544486528 run.py:483] Algo bellman_ford step 7641 current loss 0.018698, current_train_items 244544.
I0304 19:31:45.992123 23118544486528 run.py:483] Algo bellman_ford step 7642 current loss 0.026396, current_train_items 244576.
I0304 19:31:46.026634 23118544486528 run.py:483] Algo bellman_ford step 7643 current loss 0.060831, current_train_items 244608.
I0304 19:31:46.062217 23118544486528 run.py:483] Algo bellman_ford step 7644 current loss 0.059890, current_train_items 244640.
I0304 19:31:46.081867 23118544486528 run.py:483] Algo bellman_ford step 7645 current loss 0.008031, current_train_items 244672.
I0304 19:31:46.098381 23118544486528 run.py:483] Algo bellman_ford step 7646 current loss 0.016999, current_train_items 244704.
I0304 19:31:46.123713 23118544486528 run.py:483] Algo bellman_ford step 7647 current loss 0.043161, current_train_items 244736.
I0304 19:31:46.157535 23118544486528 run.py:483] Algo bellman_ford step 7648 current loss 0.082281, current_train_items 244768.
I0304 19:31:46.193301 23118544486528 run.py:483] Algo bellman_ford step 7649 current loss 0.128650, current_train_items 244800.
I0304 19:31:46.213097 23118544486528 run.py:483] Algo bellman_ford step 7650 current loss 0.005661, current_train_items 244832.
I0304 19:31:46.220991 23118544486528 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0304 19:31:46.221097 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:31:46.237730 23118544486528 run.py:483] Algo bellman_ford step 7651 current loss 0.005036, current_train_items 244864.
I0304 19:31:46.263208 23118544486528 run.py:483] Algo bellman_ford step 7652 current loss 0.064134, current_train_items 244896.
I0304 19:31:46.295212 23118544486528 run.py:483] Algo bellman_ford step 7653 current loss 0.042601, current_train_items 244928.
I0304 19:31:46.329349 23118544486528 run.py:483] Algo bellman_ford step 7654 current loss 0.090401, current_train_items 244960.
I0304 19:31:46.349271 23118544486528 run.py:483] Algo bellman_ford step 7655 current loss 0.007541, current_train_items 244992.
I0304 19:31:46.365195 23118544486528 run.py:483] Algo bellman_ford step 7656 current loss 0.010996, current_train_items 245024.
I0304 19:31:46.389801 23118544486528 run.py:483] Algo bellman_ford step 7657 current loss 0.048377, current_train_items 245056.
I0304 19:31:46.422910 23118544486528 run.py:483] Algo bellman_ford step 7658 current loss 0.050600, current_train_items 245088.
I0304 19:31:46.458482 23118544486528 run.py:483] Algo bellman_ford step 7659 current loss 0.355236, current_train_items 245120.
I0304 19:31:46.478337 23118544486528 run.py:483] Algo bellman_ford step 7660 current loss 0.006790, current_train_items 245152.
I0304 19:31:46.494745 23118544486528 run.py:483] Algo bellman_ford step 7661 current loss 0.007677, current_train_items 245184.
I0304 19:31:46.518614 23118544486528 run.py:483] Algo bellman_ford step 7662 current loss 0.058142, current_train_items 245216.
I0304 19:31:46.550024 23118544486528 run.py:483] Algo bellman_ford step 7663 current loss 0.068482, current_train_items 245248.
I0304 19:31:46.584265 23118544486528 run.py:483] Algo bellman_ford step 7664 current loss 0.051094, current_train_items 245280.
I0304 19:31:46.603869 23118544486528 run.py:483] Algo bellman_ford step 7665 current loss 0.033825, current_train_items 245312.
I0304 19:31:46.620647 23118544486528 run.py:483] Algo bellman_ford step 7666 current loss 0.028792, current_train_items 245344.
I0304 19:31:46.644541 23118544486528 run.py:483] Algo bellman_ford step 7667 current loss 0.107283, current_train_items 245376.
I0304 19:31:46.678774 23118544486528 run.py:483] Algo bellman_ford step 7668 current loss 0.073454, current_train_items 245408.
I0304 19:31:46.713568 23118544486528 run.py:483] Algo bellman_ford step 7669 current loss 0.058461, current_train_items 245440.
I0304 19:31:46.733459 23118544486528 run.py:483] Algo bellman_ford step 7670 current loss 0.035925, current_train_items 245472.
I0304 19:31:46.750360 23118544486528 run.py:483] Algo bellman_ford step 7671 current loss 0.048800, current_train_items 245504.
I0304 19:31:46.774164 23118544486528 run.py:483] Algo bellman_ford step 7672 current loss 0.095970, current_train_items 245536.
I0304 19:31:46.806497 23118544486528 run.py:483] Algo bellman_ford step 7673 current loss 0.065742, current_train_items 245568.
I0304 19:31:46.841279 23118544486528 run.py:483] Algo bellman_ford step 7674 current loss 0.238617, current_train_items 245600.
I0304 19:31:46.861462 23118544486528 run.py:483] Algo bellman_ford step 7675 current loss 0.010130, current_train_items 245632.
I0304 19:31:46.878681 23118544486528 run.py:483] Algo bellman_ford step 7676 current loss 0.040629, current_train_items 245664.
I0304 19:31:46.903290 23118544486528 run.py:483] Algo bellman_ford step 7677 current loss 0.056355, current_train_items 245696.
I0304 19:31:46.934756 23118544486528 run.py:483] Algo bellman_ford step 7678 current loss 0.025926, current_train_items 245728.
I0304 19:31:46.970060 23118544486528 run.py:483] Algo bellman_ford step 7679 current loss 0.093828, current_train_items 245760.
I0304 19:31:46.989832 23118544486528 run.py:483] Algo bellman_ford step 7680 current loss 0.003806, current_train_items 245792.
I0304 19:31:47.006375 23118544486528 run.py:483] Algo bellman_ford step 7681 current loss 0.021871, current_train_items 245824.
I0304 19:31:47.030266 23118544486528 run.py:483] Algo bellman_ford step 7682 current loss 0.040070, current_train_items 245856.
I0304 19:31:47.062384 23118544486528 run.py:483] Algo bellman_ford step 7683 current loss 0.090034, current_train_items 245888.
I0304 19:31:47.097645 23118544486528 run.py:483] Algo bellman_ford step 7684 current loss 0.080919, current_train_items 245920.
I0304 19:31:47.117759 23118544486528 run.py:483] Algo bellman_ford step 7685 current loss 0.023906, current_train_items 245952.
I0304 19:31:47.134386 23118544486528 run.py:483] Algo bellman_ford step 7686 current loss 0.028539, current_train_items 245984.
I0304 19:31:47.157964 23118544486528 run.py:483] Algo bellman_ford step 7687 current loss 0.044094, current_train_items 246016.
I0304 19:31:47.191012 23118544486528 run.py:483] Algo bellman_ford step 7688 current loss 0.069522, current_train_items 246048.
I0304 19:31:47.225117 23118544486528 run.py:483] Algo bellman_ford step 7689 current loss 0.149909, current_train_items 246080.
I0304 19:31:47.244816 23118544486528 run.py:483] Algo bellman_ford step 7690 current loss 0.009687, current_train_items 246112.
I0304 19:31:47.261349 23118544486528 run.py:483] Algo bellman_ford step 7691 current loss 0.019804, current_train_items 246144.
I0304 19:31:47.284976 23118544486528 run.py:483] Algo bellman_ford step 7692 current loss 0.043629, current_train_items 246176.
I0304 19:31:47.318491 23118544486528 run.py:483] Algo bellman_ford step 7693 current loss 0.067026, current_train_items 246208.
I0304 19:31:47.352051 23118544486528 run.py:483] Algo bellman_ford step 7694 current loss 0.060165, current_train_items 246240.
I0304 19:31:47.371745 23118544486528 run.py:483] Algo bellman_ford step 7695 current loss 0.009998, current_train_items 246272.
I0304 19:31:47.388335 23118544486528 run.py:483] Algo bellman_ford step 7696 current loss 0.025731, current_train_items 246304.
I0304 19:31:47.413424 23118544486528 run.py:483] Algo bellman_ford step 7697 current loss 0.047646, current_train_items 246336.
I0304 19:31:47.446336 23118544486528 run.py:483] Algo bellman_ford step 7698 current loss 0.062204, current_train_items 246368.
I0304 19:31:47.482080 23118544486528 run.py:483] Algo bellman_ford step 7699 current loss 0.059699, current_train_items 246400.
I0304 19:31:47.502465 23118544486528 run.py:483] Algo bellman_ford step 7700 current loss 0.008855, current_train_items 246432.
I0304 19:31:47.510641 23118544486528 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0304 19:31:47.510758 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:47.528169 23118544486528 run.py:483] Algo bellman_ford step 7701 current loss 0.032886, current_train_items 246464.
I0304 19:31:47.553129 23118544486528 run.py:483] Algo bellman_ford step 7702 current loss 0.028565, current_train_items 246496.
I0304 19:31:47.584608 23118544486528 run.py:483] Algo bellman_ford step 7703 current loss 0.031219, current_train_items 246528.
I0304 19:31:47.618443 23118544486528 run.py:483] Algo bellman_ford step 7704 current loss 0.058646, current_train_items 246560.
I0304 19:31:47.638310 23118544486528 run.py:483] Algo bellman_ford step 7705 current loss 0.007025, current_train_items 246592.
I0304 19:31:47.654312 23118544486528 run.py:483] Algo bellman_ford step 7706 current loss 0.017803, current_train_items 246624.
I0304 19:31:47.679735 23118544486528 run.py:483] Algo bellman_ford step 7707 current loss 0.032882, current_train_items 246656.
I0304 19:31:47.710921 23118544486528 run.py:483] Algo bellman_ford step 7708 current loss 0.048902, current_train_items 246688.
I0304 19:31:47.743791 23118544486528 run.py:483] Algo bellman_ford step 7709 current loss 0.038613, current_train_items 246720.
I0304 19:31:47.763659 23118544486528 run.py:483] Algo bellman_ford step 7710 current loss 0.006282, current_train_items 246752.
I0304 19:31:47.780099 23118544486528 run.py:483] Algo bellman_ford step 7711 current loss 0.007495, current_train_items 246784.
I0304 19:31:47.804306 23118544486528 run.py:483] Algo bellman_ford step 7712 current loss 0.060996, current_train_items 246816.
I0304 19:31:47.835912 23118544486528 run.py:483] Algo bellman_ford step 7713 current loss 0.051693, current_train_items 246848.
I0304 19:31:47.869441 23118544486528 run.py:483] Algo bellman_ford step 7714 current loss 0.040310, current_train_items 246880.
I0304 19:31:47.889568 23118544486528 run.py:483] Algo bellman_ford step 7715 current loss 0.015914, current_train_items 246912.
I0304 19:31:47.905858 23118544486528 run.py:483] Algo bellman_ford step 7716 current loss 0.010800, current_train_items 246944.
I0304 19:31:47.930401 23118544486528 run.py:483] Algo bellman_ford step 7717 current loss 0.059241, current_train_items 246976.
I0304 19:31:47.962910 23118544486528 run.py:483] Algo bellman_ford step 7718 current loss 0.064561, current_train_items 247008.
I0304 19:31:47.995619 23118544486528 run.py:483] Algo bellman_ford step 7719 current loss 0.056109, current_train_items 247040.
I0304 19:31:48.015584 23118544486528 run.py:483] Algo bellman_ford step 7720 current loss 0.004887, current_train_items 247072.
I0304 19:31:48.031803 23118544486528 run.py:483] Algo bellman_ford step 7721 current loss 0.019859, current_train_items 247104.
I0304 19:31:48.056818 23118544486528 run.py:483] Algo bellman_ford step 7722 current loss 0.047519, current_train_items 247136.
I0304 19:31:48.090118 23118544486528 run.py:483] Algo bellman_ford step 7723 current loss 0.066628, current_train_items 247168.
I0304 19:31:48.125056 23118544486528 run.py:483] Algo bellman_ford step 7724 current loss 0.092722, current_train_items 247200.
I0304 19:31:48.145219 23118544486528 run.py:483] Algo bellman_ford step 7725 current loss 0.002924, current_train_items 247232.
I0304 19:31:48.161608 23118544486528 run.py:483] Algo bellman_ford step 7726 current loss 0.020674, current_train_items 247264.
I0304 19:31:48.184885 23118544486528 run.py:483] Algo bellman_ford step 7727 current loss 0.025641, current_train_items 247296.
I0304 19:31:48.216658 23118544486528 run.py:483] Algo bellman_ford step 7728 current loss 0.044049, current_train_items 247328.
I0304 19:31:48.249838 23118544486528 run.py:483] Algo bellman_ford step 7729 current loss 0.086964, current_train_items 247360.
I0304 19:31:48.269598 23118544486528 run.py:483] Algo bellman_ford step 7730 current loss 0.019561, current_train_items 247392.
I0304 19:31:48.285714 23118544486528 run.py:483] Algo bellman_ford step 7731 current loss 0.006529, current_train_items 247424.
I0304 19:31:48.309417 23118544486528 run.py:483] Algo bellman_ford step 7732 current loss 0.033883, current_train_items 247456.
I0304 19:31:48.341776 23118544486528 run.py:483] Algo bellman_ford step 7733 current loss 0.059463, current_train_items 247488.
I0304 19:31:48.374465 23118544486528 run.py:483] Algo bellman_ford step 7734 current loss 0.081538, current_train_items 247520.
I0304 19:31:48.394275 23118544486528 run.py:483] Algo bellman_ford step 7735 current loss 0.005957, current_train_items 247552.
I0304 19:31:48.410676 23118544486528 run.py:483] Algo bellman_ford step 7736 current loss 0.012869, current_train_items 247584.
I0304 19:31:48.434931 23118544486528 run.py:483] Algo bellman_ford step 7737 current loss 0.026101, current_train_items 247616.
I0304 19:31:48.466488 23118544486528 run.py:483] Algo bellman_ford step 7738 current loss 0.049055, current_train_items 247648.
I0304 19:31:48.500971 23118544486528 run.py:483] Algo bellman_ford step 7739 current loss 0.080967, current_train_items 247680.
I0304 19:31:48.520697 23118544486528 run.py:483] Algo bellman_ford step 7740 current loss 0.011427, current_train_items 247712.
I0304 19:31:48.537298 23118544486528 run.py:483] Algo bellman_ford step 7741 current loss 0.018151, current_train_items 247744.
I0304 19:31:48.563376 23118544486528 run.py:483] Algo bellman_ford step 7742 current loss 0.061965, current_train_items 247776.
I0304 19:31:48.596227 23118544486528 run.py:483] Algo bellman_ford step 7743 current loss 0.061888, current_train_items 247808.
I0304 19:31:48.630422 23118544486528 run.py:483] Algo bellman_ford step 7744 current loss 0.083259, current_train_items 247840.
I0304 19:31:48.650107 23118544486528 run.py:483] Algo bellman_ford step 7745 current loss 0.040699, current_train_items 247872.
I0304 19:31:48.666300 23118544486528 run.py:483] Algo bellman_ford step 7746 current loss 0.007581, current_train_items 247904.
I0304 19:31:48.690655 23118544486528 run.py:483] Algo bellman_ford step 7747 current loss 0.034806, current_train_items 247936.
I0304 19:31:48.724142 23118544486528 run.py:483] Algo bellman_ford step 7748 current loss 0.089970, current_train_items 247968.
I0304 19:31:48.759072 23118544486528 run.py:483] Algo bellman_ford step 7749 current loss 0.048205, current_train_items 248000.
I0304 19:31:48.779504 23118544486528 run.py:483] Algo bellman_ford step 7750 current loss 0.009785, current_train_items 248032.
I0304 19:31:48.787702 23118544486528 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0304 19:31:48.787808 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:31:48.805069 23118544486528 run.py:483] Algo bellman_ford step 7751 current loss 0.029153, current_train_items 248064.
I0304 19:31:48.829836 23118544486528 run.py:483] Algo bellman_ford step 7752 current loss 0.035440, current_train_items 248096.
I0304 19:31:48.862407 23118544486528 run.py:483] Algo bellman_ford step 7753 current loss 0.060218, current_train_items 248128.
I0304 19:31:48.898593 23118544486528 run.py:483] Algo bellman_ford step 7754 current loss 0.075527, current_train_items 248160.
I0304 19:31:48.918729 23118544486528 run.py:483] Algo bellman_ford step 7755 current loss 0.003809, current_train_items 248192.
I0304 19:31:48.934391 23118544486528 run.py:483] Algo bellman_ford step 7756 current loss 0.019653, current_train_items 248224.
I0304 19:31:48.959036 23118544486528 run.py:483] Algo bellman_ford step 7757 current loss 0.027923, current_train_items 248256.
I0304 19:31:48.990849 23118544486528 run.py:483] Algo bellman_ford step 7758 current loss 0.053609, current_train_items 248288.
I0304 19:31:49.024874 23118544486528 run.py:483] Algo bellman_ford step 7759 current loss 0.052422, current_train_items 248320.
I0304 19:31:49.045401 23118544486528 run.py:483] Algo bellman_ford step 7760 current loss 0.004218, current_train_items 248352.
I0304 19:31:49.061928 23118544486528 run.py:483] Algo bellman_ford step 7761 current loss 0.026171, current_train_items 248384.
I0304 19:31:49.086780 23118544486528 run.py:483] Algo bellman_ford step 7762 current loss 0.029751, current_train_items 248416.
I0304 19:31:49.119080 23118544486528 run.py:483] Algo bellman_ford step 7763 current loss 0.044499, current_train_items 248448.
I0304 19:31:49.152369 23118544486528 run.py:483] Algo bellman_ford step 7764 current loss 0.027999, current_train_items 248480.
I0304 19:31:49.172769 23118544486528 run.py:483] Algo bellman_ford step 7765 current loss 0.003699, current_train_items 248512.
I0304 19:31:49.189229 23118544486528 run.py:483] Algo bellman_ford step 7766 current loss 0.017617, current_train_items 248544.
I0304 19:31:49.213314 23118544486528 run.py:483] Algo bellman_ford step 7767 current loss 0.019958, current_train_items 248576.
I0304 19:31:49.245771 23118544486528 run.py:483] Algo bellman_ford step 7768 current loss 0.034974, current_train_items 248608.
I0304 19:31:49.279982 23118544486528 run.py:483] Algo bellman_ford step 7769 current loss 0.058601, current_train_items 248640.
I0304 19:31:49.299887 23118544486528 run.py:483] Algo bellman_ford step 7770 current loss 0.002525, current_train_items 248672.
I0304 19:31:49.316623 23118544486528 run.py:483] Algo bellman_ford step 7771 current loss 0.020526, current_train_items 248704.
I0304 19:31:49.340749 23118544486528 run.py:483] Algo bellman_ford step 7772 current loss 0.031612, current_train_items 248736.
I0304 19:31:49.373540 23118544486528 run.py:483] Algo bellman_ford step 7773 current loss 0.022410, current_train_items 248768.
I0304 19:31:49.407571 23118544486528 run.py:483] Algo bellman_ford step 7774 current loss 0.047922, current_train_items 248800.
I0304 19:31:49.428200 23118544486528 run.py:483] Algo bellman_ford step 7775 current loss 0.003485, current_train_items 248832.
I0304 19:31:49.444888 23118544486528 run.py:483] Algo bellman_ford step 7776 current loss 0.015426, current_train_items 248864.
I0304 19:31:49.470187 23118544486528 run.py:483] Algo bellman_ford step 7777 current loss 0.023567, current_train_items 248896.
I0304 19:31:49.502763 23118544486528 run.py:483] Algo bellman_ford step 7778 current loss 0.067547, current_train_items 248928.
I0304 19:31:49.538897 23118544486528 run.py:483] Algo bellman_ford step 7779 current loss 0.098867, current_train_items 248960.
I0304 19:31:49.558442 23118544486528 run.py:483] Algo bellman_ford step 7780 current loss 0.001808, current_train_items 248992.
I0304 19:31:49.575293 23118544486528 run.py:483] Algo bellman_ford step 7781 current loss 0.010569, current_train_items 249024.
I0304 19:31:49.600125 23118544486528 run.py:483] Algo bellman_ford step 7782 current loss 0.032670, current_train_items 249056.
I0304 19:31:49.633201 23118544486528 run.py:483] Algo bellman_ford step 7783 current loss 0.053095, current_train_items 249088.
I0304 19:31:49.668411 23118544486528 run.py:483] Algo bellman_ford step 7784 current loss 0.037902, current_train_items 249120.
I0304 19:31:49.688753 23118544486528 run.py:483] Algo bellman_ford step 7785 current loss 0.005493, current_train_items 249152.
I0304 19:31:49.705260 23118544486528 run.py:483] Algo bellman_ford step 7786 current loss 0.016250, current_train_items 249184.
I0304 19:31:49.729117 23118544486528 run.py:483] Algo bellman_ford step 7787 current loss 0.035910, current_train_items 249216.
I0304 19:31:49.761242 23118544486528 run.py:483] Algo bellman_ford step 7788 current loss 0.027569, current_train_items 249248.
I0304 19:31:49.793721 23118544486528 run.py:483] Algo bellman_ford step 7789 current loss 0.054420, current_train_items 249280.
I0304 19:31:49.813977 23118544486528 run.py:483] Algo bellman_ford step 7790 current loss 0.003024, current_train_items 249312.
I0304 19:31:49.830016 23118544486528 run.py:483] Algo bellman_ford step 7791 current loss 0.016922, current_train_items 249344.
I0304 19:31:49.854622 23118544486528 run.py:483] Algo bellman_ford step 7792 current loss 0.042808, current_train_items 249376.
I0304 19:31:49.886314 23118544486528 run.py:483] Algo bellman_ford step 7793 current loss 0.035322, current_train_items 249408.
I0304 19:31:49.920582 23118544486528 run.py:483] Algo bellman_ford step 7794 current loss 0.032383, current_train_items 249440.
I0304 19:31:49.940607 23118544486528 run.py:483] Algo bellman_ford step 7795 current loss 0.027149, current_train_items 249472.
I0304 19:31:49.956693 23118544486528 run.py:483] Algo bellman_ford step 7796 current loss 0.012969, current_train_items 249504.
I0304 19:31:49.980350 23118544486528 run.py:483] Algo bellman_ford step 7797 current loss 0.032237, current_train_items 249536.
I0304 19:31:50.012212 23118544486528 run.py:483] Algo bellman_ford step 7798 current loss 0.029533, current_train_items 249568.
I0304 19:31:50.047600 23118544486528 run.py:483] Algo bellman_ford step 7799 current loss 0.081965, current_train_items 249600.
I0304 19:31:50.067795 23118544486528 run.py:483] Algo bellman_ford step 7800 current loss 0.005752, current_train_items 249632.
I0304 19:31:50.075562 23118544486528 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0304 19:31:50.075667 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:50.092664 23118544486528 run.py:483] Algo bellman_ford step 7801 current loss 0.038408, current_train_items 249664.
I0304 19:31:50.117357 23118544486528 run.py:483] Algo bellman_ford step 7802 current loss 0.041848, current_train_items 249696.
I0304 19:31:50.151283 23118544486528 run.py:483] Algo bellman_ford step 7803 current loss 0.064837, current_train_items 249728.
I0304 19:31:50.187730 23118544486528 run.py:483] Algo bellman_ford step 7804 current loss 0.087861, current_train_items 249760.
I0304 19:31:50.208395 23118544486528 run.py:483] Algo bellman_ford step 7805 current loss 0.006894, current_train_items 249792.
I0304 19:31:50.224758 23118544486528 run.py:483] Algo bellman_ford step 7806 current loss 0.006318, current_train_items 249824.
I0304 19:31:50.248501 23118544486528 run.py:483] Algo bellman_ford step 7807 current loss 0.020695, current_train_items 249856.
I0304 19:31:50.279672 23118544486528 run.py:483] Algo bellman_ford step 7808 current loss 0.056938, current_train_items 249888.
I0304 19:31:50.314268 23118544486528 run.py:483] Algo bellman_ford step 7809 current loss 0.213370, current_train_items 249920.
I0304 19:31:50.334315 23118544486528 run.py:483] Algo bellman_ford step 7810 current loss 0.004495, current_train_items 249952.
I0304 19:31:50.351473 23118544486528 run.py:483] Algo bellman_ford step 7811 current loss 0.010569, current_train_items 249984.
I0304 19:31:50.376145 23118544486528 run.py:483] Algo bellman_ford step 7812 current loss 0.075431, current_train_items 250016.
I0304 19:31:50.409469 23118544486528 run.py:483] Algo bellman_ford step 7813 current loss 0.077673, current_train_items 250048.
I0304 19:31:50.444083 23118544486528 run.py:483] Algo bellman_ford step 7814 current loss 0.073813, current_train_items 250080.
I0304 19:31:50.463667 23118544486528 run.py:483] Algo bellman_ford step 7815 current loss 0.006305, current_train_items 250112.
I0304 19:31:50.479921 23118544486528 run.py:483] Algo bellman_ford step 7816 current loss 0.076458, current_train_items 250144.
I0304 19:31:50.504967 23118544486528 run.py:483] Algo bellman_ford step 7817 current loss 0.047278, current_train_items 250176.
I0304 19:31:50.536040 23118544486528 run.py:483] Algo bellman_ford step 7818 current loss 0.080106, current_train_items 250208.
I0304 19:31:50.569301 23118544486528 run.py:483] Algo bellman_ford step 7819 current loss 0.045574, current_train_items 250240.
I0304 19:31:50.588912 23118544486528 run.py:483] Algo bellman_ford step 7820 current loss 0.005559, current_train_items 250272.
I0304 19:31:50.605642 23118544486528 run.py:483] Algo bellman_ford step 7821 current loss 0.027842, current_train_items 250304.
I0304 19:31:50.631453 23118544486528 run.py:483] Algo bellman_ford step 7822 current loss 0.047150, current_train_items 250336.
I0304 19:31:50.665048 23118544486528 run.py:483] Algo bellman_ford step 7823 current loss 0.257871, current_train_items 250368.
I0304 19:31:50.698634 23118544486528 run.py:483] Algo bellman_ford step 7824 current loss 0.261078, current_train_items 250400.
I0304 19:31:50.718481 23118544486528 run.py:483] Algo bellman_ford step 7825 current loss 0.006344, current_train_items 250432.
I0304 19:31:50.735367 23118544486528 run.py:483] Algo bellman_ford step 7826 current loss 0.069153, current_train_items 250464.
I0304 19:31:50.760280 23118544486528 run.py:483] Algo bellman_ford step 7827 current loss 0.029014, current_train_items 250496.
I0304 19:31:50.792921 23118544486528 run.py:483] Algo bellman_ford step 7828 current loss 0.082492, current_train_items 250528.
I0304 19:31:50.827257 23118544486528 run.py:483] Algo bellman_ford step 7829 current loss 0.095788, current_train_items 250560.
I0304 19:31:50.847367 23118544486528 run.py:483] Algo bellman_ford step 7830 current loss 0.005647, current_train_items 250592.
I0304 19:31:50.863582 23118544486528 run.py:483] Algo bellman_ford step 7831 current loss 0.037310, current_train_items 250624.
I0304 19:31:50.887961 23118544486528 run.py:483] Algo bellman_ford step 7832 current loss 0.038752, current_train_items 250656.
I0304 19:31:50.921538 23118544486528 run.py:483] Algo bellman_ford step 7833 current loss 0.050421, current_train_items 250688.
I0304 19:31:50.954006 23118544486528 run.py:483] Algo bellman_ford step 7834 current loss 0.045784, current_train_items 250720.
I0304 19:31:50.973806 23118544486528 run.py:483] Algo bellman_ford step 7835 current loss 0.016221, current_train_items 250752.
I0304 19:31:50.990442 23118544486528 run.py:483] Algo bellman_ford step 7836 current loss 0.014875, current_train_items 250784.
I0304 19:31:51.015425 23118544486528 run.py:483] Algo bellman_ford step 7837 current loss 0.069610, current_train_items 250816.
I0304 19:31:51.049042 23118544486528 run.py:483] Algo bellman_ford step 7838 current loss 0.042182, current_train_items 250848.
I0304 19:31:51.084772 23118544486528 run.py:483] Algo bellman_ford step 7839 current loss 0.133530, current_train_items 250880.
I0304 19:31:51.104539 23118544486528 run.py:483] Algo bellman_ford step 7840 current loss 0.011657, current_train_items 250912.
I0304 19:31:51.121047 23118544486528 run.py:483] Algo bellman_ford step 7841 current loss 0.018758, current_train_items 250944.
I0304 19:31:51.145496 23118544486528 run.py:483] Algo bellman_ford step 7842 current loss 0.021204, current_train_items 250976.
I0304 19:31:51.179097 23118544486528 run.py:483] Algo bellman_ford step 7843 current loss 0.050456, current_train_items 251008.
I0304 19:31:51.214521 23118544486528 run.py:483] Algo bellman_ford step 7844 current loss 0.081076, current_train_items 251040.
I0304 19:31:51.234523 23118544486528 run.py:483] Algo bellman_ford step 7845 current loss 0.004025, current_train_items 251072.
I0304 19:31:51.250949 23118544486528 run.py:483] Algo bellman_ford step 7846 current loss 0.007171, current_train_items 251104.
I0304 19:31:51.275623 23118544486528 run.py:483] Algo bellman_ford step 7847 current loss 0.044729, current_train_items 251136.
I0304 19:31:51.308381 23118544486528 run.py:483] Algo bellman_ford step 7848 current loss 0.041284, current_train_items 251168.
I0304 19:31:51.343424 23118544486528 run.py:483] Algo bellman_ford step 7849 current loss 0.090621, current_train_items 251200.
I0304 19:31:51.363259 23118544486528 run.py:483] Algo bellman_ford step 7850 current loss 0.004814, current_train_items 251232.
I0304 19:31:51.371270 23118544486528 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0304 19:31:51.371375 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:31:51.388456 23118544486528 run.py:483] Algo bellman_ford step 7851 current loss 0.032904, current_train_items 251264.
I0304 19:31:51.413130 23118544486528 run.py:483] Algo bellman_ford step 7852 current loss 0.020831, current_train_items 251296.
I0304 19:31:51.446820 23118544486528 run.py:483] Algo bellman_ford step 7853 current loss 0.051782, current_train_items 251328.
I0304 19:31:51.483451 23118544486528 run.py:483] Algo bellman_ford step 7854 current loss 0.079133, current_train_items 251360.
I0304 19:31:51.503479 23118544486528 run.py:483] Algo bellman_ford step 7855 current loss 0.003270, current_train_items 251392.
I0304 19:31:51.520159 23118544486528 run.py:483] Algo bellman_ford step 7856 current loss 0.012176, current_train_items 251424.
I0304 19:31:51.545377 23118544486528 run.py:483] Algo bellman_ford step 7857 current loss 0.051906, current_train_items 251456.
I0304 19:31:51.577569 23118544486528 run.py:483] Algo bellman_ford step 7858 current loss 0.025915, current_train_items 251488.
I0304 19:31:51.612974 23118544486528 run.py:483] Algo bellman_ford step 7859 current loss 0.080861, current_train_items 251520.
I0304 19:31:51.633117 23118544486528 run.py:483] Algo bellman_ford step 7860 current loss 0.017245, current_train_items 251552.
I0304 19:31:51.649589 23118544486528 run.py:483] Algo bellman_ford step 7861 current loss 0.013314, current_train_items 251584.
I0304 19:31:51.674165 23118544486528 run.py:483] Algo bellman_ford step 7862 current loss 0.053453, current_train_items 251616.
I0304 19:31:51.706406 23118544486528 run.py:483] Algo bellman_ford step 7863 current loss 0.082691, current_train_items 251648.
I0304 19:31:51.739361 23118544486528 run.py:483] Algo bellman_ford step 7864 current loss 0.043483, current_train_items 251680.
I0304 19:31:51.759338 23118544486528 run.py:483] Algo bellman_ford step 7865 current loss 0.004572, current_train_items 251712.
I0304 19:31:51.776128 23118544486528 run.py:483] Algo bellman_ford step 7866 current loss 0.020654, current_train_items 251744.
I0304 19:31:51.801239 23118544486528 run.py:483] Algo bellman_ford step 7867 current loss 0.070978, current_train_items 251776.
I0304 19:31:51.833063 23118544486528 run.py:483] Algo bellman_ford step 7868 current loss 0.067864, current_train_items 251808.
I0304 19:31:51.866676 23118544486528 run.py:483] Algo bellman_ford step 7869 current loss 0.038064, current_train_items 251840.
I0304 19:31:51.887044 23118544486528 run.py:483] Algo bellman_ford step 7870 current loss 0.003275, current_train_items 251872.
I0304 19:31:51.903570 23118544486528 run.py:483] Algo bellman_ford step 7871 current loss 0.020036, current_train_items 251904.
I0304 19:31:51.928058 23118544486528 run.py:483] Algo bellman_ford step 7872 current loss 0.037635, current_train_items 251936.
I0304 19:31:51.959841 23118544486528 run.py:483] Algo bellman_ford step 7873 current loss 0.062068, current_train_items 251968.
I0304 19:31:51.993723 23118544486528 run.py:483] Algo bellman_ford step 7874 current loss 0.070528, current_train_items 252000.
I0304 19:31:52.013705 23118544486528 run.py:483] Algo bellman_ford step 7875 current loss 0.005939, current_train_items 252032.
I0304 19:31:52.029799 23118544486528 run.py:483] Algo bellman_ford step 7876 current loss 0.047569, current_train_items 252064.
I0304 19:31:52.053820 23118544486528 run.py:483] Algo bellman_ford step 7877 current loss 0.080581, current_train_items 252096.
I0304 19:31:52.086321 23118544486528 run.py:483] Algo bellman_ford step 7878 current loss 0.063891, current_train_items 252128.
I0304 19:31:52.120534 23118544486528 run.py:483] Algo bellman_ford step 7879 current loss 0.099961, current_train_items 252160.
I0304 19:31:52.140463 23118544486528 run.py:483] Algo bellman_ford step 7880 current loss 0.007285, current_train_items 252192.
I0304 19:31:52.156423 23118544486528 run.py:483] Algo bellman_ford step 7881 current loss 0.012234, current_train_items 252224.
I0304 19:31:52.181125 23118544486528 run.py:483] Algo bellman_ford step 7882 current loss 0.088705, current_train_items 252256.
I0304 19:31:52.213611 23118544486528 run.py:483] Algo bellman_ford step 7883 current loss 0.060408, current_train_items 252288.
I0304 19:31:52.248307 23118544486528 run.py:483] Algo bellman_ford step 7884 current loss 0.147739, current_train_items 252320.
I0304 19:31:52.268577 23118544486528 run.py:483] Algo bellman_ford step 7885 current loss 0.010515, current_train_items 252352.
I0304 19:31:52.284985 23118544486528 run.py:483] Algo bellman_ford step 7886 current loss 0.021864, current_train_items 252384.
I0304 19:31:52.308418 23118544486528 run.py:483] Algo bellman_ford step 7887 current loss 0.044881, current_train_items 252416.
I0304 19:31:52.338751 23118544486528 run.py:483] Algo bellman_ford step 7888 current loss 0.026695, current_train_items 252448.
I0304 19:31:52.372802 23118544486528 run.py:483] Algo bellman_ford step 7889 current loss 0.035971, current_train_items 252480.
I0304 19:31:52.392883 23118544486528 run.py:483] Algo bellman_ford step 7890 current loss 0.007934, current_train_items 252512.
I0304 19:31:52.409297 23118544486528 run.py:483] Algo bellman_ford step 7891 current loss 0.006373, current_train_items 252544.
I0304 19:31:52.433703 23118544486528 run.py:483] Algo bellman_ford step 7892 current loss 0.039402, current_train_items 252576.
I0304 19:31:52.466397 23118544486528 run.py:483] Algo bellman_ford step 7893 current loss 0.040629, current_train_items 252608.
I0304 19:31:52.500306 23118544486528 run.py:483] Algo bellman_ford step 7894 current loss 0.075601, current_train_items 252640.
I0304 19:31:52.520349 23118544486528 run.py:483] Algo bellman_ford step 7895 current loss 0.010820, current_train_items 252672.
I0304 19:31:52.536334 23118544486528 run.py:483] Algo bellman_ford step 7896 current loss 0.022500, current_train_items 252704.
I0304 19:31:52.561408 23118544486528 run.py:483] Algo bellman_ford step 7897 current loss 0.067298, current_train_items 252736.
I0304 19:31:52.594659 23118544486528 run.py:483] Algo bellman_ford step 7898 current loss 0.036229, current_train_items 252768.
I0304 19:31:52.629917 23118544486528 run.py:483] Algo bellman_ford step 7899 current loss 0.062286, current_train_items 252800.
I0304 19:31:52.649907 23118544486528 run.py:483] Algo bellman_ford step 7900 current loss 0.004008, current_train_items 252832.
I0304 19:31:52.657553 23118544486528 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0304 19:31:52.657657 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:31:52.674707 23118544486528 run.py:483] Algo bellman_ford step 7901 current loss 0.024703, current_train_items 252864.
I0304 19:31:52.699696 23118544486528 run.py:483] Algo bellman_ford step 7902 current loss 0.035237, current_train_items 252896.
I0304 19:31:52.732933 23118544486528 run.py:483] Algo bellman_ford step 7903 current loss 0.029729, current_train_items 252928.
I0304 19:31:52.770000 23118544486528 run.py:483] Algo bellman_ford step 7904 current loss 0.065088, current_train_items 252960.
I0304 19:31:52.789821 23118544486528 run.py:483] Algo bellman_ford step 7905 current loss 0.003386, current_train_items 252992.
I0304 19:31:52.806511 23118544486528 run.py:483] Algo bellman_ford step 7906 current loss 0.027323, current_train_items 253024.
I0304 19:31:52.831639 23118544486528 run.py:483] Algo bellman_ford step 7907 current loss 0.048151, current_train_items 253056.
I0304 19:31:52.863415 23118544486528 run.py:483] Algo bellman_ford step 7908 current loss 0.032514, current_train_items 253088.
I0304 19:31:52.895169 23118544486528 run.py:483] Algo bellman_ford step 7909 current loss 0.038877, current_train_items 253120.
I0304 19:31:52.915020 23118544486528 run.py:483] Algo bellman_ford step 7910 current loss 0.009733, current_train_items 253152.
I0304 19:31:52.931922 23118544486528 run.py:483] Algo bellman_ford step 7911 current loss 0.011989, current_train_items 253184.
I0304 19:31:52.956799 23118544486528 run.py:483] Algo bellman_ford step 7912 current loss 0.027995, current_train_items 253216.
I0304 19:31:52.989068 23118544486528 run.py:483] Algo bellman_ford step 7913 current loss 0.037121, current_train_items 253248.
I0304 19:31:53.023391 23118544486528 run.py:483] Algo bellman_ford step 7914 current loss 0.094855, current_train_items 253280.
I0304 19:31:53.043211 23118544486528 run.py:483] Algo bellman_ford step 7915 current loss 0.016773, current_train_items 253312.
I0304 19:31:53.059879 23118544486528 run.py:483] Algo bellman_ford step 7916 current loss 0.017637, current_train_items 253344.
I0304 19:31:53.084257 23118544486528 run.py:483] Algo bellman_ford step 7917 current loss 0.043147, current_train_items 253376.
I0304 19:31:53.116461 23118544486528 run.py:483] Algo bellman_ford step 7918 current loss 0.034054, current_train_items 253408.
I0304 19:31:53.152316 23118544486528 run.py:483] Algo bellman_ford step 7919 current loss 0.048751, current_train_items 253440.
I0304 19:31:53.171925 23118544486528 run.py:483] Algo bellman_ford step 7920 current loss 0.004555, current_train_items 253472.
I0304 19:31:53.188562 23118544486528 run.py:483] Algo bellman_ford step 7921 current loss 0.021427, current_train_items 253504.
I0304 19:31:53.213356 23118544486528 run.py:483] Algo bellman_ford step 7922 current loss 0.074274, current_train_items 253536.
I0304 19:31:53.244977 23118544486528 run.py:483] Algo bellman_ford step 7923 current loss 0.034939, current_train_items 253568.
I0304 19:31:53.279476 23118544486528 run.py:483] Algo bellman_ford step 7924 current loss 0.051988, current_train_items 253600.
I0304 19:31:53.298841 23118544486528 run.py:483] Algo bellman_ford step 7925 current loss 0.025480, current_train_items 253632.
I0304 19:31:53.315296 23118544486528 run.py:483] Algo bellman_ford step 7926 current loss 0.014134, current_train_items 253664.
I0304 19:31:53.339589 23118544486528 run.py:483] Algo bellman_ford step 7927 current loss 0.063704, current_train_items 253696.
I0304 19:31:53.372869 23118544486528 run.py:483] Algo bellman_ford step 7928 current loss 0.059656, current_train_items 253728.
I0304 19:31:53.406518 23118544486528 run.py:483] Algo bellman_ford step 7929 current loss 0.052678, current_train_items 253760.
I0304 19:31:53.426228 23118544486528 run.py:483] Algo bellman_ford step 7930 current loss 0.007169, current_train_items 253792.
I0304 19:31:53.442603 23118544486528 run.py:483] Algo bellman_ford step 7931 current loss 0.027980, current_train_items 253824.
I0304 19:31:53.466046 23118544486528 run.py:483] Algo bellman_ford step 7932 current loss 0.030945, current_train_items 253856.
I0304 19:31:53.498714 23118544486528 run.py:483] Algo bellman_ford step 7933 current loss 0.090105, current_train_items 253888.
I0304 19:31:53.533329 23118544486528 run.py:483] Algo bellman_ford step 7934 current loss 0.132644, current_train_items 253920.
I0304 19:31:53.552964 23118544486528 run.py:483] Algo bellman_ford step 7935 current loss 0.004341, current_train_items 253952.
I0304 19:31:53.569085 23118544486528 run.py:483] Algo bellman_ford step 7936 current loss 0.016180, current_train_items 253984.
I0304 19:31:53.594055 23118544486528 run.py:483] Algo bellman_ford step 7937 current loss 0.034867, current_train_items 254016.
I0304 19:31:53.625895 23118544486528 run.py:483] Algo bellman_ford step 7938 current loss 0.049237, current_train_items 254048.
I0304 19:31:53.660944 23118544486528 run.py:483] Algo bellman_ford step 7939 current loss 0.069579, current_train_items 254080.
I0304 19:31:53.680694 23118544486528 run.py:483] Algo bellman_ford step 7940 current loss 0.006241, current_train_items 254112.
I0304 19:31:53.696472 23118544486528 run.py:483] Algo bellman_ford step 7941 current loss 0.003893, current_train_items 254144.
I0304 19:31:53.721775 23118544486528 run.py:483] Algo bellman_ford step 7942 current loss 0.032447, current_train_items 254176.
I0304 19:31:53.755487 23118544486528 run.py:483] Algo bellman_ford step 7943 current loss 0.092251, current_train_items 254208.
I0304 19:31:53.788897 23118544486528 run.py:483] Algo bellman_ford step 7944 current loss 0.099572, current_train_items 254240.
I0304 19:31:53.808483 23118544486528 run.py:483] Algo bellman_ford step 7945 current loss 0.010699, current_train_items 254272.
I0304 19:31:53.824486 23118544486528 run.py:483] Algo bellman_ford step 7946 current loss 0.018455, current_train_items 254304.
I0304 19:31:53.847966 23118544486528 run.py:483] Algo bellman_ford step 7947 current loss 0.041979, current_train_items 254336.
I0304 19:31:53.878766 23118544486528 run.py:483] Algo bellman_ford step 7948 current loss 0.041029, current_train_items 254368.
I0304 19:31:53.913285 23118544486528 run.py:483] Algo bellman_ford step 7949 current loss 0.072572, current_train_items 254400.
I0304 19:31:53.933185 23118544486528 run.py:483] Algo bellman_ford step 7950 current loss 0.027192, current_train_items 254432.
I0304 19:31:53.941174 23118544486528 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0304 19:31:53.941279 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:31:53.958366 23118544486528 run.py:483] Algo bellman_ford step 7951 current loss 0.022994, current_train_items 254464.
I0304 19:31:53.983305 23118544486528 run.py:483] Algo bellman_ford step 7952 current loss 0.032483, current_train_items 254496.
I0304 19:31:54.016878 23118544486528 run.py:483] Algo bellman_ford step 7953 current loss 0.035453, current_train_items 254528.
I0304 19:31:54.051015 23118544486528 run.py:483] Algo bellman_ford step 7954 current loss 0.086547, current_train_items 254560.
I0304 19:31:54.071091 23118544486528 run.py:483] Algo bellman_ford step 7955 current loss 0.008106, current_train_items 254592.
I0304 19:31:54.087371 23118544486528 run.py:483] Algo bellman_ford step 7956 current loss 0.036524, current_train_items 254624.
I0304 19:31:54.111877 23118544486528 run.py:483] Algo bellman_ford step 7957 current loss 0.035096, current_train_items 254656.
I0304 19:31:54.144239 23118544486528 run.py:483] Algo bellman_ford step 7958 current loss 0.045921, current_train_items 254688.
I0304 19:31:54.178125 23118544486528 run.py:483] Algo bellman_ford step 7959 current loss 0.034274, current_train_items 254720.
I0304 19:31:54.198127 23118544486528 run.py:483] Algo bellman_ford step 7960 current loss 0.003294, current_train_items 254752.
I0304 19:31:54.215026 23118544486528 run.py:483] Algo bellman_ford step 7961 current loss 0.021154, current_train_items 254784.
I0304 19:31:54.238955 23118544486528 run.py:483] Algo bellman_ford step 7962 current loss 0.040690, current_train_items 254816.
I0304 19:31:54.270236 23118544486528 run.py:483] Algo bellman_ford step 7963 current loss 0.045723, current_train_items 254848.
I0304 19:31:54.305031 23118544486528 run.py:483] Algo bellman_ford step 7964 current loss 0.037213, current_train_items 254880.
I0304 19:31:54.324593 23118544486528 run.py:483] Algo bellman_ford step 7965 current loss 0.004977, current_train_items 254912.
I0304 19:31:54.341308 23118544486528 run.py:483] Algo bellman_ford step 7966 current loss 0.014216, current_train_items 254944.
I0304 19:31:54.366749 23118544486528 run.py:483] Algo bellman_ford step 7967 current loss 0.066372, current_train_items 254976.
I0304 19:31:54.399082 23118544486528 run.py:483] Algo bellman_ford step 7968 current loss 0.032496, current_train_items 255008.
I0304 19:31:54.431797 23118544486528 run.py:483] Algo bellman_ford step 7969 current loss 0.051524, current_train_items 255040.
I0304 19:31:54.451554 23118544486528 run.py:483] Algo bellman_ford step 7970 current loss 0.004977, current_train_items 255072.
I0304 19:31:54.467944 23118544486528 run.py:483] Algo bellman_ford step 7971 current loss 0.020169, current_train_items 255104.
I0304 19:31:54.492592 23118544486528 run.py:483] Algo bellman_ford step 7972 current loss 0.038332, current_train_items 255136.
I0304 19:31:54.524006 23118544486528 run.py:483] Algo bellman_ford step 7973 current loss 0.040489, current_train_items 255168.
I0304 19:31:54.559408 23118544486528 run.py:483] Algo bellman_ford step 7974 current loss 0.065008, current_train_items 255200.
I0304 19:31:54.579462 23118544486528 run.py:483] Algo bellman_ford step 7975 current loss 0.008244, current_train_items 255232.
I0304 19:31:54.596072 23118544486528 run.py:483] Algo bellman_ford step 7976 current loss 0.021016, current_train_items 255264.
I0304 19:31:54.620980 23118544486528 run.py:483] Algo bellman_ford step 7977 current loss 0.045145, current_train_items 255296.
I0304 19:31:54.653088 23118544486528 run.py:483] Algo bellman_ford step 7978 current loss 0.047608, current_train_items 255328.
I0304 19:31:54.685923 23118544486528 run.py:483] Algo bellman_ford step 7979 current loss 0.031971, current_train_items 255360.
I0304 19:31:54.705558 23118544486528 run.py:483] Algo bellman_ford step 7980 current loss 0.005333, current_train_items 255392.
I0304 19:31:54.721761 23118544486528 run.py:483] Algo bellman_ford step 7981 current loss 0.029398, current_train_items 255424.
I0304 19:31:54.745190 23118544486528 run.py:483] Algo bellman_ford step 7982 current loss 0.034649, current_train_items 255456.
I0304 19:31:54.776912 23118544486528 run.py:483] Algo bellman_ford step 7983 current loss 0.032439, current_train_items 255488.
I0304 19:31:54.811487 23118544486528 run.py:483] Algo bellman_ford step 7984 current loss 0.060254, current_train_items 255520.
I0304 19:31:54.831227 23118544486528 run.py:483] Algo bellman_ford step 7985 current loss 0.005010, current_train_items 255552.
I0304 19:31:54.847892 23118544486528 run.py:483] Algo bellman_ford step 7986 current loss 0.019790, current_train_items 255584.
I0304 19:31:54.871764 23118544486528 run.py:483] Algo bellman_ford step 7987 current loss 0.081497, current_train_items 255616.
I0304 19:31:54.904396 23118544486528 run.py:483] Algo bellman_ford step 7988 current loss 0.054723, current_train_items 255648.
I0304 19:31:54.937463 23118544486528 run.py:483] Algo bellman_ford step 7989 current loss 0.043493, current_train_items 255680.
I0304 19:31:54.957363 23118544486528 run.py:483] Algo bellman_ford step 7990 current loss 0.003379, current_train_items 255712.
I0304 19:31:54.973848 23118544486528 run.py:483] Algo bellman_ford step 7991 current loss 0.044816, current_train_items 255744.
I0304 19:31:54.998167 23118544486528 run.py:483] Algo bellman_ford step 7992 current loss 0.028740, current_train_items 255776.
I0304 19:31:55.032113 23118544486528 run.py:483] Algo bellman_ford step 7993 current loss 0.063826, current_train_items 255808.
I0304 19:31:55.067324 23118544486528 run.py:483] Algo bellman_ford step 7994 current loss 0.055942, current_train_items 255840.
I0304 19:31:55.087174 23118544486528 run.py:483] Algo bellman_ford step 7995 current loss 0.004232, current_train_items 255872.
I0304 19:31:55.103116 23118544486528 run.py:483] Algo bellman_ford step 7996 current loss 0.009829, current_train_items 255904.
I0304 19:31:55.127343 23118544486528 run.py:483] Algo bellman_ford step 7997 current loss 0.026357, current_train_items 255936.
I0304 19:31:55.159396 23118544486528 run.py:483] Algo bellman_ford step 7998 current loss 0.026405, current_train_items 255968.
I0304 19:31:55.191901 23118544486528 run.py:483] Algo bellman_ford step 7999 current loss 0.052779, current_train_items 256000.
I0304 19:31:55.211769 23118544486528 run.py:483] Algo bellman_ford step 8000 current loss 0.002344, current_train_items 256032.
I0304 19:31:55.219337 23118544486528 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0304 19:31:55.219442 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:31:55.236546 23118544486528 run.py:483] Algo bellman_ford step 8001 current loss 0.008605, current_train_items 256064.
I0304 19:31:55.260981 23118544486528 run.py:483] Algo bellman_ford step 8002 current loss 0.033910, current_train_items 256096.
I0304 19:31:55.294309 23118544486528 run.py:483] Algo bellman_ford step 8003 current loss 0.045786, current_train_items 256128.
I0304 19:31:55.328147 23118544486528 run.py:483] Algo bellman_ford step 8004 current loss 0.051227, current_train_items 256160.
I0304 19:31:55.348037 23118544486528 run.py:483] Algo bellman_ford step 8005 current loss 0.003729, current_train_items 256192.
I0304 19:31:55.364156 23118544486528 run.py:483] Algo bellman_ford step 8006 current loss 0.011668, current_train_items 256224.
I0304 19:31:55.388542 23118544486528 run.py:483] Algo bellman_ford step 8007 current loss 0.023792, current_train_items 256256.
I0304 19:31:55.421941 23118544486528 run.py:483] Algo bellman_ford step 8008 current loss 0.044155, current_train_items 256288.
I0304 19:31:55.456504 23118544486528 run.py:483] Algo bellman_ford step 8009 current loss 0.044440, current_train_items 256320.
I0304 19:31:55.476067 23118544486528 run.py:483] Algo bellman_ford step 8010 current loss 0.003163, current_train_items 256352.
I0304 19:31:55.492168 23118544486528 run.py:483] Algo bellman_ford step 8011 current loss 0.025645, current_train_items 256384.
I0304 19:31:55.517041 23118544486528 run.py:483] Algo bellman_ford step 8012 current loss 0.024199, current_train_items 256416.
I0304 19:31:55.549045 23118544486528 run.py:483] Algo bellman_ford step 8013 current loss 0.059734, current_train_items 256448.
I0304 19:31:55.581180 23118544486528 run.py:483] Algo bellman_ford step 8014 current loss 0.079088, current_train_items 256480.
I0304 19:31:55.601233 23118544486528 run.py:483] Algo bellman_ford step 8015 current loss 0.018892, current_train_items 256512.
I0304 19:31:55.618139 23118544486528 run.py:483] Algo bellman_ford step 8016 current loss 0.027817, current_train_items 256544.
I0304 19:31:55.642151 23118544486528 run.py:483] Algo bellman_ford step 8017 current loss 0.016745, current_train_items 256576.
I0304 19:31:55.674348 23118544486528 run.py:483] Algo bellman_ford step 8018 current loss 0.032524, current_train_items 256608.
I0304 19:31:55.707047 23118544486528 run.py:483] Algo bellman_ford step 8019 current loss 0.041867, current_train_items 256640.
I0304 19:31:55.726488 23118544486528 run.py:483] Algo bellman_ford step 8020 current loss 0.004042, current_train_items 256672.
I0304 19:31:55.742931 23118544486528 run.py:483] Algo bellman_ford step 8021 current loss 0.009577, current_train_items 256704.
I0304 19:31:55.767230 23118544486528 run.py:483] Algo bellman_ford step 8022 current loss 0.063205, current_train_items 256736.
I0304 19:31:55.798283 23118544486528 run.py:483] Algo bellman_ford step 8023 current loss 0.067106, current_train_items 256768.
I0304 19:31:55.833667 23118544486528 run.py:483] Algo bellman_ford step 8024 current loss 0.060736, current_train_items 256800.
I0304 19:31:55.853215 23118544486528 run.py:483] Algo bellman_ford step 8025 current loss 0.013392, current_train_items 256832.
I0304 19:31:55.870169 23118544486528 run.py:483] Algo bellman_ford step 8026 current loss 0.008497, current_train_items 256864.
I0304 19:31:55.895152 23118544486528 run.py:483] Algo bellman_ford step 8027 current loss 0.022890, current_train_items 256896.
I0304 19:31:55.926126 23118544486528 run.py:483] Algo bellman_ford step 8028 current loss 0.037884, current_train_items 256928.
I0304 19:31:55.959989 23118544486528 run.py:483] Algo bellman_ford step 8029 current loss 0.107611, current_train_items 256960.
I0304 19:31:55.979761 23118544486528 run.py:483] Algo bellman_ford step 8030 current loss 0.002988, current_train_items 256992.
I0304 19:31:55.995980 23118544486528 run.py:483] Algo bellman_ford step 8031 current loss 0.011797, current_train_items 257024.
I0304 19:31:56.021124 23118544486528 run.py:483] Algo bellman_ford step 8032 current loss 0.030562, current_train_items 257056.
I0304 19:31:56.053942 23118544486528 run.py:483] Algo bellman_ford step 8033 current loss 0.047912, current_train_items 257088.
I0304 19:31:56.088841 23118544486528 run.py:483] Algo bellman_ford step 8034 current loss 0.055688, current_train_items 257120.
I0304 19:31:56.108475 23118544486528 run.py:483] Algo bellman_ford step 8035 current loss 0.005421, current_train_items 257152.
I0304 19:31:56.125271 23118544486528 run.py:483] Algo bellman_ford step 8036 current loss 0.040509, current_train_items 257184.
I0304 19:31:56.149661 23118544486528 run.py:483] Algo bellman_ford step 8037 current loss 0.039340, current_train_items 257216.
I0304 19:31:56.181735 23118544486528 run.py:483] Algo bellman_ford step 8038 current loss 0.036494, current_train_items 257248.
I0304 19:31:56.217122 23118544486528 run.py:483] Algo bellman_ford step 8039 current loss 0.068010, current_train_items 257280.
I0304 19:31:56.237058 23118544486528 run.py:483] Algo bellman_ford step 8040 current loss 0.004440, current_train_items 257312.
I0304 19:31:56.253579 23118544486528 run.py:483] Algo bellman_ford step 8041 current loss 0.019198, current_train_items 257344.
I0304 19:31:56.277979 23118544486528 run.py:483] Algo bellman_ford step 8042 current loss 0.048735, current_train_items 257376.
I0304 19:31:56.310398 23118544486528 run.py:483] Algo bellman_ford step 8043 current loss 0.050722, current_train_items 257408.
I0304 19:31:56.345352 23118544486528 run.py:483] Algo bellman_ford step 8044 current loss 0.049518, current_train_items 257440.
I0304 19:31:56.365160 23118544486528 run.py:483] Algo bellman_ford step 8045 current loss 0.003588, current_train_items 257472.
I0304 19:31:56.381989 23118544486528 run.py:483] Algo bellman_ford step 8046 current loss 0.057204, current_train_items 257504.
I0304 19:31:56.405769 23118544486528 run.py:483] Algo bellman_ford step 8047 current loss 0.047320, current_train_items 257536.
I0304 19:31:56.438125 23118544486528 run.py:483] Algo bellman_ford step 8048 current loss 0.115035, current_train_items 257568.
I0304 19:31:56.470906 23118544486528 run.py:483] Algo bellman_ford step 8049 current loss 0.069653, current_train_items 257600.
I0304 19:31:56.490613 23118544486528 run.py:483] Algo bellman_ford step 8050 current loss 0.006487, current_train_items 257632.
I0304 19:31:56.498661 23118544486528 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0304 19:31:56.498771 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:31:56.515933 23118544486528 run.py:483] Algo bellman_ford step 8051 current loss 0.025555, current_train_items 257664.
I0304 19:31:56.540364 23118544486528 run.py:483] Algo bellman_ford step 8052 current loss 0.022617, current_train_items 257696.
I0304 19:31:56.572609 23118544486528 run.py:483] Algo bellman_ford step 8053 current loss 0.042969, current_train_items 257728.
I0304 19:31:56.608137 23118544486528 run.py:483] Algo bellman_ford step 8054 current loss 0.118030, current_train_items 257760.
I0304 19:31:56.628066 23118544486528 run.py:483] Algo bellman_ford step 8055 current loss 0.015821, current_train_items 257792.
I0304 19:31:56.643505 23118544486528 run.py:483] Algo bellman_ford step 8056 current loss 0.004918, current_train_items 257824.
I0304 19:31:56.666974 23118544486528 run.py:483] Algo bellman_ford step 8057 current loss 0.032218, current_train_items 257856.
I0304 19:31:56.700556 23118544486528 run.py:483] Algo bellman_ford step 8058 current loss 0.064216, current_train_items 257888.
I0304 19:31:56.737056 23118544486528 run.py:483] Algo bellman_ford step 8059 current loss 0.061633, current_train_items 257920.
I0304 19:31:56.757024 23118544486528 run.py:483] Algo bellman_ford step 8060 current loss 0.008379, current_train_items 257952.
I0304 19:31:56.773628 23118544486528 run.py:483] Algo bellman_ford step 8061 current loss 0.006265, current_train_items 257984.
I0304 19:31:56.799148 23118544486528 run.py:483] Algo bellman_ford step 8062 current loss 0.040160, current_train_items 258016.
I0304 19:31:56.831342 23118544486528 run.py:483] Algo bellman_ford step 8063 current loss 0.058818, current_train_items 258048.
I0304 19:31:56.864551 23118544486528 run.py:483] Algo bellman_ford step 8064 current loss 0.063191, current_train_items 258080.
I0304 19:31:56.885004 23118544486528 run.py:483] Algo bellman_ford step 8065 current loss 0.003114, current_train_items 258112.
I0304 19:31:56.901448 23118544486528 run.py:483] Algo bellman_ford step 8066 current loss 0.019124, current_train_items 258144.
I0304 19:31:56.926066 23118544486528 run.py:483] Algo bellman_ford step 8067 current loss 0.034561, current_train_items 258176.
I0304 19:31:56.958024 23118544486528 run.py:483] Algo bellman_ford step 8068 current loss 0.044439, current_train_items 258208.
I0304 19:31:56.991239 23118544486528 run.py:483] Algo bellman_ford step 8069 current loss 0.048436, current_train_items 258240.
I0304 19:31:57.011482 23118544486528 run.py:483] Algo bellman_ford step 8070 current loss 0.004389, current_train_items 258272.
I0304 19:31:57.028777 23118544486528 run.py:483] Algo bellman_ford step 8071 current loss 0.056766, current_train_items 258304.
I0304 19:31:57.053404 23118544486528 run.py:483] Algo bellman_ford step 8072 current loss 0.032984, current_train_items 258336.
I0304 19:31:57.085800 23118544486528 run.py:483] Algo bellman_ford step 8073 current loss 0.031608, current_train_items 258368.
I0304 19:31:57.120851 23118544486528 run.py:483] Algo bellman_ford step 8074 current loss 0.053666, current_train_items 258400.
I0304 19:31:57.140795 23118544486528 run.py:483] Algo bellman_ford step 8075 current loss 0.004458, current_train_items 258432.
I0304 19:31:57.157270 23118544486528 run.py:483] Algo bellman_ford step 8076 current loss 0.043874, current_train_items 258464.
I0304 19:31:57.181539 23118544486528 run.py:483] Algo bellman_ford step 8077 current loss 0.014467, current_train_items 258496.
I0304 19:31:57.213190 23118544486528 run.py:483] Algo bellman_ford step 8078 current loss 0.077988, current_train_items 258528.
I0304 19:31:57.245815 23118544486528 run.py:483] Algo bellman_ford step 8079 current loss 0.047690, current_train_items 258560.
I0304 19:31:57.265729 23118544486528 run.py:483] Algo bellman_ford step 8080 current loss 0.010220, current_train_items 258592.
I0304 19:31:57.282155 23118544486528 run.py:483] Algo bellman_ford step 8081 current loss 0.011475, current_train_items 258624.
I0304 19:31:57.306573 23118544486528 run.py:483] Algo bellman_ford step 8082 current loss 0.050684, current_train_items 258656.
I0304 19:31:57.339496 23118544486528 run.py:483] Algo bellman_ford step 8083 current loss 0.042052, current_train_items 258688.
I0304 19:31:57.376787 23118544486528 run.py:483] Algo bellman_ford step 8084 current loss 0.061473, current_train_items 258720.
I0304 19:31:57.396944 23118544486528 run.py:483] Algo bellman_ford step 8085 current loss 0.005788, current_train_items 258752.
I0304 19:31:57.413856 23118544486528 run.py:483] Algo bellman_ford step 8086 current loss 0.030367, current_train_items 258784.
I0304 19:31:57.437653 23118544486528 run.py:483] Algo bellman_ford step 8087 current loss 0.040947, current_train_items 258816.
I0304 19:31:57.471729 23118544486528 run.py:483] Algo bellman_ford step 8088 current loss 0.118454, current_train_items 258848.
I0304 19:31:57.505001 23118544486528 run.py:483] Algo bellman_ford step 8089 current loss 0.076390, current_train_items 258880.
I0304 19:31:57.525278 23118544486528 run.py:483] Algo bellman_ford step 8090 current loss 0.006061, current_train_items 258912.
I0304 19:31:57.541259 23118544486528 run.py:483] Algo bellman_ford step 8091 current loss 0.013896, current_train_items 258944.
I0304 19:31:57.565983 23118544486528 run.py:483] Algo bellman_ford step 8092 current loss 0.023773, current_train_items 258976.
I0304 19:31:57.597737 23118544486528 run.py:483] Algo bellman_ford step 8093 current loss 0.038867, current_train_items 259008.
I0304 19:31:57.632515 23118544486528 run.py:483] Algo bellman_ford step 8094 current loss 0.072728, current_train_items 259040.
I0304 19:31:57.652322 23118544486528 run.py:483] Algo bellman_ford step 8095 current loss 0.013557, current_train_items 259072.
I0304 19:31:57.668830 23118544486528 run.py:483] Algo bellman_ford step 8096 current loss 0.019326, current_train_items 259104.
I0304 19:31:57.694576 23118544486528 run.py:483] Algo bellman_ford step 8097 current loss 0.041989, current_train_items 259136.
I0304 19:31:57.727707 23118544486528 run.py:483] Algo bellman_ford step 8098 current loss 0.048482, current_train_items 259168.
I0304 19:31:57.761716 23118544486528 run.py:483] Algo bellman_ford step 8099 current loss 0.044676, current_train_items 259200.
I0304 19:31:57.781824 23118544486528 run.py:483] Algo bellman_ford step 8100 current loss 0.002138, current_train_items 259232.
I0304 19:31:57.789672 23118544486528 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0304 19:31:57.789785 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:31:57.806934 23118544486528 run.py:483] Algo bellman_ford step 8101 current loss 0.010016, current_train_items 259264.
I0304 19:31:57.833191 23118544486528 run.py:483] Algo bellman_ford step 8102 current loss 0.046589, current_train_items 259296.
I0304 19:31:57.865657 23118544486528 run.py:483] Algo bellman_ford step 8103 current loss 0.035701, current_train_items 259328.
I0304 19:31:57.899997 23118544486528 run.py:483] Algo bellman_ford step 8104 current loss 0.071452, current_train_items 259360.
I0304 19:31:57.920472 23118544486528 run.py:483] Algo bellman_ford step 8105 current loss 0.005408, current_train_items 259392.
I0304 19:31:57.936833 23118544486528 run.py:483] Algo bellman_ford step 8106 current loss 0.023721, current_train_items 259424.
I0304 19:31:57.960950 23118544486528 run.py:483] Algo bellman_ford step 8107 current loss 0.054751, current_train_items 259456.
I0304 19:31:57.992973 23118544486528 run.py:483] Algo bellman_ford step 8108 current loss 0.109555, current_train_items 259488.
I0304 19:31:58.027901 23118544486528 run.py:483] Algo bellman_ford step 8109 current loss 0.066369, current_train_items 259520.
I0304 19:31:58.047845 23118544486528 run.py:483] Algo bellman_ford step 8110 current loss 0.023396, current_train_items 259552.
I0304 19:31:58.063743 23118544486528 run.py:483] Algo bellman_ford step 8111 current loss 0.021250, current_train_items 259584.
I0304 19:31:58.087893 23118544486528 run.py:483] Algo bellman_ford step 8112 current loss 0.059733, current_train_items 259616.
I0304 19:31:58.118308 23118544486528 run.py:483] Algo bellman_ford step 8113 current loss 0.037703, current_train_items 259648.
I0304 19:31:58.154464 23118544486528 run.py:483] Algo bellman_ford step 8114 current loss 0.076414, current_train_items 259680.
I0304 19:31:58.174228 23118544486528 run.py:483] Algo bellman_ford step 8115 current loss 0.005423, current_train_items 259712.
I0304 19:31:58.190668 23118544486528 run.py:483] Algo bellman_ford step 8116 current loss 0.005166, current_train_items 259744.
I0304 19:31:58.215509 23118544486528 run.py:483] Algo bellman_ford step 8117 current loss 0.085834, current_train_items 259776.
I0304 19:31:58.248409 23118544486528 run.py:483] Algo bellman_ford step 8118 current loss 0.061314, current_train_items 259808.
I0304 19:31:58.282024 23118544486528 run.py:483] Algo bellman_ford step 8119 current loss 0.091204, current_train_items 259840.
I0304 19:31:58.301504 23118544486528 run.py:483] Algo bellman_ford step 8120 current loss 0.006949, current_train_items 259872.
I0304 19:31:58.318001 23118544486528 run.py:483] Algo bellman_ford step 8121 current loss 0.006039, current_train_items 259904.
I0304 19:31:58.342588 23118544486528 run.py:483] Algo bellman_ford step 8122 current loss 0.045623, current_train_items 259936.
I0304 19:31:58.375983 23118544486528 run.py:483] Algo bellman_ford step 8123 current loss 0.051336, current_train_items 259968.
I0304 19:31:58.408362 23118544486528 run.py:483] Algo bellman_ford step 8124 current loss 0.037450, current_train_items 260000.
I0304 19:31:58.428078 23118544486528 run.py:483] Algo bellman_ford step 8125 current loss 0.002678, current_train_items 260032.
I0304 19:31:58.444724 23118544486528 run.py:483] Algo bellman_ford step 8126 current loss 0.017612, current_train_items 260064.
I0304 19:31:58.469751 23118544486528 run.py:483] Algo bellman_ford step 8127 current loss 0.028243, current_train_items 260096.
I0304 19:31:58.502420 23118544486528 run.py:483] Algo bellman_ford step 8128 current loss 0.058871, current_train_items 260128.
I0304 19:31:58.538990 23118544486528 run.py:483] Algo bellman_ford step 8129 current loss 0.110200, current_train_items 260160.
I0304 19:31:58.559049 23118544486528 run.py:483] Algo bellman_ford step 8130 current loss 0.009203, current_train_items 260192.
I0304 19:31:58.575124 23118544486528 run.py:483] Algo bellman_ford step 8131 current loss 0.023218, current_train_items 260224.
I0304 19:31:58.599602 23118544486528 run.py:483] Algo bellman_ford step 8132 current loss 0.029280, current_train_items 260256.
I0304 19:31:58.631795 23118544486528 run.py:483] Algo bellman_ford step 8133 current loss 0.038265, current_train_items 260288.
I0304 19:31:58.666243 23118544486528 run.py:483] Algo bellman_ford step 8134 current loss 0.050636, current_train_items 260320.
I0304 19:31:58.685834 23118544486528 run.py:483] Algo bellman_ford step 8135 current loss 0.002524, current_train_items 260352.
I0304 19:31:58.701836 23118544486528 run.py:483] Algo bellman_ford step 8136 current loss 0.016758, current_train_items 260384.
I0304 19:31:58.726117 23118544486528 run.py:483] Algo bellman_ford step 8137 current loss 0.026602, current_train_items 260416.
I0304 19:31:58.758505 23118544486528 run.py:483] Algo bellman_ford step 8138 current loss 0.044104, current_train_items 260448.
I0304 19:31:58.794243 23118544486528 run.py:483] Algo bellman_ford step 8139 current loss 0.065143, current_train_items 260480.
I0304 19:31:58.813794 23118544486528 run.py:483] Algo bellman_ford step 8140 current loss 0.003562, current_train_items 260512.
I0304 19:31:58.829991 23118544486528 run.py:483] Algo bellman_ford step 8141 current loss 0.025249, current_train_items 260544.
I0304 19:31:58.855004 23118544486528 run.py:483] Algo bellman_ford step 8142 current loss 0.057882, current_train_items 260576.
I0304 19:31:58.887552 23118544486528 run.py:483] Algo bellman_ford step 8143 current loss 0.051307, current_train_items 260608.
I0304 19:31:58.922554 23118544486528 run.py:483] Algo bellman_ford step 8144 current loss 0.055684, current_train_items 260640.
I0304 19:31:58.942110 23118544486528 run.py:483] Algo bellman_ford step 8145 current loss 0.004786, current_train_items 260672.
I0304 19:31:58.958702 23118544486528 run.py:483] Algo bellman_ford step 8146 current loss 0.017521, current_train_items 260704.
I0304 19:31:58.982916 23118544486528 run.py:483] Algo bellman_ford step 8147 current loss 0.048611, current_train_items 260736.
I0304 19:31:59.015490 23118544486528 run.py:483] Algo bellman_ford step 8148 current loss 0.047380, current_train_items 260768.
I0304 19:31:59.049035 23118544486528 run.py:483] Algo bellman_ford step 8149 current loss 0.035786, current_train_items 260800.
I0304 19:31:59.068802 23118544486528 run.py:483] Algo bellman_ford step 8150 current loss 0.003459, current_train_items 260832.
I0304 19:31:59.076669 23118544486528 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0304 19:31:59.076783 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:31:59.093745 23118544486528 run.py:483] Algo bellman_ford step 8151 current loss 0.008668, current_train_items 260864.
I0304 19:31:59.118702 23118544486528 run.py:483] Algo bellman_ford step 8152 current loss 0.053194, current_train_items 260896.
I0304 19:31:59.149989 23118544486528 run.py:483] Algo bellman_ford step 8153 current loss 0.015625, current_train_items 260928.
I0304 19:31:59.185833 23118544486528 run.py:483] Algo bellman_ford step 8154 current loss 0.088390, current_train_items 260960.
I0304 19:31:59.206244 23118544486528 run.py:483] Algo bellman_ford step 8155 current loss 0.006240, current_train_items 260992.
I0304 19:31:59.221962 23118544486528 run.py:483] Algo bellman_ford step 8156 current loss 0.014682, current_train_items 261024.
I0304 19:31:59.246981 23118544486528 run.py:483] Algo bellman_ford step 8157 current loss 0.028417, current_train_items 261056.
I0304 19:31:59.278790 23118544486528 run.py:483] Algo bellman_ford step 8158 current loss 0.050625, current_train_items 261088.
I0304 19:31:59.312481 23118544486528 run.py:483] Algo bellman_ford step 8159 current loss 0.088160, current_train_items 261120.
I0304 19:31:59.332714 23118544486528 run.py:483] Algo bellman_ford step 8160 current loss 0.004900, current_train_items 261152.
I0304 19:31:59.349392 23118544486528 run.py:483] Algo bellman_ford step 8161 current loss 0.014427, current_train_items 261184.
I0304 19:31:59.373668 23118544486528 run.py:483] Algo bellman_ford step 8162 current loss 0.060522, current_train_items 261216.
I0304 19:31:59.406109 23118544486528 run.py:483] Algo bellman_ford step 8163 current loss 0.055363, current_train_items 261248.
I0304 19:31:59.436989 23118544486528 run.py:483] Algo bellman_ford step 8164 current loss 0.027280, current_train_items 261280.
I0304 19:31:59.456637 23118544486528 run.py:483] Algo bellman_ford step 8165 current loss 0.005096, current_train_items 261312.
I0304 19:31:59.473070 23118544486528 run.py:483] Algo bellman_ford step 8166 current loss 0.018322, current_train_items 261344.
I0304 19:31:59.497801 23118544486528 run.py:483] Algo bellman_ford step 8167 current loss 0.038226, current_train_items 261376.
I0304 19:31:59.529560 23118544486528 run.py:483] Algo bellman_ford step 8168 current loss 0.022599, current_train_items 261408.
I0304 19:31:59.565197 23118544486528 run.py:483] Algo bellman_ford step 8169 current loss 0.063121, current_train_items 261440.
I0304 19:31:59.585182 23118544486528 run.py:483] Algo bellman_ford step 8170 current loss 0.005147, current_train_items 261472.
I0304 19:31:59.601579 23118544486528 run.py:483] Algo bellman_ford step 8171 current loss 0.012184, current_train_items 261504.
I0304 19:31:59.626440 23118544486528 run.py:483] Algo bellman_ford step 8172 current loss 0.042128, current_train_items 261536.
I0304 19:31:59.658913 23118544486528 run.py:483] Algo bellman_ford step 8173 current loss 0.035611, current_train_items 261568.
I0304 19:31:59.692690 23118544486528 run.py:483] Algo bellman_ford step 8174 current loss 0.053271, current_train_items 261600.
I0304 19:31:59.712723 23118544486528 run.py:483] Algo bellman_ford step 8175 current loss 0.004349, current_train_items 261632.
I0304 19:31:59.729555 23118544486528 run.py:483] Algo bellman_ford step 8176 current loss 0.036477, current_train_items 261664.
I0304 19:31:59.753571 23118544486528 run.py:483] Algo bellman_ford step 8177 current loss 0.025991, current_train_items 261696.
I0304 19:31:59.786674 23118544486528 run.py:483] Algo bellman_ford step 8178 current loss 0.029833, current_train_items 261728.
I0304 19:31:59.818342 23118544486528 run.py:483] Algo bellman_ford step 8179 current loss 0.044829, current_train_items 261760.
I0304 19:31:59.838265 23118544486528 run.py:483] Algo bellman_ford step 8180 current loss 0.004048, current_train_items 261792.
I0304 19:31:59.854544 23118544486528 run.py:483] Algo bellman_ford step 8181 current loss 0.014479, current_train_items 261824.
I0304 19:31:59.878661 23118544486528 run.py:483] Algo bellman_ford step 8182 current loss 0.022824, current_train_items 261856.
I0304 19:31:59.911623 23118544486528 run.py:483] Algo bellman_ford step 8183 current loss 0.067250, current_train_items 261888.
I0304 19:31:59.948148 23118544486528 run.py:483] Algo bellman_ford step 8184 current loss 0.065005, current_train_items 261920.
I0304 19:31:59.968103 23118544486528 run.py:483] Algo bellman_ford step 8185 current loss 0.004163, current_train_items 261952.
I0304 19:31:59.984499 23118544486528 run.py:483] Algo bellman_ford step 8186 current loss 0.008040, current_train_items 261984.
I0304 19:32:00.009295 23118544486528 run.py:483] Algo bellman_ford step 8187 current loss 0.111707, current_train_items 262016.
I0304 19:32:00.041339 23118544486528 run.py:483] Algo bellman_ford step 8188 current loss 0.124812, current_train_items 262048.
I0304 19:32:00.075168 23118544486528 run.py:483] Algo bellman_ford step 8189 current loss 0.070164, current_train_items 262080.
I0304 19:32:00.095396 23118544486528 run.py:483] Algo bellman_ford step 8190 current loss 0.014439, current_train_items 262112.
I0304 19:32:00.111979 23118544486528 run.py:483] Algo bellman_ford step 8191 current loss 0.015255, current_train_items 262144.
I0304 19:32:00.136120 23118544486528 run.py:483] Algo bellman_ford step 8192 current loss 0.026130, current_train_items 262176.
I0304 19:32:00.168099 23118544486528 run.py:483] Algo bellman_ford step 8193 current loss 0.031859, current_train_items 262208.
I0304 19:32:00.202182 23118544486528 run.py:483] Algo bellman_ford step 8194 current loss 0.043175, current_train_items 262240.
I0304 19:32:00.221702 23118544486528 run.py:483] Algo bellman_ford step 8195 current loss 0.002807, current_train_items 262272.
I0304 19:32:00.238105 23118544486528 run.py:483] Algo bellman_ford step 8196 current loss 0.007739, current_train_items 262304.
I0304 19:32:00.261762 23118544486528 run.py:483] Algo bellman_ford step 8197 current loss 0.023488, current_train_items 262336.
I0304 19:32:00.294219 23118544486528 run.py:483] Algo bellman_ford step 8198 current loss 0.023663, current_train_items 262368.
I0304 19:32:00.328036 23118544486528 run.py:483] Algo bellman_ford step 8199 current loss 0.052767, current_train_items 262400.
I0304 19:32:00.348013 23118544486528 run.py:483] Algo bellman_ford step 8200 current loss 0.004964, current_train_items 262432.
I0304 19:32:00.355581 23118544486528 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0304 19:32:00.355695 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:00.372535 23118544486528 run.py:483] Algo bellman_ford step 8201 current loss 0.005792, current_train_items 262464.
I0304 19:32:00.397675 23118544486528 run.py:483] Algo bellman_ford step 8202 current loss 0.033540, current_train_items 262496.
I0304 19:32:00.429624 23118544486528 run.py:483] Algo bellman_ford step 8203 current loss 0.026317, current_train_items 262528.
I0304 19:32:00.464946 23118544486528 run.py:483] Algo bellman_ford step 8204 current loss 0.040230, current_train_items 262560.
I0304 19:32:00.484523 23118544486528 run.py:483] Algo bellman_ford step 8205 current loss 0.002095, current_train_items 262592.
I0304 19:32:00.500907 23118544486528 run.py:483] Algo bellman_ford step 8206 current loss 0.007334, current_train_items 262624.
I0304 19:32:00.525976 23118544486528 run.py:483] Algo bellman_ford step 8207 current loss 0.042731, current_train_items 262656.
I0304 19:32:00.557824 23118544486528 run.py:483] Algo bellman_ford step 8208 current loss 0.038276, current_train_items 262688.
I0304 19:32:00.589506 23118544486528 run.py:483] Algo bellman_ford step 8209 current loss 0.021668, current_train_items 262720.
I0304 19:32:00.609141 23118544486528 run.py:483] Algo bellman_ford step 8210 current loss 0.001726, current_train_items 262752.
I0304 19:32:00.625354 23118544486528 run.py:483] Algo bellman_ford step 8211 current loss 0.043886, current_train_items 262784.
I0304 19:32:00.649172 23118544486528 run.py:483] Algo bellman_ford step 8212 current loss 0.039785, current_train_items 262816.
I0304 19:32:00.681324 23118544486528 run.py:483] Algo bellman_ford step 8213 current loss 0.032657, current_train_items 262848.
I0304 19:32:00.715415 23118544486528 run.py:483] Algo bellman_ford step 8214 current loss 0.078400, current_train_items 262880.
I0304 19:32:00.734980 23118544486528 run.py:483] Algo bellman_ford step 8215 current loss 0.003211, current_train_items 262912.
I0304 19:32:00.750696 23118544486528 run.py:483] Algo bellman_ford step 8216 current loss 0.003842, current_train_items 262944.
I0304 19:32:00.774114 23118544486528 run.py:483] Algo bellman_ford step 8217 current loss 0.038256, current_train_items 262976.
I0304 19:32:00.807298 23118544486528 run.py:483] Algo bellman_ford step 8218 current loss 0.041819, current_train_items 263008.
I0304 19:32:00.842106 23118544486528 run.py:483] Algo bellman_ford step 8219 current loss 0.060974, current_train_items 263040.
I0304 19:32:00.861587 23118544486528 run.py:483] Algo bellman_ford step 8220 current loss 0.002964, current_train_items 263072.
I0304 19:32:00.877680 23118544486528 run.py:483] Algo bellman_ford step 8221 current loss 0.030301, current_train_items 263104.
I0304 19:32:00.902756 23118544486528 run.py:483] Algo bellman_ford step 8222 current loss 0.047837, current_train_items 263136.
I0304 19:32:00.935225 23118544486528 run.py:483] Algo bellman_ford step 8223 current loss 0.066272, current_train_items 263168.
I0304 19:32:00.966917 23118544486528 run.py:483] Algo bellman_ford step 8224 current loss 0.036399, current_train_items 263200.
I0304 19:32:00.986853 23118544486528 run.py:483] Algo bellman_ford step 8225 current loss 0.003708, current_train_items 263232.
I0304 19:32:01.003402 23118544486528 run.py:483] Algo bellman_ford step 8226 current loss 0.018220, current_train_items 263264.
I0304 19:32:01.028272 23118544486528 run.py:483] Algo bellman_ford step 8227 current loss 0.015797, current_train_items 263296.
I0304 19:32:01.059532 23118544486528 run.py:483] Algo bellman_ford step 8228 current loss 0.054526, current_train_items 263328.
I0304 19:32:01.094263 23118544486528 run.py:483] Algo bellman_ford step 8229 current loss 0.043472, current_train_items 263360.
I0304 19:32:01.113858 23118544486528 run.py:483] Algo bellman_ford step 8230 current loss 0.003801, current_train_items 263392.
I0304 19:32:01.130118 23118544486528 run.py:483] Algo bellman_ford step 8231 current loss 0.011709, current_train_items 263424.
I0304 19:32:01.155138 23118544486528 run.py:483] Algo bellman_ford step 8232 current loss 0.040139, current_train_items 263456.
I0304 19:32:01.186886 23118544486528 run.py:483] Algo bellman_ford step 8233 current loss 0.073299, current_train_items 263488.
I0304 19:32:01.220462 23118544486528 run.py:483] Algo bellman_ford step 8234 current loss 0.041071, current_train_items 263520.
I0304 19:32:01.239839 23118544486528 run.py:483] Algo bellman_ford step 8235 current loss 0.002758, current_train_items 263552.
I0304 19:32:01.256654 23118544486528 run.py:483] Algo bellman_ford step 8236 current loss 0.009934, current_train_items 263584.
I0304 19:32:01.281703 23118544486528 run.py:483] Algo bellman_ford step 8237 current loss 0.034260, current_train_items 263616.
I0304 19:32:01.314384 23118544486528 run.py:483] Algo bellman_ford step 8238 current loss 0.033503, current_train_items 263648.
I0304 19:32:01.347845 23118544486528 run.py:483] Algo bellman_ford step 8239 current loss 0.044626, current_train_items 263680.
I0304 19:32:01.367329 23118544486528 run.py:483] Algo bellman_ford step 8240 current loss 0.002626, current_train_items 263712.
I0304 19:32:01.383694 23118544486528 run.py:483] Algo bellman_ford step 8241 current loss 0.009125, current_train_items 263744.
I0304 19:32:01.408050 23118544486528 run.py:483] Algo bellman_ford step 8242 current loss 0.036232, current_train_items 263776.
I0304 19:32:01.440011 23118544486528 run.py:483] Algo bellman_ford step 8243 current loss 0.056848, current_train_items 263808.
I0304 19:32:01.475907 23118544486528 run.py:483] Algo bellman_ford step 8244 current loss 0.051542, current_train_items 263840.
I0304 19:32:01.495598 23118544486528 run.py:483] Algo bellman_ford step 8245 current loss 0.020651, current_train_items 263872.
I0304 19:32:01.511931 23118544486528 run.py:483] Algo bellman_ford step 8246 current loss 0.014255, current_train_items 263904.
I0304 19:32:01.535313 23118544486528 run.py:483] Algo bellman_ford step 8247 current loss 0.023041, current_train_items 263936.
I0304 19:32:01.567015 23118544486528 run.py:483] Algo bellman_ford step 8248 current loss 0.043017, current_train_items 263968.
I0304 19:32:01.601479 23118544486528 run.py:483] Algo bellman_ford step 8249 current loss 0.033979, current_train_items 264000.
I0304 19:32:01.621132 23118544486528 run.py:483] Algo bellman_ford step 8250 current loss 0.004703, current_train_items 264032.
I0304 19:32:01.629386 23118544486528 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9990234375, 'score': 0.9990234375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0304 19:32:01.629492 23118544486528 run.py:519] Checkpointing best model, best avg val score was 0.993, current avg val score is 0.999, val scores are: bellman_ford: 0.999
I0304 19:32:01.658695 23118544486528 run.py:483] Algo bellman_ford step 8251 current loss 0.013094, current_train_items 264064.
I0304 19:32:01.683450 23118544486528 run.py:483] Algo bellman_ford step 8252 current loss 0.032664, current_train_items 264096.
I0304 19:32:01.717679 23118544486528 run.py:483] Algo bellman_ford step 8253 current loss 0.039535, current_train_items 264128.
I0304 19:32:01.751373 23118544486528 run.py:483] Algo bellman_ford step 8254 current loss 0.048289, current_train_items 264160.
I0304 19:32:01.771645 23118544486528 run.py:483] Algo bellman_ford step 8255 current loss 0.019887, current_train_items 264192.
I0304 19:32:01.788399 23118544486528 run.py:483] Algo bellman_ford step 8256 current loss 0.024057, current_train_items 264224.
I0304 19:32:01.813736 23118544486528 run.py:483] Algo bellman_ford step 8257 current loss 0.024015, current_train_items 264256.
I0304 19:32:01.846716 23118544486528 run.py:483] Algo bellman_ford step 8258 current loss 0.072002, current_train_items 264288.
I0304 19:32:01.882214 23118544486528 run.py:483] Algo bellman_ford step 8259 current loss 0.070497, current_train_items 264320.
I0304 19:32:01.902519 23118544486528 run.py:483] Algo bellman_ford step 8260 current loss 0.017190, current_train_items 264352.
I0304 19:32:01.919218 23118544486528 run.py:483] Algo bellman_ford step 8261 current loss 0.006072, current_train_items 264384.
I0304 19:32:01.944541 23118544486528 run.py:483] Algo bellman_ford step 8262 current loss 0.046862, current_train_items 264416.
I0304 19:32:01.977571 23118544486528 run.py:483] Algo bellman_ford step 8263 current loss 0.024593, current_train_items 264448.
I0304 19:32:02.013336 23118544486528 run.py:483] Algo bellman_ford step 8264 current loss 0.044074, current_train_items 264480.
I0304 19:32:02.033034 23118544486528 run.py:483] Algo bellman_ford step 8265 current loss 0.023609, current_train_items 264512.
I0304 19:32:02.049090 23118544486528 run.py:483] Algo bellman_ford step 8266 current loss 0.008686, current_train_items 264544.
I0304 19:32:02.073521 23118544486528 run.py:483] Algo bellman_ford step 8267 current loss 0.030237, current_train_items 264576.
I0304 19:32:02.106640 23118544486528 run.py:483] Algo bellman_ford step 8268 current loss 0.033836, current_train_items 264608.
I0304 19:32:02.139892 23118544486528 run.py:483] Algo bellman_ford step 8269 current loss 0.068690, current_train_items 264640.
I0304 19:32:02.160457 23118544486528 run.py:483] Algo bellman_ford step 8270 current loss 0.006614, current_train_items 264672.
I0304 19:32:02.176324 23118544486528 run.py:483] Algo bellman_ford step 8271 current loss 0.005642, current_train_items 264704.
I0304 19:32:02.200300 23118544486528 run.py:483] Algo bellman_ford step 8272 current loss 0.029768, current_train_items 264736.
I0304 19:32:02.231724 23118544486528 run.py:483] Algo bellman_ford step 8273 current loss 0.018504, current_train_items 264768.
I0304 19:32:02.267297 23118544486528 run.py:483] Algo bellman_ford step 8274 current loss 0.065563, current_train_items 264800.
I0304 19:32:02.287630 23118544486528 run.py:483] Algo bellman_ford step 8275 current loss 0.002821, current_train_items 264832.
I0304 19:32:02.304773 23118544486528 run.py:483] Algo bellman_ford step 8276 current loss 0.028092, current_train_items 264864.
I0304 19:32:02.329783 23118544486528 run.py:483] Algo bellman_ford step 8277 current loss 0.023535, current_train_items 264896.
I0304 19:32:02.362054 23118544486528 run.py:483] Algo bellman_ford step 8278 current loss 0.032396, current_train_items 264928.
I0304 19:32:02.395956 23118544486528 run.py:483] Algo bellman_ford step 8279 current loss 0.047635, current_train_items 264960.
I0304 19:32:02.415917 23118544486528 run.py:483] Algo bellman_ford step 8280 current loss 0.013936, current_train_items 264992.
I0304 19:32:02.432737 23118544486528 run.py:483] Algo bellman_ford step 8281 current loss 0.018636, current_train_items 265024.
I0304 19:32:02.456942 23118544486528 run.py:483] Algo bellman_ford step 8282 current loss 0.008148, current_train_items 265056.
I0304 19:32:02.488570 23118544486528 run.py:483] Algo bellman_ford step 8283 current loss 0.043117, current_train_items 265088.
I0304 19:32:02.522125 23118544486528 run.py:483] Algo bellman_ford step 8284 current loss 0.049073, current_train_items 265120.
I0304 19:32:02.542498 23118544486528 run.py:483] Algo bellman_ford step 8285 current loss 0.004950, current_train_items 265152.
I0304 19:32:02.559180 23118544486528 run.py:483] Algo bellman_ford step 8286 current loss 0.031365, current_train_items 265184.
I0304 19:32:02.583823 23118544486528 run.py:483] Algo bellman_ford step 8287 current loss 0.029483, current_train_items 265216.
I0304 19:32:02.617502 23118544486528 run.py:483] Algo bellman_ford step 8288 current loss 0.050973, current_train_items 265248.
I0304 19:32:02.652171 23118544486528 run.py:483] Algo bellman_ford step 8289 current loss 0.060535, current_train_items 265280.
I0304 19:32:02.672031 23118544486528 run.py:483] Algo bellman_ford step 8290 current loss 0.004729, current_train_items 265312.
I0304 19:32:02.688534 23118544486528 run.py:483] Algo bellman_ford step 8291 current loss 0.007978, current_train_items 265344.
I0304 19:32:02.713729 23118544486528 run.py:483] Algo bellman_ford step 8292 current loss 0.025296, current_train_items 265376.
I0304 19:32:02.745026 23118544486528 run.py:483] Algo bellman_ford step 8293 current loss 0.041083, current_train_items 265408.
I0304 19:32:02.781464 23118544486528 run.py:483] Algo bellman_ford step 8294 current loss 0.060167, current_train_items 265440.
I0304 19:32:02.801343 23118544486528 run.py:483] Algo bellman_ford step 8295 current loss 0.020965, current_train_items 265472.
I0304 19:32:02.817906 23118544486528 run.py:483] Algo bellman_ford step 8296 current loss 0.034562, current_train_items 265504.
I0304 19:32:02.843591 23118544486528 run.py:483] Algo bellman_ford step 8297 current loss 0.046721, current_train_items 265536.
I0304 19:32:02.875856 23118544486528 run.py:483] Algo bellman_ford step 8298 current loss 0.036265, current_train_items 265568.
I0304 19:32:02.909656 23118544486528 run.py:483] Algo bellman_ford step 8299 current loss 0.048204, current_train_items 265600.
I0304 19:32:02.929778 23118544486528 run.py:483] Algo bellman_ford step 8300 current loss 0.017554, current_train_items 265632.
I0304 19:32:02.937779 23118544486528 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0304 19:32:02.937886 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:02.954716 23118544486528 run.py:483] Algo bellman_ford step 8301 current loss 0.006165, current_train_items 265664.
I0304 19:32:02.980879 23118544486528 run.py:483] Algo bellman_ford step 8302 current loss 0.082994, current_train_items 265696.
I0304 19:32:03.014052 23118544486528 run.py:483] Algo bellman_ford step 8303 current loss 0.064023, current_train_items 265728.
I0304 19:32:03.048093 23118544486528 run.py:483] Algo bellman_ford step 8304 current loss 0.069259, current_train_items 265760.
I0304 19:32:03.068192 23118544486528 run.py:483] Algo bellman_ford step 8305 current loss 0.005820, current_train_items 265792.
I0304 19:32:03.084055 23118544486528 run.py:483] Algo bellman_ford step 8306 current loss 0.015488, current_train_items 265824.
I0304 19:32:03.108937 23118544486528 run.py:483] Algo bellman_ford step 8307 current loss 0.067537, current_train_items 265856.
I0304 19:32:03.139025 23118544486528 run.py:483] Algo bellman_ford step 8308 current loss 0.050435, current_train_items 265888.
I0304 19:32:03.175095 23118544486528 run.py:483] Algo bellman_ford step 8309 current loss 0.146833, current_train_items 265920.
I0304 19:32:03.194990 23118544486528 run.py:483] Algo bellman_ford step 8310 current loss 0.006230, current_train_items 265952.
I0304 19:32:03.211157 23118544486528 run.py:483] Algo bellman_ford step 8311 current loss 0.012086, current_train_items 265984.
I0304 19:32:03.235671 23118544486528 run.py:483] Algo bellman_ford step 8312 current loss 0.053482, current_train_items 266016.
I0304 19:32:03.267484 23118544486528 run.py:483] Algo bellman_ford step 8313 current loss 0.032070, current_train_items 266048.
I0304 19:32:03.299801 23118544486528 run.py:483] Algo bellman_ford step 8314 current loss 0.087177, current_train_items 266080.
I0304 19:32:03.319350 23118544486528 run.py:483] Algo bellman_ford step 8315 current loss 0.004101, current_train_items 266112.
I0304 19:32:03.336047 23118544486528 run.py:483] Algo bellman_ford step 8316 current loss 0.022432, current_train_items 266144.
I0304 19:32:03.360166 23118544486528 run.py:483] Algo bellman_ford step 8317 current loss 0.044383, current_train_items 266176.
I0304 19:32:03.390643 23118544486528 run.py:483] Algo bellman_ford step 8318 current loss 0.031346, current_train_items 266208.
I0304 19:32:03.424499 23118544486528 run.py:483] Algo bellman_ford step 8319 current loss 0.050877, current_train_items 266240.
I0304 19:32:03.444227 23118544486528 run.py:483] Algo bellman_ford step 8320 current loss 0.007626, current_train_items 266272.
I0304 19:32:03.460900 23118544486528 run.py:483] Algo bellman_ford step 8321 current loss 0.017223, current_train_items 266304.
I0304 19:32:03.486222 23118544486528 run.py:483] Algo bellman_ford step 8322 current loss 0.035985, current_train_items 266336.
I0304 19:32:03.517942 23118544486528 run.py:483] Algo bellman_ford step 8323 current loss 0.040088, current_train_items 266368.
I0304 19:32:03.553296 23118544486528 run.py:483] Algo bellman_ford step 8324 current loss 0.041653, current_train_items 266400.
I0304 19:32:03.573147 23118544486528 run.py:483] Algo bellman_ford step 8325 current loss 0.005023, current_train_items 266432.
I0304 19:32:03.589060 23118544486528 run.py:483] Algo bellman_ford step 8326 current loss 0.012725, current_train_items 266464.
I0304 19:32:03.613669 23118544486528 run.py:483] Algo bellman_ford step 8327 current loss 0.046971, current_train_items 266496.
I0304 19:32:03.644787 23118544486528 run.py:483] Algo bellman_ford step 8328 current loss 0.146886, current_train_items 266528.
I0304 19:32:03.679982 23118544486528 run.py:483] Algo bellman_ford step 8329 current loss 0.074753, current_train_items 266560.
I0304 19:32:03.699558 23118544486528 run.py:483] Algo bellman_ford step 8330 current loss 0.010966, current_train_items 266592.
I0304 19:32:03.715798 23118544486528 run.py:483] Algo bellman_ford step 8331 current loss 0.042550, current_train_items 266624.
I0304 19:32:03.739532 23118544486528 run.py:483] Algo bellman_ford step 8332 current loss 0.014642, current_train_items 266656.
I0304 19:32:03.772557 23118544486528 run.py:483] Algo bellman_ford step 8333 current loss 0.072025, current_train_items 266688.
I0304 19:32:03.806327 23118544486528 run.py:483] Algo bellman_ford step 8334 current loss 0.074536, current_train_items 266720.
I0304 19:32:03.825928 23118544486528 run.py:483] Algo bellman_ford step 8335 current loss 0.022421, current_train_items 266752.
I0304 19:32:03.842216 23118544486528 run.py:483] Algo bellman_ford step 8336 current loss 0.033584, current_train_items 266784.
I0304 19:32:03.867698 23118544486528 run.py:483] Algo bellman_ford step 8337 current loss 0.022399, current_train_items 266816.
I0304 19:32:03.899695 23118544486528 run.py:483] Algo bellman_ford step 8338 current loss 0.084715, current_train_items 266848.
I0304 19:32:03.932711 23118544486528 run.py:483] Algo bellman_ford step 8339 current loss 0.064665, current_train_items 266880.
I0304 19:32:03.952317 23118544486528 run.py:483] Algo bellman_ford step 8340 current loss 0.006573, current_train_items 266912.
I0304 19:32:03.968527 23118544486528 run.py:483] Algo bellman_ford step 8341 current loss 0.008299, current_train_items 266944.
I0304 19:32:03.992717 23118544486528 run.py:483] Algo bellman_ford step 8342 current loss 0.047012, current_train_items 266976.
I0304 19:32:04.023728 23118544486528 run.py:483] Algo bellman_ford step 8343 current loss 0.057498, current_train_items 267008.
I0304 19:32:04.058857 23118544486528 run.py:483] Algo bellman_ford step 8344 current loss 0.055748, current_train_items 267040.
I0304 19:32:04.078529 23118544486528 run.py:483] Algo bellman_ford step 8345 current loss 0.007398, current_train_items 267072.
I0304 19:32:04.094969 23118544486528 run.py:483] Algo bellman_ford step 8346 current loss 0.009741, current_train_items 267104.
I0304 19:32:04.120149 23118544486528 run.py:483] Algo bellman_ford step 8347 current loss 0.063114, current_train_items 267136.
I0304 19:32:04.152933 23118544486528 run.py:483] Algo bellman_ford step 8348 current loss 0.043428, current_train_items 267168.
I0304 19:32:04.188190 23118544486528 run.py:483] Algo bellman_ford step 8349 current loss 0.091732, current_train_items 267200.
I0304 19:32:04.207829 23118544486528 run.py:483] Algo bellman_ford step 8350 current loss 0.004886, current_train_items 267232.
I0304 19:32:04.215986 23118544486528 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0304 19:32:04.216120 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:04.233138 23118544486528 run.py:483] Algo bellman_ford step 8351 current loss 0.014824, current_train_items 267264.
I0304 19:32:04.258444 23118544486528 run.py:483] Algo bellman_ford step 8352 current loss 0.041390, current_train_items 267296.
I0304 19:32:04.291520 23118544486528 run.py:483] Algo bellman_ford step 8353 current loss 0.018438, current_train_items 267328.
I0304 19:32:04.327916 23118544486528 run.py:483] Algo bellman_ford step 8354 current loss 0.090920, current_train_items 267360.
I0304 19:32:04.347965 23118544486528 run.py:483] Algo bellman_ford step 8355 current loss 0.005795, current_train_items 267392.
I0304 19:32:04.363786 23118544486528 run.py:483] Algo bellman_ford step 8356 current loss 0.040027, current_train_items 267424.
I0304 19:32:04.388482 23118544486528 run.py:483] Algo bellman_ford step 8357 current loss 0.033584, current_train_items 267456.
I0304 19:32:04.420841 23118544486528 run.py:483] Algo bellman_ford step 8358 current loss 0.053841, current_train_items 267488.
I0304 19:32:04.454045 23118544486528 run.py:483] Algo bellman_ford step 8359 current loss 0.088043, current_train_items 267520.
I0304 19:32:04.474359 23118544486528 run.py:483] Algo bellman_ford step 8360 current loss 0.003387, current_train_items 267552.
I0304 19:32:04.490526 23118544486528 run.py:483] Algo bellman_ford step 8361 current loss 0.017373, current_train_items 267584.
I0304 19:32:04.515030 23118544486528 run.py:483] Algo bellman_ford step 8362 current loss 0.031181, current_train_items 267616.
I0304 19:32:04.546796 23118544486528 run.py:483] Algo bellman_ford step 8363 current loss 0.051092, current_train_items 267648.
I0304 19:32:04.581540 23118544486528 run.py:483] Algo bellman_ford step 8364 current loss 0.078540, current_train_items 267680.
I0304 19:32:04.601480 23118544486528 run.py:483] Algo bellman_ford step 8365 current loss 0.007419, current_train_items 267712.
I0304 19:32:04.618183 23118544486528 run.py:483] Algo bellman_ford step 8366 current loss 0.012283, current_train_items 267744.
I0304 19:32:04.642082 23118544486528 run.py:483] Algo bellman_ford step 8367 current loss 0.027436, current_train_items 267776.
I0304 19:32:04.674643 23118544486528 run.py:483] Algo bellman_ford step 8368 current loss 0.049495, current_train_items 267808.
I0304 19:32:04.711172 23118544486528 run.py:483] Algo bellman_ford step 8369 current loss 0.070190, current_train_items 267840.
I0304 19:32:04.731264 23118544486528 run.py:483] Algo bellman_ford step 8370 current loss 0.004824, current_train_items 267872.
I0304 19:32:04.747918 23118544486528 run.py:483] Algo bellman_ford step 8371 current loss 0.029951, current_train_items 267904.
I0304 19:32:04.771639 23118544486528 run.py:483] Algo bellman_ford step 8372 current loss 0.091408, current_train_items 267936.
I0304 19:32:04.804114 23118544486528 run.py:483] Algo bellman_ford step 8373 current loss 0.041950, current_train_items 267968.
I0304 19:32:04.839311 23118544486528 run.py:483] Algo bellman_ford step 8374 current loss 0.042296, current_train_items 268000.
I0304 19:32:04.859647 23118544486528 run.py:483] Algo bellman_ford step 8375 current loss 0.006777, current_train_items 268032.
I0304 19:32:04.875697 23118544486528 run.py:483] Algo bellman_ford step 8376 current loss 0.009608, current_train_items 268064.
I0304 19:32:04.899644 23118544486528 run.py:483] Algo bellman_ford step 8377 current loss 0.025892, current_train_items 268096.
I0304 19:32:04.932006 23118544486528 run.py:483] Algo bellman_ford step 8378 current loss 0.043545, current_train_items 268128.
I0304 19:32:04.967179 23118544486528 run.py:483] Algo bellman_ford step 8379 current loss 0.060534, current_train_items 268160.
I0304 19:32:04.987220 23118544486528 run.py:483] Algo bellman_ford step 8380 current loss 0.025832, current_train_items 268192.
I0304 19:32:05.003670 23118544486528 run.py:483] Algo bellman_ford step 8381 current loss 0.009711, current_train_items 268224.
I0304 19:32:05.027420 23118544486528 run.py:483] Algo bellman_ford step 8382 current loss 0.047567, current_train_items 268256.
I0304 19:32:05.060245 23118544486528 run.py:483] Algo bellman_ford step 8383 current loss 0.074927, current_train_items 268288.
I0304 19:32:05.093492 23118544486528 run.py:483] Algo bellman_ford step 8384 current loss 0.037765, current_train_items 268320.
I0304 19:32:05.113634 23118544486528 run.py:483] Algo bellman_ford step 8385 current loss 0.008620, current_train_items 268352.
I0304 19:32:05.129611 23118544486528 run.py:483] Algo bellman_ford step 8386 current loss 0.015978, current_train_items 268384.
I0304 19:32:05.153419 23118544486528 run.py:483] Algo bellman_ford step 8387 current loss 0.039985, current_train_items 268416.
I0304 19:32:05.186014 23118544486528 run.py:483] Algo bellman_ford step 8388 current loss 0.059776, current_train_items 268448.
I0304 19:32:05.219815 23118544486528 run.py:483] Algo bellman_ford step 8389 current loss 0.070825, current_train_items 268480.
I0304 19:32:05.240186 23118544486528 run.py:483] Algo bellman_ford step 8390 current loss 0.004869, current_train_items 268512.
I0304 19:32:05.256790 23118544486528 run.py:483] Algo bellman_ford step 8391 current loss 0.007638, current_train_items 268544.
I0304 19:32:05.280164 23118544486528 run.py:483] Algo bellman_ford step 8392 current loss 0.047581, current_train_items 268576.
I0304 19:32:05.313634 23118544486528 run.py:483] Algo bellman_ford step 8393 current loss 0.061546, current_train_items 268608.
I0304 19:32:05.346625 23118544486528 run.py:483] Algo bellman_ford step 8394 current loss 0.044382, current_train_items 268640.
I0304 19:32:05.366756 23118544486528 run.py:483] Algo bellman_ford step 8395 current loss 0.008328, current_train_items 268672.
I0304 19:32:05.383357 23118544486528 run.py:483] Algo bellman_ford step 8396 current loss 0.026295, current_train_items 268704.
I0304 19:32:05.407783 23118544486528 run.py:483] Algo bellman_ford step 8397 current loss 0.048948, current_train_items 268736.
I0304 19:32:05.440737 23118544486528 run.py:483] Algo bellman_ford step 8398 current loss 0.091111, current_train_items 268768.
I0304 19:32:05.475875 23118544486528 run.py:483] Algo bellman_ford step 8399 current loss 0.080673, current_train_items 268800.
I0304 19:32:05.496307 23118544486528 run.py:483] Algo bellman_ford step 8400 current loss 0.002869, current_train_items 268832.
I0304 19:32:05.503887 23118544486528 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0304 19:32:05.503994 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0304 19:32:05.521241 23118544486528 run.py:483] Algo bellman_ford step 8401 current loss 0.022154, current_train_items 268864.
I0304 19:32:05.546853 23118544486528 run.py:483] Algo bellman_ford step 8402 current loss 0.034753, current_train_items 268896.
I0304 19:32:05.580415 23118544486528 run.py:483] Algo bellman_ford step 8403 current loss 0.043262, current_train_items 268928.
I0304 19:32:05.617377 23118544486528 run.py:483] Algo bellman_ford step 8404 current loss 0.090380, current_train_items 268960.
I0304 19:32:05.637185 23118544486528 run.py:483] Algo bellman_ford step 8405 current loss 0.003448, current_train_items 268992.
I0304 19:32:05.653366 23118544486528 run.py:483] Algo bellman_ford step 8406 current loss 0.025630, current_train_items 269024.
I0304 19:32:05.677494 23118544486528 run.py:483] Algo bellman_ford step 8407 current loss 0.041402, current_train_items 269056.
I0304 19:32:05.708969 23118544486528 run.py:483] Algo bellman_ford step 8408 current loss 0.051997, current_train_items 269088.
I0304 19:32:05.744109 23118544486528 run.py:483] Algo bellman_ford step 8409 current loss 0.053210, current_train_items 269120.
I0304 19:32:05.763898 23118544486528 run.py:483] Algo bellman_ford step 8410 current loss 0.004255, current_train_items 269152.
I0304 19:32:05.780532 23118544486528 run.py:483] Algo bellman_ford step 8411 current loss 0.009798, current_train_items 269184.
I0304 19:32:05.805341 23118544486528 run.py:483] Algo bellman_ford step 8412 current loss 0.061914, current_train_items 269216.
I0304 19:32:05.838720 23118544486528 run.py:483] Algo bellman_ford step 8413 current loss 0.054931, current_train_items 269248.
I0304 19:32:05.873771 23118544486528 run.py:483] Algo bellman_ford step 8414 current loss 0.047615, current_train_items 269280.
I0304 19:32:05.893702 23118544486528 run.py:483] Algo bellman_ford step 8415 current loss 0.007296, current_train_items 269312.
I0304 19:32:05.910227 23118544486528 run.py:483] Algo bellman_ford step 8416 current loss 0.009938, current_train_items 269344.
I0304 19:32:05.934978 23118544486528 run.py:483] Algo bellman_ford step 8417 current loss 0.077298, current_train_items 269376.
I0304 19:32:05.968086 23118544486528 run.py:483] Algo bellman_ford step 8418 current loss 0.027467, current_train_items 269408.
I0304 19:32:06.005452 23118544486528 run.py:483] Algo bellman_ford step 8419 current loss 0.121112, current_train_items 269440.
I0304 19:32:06.025034 23118544486528 run.py:483] Algo bellman_ford step 8420 current loss 0.010565, current_train_items 269472.
I0304 19:32:06.041579 23118544486528 run.py:483] Algo bellman_ford step 8421 current loss 0.037144, current_train_items 269504.
I0304 19:32:06.067253 23118544486528 run.py:483] Algo bellman_ford step 8422 current loss 0.043729, current_train_items 269536.
I0304 19:32:06.099865 23118544486528 run.py:483] Algo bellman_ford step 8423 current loss 0.051013, current_train_items 269568.
I0304 19:32:06.135034 23118544486528 run.py:483] Algo bellman_ford step 8424 current loss 0.041348, current_train_items 269600.
I0304 19:32:06.154953 23118544486528 run.py:483] Algo bellman_ford step 8425 current loss 0.011455, current_train_items 269632.
I0304 19:32:06.171264 23118544486528 run.py:483] Algo bellman_ford step 8426 current loss 0.024791, current_train_items 269664.
I0304 19:32:06.195749 23118544486528 run.py:483] Algo bellman_ford step 8427 current loss 0.029507, current_train_items 269696.
I0304 19:32:06.227610 23118544486528 run.py:483] Algo bellman_ford step 8428 current loss 0.060936, current_train_items 269728.
I0304 19:32:06.261712 23118544486528 run.py:483] Algo bellman_ford step 8429 current loss 0.039595, current_train_items 269760.
I0304 19:32:06.281655 23118544486528 run.py:483] Algo bellman_ford step 8430 current loss 0.008296, current_train_items 269792.
I0304 19:32:06.298731 23118544486528 run.py:483] Algo bellman_ford step 8431 current loss 0.016232, current_train_items 269824.
I0304 19:32:06.322770 23118544486528 run.py:483] Algo bellman_ford step 8432 current loss 0.030595, current_train_items 269856.
I0304 19:32:06.356357 23118544486528 run.py:483] Algo bellman_ford step 8433 current loss 0.132532, current_train_items 269888.
I0304 19:32:06.389220 23118544486528 run.py:483] Algo bellman_ford step 8434 current loss 0.043389, current_train_items 269920.
I0304 19:32:06.408679 23118544486528 run.py:483] Algo bellman_ford step 8435 current loss 0.002462, current_train_items 269952.
I0304 19:32:06.424849 23118544486528 run.py:483] Algo bellman_ford step 8436 current loss 0.032538, current_train_items 269984.
I0304 19:32:06.449132 23118544486528 run.py:483] Algo bellman_ford step 8437 current loss 0.048148, current_train_items 270016.
I0304 19:32:06.481595 23118544486528 run.py:483] Algo bellman_ford step 8438 current loss 0.017307, current_train_items 270048.
I0304 19:32:06.514226 23118544486528 run.py:483] Algo bellman_ford step 8439 current loss 0.034609, current_train_items 270080.
I0304 19:32:06.534268 23118544486528 run.py:483] Algo bellman_ford step 8440 current loss 0.002597, current_train_items 270112.
I0304 19:32:06.550880 23118544486528 run.py:483] Algo bellman_ford step 8441 current loss 0.006769, current_train_items 270144.
I0304 19:32:06.575486 23118544486528 run.py:483] Algo bellman_ford step 8442 current loss 0.017946, current_train_items 270176.
I0304 19:32:06.607645 23118544486528 run.py:483] Algo bellman_ford step 8443 current loss 0.037763, current_train_items 270208.
I0304 19:32:06.642580 23118544486528 run.py:483] Algo bellman_ford step 8444 current loss 0.032834, current_train_items 270240.
I0304 19:32:06.662110 23118544486528 run.py:483] Algo bellman_ford step 8445 current loss 0.001905, current_train_items 270272.
I0304 19:32:06.678376 23118544486528 run.py:483] Algo bellman_ford step 8446 current loss 0.011254, current_train_items 270304.
I0304 19:32:06.702733 23118544486528 run.py:483] Algo bellman_ford step 8447 current loss 0.041765, current_train_items 270336.
I0304 19:32:06.734491 23118544486528 run.py:483] Algo bellman_ford step 8448 current loss 0.035880, current_train_items 270368.
I0304 19:32:06.769062 23118544486528 run.py:483] Algo bellman_ford step 8449 current loss 0.052093, current_train_items 270400.
I0304 19:32:06.788788 23118544486528 run.py:483] Algo bellman_ford step 8450 current loss 0.005012, current_train_items 270432.
I0304 19:32:06.796752 23118544486528 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0304 19:32:06.796859 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:06.813609 23118544486528 run.py:483] Algo bellman_ford step 8451 current loss 0.059700, current_train_items 270464.
I0304 19:32:06.839006 23118544486528 run.py:483] Algo bellman_ford step 8452 current loss 0.025559, current_train_items 270496.
I0304 19:32:06.873028 23118544486528 run.py:483] Algo bellman_ford step 8453 current loss 0.054173, current_train_items 270528.
I0304 19:32:06.908798 23118544486528 run.py:483] Algo bellman_ford step 8454 current loss 0.053071, current_train_items 270560.
I0304 19:32:06.928807 23118544486528 run.py:483] Algo bellman_ford step 8455 current loss 0.003536, current_train_items 270592.
I0304 19:32:06.945313 23118544486528 run.py:483] Algo bellman_ford step 8456 current loss 0.009371, current_train_items 270624.
I0304 19:32:06.970631 23118544486528 run.py:483] Algo bellman_ford step 8457 current loss 0.034344, current_train_items 270656.
I0304 19:32:07.002046 23118544486528 run.py:483] Algo bellman_ford step 8458 current loss 0.060288, current_train_items 270688.
I0304 19:32:07.033864 23118544486528 run.py:483] Algo bellman_ford step 8459 current loss 0.035179, current_train_items 270720.
I0304 19:32:07.054284 23118544486528 run.py:483] Algo bellman_ford step 8460 current loss 0.019061, current_train_items 270752.
I0304 19:32:07.070583 23118544486528 run.py:483] Algo bellman_ford step 8461 current loss 0.005416, current_train_items 270784.
I0304 19:32:07.093694 23118544486528 run.py:483] Algo bellman_ford step 8462 current loss 0.036085, current_train_items 270816.
I0304 19:32:07.124962 23118544486528 run.py:483] Algo bellman_ford step 8463 current loss 0.053604, current_train_items 270848.
I0304 19:32:07.160481 23118544486528 run.py:483] Algo bellman_ford step 8464 current loss 0.056888, current_train_items 270880.
I0304 19:32:07.180252 23118544486528 run.py:483] Algo bellman_ford step 8465 current loss 0.004020, current_train_items 270912.
I0304 19:32:07.197095 23118544486528 run.py:483] Algo bellman_ford step 8466 current loss 0.058212, current_train_items 270944.
I0304 19:32:07.221175 23118544486528 run.py:483] Algo bellman_ford step 8467 current loss 0.019670, current_train_items 270976.
I0304 19:32:07.253052 23118544486528 run.py:483] Algo bellman_ford step 8468 current loss 0.028449, current_train_items 271008.
I0304 19:32:07.286690 23118544486528 run.py:483] Algo bellman_ford step 8469 current loss 0.043959, current_train_items 271040.
I0304 19:32:07.306928 23118544486528 run.py:483] Algo bellman_ford step 8470 current loss 0.004902, current_train_items 271072.
I0304 19:32:07.323802 23118544486528 run.py:483] Algo bellman_ford step 8471 current loss 0.051863, current_train_items 271104.
I0304 19:32:07.347728 23118544486528 run.py:483] Algo bellman_ford step 8472 current loss 0.018402, current_train_items 271136.
I0304 19:32:07.378108 23118544486528 run.py:483] Algo bellman_ford step 8473 current loss 0.028819, current_train_items 271168.
I0304 19:32:07.412732 23118544486528 run.py:483] Algo bellman_ford step 8474 current loss 0.049247, current_train_items 271200.
I0304 19:32:07.433274 23118544486528 run.py:483] Algo bellman_ford step 8475 current loss 0.005512, current_train_items 271232.
I0304 19:32:07.450023 23118544486528 run.py:483] Algo bellman_ford step 8476 current loss 0.023778, current_train_items 271264.
I0304 19:32:07.475667 23118544486528 run.py:483] Algo bellman_ford step 8477 current loss 0.088154, current_train_items 271296.
I0304 19:32:07.508180 23118544486528 run.py:483] Algo bellman_ford step 8478 current loss 0.044533, current_train_items 271328.
I0304 19:32:07.543549 23118544486528 run.py:483] Algo bellman_ford step 8479 current loss 0.045886, current_train_items 271360.
I0304 19:32:07.563152 23118544486528 run.py:483] Algo bellman_ford step 8480 current loss 0.005961, current_train_items 271392.
I0304 19:32:07.579650 23118544486528 run.py:483] Algo bellman_ford step 8481 current loss 0.060312, current_train_items 271424.
I0304 19:32:07.604370 23118544486528 run.py:483] Algo bellman_ford step 8482 current loss 0.088628, current_train_items 271456.
I0304 19:32:07.636720 23118544486528 run.py:483] Algo bellman_ford step 8483 current loss 0.054096, current_train_items 271488.
I0304 19:32:07.669717 23118544486528 run.py:483] Algo bellman_ford step 8484 current loss 0.044194, current_train_items 271520.
I0304 19:32:07.690076 23118544486528 run.py:483] Algo bellman_ford step 8485 current loss 0.007051, current_train_items 271552.
I0304 19:32:07.706191 23118544486528 run.py:483] Algo bellman_ford step 8486 current loss 0.015012, current_train_items 271584.
I0304 19:32:07.730447 23118544486528 run.py:483] Algo bellman_ford step 8487 current loss 0.041414, current_train_items 271616.
I0304 19:32:07.763859 23118544486528 run.py:483] Algo bellman_ford step 8488 current loss 0.118386, current_train_items 271648.
I0304 19:32:07.798270 23118544486528 run.py:483] Algo bellman_ford step 8489 current loss 0.086693, current_train_items 271680.
I0304 19:32:07.818722 23118544486528 run.py:483] Algo bellman_ford step 8490 current loss 0.011931, current_train_items 271712.
I0304 19:32:07.835129 23118544486528 run.py:483] Algo bellman_ford step 8491 current loss 0.043355, current_train_items 271744.
I0304 19:32:07.860275 23118544486528 run.py:483] Algo bellman_ford step 8492 current loss 0.038179, current_train_items 271776.
I0304 19:32:07.892174 23118544486528 run.py:483] Algo bellman_ford step 8493 current loss 0.048774, current_train_items 271808.
I0304 19:32:07.925169 23118544486528 run.py:483] Algo bellman_ford step 8494 current loss 0.081573, current_train_items 271840.
I0304 19:32:07.945404 23118544486528 run.py:483] Algo bellman_ford step 8495 current loss 0.008236, current_train_items 271872.
I0304 19:32:07.962188 23118544486528 run.py:483] Algo bellman_ford step 8496 current loss 0.065426, current_train_items 271904.
I0304 19:32:07.986552 23118544486528 run.py:483] Algo bellman_ford step 8497 current loss 0.019143, current_train_items 271936.
I0304 19:32:08.019469 23118544486528 run.py:483] Algo bellman_ford step 8498 current loss 0.079517, current_train_items 271968.
I0304 19:32:08.052292 23118544486528 run.py:483] Algo bellman_ford step 8499 current loss 0.051562, current_train_items 272000.
I0304 19:32:08.072361 23118544486528 run.py:483] Algo bellman_ford step 8500 current loss 0.003698, current_train_items 272032.
I0304 19:32:08.080424 23118544486528 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0304 19:32:08.080530 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:08.098182 23118544486528 run.py:483] Algo bellman_ford step 8501 current loss 0.018958, current_train_items 272064.
I0304 19:32:08.123762 23118544486528 run.py:483] Algo bellman_ford step 8502 current loss 0.037461, current_train_items 272096.
I0304 19:32:08.155679 23118544486528 run.py:483] Algo bellman_ford step 8503 current loss 0.038793, current_train_items 272128.
I0304 19:32:08.189420 23118544486528 run.py:483] Algo bellman_ford step 8504 current loss 0.057411, current_train_items 272160.
I0304 19:32:08.209411 23118544486528 run.py:483] Algo bellman_ford step 8505 current loss 0.003483, current_train_items 272192.
I0304 19:32:08.224853 23118544486528 run.py:483] Algo bellman_ford step 8506 current loss 0.004305, current_train_items 272224.
I0304 19:32:08.248109 23118544486528 run.py:483] Algo bellman_ford step 8507 current loss 0.039504, current_train_items 272256.
I0304 19:32:08.280915 23118544486528 run.py:483] Algo bellman_ford step 8508 current loss 0.041300, current_train_items 272288.
I0304 19:32:08.316346 23118544486528 run.py:483] Algo bellman_ford step 8509 current loss 0.073176, current_train_items 272320.
I0304 19:32:08.336338 23118544486528 run.py:483] Algo bellman_ford step 8510 current loss 0.002453, current_train_items 272352.
I0304 19:32:08.352163 23118544486528 run.py:483] Algo bellman_ford step 8511 current loss 0.030093, current_train_items 272384.
I0304 19:32:08.377725 23118544486528 run.py:483] Algo bellman_ford step 8512 current loss 0.034458, current_train_items 272416.
I0304 19:32:08.409490 23118544486528 run.py:483] Algo bellman_ford step 8513 current loss 0.040138, current_train_items 272448.
I0304 19:32:08.444568 23118544486528 run.py:483] Algo bellman_ford step 8514 current loss 0.046080, current_train_items 272480.
I0304 19:32:08.464764 23118544486528 run.py:483] Algo bellman_ford step 8515 current loss 0.005498, current_train_items 272512.
I0304 19:32:08.481545 23118544486528 run.py:483] Algo bellman_ford step 8516 current loss 0.016520, current_train_items 272544.
I0304 19:32:08.506070 23118544486528 run.py:483] Algo bellman_ford step 8517 current loss 0.128721, current_train_items 272576.
I0304 19:32:08.539734 23118544486528 run.py:483] Algo bellman_ford step 8518 current loss 0.096876, current_train_items 272608.
I0304 19:32:08.573437 23118544486528 run.py:483] Algo bellman_ford step 8519 current loss 0.085255, current_train_items 272640.
I0304 19:32:08.593110 23118544486528 run.py:483] Algo bellman_ford step 8520 current loss 0.004515, current_train_items 272672.
I0304 19:32:08.609269 23118544486528 run.py:483] Algo bellman_ford step 8521 current loss 0.005268, current_train_items 272704.
I0304 19:32:08.633799 23118544486528 run.py:483] Algo bellman_ford step 8522 current loss 0.050055, current_train_items 272736.
I0304 19:32:08.666215 23118544486528 run.py:483] Algo bellman_ford step 8523 current loss 0.057412, current_train_items 272768.
I0304 19:32:08.701385 23118544486528 run.py:483] Algo bellman_ford step 8524 current loss 0.098867, current_train_items 272800.
I0304 19:32:08.721385 23118544486528 run.py:483] Algo bellman_ford step 8525 current loss 0.004391, current_train_items 272832.
I0304 19:32:08.737822 23118544486528 run.py:483] Algo bellman_ford step 8526 current loss 0.019274, current_train_items 272864.
I0304 19:32:08.762453 23118544486528 run.py:483] Algo bellman_ford step 8527 current loss 0.060910, current_train_items 272896.
I0304 19:32:08.794373 23118544486528 run.py:483] Algo bellman_ford step 8528 current loss 0.047221, current_train_items 272928.
I0304 19:32:08.829905 23118544486528 run.py:483] Algo bellman_ford step 8529 current loss 0.125202, current_train_items 272960.
I0304 19:32:08.849833 23118544486528 run.py:483] Algo bellman_ford step 8530 current loss 0.005069, current_train_items 272992.
I0304 19:32:08.866240 23118544486528 run.py:483] Algo bellman_ford step 8531 current loss 0.009765, current_train_items 273024.
I0304 19:32:08.889622 23118544486528 run.py:483] Algo bellman_ford step 8532 current loss 0.028458, current_train_items 273056.
I0304 19:32:08.921537 23118544486528 run.py:483] Algo bellman_ford step 8533 current loss 0.042101, current_train_items 273088.
I0304 19:32:08.955134 23118544486528 run.py:483] Algo bellman_ford step 8534 current loss 0.066042, current_train_items 273120.
I0304 19:32:08.974701 23118544486528 run.py:483] Algo bellman_ford step 8535 current loss 0.003696, current_train_items 273152.
I0304 19:32:08.991056 23118544486528 run.py:483] Algo bellman_ford step 8536 current loss 0.022008, current_train_items 273184.
I0304 19:32:09.015692 23118544486528 run.py:483] Algo bellman_ford step 8537 current loss 0.034138, current_train_items 273216.
I0304 19:32:09.048453 23118544486528 run.py:483] Algo bellman_ford step 8538 current loss 0.037398, current_train_items 273248.
I0304 19:32:09.084630 23118544486528 run.py:483] Algo bellman_ford step 8539 current loss 0.066701, current_train_items 273280.
I0304 19:32:09.104776 23118544486528 run.py:483] Algo bellman_ford step 8540 current loss 0.022154, current_train_items 273312.
I0304 19:32:09.121082 23118544486528 run.py:483] Algo bellman_ford step 8541 current loss 0.040629, current_train_items 273344.
I0304 19:32:09.145831 23118544486528 run.py:483] Algo bellman_ford step 8542 current loss 0.024851, current_train_items 273376.
I0304 19:32:09.178673 23118544486528 run.py:483] Algo bellman_ford step 8543 current loss 0.046337, current_train_items 273408.
I0304 19:32:09.212587 23118544486528 run.py:483] Algo bellman_ford step 8544 current loss 0.069521, current_train_items 273440.
I0304 19:32:09.232750 23118544486528 run.py:483] Algo bellman_ford step 8545 current loss 0.002926, current_train_items 273472.
I0304 19:32:09.248596 23118544486528 run.py:483] Algo bellman_ford step 8546 current loss 0.013419, current_train_items 273504.
I0304 19:32:09.273417 23118544486528 run.py:483] Algo bellman_ford step 8547 current loss 0.064797, current_train_items 273536.
I0304 19:32:09.305594 23118544486528 run.py:483] Algo bellman_ford step 8548 current loss 0.046328, current_train_items 273568.
I0304 19:32:09.340834 23118544486528 run.py:483] Algo bellman_ford step 8549 current loss 0.068648, current_train_items 273600.
I0304 19:32:09.360417 23118544486528 run.py:483] Algo bellman_ford step 8550 current loss 0.006765, current_train_items 273632.
I0304 19:32:09.368405 23118544486528 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0304 19:32:09.368511 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0304 19:32:09.385904 23118544486528 run.py:483] Algo bellman_ford step 8551 current loss 0.010814, current_train_items 273664.
I0304 19:32:09.411597 23118544486528 run.py:483] Algo bellman_ford step 8552 current loss 0.026503, current_train_items 273696.
I0304 19:32:09.444198 23118544486528 run.py:483] Algo bellman_ford step 8553 current loss 0.049922, current_train_items 273728.
I0304 19:32:09.476752 23118544486528 run.py:483] Algo bellman_ford step 8554 current loss 0.024946, current_train_items 273760.
I0304 19:32:09.496679 23118544486528 run.py:483] Algo bellman_ford step 8555 current loss 0.004670, current_train_items 273792.
I0304 19:32:09.512836 23118544486528 run.py:483] Algo bellman_ford step 8556 current loss 0.028225, current_train_items 273824.
I0304 19:32:09.536900 23118544486528 run.py:483] Algo bellman_ford step 8557 current loss 0.025655, current_train_items 273856.
I0304 19:32:09.569275 23118544486528 run.py:483] Algo bellman_ford step 8558 current loss 0.062294, current_train_items 273888.
I0304 19:32:09.604518 23118544486528 run.py:483] Algo bellman_ford step 8559 current loss 0.069135, current_train_items 273920.
I0304 19:32:09.624360 23118544486528 run.py:483] Algo bellman_ford step 8560 current loss 0.003749, current_train_items 273952.
I0304 19:32:09.641159 23118544486528 run.py:483] Algo bellman_ford step 8561 current loss 0.026366, current_train_items 273984.
I0304 19:32:09.665681 23118544486528 run.py:483] Algo bellman_ford step 8562 current loss 0.053913, current_train_items 274016.
I0304 19:32:09.699117 23118544486528 run.py:483] Algo bellman_ford step 8563 current loss 0.110864, current_train_items 274048.
I0304 19:32:09.732030 23118544486528 run.py:483] Algo bellman_ford step 8564 current loss 0.067200, current_train_items 274080.
I0304 19:32:09.751823 23118544486528 run.py:483] Algo bellman_ford step 8565 current loss 0.002687, current_train_items 274112.
I0304 19:32:09.768817 23118544486528 run.py:483] Algo bellman_ford step 8566 current loss 0.034197, current_train_items 274144.
I0304 19:32:09.794138 23118544486528 run.py:483] Algo bellman_ford step 8567 current loss 0.032293, current_train_items 274176.
I0304 19:32:09.826534 23118544486528 run.py:483] Algo bellman_ford step 8568 current loss 0.071519, current_train_items 274208.
I0304 19:32:09.861452 23118544486528 run.py:483] Algo bellman_ford step 8569 current loss 0.083497, current_train_items 274240.
I0304 19:32:09.881605 23118544486528 run.py:483] Algo bellman_ford step 8570 current loss 0.004236, current_train_items 274272.
I0304 19:32:09.897912 23118544486528 run.py:483] Algo bellman_ford step 8571 current loss 0.026416, current_train_items 274304.
I0304 19:32:09.921424 23118544486528 run.py:483] Algo bellman_ford step 8572 current loss 0.041481, current_train_items 274336.
I0304 19:32:09.953291 23118544486528 run.py:483] Algo bellman_ford step 8573 current loss 0.040325, current_train_items 274368.
I0304 19:32:09.985931 23118544486528 run.py:483] Algo bellman_ford step 8574 current loss 0.056635, current_train_items 274400.
I0304 19:32:10.005928 23118544486528 run.py:483] Algo bellman_ford step 8575 current loss 0.005438, current_train_items 274432.
I0304 19:32:10.022244 23118544486528 run.py:483] Algo bellman_ford step 8576 current loss 0.012136, current_train_items 274464.
I0304 19:32:10.046495 23118544486528 run.py:483] Algo bellman_ford step 8577 current loss 0.022785, current_train_items 274496.
I0304 19:32:10.079346 23118544486528 run.py:483] Algo bellman_ford step 8578 current loss 0.042877, current_train_items 274528.
I0304 19:32:10.113828 23118544486528 run.py:483] Algo bellman_ford step 8579 current loss 0.073887, current_train_items 274560.
I0304 19:32:10.133443 23118544486528 run.py:483] Algo bellman_ford step 8580 current loss 0.012254, current_train_items 274592.
I0304 19:32:10.149855 23118544486528 run.py:483] Algo bellman_ford step 8581 current loss 0.007006, current_train_items 274624.
I0304 19:32:10.173407 23118544486528 run.py:483] Algo bellman_ford step 8582 current loss 0.016992, current_train_items 274656.
I0304 19:32:10.205534 23118544486528 run.py:483] Algo bellman_ford step 8583 current loss 0.034444, current_train_items 274688.
I0304 19:32:10.240737 23118544486528 run.py:483] Algo bellman_ford step 8584 current loss 0.053191, current_train_items 274720.
I0304 19:32:10.260411 23118544486528 run.py:483] Algo bellman_ford step 8585 current loss 0.004183, current_train_items 274752.
I0304 19:32:10.276975 23118544486528 run.py:483] Algo bellman_ford step 8586 current loss 0.007152, current_train_items 274784.
I0304 19:32:10.302550 23118544486528 run.py:483] Algo bellman_ford step 8587 current loss 0.073778, current_train_items 274816.
I0304 19:32:10.334451 23118544486528 run.py:483] Algo bellman_ford step 8588 current loss 0.028171, current_train_items 274848.
I0304 19:32:10.367592 23118544486528 run.py:483] Algo bellman_ford step 8589 current loss 0.028461, current_train_items 274880.
I0304 19:32:10.387540 23118544486528 run.py:483] Algo bellman_ford step 8590 current loss 0.038437, current_train_items 274912.
I0304 19:32:10.403468 23118544486528 run.py:483] Algo bellman_ford step 8591 current loss 0.016621, current_train_items 274944.
I0304 19:32:10.427708 23118544486528 run.py:483] Algo bellman_ford step 8592 current loss 0.028644, current_train_items 274976.
I0304 19:32:10.460264 23118544486528 run.py:483] Algo bellman_ford step 8593 current loss 0.041685, current_train_items 275008.
I0304 19:32:10.494992 23118544486528 run.py:483] Algo bellman_ford step 8594 current loss 0.080384, current_train_items 275040.
I0304 19:32:10.514409 23118544486528 run.py:483] Algo bellman_ford step 8595 current loss 0.005082, current_train_items 275072.
I0304 19:32:10.530492 23118544486528 run.py:483] Algo bellman_ford step 8596 current loss 0.008921, current_train_items 275104.
I0304 19:32:10.554803 23118544486528 run.py:483] Algo bellman_ford step 8597 current loss 0.037719, current_train_items 275136.
I0304 19:32:10.585886 23118544486528 run.py:483] Algo bellman_ford step 8598 current loss 0.041500, current_train_items 275168.
I0304 19:32:10.620812 23118544486528 run.py:483] Algo bellman_ford step 8599 current loss 0.054099, current_train_items 275200.
I0304 19:32:10.640856 23118544486528 run.py:483] Algo bellman_ford step 8600 current loss 0.015366, current_train_items 275232.
I0304 19:32:10.648638 23118544486528 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0304 19:32:10.648753 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:10.665851 23118544486528 run.py:483] Algo bellman_ford step 8601 current loss 0.025016, current_train_items 275264.
I0304 19:32:10.691979 23118544486528 run.py:483] Algo bellman_ford step 8602 current loss 0.047821, current_train_items 275296.
I0304 19:32:10.725778 23118544486528 run.py:483] Algo bellman_ford step 8603 current loss 0.041504, current_train_items 275328.
I0304 19:32:10.760730 23118544486528 run.py:483] Algo bellman_ford step 8604 current loss 0.056549, current_train_items 275360.
I0304 19:32:10.781038 23118544486528 run.py:483] Algo bellman_ford step 8605 current loss 0.002619, current_train_items 275392.
I0304 19:32:10.796648 23118544486528 run.py:483] Algo bellman_ford step 8606 current loss 0.009330, current_train_items 275424.
I0304 19:32:10.821299 23118544486528 run.py:483] Algo bellman_ford step 8607 current loss 0.035012, current_train_items 275456.
I0304 19:32:10.853409 23118544486528 run.py:483] Algo bellman_ford step 8608 current loss 0.046470, current_train_items 275488.
I0304 19:32:10.889563 23118544486528 run.py:483] Algo bellman_ford step 8609 current loss 0.102524, current_train_items 275520.
I0304 19:32:10.909569 23118544486528 run.py:483] Algo bellman_ford step 8610 current loss 0.049055, current_train_items 275552.
I0304 19:32:10.926292 23118544486528 run.py:483] Algo bellman_ford step 8611 current loss 0.006144, current_train_items 275584.
I0304 19:32:10.951295 23118544486528 run.py:483] Algo bellman_ford step 8612 current loss 0.046966, current_train_items 275616.
I0304 19:32:10.983869 23118544486528 run.py:483] Algo bellman_ford step 8613 current loss 0.072999, current_train_items 275648.
I0304 19:32:11.016386 23118544486528 run.py:483] Algo bellman_ford step 8614 current loss 0.030102, current_train_items 275680.
I0304 19:32:11.036597 23118544486528 run.py:483] Algo bellman_ford step 8615 current loss 0.006672, current_train_items 275712.
I0304 19:32:11.053008 23118544486528 run.py:483] Algo bellman_ford step 8616 current loss 0.069055, current_train_items 275744.
I0304 19:32:11.077819 23118544486528 run.py:483] Algo bellman_ford step 8617 current loss 0.034748, current_train_items 275776.
I0304 19:32:11.110984 23118544486528 run.py:483] Algo bellman_ford step 8618 current loss 0.043705, current_train_items 275808.
I0304 19:32:11.145321 23118544486528 run.py:483] Algo bellman_ford step 8619 current loss 0.087996, current_train_items 275840.
I0304 19:32:11.165046 23118544486528 run.py:483] Algo bellman_ford step 8620 current loss 0.006185, current_train_items 275872.
I0304 19:32:11.181381 23118544486528 run.py:483] Algo bellman_ford step 8621 current loss 0.043978, current_train_items 275904.
I0304 19:32:11.206401 23118544486528 run.py:483] Algo bellman_ford step 8622 current loss 0.070163, current_train_items 275936.
I0304 19:32:11.239179 23118544486528 run.py:483] Algo bellman_ford step 8623 current loss 0.033886, current_train_items 275968.
I0304 19:32:11.273387 23118544486528 run.py:483] Algo bellman_ford step 8624 current loss 0.030834, current_train_items 276000.
I0304 19:32:11.293255 23118544486528 run.py:483] Algo bellman_ford step 8625 current loss 0.005264, current_train_items 276032.
I0304 19:32:11.309692 23118544486528 run.py:483] Algo bellman_ford step 8626 current loss 0.021448, current_train_items 276064.
I0304 19:32:11.334633 23118544486528 run.py:483] Algo bellman_ford step 8627 current loss 0.042078, current_train_items 276096.
I0304 19:32:11.368194 23118544486528 run.py:483] Algo bellman_ford step 8628 current loss 0.033965, current_train_items 276128.
I0304 19:32:11.401832 23118544486528 run.py:483] Algo bellman_ford step 8629 current loss 0.062156, current_train_items 276160.
I0304 19:32:11.422076 23118544486528 run.py:483] Algo bellman_ford step 8630 current loss 0.003026, current_train_items 276192.
I0304 19:32:11.438529 23118544486528 run.py:483] Algo bellman_ford step 8631 current loss 0.014515, current_train_items 276224.
I0304 19:32:11.462621 23118544486528 run.py:483] Algo bellman_ford step 8632 current loss 0.030657, current_train_items 276256.
I0304 19:32:11.494448 23118544486528 run.py:483] Algo bellman_ford step 8633 current loss 0.048762, current_train_items 276288.
I0304 19:32:11.528838 23118544486528 run.py:483] Algo bellman_ford step 8634 current loss 0.064550, current_train_items 276320.
I0304 19:32:11.548845 23118544486528 run.py:483] Algo bellman_ford step 8635 current loss 0.012566, current_train_items 276352.
I0304 19:32:11.565155 23118544486528 run.py:483] Algo bellman_ford step 8636 current loss 0.014568, current_train_items 276384.
I0304 19:32:11.590457 23118544486528 run.py:483] Algo bellman_ford step 8637 current loss 0.051511, current_train_items 276416.
I0304 19:32:11.622438 23118544486528 run.py:483] Algo bellman_ford step 8638 current loss 0.091639, current_train_items 276448.
I0304 19:32:11.657912 23118544486528 run.py:483] Algo bellman_ford step 8639 current loss 0.101808, current_train_items 276480.
I0304 19:32:11.677988 23118544486528 run.py:483] Algo bellman_ford step 8640 current loss 0.026631, current_train_items 276512.
I0304 19:32:11.694658 23118544486528 run.py:483] Algo bellman_ford step 8641 current loss 0.054112, current_train_items 276544.
I0304 19:32:11.719531 23118544486528 run.py:483] Algo bellman_ford step 8642 current loss 0.063737, current_train_items 276576.
I0304 19:32:11.752286 23118544486528 run.py:483] Algo bellman_ford step 8643 current loss 0.061038, current_train_items 276608.
I0304 19:32:11.786978 23118544486528 run.py:483] Algo bellman_ford step 8644 current loss 0.076741, current_train_items 276640.
I0304 19:32:11.806605 23118544486528 run.py:483] Algo bellman_ford step 8645 current loss 0.005062, current_train_items 276672.
I0304 19:32:11.823136 23118544486528 run.py:483] Algo bellman_ford step 8646 current loss 0.052923, current_train_items 276704.
I0304 19:32:11.847785 23118544486528 run.py:483] Algo bellman_ford step 8647 current loss 0.047071, current_train_items 276736.
I0304 19:32:11.881014 23118544486528 run.py:483] Algo bellman_ford step 8648 current loss 0.105131, current_train_items 276768.
I0304 19:32:11.915051 23118544486528 run.py:483] Algo bellman_ford step 8649 current loss 0.070492, current_train_items 276800.
I0304 19:32:11.935137 23118544486528 run.py:483] Algo bellman_ford step 8650 current loss 0.006847, current_train_items 276832.
I0304 19:32:11.943072 23118544486528 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0304 19:32:11.943178 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:11.960269 23118544486528 run.py:483] Algo bellman_ford step 8651 current loss 0.020596, current_train_items 276864.
I0304 19:32:11.985011 23118544486528 run.py:483] Algo bellman_ford step 8652 current loss 0.073375, current_train_items 276896.
I0304 19:32:12.017916 23118544486528 run.py:483] Algo bellman_ford step 8653 current loss 0.052562, current_train_items 276928.
I0304 19:32:12.053640 23118544486528 run.py:483] Algo bellman_ford step 8654 current loss 0.121733, current_train_items 276960.
I0304 19:32:12.073792 23118544486528 run.py:483] Algo bellman_ford step 8655 current loss 0.002679, current_train_items 276992.
I0304 19:32:12.090153 23118544486528 run.py:483] Algo bellman_ford step 8656 current loss 0.012605, current_train_items 277024.
I0304 19:32:12.115888 23118544486528 run.py:483] Algo bellman_ford step 8657 current loss 0.084887, current_train_items 277056.
I0304 19:32:12.148538 23118544486528 run.py:483] Algo bellman_ford step 8658 current loss 0.059765, current_train_items 277088.
I0304 19:32:12.182732 23118544486528 run.py:483] Algo bellman_ford step 8659 current loss 0.080222, current_train_items 277120.
I0304 19:32:12.202795 23118544486528 run.py:483] Algo bellman_ford step 8660 current loss 0.004739, current_train_items 277152.
I0304 19:32:12.219011 23118544486528 run.py:483] Algo bellman_ford step 8661 current loss 0.019051, current_train_items 277184.
I0304 19:32:12.242736 23118544486528 run.py:483] Algo bellman_ford step 8662 current loss 0.035224, current_train_items 277216.
I0304 19:32:12.274495 23118544486528 run.py:483] Algo bellman_ford step 8663 current loss 0.086609, current_train_items 277248.
I0304 19:32:12.309857 23118544486528 run.py:483] Algo bellman_ford step 8664 current loss 0.063596, current_train_items 277280.
I0304 19:32:12.329571 23118544486528 run.py:483] Algo bellman_ford step 8665 current loss 0.005893, current_train_items 277312.
I0304 19:32:12.345487 23118544486528 run.py:483] Algo bellman_ford step 8666 current loss 0.026086, current_train_items 277344.
I0304 19:32:12.369854 23118544486528 run.py:483] Algo bellman_ford step 8667 current loss 0.017644, current_train_items 277376.
I0304 19:32:12.401633 23118544486528 run.py:483] Algo bellman_ford step 8668 current loss 0.035517, current_train_items 277408.
I0304 19:32:12.434818 23118544486528 run.py:483] Algo bellman_ford step 8669 current loss 0.040101, current_train_items 277440.
I0304 19:32:12.455055 23118544486528 run.py:483] Algo bellman_ford step 8670 current loss 0.004169, current_train_items 277472.
I0304 19:32:12.471724 23118544486528 run.py:483] Algo bellman_ford step 8671 current loss 0.022908, current_train_items 277504.
I0304 19:32:12.495439 23118544486528 run.py:483] Algo bellman_ford step 8672 current loss 0.032973, current_train_items 277536.
I0304 19:32:12.527610 23118544486528 run.py:483] Algo bellman_ford step 8673 current loss 0.043197, current_train_items 277568.
I0304 19:32:12.559764 23118544486528 run.py:483] Algo bellman_ford step 8674 current loss 0.084847, current_train_items 277600.
I0304 19:32:12.579571 23118544486528 run.py:483] Algo bellman_ford step 8675 current loss 0.004655, current_train_items 277632.
I0304 19:32:12.595899 23118544486528 run.py:483] Algo bellman_ford step 8676 current loss 0.021455, current_train_items 277664.
I0304 19:32:12.620456 23118544486528 run.py:483] Algo bellman_ford step 8677 current loss 0.031868, current_train_items 277696.
I0304 19:32:12.652950 23118544486528 run.py:483] Algo bellman_ford step 8678 current loss 0.053577, current_train_items 277728.
I0304 19:32:12.686620 23118544486528 run.py:483] Algo bellman_ford step 8679 current loss 0.075173, current_train_items 277760.
I0304 19:32:12.706187 23118544486528 run.py:483] Algo bellman_ford step 8680 current loss 0.003232, current_train_items 277792.
I0304 19:32:12.722857 23118544486528 run.py:483] Algo bellman_ford step 8681 current loss 0.008844, current_train_items 277824.
I0304 19:32:12.747422 23118544486528 run.py:483] Algo bellman_ford step 8682 current loss 0.030745, current_train_items 277856.
I0304 19:32:12.779456 23118544486528 run.py:483] Algo bellman_ford step 8683 current loss 0.030843, current_train_items 277888.
I0304 19:32:12.812981 23118544486528 run.py:483] Algo bellman_ford step 8684 current loss 0.059639, current_train_items 277920.
I0304 19:32:12.832953 23118544486528 run.py:483] Algo bellman_ford step 8685 current loss 0.005638, current_train_items 277952.
I0304 19:32:12.849050 23118544486528 run.py:483] Algo bellman_ford step 8686 current loss 0.013346, current_train_items 277984.
I0304 19:32:12.873842 23118544486528 run.py:483] Algo bellman_ford step 8687 current loss 0.042942, current_train_items 278016.
I0304 19:32:12.906583 23118544486528 run.py:483] Algo bellman_ford step 8688 current loss 0.031796, current_train_items 278048.
I0304 19:32:12.939596 23118544486528 run.py:483] Algo bellman_ford step 8689 current loss 0.060605, current_train_items 278080.
I0304 19:32:12.959215 23118544486528 run.py:483] Algo bellman_ford step 8690 current loss 0.002571, current_train_items 278112.
I0304 19:32:12.975726 23118544486528 run.py:483] Algo bellman_ford step 8691 current loss 0.036677, current_train_items 278144.
I0304 19:32:12.999694 23118544486528 run.py:483] Algo bellman_ford step 8692 current loss 0.050214, current_train_items 278176.
I0304 19:32:13.031600 23118544486528 run.py:483] Algo bellman_ford step 8693 current loss 0.032619, current_train_items 278208.
I0304 19:32:13.065342 23118544486528 run.py:483] Algo bellman_ford step 8694 current loss 0.064675, current_train_items 278240.
I0304 19:32:13.085047 23118544486528 run.py:483] Algo bellman_ford step 8695 current loss 0.003071, current_train_items 278272.
I0304 19:32:13.101528 23118544486528 run.py:483] Algo bellman_ford step 8696 current loss 0.007278, current_train_items 278304.
I0304 19:32:13.126028 23118544486528 run.py:483] Algo bellman_ford step 8697 current loss 0.044341, current_train_items 278336.
I0304 19:32:13.158551 23118544486528 run.py:483] Algo bellman_ford step 8698 current loss 0.026435, current_train_items 278368.
I0304 19:32:13.194513 23118544486528 run.py:483] Algo bellman_ford step 8699 current loss 0.090750, current_train_items 278400.
I0304 19:32:13.214437 23118544486528 run.py:483] Algo bellman_ford step 8700 current loss 0.004593, current_train_items 278432.
I0304 19:32:13.222322 23118544486528 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0304 19:32:13.222427 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0304 19:32:13.239440 23118544486528 run.py:483] Algo bellman_ford step 8701 current loss 0.009864, current_train_items 278464.
I0304 19:32:13.263911 23118544486528 run.py:483] Algo bellman_ford step 8702 current loss 0.022129, current_train_items 278496.
I0304 19:32:13.297144 23118544486528 run.py:483] Algo bellman_ford step 8703 current loss 0.040920, current_train_items 278528.
I0304 19:32:13.333433 23118544486528 run.py:483] Algo bellman_ford step 8704 current loss 0.080023, current_train_items 278560.
I0304 19:32:13.353641 23118544486528 run.py:483] Algo bellman_ford step 8705 current loss 0.017009, current_train_items 278592.
I0304 19:32:13.369521 23118544486528 run.py:483] Algo bellman_ford step 8706 current loss 0.028042, current_train_items 278624.
I0304 19:32:13.394594 23118544486528 run.py:483] Algo bellman_ford step 8707 current loss 0.030908, current_train_items 278656.
I0304 19:32:13.427455 23118544486528 run.py:483] Algo bellman_ford step 8708 current loss 0.045138, current_train_items 278688.
I0304 19:32:13.461834 23118544486528 run.py:483] Algo bellman_ford step 8709 current loss 0.064154, current_train_items 278720.
I0304 19:32:13.481347 23118544486528 run.py:483] Algo bellman_ford step 8710 current loss 0.017107, current_train_items 278752.
I0304 19:32:13.497949 23118544486528 run.py:483] Algo bellman_ford step 8711 current loss 0.012545, current_train_items 278784.
I0304 19:32:13.522224 23118544486528 run.py:483] Algo bellman_ford step 8712 current loss 0.024416, current_train_items 278816.
I0304 19:32:13.556606 23118544486528 run.py:483] Algo bellman_ford step 8713 current loss 0.076334, current_train_items 278848.
I0304 19:32:13.589790 23118544486528 run.py:483] Algo bellman_ford step 8714 current loss 0.046773, current_train_items 278880.
I0304 19:32:13.609403 23118544486528 run.py:483] Algo bellman_ford step 8715 current loss 0.005199, current_train_items 278912.
I0304 19:32:13.626014 23118544486528 run.py:483] Algo bellman_ford step 8716 current loss 0.026275, current_train_items 278944.
I0304 19:32:13.650368 23118544486528 run.py:483] Algo bellman_ford step 8717 current loss 0.021593, current_train_items 278976.
I0304 19:32:13.681826 23118544486528 run.py:483] Algo bellman_ford step 8718 current loss 0.042830, current_train_items 279008.
I0304 19:32:13.714997 23118544486528 run.py:483] Algo bellman_ford step 8719 current loss 0.049094, current_train_items 279040.
I0304 19:32:13.734772 23118544486528 run.py:483] Algo bellman_ford step 8720 current loss 0.006245, current_train_items 279072.
I0304 19:32:13.751468 23118544486528 run.py:483] Algo bellman_ford step 8721 current loss 0.021177, current_train_items 279104.
I0304 19:32:13.777042 23118544486528 run.py:483] Algo bellman_ford step 8722 current loss 0.038820, current_train_items 279136.
I0304 19:32:13.808533 23118544486528 run.py:483] Algo bellman_ford step 8723 current loss 0.039746, current_train_items 279168.
I0304 19:32:13.842016 23118544486528 run.py:483] Algo bellman_ford step 8724 current loss 0.057596, current_train_items 279200.
I0304 19:32:13.862264 23118544486528 run.py:483] Algo bellman_ford step 8725 current loss 0.007386, current_train_items 279232.
I0304 19:32:13.878722 23118544486528 run.py:483] Algo bellman_ford step 8726 current loss 0.026444, current_train_items 279264.
I0304 19:32:13.903413 23118544486528 run.py:483] Algo bellman_ford step 8727 current loss 0.040373, current_train_items 279296.
I0304 19:32:13.935337 23118544486528 run.py:483] Algo bellman_ford step 8728 current loss 0.042516, current_train_items 279328.
I0304 19:32:13.969486 23118544486528 run.py:483] Algo bellman_ford step 8729 current loss 0.047913, current_train_items 279360.
I0304 19:32:13.989308 23118544486528 run.py:483] Algo bellman_ford step 8730 current loss 0.005313, current_train_items 279392.
I0304 19:32:14.005155 23118544486528 run.py:483] Algo bellman_ford step 8731 current loss 0.003546, current_train_items 279424.
I0304 19:32:14.029659 23118544486528 run.py:483] Algo bellman_ford step 8732 current loss 0.037275, current_train_items 279456.
I0304 19:32:14.062272 23118544486528 run.py:483] Algo bellman_ford step 8733 current loss 0.022816, current_train_items 279488.
I0304 19:32:14.098403 23118544486528 run.py:483] Algo bellman_ford step 8734 current loss 0.041302, current_train_items 279520.
I0304 19:32:14.118083 23118544486528 run.py:483] Algo bellman_ford step 8735 current loss 0.002588, current_train_items 279552.
I0304 19:32:14.134318 23118544486528 run.py:483] Algo bellman_ford step 8736 current loss 0.019588, current_train_items 279584.
I0304 19:32:14.159634 23118544486528 run.py:483] Algo bellman_ford step 8737 current loss 0.060852, current_train_items 279616.
I0304 19:32:14.191621 23118544486528 run.py:483] Algo bellman_ford step 8738 current loss 0.046788, current_train_items 279648.
I0304 19:32:14.226906 23118544486528 run.py:483] Algo bellman_ford step 8739 current loss 0.055178, current_train_items 279680.
I0304 19:32:14.247232 23118544486528 run.py:483] Algo bellman_ford step 8740 current loss 0.005727, current_train_items 279712.
I0304 19:32:14.263291 23118544486528 run.py:483] Algo bellman_ford step 8741 current loss 0.006261, current_train_items 279744.
I0304 19:32:14.288145 23118544486528 run.py:483] Algo bellman_ford step 8742 current loss 0.067310, current_train_items 279776.
I0304 19:32:14.321037 23118544486528 run.py:483] Algo bellman_ford step 8743 current loss 0.083042, current_train_items 279808.
I0304 19:32:14.355504 23118544486528 run.py:483] Algo bellman_ford step 8744 current loss 0.051069, current_train_items 279840.
I0304 19:32:14.375364 23118544486528 run.py:483] Algo bellman_ford step 8745 current loss 0.017119, current_train_items 279872.
I0304 19:32:14.391948 23118544486528 run.py:483] Algo bellman_ford step 8746 current loss 0.049445, current_train_items 279904.
I0304 19:32:14.416138 23118544486528 run.py:483] Algo bellman_ford step 8747 current loss 0.063194, current_train_items 279936.
I0304 19:32:14.449593 23118544486528 run.py:483] Algo bellman_ford step 8748 current loss 0.054055, current_train_items 279968.
I0304 19:32:14.485494 23118544486528 run.py:483] Algo bellman_ford step 8749 current loss 0.044769, current_train_items 280000.
I0304 19:32:14.505068 23118544486528 run.py:483] Algo bellman_ford step 8750 current loss 0.003155, current_train_items 280032.
I0304 19:32:14.512935 23118544486528 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0304 19:32:14.513040 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0304 19:32:14.530057 23118544486528 run.py:483] Algo bellman_ford step 8751 current loss 0.029476, current_train_items 280064.
I0304 19:32:14.554368 23118544486528 run.py:483] Algo bellman_ford step 8752 current loss 0.054683, current_train_items 280096.
I0304 19:32:14.587375 23118544486528 run.py:483] Algo bellman_ford step 8753 current loss 0.032833, current_train_items 280128.
I0304 19:32:14.622078 23118544486528 run.py:483] Algo bellman_ford step 8754 current loss 0.046560, current_train_items 280160.
I0304 19:32:14.642418 23118544486528 run.py:483] Algo bellman_ford step 8755 current loss 0.005611, current_train_items 280192.
I0304 19:32:14.658256 23118544486528 run.py:483] Algo bellman_ford step 8756 current loss 0.017461, current_train_items 280224.
I0304 19:32:14.683771 23118544486528 run.py:483] Algo bellman_ford step 8757 current loss 0.029083, current_train_items 280256.
I0304 19:32:14.715551 23118544486528 run.py:483] Algo bellman_ford step 8758 current loss 0.040846, current_train_items 280288.
I0304 19:32:14.749740 23118544486528 run.py:483] Algo bellman_ford step 8759 current loss 0.059902, current_train_items 280320.
I0304 19:32:14.769955 23118544486528 run.py:483] Algo bellman_ford step 8760 current loss 0.006968, current_train_items 280352.
I0304 19:32:14.787054 23118544486528 run.py:483] Algo bellman_ford step 8761 current loss 0.011544, current_train_items 280384.
I0304 19:32:14.811580 23118544486528 run.py:483] Algo bellman_ford step 8762 current loss 0.043313, current_train_items 280416.
I0304 19:32:14.843148 23118544486528 run.py:483] Algo bellman_ford step 8763 current loss 0.029842, current_train_items 280448.
I0304 19:32:14.879063 23118544486528 run.py:483] Algo bellman_ford step 8764 current loss 0.111296, current_train_items 280480.
I0304 19:32:14.898679 23118544486528 run.py:483] Algo bellman_ford step 8765 current loss 0.004273, current_train_items 280512.
I0304 19:32:14.915432 23118544486528 run.py:483] Algo bellman_ford step 8766 current loss 0.020063, current_train_items 280544.
I0304 19:32:14.938591 23118544486528 run.py:483] Algo bellman_ford step 8767 current loss 0.015071, current_train_items 280576.
I0304 19:32:14.970182 23118544486528 run.py:483] Algo bellman_ford step 8768 current loss 0.075012, current_train_items 280608.
I0304 19:32:15.002551 23118544486528 run.py:483] Algo bellman_ford step 8769 current loss 0.045509, current_train_items 280640.
I0304 19:32:15.022486 23118544486528 run.py:483] Algo bellman_ford step 8770 current loss 0.002002, current_train_items 280672.
I0304 19:32:15.039271 23118544486528 run.py:483] Algo bellman_ford step 8771 current loss 0.007057, current_train_items 280704.
I0304 19:32:15.063848 23118544486528 run.py:483] Algo bellman_ford step 8772 current loss 0.033388, current_train_items 280736.
I0304 19:32:15.096480 23118544486528 run.py:483] Algo bellman_ford step 8773 current loss 0.051478, current_train_items 280768.
I0304 19:32:15.129920 23118544486528 run.py:483] Algo bellman_ford step 8774 current loss 0.046577, current_train_items 280800.
I0304 19:32:15.149712 23118544486528 run.py:483] Algo bellman_ford step 8775 current loss 0.006045, current_train_items 280832.
I0304 19:32:15.166130 23118544486528 run.py:483] Algo bellman_ford step 8776 current loss 0.016753, current_train_items 280864.
I0304 19:32:15.189544 23118544486528 run.py:483] Algo bellman_ford step 8777 current loss 0.025921, current_train_items 280896.
I0304 19:32:15.221426 23118544486528 run.py:483] Algo bellman_ford step 8778 current loss 0.054161, current_train_items 280928.
I0304 19:32:15.256424 23118544486528 run.py:483] Algo bellman_ford step 8779 current loss 0.030063, current_train_items 280960.
I0304 19:32:15.276094 23118544486528 run.py:483] Algo bellman_ford step 8780 current loss 0.002999, current_train_items 280992.
I0304 19:32:15.292316 23118544486528 run.py:483] Algo bellman_ford step 8781 current loss 0.050528, current_train_items 281024.
I0304 19:32:15.316664 23118544486528 run.py:483] Algo bellman_ford step 8782 current loss 0.042457, current_train_items 281056.
I0304 19:32:15.349775 23118544486528 run.py:483] Algo bellman_ford step 8783 current loss 0.023199, current_train_items 281088.
I0304 19:32:15.384522 23118544486528 run.py:483] Algo bellman_ford step 8784 current loss 0.059041, current_train_items 281120.
I0304 19:32:15.404410 23118544486528 run.py:483] Algo bellman_ford step 8785 current loss 0.001741, current_train_items 281152.
I0304 19:32:15.420746 23118544486528 run.py:483] Algo bellman_ford step 8786 current loss 0.020954, current_train_items 281184.
I0304 19:32:15.445446 23118544486528 run.py:483] Algo bellman_ford step 8787 current loss 0.052389, current_train_items 281216.
I0304 19:32:15.477247 23118544486528 run.py:483] Algo bellman_ford step 8788 current loss 0.050671, current_train_items 281248.
I0304 19:32:15.511556 23118544486528 run.py:483] Algo bellman_ford step 8789 current loss 0.037454, current_train_items 281280.
I0304 19:32:15.531959 23118544486528 run.py:483] Algo bellman_ford step 8790 current loss 0.071861, current_train_items 281312.
I0304 19:32:15.548826 23118544486528 run.py:483] Algo bellman_ford step 8791 current loss 0.009440, current_train_items 281344.
I0304 19:32:15.572881 23118544486528 run.py:483] Algo bellman_ford step 8792 current loss 0.024403, current_train_items 281376.
I0304 19:32:15.605216 23118544486528 run.py:483] Algo bellman_ford step 8793 current loss 0.055433, current_train_items 281408.
I0304 19:32:15.640769 23118544486528 run.py:483] Algo bellman_ford step 8794 current loss 0.112492, current_train_items 281440.
I0304 19:32:15.660470 23118544486528 run.py:483] Algo bellman_ford step 8795 current loss 0.005203, current_train_items 281472.
I0304 19:32:15.676798 23118544486528 run.py:483] Algo bellman_ford step 8796 current loss 0.021353, current_train_items 281504.
I0304 19:32:15.700855 23118544486528 run.py:483] Algo bellman_ford step 8797 current loss 0.039445, current_train_items 281536.
I0304 19:32:15.733635 23118544486528 run.py:483] Algo bellman_ford step 8798 current loss 0.052365, current_train_items 281568.
I0304 19:32:15.769409 23118544486528 run.py:483] Algo bellman_ford step 8799 current loss 0.095166, current_train_items 281600.
I0304 19:32:15.789355 23118544486528 run.py:483] Algo bellman_ford step 8800 current loss 0.005123, current_train_items 281632.
I0304 19:32:15.797205 23118544486528 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0304 19:32:15.797311 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:15.813964 23118544486528 run.py:483] Algo bellman_ford step 8801 current loss 0.017120, current_train_items 281664.
I0304 19:32:15.838670 23118544486528 run.py:483] Algo bellman_ford step 8802 current loss 0.037500, current_train_items 281696.
I0304 19:32:15.872732 23118544486528 run.py:483] Algo bellman_ford step 8803 current loss 0.044704, current_train_items 281728.
I0304 19:32:15.907403 23118544486528 run.py:483] Algo bellman_ford step 8804 current loss 0.033226, current_train_items 281760.
I0304 19:32:15.927601 23118544486528 run.py:483] Algo bellman_ford step 8805 current loss 0.005334, current_train_items 281792.
I0304 19:32:15.943656 23118544486528 run.py:483] Algo bellman_ford step 8806 current loss 0.028730, current_train_items 281824.
I0304 19:32:15.968654 23118544486528 run.py:483] Algo bellman_ford step 8807 current loss 0.024689, current_train_items 281856.
I0304 19:32:16.000778 23118544486528 run.py:483] Algo bellman_ford step 8808 current loss 0.049396, current_train_items 281888.
I0304 19:32:16.037269 23118544486528 run.py:483] Algo bellman_ford step 8809 current loss 0.056750, current_train_items 281920.
I0304 19:32:16.056973 23118544486528 run.py:483] Algo bellman_ford step 8810 current loss 0.002998, current_train_items 281952.
I0304 19:32:16.073426 23118544486528 run.py:483] Algo bellman_ford step 8811 current loss 0.038682, current_train_items 281984.
I0304 19:32:16.098115 23118544486528 run.py:483] Algo bellman_ford step 8812 current loss 0.052660, current_train_items 282016.
I0304 19:32:16.130628 23118544486528 run.py:483] Algo bellman_ford step 8813 current loss 0.060942, current_train_items 282048.
I0304 19:32:16.164824 23118544486528 run.py:483] Algo bellman_ford step 8814 current loss 0.099272, current_train_items 282080.
I0304 19:32:16.185020 23118544486528 run.py:483] Algo bellman_ford step 8815 current loss 0.003879, current_train_items 282112.
I0304 19:32:16.201472 23118544486528 run.py:483] Algo bellman_ford step 8816 current loss 0.130906, current_train_items 282144.
I0304 19:32:16.225455 23118544486528 run.py:483] Algo bellman_ford step 8817 current loss 0.065244, current_train_items 282176.
I0304 19:32:16.256667 23118544486528 run.py:483] Algo bellman_ford step 8818 current loss 0.074671, current_train_items 282208.
I0304 19:32:16.294576 23118544486528 run.py:483] Algo bellman_ford step 8819 current loss 0.067988, current_train_items 282240.
I0304 19:32:16.314589 23118544486528 run.py:483] Algo bellman_ford step 8820 current loss 0.005771, current_train_items 282272.
I0304 19:32:16.330826 23118544486528 run.py:483] Algo bellman_ford step 8821 current loss 0.020656, current_train_items 282304.
I0304 19:32:16.354901 23118544486528 run.py:483] Algo bellman_ford step 8822 current loss 0.039528, current_train_items 282336.
I0304 19:32:16.387955 23118544486528 run.py:483] Algo bellman_ford step 8823 current loss 0.083676, current_train_items 282368.
I0304 19:32:16.421469 23118544486528 run.py:483] Algo bellman_ford step 8824 current loss 0.125574, current_train_items 282400.
I0304 19:32:16.441125 23118544486528 run.py:483] Algo bellman_ford step 8825 current loss 0.003893, current_train_items 282432.
I0304 19:32:16.457649 23118544486528 run.py:483] Algo bellman_ford step 8826 current loss 0.026008, current_train_items 282464.
I0304 19:32:16.482561 23118544486528 run.py:483] Algo bellman_ford step 8827 current loss 0.024581, current_train_items 282496.
I0304 19:32:16.515104 23118544486528 run.py:483] Algo bellman_ford step 8828 current loss 0.042045, current_train_items 282528.
I0304 19:32:16.549201 23118544486528 run.py:483] Algo bellman_ford step 8829 current loss 0.042668, current_train_items 282560.
I0304 19:32:16.569064 23118544486528 run.py:483] Algo bellman_ford step 8830 current loss 0.005072, current_train_items 282592.
I0304 19:32:16.585137 23118544486528 run.py:483] Algo bellman_ford step 8831 current loss 0.005005, current_train_items 282624.
I0304 19:32:16.610066 23118544486528 run.py:483] Algo bellman_ford step 8832 current loss 0.038824, current_train_items 282656.
I0304 19:32:16.641307 23118544486528 run.py:483] Algo bellman_ford step 8833 current loss 0.030495, current_train_items 282688.
I0304 19:32:16.674363 23118544486528 run.py:483] Algo bellman_ford step 8834 current loss 0.051629, current_train_items 282720.
I0304 19:32:16.694355 23118544486528 run.py:483] Algo bellman_ford step 8835 current loss 0.005703, current_train_items 282752.
I0304 19:32:16.710495 23118544486528 run.py:483] Algo bellman_ford step 8836 current loss 0.035212, current_train_items 282784.
I0304 19:32:16.735516 23118544486528 run.py:483] Algo bellman_ford step 8837 current loss 0.060563, current_train_items 282816.
I0304 19:32:16.767715 23118544486528 run.py:483] Algo bellman_ford step 8838 current loss 0.057801, current_train_items 282848.
I0304 19:32:16.802008 23118544486528 run.py:483] Algo bellman_ford step 8839 current loss 0.074795, current_train_items 282880.
I0304 19:32:16.822400 23118544486528 run.py:483] Algo bellman_ford step 8840 current loss 0.004649, current_train_items 282912.
I0304 19:32:16.839022 23118544486528 run.py:483] Algo bellman_ford step 8841 current loss 0.017514, current_train_items 282944.
I0304 19:32:16.863790 23118544486528 run.py:483] Algo bellman_ford step 8842 current loss 0.063586, current_train_items 282976.
I0304 19:32:16.896064 23118544486528 run.py:483] Algo bellman_ford step 8843 current loss 0.061551, current_train_items 283008.
I0304 19:32:16.932085 23118544486528 run.py:483] Algo bellman_ford step 8844 current loss 0.089857, current_train_items 283040.
I0304 19:32:16.951779 23118544486528 run.py:483] Algo bellman_ford step 8845 current loss 0.012459, current_train_items 283072.
I0304 19:32:16.967974 23118544486528 run.py:483] Algo bellman_ford step 8846 current loss 0.035794, current_train_items 283104.
I0304 19:32:16.992192 23118544486528 run.py:483] Algo bellman_ford step 8847 current loss 0.061647, current_train_items 283136.
I0304 19:32:17.023016 23118544486528 run.py:483] Algo bellman_ford step 8848 current loss 0.022515, current_train_items 283168.
I0304 19:32:17.057461 23118544486528 run.py:483] Algo bellman_ford step 8849 current loss 0.084649, current_train_items 283200.
I0304 19:32:17.076852 23118544486528 run.py:483] Algo bellman_ford step 8850 current loss 0.003348, current_train_items 283232.
I0304 19:32:17.084870 23118544486528 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0304 19:32:17.084973 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:17.101956 23118544486528 run.py:483] Algo bellman_ford step 8851 current loss 0.018285, current_train_items 283264.
I0304 19:32:17.127396 23118544486528 run.py:483] Algo bellman_ford step 8852 current loss 0.046527, current_train_items 283296.
I0304 19:32:17.160136 23118544486528 run.py:483] Algo bellman_ford step 8853 current loss 0.095070, current_train_items 283328.
I0304 19:32:17.194704 23118544486528 run.py:483] Algo bellman_ford step 8854 current loss 0.064471, current_train_items 283360.
I0304 19:32:17.214952 23118544486528 run.py:483] Algo bellman_ford step 8855 current loss 0.002632, current_train_items 283392.
I0304 19:32:17.231326 23118544486528 run.py:483] Algo bellman_ford step 8856 current loss 0.016119, current_train_items 283424.
I0304 19:32:17.255606 23118544486528 run.py:483] Algo bellman_ford step 8857 current loss 0.036340, current_train_items 283456.
I0304 19:32:17.288505 23118544486528 run.py:483] Algo bellman_ford step 8858 current loss 0.060594, current_train_items 283488.
I0304 19:32:17.321417 23118544486528 run.py:483] Algo bellman_ford step 8859 current loss 0.045872, current_train_items 283520.
I0304 19:32:17.341675 23118544486528 run.py:483] Algo bellman_ford step 8860 current loss 0.004019, current_train_items 283552.
I0304 19:32:17.358662 23118544486528 run.py:483] Algo bellman_ford step 8861 current loss 0.009684, current_train_items 283584.
I0304 19:32:17.383201 23118544486528 run.py:483] Algo bellman_ford step 8862 current loss 0.062433, current_train_items 283616.
I0304 19:32:17.414350 23118544486528 run.py:483] Algo bellman_ford step 8863 current loss 0.027767, current_train_items 283648.
I0304 19:32:17.447795 23118544486528 run.py:483] Algo bellman_ford step 8864 current loss 0.082809, current_train_items 283680.
I0304 19:32:17.467602 23118544486528 run.py:483] Algo bellman_ford step 8865 current loss 0.025521, current_train_items 283712.
I0304 19:32:17.483840 23118544486528 run.py:483] Algo bellman_ford step 8866 current loss 0.027241, current_train_items 283744.
I0304 19:32:17.507202 23118544486528 run.py:483] Algo bellman_ford step 8867 current loss 0.045562, current_train_items 283776.
I0304 19:32:17.540141 23118544486528 run.py:483] Algo bellman_ford step 8868 current loss 0.129253, current_train_items 283808.
I0304 19:32:17.573793 23118544486528 run.py:483] Algo bellman_ford step 8869 current loss 0.070263, current_train_items 283840.
I0304 19:32:17.593881 23118544486528 run.py:483] Algo bellman_ford step 8870 current loss 0.023422, current_train_items 283872.
I0304 19:32:17.610229 23118544486528 run.py:483] Algo bellman_ford step 8871 current loss 0.005367, current_train_items 283904.
I0304 19:32:17.634890 23118544486528 run.py:483] Algo bellman_ford step 8872 current loss 0.046889, current_train_items 283936.
I0304 19:32:17.666909 23118544486528 run.py:483] Algo bellman_ford step 8873 current loss 0.040389, current_train_items 283968.
I0304 19:32:17.702560 23118544486528 run.py:483] Algo bellman_ford step 8874 current loss 0.189361, current_train_items 284000.
I0304 19:32:17.722757 23118544486528 run.py:483] Algo bellman_ford step 8875 current loss 0.003056, current_train_items 284032.
I0304 19:32:17.739226 23118544486528 run.py:483] Algo bellman_ford step 8876 current loss 0.022212, current_train_items 284064.
I0304 19:32:17.763180 23118544486528 run.py:483] Algo bellman_ford step 8877 current loss 0.033343, current_train_items 284096.
I0304 19:32:17.794862 23118544486528 run.py:483] Algo bellman_ford step 8878 current loss 0.028155, current_train_items 284128.
I0304 19:32:17.829485 23118544486528 run.py:483] Algo bellman_ford step 8879 current loss 0.083510, current_train_items 284160.
I0304 19:32:17.849526 23118544486528 run.py:483] Algo bellman_ford step 8880 current loss 0.004076, current_train_items 284192.
I0304 19:32:17.865750 23118544486528 run.py:483] Algo bellman_ford step 8881 current loss 0.031519, current_train_items 284224.
I0304 19:32:17.889209 23118544486528 run.py:483] Algo bellman_ford step 8882 current loss 0.051327, current_train_items 284256.
I0304 19:32:17.922647 23118544486528 run.py:483] Algo bellman_ford step 8883 current loss 0.057741, current_train_items 284288.
I0304 19:32:17.957473 23118544486528 run.py:483] Algo bellman_ford step 8884 current loss 0.046477, current_train_items 284320.
I0304 19:32:17.977777 23118544486528 run.py:483] Algo bellman_ford step 8885 current loss 0.009084, current_train_items 284352.
I0304 19:32:17.994698 23118544486528 run.py:483] Algo bellman_ford step 8886 current loss 0.040817, current_train_items 284384.
I0304 19:32:18.019245 23118544486528 run.py:483] Algo bellman_ford step 8887 current loss 0.057710, current_train_items 284416.
I0304 19:32:18.050808 23118544486528 run.py:483] Algo bellman_ford step 8888 current loss 0.069433, current_train_items 284448.
I0304 19:32:18.086120 23118544486528 run.py:483] Algo bellman_ford step 8889 current loss 0.064700, current_train_items 284480.
I0304 19:32:18.105907 23118544486528 run.py:483] Algo bellman_ford step 8890 current loss 0.003701, current_train_items 284512.
I0304 19:32:18.121980 23118544486528 run.py:483] Algo bellman_ford step 8891 current loss 0.028981, current_train_items 284544.
I0304 19:32:18.146202 23118544486528 run.py:483] Algo bellman_ford step 8892 current loss 0.030496, current_train_items 284576.
I0304 19:32:18.180065 23118544486528 run.py:483] Algo bellman_ford step 8893 current loss 0.075446, current_train_items 284608.
I0304 19:32:18.214251 23118544486528 run.py:483] Algo bellman_ford step 8894 current loss 0.075235, current_train_items 284640.
I0304 19:32:18.233990 23118544486528 run.py:483] Algo bellman_ford step 8895 current loss 0.003835, current_train_items 284672.
I0304 19:32:18.250191 23118544486528 run.py:483] Algo bellman_ford step 8896 current loss 0.016518, current_train_items 284704.
I0304 19:32:18.274124 23118544486528 run.py:483] Algo bellman_ford step 8897 current loss 0.038281, current_train_items 284736.
I0304 19:32:18.307307 23118544486528 run.py:483] Algo bellman_ford step 8898 current loss 0.059073, current_train_items 284768.
I0304 19:32:18.343046 23118544486528 run.py:483] Algo bellman_ford step 8899 current loss 0.098837, current_train_items 284800.
I0304 19:32:18.363046 23118544486528 run.py:483] Algo bellman_ford step 8900 current loss 0.005499, current_train_items 284832.
I0304 19:32:18.370703 23118544486528 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0304 19:32:18.370811 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0304 19:32:18.387930 23118544486528 run.py:483] Algo bellman_ford step 8901 current loss 0.019874, current_train_items 284864.
I0304 19:32:18.412832 23118544486528 run.py:483] Algo bellman_ford step 8902 current loss 0.036457, current_train_items 284896.
I0304 19:32:18.444096 23118544486528 run.py:483] Algo bellman_ford step 8903 current loss 0.071321, current_train_items 284928.
I0304 19:32:18.480476 23118544486528 run.py:483] Algo bellman_ford step 8904 current loss 0.068495, current_train_items 284960.
I0304 19:32:18.500345 23118544486528 run.py:483] Algo bellman_ford step 8905 current loss 0.003446, current_train_items 284992.
I0304 19:32:18.516500 23118544486528 run.py:483] Algo bellman_ford step 8906 current loss 0.017812, current_train_items 285024.
I0304 19:32:18.540820 23118544486528 run.py:483] Algo bellman_ford step 8907 current loss 0.042481, current_train_items 285056.
I0304 19:32:18.572613 23118544486528 run.py:483] Algo bellman_ford step 8908 current loss 0.060652, current_train_items 285088.
I0304 19:32:18.605203 23118544486528 run.py:483] Algo bellman_ford step 8909 current loss 0.048799, current_train_items 285120.
I0304 19:32:18.625000 23118544486528 run.py:483] Algo bellman_ford step 8910 current loss 0.006115, current_train_items 285152.
I0304 19:32:18.641163 23118544486528 run.py:483] Algo bellman_ford step 8911 current loss 0.036132, current_train_items 285184.
I0304 19:32:18.666308 23118544486528 run.py:483] Algo bellman_ford step 8912 current loss 0.057887, current_train_items 285216.
I0304 19:32:18.697292 23118544486528 run.py:483] Algo bellman_ford step 8913 current loss 0.063662, current_train_items 285248.
I0304 19:32:18.732805 23118544486528 run.py:483] Algo bellman_ford step 8914 current loss 0.063883, current_train_items 285280.
I0304 19:32:18.752485 23118544486528 run.py:483] Algo bellman_ford step 8915 current loss 0.005046, current_train_items 285312.
I0304 19:32:18.768461 23118544486528 run.py:483] Algo bellman_ford step 8916 current loss 0.018011, current_train_items 285344.
I0304 19:32:18.792964 23118544486528 run.py:483] Algo bellman_ford step 8917 current loss 0.073039, current_train_items 285376.
I0304 19:32:18.825173 23118544486528 run.py:483] Algo bellman_ford step 8918 current loss 0.024909, current_train_items 285408.
I0304 19:32:18.859120 23118544486528 run.py:483] Algo bellman_ford step 8919 current loss 0.088659, current_train_items 285440.
I0304 19:32:18.878909 23118544486528 run.py:483] Algo bellman_ford step 8920 current loss 0.005660, current_train_items 285472.
I0304 19:32:18.895591 23118544486528 run.py:483] Algo bellman_ford step 8921 current loss 0.028370, current_train_items 285504.
I0304 19:32:18.920598 23118544486528 run.py:483] Algo bellman_ford step 8922 current loss 0.035142, current_train_items 285536.
I0304 19:32:18.952198 23118544486528 run.py:483] Algo bellman_ford step 8923 current loss 0.054220, current_train_items 285568.
I0304 19:32:18.987060 23118544486528 run.py:483] Algo bellman_ford step 8924 current loss 0.079415, current_train_items 285600.
I0304 19:32:19.006801 23118544486528 run.py:483] Algo bellman_ford step 8925 current loss 0.002964, current_train_items 285632.
I0304 19:32:19.022747 23118544486528 run.py:483] Algo bellman_ford step 8926 current loss 0.024059, current_train_items 285664.
I0304 19:32:19.047282 23118544486528 run.py:483] Algo bellman_ford step 8927 current loss 0.054695, current_train_items 285696.
I0304 19:32:19.078284 23118544486528 run.py:483] Algo bellman_ford step 8928 current loss 0.025425, current_train_items 285728.
I0304 19:32:19.112530 23118544486528 run.py:483] Algo bellman_ford step 8929 current loss 0.052967, current_train_items 285760.
I0304 19:32:19.132389 23118544486528 run.py:483] Algo bellman_ford step 8930 current loss 0.003780, current_train_items 285792.
I0304 19:32:19.149222 23118544486528 run.py:483] Algo bellman_ford step 8931 current loss 0.014563, current_train_items 285824.
I0304 19:32:19.173492 23118544486528 run.py:483] Algo bellman_ford step 8932 current loss 0.023695, current_train_items 285856.
I0304 19:32:19.206957 23118544486528 run.py:483] Algo bellman_ford step 8933 current loss 0.053546, current_train_items 285888.
I0304 19:32:19.241924 23118544486528 run.py:483] Algo bellman_ford step 8934 current loss 0.057642, current_train_items 285920.
I0304 19:32:19.261669 23118544486528 run.py:483] Algo bellman_ford step 8935 current loss 0.003243, current_train_items 285952.
I0304 19:32:19.277818 23118544486528 run.py:483] Algo bellman_ford step 8936 current loss 0.016899, current_train_items 285984.
I0304 19:32:19.302124 23118544486528 run.py:483] Algo bellman_ford step 8937 current loss 0.061666, current_train_items 286016.
I0304 19:32:19.333956 23118544486528 run.py:483] Algo bellman_ford step 8938 current loss 0.049043, current_train_items 286048.
I0304 19:32:19.366944 23118544486528 run.py:483] Algo bellman_ford step 8939 current loss 0.065587, current_train_items 286080.
I0304 19:32:19.386581 23118544486528 run.py:483] Algo bellman_ford step 8940 current loss 0.004605, current_train_items 286112.
I0304 19:32:19.402733 23118544486528 run.py:483] Algo bellman_ford step 8941 current loss 0.006132, current_train_items 286144.
I0304 19:32:19.427556 23118544486528 run.py:483] Algo bellman_ford step 8942 current loss 0.043934, current_train_items 286176.
I0304 19:32:19.460663 23118544486528 run.py:483] Algo bellman_ford step 8943 current loss 0.053430, current_train_items 286208.
I0304 19:32:19.495005 23118544486528 run.py:483] Algo bellman_ford step 8944 current loss 0.052397, current_train_items 286240.
I0304 19:32:19.514668 23118544486528 run.py:483] Algo bellman_ford step 8945 current loss 0.007154, current_train_items 286272.
I0304 19:32:19.531000 23118544486528 run.py:483] Algo bellman_ford step 8946 current loss 0.017277, current_train_items 286304.
I0304 19:32:19.554771 23118544486528 run.py:483] Algo bellman_ford step 8947 current loss 0.090844, current_train_items 286336.
I0304 19:32:19.587311 23118544486528 run.py:483] Algo bellman_ford step 8948 current loss 0.055578, current_train_items 286368.
I0304 19:32:19.621419 23118544486528 run.py:483] Algo bellman_ford step 8949 current loss 0.043894, current_train_items 286400.
I0304 19:32:19.640861 23118544486528 run.py:483] Algo bellman_ford step 8950 current loss 0.002397, current_train_items 286432.
I0304 19:32:19.648698 23118544486528 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0304 19:32:19.648804 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0304 19:32:19.666028 23118544486528 run.py:483] Algo bellman_ford step 8951 current loss 0.014119, current_train_items 286464.
I0304 19:32:19.690183 23118544486528 run.py:483] Algo bellman_ford step 8952 current loss 0.033069, current_train_items 286496.
I0304 19:32:19.725451 23118544486528 run.py:483] Algo bellman_ford step 8953 current loss 0.058431, current_train_items 286528.
I0304 19:32:19.761205 23118544486528 run.py:483] Algo bellman_ford step 8954 current loss 0.050749, current_train_items 286560.
I0304 19:32:19.781019 23118544486528 run.py:483] Algo bellman_ford step 8955 current loss 0.003415, current_train_items 286592.
I0304 19:32:19.797493 23118544486528 run.py:483] Algo bellman_ford step 8956 current loss 0.015559, current_train_items 286624.
I0304 19:32:19.822452 23118544486528 run.py:483] Algo bellman_ford step 8957 current loss 0.057228, current_train_items 286656.
I0304 19:32:19.854071 23118544486528 run.py:483] Algo bellman_ford step 8958 current loss 0.034198, current_train_items 286688.
I0304 19:32:19.889347 23118544486528 run.py:483] Algo bellman_ford step 8959 current loss 0.078798, current_train_items 286720.
I0304 19:32:19.909434 23118544486528 run.py:483] Algo bellman_ford step 8960 current loss 0.004083, current_train_items 286752.
I0304 19:32:19.925407 23118544486528 run.py:483] Algo bellman_ford step 8961 current loss 0.016737, current_train_items 286784.
I0304 19:32:19.950527 23118544486528 run.py:483] Algo bellman_ford step 8962 current loss 0.051482, current_train_items 286816.
I0304 19:32:19.982779 23118544486528 run.py:483] Algo bellman_ford step 8963 current loss 0.028751, current_train_items 286848.
I0304 19:32:20.015529 23118544486528 run.py:483] Algo bellman_ford step 8964 current loss 0.028531, current_train_items 286880.
I0304 19:32:20.035464 23118544486528 run.py:483] Algo bellman_ford step 8965 current loss 0.005092, current_train_items 286912.
I0304 19:32:20.051903 23118544486528 run.py:483] Algo bellman_ford step 8966 current loss 0.022509, current_train_items 286944.
I0304 19:32:20.076277 23118544486528 run.py:483] Algo bellman_ford step 8967 current loss 0.025191, current_train_items 286976.
I0304 19:32:20.108614 23118544486528 run.py:483] Algo bellman_ford step 8968 current loss 0.059674, current_train_items 287008.
I0304 19:32:20.142430 23118544486528 run.py:483] Algo bellman_ford step 8969 current loss 0.041569, current_train_items 287040.
I0304 19:32:20.161991 23118544486528 run.py:483] Algo bellman_ford step 8970 current loss 0.002537, current_train_items 287072.
I0304 19:32:20.178258 23118544486528 run.py:483] Algo bellman_ford step 8971 current loss 0.005918, current_train_items 287104.
I0304 19:32:20.202276 23118544486528 run.py:483] Algo bellman_ford step 8972 current loss 0.036861, current_train_items 287136.
I0304 19:32:20.233663 23118544486528 run.py:483] Algo bellman_ford step 8973 current loss 0.021723, current_train_items 287168.
I0304 19:32:20.266880 23118544486528 run.py:483] Algo bellman_ford step 8974 current loss 0.053688, current_train_items 287200.
I0304 19:32:20.286898 23118544486528 run.py:483] Algo bellman_ford step 8975 current loss 0.006263, current_train_items 287232.
I0304 19:32:20.303123 23118544486528 run.py:483] Algo bellman_ford step 8976 current loss 0.013198, current_train_items 287264.
I0304 19:32:20.326177 23118544486528 run.py:483] Algo bellman_ford step 8977 current loss 0.013557, current_train_items 287296.
I0304 19:32:20.358482 23118544486528 run.py:483] Algo bellman_ford step 8978 current loss 0.046973, current_train_items 287328.
I0304 19:32:20.391940 23118544486528 run.py:483] Algo bellman_ford step 8979 current loss 0.062304, current_train_items 287360.
I0304 19:32:20.411463 23118544486528 run.py:483] Algo bellman_ford step 8980 current loss 0.003791, current_train_items 287392.
I0304 19:32:20.427985 23118544486528 run.py:483] Algo bellman_ford step 8981 current loss 0.025174, current_train_items 287424.
I0304 19:32:20.452515 23118544486528 run.py:483] Algo bellman_ford step 8982 current loss 0.041527, current_train_items 287456.
I0304 19:32:20.484878 23118544486528 run.py:483] Algo bellman_ford step 8983 current loss 0.032294, current_train_items 287488.
I0304 19:32:20.518771 23118544486528 run.py:483] Algo bellman_ford step 8984 current loss 0.061057, current_train_items 287520.
I0304 19:32:20.538667 23118544486528 run.py:483] Algo bellman_ford step 8985 current loss 0.054104, current_train_items 287552.
I0304 19:32:20.554979 23118544486528 run.py:483] Algo bellman_ford step 8986 current loss 0.008881, current_train_items 287584.
I0304 19:32:20.578090 23118544486528 run.py:483] Algo bellman_ford step 8987 current loss 0.036234, current_train_items 287616.
I0304 19:32:20.608531 23118544486528 run.py:483] Algo bellman_ford step 8988 current loss 0.026502, current_train_items 287648.
I0304 19:32:20.644363 23118544486528 run.py:483] Algo bellman_ford step 8989 current loss 0.065960, current_train_items 287680.
I0304 19:32:20.663974 23118544486528 run.py:483] Algo bellman_ford step 8990 current loss 0.031321, current_train_items 287712.
I0304 19:32:20.680032 23118544486528 run.py:483] Algo bellman_ford step 8991 current loss 0.016125, current_train_items 287744.
I0304 19:32:20.703516 23118544486528 run.py:483] Algo bellman_ford step 8992 current loss 0.048181, current_train_items 287776.
I0304 19:32:20.735253 23118544486528 run.py:483] Algo bellman_ford step 8993 current loss 0.061170, current_train_items 287808.
I0304 19:32:20.769068 23118544486528 run.py:483] Algo bellman_ford step 8994 current loss 0.129063, current_train_items 287840.
I0304 19:32:20.788327 23118544486528 run.py:483] Algo bellman_ford step 8995 current loss 0.073811, current_train_items 287872.
I0304 19:32:20.804629 23118544486528 run.py:483] Algo bellman_ford step 8996 current loss 0.026442, current_train_items 287904.
I0304 19:32:20.829411 23118544486528 run.py:483] Algo bellman_ford step 8997 current loss 0.044976, current_train_items 287936.
I0304 19:32:20.862906 23118544486528 run.py:483] Algo bellman_ford step 8998 current loss 0.052333, current_train_items 287968.
I0304 19:32:20.895607 23118544486528 run.py:483] Algo bellman_ford step 8999 current loss 0.052585, current_train_items 288000.
I0304 19:32:20.915603 23118544486528 run.py:483] Algo bellman_ford step 9000 current loss 0.005826, current_train_items 288032.
I0304 19:32:20.923358 23118544486528 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0304 19:32:20.923463 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:20.940733 23118544486528 run.py:483] Algo bellman_ford step 9001 current loss 0.029703, current_train_items 288064.
I0304 19:32:20.965038 23118544486528 run.py:483] Algo bellman_ford step 9002 current loss 0.034096, current_train_items 288096.
I0304 19:32:20.997436 23118544486528 run.py:483] Algo bellman_ford step 9003 current loss 0.076062, current_train_items 288128.
I0304 19:32:21.032153 23118544486528 run.py:483] Algo bellman_ford step 9004 current loss 0.055051, current_train_items 288160.
I0304 19:32:21.052690 23118544486528 run.py:483] Algo bellman_ford step 9005 current loss 0.004726, current_train_items 288192.
I0304 19:32:21.069062 23118544486528 run.py:483] Algo bellman_ford step 9006 current loss 0.009126, current_train_items 288224.
I0304 19:32:21.093002 23118544486528 run.py:483] Algo bellman_ford step 9007 current loss 0.020240, current_train_items 288256.
I0304 19:32:21.126105 23118544486528 run.py:483] Algo bellman_ford step 9008 current loss 0.043850, current_train_items 288288.
I0304 19:32:21.160465 23118544486528 run.py:483] Algo bellman_ford step 9009 current loss 0.058449, current_train_items 288320.
I0304 19:32:21.180316 23118544486528 run.py:483] Algo bellman_ford step 9010 current loss 0.061310, current_train_items 288352.
I0304 19:32:21.196645 23118544486528 run.py:483] Algo bellman_ford step 9011 current loss 0.036110, current_train_items 288384.
I0304 19:32:21.220563 23118544486528 run.py:483] Algo bellman_ford step 9012 current loss 0.014255, current_train_items 288416.
I0304 19:32:21.252410 23118544486528 run.py:483] Algo bellman_ford step 9013 current loss 0.041992, current_train_items 288448.
I0304 19:32:21.288660 23118544486528 run.py:483] Algo bellman_ford step 9014 current loss 0.060196, current_train_items 288480.
I0304 19:32:21.308087 23118544486528 run.py:483] Algo bellman_ford step 9015 current loss 0.003972, current_train_items 288512.
I0304 19:32:21.324655 23118544486528 run.py:483] Algo bellman_ford step 9016 current loss 0.040893, current_train_items 288544.
I0304 19:32:21.349878 23118544486528 run.py:483] Algo bellman_ford step 9017 current loss 0.026934, current_train_items 288576.
I0304 19:32:21.382539 23118544486528 run.py:483] Algo bellman_ford step 9018 current loss 0.055671, current_train_items 288608.
I0304 19:32:21.418059 23118544486528 run.py:483] Algo bellman_ford step 9019 current loss 0.088670, current_train_items 288640.
I0304 19:32:21.437768 23118544486528 run.py:483] Algo bellman_ford step 9020 current loss 0.002831, current_train_items 288672.
I0304 19:32:21.454016 23118544486528 run.py:483] Algo bellman_ford step 9021 current loss 0.014399, current_train_items 288704.
I0304 19:32:21.479710 23118544486528 run.py:483] Algo bellman_ford step 9022 current loss 0.047455, current_train_items 288736.
I0304 19:32:21.513267 23118544486528 run.py:483] Algo bellman_ford step 9023 current loss 0.061239, current_train_items 288768.
I0304 19:32:21.547703 23118544486528 run.py:483] Algo bellman_ford step 9024 current loss 0.049634, current_train_items 288800.
I0304 19:32:21.567605 23118544486528 run.py:483] Algo bellman_ford step 9025 current loss 0.004561, current_train_items 288832.
I0304 19:32:21.583656 23118544486528 run.py:483] Algo bellman_ford step 9026 current loss 0.012620, current_train_items 288864.
I0304 19:32:21.608605 23118544486528 run.py:483] Algo bellman_ford step 9027 current loss 0.054823, current_train_items 288896.
I0304 19:32:21.641567 23118544486528 run.py:483] Algo bellman_ford step 9028 current loss 0.038923, current_train_items 288928.
I0304 19:32:21.676208 23118544486528 run.py:483] Algo bellman_ford step 9029 current loss 0.070716, current_train_items 288960.
I0304 19:32:21.696499 23118544486528 run.py:483] Algo bellman_ford step 9030 current loss 0.005478, current_train_items 288992.
I0304 19:32:21.712898 23118544486528 run.py:483] Algo bellman_ford step 9031 current loss 0.032615, current_train_items 289024.
I0304 19:32:21.737987 23118544486528 run.py:483] Algo bellman_ford step 9032 current loss 0.049412, current_train_items 289056.
I0304 19:32:21.771001 23118544486528 run.py:483] Algo bellman_ford step 9033 current loss 0.045070, current_train_items 289088.
I0304 19:32:21.807627 23118544486528 run.py:483] Algo bellman_ford step 9034 current loss 0.118070, current_train_items 289120.
I0304 19:32:21.827177 23118544486528 run.py:483] Algo bellman_ford step 9035 current loss 0.004970, current_train_items 289152.
I0304 19:32:21.843226 23118544486528 run.py:483] Algo bellman_ford step 9036 current loss 0.043260, current_train_items 289184.
I0304 19:32:21.868485 23118544486528 run.py:483] Algo bellman_ford step 9037 current loss 0.059501, current_train_items 289216.
I0304 19:32:21.900902 23118544486528 run.py:483] Algo bellman_ford step 9038 current loss 0.038790, current_train_items 289248.
I0304 19:32:21.935047 23118544486528 run.py:483] Algo bellman_ford step 9039 current loss 0.103082, current_train_items 289280.
I0304 19:32:21.955163 23118544486528 run.py:483] Algo bellman_ford step 9040 current loss 0.005695, current_train_items 289312.
I0304 19:32:21.971769 23118544486528 run.py:483] Algo bellman_ford step 9041 current loss 0.010609, current_train_items 289344.
I0304 19:32:21.996016 23118544486528 run.py:483] Algo bellman_ford step 9042 current loss 0.019195, current_train_items 289376.
I0304 19:32:22.028119 23118544486528 run.py:483] Algo bellman_ford step 9043 current loss 0.064136, current_train_items 289408.
I0304 19:32:22.062804 23118544486528 run.py:483] Algo bellman_ford step 9044 current loss 0.061950, current_train_items 289440.
I0304 19:32:22.082666 23118544486528 run.py:483] Algo bellman_ford step 9045 current loss 0.004704, current_train_items 289472.
I0304 19:32:22.099245 23118544486528 run.py:483] Algo bellman_ford step 9046 current loss 0.016137, current_train_items 289504.
I0304 19:32:22.122709 23118544486528 run.py:483] Algo bellman_ford step 9047 current loss 0.032078, current_train_items 289536.
I0304 19:32:22.155233 23118544486528 run.py:483] Algo bellman_ford step 9048 current loss 0.074852, current_train_items 289568.
I0304 19:32:22.189325 23118544486528 run.py:483] Algo bellman_ford step 9049 current loss 0.082834, current_train_items 289600.
I0304 19:32:22.208752 23118544486528 run.py:483] Algo bellman_ford step 9050 current loss 0.008252, current_train_items 289632.
I0304 19:32:22.216790 23118544486528 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0304 19:32:22.216897 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:22.233611 23118544486528 run.py:483] Algo bellman_ford step 9051 current loss 0.034739, current_train_items 289664.
I0304 19:32:22.258571 23118544486528 run.py:483] Algo bellman_ford step 9052 current loss 0.041845, current_train_items 289696.
I0304 19:32:22.291930 23118544486528 run.py:483] Algo bellman_ford step 9053 current loss 0.047359, current_train_items 289728.
I0304 19:32:22.325074 23118544486528 run.py:483] Algo bellman_ford step 9054 current loss 0.048427, current_train_items 289760.
I0304 19:32:22.344995 23118544486528 run.py:483] Algo bellman_ford step 9055 current loss 0.003025, current_train_items 289792.
I0304 19:32:22.360941 23118544486528 run.py:483] Algo bellman_ford step 9056 current loss 0.044456, current_train_items 289824.
I0304 19:32:22.384937 23118544486528 run.py:483] Algo bellman_ford step 9057 current loss 0.046932, current_train_items 289856.
I0304 19:32:22.416037 23118544486528 run.py:483] Algo bellman_ford step 9058 current loss 0.066073, current_train_items 289888.
I0304 19:32:22.448946 23118544486528 run.py:483] Algo bellman_ford step 9059 current loss 0.075955, current_train_items 289920.
I0304 19:32:22.468869 23118544486528 run.py:483] Algo bellman_ford step 9060 current loss 0.011650, current_train_items 289952.
I0304 19:32:22.485792 23118544486528 run.py:483] Algo bellman_ford step 9061 current loss 0.016547, current_train_items 289984.
I0304 19:32:22.511016 23118544486528 run.py:483] Algo bellman_ford step 9062 current loss 0.050904, current_train_items 290016.
I0304 19:32:22.541903 23118544486528 run.py:483] Algo bellman_ford step 9063 current loss 0.019443, current_train_items 290048.
I0304 19:32:22.576954 23118544486528 run.py:483] Algo bellman_ford step 9064 current loss 0.070128, current_train_items 290080.
I0304 19:32:22.596743 23118544486528 run.py:483] Algo bellman_ford step 9065 current loss 0.005566, current_train_items 290112.
I0304 19:32:22.613151 23118544486528 run.py:483] Algo bellman_ford step 9066 current loss 0.025914, current_train_items 290144.
I0304 19:32:22.636047 23118544486528 run.py:483] Algo bellman_ford step 9067 current loss 0.033314, current_train_items 290176.
I0304 19:32:22.668470 23118544486528 run.py:483] Algo bellman_ford step 9068 current loss 0.048612, current_train_items 290208.
I0304 19:32:22.703592 23118544486528 run.py:483] Algo bellman_ford step 9069 current loss 0.130971, current_train_items 290240.
I0304 19:32:22.723934 23118544486528 run.py:483] Algo bellman_ford step 9070 current loss 0.003405, current_train_items 290272.
I0304 19:32:22.740609 23118544486528 run.py:483] Algo bellman_ford step 9071 current loss 0.027503, current_train_items 290304.
I0304 19:32:22.765554 23118544486528 run.py:483] Algo bellman_ford step 9072 current loss 0.077820, current_train_items 290336.
I0304 19:32:22.798390 23118544486528 run.py:483] Algo bellman_ford step 9073 current loss 0.060623, current_train_items 290368.
I0304 19:32:22.832329 23118544486528 run.py:483] Algo bellman_ford step 9074 current loss 0.055752, current_train_items 290400.
I0304 19:32:22.851944 23118544486528 run.py:483] Algo bellman_ford step 9075 current loss 0.009344, current_train_items 290432.
I0304 19:32:22.868282 23118544486528 run.py:483] Algo bellman_ford step 9076 current loss 0.021281, current_train_items 290464.
I0304 19:32:22.892731 23118544486528 run.py:483] Algo bellman_ford step 9077 current loss 0.066475, current_train_items 290496.
I0304 19:32:22.924655 23118544486528 run.py:483] Algo bellman_ford step 9078 current loss 0.054482, current_train_items 290528.
I0304 19:32:22.959363 23118544486528 run.py:483] Algo bellman_ford step 9079 current loss 0.081750, current_train_items 290560.
I0304 19:32:22.979091 23118544486528 run.py:483] Algo bellman_ford step 9080 current loss 0.004239, current_train_items 290592.
I0304 19:32:22.995247 23118544486528 run.py:483] Algo bellman_ford step 9081 current loss 0.021946, current_train_items 290624.
I0304 19:32:23.019354 23118544486528 run.py:483] Algo bellman_ford step 9082 current loss 0.013007, current_train_items 290656.
I0304 19:32:23.050905 23118544486528 run.py:483] Algo bellman_ford step 9083 current loss 0.037397, current_train_items 290688.
I0304 19:32:23.084010 23118544486528 run.py:483] Algo bellman_ford step 9084 current loss 0.056476, current_train_items 290720.
I0304 19:32:23.103908 23118544486528 run.py:483] Algo bellman_ford step 9085 current loss 0.004910, current_train_items 290752.
I0304 19:32:23.120626 23118544486528 run.py:483] Algo bellman_ford step 9086 current loss 0.017110, current_train_items 290784.
I0304 19:32:23.145290 23118544486528 run.py:483] Algo bellman_ford step 9087 current loss 0.035532, current_train_items 290816.
I0304 19:32:23.176873 23118544486528 run.py:483] Algo bellman_ford step 9088 current loss 0.019542, current_train_items 290848.
I0304 19:32:23.212476 23118544486528 run.py:483] Algo bellman_ford step 9089 current loss 0.067198, current_train_items 290880.
I0304 19:32:23.232416 23118544486528 run.py:483] Algo bellman_ford step 9090 current loss 0.002345, current_train_items 290912.
I0304 19:32:23.248669 23118544486528 run.py:483] Algo bellman_ford step 9091 current loss 0.024850, current_train_items 290944.
I0304 19:32:23.272719 23118544486528 run.py:483] Algo bellman_ford step 9092 current loss 0.057872, current_train_items 290976.
I0304 19:32:23.302931 23118544486528 run.py:483] Algo bellman_ford step 9093 current loss 0.047940, current_train_items 291008.
I0304 19:32:23.339709 23118544486528 run.py:483] Algo bellman_ford step 9094 current loss 0.070327, current_train_items 291040.
I0304 19:32:23.359401 23118544486528 run.py:483] Algo bellman_ford step 9095 current loss 0.005857, current_train_items 291072.
I0304 19:32:23.375658 23118544486528 run.py:483] Algo bellman_ford step 9096 current loss 0.014022, current_train_items 291104.
I0304 19:32:23.400123 23118544486528 run.py:483] Algo bellman_ford step 9097 current loss 0.044198, current_train_items 291136.
I0304 19:32:23.432787 23118544486528 run.py:483] Algo bellman_ford step 9098 current loss 0.040295, current_train_items 291168.
I0304 19:32:23.466158 23118544486528 run.py:483] Algo bellman_ford step 9099 current loss 0.047111, current_train_items 291200.
I0304 19:32:23.485972 23118544486528 run.py:483] Algo bellman_ford step 9100 current loss 0.003778, current_train_items 291232.
I0304 19:32:23.493522 23118544486528 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0304 19:32:23.493629 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:23.510543 23118544486528 run.py:483] Algo bellman_ford step 9101 current loss 0.005398, current_train_items 291264.
I0304 19:32:23.536202 23118544486528 run.py:483] Algo bellman_ford step 9102 current loss 0.049777, current_train_items 291296.
I0304 19:32:23.568796 23118544486528 run.py:483] Algo bellman_ford step 9103 current loss 0.022124, current_train_items 291328.
I0304 19:32:23.604987 23118544486528 run.py:483] Algo bellman_ford step 9104 current loss 0.047832, current_train_items 291360.
I0304 19:32:23.624960 23118544486528 run.py:483] Algo bellman_ford step 9105 current loss 0.015286, current_train_items 291392.
I0304 19:32:23.641220 23118544486528 run.py:483] Algo bellman_ford step 9106 current loss 0.023126, current_train_items 291424.
I0304 19:32:23.666113 23118544486528 run.py:483] Algo bellman_ford step 9107 current loss 0.043200, current_train_items 291456.
I0304 19:32:23.698588 23118544486528 run.py:483] Algo bellman_ford step 9108 current loss 0.051086, current_train_items 291488.
I0304 19:32:23.733006 23118544486528 run.py:483] Algo bellman_ford step 9109 current loss 0.065080, current_train_items 291520.
I0304 19:32:23.752628 23118544486528 run.py:483] Algo bellman_ford step 9110 current loss 0.004117, current_train_items 291552.
I0304 19:32:23.768927 23118544486528 run.py:483] Algo bellman_ford step 9111 current loss 0.017946, current_train_items 291584.
I0304 19:32:23.792915 23118544486528 run.py:483] Algo bellman_ford step 9112 current loss 0.030730, current_train_items 291616.
I0304 19:32:23.826885 23118544486528 run.py:483] Algo bellman_ford step 9113 current loss 0.045956, current_train_items 291648.
I0304 19:32:23.862360 23118544486528 run.py:483] Algo bellman_ford step 9114 current loss 0.059108, current_train_items 291680.
I0304 19:32:23.882019 23118544486528 run.py:483] Algo bellman_ford step 9115 current loss 0.064661, current_train_items 291712.
I0304 19:32:23.898649 23118544486528 run.py:483] Algo bellman_ford step 9116 current loss 0.006160, current_train_items 291744.
I0304 19:32:23.922824 23118544486528 run.py:483] Algo bellman_ford step 9117 current loss 0.039639, current_train_items 291776.
I0304 19:32:23.954668 23118544486528 run.py:483] Algo bellman_ford step 9118 current loss 0.030583, current_train_items 291808.
I0304 19:32:23.990599 23118544486528 run.py:483] Algo bellman_ford step 9119 current loss 0.065356, current_train_items 291840.
I0304 19:32:24.010195 23118544486528 run.py:483] Algo bellman_ford step 9120 current loss 0.004176, current_train_items 291872.
I0304 19:32:24.026522 23118544486528 run.py:483] Algo bellman_ford step 9121 current loss 0.027168, current_train_items 291904.
I0304 19:32:24.050718 23118544486528 run.py:483] Algo bellman_ford step 9122 current loss 0.024055, current_train_items 291936.
I0304 19:32:24.084431 23118544486528 run.py:483] Algo bellman_ford step 9123 current loss 0.067526, current_train_items 291968.
I0304 19:32:24.120442 23118544486528 run.py:483] Algo bellman_ford step 9124 current loss 0.063534, current_train_items 292000.
I0304 19:32:24.140137 23118544486528 run.py:483] Algo bellman_ford step 9125 current loss 0.004375, current_train_items 292032.
I0304 19:32:24.156178 23118544486528 run.py:483] Algo bellman_ford step 9126 current loss 0.023992, current_train_items 292064.
I0304 19:32:24.179519 23118544486528 run.py:483] Algo bellman_ford step 9127 current loss 0.042728, current_train_items 292096.
I0304 19:32:24.212692 23118544486528 run.py:483] Algo bellman_ford step 9128 current loss 0.043862, current_train_items 292128.
I0304 19:32:24.246333 23118544486528 run.py:483] Algo bellman_ford step 9129 current loss 0.075874, current_train_items 292160.
I0304 19:32:24.266126 23118544486528 run.py:483] Algo bellman_ford step 9130 current loss 0.007257, current_train_items 292192.
I0304 19:32:24.282789 23118544486528 run.py:483] Algo bellman_ford step 9131 current loss 0.004999, current_train_items 292224.
I0304 19:32:24.308001 23118544486528 run.py:483] Algo bellman_ford step 9132 current loss 0.105903, current_train_items 292256.
I0304 19:32:24.340588 23118544486528 run.py:483] Algo bellman_ford step 9133 current loss 0.133883, current_train_items 292288.
I0304 19:32:24.373604 23118544486528 run.py:483] Algo bellman_ford step 9134 current loss 0.058195, current_train_items 292320.
I0304 19:32:24.393298 23118544486528 run.py:483] Algo bellman_ford step 9135 current loss 0.003897, current_train_items 292352.
I0304 19:32:24.409151 23118544486528 run.py:483] Algo bellman_ford step 9136 current loss 0.011552, current_train_items 292384.
I0304 19:32:24.432617 23118544486528 run.py:483] Algo bellman_ford step 9137 current loss 0.020953, current_train_items 292416.
I0304 19:32:24.464970 23118544486528 run.py:483] Algo bellman_ford step 9138 current loss 0.033024, current_train_items 292448.
I0304 19:32:24.498018 23118544486528 run.py:483] Algo bellman_ford step 9139 current loss 0.078453, current_train_items 292480.
I0304 19:32:24.517714 23118544486528 run.py:483] Algo bellman_ford step 9140 current loss 0.004113, current_train_items 292512.
I0304 19:32:24.533987 23118544486528 run.py:483] Algo bellman_ford step 9141 current loss 0.010988, current_train_items 292544.
I0304 19:32:24.558234 23118544486528 run.py:483] Algo bellman_ford step 9142 current loss 0.029972, current_train_items 292576.
I0304 19:32:24.589954 23118544486528 run.py:483] Algo bellman_ford step 9143 current loss 0.041973, current_train_items 292608.
I0304 19:32:24.624997 23118544486528 run.py:483] Algo bellman_ford step 9144 current loss 0.073307, current_train_items 292640.
I0304 19:32:24.644421 23118544486528 run.py:483] Algo bellman_ford step 9145 current loss 0.004295, current_train_items 292672.
I0304 19:32:24.661062 23118544486528 run.py:483] Algo bellman_ford step 9146 current loss 0.080490, current_train_items 292704.
I0304 19:32:24.685567 23118544486528 run.py:483] Algo bellman_ford step 9147 current loss 0.027715, current_train_items 292736.
I0304 19:32:24.717616 23118544486528 run.py:483] Algo bellman_ford step 9148 current loss 0.041065, current_train_items 292768.
I0304 19:32:24.748527 23118544486528 run.py:483] Algo bellman_ford step 9149 current loss 0.043298, current_train_items 292800.
I0304 19:32:24.768016 23118544486528 run.py:483] Algo bellman_ford step 9150 current loss 0.004253, current_train_items 292832.
I0304 19:32:24.776073 23118544486528 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0304 19:32:24.776178 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:24.793126 23118544486528 run.py:483] Algo bellman_ford step 9151 current loss 0.014016, current_train_items 292864.
I0304 19:32:24.817519 23118544486528 run.py:483] Algo bellman_ford step 9152 current loss 0.030837, current_train_items 292896.
I0304 19:32:24.850388 23118544486528 run.py:483] Algo bellman_ford step 9153 current loss 0.046387, current_train_items 292928.
I0304 19:32:24.882895 23118544486528 run.py:483] Algo bellman_ford step 9154 current loss 0.044218, current_train_items 292960.
I0304 19:32:24.902739 23118544486528 run.py:483] Algo bellman_ford step 9155 current loss 0.003501, current_train_items 292992.
I0304 19:32:24.919347 23118544486528 run.py:483] Algo bellman_ford step 9156 current loss 0.016922, current_train_items 293024.
I0304 19:32:24.944506 23118544486528 run.py:483] Algo bellman_ford step 9157 current loss 0.070058, current_train_items 293056.
I0304 19:32:24.976321 23118544486528 run.py:483] Algo bellman_ford step 9158 current loss 0.021885, current_train_items 293088.
I0304 19:32:25.011987 23118544486528 run.py:483] Algo bellman_ford step 9159 current loss 0.054458, current_train_items 293120.
I0304 19:32:25.031826 23118544486528 run.py:483] Algo bellman_ford step 9160 current loss 0.004730, current_train_items 293152.
I0304 19:32:25.048362 23118544486528 run.py:483] Algo bellman_ford step 9161 current loss 0.025124, current_train_items 293184.
I0304 19:32:25.072336 23118544486528 run.py:483] Algo bellman_ford step 9162 current loss 0.028453, current_train_items 293216.
I0304 19:32:25.104316 23118544486528 run.py:483] Algo bellman_ford step 9163 current loss 0.030897, current_train_items 293248.
I0304 19:32:25.140338 23118544486528 run.py:483] Algo bellman_ford step 9164 current loss 0.040526, current_train_items 293280.
I0304 19:32:25.160039 23118544486528 run.py:483] Algo bellman_ford step 9165 current loss 0.003765, current_train_items 293312.
I0304 19:32:25.176434 23118544486528 run.py:483] Algo bellman_ford step 9166 current loss 0.030620, current_train_items 293344.
I0304 19:32:25.200841 23118544486528 run.py:483] Algo bellman_ford step 9167 current loss 0.030081, current_train_items 293376.
I0304 19:32:25.233206 23118544486528 run.py:483] Algo bellman_ford step 9168 current loss 0.049742, current_train_items 293408.
I0304 19:32:25.266650 23118544486528 run.py:483] Algo bellman_ford step 9169 current loss 0.043319, current_train_items 293440.
I0304 19:32:25.286447 23118544486528 run.py:483] Algo bellman_ford step 9170 current loss 0.002564, current_train_items 293472.
I0304 19:32:25.302592 23118544486528 run.py:483] Algo bellman_ford step 9171 current loss 0.015967, current_train_items 293504.
I0304 19:32:25.328347 23118544486528 run.py:483] Algo bellman_ford step 9172 current loss 0.052646, current_train_items 293536.
I0304 19:32:25.361099 23118544486528 run.py:483] Algo bellman_ford step 9173 current loss 0.044386, current_train_items 293568.
I0304 19:32:25.395910 23118544486528 run.py:483] Algo bellman_ford step 9174 current loss 0.060570, current_train_items 293600.
I0304 19:32:25.415814 23118544486528 run.py:483] Algo bellman_ford step 9175 current loss 0.012264, current_train_items 293632.
I0304 19:32:25.432608 23118544486528 run.py:483] Algo bellman_ford step 9176 current loss 0.020971, current_train_items 293664.
I0304 19:32:25.456189 23118544486528 run.py:483] Algo bellman_ford step 9177 current loss 0.027563, current_train_items 293696.
I0304 19:32:25.486783 23118544486528 run.py:483] Algo bellman_ford step 9178 current loss 0.028505, current_train_items 293728.
I0304 19:32:25.522579 23118544486528 run.py:483] Algo bellman_ford step 9179 current loss 0.137651, current_train_items 293760.
I0304 19:32:25.541881 23118544486528 run.py:483] Algo bellman_ford step 9180 current loss 0.004344, current_train_items 293792.
I0304 19:32:25.558187 23118544486528 run.py:483] Algo bellman_ford step 9181 current loss 0.034143, current_train_items 293824.
I0304 19:32:25.583371 23118544486528 run.py:483] Algo bellman_ford step 9182 current loss 0.073620, current_train_items 293856.
I0304 19:32:25.614353 23118544486528 run.py:483] Algo bellman_ford step 9183 current loss 0.054343, current_train_items 293888.
I0304 19:32:25.647730 23118544486528 run.py:483] Algo bellman_ford step 9184 current loss 0.059696, current_train_items 293920.
I0304 19:32:25.667583 23118544486528 run.py:483] Algo bellman_ford step 9185 current loss 0.026197, current_train_items 293952.
I0304 19:32:25.684129 23118544486528 run.py:483] Algo bellman_ford step 9186 current loss 0.061985, current_train_items 293984.
I0304 19:32:25.708759 23118544486528 run.py:483] Algo bellman_ford step 9187 current loss 0.048843, current_train_items 294016.
I0304 19:32:25.742150 23118544486528 run.py:483] Algo bellman_ford step 9188 current loss 0.068871, current_train_items 294048.
I0304 19:32:25.778369 23118544486528 run.py:483] Algo bellman_ford step 9189 current loss 0.037908, current_train_items 294080.
I0304 19:32:25.798380 23118544486528 run.py:483] Algo bellman_ford step 9190 current loss 0.004567, current_train_items 294112.
I0304 19:32:25.814547 23118544486528 run.py:483] Algo bellman_ford step 9191 current loss 0.051963, current_train_items 294144.
I0304 19:32:25.839046 23118544486528 run.py:483] Algo bellman_ford step 9192 current loss 0.047763, current_train_items 294176.
I0304 19:32:25.871197 23118544486528 run.py:483] Algo bellman_ford step 9193 current loss 0.045169, current_train_items 294208.
I0304 19:32:25.904420 23118544486528 run.py:483] Algo bellman_ford step 9194 current loss 0.034739, current_train_items 294240.
I0304 19:32:25.923976 23118544486528 run.py:483] Algo bellman_ford step 9195 current loss 0.006158, current_train_items 294272.
I0304 19:32:25.941020 23118544486528 run.py:483] Algo bellman_ford step 9196 current loss 0.020184, current_train_items 294304.
I0304 19:32:25.965588 23118544486528 run.py:483] Algo bellman_ford step 9197 current loss 0.076128, current_train_items 294336.
I0304 19:32:25.997126 23118544486528 run.py:483] Algo bellman_ford step 9198 current loss 0.020495, current_train_items 294368.
I0304 19:32:26.030109 23118544486528 run.py:483] Algo bellman_ford step 9199 current loss 0.038328, current_train_items 294400.
I0304 19:32:26.050264 23118544486528 run.py:483] Algo bellman_ford step 9200 current loss 0.002395, current_train_items 294432.
I0304 19:32:26.058230 23118544486528 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0304 19:32:26.058336 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0304 19:32:26.076017 23118544486528 run.py:483] Algo bellman_ford step 9201 current loss 0.008951, current_train_items 294464.
I0304 19:32:26.100768 23118544486528 run.py:483] Algo bellman_ford step 9202 current loss 0.032251, current_train_items 294496.
I0304 19:32:26.134615 23118544486528 run.py:483] Algo bellman_ford step 9203 current loss 0.047408, current_train_items 294528.
I0304 19:32:26.170016 23118544486528 run.py:483] Algo bellman_ford step 9204 current loss 0.035412, current_train_items 294560.
I0304 19:32:26.190395 23118544486528 run.py:483] Algo bellman_ford step 9205 current loss 0.005840, current_train_items 294592.
I0304 19:32:26.206671 23118544486528 run.py:483] Algo bellman_ford step 9206 current loss 0.021237, current_train_items 294624.
I0304 19:32:26.231585 23118544486528 run.py:483] Algo bellman_ford step 9207 current loss 0.031508, current_train_items 294656.
I0304 19:32:26.264288 23118544486528 run.py:483] Algo bellman_ford step 9208 current loss 0.063795, current_train_items 294688.
I0304 19:32:26.296837 23118544486528 run.py:483] Algo bellman_ford step 9209 current loss 0.045592, current_train_items 294720.
I0304 19:32:26.316615 23118544486528 run.py:483] Algo bellman_ford step 9210 current loss 0.003611, current_train_items 294752.
I0304 19:32:26.333486 23118544486528 run.py:483] Algo bellman_ford step 9211 current loss 0.022780, current_train_items 294784.
I0304 19:32:26.357758 23118544486528 run.py:483] Algo bellman_ford step 9212 current loss 0.031814, current_train_items 294816.
I0304 19:32:26.389807 23118544486528 run.py:483] Algo bellman_ford step 9213 current loss 0.037249, current_train_items 294848.
I0304 19:32:26.422854 23118544486528 run.py:483] Algo bellman_ford step 9214 current loss 0.058064, current_train_items 294880.
I0304 19:32:26.442564 23118544486528 run.py:483] Algo bellman_ford step 9215 current loss 0.033262, current_train_items 294912.
I0304 19:32:26.459062 23118544486528 run.py:483] Algo bellman_ford step 9216 current loss 0.013961, current_train_items 294944.
I0304 19:32:26.483462 23118544486528 run.py:483] Algo bellman_ford step 9217 current loss 0.016186, current_train_items 294976.
I0304 19:32:26.515483 23118544486528 run.py:483] Algo bellman_ford step 9218 current loss 0.045249, current_train_items 295008.
I0304 19:32:26.549483 23118544486528 run.py:483] Algo bellman_ford step 9219 current loss 0.049078, current_train_items 295040.
I0304 19:32:26.569169 23118544486528 run.py:483] Algo bellman_ford step 9220 current loss 0.027789, current_train_items 295072.
I0304 19:32:26.585369 23118544486528 run.py:483] Algo bellman_ford step 9221 current loss 0.045059, current_train_items 295104.
I0304 19:32:26.610784 23118544486528 run.py:483] Algo bellman_ford step 9222 current loss 0.080450, current_train_items 295136.
I0304 19:32:26.642858 23118544486528 run.py:483] Algo bellman_ford step 9223 current loss 0.091694, current_train_items 295168.
I0304 19:32:26.677106 23118544486528 run.py:483] Algo bellman_ford step 9224 current loss 0.077236, current_train_items 295200.
I0304 19:32:26.697179 23118544486528 run.py:483] Algo bellman_ford step 9225 current loss 0.004772, current_train_items 295232.
I0304 19:32:26.713502 23118544486528 run.py:483] Algo bellman_ford step 9226 current loss 0.006637, current_train_items 295264.
I0304 19:32:26.738752 23118544486528 run.py:483] Algo bellman_ford step 9227 current loss 0.042331, current_train_items 295296.
I0304 19:32:26.771030 23118544486528 run.py:483] Algo bellman_ford step 9228 current loss 0.099973, current_train_items 295328.
I0304 19:32:26.802824 23118544486528 run.py:483] Algo bellman_ford step 9229 current loss 0.039997, current_train_items 295360.
I0304 19:32:26.822556 23118544486528 run.py:483] Algo bellman_ford step 9230 current loss 0.005205, current_train_items 295392.
I0304 19:32:26.839170 23118544486528 run.py:483] Algo bellman_ford step 9231 current loss 0.033622, current_train_items 295424.
I0304 19:32:26.864202 23118544486528 run.py:483] Algo bellman_ford step 9232 current loss 0.090711, current_train_items 295456.
I0304 19:32:26.898918 23118544486528 run.py:483] Algo bellman_ford step 9233 current loss 0.080116, current_train_items 295488.
I0304 19:32:26.933461 23118544486528 run.py:483] Algo bellman_ford step 9234 current loss 0.046608, current_train_items 295520.
I0304 19:32:26.953272 23118544486528 run.py:483] Algo bellman_ford step 9235 current loss 0.002289, current_train_items 295552.
I0304 19:32:26.969711 23118544486528 run.py:483] Algo bellman_ford step 9236 current loss 0.048032, current_train_items 295584.
I0304 19:32:26.994339 23118544486528 run.py:483] Algo bellman_ford step 9237 current loss 0.042217, current_train_items 295616.
I0304 19:32:27.027917 23118544486528 run.py:483] Algo bellman_ford step 9238 current loss 0.081978, current_train_items 295648.
I0304 19:32:27.062703 23118544486528 run.py:483] Algo bellman_ford step 9239 current loss 0.072919, current_train_items 295680.
I0304 19:32:27.082879 23118544486528 run.py:483] Algo bellman_ford step 9240 current loss 0.004860, current_train_items 295712.
I0304 19:32:27.099793 23118544486528 run.py:483] Algo bellman_ford step 9241 current loss 0.012464, current_train_items 295744.
I0304 19:32:27.125204 23118544486528 run.py:483] Algo bellman_ford step 9242 current loss 0.053755, current_train_items 295776.
I0304 19:32:27.157851 23118544486528 run.py:483] Algo bellman_ford step 9243 current loss 0.034740, current_train_items 295808.
I0304 19:32:27.191018 23118544486528 run.py:483] Algo bellman_ford step 9244 current loss 0.055892, current_train_items 295840.
I0304 19:32:27.211149 23118544486528 run.py:483] Algo bellman_ford step 9245 current loss 0.003513, current_train_items 295872.
I0304 19:32:27.227606 23118544486528 run.py:483] Algo bellman_ford step 9246 current loss 0.021270, current_train_items 295904.
I0304 19:32:27.251775 23118544486528 run.py:483] Algo bellman_ford step 9247 current loss 0.020684, current_train_items 295936.
I0304 19:32:27.284170 23118544486528 run.py:483] Algo bellman_ford step 9248 current loss 0.035102, current_train_items 295968.
I0304 19:32:27.319388 23118544486528 run.py:483] Algo bellman_ford step 9249 current loss 0.061732, current_train_items 296000.
I0304 19:32:27.339515 23118544486528 run.py:483] Algo bellman_ford step 9250 current loss 0.007077, current_train_items 296032.
I0304 19:32:27.347841 23118544486528 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0304 19:32:27.347946 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:27.364875 23118544486528 run.py:483] Algo bellman_ford step 9251 current loss 0.020284, current_train_items 296064.
I0304 19:32:27.390134 23118544486528 run.py:483] Algo bellman_ford step 9252 current loss 0.048906, current_train_items 296096.
I0304 19:32:27.422306 23118544486528 run.py:483] Algo bellman_ford step 9253 current loss 0.034093, current_train_items 296128.
I0304 19:32:27.455133 23118544486528 run.py:483] Algo bellman_ford step 9254 current loss 0.037465, current_train_items 296160.
I0304 19:32:27.474899 23118544486528 run.py:483] Algo bellman_ford step 9255 current loss 0.004694, current_train_items 296192.
I0304 19:32:27.491026 23118544486528 run.py:483] Algo bellman_ford step 9256 current loss 0.048696, current_train_items 296224.
I0304 19:32:27.515806 23118544486528 run.py:483] Algo bellman_ford step 9257 current loss 0.061203, current_train_items 296256.
I0304 19:32:27.548562 23118544486528 run.py:483] Algo bellman_ford step 9258 current loss 0.069026, current_train_items 296288.
I0304 19:32:27.582821 23118544486528 run.py:483] Algo bellman_ford step 9259 current loss 0.065641, current_train_items 296320.
I0304 19:32:27.603037 23118544486528 run.py:483] Algo bellman_ford step 9260 current loss 0.004074, current_train_items 296352.
I0304 19:32:27.620163 23118544486528 run.py:483] Algo bellman_ford step 9261 current loss 0.015380, current_train_items 296384.
I0304 19:32:27.643312 23118544486528 run.py:483] Algo bellman_ford step 9262 current loss 0.031683, current_train_items 296416.
I0304 19:32:27.675941 23118544486528 run.py:483] Algo bellman_ford step 9263 current loss 0.146059, current_train_items 296448.
I0304 19:32:27.710717 23118544486528 run.py:483] Algo bellman_ford step 9264 current loss 0.131917, current_train_items 296480.
I0304 19:32:27.730395 23118544486528 run.py:483] Algo bellman_ford step 9265 current loss 0.021233, current_train_items 296512.
I0304 19:32:27.746818 23118544486528 run.py:483] Algo bellman_ford step 9266 current loss 0.014978, current_train_items 296544.
I0304 19:32:27.770211 23118544486528 run.py:483] Algo bellman_ford step 9267 current loss 0.020788, current_train_items 296576.
I0304 19:32:27.802693 23118544486528 run.py:483] Algo bellman_ford step 9268 current loss 0.050047, current_train_items 296608.
I0304 19:32:27.836856 23118544486528 run.py:483] Algo bellman_ford step 9269 current loss 0.067064, current_train_items 296640.
I0304 19:32:27.856856 23118544486528 run.py:483] Algo bellman_ford step 9270 current loss 0.002499, current_train_items 296672.
I0304 19:32:27.873199 23118544486528 run.py:483] Algo bellman_ford step 9271 current loss 0.010779, current_train_items 296704.
I0304 19:32:27.897162 23118544486528 run.py:483] Algo bellman_ford step 9272 current loss 0.015321, current_train_items 296736.
I0304 19:32:27.931463 23118544486528 run.py:483] Algo bellman_ford step 9273 current loss 0.043894, current_train_items 296768.
I0304 19:32:27.966797 23118544486528 run.py:483] Algo bellman_ford step 9274 current loss 0.038995, current_train_items 296800.
I0304 19:32:27.986963 23118544486528 run.py:483] Algo bellman_ford step 9275 current loss 0.023410, current_train_items 296832.
I0304 19:32:28.003441 23118544486528 run.py:483] Algo bellman_ford step 9276 current loss 0.012758, current_train_items 296864.
I0304 19:32:28.027998 23118544486528 run.py:483] Algo bellman_ford step 9277 current loss 0.061841, current_train_items 296896.
I0304 19:32:28.060577 23118544486528 run.py:483] Algo bellman_ford step 9278 current loss 0.059019, current_train_items 296928.
I0304 19:32:28.094960 23118544486528 run.py:483] Algo bellman_ford step 9279 current loss 0.073484, current_train_items 296960.
I0304 19:32:28.114520 23118544486528 run.py:483] Algo bellman_ford step 9280 current loss 0.002292, current_train_items 296992.
I0304 19:32:28.131113 23118544486528 run.py:483] Algo bellman_ford step 9281 current loss 0.033523, current_train_items 297024.
I0304 19:32:28.155464 23118544486528 run.py:483] Algo bellman_ford step 9282 current loss 0.016934, current_train_items 297056.
I0304 19:32:28.186104 23118544486528 run.py:483] Algo bellman_ford step 9283 current loss 0.030648, current_train_items 297088.
I0304 19:32:28.219566 23118544486528 run.py:483] Algo bellman_ford step 9284 current loss 0.076139, current_train_items 297120.
I0304 19:32:28.239677 23118544486528 run.py:483] Algo bellman_ford step 9285 current loss 0.004048, current_train_items 297152.
I0304 19:32:28.255668 23118544486528 run.py:483] Algo bellman_ford step 9286 current loss 0.013349, current_train_items 297184.
I0304 19:32:28.281242 23118544486528 run.py:483] Algo bellman_ford step 9287 current loss 0.103804, current_train_items 297216.
I0304 19:32:28.312860 23118544486528 run.py:483] Algo bellman_ford step 9288 current loss 0.052385, current_train_items 297248.
I0304 19:32:28.348584 23118544486528 run.py:483] Algo bellman_ford step 9289 current loss 0.059067, current_train_items 297280.
I0304 19:32:28.368841 23118544486528 run.py:483] Algo bellman_ford step 9290 current loss 0.005811, current_train_items 297312.
I0304 19:32:28.385392 23118544486528 run.py:483] Algo bellman_ford step 9291 current loss 0.020052, current_train_items 297344.
I0304 19:32:28.410170 23118544486528 run.py:483] Algo bellman_ford step 9292 current loss 0.059833, current_train_items 297376.
I0304 19:32:28.441495 23118544486528 run.py:483] Algo bellman_ford step 9293 current loss 0.055634, current_train_items 297408.
I0304 19:32:28.477119 23118544486528 run.py:483] Algo bellman_ford step 9294 current loss 0.047712, current_train_items 297440.
I0304 19:32:28.496976 23118544486528 run.py:483] Algo bellman_ford step 9295 current loss 0.005170, current_train_items 297472.
I0304 19:32:28.513324 23118544486528 run.py:483] Algo bellman_ford step 9296 current loss 0.008625, current_train_items 297504.
I0304 19:32:28.537783 23118544486528 run.py:483] Algo bellman_ford step 9297 current loss 0.040106, current_train_items 297536.
I0304 19:32:28.570810 23118544486528 run.py:483] Algo bellman_ford step 9298 current loss 0.067146, current_train_items 297568.
I0304 19:32:28.605733 23118544486528 run.py:483] Algo bellman_ford step 9299 current loss 0.040438, current_train_items 297600.
I0304 19:32:28.626232 23118544486528 run.py:483] Algo bellman_ford step 9300 current loss 0.003039, current_train_items 297632.
I0304 19:32:28.634303 23118544486528 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0304 19:32:28.634407 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:28.651473 23118544486528 run.py:483] Algo bellman_ford step 9301 current loss 0.014175, current_train_items 297664.
I0304 19:32:28.676456 23118544486528 run.py:483] Algo bellman_ford step 9302 current loss 0.027867, current_train_items 297696.
I0304 19:32:28.709568 23118544486528 run.py:483] Algo bellman_ford step 9303 current loss 0.034638, current_train_items 297728.
I0304 19:32:28.745318 23118544486528 run.py:483] Algo bellman_ford step 9304 current loss 0.062451, current_train_items 297760.
I0304 19:32:28.765272 23118544486528 run.py:483] Algo bellman_ford step 9305 current loss 0.003166, current_train_items 297792.
I0304 19:32:28.781187 23118544486528 run.py:483] Algo bellman_ford step 9306 current loss 0.017414, current_train_items 297824.
I0304 19:32:28.805995 23118544486528 run.py:483] Algo bellman_ford step 9307 current loss 0.019136, current_train_items 297856.
I0304 19:32:28.838453 23118544486528 run.py:483] Algo bellman_ford step 9308 current loss 0.104890, current_train_items 297888.
I0304 19:32:28.872651 23118544486528 run.py:483] Algo bellman_ford step 9309 current loss 0.062991, current_train_items 297920.
I0304 19:32:28.892285 23118544486528 run.py:483] Algo bellman_ford step 9310 current loss 0.003076, current_train_items 297952.
I0304 19:32:28.908553 23118544486528 run.py:483] Algo bellman_ford step 9311 current loss 0.008578, current_train_items 297984.
I0304 19:32:28.932767 23118544486528 run.py:483] Algo bellman_ford step 9312 current loss 0.023680, current_train_items 298016.
I0304 19:32:28.963983 23118544486528 run.py:483] Algo bellman_ford step 9313 current loss 0.037008, current_train_items 298048.
I0304 19:32:28.998896 23118544486528 run.py:483] Algo bellman_ford step 9314 current loss 0.087962, current_train_items 298080.
I0304 19:32:29.018776 23118544486528 run.py:483] Algo bellman_ford step 9315 current loss 0.003027, current_train_items 298112.
I0304 19:32:29.034852 23118544486528 run.py:483] Algo bellman_ford step 9316 current loss 0.005024, current_train_items 298144.
I0304 19:32:29.059203 23118544486528 run.py:483] Algo bellman_ford step 9317 current loss 0.044922, current_train_items 298176.
I0304 19:32:29.092212 23118544486528 run.py:483] Algo bellman_ford step 9318 current loss 0.057829, current_train_items 298208.
I0304 19:32:29.126747 23118544486528 run.py:483] Algo bellman_ford step 9319 current loss 0.039846, current_train_items 298240.
I0304 19:32:29.146967 23118544486528 run.py:483] Algo bellman_ford step 9320 current loss 0.005854, current_train_items 298272.
I0304 19:32:29.163416 23118544486528 run.py:483] Algo bellman_ford step 9321 current loss 0.012962, current_train_items 298304.
I0304 19:32:29.187062 23118544486528 run.py:483] Algo bellman_ford step 9322 current loss 0.024506, current_train_items 298336.
I0304 19:32:29.218643 23118544486528 run.py:483] Algo bellman_ford step 9323 current loss 0.031940, current_train_items 298368.
I0304 19:32:29.255914 23118544486528 run.py:483] Algo bellman_ford step 9324 current loss 0.077036, current_train_items 298400.
I0304 19:32:29.275659 23118544486528 run.py:483] Algo bellman_ford step 9325 current loss 0.003880, current_train_items 298432.
I0304 19:32:29.291959 23118544486528 run.py:483] Algo bellman_ford step 9326 current loss 0.023709, current_train_items 298464.
I0304 19:32:29.317032 23118544486528 run.py:483] Algo bellman_ford step 9327 current loss 0.053731, current_train_items 298496.
I0304 19:32:29.350423 23118544486528 run.py:483] Algo bellman_ford step 9328 current loss 0.041771, current_train_items 298528.
I0304 19:32:29.381999 23118544486528 run.py:483] Algo bellman_ford step 9329 current loss 0.031621, current_train_items 298560.
I0304 19:32:29.401749 23118544486528 run.py:483] Algo bellman_ford step 9330 current loss 0.003742, current_train_items 298592.
I0304 19:32:29.418281 23118544486528 run.py:483] Algo bellman_ford step 9331 current loss 0.036103, current_train_items 298624.
I0304 19:32:29.442506 23118544486528 run.py:483] Algo bellman_ford step 9332 current loss 0.044134, current_train_items 298656.
I0304 19:32:29.474829 23118544486528 run.py:483] Algo bellman_ford step 9333 current loss 0.029217, current_train_items 298688.
I0304 19:32:29.508471 23118544486528 run.py:483] Algo bellman_ford step 9334 current loss 0.080095, current_train_items 298720.
I0304 19:32:29.528532 23118544486528 run.py:483] Algo bellman_ford step 9335 current loss 0.002554, current_train_items 298752.
I0304 19:32:29.545013 23118544486528 run.py:483] Algo bellman_ford step 9336 current loss 0.033651, current_train_items 298784.
I0304 19:32:29.570665 23118544486528 run.py:483] Algo bellman_ford step 9337 current loss 0.043544, current_train_items 298816.
I0304 19:32:29.602984 23118544486528 run.py:483] Algo bellman_ford step 9338 current loss 0.041619, current_train_items 298848.
I0304 19:32:29.637870 23118544486528 run.py:483] Algo bellman_ford step 9339 current loss 0.062216, current_train_items 298880.
I0304 19:32:29.657620 23118544486528 run.py:483] Algo bellman_ford step 9340 current loss 0.006392, current_train_items 298912.
I0304 19:32:29.673678 23118544486528 run.py:483] Algo bellman_ford step 9341 current loss 0.009567, current_train_items 298944.
I0304 19:32:29.698823 23118544486528 run.py:483] Algo bellman_ford step 9342 current loss 0.062800, current_train_items 298976.
I0304 19:32:29.731589 23118544486528 run.py:483] Algo bellman_ford step 9343 current loss 0.042129, current_train_items 299008.
I0304 19:32:29.766307 23118544486528 run.py:483] Algo bellman_ford step 9344 current loss 0.076538, current_train_items 299040.
I0304 19:32:29.786220 23118544486528 run.py:483] Algo bellman_ford step 9345 current loss 0.003347, current_train_items 299072.
I0304 19:32:29.802603 23118544486528 run.py:483] Algo bellman_ford step 9346 current loss 0.006911, current_train_items 299104.
I0304 19:32:29.827651 23118544486528 run.py:483] Algo bellman_ford step 9347 current loss 0.018263, current_train_items 299136.
I0304 19:32:29.860247 23118544486528 run.py:483] Algo bellman_ford step 9348 current loss 0.035773, current_train_items 299168.
I0304 19:32:29.893328 23118544486528 run.py:483] Algo bellman_ford step 9349 current loss 0.060690, current_train_items 299200.
I0304 19:32:29.913929 23118544486528 run.py:483] Algo bellman_ford step 9350 current loss 0.040754, current_train_items 299232.
I0304 19:32:29.921803 23118544486528 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0304 19:32:29.921908 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0304 19:32:29.938639 23118544486528 run.py:483] Algo bellman_ford step 9351 current loss 0.003474, current_train_items 299264.
I0304 19:32:29.964484 23118544486528 run.py:483] Algo bellman_ford step 9352 current loss 0.037223, current_train_items 299296.
I0304 19:32:29.997525 23118544486528 run.py:483] Algo bellman_ford step 9353 current loss 0.033292, current_train_items 299328.
I0304 19:32:30.033972 23118544486528 run.py:483] Algo bellman_ford step 9354 current loss 0.091879, current_train_items 299360.
I0304 19:32:30.053736 23118544486528 run.py:483] Algo bellman_ford step 9355 current loss 0.039265, current_train_items 299392.
I0304 19:32:30.069802 23118544486528 run.py:483] Algo bellman_ford step 9356 current loss 0.027531, current_train_items 299424.
I0304 19:32:30.094840 23118544486528 run.py:483] Algo bellman_ford step 9357 current loss 0.068252, current_train_items 299456.
I0304 19:32:30.127504 23118544486528 run.py:483] Algo bellman_ford step 9358 current loss 0.023299, current_train_items 299488.
I0304 19:32:30.161469 23118544486528 run.py:483] Algo bellman_ford step 9359 current loss 0.028563, current_train_items 299520.
I0304 19:32:30.181585 23118544486528 run.py:483] Algo bellman_ford step 9360 current loss 0.015227, current_train_items 299552.
I0304 19:32:30.198126 23118544486528 run.py:483] Algo bellman_ford step 9361 current loss 0.011508, current_train_items 299584.
I0304 19:32:30.222097 23118544486528 run.py:483] Algo bellman_ford step 9362 current loss 0.040825, current_train_items 299616.
I0304 19:32:30.255165 23118544486528 run.py:483] Algo bellman_ford step 9363 current loss 0.032959, current_train_items 299648.
I0304 19:32:30.290329 23118544486528 run.py:483] Algo bellman_ford step 9364 current loss 0.053694, current_train_items 299680.
I0304 19:32:30.310108 23118544486528 run.py:483] Algo bellman_ford step 9365 current loss 0.003538, current_train_items 299712.
I0304 19:32:30.327187 23118544486528 run.py:483] Algo bellman_ford step 9366 current loss 0.037652, current_train_items 299744.
I0304 19:32:30.353080 23118544486528 run.py:483] Algo bellman_ford step 9367 current loss 0.041436, current_train_items 299776.
I0304 19:32:30.386529 23118544486528 run.py:483] Algo bellman_ford step 9368 current loss 0.046419, current_train_items 299808.
I0304 19:32:30.420348 23118544486528 run.py:483] Algo bellman_ford step 9369 current loss 0.045138, current_train_items 299840.
I0304 19:32:30.441267 23118544486528 run.py:483] Algo bellman_ford step 9370 current loss 0.014694, current_train_items 299872.
I0304 19:32:30.457903 23118544486528 run.py:483] Algo bellman_ford step 9371 current loss 0.020829, current_train_items 299904.
I0304 19:32:30.482165 23118544486528 run.py:483] Algo bellman_ford step 9372 current loss 0.018463, current_train_items 299936.
I0304 19:32:30.515519 23118544486528 run.py:483] Algo bellman_ford step 9373 current loss 0.054683, current_train_items 299968.
I0304 19:32:30.550275 23118544486528 run.py:483] Algo bellman_ford step 9374 current loss 0.040691, current_train_items 300000.
I0304 19:32:30.570416 23118544486528 run.py:483] Algo bellman_ford step 9375 current loss 0.016002, current_train_items 300032.
I0304 19:32:30.586448 23118544486528 run.py:483] Algo bellman_ford step 9376 current loss 0.023804, current_train_items 300064.
I0304 19:32:30.610926 23118544486528 run.py:483] Algo bellman_ford step 9377 current loss 0.043615, current_train_items 300096.
I0304 19:32:30.642486 23118544486528 run.py:483] Algo bellman_ford step 9378 current loss 0.031936, current_train_items 300128.
I0304 19:32:30.677616 23118544486528 run.py:483] Algo bellman_ford step 9379 current loss 0.055833, current_train_items 300160.
I0304 19:32:30.697558 23118544486528 run.py:483] Algo bellman_ford step 9380 current loss 0.002952, current_train_items 300192.
I0304 19:32:30.713482 23118544486528 run.py:483] Algo bellman_ford step 9381 current loss 0.036562, current_train_items 300224.
I0304 19:32:30.738124 23118544486528 run.py:483] Algo bellman_ford step 9382 current loss 0.030337, current_train_items 300256.
I0304 19:32:30.769741 23118544486528 run.py:483] Algo bellman_ford step 9383 current loss 0.036956, current_train_items 300288.
I0304 19:32:30.804345 23118544486528 run.py:483] Algo bellman_ford step 9384 current loss 0.042389, current_train_items 300320.
I0304 19:32:30.824728 23118544486528 run.py:483] Algo bellman_ford step 9385 current loss 0.003467, current_train_items 300352.
I0304 19:32:30.841617 23118544486528 run.py:483] Algo bellman_ford step 9386 current loss 0.026302, current_train_items 300384.
I0304 19:32:30.865765 23118544486528 run.py:483] Algo bellman_ford step 9387 current loss 0.028047, current_train_items 300416.
I0304 19:32:30.898014 23118544486528 run.py:483] Algo bellman_ford step 9388 current loss 0.038324, current_train_items 300448.
I0304 19:32:30.934577 23118544486528 run.py:483] Algo bellman_ford step 9389 current loss 0.072272, current_train_items 300480.
I0304 19:32:30.955027 23118544486528 run.py:483] Algo bellman_ford step 9390 current loss 0.015527, current_train_items 300512.
I0304 19:32:30.971923 23118544486528 run.py:483] Algo bellman_ford step 9391 current loss 0.008774, current_train_items 300544.
I0304 19:32:30.995795 23118544486528 run.py:483] Algo bellman_ford step 9392 current loss 0.022825, current_train_items 300576.
I0304 19:32:31.027632 23118544486528 run.py:483] Algo bellman_ford step 9393 current loss 0.029545, current_train_items 300608.
I0304 19:32:31.061464 23118544486528 run.py:483] Algo bellman_ford step 9394 current loss 0.044017, current_train_items 300640.
I0304 19:32:31.081490 23118544486528 run.py:483] Algo bellman_ford step 9395 current loss 0.013872, current_train_items 300672.
I0304 19:32:31.098026 23118544486528 run.py:483] Algo bellman_ford step 9396 current loss 0.025513, current_train_items 300704.
I0304 19:32:31.123387 23118544486528 run.py:483] Algo bellman_ford step 9397 current loss 0.067497, current_train_items 300736.
I0304 19:32:31.156012 23118544486528 run.py:483] Algo bellman_ford step 9398 current loss 0.085315, current_train_items 300768.
I0304 19:32:31.190575 23118544486528 run.py:483] Algo bellman_ford step 9399 current loss 0.067982, current_train_items 300800.
I0304 19:32:31.211211 23118544486528 run.py:483] Algo bellman_ford step 9400 current loss 0.007588, current_train_items 300832.
I0304 19:32:31.219288 23118544486528 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0304 19:32:31.219393 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0304 19:32:31.236401 23118544486528 run.py:483] Algo bellman_ford step 9401 current loss 0.010263, current_train_items 300864.
I0304 19:32:31.262190 23118544486528 run.py:483] Algo bellman_ford step 9402 current loss 0.057428, current_train_items 300896.
I0304 19:32:31.294542 23118544486528 run.py:483] Algo bellman_ford step 9403 current loss 0.064153, current_train_items 300928.
I0304 19:32:31.330238 23118544486528 run.py:483] Algo bellman_ford step 9404 current loss 0.072919, current_train_items 300960.
I0304 19:32:31.350409 23118544486528 run.py:483] Algo bellman_ford step 9405 current loss 0.006616, current_train_items 300992.
I0304 19:32:31.366399 23118544486528 run.py:483] Algo bellman_ford step 9406 current loss 0.018362, current_train_items 301024.
I0304 19:32:31.392208 23118544486528 run.py:483] Algo bellman_ford step 9407 current loss 0.040105, current_train_items 301056.
I0304 19:32:31.424011 23118544486528 run.py:483] Algo bellman_ford step 9408 current loss 0.047186, current_train_items 301088.
I0304 19:32:31.459573 23118544486528 run.py:483] Algo bellman_ford step 9409 current loss 0.068645, current_train_items 301120.
I0304 19:32:31.479390 23118544486528 run.py:483] Algo bellman_ford step 9410 current loss 0.003547, current_train_items 301152.
I0304 19:32:31.495582 23118544486528 run.py:483] Algo bellman_ford step 9411 current loss 0.009386, current_train_items 301184.
I0304 19:32:31.520148 23118544486528 run.py:483] Algo bellman_ford step 9412 current loss 0.023426, current_train_items 301216.
I0304 19:32:31.552910 23118544486528 run.py:483] Algo bellman_ford step 9413 current loss 0.036663, current_train_items 301248.
I0304 19:32:31.586690 23118544486528 run.py:483] Algo bellman_ford step 9414 current loss 0.078256, current_train_items 301280.
I0304 19:32:31.606473 23118544486528 run.py:483] Algo bellman_ford step 9415 current loss 0.013107, current_train_items 301312.
I0304 19:32:31.622991 23118544486528 run.py:483] Algo bellman_ford step 9416 current loss 0.020372, current_train_items 301344.
I0304 19:32:31.647973 23118544486528 run.py:483] Algo bellman_ford step 9417 current loss 0.028424, current_train_items 301376.
I0304 19:32:31.679654 23118544486528 run.py:483] Algo bellman_ford step 9418 current loss 0.031040, current_train_items 301408.
I0304 19:32:31.711829 23118544486528 run.py:483] Algo bellman_ford step 9419 current loss 0.019633, current_train_items 301440.
I0304 19:32:31.731372 23118544486528 run.py:483] Algo bellman_ford step 9420 current loss 0.008667, current_train_items 301472.
I0304 19:32:31.747873 23118544486528 run.py:483] Algo bellman_ford step 9421 current loss 0.034929, current_train_items 301504.
I0304 19:32:31.772759 23118544486528 run.py:483] Algo bellman_ford step 9422 current loss 0.027390, current_train_items 301536.
I0304 19:32:31.805071 23118544486528 run.py:483] Algo bellman_ford step 9423 current loss 0.031048, current_train_items 301568.
I0304 19:32:31.839478 23118544486528 run.py:483] Algo bellman_ford step 9424 current loss 0.037324, current_train_items 301600.
I0304 19:32:31.859324 23118544486528 run.py:483] Algo bellman_ford step 9425 current loss 0.037257, current_train_items 301632.
I0304 19:32:31.875288 23118544486528 run.py:483] Algo bellman_ford step 9426 current loss 0.012780, current_train_items 301664.
I0304 19:32:31.899190 23118544486528 run.py:483] Algo bellman_ford step 9427 current loss 0.023389, current_train_items 301696.
I0304 19:32:31.932450 23118544486528 run.py:483] Algo bellman_ford step 9428 current loss 0.040946, current_train_items 301728.
I0304 19:32:31.964118 23118544486528 run.py:483] Algo bellman_ford step 9429 current loss 0.092819, current_train_items 301760.
I0304 19:32:31.984017 23118544486528 run.py:483] Algo bellman_ford step 9430 current loss 0.005402, current_train_items 301792.
I0304 19:32:32.000277 23118544486528 run.py:483] Algo bellman_ford step 9431 current loss 0.038771, current_train_items 301824.
I0304 19:32:32.024767 23118544486528 run.py:483] Algo bellman_ford step 9432 current loss 0.050439, current_train_items 301856.
I0304 19:32:32.056369 23118544486528 run.py:483] Algo bellman_ford step 9433 current loss 0.040105, current_train_items 301888.
I0304 19:32:32.091347 23118544486528 run.py:483] Algo bellman_ford step 9434 current loss 0.056107, current_train_items 301920.
I0304 19:32:32.111351 23118544486528 run.py:483] Algo bellman_ford step 9435 current loss 0.003172, current_train_items 301952.
I0304 19:32:32.127895 23118544486528 run.py:483] Algo bellman_ford step 9436 current loss 0.039064, current_train_items 301984.
I0304 19:32:32.152595 23118544486528 run.py:483] Algo bellman_ford step 9437 current loss 0.021072, current_train_items 302016.
I0304 19:32:32.185405 23118544486528 run.py:483] Algo bellman_ford step 9438 current loss 0.040522, current_train_items 302048.
I0304 19:32:32.218587 23118544486528 run.py:483] Algo bellman_ford step 9439 current loss 0.087137, current_train_items 302080.
I0304 19:32:32.238671 23118544486528 run.py:483] Algo bellman_ford step 9440 current loss 0.044606, current_train_items 302112.
I0304 19:32:32.255508 23118544486528 run.py:483] Algo bellman_ford step 9441 current loss 0.039364, current_train_items 302144.
I0304 19:32:32.280390 23118544486528 run.py:483] Algo bellman_ford step 9442 current loss 0.042118, current_train_items 302176.
I0304 19:32:32.313214 23118544486528 run.py:483] Algo bellman_ford step 9443 current loss 0.023786, current_train_items 302208.
I0304 19:32:32.349865 23118544486528 run.py:483] Algo bellman_ford step 9444 current loss 0.090066, current_train_items 302240.
I0304 19:32:32.369739 23118544486528 run.py:483] Algo bellman_ford step 9445 current loss 0.048721, current_train_items 302272.
I0304 19:32:32.385638 23118544486528 run.py:483] Algo bellman_ford step 9446 current loss 0.011323, current_train_items 302304.
I0304 19:32:32.409994 23118544486528 run.py:483] Algo bellman_ford step 9447 current loss 0.043720, current_train_items 302336.
I0304 19:32:32.441879 23118544486528 run.py:483] Algo bellman_ford step 9448 current loss 0.094847, current_train_items 302368.
I0304 19:32:32.474899 23118544486528 run.py:483] Algo bellman_ford step 9449 current loss 0.135883, current_train_items 302400.
I0304 19:32:32.494762 23118544486528 run.py:483] Algo bellman_ford step 9450 current loss 0.004548, current_train_items 302432.
I0304 19:32:32.502860 23118544486528 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0304 19:32:32.502967 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0304 19:32:32.520217 23118544486528 run.py:483] Algo bellman_ford step 9451 current loss 0.008431, current_train_items 302464.
I0304 19:32:32.544157 23118544486528 run.py:483] Algo bellman_ford step 9452 current loss 0.034355, current_train_items 302496.
I0304 19:32:32.577153 23118544486528 run.py:483] Algo bellman_ford step 9453 current loss 0.040645, current_train_items 302528.
I0304 19:32:32.612198 23118544486528 run.py:483] Algo bellman_ford step 9454 current loss 0.075001, current_train_items 302560.
I0304 19:32:32.632487 23118544486528 run.py:483] Algo bellman_ford step 9455 current loss 0.002887, current_train_items 302592.
I0304 19:32:32.648589 23118544486528 run.py:483] Algo bellman_ford step 9456 current loss 0.007794, current_train_items 302624.
I0304 19:32:32.673013 23118544486528 run.py:483] Algo bellman_ford step 9457 current loss 0.046842, current_train_items 302656.
I0304 19:32:32.705690 23118544486528 run.py:483] Algo bellman_ford step 9458 current loss 0.051207, current_train_items 302688.
I0304 19:32:32.738435 23118544486528 run.py:483] Algo bellman_ford step 9459 current loss 0.058537, current_train_items 302720.
I0304 19:32:32.759172 23118544486528 run.py:483] Algo bellman_ford step 9460 current loss 0.004450, current_train_items 302752.
I0304 19:32:32.775911 23118544486528 run.py:483] Algo bellman_ford step 9461 current loss 0.034491, current_train_items 302784.
I0304 19:32:32.799468 23118544486528 run.py:483] Algo bellman_ford step 9462 current loss 0.016944, current_train_items 302816.
I0304 19:32:32.832581 23118544486528 run.py:483] Algo bellman_ford step 9463 current loss 0.043559, current_train_items 302848.
I0304 19:32:32.866590 23118544486528 run.py:483] Algo bellman_ford step 9464 current loss 0.046669, current_train_items 302880.
I0304 19:32:32.886322 23118544486528 run.py:483] Algo bellman_ford step 9465 current loss 0.006322, current_train_items 302912.
I0304 19:32:32.903128 23118544486528 run.py:483] Algo bellman_ford step 9466 current loss 0.029382, current_train_items 302944.
I0304 19:32:32.929445 23118544486528 run.py:483] Algo bellman_ford step 9467 current loss 0.107407, current_train_items 302976.
I0304 19:32:32.962412 23118544486528 run.py:483] Algo bellman_ford step 9468 current loss 0.049127, current_train_items 303008.
I0304 19:32:32.997496 23118544486528 run.py:483] Algo bellman_ford step 9469 current loss 0.080166, current_train_items 303040.
I0304 19:32:33.017982 23118544486528 run.py:483] Algo bellman_ford step 9470 current loss 0.012769, current_train_items 303072.
I0304 19:32:33.034061 23118544486528 run.py:483] Algo bellman_ford step 9471 current loss 0.023864, current_train_items 303104.
I0304 19:32:33.058485 23118544486528 run.py:483] Algo bellman_ford step 9472 current loss 0.022867, current_train_items 303136.
I0304 19:32:33.091090 23118544486528 run.py:483] Algo bellman_ford step 9473 current loss 0.049905, current_train_items 303168.
I0304 19:32:33.124490 23118544486528 run.py:483] Algo bellman_ford step 9474 current loss 0.076619, current_train_items 303200.
I0304 19:32:33.144569 23118544486528 run.py:483] Algo bellman_ford step 9475 current loss 0.003092, current_train_items 303232.
I0304 19:32:33.161124 23118544486528 run.py:483] Algo bellman_ford step 9476 current loss 0.057013, current_train_items 303264.
I0304 19:32:33.185322 23118544486528 run.py:483] Algo bellman_ford step 9477 current loss 0.025933, current_train_items 303296.
I0304 19:32:33.217411 23118544486528 run.py:483] Algo bellman_ford step 9478 current loss 0.049449, current_train_items 303328.
I0304 19:32:33.251031 23118544486528 run.py:483] Algo bellman_ford step 9479 current loss 0.090434, current_train_items 303360.
I0304 19:32:33.270707 23118544486528 run.py:483] Algo bellman_ford step 9480 current loss 0.005974, current_train_items 303392.
I0304 19:32:33.286897 23118544486528 run.py:483] Algo bellman_ford step 9481 current loss 0.023303, current_train_items 303424.
I0304 19:32:33.312675 23118544486528 run.py:483] Algo bellman_ford step 9482 current loss 0.049116, current_train_items 303456.
I0304 19:32:33.343836 23118544486528 run.py:483] Algo bellman_ford step 9483 current loss 0.079730, current_train_items 303488.
I0304 19:32:33.377401 23118544486528 run.py:483] Algo bellman_ford step 9484 current loss 0.076261, current_train_items 303520.
I0304 19:32:33.397412 23118544486528 run.py:483] Algo bellman_ford step 9485 current loss 0.005473, current_train_items 303552.
I0304 19:32:33.414116 23118544486528 run.py:483] Algo bellman_ford step 9486 current loss 0.007563, current_train_items 303584.
I0304 19:32:33.439502 23118544486528 run.py:483] Algo bellman_ford step 9487 current loss 0.032753, current_train_items 303616.
I0304 19:32:33.473078 23118544486528 run.py:483] Algo bellman_ford step 9488 current loss 0.050015, current_train_items 303648.
I0304 19:32:33.506137 23118544486528 run.py:483] Algo bellman_ford step 9489 current loss 0.042789, current_train_items 303680.
I0304 19:32:33.526379 23118544486528 run.py:483] Algo bellman_ford step 9490 current loss 0.004751, current_train_items 303712.
I0304 19:32:33.542590 23118544486528 run.py:483] Algo bellman_ford step 9491 current loss 0.023773, current_train_items 303744.
I0304 19:32:33.567578 23118544486528 run.py:483] Algo bellman_ford step 9492 current loss 0.025325, current_train_items 303776.
I0304 19:32:33.598999 23118544486528 run.py:483] Algo bellman_ford step 9493 current loss 0.020917, current_train_items 303808.
I0304 19:32:33.633038 23118544486528 run.py:483] Algo bellman_ford step 9494 current loss 0.077698, current_train_items 303840.
I0304 19:32:33.652829 23118544486528 run.py:483] Algo bellman_ford step 9495 current loss 0.006028, current_train_items 303872.
I0304 19:32:33.669735 23118544486528 run.py:483] Algo bellman_ford step 9496 current loss 0.019805, current_train_items 303904.
I0304 19:32:33.694424 23118544486528 run.py:483] Algo bellman_ford step 9497 current loss 0.026104, current_train_items 303936.
I0304 19:32:33.726068 23118544486528 run.py:483] Algo bellman_ford step 9498 current loss 0.039871, current_train_items 303968.
I0304 19:32:33.760965 23118544486528 run.py:483] Algo bellman_ford step 9499 current loss 0.039076, current_train_items 304000.
I0304 19:32:33.781371 23118544486528 run.py:483] Algo bellman_ford step 9500 current loss 0.003236, current_train_items 304032.
I0304 19:32:33.789155 23118544486528 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0304 19:32:33.789278 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:33.805985 23118544486528 run.py:483] Algo bellman_ford step 9501 current loss 0.016554, current_train_items 304064.
I0304 19:32:33.831498 23118544486528 run.py:483] Algo bellman_ford step 9502 current loss 0.034612, current_train_items 304096.
I0304 19:32:33.864384 23118544486528 run.py:483] Algo bellman_ford step 9503 current loss 0.021539, current_train_items 304128.
I0304 19:32:33.897706 23118544486528 run.py:483] Algo bellman_ford step 9504 current loss 0.076748, current_train_items 304160.
I0304 19:32:33.918130 23118544486528 run.py:483] Algo bellman_ford step 9505 current loss 0.006579, current_train_items 304192.
I0304 19:32:33.934414 23118544486528 run.py:483] Algo bellman_ford step 9506 current loss 0.008579, current_train_items 304224.
I0304 19:32:33.959765 23118544486528 run.py:483] Algo bellman_ford step 9507 current loss 0.024977, current_train_items 304256.
I0304 19:32:33.991091 23118544486528 run.py:483] Algo bellman_ford step 9508 current loss 0.039917, current_train_items 304288.
I0304 19:32:34.028216 23118544486528 run.py:483] Algo bellman_ford step 9509 current loss 0.089221, current_train_items 304320.
I0304 19:32:34.048464 23118544486528 run.py:483] Algo bellman_ford step 9510 current loss 0.004148, current_train_items 304352.
I0304 19:32:34.065496 23118544486528 run.py:483] Algo bellman_ford step 9511 current loss 0.010493, current_train_items 304384.
I0304 19:32:34.090483 23118544486528 run.py:483] Algo bellman_ford step 9512 current loss 0.032951, current_train_items 304416.
I0304 19:32:34.122692 23118544486528 run.py:483] Algo bellman_ford step 9513 current loss 0.036231, current_train_items 304448.
I0304 19:32:34.155926 23118544486528 run.py:483] Algo bellman_ford step 9514 current loss 0.031115, current_train_items 304480.
I0304 19:32:34.176030 23118544486528 run.py:483] Algo bellman_ford step 9515 current loss 0.008990, current_train_items 304512.
I0304 19:32:34.192011 23118544486528 run.py:483] Algo bellman_ford step 9516 current loss 0.012269, current_train_items 304544.
I0304 19:32:34.216902 23118544486528 run.py:483] Algo bellman_ford step 9517 current loss 0.020634, current_train_items 304576.
I0304 19:32:34.249573 23118544486528 run.py:483] Algo bellman_ford step 9518 current loss 0.052139, current_train_items 304608.
I0304 19:32:34.282440 23118544486528 run.py:483] Algo bellman_ford step 9519 current loss 0.032416, current_train_items 304640.
I0304 19:32:34.302402 23118544486528 run.py:483] Algo bellman_ford step 9520 current loss 0.002347, current_train_items 304672.
I0304 19:32:34.318081 23118544486528 run.py:483] Algo bellman_ford step 9521 current loss 0.004145, current_train_items 304704.
I0304 19:32:34.342536 23118544486528 run.py:483] Algo bellman_ford step 9522 current loss 0.037434, current_train_items 304736.
I0304 19:32:34.373282 23118544486528 run.py:483] Algo bellman_ford step 9523 current loss 0.058339, current_train_items 304768.
I0304 19:32:34.406424 23118544486528 run.py:483] Algo bellman_ford step 9524 current loss 0.049706, current_train_items 304800.
I0304 19:32:34.425753 23118544486528 run.py:483] Algo bellman_ford step 9525 current loss 0.003161, current_train_items 304832.
I0304 19:32:34.442092 23118544486528 run.py:483] Algo bellman_ford step 9526 current loss 0.007028, current_train_items 304864.
I0304 19:32:34.466890 23118544486528 run.py:483] Algo bellman_ford step 9527 current loss 0.056224, current_train_items 304896.
I0304 19:32:34.499745 23118544486528 run.py:483] Algo bellman_ford step 9528 current loss 0.089918, current_train_items 304928.
I0304 19:32:34.535051 23118544486528 run.py:483] Algo bellman_ford step 9529 current loss 0.039453, current_train_items 304960.
I0304 19:32:34.554957 23118544486528 run.py:483] Algo bellman_ford step 9530 current loss 0.004145, current_train_items 304992.
I0304 19:32:34.571242 23118544486528 run.py:483] Algo bellman_ford step 9531 current loss 0.039248, current_train_items 305024.
I0304 19:32:34.595391 23118544486528 run.py:483] Algo bellman_ford step 9532 current loss 0.032290, current_train_items 305056.
I0304 19:32:34.629229 23118544486528 run.py:483] Algo bellman_ford step 9533 current loss 0.084323, current_train_items 305088.
I0304 19:32:34.663084 23118544486528 run.py:483] Algo bellman_ford step 9534 current loss 0.052621, current_train_items 305120.
I0304 19:32:34.683388 23118544486528 run.py:483] Algo bellman_ford step 9535 current loss 0.018566, current_train_items 305152.
I0304 19:32:34.700031 23118544486528 run.py:483] Algo bellman_ford step 9536 current loss 0.015965, current_train_items 305184.
I0304 19:32:34.724549 23118544486528 run.py:483] Algo bellman_ford step 9537 current loss 0.044171, current_train_items 305216.
I0304 19:32:34.757712 23118544486528 run.py:483] Algo bellman_ford step 9538 current loss 0.050505, current_train_items 305248.
I0304 19:32:34.789674 23118544486528 run.py:483] Algo bellman_ford step 9539 current loss 0.050761, current_train_items 305280.
I0304 19:32:34.809894 23118544486528 run.py:483] Algo bellman_ford step 9540 current loss 0.005397, current_train_items 305312.
I0304 19:32:34.826043 23118544486528 run.py:483] Algo bellman_ford step 9541 current loss 0.057854, current_train_items 305344.
I0304 19:32:34.850671 23118544486528 run.py:483] Algo bellman_ford step 9542 current loss 0.027883, current_train_items 305376.
I0304 19:32:34.884753 23118544486528 run.py:483] Algo bellman_ford step 9543 current loss 0.061227, current_train_items 305408.
I0304 19:32:34.919405 23118544486528 run.py:483] Algo bellman_ford step 9544 current loss 0.051587, current_train_items 305440.
I0304 19:32:34.939301 23118544486528 run.py:483] Algo bellman_ford step 9545 current loss 0.003787, current_train_items 305472.
I0304 19:32:34.955112 23118544486528 run.py:483] Algo bellman_ford step 9546 current loss 0.030888, current_train_items 305504.
I0304 19:32:34.977355 23118544486528 run.py:483] Algo bellman_ford step 9547 current loss 0.019099, current_train_items 305536.
I0304 19:32:35.009506 23118544486528 run.py:483] Algo bellman_ford step 9548 current loss 0.042143, current_train_items 305568.
I0304 19:32:35.044281 23118544486528 run.py:483] Algo bellman_ford step 9549 current loss 0.038273, current_train_items 305600.
I0304 19:32:35.064085 23118544486528 run.py:483] Algo bellman_ford step 9550 current loss 0.005501, current_train_items 305632.
I0304 19:32:35.072103 23118544486528 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0304 19:32:35.072208 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:35.089129 23118544486528 run.py:483] Algo bellman_ford step 9551 current loss 0.024012, current_train_items 305664.
I0304 19:32:35.114643 23118544486528 run.py:483] Algo bellman_ford step 9552 current loss 0.091851, current_train_items 305696.
I0304 19:32:35.149357 23118544486528 run.py:483] Algo bellman_ford step 9553 current loss 0.082337, current_train_items 305728.
I0304 19:32:35.183208 23118544486528 run.py:483] Algo bellman_ford step 9554 current loss 0.048816, current_train_items 305760.
I0304 19:32:35.203765 23118544486528 run.py:483] Algo bellman_ford step 9555 current loss 0.005879, current_train_items 305792.
I0304 19:32:35.219474 23118544486528 run.py:483] Algo bellman_ford step 9556 current loss 0.009956, current_train_items 305824.
I0304 19:32:35.244254 23118544486528 run.py:483] Algo bellman_ford step 9557 current loss 0.066447, current_train_items 305856.
I0304 19:32:35.276554 23118544486528 run.py:483] Algo bellman_ford step 9558 current loss 0.047570, current_train_items 305888.
I0304 19:32:35.310191 23118544486528 run.py:483] Algo bellman_ford step 9559 current loss 0.070493, current_train_items 305920.
I0304 19:32:35.330346 23118544486528 run.py:483] Algo bellman_ford step 9560 current loss 0.004422, current_train_items 305952.
I0304 19:32:35.346827 23118544486528 run.py:483] Algo bellman_ford step 9561 current loss 0.011093, current_train_items 305984.
I0304 19:32:35.371035 23118544486528 run.py:483] Algo bellman_ford step 9562 current loss 0.018740, current_train_items 306016.
I0304 19:32:35.402838 23118544486528 run.py:483] Algo bellman_ford step 9563 current loss 0.031825, current_train_items 306048.
I0304 19:32:35.436387 23118544486528 run.py:483] Algo bellman_ford step 9564 current loss 0.049269, current_train_items 306080.
I0304 19:32:35.456131 23118544486528 run.py:483] Algo bellman_ford step 9565 current loss 0.016359, current_train_items 306112.
I0304 19:32:35.473016 23118544486528 run.py:483] Algo bellman_ford step 9566 current loss 0.010092, current_train_items 306144.
I0304 19:32:35.498535 23118544486528 run.py:483] Algo bellman_ford step 9567 current loss 0.053833, current_train_items 306176.
I0304 19:32:35.531511 23118544486528 run.py:483] Algo bellman_ford step 9568 current loss 0.057917, current_train_items 306208.
I0304 19:32:35.565351 23118544486528 run.py:483] Algo bellman_ford step 9569 current loss 0.062534, current_train_items 306240.
I0304 19:32:35.585617 23118544486528 run.py:483] Algo bellman_ford step 9570 current loss 0.004742, current_train_items 306272.
I0304 19:32:35.602039 23118544486528 run.py:483] Algo bellman_ford step 9571 current loss 0.006389, current_train_items 306304.
I0304 19:32:35.627232 23118544486528 run.py:483] Algo bellman_ford step 9572 current loss 0.103238, current_train_items 306336.
I0304 19:32:35.659332 23118544486528 run.py:483] Algo bellman_ford step 9573 current loss 0.065910, current_train_items 306368.
I0304 19:32:35.694326 23118544486528 run.py:483] Algo bellman_ford step 9574 current loss 0.086317, current_train_items 306400.
I0304 19:32:35.714674 23118544486528 run.py:483] Algo bellman_ford step 9575 current loss 0.005592, current_train_items 306432.
I0304 19:32:35.731742 23118544486528 run.py:483] Algo bellman_ford step 9576 current loss 0.024950, current_train_items 306464.
I0304 19:32:35.756785 23118544486528 run.py:483] Algo bellman_ford step 9577 current loss 0.056298, current_train_items 306496.
I0304 19:32:35.788446 23118544486528 run.py:483] Algo bellman_ford step 9578 current loss 0.022539, current_train_items 306528.
I0304 19:32:35.822961 23118544486528 run.py:483] Algo bellman_ford step 9579 current loss 0.061158, current_train_items 306560.
I0304 19:32:35.842568 23118544486528 run.py:483] Algo bellman_ford step 9580 current loss 0.005742, current_train_items 306592.
I0304 19:32:35.858917 23118544486528 run.py:483] Algo bellman_ford step 9581 current loss 0.013661, current_train_items 306624.
I0304 19:32:35.885082 23118544486528 run.py:483] Algo bellman_ford step 9582 current loss 0.036789, current_train_items 306656.
I0304 19:32:35.918368 23118544486528 run.py:483] Algo bellman_ford step 9583 current loss 0.050330, current_train_items 306688.
I0304 19:32:35.953101 23118544486528 run.py:483] Algo bellman_ford step 9584 current loss 0.066320, current_train_items 306720.
I0304 19:32:35.973423 23118544486528 run.py:483] Algo bellman_ford step 9585 current loss 0.025387, current_train_items 306752.
I0304 19:32:35.989988 23118544486528 run.py:483] Algo bellman_ford step 9586 current loss 0.031690, current_train_items 306784.
I0304 19:32:36.015198 23118544486528 run.py:483] Algo bellman_ford step 9587 current loss 0.048454, current_train_items 306816.
I0304 19:32:36.048062 23118544486528 run.py:483] Algo bellman_ford step 9588 current loss 0.078863, current_train_items 306848.
I0304 19:32:36.083412 23118544486528 run.py:483] Algo bellman_ford step 9589 current loss 0.147813, current_train_items 306880.
I0304 19:32:36.103472 23118544486528 run.py:483] Algo bellman_ford step 9590 current loss 0.006932, current_train_items 306912.
I0304 19:32:36.119833 23118544486528 run.py:483] Algo bellman_ford step 9591 current loss 0.016225, current_train_items 306944.
I0304 19:32:36.144659 23118544486528 run.py:483] Algo bellman_ford step 9592 current loss 0.076761, current_train_items 306976.
I0304 19:32:36.177531 23118544486528 run.py:483] Algo bellman_ford step 9593 current loss 0.059855, current_train_items 307008.
I0304 19:32:36.211123 23118544486528 run.py:483] Algo bellman_ford step 9594 current loss 0.045454, current_train_items 307040.
I0304 19:32:36.231257 23118544486528 run.py:483] Algo bellman_ford step 9595 current loss 0.006562, current_train_items 307072.
I0304 19:32:36.247902 23118544486528 run.py:483] Algo bellman_ford step 9596 current loss 0.013172, current_train_items 307104.
I0304 19:32:36.273330 23118544486528 run.py:483] Algo bellman_ford step 9597 current loss 0.057835, current_train_items 307136.
I0304 19:32:36.304858 23118544486528 run.py:483] Algo bellman_ford step 9598 current loss 0.038729, current_train_items 307168.
I0304 19:32:36.338779 23118544486528 run.py:483] Algo bellman_ford step 9599 current loss 0.042992, current_train_items 307200.
I0304 19:32:36.359114 23118544486528 run.py:483] Algo bellman_ford step 9600 current loss 0.003859, current_train_items 307232.
I0304 19:32:36.366827 23118544486528 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0304 19:32:36.366933 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0304 19:32:36.384431 23118544486528 run.py:483] Algo bellman_ford step 9601 current loss 0.019365, current_train_items 307264.
I0304 19:32:36.409711 23118544486528 run.py:483] Algo bellman_ford step 9602 current loss 0.033301, current_train_items 307296.
I0304 19:32:36.441817 23118544486528 run.py:483] Algo bellman_ford step 9603 current loss 0.033722, current_train_items 307328.
I0304 19:32:36.477519 23118544486528 run.py:483] Algo bellman_ford step 9604 current loss 0.062601, current_train_items 307360.
I0304 19:32:36.497829 23118544486528 run.py:483] Algo bellman_ford step 9605 current loss 0.007091, current_train_items 307392.
I0304 19:32:36.514121 23118544486528 run.py:483] Algo bellman_ford step 9606 current loss 0.007269, current_train_items 307424.
I0304 19:32:36.538599 23118544486528 run.py:483] Algo bellman_ford step 9607 current loss 0.029577, current_train_items 307456.
I0304 19:32:36.570504 23118544486528 run.py:483] Algo bellman_ford step 9608 current loss 0.024374, current_train_items 307488.
I0304 19:32:36.604665 23118544486528 run.py:483] Algo bellman_ford step 9609 current loss 0.048958, current_train_items 307520.
I0304 19:32:36.624382 23118544486528 run.py:483] Algo bellman_ford step 9610 current loss 0.034274, current_train_items 307552.
I0304 19:32:36.641145 23118544486528 run.py:483] Algo bellman_ford step 9611 current loss 0.020652, current_train_items 307584.
I0304 19:32:36.666225 23118544486528 run.py:483] Algo bellman_ford step 9612 current loss 0.030055, current_train_items 307616.
I0304 19:32:36.698847 23118544486528 run.py:483] Algo bellman_ford step 9613 current loss 0.042119, current_train_items 307648.
I0304 19:32:36.733072 23118544486528 run.py:483] Algo bellman_ford step 9614 current loss 0.046516, current_train_items 307680.
I0304 19:32:36.753286 23118544486528 run.py:483] Algo bellman_ford step 9615 current loss 0.003971, current_train_items 307712.
I0304 19:32:36.769717 23118544486528 run.py:483] Algo bellman_ford step 9616 current loss 0.020103, current_train_items 307744.
I0304 19:32:36.794787 23118544486528 run.py:483] Algo bellman_ford step 9617 current loss 0.061251, current_train_items 307776.
I0304 19:32:36.826959 23118544486528 run.py:483] Algo bellman_ford step 9618 current loss 0.030300, current_train_items 307808.
I0304 19:32:36.861578 23118544486528 run.py:483] Algo bellman_ford step 9619 current loss 0.038612, current_train_items 307840.
I0304 19:32:36.881229 23118544486528 run.py:483] Algo bellman_ford step 9620 current loss 0.003915, current_train_items 307872.
I0304 19:32:36.897404 23118544486528 run.py:483] Algo bellman_ford step 9621 current loss 0.011468, current_train_items 307904.
I0304 19:32:36.922317 23118544486528 run.py:483] Algo bellman_ford step 9622 current loss 0.064178, current_train_items 307936.
I0304 19:32:36.954893 23118544486528 run.py:483] Algo bellman_ford step 9623 current loss 0.048274, current_train_items 307968.
I0304 19:32:36.989647 23118544486528 run.py:483] Algo bellman_ford step 9624 current loss 0.058076, current_train_items 308000.
I0304 19:32:37.009796 23118544486528 run.py:483] Algo bellman_ford step 9625 current loss 0.003832, current_train_items 308032.
I0304 19:32:37.026154 23118544486528 run.py:483] Algo bellman_ford step 9626 current loss 0.010245, current_train_items 308064.
I0304 19:32:37.050951 23118544486528 run.py:483] Algo bellman_ford step 9627 current loss 0.038807, current_train_items 308096.
I0304 19:32:37.082913 23118544486528 run.py:483] Algo bellman_ford step 9628 current loss 0.030385, current_train_items 308128.
I0304 19:32:37.118030 23118544486528 run.py:483] Algo bellman_ford step 9629 current loss 0.069504, current_train_items 308160.
I0304 19:32:37.138147 23118544486528 run.py:483] Algo bellman_ford step 9630 current loss 0.049570, current_train_items 308192.
I0304 19:32:37.154931 23118544486528 run.py:483] Algo bellman_ford step 9631 current loss 0.009372, current_train_items 308224.
I0304 19:32:37.180806 23118544486528 run.py:483] Algo bellman_ford step 9632 current loss 0.108657, current_train_items 308256.
I0304 19:32:37.212519 23118544486528 run.py:483] Algo bellman_ford step 9633 current loss 0.048987, current_train_items 308288.
I0304 19:32:37.247632 23118544486528 run.py:483] Algo bellman_ford step 9634 current loss 0.086613, current_train_items 308320.
I0304 19:32:37.267258 23118544486528 run.py:483] Algo bellman_ford step 9635 current loss 0.005221, current_train_items 308352.
I0304 19:32:37.283771 23118544486528 run.py:483] Algo bellman_ford step 9636 current loss 0.019881, current_train_items 308384.
I0304 19:32:37.308899 23118544486528 run.py:483] Algo bellman_ford step 9637 current loss 0.033467, current_train_items 308416.
I0304 19:32:37.342180 23118544486528 run.py:483] Algo bellman_ford step 9638 current loss 0.038050, current_train_items 308448.
I0304 19:32:37.376588 23118544486528 run.py:483] Algo bellman_ford step 9639 current loss 0.091402, current_train_items 308480.
I0304 19:32:37.396669 23118544486528 run.py:483] Algo bellman_ford step 9640 current loss 0.004976, current_train_items 308512.
I0304 19:32:37.412733 23118544486528 run.py:483] Algo bellman_ford step 9641 current loss 0.009480, current_train_items 308544.
I0304 19:32:37.437856 23118544486528 run.py:483] Algo bellman_ford step 9642 current loss 0.054779, current_train_items 308576.
I0304 19:32:37.471777 23118544486528 run.py:483] Algo bellman_ford step 9643 current loss 0.036366, current_train_items 308608.
I0304 19:32:37.504181 23118544486528 run.py:483] Algo bellman_ford step 9644 current loss 0.045859, current_train_items 308640.
I0304 19:32:37.523614 23118544486528 run.py:483] Algo bellman_ford step 9645 current loss 0.019093, current_train_items 308672.
I0304 19:32:37.540159 23118544486528 run.py:483] Algo bellman_ford step 9646 current loss 0.020481, current_train_items 308704.
I0304 19:32:37.564275 23118544486528 run.py:483] Algo bellman_ford step 9647 current loss 0.013711, current_train_items 308736.
I0304 19:32:37.597353 23118544486528 run.py:483] Algo bellman_ford step 9648 current loss 0.037448, current_train_items 308768.
I0304 19:32:37.632169 23118544486528 run.py:483] Algo bellman_ford step 9649 current loss 0.043282, current_train_items 308800.
I0304 19:32:37.651673 23118544486528 run.py:483] Algo bellman_ford step 9650 current loss 0.007585, current_train_items 308832.
I0304 19:32:37.659573 23118544486528 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0304 19:32:37.659717 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:37.676837 23118544486528 run.py:483] Algo bellman_ford step 9651 current loss 0.014211, current_train_items 308864.
I0304 19:32:37.703510 23118544486528 run.py:483] Algo bellman_ford step 9652 current loss 0.030409, current_train_items 308896.
I0304 19:32:37.736403 23118544486528 run.py:483] Algo bellman_ford step 9653 current loss 0.059064, current_train_items 308928.
I0304 19:32:37.772543 23118544486528 run.py:483] Algo bellman_ford step 9654 current loss 0.040953, current_train_items 308960.
I0304 19:32:37.792998 23118544486528 run.py:483] Algo bellman_ford step 9655 current loss 0.003911, current_train_items 308992.
I0304 19:32:37.809332 23118544486528 run.py:483] Algo bellman_ford step 9656 current loss 0.014341, current_train_items 309024.
I0304 19:32:37.834125 23118544486528 run.py:483] Algo bellman_ford step 9657 current loss 0.055634, current_train_items 309056.
I0304 19:32:37.867555 23118544486528 run.py:483] Algo bellman_ford step 9658 current loss 0.072239, current_train_items 309088.
I0304 19:32:37.900804 23118544486528 run.py:483] Algo bellman_ford step 9659 current loss 0.041662, current_train_items 309120.
I0304 19:32:37.920651 23118544486528 run.py:483] Algo bellman_ford step 9660 current loss 0.004641, current_train_items 309152.
I0304 19:32:37.937452 23118544486528 run.py:483] Algo bellman_ford step 9661 current loss 0.009854, current_train_items 309184.
I0304 19:32:37.962179 23118544486528 run.py:483] Algo bellman_ford step 9662 current loss 0.089708, current_train_items 309216.
I0304 19:32:37.994663 23118544486528 run.py:483] Algo bellman_ford step 9663 current loss 0.039735, current_train_items 309248.
I0304 19:32:38.029120 23118544486528 run.py:483] Algo bellman_ford step 9664 current loss 0.080123, current_train_items 309280.
I0304 19:32:38.048986 23118544486528 run.py:483] Algo bellman_ford step 9665 current loss 0.004524, current_train_items 309312.
I0304 19:32:38.065469 23118544486528 run.py:483] Algo bellman_ford step 9666 current loss 0.012947, current_train_items 309344.
I0304 19:32:38.089484 23118544486528 run.py:483] Algo bellman_ford step 9667 current loss 0.034974, current_train_items 309376.
I0304 19:32:38.121618 23118544486528 run.py:483] Algo bellman_ford step 9668 current loss 0.071106, current_train_items 309408.
I0304 19:32:38.155842 23118544486528 run.py:483] Algo bellman_ford step 9669 current loss 0.066103, current_train_items 309440.
I0304 19:32:38.176466 23118544486528 run.py:483] Algo bellman_ford step 9670 current loss 0.006385, current_train_items 309472.
I0304 19:32:38.193167 23118544486528 run.py:483] Algo bellman_ford step 9671 current loss 0.010783, current_train_items 309504.
I0304 19:32:38.217794 23118544486528 run.py:483] Algo bellman_ford step 9672 current loss 0.048048, current_train_items 309536.
I0304 19:32:38.249847 23118544486528 run.py:483] Algo bellman_ford step 9673 current loss 0.052162, current_train_items 309568.
I0304 19:32:38.283889 23118544486528 run.py:483] Algo bellman_ford step 9674 current loss 0.069213, current_train_items 309600.
I0304 19:32:38.303844 23118544486528 run.py:483] Algo bellman_ford step 9675 current loss 0.003907, current_train_items 309632.
I0304 19:32:38.320582 23118544486528 run.py:483] Algo bellman_ford step 9676 current loss 0.010368, current_train_items 309664.
I0304 19:32:38.345255 23118544486528 run.py:483] Algo bellman_ford step 9677 current loss 0.060596, current_train_items 309696.
I0304 19:32:38.376388 23118544486528 run.py:483] Algo bellman_ford step 9678 current loss 0.046600, current_train_items 309728.
I0304 19:32:38.411331 23118544486528 run.py:483] Algo bellman_ford step 9679 current loss 0.043385, current_train_items 309760.
I0304 19:32:38.431351 23118544486528 run.py:483] Algo bellman_ford step 9680 current loss 0.003781, current_train_items 309792.
I0304 19:32:38.448338 23118544486528 run.py:483] Algo bellman_ford step 9681 current loss 0.014495, current_train_items 309824.
I0304 19:32:38.472781 23118544486528 run.py:483] Algo bellman_ford step 9682 current loss 0.012506, current_train_items 309856.
I0304 19:32:38.504458 23118544486528 run.py:483] Algo bellman_ford step 9683 current loss 0.033275, current_train_items 309888.
I0304 19:32:38.538106 23118544486528 run.py:483] Algo bellman_ford step 9684 current loss 0.072861, current_train_items 309920.
I0304 19:32:38.558362 23118544486528 run.py:483] Algo bellman_ford step 9685 current loss 0.004057, current_train_items 309952.
I0304 19:32:38.574626 23118544486528 run.py:483] Algo bellman_ford step 9686 current loss 0.011020, current_train_items 309984.
I0304 19:32:38.599275 23118544486528 run.py:483] Algo bellman_ford step 9687 current loss 0.050300, current_train_items 310016.
I0304 19:32:38.631375 23118544486528 run.py:483] Algo bellman_ford step 9688 current loss 0.051253, current_train_items 310048.
I0304 19:32:38.665930 23118544486528 run.py:483] Algo bellman_ford step 9689 current loss 0.054799, current_train_items 310080.
I0304 19:32:38.686360 23118544486528 run.py:483] Algo bellman_ford step 9690 current loss 0.010446, current_train_items 310112.
I0304 19:32:38.702337 23118544486528 run.py:483] Algo bellman_ford step 9691 current loss 0.005511, current_train_items 310144.
I0304 19:32:38.726831 23118544486528 run.py:483] Algo bellman_ford step 9692 current loss 0.015326, current_train_items 310176.
I0304 19:32:38.758638 23118544486528 run.py:483] Algo bellman_ford step 9693 current loss 0.098450, current_train_items 310208.
I0304 19:32:38.793069 23118544486528 run.py:483] Algo bellman_ford step 9694 current loss 0.038272, current_train_items 310240.
I0304 19:32:38.812944 23118544486528 run.py:483] Algo bellman_ford step 9695 current loss 0.002609, current_train_items 310272.
I0304 19:32:38.829561 23118544486528 run.py:483] Algo bellman_ford step 9696 current loss 0.034081, current_train_items 310304.
I0304 19:32:38.853884 23118544486528 run.py:483] Algo bellman_ford step 9697 current loss 0.027669, current_train_items 310336.
I0304 19:32:38.888584 23118544486528 run.py:483] Algo bellman_ford step 9698 current loss 0.090836, current_train_items 310368.
I0304 19:32:38.924454 23118544486528 run.py:483] Algo bellman_ford step 9699 current loss 0.050453, current_train_items 310400.
I0304 19:32:38.944827 23118544486528 run.py:483] Algo bellman_ford step 9700 current loss 0.009078, current_train_items 310432.
I0304 19:32:38.952733 23118544486528 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0304 19:32:38.952840 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0304 19:32:38.970044 23118544486528 run.py:483] Algo bellman_ford step 9701 current loss 0.007740, current_train_items 310464.
I0304 19:32:38.995672 23118544486528 run.py:483] Algo bellman_ford step 9702 current loss 0.023555, current_train_items 310496.
I0304 19:32:39.028825 23118544486528 run.py:483] Algo bellman_ford step 9703 current loss 0.077960, current_train_items 310528.
I0304 19:32:39.062677 23118544486528 run.py:483] Algo bellman_ford step 9704 current loss 0.096385, current_train_items 310560.
I0304 19:32:39.083157 23118544486528 run.py:483] Algo bellman_ford step 9705 current loss 0.003034, current_train_items 310592.
I0304 19:32:39.099269 23118544486528 run.py:483] Algo bellman_ford step 9706 current loss 0.023059, current_train_items 310624.
I0304 19:32:39.125008 23118544486528 run.py:483] Algo bellman_ford step 9707 current loss 0.019272, current_train_items 310656.
I0304 19:32:39.157819 23118544486528 run.py:483] Algo bellman_ford step 9708 current loss 0.053953, current_train_items 310688.
I0304 19:32:39.190134 23118544486528 run.py:483] Algo bellman_ford step 9709 current loss 0.056666, current_train_items 310720.
I0304 19:32:39.209937 23118544486528 run.py:483] Algo bellman_ford step 9710 current loss 0.004261, current_train_items 310752.
I0304 19:32:39.226211 23118544486528 run.py:483] Algo bellman_ford step 9711 current loss 0.012411, current_train_items 310784.
I0304 19:32:39.250840 23118544486528 run.py:483] Algo bellman_ford step 9712 current loss 0.027305, current_train_items 310816.
I0304 19:32:39.283933 23118544486528 run.py:483] Algo bellman_ford step 9713 current loss 0.025511, current_train_items 310848.
I0304 19:32:39.319215 23118544486528 run.py:483] Algo bellman_ford step 9714 current loss 0.025811, current_train_items 310880.
I0304 19:32:39.339316 23118544486528 run.py:483] Algo bellman_ford step 9715 current loss 0.003070, current_train_items 310912.
I0304 19:32:39.356183 23118544486528 run.py:483] Algo bellman_ford step 9716 current loss 0.031378, current_train_items 310944.
I0304 19:32:39.379527 23118544486528 run.py:483] Algo bellman_ford step 9717 current loss 0.025856, current_train_items 310976.
I0304 19:32:39.412347 23118544486528 run.py:483] Algo bellman_ford step 9718 current loss 0.050411, current_train_items 311008.
I0304 19:32:39.444093 23118544486528 run.py:483] Algo bellman_ford step 9719 current loss 0.039773, current_train_items 311040.
I0304 19:32:39.464460 23118544486528 run.py:483] Algo bellman_ford step 9720 current loss 0.003004, current_train_items 311072.
I0304 19:32:39.480431 23118544486528 run.py:483] Algo bellman_ford step 9721 current loss 0.005473, current_train_items 311104.
I0304 19:32:39.504445 23118544486528 run.py:483] Algo bellman_ford step 9722 current loss 0.037896, current_train_items 311136.
I0304 19:32:39.537212 23118544486528 run.py:483] Algo bellman_ford step 9723 current loss 0.051651, current_train_items 311168.
I0304 19:32:39.571400 23118544486528 run.py:483] Algo bellman_ford step 9724 current loss 0.066465, current_train_items 311200.
I0304 19:32:39.591550 23118544486528 run.py:483] Algo bellman_ford step 9725 current loss 0.003651, current_train_items 311232.
I0304 19:32:39.607745 23118544486528 run.py:483] Algo bellman_ford step 9726 current loss 0.012458, current_train_items 311264.
I0304 19:32:39.631339 23118544486528 run.py:483] Algo bellman_ford step 9727 current loss 0.015172, current_train_items 311296.
I0304 19:32:39.664103 23118544486528 run.py:483] Algo bellman_ford step 9728 current loss 0.048104, current_train_items 311328.
I0304 19:32:39.698821 23118544486528 run.py:483] Algo bellman_ford step 9729 current loss 0.040685, current_train_items 311360.
I0304 19:32:39.718615 23118544486528 run.py:483] Algo bellman_ford step 9730 current loss 0.008343, current_train_items 311392.
I0304 19:32:39.735107 23118544486528 run.py:483] Algo bellman_ford step 9731 current loss 0.010810, current_train_items 311424.
I0304 19:32:39.759172 23118544486528 run.py:483] Algo bellman_ford step 9732 current loss 0.021695, current_train_items 311456.
I0304 19:32:39.791629 23118544486528 run.py:483] Algo bellman_ford step 9733 current loss 0.060697, current_train_items 311488.
I0304 19:32:39.824960 23118544486528 run.py:483] Algo bellman_ford step 9734 current loss 0.054499, current_train_items 311520.
I0304 19:32:39.844955 23118544486528 run.py:483] Algo bellman_ford step 9735 current loss 0.005043, current_train_items 311552.
I0304 19:32:39.861297 23118544486528 run.py:483] Algo bellman_ford step 9736 current loss 0.021543, current_train_items 311584.
I0304 19:32:39.885443 23118544486528 run.py:483] Algo bellman_ford step 9737 current loss 0.025569, current_train_items 311616.
I0304 19:32:39.919318 23118544486528 run.py:483] Algo bellman_ford step 9738 current loss 0.048004, current_train_items 311648.
I0304 19:32:39.954673 23118544486528 run.py:483] Algo bellman_ford step 9739 current loss 0.043721, current_train_items 311680.
I0304 19:32:39.974257 23118544486528 run.py:483] Algo bellman_ford step 9740 current loss 0.001694, current_train_items 311712.
I0304 19:32:39.991237 23118544486528 run.py:483] Algo bellman_ford step 9741 current loss 0.012885, current_train_items 311744.
I0304 19:32:40.017248 23118544486528 run.py:483] Algo bellman_ford step 9742 current loss 0.047370, current_train_items 311776.
I0304 19:32:40.050283 23118544486528 run.py:483] Algo bellman_ford step 9743 current loss 0.029293, current_train_items 311808.
I0304 19:32:40.085227 23118544486528 run.py:483] Algo bellman_ford step 9744 current loss 0.035702, current_train_items 311840.
I0304 19:32:40.104994 23118544486528 run.py:483] Algo bellman_ford step 9745 current loss 0.018113, current_train_items 311872.
I0304 19:32:40.122153 23118544486528 run.py:483] Algo bellman_ford step 9746 current loss 0.008596, current_train_items 311904.
I0304 19:32:40.146371 23118544486528 run.py:483] Algo bellman_ford step 9747 current loss 0.019804, current_train_items 311936.
I0304 19:32:40.176834 23118544486528 run.py:483] Algo bellman_ford step 9748 current loss 0.014163, current_train_items 311968.
I0304 19:32:40.209822 23118544486528 run.py:483] Algo bellman_ford step 9749 current loss 0.061632, current_train_items 312000.
I0304 19:32:40.229693 23118544486528 run.py:483] Algo bellman_ford step 9750 current loss 0.001840, current_train_items 312032.
I0304 19:32:40.237878 23118544486528 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0304 19:32:40.237983 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:40.254824 23118544486528 run.py:483] Algo bellman_ford step 9751 current loss 0.012539, current_train_items 312064.
I0304 19:32:40.281328 23118544486528 run.py:483] Algo bellman_ford step 9752 current loss 0.023984, current_train_items 312096.
I0304 19:32:40.313513 23118544486528 run.py:483] Algo bellman_ford step 9753 current loss 0.034415, current_train_items 312128.
I0304 19:32:40.347251 23118544486528 run.py:483] Algo bellman_ford step 9754 current loss 0.054191, current_train_items 312160.
I0304 19:32:40.367212 23118544486528 run.py:483] Algo bellman_ford step 9755 current loss 0.017254, current_train_items 312192.
I0304 19:32:40.383907 23118544486528 run.py:483] Algo bellman_ford step 9756 current loss 0.018400, current_train_items 312224.
I0304 19:32:40.407535 23118544486528 run.py:483] Algo bellman_ford step 9757 current loss 0.027363, current_train_items 312256.
I0304 19:32:40.440721 23118544486528 run.py:483] Algo bellman_ford step 9758 current loss 0.069844, current_train_items 312288.
I0304 19:32:40.475124 23118544486528 run.py:483] Algo bellman_ford step 9759 current loss 0.085278, current_train_items 312320.
I0304 19:32:40.495438 23118544486528 run.py:483] Algo bellman_ford step 9760 current loss 0.006983, current_train_items 312352.
I0304 19:32:40.512223 23118544486528 run.py:483] Algo bellman_ford step 9761 current loss 0.020455, current_train_items 312384.
I0304 19:32:40.537060 23118544486528 run.py:483] Algo bellman_ford step 9762 current loss 0.021665, current_train_items 312416.
I0304 19:32:40.570241 23118544486528 run.py:483] Algo bellman_ford step 9763 current loss 0.074244, current_train_items 312448.
I0304 19:32:40.602296 23118544486528 run.py:483] Algo bellman_ford step 9764 current loss 0.033024, current_train_items 312480.
I0304 19:32:40.622438 23118544486528 run.py:483] Algo bellman_ford step 9765 current loss 0.014503, current_train_items 312512.
I0304 19:32:40.638478 23118544486528 run.py:483] Algo bellman_ford step 9766 current loss 0.026522, current_train_items 312544.
I0304 19:32:40.663301 23118544486528 run.py:483] Algo bellman_ford step 9767 current loss 0.065630, current_train_items 312576.
I0304 19:32:40.695177 23118544486528 run.py:483] Algo bellman_ford step 9768 current loss 0.051488, current_train_items 312608.
I0304 19:32:40.729912 23118544486528 run.py:483] Algo bellman_ford step 9769 current loss 0.057723, current_train_items 312640.
I0304 19:32:40.750356 23118544486528 run.py:483] Algo bellman_ford step 9770 current loss 0.003166, current_train_items 312672.
I0304 19:32:40.766521 23118544486528 run.py:483] Algo bellman_ford step 9771 current loss 0.014080, current_train_items 312704.
I0304 19:32:40.790035 23118544486528 run.py:483] Algo bellman_ford step 9772 current loss 0.055516, current_train_items 312736.
I0304 19:32:40.823535 23118544486528 run.py:483] Algo bellman_ford step 9773 current loss 0.054379, current_train_items 312768.
I0304 19:32:40.858159 23118544486528 run.py:483] Algo bellman_ford step 9774 current loss 0.061439, current_train_items 312800.
I0304 19:32:40.878365 23118544486528 run.py:483] Algo bellman_ford step 9775 current loss 0.002342, current_train_items 312832.
I0304 19:32:40.894576 23118544486528 run.py:483] Algo bellman_ford step 9776 current loss 0.004718, current_train_items 312864.
I0304 19:32:40.919870 23118544486528 run.py:483] Algo bellman_ford step 9777 current loss 0.025356, current_train_items 312896.
I0304 19:32:40.953046 23118544486528 run.py:483] Algo bellman_ford step 9778 current loss 0.031372, current_train_items 312928.
I0304 19:32:40.987694 23118544486528 run.py:483] Algo bellman_ford step 9779 current loss 0.029580, current_train_items 312960.
I0304 19:32:41.007585 23118544486528 run.py:483] Algo bellman_ford step 9780 current loss 0.004132, current_train_items 312992.
I0304 19:32:41.024183 23118544486528 run.py:483] Algo bellman_ford step 9781 current loss 0.009350, current_train_items 313024.
I0304 19:32:41.048066 23118544486528 run.py:483] Algo bellman_ford step 9782 current loss 0.049753, current_train_items 313056.
I0304 19:32:41.082334 23118544486528 run.py:483] Algo bellman_ford step 9783 current loss 0.070460, current_train_items 313088.
I0304 19:32:41.115082 23118544486528 run.py:483] Algo bellman_ford step 9784 current loss 0.037022, current_train_items 313120.
I0304 19:32:41.135277 23118544486528 run.py:483] Algo bellman_ford step 9785 current loss 0.003569, current_train_items 313152.
I0304 19:32:41.152017 23118544486528 run.py:483] Algo bellman_ford step 9786 current loss 0.022977, current_train_items 313184.
I0304 19:32:41.176198 23118544486528 run.py:483] Algo bellman_ford step 9787 current loss 0.054955, current_train_items 313216.
I0304 19:32:41.207953 23118544486528 run.py:483] Algo bellman_ford step 9788 current loss 0.039432, current_train_items 313248.
I0304 19:32:41.242359 23118544486528 run.py:483] Algo bellman_ford step 9789 current loss 0.055490, current_train_items 313280.
I0304 19:32:41.262578 23118544486528 run.py:483] Algo bellman_ford step 9790 current loss 0.011008, current_train_items 313312.
I0304 19:32:41.278823 23118544486528 run.py:483] Algo bellman_ford step 9791 current loss 0.008190, current_train_items 313344.
I0304 19:32:41.302761 23118544486528 run.py:483] Algo bellman_ford step 9792 current loss 0.063751, current_train_items 313376.
I0304 19:32:41.335214 23118544486528 run.py:483] Algo bellman_ford step 9793 current loss 0.061397, current_train_items 313408.
I0304 19:32:41.367583 23118544486528 run.py:483] Algo bellman_ford step 9794 current loss 0.026254, current_train_items 313440.
I0304 19:32:41.387613 23118544486528 run.py:483] Algo bellman_ford step 9795 current loss 0.006958, current_train_items 313472.
I0304 19:32:41.404309 23118544486528 run.py:483] Algo bellman_ford step 9796 current loss 0.007743, current_train_items 313504.
I0304 19:32:41.428443 23118544486528 run.py:483] Algo bellman_ford step 9797 current loss 0.032061, current_train_items 313536.
I0304 19:32:41.459995 23118544486528 run.py:483] Algo bellman_ford step 9798 current loss 0.040012, current_train_items 313568.
I0304 19:32:41.493628 23118544486528 run.py:483] Algo bellman_ford step 9799 current loss 0.061908, current_train_items 313600.
I0304 19:32:41.513667 23118544486528 run.py:483] Algo bellman_ford step 9800 current loss 0.024359, current_train_items 313632.
I0304 19:32:41.521517 23118544486528 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0304 19:32:41.521624 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0304 19:32:41.538309 23118544486528 run.py:483] Algo bellman_ford step 9801 current loss 0.022475, current_train_items 313664.
I0304 19:32:41.563760 23118544486528 run.py:483] Algo bellman_ford step 9802 current loss 0.050865, current_train_items 313696.
I0304 19:32:41.597051 23118544486528 run.py:483] Algo bellman_ford step 9803 current loss 0.071081, current_train_items 313728.
I0304 19:32:41.630481 23118544486528 run.py:483] Algo bellman_ford step 9804 current loss 0.055245, current_train_items 313760.
I0304 19:32:41.650725 23118544486528 run.py:483] Algo bellman_ford step 9805 current loss 0.006598, current_train_items 313792.
I0304 19:32:41.667007 23118544486528 run.py:483] Algo bellman_ford step 9806 current loss 0.012281, current_train_items 313824.
I0304 19:32:41.690733 23118544486528 run.py:483] Algo bellman_ford step 9807 current loss 0.019520, current_train_items 313856.
I0304 19:32:41.723156 23118544486528 run.py:483] Algo bellman_ford step 9808 current loss 0.037275, current_train_items 313888.
I0304 19:32:41.759259 23118544486528 run.py:483] Algo bellman_ford step 9809 current loss 0.054398, current_train_items 313920.
I0304 19:32:41.778969 23118544486528 run.py:483] Algo bellman_ford step 9810 current loss 0.005512, current_train_items 313952.
I0304 19:32:41.794931 23118544486528 run.py:483] Algo bellman_ford step 9811 current loss 0.025420, current_train_items 313984.
I0304 19:32:41.820290 23118544486528 run.py:483] Algo bellman_ford step 9812 current loss 0.091951, current_train_items 314016.
I0304 19:32:41.850456 23118544486528 run.py:483] Algo bellman_ford step 9813 current loss 0.041985, current_train_items 314048.
I0304 19:32:41.885111 23118544486528 run.py:483] Algo bellman_ford step 9814 current loss 0.072808, current_train_items 314080.
I0304 19:32:41.904877 23118544486528 run.py:483] Algo bellman_ford step 9815 current loss 0.003868, current_train_items 314112.
I0304 19:32:41.921547 23118544486528 run.py:483] Algo bellman_ford step 9816 current loss 0.026019, current_train_items 314144.
I0304 19:32:41.945479 23118544486528 run.py:483] Algo bellman_ford step 9817 current loss 0.026288, current_train_items 314176.
I0304 19:32:41.978855 23118544486528 run.py:483] Algo bellman_ford step 9818 current loss 0.079733, current_train_items 314208.
I0304 19:32:42.012431 23118544486528 run.py:483] Algo bellman_ford step 9819 current loss 0.063629, current_train_items 314240.
I0304 19:32:42.032275 23118544486528 run.py:483] Algo bellman_ford step 9820 current loss 0.002512, current_train_items 314272.
I0304 19:32:42.048500 23118544486528 run.py:483] Algo bellman_ford step 9821 current loss 0.045315, current_train_items 314304.
I0304 19:32:42.073378 23118544486528 run.py:483] Algo bellman_ford step 9822 current loss 0.081289, current_train_items 314336.
I0304 19:32:42.105578 23118544486528 run.py:483] Algo bellman_ford step 9823 current loss 0.038150, current_train_items 314368.
I0304 19:32:42.139330 23118544486528 run.py:483] Algo bellman_ford step 9824 current loss 0.075927, current_train_items 314400.
I0304 19:32:42.159390 23118544486528 run.py:483] Algo bellman_ford step 9825 current loss 0.003446, current_train_items 314432.
I0304 19:32:42.175544 23118544486528 run.py:483] Algo bellman_ford step 9826 current loss 0.016206, current_train_items 314464.
I0304 19:32:42.199898 23118544486528 run.py:483] Algo bellman_ford step 9827 current loss 0.035176, current_train_items 314496.
I0304 19:32:42.231220 23118544486528 run.py:483] Algo bellman_ford step 9828 current loss 0.053056, current_train_items 314528.
I0304 19:32:42.266333 23118544486528 run.py:483] Algo bellman_ford step 9829 current loss 0.066081, current_train_items 314560.
I0304 19:32:42.285842 23118544486528 run.py:483] Algo bellman_ford step 9830 current loss 0.001790, current_train_items 314592.
I0304 19:32:42.302711 23118544486528 run.py:483] Algo bellman_ford step 9831 current loss 0.012651, current_train_items 314624.
I0304 19:32:42.327414 23118544486528 run.py:483] Algo bellman_ford step 9832 current loss 0.036996, current_train_items 314656.
I0304 19:32:42.359941 23118544486528 run.py:483] Algo bellman_ford step 9833 current loss 0.061653, current_train_items 314688.
I0304 19:32:42.392470 23118544486528 run.py:483] Algo bellman_ford step 9834 current loss 0.050623, current_train_items 314720.
I0304 19:32:42.412116 23118544486528 run.py:483] Algo bellman_ford step 9835 current loss 0.030788, current_train_items 314752.
I0304 19:32:42.428334 23118544486528 run.py:483] Algo bellman_ford step 9836 current loss 0.014571, current_train_items 314784.
I0304 19:32:42.453019 23118544486528 run.py:483] Algo bellman_ford step 9837 current loss 0.029236, current_train_items 314816.
I0304 19:32:42.486554 23118544486528 run.py:483] Algo bellman_ford step 9838 current loss 0.056978, current_train_items 314848.
I0304 19:32:42.522163 23118544486528 run.py:483] Algo bellman_ford step 9839 current loss 0.045618, current_train_items 314880.
I0304 19:32:42.542029 23118544486528 run.py:483] Algo bellman_ford step 9840 current loss 0.008838, current_train_items 314912.
I0304 19:32:42.558394 23118544486528 run.py:483] Algo bellman_ford step 9841 current loss 0.011934, current_train_items 314944.
I0304 19:32:42.583425 23118544486528 run.py:483] Algo bellman_ford step 9842 current loss 0.053776, current_train_items 314976.
I0304 19:32:42.615631 23118544486528 run.py:483] Algo bellman_ford step 9843 current loss 0.029671, current_train_items 315008.
I0304 19:32:42.647736 23118544486528 run.py:483] Algo bellman_ford step 9844 current loss 0.048807, current_train_items 315040.
I0304 19:32:42.667625 23118544486528 run.py:483] Algo bellman_ford step 9845 current loss 0.002996, current_train_items 315072.
I0304 19:32:42.684386 23118544486528 run.py:483] Algo bellman_ford step 9846 current loss 0.007756, current_train_items 315104.
I0304 19:32:42.709408 23118544486528 run.py:483] Algo bellman_ford step 9847 current loss 0.030903, current_train_items 315136.
I0304 19:32:42.741362 23118544486528 run.py:483] Algo bellman_ford step 9848 current loss 0.089851, current_train_items 315168.
I0304 19:32:42.775756 23118544486528 run.py:483] Algo bellman_ford step 9849 current loss 0.078017, current_train_items 315200.
I0304 19:32:42.795854 23118544486528 run.py:483] Algo bellman_ford step 9850 current loss 0.006539, current_train_items 315232.
I0304 19:32:42.804008 23118544486528 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0304 19:32:42.804113 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0304 19:32:42.821738 23118544486528 run.py:483] Algo bellman_ford step 9851 current loss 0.027331, current_train_items 315264.
I0304 19:32:42.848298 23118544486528 run.py:483] Algo bellman_ford step 9852 current loss 0.031110, current_train_items 315296.
I0304 19:32:42.880723 23118544486528 run.py:483] Algo bellman_ford step 9853 current loss 0.047740, current_train_items 315328.
I0304 19:32:42.915365 23118544486528 run.py:483] Algo bellman_ford step 9854 current loss 0.036508, current_train_items 315360.
I0304 19:32:42.934879 23118544486528 run.py:483] Algo bellman_ford step 9855 current loss 0.004951, current_train_items 315392.
I0304 19:32:42.951029 23118544486528 run.py:483] Algo bellman_ford step 9856 current loss 0.024550, current_train_items 315424.
I0304 19:32:42.976225 23118544486528 run.py:483] Algo bellman_ford step 9857 current loss 0.029743, current_train_items 315456.
I0304 19:32:43.008158 23118544486528 run.py:483] Algo bellman_ford step 9858 current loss 0.038086, current_train_items 315488.
I0304 19:32:43.042135 23118544486528 run.py:483] Algo bellman_ford step 9859 current loss 0.075722, current_train_items 315520.
I0304 19:32:43.063117 23118544486528 run.py:483] Algo bellman_ford step 9860 current loss 0.004006, current_train_items 315552.
I0304 19:32:43.079513 23118544486528 run.py:483] Algo bellman_ford step 9861 current loss 0.017635, current_train_items 315584.
I0304 19:32:43.104112 23118544486528 run.py:483] Algo bellman_ford step 9862 current loss 0.029639, current_train_items 315616.
I0304 19:32:43.136789 23118544486528 run.py:483] Algo bellman_ford step 9863 current loss 0.080603, current_train_items 315648.
I0304 19:32:43.170737 23118544486528 run.py:483] Algo bellman_ford step 9864 current loss 0.047146, current_train_items 315680.
I0304 19:32:43.190836 23118544486528 run.py:483] Algo bellman_ford step 9865 current loss 0.003482, current_train_items 315712.
I0304 19:32:43.207051 23118544486528 run.py:483] Algo bellman_ford step 9866 current loss 0.011062, current_train_items 315744.
I0304 19:32:43.231437 23118544486528 run.py:483] Algo bellman_ford step 9867 current loss 0.068030, current_train_items 315776.
I0304 19:32:43.264504 23118544486528 run.py:483] Algo bellman_ford step 9868 current loss 0.085380, current_train_items 315808.
I0304 19:32:43.299227 23118544486528 run.py:483] Algo bellman_ford step 9869 current loss 0.050870, current_train_items 315840.
I0304 19:32:43.319477 23118544486528 run.py:483] Algo bellman_ford step 9870 current loss 0.005375, current_train_items 315872.
I0304 19:32:43.336434 23118544486528 run.py:483] Algo bellman_ford step 9871 current loss 0.035841, current_train_items 315904.
I0304 19:32:43.361127 23118544486528 run.py:483] Algo bellman_ford step 9872 current loss 0.065897, current_train_items 315936.
I0304 19:32:43.393558 23118544486528 run.py:483] Algo bellman_ford step 9873 current loss 0.037035, current_train_items 315968.
I0304 19:32:43.426240 23118544486528 run.py:483] Algo bellman_ford step 9874 current loss 0.067735, current_train_items 316000.
I0304 19:32:43.446304 23118544486528 run.py:483] Algo bellman_ford step 9875 current loss 0.002106, current_train_items 316032.
I0304 19:32:43.462704 23118544486528 run.py:483] Algo bellman_ford step 9876 current loss 0.013820, current_train_items 316064.
I0304 19:32:43.485977 23118544486528 run.py:483] Algo bellman_ford step 9877 current loss 0.041596, current_train_items 316096.
I0304 19:32:43.518347 23118544486528 run.py:483] Algo bellman_ford step 9878 current loss 0.033915, current_train_items 316128.
I0304 19:32:43.552058 23118544486528 run.py:483] Algo bellman_ford step 9879 current loss 0.064364, current_train_items 316160.
I0304 19:32:43.571972 23118544486528 run.py:483] Algo bellman_ford step 9880 current loss 0.004622, current_train_items 316192.
I0304 19:32:43.588398 23118544486528 run.py:483] Algo bellman_ford step 9881 current loss 0.046263, current_train_items 316224.
I0304 19:32:43.613363 23118544486528 run.py:483] Algo bellman_ford step 9882 current loss 0.010684, current_train_items 316256.
I0304 19:32:43.645594 23118544486528 run.py:483] Algo bellman_ford step 9883 current loss 0.034122, current_train_items 316288.
I0304 19:32:43.679749 23118544486528 run.py:483] Algo bellman_ford step 9884 current loss 0.056717, current_train_items 316320.
I0304 19:32:43.699977 23118544486528 run.py:483] Algo bellman_ford step 9885 current loss 0.003689, current_train_items 316352.
I0304 19:32:43.716406 23118544486528 run.py:483] Algo bellman_ford step 9886 current loss 0.008387, current_train_items 316384.
I0304 19:32:43.740815 23118544486528 run.py:483] Algo bellman_ford step 9887 current loss 0.021229, current_train_items 316416.
I0304 19:32:43.772751 23118544486528 run.py:483] Algo bellman_ford step 9888 current loss 0.029860, current_train_items 316448.
I0304 19:32:43.806171 23118544486528 run.py:483] Algo bellman_ford step 9889 current loss 0.053797, current_train_items 316480.
I0304 19:32:43.826199 23118544486528 run.py:483] Algo bellman_ford step 9890 current loss 0.007672, current_train_items 316512.
I0304 19:32:43.842573 23118544486528 run.py:483] Algo bellman_ford step 9891 current loss 0.018370, current_train_items 316544.
I0304 19:32:43.866039 23118544486528 run.py:483] Algo bellman_ford step 9892 current loss 0.036561, current_train_items 316576.
I0304 19:32:43.899092 23118544486528 run.py:483] Algo bellman_ford step 9893 current loss 0.037502, current_train_items 316608.
I0304 19:32:43.934383 23118544486528 run.py:483] Algo bellman_ford step 9894 current loss 0.068771, current_train_items 316640.
I0304 19:32:43.954314 23118544486528 run.py:483] Algo bellman_ford step 9895 current loss 0.005203, current_train_items 316672.
I0304 19:32:43.970556 23118544486528 run.py:483] Algo bellman_ford step 9896 current loss 0.031112, current_train_items 316704.
I0304 19:32:43.995761 23118544486528 run.py:483] Algo bellman_ford step 9897 current loss 0.053151, current_train_items 316736.
I0304 19:32:44.028550 23118544486528 run.py:483] Algo bellman_ford step 9898 current loss 0.072698, current_train_items 316768.
I0304 19:32:44.064399 23118544486528 run.py:483] Algo bellman_ford step 9899 current loss 0.038405, current_train_items 316800.
I0304 19:32:44.084614 23118544486528 run.py:483] Algo bellman_ford step 9900 current loss 0.004508, current_train_items 316832.
I0304 19:32:44.092319 23118544486528 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0304 19:32:44.092427 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0304 19:32:44.110090 23118544486528 run.py:483] Algo bellman_ford step 9901 current loss 0.014050, current_train_items 316864.
I0304 19:32:44.134551 23118544486528 run.py:483] Algo bellman_ford step 9902 current loss 0.048031, current_train_items 316896.
I0304 19:32:44.166424 23118544486528 run.py:483] Algo bellman_ford step 9903 current loss 0.037520, current_train_items 316928.
I0304 19:32:44.201442 23118544486528 run.py:483] Algo bellman_ford step 9904 current loss 0.037711, current_train_items 316960.
I0304 19:32:44.221615 23118544486528 run.py:483] Algo bellman_ford step 9905 current loss 0.004836, current_train_items 316992.
I0304 19:32:44.238047 23118544486528 run.py:483] Algo bellman_ford step 9906 current loss 0.034281, current_train_items 317024.
I0304 19:32:44.263057 23118544486528 run.py:483] Algo bellman_ford step 9907 current loss 0.080415, current_train_items 317056.
I0304 19:32:44.295291 23118544486528 run.py:483] Algo bellman_ford step 9908 current loss 0.039472, current_train_items 317088.
I0304 19:32:44.327599 23118544486528 run.py:483] Algo bellman_ford step 9909 current loss 0.076227, current_train_items 317120.
I0304 19:32:44.347596 23118544486528 run.py:483] Algo bellman_ford step 9910 current loss 0.009375, current_train_items 317152.
I0304 19:32:44.364447 23118544486528 run.py:483] Algo bellman_ford step 9911 current loss 0.012105, current_train_items 317184.
I0304 19:32:44.388940 23118544486528 run.py:483] Algo bellman_ford step 9912 current loss 0.046885, current_train_items 317216.
I0304 19:32:44.421649 23118544486528 run.py:483] Algo bellman_ford step 9913 current loss 0.069745, current_train_items 317248.
I0304 19:32:44.455343 23118544486528 run.py:483] Algo bellman_ford step 9914 current loss 0.076690, current_train_items 317280.
I0304 19:32:44.475215 23118544486528 run.py:483] Algo bellman_ford step 9915 current loss 0.025990, current_train_items 317312.
I0304 19:32:44.491925 23118544486528 run.py:483] Algo bellman_ford step 9916 current loss 0.007147, current_train_items 317344.
I0304 19:32:44.516774 23118544486528 run.py:483] Algo bellman_ford step 9917 current loss 0.047605, current_train_items 317376.
I0304 19:32:44.549289 23118544486528 run.py:483] Algo bellman_ford step 9918 current loss 0.065927, current_train_items 317408.
I0304 19:32:44.583109 23118544486528 run.py:483] Algo bellman_ford step 9919 current loss 0.070222, current_train_items 317440.
I0304 19:32:44.602767 23118544486528 run.py:483] Algo bellman_ford step 9920 current loss 0.003560, current_train_items 317472.
I0304 19:32:44.619442 23118544486528 run.py:483] Algo bellman_ford step 9921 current loss 0.017518, current_train_items 317504.
I0304 19:32:44.643758 23118544486528 run.py:483] Algo bellman_ford step 9922 current loss 0.047774, current_train_items 317536.
I0304 19:32:44.674176 23118544486528 run.py:483] Algo bellman_ford step 9923 current loss 0.040098, current_train_items 317568.
I0304 19:32:44.711124 23118544486528 run.py:483] Algo bellman_ford step 9924 current loss 0.104765, current_train_items 317600.
I0304 19:32:44.731004 23118544486528 run.py:483] Algo bellman_ford step 9925 current loss 0.015547, current_train_items 317632.
I0304 19:32:44.747226 23118544486528 run.py:483] Algo bellman_ford step 9926 current loss 0.022607, current_train_items 317664.
I0304 19:32:44.772210 23118544486528 run.py:483] Algo bellman_ford step 9927 current loss 0.057961, current_train_items 317696.
I0304 19:32:44.805932 23118544486528 run.py:483] Algo bellman_ford step 9928 current loss 0.035263, current_train_items 317728.
I0304 19:32:44.837475 23118544486528 run.py:483] Algo bellman_ford step 9929 current loss 0.094168, current_train_items 317760.
I0304 19:32:44.857027 23118544486528 run.py:483] Algo bellman_ford step 9930 current loss 0.003577, current_train_items 317792.
I0304 19:32:44.872678 23118544486528 run.py:483] Algo bellman_ford step 9931 current loss 0.010194, current_train_items 317824.
I0304 19:32:44.897446 23118544486528 run.py:483] Algo bellman_ford step 9932 current loss 0.036816, current_train_items 317856.
I0304 19:32:44.929504 23118544486528 run.py:483] Algo bellman_ford step 9933 current loss 0.051891, current_train_items 317888.
I0304 19:32:44.962159 23118544486528 run.py:483] Algo bellman_ford step 9934 current loss 0.070559, current_train_items 317920.
I0304 19:32:44.981581 23118544486528 run.py:483] Algo bellman_ford step 9935 current loss 0.004647, current_train_items 317952.
I0304 19:32:44.997558 23118544486528 run.py:483] Algo bellman_ford step 9936 current loss 0.008303, current_train_items 317984.
I0304 19:32:45.021697 23118544486528 run.py:483] Algo bellman_ford step 9937 current loss 0.043231, current_train_items 318016.
I0304 19:32:45.053482 23118544486528 run.py:483] Algo bellman_ford step 9938 current loss 0.027601, current_train_items 318048.
I0304 19:32:45.088096 23118544486528 run.py:483] Algo bellman_ford step 9939 current loss 0.101063, current_train_items 318080.
I0304 19:32:45.107898 23118544486528 run.py:483] Algo bellman_ford step 9940 current loss 0.007114, current_train_items 318112.
I0304 19:32:45.124280 23118544486528 run.py:483] Algo bellman_ford step 9941 current loss 0.011444, current_train_items 318144.
I0304 19:32:45.148331 23118544486528 run.py:483] Algo bellman_ford step 9942 current loss 0.046414, current_train_items 318176.
I0304 19:32:45.180642 23118544486528 run.py:483] Algo bellman_ford step 9943 current loss 0.037713, current_train_items 318208.
I0304 19:32:45.212914 23118544486528 run.py:483] Algo bellman_ford step 9944 current loss 0.064027, current_train_items 318240.
I0304 19:32:45.232206 23118544486528 run.py:483] Algo bellman_ford step 9945 current loss 0.005700, current_train_items 318272.
I0304 19:32:45.248239 23118544486528 run.py:483] Algo bellman_ford step 9946 current loss 0.041926, current_train_items 318304.
I0304 19:32:45.272785 23118544486528 run.py:483] Algo bellman_ford step 9947 current loss 0.023648, current_train_items 318336.
I0304 19:32:45.304884 23118544486528 run.py:483] Algo bellman_ford step 9948 current loss 0.029547, current_train_items 318368.
I0304 19:32:45.340290 23118544486528 run.py:483] Algo bellman_ford step 9949 current loss 0.043859, current_train_items 318400.
I0304 19:32:45.359887 23118544486528 run.py:483] Algo bellman_ford step 9950 current loss 0.003210, current_train_items 318432.
I0304 19:32:45.368320 23118544486528 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0304 19:32:45.368426 23118544486528 run.py:522] Not saving new best model, best avg val score was 0.999, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0304 19:32:45.385642 23118544486528 run.py:483] Algo bellman_ford step 9951 current loss 0.019737, current_train_items 318464.
I0304 19:32:45.410477 23118544486528 run.py:483] Algo bellman_ford step 9952 current loss 0.017712, current_train_items 318496.
I0304 19:32:45.444076 23118544486528 run.py:483] Algo bellman_ford step 9953 current loss 0.062766, current_train_items 318528.
I0304 19:32:45.479743 23118544486528 run.py:483] Algo bellman_ford step 9954 current loss 0.053107, current_train_items 318560.
I0304 19:32:45.499474 23118544486528 run.py:483] Algo bellman_ford step 9955 current loss 0.006035, current_train_items 318592.
I0304 19:32:45.516003 23118544486528 run.py:483] Algo bellman_ford step 9956 current loss 0.026779, current_train_items 318624.
I0304 19:32:45.540780 23118544486528 run.py:483] Algo bellman_ford step 9957 current loss 0.011050, current_train_items 318656.
I0304 19:32:45.573385 23118544486528 run.py:483] Algo bellman_ford step 9958 current loss 0.034704, current_train_items 318688.
I0304 19:32:45.608790 23118544486528 run.py:483] Algo bellman_ford step 9959 current loss 0.059308, current_train_items 318720.
I0304 19:32:45.628716 23118544486528 run.py:483] Algo bellman_ford step 9960 current loss 0.003434, current_train_items 318752.
I0304 19:32:45.644571 23118544486528 run.py:483] Algo bellman_ford step 9961 current loss 0.026886, current_train_items 318784.
I0304 19:32:45.669488 23118544486528 run.py:483] Algo bellman_ford step 9962 current loss 0.029099, current_train_items 318816.
I0304 19:32:45.701717 23118544486528 run.py:483] Algo bellman_ford step 9963 current loss 0.046270, current_train_items 318848.
I0304 19:32:45.735475 23118544486528 run.py:483] Algo bellman_ford step 9964 current loss 0.061010, current_train_items 318880.
I0304 19:32:45.754930 23118544486528 run.py:483] Algo bellman_ford step 9965 current loss 0.003143, current_train_items 318912.
I0304 19:32:45.771617 23118544486528 run.py:483] Algo bellman_ford step 9966 current loss 0.009944, current_train_items 318944.
I0304 19:32:45.796113 23118544486528 run.py:483] Algo bellman_ford step 9967 current loss 0.034273, current_train_items 318976.
I0304 19:32:45.828986 23118544486528 run.py:483] Algo bellman_ford step 9968 current loss 0.080305, current_train_items 319008.
I0304 19:32:45.863308 23118544486528 run.py:483] Algo bellman_ford step 9969 current loss 0.070268, current_train_items 319040.
I0304 19:32:45.883345 23118544486528 run.py:483] Algo bellman_ford step 9970 current loss 0.003206, current_train_items 319072.
I0304 19:32:45.899568 23118544486528 run.py:483] Algo bellman_ford step 9971 current loss 0.021551, current_train_items 319104.
I0304 19:32:45.923418 23118544486528 run.py:483] Algo bellman_ford step 9972 current loss 0.022802, current_train_items 319136.
I0304 19:32:45.955173 23118544486528 run.py:483] Algo bellman_ford step 9973 current loss 0.048129, current_train_items 319168.
I0304 19:32:45.989068 23118544486528 run.py:483] Algo bellman_ford step 9974 current loss 0.045538, current_train_items 319200.
I0304 19:32:46.009161 23118544486528 run.py:483] Algo bellman_ford step 9975 current loss 0.003183, current_train_items 319232.
I0304 19:32:46.025818 23118544486528 run.py:483] Algo bellman_ford step 9976 current loss 0.016409, current_train_items 319264.
I0304 19:32:46.050033 23118544486528 run.py:483] Algo bellman_ford step 9977 current loss 0.070021, current_train_items 319296.
I0304 19:32:46.082031 23118544486528 run.py:483] Algo bellman_ford step 9978 current loss 0.015850, current_train_items 319328.
I0304 19:32:46.114815 23118544486528 run.py:483] Algo bellman_ford step 9979 current loss 0.027817, current_train_items 319360.
I0304 19:32:46.134647 23118544486528 run.py:483] Algo bellman_ford step 9980 current loss 0.004727, current_train_items 319392.
I0304 19:32:46.151080 23118544486528 run.py:483] Algo bellman_ford step 9981 current loss 0.013500, current_train_items 319424.
I0304 19:32:46.176158 23118544486528 run.py:483] Algo bellman_ford step 9982 current loss 0.036129, current_train_items 319456.
I0304 19:32:46.209822 23118544486528 run.py:483] Algo bellman_ford step 9983 current loss 0.040344, current_train_items 319488.
I0304 19:32:46.244359 23118544486528 run.py:483] Algo bellman_ford step 9984 current loss 0.058160, current_train_items 319520.
I0304 19:32:46.264222 23118544486528 run.py:483] Algo bellman_ford step 9985 current loss 0.006278, current_train_items 319552.
I0304 19:32:46.280253 23118544486528 run.py:483] Algo bellman_ford step 9986 current loss 0.008018, current_train_items 319584.
I0304 19:32:46.304374 23118544486528 run.py:483] Algo bellman_ford step 9987 current loss 0.025246, current_train_items 319616.
I0304 19:32:46.336620 23118544486528 run.py:483] Algo bellman_ford step 9988 current loss 0.040720, current_train_items 319648.
I0304 19:32:46.370113 23118544486528 run.py:483] Algo bellman_ford step 9989 current loss 0.041188, current_train_items 319680.
I0304 19:32:46.389978 23118544486528 run.py:483] Algo bellman_ford step 9990 current loss 0.045338, current_train_items 319712.
I0304 19:32:46.406414 23118544486528 run.py:483] Algo bellman_ford step 9991 current loss 0.015966, current_train_items 319744.
I0304 19:32:46.430598 23118544486528 run.py:483] Algo bellman_ford step 9992 current loss 0.081314, current_train_items 319776.
I0304 19:32:46.461858 23118544486528 run.py:483] Algo bellman_ford step 9993 current loss 0.033803, current_train_items 319808.
I0304 19:32:46.495133 23118544486528 run.py:483] Algo bellman_ford step 9994 current loss 0.061140, current_train_items 319840.
I0304 19:32:46.514861 23118544486528 run.py:483] Algo bellman_ford step 9995 current loss 0.004556, current_train_items 319872.
I0304 19:32:46.531467 23118544486528 run.py:483] Algo bellman_ford step 9996 current loss 0.088135, current_train_items 319904.
I0304 19:32:46.555860 23118544486528 run.py:483] Algo bellman_ford step 9997 current loss 0.029182, current_train_items 319936.
I0304 19:32:46.588289 23118544486528 run.py:483] Algo bellman_ford step 9998 current loss 0.042627, current_train_items 319968.
I0304 19:32:46.620097 23118544486528 run.py:483] Algo bellman_ford step 9999 current loss 0.065101, current_train_items 320000.
I0304 19:32:46.626130 23118544486528 run.py:527] Restoring best model from checkpoint...
I0304 19:32:49.231773 23118544486528 run.py:542] (test) algo bellman_ford : {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0304 19:32:49.232023 23118544486528 run.py:544] Done!

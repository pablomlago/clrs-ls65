Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:31.858938: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:31.859242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:31.880434: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:36.422704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:46.844886 22447428132992 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:46.853241 22447428132992 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:47.317049 22447428132992 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:47.317450 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.317712 22447428132992 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.522749 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.522985 22447428132992 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.765336 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.765587 22447428132992 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.100607 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.100840 22447428132992 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.511838 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.512075 22447428132992 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:49.034270 22447428132992 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:49.034565 22447428132992 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:49.072316 22447428132992 run.py:166] Dataset not found in ./datasets_1/122/CLRS30_v1.0.0. Downloading...
I0302 18:57:03.617536 22447428132992 dataset_info.py:482] Load dataset info from ./datasets_1/122/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:03.620179 22447428132992 dataset_info.py:482] Load dataset info from ./datasets_1/122/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:03.621029 22447428132992 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/122/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:57:03.621108 22447428132992 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/122/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:19.332663 22447428132992 run.py:483] Algo bellman_ford step 0 current loss 114.227386, current_train_items 32.
I0302 18:57:22.111137 22447428132992 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.2177734375, 'score': 0.2177734375, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:22.111325 22447428132992 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.218, val scores are: bellman_ford: 0.218
I0302 18:57:31.866444 22447428132992 run.py:483] Algo bellman_ford step 1 current loss 13644.869141, current_train_items 64.
I0302 18:57:42.625781 22447428132992 run.py:483] Algo bellman_ford step 2 current loss 2688580608.000000, current_train_items 96.
I0302 18:57:53.383530 22447428132992 run.py:483] Algo bellman_ford step 3 current loss 144671555584.000000, current_train_items 128.
I0302 18:58:03.228553 22447428132992 run.py:483] Algo bellman_ford step 4 current loss 83344498688.000000, current_train_items 160.
I0302 18:58:03.246894 22447428132992 run.py:483] Algo bellman_ford step 5 current loss 11.772502, current_train_items 192.
I0302 18:58:03.263588 22447428132992 run.py:483] Algo bellman_ford step 6 current loss 639.697998, current_train_items 224.
I0302 18:58:03.286147 22447428132992 run.py:483] Algo bellman_ford step 7 current loss 34381820.000000, current_train_items 256.
I0302 18:58:03.315490 22447428132992 run.py:483] Algo bellman_ford step 8 current loss 1810720384.000000, current_train_items 288.
I0302 18:58:03.346948 22447428132992 run.py:483] Algo bellman_ford step 9 current loss 442570080.000000, current_train_items 320.
I0302 18:58:03.364593 22447428132992 run.py:483] Algo bellman_ford step 10 current loss 4.247809, current_train_items 352.
I0302 18:58:03.381093 22447428132992 run.py:483] Algo bellman_ford step 11 current loss 17.829697, current_train_items 384.
I0302 18:58:03.402888 22447428132992 run.py:483] Algo bellman_ford step 12 current loss 59776.250000, current_train_items 416.
I0302 18:58:03.433433 22447428132992 run.py:483] Algo bellman_ford step 13 current loss 1667191.000000, current_train_items 448.
I0302 18:58:03.461582 22447428132992 run.py:483] Algo bellman_ford step 14 current loss 15726118.000000, current_train_items 480.
I0302 18:58:03.479762 22447428132992 run.py:483] Algo bellman_ford step 15 current loss 2.237138, current_train_items 512.
I0302 18:58:03.495875 22447428132992 run.py:483] Algo bellman_ford step 16 current loss 3.029747, current_train_items 544.
I0302 18:58:03.520489 22447428132992 run.py:483] Algo bellman_ford step 17 current loss 31.126720, current_train_items 576.
I0302 18:58:03.549234 22447428132992 run.py:483] Algo bellman_ford step 18 current loss 295.728119, current_train_items 608.
I0302 18:58:03.581485 22447428132992 run.py:483] Algo bellman_ford step 19 current loss 2310.664551, current_train_items 640.
I0302 18:58:03.598993 22447428132992 run.py:483] Algo bellman_ford step 20 current loss 1.517245, current_train_items 672.
I0302 18:58:03.614678 22447428132992 run.py:483] Algo bellman_ford step 21 current loss 1.935054, current_train_items 704.
I0302 18:58:03.637772 22447428132992 run.py:483] Algo bellman_ford step 22 current loss 2.685718, current_train_items 736.
I0302 18:58:03.666832 22447428132992 run.py:483] Algo bellman_ford step 23 current loss 2.765040, current_train_items 768.
I0302 18:58:03.697609 22447428132992 run.py:483] Algo bellman_ford step 24 current loss 2.869769, current_train_items 800.
I0302 18:58:03.715319 22447428132992 run.py:483] Algo bellman_ford step 25 current loss 1.392118, current_train_items 832.
I0302 18:58:03.731504 22447428132992 run.py:483] Algo bellman_ford step 26 current loss 1.970754, current_train_items 864.
I0302 18:58:03.755137 22447428132992 run.py:483] Algo bellman_ford step 27 current loss 2.384943, current_train_items 896.
I0302 18:58:03.784769 22447428132992 run.py:483] Algo bellman_ford step 28 current loss 2.513274, current_train_items 928.
I0302 18:58:03.816963 22447428132992 run.py:483] Algo bellman_ford step 29 current loss 2.815158, current_train_items 960.
I0302 18:58:03.834933 22447428132992 run.py:483] Algo bellman_ford step 30 current loss 1.025089, current_train_items 992.
I0302 18:58:03.850230 22447428132992 run.py:483] Algo bellman_ford step 31 current loss 1.411292, current_train_items 1024.
I0302 18:58:03.872640 22447428132992 run.py:483] Algo bellman_ford step 32 current loss 2.133781, current_train_items 1056.
I0302 18:58:03.902628 22447428132992 run.py:483] Algo bellman_ford step 33 current loss 2.447178, current_train_items 1088.
I0302 18:58:03.933574 22447428132992 run.py:483] Algo bellman_ford step 34 current loss 2.559466, current_train_items 1120.
I0302 18:58:03.951375 22447428132992 run.py:483] Algo bellman_ford step 35 current loss 1.055079, current_train_items 1152.
I0302 18:58:03.966904 22447428132992 run.py:483] Algo bellman_ford step 36 current loss 1.326815, current_train_items 1184.
I0302 18:58:03.990046 22447428132992 run.py:483] Algo bellman_ford step 37 current loss 2.123147, current_train_items 1216.
I0302 18:58:04.019224 22447428132992 run.py:483] Algo bellman_ford step 38 current loss 2.357395, current_train_items 1248.
W0302 18:58:04.042195 22447428132992 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:58:10.635695 22447428132992 run.py:483] Algo bellman_ford step 39 current loss 2.764582, current_train_items 1280.
I0302 18:58:10.655428 22447428132992 run.py:483] Algo bellman_ford step 40 current loss 1.018267, current_train_items 1312.
I0302 18:58:10.671745 22447428132992 run.py:483] Algo bellman_ford step 41 current loss 1.431631, current_train_items 1344.
I0302 18:58:10.694334 22447428132992 run.py:483] Algo bellman_ford step 42 current loss 1.810457, current_train_items 1376.
I0302 18:58:10.724996 22447428132992 run.py:483] Algo bellman_ford step 43 current loss 2.178474, current_train_items 1408.
I0302 18:58:10.756983 22447428132992 run.py:483] Algo bellman_ford step 44 current loss 2.506865, current_train_items 1440.
I0302 18:58:10.776410 22447428132992 run.py:483] Algo bellman_ford step 45 current loss 0.949246, current_train_items 1472.
I0302 18:58:10.792746 22447428132992 run.py:483] Algo bellman_ford step 46 current loss 1.444959, current_train_items 1504.
I0302 18:58:10.814994 22447428132992 run.py:483] Algo bellman_ford step 47 current loss 1.807438, current_train_items 1536.
I0302 18:58:10.842077 22447428132992 run.py:483] Algo bellman_ford step 48 current loss 1.757660, current_train_items 1568.
I0302 18:58:10.871036 22447428132992 run.py:483] Algo bellman_ford step 49 current loss 2.182269, current_train_items 1600.
I0302 18:58:10.889625 22447428132992 run.py:483] Algo bellman_ford step 50 current loss 0.822127, current_train_items 1632.
I0302 18:58:10.899243 22447428132992 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.6943359375, 'score': 0.6943359375, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:58:10.899356 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.218, current avg val score is 0.694, val scores are: bellman_ford: 0.694
I0302 18:58:10.928520 22447428132992 run.py:483] Algo bellman_ford step 51 current loss 1.282829, current_train_items 1664.
I0302 18:58:10.951478 22447428132992 run.py:483] Algo bellman_ford step 52 current loss 1.885342, current_train_items 1696.
I0302 18:58:10.980044 22447428132992 run.py:483] Algo bellman_ford step 53 current loss 1.888444, current_train_items 1728.
I0302 18:58:11.012200 22447428132992 run.py:483] Algo bellman_ford step 54 current loss 2.392609, current_train_items 1760.
I0302 18:58:11.031558 22447428132992 run.py:483] Algo bellman_ford step 55 current loss 0.892049, current_train_items 1792.
I0302 18:58:11.047459 22447428132992 run.py:483] Algo bellman_ford step 56 current loss 1.174252, current_train_items 1824.
I0302 18:58:11.069903 22447428132992 run.py:483] Algo bellman_ford step 57 current loss 1.915428, current_train_items 1856.
I0302 18:58:11.097343 22447428132992 run.py:483] Algo bellman_ford step 58 current loss 1.821830, current_train_items 1888.
I0302 18:58:11.129889 22447428132992 run.py:483] Algo bellman_ford step 59 current loss 2.202279, current_train_items 1920.
I0302 18:58:11.148451 22447428132992 run.py:483] Algo bellman_ford step 60 current loss 0.902448, current_train_items 1952.
W0302 18:58:11.157706 22447428132992 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:58:17.440868 22447428132992 run.py:483] Algo bellman_ford step 61 current loss 1.234620, current_train_items 1984.
I0302 18:58:17.464583 22447428132992 run.py:483] Algo bellman_ford step 62 current loss 1.705629, current_train_items 2016.
I0302 18:58:17.493927 22447428132992 run.py:483] Algo bellman_ford step 63 current loss 2.005464, current_train_items 2048.
I0302 18:58:17.528196 22447428132992 run.py:483] Algo bellman_ford step 64 current loss 2.399264, current_train_items 2080.
I0302 18:58:17.547846 22447428132992 run.py:483] Algo bellman_ford step 65 current loss 0.790076, current_train_items 2112.
I0302 18:58:17.563835 22447428132992 run.py:483] Algo bellman_ford step 66 current loss 1.241926, current_train_items 2144.
I0302 18:58:17.587882 22447428132992 run.py:483] Algo bellman_ford step 67 current loss 1.795457, current_train_items 2176.
I0302 18:58:17.615768 22447428132992 run.py:483] Algo bellman_ford step 68 current loss 1.771268, current_train_items 2208.
I0302 18:58:17.647836 22447428132992 run.py:483] Algo bellman_ford step 69 current loss 2.091905, current_train_items 2240.
I0302 18:58:17.666411 22447428132992 run.py:483] Algo bellman_ford step 70 current loss 0.797369, current_train_items 2272.
I0302 18:58:17.682695 22447428132992 run.py:483] Algo bellman_ford step 71 current loss 1.248383, current_train_items 2304.
I0302 18:58:17.705818 22447428132992 run.py:483] Algo bellman_ford step 72 current loss 1.722504, current_train_items 2336.
I0302 18:58:17.735332 22447428132992 run.py:483] Algo bellman_ford step 73 current loss 1.848164, current_train_items 2368.
I0302 18:58:17.767943 22447428132992 run.py:483] Algo bellman_ford step 74 current loss 2.161798, current_train_items 2400.
I0302 18:58:17.786630 22447428132992 run.py:483] Algo bellman_ford step 75 current loss 0.566115, current_train_items 2432.
I0302 18:58:17.803282 22447428132992 run.py:483] Algo bellman_ford step 76 current loss 1.339965, current_train_items 2464.
I0302 18:58:17.825927 22447428132992 run.py:483] Algo bellman_ford step 77 current loss 1.717826, current_train_items 2496.
I0302 18:58:17.854357 22447428132992 run.py:483] Algo bellman_ford step 78 current loss 1.863937, current_train_items 2528.
I0302 18:58:17.883673 22447428132992 run.py:483] Algo bellman_ford step 79 current loss 1.981536, current_train_items 2560.
I0302 18:58:17.902800 22447428132992 run.py:483] Algo bellman_ford step 80 current loss 0.731793, current_train_items 2592.
I0302 18:58:17.918780 22447428132992 run.py:483] Algo bellman_ford step 81 current loss 1.151975, current_train_items 2624.
I0302 18:58:17.941895 22447428132992 run.py:483] Algo bellman_ford step 82 current loss 1.669755, current_train_items 2656.
I0302 18:58:17.970713 22447428132992 run.py:483] Algo bellman_ford step 83 current loss 1.870658, current_train_items 2688.
I0302 18:58:18.000853 22447428132992 run.py:483] Algo bellman_ford step 84 current loss 2.152020, current_train_items 2720.
I0302 18:58:18.019113 22447428132992 run.py:483] Algo bellman_ford step 85 current loss 0.821755, current_train_items 2752.
I0302 18:58:18.035446 22447428132992 run.py:483] Algo bellman_ford step 86 current loss 1.254327, current_train_items 2784.
I0302 18:58:18.059647 22447428132992 run.py:483] Algo bellman_ford step 87 current loss 1.815043, current_train_items 2816.
I0302 18:58:18.088838 22447428132992 run.py:483] Algo bellman_ford step 88 current loss 1.937463, current_train_items 2848.
I0302 18:58:18.121043 22447428132992 run.py:483] Algo bellman_ford step 89 current loss 2.262852, current_train_items 2880.
I0302 18:58:18.139662 22447428132992 run.py:483] Algo bellman_ford step 90 current loss 0.736102, current_train_items 2912.
I0302 18:58:18.155817 22447428132992 run.py:483] Algo bellman_ford step 91 current loss 1.258186, current_train_items 2944.
I0302 18:58:18.178362 22447428132992 run.py:483] Algo bellman_ford step 92 current loss 1.560202, current_train_items 2976.
I0302 18:58:18.208564 22447428132992 run.py:483] Algo bellman_ford step 93 current loss 1.790254, current_train_items 3008.
I0302 18:58:18.239733 22447428132992 run.py:483] Algo bellman_ford step 94 current loss 1.988996, current_train_items 3040.
I0302 18:58:18.258687 22447428132992 run.py:483] Algo bellman_ford step 95 current loss 0.677527, current_train_items 3072.
I0302 18:58:18.274531 22447428132992 run.py:483] Algo bellman_ford step 96 current loss 1.135563, current_train_items 3104.
I0302 18:58:18.297701 22447428132992 run.py:483] Algo bellman_ford step 97 current loss 1.643440, current_train_items 3136.
I0302 18:58:18.326902 22447428132992 run.py:483] Algo bellman_ford step 98 current loss 1.861089, current_train_items 3168.
I0302 18:58:18.358652 22447428132992 run.py:483] Algo bellman_ford step 99 current loss 2.077683, current_train_items 3200.
I0302 18:58:18.376888 22447428132992 run.py:483] Algo bellman_ford step 100 current loss 0.657278, current_train_items 3232.
I0302 18:58:18.386548 22447428132992 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.7314453125, 'score': 0.7314453125, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:58:18.386660 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.694, current avg val score is 0.731, val scores are: bellman_ford: 0.731
I0302 18:58:18.416621 22447428132992 run.py:483] Algo bellman_ford step 101 current loss 1.397909, current_train_items 3264.
I0302 18:58:18.439539 22447428132992 run.py:483] Algo bellman_ford step 102 current loss 1.507942, current_train_items 3296.
I0302 18:58:18.468056 22447428132992 run.py:483] Algo bellman_ford step 103 current loss 1.683031, current_train_items 3328.
I0302 18:58:18.502632 22447428132992 run.py:483] Algo bellman_ford step 104 current loss 2.314653, current_train_items 3360.
I0302 18:58:18.521429 22447428132992 run.py:483] Algo bellman_ford step 105 current loss 0.630138, current_train_items 3392.
I0302 18:58:18.537634 22447428132992 run.py:483] Algo bellman_ford step 106 current loss 1.104874, current_train_items 3424.
I0302 18:58:18.560912 22447428132992 run.py:483] Algo bellman_ford step 107 current loss 1.448095, current_train_items 3456.
I0302 18:58:18.588999 22447428132992 run.py:483] Algo bellman_ford step 108 current loss 1.714472, current_train_items 3488.
I0302 18:58:18.619129 22447428132992 run.py:483] Algo bellman_ford step 109 current loss 1.686429, current_train_items 3520.
I0302 18:58:18.637613 22447428132992 run.py:483] Algo bellman_ford step 110 current loss 0.769100, current_train_items 3552.
I0302 18:58:18.653539 22447428132992 run.py:483] Algo bellman_ford step 111 current loss 1.195398, current_train_items 3584.
I0302 18:58:18.675605 22447428132992 run.py:483] Algo bellman_ford step 112 current loss 1.493540, current_train_items 3616.
I0302 18:58:18.704694 22447428132992 run.py:483] Algo bellman_ford step 113 current loss 2.021823, current_train_items 3648.
I0302 18:58:18.735126 22447428132992 run.py:483] Algo bellman_ford step 114 current loss 2.088783, current_train_items 3680.
I0302 18:58:18.753341 22447428132992 run.py:483] Algo bellman_ford step 115 current loss 0.718444, current_train_items 3712.
I0302 18:58:18.769776 22447428132992 run.py:483] Algo bellman_ford step 116 current loss 1.394415, current_train_items 3744.
I0302 18:58:18.793663 22447428132992 run.py:483] Algo bellman_ford step 117 current loss 1.783385, current_train_items 3776.
I0302 18:58:18.821892 22447428132992 run.py:483] Algo bellman_ford step 118 current loss 1.859437, current_train_items 3808.
I0302 18:58:18.850909 22447428132992 run.py:483] Algo bellman_ford step 119 current loss 2.137579, current_train_items 3840.
I0302 18:58:18.869498 22447428132992 run.py:483] Algo bellman_ford step 120 current loss 0.687408, current_train_items 3872.
I0302 18:58:18.885925 22447428132992 run.py:483] Algo bellman_ford step 121 current loss 1.141098, current_train_items 3904.
I0302 18:58:18.909319 22447428132992 run.py:483] Algo bellman_ford step 122 current loss 1.501153, current_train_items 3936.
I0302 18:58:18.938851 22447428132992 run.py:483] Algo bellman_ford step 123 current loss 1.852925, current_train_items 3968.
I0302 18:58:18.973596 22447428132992 run.py:483] Algo bellman_ford step 124 current loss 2.408004, current_train_items 4000.
I0302 18:58:18.992043 22447428132992 run.py:483] Algo bellman_ford step 125 current loss 0.828280, current_train_items 4032.
I0302 18:58:19.008405 22447428132992 run.py:483] Algo bellman_ford step 126 current loss 1.183438, current_train_items 4064.
I0302 18:58:19.031327 22447428132992 run.py:483] Algo bellman_ford step 127 current loss 1.663013, current_train_items 4096.
I0302 18:58:19.060072 22447428132992 run.py:483] Algo bellman_ford step 128 current loss 1.794703, current_train_items 4128.
I0302 18:58:19.091770 22447428132992 run.py:483] Algo bellman_ford step 129 current loss 1.993337, current_train_items 4160.
I0302 18:58:19.110061 22447428132992 run.py:483] Algo bellman_ford step 130 current loss 0.696708, current_train_items 4192.
I0302 18:58:19.126019 22447428132992 run.py:483] Algo bellman_ford step 131 current loss 1.041529, current_train_items 4224.
I0302 18:58:19.149495 22447428132992 run.py:483] Algo bellman_ford step 132 current loss 1.637774, current_train_items 4256.
I0302 18:58:19.178082 22447428132992 run.py:483] Algo bellman_ford step 133 current loss 1.805618, current_train_items 4288.
I0302 18:58:19.208919 22447428132992 run.py:483] Algo bellman_ford step 134 current loss 2.204092, current_train_items 4320.
I0302 18:58:19.226920 22447428132992 run.py:483] Algo bellman_ford step 135 current loss 0.664061, current_train_items 4352.
I0302 18:58:19.242902 22447428132992 run.py:483] Algo bellman_ford step 136 current loss 1.239456, current_train_items 4384.
I0302 18:58:19.266353 22447428132992 run.py:483] Algo bellman_ford step 137 current loss 1.630708, current_train_items 4416.
I0302 18:58:19.294882 22447428132992 run.py:483] Algo bellman_ford step 138 current loss 1.958826, current_train_items 4448.
I0302 18:58:19.326963 22447428132992 run.py:483] Algo bellman_ford step 139 current loss 2.056389, current_train_items 4480.
I0302 18:58:19.345153 22447428132992 run.py:483] Algo bellman_ford step 140 current loss 0.584918, current_train_items 4512.
I0302 18:58:19.361526 22447428132992 run.py:483] Algo bellman_ford step 141 current loss 1.200531, current_train_items 4544.
I0302 18:58:19.384307 22447428132992 run.py:483] Algo bellman_ford step 142 current loss 1.628332, current_train_items 4576.
I0302 18:58:19.413736 22447428132992 run.py:483] Algo bellman_ford step 143 current loss 1.840803, current_train_items 4608.
I0302 18:58:19.444581 22447428132992 run.py:483] Algo bellman_ford step 144 current loss 1.935289, current_train_items 4640.
I0302 18:58:19.462782 22447428132992 run.py:483] Algo bellman_ford step 145 current loss 0.754577, current_train_items 4672.
I0302 18:58:19.478870 22447428132992 run.py:483] Algo bellman_ford step 146 current loss 1.161070, current_train_items 4704.
I0302 18:58:19.501108 22447428132992 run.py:483] Algo bellman_ford step 147 current loss 1.568990, current_train_items 4736.
I0302 18:58:19.529697 22447428132992 run.py:483] Algo bellman_ford step 148 current loss 2.017274, current_train_items 4768.
I0302 18:58:19.559453 22447428132992 run.py:483] Algo bellman_ford step 149 current loss 1.762799, current_train_items 4800.
I0302 18:58:19.577674 22447428132992 run.py:483] Algo bellman_ford step 150 current loss 0.737400, current_train_items 4832.
I0302 18:58:19.585816 22447428132992 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.7978515625, 'score': 0.7978515625, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:58:19.585929 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.731, current avg val score is 0.798, val scores are: bellman_ford: 0.798
I0302 18:58:19.615069 22447428132992 run.py:483] Algo bellman_ford step 151 current loss 0.870283, current_train_items 4864.
I0302 18:58:19.638135 22447428132992 run.py:483] Algo bellman_ford step 152 current loss 1.471978, current_train_items 4896.
I0302 18:58:19.666979 22447428132992 run.py:483] Algo bellman_ford step 153 current loss 1.677351, current_train_items 4928.
I0302 18:58:19.702471 22447428132992 run.py:483] Algo bellman_ford step 154 current loss 2.024066, current_train_items 4960.
I0302 18:58:19.721393 22447428132992 run.py:483] Algo bellman_ford step 155 current loss 0.725881, current_train_items 4992.
I0302 18:58:19.737750 22447428132992 run.py:483] Algo bellman_ford step 156 current loss 1.156620, current_train_items 5024.
I0302 18:58:19.760204 22447428132992 run.py:483] Algo bellman_ford step 157 current loss 1.449420, current_train_items 5056.
I0302 18:58:19.788193 22447428132992 run.py:483] Algo bellman_ford step 158 current loss 1.624374, current_train_items 5088.
I0302 18:58:19.819818 22447428132992 run.py:483] Algo bellman_ford step 159 current loss 1.878214, current_train_items 5120.
I0302 18:58:19.838769 22447428132992 run.py:483] Algo bellman_ford step 160 current loss 0.759720, current_train_items 5152.
I0302 18:58:19.855165 22447428132992 run.py:483] Algo bellman_ford step 161 current loss 0.988772, current_train_items 5184.
I0302 18:58:19.876922 22447428132992 run.py:483] Algo bellman_ford step 162 current loss 1.434037, current_train_items 5216.
I0302 18:58:19.905673 22447428132992 run.py:483] Algo bellman_ford step 163 current loss 1.846383, current_train_items 5248.
I0302 18:58:19.936801 22447428132992 run.py:483] Algo bellman_ford step 164 current loss 1.817353, current_train_items 5280.
I0302 18:58:19.955296 22447428132992 run.py:483] Algo bellman_ford step 165 current loss 0.602037, current_train_items 5312.
I0302 18:58:19.971824 22447428132992 run.py:483] Algo bellman_ford step 166 current loss 1.498088, current_train_items 5344.
I0302 18:58:19.994018 22447428132992 run.py:483] Algo bellman_ford step 167 current loss 1.998673, current_train_items 5376.
I0302 18:58:20.023287 22447428132992 run.py:483] Algo bellman_ford step 168 current loss 2.075318, current_train_items 5408.
I0302 18:58:20.054477 22447428132992 run.py:483] Algo bellman_ford step 169 current loss 2.026110, current_train_items 5440.
I0302 18:58:20.072977 22447428132992 run.py:483] Algo bellman_ford step 170 current loss 0.930154, current_train_items 5472.
I0302 18:58:20.089243 22447428132992 run.py:483] Algo bellman_ford step 171 current loss 1.325244, current_train_items 5504.
I0302 18:58:20.111353 22447428132992 run.py:483] Algo bellman_ford step 172 current loss 2.286015, current_train_items 5536.
I0302 18:58:20.140608 22447428132992 run.py:483] Algo bellman_ford step 173 current loss 2.797127, current_train_items 5568.
I0302 18:58:20.175745 22447428132992 run.py:483] Algo bellman_ford step 174 current loss 3.986310, current_train_items 5600.
I0302 18:58:20.193759 22447428132992 run.py:483] Algo bellman_ford step 175 current loss 0.780327, current_train_items 5632.
I0302 18:58:20.209921 22447428132992 run.py:483] Algo bellman_ford step 176 current loss 1.106961, current_train_items 5664.
I0302 18:58:20.233218 22447428132992 run.py:483] Algo bellman_ford step 177 current loss 1.527640, current_train_items 5696.
I0302 18:58:20.260844 22447428132992 run.py:483] Algo bellman_ford step 178 current loss 1.556393, current_train_items 5728.
I0302 18:58:20.290817 22447428132992 run.py:483] Algo bellman_ford step 179 current loss 2.101542, current_train_items 5760.
I0302 18:58:20.309698 22447428132992 run.py:483] Algo bellman_ford step 180 current loss 0.887729, current_train_items 5792.
I0302 18:58:20.325919 22447428132992 run.py:483] Algo bellman_ford step 181 current loss 1.280162, current_train_items 5824.
I0302 18:58:20.349660 22447428132992 run.py:483] Algo bellman_ford step 182 current loss 1.679666, current_train_items 5856.
I0302 18:58:20.378235 22447428132992 run.py:483] Algo bellman_ford step 183 current loss 1.569759, current_train_items 5888.
I0302 18:58:20.407711 22447428132992 run.py:483] Algo bellman_ford step 184 current loss 2.131589, current_train_items 5920.
I0302 18:58:20.426308 22447428132992 run.py:483] Algo bellman_ford step 185 current loss 0.652515, current_train_items 5952.
I0302 18:58:20.442761 22447428132992 run.py:483] Algo bellman_ford step 186 current loss 1.323133, current_train_items 5984.
I0302 18:58:20.464653 22447428132992 run.py:483] Algo bellman_ford step 187 current loss 1.565969, current_train_items 6016.
I0302 18:58:20.493005 22447428132992 run.py:483] Algo bellman_ford step 188 current loss 1.736790, current_train_items 6048.
I0302 18:58:20.525796 22447428132992 run.py:483] Algo bellman_ford step 189 current loss 2.005454, current_train_items 6080.
I0302 18:58:20.544099 22447428132992 run.py:483] Algo bellman_ford step 190 current loss 0.744765, current_train_items 6112.
I0302 18:58:20.560421 22447428132992 run.py:483] Algo bellman_ford step 191 current loss 1.082281, current_train_items 6144.
I0302 18:58:20.583780 22447428132992 run.py:483] Algo bellman_ford step 192 current loss 1.679295, current_train_items 6176.
I0302 18:58:20.612412 22447428132992 run.py:483] Algo bellman_ford step 193 current loss 1.617424, current_train_items 6208.
I0302 18:58:20.643244 22447428132992 run.py:483] Algo bellman_ford step 194 current loss 1.778041, current_train_items 6240.
I0302 18:58:20.661906 22447428132992 run.py:483] Algo bellman_ford step 195 current loss 0.745313, current_train_items 6272.
I0302 18:58:20.678073 22447428132992 run.py:483] Algo bellman_ford step 196 current loss 1.160013, current_train_items 6304.
I0302 18:58:20.700749 22447428132992 run.py:483] Algo bellman_ford step 197 current loss 1.617381, current_train_items 6336.
I0302 18:58:20.730077 22447428132992 run.py:483] Algo bellman_ford step 198 current loss 2.120673, current_train_items 6368.
I0302 18:58:20.762229 22447428132992 run.py:483] Algo bellman_ford step 199 current loss 2.028672, current_train_items 6400.
I0302 18:58:20.780644 22447428132992 run.py:483] Algo bellman_ford step 200 current loss 0.701584, current_train_items 6432.
I0302 18:58:20.788531 22447428132992 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.75390625, 'score': 0.75390625, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:58:20.788639 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.798, current avg val score is 0.754, val scores are: bellman_ford: 0.754
I0302 18:58:20.805243 22447428132992 run.py:483] Algo bellman_ford step 201 current loss 1.162054, current_train_items 6464.
I0302 18:58:20.828833 22447428132992 run.py:483] Algo bellman_ford step 202 current loss 1.426932, current_train_items 6496.
I0302 18:58:20.859853 22447428132992 run.py:483] Algo bellman_ford step 203 current loss 1.924823, current_train_items 6528.
I0302 18:58:20.894654 22447428132992 run.py:483] Algo bellman_ford step 204 current loss 2.028295, current_train_items 6560.
I0302 18:58:20.913552 22447428132992 run.py:483] Algo bellman_ford step 205 current loss 0.718820, current_train_items 6592.
I0302 18:58:20.928912 22447428132992 run.py:483] Algo bellman_ford step 206 current loss 0.965999, current_train_items 6624.
I0302 18:58:20.952415 22447428132992 run.py:483] Algo bellman_ford step 207 current loss 1.631861, current_train_items 6656.
I0302 18:58:20.981874 22447428132992 run.py:483] Algo bellman_ford step 208 current loss 1.815480, current_train_items 6688.
I0302 18:58:21.012510 22447428132992 run.py:483] Algo bellman_ford step 209 current loss 1.956521, current_train_items 6720.
I0302 18:58:21.031287 22447428132992 run.py:483] Algo bellman_ford step 210 current loss 0.877017, current_train_items 6752.
I0302 18:58:21.047589 22447428132992 run.py:483] Algo bellman_ford step 211 current loss 1.186414, current_train_items 6784.
I0302 18:58:21.071181 22447428132992 run.py:483] Algo bellman_ford step 212 current loss 1.872565, current_train_items 6816.
I0302 18:58:21.100816 22447428132992 run.py:483] Algo bellman_ford step 213 current loss 1.755283, current_train_items 6848.
I0302 18:58:21.128778 22447428132992 run.py:483] Algo bellman_ford step 214 current loss 1.816870, current_train_items 6880.
I0302 18:58:21.147467 22447428132992 run.py:483] Algo bellman_ford step 215 current loss 0.636948, current_train_items 6912.
I0302 18:58:21.163388 22447428132992 run.py:483] Algo bellman_ford step 216 current loss 1.144754, current_train_items 6944.
I0302 18:58:21.186193 22447428132992 run.py:483] Algo bellman_ford step 217 current loss 1.683158, current_train_items 6976.
I0302 18:58:21.214797 22447428132992 run.py:483] Algo bellman_ford step 218 current loss 1.704446, current_train_items 7008.
I0302 18:58:21.245426 22447428132992 run.py:483] Algo bellman_ford step 219 current loss 1.687527, current_train_items 7040.
I0302 18:58:21.263880 22447428132992 run.py:483] Algo bellman_ford step 220 current loss 0.734576, current_train_items 7072.
I0302 18:58:21.280113 22447428132992 run.py:483] Algo bellman_ford step 221 current loss 1.093864, current_train_items 7104.
I0302 18:58:21.303764 22447428132992 run.py:483] Algo bellman_ford step 222 current loss 1.595790, current_train_items 7136.
I0302 18:58:21.332231 22447428132992 run.py:483] Algo bellman_ford step 223 current loss 1.601505, current_train_items 7168.
I0302 18:58:21.363900 22447428132992 run.py:483] Algo bellman_ford step 224 current loss 1.971882, current_train_items 7200.
I0302 18:58:21.382318 22447428132992 run.py:483] Algo bellman_ford step 225 current loss 0.679294, current_train_items 7232.
I0302 18:58:21.398618 22447428132992 run.py:483] Algo bellman_ford step 226 current loss 1.127722, current_train_items 7264.
I0302 18:58:21.421650 22447428132992 run.py:483] Algo bellman_ford step 227 current loss 1.591470, current_train_items 7296.
I0302 18:58:21.451053 22447428132992 run.py:483] Algo bellman_ford step 228 current loss 1.725087, current_train_items 7328.
I0302 18:58:21.483470 22447428132992 run.py:483] Algo bellman_ford step 229 current loss 1.978312, current_train_items 7360.
I0302 18:58:21.501937 22447428132992 run.py:483] Algo bellman_ford step 230 current loss 0.652026, current_train_items 7392.
I0302 18:58:21.518118 22447428132992 run.py:483] Algo bellman_ford step 231 current loss 1.078704, current_train_items 7424.
I0302 18:58:21.541768 22447428132992 run.py:483] Algo bellman_ford step 232 current loss 1.354326, current_train_items 7456.
I0302 18:58:21.571326 22447428132992 run.py:483] Algo bellman_ford step 233 current loss 1.630588, current_train_items 7488.
I0302 18:58:21.604332 22447428132992 run.py:483] Algo bellman_ford step 234 current loss 1.719381, current_train_items 7520.
I0302 18:58:21.622882 22447428132992 run.py:483] Algo bellman_ford step 235 current loss 0.731703, current_train_items 7552.
I0302 18:58:21.638784 22447428132992 run.py:483] Algo bellman_ford step 236 current loss 1.015113, current_train_items 7584.
I0302 18:58:21.661462 22447428132992 run.py:483] Algo bellman_ford step 237 current loss 1.635649, current_train_items 7616.
I0302 18:58:21.690728 22447428132992 run.py:483] Algo bellman_ford step 238 current loss 1.831937, current_train_items 7648.
I0302 18:58:21.722325 22447428132992 run.py:483] Algo bellman_ford step 239 current loss 1.928807, current_train_items 7680.
I0302 18:58:21.740684 22447428132992 run.py:483] Algo bellman_ford step 240 current loss 0.679538, current_train_items 7712.
I0302 18:58:21.757036 22447428132992 run.py:483] Algo bellman_ford step 241 current loss 1.309262, current_train_items 7744.
I0302 18:58:21.780084 22447428132992 run.py:483] Algo bellman_ford step 242 current loss 1.599651, current_train_items 7776.
I0302 18:58:21.808794 22447428132992 run.py:483] Algo bellman_ford step 243 current loss 1.767447, current_train_items 7808.
I0302 18:58:21.837947 22447428132992 run.py:483] Algo bellman_ford step 244 current loss 1.672547, current_train_items 7840.
I0302 18:58:21.856850 22447428132992 run.py:483] Algo bellman_ford step 245 current loss 0.769802, current_train_items 7872.
I0302 18:58:21.872912 22447428132992 run.py:483] Algo bellman_ford step 246 current loss 1.199675, current_train_items 7904.
I0302 18:58:21.895354 22447428132992 run.py:483] Algo bellman_ford step 247 current loss 1.618073, current_train_items 7936.
I0302 18:58:21.925374 22447428132992 run.py:483] Algo bellman_ford step 248 current loss 1.852262, current_train_items 7968.
I0302 18:58:21.957309 22447428132992 run.py:483] Algo bellman_ford step 249 current loss 1.726599, current_train_items 8000.
I0302 18:58:21.975891 22447428132992 run.py:483] Algo bellman_ford step 250 current loss 0.799831, current_train_items 8032.
I0302 18:58:21.984304 22447428132992 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.7822265625, 'score': 0.7822265625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:58:21.984419 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.798, current avg val score is 0.782, val scores are: bellman_ford: 0.782
I0302 18:58:22.001439 22447428132992 run.py:483] Algo bellman_ford step 251 current loss 1.234948, current_train_items 8064.
I0302 18:58:22.024942 22447428132992 run.py:483] Algo bellman_ford step 252 current loss 1.560446, current_train_items 8096.
I0302 18:58:22.054976 22447428132992 run.py:483] Algo bellman_ford step 253 current loss 1.662219, current_train_items 8128.
I0302 18:58:22.087855 22447428132992 run.py:483] Algo bellman_ford step 254 current loss 2.001529, current_train_items 8160.
I0302 18:58:22.106661 22447428132992 run.py:483] Algo bellman_ford step 255 current loss 0.856857, current_train_items 8192.
I0302 18:58:22.122349 22447428132992 run.py:483] Algo bellman_ford step 256 current loss 1.174427, current_train_items 8224.
I0302 18:58:22.144710 22447428132992 run.py:483] Algo bellman_ford step 257 current loss 1.514775, current_train_items 8256.
I0302 18:58:22.173415 22447428132992 run.py:483] Algo bellman_ford step 258 current loss 1.713770, current_train_items 8288.
I0302 18:58:22.203810 22447428132992 run.py:483] Algo bellman_ford step 259 current loss 1.828429, current_train_items 8320.
I0302 18:58:22.222680 22447428132992 run.py:483] Algo bellman_ford step 260 current loss 0.716815, current_train_items 8352.
I0302 18:58:22.238731 22447428132992 run.py:483] Algo bellman_ford step 261 current loss 1.177509, current_train_items 8384.
I0302 18:58:22.261034 22447428132992 run.py:483] Algo bellman_ford step 262 current loss 1.437151, current_train_items 8416.
I0302 18:58:22.289362 22447428132992 run.py:483] Algo bellman_ford step 263 current loss 1.557592, current_train_items 8448.
I0302 18:58:22.319447 22447428132992 run.py:483] Algo bellman_ford step 264 current loss 1.714197, current_train_items 8480.
I0302 18:58:22.337946 22447428132992 run.py:483] Algo bellman_ford step 265 current loss 0.644859, current_train_items 8512.
I0302 18:58:22.354259 22447428132992 run.py:483] Algo bellman_ford step 266 current loss 1.084960, current_train_items 8544.
I0302 18:58:22.377831 22447428132992 run.py:483] Algo bellman_ford step 267 current loss 1.520789, current_train_items 8576.
I0302 18:58:22.407926 22447428132992 run.py:483] Algo bellman_ford step 268 current loss 1.801588, current_train_items 8608.
I0302 18:58:22.437790 22447428132992 run.py:483] Algo bellman_ford step 269 current loss 1.743369, current_train_items 8640.
I0302 18:58:22.456431 22447428132992 run.py:483] Algo bellman_ford step 270 current loss 0.738588, current_train_items 8672.
I0302 18:58:22.472130 22447428132992 run.py:483] Algo bellman_ford step 271 current loss 0.994266, current_train_items 8704.
I0302 18:58:22.495373 22447428132992 run.py:483] Algo bellman_ford step 272 current loss 1.395177, current_train_items 8736.
I0302 18:58:22.523983 22447428132992 run.py:483] Algo bellman_ford step 273 current loss 1.659078, current_train_items 8768.
I0302 18:58:22.553890 22447428132992 run.py:483] Algo bellman_ford step 274 current loss 1.929250, current_train_items 8800.
I0302 18:58:22.572620 22447428132992 run.py:483] Algo bellman_ford step 275 current loss 0.702437, current_train_items 8832.
I0302 18:58:22.588771 22447428132992 run.py:483] Algo bellman_ford step 276 current loss 1.039980, current_train_items 8864.
I0302 18:58:22.612355 22447428132992 run.py:483] Algo bellman_ford step 277 current loss 1.590754, current_train_items 8896.
I0302 18:58:22.642289 22447428132992 run.py:483] Algo bellman_ford step 278 current loss 1.716402, current_train_items 8928.
I0302 18:58:22.673285 22447428132992 run.py:483] Algo bellman_ford step 279 current loss 1.783032, current_train_items 8960.
I0302 18:58:22.691336 22447428132992 run.py:483] Algo bellman_ford step 280 current loss 0.623699, current_train_items 8992.
I0302 18:58:22.707777 22447428132992 run.py:483] Algo bellman_ford step 281 current loss 1.222128, current_train_items 9024.
I0302 18:58:22.731220 22447428132992 run.py:483] Algo bellman_ford step 282 current loss 1.306381, current_train_items 9056.
I0302 18:58:22.759567 22447428132992 run.py:483] Algo bellman_ford step 283 current loss 1.495658, current_train_items 9088.
I0302 18:58:22.791980 22447428132992 run.py:483] Algo bellman_ford step 284 current loss 2.078510, current_train_items 9120.
I0302 18:58:22.810553 22447428132992 run.py:483] Algo bellman_ford step 285 current loss 0.706897, current_train_items 9152.
I0302 18:58:22.826922 22447428132992 run.py:483] Algo bellman_ford step 286 current loss 1.119611, current_train_items 9184.
I0302 18:58:22.849270 22447428132992 run.py:483] Algo bellman_ford step 287 current loss 1.479618, current_train_items 9216.
I0302 18:58:22.878686 22447428132992 run.py:483] Algo bellman_ford step 288 current loss 1.811519, current_train_items 9248.
I0302 18:58:22.910687 22447428132992 run.py:483] Algo bellman_ford step 289 current loss 1.966241, current_train_items 9280.
I0302 18:58:22.929311 22447428132992 run.py:483] Algo bellman_ford step 290 current loss 0.671412, current_train_items 9312.
I0302 18:58:22.945417 22447428132992 run.py:483] Algo bellman_ford step 291 current loss 1.096674, current_train_items 9344.
I0302 18:58:22.968130 22447428132992 run.py:483] Algo bellman_ford step 292 current loss 1.251652, current_train_items 9376.
I0302 18:58:22.996576 22447428132992 run.py:483] Algo bellman_ford step 293 current loss 1.493210, current_train_items 9408.
I0302 18:58:23.027586 22447428132992 run.py:483] Algo bellman_ford step 294 current loss 1.672844, current_train_items 9440.
I0302 18:58:23.045743 22447428132992 run.py:483] Algo bellman_ford step 295 current loss 0.677309, current_train_items 9472.
I0302 18:58:23.061522 22447428132992 run.py:483] Algo bellman_ford step 296 current loss 0.948101, current_train_items 9504.
I0302 18:58:23.084995 22447428132992 run.py:483] Algo bellman_ford step 297 current loss 1.381483, current_train_items 9536.
I0302 18:58:23.115006 22447428132992 run.py:483] Algo bellman_ford step 298 current loss 1.477065, current_train_items 9568.
I0302 18:58:23.145578 22447428132992 run.py:483] Algo bellman_ford step 299 current loss 1.670428, current_train_items 9600.
I0302 18:58:23.164528 22447428132992 run.py:483] Algo bellman_ford step 300 current loss 0.718479, current_train_items 9632.
I0302 18:58:23.172366 22447428132992 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.7880859375, 'score': 0.7880859375, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:58:23.172481 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.798, current avg val score is 0.788, val scores are: bellman_ford: 0.788
I0302 18:58:23.189144 22447428132992 run.py:483] Algo bellman_ford step 301 current loss 1.035052, current_train_items 9664.
I0302 18:58:23.211629 22447428132992 run.py:483] Algo bellman_ford step 302 current loss 1.392566, current_train_items 9696.
I0302 18:58:23.239825 22447428132992 run.py:483] Algo bellman_ford step 303 current loss 1.734677, current_train_items 9728.
I0302 18:58:23.272292 22447428132992 run.py:483] Algo bellman_ford step 304 current loss 1.744610, current_train_items 9760.
I0302 18:58:23.291004 22447428132992 run.py:483] Algo bellman_ford step 305 current loss 0.790995, current_train_items 9792.
I0302 18:58:23.307528 22447428132992 run.py:483] Algo bellman_ford step 306 current loss 1.380851, current_train_items 9824.
I0302 18:58:23.331238 22447428132992 run.py:483] Algo bellman_ford step 307 current loss 1.456035, current_train_items 9856.
I0302 18:58:23.360274 22447428132992 run.py:483] Algo bellman_ford step 308 current loss 1.410429, current_train_items 9888.
I0302 18:58:23.391439 22447428132992 run.py:483] Algo bellman_ford step 309 current loss 1.590068, current_train_items 9920.
I0302 18:58:23.409785 22447428132992 run.py:483] Algo bellman_ford step 310 current loss 0.745428, current_train_items 9952.
I0302 18:58:23.425769 22447428132992 run.py:483] Algo bellman_ford step 311 current loss 0.996569, current_train_items 9984.
I0302 18:58:23.448623 22447428132992 run.py:483] Algo bellman_ford step 312 current loss 1.323967, current_train_items 10016.
I0302 18:58:23.477962 22447428132992 run.py:483] Algo bellman_ford step 313 current loss 1.596942, current_train_items 10048.
I0302 18:58:23.509240 22447428132992 run.py:483] Algo bellman_ford step 314 current loss 1.686930, current_train_items 10080.
I0302 18:58:23.527480 22447428132992 run.py:483] Algo bellman_ford step 315 current loss 0.831474, current_train_items 10112.
I0302 18:58:23.543736 22447428132992 run.py:483] Algo bellman_ford step 316 current loss 1.063664, current_train_items 10144.
I0302 18:58:23.566057 22447428132992 run.py:483] Algo bellman_ford step 317 current loss 1.180758, current_train_items 10176.
I0302 18:58:23.594256 22447428132992 run.py:483] Algo bellman_ford step 318 current loss 1.477800, current_train_items 10208.
I0302 18:58:23.624772 22447428132992 run.py:483] Algo bellman_ford step 319 current loss 1.778861, current_train_items 10240.
I0302 18:58:23.642957 22447428132992 run.py:483] Algo bellman_ford step 320 current loss 0.648689, current_train_items 10272.
I0302 18:58:23.659208 22447428132992 run.py:483] Algo bellman_ford step 321 current loss 1.220632, current_train_items 10304.
I0302 18:58:23.682334 22447428132992 run.py:483] Algo bellman_ford step 322 current loss 1.464292, current_train_items 10336.
I0302 18:58:23.710195 22447428132992 run.py:483] Algo bellman_ford step 323 current loss 1.410244, current_train_items 10368.
I0302 18:58:23.741465 22447428132992 run.py:483] Algo bellman_ford step 324 current loss 1.979953, current_train_items 10400.
I0302 18:58:23.759959 22447428132992 run.py:483] Algo bellman_ford step 325 current loss 0.763809, current_train_items 10432.
I0302 18:58:23.775764 22447428132992 run.py:483] Algo bellman_ford step 326 current loss 1.188933, current_train_items 10464.
I0302 18:58:23.798663 22447428132992 run.py:483] Algo bellman_ford step 327 current loss 1.479126, current_train_items 10496.
I0302 18:58:23.828155 22447428132992 run.py:483] Algo bellman_ford step 328 current loss 1.763835, current_train_items 10528.
I0302 18:58:23.859970 22447428132992 run.py:483] Algo bellman_ford step 329 current loss 1.723854, current_train_items 10560.
I0302 18:58:23.878303 22447428132992 run.py:483] Algo bellman_ford step 330 current loss 0.733301, current_train_items 10592.
I0302 18:58:23.894304 22447428132992 run.py:483] Algo bellman_ford step 331 current loss 1.070742, current_train_items 10624.
I0302 18:58:23.917241 22447428132992 run.py:483] Algo bellman_ford step 332 current loss 1.641532, current_train_items 10656.
I0302 18:58:23.946516 22447428132992 run.py:483] Algo bellman_ford step 333 current loss 1.846222, current_train_items 10688.
I0302 18:58:23.979019 22447428132992 run.py:483] Algo bellman_ford step 334 current loss 2.764824, current_train_items 10720.
I0302 18:58:23.997426 22447428132992 run.py:483] Algo bellman_ford step 335 current loss 0.783877, current_train_items 10752.
I0302 18:58:24.013926 22447428132992 run.py:483] Algo bellman_ford step 336 current loss 1.287168, current_train_items 10784.
I0302 18:58:24.037859 22447428132992 run.py:483] Algo bellman_ford step 337 current loss 1.527392, current_train_items 10816.
I0302 18:58:24.066889 22447428132992 run.py:483] Algo bellman_ford step 338 current loss 1.456111, current_train_items 10848.
I0302 18:58:24.099118 22447428132992 run.py:483] Algo bellman_ford step 339 current loss 1.675607, current_train_items 10880.
I0302 18:58:24.117525 22447428132992 run.py:483] Algo bellman_ford step 340 current loss 0.729550, current_train_items 10912.
I0302 18:58:24.133309 22447428132992 run.py:483] Algo bellman_ford step 341 current loss 1.047879, current_train_items 10944.
I0302 18:58:24.155888 22447428132992 run.py:483] Algo bellman_ford step 342 current loss 1.622139, current_train_items 10976.
I0302 18:58:24.183649 22447428132992 run.py:483] Algo bellman_ford step 343 current loss 1.586964, current_train_items 11008.
I0302 18:58:24.215662 22447428132992 run.py:483] Algo bellman_ford step 344 current loss 1.776030, current_train_items 11040.
I0302 18:58:24.234131 22447428132992 run.py:483] Algo bellman_ford step 345 current loss 0.764851, current_train_items 11072.
I0302 18:58:24.250154 22447428132992 run.py:483] Algo bellman_ford step 346 current loss 1.122943, current_train_items 11104.
I0302 18:58:24.274070 22447428132992 run.py:483] Algo bellman_ford step 347 current loss 1.546460, current_train_items 11136.
I0302 18:58:24.302686 22447428132992 run.py:483] Algo bellman_ford step 348 current loss 1.477988, current_train_items 11168.
I0302 18:58:24.335230 22447428132992 run.py:483] Algo bellman_ford step 349 current loss 1.757976, current_train_items 11200.
I0302 18:58:24.353840 22447428132992 run.py:483] Algo bellman_ford step 350 current loss 0.828982, current_train_items 11232.
I0302 18:58:24.362026 22447428132992 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.818359375, 'score': 0.818359375, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:58:24.362133 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.798, current avg val score is 0.818, val scores are: bellman_ford: 0.818
I0302 18:58:24.390720 22447428132992 run.py:483] Algo bellman_ford step 351 current loss 1.081352, current_train_items 11264.
I0302 18:58:24.414255 22447428132992 run.py:483] Algo bellman_ford step 352 current loss 1.457887, current_train_items 11296.
I0302 18:58:24.444181 22447428132992 run.py:483] Algo bellman_ford step 353 current loss 1.685543, current_train_items 11328.
I0302 18:58:24.477868 22447428132992 run.py:483] Algo bellman_ford step 354 current loss 1.787880, current_train_items 11360.
I0302 18:58:24.497157 22447428132992 run.py:483] Algo bellman_ford step 355 current loss 0.733920, current_train_items 11392.
I0302 18:58:24.512984 22447428132992 run.py:483] Algo bellman_ford step 356 current loss 1.076742, current_train_items 11424.
I0302 18:58:24.535478 22447428132992 run.py:483] Algo bellman_ford step 357 current loss 1.362937, current_train_items 11456.
I0302 18:58:24.565777 22447428132992 run.py:483] Algo bellman_ford step 358 current loss 1.596699, current_train_items 11488.
I0302 18:58:24.598546 22447428132992 run.py:483] Algo bellman_ford step 359 current loss 1.845810, current_train_items 11520.
I0302 18:58:24.617454 22447428132992 run.py:483] Algo bellman_ford step 360 current loss 0.715212, current_train_items 11552.
I0302 18:58:24.633920 22447428132992 run.py:483] Algo bellman_ford step 361 current loss 1.036074, current_train_items 11584.
I0302 18:58:24.655632 22447428132992 run.py:483] Algo bellman_ford step 362 current loss 1.375495, current_train_items 11616.
I0302 18:58:24.684460 22447428132992 run.py:483] Algo bellman_ford step 363 current loss 1.523543, current_train_items 11648.
I0302 18:58:24.714517 22447428132992 run.py:483] Algo bellman_ford step 364 current loss 1.478227, current_train_items 11680.
I0302 18:58:24.733162 22447428132992 run.py:483] Algo bellman_ford step 365 current loss 0.691175, current_train_items 11712.
I0302 18:58:24.749324 22447428132992 run.py:483] Algo bellman_ford step 366 current loss 1.114482, current_train_items 11744.
I0302 18:58:24.771871 22447428132992 run.py:483] Algo bellman_ford step 367 current loss 1.292216, current_train_items 11776.
I0302 18:58:24.801503 22447428132992 run.py:483] Algo bellman_ford step 368 current loss 1.680531, current_train_items 11808.
I0302 18:58:24.831062 22447428132992 run.py:483] Algo bellman_ford step 369 current loss 1.639355, current_train_items 11840.
I0302 18:58:24.849638 22447428132992 run.py:483] Algo bellman_ford step 370 current loss 0.597302, current_train_items 11872.
I0302 18:58:24.865563 22447428132992 run.py:483] Algo bellman_ford step 371 current loss 1.029299, current_train_items 11904.
I0302 18:58:24.887806 22447428132992 run.py:483] Algo bellman_ford step 372 current loss 1.270802, current_train_items 11936.
I0302 18:58:24.917601 22447428132992 run.py:483] Algo bellman_ford step 373 current loss 1.819847, current_train_items 11968.
I0302 18:58:24.951062 22447428132992 run.py:483] Algo bellman_ford step 374 current loss 2.078356, current_train_items 12000.
I0302 18:58:24.969715 22447428132992 run.py:483] Algo bellman_ford step 375 current loss 0.666202, current_train_items 12032.
I0302 18:58:24.985563 22447428132992 run.py:483] Algo bellman_ford step 376 current loss 1.053414, current_train_items 12064.
I0302 18:58:25.008927 22447428132992 run.py:483] Algo bellman_ford step 377 current loss 1.459504, current_train_items 12096.
I0302 18:58:25.037720 22447428132992 run.py:483] Algo bellman_ford step 378 current loss 1.485171, current_train_items 12128.
I0302 18:58:25.070320 22447428132992 run.py:483] Algo bellman_ford step 379 current loss 1.949606, current_train_items 12160.
I0302 18:58:25.088692 22447428132992 run.py:483] Algo bellman_ford step 380 current loss 0.633373, current_train_items 12192.
I0302 18:58:25.105132 22447428132992 run.py:483] Algo bellman_ford step 381 current loss 1.084603, current_train_items 12224.
I0302 18:58:25.128356 22447428132992 run.py:483] Algo bellman_ford step 382 current loss 1.296209, current_train_items 12256.
I0302 18:58:25.157414 22447428132992 run.py:483] Algo bellman_ford step 383 current loss 1.550446, current_train_items 12288.
I0302 18:58:25.190094 22447428132992 run.py:483] Algo bellman_ford step 384 current loss 1.715592, current_train_items 12320.
I0302 18:58:25.208751 22447428132992 run.py:483] Algo bellman_ford step 385 current loss 0.742537, current_train_items 12352.
I0302 18:58:25.225164 22447428132992 run.py:483] Algo bellman_ford step 386 current loss 0.942896, current_train_items 12384.
I0302 18:58:25.247416 22447428132992 run.py:483] Algo bellman_ford step 387 current loss 1.606082, current_train_items 12416.
I0302 18:58:25.275957 22447428132992 run.py:483] Algo bellman_ford step 388 current loss 1.446782, current_train_items 12448.
I0302 18:58:25.308165 22447428132992 run.py:483] Algo bellman_ford step 389 current loss 1.913087, current_train_items 12480.
I0302 18:58:25.326647 22447428132992 run.py:483] Algo bellman_ford step 390 current loss 0.768861, current_train_items 12512.
I0302 18:58:25.342526 22447428132992 run.py:483] Algo bellman_ford step 391 current loss 1.048389, current_train_items 12544.
I0302 18:58:25.365097 22447428132992 run.py:483] Algo bellman_ford step 392 current loss 1.260858, current_train_items 12576.
I0302 18:58:25.395596 22447428132992 run.py:483] Algo bellman_ford step 393 current loss 1.851374, current_train_items 12608.
I0302 18:58:25.426685 22447428132992 run.py:483] Algo bellman_ford step 394 current loss 1.802400, current_train_items 12640.
I0302 18:58:25.444786 22447428132992 run.py:483] Algo bellman_ford step 395 current loss 0.669682, current_train_items 12672.
I0302 18:58:25.460968 22447428132992 run.py:483] Algo bellman_ford step 396 current loss 0.983971, current_train_items 12704.
I0302 18:58:25.484497 22447428132992 run.py:483] Algo bellman_ford step 397 current loss 1.424465, current_train_items 12736.
I0302 18:58:25.514909 22447428132992 run.py:483] Algo bellman_ford step 398 current loss 1.501686, current_train_items 12768.
I0302 18:58:25.544569 22447428132992 run.py:483] Algo bellman_ford step 399 current loss 1.502495, current_train_items 12800.
I0302 18:58:25.562988 22447428132992 run.py:483] Algo bellman_ford step 400 current loss 0.603454, current_train_items 12832.
I0302 18:58:25.570878 22447428132992 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.83984375, 'score': 0.83984375, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:58:25.570983 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.818, current avg val score is 0.840, val scores are: bellman_ford: 0.840
I0302 18:58:25.600989 22447428132992 run.py:483] Algo bellman_ford step 401 current loss 0.981545, current_train_items 12864.
I0302 18:58:25.624831 22447428132992 run.py:483] Algo bellman_ford step 402 current loss 1.351848, current_train_items 12896.
I0302 18:58:25.655102 22447428132992 run.py:483] Algo bellman_ford step 403 current loss 1.803125, current_train_items 12928.
I0302 18:58:25.686148 22447428132992 run.py:483] Algo bellman_ford step 404 current loss 1.901529, current_train_items 12960.
I0302 18:58:25.705460 22447428132992 run.py:483] Algo bellman_ford step 405 current loss 0.747348, current_train_items 12992.
I0302 18:58:25.720840 22447428132992 run.py:483] Algo bellman_ford step 406 current loss 0.903133, current_train_items 13024.
I0302 18:58:25.743830 22447428132992 run.py:483] Algo bellman_ford step 407 current loss 1.294143, current_train_items 13056.
I0302 18:58:25.774182 22447428132992 run.py:483] Algo bellman_ford step 408 current loss 1.455703, current_train_items 13088.
I0302 18:58:25.805701 22447428132992 run.py:483] Algo bellman_ford step 409 current loss 1.880937, current_train_items 13120.
I0302 18:58:25.824542 22447428132992 run.py:483] Algo bellman_ford step 410 current loss 0.660564, current_train_items 13152.
I0302 18:58:25.840367 22447428132992 run.py:483] Algo bellman_ford step 411 current loss 0.983438, current_train_items 13184.
I0302 18:58:25.863767 22447428132992 run.py:483] Algo bellman_ford step 412 current loss 1.362672, current_train_items 13216.
I0302 18:58:25.893439 22447428132992 run.py:483] Algo bellman_ford step 413 current loss 1.552048, current_train_items 13248.
I0302 18:58:25.925629 22447428132992 run.py:483] Algo bellman_ford step 414 current loss 1.594869, current_train_items 13280.
I0302 18:58:25.944353 22447428132992 run.py:483] Algo bellman_ford step 415 current loss 0.691817, current_train_items 13312.
I0302 18:58:25.960165 22447428132992 run.py:483] Algo bellman_ford step 416 current loss 0.986379, current_train_items 13344.
I0302 18:58:25.983458 22447428132992 run.py:483] Algo bellman_ford step 417 current loss 1.407663, current_train_items 13376.
I0302 18:58:26.014892 22447428132992 run.py:483] Algo bellman_ford step 418 current loss 1.527350, current_train_items 13408.
I0302 18:58:26.046784 22447428132992 run.py:483] Algo bellman_ford step 419 current loss 1.542708, current_train_items 13440.
I0302 18:58:26.065047 22447428132992 run.py:483] Algo bellman_ford step 420 current loss 0.792548, current_train_items 13472.
I0302 18:58:26.080960 22447428132992 run.py:483] Algo bellman_ford step 421 current loss 0.997992, current_train_items 13504.
I0302 18:58:26.104521 22447428132992 run.py:483] Algo bellman_ford step 422 current loss 1.515332, current_train_items 13536.
I0302 18:58:26.132904 22447428132992 run.py:483] Algo bellman_ford step 423 current loss 1.503502, current_train_items 13568.
I0302 18:58:26.164750 22447428132992 run.py:483] Algo bellman_ford step 424 current loss 2.122786, current_train_items 13600.
I0302 18:58:26.183132 22447428132992 run.py:483] Algo bellman_ford step 425 current loss 0.737376, current_train_items 13632.
I0302 18:58:26.198929 22447428132992 run.py:483] Algo bellman_ford step 426 current loss 0.806351, current_train_items 13664.
I0302 18:58:26.221624 22447428132992 run.py:483] Algo bellman_ford step 427 current loss 1.205903, current_train_items 13696.
I0302 18:58:26.250211 22447428132992 run.py:483] Algo bellman_ford step 428 current loss 1.515422, current_train_items 13728.
I0302 18:58:26.281847 22447428132992 run.py:483] Algo bellman_ford step 429 current loss 1.654328, current_train_items 13760.
I0302 18:58:26.300206 22447428132992 run.py:483] Algo bellman_ford step 430 current loss 0.735767, current_train_items 13792.
I0302 18:58:26.316244 22447428132992 run.py:483] Algo bellman_ford step 431 current loss 1.007566, current_train_items 13824.
I0302 18:58:26.339823 22447428132992 run.py:483] Algo bellman_ford step 432 current loss 1.446443, current_train_items 13856.
I0302 18:58:26.367900 22447428132992 run.py:483] Algo bellman_ford step 433 current loss 1.346049, current_train_items 13888.
I0302 18:58:26.399344 22447428132992 run.py:483] Algo bellman_ford step 434 current loss 1.686820, current_train_items 13920.
I0302 18:58:26.417462 22447428132992 run.py:483] Algo bellman_ford step 435 current loss 0.678233, current_train_items 13952.
I0302 18:58:26.433846 22447428132992 run.py:483] Algo bellman_ford step 436 current loss 1.158729, current_train_items 13984.
I0302 18:58:26.456341 22447428132992 run.py:483] Algo bellman_ford step 437 current loss 1.439179, current_train_items 14016.
I0302 18:58:26.485165 22447428132992 run.py:483] Algo bellman_ford step 438 current loss 1.500713, current_train_items 14048.
I0302 18:58:26.516728 22447428132992 run.py:483] Algo bellman_ford step 439 current loss 1.596315, current_train_items 14080.
I0302 18:58:26.535219 22447428132992 run.py:483] Algo bellman_ford step 440 current loss 0.667377, current_train_items 14112.
I0302 18:58:26.551138 22447428132992 run.py:483] Algo bellman_ford step 441 current loss 0.957114, current_train_items 14144.
I0302 18:58:26.573573 22447428132992 run.py:483] Algo bellman_ford step 442 current loss 1.659873, current_train_items 14176.
I0302 18:58:26.602802 22447428132992 run.py:483] Algo bellman_ford step 443 current loss 1.559999, current_train_items 14208.
I0302 18:58:26.633695 22447428132992 run.py:483] Algo bellman_ford step 444 current loss 1.493443, current_train_items 14240.
I0302 18:58:26.652076 22447428132992 run.py:483] Algo bellman_ford step 445 current loss 0.673588, current_train_items 14272.
I0302 18:58:26.669006 22447428132992 run.py:483] Algo bellman_ford step 446 current loss 1.356896, current_train_items 14304.
I0302 18:58:26.692778 22447428132992 run.py:483] Algo bellman_ford step 447 current loss 1.416680, current_train_items 14336.
I0302 18:58:26.720676 22447428132992 run.py:483] Algo bellman_ford step 448 current loss 1.565754, current_train_items 14368.
I0302 18:58:26.750342 22447428132992 run.py:483] Algo bellman_ford step 449 current loss 1.550066, current_train_items 14400.
I0302 18:58:26.769074 22447428132992 run.py:483] Algo bellman_ford step 450 current loss 0.771281, current_train_items 14432.
I0302 18:58:26.777097 22447428132992 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.806640625, 'score': 0.806640625, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:58:26.777203 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.807, val scores are: bellman_ford: 0.807
I0302 18:58:26.794028 22447428132992 run.py:483] Algo bellman_ford step 451 current loss 1.129073, current_train_items 14464.
I0302 18:58:26.817023 22447428132992 run.py:483] Algo bellman_ford step 452 current loss 1.364320, current_train_items 14496.
I0302 18:58:26.847512 22447428132992 run.py:483] Algo bellman_ford step 453 current loss 1.584506, current_train_items 14528.
I0302 18:58:26.879632 22447428132992 run.py:483] Algo bellman_ford step 454 current loss 1.566028, current_train_items 14560.
I0302 18:58:26.898523 22447428132992 run.py:483] Algo bellman_ford step 455 current loss 0.768232, current_train_items 14592.
I0302 18:58:26.914507 22447428132992 run.py:483] Algo bellman_ford step 456 current loss 1.092672, current_train_items 14624.
I0302 18:58:26.937405 22447428132992 run.py:483] Algo bellman_ford step 457 current loss 1.523483, current_train_items 14656.
I0302 18:58:26.966457 22447428132992 run.py:483] Algo bellman_ford step 458 current loss 1.483508, current_train_items 14688.
I0302 18:58:26.997128 22447428132992 run.py:483] Algo bellman_ford step 459 current loss 1.609326, current_train_items 14720.
I0302 18:58:27.015808 22447428132992 run.py:483] Algo bellman_ford step 460 current loss 0.649330, current_train_items 14752.
I0302 18:58:27.032270 22447428132992 run.py:483] Algo bellman_ford step 461 current loss 1.115315, current_train_items 14784.
I0302 18:58:27.055352 22447428132992 run.py:483] Algo bellman_ford step 462 current loss 1.370875, current_train_items 14816.
I0302 18:58:27.084702 22447428132992 run.py:483] Algo bellman_ford step 463 current loss 1.581564, current_train_items 14848.
I0302 18:58:27.117813 22447428132992 run.py:483] Algo bellman_ford step 464 current loss 1.577342, current_train_items 14880.
I0302 18:58:27.136246 22447428132992 run.py:483] Algo bellman_ford step 465 current loss 0.870110, current_train_items 14912.
I0302 18:58:27.151899 22447428132992 run.py:483] Algo bellman_ford step 466 current loss 0.995364, current_train_items 14944.
I0302 18:58:27.175466 22447428132992 run.py:483] Algo bellman_ford step 467 current loss 1.465939, current_train_items 14976.
I0302 18:58:27.203353 22447428132992 run.py:483] Algo bellman_ford step 468 current loss 1.552518, current_train_items 15008.
I0302 18:58:27.236258 22447428132992 run.py:483] Algo bellman_ford step 469 current loss 1.579527, current_train_items 15040.
I0302 18:58:27.255122 22447428132992 run.py:483] Algo bellman_ford step 470 current loss 0.699868, current_train_items 15072.
I0302 18:58:27.271226 22447428132992 run.py:483] Algo bellman_ford step 471 current loss 1.017428, current_train_items 15104.
I0302 18:58:27.294800 22447428132992 run.py:483] Algo bellman_ford step 472 current loss 1.452507, current_train_items 15136.
I0302 18:58:27.324238 22447428132992 run.py:483] Algo bellman_ford step 473 current loss 1.497760, current_train_items 15168.
I0302 18:58:27.357038 22447428132992 run.py:483] Algo bellman_ford step 474 current loss 1.709219, current_train_items 15200.
I0302 18:58:27.375946 22447428132992 run.py:483] Algo bellman_ford step 475 current loss 0.807204, current_train_items 15232.
I0302 18:58:27.391799 22447428132992 run.py:483] Algo bellman_ford step 476 current loss 1.021115, current_train_items 15264.
I0302 18:58:27.415217 22447428132992 run.py:483] Algo bellman_ford step 477 current loss 1.250790, current_train_items 15296.
I0302 18:58:27.443303 22447428132992 run.py:483] Algo bellman_ford step 478 current loss 1.368367, current_train_items 15328.
I0302 18:58:27.476008 22447428132992 run.py:483] Algo bellman_ford step 479 current loss 1.860167, current_train_items 15360.
I0302 18:58:27.494344 22447428132992 run.py:483] Algo bellman_ford step 480 current loss 0.709579, current_train_items 15392.
I0302 18:58:27.510452 22447428132992 run.py:483] Algo bellman_ford step 481 current loss 1.003267, current_train_items 15424.
I0302 18:58:27.533098 22447428132992 run.py:483] Algo bellman_ford step 482 current loss 1.168092, current_train_items 15456.
I0302 18:58:27.562453 22447428132992 run.py:483] Algo bellman_ford step 483 current loss 1.373249, current_train_items 15488.
I0302 18:58:27.594355 22447428132992 run.py:483] Algo bellman_ford step 484 current loss 1.548594, current_train_items 15520.
I0302 18:58:27.613032 22447428132992 run.py:483] Algo bellman_ford step 485 current loss 0.723287, current_train_items 15552.
I0302 18:58:27.629114 22447428132992 run.py:483] Algo bellman_ford step 486 current loss 1.037044, current_train_items 15584.
I0302 18:58:27.652300 22447428132992 run.py:483] Algo bellman_ford step 487 current loss 1.392265, current_train_items 15616.
I0302 18:58:27.681203 22447428132992 run.py:483] Algo bellman_ford step 488 current loss 1.453687, current_train_items 15648.
I0302 18:58:27.711977 22447428132992 run.py:483] Algo bellman_ford step 489 current loss 1.739889, current_train_items 15680.
I0302 18:58:27.730926 22447428132992 run.py:483] Algo bellman_ford step 490 current loss 0.690326, current_train_items 15712.
I0302 18:58:27.747209 22447428132992 run.py:483] Algo bellman_ford step 491 current loss 1.053547, current_train_items 15744.
I0302 18:58:27.770097 22447428132992 run.py:483] Algo bellman_ford step 492 current loss 1.302548, current_train_items 15776.
I0302 18:58:27.799101 22447428132992 run.py:483] Algo bellman_ford step 493 current loss 1.529904, current_train_items 15808.
I0302 18:58:27.832115 22447428132992 run.py:483] Algo bellman_ford step 494 current loss 1.918786, current_train_items 15840.
I0302 18:58:27.850437 22447428132992 run.py:483] Algo bellman_ford step 495 current loss 0.698560, current_train_items 15872.
I0302 18:58:27.866348 22447428132992 run.py:483] Algo bellman_ford step 496 current loss 0.965337, current_train_items 15904.
I0302 18:58:27.890702 22447428132992 run.py:483] Algo bellman_ford step 497 current loss 1.378125, current_train_items 15936.
I0302 18:58:27.920768 22447428132992 run.py:483] Algo bellman_ford step 498 current loss 1.426344, current_train_items 15968.
I0302 18:58:27.952145 22447428132992 run.py:483] Algo bellman_ford step 499 current loss 1.367504, current_train_items 16000.
I0302 18:58:27.970863 22447428132992 run.py:483] Algo bellman_ford step 500 current loss 0.761091, current_train_items 16032.
I0302 18:58:27.978749 22447428132992 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.7880859375, 'score': 0.7880859375, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:27.978856 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.788, val scores are: bellman_ford: 0.788
I0302 18:58:27.995078 22447428132992 run.py:483] Algo bellman_ford step 501 current loss 0.916392, current_train_items 16064.
I0302 18:58:28.018607 22447428132992 run.py:483] Algo bellman_ford step 502 current loss 1.490519, current_train_items 16096.
I0302 18:58:28.049092 22447428132992 run.py:483] Algo bellman_ford step 503 current loss 1.785126, current_train_items 16128.
I0302 18:58:28.083403 22447428132992 run.py:483] Algo bellman_ford step 504 current loss 2.161509, current_train_items 16160.
I0302 18:58:28.102462 22447428132992 run.py:483] Algo bellman_ford step 505 current loss 0.726398, current_train_items 16192.
I0302 18:58:28.117668 22447428132992 run.py:483] Algo bellman_ford step 506 current loss 0.839398, current_train_items 16224.
I0302 18:58:28.140060 22447428132992 run.py:483] Algo bellman_ford step 507 current loss 1.244923, current_train_items 16256.
I0302 18:58:28.169763 22447428132992 run.py:483] Algo bellman_ford step 508 current loss 1.486342, current_train_items 16288.
I0302 18:58:28.201984 22447428132992 run.py:483] Algo bellman_ford step 509 current loss 1.761642, current_train_items 16320.
I0302 18:58:28.220473 22447428132992 run.py:483] Algo bellman_ford step 510 current loss 0.744416, current_train_items 16352.
I0302 18:58:28.236755 22447428132992 run.py:483] Algo bellman_ford step 511 current loss 1.039713, current_train_items 16384.
I0302 18:58:28.259688 22447428132992 run.py:483] Algo bellman_ford step 512 current loss 1.237187, current_train_items 16416.
I0302 18:58:28.289185 22447428132992 run.py:483] Algo bellman_ford step 513 current loss 1.575012, current_train_items 16448.
I0302 18:58:28.320992 22447428132992 run.py:483] Algo bellman_ford step 514 current loss 1.586151, current_train_items 16480.
I0302 18:58:28.338988 22447428132992 run.py:483] Algo bellman_ford step 515 current loss 0.656497, current_train_items 16512.
I0302 18:58:28.354808 22447428132992 run.py:483] Algo bellman_ford step 516 current loss 0.882027, current_train_items 16544.
I0302 18:58:28.377995 22447428132992 run.py:483] Algo bellman_ford step 517 current loss 1.279931, current_train_items 16576.
I0302 18:58:28.408314 22447428132992 run.py:483] Algo bellman_ford step 518 current loss 1.613772, current_train_items 16608.
I0302 18:58:28.439991 22447428132992 run.py:483] Algo bellman_ford step 519 current loss 1.805830, current_train_items 16640.
I0302 18:58:28.458068 22447428132992 run.py:483] Algo bellman_ford step 520 current loss 0.754469, current_train_items 16672.
I0302 18:58:28.474430 22447428132992 run.py:483] Algo bellman_ford step 521 current loss 0.957163, current_train_items 16704.
I0302 18:58:28.497110 22447428132992 run.py:483] Algo bellman_ford step 522 current loss 1.266846, current_train_items 16736.
I0302 18:58:28.526756 22447428132992 run.py:483] Algo bellman_ford step 523 current loss 1.441061, current_train_items 16768.
I0302 18:58:28.557701 22447428132992 run.py:483] Algo bellman_ford step 524 current loss 1.777058, current_train_items 16800.
I0302 18:58:28.575859 22447428132992 run.py:483] Algo bellman_ford step 525 current loss 0.676887, current_train_items 16832.
I0302 18:58:28.592155 22447428132992 run.py:483] Algo bellman_ford step 526 current loss 1.050154, current_train_items 16864.
I0302 18:58:28.614945 22447428132992 run.py:483] Algo bellman_ford step 527 current loss 1.397920, current_train_items 16896.
I0302 18:58:28.643456 22447428132992 run.py:483] Algo bellman_ford step 528 current loss 1.359658, current_train_items 16928.
I0302 18:58:28.675362 22447428132992 run.py:483] Algo bellman_ford step 529 current loss 1.554358, current_train_items 16960.
I0302 18:58:28.693708 22447428132992 run.py:483] Algo bellman_ford step 530 current loss 0.864807, current_train_items 16992.
I0302 18:58:28.709885 22447428132992 run.py:483] Algo bellman_ford step 531 current loss 1.025510, current_train_items 17024.
I0302 18:58:28.732015 22447428132992 run.py:483] Algo bellman_ford step 532 current loss 1.280386, current_train_items 17056.
I0302 18:58:28.761295 22447428132992 run.py:483] Algo bellman_ford step 533 current loss 1.830093, current_train_items 17088.
I0302 18:58:28.793146 22447428132992 run.py:483] Algo bellman_ford step 534 current loss 2.164372, current_train_items 17120.
I0302 18:58:28.811367 22447428132992 run.py:483] Algo bellman_ford step 535 current loss 0.591616, current_train_items 17152.
I0302 18:58:28.827083 22447428132992 run.py:483] Algo bellman_ford step 536 current loss 0.952823, current_train_items 17184.
I0302 18:58:28.849368 22447428132992 run.py:483] Algo bellman_ford step 537 current loss 1.187716, current_train_items 17216.
I0302 18:58:28.877566 22447428132992 run.py:483] Algo bellman_ford step 538 current loss 1.465309, current_train_items 17248.
I0302 18:58:28.907901 22447428132992 run.py:483] Algo bellman_ford step 539 current loss 1.445049, current_train_items 17280.
I0302 18:58:28.925911 22447428132992 run.py:483] Algo bellman_ford step 540 current loss 0.690420, current_train_items 17312.
I0302 18:58:28.941435 22447428132992 run.py:483] Algo bellman_ford step 541 current loss 0.821320, current_train_items 17344.
I0302 18:58:28.964596 22447428132992 run.py:483] Algo bellman_ford step 542 current loss 1.294184, current_train_items 17376.
I0302 18:58:28.994134 22447428132992 run.py:483] Algo bellman_ford step 543 current loss 1.430768, current_train_items 17408.
I0302 18:58:29.025912 22447428132992 run.py:483] Algo bellman_ford step 544 current loss 1.677852, current_train_items 17440.
I0302 18:58:29.044270 22447428132992 run.py:483] Algo bellman_ford step 545 current loss 0.954034, current_train_items 17472.
I0302 18:58:29.060313 22447428132992 run.py:483] Algo bellman_ford step 546 current loss 1.149382, current_train_items 17504.
I0302 18:58:29.083000 22447428132992 run.py:483] Algo bellman_ford step 547 current loss 1.298561, current_train_items 17536.
I0302 18:58:29.112648 22447428132992 run.py:483] Algo bellman_ford step 548 current loss 1.517580, current_train_items 17568.
I0302 18:58:29.142120 22447428132992 run.py:483] Algo bellman_ford step 549 current loss 1.687596, current_train_items 17600.
I0302 18:58:29.160427 22447428132992 run.py:483] Algo bellman_ford step 550 current loss 0.705736, current_train_items 17632.
I0302 18:58:29.168638 22447428132992 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.796875, 'score': 0.796875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:29.168740 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.797, val scores are: bellman_ford: 0.797
I0302 18:58:29.185261 22447428132992 run.py:483] Algo bellman_ford step 551 current loss 0.881842, current_train_items 17664.
I0302 18:58:29.208350 22447428132992 run.py:483] Algo bellman_ford step 552 current loss 1.256045, current_train_items 17696.
I0302 18:58:29.239490 22447428132992 run.py:483] Algo bellman_ford step 553 current loss 1.564759, current_train_items 17728.
I0302 18:58:29.271667 22447428132992 run.py:483] Algo bellman_ford step 554 current loss 1.738117, current_train_items 17760.
I0302 18:58:29.290795 22447428132992 run.py:483] Algo bellman_ford step 555 current loss 0.878193, current_train_items 17792.
I0302 18:58:29.306431 22447428132992 run.py:483] Algo bellman_ford step 556 current loss 1.044630, current_train_items 17824.
I0302 18:58:29.330191 22447428132992 run.py:483] Algo bellman_ford step 557 current loss 1.375524, current_train_items 17856.
I0302 18:58:29.358932 22447428132992 run.py:483] Algo bellman_ford step 558 current loss 1.647573, current_train_items 17888.
I0302 18:58:29.392488 22447428132992 run.py:483] Algo bellman_ford step 559 current loss 2.276271, current_train_items 17920.
I0302 18:58:29.411613 22447428132992 run.py:483] Algo bellman_ford step 560 current loss 0.775389, current_train_items 17952.
I0302 18:58:29.427812 22447428132992 run.py:483] Algo bellman_ford step 561 current loss 0.931413, current_train_items 17984.
I0302 18:58:29.451096 22447428132992 run.py:483] Algo bellman_ford step 562 current loss 1.253149, current_train_items 18016.
I0302 18:58:29.480100 22447428132992 run.py:483] Algo bellman_ford step 563 current loss 1.361486, current_train_items 18048.
I0302 18:58:29.509719 22447428132992 run.py:483] Algo bellman_ford step 564 current loss 1.420649, current_train_items 18080.
I0302 18:58:29.528319 22447428132992 run.py:483] Algo bellman_ford step 565 current loss 0.818239, current_train_items 18112.
I0302 18:58:29.544673 22447428132992 run.py:483] Algo bellman_ford step 566 current loss 1.122628, current_train_items 18144.
I0302 18:58:29.568700 22447428132992 run.py:483] Algo bellman_ford step 567 current loss 1.405496, current_train_items 18176.
I0302 18:58:29.597919 22447428132992 run.py:483] Algo bellman_ford step 568 current loss 1.359668, current_train_items 18208.
I0302 18:58:29.628782 22447428132992 run.py:483] Algo bellman_ford step 569 current loss 1.992706, current_train_items 18240.
I0302 18:58:29.647775 22447428132992 run.py:483] Algo bellman_ford step 570 current loss 0.778619, current_train_items 18272.
I0302 18:58:29.663599 22447428132992 run.py:483] Algo bellman_ford step 571 current loss 0.964408, current_train_items 18304.
I0302 18:58:29.686932 22447428132992 run.py:483] Algo bellman_ford step 572 current loss 1.433519, current_train_items 18336.
I0302 18:58:29.717585 22447428132992 run.py:483] Algo bellman_ford step 573 current loss 1.540152, current_train_items 18368.
I0302 18:58:29.747992 22447428132992 run.py:483] Algo bellman_ford step 574 current loss 1.651901, current_train_items 18400.
I0302 18:58:29.767237 22447428132992 run.py:483] Algo bellman_ford step 575 current loss 0.820028, current_train_items 18432.
I0302 18:58:29.783152 22447428132992 run.py:483] Algo bellman_ford step 576 current loss 1.019631, current_train_items 18464.
I0302 18:58:29.806428 22447428132992 run.py:483] Algo bellman_ford step 577 current loss 1.250684, current_train_items 18496.
I0302 18:58:29.836114 22447428132992 run.py:483] Algo bellman_ford step 578 current loss 1.274872, current_train_items 18528.
I0302 18:58:29.869046 22447428132992 run.py:483] Algo bellman_ford step 579 current loss 1.618550, current_train_items 18560.
I0302 18:58:29.887441 22447428132992 run.py:483] Algo bellman_ford step 580 current loss 0.756024, current_train_items 18592.
I0302 18:58:29.903456 22447428132992 run.py:483] Algo bellman_ford step 581 current loss 0.876525, current_train_items 18624.
I0302 18:58:29.925320 22447428132992 run.py:483] Algo bellman_ford step 582 current loss 1.125139, current_train_items 18656.
I0302 18:58:29.953523 22447428132992 run.py:483] Algo bellman_ford step 583 current loss 1.231684, current_train_items 18688.
I0302 18:58:29.985132 22447428132992 run.py:483] Algo bellman_ford step 584 current loss 1.439971, current_train_items 18720.
I0302 18:58:30.003696 22447428132992 run.py:483] Algo bellman_ford step 585 current loss 0.633439, current_train_items 18752.
I0302 18:58:30.019804 22447428132992 run.py:483] Algo bellman_ford step 586 current loss 0.950935, current_train_items 18784.
I0302 18:58:30.041713 22447428132992 run.py:483] Algo bellman_ford step 587 current loss 1.182220, current_train_items 18816.
I0302 18:58:30.070243 22447428132992 run.py:483] Algo bellman_ford step 588 current loss 1.346377, current_train_items 18848.
I0302 18:58:30.103002 22447428132992 run.py:483] Algo bellman_ford step 589 current loss 1.710083, current_train_items 18880.
I0302 18:58:30.121838 22447428132992 run.py:483] Algo bellman_ford step 590 current loss 0.687642, current_train_items 18912.
I0302 18:58:30.138056 22447428132992 run.py:483] Algo bellman_ford step 591 current loss 1.005193, current_train_items 18944.
I0302 18:58:30.161510 22447428132992 run.py:483] Algo bellman_ford step 592 current loss 1.219236, current_train_items 18976.
I0302 18:58:30.190842 22447428132992 run.py:483] Algo bellman_ford step 593 current loss 1.518165, current_train_items 19008.
I0302 18:58:30.222190 22447428132992 run.py:483] Algo bellman_ford step 594 current loss 1.677500, current_train_items 19040.
I0302 18:58:30.240745 22447428132992 run.py:483] Algo bellman_ford step 595 current loss 0.676357, current_train_items 19072.
I0302 18:58:30.256869 22447428132992 run.py:483] Algo bellman_ford step 596 current loss 1.041652, current_train_items 19104.
I0302 18:58:30.280677 22447428132992 run.py:483] Algo bellman_ford step 597 current loss 1.227509, current_train_items 19136.
I0302 18:58:30.310237 22447428132992 run.py:483] Algo bellman_ford step 598 current loss 1.477452, current_train_items 19168.
I0302 18:58:30.341912 22447428132992 run.py:483] Algo bellman_ford step 599 current loss 1.583427, current_train_items 19200.
I0302 18:58:30.360598 22447428132992 run.py:483] Algo bellman_ford step 600 current loss 0.813814, current_train_items 19232.
I0302 18:58:30.368447 22447428132992 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.814453125, 'score': 0.814453125, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:30.368551 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.814, val scores are: bellman_ford: 0.814
I0302 18:58:30.385020 22447428132992 run.py:483] Algo bellman_ford step 601 current loss 0.824207, current_train_items 19264.
I0302 18:58:30.408079 22447428132992 run.py:483] Algo bellman_ford step 602 current loss 1.109388, current_train_items 19296.
I0302 18:58:30.436781 22447428132992 run.py:483] Algo bellman_ford step 603 current loss 1.289108, current_train_items 19328.
I0302 18:58:30.467838 22447428132992 run.py:483] Algo bellman_ford step 604 current loss 1.528332, current_train_items 19360.
I0302 18:58:30.486542 22447428132992 run.py:483] Algo bellman_ford step 605 current loss 0.625632, current_train_items 19392.
I0302 18:58:30.501963 22447428132992 run.py:483] Algo bellman_ford step 606 current loss 0.951012, current_train_items 19424.
I0302 18:58:30.525474 22447428132992 run.py:483] Algo bellman_ford step 607 current loss 1.396138, current_train_items 19456.
I0302 18:58:30.553837 22447428132992 run.py:483] Algo bellman_ford step 608 current loss 1.297155, current_train_items 19488.
I0302 18:58:30.585322 22447428132992 run.py:483] Algo bellman_ford step 609 current loss 1.611673, current_train_items 19520.
I0302 18:58:30.603635 22447428132992 run.py:483] Algo bellman_ford step 610 current loss 0.736032, current_train_items 19552.
I0302 18:58:30.619908 22447428132992 run.py:483] Algo bellman_ford step 611 current loss 0.896522, current_train_items 19584.
I0302 18:58:30.642969 22447428132992 run.py:483] Algo bellman_ford step 612 current loss 1.178968, current_train_items 19616.
I0302 18:58:30.670462 22447428132992 run.py:483] Algo bellman_ford step 613 current loss 1.300650, current_train_items 19648.
I0302 18:58:30.701676 22447428132992 run.py:483] Algo bellman_ford step 614 current loss 1.694243, current_train_items 19680.
I0302 18:58:30.719848 22447428132992 run.py:483] Algo bellman_ford step 615 current loss 0.631963, current_train_items 19712.
I0302 18:58:30.736391 22447428132992 run.py:483] Algo bellman_ford step 616 current loss 0.913749, current_train_items 19744.
I0302 18:58:30.758752 22447428132992 run.py:483] Algo bellman_ford step 617 current loss 1.168009, current_train_items 19776.
I0302 18:58:30.787310 22447428132992 run.py:483] Algo bellman_ford step 618 current loss 1.291718, current_train_items 19808.
I0302 18:58:30.818643 22447428132992 run.py:483] Algo bellman_ford step 619 current loss 1.408430, current_train_items 19840.
I0302 18:58:30.837136 22447428132992 run.py:483] Algo bellman_ford step 620 current loss 0.773299, current_train_items 19872.
I0302 18:58:30.853729 22447428132992 run.py:483] Algo bellman_ford step 621 current loss 1.046439, current_train_items 19904.
I0302 18:58:30.877357 22447428132992 run.py:483] Algo bellman_ford step 622 current loss 1.266130, current_train_items 19936.
I0302 18:58:30.905920 22447428132992 run.py:483] Algo bellman_ford step 623 current loss 1.402571, current_train_items 19968.
I0302 18:58:30.937000 22447428132992 run.py:483] Algo bellman_ford step 624 current loss 1.512563, current_train_items 20000.
I0302 18:58:30.955313 22447428132992 run.py:483] Algo bellman_ford step 625 current loss 0.745245, current_train_items 20032.
I0302 18:58:30.971075 22447428132992 run.py:483] Algo bellman_ford step 626 current loss 0.834183, current_train_items 20064.
I0302 18:58:30.994653 22447428132992 run.py:483] Algo bellman_ford step 627 current loss 1.364199, current_train_items 20096.
I0302 18:58:31.024224 22447428132992 run.py:483] Algo bellman_ford step 628 current loss 1.596576, current_train_items 20128.
I0302 18:58:31.056437 22447428132992 run.py:483] Algo bellman_ford step 629 current loss 1.673981, current_train_items 20160.
I0302 18:58:31.074816 22447428132992 run.py:483] Algo bellman_ford step 630 current loss 0.695299, current_train_items 20192.
I0302 18:58:31.090922 22447428132992 run.py:483] Algo bellman_ford step 631 current loss 0.911241, current_train_items 20224.
I0302 18:58:31.113563 22447428132992 run.py:483] Algo bellman_ford step 632 current loss 1.329898, current_train_items 20256.
I0302 18:58:31.142688 22447428132992 run.py:483] Algo bellman_ford step 633 current loss 1.335997, current_train_items 20288.
I0302 18:58:31.173631 22447428132992 run.py:483] Algo bellman_ford step 634 current loss 1.511660, current_train_items 20320.
I0302 18:58:31.191988 22447428132992 run.py:483] Algo bellman_ford step 635 current loss 0.636227, current_train_items 20352.
I0302 18:58:31.207858 22447428132992 run.py:483] Algo bellman_ford step 636 current loss 0.925930, current_train_items 20384.
I0302 18:58:31.230785 22447428132992 run.py:483] Algo bellman_ford step 637 current loss 1.099977, current_train_items 20416.
I0302 18:58:31.260951 22447428132992 run.py:483] Algo bellman_ford step 638 current loss 1.328597, current_train_items 20448.
I0302 18:58:31.289515 22447428132992 run.py:483] Algo bellman_ford step 639 current loss 1.536684, current_train_items 20480.
I0302 18:58:31.307944 22447428132992 run.py:483] Algo bellman_ford step 640 current loss 0.881304, current_train_items 20512.
I0302 18:58:31.323803 22447428132992 run.py:483] Algo bellman_ford step 641 current loss 1.129485, current_train_items 20544.
I0302 18:58:31.345689 22447428132992 run.py:483] Algo bellman_ford step 642 current loss 1.423845, current_train_items 20576.
I0302 18:58:31.374567 22447428132992 run.py:483] Algo bellman_ford step 643 current loss 1.340200, current_train_items 20608.
I0302 18:58:31.405868 22447428132992 run.py:483] Algo bellman_ford step 644 current loss 1.942258, current_train_items 20640.
I0302 18:58:31.424005 22447428132992 run.py:483] Algo bellman_ford step 645 current loss 0.708190, current_train_items 20672.
I0302 18:58:31.441029 22447428132992 run.py:483] Algo bellman_ford step 646 current loss 1.133813, current_train_items 20704.
I0302 18:58:31.463326 22447428132992 run.py:483] Algo bellman_ford step 647 current loss 1.297043, current_train_items 20736.
I0302 18:58:31.493060 22447428132992 run.py:483] Algo bellman_ford step 648 current loss 1.830119, current_train_items 20768.
I0302 18:58:31.525212 22447428132992 run.py:483] Algo bellman_ford step 649 current loss 1.659095, current_train_items 20800.
I0302 18:58:31.543713 22447428132992 run.py:483] Algo bellman_ford step 650 current loss 0.895437, current_train_items 20832.
I0302 18:58:31.551929 22447428132992 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.77734375, 'score': 0.77734375, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:31.552034 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.777, val scores are: bellman_ford: 0.777
I0302 18:58:31.568774 22447428132992 run.py:483] Algo bellman_ford step 651 current loss 1.018457, current_train_items 20864.
I0302 18:58:31.591857 22447428132992 run.py:483] Algo bellman_ford step 652 current loss 1.280183, current_train_items 20896.
I0302 18:58:31.619962 22447428132992 run.py:483] Algo bellman_ford step 653 current loss 1.428030, current_train_items 20928.
I0302 18:58:31.651672 22447428132992 run.py:483] Algo bellman_ford step 654 current loss 1.679427, current_train_items 20960.
I0302 18:58:31.670176 22447428132992 run.py:483] Algo bellman_ford step 655 current loss 0.672044, current_train_items 20992.
I0302 18:58:31.685657 22447428132992 run.py:483] Algo bellman_ford step 656 current loss 1.125637, current_train_items 21024.
I0302 18:58:31.709168 22447428132992 run.py:483] Algo bellman_ford step 657 current loss 1.281273, current_train_items 21056.
I0302 18:58:31.738455 22447428132992 run.py:483] Algo bellman_ford step 658 current loss 1.378929, current_train_items 21088.
I0302 18:58:31.769351 22447428132992 run.py:483] Algo bellman_ford step 659 current loss 1.496416, current_train_items 21120.
I0302 18:58:31.787913 22447428132992 run.py:483] Algo bellman_ford step 660 current loss 0.718725, current_train_items 21152.
I0302 18:58:31.803822 22447428132992 run.py:483] Algo bellman_ford step 661 current loss 0.919967, current_train_items 21184.
I0302 18:58:31.826481 22447428132992 run.py:483] Algo bellman_ford step 662 current loss 1.311924, current_train_items 21216.
I0302 18:58:31.855327 22447428132992 run.py:483] Algo bellman_ford step 663 current loss 1.439509, current_train_items 21248.
I0302 18:58:31.888984 22447428132992 run.py:483] Algo bellman_ford step 664 current loss 2.082637, current_train_items 21280.
I0302 18:58:31.907565 22447428132992 run.py:483] Algo bellman_ford step 665 current loss 0.764621, current_train_items 21312.
I0302 18:58:31.923590 22447428132992 run.py:483] Algo bellman_ford step 666 current loss 0.912856, current_train_items 21344.
I0302 18:58:31.946975 22447428132992 run.py:483] Algo bellman_ford step 667 current loss 1.478899, current_train_items 21376.
I0302 18:58:31.975795 22447428132992 run.py:483] Algo bellman_ford step 668 current loss 1.421998, current_train_items 21408.
I0302 18:58:32.006353 22447428132992 run.py:483] Algo bellman_ford step 669 current loss 1.481270, current_train_items 21440.
I0302 18:58:32.024948 22447428132992 run.py:483] Algo bellman_ford step 670 current loss 0.621227, current_train_items 21472.
I0302 18:58:32.040797 22447428132992 run.py:483] Algo bellman_ford step 671 current loss 0.950623, current_train_items 21504.
I0302 18:58:32.064041 22447428132992 run.py:483] Algo bellman_ford step 672 current loss 1.161166, current_train_items 21536.
I0302 18:58:32.092680 22447428132992 run.py:483] Algo bellman_ford step 673 current loss 1.581483, current_train_items 21568.
I0302 18:58:32.124957 22447428132992 run.py:483] Algo bellman_ford step 674 current loss 1.727067, current_train_items 21600.
I0302 18:58:32.143875 22447428132992 run.py:483] Algo bellman_ford step 675 current loss 0.692053, current_train_items 21632.
I0302 18:58:32.159468 22447428132992 run.py:483] Algo bellman_ford step 676 current loss 0.863209, current_train_items 21664.
I0302 18:58:32.181861 22447428132992 run.py:483] Algo bellman_ford step 677 current loss 1.263290, current_train_items 21696.
I0302 18:58:32.211341 22447428132992 run.py:483] Algo bellman_ford step 678 current loss 1.334265, current_train_items 21728.
I0302 18:58:32.243999 22447428132992 run.py:483] Algo bellman_ford step 679 current loss 1.534957, current_train_items 21760.
I0302 18:58:32.262261 22447428132992 run.py:483] Algo bellman_ford step 680 current loss 0.758348, current_train_items 21792.
I0302 18:58:32.278254 22447428132992 run.py:483] Algo bellman_ford step 681 current loss 1.069610, current_train_items 21824.
I0302 18:58:32.301090 22447428132992 run.py:483] Algo bellman_ford step 682 current loss 1.340953, current_train_items 21856.
I0302 18:58:32.329484 22447428132992 run.py:483] Algo bellman_ford step 683 current loss 1.297756, current_train_items 21888.
I0302 18:58:32.361684 22447428132992 run.py:483] Algo bellman_ford step 684 current loss 1.734600, current_train_items 21920.
I0302 18:58:32.380266 22447428132992 run.py:483] Algo bellman_ford step 685 current loss 0.728118, current_train_items 21952.
I0302 18:58:32.396223 22447428132992 run.py:483] Algo bellman_ford step 686 current loss 0.877846, current_train_items 21984.
I0302 18:58:32.419408 22447428132992 run.py:483] Algo bellman_ford step 687 current loss 1.268960, current_train_items 22016.
I0302 18:58:32.446729 22447428132992 run.py:483] Algo bellman_ford step 688 current loss 1.432047, current_train_items 22048.
I0302 18:58:32.479783 22447428132992 run.py:483] Algo bellman_ford step 689 current loss 1.703673, current_train_items 22080.
I0302 18:58:32.498106 22447428132992 run.py:483] Algo bellman_ford step 690 current loss 0.876587, current_train_items 22112.
I0302 18:58:32.514627 22447428132992 run.py:483] Algo bellman_ford step 691 current loss 1.149674, current_train_items 22144.
I0302 18:58:32.537901 22447428132992 run.py:483] Algo bellman_ford step 692 current loss 1.294715, current_train_items 22176.
I0302 18:58:32.566210 22447428132992 run.py:483] Algo bellman_ford step 693 current loss 1.646755, current_train_items 22208.
I0302 18:58:32.599220 22447428132992 run.py:483] Algo bellman_ford step 694 current loss 1.616115, current_train_items 22240.
I0302 18:58:32.617596 22447428132992 run.py:483] Algo bellman_ford step 695 current loss 0.686260, current_train_items 22272.
I0302 18:58:32.633407 22447428132992 run.py:483] Algo bellman_ford step 696 current loss 0.973981, current_train_items 22304.
I0302 18:58:32.654968 22447428132992 run.py:483] Algo bellman_ford step 697 current loss 1.341436, current_train_items 22336.
I0302 18:58:32.684151 22447428132992 run.py:483] Algo bellman_ford step 698 current loss 1.394248, current_train_items 22368.
I0302 18:58:32.716716 22447428132992 run.py:483] Algo bellman_ford step 699 current loss 1.589712, current_train_items 22400.
I0302 18:58:32.735355 22447428132992 run.py:483] Algo bellman_ford step 700 current loss 0.734380, current_train_items 22432.
I0302 18:58:32.743224 22447428132992 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.8388671875, 'score': 0.8388671875, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:32.743326 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.839, val scores are: bellman_ford: 0.839
I0302 18:58:32.759425 22447428132992 run.py:483] Algo bellman_ford step 701 current loss 0.852172, current_train_items 22464.
I0302 18:58:32.782728 22447428132992 run.py:483] Algo bellman_ford step 702 current loss 1.284310, current_train_items 22496.
I0302 18:58:32.810655 22447428132992 run.py:483] Algo bellman_ford step 703 current loss 1.237354, current_train_items 22528.
I0302 18:58:32.841852 22447428132992 run.py:483] Algo bellman_ford step 704 current loss 1.435208, current_train_items 22560.
I0302 18:58:32.860800 22447428132992 run.py:483] Algo bellman_ford step 705 current loss 0.815121, current_train_items 22592.
I0302 18:58:32.876545 22447428132992 run.py:483] Algo bellman_ford step 706 current loss 0.917487, current_train_items 22624.
I0302 18:58:32.899853 22447428132992 run.py:483] Algo bellman_ford step 707 current loss 1.095455, current_train_items 22656.
I0302 18:58:32.928925 22447428132992 run.py:483] Algo bellman_ford step 708 current loss 1.285831, current_train_items 22688.
I0302 18:58:32.961515 22447428132992 run.py:483] Algo bellman_ford step 709 current loss 1.631204, current_train_items 22720.
I0302 18:58:32.979942 22447428132992 run.py:483] Algo bellman_ford step 710 current loss 0.741372, current_train_items 22752.
I0302 18:58:32.996110 22447428132992 run.py:483] Algo bellman_ford step 711 current loss 0.841628, current_train_items 22784.
I0302 18:58:33.019773 22447428132992 run.py:483] Algo bellman_ford step 712 current loss 1.274216, current_train_items 22816.
I0302 18:58:33.050101 22447428132992 run.py:483] Algo bellman_ford step 713 current loss 1.425126, current_train_items 22848.
I0302 18:58:33.080261 22447428132992 run.py:483] Algo bellman_ford step 714 current loss 1.486267, current_train_items 22880.
I0302 18:58:33.098991 22447428132992 run.py:483] Algo bellman_ford step 715 current loss 0.699400, current_train_items 22912.
I0302 18:58:33.115465 22447428132992 run.py:483] Algo bellman_ford step 716 current loss 1.019777, current_train_items 22944.
I0302 18:58:33.139712 22447428132992 run.py:483] Algo bellman_ford step 717 current loss 1.283465, current_train_items 22976.
I0302 18:58:33.167779 22447428132992 run.py:483] Algo bellman_ford step 718 current loss 1.399090, current_train_items 23008.
I0302 18:58:33.199496 22447428132992 run.py:483] Algo bellman_ford step 719 current loss 2.660430, current_train_items 23040.
I0302 18:58:33.218192 22447428132992 run.py:483] Algo bellman_ford step 720 current loss 0.912665, current_train_items 23072.
I0302 18:58:33.234670 22447428132992 run.py:483] Algo bellman_ford step 721 current loss 1.038793, current_train_items 23104.
I0302 18:58:33.257469 22447428132992 run.py:483] Algo bellman_ford step 722 current loss 1.089467, current_train_items 23136.
I0302 18:58:33.286075 22447428132992 run.py:483] Algo bellman_ford step 723 current loss 1.320346, current_train_items 23168.
I0302 18:58:33.316717 22447428132992 run.py:483] Algo bellman_ford step 724 current loss 1.307662, current_train_items 23200.
I0302 18:58:33.335331 22447428132992 run.py:483] Algo bellman_ford step 725 current loss 0.710557, current_train_items 23232.
I0302 18:58:33.351834 22447428132992 run.py:483] Algo bellman_ford step 726 current loss 1.049369, current_train_items 23264.
I0302 18:58:33.375495 22447428132992 run.py:483] Algo bellman_ford step 727 current loss 1.336126, current_train_items 23296.
I0302 18:58:33.404534 22447428132992 run.py:483] Algo bellman_ford step 728 current loss 1.279582, current_train_items 23328.
I0302 18:58:33.436037 22447428132992 run.py:483] Algo bellman_ford step 729 current loss 1.654024, current_train_items 23360.
I0302 18:58:33.454606 22447428132992 run.py:483] Algo bellman_ford step 730 current loss 0.628289, current_train_items 23392.
I0302 18:58:33.470718 22447428132992 run.py:483] Algo bellman_ford step 731 current loss 0.930280, current_train_items 23424.
I0302 18:58:33.494080 22447428132992 run.py:483] Algo bellman_ford step 732 current loss 1.155655, current_train_items 23456.
I0302 18:58:33.523221 22447428132992 run.py:483] Algo bellman_ford step 733 current loss 1.274960, current_train_items 23488.
I0302 18:58:33.555579 22447428132992 run.py:483] Algo bellman_ford step 734 current loss 1.488153, current_train_items 23520.
I0302 18:58:33.574084 22447428132992 run.py:483] Algo bellman_ford step 735 current loss 0.887336, current_train_items 23552.
I0302 18:58:33.590485 22447428132992 run.py:483] Algo bellman_ford step 736 current loss 1.035062, current_train_items 23584.
I0302 18:58:33.613280 22447428132992 run.py:483] Algo bellman_ford step 737 current loss 1.092454, current_train_items 23616.
I0302 18:58:33.642486 22447428132992 run.py:483] Algo bellman_ford step 738 current loss 1.255630, current_train_items 23648.
I0302 18:58:33.672345 22447428132992 run.py:483] Algo bellman_ford step 739 current loss 1.347903, current_train_items 23680.
I0302 18:58:33.691036 22447428132992 run.py:483] Algo bellman_ford step 740 current loss 0.760199, current_train_items 23712.
I0302 18:58:33.707243 22447428132992 run.py:483] Algo bellman_ford step 741 current loss 0.948747, current_train_items 23744.
I0302 18:58:33.730126 22447428132992 run.py:483] Algo bellman_ford step 742 current loss 1.076487, current_train_items 23776.
I0302 18:58:33.758522 22447428132992 run.py:483] Algo bellman_ford step 743 current loss 1.217198, current_train_items 23808.
I0302 18:58:33.791999 22447428132992 run.py:483] Algo bellman_ford step 744 current loss 1.535335, current_train_items 23840.
I0302 18:58:33.810569 22447428132992 run.py:483] Algo bellman_ford step 745 current loss 0.804900, current_train_items 23872.
I0302 18:58:33.826563 22447428132992 run.py:483] Algo bellman_ford step 746 current loss 1.120679, current_train_items 23904.
I0302 18:58:33.849439 22447428132992 run.py:483] Algo bellman_ford step 747 current loss 1.294525, current_train_items 23936.
I0302 18:58:33.879250 22447428132992 run.py:483] Algo bellman_ford step 748 current loss 1.619347, current_train_items 23968.
I0302 18:58:33.910471 22447428132992 run.py:483] Algo bellman_ford step 749 current loss 1.552029, current_train_items 24000.
I0302 18:58:33.928718 22447428132992 run.py:483] Algo bellman_ford step 750 current loss 0.818575, current_train_items 24032.
I0302 18:58:33.936756 22447428132992 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.8271484375, 'score': 0.8271484375, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:33.936863 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.827, val scores are: bellman_ford: 0.827
I0302 18:58:33.953643 22447428132992 run.py:483] Algo bellman_ford step 751 current loss 0.934752, current_train_items 24064.
I0302 18:58:33.976447 22447428132992 run.py:483] Algo bellman_ford step 752 current loss 1.094239, current_train_items 24096.
I0302 18:58:34.005400 22447428132992 run.py:483] Algo bellman_ford step 753 current loss 1.453413, current_train_items 24128.
I0302 18:58:34.038331 22447428132992 run.py:483] Algo bellman_ford step 754 current loss 1.518452, current_train_items 24160.
I0302 18:58:34.057180 22447428132992 run.py:483] Algo bellman_ford step 755 current loss 0.676079, current_train_items 24192.
I0302 18:58:34.072807 22447428132992 run.py:483] Algo bellman_ford step 756 current loss 0.906645, current_train_items 24224.
I0302 18:58:34.097086 22447428132992 run.py:483] Algo bellman_ford step 757 current loss 1.216987, current_train_items 24256.
I0302 18:58:34.126135 22447428132992 run.py:483] Algo bellman_ford step 758 current loss 1.750151, current_train_items 24288.
I0302 18:58:34.157459 22447428132992 run.py:483] Algo bellman_ford step 759 current loss 1.865405, current_train_items 24320.
I0302 18:58:34.176424 22447428132992 run.py:483] Algo bellman_ford step 760 current loss 0.620016, current_train_items 24352.
I0302 18:58:34.192468 22447428132992 run.py:483] Algo bellman_ford step 761 current loss 0.984069, current_train_items 24384.
I0302 18:58:34.215658 22447428132992 run.py:483] Algo bellman_ford step 762 current loss 1.343127, current_train_items 24416.
I0302 18:58:34.244052 22447428132992 run.py:483] Algo bellman_ford step 763 current loss 1.284478, current_train_items 24448.
I0302 18:58:34.276090 22447428132992 run.py:483] Algo bellman_ford step 764 current loss 1.521423, current_train_items 24480.
I0302 18:58:34.294749 22447428132992 run.py:483] Algo bellman_ford step 765 current loss 0.687776, current_train_items 24512.
I0302 18:58:34.311301 22447428132992 run.py:483] Algo bellman_ford step 766 current loss 1.006330, current_train_items 24544.
I0302 18:58:34.334644 22447428132992 run.py:483] Algo bellman_ford step 767 current loss 1.277020, current_train_items 24576.
I0302 18:58:34.364001 22447428132992 run.py:483] Algo bellman_ford step 768 current loss 1.565150, current_train_items 24608.
I0302 18:58:34.396583 22447428132992 run.py:483] Algo bellman_ford step 769 current loss 1.521583, current_train_items 24640.
I0302 18:58:34.415194 22447428132992 run.py:483] Algo bellman_ford step 770 current loss 0.630271, current_train_items 24672.
I0302 18:58:34.431533 22447428132992 run.py:483] Algo bellman_ford step 771 current loss 1.018619, current_train_items 24704.
I0302 18:58:34.454038 22447428132992 run.py:483] Algo bellman_ford step 772 current loss 1.357945, current_train_items 24736.
I0302 18:58:34.482869 22447428132992 run.py:483] Algo bellman_ford step 773 current loss 1.332612, current_train_items 24768.
I0302 18:58:34.515613 22447428132992 run.py:483] Algo bellman_ford step 774 current loss 1.622912, current_train_items 24800.
I0302 18:58:34.534489 22447428132992 run.py:483] Algo bellman_ford step 775 current loss 0.712421, current_train_items 24832.
I0302 18:58:34.551248 22447428132992 run.py:483] Algo bellman_ford step 776 current loss 0.907830, current_train_items 24864.
I0302 18:58:34.574804 22447428132992 run.py:483] Algo bellman_ford step 777 current loss 1.327883, current_train_items 24896.
I0302 18:58:34.603246 22447428132992 run.py:483] Algo bellman_ford step 778 current loss 1.273391, current_train_items 24928.
I0302 18:58:34.634474 22447428132992 run.py:483] Algo bellman_ford step 779 current loss 1.359311, current_train_items 24960.
I0302 18:58:34.653140 22447428132992 run.py:483] Algo bellman_ford step 780 current loss 0.859213, current_train_items 24992.
I0302 18:58:34.669505 22447428132992 run.py:483] Algo bellman_ford step 781 current loss 1.194163, current_train_items 25024.
I0302 18:58:34.692089 22447428132992 run.py:483] Algo bellman_ford step 782 current loss 1.202503, current_train_items 25056.
I0302 18:58:34.721229 22447428132992 run.py:483] Algo bellman_ford step 783 current loss 1.330352, current_train_items 25088.
I0302 18:58:34.754035 22447428132992 run.py:483] Algo bellman_ford step 784 current loss 1.826585, current_train_items 25120.
I0302 18:58:34.772551 22447428132992 run.py:483] Algo bellman_ford step 785 current loss 0.702140, current_train_items 25152.
I0302 18:58:34.788995 22447428132992 run.py:483] Algo bellman_ford step 786 current loss 0.927519, current_train_items 25184.
I0302 18:58:34.811720 22447428132992 run.py:483] Algo bellman_ford step 787 current loss 1.384841, current_train_items 25216.
I0302 18:58:34.840903 22447428132992 run.py:483] Algo bellman_ford step 788 current loss 1.669890, current_train_items 25248.
I0302 18:58:34.870459 22447428132992 run.py:483] Algo bellman_ford step 789 current loss 1.513198, current_train_items 25280.
I0302 18:58:34.889179 22447428132992 run.py:483] Algo bellman_ford step 790 current loss 0.677267, current_train_items 25312.
I0302 18:58:34.905670 22447428132992 run.py:483] Algo bellman_ford step 791 current loss 1.127646, current_train_items 25344.
I0302 18:58:34.927538 22447428132992 run.py:483] Algo bellman_ford step 792 current loss 1.095575, current_train_items 25376.
I0302 18:58:34.956783 22447428132992 run.py:483] Algo bellman_ford step 793 current loss 1.349402, current_train_items 25408.
I0302 18:58:34.987264 22447428132992 run.py:483] Algo bellman_ford step 794 current loss 1.417534, current_train_items 25440.
I0302 18:58:35.005756 22447428132992 run.py:483] Algo bellman_ford step 795 current loss 0.643366, current_train_items 25472.
I0302 18:58:35.021471 22447428132992 run.py:483] Algo bellman_ford step 796 current loss 0.851828, current_train_items 25504.
I0302 18:58:35.044649 22447428132992 run.py:483] Algo bellman_ford step 797 current loss 1.215925, current_train_items 25536.
I0302 18:58:35.072991 22447428132992 run.py:483] Algo bellman_ford step 798 current loss 1.395641, current_train_items 25568.
I0302 18:58:35.104176 22447428132992 run.py:483] Algo bellman_ford step 799 current loss 1.511759, current_train_items 25600.
I0302 18:58:35.122939 22447428132992 run.py:483] Algo bellman_ford step 800 current loss 0.690145, current_train_items 25632.
I0302 18:58:35.130665 22447428132992 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.8134765625, 'score': 0.8134765625, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:35.130769 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.813, val scores are: bellman_ford: 0.813
I0302 18:58:35.147436 22447428132992 run.py:483] Algo bellman_ford step 801 current loss 1.188577, current_train_items 25664.
I0302 18:58:35.171535 22447428132992 run.py:483] Algo bellman_ford step 802 current loss 1.572669, current_train_items 25696.
I0302 18:58:35.201201 22447428132992 run.py:483] Algo bellman_ford step 803 current loss 1.555419, current_train_items 25728.
I0302 18:58:35.232497 22447428132992 run.py:483] Algo bellman_ford step 804 current loss 1.684635, current_train_items 25760.
I0302 18:58:35.251412 22447428132992 run.py:483] Algo bellman_ford step 805 current loss 0.739675, current_train_items 25792.
I0302 18:58:35.267070 22447428132992 run.py:483] Algo bellman_ford step 806 current loss 1.013740, current_train_items 25824.
I0302 18:58:35.290940 22447428132992 run.py:483] Algo bellman_ford step 807 current loss 1.602744, current_train_items 25856.
I0302 18:58:35.319669 22447428132992 run.py:483] Algo bellman_ford step 808 current loss 1.598172, current_train_items 25888.
I0302 18:58:35.350248 22447428132992 run.py:483] Algo bellman_ford step 809 current loss 1.681951, current_train_items 25920.
I0302 18:58:35.368685 22447428132992 run.py:483] Algo bellman_ford step 810 current loss 0.774761, current_train_items 25952.
I0302 18:58:35.385308 22447428132992 run.py:483] Algo bellman_ford step 811 current loss 1.265030, current_train_items 25984.
I0302 18:58:35.407799 22447428132992 run.py:483] Algo bellman_ford step 812 current loss 1.226241, current_train_items 26016.
I0302 18:58:35.436136 22447428132992 run.py:483] Algo bellman_ford step 813 current loss 1.197942, current_train_items 26048.
I0302 18:58:35.467365 22447428132992 run.py:483] Algo bellman_ford step 814 current loss 1.504603, current_train_items 26080.
I0302 18:58:35.485982 22447428132992 run.py:483] Algo bellman_ford step 815 current loss 0.657323, current_train_items 26112.
I0302 18:58:35.502323 22447428132992 run.py:483] Algo bellman_ford step 816 current loss 1.098649, current_train_items 26144.
I0302 18:58:35.525440 22447428132992 run.py:483] Algo bellman_ford step 817 current loss 1.166894, current_train_items 26176.
I0302 18:58:35.555213 22447428132992 run.py:483] Algo bellman_ford step 818 current loss 1.448037, current_train_items 26208.
I0302 18:58:35.588323 22447428132992 run.py:483] Algo bellman_ford step 819 current loss 1.552605, current_train_items 26240.
I0302 18:58:35.606793 22447428132992 run.py:483] Algo bellman_ford step 820 current loss 0.916963, current_train_items 26272.
I0302 18:58:35.622595 22447428132992 run.py:483] Algo bellman_ford step 821 current loss 0.981644, current_train_items 26304.
I0302 18:58:35.644947 22447428132992 run.py:483] Algo bellman_ford step 822 current loss 1.055496, current_train_items 26336.
I0302 18:58:35.673929 22447428132992 run.py:483] Algo bellman_ford step 823 current loss 1.371866, current_train_items 26368.
I0302 18:58:35.704519 22447428132992 run.py:483] Algo bellman_ford step 824 current loss 1.762322, current_train_items 26400.
I0302 18:58:35.722950 22447428132992 run.py:483] Algo bellman_ford step 825 current loss 0.783965, current_train_items 26432.
I0302 18:58:35.739376 22447428132992 run.py:483] Algo bellman_ford step 826 current loss 1.085367, current_train_items 26464.
I0302 18:58:35.763029 22447428132992 run.py:483] Algo bellman_ford step 827 current loss 1.239693, current_train_items 26496.
I0302 18:58:35.792135 22447428132992 run.py:483] Algo bellman_ford step 828 current loss 1.423908, current_train_items 26528.
I0302 18:58:35.823770 22447428132992 run.py:483] Algo bellman_ford step 829 current loss 1.383611, current_train_items 26560.
I0302 18:58:35.842031 22447428132992 run.py:483] Algo bellman_ford step 830 current loss 0.723807, current_train_items 26592.
I0302 18:58:35.858059 22447428132992 run.py:483] Algo bellman_ford step 831 current loss 1.041330, current_train_items 26624.
I0302 18:58:35.881110 22447428132992 run.py:483] Algo bellman_ford step 832 current loss 1.371249, current_train_items 26656.
I0302 18:58:35.910395 22447428132992 run.py:483] Algo bellman_ford step 833 current loss 1.314880, current_train_items 26688.
I0302 18:58:35.940702 22447428132992 run.py:483] Algo bellman_ford step 834 current loss 1.427838, current_train_items 26720.
I0302 18:58:35.959502 22447428132992 run.py:483] Algo bellman_ford step 835 current loss 0.668330, current_train_items 26752.
I0302 18:58:35.975535 22447428132992 run.py:483] Algo bellman_ford step 836 current loss 0.853184, current_train_items 26784.
I0302 18:58:35.998972 22447428132992 run.py:483] Algo bellman_ford step 837 current loss 1.419790, current_train_items 26816.
I0302 18:58:36.027886 22447428132992 run.py:483] Algo bellman_ford step 838 current loss 1.281772, current_train_items 26848.
I0302 18:58:36.060051 22447428132992 run.py:483] Algo bellman_ford step 839 current loss 1.606070, current_train_items 26880.
I0302 18:58:36.078145 22447428132992 run.py:483] Algo bellman_ford step 840 current loss 0.844672, current_train_items 26912.
I0302 18:58:36.094289 22447428132992 run.py:483] Algo bellman_ford step 841 current loss 0.970523, current_train_items 26944.
I0302 18:58:36.117588 22447428132992 run.py:483] Algo bellman_ford step 842 current loss 1.330825, current_train_items 26976.
I0302 18:58:36.145306 22447428132992 run.py:483] Algo bellman_ford step 843 current loss 1.212416, current_train_items 27008.
I0302 18:58:36.175960 22447428132992 run.py:483] Algo bellman_ford step 844 current loss 1.297311, current_train_items 27040.
I0302 18:58:36.194269 22447428132992 run.py:483] Algo bellman_ford step 845 current loss 0.705168, current_train_items 27072.
I0302 18:58:36.210634 22447428132992 run.py:483] Algo bellman_ford step 846 current loss 0.981798, current_train_items 27104.
I0302 18:58:36.234863 22447428132992 run.py:483] Algo bellman_ford step 847 current loss 1.363975, current_train_items 27136.
I0302 18:58:36.263527 22447428132992 run.py:483] Algo bellman_ford step 848 current loss 1.244399, current_train_items 27168.
I0302 18:58:36.295430 22447428132992 run.py:483] Algo bellman_ford step 849 current loss 1.627355, current_train_items 27200.
I0302 18:58:36.314057 22447428132992 run.py:483] Algo bellman_ford step 850 current loss 0.774595, current_train_items 27232.
I0302 18:58:36.322156 22447428132992 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.802734375, 'score': 0.802734375, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:36.322260 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.803, val scores are: bellman_ford: 0.803
I0302 18:58:36.338574 22447428132992 run.py:483] Algo bellman_ford step 851 current loss 1.003019, current_train_items 27264.
I0302 18:58:36.361922 22447428132992 run.py:483] Algo bellman_ford step 852 current loss 1.401356, current_train_items 27296.
I0302 18:58:36.392366 22447428132992 run.py:483] Algo bellman_ford step 853 current loss 1.565014, current_train_items 27328.
I0302 18:58:36.423482 22447428132992 run.py:483] Algo bellman_ford step 854 current loss 1.465675, current_train_items 27360.
I0302 18:58:36.442045 22447428132992 run.py:483] Algo bellman_ford step 855 current loss 0.696273, current_train_items 27392.
I0302 18:58:36.458047 22447428132992 run.py:483] Algo bellman_ford step 856 current loss 0.946217, current_train_items 27424.
I0302 18:58:36.480201 22447428132992 run.py:483] Algo bellman_ford step 857 current loss 1.192932, current_train_items 27456.
I0302 18:58:36.510161 22447428132992 run.py:483] Algo bellman_ford step 858 current loss 1.477473, current_train_items 27488.
I0302 18:58:36.540786 22447428132992 run.py:483] Algo bellman_ford step 859 current loss 1.557590, current_train_items 27520.
I0302 18:58:36.559822 22447428132992 run.py:483] Algo bellman_ford step 860 current loss 0.635372, current_train_items 27552.
I0302 18:58:36.576190 22447428132992 run.py:483] Algo bellman_ford step 861 current loss 1.039739, current_train_items 27584.
I0302 18:58:36.598890 22447428132992 run.py:483] Algo bellman_ford step 862 current loss 1.228402, current_train_items 27616.
I0302 18:58:36.627680 22447428132992 run.py:483] Algo bellman_ford step 863 current loss 1.414653, current_train_items 27648.
I0302 18:58:36.658469 22447428132992 run.py:483] Algo bellman_ford step 864 current loss 1.532247, current_train_items 27680.
I0302 18:58:36.676602 22447428132992 run.py:483] Algo bellman_ford step 865 current loss 0.704008, current_train_items 27712.
I0302 18:58:36.692438 22447428132992 run.py:483] Algo bellman_ford step 866 current loss 0.938615, current_train_items 27744.
I0302 18:58:36.716276 22447428132992 run.py:483] Algo bellman_ford step 867 current loss 1.383512, current_train_items 27776.
I0302 18:58:36.745036 22447428132992 run.py:483] Algo bellman_ford step 868 current loss 1.243936, current_train_items 27808.
I0302 18:58:36.778035 22447428132992 run.py:483] Algo bellman_ford step 869 current loss 1.407624, current_train_items 27840.
I0302 18:58:36.796570 22447428132992 run.py:483] Algo bellman_ford step 870 current loss 0.868890, current_train_items 27872.
I0302 18:58:36.812959 22447428132992 run.py:483] Algo bellman_ford step 871 current loss 1.137956, current_train_items 27904.
I0302 18:58:36.834932 22447428132992 run.py:483] Algo bellman_ford step 872 current loss 1.120293, current_train_items 27936.
I0302 18:58:36.864387 22447428132992 run.py:483] Algo bellman_ford step 873 current loss 1.503338, current_train_items 27968.
I0302 18:58:36.895370 22447428132992 run.py:483] Algo bellman_ford step 874 current loss 1.431894, current_train_items 28000.
I0302 18:58:36.914005 22447428132992 run.py:483] Algo bellman_ford step 875 current loss 0.672833, current_train_items 28032.
I0302 18:58:36.930374 22447428132992 run.py:483] Algo bellman_ford step 876 current loss 0.953342, current_train_items 28064.
I0302 18:58:36.954769 22447428132992 run.py:483] Algo bellman_ford step 877 current loss 1.470164, current_train_items 28096.
I0302 18:58:36.982887 22447428132992 run.py:483] Algo bellman_ford step 878 current loss 1.436426, current_train_items 28128.
I0302 18:58:37.015108 22447428132992 run.py:483] Algo bellman_ford step 879 current loss 1.649373, current_train_items 28160.
I0302 18:58:37.033367 22447428132992 run.py:483] Algo bellman_ford step 880 current loss 0.606188, current_train_items 28192.
I0302 18:58:37.049338 22447428132992 run.py:483] Algo bellman_ford step 881 current loss 1.012669, current_train_items 28224.
I0302 18:58:37.071352 22447428132992 run.py:483] Algo bellman_ford step 882 current loss 1.115260, current_train_items 28256.
I0302 18:58:37.100554 22447428132992 run.py:483] Algo bellman_ford step 883 current loss 1.279447, current_train_items 28288.
I0302 18:58:37.133178 22447428132992 run.py:483] Algo bellman_ford step 884 current loss 1.565875, current_train_items 28320.
I0302 18:58:37.152325 22447428132992 run.py:483] Algo bellman_ford step 885 current loss 0.625462, current_train_items 28352.
I0302 18:58:37.168274 22447428132992 run.py:483] Algo bellman_ford step 886 current loss 0.885802, current_train_items 28384.
I0302 18:58:37.191273 22447428132992 run.py:483] Algo bellman_ford step 887 current loss 1.238335, current_train_items 28416.
I0302 18:58:37.219751 22447428132992 run.py:483] Algo bellman_ford step 888 current loss 1.257656, current_train_items 28448.
I0302 18:58:37.252295 22447428132992 run.py:483] Algo bellman_ford step 889 current loss 1.395521, current_train_items 28480.
I0302 18:58:37.271031 22447428132992 run.py:483] Algo bellman_ford step 890 current loss 0.860384, current_train_items 28512.
I0302 18:58:37.287308 22447428132992 run.py:483] Algo bellman_ford step 891 current loss 0.892495, current_train_items 28544.
I0302 18:58:37.310227 22447428132992 run.py:483] Algo bellman_ford step 892 current loss 1.330915, current_train_items 28576.
I0302 18:58:37.340412 22447428132992 run.py:483] Algo bellman_ford step 893 current loss 1.332811, current_train_items 28608.
I0302 18:58:37.371375 22447428132992 run.py:483] Algo bellman_ford step 894 current loss 1.372955, current_train_items 28640.
I0302 18:58:37.389710 22447428132992 run.py:483] Algo bellman_ford step 895 current loss 0.790641, current_train_items 28672.
I0302 18:58:37.405573 22447428132992 run.py:483] Algo bellman_ford step 896 current loss 0.977123, current_train_items 28704.
I0302 18:58:37.428807 22447428132992 run.py:483] Algo bellman_ford step 897 current loss 1.233435, current_train_items 28736.
I0302 18:58:37.458304 22447428132992 run.py:483] Algo bellman_ford step 898 current loss 1.362584, current_train_items 28768.
I0302 18:58:37.491598 22447428132992 run.py:483] Algo bellman_ford step 899 current loss 2.019167, current_train_items 28800.
I0302 18:58:37.510477 22447428132992 run.py:483] Algo bellman_ford step 900 current loss 0.688830, current_train_items 28832.
I0302 18:58:37.518351 22447428132992 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.7919921875, 'score': 0.7919921875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:37.518465 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.792, val scores are: bellman_ford: 0.792
I0302 18:58:37.535360 22447428132992 run.py:483] Algo bellman_ford step 901 current loss 0.915761, current_train_items 28864.
I0302 18:58:37.559242 22447428132992 run.py:483] Algo bellman_ford step 902 current loss 1.360194, current_train_items 28896.
I0302 18:58:37.589775 22447428132992 run.py:483] Algo bellman_ford step 903 current loss 1.370359, current_train_items 28928.
I0302 18:58:37.622071 22447428132992 run.py:483] Algo bellman_ford step 904 current loss 1.467408, current_train_items 28960.
I0302 18:58:37.640922 22447428132992 run.py:483] Algo bellman_ford step 905 current loss 0.737063, current_train_items 28992.
I0302 18:58:37.657191 22447428132992 run.py:483] Algo bellman_ford step 906 current loss 1.111423, current_train_items 29024.
I0302 18:58:37.680433 22447428132992 run.py:483] Algo bellman_ford step 907 current loss 1.258883, current_train_items 29056.
I0302 18:58:37.710485 22447428132992 run.py:483] Algo bellman_ford step 908 current loss 1.335328, current_train_items 29088.
I0302 18:58:37.742635 22447428132992 run.py:483] Algo bellman_ford step 909 current loss 1.476493, current_train_items 29120.
I0302 18:58:37.761163 22447428132992 run.py:483] Algo bellman_ford step 910 current loss 0.648433, current_train_items 29152.
I0302 18:58:37.777983 22447428132992 run.py:483] Algo bellman_ford step 911 current loss 1.065595, current_train_items 29184.
I0302 18:58:37.801963 22447428132992 run.py:483] Algo bellman_ford step 912 current loss 1.402110, current_train_items 29216.
I0302 18:58:37.829767 22447428132992 run.py:483] Algo bellman_ford step 913 current loss 1.333643, current_train_items 29248.
I0302 18:58:37.862210 22447428132992 run.py:483] Algo bellman_ford step 914 current loss 1.345375, current_train_items 29280.
I0302 18:58:37.880717 22447428132992 run.py:483] Algo bellman_ford step 915 current loss 0.657794, current_train_items 29312.
I0302 18:58:37.897053 22447428132992 run.py:483] Algo bellman_ford step 916 current loss 0.958060, current_train_items 29344.
I0302 18:58:37.920887 22447428132992 run.py:483] Algo bellman_ford step 917 current loss 1.202780, current_train_items 29376.
I0302 18:58:37.950266 22447428132992 run.py:483] Algo bellman_ford step 918 current loss 1.431581, current_train_items 29408.
I0302 18:58:37.983266 22447428132992 run.py:483] Algo bellman_ford step 919 current loss 1.537835, current_train_items 29440.
I0302 18:58:38.001931 22447428132992 run.py:483] Algo bellman_ford step 920 current loss 0.659649, current_train_items 29472.
I0302 18:58:38.018628 22447428132992 run.py:483] Algo bellman_ford step 921 current loss 0.900074, current_train_items 29504.
I0302 18:58:38.042300 22447428132992 run.py:483] Algo bellman_ford step 922 current loss 1.153791, current_train_items 29536.
I0302 18:58:38.071316 22447428132992 run.py:483] Algo bellman_ford step 923 current loss 1.172521, current_train_items 29568.
I0302 18:58:38.102792 22447428132992 run.py:483] Algo bellman_ford step 924 current loss 1.437170, current_train_items 29600.
I0302 18:58:38.121422 22447428132992 run.py:483] Algo bellman_ford step 925 current loss 0.628824, current_train_items 29632.
I0302 18:58:38.137315 22447428132992 run.py:483] Algo bellman_ford step 926 current loss 0.790162, current_train_items 29664.
I0302 18:58:38.160620 22447428132992 run.py:483] Algo bellman_ford step 927 current loss 1.215213, current_train_items 29696.
I0302 18:58:38.190871 22447428132992 run.py:483] Algo bellman_ford step 928 current loss 1.484636, current_train_items 29728.
I0302 18:58:38.221350 22447428132992 run.py:483] Algo bellman_ford step 929 current loss 1.604006, current_train_items 29760.
I0302 18:58:38.240034 22447428132992 run.py:483] Algo bellman_ford step 930 current loss 0.639778, current_train_items 29792.
I0302 18:58:38.256207 22447428132992 run.py:483] Algo bellman_ford step 931 current loss 1.026225, current_train_items 29824.
I0302 18:58:38.279479 22447428132992 run.py:483] Algo bellman_ford step 932 current loss 1.209074, current_train_items 29856.
I0302 18:58:38.307886 22447428132992 run.py:483] Algo bellman_ford step 933 current loss 1.212232, current_train_items 29888.
I0302 18:58:38.340532 22447428132992 run.py:483] Algo bellman_ford step 934 current loss 1.424355, current_train_items 29920.
I0302 18:58:38.358890 22447428132992 run.py:483] Algo bellman_ford step 935 current loss 0.688387, current_train_items 29952.
I0302 18:58:38.375072 22447428132992 run.py:483] Algo bellman_ford step 936 current loss 0.949948, current_train_items 29984.
I0302 18:58:38.397854 22447428132992 run.py:483] Algo bellman_ford step 937 current loss 1.127659, current_train_items 30016.
I0302 18:58:38.426456 22447428132992 run.py:483] Algo bellman_ford step 938 current loss 1.305940, current_train_items 30048.
I0302 18:58:38.458427 22447428132992 run.py:483] Algo bellman_ford step 939 current loss 1.758448, current_train_items 30080.
I0302 18:58:38.477051 22447428132992 run.py:483] Algo bellman_ford step 940 current loss 0.592704, current_train_items 30112.
I0302 18:58:38.493307 22447428132992 run.py:483] Algo bellman_ford step 941 current loss 1.022327, current_train_items 30144.
I0302 18:58:38.517143 22447428132992 run.py:483] Algo bellman_ford step 942 current loss 1.188474, current_train_items 30176.
I0302 18:58:38.546854 22447428132992 run.py:483] Algo bellman_ford step 943 current loss 1.258568, current_train_items 30208.
I0302 18:58:38.579337 22447428132992 run.py:483] Algo bellman_ford step 944 current loss 1.706907, current_train_items 30240.
I0302 18:58:38.597755 22447428132992 run.py:483] Algo bellman_ford step 945 current loss 0.648057, current_train_items 30272.
I0302 18:58:38.613437 22447428132992 run.py:483] Algo bellman_ford step 946 current loss 0.912264, current_train_items 30304.
I0302 18:58:38.637881 22447428132992 run.py:483] Algo bellman_ford step 947 current loss 1.319051, current_train_items 30336.
I0302 18:58:38.666673 22447428132992 run.py:483] Algo bellman_ford step 948 current loss 1.316845, current_train_items 30368.
I0302 18:58:38.698316 22447428132992 run.py:483] Algo bellman_ford step 949 current loss 1.764357, current_train_items 30400.
I0302 18:58:38.716931 22447428132992 run.py:483] Algo bellman_ford step 950 current loss 0.692254, current_train_items 30432.
I0302 18:58:38.724640 22447428132992 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.7705078125, 'score': 0.7705078125, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:38.724745 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.771, val scores are: bellman_ford: 0.771
I0302 18:58:38.741324 22447428132992 run.py:483] Algo bellman_ford step 951 current loss 0.976841, current_train_items 30464.
I0302 18:58:38.764580 22447428132992 run.py:483] Algo bellman_ford step 952 current loss 1.239408, current_train_items 30496.
I0302 18:58:38.793969 22447428132992 run.py:483] Algo bellman_ford step 953 current loss 1.573681, current_train_items 30528.
I0302 18:58:38.823682 22447428132992 run.py:483] Algo bellman_ford step 954 current loss 1.566536, current_train_items 30560.
I0302 18:58:38.842267 22447428132992 run.py:483] Algo bellman_ford step 955 current loss 0.674918, current_train_items 30592.
I0302 18:58:38.858222 22447428132992 run.py:483] Algo bellman_ford step 956 current loss 1.142355, current_train_items 30624.
I0302 18:58:38.881548 22447428132992 run.py:483] Algo bellman_ford step 957 current loss 1.531026, current_train_items 30656.
I0302 18:58:38.909167 22447428132992 run.py:483] Algo bellman_ford step 958 current loss 1.304206, current_train_items 30688.
I0302 18:58:38.941502 22447428132992 run.py:483] Algo bellman_ford step 959 current loss 1.520212, current_train_items 30720.
I0302 18:58:38.960261 22447428132992 run.py:483] Algo bellman_ford step 960 current loss 0.663564, current_train_items 30752.
I0302 18:58:38.977026 22447428132992 run.py:483] Algo bellman_ford step 961 current loss 1.214517, current_train_items 30784.
I0302 18:58:38.998435 22447428132992 run.py:483] Algo bellman_ford step 962 current loss 1.271932, current_train_items 30816.
I0302 18:58:39.025761 22447428132992 run.py:483] Algo bellman_ford step 963 current loss 1.688345, current_train_items 30848.
I0302 18:58:39.057821 22447428132992 run.py:483] Algo bellman_ford step 964 current loss 2.432324, current_train_items 30880.
I0302 18:58:39.076211 22447428132992 run.py:483] Algo bellman_ford step 965 current loss 0.822216, current_train_items 30912.
I0302 18:58:39.092165 22447428132992 run.py:483] Algo bellman_ford step 966 current loss 0.846236, current_train_items 30944.
I0302 18:58:39.115517 22447428132992 run.py:483] Algo bellman_ford step 967 current loss 1.236765, current_train_items 30976.
I0302 18:58:39.145066 22447428132992 run.py:483] Algo bellman_ford step 968 current loss 1.416163, current_train_items 31008.
I0302 18:58:39.176124 22447428132992 run.py:483] Algo bellman_ford step 969 current loss 1.654027, current_train_items 31040.
I0302 18:58:39.194604 22447428132992 run.py:483] Algo bellman_ford step 970 current loss 0.702826, current_train_items 31072.
I0302 18:58:39.210973 22447428132992 run.py:483] Algo bellman_ford step 971 current loss 1.028632, current_train_items 31104.
I0302 18:58:39.234085 22447428132992 run.py:483] Algo bellman_ford step 972 current loss 1.352845, current_train_items 31136.
I0302 18:58:39.263781 22447428132992 run.py:483] Algo bellman_ford step 973 current loss 1.334698, current_train_items 31168.
I0302 18:58:39.296856 22447428132992 run.py:483] Algo bellman_ford step 974 current loss 1.864112, current_train_items 31200.
I0302 18:58:39.315517 22447428132992 run.py:483] Algo bellman_ford step 975 current loss 0.716750, current_train_items 31232.
I0302 18:58:39.331501 22447428132992 run.py:483] Algo bellman_ford step 976 current loss 0.785353, current_train_items 31264.
I0302 18:58:39.354006 22447428132992 run.py:483] Algo bellman_ford step 977 current loss 1.213346, current_train_items 31296.
I0302 18:58:39.382525 22447428132992 run.py:483] Algo bellman_ford step 978 current loss 1.274667, current_train_items 31328.
I0302 18:58:39.413545 22447428132992 run.py:483] Algo bellman_ford step 979 current loss 1.316770, current_train_items 31360.
I0302 18:58:39.431547 22447428132992 run.py:483] Algo bellman_ford step 980 current loss 0.755267, current_train_items 31392.
I0302 18:58:39.447745 22447428132992 run.py:483] Algo bellman_ford step 981 current loss 1.177998, current_train_items 31424.
I0302 18:58:39.470394 22447428132992 run.py:483] Algo bellman_ford step 982 current loss 1.169943, current_train_items 31456.
I0302 18:58:39.498771 22447428132992 run.py:483] Algo bellman_ford step 983 current loss 1.390923, current_train_items 31488.
I0302 18:58:39.530779 22447428132992 run.py:483] Algo bellman_ford step 984 current loss 1.516723, current_train_items 31520.
I0302 18:58:39.549732 22447428132992 run.py:483] Algo bellman_ford step 985 current loss 0.718806, current_train_items 31552.
I0302 18:58:39.566350 22447428132992 run.py:483] Algo bellman_ford step 986 current loss 0.978776, current_train_items 31584.
I0302 18:58:39.589050 22447428132992 run.py:483] Algo bellman_ford step 987 current loss 1.217385, current_train_items 31616.
I0302 18:58:39.617893 22447428132992 run.py:483] Algo bellman_ford step 988 current loss 1.364702, current_train_items 31648.
I0302 18:58:39.648617 22447428132992 run.py:483] Algo bellman_ford step 989 current loss 1.380585, current_train_items 31680.
I0302 18:58:39.667114 22447428132992 run.py:483] Algo bellman_ford step 990 current loss 0.684112, current_train_items 31712.
I0302 18:58:39.683147 22447428132992 run.py:483] Algo bellman_ford step 991 current loss 0.923813, current_train_items 31744.
I0302 18:58:39.706096 22447428132992 run.py:483] Algo bellman_ford step 992 current loss 1.320170, current_train_items 31776.
I0302 18:58:39.733575 22447428132992 run.py:483] Algo bellman_ford step 993 current loss 1.252831, current_train_items 31808.
I0302 18:58:39.767339 22447428132992 run.py:483] Algo bellman_ford step 994 current loss 1.607419, current_train_items 31840.
I0302 18:58:39.785767 22447428132992 run.py:483] Algo bellman_ford step 995 current loss 0.703761, current_train_items 31872.
I0302 18:58:39.801781 22447428132992 run.py:483] Algo bellman_ford step 996 current loss 0.947622, current_train_items 31904.
I0302 18:58:39.824553 22447428132992 run.py:483] Algo bellman_ford step 997 current loss 1.225741, current_train_items 31936.
I0302 18:58:39.854039 22447428132992 run.py:483] Algo bellman_ford step 998 current loss 1.586791, current_train_items 31968.
I0302 18:58:39.884994 22447428132992 run.py:483] Algo bellman_ford step 999 current loss 1.607489, current_train_items 32000.
I0302 18:58:39.903624 22447428132992 run.py:483] Algo bellman_ford step 1000 current loss 0.688120, current_train_items 32032.
I0302 18:58:39.911575 22447428132992 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.8369140625, 'score': 0.8369140625, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:39.911678 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.837, val scores are: bellman_ford: 0.837
I0302 18:58:39.928402 22447428132992 run.py:483] Algo bellman_ford step 1001 current loss 0.856555, current_train_items 32064.
I0302 18:58:39.952213 22447428132992 run.py:483] Algo bellman_ford step 1002 current loss 1.049256, current_train_items 32096.
I0302 18:58:39.979893 22447428132992 run.py:483] Algo bellman_ford step 1003 current loss 1.426476, current_train_items 32128.
I0302 18:58:40.014710 22447428132992 run.py:483] Algo bellman_ford step 1004 current loss 1.760872, current_train_items 32160.
I0302 18:58:40.033496 22447428132992 run.py:483] Algo bellman_ford step 1005 current loss 0.735258, current_train_items 32192.
I0302 18:58:40.049072 22447428132992 run.py:483] Algo bellman_ford step 1006 current loss 0.952093, current_train_items 32224.
I0302 18:58:40.073104 22447428132992 run.py:483] Algo bellman_ford step 1007 current loss 1.159797, current_train_items 32256.
I0302 18:58:40.101571 22447428132992 run.py:483] Algo bellman_ford step 1008 current loss 1.294583, current_train_items 32288.
I0302 18:58:40.134071 22447428132992 run.py:483] Algo bellman_ford step 1009 current loss 1.591678, current_train_items 32320.
I0302 18:58:40.152558 22447428132992 run.py:483] Algo bellman_ford step 1010 current loss 0.767668, current_train_items 32352.
I0302 18:58:40.168774 22447428132992 run.py:483] Algo bellman_ford step 1011 current loss 0.932424, current_train_items 32384.
I0302 18:58:40.192755 22447428132992 run.py:483] Algo bellman_ford step 1012 current loss 1.209279, current_train_items 32416.
I0302 18:58:40.222140 22447428132992 run.py:483] Algo bellman_ford step 1013 current loss 1.371516, current_train_items 32448.
I0302 18:58:40.254394 22447428132992 run.py:483] Algo bellman_ford step 1014 current loss 1.317648, current_train_items 32480.
I0302 18:58:40.272739 22447428132992 run.py:483] Algo bellman_ford step 1015 current loss 0.806262, current_train_items 32512.
I0302 18:58:40.289135 22447428132992 run.py:483] Algo bellman_ford step 1016 current loss 0.942638, current_train_items 32544.
I0302 18:58:40.312413 22447428132992 run.py:483] Algo bellman_ford step 1017 current loss 1.178355, current_train_items 32576.
I0302 18:58:40.341851 22447428132992 run.py:483] Algo bellman_ford step 1018 current loss 1.333859, current_train_items 32608.
I0302 18:58:40.376212 22447428132992 run.py:483] Algo bellman_ford step 1019 current loss 1.915705, current_train_items 32640.
I0302 18:58:40.394331 22447428132992 run.py:483] Algo bellman_ford step 1020 current loss 0.665682, current_train_items 32672.
I0302 18:58:40.410403 22447428132992 run.py:483] Algo bellman_ford step 1021 current loss 0.955444, current_train_items 32704.
I0302 18:58:40.433086 22447428132992 run.py:483] Algo bellman_ford step 1022 current loss 1.063305, current_train_items 32736.
I0302 18:58:40.461367 22447428132992 run.py:483] Algo bellman_ford step 1023 current loss 1.213476, current_train_items 32768.
I0302 18:58:40.491250 22447428132992 run.py:483] Algo bellman_ford step 1024 current loss 1.300893, current_train_items 32800.
I0302 18:58:40.509703 22447428132992 run.py:483] Algo bellman_ford step 1025 current loss 0.746950, current_train_items 32832.
I0302 18:58:40.526480 22447428132992 run.py:483] Algo bellman_ford step 1026 current loss 1.064389, current_train_items 32864.
I0302 18:58:40.549595 22447428132992 run.py:483] Algo bellman_ford step 1027 current loss 1.226884, current_train_items 32896.
I0302 18:58:40.579262 22447428132992 run.py:483] Algo bellman_ford step 1028 current loss 1.337889, current_train_items 32928.
I0302 18:58:40.610421 22447428132992 run.py:483] Algo bellman_ford step 1029 current loss 1.660321, current_train_items 32960.
I0302 18:58:40.628746 22447428132992 run.py:483] Algo bellman_ford step 1030 current loss 0.745846, current_train_items 32992.
I0302 18:58:40.644521 22447428132992 run.py:483] Algo bellman_ford step 1031 current loss 0.767970, current_train_items 33024.
I0302 18:58:40.666853 22447428132992 run.py:483] Algo bellman_ford step 1032 current loss 1.059617, current_train_items 33056.
I0302 18:58:40.695008 22447428132992 run.py:483] Algo bellman_ford step 1033 current loss 1.256178, current_train_items 33088.
I0302 18:58:40.727483 22447428132992 run.py:483] Algo bellman_ford step 1034 current loss 1.430205, current_train_items 33120.
I0302 18:58:40.745649 22447428132992 run.py:483] Algo bellman_ford step 1035 current loss 0.690899, current_train_items 33152.
I0302 18:58:40.761777 22447428132992 run.py:483] Algo bellman_ford step 1036 current loss 0.886015, current_train_items 33184.
I0302 18:58:40.785477 22447428132992 run.py:483] Algo bellman_ford step 1037 current loss 1.094262, current_train_items 33216.
I0302 18:58:40.815346 22447428132992 run.py:483] Algo bellman_ford step 1038 current loss 1.400826, current_train_items 33248.
I0302 18:58:40.848533 22447428132992 run.py:483] Algo bellman_ford step 1039 current loss 1.917060, current_train_items 33280.
I0302 18:58:40.866984 22447428132992 run.py:483] Algo bellman_ford step 1040 current loss 0.760235, current_train_items 33312.
I0302 18:58:40.883799 22447428132992 run.py:483] Algo bellman_ford step 1041 current loss 1.021217, current_train_items 33344.
I0302 18:58:40.907230 22447428132992 run.py:483] Algo bellman_ford step 1042 current loss 1.189939, current_train_items 33376.
I0302 18:58:40.936140 22447428132992 run.py:483] Algo bellman_ford step 1043 current loss 1.176079, current_train_items 33408.
I0302 18:58:40.967530 22447428132992 run.py:483] Algo bellman_ford step 1044 current loss 1.354560, current_train_items 33440.
I0302 18:58:40.985993 22447428132992 run.py:483] Algo bellman_ford step 1045 current loss 0.728267, current_train_items 33472.
I0302 18:58:41.001804 22447428132992 run.py:483] Algo bellman_ford step 1046 current loss 0.770412, current_train_items 33504.
I0302 18:58:41.025150 22447428132992 run.py:483] Algo bellman_ford step 1047 current loss 1.227862, current_train_items 33536.
I0302 18:58:41.055219 22447428132992 run.py:483] Algo bellman_ford step 1048 current loss 1.482323, current_train_items 33568.
I0302 18:58:41.087487 22447428132992 run.py:483] Algo bellman_ford step 1049 current loss 1.788264, current_train_items 33600.
I0302 18:58:41.106282 22447428132992 run.py:483] Algo bellman_ford step 1050 current loss 0.720099, current_train_items 33632.
I0302 18:58:41.114481 22447428132992 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.830078125, 'score': 0.830078125, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:41.114584 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.840, current avg val score is 0.830, val scores are: bellman_ford: 0.830
I0302 18:58:41.130850 22447428132992 run.py:483] Algo bellman_ford step 1051 current loss 0.865186, current_train_items 33664.
I0302 18:58:41.155087 22447428132992 run.py:483] Algo bellman_ford step 1052 current loss 1.325731, current_train_items 33696.
I0302 18:58:41.183746 22447428132992 run.py:483] Algo bellman_ford step 1053 current loss 1.277109, current_train_items 33728.
I0302 18:58:41.216283 22447428132992 run.py:483] Algo bellman_ford step 1054 current loss 1.476373, current_train_items 33760.
I0302 18:58:41.235367 22447428132992 run.py:483] Algo bellman_ford step 1055 current loss 0.691022, current_train_items 33792.
I0302 18:58:41.251502 22447428132992 run.py:483] Algo bellman_ford step 1056 current loss 1.008749, current_train_items 33824.
I0302 18:58:41.275247 22447428132992 run.py:483] Algo bellman_ford step 1057 current loss 1.238429, current_train_items 33856.
I0302 18:58:41.305172 22447428132992 run.py:483] Algo bellman_ford step 1058 current loss 1.298460, current_train_items 33888.
I0302 18:58:41.334803 22447428132992 run.py:483] Algo bellman_ford step 1059 current loss 1.392366, current_train_items 33920.
I0302 18:58:41.353560 22447428132992 run.py:483] Algo bellman_ford step 1060 current loss 0.652032, current_train_items 33952.
I0302 18:58:41.369742 22447428132992 run.py:483] Algo bellman_ford step 1061 current loss 1.012733, current_train_items 33984.
I0302 18:58:41.391167 22447428132992 run.py:483] Algo bellman_ford step 1062 current loss 0.969944, current_train_items 34016.
I0302 18:58:41.419713 22447428132992 run.py:483] Algo bellman_ford step 1063 current loss 1.299620, current_train_items 34048.
I0302 18:58:41.452312 22447428132992 run.py:483] Algo bellman_ford step 1064 current loss 1.596128, current_train_items 34080.
I0302 18:58:41.470851 22447428132992 run.py:483] Algo bellman_ford step 1065 current loss 0.736758, current_train_items 34112.
I0302 18:58:41.486910 22447428132992 run.py:483] Algo bellman_ford step 1066 current loss 0.938258, current_train_items 34144.
I0302 18:58:41.510607 22447428132992 run.py:483] Algo bellman_ford step 1067 current loss 1.181203, current_train_items 34176.
I0302 18:58:41.539840 22447428132992 run.py:483] Algo bellman_ford step 1068 current loss 1.202205, current_train_items 34208.
I0302 18:58:41.572973 22447428132992 run.py:483] Algo bellman_ford step 1069 current loss 1.712853, current_train_items 34240.
I0302 18:58:41.591517 22447428132992 run.py:483] Algo bellman_ford step 1070 current loss 0.579807, current_train_items 34272.
I0302 18:58:41.608035 22447428132992 run.py:483] Algo bellman_ford step 1071 current loss 1.018502, current_train_items 34304.
I0302 18:58:41.631021 22447428132992 run.py:483] Algo bellman_ford step 1072 current loss 1.274367, current_train_items 34336.
I0302 18:58:41.660406 22447428132992 run.py:483] Algo bellman_ford step 1073 current loss 1.223314, current_train_items 34368.
I0302 18:58:41.691321 22447428132992 run.py:483] Algo bellman_ford step 1074 current loss 1.326632, current_train_items 34400.
I0302 18:58:41.709801 22447428132992 run.py:483] Algo bellman_ford step 1075 current loss 0.670728, current_train_items 34432.
I0302 18:58:41.726186 22447428132992 run.py:483] Algo bellman_ford step 1076 current loss 1.126639, current_train_items 34464.
I0302 18:58:41.749469 22447428132992 run.py:483] Algo bellman_ford step 1077 current loss 1.386679, current_train_items 34496.
I0302 18:58:41.779181 22447428132992 run.py:483] Algo bellman_ford step 1078 current loss 1.493697, current_train_items 34528.
I0302 18:58:41.810365 22447428132992 run.py:483] Algo bellman_ford step 1079 current loss 1.487952, current_train_items 34560.
I0302 18:58:41.828730 22447428132992 run.py:483] Algo bellman_ford step 1080 current loss 0.648831, current_train_items 34592.
I0302 18:58:41.845251 22447428132992 run.py:483] Algo bellman_ford step 1081 current loss 0.978639, current_train_items 34624.
I0302 18:58:41.867828 22447428132992 run.py:483] Algo bellman_ford step 1082 current loss 1.305449, current_train_items 34656.
I0302 18:58:41.896451 22447428132992 run.py:483] Algo bellman_ford step 1083 current loss 1.403661, current_train_items 34688.
I0302 18:58:41.928887 22447428132992 run.py:483] Algo bellman_ford step 1084 current loss 1.774778, current_train_items 34720.
I0302 18:58:41.947882 22447428132992 run.py:483] Algo bellman_ford step 1085 current loss 0.676475, current_train_items 34752.
I0302 18:58:41.964173 22447428132992 run.py:483] Algo bellman_ford step 1086 current loss 0.960718, current_train_items 34784.
I0302 18:58:41.987550 22447428132992 run.py:483] Algo bellman_ford step 1087 current loss 1.230507, current_train_items 34816.
I0302 18:58:42.015304 22447428132992 run.py:483] Algo bellman_ford step 1088 current loss 1.096695, current_train_items 34848.
I0302 18:58:42.045559 22447428132992 run.py:483] Algo bellman_ford step 1089 current loss 1.326007, current_train_items 34880.
I0302 18:58:42.064455 22447428132992 run.py:483] Algo bellman_ford step 1090 current loss 0.802526, current_train_items 34912.
I0302 18:58:42.081034 22447428132992 run.py:483] Algo bellman_ford step 1091 current loss 0.900079, current_train_items 34944.
I0302 18:58:42.103418 22447428132992 run.py:483] Algo bellman_ford step 1092 current loss 1.187949, current_train_items 34976.
I0302 18:58:42.133234 22447428132992 run.py:483] Algo bellman_ford step 1093 current loss 1.625002, current_train_items 35008.
I0302 18:58:42.165937 22447428132992 run.py:483] Algo bellman_ford step 1094 current loss 2.040514, current_train_items 35040.
I0302 18:58:42.184272 22447428132992 run.py:483] Algo bellman_ford step 1095 current loss 0.594652, current_train_items 35072.
I0302 18:58:42.200303 22447428132992 run.py:483] Algo bellman_ford step 1096 current loss 0.965669, current_train_items 35104.
I0302 18:58:42.223630 22447428132992 run.py:483] Algo bellman_ford step 1097 current loss 1.172110, current_train_items 35136.
I0302 18:58:42.253067 22447428132992 run.py:483] Algo bellman_ford step 1098 current loss 1.279268, current_train_items 35168.
I0302 18:58:42.284147 22447428132992 run.py:483] Algo bellman_ford step 1099 current loss 1.352840, current_train_items 35200.
I0302 18:58:42.302855 22447428132992 run.py:483] Algo bellman_ford step 1100 current loss 0.677716, current_train_items 35232.
I0302 18:58:42.310727 22447428132992 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:42.310832 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.840, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:58:42.341047 22447428132992 run.py:483] Algo bellman_ford step 1101 current loss 0.999283, current_train_items 35264.
I0302 18:58:42.364053 22447428132992 run.py:483] Algo bellman_ford step 1102 current loss 1.056350, current_train_items 35296.
I0302 18:58:42.394426 22447428132992 run.py:483] Algo bellman_ford step 1103 current loss 1.266145, current_train_items 35328.
I0302 18:58:42.428956 22447428132992 run.py:483] Algo bellman_ford step 1104 current loss 1.537892, current_train_items 35360.
I0302 18:58:42.447954 22447428132992 run.py:483] Algo bellman_ford step 1105 current loss 0.626567, current_train_items 35392.
I0302 18:58:42.463679 22447428132992 run.py:483] Algo bellman_ford step 1106 current loss 0.942059, current_train_items 35424.
I0302 18:58:42.487329 22447428132992 run.py:483] Algo bellman_ford step 1107 current loss 1.135106, current_train_items 35456.
I0302 18:58:42.515614 22447428132992 run.py:483] Algo bellman_ford step 1108 current loss 1.366174, current_train_items 35488.
I0302 18:58:42.545678 22447428132992 run.py:483] Algo bellman_ford step 1109 current loss 1.233137, current_train_items 35520.
I0302 18:58:42.564409 22447428132992 run.py:483] Algo bellman_ford step 1110 current loss 0.769138, current_train_items 35552.
I0302 18:58:42.580780 22447428132992 run.py:483] Algo bellman_ford step 1111 current loss 0.969968, current_train_items 35584.
I0302 18:58:42.603885 22447428132992 run.py:483] Algo bellman_ford step 1112 current loss 1.196016, current_train_items 35616.
I0302 18:58:42.631829 22447428132992 run.py:483] Algo bellman_ford step 1113 current loss 1.218577, current_train_items 35648.
I0302 18:58:42.664666 22447428132992 run.py:483] Algo bellman_ford step 1114 current loss 1.652183, current_train_items 35680.
I0302 18:58:42.683091 22447428132992 run.py:483] Algo bellman_ford step 1115 current loss 0.616416, current_train_items 35712.
I0302 18:58:42.698946 22447428132992 run.py:483] Algo bellman_ford step 1116 current loss 0.858985, current_train_items 35744.
I0302 18:58:42.722606 22447428132992 run.py:483] Algo bellman_ford step 1117 current loss 1.254765, current_train_items 35776.
I0302 18:58:42.752340 22447428132992 run.py:483] Algo bellman_ford step 1118 current loss 1.415667, current_train_items 35808.
I0302 18:58:42.784870 22447428132992 run.py:483] Algo bellman_ford step 1119 current loss 1.810508, current_train_items 35840.
I0302 18:58:42.803673 22447428132992 run.py:483] Algo bellman_ford step 1120 current loss 0.715217, current_train_items 35872.
I0302 18:58:42.819331 22447428132992 run.py:483] Algo bellman_ford step 1121 current loss 0.712474, current_train_items 35904.
I0302 18:58:42.843148 22447428132992 run.py:483] Algo bellman_ford step 1122 current loss 1.100274, current_train_items 35936.
I0302 18:58:42.871049 22447428132992 run.py:483] Algo bellman_ford step 1123 current loss 1.226465, current_train_items 35968.
I0302 18:58:42.904413 22447428132992 run.py:483] Algo bellman_ford step 1124 current loss 1.410573, current_train_items 36000.
I0302 18:58:42.922753 22447428132992 run.py:483] Algo bellman_ford step 1125 current loss 0.665283, current_train_items 36032.
I0302 18:58:42.938638 22447428132992 run.py:483] Algo bellman_ford step 1126 current loss 0.906332, current_train_items 36064.
I0302 18:58:42.961929 22447428132992 run.py:483] Algo bellman_ford step 1127 current loss 1.200312, current_train_items 36096.
I0302 18:58:42.991153 22447428132992 run.py:483] Algo bellman_ford step 1128 current loss 1.279019, current_train_items 36128.
I0302 18:58:43.020943 22447428132992 run.py:483] Algo bellman_ford step 1129 current loss 1.440564, current_train_items 36160.
I0302 18:58:43.039767 22447428132992 run.py:483] Algo bellman_ford step 1130 current loss 0.756200, current_train_items 36192.
I0302 18:58:43.055958 22447428132992 run.py:483] Algo bellman_ford step 1131 current loss 0.864453, current_train_items 36224.
I0302 18:58:43.079461 22447428132992 run.py:483] Algo bellman_ford step 1132 current loss 1.114379, current_train_items 36256.
I0302 18:58:43.108724 22447428132992 run.py:483] Algo bellman_ford step 1133 current loss 1.153093, current_train_items 36288.
I0302 18:58:43.139462 22447428132992 run.py:483] Algo bellman_ford step 1134 current loss 1.341666, current_train_items 36320.
I0302 18:58:43.157976 22447428132992 run.py:483] Algo bellman_ford step 1135 current loss 0.981872, current_train_items 36352.
I0302 18:58:43.174118 22447428132992 run.py:483] Algo bellman_ford step 1136 current loss 0.908129, current_train_items 36384.
I0302 18:58:43.196798 22447428132992 run.py:483] Algo bellman_ford step 1137 current loss 0.980396, current_train_items 36416.
I0302 18:58:43.226954 22447428132992 run.py:483] Algo bellman_ford step 1138 current loss 1.247140, current_train_items 36448.
I0302 18:58:43.258216 22447428132992 run.py:483] Algo bellman_ford step 1139 current loss 1.285059, current_train_items 36480.
I0302 18:58:43.276704 22447428132992 run.py:483] Algo bellman_ford step 1140 current loss 0.692828, current_train_items 36512.
I0302 18:58:43.293087 22447428132992 run.py:483] Algo bellman_ford step 1141 current loss 0.871202, current_train_items 36544.
I0302 18:58:43.315415 22447428132992 run.py:483] Algo bellman_ford step 1142 current loss 1.092150, current_train_items 36576.
I0302 18:58:43.344048 22447428132992 run.py:483] Algo bellman_ford step 1143 current loss 1.199599, current_train_items 36608.
I0302 18:58:43.375539 22447428132992 run.py:483] Algo bellman_ford step 1144 current loss 1.331612, current_train_items 36640.
I0302 18:58:43.394184 22447428132992 run.py:483] Algo bellman_ford step 1145 current loss 0.930254, current_train_items 36672.
I0302 18:58:43.410537 22447428132992 run.py:483] Algo bellman_ford step 1146 current loss 1.040622, current_train_items 36704.
I0302 18:58:43.434148 22447428132992 run.py:483] Algo bellman_ford step 1147 current loss 1.298368, current_train_items 36736.
I0302 18:58:43.463308 22447428132992 run.py:483] Algo bellman_ford step 1148 current loss 1.281021, current_train_items 36768.
I0302 18:58:43.492825 22447428132992 run.py:483] Algo bellman_ford step 1149 current loss 1.233354, current_train_items 36800.
I0302 18:58:43.511641 22447428132992 run.py:483] Algo bellman_ford step 1150 current loss 0.713995, current_train_items 36832.
I0302 18:58:43.519917 22447428132992 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.828125, 'score': 0.828125, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:43.520023 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.885, current avg val score is 0.828, val scores are: bellman_ford: 0.828
I0302 18:58:43.536505 22447428132992 run.py:483] Algo bellman_ford step 1151 current loss 0.798060, current_train_items 36864.
I0302 18:58:43.560668 22447428132992 run.py:483] Algo bellman_ford step 1152 current loss 1.274045, current_train_items 36896.
I0302 18:58:43.591722 22447428132992 run.py:483] Algo bellman_ford step 1153 current loss 1.400710, current_train_items 36928.
W0302 18:58:43.612742 22447428132992 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:50.234070 22447428132992 run.py:483] Algo bellman_ford step 1154 current loss 1.628227, current_train_items 36960.
I0302 18:58:50.254727 22447428132992 run.py:483] Algo bellman_ford step 1155 current loss 0.671907, current_train_items 36992.
I0302 18:58:50.270785 22447428132992 run.py:483] Algo bellman_ford step 1156 current loss 0.946903, current_train_items 37024.
I0302 18:58:50.294225 22447428132992 run.py:483] Algo bellman_ford step 1157 current loss 1.076536, current_train_items 37056.
I0302 18:58:50.323415 22447428132992 run.py:483] Algo bellman_ford step 1158 current loss 1.354124, current_train_items 37088.
I0302 18:58:50.353727 22447428132992 run.py:483] Algo bellman_ford step 1159 current loss 1.309317, current_train_items 37120.
I0302 18:58:50.373546 22447428132992 run.py:483] Algo bellman_ford step 1160 current loss 0.715693, current_train_items 37152.
I0302 18:58:50.389981 22447428132992 run.py:483] Algo bellman_ford step 1161 current loss 0.813519, current_train_items 37184.
I0302 18:58:50.412450 22447428132992 run.py:483] Algo bellman_ford step 1162 current loss 1.118346, current_train_items 37216.
I0302 18:58:50.441272 22447428132992 run.py:483] Algo bellman_ford step 1163 current loss 315.726135, current_train_items 37248.
I0302 18:58:50.473496 22447428132992 run.py:483] Algo bellman_ford step 1164 current loss 2935070.000000, current_train_items 37280.
I0302 18:58:50.492643 22447428132992 run.py:483] Algo bellman_ford step 1165 current loss 0.985751, current_train_items 37312.
I0302 18:58:50.508800 22447428132992 run.py:483] Algo bellman_ford step 1166 current loss 5.498267, current_train_items 37344.
I0302 18:58:50.532344 22447428132992 run.py:483] Algo bellman_ford step 1167 current loss 1635.918091, current_train_items 37376.
I0302 18:58:50.561670 22447428132992 run.py:483] Algo bellman_ford step 1168 current loss 69357.593750, current_train_items 37408.
I0302 18:58:50.595174 22447428132992 run.py:483] Algo bellman_ford step 1169 current loss 1269485.000000, current_train_items 37440.
I0302 18:58:50.614696 22447428132992 run.py:483] Algo bellman_ford step 1170 current loss 0.673509, current_train_items 37472.
I0302 18:58:50.631277 22447428132992 run.py:483] Algo bellman_ford step 1171 current loss 1.177018, current_train_items 37504.
I0302 18:58:50.653964 22447428132992 run.py:483] Algo bellman_ford step 1172 current loss 1.291729, current_train_items 37536.
I0302 18:58:50.683761 22447428132992 run.py:483] Algo bellman_ford step 1173 current loss 1.342416, current_train_items 37568.
I0302 18:58:50.715911 22447428132992 run.py:483] Algo bellman_ford step 1174 current loss 1.534761, current_train_items 37600.
I0302 18:58:50.735041 22447428132992 run.py:483] Algo bellman_ford step 1175 current loss 0.657216, current_train_items 37632.
I0302 18:58:50.751376 22447428132992 run.py:483] Algo bellman_ford step 1176 current loss 0.930787, current_train_items 37664.
I0302 18:58:50.773571 22447428132992 run.py:483] Algo bellman_ford step 1177 current loss 1.127510, current_train_items 37696.
I0302 18:58:50.802395 22447428132992 run.py:483] Algo bellman_ford step 1178 current loss 1.275930, current_train_items 37728.
I0302 18:58:50.835909 22447428132992 run.py:483] Algo bellman_ford step 1179 current loss 1.451675, current_train_items 37760.
I0302 18:58:50.854739 22447428132992 run.py:483] Algo bellman_ford step 1180 current loss 0.636813, current_train_items 37792.
I0302 18:58:50.871243 22447428132992 run.py:483] Algo bellman_ford step 1181 current loss 1.003954, current_train_items 37824.
I0302 18:58:50.894259 22447428132992 run.py:483] Algo bellman_ford step 1182 current loss 1.117161, current_train_items 37856.
I0302 18:58:50.922473 22447428132992 run.py:483] Algo bellman_ford step 1183 current loss 1.387992, current_train_items 37888.
I0302 18:58:50.954535 22447428132992 run.py:483] Algo bellman_ford step 1184 current loss 1.621928, current_train_items 37920.
I0302 18:58:50.974091 22447428132992 run.py:483] Algo bellman_ford step 1185 current loss 0.799239, current_train_items 37952.
I0302 18:58:50.990023 22447428132992 run.py:483] Algo bellman_ford step 1186 current loss 0.849207, current_train_items 37984.
I0302 18:58:51.013111 22447428132992 run.py:483] Algo bellman_ford step 1187 current loss 1.145197, current_train_items 38016.
I0302 18:58:51.042064 22447428132992 run.py:483] Algo bellman_ford step 1188 current loss 1.223230, current_train_items 38048.
I0302 18:58:51.073014 22447428132992 run.py:483] Algo bellman_ford step 1189 current loss 1.422757, current_train_items 38080.
I0302 18:58:51.092313 22447428132992 run.py:483] Algo bellman_ford step 1190 current loss 0.825389, current_train_items 38112.
I0302 18:58:51.108655 22447428132992 run.py:483] Algo bellman_ford step 1191 current loss 0.862792, current_train_items 38144.
I0302 18:58:51.132569 22447428132992 run.py:483] Algo bellman_ford step 1192 current loss 1.255275, current_train_items 38176.
I0302 18:58:51.161786 22447428132992 run.py:483] Algo bellman_ford step 1193 current loss 1.384075, current_train_items 38208.
I0302 18:58:51.193832 22447428132992 run.py:483] Algo bellman_ford step 1194 current loss 1.280261, current_train_items 38240.
I0302 18:58:51.213002 22447428132992 run.py:483] Algo bellman_ford step 1195 current loss 0.676141, current_train_items 38272.
I0302 18:58:51.229161 22447428132992 run.py:483] Algo bellman_ford step 1196 current loss 0.996975, current_train_items 38304.
I0302 18:58:51.252085 22447428132992 run.py:483] Algo bellman_ford step 1197 current loss 1.227184, current_train_items 38336.
I0302 18:58:51.282497 22447428132992 run.py:483] Algo bellman_ford step 1198 current loss 1.201037, current_train_items 38368.
I0302 18:58:51.315735 22447428132992 run.py:483] Algo bellman_ford step 1199 current loss 1.627910, current_train_items 38400.
I0302 18:58:51.335179 22447428132992 run.py:483] Algo bellman_ford step 1200 current loss 0.706775, current_train_items 38432.
I0302 18:58:51.344808 22447428132992 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:51.344915 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.885, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 18:58:51.374448 22447428132992 run.py:483] Algo bellman_ford step 1201 current loss 1.008765, current_train_items 38464.
I0302 18:58:51.397460 22447428132992 run.py:483] Algo bellman_ford step 1202 current loss 1.143982, current_train_items 38496.
I0302 18:58:51.428254 22447428132992 run.py:483] Algo bellman_ford step 1203 current loss 1.272221, current_train_items 38528.
I0302 18:58:51.461576 22447428132992 run.py:483] Algo bellman_ford step 1204 current loss 1.415427, current_train_items 38560.
I0302 18:58:51.481296 22447428132992 run.py:483] Algo bellman_ford step 1205 current loss 0.738820, current_train_items 38592.
I0302 18:58:51.497492 22447428132992 run.py:483] Algo bellman_ford step 1206 current loss 0.885428, current_train_items 38624.
I0302 18:58:51.520109 22447428132992 run.py:483] Algo bellman_ford step 1207 current loss 1.321690, current_train_items 38656.
I0302 18:58:51.549959 22447428132992 run.py:483] Algo bellman_ford step 1208 current loss 1.264504, current_train_items 38688.
I0302 18:58:51.583704 22447428132992 run.py:483] Algo bellman_ford step 1209 current loss 2.127867, current_train_items 38720.
I0302 18:58:51.602991 22447428132992 run.py:483] Algo bellman_ford step 1210 current loss 0.665155, current_train_items 38752.
I0302 18:58:51.619153 22447428132992 run.py:483] Algo bellman_ford step 1211 current loss 0.906678, current_train_items 38784.
I0302 18:58:51.641761 22447428132992 run.py:483] Algo bellman_ford step 1212 current loss 1.167248, current_train_items 38816.
I0302 18:58:51.669857 22447428132992 run.py:483] Algo bellman_ford step 1213 current loss 1.221307, current_train_items 38848.
I0302 18:58:51.701803 22447428132992 run.py:483] Algo bellman_ford step 1214 current loss 1.483011, current_train_items 38880.
I0302 18:58:51.721019 22447428132992 run.py:483] Algo bellman_ford step 1215 current loss 0.666595, current_train_items 38912.
I0302 18:58:51.736905 22447428132992 run.py:483] Algo bellman_ford step 1216 current loss 0.864042, current_train_items 38944.
I0302 18:58:51.759754 22447428132992 run.py:483] Algo bellman_ford step 1217 current loss 1.121735, current_train_items 38976.
I0302 18:58:51.789922 22447428132992 run.py:483] Algo bellman_ford step 1218 current loss 1.358552, current_train_items 39008.
I0302 18:58:51.822166 22447428132992 run.py:483] Algo bellman_ford step 1219 current loss 1.427298, current_train_items 39040.
I0302 18:58:51.841444 22447428132992 run.py:483] Algo bellman_ford step 1220 current loss 0.714997, current_train_items 39072.
I0302 18:58:51.857364 22447428132992 run.py:483] Algo bellman_ford step 1221 current loss 0.912315, current_train_items 39104.
I0302 18:58:51.881627 22447428132992 run.py:483] Algo bellman_ford step 1222 current loss 1.155710, current_train_items 39136.
I0302 18:58:51.911476 22447428132992 run.py:483] Algo bellman_ford step 1223 current loss 1.261846, current_train_items 39168.
I0302 18:58:51.943060 22447428132992 run.py:483] Algo bellman_ford step 1224 current loss 1.442457, current_train_items 39200.
I0302 18:58:51.961879 22447428132992 run.py:483] Algo bellman_ford step 1225 current loss 0.584236, current_train_items 39232.
I0302 18:58:51.978115 22447428132992 run.py:483] Algo bellman_ford step 1226 current loss 0.921762, current_train_items 39264.
I0302 18:58:52.002063 22447428132992 run.py:483] Algo bellman_ford step 1227 current loss 1.170246, current_train_items 39296.
I0302 18:58:52.031020 22447428132992 run.py:483] Algo bellman_ford step 1228 current loss 1.296975, current_train_items 39328.
I0302 18:58:52.064657 22447428132992 run.py:483] Algo bellman_ford step 1229 current loss 2.229848, current_train_items 39360.
I0302 18:58:52.083857 22447428132992 run.py:483] Algo bellman_ford step 1230 current loss 0.645471, current_train_items 39392.
I0302 18:58:52.100128 22447428132992 run.py:483] Algo bellman_ford step 1231 current loss 0.924513, current_train_items 39424.
I0302 18:58:52.124027 22447428132992 run.py:483] Algo bellman_ford step 1232 current loss 1.243246, current_train_items 39456.
I0302 18:58:52.153474 22447428132992 run.py:483] Algo bellman_ford step 1233 current loss 1.281668, current_train_items 39488.
I0302 18:58:52.184121 22447428132992 run.py:483] Algo bellman_ford step 1234 current loss 1.414532, current_train_items 39520.
I0302 18:58:52.203126 22447428132992 run.py:483] Algo bellman_ford step 1235 current loss 0.811389, current_train_items 39552.
I0302 18:58:52.218848 22447428132992 run.py:483] Algo bellman_ford step 1236 current loss 1.061455, current_train_items 39584.
I0302 18:58:52.241647 22447428132992 run.py:483] Algo bellman_ford step 1237 current loss 1.217210, current_train_items 39616.
I0302 18:58:52.271083 22447428132992 run.py:483] Algo bellman_ford step 1238 current loss 1.342099, current_train_items 39648.
I0302 18:58:52.304194 22447428132992 run.py:483] Algo bellman_ford step 1239 current loss 1.345570, current_train_items 39680.
I0302 18:58:52.323376 22447428132992 run.py:483] Algo bellman_ford step 1240 current loss 0.668281, current_train_items 39712.
I0302 18:58:52.339530 22447428132992 run.py:483] Algo bellman_ford step 1241 current loss 0.998718, current_train_items 39744.
I0302 18:58:52.363239 22447428132992 run.py:483] Algo bellman_ford step 1242 current loss 1.381809, current_train_items 39776.
I0302 18:58:52.392234 22447428132992 run.py:483] Algo bellman_ford step 1243 current loss 2.358238, current_train_items 39808.
I0302 18:58:52.424536 22447428132992 run.py:483] Algo bellman_ford step 1244 current loss 3.815743, current_train_items 39840.
I0302 18:58:52.443450 22447428132992 run.py:483] Algo bellman_ford step 1245 current loss 0.662492, current_train_items 39872.
I0302 18:58:52.459489 22447428132992 run.py:483] Algo bellman_ford step 1246 current loss 0.873839, current_train_items 39904.
I0302 18:58:52.481739 22447428132992 run.py:483] Algo bellman_ford step 1247 current loss 1.058485, current_train_items 39936.
I0302 18:58:52.511592 22447428132992 run.py:483] Algo bellman_ford step 1248 current loss 1.167334, current_train_items 39968.
I0302 18:58:52.542976 22447428132992 run.py:483] Algo bellman_ford step 1249 current loss 1.448220, current_train_items 40000.
I0302 18:58:52.561957 22447428132992 run.py:483] Algo bellman_ford step 1250 current loss 0.599709, current_train_items 40032.
I0302 18:58:52.570154 22447428132992 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.8310546875, 'score': 0.8310546875, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:52.570258 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.831, val scores are: bellman_ford: 0.831
I0302 18:58:52.586899 22447428132992 run.py:483] Algo bellman_ford step 1251 current loss 0.852765, current_train_items 40064.
I0302 18:58:52.610229 22447428132992 run.py:483] Algo bellman_ford step 1252 current loss 1.249920, current_train_items 40096.
I0302 18:58:52.638012 22447428132992 run.py:483] Algo bellman_ford step 1253 current loss 1.167771, current_train_items 40128.
I0302 18:58:52.667839 22447428132992 run.py:483] Algo bellman_ford step 1254 current loss 1.406455, current_train_items 40160.
I0302 18:58:52.686954 22447428132992 run.py:483] Algo bellman_ford step 1255 current loss 0.698202, current_train_items 40192.
I0302 18:58:52.703173 22447428132992 run.py:483] Algo bellman_ford step 1256 current loss 0.916573, current_train_items 40224.
I0302 18:58:52.725966 22447428132992 run.py:483] Algo bellman_ford step 1257 current loss 1.221586, current_train_items 40256.
I0302 18:58:52.754429 22447428132992 run.py:483] Algo bellman_ford step 1258 current loss 1.307881, current_train_items 40288.
I0302 18:58:52.785729 22447428132992 run.py:483] Algo bellman_ford step 1259 current loss 1.785592, current_train_items 40320.
I0302 18:58:52.804773 22447428132992 run.py:483] Algo bellman_ford step 1260 current loss 0.731667, current_train_items 40352.
I0302 18:58:52.821593 22447428132992 run.py:483] Algo bellman_ford step 1261 current loss 1.047937, current_train_items 40384.
I0302 18:58:52.844953 22447428132992 run.py:483] Algo bellman_ford step 1262 current loss 1.277195, current_train_items 40416.
I0302 18:58:52.871348 22447428132992 run.py:483] Algo bellman_ford step 1263 current loss 1.159364, current_train_items 40448.
I0302 18:58:52.905647 22447428132992 run.py:483] Algo bellman_ford step 1264 current loss 1.397614, current_train_items 40480.
I0302 18:58:52.924541 22447428132992 run.py:483] Algo bellman_ford step 1265 current loss 0.657049, current_train_items 40512.
I0302 18:58:52.940644 22447428132992 run.py:483] Algo bellman_ford step 1266 current loss 0.925062, current_train_items 40544.
I0302 18:58:52.962742 22447428132992 run.py:483] Algo bellman_ford step 1267 current loss 1.173617, current_train_items 40576.
I0302 18:58:52.991258 22447428132992 run.py:483] Algo bellman_ford step 1268 current loss 1.262011, current_train_items 40608.
I0302 18:58:53.023702 22447428132992 run.py:483] Algo bellman_ford step 1269 current loss 1.577795, current_train_items 40640.
I0302 18:58:53.042855 22447428132992 run.py:483] Algo bellman_ford step 1270 current loss 0.706506, current_train_items 40672.
I0302 18:58:53.058766 22447428132992 run.py:483] Algo bellman_ford step 1271 current loss 0.860352, current_train_items 40704.
I0302 18:58:53.080304 22447428132992 run.py:483] Algo bellman_ford step 1272 current loss 1.001930, current_train_items 40736.
I0302 18:58:53.107973 22447428132992 run.py:483] Algo bellman_ford step 1273 current loss 1.005422, current_train_items 40768.
I0302 18:58:53.140207 22447428132992 run.py:483] Algo bellman_ford step 1274 current loss 1.353268, current_train_items 40800.
I0302 18:58:53.159203 22447428132992 run.py:483] Algo bellman_ford step 1275 current loss 0.767499, current_train_items 40832.
I0302 18:58:53.175465 22447428132992 run.py:483] Algo bellman_ford step 1276 current loss 0.996599, current_train_items 40864.
I0302 18:58:53.197005 22447428132992 run.py:483] Algo bellman_ford step 1277 current loss 1.121804, current_train_items 40896.
I0302 18:58:53.225642 22447428132992 run.py:483] Algo bellman_ford step 1278 current loss 1.155214, current_train_items 40928.
I0302 18:58:53.257177 22447428132992 run.py:483] Algo bellman_ford step 1279 current loss 1.349652, current_train_items 40960.
I0302 18:58:53.276061 22447428132992 run.py:483] Algo bellman_ford step 1280 current loss 0.613612, current_train_items 40992.
I0302 18:58:53.292077 22447428132992 run.py:483] Algo bellman_ford step 1281 current loss 0.891152, current_train_items 41024.
I0302 18:58:53.315146 22447428132992 run.py:483] Algo bellman_ford step 1282 current loss 1.177119, current_train_items 41056.
I0302 18:58:53.345210 22447428132992 run.py:483] Algo bellman_ford step 1283 current loss 1.375375, current_train_items 41088.
I0302 18:58:53.376144 22447428132992 run.py:483] Algo bellman_ford step 1284 current loss 1.481761, current_train_items 41120.
I0302 18:58:53.395660 22447428132992 run.py:483] Algo bellman_ford step 1285 current loss 0.670070, current_train_items 41152.
I0302 18:58:53.411818 22447428132992 run.py:483] Algo bellman_ford step 1286 current loss 0.859583, current_train_items 41184.
I0302 18:58:53.436038 22447428132992 run.py:483] Algo bellman_ford step 1287 current loss 1.220423, current_train_items 41216.
I0302 18:58:53.465437 22447428132992 run.py:483] Algo bellman_ford step 1288 current loss 1.284102, current_train_items 41248.
I0302 18:58:53.497376 22447428132992 run.py:483] Algo bellman_ford step 1289 current loss 1.357782, current_train_items 41280.
I0302 18:58:53.516687 22447428132992 run.py:483] Algo bellman_ford step 1290 current loss 0.751189, current_train_items 41312.
I0302 18:58:53.532980 22447428132992 run.py:483] Algo bellman_ford step 1291 current loss 1.025069, current_train_items 41344.
I0302 18:58:53.555119 22447428132992 run.py:483] Algo bellman_ford step 1292 current loss 1.040244, current_train_items 41376.
I0302 18:58:53.583901 22447428132992 run.py:483] Algo bellman_ford step 1293 current loss 1.248203, current_train_items 41408.
I0302 18:58:53.616976 22447428132992 run.py:483] Algo bellman_ford step 1294 current loss 1.361652, current_train_items 41440.
I0302 18:58:53.635709 22447428132992 run.py:483] Algo bellman_ford step 1295 current loss 0.820896, current_train_items 41472.
I0302 18:58:53.651859 22447428132992 run.py:483] Algo bellman_ford step 1296 current loss 0.932329, current_train_items 41504.
I0302 18:58:53.675159 22447428132992 run.py:483] Algo bellman_ford step 1297 current loss 1.300524, current_train_items 41536.
I0302 18:58:53.702659 22447428132992 run.py:483] Algo bellman_ford step 1298 current loss 1.069542, current_train_items 41568.
I0302 18:58:53.732321 22447428132992 run.py:483] Algo bellman_ford step 1299 current loss 1.306771, current_train_items 41600.
I0302 18:58:53.751335 22447428132992 run.py:483] Algo bellman_ford step 1300 current loss 0.615095, current_train_items 41632.
I0302 18:58:53.759162 22447428132992 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.765625, 'score': 0.765625, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:53.759264 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.766, val scores are: bellman_ford: 0.766
I0302 18:58:53.775801 22447428132992 run.py:483] Algo bellman_ford step 1301 current loss 0.948391, current_train_items 41664.
I0302 18:58:53.799820 22447428132992 run.py:483] Algo bellman_ford step 1302 current loss 1.228886, current_train_items 41696.
I0302 18:58:53.830012 22447428132992 run.py:483] Algo bellman_ford step 1303 current loss 1.425030, current_train_items 41728.
I0302 18:58:53.861499 22447428132992 run.py:483] Algo bellman_ford step 1304 current loss 1.747993, current_train_items 41760.
I0302 18:58:53.880965 22447428132992 run.py:483] Algo bellman_ford step 1305 current loss 0.637959, current_train_items 41792.
I0302 18:58:53.896866 22447428132992 run.py:483] Algo bellman_ford step 1306 current loss 0.848146, current_train_items 41824.
I0302 18:58:53.919299 22447428132992 run.py:483] Algo bellman_ford step 1307 current loss 1.043769, current_train_items 41856.
I0302 18:58:53.948212 22447428132992 run.py:483] Algo bellman_ford step 1308 current loss 1.180734, current_train_items 41888.
I0302 18:58:53.979509 22447428132992 run.py:483] Algo bellman_ford step 1309 current loss 1.312981, current_train_items 41920.
I0302 18:58:53.998749 22447428132992 run.py:483] Algo bellman_ford step 1310 current loss 0.578294, current_train_items 41952.
I0302 18:58:54.014853 22447428132992 run.py:483] Algo bellman_ford step 1311 current loss 0.877696, current_train_items 41984.
I0302 18:58:54.037361 22447428132992 run.py:483] Algo bellman_ford step 1312 current loss 1.094710, current_train_items 42016.
I0302 18:58:54.067181 22447428132992 run.py:483] Algo bellman_ford step 1313 current loss 1.354320, current_train_items 42048.
I0302 18:58:54.099435 22447428132992 run.py:483] Algo bellman_ford step 1314 current loss 1.605296, current_train_items 42080.
I0302 18:58:54.118519 22447428132992 run.py:483] Algo bellman_ford step 1315 current loss 0.581607, current_train_items 42112.
I0302 18:58:54.134489 22447428132992 run.py:483] Algo bellman_ford step 1316 current loss 0.813715, current_train_items 42144.
I0302 18:58:54.157425 22447428132992 run.py:483] Algo bellman_ford step 1317 current loss 1.092905, current_train_items 42176.
I0302 18:58:54.186660 22447428132992 run.py:483] Algo bellman_ford step 1318 current loss 1.214526, current_train_items 42208.
I0302 18:58:54.218463 22447428132992 run.py:483] Algo bellman_ford step 1319 current loss 1.286568, current_train_items 42240.
I0302 18:58:54.237218 22447428132992 run.py:483] Algo bellman_ford step 1320 current loss 0.674307, current_train_items 42272.
I0302 18:58:54.253407 22447428132992 run.py:483] Algo bellman_ford step 1321 current loss 0.831220, current_train_items 42304.
I0302 18:58:54.276893 22447428132992 run.py:483] Algo bellman_ford step 1322 current loss 1.460927, current_train_items 42336.
I0302 18:58:54.306849 22447428132992 run.py:483] Algo bellman_ford step 1323 current loss 1.410706, current_train_items 42368.
I0302 18:58:54.338222 22447428132992 run.py:483] Algo bellman_ford step 1324 current loss 1.445358, current_train_items 42400.
I0302 18:58:54.357475 22447428132992 run.py:483] Algo bellman_ford step 1325 current loss 0.760317, current_train_items 42432.
I0302 18:58:54.373460 22447428132992 run.py:483] Algo bellman_ford step 1326 current loss 0.846703, current_train_items 42464.
I0302 18:58:54.395853 22447428132992 run.py:483] Algo bellman_ford step 1327 current loss 1.055566, current_train_items 42496.
I0302 18:58:54.424909 22447428132992 run.py:483] Algo bellman_ford step 1328 current loss 1.248228, current_train_items 42528.
I0302 18:58:54.457190 22447428132992 run.py:483] Algo bellman_ford step 1329 current loss 1.405007, current_train_items 42560.
I0302 18:58:54.476269 22447428132992 run.py:483] Algo bellman_ford step 1330 current loss 0.726707, current_train_items 42592.
I0302 18:58:54.492190 22447428132992 run.py:483] Algo bellman_ford step 1331 current loss 0.833717, current_train_items 42624.
I0302 18:58:54.514446 22447428132992 run.py:483] Algo bellman_ford step 1332 current loss 1.085062, current_train_items 42656.
I0302 18:58:54.543495 22447428132992 run.py:483] Algo bellman_ford step 1333 current loss 1.246400, current_train_items 42688.
I0302 18:58:54.574622 22447428132992 run.py:483] Algo bellman_ford step 1334 current loss 1.449132, current_train_items 42720.
I0302 18:58:54.593738 22447428132992 run.py:483] Algo bellman_ford step 1335 current loss 0.657604, current_train_items 42752.
I0302 18:58:54.609587 22447428132992 run.py:483] Algo bellman_ford step 1336 current loss 0.811657, current_train_items 42784.
I0302 18:58:54.633772 22447428132992 run.py:483] Algo bellman_ford step 1337 current loss 1.149402, current_train_items 42816.
I0302 18:58:54.662926 22447428132992 run.py:483] Algo bellman_ford step 1338 current loss 1.438709, current_train_items 42848.
I0302 18:58:54.697273 22447428132992 run.py:483] Algo bellman_ford step 1339 current loss 1.379448, current_train_items 42880.
I0302 18:58:54.715955 22447428132992 run.py:483] Algo bellman_ford step 1340 current loss 0.631866, current_train_items 42912.
I0302 18:58:54.732110 22447428132992 run.py:483] Algo bellman_ford step 1341 current loss 0.985625, current_train_items 42944.
I0302 18:58:54.755323 22447428132992 run.py:483] Algo bellman_ford step 1342 current loss 1.276279, current_train_items 42976.
I0302 18:58:54.784077 22447428132992 run.py:483] Algo bellman_ford step 1343 current loss 1.275909, current_train_items 43008.
I0302 18:58:54.815931 22447428132992 run.py:483] Algo bellman_ford step 1344 current loss 1.396757, current_train_items 43040.
I0302 18:58:54.835179 22447428132992 run.py:483] Algo bellman_ford step 1345 current loss 0.625798, current_train_items 43072.
I0302 18:58:54.851250 22447428132992 run.py:483] Algo bellman_ford step 1346 current loss 0.888056, current_train_items 43104.
I0302 18:58:54.875505 22447428132992 run.py:483] Algo bellman_ford step 1347 current loss 1.248628, current_train_items 43136.
I0302 18:58:54.903415 22447428132992 run.py:483] Algo bellman_ford step 1348 current loss 1.169034, current_train_items 43168.
I0302 18:58:54.934411 22447428132992 run.py:483] Algo bellman_ford step 1349 current loss 1.481262, current_train_items 43200.
I0302 18:58:54.953609 22447428132992 run.py:483] Algo bellman_ford step 1350 current loss 0.691186, current_train_items 43232.
I0302 18:58:54.961959 22447428132992 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.8583984375, 'score': 0.8583984375, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:54.962064 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.858, val scores are: bellman_ford: 0.858
I0302 18:58:54.978904 22447428132992 run.py:483] Algo bellman_ford step 1351 current loss 0.902785, current_train_items 43264.
I0302 18:58:55.003336 22447428132992 run.py:483] Algo bellman_ford step 1352 current loss 1.114655, current_train_items 43296.
I0302 18:58:55.031207 22447428132992 run.py:483] Algo bellman_ford step 1353 current loss 1.213255, current_train_items 43328.
I0302 18:58:55.065788 22447428132992 run.py:483] Algo bellman_ford step 1354 current loss 1.426970, current_train_items 43360.
I0302 18:58:55.085609 22447428132992 run.py:483] Algo bellman_ford step 1355 current loss 0.644318, current_train_items 43392.
I0302 18:58:55.101361 22447428132992 run.py:483] Algo bellman_ford step 1356 current loss 0.856990, current_train_items 43424.
I0302 18:58:55.123749 22447428132992 run.py:483] Algo bellman_ford step 1357 current loss 1.047592, current_train_items 43456.
I0302 18:58:55.152962 22447428132992 run.py:483] Algo bellman_ford step 1358 current loss 1.226515, current_train_items 43488.
I0302 18:58:55.186261 22447428132992 run.py:483] Algo bellman_ford step 1359 current loss 1.562468, current_train_items 43520.
I0302 18:58:55.205646 22447428132992 run.py:483] Algo bellman_ford step 1360 current loss 0.605623, current_train_items 43552.
I0302 18:58:55.222499 22447428132992 run.py:483] Algo bellman_ford step 1361 current loss 0.999034, current_train_items 43584.
I0302 18:58:55.246042 22447428132992 run.py:483] Algo bellman_ford step 1362 current loss 1.149653, current_train_items 43616.
I0302 18:58:55.275328 22447428132992 run.py:483] Algo bellman_ford step 1363 current loss 1.377623, current_train_items 43648.
I0302 18:58:55.308777 22447428132992 run.py:483] Algo bellman_ford step 1364 current loss 1.393323, current_train_items 43680.
I0302 18:58:55.328189 22447428132992 run.py:483] Algo bellman_ford step 1365 current loss 0.681312, current_train_items 43712.
I0302 18:58:55.344096 22447428132992 run.py:483] Algo bellman_ford step 1366 current loss 0.815860, current_train_items 43744.
I0302 18:58:55.366836 22447428132992 run.py:483] Algo bellman_ford step 1367 current loss 1.103531, current_train_items 43776.
I0302 18:58:55.394918 22447428132992 run.py:483] Algo bellman_ford step 1368 current loss 1.188353, current_train_items 43808.
I0302 18:58:55.426901 22447428132992 run.py:483] Algo bellman_ford step 1369 current loss 1.442554, current_train_items 43840.
I0302 18:58:55.446488 22447428132992 run.py:483] Algo bellman_ford step 1370 current loss 0.666816, current_train_items 43872.
I0302 18:58:55.463038 22447428132992 run.py:483] Algo bellman_ford step 1371 current loss 0.935552, current_train_items 43904.
I0302 18:58:55.486620 22447428132992 run.py:483] Algo bellman_ford step 1372 current loss 1.068068, current_train_items 43936.
I0302 18:58:55.516441 22447428132992 run.py:483] Algo bellman_ford step 1373 current loss 1.299074, current_train_items 43968.
I0302 18:58:55.549403 22447428132992 run.py:483] Algo bellman_ford step 1374 current loss 1.553496, current_train_items 44000.
I0302 18:58:55.568852 22447428132992 run.py:483] Algo bellman_ford step 1375 current loss 0.667628, current_train_items 44032.
I0302 18:58:55.584609 22447428132992 run.py:483] Algo bellman_ford step 1376 current loss 0.861531, current_train_items 44064.
I0302 18:58:55.607200 22447428132992 run.py:483] Algo bellman_ford step 1377 current loss 1.170039, current_train_items 44096.
I0302 18:58:55.636563 22447428132992 run.py:483] Algo bellman_ford step 1378 current loss 1.300688, current_train_items 44128.
I0302 18:58:55.668726 22447428132992 run.py:483] Algo bellman_ford step 1379 current loss 1.271325, current_train_items 44160.
I0302 18:58:55.687691 22447428132992 run.py:483] Algo bellman_ford step 1380 current loss 0.697580, current_train_items 44192.
I0302 18:58:55.704121 22447428132992 run.py:483] Algo bellman_ford step 1381 current loss 0.941485, current_train_items 44224.
I0302 18:58:55.726642 22447428132992 run.py:483] Algo bellman_ford step 1382 current loss 1.058003, current_train_items 44256.
I0302 18:58:55.755815 22447428132992 run.py:483] Algo bellman_ford step 1383 current loss 1.111666, current_train_items 44288.
I0302 18:58:55.785529 22447428132992 run.py:483] Algo bellman_ford step 1384 current loss 1.393781, current_train_items 44320.
I0302 18:58:55.804727 22447428132992 run.py:483] Algo bellman_ford step 1385 current loss 0.632679, current_train_items 44352.
I0302 18:58:55.820528 22447428132992 run.py:483] Algo bellman_ford step 1386 current loss 0.879210, current_train_items 44384.
I0302 18:58:55.842046 22447428132992 run.py:483] Algo bellman_ford step 1387 current loss 0.938989, current_train_items 44416.
I0302 18:58:55.871513 22447428132992 run.py:483] Algo bellman_ford step 1388 current loss 1.238075, current_train_items 44448.
I0302 18:58:55.903279 22447428132992 run.py:483] Algo bellman_ford step 1389 current loss 1.377714, current_train_items 44480.
I0302 18:58:55.922420 22447428132992 run.py:483] Algo bellman_ford step 1390 current loss 0.594990, current_train_items 44512.
I0302 18:58:55.938286 22447428132992 run.py:483] Algo bellman_ford step 1391 current loss 0.929984, current_train_items 44544.
I0302 18:58:55.961568 22447428132992 run.py:483] Algo bellman_ford step 1392 current loss 1.118134, current_train_items 44576.
I0302 18:58:55.990349 22447428132992 run.py:483] Algo bellman_ford step 1393 current loss 1.188742, current_train_items 44608.
I0302 18:58:56.022098 22447428132992 run.py:483] Algo bellman_ford step 1394 current loss 1.696503, current_train_items 44640.
I0302 18:58:56.040734 22447428132992 run.py:483] Algo bellman_ford step 1395 current loss 0.601526, current_train_items 44672.
I0302 18:58:56.056674 22447428132992 run.py:483] Algo bellman_ford step 1396 current loss 0.835766, current_train_items 44704.
I0302 18:58:56.079591 22447428132992 run.py:483] Algo bellman_ford step 1397 current loss 1.108151, current_train_items 44736.
I0302 18:58:56.109492 22447428132992 run.py:483] Algo bellman_ford step 1398 current loss 1.277841, current_train_items 44768.
I0302 18:58:56.140345 22447428132992 run.py:483] Algo bellman_ford step 1399 current loss 1.232139, current_train_items 44800.
I0302 18:58:56.159742 22447428132992 run.py:483] Algo bellman_ford step 1400 current loss 0.832492, current_train_items 44832.
I0302 18:58:56.167251 22447428132992 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.857421875, 'score': 0.857421875, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:58:56.167357 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.857, val scores are: bellman_ford: 0.857
I0302 18:58:56.183835 22447428132992 run.py:483] Algo bellman_ford step 1401 current loss 0.983046, current_train_items 44864.
I0302 18:58:56.207583 22447428132992 run.py:483] Algo bellman_ford step 1402 current loss 1.132632, current_train_items 44896.
I0302 18:58:56.238014 22447428132992 run.py:483] Algo bellman_ford step 1403 current loss 1.288946, current_train_items 44928.
I0302 18:58:56.271001 22447428132992 run.py:483] Algo bellman_ford step 1404 current loss 1.384576, current_train_items 44960.
I0302 18:58:56.290235 22447428132992 run.py:483] Algo bellman_ford step 1405 current loss 0.747188, current_train_items 44992.
I0302 18:58:56.306421 22447428132992 run.py:483] Algo bellman_ford step 1406 current loss 0.989704, current_train_items 45024.
I0302 18:58:56.329211 22447428132992 run.py:483] Algo bellman_ford step 1407 current loss 1.029527, current_train_items 45056.
I0302 18:58:56.358838 22447428132992 run.py:483] Algo bellman_ford step 1408 current loss 1.249214, current_train_items 45088.
I0302 18:58:56.390760 22447428132992 run.py:483] Algo bellman_ford step 1409 current loss 1.367980, current_train_items 45120.
I0302 18:58:56.409592 22447428132992 run.py:483] Algo bellman_ford step 1410 current loss 0.664948, current_train_items 45152.
I0302 18:58:56.426011 22447428132992 run.py:483] Algo bellman_ford step 1411 current loss 0.856421, current_train_items 45184.
I0302 18:58:56.448875 22447428132992 run.py:483] Algo bellman_ford step 1412 current loss 1.193953, current_train_items 45216.
I0302 18:58:56.476003 22447428132992 run.py:483] Algo bellman_ford step 1413 current loss 1.067580, current_train_items 45248.
I0302 18:58:56.507069 22447428132992 run.py:483] Algo bellman_ford step 1414 current loss 1.563200, current_train_items 45280.
I0302 18:58:56.525971 22447428132992 run.py:483] Algo bellman_ford step 1415 current loss 0.612215, current_train_items 45312.
I0302 18:58:56.542527 22447428132992 run.py:483] Algo bellman_ford step 1416 current loss 1.003580, current_train_items 45344.
I0302 18:58:56.564890 22447428132992 run.py:483] Algo bellman_ford step 1417 current loss 0.953451, current_train_items 45376.
I0302 18:58:56.592958 22447428132992 run.py:483] Algo bellman_ford step 1418 current loss 1.320268, current_train_items 45408.
I0302 18:58:56.625347 22447428132992 run.py:483] Algo bellman_ford step 1419 current loss 1.203552, current_train_items 45440.
I0302 18:58:56.644452 22447428132992 run.py:483] Algo bellman_ford step 1420 current loss 0.864285, current_train_items 45472.
I0302 18:58:56.660557 22447428132992 run.py:483] Algo bellman_ford step 1421 current loss 0.968632, current_train_items 45504.
I0302 18:58:56.684056 22447428132992 run.py:483] Algo bellman_ford step 1422 current loss 1.096700, current_train_items 45536.
I0302 18:58:56.713978 22447428132992 run.py:483] Algo bellman_ford step 1423 current loss 1.202090, current_train_items 45568.
I0302 18:58:56.744895 22447428132992 run.py:483] Algo bellman_ford step 1424 current loss 1.333359, current_train_items 45600.
I0302 18:58:56.763706 22447428132992 run.py:483] Algo bellman_ford step 1425 current loss 0.670469, current_train_items 45632.
I0302 18:58:56.780269 22447428132992 run.py:483] Algo bellman_ford step 1426 current loss 0.951039, current_train_items 45664.
I0302 18:58:56.803959 22447428132992 run.py:483] Algo bellman_ford step 1427 current loss 1.109183, current_train_items 45696.
I0302 18:58:56.833202 22447428132992 run.py:483] Algo bellman_ford step 1428 current loss 1.256318, current_train_items 45728.
I0302 18:58:56.863789 22447428132992 run.py:483] Algo bellman_ford step 1429 current loss 1.258329, current_train_items 45760.
I0302 18:58:56.882509 22447428132992 run.py:483] Algo bellman_ford step 1430 current loss 0.755592, current_train_items 45792.
I0302 18:58:56.898391 22447428132992 run.py:483] Algo bellman_ford step 1431 current loss 0.982528, current_train_items 45824.
I0302 18:58:56.922627 22447428132992 run.py:483] Algo bellman_ford step 1432 current loss 1.177419, current_train_items 45856.
I0302 18:58:56.950154 22447428132992 run.py:483] Algo bellman_ford step 1433 current loss 1.189779, current_train_items 45888.
I0302 18:58:56.980941 22447428132992 run.py:483] Algo bellman_ford step 1434 current loss 1.359390, current_train_items 45920.
I0302 18:58:56.999558 22447428132992 run.py:483] Algo bellman_ford step 1435 current loss 0.546984, current_train_items 45952.
I0302 18:58:57.015638 22447428132992 run.py:483] Algo bellman_ford step 1436 current loss 0.940197, current_train_items 45984.
I0302 18:58:57.038088 22447428132992 run.py:483] Algo bellman_ford step 1437 current loss 1.071134, current_train_items 46016.
I0302 18:58:57.066479 22447428132992 run.py:483] Algo bellman_ford step 1438 current loss 1.264536, current_train_items 46048.
I0302 18:58:57.099640 22447428132992 run.py:483] Algo bellman_ford step 1439 current loss 1.647088, current_train_items 46080.
I0302 18:58:57.118452 22447428132992 run.py:483] Algo bellman_ford step 1440 current loss 0.618432, current_train_items 46112.
I0302 18:58:57.134670 22447428132992 run.py:483] Algo bellman_ford step 1441 current loss 0.817862, current_train_items 46144.
I0302 18:58:57.157206 22447428132992 run.py:483] Algo bellman_ford step 1442 current loss 0.952564, current_train_items 46176.
I0302 18:58:57.186136 22447428132992 run.py:483] Algo bellman_ford step 1443 current loss 1.212607, current_train_items 46208.
I0302 18:58:57.221885 22447428132992 run.py:483] Algo bellman_ford step 1444 current loss 1.483675, current_train_items 46240.
I0302 18:58:57.240822 22447428132992 run.py:483] Algo bellman_ford step 1445 current loss 0.772815, current_train_items 46272.
I0302 18:58:57.256463 22447428132992 run.py:483] Algo bellman_ford step 1446 current loss 0.865084, current_train_items 46304.
I0302 18:58:57.279634 22447428132992 run.py:483] Algo bellman_ford step 1447 current loss 1.096836, current_train_items 46336.
I0302 18:58:57.308362 22447428132992 run.py:483] Algo bellman_ford step 1448 current loss 1.276259, current_train_items 46368.
I0302 18:58:57.339240 22447428132992 run.py:483] Algo bellman_ford step 1449 current loss 1.712963, current_train_items 46400.
I0302 18:58:57.358118 22447428132992 run.py:483] Algo bellman_ford step 1450 current loss 0.632939, current_train_items 46432.
I0302 18:58:57.366458 22447428132992 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.7880859375, 'score': 0.7880859375, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:58:57.366562 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.788, val scores are: bellman_ford: 0.788
I0302 18:58:57.382785 22447428132992 run.py:483] Algo bellman_ford step 1451 current loss 0.832044, current_train_items 46464.
I0302 18:58:57.406038 22447428132992 run.py:483] Algo bellman_ford step 1452 current loss 1.096338, current_train_items 46496.
I0302 18:58:57.436692 22447428132992 run.py:483] Algo bellman_ford step 1453 current loss 1.336939, current_train_items 46528.
I0302 18:58:57.470762 22447428132992 run.py:483] Algo bellman_ford step 1454 current loss 1.554449, current_train_items 46560.
I0302 18:58:57.490339 22447428132992 run.py:483] Algo bellman_ford step 1455 current loss 0.683773, current_train_items 46592.
I0302 18:58:57.506248 22447428132992 run.py:483] Algo bellman_ford step 1456 current loss 0.886783, current_train_items 46624.
I0302 18:58:57.528562 22447428132992 run.py:483] Algo bellman_ford step 1457 current loss 0.958823, current_train_items 46656.
I0302 18:58:57.558498 22447428132992 run.py:483] Algo bellman_ford step 1458 current loss 1.256666, current_train_items 46688.
I0302 18:58:57.590320 22447428132992 run.py:483] Algo bellman_ford step 1459 current loss 1.280464, current_train_items 46720.
I0302 18:58:57.609726 22447428132992 run.py:483] Algo bellman_ford step 1460 current loss 0.597651, current_train_items 46752.
I0302 18:58:57.625506 22447428132992 run.py:483] Algo bellman_ford step 1461 current loss 0.923025, current_train_items 46784.
I0302 18:58:57.647641 22447428132992 run.py:483] Algo bellman_ford step 1462 current loss 0.950576, current_train_items 46816.
I0302 18:58:57.676674 22447428132992 run.py:483] Algo bellman_ford step 1463 current loss 1.166883, current_train_items 46848.
I0302 18:58:57.707739 22447428132992 run.py:483] Algo bellman_ford step 1464 current loss 1.299896, current_train_items 46880.
I0302 18:58:57.726761 22447428132992 run.py:483] Algo bellman_ford step 1465 current loss 0.720911, current_train_items 46912.
I0302 18:58:57.743151 22447428132992 run.py:483] Algo bellman_ford step 1466 current loss 0.929534, current_train_items 46944.
I0302 18:58:57.766114 22447428132992 run.py:483] Algo bellman_ford step 1467 current loss 1.021715, current_train_items 46976.
I0302 18:58:57.793495 22447428132992 run.py:483] Algo bellman_ford step 1468 current loss 1.056370, current_train_items 47008.
I0302 18:58:57.823987 22447428132992 run.py:483] Algo bellman_ford step 1469 current loss 1.392871, current_train_items 47040.
I0302 18:58:57.843359 22447428132992 run.py:483] Algo bellman_ford step 1470 current loss 0.615276, current_train_items 47072.
I0302 18:58:57.859906 22447428132992 run.py:483] Algo bellman_ford step 1471 current loss 0.884465, current_train_items 47104.
I0302 18:58:57.882307 22447428132992 run.py:483] Algo bellman_ford step 1472 current loss 1.053819, current_train_items 47136.
I0302 18:58:57.911489 22447428132992 run.py:483] Algo bellman_ford step 1473 current loss 1.290351, current_train_items 47168.
I0302 18:58:57.944161 22447428132992 run.py:483] Algo bellman_ford step 1474 current loss 1.446446, current_train_items 47200.
I0302 18:58:57.963699 22447428132992 run.py:483] Algo bellman_ford step 1475 current loss 0.744053, current_train_items 47232.
I0302 18:58:57.979694 22447428132992 run.py:483] Algo bellman_ford step 1476 current loss 0.790625, current_train_items 47264.
I0302 18:58:58.002572 22447428132992 run.py:483] Algo bellman_ford step 1477 current loss 1.004751, current_train_items 47296.
I0302 18:58:58.031561 22447428132992 run.py:483] Algo bellman_ford step 1478 current loss 1.199388, current_train_items 47328.
I0302 18:58:58.065159 22447428132992 run.py:483] Algo bellman_ford step 1479 current loss 1.354999, current_train_items 47360.
I0302 18:58:58.084072 22447428132992 run.py:483] Algo bellman_ford step 1480 current loss 0.661059, current_train_items 47392.
I0302 18:58:58.099904 22447428132992 run.py:483] Algo bellman_ford step 1481 current loss 0.832911, current_train_items 47424.
I0302 18:58:58.122198 22447428132992 run.py:483] Algo bellman_ford step 1482 current loss 1.014799, current_train_items 47456.
I0302 18:58:58.151876 22447428132992 run.py:483] Algo bellman_ford step 1483 current loss 1.228525, current_train_items 47488.
I0302 18:58:58.182124 22447428132992 run.py:483] Algo bellman_ford step 1484 current loss 1.196036, current_train_items 47520.
I0302 18:58:58.201689 22447428132992 run.py:483] Algo bellman_ford step 1485 current loss 0.814030, current_train_items 47552.
I0302 18:58:58.217430 22447428132992 run.py:483] Algo bellman_ford step 1486 current loss 0.824397, current_train_items 47584.
I0302 18:58:58.240903 22447428132992 run.py:483] Algo bellman_ford step 1487 current loss 1.088799, current_train_items 47616.
I0302 18:58:58.268435 22447428132992 run.py:483] Algo bellman_ford step 1488 current loss 1.120864, current_train_items 47648.
I0302 18:58:58.301118 22447428132992 run.py:483] Algo bellman_ford step 1489 current loss 1.451408, current_train_items 47680.
I0302 18:58:58.320167 22447428132992 run.py:483] Algo bellman_ford step 1490 current loss 0.683490, current_train_items 47712.
I0302 18:58:58.336404 22447428132992 run.py:483] Algo bellman_ford step 1491 current loss 0.817370, current_train_items 47744.
I0302 18:58:58.359365 22447428132992 run.py:483] Algo bellman_ford step 1492 current loss 1.040520, current_train_items 47776.
I0302 18:58:58.388271 22447428132992 run.py:483] Algo bellman_ford step 1493 current loss 1.099323, current_train_items 47808.
I0302 18:58:58.421451 22447428132992 run.py:483] Algo bellman_ford step 1494 current loss 1.421372, current_train_items 47840.
I0302 18:58:58.440267 22447428132992 run.py:483] Algo bellman_ford step 1495 current loss 0.585673, current_train_items 47872.
I0302 18:58:58.456740 22447428132992 run.py:483] Algo bellman_ford step 1496 current loss 0.941143, current_train_items 47904.
I0302 18:58:58.479990 22447428132992 run.py:483] Algo bellman_ford step 1497 current loss 1.088860, current_train_items 47936.
I0302 18:58:58.509974 22447428132992 run.py:483] Algo bellman_ford step 1498 current loss 1.251110, current_train_items 47968.
I0302 18:58:58.543475 22447428132992 run.py:483] Algo bellman_ford step 1499 current loss 1.328837, current_train_items 48000.
I0302 18:58:58.562644 22447428132992 run.py:483] Algo bellman_ford step 1500 current loss 0.695711, current_train_items 48032.
I0302 18:58:58.570535 22447428132992 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.8671875, 'score': 0.8671875, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:58:58.570673 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.867, val scores are: bellman_ford: 0.867
I0302 18:58:58.587487 22447428132992 run.py:483] Algo bellman_ford step 1501 current loss 0.927184, current_train_items 48064.
I0302 18:58:58.611238 22447428132992 run.py:483] Algo bellman_ford step 1502 current loss 1.244416, current_train_items 48096.
I0302 18:58:58.640506 22447428132992 run.py:483] Algo bellman_ford step 1503 current loss 1.176940, current_train_items 48128.
I0302 18:58:58.672644 22447428132992 run.py:483] Algo bellman_ford step 1504 current loss 1.273199, current_train_items 48160.
I0302 18:58:58.692045 22447428132992 run.py:483] Algo bellman_ford step 1505 current loss 0.639332, current_train_items 48192.
I0302 18:58:58.707814 22447428132992 run.py:483] Algo bellman_ford step 1506 current loss 0.794026, current_train_items 48224.
I0302 18:58:58.730299 22447428132992 run.py:483] Algo bellman_ford step 1507 current loss 1.149611, current_train_items 48256.
I0302 18:58:58.759079 22447428132992 run.py:483] Algo bellman_ford step 1508 current loss 1.360710, current_train_items 48288.
I0302 18:58:58.790853 22447428132992 run.py:483] Algo bellman_ford step 1509 current loss 1.574059, current_train_items 48320.
I0302 18:58:58.809792 22447428132992 run.py:483] Algo bellman_ford step 1510 current loss 0.636781, current_train_items 48352.
I0302 18:58:58.826129 22447428132992 run.py:483] Algo bellman_ford step 1511 current loss 0.894991, current_train_items 48384.
I0302 18:58:58.849476 22447428132992 run.py:483] Algo bellman_ford step 1512 current loss 0.997539, current_train_items 48416.
I0302 18:58:58.877001 22447428132992 run.py:483] Algo bellman_ford step 1513 current loss 1.056830, current_train_items 48448.
I0302 18:58:58.910612 22447428132992 run.py:483] Algo bellman_ford step 1514 current loss 1.374073, current_train_items 48480.
I0302 18:58:58.929893 22447428132992 run.py:483] Algo bellman_ford step 1515 current loss 0.808177, current_train_items 48512.
I0302 18:58:58.945652 22447428132992 run.py:483] Algo bellman_ford step 1516 current loss 0.876622, current_train_items 48544.
I0302 18:58:58.968465 22447428132992 run.py:483] Algo bellman_ford step 1517 current loss 1.036346, current_train_items 48576.
I0302 18:58:58.998275 22447428132992 run.py:483] Algo bellman_ford step 1518 current loss 1.181363, current_train_items 48608.
I0302 18:58:59.030016 22447428132992 run.py:483] Algo bellman_ford step 1519 current loss 1.530195, current_train_items 48640.
I0302 18:58:59.049038 22447428132992 run.py:483] Algo bellman_ford step 1520 current loss 0.655674, current_train_items 48672.
I0302 18:58:59.064830 22447428132992 run.py:483] Algo bellman_ford step 1521 current loss 0.904527, current_train_items 48704.
I0302 18:58:59.088395 22447428132992 run.py:483] Algo bellman_ford step 1522 current loss 1.080214, current_train_items 48736.
I0302 18:58:59.117593 22447428132992 run.py:483] Algo bellman_ford step 1523 current loss 1.213577, current_train_items 48768.
I0302 18:58:59.149651 22447428132992 run.py:483] Algo bellman_ford step 1524 current loss 1.556467, current_train_items 48800.
I0302 18:58:59.168684 22447428132992 run.py:483] Algo bellman_ford step 1525 current loss 0.781695, current_train_items 48832.
I0302 18:58:59.184512 22447428132992 run.py:483] Algo bellman_ford step 1526 current loss 0.832096, current_train_items 48864.
I0302 18:58:59.208562 22447428132992 run.py:483] Algo bellman_ford step 1527 current loss 1.118598, current_train_items 48896.
I0302 18:58:59.237698 22447428132992 run.py:483] Algo bellman_ford step 1528 current loss 1.085554, current_train_items 48928.
I0302 18:58:59.270281 22447428132992 run.py:483] Algo bellman_ford step 1529 current loss 1.437793, current_train_items 48960.
I0302 18:58:59.289510 22447428132992 run.py:483] Algo bellman_ford step 1530 current loss 0.687224, current_train_items 48992.
I0302 18:58:59.305373 22447428132992 run.py:483] Algo bellman_ford step 1531 current loss 0.801198, current_train_items 49024.
I0302 18:58:59.327965 22447428132992 run.py:483] Algo bellman_ford step 1532 current loss 1.066225, current_train_items 49056.
I0302 18:58:59.357451 22447428132992 run.py:483] Algo bellman_ford step 1533 current loss 1.148712, current_train_items 49088.
I0302 18:58:59.390283 22447428132992 run.py:483] Algo bellman_ford step 1534 current loss 1.592911, current_train_items 49120.
I0302 18:58:59.409677 22447428132992 run.py:483] Algo bellman_ford step 1535 current loss 0.659027, current_train_items 49152.
I0302 18:58:59.426120 22447428132992 run.py:483] Algo bellman_ford step 1536 current loss 0.848912, current_train_items 49184.
I0302 18:58:59.449467 22447428132992 run.py:483] Algo bellman_ford step 1537 current loss 1.031513, current_train_items 49216.
I0302 18:58:59.478841 22447428132992 run.py:483] Algo bellman_ford step 1538 current loss 1.174694, current_train_items 49248.
I0302 18:58:59.513862 22447428132992 run.py:483] Algo bellman_ford step 1539 current loss 1.663381, current_train_items 49280.
I0302 18:58:59.533021 22447428132992 run.py:483] Algo bellman_ford step 1540 current loss 0.648037, current_train_items 49312.
I0302 18:58:59.548782 22447428132992 run.py:483] Algo bellman_ford step 1541 current loss 0.786172, current_train_items 49344.
I0302 18:58:59.571634 22447428132992 run.py:483] Algo bellman_ford step 1542 current loss 1.082590, current_train_items 49376.
I0302 18:58:59.600071 22447428132992 run.py:483] Algo bellman_ford step 1543 current loss 1.096192, current_train_items 49408.
I0302 18:58:59.632198 22447428132992 run.py:483] Algo bellman_ford step 1544 current loss 1.354484, current_train_items 49440.
I0302 18:58:59.651338 22447428132992 run.py:483] Algo bellman_ford step 1545 current loss 0.695485, current_train_items 49472.
I0302 18:58:59.667412 22447428132992 run.py:483] Algo bellman_ford step 1546 current loss 0.924456, current_train_items 49504.
I0302 18:58:59.690615 22447428132992 run.py:483] Algo bellman_ford step 1547 current loss 1.002980, current_train_items 49536.
I0302 18:58:59.720732 22447428132992 run.py:483] Algo bellman_ford step 1548 current loss 1.360116, current_train_items 49568.
I0302 18:58:59.754406 22447428132992 run.py:483] Algo bellman_ford step 1549 current loss 1.615497, current_train_items 49600.
I0302 18:58:59.773492 22447428132992 run.py:483] Algo bellman_ford step 1550 current loss 0.713861, current_train_items 49632.
I0302 18:58:59.781543 22447428132992 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:58:59.781650 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 18:58:59.798823 22447428132992 run.py:483] Algo bellman_ford step 1551 current loss 0.865282, current_train_items 49664.
I0302 18:58:59.821530 22447428132992 run.py:483] Algo bellman_ford step 1552 current loss 1.139884, current_train_items 49696.
I0302 18:58:59.850854 22447428132992 run.py:483] Algo bellman_ford step 1553 current loss 1.185366, current_train_items 49728.
I0302 18:58:59.881622 22447428132992 run.py:483] Algo bellman_ford step 1554 current loss 1.245410, current_train_items 49760.
I0302 18:58:59.900627 22447428132992 run.py:483] Algo bellman_ford step 1555 current loss 0.679669, current_train_items 49792.
I0302 18:58:59.916300 22447428132992 run.py:483] Algo bellman_ford step 1556 current loss 0.981227, current_train_items 49824.
I0302 18:58:59.939369 22447428132992 run.py:483] Algo bellman_ford step 1557 current loss 1.315080, current_train_items 49856.
I0302 18:58:59.968658 22447428132992 run.py:483] Algo bellman_ford step 1558 current loss 1.259952, current_train_items 49888.
I0302 18:59:00.001078 22447428132992 run.py:483] Algo bellman_ford step 1559 current loss 1.357900, current_train_items 49920.
I0302 18:59:00.020259 22447428132992 run.py:483] Algo bellman_ford step 1560 current loss 0.628365, current_train_items 49952.
I0302 18:59:00.036652 22447428132992 run.py:483] Algo bellman_ford step 1561 current loss 0.898427, current_train_items 49984.
I0302 18:59:00.059268 22447428132992 run.py:483] Algo bellman_ford step 1562 current loss 1.078623, current_train_items 50016.
I0302 18:59:00.088883 22447428132992 run.py:483] Algo bellman_ford step 1563 current loss 1.539566, current_train_items 50048.
I0302 18:59:00.121715 22447428132992 run.py:483] Algo bellman_ford step 1564 current loss 2.089374, current_train_items 50080.
I0302 18:59:00.140390 22447428132992 run.py:483] Algo bellman_ford step 1565 current loss 0.595380, current_train_items 50112.
I0302 18:59:00.156609 22447428132992 run.py:483] Algo bellman_ford step 1566 current loss 0.971142, current_train_items 50144.
I0302 18:59:00.179643 22447428132992 run.py:483] Algo bellman_ford step 1567 current loss 0.930483, current_train_items 50176.
I0302 18:59:00.208308 22447428132992 run.py:483] Algo bellman_ford step 1568 current loss 1.219404, current_train_items 50208.
I0302 18:59:00.241691 22447428132992 run.py:483] Algo bellman_ford step 1569 current loss 1.391599, current_train_items 50240.
I0302 18:59:00.260689 22447428132992 run.py:483] Algo bellman_ford step 1570 current loss 0.622772, current_train_items 50272.
I0302 18:59:00.276969 22447428132992 run.py:483] Algo bellman_ford step 1571 current loss 0.889929, current_train_items 50304.
I0302 18:59:00.300430 22447428132992 run.py:483] Algo bellman_ford step 1572 current loss 1.156175, current_train_items 50336.
I0302 18:59:00.329292 22447428132992 run.py:483] Algo bellman_ford step 1573 current loss 1.140742, current_train_items 50368.
I0302 18:59:00.359203 22447428132992 run.py:483] Algo bellman_ford step 1574 current loss 1.337151, current_train_items 50400.
I0302 18:59:00.378643 22447428132992 run.py:483] Algo bellman_ford step 1575 current loss 0.731611, current_train_items 50432.
I0302 18:59:00.394786 22447428132992 run.py:483] Algo bellman_ford step 1576 current loss 0.857792, current_train_items 50464.
I0302 18:59:00.416368 22447428132992 run.py:483] Algo bellman_ford step 1577 current loss 0.999843, current_train_items 50496.
I0302 18:59:00.446338 22447428132992 run.py:483] Algo bellman_ford step 1578 current loss 1.280485, current_train_items 50528.
I0302 18:59:00.478116 22447428132992 run.py:483] Algo bellman_ford step 1579 current loss 1.437383, current_train_items 50560.
I0302 18:59:00.496843 22447428132992 run.py:483] Algo bellman_ford step 1580 current loss 0.793495, current_train_items 50592.
I0302 18:59:00.512562 22447428132992 run.py:483] Algo bellman_ford step 1581 current loss 0.962599, current_train_items 50624.
I0302 18:59:00.535032 22447428132992 run.py:483] Algo bellman_ford step 1582 current loss 0.994429, current_train_items 50656.
I0302 18:59:00.564003 22447428132992 run.py:483] Algo bellman_ford step 1583 current loss 1.287897, current_train_items 50688.
I0302 18:59:00.596748 22447428132992 run.py:483] Algo bellman_ford step 1584 current loss 1.536725, current_train_items 50720.
I0302 18:59:00.615752 22447428132992 run.py:483] Algo bellman_ford step 1585 current loss 0.717578, current_train_items 50752.
I0302 18:59:00.631423 22447428132992 run.py:483] Algo bellman_ford step 1586 current loss 0.883132, current_train_items 50784.
I0302 18:59:00.654244 22447428132992 run.py:483] Algo bellman_ford step 1587 current loss 1.004517, current_train_items 50816.
I0302 18:59:00.683041 22447428132992 run.py:483] Algo bellman_ford step 1588 current loss 1.368017, current_train_items 50848.
I0302 18:59:00.714588 22447428132992 run.py:483] Algo bellman_ford step 1589 current loss 1.497113, current_train_items 50880.
I0302 18:59:00.733659 22447428132992 run.py:483] Algo bellman_ford step 1590 current loss 0.651552, current_train_items 50912.
I0302 18:59:00.749807 22447428132992 run.py:483] Algo bellman_ford step 1591 current loss 0.777597, current_train_items 50944.
I0302 18:59:00.771733 22447428132992 run.py:483] Algo bellman_ford step 1592 current loss 1.056882, current_train_items 50976.
I0302 18:59:00.801580 22447428132992 run.py:483] Algo bellman_ford step 1593 current loss 1.306641, current_train_items 51008.
I0302 18:59:00.834627 22447428132992 run.py:483] Algo bellman_ford step 1594 current loss 1.453200, current_train_items 51040.
I0302 18:59:00.853513 22447428132992 run.py:483] Algo bellman_ford step 1595 current loss 0.732503, current_train_items 51072.
I0302 18:59:00.869180 22447428132992 run.py:483] Algo bellman_ford step 1596 current loss 0.886677, current_train_items 51104.
I0302 18:59:00.892442 22447428132992 run.py:483] Algo bellman_ford step 1597 current loss 1.149890, current_train_items 51136.
I0302 18:59:00.920394 22447428132992 run.py:483] Algo bellman_ford step 1598 current loss 1.131876, current_train_items 51168.
I0302 18:59:00.952640 22447428132992 run.py:483] Algo bellman_ford step 1599 current loss 1.335798, current_train_items 51200.
I0302 18:59:00.971689 22447428132992 run.py:483] Algo bellman_ford step 1600 current loss 0.635551, current_train_items 51232.
I0302 18:59:00.979407 22447428132992 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.865234375, 'score': 0.865234375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:59:00.979515 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.865, val scores are: bellman_ford: 0.865
I0302 18:59:00.996052 22447428132992 run.py:483] Algo bellman_ford step 1601 current loss 0.830203, current_train_items 51264.
I0302 18:59:01.019971 22447428132992 run.py:483] Algo bellman_ford step 1602 current loss 1.101153, current_train_items 51296.
I0302 18:59:01.049516 22447428132992 run.py:483] Algo bellman_ford step 1603 current loss 1.450341, current_train_items 51328.
I0302 18:59:01.082355 22447428132992 run.py:483] Algo bellman_ford step 1604 current loss 2.138565, current_train_items 51360.
I0302 18:59:01.101717 22447428132992 run.py:483] Algo bellman_ford step 1605 current loss 0.679432, current_train_items 51392.
I0302 18:59:01.117040 22447428132992 run.py:483] Algo bellman_ford step 1606 current loss 0.821943, current_train_items 51424.
I0302 18:59:01.139630 22447428132992 run.py:483] Algo bellman_ford step 1607 current loss 1.036623, current_train_items 51456.
I0302 18:59:01.169568 22447428132992 run.py:483] Algo bellman_ford step 1608 current loss 1.246424, current_train_items 51488.
I0302 18:59:01.202345 22447428132992 run.py:483] Algo bellman_ford step 1609 current loss 1.449556, current_train_items 51520.
I0302 18:59:01.221443 22447428132992 run.py:483] Algo bellman_ford step 1610 current loss 0.918368, current_train_items 51552.
I0302 18:59:01.237780 22447428132992 run.py:483] Algo bellman_ford step 1611 current loss 0.957644, current_train_items 51584.
I0302 18:59:01.260512 22447428132992 run.py:483] Algo bellman_ford step 1612 current loss 1.151726, current_train_items 51616.
I0302 18:59:01.290122 22447428132992 run.py:483] Algo bellman_ford step 1613 current loss 1.267751, current_train_items 51648.
I0302 18:59:01.321282 22447428132992 run.py:483] Algo bellman_ford step 1614 current loss 1.639692, current_train_items 51680.
I0302 18:59:01.340338 22447428132992 run.py:483] Algo bellman_ford step 1615 current loss 0.663800, current_train_items 51712.
I0302 18:59:01.356613 22447428132992 run.py:483] Algo bellman_ford step 1616 current loss 0.846803, current_train_items 51744.
I0302 18:59:01.379930 22447428132992 run.py:483] Algo bellman_ford step 1617 current loss 1.113963, current_train_items 51776.
I0302 18:59:01.408085 22447428132992 run.py:483] Algo bellman_ford step 1618 current loss 1.197438, current_train_items 51808.
I0302 18:59:01.441549 22447428132992 run.py:483] Algo bellman_ford step 1619 current loss 1.523530, current_train_items 51840.
I0302 18:59:01.460277 22447428132992 run.py:483] Algo bellman_ford step 1620 current loss 0.838035, current_train_items 51872.
I0302 18:59:01.476141 22447428132992 run.py:483] Algo bellman_ford step 1621 current loss 0.854578, current_train_items 51904.
I0302 18:59:01.499147 22447428132992 run.py:483] Algo bellman_ford step 1622 current loss 1.216215, current_train_items 51936.
I0302 18:59:01.528627 22447428132992 run.py:483] Algo bellman_ford step 1623 current loss 1.380498, current_train_items 51968.
I0302 18:59:01.560518 22447428132992 run.py:483] Algo bellman_ford step 1624 current loss 1.365676, current_train_items 52000.
I0302 18:59:01.579467 22447428132992 run.py:483] Algo bellman_ford step 1625 current loss 0.673614, current_train_items 52032.
I0302 18:59:01.595688 22447428132992 run.py:483] Algo bellman_ford step 1626 current loss 0.781922, current_train_items 52064.
I0302 18:59:01.618304 22447428132992 run.py:483] Algo bellman_ford step 1627 current loss 1.149211, current_train_items 52096.
I0302 18:59:01.648897 22447428132992 run.py:483] Algo bellman_ford step 1628 current loss 1.413327, current_train_items 52128.
I0302 18:59:01.680177 22447428132992 run.py:483] Algo bellman_ford step 1629 current loss 2.011218, current_train_items 52160.
I0302 18:59:01.699179 22447428132992 run.py:483] Algo bellman_ford step 1630 current loss 0.651484, current_train_items 52192.
I0302 18:59:01.715757 22447428132992 run.py:483] Algo bellman_ford step 1631 current loss 0.924364, current_train_items 52224.
I0302 18:59:01.739157 22447428132992 run.py:483] Algo bellman_ford step 1632 current loss 1.214304, current_train_items 52256.
I0302 18:59:01.767794 22447428132992 run.py:483] Algo bellman_ford step 1633 current loss 1.080767, current_train_items 52288.
I0302 18:59:01.800211 22447428132992 run.py:483] Algo bellman_ford step 1634 current loss 1.363908, current_train_items 52320.
I0302 18:59:01.819031 22447428132992 run.py:483] Algo bellman_ford step 1635 current loss 0.819440, current_train_items 52352.
I0302 18:59:01.835135 22447428132992 run.py:483] Algo bellman_ford step 1636 current loss 1.034996, current_train_items 52384.
I0302 18:59:01.858207 22447428132992 run.py:483] Algo bellman_ford step 1637 current loss 1.135902, current_train_items 52416.
I0302 18:59:01.887899 22447428132992 run.py:483] Algo bellman_ford step 1638 current loss 1.056560, current_train_items 52448.
I0302 18:59:01.922058 22447428132992 run.py:483] Algo bellman_ford step 1639 current loss 1.457828, current_train_items 52480.
I0302 18:59:01.940722 22447428132992 run.py:483] Algo bellman_ford step 1640 current loss 0.659833, current_train_items 52512.
I0302 18:59:01.956436 22447428132992 run.py:483] Algo bellman_ford step 1641 current loss 0.844583, current_train_items 52544.
I0302 18:59:01.979544 22447428132992 run.py:483] Algo bellman_ford step 1642 current loss 1.148583, current_train_items 52576.
I0302 18:59:02.008879 22447428132992 run.py:483] Algo bellman_ford step 1643 current loss 1.396710, current_train_items 52608.
I0302 18:59:02.040136 22447428132992 run.py:483] Algo bellman_ford step 1644 current loss 1.621146, current_train_items 52640.
I0302 18:59:02.058821 22447428132992 run.py:483] Algo bellman_ford step 1645 current loss 0.626854, current_train_items 52672.
I0302 18:59:02.074864 22447428132992 run.py:483] Algo bellman_ford step 1646 current loss 0.836098, current_train_items 52704.
I0302 18:59:02.098696 22447428132992 run.py:483] Algo bellman_ford step 1647 current loss 1.137561, current_train_items 52736.
I0302 18:59:02.126487 22447428132992 run.py:483] Algo bellman_ford step 1648 current loss 1.123409, current_train_items 52768.
I0302 18:59:02.159325 22447428132992 run.py:483] Algo bellman_ford step 1649 current loss 1.355180, current_train_items 52800.
I0302 18:59:02.178220 22447428132992 run.py:483] Algo bellman_ford step 1650 current loss 0.786707, current_train_items 52832.
I0302 18:59:02.186491 22447428132992 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:59:02.186597 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:59:02.203692 22447428132992 run.py:483] Algo bellman_ford step 1651 current loss 0.877498, current_train_items 52864.
I0302 18:59:02.227837 22447428132992 run.py:483] Algo bellman_ford step 1652 current loss 1.083750, current_train_items 52896.
I0302 18:59:02.256861 22447428132992 run.py:483] Algo bellman_ford step 1653 current loss 1.138699, current_train_items 52928.
I0302 18:59:02.288275 22447428132992 run.py:483] Algo bellman_ford step 1654 current loss 1.381953, current_train_items 52960.
I0302 18:59:02.307529 22447428132992 run.py:483] Algo bellman_ford step 1655 current loss 0.662178, current_train_items 52992.
I0302 18:59:02.323421 22447428132992 run.py:483] Algo bellman_ford step 1656 current loss 0.859836, current_train_items 53024.
I0302 18:59:02.346180 22447428132992 run.py:483] Algo bellman_ford step 1657 current loss 1.110839, current_train_items 53056.
I0302 18:59:02.374439 22447428132992 run.py:483] Algo bellman_ford step 1658 current loss 1.089920, current_train_items 53088.
I0302 18:59:02.404823 22447428132992 run.py:483] Algo bellman_ford step 1659 current loss 1.306550, current_train_items 53120.
I0302 18:59:02.423867 22447428132992 run.py:483] Algo bellman_ford step 1660 current loss 0.647230, current_train_items 53152.
I0302 18:59:02.440030 22447428132992 run.py:483] Algo bellman_ford step 1661 current loss 0.806173, current_train_items 53184.
I0302 18:59:02.462107 22447428132992 run.py:483] Algo bellman_ford step 1662 current loss 1.094226, current_train_items 53216.
I0302 18:59:02.492594 22447428132992 run.py:483] Algo bellman_ford step 1663 current loss 1.253601, current_train_items 53248.
I0302 18:59:02.525582 22447428132992 run.py:483] Algo bellman_ford step 1664 current loss 1.751715, current_train_items 53280.
I0302 18:59:02.544426 22447428132992 run.py:483] Algo bellman_ford step 1665 current loss 0.609976, current_train_items 53312.
I0302 18:59:02.560530 22447428132992 run.py:483] Algo bellman_ford step 1666 current loss 0.941270, current_train_items 53344.
I0302 18:59:02.582913 22447428132992 run.py:483] Algo bellman_ford step 1667 current loss 1.035002, current_train_items 53376.
I0302 18:59:02.610959 22447428132992 run.py:483] Algo bellman_ford step 1668 current loss 1.065417, current_train_items 53408.
I0302 18:59:02.641589 22447428132992 run.py:483] Algo bellman_ford step 1669 current loss 1.417111, current_train_items 53440.
I0302 18:59:02.661093 22447428132992 run.py:483] Algo bellman_ford step 1670 current loss 0.725752, current_train_items 53472.
I0302 18:59:02.677005 22447428132992 run.py:483] Algo bellman_ford step 1671 current loss 0.848894, current_train_items 53504.
I0302 18:59:02.699074 22447428132992 run.py:483] Algo bellman_ford step 1672 current loss 1.029506, current_train_items 53536.
I0302 18:59:02.728917 22447428132992 run.py:483] Algo bellman_ford step 1673 current loss 1.174940, current_train_items 53568.
I0302 18:59:02.761284 22447428132992 run.py:483] Algo bellman_ford step 1674 current loss 1.275331, current_train_items 53600.
I0302 18:59:02.780204 22447428132992 run.py:483] Algo bellman_ford step 1675 current loss 0.581065, current_train_items 53632.
I0302 18:59:02.796539 22447428132992 run.py:483] Algo bellman_ford step 1676 current loss 0.874423, current_train_items 53664.
I0302 18:59:02.818958 22447428132992 run.py:483] Algo bellman_ford step 1677 current loss 0.980518, current_train_items 53696.
I0302 18:59:02.847837 22447428132992 run.py:483] Algo bellman_ford step 1678 current loss 1.296510, current_train_items 53728.
I0302 18:59:02.880747 22447428132992 run.py:483] Algo bellman_ford step 1679 current loss 1.438808, current_train_items 53760.
I0302 18:59:02.899531 22447428132992 run.py:483] Algo bellman_ford step 1680 current loss 0.719734, current_train_items 53792.
I0302 18:59:02.915345 22447428132992 run.py:483] Algo bellman_ford step 1681 current loss 0.780714, current_train_items 53824.
I0302 18:59:02.937465 22447428132992 run.py:483] Algo bellman_ford step 1682 current loss 1.123996, current_train_items 53856.
I0302 18:59:02.965304 22447428132992 run.py:483] Algo bellman_ford step 1683 current loss 1.108731, current_train_items 53888.
I0302 18:59:02.997397 22447428132992 run.py:483] Algo bellman_ford step 1684 current loss 1.342595, current_train_items 53920.
I0302 18:59:03.016773 22447428132992 run.py:483] Algo bellman_ford step 1685 current loss 0.607807, current_train_items 53952.
I0302 18:59:03.032762 22447428132992 run.py:483] Algo bellman_ford step 1686 current loss 0.875552, current_train_items 53984.
I0302 18:59:03.055318 22447428132992 run.py:483] Algo bellman_ford step 1687 current loss 1.091760, current_train_items 54016.
I0302 18:59:03.084222 22447428132992 run.py:483] Algo bellman_ford step 1688 current loss 1.217346, current_train_items 54048.
I0302 18:59:03.118041 22447428132992 run.py:483] Algo bellman_ford step 1689 current loss 1.425560, current_train_items 54080.
I0302 18:59:03.136913 22447428132992 run.py:483] Algo bellman_ford step 1690 current loss 0.608751, current_train_items 54112.
I0302 18:59:03.153400 22447428132992 run.py:483] Algo bellman_ford step 1691 current loss 0.931116, current_train_items 54144.
I0302 18:59:03.176206 22447428132992 run.py:483] Algo bellman_ford step 1692 current loss 1.130601, current_train_items 54176.
I0302 18:59:03.206280 22447428132992 run.py:483] Algo bellman_ford step 1693 current loss 1.172680, current_train_items 54208.
I0302 18:59:03.238658 22447428132992 run.py:483] Algo bellman_ford step 1694 current loss 1.384158, current_train_items 54240.
I0302 18:59:03.257472 22447428132992 run.py:483] Algo bellman_ford step 1695 current loss 0.820671, current_train_items 54272.
I0302 18:59:03.273781 22447428132992 run.py:483] Algo bellman_ford step 1696 current loss 0.904851, current_train_items 54304.
I0302 18:59:03.294961 22447428132992 run.py:483] Algo bellman_ford step 1697 current loss 1.074785, current_train_items 54336.
I0302 18:59:03.322780 22447428132992 run.py:483] Algo bellman_ford step 1698 current loss 1.329616, current_train_items 54368.
I0302 18:59:03.355429 22447428132992 run.py:483] Algo bellman_ford step 1699 current loss 1.430440, current_train_items 54400.
I0302 18:59:03.374766 22447428132992 run.py:483] Algo bellman_ford step 1700 current loss 0.588509, current_train_items 54432.
I0302 18:59:03.382711 22447428132992 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.873046875, 'score': 0.873046875, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:59:03.382813 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.873, val scores are: bellman_ford: 0.873
I0302 18:59:03.399459 22447428132992 run.py:483] Algo bellman_ford step 1701 current loss 0.876293, current_train_items 54464.
I0302 18:59:03.422046 22447428132992 run.py:483] Algo bellman_ford step 1702 current loss 0.954123, current_train_items 54496.
I0302 18:59:03.451622 22447428132992 run.py:483] Algo bellman_ford step 1703 current loss 1.127762, current_train_items 54528.
I0302 18:59:03.483663 22447428132992 run.py:483] Algo bellman_ford step 1704 current loss 1.501475, current_train_items 54560.
I0302 18:59:03.503592 22447428132992 run.py:483] Algo bellman_ford step 1705 current loss 0.631604, current_train_items 54592.
I0302 18:59:03.519147 22447428132992 run.py:483] Algo bellman_ford step 1706 current loss 0.798506, current_train_items 54624.
I0302 18:59:03.541893 22447428132992 run.py:483] Algo bellman_ford step 1707 current loss 1.096862, current_train_items 54656.
I0302 18:59:03.570976 22447428132992 run.py:483] Algo bellman_ford step 1708 current loss 1.214979, current_train_items 54688.
I0302 18:59:03.604694 22447428132992 run.py:483] Algo bellman_ford step 1709 current loss 1.291111, current_train_items 54720.
I0302 18:59:03.623528 22447428132992 run.py:483] Algo bellman_ford step 1710 current loss 0.749478, current_train_items 54752.
I0302 18:59:03.639793 22447428132992 run.py:483] Algo bellman_ford step 1711 current loss 0.898528, current_train_items 54784.
I0302 18:59:03.662764 22447428132992 run.py:483] Algo bellman_ford step 1712 current loss 1.052844, current_train_items 54816.
I0302 18:59:03.691674 22447428132992 run.py:483] Algo bellman_ford step 1713 current loss 1.120180, current_train_items 54848.
I0302 18:59:03.723107 22447428132992 run.py:483] Algo bellman_ford step 1714 current loss 1.312020, current_train_items 54880.
I0302 18:59:03.741793 22447428132992 run.py:483] Algo bellman_ford step 1715 current loss 0.673373, current_train_items 54912.
I0302 18:59:03.757541 22447428132992 run.py:483] Algo bellman_ford step 1716 current loss 0.803301, current_train_items 54944.
I0302 18:59:03.780669 22447428132992 run.py:483] Algo bellman_ford step 1717 current loss 1.027165, current_train_items 54976.
I0302 18:59:03.808122 22447428132992 run.py:483] Algo bellman_ford step 1718 current loss 1.059930, current_train_items 55008.
I0302 18:59:03.840225 22447428132992 run.py:483] Algo bellman_ford step 1719 current loss 1.218948, current_train_items 55040.
I0302 18:59:03.859026 22447428132992 run.py:483] Algo bellman_ford step 1720 current loss 0.789267, current_train_items 55072.
I0302 18:59:03.874900 22447428132992 run.py:483] Algo bellman_ford step 1721 current loss 0.983369, current_train_items 55104.
I0302 18:59:03.897479 22447428132992 run.py:483] Algo bellman_ford step 1722 current loss 1.090787, current_train_items 55136.
I0302 18:59:03.926267 22447428132992 run.py:483] Algo bellman_ford step 1723 current loss 1.210022, current_train_items 55168.
I0302 18:59:03.958247 22447428132992 run.py:483] Algo bellman_ford step 1724 current loss 1.276097, current_train_items 55200.
I0302 18:59:03.976834 22447428132992 run.py:483] Algo bellman_ford step 1725 current loss 0.540844, current_train_items 55232.
I0302 18:59:03.992995 22447428132992 run.py:483] Algo bellman_ford step 1726 current loss 0.912743, current_train_items 55264.
I0302 18:59:04.017112 22447428132992 run.py:483] Algo bellman_ford step 1727 current loss 1.102241, current_train_items 55296.
I0302 18:59:04.045791 22447428132992 run.py:483] Algo bellman_ford step 1728 current loss 1.140878, current_train_items 55328.
I0302 18:59:04.077517 22447428132992 run.py:483] Algo bellman_ford step 1729 current loss 1.572310, current_train_items 55360.
I0302 18:59:04.096567 22447428132992 run.py:483] Algo bellman_ford step 1730 current loss 0.584454, current_train_items 55392.
I0302 18:59:04.112663 22447428132992 run.py:483] Algo bellman_ford step 1731 current loss 0.873431, current_train_items 55424.
I0302 18:59:04.135411 22447428132992 run.py:483] Algo bellman_ford step 1732 current loss 1.108123, current_train_items 55456.
I0302 18:59:04.164529 22447428132992 run.py:483] Algo bellman_ford step 1733 current loss 1.207524, current_train_items 55488.
I0302 18:59:04.194204 22447428132992 run.py:483] Algo bellman_ford step 1734 current loss 1.359550, current_train_items 55520.
I0302 18:59:04.213139 22447428132992 run.py:483] Algo bellman_ford step 1735 current loss 1.055826, current_train_items 55552.
I0302 18:59:04.229311 22447428132992 run.py:483] Algo bellman_ford step 1736 current loss 0.947051, current_train_items 55584.
I0302 18:59:04.252549 22447428132992 run.py:483] Algo bellman_ford step 1737 current loss 1.162412, current_train_items 55616.
I0302 18:59:04.281347 22447428132992 run.py:483] Algo bellman_ford step 1738 current loss 1.282306, current_train_items 55648.
I0302 18:59:04.313903 22447428132992 run.py:483] Algo bellman_ford step 1739 current loss 1.676115, current_train_items 55680.
I0302 18:59:04.332560 22447428132992 run.py:483] Algo bellman_ford step 1740 current loss 0.628459, current_train_items 55712.
I0302 18:59:04.348656 22447428132992 run.py:483] Algo bellman_ford step 1741 current loss 0.786609, current_train_items 55744.
I0302 18:59:04.371436 22447428132992 run.py:483] Algo bellman_ford step 1742 current loss 1.010724, current_train_items 55776.
I0302 18:59:04.400938 22447428132992 run.py:483] Algo bellman_ford step 1743 current loss 1.183490, current_train_items 55808.
I0302 18:59:04.431638 22447428132992 run.py:483] Algo bellman_ford step 1744 current loss 1.384741, current_train_items 55840.
I0302 18:59:04.450359 22447428132992 run.py:483] Algo bellman_ford step 1745 current loss 0.674537, current_train_items 55872.
I0302 18:59:04.466632 22447428132992 run.py:483] Algo bellman_ford step 1746 current loss 1.055446, current_train_items 55904.
I0302 18:59:04.489223 22447428132992 run.py:483] Algo bellman_ford step 1747 current loss 1.083233, current_train_items 55936.
I0302 18:59:04.518175 22447428132992 run.py:483] Algo bellman_ford step 1748 current loss 1.248764, current_train_items 55968.
I0302 18:59:04.548679 22447428132992 run.py:483] Algo bellman_ford step 1749 current loss 1.287197, current_train_items 56000.
I0302 18:59:04.567538 22447428132992 run.py:483] Algo bellman_ford step 1750 current loss 0.608178, current_train_items 56032.
I0302 18:59:04.575653 22447428132992 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.853515625, 'score': 0.853515625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:59:04.575759 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:59:04.592483 22447428132992 run.py:483] Algo bellman_ford step 1751 current loss 0.843449, current_train_items 56064.
I0302 18:59:04.615676 22447428132992 run.py:483] Algo bellman_ford step 1752 current loss 1.033536, current_train_items 56096.
I0302 18:59:04.646464 22447428132992 run.py:483] Algo bellman_ford step 1753 current loss 1.292120, current_train_items 56128.
I0302 18:59:04.680706 22447428132992 run.py:483] Algo bellman_ford step 1754 current loss 1.730912, current_train_items 56160.
I0302 18:59:04.700179 22447428132992 run.py:483] Algo bellman_ford step 1755 current loss 0.610395, current_train_items 56192.
I0302 18:59:04.716243 22447428132992 run.py:483] Algo bellman_ford step 1756 current loss 0.836675, current_train_items 56224.
I0302 18:59:04.739936 22447428132992 run.py:483] Algo bellman_ford step 1757 current loss 1.148147, current_train_items 56256.
I0302 18:59:04.769955 22447428132992 run.py:483] Algo bellman_ford step 1758 current loss 1.333825, current_train_items 56288.
I0302 18:59:04.800208 22447428132992 run.py:483] Algo bellman_ford step 1759 current loss 1.132368, current_train_items 56320.
I0302 18:59:04.819450 22447428132992 run.py:483] Algo bellman_ford step 1760 current loss 0.828459, current_train_items 56352.
I0302 18:59:04.835625 22447428132992 run.py:483] Algo bellman_ford step 1761 current loss 1.041477, current_train_items 56384.
I0302 18:59:04.859246 22447428132992 run.py:483] Algo bellman_ford step 1762 current loss 1.297849, current_train_items 56416.
I0302 18:59:04.890280 22447428132992 run.py:483] Algo bellman_ford step 1763 current loss 1.291210, current_train_items 56448.
I0302 18:59:04.924391 22447428132992 run.py:483] Algo bellman_ford step 1764 current loss 1.484598, current_train_items 56480.
I0302 18:59:04.943428 22447428132992 run.py:483] Algo bellman_ford step 1765 current loss 0.693816, current_train_items 56512.
I0302 18:59:04.959331 22447428132992 run.py:483] Algo bellman_ford step 1766 current loss 0.736584, current_train_items 56544.
I0302 18:59:04.982152 22447428132992 run.py:483] Algo bellman_ford step 1767 current loss 1.076101, current_train_items 56576.
I0302 18:59:05.011533 22447428132992 run.py:483] Algo bellman_ford step 1768 current loss 1.743552, current_train_items 56608.
I0302 18:59:05.042765 22447428132992 run.py:483] Algo bellman_ford step 1769 current loss 2.081012, current_train_items 56640.
I0302 18:59:05.062234 22447428132992 run.py:483] Algo bellman_ford step 1770 current loss 0.604746, current_train_items 56672.
I0302 18:59:05.078472 22447428132992 run.py:483] Algo bellman_ford step 1771 current loss 0.883586, current_train_items 56704.
I0302 18:59:05.101867 22447428132992 run.py:483] Algo bellman_ford step 1772 current loss 1.117353, current_train_items 56736.
I0302 18:59:05.131876 22447428132992 run.py:483] Algo bellman_ford step 1773 current loss 1.196677, current_train_items 56768.
I0302 18:59:05.163152 22447428132992 run.py:483] Algo bellman_ford step 1774 current loss 1.171487, current_train_items 56800.
I0302 18:59:05.182228 22447428132992 run.py:483] Algo bellman_ford step 1775 current loss 0.605168, current_train_items 56832.
I0302 18:59:05.198579 22447428132992 run.py:483] Algo bellman_ford step 1776 current loss 0.946930, current_train_items 56864.
I0302 18:59:05.221678 22447428132992 run.py:483] Algo bellman_ford step 1777 current loss 1.186234, current_train_items 56896.
I0302 18:59:05.250983 22447428132992 run.py:483] Algo bellman_ford step 1778 current loss 1.212703, current_train_items 56928.
I0302 18:59:05.281485 22447428132992 run.py:483] Algo bellman_ford step 1779 current loss 1.212840, current_train_items 56960.
I0302 18:59:05.300691 22447428132992 run.py:483] Algo bellman_ford step 1780 current loss 0.566808, current_train_items 56992.
I0302 18:59:05.316681 22447428132992 run.py:483] Algo bellman_ford step 1781 current loss 0.842994, current_train_items 57024.
I0302 18:59:05.339364 22447428132992 run.py:483] Algo bellman_ford step 1782 current loss 0.984698, current_train_items 57056.
I0302 18:59:05.368775 22447428132992 run.py:483] Algo bellman_ford step 1783 current loss 1.320221, current_train_items 57088.
I0302 18:59:05.400093 22447428132992 run.py:483] Algo bellman_ford step 1784 current loss 1.502164, current_train_items 57120.
I0302 18:59:05.419390 22447428132992 run.py:483] Algo bellman_ford step 1785 current loss 0.641995, current_train_items 57152.
I0302 18:59:05.435804 22447428132992 run.py:483] Algo bellman_ford step 1786 current loss 0.950259, current_train_items 57184.
I0302 18:59:05.458045 22447428132992 run.py:483] Algo bellman_ford step 1787 current loss 0.992842, current_train_items 57216.
I0302 18:59:05.486658 22447428132992 run.py:483] Algo bellman_ford step 1788 current loss 1.124028, current_train_items 57248.
I0302 18:59:05.518886 22447428132992 run.py:483] Algo bellman_ford step 1789 current loss 1.458506, current_train_items 57280.
I0302 18:59:05.538409 22447428132992 run.py:483] Algo bellman_ford step 1790 current loss 0.622353, current_train_items 57312.
I0302 18:59:05.555093 22447428132992 run.py:483] Algo bellman_ford step 1791 current loss 0.873042, current_train_items 57344.
I0302 18:59:05.578156 22447428132992 run.py:483] Algo bellman_ford step 1792 current loss 1.094243, current_train_items 57376.
I0302 18:59:05.606212 22447428132992 run.py:483] Algo bellman_ford step 1793 current loss 1.222496, current_train_items 57408.
I0302 18:59:05.637903 22447428132992 run.py:483] Algo bellman_ford step 1794 current loss 1.464842, current_train_items 57440.
I0302 18:59:05.656773 22447428132992 run.py:483] Algo bellman_ford step 1795 current loss 0.676688, current_train_items 57472.
I0302 18:59:05.672818 22447428132992 run.py:483] Algo bellman_ford step 1796 current loss 0.896648, current_train_items 57504.
I0302 18:59:05.696637 22447428132992 run.py:483] Algo bellman_ford step 1797 current loss 1.092512, current_train_items 57536.
I0302 18:59:05.726464 22447428132992 run.py:483] Algo bellman_ford step 1798 current loss 1.150098, current_train_items 57568.
I0302 18:59:05.759301 22447428132992 run.py:483] Algo bellman_ford step 1799 current loss 1.238896, current_train_items 57600.
I0302 18:59:05.778563 22447428132992 run.py:483] Algo bellman_ford step 1800 current loss 0.741610, current_train_items 57632.
I0302 18:59:05.786511 22447428132992 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.841796875, 'score': 0.841796875, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:59:05.786616 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.842, val scores are: bellman_ford: 0.842
I0302 18:59:05.802989 22447428132992 run.py:483] Algo bellman_ford step 1801 current loss 0.731254, current_train_items 57664.
I0302 18:59:05.826609 22447428132992 run.py:483] Algo bellman_ford step 1802 current loss 0.989615, current_train_items 57696.
I0302 18:59:05.855195 22447428132992 run.py:483] Algo bellman_ford step 1803 current loss 1.126199, current_train_items 57728.
I0302 18:59:05.886454 22447428132992 run.py:483] Algo bellman_ford step 1804 current loss 1.578257, current_train_items 57760.
I0302 18:59:05.905749 22447428132992 run.py:483] Algo bellman_ford step 1805 current loss 0.629995, current_train_items 57792.
I0302 18:59:05.921409 22447428132992 run.py:483] Algo bellman_ford step 1806 current loss 0.785634, current_train_items 57824.
I0302 18:59:05.943769 22447428132992 run.py:483] Algo bellman_ford step 1807 current loss 1.039713, current_train_items 57856.
I0302 18:59:05.971652 22447428132992 run.py:483] Algo bellman_ford step 1808 current loss 1.037527, current_train_items 57888.
I0302 18:59:06.004425 22447428132992 run.py:483] Algo bellman_ford step 1809 current loss 1.354696, current_train_items 57920.
I0302 18:59:06.023057 22447428132992 run.py:483] Algo bellman_ford step 1810 current loss 0.727397, current_train_items 57952.
I0302 18:59:06.038983 22447428132992 run.py:483] Algo bellman_ford step 1811 current loss 0.904337, current_train_items 57984.
I0302 18:59:06.061900 22447428132992 run.py:483] Algo bellman_ford step 1812 current loss 1.153260, current_train_items 58016.
I0302 18:59:06.088980 22447428132992 run.py:483] Algo bellman_ford step 1813 current loss 1.122302, current_train_items 58048.
I0302 18:59:06.120654 22447428132992 run.py:483] Algo bellman_ford step 1814 current loss 1.188116, current_train_items 58080.
I0302 18:59:06.139354 22447428132992 run.py:483] Algo bellman_ford step 1815 current loss 0.677554, current_train_items 58112.
I0302 18:59:06.155848 22447428132992 run.py:483] Algo bellman_ford step 1816 current loss 1.006078, current_train_items 58144.
I0302 18:59:06.178228 22447428132992 run.py:483] Algo bellman_ford step 1817 current loss 1.122855, current_train_items 58176.
I0302 18:59:06.207117 22447428132992 run.py:483] Algo bellman_ford step 1818 current loss 1.673803, current_train_items 58208.
I0302 18:59:06.238610 22447428132992 run.py:483] Algo bellman_ford step 1819 current loss 2.367674, current_train_items 58240.
I0302 18:59:06.257295 22447428132992 run.py:483] Algo bellman_ford step 1820 current loss 0.577600, current_train_items 58272.
I0302 18:59:06.273830 22447428132992 run.py:483] Algo bellman_ford step 1821 current loss 0.902922, current_train_items 58304.
I0302 18:59:06.298592 22447428132992 run.py:483] Algo bellman_ford step 1822 current loss 1.197281, current_train_items 58336.
I0302 18:59:06.328487 22447428132992 run.py:483] Algo bellman_ford step 1823 current loss 1.172116, current_train_items 58368.
I0302 18:59:06.359098 22447428132992 run.py:483] Algo bellman_ford step 1824 current loss 1.217412, current_train_items 58400.
I0302 18:59:06.377945 22447428132992 run.py:483] Algo bellman_ford step 1825 current loss 0.750025, current_train_items 58432.
I0302 18:59:06.394225 22447428132992 run.py:483] Algo bellman_ford step 1826 current loss 0.981419, current_train_items 58464.
I0302 18:59:06.416778 22447428132992 run.py:483] Algo bellman_ford step 1827 current loss 1.101020, current_train_items 58496.
I0302 18:59:06.446822 22447428132992 run.py:483] Algo bellman_ford step 1828 current loss 1.550203, current_train_items 58528.
I0302 18:59:06.481146 22447428132992 run.py:483] Algo bellman_ford step 1829 current loss 1.489487, current_train_items 58560.
I0302 18:59:06.499967 22447428132992 run.py:483] Algo bellman_ford step 1830 current loss 0.648870, current_train_items 58592.
I0302 18:59:06.516229 22447428132992 run.py:483] Algo bellman_ford step 1831 current loss 0.988423, current_train_items 58624.
I0302 18:59:06.540678 22447428132992 run.py:483] Algo bellman_ford step 1832 current loss 1.205449, current_train_items 58656.
I0302 18:59:06.569985 22447428132992 run.py:483] Algo bellman_ford step 1833 current loss 1.552913, current_train_items 58688.
I0302 18:59:06.601054 22447428132992 run.py:483] Algo bellman_ford step 1834 current loss 1.606276, current_train_items 58720.
I0302 18:59:06.619801 22447428132992 run.py:483] Algo bellman_ford step 1835 current loss 0.642909, current_train_items 58752.
I0302 18:59:06.636162 22447428132992 run.py:483] Algo bellman_ford step 1836 current loss 0.941948, current_train_items 58784.
I0302 18:59:06.660119 22447428132992 run.py:483] Algo bellman_ford step 1837 current loss 1.219177, current_train_items 58816.
I0302 18:59:06.688687 22447428132992 run.py:483] Algo bellman_ford step 1838 current loss 1.303427, current_train_items 58848.
I0302 18:59:06.720882 22447428132992 run.py:483] Algo bellman_ford step 1839 current loss 1.206676, current_train_items 58880.
I0302 18:59:06.739508 22447428132992 run.py:483] Algo bellman_ford step 1840 current loss 0.659890, current_train_items 58912.
I0302 18:59:06.755778 22447428132992 run.py:483] Algo bellman_ford step 1841 current loss 0.973043, current_train_items 58944.
I0302 18:59:06.779195 22447428132992 run.py:483] Algo bellman_ford step 1842 current loss 1.151713, current_train_items 58976.
I0302 18:59:06.808694 22447428132992 run.py:483] Algo bellman_ford step 1843 current loss 1.251513, current_train_items 59008.
I0302 18:59:06.840579 22447428132992 run.py:483] Algo bellman_ford step 1844 current loss 1.336945, current_train_items 59040.
I0302 18:59:06.859807 22447428132992 run.py:483] Algo bellman_ford step 1845 current loss 0.648609, current_train_items 59072.
I0302 18:59:06.875806 22447428132992 run.py:483] Algo bellman_ford step 1846 current loss 0.810600, current_train_items 59104.
I0302 18:59:06.898983 22447428132992 run.py:483] Algo bellman_ford step 1847 current loss 1.074847, current_train_items 59136.
I0302 18:59:06.928272 22447428132992 run.py:483] Algo bellman_ford step 1848 current loss 1.124359, current_train_items 59168.
I0302 18:59:06.961809 22447428132992 run.py:483] Algo bellman_ford step 1849 current loss 1.600277, current_train_items 59200.
I0302 18:59:06.980697 22447428132992 run.py:483] Algo bellman_ford step 1850 current loss 0.602516, current_train_items 59232.
I0302 18:59:06.988840 22447428132992 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.857421875, 'score': 0.857421875, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:59:06.988946 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.857, val scores are: bellman_ford: 0.857
I0302 18:59:07.005630 22447428132992 run.py:483] Algo bellman_ford step 1851 current loss 0.844103, current_train_items 59264.
I0302 18:59:07.029248 22447428132992 run.py:483] Algo bellman_ford step 1852 current loss 1.026474, current_train_items 59296.
I0302 18:59:07.058950 22447428132992 run.py:483] Algo bellman_ford step 1853 current loss 1.219813, current_train_items 59328.
I0302 18:59:07.091431 22447428132992 run.py:483] Algo bellman_ford step 1854 current loss 1.318227, current_train_items 59360.
I0302 18:59:07.110670 22447428132992 run.py:483] Algo bellman_ford step 1855 current loss 0.618531, current_train_items 59392.
I0302 18:59:07.125967 22447428132992 run.py:483] Algo bellman_ford step 1856 current loss 0.829465, current_train_items 59424.
I0302 18:59:07.149077 22447428132992 run.py:483] Algo bellman_ford step 1857 current loss 1.022224, current_train_items 59456.
I0302 18:59:07.177155 22447428132992 run.py:483] Algo bellman_ford step 1858 current loss 0.950605, current_train_items 59488.
I0302 18:59:07.209168 22447428132992 run.py:483] Algo bellman_ford step 1859 current loss 1.319701, current_train_items 59520.
I0302 18:59:07.228804 22447428132992 run.py:483] Algo bellman_ford step 1860 current loss 0.608169, current_train_items 59552.
I0302 18:59:07.245333 22447428132992 run.py:483] Algo bellman_ford step 1861 current loss 0.771814, current_train_items 59584.
I0302 18:59:07.267740 22447428132992 run.py:483] Algo bellman_ford step 1862 current loss 1.091800, current_train_items 59616.
I0302 18:59:07.296912 22447428132992 run.py:483] Algo bellman_ford step 1863 current loss 1.316977, current_train_items 59648.
I0302 18:59:07.329534 22447428132992 run.py:483] Algo bellman_ford step 1864 current loss 1.723160, current_train_items 59680.
I0302 18:59:07.348447 22447428132992 run.py:483] Algo bellman_ford step 1865 current loss 0.671701, current_train_items 59712.
I0302 18:59:07.364617 22447428132992 run.py:483] Algo bellman_ford step 1866 current loss 0.891606, current_train_items 59744.
I0302 18:59:07.388396 22447428132992 run.py:483] Algo bellman_ford step 1867 current loss 1.250509, current_train_items 59776.
I0302 18:59:07.417767 22447428132992 run.py:483] Algo bellman_ford step 1868 current loss 1.159029, current_train_items 59808.
I0302 18:59:07.449465 22447428132992 run.py:483] Algo bellman_ford step 1869 current loss 1.243485, current_train_items 59840.
I0302 18:59:07.468428 22447428132992 run.py:483] Algo bellman_ford step 1870 current loss 0.652326, current_train_items 59872.
I0302 18:59:07.484445 22447428132992 run.py:483] Algo bellman_ford step 1871 current loss 0.785515, current_train_items 59904.
I0302 18:59:07.506743 22447428132992 run.py:483] Algo bellman_ford step 1872 current loss 1.054929, current_train_items 59936.
I0302 18:59:07.535983 22447428132992 run.py:483] Algo bellman_ford step 1873 current loss 1.119178, current_train_items 59968.
I0302 18:59:07.567391 22447428132992 run.py:483] Algo bellman_ford step 1874 current loss 1.309893, current_train_items 60000.
I0302 18:59:07.586459 22447428132992 run.py:483] Algo bellman_ford step 1875 current loss 0.777656, current_train_items 60032.
I0302 18:59:07.602477 22447428132992 run.py:483] Algo bellman_ford step 1876 current loss 0.759949, current_train_items 60064.
I0302 18:59:07.625269 22447428132992 run.py:483] Algo bellman_ford step 1877 current loss 1.137836, current_train_items 60096.
I0302 18:59:07.653527 22447428132992 run.py:483] Algo bellman_ford step 1878 current loss 1.136100, current_train_items 60128.
I0302 18:59:07.687986 22447428132992 run.py:483] Algo bellman_ford step 1879 current loss 1.855136, current_train_items 60160.
I0302 18:59:07.707129 22447428132992 run.py:483] Algo bellman_ford step 1880 current loss 0.657430, current_train_items 60192.
I0302 18:59:07.723597 22447428132992 run.py:483] Algo bellman_ford step 1881 current loss 0.859719, current_train_items 60224.
I0302 18:59:07.746021 22447428132992 run.py:483] Algo bellman_ford step 1882 current loss 1.038476, current_train_items 60256.
I0302 18:59:07.775269 22447428132992 run.py:483] Algo bellman_ford step 1883 current loss 1.180893, current_train_items 60288.
I0302 18:59:07.807340 22447428132992 run.py:483] Algo bellman_ford step 1884 current loss 1.343037, current_train_items 60320.
I0302 18:59:07.826344 22447428132992 run.py:483] Algo bellman_ford step 1885 current loss 0.717624, current_train_items 60352.
I0302 18:59:07.842837 22447428132992 run.py:483] Algo bellman_ford step 1886 current loss 0.924894, current_train_items 60384.
I0302 18:59:07.865423 22447428132992 run.py:483] Algo bellman_ford step 1887 current loss 1.138888, current_train_items 60416.
I0302 18:59:07.893329 22447428132992 run.py:483] Algo bellman_ford step 1888 current loss 1.109926, current_train_items 60448.
I0302 18:59:07.926919 22447428132992 run.py:483] Algo bellman_ford step 1889 current loss 1.286932, current_train_items 60480.
I0302 18:59:07.945780 22447428132992 run.py:483] Algo bellman_ford step 1890 current loss 0.745771, current_train_items 60512.
I0302 18:59:07.961972 22447428132992 run.py:483] Algo bellman_ford step 1891 current loss 0.810937, current_train_items 60544.
I0302 18:59:07.984049 22447428132992 run.py:483] Algo bellman_ford step 1892 current loss 1.054620, current_train_items 60576.
I0302 18:59:08.013698 22447428132992 run.py:483] Algo bellman_ford step 1893 current loss 1.364029, current_train_items 60608.
I0302 18:59:08.048448 22447428132992 run.py:483] Algo bellman_ford step 1894 current loss 1.590836, current_train_items 60640.
I0302 18:59:08.067060 22447428132992 run.py:483] Algo bellman_ford step 1895 current loss 0.707144, current_train_items 60672.
I0302 18:59:08.083139 22447428132992 run.py:483] Algo bellman_ford step 1896 current loss 0.903676, current_train_items 60704.
I0302 18:59:08.106221 22447428132992 run.py:483] Algo bellman_ford step 1897 current loss 1.174862, current_train_items 60736.
I0302 18:59:08.135900 22447428132992 run.py:483] Algo bellman_ford step 1898 current loss 1.232683, current_train_items 60768.
I0302 18:59:08.168174 22447428132992 run.py:483] Algo bellman_ford step 1899 current loss 1.280661, current_train_items 60800.
I0302 18:59:08.187392 22447428132992 run.py:483] Algo bellman_ford step 1900 current loss 0.781863, current_train_items 60832.
I0302 18:59:08.195224 22447428132992 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.876953125, 'score': 0.876953125, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:59:08.195329 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.886, current avg val score is 0.877, val scores are: bellman_ford: 0.877
I0302 18:59:08.211814 22447428132992 run.py:483] Algo bellman_ford step 1901 current loss 0.839358, current_train_items 60864.
I0302 18:59:08.234735 22447428132992 run.py:483] Algo bellman_ford step 1902 current loss 0.964342, current_train_items 60896.
I0302 18:59:08.262500 22447428132992 run.py:483] Algo bellman_ford step 1903 current loss 1.045350, current_train_items 60928.
I0302 18:59:08.294930 22447428132992 run.py:483] Algo bellman_ford step 1904 current loss 1.275041, current_train_items 60960.
I0302 18:59:08.314479 22447428132992 run.py:483] Algo bellman_ford step 1905 current loss 0.668263, current_train_items 60992.
I0302 18:59:08.330097 22447428132992 run.py:483] Algo bellman_ford step 1906 current loss 0.824776, current_train_items 61024.
I0302 18:59:08.352328 22447428132992 run.py:483] Algo bellman_ford step 1907 current loss 1.050708, current_train_items 61056.
I0302 18:59:08.381046 22447428132992 run.py:483] Algo bellman_ford step 1908 current loss 1.306693, current_train_items 61088.
I0302 18:59:08.412724 22447428132992 run.py:483] Algo bellman_ford step 1909 current loss 1.773185, current_train_items 61120.
I0302 18:59:08.431784 22447428132992 run.py:483] Algo bellman_ford step 1910 current loss 0.610243, current_train_items 61152.
I0302 18:59:08.448165 22447428132992 run.py:483] Algo bellman_ford step 1911 current loss 0.889552, current_train_items 61184.
I0302 18:59:08.471973 22447428132992 run.py:483] Algo bellman_ford step 1912 current loss 1.108933, current_train_items 61216.
I0302 18:59:08.501003 22447428132992 run.py:483] Algo bellman_ford step 1913 current loss 1.194348, current_train_items 61248.
I0302 18:59:08.533770 22447428132992 run.py:483] Algo bellman_ford step 1914 current loss 1.287390, current_train_items 61280.
I0302 18:59:08.552864 22447428132992 run.py:483] Algo bellman_ford step 1915 current loss 0.611931, current_train_items 61312.
I0302 18:59:08.569109 22447428132992 run.py:483] Algo bellman_ford step 1916 current loss 0.855766, current_train_items 61344.
I0302 18:59:08.592711 22447428132992 run.py:483] Algo bellman_ford step 1917 current loss 1.110101, current_train_items 61376.
I0302 18:59:08.622375 22447428132992 run.py:483] Algo bellman_ford step 1918 current loss 1.213125, current_train_items 61408.
I0302 18:59:08.656111 22447428132992 run.py:483] Algo bellman_ford step 1919 current loss 1.547065, current_train_items 61440.
I0302 18:59:08.675241 22447428132992 run.py:483] Algo bellman_ford step 1920 current loss 0.570310, current_train_items 61472.
I0302 18:59:08.691506 22447428132992 run.py:483] Algo bellman_ford step 1921 current loss 0.832475, current_train_items 61504.
I0302 18:59:08.714996 22447428132992 run.py:483] Algo bellman_ford step 1922 current loss 1.063511, current_train_items 61536.
I0302 18:59:08.743001 22447428132992 run.py:483] Algo bellman_ford step 1923 current loss 1.068176, current_train_items 61568.
I0302 18:59:08.775083 22447428132992 run.py:483] Algo bellman_ford step 1924 current loss 1.512165, current_train_items 61600.
I0302 18:59:08.794300 22447428132992 run.py:483] Algo bellman_ford step 1925 current loss 0.597263, current_train_items 61632.
I0302 18:59:08.810831 22447428132992 run.py:483] Algo bellman_ford step 1926 current loss 0.920495, current_train_items 61664.
I0302 18:59:08.832839 22447428132992 run.py:483] Algo bellman_ford step 1927 current loss 0.924988, current_train_items 61696.
I0302 18:59:08.861207 22447428132992 run.py:483] Algo bellman_ford step 1928 current loss 1.057562, current_train_items 61728.
I0302 18:59:08.891687 22447428132992 run.py:483] Algo bellman_ford step 1929 current loss 1.247410, current_train_items 61760.
I0302 18:59:08.910959 22447428132992 run.py:483] Algo bellman_ford step 1930 current loss 0.751201, current_train_items 61792.
I0302 18:59:08.926566 22447428132992 run.py:483] Algo bellman_ford step 1931 current loss 0.721707, current_train_items 61824.
I0302 18:59:08.950548 22447428132992 run.py:483] Algo bellman_ford step 1932 current loss 1.082756, current_train_items 61856.
I0302 18:59:08.979566 22447428132992 run.py:483] Algo bellman_ford step 1933 current loss 1.244857, current_train_items 61888.
I0302 18:59:09.010008 22447428132992 run.py:483] Algo bellman_ford step 1934 current loss 1.282172, current_train_items 61920.
I0302 18:59:09.028894 22447428132992 run.py:483] Algo bellman_ford step 1935 current loss 0.722755, current_train_items 61952.
I0302 18:59:09.045067 22447428132992 run.py:483] Algo bellman_ford step 1936 current loss 0.874342, current_train_items 61984.
I0302 18:59:09.069107 22447428132992 run.py:483] Algo bellman_ford step 1937 current loss 1.020249, current_train_items 62016.
I0302 18:59:09.097638 22447428132992 run.py:483] Algo bellman_ford step 1938 current loss 1.103289, current_train_items 62048.
I0302 18:59:09.130319 22447428132992 run.py:483] Algo bellman_ford step 1939 current loss 1.404264, current_train_items 62080.
I0302 18:59:09.149389 22447428132992 run.py:483] Algo bellman_ford step 1940 current loss 0.733339, current_train_items 62112.
I0302 18:59:09.165587 22447428132992 run.py:483] Algo bellman_ford step 1941 current loss 0.859532, current_train_items 62144.
I0302 18:59:09.188440 22447428132992 run.py:483] Algo bellman_ford step 1942 current loss 1.060405, current_train_items 62176.
I0302 18:59:09.216646 22447428132992 run.py:483] Algo bellman_ford step 1943 current loss 1.058604, current_train_items 62208.
I0302 18:59:09.249007 22447428132992 run.py:483] Algo bellman_ford step 1944 current loss 1.262514, current_train_items 62240.
I0302 18:59:09.268300 22447428132992 run.py:483] Algo bellman_ford step 1945 current loss 0.657981, current_train_items 62272.
I0302 18:59:09.284501 22447428132992 run.py:483] Algo bellman_ford step 1946 current loss 0.817597, current_train_items 62304.
I0302 18:59:09.307856 22447428132992 run.py:483] Algo bellman_ford step 1947 current loss 0.943043, current_train_items 62336.
I0302 18:59:09.337830 22447428132992 run.py:483] Algo bellman_ford step 1948 current loss 1.267857, current_train_items 62368.
I0302 18:59:09.369174 22447428132992 run.py:483] Algo bellman_ford step 1949 current loss 1.215795, current_train_items 62400.
I0302 18:59:09.388343 22447428132992 run.py:483] Algo bellman_ford step 1950 current loss 0.647760, current_train_items 62432.
I0302 18:59:09.396269 22447428132992 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:59:09.396375 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.886, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:09.427009 22447428132992 run.py:483] Algo bellman_ford step 1951 current loss 0.918117, current_train_items 62464.
I0302 18:59:09.450447 22447428132992 run.py:483] Algo bellman_ford step 1952 current loss 1.018002, current_train_items 62496.
I0302 18:59:09.482077 22447428132992 run.py:483] Algo bellman_ford step 1953 current loss 1.223816, current_train_items 62528.
I0302 18:59:09.514333 22447428132992 run.py:483] Algo bellman_ford step 1954 current loss 1.481305, current_train_items 62560.
I0302 18:59:09.533846 22447428132992 run.py:483] Algo bellman_ford step 1955 current loss 0.626174, current_train_items 62592.
I0302 18:59:09.549078 22447428132992 run.py:483] Algo bellman_ford step 1956 current loss 0.747840, current_train_items 62624.
I0302 18:59:09.572577 22447428132992 run.py:483] Algo bellman_ford step 1957 current loss 1.018870, current_train_items 62656.
I0302 18:59:09.602309 22447428132992 run.py:483] Algo bellman_ford step 1958 current loss 1.086183, current_train_items 62688.
I0302 18:59:09.635027 22447428132992 run.py:483] Algo bellman_ford step 1959 current loss 1.139881, current_train_items 62720.
I0302 18:59:09.654252 22447428132992 run.py:483] Algo bellman_ford step 1960 current loss 0.703393, current_train_items 62752.
I0302 18:59:09.670463 22447428132992 run.py:483] Algo bellman_ford step 1961 current loss 0.895071, current_train_items 62784.
I0302 18:59:09.692888 22447428132992 run.py:483] Algo bellman_ford step 1962 current loss 1.063241, current_train_items 62816.
I0302 18:59:09.721277 22447428132992 run.py:483] Algo bellman_ford step 1963 current loss 1.027109, current_train_items 62848.
I0302 18:59:09.756192 22447428132992 run.py:483] Algo bellman_ford step 1964 current loss 1.324069, current_train_items 62880.
I0302 18:59:09.775533 22447428132992 run.py:483] Algo bellman_ford step 1965 current loss 0.701186, current_train_items 62912.
I0302 18:59:09.791413 22447428132992 run.py:483] Algo bellman_ford step 1966 current loss 0.739750, current_train_items 62944.
I0302 18:59:09.814280 22447428132992 run.py:483] Algo bellman_ford step 1967 current loss 1.128403, current_train_items 62976.
I0302 18:59:09.843141 22447428132992 run.py:483] Algo bellman_ford step 1968 current loss 1.025350, current_train_items 63008.
I0302 18:59:09.874624 22447428132992 run.py:483] Algo bellman_ford step 1969 current loss 1.278636, current_train_items 63040.
I0302 18:59:09.893835 22447428132992 run.py:483] Algo bellman_ford step 1970 current loss 0.634436, current_train_items 63072.
I0302 18:59:09.909830 22447428132992 run.py:483] Algo bellman_ford step 1971 current loss 0.800112, current_train_items 63104.
I0302 18:59:09.931957 22447428132992 run.py:483] Algo bellman_ford step 1972 current loss 1.009331, current_train_items 63136.
I0302 18:59:09.961329 22447428132992 run.py:483] Algo bellman_ford step 1973 current loss 1.106782, current_train_items 63168.
I0302 18:59:09.996269 22447428132992 run.py:483] Algo bellman_ford step 1974 current loss 1.474880, current_train_items 63200.
I0302 18:59:10.015336 22447428132992 run.py:483] Algo bellman_ford step 1975 current loss 0.630817, current_train_items 63232.
I0302 18:59:10.031160 22447428132992 run.py:483] Algo bellman_ford step 1976 current loss 0.784961, current_train_items 63264.
I0302 18:59:10.053215 22447428132992 run.py:483] Algo bellman_ford step 1977 current loss 1.002600, current_train_items 63296.
I0302 18:59:10.081680 22447428132992 run.py:483] Algo bellman_ford step 1978 current loss 1.124185, current_train_items 63328.
I0302 18:59:10.114168 22447428132992 run.py:483] Algo bellman_ford step 1979 current loss 1.344237, current_train_items 63360.
I0302 18:59:10.133010 22447428132992 run.py:483] Algo bellman_ford step 1980 current loss 0.682388, current_train_items 63392.
I0302 18:59:10.149251 22447428132992 run.py:483] Algo bellman_ford step 1981 current loss 0.788617, current_train_items 63424.
I0302 18:59:10.171991 22447428132992 run.py:483] Algo bellman_ford step 1982 current loss 0.981420, current_train_items 63456.
I0302 18:59:10.200281 22447428132992 run.py:483] Algo bellman_ford step 1983 current loss 1.111693, current_train_items 63488.
I0302 18:59:10.235000 22447428132992 run.py:483] Algo bellman_ford step 1984 current loss 1.781169, current_train_items 63520.
I0302 18:59:10.254405 22447428132992 run.py:483] Algo bellman_ford step 1985 current loss 0.608962, current_train_items 63552.
I0302 18:59:10.270516 22447428132992 run.py:483] Algo bellman_ford step 1986 current loss 0.876258, current_train_items 63584.
I0302 18:59:10.292412 22447428132992 run.py:483] Algo bellman_ford step 1987 current loss 0.983516, current_train_items 63616.
I0302 18:59:10.320644 22447428132992 run.py:483] Algo bellman_ford step 1988 current loss 1.064695, current_train_items 63648.
I0302 18:59:10.353894 22447428132992 run.py:483] Algo bellman_ford step 1989 current loss 1.303598, current_train_items 63680.
I0302 18:59:10.372877 22447428132992 run.py:483] Algo bellman_ford step 1990 current loss 0.641127, current_train_items 63712.
I0302 18:59:10.388886 22447428132992 run.py:483] Algo bellman_ford step 1991 current loss 0.898284, current_train_items 63744.
I0302 18:59:10.411659 22447428132992 run.py:483] Algo bellman_ford step 1992 current loss 0.998365, current_train_items 63776.
I0302 18:59:10.441006 22447428132992 run.py:483] Algo bellman_ford step 1993 current loss 1.052818, current_train_items 63808.
I0302 18:59:10.469310 22447428132992 run.py:483] Algo bellman_ford step 1994 current loss 1.150133, current_train_items 63840.
I0302 18:59:10.488254 22447428132992 run.py:483] Algo bellman_ford step 1995 current loss 0.721986, current_train_items 63872.
I0302 18:59:10.504826 22447428132992 run.py:483] Algo bellman_ford step 1996 current loss 0.932702, current_train_items 63904.
I0302 18:59:10.527228 22447428132992 run.py:483] Algo bellman_ford step 1997 current loss 0.995910, current_train_items 63936.
I0302 18:59:10.557490 22447428132992 run.py:483] Algo bellman_ford step 1998 current loss 1.269385, current_train_items 63968.
I0302 18:59:10.588815 22447428132992 run.py:483] Algo bellman_ford step 1999 current loss 1.349047, current_train_items 64000.
I0302 18:59:10.607802 22447428132992 run.py:483] Algo bellman_ford step 2000 current loss 0.692539, current_train_items 64032.
I0302 18:59:10.615687 22447428132992 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.8310546875, 'score': 0.8310546875, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:59:10.615794 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.831, val scores are: bellman_ford: 0.831
I0302 18:59:10.632189 22447428132992 run.py:483] Algo bellman_ford step 2001 current loss 0.743022, current_train_items 64064.
I0302 18:59:10.656121 22447428132992 run.py:483] Algo bellman_ford step 2002 current loss 1.143181, current_train_items 64096.
I0302 18:59:10.685756 22447428132992 run.py:483] Algo bellman_ford step 2003 current loss 1.123846, current_train_items 64128.
I0302 18:59:10.718343 22447428132992 run.py:483] Algo bellman_ford step 2004 current loss 1.361603, current_train_items 64160.
I0302 18:59:10.737867 22447428132992 run.py:483] Algo bellman_ford step 2005 current loss 0.725357, current_train_items 64192.
I0302 18:59:10.753702 22447428132992 run.py:483] Algo bellman_ford step 2006 current loss 0.807783, current_train_items 64224.
I0302 18:59:10.776619 22447428132992 run.py:483] Algo bellman_ford step 2007 current loss 1.020036, current_train_items 64256.
I0302 18:59:10.806596 22447428132992 run.py:483] Algo bellman_ford step 2008 current loss 1.201319, current_train_items 64288.
I0302 18:59:10.838613 22447428132992 run.py:483] Algo bellman_ford step 2009 current loss 1.277025, current_train_items 64320.
I0302 18:59:10.857623 22447428132992 run.py:483] Algo bellman_ford step 2010 current loss 0.846292, current_train_items 64352.
I0302 18:59:10.873912 22447428132992 run.py:483] Algo bellman_ford step 2011 current loss 0.889424, current_train_items 64384.
I0302 18:59:10.897182 22447428132992 run.py:483] Algo bellman_ford step 2012 current loss 0.989332, current_train_items 64416.
I0302 18:59:10.925597 22447428132992 run.py:483] Algo bellman_ford step 2013 current loss 1.190363, current_train_items 64448.
I0302 18:59:10.957528 22447428132992 run.py:483] Algo bellman_ford step 2014 current loss 1.284350, current_train_items 64480.
I0302 18:59:10.976375 22447428132992 run.py:483] Algo bellman_ford step 2015 current loss 0.684654, current_train_items 64512.
I0302 18:59:10.992982 22447428132992 run.py:483] Algo bellman_ford step 2016 current loss 0.883907, current_train_items 64544.
I0302 18:59:11.016386 22447428132992 run.py:483] Algo bellman_ford step 2017 current loss 1.090876, current_train_items 64576.
I0302 18:59:11.043664 22447428132992 run.py:483] Algo bellman_ford step 2018 current loss 1.040850, current_train_items 64608.
I0302 18:59:11.076552 22447428132992 run.py:483] Algo bellman_ford step 2019 current loss 1.451565, current_train_items 64640.
I0302 18:59:11.095620 22447428132992 run.py:483] Algo bellman_ford step 2020 current loss 0.576217, current_train_items 64672.
I0302 18:59:11.111650 22447428132992 run.py:483] Algo bellman_ford step 2021 current loss 0.773973, current_train_items 64704.
I0302 18:59:11.134911 22447428132992 run.py:483] Algo bellman_ford step 2022 current loss 1.076450, current_train_items 64736.
I0302 18:59:11.164119 22447428132992 run.py:483] Algo bellman_ford step 2023 current loss 1.136770, current_train_items 64768.
I0302 18:59:11.199338 22447428132992 run.py:483] Algo bellman_ford step 2024 current loss 1.364254, current_train_items 64800.
I0302 18:59:11.218496 22447428132992 run.py:483] Algo bellman_ford step 2025 current loss 0.723885, current_train_items 64832.
I0302 18:59:11.234417 22447428132992 run.py:483] Algo bellman_ford step 2026 current loss 0.706153, current_train_items 64864.
I0302 18:59:11.257761 22447428132992 run.py:483] Algo bellman_ford step 2027 current loss 1.060684, current_train_items 64896.
I0302 18:59:11.286733 22447428132992 run.py:483] Algo bellman_ford step 2028 current loss 1.200406, current_train_items 64928.
I0302 18:59:11.318434 22447428132992 run.py:483] Algo bellman_ford step 2029 current loss 1.221247, current_train_items 64960.
I0302 18:59:11.337570 22447428132992 run.py:483] Algo bellman_ford step 2030 current loss 0.635578, current_train_items 64992.
I0302 18:59:11.354050 22447428132992 run.py:483] Algo bellman_ford step 2031 current loss 0.890921, current_train_items 65024.
I0302 18:59:11.376457 22447428132992 run.py:483] Algo bellman_ford step 2032 current loss 0.966792, current_train_items 65056.
I0302 18:59:11.406749 22447428132992 run.py:483] Algo bellman_ford step 2033 current loss 1.175933, current_train_items 65088.
I0302 18:59:11.438115 22447428132992 run.py:483] Algo bellman_ford step 2034 current loss 1.252128, current_train_items 65120.
I0302 18:59:11.457312 22447428132992 run.py:483] Algo bellman_ford step 2035 current loss 0.616893, current_train_items 65152.
I0302 18:59:11.473198 22447428132992 run.py:483] Algo bellman_ford step 2036 current loss 0.765633, current_train_items 65184.
I0302 18:59:11.495553 22447428132992 run.py:483] Algo bellman_ford step 2037 current loss 1.109557, current_train_items 65216.
I0302 18:59:11.525014 22447428132992 run.py:483] Algo bellman_ford step 2038 current loss 1.058617, current_train_items 65248.
I0302 18:59:11.557578 22447428132992 run.py:483] Algo bellman_ford step 2039 current loss 1.172134, current_train_items 65280.
I0302 18:59:11.576814 22447428132992 run.py:483] Algo bellman_ford step 2040 current loss 0.755599, current_train_items 65312.
I0302 18:59:11.592735 22447428132992 run.py:483] Algo bellman_ford step 2041 current loss 0.868419, current_train_items 65344.
I0302 18:59:11.616739 22447428132992 run.py:483] Algo bellman_ford step 2042 current loss 1.129951, current_train_items 65376.
I0302 18:59:11.646503 22447428132992 run.py:483] Algo bellman_ford step 2043 current loss 1.097116, current_train_items 65408.
I0302 18:59:11.673556 22447428132992 run.py:483] Algo bellman_ford step 2044 current loss 1.015656, current_train_items 65440.
I0302 18:59:11.692870 22447428132992 run.py:483] Algo bellman_ford step 2045 current loss 0.575397, current_train_items 65472.
I0302 18:59:11.709563 22447428132992 run.py:483] Algo bellman_ford step 2046 current loss 0.996004, current_train_items 65504.
I0302 18:59:11.732550 22447428132992 run.py:483] Algo bellman_ford step 2047 current loss 1.160532, current_train_items 65536.
I0302 18:59:11.761542 22447428132992 run.py:483] Algo bellman_ford step 2048 current loss 1.020184, current_train_items 65568.
I0302 18:59:11.791586 22447428132992 run.py:483] Algo bellman_ford step 2049 current loss 1.576676, current_train_items 65600.
I0302 18:59:11.810635 22447428132992 run.py:483] Algo bellman_ford step 2050 current loss 0.651910, current_train_items 65632.
I0302 18:59:11.818871 22447428132992 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.876953125, 'score': 0.876953125, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:59:11.818978 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.877, val scores are: bellman_ford: 0.877
I0302 18:59:11.836162 22447428132992 run.py:483] Algo bellman_ford step 2051 current loss 0.819973, current_train_items 65664.
I0302 18:59:11.859176 22447428132992 run.py:483] Algo bellman_ford step 2052 current loss 1.014744, current_train_items 65696.
I0302 18:59:11.887909 22447428132992 run.py:483] Algo bellman_ford step 2053 current loss 1.100736, current_train_items 65728.
I0302 18:59:11.923892 22447428132992 run.py:483] Algo bellman_ford step 2054 current loss 1.418457, current_train_items 65760.
I0302 18:59:11.942993 22447428132992 run.py:483] Algo bellman_ford step 2055 current loss 0.801489, current_train_items 65792.
I0302 18:59:11.958669 22447428132992 run.py:483] Algo bellman_ford step 2056 current loss 0.837747, current_train_items 65824.
I0302 18:59:11.981264 22447428132992 run.py:483] Algo bellman_ford step 2057 current loss 0.996189, current_train_items 65856.
I0302 18:59:12.009847 22447428132992 run.py:483] Algo bellman_ford step 2058 current loss 0.994514, current_train_items 65888.
I0302 18:59:12.042825 22447428132992 run.py:483] Algo bellman_ford step 2059 current loss 1.272009, current_train_items 65920.
I0302 18:59:12.062073 22447428132992 run.py:483] Algo bellman_ford step 2060 current loss 0.878658, current_train_items 65952.
I0302 18:59:12.078366 22447428132992 run.py:483] Algo bellman_ford step 2061 current loss 0.782966, current_train_items 65984.
I0302 18:59:12.101001 22447428132992 run.py:483] Algo bellman_ford step 2062 current loss 1.020663, current_train_items 66016.
I0302 18:59:12.130535 22447428132992 run.py:483] Algo bellman_ford step 2063 current loss 1.081243, current_train_items 66048.
I0302 18:59:12.164113 22447428132992 run.py:483] Algo bellman_ford step 2064 current loss 1.381843, current_train_items 66080.
I0302 18:59:12.182932 22447428132992 run.py:483] Algo bellman_ford step 2065 current loss 0.577528, current_train_items 66112.
I0302 18:59:12.199062 22447428132992 run.py:483] Algo bellman_ford step 2066 current loss 0.723490, current_train_items 66144.
I0302 18:59:12.222075 22447428132992 run.py:483] Algo bellman_ford step 2067 current loss 1.012824, current_train_items 66176.
I0302 18:59:12.251970 22447428132992 run.py:483] Algo bellman_ford step 2068 current loss 1.083227, current_train_items 66208.
I0302 18:59:12.283933 22447428132992 run.py:483] Algo bellman_ford step 2069 current loss 1.270490, current_train_items 66240.
I0302 18:59:12.303082 22447428132992 run.py:483] Algo bellman_ford step 2070 current loss 0.827072, current_train_items 66272.
I0302 18:59:12.319495 22447428132992 run.py:483] Algo bellman_ford step 2071 current loss 0.947062, current_train_items 66304.
I0302 18:59:12.342358 22447428132992 run.py:483] Algo bellman_ford step 2072 current loss 1.048976, current_train_items 66336.
I0302 18:59:12.372111 22447428132992 run.py:483] Algo bellman_ford step 2073 current loss 1.292150, current_train_items 66368.
I0302 18:59:12.404083 22447428132992 run.py:483] Algo bellman_ford step 2074 current loss 1.186347, current_train_items 66400.
I0302 18:59:12.423059 22447428132992 run.py:483] Algo bellman_ford step 2075 current loss 0.695162, current_train_items 66432.
I0302 18:59:12.439710 22447428132992 run.py:483] Algo bellman_ford step 2076 current loss 0.936856, current_train_items 66464.
I0302 18:59:12.462912 22447428132992 run.py:483] Algo bellman_ford step 2077 current loss 1.010060, current_train_items 66496.
I0302 18:59:12.490065 22447428132992 run.py:483] Algo bellman_ford step 2078 current loss 1.090841, current_train_items 66528.
I0302 18:59:12.521436 22447428132992 run.py:483] Algo bellman_ford step 2079 current loss 1.457431, current_train_items 66560.
I0302 18:59:12.540238 22447428132992 run.py:483] Algo bellman_ford step 2080 current loss 0.584140, current_train_items 66592.
I0302 18:59:12.556391 22447428132992 run.py:483] Algo bellman_ford step 2081 current loss 0.806570, current_train_items 66624.
I0302 18:59:12.579116 22447428132992 run.py:483] Algo bellman_ford step 2082 current loss 0.987074, current_train_items 66656.
I0302 18:59:12.609370 22447428132992 run.py:483] Algo bellman_ford step 2083 current loss 1.240120, current_train_items 66688.
I0302 18:59:12.641770 22447428132992 run.py:483] Algo bellman_ford step 2084 current loss 1.340278, current_train_items 66720.
I0302 18:59:12.661037 22447428132992 run.py:483] Algo bellman_ford step 2085 current loss 0.675684, current_train_items 66752.
I0302 18:59:12.677172 22447428132992 run.py:483] Algo bellman_ford step 2086 current loss 0.886251, current_train_items 66784.
I0302 18:59:12.699985 22447428132992 run.py:483] Algo bellman_ford step 2087 current loss 1.156455, current_train_items 66816.
I0302 18:59:12.727283 22447428132992 run.py:483] Algo bellman_ford step 2088 current loss 0.934049, current_train_items 66848.
I0302 18:59:12.759311 22447428132992 run.py:483] Algo bellman_ford step 2089 current loss 1.348689, current_train_items 66880.
I0302 18:59:12.778517 22447428132992 run.py:483] Algo bellman_ford step 2090 current loss 0.610632, current_train_items 66912.
I0302 18:59:12.794710 22447428132992 run.py:483] Algo bellman_ford step 2091 current loss 0.923876, current_train_items 66944.
I0302 18:59:12.816958 22447428132992 run.py:483] Algo bellman_ford step 2092 current loss 1.134568, current_train_items 66976.
I0302 18:59:12.846075 22447428132992 run.py:483] Algo bellman_ford step 2093 current loss 1.092729, current_train_items 67008.
I0302 18:59:12.878229 22447428132992 run.py:483] Algo bellman_ford step 2094 current loss 1.551770, current_train_items 67040.
I0302 18:59:12.897370 22447428132992 run.py:483] Algo bellman_ford step 2095 current loss 0.617649, current_train_items 67072.
I0302 18:59:12.913637 22447428132992 run.py:483] Algo bellman_ford step 2096 current loss 0.812826, current_train_items 67104.
I0302 18:59:12.936462 22447428132992 run.py:483] Algo bellman_ford step 2097 current loss 1.025762, current_train_items 67136.
I0302 18:59:12.964599 22447428132992 run.py:483] Algo bellman_ford step 2098 current loss 1.022211, current_train_items 67168.
I0302 18:59:12.996438 22447428132992 run.py:483] Algo bellman_ford step 2099 current loss 1.137937, current_train_items 67200.
I0302 18:59:13.015687 22447428132992 run.py:483] Algo bellman_ford step 2100 current loss 0.814760, current_train_items 67232.
I0302 18:59:13.023427 22447428132992 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:59:13.023534 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 18:59:13.039924 22447428132992 run.py:483] Algo bellman_ford step 2101 current loss 0.956098, current_train_items 67264.
I0302 18:59:13.062855 22447428132992 run.py:483] Algo bellman_ford step 2102 current loss 1.133754, current_train_items 67296.
I0302 18:59:13.092241 22447428132992 run.py:483] Algo bellman_ford step 2103 current loss 1.258524, current_train_items 67328.
I0302 18:59:13.124470 22447428132992 run.py:483] Algo bellman_ford step 2104 current loss 1.278554, current_train_items 67360.
I0302 18:59:13.143998 22447428132992 run.py:483] Algo bellman_ford step 2105 current loss 0.847361, current_train_items 67392.
I0302 18:59:13.159040 22447428132992 run.py:483] Algo bellman_ford step 2106 current loss 0.815902, current_train_items 67424.
I0302 18:59:13.182617 22447428132992 run.py:483] Algo bellman_ford step 2107 current loss 1.073455, current_train_items 67456.
I0302 18:59:13.213272 22447428132992 run.py:483] Algo bellman_ford step 2108 current loss 1.454971, current_train_items 67488.
I0302 18:59:13.244155 22447428132992 run.py:483] Algo bellman_ford step 2109 current loss 2.072402, current_train_items 67520.
I0302 18:59:13.263254 22447428132992 run.py:483] Algo bellman_ford step 2110 current loss 0.644683, current_train_items 67552.
I0302 18:59:13.279440 22447428132992 run.py:483] Algo bellman_ford step 2111 current loss 0.888509, current_train_items 67584.
I0302 18:59:13.301884 22447428132992 run.py:483] Algo bellman_ford step 2112 current loss 1.289650, current_train_items 67616.
I0302 18:59:13.330396 22447428132992 run.py:483] Algo bellman_ford step 2113 current loss 1.215246, current_train_items 67648.
I0302 18:59:13.361329 22447428132992 run.py:483] Algo bellman_ford step 2114 current loss 1.349854, current_train_items 67680.
I0302 18:59:13.380289 22447428132992 run.py:483] Algo bellman_ford step 2115 current loss 0.710320, current_train_items 67712.
I0302 18:59:13.396365 22447428132992 run.py:483] Algo bellman_ford step 2116 current loss 0.820666, current_train_items 67744.
I0302 18:59:13.420088 22447428132992 run.py:483] Algo bellman_ford step 2117 current loss 1.169541, current_train_items 67776.
I0302 18:59:13.449995 22447428132992 run.py:483] Algo bellman_ford step 2118 current loss 1.233037, current_train_items 67808.
I0302 18:59:13.481045 22447428132992 run.py:483] Algo bellman_ford step 2119 current loss 1.204465, current_train_items 67840.
I0302 18:59:13.500182 22447428132992 run.py:483] Algo bellman_ford step 2120 current loss 0.685474, current_train_items 67872.
I0302 18:59:13.515973 22447428132992 run.py:483] Algo bellman_ford step 2121 current loss 0.831709, current_train_items 67904.
I0302 18:59:13.538677 22447428132992 run.py:483] Algo bellman_ford step 2122 current loss 0.899268, current_train_items 67936.
I0302 18:59:13.568316 22447428132992 run.py:483] Algo bellman_ford step 2123 current loss 1.008417, current_train_items 67968.
I0302 18:59:13.596720 22447428132992 run.py:483] Algo bellman_ford step 2124 current loss 1.596399, current_train_items 68000.
I0302 18:59:13.615516 22447428132992 run.py:483] Algo bellman_ford step 2125 current loss 0.535642, current_train_items 68032.
I0302 18:59:13.632000 22447428132992 run.py:483] Algo bellman_ford step 2126 current loss 0.911818, current_train_items 68064.
I0302 18:59:13.656394 22447428132992 run.py:483] Algo bellman_ford step 2127 current loss 1.207521, current_train_items 68096.
I0302 18:59:13.684978 22447428132992 run.py:483] Algo bellman_ford step 2128 current loss 1.225342, current_train_items 68128.
I0302 18:59:13.716414 22447428132992 run.py:483] Algo bellman_ford step 2129 current loss 1.295263, current_train_items 68160.
I0302 18:59:13.735662 22447428132992 run.py:483] Algo bellman_ford step 2130 current loss 0.640061, current_train_items 68192.
I0302 18:59:13.752274 22447428132992 run.py:483] Algo bellman_ford step 2131 current loss 0.922046, current_train_items 68224.
I0302 18:59:13.775840 22447428132992 run.py:483] Algo bellman_ford step 2132 current loss 1.075747, current_train_items 68256.
I0302 18:59:13.804892 22447428132992 run.py:483] Algo bellman_ford step 2133 current loss 1.187823, current_train_items 68288.
I0302 18:59:13.838370 22447428132992 run.py:483] Algo bellman_ford step 2134 current loss 1.211667, current_train_items 68320.
I0302 18:59:13.857234 22447428132992 run.py:483] Algo bellman_ford step 2135 current loss 0.623752, current_train_items 68352.
I0302 18:59:13.873562 22447428132992 run.py:483] Algo bellman_ford step 2136 current loss 0.861227, current_train_items 68384.
I0302 18:59:13.896125 22447428132992 run.py:483] Algo bellman_ford step 2137 current loss 1.001161, current_train_items 68416.
I0302 18:59:13.925717 22447428132992 run.py:483] Algo bellman_ford step 2138 current loss 1.318329, current_train_items 68448.
I0302 18:59:13.960526 22447428132992 run.py:483] Algo bellman_ford step 2139 current loss 1.464810, current_train_items 68480.
I0302 18:59:13.979336 22447428132992 run.py:483] Algo bellman_ford step 2140 current loss 0.725752, current_train_items 68512.
I0302 18:59:13.995315 22447428132992 run.py:483] Algo bellman_ford step 2141 current loss 0.754927, current_train_items 68544.
I0302 18:59:14.016977 22447428132992 run.py:483] Algo bellman_ford step 2142 current loss 0.922789, current_train_items 68576.
I0302 18:59:14.045733 22447428132992 run.py:483] Algo bellman_ford step 2143 current loss 1.052389, current_train_items 68608.
I0302 18:59:14.077929 22447428132992 run.py:483] Algo bellman_ford step 2144 current loss 1.241168, current_train_items 68640.
I0302 18:59:14.096746 22447428132992 run.py:483] Algo bellman_ford step 2145 current loss 0.605377, current_train_items 68672.
I0302 18:59:14.113270 22447428132992 run.py:483] Algo bellman_ford step 2146 current loss 0.912076, current_train_items 68704.
I0302 18:59:14.136623 22447428132992 run.py:483] Algo bellman_ford step 2147 current loss 1.225878, current_train_items 68736.
I0302 18:59:14.166247 22447428132992 run.py:483] Algo bellman_ford step 2148 current loss 1.257450, current_train_items 68768.
I0302 18:59:14.197252 22447428132992 run.py:483] Algo bellman_ford step 2149 current loss 1.166734, current_train_items 68800.
I0302 18:59:14.216052 22447428132992 run.py:483] Algo bellman_ford step 2150 current loss 0.656791, current_train_items 68832.
I0302 18:59:14.224202 22447428132992 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:59:14.224308 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 18:59:14.241053 22447428132992 run.py:483] Algo bellman_ford step 2151 current loss 0.803485, current_train_items 68864.
I0302 18:59:14.264431 22447428132992 run.py:483] Algo bellman_ford step 2152 current loss 0.980605, current_train_items 68896.
I0302 18:59:14.293714 22447428132992 run.py:483] Algo bellman_ford step 2153 current loss 1.258030, current_train_items 68928.
I0302 18:59:14.325964 22447428132992 run.py:483] Algo bellman_ford step 2154 current loss 1.537200, current_train_items 68960.
I0302 18:59:14.345184 22447428132992 run.py:483] Algo bellman_ford step 2155 current loss 0.626392, current_train_items 68992.
I0302 18:59:14.360931 22447428132992 run.py:483] Algo bellman_ford step 2156 current loss 0.788964, current_train_items 69024.
I0302 18:59:14.384504 22447428132992 run.py:483] Algo bellman_ford step 2157 current loss 1.060458, current_train_items 69056.
I0302 18:59:14.414148 22447428132992 run.py:483] Algo bellman_ford step 2158 current loss 1.151891, current_train_items 69088.
I0302 18:59:14.447066 22447428132992 run.py:483] Algo bellman_ford step 2159 current loss 1.380374, current_train_items 69120.
I0302 18:59:14.466795 22447428132992 run.py:483] Algo bellman_ford step 2160 current loss 0.731293, current_train_items 69152.
I0302 18:59:14.482802 22447428132992 run.py:483] Algo bellman_ford step 2161 current loss 0.750022, current_train_items 69184.
I0302 18:59:14.505986 22447428132992 run.py:483] Algo bellman_ford step 2162 current loss 1.055615, current_train_items 69216.
I0302 18:59:14.533791 22447428132992 run.py:483] Algo bellman_ford step 2163 current loss 1.106778, current_train_items 69248.
I0302 18:59:14.567044 22447428132992 run.py:483] Algo bellman_ford step 2164 current loss 1.457858, current_train_items 69280.
I0302 18:59:14.586265 22447428132992 run.py:483] Algo bellman_ford step 2165 current loss 0.809421, current_train_items 69312.
I0302 18:59:14.602777 22447428132992 run.py:483] Algo bellman_ford step 2166 current loss 0.891625, current_train_items 69344.
I0302 18:59:14.625312 22447428132992 run.py:483] Algo bellman_ford step 2167 current loss 0.995760, current_train_items 69376.
I0302 18:59:14.653115 22447428132992 run.py:483] Algo bellman_ford step 2168 current loss 1.052727, current_train_items 69408.
I0302 18:59:14.685309 22447428132992 run.py:483] Algo bellman_ford step 2169 current loss 1.769750, current_train_items 69440.
I0302 18:59:14.704720 22447428132992 run.py:483] Algo bellman_ford step 2170 current loss 0.636740, current_train_items 69472.
I0302 18:59:14.720691 22447428132992 run.py:483] Algo bellman_ford step 2171 current loss 0.889856, current_train_items 69504.
I0302 18:59:14.743200 22447428132992 run.py:483] Algo bellman_ford step 2172 current loss 1.020685, current_train_items 69536.
I0302 18:59:14.773286 22447428132992 run.py:483] Algo bellman_ford step 2173 current loss 1.112125, current_train_items 69568.
I0302 18:59:14.803819 22447428132992 run.py:483] Algo bellman_ford step 2174 current loss 1.299658, current_train_items 69600.
I0302 18:59:14.823207 22447428132992 run.py:483] Algo bellman_ford step 2175 current loss 0.623101, current_train_items 69632.
I0302 18:59:14.838938 22447428132992 run.py:483] Algo bellman_ford step 2176 current loss 0.754014, current_train_items 69664.
I0302 18:59:14.862216 22447428132992 run.py:483] Algo bellman_ford step 2177 current loss 1.178298, current_train_items 69696.
I0302 18:59:14.890704 22447428132992 run.py:483] Algo bellman_ford step 2178 current loss 1.048925, current_train_items 69728.
I0302 18:59:14.923437 22447428132992 run.py:483] Algo bellman_ford step 2179 current loss 1.216127, current_train_items 69760.
I0302 18:59:14.942045 22447428132992 run.py:483] Algo bellman_ford step 2180 current loss 0.587540, current_train_items 69792.
I0302 18:59:14.958250 22447428132992 run.py:483] Algo bellman_ford step 2181 current loss 0.814811, current_train_items 69824.
I0302 18:59:14.981350 22447428132992 run.py:483] Algo bellman_ford step 2182 current loss 1.045319, current_train_items 69856.
I0302 18:59:15.009990 22447428132992 run.py:483] Algo bellman_ford step 2183 current loss 1.084361, current_train_items 69888.
I0302 18:59:15.043322 22447428132992 run.py:483] Algo bellman_ford step 2184 current loss 1.444991, current_train_items 69920.
I0302 18:59:15.062913 22447428132992 run.py:483] Algo bellman_ford step 2185 current loss 0.625148, current_train_items 69952.
I0302 18:59:15.078588 22447428132992 run.py:483] Algo bellman_ford step 2186 current loss 0.630494, current_train_items 69984.
I0302 18:59:15.101506 22447428132992 run.py:483] Algo bellman_ford step 2187 current loss 0.949351, current_train_items 70016.
I0302 18:59:15.130782 22447428132992 run.py:483] Algo bellman_ford step 2188 current loss 1.158733, current_train_items 70048.
W0302 18:59:15.155337 22447428132992 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:59:21.762185 22447428132992 run.py:483] Algo bellman_ford step 2189 current loss 1.530806, current_train_items 70080.
I0302 18:59:21.782524 22447428132992 run.py:483] Algo bellman_ford step 2190 current loss 0.586041, current_train_items 70112.
I0302 18:59:21.798958 22447428132992 run.py:483] Algo bellman_ford step 2191 current loss 0.845289, current_train_items 70144.
I0302 18:59:21.821989 22447428132992 run.py:483] Algo bellman_ford step 2192 current loss 1.050753, current_train_items 70176.
W0302 18:59:21.842925 22447428132992 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:59:28.674010 22447428132992 run.py:483] Algo bellman_ford step 2193 current loss 1.441971, current_train_items 70208.
I0302 18:59:28.705969 22447428132992 run.py:483] Algo bellman_ford step 2194 current loss 32147.419922, current_train_items 70240.
I0302 18:59:28.725739 22447428132992 run.py:483] Algo bellman_ford step 2195 current loss 0.696002, current_train_items 70272.
I0302 18:59:28.742303 22447428132992 run.py:483] Algo bellman_ford step 2196 current loss 0.845890, current_train_items 70304.
I0302 18:59:28.764906 22447428132992 run.py:483] Algo bellman_ford step 2197 current loss 1.012956, current_train_items 70336.
I0302 18:59:28.792948 22447428132992 run.py:483] Algo bellman_ford step 2198 current loss 1.153046, current_train_items 70368.
I0302 18:59:28.825432 22447428132992 run.py:483] Algo bellman_ford step 2199 current loss 1.672891, current_train_items 70400.
I0302 18:59:28.844991 22447428132992 run.py:483] Algo bellman_ford step 2200 current loss 0.573881, current_train_items 70432.
I0302 18:59:28.854454 22447428132992 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:59:28.854589 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 18:59:28.871513 22447428132992 run.py:483] Algo bellman_ford step 2201 current loss 0.828775, current_train_items 70464.
I0302 18:59:28.894268 22447428132992 run.py:483] Algo bellman_ford step 2202 current loss 0.922900, current_train_items 70496.
I0302 18:59:28.923680 22447428132992 run.py:483] Algo bellman_ford step 2203 current loss 1.047120, current_train_items 70528.
I0302 18:59:28.957293 22447428132992 run.py:483] Algo bellman_ford step 2204 current loss 1.136183, current_train_items 70560.
I0302 18:59:28.977246 22447428132992 run.py:483] Algo bellman_ford step 2205 current loss 0.714470, current_train_items 70592.
I0302 18:59:28.993326 22447428132992 run.py:483] Algo bellman_ford step 2206 current loss 0.762317, current_train_items 70624.
I0302 18:59:29.015820 22447428132992 run.py:483] Algo bellman_ford step 2207 current loss 1.000756, current_train_items 70656.
I0302 18:59:29.045321 22447428132992 run.py:483] Algo bellman_ford step 2208 current loss 1.190844, current_train_items 70688.
I0302 18:59:29.078909 22447428132992 run.py:483] Algo bellman_ford step 2209 current loss 1.367666, current_train_items 70720.
I0302 18:59:29.098401 22447428132992 run.py:483] Algo bellman_ford step 2210 current loss 0.618876, current_train_items 70752.
I0302 18:59:29.114607 22447428132992 run.py:483] Algo bellman_ford step 2211 current loss 0.820323, current_train_items 70784.
I0302 18:59:29.137882 22447428132992 run.py:483] Algo bellman_ford step 2212 current loss 1.144231, current_train_items 70816.
I0302 18:59:29.167933 22447428132992 run.py:483] Algo bellman_ford step 2213 current loss 1.553905, current_train_items 70848.
I0302 18:59:29.201691 22447428132992 run.py:483] Algo bellman_ford step 2214 current loss 1.453097, current_train_items 70880.
I0302 18:59:29.221281 22447428132992 run.py:483] Algo bellman_ford step 2215 current loss 0.598942, current_train_items 70912.
I0302 18:59:29.237391 22447428132992 run.py:483] Algo bellman_ford step 2216 current loss 0.815239, current_train_items 70944.
I0302 18:59:29.259151 22447428132992 run.py:483] Algo bellman_ford step 2217 current loss 1.034930, current_train_items 70976.
I0302 18:59:29.289618 22447428132992 run.py:483] Algo bellman_ford step 2218 current loss 1.301293, current_train_items 71008.
I0302 18:59:29.321342 22447428132992 run.py:483] Algo bellman_ford step 2219 current loss 1.272228, current_train_items 71040.
I0302 18:59:29.340710 22447428132992 run.py:483] Algo bellman_ford step 2220 current loss 0.653285, current_train_items 71072.
I0302 18:59:29.356836 22447428132992 run.py:483] Algo bellman_ford step 2221 current loss 0.958869, current_train_items 71104.
I0302 18:59:29.379334 22447428132992 run.py:483] Algo bellman_ford step 2222 current loss 1.045892, current_train_items 71136.
I0302 18:59:29.409586 22447428132992 run.py:483] Algo bellman_ford step 2223 current loss 1.157228, current_train_items 71168.
I0302 18:59:29.443483 22447428132992 run.py:483] Algo bellman_ford step 2224 current loss 1.293193, current_train_items 71200.
I0302 18:59:29.462712 22447428132992 run.py:483] Algo bellman_ford step 2225 current loss 0.683313, current_train_items 71232.
I0302 18:59:29.478942 22447428132992 run.py:483] Algo bellman_ford step 2226 current loss 0.911333, current_train_items 71264.
I0302 18:59:29.501930 22447428132992 run.py:483] Algo bellman_ford step 2227 current loss 1.018124, current_train_items 71296.
I0302 18:59:29.531805 22447428132992 run.py:483] Algo bellman_ford step 2228 current loss 1.135385, current_train_items 71328.
I0302 18:59:29.562288 22447428132992 run.py:483] Algo bellman_ford step 2229 current loss 1.189576, current_train_items 71360.
I0302 18:59:29.581879 22447428132992 run.py:483] Algo bellman_ford step 2230 current loss 0.692138, current_train_items 71392.
I0302 18:59:29.598242 22447428132992 run.py:483] Algo bellman_ford step 2231 current loss 0.858877, current_train_items 71424.
I0302 18:59:29.621041 22447428132992 run.py:483] Algo bellman_ford step 2232 current loss 1.063893, current_train_items 71456.
I0302 18:59:29.650130 22447428132992 run.py:483] Algo bellman_ford step 2233 current loss 1.087910, current_train_items 71488.
I0302 18:59:29.682339 22447428132992 run.py:483] Algo bellman_ford step 2234 current loss 1.250901, current_train_items 71520.
I0302 18:59:29.701976 22447428132992 run.py:483] Algo bellman_ford step 2235 current loss 0.641579, current_train_items 71552.
I0302 18:59:29.718276 22447428132992 run.py:483] Algo bellman_ford step 2236 current loss 0.841255, current_train_items 71584.
I0302 18:59:29.742190 22447428132992 run.py:483] Algo bellman_ford step 2237 current loss 1.024380, current_train_items 71616.
I0302 18:59:29.772587 22447428132992 run.py:483] Algo bellman_ford step 2238 current loss 1.235760, current_train_items 71648.
I0302 18:59:29.804217 22447428132992 run.py:483] Algo bellman_ford step 2239 current loss 1.327944, current_train_items 71680.
I0302 18:59:29.823596 22447428132992 run.py:483] Algo bellman_ford step 2240 current loss 0.734726, current_train_items 71712.
I0302 18:59:29.840130 22447428132992 run.py:483] Algo bellman_ford step 2241 current loss 0.958020, current_train_items 71744.
I0302 18:59:29.863857 22447428132992 run.py:483] Algo bellman_ford step 2242 current loss 0.989659, current_train_items 71776.
I0302 18:59:29.893783 22447428132992 run.py:483] Algo bellman_ford step 2243 current loss 1.270753, current_train_items 71808.
I0302 18:59:29.924868 22447428132992 run.py:483] Algo bellman_ford step 2244 current loss 1.216305, current_train_items 71840.
I0302 18:59:29.944196 22447428132992 run.py:483] Algo bellman_ford step 2245 current loss 0.623406, current_train_items 71872.
I0302 18:59:29.960546 22447428132992 run.py:483] Algo bellman_ford step 2246 current loss 0.869574, current_train_items 71904.
I0302 18:59:29.982871 22447428132992 run.py:483] Algo bellman_ford step 2247 current loss 1.017747, current_train_items 71936.
I0302 18:59:30.013775 22447428132992 run.py:483] Algo bellman_ford step 2248 current loss 1.107502, current_train_items 71968.
I0302 18:59:30.046700 22447428132992 run.py:483] Algo bellman_ford step 2249 current loss 1.196373, current_train_items 72000.
I0302 18:59:30.066191 22447428132992 run.py:483] Algo bellman_ford step 2250 current loss 0.844647, current_train_items 72032.
I0302 18:59:30.074597 22447428132992 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:59:30.074708 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.902, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 18:59:30.091305 22447428132992 run.py:483] Algo bellman_ford step 2251 current loss 0.972511, current_train_items 72064.
I0302 18:59:30.113859 22447428132992 run.py:483] Algo bellman_ford step 2252 current loss 0.991971, current_train_items 72096.
I0302 18:59:30.145190 22447428132992 run.py:483] Algo bellman_ford step 2253 current loss 1.175531, current_train_items 72128.
I0302 18:59:30.179211 22447428132992 run.py:483] Algo bellman_ford step 2254 current loss 1.102098, current_train_items 72160.
I0302 18:59:30.199025 22447428132992 run.py:483] Algo bellman_ford step 2255 current loss 0.684999, current_train_items 72192.
I0302 18:59:30.214943 22447428132992 run.py:483] Algo bellman_ford step 2256 current loss 0.822868, current_train_items 72224.
I0302 18:59:30.237828 22447428132992 run.py:483] Algo bellman_ford step 2257 current loss 1.045578, current_train_items 72256.
I0302 18:59:30.266317 22447428132992 run.py:483] Algo bellman_ford step 2258 current loss 1.095463, current_train_items 72288.
I0302 18:59:30.300215 22447428132992 run.py:483] Algo bellman_ford step 2259 current loss 1.777048, current_train_items 72320.
I0302 18:59:30.320125 22447428132992 run.py:483] Algo bellman_ford step 2260 current loss 0.617991, current_train_items 72352.
I0302 18:59:30.336796 22447428132992 run.py:483] Algo bellman_ford step 2261 current loss 0.925858, current_train_items 72384.
I0302 18:59:30.360779 22447428132992 run.py:483] Algo bellman_ford step 2262 current loss 1.149097, current_train_items 72416.
I0302 18:59:30.389604 22447428132992 run.py:483] Algo bellman_ford step 2263 current loss 1.153259, current_train_items 72448.
I0302 18:59:30.422235 22447428132992 run.py:483] Algo bellman_ford step 2264 current loss 1.300501, current_train_items 72480.
I0302 18:59:30.441623 22447428132992 run.py:483] Algo bellman_ford step 2265 current loss 0.545207, current_train_items 72512.
I0302 18:59:30.457723 22447428132992 run.py:483] Algo bellman_ford step 2266 current loss 0.785502, current_train_items 72544.
I0302 18:59:30.481365 22447428132992 run.py:483] Algo bellman_ford step 2267 current loss 1.032529, current_train_items 72576.
I0302 18:59:30.511536 22447428132992 run.py:483] Algo bellman_ford step 2268 current loss 1.174533, current_train_items 72608.
I0302 18:59:30.544321 22447428132992 run.py:483] Algo bellman_ford step 2269 current loss 1.194883, current_train_items 72640.
I0302 18:59:30.564180 22447428132992 run.py:483] Algo bellman_ford step 2270 current loss 0.729545, current_train_items 72672.
I0302 18:59:30.580176 22447428132992 run.py:483] Algo bellman_ford step 2271 current loss 0.730501, current_train_items 72704.
I0302 18:59:30.601944 22447428132992 run.py:483] Algo bellman_ford step 2272 current loss 0.965539, current_train_items 72736.
I0302 18:59:30.632632 22447428132992 run.py:483] Algo bellman_ford step 2273 current loss 1.163979, current_train_items 72768.
I0302 18:59:30.667767 22447428132992 run.py:483] Algo bellman_ford step 2274 current loss 1.299089, current_train_items 72800.
I0302 18:59:30.687463 22447428132992 run.py:483] Algo bellman_ford step 2275 current loss 0.721168, current_train_items 72832.
I0302 18:59:30.703491 22447428132992 run.py:483] Algo bellman_ford step 2276 current loss 0.742737, current_train_items 72864.
I0302 18:59:30.724495 22447428132992 run.py:483] Algo bellman_ford step 2277 current loss 0.916723, current_train_items 72896.
I0302 18:59:30.753669 22447428132992 run.py:483] Algo bellman_ford step 2278 current loss 1.104339, current_train_items 72928.
I0302 18:59:30.787680 22447428132992 run.py:483] Algo bellman_ford step 2279 current loss 2.680647, current_train_items 72960.
I0302 18:59:30.806951 22447428132992 run.py:483] Algo bellman_ford step 2280 current loss 0.560319, current_train_items 72992.
I0302 18:59:30.822747 22447428132992 run.py:483] Algo bellman_ford step 2281 current loss 0.889943, current_train_items 73024.
I0302 18:59:30.846107 22447428132992 run.py:483] Algo bellman_ford step 2282 current loss 1.026426, current_train_items 73056.
I0302 18:59:30.878441 22447428132992 run.py:483] Algo bellman_ford step 2283 current loss 1.313575, current_train_items 73088.
I0302 18:59:30.912412 22447428132992 run.py:483] Algo bellman_ford step 2284 current loss 1.743961, current_train_items 73120.
I0302 18:59:30.932114 22447428132992 run.py:483] Algo bellman_ford step 2285 current loss 0.683606, current_train_items 73152.
I0302 18:59:30.948270 22447428132992 run.py:483] Algo bellman_ford step 2286 current loss 0.786341, current_train_items 73184.
I0302 18:59:30.970728 22447428132992 run.py:483] Algo bellman_ford step 2287 current loss 0.942078, current_train_items 73216.
I0302 18:59:30.999907 22447428132992 run.py:483] Algo bellman_ford step 2288 current loss 1.136830, current_train_items 73248.
I0302 18:59:31.035078 22447428132992 run.py:483] Algo bellman_ford step 2289 current loss 1.313167, current_train_items 73280.
I0302 18:59:31.054921 22447428132992 run.py:483] Algo bellman_ford step 2290 current loss 0.638443, current_train_items 73312.
I0302 18:59:31.070521 22447428132992 run.py:483] Algo bellman_ford step 2291 current loss 0.787788, current_train_items 73344.
I0302 18:59:31.092658 22447428132992 run.py:483] Algo bellman_ford step 2292 current loss 0.913742, current_train_items 73376.
I0302 18:59:31.122828 22447428132992 run.py:483] Algo bellman_ford step 2293 current loss 1.081330, current_train_items 73408.
I0302 18:59:31.156175 22447428132992 run.py:483] Algo bellman_ford step 2294 current loss 1.230332, current_train_items 73440.
I0302 18:59:31.175466 22447428132992 run.py:483] Algo bellman_ford step 2295 current loss 0.607102, current_train_items 73472.
I0302 18:59:31.191605 22447428132992 run.py:483] Algo bellman_ford step 2296 current loss 0.774344, current_train_items 73504.
I0302 18:59:31.213587 22447428132992 run.py:483] Algo bellman_ford step 2297 current loss 0.925336, current_train_items 73536.
I0302 18:59:31.242374 22447428132992 run.py:483] Algo bellman_ford step 2298 current loss 1.146309, current_train_items 73568.
I0302 18:59:31.275433 22447428132992 run.py:483] Algo bellman_ford step 2299 current loss 1.331653, current_train_items 73600.
I0302 18:59:31.295095 22447428132992 run.py:483] Algo bellman_ford step 2300 current loss 0.557360, current_train_items 73632.
I0302 18:59:31.303018 22447428132992 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:59:31.303123 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.902, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 18:59:31.332813 22447428132992 run.py:483] Algo bellman_ford step 2301 current loss 0.895386, current_train_items 73664.
I0302 18:59:31.356389 22447428132992 run.py:483] Algo bellman_ford step 2302 current loss 1.022126, current_train_items 73696.
I0302 18:59:31.387945 22447428132992 run.py:483] Algo bellman_ford step 2303 current loss 1.096291, current_train_items 73728.
I0302 18:59:31.422183 22447428132992 run.py:483] Algo bellman_ford step 2304 current loss 1.230803, current_train_items 73760.
I0302 18:59:31.442492 22447428132992 run.py:483] Algo bellman_ford step 2305 current loss 0.725995, current_train_items 73792.
I0302 18:59:31.458893 22447428132992 run.py:483] Algo bellman_ford step 2306 current loss 0.934888, current_train_items 73824.
I0302 18:59:31.481828 22447428132992 run.py:483] Algo bellman_ford step 2307 current loss 1.058904, current_train_items 73856.
I0302 18:59:31.510773 22447428132992 run.py:483] Algo bellman_ford step 2308 current loss 1.094423, current_train_items 73888.
I0302 18:59:31.544421 22447428132992 run.py:483] Algo bellman_ford step 2309 current loss 1.238583, current_train_items 73920.
I0302 18:59:31.563831 22447428132992 run.py:483] Algo bellman_ford step 2310 current loss 0.739835, current_train_items 73952.
I0302 18:59:31.580136 22447428132992 run.py:483] Algo bellman_ford step 2311 current loss 0.771693, current_train_items 73984.
I0302 18:59:31.602810 22447428132992 run.py:483] Algo bellman_ford step 2312 current loss 1.039723, current_train_items 74016.
I0302 18:59:31.632738 22447428132992 run.py:483] Algo bellman_ford step 2313 current loss 1.004899, current_train_items 74048.
I0302 18:59:31.666033 22447428132992 run.py:483] Algo bellman_ford step 2314 current loss 1.178977, current_train_items 74080.
I0302 18:59:31.685467 22447428132992 run.py:483] Algo bellman_ford step 2315 current loss 0.643454, current_train_items 74112.
I0302 18:59:31.701461 22447428132992 run.py:483] Algo bellman_ford step 2316 current loss 0.861934, current_train_items 74144.
I0302 18:59:31.723773 22447428132992 run.py:483] Algo bellman_ford step 2317 current loss 0.980996, current_train_items 74176.
I0302 18:59:31.754342 22447428132992 run.py:483] Algo bellman_ford step 2318 current loss 1.138551, current_train_items 74208.
I0302 18:59:31.788207 22447428132992 run.py:483] Algo bellman_ford step 2319 current loss 1.336485, current_train_items 74240.
I0302 18:59:31.807722 22447428132992 run.py:483] Algo bellman_ford step 2320 current loss 0.626679, current_train_items 74272.
I0302 18:59:31.823677 22447428132992 run.py:483] Algo bellman_ford step 2321 current loss 0.739253, current_train_items 74304.
I0302 18:59:31.847185 22447428132992 run.py:483] Algo bellman_ford step 2322 current loss 1.066716, current_train_items 74336.
I0302 18:59:31.876521 22447428132992 run.py:483] Algo bellman_ford step 2323 current loss 1.143844, current_train_items 74368.
I0302 18:59:31.909780 22447428132992 run.py:483] Algo bellman_ford step 2324 current loss 1.561237, current_train_items 74400.
I0302 18:59:31.929296 22447428132992 run.py:483] Algo bellman_ford step 2325 current loss 0.615185, current_train_items 74432.
I0302 18:59:31.945249 22447428132992 run.py:483] Algo bellman_ford step 2326 current loss 0.847175, current_train_items 74464.
I0302 18:59:31.967411 22447428132992 run.py:483] Algo bellman_ford step 2327 current loss 0.959097, current_train_items 74496.
I0302 18:59:31.995834 22447428132992 run.py:483] Algo bellman_ford step 2328 current loss 1.023702, current_train_items 74528.
I0302 18:59:32.027732 22447428132992 run.py:483] Algo bellman_ford step 2329 current loss 1.311632, current_train_items 74560.
I0302 18:59:32.047544 22447428132992 run.py:483] Algo bellman_ford step 2330 current loss 0.767152, current_train_items 74592.
I0302 18:59:32.063124 22447428132992 run.py:483] Algo bellman_ford step 2331 current loss 0.850061, current_train_items 74624.
I0302 18:59:32.086256 22447428132992 run.py:483] Algo bellman_ford step 2332 current loss 1.104133, current_train_items 74656.
I0302 18:59:32.116547 22447428132992 run.py:483] Algo bellman_ford step 2333 current loss 1.123728, current_train_items 74688.
I0302 18:59:32.150830 22447428132992 run.py:483] Algo bellman_ford step 2334 current loss 1.147273, current_train_items 74720.
I0302 18:59:32.170178 22447428132992 run.py:483] Algo bellman_ford step 2335 current loss 0.614660, current_train_items 74752.
I0302 18:59:32.186553 22447428132992 run.py:483] Algo bellman_ford step 2336 current loss 0.820108, current_train_items 74784.
I0302 18:59:32.209442 22447428132992 run.py:483] Algo bellman_ford step 2337 current loss 1.004343, current_train_items 74816.
I0302 18:59:32.238912 22447428132992 run.py:483] Algo bellman_ford step 2338 current loss 1.272152, current_train_items 74848.
I0302 18:59:32.271175 22447428132992 run.py:483] Algo bellman_ford step 2339 current loss 1.387610, current_train_items 74880.
I0302 18:59:32.290716 22447428132992 run.py:483] Algo bellman_ford step 2340 current loss 0.596446, current_train_items 74912.
I0302 18:59:32.307015 22447428132992 run.py:483] Algo bellman_ford step 2341 current loss 0.823690, current_train_items 74944.
I0302 18:59:32.328931 22447428132992 run.py:483] Algo bellman_ford step 2342 current loss 0.966055, current_train_items 74976.
I0302 18:59:32.357810 22447428132992 run.py:483] Algo bellman_ford step 2343 current loss 1.148134, current_train_items 75008.
I0302 18:59:32.389198 22447428132992 run.py:483] Algo bellman_ford step 2344 current loss 1.221765, current_train_items 75040.
I0302 18:59:32.408665 22447428132992 run.py:483] Algo bellman_ford step 2345 current loss 0.771486, current_train_items 75072.
I0302 18:59:32.424569 22447428132992 run.py:483] Algo bellman_ford step 2346 current loss 0.846391, current_train_items 75104.
I0302 18:59:32.447290 22447428132992 run.py:483] Algo bellman_ford step 2347 current loss 1.021700, current_train_items 75136.
I0302 18:59:32.475894 22447428132992 run.py:483] Algo bellman_ford step 2348 current loss 1.017045, current_train_items 75168.
I0302 18:59:32.507255 22447428132992 run.py:483] Algo bellman_ford step 2349 current loss 1.081102, current_train_items 75200.
I0302 18:59:32.526553 22447428132992 run.py:483] Algo bellman_ford step 2350 current loss 0.666162, current_train_items 75232.
I0302 18:59:32.534914 22447428132992 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:59:32.535018 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.903, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:59:32.551646 22447428132992 run.py:483] Algo bellman_ford step 2351 current loss 0.842695, current_train_items 75264.
I0302 18:59:32.574847 22447428132992 run.py:483] Algo bellman_ford step 2352 current loss 0.957572, current_train_items 75296.
I0302 18:59:32.603830 22447428132992 run.py:483] Algo bellman_ford step 2353 current loss 0.996105, current_train_items 75328.
I0302 18:59:32.639003 22447428132992 run.py:483] Algo bellman_ford step 2354 current loss 1.624200, current_train_items 75360.
I0302 18:59:32.659095 22447428132992 run.py:483] Algo bellman_ford step 2355 current loss 0.703289, current_train_items 75392.
I0302 18:59:32.674586 22447428132992 run.py:483] Algo bellman_ford step 2356 current loss 0.781547, current_train_items 75424.
I0302 18:59:32.698066 22447428132992 run.py:483] Algo bellman_ford step 2357 current loss 1.022698, current_train_items 75456.
I0302 18:59:32.726758 22447428132992 run.py:483] Algo bellman_ford step 2358 current loss 0.905811, current_train_items 75488.
I0302 18:59:32.761701 22447428132992 run.py:483] Algo bellman_ford step 2359 current loss 1.624531, current_train_items 75520.
I0302 18:59:32.781414 22447428132992 run.py:483] Algo bellman_ford step 2360 current loss 0.603634, current_train_items 75552.
I0302 18:59:32.798281 22447428132992 run.py:483] Algo bellman_ford step 2361 current loss 0.873091, current_train_items 75584.
I0302 18:59:32.822324 22447428132992 run.py:483] Algo bellman_ford step 2362 current loss 1.068531, current_train_items 75616.
I0302 18:59:32.851482 22447428132992 run.py:483] Algo bellman_ford step 2363 current loss 1.093906, current_train_items 75648.
I0302 18:59:32.884584 22447428132992 run.py:483] Algo bellman_ford step 2364 current loss 1.216917, current_train_items 75680.
I0302 18:59:32.904035 22447428132992 run.py:483] Algo bellman_ford step 2365 current loss 0.574696, current_train_items 75712.
I0302 18:59:32.920303 22447428132992 run.py:483] Algo bellman_ford step 2366 current loss 0.747005, current_train_items 75744.
I0302 18:59:32.943653 22447428132992 run.py:483] Algo bellman_ford step 2367 current loss 1.054919, current_train_items 75776.
I0302 18:59:32.974346 22447428132992 run.py:483] Algo bellman_ford step 2368 current loss 1.172668, current_train_items 75808.
I0302 18:59:33.008596 22447428132992 run.py:483] Algo bellman_ford step 2369 current loss 1.374136, current_train_items 75840.
I0302 18:59:33.028846 22447428132992 run.py:483] Algo bellman_ford step 2370 current loss 0.609328, current_train_items 75872.
I0302 18:59:33.044909 22447428132992 run.py:483] Algo bellman_ford step 2371 current loss 0.857280, current_train_items 75904.
I0302 18:59:33.067724 22447428132992 run.py:483] Algo bellman_ford step 2372 current loss 0.971072, current_train_items 75936.
I0302 18:59:33.096737 22447428132992 run.py:483] Algo bellman_ford step 2373 current loss 1.211332, current_train_items 75968.
I0302 18:59:33.129178 22447428132992 run.py:483] Algo bellman_ford step 2374 current loss 1.161820, current_train_items 76000.
I0302 18:59:33.148853 22447428132992 run.py:483] Algo bellman_ford step 2375 current loss 0.698936, current_train_items 76032.
I0302 18:59:33.165121 22447428132992 run.py:483] Algo bellman_ford step 2376 current loss 0.808150, current_train_items 76064.
I0302 18:59:33.188080 22447428132992 run.py:483] Algo bellman_ford step 2377 current loss 1.049564, current_train_items 76096.
I0302 18:59:33.219101 22447428132992 run.py:483] Algo bellman_ford step 2378 current loss 1.051887, current_train_items 76128.
I0302 18:59:33.253532 22447428132992 run.py:483] Algo bellman_ford step 2379 current loss 1.392056, current_train_items 76160.
I0302 18:59:33.273123 22447428132992 run.py:483] Algo bellman_ford step 2380 current loss 0.622607, current_train_items 76192.
I0302 18:59:33.289674 22447428132992 run.py:483] Algo bellman_ford step 2381 current loss 0.876869, current_train_items 76224.
I0302 18:59:33.313446 22447428132992 run.py:483] Algo bellman_ford step 2382 current loss 1.103351, current_train_items 76256.
I0302 18:59:33.342164 22447428132992 run.py:483] Algo bellman_ford step 2383 current loss 0.998583, current_train_items 76288.
I0302 18:59:33.375727 22447428132992 run.py:483] Algo bellman_ford step 2384 current loss 1.349379, current_train_items 76320.
I0302 18:59:33.395596 22447428132992 run.py:483] Algo bellman_ford step 2385 current loss 0.593866, current_train_items 76352.
I0302 18:59:33.411496 22447428132992 run.py:483] Algo bellman_ford step 2386 current loss 0.763233, current_train_items 76384.
I0302 18:59:33.433793 22447428132992 run.py:483] Algo bellman_ford step 2387 current loss 1.072104, current_train_items 76416.
I0302 18:59:33.463701 22447428132992 run.py:483] Algo bellman_ford step 2388 current loss 1.049328, current_train_items 76448.
I0302 18:59:33.498562 22447428132992 run.py:483] Algo bellman_ford step 2389 current loss 1.154310, current_train_items 76480.
I0302 18:59:33.518606 22447428132992 run.py:483] Algo bellman_ford step 2390 current loss 0.601969, current_train_items 76512.
I0302 18:59:33.534387 22447428132992 run.py:483] Algo bellman_ford step 2391 current loss 0.673488, current_train_items 76544.
I0302 18:59:33.555586 22447428132992 run.py:483] Algo bellman_ford step 2392 current loss 1.045998, current_train_items 76576.
I0302 18:59:33.585362 22447428132992 run.py:483] Algo bellman_ford step 2393 current loss 1.261510, current_train_items 76608.
I0302 18:59:33.621492 22447428132992 run.py:483] Algo bellman_ford step 2394 current loss 1.796769, current_train_items 76640.
I0302 18:59:33.641392 22447428132992 run.py:483] Algo bellman_ford step 2395 current loss 0.691009, current_train_items 76672.
I0302 18:59:33.657312 22447428132992 run.py:483] Algo bellman_ford step 2396 current loss 0.806993, current_train_items 76704.
I0302 18:59:33.679095 22447428132992 run.py:483] Algo bellman_ford step 2397 current loss 0.906004, current_train_items 76736.
I0302 18:59:33.708315 22447428132992 run.py:483] Algo bellman_ford step 2398 current loss 1.061890, current_train_items 76768.
I0302 18:59:33.742332 22447428132992 run.py:483] Algo bellman_ford step 2399 current loss 1.277261, current_train_items 76800.
I0302 18:59:33.762321 22447428132992 run.py:483] Algo bellman_ford step 2400 current loss 0.705985, current_train_items 76832.
I0302 18:59:33.770266 22447428132992 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.876953125, 'score': 0.876953125, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:33.770370 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.903, current avg val score is 0.877, val scores are: bellman_ford: 0.877
I0302 18:59:33.786706 22447428132992 run.py:483] Algo bellman_ford step 2401 current loss 0.763134, current_train_items 76864.
I0302 18:59:33.810187 22447428132992 run.py:483] Algo bellman_ford step 2402 current loss 0.982354, current_train_items 76896.
I0302 18:59:33.841675 22447428132992 run.py:483] Algo bellman_ford step 2403 current loss 1.293284, current_train_items 76928.
I0302 18:59:33.878562 22447428132992 run.py:483] Algo bellman_ford step 2404 current loss 1.284916, current_train_items 76960.
I0302 18:59:33.898942 22447428132992 run.py:483] Algo bellman_ford step 2405 current loss 0.900154, current_train_items 76992.
I0302 18:59:33.914843 22447428132992 run.py:483] Algo bellman_ford step 2406 current loss 1.004181, current_train_items 77024.
I0302 18:59:33.937884 22447428132992 run.py:483] Algo bellman_ford step 2407 current loss 0.995884, current_train_items 77056.
I0302 18:59:33.967267 22447428132992 run.py:483] Algo bellman_ford step 2408 current loss 1.164246, current_train_items 77088.
I0302 18:59:34.000090 22447428132992 run.py:483] Algo bellman_ford step 2409 current loss 1.571048, current_train_items 77120.
I0302 18:59:34.019234 22447428132992 run.py:483] Algo bellman_ford step 2410 current loss 0.569429, current_train_items 77152.
I0302 18:59:34.035316 22447428132992 run.py:483] Algo bellman_ford step 2411 current loss 0.825556, current_train_items 77184.
I0302 18:59:34.059695 22447428132992 run.py:483] Algo bellman_ford step 2412 current loss 1.202603, current_train_items 77216.
I0302 18:59:34.090346 22447428132992 run.py:483] Algo bellman_ford step 2413 current loss 1.206113, current_train_items 77248.
I0302 18:59:34.124774 22447428132992 run.py:483] Algo bellman_ford step 2414 current loss 1.528911, current_train_items 77280.
I0302 18:59:34.144556 22447428132992 run.py:483] Algo bellman_ford step 2415 current loss 0.605842, current_train_items 77312.
I0302 18:59:34.160066 22447428132992 run.py:483] Algo bellman_ford step 2416 current loss 0.802696, current_train_items 77344.
I0302 18:59:34.183170 22447428132992 run.py:483] Algo bellman_ford step 2417 current loss 1.066209, current_train_items 77376.
I0302 18:59:34.214215 22447428132992 run.py:483] Algo bellman_ford step 2418 current loss 1.199556, current_train_items 77408.
I0302 18:59:34.245769 22447428132992 run.py:483] Algo bellman_ford step 2419 current loss 1.184635, current_train_items 77440.
I0302 18:59:34.265371 22447428132992 run.py:483] Algo bellman_ford step 2420 current loss 0.602667, current_train_items 77472.
I0302 18:59:34.281303 22447428132992 run.py:483] Algo bellman_ford step 2421 current loss 0.806491, current_train_items 77504.
I0302 18:59:34.305033 22447428132992 run.py:483] Algo bellman_ford step 2422 current loss 1.017021, current_train_items 77536.
I0302 18:59:34.335212 22447428132992 run.py:483] Algo bellman_ford step 2423 current loss 1.248970, current_train_items 77568.
I0302 18:59:34.369395 22447428132992 run.py:483] Algo bellman_ford step 2424 current loss 1.450371, current_train_items 77600.
I0302 18:59:34.389080 22447428132992 run.py:483] Algo bellman_ford step 2425 current loss 0.639780, current_train_items 77632.
I0302 18:59:34.405029 22447428132992 run.py:483] Algo bellman_ford step 2426 current loss 0.762982, current_train_items 77664.
I0302 18:59:34.429538 22447428132992 run.py:483] Algo bellman_ford step 2427 current loss 1.119978, current_train_items 77696.
I0302 18:59:34.458372 22447428132992 run.py:483] Algo bellman_ford step 2428 current loss 1.247590, current_train_items 77728.
I0302 18:59:34.491343 22447428132992 run.py:483] Algo bellman_ford step 2429 current loss 1.295962, current_train_items 77760.
I0302 18:59:34.511210 22447428132992 run.py:483] Algo bellman_ford step 2430 current loss 0.651651, current_train_items 77792.
I0302 18:59:34.527542 22447428132992 run.py:483] Algo bellman_ford step 2431 current loss 0.821078, current_train_items 77824.
I0302 18:59:34.551294 22447428132992 run.py:483] Algo bellman_ford step 2432 current loss 1.171379, current_train_items 77856.
I0302 18:59:34.581206 22447428132992 run.py:483] Algo bellman_ford step 2433 current loss 1.080368, current_train_items 77888.
I0302 18:59:34.614998 22447428132992 run.py:483] Algo bellman_ford step 2434 current loss 1.177206, current_train_items 77920.
I0302 18:59:34.634330 22447428132992 run.py:483] Algo bellman_ford step 2435 current loss 0.716420, current_train_items 77952.
I0302 18:59:34.650686 22447428132992 run.py:483] Algo bellman_ford step 2436 current loss 0.911194, current_train_items 77984.
I0302 18:59:34.673745 22447428132992 run.py:483] Algo bellman_ford step 2437 current loss 1.047722, current_train_items 78016.
I0302 18:59:34.702280 22447428132992 run.py:483] Algo bellman_ford step 2438 current loss 1.085488, current_train_items 78048.
I0302 18:59:34.734666 22447428132992 run.py:483] Algo bellman_ford step 2439 current loss 1.380068, current_train_items 78080.
I0302 18:59:34.754525 22447428132992 run.py:483] Algo bellman_ford step 2440 current loss 0.579044, current_train_items 78112.
I0302 18:59:34.770566 22447428132992 run.py:483] Algo bellman_ford step 2441 current loss 0.708768, current_train_items 78144.
I0302 18:59:34.795073 22447428132992 run.py:483] Algo bellman_ford step 2442 current loss 1.152568, current_train_items 78176.
I0302 18:59:34.825025 22447428132992 run.py:483] Algo bellman_ford step 2443 current loss 1.186294, current_train_items 78208.
I0302 18:59:34.859915 22447428132992 run.py:483] Algo bellman_ford step 2444 current loss 1.270492, current_train_items 78240.
I0302 18:59:34.879839 22447428132992 run.py:483] Algo bellman_ford step 2445 current loss 0.649805, current_train_items 78272.
I0302 18:59:34.895954 22447428132992 run.py:483] Algo bellman_ford step 2446 current loss 0.772026, current_train_items 78304.
I0302 18:59:34.919925 22447428132992 run.py:483] Algo bellman_ford step 2447 current loss 1.034227, current_train_items 78336.
I0302 18:59:34.949223 22447428132992 run.py:483] Algo bellman_ford step 2448 current loss 1.143520, current_train_items 78368.
I0302 18:59:34.982249 22447428132992 run.py:483] Algo bellman_ford step 2449 current loss 1.281649, current_train_items 78400.
I0302 18:59:35.001760 22447428132992 run.py:483] Algo bellman_ford step 2450 current loss 0.536295, current_train_items 78432.
I0302 18:59:35.009852 22447428132992 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:35.009956 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.903, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 18:59:35.038472 22447428132992 run.py:483] Algo bellman_ford step 2451 current loss 0.735107, current_train_items 78464.
I0302 18:59:35.062190 22447428132992 run.py:483] Algo bellman_ford step 2452 current loss 1.089224, current_train_items 78496.
I0302 18:59:35.092027 22447428132992 run.py:483] Algo bellman_ford step 2453 current loss 1.041877, current_train_items 78528.
I0302 18:59:35.127325 22447428132992 run.py:483] Algo bellman_ford step 2454 current loss 1.325291, current_train_items 78560.
I0302 18:59:35.147570 22447428132992 run.py:483] Algo bellman_ford step 2455 current loss 0.623814, current_train_items 78592.
I0302 18:59:35.163605 22447428132992 run.py:483] Algo bellman_ford step 2456 current loss 0.743536, current_train_items 78624.
I0302 18:59:35.185783 22447428132992 run.py:483] Algo bellman_ford step 2457 current loss 0.851542, current_train_items 78656.
I0302 18:59:35.216038 22447428132992 run.py:483] Algo bellman_ford step 2458 current loss 1.008983, current_train_items 78688.
I0302 18:59:35.250648 22447428132992 run.py:483] Algo bellman_ford step 2459 current loss 1.286084, current_train_items 78720.
I0302 18:59:35.270707 22447428132992 run.py:483] Algo bellman_ford step 2460 current loss 0.716101, current_train_items 78752.
I0302 18:59:35.287397 22447428132992 run.py:483] Algo bellman_ford step 2461 current loss 0.751187, current_train_items 78784.
I0302 18:59:35.309889 22447428132992 run.py:483] Algo bellman_ford step 2462 current loss 1.013015, current_train_items 78816.
I0302 18:59:35.340677 22447428132992 run.py:483] Algo bellman_ford step 2463 current loss 1.162602, current_train_items 78848.
I0302 18:59:35.373980 22447428132992 run.py:483] Algo bellman_ford step 2464 current loss 1.229812, current_train_items 78880.
I0302 18:59:35.393321 22447428132992 run.py:483] Algo bellman_ford step 2465 current loss 0.548642, current_train_items 78912.
I0302 18:59:35.409459 22447428132992 run.py:483] Algo bellman_ford step 2466 current loss 0.880521, current_train_items 78944.
I0302 18:59:35.432708 22447428132992 run.py:483] Algo bellman_ford step 2467 current loss 0.990496, current_train_items 78976.
I0302 18:59:35.463179 22447428132992 run.py:483] Algo bellman_ford step 2468 current loss 1.154141, current_train_items 79008.
I0302 18:59:35.497810 22447428132992 run.py:483] Algo bellman_ford step 2469 current loss 1.420379, current_train_items 79040.
I0302 18:59:35.517517 22447428132992 run.py:483] Algo bellman_ford step 2470 current loss 0.599115, current_train_items 79072.
I0302 18:59:35.533727 22447428132992 run.py:483] Algo bellman_ford step 2471 current loss 0.762883, current_train_items 79104.
I0302 18:59:35.557012 22447428132992 run.py:483] Algo bellman_ford step 2472 current loss 1.190589, current_train_items 79136.
I0302 18:59:35.587878 22447428132992 run.py:483] Algo bellman_ford step 2473 current loss 1.185520, current_train_items 79168.
I0302 18:59:35.620467 22447428132992 run.py:483] Algo bellman_ford step 2474 current loss 1.263102, current_train_items 79200.
I0302 18:59:35.640306 22447428132992 run.py:483] Algo bellman_ford step 2475 current loss 0.601777, current_train_items 79232.
I0302 18:59:35.656466 22447428132992 run.py:483] Algo bellman_ford step 2476 current loss 0.859181, current_train_items 79264.
I0302 18:59:35.677994 22447428132992 run.py:483] Algo bellman_ford step 2477 current loss 0.882587, current_train_items 79296.
I0302 18:59:35.707109 22447428132992 run.py:483] Algo bellman_ford step 2478 current loss 1.082878, current_train_items 79328.
I0302 18:59:35.741520 22447428132992 run.py:483] Algo bellman_ford step 2479 current loss 1.269217, current_train_items 79360.
I0302 18:59:35.760876 22447428132992 run.py:483] Algo bellman_ford step 2480 current loss 0.701666, current_train_items 79392.
I0302 18:59:35.776670 22447428132992 run.py:483] Algo bellman_ford step 2481 current loss 0.771089, current_train_items 79424.
I0302 18:59:35.800538 22447428132992 run.py:483] Algo bellman_ford step 2482 current loss 1.171803, current_train_items 79456.
I0302 18:59:35.830832 22447428132992 run.py:483] Algo bellman_ford step 2483 current loss 1.231442, current_train_items 79488.
I0302 18:59:35.865460 22447428132992 run.py:483] Algo bellman_ford step 2484 current loss 1.323157, current_train_items 79520.
I0302 18:59:35.885284 22447428132992 run.py:483] Algo bellman_ford step 2485 current loss 0.707820, current_train_items 79552.
I0302 18:59:35.901638 22447428132992 run.py:483] Algo bellman_ford step 2486 current loss 0.917721, current_train_items 79584.
I0302 18:59:35.925171 22447428132992 run.py:483] Algo bellman_ford step 2487 current loss 1.080099, current_train_items 79616.
I0302 18:59:35.952508 22447428132992 run.py:483] Algo bellman_ford step 2488 current loss 1.083661, current_train_items 79648.
I0302 18:59:35.986531 22447428132992 run.py:483] Algo bellman_ford step 2489 current loss 1.215503, current_train_items 79680.
I0302 18:59:36.006369 22447428132992 run.py:483] Algo bellman_ford step 2490 current loss 0.570024, current_train_items 79712.
I0302 18:59:36.022493 22447428132992 run.py:483] Algo bellman_ford step 2491 current loss 0.855646, current_train_items 79744.
I0302 18:59:36.044863 22447428132992 run.py:483] Algo bellman_ford step 2492 current loss 1.017454, current_train_items 79776.
I0302 18:59:36.074611 22447428132992 run.py:483] Algo bellman_ford step 2493 current loss 1.030566, current_train_items 79808.
I0302 18:59:36.106795 22447428132992 run.py:483] Algo bellman_ford step 2494 current loss 1.220595, current_train_items 79840.
I0302 18:59:36.126241 22447428132992 run.py:483] Algo bellman_ford step 2495 current loss 0.605217, current_train_items 79872.
I0302 18:59:36.142777 22447428132992 run.py:483] Algo bellman_ford step 2496 current loss 0.805789, current_train_items 79904.
I0302 18:59:36.165210 22447428132992 run.py:483] Algo bellman_ford step 2497 current loss 0.960724, current_train_items 79936.
I0302 18:59:36.194018 22447428132992 run.py:483] Algo bellman_ford step 2498 current loss 1.130591, current_train_items 79968.
I0302 18:59:36.227073 22447428132992 run.py:483] Algo bellman_ford step 2499 current loss 1.246829, current_train_items 80000.
I0302 18:59:36.246817 22447428132992 run.py:483] Algo bellman_ford step 2500 current loss 0.617451, current_train_items 80032.
I0302 18:59:36.254533 22447428132992 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:36.254640 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.904, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 18:59:36.271408 22447428132992 run.py:483] Algo bellman_ford step 2501 current loss 0.803746, current_train_items 80064.
I0302 18:59:36.295120 22447428132992 run.py:483] Algo bellman_ford step 2502 current loss 1.030818, current_train_items 80096.
I0302 18:59:36.324355 22447428132992 run.py:483] Algo bellman_ford step 2503 current loss 0.974787, current_train_items 80128.
I0302 18:59:36.358962 22447428132992 run.py:483] Algo bellman_ford step 2504 current loss 1.247146, current_train_items 80160.
I0302 18:59:36.378814 22447428132992 run.py:483] Algo bellman_ford step 2505 current loss 0.668280, current_train_items 80192.
I0302 18:59:36.394682 22447428132992 run.py:483] Algo bellman_ford step 2506 current loss 0.748475, current_train_items 80224.
I0302 18:59:36.418130 22447428132992 run.py:483] Algo bellman_ford step 2507 current loss 1.006259, current_train_items 80256.
I0302 18:59:36.446922 22447428132992 run.py:483] Algo bellman_ford step 2508 current loss 1.093448, current_train_items 80288.
I0302 18:59:36.481051 22447428132992 run.py:483] Algo bellman_ford step 2509 current loss 1.319135, current_train_items 80320.
I0302 18:59:36.500428 22447428132992 run.py:483] Algo bellman_ford step 2510 current loss 0.583384, current_train_items 80352.
I0302 18:59:36.516834 22447428132992 run.py:483] Algo bellman_ford step 2511 current loss 0.913565, current_train_items 80384.
I0302 18:59:36.539666 22447428132992 run.py:483] Algo bellman_ford step 2512 current loss 1.116343, current_train_items 80416.
I0302 18:59:36.568506 22447428132992 run.py:483] Algo bellman_ford step 2513 current loss 1.111748, current_train_items 80448.
I0302 18:59:36.602615 22447428132992 run.py:483] Algo bellman_ford step 2514 current loss 1.246305, current_train_items 80480.
I0302 18:59:36.621865 22447428132992 run.py:483] Algo bellman_ford step 2515 current loss 0.582139, current_train_items 80512.
I0302 18:59:36.638075 22447428132992 run.py:483] Algo bellman_ford step 2516 current loss 0.779285, current_train_items 80544.
I0302 18:59:36.661286 22447428132992 run.py:483] Algo bellman_ford step 2517 current loss 1.128629, current_train_items 80576.
I0302 18:59:36.690293 22447428132992 run.py:483] Algo bellman_ford step 2518 current loss 1.071260, current_train_items 80608.
I0302 18:59:36.724004 22447428132992 run.py:483] Algo bellman_ford step 2519 current loss 1.219021, current_train_items 80640.
I0302 18:59:36.743222 22447428132992 run.py:483] Algo bellman_ford step 2520 current loss 0.711032, current_train_items 80672.
I0302 18:59:36.759347 22447428132992 run.py:483] Algo bellman_ford step 2521 current loss 0.887010, current_train_items 80704.
I0302 18:59:36.780988 22447428132992 run.py:483] Algo bellman_ford step 2522 current loss 0.890304, current_train_items 80736.
I0302 18:59:36.809902 22447428132992 run.py:483] Algo bellman_ford step 2523 current loss 1.057047, current_train_items 80768.
I0302 18:59:36.842523 22447428132992 run.py:483] Algo bellman_ford step 2524 current loss 1.154243, current_train_items 80800.
I0302 18:59:36.862141 22447428132992 run.py:483] Algo bellman_ford step 2525 current loss 0.759408, current_train_items 80832.
I0302 18:59:36.878215 22447428132992 run.py:483] Algo bellman_ford step 2526 current loss 0.783660, current_train_items 80864.
I0302 18:59:36.900990 22447428132992 run.py:483] Algo bellman_ford step 2527 current loss 0.975963, current_train_items 80896.
I0302 18:59:36.929768 22447428132992 run.py:483] Algo bellman_ford step 2528 current loss 1.105344, current_train_items 80928.
I0302 18:59:36.963783 22447428132992 run.py:483] Algo bellman_ford step 2529 current loss 1.475107, current_train_items 80960.
I0302 18:59:36.983133 22447428132992 run.py:483] Algo bellman_ford step 2530 current loss 0.573154, current_train_items 80992.
I0302 18:59:36.998863 22447428132992 run.py:483] Algo bellman_ford step 2531 current loss 0.784265, current_train_items 81024.
I0302 18:59:37.021951 22447428132992 run.py:483] Algo bellman_ford step 2532 current loss 1.094396, current_train_items 81056.
I0302 18:59:37.051501 22447428132992 run.py:483] Algo bellman_ford step 2533 current loss 1.061469, current_train_items 81088.
I0302 18:59:37.084006 22447428132992 run.py:483] Algo bellman_ford step 2534 current loss 1.361670, current_train_items 81120.
I0302 18:59:37.103458 22447428132992 run.py:483] Algo bellman_ford step 2535 current loss 0.630515, current_train_items 81152.
I0302 18:59:37.119124 22447428132992 run.py:483] Algo bellman_ford step 2536 current loss 0.787950, current_train_items 81184.
W0302 18:59:37.135306 22447428132992 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:43.706517 22447428132992 run.py:483] Algo bellman_ford step 2537 current loss 1.169778, current_train_items 81216.
I0302 18:59:43.738454 22447428132992 run.py:483] Algo bellman_ford step 2538 current loss 1.124491, current_train_items 81248.
I0302 18:59:43.772941 22447428132992 run.py:483] Algo bellman_ford step 2539 current loss 1.288565, current_train_items 81280.
I0302 18:59:43.792837 22447428132992 run.py:483] Algo bellman_ford step 2540 current loss 0.592673, current_train_items 81312.
I0302 18:59:43.808687 22447428132992 run.py:483] Algo bellman_ford step 2541 current loss 0.721961, current_train_items 81344.
I0302 18:59:43.832019 22447428132992 run.py:483] Algo bellman_ford step 2542 current loss 1.120383, current_train_items 81376.
I0302 18:59:43.861223 22447428132992 run.py:483] Algo bellman_ford step 2543 current loss 1.026746, current_train_items 81408.
I0302 18:59:43.894524 22447428132992 run.py:483] Algo bellman_ford step 2544 current loss 1.225045, current_train_items 81440.
I0302 18:59:43.913946 22447428132992 run.py:483] Algo bellman_ford step 2545 current loss 0.745432, current_train_items 81472.
I0302 18:59:43.929905 22447428132992 run.py:483] Algo bellman_ford step 2546 current loss 0.853753, current_train_items 81504.
I0302 18:59:43.953372 22447428132992 run.py:483] Algo bellman_ford step 2547 current loss 0.958218, current_train_items 81536.
I0302 18:59:43.984754 22447428132992 run.py:483] Algo bellman_ford step 2548 current loss 1.124073, current_train_items 81568.
I0302 18:59:44.016968 22447428132992 run.py:483] Algo bellman_ford step 2549 current loss 1.316528, current_train_items 81600.
I0302 18:59:44.036568 22447428132992 run.py:483] Algo bellman_ford step 2550 current loss 0.601396, current_train_items 81632.
I0302 18:59:44.046645 22447428132992 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:44.046750 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.904, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 18:59:44.076061 22447428132992 run.py:483] Algo bellman_ford step 2551 current loss 0.931901, current_train_items 81664.
I0302 18:59:44.099874 22447428132992 run.py:483] Algo bellman_ford step 2552 current loss 1.034824, current_train_items 81696.
I0302 18:59:44.132137 22447428132992 run.py:483] Algo bellman_ford step 2553 current loss 1.105514, current_train_items 81728.
I0302 18:59:44.166466 22447428132992 run.py:483] Algo bellman_ford step 2554 current loss 1.164891, current_train_items 81760.
I0302 18:59:44.186539 22447428132992 run.py:483] Algo bellman_ford step 2555 current loss 0.644713, current_train_items 81792.
I0302 18:59:44.202825 22447428132992 run.py:483] Algo bellman_ford step 2556 current loss 0.877526, current_train_items 81824.
I0302 18:59:44.226308 22447428132992 run.py:483] Algo bellman_ford step 2557 current loss 0.925981, current_train_items 81856.
I0302 18:59:44.257454 22447428132992 run.py:483] Algo bellman_ford step 2558 current loss 1.181721, current_train_items 81888.
I0302 18:59:44.292552 22447428132992 run.py:483] Algo bellman_ford step 2559 current loss 1.349635, current_train_items 81920.
I0302 18:59:44.312504 22447428132992 run.py:483] Algo bellman_ford step 2560 current loss 0.729747, current_train_items 81952.
I0302 18:59:44.329069 22447428132992 run.py:483] Algo bellman_ford step 2561 current loss 0.843574, current_train_items 81984.
I0302 18:59:44.352035 22447428132992 run.py:483] Algo bellman_ford step 2562 current loss 1.017020, current_train_items 82016.
I0302 18:59:44.384011 22447428132992 run.py:483] Algo bellman_ford step 2563 current loss 1.275360, current_train_items 82048.
I0302 18:59:44.417612 22447428132992 run.py:483] Algo bellman_ford step 2564 current loss 1.178071, current_train_items 82080.
I0302 18:59:44.437020 22447428132992 run.py:483] Algo bellman_ford step 2565 current loss 0.742449, current_train_items 82112.
I0302 18:59:44.453367 22447428132992 run.py:483] Algo bellman_ford step 2566 current loss 0.979877, current_train_items 82144.
I0302 18:59:44.476227 22447428132992 run.py:483] Algo bellman_ford step 2567 current loss 0.973027, current_train_items 82176.
I0302 18:59:44.507624 22447428132992 run.py:483] Algo bellman_ford step 2568 current loss 1.112738, current_train_items 82208.
I0302 18:59:44.540406 22447428132992 run.py:483] Algo bellman_ford step 2569 current loss 1.233770, current_train_items 82240.
I0302 18:59:44.560211 22447428132992 run.py:483] Algo bellman_ford step 2570 current loss 0.523962, current_train_items 82272.
I0302 18:59:44.576347 22447428132992 run.py:483] Algo bellman_ford step 2571 current loss 0.825033, current_train_items 82304.
I0302 18:59:44.598231 22447428132992 run.py:483] Algo bellman_ford step 2572 current loss 0.898368, current_train_items 82336.
I0302 18:59:44.628926 22447428132992 run.py:483] Algo bellman_ford step 2573 current loss 1.070725, current_train_items 82368.
I0302 18:59:44.663456 22447428132992 run.py:483] Algo bellman_ford step 2574 current loss 1.646109, current_train_items 82400.
I0302 18:59:44.683593 22447428132992 run.py:483] Algo bellman_ford step 2575 current loss 0.722286, current_train_items 82432.
I0302 18:59:44.699522 22447428132992 run.py:483] Algo bellman_ford step 2576 current loss 0.874450, current_train_items 82464.
I0302 18:59:44.722014 22447428132992 run.py:483] Algo bellman_ford step 2577 current loss 1.007885, current_train_items 82496.
I0302 18:59:44.753169 22447428132992 run.py:483] Algo bellman_ford step 2578 current loss 1.356328, current_train_items 82528.
I0302 18:59:44.785651 22447428132992 run.py:483] Algo bellman_ford step 2579 current loss 1.704108, current_train_items 82560.
I0302 18:59:44.805299 22447428132992 run.py:483] Algo bellman_ford step 2580 current loss 0.704712, current_train_items 82592.
I0302 18:59:44.821548 22447428132992 run.py:483] Algo bellman_ford step 2581 current loss 0.865174, current_train_items 82624.
I0302 18:59:44.844863 22447428132992 run.py:483] Algo bellman_ford step 2582 current loss 1.048381, current_train_items 82656.
I0302 18:59:44.874805 22447428132992 run.py:483] Algo bellman_ford step 2583 current loss 1.042804, current_train_items 82688.
I0302 18:59:44.909446 22447428132992 run.py:483] Algo bellman_ford step 2584 current loss 1.189815, current_train_items 82720.
I0302 18:59:44.929050 22447428132992 run.py:483] Algo bellman_ford step 2585 current loss 0.590234, current_train_items 82752.
I0302 18:59:44.945258 22447428132992 run.py:483] Algo bellman_ford step 2586 current loss 0.860787, current_train_items 82784.
I0302 18:59:44.968014 22447428132992 run.py:483] Algo bellman_ford step 2587 current loss 0.886396, current_train_items 82816.
I0302 18:59:44.998065 22447428132992 run.py:483] Algo bellman_ford step 2588 current loss 0.998062, current_train_items 82848.
I0302 18:59:45.031857 22447428132992 run.py:483] Algo bellman_ford step 2589 current loss 1.347846, current_train_items 82880.
I0302 18:59:45.051556 22447428132992 run.py:483] Algo bellman_ford step 2590 current loss 0.612747, current_train_items 82912.
I0302 18:59:45.067910 22447428132992 run.py:483] Algo bellman_ford step 2591 current loss 0.800277, current_train_items 82944.
I0302 18:59:45.091554 22447428132992 run.py:483] Algo bellman_ford step 2592 current loss 0.998500, current_train_items 82976.
I0302 18:59:45.122763 22447428132992 run.py:483] Algo bellman_ford step 2593 current loss 1.133952, current_train_items 83008.
I0302 18:59:45.154234 22447428132992 run.py:483] Algo bellman_ford step 2594 current loss 1.213279, current_train_items 83040.
I0302 18:59:45.173511 22447428132992 run.py:483] Algo bellman_ford step 2595 current loss 0.667775, current_train_items 83072.
I0302 18:59:45.189919 22447428132992 run.py:483] Algo bellman_ford step 2596 current loss 0.875017, current_train_items 83104.
I0302 18:59:45.213812 22447428132992 run.py:483] Algo bellman_ford step 2597 current loss 1.002465, current_train_items 83136.
I0302 18:59:45.244336 22447428132992 run.py:483] Algo bellman_ford step 2598 current loss 1.095399, current_train_items 83168.
I0302 18:59:45.277236 22447428132992 run.py:483] Algo bellman_ford step 2599 current loss 1.170190, current_train_items 83200.
I0302 18:59:45.297152 22447428132992 run.py:483] Algo bellman_ford step 2600 current loss 0.630977, current_train_items 83232.
I0302 18:59:45.305116 22447428132992 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:45.305222 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 18:59:45.321879 22447428132992 run.py:483] Algo bellman_ford step 2601 current loss 0.844581, current_train_items 83264.
I0302 18:59:45.345132 22447428132992 run.py:483] Algo bellman_ford step 2602 current loss 0.964547, current_train_items 83296.
I0302 18:59:45.374566 22447428132992 run.py:483] Algo bellman_ford step 2603 current loss 0.977142, current_train_items 83328.
I0302 18:59:45.408304 22447428132992 run.py:483] Algo bellman_ford step 2604 current loss 1.340636, current_train_items 83360.
I0302 18:59:45.428416 22447428132992 run.py:483] Algo bellman_ford step 2605 current loss 0.725761, current_train_items 83392.
I0302 18:59:45.444159 22447428132992 run.py:483] Algo bellman_ford step 2606 current loss 0.704495, current_train_items 83424.
I0302 18:59:45.467771 22447428132992 run.py:483] Algo bellman_ford step 2607 current loss 0.980652, current_train_items 83456.
I0302 18:59:45.498795 22447428132992 run.py:483] Algo bellman_ford step 2608 current loss 1.089967, current_train_items 83488.
I0302 18:59:45.532594 22447428132992 run.py:483] Algo bellman_ford step 2609 current loss 1.322333, current_train_items 83520.
I0302 18:59:45.552203 22447428132992 run.py:483] Algo bellman_ford step 2610 current loss 0.779067, current_train_items 83552.
I0302 18:59:45.568273 22447428132992 run.py:483] Algo bellman_ford step 2611 current loss 0.863640, current_train_items 83584.
I0302 18:59:45.591203 22447428132992 run.py:483] Algo bellman_ford step 2612 current loss 0.961134, current_train_items 83616.
I0302 18:59:45.621529 22447428132992 run.py:483] Algo bellman_ford step 2613 current loss 1.036617, current_train_items 83648.
I0302 18:59:45.653833 22447428132992 run.py:483] Algo bellman_ford step 2614 current loss 1.200266, current_train_items 83680.
I0302 18:59:45.673369 22447428132992 run.py:483] Algo bellman_ford step 2615 current loss 0.601994, current_train_items 83712.
I0302 18:59:45.689173 22447428132992 run.py:483] Algo bellman_ford step 2616 current loss 0.777490, current_train_items 83744.
I0302 18:59:45.713701 22447428132992 run.py:483] Algo bellman_ford step 2617 current loss 1.023728, current_train_items 83776.
I0302 18:59:45.745059 22447428132992 run.py:483] Algo bellman_ford step 2618 current loss 1.109251, current_train_items 83808.
I0302 18:59:45.776924 22447428132992 run.py:483] Algo bellman_ford step 2619 current loss 1.131275, current_train_items 83840.
I0302 18:59:45.796573 22447428132992 run.py:483] Algo bellman_ford step 2620 current loss 0.595900, current_train_items 83872.
I0302 18:59:45.812714 22447428132992 run.py:483] Algo bellman_ford step 2621 current loss 0.769060, current_train_items 83904.
I0302 18:59:45.836265 22447428132992 run.py:483] Algo bellman_ford step 2622 current loss 0.962234, current_train_items 83936.
I0302 18:59:45.866424 22447428132992 run.py:483] Algo bellman_ford step 2623 current loss 1.084426, current_train_items 83968.
I0302 18:59:45.898654 22447428132992 run.py:483] Algo bellman_ford step 2624 current loss 1.228309, current_train_items 84000.
I0302 18:59:45.918509 22447428132992 run.py:483] Algo bellman_ford step 2625 current loss 0.738961, current_train_items 84032.
I0302 18:59:45.934564 22447428132992 run.py:483] Algo bellman_ford step 2626 current loss 0.788396, current_train_items 84064.
I0302 18:59:45.958220 22447428132992 run.py:483] Algo bellman_ford step 2627 current loss 1.039497, current_train_items 84096.
I0302 18:59:45.988937 22447428132992 run.py:483] Algo bellman_ford step 2628 current loss 1.052286, current_train_items 84128.
I0302 18:59:46.022376 22447428132992 run.py:483] Algo bellman_ford step 2629 current loss 1.167434, current_train_items 84160.
I0302 18:59:46.041980 22447428132992 run.py:483] Algo bellman_ford step 2630 current loss 0.617664, current_train_items 84192.
I0302 18:59:46.058089 22447428132992 run.py:483] Algo bellman_ford step 2631 current loss 0.828275, current_train_items 84224.
I0302 18:59:46.081151 22447428132992 run.py:483] Algo bellman_ford step 2632 current loss 1.000978, current_train_items 84256.
I0302 18:59:46.111215 22447428132992 run.py:483] Algo bellman_ford step 2633 current loss 1.013806, current_train_items 84288.
I0302 18:59:46.146602 22447428132992 run.py:483] Algo bellman_ford step 2634 current loss 1.267139, current_train_items 84320.
I0302 18:59:46.166400 22447428132992 run.py:483] Algo bellman_ford step 2635 current loss 0.657371, current_train_items 84352.
I0302 18:59:46.182327 22447428132992 run.py:483] Algo bellman_ford step 2636 current loss 0.785195, current_train_items 84384.
I0302 18:59:46.206291 22447428132992 run.py:483] Algo bellman_ford step 2637 current loss 1.036627, current_train_items 84416.
I0302 18:59:46.237785 22447428132992 run.py:483] Algo bellman_ford step 2638 current loss 1.166749, current_train_items 84448.
I0302 18:59:46.271615 22447428132992 run.py:483] Algo bellman_ford step 2639 current loss 1.309897, current_train_items 84480.
I0302 18:59:46.291275 22447428132992 run.py:483] Algo bellman_ford step 2640 current loss 0.629306, current_train_items 84512.
I0302 18:59:46.307264 22447428132992 run.py:483] Algo bellman_ford step 2641 current loss 0.800615, current_train_items 84544.
I0302 18:59:46.329916 22447428132992 run.py:483] Algo bellman_ford step 2642 current loss 1.017736, current_train_items 84576.
I0302 18:59:46.360122 22447428132992 run.py:483] Algo bellman_ford step 2643 current loss 1.062349, current_train_items 84608.
I0302 18:59:46.394739 22447428132992 run.py:483] Algo bellman_ford step 2644 current loss 1.395524, current_train_items 84640.
I0302 18:59:46.414540 22447428132992 run.py:483] Algo bellman_ford step 2645 current loss 0.577489, current_train_items 84672.
I0302 18:59:46.430486 22447428132992 run.py:483] Algo bellman_ford step 2646 current loss 0.782252, current_train_items 84704.
I0302 18:59:46.453641 22447428132992 run.py:483] Algo bellman_ford step 2647 current loss 1.038176, current_train_items 84736.
I0302 18:59:46.484369 22447428132992 run.py:483] Algo bellman_ford step 2648 current loss 1.098474, current_train_items 84768.
I0302 18:59:46.519922 22447428132992 run.py:483] Algo bellman_ford step 2649 current loss 1.249349, current_train_items 84800.
I0302 18:59:46.539844 22447428132992 run.py:483] Algo bellman_ford step 2650 current loss 0.669649, current_train_items 84832.
I0302 18:59:46.548078 22447428132992 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:46.548185 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 18:59:46.564535 22447428132992 run.py:483] Algo bellman_ford step 2651 current loss 0.819929, current_train_items 84864.
I0302 18:59:46.588057 22447428132992 run.py:483] Algo bellman_ford step 2652 current loss 1.014126, current_train_items 84896.
I0302 18:59:46.619422 22447428132992 run.py:483] Algo bellman_ford step 2653 current loss 1.035439, current_train_items 84928.
I0302 18:59:46.653038 22447428132992 run.py:483] Algo bellman_ford step 2654 current loss 1.258900, current_train_items 84960.
I0302 18:59:46.672896 22447428132992 run.py:483] Algo bellman_ford step 2655 current loss 0.649529, current_train_items 84992.
I0302 18:59:46.688892 22447428132992 run.py:483] Algo bellman_ford step 2656 current loss 0.864410, current_train_items 85024.
I0302 18:59:46.711919 22447428132992 run.py:483] Algo bellman_ford step 2657 current loss 0.932190, current_train_items 85056.
I0302 18:59:46.741541 22447428132992 run.py:483] Algo bellman_ford step 2658 current loss 1.153474, current_train_items 85088.
I0302 18:59:46.776932 22447428132992 run.py:483] Algo bellman_ford step 2659 current loss 1.289326, current_train_items 85120.
I0302 18:59:46.796677 22447428132992 run.py:483] Algo bellman_ford step 2660 current loss 0.547454, current_train_items 85152.
I0302 18:59:46.813331 22447428132992 run.py:483] Algo bellman_ford step 2661 current loss 0.782394, current_train_items 85184.
I0302 18:59:46.835947 22447428132992 run.py:483] Algo bellman_ford step 2662 current loss 1.023453, current_train_items 85216.
I0302 18:59:46.866643 22447428132992 run.py:483] Algo bellman_ford step 2663 current loss 1.022938, current_train_items 85248.
I0302 18:59:46.902321 22447428132992 run.py:483] Algo bellman_ford step 2664 current loss 1.312793, current_train_items 85280.
I0302 18:59:46.921519 22447428132992 run.py:483] Algo bellman_ford step 2665 current loss 0.592569, current_train_items 85312.
I0302 18:59:46.937634 22447428132992 run.py:483] Algo bellman_ford step 2666 current loss 0.829376, current_train_items 85344.
I0302 18:59:46.962016 22447428132992 run.py:483] Algo bellman_ford step 2667 current loss 1.091716, current_train_items 85376.
I0302 18:59:46.992918 22447428132992 run.py:483] Algo bellman_ford step 2668 current loss 1.115392, current_train_items 85408.
I0302 18:59:47.025363 22447428132992 run.py:483] Algo bellman_ford step 2669 current loss 1.308009, current_train_items 85440.
I0302 18:59:47.045135 22447428132992 run.py:483] Algo bellman_ford step 2670 current loss 0.603754, current_train_items 85472.
I0302 18:59:47.061516 22447428132992 run.py:483] Algo bellman_ford step 2671 current loss 0.750873, current_train_items 85504.
I0302 18:59:47.084754 22447428132992 run.py:483] Algo bellman_ford step 2672 current loss 0.950626, current_train_items 85536.
I0302 18:59:47.116061 22447428132992 run.py:483] Algo bellman_ford step 2673 current loss 1.125999, current_train_items 85568.
I0302 18:59:47.151226 22447428132992 run.py:483] Algo bellman_ford step 2674 current loss 1.262853, current_train_items 85600.
I0302 18:59:47.170923 22447428132992 run.py:483] Algo bellman_ford step 2675 current loss 0.554782, current_train_items 85632.
I0302 18:59:47.187016 22447428132992 run.py:483] Algo bellman_ford step 2676 current loss 0.780777, current_train_items 85664.
I0302 18:59:47.210813 22447428132992 run.py:483] Algo bellman_ford step 2677 current loss 1.084895, current_train_items 85696.
I0302 18:59:47.239845 22447428132992 run.py:483] Algo bellman_ford step 2678 current loss 1.002861, current_train_items 85728.
I0302 18:59:47.271754 22447428132992 run.py:483] Algo bellman_ford step 2679 current loss 1.031223, current_train_items 85760.
I0302 18:59:47.291364 22447428132992 run.py:483] Algo bellman_ford step 2680 current loss 0.704447, current_train_items 85792.
I0302 18:59:47.307488 22447428132992 run.py:483] Algo bellman_ford step 2681 current loss 0.783042, current_train_items 85824.
I0302 18:59:47.330784 22447428132992 run.py:483] Algo bellman_ford step 2682 current loss 1.014142, current_train_items 85856.
I0302 18:59:47.362584 22447428132992 run.py:483] Algo bellman_ford step 2683 current loss 1.082178, current_train_items 85888.
I0302 18:59:47.396209 22447428132992 run.py:483] Algo bellman_ford step 2684 current loss 1.262219, current_train_items 85920.
I0302 18:59:47.416039 22447428132992 run.py:483] Algo bellman_ford step 2685 current loss 0.614233, current_train_items 85952.
I0302 18:59:47.432085 22447428132992 run.py:483] Algo bellman_ford step 2686 current loss 0.784118, current_train_items 85984.
I0302 18:59:47.455464 22447428132992 run.py:483] Algo bellman_ford step 2687 current loss 0.987297, current_train_items 86016.
I0302 18:59:47.485725 22447428132992 run.py:483] Algo bellman_ford step 2688 current loss 1.093590, current_train_items 86048.
I0302 18:59:47.517874 22447428132992 run.py:483] Algo bellman_ford step 2689 current loss 1.234055, current_train_items 86080.
I0302 18:59:47.537716 22447428132992 run.py:483] Algo bellman_ford step 2690 current loss 0.648364, current_train_items 86112.
I0302 18:59:47.553865 22447428132992 run.py:483] Algo bellman_ford step 2691 current loss 0.889450, current_train_items 86144.
I0302 18:59:47.577967 22447428132992 run.py:483] Algo bellman_ford step 2692 current loss 1.000433, current_train_items 86176.
I0302 18:59:47.609505 22447428132992 run.py:483] Algo bellman_ford step 2693 current loss 1.142747, current_train_items 86208.
I0302 18:59:47.643097 22447428132992 run.py:483] Algo bellman_ford step 2694 current loss 1.150366, current_train_items 86240.
I0302 18:59:47.662709 22447428132992 run.py:483] Algo bellman_ford step 2695 current loss 0.566522, current_train_items 86272.
I0302 18:59:47.678560 22447428132992 run.py:483] Algo bellman_ford step 2696 current loss 0.750056, current_train_items 86304.
I0302 18:59:47.701938 22447428132992 run.py:483] Algo bellman_ford step 2697 current loss 0.984476, current_train_items 86336.
I0302 18:59:47.731993 22447428132992 run.py:483] Algo bellman_ford step 2698 current loss 1.103399, current_train_items 86368.
I0302 18:59:47.763478 22447428132992 run.py:483] Algo bellman_ford step 2699 current loss 1.446442, current_train_items 86400.
I0302 18:59:47.783168 22447428132992 run.py:483] Algo bellman_ford step 2700 current loss 0.633010, current_train_items 86432.
I0302 18:59:47.790899 22447428132992 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.8681640625, 'score': 0.8681640625, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:47.791004 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.868, val scores are: bellman_ford: 0.868
I0302 18:59:47.807611 22447428132992 run.py:483] Algo bellman_ford step 2701 current loss 0.793341, current_train_items 86464.
I0302 18:59:47.831295 22447428132992 run.py:483] Algo bellman_ford step 2702 current loss 0.972934, current_train_items 86496.
I0302 18:59:47.863726 22447428132992 run.py:483] Algo bellman_ford step 2703 current loss 1.148647, current_train_items 86528.
I0302 18:59:47.899608 22447428132992 run.py:483] Algo bellman_ford step 2704 current loss 1.232635, current_train_items 86560.
I0302 18:59:47.919260 22447428132992 run.py:483] Algo bellman_ford step 2705 current loss 0.534469, current_train_items 86592.
I0302 18:59:47.934922 22447428132992 run.py:483] Algo bellman_ford step 2706 current loss 0.894776, current_train_items 86624.
I0302 18:59:47.958230 22447428132992 run.py:483] Algo bellman_ford step 2707 current loss 1.016568, current_train_items 86656.
I0302 18:59:47.987200 22447428132992 run.py:483] Algo bellman_ford step 2708 current loss 1.030074, current_train_items 86688.
I0302 18:59:48.019067 22447428132992 run.py:483] Algo bellman_ford step 2709 current loss 1.081766, current_train_items 86720.
I0302 18:59:48.038783 22447428132992 run.py:483] Algo bellman_ford step 2710 current loss 0.672800, current_train_items 86752.
I0302 18:59:48.055190 22447428132992 run.py:483] Algo bellman_ford step 2711 current loss 0.785873, current_train_items 86784.
I0302 18:59:48.078901 22447428132992 run.py:483] Algo bellman_ford step 2712 current loss 1.001102, current_train_items 86816.
I0302 18:59:48.110476 22447428132992 run.py:483] Algo bellman_ford step 2713 current loss 1.054057, current_train_items 86848.
I0302 18:59:48.145841 22447428132992 run.py:483] Algo bellman_ford step 2714 current loss 1.299987, current_train_items 86880.
I0302 18:59:48.165339 22447428132992 run.py:483] Algo bellman_ford step 2715 current loss 0.677223, current_train_items 86912.
I0302 18:59:48.181008 22447428132992 run.py:483] Algo bellman_ford step 2716 current loss 0.661431, current_train_items 86944.
I0302 18:59:48.205177 22447428132992 run.py:483] Algo bellman_ford step 2717 current loss 0.974562, current_train_items 86976.
I0302 18:59:48.234425 22447428132992 run.py:483] Algo bellman_ford step 2718 current loss 1.048415, current_train_items 87008.
I0302 18:59:48.267795 22447428132992 run.py:483] Algo bellman_ford step 2719 current loss 1.275499, current_train_items 87040.
I0302 18:59:48.287190 22447428132992 run.py:483] Algo bellman_ford step 2720 current loss 0.558422, current_train_items 87072.
I0302 18:59:48.303274 22447428132992 run.py:483] Algo bellman_ford step 2721 current loss 0.847428, current_train_items 87104.
I0302 18:59:48.326406 22447428132992 run.py:483] Algo bellman_ford step 2722 current loss 1.005682, current_train_items 87136.
I0302 18:59:48.358257 22447428132992 run.py:483] Algo bellman_ford step 2723 current loss 1.104845, current_train_items 87168.
I0302 18:59:48.391509 22447428132992 run.py:483] Algo bellman_ford step 2724 current loss 1.189173, current_train_items 87200.
I0302 18:59:48.410826 22447428132992 run.py:483] Algo bellman_ford step 2725 current loss 0.678352, current_train_items 87232.
I0302 18:59:48.426782 22447428132992 run.py:483] Algo bellman_ford step 2726 current loss 0.853626, current_train_items 87264.
I0302 18:59:48.450210 22447428132992 run.py:483] Algo bellman_ford step 2727 current loss 1.047875, current_train_items 87296.
I0302 18:59:48.480801 22447428132992 run.py:483] Algo bellman_ford step 2728 current loss 1.147312, current_train_items 87328.
I0302 18:59:48.513951 22447428132992 run.py:483] Algo bellman_ford step 2729 current loss 1.411906, current_train_items 87360.
I0302 18:59:48.533459 22447428132992 run.py:483] Algo bellman_ford step 2730 current loss 0.553314, current_train_items 87392.
I0302 18:59:48.549404 22447428132992 run.py:483] Algo bellman_ford step 2731 current loss 0.675308, current_train_items 87424.
I0302 18:59:48.571429 22447428132992 run.py:483] Algo bellman_ford step 2732 current loss 0.888470, current_train_items 87456.
I0302 18:59:48.600663 22447428132992 run.py:483] Algo bellman_ford step 2733 current loss 0.998730, current_train_items 87488.
I0302 18:59:48.634252 22447428132992 run.py:483] Algo bellman_ford step 2734 current loss 1.186270, current_train_items 87520.
I0302 18:59:48.653819 22447428132992 run.py:483] Algo bellman_ford step 2735 current loss 0.711558, current_train_items 87552.
I0302 18:59:48.669410 22447428132992 run.py:483] Algo bellman_ford step 2736 current loss 0.736424, current_train_items 87584.
I0302 18:59:48.692262 22447428132992 run.py:483] Algo bellman_ford step 2737 current loss 0.881791, current_train_items 87616.
I0302 18:59:48.721898 22447428132992 run.py:483] Algo bellman_ford step 2738 current loss 0.925758, current_train_items 87648.
I0302 18:59:48.754188 22447428132992 run.py:483] Algo bellman_ford step 2739 current loss 1.182678, current_train_items 87680.
I0302 18:59:48.773664 22447428132992 run.py:483] Algo bellman_ford step 2740 current loss 0.652938, current_train_items 87712.
I0302 18:59:48.789517 22447428132992 run.py:483] Algo bellman_ford step 2741 current loss 0.763136, current_train_items 87744.
I0302 18:59:48.812666 22447428132992 run.py:483] Algo bellman_ford step 2742 current loss 1.043722, current_train_items 87776.
I0302 18:59:48.844502 22447428132992 run.py:483] Algo bellman_ford step 2743 current loss 1.124975, current_train_items 87808.
I0302 18:59:48.877555 22447428132992 run.py:483] Algo bellman_ford step 2744 current loss 1.245891, current_train_items 87840.
I0302 18:59:48.896824 22447428132992 run.py:483] Algo bellman_ford step 2745 current loss 0.558673, current_train_items 87872.
I0302 18:59:48.912696 22447428132992 run.py:483] Algo bellman_ford step 2746 current loss 0.821985, current_train_items 87904.
I0302 18:59:48.936062 22447428132992 run.py:483] Algo bellman_ford step 2747 current loss 0.985691, current_train_items 87936.
I0302 18:59:48.966995 22447428132992 run.py:483] Algo bellman_ford step 2748 current loss 1.191520, current_train_items 87968.
I0302 18:59:48.999837 22447428132992 run.py:483] Algo bellman_ford step 2749 current loss 1.267593, current_train_items 88000.
I0302 18:59:49.019245 22447428132992 run.py:483] Algo bellman_ford step 2750 current loss 0.618341, current_train_items 88032.
I0302 18:59:49.027393 22447428132992 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.857421875, 'score': 0.857421875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:49.027502 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.857, val scores are: bellman_ford: 0.857
I0302 18:59:49.044210 22447428132992 run.py:483] Algo bellman_ford step 2751 current loss 0.905888, current_train_items 88064.
I0302 18:59:49.068303 22447428132992 run.py:483] Algo bellman_ford step 2752 current loss 1.098418, current_train_items 88096.
I0302 18:59:49.099959 22447428132992 run.py:483] Algo bellman_ford step 2753 current loss 1.196443, current_train_items 88128.
I0302 18:59:49.132647 22447428132992 run.py:483] Algo bellman_ford step 2754 current loss 1.382539, current_train_items 88160.
I0302 18:59:49.152632 22447428132992 run.py:483] Algo bellman_ford step 2755 current loss 0.502558, current_train_items 88192.
I0302 18:59:49.168723 22447428132992 run.py:483] Algo bellman_ford step 2756 current loss 0.840574, current_train_items 88224.
I0302 18:59:49.191776 22447428132992 run.py:483] Algo bellman_ford step 2757 current loss 0.980670, current_train_items 88256.
I0302 18:59:49.222806 22447428132992 run.py:483] Algo bellman_ford step 2758 current loss 1.134969, current_train_items 88288.
I0302 18:59:49.258494 22447428132992 run.py:483] Algo bellman_ford step 2759 current loss 1.299378, current_train_items 88320.
I0302 18:59:49.278186 22447428132992 run.py:483] Algo bellman_ford step 2760 current loss 0.602593, current_train_items 88352.
I0302 18:59:49.294791 22447428132992 run.py:483] Algo bellman_ford step 2761 current loss 0.821864, current_train_items 88384.
I0302 18:59:49.318403 22447428132992 run.py:483] Algo bellman_ford step 2762 current loss 1.108699, current_train_items 88416.
I0302 18:59:49.349412 22447428132992 run.py:483] Algo bellman_ford step 2763 current loss 0.952125, current_train_items 88448.
I0302 18:59:49.382977 22447428132992 run.py:483] Algo bellman_ford step 2764 current loss 1.176042, current_train_items 88480.
I0302 18:59:49.402368 22447428132992 run.py:483] Algo bellman_ford step 2765 current loss 0.695167, current_train_items 88512.
I0302 18:59:49.418771 22447428132992 run.py:483] Algo bellman_ford step 2766 current loss 0.875739, current_train_items 88544.
I0302 18:59:49.442231 22447428132992 run.py:483] Algo bellman_ford step 2767 current loss 1.045061, current_train_items 88576.
I0302 18:59:49.473771 22447428132992 run.py:483] Algo bellman_ford step 2768 current loss 1.058404, current_train_items 88608.
I0302 18:59:49.507231 22447428132992 run.py:483] Algo bellman_ford step 2769 current loss 1.172635, current_train_items 88640.
I0302 18:59:49.526887 22447428132992 run.py:483] Algo bellman_ford step 2770 current loss 0.565817, current_train_items 88672.
I0302 18:59:49.543147 22447428132992 run.py:483] Algo bellman_ford step 2771 current loss 0.813813, current_train_items 88704.
I0302 18:59:49.566760 22447428132992 run.py:483] Algo bellman_ford step 2772 current loss 1.038949, current_train_items 88736.
I0302 18:59:49.597042 22447428132992 run.py:483] Algo bellman_ford step 2773 current loss 1.130759, current_train_items 88768.
I0302 18:59:49.630532 22447428132992 run.py:483] Algo bellman_ford step 2774 current loss 1.197894, current_train_items 88800.
I0302 18:59:49.650163 22447428132992 run.py:483] Algo bellman_ford step 2775 current loss 0.567745, current_train_items 88832.
I0302 18:59:49.666823 22447428132992 run.py:483] Algo bellman_ford step 2776 current loss 0.855610, current_train_items 88864.
I0302 18:59:49.689682 22447428132992 run.py:483] Algo bellman_ford step 2777 current loss 0.971908, current_train_items 88896.
I0302 18:59:49.719823 22447428132992 run.py:483] Algo bellman_ford step 2778 current loss 1.066437, current_train_items 88928.
I0302 18:59:49.754147 22447428132992 run.py:483] Algo bellman_ford step 2779 current loss 1.405760, current_train_items 88960.
I0302 18:59:49.774051 22447428132992 run.py:483] Algo bellman_ford step 2780 current loss 0.667854, current_train_items 88992.
I0302 18:59:49.790063 22447428132992 run.py:483] Algo bellman_ford step 2781 current loss 0.840671, current_train_items 89024.
I0302 18:59:49.813917 22447428132992 run.py:483] Algo bellman_ford step 2782 current loss 0.971412, current_train_items 89056.
I0302 18:59:49.844108 22447428132992 run.py:483] Algo bellman_ford step 2783 current loss 1.006345, current_train_items 89088.
I0302 18:59:49.878428 22447428132992 run.py:483] Algo bellman_ford step 2784 current loss 1.178090, current_train_items 89120.
I0302 18:59:49.898529 22447428132992 run.py:483] Algo bellman_ford step 2785 current loss 0.653706, current_train_items 89152.
I0302 18:59:49.914606 22447428132992 run.py:483] Algo bellman_ford step 2786 current loss 0.776081, current_train_items 89184.
I0302 18:59:49.936853 22447428132992 run.py:483] Algo bellman_ford step 2787 current loss 0.899012, current_train_items 89216.
I0302 18:59:49.966936 22447428132992 run.py:483] Algo bellman_ford step 2788 current loss 0.996831, current_train_items 89248.
I0302 18:59:49.997666 22447428132992 run.py:483] Algo bellman_ford step 2789 current loss 1.030389, current_train_items 89280.
I0302 18:59:50.017472 22447428132992 run.py:483] Algo bellman_ford step 2790 current loss 0.584048, current_train_items 89312.
I0302 18:59:50.033555 22447428132992 run.py:483] Algo bellman_ford step 2791 current loss 0.870261, current_train_items 89344.
I0302 18:59:50.056479 22447428132992 run.py:483] Algo bellman_ford step 2792 current loss 0.848742, current_train_items 89376.
I0302 18:59:50.086554 22447428132992 run.py:483] Algo bellman_ford step 2793 current loss 1.050990, current_train_items 89408.
I0302 18:59:50.122426 22447428132992 run.py:483] Algo bellman_ford step 2794 current loss 1.187134, current_train_items 89440.
I0302 18:59:50.142007 22447428132992 run.py:483] Algo bellman_ford step 2795 current loss 0.699807, current_train_items 89472.
I0302 18:59:50.158662 22447428132992 run.py:483] Algo bellman_ford step 2796 current loss 0.879006, current_train_items 89504.
I0302 18:59:50.181882 22447428132992 run.py:483] Algo bellman_ford step 2797 current loss 1.019422, current_train_items 89536.
I0302 18:59:50.213337 22447428132992 run.py:483] Algo bellman_ford step 2798 current loss 1.136786, current_train_items 89568.
I0302 18:59:50.246859 22447428132992 run.py:483] Algo bellman_ford step 2799 current loss 1.376531, current_train_items 89600.
I0302 18:59:50.267046 22447428132992 run.py:483] Algo bellman_ford step 2800 current loss 0.597758, current_train_items 89632.
I0302 18:59:50.274947 22447428132992 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:50.275054 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.907, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 18:59:50.304091 22447428132992 run.py:483] Algo bellman_ford step 2801 current loss 0.725035, current_train_items 89664.
I0302 18:59:50.328592 22447428132992 run.py:483] Algo bellman_ford step 2802 current loss 1.100726, current_train_items 89696.
I0302 18:59:50.360044 22447428132992 run.py:483] Algo bellman_ford step 2803 current loss 1.125303, current_train_items 89728.
I0302 18:59:50.393867 22447428132992 run.py:483] Algo bellman_ford step 2804 current loss 1.225531, current_train_items 89760.
I0302 18:59:50.414527 22447428132992 run.py:483] Algo bellman_ford step 2805 current loss 0.547595, current_train_items 89792.
I0302 18:59:50.430742 22447428132992 run.py:483] Algo bellman_ford step 2806 current loss 0.813283, current_train_items 89824.
I0302 18:59:50.453466 22447428132992 run.py:483] Algo bellman_ford step 2807 current loss 0.873949, current_train_items 89856.
I0302 18:59:50.484818 22447428132992 run.py:483] Algo bellman_ford step 2808 current loss 1.145999, current_train_items 89888.
I0302 18:59:50.519136 22447428132992 run.py:483] Algo bellman_ford step 2809 current loss 1.150025, current_train_items 89920.
I0302 18:59:50.538658 22447428132992 run.py:483] Algo bellman_ford step 2810 current loss 0.630607, current_train_items 89952.
I0302 18:59:50.555151 22447428132992 run.py:483] Algo bellman_ford step 2811 current loss 0.865093, current_train_items 89984.
I0302 18:59:50.577337 22447428132992 run.py:483] Algo bellman_ford step 2812 current loss 0.903675, current_train_items 90016.
I0302 18:59:50.608919 22447428132992 run.py:483] Algo bellman_ford step 2813 current loss 1.120556, current_train_items 90048.
I0302 18:59:50.641388 22447428132992 run.py:483] Algo bellman_ford step 2814 current loss 1.308489, current_train_items 90080.
I0302 18:59:50.660475 22447428132992 run.py:483] Algo bellman_ford step 2815 current loss 0.568987, current_train_items 90112.
I0302 18:59:50.676759 22447428132992 run.py:483] Algo bellman_ford step 2816 current loss 0.751215, current_train_items 90144.
I0302 18:59:50.700251 22447428132992 run.py:483] Algo bellman_ford step 2817 current loss 1.070756, current_train_items 90176.
I0302 18:59:50.727950 22447428132992 run.py:483] Algo bellman_ford step 2818 current loss 0.941322, current_train_items 90208.
I0302 18:59:50.761141 22447428132992 run.py:483] Algo bellman_ford step 2819 current loss 1.213016, current_train_items 90240.
I0302 18:59:50.780707 22447428132992 run.py:483] Algo bellman_ford step 2820 current loss 0.632950, current_train_items 90272.
I0302 18:59:50.796676 22447428132992 run.py:483] Algo bellman_ford step 2821 current loss 0.745928, current_train_items 90304.
I0302 18:59:50.819561 22447428132992 run.py:483] Algo bellman_ford step 2822 current loss 0.954447, current_train_items 90336.
I0302 18:59:50.849775 22447428132992 run.py:483] Algo bellman_ford step 2823 current loss 1.257885, current_train_items 90368.
I0302 18:59:50.885297 22447428132992 run.py:483] Algo bellman_ford step 2824 current loss 1.199548, current_train_items 90400.
I0302 18:59:50.905071 22447428132992 run.py:483] Algo bellman_ford step 2825 current loss 0.566554, current_train_items 90432.
I0302 18:59:50.920863 22447428132992 run.py:483] Algo bellman_ford step 2826 current loss 0.843427, current_train_items 90464.
I0302 18:59:50.944095 22447428132992 run.py:483] Algo bellman_ford step 2827 current loss 0.964079, current_train_items 90496.
I0302 18:59:50.974657 22447428132992 run.py:483] Algo bellman_ford step 2828 current loss 1.086970, current_train_items 90528.
I0302 18:59:51.006770 22447428132992 run.py:483] Algo bellman_ford step 2829 current loss 1.032735, current_train_items 90560.
I0302 18:59:51.026075 22447428132992 run.py:483] Algo bellman_ford step 2830 current loss 0.629723, current_train_items 90592.
I0302 18:59:51.042515 22447428132992 run.py:483] Algo bellman_ford step 2831 current loss 0.886324, current_train_items 90624.
I0302 18:59:51.065469 22447428132992 run.py:483] Algo bellman_ford step 2832 current loss 0.996273, current_train_items 90656.
I0302 18:59:51.095343 22447428132992 run.py:483] Algo bellman_ford step 2833 current loss 1.075633, current_train_items 90688.
I0302 18:59:51.128238 22447428132992 run.py:483] Algo bellman_ford step 2834 current loss 1.603582, current_train_items 90720.
I0302 18:59:51.147557 22447428132992 run.py:483] Algo bellman_ford step 2835 current loss 0.623698, current_train_items 90752.
I0302 18:59:51.163232 22447428132992 run.py:483] Algo bellman_ford step 2836 current loss 0.813391, current_train_items 90784.
I0302 18:59:51.185739 22447428132992 run.py:483] Algo bellman_ford step 2837 current loss 0.918132, current_train_items 90816.
I0302 18:59:51.216048 22447428132992 run.py:483] Algo bellman_ford step 2838 current loss 1.078200, current_train_items 90848.
I0302 18:59:51.251466 22447428132992 run.py:483] Algo bellman_ford step 2839 current loss 1.300341, current_train_items 90880.
I0302 18:59:51.270869 22447428132992 run.py:483] Algo bellman_ford step 2840 current loss 0.699108, current_train_items 90912.
I0302 18:59:51.286680 22447428132992 run.py:483] Algo bellman_ford step 2841 current loss 0.687781, current_train_items 90944.
I0302 18:59:51.309872 22447428132992 run.py:483] Algo bellman_ford step 2842 current loss 0.996354, current_train_items 90976.
I0302 18:59:51.341157 22447428132992 run.py:483] Algo bellman_ford step 2843 current loss 1.144041, current_train_items 91008.
I0302 18:59:51.374921 22447428132992 run.py:483] Algo bellman_ford step 2844 current loss 1.321897, current_train_items 91040.
I0302 18:59:51.394435 22447428132992 run.py:483] Algo bellman_ford step 2845 current loss 0.679352, current_train_items 91072.
I0302 18:59:51.410918 22447428132992 run.py:483] Algo bellman_ford step 2846 current loss 0.769341, current_train_items 91104.
I0302 18:59:51.433864 22447428132992 run.py:483] Algo bellman_ford step 2847 current loss 0.912510, current_train_items 91136.
I0302 18:59:51.464211 22447428132992 run.py:483] Algo bellman_ford step 2848 current loss 0.989632, current_train_items 91168.
I0302 18:59:51.498785 22447428132992 run.py:483] Algo bellman_ford step 2849 current loss 1.284391, current_train_items 91200.
I0302 18:59:51.518138 22447428132992 run.py:483] Algo bellman_ford step 2850 current loss 0.620626, current_train_items 91232.
I0302 18:59:51.526266 22447428132992 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:51.526371 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:51.542539 22447428132992 run.py:483] Algo bellman_ford step 2851 current loss 0.639340, current_train_items 91264.
I0302 18:59:51.565911 22447428132992 run.py:483] Algo bellman_ford step 2852 current loss 0.889383, current_train_items 91296.
I0302 18:59:51.597701 22447428132992 run.py:483] Algo bellman_ford step 2853 current loss 1.101660, current_train_items 91328.
I0302 18:59:51.631495 22447428132992 run.py:483] Algo bellman_ford step 2854 current loss 1.202243, current_train_items 91360.
I0302 18:59:51.651405 22447428132992 run.py:483] Algo bellman_ford step 2855 current loss 0.652958, current_train_items 91392.
I0302 18:59:51.667320 22447428132992 run.py:483] Algo bellman_ford step 2856 current loss 0.807989, current_train_items 91424.
I0302 18:59:51.690849 22447428132992 run.py:483] Algo bellman_ford step 2857 current loss 1.117092, current_train_items 91456.
I0302 18:59:51.719907 22447428132992 run.py:483] Algo bellman_ford step 2858 current loss 0.880100, current_train_items 91488.
I0302 18:59:51.752106 22447428132992 run.py:483] Algo bellman_ford step 2859 current loss 1.080879, current_train_items 91520.
I0302 18:59:51.772220 22447428132992 run.py:483] Algo bellman_ford step 2860 current loss 0.615091, current_train_items 91552.
I0302 18:59:51.788459 22447428132992 run.py:483] Algo bellman_ford step 2861 current loss 0.847523, current_train_items 91584.
I0302 18:59:51.811833 22447428132992 run.py:483] Algo bellman_ford step 2862 current loss 0.919729, current_train_items 91616.
I0302 18:59:51.843500 22447428132992 run.py:483] Algo bellman_ford step 2863 current loss 1.189862, current_train_items 91648.
I0302 18:59:51.874208 22447428132992 run.py:483] Algo bellman_ford step 2864 current loss 1.210121, current_train_items 91680.
I0302 18:59:51.894151 22447428132992 run.py:483] Algo bellman_ford step 2865 current loss 0.747509, current_train_items 91712.
I0302 18:59:51.910286 22447428132992 run.py:483] Algo bellman_ford step 2866 current loss 0.924494, current_train_items 91744.
I0302 18:59:51.933533 22447428132992 run.py:483] Algo bellman_ford step 2867 current loss 1.063275, current_train_items 91776.
I0302 18:59:51.963607 22447428132992 run.py:483] Algo bellman_ford step 2868 current loss 1.044142, current_train_items 91808.
I0302 18:59:51.997590 22447428132992 run.py:483] Algo bellman_ford step 2869 current loss 1.157862, current_train_items 91840.
I0302 18:59:52.017524 22447428132992 run.py:483] Algo bellman_ford step 2870 current loss 0.611584, current_train_items 91872.
I0302 18:59:52.033712 22447428132992 run.py:483] Algo bellman_ford step 2871 current loss 0.751310, current_train_items 91904.
I0302 18:59:52.057530 22447428132992 run.py:483] Algo bellman_ford step 2872 current loss 0.951196, current_train_items 91936.
I0302 18:59:52.089269 22447428132992 run.py:483] Algo bellman_ford step 2873 current loss 1.172635, current_train_items 91968.
I0302 18:59:52.121572 22447428132992 run.py:483] Algo bellman_ford step 2874 current loss 1.197923, current_train_items 92000.
I0302 18:59:52.141678 22447428132992 run.py:483] Algo bellman_ford step 2875 current loss 0.614501, current_train_items 92032.
I0302 18:59:52.157866 22447428132992 run.py:483] Algo bellman_ford step 2876 current loss 0.856337, current_train_items 92064.
I0302 18:59:52.180841 22447428132992 run.py:483] Algo bellman_ford step 2877 current loss 0.885475, current_train_items 92096.
I0302 18:59:52.212654 22447428132992 run.py:483] Algo bellman_ford step 2878 current loss 1.106078, current_train_items 92128.
I0302 18:59:52.245238 22447428132992 run.py:483] Algo bellman_ford step 2879 current loss 1.220718, current_train_items 92160.
I0302 18:59:52.265057 22447428132992 run.py:483] Algo bellman_ford step 2880 current loss 0.687114, current_train_items 92192.
I0302 18:59:52.281147 22447428132992 run.py:483] Algo bellman_ford step 2881 current loss 0.815507, current_train_items 92224.
I0302 18:59:52.304580 22447428132992 run.py:483] Algo bellman_ford step 2882 current loss 1.039949, current_train_items 92256.
I0302 18:59:52.335374 22447428132992 run.py:483] Algo bellman_ford step 2883 current loss 1.045165, current_train_items 92288.
I0302 18:59:52.366977 22447428132992 run.py:483] Algo bellman_ford step 2884 current loss 1.214545, current_train_items 92320.
I0302 18:59:52.386696 22447428132992 run.py:483] Algo bellman_ford step 2885 current loss 0.574907, current_train_items 92352.
I0302 18:59:52.402802 22447428132992 run.py:483] Algo bellman_ford step 2886 current loss 0.786501, current_train_items 92384.
I0302 18:59:52.426061 22447428132992 run.py:483] Algo bellman_ford step 2887 current loss 1.064626, current_train_items 92416.
I0302 18:59:52.456111 22447428132992 run.py:483] Algo bellman_ford step 2888 current loss 0.999329, current_train_items 92448.
I0302 18:59:52.490242 22447428132992 run.py:483] Algo bellman_ford step 2889 current loss 1.310736, current_train_items 92480.
I0302 18:59:52.510431 22447428132992 run.py:483] Algo bellman_ford step 2890 current loss 0.557602, current_train_items 92512.
I0302 18:59:52.526521 22447428132992 run.py:483] Algo bellman_ford step 2891 current loss 0.769458, current_train_items 92544.
I0302 18:59:52.551021 22447428132992 run.py:483] Algo bellman_ford step 2892 current loss 1.037683, current_train_items 92576.
I0302 18:59:52.580598 22447428132992 run.py:483] Algo bellman_ford step 2893 current loss 1.104554, current_train_items 92608.
I0302 18:59:52.614460 22447428132992 run.py:483] Algo bellman_ford step 2894 current loss 1.316581, current_train_items 92640.
I0302 18:59:52.634160 22447428132992 run.py:483] Algo bellman_ford step 2895 current loss 0.571001, current_train_items 92672.
I0302 18:59:52.650889 22447428132992 run.py:483] Algo bellman_ford step 2896 current loss 0.978854, current_train_items 92704.
I0302 18:59:52.672914 22447428132992 run.py:483] Algo bellman_ford step 2897 current loss 0.919053, current_train_items 92736.
I0302 18:59:52.703153 22447428132992 run.py:483] Algo bellman_ford step 2898 current loss 1.024982, current_train_items 92768.
I0302 18:59:52.734122 22447428132992 run.py:483] Algo bellman_ford step 2899 current loss 1.001648, current_train_items 92800.
I0302 18:59:52.753830 22447428132992 run.py:483] Algo bellman_ford step 2900 current loss 0.554181, current_train_items 92832.
I0302 18:59:52.761773 22447428132992 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:52.761912 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 18:59:52.778031 22447428132992 run.py:483] Algo bellman_ford step 2901 current loss 0.756116, current_train_items 92864.
I0302 18:59:52.801090 22447428132992 run.py:483] Algo bellman_ford step 2902 current loss 0.983050, current_train_items 92896.
I0302 18:59:52.831781 22447428132992 run.py:483] Algo bellman_ford step 2903 current loss 1.064577, current_train_items 92928.
I0302 18:59:52.866335 22447428132992 run.py:483] Algo bellman_ford step 2904 current loss 1.244656, current_train_items 92960.
I0302 18:59:52.886270 22447428132992 run.py:483] Algo bellman_ford step 2905 current loss 0.775435, current_train_items 92992.
I0302 18:59:52.902282 22447428132992 run.py:483] Algo bellman_ford step 2906 current loss 0.890373, current_train_items 93024.
I0302 18:59:52.926110 22447428132992 run.py:483] Algo bellman_ford step 2907 current loss 1.041707, current_train_items 93056.
I0302 18:59:52.958420 22447428132992 run.py:483] Algo bellman_ford step 2908 current loss 1.186761, current_train_items 93088.
I0302 18:59:52.992538 22447428132992 run.py:483] Algo bellman_ford step 2909 current loss 1.183350, current_train_items 93120.
I0302 18:59:53.012117 22447428132992 run.py:483] Algo bellman_ford step 2910 current loss 0.598706, current_train_items 93152.
I0302 18:59:53.028663 22447428132992 run.py:483] Algo bellman_ford step 2911 current loss 0.831278, current_train_items 93184.
I0302 18:59:53.051964 22447428132992 run.py:483] Algo bellman_ford step 2912 current loss 0.960216, current_train_items 93216.
I0302 18:59:53.082221 22447428132992 run.py:483] Algo bellman_ford step 2913 current loss 1.020269, current_train_items 93248.
I0302 18:59:53.117261 22447428132992 run.py:483] Algo bellman_ford step 2914 current loss 1.394122, current_train_items 93280.
I0302 18:59:53.137046 22447428132992 run.py:483] Algo bellman_ford step 2915 current loss 0.570782, current_train_items 93312.
I0302 18:59:53.153177 22447428132992 run.py:483] Algo bellman_ford step 2916 current loss 0.729189, current_train_items 93344.
I0302 18:59:53.176017 22447428132992 run.py:483] Algo bellman_ford step 2917 current loss 0.955007, current_train_items 93376.
I0302 18:59:53.207947 22447428132992 run.py:483] Algo bellman_ford step 2918 current loss 1.124522, current_train_items 93408.
I0302 18:59:53.243190 22447428132992 run.py:483] Algo bellman_ford step 2919 current loss 1.295925, current_train_items 93440.
I0302 18:59:53.262901 22447428132992 run.py:483] Algo bellman_ford step 2920 current loss 0.679548, current_train_items 93472.
I0302 18:59:53.279021 22447428132992 run.py:483] Algo bellman_ford step 2921 current loss 0.911529, current_train_items 93504.
I0302 18:59:53.303046 22447428132992 run.py:483] Algo bellman_ford step 2922 current loss 1.068977, current_train_items 93536.
I0302 18:59:53.332991 22447428132992 run.py:483] Algo bellman_ford step 2923 current loss 1.024571, current_train_items 93568.
I0302 18:59:53.368505 22447428132992 run.py:483] Algo bellman_ford step 2924 current loss 1.270951, current_train_items 93600.
I0302 18:59:53.388194 22447428132992 run.py:483] Algo bellman_ford step 2925 current loss 0.711674, current_train_items 93632.
I0302 18:59:53.404191 22447428132992 run.py:483] Algo bellman_ford step 2926 current loss 0.766604, current_train_items 93664.
I0302 18:59:53.428544 22447428132992 run.py:483] Algo bellman_ford step 2927 current loss 1.106140, current_train_items 93696.
I0302 18:59:53.459822 22447428132992 run.py:483] Algo bellman_ford step 2928 current loss 1.024313, current_train_items 93728.
I0302 18:59:53.493746 22447428132992 run.py:483] Algo bellman_ford step 2929 current loss 1.256705, current_train_items 93760.
I0302 18:59:53.513376 22447428132992 run.py:483] Algo bellman_ford step 2930 current loss 0.618724, current_train_items 93792.
I0302 18:59:53.529321 22447428132992 run.py:483] Algo bellman_ford step 2931 current loss 0.688780, current_train_items 93824.
I0302 18:59:53.554337 22447428132992 run.py:483] Algo bellman_ford step 2932 current loss 1.080422, current_train_items 93856.
I0302 18:59:53.584240 22447428132992 run.py:483] Algo bellman_ford step 2933 current loss 1.146804, current_train_items 93888.
I0302 18:59:53.617330 22447428132992 run.py:483] Algo bellman_ford step 2934 current loss 1.190359, current_train_items 93920.
I0302 18:59:53.637417 22447428132992 run.py:483] Algo bellman_ford step 2935 current loss 0.644929, current_train_items 93952.
I0302 18:59:53.653287 22447428132992 run.py:483] Algo bellman_ford step 2936 current loss 0.737680, current_train_items 93984.
I0302 18:59:53.677030 22447428132992 run.py:483] Algo bellman_ford step 2937 current loss 1.027879, current_train_items 94016.
I0302 18:59:53.705944 22447428132992 run.py:483] Algo bellman_ford step 2938 current loss 1.018120, current_train_items 94048.
I0302 18:59:53.739376 22447428132992 run.py:483] Algo bellman_ford step 2939 current loss 1.196091, current_train_items 94080.
I0302 18:59:53.758696 22447428132992 run.py:483] Algo bellman_ford step 2940 current loss 0.679925, current_train_items 94112.
I0302 18:59:53.775027 22447428132992 run.py:483] Algo bellman_ford step 2941 current loss 0.925648, current_train_items 94144.
I0302 18:59:53.799047 22447428132992 run.py:483] Algo bellman_ford step 2942 current loss 1.013266, current_train_items 94176.
I0302 18:59:53.830636 22447428132992 run.py:483] Algo bellman_ford step 2943 current loss 1.132904, current_train_items 94208.
I0302 18:59:53.866043 22447428132992 run.py:483] Algo bellman_ford step 2944 current loss 1.176659, current_train_items 94240.
I0302 18:59:53.885712 22447428132992 run.py:483] Algo bellman_ford step 2945 current loss 0.644879, current_train_items 94272.
I0302 18:59:53.902096 22447428132992 run.py:483] Algo bellman_ford step 2946 current loss 0.770389, current_train_items 94304.
I0302 18:59:53.925686 22447428132992 run.py:483] Algo bellman_ford step 2947 current loss 0.935983, current_train_items 94336.
I0302 18:59:53.955977 22447428132992 run.py:483] Algo bellman_ford step 2948 current loss 1.127941, current_train_items 94368.
I0302 18:59:53.989458 22447428132992 run.py:483] Algo bellman_ford step 2949 current loss 1.447979, current_train_items 94400.
I0302 18:59:54.008943 22447428132992 run.py:483] Algo bellman_ford step 2950 current loss 0.636442, current_train_items 94432.
I0302 18:59:54.017010 22447428132992 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.798828125, 'score': 0.798828125, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:54.017115 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.799, val scores are: bellman_ford: 0.799
I0302 18:59:54.033753 22447428132992 run.py:483] Algo bellman_ford step 2951 current loss 0.920599, current_train_items 94464.
I0302 18:59:54.055739 22447428132992 run.py:483] Algo bellman_ford step 2952 current loss 0.899272, current_train_items 94496.
I0302 18:59:54.086196 22447428132992 run.py:483] Algo bellman_ford step 2953 current loss 1.140270, current_train_items 94528.
I0302 18:59:54.120291 22447428132992 run.py:483] Algo bellman_ford step 2954 current loss 1.660565, current_train_items 94560.
I0302 18:59:54.140103 22447428132992 run.py:483] Algo bellman_ford step 2955 current loss 0.614249, current_train_items 94592.
I0302 18:59:54.155587 22447428132992 run.py:483] Algo bellman_ford step 2956 current loss 0.755584, current_train_items 94624.
I0302 18:59:54.179510 22447428132992 run.py:483] Algo bellman_ford step 2957 current loss 1.098286, current_train_items 94656.
I0302 18:59:54.210957 22447428132992 run.py:483] Algo bellman_ford step 2958 current loss 1.087822, current_train_items 94688.
I0302 18:59:54.244554 22447428132992 run.py:483] Algo bellman_ford step 2959 current loss 1.264467, current_train_items 94720.
I0302 18:59:54.264585 22447428132992 run.py:483] Algo bellman_ford step 2960 current loss 0.654300, current_train_items 94752.
I0302 18:59:54.280968 22447428132992 run.py:483] Algo bellman_ford step 2961 current loss 0.794850, current_train_items 94784.
I0302 18:59:54.303696 22447428132992 run.py:483] Algo bellman_ford step 2962 current loss 1.055341, current_train_items 94816.
I0302 18:59:54.333290 22447428132992 run.py:483] Algo bellman_ford step 2963 current loss 1.114483, current_train_items 94848.
I0302 18:59:54.367600 22447428132992 run.py:483] Algo bellman_ford step 2964 current loss 1.252623, current_train_items 94880.
I0302 18:59:54.386824 22447428132992 run.py:483] Algo bellman_ford step 2965 current loss 0.552179, current_train_items 94912.
I0302 18:59:54.402875 22447428132992 run.py:483] Algo bellman_ford step 2966 current loss 0.796915, current_train_items 94944.
I0302 18:59:54.425967 22447428132992 run.py:483] Algo bellman_ford step 2967 current loss 0.998059, current_train_items 94976.
I0302 18:59:54.456030 22447428132992 run.py:483] Algo bellman_ford step 2968 current loss 1.051958, current_train_items 95008.
I0302 18:59:54.490507 22447428132992 run.py:483] Algo bellman_ford step 2969 current loss 1.244834, current_train_items 95040.
I0302 18:59:54.510123 22447428132992 run.py:483] Algo bellman_ford step 2970 current loss 0.893988, current_train_items 95072.
I0302 18:59:54.526031 22447428132992 run.py:483] Algo bellman_ford step 2971 current loss 0.759557, current_train_items 95104.
I0302 18:59:54.548849 22447428132992 run.py:483] Algo bellman_ford step 2972 current loss 1.027275, current_train_items 95136.
I0302 18:59:54.578557 22447428132992 run.py:483] Algo bellman_ford step 2973 current loss 1.042079, current_train_items 95168.
I0302 18:59:54.614562 22447428132992 run.py:483] Algo bellman_ford step 2974 current loss 1.534903, current_train_items 95200.
I0302 18:59:54.634211 22447428132992 run.py:483] Algo bellman_ford step 2975 current loss 0.724414, current_train_items 95232.
I0302 18:59:54.650940 22447428132992 run.py:483] Algo bellman_ford step 2976 current loss 0.875648, current_train_items 95264.
I0302 18:59:54.673877 22447428132992 run.py:483] Algo bellman_ford step 2977 current loss 1.025442, current_train_items 95296.
I0302 18:59:54.703859 22447428132992 run.py:483] Algo bellman_ford step 2978 current loss 1.152049, current_train_items 95328.
I0302 18:59:54.738228 22447428132992 run.py:483] Algo bellman_ford step 2979 current loss 1.479664, current_train_items 95360.
I0302 18:59:54.757440 22447428132992 run.py:483] Algo bellman_ford step 2980 current loss 0.722812, current_train_items 95392.
I0302 18:59:54.773368 22447428132992 run.py:483] Algo bellman_ford step 2981 current loss 0.727144, current_train_items 95424.
I0302 18:59:54.797290 22447428132992 run.py:483] Algo bellman_ford step 2982 current loss 1.087447, current_train_items 95456.
I0302 18:59:54.829600 22447428132992 run.py:483] Algo bellman_ford step 2983 current loss 1.158270, current_train_items 95488.
I0302 18:59:54.863306 22447428132992 run.py:483] Algo bellman_ford step 2984 current loss 1.204189, current_train_items 95520.
I0302 18:59:54.882816 22447428132992 run.py:483] Algo bellman_ford step 2985 current loss 0.632526, current_train_items 95552.
I0302 18:59:54.899425 22447428132992 run.py:483] Algo bellman_ford step 2986 current loss 0.954881, current_train_items 95584.
I0302 18:59:54.923227 22447428132992 run.py:483] Algo bellman_ford step 2987 current loss 0.990079, current_train_items 95616.
I0302 18:59:54.953525 22447428132992 run.py:483] Algo bellman_ford step 2988 current loss 1.049122, current_train_items 95648.
I0302 18:59:54.987321 22447428132992 run.py:483] Algo bellman_ford step 2989 current loss 1.248237, current_train_items 95680.
I0302 18:59:55.006928 22447428132992 run.py:483] Algo bellman_ford step 2990 current loss 0.543372, current_train_items 95712.
I0302 18:59:55.023648 22447428132992 run.py:483] Algo bellman_ford step 2991 current loss 0.949500, current_train_items 95744.
I0302 18:59:55.047099 22447428132992 run.py:483] Algo bellman_ford step 2992 current loss 1.037641, current_train_items 95776.
I0302 18:59:55.078637 22447428132992 run.py:483] Algo bellman_ford step 2993 current loss 1.141159, current_train_items 95808.
I0302 18:59:55.113322 22447428132992 run.py:483] Algo bellman_ford step 2994 current loss 1.257031, current_train_items 95840.
I0302 18:59:55.132675 22447428132992 run.py:483] Algo bellman_ford step 2995 current loss 0.697757, current_train_items 95872.
I0302 18:59:55.149025 22447428132992 run.py:483] Algo bellman_ford step 2996 current loss 0.900161, current_train_items 95904.
I0302 18:59:55.173634 22447428132992 run.py:483] Algo bellman_ford step 2997 current loss 1.046117, current_train_items 95936.
I0302 18:59:55.204781 22447428132992 run.py:483] Algo bellman_ford step 2998 current loss 1.013462, current_train_items 95968.
I0302 18:59:55.237283 22447428132992 run.py:483] Algo bellman_ford step 2999 current loss 1.173333, current_train_items 96000.
I0302 18:59:55.257364 22447428132992 run.py:483] Algo bellman_ford step 3000 current loss 0.545126, current_train_items 96032.
I0302 18:59:55.265285 22447428132992 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:55.265398 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:55.281886 22447428132992 run.py:483] Algo bellman_ford step 3001 current loss 0.860898, current_train_items 96064.
I0302 18:59:55.305328 22447428132992 run.py:483] Algo bellman_ford step 3002 current loss 0.906594, current_train_items 96096.
I0302 18:59:55.336449 22447428132992 run.py:483] Algo bellman_ford step 3003 current loss 1.115388, current_train_items 96128.
I0302 18:59:55.369631 22447428132992 run.py:483] Algo bellman_ford step 3004 current loss 1.191889, current_train_items 96160.
I0302 18:59:55.389590 22447428132992 run.py:483] Algo bellman_ford step 3005 current loss 0.573219, current_train_items 96192.
I0302 18:59:55.405395 22447428132992 run.py:483] Algo bellman_ford step 3006 current loss 0.849297, current_train_items 96224.
I0302 18:59:55.429644 22447428132992 run.py:483] Algo bellman_ford step 3007 current loss 0.965906, current_train_items 96256.
I0302 18:59:55.461231 22447428132992 run.py:483] Algo bellman_ford step 3008 current loss 1.171340, current_train_items 96288.
I0302 18:59:55.497299 22447428132992 run.py:483] Algo bellman_ford step 3009 current loss 1.443686, current_train_items 96320.
I0302 18:59:55.516838 22447428132992 run.py:483] Algo bellman_ford step 3010 current loss 0.607126, current_train_items 96352.
I0302 18:59:55.533058 22447428132992 run.py:483] Algo bellman_ford step 3011 current loss 0.874669, current_train_items 96384.
I0302 18:59:55.556583 22447428132992 run.py:483] Algo bellman_ford step 3012 current loss 1.007006, current_train_items 96416.
I0302 18:59:55.586707 22447428132992 run.py:483] Algo bellman_ford step 3013 current loss 1.021749, current_train_items 96448.
I0302 18:59:55.621194 22447428132992 run.py:483] Algo bellman_ford step 3014 current loss 1.226094, current_train_items 96480.
I0302 18:59:55.640944 22447428132992 run.py:483] Algo bellman_ford step 3015 current loss 0.594548, current_train_items 96512.
I0302 18:59:55.657516 22447428132992 run.py:483] Algo bellman_ford step 3016 current loss 0.919507, current_train_items 96544.
I0302 18:59:55.680786 22447428132992 run.py:483] Algo bellman_ford step 3017 current loss 1.078330, current_train_items 96576.
I0302 18:59:55.710702 22447428132992 run.py:483] Algo bellman_ford step 3018 current loss 1.012282, current_train_items 96608.
I0302 18:59:55.742773 22447428132992 run.py:483] Algo bellman_ford step 3019 current loss 1.140935, current_train_items 96640.
I0302 18:59:55.762170 22447428132992 run.py:483] Algo bellman_ford step 3020 current loss 0.619064, current_train_items 96672.
I0302 18:59:55.777899 22447428132992 run.py:483] Algo bellman_ford step 3021 current loss 0.800598, current_train_items 96704.
I0302 18:59:55.802231 22447428132992 run.py:483] Algo bellman_ford step 3022 current loss 1.025504, current_train_items 96736.
I0302 18:59:55.832889 22447428132992 run.py:483] Algo bellman_ford step 3023 current loss 1.077105, current_train_items 96768.
I0302 18:59:55.866287 22447428132992 run.py:483] Algo bellman_ford step 3024 current loss 1.213481, current_train_items 96800.
I0302 18:59:55.885700 22447428132992 run.py:483] Algo bellman_ford step 3025 current loss 0.574842, current_train_items 96832.
I0302 18:59:55.901674 22447428132992 run.py:483] Algo bellman_ford step 3026 current loss 0.847645, current_train_items 96864.
I0302 18:59:55.924075 22447428132992 run.py:483] Algo bellman_ford step 3027 current loss 0.923136, current_train_items 96896.
I0302 18:59:55.954512 22447428132992 run.py:483] Algo bellman_ford step 3028 current loss 0.992415, current_train_items 96928.
I0302 18:59:55.986546 22447428132992 run.py:483] Algo bellman_ford step 3029 current loss 1.129143, current_train_items 96960.
I0302 18:59:56.005942 22447428132992 run.py:483] Algo bellman_ford step 3030 current loss 0.607176, current_train_items 96992.
I0302 18:59:56.021806 22447428132992 run.py:483] Algo bellman_ford step 3031 current loss 0.750565, current_train_items 97024.
I0302 18:59:56.046077 22447428132992 run.py:483] Algo bellman_ford step 3032 current loss 1.002875, current_train_items 97056.
I0302 18:59:56.077367 22447428132992 run.py:483] Algo bellman_ford step 3033 current loss 1.157752, current_train_items 97088.
I0302 18:59:56.111579 22447428132992 run.py:483] Algo bellman_ford step 3034 current loss 1.275201, current_train_items 97120.
I0302 18:59:56.131188 22447428132992 run.py:483] Algo bellman_ford step 3035 current loss 0.635269, current_train_items 97152.
I0302 18:59:56.147639 22447428132992 run.py:483] Algo bellman_ford step 3036 current loss 0.850159, current_train_items 97184.
I0302 18:59:56.170842 22447428132992 run.py:483] Algo bellman_ford step 3037 current loss 1.058869, current_train_items 97216.
I0302 18:59:56.201817 22447428132992 run.py:483] Algo bellman_ford step 3038 current loss 1.085673, current_train_items 97248.
I0302 18:59:56.233883 22447428132992 run.py:483] Algo bellman_ford step 3039 current loss 1.201946, current_train_items 97280.
I0302 18:59:56.253322 22447428132992 run.py:483] Algo bellman_ford step 3040 current loss 0.787906, current_train_items 97312.
I0302 18:59:56.269422 22447428132992 run.py:483] Algo bellman_ford step 3041 current loss 0.825847, current_train_items 97344.
I0302 18:59:56.292793 22447428132992 run.py:483] Algo bellman_ford step 3042 current loss 0.951635, current_train_items 97376.
I0302 18:59:56.323439 22447428132992 run.py:483] Algo bellman_ford step 3043 current loss 0.998394, current_train_items 97408.
I0302 18:59:56.356057 22447428132992 run.py:483] Algo bellman_ford step 3044 current loss 1.108665, current_train_items 97440.
I0302 18:59:56.375558 22447428132992 run.py:483] Algo bellman_ford step 3045 current loss 0.667715, current_train_items 97472.
I0302 18:59:56.391641 22447428132992 run.py:483] Algo bellman_ford step 3046 current loss 0.836404, current_train_items 97504.
I0302 18:59:56.414833 22447428132992 run.py:483] Algo bellman_ford step 3047 current loss 1.061744, current_train_items 97536.
I0302 18:59:56.446063 22447428132992 run.py:483] Algo bellman_ford step 3048 current loss 1.036474, current_train_items 97568.
I0302 18:59:56.479251 22447428132992 run.py:483] Algo bellman_ford step 3049 current loss 1.233467, current_train_items 97600.
I0302 18:59:56.498798 22447428132992 run.py:483] Algo bellman_ford step 3050 current loss 0.522435, current_train_items 97632.
I0302 18:59:56.506936 22447428132992 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 18:59:56.507040 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:56.523455 22447428132992 run.py:483] Algo bellman_ford step 3051 current loss 0.811686, current_train_items 97664.
I0302 18:59:56.547267 22447428132992 run.py:483] Algo bellman_ford step 3052 current loss 1.146573, current_train_items 97696.
I0302 18:59:56.578536 22447428132992 run.py:483] Algo bellman_ford step 3053 current loss 1.041104, current_train_items 97728.
I0302 18:59:56.610735 22447428132992 run.py:483] Algo bellman_ford step 3054 current loss 1.230776, current_train_items 97760.
I0302 18:59:56.630940 22447428132992 run.py:483] Algo bellman_ford step 3055 current loss 0.571369, current_train_items 97792.
I0302 18:59:56.646697 22447428132992 run.py:483] Algo bellman_ford step 3056 current loss 0.861184, current_train_items 97824.
I0302 18:59:56.669985 22447428132992 run.py:483] Algo bellman_ford step 3057 current loss 0.915245, current_train_items 97856.
I0302 18:59:56.700928 22447428132992 run.py:483] Algo bellman_ford step 3058 current loss 1.007169, current_train_items 97888.
I0302 18:59:56.733127 22447428132992 run.py:483] Algo bellman_ford step 3059 current loss 1.153942, current_train_items 97920.
I0302 18:59:56.753060 22447428132992 run.py:483] Algo bellman_ford step 3060 current loss 0.587472, current_train_items 97952.
I0302 18:59:56.769444 22447428132992 run.py:483] Algo bellman_ford step 3061 current loss 0.784451, current_train_items 97984.
I0302 18:59:56.792238 22447428132992 run.py:483] Algo bellman_ford step 3062 current loss 0.904679, current_train_items 98016.
I0302 18:59:56.824173 22447428132992 run.py:483] Algo bellman_ford step 3063 current loss 1.201685, current_train_items 98048.
I0302 18:59:56.858352 22447428132992 run.py:483] Algo bellman_ford step 3064 current loss 1.159000, current_train_items 98080.
I0302 18:59:56.878241 22447428132992 run.py:483] Algo bellman_ford step 3065 current loss 0.686102, current_train_items 98112.
I0302 18:59:56.894297 22447428132992 run.py:483] Algo bellman_ford step 3066 current loss 0.893653, current_train_items 98144.
I0302 18:59:56.918783 22447428132992 run.py:483] Algo bellman_ford step 3067 current loss 1.022312, current_train_items 98176.
I0302 18:59:56.949071 22447428132992 run.py:483] Algo bellman_ford step 3068 current loss 1.112741, current_train_items 98208.
I0302 18:59:56.984978 22447428132992 run.py:483] Algo bellman_ford step 3069 current loss 1.679848, current_train_items 98240.
I0302 18:59:57.005074 22447428132992 run.py:483] Algo bellman_ford step 3070 current loss 0.581369, current_train_items 98272.
I0302 18:59:57.021282 22447428132992 run.py:483] Algo bellman_ford step 3071 current loss 0.799628, current_train_items 98304.
I0302 18:59:57.044149 22447428132992 run.py:483] Algo bellman_ford step 3072 current loss 0.994972, current_train_items 98336.
I0302 18:59:57.074016 22447428132992 run.py:483] Algo bellman_ford step 3073 current loss 1.043817, current_train_items 98368.
I0302 18:59:57.109316 22447428132992 run.py:483] Algo bellman_ford step 3074 current loss 1.321842, current_train_items 98400.
I0302 18:59:57.129223 22447428132992 run.py:483] Algo bellman_ford step 3075 current loss 0.721464, current_train_items 98432.
I0302 18:59:57.146354 22447428132992 run.py:483] Algo bellman_ford step 3076 current loss 0.922580, current_train_items 98464.
I0302 18:59:57.169631 22447428132992 run.py:483] Algo bellman_ford step 3077 current loss 0.936460, current_train_items 98496.
I0302 18:59:57.200562 22447428132992 run.py:483] Algo bellman_ford step 3078 current loss 1.135915, current_train_items 98528.
I0302 18:59:57.232287 22447428132992 run.py:483] Algo bellman_ford step 3079 current loss 1.142727, current_train_items 98560.
I0302 18:59:57.252087 22447428132992 run.py:483] Algo bellman_ford step 3080 current loss 0.750117, current_train_items 98592.
I0302 18:59:57.268402 22447428132992 run.py:483] Algo bellman_ford step 3081 current loss 0.886024, current_train_items 98624.
I0302 18:59:57.291626 22447428132992 run.py:483] Algo bellman_ford step 3082 current loss 0.977555, current_train_items 98656.
I0302 18:59:57.322273 22447428132992 run.py:483] Algo bellman_ford step 3083 current loss 1.144192, current_train_items 98688.
I0302 18:59:57.356028 22447428132992 run.py:483] Algo bellman_ford step 3084 current loss 1.149554, current_train_items 98720.
I0302 18:59:57.375796 22447428132992 run.py:483] Algo bellman_ford step 3085 current loss 0.622220, current_train_items 98752.
I0302 18:59:57.392158 22447428132992 run.py:483] Algo bellman_ford step 3086 current loss 0.759603, current_train_items 98784.
I0302 18:59:57.414130 22447428132992 run.py:483] Algo bellman_ford step 3087 current loss 0.964610, current_train_items 98816.
I0302 18:59:57.444763 22447428132992 run.py:483] Algo bellman_ford step 3088 current loss 1.061623, current_train_items 98848.
I0302 18:59:57.479634 22447428132992 run.py:483] Algo bellman_ford step 3089 current loss 1.197445, current_train_items 98880.
I0302 18:59:57.499638 22447428132992 run.py:483] Algo bellman_ford step 3090 current loss 0.542244, current_train_items 98912.
I0302 18:59:57.515631 22447428132992 run.py:483] Algo bellman_ford step 3091 current loss 0.716895, current_train_items 98944.
I0302 18:59:57.537686 22447428132992 run.py:483] Algo bellman_ford step 3092 current loss 0.892818, current_train_items 98976.
I0302 18:59:57.568623 22447428132992 run.py:483] Algo bellman_ford step 3093 current loss 1.124944, current_train_items 99008.
I0302 18:59:57.601871 22447428132992 run.py:483] Algo bellman_ford step 3094 current loss 1.274346, current_train_items 99040.
I0302 18:59:57.621542 22447428132992 run.py:483] Algo bellman_ford step 3095 current loss 0.589367, current_train_items 99072.
I0302 18:59:57.638134 22447428132992 run.py:483] Algo bellman_ford step 3096 current loss 0.840190, current_train_items 99104.
I0302 18:59:57.662643 22447428132992 run.py:483] Algo bellman_ford step 3097 current loss 1.024726, current_train_items 99136.
I0302 18:59:57.692728 22447428132992 run.py:483] Algo bellman_ford step 3098 current loss 1.064199, current_train_items 99168.
I0302 18:59:57.725655 22447428132992 run.py:483] Algo bellman_ford step 3099 current loss 1.309841, current_train_items 99200.
I0302 18:59:57.745489 22447428132992 run.py:483] Algo bellman_ford step 3100 current loss 0.627927, current_train_items 99232.
I0302 18:59:57.753481 22447428132992 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 18:59:57.753584 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 18:59:57.771050 22447428132992 run.py:483] Algo bellman_ford step 3101 current loss 0.972388, current_train_items 99264.
I0302 18:59:57.795107 22447428132992 run.py:483] Algo bellman_ford step 3102 current loss 1.080281, current_train_items 99296.
I0302 18:59:57.826610 22447428132992 run.py:483] Algo bellman_ford step 3103 current loss 0.980796, current_train_items 99328.
I0302 18:59:57.861140 22447428132992 run.py:483] Algo bellman_ford step 3104 current loss 1.169704, current_train_items 99360.
I0302 18:59:57.881494 22447428132992 run.py:483] Algo bellman_ford step 3105 current loss 0.732468, current_train_items 99392.
I0302 18:59:57.896914 22447428132992 run.py:483] Algo bellman_ford step 3106 current loss 0.810663, current_train_items 99424.
I0302 18:59:57.920091 22447428132992 run.py:483] Algo bellman_ford step 3107 current loss 0.990246, current_train_items 99456.
I0302 18:59:57.951625 22447428132992 run.py:483] Algo bellman_ford step 3108 current loss 1.139311, current_train_items 99488.
I0302 18:59:57.984032 22447428132992 run.py:483] Algo bellman_ford step 3109 current loss 1.182541, current_train_items 99520.
I0302 18:59:58.003669 22447428132992 run.py:483] Algo bellman_ford step 3110 current loss 0.641068, current_train_items 99552.
I0302 18:59:58.019857 22447428132992 run.py:483] Algo bellman_ford step 3111 current loss 0.825932, current_train_items 99584.
I0302 18:59:58.043123 22447428132992 run.py:483] Algo bellman_ford step 3112 current loss 0.948579, current_train_items 99616.
I0302 18:59:58.073706 22447428132992 run.py:483] Algo bellman_ford step 3113 current loss 1.115642, current_train_items 99648.
I0302 18:59:58.107810 22447428132992 run.py:483] Algo bellman_ford step 3114 current loss 1.192067, current_train_items 99680.
I0302 18:59:58.127680 22447428132992 run.py:483] Algo bellman_ford step 3115 current loss 0.580631, current_train_items 99712.
I0302 18:59:58.144008 22447428132992 run.py:483] Algo bellman_ford step 3116 current loss 0.797763, current_train_items 99744.
I0302 18:59:58.167344 22447428132992 run.py:483] Algo bellman_ford step 3117 current loss 0.989899, current_train_items 99776.
I0302 18:59:58.198567 22447428132992 run.py:483] Algo bellman_ford step 3118 current loss 1.115196, current_train_items 99808.
I0302 18:59:58.231308 22447428132992 run.py:483] Algo bellman_ford step 3119 current loss 1.239628, current_train_items 99840.
I0302 18:59:58.250861 22447428132992 run.py:483] Algo bellman_ford step 3120 current loss 0.606496, current_train_items 99872.
I0302 18:59:58.266956 22447428132992 run.py:483] Algo bellman_ford step 3121 current loss 0.806977, current_train_items 99904.
I0302 18:59:58.290651 22447428132992 run.py:483] Algo bellman_ford step 3122 current loss 0.927790, current_train_items 99936.
I0302 18:59:58.321710 22447428132992 run.py:483] Algo bellman_ford step 3123 current loss 1.041762, current_train_items 99968.
I0302 18:59:58.355038 22447428132992 run.py:483] Algo bellman_ford step 3124 current loss 1.178013, current_train_items 100000.
I0302 18:59:58.374417 22447428132992 run.py:483] Algo bellman_ford step 3125 current loss 0.700677, current_train_items 100032.
I0302 18:59:58.390610 22447428132992 run.py:483] Algo bellman_ford step 3126 current loss 0.862602, current_train_items 100064.
I0302 18:59:58.413265 22447428132992 run.py:483] Algo bellman_ford step 3127 current loss 0.860617, current_train_items 100096.
I0302 18:59:58.445061 22447428132992 run.py:483] Algo bellman_ford step 3128 current loss 1.074538, current_train_items 100128.
I0302 18:59:58.476987 22447428132992 run.py:483] Algo bellman_ford step 3129 current loss 1.331131, current_train_items 100160.
I0302 18:59:58.496569 22447428132992 run.py:483] Algo bellman_ford step 3130 current loss 0.559874, current_train_items 100192.
I0302 18:59:58.512678 22447428132992 run.py:483] Algo bellman_ford step 3131 current loss 0.735815, current_train_items 100224.
I0302 18:59:58.536287 22447428132992 run.py:483] Algo bellman_ford step 3132 current loss 0.948675, current_train_items 100256.
I0302 18:59:58.566004 22447428132992 run.py:483] Algo bellman_ford step 3133 current loss 1.157676, current_train_items 100288.
I0302 18:59:58.601111 22447428132992 run.py:483] Algo bellman_ford step 3134 current loss 1.310135, current_train_items 100320.
I0302 18:59:58.620542 22447428132992 run.py:483] Algo bellman_ford step 3135 current loss 0.636338, current_train_items 100352.
I0302 18:59:58.636564 22447428132992 run.py:483] Algo bellman_ford step 3136 current loss 0.767658, current_train_items 100384.
I0302 18:59:58.660583 22447428132992 run.py:483] Algo bellman_ford step 3137 current loss 0.963822, current_train_items 100416.
I0302 18:59:58.690090 22447428132992 run.py:483] Algo bellman_ford step 3138 current loss 1.037968, current_train_items 100448.
I0302 18:59:58.724139 22447428132992 run.py:483] Algo bellman_ford step 3139 current loss 1.471551, current_train_items 100480.
I0302 18:59:58.743870 22447428132992 run.py:483] Algo bellman_ford step 3140 current loss 0.801947, current_train_items 100512.
I0302 18:59:58.760506 22447428132992 run.py:483] Algo bellman_ford step 3141 current loss 0.819583, current_train_items 100544.
I0302 18:59:58.782700 22447428132992 run.py:483] Algo bellman_ford step 3142 current loss 0.927965, current_train_items 100576.
I0302 18:59:58.814572 22447428132992 run.py:483] Algo bellman_ford step 3143 current loss 1.012472, current_train_items 100608.
I0302 18:59:58.847558 22447428132992 run.py:483] Algo bellman_ford step 3144 current loss 1.216033, current_train_items 100640.
I0302 18:59:58.867130 22447428132992 run.py:483] Algo bellman_ford step 3145 current loss 0.556715, current_train_items 100672.
I0302 18:59:58.883217 22447428132992 run.py:483] Algo bellman_ford step 3146 current loss 0.795844, current_train_items 100704.
I0302 18:59:58.905982 22447428132992 run.py:483] Algo bellman_ford step 3147 current loss 0.978450, current_train_items 100736.
I0302 18:59:58.934706 22447428132992 run.py:483] Algo bellman_ford step 3148 current loss 0.904580, current_train_items 100768.
I0302 18:59:58.970077 22447428132992 run.py:483] Algo bellman_ford step 3149 current loss 1.240608, current_train_items 100800.
I0302 18:59:58.989389 22447428132992 run.py:483] Algo bellman_ford step 3150 current loss 0.714399, current_train_items 100832.
I0302 18:59:58.997874 22447428132992 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 18:59:58.997978 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:59:59.014718 22447428132992 run.py:483] Algo bellman_ford step 3151 current loss 0.814717, current_train_items 100864.
I0302 18:59:59.039365 22447428132992 run.py:483] Algo bellman_ford step 3152 current loss 1.204916, current_train_items 100896.
I0302 18:59:59.070573 22447428132992 run.py:483] Algo bellman_ford step 3153 current loss 1.054755, current_train_items 100928.
I0302 18:59:59.105197 22447428132992 run.py:483] Algo bellman_ford step 3154 current loss 1.253014, current_train_items 100960.
I0302 18:59:59.125441 22447428132992 run.py:483] Algo bellman_ford step 3155 current loss 0.633344, current_train_items 100992.
I0302 18:59:59.141291 22447428132992 run.py:483] Algo bellman_ford step 3156 current loss 0.841304, current_train_items 101024.
I0302 18:59:59.165098 22447428132992 run.py:483] Algo bellman_ford step 3157 current loss 1.017836, current_train_items 101056.
I0302 18:59:59.194934 22447428132992 run.py:483] Algo bellman_ford step 3158 current loss 1.015974, current_train_items 101088.
I0302 18:59:59.227781 22447428132992 run.py:483] Algo bellman_ford step 3159 current loss 1.261108, current_train_items 101120.
I0302 18:59:59.247722 22447428132992 run.py:483] Algo bellman_ford step 3160 current loss 0.550710, current_train_items 101152.
I0302 18:59:59.263667 22447428132992 run.py:483] Algo bellman_ford step 3161 current loss 0.745527, current_train_items 101184.
I0302 18:59:59.286567 22447428132992 run.py:483] Algo bellman_ford step 3162 current loss 1.024574, current_train_items 101216.
I0302 18:59:59.316307 22447428132992 run.py:483] Algo bellman_ford step 3163 current loss 1.010462, current_train_items 101248.
I0302 18:59:59.351089 22447428132992 run.py:483] Algo bellman_ford step 3164 current loss 1.315895, current_train_items 101280.
I0302 18:59:59.370949 22447428132992 run.py:483] Algo bellman_ford step 3165 current loss 0.676577, current_train_items 101312.
I0302 18:59:59.387679 22447428132992 run.py:483] Algo bellman_ford step 3166 current loss 0.839452, current_train_items 101344.
I0302 18:59:59.411318 22447428132992 run.py:483] Algo bellman_ford step 3167 current loss 1.064796, current_train_items 101376.
I0302 18:59:59.442219 22447428132992 run.py:483] Algo bellman_ford step 3168 current loss 1.093778, current_train_items 101408.
I0302 18:59:59.475338 22447428132992 run.py:483] Algo bellman_ford step 3169 current loss 1.190027, current_train_items 101440.
I0302 18:59:59.495499 22447428132992 run.py:483] Algo bellman_ford step 3170 current loss 0.642185, current_train_items 101472.
I0302 18:59:59.512049 22447428132992 run.py:483] Algo bellman_ford step 3171 current loss 0.817623, current_train_items 101504.
I0302 18:59:59.535558 22447428132992 run.py:483] Algo bellman_ford step 3172 current loss 0.959010, current_train_items 101536.
I0302 18:59:59.566545 22447428132992 run.py:483] Algo bellman_ford step 3173 current loss 1.048885, current_train_items 101568.
I0302 18:59:59.597955 22447428132992 run.py:483] Algo bellman_ford step 3174 current loss 1.140410, current_train_items 101600.
I0302 18:59:59.617867 22447428132992 run.py:483] Algo bellman_ford step 3175 current loss 0.604392, current_train_items 101632.
I0302 18:59:59.634142 22447428132992 run.py:483] Algo bellman_ford step 3176 current loss 0.749990, current_train_items 101664.
I0302 18:59:59.656940 22447428132992 run.py:483] Algo bellman_ford step 3177 current loss 0.902281, current_train_items 101696.
I0302 18:59:59.685752 22447428132992 run.py:483] Algo bellman_ford step 3178 current loss 1.008442, current_train_items 101728.
I0302 18:59:59.720235 22447428132992 run.py:483] Algo bellman_ford step 3179 current loss 1.203063, current_train_items 101760.
I0302 18:59:59.739650 22447428132992 run.py:483] Algo bellman_ford step 3180 current loss 0.635961, current_train_items 101792.
I0302 18:59:59.755950 22447428132992 run.py:483] Algo bellman_ford step 3181 current loss 0.742179, current_train_items 101824.
I0302 18:59:59.779595 22447428132992 run.py:483] Algo bellman_ford step 3182 current loss 0.987376, current_train_items 101856.
I0302 18:59:59.810497 22447428132992 run.py:483] Algo bellman_ford step 3183 current loss 1.107697, current_train_items 101888.
I0302 18:59:59.846475 22447428132992 run.py:483] Algo bellman_ford step 3184 current loss 1.195196, current_train_items 101920.
I0302 18:59:59.866571 22447428132992 run.py:483] Algo bellman_ford step 3185 current loss 0.578689, current_train_items 101952.
I0302 18:59:59.882683 22447428132992 run.py:483] Algo bellman_ford step 3186 current loss 0.788512, current_train_items 101984.
I0302 18:59:59.906544 22447428132992 run.py:483] Algo bellman_ford step 3187 current loss 1.076095, current_train_items 102016.
I0302 18:59:59.936969 22447428132992 run.py:483] Algo bellman_ford step 3188 current loss 1.035025, current_train_items 102048.
I0302 18:59:59.970158 22447428132992 run.py:483] Algo bellman_ford step 3189 current loss 1.105648, current_train_items 102080.
I0302 18:59:59.989893 22447428132992 run.py:483] Algo bellman_ford step 3190 current loss 0.897553, current_train_items 102112.
I0302 19:00:00.006152 22447428132992 run.py:483] Algo bellman_ford step 3191 current loss 0.823535, current_train_items 102144.
I0302 19:00:00.029708 22447428132992 run.py:483] Algo bellman_ford step 3192 current loss 1.025334, current_train_items 102176.
I0302 19:00:00.059941 22447428132992 run.py:483] Algo bellman_ford step 3193 current loss 1.016486, current_train_items 102208.
I0302 19:00:00.093861 22447428132992 run.py:483] Algo bellman_ford step 3194 current loss 1.221287, current_train_items 102240.
I0302 19:00:00.113671 22447428132992 run.py:483] Algo bellman_ford step 3195 current loss 0.592396, current_train_items 102272.
I0302 19:00:00.129375 22447428132992 run.py:483] Algo bellman_ford step 3196 current loss 0.699419, current_train_items 102304.
I0302 19:00:00.152478 22447428132992 run.py:483] Algo bellman_ford step 3197 current loss 0.995793, current_train_items 102336.
I0302 19:00:00.183242 22447428132992 run.py:483] Algo bellman_ford step 3198 current loss 1.070121, current_train_items 102368.
I0302 19:00:00.216897 22447428132992 run.py:483] Algo bellman_ford step 3199 current loss 1.275190, current_train_items 102400.
I0302 19:00:00.237041 22447428132992 run.py:483] Algo bellman_ford step 3200 current loss 0.576707, current_train_items 102432.
I0302 19:00:00.244827 22447428132992 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 19:00:00.244934 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:00:00.261081 22447428132992 run.py:483] Algo bellman_ford step 3201 current loss 0.784991, current_train_items 102464.
I0302 19:00:00.284067 22447428132992 run.py:483] Algo bellman_ford step 3202 current loss 0.966312, current_train_items 102496.
I0302 19:00:00.316341 22447428132992 run.py:483] Algo bellman_ford step 3203 current loss 1.198557, current_train_items 102528.
I0302 19:00:00.350248 22447428132992 run.py:483] Algo bellman_ford step 3204 current loss 1.275694, current_train_items 102560.
I0302 19:00:00.370108 22447428132992 run.py:483] Algo bellman_ford step 3205 current loss 0.742106, current_train_items 102592.
I0302 19:00:00.385582 22447428132992 run.py:483] Algo bellman_ford step 3206 current loss 0.776440, current_train_items 102624.
I0302 19:00:00.407562 22447428132992 run.py:483] Algo bellman_ford step 3207 current loss 0.907492, current_train_items 102656.
I0302 19:00:00.438942 22447428132992 run.py:483] Algo bellman_ford step 3208 current loss 1.061623, current_train_items 102688.
I0302 19:00:00.472544 22447428132992 run.py:483] Algo bellman_ford step 3209 current loss 1.259423, current_train_items 102720.
I0302 19:00:00.491867 22447428132992 run.py:483] Algo bellman_ford step 3210 current loss 0.608909, current_train_items 102752.
I0302 19:00:00.507610 22447428132992 run.py:483] Algo bellman_ford step 3211 current loss 0.754386, current_train_items 102784.
I0302 19:00:00.530319 22447428132992 run.py:483] Algo bellman_ford step 3212 current loss 0.964135, current_train_items 102816.
I0302 19:00:00.560826 22447428132992 run.py:483] Algo bellman_ford step 3213 current loss 1.131273, current_train_items 102848.
I0302 19:00:00.593892 22447428132992 run.py:483] Algo bellman_ford step 3214 current loss 1.101125, current_train_items 102880.
I0302 19:00:00.613390 22447428132992 run.py:483] Algo bellman_ford step 3215 current loss 0.727338, current_train_items 102912.
I0302 19:00:00.629327 22447428132992 run.py:483] Algo bellman_ford step 3216 current loss 0.877179, current_train_items 102944.
I0302 19:00:00.653188 22447428132992 run.py:483] Algo bellman_ford step 3217 current loss 1.001952, current_train_items 102976.
I0302 19:00:00.682443 22447428132992 run.py:483] Algo bellman_ford step 3218 current loss 1.025142, current_train_items 103008.
I0302 19:00:00.714817 22447428132992 run.py:483] Algo bellman_ford step 3219 current loss 1.392336, current_train_items 103040.
I0302 19:00:00.734249 22447428132992 run.py:483] Algo bellman_ford step 3220 current loss 0.642324, current_train_items 103072.
I0302 19:00:00.750306 22447428132992 run.py:483] Algo bellman_ford step 3221 current loss 0.781700, current_train_items 103104.
I0302 19:00:00.774036 22447428132992 run.py:483] Algo bellman_ford step 3222 current loss 0.973587, current_train_items 103136.
I0302 19:00:00.804719 22447428132992 run.py:483] Algo bellman_ford step 3223 current loss 1.133327, current_train_items 103168.
I0302 19:00:00.836366 22447428132992 run.py:483] Algo bellman_ford step 3224 current loss 1.087478, current_train_items 103200.
I0302 19:00:00.855708 22447428132992 run.py:483] Algo bellman_ford step 3225 current loss 0.697303, current_train_items 103232.
I0302 19:00:00.871065 22447428132992 run.py:483] Algo bellman_ford step 3226 current loss 0.691399, current_train_items 103264.
I0302 19:00:00.894307 22447428132992 run.py:483] Algo bellman_ford step 3227 current loss 0.986322, current_train_items 103296.
I0302 19:00:00.925416 22447428132992 run.py:483] Algo bellman_ford step 3228 current loss 1.130550, current_train_items 103328.
I0302 19:00:00.962303 22447428132992 run.py:483] Algo bellman_ford step 3229 current loss 1.360247, current_train_items 103360.
I0302 19:00:00.981527 22447428132992 run.py:483] Algo bellman_ford step 3230 current loss 0.703489, current_train_items 103392.
I0302 19:00:00.997674 22447428132992 run.py:483] Algo bellman_ford step 3231 current loss 0.854065, current_train_items 103424.
I0302 19:00:01.020097 22447428132992 run.py:483] Algo bellman_ford step 3232 current loss 0.894826, current_train_items 103456.
I0302 19:00:01.050541 22447428132992 run.py:483] Algo bellman_ford step 3233 current loss 1.029136, current_train_items 103488.
I0302 19:00:01.085053 22447428132992 run.py:483] Algo bellman_ford step 3234 current loss 1.260151, current_train_items 103520.
I0302 19:00:01.104234 22447428132992 run.py:483] Algo bellman_ford step 3235 current loss 0.618754, current_train_items 103552.
I0302 19:00:01.120487 22447428132992 run.py:483] Algo bellman_ford step 3236 current loss 0.818586, current_train_items 103584.
I0302 19:00:01.142903 22447428132992 run.py:483] Algo bellman_ford step 3237 current loss 0.923418, current_train_items 103616.
I0302 19:00:01.174592 22447428132992 run.py:483] Algo bellman_ford step 3238 current loss 1.132680, current_train_items 103648.
I0302 19:00:01.207700 22447428132992 run.py:483] Algo bellman_ford step 3239 current loss 1.142164, current_train_items 103680.
I0302 19:00:01.227026 22447428132992 run.py:483] Algo bellman_ford step 3240 current loss 0.597623, current_train_items 103712.
I0302 19:00:01.243228 22447428132992 run.py:483] Algo bellman_ford step 3241 current loss 0.803263, current_train_items 103744.
I0302 19:00:01.266047 22447428132992 run.py:483] Algo bellman_ford step 3242 current loss 0.966225, current_train_items 103776.
I0302 19:00:01.295536 22447428132992 run.py:483] Algo bellman_ford step 3243 current loss 1.004750, current_train_items 103808.
I0302 19:00:01.329530 22447428132992 run.py:483] Algo bellman_ford step 3244 current loss 1.219953, current_train_items 103840.
I0302 19:00:01.348764 22447428132992 run.py:483] Algo bellman_ford step 3245 current loss 0.556339, current_train_items 103872.
I0302 19:00:01.365014 22447428132992 run.py:483] Algo bellman_ford step 3246 current loss 0.869889, current_train_items 103904.
I0302 19:00:01.387815 22447428132992 run.py:483] Algo bellman_ford step 3247 current loss 0.947825, current_train_items 103936.
I0302 19:00:01.418656 22447428132992 run.py:483] Algo bellman_ford step 3248 current loss 1.030303, current_train_items 103968.
I0302 19:00:01.450601 22447428132992 run.py:483] Algo bellman_ford step 3249 current loss 1.066227, current_train_items 104000.
I0302 19:00:01.470052 22447428132992 run.py:483] Algo bellman_ford step 3250 current loss 0.601568, current_train_items 104032.
I0302 19:00:01.478160 22447428132992 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 19:00:01.478264 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:01.494743 22447428132992 run.py:483] Algo bellman_ford step 3251 current loss 0.787766, current_train_items 104064.
I0302 19:00:01.517037 22447428132992 run.py:483] Algo bellman_ford step 3252 current loss 0.960097, current_train_items 104096.
I0302 19:00:01.549148 22447428132992 run.py:483] Algo bellman_ford step 3253 current loss 1.160261, current_train_items 104128.
I0302 19:00:01.583108 22447428132992 run.py:483] Algo bellman_ford step 3254 current loss 1.158272, current_train_items 104160.
I0302 19:00:01.603426 22447428132992 run.py:483] Algo bellman_ford step 3255 current loss 0.654249, current_train_items 104192.
I0302 19:00:01.619048 22447428132992 run.py:483] Algo bellman_ford step 3256 current loss 0.713127, current_train_items 104224.
I0302 19:00:01.641561 22447428132992 run.py:483] Algo bellman_ford step 3257 current loss 0.962226, current_train_items 104256.
I0302 19:00:01.673743 22447428132992 run.py:483] Algo bellman_ford step 3258 current loss 1.164269, current_train_items 104288.
I0302 19:00:01.708801 22447428132992 run.py:483] Algo bellman_ford step 3259 current loss 1.313375, current_train_items 104320.
I0302 19:00:01.728615 22447428132992 run.py:483] Algo bellman_ford step 3260 current loss 0.572555, current_train_items 104352.
I0302 19:00:01.744788 22447428132992 run.py:483] Algo bellman_ford step 3261 current loss 0.800156, current_train_items 104384.
I0302 19:00:01.767784 22447428132992 run.py:483] Algo bellman_ford step 3262 current loss 0.964511, current_train_items 104416.
I0302 19:00:01.798935 22447428132992 run.py:483] Algo bellman_ford step 3263 current loss 1.139710, current_train_items 104448.
I0302 19:00:01.833188 22447428132992 run.py:483] Algo bellman_ford step 3264 current loss 1.343566, current_train_items 104480.
I0302 19:00:01.852884 22447428132992 run.py:483] Algo bellman_ford step 3265 current loss 0.676566, current_train_items 104512.
I0302 19:00:01.869143 22447428132992 run.py:483] Algo bellman_ford step 3266 current loss 0.861063, current_train_items 104544.
I0302 19:00:01.892099 22447428132992 run.py:483] Algo bellman_ford step 3267 current loss 0.925098, current_train_items 104576.
I0302 19:00:01.921464 22447428132992 run.py:483] Algo bellman_ford step 3268 current loss 0.955627, current_train_items 104608.
I0302 19:00:01.957326 22447428132992 run.py:483] Algo bellman_ford step 3269 current loss 1.511695, current_train_items 104640.
I0302 19:00:01.977186 22447428132992 run.py:483] Algo bellman_ford step 3270 current loss 0.595259, current_train_items 104672.
I0302 19:00:01.993443 22447428132992 run.py:483] Algo bellman_ford step 3271 current loss 0.691177, current_train_items 104704.
I0302 19:00:02.016594 22447428132992 run.py:483] Algo bellman_ford step 3272 current loss 0.977299, current_train_items 104736.
I0302 19:00:02.046184 22447428132992 run.py:483] Algo bellman_ford step 3273 current loss 0.958286, current_train_items 104768.
I0302 19:00:02.078057 22447428132992 run.py:483] Algo bellman_ford step 3274 current loss 1.020874, current_train_items 104800.
I0302 19:00:02.097904 22447428132992 run.py:483] Algo bellman_ford step 3275 current loss 0.635033, current_train_items 104832.
I0302 19:00:02.113722 22447428132992 run.py:483] Algo bellman_ford step 3276 current loss 0.747649, current_train_items 104864.
I0302 19:00:02.136426 22447428132992 run.py:483] Algo bellman_ford step 3277 current loss 0.918728, current_train_items 104896.
I0302 19:00:02.168057 22447428132992 run.py:483] Algo bellman_ford step 3278 current loss 1.043224, current_train_items 104928.
I0302 19:00:02.202080 22447428132992 run.py:483] Algo bellman_ford step 3279 current loss 1.110099, current_train_items 104960.
I0302 19:00:02.221455 22447428132992 run.py:483] Algo bellman_ford step 3280 current loss 0.669272, current_train_items 104992.
I0302 19:00:02.237112 22447428132992 run.py:483] Algo bellman_ford step 3281 current loss 0.821203, current_train_items 105024.
I0302 19:00:02.259940 22447428132992 run.py:483] Algo bellman_ford step 3282 current loss 0.969479, current_train_items 105056.
I0302 19:00:02.291007 22447428132992 run.py:483] Algo bellman_ford step 3283 current loss 1.134744, current_train_items 105088.
I0302 19:00:02.324961 22447428132992 run.py:483] Algo bellman_ford step 3284 current loss 1.259336, current_train_items 105120.
I0302 19:00:02.344974 22447428132992 run.py:483] Algo bellman_ford step 3285 current loss 0.597530, current_train_items 105152.
I0302 19:00:02.361403 22447428132992 run.py:483] Algo bellman_ford step 3286 current loss 0.860122, current_train_items 105184.
I0302 19:00:02.384836 22447428132992 run.py:483] Algo bellman_ford step 3287 current loss 1.068399, current_train_items 105216.
I0302 19:00:02.414643 22447428132992 run.py:483] Algo bellman_ford step 3288 current loss 1.012400, current_train_items 105248.
I0302 19:00:02.446133 22447428132992 run.py:483] Algo bellman_ford step 3289 current loss 1.181892, current_train_items 105280.
I0302 19:00:02.466113 22447428132992 run.py:483] Algo bellman_ford step 3290 current loss 0.781688, current_train_items 105312.
I0302 19:00:02.482640 22447428132992 run.py:483] Algo bellman_ford step 3291 current loss 0.755868, current_train_items 105344.
I0302 19:00:02.505704 22447428132992 run.py:483] Algo bellman_ford step 3292 current loss 0.982936, current_train_items 105376.
I0302 19:00:02.536464 22447428132992 run.py:483] Algo bellman_ford step 3293 current loss 1.000867, current_train_items 105408.
I0302 19:00:02.569957 22447428132992 run.py:483] Algo bellman_ford step 3294 current loss 1.139180, current_train_items 105440.
I0302 19:00:02.589344 22447428132992 run.py:483] Algo bellman_ford step 3295 current loss 0.594992, current_train_items 105472.
I0302 19:00:02.605024 22447428132992 run.py:483] Algo bellman_ford step 3296 current loss 0.734584, current_train_items 105504.
I0302 19:00:02.628848 22447428132992 run.py:483] Algo bellman_ford step 3297 current loss 1.152659, current_train_items 105536.
I0302 19:00:02.660888 22447428132992 run.py:483] Algo bellman_ford step 3298 current loss 1.121902, current_train_items 105568.
I0302 19:00:02.695461 22447428132992 run.py:483] Algo bellman_ford step 3299 current loss 1.234004, current_train_items 105600.
I0302 19:00:02.715481 22447428132992 run.py:483] Algo bellman_ford step 3300 current loss 0.636660, current_train_items 105632.
I0302 19:00:02.723035 22447428132992 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 19:00:02.723142 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:00:02.739750 22447428132992 run.py:483] Algo bellman_ford step 3301 current loss 0.874362, current_train_items 105664.
I0302 19:00:02.763009 22447428132992 run.py:483] Algo bellman_ford step 3302 current loss 1.012630, current_train_items 105696.
I0302 19:00:02.792734 22447428132992 run.py:483] Algo bellman_ford step 3303 current loss 1.010769, current_train_items 105728.
I0302 19:00:02.824450 22447428132992 run.py:483] Algo bellman_ford step 3304 current loss 1.058880, current_train_items 105760.
I0302 19:00:02.844241 22447428132992 run.py:483] Algo bellman_ford step 3305 current loss 0.556499, current_train_items 105792.
I0302 19:00:02.860199 22447428132992 run.py:483] Algo bellman_ford step 3306 current loss 0.943273, current_train_items 105824.
I0302 19:00:02.883981 22447428132992 run.py:483] Algo bellman_ford step 3307 current loss 1.007331, current_train_items 105856.
I0302 19:00:02.914594 22447428132992 run.py:483] Algo bellman_ford step 3308 current loss 1.123868, current_train_items 105888.
I0302 19:00:02.949710 22447428132992 run.py:483] Algo bellman_ford step 3309 current loss 1.286247, current_train_items 105920.
I0302 19:00:02.969143 22447428132992 run.py:483] Algo bellman_ford step 3310 current loss 0.623794, current_train_items 105952.
I0302 19:00:02.985516 22447428132992 run.py:483] Algo bellman_ford step 3311 current loss 0.862307, current_train_items 105984.
I0302 19:00:03.008646 22447428132992 run.py:483] Algo bellman_ford step 3312 current loss 0.927656, current_train_items 106016.
I0302 19:00:03.039496 22447428132992 run.py:483] Algo bellman_ford step 3313 current loss 1.133864, current_train_items 106048.
I0302 19:00:03.071580 22447428132992 run.py:483] Algo bellman_ford step 3314 current loss 1.220210, current_train_items 106080.
I0302 19:00:03.090846 22447428132992 run.py:483] Algo bellman_ford step 3315 current loss 0.542012, current_train_items 106112.
I0302 19:00:03.107273 22447428132992 run.py:483] Algo bellman_ford step 3316 current loss 0.890697, current_train_items 106144.
I0302 19:00:03.129925 22447428132992 run.py:483] Algo bellman_ford step 3317 current loss 0.983545, current_train_items 106176.
I0302 19:00:03.160470 22447428132992 run.py:483] Algo bellman_ford step 3318 current loss 1.110073, current_train_items 106208.
I0302 19:00:03.194399 22447428132992 run.py:483] Algo bellman_ford step 3319 current loss 1.253163, current_train_items 106240.
I0302 19:00:03.213875 22447428132992 run.py:483] Algo bellman_ford step 3320 current loss 0.586013, current_train_items 106272.
I0302 19:00:03.230047 22447428132992 run.py:483] Algo bellman_ford step 3321 current loss 0.769463, current_train_items 106304.
I0302 19:00:03.253876 22447428132992 run.py:483] Algo bellman_ford step 3322 current loss 1.077134, current_train_items 106336.
I0302 19:00:03.285061 22447428132992 run.py:483] Algo bellman_ford step 3323 current loss 0.989880, current_train_items 106368.
I0302 19:00:03.317280 22447428132992 run.py:483] Algo bellman_ford step 3324 current loss 1.160643, current_train_items 106400.
I0302 19:00:03.336400 22447428132992 run.py:483] Algo bellman_ford step 3325 current loss 0.549847, current_train_items 106432.
I0302 19:00:03.352664 22447428132992 run.py:483] Algo bellman_ford step 3326 current loss 0.791195, current_train_items 106464.
I0302 19:00:03.375388 22447428132992 run.py:483] Algo bellman_ford step 3327 current loss 0.927426, current_train_items 106496.
I0302 19:00:03.405483 22447428132992 run.py:483] Algo bellman_ford step 3328 current loss 1.058159, current_train_items 106528.
I0302 19:00:03.438167 22447428132992 run.py:483] Algo bellman_ford step 3329 current loss 1.085499, current_train_items 106560.
I0302 19:00:03.457932 22447428132992 run.py:483] Algo bellman_ford step 3330 current loss 0.676731, current_train_items 106592.
I0302 19:00:03.473850 22447428132992 run.py:483] Algo bellman_ford step 3331 current loss 0.798621, current_train_items 106624.
I0302 19:00:03.496639 22447428132992 run.py:483] Algo bellman_ford step 3332 current loss 1.053779, current_train_items 106656.
I0302 19:00:03.526422 22447428132992 run.py:483] Algo bellman_ford step 3333 current loss 0.927573, current_train_items 106688.
I0302 19:00:03.558362 22447428132992 run.py:483] Algo bellman_ford step 3334 current loss 1.107404, current_train_items 106720.
I0302 19:00:03.577538 22447428132992 run.py:483] Algo bellman_ford step 3335 current loss 0.587471, current_train_items 106752.
I0302 19:00:03.593169 22447428132992 run.py:483] Algo bellman_ford step 3336 current loss 0.816351, current_train_items 106784.
I0302 19:00:03.616885 22447428132992 run.py:483] Algo bellman_ford step 3337 current loss 1.082773, current_train_items 106816.
I0302 19:00:03.648481 22447428132992 run.py:483] Algo bellman_ford step 3338 current loss 1.086054, current_train_items 106848.
I0302 19:00:03.683301 22447428132992 run.py:483] Algo bellman_ford step 3339 current loss 1.171205, current_train_items 106880.
I0302 19:00:03.702795 22447428132992 run.py:483] Algo bellman_ford step 3340 current loss 0.630608, current_train_items 106912.
I0302 19:00:03.719340 22447428132992 run.py:483] Algo bellman_ford step 3341 current loss 0.765394, current_train_items 106944.
I0302 19:00:03.742781 22447428132992 run.py:483] Algo bellman_ford step 3342 current loss 1.073283, current_train_items 106976.
I0302 19:00:03.774070 22447428132992 run.py:483] Algo bellman_ford step 3343 current loss 1.028917, current_train_items 107008.
I0302 19:00:03.805426 22447428132992 run.py:483] Algo bellman_ford step 3344 current loss 1.094644, current_train_items 107040.
I0302 19:00:03.824938 22447428132992 run.py:483] Algo bellman_ford step 3345 current loss 0.642508, current_train_items 107072.
I0302 19:00:03.840761 22447428132992 run.py:483] Algo bellman_ford step 3346 current loss 0.735125, current_train_items 107104.
I0302 19:00:03.863835 22447428132992 run.py:483] Algo bellman_ford step 3347 current loss 1.008985, current_train_items 107136.
I0302 19:00:03.893706 22447428132992 run.py:483] Algo bellman_ford step 3348 current loss 1.060710, current_train_items 107168.
I0302 19:00:03.925129 22447428132992 run.py:483] Algo bellman_ford step 3349 current loss 1.165750, current_train_items 107200.
I0302 19:00:03.944957 22447428132992 run.py:483] Algo bellman_ford step 3350 current loss 0.759399, current_train_items 107232.
I0302 19:00:03.953199 22447428132992 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.8740234375, 'score': 0.8740234375, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 19:00:03.953306 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.874, val scores are: bellman_ford: 0.874
I0302 19:00:03.970319 22447428132992 run.py:483] Algo bellman_ford step 3351 current loss 0.947804, current_train_items 107264.
I0302 19:00:03.994395 22447428132992 run.py:483] Algo bellman_ford step 3352 current loss 1.100735, current_train_items 107296.
I0302 19:00:04.026445 22447428132992 run.py:483] Algo bellman_ford step 3353 current loss 1.043979, current_train_items 107328.
I0302 19:00:04.060304 22447428132992 run.py:483] Algo bellman_ford step 3354 current loss 1.324360, current_train_items 107360.
I0302 19:00:04.080026 22447428132992 run.py:483] Algo bellman_ford step 3355 current loss 0.528832, current_train_items 107392.
I0302 19:00:04.095719 22447428132992 run.py:483] Algo bellman_ford step 3356 current loss 0.843938, current_train_items 107424.
I0302 19:00:04.118732 22447428132992 run.py:483] Algo bellman_ford step 3357 current loss 1.090338, current_train_items 107456.
I0302 19:00:04.149035 22447428132992 run.py:483] Algo bellman_ford step 3358 current loss 1.169402, current_train_items 107488.
I0302 19:00:04.183553 22447428132992 run.py:483] Algo bellman_ford step 3359 current loss 1.185835, current_train_items 107520.
I0302 19:00:04.203388 22447428132992 run.py:483] Algo bellman_ford step 3360 current loss 0.635088, current_train_items 107552.
I0302 19:00:04.219360 22447428132992 run.py:483] Algo bellman_ford step 3361 current loss 0.754013, current_train_items 107584.
I0302 19:00:04.242682 22447428132992 run.py:483] Algo bellman_ford step 3362 current loss 1.011873, current_train_items 107616.
I0302 19:00:04.272820 22447428132992 run.py:483] Algo bellman_ford step 3363 current loss 0.928014, current_train_items 107648.
I0302 19:00:04.305216 22447428132992 run.py:483] Algo bellman_ford step 3364 current loss 1.226346, current_train_items 107680.
I0302 19:00:04.324576 22447428132992 run.py:483] Algo bellman_ford step 3365 current loss 0.621874, current_train_items 107712.
I0302 19:00:04.340303 22447428132992 run.py:483] Algo bellman_ford step 3366 current loss 0.758788, current_train_items 107744.
I0302 19:00:04.364090 22447428132992 run.py:483] Algo bellman_ford step 3367 current loss 1.001762, current_train_items 107776.
I0302 19:00:04.393263 22447428132992 run.py:483] Algo bellman_ford step 3368 current loss 0.996327, current_train_items 107808.
I0302 19:00:04.425319 22447428132992 run.py:483] Algo bellman_ford step 3369 current loss 1.086294, current_train_items 107840.
I0302 19:00:04.445179 22447428132992 run.py:483] Algo bellman_ford step 3370 current loss 0.729160, current_train_items 107872.
I0302 19:00:04.461477 22447428132992 run.py:483] Algo bellman_ford step 3371 current loss 0.776995, current_train_items 107904.
I0302 19:00:04.484330 22447428132992 run.py:483] Algo bellman_ford step 3372 current loss 1.089338, current_train_items 107936.
I0302 19:00:04.516304 22447428132992 run.py:483] Algo bellman_ford step 3373 current loss 1.166695, current_train_items 107968.
I0302 19:00:04.548875 22447428132992 run.py:483] Algo bellman_ford step 3374 current loss 1.161744, current_train_items 108000.
I0302 19:00:04.568875 22447428132992 run.py:483] Algo bellman_ford step 3375 current loss 0.646586, current_train_items 108032.
I0302 19:00:04.584658 22447428132992 run.py:483] Algo bellman_ford step 3376 current loss 0.689221, current_train_items 108064.
I0302 19:00:04.606975 22447428132992 run.py:483] Algo bellman_ford step 3377 current loss 0.933231, current_train_items 108096.
I0302 19:00:04.637035 22447428132992 run.py:483] Algo bellman_ford step 3378 current loss 0.995626, current_train_items 108128.
I0302 19:00:04.671673 22447428132992 run.py:483] Algo bellman_ford step 3379 current loss 1.255824, current_train_items 108160.
I0302 19:00:04.691062 22447428132992 run.py:483] Algo bellman_ford step 3380 current loss 0.621385, current_train_items 108192.
I0302 19:00:04.707367 22447428132992 run.py:483] Algo bellman_ford step 3381 current loss 0.822174, current_train_items 108224.
I0302 19:00:04.731479 22447428132992 run.py:483] Algo bellman_ford step 3382 current loss 0.925989, current_train_items 108256.
I0302 19:00:04.761355 22447428132992 run.py:483] Algo bellman_ford step 3383 current loss 1.037640, current_train_items 108288.
I0302 19:00:04.796298 22447428132992 run.py:483] Algo bellman_ford step 3384 current loss 1.620593, current_train_items 108320.
I0302 19:00:04.816086 22447428132992 run.py:483] Algo bellman_ford step 3385 current loss 0.653028, current_train_items 108352.
I0302 19:00:04.832485 22447428132992 run.py:483] Algo bellman_ford step 3386 current loss 0.841119, current_train_items 108384.
I0302 19:00:04.855149 22447428132992 run.py:483] Algo bellman_ford step 3387 current loss 1.001348, current_train_items 108416.
I0302 19:00:04.885202 22447428132992 run.py:483] Algo bellman_ford step 3388 current loss 1.244928, current_train_items 108448.
I0302 19:00:04.916856 22447428132992 run.py:483] Algo bellman_ford step 3389 current loss 1.160960, current_train_items 108480.
I0302 19:00:04.936782 22447428132992 run.py:483] Algo bellman_ford step 3390 current loss 0.638434, current_train_items 108512.
I0302 19:00:04.952920 22447428132992 run.py:483] Algo bellman_ford step 3391 current loss 0.871348, current_train_items 108544.
I0302 19:00:04.975903 22447428132992 run.py:483] Algo bellman_ford step 3392 current loss 0.970568, current_train_items 108576.
I0302 19:00:05.005874 22447428132992 run.py:483] Algo bellman_ford step 3393 current loss 0.996244, current_train_items 108608.
I0302 19:00:05.037293 22447428132992 run.py:483] Algo bellman_ford step 3394 current loss 1.165358, current_train_items 108640.
I0302 19:00:05.056949 22447428132992 run.py:483] Algo bellman_ford step 3395 current loss 0.673735, current_train_items 108672.
I0302 19:00:05.073206 22447428132992 run.py:483] Algo bellman_ford step 3396 current loss 0.758055, current_train_items 108704.
I0302 19:00:05.096269 22447428132992 run.py:483] Algo bellman_ford step 3397 current loss 0.914533, current_train_items 108736.
I0302 19:00:05.127562 22447428132992 run.py:483] Algo bellman_ford step 3398 current loss 1.077382, current_train_items 108768.
I0302 19:00:05.159640 22447428132992 run.py:483] Algo bellman_ford step 3399 current loss 1.129369, current_train_items 108800.
I0302 19:00:05.179578 22447428132992 run.py:483] Algo bellman_ford step 3400 current loss 0.637646, current_train_items 108832.
I0302 19:00:05.187395 22447428132992 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 19:00:05.187502 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:00:05.204416 22447428132992 run.py:483] Algo bellman_ford step 3401 current loss 0.794537, current_train_items 108864.
I0302 19:00:05.227800 22447428132992 run.py:483] Algo bellman_ford step 3402 current loss 0.909979, current_train_items 108896.
I0302 19:00:05.258711 22447428132992 run.py:483] Algo bellman_ford step 3403 current loss 1.073134, current_train_items 108928.
I0302 19:00:05.293621 22447428132992 run.py:483] Algo bellman_ford step 3404 current loss 1.354426, current_train_items 108960.
I0302 19:00:05.313530 22447428132992 run.py:483] Algo bellman_ford step 3405 current loss 0.596096, current_train_items 108992.
I0302 19:00:05.328965 22447428132992 run.py:483] Algo bellman_ford step 3406 current loss 0.750795, current_train_items 109024.
I0302 19:00:05.352415 22447428132992 run.py:483] Algo bellman_ford step 3407 current loss 1.014965, current_train_items 109056.
I0302 19:00:05.384375 22447428132992 run.py:483] Algo bellman_ford step 3408 current loss 1.113055, current_train_items 109088.
I0302 19:00:05.416836 22447428132992 run.py:483] Algo bellman_ford step 3409 current loss 1.350134, current_train_items 109120.
I0302 19:00:05.435950 22447428132992 run.py:483] Algo bellman_ford step 3410 current loss 0.546186, current_train_items 109152.
I0302 19:00:05.452234 22447428132992 run.py:483] Algo bellman_ford step 3411 current loss 0.916349, current_train_items 109184.
I0302 19:00:05.476109 22447428132992 run.py:483] Algo bellman_ford step 3412 current loss 0.962108, current_train_items 109216.
I0302 19:00:05.507749 22447428132992 run.py:483] Algo bellman_ford step 3413 current loss 1.253986, current_train_items 109248.
I0302 19:00:05.543707 22447428132992 run.py:483] Algo bellman_ford step 3414 current loss 1.243979, current_train_items 109280.
I0302 19:00:05.563117 22447428132992 run.py:483] Algo bellman_ford step 3415 current loss 0.663952, current_train_items 109312.
I0302 19:00:05.579285 22447428132992 run.py:483] Algo bellman_ford step 3416 current loss 0.782327, current_train_items 109344.
I0302 19:00:05.602786 22447428132992 run.py:483] Algo bellman_ford step 3417 current loss 1.103037, current_train_items 109376.
I0302 19:00:05.633339 22447428132992 run.py:483] Algo bellman_ford step 3418 current loss 1.109649, current_train_items 109408.
I0302 19:00:05.665654 22447428132992 run.py:483] Algo bellman_ford step 3419 current loss 1.083827, current_train_items 109440.
I0302 19:00:05.685122 22447428132992 run.py:483] Algo bellman_ford step 3420 current loss 0.883047, current_train_items 109472.
I0302 19:00:05.700971 22447428132992 run.py:483] Algo bellman_ford step 3421 current loss 0.857683, current_train_items 109504.
I0302 19:00:05.724279 22447428132992 run.py:483] Algo bellman_ford step 3422 current loss 0.917512, current_train_items 109536.
I0302 19:00:05.755501 22447428132992 run.py:483] Algo bellman_ford step 3423 current loss 1.000993, current_train_items 109568.
I0302 19:00:05.788260 22447428132992 run.py:483] Algo bellman_ford step 3424 current loss 1.238251, current_train_items 109600.
I0302 19:00:05.807603 22447428132992 run.py:483] Algo bellman_ford step 3425 current loss 0.656092, current_train_items 109632.
I0302 19:00:05.824155 22447428132992 run.py:483] Algo bellman_ford step 3426 current loss 0.775328, current_train_items 109664.
I0302 19:00:05.847348 22447428132992 run.py:483] Algo bellman_ford step 3427 current loss 0.964619, current_train_items 109696.
I0302 19:00:05.878102 22447428132992 run.py:483] Algo bellman_ford step 3428 current loss 1.087996, current_train_items 109728.
I0302 19:00:05.908691 22447428132992 run.py:483] Algo bellman_ford step 3429 current loss 1.238782, current_train_items 109760.
I0302 19:00:05.928087 22447428132992 run.py:483] Algo bellman_ford step 3430 current loss 0.581335, current_train_items 109792.
I0302 19:00:05.944436 22447428132992 run.py:483] Algo bellman_ford step 3431 current loss 0.728778, current_train_items 109824.
I0302 19:00:05.967187 22447428132992 run.py:483] Algo bellman_ford step 3432 current loss 1.035938, current_train_items 109856.
I0302 19:00:05.999545 22447428132992 run.py:483] Algo bellman_ford step 3433 current loss 1.305533, current_train_items 109888.
I0302 19:00:06.031705 22447428132992 run.py:483] Algo bellman_ford step 3434 current loss 1.238617, current_train_items 109920.
I0302 19:00:06.051100 22447428132992 run.py:483] Algo bellman_ford step 3435 current loss 0.514913, current_train_items 109952.
I0302 19:00:06.067218 22447428132992 run.py:483] Algo bellman_ford step 3436 current loss 0.839748, current_train_items 109984.
I0302 19:00:06.090489 22447428132992 run.py:483] Algo bellman_ford step 3437 current loss 1.073365, current_train_items 110016.
I0302 19:00:06.120048 22447428132992 run.py:483] Algo bellman_ford step 3438 current loss 1.161925, current_train_items 110048.
I0302 19:00:06.151642 22447428132992 run.py:483] Algo bellman_ford step 3439 current loss 1.227751, current_train_items 110080.
I0302 19:00:06.171026 22447428132992 run.py:483] Algo bellman_ford step 3440 current loss 0.605059, current_train_items 110112.
I0302 19:00:06.187416 22447428132992 run.py:483] Algo bellman_ford step 3441 current loss 0.887240, current_train_items 110144.
I0302 19:00:06.209974 22447428132992 run.py:483] Algo bellman_ford step 3442 current loss 1.033734, current_train_items 110176.
I0302 19:00:06.240940 22447428132992 run.py:483] Algo bellman_ford step 3443 current loss 1.229037, current_train_items 110208.
I0302 19:00:06.275050 22447428132992 run.py:483] Algo bellman_ford step 3444 current loss 1.117238, current_train_items 110240.
I0302 19:00:06.294300 22447428132992 run.py:483] Algo bellman_ford step 3445 current loss 0.538972, current_train_items 110272.
I0302 19:00:06.310739 22447428132992 run.py:483] Algo bellman_ford step 3446 current loss 0.873946, current_train_items 110304.
I0302 19:00:06.334085 22447428132992 run.py:483] Algo bellman_ford step 3447 current loss 0.980916, current_train_items 110336.
I0302 19:00:06.364219 22447428132992 run.py:483] Algo bellman_ford step 3448 current loss 0.956989, current_train_items 110368.
I0302 19:00:06.397342 22447428132992 run.py:483] Algo bellman_ford step 3449 current loss 1.282197, current_train_items 110400.
I0302 19:00:06.416726 22447428132992 run.py:483] Algo bellman_ford step 3450 current loss 0.641848, current_train_items 110432.
I0302 19:00:06.424744 22447428132992 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 19:00:06.424853 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 19:00:06.441385 22447428132992 run.py:483] Algo bellman_ford step 3451 current loss 0.828919, current_train_items 110464.
I0302 19:00:06.465593 22447428132992 run.py:483] Algo bellman_ford step 3452 current loss 1.043068, current_train_items 110496.
I0302 19:00:06.495457 22447428132992 run.py:483] Algo bellman_ford step 3453 current loss 0.976542, current_train_items 110528.
I0302 19:00:06.531599 22447428132992 run.py:483] Algo bellman_ford step 3454 current loss 1.240986, current_train_items 110560.
I0302 19:00:06.551697 22447428132992 run.py:483] Algo bellman_ford step 3455 current loss 0.596952, current_train_items 110592.
I0302 19:00:06.567205 22447428132992 run.py:483] Algo bellman_ford step 3456 current loss 0.748985, current_train_items 110624.
I0302 19:00:06.590602 22447428132992 run.py:483] Algo bellman_ford step 3457 current loss 0.996097, current_train_items 110656.
I0302 19:00:06.622242 22447428132992 run.py:483] Algo bellman_ford step 3458 current loss 1.092485, current_train_items 110688.
I0302 19:00:06.654625 22447428132992 run.py:483] Algo bellman_ford step 3459 current loss 1.151765, current_train_items 110720.
I0302 19:00:06.674343 22447428132992 run.py:483] Algo bellman_ford step 3460 current loss 0.579134, current_train_items 110752.
I0302 19:00:06.690760 22447428132992 run.py:483] Algo bellman_ford step 3461 current loss 0.803192, current_train_items 110784.
I0302 19:00:06.713853 22447428132992 run.py:483] Algo bellman_ford step 3462 current loss 1.075619, current_train_items 110816.
I0302 19:00:06.744124 22447428132992 run.py:483] Algo bellman_ford step 3463 current loss 1.012473, current_train_items 110848.
I0302 19:00:06.776705 22447428132992 run.py:483] Algo bellman_ford step 3464 current loss 1.198777, current_train_items 110880.
I0302 19:00:06.796251 22447428132992 run.py:483] Algo bellman_ford step 3465 current loss 0.571332, current_train_items 110912.
I0302 19:00:06.812605 22447428132992 run.py:483] Algo bellman_ford step 3466 current loss 0.782795, current_train_items 110944.
I0302 19:00:06.836536 22447428132992 run.py:483] Algo bellman_ford step 3467 current loss 1.121004, current_train_items 110976.
I0302 19:00:06.868643 22447428132992 run.py:483] Algo bellman_ford step 3468 current loss 1.093854, current_train_items 111008.
I0302 19:00:06.904102 22447428132992 run.py:483] Algo bellman_ford step 3469 current loss 1.545750, current_train_items 111040.
I0302 19:00:06.923818 22447428132992 run.py:483] Algo bellman_ford step 3470 current loss 0.731869, current_train_items 111072.
I0302 19:00:06.940462 22447428132992 run.py:483] Algo bellman_ford step 3471 current loss 0.786421, current_train_items 111104.
I0302 19:00:06.963103 22447428132992 run.py:483] Algo bellman_ford step 3472 current loss 0.884043, current_train_items 111136.
I0302 19:00:06.994790 22447428132992 run.py:483] Algo bellman_ford step 3473 current loss 1.070302, current_train_items 111168.
I0302 19:00:07.028271 22447428132992 run.py:483] Algo bellman_ford step 3474 current loss 1.149182, current_train_items 111200.
I0302 19:00:07.048026 22447428132992 run.py:483] Algo bellman_ford step 3475 current loss 0.563878, current_train_items 111232.
I0302 19:00:07.064078 22447428132992 run.py:483] Algo bellman_ford step 3476 current loss 0.709448, current_train_items 111264.
I0302 19:00:07.086661 22447428132992 run.py:483] Algo bellman_ford step 3477 current loss 0.967000, current_train_items 111296.
I0302 19:00:07.116765 22447428132992 run.py:483] Algo bellman_ford step 3478 current loss 0.893999, current_train_items 111328.
I0302 19:00:07.151411 22447428132992 run.py:483] Algo bellman_ford step 3479 current loss 1.354154, current_train_items 111360.
I0302 19:00:07.171040 22447428132992 run.py:483] Algo bellman_ford step 3480 current loss 0.573710, current_train_items 111392.
I0302 19:00:07.187050 22447428132992 run.py:483] Algo bellman_ford step 3481 current loss 0.828154, current_train_items 111424.
I0302 19:00:07.210267 22447428132992 run.py:483] Algo bellman_ford step 3482 current loss 0.952873, current_train_items 111456.
I0302 19:00:07.240067 22447428132992 run.py:483] Algo bellman_ford step 3483 current loss 0.951962, current_train_items 111488.
I0302 19:00:07.272934 22447428132992 run.py:483] Algo bellman_ford step 3484 current loss 1.232874, current_train_items 111520.
I0302 19:00:07.292573 22447428132992 run.py:483] Algo bellman_ford step 3485 current loss 0.581652, current_train_items 111552.
I0302 19:00:07.308868 22447428132992 run.py:483] Algo bellman_ford step 3486 current loss 0.842190, current_train_items 111584.
I0302 19:00:07.332367 22447428132992 run.py:483] Algo bellman_ford step 3487 current loss 1.060477, current_train_items 111616.
I0302 19:00:07.362269 22447428132992 run.py:483] Algo bellman_ford step 3488 current loss 0.990893, current_train_items 111648.
I0302 19:00:07.394814 22447428132992 run.py:483] Algo bellman_ford step 3489 current loss 1.117891, current_train_items 111680.
I0302 19:00:07.414627 22447428132992 run.py:483] Algo bellman_ford step 3490 current loss 0.644327, current_train_items 111712.
I0302 19:00:07.430370 22447428132992 run.py:483] Algo bellman_ford step 3491 current loss 0.714785, current_train_items 111744.
I0302 19:00:07.453061 22447428132992 run.py:483] Algo bellman_ford step 3492 current loss 0.996177, current_train_items 111776.
I0302 19:00:07.484843 22447428132992 run.py:483] Algo bellman_ford step 3493 current loss 1.094077, current_train_items 111808.
I0302 19:00:07.517445 22447428132992 run.py:483] Algo bellman_ford step 3494 current loss 1.110331, current_train_items 111840.
I0302 19:00:07.536664 22447428132992 run.py:483] Algo bellman_ford step 3495 current loss 0.595214, current_train_items 111872.
I0302 19:00:07.552776 22447428132992 run.py:483] Algo bellman_ford step 3496 current loss 0.744415, current_train_items 111904.
I0302 19:00:07.575759 22447428132992 run.py:483] Algo bellman_ford step 3497 current loss 1.002259, current_train_items 111936.
I0302 19:00:07.606597 22447428132992 run.py:483] Algo bellman_ford step 3498 current loss 1.045195, current_train_items 111968.
I0302 19:00:07.640422 22447428132992 run.py:483] Algo bellman_ford step 3499 current loss 1.309431, current_train_items 112000.
I0302 19:00:07.660109 22447428132992 run.py:483] Algo bellman_ford step 3500 current loss 0.753096, current_train_items 112032.
I0302 19:00:07.667927 22447428132992 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.84375, 'score': 0.84375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 19:00:07.668034 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.844, val scores are: bellman_ford: 0.844
I0302 19:00:07.684086 22447428132992 run.py:483] Algo bellman_ford step 3501 current loss 0.821686, current_train_items 112064.
I0302 19:00:07.707410 22447428132992 run.py:483] Algo bellman_ford step 3502 current loss 0.964817, current_train_items 112096.
I0302 19:00:07.738886 22447428132992 run.py:483] Algo bellman_ford step 3503 current loss 1.297873, current_train_items 112128.
I0302 19:00:07.774538 22447428132992 run.py:483] Algo bellman_ford step 3504 current loss 1.302616, current_train_items 112160.
I0302 19:00:07.794441 22447428132992 run.py:483] Algo bellman_ford step 3505 current loss 0.768206, current_train_items 112192.
I0302 19:00:07.809563 22447428132992 run.py:483] Algo bellman_ford step 3506 current loss 0.725962, current_train_items 112224.
I0302 19:00:07.833229 22447428132992 run.py:483] Algo bellman_ford step 3507 current loss 1.068246, current_train_items 112256.
I0302 19:00:07.863304 22447428132992 run.py:483] Algo bellman_ford step 3508 current loss 1.074444, current_train_items 112288.
I0302 19:00:07.893860 22447428132992 run.py:483] Algo bellman_ford step 3509 current loss 1.186577, current_train_items 112320.
I0302 19:00:07.913083 22447428132992 run.py:483] Algo bellman_ford step 3510 current loss 0.584222, current_train_items 112352.
I0302 19:00:07.929006 22447428132992 run.py:483] Algo bellman_ford step 3511 current loss 0.830324, current_train_items 112384.
I0302 19:00:07.951517 22447428132992 run.py:483] Algo bellman_ford step 3512 current loss 0.937009, current_train_items 112416.
I0302 19:00:07.981026 22447428132992 run.py:483] Algo bellman_ford step 3513 current loss 1.170460, current_train_items 112448.
I0302 19:00:08.013021 22447428132992 run.py:483] Algo bellman_ford step 3514 current loss 0.992173, current_train_items 112480.
I0302 19:00:08.032316 22447428132992 run.py:483] Algo bellman_ford step 3515 current loss 0.591202, current_train_items 112512.
I0302 19:00:08.047947 22447428132992 run.py:483] Algo bellman_ford step 3516 current loss 0.735374, current_train_items 112544.
I0302 19:00:08.070991 22447428132992 run.py:483] Algo bellman_ford step 3517 current loss 0.988706, current_train_items 112576.
I0302 19:00:08.101778 22447428132992 run.py:483] Algo bellman_ford step 3518 current loss 1.059210, current_train_items 112608.
I0302 19:00:08.137168 22447428132992 run.py:483] Algo bellman_ford step 3519 current loss 1.273593, current_train_items 112640.
I0302 19:00:08.156842 22447428132992 run.py:483] Algo bellman_ford step 3520 current loss 0.662821, current_train_items 112672.
I0302 19:00:08.172723 22447428132992 run.py:483] Algo bellman_ford step 3521 current loss 0.740828, current_train_items 112704.
I0302 19:00:08.195335 22447428132992 run.py:483] Algo bellman_ford step 3522 current loss 0.819906, current_train_items 112736.
I0302 19:00:08.225712 22447428132992 run.py:483] Algo bellman_ford step 3523 current loss 1.154854, current_train_items 112768.
I0302 19:00:08.258489 22447428132992 run.py:483] Algo bellman_ford step 3524 current loss 1.198715, current_train_items 112800.
I0302 19:00:08.277863 22447428132992 run.py:483] Algo bellman_ford step 3525 current loss 0.627513, current_train_items 112832.
I0302 19:00:08.293659 22447428132992 run.py:483] Algo bellman_ford step 3526 current loss 0.750576, current_train_items 112864.
I0302 19:00:08.316877 22447428132992 run.py:483] Algo bellman_ford step 3527 current loss 0.987346, current_train_items 112896.
I0302 19:00:08.349005 22447428132992 run.py:483] Algo bellman_ford step 3528 current loss 1.054408, current_train_items 112928.
I0302 19:00:08.381089 22447428132992 run.py:483] Algo bellman_ford step 3529 current loss 1.133584, current_train_items 112960.
I0302 19:00:08.400390 22447428132992 run.py:483] Algo bellman_ford step 3530 current loss 0.689431, current_train_items 112992.
I0302 19:00:08.416670 22447428132992 run.py:483] Algo bellman_ford step 3531 current loss 0.828421, current_train_items 113024.
I0302 19:00:08.441124 22447428132992 run.py:483] Algo bellman_ford step 3532 current loss 1.023687, current_train_items 113056.
I0302 19:00:08.474580 22447428132992 run.py:483] Algo bellman_ford step 3533 current loss 1.173504, current_train_items 113088.
I0302 19:00:08.508086 22447428132992 run.py:483] Algo bellman_ford step 3534 current loss 1.272622, current_train_items 113120.
I0302 19:00:08.527546 22447428132992 run.py:483] Algo bellman_ford step 3535 current loss 0.609806, current_train_items 113152.
I0302 19:00:08.543698 22447428132992 run.py:483] Algo bellman_ford step 3536 current loss 0.867094, current_train_items 113184.
I0302 19:00:08.566611 22447428132992 run.py:483] Algo bellman_ford step 3537 current loss 0.998371, current_train_items 113216.
I0302 19:00:08.598186 22447428132992 run.py:483] Algo bellman_ford step 3538 current loss 1.148433, current_train_items 113248.
I0302 19:00:08.633206 22447428132992 run.py:483] Algo bellman_ford step 3539 current loss 1.132854, current_train_items 113280.
I0302 19:00:08.652681 22447428132992 run.py:483] Algo bellman_ford step 3540 current loss 0.720370, current_train_items 113312.
I0302 19:00:08.668668 22447428132992 run.py:483] Algo bellman_ford step 3541 current loss 0.807388, current_train_items 113344.
I0302 19:00:08.692748 22447428132992 run.py:483] Algo bellman_ford step 3542 current loss 1.005308, current_train_items 113376.
I0302 19:00:08.722982 22447428132992 run.py:483] Algo bellman_ford step 3543 current loss 1.066507, current_train_items 113408.
I0302 19:00:08.755506 22447428132992 run.py:483] Algo bellman_ford step 3544 current loss 1.160177, current_train_items 113440.
I0302 19:00:08.774865 22447428132992 run.py:483] Algo bellman_ford step 3545 current loss 0.558001, current_train_items 113472.
I0302 19:00:08.790670 22447428132992 run.py:483] Algo bellman_ford step 3546 current loss 0.777220, current_train_items 113504.
I0302 19:00:08.814108 22447428132992 run.py:483] Algo bellman_ford step 3547 current loss 1.044267, current_train_items 113536.
I0302 19:00:08.843810 22447428132992 run.py:483] Algo bellman_ford step 3548 current loss 0.986470, current_train_items 113568.
I0302 19:00:08.876737 22447428132992 run.py:483] Algo bellman_ford step 3549 current loss 1.231835, current_train_items 113600.
I0302 19:00:08.896106 22447428132992 run.py:483] Algo bellman_ford step 3550 current loss 0.636564, current_train_items 113632.
I0302 19:00:08.903977 22447428132992 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 19:00:08.904082 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:00:08.921406 22447428132992 run.py:483] Algo bellman_ford step 3551 current loss 0.872232, current_train_items 113664.
I0302 19:00:08.946616 22447428132992 run.py:483] Algo bellman_ford step 3552 current loss 1.003657, current_train_items 113696.
I0302 19:00:08.977366 22447428132992 run.py:483] Algo bellman_ford step 3553 current loss 0.960641, current_train_items 113728.
I0302 19:00:09.011226 22447428132992 run.py:483] Algo bellman_ford step 3554 current loss 1.123981, current_train_items 113760.
I0302 19:00:09.031648 22447428132992 run.py:483] Algo bellman_ford step 3555 current loss 0.589602, current_train_items 113792.
I0302 19:00:09.047511 22447428132992 run.py:483] Algo bellman_ford step 3556 current loss 0.817711, current_train_items 113824.
I0302 19:00:09.070974 22447428132992 run.py:483] Algo bellman_ford step 3557 current loss 1.045651, current_train_items 113856.
I0302 19:00:09.102236 22447428132992 run.py:483] Algo bellman_ford step 3558 current loss 1.119735, current_train_items 113888.
I0302 19:00:09.135225 22447428132992 run.py:483] Algo bellman_ford step 3559 current loss 1.095593, current_train_items 113920.
I0302 19:00:09.154968 22447428132992 run.py:483] Algo bellman_ford step 3560 current loss 0.651172, current_train_items 113952.
I0302 19:00:09.171128 22447428132992 run.py:483] Algo bellman_ford step 3561 current loss 0.746593, current_train_items 113984.
I0302 19:00:09.195545 22447428132992 run.py:483] Algo bellman_ford step 3562 current loss 1.073478, current_train_items 114016.
I0302 19:00:09.225286 22447428132992 run.py:483] Algo bellman_ford step 3563 current loss 1.049989, current_train_items 114048.
I0302 19:00:09.262114 22447428132992 run.py:483] Algo bellman_ford step 3564 current loss 1.339485, current_train_items 114080.
I0302 19:00:09.281838 22447428132992 run.py:483] Algo bellman_ford step 3565 current loss 0.639488, current_train_items 114112.
I0302 19:00:09.298060 22447428132992 run.py:483] Algo bellman_ford step 3566 current loss 0.794833, current_train_items 114144.
I0302 19:00:09.321609 22447428132992 run.py:483] Algo bellman_ford step 3567 current loss 0.822133, current_train_items 114176.
I0302 19:00:09.351946 22447428132992 run.py:483] Algo bellman_ford step 3568 current loss 1.036809, current_train_items 114208.
I0302 19:00:09.386150 22447428132992 run.py:483] Algo bellman_ford step 3569 current loss 1.176885, current_train_items 114240.
I0302 19:00:09.405873 22447428132992 run.py:483] Algo bellman_ford step 3570 current loss 0.598962, current_train_items 114272.
I0302 19:00:09.421691 22447428132992 run.py:483] Algo bellman_ford step 3571 current loss 0.718039, current_train_items 114304.
I0302 19:00:09.444583 22447428132992 run.py:483] Algo bellman_ford step 3572 current loss 1.062047, current_train_items 114336.
I0302 19:00:09.474369 22447428132992 run.py:483] Algo bellman_ford step 3573 current loss 0.923063, current_train_items 114368.
I0302 19:00:09.510070 22447428132992 run.py:483] Algo bellman_ford step 3574 current loss 1.262058, current_train_items 114400.
I0302 19:00:09.529677 22447428132992 run.py:483] Algo bellman_ford step 3575 current loss 0.648951, current_train_items 114432.
I0302 19:00:09.545866 22447428132992 run.py:483] Algo bellman_ford step 3576 current loss 0.767533, current_train_items 114464.
I0302 19:00:09.568776 22447428132992 run.py:483] Algo bellman_ford step 3577 current loss 0.921140, current_train_items 114496.
I0302 19:00:09.599363 22447428132992 run.py:483] Algo bellman_ford step 3578 current loss 1.008581, current_train_items 114528.
I0302 19:00:09.632191 22447428132992 run.py:483] Algo bellman_ford step 3579 current loss 1.164683, current_train_items 114560.
I0302 19:00:09.651811 22447428132992 run.py:483] Algo bellman_ford step 3580 current loss 0.542588, current_train_items 114592.
I0302 19:00:09.667955 22447428132992 run.py:483] Algo bellman_ford step 3581 current loss 0.787969, current_train_items 114624.
I0302 19:00:09.692356 22447428132992 run.py:483] Algo bellman_ford step 3582 current loss 1.031965, current_train_items 114656.
I0302 19:00:09.723452 22447428132992 run.py:483] Algo bellman_ford step 3583 current loss 1.070867, current_train_items 114688.
I0302 19:00:09.757389 22447428132992 run.py:483] Algo bellman_ford step 3584 current loss 1.192454, current_train_items 114720.
I0302 19:00:09.777142 22447428132992 run.py:483] Algo bellman_ford step 3585 current loss 0.553275, current_train_items 114752.
I0302 19:00:09.793303 22447428132992 run.py:483] Algo bellman_ford step 3586 current loss 0.870647, current_train_items 114784.
I0302 19:00:09.816028 22447428132992 run.py:483] Algo bellman_ford step 3587 current loss 1.013639, current_train_items 114816.
I0302 19:00:09.847747 22447428132992 run.py:483] Algo bellman_ford step 3588 current loss 1.099868, current_train_items 114848.
I0302 19:00:09.883033 22447428132992 run.py:483] Algo bellman_ford step 3589 current loss 1.259174, current_train_items 114880.
I0302 19:00:09.902871 22447428132992 run.py:483] Algo bellman_ford step 3590 current loss 0.566857, current_train_items 114912.
I0302 19:00:09.918790 22447428132992 run.py:483] Algo bellman_ford step 3591 current loss 0.739969, current_train_items 114944.
I0302 19:00:09.943365 22447428132992 run.py:483] Algo bellman_ford step 3592 current loss 1.046137, current_train_items 114976.
I0302 19:00:09.975920 22447428132992 run.py:483] Algo bellman_ford step 3593 current loss 1.118874, current_train_items 115008.
I0302 19:00:10.010108 22447428132992 run.py:483] Algo bellman_ford step 3594 current loss 1.226224, current_train_items 115040.
I0302 19:00:10.029497 22447428132992 run.py:483] Algo bellman_ford step 3595 current loss 0.647905, current_train_items 115072.
I0302 19:00:10.045888 22447428132992 run.py:483] Algo bellman_ford step 3596 current loss 0.792700, current_train_items 115104.
I0302 19:00:10.068834 22447428132992 run.py:483] Algo bellman_ford step 3597 current loss 0.928099, current_train_items 115136.
I0302 19:00:10.098400 22447428132992 run.py:483] Algo bellman_ford step 3598 current loss 0.983556, current_train_items 115168.
I0302 19:00:10.132726 22447428132992 run.py:483] Algo bellman_ford step 3599 current loss 1.309326, current_train_items 115200.
I0302 19:00:10.152695 22447428132992 run.py:483] Algo bellman_ford step 3600 current loss 0.555771, current_train_items 115232.
I0302 19:00:10.160204 22447428132992 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 19:00:10.160310 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:10.177302 22447428132992 run.py:483] Algo bellman_ford step 3601 current loss 0.818162, current_train_items 115264.
I0302 19:00:10.201871 22447428132992 run.py:483] Algo bellman_ford step 3602 current loss 0.977018, current_train_items 115296.
I0302 19:00:10.232398 22447428132992 run.py:483] Algo bellman_ford step 3603 current loss 1.036937, current_train_items 115328.
I0302 19:00:10.268842 22447428132992 run.py:483] Algo bellman_ford step 3604 current loss 1.247149, current_train_items 115360.
I0302 19:00:10.288688 22447428132992 run.py:483] Algo bellman_ford step 3605 current loss 0.542957, current_train_items 115392.
I0302 19:00:10.305238 22447428132992 run.py:483] Algo bellman_ford step 3606 current loss 0.868577, current_train_items 115424.
I0302 19:00:10.327593 22447428132992 run.py:483] Algo bellman_ford step 3607 current loss 0.781599, current_train_items 115456.
I0302 19:00:10.357157 22447428132992 run.py:483] Algo bellman_ford step 3608 current loss 1.140457, current_train_items 115488.
I0302 19:00:10.389056 22447428132992 run.py:483] Algo bellman_ford step 3609 current loss 1.055757, current_train_items 115520.
I0302 19:00:10.409142 22447428132992 run.py:483] Algo bellman_ford step 3610 current loss 0.631727, current_train_items 115552.
I0302 19:00:10.425320 22447428132992 run.py:483] Algo bellman_ford step 3611 current loss 0.807218, current_train_items 115584.
I0302 19:00:10.449755 22447428132992 run.py:483] Algo bellman_ford step 3612 current loss 0.986142, current_train_items 115616.
I0302 19:00:10.479737 22447428132992 run.py:483] Algo bellman_ford step 3613 current loss 0.944394, current_train_items 115648.
I0302 19:00:10.511823 22447428132992 run.py:483] Algo bellman_ford step 3614 current loss 1.136510, current_train_items 115680.
I0302 19:00:10.531306 22447428132992 run.py:483] Algo bellman_ford step 3615 current loss 0.617243, current_train_items 115712.
I0302 19:00:10.547681 22447428132992 run.py:483] Algo bellman_ford step 3616 current loss 0.901723, current_train_items 115744.
I0302 19:00:10.570763 22447428132992 run.py:483] Algo bellman_ford step 3617 current loss 0.958055, current_train_items 115776.
I0302 19:00:10.602044 22447428132992 run.py:483] Algo bellman_ford step 3618 current loss 1.171243, current_train_items 115808.
I0302 19:00:10.632295 22447428132992 run.py:483] Algo bellman_ford step 3619 current loss 1.045632, current_train_items 115840.
I0302 19:00:10.651727 22447428132992 run.py:483] Algo bellman_ford step 3620 current loss 0.663041, current_train_items 115872.
I0302 19:00:10.667875 22447428132992 run.py:483] Algo bellman_ford step 3621 current loss 0.729983, current_train_items 115904.
I0302 19:00:10.691121 22447428132992 run.py:483] Algo bellman_ford step 3622 current loss 0.949035, current_train_items 115936.
I0302 19:00:10.722511 22447428132992 run.py:483] Algo bellman_ford step 3623 current loss 1.136858, current_train_items 115968.
I0302 19:00:10.755594 22447428132992 run.py:483] Algo bellman_ford step 3624 current loss 1.019326, current_train_items 116000.
I0302 19:00:10.775220 22447428132992 run.py:483] Algo bellman_ford step 3625 current loss 0.533667, current_train_items 116032.
I0302 19:00:10.791511 22447428132992 run.py:483] Algo bellman_ford step 3626 current loss 0.764046, current_train_items 116064.
I0302 19:00:10.814977 22447428132992 run.py:483] Algo bellman_ford step 3627 current loss 0.922037, current_train_items 116096.
I0302 19:00:10.846303 22447428132992 run.py:483] Algo bellman_ford step 3628 current loss 1.047179, current_train_items 116128.
I0302 19:00:10.878885 22447428132992 run.py:483] Algo bellman_ford step 3629 current loss 1.162190, current_train_items 116160.
I0302 19:00:10.898849 22447428132992 run.py:483] Algo bellman_ford step 3630 current loss 0.630714, current_train_items 116192.
I0302 19:00:10.914808 22447428132992 run.py:483] Algo bellman_ford step 3631 current loss 0.804746, current_train_items 116224.
I0302 19:00:10.938569 22447428132992 run.py:483] Algo bellman_ford step 3632 current loss 0.931545, current_train_items 116256.
I0302 19:00:10.969205 22447428132992 run.py:483] Algo bellman_ford step 3633 current loss 1.172523, current_train_items 116288.
I0302 19:00:11.002176 22447428132992 run.py:483] Algo bellman_ford step 3634 current loss 1.126179, current_train_items 116320.
I0302 19:00:11.021836 22447428132992 run.py:483] Algo bellman_ford step 3635 current loss 0.601092, current_train_items 116352.
I0302 19:00:11.037875 22447428132992 run.py:483] Algo bellman_ford step 3636 current loss 0.774707, current_train_items 116384.
I0302 19:00:11.060156 22447428132992 run.py:483] Algo bellman_ford step 3637 current loss 0.984528, current_train_items 116416.
I0302 19:00:11.091430 22447428132992 run.py:483] Algo bellman_ford step 3638 current loss 1.085501, current_train_items 116448.
I0302 19:00:11.124076 22447428132992 run.py:483] Algo bellman_ford step 3639 current loss 1.074168, current_train_items 116480.
I0302 19:00:11.143468 22447428132992 run.py:483] Algo bellman_ford step 3640 current loss 0.557535, current_train_items 116512.
I0302 19:00:11.159196 22447428132992 run.py:483] Algo bellman_ford step 3641 current loss 0.790769, current_train_items 116544.
I0302 19:00:11.182943 22447428132992 run.py:483] Algo bellman_ford step 3642 current loss 1.090060, current_train_items 116576.
I0302 19:00:11.216544 22447428132992 run.py:483] Algo bellman_ford step 3643 current loss 1.135425, current_train_items 116608.
I0302 19:00:11.250880 22447428132992 run.py:483] Algo bellman_ford step 3644 current loss 1.299658, current_train_items 116640.
I0302 19:00:11.270372 22447428132992 run.py:483] Algo bellman_ford step 3645 current loss 0.544289, current_train_items 116672.
I0302 19:00:11.286525 22447428132992 run.py:483] Algo bellman_ford step 3646 current loss 0.737405, current_train_items 116704.
I0302 19:00:11.311683 22447428132992 run.py:483] Algo bellman_ford step 3647 current loss 1.027529, current_train_items 116736.
I0302 19:00:11.341970 22447428132992 run.py:483] Algo bellman_ford step 3648 current loss 0.934918, current_train_items 116768.
I0302 19:00:11.375922 22447428132992 run.py:483] Algo bellman_ford step 3649 current loss 1.280208, current_train_items 116800.
I0302 19:00:11.395416 22447428132992 run.py:483] Algo bellman_ford step 3650 current loss 0.608148, current_train_items 116832.
I0302 19:00:11.403622 22447428132992 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 19:00:11.403728 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 19:00:11.420561 22447428132992 run.py:483] Algo bellman_ford step 3651 current loss 0.746542, current_train_items 116864.
I0302 19:00:11.443124 22447428132992 run.py:483] Algo bellman_ford step 3652 current loss 0.920263, current_train_items 116896.
I0302 19:00:11.473391 22447428132992 run.py:483] Algo bellman_ford step 3653 current loss 1.027511, current_train_items 116928.
I0302 19:00:11.506453 22447428132992 run.py:483] Algo bellman_ford step 3654 current loss 1.274433, current_train_items 116960.
I0302 19:00:11.525917 22447428132992 run.py:483] Algo bellman_ford step 3655 current loss 0.521748, current_train_items 116992.
I0302 19:00:11.541267 22447428132992 run.py:483] Algo bellman_ford step 3656 current loss 0.772637, current_train_items 117024.
I0302 19:00:11.564726 22447428132992 run.py:483] Algo bellman_ford step 3657 current loss 0.970892, current_train_items 117056.
I0302 19:00:11.594981 22447428132992 run.py:483] Algo bellman_ford step 3658 current loss 1.018969, current_train_items 117088.
I0302 19:00:11.628333 22447428132992 run.py:483] Algo bellman_ford step 3659 current loss 1.232345, current_train_items 117120.
I0302 19:00:11.648185 22447428132992 run.py:483] Algo bellman_ford step 3660 current loss 0.577516, current_train_items 117152.
I0302 19:00:11.664576 22447428132992 run.py:483] Algo bellman_ford step 3661 current loss 0.865097, current_train_items 117184.
I0302 19:00:11.687123 22447428132992 run.py:483] Algo bellman_ford step 3662 current loss 0.888408, current_train_items 117216.
I0302 19:00:11.718359 22447428132992 run.py:483] Algo bellman_ford step 3663 current loss 1.095447, current_train_items 117248.
I0302 19:00:11.752732 22447428132992 run.py:483] Algo bellman_ford step 3664 current loss 1.218012, current_train_items 117280.
I0302 19:00:11.772009 22447428132992 run.py:483] Algo bellman_ford step 3665 current loss 0.649846, current_train_items 117312.
I0302 19:00:11.787784 22447428132992 run.py:483] Algo bellman_ford step 3666 current loss 0.740105, current_train_items 117344.
I0302 19:00:11.811206 22447428132992 run.py:483] Algo bellman_ford step 3667 current loss 0.897279, current_train_items 117376.
I0302 19:00:11.842157 22447428132992 run.py:483] Algo bellman_ford step 3668 current loss 1.049768, current_train_items 117408.
I0302 19:00:11.876775 22447428132992 run.py:483] Algo bellman_ford step 3669 current loss 1.191739, current_train_items 117440.
I0302 19:00:11.896757 22447428132992 run.py:483] Algo bellman_ford step 3670 current loss 0.652023, current_train_items 117472.
I0302 19:00:11.913046 22447428132992 run.py:483] Algo bellman_ford step 3671 current loss 0.838685, current_train_items 117504.
I0302 19:00:11.935927 22447428132992 run.py:483] Algo bellman_ford step 3672 current loss 0.919333, current_train_items 117536.
I0302 19:00:11.966413 22447428132992 run.py:483] Algo bellman_ford step 3673 current loss 0.969662, current_train_items 117568.
I0302 19:00:12.000532 22447428132992 run.py:483] Algo bellman_ford step 3674 current loss 1.495108, current_train_items 117600.
I0302 19:00:12.020249 22447428132992 run.py:483] Algo bellman_ford step 3675 current loss 0.514415, current_train_items 117632.
I0302 19:00:12.036041 22447428132992 run.py:483] Algo bellman_ford step 3676 current loss 0.759082, current_train_items 117664.
I0302 19:00:12.058693 22447428132992 run.py:483] Algo bellman_ford step 3677 current loss 0.900083, current_train_items 117696.
I0302 19:00:12.089514 22447428132992 run.py:483] Algo bellman_ford step 3678 current loss 1.086598, current_train_items 117728.
I0302 19:00:12.123132 22447428132992 run.py:483] Algo bellman_ford step 3679 current loss 1.391959, current_train_items 117760.
I0302 19:00:12.142236 22447428132992 run.py:483] Algo bellman_ford step 3680 current loss 0.560281, current_train_items 117792.
I0302 19:00:12.158572 22447428132992 run.py:483] Algo bellman_ford step 3681 current loss 0.791924, current_train_items 117824.
I0302 19:00:12.182075 22447428132992 run.py:483] Algo bellman_ford step 3682 current loss 1.043760, current_train_items 117856.
I0302 19:00:12.211905 22447428132992 run.py:483] Algo bellman_ford step 3683 current loss 1.030884, current_train_items 117888.
I0302 19:00:12.245430 22447428132992 run.py:483] Algo bellman_ford step 3684 current loss 1.541943, current_train_items 117920.
I0302 19:00:12.265110 22447428132992 run.py:483] Algo bellman_ford step 3685 current loss 0.575249, current_train_items 117952.
I0302 19:00:12.281165 22447428132992 run.py:483] Algo bellman_ford step 3686 current loss 0.812611, current_train_items 117984.
I0302 19:00:12.303806 22447428132992 run.py:483] Algo bellman_ford step 3687 current loss 0.904622, current_train_items 118016.
I0302 19:00:12.336034 22447428132992 run.py:483] Algo bellman_ford step 3688 current loss 1.138517, current_train_items 118048.
I0302 19:00:12.370938 22447428132992 run.py:483] Algo bellman_ford step 3689 current loss 1.266232, current_train_items 118080.
I0302 19:00:12.390581 22447428132992 run.py:483] Algo bellman_ford step 3690 current loss 0.600004, current_train_items 118112.
I0302 19:00:12.406682 22447428132992 run.py:483] Algo bellman_ford step 3691 current loss 0.930024, current_train_items 118144.
I0302 19:00:12.428102 22447428132992 run.py:483] Algo bellman_ford step 3692 current loss 0.932799, current_train_items 118176.
I0302 19:00:12.460001 22447428132992 run.py:483] Algo bellman_ford step 3693 current loss 1.094414, current_train_items 118208.
I0302 19:00:12.492476 22447428132992 run.py:483] Algo bellman_ford step 3694 current loss 1.113787, current_train_items 118240.
I0302 19:00:12.512133 22447428132992 run.py:483] Algo bellman_ford step 3695 current loss 0.654562, current_train_items 118272.
I0302 19:00:12.528870 22447428132992 run.py:483] Algo bellman_ford step 3696 current loss 0.839789, current_train_items 118304.
I0302 19:00:12.552288 22447428132992 run.py:483] Algo bellman_ford step 3697 current loss 1.010758, current_train_items 118336.
I0302 19:00:12.583919 22447428132992 run.py:483] Algo bellman_ford step 3698 current loss 1.097496, current_train_items 118368.
I0302 19:00:12.615703 22447428132992 run.py:483] Algo bellman_ford step 3699 current loss 1.262835, current_train_items 118400.
I0302 19:00:12.635643 22447428132992 run.py:483] Algo bellman_ford step 3700 current loss 0.925856, current_train_items 118432.
I0302 19:00:12.643523 22447428132992 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 19:00:12.643632 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 19:00:12.659904 22447428132992 run.py:483] Algo bellman_ford step 3701 current loss 0.799013, current_train_items 118464.
I0302 19:00:12.683723 22447428132992 run.py:483] Algo bellman_ford step 3702 current loss 1.029519, current_train_items 118496.
I0302 19:00:12.715976 22447428132992 run.py:483] Algo bellman_ford step 3703 current loss 1.120369, current_train_items 118528.
I0302 19:00:12.749421 22447428132992 run.py:483] Algo bellman_ford step 3704 current loss 1.091199, current_train_items 118560.
I0302 19:00:12.769042 22447428132992 run.py:483] Algo bellman_ford step 3705 current loss 0.603997, current_train_items 118592.
I0302 19:00:12.785094 22447428132992 run.py:483] Algo bellman_ford step 3706 current loss 0.874832, current_train_items 118624.
I0302 19:00:12.808258 22447428132992 run.py:483] Algo bellman_ford step 3707 current loss 1.052167, current_train_items 118656.
I0302 19:00:12.837757 22447428132992 run.py:483] Algo bellman_ford step 3708 current loss 1.026663, current_train_items 118688.
I0302 19:00:12.871007 22447428132992 run.py:483] Algo bellman_ford step 3709 current loss 1.246314, current_train_items 118720.
I0302 19:00:12.890540 22447428132992 run.py:483] Algo bellman_ford step 3710 current loss 0.642356, current_train_items 118752.
I0302 19:00:12.906759 22447428132992 run.py:483] Algo bellman_ford step 3711 current loss 0.933318, current_train_items 118784.
I0302 19:00:12.929961 22447428132992 run.py:483] Algo bellman_ford step 3712 current loss 0.993527, current_train_items 118816.
I0302 19:00:12.961764 22447428132992 run.py:483] Algo bellman_ford step 3713 current loss 1.164425, current_train_items 118848.
I0302 19:00:12.995659 22447428132992 run.py:483] Algo bellman_ford step 3714 current loss 1.297479, current_train_items 118880.
I0302 19:00:13.014710 22447428132992 run.py:483] Algo bellman_ford step 3715 current loss 0.555878, current_train_items 118912.
I0302 19:00:13.030625 22447428132992 run.py:483] Algo bellman_ford step 3716 current loss 0.820147, current_train_items 118944.
I0302 19:00:13.053552 22447428132992 run.py:483] Algo bellman_ford step 3717 current loss 1.012201, current_train_items 118976.
I0302 19:00:13.084223 22447428132992 run.py:483] Algo bellman_ford step 3718 current loss 1.054370, current_train_items 119008.
I0302 19:00:13.116352 22447428132992 run.py:483] Algo bellman_ford step 3719 current loss 1.149367, current_train_items 119040.
I0302 19:00:13.135810 22447428132992 run.py:483] Algo bellman_ford step 3720 current loss 0.629488, current_train_items 119072.
I0302 19:00:13.152054 22447428132992 run.py:483] Algo bellman_ford step 3721 current loss 0.853387, current_train_items 119104.
I0302 19:00:13.175103 22447428132992 run.py:483] Algo bellman_ford step 3722 current loss 0.951738, current_train_items 119136.
I0302 19:00:13.205063 22447428132992 run.py:483] Algo bellman_ford step 3723 current loss 1.128976, current_train_items 119168.
I0302 19:00:13.237709 22447428132992 run.py:483] Algo bellman_ford step 3724 current loss 1.049936, current_train_items 119200.
I0302 19:00:13.256910 22447428132992 run.py:483] Algo bellman_ford step 3725 current loss 0.607626, current_train_items 119232.
I0302 19:00:13.273265 22447428132992 run.py:483] Algo bellman_ford step 3726 current loss 0.760185, current_train_items 119264.
I0302 19:00:13.297224 22447428132992 run.py:483] Algo bellman_ford step 3727 current loss 0.896803, current_train_items 119296.
I0302 19:00:13.327966 22447428132992 run.py:483] Algo bellman_ford step 3728 current loss 1.192702, current_train_items 119328.
I0302 19:00:13.361010 22447428132992 run.py:483] Algo bellman_ford step 3729 current loss 1.212397, current_train_items 119360.
I0302 19:00:13.380275 22447428132992 run.py:483] Algo bellman_ford step 3730 current loss 0.599083, current_train_items 119392.
I0302 19:00:13.396134 22447428132992 run.py:483] Algo bellman_ford step 3731 current loss 0.735451, current_train_items 119424.
I0302 19:00:13.419393 22447428132992 run.py:483] Algo bellman_ford step 3732 current loss 0.899015, current_train_items 119456.
I0302 19:00:13.448024 22447428132992 run.py:483] Algo bellman_ford step 3733 current loss 1.073036, current_train_items 119488.
I0302 19:00:13.481657 22447428132992 run.py:483] Algo bellman_ford step 3734 current loss 1.309753, current_train_items 119520.
I0302 19:00:13.500835 22447428132992 run.py:483] Algo bellman_ford step 3735 current loss 0.607241, current_train_items 119552.
I0302 19:00:13.516671 22447428132992 run.py:483] Algo bellman_ford step 3736 current loss 0.767676, current_train_items 119584.
I0302 19:00:13.539156 22447428132992 run.py:483] Algo bellman_ford step 3737 current loss 0.882580, current_train_items 119616.
I0302 19:00:13.569906 22447428132992 run.py:483] Algo bellman_ford step 3738 current loss 1.129933, current_train_items 119648.
I0302 19:00:13.602747 22447428132992 run.py:483] Algo bellman_ford step 3739 current loss 1.276939, current_train_items 119680.
I0302 19:00:13.622164 22447428132992 run.py:483] Algo bellman_ford step 3740 current loss 0.639393, current_train_items 119712.
I0302 19:00:13.638056 22447428132992 run.py:483] Algo bellman_ford step 3741 current loss 0.730448, current_train_items 119744.
I0302 19:00:13.660793 22447428132992 run.py:483] Algo bellman_ford step 3742 current loss 0.948440, current_train_items 119776.
I0302 19:00:13.690908 22447428132992 run.py:483] Algo bellman_ford step 3743 current loss 1.030473, current_train_items 119808.
I0302 19:00:13.721864 22447428132992 run.py:483] Algo bellman_ford step 3744 current loss 1.051682, current_train_items 119840.
I0302 19:00:13.741066 22447428132992 run.py:483] Algo bellman_ford step 3745 current loss 0.606970, current_train_items 119872.
I0302 19:00:13.756922 22447428132992 run.py:483] Algo bellman_ford step 3746 current loss 0.686423, current_train_items 119904.
I0302 19:00:13.780336 22447428132992 run.py:483] Algo bellman_ford step 3747 current loss 0.931498, current_train_items 119936.
I0302 19:00:13.811425 22447428132992 run.py:483] Algo bellman_ford step 3748 current loss 1.002007, current_train_items 119968.
I0302 19:00:13.846060 22447428132992 run.py:483] Algo bellman_ford step 3749 current loss 1.420360, current_train_items 120000.
I0302 19:00:13.865451 22447428132992 run.py:483] Algo bellman_ford step 3750 current loss 0.613929, current_train_items 120032.
I0302 19:00:13.873448 22447428132992 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 19:00:13.873591 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:00:13.890372 22447428132992 run.py:483] Algo bellman_ford step 3751 current loss 0.806642, current_train_items 120064.
I0302 19:00:13.914298 22447428132992 run.py:483] Algo bellman_ford step 3752 current loss 0.926152, current_train_items 120096.
I0302 19:00:13.945399 22447428132992 run.py:483] Algo bellman_ford step 3753 current loss 0.994549, current_train_items 120128.
I0302 19:00:13.981198 22447428132992 run.py:483] Algo bellman_ford step 3754 current loss 1.148373, current_train_items 120160.
I0302 19:00:14.001224 22447428132992 run.py:483] Algo bellman_ford step 3755 current loss 0.634697, current_train_items 120192.
I0302 19:00:14.017035 22447428132992 run.py:483] Algo bellman_ford step 3756 current loss 0.810153, current_train_items 120224.
I0302 19:00:14.040706 22447428132992 run.py:483] Algo bellman_ford step 3757 current loss 0.979192, current_train_items 120256.
I0302 19:00:14.071240 22447428132992 run.py:483] Algo bellman_ford step 3758 current loss 1.032377, current_train_items 120288.
I0302 19:00:14.103212 22447428132992 run.py:483] Algo bellman_ford step 3759 current loss 1.016440, current_train_items 120320.
I0302 19:00:14.123193 22447428132992 run.py:483] Algo bellman_ford step 3760 current loss 0.760957, current_train_items 120352.
I0302 19:00:14.139436 22447428132992 run.py:483] Algo bellman_ford step 3761 current loss 0.904121, current_train_items 120384.
I0302 19:00:14.162005 22447428132992 run.py:483] Algo bellman_ford step 3762 current loss 0.875783, current_train_items 120416.
I0302 19:00:14.192126 22447428132992 run.py:483] Algo bellman_ford step 3763 current loss 1.040317, current_train_items 120448.
I0302 19:00:14.225862 22447428132992 run.py:483] Algo bellman_ford step 3764 current loss 1.205137, current_train_items 120480.
I0302 19:00:14.245283 22447428132992 run.py:483] Algo bellman_ford step 3765 current loss 0.566551, current_train_items 120512.
I0302 19:00:14.261766 22447428132992 run.py:483] Algo bellman_ford step 3766 current loss 0.738142, current_train_items 120544.
I0302 19:00:14.285532 22447428132992 run.py:483] Algo bellman_ford step 3767 current loss 1.093899, current_train_items 120576.
I0302 19:00:14.317296 22447428132992 run.py:483] Algo bellman_ford step 3768 current loss 1.150873, current_train_items 120608.
I0302 19:00:14.349819 22447428132992 run.py:483] Algo bellman_ford step 3769 current loss 1.243411, current_train_items 120640.
I0302 19:00:14.369867 22447428132992 run.py:483] Algo bellman_ford step 3770 current loss 0.557542, current_train_items 120672.
I0302 19:00:14.386143 22447428132992 run.py:483] Algo bellman_ford step 3771 current loss 0.828649, current_train_items 120704.
I0302 19:00:14.408592 22447428132992 run.py:483] Algo bellman_ford step 3772 current loss 0.964047, current_train_items 120736.
I0302 19:00:14.438787 22447428132992 run.py:483] Algo bellman_ford step 3773 current loss 1.011153, current_train_items 120768.
I0302 19:00:14.469457 22447428132992 run.py:483] Algo bellman_ford step 3774 current loss 0.966187, current_train_items 120800.
I0302 19:00:14.489192 22447428132992 run.py:483] Algo bellman_ford step 3775 current loss 0.661884, current_train_items 120832.
I0302 19:00:14.505738 22447428132992 run.py:483] Algo bellman_ford step 3776 current loss 0.850284, current_train_items 120864.
I0302 19:00:14.528104 22447428132992 run.py:483] Algo bellman_ford step 3777 current loss 0.897471, current_train_items 120896.
I0302 19:00:14.559684 22447428132992 run.py:483] Algo bellman_ford step 3778 current loss 1.086842, current_train_items 120928.
I0302 19:00:14.593050 22447428132992 run.py:483] Algo bellman_ford step 3779 current loss 1.683154, current_train_items 120960.
I0302 19:00:14.612756 22447428132992 run.py:483] Algo bellman_ford step 3780 current loss 0.600536, current_train_items 120992.
I0302 19:00:14.629098 22447428132992 run.py:483] Algo bellman_ford step 3781 current loss 0.891369, current_train_items 121024.
I0302 19:00:14.652396 22447428132992 run.py:483] Algo bellman_ford step 3782 current loss 0.951420, current_train_items 121056.
I0302 19:00:14.681548 22447428132992 run.py:483] Algo bellman_ford step 3783 current loss 1.073444, current_train_items 121088.
I0302 19:00:14.717623 22447428132992 run.py:483] Algo bellman_ford step 3784 current loss 1.349507, current_train_items 121120.
I0302 19:00:14.737264 22447428132992 run.py:483] Algo bellman_ford step 3785 current loss 0.584786, current_train_items 121152.
I0302 19:00:14.753286 22447428132992 run.py:483] Algo bellman_ford step 3786 current loss 0.779500, current_train_items 121184.
I0302 19:00:14.776616 22447428132992 run.py:483] Algo bellman_ford step 3787 current loss 0.991828, current_train_items 121216.
I0302 19:00:14.807322 22447428132992 run.py:483] Algo bellman_ford step 3788 current loss 1.007855, current_train_items 121248.
I0302 19:00:14.841002 22447428132992 run.py:483] Algo bellman_ford step 3789 current loss 1.104566, current_train_items 121280.
I0302 19:00:14.860790 22447428132992 run.py:483] Algo bellman_ford step 3790 current loss 0.595017, current_train_items 121312.
I0302 19:00:14.876973 22447428132992 run.py:483] Algo bellman_ford step 3791 current loss 0.729017, current_train_items 121344.
I0302 19:00:14.899696 22447428132992 run.py:483] Algo bellman_ford step 3792 current loss 0.875746, current_train_items 121376.
I0302 19:00:14.931492 22447428132992 run.py:483] Algo bellman_ford step 3793 current loss 1.152974, current_train_items 121408.
I0302 19:00:14.964582 22447428132992 run.py:483] Algo bellman_ford step 3794 current loss 1.309365, current_train_items 121440.
I0302 19:00:14.984376 22447428132992 run.py:483] Algo bellman_ford step 3795 current loss 0.577444, current_train_items 121472.
I0302 19:00:15.000800 22447428132992 run.py:483] Algo bellman_ford step 3796 current loss 0.804384, current_train_items 121504.
I0302 19:00:15.024653 22447428132992 run.py:483] Algo bellman_ford step 3797 current loss 0.914670, current_train_items 121536.
I0302 19:00:15.055921 22447428132992 run.py:483] Algo bellman_ford step 3798 current loss 1.110030, current_train_items 121568.
I0302 19:00:15.088559 22447428132992 run.py:483] Algo bellman_ford step 3799 current loss 1.134765, current_train_items 121600.
I0302 19:00:15.108375 22447428132992 run.py:483] Algo bellman_ford step 3800 current loss 0.678345, current_train_items 121632.
I0302 19:00:15.116239 22447428132992 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 19:00:15.116345 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:15.132948 22447428132992 run.py:483] Algo bellman_ford step 3801 current loss 0.765751, current_train_items 121664.
I0302 19:00:15.157492 22447428132992 run.py:483] Algo bellman_ford step 3802 current loss 0.955917, current_train_items 121696.
I0302 19:00:15.188558 22447428132992 run.py:483] Algo bellman_ford step 3803 current loss 1.107613, current_train_items 121728.
I0302 19:00:15.220731 22447428132992 run.py:483] Algo bellman_ford step 3804 current loss 1.039485, current_train_items 121760.
I0302 19:00:15.240975 22447428132992 run.py:483] Algo bellman_ford step 3805 current loss 0.593761, current_train_items 121792.
I0302 19:00:15.257022 22447428132992 run.py:483] Algo bellman_ford step 3806 current loss 0.811711, current_train_items 121824.
I0302 19:00:15.280661 22447428132992 run.py:483] Algo bellman_ford step 3807 current loss 0.873865, current_train_items 121856.
I0302 19:00:15.311439 22447428132992 run.py:483] Algo bellman_ford step 3808 current loss 1.031920, current_train_items 121888.
I0302 19:00:15.346275 22447428132992 run.py:483] Algo bellman_ford step 3809 current loss 1.206281, current_train_items 121920.
I0302 19:00:15.365986 22447428132992 run.py:483] Algo bellman_ford step 3810 current loss 0.635015, current_train_items 121952.
I0302 19:00:15.382364 22447428132992 run.py:483] Algo bellman_ford step 3811 current loss 0.795153, current_train_items 121984.
I0302 19:00:15.406185 22447428132992 run.py:483] Algo bellman_ford step 3812 current loss 0.854676, current_train_items 122016.
I0302 19:00:15.438694 22447428132992 run.py:483] Algo bellman_ford step 3813 current loss 1.242032, current_train_items 122048.
I0302 19:00:15.473917 22447428132992 run.py:483] Algo bellman_ford step 3814 current loss 1.355516, current_train_items 122080.
I0302 19:00:15.493787 22447428132992 run.py:483] Algo bellman_ford step 3815 current loss 0.827533, current_train_items 122112.
I0302 19:00:15.510305 22447428132992 run.py:483] Algo bellman_ford step 3816 current loss 0.866760, current_train_items 122144.
I0302 19:00:15.533695 22447428132992 run.py:483] Algo bellman_ford step 3817 current loss 0.939406, current_train_items 122176.
I0302 19:00:15.564789 22447428132992 run.py:483] Algo bellman_ford step 3818 current loss 1.006965, current_train_items 122208.
I0302 19:00:15.598515 22447428132992 run.py:483] Algo bellman_ford step 3819 current loss 1.121813, current_train_items 122240.
I0302 19:00:15.617889 22447428132992 run.py:483] Algo bellman_ford step 3820 current loss 0.718390, current_train_items 122272.
I0302 19:00:15.633675 22447428132992 run.py:483] Algo bellman_ford step 3821 current loss 0.704392, current_train_items 122304.
I0302 19:00:15.657314 22447428132992 run.py:483] Algo bellman_ford step 3822 current loss 0.992520, current_train_items 122336.
I0302 19:00:15.688239 22447428132992 run.py:483] Algo bellman_ford step 3823 current loss 1.129574, current_train_items 122368.
I0302 19:00:15.722275 22447428132992 run.py:483] Algo bellman_ford step 3824 current loss 1.352347, current_train_items 122400.
I0302 19:00:15.742012 22447428132992 run.py:483] Algo bellman_ford step 3825 current loss 0.601749, current_train_items 122432.
I0302 19:00:15.757894 22447428132992 run.py:483] Algo bellman_ford step 3826 current loss 0.729111, current_train_items 122464.
I0302 19:00:15.781571 22447428132992 run.py:483] Algo bellman_ford step 3827 current loss 0.934439, current_train_items 122496.
I0302 19:00:15.812810 22447428132992 run.py:483] Algo bellman_ford step 3828 current loss 1.092149, current_train_items 122528.
I0302 19:00:15.844820 22447428132992 run.py:483] Algo bellman_ford step 3829 current loss 1.176417, current_train_items 122560.
I0302 19:00:15.864396 22447428132992 run.py:483] Algo bellman_ford step 3830 current loss 0.565952, current_train_items 122592.
I0302 19:00:15.880572 22447428132992 run.py:483] Algo bellman_ford step 3831 current loss 0.830774, current_train_items 122624.
I0302 19:00:15.903059 22447428132992 run.py:483] Algo bellman_ford step 3832 current loss 0.975441, current_train_items 122656.
I0302 19:00:15.934668 22447428132992 run.py:483] Algo bellman_ford step 3833 current loss 1.123729, current_train_items 122688.
I0302 19:00:15.967393 22447428132992 run.py:483] Algo bellman_ford step 3834 current loss 1.098936, current_train_items 122720.
I0302 19:00:15.987077 22447428132992 run.py:483] Algo bellman_ford step 3835 current loss 0.639676, current_train_items 122752.
I0302 19:00:16.002654 22447428132992 run.py:483] Algo bellman_ford step 3836 current loss 0.669583, current_train_items 122784.
I0302 19:00:16.027175 22447428132992 run.py:483] Algo bellman_ford step 3837 current loss 0.966303, current_train_items 122816.
I0302 19:00:16.058089 22447428132992 run.py:483] Algo bellman_ford step 3838 current loss 1.102353, current_train_items 122848.
I0302 19:00:16.091318 22447428132992 run.py:483] Algo bellman_ford step 3839 current loss 1.140351, current_train_items 122880.
I0302 19:00:16.110781 22447428132992 run.py:483] Algo bellman_ford step 3840 current loss 0.630293, current_train_items 122912.
I0302 19:00:16.126535 22447428132992 run.py:483] Algo bellman_ford step 3841 current loss 0.813599, current_train_items 122944.
I0302 19:00:16.148972 22447428132992 run.py:483] Algo bellman_ford step 3842 current loss 0.944053, current_train_items 122976.
I0302 19:00:16.179985 22447428132992 run.py:483] Algo bellman_ford step 3843 current loss 1.109939, current_train_items 123008.
I0302 19:00:16.214749 22447428132992 run.py:483] Algo bellman_ford step 3844 current loss 1.256283, current_train_items 123040.
I0302 19:00:16.234166 22447428132992 run.py:483] Algo bellman_ford step 3845 current loss 0.605640, current_train_items 123072.
I0302 19:00:16.250171 22447428132992 run.py:483] Algo bellman_ford step 3846 current loss 0.749692, current_train_items 123104.
I0302 19:00:16.275068 22447428132992 run.py:483] Algo bellman_ford step 3847 current loss 1.061183, current_train_items 123136.
I0302 19:00:16.305553 22447428132992 run.py:483] Algo bellman_ford step 3848 current loss 1.021288, current_train_items 123168.
I0302 19:00:16.338999 22447428132992 run.py:483] Algo bellman_ford step 3849 current loss 1.159414, current_train_items 123200.
I0302 19:00:16.358577 22447428132992 run.py:483] Algo bellman_ford step 3850 current loss 0.557179, current_train_items 123232.
I0302 19:00:16.366821 22447428132992 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 19:00:16.366925 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:00:16.383140 22447428132992 run.py:483] Algo bellman_ford step 3851 current loss 0.655936, current_train_items 123264.
I0302 19:00:16.405969 22447428132992 run.py:483] Algo bellman_ford step 3852 current loss 1.007053, current_train_items 123296.
I0302 19:00:16.437811 22447428132992 run.py:483] Algo bellman_ford step 3853 current loss 1.134765, current_train_items 123328.
I0302 19:00:16.472993 22447428132992 run.py:483] Algo bellman_ford step 3854 current loss 1.229072, current_train_items 123360.
I0302 19:00:16.493150 22447428132992 run.py:483] Algo bellman_ford step 3855 current loss 0.578782, current_train_items 123392.
I0302 19:00:16.508744 22447428132992 run.py:483] Algo bellman_ford step 3856 current loss 0.746199, current_train_items 123424.
I0302 19:00:16.531157 22447428132992 run.py:483] Algo bellman_ford step 3857 current loss 0.883277, current_train_items 123456.
I0302 19:00:16.562273 22447428132992 run.py:483] Algo bellman_ford step 3858 current loss 1.056744, current_train_items 123488.
I0302 19:00:16.598140 22447428132992 run.py:483] Algo bellman_ford step 3859 current loss 1.308506, current_train_items 123520.
I0302 19:00:16.617856 22447428132992 run.py:483] Algo bellman_ford step 3860 current loss 0.580930, current_train_items 123552.
I0302 19:00:16.634287 22447428132992 run.py:483] Algo bellman_ford step 3861 current loss 0.812713, current_train_items 123584.
I0302 19:00:16.657411 22447428132992 run.py:483] Algo bellman_ford step 3862 current loss 0.921345, current_train_items 123616.
I0302 19:00:16.688045 22447428132992 run.py:483] Algo bellman_ford step 3863 current loss 1.168777, current_train_items 123648.
I0302 19:00:16.723832 22447428132992 run.py:483] Algo bellman_ford step 3864 current loss 1.206691, current_train_items 123680.
I0302 19:00:16.743000 22447428132992 run.py:483] Algo bellman_ford step 3865 current loss 0.590378, current_train_items 123712.
I0302 19:00:16.758861 22447428132992 run.py:483] Algo bellman_ford step 3866 current loss 0.869255, current_train_items 123744.
I0302 19:00:16.782198 22447428132992 run.py:483] Algo bellman_ford step 3867 current loss 0.889196, current_train_items 123776.
I0302 19:00:16.812627 22447428132992 run.py:483] Algo bellman_ford step 3868 current loss 0.962896, current_train_items 123808.
I0302 19:00:16.845852 22447428132992 run.py:483] Algo bellman_ford step 3869 current loss 1.092409, current_train_items 123840.
I0302 19:00:16.865784 22447428132992 run.py:483] Algo bellman_ford step 3870 current loss 0.701940, current_train_items 123872.
I0302 19:00:16.881522 22447428132992 run.py:483] Algo bellman_ford step 3871 current loss 0.722647, current_train_items 123904.
I0302 19:00:16.905120 22447428132992 run.py:483] Algo bellman_ford step 3872 current loss 0.921795, current_train_items 123936.
I0302 19:00:16.934179 22447428132992 run.py:483] Algo bellman_ford step 3873 current loss 0.972672, current_train_items 123968.
I0302 19:00:16.966444 22447428132992 run.py:483] Algo bellman_ford step 3874 current loss 1.175259, current_train_items 124000.
I0302 19:00:16.986505 22447428132992 run.py:483] Algo bellman_ford step 3875 current loss 0.709622, current_train_items 124032.
I0302 19:00:17.002852 22447428132992 run.py:483] Algo bellman_ford step 3876 current loss 0.810675, current_train_items 124064.
I0302 19:00:17.026732 22447428132992 run.py:483] Algo bellman_ford step 3877 current loss 1.035040, current_train_items 124096.
I0302 19:00:17.057900 22447428132992 run.py:483] Algo bellman_ford step 3878 current loss 1.065944, current_train_items 124128.
I0302 19:00:17.091355 22447428132992 run.py:483] Algo bellman_ford step 3879 current loss 1.300446, current_train_items 124160.
I0302 19:00:17.110912 22447428132992 run.py:483] Algo bellman_ford step 3880 current loss 0.628621, current_train_items 124192.
I0302 19:00:17.126877 22447428132992 run.py:483] Algo bellman_ford step 3881 current loss 0.703840, current_train_items 124224.
I0302 19:00:17.150421 22447428132992 run.py:483] Algo bellman_ford step 3882 current loss 1.071228, current_train_items 124256.
I0302 19:00:17.180780 22447428132992 run.py:483] Algo bellman_ford step 3883 current loss 1.071143, current_train_items 124288.
I0302 19:00:17.213138 22447428132992 run.py:483] Algo bellman_ford step 3884 current loss 1.121571, current_train_items 124320.
I0302 19:00:17.233123 22447428132992 run.py:483] Algo bellman_ford step 3885 current loss 0.728793, current_train_items 124352.
I0302 19:00:17.249649 22447428132992 run.py:483] Algo bellman_ford step 3886 current loss 0.735698, current_train_items 124384.
I0302 19:00:17.272784 22447428132992 run.py:483] Algo bellman_ford step 3887 current loss 0.968972, current_train_items 124416.
I0302 19:00:17.303209 22447428132992 run.py:483] Algo bellman_ford step 3888 current loss 0.997998, current_train_items 124448.
I0302 19:00:17.337752 22447428132992 run.py:483] Algo bellman_ford step 3889 current loss 1.194981, current_train_items 124480.
I0302 19:00:17.357469 22447428132992 run.py:483] Algo bellman_ford step 3890 current loss 0.628742, current_train_items 124512.
I0302 19:00:17.373703 22447428132992 run.py:483] Algo bellman_ford step 3891 current loss 0.868088, current_train_items 124544.
I0302 19:00:17.397581 22447428132992 run.py:483] Algo bellman_ford step 3892 current loss 1.003945, current_train_items 124576.
I0302 19:00:17.426541 22447428132992 run.py:483] Algo bellman_ford step 3893 current loss 0.909357, current_train_items 124608.
I0302 19:00:17.460621 22447428132992 run.py:483] Algo bellman_ford step 3894 current loss 1.256223, current_train_items 124640.
I0302 19:00:17.480149 22447428132992 run.py:483] Algo bellman_ford step 3895 current loss 0.656872, current_train_items 124672.
I0302 19:00:17.496483 22447428132992 run.py:483] Algo bellman_ford step 3896 current loss 0.843656, current_train_items 124704.
I0302 19:00:17.520891 22447428132992 run.py:483] Algo bellman_ford step 3897 current loss 1.071934, current_train_items 124736.
I0302 19:00:17.552126 22447428132992 run.py:483] Algo bellman_ford step 3898 current loss 1.066230, current_train_items 124768.
I0302 19:00:17.585092 22447428132992 run.py:483] Algo bellman_ford step 3899 current loss 1.173606, current_train_items 124800.
I0302 19:00:17.605136 22447428132992 run.py:483] Algo bellman_ford step 3900 current loss 0.706922, current_train_items 124832.
I0302 19:00:17.612745 22447428132992 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 19:00:17.612850 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:00:17.629953 22447428132992 run.py:483] Algo bellman_ford step 3901 current loss 0.812815, current_train_items 124864.
I0302 19:00:17.653259 22447428132992 run.py:483] Algo bellman_ford step 3902 current loss 0.978956, current_train_items 124896.
I0302 19:00:17.684471 22447428132992 run.py:483] Algo bellman_ford step 3903 current loss 1.085929, current_train_items 124928.
I0302 19:00:17.717803 22447428132992 run.py:483] Algo bellman_ford step 3904 current loss 1.168434, current_train_items 124960.
I0302 19:00:17.737806 22447428132992 run.py:483] Algo bellman_ford step 3905 current loss 0.615939, current_train_items 124992.
I0302 19:00:17.753539 22447428132992 run.py:483] Algo bellman_ford step 3906 current loss 0.755807, current_train_items 125024.
I0302 19:00:17.775652 22447428132992 run.py:483] Algo bellman_ford step 3907 current loss 0.890427, current_train_items 125056.
I0302 19:00:17.806451 22447428132992 run.py:483] Algo bellman_ford step 3908 current loss 0.963411, current_train_items 125088.
I0302 19:00:17.840136 22447428132992 run.py:483] Algo bellman_ford step 3909 current loss 1.203312, current_train_items 125120.
I0302 19:00:17.859792 22447428132992 run.py:483] Algo bellman_ford step 3910 current loss 0.590639, current_train_items 125152.
I0302 19:00:17.875637 22447428132992 run.py:483] Algo bellman_ford step 3911 current loss 0.828025, current_train_items 125184.
I0302 19:00:17.898878 22447428132992 run.py:483] Algo bellman_ford step 3912 current loss 1.051991, current_train_items 125216.
I0302 19:00:17.928794 22447428132992 run.py:483] Algo bellman_ford step 3913 current loss 1.121956, current_train_items 125248.
I0302 19:00:17.958850 22447428132992 run.py:483] Algo bellman_ford step 3914 current loss 1.241201, current_train_items 125280.
I0302 19:00:17.978150 22447428132992 run.py:483] Algo bellman_ford step 3915 current loss 0.593468, current_train_items 125312.
I0302 19:00:17.994067 22447428132992 run.py:483] Algo bellman_ford step 3916 current loss 0.786724, current_train_items 125344.
I0302 19:00:18.018564 22447428132992 run.py:483] Algo bellman_ford step 3917 current loss 1.027233, current_train_items 125376.
I0302 19:00:18.048082 22447428132992 run.py:483] Algo bellman_ford step 3918 current loss 1.040240, current_train_items 125408.
I0302 19:00:18.081093 22447428132992 run.py:483] Algo bellman_ford step 3919 current loss 1.095428, current_train_items 125440.
I0302 19:00:18.100582 22447428132992 run.py:483] Algo bellman_ford step 3920 current loss 0.532039, current_train_items 125472.
I0302 19:00:18.116826 22447428132992 run.py:483] Algo bellman_ford step 3921 current loss 0.886370, current_train_items 125504.
I0302 19:00:18.141253 22447428132992 run.py:483] Algo bellman_ford step 3922 current loss 0.916359, current_train_items 125536.
I0302 19:00:18.172447 22447428132992 run.py:483] Algo bellman_ford step 3923 current loss 1.200320, current_train_items 125568.
I0302 19:00:18.207861 22447428132992 run.py:483] Algo bellman_ford step 3924 current loss 1.197873, current_train_items 125600.
I0302 19:00:18.227459 22447428132992 run.py:483] Algo bellman_ford step 3925 current loss 0.533066, current_train_items 125632.
I0302 19:00:18.243547 22447428132992 run.py:483] Algo bellman_ford step 3926 current loss 0.734066, current_train_items 125664.
I0302 19:00:18.266697 22447428132992 run.py:483] Algo bellman_ford step 3927 current loss 0.914825, current_train_items 125696.
I0302 19:00:18.296770 22447428132992 run.py:483] Algo bellman_ford step 3928 current loss 1.074119, current_train_items 125728.
I0302 19:00:18.329636 22447428132992 run.py:483] Algo bellman_ford step 3929 current loss 1.211563, current_train_items 125760.
I0302 19:00:18.348737 22447428132992 run.py:483] Algo bellman_ford step 3930 current loss 0.569310, current_train_items 125792.
I0302 19:00:18.364559 22447428132992 run.py:483] Algo bellman_ford step 3931 current loss 0.813576, current_train_items 125824.
I0302 19:00:18.387615 22447428132992 run.py:483] Algo bellman_ford step 3932 current loss 0.925740, current_train_items 125856.
I0302 19:00:18.419649 22447428132992 run.py:483] Algo bellman_ford step 3933 current loss 1.066341, current_train_items 125888.
I0302 19:00:18.452925 22447428132992 run.py:483] Algo bellman_ford step 3934 current loss 1.181761, current_train_items 125920.
I0302 19:00:18.472205 22447428132992 run.py:483] Algo bellman_ford step 3935 current loss 0.658748, current_train_items 125952.
I0302 19:00:18.488513 22447428132992 run.py:483] Algo bellman_ford step 3936 current loss 0.847370, current_train_items 125984.
I0302 19:00:18.511360 22447428132992 run.py:483] Algo bellman_ford step 3937 current loss 0.884765, current_train_items 126016.
I0302 19:00:18.542544 22447428132992 run.py:483] Algo bellman_ford step 3938 current loss 1.191514, current_train_items 126048.
I0302 19:00:18.576856 22447428132992 run.py:483] Algo bellman_ford step 3939 current loss 1.882402, current_train_items 126080.
I0302 19:00:18.596114 22447428132992 run.py:483] Algo bellman_ford step 3940 current loss 0.570367, current_train_items 126112.
I0302 19:00:18.612111 22447428132992 run.py:483] Algo bellman_ford step 3941 current loss 0.729371, current_train_items 126144.
I0302 19:00:18.635239 22447428132992 run.py:483] Algo bellman_ford step 3942 current loss 0.938169, current_train_items 126176.
I0302 19:00:18.664576 22447428132992 run.py:483] Algo bellman_ford step 3943 current loss 1.017087, current_train_items 126208.
I0302 19:00:18.698830 22447428132992 run.py:483] Algo bellman_ford step 3944 current loss 1.230952, current_train_items 126240.
I0302 19:00:18.718404 22447428132992 run.py:483] Algo bellman_ford step 3945 current loss 0.611502, current_train_items 126272.
I0302 19:00:18.734757 22447428132992 run.py:483] Algo bellman_ford step 3946 current loss 0.779786, current_train_items 126304.
I0302 19:00:18.757739 22447428132992 run.py:483] Algo bellman_ford step 3947 current loss 0.966226, current_train_items 126336.
I0302 19:00:18.786583 22447428132992 run.py:483] Algo bellman_ford step 3948 current loss 1.088202, current_train_items 126368.
I0302 19:00:18.820429 22447428132992 run.py:483] Algo bellman_ford step 3949 current loss 1.021920, current_train_items 126400.
I0302 19:00:18.839731 22447428132992 run.py:483] Algo bellman_ford step 3950 current loss 0.573489, current_train_items 126432.
I0302 19:00:18.847892 22447428132992 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 19:00:18.847995 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:18.864908 22447428132992 run.py:483] Algo bellman_ford step 3951 current loss 0.749353, current_train_items 126464.
I0302 19:00:18.889074 22447428132992 run.py:483] Algo bellman_ford step 3952 current loss 0.967307, current_train_items 126496.
I0302 19:00:18.919283 22447428132992 run.py:483] Algo bellman_ford step 3953 current loss 0.964550, current_train_items 126528.
I0302 19:00:18.951169 22447428132992 run.py:483] Algo bellman_ford step 3954 current loss 1.268855, current_train_items 126560.
I0302 19:00:18.971103 22447428132992 run.py:483] Algo bellman_ford step 3955 current loss 0.579155, current_train_items 126592.
I0302 19:00:18.986938 22447428132992 run.py:483] Algo bellman_ford step 3956 current loss 0.709911, current_train_items 126624.
I0302 19:00:19.010448 22447428132992 run.py:483] Algo bellman_ford step 3957 current loss 0.931896, current_train_items 126656.
I0302 19:00:19.042250 22447428132992 run.py:483] Algo bellman_ford step 3958 current loss 1.097435, current_train_items 126688.
I0302 19:00:19.077830 22447428132992 run.py:483] Algo bellman_ford step 3959 current loss 1.190939, current_train_items 126720.
I0302 19:00:19.097684 22447428132992 run.py:483] Algo bellman_ford step 3960 current loss 0.592315, current_train_items 126752.
I0302 19:00:19.113677 22447428132992 run.py:483] Algo bellman_ford step 3961 current loss 0.802512, current_train_items 126784.
I0302 19:00:19.135890 22447428132992 run.py:483] Algo bellman_ford step 3962 current loss 0.838528, current_train_items 126816.
I0302 19:00:19.167042 22447428132992 run.py:483] Algo bellman_ford step 3963 current loss 0.937345, current_train_items 126848.
I0302 19:00:19.200796 22447428132992 run.py:483] Algo bellman_ford step 3964 current loss 1.354955, current_train_items 126880.
I0302 19:00:19.220400 22447428132992 run.py:483] Algo bellman_ford step 3965 current loss 0.580088, current_train_items 126912.
I0302 19:00:19.236506 22447428132992 run.py:483] Algo bellman_ford step 3966 current loss 0.756270, current_train_items 126944.
I0302 19:00:19.261396 22447428132992 run.py:483] Algo bellman_ford step 3967 current loss 1.073611, current_train_items 126976.
I0302 19:00:19.292102 22447428132992 run.py:483] Algo bellman_ford step 3968 current loss 1.115076, current_train_items 127008.
I0302 19:00:19.324887 22447428132992 run.py:483] Algo bellman_ford step 3969 current loss 1.344061, current_train_items 127040.
I0302 19:00:19.345043 22447428132992 run.py:483] Algo bellman_ford step 3970 current loss 0.551092, current_train_items 127072.
I0302 19:00:19.361055 22447428132992 run.py:483] Algo bellman_ford step 3971 current loss 0.826656, current_train_items 127104.
I0302 19:00:19.384995 22447428132992 run.py:483] Algo bellman_ford step 3972 current loss 0.919714, current_train_items 127136.
I0302 19:00:19.416620 22447428132992 run.py:483] Algo bellman_ford step 3973 current loss 1.184427, current_train_items 127168.
I0302 19:00:19.448576 22447428132992 run.py:483] Algo bellman_ford step 3974 current loss 1.252886, current_train_items 127200.
I0302 19:00:19.468526 22447428132992 run.py:483] Algo bellman_ford step 3975 current loss 0.597095, current_train_items 127232.
I0302 19:00:19.484975 22447428132992 run.py:483] Algo bellman_ford step 3976 current loss 0.849853, current_train_items 127264.
I0302 19:00:19.507729 22447428132992 run.py:483] Algo bellman_ford step 3977 current loss 0.945058, current_train_items 127296.
I0302 19:00:19.539211 22447428132992 run.py:483] Algo bellman_ford step 3978 current loss 1.188299, current_train_items 127328.
I0302 19:00:19.572430 22447428132992 run.py:483] Algo bellman_ford step 3979 current loss 1.202714, current_train_items 127360.
I0302 19:00:19.592016 22447428132992 run.py:483] Algo bellman_ford step 3980 current loss 0.620889, current_train_items 127392.
I0302 19:00:19.608009 22447428132992 run.py:483] Algo bellman_ford step 3981 current loss 0.697378, current_train_items 127424.
I0302 19:00:19.630756 22447428132992 run.py:483] Algo bellman_ford step 3982 current loss 0.927051, current_train_items 127456.
I0302 19:00:19.662032 22447428132992 run.py:483] Algo bellman_ford step 3983 current loss 1.100838, current_train_items 127488.
I0302 19:00:19.694175 22447428132992 run.py:483] Algo bellman_ford step 3984 current loss 1.350519, current_train_items 127520.
I0302 19:00:19.714010 22447428132992 run.py:483] Algo bellman_ford step 3985 current loss 0.695041, current_train_items 127552.
I0302 19:00:19.730355 22447428132992 run.py:483] Algo bellman_ford step 3986 current loss 0.775118, current_train_items 127584.
I0302 19:00:19.753120 22447428132992 run.py:483] Algo bellman_ford step 3987 current loss 0.956856, current_train_items 127616.
I0302 19:00:19.784310 22447428132992 run.py:483] Algo bellman_ford step 3988 current loss 1.053523, current_train_items 127648.
I0302 19:00:19.817988 22447428132992 run.py:483] Algo bellman_ford step 3989 current loss 1.469653, current_train_items 127680.
I0302 19:00:19.838126 22447428132992 run.py:483] Algo bellman_ford step 3990 current loss 0.571653, current_train_items 127712.
I0302 19:00:19.854538 22447428132992 run.py:483] Algo bellman_ford step 3991 current loss 0.743888, current_train_items 127744.
I0302 19:00:19.876870 22447428132992 run.py:483] Algo bellman_ford step 3992 current loss 0.900735, current_train_items 127776.
I0302 19:00:19.907600 22447428132992 run.py:483] Algo bellman_ford step 3993 current loss 1.083806, current_train_items 127808.
I0302 19:00:19.941195 22447428132992 run.py:483] Algo bellman_ford step 3994 current loss 1.355902, current_train_items 127840.
I0302 19:00:19.961017 22447428132992 run.py:483] Algo bellman_ford step 3995 current loss 0.695585, current_train_items 127872.
I0302 19:00:19.977264 22447428132992 run.py:483] Algo bellman_ford step 3996 current loss 0.901926, current_train_items 127904.
I0302 19:00:20.000948 22447428132992 run.py:483] Algo bellman_ford step 3997 current loss 1.005203, current_train_items 127936.
I0302 19:00:20.030951 22447428132992 run.py:483] Algo bellman_ford step 3998 current loss 0.994410, current_train_items 127968.
I0302 19:00:20.065195 22447428132992 run.py:483] Algo bellman_ford step 3999 current loss 1.205368, current_train_items 128000.
I0302 19:00:20.085468 22447428132992 run.py:483] Algo bellman_ford step 4000 current loss 0.931893, current_train_items 128032.
I0302 19:00:20.093313 22447428132992 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 19:00:20.093430 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:00:20.109933 22447428132992 run.py:483] Algo bellman_ford step 4001 current loss 0.798454, current_train_items 128064.
I0302 19:00:20.133897 22447428132992 run.py:483] Algo bellman_ford step 4002 current loss 1.013645, current_train_items 128096.
I0302 19:00:20.164781 22447428132992 run.py:483] Algo bellman_ford step 4003 current loss 0.966294, current_train_items 128128.
I0302 19:00:20.197771 22447428132992 run.py:483] Algo bellman_ford step 4004 current loss 1.098874, current_train_items 128160.
I0302 19:00:20.217739 22447428132992 run.py:483] Algo bellman_ford step 4005 current loss 0.535132, current_train_items 128192.
I0302 19:00:20.233700 22447428132992 run.py:483] Algo bellman_ford step 4006 current loss 0.888822, current_train_items 128224.
I0302 19:00:20.257216 22447428132992 run.py:483] Algo bellman_ford step 4007 current loss 1.133732, current_train_items 128256.
I0302 19:00:20.288732 22447428132992 run.py:483] Algo bellman_ford step 4008 current loss 1.039395, current_train_items 128288.
I0302 19:00:20.320977 22447428132992 run.py:483] Algo bellman_ford step 4009 current loss 1.220864, current_train_items 128320.
I0302 19:00:20.340936 22447428132992 run.py:483] Algo bellman_ford step 4010 current loss 0.697547, current_train_items 128352.
I0302 19:00:20.356974 22447428132992 run.py:483] Algo bellman_ford step 4011 current loss 0.774114, current_train_items 128384.
I0302 19:00:20.379583 22447428132992 run.py:483] Algo bellman_ford step 4012 current loss 0.896957, current_train_items 128416.
I0302 19:00:20.410291 22447428132992 run.py:483] Algo bellman_ford step 4013 current loss 1.065135, current_train_items 128448.
I0302 19:00:20.443926 22447428132992 run.py:483] Algo bellman_ford step 4014 current loss 1.217075, current_train_items 128480.
I0302 19:00:20.463802 22447428132992 run.py:483] Algo bellman_ford step 4015 current loss 0.558328, current_train_items 128512.
I0302 19:00:20.479872 22447428132992 run.py:483] Algo bellman_ford step 4016 current loss 0.784919, current_train_items 128544.
I0302 19:00:20.503537 22447428132992 run.py:483] Algo bellman_ford step 4017 current loss 0.916824, current_train_items 128576.
I0302 19:00:20.534222 22447428132992 run.py:483] Algo bellman_ford step 4018 current loss 1.064731, current_train_items 128608.
I0302 19:00:20.566914 22447428132992 run.py:483] Algo bellman_ford step 4019 current loss 1.141984, current_train_items 128640.
I0302 19:00:20.586268 22447428132992 run.py:483] Algo bellman_ford step 4020 current loss 0.537421, current_train_items 128672.
I0302 19:00:20.602234 22447428132992 run.py:483] Algo bellman_ford step 4021 current loss 0.755928, current_train_items 128704.
I0302 19:00:20.626644 22447428132992 run.py:483] Algo bellman_ford step 4022 current loss 0.940084, current_train_items 128736.
I0302 19:00:20.656663 22447428132992 run.py:483] Algo bellman_ford step 4023 current loss 0.961797, current_train_items 128768.
I0302 19:00:20.692698 22447428132992 run.py:483] Algo bellman_ford step 4024 current loss 1.244280, current_train_items 128800.
I0302 19:00:20.712420 22447428132992 run.py:483] Algo bellman_ford step 4025 current loss 0.664236, current_train_items 128832.
I0302 19:00:20.728665 22447428132992 run.py:483] Algo bellman_ford step 4026 current loss 0.714533, current_train_items 128864.
I0302 19:00:20.751746 22447428132992 run.py:483] Algo bellman_ford step 4027 current loss 0.890986, current_train_items 128896.
I0302 19:00:20.781799 22447428132992 run.py:483] Algo bellman_ford step 4028 current loss 1.075758, current_train_items 128928.
I0302 19:00:20.814449 22447428132992 run.py:483] Algo bellman_ford step 4029 current loss 1.013437, current_train_items 128960.
I0302 19:00:20.833628 22447428132992 run.py:483] Algo bellman_ford step 4030 current loss 0.614181, current_train_items 128992.
I0302 19:00:20.849399 22447428132992 run.py:483] Algo bellman_ford step 4031 current loss 0.715010, current_train_items 129024.
I0302 19:00:20.872820 22447428132992 run.py:483] Algo bellman_ford step 4032 current loss 0.881554, current_train_items 129056.
I0302 19:00:20.904785 22447428132992 run.py:483] Algo bellman_ford step 4033 current loss 1.040063, current_train_items 129088.
I0302 19:00:20.937691 22447428132992 run.py:483] Algo bellman_ford step 4034 current loss 1.091545, current_train_items 129120.
I0302 19:00:20.957113 22447428132992 run.py:483] Algo bellman_ford step 4035 current loss 0.593453, current_train_items 129152.
I0302 19:00:20.973063 22447428132992 run.py:483] Algo bellman_ford step 4036 current loss 0.758868, current_train_items 129184.
I0302 19:00:20.996292 22447428132992 run.py:483] Algo bellman_ford step 4037 current loss 0.859265, current_train_items 129216.
I0302 19:00:21.027771 22447428132992 run.py:483] Algo bellman_ford step 4038 current loss 1.123159, current_train_items 129248.
I0302 19:00:21.061510 22447428132992 run.py:483] Algo bellman_ford step 4039 current loss 1.583291, current_train_items 129280.
I0302 19:00:21.081144 22447428132992 run.py:483] Algo bellman_ford step 4040 current loss 0.548424, current_train_items 129312.
I0302 19:00:21.097333 22447428132992 run.py:483] Algo bellman_ford step 4041 current loss 0.830671, current_train_items 129344.
I0302 19:00:21.120487 22447428132992 run.py:483] Algo bellman_ford step 4042 current loss 0.939711, current_train_items 129376.
I0302 19:00:21.152436 22447428132992 run.py:483] Algo bellman_ford step 4043 current loss 1.106188, current_train_items 129408.
I0302 19:00:21.184892 22447428132992 run.py:483] Algo bellman_ford step 4044 current loss 1.241028, current_train_items 129440.
I0302 19:00:21.204332 22447428132992 run.py:483] Algo bellman_ford step 4045 current loss 0.558092, current_train_items 129472.
I0302 19:00:21.220689 22447428132992 run.py:483] Algo bellman_ford step 4046 current loss 0.745700, current_train_items 129504.
I0302 19:00:21.243856 22447428132992 run.py:483] Algo bellman_ford step 4047 current loss 0.990979, current_train_items 129536.
I0302 19:00:21.273720 22447428132992 run.py:483] Algo bellman_ford step 4048 current loss 0.999495, current_train_items 129568.
I0302 19:00:21.308293 22447428132992 run.py:483] Algo bellman_ford step 4049 current loss 1.212171, current_train_items 129600.
I0302 19:00:21.327740 22447428132992 run.py:483] Algo bellman_ford step 4050 current loss 0.610912, current_train_items 129632.
I0302 19:00:21.335862 22447428132992 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 19:00:21.335968 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.909, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:00:21.352761 22447428132992 run.py:483] Algo bellman_ford step 4051 current loss 0.801034, current_train_items 129664.
I0302 19:00:21.375614 22447428132992 run.py:483] Algo bellman_ford step 4052 current loss 0.900727, current_train_items 129696.
I0302 19:00:21.406154 22447428132992 run.py:483] Algo bellman_ford step 4053 current loss 1.165942, current_train_items 129728.
I0302 19:00:21.441548 22447428132992 run.py:483] Algo bellman_ford step 4054 current loss 1.269566, current_train_items 129760.
I0302 19:00:21.461430 22447428132992 run.py:483] Algo bellman_ford step 4055 current loss 0.553653, current_train_items 129792.
I0302 19:00:21.477038 22447428132992 run.py:483] Algo bellman_ford step 4056 current loss 0.744419, current_train_items 129824.
I0302 19:00:21.499951 22447428132992 run.py:483] Algo bellman_ford step 4057 current loss 0.880466, current_train_items 129856.
I0302 19:00:21.529635 22447428132992 run.py:483] Algo bellman_ford step 4058 current loss 1.064913, current_train_items 129888.
I0302 19:00:21.561371 22447428132992 run.py:483] Algo bellman_ford step 4059 current loss 1.141614, current_train_items 129920.
I0302 19:00:21.581076 22447428132992 run.py:483] Algo bellman_ford step 4060 current loss 0.776204, current_train_items 129952.
I0302 19:00:21.597609 22447428132992 run.py:483] Algo bellman_ford step 4061 current loss 0.761461, current_train_items 129984.
I0302 19:00:21.619709 22447428132992 run.py:483] Algo bellman_ford step 4062 current loss 0.987094, current_train_items 130016.
I0302 19:00:21.648957 22447428132992 run.py:483] Algo bellman_ford step 4063 current loss 1.023072, current_train_items 130048.
I0302 19:00:21.683400 22447428132992 run.py:483] Algo bellman_ford step 4064 current loss 1.227481, current_train_items 130080.
I0302 19:00:21.702753 22447428132992 run.py:483] Algo bellman_ford step 4065 current loss 0.560109, current_train_items 130112.
I0302 19:00:21.718370 22447428132992 run.py:483] Algo bellman_ford step 4066 current loss 0.832777, current_train_items 130144.
I0302 19:00:21.742647 22447428132992 run.py:483] Algo bellman_ford step 4067 current loss 1.080356, current_train_items 130176.
I0302 19:00:21.772807 22447428132992 run.py:483] Algo bellman_ford step 4068 current loss 1.001865, current_train_items 130208.
I0302 19:00:21.807286 22447428132992 run.py:483] Algo bellman_ford step 4069 current loss 1.261061, current_train_items 130240.
I0302 19:00:21.826860 22447428132992 run.py:483] Algo bellman_ford step 4070 current loss 0.547170, current_train_items 130272.
I0302 19:00:21.843107 22447428132992 run.py:483] Algo bellman_ford step 4071 current loss 0.760694, current_train_items 130304.
I0302 19:00:21.865840 22447428132992 run.py:483] Algo bellman_ford step 4072 current loss 1.006421, current_train_items 130336.
I0302 19:00:21.894768 22447428132992 run.py:483] Algo bellman_ford step 4073 current loss 1.033690, current_train_items 130368.
I0302 19:00:21.927073 22447428132992 run.py:483] Algo bellman_ford step 4074 current loss 1.257399, current_train_items 130400.
I0302 19:00:21.946672 22447428132992 run.py:483] Algo bellman_ford step 4075 current loss 0.617744, current_train_items 130432.
I0302 19:00:21.963278 22447428132992 run.py:483] Algo bellman_ford step 4076 current loss 0.791299, current_train_items 130464.
I0302 19:00:21.986912 22447428132992 run.py:483] Algo bellman_ford step 4077 current loss 1.019338, current_train_items 130496.
I0302 19:00:22.016881 22447428132992 run.py:483] Algo bellman_ford step 4078 current loss 1.050224, current_train_items 130528.
I0302 19:00:22.049468 22447428132992 run.py:483] Algo bellman_ford step 4079 current loss 1.079891, current_train_items 130560.
I0302 19:00:22.068651 22447428132992 run.py:483] Algo bellman_ford step 4080 current loss 0.636271, current_train_items 130592.
I0302 19:00:22.085029 22447428132992 run.py:483] Algo bellman_ford step 4081 current loss 0.744335, current_train_items 130624.
I0302 19:00:22.108934 22447428132992 run.py:483] Algo bellman_ford step 4082 current loss 1.019597, current_train_items 130656.
I0302 19:00:22.140678 22447428132992 run.py:483] Algo bellman_ford step 4083 current loss 1.085286, current_train_items 130688.
I0302 19:00:22.171323 22447428132992 run.py:483] Algo bellman_ford step 4084 current loss 1.044202, current_train_items 130720.
I0302 19:00:22.191273 22447428132992 run.py:483] Algo bellman_ford step 4085 current loss 0.732838, current_train_items 130752.
I0302 19:00:22.207494 22447428132992 run.py:483] Algo bellman_ford step 4086 current loss 0.799121, current_train_items 130784.
I0302 19:00:22.229881 22447428132992 run.py:483] Algo bellman_ford step 4087 current loss 0.981093, current_train_items 130816.
I0302 19:00:22.259794 22447428132992 run.py:483] Algo bellman_ford step 4088 current loss 1.057778, current_train_items 130848.
I0302 19:00:22.291443 22447428132992 run.py:483] Algo bellman_ford step 4089 current loss 1.157305, current_train_items 130880.
I0302 19:00:22.310942 22447428132992 run.py:483] Algo bellman_ford step 4090 current loss 0.659996, current_train_items 130912.
I0302 19:00:22.326955 22447428132992 run.py:483] Algo bellman_ford step 4091 current loss 0.750655, current_train_items 130944.
I0302 19:00:22.350424 22447428132992 run.py:483] Algo bellman_ford step 4092 current loss 1.114670, current_train_items 130976.
I0302 19:00:22.381180 22447428132992 run.py:483] Algo bellman_ford step 4093 current loss 1.092046, current_train_items 131008.
I0302 19:00:22.414628 22447428132992 run.py:483] Algo bellman_ford step 4094 current loss 1.372325, current_train_items 131040.
I0302 19:00:22.433988 22447428132992 run.py:483] Algo bellman_ford step 4095 current loss 0.582028, current_train_items 131072.
I0302 19:00:22.450351 22447428132992 run.py:483] Algo bellman_ford step 4096 current loss 0.826528, current_train_items 131104.
I0302 19:00:22.473917 22447428132992 run.py:483] Algo bellman_ford step 4097 current loss 0.935317, current_train_items 131136.
I0302 19:00:22.504511 22447428132992 run.py:483] Algo bellman_ford step 4098 current loss 1.053968, current_train_items 131168.
I0302 19:00:22.538326 22447428132992 run.py:483] Algo bellman_ford step 4099 current loss 1.110399, current_train_items 131200.
I0302 19:00:22.557986 22447428132992 run.py:483] Algo bellman_ford step 4100 current loss 0.564016, current_train_items 131232.
I0302 19:00:22.565919 22447428132992 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 19:00:22.566027 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.909, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:22.595189 22447428132992 run.py:483] Algo bellman_ford step 4101 current loss 0.694332, current_train_items 131264.
I0302 19:00:22.616787 22447428132992 run.py:483] Algo bellman_ford step 4102 current loss 0.933905, current_train_items 131296.
I0302 19:00:22.648596 22447428132992 run.py:483] Algo bellman_ford step 4103 current loss 1.086100, current_train_items 131328.
I0302 19:00:22.683642 22447428132992 run.py:483] Algo bellman_ford step 4104 current loss 1.208573, current_train_items 131360.
I0302 19:00:22.703346 22447428132992 run.py:483] Algo bellman_ford step 4105 current loss 0.590002, current_train_items 131392.
I0302 19:00:22.719391 22447428132992 run.py:483] Algo bellman_ford step 4106 current loss 0.802372, current_train_items 131424.
I0302 19:00:22.743141 22447428132992 run.py:483] Algo bellman_ford step 4107 current loss 1.041440, current_train_items 131456.
I0302 19:00:22.773526 22447428132992 run.py:483] Algo bellman_ford step 4108 current loss 1.054169, current_train_items 131488.
I0302 19:00:22.808849 22447428132992 run.py:483] Algo bellman_ford step 4109 current loss 1.234892, current_train_items 131520.
I0302 19:00:22.828130 22447428132992 run.py:483] Algo bellman_ford step 4110 current loss 0.638266, current_train_items 131552.
I0302 19:00:22.844832 22447428132992 run.py:483] Algo bellman_ford step 4111 current loss 0.793462, current_train_items 131584.
I0302 19:00:22.867164 22447428132992 run.py:483] Algo bellman_ford step 4112 current loss 0.888483, current_train_items 131616.
I0302 19:00:22.898513 22447428132992 run.py:483] Algo bellman_ford step 4113 current loss 0.966389, current_train_items 131648.
I0302 19:00:22.932657 22447428132992 run.py:483] Algo bellman_ford step 4114 current loss 1.148075, current_train_items 131680.
I0302 19:00:22.951948 22447428132992 run.py:483] Algo bellman_ford step 4115 current loss 0.593083, current_train_items 131712.
I0302 19:00:22.968305 22447428132992 run.py:483] Algo bellman_ford step 4116 current loss 0.844194, current_train_items 131744.
I0302 19:00:22.992522 22447428132992 run.py:483] Algo bellman_ford step 4117 current loss 1.089838, current_train_items 131776.
I0302 19:00:23.021989 22447428132992 run.py:483] Algo bellman_ford step 4118 current loss 1.032345, current_train_items 131808.
I0302 19:00:23.055694 22447428132992 run.py:483] Algo bellman_ford step 4119 current loss 1.720751, current_train_items 131840.
I0302 19:00:23.074782 22447428132992 run.py:483] Algo bellman_ford step 4120 current loss 0.558212, current_train_items 131872.
I0302 19:00:23.090925 22447428132992 run.py:483] Algo bellman_ford step 4121 current loss 0.692470, current_train_items 131904.
I0302 19:00:23.114982 22447428132992 run.py:483] Algo bellman_ford step 4122 current loss 1.029417, current_train_items 131936.
I0302 19:00:23.146477 22447428132992 run.py:483] Algo bellman_ford step 4123 current loss 1.339474, current_train_items 131968.
I0302 19:00:23.179706 22447428132992 run.py:483] Algo bellman_ford step 4124 current loss 1.548216, current_train_items 132000.
I0302 19:00:23.198847 22447428132992 run.py:483] Algo bellman_ford step 4125 current loss 0.569926, current_train_items 132032.
I0302 19:00:23.215011 22447428132992 run.py:483] Algo bellman_ford step 4126 current loss 0.806960, current_train_items 132064.
I0302 19:00:23.239254 22447428132992 run.py:483] Algo bellman_ford step 4127 current loss 1.101221, current_train_items 132096.
I0302 19:00:23.269298 22447428132992 run.py:483] Algo bellman_ford step 4128 current loss 1.212656, current_train_items 132128.
I0302 19:00:23.301339 22447428132992 run.py:483] Algo bellman_ford step 4129 current loss 1.182150, current_train_items 132160.
I0302 19:00:23.320926 22447428132992 run.py:483] Algo bellman_ford step 4130 current loss 0.570054, current_train_items 132192.
I0302 19:00:23.337222 22447428132992 run.py:483] Algo bellman_ford step 4131 current loss 0.828487, current_train_items 132224.
I0302 19:00:23.360437 22447428132992 run.py:483] Algo bellman_ford step 4132 current loss 1.052653, current_train_items 132256.
I0302 19:00:23.391723 22447428132992 run.py:483] Algo bellman_ford step 4133 current loss 1.052318, current_train_items 132288.
I0302 19:00:23.426169 22447428132992 run.py:483] Algo bellman_ford step 4134 current loss 1.124515, current_train_items 132320.
I0302 19:00:23.445608 22447428132992 run.py:483] Algo bellman_ford step 4135 current loss 0.581291, current_train_items 132352.
I0302 19:00:23.461569 22447428132992 run.py:483] Algo bellman_ford step 4136 current loss 0.780475, current_train_items 132384.
I0302 19:00:23.484502 22447428132992 run.py:483] Algo bellman_ford step 4137 current loss 0.935528, current_train_items 132416.
I0302 19:00:23.514556 22447428132992 run.py:483] Algo bellman_ford step 4138 current loss 1.000773, current_train_items 132448.
I0302 19:00:23.548497 22447428132992 run.py:483] Algo bellman_ford step 4139 current loss 1.137732, current_train_items 132480.
I0302 19:00:23.567855 22447428132992 run.py:483] Algo bellman_ford step 4140 current loss 0.669417, current_train_items 132512.
I0302 19:00:23.583545 22447428132992 run.py:483] Algo bellman_ford step 4141 current loss 0.733390, current_train_items 132544.
I0302 19:00:23.607825 22447428132992 run.py:483] Algo bellman_ford step 4142 current loss 1.045544, current_train_items 132576.
I0302 19:00:23.637537 22447428132992 run.py:483] Algo bellman_ford step 4143 current loss 0.978810, current_train_items 132608.
I0302 19:00:23.670191 22447428132992 run.py:483] Algo bellman_ford step 4144 current loss 1.118107, current_train_items 132640.
I0302 19:00:23.689618 22447428132992 run.py:483] Algo bellman_ford step 4145 current loss 0.577801, current_train_items 132672.
I0302 19:00:23.705390 22447428132992 run.py:483] Algo bellman_ford step 4146 current loss 0.736207, current_train_items 132704.
I0302 19:00:23.728260 22447428132992 run.py:483] Algo bellman_ford step 4147 current loss 0.957873, current_train_items 132736.
I0302 19:00:23.757343 22447428132992 run.py:483] Algo bellman_ford step 4148 current loss 0.900844, current_train_items 132768.
I0302 19:00:23.789904 22447428132992 run.py:483] Algo bellman_ford step 4149 current loss 1.251862, current_train_items 132800.
I0302 19:00:23.809217 22447428132992 run.py:483] Algo bellman_ford step 4150 current loss 0.662513, current_train_items 132832.
I0302 19:00:23.817394 22447428132992 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 19:00:23.817498 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 19:00:23.834125 22447428132992 run.py:483] Algo bellman_ford step 4151 current loss 0.846513, current_train_items 132864.
I0302 19:00:23.858375 22447428132992 run.py:483] Algo bellman_ford step 4152 current loss 0.978795, current_train_items 132896.
I0302 19:00:23.889335 22447428132992 run.py:483] Algo bellman_ford step 4153 current loss 1.026017, current_train_items 132928.
I0302 19:00:23.924638 22447428132992 run.py:483] Algo bellman_ford step 4154 current loss 1.235959, current_train_items 132960.
I0302 19:00:23.944668 22447428132992 run.py:483] Algo bellman_ford step 4155 current loss 0.592691, current_train_items 132992.
I0302 19:00:23.961066 22447428132992 run.py:483] Algo bellman_ford step 4156 current loss 0.822249, current_train_items 133024.
I0302 19:00:23.983708 22447428132992 run.py:483] Algo bellman_ford step 4157 current loss 0.872793, current_train_items 133056.
I0302 19:00:24.013923 22447428132992 run.py:483] Algo bellman_ford step 4158 current loss 0.997081, current_train_items 133088.
I0302 19:00:24.047777 22447428132992 run.py:483] Algo bellman_ford step 4159 current loss 1.178425, current_train_items 133120.
I0302 19:00:24.067900 22447428132992 run.py:483] Algo bellman_ford step 4160 current loss 0.688377, current_train_items 133152.
I0302 19:00:24.084183 22447428132992 run.py:483] Algo bellman_ford step 4161 current loss 0.717103, current_train_items 133184.
I0302 19:00:24.106191 22447428132992 run.py:483] Algo bellman_ford step 4162 current loss 0.961937, current_train_items 133216.
I0302 19:00:24.137209 22447428132992 run.py:483] Algo bellman_ford step 4163 current loss 1.037912, current_train_items 133248.
I0302 19:00:24.171072 22447428132992 run.py:483] Algo bellman_ford step 4164 current loss 1.085611, current_train_items 133280.
I0302 19:00:24.190972 22447428132992 run.py:483] Algo bellman_ford step 4165 current loss 0.641968, current_train_items 133312.
I0302 19:00:24.207043 22447428132992 run.py:483] Algo bellman_ford step 4166 current loss 0.782894, current_train_items 133344.
I0302 19:00:24.231559 22447428132992 run.py:483] Algo bellman_ford step 4167 current loss 1.032102, current_train_items 133376.
I0302 19:00:24.262071 22447428132992 run.py:483] Algo bellman_ford step 4168 current loss 0.986488, current_train_items 133408.
I0302 19:00:24.294864 22447428132992 run.py:483] Algo bellman_ford step 4169 current loss 1.093841, current_train_items 133440.
I0302 19:00:24.314651 22447428132992 run.py:483] Algo bellman_ford step 4170 current loss 0.524230, current_train_items 133472.
I0302 19:00:24.330211 22447428132992 run.py:483] Algo bellman_ford step 4171 current loss 0.690298, current_train_items 133504.
I0302 19:00:24.352948 22447428132992 run.py:483] Algo bellman_ford step 4172 current loss 0.949537, current_train_items 133536.
I0302 19:00:24.383121 22447428132992 run.py:483] Algo bellman_ford step 4173 current loss 0.966118, current_train_items 133568.
I0302 19:00:24.418282 22447428132992 run.py:483] Algo bellman_ford step 4174 current loss 1.169028, current_train_items 133600.
I0302 19:00:24.437802 22447428132992 run.py:483] Algo bellman_ford step 4175 current loss 0.654467, current_train_items 133632.
I0302 19:00:24.454480 22447428132992 run.py:483] Algo bellman_ford step 4176 current loss 0.780146, current_train_items 133664.
I0302 19:00:24.477708 22447428132992 run.py:483] Algo bellman_ford step 4177 current loss 0.964079, current_train_items 133696.
I0302 19:00:24.508737 22447428132992 run.py:483] Algo bellman_ford step 4178 current loss 1.089100, current_train_items 133728.
I0302 19:00:24.542767 22447428132992 run.py:483] Algo bellman_ford step 4179 current loss 1.062968, current_train_items 133760.
I0302 19:00:24.562604 22447428132992 run.py:483] Algo bellman_ford step 4180 current loss 0.679877, current_train_items 133792.
I0302 19:00:24.578690 22447428132992 run.py:483] Algo bellman_ford step 4181 current loss 0.777804, current_train_items 133824.
I0302 19:00:24.603171 22447428132992 run.py:483] Algo bellman_ford step 4182 current loss 0.966601, current_train_items 133856.
I0302 19:00:24.632410 22447428132992 run.py:483] Algo bellman_ford step 4183 current loss 1.000819, current_train_items 133888.
I0302 19:00:24.666344 22447428132992 run.py:483] Algo bellman_ford step 4184 current loss 1.286756, current_train_items 133920.
I0302 19:00:24.686194 22447428132992 run.py:483] Algo bellman_ford step 4185 current loss 0.613559, current_train_items 133952.
I0302 19:00:24.702143 22447428132992 run.py:483] Algo bellman_ford step 4186 current loss 0.756587, current_train_items 133984.
I0302 19:00:24.725492 22447428132992 run.py:483] Algo bellman_ford step 4187 current loss 0.948777, current_train_items 134016.
I0302 19:00:24.755857 22447428132992 run.py:483] Algo bellman_ford step 4188 current loss 1.080716, current_train_items 134048.
I0302 19:00:24.788436 22447428132992 run.py:483] Algo bellman_ford step 4189 current loss 1.280858, current_train_items 134080.
I0302 19:00:24.808387 22447428132992 run.py:483] Algo bellman_ford step 4190 current loss 0.706338, current_train_items 134112.
I0302 19:00:24.824908 22447428132992 run.py:483] Algo bellman_ford step 4191 current loss 0.744043, current_train_items 134144.
I0302 19:00:24.845852 22447428132992 run.py:483] Algo bellman_ford step 4192 current loss 0.802466, current_train_items 134176.
I0302 19:00:24.877048 22447428132992 run.py:483] Algo bellman_ford step 4193 current loss 1.029637, current_train_items 134208.
I0302 19:00:24.912550 22447428132992 run.py:483] Algo bellman_ford step 4194 current loss 1.160698, current_train_items 134240.
I0302 19:00:24.932120 22447428132992 run.py:483] Algo bellman_ford step 4195 current loss 0.819440, current_train_items 134272.
I0302 19:00:24.948101 22447428132992 run.py:483] Algo bellman_ford step 4196 current loss 0.816657, current_train_items 134304.
I0302 19:00:24.971709 22447428132992 run.py:483] Algo bellman_ford step 4197 current loss 0.928501, current_train_items 134336.
I0302 19:00:25.001992 22447428132992 run.py:483] Algo bellman_ford step 4198 current loss 1.081975, current_train_items 134368.
I0302 19:00:25.035256 22447428132992 run.py:483] Algo bellman_ford step 4199 current loss 1.184817, current_train_items 134400.
I0302 19:00:25.054688 22447428132992 run.py:483] Algo bellman_ford step 4200 current loss 0.598222, current_train_items 134432.
I0302 19:00:25.062564 22447428132992 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 19:00:25.062669 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 19:00:25.079553 22447428132992 run.py:483] Algo bellman_ford step 4201 current loss 0.796210, current_train_items 134464.
I0302 19:00:25.102644 22447428132992 run.py:483] Algo bellman_ford step 4202 current loss 1.004774, current_train_items 134496.
I0302 19:00:25.133048 22447428132992 run.py:483] Algo bellman_ford step 4203 current loss 1.079940, current_train_items 134528.
I0302 19:00:25.166405 22447428132992 run.py:483] Algo bellman_ford step 4204 current loss 1.174620, current_train_items 134560.
I0302 19:00:25.186091 22447428132992 run.py:483] Algo bellman_ford step 4205 current loss 0.633391, current_train_items 134592.
I0302 19:00:25.202069 22447428132992 run.py:483] Algo bellman_ford step 4206 current loss 0.813339, current_train_items 134624.
I0302 19:00:25.225750 22447428132992 run.py:483] Algo bellman_ford step 4207 current loss 0.942395, current_train_items 134656.
I0302 19:00:25.257055 22447428132992 run.py:483] Algo bellman_ford step 4208 current loss 1.059449, current_train_items 134688.
I0302 19:00:25.291092 22447428132992 run.py:483] Algo bellman_ford step 4209 current loss 1.274768, current_train_items 134720.
I0302 19:00:25.310896 22447428132992 run.py:483] Algo bellman_ford step 4210 current loss 0.691685, current_train_items 134752.
I0302 19:00:25.326974 22447428132992 run.py:483] Algo bellman_ford step 4211 current loss 0.877353, current_train_items 134784.
I0302 19:00:25.350191 22447428132992 run.py:483] Algo bellman_ford step 4212 current loss 0.926638, current_train_items 134816.
I0302 19:00:25.381492 22447428132992 run.py:483] Algo bellman_ford step 4213 current loss 1.026583, current_train_items 134848.
I0302 19:00:25.414478 22447428132992 run.py:483] Algo bellman_ford step 4214 current loss 1.084737, current_train_items 134880.
I0302 19:00:25.433703 22447428132992 run.py:483] Algo bellman_ford step 4215 current loss 0.750011, current_train_items 134912.
I0302 19:00:25.449819 22447428132992 run.py:483] Algo bellman_ford step 4216 current loss 0.777029, current_train_items 134944.
I0302 19:00:25.472424 22447428132992 run.py:483] Algo bellman_ford step 4217 current loss 0.924728, current_train_items 134976.
I0302 19:00:25.503057 22447428132992 run.py:483] Algo bellman_ford step 4218 current loss 1.032514, current_train_items 135008.
I0302 19:00:25.536233 22447428132992 run.py:483] Algo bellman_ford step 4219 current loss 1.060550, current_train_items 135040.
I0302 19:00:25.555554 22447428132992 run.py:483] Algo bellman_ford step 4220 current loss 0.565542, current_train_items 135072.
I0302 19:00:25.571787 22447428132992 run.py:483] Algo bellman_ford step 4221 current loss 0.786883, current_train_items 135104.
I0302 19:00:25.595267 22447428132992 run.py:483] Algo bellman_ford step 4222 current loss 0.895204, current_train_items 135136.
I0302 19:00:25.626694 22447428132992 run.py:483] Algo bellman_ford step 4223 current loss 1.078193, current_train_items 135168.
I0302 19:00:25.661992 22447428132992 run.py:483] Algo bellman_ford step 4224 current loss 1.111968, current_train_items 135200.
I0302 19:00:25.681304 22447428132992 run.py:483] Algo bellman_ford step 4225 current loss 0.700947, current_train_items 135232.
I0302 19:00:25.697244 22447428132992 run.py:483] Algo bellman_ford step 4226 current loss 0.723880, current_train_items 135264.
I0302 19:00:25.719960 22447428132992 run.py:483] Algo bellman_ford step 4227 current loss 0.891890, current_train_items 135296.
I0302 19:00:25.749254 22447428132992 run.py:483] Algo bellman_ford step 4228 current loss 1.043067, current_train_items 135328.
I0302 19:00:25.780579 22447428132992 run.py:483] Algo bellman_ford step 4229 current loss 0.980340, current_train_items 135360.
I0302 19:00:25.799725 22447428132992 run.py:483] Algo bellman_ford step 4230 current loss 0.585411, current_train_items 135392.
I0302 19:00:25.815545 22447428132992 run.py:483] Algo bellman_ford step 4231 current loss 0.660882, current_train_items 135424.
I0302 19:00:25.838655 22447428132992 run.py:483] Algo bellman_ford step 4232 current loss 1.014188, current_train_items 135456.
I0302 19:00:25.869214 22447428132992 run.py:483] Algo bellman_ford step 4233 current loss 0.997304, current_train_items 135488.
I0302 19:00:25.901844 22447428132992 run.py:483] Algo bellman_ford step 4234 current loss 1.129264, current_train_items 135520.
I0302 19:00:25.921240 22447428132992 run.py:483] Algo bellman_ford step 4235 current loss 0.595510, current_train_items 135552.
I0302 19:00:25.937898 22447428132992 run.py:483] Algo bellman_ford step 4236 current loss 0.855407, current_train_items 135584.
I0302 19:00:25.961053 22447428132992 run.py:483] Algo bellman_ford step 4237 current loss 0.943486, current_train_items 135616.
I0302 19:00:25.991894 22447428132992 run.py:483] Algo bellman_ford step 4238 current loss 0.977094, current_train_items 135648.
I0302 19:00:26.026838 22447428132992 run.py:483] Algo bellman_ford step 4239 current loss 1.172659, current_train_items 135680.
I0302 19:00:26.046232 22447428132992 run.py:483] Algo bellman_ford step 4240 current loss 0.606183, current_train_items 135712.
I0302 19:00:26.062177 22447428132992 run.py:483] Algo bellman_ford step 4241 current loss 0.713264, current_train_items 135744.
I0302 19:00:26.084820 22447428132992 run.py:483] Algo bellman_ford step 4242 current loss 0.842470, current_train_items 135776.
I0302 19:00:26.116131 22447428132992 run.py:483] Algo bellman_ford step 4243 current loss 1.114458, current_train_items 135808.
I0302 19:00:26.148631 22447428132992 run.py:483] Algo bellman_ford step 4244 current loss 1.165087, current_train_items 135840.
I0302 19:00:26.167810 22447428132992 run.py:483] Algo bellman_ford step 4245 current loss 0.550830, current_train_items 135872.
I0302 19:00:26.183548 22447428132992 run.py:483] Algo bellman_ford step 4246 current loss 0.683863, current_train_items 135904.
I0302 19:00:26.207977 22447428132992 run.py:483] Algo bellman_ford step 4247 current loss 0.981428, current_train_items 135936.
I0302 19:00:26.239003 22447428132992 run.py:483] Algo bellman_ford step 4248 current loss 1.084419, current_train_items 135968.
I0302 19:00:26.273934 22447428132992 run.py:483] Algo bellman_ford step 4249 current loss 1.102517, current_train_items 136000.
I0302 19:00:26.293798 22447428132992 run.py:483] Algo bellman_ford step 4250 current loss 0.530604, current_train_items 136032.
I0302 19:00:26.301824 22447428132992 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 19:00:26.301929 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:00:26.318414 22447428132992 run.py:483] Algo bellman_ford step 4251 current loss 0.690059, current_train_items 136064.
I0302 19:00:26.342075 22447428132992 run.py:483] Algo bellman_ford step 4252 current loss 0.980305, current_train_items 136096.
I0302 19:00:26.372884 22447428132992 run.py:483] Algo bellman_ford step 4253 current loss 1.070248, current_train_items 136128.
I0302 19:00:26.406686 22447428132992 run.py:483] Algo bellman_ford step 4254 current loss 1.159890, current_train_items 136160.
I0302 19:00:26.426148 22447428132992 run.py:483] Algo bellman_ford step 4255 current loss 0.594585, current_train_items 136192.
I0302 19:00:26.441789 22447428132992 run.py:483] Algo bellman_ford step 4256 current loss 0.739044, current_train_items 136224.
I0302 19:00:26.464287 22447428132992 run.py:483] Algo bellman_ford step 4257 current loss 0.815601, current_train_items 136256.
I0302 19:00:26.495237 22447428132992 run.py:483] Algo bellman_ford step 4258 current loss 1.138430, current_train_items 136288.
I0302 19:00:26.528917 22447428132992 run.py:483] Algo bellman_ford step 4259 current loss 1.083039, current_train_items 136320.
I0302 19:00:26.548781 22447428132992 run.py:483] Algo bellman_ford step 4260 current loss 0.581787, current_train_items 136352.
I0302 19:00:26.565459 22447428132992 run.py:483] Algo bellman_ford step 4261 current loss 0.885543, current_train_items 136384.
I0302 19:00:26.588070 22447428132992 run.py:483] Algo bellman_ford step 4262 current loss 0.941704, current_train_items 136416.
I0302 19:00:26.618522 22447428132992 run.py:483] Algo bellman_ford step 4263 current loss 1.044404, current_train_items 136448.
I0302 19:00:26.651885 22447428132992 run.py:483] Algo bellman_ford step 4264 current loss 1.221866, current_train_items 136480.
I0302 19:00:26.671271 22447428132992 run.py:483] Algo bellman_ford step 4265 current loss 0.682323, current_train_items 136512.
I0302 19:00:26.687359 22447428132992 run.py:483] Algo bellman_ford step 4266 current loss 0.844336, current_train_items 136544.
I0302 19:00:26.710230 22447428132992 run.py:483] Algo bellman_ford step 4267 current loss 0.837150, current_train_items 136576.
I0302 19:00:26.740369 22447428132992 run.py:483] Algo bellman_ford step 4268 current loss 1.031404, current_train_items 136608.
I0302 19:00:26.772887 22447428132992 run.py:483] Algo bellman_ford step 4269 current loss 1.214866, current_train_items 136640.
I0302 19:00:26.792680 22447428132992 run.py:483] Algo bellman_ford step 4270 current loss 0.593885, current_train_items 136672.
I0302 19:00:26.808605 22447428132992 run.py:483] Algo bellman_ford step 4271 current loss 0.754064, current_train_items 136704.
I0302 19:00:26.830404 22447428132992 run.py:483] Algo bellman_ford step 4272 current loss 0.809551, current_train_items 136736.
I0302 19:00:26.860645 22447428132992 run.py:483] Algo bellman_ford step 4273 current loss 1.065184, current_train_items 136768.
I0302 19:00:26.892955 22447428132992 run.py:483] Algo bellman_ford step 4274 current loss 1.296638, current_train_items 136800.
I0302 19:00:26.912633 22447428132992 run.py:483] Algo bellman_ford step 4275 current loss 0.590549, current_train_items 136832.
I0302 19:00:26.928669 22447428132992 run.py:483] Algo bellman_ford step 4276 current loss 0.688468, current_train_items 136864.
I0302 19:00:26.951182 22447428132992 run.py:483] Algo bellman_ford step 4277 current loss 0.934512, current_train_items 136896.
I0302 19:00:26.983726 22447428132992 run.py:483] Algo bellman_ford step 4278 current loss 1.084606, current_train_items 136928.
I0302 19:00:27.017736 22447428132992 run.py:483] Algo bellman_ford step 4279 current loss 1.138902, current_train_items 136960.
I0302 19:00:27.037086 22447428132992 run.py:483] Algo bellman_ford step 4280 current loss 0.535062, current_train_items 136992.
I0302 19:00:27.052944 22447428132992 run.py:483] Algo bellman_ford step 4281 current loss 0.722869, current_train_items 137024.
I0302 19:00:27.075984 22447428132992 run.py:483] Algo bellman_ford step 4282 current loss 0.874534, current_train_items 137056.
I0302 19:00:27.105719 22447428132992 run.py:483] Algo bellman_ford step 4283 current loss 0.954158, current_train_items 137088.
I0302 19:00:27.140573 22447428132992 run.py:483] Algo bellman_ford step 4284 current loss 1.291503, current_train_items 137120.
I0302 19:00:27.160259 22447428132992 run.py:483] Algo bellman_ford step 4285 current loss 0.517623, current_train_items 137152.
I0302 19:00:27.176267 22447428132992 run.py:483] Algo bellman_ford step 4286 current loss 0.731264, current_train_items 137184.
I0302 19:00:27.198921 22447428132992 run.py:483] Algo bellman_ford step 4287 current loss 0.904148, current_train_items 137216.
I0302 19:00:27.231126 22447428132992 run.py:483] Algo bellman_ford step 4288 current loss 0.980354, current_train_items 137248.
I0302 19:00:27.264890 22447428132992 run.py:483] Algo bellman_ford step 4289 current loss 1.402514, current_train_items 137280.
I0302 19:00:27.284820 22447428132992 run.py:483] Algo bellman_ford step 4290 current loss 0.599835, current_train_items 137312.
I0302 19:00:27.301323 22447428132992 run.py:483] Algo bellman_ford step 4291 current loss 0.676002, current_train_items 137344.
I0302 19:00:27.325397 22447428132992 run.py:483] Algo bellman_ford step 4292 current loss 0.922257, current_train_items 137376.
I0302 19:00:27.357682 22447428132992 run.py:483] Algo bellman_ford step 4293 current loss 1.109524, current_train_items 137408.
I0302 19:00:27.392709 22447428132992 run.py:483] Algo bellman_ford step 4294 current loss 1.390863, current_train_items 137440.
I0302 19:00:27.411957 22447428132992 run.py:483] Algo bellman_ford step 4295 current loss 0.549740, current_train_items 137472.
I0302 19:00:27.427876 22447428132992 run.py:483] Algo bellman_ford step 4296 current loss 0.808036, current_train_items 137504.
I0302 19:00:27.450434 22447428132992 run.py:483] Algo bellman_ford step 4297 current loss 0.923025, current_train_items 137536.
I0302 19:00:27.481319 22447428132992 run.py:483] Algo bellman_ford step 4298 current loss 1.021023, current_train_items 137568.
I0302 19:00:27.511167 22447428132992 run.py:483] Algo bellman_ford step 4299 current loss 1.176983, current_train_items 137600.
I0302 19:00:27.530853 22447428132992 run.py:483] Algo bellman_ford step 4300 current loss 0.621048, current_train_items 137632.
I0302 19:00:27.538990 22447428132992 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 19:00:27.539094 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0302 19:00:27.555495 22447428132992 run.py:483] Algo bellman_ford step 4301 current loss 0.831045, current_train_items 137664.
I0302 19:00:27.579052 22447428132992 run.py:483] Algo bellman_ford step 4302 current loss 0.998420, current_train_items 137696.
I0302 19:00:27.611487 22447428132992 run.py:483] Algo bellman_ford step 4303 current loss 1.538166, current_train_items 137728.
I0302 19:00:27.647565 22447428132992 run.py:483] Algo bellman_ford step 4304 current loss 1.677265, current_train_items 137760.
I0302 19:00:27.667748 22447428132992 run.py:483] Algo bellman_ford step 4305 current loss 0.506704, current_train_items 137792.
I0302 19:00:27.683567 22447428132992 run.py:483] Algo bellman_ford step 4306 current loss 0.773778, current_train_items 137824.
I0302 19:00:27.708614 22447428132992 run.py:483] Algo bellman_ford step 4307 current loss 1.044465, current_train_items 137856.
I0302 19:00:27.740040 22447428132992 run.py:483] Algo bellman_ford step 4308 current loss 1.146161, current_train_items 137888.
I0302 19:00:27.775805 22447428132992 run.py:483] Algo bellman_ford step 4309 current loss 1.193525, current_train_items 137920.
I0302 19:00:27.795679 22447428132992 run.py:483] Algo bellman_ford step 4310 current loss 0.724567, current_train_items 137952.
I0302 19:00:27.811770 22447428132992 run.py:483] Algo bellman_ford step 4311 current loss 0.825209, current_train_items 137984.
I0302 19:00:27.834091 22447428132992 run.py:483] Algo bellman_ford step 4312 current loss 0.954019, current_train_items 138016.
I0302 19:00:27.865623 22447428132992 run.py:483] Algo bellman_ford step 4313 current loss 1.034824, current_train_items 138048.
I0302 19:00:27.899412 22447428132992 run.py:483] Algo bellman_ford step 4314 current loss 1.249486, current_train_items 138080.
I0302 19:00:27.918963 22447428132992 run.py:483] Algo bellman_ford step 4315 current loss 0.646812, current_train_items 138112.
I0302 19:00:27.934850 22447428132992 run.py:483] Algo bellman_ford step 4316 current loss 0.762309, current_train_items 138144.
I0302 19:00:27.958349 22447428132992 run.py:483] Algo bellman_ford step 4317 current loss 0.929654, current_train_items 138176.
I0302 19:00:27.988362 22447428132992 run.py:483] Algo bellman_ford step 4318 current loss 0.992785, current_train_items 138208.
I0302 19:00:28.021930 22447428132992 run.py:483] Algo bellman_ford step 4319 current loss 1.256513, current_train_items 138240.
I0302 19:00:28.041221 22447428132992 run.py:483] Algo bellman_ford step 4320 current loss 0.559195, current_train_items 138272.
I0302 19:00:28.057269 22447428132992 run.py:483] Algo bellman_ford step 4321 current loss 0.708226, current_train_items 138304.
I0302 19:00:28.080759 22447428132992 run.py:483] Algo bellman_ford step 4322 current loss 0.852615, current_train_items 138336.
I0302 19:00:28.110265 22447428132992 run.py:483] Algo bellman_ford step 4323 current loss 0.970878, current_train_items 138368.
I0302 19:00:28.144204 22447428132992 run.py:483] Algo bellman_ford step 4324 current loss 1.365444, current_train_items 138400.
I0302 19:00:28.163612 22447428132992 run.py:483] Algo bellman_ford step 4325 current loss 0.589523, current_train_items 138432.
I0302 19:00:28.179744 22447428132992 run.py:483] Algo bellman_ford step 4326 current loss 0.766707, current_train_items 138464.
I0302 19:00:28.203115 22447428132992 run.py:483] Algo bellman_ford step 4327 current loss 0.932940, current_train_items 138496.
I0302 19:00:28.235286 22447428132992 run.py:483] Algo bellman_ford step 4328 current loss 1.122517, current_train_items 138528.
I0302 19:00:28.267537 22447428132992 run.py:483] Algo bellman_ford step 4329 current loss 1.129714, current_train_items 138560.
I0302 19:00:28.287088 22447428132992 run.py:483] Algo bellman_ford step 4330 current loss 0.559196, current_train_items 138592.
I0302 19:00:28.303596 22447428132992 run.py:483] Algo bellman_ford step 4331 current loss 0.822958, current_train_items 138624.
I0302 19:00:28.327319 22447428132992 run.py:483] Algo bellman_ford step 4332 current loss 1.019949, current_train_items 138656.
I0302 19:00:28.357430 22447428132992 run.py:483] Algo bellman_ford step 4333 current loss 1.033975, current_train_items 138688.
I0302 19:00:28.388885 22447428132992 run.py:483] Algo bellman_ford step 4334 current loss 1.020721, current_train_items 138720.
I0302 19:00:28.408624 22447428132992 run.py:483] Algo bellman_ford step 4335 current loss 0.672272, current_train_items 138752.
I0302 19:00:28.424647 22447428132992 run.py:483] Algo bellman_ford step 4336 current loss 0.897834, current_train_items 138784.
I0302 19:00:28.448141 22447428132992 run.py:483] Algo bellman_ford step 4337 current loss 1.019673, current_train_items 138816.
I0302 19:00:28.480362 22447428132992 run.py:483] Algo bellman_ford step 4338 current loss 1.144380, current_train_items 138848.
I0302 19:00:28.513040 22447428132992 run.py:483] Algo bellman_ford step 4339 current loss 1.212455, current_train_items 138880.
I0302 19:00:28.532446 22447428132992 run.py:483] Algo bellman_ford step 4340 current loss 0.643159, current_train_items 138912.
I0302 19:00:28.548270 22447428132992 run.py:483] Algo bellman_ford step 4341 current loss 0.706001, current_train_items 138944.
I0302 19:00:28.571709 22447428132992 run.py:483] Algo bellman_ford step 4342 current loss 0.881854, current_train_items 138976.
I0302 19:00:28.602723 22447428132992 run.py:483] Algo bellman_ford step 4343 current loss 1.082035, current_train_items 139008.
I0302 19:00:28.637177 22447428132992 run.py:483] Algo bellman_ford step 4344 current loss 1.190333, current_train_items 139040.
I0302 19:00:28.656678 22447428132992 run.py:483] Algo bellman_ford step 4345 current loss 0.606649, current_train_items 139072.
I0302 19:00:28.673086 22447428132992 run.py:483] Algo bellman_ford step 4346 current loss 0.832905, current_train_items 139104.
I0302 19:00:28.696573 22447428132992 run.py:483] Algo bellman_ford step 4347 current loss 0.967267, current_train_items 139136.
I0302 19:00:28.727796 22447428132992 run.py:483] Algo bellman_ford step 4348 current loss 1.043638, current_train_items 139168.
I0302 19:00:28.760567 22447428132992 run.py:483] Algo bellman_ford step 4349 current loss 1.222142, current_train_items 139200.
I0302 19:00:28.779951 22447428132992 run.py:483] Algo bellman_ford step 4350 current loss 0.668294, current_train_items 139232.
I0302 19:00:28.788171 22447428132992 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 19:00:28.788273 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:00:28.804983 22447428132992 run.py:483] Algo bellman_ford step 4351 current loss 0.737119, current_train_items 139264.
I0302 19:00:28.829241 22447428132992 run.py:483] Algo bellman_ford step 4352 current loss 1.123156, current_train_items 139296.
I0302 19:00:28.859771 22447428132992 run.py:483] Algo bellman_ford step 4353 current loss 1.037523, current_train_items 139328.
I0302 19:00:28.894082 22447428132992 run.py:483] Algo bellman_ford step 4354 current loss 1.155685, current_train_items 139360.
I0302 19:00:28.914003 22447428132992 run.py:483] Algo bellman_ford step 4355 current loss 0.530149, current_train_items 139392.
I0302 19:00:28.929895 22447428132992 run.py:483] Algo bellman_ford step 4356 current loss 0.650431, current_train_items 139424.
I0302 19:00:28.953914 22447428132992 run.py:483] Algo bellman_ford step 4357 current loss 0.969003, current_train_items 139456.
I0302 19:00:28.982831 22447428132992 run.py:483] Algo bellman_ford step 4358 current loss 1.028950, current_train_items 139488.
I0302 19:00:29.015113 22447428132992 run.py:483] Algo bellman_ford step 4359 current loss 1.078776, current_train_items 139520.
I0302 19:00:29.035240 22447428132992 run.py:483] Algo bellman_ford step 4360 current loss 0.612792, current_train_items 139552.
I0302 19:00:29.051917 22447428132992 run.py:483] Algo bellman_ford step 4361 current loss 0.842434, current_train_items 139584.
I0302 19:00:29.074254 22447428132992 run.py:483] Algo bellman_ford step 4362 current loss 0.918411, current_train_items 139616.
I0302 19:00:29.106320 22447428132992 run.py:483] Algo bellman_ford step 4363 current loss 1.114786, current_train_items 139648.
I0302 19:00:29.141466 22447428132992 run.py:483] Algo bellman_ford step 4364 current loss 1.239311, current_train_items 139680.
I0302 19:00:29.161043 22447428132992 run.py:483] Algo bellman_ford step 4365 current loss 0.650757, current_train_items 139712.
I0302 19:00:29.177224 22447428132992 run.py:483] Algo bellman_ford step 4366 current loss 0.889031, current_train_items 139744.
I0302 19:00:29.200330 22447428132992 run.py:483] Algo bellman_ford step 4367 current loss 0.854146, current_train_items 139776.
I0302 19:00:29.229974 22447428132992 run.py:483] Algo bellman_ford step 4368 current loss 0.987682, current_train_items 139808.
I0302 19:00:29.263897 22447428132992 run.py:483] Algo bellman_ford step 4369 current loss 1.151633, current_train_items 139840.
I0302 19:00:29.283610 22447428132992 run.py:483] Algo bellman_ford step 4370 current loss 0.527342, current_train_items 139872.
I0302 19:00:29.299627 22447428132992 run.py:483] Algo bellman_ford step 4371 current loss 0.774758, current_train_items 139904.
I0302 19:00:29.322268 22447428132992 run.py:483] Algo bellman_ford step 4372 current loss 0.985209, current_train_items 139936.
I0302 19:00:29.352396 22447428132992 run.py:483] Algo bellman_ford step 4373 current loss 1.105278, current_train_items 139968.
I0302 19:00:29.385419 22447428132992 run.py:483] Algo bellman_ford step 4374 current loss 1.196366, current_train_items 140000.
I0302 19:00:29.405462 22447428132992 run.py:483] Algo bellman_ford step 4375 current loss 0.594843, current_train_items 140032.
I0302 19:00:29.421509 22447428132992 run.py:483] Algo bellman_ford step 4376 current loss 0.781383, current_train_items 140064.
I0302 19:00:29.444297 22447428132992 run.py:483] Algo bellman_ford step 4377 current loss 0.959870, current_train_items 140096.
I0302 19:00:29.474418 22447428132992 run.py:483] Algo bellman_ford step 4378 current loss 0.955476, current_train_items 140128.
I0302 19:00:29.508010 22447428132992 run.py:483] Algo bellman_ford step 4379 current loss 1.292416, current_train_items 140160.
I0302 19:00:29.527662 22447428132992 run.py:483] Algo bellman_ford step 4380 current loss 0.513593, current_train_items 140192.
I0302 19:00:29.543630 22447428132992 run.py:483] Algo bellman_ford step 4381 current loss 0.930171, current_train_items 140224.
I0302 19:00:29.567111 22447428132992 run.py:483] Algo bellman_ford step 4382 current loss 0.969430, current_train_items 140256.
I0302 19:00:29.597569 22447428132992 run.py:483] Algo bellman_ford step 4383 current loss 0.989310, current_train_items 140288.
I0302 19:00:29.631201 22447428132992 run.py:483] Algo bellman_ford step 4384 current loss 1.394321, current_train_items 140320.
I0302 19:00:29.650959 22447428132992 run.py:483] Algo bellman_ford step 4385 current loss 0.673559, current_train_items 140352.
I0302 19:00:29.666814 22447428132992 run.py:483] Algo bellman_ford step 4386 current loss 0.759520, current_train_items 140384.
I0302 19:00:29.690118 22447428132992 run.py:483] Algo bellman_ford step 4387 current loss 0.999184, current_train_items 140416.
I0302 19:00:29.720853 22447428132992 run.py:483] Algo bellman_ford step 4388 current loss 1.013900, current_train_items 140448.
I0302 19:00:29.752967 22447428132992 run.py:483] Algo bellman_ford step 4389 current loss 1.170873, current_train_items 140480.
I0302 19:00:29.772880 22447428132992 run.py:483] Algo bellman_ford step 4390 current loss 0.594902, current_train_items 140512.
I0302 19:00:29.788862 22447428132992 run.py:483] Algo bellman_ford step 4391 current loss 0.718855, current_train_items 140544.
I0302 19:00:29.812019 22447428132992 run.py:483] Algo bellman_ford step 4392 current loss 0.872985, current_train_items 140576.
I0302 19:00:29.842777 22447428132992 run.py:483] Algo bellman_ford step 4393 current loss 1.157741, current_train_items 140608.
I0302 19:00:29.876777 22447428132992 run.py:483] Algo bellman_ford step 4394 current loss 1.438721, current_train_items 140640.
I0302 19:00:29.896554 22447428132992 run.py:483] Algo bellman_ford step 4395 current loss 0.613830, current_train_items 140672.
I0302 19:00:29.912902 22447428132992 run.py:483] Algo bellman_ford step 4396 current loss 0.723772, current_train_items 140704.
I0302 19:00:29.936656 22447428132992 run.py:483] Algo bellman_ford step 4397 current loss 0.943698, current_train_items 140736.
I0302 19:00:29.967243 22447428132992 run.py:483] Algo bellman_ford step 4398 current loss 1.063455, current_train_items 140768.
I0302 19:00:30.000375 22447428132992 run.py:483] Algo bellman_ford step 4399 current loss 1.205142, current_train_items 140800.
I0302 19:00:30.020269 22447428132992 run.py:483] Algo bellman_ford step 4400 current loss 0.610486, current_train_items 140832.
I0302 19:00:30.028146 22447428132992 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 19:00:30.028251 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.918, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:00:30.045079 22447428132992 run.py:483] Algo bellman_ford step 4401 current loss 0.667137, current_train_items 140864.
I0302 19:00:30.068560 22447428132992 run.py:483] Algo bellman_ford step 4402 current loss 0.917308, current_train_items 140896.
I0302 19:00:30.098932 22447428132992 run.py:483] Algo bellman_ford step 4403 current loss 1.032577, current_train_items 140928.
I0302 19:00:30.137138 22447428132992 run.py:483] Algo bellman_ford step 4404 current loss 1.645937, current_train_items 140960.
I0302 19:00:30.157406 22447428132992 run.py:483] Algo bellman_ford step 4405 current loss 0.566160, current_train_items 140992.
I0302 19:00:30.173343 22447428132992 run.py:483] Algo bellman_ford step 4406 current loss 0.794621, current_train_items 141024.
I0302 19:00:30.196831 22447428132992 run.py:483] Algo bellman_ford step 4407 current loss 1.014055, current_train_items 141056.
I0302 19:00:30.227476 22447428132992 run.py:483] Algo bellman_ford step 4408 current loss 0.900912, current_train_items 141088.
I0302 19:00:30.262729 22447428132992 run.py:483] Algo bellman_ford step 4409 current loss 1.238852, current_train_items 141120.
I0302 19:00:30.282551 22447428132992 run.py:483] Algo bellman_ford step 4410 current loss 0.598113, current_train_items 141152.
I0302 19:00:30.298627 22447428132992 run.py:483] Algo bellman_ford step 4411 current loss 0.758283, current_train_items 141184.
I0302 19:00:30.322352 22447428132992 run.py:483] Algo bellman_ford step 4412 current loss 0.995712, current_train_items 141216.
I0302 19:00:30.354275 22447428132992 run.py:483] Algo bellman_ford step 4413 current loss 1.220499, current_train_items 141248.
I0302 19:00:30.386598 22447428132992 run.py:483] Algo bellman_ford step 4414 current loss 1.383071, current_train_items 141280.
I0302 19:00:30.406038 22447428132992 run.py:483] Algo bellman_ford step 4415 current loss 0.570887, current_train_items 141312.
I0302 19:00:30.422618 22447428132992 run.py:483] Algo bellman_ford step 4416 current loss 0.863717, current_train_items 141344.
I0302 19:00:30.445587 22447428132992 run.py:483] Algo bellman_ford step 4417 current loss 0.943355, current_train_items 141376.
I0302 19:00:30.476047 22447428132992 run.py:483] Algo bellman_ford step 4418 current loss 1.090626, current_train_items 141408.
I0302 19:00:30.509486 22447428132992 run.py:483] Algo bellman_ford step 4419 current loss 1.218096, current_train_items 141440.
I0302 19:00:30.528957 22447428132992 run.py:483] Algo bellman_ford step 4420 current loss 0.562197, current_train_items 141472.
I0302 19:00:30.544821 22447428132992 run.py:483] Algo bellman_ford step 4421 current loss 0.888756, current_train_items 141504.
I0302 19:00:30.567900 22447428132992 run.py:483] Algo bellman_ford step 4422 current loss 1.003923, current_train_items 141536.
I0302 19:00:30.598343 22447428132992 run.py:483] Algo bellman_ford step 4423 current loss 0.961877, current_train_items 141568.
I0302 19:00:30.632644 22447428132992 run.py:483] Algo bellman_ford step 4424 current loss 1.218002, current_train_items 141600.
I0302 19:00:30.652370 22447428132992 run.py:483] Algo bellman_ford step 4425 current loss 0.565726, current_train_items 141632.
I0302 19:00:30.667686 22447428132992 run.py:483] Algo bellman_ford step 4426 current loss 0.656316, current_train_items 141664.
I0302 19:00:30.691459 22447428132992 run.py:483] Algo bellman_ford step 4427 current loss 0.993084, current_train_items 141696.
I0302 19:00:30.722512 22447428132992 run.py:483] Algo bellman_ford step 4428 current loss 0.968944, current_train_items 141728.
I0302 19:00:30.755710 22447428132992 run.py:483] Algo bellman_ford step 4429 current loss 1.150141, current_train_items 141760.
I0302 19:00:30.775205 22447428132992 run.py:483] Algo bellman_ford step 4430 current loss 0.537010, current_train_items 141792.
I0302 19:00:30.790875 22447428132992 run.py:483] Algo bellman_ford step 4431 current loss 0.698118, current_train_items 141824.
I0302 19:00:30.814485 22447428132992 run.py:483] Algo bellman_ford step 4432 current loss 0.894295, current_train_items 141856.
I0302 19:00:30.844105 22447428132992 run.py:483] Algo bellman_ford step 4433 current loss 1.153871, current_train_items 141888.
I0302 19:00:30.877485 22447428132992 run.py:483] Algo bellman_ford step 4434 current loss 1.380465, current_train_items 141920.
I0302 19:00:30.896939 22447428132992 run.py:483] Algo bellman_ford step 4435 current loss 0.637775, current_train_items 141952.
I0302 19:00:30.913103 22447428132992 run.py:483] Algo bellman_ford step 4436 current loss 0.858250, current_train_items 141984.
I0302 19:00:30.937029 22447428132992 run.py:483] Algo bellman_ford step 4437 current loss 1.045667, current_train_items 142016.
I0302 19:00:30.966821 22447428132992 run.py:483] Algo bellman_ford step 4438 current loss 0.886174, current_train_items 142048.
I0302 19:00:31.001528 22447428132992 run.py:483] Algo bellman_ford step 4439 current loss 1.201659, current_train_items 142080.
I0302 19:00:31.020964 22447428132992 run.py:483] Algo bellman_ford step 4440 current loss 0.577892, current_train_items 142112.
I0302 19:00:31.037344 22447428132992 run.py:483] Algo bellman_ford step 4441 current loss 0.904333, current_train_items 142144.
I0302 19:00:31.060307 22447428132992 run.py:483] Algo bellman_ford step 4442 current loss 0.950796, current_train_items 142176.
I0302 19:00:31.090823 22447428132992 run.py:483] Algo bellman_ford step 4443 current loss 1.017191, current_train_items 142208.
I0302 19:00:31.125360 22447428132992 run.py:483] Algo bellman_ford step 4444 current loss 1.197121, current_train_items 142240.
I0302 19:00:31.145058 22447428132992 run.py:483] Algo bellman_ford step 4445 current loss 0.541706, current_train_items 142272.
I0302 19:00:31.160929 22447428132992 run.py:483] Algo bellman_ford step 4446 current loss 0.882089, current_train_items 142304.
I0302 19:00:31.184358 22447428132992 run.py:483] Algo bellman_ford step 4447 current loss 0.904022, current_train_items 142336.
I0302 19:00:31.215721 22447428132992 run.py:483] Algo bellman_ford step 4448 current loss 1.221616, current_train_items 142368.
I0302 19:00:31.249222 22447428132992 run.py:483] Algo bellman_ford step 4449 current loss 1.148826, current_train_items 142400.
I0302 19:00:31.268659 22447428132992 run.py:483] Algo bellman_ford step 4450 current loss 0.624334, current_train_items 142432.
I0302 19:00:31.276855 22447428132992 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 19:00:31.276960 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.918, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:31.305574 22447428132992 run.py:483] Algo bellman_ford step 4451 current loss 0.777213, current_train_items 142464.
I0302 19:00:31.329598 22447428132992 run.py:483] Algo bellman_ford step 4452 current loss 0.993364, current_train_items 142496.
I0302 19:00:31.359689 22447428132992 run.py:483] Algo bellman_ford step 4453 current loss 0.901687, current_train_items 142528.
I0302 19:00:31.392682 22447428132992 run.py:483] Algo bellman_ford step 4454 current loss 1.088736, current_train_items 142560.
I0302 19:00:31.412866 22447428132992 run.py:483] Algo bellman_ford step 4455 current loss 0.584950, current_train_items 142592.
I0302 19:00:31.428680 22447428132992 run.py:483] Algo bellman_ford step 4456 current loss 0.795713, current_train_items 142624.
I0302 19:00:31.452486 22447428132992 run.py:483] Algo bellman_ford step 4457 current loss 0.968051, current_train_items 142656.
I0302 19:00:31.481720 22447428132992 run.py:483] Algo bellman_ford step 4458 current loss 1.002796, current_train_items 142688.
I0302 19:00:31.517987 22447428132992 run.py:483] Algo bellman_ford step 4459 current loss 1.288164, current_train_items 142720.
I0302 19:00:31.538228 22447428132992 run.py:483] Algo bellman_ford step 4460 current loss 0.790316, current_train_items 142752.
I0302 19:00:31.554531 22447428132992 run.py:483] Algo bellman_ford step 4461 current loss 0.859208, current_train_items 142784.
I0302 19:00:31.578620 22447428132992 run.py:483] Algo bellman_ford step 4462 current loss 1.080664, current_train_items 142816.
I0302 19:00:31.607804 22447428132992 run.py:483] Algo bellman_ford step 4463 current loss 0.974676, current_train_items 142848.
I0302 19:00:31.641264 22447428132992 run.py:483] Algo bellman_ford step 4464 current loss 1.245415, current_train_items 142880.
I0302 19:00:31.660989 22447428132992 run.py:483] Algo bellman_ford step 4465 current loss 0.604470, current_train_items 142912.
I0302 19:00:31.677191 22447428132992 run.py:483] Algo bellman_ford step 4466 current loss 0.865600, current_train_items 142944.
I0302 19:00:31.700595 22447428132992 run.py:483] Algo bellman_ford step 4467 current loss 1.071187, current_train_items 142976.
I0302 19:00:31.732934 22447428132992 run.py:483] Algo bellman_ford step 4468 current loss 1.024816, current_train_items 143008.
I0302 19:00:31.767090 22447428132992 run.py:483] Algo bellman_ford step 4469 current loss 1.140821, current_train_items 143040.
I0302 19:00:31.786921 22447428132992 run.py:483] Algo bellman_ford step 4470 current loss 0.644067, current_train_items 143072.
I0302 19:00:31.802894 22447428132992 run.py:483] Algo bellman_ford step 4471 current loss 0.719367, current_train_items 143104.
I0302 19:00:31.825910 22447428132992 run.py:483] Algo bellman_ford step 4472 current loss 0.918428, current_train_items 143136.
I0302 19:00:31.856475 22447428132992 run.py:483] Algo bellman_ford step 4473 current loss 0.995432, current_train_items 143168.
I0302 19:00:31.892682 22447428132992 run.py:483] Algo bellman_ford step 4474 current loss 1.213189, current_train_items 143200.
I0302 19:00:31.912832 22447428132992 run.py:483] Algo bellman_ford step 4475 current loss 0.752844, current_train_items 143232.
I0302 19:00:31.928837 22447428132992 run.py:483] Algo bellman_ford step 4476 current loss 0.781960, current_train_items 143264.
I0302 19:00:31.951809 22447428132992 run.py:483] Algo bellman_ford step 4477 current loss 0.956082, current_train_items 143296.
I0302 19:00:31.982439 22447428132992 run.py:483] Algo bellman_ford step 4478 current loss 0.988854, current_train_items 143328.
I0302 19:00:32.017688 22447428132992 run.py:483] Algo bellman_ford step 4479 current loss 1.340899, current_train_items 143360.
I0302 19:00:32.036964 22447428132992 run.py:483] Algo bellman_ford step 4480 current loss 0.536179, current_train_items 143392.
I0302 19:00:32.053001 22447428132992 run.py:483] Algo bellman_ford step 4481 current loss 0.818846, current_train_items 143424.
I0302 19:00:32.076640 22447428132992 run.py:483] Algo bellman_ford step 4482 current loss 1.009256, current_train_items 143456.
I0302 19:00:32.107701 22447428132992 run.py:483] Algo bellman_ford step 4483 current loss 1.096603, current_train_items 143488.
I0302 19:00:32.138664 22447428132992 run.py:483] Algo bellman_ford step 4484 current loss 1.202546, current_train_items 143520.
I0302 19:00:32.158651 22447428132992 run.py:483] Algo bellman_ford step 4485 current loss 0.558379, current_train_items 143552.
I0302 19:00:32.175006 22447428132992 run.py:483] Algo bellman_ford step 4486 current loss 0.795810, current_train_items 143584.
I0302 19:00:32.199332 22447428132992 run.py:483] Algo bellman_ford step 4487 current loss 1.152674, current_train_items 143616.
I0302 19:00:32.230601 22447428132992 run.py:483] Algo bellman_ford step 4488 current loss 1.004367, current_train_items 143648.
I0302 19:00:32.263467 22447428132992 run.py:483] Algo bellman_ford step 4489 current loss 1.245240, current_train_items 143680.
I0302 19:00:32.283614 22447428132992 run.py:483] Algo bellman_ford step 4490 current loss 0.625697, current_train_items 143712.
I0302 19:00:32.299293 22447428132992 run.py:483] Algo bellman_ford step 4491 current loss 0.736023, current_train_items 143744.
I0302 19:00:32.322758 22447428132992 run.py:483] Algo bellman_ford step 4492 current loss 1.007067, current_train_items 143776.
I0302 19:00:32.353715 22447428132992 run.py:483] Algo bellman_ford step 4493 current loss 1.099203, current_train_items 143808.
I0302 19:00:32.387874 22447428132992 run.py:483] Algo bellman_ford step 4494 current loss 1.162045, current_train_items 143840.
I0302 19:00:32.407500 22447428132992 run.py:483] Algo bellman_ford step 4495 current loss 0.661846, current_train_items 143872.
I0302 19:00:32.423933 22447428132992 run.py:483] Algo bellman_ford step 4496 current loss 0.774575, current_train_items 143904.
I0302 19:00:32.447433 22447428132992 run.py:483] Algo bellman_ford step 4497 current loss 0.983838, current_train_items 143936.
I0302 19:00:32.476638 22447428132992 run.py:483] Algo bellman_ford step 4498 current loss 0.943973, current_train_items 143968.
I0302 19:00:32.508137 22447428132992 run.py:483] Algo bellman_ford step 4499 current loss 1.092692, current_train_items 144000.
I0302 19:00:32.528032 22447428132992 run.py:483] Algo bellman_ford step 4500 current loss 0.587930, current_train_items 144032.
I0302 19:00:32.535919 22447428132992 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 19:00:32.536023 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:32.552572 22447428132992 run.py:483] Algo bellman_ford step 4501 current loss 0.769231, current_train_items 144064.
I0302 19:00:32.576972 22447428132992 run.py:483] Algo bellman_ford step 4502 current loss 1.154262, current_train_items 144096.
I0302 19:00:32.607865 22447428132992 run.py:483] Algo bellman_ford step 4503 current loss 1.041104, current_train_items 144128.
I0302 19:00:32.642318 22447428132992 run.py:483] Algo bellman_ford step 4504 current loss 1.124449, current_train_items 144160.
I0302 19:00:32.662359 22447428132992 run.py:483] Algo bellman_ford step 4505 current loss 0.604702, current_train_items 144192.
I0302 19:00:32.678367 22447428132992 run.py:483] Algo bellman_ford step 4506 current loss 0.847569, current_train_items 144224.
I0302 19:00:32.701269 22447428132992 run.py:483] Algo bellman_ford step 4507 current loss 0.917124, current_train_items 144256.
I0302 19:00:32.731376 22447428132992 run.py:483] Algo bellman_ford step 4508 current loss 0.985242, current_train_items 144288.
I0302 19:00:32.764152 22447428132992 run.py:483] Algo bellman_ford step 4509 current loss 1.070274, current_train_items 144320.
I0302 19:00:32.783986 22447428132992 run.py:483] Algo bellman_ford step 4510 current loss 0.631372, current_train_items 144352.
I0302 19:00:32.800262 22447428132992 run.py:483] Algo bellman_ford step 4511 current loss 0.740724, current_train_items 144384.
I0302 19:00:32.824105 22447428132992 run.py:483] Algo bellman_ford step 4512 current loss 0.915485, current_train_items 144416.
I0302 19:00:32.854619 22447428132992 run.py:483] Algo bellman_ford step 4513 current loss 0.980531, current_train_items 144448.
I0302 19:00:32.887482 22447428132992 run.py:483] Algo bellman_ford step 4514 current loss 1.118242, current_train_items 144480.
I0302 19:00:32.907110 22447428132992 run.py:483] Algo bellman_ford step 4515 current loss 0.609876, current_train_items 144512.
I0302 19:00:32.923405 22447428132992 run.py:483] Algo bellman_ford step 4516 current loss 0.749546, current_train_items 144544.
I0302 19:00:32.946144 22447428132992 run.py:483] Algo bellman_ford step 4517 current loss 0.841111, current_train_items 144576.
I0302 19:00:32.977646 22447428132992 run.py:483] Algo bellman_ford step 4518 current loss 1.013733, current_train_items 144608.
I0302 19:00:33.014443 22447428132992 run.py:483] Algo bellman_ford step 4519 current loss 1.376744, current_train_items 144640.
I0302 19:00:33.033962 22447428132992 run.py:483] Algo bellman_ford step 4520 current loss 0.582143, current_train_items 144672.
I0302 19:00:33.050155 22447428132992 run.py:483] Algo bellman_ford step 4521 current loss 0.771020, current_train_items 144704.
I0302 19:00:33.073349 22447428132992 run.py:483] Algo bellman_ford step 4522 current loss 0.836180, current_train_items 144736.
I0302 19:00:33.105876 22447428132992 run.py:483] Algo bellman_ford step 4523 current loss 1.162377, current_train_items 144768.
I0302 19:00:33.139178 22447428132992 run.py:483] Algo bellman_ford step 4524 current loss 1.071848, current_train_items 144800.
I0302 19:00:33.158724 22447428132992 run.py:483] Algo bellman_ford step 4525 current loss 0.552503, current_train_items 144832.
I0302 19:00:33.174862 22447428132992 run.py:483] Algo bellman_ford step 4526 current loss 0.719925, current_train_items 144864.
I0302 19:00:33.199223 22447428132992 run.py:483] Algo bellman_ford step 4527 current loss 1.059661, current_train_items 144896.
I0302 19:00:33.230269 22447428132992 run.py:483] Algo bellman_ford step 4528 current loss 0.968408, current_train_items 144928.
I0302 19:00:33.264289 22447428132992 run.py:483] Algo bellman_ford step 4529 current loss 1.199273, current_train_items 144960.
I0302 19:00:33.284085 22447428132992 run.py:483] Algo bellman_ford step 4530 current loss 0.623909, current_train_items 144992.
I0302 19:00:33.299941 22447428132992 run.py:483] Algo bellman_ford step 4531 current loss 0.724782, current_train_items 145024.
I0302 19:00:33.322271 22447428132992 run.py:483] Algo bellman_ford step 4532 current loss 0.849020, current_train_items 145056.
I0302 19:00:33.351557 22447428132992 run.py:483] Algo bellman_ford step 4533 current loss 0.904943, current_train_items 145088.
I0302 19:00:33.386334 22447428132992 run.py:483] Algo bellman_ford step 4534 current loss 1.102931, current_train_items 145120.
I0302 19:00:33.406203 22447428132992 run.py:483] Algo bellman_ford step 4535 current loss 0.607829, current_train_items 145152.
I0302 19:00:33.422239 22447428132992 run.py:483] Algo bellman_ford step 4536 current loss 0.766245, current_train_items 145184.
I0302 19:00:33.445687 22447428132992 run.py:483] Algo bellman_ford step 4537 current loss 0.870208, current_train_items 145216.
I0302 19:00:33.476057 22447428132992 run.py:483] Algo bellman_ford step 4538 current loss 0.949877, current_train_items 145248.
I0302 19:00:33.510995 22447428132992 run.py:483] Algo bellman_ford step 4539 current loss 1.310956, current_train_items 145280.
I0302 19:00:33.530645 22447428132992 run.py:483] Algo bellman_ford step 4540 current loss 0.602970, current_train_items 145312.
I0302 19:00:33.546653 22447428132992 run.py:483] Algo bellman_ford step 4541 current loss 0.727868, current_train_items 145344.
I0302 19:00:33.569529 22447428132992 run.py:483] Algo bellman_ford step 4542 current loss 0.949284, current_train_items 145376.
I0302 19:00:33.599648 22447428132992 run.py:483] Algo bellman_ford step 4543 current loss 0.993313, current_train_items 145408.
I0302 19:00:33.633739 22447428132992 run.py:483] Algo bellman_ford step 4544 current loss 1.260468, current_train_items 145440.
I0302 19:00:33.653025 22447428132992 run.py:483] Algo bellman_ford step 4545 current loss 0.687152, current_train_items 145472.
I0302 19:00:33.669017 22447428132992 run.py:483] Algo bellman_ford step 4546 current loss 0.689309, current_train_items 145504.
I0302 19:00:33.692029 22447428132992 run.py:483] Algo bellman_ford step 4547 current loss 0.896454, current_train_items 145536.
I0302 19:00:33.721663 22447428132992 run.py:483] Algo bellman_ford step 4548 current loss 0.962643, current_train_items 145568.
I0302 19:00:33.756163 22447428132992 run.py:483] Algo bellman_ford step 4549 current loss 1.189977, current_train_items 145600.
I0302 19:00:33.775916 22447428132992 run.py:483] Algo bellman_ford step 4550 current loss 0.710705, current_train_items 145632.
I0302 19:00:33.784002 22447428132992 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.876953125, 'score': 0.876953125, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 19:00:33.784107 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.877, val scores are: bellman_ford: 0.877
I0302 19:00:33.801042 22447428132992 run.py:483] Algo bellman_ford step 4551 current loss 0.803421, current_train_items 145664.
I0302 19:00:33.824863 22447428132992 run.py:483] Algo bellman_ford step 4552 current loss 0.919058, current_train_items 145696.
I0302 19:00:33.856614 22447428132992 run.py:483] Algo bellman_ford step 4553 current loss 0.991853, current_train_items 145728.
I0302 19:00:33.890601 22447428132992 run.py:483] Algo bellman_ford step 4554 current loss 1.051865, current_train_items 145760.
I0302 19:00:33.910652 22447428132992 run.py:483] Algo bellman_ford step 4555 current loss 0.621013, current_train_items 145792.
I0302 19:00:33.925983 22447428132992 run.py:483] Algo bellman_ford step 4556 current loss 0.747371, current_train_items 145824.
I0302 19:00:33.948573 22447428132992 run.py:483] Algo bellman_ford step 4557 current loss 0.967011, current_train_items 145856.
I0302 19:00:33.980628 22447428132992 run.py:483] Algo bellman_ford step 4558 current loss 1.005349, current_train_items 145888.
I0302 19:00:34.015680 22447428132992 run.py:483] Algo bellman_ford step 4559 current loss 1.185097, current_train_items 145920.
I0302 19:00:34.035592 22447428132992 run.py:483] Algo bellman_ford step 4560 current loss 0.603181, current_train_items 145952.
I0302 19:00:34.051887 22447428132992 run.py:483] Algo bellman_ford step 4561 current loss 0.751254, current_train_items 145984.
I0302 19:00:34.074732 22447428132992 run.py:483] Algo bellman_ford step 4562 current loss 0.904261, current_train_items 146016.
I0302 19:00:34.104996 22447428132992 run.py:483] Algo bellman_ford step 4563 current loss 0.987251, current_train_items 146048.
I0302 19:00:34.139920 22447428132992 run.py:483] Algo bellman_ford step 4564 current loss 1.186724, current_train_items 146080.
I0302 19:00:34.159758 22447428132992 run.py:483] Algo bellman_ford step 4565 current loss 0.586721, current_train_items 146112.
I0302 19:00:34.175827 22447428132992 run.py:483] Algo bellman_ford step 4566 current loss 0.791173, current_train_items 146144.
I0302 19:00:34.199850 22447428132992 run.py:483] Algo bellman_ford step 4567 current loss 0.995275, current_train_items 146176.
I0302 19:00:34.231211 22447428132992 run.py:483] Algo bellman_ford step 4568 current loss 1.089404, current_train_items 146208.
I0302 19:00:34.263998 22447428132992 run.py:483] Algo bellman_ford step 4569 current loss 1.247002, current_train_items 146240.
I0302 19:00:34.283922 22447428132992 run.py:483] Algo bellman_ford step 4570 current loss 0.646770, current_train_items 146272.
I0302 19:00:34.299944 22447428132992 run.py:483] Algo bellman_ford step 4571 current loss 0.693896, current_train_items 146304.
I0302 19:00:34.323426 22447428132992 run.py:483] Algo bellman_ford step 4572 current loss 0.995419, current_train_items 146336.
I0302 19:00:34.353520 22447428132992 run.py:483] Algo bellman_ford step 4573 current loss 0.961270, current_train_items 146368.
I0302 19:00:34.385983 22447428132992 run.py:483] Algo bellman_ford step 4574 current loss 1.161043, current_train_items 146400.
I0302 19:00:34.405603 22447428132992 run.py:483] Algo bellman_ford step 4575 current loss 0.638200, current_train_items 146432.
I0302 19:00:34.421723 22447428132992 run.py:483] Algo bellman_ford step 4576 current loss 0.797223, current_train_items 146464.
I0302 19:00:34.444915 22447428132992 run.py:483] Algo bellman_ford step 4577 current loss 0.997493, current_train_items 146496.
I0302 19:00:34.475946 22447428132992 run.py:483] Algo bellman_ford step 4578 current loss 1.123841, current_train_items 146528.
I0302 19:00:34.509777 22447428132992 run.py:483] Algo bellman_ford step 4579 current loss 1.155856, current_train_items 146560.
I0302 19:00:34.529317 22447428132992 run.py:483] Algo bellman_ford step 4580 current loss 0.687124, current_train_items 146592.
I0302 19:00:34.545850 22447428132992 run.py:483] Algo bellman_ford step 4581 current loss 0.782537, current_train_items 146624.
I0302 19:00:34.568839 22447428132992 run.py:483] Algo bellman_ford step 4582 current loss 0.942802, current_train_items 146656.
I0302 19:00:34.599853 22447428132992 run.py:483] Algo bellman_ford step 4583 current loss 0.975125, current_train_items 146688.
I0302 19:00:34.633879 22447428132992 run.py:483] Algo bellman_ford step 4584 current loss 1.329038, current_train_items 146720.
I0302 19:00:34.654016 22447428132992 run.py:483] Algo bellman_ford step 4585 current loss 0.652705, current_train_items 146752.
I0302 19:00:34.670422 22447428132992 run.py:483] Algo bellman_ford step 4586 current loss 0.815335, current_train_items 146784.
I0302 19:00:34.692883 22447428132992 run.py:483] Algo bellman_ford step 4587 current loss 0.923369, current_train_items 146816.
I0302 19:00:34.723911 22447428132992 run.py:483] Algo bellman_ford step 4588 current loss 1.102831, current_train_items 146848.
I0302 19:00:34.756868 22447428132992 run.py:483] Algo bellman_ford step 4589 current loss 1.245188, current_train_items 146880.
I0302 19:00:34.776639 22447428132992 run.py:483] Algo bellman_ford step 4590 current loss 0.703085, current_train_items 146912.
I0302 19:00:34.792730 22447428132992 run.py:483] Algo bellman_ford step 4591 current loss 0.863812, current_train_items 146944.
I0302 19:00:34.815481 22447428132992 run.py:483] Algo bellman_ford step 4592 current loss 0.971853, current_train_items 146976.
I0302 19:00:34.847034 22447428132992 run.py:483] Algo bellman_ford step 4593 current loss 1.027290, current_train_items 147008.
I0302 19:00:34.881179 22447428132992 run.py:483] Algo bellman_ford step 4594 current loss 1.257814, current_train_items 147040.
I0302 19:00:34.900615 22447428132992 run.py:483] Algo bellman_ford step 4595 current loss 0.569311, current_train_items 147072.
I0302 19:00:34.916682 22447428132992 run.py:483] Algo bellman_ford step 4596 current loss 0.678907, current_train_items 147104.
I0302 19:00:34.941982 22447428132992 run.py:483] Algo bellman_ford step 4597 current loss 1.153616, current_train_items 147136.
I0302 19:00:34.972437 22447428132992 run.py:483] Algo bellman_ford step 4598 current loss 0.996218, current_train_items 147168.
I0302 19:00:35.004444 22447428132992 run.py:483] Algo bellman_ford step 4599 current loss 1.021685, current_train_items 147200.
I0302 19:00:35.024673 22447428132992 run.py:483] Algo bellman_ford step 4600 current loss 0.651752, current_train_items 147232.
I0302 19:00:35.032345 22447428132992 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 19:00:35.032459 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:35.049365 22447428132992 run.py:483] Algo bellman_ford step 4601 current loss 0.850919, current_train_items 147264.
I0302 19:00:35.073699 22447428132992 run.py:483] Algo bellman_ford step 4602 current loss 1.007503, current_train_items 147296.
I0302 19:00:35.104399 22447428132992 run.py:483] Algo bellman_ford step 4603 current loss 1.079642, current_train_items 147328.
I0302 19:00:35.138839 22447428132992 run.py:483] Algo bellman_ford step 4604 current loss 1.230903, current_train_items 147360.
I0302 19:00:35.158468 22447428132992 run.py:483] Algo bellman_ford step 4605 current loss 0.556484, current_train_items 147392.
I0302 19:00:35.174475 22447428132992 run.py:483] Algo bellman_ford step 4606 current loss 0.910747, current_train_items 147424.
I0302 19:00:35.197275 22447428132992 run.py:483] Algo bellman_ford step 4607 current loss 0.858605, current_train_items 147456.
I0302 19:00:35.228642 22447428132992 run.py:483] Algo bellman_ford step 4608 current loss 1.040248, current_train_items 147488.
I0302 19:00:35.260000 22447428132992 run.py:483] Algo bellman_ford step 4609 current loss 1.378731, current_train_items 147520.
I0302 19:00:35.279198 22447428132992 run.py:483] Algo bellman_ford step 4610 current loss 0.574127, current_train_items 147552.
I0302 19:00:35.295161 22447428132992 run.py:483] Algo bellman_ford step 4611 current loss 0.847057, current_train_items 147584.
I0302 19:00:35.318697 22447428132992 run.py:483] Algo bellman_ford step 4612 current loss 0.877567, current_train_items 147616.
I0302 19:00:35.349564 22447428132992 run.py:483] Algo bellman_ford step 4613 current loss 1.127553, current_train_items 147648.
I0302 19:00:35.383708 22447428132992 run.py:483] Algo bellman_ford step 4614 current loss 1.259969, current_train_items 147680.
I0302 19:00:35.403093 22447428132992 run.py:483] Algo bellman_ford step 4615 current loss 0.625045, current_train_items 147712.
I0302 19:00:35.419213 22447428132992 run.py:483] Algo bellman_ford step 4616 current loss 0.842116, current_train_items 147744.
I0302 19:00:35.442744 22447428132992 run.py:483] Algo bellman_ford step 4617 current loss 0.964814, current_train_items 147776.
I0302 19:00:35.472595 22447428132992 run.py:483] Algo bellman_ford step 4618 current loss 0.991646, current_train_items 147808.
I0302 19:00:35.505801 22447428132992 run.py:483] Algo bellman_ford step 4619 current loss 1.079401, current_train_items 147840.
I0302 19:00:35.524955 22447428132992 run.py:483] Algo bellman_ford step 4620 current loss 0.635507, current_train_items 147872.
I0302 19:00:35.540718 22447428132992 run.py:483] Algo bellman_ford step 4621 current loss 0.642215, current_train_items 147904.
I0302 19:00:35.563719 22447428132992 run.py:483] Algo bellman_ford step 4622 current loss 0.898931, current_train_items 147936.
I0302 19:00:35.594328 22447428132992 run.py:483] Algo bellman_ford step 4623 current loss 0.986257, current_train_items 147968.
I0302 19:00:35.626778 22447428132992 run.py:483] Algo bellman_ford step 4624 current loss 1.493420, current_train_items 148000.
I0302 19:00:35.646044 22447428132992 run.py:483] Algo bellman_ford step 4625 current loss 0.576732, current_train_items 148032.
I0302 19:00:35.662100 22447428132992 run.py:483] Algo bellman_ford step 4626 current loss 0.870418, current_train_items 148064.
I0302 19:00:35.685629 22447428132992 run.py:483] Algo bellman_ford step 4627 current loss 0.969465, current_train_items 148096.
I0302 19:00:35.716825 22447428132992 run.py:483] Algo bellman_ford step 4628 current loss 1.125504, current_train_items 148128.
I0302 19:00:35.751712 22447428132992 run.py:483] Algo bellman_ford step 4629 current loss 1.307708, current_train_items 148160.
I0302 19:00:35.770794 22447428132992 run.py:483] Algo bellman_ford step 4630 current loss 0.584732, current_train_items 148192.
I0302 19:00:35.787065 22447428132992 run.py:483] Algo bellman_ford step 4631 current loss 0.813380, current_train_items 148224.
I0302 19:00:35.811008 22447428132992 run.py:483] Algo bellman_ford step 4632 current loss 1.081574, current_train_items 148256.
I0302 19:00:35.840182 22447428132992 run.py:483] Algo bellman_ford step 4633 current loss 0.958519, current_train_items 148288.
I0302 19:00:35.873892 22447428132992 run.py:483] Algo bellman_ford step 4634 current loss 1.064344, current_train_items 148320.
I0302 19:00:35.893204 22447428132992 run.py:483] Algo bellman_ford step 4635 current loss 0.666670, current_train_items 148352.
I0302 19:00:35.909803 22447428132992 run.py:483] Algo bellman_ford step 4636 current loss 0.809718, current_train_items 148384.
I0302 19:00:35.933217 22447428132992 run.py:483] Algo bellman_ford step 4637 current loss 1.169905, current_train_items 148416.
I0302 19:00:35.965276 22447428132992 run.py:483] Algo bellman_ford step 4638 current loss 1.242702, current_train_items 148448.
I0302 19:00:35.999007 22447428132992 run.py:483] Algo bellman_ford step 4639 current loss 1.158551, current_train_items 148480.
I0302 19:00:36.018191 22447428132992 run.py:483] Algo bellman_ford step 4640 current loss 0.617523, current_train_items 148512.
I0302 19:00:36.034172 22447428132992 run.py:483] Algo bellman_ford step 4641 current loss 0.846473, current_train_items 148544.
I0302 19:00:36.057372 22447428132992 run.py:483] Algo bellman_ford step 4642 current loss 1.014678, current_train_items 148576.
I0302 19:00:36.087990 22447428132992 run.py:483] Algo bellman_ford step 4643 current loss 1.070067, current_train_items 148608.
I0302 19:00:36.121929 22447428132992 run.py:483] Algo bellman_ford step 4644 current loss 1.266977, current_train_items 148640.
I0302 19:00:36.141260 22447428132992 run.py:483] Algo bellman_ford step 4645 current loss 0.635416, current_train_items 148672.
I0302 19:00:36.157340 22447428132992 run.py:483] Algo bellman_ford step 4646 current loss 0.844450, current_train_items 148704.
I0302 19:00:36.179416 22447428132992 run.py:483] Algo bellman_ford step 4647 current loss 0.957517, current_train_items 148736.
I0302 19:00:36.208127 22447428132992 run.py:483] Algo bellman_ford step 4648 current loss 1.116253, current_train_items 148768.
I0302 19:00:36.239479 22447428132992 run.py:483] Algo bellman_ford step 4649 current loss 1.179711, current_train_items 148800.
I0302 19:00:36.258737 22447428132992 run.py:483] Algo bellman_ford step 4650 current loss 0.570984, current_train_items 148832.
I0302 19:00:36.266775 22447428132992 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:36.266880 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:36.283755 22447428132992 run.py:483] Algo bellman_ford step 4651 current loss 0.786612, current_train_items 148864.
I0302 19:00:36.307713 22447428132992 run.py:483] Algo bellman_ford step 4652 current loss 0.955886, current_train_items 148896.
I0302 19:00:36.339957 22447428132992 run.py:483] Algo bellman_ford step 4653 current loss 1.108321, current_train_items 148928.
I0302 19:00:36.374188 22447428132992 run.py:483] Algo bellman_ford step 4654 current loss 1.222542, current_train_items 148960.
I0302 19:00:36.394088 22447428132992 run.py:483] Algo bellman_ford step 4655 current loss 0.640598, current_train_items 148992.
I0302 19:00:36.409974 22447428132992 run.py:483] Algo bellman_ford step 4656 current loss 0.877033, current_train_items 149024.
I0302 19:00:36.434158 22447428132992 run.py:483] Algo bellman_ford step 4657 current loss 0.945393, current_train_items 149056.
I0302 19:00:36.464884 22447428132992 run.py:483] Algo bellman_ford step 4658 current loss 1.031335, current_train_items 149088.
I0302 19:00:36.497275 22447428132992 run.py:483] Algo bellman_ford step 4659 current loss 1.003243, current_train_items 149120.
I0302 19:00:36.517368 22447428132992 run.py:483] Algo bellman_ford step 4660 current loss 0.680026, current_train_items 149152.
I0302 19:00:36.533659 22447428132992 run.py:483] Algo bellman_ford step 4661 current loss 0.932280, current_train_items 149184.
I0302 19:00:36.557774 22447428132992 run.py:483] Algo bellman_ford step 4662 current loss 0.996835, current_train_items 149216.
I0302 19:00:36.588204 22447428132992 run.py:483] Algo bellman_ford step 4663 current loss 1.078832, current_train_items 149248.
I0302 19:00:36.624963 22447428132992 run.py:483] Algo bellman_ford step 4664 current loss 1.401805, current_train_items 149280.
I0302 19:00:36.644618 22447428132992 run.py:483] Algo bellman_ford step 4665 current loss 0.615234, current_train_items 149312.
I0302 19:00:36.660488 22447428132992 run.py:483] Algo bellman_ford step 4666 current loss 0.669183, current_train_items 149344.
I0302 19:00:36.683755 22447428132992 run.py:483] Algo bellman_ford step 4667 current loss 0.906014, current_train_items 149376.
I0302 19:00:36.715542 22447428132992 run.py:483] Algo bellman_ford step 4668 current loss 1.147847, current_train_items 149408.
I0302 19:00:36.749638 22447428132992 run.py:483] Algo bellman_ford step 4669 current loss 1.195760, current_train_items 149440.
I0302 19:00:36.769319 22447428132992 run.py:483] Algo bellman_ford step 4670 current loss 0.550067, current_train_items 149472.
I0302 19:00:36.785065 22447428132992 run.py:483] Algo bellman_ford step 4671 current loss 0.671073, current_train_items 149504.
I0302 19:00:36.807646 22447428132992 run.py:483] Algo bellman_ford step 4672 current loss 0.891454, current_train_items 149536.
I0302 19:00:36.837040 22447428132992 run.py:483] Algo bellman_ford step 4673 current loss 0.995908, current_train_items 149568.
I0302 19:00:36.870505 22447428132992 run.py:483] Algo bellman_ford step 4674 current loss 1.052111, current_train_items 149600.
I0302 19:00:36.890345 22447428132992 run.py:483] Algo bellman_ford step 4675 current loss 0.565715, current_train_items 149632.
I0302 19:00:36.906984 22447428132992 run.py:483] Algo bellman_ford step 4676 current loss 0.801745, current_train_items 149664.
I0302 19:00:36.930505 22447428132992 run.py:483] Algo bellman_ford step 4677 current loss 0.961128, current_train_items 149696.
I0302 19:00:36.962871 22447428132992 run.py:483] Algo bellman_ford step 4678 current loss 1.050015, current_train_items 149728.
I0302 19:00:36.996435 22447428132992 run.py:483] Algo bellman_ford step 4679 current loss 1.102849, current_train_items 149760.
I0302 19:00:37.016107 22447428132992 run.py:483] Algo bellman_ford step 4680 current loss 0.617917, current_train_items 149792.
I0302 19:00:37.031885 22447428132992 run.py:483] Algo bellman_ford step 4681 current loss 0.771248, current_train_items 149824.
I0302 19:00:37.054859 22447428132992 run.py:483] Algo bellman_ford step 4682 current loss 0.859013, current_train_items 149856.
I0302 19:00:37.085180 22447428132992 run.py:483] Algo bellman_ford step 4683 current loss 0.965342, current_train_items 149888.
I0302 19:00:37.119900 22447428132992 run.py:483] Algo bellman_ford step 4684 current loss 1.268339, current_train_items 149920.
I0302 19:00:37.139967 22447428132992 run.py:483] Algo bellman_ford step 4685 current loss 0.583400, current_train_items 149952.
I0302 19:00:37.155772 22447428132992 run.py:483] Algo bellman_ford step 4686 current loss 0.714778, current_train_items 149984.
I0302 19:00:37.179191 22447428132992 run.py:483] Algo bellman_ford step 4687 current loss 0.989397, current_train_items 150016.
I0302 19:00:37.210350 22447428132992 run.py:483] Algo bellman_ford step 4688 current loss 1.144424, current_train_items 150048.
I0302 19:00:37.245175 22447428132992 run.py:483] Algo bellman_ford step 4689 current loss 1.416544, current_train_items 150080.
I0302 19:00:37.265124 22447428132992 run.py:483] Algo bellman_ford step 4690 current loss 0.561983, current_train_items 150112.
I0302 19:00:37.281228 22447428132992 run.py:483] Algo bellman_ford step 4691 current loss 0.683488, current_train_items 150144.
I0302 19:00:37.304032 22447428132992 run.py:483] Algo bellman_ford step 4692 current loss 0.851929, current_train_items 150176.
I0302 19:00:37.335184 22447428132992 run.py:483] Algo bellman_ford step 4693 current loss 1.039098, current_train_items 150208.
I0302 19:00:37.367969 22447428132992 run.py:483] Algo bellman_ford step 4694 current loss 1.222989, current_train_items 150240.
I0302 19:00:37.387848 22447428132992 run.py:483] Algo bellman_ford step 4695 current loss 0.535816, current_train_items 150272.
I0302 19:00:37.403997 22447428132992 run.py:483] Algo bellman_ford step 4696 current loss 0.786277, current_train_items 150304.
I0302 19:00:37.428197 22447428132992 run.py:483] Algo bellman_ford step 4697 current loss 1.015957, current_train_items 150336.
I0302 19:00:37.458795 22447428132992 run.py:483] Algo bellman_ford step 4698 current loss 1.087656, current_train_items 150368.
I0302 19:00:37.490756 22447428132992 run.py:483] Algo bellman_ford step 4699 current loss 1.381209, current_train_items 150400.
I0302 19:00:37.510597 22447428132992 run.py:483] Algo bellman_ford step 4700 current loss 0.689814, current_train_items 150432.
I0302 19:00:37.518425 22447428132992 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:37.518531 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:00:37.534716 22447428132992 run.py:483] Algo bellman_ford step 4701 current loss 0.820676, current_train_items 150464.
I0302 19:00:37.558669 22447428132992 run.py:483] Algo bellman_ford step 4702 current loss 0.976673, current_train_items 150496.
I0302 19:00:37.590625 22447428132992 run.py:483] Algo bellman_ford step 4703 current loss 1.078031, current_train_items 150528.
I0302 19:00:37.625811 22447428132992 run.py:483] Algo bellman_ford step 4704 current loss 1.346580, current_train_items 150560.
I0302 19:00:37.645564 22447428132992 run.py:483] Algo bellman_ford step 4705 current loss 0.728661, current_train_items 150592.
I0302 19:00:37.661082 22447428132992 run.py:483] Algo bellman_ford step 4706 current loss 0.673781, current_train_items 150624.
I0302 19:00:37.684651 22447428132992 run.py:483] Algo bellman_ford step 4707 current loss 1.130549, current_train_items 150656.
I0302 19:00:37.716561 22447428132992 run.py:483] Algo bellman_ford step 4708 current loss 1.130308, current_train_items 150688.
I0302 19:00:37.749880 22447428132992 run.py:483] Algo bellman_ford step 4709 current loss 1.186572, current_train_items 150720.
I0302 19:00:37.769549 22447428132992 run.py:483] Algo bellman_ford step 4710 current loss 0.676826, current_train_items 150752.
I0302 19:00:37.785640 22447428132992 run.py:483] Algo bellman_ford step 4711 current loss 0.803857, current_train_items 150784.
I0302 19:00:37.809466 22447428132992 run.py:483] Algo bellman_ford step 4712 current loss 0.969533, current_train_items 150816.
I0302 19:00:37.840335 22447428132992 run.py:483] Algo bellman_ford step 4713 current loss 1.018851, current_train_items 150848.
I0302 19:00:37.872349 22447428132992 run.py:483] Algo bellman_ford step 4714 current loss 1.222868, current_train_items 150880.
I0302 19:00:37.891489 22447428132992 run.py:483] Algo bellman_ford step 4715 current loss 0.515207, current_train_items 150912.
I0302 19:00:37.907672 22447428132992 run.py:483] Algo bellman_ford step 4716 current loss 0.901132, current_train_items 150944.
I0302 19:00:37.929270 22447428132992 run.py:483] Algo bellman_ford step 4717 current loss 0.863434, current_train_items 150976.
I0302 19:00:37.959674 22447428132992 run.py:483] Algo bellman_ford step 4718 current loss 1.076639, current_train_items 151008.
I0302 19:00:37.995022 22447428132992 run.py:483] Algo bellman_ford step 4719 current loss 1.259278, current_train_items 151040.
I0302 19:00:38.014443 22447428132992 run.py:483] Algo bellman_ford step 4720 current loss 0.601026, current_train_items 151072.
I0302 19:00:38.030245 22447428132992 run.py:483] Algo bellman_ford step 4721 current loss 0.733160, current_train_items 151104.
I0302 19:00:38.054190 22447428132992 run.py:483] Algo bellman_ford step 4722 current loss 0.973497, current_train_items 151136.
I0302 19:00:38.083470 22447428132992 run.py:483] Algo bellman_ford step 4723 current loss 0.970334, current_train_items 151168.
I0302 19:00:38.117315 22447428132992 run.py:483] Algo bellman_ford step 4724 current loss 1.130116, current_train_items 151200.
I0302 19:00:38.136512 22447428132992 run.py:483] Algo bellman_ford step 4725 current loss 0.614600, current_train_items 151232.
I0302 19:00:38.152789 22447428132992 run.py:483] Algo bellman_ford step 4726 current loss 0.763607, current_train_items 151264.
I0302 19:00:38.177062 22447428132992 run.py:483] Algo bellman_ford step 4727 current loss 1.076501, current_train_items 151296.
I0302 19:00:38.206111 22447428132992 run.py:483] Algo bellman_ford step 4728 current loss 0.974952, current_train_items 151328.
I0302 19:00:38.235712 22447428132992 run.py:483] Algo bellman_ford step 4729 current loss 0.960543, current_train_items 151360.
I0302 19:00:38.255263 22447428132992 run.py:483] Algo bellman_ford step 4730 current loss 0.576561, current_train_items 151392.
I0302 19:00:38.271040 22447428132992 run.py:483] Algo bellman_ford step 4731 current loss 0.661221, current_train_items 151424.
I0302 19:00:38.293521 22447428132992 run.py:483] Algo bellman_ford step 4732 current loss 0.847778, current_train_items 151456.
I0302 19:00:38.323750 22447428132992 run.py:483] Algo bellman_ford step 4733 current loss 1.056927, current_train_items 151488.
I0302 19:00:38.357462 22447428132992 run.py:483] Algo bellman_ford step 4734 current loss 1.492898, current_train_items 151520.
I0302 19:00:38.376533 22447428132992 run.py:483] Algo bellman_ford step 4735 current loss 0.613222, current_train_items 151552.
I0302 19:00:38.392920 22447428132992 run.py:483] Algo bellman_ford step 4736 current loss 0.827848, current_train_items 151584.
I0302 19:00:38.415881 22447428132992 run.py:483] Algo bellman_ford step 4737 current loss 1.020131, current_train_items 151616.
I0302 19:00:38.446126 22447428132992 run.py:483] Algo bellman_ford step 4738 current loss 1.057727, current_train_items 151648.
I0302 19:00:38.478586 22447428132992 run.py:483] Algo bellman_ford step 4739 current loss 1.347111, current_train_items 151680.
I0302 19:00:38.497903 22447428132992 run.py:483] Algo bellman_ford step 4740 current loss 0.604327, current_train_items 151712.
I0302 19:00:38.513739 22447428132992 run.py:483] Algo bellman_ford step 4741 current loss 0.760839, current_train_items 151744.
I0302 19:00:38.536111 22447428132992 run.py:483] Algo bellman_ford step 4742 current loss 0.852386, current_train_items 151776.
I0302 19:00:38.567580 22447428132992 run.py:483] Algo bellman_ford step 4743 current loss 1.116054, current_train_items 151808.
I0302 19:00:38.599455 22447428132992 run.py:483] Algo bellman_ford step 4744 current loss 1.122239, current_train_items 151840.
I0302 19:00:38.619144 22447428132992 run.py:483] Algo bellman_ford step 4745 current loss 0.684105, current_train_items 151872.
I0302 19:00:38.635623 22447428132992 run.py:483] Algo bellman_ford step 4746 current loss 0.833391, current_train_items 151904.
I0302 19:00:38.660073 22447428132992 run.py:483] Algo bellman_ford step 4747 current loss 0.903354, current_train_items 151936.
I0302 19:00:38.690387 22447428132992 run.py:483] Algo bellman_ford step 4748 current loss 1.033419, current_train_items 151968.
I0302 19:00:38.723603 22447428132992 run.py:483] Algo bellman_ford step 4749 current loss 1.143790, current_train_items 152000.
I0302 19:00:38.742890 22447428132992 run.py:483] Algo bellman_ford step 4750 current loss 0.552365, current_train_items 152032.
I0302 19:00:38.750889 22447428132992 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:38.750993 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:00:38.767435 22447428132992 run.py:483] Algo bellman_ford step 4751 current loss 0.831885, current_train_items 152064.
I0302 19:00:38.791604 22447428132992 run.py:483] Algo bellman_ford step 4752 current loss 1.046366, current_train_items 152096.
I0302 19:00:38.824682 22447428132992 run.py:483] Algo bellman_ford step 4753 current loss 1.077662, current_train_items 152128.
I0302 19:00:38.857984 22447428132992 run.py:483] Algo bellman_ford step 4754 current loss 1.050484, current_train_items 152160.
I0302 19:00:38.877817 22447428132992 run.py:483] Algo bellman_ford step 4755 current loss 1.103607, current_train_items 152192.
I0302 19:00:38.893588 22447428132992 run.py:483] Algo bellman_ford step 4756 current loss 0.892089, current_train_items 152224.
I0302 19:00:38.917310 22447428132992 run.py:483] Algo bellman_ford step 4757 current loss 0.921814, current_train_items 152256.
I0302 19:00:38.948544 22447428132992 run.py:483] Algo bellman_ford step 4758 current loss 1.000376, current_train_items 152288.
I0302 19:00:38.982687 22447428132992 run.py:483] Algo bellman_ford step 4759 current loss 1.326096, current_train_items 152320.
I0302 19:00:39.002442 22447428132992 run.py:483] Algo bellman_ford step 4760 current loss 0.781962, current_train_items 152352.
I0302 19:00:39.018746 22447428132992 run.py:483] Algo bellman_ford step 4761 current loss 0.784388, current_train_items 152384.
I0302 19:00:39.042078 22447428132992 run.py:483] Algo bellman_ford step 4762 current loss 0.943368, current_train_items 152416.
I0302 19:00:39.072511 22447428132992 run.py:483] Algo bellman_ford step 4763 current loss 1.087168, current_train_items 152448.
I0302 19:00:39.106580 22447428132992 run.py:483] Algo bellman_ford step 4764 current loss 1.191416, current_train_items 152480.
I0302 19:00:39.126127 22447428132992 run.py:483] Algo bellman_ford step 4765 current loss 0.527879, current_train_items 152512.
I0302 19:00:39.141760 22447428132992 run.py:483] Algo bellman_ford step 4766 current loss 0.698737, current_train_items 152544.
I0302 19:00:39.165987 22447428132992 run.py:483] Algo bellman_ford step 4767 current loss 1.058594, current_train_items 152576.
I0302 19:00:39.195698 22447428132992 run.py:483] Algo bellman_ford step 4768 current loss 1.060321, current_train_items 152608.
I0302 19:00:39.228292 22447428132992 run.py:483] Algo bellman_ford step 4769 current loss 1.228765, current_train_items 152640.
I0302 19:00:39.247962 22447428132992 run.py:483] Algo bellman_ford step 4770 current loss 0.648244, current_train_items 152672.
I0302 19:00:39.263943 22447428132992 run.py:483] Algo bellman_ford step 4771 current loss 0.736681, current_train_items 152704.
I0302 19:00:39.286376 22447428132992 run.py:483] Algo bellman_ford step 4772 current loss 0.899928, current_train_items 152736.
I0302 19:00:39.316851 22447428132992 run.py:483] Algo bellman_ford step 4773 current loss 1.132229, current_train_items 152768.
I0302 19:00:39.351103 22447428132992 run.py:483] Algo bellman_ford step 4774 current loss 1.272955, current_train_items 152800.
I0302 19:00:39.370979 22447428132992 run.py:483] Algo bellman_ford step 4775 current loss 0.746676, current_train_items 152832.
I0302 19:00:39.387088 22447428132992 run.py:483] Algo bellman_ford step 4776 current loss 0.787434, current_train_items 152864.
I0302 19:00:39.410294 22447428132992 run.py:483] Algo bellman_ford step 4777 current loss 0.902357, current_train_items 152896.
I0302 19:00:39.441371 22447428132992 run.py:483] Algo bellman_ford step 4778 current loss 1.030383, current_train_items 152928.
I0302 19:00:39.473268 22447428132992 run.py:483] Algo bellman_ford step 4779 current loss 1.087504, current_train_items 152960.
I0302 19:00:39.492987 22447428132992 run.py:483] Algo bellman_ford step 4780 current loss 0.533730, current_train_items 152992.
I0302 19:00:39.509090 22447428132992 run.py:483] Algo bellman_ford step 4781 current loss 0.754430, current_train_items 153024.
I0302 19:00:39.532926 22447428132992 run.py:483] Algo bellman_ford step 4782 current loss 1.011391, current_train_items 153056.
I0302 19:00:39.564101 22447428132992 run.py:483] Algo bellman_ford step 4783 current loss 1.116511, current_train_items 153088.
I0302 19:00:39.598575 22447428132992 run.py:483] Algo bellman_ford step 4784 current loss 1.389053, current_train_items 153120.
I0302 19:00:39.618503 22447428132992 run.py:483] Algo bellman_ford step 4785 current loss 0.763306, current_train_items 153152.
I0302 19:00:39.634938 22447428132992 run.py:483] Algo bellman_ford step 4786 current loss 0.773218, current_train_items 153184.
I0302 19:00:39.658705 22447428132992 run.py:483] Algo bellman_ford step 4787 current loss 0.955067, current_train_items 153216.
I0302 19:00:39.688946 22447428132992 run.py:483] Algo bellman_ford step 4788 current loss 1.050342, current_train_items 153248.
I0302 19:00:39.721176 22447428132992 run.py:483] Algo bellman_ford step 4789 current loss 1.377708, current_train_items 153280.
I0302 19:00:39.740495 22447428132992 run.py:483] Algo bellman_ford step 4790 current loss 0.592708, current_train_items 153312.
I0302 19:00:39.756638 22447428132992 run.py:483] Algo bellman_ford step 4791 current loss 0.767531, current_train_items 153344.
I0302 19:00:39.780210 22447428132992 run.py:483] Algo bellman_ford step 4792 current loss 0.966044, current_train_items 153376.
I0302 19:00:39.810488 22447428132992 run.py:483] Algo bellman_ford step 4793 current loss 1.085221, current_train_items 153408.
I0302 19:00:39.846204 22447428132992 run.py:483] Algo bellman_ford step 4794 current loss 1.176767, current_train_items 153440.
I0302 19:00:39.865503 22447428132992 run.py:483] Algo bellman_ford step 4795 current loss 0.538427, current_train_items 153472.
I0302 19:00:39.881599 22447428132992 run.py:483] Algo bellman_ford step 4796 current loss 0.783473, current_train_items 153504.
I0302 19:00:39.905519 22447428132992 run.py:483] Algo bellman_ford step 4797 current loss 1.063093, current_train_items 153536.
I0302 19:00:39.935756 22447428132992 run.py:483] Algo bellman_ford step 4798 current loss 0.969208, current_train_items 153568.
I0302 19:00:39.968708 22447428132992 run.py:483] Algo bellman_ford step 4799 current loss 1.198337, current_train_items 153600.
I0302 19:00:39.988466 22447428132992 run.py:483] Algo bellman_ford step 4800 current loss 0.553997, current_train_items 153632.
I0302 19:00:39.996223 22447428132992 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.8583984375, 'score': 0.8583984375, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:39.996329 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.858, val scores are: bellman_ford: 0.858
I0302 19:00:40.012930 22447428132992 run.py:483] Algo bellman_ford step 4801 current loss 0.861101, current_train_items 153664.
I0302 19:00:40.036786 22447428132992 run.py:483] Algo bellman_ford step 4802 current loss 1.040168, current_train_items 153696.
I0302 19:00:40.067433 22447428132992 run.py:483] Algo bellman_ford step 4803 current loss 1.046686, current_train_items 153728.
I0302 19:00:40.104094 22447428132992 run.py:483] Algo bellman_ford step 4804 current loss 1.376422, current_train_items 153760.
I0302 19:00:40.124129 22447428132992 run.py:483] Algo bellman_ford step 4805 current loss 0.628221, current_train_items 153792.
I0302 19:00:40.140438 22447428132992 run.py:483] Algo bellman_ford step 4806 current loss 0.836683, current_train_items 153824.
I0302 19:00:40.164000 22447428132992 run.py:483] Algo bellman_ford step 4807 current loss 0.981218, current_train_items 153856.
I0302 19:00:40.194873 22447428132992 run.py:483] Algo bellman_ford step 4808 current loss 1.072204, current_train_items 153888.
I0302 19:00:40.228371 22447428132992 run.py:483] Algo bellman_ford step 4809 current loss 1.173284, current_train_items 153920.
I0302 19:00:40.248287 22447428132992 run.py:483] Algo bellman_ford step 4810 current loss 0.555878, current_train_items 153952.
I0302 19:00:40.264412 22447428132992 run.py:483] Algo bellman_ford step 4811 current loss 0.835728, current_train_items 153984.
I0302 19:00:40.287626 22447428132992 run.py:483] Algo bellman_ford step 4812 current loss 0.993478, current_train_items 154016.
I0302 19:00:40.317937 22447428132992 run.py:483] Algo bellman_ford step 4813 current loss 1.104839, current_train_items 154048.
I0302 19:00:40.352628 22447428132992 run.py:483] Algo bellman_ford step 4814 current loss 1.278595, current_train_items 154080.
I0302 19:00:40.372406 22447428132992 run.py:483] Algo bellman_ford step 4815 current loss 0.536061, current_train_items 154112.
I0302 19:00:40.388487 22447428132992 run.py:483] Algo bellman_ford step 4816 current loss 0.812834, current_train_items 154144.
I0302 19:00:40.413315 22447428132992 run.py:483] Algo bellman_ford step 4817 current loss 1.076962, current_train_items 154176.
I0302 19:00:40.444130 22447428132992 run.py:483] Algo bellman_ford step 4818 current loss 0.963099, current_train_items 154208.
I0302 19:00:40.473715 22447428132992 run.py:483] Algo bellman_ford step 4819 current loss 1.048939, current_train_items 154240.
I0302 19:00:40.493621 22447428132992 run.py:483] Algo bellman_ford step 4820 current loss 0.547721, current_train_items 154272.
I0302 19:00:40.509577 22447428132992 run.py:483] Algo bellman_ford step 4821 current loss 0.730303, current_train_items 154304.
I0302 19:00:40.533225 22447428132992 run.py:483] Algo bellman_ford step 4822 current loss 0.927074, current_train_items 154336.
I0302 19:00:40.563720 22447428132992 run.py:483] Algo bellman_ford step 4823 current loss 1.069200, current_train_items 154368.
I0302 19:00:40.597874 22447428132992 run.py:483] Algo bellman_ford step 4824 current loss 1.179688, current_train_items 154400.
I0302 19:00:40.617435 22447428132992 run.py:483] Algo bellman_ford step 4825 current loss 0.604827, current_train_items 154432.
I0302 19:00:40.634102 22447428132992 run.py:483] Algo bellman_ford step 4826 current loss 0.708443, current_train_items 154464.
I0302 19:00:40.657180 22447428132992 run.py:483] Algo bellman_ford step 4827 current loss 0.896189, current_train_items 154496.
I0302 19:00:40.688266 22447428132992 run.py:483] Algo bellman_ford step 4828 current loss 0.961266, current_train_items 154528.
I0302 19:00:40.722503 22447428132992 run.py:483] Algo bellman_ford step 4829 current loss 1.077040, current_train_items 154560.
I0302 19:00:40.742151 22447428132992 run.py:483] Algo bellman_ford step 4830 current loss 0.540166, current_train_items 154592.
I0302 19:00:40.758402 22447428132992 run.py:483] Algo bellman_ford step 4831 current loss 0.742886, current_train_items 154624.
I0302 19:00:40.782529 22447428132992 run.py:483] Algo bellman_ford step 4832 current loss 0.987399, current_train_items 154656.
I0302 19:00:40.814028 22447428132992 run.py:483] Algo bellman_ford step 4833 current loss 1.120114, current_train_items 154688.
I0302 19:00:40.847145 22447428132992 run.py:483] Algo bellman_ford step 4834 current loss 1.058588, current_train_items 154720.
I0302 19:00:40.866269 22447428132992 run.py:483] Algo bellman_ford step 4835 current loss 0.533611, current_train_items 154752.
I0302 19:00:40.882344 22447428132992 run.py:483] Algo bellman_ford step 4836 current loss 0.767460, current_train_items 154784.
I0302 19:00:40.904561 22447428132992 run.py:483] Algo bellman_ford step 4837 current loss 0.903557, current_train_items 154816.
I0302 19:00:40.935394 22447428132992 run.py:483] Algo bellman_ford step 4838 current loss 0.957702, current_train_items 154848.
I0302 19:00:40.966759 22447428132992 run.py:483] Algo bellman_ford step 4839 current loss 1.131113, current_train_items 154880.
I0302 19:00:40.986513 22447428132992 run.py:483] Algo bellman_ford step 4840 current loss 0.739918, current_train_items 154912.
I0302 19:00:41.002471 22447428132992 run.py:483] Algo bellman_ford step 4841 current loss 0.709501, current_train_items 154944.
I0302 19:00:41.025622 22447428132992 run.py:483] Algo bellman_ford step 4842 current loss 0.938198, current_train_items 154976.
I0302 19:00:41.055601 22447428132992 run.py:483] Algo bellman_ford step 4843 current loss 1.023371, current_train_items 155008.
I0302 19:00:41.090144 22447428132992 run.py:483] Algo bellman_ford step 4844 current loss 1.373866, current_train_items 155040.
I0302 19:00:41.109907 22447428132992 run.py:483] Algo bellman_ford step 4845 current loss 0.579370, current_train_items 155072.
I0302 19:00:41.126207 22447428132992 run.py:483] Algo bellman_ford step 4846 current loss 0.764584, current_train_items 155104.
I0302 19:00:41.148936 22447428132992 run.py:483] Algo bellman_ford step 4847 current loss 0.957386, current_train_items 155136.
I0302 19:00:41.178849 22447428132992 run.py:483] Algo bellman_ford step 4848 current loss 1.035649, current_train_items 155168.
I0302 19:00:41.212167 22447428132992 run.py:483] Algo bellman_ford step 4849 current loss 1.119448, current_train_items 155200.
I0302 19:00:41.231959 22447428132992 run.py:483] Algo bellman_ford step 4850 current loss 0.645787, current_train_items 155232.
I0302 19:00:41.240450 22447428132992 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:41.240565 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:00:41.257554 22447428132992 run.py:483] Algo bellman_ford step 4851 current loss 0.807418, current_train_items 155264.
I0302 19:00:41.279972 22447428132992 run.py:483] Algo bellman_ford step 4852 current loss 0.927331, current_train_items 155296.
I0302 19:00:41.310387 22447428132992 run.py:483] Algo bellman_ford step 4853 current loss 1.038987, current_train_items 155328.
I0302 19:00:41.343958 22447428132992 run.py:483] Algo bellman_ford step 4854 current loss 1.163339, current_train_items 155360.
I0302 19:00:41.363620 22447428132992 run.py:483] Algo bellman_ford step 4855 current loss 0.541193, current_train_items 155392.
I0302 19:00:41.379938 22447428132992 run.py:483] Algo bellman_ford step 4856 current loss 0.902842, current_train_items 155424.
I0302 19:00:41.404337 22447428132992 run.py:483] Algo bellman_ford step 4857 current loss 1.035793, current_train_items 155456.
I0302 19:00:41.435513 22447428132992 run.py:483] Algo bellman_ford step 4858 current loss 1.013322, current_train_items 155488.
I0302 19:00:41.471075 22447428132992 run.py:483] Algo bellman_ford step 4859 current loss 1.239172, current_train_items 155520.
I0302 19:00:41.490614 22447428132992 run.py:483] Algo bellman_ford step 4860 current loss 0.557195, current_train_items 155552.
I0302 19:00:41.507607 22447428132992 run.py:483] Algo bellman_ford step 4861 current loss 0.820134, current_train_items 155584.
I0302 19:00:41.530393 22447428132992 run.py:483] Algo bellman_ford step 4862 current loss 1.012021, current_train_items 155616.
I0302 19:00:41.561543 22447428132992 run.py:483] Algo bellman_ford step 4863 current loss 0.998861, current_train_items 155648.
I0302 19:00:41.593769 22447428132992 run.py:483] Algo bellman_ford step 4864 current loss 1.066210, current_train_items 155680.
I0302 19:00:41.612878 22447428132992 run.py:483] Algo bellman_ford step 4865 current loss 0.526476, current_train_items 155712.
I0302 19:00:41.628671 22447428132992 run.py:483] Algo bellman_ford step 4866 current loss 0.810434, current_train_items 155744.
I0302 19:00:41.652416 22447428132992 run.py:483] Algo bellman_ford step 4867 current loss 1.075801, current_train_items 155776.
I0302 19:00:41.684128 22447428132992 run.py:483] Algo bellman_ford step 4868 current loss 1.115786, current_train_items 155808.
I0302 19:00:41.716346 22447428132992 run.py:483] Algo bellman_ford step 4869 current loss 1.094426, current_train_items 155840.
I0302 19:00:41.736587 22447428132992 run.py:483] Algo bellman_ford step 4870 current loss 0.575228, current_train_items 155872.
I0302 19:00:41.752713 22447428132992 run.py:483] Algo bellman_ford step 4871 current loss 0.833324, current_train_items 155904.
I0302 19:00:41.775632 22447428132992 run.py:483] Algo bellman_ford step 4872 current loss 0.927127, current_train_items 155936.
I0302 19:00:41.804403 22447428132992 run.py:483] Algo bellman_ford step 4873 current loss 0.932414, current_train_items 155968.
I0302 19:00:41.836287 22447428132992 run.py:483] Algo bellman_ford step 4874 current loss 1.154243, current_train_items 156000.
I0302 19:00:41.856045 22447428132992 run.py:483] Algo bellman_ford step 4875 current loss 0.641671, current_train_items 156032.
I0302 19:00:41.871978 22447428132992 run.py:483] Algo bellman_ford step 4876 current loss 0.718260, current_train_items 156064.
I0302 19:00:41.894126 22447428132992 run.py:483] Algo bellman_ford step 4877 current loss 0.884032, current_train_items 156096.
I0302 19:00:41.923774 22447428132992 run.py:483] Algo bellman_ford step 4878 current loss 1.083571, current_train_items 156128.
I0302 19:00:41.957276 22447428132992 run.py:483] Algo bellman_ford step 4879 current loss 1.050599, current_train_items 156160.
I0302 19:00:41.976708 22447428132992 run.py:483] Algo bellman_ford step 4880 current loss 0.679261, current_train_items 156192.
I0302 19:00:41.992647 22447428132992 run.py:483] Algo bellman_ford step 4881 current loss 0.796005, current_train_items 156224.
I0302 19:00:42.016407 22447428132992 run.py:483] Algo bellman_ford step 4882 current loss 0.893034, current_train_items 156256.
I0302 19:00:42.047307 22447428132992 run.py:483] Algo bellman_ford step 4883 current loss 1.102682, current_train_items 156288.
I0302 19:00:42.081282 22447428132992 run.py:483] Algo bellman_ford step 4884 current loss 1.226206, current_train_items 156320.
I0302 19:00:42.101157 22447428132992 run.py:483] Algo bellman_ford step 4885 current loss 0.628211, current_train_items 156352.
I0302 19:00:42.117614 22447428132992 run.py:483] Algo bellman_ford step 4886 current loss 0.873585, current_train_items 156384.
I0302 19:00:42.140537 22447428132992 run.py:483] Algo bellman_ford step 4887 current loss 1.031989, current_train_items 156416.
I0302 19:00:42.170810 22447428132992 run.py:483] Algo bellman_ford step 4888 current loss 1.006936, current_train_items 156448.
I0302 19:00:42.203480 22447428132992 run.py:483] Algo bellman_ford step 4889 current loss 1.211961, current_train_items 156480.
I0302 19:00:42.223459 22447428132992 run.py:483] Algo bellman_ford step 4890 current loss 0.592544, current_train_items 156512.
I0302 19:00:42.239513 22447428132992 run.py:483] Algo bellman_ford step 4891 current loss 0.671950, current_train_items 156544.
I0302 19:00:42.262312 22447428132992 run.py:483] Algo bellman_ford step 4892 current loss 0.870867, current_train_items 156576.
I0302 19:00:42.291886 22447428132992 run.py:483] Algo bellman_ford step 4893 current loss 1.028907, current_train_items 156608.
I0302 19:00:42.324178 22447428132992 run.py:483] Algo bellman_ford step 4894 current loss 1.194062, current_train_items 156640.
I0302 19:00:42.343364 22447428132992 run.py:483] Algo bellman_ford step 4895 current loss 0.573949, current_train_items 156672.
I0302 19:00:42.359542 22447428132992 run.py:483] Algo bellman_ford step 4896 current loss 0.807459, current_train_items 156704.
I0302 19:00:42.381859 22447428132992 run.py:483] Algo bellman_ford step 4897 current loss 0.888821, current_train_items 156736.
I0302 19:00:42.411724 22447428132992 run.py:483] Algo bellman_ford step 4898 current loss 1.029337, current_train_items 156768.
I0302 19:00:42.445778 22447428132992 run.py:483] Algo bellman_ford step 4899 current loss 1.209476, current_train_items 156800.
I0302 19:00:42.465476 22447428132992 run.py:483] Algo bellman_ford step 4900 current loss 0.628400, current_train_items 156832.
I0302 19:00:42.473139 22447428132992 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:42.473284 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:00:42.489469 22447428132992 run.py:483] Algo bellman_ford step 4901 current loss 0.763378, current_train_items 156864.
I0302 19:00:42.513332 22447428132992 run.py:483] Algo bellman_ford step 4902 current loss 0.956056, current_train_items 156896.
I0302 19:00:42.545988 22447428132992 run.py:483] Algo bellman_ford step 4903 current loss 1.032684, current_train_items 156928.
I0302 19:00:42.579141 22447428132992 run.py:483] Algo bellman_ford step 4904 current loss 1.173280, current_train_items 156960.
I0302 19:00:42.598903 22447428132992 run.py:483] Algo bellman_ford step 4905 current loss 0.695351, current_train_items 156992.
I0302 19:00:42.614630 22447428132992 run.py:483] Algo bellman_ford step 4906 current loss 0.826348, current_train_items 157024.
I0302 19:00:42.638192 22447428132992 run.py:483] Algo bellman_ford step 4907 current loss 0.880759, current_train_items 157056.
I0302 19:00:42.668879 22447428132992 run.py:483] Algo bellman_ford step 4908 current loss 1.002664, current_train_items 157088.
I0302 19:00:42.701610 22447428132992 run.py:483] Algo bellman_ford step 4909 current loss 1.230217, current_train_items 157120.
I0302 19:00:42.720931 22447428132992 run.py:483] Algo bellman_ford step 4910 current loss 0.616250, current_train_items 157152.
I0302 19:00:42.737136 22447428132992 run.py:483] Algo bellman_ford step 4911 current loss 0.777903, current_train_items 157184.
I0302 19:00:42.760333 22447428132992 run.py:483] Algo bellman_ford step 4912 current loss 0.904348, current_train_items 157216.
I0302 19:00:42.791792 22447428132992 run.py:483] Algo bellman_ford step 4913 current loss 1.140704, current_train_items 157248.
I0302 19:00:42.824541 22447428132992 run.py:483] Algo bellman_ford step 4914 current loss 1.071765, current_train_items 157280.
I0302 19:00:42.843804 22447428132992 run.py:483] Algo bellman_ford step 4915 current loss 0.710239, current_train_items 157312.
I0302 19:00:42.859576 22447428132992 run.py:483] Algo bellman_ford step 4916 current loss 0.717142, current_train_items 157344.
I0302 19:00:42.882708 22447428132992 run.py:483] Algo bellman_ford step 4917 current loss 0.902825, current_train_items 157376.
I0302 19:00:42.912053 22447428132992 run.py:483] Algo bellman_ford step 4918 current loss 1.020522, current_train_items 157408.
I0302 19:00:42.945122 22447428132992 run.py:483] Algo bellman_ford step 4919 current loss 1.156123, current_train_items 157440.
I0302 19:00:42.964356 22447428132992 run.py:483] Algo bellman_ford step 4920 current loss 0.550359, current_train_items 157472.
I0302 19:00:42.979820 22447428132992 run.py:483] Algo bellman_ford step 4921 current loss 0.694054, current_train_items 157504.
I0302 19:00:43.003310 22447428132992 run.py:483] Algo bellman_ford step 4922 current loss 0.904472, current_train_items 157536.
I0302 19:00:43.033674 22447428132992 run.py:483] Algo bellman_ford step 4923 current loss 1.004608, current_train_items 157568.
I0302 19:00:43.066750 22447428132992 run.py:483] Algo bellman_ford step 4924 current loss 1.341418, current_train_items 157600.
I0302 19:00:43.086202 22447428132992 run.py:483] Algo bellman_ford step 4925 current loss 0.518164, current_train_items 157632.
I0302 19:00:43.102361 22447428132992 run.py:483] Algo bellman_ford step 4926 current loss 0.827858, current_train_items 157664.
I0302 19:00:43.126055 22447428132992 run.py:483] Algo bellman_ford step 4927 current loss 0.997802, current_train_items 157696.
I0302 19:00:43.155468 22447428132992 run.py:483] Algo bellman_ford step 4928 current loss 1.015777, current_train_items 157728.
I0302 19:00:43.188788 22447428132992 run.py:483] Algo bellman_ford step 4929 current loss 1.287798, current_train_items 157760.
I0302 19:00:43.208031 22447428132992 run.py:483] Algo bellman_ford step 4930 current loss 0.512926, current_train_items 157792.
I0302 19:00:43.223767 22447428132992 run.py:483] Algo bellman_ford step 4931 current loss 0.766946, current_train_items 157824.
I0302 19:00:43.246755 22447428132992 run.py:483] Algo bellman_ford step 4932 current loss 0.921451, current_train_items 157856.
I0302 19:00:43.276483 22447428132992 run.py:483] Algo bellman_ford step 4933 current loss 0.956152, current_train_items 157888.
I0302 19:00:43.309539 22447428132992 run.py:483] Algo bellman_ford step 4934 current loss 1.151862, current_train_items 157920.
I0302 19:00:43.328824 22447428132992 run.py:483] Algo bellman_ford step 4935 current loss 0.568046, current_train_items 157952.
I0302 19:00:43.344877 22447428132992 run.py:483] Algo bellman_ford step 4936 current loss 0.783902, current_train_items 157984.
I0302 19:00:43.369220 22447428132992 run.py:483] Algo bellman_ford step 4937 current loss 1.026797, current_train_items 158016.
I0302 19:00:43.399508 22447428132992 run.py:483] Algo bellman_ford step 4938 current loss 1.069160, current_train_items 158048.
I0302 19:00:43.434489 22447428132992 run.py:483] Algo bellman_ford step 4939 current loss 1.166918, current_train_items 158080.
I0302 19:00:43.453811 22447428132992 run.py:483] Algo bellman_ford step 4940 current loss 0.539272, current_train_items 158112.
I0302 19:00:43.469506 22447428132992 run.py:483] Algo bellman_ford step 4941 current loss 0.763610, current_train_items 158144.
I0302 19:00:43.492593 22447428132992 run.py:483] Algo bellman_ford step 4942 current loss 0.917447, current_train_items 158176.
I0302 19:00:43.524091 22447428132992 run.py:483] Algo bellman_ford step 4943 current loss 1.041788, current_train_items 158208.
I0302 19:00:43.555539 22447428132992 run.py:483] Algo bellman_ford step 4944 current loss 1.095960, current_train_items 158240.
I0302 19:00:43.574617 22447428132992 run.py:483] Algo bellman_ford step 4945 current loss 0.727414, current_train_items 158272.
I0302 19:00:43.590661 22447428132992 run.py:483] Algo bellman_ford step 4946 current loss 0.781462, current_train_items 158304.
I0302 19:00:43.613502 22447428132992 run.py:483] Algo bellman_ford step 4947 current loss 0.938372, current_train_items 158336.
I0302 19:00:43.644056 22447428132992 run.py:483] Algo bellman_ford step 4948 current loss 1.165621, current_train_items 158368.
I0302 19:00:43.678747 22447428132992 run.py:483] Algo bellman_ford step 4949 current loss 1.104954, current_train_items 158400.
I0302 19:00:43.697902 22447428132992 run.py:483] Algo bellman_ford step 4950 current loss 0.523074, current_train_items 158432.
I0302 19:00:43.705992 22447428132992 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:43.706096 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:00:43.722456 22447428132992 run.py:483] Algo bellman_ford step 4951 current loss 0.715049, current_train_items 158464.
I0302 19:00:43.745896 22447428132992 run.py:483] Algo bellman_ford step 4952 current loss 0.890408, current_train_items 158496.
I0302 19:00:43.777141 22447428132992 run.py:483] Algo bellman_ford step 4953 current loss 0.984859, current_train_items 158528.
I0302 19:00:43.812834 22447428132992 run.py:483] Algo bellman_ford step 4954 current loss 1.136162, current_train_items 158560.
I0302 19:00:43.833189 22447428132992 run.py:483] Algo bellman_ford step 4955 current loss 0.707944, current_train_items 158592.
I0302 19:00:43.849371 22447428132992 run.py:483] Algo bellman_ford step 4956 current loss 0.812075, current_train_items 158624.
I0302 19:00:43.874020 22447428132992 run.py:483] Algo bellman_ford step 4957 current loss 0.998816, current_train_items 158656.
I0302 19:00:43.905071 22447428132992 run.py:483] Algo bellman_ford step 4958 current loss 1.102264, current_train_items 158688.
I0302 19:00:43.936358 22447428132992 run.py:483] Algo bellman_ford step 4959 current loss 1.131727, current_train_items 158720.
I0302 19:00:43.956357 22447428132992 run.py:483] Algo bellman_ford step 4960 current loss 0.572267, current_train_items 158752.
I0302 19:00:43.972687 22447428132992 run.py:483] Algo bellman_ford step 4961 current loss 0.707937, current_train_items 158784.
I0302 19:00:43.995996 22447428132992 run.py:483] Algo bellman_ford step 4962 current loss 0.900476, current_train_items 158816.
I0302 19:00:44.025658 22447428132992 run.py:483] Algo bellman_ford step 4963 current loss 0.892476, current_train_items 158848.
I0302 19:00:44.059229 22447428132992 run.py:483] Algo bellman_ford step 4964 current loss 1.113896, current_train_items 158880.
I0302 19:00:44.078525 22447428132992 run.py:483] Algo bellman_ford step 4965 current loss 0.504760, current_train_items 158912.
I0302 19:00:44.094632 22447428132992 run.py:483] Algo bellman_ford step 4966 current loss 0.774854, current_train_items 158944.
I0302 19:00:44.119078 22447428132992 run.py:483] Algo bellman_ford step 4967 current loss 1.034244, current_train_items 158976.
I0302 19:00:44.150834 22447428132992 run.py:483] Algo bellman_ford step 4968 current loss 1.037481, current_train_items 159008.
I0302 19:00:44.182702 22447428132992 run.py:483] Algo bellman_ford step 4969 current loss 1.076601, current_train_items 159040.
I0302 19:00:44.202515 22447428132992 run.py:483] Algo bellman_ford step 4970 current loss 0.694943, current_train_items 159072.
I0302 19:00:44.218541 22447428132992 run.py:483] Algo bellman_ford step 4971 current loss 0.750464, current_train_items 159104.
I0302 19:00:44.242201 22447428132992 run.py:483] Algo bellman_ford step 4972 current loss 0.904099, current_train_items 159136.
I0302 19:00:44.273628 22447428132992 run.py:483] Algo bellman_ford step 4973 current loss 1.042766, current_train_items 159168.
I0302 19:00:44.306641 22447428132992 run.py:483] Algo bellman_ford step 4974 current loss 1.301440, current_train_items 159200.
I0302 19:00:44.326510 22447428132992 run.py:483] Algo bellman_ford step 4975 current loss 0.507474, current_train_items 159232.
I0302 19:00:44.342978 22447428132992 run.py:483] Algo bellman_ford step 4976 current loss 0.750294, current_train_items 159264.
I0302 19:00:44.366732 22447428132992 run.py:483] Algo bellman_ford step 4977 current loss 1.027503, current_train_items 159296.
I0302 19:00:44.397994 22447428132992 run.py:483] Algo bellman_ford step 4978 current loss 1.095563, current_train_items 159328.
I0302 19:00:44.432297 22447428132992 run.py:483] Algo bellman_ford step 4979 current loss 1.277245, current_train_items 159360.
I0302 19:00:44.452147 22447428132992 run.py:483] Algo bellman_ford step 4980 current loss 0.581617, current_train_items 159392.
I0302 19:00:44.468427 22447428132992 run.py:483] Algo bellman_ford step 4981 current loss 0.760971, current_train_items 159424.
I0302 19:00:44.493001 22447428132992 run.py:483] Algo bellman_ford step 4982 current loss 0.960478, current_train_items 159456.
I0302 19:00:44.521927 22447428132992 run.py:483] Algo bellman_ford step 4983 current loss 1.048540, current_train_items 159488.
I0302 19:00:44.554830 22447428132992 run.py:483] Algo bellman_ford step 4984 current loss 1.193674, current_train_items 159520.
I0302 19:00:44.574846 22447428132992 run.py:483] Algo bellman_ford step 4985 current loss 0.770158, current_train_items 159552.
I0302 19:00:44.591097 22447428132992 run.py:483] Algo bellman_ford step 4986 current loss 0.779120, current_train_items 159584.
I0302 19:00:44.613131 22447428132992 run.py:483] Algo bellman_ford step 4987 current loss 0.914485, current_train_items 159616.
I0302 19:00:44.642968 22447428132992 run.py:483] Algo bellman_ford step 4988 current loss 0.973556, current_train_items 159648.
I0302 19:00:44.678855 22447428132992 run.py:483] Algo bellman_ford step 4989 current loss 1.288396, current_train_items 159680.
I0302 19:00:44.698761 22447428132992 run.py:483] Algo bellman_ford step 4990 current loss 0.559422, current_train_items 159712.
I0302 19:00:44.714654 22447428132992 run.py:483] Algo bellman_ford step 4991 current loss 0.687461, current_train_items 159744.
I0302 19:00:44.738022 22447428132992 run.py:483] Algo bellman_ford step 4992 current loss 1.021286, current_train_items 159776.
I0302 19:00:44.768651 22447428132992 run.py:483] Algo bellman_ford step 4993 current loss 1.019634, current_train_items 159808.
I0302 19:00:44.802481 22447428132992 run.py:483] Algo bellman_ford step 4994 current loss 1.427594, current_train_items 159840.
I0302 19:00:44.821912 22447428132992 run.py:483] Algo bellman_ford step 4995 current loss 0.539936, current_train_items 159872.
I0302 19:00:44.838324 22447428132992 run.py:483] Algo bellman_ford step 4996 current loss 0.761727, current_train_items 159904.
I0302 19:00:44.861623 22447428132992 run.py:483] Algo bellman_ford step 4997 current loss 1.077779, current_train_items 159936.
I0302 19:00:44.892705 22447428132992 run.py:483] Algo bellman_ford step 4998 current loss 1.043390, current_train_items 159968.
I0302 19:00:44.926900 22447428132992 run.py:483] Algo bellman_ford step 4999 current loss 1.242018, current_train_items 160000.
I0302 19:00:44.946745 22447428132992 run.py:483] Algo bellman_ford step 5000 current loss 0.690437, current_train_items 160032.
I0302 19:00:44.954848 22447428132992 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:44.954952 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:00:44.971706 22447428132992 run.py:483] Algo bellman_ford step 5001 current loss 0.814177, current_train_items 160064.
I0302 19:00:44.995452 22447428132992 run.py:483] Algo bellman_ford step 5002 current loss 0.954454, current_train_items 160096.
I0302 19:00:45.024856 22447428132992 run.py:483] Algo bellman_ford step 5003 current loss 0.899272, current_train_items 160128.
I0302 19:00:45.057734 22447428132992 run.py:483] Algo bellman_ford step 5004 current loss 1.183513, current_train_items 160160.
I0302 19:00:45.077442 22447428132992 run.py:483] Algo bellman_ford step 5005 current loss 0.607587, current_train_items 160192.
I0302 19:00:45.093479 22447428132992 run.py:483] Algo bellman_ford step 5006 current loss 0.776713, current_train_items 160224.
I0302 19:00:45.117245 22447428132992 run.py:483] Algo bellman_ford step 5007 current loss 1.060064, current_train_items 160256.
I0302 19:00:45.149803 22447428132992 run.py:483] Algo bellman_ford step 5008 current loss 1.101723, current_train_items 160288.
I0302 19:00:45.181396 22447428132992 run.py:483] Algo bellman_ford step 5009 current loss 1.260820, current_train_items 160320.
I0302 19:00:45.200526 22447428132992 run.py:483] Algo bellman_ford step 5010 current loss 0.569249, current_train_items 160352.
I0302 19:00:45.216296 22447428132992 run.py:483] Algo bellman_ford step 5011 current loss 0.713600, current_train_items 160384.
I0302 19:00:45.239670 22447428132992 run.py:483] Algo bellman_ford step 5012 current loss 0.898998, current_train_items 160416.
I0302 19:00:45.269697 22447428132992 run.py:483] Algo bellman_ford step 5013 current loss 1.134424, current_train_items 160448.
I0302 19:00:45.303896 22447428132992 run.py:483] Algo bellman_ford step 5014 current loss 1.063197, current_train_items 160480.
I0302 19:00:45.323400 22447428132992 run.py:483] Algo bellman_ford step 5015 current loss 0.901905, current_train_items 160512.
I0302 19:00:45.340114 22447428132992 run.py:483] Algo bellman_ford step 5016 current loss 0.798633, current_train_items 160544.
I0302 19:00:45.363035 22447428132992 run.py:483] Algo bellman_ford step 5017 current loss 0.940227, current_train_items 160576.
I0302 19:00:45.393935 22447428132992 run.py:483] Algo bellman_ford step 5018 current loss 0.978840, current_train_items 160608.
I0302 19:00:45.428775 22447428132992 run.py:483] Algo bellman_ford step 5019 current loss 1.251274, current_train_items 160640.
I0302 19:00:45.448018 22447428132992 run.py:483] Algo bellman_ford step 5020 current loss 0.636582, current_train_items 160672.
I0302 19:00:45.463841 22447428132992 run.py:483] Algo bellman_ford step 5021 current loss 0.669771, current_train_items 160704.
I0302 19:00:45.487891 22447428132992 run.py:483] Algo bellman_ford step 5022 current loss 1.068686, current_train_items 160736.
I0302 19:00:45.518503 22447428132992 run.py:483] Algo bellman_ford step 5023 current loss 1.026839, current_train_items 160768.
I0302 19:00:45.552256 22447428132992 run.py:483] Algo bellman_ford step 5024 current loss 1.072134, current_train_items 160800.
I0302 19:00:45.571376 22447428132992 run.py:483] Algo bellman_ford step 5025 current loss 0.736757, current_train_items 160832.
I0302 19:00:45.587033 22447428132992 run.py:483] Algo bellman_ford step 5026 current loss 0.776829, current_train_items 160864.
I0302 19:00:45.609265 22447428132992 run.py:483] Algo bellman_ford step 5027 current loss 0.869551, current_train_items 160896.
I0302 19:00:45.638281 22447428132992 run.py:483] Algo bellman_ford step 5028 current loss 0.943497, current_train_items 160928.
I0302 19:00:45.671166 22447428132992 run.py:483] Algo bellman_ford step 5029 current loss 1.287148, current_train_items 160960.
I0302 19:00:45.690658 22447428132992 run.py:483] Algo bellman_ford step 5030 current loss 0.610205, current_train_items 160992.
I0302 19:00:45.706743 22447428132992 run.py:483] Algo bellman_ford step 5031 current loss 0.866935, current_train_items 161024.
I0302 19:00:45.730501 22447428132992 run.py:483] Algo bellman_ford step 5032 current loss 0.912066, current_train_items 161056.
I0302 19:00:45.759774 22447428132992 run.py:483] Algo bellman_ford step 5033 current loss 0.994698, current_train_items 161088.
I0302 19:00:45.794055 22447428132992 run.py:483] Algo bellman_ford step 5034 current loss 1.243362, current_train_items 161120.
I0302 19:00:45.813654 22447428132992 run.py:483] Algo bellman_ford step 5035 current loss 0.699467, current_train_items 161152.
I0302 19:00:45.829817 22447428132992 run.py:483] Algo bellman_ford step 5036 current loss 0.821829, current_train_items 161184.
I0302 19:00:45.853068 22447428132992 run.py:483] Algo bellman_ford step 5037 current loss 0.973571, current_train_items 161216.
I0302 19:00:45.884957 22447428132992 run.py:483] Algo bellman_ford step 5038 current loss 1.021349, current_train_items 161248.
I0302 19:00:45.918540 22447428132992 run.py:483] Algo bellman_ford step 5039 current loss 1.197977, current_train_items 161280.
I0302 19:00:45.938002 22447428132992 run.py:483] Algo bellman_ford step 5040 current loss 0.598024, current_train_items 161312.
I0302 19:00:45.953604 22447428132992 run.py:483] Algo bellman_ford step 5041 current loss 0.704510, current_train_items 161344.
I0302 19:00:45.977535 22447428132992 run.py:483] Algo bellman_ford step 5042 current loss 1.003033, current_train_items 161376.
I0302 19:00:46.006953 22447428132992 run.py:483] Algo bellman_ford step 5043 current loss 0.980715, current_train_items 161408.
I0302 19:00:46.039898 22447428132992 run.py:483] Algo bellman_ford step 5044 current loss 1.180773, current_train_items 161440.
I0302 19:00:46.059059 22447428132992 run.py:483] Algo bellman_ford step 5045 current loss 0.558822, current_train_items 161472.
I0302 19:00:46.075055 22447428132992 run.py:483] Algo bellman_ford step 5046 current loss 0.799378, current_train_items 161504.
I0302 19:00:46.097189 22447428132992 run.py:483] Algo bellman_ford step 5047 current loss 0.877853, current_train_items 161536.
I0302 19:00:46.127488 22447428132992 run.py:483] Algo bellman_ford step 5048 current loss 1.071086, current_train_items 161568.
I0302 19:00:46.161364 22447428132992 run.py:483] Algo bellman_ford step 5049 current loss 1.165454, current_train_items 161600.
I0302 19:00:46.180583 22447428132992 run.py:483] Algo bellman_ford step 5050 current loss 0.621830, current_train_items 161632.
I0302 19:00:46.188746 22447428132992 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:46.188852 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:46.205296 22447428132992 run.py:483] Algo bellman_ford step 5051 current loss 0.723822, current_train_items 161664.
I0302 19:00:46.230083 22447428132992 run.py:483] Algo bellman_ford step 5052 current loss 0.920069, current_train_items 161696.
I0302 19:00:46.260999 22447428132992 run.py:483] Algo bellman_ford step 5053 current loss 1.016568, current_train_items 161728.
I0302 19:00:46.293469 22447428132992 run.py:483] Algo bellman_ford step 5054 current loss 1.078988, current_train_items 161760.
I0302 19:00:46.313952 22447428132992 run.py:483] Algo bellman_ford step 5055 current loss 0.712027, current_train_items 161792.
I0302 19:00:46.330791 22447428132992 run.py:483] Algo bellman_ford step 5056 current loss 0.895053, current_train_items 161824.
I0302 19:00:46.355840 22447428132992 run.py:483] Algo bellman_ford step 5057 current loss 0.986803, current_train_items 161856.
I0302 19:00:46.386015 22447428132992 run.py:483] Algo bellman_ford step 5058 current loss 1.048074, current_train_items 161888.
I0302 19:00:46.420188 22447428132992 run.py:483] Algo bellman_ford step 5059 current loss 1.145813, current_train_items 161920.
I0302 19:00:46.440186 22447428132992 run.py:483] Algo bellman_ford step 5060 current loss 0.631294, current_train_items 161952.
I0302 19:00:46.456366 22447428132992 run.py:483] Algo bellman_ford step 5061 current loss 0.791415, current_train_items 161984.
I0302 19:00:46.478350 22447428132992 run.py:483] Algo bellman_ford step 5062 current loss 0.973790, current_train_items 162016.
I0302 19:00:46.509165 22447428132992 run.py:483] Algo bellman_ford step 5063 current loss 1.124794, current_train_items 162048.
I0302 19:00:46.540855 22447428132992 run.py:483] Algo bellman_ford step 5064 current loss 1.181513, current_train_items 162080.
I0302 19:00:46.560376 22447428132992 run.py:483] Algo bellman_ford step 5065 current loss 0.580178, current_train_items 162112.
I0302 19:00:46.576438 22447428132992 run.py:483] Algo bellman_ford step 5066 current loss 0.728872, current_train_items 162144.
I0302 19:00:46.600956 22447428132992 run.py:483] Algo bellman_ford step 5067 current loss 0.923469, current_train_items 162176.
I0302 19:00:46.631355 22447428132992 run.py:483] Algo bellman_ford step 5068 current loss 1.014981, current_train_items 162208.
I0302 19:00:46.666278 22447428132992 run.py:483] Algo bellman_ford step 5069 current loss 1.095408, current_train_items 162240.
I0302 19:00:46.686084 22447428132992 run.py:483] Algo bellman_ford step 5070 current loss 0.636951, current_train_items 162272.
I0302 19:00:46.702618 22447428132992 run.py:483] Algo bellman_ford step 5071 current loss 0.788436, current_train_items 162304.
I0302 19:00:46.724325 22447428132992 run.py:483] Algo bellman_ford step 5072 current loss 0.904223, current_train_items 162336.
I0302 19:00:46.755680 22447428132992 run.py:483] Algo bellman_ford step 5073 current loss 1.040430, current_train_items 162368.
I0302 19:00:46.786929 22447428132992 run.py:483] Algo bellman_ford step 5074 current loss 0.999448, current_train_items 162400.
I0302 19:00:46.806982 22447428132992 run.py:483] Algo bellman_ford step 5075 current loss 0.529496, current_train_items 162432.
I0302 19:00:46.823064 22447428132992 run.py:483] Algo bellman_ford step 5076 current loss 0.778339, current_train_items 162464.
I0302 19:00:46.846279 22447428132992 run.py:483] Algo bellman_ford step 5077 current loss 0.833067, current_train_items 162496.
I0302 19:00:46.876911 22447428132992 run.py:483] Algo bellman_ford step 5078 current loss 1.047451, current_train_items 162528.
I0302 19:00:46.909624 22447428132992 run.py:483] Algo bellman_ford step 5079 current loss 1.280194, current_train_items 162560.
I0302 19:00:46.929244 22447428132992 run.py:483] Algo bellman_ford step 5080 current loss 0.546855, current_train_items 162592.
I0302 19:00:46.945049 22447428132992 run.py:483] Algo bellman_ford step 5081 current loss 0.709798, current_train_items 162624.
I0302 19:00:46.968440 22447428132992 run.py:483] Algo bellman_ford step 5082 current loss 0.912888, current_train_items 162656.
I0302 19:00:46.999552 22447428132992 run.py:483] Algo bellman_ford step 5083 current loss 1.048082, current_train_items 162688.
I0302 19:00:47.032905 22447428132992 run.py:483] Algo bellman_ford step 5084 current loss 1.031635, current_train_items 162720.
I0302 19:00:47.052945 22447428132992 run.py:483] Algo bellman_ford step 5085 current loss 0.820845, current_train_items 162752.
I0302 19:00:47.069034 22447428132992 run.py:483] Algo bellman_ford step 5086 current loss 0.796243, current_train_items 162784.
I0302 19:00:47.092395 22447428132992 run.py:483] Algo bellman_ford step 5087 current loss 1.004845, current_train_items 162816.
I0302 19:00:47.123532 22447428132992 run.py:483] Algo bellman_ford step 5088 current loss 0.972608, current_train_items 162848.
I0302 19:00:47.158155 22447428132992 run.py:483] Algo bellman_ford step 5089 current loss 1.172955, current_train_items 162880.
I0302 19:00:47.177772 22447428132992 run.py:483] Algo bellman_ford step 5090 current loss 0.559560, current_train_items 162912.
I0302 19:00:47.194125 22447428132992 run.py:483] Algo bellman_ford step 5091 current loss 0.716567, current_train_items 162944.
I0302 19:00:47.215987 22447428132992 run.py:483] Algo bellman_ford step 5092 current loss 0.895164, current_train_items 162976.
I0302 19:00:47.245746 22447428132992 run.py:483] Algo bellman_ford step 5093 current loss 0.899141, current_train_items 163008.
I0302 19:00:47.278347 22447428132992 run.py:483] Algo bellman_ford step 5094 current loss 1.057349, current_train_items 163040.
I0302 19:00:47.297780 22447428132992 run.py:483] Algo bellman_ford step 5095 current loss 0.563151, current_train_items 163072.
I0302 19:00:47.313953 22447428132992 run.py:483] Algo bellman_ford step 5096 current loss 0.799476, current_train_items 163104.
I0302 19:00:47.338078 22447428132992 run.py:483] Algo bellman_ford step 5097 current loss 1.058294, current_train_items 163136.
I0302 19:00:47.369829 22447428132992 run.py:483] Algo bellman_ford step 5098 current loss 1.166730, current_train_items 163168.
I0302 19:00:47.404807 22447428132992 run.py:483] Algo bellman_ford step 5099 current loss 1.292335, current_train_items 163200.
I0302 19:00:47.424570 22447428132992 run.py:483] Algo bellman_ford step 5100 current loss 0.552926, current_train_items 163232.
I0302 19:00:47.432404 22447428132992 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:47.432511 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:00:47.448899 22447428132992 run.py:483] Algo bellman_ford step 5101 current loss 0.757130, current_train_items 163264.
I0302 19:00:47.470801 22447428132992 run.py:483] Algo bellman_ford step 5102 current loss 0.818348, current_train_items 163296.
I0302 19:00:47.502361 22447428132992 run.py:483] Algo bellman_ford step 5103 current loss 1.104147, current_train_items 163328.
I0302 19:00:47.536941 22447428132992 run.py:483] Algo bellman_ford step 5104 current loss 1.178419, current_train_items 163360.
I0302 19:00:47.557168 22447428132992 run.py:483] Algo bellman_ford step 5105 current loss 0.877305, current_train_items 163392.
I0302 19:00:47.573440 22447428132992 run.py:483] Algo bellman_ford step 5106 current loss 0.899152, current_train_items 163424.
I0302 19:00:47.596982 22447428132992 run.py:483] Algo bellman_ford step 5107 current loss 0.950837, current_train_items 163456.
I0302 19:00:47.626371 22447428132992 run.py:483] Algo bellman_ford step 5108 current loss 1.031133, current_train_items 163488.
I0302 19:00:47.660666 22447428132992 run.py:483] Algo bellman_ford step 5109 current loss 1.073419, current_train_items 163520.
I0302 19:00:47.680454 22447428132992 run.py:483] Algo bellman_ford step 5110 current loss 0.722812, current_train_items 163552.
I0302 19:00:47.696699 22447428132992 run.py:483] Algo bellman_ford step 5111 current loss 0.723022, current_train_items 163584.
I0302 19:00:47.719681 22447428132992 run.py:483] Algo bellman_ford step 5112 current loss 0.913984, current_train_items 163616.
I0302 19:00:47.749768 22447428132992 run.py:483] Algo bellman_ford step 5113 current loss 0.968149, current_train_items 163648.
I0302 19:00:47.782595 22447428132992 run.py:483] Algo bellman_ford step 5114 current loss 1.186877, current_train_items 163680.
I0302 19:00:47.802358 22447428132992 run.py:483] Algo bellman_ford step 5115 current loss 0.658656, current_train_items 163712.
I0302 19:00:47.818679 22447428132992 run.py:483] Algo bellman_ford step 5116 current loss 0.824805, current_train_items 163744.
I0302 19:00:47.842744 22447428132992 run.py:483] Algo bellman_ford step 5117 current loss 1.044589, current_train_items 163776.
I0302 19:00:47.872752 22447428132992 run.py:483] Algo bellman_ford step 5118 current loss 0.931004, current_train_items 163808.
I0302 19:00:47.906991 22447428132992 run.py:483] Algo bellman_ford step 5119 current loss 1.232847, current_train_items 163840.
I0302 19:00:47.926837 22447428132992 run.py:483] Algo bellman_ford step 5120 current loss 0.539402, current_train_items 163872.
I0302 19:00:47.942563 22447428132992 run.py:483] Algo bellman_ford step 5121 current loss 0.724059, current_train_items 163904.
I0302 19:00:47.965553 22447428132992 run.py:483] Algo bellman_ford step 5122 current loss 0.925642, current_train_items 163936.
I0302 19:00:47.995156 22447428132992 run.py:483] Algo bellman_ford step 5123 current loss 0.997992, current_train_items 163968.
I0302 19:00:48.028476 22447428132992 run.py:483] Algo bellman_ford step 5124 current loss 1.096089, current_train_items 164000.
I0302 19:00:48.047897 22447428132992 run.py:483] Algo bellman_ford step 5125 current loss 0.691484, current_train_items 164032.
I0302 19:00:48.063917 22447428132992 run.py:483] Algo bellman_ford step 5126 current loss 0.784721, current_train_items 164064.
I0302 19:00:48.087082 22447428132992 run.py:483] Algo bellman_ford step 5127 current loss 1.040351, current_train_items 164096.
I0302 19:00:48.117644 22447428132992 run.py:483] Algo bellman_ford step 5128 current loss 1.048064, current_train_items 164128.
I0302 19:00:48.152403 22447428132992 run.py:483] Algo bellman_ford step 5129 current loss 1.162105, current_train_items 164160.
I0302 19:00:48.172005 22447428132992 run.py:483] Algo bellman_ford step 5130 current loss 0.555353, current_train_items 164192.
I0302 19:00:48.188306 22447428132992 run.py:483] Algo bellman_ford step 5131 current loss 0.796671, current_train_items 164224.
I0302 19:00:48.210956 22447428132992 run.py:483] Algo bellman_ford step 5132 current loss 0.941918, current_train_items 164256.
I0302 19:00:48.242257 22447428132992 run.py:483] Algo bellman_ford step 5133 current loss 1.064547, current_train_items 164288.
I0302 19:00:48.274887 22447428132992 run.py:483] Algo bellman_ford step 5134 current loss 1.118638, current_train_items 164320.
I0302 19:00:48.294747 22447428132992 run.py:483] Algo bellman_ford step 5135 current loss 0.560343, current_train_items 164352.
I0302 19:00:48.310744 22447428132992 run.py:483] Algo bellman_ford step 5136 current loss 0.702730, current_train_items 164384.
I0302 19:00:48.334273 22447428132992 run.py:483] Algo bellman_ford step 5137 current loss 1.006506, current_train_items 164416.
I0302 19:00:48.364820 22447428132992 run.py:483] Algo bellman_ford step 5138 current loss 0.996040, current_train_items 164448.
I0302 19:00:48.398584 22447428132992 run.py:483] Algo bellman_ford step 5139 current loss 1.009082, current_train_items 164480.
I0302 19:00:48.417963 22447428132992 run.py:483] Algo bellman_ford step 5140 current loss 0.634635, current_train_items 164512.
I0302 19:00:48.434256 22447428132992 run.py:483] Algo bellman_ford step 5141 current loss 0.832598, current_train_items 164544.
I0302 19:00:48.458399 22447428132992 run.py:483] Algo bellman_ford step 5142 current loss 0.987747, current_train_items 164576.
I0302 19:00:48.489080 22447428132992 run.py:483] Algo bellman_ford step 5143 current loss 0.999871, current_train_items 164608.
I0302 19:00:48.522522 22447428132992 run.py:483] Algo bellman_ford step 5144 current loss 1.131674, current_train_items 164640.
I0302 19:00:48.541944 22447428132992 run.py:483] Algo bellman_ford step 5145 current loss 0.614670, current_train_items 164672.
I0302 19:00:48.557886 22447428132992 run.py:483] Algo bellman_ford step 5146 current loss 0.833567, current_train_items 164704.
I0302 19:00:48.579715 22447428132992 run.py:483] Algo bellman_ford step 5147 current loss 1.027577, current_train_items 164736.
I0302 19:00:48.609531 22447428132992 run.py:483] Algo bellman_ford step 5148 current loss 1.031531, current_train_items 164768.
I0302 19:00:48.642306 22447428132992 run.py:483] Algo bellman_ford step 5149 current loss 1.089571, current_train_items 164800.
I0302 19:00:48.661959 22447428132992 run.py:483] Algo bellman_ford step 5150 current loss 0.581485, current_train_items 164832.
I0302 19:00:48.670026 22447428132992 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:48.670131 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:48.686783 22447428132992 run.py:483] Algo bellman_ford step 5151 current loss 0.817550, current_train_items 164864.
I0302 19:00:48.710161 22447428132992 run.py:483] Algo bellman_ford step 5152 current loss 0.952682, current_train_items 164896.
I0302 19:00:48.739796 22447428132992 run.py:483] Algo bellman_ford step 5153 current loss 1.005723, current_train_items 164928.
I0302 19:00:48.772621 22447428132992 run.py:483] Algo bellman_ford step 5154 current loss 1.287057, current_train_items 164960.
I0302 19:00:48.792491 22447428132992 run.py:483] Algo bellman_ford step 5155 current loss 0.594196, current_train_items 164992.
I0302 19:00:48.808289 22447428132992 run.py:483] Algo bellman_ford step 5156 current loss 0.820664, current_train_items 165024.
I0302 19:00:48.832209 22447428132992 run.py:483] Algo bellman_ford step 5157 current loss 0.993942, current_train_items 165056.
I0302 19:00:48.864111 22447428132992 run.py:483] Algo bellman_ford step 5158 current loss 1.333361, current_train_items 165088.
I0302 19:00:48.896515 22447428132992 run.py:483] Algo bellman_ford step 5159 current loss 1.302989, current_train_items 165120.
I0302 19:00:48.916025 22447428132992 run.py:483] Algo bellman_ford step 5160 current loss 0.550248, current_train_items 165152.
I0302 19:00:48.932006 22447428132992 run.py:483] Algo bellman_ford step 5161 current loss 0.746938, current_train_items 165184.
I0302 19:00:48.955944 22447428132992 run.py:483] Algo bellman_ford step 5162 current loss 1.082307, current_train_items 165216.
I0302 19:00:48.987729 22447428132992 run.py:483] Algo bellman_ford step 5163 current loss 1.023089, current_train_items 165248.
I0302 19:00:49.019447 22447428132992 run.py:483] Algo bellman_ford step 5164 current loss 1.155353, current_train_items 165280.
I0302 19:00:49.038642 22447428132992 run.py:483] Algo bellman_ford step 5165 current loss 0.603028, current_train_items 165312.
I0302 19:00:49.054514 22447428132992 run.py:483] Algo bellman_ford step 5166 current loss 0.721684, current_train_items 165344.
I0302 19:00:49.076541 22447428132992 run.py:483] Algo bellman_ford step 5167 current loss 1.023475, current_train_items 165376.
I0302 19:00:49.107430 22447428132992 run.py:483] Algo bellman_ford step 5168 current loss 1.007531, current_train_items 165408.
I0302 19:00:49.141633 22447428132992 run.py:483] Algo bellman_ford step 5169 current loss 1.104241, current_train_items 165440.
I0302 19:00:49.161280 22447428132992 run.py:483] Algo bellman_ford step 5170 current loss 0.681134, current_train_items 165472.
I0302 19:00:49.177496 22447428132992 run.py:483] Algo bellman_ford step 5171 current loss 0.761690, current_train_items 165504.
I0302 19:00:49.200845 22447428132992 run.py:483] Algo bellman_ford step 5172 current loss 1.003841, current_train_items 165536.
I0302 19:00:49.232223 22447428132992 run.py:483] Algo bellman_ford step 5173 current loss 1.143814, current_train_items 165568.
I0302 19:00:49.264538 22447428132992 run.py:483] Algo bellman_ford step 5174 current loss 1.132377, current_train_items 165600.
I0302 19:00:49.283873 22447428132992 run.py:483] Algo bellman_ford step 5175 current loss 0.530471, current_train_items 165632.
I0302 19:00:49.300123 22447428132992 run.py:483] Algo bellman_ford step 5176 current loss 0.765320, current_train_items 165664.
I0302 19:00:49.323258 22447428132992 run.py:483] Algo bellman_ford step 5177 current loss 0.954091, current_train_items 165696.
I0302 19:00:49.353370 22447428132992 run.py:483] Algo bellman_ford step 5178 current loss 1.016114, current_train_items 165728.
I0302 19:00:49.385128 22447428132992 run.py:483] Algo bellman_ford step 5179 current loss 1.203640, current_train_items 165760.
I0302 19:00:49.404325 22447428132992 run.py:483] Algo bellman_ford step 5180 current loss 0.553741, current_train_items 165792.
I0302 19:00:49.420327 22447428132992 run.py:483] Algo bellman_ford step 5181 current loss 0.721653, current_train_items 165824.
I0302 19:00:49.443399 22447428132992 run.py:483] Algo bellman_ford step 5182 current loss 0.873138, current_train_items 165856.
I0302 19:00:49.473687 22447428132992 run.py:483] Algo bellman_ford step 5183 current loss 0.991927, current_train_items 165888.
I0302 19:00:49.505407 22447428132992 run.py:483] Algo bellman_ford step 5184 current loss 1.061673, current_train_items 165920.
I0302 19:00:49.525112 22447428132992 run.py:483] Algo bellman_ford step 5185 current loss 0.691527, current_train_items 165952.
I0302 19:00:49.541650 22447428132992 run.py:483] Algo bellman_ford step 5186 current loss 0.873104, current_train_items 165984.
I0302 19:00:49.563430 22447428132992 run.py:483] Algo bellman_ford step 5187 current loss 0.905027, current_train_items 166016.
I0302 19:00:49.594519 22447428132992 run.py:483] Algo bellman_ford step 5188 current loss 0.973739, current_train_items 166048.
I0302 19:00:49.626737 22447428132992 run.py:483] Algo bellman_ford step 5189 current loss 1.184726, current_train_items 166080.
I0302 19:00:49.646408 22447428132992 run.py:483] Algo bellman_ford step 5190 current loss 0.624515, current_train_items 166112.
I0302 19:00:49.662420 22447428132992 run.py:483] Algo bellman_ford step 5191 current loss 0.706223, current_train_items 166144.
I0302 19:00:49.685911 22447428132992 run.py:483] Algo bellman_ford step 5192 current loss 0.891015, current_train_items 166176.
I0302 19:00:49.715366 22447428132992 run.py:483] Algo bellman_ford step 5193 current loss 1.142475, current_train_items 166208.
I0302 19:00:49.748424 22447428132992 run.py:483] Algo bellman_ford step 5194 current loss 1.579058, current_train_items 166240.
I0302 19:00:49.767741 22447428132992 run.py:483] Algo bellman_ford step 5195 current loss 0.600892, current_train_items 166272.
I0302 19:00:49.783869 22447428132992 run.py:483] Algo bellman_ford step 5196 current loss 0.737678, current_train_items 166304.
I0302 19:00:49.807318 22447428132992 run.py:483] Algo bellman_ford step 5197 current loss 1.050166, current_train_items 166336.
I0302 19:00:49.838863 22447428132992 run.py:483] Algo bellman_ford step 5198 current loss 1.067521, current_train_items 166368.
I0302 19:00:49.871449 22447428132992 run.py:483] Algo bellman_ford step 5199 current loss 1.243964, current_train_items 166400.
I0302 19:00:49.890964 22447428132992 run.py:483] Algo bellman_ford step 5200 current loss 0.553731, current_train_items 166432.
I0302 19:00:49.898802 22447428132992 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:49.898907 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:00:49.915634 22447428132992 run.py:483] Algo bellman_ford step 5201 current loss 0.773917, current_train_items 166464.
I0302 19:00:49.939970 22447428132992 run.py:483] Algo bellman_ford step 5202 current loss 0.946720, current_train_items 166496.
I0302 19:00:49.971935 22447428132992 run.py:483] Algo bellman_ford step 5203 current loss 1.102728, current_train_items 166528.
I0302 19:00:50.006862 22447428132992 run.py:483] Algo bellman_ford step 5204 current loss 1.064841, current_train_items 166560.
I0302 19:00:50.026874 22447428132992 run.py:483] Algo bellman_ford step 5205 current loss 0.566984, current_train_items 166592.
I0302 19:00:50.042442 22447428132992 run.py:483] Algo bellman_ford step 5206 current loss 0.744371, current_train_items 166624.
I0302 19:00:50.065937 22447428132992 run.py:483] Algo bellman_ford step 5207 current loss 0.977026, current_train_items 166656.
I0302 19:00:50.095826 22447428132992 run.py:483] Algo bellman_ford step 5208 current loss 0.924622, current_train_items 166688.
I0302 19:00:50.131987 22447428132992 run.py:483] Algo bellman_ford step 5209 current loss 1.203374, current_train_items 166720.
I0302 19:00:50.151476 22447428132992 run.py:483] Algo bellman_ford step 5210 current loss 0.560236, current_train_items 166752.
I0302 19:00:50.167103 22447428132992 run.py:483] Algo bellman_ford step 5211 current loss 0.754217, current_train_items 166784.
I0302 19:00:50.191082 22447428132992 run.py:483] Algo bellman_ford step 5212 current loss 0.990688, current_train_items 166816.
I0302 19:00:50.223309 22447428132992 run.py:483] Algo bellman_ford step 5213 current loss 1.086806, current_train_items 166848.
I0302 19:00:50.256446 22447428132992 run.py:483] Algo bellman_ford step 5214 current loss 1.487647, current_train_items 166880.
I0302 19:00:50.275951 22447428132992 run.py:483] Algo bellman_ford step 5215 current loss 0.583178, current_train_items 166912.
I0302 19:00:50.292336 22447428132992 run.py:483] Algo bellman_ford step 5216 current loss 0.748954, current_train_items 166944.
I0302 19:00:50.315849 22447428132992 run.py:483] Algo bellman_ford step 5217 current loss 0.980596, current_train_items 166976.
I0302 19:00:50.345931 22447428132992 run.py:483] Algo bellman_ford step 5218 current loss 0.934035, current_train_items 167008.
I0302 19:00:50.379231 22447428132992 run.py:483] Algo bellman_ford step 5219 current loss 1.120353, current_train_items 167040.
I0302 19:00:50.398712 22447428132992 run.py:483] Algo bellman_ford step 5220 current loss 0.753265, current_train_items 167072.
I0302 19:00:50.414665 22447428132992 run.py:483] Algo bellman_ford step 5221 current loss 0.684893, current_train_items 167104.
I0302 19:00:50.437111 22447428132992 run.py:483] Algo bellman_ford step 5222 current loss 0.813450, current_train_items 167136.
I0302 19:00:50.468881 22447428132992 run.py:483] Algo bellman_ford step 5223 current loss 1.014941, current_train_items 167168.
I0302 19:00:50.501739 22447428132992 run.py:483] Algo bellman_ford step 5224 current loss 1.073024, current_train_items 167200.
I0302 19:00:50.521559 22447428132992 run.py:483] Algo bellman_ford step 5225 current loss 0.635582, current_train_items 167232.
I0302 19:00:50.537459 22447428132992 run.py:483] Algo bellman_ford step 5226 current loss 0.750393, current_train_items 167264.
I0302 19:00:50.561239 22447428132992 run.py:483] Algo bellman_ford step 5227 current loss 0.939245, current_train_items 167296.
I0302 19:00:50.593224 22447428132992 run.py:483] Algo bellman_ford step 5228 current loss 1.094656, current_train_items 167328.
I0302 19:00:50.627979 22447428132992 run.py:483] Algo bellman_ford step 5229 current loss 1.275835, current_train_items 167360.
I0302 19:00:50.647782 22447428132992 run.py:483] Algo bellman_ford step 5230 current loss 0.631442, current_train_items 167392.
I0302 19:00:50.664007 22447428132992 run.py:483] Algo bellman_ford step 5231 current loss 0.802324, current_train_items 167424.
I0302 19:00:50.688006 22447428132992 run.py:483] Algo bellman_ford step 5232 current loss 0.906858, current_train_items 167456.
I0302 19:00:50.718173 22447428132992 run.py:483] Algo bellman_ford step 5233 current loss 0.916706, current_train_items 167488.
I0302 19:00:50.753227 22447428132992 run.py:483] Algo bellman_ford step 5234 current loss 1.113292, current_train_items 167520.
I0302 19:00:50.772949 22447428132992 run.py:483] Algo bellman_ford step 5235 current loss 0.665057, current_train_items 167552.
I0302 19:00:50.788841 22447428132992 run.py:483] Algo bellman_ford step 5236 current loss 0.784393, current_train_items 167584.
I0302 19:00:50.812282 22447428132992 run.py:483] Algo bellman_ford step 5237 current loss 0.928956, current_train_items 167616.
I0302 19:00:50.844863 22447428132992 run.py:483] Algo bellman_ford step 5238 current loss 1.115780, current_train_items 167648.
I0302 19:00:50.879050 22447428132992 run.py:483] Algo bellman_ford step 5239 current loss 1.126974, current_train_items 167680.
I0302 19:00:50.898688 22447428132992 run.py:483] Algo bellman_ford step 5240 current loss 0.619450, current_train_items 167712.
I0302 19:00:50.914799 22447428132992 run.py:483] Algo bellman_ford step 5241 current loss 0.789697, current_train_items 167744.
I0302 19:00:50.938466 22447428132992 run.py:483] Algo bellman_ford step 5242 current loss 1.060591, current_train_items 167776.
I0302 19:00:50.969128 22447428132992 run.py:483] Algo bellman_ford step 5243 current loss 0.977179, current_train_items 167808.
I0302 19:00:51.000302 22447428132992 run.py:483] Algo bellman_ford step 5244 current loss 0.987460, current_train_items 167840.
I0302 19:00:51.019541 22447428132992 run.py:483] Algo bellman_ford step 5245 current loss 0.578286, current_train_items 167872.
I0302 19:00:51.035594 22447428132992 run.py:483] Algo bellman_ford step 5246 current loss 0.822757, current_train_items 167904.
I0302 19:00:51.059873 22447428132992 run.py:483] Algo bellman_ford step 5247 current loss 0.896136, current_train_items 167936.
I0302 19:00:51.091607 22447428132992 run.py:483] Algo bellman_ford step 5248 current loss 0.995946, current_train_items 167968.
I0302 19:00:51.125486 22447428132992 run.py:483] Algo bellman_ford step 5249 current loss 1.095614, current_train_items 168000.
I0302 19:00:51.145115 22447428132992 run.py:483] Algo bellman_ford step 5250 current loss 0.571580, current_train_items 168032.
I0302 19:00:51.153228 22447428132992 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:51.153333 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:51.169847 22447428132992 run.py:483] Algo bellman_ford step 5251 current loss 0.716471, current_train_items 168064.
I0302 19:00:51.193935 22447428132992 run.py:483] Algo bellman_ford step 5252 current loss 1.028939, current_train_items 168096.
I0302 19:00:51.225540 22447428132992 run.py:483] Algo bellman_ford step 5253 current loss 0.994045, current_train_items 168128.
I0302 19:00:51.256874 22447428132992 run.py:483] Algo bellman_ford step 5254 current loss 0.982726, current_train_items 168160.
I0302 19:00:51.276796 22447428132992 run.py:483] Algo bellman_ford step 5255 current loss 0.612514, current_train_items 168192.
I0302 19:00:51.292562 22447428132992 run.py:483] Algo bellman_ford step 5256 current loss 0.772913, current_train_items 168224.
I0302 19:00:51.315015 22447428132992 run.py:483] Algo bellman_ford step 5257 current loss 0.954705, current_train_items 168256.
I0302 19:00:51.346084 22447428132992 run.py:483] Algo bellman_ford step 5258 current loss 1.107953, current_train_items 168288.
I0302 19:00:51.377213 22447428132992 run.py:483] Algo bellman_ford step 5259 current loss 0.980801, current_train_items 168320.
I0302 19:00:51.396845 22447428132992 run.py:483] Algo bellman_ford step 5260 current loss 0.592753, current_train_items 168352.
I0302 19:00:51.413064 22447428132992 run.py:483] Algo bellman_ford step 5261 current loss 0.711054, current_train_items 168384.
I0302 19:00:51.435554 22447428132992 run.py:483] Algo bellman_ford step 5262 current loss 0.898896, current_train_items 168416.
I0302 19:00:51.466998 22447428132992 run.py:483] Algo bellman_ford step 5263 current loss 1.118212, current_train_items 168448.
I0302 19:00:51.498910 22447428132992 run.py:483] Algo bellman_ford step 5264 current loss 1.035026, current_train_items 168480.
I0302 19:00:51.518355 22447428132992 run.py:483] Algo bellman_ford step 5265 current loss 0.608512, current_train_items 168512.
I0302 19:00:51.534507 22447428132992 run.py:483] Algo bellman_ford step 5266 current loss 0.867421, current_train_items 168544.
I0302 19:00:51.558621 22447428132992 run.py:483] Algo bellman_ford step 5267 current loss 0.922734, current_train_items 168576.
I0302 19:00:51.588537 22447428132992 run.py:483] Algo bellman_ford step 5268 current loss 0.923240, current_train_items 168608.
I0302 19:00:51.619953 22447428132992 run.py:483] Algo bellman_ford step 5269 current loss 1.175501, current_train_items 168640.
I0302 19:00:51.639512 22447428132992 run.py:483] Algo bellman_ford step 5270 current loss 0.733935, current_train_items 168672.
I0302 19:00:51.655449 22447428132992 run.py:483] Algo bellman_ford step 5271 current loss 0.798585, current_train_items 168704.
I0302 19:00:51.677639 22447428132992 run.py:483] Algo bellman_ford step 5272 current loss 0.968700, current_train_items 168736.
I0302 19:00:51.707748 22447428132992 run.py:483] Algo bellman_ford step 5273 current loss 0.966663, current_train_items 168768.
I0302 19:00:51.740006 22447428132992 run.py:483] Algo bellman_ford step 5274 current loss 1.140455, current_train_items 168800.
I0302 19:00:51.759757 22447428132992 run.py:483] Algo bellman_ford step 5275 current loss 0.552646, current_train_items 168832.
I0302 19:00:51.775757 22447428132992 run.py:483] Algo bellman_ford step 5276 current loss 0.789284, current_train_items 168864.
I0302 19:00:51.798304 22447428132992 run.py:483] Algo bellman_ford step 5277 current loss 0.895395, current_train_items 168896.
I0302 19:00:51.828945 22447428132992 run.py:483] Algo bellman_ford step 5278 current loss 0.977117, current_train_items 168928.
I0302 19:00:51.862691 22447428132992 run.py:483] Algo bellman_ford step 5279 current loss 1.358635, current_train_items 168960.
I0302 19:00:51.881860 22447428132992 run.py:483] Algo bellman_ford step 5280 current loss 0.589231, current_train_items 168992.
I0302 19:00:51.898151 22447428132992 run.py:483] Algo bellman_ford step 5281 current loss 0.779800, current_train_items 169024.
I0302 19:00:51.922342 22447428132992 run.py:483] Algo bellman_ford step 5282 current loss 1.118665, current_train_items 169056.
I0302 19:00:51.953014 22447428132992 run.py:483] Algo bellman_ford step 5283 current loss 1.113980, current_train_items 169088.
I0302 19:00:51.985579 22447428132992 run.py:483] Algo bellman_ford step 5284 current loss 1.300467, current_train_items 169120.
I0302 19:00:52.005298 22447428132992 run.py:483] Algo bellman_ford step 5285 current loss 0.569128, current_train_items 169152.
I0302 19:00:52.021502 22447428132992 run.py:483] Algo bellman_ford step 5286 current loss 0.691307, current_train_items 169184.
I0302 19:00:52.045488 22447428132992 run.py:483] Algo bellman_ford step 5287 current loss 1.061550, current_train_items 169216.
I0302 19:00:52.075846 22447428132992 run.py:483] Algo bellman_ford step 5288 current loss 1.008593, current_train_items 169248.
I0302 19:00:52.109798 22447428132992 run.py:483] Algo bellman_ford step 5289 current loss 1.199367, current_train_items 169280.
I0302 19:00:52.129447 22447428132992 run.py:483] Algo bellman_ford step 5290 current loss 0.615284, current_train_items 169312.
I0302 19:00:52.145948 22447428132992 run.py:483] Algo bellman_ford step 5291 current loss 0.705911, current_train_items 169344.
I0302 19:00:52.168588 22447428132992 run.py:483] Algo bellman_ford step 5292 current loss 0.876131, current_train_items 169376.
I0302 19:00:52.199226 22447428132992 run.py:483] Algo bellman_ford step 5293 current loss 1.010897, current_train_items 169408.
I0302 19:00:52.231578 22447428132992 run.py:483] Algo bellman_ford step 5294 current loss 1.142236, current_train_items 169440.
I0302 19:00:52.250773 22447428132992 run.py:483] Algo bellman_ford step 5295 current loss 0.518817, current_train_items 169472.
I0302 19:00:52.266681 22447428132992 run.py:483] Algo bellman_ford step 5296 current loss 0.676861, current_train_items 169504.
I0302 19:00:52.289222 22447428132992 run.py:483] Algo bellman_ford step 5297 current loss 0.834196, current_train_items 169536.
I0302 19:00:52.320716 22447428132992 run.py:483] Algo bellman_ford step 5298 current loss 0.918931, current_train_items 169568.
I0302 19:00:52.356310 22447428132992 run.py:483] Algo bellman_ford step 5299 current loss 1.210784, current_train_items 169600.
I0302 19:00:52.376074 22447428132992 run.py:483] Algo bellman_ford step 5300 current loss 0.541323, current_train_items 169632.
I0302 19:00:52.383966 22447428132992 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:52.384073 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 19:00:52.400223 22447428132992 run.py:483] Algo bellman_ford step 5301 current loss 0.693427, current_train_items 169664.
I0302 19:00:52.424518 22447428132992 run.py:483] Algo bellman_ford step 5302 current loss 1.032576, current_train_items 169696.
I0302 19:00:52.456600 22447428132992 run.py:483] Algo bellman_ford step 5303 current loss 1.026819, current_train_items 169728.
I0302 19:00:52.491359 22447428132992 run.py:483] Algo bellman_ford step 5304 current loss 1.236095, current_train_items 169760.
I0302 19:00:52.511516 22447428132992 run.py:483] Algo bellman_ford step 5305 current loss 0.560668, current_train_items 169792.
I0302 19:00:52.527510 22447428132992 run.py:483] Algo bellman_ford step 5306 current loss 0.799917, current_train_items 169824.
I0302 19:00:52.552235 22447428132992 run.py:483] Algo bellman_ford step 5307 current loss 0.969508, current_train_items 169856.
I0302 19:00:52.584012 22447428132992 run.py:483] Algo bellman_ford step 5308 current loss 1.044034, current_train_items 169888.
I0302 19:00:52.616188 22447428132992 run.py:483] Algo bellman_ford step 5309 current loss 0.990363, current_train_items 169920.
I0302 19:00:52.635791 22447428132992 run.py:483] Algo bellman_ford step 5310 current loss 0.675866, current_train_items 169952.
I0302 19:00:52.651389 22447428132992 run.py:483] Algo bellman_ford step 5311 current loss 0.714733, current_train_items 169984.
I0302 19:00:52.674654 22447428132992 run.py:483] Algo bellman_ford step 5312 current loss 0.875881, current_train_items 170016.
I0302 19:00:52.705356 22447428132992 run.py:483] Algo bellman_ford step 5313 current loss 1.050808, current_train_items 170048.
I0302 19:00:52.739871 22447428132992 run.py:483] Algo bellman_ford step 5314 current loss 1.332719, current_train_items 170080.
I0302 19:00:52.759387 22447428132992 run.py:483] Algo bellman_ford step 5315 current loss 0.670732, current_train_items 170112.
I0302 19:00:52.775354 22447428132992 run.py:483] Algo bellman_ford step 5316 current loss 0.781349, current_train_items 170144.
I0302 19:00:52.798922 22447428132992 run.py:483] Algo bellman_ford step 5317 current loss 0.989806, current_train_items 170176.
I0302 19:00:52.828284 22447428132992 run.py:483] Algo bellman_ford step 5318 current loss 1.036793, current_train_items 170208.
I0302 19:00:52.862551 22447428132992 run.py:483] Algo bellman_ford step 5319 current loss 1.234183, current_train_items 170240.
I0302 19:00:52.882040 22447428132992 run.py:483] Algo bellman_ford step 5320 current loss 0.601166, current_train_items 170272.
I0302 19:00:52.898071 22447428132992 run.py:483] Algo bellman_ford step 5321 current loss 0.762215, current_train_items 170304.
I0302 19:00:52.922201 22447428132992 run.py:483] Algo bellman_ford step 5322 current loss 1.007956, current_train_items 170336.
I0302 19:00:52.951441 22447428132992 run.py:483] Algo bellman_ford step 5323 current loss 0.979746, current_train_items 170368.
I0302 19:00:52.986100 22447428132992 run.py:483] Algo bellman_ford step 5324 current loss 1.110453, current_train_items 170400.
I0302 19:00:53.005647 22447428132992 run.py:483] Algo bellman_ford step 5325 current loss 0.536403, current_train_items 170432.
I0302 19:00:53.021659 22447428132992 run.py:483] Algo bellman_ford step 5326 current loss 0.749583, current_train_items 170464.
I0302 19:00:53.044487 22447428132992 run.py:483] Algo bellman_ford step 5327 current loss 0.906723, current_train_items 170496.
I0302 19:00:53.076287 22447428132992 run.py:483] Algo bellman_ford step 5328 current loss 1.027059, current_train_items 170528.
I0302 19:00:53.108984 22447428132992 run.py:483] Algo bellman_ford step 5329 current loss 1.228675, current_train_items 170560.
I0302 19:00:53.128616 22447428132992 run.py:483] Algo bellman_ford step 5330 current loss 0.613191, current_train_items 170592.
I0302 19:00:53.144685 22447428132992 run.py:483] Algo bellman_ford step 5331 current loss 0.766956, current_train_items 170624.
I0302 19:00:53.168089 22447428132992 run.py:483] Algo bellman_ford step 5332 current loss 0.974872, current_train_items 170656.
I0302 19:00:53.197634 22447428132992 run.py:483] Algo bellman_ford step 5333 current loss 0.998370, current_train_items 170688.
I0302 19:00:53.229945 22447428132992 run.py:483] Algo bellman_ford step 5334 current loss 1.046222, current_train_items 170720.
I0302 19:00:53.249879 22447428132992 run.py:483] Algo bellman_ford step 5335 current loss 0.630612, current_train_items 170752.
I0302 19:00:53.266222 22447428132992 run.py:483] Algo bellman_ford step 5336 current loss 0.785525, current_train_items 170784.
I0302 19:00:53.289262 22447428132992 run.py:483] Algo bellman_ford step 5337 current loss 1.037262, current_train_items 170816.
I0302 19:00:53.319349 22447428132992 run.py:483] Algo bellman_ford step 5338 current loss 1.019771, current_train_items 170848.
I0302 19:00:53.353611 22447428132992 run.py:483] Algo bellman_ford step 5339 current loss 1.165341, current_train_items 170880.
I0302 19:00:53.372973 22447428132992 run.py:483] Algo bellman_ford step 5340 current loss 0.550204, current_train_items 170912.
I0302 19:00:53.389185 22447428132992 run.py:483] Algo bellman_ford step 5341 current loss 0.852196, current_train_items 170944.
I0302 19:00:53.414408 22447428132992 run.py:483] Algo bellman_ford step 5342 current loss 1.012329, current_train_items 170976.
I0302 19:00:53.445683 22447428132992 run.py:483] Algo bellman_ford step 5343 current loss 1.115943, current_train_items 171008.
I0302 19:00:53.478848 22447428132992 run.py:483] Algo bellman_ford step 5344 current loss 1.147728, current_train_items 171040.
I0302 19:00:53.498219 22447428132992 run.py:483] Algo bellman_ford step 5345 current loss 0.539225, current_train_items 171072.
I0302 19:00:53.513745 22447428132992 run.py:483] Algo bellman_ford step 5346 current loss 0.681294, current_train_items 171104.
I0302 19:00:53.537001 22447428132992 run.py:483] Algo bellman_ford step 5347 current loss 0.891991, current_train_items 171136.
I0302 19:00:53.568814 22447428132992 run.py:483] Algo bellman_ford step 5348 current loss 1.042730, current_train_items 171168.
I0302 19:00:53.602676 22447428132992 run.py:483] Algo bellman_ford step 5349 current loss 1.146694, current_train_items 171200.
I0302 19:00:53.622459 22447428132992 run.py:483] Algo bellman_ford step 5350 current loss 0.541441, current_train_items 171232.
I0302 19:00:53.630720 22447428132992 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.91796875, 'score': 0.91796875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:53.630823 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.918, val scores are: bellman_ford: 0.918
I0302 19:00:53.647015 22447428132992 run.py:483] Algo bellman_ford step 5351 current loss 0.662688, current_train_items 171264.
I0302 19:00:53.671074 22447428132992 run.py:483] Algo bellman_ford step 5352 current loss 0.947655, current_train_items 171296.
I0302 19:00:53.701010 22447428132992 run.py:483] Algo bellman_ford step 5353 current loss 1.064692, current_train_items 171328.
I0302 19:00:53.734497 22447428132992 run.py:483] Algo bellman_ford step 5354 current loss 1.014840, current_train_items 171360.
I0302 19:00:53.754768 22447428132992 run.py:483] Algo bellman_ford step 5355 current loss 0.602235, current_train_items 171392.
I0302 19:00:53.770702 22447428132992 run.py:483] Algo bellman_ford step 5356 current loss 0.802151, current_train_items 171424.
I0302 19:00:53.793995 22447428132992 run.py:483] Algo bellman_ford step 5357 current loss 1.006431, current_train_items 171456.
I0302 19:00:53.823970 22447428132992 run.py:483] Algo bellman_ford step 5358 current loss 0.880709, current_train_items 171488.
I0302 19:00:53.857314 22447428132992 run.py:483] Algo bellman_ford step 5359 current loss 1.227086, current_train_items 171520.
I0302 19:00:53.877362 22447428132992 run.py:483] Algo bellman_ford step 5360 current loss 0.586309, current_train_items 171552.
I0302 19:00:53.893586 22447428132992 run.py:483] Algo bellman_ford step 5361 current loss 0.822969, current_train_items 171584.
I0302 19:00:53.917227 22447428132992 run.py:483] Algo bellman_ford step 5362 current loss 0.953310, current_train_items 171616.
I0302 19:00:53.948331 22447428132992 run.py:483] Algo bellman_ford step 5363 current loss 0.981494, current_train_items 171648.
I0302 19:00:53.981725 22447428132992 run.py:483] Algo bellman_ford step 5364 current loss 1.051901, current_train_items 171680.
I0302 19:00:54.001556 22447428132992 run.py:483] Algo bellman_ford step 5365 current loss 0.580168, current_train_items 171712.
I0302 19:00:54.017667 22447428132992 run.py:483] Algo bellman_ford step 5366 current loss 0.736610, current_train_items 171744.
I0302 19:00:54.041051 22447428132992 run.py:483] Algo bellman_ford step 5367 current loss 0.951444, current_train_items 171776.
I0302 19:00:54.071485 22447428132992 run.py:483] Algo bellman_ford step 5368 current loss 1.037474, current_train_items 171808.
I0302 19:00:54.104148 22447428132992 run.py:483] Algo bellman_ford step 5369 current loss 1.052055, current_train_items 171840.
I0302 19:00:54.123964 22447428132992 run.py:483] Algo bellman_ford step 5370 current loss 0.596760, current_train_items 171872.
I0302 19:00:54.140089 22447428132992 run.py:483] Algo bellman_ford step 5371 current loss 0.659420, current_train_items 171904.
I0302 19:00:54.163046 22447428132992 run.py:483] Algo bellman_ford step 5372 current loss 0.868885, current_train_items 171936.
I0302 19:00:54.193516 22447428132992 run.py:483] Algo bellman_ford step 5373 current loss 1.043555, current_train_items 171968.
I0302 19:00:54.224375 22447428132992 run.py:483] Algo bellman_ford step 5374 current loss 1.005086, current_train_items 172000.
I0302 19:00:54.244027 22447428132992 run.py:483] Algo bellman_ford step 5375 current loss 0.543681, current_train_items 172032.
I0302 19:00:54.260193 22447428132992 run.py:483] Algo bellman_ford step 5376 current loss 0.804776, current_train_items 172064.
I0302 19:00:54.283392 22447428132992 run.py:483] Algo bellman_ford step 5377 current loss 1.016686, current_train_items 172096.
I0302 19:00:54.313209 22447428132992 run.py:483] Algo bellman_ford step 5378 current loss 0.903415, current_train_items 172128.
I0302 19:00:54.343932 22447428132992 run.py:483] Algo bellman_ford step 5379 current loss 0.999801, current_train_items 172160.
I0302 19:00:54.363284 22447428132992 run.py:483] Algo bellman_ford step 5380 current loss 0.770841, current_train_items 172192.
I0302 19:00:54.379460 22447428132992 run.py:483] Algo bellman_ford step 5381 current loss 0.717995, current_train_items 172224.
I0302 19:00:54.403593 22447428132992 run.py:483] Algo bellman_ford step 5382 current loss 1.051542, current_train_items 172256.
I0302 19:00:54.434335 22447428132992 run.py:483] Algo bellman_ford step 5383 current loss 1.031301, current_train_items 172288.
I0302 19:00:54.466630 22447428132992 run.py:483] Algo bellman_ford step 5384 current loss 1.137771, current_train_items 172320.
I0302 19:00:54.486602 22447428132992 run.py:483] Algo bellman_ford step 5385 current loss 0.580753, current_train_items 172352.
I0302 19:00:54.502327 22447428132992 run.py:483] Algo bellman_ford step 5386 current loss 0.756625, current_train_items 172384.
I0302 19:00:54.524868 22447428132992 run.py:483] Algo bellman_ford step 5387 current loss 0.937636, current_train_items 172416.
I0302 19:00:54.554664 22447428132992 run.py:483] Algo bellman_ford step 5388 current loss 0.923103, current_train_items 172448.
I0302 19:00:54.588277 22447428132992 run.py:483] Algo bellman_ford step 5389 current loss 1.274988, current_train_items 172480.
I0302 19:00:54.608231 22447428132992 run.py:483] Algo bellman_ford step 5390 current loss 0.628298, current_train_items 172512.
I0302 19:00:54.624749 22447428132992 run.py:483] Algo bellman_ford step 5391 current loss 0.769482, current_train_items 172544.
I0302 19:00:54.647484 22447428132992 run.py:483] Algo bellman_ford step 5392 current loss 0.832783, current_train_items 172576.
I0302 19:00:54.679064 22447428132992 run.py:483] Algo bellman_ford step 5393 current loss 1.045012, current_train_items 172608.
I0302 19:00:54.713339 22447428132992 run.py:483] Algo bellman_ford step 5394 current loss 1.191820, current_train_items 172640.
I0302 19:00:54.732963 22447428132992 run.py:483] Algo bellman_ford step 5395 current loss 0.656236, current_train_items 172672.
I0302 19:00:54.749474 22447428132992 run.py:483] Algo bellman_ford step 5396 current loss 0.829357, current_train_items 172704.
I0302 19:00:54.772827 22447428132992 run.py:483] Algo bellman_ford step 5397 current loss 0.878234, current_train_items 172736.
I0302 19:00:54.803564 22447428132992 run.py:483] Algo bellman_ford step 5398 current loss 0.999478, current_train_items 172768.
I0302 19:00:54.837802 22447428132992 run.py:483] Algo bellman_ford step 5399 current loss 1.324764, current_train_items 172800.
I0302 19:00:54.857474 22447428132992 run.py:483] Algo bellman_ford step 5400 current loss 0.653478, current_train_items 172832.
I0302 19:00:54.865233 22447428132992 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:54.865336 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:00:54.882407 22447428132992 run.py:483] Algo bellman_ford step 5401 current loss 0.800048, current_train_items 172864.
I0302 19:00:54.906353 22447428132992 run.py:483] Algo bellman_ford step 5402 current loss 0.990730, current_train_items 172896.
I0302 19:00:54.938634 22447428132992 run.py:483] Algo bellman_ford step 5403 current loss 1.217927, current_train_items 172928.
I0302 19:00:54.973210 22447428132992 run.py:483] Algo bellman_ford step 5404 current loss 1.171182, current_train_items 172960.
I0302 19:00:54.992945 22447428132992 run.py:483] Algo bellman_ford step 5405 current loss 0.536671, current_train_items 172992.
I0302 19:00:55.008480 22447428132992 run.py:483] Algo bellman_ford step 5406 current loss 0.721405, current_train_items 173024.
I0302 19:00:55.031119 22447428132992 run.py:483] Algo bellman_ford step 5407 current loss 0.901860, current_train_items 173056.
I0302 19:00:55.060060 22447428132992 run.py:483] Algo bellman_ford step 5408 current loss 0.889547, current_train_items 173088.
I0302 19:00:55.092308 22447428132992 run.py:483] Algo bellman_ford step 5409 current loss 1.006782, current_train_items 173120.
I0302 19:00:55.111717 22447428132992 run.py:483] Algo bellman_ford step 5410 current loss 0.719843, current_train_items 173152.
I0302 19:00:55.128260 22447428132992 run.py:483] Algo bellman_ford step 5411 current loss 0.821341, current_train_items 173184.
I0302 19:00:55.151662 22447428132992 run.py:483] Algo bellman_ford step 5412 current loss 1.029993, current_train_items 173216.
I0302 19:00:55.182838 22447428132992 run.py:483] Algo bellman_ford step 5413 current loss 1.025587, current_train_items 173248.
I0302 19:00:55.216503 22447428132992 run.py:483] Algo bellman_ford step 5414 current loss 1.166848, current_train_items 173280.
I0302 19:00:55.236022 22447428132992 run.py:483] Algo bellman_ford step 5415 current loss 0.600652, current_train_items 173312.
I0302 19:00:55.252424 22447428132992 run.py:483] Algo bellman_ford step 5416 current loss 0.888762, current_train_items 173344.
I0302 19:00:55.275947 22447428132992 run.py:483] Algo bellman_ford step 5417 current loss 0.979409, current_train_items 173376.
I0302 19:00:55.307947 22447428132992 run.py:483] Algo bellman_ford step 5418 current loss 1.055388, current_train_items 173408.
I0302 19:00:55.342743 22447428132992 run.py:483] Algo bellman_ford step 5419 current loss 1.224856, current_train_items 173440.
I0302 19:00:55.361969 22447428132992 run.py:483] Algo bellman_ford step 5420 current loss 0.651982, current_train_items 173472.
I0302 19:00:55.378227 22447428132992 run.py:483] Algo bellman_ford step 5421 current loss 0.791990, current_train_items 173504.
I0302 19:00:55.400974 22447428132992 run.py:483] Algo bellman_ford step 5422 current loss 0.856366, current_train_items 173536.
I0302 19:00:55.430838 22447428132992 run.py:483] Algo bellman_ford step 5423 current loss 0.966659, current_train_items 173568.
I0302 19:00:55.462150 22447428132992 run.py:483] Algo bellman_ford step 5424 current loss 1.141959, current_train_items 173600.
I0302 19:00:55.481781 22447428132992 run.py:483] Algo bellman_ford step 5425 current loss 0.660040, current_train_items 173632.
I0302 19:00:55.497545 22447428132992 run.py:483] Algo bellman_ford step 5426 current loss 0.707379, current_train_items 173664.
I0302 19:00:55.521029 22447428132992 run.py:483] Algo bellman_ford step 5427 current loss 0.956357, current_train_items 173696.
I0302 19:00:55.551298 22447428132992 run.py:483] Algo bellman_ford step 5428 current loss 1.059663, current_train_items 173728.
I0302 19:00:55.585582 22447428132992 run.py:483] Algo bellman_ford step 5429 current loss 1.052637, current_train_items 173760.
I0302 19:00:55.605023 22447428132992 run.py:483] Algo bellman_ford step 5430 current loss 0.567784, current_train_items 173792.
I0302 19:00:55.621388 22447428132992 run.py:483] Algo bellman_ford step 5431 current loss 0.723774, current_train_items 173824.
I0302 19:00:55.644853 22447428132992 run.py:483] Algo bellman_ford step 5432 current loss 0.861840, current_train_items 173856.
I0302 19:00:55.677713 22447428132992 run.py:483] Algo bellman_ford step 5433 current loss 1.258027, current_train_items 173888.
I0302 19:00:55.713008 22447428132992 run.py:483] Algo bellman_ford step 5434 current loss 1.252434, current_train_items 173920.
I0302 19:00:55.732415 22447428132992 run.py:483] Algo bellman_ford step 5435 current loss 0.524333, current_train_items 173952.
I0302 19:00:55.748450 22447428132992 run.py:483] Algo bellman_ford step 5436 current loss 0.736224, current_train_items 173984.
I0302 19:00:55.771695 22447428132992 run.py:483] Algo bellman_ford step 5437 current loss 0.968353, current_train_items 174016.
I0302 19:00:55.801884 22447428132992 run.py:483] Algo bellman_ford step 5438 current loss 1.124760, current_train_items 174048.
I0302 19:00:55.838583 22447428132992 run.py:483] Algo bellman_ford step 5439 current loss 1.369654, current_train_items 174080.
I0302 19:00:55.857797 22447428132992 run.py:483] Algo bellman_ford step 5440 current loss 0.578055, current_train_items 174112.
I0302 19:00:55.873709 22447428132992 run.py:483] Algo bellman_ford step 5441 current loss 0.698268, current_train_items 174144.
I0302 19:00:55.896191 22447428132992 run.py:483] Algo bellman_ford step 5442 current loss 0.958416, current_train_items 174176.
I0302 19:00:55.926332 22447428132992 run.py:483] Algo bellman_ford step 5443 current loss 1.053341, current_train_items 174208.
I0302 19:00:55.959680 22447428132992 run.py:483] Algo bellman_ford step 5444 current loss 1.141912, current_train_items 174240.
I0302 19:00:55.978850 22447428132992 run.py:483] Algo bellman_ford step 5445 current loss 0.552174, current_train_items 174272.
I0302 19:00:55.994889 22447428132992 run.py:483] Algo bellman_ford step 5446 current loss 0.821674, current_train_items 174304.
I0302 19:00:56.018548 22447428132992 run.py:483] Algo bellman_ford step 5447 current loss 0.957241, current_train_items 174336.
I0302 19:00:56.050250 22447428132992 run.py:483] Algo bellman_ford step 5448 current loss 1.135349, current_train_items 174368.
I0302 19:00:56.083488 22447428132992 run.py:483] Algo bellman_ford step 5449 current loss 1.036851, current_train_items 174400.
I0302 19:00:56.102661 22447428132992 run.py:483] Algo bellman_ford step 5450 current loss 0.558619, current_train_items 174432.
I0302 19:00:56.110752 22447428132992 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:00:56.110857 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:56.127061 22447428132992 run.py:483] Algo bellman_ford step 5451 current loss 0.779909, current_train_items 174464.
I0302 19:00:56.150615 22447428132992 run.py:483] Algo bellman_ford step 5452 current loss 0.920789, current_train_items 174496.
I0302 19:00:56.180387 22447428132992 run.py:483] Algo bellman_ford step 5453 current loss 1.031875, current_train_items 174528.
I0302 19:00:56.213972 22447428132992 run.py:483] Algo bellman_ford step 5454 current loss 1.143510, current_train_items 174560.
I0302 19:00:56.233698 22447428132992 run.py:483] Algo bellman_ford step 5455 current loss 0.512663, current_train_items 174592.
I0302 19:00:56.249809 22447428132992 run.py:483] Algo bellman_ford step 5456 current loss 0.681369, current_train_items 174624.
I0302 19:00:56.273715 22447428132992 run.py:483] Algo bellman_ford step 5457 current loss 0.994389, current_train_items 174656.
I0302 19:00:56.304962 22447428132992 run.py:483] Algo bellman_ford step 5458 current loss 1.030017, current_train_items 174688.
I0302 19:00:56.335777 22447428132992 run.py:483] Algo bellman_ford step 5459 current loss 1.120210, current_train_items 174720.
I0302 19:00:56.355475 22447428132992 run.py:483] Algo bellman_ford step 5460 current loss 0.724745, current_train_items 174752.
I0302 19:00:56.371920 22447428132992 run.py:483] Algo bellman_ford step 5461 current loss 0.807250, current_train_items 174784.
I0302 19:00:56.395253 22447428132992 run.py:483] Algo bellman_ford step 5462 current loss 0.860852, current_train_items 174816.
I0302 19:00:56.425385 22447428132992 run.py:483] Algo bellman_ford step 5463 current loss 0.953592, current_train_items 174848.
I0302 19:00:56.458892 22447428132992 run.py:483] Algo bellman_ford step 5464 current loss 1.157003, current_train_items 174880.
I0302 19:00:56.478160 22447428132992 run.py:483] Algo bellman_ford step 5465 current loss 0.568287, current_train_items 174912.
I0302 19:00:56.494061 22447428132992 run.py:483] Algo bellman_ford step 5466 current loss 0.761111, current_train_items 174944.
I0302 19:00:56.516820 22447428132992 run.py:483] Algo bellman_ford step 5467 current loss 0.876214, current_train_items 174976.
I0302 19:00:56.546392 22447428132992 run.py:483] Algo bellman_ford step 5468 current loss 0.908083, current_train_items 175008.
I0302 19:00:56.579517 22447428132992 run.py:483] Algo bellman_ford step 5469 current loss 1.078398, current_train_items 175040.
I0302 19:00:56.599581 22447428132992 run.py:483] Algo bellman_ford step 5470 current loss 0.583005, current_train_items 175072.
I0302 19:00:56.615871 22447428132992 run.py:483] Algo bellman_ford step 5471 current loss 0.871849, current_train_items 175104.
I0302 19:00:56.638703 22447428132992 run.py:483] Algo bellman_ford step 5472 current loss 0.922772, current_train_items 175136.
I0302 19:00:56.668919 22447428132992 run.py:483] Algo bellman_ford step 5473 current loss 0.988808, current_train_items 175168.
I0302 19:00:56.703037 22447428132992 run.py:483] Algo bellman_ford step 5474 current loss 1.229812, current_train_items 175200.
I0302 19:00:56.722605 22447428132992 run.py:483] Algo bellman_ford step 5475 current loss 0.564661, current_train_items 175232.
I0302 19:00:56.738770 22447428132992 run.py:483] Algo bellman_ford step 5476 current loss 0.808219, current_train_items 175264.
I0302 19:00:56.761419 22447428132992 run.py:483] Algo bellman_ford step 5477 current loss 0.901858, current_train_items 175296.
I0302 19:00:56.792211 22447428132992 run.py:483] Algo bellman_ford step 5478 current loss 1.001440, current_train_items 175328.
I0302 19:00:56.824975 22447428132992 run.py:483] Algo bellman_ford step 5479 current loss 1.259696, current_train_items 175360.
I0302 19:00:56.844935 22447428132992 run.py:483] Algo bellman_ford step 5480 current loss 0.921274, current_train_items 175392.
I0302 19:00:56.861525 22447428132992 run.py:483] Algo bellman_ford step 5481 current loss 0.843147, current_train_items 175424.
I0302 19:00:56.885817 22447428132992 run.py:483] Algo bellman_ford step 5482 current loss 1.032469, current_train_items 175456.
I0302 19:00:56.917197 22447428132992 run.py:483] Algo bellman_ford step 5483 current loss 1.053004, current_train_items 175488.
I0302 19:00:56.952304 22447428132992 run.py:483] Algo bellman_ford step 5484 current loss 1.168242, current_train_items 175520.
I0302 19:00:56.972172 22447428132992 run.py:483] Algo bellman_ford step 5485 current loss 0.651802, current_train_items 175552.
I0302 19:00:56.988138 22447428132992 run.py:483] Algo bellman_ford step 5486 current loss 0.731953, current_train_items 175584.
I0302 19:00:57.010656 22447428132992 run.py:483] Algo bellman_ford step 5487 current loss 0.894134, current_train_items 175616.
I0302 19:00:57.042599 22447428132992 run.py:483] Algo bellman_ford step 5488 current loss 1.029875, current_train_items 175648.
I0302 19:00:57.074958 22447428132992 run.py:483] Algo bellman_ford step 5489 current loss 1.078213, current_train_items 175680.
I0302 19:00:57.094579 22447428132992 run.py:483] Algo bellman_ford step 5490 current loss 0.571460, current_train_items 175712.
I0302 19:00:57.110791 22447428132992 run.py:483] Algo bellman_ford step 5491 current loss 0.736795, current_train_items 175744.
I0302 19:00:57.134344 22447428132992 run.py:483] Algo bellman_ford step 5492 current loss 0.965824, current_train_items 175776.
I0302 19:00:57.164675 22447428132992 run.py:483] Algo bellman_ford step 5493 current loss 0.991545, current_train_items 175808.
I0302 19:00:57.197817 22447428132992 run.py:483] Algo bellman_ford step 5494 current loss 1.048423, current_train_items 175840.
I0302 19:00:57.217132 22447428132992 run.py:483] Algo bellman_ford step 5495 current loss 0.860373, current_train_items 175872.
I0302 19:00:57.233486 22447428132992 run.py:483] Algo bellman_ford step 5496 current loss 0.841859, current_train_items 175904.
I0302 19:00:57.256978 22447428132992 run.py:483] Algo bellman_ford step 5497 current loss 0.952591, current_train_items 175936.
I0302 19:00:57.287793 22447428132992 run.py:483] Algo bellman_ford step 5498 current loss 0.858033, current_train_items 175968.
I0302 19:00:57.321996 22447428132992 run.py:483] Algo bellman_ford step 5499 current loss 1.124930, current_train_items 176000.
I0302 19:00:57.341885 22447428132992 run.py:483] Algo bellman_ford step 5500 current loss 0.565104, current_train_items 176032.
I0302 19:00:57.349674 22447428132992 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:00:57.349781 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:57.366765 22447428132992 run.py:483] Algo bellman_ford step 5501 current loss 0.817439, current_train_items 176064.
I0302 19:00:57.391057 22447428132992 run.py:483] Algo bellman_ford step 5502 current loss 0.957981, current_train_items 176096.
I0302 19:00:57.421787 22447428132992 run.py:483] Algo bellman_ford step 5503 current loss 0.985457, current_train_items 176128.
I0302 19:00:57.456137 22447428132992 run.py:483] Algo bellman_ford step 5504 current loss 1.194409, current_train_items 176160.
I0302 19:00:57.476022 22447428132992 run.py:483] Algo bellman_ford step 5505 current loss 0.591033, current_train_items 176192.
I0302 19:00:57.492019 22447428132992 run.py:483] Algo bellman_ford step 5506 current loss 0.773025, current_train_items 176224.
I0302 19:00:57.515199 22447428132992 run.py:483] Algo bellman_ford step 5507 current loss 0.871522, current_train_items 176256.
I0302 19:00:57.544925 22447428132992 run.py:483] Algo bellman_ford step 5508 current loss 0.997892, current_train_items 176288.
I0302 19:00:57.577997 22447428132992 run.py:483] Algo bellman_ford step 5509 current loss 1.101377, current_train_items 176320.
I0302 19:00:57.597280 22447428132992 run.py:483] Algo bellman_ford step 5510 current loss 0.605299, current_train_items 176352.
I0302 19:00:57.613703 22447428132992 run.py:483] Algo bellman_ford step 5511 current loss 0.752299, current_train_items 176384.
I0302 19:00:57.638250 22447428132992 run.py:483] Algo bellman_ford step 5512 current loss 1.041189, current_train_items 176416.
I0302 19:00:57.668623 22447428132992 run.py:483] Algo bellman_ford step 5513 current loss 0.972635, current_train_items 176448.
I0302 19:00:57.700487 22447428132992 run.py:483] Algo bellman_ford step 5514 current loss 1.159910, current_train_items 176480.
I0302 19:00:57.720172 22447428132992 run.py:483] Algo bellman_ford step 5515 current loss 0.622564, current_train_items 176512.
I0302 19:00:57.736036 22447428132992 run.py:483] Algo bellman_ford step 5516 current loss 0.705393, current_train_items 176544.
I0302 19:00:57.759871 22447428132992 run.py:483] Algo bellman_ford step 5517 current loss 0.903461, current_train_items 176576.
I0302 19:00:57.790204 22447428132992 run.py:483] Algo bellman_ford step 5518 current loss 0.982772, current_train_items 176608.
I0302 19:00:57.825011 22447428132992 run.py:483] Algo bellman_ford step 5519 current loss 1.097810, current_train_items 176640.
I0302 19:00:57.844710 22447428132992 run.py:483] Algo bellman_ford step 5520 current loss 0.641496, current_train_items 176672.
I0302 19:00:57.861177 22447428132992 run.py:483] Algo bellman_ford step 5521 current loss 0.771958, current_train_items 176704.
I0302 19:00:57.884098 22447428132992 run.py:483] Algo bellman_ford step 5522 current loss 0.922562, current_train_items 176736.
I0302 19:00:57.915514 22447428132992 run.py:483] Algo bellman_ford step 5523 current loss 0.962785, current_train_items 176768.
I0302 19:00:57.949953 22447428132992 run.py:483] Algo bellman_ford step 5524 current loss 1.386949, current_train_items 176800.
I0302 19:00:57.969608 22447428132992 run.py:483] Algo bellman_ford step 5525 current loss 0.538060, current_train_items 176832.
I0302 19:00:57.985405 22447428132992 run.py:483] Algo bellman_ford step 5526 current loss 0.847110, current_train_items 176864.
I0302 19:00:58.009591 22447428132992 run.py:483] Algo bellman_ford step 5527 current loss 0.936037, current_train_items 176896.
I0302 19:00:58.039403 22447428132992 run.py:483] Algo bellman_ford step 5528 current loss 1.059945, current_train_items 176928.
I0302 19:00:58.071697 22447428132992 run.py:483] Algo bellman_ford step 5529 current loss 1.115460, current_train_items 176960.
I0302 19:00:58.091406 22447428132992 run.py:483] Algo bellman_ford step 5530 current loss 0.662407, current_train_items 176992.
I0302 19:00:58.107566 22447428132992 run.py:483] Algo bellman_ford step 5531 current loss 0.668140, current_train_items 177024.
I0302 19:00:58.131485 22447428132992 run.py:483] Algo bellman_ford step 5532 current loss 1.019068, current_train_items 177056.
I0302 19:00:58.161795 22447428132992 run.py:483] Algo bellman_ford step 5533 current loss 1.085907, current_train_items 177088.
I0302 19:00:58.197174 22447428132992 run.py:483] Algo bellman_ford step 5534 current loss 1.199039, current_train_items 177120.
I0302 19:00:58.216551 22447428132992 run.py:483] Algo bellman_ford step 5535 current loss 0.598217, current_train_items 177152.
I0302 19:00:58.232900 22447428132992 run.py:483] Algo bellman_ford step 5536 current loss 0.741526, current_train_items 177184.
I0302 19:00:58.255612 22447428132992 run.py:483] Algo bellman_ford step 5537 current loss 0.937149, current_train_items 177216.
I0302 19:00:58.285693 22447428132992 run.py:483] Algo bellman_ford step 5538 current loss 0.943458, current_train_items 177248.
I0302 19:00:58.321058 22447428132992 run.py:483] Algo bellman_ford step 5539 current loss 1.236521, current_train_items 177280.
I0302 19:00:58.340610 22447428132992 run.py:483] Algo bellman_ford step 5540 current loss 0.523696, current_train_items 177312.
I0302 19:00:58.357109 22447428132992 run.py:483] Algo bellman_ford step 5541 current loss 0.807429, current_train_items 177344.
I0302 19:00:58.381188 22447428132992 run.py:483] Algo bellman_ford step 5542 current loss 0.951357, current_train_items 177376.
I0302 19:00:58.411774 22447428132992 run.py:483] Algo bellman_ford step 5543 current loss 1.003180, current_train_items 177408.
I0302 19:00:58.444909 22447428132992 run.py:483] Algo bellman_ford step 5544 current loss 1.189012, current_train_items 177440.
I0302 19:00:58.464651 22447428132992 run.py:483] Algo bellman_ford step 5545 current loss 0.544665, current_train_items 177472.
I0302 19:00:58.481282 22447428132992 run.py:483] Algo bellman_ford step 5546 current loss 0.862951, current_train_items 177504.
I0302 19:00:58.504740 22447428132992 run.py:483] Algo bellman_ford step 5547 current loss 0.905842, current_train_items 177536.
I0302 19:00:58.534843 22447428132992 run.py:483] Algo bellman_ford step 5548 current loss 0.935715, current_train_items 177568.
I0302 19:00:58.567553 22447428132992 run.py:483] Algo bellman_ford step 5549 current loss 1.109469, current_train_items 177600.
I0302 19:00:58.588451 22447428132992 run.py:483] Algo bellman_ford step 5550 current loss 0.708584, current_train_items 177632.
I0302 19:00:58.596488 22447428132992 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:00:58.596591 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:00:58.613140 22447428132992 run.py:483] Algo bellman_ford step 5551 current loss 0.672728, current_train_items 177664.
I0302 19:00:58.637990 22447428132992 run.py:483] Algo bellman_ford step 5552 current loss 0.989826, current_train_items 177696.
I0302 19:00:58.668698 22447428132992 run.py:483] Algo bellman_ford step 5553 current loss 1.006532, current_train_items 177728.
I0302 19:00:58.704877 22447428132992 run.py:483] Algo bellman_ford step 5554 current loss 1.200968, current_train_items 177760.
I0302 19:00:58.725354 22447428132992 run.py:483] Algo bellman_ford step 5555 current loss 0.607576, current_train_items 177792.
I0302 19:00:58.741096 22447428132992 run.py:483] Algo bellman_ford step 5556 current loss 0.738630, current_train_items 177824.
I0302 19:00:58.764746 22447428132992 run.py:483] Algo bellman_ford step 5557 current loss 0.947497, current_train_items 177856.
I0302 19:00:58.794980 22447428132992 run.py:483] Algo bellman_ford step 5558 current loss 1.007125, current_train_items 177888.
I0302 19:00:58.829084 22447428132992 run.py:483] Algo bellman_ford step 5559 current loss 1.175118, current_train_items 177920.
I0302 19:00:58.848976 22447428132992 run.py:483] Algo bellman_ford step 5560 current loss 0.574856, current_train_items 177952.
I0302 19:00:58.865035 22447428132992 run.py:483] Algo bellman_ford step 5561 current loss 0.782929, current_train_items 177984.
I0302 19:00:58.888193 22447428132992 run.py:483] Algo bellman_ford step 5562 current loss 0.901376, current_train_items 178016.
I0302 19:00:58.919467 22447428132992 run.py:483] Algo bellman_ford step 5563 current loss 1.023869, current_train_items 178048.
I0302 19:00:58.954011 22447428132992 run.py:483] Algo bellman_ford step 5564 current loss 1.075000, current_train_items 178080.
I0302 19:00:58.973861 22447428132992 run.py:483] Algo bellman_ford step 5565 current loss 0.645026, current_train_items 178112.
I0302 19:00:58.989959 22447428132992 run.py:483] Algo bellman_ford step 5566 current loss 0.783653, current_train_items 178144.
I0302 19:00:59.013837 22447428132992 run.py:483] Algo bellman_ford step 5567 current loss 0.821033, current_train_items 178176.
I0302 19:00:59.045874 22447428132992 run.py:483] Algo bellman_ford step 5568 current loss 1.054473, current_train_items 178208.
I0302 19:00:59.080131 22447428132992 run.py:483] Algo bellman_ford step 5569 current loss 1.305220, current_train_items 178240.
I0302 19:00:59.100322 22447428132992 run.py:483] Algo bellman_ford step 5570 current loss 0.636903, current_train_items 178272.
I0302 19:00:59.116315 22447428132992 run.py:483] Algo bellman_ford step 5571 current loss 0.698723, current_train_items 178304.
I0302 19:00:59.139536 22447428132992 run.py:483] Algo bellman_ford step 5572 current loss 0.901903, current_train_items 178336.
I0302 19:00:59.170177 22447428132992 run.py:483] Algo bellman_ford step 5573 current loss 0.951957, current_train_items 178368.
I0302 19:00:59.204262 22447428132992 run.py:483] Algo bellman_ford step 5574 current loss 1.130185, current_train_items 178400.
I0302 19:00:59.224404 22447428132992 run.py:483] Algo bellman_ford step 5575 current loss 0.546238, current_train_items 178432.
I0302 19:00:59.240479 22447428132992 run.py:483] Algo bellman_ford step 5576 current loss 0.867756, current_train_items 178464.
I0302 19:00:59.263154 22447428132992 run.py:483] Algo bellman_ford step 5577 current loss 0.957487, current_train_items 178496.
I0302 19:00:59.293764 22447428132992 run.py:483] Algo bellman_ford step 5578 current loss 1.084127, current_train_items 178528.
I0302 19:00:59.329104 22447428132992 run.py:483] Algo bellman_ford step 5579 current loss 1.213875, current_train_items 178560.
I0302 19:00:59.348618 22447428132992 run.py:483] Algo bellman_ford step 5580 current loss 0.568803, current_train_items 178592.
I0302 19:00:59.364286 22447428132992 run.py:483] Algo bellman_ford step 5581 current loss 0.729861, current_train_items 178624.
I0302 19:00:59.388390 22447428132992 run.py:483] Algo bellman_ford step 5582 current loss 0.897284, current_train_items 178656.
I0302 19:00:59.418813 22447428132992 run.py:483] Algo bellman_ford step 5583 current loss 1.045856, current_train_items 178688.
I0302 19:00:59.452628 22447428132992 run.py:483] Algo bellman_ford step 5584 current loss 1.218023, current_train_items 178720.
I0302 19:00:59.472802 22447428132992 run.py:483] Algo bellman_ford step 5585 current loss 0.558170, current_train_items 178752.
I0302 19:00:59.488950 22447428132992 run.py:483] Algo bellman_ford step 5586 current loss 0.717056, current_train_items 178784.
I0302 19:00:59.513695 22447428132992 run.py:483] Algo bellman_ford step 5587 current loss 0.983402, current_train_items 178816.
I0302 19:00:59.545028 22447428132992 run.py:483] Algo bellman_ford step 5588 current loss 1.113114, current_train_items 178848.
I0302 19:00:59.576596 22447428132992 run.py:483] Algo bellman_ford step 5589 current loss 1.093192, current_train_items 178880.
I0302 19:00:59.596470 22447428132992 run.py:483] Algo bellman_ford step 5590 current loss 0.623420, current_train_items 178912.
I0302 19:00:59.612567 22447428132992 run.py:483] Algo bellman_ford step 5591 current loss 0.715190, current_train_items 178944.
I0302 19:00:59.634763 22447428132992 run.py:483] Algo bellman_ford step 5592 current loss 0.893220, current_train_items 178976.
I0302 19:00:59.665462 22447428132992 run.py:483] Algo bellman_ford step 5593 current loss 1.020605, current_train_items 179008.
I0302 19:00:59.700015 22447428132992 run.py:483] Algo bellman_ford step 5594 current loss 1.014846, current_train_items 179040.
I0302 19:00:59.719313 22447428132992 run.py:483] Algo bellman_ford step 5595 current loss 0.708674, current_train_items 179072.
I0302 19:00:59.735183 22447428132992 run.py:483] Algo bellman_ford step 5596 current loss 0.719642, current_train_items 179104.
I0302 19:00:59.758657 22447428132992 run.py:483] Algo bellman_ford step 5597 current loss 0.993340, current_train_items 179136.
I0302 19:00:59.789498 22447428132992 run.py:483] Algo bellman_ford step 5598 current loss 0.938311, current_train_items 179168.
I0302 19:00:59.819373 22447428132992 run.py:483] Algo bellman_ford step 5599 current loss 1.014735, current_train_items 179200.
I0302 19:00:59.839523 22447428132992 run.py:483] Algo bellman_ford step 5600 current loss 0.604374, current_train_items 179232.
I0302 19:00:59.847369 22447428132992 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:00:59.847481 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.920, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:59.864070 22447428132992 run.py:483] Algo bellman_ford step 5601 current loss 0.749483, current_train_items 179264.
I0302 19:00:59.888330 22447428132992 run.py:483] Algo bellman_ford step 5602 current loss 1.021096, current_train_items 179296.
I0302 19:00:59.919314 22447428132992 run.py:483] Algo bellman_ford step 5603 current loss 1.065492, current_train_items 179328.
I0302 19:00:59.951705 22447428132992 run.py:483] Algo bellman_ford step 5604 current loss 1.071818, current_train_items 179360.
I0302 19:00:59.971819 22447428132992 run.py:483] Algo bellman_ford step 5605 current loss 0.539018, current_train_items 179392.
I0302 19:00:59.987408 22447428132992 run.py:483] Algo bellman_ford step 5606 current loss 0.763786, current_train_items 179424.
I0302 19:01:00.010754 22447428132992 run.py:483] Algo bellman_ford step 5607 current loss 0.852722, current_train_items 179456.
I0302 19:01:00.040578 22447428132992 run.py:483] Algo bellman_ford step 5608 current loss 0.999164, current_train_items 179488.
I0302 19:01:00.074366 22447428132992 run.py:483] Algo bellman_ford step 5609 current loss 1.181336, current_train_items 179520.
I0302 19:01:00.094051 22447428132992 run.py:483] Algo bellman_ford step 5610 current loss 0.610746, current_train_items 179552.
I0302 19:01:00.110242 22447428132992 run.py:483] Algo bellman_ford step 5611 current loss 0.758943, current_train_items 179584.
I0302 19:01:00.133329 22447428132992 run.py:483] Algo bellman_ford step 5612 current loss 0.917273, current_train_items 179616.
I0302 19:01:00.165086 22447428132992 run.py:483] Algo bellman_ford step 5613 current loss 0.992894, current_train_items 179648.
I0302 19:01:00.197458 22447428132992 run.py:483] Algo bellman_ford step 5614 current loss 1.076109, current_train_items 179680.
I0302 19:01:00.217007 22447428132992 run.py:483] Algo bellman_ford step 5615 current loss 0.567136, current_train_items 179712.
I0302 19:01:00.233388 22447428132992 run.py:483] Algo bellman_ford step 5616 current loss 0.780387, current_train_items 179744.
I0302 19:01:00.257046 22447428132992 run.py:483] Algo bellman_ford step 5617 current loss 0.929967, current_train_items 179776.
I0302 19:01:00.287131 22447428132992 run.py:483] Algo bellman_ford step 5618 current loss 0.964597, current_train_items 179808.
I0302 19:01:00.321889 22447428132992 run.py:483] Algo bellman_ford step 5619 current loss 1.115858, current_train_items 179840.
I0302 19:01:00.341396 22447428132992 run.py:483] Algo bellman_ford step 5620 current loss 0.800953, current_train_items 179872.
I0302 19:01:00.357524 22447428132992 run.py:483] Algo bellman_ford step 5621 current loss 0.766729, current_train_items 179904.
I0302 19:01:00.380769 22447428132992 run.py:483] Algo bellman_ford step 5622 current loss 0.806983, current_train_items 179936.
I0302 19:01:00.411165 22447428132992 run.py:483] Algo bellman_ford step 5623 current loss 0.959406, current_train_items 179968.
I0302 19:01:00.445075 22447428132992 run.py:483] Algo bellman_ford step 5624 current loss 1.002497, current_train_items 180000.
I0302 19:01:00.464620 22447428132992 run.py:483] Algo bellman_ford step 5625 current loss 0.528760, current_train_items 180032.
I0302 19:01:00.481115 22447428132992 run.py:483] Algo bellman_ford step 5626 current loss 0.863578, current_train_items 180064.
I0302 19:01:00.504767 22447428132992 run.py:483] Algo bellman_ford step 5627 current loss 0.952390, current_train_items 180096.
I0302 19:01:00.535907 22447428132992 run.py:483] Algo bellman_ford step 5628 current loss 1.034813, current_train_items 180128.
I0302 19:01:00.570322 22447428132992 run.py:483] Algo bellman_ford step 5629 current loss 1.099079, current_train_items 180160.
I0302 19:01:00.589775 22447428132992 run.py:483] Algo bellman_ford step 5630 current loss 0.575095, current_train_items 180192.
I0302 19:01:00.605695 22447428132992 run.py:483] Algo bellman_ford step 5631 current loss 0.691850, current_train_items 180224.
I0302 19:01:00.629879 22447428132992 run.py:483] Algo bellman_ford step 5632 current loss 1.007404, current_train_items 180256.
I0302 19:01:00.660584 22447428132992 run.py:483] Algo bellman_ford step 5633 current loss 1.026609, current_train_items 180288.
I0302 19:01:00.694393 22447428132992 run.py:483] Algo bellman_ford step 5634 current loss 1.328641, current_train_items 180320.
I0302 19:01:00.714056 22447428132992 run.py:483] Algo bellman_ford step 5635 current loss 0.664658, current_train_items 180352.
I0302 19:01:00.730289 22447428132992 run.py:483] Algo bellman_ford step 5636 current loss 0.754548, current_train_items 180384.
I0302 19:01:00.753665 22447428132992 run.py:483] Algo bellman_ford step 5637 current loss 0.975613, current_train_items 180416.
I0302 19:01:00.783347 22447428132992 run.py:483] Algo bellman_ford step 5638 current loss 0.978884, current_train_items 180448.
I0302 19:01:00.815610 22447428132992 run.py:483] Algo bellman_ford step 5639 current loss 1.159383, current_train_items 180480.
I0302 19:01:00.835332 22447428132992 run.py:483] Algo bellman_ford step 5640 current loss 0.584197, current_train_items 180512.
I0302 19:01:00.851214 22447428132992 run.py:483] Algo bellman_ford step 5641 current loss 0.728071, current_train_items 180544.
I0302 19:01:00.874752 22447428132992 run.py:483] Algo bellman_ford step 5642 current loss 0.954596, current_train_items 180576.
I0302 19:01:00.905467 22447428132992 run.py:483] Algo bellman_ford step 5643 current loss 1.025321, current_train_items 180608.
I0302 19:01:00.938595 22447428132992 run.py:483] Algo bellman_ford step 5644 current loss 1.148906, current_train_items 180640.
I0302 19:01:00.958275 22447428132992 run.py:483] Algo bellman_ford step 5645 current loss 0.568165, current_train_items 180672.
I0302 19:01:00.974870 22447428132992 run.py:483] Algo bellman_ford step 5646 current loss 0.775017, current_train_items 180704.
I0302 19:01:00.997935 22447428132992 run.py:483] Algo bellman_ford step 5647 current loss 0.914858, current_train_items 180736.
I0302 19:01:01.028635 22447428132992 run.py:483] Algo bellman_ford step 5648 current loss 0.883948, current_train_items 180768.
I0302 19:01:01.063824 22447428132992 run.py:483] Algo bellman_ford step 5649 current loss 1.075574, current_train_items 180800.
I0302 19:01:01.083480 22447428132992 run.py:483] Algo bellman_ford step 5650 current loss 0.807981, current_train_items 180832.
I0302 19:01:01.091471 22447428132992 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:01:01.091577 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.920, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:01.121371 22447428132992 run.py:483] Algo bellman_ford step 5651 current loss 0.852548, current_train_items 180864.
I0302 19:01:01.144432 22447428132992 run.py:483] Algo bellman_ford step 5652 current loss 0.889968, current_train_items 180896.
I0302 19:01:01.177123 22447428132992 run.py:483] Algo bellman_ford step 5653 current loss 1.061893, current_train_items 180928.
I0302 19:01:01.212492 22447428132992 run.py:483] Algo bellman_ford step 5654 current loss 1.142033, current_train_items 180960.
I0302 19:01:01.232430 22447428132992 run.py:483] Algo bellman_ford step 5655 current loss 0.605393, current_train_items 180992.
I0302 19:01:01.248536 22447428132992 run.py:483] Algo bellman_ford step 5656 current loss 0.841468, current_train_items 181024.
I0302 19:01:01.270797 22447428132992 run.py:483] Algo bellman_ford step 5657 current loss 0.950597, current_train_items 181056.
I0302 19:01:01.302258 22447428132992 run.py:483] Algo bellman_ford step 5658 current loss 1.070312, current_train_items 181088.
I0302 19:01:01.335899 22447428132992 run.py:483] Algo bellman_ford step 5659 current loss 1.219209, current_train_items 181120.
I0302 19:01:01.355685 22447428132992 run.py:483] Algo bellman_ford step 5660 current loss 0.667695, current_train_items 181152.
I0302 19:01:01.371598 22447428132992 run.py:483] Algo bellman_ford step 5661 current loss 0.680417, current_train_items 181184.
I0302 19:01:01.394051 22447428132992 run.py:483] Algo bellman_ford step 5662 current loss 0.871488, current_train_items 181216.
I0302 19:01:01.426578 22447428132992 run.py:483] Algo bellman_ford step 5663 current loss 1.113793, current_train_items 181248.
I0302 19:01:01.460181 22447428132992 run.py:483] Algo bellman_ford step 5664 current loss 1.090150, current_train_items 181280.
I0302 19:01:01.479412 22447428132992 run.py:483] Algo bellman_ford step 5665 current loss 0.637362, current_train_items 181312.
I0302 19:01:01.495904 22447428132992 run.py:483] Algo bellman_ford step 5666 current loss 0.793089, current_train_items 181344.
I0302 19:01:01.518744 22447428132992 run.py:483] Algo bellman_ford step 5667 current loss 0.896341, current_train_items 181376.
I0302 19:01:01.550170 22447428132992 run.py:483] Algo bellman_ford step 5668 current loss 1.098538, current_train_items 181408.
I0302 19:01:01.582575 22447428132992 run.py:483] Algo bellman_ford step 5669 current loss 0.982028, current_train_items 181440.
I0302 19:01:01.602002 22447428132992 run.py:483] Algo bellman_ford step 5670 current loss 0.589091, current_train_items 181472.
I0302 19:01:01.618058 22447428132992 run.py:483] Algo bellman_ford step 5671 current loss 0.731151, current_train_items 181504.
I0302 19:01:01.640640 22447428132992 run.py:483] Algo bellman_ford step 5672 current loss 0.903164, current_train_items 181536.
I0302 19:01:01.670875 22447428132992 run.py:483] Algo bellman_ford step 5673 current loss 0.962547, current_train_items 181568.
I0302 19:01:01.704065 22447428132992 run.py:483] Algo bellman_ford step 5674 current loss 1.026391, current_train_items 181600.
I0302 19:01:01.723452 22447428132992 run.py:483] Algo bellman_ford step 5675 current loss 0.569612, current_train_items 181632.
I0302 19:01:01.739352 22447428132992 run.py:483] Algo bellman_ford step 5676 current loss 0.755605, current_train_items 181664.
I0302 19:01:01.762505 22447428132992 run.py:483] Algo bellman_ford step 5677 current loss 1.011948, current_train_items 181696.
I0302 19:01:01.794308 22447428132992 run.py:483] Algo bellman_ford step 5678 current loss 1.117630, current_train_items 181728.
I0302 19:01:01.827583 22447428132992 run.py:483] Algo bellman_ford step 5679 current loss 1.057618, current_train_items 181760.
I0302 19:01:01.847027 22447428132992 run.py:483] Algo bellman_ford step 5680 current loss 0.604716, current_train_items 181792.
I0302 19:01:01.863250 22447428132992 run.py:483] Algo bellman_ford step 5681 current loss 0.784571, current_train_items 181824.
I0302 19:01:01.886404 22447428132992 run.py:483] Algo bellman_ford step 5682 current loss 0.965744, current_train_items 181856.
I0302 19:01:01.916754 22447428132992 run.py:483] Algo bellman_ford step 5683 current loss 1.039660, current_train_items 181888.
I0302 19:01:01.948916 22447428132992 run.py:483] Algo bellman_ford step 5684 current loss 1.023567, current_train_items 181920.
I0302 19:01:01.968576 22447428132992 run.py:483] Algo bellman_ford step 5685 current loss 0.625573, current_train_items 181952.
I0302 19:01:01.984632 22447428132992 run.py:483] Algo bellman_ford step 5686 current loss 0.836964, current_train_items 181984.
I0302 19:01:02.007945 22447428132992 run.py:483] Algo bellman_ford step 5687 current loss 0.943538, current_train_items 182016.
I0302 19:01:02.038799 22447428132992 run.py:483] Algo bellman_ford step 5688 current loss 1.059652, current_train_items 182048.
I0302 19:01:02.071489 22447428132992 run.py:483] Algo bellman_ford step 5689 current loss 1.225107, current_train_items 182080.
I0302 19:01:02.090990 22447428132992 run.py:483] Algo bellman_ford step 5690 current loss 0.605482, current_train_items 182112.
I0302 19:01:02.107690 22447428132992 run.py:483] Algo bellman_ford step 5691 current loss 0.764782, current_train_items 182144.
I0302 19:01:02.130425 22447428132992 run.py:483] Algo bellman_ford step 5692 current loss 0.953445, current_train_items 182176.
I0302 19:01:02.160833 22447428132992 run.py:483] Algo bellman_ford step 5693 current loss 1.009182, current_train_items 182208.
I0302 19:01:02.193054 22447428132992 run.py:483] Algo bellman_ford step 5694 current loss 1.060208, current_train_items 182240.
I0302 19:01:02.212507 22447428132992 run.py:483] Algo bellman_ford step 5695 current loss 0.554515, current_train_items 182272.
I0302 19:01:02.228868 22447428132992 run.py:483] Algo bellman_ford step 5696 current loss 0.783246, current_train_items 182304.
I0302 19:01:02.251928 22447428132992 run.py:483] Algo bellman_ford step 5697 current loss 1.034169, current_train_items 182336.
I0302 19:01:02.283408 22447428132992 run.py:483] Algo bellman_ford step 5698 current loss 1.138895, current_train_items 182368.
I0302 19:01:02.316960 22447428132992 run.py:483] Algo bellman_ford step 5699 current loss 1.163980, current_train_items 182400.
I0302 19:01:02.336454 22447428132992 run.py:483] Algo bellman_ford step 5700 current loss 0.609889, current_train_items 182432.
I0302 19:01:02.344371 22447428132992 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:01:02.344487 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:01:02.360756 22447428132992 run.py:483] Algo bellman_ford step 5701 current loss 0.727150, current_train_items 182464.
I0302 19:01:02.383795 22447428132992 run.py:483] Algo bellman_ford step 5702 current loss 0.896653, current_train_items 182496.
I0302 19:01:02.415326 22447428132992 run.py:483] Algo bellman_ford step 5703 current loss 1.063660, current_train_items 182528.
I0302 19:01:02.450483 22447428132992 run.py:483] Algo bellman_ford step 5704 current loss 1.254933, current_train_items 182560.
I0302 19:01:02.470646 22447428132992 run.py:483] Algo bellman_ford step 5705 current loss 0.553913, current_train_items 182592.
I0302 19:01:02.486337 22447428132992 run.py:483] Algo bellman_ford step 5706 current loss 0.719462, current_train_items 182624.
I0302 19:01:02.510388 22447428132992 run.py:483] Algo bellman_ford step 5707 current loss 1.015471, current_train_items 182656.
I0302 19:01:02.540993 22447428132992 run.py:483] Algo bellman_ford step 5708 current loss 1.011604, current_train_items 182688.
I0302 19:01:02.574600 22447428132992 run.py:483] Algo bellman_ford step 5709 current loss 1.217040, current_train_items 182720.
I0302 19:01:02.594352 22447428132992 run.py:483] Algo bellman_ford step 5710 current loss 0.569659, current_train_items 182752.
I0302 19:01:02.610244 22447428132992 run.py:483] Algo bellman_ford step 5711 current loss 0.887552, current_train_items 182784.
I0302 19:01:02.632934 22447428132992 run.py:483] Algo bellman_ford step 5712 current loss 0.917486, current_train_items 182816.
I0302 19:01:02.663063 22447428132992 run.py:483] Algo bellman_ford step 5713 current loss 0.994014, current_train_items 182848.
I0302 19:01:02.695939 22447428132992 run.py:483] Algo bellman_ford step 5714 current loss 1.058758, current_train_items 182880.
I0302 19:01:02.715687 22447428132992 run.py:483] Algo bellman_ford step 5715 current loss 0.752595, current_train_items 182912.
I0302 19:01:02.731806 22447428132992 run.py:483] Algo bellman_ford step 5716 current loss 0.837475, current_train_items 182944.
I0302 19:01:02.755739 22447428132992 run.py:483] Algo bellman_ford step 5717 current loss 0.844553, current_train_items 182976.
I0302 19:01:02.785797 22447428132992 run.py:483] Algo bellman_ford step 5718 current loss 1.085513, current_train_items 183008.
I0302 19:01:02.819614 22447428132992 run.py:483] Algo bellman_ford step 5719 current loss 1.372741, current_train_items 183040.
I0302 19:01:02.839174 22447428132992 run.py:483] Algo bellman_ford step 5720 current loss 0.532780, current_train_items 183072.
I0302 19:01:02.855197 22447428132992 run.py:483] Algo bellman_ford step 5721 current loss 0.738414, current_train_items 183104.
I0302 19:01:02.878087 22447428132992 run.py:483] Algo bellman_ford step 5722 current loss 0.930879, current_train_items 183136.
I0302 19:01:02.908656 22447428132992 run.py:483] Algo bellman_ford step 5723 current loss 1.010268, current_train_items 183168.
I0302 19:01:02.941202 22447428132992 run.py:483] Algo bellman_ford step 5724 current loss 1.296384, current_train_items 183200.
I0302 19:01:02.960973 22447428132992 run.py:483] Algo bellman_ford step 5725 current loss 0.630920, current_train_items 183232.
I0302 19:01:02.977141 22447428132992 run.py:483] Algo bellman_ford step 5726 current loss 0.727304, current_train_items 183264.
I0302 19:01:03.001095 22447428132992 run.py:483] Algo bellman_ford step 5727 current loss 0.998094, current_train_items 183296.
I0302 19:01:03.030878 22447428132992 run.py:483] Algo bellman_ford step 5728 current loss 0.986958, current_train_items 183328.
I0302 19:01:03.064223 22447428132992 run.py:483] Algo bellman_ford step 5729 current loss 1.199165, current_train_items 183360.
I0302 19:01:03.083710 22447428132992 run.py:483] Algo bellman_ford step 5730 current loss 0.532169, current_train_items 183392.
I0302 19:01:03.100110 22447428132992 run.py:483] Algo bellman_ford step 5731 current loss 0.793993, current_train_items 183424.
I0302 19:01:03.124170 22447428132992 run.py:483] Algo bellman_ford step 5732 current loss 1.093413, current_train_items 183456.
I0302 19:01:03.154408 22447428132992 run.py:483] Algo bellman_ford step 5733 current loss 0.997794, current_train_items 183488.
I0302 19:01:03.188456 22447428132992 run.py:483] Algo bellman_ford step 5734 current loss 1.192444, current_train_items 183520.
I0302 19:01:03.207935 22447428132992 run.py:483] Algo bellman_ford step 5735 current loss 0.561123, current_train_items 183552.
I0302 19:01:03.224350 22447428132992 run.py:483] Algo bellman_ford step 5736 current loss 0.866000, current_train_items 183584.
I0302 19:01:03.247783 22447428132992 run.py:483] Algo bellman_ford step 5737 current loss 0.964839, current_train_items 183616.
I0302 19:01:03.278157 22447428132992 run.py:483] Algo bellman_ford step 5738 current loss 1.093765, current_train_items 183648.
I0302 19:01:03.311903 22447428132992 run.py:483] Algo bellman_ford step 5739 current loss 1.155072, current_train_items 183680.
I0302 19:01:03.331523 22447428132992 run.py:483] Algo bellman_ford step 5740 current loss 0.593984, current_train_items 183712.
I0302 19:01:03.347996 22447428132992 run.py:483] Algo bellman_ford step 5741 current loss 0.813751, current_train_items 183744.
I0302 19:01:03.372476 22447428132992 run.py:483] Algo bellman_ford step 5742 current loss 0.836497, current_train_items 183776.
I0302 19:01:03.404037 22447428132992 run.py:483] Algo bellman_ford step 5743 current loss 1.041741, current_train_items 183808.
I0302 19:01:03.438032 22447428132992 run.py:483] Algo bellman_ford step 5744 current loss 1.213544, current_train_items 183840.
I0302 19:01:03.457504 22447428132992 run.py:483] Algo bellman_ford step 5745 current loss 0.719830, current_train_items 183872.
I0302 19:01:03.474049 22447428132992 run.py:483] Algo bellman_ford step 5746 current loss 0.813519, current_train_items 183904.
I0302 19:01:03.497643 22447428132992 run.py:483] Algo bellman_ford step 5747 current loss 0.875011, current_train_items 183936.
I0302 19:01:03.528615 22447428132992 run.py:483] Algo bellman_ford step 5748 current loss 1.217234, current_train_items 183968.
I0302 19:01:03.561842 22447428132992 run.py:483] Algo bellman_ford step 5749 current loss 1.149894, current_train_items 184000.
I0302 19:01:03.581467 22447428132992 run.py:483] Algo bellman_ford step 5750 current loss 0.653553, current_train_items 184032.
I0302 19:01:03.589486 22447428132992 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:01:03.589591 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:03.606631 22447428132992 run.py:483] Algo bellman_ford step 5751 current loss 0.742212, current_train_items 184064.
I0302 19:01:03.629745 22447428132992 run.py:483] Algo bellman_ford step 5752 current loss 0.968959, current_train_items 184096.
I0302 19:01:03.659572 22447428132992 run.py:483] Algo bellman_ford step 5753 current loss 0.985334, current_train_items 184128.
I0302 19:01:03.694281 22447428132992 run.py:483] Algo bellman_ford step 5754 current loss 1.313786, current_train_items 184160.
I0302 19:01:03.714354 22447428132992 run.py:483] Algo bellman_ford step 5755 current loss 0.560368, current_train_items 184192.
I0302 19:01:03.729863 22447428132992 run.py:483] Algo bellman_ford step 5756 current loss 0.709142, current_train_items 184224.
I0302 19:01:03.753567 22447428132992 run.py:483] Algo bellman_ford step 5757 current loss 0.973046, current_train_items 184256.
I0302 19:01:03.785034 22447428132992 run.py:483] Algo bellman_ford step 5758 current loss 1.101143, current_train_items 184288.
I0302 19:01:03.818668 22447428132992 run.py:483] Algo bellman_ford step 5759 current loss 1.103118, current_train_items 184320.
I0302 19:01:03.837930 22447428132992 run.py:483] Algo bellman_ford step 5760 current loss 0.589215, current_train_items 184352.
I0302 19:01:03.854280 22447428132992 run.py:483] Algo bellman_ford step 5761 current loss 0.750736, current_train_items 184384.
I0302 19:01:03.877388 22447428132992 run.py:483] Algo bellman_ford step 5762 current loss 1.033605, current_train_items 184416.
I0302 19:01:03.907054 22447428132992 run.py:483] Algo bellman_ford step 5763 current loss 1.024071, current_train_items 184448.
I0302 19:01:03.940917 22447428132992 run.py:483] Algo bellman_ford step 5764 current loss 1.110745, current_train_items 184480.
I0302 19:01:03.960489 22447428132992 run.py:483] Algo bellman_ford step 5765 current loss 0.608768, current_train_items 184512.
I0302 19:01:03.976627 22447428132992 run.py:483] Algo bellman_ford step 5766 current loss 0.707009, current_train_items 184544.
I0302 19:01:03.999564 22447428132992 run.py:483] Algo bellman_ford step 5767 current loss 0.911810, current_train_items 184576.
I0302 19:01:04.031077 22447428132992 run.py:483] Algo bellman_ford step 5768 current loss 1.109996, current_train_items 184608.
I0302 19:01:04.063951 22447428132992 run.py:483] Algo bellman_ford step 5769 current loss 1.147989, current_train_items 184640.
I0302 19:01:04.083627 22447428132992 run.py:483] Algo bellman_ford step 5770 current loss 0.560010, current_train_items 184672.
I0302 19:01:04.099669 22447428132992 run.py:483] Algo bellman_ford step 5771 current loss 0.700222, current_train_items 184704.
I0302 19:01:04.121467 22447428132992 run.py:483] Algo bellman_ford step 5772 current loss 0.918592, current_train_items 184736.
I0302 19:01:04.151534 22447428132992 run.py:483] Algo bellman_ford step 5773 current loss 1.041161, current_train_items 184768.
I0302 19:01:04.183844 22447428132992 run.py:483] Algo bellman_ford step 5774 current loss 1.036252, current_train_items 184800.
I0302 19:01:04.203344 22447428132992 run.py:483] Algo bellman_ford step 5775 current loss 0.917716, current_train_items 184832.
I0302 19:01:04.219777 22447428132992 run.py:483] Algo bellman_ford step 5776 current loss 0.798638, current_train_items 184864.
I0302 19:01:04.241905 22447428132992 run.py:483] Algo bellman_ford step 5777 current loss 0.864697, current_train_items 184896.
I0302 19:01:04.272600 22447428132992 run.py:483] Algo bellman_ford step 5778 current loss 0.916390, current_train_items 184928.
I0302 19:01:04.305941 22447428132992 run.py:483] Algo bellman_ford step 5779 current loss 1.199883, current_train_items 184960.
I0302 19:01:04.325224 22447428132992 run.py:483] Algo bellman_ford step 5780 current loss 0.627787, current_train_items 184992.
I0302 19:01:04.341237 22447428132992 run.py:483] Algo bellman_ford step 5781 current loss 0.726313, current_train_items 185024.
I0302 19:01:04.364873 22447428132992 run.py:483] Algo bellman_ford step 5782 current loss 0.972148, current_train_items 185056.
I0302 19:01:04.394328 22447428132992 run.py:483] Algo bellman_ford step 5783 current loss 0.921386, current_train_items 185088.
I0302 19:01:04.425662 22447428132992 run.py:483] Algo bellman_ford step 5784 current loss 1.140196, current_train_items 185120.
I0302 19:01:04.445231 22447428132992 run.py:483] Algo bellman_ford step 5785 current loss 0.644485, current_train_items 185152.
I0302 19:01:04.461151 22447428132992 run.py:483] Algo bellman_ford step 5786 current loss 0.728222, current_train_items 185184.
I0302 19:01:04.482915 22447428132992 run.py:483] Algo bellman_ford step 5787 current loss 0.909903, current_train_items 185216.
I0302 19:01:04.514045 22447428132992 run.py:483] Algo bellman_ford step 5788 current loss 1.042456, current_train_items 185248.
I0302 19:01:04.545842 22447428132992 run.py:483] Algo bellman_ford step 5789 current loss 1.452676, current_train_items 185280.
I0302 19:01:04.565366 22447428132992 run.py:483] Algo bellman_ford step 5790 current loss 0.622728, current_train_items 185312.
I0302 19:01:04.581801 22447428132992 run.py:483] Algo bellman_ford step 5791 current loss 0.900667, current_train_items 185344.
I0302 19:01:04.603773 22447428132992 run.py:483] Algo bellman_ford step 5792 current loss 1.012454, current_train_items 185376.
I0302 19:01:04.634127 22447428132992 run.py:483] Algo bellman_ford step 5793 current loss 1.074408, current_train_items 185408.
I0302 19:01:04.667018 22447428132992 run.py:483] Algo bellman_ford step 5794 current loss 1.108588, current_train_items 185440.
I0302 19:01:04.686358 22447428132992 run.py:483] Algo bellman_ford step 5795 current loss 0.535848, current_train_items 185472.
I0302 19:01:04.702223 22447428132992 run.py:483] Algo bellman_ford step 5796 current loss 0.869880, current_train_items 185504.
I0302 19:01:04.725756 22447428132992 run.py:483] Algo bellman_ford step 5797 current loss 1.010724, current_train_items 185536.
I0302 19:01:04.756356 22447428132992 run.py:483] Algo bellman_ford step 5798 current loss 0.992089, current_train_items 185568.
I0302 19:01:04.789911 22447428132992 run.py:483] Algo bellman_ford step 5799 current loss 1.146206, current_train_items 185600.
I0302 19:01:04.809402 22447428132992 run.py:483] Algo bellman_ford step 5800 current loss 0.553509, current_train_items 185632.
I0302 19:01:04.817128 22447428132992 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:01:04.817234 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:01:04.833962 22447428132992 run.py:483] Algo bellman_ford step 5801 current loss 0.797219, current_train_items 185664.
I0302 19:01:04.856558 22447428132992 run.py:483] Algo bellman_ford step 5802 current loss 0.925249, current_train_items 185696.
I0302 19:01:04.887966 22447428132992 run.py:483] Algo bellman_ford step 5803 current loss 1.115489, current_train_items 185728.
I0302 19:01:04.923038 22447428132992 run.py:483] Algo bellman_ford step 5804 current loss 1.132587, current_train_items 185760.
I0302 19:01:04.942862 22447428132992 run.py:483] Algo bellman_ford step 5805 current loss 0.650422, current_train_items 185792.
I0302 19:01:04.958822 22447428132992 run.py:483] Algo bellman_ford step 5806 current loss 0.717405, current_train_items 185824.
I0302 19:01:04.982682 22447428132992 run.py:483] Algo bellman_ford step 5807 current loss 0.974616, current_train_items 185856.
I0302 19:01:05.013709 22447428132992 run.py:483] Algo bellman_ford step 5808 current loss 1.039730, current_train_items 185888.
I0302 19:01:05.046682 22447428132992 run.py:483] Algo bellman_ford step 5809 current loss 1.031123, current_train_items 185920.
I0302 19:01:05.066260 22447428132992 run.py:483] Algo bellman_ford step 5810 current loss 0.549119, current_train_items 185952.
I0302 19:01:05.082235 22447428132992 run.py:483] Algo bellman_ford step 5811 current loss 0.719003, current_train_items 185984.
I0302 19:01:05.105735 22447428132992 run.py:483] Algo bellman_ford step 5812 current loss 0.918119, current_train_items 186016.
I0302 19:01:05.137290 22447428132992 run.py:483] Algo bellman_ford step 5813 current loss 1.063468, current_train_items 186048.
I0302 19:01:05.171391 22447428132992 run.py:483] Algo bellman_ford step 5814 current loss 1.135838, current_train_items 186080.
I0302 19:01:05.191369 22447428132992 run.py:483] Algo bellman_ford step 5815 current loss 0.608429, current_train_items 186112.
I0302 19:01:05.207586 22447428132992 run.py:483] Algo bellman_ford step 5816 current loss 0.723142, current_train_items 186144.
I0302 19:01:05.231217 22447428132992 run.py:483] Algo bellman_ford step 5817 current loss 0.914672, current_train_items 186176.
I0302 19:01:05.260692 22447428132992 run.py:483] Algo bellman_ford step 5818 current loss 0.941398, current_train_items 186208.
I0302 19:01:05.294589 22447428132992 run.py:483] Algo bellman_ford step 5819 current loss 1.171073, current_train_items 186240.
I0302 19:01:05.313857 22447428132992 run.py:483] Algo bellman_ford step 5820 current loss 0.589331, current_train_items 186272.
I0302 19:01:05.329912 22447428132992 run.py:483] Algo bellman_ford step 5821 current loss 0.646907, current_train_items 186304.
I0302 19:01:05.353193 22447428132992 run.py:483] Algo bellman_ford step 5822 current loss 0.972439, current_train_items 186336.
I0302 19:01:05.384066 22447428132992 run.py:483] Algo bellman_ford step 5823 current loss 0.974139, current_train_items 186368.
I0302 19:01:05.416840 22447428132992 run.py:483] Algo bellman_ford step 5824 current loss 1.184634, current_train_items 186400.
I0302 19:01:05.436255 22447428132992 run.py:483] Algo bellman_ford step 5825 current loss 0.600487, current_train_items 186432.
I0302 19:01:05.452470 22447428132992 run.py:483] Algo bellman_ford step 5826 current loss 0.702734, current_train_items 186464.
I0302 19:01:05.476031 22447428132992 run.py:483] Algo bellman_ford step 5827 current loss 0.884858, current_train_items 186496.
I0302 19:01:05.507312 22447428132992 run.py:483] Algo bellman_ford step 5828 current loss 0.993435, current_train_items 186528.
I0302 19:01:05.541753 22447428132992 run.py:483] Algo bellman_ford step 5829 current loss 1.237780, current_train_items 186560.
I0302 19:01:05.561425 22447428132992 run.py:483] Algo bellman_ford step 5830 current loss 0.645036, current_train_items 186592.
I0302 19:01:05.577709 22447428132992 run.py:483] Algo bellman_ford step 5831 current loss 0.773287, current_train_items 186624.
I0302 19:01:05.600317 22447428132992 run.py:483] Algo bellman_ford step 5832 current loss 0.887694, current_train_items 186656.
I0302 19:01:05.631181 22447428132992 run.py:483] Algo bellman_ford step 5833 current loss 0.928280, current_train_items 186688.
I0302 19:01:05.661213 22447428132992 run.py:483] Algo bellman_ford step 5834 current loss 0.904101, current_train_items 186720.
I0302 19:01:05.680820 22447428132992 run.py:483] Algo bellman_ford step 5835 current loss 0.602290, current_train_items 186752.
I0302 19:01:05.696277 22447428132992 run.py:483] Algo bellman_ford step 5836 current loss 0.675063, current_train_items 186784.
I0302 19:01:05.720977 22447428132992 run.py:483] Algo bellman_ford step 5837 current loss 0.997562, current_train_items 186816.
I0302 19:01:05.750228 22447428132992 run.py:483] Algo bellman_ford step 5838 current loss 0.906568, current_train_items 186848.
I0302 19:01:05.784707 22447428132992 run.py:483] Algo bellman_ford step 5839 current loss 1.124459, current_train_items 186880.
I0302 19:01:05.804204 22447428132992 run.py:483] Algo bellman_ford step 5840 current loss 0.538815, current_train_items 186912.
I0302 19:01:05.820544 22447428132992 run.py:483] Algo bellman_ford step 5841 current loss 0.725487, current_train_items 186944.
I0302 19:01:05.843210 22447428132992 run.py:483] Algo bellman_ford step 5842 current loss 0.921350, current_train_items 186976.
I0302 19:01:05.872568 22447428132992 run.py:483] Algo bellman_ford step 5843 current loss 0.950883, current_train_items 187008.
I0302 19:01:05.905329 22447428132992 run.py:483] Algo bellman_ford step 5844 current loss 1.056407, current_train_items 187040.
I0302 19:01:05.924856 22447428132992 run.py:483] Algo bellman_ford step 5845 current loss 0.959342, current_train_items 187072.
I0302 19:01:05.941080 22447428132992 run.py:483] Algo bellman_ford step 5846 current loss 0.778976, current_train_items 187104.
I0302 19:01:05.965615 22447428132992 run.py:483] Algo bellman_ford step 5847 current loss 0.934991, current_train_items 187136.
I0302 19:01:05.994665 22447428132992 run.py:483] Algo bellman_ford step 5848 current loss 0.937334, current_train_items 187168.
I0302 19:01:06.028191 22447428132992 run.py:483] Algo bellman_ford step 5849 current loss 1.092668, current_train_items 187200.
I0302 19:01:06.047426 22447428132992 run.py:483] Algo bellman_ford step 5850 current loss 0.665548, current_train_items 187232.
I0302 19:01:06.055644 22447428132992 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:01:06.055749 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:06.072490 22447428132992 run.py:483] Algo bellman_ford step 5851 current loss 0.815165, current_train_items 187264.
I0302 19:01:06.095474 22447428132992 run.py:483] Algo bellman_ford step 5852 current loss 0.989844, current_train_items 187296.
I0302 19:01:06.125557 22447428132992 run.py:483] Algo bellman_ford step 5853 current loss 0.943763, current_train_items 187328.
I0302 19:01:06.159668 22447428132992 run.py:483] Algo bellman_ford step 5854 current loss 1.158568, current_train_items 187360.
I0302 19:01:06.179169 22447428132992 run.py:483] Algo bellman_ford step 5855 current loss 0.782255, current_train_items 187392.
I0302 19:01:06.194985 22447428132992 run.py:483] Algo bellman_ford step 5856 current loss 0.903110, current_train_items 187424.
I0302 19:01:06.218585 22447428132992 run.py:483] Algo bellman_ford step 5857 current loss 0.890167, current_train_items 187456.
I0302 19:01:06.248444 22447428132992 run.py:483] Algo bellman_ford step 5858 current loss 0.952605, current_train_items 187488.
I0302 19:01:06.282191 22447428132992 run.py:483] Algo bellman_ford step 5859 current loss 1.084562, current_train_items 187520.
I0302 19:01:06.302091 22447428132992 run.py:483] Algo bellman_ford step 5860 current loss 0.660433, current_train_items 187552.
I0302 19:01:06.318409 22447428132992 run.py:483] Algo bellman_ford step 5861 current loss 0.801360, current_train_items 187584.
I0302 19:01:06.340595 22447428132992 run.py:483] Algo bellman_ford step 5862 current loss 0.977613, current_train_items 187616.
I0302 19:01:06.371424 22447428132992 run.py:483] Algo bellman_ford step 5863 current loss 1.001495, current_train_items 187648.
I0302 19:01:06.405460 22447428132992 run.py:483] Algo bellman_ford step 5864 current loss 1.310011, current_train_items 187680.
I0302 19:01:06.424874 22447428132992 run.py:483] Algo bellman_ford step 5865 current loss 0.557511, current_train_items 187712.
I0302 19:01:06.441536 22447428132992 run.py:483] Algo bellman_ford step 5866 current loss 0.757613, current_train_items 187744.
I0302 19:01:06.464615 22447428132992 run.py:483] Algo bellman_ford step 5867 current loss 0.936872, current_train_items 187776.
I0302 19:01:06.494688 22447428132992 run.py:483] Algo bellman_ford step 5868 current loss 1.179058, current_train_items 187808.
I0302 19:01:06.529665 22447428132992 run.py:483] Algo bellman_ford step 5869 current loss 1.153412, current_train_items 187840.
I0302 19:01:06.549521 22447428132992 run.py:483] Algo bellman_ford step 5870 current loss 0.569840, current_train_items 187872.
I0302 19:01:06.565714 22447428132992 run.py:483] Algo bellman_ford step 5871 current loss 0.791131, current_train_items 187904.
I0302 19:01:06.587755 22447428132992 run.py:483] Algo bellman_ford step 5872 current loss 0.851581, current_train_items 187936.
I0302 19:01:06.618020 22447428132992 run.py:483] Algo bellman_ford step 5873 current loss 0.939575, current_train_items 187968.
I0302 19:01:06.651303 22447428132992 run.py:483] Algo bellman_ford step 5874 current loss 1.177771, current_train_items 188000.
I0302 19:01:06.671034 22447428132992 run.py:483] Algo bellman_ford step 5875 current loss 0.556010, current_train_items 188032.
I0302 19:01:06.687138 22447428132992 run.py:483] Algo bellman_ford step 5876 current loss 0.786193, current_train_items 188064.
I0302 19:01:06.709198 22447428132992 run.py:483] Algo bellman_ford step 5877 current loss 0.870191, current_train_items 188096.
I0302 19:01:06.739274 22447428132992 run.py:483] Algo bellman_ford step 5878 current loss 0.995404, current_train_items 188128.
I0302 19:01:06.773024 22447428132992 run.py:483] Algo bellman_ford step 5879 current loss 1.189084, current_train_items 188160.
I0302 19:01:06.792494 22447428132992 run.py:483] Algo bellman_ford step 5880 current loss 0.660405, current_train_items 188192.
I0302 19:01:06.808394 22447428132992 run.py:483] Algo bellman_ford step 5881 current loss 0.842071, current_train_items 188224.
I0302 19:01:06.832351 22447428132992 run.py:483] Algo bellman_ford step 5882 current loss 0.926991, current_train_items 188256.
I0302 19:01:06.862598 22447428132992 run.py:483] Algo bellman_ford step 5883 current loss 1.018947, current_train_items 188288.
I0302 19:01:06.897181 22447428132992 run.py:483] Algo bellman_ford step 5884 current loss 1.134064, current_train_items 188320.
I0302 19:01:06.917106 22447428132992 run.py:483] Algo bellman_ford step 5885 current loss 0.513537, current_train_items 188352.
I0302 19:01:06.933088 22447428132992 run.py:483] Algo bellman_ford step 5886 current loss 0.748702, current_train_items 188384.
I0302 19:01:06.955868 22447428132992 run.py:483] Algo bellman_ford step 5887 current loss 0.979617, current_train_items 188416.
I0302 19:01:06.985732 22447428132992 run.py:483] Algo bellman_ford step 5888 current loss 0.904910, current_train_items 188448.
I0302 19:01:07.017685 22447428132992 run.py:483] Algo bellman_ford step 5889 current loss 1.207837, current_train_items 188480.
I0302 19:01:07.037326 22447428132992 run.py:483] Algo bellman_ford step 5890 current loss 0.574161, current_train_items 188512.
I0302 19:01:07.053419 22447428132992 run.py:483] Algo bellman_ford step 5891 current loss 0.837157, current_train_items 188544.
I0302 19:01:07.076212 22447428132992 run.py:483] Algo bellman_ford step 5892 current loss 0.938827, current_train_items 188576.
I0302 19:01:07.105890 22447428132992 run.py:483] Algo bellman_ford step 5893 current loss 0.900459, current_train_items 188608.
I0302 19:01:07.136862 22447428132992 run.py:483] Algo bellman_ford step 5894 current loss 1.016199, current_train_items 188640.
I0302 19:01:07.156260 22447428132992 run.py:483] Algo bellman_ford step 5895 current loss 0.639049, current_train_items 188672.
I0302 19:01:07.172476 22447428132992 run.py:483] Algo bellman_ford step 5896 current loss 0.922649, current_train_items 188704.
I0302 19:01:07.196471 22447428132992 run.py:483] Algo bellman_ford step 5897 current loss 0.992680, current_train_items 188736.
I0302 19:01:07.227391 22447428132992 run.py:483] Algo bellman_ford step 5898 current loss 1.028174, current_train_items 188768.
I0302 19:01:07.260533 22447428132992 run.py:483] Algo bellman_ford step 5899 current loss 1.185101, current_train_items 188800.
I0302 19:01:07.280281 22447428132992 run.py:483] Algo bellman_ford step 5900 current loss 0.674974, current_train_items 188832.
I0302 19:01:07.288030 22447428132992 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:01:07.288136 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.929, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:07.304523 22447428132992 run.py:483] Algo bellman_ford step 5901 current loss 0.773370, current_train_items 188864.
I0302 19:01:07.328729 22447428132992 run.py:483] Algo bellman_ford step 5902 current loss 0.997265, current_train_items 188896.
I0302 19:01:07.360044 22447428132992 run.py:483] Algo bellman_ford step 5903 current loss 1.014151, current_train_items 188928.
I0302 19:01:07.392097 22447428132992 run.py:483] Algo bellman_ford step 5904 current loss 1.148073, current_train_items 188960.
I0302 19:01:07.412006 22447428132992 run.py:483] Algo bellman_ford step 5905 current loss 0.548868, current_train_items 188992.
I0302 19:01:07.428210 22447428132992 run.py:483] Algo bellman_ford step 5906 current loss 0.841410, current_train_items 189024.
I0302 19:01:07.451230 22447428132992 run.py:483] Algo bellman_ford step 5907 current loss 0.912262, current_train_items 189056.
I0302 19:01:07.482048 22447428132992 run.py:483] Algo bellman_ford step 5908 current loss 0.919050, current_train_items 189088.
I0302 19:01:07.514148 22447428132992 run.py:483] Algo bellman_ford step 5909 current loss 1.104612, current_train_items 189120.
I0302 19:01:07.533914 22447428132992 run.py:483] Algo bellman_ford step 5910 current loss 0.530609, current_train_items 189152.
I0302 19:01:07.550197 22447428132992 run.py:483] Algo bellman_ford step 5911 current loss 0.850093, current_train_items 189184.
I0302 19:01:07.573809 22447428132992 run.py:483] Algo bellman_ford step 5912 current loss 1.036243, current_train_items 189216.
I0302 19:01:07.603687 22447428132992 run.py:483] Algo bellman_ford step 5913 current loss 1.019027, current_train_items 189248.
I0302 19:01:07.635936 22447428132992 run.py:483] Algo bellman_ford step 5914 current loss 1.221008, current_train_items 189280.
I0302 19:01:07.655336 22447428132992 run.py:483] Algo bellman_ford step 5915 current loss 0.598630, current_train_items 189312.
I0302 19:01:07.671666 22447428132992 run.py:483] Algo bellman_ford step 5916 current loss 0.782591, current_train_items 189344.
I0302 19:01:07.694730 22447428132992 run.py:483] Algo bellman_ford step 5917 current loss 0.885928, current_train_items 189376.
I0302 19:01:07.724684 22447428132992 run.py:483] Algo bellman_ford step 5918 current loss 0.958443, current_train_items 189408.
I0302 19:01:07.758568 22447428132992 run.py:483] Algo bellman_ford step 5919 current loss 1.158992, current_train_items 189440.
I0302 19:01:07.778349 22447428132992 run.py:483] Algo bellman_ford step 5920 current loss 0.532798, current_train_items 189472.
I0302 19:01:07.794456 22447428132992 run.py:483] Algo bellman_ford step 5921 current loss 0.739689, current_train_items 189504.
I0302 19:01:07.817260 22447428132992 run.py:483] Algo bellman_ford step 5922 current loss 0.952322, current_train_items 189536.
I0302 19:01:07.846358 22447428132992 run.py:483] Algo bellman_ford step 5923 current loss 1.046911, current_train_items 189568.
I0302 19:01:07.879410 22447428132992 run.py:483] Algo bellman_ford step 5924 current loss 1.083688, current_train_items 189600.
I0302 19:01:07.898606 22447428132992 run.py:483] Algo bellman_ford step 5925 current loss 0.488913, current_train_items 189632.
I0302 19:01:07.914426 22447428132992 run.py:483] Algo bellman_ford step 5926 current loss 0.721920, current_train_items 189664.
I0302 19:01:07.936785 22447428132992 run.py:483] Algo bellman_ford step 5927 current loss 0.819008, current_train_items 189696.
I0302 19:01:07.967759 22447428132992 run.py:483] Algo bellman_ford step 5928 current loss 1.064690, current_train_items 189728.
I0302 19:01:08.000077 22447428132992 run.py:483] Algo bellman_ford step 5929 current loss 1.146953, current_train_items 189760.
I0302 19:01:08.019683 22447428132992 run.py:483] Algo bellman_ford step 5930 current loss 0.644596, current_train_items 189792.
I0302 19:01:08.035932 22447428132992 run.py:483] Algo bellman_ford step 5931 current loss 0.752227, current_train_items 189824.
I0302 19:01:08.059036 22447428132992 run.py:483] Algo bellman_ford step 5932 current loss 1.070516, current_train_items 189856.
I0302 19:01:08.088537 22447428132992 run.py:483] Algo bellman_ford step 5933 current loss 0.938102, current_train_items 189888.
I0302 19:01:08.122872 22447428132992 run.py:483] Algo bellman_ford step 5934 current loss 1.149682, current_train_items 189920.
I0302 19:01:08.142418 22447428132992 run.py:483] Algo bellman_ford step 5935 current loss 0.518607, current_train_items 189952.
I0302 19:01:08.158087 22447428132992 run.py:483] Algo bellman_ford step 5936 current loss 0.720302, current_train_items 189984.
I0302 19:01:08.181494 22447428132992 run.py:483] Algo bellman_ford step 5937 current loss 0.997578, current_train_items 190016.
I0302 19:01:08.212786 22447428132992 run.py:483] Algo bellman_ford step 5938 current loss 1.000746, current_train_items 190048.
I0302 19:01:08.244474 22447428132992 run.py:483] Algo bellman_ford step 5939 current loss 1.170650, current_train_items 190080.
I0302 19:01:08.264430 22447428132992 run.py:483] Algo bellman_ford step 5940 current loss 0.724678, current_train_items 190112.
I0302 19:01:08.280272 22447428132992 run.py:483] Algo bellman_ford step 5941 current loss 0.711577, current_train_items 190144.
I0302 19:01:08.304004 22447428132992 run.py:483] Algo bellman_ford step 5942 current loss 0.921632, current_train_items 190176.
I0302 19:01:08.334671 22447428132992 run.py:483] Algo bellman_ford step 5943 current loss 0.918163, current_train_items 190208.
I0302 19:01:08.368324 22447428132992 run.py:483] Algo bellman_ford step 5944 current loss 1.071016, current_train_items 190240.
I0302 19:01:08.387576 22447428132992 run.py:483] Algo bellman_ford step 5945 current loss 0.570856, current_train_items 190272.
I0302 19:01:08.403824 22447428132992 run.py:483] Algo bellman_ford step 5946 current loss 0.800315, current_train_items 190304.
I0302 19:01:08.427400 22447428132992 run.py:483] Algo bellman_ford step 5947 current loss 0.990086, current_train_items 190336.
I0302 19:01:08.456642 22447428132992 run.py:483] Algo bellman_ford step 5948 current loss 0.989634, current_train_items 190368.
I0302 19:01:08.490327 22447428132992 run.py:483] Algo bellman_ford step 5949 current loss 1.295936, current_train_items 190400.
I0302 19:01:08.509745 22447428132992 run.py:483] Algo bellman_ford step 5950 current loss 0.616444, current_train_items 190432.
I0302 19:01:08.517790 22447428132992 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:01:08.517896 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.929, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:01:08.547061 22447428132992 run.py:483] Algo bellman_ford step 5951 current loss 0.734651, current_train_items 190464.
I0302 19:01:08.571162 22447428132992 run.py:483] Algo bellman_ford step 5952 current loss 0.975642, current_train_items 190496.
I0302 19:01:08.602159 22447428132992 run.py:483] Algo bellman_ford step 5953 current loss 1.074945, current_train_items 190528.
I0302 19:01:08.636112 22447428132992 run.py:483] Algo bellman_ford step 5954 current loss 1.064543, current_train_items 190560.
I0302 19:01:08.656626 22447428132992 run.py:483] Algo bellman_ford step 5955 current loss 0.705846, current_train_items 190592.
I0302 19:01:08.672907 22447428132992 run.py:483] Algo bellman_ford step 5956 current loss 0.777769, current_train_items 190624.
I0302 19:01:08.695181 22447428132992 run.py:483] Algo bellman_ford step 5957 current loss 0.876030, current_train_items 190656.
I0302 19:01:08.725453 22447428132992 run.py:483] Algo bellman_ford step 5958 current loss 0.863627, current_train_items 190688.
I0302 19:01:08.758666 22447428132992 run.py:483] Algo bellman_ford step 5959 current loss 1.002547, current_train_items 190720.
I0302 19:01:08.778788 22447428132992 run.py:483] Algo bellman_ford step 5960 current loss 0.702404, current_train_items 190752.
I0302 19:01:08.794971 22447428132992 run.py:483] Algo bellman_ford step 5961 current loss 0.792444, current_train_items 190784.
I0302 19:01:08.818719 22447428132992 run.py:483] Algo bellman_ford step 5962 current loss 0.933299, current_train_items 190816.
I0302 19:01:08.847788 22447428132992 run.py:483] Algo bellman_ford step 5963 current loss 0.951860, current_train_items 190848.
I0302 19:01:08.880789 22447428132992 run.py:483] Algo bellman_ford step 5964 current loss 1.126633, current_train_items 190880.
I0302 19:01:08.900424 22447428132992 run.py:483] Algo bellman_ford step 5965 current loss 0.566701, current_train_items 190912.
I0302 19:01:08.916358 22447428132992 run.py:483] Algo bellman_ford step 5966 current loss 0.735044, current_train_items 190944.
I0302 19:01:08.940494 22447428132992 run.py:483] Algo bellman_ford step 5967 current loss 0.859930, current_train_items 190976.
I0302 19:01:08.970433 22447428132992 run.py:483] Algo bellman_ford step 5968 current loss 0.911988, current_train_items 191008.
I0302 19:01:09.004490 22447428132992 run.py:483] Algo bellman_ford step 5969 current loss 1.134005, current_train_items 191040.
I0302 19:01:09.024289 22447428132992 run.py:483] Algo bellman_ford step 5970 current loss 0.636547, current_train_items 191072.
I0302 19:01:09.040728 22447428132992 run.py:483] Algo bellman_ford step 5971 current loss 0.763752, current_train_items 191104.
I0302 19:01:09.063371 22447428132992 run.py:483] Algo bellman_ford step 5972 current loss 0.881404, current_train_items 191136.
I0302 19:01:09.094263 22447428132992 run.py:483] Algo bellman_ford step 5973 current loss 0.960142, current_train_items 191168.
I0302 19:01:09.127180 22447428132992 run.py:483] Algo bellman_ford step 5974 current loss 0.993568, current_train_items 191200.
I0302 19:01:09.147285 22447428132992 run.py:483] Algo bellman_ford step 5975 current loss 0.642280, current_train_items 191232.
I0302 19:01:09.163302 22447428132992 run.py:483] Algo bellman_ford step 5976 current loss 0.709922, current_train_items 191264.
I0302 19:01:09.185879 22447428132992 run.py:483] Algo bellman_ford step 5977 current loss 0.906751, current_train_items 191296.
I0302 19:01:09.216517 22447428132992 run.py:483] Algo bellman_ford step 5978 current loss 1.127747, current_train_items 191328.
I0302 19:01:09.248845 22447428132992 run.py:483] Algo bellman_ford step 5979 current loss 1.135108, current_train_items 191360.
I0302 19:01:09.268085 22447428132992 run.py:483] Algo bellman_ford step 5980 current loss 0.536401, current_train_items 191392.
I0302 19:01:09.284976 22447428132992 run.py:483] Algo bellman_ford step 5981 current loss 0.761588, current_train_items 191424.
I0302 19:01:09.307420 22447428132992 run.py:483] Algo bellman_ford step 5982 current loss 0.899385, current_train_items 191456.
I0302 19:01:09.337561 22447428132992 run.py:483] Algo bellman_ford step 5983 current loss 0.943518, current_train_items 191488.
I0302 19:01:09.370286 22447428132992 run.py:483] Algo bellman_ford step 5984 current loss 1.105460, current_train_items 191520.
I0302 19:01:09.390130 22447428132992 run.py:483] Algo bellman_ford step 5985 current loss 0.609030, current_train_items 191552.
I0302 19:01:09.406676 22447428132992 run.py:483] Algo bellman_ford step 5986 current loss 0.776573, current_train_items 191584.
I0302 19:01:09.429990 22447428132992 run.py:483] Algo bellman_ford step 5987 current loss 0.978846, current_train_items 191616.
I0302 19:01:09.459513 22447428132992 run.py:483] Algo bellman_ford step 5988 current loss 1.247126, current_train_items 191648.
I0302 19:01:09.492859 22447428132992 run.py:483] Algo bellman_ford step 5989 current loss 1.149694, current_train_items 191680.
I0302 19:01:09.512828 22447428132992 run.py:483] Algo bellman_ford step 5990 current loss 0.547995, current_train_items 191712.
I0302 19:01:09.528479 22447428132992 run.py:483] Algo bellman_ford step 5991 current loss 0.703071, current_train_items 191744.
I0302 19:01:09.551649 22447428132992 run.py:483] Algo bellman_ford step 5992 current loss 0.993429, current_train_items 191776.
I0302 19:01:09.583618 22447428132992 run.py:483] Algo bellman_ford step 5993 current loss 1.074187, current_train_items 191808.
I0302 19:01:09.616816 22447428132992 run.py:483] Algo bellman_ford step 5994 current loss 1.023568, current_train_items 191840.
I0302 19:01:09.636058 22447428132992 run.py:483] Algo bellman_ford step 5995 current loss 0.561667, current_train_items 191872.
I0302 19:01:09.652100 22447428132992 run.py:483] Algo bellman_ford step 5996 current loss 0.728196, current_train_items 191904.
I0302 19:01:09.675654 22447428132992 run.py:483] Algo bellman_ford step 5997 current loss 0.930125, current_train_items 191936.
I0302 19:01:09.705590 22447428132992 run.py:483] Algo bellman_ford step 5998 current loss 1.038358, current_train_items 191968.
I0302 19:01:09.737852 22447428132992 run.py:483] Algo bellman_ford step 5999 current loss 1.014971, current_train_items 192000.
I0302 19:01:09.757797 22447428132992 run.py:483] Algo bellman_ford step 6000 current loss 0.599511, current_train_items 192032.
I0302 19:01:09.765868 22447428132992 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:01:09.765973 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.935, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:09.782610 22447428132992 run.py:483] Algo bellman_ford step 6001 current loss 0.758536, current_train_items 192064.
I0302 19:01:09.806576 22447428132992 run.py:483] Algo bellman_ford step 6002 current loss 0.873146, current_train_items 192096.
I0302 19:01:09.835884 22447428132992 run.py:483] Algo bellman_ford step 6003 current loss 0.939551, current_train_items 192128.
I0302 19:01:09.869204 22447428132992 run.py:483] Algo bellman_ford step 6004 current loss 1.071239, current_train_items 192160.
I0302 19:01:09.889231 22447428132992 run.py:483] Algo bellman_ford step 6005 current loss 0.585102, current_train_items 192192.
I0302 19:01:09.904958 22447428132992 run.py:483] Algo bellman_ford step 6006 current loss 0.640025, current_train_items 192224.
I0302 19:01:09.929075 22447428132992 run.py:483] Algo bellman_ford step 6007 current loss 0.923178, current_train_items 192256.
I0302 19:01:09.961558 22447428132992 run.py:483] Algo bellman_ford step 6008 current loss 1.108363, current_train_items 192288.
I0302 19:01:09.996704 22447428132992 run.py:483] Algo bellman_ford step 6009 current loss 1.050662, current_train_items 192320.
I0302 19:01:10.016340 22447428132992 run.py:483] Algo bellman_ford step 6010 current loss 0.557915, current_train_items 192352.
I0302 19:01:10.032462 22447428132992 run.py:483] Algo bellman_ford step 6011 current loss 0.814952, current_train_items 192384.
I0302 19:01:10.055795 22447428132992 run.py:483] Algo bellman_ford step 6012 current loss 0.910286, current_train_items 192416.
I0302 19:01:10.085744 22447428132992 run.py:483] Algo bellman_ford step 6013 current loss 0.923826, current_train_items 192448.
I0302 19:01:10.121266 22447428132992 run.py:483] Algo bellman_ford step 6014 current loss 1.164273, current_train_items 192480.
I0302 19:01:10.140753 22447428132992 run.py:483] Algo bellman_ford step 6015 current loss 0.694260, current_train_items 192512.
I0302 19:01:10.157054 22447428132992 run.py:483] Algo bellman_ford step 6016 current loss 0.787762, current_train_items 192544.
I0302 19:01:10.180065 22447428132992 run.py:483] Algo bellman_ford step 6017 current loss 0.807731, current_train_items 192576.
I0302 19:01:10.211200 22447428132992 run.py:483] Algo bellman_ford step 6018 current loss 1.023807, current_train_items 192608.
I0302 19:01:10.244724 22447428132992 run.py:483] Algo bellman_ford step 6019 current loss 1.403825, current_train_items 192640.
I0302 19:01:10.264205 22447428132992 run.py:483] Algo bellman_ford step 6020 current loss 0.591036, current_train_items 192672.
I0302 19:01:10.280600 22447428132992 run.py:483] Algo bellman_ford step 6021 current loss 0.793603, current_train_items 192704.
I0302 19:01:10.304148 22447428132992 run.py:483] Algo bellman_ford step 6022 current loss 0.865647, current_train_items 192736.
I0302 19:01:10.336550 22447428132992 run.py:483] Algo bellman_ford step 6023 current loss 1.161168, current_train_items 192768.
I0302 19:01:10.370270 22447428132992 run.py:483] Algo bellman_ford step 6024 current loss 1.043968, current_train_items 192800.
I0302 19:01:10.389957 22447428132992 run.py:483] Algo bellman_ford step 6025 current loss 0.846146, current_train_items 192832.
I0302 19:01:10.406228 22447428132992 run.py:483] Algo bellman_ford step 6026 current loss 0.969653, current_train_items 192864.
I0302 19:01:10.429769 22447428132992 run.py:483] Algo bellman_ford step 6027 current loss 0.899123, current_train_items 192896.
I0302 19:01:10.460398 22447428132992 run.py:483] Algo bellman_ford step 6028 current loss 1.078127, current_train_items 192928.
I0302 19:01:10.493970 22447428132992 run.py:483] Algo bellman_ford step 6029 current loss 1.124829, current_train_items 192960.
I0302 19:01:10.513363 22447428132992 run.py:483] Algo bellman_ford step 6030 current loss 0.565221, current_train_items 192992.
I0302 19:01:10.529237 22447428132992 run.py:483] Algo bellman_ford step 6031 current loss 0.758780, current_train_items 193024.
I0302 19:01:10.552397 22447428132992 run.py:483] Algo bellman_ford step 6032 current loss 0.838135, current_train_items 193056.
I0302 19:01:10.584456 22447428132992 run.py:483] Algo bellman_ford step 6033 current loss 1.110959, current_train_items 193088.
I0302 19:01:10.617269 22447428132992 run.py:483] Algo bellman_ford step 6034 current loss 1.152406, current_train_items 193120.
I0302 19:01:10.636718 22447428132992 run.py:483] Algo bellman_ford step 6035 current loss 0.571047, current_train_items 193152.
I0302 19:01:10.652755 22447428132992 run.py:483] Algo bellman_ford step 6036 current loss 0.663417, current_train_items 193184.
I0302 19:01:10.676450 22447428132992 run.py:483] Algo bellman_ford step 6037 current loss 1.003670, current_train_items 193216.
I0302 19:01:10.707606 22447428132992 run.py:483] Algo bellman_ford step 6038 current loss 1.018505, current_train_items 193248.
I0302 19:01:10.739774 22447428132992 run.py:483] Algo bellman_ford step 6039 current loss 1.127064, current_train_items 193280.
I0302 19:01:10.758924 22447428132992 run.py:483] Algo bellman_ford step 6040 current loss 0.558946, current_train_items 193312.
I0302 19:01:10.775049 22447428132992 run.py:483] Algo bellman_ford step 6041 current loss 0.779936, current_train_items 193344.
I0302 19:01:10.799438 22447428132992 run.py:483] Algo bellman_ford step 6042 current loss 0.945707, current_train_items 193376.
I0302 19:01:10.829813 22447428132992 run.py:483] Algo bellman_ford step 6043 current loss 0.952969, current_train_items 193408.
I0302 19:01:10.865644 22447428132992 run.py:483] Algo bellman_ford step 6044 current loss 1.181446, current_train_items 193440.
I0302 19:01:10.885215 22447428132992 run.py:483] Algo bellman_ford step 6045 current loss 0.727271, current_train_items 193472.
I0302 19:01:10.901643 22447428132992 run.py:483] Algo bellman_ford step 6046 current loss 0.771132, current_train_items 193504.
I0302 19:01:10.924687 22447428132992 run.py:483] Algo bellman_ford step 6047 current loss 0.973845, current_train_items 193536.
I0302 19:01:10.956407 22447428132992 run.py:483] Algo bellman_ford step 6048 current loss 1.073291, current_train_items 193568.
I0302 19:01:10.990846 22447428132992 run.py:483] Algo bellman_ford step 6049 current loss 1.075994, current_train_items 193600.
I0302 19:01:11.010341 22447428132992 run.py:483] Algo bellman_ford step 6050 current loss 0.654629, current_train_items 193632.
I0302 19:01:11.018342 22447428132992 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:01:11.018495 22447428132992 run.py:519] Checkpointing best model, best avg val score was 0.935, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:01:11.047311 22447428132992 run.py:483] Algo bellman_ford step 6051 current loss 0.794905, current_train_items 193664.
I0302 19:01:11.071132 22447428132992 run.py:483] Algo bellman_ford step 6052 current loss 0.884924, current_train_items 193696.
I0302 19:01:11.101897 22447428132992 run.py:483] Algo bellman_ford step 6053 current loss 1.020049, current_train_items 193728.
I0302 19:01:11.136859 22447428132992 run.py:483] Algo bellman_ford step 6054 current loss 1.101756, current_train_items 193760.
I0302 19:01:11.157169 22447428132992 run.py:483] Algo bellman_ford step 6055 current loss 0.697029, current_train_items 193792.
I0302 19:01:11.172809 22447428132992 run.py:483] Algo bellman_ford step 6056 current loss 0.739058, current_train_items 193824.
I0302 19:01:11.195794 22447428132992 run.py:483] Algo bellman_ford step 6057 current loss 0.913203, current_train_items 193856.
I0302 19:01:11.225949 22447428132992 run.py:483] Algo bellman_ford step 6058 current loss 1.008013, current_train_items 193888.
I0302 19:01:11.257188 22447428132992 run.py:483] Algo bellman_ford step 6059 current loss 1.004309, current_train_items 193920.
I0302 19:01:11.277410 22447428132992 run.py:483] Algo bellman_ford step 6060 current loss 0.605744, current_train_items 193952.
I0302 19:01:11.293704 22447428132992 run.py:483] Algo bellman_ford step 6061 current loss 0.755491, current_train_items 193984.
I0302 19:01:11.315360 22447428132992 run.py:483] Algo bellman_ford step 6062 current loss 0.806426, current_train_items 194016.
I0302 19:01:11.344813 22447428132992 run.py:483] Algo bellman_ford step 6063 current loss 1.096322, current_train_items 194048.
I0302 19:01:11.378532 22447428132992 run.py:483] Algo bellman_ford step 6064 current loss 1.107567, current_train_items 194080.
I0302 19:01:11.398259 22447428132992 run.py:483] Algo bellman_ford step 6065 current loss 0.580263, current_train_items 194112.
I0302 19:01:11.414341 22447428132992 run.py:483] Algo bellman_ford step 6066 current loss 0.751816, current_train_items 194144.
I0302 19:01:11.438454 22447428132992 run.py:483] Algo bellman_ford step 6067 current loss 0.923781, current_train_items 194176.
I0302 19:01:11.469745 22447428132992 run.py:483] Algo bellman_ford step 6068 current loss 1.004126, current_train_items 194208.
I0302 19:01:11.502472 22447428132992 run.py:483] Algo bellman_ford step 6069 current loss 1.051258, current_train_items 194240.
I0302 19:01:11.522601 22447428132992 run.py:483] Algo bellman_ford step 6070 current loss 0.691957, current_train_items 194272.
I0302 19:01:11.538579 22447428132992 run.py:483] Algo bellman_ford step 6071 current loss 0.744577, current_train_items 194304.
I0302 19:01:11.561728 22447428132992 run.py:483] Algo bellman_ford step 6072 current loss 0.923822, current_train_items 194336.
I0302 19:01:11.591694 22447428132992 run.py:483] Algo bellman_ford step 6073 current loss 0.934638, current_train_items 194368.
I0302 19:01:11.625598 22447428132992 run.py:483] Algo bellman_ford step 6074 current loss 1.148896, current_train_items 194400.
I0302 19:01:11.645678 22447428132992 run.py:483] Algo bellman_ford step 6075 current loss 0.541612, current_train_items 194432.
I0302 19:01:11.661864 22447428132992 run.py:483] Algo bellman_ford step 6076 current loss 0.705338, current_train_items 194464.
I0302 19:01:11.684223 22447428132992 run.py:483] Algo bellman_ford step 6077 current loss 0.830880, current_train_items 194496.
I0302 19:01:11.714439 22447428132992 run.py:483] Algo bellman_ford step 6078 current loss 1.027099, current_train_items 194528.
I0302 19:01:11.748526 22447428132992 run.py:483] Algo bellman_ford step 6079 current loss 1.223004, current_train_items 194560.
I0302 19:01:11.768032 22447428132992 run.py:483] Algo bellman_ford step 6080 current loss 0.602263, current_train_items 194592.
I0302 19:01:11.784253 22447428132992 run.py:483] Algo bellman_ford step 6081 current loss 0.732007, current_train_items 194624.
I0302 19:01:11.807290 22447428132992 run.py:483] Algo bellman_ford step 6082 current loss 0.876581, current_train_items 194656.
I0302 19:01:11.839314 22447428132992 run.py:483] Algo bellman_ford step 6083 current loss 1.012111, current_train_items 194688.
I0302 19:01:11.871486 22447428132992 run.py:483] Algo bellman_ford step 6084 current loss 1.126047, current_train_items 194720.
I0302 19:01:11.891360 22447428132992 run.py:483] Algo bellman_ford step 6085 current loss 0.541434, current_train_items 194752.
I0302 19:01:11.907691 22447428132992 run.py:483] Algo bellman_ford step 6086 current loss 0.830101, current_train_items 194784.
I0302 19:01:11.930601 22447428132992 run.py:483] Algo bellman_ford step 6087 current loss 0.971189, current_train_items 194816.
I0302 19:01:11.960623 22447428132992 run.py:483] Algo bellman_ford step 6088 current loss 1.091221, current_train_items 194848.
I0302 19:01:11.994680 22447428132992 run.py:483] Algo bellman_ford step 6089 current loss 1.219882, current_train_items 194880.
I0302 19:01:12.014720 22447428132992 run.py:483] Algo bellman_ford step 6090 current loss 0.810520, current_train_items 194912.
I0302 19:01:12.030956 22447428132992 run.py:483] Algo bellman_ford step 6091 current loss 0.756547, current_train_items 194944.
I0302 19:01:12.053914 22447428132992 run.py:483] Algo bellman_ford step 6092 current loss 0.912575, current_train_items 194976.
I0302 19:01:12.084165 22447428132992 run.py:483] Algo bellman_ford step 6093 current loss 0.953264, current_train_items 195008.
I0302 19:01:12.116565 22447428132992 run.py:483] Algo bellman_ford step 6094 current loss 1.232992, current_train_items 195040.
I0302 19:01:12.136471 22447428132992 run.py:483] Algo bellman_ford step 6095 current loss 0.625899, current_train_items 195072.
I0302 19:01:12.152704 22447428132992 run.py:483] Algo bellman_ford step 6096 current loss 0.641107, current_train_items 195104.
I0302 19:01:12.176472 22447428132992 run.py:483] Algo bellman_ford step 6097 current loss 0.983426, current_train_items 195136.
I0302 19:01:12.206089 22447428132992 run.py:483] Algo bellman_ford step 6098 current loss 0.954687, current_train_items 195168.
I0302 19:01:12.239855 22447428132992 run.py:483] Algo bellman_ford step 6099 current loss 1.226482, current_train_items 195200.
I0302 19:01:12.259771 22447428132992 run.py:483] Algo bellman_ford step 6100 current loss 0.508737, current_train_items 195232.
I0302 19:01:12.267814 22447428132992 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:01:12.267920 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:01:12.283915 22447428132992 run.py:483] Algo bellman_ford step 6101 current loss 0.701554, current_train_items 195264.
I0302 19:01:12.308260 22447428132992 run.py:483] Algo bellman_ford step 6102 current loss 0.853826, current_train_items 195296.
I0302 19:01:12.340155 22447428132992 run.py:483] Algo bellman_ford step 6103 current loss 1.099514, current_train_items 195328.
I0302 19:01:12.372894 22447428132992 run.py:483] Algo bellman_ford step 6104 current loss 1.131266, current_train_items 195360.
I0302 19:01:12.392570 22447428132992 run.py:483] Algo bellman_ford step 6105 current loss 0.562470, current_train_items 195392.
I0302 19:01:12.407831 22447428132992 run.py:483] Algo bellman_ford step 6106 current loss 0.786365, current_train_items 195424.
I0302 19:01:12.430920 22447428132992 run.py:483] Algo bellman_ford step 6107 current loss 1.001855, current_train_items 195456.
I0302 19:01:12.461485 22447428132992 run.py:483] Algo bellman_ford step 6108 current loss 0.924672, current_train_items 195488.
I0302 19:01:12.494255 22447428132992 run.py:483] Algo bellman_ford step 6109 current loss 1.152809, current_train_items 195520.
I0302 19:01:12.513723 22447428132992 run.py:483] Algo bellman_ford step 6110 current loss 0.529951, current_train_items 195552.
I0302 19:01:12.529681 22447428132992 run.py:483] Algo bellman_ford step 6111 current loss 0.726375, current_train_items 195584.
I0302 19:01:12.552993 22447428132992 run.py:483] Algo bellman_ford step 6112 current loss 1.045954, current_train_items 195616.
I0302 19:01:12.582895 22447428132992 run.py:483] Algo bellman_ford step 6113 current loss 0.920210, current_train_items 195648.
I0302 19:01:12.615864 22447428132992 run.py:483] Algo bellman_ford step 6114 current loss 1.014989, current_train_items 195680.
I0302 19:01:12.635524 22447428132992 run.py:483] Algo bellman_ford step 6115 current loss 0.621507, current_train_items 195712.
I0302 19:01:12.651596 22447428132992 run.py:483] Algo bellman_ford step 6116 current loss 0.834829, current_train_items 195744.
I0302 19:01:12.674902 22447428132992 run.py:483] Algo bellman_ford step 6117 current loss 0.846627, current_train_items 195776.
I0302 19:01:12.706558 22447428132992 run.py:483] Algo bellman_ford step 6118 current loss 1.074628, current_train_items 195808.
I0302 19:01:12.738930 22447428132992 run.py:483] Algo bellman_ford step 6119 current loss 1.216383, current_train_items 195840.
I0302 19:01:12.758611 22447428132992 run.py:483] Algo bellman_ford step 6120 current loss 0.611777, current_train_items 195872.
I0302 19:01:12.774875 22447428132992 run.py:483] Algo bellman_ford step 6121 current loss 0.793343, current_train_items 195904.
I0302 19:01:12.797725 22447428132992 run.py:483] Algo bellman_ford step 6122 current loss 0.864615, current_train_items 195936.
I0302 19:01:12.829284 22447428132992 run.py:483] Algo bellman_ford step 6123 current loss 1.053250, current_train_items 195968.
I0302 19:01:12.862308 22447428132992 run.py:483] Algo bellman_ford step 6124 current loss 1.352946, current_train_items 196000.
I0302 19:01:12.881901 22447428132992 run.py:483] Algo bellman_ford step 6125 current loss 0.592474, current_train_items 196032.
I0302 19:01:12.897535 22447428132992 run.py:483] Algo bellman_ford step 6126 current loss 0.775231, current_train_items 196064.
I0302 19:01:12.922364 22447428132992 run.py:483] Algo bellman_ford step 6127 current loss 1.026767, current_train_items 196096.
I0302 19:01:12.953152 22447428132992 run.py:483] Algo bellman_ford step 6128 current loss 0.881824, current_train_items 196128.
I0302 19:01:12.985724 22447428132992 run.py:483] Algo bellman_ford step 6129 current loss 0.956654, current_train_items 196160.
I0302 19:01:13.005096 22447428132992 run.py:483] Algo bellman_ford step 6130 current loss 0.651078, current_train_items 196192.
I0302 19:01:13.021074 22447428132992 run.py:483] Algo bellman_ford step 6131 current loss 0.726106, current_train_items 196224.
I0302 19:01:13.044889 22447428132992 run.py:483] Algo bellman_ford step 6132 current loss 1.040015, current_train_items 196256.
I0302 19:01:13.074601 22447428132992 run.py:483] Algo bellman_ford step 6133 current loss 0.960928, current_train_items 196288.
I0302 19:01:13.106768 22447428132992 run.py:483] Algo bellman_ford step 6134 current loss 0.982269, current_train_items 196320.
I0302 19:01:13.126023 22447428132992 run.py:483] Algo bellman_ford step 6135 current loss 0.566187, current_train_items 196352.
I0302 19:01:13.141643 22447428132992 run.py:483] Algo bellman_ford step 6136 current loss 0.763043, current_train_items 196384.
I0302 19:01:13.165261 22447428132992 run.py:483] Algo bellman_ford step 6137 current loss 0.963926, current_train_items 196416.
I0302 19:01:13.196480 22447428132992 run.py:483] Algo bellman_ford step 6138 current loss 1.083112, current_train_items 196448.
I0302 19:01:13.229691 22447428132992 run.py:483] Algo bellman_ford step 6139 current loss 1.310312, current_train_items 196480.
I0302 19:01:13.249208 22447428132992 run.py:483] Algo bellman_ford step 6140 current loss 0.582741, current_train_items 196512.
I0302 19:01:13.265239 22447428132992 run.py:483] Algo bellman_ford step 6141 current loss 0.676046, current_train_items 196544.
I0302 19:01:13.288934 22447428132992 run.py:483] Algo bellman_ford step 6142 current loss 0.830745, current_train_items 196576.
I0302 19:01:13.320109 22447428132992 run.py:483] Algo bellman_ford step 6143 current loss 0.989134, current_train_items 196608.
I0302 19:01:13.352811 22447428132992 run.py:483] Algo bellman_ford step 6144 current loss 1.436575, current_train_items 196640.
I0302 19:01:13.371961 22447428132992 run.py:483] Algo bellman_ford step 6145 current loss 0.531340, current_train_items 196672.
I0302 19:01:13.388075 22447428132992 run.py:483] Algo bellman_ford step 6146 current loss 0.853810, current_train_items 196704.
I0302 19:01:13.410851 22447428132992 run.py:483] Algo bellman_ford step 6147 current loss 0.861006, current_train_items 196736.
I0302 19:01:13.440977 22447428132992 run.py:483] Algo bellman_ford step 6148 current loss 1.001996, current_train_items 196768.
I0302 19:01:13.475837 22447428132992 run.py:483] Algo bellman_ford step 6149 current loss 1.127467, current_train_items 196800.
I0302 19:01:13.495287 22447428132992 run.py:483] Algo bellman_ford step 6150 current loss 0.957584, current_train_items 196832.
I0302 19:01:13.503434 22447428132992 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:01:13.503541 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:01:13.520071 22447428132992 run.py:483] Algo bellman_ford step 6151 current loss 0.966499, current_train_items 196864.
I0302 19:01:13.542895 22447428132992 run.py:483] Algo bellman_ford step 6152 current loss 0.883920, current_train_items 196896.
I0302 19:01:13.572499 22447428132992 run.py:483] Algo bellman_ford step 6153 current loss 0.975020, current_train_items 196928.
I0302 19:01:13.605790 22447428132992 run.py:483] Algo bellman_ford step 6154 current loss 1.013466, current_train_items 196960.
I0302 19:01:13.625521 22447428132992 run.py:483] Algo bellman_ford step 6155 current loss 0.615190, current_train_items 196992.
I0302 19:01:13.641260 22447428132992 run.py:483] Algo bellman_ford step 6156 current loss 0.794521, current_train_items 197024.
I0302 19:01:13.664066 22447428132992 run.py:483] Algo bellman_ford step 6157 current loss 0.880695, current_train_items 197056.
I0302 19:01:13.693649 22447428132992 run.py:483] Algo bellman_ford step 6158 current loss 0.973681, current_train_items 197088.
I0302 19:01:13.726666 22447428132992 run.py:483] Algo bellman_ford step 6159 current loss 1.040179, current_train_items 197120.
I0302 19:01:13.746508 22447428132992 run.py:483] Algo bellman_ford step 6160 current loss 0.571256, current_train_items 197152.
I0302 19:01:13.762904 22447428132992 run.py:483] Algo bellman_ford step 6161 current loss 0.715524, current_train_items 197184.
I0302 19:01:13.785908 22447428132992 run.py:483] Algo bellman_ford step 6162 current loss 0.986423, current_train_items 197216.
I0302 19:01:13.817929 22447428132992 run.py:483] Algo bellman_ford step 6163 current loss 1.098094, current_train_items 197248.
I0302 19:01:13.851973 22447428132992 run.py:483] Algo bellman_ford step 6164 current loss 1.245693, current_train_items 197280.
I0302 19:01:13.871584 22447428132992 run.py:483] Algo bellman_ford step 6165 current loss 0.618939, current_train_items 197312.
I0302 19:01:13.887759 22447428132992 run.py:483] Algo bellman_ford step 6166 current loss 0.745257, current_train_items 197344.
I0302 19:01:13.911584 22447428132992 run.py:483] Algo bellman_ford step 6167 current loss 0.897285, current_train_items 197376.
I0302 19:01:13.942820 22447428132992 run.py:483] Algo bellman_ford step 6168 current loss 0.965310, current_train_items 197408.
I0302 19:01:13.976319 22447428132992 run.py:483] Algo bellman_ford step 6169 current loss 1.073077, current_train_items 197440.
I0302 19:01:13.996544 22447428132992 run.py:483] Algo bellman_ford step 6170 current loss 0.678267, current_train_items 197472.
I0302 19:01:14.012460 22447428132992 run.py:483] Algo bellman_ford step 6171 current loss 0.685072, current_train_items 197504.
I0302 19:01:14.035978 22447428132992 run.py:483] Algo bellman_ford step 6172 current loss 0.910937, current_train_items 197536.
I0302 19:01:14.067782 22447428132992 run.py:483] Algo bellman_ford step 6173 current loss 1.000286, current_train_items 197568.
I0302 19:01:14.101061 22447428132992 run.py:483] Algo bellman_ford step 6174 current loss 1.144869, current_train_items 197600.
I0302 19:01:14.120783 22447428132992 run.py:483] Algo bellman_ford step 6175 current loss 0.555197, current_train_items 197632.
I0302 19:01:14.137185 22447428132992 run.py:483] Algo bellman_ford step 6176 current loss 0.792614, current_train_items 197664.
I0302 19:01:14.159673 22447428132992 run.py:483] Algo bellman_ford step 6177 current loss 0.931991, current_train_items 197696.
I0302 19:01:14.190532 22447428132992 run.py:483] Algo bellman_ford step 6178 current loss 0.956255, current_train_items 197728.
I0302 19:01:14.224184 22447428132992 run.py:483] Algo bellman_ford step 6179 current loss 1.433955, current_train_items 197760.
I0302 19:01:14.243649 22447428132992 run.py:483] Algo bellman_ford step 6180 current loss 0.525733, current_train_items 197792.
I0302 19:01:14.259829 22447428132992 run.py:483] Algo bellman_ford step 6181 current loss 0.804267, current_train_items 197824.
I0302 19:01:14.282989 22447428132992 run.py:483] Algo bellman_ford step 6182 current loss 0.948411, current_train_items 197856.
I0302 19:01:14.313788 22447428132992 run.py:483] Algo bellman_ford step 6183 current loss 1.057946, current_train_items 197888.
I0302 19:01:14.349688 22447428132992 run.py:483] Algo bellman_ford step 6184 current loss 1.296779, current_train_items 197920.
I0302 19:01:14.369459 22447428132992 run.py:483] Algo bellman_ford step 6185 current loss 0.600862, current_train_items 197952.
I0302 19:01:14.385655 22447428132992 run.py:483] Algo bellman_ford step 6186 current loss 0.764848, current_train_items 197984.
I0302 19:01:14.407982 22447428132992 run.py:483] Algo bellman_ford step 6187 current loss 0.989059, current_train_items 198016.
I0302 19:01:14.438392 22447428132992 run.py:483] Algo bellman_ford step 6188 current loss 0.990424, current_train_items 198048.
I0302 19:01:14.472039 22447428132992 run.py:483] Algo bellman_ford step 6189 current loss 1.490846, current_train_items 198080.
I0302 19:01:14.492245 22447428132992 run.py:483] Algo bellman_ford step 6190 current loss 0.579137, current_train_items 198112.
I0302 19:01:14.508697 22447428132992 run.py:483] Algo bellman_ford step 6191 current loss 0.877661, current_train_items 198144.
I0302 19:01:14.532203 22447428132992 run.py:483] Algo bellman_ford step 6192 current loss 1.017329, current_train_items 198176.
I0302 19:01:14.563911 22447428132992 run.py:483] Algo bellman_ford step 6193 current loss 0.963546, current_train_items 198208.
I0302 19:01:14.596418 22447428132992 run.py:483] Algo bellman_ford step 6194 current loss 0.993653, current_train_items 198240.
I0302 19:01:14.616088 22447428132992 run.py:483] Algo bellman_ford step 6195 current loss 0.632536, current_train_items 198272.
I0302 19:01:14.631679 22447428132992 run.py:483] Algo bellman_ford step 6196 current loss 0.650453, current_train_items 198304.
I0302 19:01:14.655817 22447428132992 run.py:483] Algo bellman_ford step 6197 current loss 0.998413, current_train_items 198336.
I0302 19:01:14.685399 22447428132992 run.py:483] Algo bellman_ford step 6198 current loss 0.931640, current_train_items 198368.
I0302 19:01:14.719909 22447428132992 run.py:483] Algo bellman_ford step 6199 current loss 1.142347, current_train_items 198400.
I0302 19:01:14.740095 22447428132992 run.py:483] Algo bellman_ford step 6200 current loss 0.533833, current_train_items 198432.
I0302 19:01:14.747777 22447428132992 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:01:14.747889 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:01:14.764627 22447428132992 run.py:483] Algo bellman_ford step 6201 current loss 0.772437, current_train_items 198464.
I0302 19:01:14.788865 22447428132992 run.py:483] Algo bellman_ford step 6202 current loss 0.904918, current_train_items 198496.
I0302 19:01:14.818520 22447428132992 run.py:483] Algo bellman_ford step 6203 current loss 0.956713, current_train_items 198528.
I0302 19:01:14.851217 22447428132992 run.py:483] Algo bellman_ford step 6204 current loss 1.063983, current_train_items 198560.
I0302 19:01:14.871201 22447428132992 run.py:483] Algo bellman_ford step 6205 current loss 0.738464, current_train_items 198592.
I0302 19:01:14.886307 22447428132992 run.py:483] Algo bellman_ford step 6206 current loss 0.705577, current_train_items 198624.
I0302 19:01:14.909529 22447428132992 run.py:483] Algo bellman_ford step 6207 current loss 0.952705, current_train_items 198656.
I0302 19:01:14.939945 22447428132992 run.py:483] Algo bellman_ford step 6208 current loss 1.073697, current_train_items 198688.
I0302 19:01:14.974286 22447428132992 run.py:483] Algo bellman_ford step 6209 current loss 1.179514, current_train_items 198720.
I0302 19:01:14.993606 22447428132992 run.py:483] Algo bellman_ford step 6210 current loss 0.678464, current_train_items 198752.
I0302 19:01:15.009698 22447428132992 run.py:483] Algo bellman_ford step 6211 current loss 0.731646, current_train_items 198784.
I0302 19:01:15.032253 22447428132992 run.py:483] Algo bellman_ford step 6212 current loss 0.929333, current_train_items 198816.
I0302 19:01:15.061356 22447428132992 run.py:483] Algo bellman_ford step 6213 current loss 0.923806, current_train_items 198848.
I0302 19:01:15.096047 22447428132992 run.py:483] Algo bellman_ford step 6214 current loss 1.083700, current_train_items 198880.
I0302 19:01:15.115215 22447428132992 run.py:483] Algo bellman_ford step 6215 current loss 0.578885, current_train_items 198912.
I0302 19:01:15.131263 22447428132992 run.py:483] Algo bellman_ford step 6216 current loss 0.771951, current_train_items 198944.
I0302 19:01:15.153787 22447428132992 run.py:483] Algo bellman_ford step 6217 current loss 0.892540, current_train_items 198976.
I0302 19:01:15.183862 22447428132992 run.py:483] Algo bellman_ford step 6218 current loss 0.952830, current_train_items 199008.
I0302 19:01:15.216011 22447428132992 run.py:483] Algo bellman_ford step 6219 current loss 1.164136, current_train_items 199040.
I0302 19:01:15.235566 22447428132992 run.py:483] Algo bellman_ford step 6220 current loss 0.624937, current_train_items 199072.
I0302 19:01:15.251126 22447428132992 run.py:483] Algo bellman_ford step 6221 current loss 0.764244, current_train_items 199104.
I0302 19:01:15.274743 22447428132992 run.py:483] Algo bellman_ford step 6222 current loss 0.939910, current_train_items 199136.
I0302 19:01:15.304033 22447428132992 run.py:483] Algo bellman_ford step 6223 current loss 1.033375, current_train_items 199168.
I0302 19:01:15.337939 22447428132992 run.py:483] Algo bellman_ford step 6224 current loss 1.296672, current_train_items 199200.
I0302 19:01:15.357303 22447428132992 run.py:483] Algo bellman_ford step 6225 current loss 0.613703, current_train_items 199232.
I0302 19:01:15.373255 22447428132992 run.py:483] Algo bellman_ford step 6226 current loss 0.723022, current_train_items 199264.
I0302 19:01:15.395408 22447428132992 run.py:483] Algo bellman_ford step 6227 current loss 0.862664, current_train_items 199296.
I0302 19:01:15.424580 22447428132992 run.py:483] Algo bellman_ford step 6228 current loss 0.901023, current_train_items 199328.
I0302 19:01:15.455742 22447428132992 run.py:483] Algo bellman_ford step 6229 current loss 1.069085, current_train_items 199360.
I0302 19:01:15.475092 22447428132992 run.py:483] Algo bellman_ford step 6230 current loss 0.649779, current_train_items 199392.
I0302 19:01:15.491318 22447428132992 run.py:483] Algo bellman_ford step 6231 current loss 0.751337, current_train_items 199424.
I0302 19:01:15.514765 22447428132992 run.py:483] Algo bellman_ford step 6232 current loss 0.926951, current_train_items 199456.
I0302 19:01:15.543967 22447428132992 run.py:483] Algo bellman_ford step 6233 current loss 1.011119, current_train_items 199488.
I0302 19:01:15.576750 22447428132992 run.py:483] Algo bellman_ford step 6234 current loss 1.102803, current_train_items 199520.
I0302 19:01:15.595885 22447428132992 run.py:483] Algo bellman_ford step 6235 current loss 0.568998, current_train_items 199552.
I0302 19:01:15.611972 22447428132992 run.py:483] Algo bellman_ford step 6236 current loss 0.715781, current_train_items 199584.
I0302 19:01:15.634129 22447428132992 run.py:483] Algo bellman_ford step 6237 current loss 0.853534, current_train_items 199616.
I0302 19:01:15.665014 22447428132992 run.py:483] Algo bellman_ford step 6238 current loss 0.938459, current_train_items 199648.
I0302 19:01:15.695610 22447428132992 run.py:483] Algo bellman_ford step 6239 current loss 1.317284, current_train_items 199680.
I0302 19:01:15.714879 22447428132992 run.py:483] Algo bellman_ford step 6240 current loss 0.465395, current_train_items 199712.
I0302 19:01:15.730947 22447428132992 run.py:483] Algo bellman_ford step 6241 current loss 0.793234, current_train_items 199744.
I0302 19:01:15.754202 22447428132992 run.py:483] Algo bellman_ford step 6242 current loss 0.996703, current_train_items 199776.
I0302 19:01:15.784946 22447428132992 run.py:483] Algo bellman_ford step 6243 current loss 1.080891, current_train_items 199808.
I0302 19:01:15.816719 22447428132992 run.py:483] Algo bellman_ford step 6244 current loss 1.155005, current_train_items 199840.
I0302 19:01:15.836291 22447428132992 run.py:483] Algo bellman_ford step 6245 current loss 0.670451, current_train_items 199872.
I0302 19:01:15.852352 22447428132992 run.py:483] Algo bellman_ford step 6246 current loss 0.736139, current_train_items 199904.
I0302 19:01:15.874815 22447428132992 run.py:483] Algo bellman_ford step 6247 current loss 0.880931, current_train_items 199936.
I0302 19:01:15.906018 22447428132992 run.py:483] Algo bellman_ford step 6248 current loss 1.070379, current_train_items 199968.
I0302 19:01:15.937818 22447428132992 run.py:483] Algo bellman_ford step 6249 current loss 1.039815, current_train_items 200000.
I0302 19:01:15.957045 22447428132992 run.py:483] Algo bellman_ford step 6250 current loss 0.551228, current_train_items 200032.
I0302 19:01:15.965227 22447428132992 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:01:15.965331 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:01:15.982200 22447428132992 run.py:483] Algo bellman_ford step 6251 current loss 0.815652, current_train_items 200064.
I0302 19:01:16.005609 22447428132992 run.py:483] Algo bellman_ford step 6252 current loss 0.875986, current_train_items 200096.
I0302 19:01:16.037219 22447428132992 run.py:483] Algo bellman_ford step 6253 current loss 1.012229, current_train_items 200128.
I0302 19:01:16.069601 22447428132992 run.py:483] Algo bellman_ford step 6254 current loss 1.020768, current_train_items 200160.
I0302 19:01:16.089235 22447428132992 run.py:483] Algo bellman_ford step 6255 current loss 0.515974, current_train_items 200192.
I0302 19:01:16.104913 22447428132992 run.py:483] Algo bellman_ford step 6256 current loss 0.694529, current_train_items 200224.
I0302 19:01:16.128422 22447428132992 run.py:483] Algo bellman_ford step 6257 current loss 0.913740, current_train_items 200256.
I0302 19:01:16.158479 22447428132992 run.py:483] Algo bellman_ford step 6258 current loss 1.091031, current_train_items 200288.
I0302 19:01:16.190693 22447428132992 run.py:483] Algo bellman_ford step 6259 current loss 1.426951, current_train_items 200320.
I0302 19:01:16.210442 22447428132992 run.py:483] Algo bellman_ford step 6260 current loss 0.576143, current_train_items 200352.
I0302 19:01:16.226820 22447428132992 run.py:483] Algo bellman_ford step 6261 current loss 0.746976, current_train_items 200384.
I0302 19:01:16.249560 22447428132992 run.py:483] Algo bellman_ford step 6262 current loss 0.877126, current_train_items 200416.
I0302 19:01:16.280163 22447428132992 run.py:483] Algo bellman_ford step 6263 current loss 0.976776, current_train_items 200448.
I0302 19:01:16.313416 22447428132992 run.py:483] Algo bellman_ford step 6264 current loss 1.124804, current_train_items 200480.
I0302 19:01:16.332710 22447428132992 run.py:483] Algo bellman_ford step 6265 current loss 0.596135, current_train_items 200512.
I0302 19:01:16.349480 22447428132992 run.py:483] Algo bellman_ford step 6266 current loss 0.791185, current_train_items 200544.
I0302 19:01:16.373661 22447428132992 run.py:483] Algo bellman_ford step 6267 current loss 1.063175, current_train_items 200576.
I0302 19:01:16.403834 22447428132992 run.py:483] Algo bellman_ford step 6268 current loss 0.920258, current_train_items 200608.
I0302 19:01:16.434522 22447428132992 run.py:483] Algo bellman_ford step 6269 current loss 0.924910, current_train_items 200640.
I0302 19:01:16.453923 22447428132992 run.py:483] Algo bellman_ford step 6270 current loss 0.526342, current_train_items 200672.
I0302 19:01:16.470250 22447428132992 run.py:483] Algo bellman_ford step 6271 current loss 0.742444, current_train_items 200704.
I0302 19:01:16.493255 22447428132992 run.py:483] Algo bellman_ford step 6272 current loss 0.906424, current_train_items 200736.
I0302 19:01:16.522667 22447428132992 run.py:483] Algo bellman_ford step 6273 current loss 0.978167, current_train_items 200768.
I0302 19:01:16.556992 22447428132992 run.py:483] Algo bellman_ford step 6274 current loss 1.359996, current_train_items 200800.
I0302 19:01:16.576737 22447428132992 run.py:483] Algo bellman_ford step 6275 current loss 0.559263, current_train_items 200832.
I0302 19:01:16.593062 22447428132992 run.py:483] Algo bellman_ford step 6276 current loss 0.789457, current_train_items 200864.
I0302 19:01:16.615775 22447428132992 run.py:483] Algo bellman_ford step 6277 current loss 0.928803, current_train_items 200896.
I0302 19:01:16.645358 22447428132992 run.py:483] Algo bellman_ford step 6278 current loss 1.041929, current_train_items 200928.
I0302 19:01:16.679639 22447428132992 run.py:483] Algo bellman_ford step 6279 current loss 1.107520, current_train_items 200960.
I0302 19:01:16.699090 22447428132992 run.py:483] Algo bellman_ford step 6280 current loss 0.681626, current_train_items 200992.
I0302 19:01:16.715021 22447428132992 run.py:483] Algo bellman_ford step 6281 current loss 0.766513, current_train_items 201024.
I0302 19:01:16.737847 22447428132992 run.py:483] Algo bellman_ford step 6282 current loss 0.936864, current_train_items 201056.
I0302 19:01:16.768580 22447428132992 run.py:483] Algo bellman_ford step 6283 current loss 1.018117, current_train_items 201088.
I0302 19:01:16.802805 22447428132992 run.py:483] Algo bellman_ford step 6284 current loss 1.049347, current_train_items 201120.
I0302 19:01:16.822374 22447428132992 run.py:483] Algo bellman_ford step 6285 current loss 0.572517, current_train_items 201152.
I0302 19:01:16.838651 22447428132992 run.py:483] Algo bellman_ford step 6286 current loss 0.761988, current_train_items 201184.
I0302 19:01:16.861667 22447428132992 run.py:483] Algo bellman_ford step 6287 current loss 0.927211, current_train_items 201216.
I0302 19:01:16.892847 22447428132992 run.py:483] Algo bellman_ford step 6288 current loss 1.117215, current_train_items 201248.
I0302 19:01:16.926142 22447428132992 run.py:483] Algo bellman_ford step 6289 current loss 1.211970, current_train_items 201280.
I0302 19:01:16.946014 22447428132992 run.py:483] Algo bellman_ford step 6290 current loss 0.598539, current_train_items 201312.
I0302 19:01:16.962298 22447428132992 run.py:483] Algo bellman_ford step 6291 current loss 0.709073, current_train_items 201344.
I0302 19:01:16.985010 22447428132992 run.py:483] Algo bellman_ford step 6292 current loss 0.891131, current_train_items 201376.
I0302 19:01:17.016327 22447428132992 run.py:483] Algo bellman_ford step 6293 current loss 1.060062, current_train_items 201408.
I0302 19:01:17.050867 22447428132992 run.py:483] Algo bellman_ford step 6294 current loss 1.182854, current_train_items 201440.
I0302 19:01:17.070722 22447428132992 run.py:483] Algo bellman_ford step 6295 current loss 0.567730, current_train_items 201472.
I0302 19:01:17.087431 22447428132992 run.py:483] Algo bellman_ford step 6296 current loss 0.843967, current_train_items 201504.
I0302 19:01:17.110021 22447428132992 run.py:483] Algo bellman_ford step 6297 current loss 0.892723, current_train_items 201536.
I0302 19:01:17.141101 22447428132992 run.py:483] Algo bellman_ford step 6298 current loss 1.052334, current_train_items 201568.
I0302 19:01:17.173554 22447428132992 run.py:483] Algo bellman_ford step 6299 current loss 1.034129, current_train_items 201600.
I0302 19:01:17.193261 22447428132992 run.py:483] Algo bellman_ford step 6300 current loss 0.602362, current_train_items 201632.
I0302 19:01:17.201017 22447428132992 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:01:17.201127 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 19:01:17.217914 22447428132992 run.py:483] Algo bellman_ford step 6301 current loss 0.836665, current_train_items 201664.
I0302 19:01:17.240864 22447428132992 run.py:483] Algo bellman_ford step 6302 current loss 0.996345, current_train_items 201696.
I0302 19:01:17.271241 22447428132992 run.py:483] Algo bellman_ford step 6303 current loss 0.913034, current_train_items 201728.
I0302 19:01:17.302608 22447428132992 run.py:483] Algo bellman_ford step 6304 current loss 0.918894, current_train_items 201760.
I0302 19:01:17.322579 22447428132992 run.py:483] Algo bellman_ford step 6305 current loss 0.725955, current_train_items 201792.
I0302 19:01:17.338097 22447428132992 run.py:483] Algo bellman_ford step 6306 current loss 0.687848, current_train_items 201824.
I0302 19:01:17.361153 22447428132992 run.py:483] Algo bellman_ford step 6307 current loss 0.973766, current_train_items 201856.
I0302 19:01:17.392290 22447428132992 run.py:483] Algo bellman_ford step 6308 current loss 1.020976, current_train_items 201888.
I0302 19:01:17.424617 22447428132992 run.py:483] Algo bellman_ford step 6309 current loss 1.165910, current_train_items 201920.
I0302 19:01:17.444028 22447428132992 run.py:483] Algo bellman_ford step 6310 current loss 0.573961, current_train_items 201952.
I0302 19:01:17.459927 22447428132992 run.py:483] Algo bellman_ford step 6311 current loss 0.769438, current_train_items 201984.
I0302 19:01:17.481803 22447428132992 run.py:483] Algo bellman_ford step 6312 current loss 0.816850, current_train_items 202016.
I0302 19:01:17.513103 22447428132992 run.py:483] Algo bellman_ford step 6313 current loss 0.973317, current_train_items 202048.
I0302 19:01:17.546804 22447428132992 run.py:483] Algo bellman_ford step 6314 current loss 1.343964, current_train_items 202080.
I0302 19:01:17.566342 22447428132992 run.py:483] Algo bellman_ford step 6315 current loss 0.576586, current_train_items 202112.
I0302 19:01:17.582206 22447428132992 run.py:483] Algo bellman_ford step 6316 current loss 0.774855, current_train_items 202144.
I0302 19:01:17.605201 22447428132992 run.py:483] Algo bellman_ford step 6317 current loss 0.975364, current_train_items 202176.
I0302 19:01:17.634814 22447428132992 run.py:483] Algo bellman_ford step 6318 current loss 0.959942, current_train_items 202208.
I0302 19:01:17.667142 22447428132992 run.py:483] Algo bellman_ford step 6319 current loss 1.028989, current_train_items 202240.
I0302 19:01:17.686882 22447428132992 run.py:483] Algo bellman_ford step 6320 current loss 0.770163, current_train_items 202272.
I0302 19:01:17.703174 22447428132992 run.py:483] Algo bellman_ford step 6321 current loss 0.740803, current_train_items 202304.
I0302 19:01:17.727231 22447428132992 run.py:483] Algo bellman_ford step 6322 current loss 0.948633, current_train_items 202336.
I0302 19:01:17.758863 22447428132992 run.py:483] Algo bellman_ford step 6323 current loss 1.187819, current_train_items 202368.
I0302 19:01:17.792574 22447428132992 run.py:483] Algo bellman_ford step 6324 current loss 1.171012, current_train_items 202400.
I0302 19:01:17.812295 22447428132992 run.py:483] Algo bellman_ford step 6325 current loss 0.747680, current_train_items 202432.
I0302 19:01:17.828102 22447428132992 run.py:483] Algo bellman_ford step 6326 current loss 0.796562, current_train_items 202464.
I0302 19:01:17.852090 22447428132992 run.py:483] Algo bellman_ford step 6327 current loss 1.003647, current_train_items 202496.
I0302 19:01:17.882253 22447428132992 run.py:483] Algo bellman_ford step 6328 current loss 1.034388, current_train_items 202528.
I0302 19:01:17.915917 22447428132992 run.py:483] Algo bellman_ford step 6329 current loss 1.078622, current_train_items 202560.
I0302 19:01:17.935120 22447428132992 run.py:483] Algo bellman_ford step 6330 current loss 0.608470, current_train_items 202592.
I0302 19:01:17.951144 22447428132992 run.py:483] Algo bellman_ford step 6331 current loss 0.780270, current_train_items 202624.
I0302 19:01:17.973865 22447428132992 run.py:483] Algo bellman_ford step 6332 current loss 0.894311, current_train_items 202656.
I0302 19:01:18.004591 22447428132992 run.py:483] Algo bellman_ford step 6333 current loss 1.000189, current_train_items 202688.
I0302 19:01:18.036655 22447428132992 run.py:483] Algo bellman_ford step 6334 current loss 1.113109, current_train_items 202720.
I0302 19:01:18.055921 22447428132992 run.py:483] Algo bellman_ford step 6335 current loss 0.632714, current_train_items 202752.
I0302 19:01:18.072165 22447428132992 run.py:483] Algo bellman_ford step 6336 current loss 0.766346, current_train_items 202784.
I0302 19:01:18.095749 22447428132992 run.py:483] Algo bellman_ford step 6337 current loss 0.867385, current_train_items 202816.
I0302 19:01:18.126852 22447428132992 run.py:483] Algo bellman_ford step 6338 current loss 1.050143, current_train_items 202848.
I0302 19:01:18.159874 22447428132992 run.py:483] Algo bellman_ford step 6339 current loss 1.037598, current_train_items 202880.
I0302 19:01:18.178918 22447428132992 run.py:483] Algo bellman_ford step 6340 current loss 0.561703, current_train_items 202912.
I0302 19:01:18.194580 22447428132992 run.py:483] Algo bellman_ford step 6341 current loss 0.763769, current_train_items 202944.
I0302 19:01:18.218495 22447428132992 run.py:483] Algo bellman_ford step 6342 current loss 1.003827, current_train_items 202976.
I0302 19:01:18.248606 22447428132992 run.py:483] Algo bellman_ford step 6343 current loss 0.987882, current_train_items 203008.
I0302 19:01:18.279284 22447428132992 run.py:483] Algo bellman_ford step 6344 current loss 1.081090, current_train_items 203040.
I0302 19:01:18.298676 22447428132992 run.py:483] Algo bellman_ford step 6345 current loss 0.538886, current_train_items 203072.
I0302 19:01:18.314962 22447428132992 run.py:483] Algo bellman_ford step 6346 current loss 0.766818, current_train_items 203104.
I0302 19:01:18.337830 22447428132992 run.py:483] Algo bellman_ford step 6347 current loss 0.892748, current_train_items 203136.
I0302 19:01:18.367599 22447428132992 run.py:483] Algo bellman_ford step 6348 current loss 1.004126, current_train_items 203168.
I0302 19:01:18.402432 22447428132992 run.py:483] Algo bellman_ford step 6349 current loss 1.051410, current_train_items 203200.
I0302 19:01:18.421750 22447428132992 run.py:483] Algo bellman_ford step 6350 current loss 0.564200, current_train_items 203232.
I0302 19:01:18.429922 22447428132992 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:01:18.430033 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:01:18.446914 22447428132992 run.py:483] Algo bellman_ford step 6351 current loss 0.880947, current_train_items 203264.
I0302 19:01:18.469812 22447428132992 run.py:483] Algo bellman_ford step 6352 current loss 0.984905, current_train_items 203296.
I0302 19:01:18.500755 22447428132992 run.py:483] Algo bellman_ford step 6353 current loss 0.858682, current_train_items 203328.
I0302 19:01:18.534233 22447428132992 run.py:483] Algo bellman_ford step 6354 current loss 1.090504, current_train_items 203360.
I0302 19:01:18.554554 22447428132992 run.py:483] Algo bellman_ford step 6355 current loss 0.860845, current_train_items 203392.
I0302 19:01:18.570391 22447428132992 run.py:483] Algo bellman_ford step 6356 current loss 0.777022, current_train_items 203424.
I0302 19:01:18.594135 22447428132992 run.py:483] Algo bellman_ford step 6357 current loss 0.961282, current_train_items 203456.
I0302 19:01:18.624578 22447428132992 run.py:483] Algo bellman_ford step 6358 current loss 0.994189, current_train_items 203488.
I0302 19:01:18.658170 22447428132992 run.py:483] Algo bellman_ford step 6359 current loss 1.107875, current_train_items 203520.
I0302 19:01:18.678165 22447428132992 run.py:483] Algo bellman_ford step 6360 current loss 0.693218, current_train_items 203552.
I0302 19:01:18.694528 22447428132992 run.py:483] Algo bellman_ford step 6361 current loss 0.787693, current_train_items 203584.
I0302 19:01:18.717044 22447428132992 run.py:483] Algo bellman_ford step 6362 current loss 0.938876, current_train_items 203616.
I0302 19:01:18.747772 22447428132992 run.py:483] Algo bellman_ford step 6363 current loss 0.903926, current_train_items 203648.
I0302 19:01:18.780219 22447428132992 run.py:483] Algo bellman_ford step 6364 current loss 1.134364, current_train_items 203680.
I0302 19:01:18.799956 22447428132992 run.py:483] Algo bellman_ford step 6365 current loss 0.621451, current_train_items 203712.
I0302 19:01:18.816219 22447428132992 run.py:483] Algo bellman_ford step 6366 current loss 0.668293, current_train_items 203744.
I0302 19:01:18.839413 22447428132992 run.py:483] Algo bellman_ford step 6367 current loss 0.928069, current_train_items 203776.
I0302 19:01:18.871458 22447428132992 run.py:483] Algo bellman_ford step 6368 current loss 1.177243, current_train_items 203808.
I0302 19:01:18.903400 22447428132992 run.py:483] Algo bellman_ford step 6369 current loss 1.108950, current_train_items 203840.
I0302 19:01:18.923392 22447428132992 run.py:483] Algo bellman_ford step 6370 current loss 0.530861, current_train_items 203872.
I0302 19:01:18.939736 22447428132992 run.py:483] Algo bellman_ford step 6371 current loss 0.785658, current_train_items 203904.
I0302 19:01:18.963518 22447428132992 run.py:483] Algo bellman_ford step 6372 current loss 0.945501, current_train_items 203936.
I0302 19:01:18.993698 22447428132992 run.py:483] Algo bellman_ford step 6373 current loss 1.035735, current_train_items 203968.
I0302 19:01:19.027150 22447428132992 run.py:483] Algo bellman_ford step 6374 current loss 1.064511, current_train_items 204000.
I0302 19:01:19.047017 22447428132992 run.py:483] Algo bellman_ford step 6375 current loss 0.599737, current_train_items 204032.
I0302 19:01:19.063498 22447428132992 run.py:483] Algo bellman_ford step 6376 current loss 0.719391, current_train_items 204064.
I0302 19:01:19.086964 22447428132992 run.py:483] Algo bellman_ford step 6377 current loss 0.961408, current_train_items 204096.
I0302 19:01:19.117547 22447428132992 run.py:483] Algo bellman_ford step 6378 current loss 0.902631, current_train_items 204128.
I0302 19:01:19.152597 22447428132992 run.py:483] Algo bellman_ford step 6379 current loss 1.126169, current_train_items 204160.
I0302 19:01:19.172067 22447428132992 run.py:483] Algo bellman_ford step 6380 current loss 0.627292, current_train_items 204192.
I0302 19:01:19.188062 22447428132992 run.py:483] Algo bellman_ford step 6381 current loss 0.721944, current_train_items 204224.
I0302 19:01:19.211622 22447428132992 run.py:483] Algo bellman_ford step 6382 current loss 0.908300, current_train_items 204256.
I0302 19:01:19.241279 22447428132992 run.py:483] Algo bellman_ford step 6383 current loss 1.079540, current_train_items 204288.
I0302 19:01:19.274633 22447428132992 run.py:483] Algo bellman_ford step 6384 current loss 1.039294, current_train_items 204320.
I0302 19:01:19.294511 22447428132992 run.py:483] Algo bellman_ford step 6385 current loss 0.844248, current_train_items 204352.
I0302 19:01:19.310211 22447428132992 run.py:483] Algo bellman_ford step 6386 current loss 0.653634, current_train_items 204384.
I0302 19:01:19.333671 22447428132992 run.py:483] Algo bellman_ford step 6387 current loss 0.844788, current_train_items 204416.
I0302 19:01:19.363620 22447428132992 run.py:483] Algo bellman_ford step 6388 current loss 0.922482, current_train_items 204448.
I0302 19:01:19.396531 22447428132992 run.py:483] Algo bellman_ford step 6389 current loss 0.952648, current_train_items 204480.
I0302 19:01:19.416410 22447428132992 run.py:483] Algo bellman_ford step 6390 current loss 0.597760, current_train_items 204512.
I0302 19:01:19.432716 22447428132992 run.py:483] Algo bellman_ford step 6391 current loss 0.781684, current_train_items 204544.
I0302 19:01:19.455737 22447428132992 run.py:483] Algo bellman_ford step 6392 current loss 0.940892, current_train_items 204576.
I0302 19:01:19.484877 22447428132992 run.py:483] Algo bellman_ford step 6393 current loss 0.861661, current_train_items 204608.
I0302 19:01:19.518517 22447428132992 run.py:483] Algo bellman_ford step 6394 current loss 1.290008, current_train_items 204640.
I0302 19:01:19.538139 22447428132992 run.py:483] Algo bellman_ford step 6395 current loss 0.661885, current_train_items 204672.
I0302 19:01:19.554117 22447428132992 run.py:483] Algo bellman_ford step 6396 current loss 0.677898, current_train_items 204704.
I0302 19:01:19.576843 22447428132992 run.py:483] Algo bellman_ford step 6397 current loss 0.972480, current_train_items 204736.
I0302 19:01:19.606965 22447428132992 run.py:483] Algo bellman_ford step 6398 current loss 1.011605, current_train_items 204768.
I0302 19:01:19.638424 22447428132992 run.py:483] Algo bellman_ford step 6399 current loss 1.176909, current_train_items 204800.
I0302 19:01:19.658156 22447428132992 run.py:483] Algo bellman_ford step 6400 current loss 0.518762, current_train_items 204832.
I0302 19:01:19.666002 22447428132992 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:01:19.666108 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:01:19.682054 22447428132992 run.py:483] Algo bellman_ford step 6401 current loss 0.775164, current_train_items 204864.
I0302 19:01:19.704995 22447428132992 run.py:483] Algo bellman_ford step 6402 current loss 0.870412, current_train_items 204896.
I0302 19:01:19.735883 22447428132992 run.py:483] Algo bellman_ford step 6403 current loss 1.017911, current_train_items 204928.
I0302 19:01:19.769498 22447428132992 run.py:483] Algo bellman_ford step 6404 current loss 1.008917, current_train_items 204960.
I0302 19:01:19.789054 22447428132992 run.py:483] Algo bellman_ford step 6405 current loss 0.610073, current_train_items 204992.
I0302 19:01:19.804627 22447428132992 run.py:483] Algo bellman_ford step 6406 current loss 0.724757, current_train_items 205024.
I0302 19:01:19.827250 22447428132992 run.py:483] Algo bellman_ford step 6407 current loss 0.833809, current_train_items 205056.
I0302 19:01:19.858002 22447428132992 run.py:483] Algo bellman_ford step 6408 current loss 0.985504, current_train_items 205088.
I0302 19:01:19.892329 22447428132992 run.py:483] Algo bellman_ford step 6409 current loss 1.128262, current_train_items 205120.
I0302 19:01:19.911736 22447428132992 run.py:483] Algo bellman_ford step 6410 current loss 0.607142, current_train_items 205152.
I0302 19:01:19.927463 22447428132992 run.py:483] Algo bellman_ford step 6411 current loss 0.792518, current_train_items 205184.
I0302 19:01:19.949737 22447428132992 run.py:483] Algo bellman_ford step 6412 current loss 0.916828, current_train_items 205216.
I0302 19:01:19.980257 22447428132992 run.py:483] Algo bellman_ford step 6413 current loss 1.074546, current_train_items 205248.
I0302 19:01:20.013549 22447428132992 run.py:483] Algo bellman_ford step 6414 current loss 1.025089, current_train_items 205280.
I0302 19:01:20.032938 22447428132992 run.py:483] Algo bellman_ford step 6415 current loss 0.563304, current_train_items 205312.
I0302 19:01:20.048903 22447428132992 run.py:483] Algo bellman_ford step 6416 current loss 0.751764, current_train_items 205344.
I0302 19:01:20.072942 22447428132992 run.py:483] Algo bellman_ford step 6417 current loss 0.907448, current_train_items 205376.
I0302 19:01:20.104656 22447428132992 run.py:483] Algo bellman_ford step 6418 current loss 1.058028, current_train_items 205408.
I0302 19:01:20.138242 22447428132992 run.py:483] Algo bellman_ford step 6419 current loss 1.290710, current_train_items 205440.
I0302 19:01:20.157880 22447428132992 run.py:483] Algo bellman_ford step 6420 current loss 0.678017, current_train_items 205472.
I0302 19:01:20.173982 22447428132992 run.py:483] Algo bellman_ford step 6421 current loss 0.729693, current_train_items 205504.
I0302 19:01:20.196763 22447428132992 run.py:483] Algo bellman_ford step 6422 current loss 1.004103, current_train_items 205536.
I0302 19:01:20.226753 22447428132992 run.py:483] Algo bellman_ford step 6423 current loss 1.024657, current_train_items 205568.
I0302 19:01:20.259241 22447428132992 run.py:483] Algo bellman_ford step 6424 current loss 1.172195, current_train_items 205600.
I0302 19:01:20.278669 22447428132992 run.py:483] Algo bellman_ford step 6425 current loss 0.606830, current_train_items 205632.
I0302 19:01:20.295014 22447428132992 run.py:483] Algo bellman_ford step 6426 current loss 0.764771, current_train_items 205664.
I0302 19:01:20.318165 22447428132992 run.py:483] Algo bellman_ford step 6427 current loss 0.836841, current_train_items 205696.
I0302 19:01:20.349210 22447428132992 run.py:483] Algo bellman_ford step 6428 current loss 0.971614, current_train_items 205728.
I0302 19:01:20.384898 22447428132992 run.py:483] Algo bellman_ford step 6429 current loss 1.302222, current_train_items 205760.
I0302 19:01:20.404404 22447428132992 run.py:483] Algo bellman_ford step 6430 current loss 0.625287, current_train_items 205792.
I0302 19:01:20.419943 22447428132992 run.py:483] Algo bellman_ford step 6431 current loss 0.777470, current_train_items 205824.
I0302 19:01:20.443070 22447428132992 run.py:483] Algo bellman_ford step 6432 current loss 1.081552, current_train_items 205856.
I0302 19:01:20.473902 22447428132992 run.py:483] Algo bellman_ford step 6433 current loss 1.081360, current_train_items 205888.
I0302 19:01:20.506176 22447428132992 run.py:483] Algo bellman_ford step 6434 current loss 1.331246, current_train_items 205920.
I0302 19:01:20.525375 22447428132992 run.py:483] Algo bellman_ford step 6435 current loss 0.689166, current_train_items 205952.
I0302 19:01:20.541285 22447428132992 run.py:483] Algo bellman_ford step 6436 current loss 0.809486, current_train_items 205984.
I0302 19:01:20.563876 22447428132992 run.py:483] Algo bellman_ford step 6437 current loss 0.943286, current_train_items 206016.
I0302 19:01:20.595465 22447428132992 run.py:483] Algo bellman_ford step 6438 current loss 1.019704, current_train_items 206048.
I0302 19:01:20.629157 22447428132992 run.py:483] Algo bellman_ford step 6439 current loss 1.070093, current_train_items 206080.
I0302 19:01:20.648355 22447428132992 run.py:483] Algo bellman_ford step 6440 current loss 0.536968, current_train_items 206112.
I0302 19:01:20.664689 22447428132992 run.py:483] Algo bellman_ford step 6441 current loss 0.816635, current_train_items 206144.
I0302 19:01:20.687212 22447428132992 run.py:483] Algo bellman_ford step 6442 current loss 0.923840, current_train_items 206176.
I0302 19:01:20.716239 22447428132992 run.py:483] Algo bellman_ford step 6443 current loss 1.016968, current_train_items 206208.
I0302 19:01:20.749450 22447428132992 run.py:483] Algo bellman_ford step 6444 current loss 1.040251, current_train_items 206240.
I0302 19:01:20.769010 22447428132992 run.py:483] Algo bellman_ford step 6445 current loss 0.606393, current_train_items 206272.
I0302 19:01:20.784734 22447428132992 run.py:483] Algo bellman_ford step 6446 current loss 0.705149, current_train_items 206304.
I0302 19:01:20.808351 22447428132992 run.py:483] Algo bellman_ford step 6447 current loss 0.928398, current_train_items 206336.
I0302 19:01:20.839076 22447428132992 run.py:483] Algo bellman_ford step 6448 current loss 1.031162, current_train_items 206368.
I0302 19:01:20.872463 22447428132992 run.py:483] Algo bellman_ford step 6449 current loss 1.116450, current_train_items 206400.
I0302 19:01:20.892156 22447428132992 run.py:483] Algo bellman_ford step 6450 current loss 0.554785, current_train_items 206432.
I0302 19:01:20.900271 22447428132992 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:01:20.900376 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:01:20.916960 22447428132992 run.py:483] Algo bellman_ford step 6451 current loss 0.725710, current_train_items 206464.
I0302 19:01:20.941479 22447428132992 run.py:483] Algo bellman_ford step 6452 current loss 1.013097, current_train_items 206496.
I0302 19:01:20.971230 22447428132992 run.py:483] Algo bellman_ford step 6453 current loss 1.000926, current_train_items 206528.
I0302 19:01:21.002927 22447428132992 run.py:483] Algo bellman_ford step 6454 current loss 1.025641, current_train_items 206560.
I0302 19:01:21.023014 22447428132992 run.py:483] Algo bellman_ford step 6455 current loss 0.591638, current_train_items 206592.
I0302 19:01:21.038774 22447428132992 run.py:483] Algo bellman_ford step 6456 current loss 0.730161, current_train_items 206624.
I0302 19:01:21.061069 22447428132992 run.py:483] Algo bellman_ford step 6457 current loss 1.014196, current_train_items 206656.
I0302 19:01:21.091324 22447428132992 run.py:483] Algo bellman_ford step 6458 current loss 0.943884, current_train_items 206688.
I0302 19:01:21.122862 22447428132992 run.py:483] Algo bellman_ford step 6459 current loss 1.020985, current_train_items 206720.
I0302 19:01:21.142769 22447428132992 run.py:483] Algo bellman_ford step 6460 current loss 0.599762, current_train_items 206752.
I0302 19:01:21.158952 22447428132992 run.py:483] Algo bellman_ford step 6461 current loss 0.769197, current_train_items 206784.
I0302 19:01:21.181424 22447428132992 run.py:483] Algo bellman_ford step 6462 current loss 0.962265, current_train_items 206816.
I0302 19:01:21.211122 22447428132992 run.py:483] Algo bellman_ford step 6463 current loss 0.971544, current_train_items 206848.
I0302 19:01:21.246354 22447428132992 run.py:483] Algo bellman_ford step 6464 current loss 1.198063, current_train_items 206880.
I0302 19:01:21.265503 22447428132992 run.py:483] Algo bellman_ford step 6465 current loss 0.569104, current_train_items 206912.
I0302 19:01:21.281768 22447428132992 run.py:483] Algo bellman_ford step 6466 current loss 0.673649, current_train_items 206944.
I0302 19:01:21.306191 22447428132992 run.py:483] Algo bellman_ford step 6467 current loss 1.059339, current_train_items 206976.
I0302 19:01:21.337792 22447428132992 run.py:483] Algo bellman_ford step 6468 current loss 1.074493, current_train_items 207008.
I0302 19:01:21.371951 22447428132992 run.py:483] Algo bellman_ford step 6469 current loss 1.092534, current_train_items 207040.
I0302 19:01:21.391707 22447428132992 run.py:483] Algo bellman_ford step 6470 current loss 0.535599, current_train_items 207072.
I0302 19:01:21.408314 22447428132992 run.py:483] Algo bellman_ford step 6471 current loss 0.725141, current_train_items 207104.
I0302 19:01:21.430644 22447428132992 run.py:483] Algo bellman_ford step 6472 current loss 0.837033, current_train_items 207136.
I0302 19:01:21.461364 22447428132992 run.py:483] Algo bellman_ford step 6473 current loss 1.122800, current_train_items 207168.
I0302 19:01:21.493546 22447428132992 run.py:483] Algo bellman_ford step 6474 current loss 1.086486, current_train_items 207200.
I0302 19:01:21.512971 22447428132992 run.py:483] Algo bellman_ford step 6475 current loss 0.634438, current_train_items 207232.
I0302 19:01:21.529267 22447428132992 run.py:483] Algo bellman_ford step 6476 current loss 0.830376, current_train_items 207264.
I0302 19:01:21.551292 22447428132992 run.py:483] Algo bellman_ford step 6477 current loss 0.980125, current_train_items 207296.
I0302 19:01:21.581842 22447428132992 run.py:483] Algo bellman_ford step 6478 current loss 0.997211, current_train_items 207328.
I0302 19:01:21.616051 22447428132992 run.py:483] Algo bellman_ford step 6479 current loss 1.232187, current_train_items 207360.
I0302 19:01:21.635702 22447428132992 run.py:483] Algo bellman_ford step 6480 current loss 0.561288, current_train_items 207392.
I0302 19:01:21.651465 22447428132992 run.py:483] Algo bellman_ford step 6481 current loss 0.753409, current_train_items 207424.
I0302 19:01:21.674427 22447428132992 run.py:483] Algo bellman_ford step 6482 current loss 0.908630, current_train_items 207456.
I0302 19:01:21.705513 22447428132992 run.py:483] Algo bellman_ford step 6483 current loss 1.128143, current_train_items 207488.
I0302 19:01:21.739478 22447428132992 run.py:483] Algo bellman_ford step 6484 current loss 1.124379, current_train_items 207520.
I0302 19:01:21.759361 22447428132992 run.py:483] Algo bellman_ford step 6485 current loss 0.575914, current_train_items 207552.
I0302 19:01:21.775657 22447428132992 run.py:483] Algo bellman_ford step 6486 current loss 0.717242, current_train_items 207584.
I0302 19:01:21.799882 22447428132992 run.py:483] Algo bellman_ford step 6487 current loss 0.938596, current_train_items 207616.
I0302 19:01:21.829511 22447428132992 run.py:483] Algo bellman_ford step 6488 current loss 0.976788, current_train_items 207648.
I0302 19:01:21.862966 22447428132992 run.py:483] Algo bellman_ford step 6489 current loss 1.047148, current_train_items 207680.
I0302 19:01:21.882857 22447428132992 run.py:483] Algo bellman_ford step 6490 current loss 0.649268, current_train_items 207712.
I0302 19:01:21.898808 22447428132992 run.py:483] Algo bellman_ford step 6491 current loss 0.828175, current_train_items 207744.
I0302 19:01:21.921212 22447428132992 run.py:483] Algo bellman_ford step 6492 current loss 0.926426, current_train_items 207776.
I0302 19:01:21.951470 22447428132992 run.py:483] Algo bellman_ford step 6493 current loss 0.958800, current_train_items 207808.
I0302 19:01:21.984373 22447428132992 run.py:483] Algo bellman_ford step 6494 current loss 1.117639, current_train_items 207840.
I0302 19:01:22.003547 22447428132992 run.py:483] Algo bellman_ford step 6495 current loss 0.501272, current_train_items 207872.
I0302 19:01:22.019197 22447428132992 run.py:483] Algo bellman_ford step 6496 current loss 0.713676, current_train_items 207904.
I0302 19:01:22.043362 22447428132992 run.py:483] Algo bellman_ford step 6497 current loss 1.078787, current_train_items 207936.
I0302 19:01:22.073423 22447428132992 run.py:483] Algo bellman_ford step 6498 current loss 1.009827, current_train_items 207968.
I0302 19:01:22.108446 22447428132992 run.py:483] Algo bellman_ford step 6499 current loss 1.098622, current_train_items 208000.
I0302 19:01:22.128115 22447428132992 run.py:483] Algo bellman_ford step 6500 current loss 0.749747, current_train_items 208032.
I0302 19:01:22.135991 22447428132992 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:01:22.136095 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:01:22.152667 22447428132992 run.py:483] Algo bellman_ford step 6501 current loss 0.787783, current_train_items 208064.
I0302 19:01:22.176992 22447428132992 run.py:483] Algo bellman_ford step 6502 current loss 0.972797, current_train_items 208096.
I0302 19:01:22.207212 22447428132992 run.py:483] Algo bellman_ford step 6503 current loss 1.106299, current_train_items 208128.
I0302 19:01:22.242241 22447428132992 run.py:483] Algo bellman_ford step 6504 current loss 1.023497, current_train_items 208160.
I0302 19:01:22.262118 22447428132992 run.py:483] Algo bellman_ford step 6505 current loss 0.492412, current_train_items 208192.
I0302 19:01:22.277359 22447428132992 run.py:483] Algo bellman_ford step 6506 current loss 0.660584, current_train_items 208224.
I0302 19:01:22.300805 22447428132992 run.py:483] Algo bellman_ford step 6507 current loss 1.048217, current_train_items 208256.
I0302 19:01:22.332567 22447428132992 run.py:483] Algo bellman_ford step 6508 current loss 0.932648, current_train_items 208288.
I0302 19:01:22.363882 22447428132992 run.py:483] Algo bellman_ford step 6509 current loss 1.186479, current_train_items 208320.
I0302 19:01:22.383706 22447428132992 run.py:483] Algo bellman_ford step 6510 current loss 0.534147, current_train_items 208352.
I0302 19:01:22.399868 22447428132992 run.py:483] Algo bellman_ford step 6511 current loss 0.651413, current_train_items 208384.
I0302 19:01:22.424144 22447428132992 run.py:483] Algo bellman_ford step 6512 current loss 0.947558, current_train_items 208416.
I0302 19:01:22.454702 22447428132992 run.py:483] Algo bellman_ford step 6513 current loss 0.936624, current_train_items 208448.
I0302 19:01:22.489231 22447428132992 run.py:483] Algo bellman_ford step 6514 current loss 1.100768, current_train_items 208480.
I0302 19:01:22.508810 22447428132992 run.py:483] Algo bellman_ford step 6515 current loss 0.575191, current_train_items 208512.
I0302 19:01:22.524718 22447428132992 run.py:483] Algo bellman_ford step 6516 current loss 0.713762, current_train_items 208544.
I0302 19:01:22.548364 22447428132992 run.py:483] Algo bellman_ford step 6517 current loss 0.966479, current_train_items 208576.
I0302 19:01:22.579078 22447428132992 run.py:483] Algo bellman_ford step 6518 current loss 0.932515, current_train_items 208608.
I0302 19:01:22.611634 22447428132992 run.py:483] Algo bellman_ford step 6519 current loss 1.093087, current_train_items 208640.
I0302 19:01:22.631248 22447428132992 run.py:483] Algo bellman_ford step 6520 current loss 0.558016, current_train_items 208672.
I0302 19:01:22.647279 22447428132992 run.py:483] Algo bellman_ford step 6521 current loss 0.824967, current_train_items 208704.
I0302 19:01:22.671106 22447428132992 run.py:483] Algo bellman_ford step 6522 current loss 1.030334, current_train_items 208736.
I0302 19:01:22.701222 22447428132992 run.py:483] Algo bellman_ford step 6523 current loss 0.947981, current_train_items 208768.
I0302 19:01:22.732207 22447428132992 run.py:483] Algo bellman_ford step 6524 current loss 0.966416, current_train_items 208800.
I0302 19:01:22.752012 22447428132992 run.py:483] Algo bellman_ford step 6525 current loss 0.579043, current_train_items 208832.
I0302 19:01:22.767904 22447428132992 run.py:483] Algo bellman_ford step 6526 current loss 0.745115, current_train_items 208864.
I0302 19:01:22.790951 22447428132992 run.py:483] Algo bellman_ford step 6527 current loss 0.862780, current_train_items 208896.
I0302 19:01:22.820790 22447428132992 run.py:483] Algo bellman_ford step 6528 current loss 0.923030, current_train_items 208928.
I0302 19:01:22.854776 22447428132992 run.py:483] Algo bellman_ford step 6529 current loss 1.150916, current_train_items 208960.
I0302 19:01:22.874053 22447428132992 run.py:483] Algo bellman_ford step 6530 current loss 0.615337, current_train_items 208992.
I0302 19:01:22.890293 22447428132992 run.py:483] Algo bellman_ford step 6531 current loss 0.786049, current_train_items 209024.
I0302 19:01:22.915221 22447428132992 run.py:483] Algo bellman_ford step 6532 current loss 1.033352, current_train_items 209056.
I0302 19:01:22.945185 22447428132992 run.py:483] Algo bellman_ford step 6533 current loss 1.021317, current_train_items 209088.
I0302 19:01:22.978591 22447428132992 run.py:483] Algo bellman_ford step 6534 current loss 1.176427, current_train_items 209120.
I0302 19:01:22.998037 22447428132992 run.py:483] Algo bellman_ford step 6535 current loss 0.598749, current_train_items 209152.
I0302 19:01:23.014123 22447428132992 run.py:483] Algo bellman_ford step 6536 current loss 0.730879, current_train_items 209184.
I0302 19:01:23.037342 22447428132992 run.py:483] Algo bellman_ford step 6537 current loss 0.876343, current_train_items 209216.
I0302 19:01:23.067421 22447428132992 run.py:483] Algo bellman_ford step 6538 current loss 1.016272, current_train_items 209248.
I0302 19:01:23.100923 22447428132992 run.py:483] Algo bellman_ford step 6539 current loss 1.162529, current_train_items 209280.
I0302 19:01:23.120512 22447428132992 run.py:483] Algo bellman_ford step 6540 current loss 0.608142, current_train_items 209312.
I0302 19:01:23.136551 22447428132992 run.py:483] Algo bellman_ford step 6541 current loss 0.836686, current_train_items 209344.
I0302 19:01:23.159799 22447428132992 run.py:483] Algo bellman_ford step 6542 current loss 0.965835, current_train_items 209376.
I0302 19:01:23.190788 22447428132992 run.py:483] Algo bellman_ford step 6543 current loss 1.047741, current_train_items 209408.
I0302 19:01:23.226231 22447428132992 run.py:483] Algo bellman_ford step 6544 current loss 1.243706, current_train_items 209440.
I0302 19:01:23.245872 22447428132992 run.py:483] Algo bellman_ford step 6545 current loss 0.590687, current_train_items 209472.
I0302 19:01:23.261708 22447428132992 run.py:483] Algo bellman_ford step 6546 current loss 0.738126, current_train_items 209504.
I0302 19:01:23.284614 22447428132992 run.py:483] Algo bellman_ford step 6547 current loss 0.851178, current_train_items 209536.
I0302 19:01:23.316119 22447428132992 run.py:483] Algo bellman_ford step 6548 current loss 1.072162, current_train_items 209568.
I0302 19:01:23.350500 22447428132992 run.py:483] Algo bellman_ford step 6549 current loss 1.118615, current_train_items 209600.
I0302 19:01:23.370009 22447428132992 run.py:483] Algo bellman_ford step 6550 current loss 0.618433, current_train_items 209632.
I0302 19:01:23.378216 22447428132992 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:01:23.378321 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 19:01:23.395262 22447428132992 run.py:483] Algo bellman_ford step 6551 current loss 0.776863, current_train_items 209664.
I0302 19:01:23.418686 22447428132992 run.py:483] Algo bellman_ford step 6552 current loss 0.927744, current_train_items 209696.
I0302 19:01:23.450654 22447428132992 run.py:483] Algo bellman_ford step 6553 current loss 0.996421, current_train_items 209728.
I0302 19:01:23.486293 22447428132992 run.py:483] Algo bellman_ford step 6554 current loss 1.276275, current_train_items 209760.
I0302 19:01:23.506201 22447428132992 run.py:483] Algo bellman_ford step 6555 current loss 1.105960, current_train_items 209792.
I0302 19:01:23.521984 22447428132992 run.py:483] Algo bellman_ford step 6556 current loss 0.848548, current_train_items 209824.
I0302 19:01:23.545886 22447428132992 run.py:483] Algo bellman_ford step 6557 current loss 1.025795, current_train_items 209856.
I0302 19:01:23.577132 22447428132992 run.py:483] Algo bellman_ford step 6558 current loss 1.073922, current_train_items 209888.
I0302 19:01:23.610932 22447428132992 run.py:483] Algo bellman_ford step 6559 current loss 1.184509, current_train_items 209920.
I0302 19:01:23.631063 22447428132992 run.py:483] Algo bellman_ford step 6560 current loss 0.842082, current_train_items 209952.
I0302 19:01:23.646772 22447428132992 run.py:483] Algo bellman_ford step 6561 current loss 0.722466, current_train_items 209984.
I0302 19:01:23.669303 22447428132992 run.py:483] Algo bellman_ford step 6562 current loss 0.937812, current_train_items 210016.
I0302 19:01:23.699538 22447428132992 run.py:483] Algo bellman_ford step 6563 current loss 0.990088, current_train_items 210048.
I0302 19:01:23.730716 22447428132992 run.py:483] Algo bellman_ford step 6564 current loss 1.074795, current_train_items 210080.
I0302 19:01:23.750550 22447428132992 run.py:483] Algo bellman_ford step 6565 current loss 0.585931, current_train_items 210112.
I0302 19:01:23.766291 22447428132992 run.py:483] Algo bellman_ford step 6566 current loss 0.686916, current_train_items 210144.
I0302 19:01:23.789676 22447428132992 run.py:483] Algo bellman_ford step 6567 current loss 0.905831, current_train_items 210176.
I0302 19:01:23.819001 22447428132992 run.py:483] Algo bellman_ford step 6568 current loss 0.999178, current_train_items 210208.
I0302 19:01:23.852499 22447428132992 run.py:483] Algo bellman_ford step 6569 current loss 1.068409, current_train_items 210240.
I0302 19:01:23.872339 22447428132992 run.py:483] Algo bellman_ford step 6570 current loss 0.619228, current_train_items 210272.
I0302 19:01:23.888477 22447428132992 run.py:483] Algo bellman_ford step 6571 current loss 0.796744, current_train_items 210304.
I0302 19:01:23.911337 22447428132992 run.py:483] Algo bellman_ford step 6572 current loss 0.833493, current_train_items 210336.
I0302 19:01:23.941400 22447428132992 run.py:483] Algo bellman_ford step 6573 current loss 0.885487, current_train_items 210368.
I0302 19:01:23.974149 22447428132992 run.py:483] Algo bellman_ford step 6574 current loss 1.140367, current_train_items 210400.
I0302 19:01:23.994246 22447428132992 run.py:483] Algo bellman_ford step 6575 current loss 0.621004, current_train_items 210432.
I0302 19:01:24.010555 22447428132992 run.py:483] Algo bellman_ford step 6576 current loss 0.696586, current_train_items 210464.
I0302 19:01:24.033196 22447428132992 run.py:483] Algo bellman_ford step 6577 current loss 0.824501, current_train_items 210496.
I0302 19:01:24.063983 22447428132992 run.py:483] Algo bellman_ford step 6578 current loss 0.933657, current_train_items 210528.
I0302 19:01:24.097191 22447428132992 run.py:483] Algo bellman_ford step 6579 current loss 1.094662, current_train_items 210560.
I0302 19:01:24.117178 22447428132992 run.py:483] Algo bellman_ford step 6580 current loss 0.602193, current_train_items 210592.
I0302 19:01:24.133237 22447428132992 run.py:483] Algo bellman_ford step 6581 current loss 0.766261, current_train_items 210624.
I0302 19:01:24.156883 22447428132992 run.py:483] Algo bellman_ford step 6582 current loss 0.992185, current_train_items 210656.
I0302 19:01:24.188711 22447428132992 run.py:483] Algo bellman_ford step 6583 current loss 1.049500, current_train_items 210688.
I0302 19:01:24.221340 22447428132992 run.py:483] Algo bellman_ford step 6584 current loss 0.980168, current_train_items 210720.
I0302 19:01:24.240922 22447428132992 run.py:483] Algo bellman_ford step 6585 current loss 0.686272, current_train_items 210752.
I0302 19:01:24.256571 22447428132992 run.py:483] Algo bellman_ford step 6586 current loss 0.774020, current_train_items 210784.
I0302 19:01:24.278306 22447428132992 run.py:483] Algo bellman_ford step 6587 current loss 0.940210, current_train_items 210816.
I0302 19:01:24.308007 22447428132992 run.py:483] Algo bellman_ford step 6588 current loss 1.205743, current_train_items 210848.
I0302 19:01:24.339656 22447428132992 run.py:483] Algo bellman_ford step 6589 current loss 1.091794, current_train_items 210880.
I0302 19:01:24.359680 22447428132992 run.py:483] Algo bellman_ford step 6590 current loss 0.666022, current_train_items 210912.
I0302 19:01:24.375558 22447428132992 run.py:483] Algo bellman_ford step 6591 current loss 0.676524, current_train_items 210944.
I0302 19:01:24.398541 22447428132992 run.py:483] Algo bellman_ford step 6592 current loss 0.875524, current_train_items 210976.
I0302 19:01:24.428761 22447428132992 run.py:483] Algo bellman_ford step 6593 current loss 0.863769, current_train_items 211008.
I0302 19:01:24.460969 22447428132992 run.py:483] Algo bellman_ford step 6594 current loss 1.115098, current_train_items 211040.
I0302 19:01:24.480697 22447428132992 run.py:483] Algo bellman_ford step 6595 current loss 0.608401, current_train_items 211072.
I0302 19:01:24.496688 22447428132992 run.py:483] Algo bellman_ford step 6596 current loss 0.668948, current_train_items 211104.
I0302 19:01:24.520418 22447428132992 run.py:483] Algo bellman_ford step 6597 current loss 0.893726, current_train_items 211136.
I0302 19:01:24.551590 22447428132992 run.py:483] Algo bellman_ford step 6598 current loss 0.982263, current_train_items 211168.
I0302 19:01:24.581728 22447428132992 run.py:483] Algo bellman_ford step 6599 current loss 0.932954, current_train_items 211200.
I0302 19:01:24.601498 22447428132992 run.py:483] Algo bellman_ford step 6600 current loss 0.588263, current_train_items 211232.
I0302 19:01:24.609369 22447428132992 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:01:24.609483 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:01:24.625984 22447428132992 run.py:483] Algo bellman_ford step 6601 current loss 0.733765, current_train_items 211264.
I0302 19:01:24.648268 22447428132992 run.py:483] Algo bellman_ford step 6602 current loss 0.889769, current_train_items 211296.
I0302 19:01:24.679246 22447428132992 run.py:483] Algo bellman_ford step 6603 current loss 0.998455, current_train_items 211328.
I0302 19:01:24.713010 22447428132992 run.py:483] Algo bellman_ford step 6604 current loss 1.131194, current_train_items 211360.
I0302 19:01:24.733129 22447428132992 run.py:483] Algo bellman_ford step 6605 current loss 0.542368, current_train_items 211392.
I0302 19:01:24.748770 22447428132992 run.py:483] Algo bellman_ford step 6606 current loss 0.710249, current_train_items 211424.
I0302 19:01:24.772121 22447428132992 run.py:483] Algo bellman_ford step 6607 current loss 0.868460, current_train_items 211456.
I0302 19:01:24.802909 22447428132992 run.py:483] Algo bellman_ford step 6608 current loss 0.907852, current_train_items 211488.
I0302 19:01:24.834159 22447428132992 run.py:483] Algo bellman_ford step 6609 current loss 0.940085, current_train_items 211520.
I0302 19:01:24.853642 22447428132992 run.py:483] Algo bellman_ford step 6610 current loss 0.690495, current_train_items 211552.
I0302 19:01:24.869413 22447428132992 run.py:483] Algo bellman_ford step 6611 current loss 0.774167, current_train_items 211584.
I0302 19:01:24.891302 22447428132992 run.py:483] Algo bellman_ford step 6612 current loss 0.799545, current_train_items 211616.
I0302 19:01:24.922022 22447428132992 run.py:483] Algo bellman_ford step 6613 current loss 1.001070, current_train_items 211648.
I0302 19:01:24.957571 22447428132992 run.py:483] Algo bellman_ford step 6614 current loss 1.205305, current_train_items 211680.
I0302 19:01:24.977089 22447428132992 run.py:483] Algo bellman_ford step 6615 current loss 0.626591, current_train_items 211712.
I0302 19:01:24.992956 22447428132992 run.py:483] Algo bellman_ford step 6616 current loss 0.828155, current_train_items 211744.
I0302 19:01:25.016373 22447428132992 run.py:483] Algo bellman_ford step 6617 current loss 0.909881, current_train_items 211776.
I0302 19:01:25.046811 22447428132992 run.py:483] Algo bellman_ford step 6618 current loss 0.928956, current_train_items 211808.
I0302 19:01:25.080213 22447428132992 run.py:483] Algo bellman_ford step 6619 current loss 1.223083, current_train_items 211840.
I0302 19:01:25.099756 22447428132992 run.py:483] Algo bellman_ford step 6620 current loss 0.567082, current_train_items 211872.
I0302 19:01:25.115412 22447428132992 run.py:483] Algo bellman_ford step 6621 current loss 0.725315, current_train_items 211904.
I0302 19:01:25.137256 22447428132992 run.py:483] Algo bellman_ford step 6622 current loss 0.916873, current_train_items 211936.
I0302 19:01:25.168427 22447428132992 run.py:483] Algo bellman_ford step 6623 current loss 0.971706, current_train_items 211968.
I0302 19:01:25.200493 22447428132992 run.py:483] Algo bellman_ford step 6624 current loss 1.039217, current_train_items 212000.
I0302 19:01:25.219700 22447428132992 run.py:483] Algo bellman_ford step 6625 current loss 0.548064, current_train_items 212032.
I0302 19:01:25.235795 22447428132992 run.py:483] Algo bellman_ford step 6626 current loss 0.828764, current_train_items 212064.
I0302 19:01:25.258805 22447428132992 run.py:483] Algo bellman_ford step 6627 current loss 0.912525, current_train_items 212096.
I0302 19:01:25.287969 22447428132992 run.py:483] Algo bellman_ford step 6628 current loss 1.096843, current_train_items 212128.
I0302 19:01:25.319336 22447428132992 run.py:483] Algo bellman_ford step 6629 current loss 1.075157, current_train_items 212160.
I0302 19:01:25.338421 22447428132992 run.py:483] Algo bellman_ford step 6630 current loss 0.688294, current_train_items 212192.
I0302 19:01:25.354423 22447428132992 run.py:483] Algo bellman_ford step 6631 current loss 0.800157, current_train_items 212224.
I0302 19:01:25.378340 22447428132992 run.py:483] Algo bellman_ford step 6632 current loss 1.013205, current_train_items 212256.
I0302 19:01:25.409575 22447428132992 run.py:483] Algo bellman_ford step 6633 current loss 1.056322, current_train_items 212288.
I0302 19:01:25.441197 22447428132992 run.py:483] Algo bellman_ford step 6634 current loss 1.156743, current_train_items 212320.
I0302 19:01:25.460776 22447428132992 run.py:483] Algo bellman_ford step 6635 current loss 0.513162, current_train_items 212352.
I0302 19:01:25.476934 22447428132992 run.py:483] Algo bellman_ford step 6636 current loss 0.800161, current_train_items 212384.
I0302 19:01:25.500260 22447428132992 run.py:483] Algo bellman_ford step 6637 current loss 0.874069, current_train_items 212416.
I0302 19:01:25.530601 22447428132992 run.py:483] Algo bellman_ford step 6638 current loss 1.013560, current_train_items 212448.
I0302 19:01:25.563907 22447428132992 run.py:483] Algo bellman_ford step 6639 current loss 1.158147, current_train_items 212480.
I0302 19:01:25.583153 22447428132992 run.py:483] Algo bellman_ford step 6640 current loss 0.550537, current_train_items 212512.
I0302 19:01:25.599312 22447428132992 run.py:483] Algo bellman_ford step 6641 current loss 0.821629, current_train_items 212544.
I0302 19:01:25.623431 22447428132992 run.py:483] Algo bellman_ford step 6642 current loss 1.013755, current_train_items 212576.
I0302 19:01:25.653566 22447428132992 run.py:483] Algo bellman_ford step 6643 current loss 1.104283, current_train_items 212608.
I0302 19:01:25.687782 22447428132992 run.py:483] Algo bellman_ford step 6644 current loss 1.173617, current_train_items 212640.
I0302 19:01:25.706948 22447428132992 run.py:483] Algo bellman_ford step 6645 current loss 0.565151, current_train_items 212672.
I0302 19:01:25.722885 22447428132992 run.py:483] Algo bellman_ford step 6646 current loss 0.716769, current_train_items 212704.
I0302 19:01:25.745788 22447428132992 run.py:483] Algo bellman_ford step 6647 current loss 1.014561, current_train_items 212736.
I0302 19:01:25.776698 22447428132992 run.py:483] Algo bellman_ford step 6648 current loss 1.064126, current_train_items 212768.
I0302 19:01:25.810562 22447428132992 run.py:483] Algo bellman_ford step 6649 current loss 1.151767, current_train_items 212800.
I0302 19:01:25.829935 22447428132992 run.py:483] Algo bellman_ford step 6650 current loss 0.563290, current_train_items 212832.
I0302 19:01:25.837995 22447428132992 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:01:25.838100 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:25.855129 22447428132992 run.py:483] Algo bellman_ford step 6651 current loss 0.830406, current_train_items 212864.
I0302 19:01:25.879636 22447428132992 run.py:483] Algo bellman_ford step 6652 current loss 0.918542, current_train_items 212896.
I0302 19:01:25.911087 22447428132992 run.py:483] Algo bellman_ford step 6653 current loss 1.034092, current_train_items 212928.
I0302 19:01:25.940753 22447428132992 run.py:483] Algo bellman_ford step 6654 current loss 0.966184, current_train_items 212960.
I0302 19:01:25.960675 22447428132992 run.py:483] Algo bellman_ford step 6655 current loss 0.553319, current_train_items 212992.
I0302 19:01:25.976314 22447428132992 run.py:483] Algo bellman_ford step 6656 current loss 0.758026, current_train_items 213024.
I0302 19:01:25.999369 22447428132992 run.py:483] Algo bellman_ford step 6657 current loss 0.854747, current_train_items 213056.
I0302 19:01:26.030369 22447428132992 run.py:483] Algo bellman_ford step 6658 current loss 1.050912, current_train_items 213088.
I0302 19:01:26.065248 22447428132992 run.py:483] Algo bellman_ford step 6659 current loss 1.168521, current_train_items 213120.
I0302 19:01:26.085449 22447428132992 run.py:483] Algo bellman_ford step 6660 current loss 0.938558, current_train_items 213152.
I0302 19:01:26.101831 22447428132992 run.py:483] Algo bellman_ford step 6661 current loss 0.777434, current_train_items 213184.
I0302 19:01:26.126150 22447428132992 run.py:483] Algo bellman_ford step 6662 current loss 0.963057, current_train_items 213216.
I0302 19:01:26.156014 22447428132992 run.py:483] Algo bellman_ford step 6663 current loss 0.931910, current_train_items 213248.
I0302 19:01:26.188912 22447428132992 run.py:483] Algo bellman_ford step 6664 current loss 0.998014, current_train_items 213280.
I0302 19:01:26.208475 22447428132992 run.py:483] Algo bellman_ford step 6665 current loss 0.777690, current_train_items 213312.
I0302 19:01:26.224506 22447428132992 run.py:483] Algo bellman_ford step 6666 current loss 0.742195, current_train_items 213344.
I0302 19:01:26.249865 22447428132992 run.py:483] Algo bellman_ford step 6667 current loss 0.991517, current_train_items 213376.
I0302 19:01:26.281689 22447428132992 run.py:483] Algo bellman_ford step 6668 current loss 1.061099, current_train_items 213408.
I0302 19:01:26.316602 22447428132992 run.py:483] Algo bellman_ford step 6669 current loss 1.235820, current_train_items 213440.
I0302 19:01:26.336798 22447428132992 run.py:483] Algo bellman_ford step 6670 current loss 0.630621, current_train_items 213472.
I0302 19:01:26.352912 22447428132992 run.py:483] Algo bellman_ford step 6671 current loss 0.760544, current_train_items 213504.
I0302 19:01:26.376457 22447428132992 run.py:483] Algo bellman_ford step 6672 current loss 0.908303, current_train_items 213536.
I0302 19:01:26.407541 22447428132992 run.py:483] Algo bellman_ford step 6673 current loss 1.100385, current_train_items 213568.
I0302 19:01:26.439828 22447428132992 run.py:483] Algo bellman_ford step 6674 current loss 1.084538, current_train_items 213600.
I0302 19:01:26.459998 22447428132992 run.py:483] Algo bellman_ford step 6675 current loss 0.601616, current_train_items 213632.
I0302 19:01:26.475583 22447428132992 run.py:483] Algo bellman_ford step 6676 current loss 0.779707, current_train_items 213664.
I0302 19:01:26.499521 22447428132992 run.py:483] Algo bellman_ford step 6677 current loss 1.009924, current_train_items 213696.
I0302 19:01:26.531132 22447428132992 run.py:483] Algo bellman_ford step 6678 current loss 1.048111, current_train_items 213728.
I0302 19:01:26.566521 22447428132992 run.py:483] Algo bellman_ford step 6679 current loss 1.233066, current_train_items 213760.
I0302 19:01:26.586153 22447428132992 run.py:483] Algo bellman_ford step 6680 current loss 0.544543, current_train_items 213792.
I0302 19:01:26.602239 22447428132992 run.py:483] Algo bellman_ford step 6681 current loss 0.823054, current_train_items 213824.
I0302 19:01:26.626717 22447428132992 run.py:483] Algo bellman_ford step 6682 current loss 0.900949, current_train_items 213856.
I0302 19:01:26.657113 22447428132992 run.py:483] Algo bellman_ford step 6683 current loss 1.108062, current_train_items 213888.
I0302 19:01:26.693460 22447428132992 run.py:483] Algo bellman_ford step 6684 current loss 1.294986, current_train_items 213920.
I0302 19:01:26.713480 22447428132992 run.py:483] Algo bellman_ford step 6685 current loss 0.541978, current_train_items 213952.
I0302 19:01:26.729432 22447428132992 run.py:483] Algo bellman_ford step 6686 current loss 0.803077, current_train_items 213984.
I0302 19:01:26.752894 22447428132992 run.py:483] Algo bellman_ford step 6687 current loss 0.957835, current_train_items 214016.
I0302 19:01:26.784033 22447428132992 run.py:483] Algo bellman_ford step 6688 current loss 0.997152, current_train_items 214048.
I0302 19:01:26.816820 22447428132992 run.py:483] Algo bellman_ford step 6689 current loss 1.037349, current_train_items 214080.
I0302 19:01:26.836723 22447428132992 run.py:483] Algo bellman_ford step 6690 current loss 0.654546, current_train_items 214112.
I0302 19:01:26.852619 22447428132992 run.py:483] Algo bellman_ford step 6691 current loss 0.677830, current_train_items 214144.
I0302 19:01:26.876598 22447428132992 run.py:483] Algo bellman_ford step 6692 current loss 1.008217, current_train_items 214176.
I0302 19:01:26.907612 22447428132992 run.py:483] Algo bellman_ford step 6693 current loss 0.991603, current_train_items 214208.
I0302 19:01:26.941639 22447428132992 run.py:483] Algo bellman_ford step 6694 current loss 1.022437, current_train_items 214240.
I0302 19:01:26.961427 22447428132992 run.py:483] Algo bellman_ford step 6695 current loss 0.578885, current_train_items 214272.
I0302 19:01:26.977692 22447428132992 run.py:483] Algo bellman_ford step 6696 current loss 0.779260, current_train_items 214304.
I0302 19:01:27.001408 22447428132992 run.py:483] Algo bellman_ford step 6697 current loss 1.004992, current_train_items 214336.
I0302 19:01:27.032777 22447428132992 run.py:483] Algo bellman_ford step 6698 current loss 1.052925, current_train_items 214368.
I0302 19:01:27.067338 22447428132992 run.py:483] Algo bellman_ford step 6699 current loss 1.128129, current_train_items 214400.
I0302 19:01:27.086979 22447428132992 run.py:483] Algo bellman_ford step 6700 current loss 0.767154, current_train_items 214432.
I0302 19:01:27.094805 22447428132992 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:01:27.094913 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:01:27.111960 22447428132992 run.py:483] Algo bellman_ford step 6701 current loss 0.793701, current_train_items 214464.
I0302 19:01:27.135782 22447428132992 run.py:483] Algo bellman_ford step 6702 current loss 0.997835, current_train_items 214496.
I0302 19:01:27.167331 22447428132992 run.py:483] Algo bellman_ford step 6703 current loss 1.055277, current_train_items 214528.
I0302 19:01:27.200264 22447428132992 run.py:483] Algo bellman_ford step 6704 current loss 1.024608, current_train_items 214560.
I0302 19:01:27.220305 22447428132992 run.py:483] Algo bellman_ford step 6705 current loss 0.529837, current_train_items 214592.
I0302 19:01:27.236187 22447428132992 run.py:483] Algo bellman_ford step 6706 current loss 0.774829, current_train_items 214624.
I0302 19:01:27.260927 22447428132992 run.py:483] Algo bellman_ford step 6707 current loss 0.937672, current_train_items 214656.
I0302 19:01:27.291870 22447428132992 run.py:483] Algo bellman_ford step 6708 current loss 0.983788, current_train_items 214688.
I0302 19:01:27.326224 22447428132992 run.py:483] Algo bellman_ford step 6709 current loss 1.130206, current_train_items 214720.
I0302 19:01:27.345820 22447428132992 run.py:483] Algo bellman_ford step 6710 current loss 0.599252, current_train_items 214752.
I0302 19:01:27.362103 22447428132992 run.py:483] Algo bellman_ford step 6711 current loss 0.765386, current_train_items 214784.
I0302 19:01:27.384645 22447428132992 run.py:483] Algo bellman_ford step 6712 current loss 0.841347, current_train_items 214816.
I0302 19:01:27.416761 22447428132992 run.py:483] Algo bellman_ford step 6713 current loss 1.033703, current_train_items 214848.
I0302 19:01:27.450087 22447428132992 run.py:483] Algo bellman_ford step 6714 current loss 0.993481, current_train_items 214880.
I0302 19:01:27.469550 22447428132992 run.py:483] Algo bellman_ford step 6715 current loss 0.673751, current_train_items 214912.
I0302 19:01:27.485624 22447428132992 run.py:483] Algo bellman_ford step 6716 current loss 0.751192, current_train_items 214944.
I0302 19:01:27.509268 22447428132992 run.py:483] Algo bellman_ford step 6717 current loss 0.940931, current_train_items 214976.
I0302 19:01:27.540280 22447428132992 run.py:483] Algo bellman_ford step 6718 current loss 1.023282, current_train_items 215008.
I0302 19:01:27.575363 22447428132992 run.py:483] Algo bellman_ford step 6719 current loss 1.185843, current_train_items 215040.
I0302 19:01:27.594726 22447428132992 run.py:483] Algo bellman_ford step 6720 current loss 0.573452, current_train_items 215072.
I0302 19:01:27.610723 22447428132992 run.py:483] Algo bellman_ford step 6721 current loss 0.709065, current_train_items 215104.
I0302 19:01:27.633437 22447428132992 run.py:483] Algo bellman_ford step 6722 current loss 0.889298, current_train_items 215136.
I0302 19:01:27.665162 22447428132992 run.py:483] Algo bellman_ford step 6723 current loss 1.062922, current_train_items 215168.
I0302 19:01:27.697436 22447428132992 run.py:483] Algo bellman_ford step 6724 current loss 0.961223, current_train_items 215200.
I0302 19:01:27.716845 22447428132992 run.py:483] Algo bellman_ford step 6725 current loss 0.577339, current_train_items 215232.
I0302 19:01:27.732925 22447428132992 run.py:483] Algo bellman_ford step 6726 current loss 0.770040, current_train_items 215264.
I0302 19:01:27.756171 22447428132992 run.py:483] Algo bellman_ford step 6727 current loss 0.842128, current_train_items 215296.
I0302 19:01:27.786935 22447428132992 run.py:483] Algo bellman_ford step 6728 current loss 1.009723, current_train_items 215328.
I0302 19:01:27.818828 22447428132992 run.py:483] Algo bellman_ford step 6729 current loss 0.990662, current_train_items 215360.
I0302 19:01:27.838302 22447428132992 run.py:483] Algo bellman_ford step 6730 current loss 0.543689, current_train_items 215392.
I0302 19:01:27.854199 22447428132992 run.py:483] Algo bellman_ford step 6731 current loss 0.650607, current_train_items 215424.
I0302 19:01:27.878244 22447428132992 run.py:483] Algo bellman_ford step 6732 current loss 1.042330, current_train_items 215456.
I0302 19:01:27.907052 22447428132992 run.py:483] Algo bellman_ford step 6733 current loss 0.966648, current_train_items 215488.
I0302 19:01:27.938443 22447428132992 run.py:483] Algo bellman_ford step 6734 current loss 0.995147, current_train_items 215520.
I0302 19:01:27.958209 22447428132992 run.py:483] Algo bellman_ford step 6735 current loss 0.751965, current_train_items 215552.
I0302 19:01:27.974421 22447428132992 run.py:483] Algo bellman_ford step 6736 current loss 0.860024, current_train_items 215584.
I0302 19:01:27.998757 22447428132992 run.py:483] Algo bellman_ford step 6737 current loss 0.947145, current_train_items 215616.
I0302 19:01:28.028545 22447428132992 run.py:483] Algo bellman_ford step 6738 current loss 0.956052, current_train_items 215648.
I0302 19:01:28.063703 22447428132992 run.py:483] Algo bellman_ford step 6739 current loss 1.169206, current_train_items 215680.
I0302 19:01:28.083358 22447428132992 run.py:483] Algo bellman_ford step 6740 current loss 0.556063, current_train_items 215712.
I0302 19:01:28.099555 22447428132992 run.py:483] Algo bellman_ford step 6741 current loss 0.739517, current_train_items 215744.
I0302 19:01:28.123296 22447428132992 run.py:483] Algo bellman_ford step 6742 current loss 0.876988, current_train_items 215776.
I0302 19:01:28.152956 22447428132992 run.py:483] Algo bellman_ford step 6743 current loss 0.883869, current_train_items 215808.
I0302 19:01:28.187284 22447428132992 run.py:483] Algo bellman_ford step 6744 current loss 1.141705, current_train_items 215840.
I0302 19:01:28.206848 22447428132992 run.py:483] Algo bellman_ford step 6745 current loss 0.495320, current_train_items 215872.
I0302 19:01:28.223028 22447428132992 run.py:483] Algo bellman_ford step 6746 current loss 0.716574, current_train_items 215904.
I0302 19:01:28.246685 22447428132992 run.py:483] Algo bellman_ford step 6747 current loss 0.941842, current_train_items 215936.
I0302 19:01:28.276769 22447428132992 run.py:483] Algo bellman_ford step 6748 current loss 0.913088, current_train_items 215968.
I0302 19:01:28.310337 22447428132992 run.py:483] Algo bellman_ford step 6749 current loss 1.183305, current_train_items 216000.
I0302 19:01:28.329907 22447428132992 run.py:483] Algo bellman_ford step 6750 current loss 0.545338, current_train_items 216032.
I0302 19:01:28.337859 22447428132992 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:01:28.337966 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:01:28.354525 22447428132992 run.py:483] Algo bellman_ford step 6751 current loss 0.644889, current_train_items 216064.
I0302 19:01:28.378431 22447428132992 run.py:483] Algo bellman_ford step 6752 current loss 1.035062, current_train_items 216096.
I0302 19:01:28.410183 22447428132992 run.py:483] Algo bellman_ford step 6753 current loss 1.053192, current_train_items 216128.
I0302 19:01:28.444597 22447428132992 run.py:483] Algo bellman_ford step 6754 current loss 1.148513, current_train_items 216160.
I0302 19:01:28.464309 22447428132992 run.py:483] Algo bellman_ford step 6755 current loss 0.712239, current_train_items 216192.
I0302 19:01:28.480587 22447428132992 run.py:483] Algo bellman_ford step 6756 current loss 0.794269, current_train_items 216224.
I0302 19:01:28.503426 22447428132992 run.py:483] Algo bellman_ford step 6757 current loss 0.842945, current_train_items 216256.
I0302 19:01:28.534102 22447428132992 run.py:483] Algo bellman_ford step 6758 current loss 1.104837, current_train_items 216288.
I0302 19:01:28.568425 22447428132992 run.py:483] Algo bellman_ford step 6759 current loss 1.242883, current_train_items 216320.
I0302 19:01:28.588109 22447428132992 run.py:483] Algo bellman_ford step 6760 current loss 0.577500, current_train_items 216352.
I0302 19:01:28.604541 22447428132992 run.py:483] Algo bellman_ford step 6761 current loss 0.834979, current_train_items 216384.
I0302 19:01:28.627424 22447428132992 run.py:483] Algo bellman_ford step 6762 current loss 0.952243, current_train_items 216416.
I0302 19:01:28.658296 22447428132992 run.py:483] Algo bellman_ford step 6763 current loss 1.047364, current_train_items 216448.
I0302 19:01:28.690739 22447428132992 run.py:483] Algo bellman_ford step 6764 current loss 1.093813, current_train_items 216480.
I0302 19:01:28.710061 22447428132992 run.py:483] Algo bellman_ford step 6765 current loss 0.563258, current_train_items 216512.
I0302 19:01:28.726416 22447428132992 run.py:483] Algo bellman_ford step 6766 current loss 0.770343, current_train_items 216544.
I0302 19:01:28.749515 22447428132992 run.py:483] Algo bellman_ford step 6767 current loss 0.941994, current_train_items 216576.
I0302 19:01:28.779084 22447428132992 run.py:483] Algo bellman_ford step 6768 current loss 1.012103, current_train_items 216608.
I0302 19:01:28.812046 22447428132992 run.py:483] Algo bellman_ford step 6769 current loss 1.117395, current_train_items 216640.
I0302 19:01:28.831509 22447428132992 run.py:483] Algo bellman_ford step 6770 current loss 0.621554, current_train_items 216672.
I0302 19:01:28.847459 22447428132992 run.py:483] Algo bellman_ford step 6771 current loss 0.738496, current_train_items 216704.
I0302 19:01:28.869512 22447428132992 run.py:483] Algo bellman_ford step 6772 current loss 0.841191, current_train_items 216736.
I0302 19:01:28.900126 22447428132992 run.py:483] Algo bellman_ford step 6773 current loss 0.973574, current_train_items 216768.
I0302 19:01:28.931477 22447428132992 run.py:483] Algo bellman_ford step 6774 current loss 0.978029, current_train_items 216800.
I0302 19:01:28.951055 22447428132992 run.py:483] Algo bellman_ford step 6775 current loss 0.609617, current_train_items 216832.
I0302 19:01:28.967238 22447428132992 run.py:483] Algo bellman_ford step 6776 current loss 0.744220, current_train_items 216864.
I0302 19:01:28.989618 22447428132992 run.py:483] Algo bellman_ford step 6777 current loss 0.936651, current_train_items 216896.
I0302 19:01:29.021445 22447428132992 run.py:483] Algo bellman_ford step 6778 current loss 1.117468, current_train_items 216928.
I0302 19:01:29.053820 22447428132992 run.py:483] Algo bellman_ford step 6779 current loss 0.967895, current_train_items 216960.
I0302 19:01:29.073129 22447428132992 run.py:483] Algo bellman_ford step 6780 current loss 0.547274, current_train_items 216992.
I0302 19:01:29.089011 22447428132992 run.py:483] Algo bellman_ford step 6781 current loss 0.707414, current_train_items 217024.
I0302 19:01:29.111396 22447428132992 run.py:483] Algo bellman_ford step 6782 current loss 0.922494, current_train_items 217056.
I0302 19:01:29.142855 22447428132992 run.py:483] Algo bellman_ford step 6783 current loss 1.026964, current_train_items 217088.
I0302 19:01:29.175297 22447428132992 run.py:483] Algo bellman_ford step 6784 current loss 1.214498, current_train_items 217120.
I0302 19:01:29.194771 22447428132992 run.py:483] Algo bellman_ford step 6785 current loss 0.614266, current_train_items 217152.
I0302 19:01:29.210512 22447428132992 run.py:483] Algo bellman_ford step 6786 current loss 0.694486, current_train_items 217184.
I0302 19:01:29.232938 22447428132992 run.py:483] Algo bellman_ford step 6787 current loss 0.922856, current_train_items 217216.
I0302 19:01:29.262361 22447428132992 run.py:483] Algo bellman_ford step 6788 current loss 0.925206, current_train_items 217248.
I0302 19:01:29.295898 22447428132992 run.py:483] Algo bellman_ford step 6789 current loss 1.035022, current_train_items 217280.
I0302 19:01:29.315962 22447428132992 run.py:483] Algo bellman_ford step 6790 current loss 0.692469, current_train_items 217312.
I0302 19:01:29.331836 22447428132992 run.py:483] Algo bellman_ford step 6791 current loss 0.716444, current_train_items 217344.
I0302 19:01:29.354810 22447428132992 run.py:483] Algo bellman_ford step 6792 current loss 0.807304, current_train_items 217376.
I0302 19:01:29.384456 22447428132992 run.py:483] Algo bellman_ford step 6793 current loss 0.897822, current_train_items 217408.
I0302 19:01:29.417596 22447428132992 run.py:483] Algo bellman_ford step 6794 current loss 1.365817, current_train_items 217440.
I0302 19:01:29.436853 22447428132992 run.py:483] Algo bellman_ford step 6795 current loss 0.585012, current_train_items 217472.
I0302 19:01:29.452601 22447428132992 run.py:483] Algo bellman_ford step 6796 current loss 0.733706, current_train_items 217504.
I0302 19:01:29.476448 22447428132992 run.py:483] Algo bellman_ford step 6797 current loss 0.945277, current_train_items 217536.
I0302 19:01:29.506280 22447428132992 run.py:483] Algo bellman_ford step 6798 current loss 1.010831, current_train_items 217568.
I0302 19:01:29.538996 22447428132992 run.py:483] Algo bellman_ford step 6799 current loss 1.064713, current_train_items 217600.
I0302 19:01:29.558574 22447428132992 run.py:483] Algo bellman_ford step 6800 current loss 0.529445, current_train_items 217632.
I0302 19:01:29.566459 22447428132992 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:01:29.566563 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:01:29.582788 22447428132992 run.py:483] Algo bellman_ford step 6801 current loss 0.811422, current_train_items 217664.
I0302 19:01:29.606373 22447428132992 run.py:483] Algo bellman_ford step 6802 current loss 0.872849, current_train_items 217696.
I0302 19:01:29.638491 22447428132992 run.py:483] Algo bellman_ford step 6803 current loss 0.964781, current_train_items 217728.
I0302 19:01:29.672484 22447428132992 run.py:483] Algo bellman_ford step 6804 current loss 1.067924, current_train_items 217760.
I0302 19:01:29.692317 22447428132992 run.py:483] Algo bellman_ford step 6805 current loss 0.486726, current_train_items 217792.
I0302 19:01:29.708255 22447428132992 run.py:483] Algo bellman_ford step 6806 current loss 0.692952, current_train_items 217824.
I0302 19:01:29.731814 22447428132992 run.py:483] Algo bellman_ford step 6807 current loss 0.910781, current_train_items 217856.
I0302 19:01:29.761062 22447428132992 run.py:483] Algo bellman_ford step 6808 current loss 0.926889, current_train_items 217888.
I0302 19:01:29.793941 22447428132992 run.py:483] Algo bellman_ford step 6809 current loss 1.102041, current_train_items 217920.
I0302 19:01:29.813493 22447428132992 run.py:483] Algo bellman_ford step 6810 current loss 0.559980, current_train_items 217952.
I0302 19:01:29.829031 22447428132992 run.py:483] Algo bellman_ford step 6811 current loss 0.698994, current_train_items 217984.
I0302 19:01:29.853111 22447428132992 run.py:483] Algo bellman_ford step 6812 current loss 0.972465, current_train_items 218016.
I0302 19:01:29.883143 22447428132992 run.py:483] Algo bellman_ford step 6813 current loss 1.070717, current_train_items 218048.
I0302 19:01:29.914301 22447428132992 run.py:483] Algo bellman_ford step 6814 current loss 1.164539, current_train_items 218080.
I0302 19:01:29.933626 22447428132992 run.py:483] Algo bellman_ford step 6815 current loss 0.840237, current_train_items 218112.
I0302 19:01:29.949545 22447428132992 run.py:483] Algo bellman_ford step 6816 current loss 0.695590, current_train_items 218144.
I0302 19:01:29.971529 22447428132992 run.py:483] Algo bellman_ford step 6817 current loss 0.887546, current_train_items 218176.
I0302 19:01:30.002222 22447428132992 run.py:483] Algo bellman_ford step 6818 current loss 0.984780, current_train_items 218208.
I0302 19:01:30.037271 22447428132992 run.py:483] Algo bellman_ford step 6819 current loss 1.278673, current_train_items 218240.
I0302 19:01:30.056569 22447428132992 run.py:483] Algo bellman_ford step 6820 current loss 0.598396, current_train_items 218272.
I0302 19:01:30.072487 22447428132992 run.py:483] Algo bellman_ford step 6821 current loss 0.714295, current_train_items 218304.
I0302 19:01:30.095237 22447428132992 run.py:483] Algo bellman_ford step 6822 current loss 0.948483, current_train_items 218336.
I0302 19:01:30.125780 22447428132992 run.py:483] Algo bellman_ford step 6823 current loss 1.092120, current_train_items 218368.
I0302 19:01:30.159930 22447428132992 run.py:483] Algo bellman_ford step 6824 current loss 1.061695, current_train_items 218400.
I0302 19:01:30.179346 22447428132992 run.py:483] Algo bellman_ford step 6825 current loss 0.599880, current_train_items 218432.
I0302 19:01:30.195512 22447428132992 run.py:483] Algo bellman_ford step 6826 current loss 0.770910, current_train_items 218464.
I0302 19:01:30.218434 22447428132992 run.py:483] Algo bellman_ford step 6827 current loss 0.857863, current_train_items 218496.
I0302 19:01:30.248253 22447428132992 run.py:483] Algo bellman_ford step 6828 current loss 0.888612, current_train_items 218528.
I0302 19:01:30.283308 22447428132992 run.py:483] Algo bellman_ford step 6829 current loss 1.126739, current_train_items 218560.
I0302 19:01:30.302707 22447428132992 run.py:483] Algo bellman_ford step 6830 current loss 0.634791, current_train_items 218592.
I0302 19:01:30.318475 22447428132992 run.py:483] Algo bellman_ford step 6831 current loss 0.659022, current_train_items 218624.
I0302 19:01:30.343143 22447428132992 run.py:483] Algo bellman_ford step 6832 current loss 1.022106, current_train_items 218656.
I0302 19:01:30.373374 22447428132992 run.py:483] Algo bellman_ford step 6833 current loss 0.955259, current_train_items 218688.
I0302 19:01:30.406588 22447428132992 run.py:483] Algo bellman_ford step 6834 current loss 1.215058, current_train_items 218720.
I0302 19:01:30.425934 22447428132992 run.py:483] Algo bellman_ford step 6835 current loss 0.533919, current_train_items 218752.
I0302 19:01:30.442113 22447428132992 run.py:483] Algo bellman_ford step 6836 current loss 0.714569, current_train_items 218784.
I0302 19:01:30.465119 22447428132992 run.py:483] Algo bellman_ford step 6837 current loss 0.945306, current_train_items 218816.
I0302 19:01:30.496297 22447428132992 run.py:483] Algo bellman_ford step 6838 current loss 1.035243, current_train_items 218848.
I0302 19:01:30.527869 22447428132992 run.py:483] Algo bellman_ford step 6839 current loss 0.993082, current_train_items 218880.
I0302 19:01:30.546991 22447428132992 run.py:483] Algo bellman_ford step 6840 current loss 0.721720, current_train_items 218912.
I0302 19:01:30.563391 22447428132992 run.py:483] Algo bellman_ford step 6841 current loss 0.843936, current_train_items 218944.
I0302 19:01:30.587306 22447428132992 run.py:483] Algo bellman_ford step 6842 current loss 0.866721, current_train_items 218976.
I0302 19:01:30.616690 22447428132992 run.py:483] Algo bellman_ford step 6843 current loss 0.923874, current_train_items 219008.
I0302 19:01:30.649387 22447428132992 run.py:483] Algo bellman_ford step 6844 current loss 1.081410, current_train_items 219040.
I0302 19:01:30.668500 22447428132992 run.py:483] Algo bellman_ford step 6845 current loss 0.627482, current_train_items 219072.
I0302 19:01:30.684961 22447428132992 run.py:483] Algo bellman_ford step 6846 current loss 0.837134, current_train_items 219104.
I0302 19:01:30.708781 22447428132992 run.py:483] Algo bellman_ford step 6847 current loss 0.985033, current_train_items 219136.
I0302 19:01:30.739884 22447428132992 run.py:483] Algo bellman_ford step 6848 current loss 0.968510, current_train_items 219168.
I0302 19:01:30.773774 22447428132992 run.py:483] Algo bellman_ford step 6849 current loss 1.132498, current_train_items 219200.
I0302 19:01:30.792959 22447428132992 run.py:483] Algo bellman_ford step 6850 current loss 0.685506, current_train_items 219232.
I0302 19:01:30.801018 22447428132992 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.865234375, 'score': 0.865234375, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:01:30.801124 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.865, val scores are: bellman_ford: 0.865
I0302 19:01:30.817806 22447428132992 run.py:483] Algo bellman_ford step 6851 current loss 0.856978, current_train_items 219264.
I0302 19:01:30.841738 22447428132992 run.py:483] Algo bellman_ford step 6852 current loss 0.969454, current_train_items 219296.
I0302 19:01:30.873082 22447428132992 run.py:483] Algo bellman_ford step 6853 current loss 1.034075, current_train_items 219328.
I0302 19:01:30.907229 22447428132992 run.py:483] Algo bellman_ford step 6854 current loss 1.216696, current_train_items 219360.
I0302 19:01:30.927174 22447428132992 run.py:483] Algo bellman_ford step 6855 current loss 0.632181, current_train_items 219392.
I0302 19:01:30.942739 22447428132992 run.py:483] Algo bellman_ford step 6856 current loss 0.653056, current_train_items 219424.
I0302 19:01:30.965945 22447428132992 run.py:483] Algo bellman_ford step 6857 current loss 0.938845, current_train_items 219456.
I0302 19:01:30.995993 22447428132992 run.py:483] Algo bellman_ford step 6858 current loss 0.998035, current_train_items 219488.
I0302 19:01:31.029148 22447428132992 run.py:483] Algo bellman_ford step 6859 current loss 1.117840, current_train_items 219520.
I0302 19:01:31.048640 22447428132992 run.py:483] Algo bellman_ford step 6860 current loss 0.614561, current_train_items 219552.
I0302 19:01:31.064626 22447428132992 run.py:483] Algo bellman_ford step 6861 current loss 0.813978, current_train_items 219584.
I0302 19:01:31.086827 22447428132992 run.py:483] Algo bellman_ford step 6862 current loss 0.820279, current_train_items 219616.
I0302 19:01:31.116907 22447428132992 run.py:483] Algo bellman_ford step 6863 current loss 0.971758, current_train_items 219648.
I0302 19:01:31.148610 22447428132992 run.py:483] Algo bellman_ford step 6864 current loss 1.028498, current_train_items 219680.
I0302 19:01:31.168434 22447428132992 run.py:483] Algo bellman_ford step 6865 current loss 0.634352, current_train_items 219712.
I0302 19:01:31.184482 22447428132992 run.py:483] Algo bellman_ford step 6866 current loss 0.773228, current_train_items 219744.
I0302 19:01:31.208953 22447428132992 run.py:483] Algo bellman_ford step 6867 current loss 1.044454, current_train_items 219776.
I0302 19:01:31.239758 22447428132992 run.py:483] Algo bellman_ford step 6868 current loss 1.027779, current_train_items 219808.
I0302 19:01:31.275197 22447428132992 run.py:483] Algo bellman_ford step 6869 current loss 1.122926, current_train_items 219840.
I0302 19:01:31.295285 22447428132992 run.py:483] Algo bellman_ford step 6870 current loss 0.700287, current_train_items 219872.
I0302 19:01:31.311675 22447428132992 run.py:483] Algo bellman_ford step 6871 current loss 0.738182, current_train_items 219904.
I0302 19:01:31.334371 22447428132992 run.py:483] Algo bellman_ford step 6872 current loss 0.829843, current_train_items 219936.
I0302 19:01:31.363941 22447428132992 run.py:483] Algo bellman_ford step 6873 current loss 0.902817, current_train_items 219968.
I0302 19:01:31.396916 22447428132992 run.py:483] Algo bellman_ford step 6874 current loss 1.183867, current_train_items 220000.
I0302 19:01:31.416572 22447428132992 run.py:483] Algo bellman_ford step 6875 current loss 0.641915, current_train_items 220032.
I0302 19:01:31.432270 22447428132992 run.py:483] Algo bellman_ford step 6876 current loss 0.795853, current_train_items 220064.
I0302 19:01:31.455223 22447428132992 run.py:483] Algo bellman_ford step 6877 current loss 0.924700, current_train_items 220096.
I0302 19:01:31.486109 22447428132992 run.py:483] Algo bellman_ford step 6878 current loss 0.954068, current_train_items 220128.
I0302 19:01:31.519744 22447428132992 run.py:483] Algo bellman_ford step 6879 current loss 1.222237, current_train_items 220160.
I0302 19:01:31.539353 22447428132992 run.py:483] Algo bellman_ford step 6880 current loss 0.663936, current_train_items 220192.
I0302 19:01:31.555080 22447428132992 run.py:483] Algo bellman_ford step 6881 current loss 0.702610, current_train_items 220224.
I0302 19:01:31.578832 22447428132992 run.py:483] Algo bellman_ford step 6882 current loss 0.956090, current_train_items 220256.
I0302 19:01:31.608356 22447428132992 run.py:483] Algo bellman_ford step 6883 current loss 0.936266, current_train_items 220288.
I0302 19:01:31.641172 22447428132992 run.py:483] Algo bellman_ford step 6884 current loss 1.065413, current_train_items 220320.
I0302 19:01:31.660973 22447428132992 run.py:483] Algo bellman_ford step 6885 current loss 0.542100, current_train_items 220352.
I0302 19:01:31.677165 22447428132992 run.py:483] Algo bellman_ford step 6886 current loss 0.670255, current_train_items 220384.
I0302 19:01:31.700513 22447428132992 run.py:483] Algo bellman_ford step 6887 current loss 0.896022, current_train_items 220416.
I0302 19:01:31.732040 22447428132992 run.py:483] Algo bellman_ford step 6888 current loss 0.947982, current_train_items 220448.
I0302 19:01:31.765395 22447428132992 run.py:483] Algo bellman_ford step 6889 current loss 0.996973, current_train_items 220480.
I0302 19:01:31.785130 22447428132992 run.py:483] Algo bellman_ford step 6890 current loss 0.662043, current_train_items 220512.
I0302 19:01:31.801533 22447428132992 run.py:483] Algo bellman_ford step 6891 current loss 0.722370, current_train_items 220544.
I0302 19:01:31.824584 22447428132992 run.py:483] Algo bellman_ford step 6892 current loss 0.883114, current_train_items 220576.
I0302 19:01:31.855765 22447428132992 run.py:483] Algo bellman_ford step 6893 current loss 1.018387, current_train_items 220608.
I0302 19:01:31.887947 22447428132992 run.py:483] Algo bellman_ford step 6894 current loss 1.101640, current_train_items 220640.
I0302 19:01:31.907473 22447428132992 run.py:483] Algo bellman_ford step 6895 current loss 0.805746, current_train_items 220672.
I0302 19:01:31.923441 22447428132992 run.py:483] Algo bellman_ford step 6896 current loss 0.818866, current_train_items 220704.
I0302 19:01:31.947081 22447428132992 run.py:483] Algo bellman_ford step 6897 current loss 1.011213, current_train_items 220736.
I0302 19:01:31.976575 22447428132992 run.py:483] Algo bellman_ford step 6898 current loss 0.827205, current_train_items 220768.
I0302 19:01:32.010629 22447428132992 run.py:483] Algo bellman_ford step 6899 current loss 1.081378, current_train_items 220800.
I0302 19:01:32.030479 22447428132992 run.py:483] Algo bellman_ford step 6900 current loss 0.724669, current_train_items 220832.
I0302 19:01:32.038301 22447428132992 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:01:32.038414 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:01:32.054691 22447428132992 run.py:483] Algo bellman_ford step 6901 current loss 0.650060, current_train_items 220864.
I0302 19:01:32.078089 22447428132992 run.py:483] Algo bellman_ford step 6902 current loss 0.944605, current_train_items 220896.
I0302 19:01:32.110259 22447428132992 run.py:483] Algo bellman_ford step 6903 current loss 1.048397, current_train_items 220928.
I0302 19:01:32.145395 22447428132992 run.py:483] Algo bellman_ford step 6904 current loss 1.050917, current_train_items 220960.
I0302 19:01:32.165254 22447428132992 run.py:483] Algo bellman_ford step 6905 current loss 0.563285, current_train_items 220992.
I0302 19:01:32.180675 22447428132992 run.py:483] Algo bellman_ford step 6906 current loss 0.631595, current_train_items 221024.
I0302 19:01:32.204265 22447428132992 run.py:483] Algo bellman_ford step 6907 current loss 0.973113, current_train_items 221056.
I0302 19:01:32.235846 22447428132992 run.py:483] Algo bellman_ford step 6908 current loss 1.137179, current_train_items 221088.
I0302 19:01:32.270407 22447428132992 run.py:483] Algo bellman_ford step 6909 current loss 1.233873, current_train_items 221120.
I0302 19:01:32.289767 22447428132992 run.py:483] Algo bellman_ford step 6910 current loss 0.552589, current_train_items 221152.
I0302 19:01:32.306418 22447428132992 run.py:483] Algo bellman_ford step 6911 current loss 0.741245, current_train_items 221184.
I0302 19:01:32.328539 22447428132992 run.py:483] Algo bellman_ford step 6912 current loss 0.863156, current_train_items 221216.
I0302 19:01:32.360035 22447428132992 run.py:483] Algo bellman_ford step 6913 current loss 1.032275, current_train_items 221248.
I0302 19:01:32.394989 22447428132992 run.py:483] Algo bellman_ford step 6914 current loss 1.230465, current_train_items 221280.
I0302 19:01:32.414065 22447428132992 run.py:483] Algo bellman_ford step 6915 current loss 0.607209, current_train_items 221312.
I0302 19:01:32.429684 22447428132992 run.py:483] Algo bellman_ford step 6916 current loss 0.693475, current_train_items 221344.
I0302 19:01:32.453938 22447428132992 run.py:483] Algo bellman_ford step 6917 current loss 0.942032, current_train_items 221376.
I0302 19:01:32.484993 22447428132992 run.py:483] Algo bellman_ford step 6918 current loss 1.024341, current_train_items 221408.
I0302 19:01:32.519827 22447428132992 run.py:483] Algo bellman_ford step 6919 current loss 1.059505, current_train_items 221440.
I0302 19:01:32.539070 22447428132992 run.py:483] Algo bellman_ford step 6920 current loss 0.586257, current_train_items 221472.
I0302 19:01:32.555080 22447428132992 run.py:483] Algo bellman_ford step 6921 current loss 0.822519, current_train_items 221504.
I0302 19:01:32.577709 22447428132992 run.py:483] Algo bellman_ford step 6922 current loss 0.978412, current_train_items 221536.
I0302 19:01:32.608892 22447428132992 run.py:483] Algo bellman_ford step 6923 current loss 0.990937, current_train_items 221568.
I0302 19:01:32.641989 22447428132992 run.py:483] Algo bellman_ford step 6924 current loss 1.003618, current_train_items 221600.
I0302 19:01:32.661628 22447428132992 run.py:483] Algo bellman_ford step 6925 current loss 0.553642, current_train_items 221632.
I0302 19:01:32.677497 22447428132992 run.py:483] Algo bellman_ford step 6926 current loss 0.659288, current_train_items 221664.
I0302 19:01:32.700980 22447428132992 run.py:483] Algo bellman_ford step 6927 current loss 1.006524, current_train_items 221696.
I0302 19:01:32.731868 22447428132992 run.py:483] Algo bellman_ford step 6928 current loss 1.045490, current_train_items 221728.
I0302 19:01:32.764967 22447428132992 run.py:483] Algo bellman_ford step 6929 current loss 1.133022, current_train_items 221760.
I0302 19:01:32.784412 22447428132992 run.py:483] Algo bellman_ford step 6930 current loss 0.549012, current_train_items 221792.
I0302 19:01:32.800600 22447428132992 run.py:483] Algo bellman_ford step 6931 current loss 0.705813, current_train_items 221824.
I0302 19:01:32.823273 22447428132992 run.py:483] Algo bellman_ford step 6932 current loss 0.852042, current_train_items 221856.
I0302 19:01:32.854016 22447428132992 run.py:483] Algo bellman_ford step 6933 current loss 1.013032, current_train_items 221888.
I0302 19:01:32.888365 22447428132992 run.py:483] Algo bellman_ford step 6934 current loss 1.176368, current_train_items 221920.
I0302 19:01:32.907755 22447428132992 run.py:483] Algo bellman_ford step 6935 current loss 0.574830, current_train_items 221952.
I0302 19:01:32.923589 22447428132992 run.py:483] Algo bellman_ford step 6936 current loss 0.725820, current_train_items 221984.
I0302 19:01:32.946538 22447428132992 run.py:483] Algo bellman_ford step 6937 current loss 1.062912, current_train_items 222016.
I0302 19:01:32.977347 22447428132992 run.py:483] Algo bellman_ford step 6938 current loss 1.008701, current_train_items 222048.
I0302 19:01:33.009546 22447428132992 run.py:483] Algo bellman_ford step 6939 current loss 1.020707, current_train_items 222080.
I0302 19:01:33.029213 22447428132992 run.py:483] Algo bellman_ford step 6940 current loss 0.632830, current_train_items 222112.
I0302 19:01:33.045562 22447428132992 run.py:483] Algo bellman_ford step 6941 current loss 0.796632, current_train_items 222144.
I0302 19:01:33.069242 22447428132992 run.py:483] Algo bellman_ford step 6942 current loss 0.989882, current_train_items 222176.
I0302 19:01:33.100046 22447428132992 run.py:483] Algo bellman_ford step 6943 current loss 0.993989, current_train_items 222208.
I0302 19:01:33.134147 22447428132992 run.py:483] Algo bellman_ford step 6944 current loss 1.185721, current_train_items 222240.
I0302 19:01:33.153405 22447428132992 run.py:483] Algo bellman_ford step 6945 current loss 0.551667, current_train_items 222272.
I0302 19:01:33.169618 22447428132992 run.py:483] Algo bellman_ford step 6946 current loss 0.774336, current_train_items 222304.
I0302 19:01:33.192889 22447428132992 run.py:483] Algo bellman_ford step 6947 current loss 0.926671, current_train_items 222336.
I0302 19:01:33.221555 22447428132992 run.py:483] Algo bellman_ford step 6948 current loss 0.885557, current_train_items 222368.
I0302 19:01:33.255475 22447428132992 run.py:483] Algo bellman_ford step 6949 current loss 1.056250, current_train_items 222400.
I0302 19:01:33.274819 22447428132992 run.py:483] Algo bellman_ford step 6950 current loss 0.576833, current_train_items 222432.
I0302 19:01:33.282990 22447428132992 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:01:33.283095 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:01:33.299816 22447428132992 run.py:483] Algo bellman_ford step 6951 current loss 0.772019, current_train_items 222464.
I0302 19:01:33.323890 22447428132992 run.py:483] Algo bellman_ford step 6952 current loss 0.985261, current_train_items 222496.
I0302 19:01:33.354297 22447428132992 run.py:483] Algo bellman_ford step 6953 current loss 1.000469, current_train_items 222528.
I0302 19:01:33.389173 22447428132992 run.py:483] Algo bellman_ford step 6954 current loss 1.225468, current_train_items 222560.
I0302 19:01:33.409094 22447428132992 run.py:483] Algo bellman_ford step 6955 current loss 0.674474, current_train_items 222592.
I0302 19:01:33.424974 22447428132992 run.py:483] Algo bellman_ford step 6956 current loss 0.739892, current_train_items 222624.
I0302 19:01:33.449808 22447428132992 run.py:483] Algo bellman_ford step 6957 current loss 1.095428, current_train_items 222656.
I0302 19:01:33.480115 22447428132992 run.py:483] Algo bellman_ford step 6958 current loss 1.078212, current_train_items 222688.
I0302 19:01:33.512019 22447428132992 run.py:483] Algo bellman_ford step 6959 current loss 1.107723, current_train_items 222720.
I0302 19:01:33.532008 22447428132992 run.py:483] Algo bellman_ford step 6960 current loss 0.530649, current_train_items 222752.
I0302 19:01:33.548497 22447428132992 run.py:483] Algo bellman_ford step 6961 current loss 0.807005, current_train_items 222784.
I0302 19:01:33.571446 22447428132992 run.py:483] Algo bellman_ford step 6962 current loss 0.960670, current_train_items 222816.
I0302 19:01:33.602247 22447428132992 run.py:483] Algo bellman_ford step 6963 current loss 1.127534, current_train_items 222848.
I0302 19:01:33.637237 22447428132992 run.py:483] Algo bellman_ford step 6964 current loss 1.100571, current_train_items 222880.
I0302 19:01:33.657179 22447428132992 run.py:483] Algo bellman_ford step 6965 current loss 0.718843, current_train_items 222912.
I0302 19:01:33.673361 22447428132992 run.py:483] Algo bellman_ford step 6966 current loss 0.751985, current_train_items 222944.
I0302 19:01:33.697141 22447428132992 run.py:483] Algo bellman_ford step 6967 current loss 0.844474, current_train_items 222976.
I0302 19:01:33.726976 22447428132992 run.py:483] Algo bellman_ford step 6968 current loss 0.992915, current_train_items 223008.
I0302 19:01:33.760440 22447428132992 run.py:483] Algo bellman_ford step 6969 current loss 1.123017, current_train_items 223040.
I0302 19:01:33.779933 22447428132992 run.py:483] Algo bellman_ford step 6970 current loss 0.571070, current_train_items 223072.
I0302 19:01:33.796090 22447428132992 run.py:483] Algo bellman_ford step 6971 current loss 0.637484, current_train_items 223104.
I0302 19:01:33.819201 22447428132992 run.py:483] Algo bellman_ford step 6972 current loss 0.903862, current_train_items 223136.
I0302 19:01:33.849642 22447428132992 run.py:483] Algo bellman_ford step 6973 current loss 1.017274, current_train_items 223168.
I0302 19:01:33.883924 22447428132992 run.py:483] Algo bellman_ford step 6974 current loss 1.152691, current_train_items 223200.
I0302 19:01:33.903699 22447428132992 run.py:483] Algo bellman_ford step 6975 current loss 0.586432, current_train_items 223232.
I0302 19:01:33.919969 22447428132992 run.py:483] Algo bellman_ford step 6976 current loss 0.796663, current_train_items 223264.
I0302 19:01:33.943048 22447428132992 run.py:483] Algo bellman_ford step 6977 current loss 0.954816, current_train_items 223296.
I0302 19:01:33.973003 22447428132992 run.py:483] Algo bellman_ford step 6978 current loss 0.975771, current_train_items 223328.
I0302 19:01:34.007555 22447428132992 run.py:483] Algo bellman_ford step 6979 current loss 1.096094, current_train_items 223360.
I0302 19:01:34.027044 22447428132992 run.py:483] Algo bellman_ford step 6980 current loss 0.558419, current_train_items 223392.
I0302 19:01:34.043652 22447428132992 run.py:483] Algo bellman_ford step 6981 current loss 0.820986, current_train_items 223424.
I0302 19:01:34.065993 22447428132992 run.py:483] Algo bellman_ford step 6982 current loss 0.822526, current_train_items 223456.
I0302 19:01:34.097489 22447428132992 run.py:483] Algo bellman_ford step 6983 current loss 1.066490, current_train_items 223488.
I0302 19:01:34.131006 22447428132992 run.py:483] Algo bellman_ford step 6984 current loss 1.035852, current_train_items 223520.
I0302 19:01:34.150809 22447428132992 run.py:483] Algo bellman_ford step 6985 current loss 0.605593, current_train_items 223552.
I0302 19:01:34.166788 22447428132992 run.py:483] Algo bellman_ford step 6986 current loss 0.680282, current_train_items 223584.
I0302 19:01:34.189870 22447428132992 run.py:483] Algo bellman_ford step 6987 current loss 0.847687, current_train_items 223616.
I0302 19:01:34.220516 22447428132992 run.py:483] Algo bellman_ford step 6988 current loss 1.002056, current_train_items 223648.
I0302 19:01:34.256115 22447428132992 run.py:483] Algo bellman_ford step 6989 current loss 1.182501, current_train_items 223680.
I0302 19:01:34.275833 22447428132992 run.py:483] Algo bellman_ford step 6990 current loss 0.896759, current_train_items 223712.
I0302 19:01:34.291811 22447428132992 run.py:483] Algo bellman_ford step 6991 current loss 0.888767, current_train_items 223744.
I0302 19:01:34.314920 22447428132992 run.py:483] Algo bellman_ford step 6992 current loss 0.956538, current_train_items 223776.
I0302 19:01:34.346594 22447428132992 run.py:483] Algo bellman_ford step 6993 current loss 1.081822, current_train_items 223808.
I0302 19:01:34.382769 22447428132992 run.py:483] Algo bellman_ford step 6994 current loss 1.240704, current_train_items 223840.
I0302 19:01:34.402525 22447428132992 run.py:483] Algo bellman_ford step 6995 current loss 0.640136, current_train_items 223872.
I0302 19:01:34.418884 22447428132992 run.py:483] Algo bellman_ford step 6996 current loss 0.718639, current_train_items 223904.
I0302 19:01:34.440873 22447428132992 run.py:483] Algo bellman_ford step 6997 current loss 0.970753, current_train_items 223936.
I0302 19:01:34.470952 22447428132992 run.py:483] Algo bellman_ford step 6998 current loss 1.038351, current_train_items 223968.
I0302 19:01:34.504565 22447428132992 run.py:483] Algo bellman_ford step 6999 current loss 1.185916, current_train_items 224000.
I0302 19:01:34.524413 22447428132992 run.py:483] Algo bellman_ford step 7000 current loss 0.602826, current_train_items 224032.
I0302 19:01:34.532156 22447428132992 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:01:34.532261 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:01:34.548582 22447428132992 run.py:483] Algo bellman_ford step 7001 current loss 0.755477, current_train_items 224064.
I0302 19:01:34.571223 22447428132992 run.py:483] Algo bellman_ford step 7002 current loss 0.883673, current_train_items 224096.
I0302 19:01:34.600608 22447428132992 run.py:483] Algo bellman_ford step 7003 current loss 0.942367, current_train_items 224128.
I0302 19:01:34.633262 22447428132992 run.py:483] Algo bellman_ford step 7004 current loss 1.004182, current_train_items 224160.
I0302 19:01:34.652860 22447428132992 run.py:483] Algo bellman_ford step 7005 current loss 0.481524, current_train_items 224192.
I0302 19:01:34.668737 22447428132992 run.py:483] Algo bellman_ford step 7006 current loss 0.724379, current_train_items 224224.
I0302 19:01:34.692421 22447428132992 run.py:483] Algo bellman_ford step 7007 current loss 0.965319, current_train_items 224256.
I0302 19:01:34.721898 22447428132992 run.py:483] Algo bellman_ford step 7008 current loss 0.918696, current_train_items 224288.
I0302 19:01:34.757049 22447428132992 run.py:483] Algo bellman_ford step 7009 current loss 1.151501, current_train_items 224320.
I0302 19:01:34.776438 22447428132992 run.py:483] Algo bellman_ford step 7010 current loss 0.626814, current_train_items 224352.
I0302 19:01:34.792622 22447428132992 run.py:483] Algo bellman_ford step 7011 current loss 0.741639, current_train_items 224384.
I0302 19:01:34.815443 22447428132992 run.py:483] Algo bellman_ford step 7012 current loss 0.902184, current_train_items 224416.
I0302 19:01:34.845484 22447428132992 run.py:483] Algo bellman_ford step 7013 current loss 0.896436, current_train_items 224448.
I0302 19:01:34.879490 22447428132992 run.py:483] Algo bellman_ford step 7014 current loss 0.966711, current_train_items 224480.
I0302 19:01:34.898750 22447428132992 run.py:483] Algo bellman_ford step 7015 current loss 0.639648, current_train_items 224512.
I0302 19:01:34.914607 22447428132992 run.py:483] Algo bellman_ford step 7016 current loss 0.661169, current_train_items 224544.
I0302 19:01:34.937943 22447428132992 run.py:483] Algo bellman_ford step 7017 current loss 0.874813, current_train_items 224576.
I0302 19:01:34.967676 22447428132992 run.py:483] Algo bellman_ford step 7018 current loss 0.910577, current_train_items 224608.
I0302 19:01:35.000039 22447428132992 run.py:483] Algo bellman_ford step 7019 current loss 0.994276, current_train_items 224640.
I0302 19:01:35.019215 22447428132992 run.py:483] Algo bellman_ford step 7020 current loss 0.499718, current_train_items 224672.
I0302 19:01:35.035159 22447428132992 run.py:483] Algo bellman_ford step 7021 current loss 0.855870, current_train_items 224704.
I0302 19:01:35.057964 22447428132992 run.py:483] Algo bellman_ford step 7022 current loss 0.974254, current_train_items 224736.
I0302 19:01:35.087588 22447428132992 run.py:483] Algo bellman_ford step 7023 current loss 0.950320, current_train_items 224768.
I0302 19:01:35.121201 22447428132992 run.py:483] Algo bellman_ford step 7024 current loss 1.074845, current_train_items 224800.
I0302 19:01:35.140603 22447428132992 run.py:483] Algo bellman_ford step 7025 current loss 0.548330, current_train_items 224832.
I0302 19:01:35.156991 22447428132992 run.py:483] Algo bellman_ford step 7026 current loss 0.709018, current_train_items 224864.
I0302 19:01:35.181593 22447428132992 run.py:483] Algo bellman_ford step 7027 current loss 0.971854, current_train_items 224896.
I0302 19:01:35.214098 22447428132992 run.py:483] Algo bellman_ford step 7028 current loss 1.090974, current_train_items 224928.
I0302 19:01:35.245224 22447428132992 run.py:483] Algo bellman_ford step 7029 current loss 1.055297, current_train_items 224960.
I0302 19:01:35.265042 22447428132992 run.py:483] Algo bellman_ford step 7030 current loss 1.094543, current_train_items 224992.
I0302 19:01:35.281131 22447428132992 run.py:483] Algo bellman_ford step 7031 current loss 0.844628, current_train_items 225024.
I0302 19:01:35.303822 22447428132992 run.py:483] Algo bellman_ford step 7032 current loss 0.919634, current_train_items 225056.
I0302 19:01:35.333954 22447428132992 run.py:483] Algo bellman_ford step 7033 current loss 1.003410, current_train_items 225088.
I0302 19:01:35.368828 22447428132992 run.py:483] Algo bellman_ford step 7034 current loss 1.258538, current_train_items 225120.
I0302 19:01:35.388334 22447428132992 run.py:483] Algo bellman_ford step 7035 current loss 0.679063, current_train_items 225152.
I0302 19:01:35.404475 22447428132992 run.py:483] Algo bellman_ford step 7036 current loss 0.716126, current_train_items 225184.
I0302 19:01:35.427525 22447428132992 run.py:483] Algo bellman_ford step 7037 current loss 0.896078, current_train_items 225216.
I0302 19:01:35.458673 22447428132992 run.py:483] Algo bellman_ford step 7038 current loss 0.940600, current_train_items 225248.
I0302 19:01:35.491159 22447428132992 run.py:483] Algo bellman_ford step 7039 current loss 1.023403, current_train_items 225280.
I0302 19:01:35.510627 22447428132992 run.py:483] Algo bellman_ford step 7040 current loss 0.559864, current_train_items 225312.
I0302 19:01:35.526678 22447428132992 run.py:483] Algo bellman_ford step 7041 current loss 0.719855, current_train_items 225344.
I0302 19:01:35.549788 22447428132992 run.py:483] Algo bellman_ford step 7042 current loss 0.939521, current_train_items 225376.
I0302 19:01:35.580883 22447428132992 run.py:483] Algo bellman_ford step 7043 current loss 1.202396, current_train_items 225408.
I0302 19:01:35.614492 22447428132992 run.py:483] Algo bellman_ford step 7044 current loss 1.189624, current_train_items 225440.
I0302 19:01:35.634085 22447428132992 run.py:483] Algo bellman_ford step 7045 current loss 0.565118, current_train_items 225472.
I0302 19:01:35.650420 22447428132992 run.py:483] Algo bellman_ford step 7046 current loss 0.747398, current_train_items 225504.
I0302 19:01:35.673165 22447428132992 run.py:483] Algo bellman_ford step 7047 current loss 0.982153, current_train_items 225536.
I0302 19:01:35.704710 22447428132992 run.py:483] Algo bellman_ford step 7048 current loss 0.967684, current_train_items 225568.
I0302 19:01:35.737435 22447428132992 run.py:483] Algo bellman_ford step 7049 current loss 1.085667, current_train_items 225600.
I0302 19:01:35.756894 22447428132992 run.py:483] Algo bellman_ford step 7050 current loss 0.602980, current_train_items 225632.
I0302 19:01:35.764999 22447428132992 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:01:35.765105 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:01:35.781225 22447428132992 run.py:483] Algo bellman_ford step 7051 current loss 0.686181, current_train_items 225664.
I0302 19:01:35.805479 22447428132992 run.py:483] Algo bellman_ford step 7052 current loss 1.038257, current_train_items 225696.
I0302 19:01:35.837748 22447428132992 run.py:483] Algo bellman_ford step 7053 current loss 1.299201, current_train_items 225728.
I0302 19:01:35.872368 22447428132992 run.py:483] Algo bellman_ford step 7054 current loss 1.094689, current_train_items 225760.
I0302 19:01:35.892201 22447428132992 run.py:483] Algo bellman_ford step 7055 current loss 0.673699, current_train_items 225792.
I0302 19:01:35.907995 22447428132992 run.py:483] Algo bellman_ford step 7056 current loss 0.721909, current_train_items 225824.
I0302 19:01:35.931965 22447428132992 run.py:483] Algo bellman_ford step 7057 current loss 0.975688, current_train_items 225856.
I0302 19:01:35.962643 22447428132992 run.py:483] Algo bellman_ford step 7058 current loss 1.006970, current_train_items 225888.
I0302 19:01:35.998096 22447428132992 run.py:483] Algo bellman_ford step 7059 current loss 1.202791, current_train_items 225920.
I0302 19:01:36.017885 22447428132992 run.py:483] Algo bellman_ford step 7060 current loss 0.628745, current_train_items 225952.
I0302 19:01:36.034041 22447428132992 run.py:483] Algo bellman_ford step 7061 current loss 0.684246, current_train_items 225984.
I0302 19:01:36.058133 22447428132992 run.py:483] Algo bellman_ford step 7062 current loss 0.971473, current_train_items 226016.
I0302 19:01:36.086638 22447428132992 run.py:483] Algo bellman_ford step 7063 current loss 0.808099, current_train_items 226048.
I0302 19:01:36.119687 22447428132992 run.py:483] Algo bellman_ford step 7064 current loss 1.421443, current_train_items 226080.
I0302 19:01:36.139164 22447428132992 run.py:483] Algo bellman_ford step 7065 current loss 0.541242, current_train_items 226112.
I0302 19:01:36.155623 22447428132992 run.py:483] Algo bellman_ford step 7066 current loss 0.681452, current_train_items 226144.
I0302 19:01:36.178607 22447428132992 run.py:483] Algo bellman_ford step 7067 current loss 0.864356, current_train_items 226176.
I0302 19:01:36.211023 22447428132992 run.py:483] Algo bellman_ford step 7068 current loss 1.210619, current_train_items 226208.
I0302 19:01:36.245894 22447428132992 run.py:483] Algo bellman_ford step 7069 current loss 2.974090, current_train_items 226240.
I0302 19:01:36.265692 22447428132992 run.py:483] Algo bellman_ford step 7070 current loss 0.538104, current_train_items 226272.
I0302 19:01:36.281661 22447428132992 run.py:483] Algo bellman_ford step 7071 current loss 0.794870, current_train_items 226304.
I0302 19:01:36.303475 22447428132992 run.py:483] Algo bellman_ford step 7072 current loss 1.013461, current_train_items 226336.
I0302 19:01:36.333641 22447428132992 run.py:483] Algo bellman_ford step 7073 current loss 1.125157, current_train_items 226368.
I0302 19:01:36.366846 22447428132992 run.py:483] Algo bellman_ford step 7074 current loss 2.029895, current_train_items 226400.
I0302 19:01:36.386530 22447428132992 run.py:483] Algo bellman_ford step 7075 current loss 0.554362, current_train_items 226432.
I0302 19:01:36.402005 22447428132992 run.py:483] Algo bellman_ford step 7076 current loss 0.648629, current_train_items 226464.
I0302 19:01:36.424817 22447428132992 run.py:483] Algo bellman_ford step 7077 current loss 1.002918, current_train_items 226496.
I0302 19:01:36.456096 22447428132992 run.py:483] Algo bellman_ford step 7078 current loss 1.064848, current_train_items 226528.
I0302 19:01:36.490172 22447428132992 run.py:483] Algo bellman_ford step 7079 current loss 1.428157, current_train_items 226560.
I0302 19:01:36.509599 22447428132992 run.py:483] Algo bellman_ford step 7080 current loss 0.564153, current_train_items 226592.
I0302 19:01:36.525420 22447428132992 run.py:483] Algo bellman_ford step 7081 current loss 0.680048, current_train_items 226624.
I0302 19:01:36.548408 22447428132992 run.py:483] Algo bellman_ford step 7082 current loss 1.290811, current_train_items 226656.
I0302 19:01:36.579188 22447428132992 run.py:483] Algo bellman_ford step 7083 current loss 1.036639, current_train_items 226688.
I0302 19:01:36.612160 22447428132992 run.py:483] Algo bellman_ford step 7084 current loss 1.815937, current_train_items 226720.
I0302 19:01:36.632176 22447428132992 run.py:483] Algo bellman_ford step 7085 current loss 0.638868, current_train_items 226752.
I0302 19:01:36.648076 22447428132992 run.py:483] Algo bellman_ford step 7086 current loss 0.772632, current_train_items 226784.
I0302 19:01:36.669977 22447428132992 run.py:483] Algo bellman_ford step 7087 current loss 0.756356, current_train_items 226816.
I0302 19:01:36.701002 22447428132992 run.py:483] Algo bellman_ford step 7088 current loss 0.979484, current_train_items 226848.
I0302 19:01:36.734237 22447428132992 run.py:483] Algo bellman_ford step 7089 current loss 1.178848, current_train_items 226880.
I0302 19:01:36.754468 22447428132992 run.py:483] Algo bellman_ford step 7090 current loss 0.942351, current_train_items 226912.
I0302 19:01:36.770307 22447428132992 run.py:483] Algo bellman_ford step 7091 current loss 0.667873, current_train_items 226944.
I0302 19:01:36.793831 22447428132992 run.py:483] Algo bellman_ford step 7092 current loss 0.826082, current_train_items 226976.
I0302 19:01:36.825501 22447428132992 run.py:483] Algo bellman_ford step 7093 current loss 1.098380, current_train_items 227008.
I0302 19:01:36.857216 22447428132992 run.py:483] Algo bellman_ford step 7094 current loss 1.011540, current_train_items 227040.
I0302 19:01:36.876724 22447428132992 run.py:483] Algo bellman_ford step 7095 current loss 0.705685, current_train_items 227072.
I0302 19:01:36.892780 22447428132992 run.py:483] Algo bellman_ford step 7096 current loss 0.697555, current_train_items 227104.
I0302 19:01:36.914933 22447428132992 run.py:483] Algo bellman_ford step 7097 current loss 0.916176, current_train_items 227136.
I0302 19:01:36.946227 22447428132992 run.py:483] Algo bellman_ford step 7098 current loss 1.090075, current_train_items 227168.
I0302 19:01:36.978632 22447428132992 run.py:483] Algo bellman_ford step 7099 current loss 1.173361, current_train_items 227200.
I0302 19:01:36.998924 22447428132992 run.py:483] Algo bellman_ford step 7100 current loss 0.599684, current_train_items 227232.
I0302 19:01:37.006563 22447428132992 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.861328125, 'score': 0.861328125, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:01:37.006671 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.861, val scores are: bellman_ford: 0.861
I0302 19:01:37.023212 22447428132992 run.py:483] Algo bellman_ford step 7101 current loss 0.725692, current_train_items 227264.
I0302 19:01:37.047001 22447428132992 run.py:483] Algo bellman_ford step 7102 current loss 0.851168, current_train_items 227296.
I0302 19:01:37.078200 22447428132992 run.py:483] Algo bellman_ford step 7103 current loss 0.949667, current_train_items 227328.
I0302 19:01:37.113524 22447428132992 run.py:483] Algo bellman_ford step 7104 current loss 1.177277, current_train_items 227360.
I0302 19:01:37.133820 22447428132992 run.py:483] Algo bellman_ford step 7105 current loss 0.752512, current_train_items 227392.
I0302 19:01:37.149786 22447428132992 run.py:483] Algo bellman_ford step 7106 current loss 0.723612, current_train_items 227424.
I0302 19:01:37.173675 22447428132992 run.py:483] Algo bellman_ford step 7107 current loss 0.889881, current_train_items 227456.
I0302 19:01:37.204020 22447428132992 run.py:483] Algo bellman_ford step 7108 current loss 1.036174, current_train_items 227488.
I0302 19:01:37.239707 22447428132992 run.py:483] Algo bellman_ford step 7109 current loss 1.190722, current_train_items 227520.
I0302 19:01:37.259407 22447428132992 run.py:483] Algo bellman_ford step 7110 current loss 0.640797, current_train_items 227552.
I0302 19:01:37.275618 22447428132992 run.py:483] Algo bellman_ford step 7111 current loss 0.757406, current_train_items 227584.
I0302 19:01:37.298937 22447428132992 run.py:483] Algo bellman_ford step 7112 current loss 0.918611, current_train_items 227616.
I0302 19:01:37.330235 22447428132992 run.py:483] Algo bellman_ford step 7113 current loss 1.094161, current_train_items 227648.
I0302 19:01:37.364192 22447428132992 run.py:483] Algo bellman_ford step 7114 current loss 1.358411, current_train_items 227680.
I0302 19:01:37.383669 22447428132992 run.py:483] Algo bellman_ford step 7115 current loss 0.680384, current_train_items 227712.
I0302 19:01:37.399810 22447428132992 run.py:483] Algo bellman_ford step 7116 current loss 0.930686, current_train_items 227744.
I0302 19:01:37.423444 22447428132992 run.py:483] Algo bellman_ford step 7117 current loss 0.836819, current_train_items 227776.
I0302 19:01:37.454840 22447428132992 run.py:483] Algo bellman_ford step 7118 current loss 1.135287, current_train_items 227808.
I0302 19:01:37.488631 22447428132992 run.py:483] Algo bellman_ford step 7119 current loss 1.124398, current_train_items 227840.
I0302 19:01:37.508057 22447428132992 run.py:483] Algo bellman_ford step 7120 current loss 0.519825, current_train_items 227872.
I0302 19:01:37.524212 22447428132992 run.py:483] Algo bellman_ford step 7121 current loss 0.811962, current_train_items 227904.
I0302 19:01:37.547361 22447428132992 run.py:483] Algo bellman_ford step 7122 current loss 0.936146, current_train_items 227936.
I0302 19:01:37.577599 22447428132992 run.py:483] Algo bellman_ford step 7123 current loss 0.983472, current_train_items 227968.
I0302 19:01:37.611240 22447428132992 run.py:483] Algo bellman_ford step 7124 current loss 1.163715, current_train_items 228000.
I0302 19:01:37.630753 22447428132992 run.py:483] Algo bellman_ford step 7125 current loss 0.686489, current_train_items 228032.
I0302 19:01:37.646219 22447428132992 run.py:483] Algo bellman_ford step 7126 current loss 0.661867, current_train_items 228064.
I0302 19:01:37.668728 22447428132992 run.py:483] Algo bellman_ford step 7127 current loss 0.806381, current_train_items 228096.
I0302 19:01:37.698546 22447428132992 run.py:483] Algo bellman_ford step 7128 current loss 0.965585, current_train_items 228128.
I0302 19:01:37.733115 22447428132992 run.py:483] Algo bellman_ford step 7129 current loss 1.154655, current_train_items 228160.
I0302 19:01:37.752871 22447428132992 run.py:483] Algo bellman_ford step 7130 current loss 0.676618, current_train_items 228192.
I0302 19:01:37.769224 22447428132992 run.py:483] Algo bellman_ford step 7131 current loss 0.740410, current_train_items 228224.
I0302 19:01:37.793091 22447428132992 run.py:483] Algo bellman_ford step 7132 current loss 0.867967, current_train_items 228256.
I0302 19:01:37.823957 22447428132992 run.py:483] Algo bellman_ford step 7133 current loss 1.043836, current_train_items 228288.
I0302 19:01:37.856796 22447428132992 run.py:483] Algo bellman_ford step 7134 current loss 1.122806, current_train_items 228320.
I0302 19:01:37.876192 22447428132992 run.py:483] Algo bellman_ford step 7135 current loss 0.767727, current_train_items 228352.
I0302 19:01:37.891929 22447428132992 run.py:483] Algo bellman_ford step 7136 current loss 0.735625, current_train_items 228384.
I0302 19:01:37.914323 22447428132992 run.py:483] Algo bellman_ford step 7137 current loss 0.846525, current_train_items 228416.
I0302 19:01:37.943821 22447428132992 run.py:483] Algo bellman_ford step 7138 current loss 0.940804, current_train_items 228448.
I0302 19:01:37.979642 22447428132992 run.py:483] Algo bellman_ford step 7139 current loss 1.139060, current_train_items 228480.
I0302 19:01:37.999295 22447428132992 run.py:483] Algo bellman_ford step 7140 current loss 0.741594, current_train_items 228512.
I0302 19:01:38.015146 22447428132992 run.py:483] Algo bellman_ford step 7141 current loss 0.749783, current_train_items 228544.
I0302 19:01:38.038757 22447428132992 run.py:483] Algo bellman_ford step 7142 current loss 0.987749, current_train_items 228576.
I0302 19:01:38.067878 22447428132992 run.py:483] Algo bellman_ford step 7143 current loss 0.843951, current_train_items 228608.
I0302 19:01:38.100593 22447428132992 run.py:483] Algo bellman_ford step 7144 current loss 1.039050, current_train_items 228640.
I0302 19:01:38.120004 22447428132992 run.py:483] Algo bellman_ford step 7145 current loss 0.595767, current_train_items 228672.
I0302 19:01:38.136439 22447428132992 run.py:483] Algo bellman_ford step 7146 current loss 0.711541, current_train_items 228704.
I0302 19:01:38.160137 22447428132992 run.py:483] Algo bellman_ford step 7147 current loss 0.979425, current_train_items 228736.
I0302 19:01:38.189702 22447428132992 run.py:483] Algo bellman_ford step 7148 current loss 0.994865, current_train_items 228768.
I0302 19:01:38.220579 22447428132992 run.py:483] Algo bellman_ford step 7149 current loss 1.074369, current_train_items 228800.
I0302 19:01:38.239859 22447428132992 run.py:483] Algo bellman_ford step 7150 current loss 0.539559, current_train_items 228832.
I0302 19:01:38.247973 22447428132992 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:38.248078 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:01:38.264988 22447428132992 run.py:483] Algo bellman_ford step 7151 current loss 0.659644, current_train_items 228864.
I0302 19:01:38.289171 22447428132992 run.py:483] Algo bellman_ford step 7152 current loss 0.903775, current_train_items 228896.
I0302 19:01:38.320228 22447428132992 run.py:483] Algo bellman_ford step 7153 current loss 1.072617, current_train_items 228928.
I0302 19:01:38.352880 22447428132992 run.py:483] Algo bellman_ford step 7154 current loss 0.987186, current_train_items 228960.
I0302 19:01:38.372761 22447428132992 run.py:483] Algo bellman_ford step 7155 current loss 0.868859, current_train_items 228992.
I0302 19:01:38.388790 22447428132992 run.py:483] Algo bellman_ford step 7156 current loss 0.755731, current_train_items 229024.
I0302 19:01:38.411728 22447428132992 run.py:483] Algo bellman_ford step 7157 current loss 0.940617, current_train_items 229056.
I0302 19:01:38.441694 22447428132992 run.py:483] Algo bellman_ford step 7158 current loss 0.928647, current_train_items 229088.
I0302 19:01:38.475165 22447428132992 run.py:483] Algo bellman_ford step 7159 current loss 1.098347, current_train_items 229120.
I0302 19:01:38.494684 22447428132992 run.py:483] Algo bellman_ford step 7160 current loss 0.592903, current_train_items 229152.
I0302 19:01:38.511142 22447428132992 run.py:483] Algo bellman_ford step 7161 current loss 0.880102, current_train_items 229184.
W0302 19:01:38.525280 22447428132992 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:45.363895 22447428132992 run.py:483] Algo bellman_ford step 7162 current loss 1.036044, current_train_items 229216.
I0302 19:01:45.395567 22447428132992 run.py:483] Algo bellman_ford step 7163 current loss 0.963078, current_train_items 229248.
I0302 19:01:45.430835 22447428132992 run.py:483] Algo bellman_ford step 7164 current loss 1.133476, current_train_items 229280.
I0302 19:01:45.450884 22447428132992 run.py:483] Algo bellman_ford step 7165 current loss 0.581657, current_train_items 229312.
I0302 19:01:45.467200 22447428132992 run.py:483] Algo bellman_ford step 7166 current loss 0.756034, current_train_items 229344.
I0302 19:01:45.490531 22447428132992 run.py:483] Algo bellman_ford step 7167 current loss 0.987258, current_train_items 229376.
I0302 19:01:45.521826 22447428132992 run.py:483] Algo bellman_ford step 7168 current loss 0.958008, current_train_items 229408.
I0302 19:01:45.553169 22447428132992 run.py:483] Algo bellman_ford step 7169 current loss 1.112551, current_train_items 229440.
I0302 19:01:45.573200 22447428132992 run.py:483] Algo bellman_ford step 7170 current loss 0.651727, current_train_items 229472.
I0302 19:01:45.589704 22447428132992 run.py:483] Algo bellman_ford step 7171 current loss 0.730868, current_train_items 229504.
I0302 19:01:45.612766 22447428132992 run.py:483] Algo bellman_ford step 7172 current loss 0.937562, current_train_items 229536.
I0302 19:01:45.643809 22447428132992 run.py:483] Algo bellman_ford step 7173 current loss 0.951040, current_train_items 229568.
I0302 19:01:45.677840 22447428132992 run.py:483] Algo bellman_ford step 7174 current loss 1.192916, current_train_items 229600.
I0302 19:01:45.697701 22447428132992 run.py:483] Algo bellman_ford step 7175 current loss 0.642332, current_train_items 229632.
I0302 19:01:45.713451 22447428132992 run.py:483] Algo bellman_ford step 7176 current loss 0.816225, current_train_items 229664.
I0302 19:01:45.736960 22447428132992 run.py:483] Algo bellman_ford step 7177 current loss 0.958918, current_train_items 229696.
I0302 19:01:45.769786 22447428132992 run.py:483] Algo bellman_ford step 7178 current loss 1.117543, current_train_items 229728.
I0302 19:01:45.804246 22447428132992 run.py:483] Algo bellman_ford step 7179 current loss 1.361251, current_train_items 229760.
I0302 19:01:45.824080 22447428132992 run.py:483] Algo bellman_ford step 7180 current loss 0.534585, current_train_items 229792.
I0302 19:01:45.840057 22447428132992 run.py:483] Algo bellman_ford step 7181 current loss 0.804251, current_train_items 229824.
I0302 19:01:45.863970 22447428132992 run.py:483] Algo bellman_ford step 7182 current loss 1.034495, current_train_items 229856.
I0302 19:01:45.894501 22447428132992 run.py:483] Algo bellman_ford step 7183 current loss 1.033671, current_train_items 229888.
I0302 19:01:45.931068 22447428132992 run.py:483] Algo bellman_ford step 7184 current loss 1.152361, current_train_items 229920.
I0302 19:01:45.950669 22447428132992 run.py:483] Algo bellman_ford step 7185 current loss 0.780006, current_train_items 229952.
I0302 19:01:45.966609 22447428132992 run.py:483] Algo bellman_ford step 7186 current loss 0.865331, current_train_items 229984.
I0302 19:01:45.990473 22447428132992 run.py:483] Algo bellman_ford step 7187 current loss 1.037987, current_train_items 230016.
I0302 19:01:46.021608 22447428132992 run.py:483] Algo bellman_ford step 7188 current loss 1.074462, current_train_items 230048.
I0302 19:01:46.056767 22447428132992 run.py:483] Algo bellman_ford step 7189 current loss 1.184209, current_train_items 230080.
I0302 19:01:46.076710 22447428132992 run.py:483] Algo bellman_ford step 7190 current loss 0.539574, current_train_items 230112.
I0302 19:01:46.093124 22447428132992 run.py:483] Algo bellman_ford step 7191 current loss 0.716733, current_train_items 230144.
I0302 19:01:46.116622 22447428132992 run.py:483] Algo bellman_ford step 7192 current loss 0.967234, current_train_items 230176.
I0302 19:01:46.148051 22447428132992 run.py:483] Algo bellman_ford step 7193 current loss 0.959354, current_train_items 230208.
I0302 19:01:46.181260 22447428132992 run.py:483] Algo bellman_ford step 7194 current loss 1.131951, current_train_items 230240.
I0302 19:01:46.200851 22447428132992 run.py:483] Algo bellman_ford step 7195 current loss 0.498575, current_train_items 230272.
I0302 19:01:46.216614 22447428132992 run.py:483] Algo bellman_ford step 7196 current loss 0.789185, current_train_items 230304.
I0302 19:01:46.239804 22447428132992 run.py:483] Algo bellman_ford step 7197 current loss 0.867158, current_train_items 230336.
I0302 19:01:46.271276 22447428132992 run.py:483] Algo bellman_ford step 7198 current loss 0.930949, current_train_items 230368.
I0302 19:01:46.305011 22447428132992 run.py:483] Algo bellman_ford step 7199 current loss 1.037617, current_train_items 230400.
I0302 19:01:46.324764 22447428132992 run.py:483] Algo bellman_ford step 7200 current loss 0.998943, current_train_items 230432.
I0302 19:01:46.334596 22447428132992 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:46.334704 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:01:46.351258 22447428132992 run.py:483] Algo bellman_ford step 7201 current loss 0.843702, current_train_items 230464.
I0302 19:01:46.374770 22447428132992 run.py:483] Algo bellman_ford step 7202 current loss 0.901602, current_train_items 230496.
I0302 19:01:46.406398 22447428132992 run.py:483] Algo bellman_ford step 7203 current loss 1.077258, current_train_items 230528.
I0302 19:01:46.439744 22447428132992 run.py:483] Algo bellman_ford step 7204 current loss 1.044356, current_train_items 230560.
I0302 19:01:46.459608 22447428132992 run.py:483] Algo bellman_ford step 7205 current loss 0.650365, current_train_items 230592.
I0302 19:01:46.475026 22447428132992 run.py:483] Algo bellman_ford step 7206 current loss 0.732506, current_train_items 230624.
I0302 19:01:46.498629 22447428132992 run.py:483] Algo bellman_ford step 7207 current loss 0.889715, current_train_items 230656.
I0302 19:01:46.529069 22447428132992 run.py:483] Algo bellman_ford step 7208 current loss 0.932538, current_train_items 230688.
I0302 19:01:46.563601 22447428132992 run.py:483] Algo bellman_ford step 7209 current loss 1.133285, current_train_items 230720.
I0302 19:01:46.582914 22447428132992 run.py:483] Algo bellman_ford step 7210 current loss 0.593447, current_train_items 230752.
I0302 19:01:46.598639 22447428132992 run.py:483] Algo bellman_ford step 7211 current loss 0.741832, current_train_items 230784.
I0302 19:01:46.621834 22447428132992 run.py:483] Algo bellman_ford step 7212 current loss 0.840564, current_train_items 230816.
I0302 19:01:46.652362 22447428132992 run.py:483] Algo bellman_ford step 7213 current loss 0.881108, current_train_items 230848.
I0302 19:01:46.684055 22447428132992 run.py:483] Algo bellman_ford step 7214 current loss 1.065223, current_train_items 230880.
I0302 19:01:46.703692 22447428132992 run.py:483] Algo bellman_ford step 7215 current loss 0.594436, current_train_items 230912.
I0302 19:01:46.719841 22447428132992 run.py:483] Algo bellman_ford step 7216 current loss 0.758755, current_train_items 230944.
I0302 19:01:46.743901 22447428132992 run.py:483] Algo bellman_ford step 7217 current loss 0.933361, current_train_items 230976.
I0302 19:01:46.775083 22447428132992 run.py:483] Algo bellman_ford step 7218 current loss 0.979267, current_train_items 231008.
I0302 19:01:46.808171 22447428132992 run.py:483] Algo bellman_ford step 7219 current loss 1.099125, current_train_items 231040.
I0302 19:01:46.828043 22447428132992 run.py:483] Algo bellman_ford step 7220 current loss 0.605323, current_train_items 231072.
I0302 19:01:46.844234 22447428132992 run.py:483] Algo bellman_ford step 7221 current loss 0.846259, current_train_items 231104.
I0302 19:01:46.868914 22447428132992 run.py:483] Algo bellman_ford step 7222 current loss 0.934460, current_train_items 231136.
I0302 19:01:46.899918 22447428132992 run.py:483] Algo bellman_ford step 7223 current loss 0.929382, current_train_items 231168.
I0302 19:01:46.934077 22447428132992 run.py:483] Algo bellman_ford step 7224 current loss 1.279045, current_train_items 231200.
I0302 19:01:46.953834 22447428132992 run.py:483] Algo bellman_ford step 7225 current loss 0.587762, current_train_items 231232.
I0302 19:01:46.970095 22447428132992 run.py:483] Algo bellman_ford step 7226 current loss 0.787943, current_train_items 231264.
I0302 19:01:46.994901 22447428132992 run.py:483] Algo bellman_ford step 7227 current loss 1.102311, current_train_items 231296.
I0302 19:01:47.026144 22447428132992 run.py:483] Algo bellman_ford step 7228 current loss 0.872881, current_train_items 231328.
I0302 19:01:47.057848 22447428132992 run.py:483] Algo bellman_ford step 7229 current loss 1.135085, current_train_items 231360.
I0302 19:01:47.077481 22447428132992 run.py:483] Algo bellman_ford step 7230 current loss 0.560068, current_train_items 231392.
I0302 19:01:47.093715 22447428132992 run.py:483] Algo bellman_ford step 7231 current loss 0.702574, current_train_items 231424.
I0302 19:01:47.115410 22447428132992 run.py:483] Algo bellman_ford step 7232 current loss 0.809027, current_train_items 231456.
I0302 19:01:47.147187 22447428132992 run.py:483] Algo bellman_ford step 7233 current loss 1.009769, current_train_items 231488.
I0302 19:01:47.179811 22447428132992 run.py:483] Algo bellman_ford step 7234 current loss 1.220806, current_train_items 231520.
I0302 19:01:47.199793 22447428132992 run.py:483] Algo bellman_ford step 7235 current loss 0.532643, current_train_items 231552.
I0302 19:01:47.216287 22447428132992 run.py:483] Algo bellman_ford step 7236 current loss 0.838653, current_train_items 231584.
I0302 19:01:47.240768 22447428132992 run.py:483] Algo bellman_ford step 7237 current loss 0.876003, current_train_items 231616.
I0302 19:01:47.273320 22447428132992 run.py:483] Algo bellman_ford step 7238 current loss 1.079059, current_train_items 231648.
I0302 19:01:47.306900 22447428132992 run.py:483] Algo bellman_ford step 7239 current loss 1.079914, current_train_items 231680.
I0302 19:01:47.326457 22447428132992 run.py:483] Algo bellman_ford step 7240 current loss 0.695121, current_train_items 231712.
I0302 19:01:47.342537 22447428132992 run.py:483] Algo bellman_ford step 7241 current loss 0.843620, current_train_items 231744.
I0302 19:01:47.367174 22447428132992 run.py:483] Algo bellman_ford step 7242 current loss 1.079264, current_train_items 231776.
I0302 19:01:47.398564 22447428132992 run.py:483] Algo bellman_ford step 7243 current loss 1.013632, current_train_items 231808.
I0302 19:01:47.433783 22447428132992 run.py:483] Algo bellman_ford step 7244 current loss 1.056281, current_train_items 231840.
I0302 19:01:47.453300 22447428132992 run.py:483] Algo bellman_ford step 7245 current loss 0.530092, current_train_items 231872.
I0302 19:01:47.469366 22447428132992 run.py:483] Algo bellman_ford step 7246 current loss 0.730716, current_train_items 231904.
I0302 19:01:47.493627 22447428132992 run.py:483] Algo bellman_ford step 7247 current loss 0.827354, current_train_items 231936.
I0302 19:01:47.525991 22447428132992 run.py:483] Algo bellman_ford step 7248 current loss 1.043210, current_train_items 231968.
I0302 19:01:47.560023 22447428132992 run.py:483] Algo bellman_ford step 7249 current loss 1.121980, current_train_items 232000.
I0302 19:01:47.579897 22447428132992 run.py:483] Algo bellman_ford step 7250 current loss 0.590611, current_train_items 232032.
I0302 19:01:47.588230 22447428132992 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:47.588337 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:01:47.605029 22447428132992 run.py:483] Algo bellman_ford step 7251 current loss 0.760610, current_train_items 232064.
I0302 19:01:47.628517 22447428132992 run.py:483] Algo bellman_ford step 7252 current loss 0.942479, current_train_items 232096.
I0302 19:01:47.659220 22447428132992 run.py:483] Algo bellman_ford step 7253 current loss 0.912595, current_train_items 232128.
I0302 19:01:47.693162 22447428132992 run.py:483] Algo bellman_ford step 7254 current loss 1.074423, current_train_items 232160.
I0302 19:01:47.712917 22447428132992 run.py:483] Algo bellman_ford step 7255 current loss 0.567193, current_train_items 232192.
I0302 19:01:47.728424 22447428132992 run.py:483] Algo bellman_ford step 7256 current loss 0.714939, current_train_items 232224.
I0302 19:01:47.751597 22447428132992 run.py:483] Algo bellman_ford step 7257 current loss 0.879333, current_train_items 232256.
I0302 19:01:47.784996 22447428132992 run.py:483] Algo bellman_ford step 7258 current loss 1.056894, current_train_items 232288.
I0302 19:01:47.818713 22447428132992 run.py:483] Algo bellman_ford step 7259 current loss 1.090183, current_train_items 232320.
I0302 19:01:47.838462 22447428132992 run.py:483] Algo bellman_ford step 7260 current loss 0.568420, current_train_items 232352.
I0302 19:01:47.854685 22447428132992 run.py:483] Algo bellman_ford step 7261 current loss 0.802508, current_train_items 232384.
I0302 19:01:47.877992 22447428132992 run.py:483] Algo bellman_ford step 7262 current loss 0.892295, current_train_items 232416.
I0302 19:01:47.909177 22447428132992 run.py:483] Algo bellman_ford step 7263 current loss 1.040485, current_train_items 232448.
I0302 19:01:47.942440 22447428132992 run.py:483] Algo bellman_ford step 7264 current loss 1.070463, current_train_items 232480.
I0302 19:01:47.961777 22447428132992 run.py:483] Algo bellman_ford step 7265 current loss 0.789780, current_train_items 232512.
I0302 19:01:47.977906 22447428132992 run.py:483] Algo bellman_ford step 7266 current loss 0.850103, current_train_items 232544.
I0302 19:01:48.001007 22447428132992 run.py:483] Algo bellman_ford step 7267 current loss 1.019476, current_train_items 232576.
I0302 19:01:48.032042 22447428132992 run.py:483] Algo bellman_ford step 7268 current loss 0.949932, current_train_items 232608.
I0302 19:01:48.064369 22447428132992 run.py:483] Algo bellman_ford step 7269 current loss 1.134743, current_train_items 232640.
I0302 19:01:48.083907 22447428132992 run.py:483] Algo bellman_ford step 7270 current loss 0.629104, current_train_items 232672.
I0302 19:01:48.100387 22447428132992 run.py:483] Algo bellman_ford step 7271 current loss 0.716627, current_train_items 232704.
I0302 19:01:48.123554 22447428132992 run.py:483] Algo bellman_ford step 7272 current loss 0.828841, current_train_items 232736.
I0302 19:01:48.154324 22447428132992 run.py:483] Algo bellman_ford step 7273 current loss 0.940649, current_train_items 232768.
I0302 19:01:48.187239 22447428132992 run.py:483] Algo bellman_ford step 7274 current loss 0.968628, current_train_items 232800.
I0302 19:01:48.206890 22447428132992 run.py:483] Algo bellman_ford step 7275 current loss 0.676329, current_train_items 232832.
I0302 19:01:48.222868 22447428132992 run.py:483] Algo bellman_ford step 7276 current loss 1.006070, current_train_items 232864.
I0302 19:01:48.246104 22447428132992 run.py:483] Algo bellman_ford step 7277 current loss 0.903357, current_train_items 232896.
I0302 19:01:48.277565 22447428132992 run.py:483] Algo bellman_ford step 7278 current loss 0.979399, current_train_items 232928.
I0302 19:01:48.310926 22447428132992 run.py:483] Algo bellman_ford step 7279 current loss 1.038585, current_train_items 232960.
I0302 19:01:48.330235 22447428132992 run.py:483] Algo bellman_ford step 7280 current loss 0.771330, current_train_items 232992.
I0302 19:01:48.346611 22447428132992 run.py:483] Algo bellman_ford step 7281 current loss 0.787438, current_train_items 233024.
I0302 19:01:48.369417 22447428132992 run.py:483] Algo bellman_ford step 7282 current loss 0.894708, current_train_items 233056.
I0302 19:01:48.400761 22447428132992 run.py:483] Algo bellman_ford step 7283 current loss 1.050061, current_train_items 233088.
I0302 19:01:48.433182 22447428132992 run.py:483] Algo bellman_ford step 7284 current loss 1.038346, current_train_items 233120.
I0302 19:01:48.452951 22447428132992 run.py:483] Algo bellman_ford step 7285 current loss 0.589358, current_train_items 233152.
I0302 19:01:48.470001 22447428132992 run.py:483] Algo bellman_ford step 7286 current loss 0.985117, current_train_items 233184.
I0302 19:01:48.492768 22447428132992 run.py:483] Algo bellman_ford step 7287 current loss 0.855583, current_train_items 233216.
I0302 19:01:48.523632 22447428132992 run.py:483] Algo bellman_ford step 7288 current loss 1.043117, current_train_items 233248.
I0302 19:01:48.555941 22447428132992 run.py:483] Algo bellman_ford step 7289 current loss 1.029654, current_train_items 233280.
I0302 19:01:48.575793 22447428132992 run.py:483] Algo bellman_ford step 7290 current loss 0.603628, current_train_items 233312.
I0302 19:01:48.592023 22447428132992 run.py:483] Algo bellman_ford step 7291 current loss 0.701330, current_train_items 233344.
I0302 19:01:48.614286 22447428132992 run.py:483] Algo bellman_ford step 7292 current loss 0.830757, current_train_items 233376.
I0302 19:01:48.645891 22447428132992 run.py:483] Algo bellman_ford step 7293 current loss 0.966562, current_train_items 233408.
I0302 19:01:48.677512 22447428132992 run.py:483] Algo bellman_ford step 7294 current loss 0.978968, current_train_items 233440.
I0302 19:01:48.696659 22447428132992 run.py:483] Algo bellman_ford step 7295 current loss 0.635743, current_train_items 233472.
I0302 19:01:48.712822 22447428132992 run.py:483] Algo bellman_ford step 7296 current loss 0.670759, current_train_items 233504.
I0302 19:01:48.736254 22447428132992 run.py:483] Algo bellman_ford step 7297 current loss 1.063102, current_train_items 233536.
I0302 19:01:48.767471 22447428132992 run.py:483] Algo bellman_ford step 7298 current loss 1.063274, current_train_items 233568.
I0302 19:01:48.800489 22447428132992 run.py:483] Algo bellman_ford step 7299 current loss 1.235383, current_train_items 233600.
I0302 19:01:48.820140 22447428132992 run.py:483] Algo bellman_ford step 7300 current loss 0.629106, current_train_items 233632.
I0302 19:01:48.828049 22447428132992 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.8681640625, 'score': 0.8681640625, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:48.828157 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.868, val scores are: bellman_ford: 0.868
I0302 19:01:48.845175 22447428132992 run.py:483] Algo bellman_ford step 7301 current loss 0.694814, current_train_items 233664.
I0302 19:01:48.868873 22447428132992 run.py:483] Algo bellman_ford step 7302 current loss 0.929004, current_train_items 233696.
I0302 19:01:48.900203 22447428132992 run.py:483] Algo bellman_ford step 7303 current loss 0.927891, current_train_items 233728.
I0302 19:01:48.934706 22447428132992 run.py:483] Algo bellman_ford step 7304 current loss 1.088537, current_train_items 233760.
I0302 19:01:48.954550 22447428132992 run.py:483] Algo bellman_ford step 7305 current loss 0.702531, current_train_items 233792.
I0302 19:01:48.970157 22447428132992 run.py:483] Algo bellman_ford step 7306 current loss 0.721329, current_train_items 233824.
I0302 19:01:48.993473 22447428132992 run.py:483] Algo bellman_ford step 7307 current loss 0.932205, current_train_items 233856.
I0302 19:01:49.023640 22447428132992 run.py:483] Algo bellman_ford step 7308 current loss 0.856524, current_train_items 233888.
I0302 19:01:49.057995 22447428132992 run.py:483] Algo bellman_ford step 7309 current loss 1.146002, current_train_items 233920.
I0302 19:01:49.077504 22447428132992 run.py:483] Algo bellman_ford step 7310 current loss 0.589478, current_train_items 233952.
I0302 19:01:49.093392 22447428132992 run.py:483] Algo bellman_ford step 7311 current loss 0.696638, current_train_items 233984.
I0302 19:01:49.118602 22447428132992 run.py:483] Algo bellman_ford step 7312 current loss 1.026741, current_train_items 234016.
I0302 19:01:49.149819 22447428132992 run.py:483] Algo bellman_ford step 7313 current loss 0.910400, current_train_items 234048.
I0302 19:01:49.184134 22447428132992 run.py:483] Algo bellman_ford step 7314 current loss 1.076357, current_train_items 234080.
I0302 19:01:49.203745 22447428132992 run.py:483] Algo bellman_ford step 7315 current loss 0.573689, current_train_items 234112.
I0302 19:01:49.219677 22447428132992 run.py:483] Algo bellman_ford step 7316 current loss 0.799983, current_train_items 234144.
I0302 19:01:49.244335 22447428132992 run.py:483] Algo bellman_ford step 7317 current loss 1.003311, current_train_items 234176.
I0302 19:01:49.275425 22447428132992 run.py:483] Algo bellman_ford step 7318 current loss 0.986666, current_train_items 234208.
I0302 19:01:49.310235 22447428132992 run.py:483] Algo bellman_ford step 7319 current loss 1.190391, current_train_items 234240.
I0302 19:01:49.330007 22447428132992 run.py:483] Algo bellman_ford step 7320 current loss 0.527170, current_train_items 234272.
I0302 19:01:49.346485 22447428132992 run.py:483] Algo bellman_ford step 7321 current loss 0.795048, current_train_items 234304.
I0302 19:01:49.371128 22447428132992 run.py:483] Algo bellman_ford step 7322 current loss 0.999999, current_train_items 234336.
I0302 19:01:49.401518 22447428132992 run.py:483] Algo bellman_ford step 7323 current loss 0.927220, current_train_items 234368.
I0302 19:01:49.436231 22447428132992 run.py:483] Algo bellman_ford step 7324 current loss 1.078410, current_train_items 234400.
I0302 19:01:49.455774 22447428132992 run.py:483] Algo bellman_ford step 7325 current loss 0.717326, current_train_items 234432.
I0302 19:01:49.472155 22447428132992 run.py:483] Algo bellman_ford step 7326 current loss 0.778401, current_train_items 234464.
I0302 19:01:49.496050 22447428132992 run.py:483] Algo bellman_ford step 7327 current loss 0.958072, current_train_items 234496.
I0302 19:01:49.526592 22447428132992 run.py:483] Algo bellman_ford step 7328 current loss 0.960277, current_train_items 234528.
I0302 19:01:49.560714 22447428132992 run.py:483] Algo bellman_ford step 7329 current loss 1.004441, current_train_items 234560.
I0302 19:01:49.580003 22447428132992 run.py:483] Algo bellman_ford step 7330 current loss 0.813111, current_train_items 234592.
I0302 19:01:49.596280 22447428132992 run.py:483] Algo bellman_ford step 7331 current loss 0.846222, current_train_items 234624.
I0302 19:01:49.621108 22447428132992 run.py:483] Algo bellman_ford step 7332 current loss 0.864891, current_train_items 234656.
I0302 19:01:49.652827 22447428132992 run.py:483] Algo bellman_ford step 7333 current loss 0.980150, current_train_items 234688.
I0302 19:01:49.684827 22447428132992 run.py:483] Algo bellman_ford step 7334 current loss 0.964034, current_train_items 234720.
I0302 19:01:49.704350 22447428132992 run.py:483] Algo bellman_ford step 7335 current loss 0.585235, current_train_items 234752.
I0302 19:01:49.720070 22447428132992 run.py:483] Algo bellman_ford step 7336 current loss 0.711924, current_train_items 234784.
I0302 19:01:49.743520 22447428132992 run.py:483] Algo bellman_ford step 7337 current loss 0.854342, current_train_items 234816.
I0302 19:01:49.774734 22447428132992 run.py:483] Algo bellman_ford step 7338 current loss 0.929723, current_train_items 234848.
I0302 19:01:49.810160 22447428132992 run.py:483] Algo bellman_ford step 7339 current loss 1.121730, current_train_items 234880.
I0302 19:01:49.829601 22447428132992 run.py:483] Algo bellman_ford step 7340 current loss 0.525572, current_train_items 234912.
I0302 19:01:49.845351 22447428132992 run.py:483] Algo bellman_ford step 7341 current loss 0.719124, current_train_items 234944.
I0302 19:01:49.869649 22447428132992 run.py:483] Algo bellman_ford step 7342 current loss 1.079396, current_train_items 234976.
I0302 19:01:49.901599 22447428132992 run.py:483] Algo bellman_ford step 7343 current loss 0.949245, current_train_items 235008.
I0302 19:01:49.934447 22447428132992 run.py:483] Algo bellman_ford step 7344 current loss 1.042126, current_train_items 235040.
I0302 19:01:49.954054 22447428132992 run.py:483] Algo bellman_ford step 7345 current loss 0.588394, current_train_items 235072.
I0302 19:01:49.969761 22447428132992 run.py:483] Algo bellman_ford step 7346 current loss 0.694248, current_train_items 235104.
I0302 19:01:49.993717 22447428132992 run.py:483] Algo bellman_ford step 7347 current loss 0.992856, current_train_items 235136.
I0302 19:01:50.025310 22447428132992 run.py:483] Algo bellman_ford step 7348 current loss 0.937904, current_train_items 235168.
I0302 19:01:50.055694 22447428132992 run.py:483] Algo bellman_ford step 7349 current loss 0.968353, current_train_items 235200.
I0302 19:01:50.075116 22447428132992 run.py:483] Algo bellman_ford step 7350 current loss 0.596901, current_train_items 235232.
I0302 19:01:50.083456 22447428132992 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:50.083598 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:50.100596 22447428132992 run.py:483] Algo bellman_ford step 7351 current loss 0.843855, current_train_items 235264.
I0302 19:01:50.125195 22447428132992 run.py:483] Algo bellman_ford step 7352 current loss 0.971517, current_train_items 235296.
I0302 19:01:50.157571 22447428132992 run.py:483] Algo bellman_ford step 7353 current loss 0.994790, current_train_items 235328.
I0302 19:01:50.190585 22447428132992 run.py:483] Algo bellman_ford step 7354 current loss 0.958788, current_train_items 235360.
I0302 19:01:50.210369 22447428132992 run.py:483] Algo bellman_ford step 7355 current loss 0.530964, current_train_items 235392.
I0302 19:01:50.225885 22447428132992 run.py:483] Algo bellman_ford step 7356 current loss 0.683507, current_train_items 235424.
I0302 19:01:50.250739 22447428132992 run.py:483] Algo bellman_ford step 7357 current loss 0.947784, current_train_items 235456.
I0302 19:01:50.281426 22447428132992 run.py:483] Algo bellman_ford step 7358 current loss 0.928725, current_train_items 235488.
I0302 19:01:50.314681 22447428132992 run.py:483] Algo bellman_ford step 7359 current loss 0.972579, current_train_items 235520.
I0302 19:01:50.334292 22447428132992 run.py:483] Algo bellman_ford step 7360 current loss 0.835570, current_train_items 235552.
I0302 19:01:50.351017 22447428132992 run.py:483] Algo bellman_ford step 7361 current loss 0.801650, current_train_items 235584.
I0302 19:01:50.373359 22447428132992 run.py:483] Algo bellman_ford step 7362 current loss 0.839705, current_train_items 235616.
I0302 19:01:50.406500 22447428132992 run.py:483] Algo bellman_ford step 7363 current loss 1.159677, current_train_items 235648.
I0302 19:01:50.437995 22447428132992 run.py:483] Algo bellman_ford step 7364 current loss 1.102602, current_train_items 235680.
I0302 19:01:50.457587 22447428132992 run.py:483] Algo bellman_ford step 7365 current loss 0.776054, current_train_items 235712.
I0302 19:01:50.473685 22447428132992 run.py:483] Algo bellman_ford step 7366 current loss 0.749214, current_train_items 235744.
I0302 19:01:50.497673 22447428132992 run.py:483] Algo bellman_ford step 7367 current loss 0.825527, current_train_items 235776.
I0302 19:01:50.530678 22447428132992 run.py:483] Algo bellman_ford step 7368 current loss 1.018913, current_train_items 235808.
I0302 19:01:50.562608 22447428132992 run.py:483] Algo bellman_ford step 7369 current loss 0.994382, current_train_items 235840.
I0302 19:01:50.582244 22447428132992 run.py:483] Algo bellman_ford step 7370 current loss 0.540853, current_train_items 235872.
I0302 19:01:50.598535 22447428132992 run.py:483] Algo bellman_ford step 7371 current loss 0.809790, current_train_items 235904.
I0302 19:01:50.620844 22447428132992 run.py:483] Algo bellman_ford step 7372 current loss 0.947380, current_train_items 235936.
I0302 19:01:50.651963 22447428132992 run.py:483] Algo bellman_ford step 7373 current loss 1.025479, current_train_items 235968.
I0302 19:01:50.684604 22447428132992 run.py:483] Algo bellman_ford step 7374 current loss 1.346813, current_train_items 236000.
I0302 19:01:50.704472 22447428132992 run.py:483] Algo bellman_ford step 7375 current loss 0.579330, current_train_items 236032.
I0302 19:01:50.721196 22447428132992 run.py:483] Algo bellman_ford step 7376 current loss 0.740535, current_train_items 236064.
I0302 19:01:50.744853 22447428132992 run.py:483] Algo bellman_ford step 7377 current loss 0.954014, current_train_items 236096.
I0302 19:01:50.775793 22447428132992 run.py:483] Algo bellman_ford step 7378 current loss 1.011509, current_train_items 236128.
I0302 19:01:50.807778 22447428132992 run.py:483] Algo bellman_ford step 7379 current loss 1.085347, current_train_items 236160.
I0302 19:01:50.827327 22447428132992 run.py:483] Algo bellman_ford step 7380 current loss 0.698172, current_train_items 236192.
I0302 19:01:50.843070 22447428132992 run.py:483] Algo bellman_ford step 7381 current loss 0.713892, current_train_items 236224.
I0302 19:01:50.867633 22447428132992 run.py:483] Algo bellman_ford step 7382 current loss 0.908696, current_train_items 236256.
I0302 19:01:50.897882 22447428132992 run.py:483] Algo bellman_ford step 7383 current loss 0.950799, current_train_items 236288.
I0302 19:01:50.930078 22447428132992 run.py:483] Algo bellman_ford step 7384 current loss 0.998919, current_train_items 236320.
I0302 19:01:50.950062 22447428132992 run.py:483] Algo bellman_ford step 7385 current loss 0.762663, current_train_items 236352.
I0302 19:01:50.966345 22447428132992 run.py:483] Algo bellman_ford step 7386 current loss 0.799218, current_train_items 236384.
I0302 19:01:50.989976 22447428132992 run.py:483] Algo bellman_ford step 7387 current loss 1.011901, current_train_items 236416.
I0302 19:01:51.021775 22447428132992 run.py:483] Algo bellman_ford step 7388 current loss 0.978666, current_train_items 236448.
I0302 19:01:51.053910 22447428132992 run.py:483] Algo bellman_ford step 7389 current loss 1.048901, current_train_items 236480.
I0302 19:01:51.073628 22447428132992 run.py:483] Algo bellman_ford step 7390 current loss 0.796117, current_train_items 236512.
I0302 19:01:51.090061 22447428132992 run.py:483] Algo bellman_ford step 7391 current loss 0.941300, current_train_items 236544.
I0302 19:01:51.113111 22447428132992 run.py:483] Algo bellman_ford step 7392 current loss 0.890297, current_train_items 236576.
I0302 19:01:51.143518 22447428132992 run.py:483] Algo bellman_ford step 7393 current loss 0.939047, current_train_items 236608.
I0302 19:01:51.176992 22447428132992 run.py:483] Algo bellman_ford step 7394 current loss 1.189783, current_train_items 236640.
I0302 19:01:51.196340 22447428132992 run.py:483] Algo bellman_ford step 7395 current loss 0.573848, current_train_items 236672.
I0302 19:01:51.212741 22447428132992 run.py:483] Algo bellman_ford step 7396 current loss 0.852354, current_train_items 236704.
I0302 19:01:51.237118 22447428132992 run.py:483] Algo bellman_ford step 7397 current loss 0.960514, current_train_items 236736.
I0302 19:01:51.267574 22447428132992 run.py:483] Algo bellman_ford step 7398 current loss 0.967322, current_train_items 236768.
I0302 19:01:51.299749 22447428132992 run.py:483] Algo bellman_ford step 7399 current loss 1.141495, current_train_items 236800.
I0302 19:01:51.319513 22447428132992 run.py:483] Algo bellman_ford step 7400 current loss 0.519900, current_train_items 236832.
I0302 19:01:51.327446 22447428132992 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:51.327552 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:01:51.344609 22447428132992 run.py:483] Algo bellman_ford step 7401 current loss 0.691581, current_train_items 236864.
I0302 19:01:51.369298 22447428132992 run.py:483] Algo bellman_ford step 7402 current loss 0.879628, current_train_items 236896.
I0302 19:01:51.401451 22447428132992 run.py:483] Algo bellman_ford step 7403 current loss 1.009662, current_train_items 236928.
I0302 19:01:51.435874 22447428132992 run.py:483] Algo bellman_ford step 7404 current loss 1.347365, current_train_items 236960.
I0302 19:01:51.455840 22447428132992 run.py:483] Algo bellman_ford step 7405 current loss 0.598265, current_train_items 236992.
I0302 19:01:51.471408 22447428132992 run.py:483] Algo bellman_ford step 7406 current loss 0.731865, current_train_items 237024.
I0302 19:01:51.495161 22447428132992 run.py:483] Algo bellman_ford step 7407 current loss 0.850487, current_train_items 237056.
I0302 19:01:51.526834 22447428132992 run.py:483] Algo bellman_ford step 7408 current loss 0.990504, current_train_items 237088.
I0302 19:01:51.558374 22447428132992 run.py:483] Algo bellman_ford step 7409 current loss 1.170918, current_train_items 237120.
I0302 19:01:51.577839 22447428132992 run.py:483] Algo bellman_ford step 7410 current loss 0.602818, current_train_items 237152.
I0302 19:01:51.594564 22447428132992 run.py:483] Algo bellman_ford step 7411 current loss 0.736164, current_train_items 237184.
I0302 19:01:51.619292 22447428132992 run.py:483] Algo bellman_ford step 7412 current loss 0.972986, current_train_items 237216.
I0302 19:01:51.650947 22447428132992 run.py:483] Algo bellman_ford step 7413 current loss 1.064329, current_train_items 237248.
I0302 19:01:51.682714 22447428132992 run.py:483] Algo bellman_ford step 7414 current loss 1.407172, current_train_items 237280.
I0302 19:01:51.701992 22447428132992 run.py:483] Algo bellman_ford step 7415 current loss 0.609604, current_train_items 237312.
I0302 19:01:51.717851 22447428132992 run.py:483] Algo bellman_ford step 7416 current loss 0.684965, current_train_items 237344.
I0302 19:01:51.741465 22447428132992 run.py:483] Algo bellman_ford step 7417 current loss 0.857359, current_train_items 237376.
I0302 19:01:51.772277 22447428132992 run.py:483] Algo bellman_ford step 7418 current loss 0.974626, current_train_items 237408.
I0302 19:01:51.804494 22447428132992 run.py:483] Algo bellman_ford step 7419 current loss 1.119949, current_train_items 237440.
I0302 19:01:51.823788 22447428132992 run.py:483] Algo bellman_ford step 7420 current loss 0.550960, current_train_items 237472.
I0302 19:01:51.839774 22447428132992 run.py:483] Algo bellman_ford step 7421 current loss 0.733433, current_train_items 237504.
I0302 19:01:51.863233 22447428132992 run.py:483] Algo bellman_ford step 7422 current loss 0.872418, current_train_items 237536.
I0302 19:01:51.894048 22447428132992 run.py:483] Algo bellman_ford step 7423 current loss 0.997903, current_train_items 237568.
I0302 19:01:51.926767 22447428132992 run.py:483] Algo bellman_ford step 7424 current loss 0.972907, current_train_items 237600.
I0302 19:01:51.946431 22447428132992 run.py:483] Algo bellman_ford step 7425 current loss 0.692173, current_train_items 237632.
I0302 19:01:51.962464 22447428132992 run.py:483] Algo bellman_ford step 7426 current loss 0.692706, current_train_items 237664.
I0302 19:01:51.986699 22447428132992 run.py:483] Algo bellman_ford step 7427 current loss 0.921484, current_train_items 237696.
I0302 19:01:52.017559 22447428132992 run.py:483] Algo bellman_ford step 7428 current loss 0.985110, current_train_items 237728.
I0302 19:01:52.051057 22447428132992 run.py:483] Algo bellman_ford step 7429 current loss 1.278319, current_train_items 237760.
I0302 19:01:52.070618 22447428132992 run.py:483] Algo bellman_ford step 7430 current loss 0.639797, current_train_items 237792.
I0302 19:01:52.086473 22447428132992 run.py:483] Algo bellman_ford step 7431 current loss 0.725612, current_train_items 237824.
I0302 19:01:52.110576 22447428132992 run.py:483] Algo bellman_ford step 7432 current loss 1.001879, current_train_items 237856.
I0302 19:01:52.142169 22447428132992 run.py:483] Algo bellman_ford step 7433 current loss 1.020884, current_train_items 237888.
I0302 19:01:52.173470 22447428132992 run.py:483] Algo bellman_ford step 7434 current loss 1.213093, current_train_items 237920.
I0302 19:01:52.192930 22447428132992 run.py:483] Algo bellman_ford step 7435 current loss 0.518468, current_train_items 237952.
I0302 19:01:52.208993 22447428132992 run.py:483] Algo bellman_ford step 7436 current loss 0.754999, current_train_items 237984.
I0302 19:01:52.233307 22447428132992 run.py:483] Algo bellman_ford step 7437 current loss 0.881436, current_train_items 238016.
I0302 19:01:52.263772 22447428132992 run.py:483] Algo bellman_ford step 7438 current loss 1.054903, current_train_items 238048.
I0302 19:01:52.297884 22447428132992 run.py:483] Algo bellman_ford step 7439 current loss 1.066897, current_train_items 238080.
I0302 19:01:52.317568 22447428132992 run.py:483] Algo bellman_ford step 7440 current loss 0.600336, current_train_items 238112.
I0302 19:01:52.333994 22447428132992 run.py:483] Algo bellman_ford step 7441 current loss 0.816240, current_train_items 238144.
I0302 19:01:52.357021 22447428132992 run.py:483] Algo bellman_ford step 7442 current loss 0.922556, current_train_items 238176.
I0302 19:01:52.388953 22447428132992 run.py:483] Algo bellman_ford step 7443 current loss 1.068026, current_train_items 238208.
I0302 19:01:52.422885 22447428132992 run.py:483] Algo bellman_ford step 7444 current loss 1.089815, current_train_items 238240.
I0302 19:01:52.442331 22447428132992 run.py:483] Algo bellman_ford step 7445 current loss 0.541443, current_train_items 238272.
I0302 19:01:52.458443 22447428132992 run.py:483] Algo bellman_ford step 7446 current loss 0.729422, current_train_items 238304.
I0302 19:01:52.481804 22447428132992 run.py:483] Algo bellman_ford step 7447 current loss 0.889131, current_train_items 238336.
I0302 19:01:52.512423 22447428132992 run.py:483] Algo bellman_ford step 7448 current loss 1.001934, current_train_items 238368.
I0302 19:01:52.546969 22447428132992 run.py:483] Algo bellman_ford step 7449 current loss 1.214774, current_train_items 238400.
I0302 19:01:52.566482 22447428132992 run.py:483] Algo bellman_ford step 7450 current loss 0.564841, current_train_items 238432.
I0302 19:01:52.574720 22447428132992 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:52.574826 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:01:52.591509 22447428132992 run.py:483] Algo bellman_ford step 7451 current loss 0.655163, current_train_items 238464.
I0302 19:01:52.616523 22447428132992 run.py:483] Algo bellman_ford step 7452 current loss 0.948086, current_train_items 238496.
I0302 19:01:52.647148 22447428132992 run.py:483] Algo bellman_ford step 7453 current loss 0.917502, current_train_items 238528.
I0302 19:01:52.679948 22447428132992 run.py:483] Algo bellman_ford step 7454 current loss 1.203723, current_train_items 238560.
I0302 19:01:52.699774 22447428132992 run.py:483] Algo bellman_ford step 7455 current loss 0.764440, current_train_items 238592.
I0302 19:01:52.716132 22447428132992 run.py:483] Algo bellman_ford step 7456 current loss 0.748193, current_train_items 238624.
I0302 19:01:52.739372 22447428132992 run.py:483] Algo bellman_ford step 7457 current loss 0.808940, current_train_items 238656.
I0302 19:01:52.771190 22447428132992 run.py:483] Algo bellman_ford step 7458 current loss 0.994741, current_train_items 238688.
I0302 19:01:52.803977 22447428132992 run.py:483] Algo bellman_ford step 7459 current loss 1.110459, current_train_items 238720.
I0302 19:01:52.823705 22447428132992 run.py:483] Algo bellman_ford step 7460 current loss 0.521784, current_train_items 238752.
I0302 19:01:52.840476 22447428132992 run.py:483] Algo bellman_ford step 7461 current loss 0.743184, current_train_items 238784.
I0302 19:01:52.863078 22447428132992 run.py:483] Algo bellman_ford step 7462 current loss 0.950749, current_train_items 238816.
I0302 19:01:52.893710 22447428132992 run.py:483] Algo bellman_ford step 7463 current loss 0.983222, current_train_items 238848.
I0302 19:01:52.926824 22447428132992 run.py:483] Algo bellman_ford step 7464 current loss 1.172526, current_train_items 238880.
I0302 19:01:52.946247 22447428132992 run.py:483] Algo bellman_ford step 7465 current loss 0.540792, current_train_items 238912.
I0302 19:01:52.962134 22447428132992 run.py:483] Algo bellman_ford step 7466 current loss 0.674052, current_train_items 238944.
I0302 19:01:52.985234 22447428132992 run.py:483] Algo bellman_ford step 7467 current loss 0.963667, current_train_items 238976.
I0302 19:01:53.017291 22447428132992 run.py:483] Algo bellman_ford step 7468 current loss 0.973490, current_train_items 239008.
I0302 19:01:53.049190 22447428132992 run.py:483] Algo bellman_ford step 7469 current loss 1.030306, current_train_items 239040.
I0302 19:01:53.069172 22447428132992 run.py:483] Algo bellman_ford step 7470 current loss 0.545551, current_train_items 239072.
I0302 19:01:53.085391 22447428132992 run.py:483] Algo bellman_ford step 7471 current loss 0.752571, current_train_items 239104.
I0302 19:01:53.108085 22447428132992 run.py:483] Algo bellman_ford step 7472 current loss 0.920401, current_train_items 239136.
I0302 19:01:53.139476 22447428132992 run.py:483] Algo bellman_ford step 7473 current loss 1.046575, current_train_items 239168.
I0302 19:01:53.174110 22447428132992 run.py:483] Algo bellman_ford step 7474 current loss 1.124652, current_train_items 239200.
I0302 19:01:53.193965 22447428132992 run.py:483] Algo bellman_ford step 7475 current loss 0.528130, current_train_items 239232.
I0302 19:01:53.210193 22447428132992 run.py:483] Algo bellman_ford step 7476 current loss 0.732004, current_train_items 239264.
I0302 19:01:53.232606 22447428132992 run.py:483] Algo bellman_ford step 7477 current loss 0.867638, current_train_items 239296.
I0302 19:01:53.264428 22447428132992 run.py:483] Algo bellman_ford step 7478 current loss 0.881661, current_train_items 239328.
I0302 19:01:53.300478 22447428132992 run.py:483] Algo bellman_ford step 7479 current loss 1.146778, current_train_items 239360.
I0302 19:01:53.319936 22447428132992 run.py:483] Algo bellman_ford step 7480 current loss 0.524146, current_train_items 239392.
I0302 19:01:53.336255 22447428132992 run.py:483] Algo bellman_ford step 7481 current loss 0.771027, current_train_items 239424.
I0302 19:01:53.360701 22447428132992 run.py:483] Algo bellman_ford step 7482 current loss 0.951919, current_train_items 239456.
I0302 19:01:53.392813 22447428132992 run.py:483] Algo bellman_ford step 7483 current loss 1.091288, current_train_items 239488.
I0302 19:01:53.426893 22447428132992 run.py:483] Algo bellman_ford step 7484 current loss 1.108727, current_train_items 239520.
I0302 19:01:53.446666 22447428132992 run.py:483] Algo bellman_ford step 7485 current loss 0.671379, current_train_items 239552.
I0302 19:01:53.462865 22447428132992 run.py:483] Algo bellman_ford step 7486 current loss 0.959194, current_train_items 239584.
I0302 19:01:53.485391 22447428132992 run.py:483] Algo bellman_ford step 7487 current loss 0.860086, current_train_items 239616.
I0302 19:01:53.516841 22447428132992 run.py:483] Algo bellman_ford step 7488 current loss 1.039248, current_train_items 239648.
I0302 19:01:53.551749 22447428132992 run.py:483] Algo bellman_ford step 7489 current loss 1.104214, current_train_items 239680.
I0302 19:01:53.571221 22447428132992 run.py:483] Algo bellman_ford step 7490 current loss 0.599351, current_train_items 239712.
I0302 19:01:53.586984 22447428132992 run.py:483] Algo bellman_ford step 7491 current loss 0.764823, current_train_items 239744.
I0302 19:01:53.609620 22447428132992 run.py:483] Algo bellman_ford step 7492 current loss 0.781289, current_train_items 239776.
I0302 19:01:53.640354 22447428132992 run.py:483] Algo bellman_ford step 7493 current loss 0.912138, current_train_items 239808.
I0302 19:01:53.672962 22447428132992 run.py:483] Algo bellman_ford step 7494 current loss 1.140476, current_train_items 239840.
I0302 19:01:53.692313 22447428132992 run.py:483] Algo bellman_ford step 7495 current loss 0.686718, current_train_items 239872.
I0302 19:01:53.708618 22447428132992 run.py:483] Algo bellman_ford step 7496 current loss 0.725080, current_train_items 239904.
I0302 19:01:53.732126 22447428132992 run.py:483] Algo bellman_ford step 7497 current loss 0.922053, current_train_items 239936.
I0302 19:01:53.762503 22447428132992 run.py:483] Algo bellman_ford step 7498 current loss 0.951514, current_train_items 239968.
I0302 19:01:53.795017 22447428132992 run.py:483] Algo bellman_ford step 7499 current loss 1.065103, current_train_items 240000.
I0302 19:01:53.814676 22447428132992 run.py:483] Algo bellman_ford step 7500 current loss 0.650721, current_train_items 240032.
I0302 19:01:53.822514 22447428132992 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:53.822622 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:01:53.839222 22447428132992 run.py:483] Algo bellman_ford step 7501 current loss 0.665826, current_train_items 240064.
I0302 19:01:53.862942 22447428132992 run.py:483] Algo bellman_ford step 7502 current loss 0.902977, current_train_items 240096.
I0302 19:01:53.894373 22447428132992 run.py:483] Algo bellman_ford step 7503 current loss 0.940119, current_train_items 240128.
I0302 19:01:53.927934 22447428132992 run.py:483] Algo bellman_ford step 7504 current loss 1.066072, current_train_items 240160.
I0302 19:01:53.947815 22447428132992 run.py:483] Algo bellman_ford step 7505 current loss 0.639370, current_train_items 240192.
I0302 19:01:53.963614 22447428132992 run.py:483] Algo bellman_ford step 7506 current loss 0.752392, current_train_items 240224.
I0302 19:01:53.986595 22447428132992 run.py:483] Algo bellman_ford step 7507 current loss 0.814921, current_train_items 240256.
I0302 19:01:54.017164 22447428132992 run.py:483] Algo bellman_ford step 7508 current loss 0.957931, current_train_items 240288.
I0302 19:01:54.049503 22447428132992 run.py:483] Algo bellman_ford step 7509 current loss 0.987172, current_train_items 240320.
I0302 19:01:54.068820 22447428132992 run.py:483] Algo bellman_ford step 7510 current loss 0.604692, current_train_items 240352.
I0302 19:01:54.085288 22447428132992 run.py:483] Algo bellman_ford step 7511 current loss 0.835049, current_train_items 240384.
I0302 19:01:54.107721 22447428132992 run.py:483] Algo bellman_ford step 7512 current loss 0.947092, current_train_items 240416.
I0302 19:01:54.138408 22447428132992 run.py:483] Algo bellman_ford step 7513 current loss 0.988992, current_train_items 240448.
I0302 19:01:54.170346 22447428132992 run.py:483] Algo bellman_ford step 7514 current loss 1.096673, current_train_items 240480.
I0302 19:01:54.189754 22447428132992 run.py:483] Algo bellman_ford step 7515 current loss 0.582832, current_train_items 240512.
I0302 19:01:54.205204 22447428132992 run.py:483] Algo bellman_ford step 7516 current loss 0.732538, current_train_items 240544.
I0302 19:01:54.229146 22447428132992 run.py:483] Algo bellman_ford step 7517 current loss 0.938014, current_train_items 240576.
I0302 19:01:54.260458 22447428132992 run.py:483] Algo bellman_ford step 7518 current loss 1.015885, current_train_items 240608.
I0302 19:01:54.293870 22447428132992 run.py:483] Algo bellman_ford step 7519 current loss 1.275283, current_train_items 240640.
I0302 19:01:54.313121 22447428132992 run.py:483] Algo bellman_ford step 7520 current loss 0.778672, current_train_items 240672.
I0302 19:01:54.329130 22447428132992 run.py:483] Algo bellman_ford step 7521 current loss 0.732944, current_train_items 240704.
I0302 19:01:54.352231 22447428132992 run.py:483] Algo bellman_ford step 7522 current loss 0.892008, current_train_items 240736.
I0302 19:01:54.383331 22447428132992 run.py:483] Algo bellman_ford step 7523 current loss 1.052831, current_train_items 240768.
I0302 19:01:54.413899 22447428132992 run.py:483] Algo bellman_ford step 7524 current loss 1.019073, current_train_items 240800.
I0302 19:01:54.433055 22447428132992 run.py:483] Algo bellman_ford step 7525 current loss 0.616504, current_train_items 240832.
I0302 19:01:54.449062 22447428132992 run.py:483] Algo bellman_ford step 7526 current loss 0.808650, current_train_items 240864.
I0302 19:01:54.473287 22447428132992 run.py:483] Algo bellman_ford step 7527 current loss 0.965793, current_train_items 240896.
I0302 19:01:54.503593 22447428132992 run.py:483] Algo bellman_ford step 7528 current loss 0.871422, current_train_items 240928.
I0302 19:01:54.536649 22447428132992 run.py:483] Algo bellman_ford step 7529 current loss 0.983429, current_train_items 240960.
I0302 19:01:54.556084 22447428132992 run.py:483] Algo bellman_ford step 7530 current loss 0.676067, current_train_items 240992.
I0302 19:01:54.572052 22447428132992 run.py:483] Algo bellman_ford step 7531 current loss 0.804896, current_train_items 241024.
I0302 19:01:54.594711 22447428132992 run.py:483] Algo bellman_ford step 7532 current loss 0.852108, current_train_items 241056.
I0302 19:01:54.627072 22447428132992 run.py:483] Algo bellman_ford step 7533 current loss 1.040545, current_train_items 241088.
I0302 19:01:54.661044 22447428132992 run.py:483] Algo bellman_ford step 7534 current loss 1.009078, current_train_items 241120.
I0302 19:01:54.680514 22447428132992 run.py:483] Algo bellman_ford step 7535 current loss 0.516481, current_train_items 241152.
I0302 19:01:54.696672 22447428132992 run.py:483] Algo bellman_ford step 7536 current loss 0.721116, current_train_items 241184.
I0302 19:01:54.719910 22447428132992 run.py:483] Algo bellman_ford step 7537 current loss 0.937768, current_train_items 241216.
I0302 19:01:54.751282 22447428132992 run.py:483] Algo bellman_ford step 7538 current loss 1.098287, current_train_items 241248.
I0302 19:01:54.783957 22447428132992 run.py:483] Algo bellman_ford step 7539 current loss 1.099632, current_train_items 241280.
I0302 19:01:54.803589 22447428132992 run.py:483] Algo bellman_ford step 7540 current loss 0.552781, current_train_items 241312.
I0302 19:01:54.820063 22447428132992 run.py:483] Algo bellman_ford step 7541 current loss 0.850401, current_train_items 241344.
I0302 19:01:54.844504 22447428132992 run.py:483] Algo bellman_ford step 7542 current loss 0.931817, current_train_items 241376.
I0302 19:01:54.876462 22447428132992 run.py:483] Algo bellman_ford step 7543 current loss 1.054579, current_train_items 241408.
I0302 19:01:54.909979 22447428132992 run.py:483] Algo bellman_ford step 7544 current loss 1.170933, current_train_items 241440.
I0302 19:01:54.929018 22447428132992 run.py:483] Algo bellman_ford step 7545 current loss 0.668258, current_train_items 241472.
I0302 19:01:54.945194 22447428132992 run.py:483] Algo bellman_ford step 7546 current loss 0.826296, current_train_items 241504.
I0302 19:01:54.969308 22447428132992 run.py:483] Algo bellman_ford step 7547 current loss 1.028750, current_train_items 241536.
I0302 19:01:55.000933 22447428132992 run.py:483] Algo bellman_ford step 7548 current loss 1.068440, current_train_items 241568.
I0302 19:01:55.034205 22447428132992 run.py:483] Algo bellman_ford step 7549 current loss 1.112233, current_train_items 241600.
I0302 19:01:55.053683 22447428132992 run.py:483] Algo bellman_ford step 7550 current loss 0.733252, current_train_items 241632.
I0302 19:01:55.061675 22447428132992 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:55.061784 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:01:55.078824 22447428132992 run.py:483] Algo bellman_ford step 7551 current loss 0.815983, current_train_items 241664.
I0302 19:01:55.102796 22447428132992 run.py:483] Algo bellman_ford step 7552 current loss 0.873056, current_train_items 241696.
I0302 19:01:55.134705 22447428132992 run.py:483] Algo bellman_ford step 7553 current loss 1.000998, current_train_items 241728.
I0302 19:01:55.168176 22447428132992 run.py:483] Algo bellman_ford step 7554 current loss 1.187019, current_train_items 241760.
I0302 19:01:55.187996 22447428132992 run.py:483] Algo bellman_ford step 7555 current loss 0.796904, current_train_items 241792.
I0302 19:01:55.204052 22447428132992 run.py:483] Algo bellman_ford step 7556 current loss 0.905259, current_train_items 241824.
I0302 19:01:55.227816 22447428132992 run.py:483] Algo bellman_ford step 7557 current loss 0.831577, current_train_items 241856.
I0302 19:01:55.260114 22447428132992 run.py:483] Algo bellman_ford step 7558 current loss 1.093565, current_train_items 241888.
I0302 19:01:55.292244 22447428132992 run.py:483] Algo bellman_ford step 7559 current loss 1.024127, current_train_items 241920.
I0302 19:01:55.312005 22447428132992 run.py:483] Algo bellman_ford step 7560 current loss 0.527087, current_train_items 241952.
I0302 19:01:55.328905 22447428132992 run.py:483] Algo bellman_ford step 7561 current loss 0.782592, current_train_items 241984.
I0302 19:01:55.351687 22447428132992 run.py:483] Algo bellman_ford step 7562 current loss 0.903885, current_train_items 242016.
I0302 19:01:55.383132 22447428132992 run.py:483] Algo bellman_ford step 7563 current loss 1.060614, current_train_items 242048.
I0302 19:01:55.416662 22447428132992 run.py:483] Algo bellman_ford step 7564 current loss 1.164384, current_train_items 242080.
I0302 19:01:55.436042 22447428132992 run.py:483] Algo bellman_ford step 7565 current loss 0.654365, current_train_items 242112.
I0302 19:01:55.451946 22447428132992 run.py:483] Algo bellman_ford step 7566 current loss 0.752954, current_train_items 242144.
I0302 19:01:55.475493 22447428132992 run.py:483] Algo bellman_ford step 7567 current loss 1.014107, current_train_items 242176.
I0302 19:01:55.506191 22447428132992 run.py:483] Algo bellman_ford step 7568 current loss 1.075194, current_train_items 242208.
I0302 19:01:55.538579 22447428132992 run.py:483] Algo bellman_ford step 7569 current loss 1.077559, current_train_items 242240.
I0302 19:01:55.558404 22447428132992 run.py:483] Algo bellman_ford step 7570 current loss 0.719921, current_train_items 242272.
I0302 19:01:55.574792 22447428132992 run.py:483] Algo bellman_ford step 7571 current loss 0.826866, current_train_items 242304.
I0302 19:01:55.597512 22447428132992 run.py:483] Algo bellman_ford step 7572 current loss 0.947646, current_train_items 242336.
I0302 19:01:55.627544 22447428132992 run.py:483] Algo bellman_ford step 7573 current loss 0.943668, current_train_items 242368.
I0302 19:01:55.658251 22447428132992 run.py:483] Algo bellman_ford step 7574 current loss 1.172413, current_train_items 242400.
I0302 19:01:55.677860 22447428132992 run.py:483] Algo bellman_ford step 7575 current loss 0.489660, current_train_items 242432.
I0302 19:01:55.694548 22447428132992 run.py:483] Algo bellman_ford step 7576 current loss 0.702026, current_train_items 242464.
I0302 19:01:55.716916 22447428132992 run.py:483] Algo bellman_ford step 7577 current loss 0.781505, current_train_items 242496.
I0302 19:01:55.748214 22447428132992 run.py:483] Algo bellman_ford step 7578 current loss 1.005577, current_train_items 242528.
I0302 19:01:55.782777 22447428132992 run.py:483] Algo bellman_ford step 7579 current loss 1.351447, current_train_items 242560.
I0302 19:01:55.802133 22447428132992 run.py:483] Algo bellman_ford step 7580 current loss 0.499416, current_train_items 242592.
I0302 19:01:55.818372 22447428132992 run.py:483] Algo bellman_ford step 7581 current loss 0.825193, current_train_items 242624.
I0302 19:01:55.842120 22447428132992 run.py:483] Algo bellman_ford step 7582 current loss 0.958256, current_train_items 242656.
I0302 19:01:55.872894 22447428132992 run.py:483] Algo bellman_ford step 7583 current loss 0.961537, current_train_items 242688.
I0302 19:01:55.905081 22447428132992 run.py:483] Algo bellman_ford step 7584 current loss 1.136808, current_train_items 242720.
I0302 19:01:55.924735 22447428132992 run.py:483] Algo bellman_ford step 7585 current loss 0.922044, current_train_items 242752.
I0302 19:01:55.940548 22447428132992 run.py:483] Algo bellman_ford step 7586 current loss 0.767258, current_train_items 242784.
I0302 19:01:55.963671 22447428132992 run.py:483] Algo bellman_ford step 7587 current loss 0.966365, current_train_items 242816.
I0302 19:01:55.992875 22447428132992 run.py:483] Algo bellman_ford step 7588 current loss 0.835432, current_train_items 242848.
I0302 19:01:56.026421 22447428132992 run.py:483] Algo bellman_ford step 7589 current loss 1.084531, current_train_items 242880.
I0302 19:01:56.045887 22447428132992 run.py:483] Algo bellman_ford step 7590 current loss 0.591749, current_train_items 242912.
I0302 19:01:56.061727 22447428132992 run.py:483] Algo bellman_ford step 7591 current loss 0.668919, current_train_items 242944.
I0302 19:01:56.085776 22447428132992 run.py:483] Algo bellman_ford step 7592 current loss 0.874750, current_train_items 242976.
I0302 19:01:56.115313 22447428132992 run.py:483] Algo bellman_ford step 7593 current loss 0.994883, current_train_items 243008.
I0302 19:01:56.148759 22447428132992 run.py:483] Algo bellman_ford step 7594 current loss 1.072211, current_train_items 243040.
I0302 19:01:56.168177 22447428132992 run.py:483] Algo bellman_ford step 7595 current loss 0.571184, current_train_items 243072.
I0302 19:01:56.184132 22447428132992 run.py:483] Algo bellman_ford step 7596 current loss 0.737540, current_train_items 243104.
I0302 19:01:56.207870 22447428132992 run.py:483] Algo bellman_ford step 7597 current loss 0.937654, current_train_items 243136.
I0302 19:01:56.239568 22447428132992 run.py:483] Algo bellman_ford step 7598 current loss 1.092746, current_train_items 243168.
I0302 19:01:56.269927 22447428132992 run.py:483] Algo bellman_ford step 7599 current loss 1.100747, current_train_items 243200.
I0302 19:01:56.289591 22447428132992 run.py:483] Algo bellman_ford step 7600 current loss 0.502003, current_train_items 243232.
I0302 19:01:56.297419 22447428132992 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:01:56.297527 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:56.314273 22447428132992 run.py:483] Algo bellman_ford step 7601 current loss 0.788717, current_train_items 243264.
I0302 19:01:56.339540 22447428132992 run.py:483] Algo bellman_ford step 7602 current loss 0.948737, current_train_items 243296.
I0302 19:01:56.370491 22447428132992 run.py:483] Algo bellman_ford step 7603 current loss 0.915067, current_train_items 243328.
I0302 19:01:56.404673 22447428132992 run.py:483] Algo bellman_ford step 7604 current loss 1.202896, current_train_items 243360.
I0302 19:01:56.424639 22447428132992 run.py:483] Algo bellman_ford step 7605 current loss 0.796461, current_train_items 243392.
I0302 19:01:56.439924 22447428132992 run.py:483] Algo bellman_ford step 7606 current loss 0.802843, current_train_items 243424.
I0302 19:01:56.463037 22447428132992 run.py:483] Algo bellman_ford step 7607 current loss 0.805511, current_train_items 243456.
I0302 19:01:56.494403 22447428132992 run.py:483] Algo bellman_ford step 7608 current loss 1.028222, current_train_items 243488.
I0302 19:01:56.528825 22447428132992 run.py:483] Algo bellman_ford step 7609 current loss 1.138497, current_train_items 243520.
I0302 19:01:56.548139 22447428132992 run.py:483] Algo bellman_ford step 7610 current loss 0.541608, current_train_items 243552.
I0302 19:01:56.564697 22447428132992 run.py:483] Algo bellman_ford step 7611 current loss 0.857863, current_train_items 243584.
I0302 19:01:56.588400 22447428132992 run.py:483] Algo bellman_ford step 7612 current loss 0.919172, current_train_items 243616.
I0302 19:01:56.617324 22447428132992 run.py:483] Algo bellman_ford step 7613 current loss 0.912215, current_train_items 243648.
I0302 19:01:56.649849 22447428132992 run.py:483] Algo bellman_ford step 7614 current loss 1.280676, current_train_items 243680.
I0302 19:01:56.669206 22447428132992 run.py:483] Algo bellman_ford step 7615 current loss 0.555370, current_train_items 243712.
I0302 19:01:56.685438 22447428132992 run.py:483] Algo bellman_ford step 7616 current loss 0.715936, current_train_items 243744.
I0302 19:01:56.708613 22447428132992 run.py:483] Algo bellman_ford step 7617 current loss 0.813861, current_train_items 243776.
I0302 19:01:56.738880 22447428132992 run.py:483] Algo bellman_ford step 7618 current loss 0.981927, current_train_items 243808.
I0302 19:01:56.772376 22447428132992 run.py:483] Algo bellman_ford step 7619 current loss 1.149894, current_train_items 243840.
I0302 19:01:56.791978 22447428132992 run.py:483] Algo bellman_ford step 7620 current loss 0.571856, current_train_items 243872.
I0302 19:01:56.808092 22447428132992 run.py:483] Algo bellman_ford step 7621 current loss 0.749993, current_train_items 243904.
I0302 19:01:56.831807 22447428132992 run.py:483] Algo bellman_ford step 7622 current loss 0.883157, current_train_items 243936.
I0302 19:01:56.863903 22447428132992 run.py:483] Algo bellman_ford step 7623 current loss 0.964176, current_train_items 243968.
I0302 19:01:56.896113 22447428132992 run.py:483] Algo bellman_ford step 7624 current loss 1.015800, current_train_items 244000.
I0302 19:01:56.915583 22447428132992 run.py:483] Algo bellman_ford step 7625 current loss 0.802466, current_train_items 244032.
I0302 19:01:56.930936 22447428132992 run.py:483] Algo bellman_ford step 7626 current loss 0.640967, current_train_items 244064.
I0302 19:01:56.954946 22447428132992 run.py:483] Algo bellman_ford step 7627 current loss 1.062581, current_train_items 244096.
I0302 19:01:56.986757 22447428132992 run.py:483] Algo bellman_ford step 7628 current loss 0.959774, current_train_items 244128.
I0302 19:01:57.019062 22447428132992 run.py:483] Algo bellman_ford step 7629 current loss 1.080522, current_train_items 244160.
I0302 19:01:57.038359 22447428132992 run.py:483] Algo bellman_ford step 7630 current loss 0.593799, current_train_items 244192.
I0302 19:01:57.054654 22447428132992 run.py:483] Algo bellman_ford step 7631 current loss 0.860406, current_train_items 244224.
I0302 19:01:57.077795 22447428132992 run.py:483] Algo bellman_ford step 7632 current loss 0.978802, current_train_items 244256.
I0302 19:01:57.109556 22447428132992 run.py:483] Algo bellman_ford step 7633 current loss 1.035803, current_train_items 244288.
I0302 19:01:57.143116 22447428132992 run.py:483] Algo bellman_ford step 7634 current loss 1.021337, current_train_items 244320.
I0302 19:01:57.162559 22447428132992 run.py:483] Algo bellman_ford step 7635 current loss 0.550837, current_train_items 244352.
I0302 19:01:57.178557 22447428132992 run.py:483] Algo bellman_ford step 7636 current loss 0.759370, current_train_items 244384.
I0302 19:01:57.201954 22447428132992 run.py:483] Algo bellman_ford step 7637 current loss 0.905723, current_train_items 244416.
I0302 19:01:57.231640 22447428132992 run.py:483] Algo bellman_ford step 7638 current loss 0.906142, current_train_items 244448.
I0302 19:01:57.266974 22447428132992 run.py:483] Algo bellman_ford step 7639 current loss 1.119389, current_train_items 244480.
I0302 19:01:57.286287 22447428132992 run.py:483] Algo bellman_ford step 7640 current loss 0.512077, current_train_items 244512.
I0302 19:01:57.302164 22447428132992 run.py:483] Algo bellman_ford step 7641 current loss 0.800561, current_train_items 244544.
I0302 19:01:57.324691 22447428132992 run.py:483] Algo bellman_ford step 7642 current loss 0.922933, current_train_items 244576.
I0302 19:01:57.358044 22447428132992 run.py:483] Algo bellman_ford step 7643 current loss 1.033001, current_train_items 244608.
I0302 19:01:57.392906 22447428132992 run.py:483] Algo bellman_ford step 7644 current loss 1.073981, current_train_items 244640.
I0302 19:01:57.412301 22447428132992 run.py:483] Algo bellman_ford step 7645 current loss 0.542464, current_train_items 244672.
I0302 19:01:57.428477 22447428132992 run.py:483] Algo bellman_ford step 7646 current loss 0.872659, current_train_items 244704.
I0302 19:01:57.452753 22447428132992 run.py:483] Algo bellman_ford step 7647 current loss 0.889222, current_train_items 244736.
I0302 19:01:57.485251 22447428132992 run.py:483] Algo bellman_ford step 7648 current loss 0.974217, current_train_items 244768.
I0302 19:01:57.520263 22447428132992 run.py:483] Algo bellman_ford step 7649 current loss 1.202357, current_train_items 244800.
I0302 19:01:57.539624 22447428132992 run.py:483] Algo bellman_ford step 7650 current loss 0.645068, current_train_items 244832.
I0302 19:01:57.547943 22447428132992 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:01:57.548049 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:01:57.564302 22447428132992 run.py:483] Algo bellman_ford step 7651 current loss 0.708010, current_train_items 244864.
I0302 19:01:57.589242 22447428132992 run.py:483] Algo bellman_ford step 7652 current loss 0.963412, current_train_items 244896.
I0302 19:01:57.620428 22447428132992 run.py:483] Algo bellman_ford step 7653 current loss 0.921122, current_train_items 244928.
I0302 19:01:57.654109 22447428132992 run.py:483] Algo bellman_ford step 7654 current loss 1.125176, current_train_items 244960.
I0302 19:01:57.674064 22447428132992 run.py:483] Algo bellman_ford step 7655 current loss 0.574283, current_train_items 244992.
I0302 19:01:57.689915 22447428132992 run.py:483] Algo bellman_ford step 7656 current loss 0.692439, current_train_items 245024.
I0302 19:01:57.713933 22447428132992 run.py:483] Algo bellman_ford step 7657 current loss 1.040656, current_train_items 245056.
I0302 19:01:57.745744 22447428132992 run.py:483] Algo bellman_ford step 7658 current loss 0.978158, current_train_items 245088.
I0302 19:01:57.779989 22447428132992 run.py:483] Algo bellman_ford step 7659 current loss 1.473404, current_train_items 245120.
I0302 19:01:57.800130 22447428132992 run.py:483] Algo bellman_ford step 7660 current loss 0.565408, current_train_items 245152.
I0302 19:01:57.816423 22447428132992 run.py:483] Algo bellman_ford step 7661 current loss 0.693497, current_train_items 245184.
I0302 19:01:57.839930 22447428132992 run.py:483] Algo bellman_ford step 7662 current loss 0.890255, current_train_items 245216.
I0302 19:01:57.870565 22447428132992 run.py:483] Algo bellman_ford step 7663 current loss 1.071908, current_train_items 245248.
I0302 19:01:57.903932 22447428132992 run.py:483] Algo bellman_ford step 7664 current loss 1.166279, current_train_items 245280.
I0302 19:01:57.923323 22447428132992 run.py:483] Algo bellman_ford step 7665 current loss 0.568541, current_train_items 245312.
I0302 19:01:57.939468 22447428132992 run.py:483] Algo bellman_ford step 7666 current loss 0.742978, current_train_items 245344.
I0302 19:01:57.962879 22447428132992 run.py:483] Algo bellman_ford step 7667 current loss 0.976461, current_train_items 245376.
I0302 19:01:57.996428 22447428132992 run.py:483] Algo bellman_ford step 7668 current loss 0.942997, current_train_items 245408.
I0302 19:01:58.030824 22447428132992 run.py:483] Algo bellman_ford step 7669 current loss 1.100972, current_train_items 245440.
I0302 19:01:58.050517 22447428132992 run.py:483] Algo bellman_ford step 7670 current loss 0.537749, current_train_items 245472.
I0302 19:01:58.067008 22447428132992 run.py:483] Algo bellman_ford step 7671 current loss 0.803504, current_train_items 245504.
I0302 19:01:58.090271 22447428132992 run.py:483] Algo bellman_ford step 7672 current loss 0.994221, current_train_items 245536.
I0302 19:01:58.121822 22447428132992 run.py:483] Algo bellman_ford step 7673 current loss 1.137093, current_train_items 245568.
I0302 19:01:58.155840 22447428132992 run.py:483] Algo bellman_ford step 7674 current loss 1.305933, current_train_items 245600.
I0302 19:01:58.175799 22447428132992 run.py:483] Algo bellman_ford step 7675 current loss 0.604744, current_train_items 245632.
I0302 19:01:58.192461 22447428132992 run.py:483] Algo bellman_ford step 7676 current loss 0.898481, current_train_items 245664.
I0302 19:01:58.216417 22447428132992 run.py:483] Algo bellman_ford step 7677 current loss 0.975345, current_train_items 245696.
I0302 19:01:58.247293 22447428132992 run.py:483] Algo bellman_ford step 7678 current loss 0.892472, current_train_items 245728.
I0302 19:01:58.281930 22447428132992 run.py:483] Algo bellman_ford step 7679 current loss 1.084159, current_train_items 245760.
I0302 19:01:58.301614 22447428132992 run.py:483] Algo bellman_ford step 7680 current loss 0.763681, current_train_items 245792.
I0302 19:01:58.317949 22447428132992 run.py:483] Algo bellman_ford step 7681 current loss 0.936252, current_train_items 245824.
I0302 19:01:58.341282 22447428132992 run.py:483] Algo bellman_ford step 7682 current loss 0.868174, current_train_items 245856.
I0302 19:01:58.372998 22447428132992 run.py:483] Algo bellman_ford step 7683 current loss 0.982356, current_train_items 245888.
I0302 19:01:58.407528 22447428132992 run.py:483] Algo bellman_ford step 7684 current loss 1.043698, current_train_items 245920.
I0302 19:01:58.427487 22447428132992 run.py:483] Algo bellman_ford step 7685 current loss 0.618388, current_train_items 245952.
I0302 19:01:58.443845 22447428132992 run.py:483] Algo bellman_ford step 7686 current loss 0.775010, current_train_items 245984.
I0302 19:01:58.466413 22447428132992 run.py:483] Algo bellman_ford step 7687 current loss 0.933805, current_train_items 246016.
I0302 19:01:58.498829 22447428132992 run.py:483] Algo bellman_ford step 7688 current loss 0.982194, current_train_items 246048.
I0302 19:01:58.532423 22447428132992 run.py:483] Algo bellman_ford step 7689 current loss 1.095123, current_train_items 246080.
I0302 19:01:58.552365 22447428132992 run.py:483] Algo bellman_ford step 7690 current loss 0.560081, current_train_items 246112.
I0302 19:01:58.568570 22447428132992 run.py:483] Algo bellman_ford step 7691 current loss 0.776022, current_train_items 246144.
I0302 19:01:58.591807 22447428132992 run.py:483] Algo bellman_ford step 7692 current loss 0.880315, current_train_items 246176.
I0302 19:01:58.624897 22447428132992 run.py:483] Algo bellman_ford step 7693 current loss 1.023757, current_train_items 246208.
I0302 19:01:58.657986 22447428132992 run.py:483] Algo bellman_ford step 7694 current loss 1.237544, current_train_items 246240.
I0302 19:01:58.677574 22447428132992 run.py:483] Algo bellman_ford step 7695 current loss 0.643217, current_train_items 246272.
I0302 19:01:58.693933 22447428132992 run.py:483] Algo bellman_ford step 7696 current loss 0.708909, current_train_items 246304.
I0302 19:01:58.718205 22447428132992 run.py:483] Algo bellman_ford step 7697 current loss 1.005259, current_train_items 246336.
I0302 19:01:58.750647 22447428132992 run.py:483] Algo bellman_ford step 7698 current loss 1.103533, current_train_items 246368.
I0302 19:01:58.785661 22447428132992 run.py:483] Algo bellman_ford step 7699 current loss 1.096824, current_train_items 246400.
I0302 19:01:58.805724 22447428132992 run.py:483] Algo bellman_ford step 7700 current loss 0.647970, current_train_items 246432.
I0302 19:01:58.813591 22447428132992 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.8916015625, 'score': 0.8916015625, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:01:58.813696 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.892, val scores are: bellman_ford: 0.892
I0302 19:01:58.830673 22447428132992 run.py:483] Algo bellman_ford step 7701 current loss 0.770227, current_train_items 246464.
I0302 19:01:58.854813 22447428132992 run.py:483] Algo bellman_ford step 7702 current loss 0.901225, current_train_items 246496.
I0302 19:01:58.885638 22447428132992 run.py:483] Algo bellman_ford step 7703 current loss 0.849712, current_train_items 246528.
I0302 19:01:58.918996 22447428132992 run.py:483] Algo bellman_ford step 7704 current loss 1.057836, current_train_items 246560.
I0302 19:01:58.939139 22447428132992 run.py:483] Algo bellman_ford step 7705 current loss 0.588947, current_train_items 246592.
I0302 19:01:58.955011 22447428132992 run.py:483] Algo bellman_ford step 7706 current loss 0.742029, current_train_items 246624.
I0302 19:01:58.980169 22447428132992 run.py:483] Algo bellman_ford step 7707 current loss 0.895318, current_train_items 246656.
I0302 19:01:59.010643 22447428132992 run.py:483] Algo bellman_ford step 7708 current loss 0.913921, current_train_items 246688.
I0302 19:01:59.043175 22447428132992 run.py:483] Algo bellman_ford step 7709 current loss 1.051109, current_train_items 246720.
I0302 19:01:59.062881 22447428132992 run.py:483] Algo bellman_ford step 7710 current loss 0.817849, current_train_items 246752.
I0302 19:01:59.079027 22447428132992 run.py:483] Algo bellman_ford step 7711 current loss 0.777799, current_train_items 246784.
I0302 19:01:59.102618 22447428132992 run.py:483] Algo bellman_ford step 7712 current loss 0.966839, current_train_items 246816.
I0302 19:01:59.133413 22447428132992 run.py:483] Algo bellman_ford step 7713 current loss 1.049817, current_train_items 246848.
I0302 19:01:59.166846 22447428132992 run.py:483] Algo bellman_ford step 7714 current loss 1.071202, current_train_items 246880.
I0302 19:01:59.186563 22447428132992 run.py:483] Algo bellman_ford step 7715 current loss 0.644343, current_train_items 246912.
I0302 19:01:59.202527 22447428132992 run.py:483] Algo bellman_ford step 7716 current loss 0.661981, current_train_items 246944.
I0302 19:01:59.226674 22447428132992 run.py:483] Algo bellman_ford step 7717 current loss 0.853759, current_train_items 246976.
I0302 19:01:59.257722 22447428132992 run.py:483] Algo bellman_ford step 7718 current loss 0.991742, current_train_items 247008.
I0302 19:01:59.290165 22447428132992 run.py:483] Algo bellman_ford step 7719 current loss 1.089777, current_train_items 247040.
I0302 19:01:59.310003 22447428132992 run.py:483] Algo bellman_ford step 7720 current loss 0.564238, current_train_items 247072.
I0302 19:01:59.325893 22447428132992 run.py:483] Algo bellman_ford step 7721 current loss 0.796748, current_train_items 247104.
I0302 19:01:59.350346 22447428132992 run.py:483] Algo bellman_ford step 7722 current loss 0.900926, current_train_items 247136.
I0302 19:01:59.382894 22447428132992 run.py:483] Algo bellman_ford step 7723 current loss 0.994827, current_train_items 247168.
I0302 19:01:59.417206 22447428132992 run.py:483] Algo bellman_ford step 7724 current loss 1.160007, current_train_items 247200.
I0302 19:01:59.437043 22447428132992 run.py:483] Algo bellman_ford step 7725 current loss 0.617928, current_train_items 247232.
I0302 19:01:59.453110 22447428132992 run.py:483] Algo bellman_ford step 7726 current loss 0.724269, current_train_items 247264.
I0302 19:01:59.475848 22447428132992 run.py:483] Algo bellman_ford step 7727 current loss 0.905082, current_train_items 247296.
I0302 19:01:59.506863 22447428132992 run.py:483] Algo bellman_ford step 7728 current loss 0.893389, current_train_items 247328.
I0302 19:01:59.539371 22447428132992 run.py:483] Algo bellman_ford step 7729 current loss 0.992054, current_train_items 247360.
I0302 19:01:59.559140 22447428132992 run.py:483] Algo bellman_ford step 7730 current loss 0.767534, current_train_items 247392.
I0302 19:01:59.575039 22447428132992 run.py:483] Algo bellman_ford step 7731 current loss 0.693636, current_train_items 247424.
I0302 19:01:59.598148 22447428132992 run.py:483] Algo bellman_ford step 7732 current loss 0.863676, current_train_items 247456.
I0302 19:01:59.629345 22447428132992 run.py:483] Algo bellman_ford step 7733 current loss 1.009231, current_train_items 247488.
I0302 19:01:59.661621 22447428132992 run.py:483] Algo bellman_ford step 7734 current loss 1.020025, current_train_items 247520.
I0302 19:01:59.681245 22447428132992 run.py:483] Algo bellman_ford step 7735 current loss 0.578541, current_train_items 247552.
I0302 19:01:59.697443 22447428132992 run.py:483] Algo bellman_ford step 7736 current loss 0.797870, current_train_items 247584.
I0302 19:01:59.721274 22447428132992 run.py:483] Algo bellman_ford step 7737 current loss 0.887120, current_train_items 247616.
I0302 19:01:59.752161 22447428132992 run.py:483] Algo bellman_ford step 7738 current loss 1.080535, current_train_items 247648.
I0302 19:01:59.786081 22447428132992 run.py:483] Algo bellman_ford step 7739 current loss 1.112027, current_train_items 247680.
I0302 19:01:59.805540 22447428132992 run.py:483] Algo bellman_ford step 7740 current loss 0.581952, current_train_items 247712.
I0302 19:01:59.821823 22447428132992 run.py:483] Algo bellman_ford step 7741 current loss 0.812144, current_train_items 247744.
I0302 19:01:59.847007 22447428132992 run.py:483] Algo bellman_ford step 7742 current loss 0.993621, current_train_items 247776.
I0302 19:01:59.879191 22447428132992 run.py:483] Algo bellman_ford step 7743 current loss 1.127005, current_train_items 247808.
I0302 19:01:59.912879 22447428132992 run.py:483] Algo bellman_ford step 7744 current loss 1.304282, current_train_items 247840.
I0302 19:01:59.932859 22447428132992 run.py:483] Algo bellman_ford step 7745 current loss 0.686008, current_train_items 247872.
I0302 19:01:59.948874 22447428132992 run.py:483] Algo bellman_ford step 7746 current loss 0.687288, current_train_items 247904.
I0302 19:01:59.972652 22447428132992 run.py:483] Algo bellman_ford step 7747 current loss 1.092452, current_train_items 247936.
I0302 19:02:00.005487 22447428132992 run.py:483] Algo bellman_ford step 7748 current loss 1.092871, current_train_items 247968.
I0302 19:02:00.040173 22447428132992 run.py:483] Algo bellman_ford step 7749 current loss 1.104550, current_train_items 248000.
I0302 19:02:00.060196 22447428132992 run.py:483] Algo bellman_ford step 7750 current loss 0.563184, current_train_items 248032.
I0302 19:02:00.068025 22447428132992 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:02:00.068129 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:02:00.085173 22447428132992 run.py:483] Algo bellman_ford step 7751 current loss 0.772804, current_train_items 248064.
I0302 19:02:00.109295 22447428132992 run.py:483] Algo bellman_ford step 7752 current loss 0.859248, current_train_items 248096.
I0302 19:02:00.141194 22447428132992 run.py:483] Algo bellman_ford step 7753 current loss 0.959423, current_train_items 248128.
I0302 19:02:00.176839 22447428132992 run.py:483] Algo bellman_ford step 7754 current loss 1.183572, current_train_items 248160.
I0302 19:02:00.197008 22447428132992 run.py:483] Algo bellman_ford step 7755 current loss 0.578781, current_train_items 248192.
I0302 19:02:00.212598 22447428132992 run.py:483] Algo bellman_ford step 7756 current loss 0.698319, current_train_items 248224.
I0302 19:02:00.236562 22447428132992 run.py:483] Algo bellman_ford step 7757 current loss 0.965019, current_train_items 248256.
I0302 19:02:00.266795 22447428132992 run.py:483] Algo bellman_ford step 7758 current loss 0.971447, current_train_items 248288.
I0302 19:02:00.300012 22447428132992 run.py:483] Algo bellman_ford step 7759 current loss 1.153239, current_train_items 248320.
I0302 19:02:00.320058 22447428132992 run.py:483] Algo bellman_ford step 7760 current loss 0.571015, current_train_items 248352.
I0302 19:02:00.336194 22447428132992 run.py:483] Algo bellman_ford step 7761 current loss 0.821039, current_train_items 248384.
I0302 19:02:00.360189 22447428132992 run.py:483] Algo bellman_ford step 7762 current loss 0.878804, current_train_items 248416.
I0302 19:02:00.391522 22447428132992 run.py:483] Algo bellman_ford step 7763 current loss 1.096861, current_train_items 248448.
I0302 19:02:00.423679 22447428132992 run.py:483] Algo bellman_ford step 7764 current loss 0.994988, current_train_items 248480.
I0302 19:02:00.443193 22447428132992 run.py:483] Algo bellman_ford step 7765 current loss 0.577159, current_train_items 248512.
I0302 19:02:00.459405 22447428132992 run.py:483] Algo bellman_ford step 7766 current loss 0.756190, current_train_items 248544.
I0302 19:02:00.482723 22447428132992 run.py:483] Algo bellman_ford step 7767 current loss 0.808169, current_train_items 248576.
I0302 19:02:00.514175 22447428132992 run.py:483] Algo bellman_ford step 7768 current loss 0.956891, current_train_items 248608.
I0302 19:02:00.547715 22447428132992 run.py:483] Algo bellman_ford step 7769 current loss 1.103819, current_train_items 248640.
I0302 19:02:00.567570 22447428132992 run.py:483] Algo bellman_ford step 7770 current loss 0.509643, current_train_items 248672.
I0302 19:02:00.583821 22447428132992 run.py:483] Algo bellman_ford step 7771 current loss 0.652668, current_train_items 248704.
I0302 19:02:00.607105 22447428132992 run.py:483] Algo bellman_ford step 7772 current loss 0.844633, current_train_items 248736.
I0302 19:02:00.638971 22447428132992 run.py:483] Algo bellman_ford step 7773 current loss 0.985273, current_train_items 248768.
I0302 19:02:00.671875 22447428132992 run.py:483] Algo bellman_ford step 7774 current loss 1.051006, current_train_items 248800.
I0302 19:02:00.691893 22447428132992 run.py:483] Algo bellman_ford step 7775 current loss 0.637006, current_train_items 248832.
I0302 19:02:00.708126 22447428132992 run.py:483] Algo bellman_ford step 7776 current loss 0.828869, current_train_items 248864.
I0302 19:02:00.732579 22447428132992 run.py:483] Algo bellman_ford step 7777 current loss 1.001866, current_train_items 248896.
I0302 19:02:00.764079 22447428132992 run.py:483] Algo bellman_ford step 7778 current loss 0.915867, current_train_items 248928.
I0302 19:02:00.799231 22447428132992 run.py:483] Algo bellman_ford step 7779 current loss 1.093495, current_train_items 248960.
I0302 19:02:00.818590 22447428132992 run.py:483] Algo bellman_ford step 7780 current loss 0.638559, current_train_items 248992.
I0302 19:02:00.834996 22447428132992 run.py:483] Algo bellman_ford step 7781 current loss 0.775554, current_train_items 249024.
I0302 19:02:00.858669 22447428132992 run.py:483] Algo bellman_ford step 7782 current loss 0.875292, current_train_items 249056.
I0302 19:02:00.890638 22447428132992 run.py:483] Algo bellman_ford step 7783 current loss 1.053374, current_train_items 249088.
I0302 19:02:00.924337 22447428132992 run.py:483] Algo bellman_ford step 7784 current loss 0.984695, current_train_items 249120.
I0302 19:02:00.944336 22447428132992 run.py:483] Algo bellman_ford step 7785 current loss 0.610559, current_train_items 249152.
I0302 19:02:00.960623 22447428132992 run.py:483] Algo bellman_ford step 7786 current loss 0.791408, current_train_items 249184.
I0302 19:02:00.983624 22447428132992 run.py:483] Algo bellman_ford step 7787 current loss 0.898247, current_train_items 249216.
I0302 19:02:01.014744 22447428132992 run.py:483] Algo bellman_ford step 7788 current loss 0.933628, current_train_items 249248.
I0302 19:02:01.045975 22447428132992 run.py:483] Algo bellman_ford step 7789 current loss 1.060380, current_train_items 249280.
I0302 19:02:01.065726 22447428132992 run.py:483] Algo bellman_ford step 7790 current loss 0.589235, current_train_items 249312.
I0302 19:02:01.081535 22447428132992 run.py:483] Algo bellman_ford step 7791 current loss 0.684209, current_train_items 249344.
I0302 19:02:01.105075 22447428132992 run.py:483] Algo bellman_ford step 7792 current loss 0.840740, current_train_items 249376.
I0302 19:02:01.135810 22447428132992 run.py:483] Algo bellman_ford step 7793 current loss 0.945951, current_train_items 249408.
I0302 19:02:01.168895 22447428132992 run.py:483] Algo bellman_ford step 7794 current loss 0.977412, current_train_items 249440.
I0302 19:02:01.188489 22447428132992 run.py:483] Algo bellman_ford step 7795 current loss 0.566388, current_train_items 249472.
I0302 19:02:01.204310 22447428132992 run.py:483] Algo bellman_ford step 7796 current loss 0.758958, current_train_items 249504.
I0302 19:02:01.227329 22447428132992 run.py:483] Algo bellman_ford step 7797 current loss 0.955043, current_train_items 249536.
I0302 19:02:01.257639 22447428132992 run.py:483] Algo bellman_ford step 7798 current loss 0.881832, current_train_items 249568.
I0302 19:02:01.292477 22447428132992 run.py:483] Algo bellman_ford step 7799 current loss 1.027112, current_train_items 249600.
I0302 19:02:01.312681 22447428132992 run.py:483] Algo bellman_ford step 7800 current loss 0.596952, current_train_items 249632.
I0302 19:02:01.320607 22447428132992 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:02:01.320711 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 19:02:01.337321 22447428132992 run.py:483] Algo bellman_ford step 7801 current loss 0.880562, current_train_items 249664.
I0302 19:02:01.361427 22447428132992 run.py:483] Algo bellman_ford step 7802 current loss 0.906620, current_train_items 249696.
I0302 19:02:01.393936 22447428132992 run.py:483] Algo bellman_ford step 7803 current loss 0.966761, current_train_items 249728.
I0302 19:02:01.430066 22447428132992 run.py:483] Algo bellman_ford step 7804 current loss 1.212197, current_train_items 249760.
I0302 19:02:01.450211 22447428132992 run.py:483] Algo bellman_ford step 7805 current loss 0.542453, current_train_items 249792.
I0302 19:02:01.466364 22447428132992 run.py:483] Algo bellman_ford step 7806 current loss 0.657797, current_train_items 249824.
I0302 19:02:01.489233 22447428132992 run.py:483] Algo bellman_ford step 7807 current loss 0.835205, current_train_items 249856.
I0302 19:02:01.519840 22447428132992 run.py:483] Algo bellman_ford step 7808 current loss 0.993365, current_train_items 249888.
I0302 19:02:01.553302 22447428132992 run.py:483] Algo bellman_ford step 7809 current loss 1.154122, current_train_items 249920.
I0302 19:02:01.573179 22447428132992 run.py:483] Algo bellman_ford step 7810 current loss 0.548355, current_train_items 249952.
I0302 19:02:01.589617 22447428132992 run.py:483] Algo bellman_ford step 7811 current loss 0.751159, current_train_items 249984.
I0302 19:02:01.613516 22447428132992 run.py:483] Algo bellman_ford step 7812 current loss 0.852564, current_train_items 250016.
I0302 19:02:01.645890 22447428132992 run.py:483] Algo bellman_ford step 7813 current loss 1.106060, current_train_items 250048.
I0302 19:02:01.679447 22447428132992 run.py:483] Algo bellman_ford step 7814 current loss 1.145099, current_train_items 250080.
I0302 19:02:01.698982 22447428132992 run.py:483] Algo bellman_ford step 7815 current loss 0.619293, current_train_items 250112.
I0302 19:02:01.714895 22447428132992 run.py:483] Algo bellman_ford step 7816 current loss 0.768336, current_train_items 250144.
I0302 19:02:01.738739 22447428132992 run.py:483] Algo bellman_ford step 7817 current loss 0.866007, current_train_items 250176.
I0302 19:02:01.768886 22447428132992 run.py:483] Algo bellman_ford step 7818 current loss 0.908311, current_train_items 250208.
I0302 19:02:01.801394 22447428132992 run.py:483] Algo bellman_ford step 7819 current loss 1.016810, current_train_items 250240.
I0302 19:02:01.820822 22447428132992 run.py:483] Algo bellman_ford step 7820 current loss 0.647117, current_train_items 250272.
I0302 19:02:01.837248 22447428132992 run.py:483] Algo bellman_ford step 7821 current loss 0.860272, current_train_items 250304.
I0302 19:02:01.862021 22447428132992 run.py:483] Algo bellman_ford step 7822 current loss 0.911237, current_train_items 250336.
I0302 19:02:01.894824 22447428132992 run.py:483] Algo bellman_ford step 7823 current loss 1.015846, current_train_items 250368.
I0302 19:02:01.927649 22447428132992 run.py:483] Algo bellman_ford step 7824 current loss 1.181158, current_train_items 250400.
I0302 19:02:01.947450 22447428132992 run.py:483] Algo bellman_ford step 7825 current loss 0.571507, current_train_items 250432.
I0302 19:02:01.963983 22447428132992 run.py:483] Algo bellman_ford step 7826 current loss 0.784639, current_train_items 250464.
I0302 19:02:01.988273 22447428132992 run.py:483] Algo bellman_ford step 7827 current loss 0.935750, current_train_items 250496.
I0302 19:02:02.019969 22447428132992 run.py:483] Algo bellman_ford step 7828 current loss 0.994381, current_train_items 250528.
I0302 19:02:02.053874 22447428132992 run.py:483] Algo bellman_ford step 7829 current loss 1.178963, current_train_items 250560.
I0302 19:02:02.073163 22447428132992 run.py:483] Algo bellman_ford step 7830 current loss 0.589089, current_train_items 250592.
I0302 19:02:02.089033 22447428132992 run.py:483] Algo bellman_ford step 7831 current loss 0.716524, current_train_items 250624.
I0302 19:02:02.112965 22447428132992 run.py:483] Algo bellman_ford step 7832 current loss 0.945415, current_train_items 250656.
I0302 19:02:02.145112 22447428132992 run.py:483] Algo bellman_ford step 7833 current loss 1.061886, current_train_items 250688.
I0302 19:02:02.176861 22447428132992 run.py:483] Algo bellman_ford step 7834 current loss 1.086851, current_train_items 250720.
I0302 19:02:02.196319 22447428132992 run.py:483] Algo bellman_ford step 7835 current loss 0.546282, current_train_items 250752.
I0302 19:02:02.212691 22447428132992 run.py:483] Algo bellman_ford step 7836 current loss 0.789978, current_train_items 250784.
I0302 19:02:02.236455 22447428132992 run.py:483] Algo bellman_ford step 7837 current loss 0.788449, current_train_items 250816.
I0302 19:02:02.268938 22447428132992 run.py:483] Algo bellman_ford step 7838 current loss 1.064415, current_train_items 250848.
I0302 19:02:02.303568 22447428132992 run.py:483] Algo bellman_ford step 7839 current loss 1.177749, current_train_items 250880.
I0302 19:02:02.323210 22447428132992 run.py:483] Algo bellman_ford step 7840 current loss 0.716707, current_train_items 250912.
I0302 19:02:02.339356 22447428132992 run.py:483] Algo bellman_ford step 7841 current loss 0.734379, current_train_items 250944.
I0302 19:02:02.363410 22447428132992 run.py:483] Algo bellman_ford step 7842 current loss 0.823008, current_train_items 250976.
I0302 19:02:02.396122 22447428132992 run.py:483] Algo bellman_ford step 7843 current loss 0.995600, current_train_items 251008.
I0302 19:02:02.430849 22447428132992 run.py:483] Algo bellman_ford step 7844 current loss 1.064910, current_train_items 251040.
I0302 19:02:02.450498 22447428132992 run.py:483] Algo bellman_ford step 7845 current loss 0.680473, current_train_items 251072.
I0302 19:02:02.466580 22447428132992 run.py:483] Algo bellman_ford step 7846 current loss 0.800955, current_train_items 251104.
I0302 19:02:02.490394 22447428132992 run.py:483] Algo bellman_ford step 7847 current loss 0.824520, current_train_items 251136.
I0302 19:02:02.522405 22447428132992 run.py:483] Algo bellman_ford step 7848 current loss 1.045198, current_train_items 251168.
I0302 19:02:02.556425 22447428132992 run.py:483] Algo bellman_ford step 7849 current loss 1.181653, current_train_items 251200.
I0302 19:02:02.575910 22447428132992 run.py:483] Algo bellman_ford step 7850 current loss 0.645623, current_train_items 251232.
I0302 19:02:02.583913 22447428132992 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.904296875, 'score': 0.904296875, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:02:02.584020 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.904, val scores are: bellman_ford: 0.904
I0302 19:02:02.600739 22447428132992 run.py:483] Algo bellman_ford step 7851 current loss 0.816246, current_train_items 251264.
I0302 19:02:02.624665 22447428132992 run.py:483] Algo bellman_ford step 7852 current loss 0.803426, current_train_items 251296.
I0302 19:02:02.657220 22447428132992 run.py:483] Algo bellman_ford step 7853 current loss 1.047255, current_train_items 251328.
I0302 19:02:02.693278 22447428132992 run.py:483] Algo bellman_ford step 7854 current loss 1.161384, current_train_items 251360.
I0302 19:02:02.713267 22447428132992 run.py:483] Algo bellman_ford step 7855 current loss 0.706226, current_train_items 251392.
I0302 19:02:02.729615 22447428132992 run.py:483] Algo bellman_ford step 7856 current loss 0.957416, current_train_items 251424.
I0302 19:02:02.753810 22447428132992 run.py:483] Algo bellman_ford step 7857 current loss 0.921010, current_train_items 251456.
I0302 19:02:02.785351 22447428132992 run.py:483] Algo bellman_ford step 7858 current loss 0.915183, current_train_items 251488.
I0302 19:02:02.820400 22447428132992 run.py:483] Algo bellman_ford step 7859 current loss 1.144822, current_train_items 251520.
I0302 19:02:02.840549 22447428132992 run.py:483] Algo bellman_ford step 7860 current loss 0.604633, current_train_items 251552.
I0302 19:02:02.856479 22447428132992 run.py:483] Algo bellman_ford step 7861 current loss 0.651972, current_train_items 251584.
I0302 19:02:02.879897 22447428132992 run.py:483] Algo bellman_ford step 7862 current loss 0.949628, current_train_items 251616.
I0302 19:02:02.911392 22447428132992 run.py:483] Algo bellman_ford step 7863 current loss 1.001585, current_train_items 251648.
I0302 19:02:02.943655 22447428132992 run.py:483] Algo bellman_ford step 7864 current loss 1.031720, current_train_items 251680.
I0302 19:02:02.963609 22447428132992 run.py:483] Algo bellman_ford step 7865 current loss 0.735779, current_train_items 251712.
I0302 19:02:02.980116 22447428132992 run.py:483] Algo bellman_ford step 7866 current loss 0.873875, current_train_items 251744.
I0302 19:02:03.004552 22447428132992 run.py:483] Algo bellman_ford step 7867 current loss 0.865260, current_train_items 251776.
I0302 19:02:03.035466 22447428132992 run.py:483] Algo bellman_ford step 7868 current loss 1.032331, current_train_items 251808.
I0302 19:02:03.068066 22447428132992 run.py:483] Algo bellman_ford step 7869 current loss 1.106737, current_train_items 251840.
I0302 19:02:03.088138 22447428132992 run.py:483] Algo bellman_ford step 7870 current loss 0.675489, current_train_items 251872.
I0302 19:02:03.104372 22447428132992 run.py:483] Algo bellman_ford step 7871 current loss 0.714202, current_train_items 251904.
I0302 19:02:03.128314 22447428132992 run.py:483] Algo bellman_ford step 7872 current loss 0.968113, current_train_items 251936.
I0302 19:02:03.159249 22447428132992 run.py:483] Algo bellman_ford step 7873 current loss 1.022014, current_train_items 251968.
I0302 19:02:03.192601 22447428132992 run.py:483] Algo bellman_ford step 7874 current loss 1.116040, current_train_items 252000.
I0302 19:02:03.212561 22447428132992 run.py:483] Algo bellman_ford step 7875 current loss 0.587230, current_train_items 252032.
I0302 19:02:03.228353 22447428132992 run.py:483] Algo bellman_ford step 7876 current loss 0.650151, current_train_items 252064.
I0302 19:02:03.251895 22447428132992 run.py:483] Algo bellman_ford step 7877 current loss 0.913016, current_train_items 252096.
I0302 19:02:03.283215 22447428132992 run.py:483] Algo bellman_ford step 7878 current loss 1.001340, current_train_items 252128.
I0302 19:02:03.317029 22447428132992 run.py:483] Algo bellman_ford step 7879 current loss 1.043728, current_train_items 252160.
I0302 19:02:03.336486 22447428132992 run.py:483] Algo bellman_ford step 7880 current loss 0.606536, current_train_items 252192.
I0302 19:02:03.352245 22447428132992 run.py:483] Algo bellman_ford step 7881 current loss 0.734969, current_train_items 252224.
I0302 19:02:03.375902 22447428132992 run.py:483] Algo bellman_ford step 7882 current loss 0.921392, current_train_items 252256.
I0302 19:02:03.407719 22447428132992 run.py:483] Algo bellman_ford step 7883 current loss 0.917428, current_train_items 252288.
I0302 19:02:03.441617 22447428132992 run.py:483] Algo bellman_ford step 7884 current loss 1.167415, current_train_items 252320.
I0302 19:02:03.461920 22447428132992 run.py:483] Algo bellman_ford step 7885 current loss 0.873217, current_train_items 252352.
I0302 19:02:03.478122 22447428132992 run.py:483] Algo bellman_ford step 7886 current loss 0.883323, current_train_items 252384.
I0302 19:02:03.500838 22447428132992 run.py:483] Algo bellman_ford step 7887 current loss 0.783617, current_train_items 252416.
I0302 19:02:03.530204 22447428132992 run.py:483] Algo bellman_ford step 7888 current loss 0.974116, current_train_items 252448.
I0302 19:02:03.563395 22447428132992 run.py:483] Algo bellman_ford step 7889 current loss 0.967489, current_train_items 252480.
I0302 19:02:03.583399 22447428132992 run.py:483] Algo bellman_ford step 7890 current loss 0.594637, current_train_items 252512.
I0302 19:02:03.599419 22447428132992 run.py:483] Algo bellman_ford step 7891 current loss 0.814524, current_train_items 252544.
I0302 19:02:03.623093 22447428132992 run.py:483] Algo bellman_ford step 7892 current loss 0.900810, current_train_items 252576.
I0302 19:02:03.654701 22447428132992 run.py:483] Algo bellman_ford step 7893 current loss 1.026932, current_train_items 252608.
I0302 19:02:03.687579 22447428132992 run.py:483] Algo bellman_ford step 7894 current loss 1.002806, current_train_items 252640.
I0302 19:02:03.707069 22447428132992 run.py:483] Algo bellman_ford step 7895 current loss 0.526823, current_train_items 252672.
I0302 19:02:03.722901 22447428132992 run.py:483] Algo bellman_ford step 7896 current loss 0.795996, current_train_items 252704.
I0302 19:02:03.746933 22447428132992 run.py:483] Algo bellman_ford step 7897 current loss 0.941398, current_train_items 252736.
I0302 19:02:03.778971 22447428132992 run.py:483] Algo bellman_ford step 7898 current loss 1.028743, current_train_items 252768.
I0302 19:02:03.812850 22447428132992 run.py:483] Algo bellman_ford step 7899 current loss 1.212661, current_train_items 252800.
I0302 19:02:03.832818 22447428132992 run.py:483] Algo bellman_ford step 7900 current loss 0.531611, current_train_items 252832.
I0302 19:02:03.840636 22447428132992 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:02:03.840742 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:03.857291 22447428132992 run.py:483] Algo bellman_ford step 7901 current loss 0.719924, current_train_items 252864.
I0302 19:02:03.881565 22447428132992 run.py:483] Algo bellman_ford step 7902 current loss 0.927943, current_train_items 252896.
I0302 19:02:03.913825 22447428132992 run.py:483] Algo bellman_ford step 7903 current loss 0.981016, current_train_items 252928.
I0302 19:02:03.950057 22447428132992 run.py:483] Algo bellman_ford step 7904 current loss 1.237310, current_train_items 252960.
I0302 19:02:03.969759 22447428132992 run.py:483] Algo bellman_ford step 7905 current loss 0.601087, current_train_items 252992.
I0302 19:02:03.986222 22447428132992 run.py:483] Algo bellman_ford step 7906 current loss 0.824464, current_train_items 253024.
I0302 19:02:04.010421 22447428132992 run.py:483] Algo bellman_ford step 7907 current loss 0.873581, current_train_items 253056.
I0302 19:02:04.041061 22447428132992 run.py:483] Algo bellman_ford step 7908 current loss 0.995415, current_train_items 253088.
I0302 19:02:04.072124 22447428132992 run.py:483] Algo bellman_ford step 7909 current loss 0.978534, current_train_items 253120.
I0302 19:02:04.091497 22447428132992 run.py:483] Algo bellman_ford step 7910 current loss 0.676504, current_train_items 253152.
I0302 19:02:04.108203 22447428132992 run.py:483] Algo bellman_ford step 7911 current loss 0.798864, current_train_items 253184.
I0302 19:02:04.132140 22447428132992 run.py:483] Algo bellman_ford step 7912 current loss 0.995441, current_train_items 253216.
I0302 19:02:04.163559 22447428132992 run.py:483] Algo bellman_ford step 7913 current loss 0.894989, current_train_items 253248.
I0302 19:02:04.197178 22447428132992 run.py:483] Algo bellman_ford step 7914 current loss 1.091054, current_train_items 253280.
I0302 19:02:04.216702 22447428132992 run.py:483] Algo bellman_ford step 7915 current loss 0.597058, current_train_items 253312.
I0302 19:02:04.233008 22447428132992 run.py:483] Algo bellman_ford step 7916 current loss 0.765547, current_train_items 253344.
I0302 19:02:04.256536 22447428132992 run.py:483] Algo bellman_ford step 7917 current loss 0.942845, current_train_items 253376.
I0302 19:02:04.288128 22447428132992 run.py:483] Algo bellman_ford step 7918 current loss 0.928561, current_train_items 253408.
I0302 19:02:04.323274 22447428132992 run.py:483] Algo bellman_ford step 7919 current loss 1.114771, current_train_items 253440.
I0302 19:02:04.342729 22447428132992 run.py:483] Algo bellman_ford step 7920 current loss 0.555888, current_train_items 253472.
I0302 19:02:04.359070 22447428132992 run.py:483] Algo bellman_ford step 7921 current loss 0.734771, current_train_items 253504.
I0302 19:02:04.382835 22447428132992 run.py:483] Algo bellman_ford step 7922 current loss 1.059434, current_train_items 253536.
I0302 19:02:04.413530 22447428132992 run.py:483] Algo bellman_ford step 7923 current loss 0.908439, current_train_items 253568.
I0302 19:02:04.447212 22447428132992 run.py:483] Algo bellman_ford step 7924 current loss 1.214029, current_train_items 253600.
I0302 19:02:04.466290 22447428132992 run.py:483] Algo bellman_ford step 7925 current loss 0.596099, current_train_items 253632.
I0302 19:02:04.482575 22447428132992 run.py:483] Algo bellman_ford step 7926 current loss 0.749282, current_train_items 253664.
I0302 19:02:04.506047 22447428132992 run.py:483] Algo bellman_ford step 7927 current loss 0.982814, current_train_items 253696.
I0302 19:02:04.538393 22447428132992 run.py:483] Algo bellman_ford step 7928 current loss 1.016949, current_train_items 253728.
I0302 19:02:04.570938 22447428132992 run.py:483] Algo bellman_ford step 7929 current loss 1.017374, current_train_items 253760.
I0302 19:02:04.590703 22447428132992 run.py:483] Algo bellman_ford step 7930 current loss 0.651465, current_train_items 253792.
I0302 19:02:04.606874 22447428132992 run.py:483] Algo bellman_ford step 7931 current loss 0.716285, current_train_items 253824.
I0302 19:02:04.629482 22447428132992 run.py:483] Algo bellman_ford step 7932 current loss 0.789407, current_train_items 253856.
I0302 19:02:04.661327 22447428132992 run.py:483] Algo bellman_ford step 7933 current loss 1.136612, current_train_items 253888.
I0302 19:02:04.695160 22447428132992 run.py:483] Algo bellman_ford step 7934 current loss 1.113742, current_train_items 253920.
I0302 19:02:04.714422 22447428132992 run.py:483] Algo bellman_ford step 7935 current loss 0.571556, current_train_items 253952.
I0302 19:02:04.730256 22447428132992 run.py:483] Algo bellman_ford step 7936 current loss 0.704899, current_train_items 253984.
I0302 19:02:04.754288 22447428132992 run.py:483] Algo bellman_ford step 7937 current loss 0.800240, current_train_items 254016.
I0302 19:02:04.785001 22447428132992 run.py:483] Algo bellman_ford step 7938 current loss 0.932163, current_train_items 254048.
I0302 19:02:04.819143 22447428132992 run.py:483] Algo bellman_ford step 7939 current loss 1.258039, current_train_items 254080.
I0302 19:02:04.838700 22447428132992 run.py:483] Algo bellman_ford step 7940 current loss 0.572307, current_train_items 254112.
I0302 19:02:04.854297 22447428132992 run.py:483] Algo bellman_ford step 7941 current loss 0.771489, current_train_items 254144.
I0302 19:02:04.878563 22447428132992 run.py:483] Algo bellman_ford step 7942 current loss 0.935393, current_train_items 254176.
I0302 19:02:04.911295 22447428132992 run.py:483] Algo bellman_ford step 7943 current loss 1.033547, current_train_items 254208.
I0302 19:02:04.944150 22447428132992 run.py:483] Algo bellman_ford step 7944 current loss 1.382185, current_train_items 254240.
I0302 19:02:04.963437 22447428132992 run.py:483] Algo bellman_ford step 7945 current loss 0.524668, current_train_items 254272.
I0302 19:02:04.979063 22447428132992 run.py:483] Algo bellman_ford step 7946 current loss 0.726409, current_train_items 254304.
I0302 19:02:05.001649 22447428132992 run.py:483] Algo bellman_ford step 7947 current loss 0.954555, current_train_items 254336.
I0302 19:02:05.031340 22447428132992 run.py:483] Algo bellman_ford step 7948 current loss 0.927125, current_train_items 254368.
I0302 19:02:05.065269 22447428132992 run.py:483] Algo bellman_ford step 7949 current loss 1.167665, current_train_items 254400.
I0302 19:02:05.084525 22447428132992 run.py:483] Algo bellman_ford step 7950 current loss 0.568092, current_train_items 254432.
I0302 19:02:05.092635 22447428132992 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:02:05.092740 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 19:02:05.109308 22447428132992 run.py:483] Algo bellman_ford step 7951 current loss 0.658585, current_train_items 254464.
I0302 19:02:05.133461 22447428132992 run.py:483] Algo bellman_ford step 7952 current loss 0.844920, current_train_items 254496.
I0302 19:02:05.165688 22447428132992 run.py:483] Algo bellman_ford step 7953 current loss 1.077721, current_train_items 254528.
I0302 19:02:05.199088 22447428132992 run.py:483] Algo bellman_ford step 7954 current loss 0.994589, current_train_items 254560.
I0302 19:02:05.218855 22447428132992 run.py:483] Algo bellman_ford step 7955 current loss 0.600078, current_train_items 254592.
I0302 19:02:05.234760 22447428132992 run.py:483] Algo bellman_ford step 7956 current loss 0.822937, current_train_items 254624.
I0302 19:02:05.258330 22447428132992 run.py:483] Algo bellman_ford step 7957 current loss 0.781530, current_train_items 254656.
I0302 19:02:05.290153 22447428132992 run.py:483] Algo bellman_ford step 7958 current loss 1.031381, current_train_items 254688.
I0302 19:02:05.323464 22447428132992 run.py:483] Algo bellman_ford step 7959 current loss 0.950237, current_train_items 254720.
I0302 19:02:05.343185 22447428132992 run.py:483] Algo bellman_ford step 7960 current loss 0.643481, current_train_items 254752.
I0302 19:02:05.359749 22447428132992 run.py:483] Algo bellman_ford step 7961 current loss 0.755768, current_train_items 254784.
I0302 19:02:05.382916 22447428132992 run.py:483] Algo bellman_ford step 7962 current loss 0.922570, current_train_items 254816.
I0302 19:02:05.413547 22447428132992 run.py:483] Algo bellman_ford step 7963 current loss 0.986695, current_train_items 254848.
I0302 19:02:05.447339 22447428132992 run.py:483] Algo bellman_ford step 7964 current loss 1.098830, current_train_items 254880.
I0302 19:02:05.466668 22447428132992 run.py:483] Algo bellman_ford step 7965 current loss 0.616080, current_train_items 254912.
I0302 19:02:05.483064 22447428132992 run.py:483] Algo bellman_ford step 7966 current loss 0.748016, current_train_items 254944.
I0302 19:02:05.507479 22447428132992 run.py:483] Algo bellman_ford step 7967 current loss 1.098381, current_train_items 254976.
I0302 19:02:05.538521 22447428132992 run.py:483] Algo bellman_ford step 7968 current loss 0.961748, current_train_items 255008.
I0302 19:02:05.570522 22447428132992 run.py:483] Algo bellman_ford step 7969 current loss 1.079374, current_train_items 255040.
I0302 19:02:05.590046 22447428132992 run.py:483] Algo bellman_ford step 7970 current loss 0.577386, current_train_items 255072.
I0302 19:02:05.606124 22447428132992 run.py:483] Algo bellman_ford step 7971 current loss 0.812666, current_train_items 255104.
I0302 19:02:05.629936 22447428132992 run.py:483] Algo bellman_ford step 7972 current loss 0.989764, current_train_items 255136.
I0302 19:02:05.660881 22447428132992 run.py:483] Algo bellman_ford step 7973 current loss 0.929001, current_train_items 255168.
I0302 19:02:05.695408 22447428132992 run.py:483] Algo bellman_ford step 7974 current loss 1.248700, current_train_items 255200.
I0302 19:02:05.715190 22447428132992 run.py:483] Algo bellman_ford step 7975 current loss 0.586785, current_train_items 255232.
I0302 19:02:05.731570 22447428132992 run.py:483] Algo bellman_ford step 7976 current loss 0.800641, current_train_items 255264.
I0302 19:02:05.755192 22447428132992 run.py:483] Algo bellman_ford step 7977 current loss 0.935166, current_train_items 255296.
I0302 19:02:05.786529 22447428132992 run.py:483] Algo bellman_ford step 7978 current loss 1.142532, current_train_items 255328.
I0302 19:02:05.817932 22447428132992 run.py:483] Algo bellman_ford step 7979 current loss 1.104566, current_train_items 255360.
I0302 19:02:05.837340 22447428132992 run.py:483] Algo bellman_ford step 7980 current loss 0.612437, current_train_items 255392.
I0302 19:02:05.853352 22447428132992 run.py:483] Algo bellman_ford step 7981 current loss 0.735644, current_train_items 255424.
I0302 19:02:05.876065 22447428132992 run.py:483] Algo bellman_ford step 7982 current loss 0.848374, current_train_items 255456.
I0302 19:02:05.906635 22447428132992 run.py:483] Algo bellman_ford step 7983 current loss 1.075269, current_train_items 255488.
I0302 19:02:05.940618 22447428132992 run.py:483] Algo bellman_ford step 7984 current loss 1.113858, current_train_items 255520.
I0302 19:02:05.960247 22447428132992 run.py:483] Algo bellman_ford step 7985 current loss 0.588281, current_train_items 255552.
I0302 19:02:05.976633 22447428132992 run.py:483] Algo bellman_ford step 7986 current loss 0.797330, current_train_items 255584.
I0302 19:02:05.999261 22447428132992 run.py:483] Algo bellman_ford step 7987 current loss 0.854657, current_train_items 255616.
I0302 19:02:06.031020 22447428132992 run.py:483] Algo bellman_ford step 7988 current loss 1.005524, current_train_items 255648.
I0302 19:02:06.063355 22447428132992 run.py:483] Algo bellman_ford step 7989 current loss 1.180352, current_train_items 255680.
I0302 19:02:06.082943 22447428132992 run.py:483] Algo bellman_ford step 7990 current loss 0.553634, current_train_items 255712.
I0302 19:02:06.099189 22447428132992 run.py:483] Algo bellman_ford step 7991 current loss 0.810503, current_train_items 255744.
I0302 19:02:06.122689 22447428132992 run.py:483] Algo bellman_ford step 7992 current loss 0.880893, current_train_items 255776.
I0302 19:02:06.155491 22447428132992 run.py:483] Algo bellman_ford step 7993 current loss 1.133295, current_train_items 255808.
I0302 19:02:06.189917 22447428132992 run.py:483] Algo bellman_ford step 7994 current loss 1.209723, current_train_items 255840.
I0302 19:02:06.209255 22447428132992 run.py:483] Algo bellman_ford step 7995 current loss 0.522026, current_train_items 255872.
I0302 19:02:06.224918 22447428132992 run.py:483] Algo bellman_ford step 7996 current loss 0.708835, current_train_items 255904.
I0302 19:02:06.248258 22447428132992 run.py:483] Algo bellman_ford step 7997 current loss 0.933617, current_train_items 255936.
I0302 19:02:06.279217 22447428132992 run.py:483] Algo bellman_ford step 7998 current loss 0.918586, current_train_items 255968.
I0302 19:02:06.311037 22447428132992 run.py:483] Algo bellman_ford step 7999 current loss 1.113080, current_train_items 256000.
I0302 19:02:06.330732 22447428132992 run.py:483] Algo bellman_ford step 8000 current loss 0.544658, current_train_items 256032.
I0302 19:02:06.338688 22447428132992 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:02:06.338794 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:02:06.355658 22447428132992 run.py:483] Algo bellman_ford step 8001 current loss 0.696014, current_train_items 256064.
I0302 19:02:06.379312 22447428132992 run.py:483] Algo bellman_ford step 8002 current loss 0.853617, current_train_items 256096.
I0302 19:02:06.411725 22447428132992 run.py:483] Algo bellman_ford step 8003 current loss 0.969850, current_train_items 256128.
I0302 19:02:06.444456 22447428132992 run.py:483] Algo bellman_ford step 8004 current loss 0.948140, current_train_items 256160.
I0302 19:02:06.464468 22447428132992 run.py:483] Algo bellman_ford step 8005 current loss 0.915999, current_train_items 256192.
I0302 19:02:06.480393 22447428132992 run.py:483] Algo bellman_ford step 8006 current loss 0.807696, current_train_items 256224.
I0302 19:02:06.504369 22447428132992 run.py:483] Algo bellman_ford step 8007 current loss 0.931568, current_train_items 256256.
I0302 19:02:06.536911 22447428132992 run.py:483] Algo bellman_ford step 8008 current loss 0.939748, current_train_items 256288.
I0302 19:02:06.570692 22447428132992 run.py:483] Algo bellman_ford step 8009 current loss 1.107267, current_train_items 256320.
I0302 19:02:06.590440 22447428132992 run.py:483] Algo bellman_ford step 8010 current loss 1.183857, current_train_items 256352.
I0302 19:02:06.606309 22447428132992 run.py:483] Algo bellman_ford step 8011 current loss 0.815037, current_train_items 256384.
I0302 19:02:06.630555 22447428132992 run.py:483] Algo bellman_ford step 8012 current loss 0.884261, current_train_items 256416.
I0302 19:02:06.661438 22447428132992 run.py:483] Algo bellman_ford step 8013 current loss 1.030163, current_train_items 256448.
I0302 19:02:06.692792 22447428132992 run.py:483] Algo bellman_ford step 8014 current loss 0.990217, current_train_items 256480.
I0302 19:02:06.712634 22447428132992 run.py:483] Algo bellman_ford step 8015 current loss 0.616756, current_train_items 256512.
I0302 19:02:06.729270 22447428132992 run.py:483] Algo bellman_ford step 8016 current loss 0.700729, current_train_items 256544.
I0302 19:02:06.752442 22447428132992 run.py:483] Algo bellman_ford step 8017 current loss 0.831629, current_train_items 256576.
I0302 19:02:06.783628 22447428132992 run.py:483] Algo bellman_ford step 8018 current loss 0.869083, current_train_items 256608.
I0302 19:02:06.815556 22447428132992 run.py:483] Algo bellman_ford step 8019 current loss 1.114954, current_train_items 256640.
I0302 19:02:06.834887 22447428132992 run.py:483] Algo bellman_ford step 8020 current loss 0.627331, current_train_items 256672.
I0302 19:02:06.851040 22447428132992 run.py:483] Algo bellman_ford step 8021 current loss 0.727206, current_train_items 256704.
I0302 19:02:06.874454 22447428132992 run.py:483] Algo bellman_ford step 8022 current loss 0.895292, current_train_items 256736.
I0302 19:02:06.905036 22447428132992 run.py:483] Algo bellman_ford step 8023 current loss 0.977642, current_train_items 256768.
I0302 19:02:06.940336 22447428132992 run.py:483] Algo bellman_ford step 8024 current loss 1.210393, current_train_items 256800.
I0302 19:02:06.959966 22447428132992 run.py:483] Algo bellman_ford step 8025 current loss 0.619338, current_train_items 256832.
I0302 19:02:06.976435 22447428132992 run.py:483] Algo bellman_ford step 8026 current loss 0.779605, current_train_items 256864.
I0302 19:02:07.000401 22447428132992 run.py:483] Algo bellman_ford step 8027 current loss 1.022859, current_train_items 256896.
I0302 19:02:07.030577 22447428132992 run.py:483] Algo bellman_ford step 8028 current loss 0.956983, current_train_items 256928.
I0302 19:02:07.064244 22447428132992 run.py:483] Algo bellman_ford step 8029 current loss 1.022039, current_train_items 256960.
I0302 19:02:07.083592 22447428132992 run.py:483] Algo bellman_ford step 8030 current loss 0.580890, current_train_items 256992.
I0302 19:02:07.099663 22447428132992 run.py:483] Algo bellman_ford step 8031 current loss 0.721514, current_train_items 257024.
I0302 19:02:07.123975 22447428132992 run.py:483] Algo bellman_ford step 8032 current loss 0.992862, current_train_items 257056.
I0302 19:02:07.155933 22447428132992 run.py:483] Algo bellman_ford step 8033 current loss 0.949334, current_train_items 257088.
I0302 19:02:07.190328 22447428132992 run.py:483] Algo bellman_ford step 8034 current loss 1.044939, current_train_items 257120.
I0302 19:02:07.209968 22447428132992 run.py:483] Algo bellman_ford step 8035 current loss 0.638748, current_train_items 257152.
I0302 19:02:07.226590 22447428132992 run.py:483] Algo bellman_ford step 8036 current loss 0.727850, current_train_items 257184.
I0302 19:02:07.250599 22447428132992 run.py:483] Algo bellman_ford step 8037 current loss 0.884703, current_train_items 257216.
I0302 19:02:07.281664 22447428132992 run.py:483] Algo bellman_ford step 8038 current loss 0.948717, current_train_items 257248.
I0302 19:02:07.316386 22447428132992 run.py:483] Algo bellman_ford step 8039 current loss 1.152717, current_train_items 257280.
I0302 19:02:07.336067 22447428132992 run.py:483] Algo bellman_ford step 8040 current loss 0.768452, current_train_items 257312.
I0302 19:02:07.352481 22447428132992 run.py:483] Algo bellman_ford step 8041 current loss 0.808726, current_train_items 257344.
I0302 19:02:07.376358 22447428132992 run.py:483] Algo bellman_ford step 8042 current loss 0.960312, current_train_items 257376.
I0302 19:02:07.408161 22447428132992 run.py:483] Algo bellman_ford step 8043 current loss 1.002314, current_train_items 257408.
I0302 19:02:07.442204 22447428132992 run.py:483] Algo bellman_ford step 8044 current loss 1.156434, current_train_items 257440.
I0302 19:02:07.462277 22447428132992 run.py:483] Algo bellman_ford step 8045 current loss 0.747480, current_train_items 257472.
I0302 19:02:07.478988 22447428132992 run.py:483] Algo bellman_ford step 8046 current loss 0.870970, current_train_items 257504.
I0302 19:02:07.502154 22447428132992 run.py:483] Algo bellman_ford step 8047 current loss 0.877726, current_train_items 257536.
I0302 19:02:07.533671 22447428132992 run.py:483] Algo bellman_ford step 8048 current loss 0.898699, current_train_items 257568.
I0302 19:02:07.565778 22447428132992 run.py:483] Algo bellman_ford step 8049 current loss 1.046666, current_train_items 257600.
I0302 19:02:07.585366 22447428132992 run.py:483] Algo bellman_ford step 8050 current loss 0.625648, current_train_items 257632.
I0302 19:02:07.593478 22447428132992 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:02:07.593585 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:02:07.610368 22447428132992 run.py:483] Algo bellman_ford step 8051 current loss 0.854045, current_train_items 257664.
I0302 19:02:07.634076 22447428132992 run.py:483] Algo bellman_ford step 8052 current loss 0.842559, current_train_items 257696.
I0302 19:02:07.665345 22447428132992 run.py:483] Algo bellman_ford step 8053 current loss 0.949625, current_train_items 257728.
I0302 19:02:07.700400 22447428132992 run.py:483] Algo bellman_ford step 8054 current loss 1.217777, current_train_items 257760.
I0302 19:02:07.720346 22447428132992 run.py:483] Algo bellman_ford step 8055 current loss 0.658423, current_train_items 257792.
I0302 19:02:07.735493 22447428132992 run.py:483] Algo bellman_ford step 8056 current loss 0.719938, current_train_items 257824.
I0302 19:02:07.757723 22447428132992 run.py:483] Algo bellman_ford step 8057 current loss 0.803143, current_train_items 257856.
I0302 19:02:07.790089 22447428132992 run.py:483] Algo bellman_ford step 8058 current loss 1.022200, current_train_items 257888.
I0302 19:02:07.825345 22447428132992 run.py:483] Algo bellman_ford step 8059 current loss 1.165714, current_train_items 257920.
I0302 19:02:07.844976 22447428132992 run.py:483] Algo bellman_ford step 8060 current loss 0.572745, current_train_items 257952.
I0302 19:02:07.860848 22447428132992 run.py:483] Algo bellman_ford step 8061 current loss 0.775161, current_train_items 257984.
I0302 19:02:07.885196 22447428132992 run.py:483] Algo bellman_ford step 8062 current loss 0.908539, current_train_items 258016.
I0302 19:02:07.916877 22447428132992 run.py:483] Algo bellman_ford step 8063 current loss 0.962622, current_train_items 258048.
I0302 19:02:07.949040 22447428132992 run.py:483] Algo bellman_ford step 8064 current loss 1.123570, current_train_items 258080.
I0302 19:02:07.968405 22447428132992 run.py:483] Algo bellman_ford step 8065 current loss 0.626325, current_train_items 258112.
I0302 19:02:07.984347 22447428132992 run.py:483] Algo bellman_ford step 8066 current loss 0.943698, current_train_items 258144.
I0302 19:02:08.007827 22447428132992 run.py:483] Algo bellman_ford step 8067 current loss 0.950674, current_train_items 258176.
I0302 19:02:08.038917 22447428132992 run.py:483] Algo bellman_ford step 8068 current loss 0.926618, current_train_items 258208.
I0302 19:02:08.070977 22447428132992 run.py:483] Algo bellman_ford step 8069 current loss 1.081664, current_train_items 258240.
I0302 19:02:08.090648 22447428132992 run.py:483] Algo bellman_ford step 8070 current loss 0.586837, current_train_items 258272.
I0302 19:02:08.107584 22447428132992 run.py:483] Algo bellman_ford step 8071 current loss 0.814923, current_train_items 258304.
I0302 19:02:08.130993 22447428132992 run.py:483] Algo bellman_ford step 8072 current loss 0.967386, current_train_items 258336.
I0302 19:02:08.162585 22447428132992 run.py:483] Algo bellman_ford step 8073 current loss 0.991203, current_train_items 258368.
I0302 19:02:08.196740 22447428132992 run.py:483] Algo bellman_ford step 8074 current loss 1.152228, current_train_items 258400.
I0302 19:02:08.216434 22447428132992 run.py:483] Algo bellman_ford step 8075 current loss 0.541977, current_train_items 258432.
I0302 19:02:08.232535 22447428132992 run.py:483] Algo bellman_ford step 8076 current loss 0.744006, current_train_items 258464.
I0302 19:02:08.255418 22447428132992 run.py:483] Algo bellman_ford step 8077 current loss 0.808895, current_train_items 258496.
I0302 19:02:08.286053 22447428132992 run.py:483] Algo bellman_ford step 8078 current loss 1.096209, current_train_items 258528.
I0302 19:02:08.317512 22447428132992 run.py:483] Algo bellman_ford step 8079 current loss 1.034618, current_train_items 258560.
I0302 19:02:08.336883 22447428132992 run.py:483] Algo bellman_ford step 8080 current loss 0.709930, current_train_items 258592.
I0302 19:02:08.353097 22447428132992 run.py:483] Algo bellman_ford step 8081 current loss 0.723478, current_train_items 258624.
I0302 19:02:08.376287 22447428132992 run.py:483] Algo bellman_ford step 8082 current loss 0.889883, current_train_items 258656.
I0302 19:02:08.407668 22447428132992 run.py:483] Algo bellman_ford step 8083 current loss 1.078863, current_train_items 258688.
I0302 19:02:08.443765 22447428132992 run.py:483] Algo bellman_ford step 8084 current loss 1.226435, current_train_items 258720.
I0302 19:02:08.463337 22447428132992 run.py:483] Algo bellman_ford step 8085 current loss 0.580420, current_train_items 258752.
I0302 19:02:08.479973 22447428132992 run.py:483] Algo bellman_ford step 8086 current loss 0.830681, current_train_items 258784.
I0302 19:02:08.502662 22447428132992 run.py:483] Algo bellman_ford step 8087 current loss 0.910970, current_train_items 258816.
I0302 19:02:08.535171 22447428132992 run.py:483] Algo bellman_ford step 8088 current loss 0.955839, current_train_items 258848.
I0302 19:02:08.567496 22447428132992 run.py:483] Algo bellman_ford step 8089 current loss 1.023362, current_train_items 258880.
I0302 19:02:08.587264 22447428132992 run.py:483] Algo bellman_ford step 8090 current loss 0.722521, current_train_items 258912.
I0302 19:02:08.602883 22447428132992 run.py:483] Algo bellman_ford step 8091 current loss 0.711491, current_train_items 258944.
I0302 19:02:08.626660 22447428132992 run.py:483] Algo bellman_ford step 8092 current loss 0.914407, current_train_items 258976.
I0302 19:02:08.657247 22447428132992 run.py:483] Algo bellman_ford step 8093 current loss 0.856116, current_train_items 259008.
I0302 19:02:08.691008 22447428132992 run.py:483] Algo bellman_ford step 8094 current loss 1.071810, current_train_items 259040.
I0302 19:02:08.711088 22447428132992 run.py:483] Algo bellman_ford step 8095 current loss 0.914559, current_train_items 259072.
I0302 19:02:08.727189 22447428132992 run.py:483] Algo bellman_ford step 8096 current loss 0.721313, current_train_items 259104.
I0302 19:02:08.751435 22447428132992 run.py:483] Algo bellman_ford step 8097 current loss 0.861699, current_train_items 259136.
I0302 19:02:08.783330 22447428132992 run.py:483] Algo bellman_ford step 8098 current loss 0.981780, current_train_items 259168.
I0302 19:02:08.816484 22447428132992 run.py:483] Algo bellman_ford step 8099 current loss 1.078348, current_train_items 259200.
I0302 19:02:08.835983 22447428132992 run.py:483] Algo bellman_ford step 8100 current loss 0.597539, current_train_items 259232.
I0302 19:02:08.843677 22447428132992 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:02:08.843783 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:02:08.860633 22447428132992 run.py:483] Algo bellman_ford step 8101 current loss 0.759588, current_train_items 259264.
I0302 19:02:08.886108 22447428132992 run.py:483] Algo bellman_ford step 8102 current loss 1.038743, current_train_items 259296.
I0302 19:02:08.918205 22447428132992 run.py:483] Algo bellman_ford step 8103 current loss 0.977228, current_train_items 259328.
I0302 19:02:08.951457 22447428132992 run.py:483] Algo bellman_ford step 8104 current loss 1.047063, current_train_items 259360.
I0302 19:02:08.971286 22447428132992 run.py:483] Algo bellman_ford step 8105 current loss 0.524370, current_train_items 259392.
I0302 19:02:08.987327 22447428132992 run.py:483] Algo bellman_ford step 8106 current loss 0.778841, current_train_items 259424.
I0302 19:02:09.010900 22447428132992 run.py:483] Algo bellman_ford step 8107 current loss 0.990031, current_train_items 259456.
I0302 19:02:09.042104 22447428132992 run.py:483] Algo bellman_ford step 8108 current loss 1.116750, current_train_items 259488.
I0302 19:02:09.076580 22447428132992 run.py:483] Algo bellman_ford step 8109 current loss 1.008408, current_train_items 259520.
I0302 19:02:09.096478 22447428132992 run.py:483] Algo bellman_ford step 8110 current loss 0.590848, current_train_items 259552.
I0302 19:02:09.112078 22447428132992 run.py:483] Algo bellman_ford step 8111 current loss 0.786993, current_train_items 259584.
I0302 19:02:09.135066 22447428132992 run.py:483] Algo bellman_ford step 8112 current loss 0.840017, current_train_items 259616.
I0302 19:02:09.165267 22447428132992 run.py:483] Algo bellman_ford step 8113 current loss 0.865778, current_train_items 259648.
I0302 19:02:09.200250 22447428132992 run.py:483] Algo bellman_ford step 8114 current loss 1.112580, current_train_items 259680.
I0302 19:02:09.219786 22447428132992 run.py:483] Algo bellman_ford step 8115 current loss 0.536783, current_train_items 259712.
I0302 19:02:09.235983 22447428132992 run.py:483] Algo bellman_ford step 8116 current loss 0.709402, current_train_items 259744.
I0302 19:02:09.259896 22447428132992 run.py:483] Algo bellman_ford step 8117 current loss 0.917834, current_train_items 259776.
I0302 19:02:09.292042 22447428132992 run.py:483] Algo bellman_ford step 8118 current loss 1.059586, current_train_items 259808.
I0302 19:02:09.325154 22447428132992 run.py:483] Algo bellman_ford step 8119 current loss 1.081796, current_train_items 259840.
I0302 19:02:09.344690 22447428132992 run.py:483] Algo bellman_ford step 8120 current loss 0.713200, current_train_items 259872.
I0302 19:02:09.360980 22447428132992 run.py:483] Algo bellman_ford step 8121 current loss 0.765159, current_train_items 259904.
I0302 19:02:09.384693 22447428132992 run.py:483] Algo bellman_ford step 8122 current loss 0.904075, current_train_items 259936.
I0302 19:02:09.417387 22447428132992 run.py:483] Algo bellman_ford step 8123 current loss 1.017544, current_train_items 259968.
I0302 19:02:09.449141 22447428132992 run.py:483] Algo bellman_ford step 8124 current loss 0.953543, current_train_items 260000.
I0302 19:02:09.468779 22447428132992 run.py:483] Algo bellman_ford step 8125 current loss 0.605449, current_train_items 260032.
I0302 19:02:09.485120 22447428132992 run.py:483] Algo bellman_ford step 8126 current loss 0.888309, current_train_items 260064.
I0302 19:02:09.509619 22447428132992 run.py:483] Algo bellman_ford step 8127 current loss 0.920158, current_train_items 260096.
I0302 19:02:09.541714 22447428132992 run.py:483] Algo bellman_ford step 8128 current loss 1.028276, current_train_items 260128.
I0302 19:02:09.577463 22447428132992 run.py:483] Algo bellman_ford step 8129 current loss 1.260134, current_train_items 260160.
I0302 19:02:09.597369 22447428132992 run.py:483] Algo bellman_ford step 8130 current loss 0.625645, current_train_items 260192.
I0302 19:02:09.613101 22447428132992 run.py:483] Algo bellman_ford step 8131 current loss 0.722856, current_train_items 260224.
I0302 19:02:09.637175 22447428132992 run.py:483] Algo bellman_ford step 8132 current loss 0.885849, current_train_items 260256.
I0302 19:02:09.668372 22447428132992 run.py:483] Algo bellman_ford step 8133 current loss 0.916968, current_train_items 260288.
I0302 19:02:09.702613 22447428132992 run.py:483] Algo bellman_ford step 8134 current loss 1.266515, current_train_items 260320.
I0302 19:02:09.722210 22447428132992 run.py:483] Algo bellman_ford step 8135 current loss 0.646139, current_train_items 260352.
I0302 19:02:09.737928 22447428132992 run.py:483] Algo bellman_ford step 8136 current loss 0.780157, current_train_items 260384.
I0302 19:02:09.761312 22447428132992 run.py:483] Algo bellman_ford step 8137 current loss 0.933290, current_train_items 260416.
I0302 19:02:09.792901 22447428132992 run.py:483] Algo bellman_ford step 8138 current loss 1.045061, current_train_items 260448.
I0302 19:02:09.828414 22447428132992 run.py:483] Algo bellman_ford step 8139 current loss 1.192191, current_train_items 260480.
I0302 19:02:09.847638 22447428132992 run.py:483] Algo bellman_ford step 8140 current loss 0.595108, current_train_items 260512.
I0302 19:02:09.863482 22447428132992 run.py:483] Algo bellman_ford step 8141 current loss 0.704370, current_train_items 260544.
I0302 19:02:09.887659 22447428132992 run.py:483] Algo bellman_ford step 8142 current loss 1.003589, current_train_items 260576.
I0302 19:02:09.919683 22447428132992 run.py:483] Algo bellman_ford step 8143 current loss 1.088281, current_train_items 260608.
I0302 19:02:09.953721 22447428132992 run.py:483] Algo bellman_ford step 8144 current loss 1.055776, current_train_items 260640.
I0302 19:02:09.973058 22447428132992 run.py:483] Algo bellman_ford step 8145 current loss 0.707262, current_train_items 260672.
I0302 19:02:09.989401 22447428132992 run.py:483] Algo bellman_ford step 8146 current loss 0.788790, current_train_items 260704.
I0302 19:02:10.012951 22447428132992 run.py:483] Algo bellman_ford step 8147 current loss 0.844108, current_train_items 260736.
I0302 19:02:10.044560 22447428132992 run.py:483] Algo bellman_ford step 8148 current loss 0.999320, current_train_items 260768.
I0302 19:02:10.077193 22447428132992 run.py:483] Algo bellman_ford step 8149 current loss 1.060251, current_train_items 260800.
I0302 19:02:10.097105 22447428132992 run.py:483] Algo bellman_ford step 8150 current loss 0.624908, current_train_items 260832.
I0302 19:02:10.105298 22447428132992 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:02:10.105411 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:10.121954 22447428132992 run.py:483] Algo bellman_ford step 8151 current loss 0.685225, current_train_items 260864.
I0302 19:02:10.146046 22447428132992 run.py:483] Algo bellman_ford step 8152 current loss 0.915578, current_train_items 260896.
I0302 19:02:10.176206 22447428132992 run.py:483] Algo bellman_ford step 8153 current loss 0.969078, current_train_items 260928.
I0302 19:02:10.211577 22447428132992 run.py:483] Algo bellman_ford step 8154 current loss 1.116795, current_train_items 260960.
I0302 19:02:10.231630 22447428132992 run.py:483] Algo bellman_ford step 8155 current loss 0.572783, current_train_items 260992.
I0302 19:02:10.247025 22447428132992 run.py:483] Algo bellman_ford step 8156 current loss 0.644657, current_train_items 261024.
I0302 19:02:10.270919 22447428132992 run.py:483] Algo bellman_ford step 8157 current loss 0.974395, current_train_items 261056.
I0302 19:02:10.302179 22447428132992 run.py:483] Algo bellman_ford step 8158 current loss 1.002676, current_train_items 261088.
I0302 19:02:10.334832 22447428132992 run.py:483] Algo bellman_ford step 8159 current loss 1.133896, current_train_items 261120.
I0302 19:02:10.354329 22447428132992 run.py:483] Algo bellman_ford step 8160 current loss 0.554070, current_train_items 261152.
I0302 19:02:10.370880 22447428132992 run.py:483] Algo bellman_ford step 8161 current loss 0.738520, current_train_items 261184.
I0302 19:02:10.394098 22447428132992 run.py:483] Algo bellman_ford step 8162 current loss 0.963131, current_train_items 261216.
I0302 19:02:10.425633 22447428132992 run.py:483] Algo bellman_ford step 8163 current loss 1.074368, current_train_items 261248.
I0302 19:02:10.455724 22447428132992 run.py:483] Algo bellman_ford step 8164 current loss 0.947867, current_train_items 261280.
I0302 19:02:10.474860 22447428132992 run.py:483] Algo bellman_ford step 8165 current loss 0.513942, current_train_items 261312.
I0302 19:02:10.491014 22447428132992 run.py:483] Algo bellman_ford step 8166 current loss 0.759880, current_train_items 261344.
I0302 19:02:10.514338 22447428132992 run.py:483] Algo bellman_ford step 8167 current loss 0.973711, current_train_items 261376.
I0302 19:02:10.545182 22447428132992 run.py:483] Algo bellman_ford step 8168 current loss 0.947239, current_train_items 261408.
I0302 19:02:10.579530 22447428132992 run.py:483] Algo bellman_ford step 8169 current loss 1.117792, current_train_items 261440.
I0302 19:02:10.599073 22447428132992 run.py:483] Algo bellman_ford step 8170 current loss 0.610152, current_train_items 261472.
I0302 19:02:10.615223 22447428132992 run.py:483] Algo bellman_ford step 8171 current loss 0.775917, current_train_items 261504.
I0302 19:02:10.638711 22447428132992 run.py:483] Algo bellman_ford step 8172 current loss 0.970266, current_train_items 261536.
I0302 19:02:10.670363 22447428132992 run.py:483] Algo bellman_ford step 8173 current loss 0.980068, current_train_items 261568.
I0302 19:02:10.702872 22447428132992 run.py:483] Algo bellman_ford step 8174 current loss 1.009759, current_train_items 261600.
I0302 19:02:10.722564 22447428132992 run.py:483] Algo bellman_ford step 8175 current loss 0.527558, current_train_items 261632.
I0302 19:02:10.739130 22447428132992 run.py:483] Algo bellman_ford step 8176 current loss 0.683625, current_train_items 261664.
I0302 19:02:10.762076 22447428132992 run.py:483] Algo bellman_ford step 8177 current loss 0.858938, current_train_items 261696.
I0302 19:02:10.794055 22447428132992 run.py:483] Algo bellman_ford step 8178 current loss 0.996560, current_train_items 261728.
I0302 19:02:10.824838 22447428132992 run.py:483] Algo bellman_ford step 8179 current loss 0.863227, current_train_items 261760.
I0302 19:02:10.844274 22447428132992 run.py:483] Algo bellman_ford step 8180 current loss 0.514477, current_train_items 261792.
I0302 19:02:10.860244 22447428132992 run.py:483] Algo bellman_ford step 8181 current loss 0.735999, current_train_items 261824.
I0302 19:02:10.883297 22447428132992 run.py:483] Algo bellman_ford step 8182 current loss 0.846903, current_train_items 261856.
I0302 19:02:10.914968 22447428132992 run.py:483] Algo bellman_ford step 8183 current loss 0.918540, current_train_items 261888.
I0302 19:02:10.950546 22447428132992 run.py:483] Algo bellman_ford step 8184 current loss 1.632143, current_train_items 261920.
I0302 19:02:10.970133 22447428132992 run.py:483] Algo bellman_ford step 8185 current loss 0.649800, current_train_items 261952.
I0302 19:02:10.986153 22447428132992 run.py:483] Algo bellman_ford step 8186 current loss 0.912370, current_train_items 261984.
I0302 19:02:11.009683 22447428132992 run.py:483] Algo bellman_ford step 8187 current loss 0.897286, current_train_items 262016.
I0302 19:02:11.040779 22447428132992 run.py:483] Algo bellman_ford step 8188 current loss 0.883437, current_train_items 262048.
I0302 19:02:11.073591 22447428132992 run.py:483] Algo bellman_ford step 8189 current loss 1.363034, current_train_items 262080.
I0302 19:02:11.093356 22447428132992 run.py:483] Algo bellman_ford step 8190 current loss 0.551036, current_train_items 262112.
I0302 19:02:11.109680 22447428132992 run.py:483] Algo bellman_ford step 8191 current loss 0.737037, current_train_items 262144.
I0302 19:02:11.133238 22447428132992 run.py:483] Algo bellman_ford step 8192 current loss 0.879482, current_train_items 262176.
I0302 19:02:11.163896 22447428132992 run.py:483] Algo bellman_ford step 8193 current loss 0.935616, current_train_items 262208.
I0302 19:02:11.196883 22447428132992 run.py:483] Algo bellman_ford step 8194 current loss 1.120370, current_train_items 262240.
I0302 19:02:11.216095 22447428132992 run.py:483] Algo bellman_ford step 8195 current loss 0.554942, current_train_items 262272.
I0302 19:02:11.232217 22447428132992 run.py:483] Algo bellman_ford step 8196 current loss 0.686835, current_train_items 262304.
I0302 19:02:11.255065 22447428132992 run.py:483] Algo bellman_ford step 8197 current loss 0.963835, current_train_items 262336.
I0302 19:02:11.286050 22447428132992 run.py:483] Algo bellman_ford step 8198 current loss 0.969956, current_train_items 262368.
I0302 19:02:11.318956 22447428132992 run.py:483] Algo bellman_ford step 8199 current loss 1.014569, current_train_items 262400.
I0302 19:02:11.338757 22447428132992 run.py:483] Algo bellman_ford step 8200 current loss 0.546728, current_train_items 262432.
I0302 19:02:11.346598 22447428132992 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:02:11.346704 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:11.362993 22447428132992 run.py:483] Algo bellman_ford step 8201 current loss 0.697446, current_train_items 262464.
I0302 19:02:11.387217 22447428132992 run.py:483] Algo bellman_ford step 8202 current loss 0.870837, current_train_items 262496.
I0302 19:02:11.418189 22447428132992 run.py:483] Algo bellman_ford step 8203 current loss 0.962773, current_train_items 262528.
I0302 19:02:11.452974 22447428132992 run.py:483] Algo bellman_ford step 8204 current loss 1.144621, current_train_items 262560.
I0302 19:02:11.472536 22447428132992 run.py:483] Algo bellman_ford step 8205 current loss 0.573251, current_train_items 262592.
I0302 19:02:11.488661 22447428132992 run.py:483] Algo bellman_ford step 8206 current loss 0.698288, current_train_items 262624.
I0302 19:02:11.512793 22447428132992 run.py:483] Algo bellman_ford step 8207 current loss 0.895245, current_train_items 262656.
I0302 19:02:11.543674 22447428132992 run.py:483] Algo bellman_ford step 8208 current loss 0.929211, current_train_items 262688.
I0302 19:02:11.574351 22447428132992 run.py:483] Algo bellman_ford step 8209 current loss 0.981221, current_train_items 262720.
I0302 19:02:11.593672 22447428132992 run.py:483] Algo bellman_ford step 8210 current loss 0.596646, current_train_items 262752.
I0302 19:02:11.609581 22447428132992 run.py:483] Algo bellman_ford step 8211 current loss 0.699466, current_train_items 262784.
I0302 19:02:11.632423 22447428132992 run.py:483] Algo bellman_ford step 8212 current loss 0.831497, current_train_items 262816.
I0302 19:02:11.663330 22447428132992 run.py:483] Algo bellman_ford step 8213 current loss 0.902192, current_train_items 262848.
I0302 19:02:11.696246 22447428132992 run.py:483] Algo bellman_ford step 8214 current loss 1.123821, current_train_items 262880.
I0302 19:02:11.715432 22447428132992 run.py:483] Algo bellman_ford step 8215 current loss 0.609439, current_train_items 262912.
I0302 19:02:11.730944 22447428132992 run.py:483] Algo bellman_ford step 8216 current loss 0.679134, current_train_items 262944.
I0302 19:02:11.753119 22447428132992 run.py:483] Algo bellman_ford step 8217 current loss 0.947051, current_train_items 262976.
I0302 19:02:11.785194 22447428132992 run.py:483] Algo bellman_ford step 8218 current loss 0.977463, current_train_items 263008.
I0302 19:02:11.818892 22447428132992 run.py:483] Algo bellman_ford step 8219 current loss 1.030679, current_train_items 263040.
I0302 19:02:11.838013 22447428132992 run.py:483] Algo bellman_ford step 8220 current loss 0.530059, current_train_items 263072.
I0302 19:02:11.853772 22447428132992 run.py:483] Algo bellman_ford step 8221 current loss 0.790322, current_train_items 263104.
I0302 19:02:11.877851 22447428132992 run.py:483] Algo bellman_ford step 8222 current loss 0.984341, current_train_items 263136.
I0302 19:02:11.909376 22447428132992 run.py:483] Algo bellman_ford step 8223 current loss 1.055368, current_train_items 263168.
I0302 19:02:11.940079 22447428132992 run.py:483] Algo bellman_ford step 8224 current loss 0.939140, current_train_items 263200.
I0302 19:02:11.959614 22447428132992 run.py:483] Algo bellman_ford step 8225 current loss 0.545485, current_train_items 263232.
I0302 19:02:11.975869 22447428132992 run.py:483] Algo bellman_ford step 8226 current loss 0.763631, current_train_items 263264.
I0302 19:02:12.000018 22447428132992 run.py:483] Algo bellman_ford step 8227 current loss 0.943519, current_train_items 263296.
I0302 19:02:12.030079 22447428132992 run.py:483] Algo bellman_ford step 8228 current loss 0.880802, current_train_items 263328.
I0302 19:02:12.063363 22447428132992 run.py:483] Algo bellman_ford step 8229 current loss 1.143979, current_train_items 263360.
I0302 19:02:12.082599 22447428132992 run.py:483] Algo bellman_ford step 8230 current loss 0.570991, current_train_items 263392.
I0302 19:02:12.098574 22447428132992 run.py:483] Algo bellman_ford step 8231 current loss 0.824500, current_train_items 263424.
I0302 19:02:12.122449 22447428132992 run.py:483] Algo bellman_ford step 8232 current loss 0.804036, current_train_items 263456.
I0302 19:02:12.153259 22447428132992 run.py:483] Algo bellman_ford step 8233 current loss 0.841403, current_train_items 263488.
I0302 19:02:12.185875 22447428132992 run.py:483] Algo bellman_ford step 8234 current loss 1.021464, current_train_items 263520.
I0302 19:02:12.204937 22447428132992 run.py:483] Algo bellman_ford step 8235 current loss 0.602763, current_train_items 263552.
I0302 19:02:12.221470 22447428132992 run.py:483] Algo bellman_ford step 8236 current loss 0.805439, current_train_items 263584.
I0302 19:02:12.245554 22447428132992 run.py:483] Algo bellman_ford step 8237 current loss 1.028411, current_train_items 263616.
I0302 19:02:12.276952 22447428132992 run.py:483] Algo bellman_ford step 8238 current loss 1.028700, current_train_items 263648.
I0302 19:02:12.309553 22447428132992 run.py:483] Algo bellman_ford step 8239 current loss 0.993163, current_train_items 263680.
I0302 19:02:12.328885 22447428132992 run.py:483] Algo bellman_ford step 8240 current loss 0.647092, current_train_items 263712.
I0302 19:02:12.344920 22447428132992 run.py:483] Algo bellman_ford step 8241 current loss 0.656582, current_train_items 263744.
I0302 19:02:12.367730 22447428132992 run.py:483] Algo bellman_ford step 8242 current loss 0.795144, current_train_items 263776.
I0302 19:02:12.398548 22447428132992 run.py:483] Algo bellman_ford step 8243 current loss 0.900609, current_train_items 263808.
I0302 19:02:12.433570 22447428132992 run.py:483] Algo bellman_ford step 8244 current loss 1.162362, current_train_items 263840.
I0302 19:02:12.453094 22447428132992 run.py:483] Algo bellman_ford step 8245 current loss 0.706870, current_train_items 263872.
I0302 19:02:12.469124 22447428132992 run.py:483] Algo bellman_ford step 8246 current loss 0.730219, current_train_items 263904.
I0302 19:02:12.491566 22447428132992 run.py:483] Algo bellman_ford step 8247 current loss 0.879485, current_train_items 263936.
I0302 19:02:12.522079 22447428132992 run.py:483] Algo bellman_ford step 8248 current loss 1.015496, current_train_items 263968.
I0302 19:02:12.555454 22447428132992 run.py:483] Algo bellman_ford step 8249 current loss 1.155466, current_train_items 264000.
I0302 19:02:12.574675 22447428132992 run.py:483] Algo bellman_ford step 8250 current loss 0.500630, current_train_items 264032.
I0302 19:02:12.582767 22447428132992 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:02:12.582874 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:12.599434 22447428132992 run.py:483] Algo bellman_ford step 8251 current loss 0.750317, current_train_items 264064.
I0302 19:02:12.623597 22447428132992 run.py:483] Algo bellman_ford step 8252 current loss 0.912572, current_train_items 264096.
I0302 19:02:12.656142 22447428132992 run.py:483] Algo bellman_ford step 8253 current loss 1.055656, current_train_items 264128.
I0302 19:02:12.688962 22447428132992 run.py:483] Algo bellman_ford step 8254 current loss 1.017128, current_train_items 264160.
I0302 19:02:12.708725 22447428132992 run.py:483] Algo bellman_ford step 8255 current loss 0.623631, current_train_items 264192.
I0302 19:02:12.725049 22447428132992 run.py:483] Algo bellman_ford step 8256 current loss 0.684912, current_train_items 264224.
I0302 19:02:12.749143 22447428132992 run.py:483] Algo bellman_ford step 8257 current loss 0.908651, current_train_items 264256.
I0302 19:02:12.780796 22447428132992 run.py:483] Algo bellman_ford step 8258 current loss 0.959280, current_train_items 264288.
I0302 19:02:12.815440 22447428132992 run.py:483] Algo bellman_ford step 8259 current loss 1.226472, current_train_items 264320.
I0302 19:02:12.834997 22447428132992 run.py:483] Algo bellman_ford step 8260 current loss 0.574207, current_train_items 264352.
I0302 19:02:12.851305 22447428132992 run.py:483] Algo bellman_ford step 8261 current loss 0.748789, current_train_items 264384.
I0302 19:02:12.875269 22447428132992 run.py:483] Algo bellman_ford step 8262 current loss 1.027461, current_train_items 264416.
I0302 19:02:12.906981 22447428132992 run.py:483] Algo bellman_ford step 8263 current loss 1.015637, current_train_items 264448.
I0302 19:02:12.941647 22447428132992 run.py:483] Algo bellman_ford step 8264 current loss 1.172346, current_train_items 264480.
I0302 19:02:12.960981 22447428132992 run.py:483] Algo bellman_ford step 8265 current loss 0.552695, current_train_items 264512.
I0302 19:02:12.976715 22447428132992 run.py:483] Algo bellman_ford step 8266 current loss 0.769924, current_train_items 264544.
I0302 19:02:13.000248 22447428132992 run.py:483] Algo bellman_ford step 8267 current loss 0.905620, current_train_items 264576.
I0302 19:02:13.032024 22447428132992 run.py:483] Algo bellman_ford step 8268 current loss 1.068129, current_train_items 264608.
I0302 19:02:13.064045 22447428132992 run.py:483] Algo bellman_ford step 8269 current loss 1.052909, current_train_items 264640.
I0302 19:02:13.084047 22447428132992 run.py:483] Algo bellman_ford step 8270 current loss 0.603968, current_train_items 264672.
I0302 19:02:13.099676 22447428132992 run.py:483] Algo bellman_ford step 8271 current loss 0.701804, current_train_items 264704.
I0302 19:02:13.122754 22447428132992 run.py:483] Algo bellman_ford step 8272 current loss 0.890749, current_train_items 264736.
I0302 19:02:13.153047 22447428132992 run.py:483] Algo bellman_ford step 8273 current loss 0.973795, current_train_items 264768.
I0302 19:02:13.187745 22447428132992 run.py:483] Algo bellman_ford step 8274 current loss 1.080391, current_train_items 264800.
I0302 19:02:13.207503 22447428132992 run.py:483] Algo bellman_ford step 8275 current loss 0.552314, current_train_items 264832.
I0302 19:02:13.224118 22447428132992 run.py:483] Algo bellman_ford step 8276 current loss 0.904083, current_train_items 264864.
I0302 19:02:13.247847 22447428132992 run.py:483] Algo bellman_ford step 8277 current loss 0.961400, current_train_items 264896.
I0302 19:02:13.278738 22447428132992 run.py:483] Algo bellman_ford step 8278 current loss 0.993123, current_train_items 264928.
I0302 19:02:13.311613 22447428132992 run.py:483] Algo bellman_ford step 8279 current loss 1.173635, current_train_items 264960.
I0302 19:02:13.330822 22447428132992 run.py:483] Algo bellman_ford step 8280 current loss 0.837422, current_train_items 264992.
I0302 19:02:13.347268 22447428132992 run.py:483] Algo bellman_ford step 8281 current loss 0.864040, current_train_items 265024.
I0302 19:02:13.370606 22447428132992 run.py:483] Algo bellman_ford step 8282 current loss 0.904949, current_train_items 265056.
I0302 19:02:13.401269 22447428132992 run.py:483] Algo bellman_ford step 8283 current loss 1.017832, current_train_items 265088.
I0302 19:02:13.433829 22447428132992 run.py:483] Algo bellman_ford step 8284 current loss 1.175735, current_train_items 265120.
I0302 19:02:13.453678 22447428132992 run.py:483] Algo bellman_ford step 8285 current loss 0.534595, current_train_items 265152.
I0302 19:02:13.470179 22447428132992 run.py:483] Algo bellman_ford step 8286 current loss 0.748711, current_train_items 265184.
I0302 19:02:13.493354 22447428132992 run.py:483] Algo bellman_ford step 8287 current loss 0.919170, current_train_items 265216.
I0302 19:02:13.525856 22447428132992 run.py:483] Algo bellman_ford step 8288 current loss 1.065057, current_train_items 265248.
I0302 19:02:13.559419 22447428132992 run.py:483] Algo bellman_ford step 8289 current loss 1.126387, current_train_items 265280.
I0302 19:02:13.578794 22447428132992 run.py:483] Algo bellman_ford step 8290 current loss 0.589877, current_train_items 265312.
I0302 19:02:13.594872 22447428132992 run.py:483] Algo bellman_ford step 8291 current loss 0.821983, current_train_items 265344.
I0302 19:02:13.618680 22447428132992 run.py:483] Algo bellman_ford step 8292 current loss 0.939203, current_train_items 265376.
I0302 19:02:13.648653 22447428132992 run.py:483] Algo bellman_ford step 8293 current loss 0.992039, current_train_items 265408.
I0302 19:02:13.683395 22447428132992 run.py:483] Algo bellman_ford step 8294 current loss 1.115293, current_train_items 265440.
I0302 19:02:13.702818 22447428132992 run.py:483] Algo bellman_ford step 8295 current loss 0.620846, current_train_items 265472.
I0302 19:02:13.719056 22447428132992 run.py:483] Algo bellman_ford step 8296 current loss 0.790166, current_train_items 265504.
I0302 19:02:13.743637 22447428132992 run.py:483] Algo bellman_ford step 8297 current loss 0.982563, current_train_items 265536.
I0302 19:02:13.774924 22447428132992 run.py:483] Algo bellman_ford step 8298 current loss 0.979533, current_train_items 265568.
I0302 19:02:13.807300 22447428132992 run.py:483] Algo bellman_ford step 8299 current loss 1.118263, current_train_items 265600.
I0302 19:02:13.826992 22447428132992 run.py:483] Algo bellman_ford step 8300 current loss 1.049582, current_train_items 265632.
I0302 19:02:13.834902 22447428132992 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:02:13.835008 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 19:02:13.851311 22447428132992 run.py:483] Algo bellman_ford step 8301 current loss 0.879612, current_train_items 265664.
I0302 19:02:13.876633 22447428132992 run.py:483] Algo bellman_ford step 8302 current loss 1.132473, current_train_items 265696.
I0302 19:02:13.908847 22447428132992 run.py:483] Algo bellman_ford step 8303 current loss 1.033621, current_train_items 265728.
I0302 19:02:13.942206 22447428132992 run.py:483] Algo bellman_ford step 8304 current loss 1.058824, current_train_items 265760.
I0302 19:02:13.962553 22447428132992 run.py:483] Algo bellman_ford step 8305 current loss 0.648209, current_train_items 265792.
I0302 19:02:13.978234 22447428132992 run.py:483] Algo bellman_ford step 8306 current loss 0.778652, current_train_items 265824.
I0302 19:02:14.001992 22447428132992 run.py:483] Algo bellman_ford step 8307 current loss 0.904341, current_train_items 265856.
I0302 19:02:14.031418 22447428132992 run.py:483] Algo bellman_ford step 8308 current loss 0.867953, current_train_items 265888.
I0302 19:02:14.066767 22447428132992 run.py:483] Algo bellman_ford step 8309 current loss 1.207154, current_train_items 265920.
I0302 19:02:14.086675 22447428132992 run.py:483] Algo bellman_ford step 8310 current loss 0.523351, current_train_items 265952.
I0302 19:02:14.102687 22447428132992 run.py:483] Algo bellman_ford step 8311 current loss 0.699836, current_train_items 265984.
I0302 19:02:14.126694 22447428132992 run.py:483] Algo bellman_ford step 8312 current loss 0.950546, current_train_items 266016.
I0302 19:02:14.157233 22447428132992 run.py:483] Algo bellman_ford step 8313 current loss 0.997403, current_train_items 266048.
I0302 19:02:14.189074 22447428132992 run.py:483] Algo bellman_ford step 8314 current loss 0.996692, current_train_items 266080.
I0302 19:02:14.208724 22447428132992 run.py:483] Algo bellman_ford step 8315 current loss 0.532622, current_train_items 266112.
I0302 19:02:14.225116 22447428132992 run.py:483] Algo bellman_ford step 8316 current loss 0.713763, current_train_items 266144.
I0302 19:02:14.248588 22447428132992 run.py:483] Algo bellman_ford step 8317 current loss 0.981204, current_train_items 266176.
I0302 19:02:14.278343 22447428132992 run.py:483] Algo bellman_ford step 8318 current loss 0.958240, current_train_items 266208.
I0302 19:02:14.311675 22447428132992 run.py:483] Algo bellman_ford step 8319 current loss 1.044221, current_train_items 266240.
I0302 19:02:14.331122 22447428132992 run.py:483] Algo bellman_ford step 8320 current loss 0.582774, current_train_items 266272.
I0302 19:02:14.347138 22447428132992 run.py:483] Algo bellman_ford step 8321 current loss 0.656188, current_train_items 266304.
I0302 19:02:14.371835 22447428132992 run.py:483] Algo bellman_ford step 8322 current loss 0.949007, current_train_items 266336.
I0302 19:02:14.402832 22447428132992 run.py:483] Algo bellman_ford step 8323 current loss 1.019772, current_train_items 266368.
I0302 19:02:14.437618 22447428132992 run.py:483] Algo bellman_ford step 8324 current loss 1.134426, current_train_items 266400.
I0302 19:02:14.457240 22447428132992 run.py:483] Algo bellman_ford step 8325 current loss 0.664481, current_train_items 266432.
I0302 19:02:14.472910 22447428132992 run.py:483] Algo bellman_ford step 8326 current loss 0.719488, current_train_items 266464.
I0302 19:02:14.496574 22447428132992 run.py:483] Algo bellman_ford step 8327 current loss 0.808105, current_train_items 266496.
I0302 19:02:14.526695 22447428132992 run.py:483] Algo bellman_ford step 8328 current loss 0.997198, current_train_items 266528.
I0302 19:02:14.561031 22447428132992 run.py:483] Algo bellman_ford step 8329 current loss 0.976832, current_train_items 266560.
I0302 19:02:14.580579 22447428132992 run.py:483] Algo bellman_ford step 8330 current loss 0.742217, current_train_items 266592.
I0302 19:02:14.596503 22447428132992 run.py:483] Algo bellman_ford step 8331 current loss 0.760747, current_train_items 266624.
I0302 19:02:14.619846 22447428132992 run.py:483] Algo bellman_ford step 8332 current loss 0.826532, current_train_items 266656.
I0302 19:02:14.651518 22447428132992 run.py:483] Algo bellman_ford step 8333 current loss 1.122219, current_train_items 266688.
I0302 19:02:14.684247 22447428132992 run.py:483] Algo bellman_ford step 8334 current loss 1.026213, current_train_items 266720.
I0302 19:02:14.703911 22447428132992 run.py:483] Algo bellman_ford step 8335 current loss 0.590746, current_train_items 266752.
I0302 19:02:14.719983 22447428132992 run.py:483] Algo bellman_ford step 8336 current loss 0.748717, current_train_items 266784.
I0302 19:02:14.744899 22447428132992 run.py:483] Algo bellman_ford step 8337 current loss 0.957922, current_train_items 266816.
I0302 19:02:14.775990 22447428132992 run.py:483] Algo bellman_ford step 8338 current loss 1.029419, current_train_items 266848.
I0302 19:02:14.808735 22447428132992 run.py:483] Algo bellman_ford step 8339 current loss 1.155240, current_train_items 266880.
I0302 19:02:14.828236 22447428132992 run.py:483] Algo bellman_ford step 8340 current loss 0.618781, current_train_items 266912.
I0302 19:02:14.844300 22447428132992 run.py:483] Algo bellman_ford step 8341 current loss 0.683701, current_train_items 266944.
I0302 19:02:14.867571 22447428132992 run.py:483] Algo bellman_ford step 8342 current loss 0.881551, current_train_items 266976.
I0302 19:02:14.898171 22447428132992 run.py:483] Algo bellman_ford step 8343 current loss 0.914410, current_train_items 267008.
I0302 19:02:14.932965 22447428132992 run.py:483] Algo bellman_ford step 8344 current loss 1.255677, current_train_items 267040.
I0302 19:02:14.952520 22447428132992 run.py:483] Algo bellman_ford step 8345 current loss 0.500599, current_train_items 267072.
I0302 19:02:14.968473 22447428132992 run.py:483] Algo bellman_ford step 8346 current loss 0.689163, current_train_items 267104.
I0302 19:02:14.992972 22447428132992 run.py:483] Algo bellman_ford step 8347 current loss 0.968084, current_train_items 267136.
I0302 19:02:15.024986 22447428132992 run.py:483] Algo bellman_ford step 8348 current loss 1.009706, current_train_items 267168.
I0302 19:02:15.059262 22447428132992 run.py:483] Algo bellman_ford step 8349 current loss 1.226889, current_train_items 267200.
I0302 19:02:15.078815 22447428132992 run.py:483] Algo bellman_ford step 8350 current loss 0.543417, current_train_items 267232.
I0302 19:02:15.086985 22447428132992 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:02:15.087090 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 19:02:15.103826 22447428132992 run.py:483] Algo bellman_ford step 8351 current loss 0.706899, current_train_items 267264.
I0302 19:02:15.128139 22447428132992 run.py:483] Algo bellman_ford step 8352 current loss 0.965366, current_train_items 267296.
I0302 19:02:15.159588 22447428132992 run.py:483] Algo bellman_ford step 8353 current loss 1.033000, current_train_items 267328.
I0302 19:02:15.194810 22447428132992 run.py:483] Algo bellman_ford step 8354 current loss 1.256237, current_train_items 267360.
I0302 19:02:15.214645 22447428132992 run.py:483] Algo bellman_ford step 8355 current loss 0.633233, current_train_items 267392.
I0302 19:02:15.230175 22447428132992 run.py:483] Algo bellman_ford step 8356 current loss 0.686914, current_train_items 267424.
I0302 19:02:15.253605 22447428132992 run.py:483] Algo bellman_ford step 8357 current loss 0.801810, current_train_items 267456.
I0302 19:02:15.284841 22447428132992 run.py:483] Algo bellman_ford step 8358 current loss 0.847966, current_train_items 267488.
I0302 19:02:15.317020 22447428132992 run.py:483] Algo bellman_ford step 8359 current loss 1.067109, current_train_items 267520.
I0302 19:02:15.336860 22447428132992 run.py:483] Algo bellman_ford step 8360 current loss 0.656328, current_train_items 267552.
I0302 19:02:15.352749 22447428132992 run.py:483] Algo bellman_ford step 8361 current loss 0.765998, current_train_items 267584.
I0302 19:02:15.376207 22447428132992 run.py:483] Algo bellman_ford step 8362 current loss 0.927085, current_train_items 267616.
I0302 19:02:15.406665 22447428132992 run.py:483] Algo bellman_ford step 8363 current loss 0.947808, current_train_items 267648.
I0302 19:02:15.440197 22447428132992 run.py:483] Algo bellman_ford step 8364 current loss 1.129605, current_train_items 267680.
I0302 19:02:15.459677 22447428132992 run.py:483] Algo bellman_ford step 8365 current loss 0.630530, current_train_items 267712.
I0302 19:02:15.476073 22447428132992 run.py:483] Algo bellman_ford step 8366 current loss 0.756103, current_train_items 267744.
I0302 19:02:15.498768 22447428132992 run.py:483] Algo bellman_ford step 8367 current loss 0.894660, current_train_items 267776.
I0302 19:02:15.529911 22447428132992 run.py:483] Algo bellman_ford step 8368 current loss 0.956055, current_train_items 267808.
I0302 19:02:15.565170 22447428132992 run.py:483] Algo bellman_ford step 8369 current loss 1.235827, current_train_items 267840.
I0302 19:02:15.584911 22447428132992 run.py:483] Algo bellman_ford step 8370 current loss 0.482318, current_train_items 267872.
I0302 19:02:15.601285 22447428132992 run.py:483] Algo bellman_ford step 8371 current loss 0.797650, current_train_items 267904.
I0302 19:02:15.623964 22447428132992 run.py:483] Algo bellman_ford step 8372 current loss 0.860191, current_train_items 267936.
I0302 19:02:15.654953 22447428132992 run.py:483] Algo bellman_ford step 8373 current loss 0.960550, current_train_items 267968.
I0302 19:02:15.688913 22447428132992 run.py:483] Algo bellman_ford step 8374 current loss 1.034077, current_train_items 268000.
I0302 19:02:15.708976 22447428132992 run.py:483] Algo bellman_ford step 8375 current loss 0.543242, current_train_items 268032.
I0302 19:02:15.724672 22447428132992 run.py:483] Algo bellman_ford step 8376 current loss 0.732773, current_train_items 268064.
I0302 19:02:15.747284 22447428132992 run.py:483] Algo bellman_ford step 8377 current loss 0.906612, current_train_items 268096.
I0302 19:02:15.778558 22447428132992 run.py:483] Algo bellman_ford step 8378 current loss 0.901202, current_train_items 268128.
I0302 19:02:15.812246 22447428132992 run.py:483] Algo bellman_ford step 8379 current loss 1.029587, current_train_items 268160.
I0302 19:02:15.831846 22447428132992 run.py:483] Algo bellman_ford step 8380 current loss 0.634291, current_train_items 268192.
I0302 19:02:15.848020 22447428132992 run.py:483] Algo bellman_ford step 8381 current loss 0.796497, current_train_items 268224.
I0302 19:02:15.870622 22447428132992 run.py:483] Algo bellman_ford step 8382 current loss 0.766471, current_train_items 268256.
I0302 19:02:15.902402 22447428132992 run.py:483] Algo bellman_ford step 8383 current loss 1.056800, current_train_items 268288.
I0302 19:02:15.934352 22447428132992 run.py:483] Algo bellman_ford step 8384 current loss 0.945173, current_train_items 268320.
I0302 19:02:15.954081 22447428132992 run.py:483] Algo bellman_ford step 8385 current loss 0.544676, current_train_items 268352.
I0302 19:02:15.969841 22447428132992 run.py:483] Algo bellman_ford step 8386 current loss 0.713435, current_train_items 268384.
I0302 19:02:15.992638 22447428132992 run.py:483] Algo bellman_ford step 8387 current loss 0.834090, current_train_items 268416.
I0302 19:02:16.024007 22447428132992 run.py:483] Algo bellman_ford step 8388 current loss 1.075135, current_train_items 268448.
I0302 19:02:16.056832 22447428132992 run.py:483] Algo bellman_ford step 8389 current loss 1.040836, current_train_items 268480.
I0302 19:02:16.076599 22447428132992 run.py:483] Algo bellman_ford step 8390 current loss 0.639904, current_train_items 268512.
I0302 19:02:16.092869 22447428132992 run.py:483] Algo bellman_ford step 8391 current loss 0.758231, current_train_items 268544.
I0302 19:02:16.115331 22447428132992 run.py:483] Algo bellman_ford step 8392 current loss 0.898350, current_train_items 268576.
I0302 19:02:16.147804 22447428132992 run.py:483] Algo bellman_ford step 8393 current loss 0.978550, current_train_items 268608.
I0302 19:02:16.179875 22447428132992 run.py:483] Algo bellman_ford step 8394 current loss 1.034241, current_train_items 268640.
I0302 19:02:16.199410 22447428132992 run.py:483] Algo bellman_ford step 8395 current loss 0.588957, current_train_items 268672.
I0302 19:02:16.215553 22447428132992 run.py:483] Algo bellman_ford step 8396 current loss 0.808054, current_train_items 268704.
I0302 19:02:16.238957 22447428132992 run.py:483] Algo bellman_ford step 8397 current loss 0.882174, current_train_items 268736.
I0302 19:02:16.270628 22447428132992 run.py:483] Algo bellman_ford step 8398 current loss 0.972821, current_train_items 268768.
I0302 19:02:16.304686 22447428132992 run.py:483] Algo bellman_ford step 8399 current loss 1.180650, current_train_items 268800.
I0302 19:02:16.324285 22447428132992 run.py:483] Algo bellman_ford step 8400 current loss 0.586004, current_train_items 268832.
I0302 19:02:16.332112 22447428132992 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:02:16.332218 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 19:02:16.349090 22447428132992 run.py:483] Algo bellman_ford step 8401 current loss 0.753788, current_train_items 268864.
I0302 19:02:16.373859 22447428132992 run.py:483] Algo bellman_ford step 8402 current loss 0.902015, current_train_items 268896.
I0302 19:02:16.406342 22447428132992 run.py:483] Algo bellman_ford step 8403 current loss 1.047472, current_train_items 268928.
I0302 19:02:16.442524 22447428132992 run.py:483] Algo bellman_ford step 8404 current loss 1.229303, current_train_items 268960.
I0302 19:02:16.462264 22447428132992 run.py:483] Algo bellman_ford step 8405 current loss 0.600422, current_train_items 268992.
I0302 19:02:16.478091 22447428132992 run.py:483] Algo bellman_ford step 8406 current loss 0.754719, current_train_items 269024.
I0302 19:02:16.501433 22447428132992 run.py:483] Algo bellman_ford step 8407 current loss 0.926731, current_train_items 269056.
I0302 19:02:16.531841 22447428132992 run.py:483] Algo bellman_ford step 8408 current loss 1.056213, current_train_items 269088.
I0302 19:02:16.566539 22447428132992 run.py:483] Algo bellman_ford step 8409 current loss 1.093095, current_train_items 269120.
I0302 19:02:16.586293 22447428132992 run.py:483] Algo bellman_ford step 8410 current loss 0.573119, current_train_items 269152.
I0302 19:02:16.602369 22447428132992 run.py:483] Algo bellman_ford step 8411 current loss 0.765606, current_train_items 269184.
I0302 19:02:16.626605 22447428132992 run.py:483] Algo bellman_ford step 8412 current loss 0.886022, current_train_items 269216.
I0302 19:02:16.658857 22447428132992 run.py:483] Algo bellman_ford step 8413 current loss 0.981512, current_train_items 269248.
I0302 19:02:16.693192 22447428132992 run.py:483] Algo bellman_ford step 8414 current loss 1.112677, current_train_items 269280.
I0302 19:02:16.712973 22447428132992 run.py:483] Algo bellman_ford step 8415 current loss 1.115470, current_train_items 269312.
I0302 19:02:16.729266 22447428132992 run.py:483] Algo bellman_ford step 8416 current loss 0.836903, current_train_items 269344.
I0302 19:02:16.752823 22447428132992 run.py:483] Algo bellman_ford step 8417 current loss 1.080126, current_train_items 269376.
I0302 19:02:16.784962 22447428132992 run.py:483] Algo bellman_ford step 8418 current loss 0.996635, current_train_items 269408.
I0302 19:02:16.821829 22447428132992 run.py:483] Algo bellman_ford step 8419 current loss 1.187418, current_train_items 269440.
I0302 19:02:16.841634 22447428132992 run.py:483] Algo bellman_ford step 8420 current loss 0.899915, current_train_items 269472.
I0302 19:02:16.857786 22447428132992 run.py:483] Algo bellman_ford step 8421 current loss 0.938651, current_train_items 269504.
I0302 19:02:16.882937 22447428132992 run.py:483] Algo bellman_ford step 8422 current loss 0.941002, current_train_items 269536.
I0302 19:02:16.914638 22447428132992 run.py:483] Algo bellman_ford step 8423 current loss 0.907091, current_train_items 269568.
I0302 19:02:16.948989 22447428132992 run.py:483] Algo bellman_ford step 8424 current loss 1.171044, current_train_items 269600.
I0302 19:02:16.968337 22447428132992 run.py:483] Algo bellman_ford step 8425 current loss 0.553989, current_train_items 269632.
I0302 19:02:16.984077 22447428132992 run.py:483] Algo bellman_ford step 8426 current loss 0.814224, current_train_items 269664.
I0302 19:02:17.008011 22447428132992 run.py:483] Algo bellman_ford step 8427 current loss 0.896955, current_train_items 269696.
I0302 19:02:17.039247 22447428132992 run.py:483] Algo bellman_ford step 8428 current loss 1.033774, current_train_items 269728.
I0302 19:02:17.072950 22447428132992 run.py:483] Algo bellman_ford step 8429 current loss 1.080518, current_train_items 269760.
I0302 19:02:17.092576 22447428132992 run.py:483] Algo bellman_ford step 8430 current loss 0.543551, current_train_items 269792.
I0302 19:02:17.109101 22447428132992 run.py:483] Algo bellman_ford step 8431 current loss 0.774144, current_train_items 269824.
I0302 19:02:17.132433 22447428132992 run.py:483] Algo bellman_ford step 8432 current loss 0.786952, current_train_items 269856.
I0302 19:02:17.165129 22447428132992 run.py:483] Algo bellman_ford step 8433 current loss 1.097299, current_train_items 269888.
I0302 19:02:17.197217 22447428132992 run.py:483] Algo bellman_ford step 8434 current loss 1.010164, current_train_items 269920.
I0302 19:02:17.216457 22447428132992 run.py:483] Algo bellman_ford step 8435 current loss 0.708193, current_train_items 269952.
I0302 19:02:17.232287 22447428132992 run.py:483] Algo bellman_ford step 8436 current loss 0.724829, current_train_items 269984.
I0302 19:02:17.255726 22447428132992 run.py:483] Algo bellman_ford step 8437 current loss 0.919362, current_train_items 270016.
I0302 19:02:17.286951 22447428132992 run.py:483] Algo bellman_ford step 8438 current loss 0.904854, current_train_items 270048.
I0302 19:02:17.319155 22447428132992 run.py:483] Algo bellman_ford step 8439 current loss 1.126010, current_train_items 270080.
I0302 19:02:17.338916 22447428132992 run.py:483] Algo bellman_ford step 8440 current loss 0.548082, current_train_items 270112.
I0302 19:02:17.355129 22447428132992 run.py:483] Algo bellman_ford step 8441 current loss 0.753702, current_train_items 270144.
I0302 19:02:17.379222 22447428132992 run.py:483] Algo bellman_ford step 8442 current loss 0.893259, current_train_items 270176.
I0302 19:02:17.410739 22447428132992 run.py:483] Algo bellman_ford step 8443 current loss 1.050091, current_train_items 270208.
I0302 19:02:17.444856 22447428132992 run.py:483] Algo bellman_ford step 8444 current loss 1.122296, current_train_items 270240.
I0302 19:02:17.464236 22447428132992 run.py:483] Algo bellman_ford step 8445 current loss 0.543865, current_train_items 270272.
I0302 19:02:17.480165 22447428132992 run.py:483] Algo bellman_ford step 8446 current loss 0.693673, current_train_items 270304.
I0302 19:02:17.503933 22447428132992 run.py:483] Algo bellman_ford step 8447 current loss 0.872729, current_train_items 270336.
I0302 19:02:17.535036 22447428132992 run.py:483] Algo bellman_ford step 8448 current loss 0.927604, current_train_items 270368.
I0302 19:02:17.568802 22447428132992 run.py:483] Algo bellman_ford step 8449 current loss 1.254351, current_train_items 270400.
I0302 19:02:17.588440 22447428132992 run.py:483] Algo bellman_ford step 8450 current loss 0.558522, current_train_items 270432.
I0302 19:02:17.596739 22447428132992 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:02:17.596846 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:17.613188 22447428132992 run.py:483] Algo bellman_ford step 8451 current loss 0.737693, current_train_items 270464.
I0302 19:02:17.637442 22447428132992 run.py:483] Algo bellman_ford step 8452 current loss 0.873769, current_train_items 270496.
I0302 19:02:17.669852 22447428132992 run.py:483] Algo bellman_ford step 8453 current loss 0.995847, current_train_items 270528.
I0302 19:02:17.704357 22447428132992 run.py:483] Algo bellman_ford step 8454 current loss 1.013645, current_train_items 270560.
I0302 19:02:17.724430 22447428132992 run.py:483] Algo bellman_ford step 8455 current loss 1.208960, current_train_items 270592.
I0302 19:02:17.740579 22447428132992 run.py:483] Algo bellman_ford step 8456 current loss 0.978972, current_train_items 270624.
I0302 19:02:17.765092 22447428132992 run.py:483] Algo bellman_ford step 8457 current loss 1.029811, current_train_items 270656.
I0302 19:02:17.795341 22447428132992 run.py:483] Algo bellman_ford step 8458 current loss 0.953008, current_train_items 270688.
I0302 19:02:17.826438 22447428132992 run.py:483] Algo bellman_ford step 8459 current loss 1.054289, current_train_items 270720.
I0302 19:02:17.846133 22447428132992 run.py:483] Algo bellman_ford step 8460 current loss 0.697773, current_train_items 270752.
I0302 19:02:17.862183 22447428132992 run.py:483] Algo bellman_ford step 8461 current loss 0.657383, current_train_items 270784.
I0302 19:02:17.884858 22447428132992 run.py:483] Algo bellman_ford step 8462 current loss 0.874027, current_train_items 270816.
I0302 19:02:17.915228 22447428132992 run.py:483] Algo bellman_ford step 8463 current loss 0.930998, current_train_items 270848.
I0302 19:02:17.949322 22447428132992 run.py:483] Algo bellman_ford step 8464 current loss 1.191916, current_train_items 270880.
I0302 19:02:17.968610 22447428132992 run.py:483] Algo bellman_ford step 8465 current loss 0.535568, current_train_items 270912.
I0302 19:02:17.985112 22447428132992 run.py:483] Algo bellman_ford step 8466 current loss 0.845495, current_train_items 270944.
I0302 19:02:18.008175 22447428132992 run.py:483] Algo bellman_ford step 8467 current loss 0.906207, current_train_items 270976.
I0302 19:02:18.038735 22447428132992 run.py:483] Algo bellman_ford step 8468 current loss 1.163384, current_train_items 271008.
I0302 19:02:18.071597 22447428132992 run.py:483] Algo bellman_ford step 8469 current loss 1.012821, current_train_items 271040.
I0302 19:02:18.091359 22447428132992 run.py:483] Algo bellman_ford step 8470 current loss 0.621554, current_train_items 271072.
I0302 19:02:18.107948 22447428132992 run.py:483] Algo bellman_ford step 8471 current loss 0.832574, current_train_items 271104.
I0302 19:02:18.130934 22447428132992 run.py:483] Algo bellman_ford step 8472 current loss 0.901187, current_train_items 271136.
I0302 19:02:18.159775 22447428132992 run.py:483] Algo bellman_ford step 8473 current loss 0.905206, current_train_items 271168.
I0302 19:02:18.193866 22447428132992 run.py:483] Algo bellman_ford step 8474 current loss 1.098652, current_train_items 271200.
I0302 19:02:18.213721 22447428132992 run.py:483] Algo bellman_ford step 8475 current loss 0.664453, current_train_items 271232.
I0302 19:02:18.230103 22447428132992 run.py:483] Algo bellman_ford step 8476 current loss 0.736252, current_train_items 271264.
I0302 19:02:18.254646 22447428132992 run.py:483] Algo bellman_ford step 8477 current loss 1.057840, current_train_items 271296.
I0302 19:02:18.285788 22447428132992 run.py:483] Algo bellman_ford step 8478 current loss 0.913044, current_train_items 271328.
I0302 19:02:18.320238 22447428132992 run.py:483] Algo bellman_ford step 8479 current loss 1.028242, current_train_items 271360.
I0302 19:02:18.339492 22447428132992 run.py:483] Algo bellman_ford step 8480 current loss 0.593454, current_train_items 271392.
I0302 19:02:18.355727 22447428132992 run.py:483] Algo bellman_ford step 8481 current loss 0.824903, current_train_items 271424.
I0302 19:02:18.379466 22447428132992 run.py:483] Algo bellman_ford step 8482 current loss 1.009315, current_train_items 271456.
I0302 19:02:18.410722 22447428132992 run.py:483] Algo bellman_ford step 8483 current loss 1.064474, current_train_items 271488.
I0302 19:02:18.442742 22447428132992 run.py:483] Algo bellman_ford step 8484 current loss 1.094046, current_train_items 271520.
I0302 19:02:18.462446 22447428132992 run.py:483] Algo bellman_ford step 8485 current loss 0.593596, current_train_items 271552.
I0302 19:02:18.478291 22447428132992 run.py:483] Algo bellman_ford step 8486 current loss 0.700608, current_train_items 271584.
I0302 19:02:18.501356 22447428132992 run.py:483] Algo bellman_ford step 8487 current loss 0.919391, current_train_items 271616.
I0302 19:02:18.533600 22447428132992 run.py:483] Algo bellman_ford step 8488 current loss 1.050278, current_train_items 271648.
I0302 19:02:18.567188 22447428132992 run.py:483] Algo bellman_ford step 8489 current loss 1.057538, current_train_items 271680.
I0302 19:02:18.586955 22447428132992 run.py:483] Algo bellman_ford step 8490 current loss 0.595766, current_train_items 271712.
I0302 19:02:18.602803 22447428132992 run.py:483] Algo bellman_ford step 8491 current loss 0.723357, current_train_items 271744.
I0302 19:02:18.626589 22447428132992 run.py:483] Algo bellman_ford step 8492 current loss 1.041710, current_train_items 271776.
I0302 19:02:18.657633 22447428132992 run.py:483] Algo bellman_ford step 8493 current loss 0.947051, current_train_items 271808.
I0302 19:02:18.689946 22447428132992 run.py:483] Algo bellman_ford step 8494 current loss 1.096251, current_train_items 271840.
I0302 19:02:18.709439 22447428132992 run.py:483] Algo bellman_ford step 8495 current loss 0.575636, current_train_items 271872.
I0302 19:02:18.725941 22447428132992 run.py:483] Algo bellman_ford step 8496 current loss 0.892102, current_train_items 271904.
I0302 19:02:18.749565 22447428132992 run.py:483] Algo bellman_ford step 8497 current loss 0.848696, current_train_items 271936.
I0302 19:02:18.780916 22447428132992 run.py:483] Algo bellman_ford step 8498 current loss 0.952112, current_train_items 271968.
I0302 19:02:18.812739 22447428132992 run.py:483] Algo bellman_ford step 8499 current loss 1.116957, current_train_items 272000.
I0302 19:02:18.832465 22447428132992 run.py:483] Algo bellman_ford step 8500 current loss 0.662011, current_train_items 272032.
I0302 19:02:18.840339 22447428132992 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.8837890625, 'score': 0.8837890625, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:02:18.840498 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.884, val scores are: bellman_ford: 0.884
I0302 19:02:18.857503 22447428132992 run.py:483] Algo bellman_ford step 8501 current loss 0.806879, current_train_items 272064.
I0302 19:02:18.882512 22447428132992 run.py:483] Algo bellman_ford step 8502 current loss 0.893839, current_train_items 272096.
I0302 19:02:18.913072 22447428132992 run.py:483] Algo bellman_ford step 8503 current loss 0.926162, current_train_items 272128.
I0302 19:02:18.946094 22447428132992 run.py:483] Algo bellman_ford step 8504 current loss 1.091149, current_train_items 272160.
I0302 19:02:18.966065 22447428132992 run.py:483] Algo bellman_ford step 8505 current loss 0.716751, current_train_items 272192.
I0302 19:02:18.981361 22447428132992 run.py:483] Algo bellman_ford step 8506 current loss 0.704475, current_train_items 272224.
I0302 19:02:19.003835 22447428132992 run.py:483] Algo bellman_ford step 8507 current loss 0.844814, current_train_items 272256.
I0302 19:02:19.035512 22447428132992 run.py:483] Algo bellman_ford step 8508 current loss 1.099110, current_train_items 272288.
I0302 19:02:19.070373 22447428132992 run.py:483] Algo bellman_ford step 8509 current loss 1.136591, current_train_items 272320.
I0302 19:02:19.090037 22447428132992 run.py:483] Algo bellman_ford step 8510 current loss 0.569664, current_train_items 272352.
I0302 19:02:19.105656 22447428132992 run.py:483] Algo bellman_ford step 8511 current loss 0.751219, current_train_items 272384.
I0302 19:02:19.130211 22447428132992 run.py:483] Algo bellman_ford step 8512 current loss 0.927723, current_train_items 272416.
I0302 19:02:19.161036 22447428132992 run.py:483] Algo bellman_ford step 8513 current loss 1.115149, current_train_items 272448.
I0302 19:02:19.195158 22447428132992 run.py:483] Algo bellman_ford step 8514 current loss 1.049166, current_train_items 272480.
I0302 19:02:19.214559 22447428132992 run.py:483] Algo bellman_ford step 8515 current loss 0.607716, current_train_items 272512.
I0302 19:02:19.230584 22447428132992 run.py:483] Algo bellman_ford step 8516 current loss 0.646387, current_train_items 272544.
I0302 19:02:19.254365 22447428132992 run.py:483] Algo bellman_ford step 8517 current loss 0.908827, current_train_items 272576.
I0302 19:02:19.286997 22447428132992 run.py:483] Algo bellman_ford step 8518 current loss 0.997692, current_train_items 272608.
I0302 19:02:19.319586 22447428132992 run.py:483] Algo bellman_ford step 8519 current loss 1.134201, current_train_items 272640.
I0302 19:02:19.339390 22447428132992 run.py:483] Algo bellman_ford step 8520 current loss 0.534621, current_train_items 272672.
I0302 19:02:19.355192 22447428132992 run.py:483] Algo bellman_ford step 8521 current loss 0.748008, current_train_items 272704.
I0302 19:02:19.378571 22447428132992 run.py:483] Algo bellman_ford step 8522 current loss 0.813544, current_train_items 272736.
I0302 19:02:19.409600 22447428132992 run.py:483] Algo bellman_ford step 8523 current loss 1.139063, current_train_items 272768.
I0302 19:02:19.443629 22447428132992 run.py:483] Algo bellman_ford step 8524 current loss 1.212318, current_train_items 272800.
I0302 19:02:19.463246 22447428132992 run.py:483] Algo bellman_ford step 8525 current loss 0.597342, current_train_items 272832.
I0302 19:02:19.479353 22447428132992 run.py:483] Algo bellman_ford step 8526 current loss 0.802448, current_train_items 272864.
I0302 19:02:19.502810 22447428132992 run.py:483] Algo bellman_ford step 8527 current loss 0.960574, current_train_items 272896.
I0302 19:02:19.533605 22447428132992 run.py:483] Algo bellman_ford step 8528 current loss 0.848599, current_train_items 272928.
I0302 19:02:19.567690 22447428132992 run.py:483] Algo bellman_ford step 8529 current loss 1.086438, current_train_items 272960.
I0302 19:02:19.587389 22447428132992 run.py:483] Algo bellman_ford step 8530 current loss 0.539551, current_train_items 272992.
I0302 19:02:19.603589 22447428132992 run.py:483] Algo bellman_ford step 8531 current loss 0.749701, current_train_items 273024.
I0302 19:02:19.626032 22447428132992 run.py:483] Algo bellman_ford step 8532 current loss 0.888092, current_train_items 273056.
I0302 19:02:19.656678 22447428132992 run.py:483] Algo bellman_ford step 8533 current loss 0.968887, current_train_items 273088.
I0302 19:02:19.689122 22447428132992 run.py:483] Algo bellman_ford step 8534 current loss 1.094906, current_train_items 273120.
I0302 19:02:19.708522 22447428132992 run.py:483] Algo bellman_ford step 8535 current loss 0.774679, current_train_items 273152.
I0302 19:02:19.724609 22447428132992 run.py:483] Algo bellman_ford step 8536 current loss 0.829368, current_train_items 273184.
I0302 19:02:19.748318 22447428132992 run.py:483] Algo bellman_ford step 8537 current loss 0.932307, current_train_items 273216.
I0302 19:02:19.779574 22447428132992 run.py:483] Algo bellman_ford step 8538 current loss 0.892835, current_train_items 273248.
I0302 19:02:19.815184 22447428132992 run.py:483] Algo bellman_ford step 8539 current loss 1.237013, current_train_items 273280.
I0302 19:02:19.834935 22447428132992 run.py:483] Algo bellman_ford step 8540 current loss 0.721945, current_train_items 273312.
I0302 19:02:19.850928 22447428132992 run.py:483] Algo bellman_ford step 8541 current loss 0.834983, current_train_items 273344.
I0302 19:02:19.874937 22447428132992 run.py:483] Algo bellman_ford step 8542 current loss 0.959545, current_train_items 273376.
I0302 19:02:19.907054 22447428132992 run.py:483] Algo bellman_ford step 8543 current loss 0.982780, current_train_items 273408.
I0302 19:02:19.940369 22447428132992 run.py:483] Algo bellman_ford step 8544 current loss 1.345906, current_train_items 273440.
I0302 19:02:19.959796 22447428132992 run.py:483] Algo bellman_ford step 8545 current loss 0.590596, current_train_items 273472.
I0302 19:02:19.975478 22447428132992 run.py:483] Algo bellman_ford step 8546 current loss 0.771627, current_train_items 273504.
I0302 19:02:19.999446 22447428132992 run.py:483] Algo bellman_ford step 8547 current loss 0.998379, current_train_items 273536.
I0302 19:02:20.030702 22447428132992 run.py:483] Algo bellman_ford step 8548 current loss 0.989584, current_train_items 273568.
I0302 19:02:20.064621 22447428132992 run.py:483] Algo bellman_ford step 8549 current loss 1.338740, current_train_items 273600.
I0302 19:02:20.084152 22447428132992 run.py:483] Algo bellman_ford step 8550 current loss 0.565804, current_train_items 273632.
I0302 19:02:20.092253 22447428132992 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.865234375, 'score': 0.865234375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:02:20.092358 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.865, val scores are: bellman_ford: 0.865
I0302 19:02:20.109408 22447428132992 run.py:483] Algo bellman_ford step 8551 current loss 0.745451, current_train_items 273664.
I0302 19:02:20.133853 22447428132992 run.py:483] Algo bellman_ford step 8552 current loss 0.983224, current_train_items 273696.
I0302 19:02:20.165495 22447428132992 run.py:483] Algo bellman_ford step 8553 current loss 1.002297, current_train_items 273728.
I0302 19:02:20.197114 22447428132992 run.py:483] Algo bellman_ford step 8554 current loss 1.010597, current_train_items 273760.
I0302 19:02:20.216994 22447428132992 run.py:483] Algo bellman_ford step 8555 current loss 0.536685, current_train_items 273792.
I0302 19:02:20.232672 22447428132992 run.py:483] Algo bellman_ford step 8556 current loss 0.715162, current_train_items 273824.
I0302 19:02:20.255889 22447428132992 run.py:483] Algo bellman_ford step 8557 current loss 0.936272, current_train_items 273856.
I0302 19:02:20.287025 22447428132992 run.py:483] Algo bellman_ford step 8558 current loss 0.975262, current_train_items 273888.
I0302 19:02:20.321527 22447428132992 run.py:483] Algo bellman_ford step 8559 current loss 1.121133, current_train_items 273920.
I0302 19:02:20.341135 22447428132992 run.py:483] Algo bellman_ford step 8560 current loss 0.585241, current_train_items 273952.
I0302 19:02:20.357618 22447428132992 run.py:483] Algo bellman_ford step 8561 current loss 0.727832, current_train_items 273984.
I0302 19:02:20.380816 22447428132992 run.py:483] Algo bellman_ford step 8562 current loss 0.910872, current_train_items 274016.
I0302 19:02:20.413284 22447428132992 run.py:483] Algo bellman_ford step 8563 current loss 0.935926, current_train_items 274048.
I0302 19:02:20.445306 22447428132992 run.py:483] Algo bellman_ford step 8564 current loss 1.117525, current_train_items 274080.
I0302 19:02:20.464441 22447428132992 run.py:483] Algo bellman_ford step 8565 current loss 0.499362, current_train_items 274112.
I0302 19:02:20.481138 22447428132992 run.py:483] Algo bellman_ford step 8566 current loss 0.813291, current_train_items 274144.
I0302 19:02:20.506166 22447428132992 run.py:483] Algo bellman_ford step 8567 current loss 0.996106, current_train_items 274176.
I0302 19:02:20.537445 22447428132992 run.py:483] Algo bellman_ford step 8568 current loss 1.074624, current_train_items 274208.
I0302 19:02:20.571355 22447428132992 run.py:483] Algo bellman_ford step 8569 current loss 1.082502, current_train_items 274240.
I0302 19:02:20.591096 22447428132992 run.py:483] Algo bellman_ford step 8570 current loss 0.521515, current_train_items 274272.
I0302 19:02:20.607334 22447428132992 run.py:483] Algo bellman_ford step 8571 current loss 0.794530, current_train_items 274304.
I0302 19:02:20.630165 22447428132992 run.py:483] Algo bellman_ford step 8572 current loss 0.860495, current_train_items 274336.
I0302 19:02:20.661143 22447428132992 run.py:483] Algo bellman_ford step 8573 current loss 0.890055, current_train_items 274368.
I0302 19:02:20.693068 22447428132992 run.py:483] Algo bellman_ford step 8574 current loss 1.068528, current_train_items 274400.
I0302 19:02:20.712889 22447428132992 run.py:483] Algo bellman_ford step 8575 current loss 0.673374, current_train_items 274432.
I0302 19:02:20.728982 22447428132992 run.py:483] Algo bellman_ford step 8576 current loss 0.670144, current_train_items 274464.
I0302 19:02:20.752496 22447428132992 run.py:483] Algo bellman_ford step 8577 current loss 0.864208, current_train_items 274496.
I0302 19:02:20.784060 22447428132992 run.py:483] Algo bellman_ford step 8578 current loss 0.971241, current_train_items 274528.
I0302 19:02:20.817712 22447428132992 run.py:483] Algo bellman_ford step 8579 current loss 1.151896, current_train_items 274560.
I0302 19:02:20.837076 22447428132992 run.py:483] Algo bellman_ford step 8580 current loss 0.651499, current_train_items 274592.
I0302 19:02:20.853190 22447428132992 run.py:483] Algo bellman_ford step 8581 current loss 0.777972, current_train_items 274624.
I0302 19:02:20.875715 22447428132992 run.py:483] Algo bellman_ford step 8582 current loss 0.851622, current_train_items 274656.
I0302 19:02:20.906951 22447428132992 run.py:483] Algo bellman_ford step 8583 current loss 0.932202, current_train_items 274688.
I0302 19:02:20.941476 22447428132992 run.py:483] Algo bellman_ford step 8584 current loss 1.056687, current_train_items 274720.
I0302 19:02:20.961058 22447428132992 run.py:483] Algo bellman_ford step 8585 current loss 0.573671, current_train_items 274752.
I0302 19:02:20.977076 22447428132992 run.py:483] Algo bellman_ford step 8586 current loss 0.793536, current_train_items 274784.
I0302 19:02:21.001643 22447428132992 run.py:483] Algo bellman_ford step 8587 current loss 0.986069, current_train_items 274816.
I0302 19:02:21.032795 22447428132992 run.py:483] Algo bellman_ford step 8588 current loss 0.952790, current_train_items 274848.
I0302 19:02:21.065236 22447428132992 run.py:483] Algo bellman_ford step 8589 current loss 1.216585, current_train_items 274880.
I0302 19:02:21.085262 22447428132992 run.py:483] Algo bellman_ford step 8590 current loss 0.565753, current_train_items 274912.
I0302 19:02:21.100690 22447428132992 run.py:483] Algo bellman_ford step 8591 current loss 0.675149, current_train_items 274944.
I0302 19:02:21.123919 22447428132992 run.py:483] Algo bellman_ford step 8592 current loss 0.928384, current_train_items 274976.
I0302 19:02:21.155636 22447428132992 run.py:483] Algo bellman_ford step 8593 current loss 1.034791, current_train_items 275008.
I0302 19:02:21.189041 22447428132992 run.py:483] Algo bellman_ford step 8594 current loss 1.154950, current_train_items 275040.
I0302 19:02:21.208227 22447428132992 run.py:483] Algo bellman_ford step 8595 current loss 0.503787, current_train_items 275072.
I0302 19:02:21.224071 22447428132992 run.py:483] Algo bellman_ford step 8596 current loss 0.688132, current_train_items 275104.
I0302 19:02:21.247601 22447428132992 run.py:483] Algo bellman_ford step 8597 current loss 0.984639, current_train_items 275136.
I0302 19:02:21.277549 22447428132992 run.py:483] Algo bellman_ford step 8598 current loss 0.938761, current_train_items 275168.
I0302 19:02:21.311662 22447428132992 run.py:483] Algo bellman_ford step 8599 current loss 1.183197, current_train_items 275200.
I0302 19:02:21.331372 22447428132992 run.py:483] Algo bellman_ford step 8600 current loss 0.572837, current_train_items 275232.
I0302 19:02:21.339137 22447428132992 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:02:21.339240 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:21.356039 22447428132992 run.py:483] Algo bellman_ford step 8601 current loss 0.804089, current_train_items 275264.
I0302 19:02:21.380555 22447428132992 run.py:483] Algo bellman_ford step 8602 current loss 1.012255, current_train_items 275296.
I0302 19:02:21.413327 22447428132992 run.py:483] Algo bellman_ford step 8603 current loss 0.897390, current_train_items 275328.
I0302 19:02:21.447313 22447428132992 run.py:483] Algo bellman_ford step 8604 current loss 1.030776, current_train_items 275360.
I0302 19:02:21.467267 22447428132992 run.py:483] Algo bellman_ford step 8605 current loss 0.571981, current_train_items 275392.
I0302 19:02:21.482480 22447428132992 run.py:483] Algo bellman_ford step 8606 current loss 0.689188, current_train_items 275424.
I0302 19:02:21.505394 22447428132992 run.py:483] Algo bellman_ford step 8607 current loss 0.916316, current_train_items 275456.
I0302 19:02:21.536488 22447428132992 run.py:483] Algo bellman_ford step 8608 current loss 1.015821, current_train_items 275488.
I0302 19:02:21.571439 22447428132992 run.py:483] Algo bellman_ford step 8609 current loss 1.220203, current_train_items 275520.
I0302 19:02:21.590917 22447428132992 run.py:483] Algo bellman_ford step 8610 current loss 0.590332, current_train_items 275552.
I0302 19:02:21.607000 22447428132992 run.py:483] Algo bellman_ford step 8611 current loss 0.723931, current_train_items 275584.
I0302 19:02:21.630955 22447428132992 run.py:483] Algo bellman_ford step 8612 current loss 0.930942, current_train_items 275616.
I0302 19:02:21.662414 22447428132992 run.py:483] Algo bellman_ford step 8613 current loss 0.967174, current_train_items 275648.
I0302 19:02:21.694164 22447428132992 run.py:483] Algo bellman_ford step 8614 current loss 1.028407, current_train_items 275680.
I0302 19:02:21.713570 22447428132992 run.py:483] Algo bellman_ford step 8615 current loss 0.745060, current_train_items 275712.
I0302 19:02:21.729654 22447428132992 run.py:483] Algo bellman_ford step 8616 current loss 0.760004, current_train_items 275744.
I0302 19:02:21.753183 22447428132992 run.py:483] Algo bellman_ford step 8617 current loss 0.885938, current_train_items 275776.
I0302 19:02:21.785217 22447428132992 run.py:483] Algo bellman_ford step 8618 current loss 1.113644, current_train_items 275808.
I0302 19:02:21.818660 22447428132992 run.py:483] Algo bellman_ford step 8619 current loss 1.122601, current_train_items 275840.
I0302 19:02:21.837966 22447428132992 run.py:483] Algo bellman_ford step 8620 current loss 0.956105, current_train_items 275872.
I0302 19:02:21.854077 22447428132992 run.py:483] Algo bellman_ford step 8621 current loss 0.808361, current_train_items 275904.
I0302 19:02:21.878105 22447428132992 run.py:483] Algo bellman_ford step 8622 current loss 0.922251, current_train_items 275936.
I0302 19:02:21.910135 22447428132992 run.py:483] Algo bellman_ford step 8623 current loss 0.901639, current_train_items 275968.
I0302 19:02:21.943166 22447428132992 run.py:483] Algo bellman_ford step 8624 current loss 0.945283, current_train_items 276000.
I0302 19:02:21.962750 22447428132992 run.py:483] Algo bellman_ford step 8625 current loss 0.654738, current_train_items 276032.
I0302 19:02:21.978889 22447428132992 run.py:483] Algo bellman_ford step 8626 current loss 0.787065, current_train_items 276064.
I0302 19:02:22.002677 22447428132992 run.py:483] Algo bellman_ford step 8627 current loss 0.874745, current_train_items 276096.
I0302 19:02:22.034962 22447428132992 run.py:483] Algo bellman_ford step 8628 current loss 1.017476, current_train_items 276128.
I0302 19:02:22.067814 22447428132992 run.py:483] Algo bellman_ford step 8629 current loss 1.098235, current_train_items 276160.
I0302 19:02:22.087747 22447428132992 run.py:483] Algo bellman_ford step 8630 current loss 0.686082, current_train_items 276192.
I0302 19:02:22.103879 22447428132992 run.py:483] Algo bellman_ford step 8631 current loss 0.703674, current_train_items 276224.
I0302 19:02:22.127246 22447428132992 run.py:483] Algo bellman_ford step 8632 current loss 0.996031, current_train_items 276256.
I0302 19:02:22.158116 22447428132992 run.py:483] Algo bellman_ford step 8633 current loss 0.911480, current_train_items 276288.
I0302 19:02:22.191468 22447428132992 run.py:483] Algo bellman_ford step 8634 current loss 1.124200, current_train_items 276320.
I0302 19:02:22.211028 22447428132992 run.py:483] Algo bellman_ford step 8635 current loss 0.611492, current_train_items 276352.
I0302 19:02:22.227089 22447428132992 run.py:483] Algo bellman_ford step 8636 current loss 0.719835, current_train_items 276384.
I0302 19:02:22.251274 22447428132992 run.py:483] Algo bellman_ford step 8637 current loss 0.944512, current_train_items 276416.
I0302 19:02:22.281751 22447428132992 run.py:483] Algo bellman_ford step 8638 current loss 0.940957, current_train_items 276448.
I0302 19:02:22.315915 22447428132992 run.py:483] Algo bellman_ford step 8639 current loss 1.076973, current_train_items 276480.
I0302 19:02:22.335399 22447428132992 run.py:483] Algo bellman_ford step 8640 current loss 0.546978, current_train_items 276512.
I0302 19:02:22.351840 22447428132992 run.py:483] Algo bellman_ford step 8641 current loss 0.792289, current_train_items 276544.
I0302 19:02:22.375457 22447428132992 run.py:483] Algo bellman_ford step 8642 current loss 0.897353, current_train_items 276576.
I0302 19:02:22.407001 22447428132992 run.py:483] Algo bellman_ford step 8643 current loss 0.966742, current_train_items 276608.
I0302 19:02:22.440460 22447428132992 run.py:483] Algo bellman_ford step 8644 current loss 1.138964, current_train_items 276640.
I0302 19:02:22.459795 22447428132992 run.py:483] Algo bellman_ford step 8645 current loss 0.714442, current_train_items 276672.
I0302 19:02:22.475861 22447428132992 run.py:483] Algo bellman_ford step 8646 current loss 0.698729, current_train_items 276704.
I0302 19:02:22.499525 22447428132992 run.py:483] Algo bellman_ford step 8647 current loss 0.962804, current_train_items 276736.
I0302 19:02:22.531549 22447428132992 run.py:483] Algo bellman_ford step 8648 current loss 0.994307, current_train_items 276768.
I0302 19:02:22.564370 22447428132992 run.py:483] Algo bellman_ford step 8649 current loss 1.048490, current_train_items 276800.
I0302 19:02:22.583717 22447428132992 run.py:483] Algo bellman_ford step 8650 current loss 0.742234, current_train_items 276832.
I0302 19:02:22.591858 22447428132992 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:02:22.591962 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:22.608764 22447428132992 run.py:483] Algo bellman_ford step 8651 current loss 0.694302, current_train_items 276864.
I0302 19:02:22.632763 22447428132992 run.py:483] Algo bellman_ford step 8652 current loss 0.817228, current_train_items 276896.
I0302 19:02:22.665028 22447428132992 run.py:483] Algo bellman_ford step 8653 current loss 0.892838, current_train_items 276928.
I0302 19:02:22.700134 22447428132992 run.py:483] Algo bellman_ford step 8654 current loss 1.070495, current_train_items 276960.
I0302 19:02:22.720178 22447428132992 run.py:483] Algo bellman_ford step 8655 current loss 0.542813, current_train_items 276992.
I0302 19:02:22.736060 22447428132992 run.py:483] Algo bellman_ford step 8656 current loss 0.741325, current_train_items 277024.
I0302 19:02:22.761369 22447428132992 run.py:483] Algo bellman_ford step 8657 current loss 0.916665, current_train_items 277056.
I0302 19:02:22.793620 22447428132992 run.py:483] Algo bellman_ford step 8658 current loss 1.049367, current_train_items 277088.
I0302 19:02:22.827236 22447428132992 run.py:483] Algo bellman_ford step 8659 current loss 1.045896, current_train_items 277120.
I0302 19:02:22.847504 22447428132992 run.py:483] Algo bellman_ford step 8660 current loss 0.594862, current_train_items 277152.
I0302 19:02:22.863465 22447428132992 run.py:483] Algo bellman_ford step 8661 current loss 0.716996, current_train_items 277184.
I0302 19:02:22.886972 22447428132992 run.py:483] Algo bellman_ford step 8662 current loss 0.841816, current_train_items 277216.
I0302 19:02:22.918087 22447428132992 run.py:483] Algo bellman_ford step 8663 current loss 1.044469, current_train_items 277248.
I0302 19:02:22.952708 22447428132992 run.py:483] Algo bellman_ford step 8664 current loss 1.003038, current_train_items 277280.
I0302 19:02:22.972462 22447428132992 run.py:483] Algo bellman_ford step 8665 current loss 0.656932, current_train_items 277312.
I0302 19:02:22.988119 22447428132992 run.py:483] Algo bellman_ford step 8666 current loss 0.686637, current_train_items 277344.
I0302 19:02:23.011772 22447428132992 run.py:483] Algo bellman_ford step 8667 current loss 0.829393, current_train_items 277376.
I0302 19:02:23.042784 22447428132992 run.py:483] Algo bellman_ford step 8668 current loss 0.922414, current_train_items 277408.
I0302 19:02:23.075609 22447428132992 run.py:483] Algo bellman_ford step 8669 current loss 1.059024, current_train_items 277440.
I0302 19:02:23.095280 22447428132992 run.py:483] Algo bellman_ford step 8670 current loss 0.897844, current_train_items 277472.
I0302 19:02:23.111674 22447428132992 run.py:483] Algo bellman_ford step 8671 current loss 0.964667, current_train_items 277504.
I0302 19:02:23.134790 22447428132992 run.py:483] Algo bellman_ford step 8672 current loss 0.892881, current_train_items 277536.
I0302 19:02:23.165962 22447428132992 run.py:483] Algo bellman_ford step 8673 current loss 1.044924, current_train_items 277568.
I0302 19:02:23.197835 22447428132992 run.py:483] Algo bellman_ford step 8674 current loss 1.046566, current_train_items 277600.
I0302 19:02:23.217563 22447428132992 run.py:483] Algo bellman_ford step 8675 current loss 0.604214, current_train_items 277632.
I0302 19:02:23.233560 22447428132992 run.py:483] Algo bellman_ford step 8676 current loss 0.827398, current_train_items 277664.
I0302 19:02:23.257472 22447428132992 run.py:483] Algo bellman_ford step 8677 current loss 0.948150, current_train_items 277696.
I0302 19:02:23.289210 22447428132992 run.py:483] Algo bellman_ford step 8678 current loss 1.029701, current_train_items 277728.
I0302 19:02:23.322332 22447428132992 run.py:483] Algo bellman_ford step 8679 current loss 1.080983, current_train_items 277760.
I0302 19:02:23.341899 22447428132992 run.py:483] Algo bellman_ford step 8680 current loss 0.592890, current_train_items 277792.
I0302 19:02:23.358105 22447428132992 run.py:483] Algo bellman_ford step 8681 current loss 0.746303, current_train_items 277824.
I0302 19:02:23.382318 22447428132992 run.py:483] Algo bellman_ford step 8682 current loss 0.975015, current_train_items 277856.
I0302 19:02:23.413518 22447428132992 run.py:483] Algo bellman_ford step 8683 current loss 1.007563, current_train_items 277888.
I0302 19:02:23.446586 22447428132992 run.py:483] Algo bellman_ford step 8684 current loss 1.126534, current_train_items 277920.
I0302 19:02:23.466662 22447428132992 run.py:483] Algo bellman_ford step 8685 current loss 0.571546, current_train_items 277952.
I0302 19:02:23.482560 22447428132992 run.py:483] Algo bellman_ford step 8686 current loss 0.748977, current_train_items 277984.
I0302 19:02:23.506630 22447428132992 run.py:483] Algo bellman_ford step 8687 current loss 1.074429, current_train_items 278016.
I0302 19:02:23.538308 22447428132992 run.py:483] Algo bellman_ford step 8688 current loss 1.010353, current_train_items 278048.
I0302 19:02:23.570892 22447428132992 run.py:483] Algo bellman_ford step 8689 current loss 1.080365, current_train_items 278080.
I0302 19:02:23.590623 22447428132992 run.py:483] Algo bellman_ford step 8690 current loss 0.561431, current_train_items 278112.
I0302 19:02:23.606949 22447428132992 run.py:483] Algo bellman_ford step 8691 current loss 0.785880, current_train_items 278144.
I0302 19:02:23.630353 22447428132992 run.py:483] Algo bellman_ford step 8692 current loss 0.967626, current_train_items 278176.
I0302 19:02:23.661689 22447428132992 run.py:483] Algo bellman_ford step 8693 current loss 0.989982, current_train_items 278208.
I0302 19:02:23.694733 22447428132992 run.py:483] Algo bellman_ford step 8694 current loss 1.142955, current_train_items 278240.
I0302 19:02:23.714150 22447428132992 run.py:483] Algo bellman_ford step 8695 current loss 0.504679, current_train_items 278272.
I0302 19:02:23.730044 22447428132992 run.py:483] Algo bellman_ford step 8696 current loss 0.733285, current_train_items 278304.
I0302 19:02:23.754125 22447428132992 run.py:483] Algo bellman_ford step 8697 current loss 0.970500, current_train_items 278336.
I0302 19:02:23.785727 22447428132992 run.py:483] Algo bellman_ford step 8698 current loss 0.994438, current_train_items 278368.
I0302 19:02:23.820801 22447428132992 run.py:483] Algo bellman_ford step 8699 current loss 1.189224, current_train_items 278400.
I0302 19:02:23.840599 22447428132992 run.py:483] Algo bellman_ford step 8700 current loss 0.568110, current_train_items 278432.
I0302 19:02:23.848466 22447428132992 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:02:23.848575 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:02:23.865204 22447428132992 run.py:483] Algo bellman_ford step 8701 current loss 0.777984, current_train_items 278464.
I0302 19:02:23.888867 22447428132992 run.py:483] Algo bellman_ford step 8702 current loss 0.837037, current_train_items 278496.
I0302 19:02:23.921031 22447428132992 run.py:483] Algo bellman_ford step 8703 current loss 1.007129, current_train_items 278528.
I0302 19:02:23.956351 22447428132992 run.py:483] Algo bellman_ford step 8704 current loss 1.209735, current_train_items 278560.
I0302 19:02:23.976396 22447428132992 run.py:483] Algo bellman_ford step 8705 current loss 0.604411, current_train_items 278592.
I0302 19:02:23.992005 22447428132992 run.py:483] Algo bellman_ford step 8706 current loss 0.782949, current_train_items 278624.
I0302 19:02:24.015533 22447428132992 run.py:483] Algo bellman_ford step 8707 current loss 0.920855, current_train_items 278656.
I0302 19:02:24.047479 22447428132992 run.py:483] Algo bellman_ford step 8708 current loss 0.987626, current_train_items 278688.
I0302 19:02:24.080831 22447428132992 run.py:483] Algo bellman_ford step 8709 current loss 1.080792, current_train_items 278720.
I0302 19:02:24.100007 22447428132992 run.py:483] Algo bellman_ford step 8710 current loss 0.658263, current_train_items 278752.
I0302 19:02:24.116262 22447428132992 run.py:483] Algo bellman_ford step 8711 current loss 0.809070, current_train_items 278784.
I0302 19:02:24.139889 22447428132992 run.py:483] Algo bellman_ford step 8712 current loss 0.930708, current_train_items 278816.
I0302 19:02:24.173210 22447428132992 run.py:483] Algo bellman_ford step 8713 current loss 1.105579, current_train_items 278848.
I0302 19:02:24.205417 22447428132992 run.py:483] Algo bellman_ford step 8714 current loss 1.062979, current_train_items 278880.
I0302 19:02:24.224863 22447428132992 run.py:483] Algo bellman_ford step 8715 current loss 0.553620, current_train_items 278912.
I0302 19:02:24.241264 22447428132992 run.py:483] Algo bellman_ford step 8716 current loss 0.775650, current_train_items 278944.
I0302 19:02:24.264370 22447428132992 run.py:483] Algo bellman_ford step 8717 current loss 0.821471, current_train_items 278976.
I0302 19:02:24.294799 22447428132992 run.py:483] Algo bellman_ford step 8718 current loss 0.948166, current_train_items 279008.
I0302 19:02:24.327168 22447428132992 run.py:483] Algo bellman_ford step 8719 current loss 1.148137, current_train_items 279040.
I0302 19:02:24.346931 22447428132992 run.py:483] Algo bellman_ford step 8720 current loss 0.549383, current_train_items 279072.
I0302 19:02:24.363359 22447428132992 run.py:483] Algo bellman_ford step 8721 current loss 0.762807, current_train_items 279104.
I0302 19:02:24.387671 22447428132992 run.py:483] Algo bellman_ford step 8722 current loss 0.908461, current_train_items 279136.
I0302 19:02:24.418439 22447428132992 run.py:483] Algo bellman_ford step 8723 current loss 0.940202, current_train_items 279168.
I0302 19:02:24.451430 22447428132992 run.py:483] Algo bellman_ford step 8724 current loss 0.991031, current_train_items 279200.
I0302 19:02:24.470992 22447428132992 run.py:483] Algo bellman_ford step 8725 current loss 0.604302, current_train_items 279232.
I0302 19:02:24.487290 22447428132992 run.py:483] Algo bellman_ford step 8726 current loss 0.743665, current_train_items 279264.
I0302 19:02:24.511127 22447428132992 run.py:483] Algo bellman_ford step 8727 current loss 0.905692, current_train_items 279296.
I0302 19:02:24.542020 22447428132992 run.py:483] Algo bellman_ford step 8728 current loss 0.983627, current_train_items 279328.
I0302 19:02:24.575148 22447428132992 run.py:483] Algo bellman_ford step 8729 current loss 1.272377, current_train_items 279360.
I0302 19:02:24.594461 22447428132992 run.py:483] Algo bellman_ford step 8730 current loss 0.633431, current_train_items 279392.
I0302 19:02:24.610071 22447428132992 run.py:483] Algo bellman_ford step 8731 current loss 0.596177, current_train_items 279424.
I0302 19:02:24.633639 22447428132992 run.py:483] Algo bellman_ford step 8732 current loss 0.855137, current_train_items 279456.
I0302 19:02:24.665280 22447428132992 run.py:483] Algo bellman_ford step 8733 current loss 0.956103, current_train_items 279488.
I0302 19:02:24.700106 22447428132992 run.py:483] Algo bellman_ford step 8734 current loss 1.141071, current_train_items 279520.
I0302 19:02:24.719504 22447428132992 run.py:483] Algo bellman_ford step 8735 current loss 0.892384, current_train_items 279552.
I0302 19:02:24.735504 22447428132992 run.py:483] Algo bellman_ford step 8736 current loss 0.703281, current_train_items 279584.
I0302 19:02:24.760174 22447428132992 run.py:483] Algo bellman_ford step 8737 current loss 0.891477, current_train_items 279616.
I0302 19:02:24.791117 22447428132992 run.py:483] Algo bellman_ford step 8738 current loss 0.927921, current_train_items 279648.
I0302 19:02:24.825172 22447428132992 run.py:483] Algo bellman_ford step 8739 current loss 1.058515, current_train_items 279680.
I0302 19:02:24.844722 22447428132992 run.py:483] Algo bellman_ford step 8740 current loss 0.846674, current_train_items 279712.
I0302 19:02:24.860608 22447428132992 run.py:483] Algo bellman_ford step 8741 current loss 0.831570, current_train_items 279744.
I0302 19:02:24.884258 22447428132992 run.py:483] Algo bellman_ford step 8742 current loss 0.976267, current_train_items 279776.
I0302 19:02:24.915997 22447428132992 run.py:483] Algo bellman_ford step 8743 current loss 1.020409, current_train_items 279808.
I0302 19:02:24.949483 22447428132992 run.py:483] Algo bellman_ford step 8744 current loss 1.173408, current_train_items 279840.
I0302 19:02:24.968882 22447428132992 run.py:483] Algo bellman_ford step 8745 current loss 0.616794, current_train_items 279872.
I0302 19:02:24.985134 22447428132992 run.py:483] Algo bellman_ford step 8746 current loss 0.740840, current_train_items 279904.
I0302 19:02:25.008435 22447428132992 run.py:483] Algo bellman_ford step 8747 current loss 0.940670, current_train_items 279936.
I0302 19:02:25.040657 22447428132992 run.py:483] Algo bellman_ford step 8748 current loss 1.062547, current_train_items 279968.
I0302 19:02:25.075266 22447428132992 run.py:483] Algo bellman_ford step 8749 current loss 1.129651, current_train_items 280000.
I0302 19:02:25.094330 22447428132992 run.py:483] Algo bellman_ford step 8750 current loss 0.567190, current_train_items 280032.
I0302 19:02:25.102416 22447428132992 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:02:25.102524 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:02:25.119226 22447428132992 run.py:483] Algo bellman_ford step 8751 current loss 0.754949, current_train_items 280064.
I0302 19:02:25.142765 22447428132992 run.py:483] Algo bellman_ford step 8752 current loss 0.850424, current_train_items 280096.
I0302 19:02:25.174879 22447428132992 run.py:483] Algo bellman_ford step 8753 current loss 1.099643, current_train_items 280128.
I0302 19:02:25.208861 22447428132992 run.py:483] Algo bellman_ford step 8754 current loss 1.127060, current_train_items 280160.
I0302 19:02:25.228990 22447428132992 run.py:483] Algo bellman_ford step 8755 current loss 0.567129, current_train_items 280192.
I0302 19:02:25.244703 22447428132992 run.py:483] Algo bellman_ford step 8756 current loss 0.636300, current_train_items 280224.
I0302 19:02:25.269404 22447428132992 run.py:483] Algo bellman_ford step 8757 current loss 0.976880, current_train_items 280256.
I0302 19:02:25.300552 22447428132992 run.py:483] Algo bellman_ford step 8758 current loss 0.965340, current_train_items 280288.
I0302 19:02:25.333928 22447428132992 run.py:483] Algo bellman_ford step 8759 current loss 1.137450, current_train_items 280320.
I0302 19:02:25.353678 22447428132992 run.py:483] Algo bellman_ford step 8760 current loss 0.609437, current_train_items 280352.
I0302 19:02:25.369946 22447428132992 run.py:483] Algo bellman_ford step 8761 current loss 0.778170, current_train_items 280384.
I0302 19:02:25.393643 22447428132992 run.py:483] Algo bellman_ford step 8762 current loss 0.955759, current_train_items 280416.
I0302 19:02:25.424636 22447428132992 run.py:483] Algo bellman_ford step 8763 current loss 1.042459, current_train_items 280448.
I0302 19:02:25.459503 22447428132992 run.py:483] Algo bellman_ford step 8764 current loss 1.115280, current_train_items 280480.
I0302 19:02:25.478775 22447428132992 run.py:483] Algo bellman_ford step 8765 current loss 0.558126, current_train_items 280512.
I0302 19:02:25.495232 22447428132992 run.py:483] Algo bellman_ford step 8766 current loss 0.711233, current_train_items 280544.
I0302 19:02:25.517715 22447428132992 run.py:483] Algo bellman_ford step 8767 current loss 0.910887, current_train_items 280576.
I0302 19:02:25.548353 22447428132992 run.py:483] Algo bellman_ford step 8768 current loss 1.004690, current_train_items 280608.
I0302 19:02:25.580323 22447428132992 run.py:483] Algo bellman_ford step 8769 current loss 0.999423, current_train_items 280640.
I0302 19:02:25.599959 22447428132992 run.py:483] Algo bellman_ford step 8770 current loss 0.590049, current_train_items 280672.
I0302 19:02:25.616350 22447428132992 run.py:483] Algo bellman_ford step 8771 current loss 0.758650, current_train_items 280704.
I0302 19:02:25.639926 22447428132992 run.py:483] Algo bellman_ford step 8772 current loss 0.897909, current_train_items 280736.
I0302 19:02:25.671502 22447428132992 run.py:483] Algo bellman_ford step 8773 current loss 1.083683, current_train_items 280768.
I0302 19:02:25.704144 22447428132992 run.py:483] Algo bellman_ford step 8774 current loss 1.114736, current_train_items 280800.
I0302 19:02:25.723701 22447428132992 run.py:483] Algo bellman_ford step 8775 current loss 0.673977, current_train_items 280832.
I0302 19:02:25.739849 22447428132992 run.py:483] Algo bellman_ford step 8776 current loss 0.729176, current_train_items 280864.
I0302 19:02:25.762530 22447428132992 run.py:483] Algo bellman_ford step 8777 current loss 0.896966, current_train_items 280896.
I0302 19:02:25.793335 22447428132992 run.py:483] Algo bellman_ford step 8778 current loss 0.866967, current_train_items 280928.
I0302 19:02:25.827606 22447428132992 run.py:483] Algo bellman_ford step 8779 current loss 1.045648, current_train_items 280960.
I0302 19:02:25.846758 22447428132992 run.py:483] Algo bellman_ford step 8780 current loss 0.565301, current_train_items 280992.
I0302 19:02:25.862712 22447428132992 run.py:483] Algo bellman_ford step 8781 current loss 0.798598, current_train_items 281024.
I0302 19:02:25.886153 22447428132992 run.py:483] Algo bellman_ford step 8782 current loss 0.909003, current_train_items 281056.
I0302 19:02:25.918130 22447428132992 run.py:483] Algo bellman_ford step 8783 current loss 1.037770, current_train_items 281088.
I0302 19:02:25.952178 22447428132992 run.py:483] Algo bellman_ford step 8784 current loss 1.137463, current_train_items 281120.
I0302 19:02:25.971862 22447428132992 run.py:483] Algo bellman_ford step 8785 current loss 0.504220, current_train_items 281152.
I0302 19:02:25.987972 22447428132992 run.py:483] Algo bellman_ford step 8786 current loss 0.743746, current_train_items 281184.
I0302 19:02:26.011770 22447428132992 run.py:483] Algo bellman_ford step 8787 current loss 0.969937, current_train_items 281216.
I0302 19:02:26.042980 22447428132992 run.py:483] Algo bellman_ford step 8788 current loss 0.931713, current_train_items 281248.
I0302 19:02:26.076457 22447428132992 run.py:483] Algo bellman_ford step 8789 current loss 1.121937, current_train_items 281280.
I0302 19:02:26.096482 22447428132992 run.py:483] Algo bellman_ford step 8790 current loss 0.588220, current_train_items 281312.
I0302 19:02:26.113016 22447428132992 run.py:483] Algo bellman_ford step 8791 current loss 0.693134, current_train_items 281344.
I0302 19:02:26.136068 22447428132992 run.py:483] Algo bellman_ford step 8792 current loss 0.873719, current_train_items 281376.
I0302 19:02:26.167530 22447428132992 run.py:483] Algo bellman_ford step 8793 current loss 0.966959, current_train_items 281408.
I0302 19:02:26.201820 22447428132992 run.py:483] Algo bellman_ford step 8794 current loss 1.067309, current_train_items 281440.
I0302 19:02:26.221222 22447428132992 run.py:483] Algo bellman_ford step 8795 current loss 0.605002, current_train_items 281472.
I0302 19:02:26.237291 22447428132992 run.py:483] Algo bellman_ford step 8796 current loss 0.680184, current_train_items 281504.
I0302 19:02:26.260461 22447428132992 run.py:483] Algo bellman_ford step 8797 current loss 0.867080, current_train_items 281536.
I0302 19:02:26.292021 22447428132992 run.py:483] Algo bellman_ford step 8798 current loss 1.009014, current_train_items 281568.
I0302 19:02:26.327046 22447428132992 run.py:483] Algo bellman_ford step 8799 current loss 1.138024, current_train_items 281600.
I0302 19:02:26.346611 22447428132992 run.py:483] Algo bellman_ford step 8800 current loss 0.619216, current_train_items 281632.
I0302 19:02:26.354553 22447428132992 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.869140625, 'score': 0.869140625, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:02:26.354657 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.869, val scores are: bellman_ford: 0.869
I0302 19:02:26.370964 22447428132992 run.py:483] Algo bellman_ford step 8801 current loss 0.841947, current_train_items 281664.
I0302 19:02:26.394877 22447428132992 run.py:483] Algo bellman_ford step 8802 current loss 0.938067, current_train_items 281696.
I0302 19:02:26.427481 22447428132992 run.py:483] Algo bellman_ford step 8803 current loss 0.997071, current_train_items 281728.
I0302 19:02:26.461181 22447428132992 run.py:483] Algo bellman_ford step 8804 current loss 0.971888, current_train_items 281760.
I0302 19:02:26.480985 22447428132992 run.py:483] Algo bellman_ford step 8805 current loss 0.715046, current_train_items 281792.
I0302 19:02:26.496817 22447428132992 run.py:483] Algo bellman_ford step 8806 current loss 0.863646, current_train_items 281824.
I0302 19:02:26.520548 22447428132992 run.py:483] Algo bellman_ford step 8807 current loss 0.944493, current_train_items 281856.
I0302 19:02:26.551414 22447428132992 run.py:483] Algo bellman_ford step 8808 current loss 0.969844, current_train_items 281888.
I0302 19:02:26.587272 22447428132992 run.py:483] Algo bellman_ford step 8809 current loss 1.050329, current_train_items 281920.
I0302 19:02:26.607069 22447428132992 run.py:483] Algo bellman_ford step 8810 current loss 0.821770, current_train_items 281952.
I0302 19:02:26.623359 22447428132992 run.py:483] Algo bellman_ford step 8811 current loss 0.946648, current_train_items 281984.
I0302 19:02:26.646911 22447428132992 run.py:483] Algo bellman_ford step 8812 current loss 0.857522, current_train_items 282016.
I0302 19:02:26.678524 22447428132992 run.py:483] Algo bellman_ford step 8813 current loss 0.956306, current_train_items 282048.
I0302 19:02:26.711900 22447428132992 run.py:483] Algo bellman_ford step 8814 current loss 1.114772, current_train_items 282080.
I0302 19:02:26.731112 22447428132992 run.py:483] Algo bellman_ford step 8815 current loss 0.618925, current_train_items 282112.
I0302 19:02:26.747294 22447428132992 run.py:483] Algo bellman_ford step 8816 current loss 0.739721, current_train_items 282144.
I0302 19:02:26.770497 22447428132992 run.py:483] Algo bellman_ford step 8817 current loss 0.901852, current_train_items 282176.
I0302 19:02:26.800701 22447428132992 run.py:483] Algo bellman_ford step 8818 current loss 1.020079, current_train_items 282208.
I0302 19:02:26.837635 22447428132992 run.py:483] Algo bellman_ford step 8819 current loss 1.244782, current_train_items 282240.
I0302 19:02:26.857062 22447428132992 run.py:483] Algo bellman_ford step 8820 current loss 0.588725, current_train_items 282272.
I0302 19:02:26.873030 22447428132992 run.py:483] Algo bellman_ford step 8821 current loss 0.698253, current_train_items 282304.
I0302 19:02:26.896366 22447428132992 run.py:483] Algo bellman_ford step 8822 current loss 0.920710, current_train_items 282336.
I0302 19:02:26.928263 22447428132992 run.py:483] Algo bellman_ford step 8823 current loss 1.094495, current_train_items 282368.
I0302 19:02:26.960789 22447428132992 run.py:483] Algo bellman_ford step 8824 current loss 1.178639, current_train_items 282400.
I0302 19:02:26.980049 22447428132992 run.py:483] Algo bellman_ford step 8825 current loss 0.619947, current_train_items 282432.
I0302 19:02:26.996279 22447428132992 run.py:483] Algo bellman_ford step 8826 current loss 0.771127, current_train_items 282464.
I0302 19:02:27.020076 22447428132992 run.py:483] Algo bellman_ford step 8827 current loss 0.948582, current_train_items 282496.
I0302 19:02:27.051396 22447428132992 run.py:483] Algo bellman_ford step 8828 current loss 0.955559, current_train_items 282528.
I0302 19:02:27.084346 22447428132992 run.py:483] Algo bellman_ford step 8829 current loss 1.136953, current_train_items 282560.
I0302 19:02:27.104124 22447428132992 run.py:483] Algo bellman_ford step 8830 current loss 0.621089, current_train_items 282592.
I0302 19:02:27.120033 22447428132992 run.py:483] Algo bellman_ford step 8831 current loss 0.755904, current_train_items 282624.
I0302 19:02:27.144053 22447428132992 run.py:483] Algo bellman_ford step 8832 current loss 0.989573, current_train_items 282656.
I0302 19:02:27.173700 22447428132992 run.py:483] Algo bellman_ford step 8833 current loss 0.990183, current_train_items 282688.
I0302 19:02:27.205907 22447428132992 run.py:483] Algo bellman_ford step 8834 current loss 1.179936, current_train_items 282720.
I0302 19:02:27.225795 22447428132992 run.py:483] Algo bellman_ford step 8835 current loss 0.551965, current_train_items 282752.
I0302 19:02:27.241640 22447428132992 run.py:483] Algo bellman_ford step 8836 current loss 0.700834, current_train_items 282784.
I0302 19:02:27.265836 22447428132992 run.py:483] Algo bellman_ford step 8837 current loss 0.904226, current_train_items 282816.
I0302 19:02:27.297552 22447428132992 run.py:483] Algo bellman_ford step 8838 current loss 0.977893, current_train_items 282848.
I0302 19:02:27.330742 22447428132992 run.py:483] Algo bellman_ford step 8839 current loss 1.105716, current_train_items 282880.
I0302 19:02:27.350312 22447428132992 run.py:483] Algo bellman_ford step 8840 current loss 0.640463, current_train_items 282912.
I0302 19:02:27.366637 22447428132992 run.py:483] Algo bellman_ford step 8841 current loss 0.694575, current_train_items 282944.
I0302 19:02:27.391016 22447428132992 run.py:483] Algo bellman_ford step 8842 current loss 0.901517, current_train_items 282976.
I0302 19:02:27.422206 22447428132992 run.py:483] Algo bellman_ford step 8843 current loss 1.053813, current_train_items 283008.
I0302 19:02:27.457106 22447428132992 run.py:483] Algo bellman_ford step 8844 current loss 1.361726, current_train_items 283040.
I0302 19:02:27.476605 22447428132992 run.py:483] Algo bellman_ford step 8845 current loss 0.640087, current_train_items 283072.
I0302 19:02:27.492573 22447428132992 run.py:483] Algo bellman_ford step 8846 current loss 0.725362, current_train_items 283104.
I0302 19:02:27.515983 22447428132992 run.py:483] Algo bellman_ford step 8847 current loss 0.865202, current_train_items 283136.
I0302 19:02:27.545776 22447428132992 run.py:483] Algo bellman_ford step 8848 current loss 0.894193, current_train_items 283168.
I0302 19:02:27.579157 22447428132992 run.py:483] Algo bellman_ford step 8849 current loss 1.055440, current_train_items 283200.
I0302 19:02:27.598397 22447428132992 run.py:483] Algo bellman_ford step 8850 current loss 0.529222, current_train_items 283232.
I0302 19:02:27.606552 22447428132992 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:02:27.606659 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:02:27.623332 22447428132992 run.py:483] Algo bellman_ford step 8851 current loss 0.704896, current_train_items 283264.
I0302 19:02:27.648007 22447428132992 run.py:483] Algo bellman_ford step 8852 current loss 0.908751, current_train_items 283296.
I0302 19:02:27.679937 22447428132992 run.py:483] Algo bellman_ford step 8853 current loss 1.026011, current_train_items 283328.
I0302 19:02:27.714022 22447428132992 run.py:483] Algo bellman_ford step 8854 current loss 1.123685, current_train_items 283360.
I0302 19:02:27.733989 22447428132992 run.py:483] Algo bellman_ford step 8855 current loss 0.561047, current_train_items 283392.
I0302 19:02:27.750010 22447428132992 run.py:483] Algo bellman_ford step 8856 current loss 0.732187, current_train_items 283424.
I0302 19:02:27.773479 22447428132992 run.py:483] Algo bellman_ford step 8857 current loss 0.902212, current_train_items 283456.
I0302 19:02:27.805486 22447428132992 run.py:483] Algo bellman_ford step 8858 current loss 1.014502, current_train_items 283488.
I0302 19:02:27.837338 22447428132992 run.py:483] Algo bellman_ford step 8859 current loss 1.158394, current_train_items 283520.
I0302 19:02:27.857104 22447428132992 run.py:483] Algo bellman_ford step 8860 current loss 0.632773, current_train_items 283552.
I0302 19:02:27.873513 22447428132992 run.py:483] Algo bellman_ford step 8861 current loss 0.761692, current_train_items 283584.
I0302 19:02:27.897333 22447428132992 run.py:483] Algo bellman_ford step 8862 current loss 0.935817, current_train_items 283616.
I0302 19:02:27.927951 22447428132992 run.py:483] Algo bellman_ford step 8863 current loss 0.905038, current_train_items 283648.
I0302 19:02:27.960760 22447428132992 run.py:483] Algo bellman_ford step 8864 current loss 1.078620, current_train_items 283680.
I0302 19:02:27.980354 22447428132992 run.py:483] Algo bellman_ford step 8865 current loss 0.714658, current_train_items 283712.
I0302 19:02:27.996280 22447428132992 run.py:483] Algo bellman_ford step 8866 current loss 0.685502, current_train_items 283744.
I0302 19:02:28.019195 22447428132992 run.py:483] Algo bellman_ford step 8867 current loss 0.914384, current_train_items 283776.
I0302 19:02:28.051306 22447428132992 run.py:483] Algo bellman_ford step 8868 current loss 1.037424, current_train_items 283808.
I0302 19:02:28.084319 22447428132992 run.py:483] Algo bellman_ford step 8869 current loss 1.018215, current_train_items 283840.
I0302 19:02:28.104559 22447428132992 run.py:483] Algo bellman_ford step 8870 current loss 0.578350, current_train_items 283872.
I0302 19:02:28.120707 22447428132992 run.py:483] Algo bellman_ford step 8871 current loss 0.659382, current_train_items 283904.
I0302 19:02:28.144691 22447428132992 run.py:483] Algo bellman_ford step 8872 current loss 0.969945, current_train_items 283936.
I0302 19:02:28.175742 22447428132992 run.py:483] Algo bellman_ford step 8873 current loss 0.937768, current_train_items 283968.
I0302 19:02:28.210308 22447428132992 run.py:483] Algo bellman_ford step 8874 current loss 1.317932, current_train_items 284000.
I0302 19:02:28.230110 22447428132992 run.py:483] Algo bellman_ford step 8875 current loss 0.508767, current_train_items 284032.
I0302 19:02:28.246368 22447428132992 run.py:483] Algo bellman_ford step 8876 current loss 0.722019, current_train_items 284064.
I0302 19:02:28.269176 22447428132992 run.py:483] Algo bellman_ford step 8877 current loss 0.771194, current_train_items 284096.
I0302 19:02:28.300280 22447428132992 run.py:483] Algo bellman_ford step 8878 current loss 0.892257, current_train_items 284128.
I0302 19:02:28.334246 22447428132992 run.py:483] Algo bellman_ford step 8879 current loss 1.169551, current_train_items 284160.
I0302 19:02:28.353861 22447428132992 run.py:483] Algo bellman_ford step 8880 current loss 0.636693, current_train_items 284192.
I0302 19:02:28.369889 22447428132992 run.py:483] Algo bellman_ford step 8881 current loss 0.754100, current_train_items 284224.
I0302 19:02:28.392636 22447428132992 run.py:483] Algo bellman_ford step 8882 current loss 0.839471, current_train_items 284256.
I0302 19:02:28.424954 22447428132992 run.py:483] Algo bellman_ford step 8883 current loss 1.021976, current_train_items 284288.
I0302 19:02:28.458570 22447428132992 run.py:483] Algo bellman_ford step 8884 current loss 1.084547, current_train_items 284320.
I0302 19:02:28.478408 22447428132992 run.py:483] Algo bellman_ford step 8885 current loss 0.600119, current_train_items 284352.
I0302 19:02:28.495059 22447428132992 run.py:483] Algo bellman_ford step 8886 current loss 0.890102, current_train_items 284384.
I0302 19:02:28.518560 22447428132992 run.py:483] Algo bellman_ford step 8887 current loss 0.810500, current_train_items 284416.
I0302 19:02:28.549265 22447428132992 run.py:483] Algo bellman_ford step 8888 current loss 1.023530, current_train_items 284448.
I0302 19:02:28.583581 22447428132992 run.py:483] Algo bellman_ford step 8889 current loss 1.074776, current_train_items 284480.
I0302 19:02:28.603444 22447428132992 run.py:483] Algo bellman_ford step 8890 current loss 0.498662, current_train_items 284512.
I0302 19:02:28.619142 22447428132992 run.py:483] Algo bellman_ford step 8891 current loss 0.708426, current_train_items 284544.
I0302 19:02:28.642712 22447428132992 run.py:483] Algo bellman_ford step 8892 current loss 0.862750, current_train_items 284576.
I0302 19:02:28.675739 22447428132992 run.py:483] Algo bellman_ford step 8893 current loss 1.042956, current_train_items 284608.
I0302 19:02:28.709203 22447428132992 run.py:483] Algo bellman_ford step 8894 current loss 1.115295, current_train_items 284640.
I0302 19:02:28.728715 22447428132992 run.py:483] Algo bellman_ford step 8895 current loss 0.568822, current_train_items 284672.
I0302 19:02:28.744624 22447428132992 run.py:483] Algo bellman_ford step 8896 current loss 0.976529, current_train_items 284704.
I0302 19:02:28.767575 22447428132992 run.py:483] Algo bellman_ford step 8897 current loss 0.887921, current_train_items 284736.
I0302 19:02:28.799879 22447428132992 run.py:483] Algo bellman_ford step 8898 current loss 1.010209, current_train_items 284768.
I0302 19:02:28.835044 22447428132992 run.py:483] Algo bellman_ford step 8899 current loss 1.001259, current_train_items 284800.
I0302 19:02:28.855076 22447428132992 run.py:483] Algo bellman_ford step 8900 current loss 0.500265, current_train_items 284832.
I0302 19:02:28.862821 22447428132992 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:02:28.862927 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:02:28.879468 22447428132992 run.py:483] Algo bellman_ford step 8901 current loss 0.696480, current_train_items 284864.
I0302 19:02:28.903867 22447428132992 run.py:483] Algo bellman_ford step 8902 current loss 0.899759, current_train_items 284896.
I0302 19:02:28.934609 22447428132992 run.py:483] Algo bellman_ford step 8903 current loss 0.981650, current_train_items 284928.
I0302 19:02:28.970297 22447428132992 run.py:483] Algo bellman_ford step 8904 current loss 1.110731, current_train_items 284960.
I0302 19:02:28.990158 22447428132992 run.py:483] Algo bellman_ford step 8905 current loss 0.713278, current_train_items 284992.
I0302 19:02:29.006185 22447428132992 run.py:483] Algo bellman_ford step 8906 current loss 0.655533, current_train_items 285024.
I0302 19:02:29.029997 22447428132992 run.py:483] Algo bellman_ford step 8907 current loss 0.878949, current_train_items 285056.
I0302 19:02:29.060908 22447428132992 run.py:483] Algo bellman_ford step 8908 current loss 0.853929, current_train_items 285088.
I0302 19:02:29.092675 22447428132992 run.py:483] Algo bellman_ford step 8909 current loss 1.112523, current_train_items 285120.
I0302 19:02:29.112427 22447428132992 run.py:483] Algo bellman_ford step 8910 current loss 0.696710, current_train_items 285152.
I0302 19:02:29.128391 22447428132992 run.py:483] Algo bellman_ford step 8911 current loss 0.663206, current_train_items 285184.
I0302 19:02:29.152522 22447428132992 run.py:483] Algo bellman_ford step 8912 current loss 0.913438, current_train_items 285216.
I0302 19:02:29.183094 22447428132992 run.py:483] Algo bellman_ford step 8913 current loss 0.893562, current_train_items 285248.
I0302 19:02:29.217596 22447428132992 run.py:483] Algo bellman_ford step 8914 current loss 1.108811, current_train_items 285280.
I0302 19:02:29.237373 22447428132992 run.py:483] Algo bellman_ford step 8915 current loss 0.644261, current_train_items 285312.
I0302 19:02:29.253153 22447428132992 run.py:483] Algo bellman_ford step 8916 current loss 0.655227, current_train_items 285344.
I0302 19:02:29.276934 22447428132992 run.py:483] Algo bellman_ford step 8917 current loss 0.909573, current_train_items 285376.
I0302 19:02:29.308349 22447428132992 run.py:483] Algo bellman_ford step 8918 current loss 0.959290, current_train_items 285408.
I0302 19:02:29.341271 22447428132992 run.py:483] Algo bellman_ford step 8919 current loss 1.073861, current_train_items 285440.
I0302 19:02:29.360824 22447428132992 run.py:483] Algo bellman_ford step 8920 current loss 0.615073, current_train_items 285472.
I0302 19:02:29.377097 22447428132992 run.py:483] Algo bellman_ford step 8921 current loss 0.753194, current_train_items 285504.
I0302 19:02:29.401474 22447428132992 run.py:483] Algo bellman_ford step 8922 current loss 0.909195, current_train_items 285536.
I0302 19:02:29.432433 22447428132992 run.py:483] Algo bellman_ford step 8923 current loss 0.856577, current_train_items 285568.
I0302 19:02:29.466338 22447428132992 run.py:483] Algo bellman_ford step 8924 current loss 1.097415, current_train_items 285600.
I0302 19:02:29.485875 22447428132992 run.py:483] Algo bellman_ford step 8925 current loss 0.729535, current_train_items 285632.
I0302 19:02:29.501624 22447428132992 run.py:483] Algo bellman_ford step 8926 current loss 0.969488, current_train_items 285664.
I0302 19:02:29.525555 22447428132992 run.py:483] Algo bellman_ford step 8927 current loss 0.912287, current_train_items 285696.
I0302 19:02:29.555777 22447428132992 run.py:483] Algo bellman_ford step 8928 current loss 0.850970, current_train_items 285728.
I0302 19:02:29.589408 22447428132992 run.py:483] Algo bellman_ford step 8929 current loss 1.082198, current_train_items 285760.
I0302 19:02:29.609103 22447428132992 run.py:483] Algo bellman_ford step 8930 current loss 0.729223, current_train_items 285792.
I0302 19:02:29.625680 22447428132992 run.py:483] Algo bellman_ford step 8931 current loss 0.790220, current_train_items 285824.
I0302 19:02:29.649124 22447428132992 run.py:483] Algo bellman_ford step 8932 current loss 0.802402, current_train_items 285856.
I0302 19:02:29.681762 22447428132992 run.py:483] Algo bellman_ford step 8933 current loss 1.018007, current_train_items 285888.
I0302 19:02:29.715818 22447428132992 run.py:483] Algo bellman_ford step 8934 current loss 1.054678, current_train_items 285920.
I0302 19:02:29.735414 22447428132992 run.py:483] Algo bellman_ford step 8935 current loss 0.754991, current_train_items 285952.
I0302 19:02:29.751314 22447428132992 run.py:483] Algo bellman_ford step 8936 current loss 0.744363, current_train_items 285984.
I0302 19:02:29.775125 22447428132992 run.py:483] Algo bellman_ford step 8937 current loss 0.980385, current_train_items 286016.
I0302 19:02:29.805953 22447428132992 run.py:483] Algo bellman_ford step 8938 current loss 1.021168, current_train_items 286048.
I0302 19:02:29.838403 22447428132992 run.py:483] Algo bellman_ford step 8939 current loss 0.974515, current_train_items 286080.
I0302 19:02:29.858262 22447428132992 run.py:483] Algo bellman_ford step 8940 current loss 0.940661, current_train_items 286112.
I0302 19:02:29.874167 22447428132992 run.py:483] Algo bellman_ford step 8941 current loss 1.075923, current_train_items 286144.
I0302 19:02:29.898075 22447428132992 run.py:483] Algo bellman_ford step 8942 current loss 0.965584, current_train_items 286176.
I0302 19:02:29.930498 22447428132992 run.py:483] Algo bellman_ford step 8943 current loss 1.060068, current_train_items 286208.
I0302 19:02:29.964158 22447428132992 run.py:483] Algo bellman_ford step 8944 current loss 1.073830, current_train_items 286240.
I0302 19:02:29.983726 22447428132992 run.py:483] Algo bellman_ford step 8945 current loss 0.595541, current_train_items 286272.
I0302 19:02:29.999753 22447428132992 run.py:483] Algo bellman_ford step 8946 current loss 0.712000, current_train_items 286304.
I0302 19:02:30.023161 22447428132992 run.py:483] Algo bellman_ford step 8947 current loss 0.924546, current_train_items 286336.
I0302 19:02:30.055155 22447428132992 run.py:483] Algo bellman_ford step 8948 current loss 1.142592, current_train_items 286368.
I0302 19:02:30.088457 22447428132992 run.py:483] Algo bellman_ford step 8949 current loss 1.124840, current_train_items 286400.
I0302 19:02:30.107955 22447428132992 run.py:483] Algo bellman_ford step 8950 current loss 0.498889, current_train_items 286432.
I0302 19:02:30.115966 22447428132992 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.8935546875, 'score': 0.8935546875, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:02:30.116072 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.894, val scores are: bellman_ford: 0.894
I0302 19:02:30.132985 22447428132992 run.py:483] Algo bellman_ford step 8951 current loss 0.756748, current_train_items 286464.
I0302 19:02:30.156341 22447428132992 run.py:483] Algo bellman_ford step 8952 current loss 0.935408, current_train_items 286496.
I0302 19:02:30.190492 22447428132992 run.py:483] Algo bellman_ford step 8953 current loss 1.009932, current_train_items 286528.
I0302 19:02:30.225513 22447428132992 run.py:483] Algo bellman_ford step 8954 current loss 1.169343, current_train_items 286560.
I0302 19:02:30.245281 22447428132992 run.py:483] Algo bellman_ford step 8955 current loss 0.605490, current_train_items 286592.
I0302 19:02:30.261637 22447428132992 run.py:483] Algo bellman_ford step 8956 current loss 0.886093, current_train_items 286624.
I0302 19:02:30.285617 22447428132992 run.py:483] Algo bellman_ford step 8957 current loss 0.994086, current_train_items 286656.
I0302 19:02:30.316457 22447428132992 run.py:483] Algo bellman_ford step 8958 current loss 1.002405, current_train_items 286688.
I0302 19:02:30.351024 22447428132992 run.py:483] Algo bellman_ford step 8959 current loss 1.244240, current_train_items 286720.
I0302 19:02:30.370873 22447428132992 run.py:483] Algo bellman_ford step 8960 current loss 0.734747, current_train_items 286752.
I0302 19:02:30.386493 22447428132992 run.py:483] Algo bellman_ford step 8961 current loss 0.719436, current_train_items 286784.
I0302 19:02:30.410632 22447428132992 run.py:483] Algo bellman_ford step 8962 current loss 0.857525, current_train_items 286816.
I0302 19:02:30.441966 22447428132992 run.py:483] Algo bellman_ford step 8963 current loss 0.960237, current_train_items 286848.
I0302 19:02:30.473967 22447428132992 run.py:483] Algo bellman_ford step 8964 current loss 1.047840, current_train_items 286880.
I0302 19:02:30.493348 22447428132992 run.py:483] Algo bellman_ford step 8965 current loss 0.648231, current_train_items 286912.
I0302 19:02:30.509586 22447428132992 run.py:483] Algo bellman_ford step 8966 current loss 0.758746, current_train_items 286944.
I0302 19:02:30.533212 22447428132992 run.py:483] Algo bellman_ford step 8967 current loss 0.998341, current_train_items 286976.
I0302 19:02:30.564599 22447428132992 run.py:483] Algo bellman_ford step 8968 current loss 1.055456, current_train_items 287008.
I0302 19:02:30.597474 22447428132992 run.py:483] Algo bellman_ford step 8969 current loss 1.179039, current_train_items 287040.
I0302 19:02:30.616917 22447428132992 run.py:483] Algo bellman_ford step 8970 current loss 0.531496, current_train_items 287072.
I0302 19:02:30.632905 22447428132992 run.py:483] Algo bellman_ford step 8971 current loss 0.760448, current_train_items 287104.
I0302 19:02:30.656247 22447428132992 run.py:483] Algo bellman_ford step 8972 current loss 0.878387, current_train_items 287136.
I0302 19:02:30.686929 22447428132992 run.py:483] Algo bellman_ford step 8973 current loss 0.957406, current_train_items 287168.
I0302 19:02:30.719391 22447428132992 run.py:483] Algo bellman_ford step 8974 current loss 1.106525, current_train_items 287200.
I0302 19:02:30.739155 22447428132992 run.py:483] Algo bellman_ford step 8975 current loss 0.629459, current_train_items 287232.
I0302 19:02:30.756107 22447428132992 run.py:483] Algo bellman_ford step 8976 current loss 0.748612, current_train_items 287264.
I0302 19:02:30.778303 22447428132992 run.py:483] Algo bellman_ford step 8977 current loss 0.904759, current_train_items 287296.
I0302 19:02:30.809584 22447428132992 run.py:483] Algo bellman_ford step 8978 current loss 0.988693, current_train_items 287328.
I0302 19:02:30.842180 22447428132992 run.py:483] Algo bellman_ford step 8979 current loss 1.144719, current_train_items 287360.
I0302 19:02:30.861632 22447428132992 run.py:483] Algo bellman_ford step 8980 current loss 0.621392, current_train_items 287392.
I0302 19:02:30.877835 22447428132992 run.py:483] Algo bellman_ford step 8981 current loss 0.676953, current_train_items 287424.
I0302 19:02:30.901334 22447428132992 run.py:483] Algo bellman_ford step 8982 current loss 0.853913, current_train_items 287456.
I0302 19:02:30.932852 22447428132992 run.py:483] Algo bellman_ford step 8983 current loss 0.963598, current_train_items 287488.
I0302 19:02:30.966081 22447428132992 run.py:483] Algo bellman_ford step 8984 current loss 1.050597, current_train_items 287520.
I0302 19:02:30.985769 22447428132992 run.py:483] Algo bellman_ford step 8985 current loss 0.553169, current_train_items 287552.
I0302 19:02:31.001935 22447428132992 run.py:483] Algo bellman_ford step 8986 current loss 0.747590, current_train_items 287584.
I0302 19:02:31.024410 22447428132992 run.py:483] Algo bellman_ford step 8987 current loss 0.819342, current_train_items 287616.
I0302 19:02:31.054096 22447428132992 run.py:483] Algo bellman_ford step 8988 current loss 0.931869, current_train_items 287648.
I0302 19:02:31.089060 22447428132992 run.py:483] Algo bellman_ford step 8989 current loss 1.136630, current_train_items 287680.
I0302 19:02:31.108659 22447428132992 run.py:483] Algo bellman_ford step 8990 current loss 0.554678, current_train_items 287712.
I0302 19:02:31.124425 22447428132992 run.py:483] Algo bellman_ford step 8991 current loss 0.702418, current_train_items 287744.
I0302 19:02:31.147227 22447428132992 run.py:483] Algo bellman_ford step 8992 current loss 0.880675, current_train_items 287776.
I0302 19:02:31.178255 22447428132992 run.py:483] Algo bellman_ford step 8993 current loss 0.980769, current_train_items 287808.
I0302 19:02:31.211254 22447428132992 run.py:483] Algo bellman_ford step 8994 current loss 1.166885, current_train_items 287840.
I0302 19:02:31.230355 22447428132992 run.py:483] Algo bellman_ford step 8995 current loss 0.607529, current_train_items 287872.
I0302 19:02:31.246505 22447428132992 run.py:483] Algo bellman_ford step 8996 current loss 0.634725, current_train_items 287904.
I0302 19:02:31.270407 22447428132992 run.py:483] Algo bellman_ford step 8997 current loss 0.908162, current_train_items 287936.
I0302 19:02:31.303107 22447428132992 run.py:483] Algo bellman_ford step 8998 current loss 1.000871, current_train_items 287968.
I0302 19:02:31.335036 22447428132992 run.py:483] Algo bellman_ford step 8999 current loss 1.093788, current_train_items 288000.
I0302 19:02:31.354802 22447428132992 run.py:483] Algo bellman_ford step 9000 current loss 0.521416, current_train_items 288032.
I0302 19:02:31.362670 22447428132992 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:02:31.362776 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:31.379661 22447428132992 run.py:483] Algo bellman_ford step 9001 current loss 0.720313, current_train_items 288064.
I0302 19:02:31.402926 22447428132992 run.py:483] Algo bellman_ford step 9002 current loss 0.798392, current_train_items 288096.
I0302 19:02:31.433666 22447428132992 run.py:483] Algo bellman_ford step 9003 current loss 0.980496, current_train_items 288128.
I0302 19:02:31.467868 22447428132992 run.py:483] Algo bellman_ford step 9004 current loss 1.071169, current_train_items 288160.
I0302 19:02:31.487744 22447428132992 run.py:483] Algo bellman_ford step 9005 current loss 0.571230, current_train_items 288192.
I0302 19:02:31.503753 22447428132992 run.py:483] Algo bellman_ford step 9006 current loss 0.725492, current_train_items 288224.
I0302 19:02:31.526553 22447428132992 run.py:483] Algo bellman_ford step 9007 current loss 0.892232, current_train_items 288256.
I0302 19:02:31.558807 22447428132992 run.py:483] Algo bellman_ford step 9008 current loss 1.142468, current_train_items 288288.
I0302 19:02:31.592524 22447428132992 run.py:483] Algo bellman_ford step 9009 current loss 1.128875, current_train_items 288320.
I0302 19:02:31.612040 22447428132992 run.py:483] Algo bellman_ford step 9010 current loss 0.566712, current_train_items 288352.
I0302 19:02:31.628089 22447428132992 run.py:483] Algo bellman_ford step 9011 current loss 0.699391, current_train_items 288384.
I0302 19:02:31.651043 22447428132992 run.py:483] Algo bellman_ford step 9012 current loss 0.869132, current_train_items 288416.
I0302 19:02:31.681354 22447428132992 run.py:483] Algo bellman_ford step 9013 current loss 0.916170, current_train_items 288448.
I0302 19:02:31.716689 22447428132992 run.py:483] Algo bellman_ford step 9014 current loss 1.147368, current_train_items 288480.
I0302 19:02:31.735897 22447428132992 run.py:483] Algo bellman_ford step 9015 current loss 0.574906, current_train_items 288512.
I0302 19:02:31.752115 22447428132992 run.py:483] Algo bellman_ford step 9016 current loss 0.750413, current_train_items 288544.
I0302 19:02:31.776200 22447428132992 run.py:483] Algo bellman_ford step 9017 current loss 0.981387, current_train_items 288576.
I0302 19:02:31.807516 22447428132992 run.py:483] Algo bellman_ford step 9018 current loss 0.979153, current_train_items 288608.
I0302 19:02:31.842348 22447428132992 run.py:483] Algo bellman_ford step 9019 current loss 1.135664, current_train_items 288640.
I0302 19:02:31.861625 22447428132992 run.py:483] Algo bellman_ford step 9020 current loss 0.525749, current_train_items 288672.
I0302 19:02:31.877648 22447428132992 run.py:483] Algo bellman_ford step 9021 current loss 0.763440, current_train_items 288704.
I0302 19:02:31.902550 22447428132992 run.py:483] Algo bellman_ford step 9022 current loss 0.939428, current_train_items 288736.
I0302 19:02:31.935013 22447428132992 run.py:483] Algo bellman_ford step 9023 current loss 1.057658, current_train_items 288768.
I0302 19:02:31.968692 22447428132992 run.py:483] Algo bellman_ford step 9024 current loss 1.276074, current_train_items 288800.
I0302 19:02:31.988171 22447428132992 run.py:483] Algo bellman_ford step 9025 current loss 0.561755, current_train_items 288832.
I0302 19:02:32.004003 22447428132992 run.py:483] Algo bellman_ford step 9026 current loss 0.747761, current_train_items 288864.
I0302 19:02:32.027835 22447428132992 run.py:483] Algo bellman_ford step 9027 current loss 0.907817, current_train_items 288896.
I0302 19:02:32.059899 22447428132992 run.py:483] Algo bellman_ford step 9028 current loss 0.861828, current_train_items 288928.
I0302 19:02:32.093750 22447428132992 run.py:483] Algo bellman_ford step 9029 current loss 1.093045, current_train_items 288960.
I0302 19:02:32.113837 22447428132992 run.py:483] Algo bellman_ford step 9030 current loss 0.873868, current_train_items 288992.
I0302 19:02:32.129885 22447428132992 run.py:483] Algo bellman_ford step 9031 current loss 0.693458, current_train_items 289024.
I0302 19:02:32.154449 22447428132992 run.py:483] Algo bellman_ford step 9032 current loss 0.964765, current_train_items 289056.
I0302 19:02:32.186226 22447428132992 run.py:483] Algo bellman_ford step 9033 current loss 0.940204, current_train_items 289088.
I0302 19:02:32.222327 22447428132992 run.py:483] Algo bellman_ford step 9034 current loss 1.206406, current_train_items 289120.
I0302 19:02:32.241478 22447428132992 run.py:483] Algo bellman_ford step 9035 current loss 0.706861, current_train_items 289152.
I0302 19:02:32.257223 22447428132992 run.py:483] Algo bellman_ford step 9036 current loss 0.888562, current_train_items 289184.
I0302 19:02:32.281728 22447428132992 run.py:483] Algo bellman_ford step 9037 current loss 0.925952, current_train_items 289216.
I0302 19:02:32.312914 22447428132992 run.py:483] Algo bellman_ford step 9038 current loss 1.076496, current_train_items 289248.
I0302 19:02:32.346069 22447428132992 run.py:483] Algo bellman_ford step 9039 current loss 1.227850, current_train_items 289280.
I0302 19:02:32.365390 22447428132992 run.py:483] Algo bellman_ford step 9040 current loss 0.608897, current_train_items 289312.
I0302 19:02:32.381740 22447428132992 run.py:483] Algo bellman_ford step 9041 current loss 0.820934, current_train_items 289344.
I0302 19:02:32.404699 22447428132992 run.py:483] Algo bellman_ford step 9042 current loss 0.882661, current_train_items 289376.
I0302 19:02:32.435743 22447428132992 run.py:483] Algo bellman_ford step 9043 current loss 0.918039, current_train_items 289408.
I0302 19:02:32.469540 22447428132992 run.py:483] Algo bellman_ford step 9044 current loss 1.116146, current_train_items 289440.
I0302 19:02:32.489015 22447428132992 run.py:483] Algo bellman_ford step 9045 current loss 0.506851, current_train_items 289472.
I0302 19:02:32.505345 22447428132992 run.py:483] Algo bellman_ford step 9046 current loss 0.742098, current_train_items 289504.
I0302 19:02:32.527881 22447428132992 run.py:483] Algo bellman_ford step 9047 current loss 0.952940, current_train_items 289536.
I0302 19:02:32.559518 22447428132992 run.py:483] Algo bellman_ford step 9048 current loss 0.969806, current_train_items 289568.
I0302 19:02:32.592890 22447428132992 run.py:483] Algo bellman_ford step 9049 current loss 1.072757, current_train_items 289600.
I0302 19:02:32.612372 22447428132992 run.py:483] Algo bellman_ford step 9050 current loss 0.537162, current_train_items 289632.
I0302 19:02:32.620470 22447428132992 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:02:32.620574 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:02:32.637001 22447428132992 run.py:483] Algo bellman_ford step 9051 current loss 0.671939, current_train_items 289664.
I0302 19:02:32.661474 22447428132992 run.py:483] Algo bellman_ford step 9052 current loss 0.935671, current_train_items 289696.
I0302 19:02:32.693438 22447428132992 run.py:483] Algo bellman_ford step 9053 current loss 1.090739, current_train_items 289728.
I0302 19:02:32.726130 22447428132992 run.py:483] Algo bellman_ford step 9054 current loss 1.011651, current_train_items 289760.
I0302 19:02:32.745963 22447428132992 run.py:483] Algo bellman_ford step 9055 current loss 0.607749, current_train_items 289792.
I0302 19:02:32.761636 22447428132992 run.py:483] Algo bellman_ford step 9056 current loss 0.674693, current_train_items 289824.
I0302 19:02:32.784789 22447428132992 run.py:483] Algo bellman_ford step 9057 current loss 0.947079, current_train_items 289856.
I0302 19:02:32.814625 22447428132992 run.py:483] Algo bellman_ford step 9058 current loss 0.968277, current_train_items 289888.
I0302 19:02:32.846283 22447428132992 run.py:483] Algo bellman_ford step 9059 current loss 1.007867, current_train_items 289920.
I0302 19:02:32.865885 22447428132992 run.py:483] Algo bellman_ford step 9060 current loss 0.673954, current_train_items 289952.
I0302 19:02:32.882532 22447428132992 run.py:483] Algo bellman_ford step 9061 current loss 0.748078, current_train_items 289984.
I0302 19:02:32.907121 22447428132992 run.py:483] Algo bellman_ford step 9062 current loss 1.039857, current_train_items 290016.
I0302 19:02:32.937049 22447428132992 run.py:483] Algo bellman_ford step 9063 current loss 0.900641, current_train_items 290048.
I0302 19:02:32.971224 22447428132992 run.py:483] Algo bellman_ford step 9064 current loss 1.236295, current_train_items 290080.
I0302 19:02:32.990828 22447428132992 run.py:483] Algo bellman_ford step 9065 current loss 0.566445, current_train_items 290112.
I0302 19:02:33.007007 22447428132992 run.py:483] Algo bellman_ford step 9066 current loss 0.799534, current_train_items 290144.
I0302 19:02:33.028766 22447428132992 run.py:483] Algo bellman_ford step 9067 current loss 0.838698, current_train_items 290176.
I0302 19:02:33.060147 22447428132992 run.py:483] Algo bellman_ford step 9068 current loss 0.949838, current_train_items 290208.
I0302 19:02:33.094280 22447428132992 run.py:483] Algo bellman_ford step 9069 current loss 1.117763, current_train_items 290240.
I0302 19:02:33.113916 22447428132992 run.py:483] Algo bellman_ford step 9070 current loss 0.529999, current_train_items 290272.
I0302 19:02:33.130327 22447428132992 run.py:483] Algo bellman_ford step 9071 current loss 0.816140, current_train_items 290304.
I0302 19:02:33.154360 22447428132992 run.py:483] Algo bellman_ford step 9072 current loss 0.854343, current_train_items 290336.
I0302 19:02:33.186071 22447428132992 run.py:483] Algo bellman_ford step 9073 current loss 0.917384, current_train_items 290368.
I0302 19:02:33.219496 22447428132992 run.py:483] Algo bellman_ford step 9074 current loss 1.042685, current_train_items 290400.
I0302 19:02:33.238874 22447428132992 run.py:483] Algo bellman_ford step 9075 current loss 0.561492, current_train_items 290432.
I0302 19:02:33.255048 22447428132992 run.py:483] Algo bellman_ford step 9076 current loss 0.718313, current_train_items 290464.
I0302 19:02:33.278609 22447428132992 run.py:483] Algo bellman_ford step 9077 current loss 0.957765, current_train_items 290496.
I0302 19:02:33.309782 22447428132992 run.py:483] Algo bellman_ford step 9078 current loss 0.886292, current_train_items 290528.
I0302 19:02:33.343815 22447428132992 run.py:483] Algo bellman_ford step 9079 current loss 1.179502, current_train_items 290560.
I0302 19:02:33.363176 22447428132992 run.py:483] Algo bellman_ford step 9080 current loss 0.772558, current_train_items 290592.
I0302 19:02:33.379116 22447428132992 run.py:483] Algo bellman_ford step 9081 current loss 0.995478, current_train_items 290624.
I0302 19:02:33.402119 22447428132992 run.py:483] Algo bellman_ford step 9082 current loss 0.872179, current_train_items 290656.
I0302 19:02:33.432852 22447428132992 run.py:483] Algo bellman_ford step 9083 current loss 1.006090, current_train_items 290688.
I0302 19:02:33.465181 22447428132992 run.py:483] Algo bellman_ford step 9084 current loss 1.007631, current_train_items 290720.
I0302 19:02:33.484806 22447428132992 run.py:483] Algo bellman_ford step 9085 current loss 0.706982, current_train_items 290752.
I0302 19:02:33.500947 22447428132992 run.py:483] Algo bellman_ford step 9086 current loss 0.712390, current_train_items 290784.
I0302 19:02:33.524723 22447428132992 run.py:483] Algo bellman_ford step 9087 current loss 0.905031, current_train_items 290816.
I0302 19:02:33.555232 22447428132992 run.py:483] Algo bellman_ford step 9088 current loss 0.966866, current_train_items 290848.
I0302 19:02:33.590193 22447428132992 run.py:483] Algo bellman_ford step 9089 current loss 1.218458, current_train_items 290880.
I0302 19:02:33.609559 22447428132992 run.py:483] Algo bellman_ford step 9090 current loss 0.573498, current_train_items 290912.
I0302 19:02:33.625596 22447428132992 run.py:483] Algo bellman_ford step 9091 current loss 0.754989, current_train_items 290944.
I0302 19:02:33.648469 22447428132992 run.py:483] Algo bellman_ford step 9092 current loss 0.800930, current_train_items 290976.
I0302 19:02:33.677960 22447428132992 run.py:483] Algo bellman_ford step 9093 current loss 0.922147, current_train_items 291008.
I0302 19:02:33.713505 22447428132992 run.py:483] Algo bellman_ford step 9094 current loss 1.214262, current_train_items 291040.
I0302 19:02:33.732922 22447428132992 run.py:483] Algo bellman_ford step 9095 current loss 0.540904, current_train_items 291072.
I0302 19:02:33.748929 22447428132992 run.py:483] Algo bellman_ford step 9096 current loss 0.735230, current_train_items 291104.
I0302 19:02:33.772141 22447428132992 run.py:483] Algo bellman_ford step 9097 current loss 0.872340, current_train_items 291136.
I0302 19:02:33.803692 22447428132992 run.py:483] Algo bellman_ford step 9098 current loss 0.996842, current_train_items 291168.
I0302 19:02:33.836277 22447428132992 run.py:483] Algo bellman_ford step 9099 current loss 1.067462, current_train_items 291200.
I0302 19:02:33.856265 22447428132992 run.py:483] Algo bellman_ford step 9100 current loss 0.591512, current_train_items 291232.
I0302 19:02:33.863977 22447428132992 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.880859375, 'score': 0.880859375, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:02:33.864084 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.881, val scores are: bellman_ford: 0.881
I0302 19:02:33.880683 22447428132992 run.py:483] Algo bellman_ford step 9101 current loss 0.682491, current_train_items 291264.
I0302 19:02:33.905507 22447428132992 run.py:483] Algo bellman_ford step 9102 current loss 0.989901, current_train_items 291296.
I0302 19:02:33.937118 22447428132992 run.py:483] Algo bellman_ford step 9103 current loss 0.926144, current_train_items 291328.
I0302 19:02:33.972487 22447428132992 run.py:483] Algo bellman_ford step 9104 current loss 1.130075, current_train_items 291360.
I0302 19:02:33.992369 22447428132992 run.py:483] Algo bellman_ford step 9105 current loss 0.552748, current_train_items 291392.
I0302 19:02:34.008450 22447428132992 run.py:483] Algo bellman_ford step 9106 current loss 0.790904, current_train_items 291424.
I0302 19:02:34.032459 22447428132992 run.py:483] Algo bellman_ford step 9107 current loss 0.918143, current_train_items 291456.
I0302 19:02:34.063997 22447428132992 run.py:483] Algo bellman_ford step 9108 current loss 0.996717, current_train_items 291488.
I0302 19:02:34.097693 22447428132992 run.py:483] Algo bellman_ford step 9109 current loss 1.063238, current_train_items 291520.
I0302 19:02:34.117155 22447428132992 run.py:483] Algo bellman_ford step 9110 current loss 0.636898, current_train_items 291552.
I0302 19:02:34.133158 22447428132992 run.py:483] Algo bellman_ford step 9111 current loss 0.657309, current_train_items 291584.
I0302 19:02:34.156249 22447428132992 run.py:483] Algo bellman_ford step 9112 current loss 1.017641, current_train_items 291616.
I0302 19:02:34.188894 22447428132992 run.py:483] Algo bellman_ford step 9113 current loss 1.024380, current_train_items 291648.
I0302 19:02:34.223515 22447428132992 run.py:483] Algo bellman_ford step 9114 current loss 1.152309, current_train_items 291680.
I0302 19:02:34.243056 22447428132992 run.py:483] Algo bellman_ford step 9115 current loss 0.659762, current_train_items 291712.
I0302 19:02:34.259435 22447428132992 run.py:483] Algo bellman_ford step 9116 current loss 0.708858, current_train_items 291744.
I0302 19:02:34.282836 22447428132992 run.py:483] Algo bellman_ford step 9117 current loss 0.954415, current_train_items 291776.
I0302 19:02:34.313637 22447428132992 run.py:483] Algo bellman_ford step 9118 current loss 1.000091, current_train_items 291808.
I0302 19:02:34.348405 22447428132992 run.py:483] Algo bellman_ford step 9119 current loss 1.118084, current_train_items 291840.
I0302 19:02:34.367782 22447428132992 run.py:483] Algo bellman_ford step 9120 current loss 0.589167, current_train_items 291872.
I0302 19:02:34.383853 22447428132992 run.py:483] Algo bellman_ford step 9121 current loss 0.700258, current_train_items 291904.
I0302 19:02:34.407083 22447428132992 run.py:483] Algo bellman_ford step 9122 current loss 0.942983, current_train_items 291936.
I0302 19:02:34.439753 22447428132992 run.py:483] Algo bellman_ford step 9123 current loss 1.104684, current_train_items 291968.
I0302 19:02:34.475180 22447428132992 run.py:483] Algo bellman_ford step 9124 current loss 1.042672, current_train_items 292000.
I0302 19:02:34.495079 22447428132992 run.py:483] Algo bellman_ford step 9125 current loss 0.657124, current_train_items 292032.
I0302 19:02:34.510926 22447428132992 run.py:483] Algo bellman_ford step 9126 current loss 0.704828, current_train_items 292064.
I0302 19:02:34.533576 22447428132992 run.py:483] Algo bellman_ford step 9127 current loss 0.883347, current_train_items 292096.
I0302 19:02:34.565830 22447428132992 run.py:483] Algo bellman_ford step 9128 current loss 1.112918, current_train_items 292128.
I0302 19:02:34.598795 22447428132992 run.py:483] Algo bellman_ford step 9129 current loss 1.196157, current_train_items 292160.
I0302 19:02:34.618542 22447428132992 run.py:483] Algo bellman_ford step 9130 current loss 0.879343, current_train_items 292192.
I0302 19:02:34.634541 22447428132992 run.py:483] Algo bellman_ford step 9131 current loss 0.886595, current_train_items 292224.
I0302 19:02:34.658850 22447428132992 run.py:483] Algo bellman_ford step 9132 current loss 0.973464, current_train_items 292256.
I0302 19:02:34.690589 22447428132992 run.py:483] Algo bellman_ford step 9133 current loss 1.028015, current_train_items 292288.
I0302 19:02:34.722841 22447428132992 run.py:483] Algo bellman_ford step 9134 current loss 1.083940, current_train_items 292320.
I0302 19:02:34.742199 22447428132992 run.py:483] Algo bellman_ford step 9135 current loss 0.589025, current_train_items 292352.
I0302 19:02:34.757796 22447428132992 run.py:483] Algo bellman_ford step 9136 current loss 0.716044, current_train_items 292384.
I0302 19:02:34.780543 22447428132992 run.py:483] Algo bellman_ford step 9137 current loss 0.839773, current_train_items 292416.
I0302 19:02:34.811408 22447428132992 run.py:483] Algo bellman_ford step 9138 current loss 1.013176, current_train_items 292448.
I0302 19:02:34.843553 22447428132992 run.py:483] Algo bellman_ford step 9139 current loss 0.941040, current_train_items 292480.
I0302 19:02:34.862859 22447428132992 run.py:483] Algo bellman_ford step 9140 current loss 0.498718, current_train_items 292512.
I0302 19:02:34.878887 22447428132992 run.py:483] Algo bellman_ford step 9141 current loss 0.739946, current_train_items 292544.
I0302 19:02:34.902101 22447428132992 run.py:483] Algo bellman_ford step 9142 current loss 0.858969, current_train_items 292576.
I0302 19:02:34.932712 22447428132992 run.py:483] Algo bellman_ford step 9143 current loss 0.930965, current_train_items 292608.
I0302 19:02:34.966977 22447428132992 run.py:483] Algo bellman_ford step 9144 current loss 1.002392, current_train_items 292640.
I0302 19:02:34.986093 22447428132992 run.py:483] Algo bellman_ford step 9145 current loss 0.918578, current_train_items 292672.
I0302 19:02:35.002233 22447428132992 run.py:483] Algo bellman_ford step 9146 current loss 0.767472, current_train_items 292704.
I0302 19:02:35.025950 22447428132992 run.py:483] Algo bellman_ford step 9147 current loss 0.895742, current_train_items 292736.
I0302 19:02:35.057451 22447428132992 run.py:483] Algo bellman_ford step 9148 current loss 1.101633, current_train_items 292768.
I0302 19:02:35.087576 22447428132992 run.py:483] Algo bellman_ford step 9149 current loss 1.066483, current_train_items 292800.
I0302 19:02:35.106768 22447428132992 run.py:483] Algo bellman_ford step 9150 current loss 0.545925, current_train_items 292832.
I0302 19:02:35.114843 22447428132992 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:02:35.114947 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:35.131625 22447428132992 run.py:483] Algo bellman_ford step 9151 current loss 0.802168, current_train_items 292864.
I0302 19:02:35.155230 22447428132992 run.py:483] Algo bellman_ford step 9152 current loss 0.800853, current_train_items 292896.
I0302 19:02:35.186910 22447428132992 run.py:483] Algo bellman_ford step 9153 current loss 1.002399, current_train_items 292928.
I0302 19:02:35.218661 22447428132992 run.py:483] Algo bellman_ford step 9154 current loss 1.066541, current_train_items 292960.
I0302 19:02:35.238267 22447428132992 run.py:483] Algo bellman_ford step 9155 current loss 0.564581, current_train_items 292992.
I0302 19:02:35.254636 22447428132992 run.py:483] Algo bellman_ford step 9156 current loss 0.769999, current_train_items 293024.
I0302 19:02:35.278967 22447428132992 run.py:483] Algo bellman_ford step 9157 current loss 0.876599, current_train_items 293056.
I0302 19:02:35.309745 22447428132992 run.py:483] Algo bellman_ford step 9158 current loss 1.008196, current_train_items 293088.
I0302 19:02:35.344250 22447428132992 run.py:483] Algo bellman_ford step 9159 current loss 1.115808, current_train_items 293120.
I0302 19:02:35.363782 22447428132992 run.py:483] Algo bellman_ford step 9160 current loss 0.549397, current_train_items 293152.
I0302 19:02:35.380118 22447428132992 run.py:483] Algo bellman_ford step 9161 current loss 0.725692, current_train_items 293184.
I0302 19:02:35.403493 22447428132992 run.py:483] Algo bellman_ford step 9162 current loss 0.891287, current_train_items 293216.
I0302 19:02:35.434370 22447428132992 run.py:483] Algo bellman_ford step 9163 current loss 0.973251, current_train_items 293248.
I0302 19:02:35.469348 22447428132992 run.py:483] Algo bellman_ford step 9164 current loss 1.130603, current_train_items 293280.
I0302 19:02:35.488762 22447428132992 run.py:483] Algo bellman_ford step 9165 current loss 0.596223, current_train_items 293312.
I0302 19:02:35.504943 22447428132992 run.py:483] Algo bellman_ford step 9166 current loss 0.786170, current_train_items 293344.
I0302 19:02:35.528347 22447428132992 run.py:483] Algo bellman_ford step 9167 current loss 0.954030, current_train_items 293376.
I0302 19:02:35.559798 22447428132992 run.py:483] Algo bellman_ford step 9168 current loss 0.894396, current_train_items 293408.
I0302 19:02:35.592529 22447428132992 run.py:483] Algo bellman_ford step 9169 current loss 1.059963, current_train_items 293440.
I0302 19:02:35.612186 22447428132992 run.py:483] Algo bellman_ford step 9170 current loss 0.526821, current_train_items 293472.
I0302 19:02:35.628093 22447428132992 run.py:483] Algo bellman_ford step 9171 current loss 0.669936, current_train_items 293504.
I0302 19:02:35.652880 22447428132992 run.py:483] Algo bellman_ford step 9172 current loss 1.019001, current_train_items 293536.
I0302 19:02:35.684254 22447428132992 run.py:483] Algo bellman_ford step 9173 current loss 1.012393, current_train_items 293568.
I0302 19:02:35.718350 22447428132992 run.py:483] Algo bellman_ford step 9174 current loss 1.195661, current_train_items 293600.
I0302 19:02:35.737922 22447428132992 run.py:483] Algo bellman_ford step 9175 current loss 0.657177, current_train_items 293632.
I0302 19:02:35.754290 22447428132992 run.py:483] Algo bellman_ford step 9176 current loss 0.649270, current_train_items 293664.
I0302 19:02:35.776700 22447428132992 run.py:483] Algo bellman_ford step 9177 current loss 0.868526, current_train_items 293696.
I0302 19:02:35.806323 22447428132992 run.py:483] Algo bellman_ford step 9178 current loss 0.961346, current_train_items 293728.
I0302 19:02:35.841085 22447428132992 run.py:483] Algo bellman_ford step 9179 current loss 1.232544, current_train_items 293760.
I0302 19:02:35.860241 22447428132992 run.py:483] Algo bellman_ford step 9180 current loss 0.577399, current_train_items 293792.
I0302 19:02:35.876315 22447428132992 run.py:483] Algo bellman_ford step 9181 current loss 0.798600, current_train_items 293824.
I0302 19:02:35.900499 22447428132992 run.py:483] Algo bellman_ford step 9182 current loss 0.970276, current_train_items 293856.
I0302 19:02:35.930603 22447428132992 run.py:483] Algo bellman_ford step 9183 current loss 0.887009, current_train_items 293888.
I0302 19:02:35.963197 22447428132992 run.py:483] Algo bellman_ford step 9184 current loss 1.177197, current_train_items 293920.
I0302 19:02:35.982897 22447428132992 run.py:483] Algo bellman_ford step 9185 current loss 0.648666, current_train_items 293952.
I0302 19:02:35.999293 22447428132992 run.py:483] Algo bellman_ford step 9186 current loss 0.729146, current_train_items 293984.
I0302 19:02:36.023109 22447428132992 run.py:483] Algo bellman_ford step 9187 current loss 0.924600, current_train_items 294016.
I0302 19:02:36.055542 22447428132992 run.py:483] Algo bellman_ford step 9188 current loss 1.030603, current_train_items 294048.
I0302 19:02:36.091011 22447428132992 run.py:483] Algo bellman_ford step 9189 current loss 1.119305, current_train_items 294080.
I0302 19:02:36.110601 22447428132992 run.py:483] Algo bellman_ford step 9190 current loss 0.800772, current_train_items 294112.
I0302 19:02:36.126521 22447428132992 run.py:483] Algo bellman_ford step 9191 current loss 0.810259, current_train_items 294144.
I0302 19:02:36.150178 22447428132992 run.py:483] Algo bellman_ford step 9192 current loss 0.890761, current_train_items 294176.
I0302 19:02:36.181444 22447428132992 run.py:483] Algo bellman_ford step 9193 current loss 0.979790, current_train_items 294208.
I0302 19:02:36.213920 22447428132992 run.py:483] Algo bellman_ford step 9194 current loss 1.084367, current_train_items 294240.
I0302 19:02:36.233283 22447428132992 run.py:483] Algo bellman_ford step 9195 current loss 0.772161, current_train_items 294272.
I0302 19:02:36.249837 22447428132992 run.py:483] Algo bellman_ford step 9196 current loss 0.813285, current_train_items 294304.
I0302 19:02:36.273586 22447428132992 run.py:483] Algo bellman_ford step 9197 current loss 1.038784, current_train_items 294336.
I0302 19:02:36.303972 22447428132992 run.py:483] Algo bellman_ford step 9198 current loss 0.921067, current_train_items 294368.
I0302 19:02:36.336202 22447428132992 run.py:483] Algo bellman_ford step 9199 current loss 1.029837, current_train_items 294400.
I0302 19:02:36.355966 22447428132992 run.py:483] Algo bellman_ford step 9200 current loss 0.633910, current_train_items 294432.
I0302 19:02:36.363883 22447428132992 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:02:36.363988 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:02:36.380624 22447428132992 run.py:483] Algo bellman_ford step 9201 current loss 0.817835, current_train_items 294464.
I0302 19:02:36.404630 22447428132992 run.py:483] Algo bellman_ford step 9202 current loss 0.816402, current_train_items 294496.
I0302 19:02:36.437374 22447428132992 run.py:483] Algo bellman_ford step 9203 current loss 0.958600, current_train_items 294528.
I0302 19:02:36.472214 22447428132992 run.py:483] Algo bellman_ford step 9204 current loss 1.041263, current_train_items 294560.
I0302 19:02:36.492374 22447428132992 run.py:483] Algo bellman_ford step 9205 current loss 0.596882, current_train_items 294592.
I0302 19:02:36.508344 22447428132992 run.py:483] Algo bellman_ford step 9206 current loss 0.666840, current_train_items 294624.
I0302 19:02:36.531942 22447428132992 run.py:483] Algo bellman_ford step 9207 current loss 0.859987, current_train_items 294656.
I0302 19:02:36.563333 22447428132992 run.py:483] Algo bellman_ford step 9208 current loss 0.872137, current_train_items 294688.
I0302 19:02:36.594844 22447428132992 run.py:483] Algo bellman_ford step 9209 current loss 1.165712, current_train_items 294720.
I0302 19:02:36.614167 22447428132992 run.py:483] Algo bellman_ford step 9210 current loss 0.746495, current_train_items 294752.
I0302 19:02:36.630645 22447428132992 run.py:483] Algo bellman_ford step 9211 current loss 0.731920, current_train_items 294784.
I0302 19:02:36.653971 22447428132992 run.py:483] Algo bellman_ford step 9212 current loss 0.911502, current_train_items 294816.
I0302 19:02:36.685044 22447428132992 run.py:483] Algo bellman_ford step 9213 current loss 0.956760, current_train_items 294848.
I0302 19:02:36.717657 22447428132992 run.py:483] Algo bellman_ford step 9214 current loss 1.081318, current_train_items 294880.
I0302 19:02:36.736880 22447428132992 run.py:483] Algo bellman_ford step 9215 current loss 0.794277, current_train_items 294912.
I0302 19:02:36.753165 22447428132992 run.py:483] Algo bellman_ford step 9216 current loss 0.814525, current_train_items 294944.
I0302 19:02:36.776263 22447428132992 run.py:483] Algo bellman_ford step 9217 current loss 0.861186, current_train_items 294976.
I0302 19:02:36.807205 22447428132992 run.py:483] Algo bellman_ford step 9218 current loss 0.905864, current_train_items 295008.
I0302 19:02:36.840293 22447428132992 run.py:483] Algo bellman_ford step 9219 current loss 1.018236, current_train_items 295040.
I0302 19:02:36.859911 22447428132992 run.py:483] Algo bellman_ford step 9220 current loss 0.661604, current_train_items 295072.
I0302 19:02:36.875767 22447428132992 run.py:483] Algo bellman_ford step 9221 current loss 0.754541, current_train_items 295104.
I0302 19:02:36.899678 22447428132992 run.py:483] Algo bellman_ford step 9222 current loss 0.919430, current_train_items 295136.
I0302 19:02:36.930255 22447428132992 run.py:483] Algo bellman_ford step 9223 current loss 0.938960, current_train_items 295168.
I0302 19:02:36.963792 22447428132992 run.py:483] Algo bellman_ford step 9224 current loss 1.101518, current_train_items 295200.
I0302 19:02:36.983151 22447428132992 run.py:483] Algo bellman_ford step 9225 current loss 0.613591, current_train_items 295232.
I0302 19:02:36.999176 22447428132992 run.py:483] Algo bellman_ford step 9226 current loss 0.748812, current_train_items 295264.
I0302 19:02:37.023090 22447428132992 run.py:483] Algo bellman_ford step 9227 current loss 0.884148, current_train_items 295296.
I0302 19:02:37.054320 22447428132992 run.py:483] Algo bellman_ford step 9228 current loss 0.938608, current_train_items 295328.
I0302 19:02:37.085241 22447428132992 run.py:483] Algo bellman_ford step 9229 current loss 1.190692, current_train_items 295360.
I0302 19:02:37.104596 22447428132992 run.py:483] Algo bellman_ford step 9230 current loss 0.529226, current_train_items 295392.
I0302 19:02:37.120980 22447428132992 run.py:483] Algo bellman_ford step 9231 current loss 0.703395, current_train_items 295424.
I0302 19:02:37.144764 22447428132992 run.py:483] Algo bellman_ford step 9232 current loss 0.993861, current_train_items 295456.
I0302 19:02:37.178371 22447428132992 run.py:483] Algo bellman_ford step 9233 current loss 0.951471, current_train_items 295488.
I0302 19:02:37.212054 22447428132992 run.py:483] Algo bellman_ford step 9234 current loss 1.085572, current_train_items 295520.
I0302 19:02:37.231302 22447428132992 run.py:483] Algo bellman_ford step 9235 current loss 0.871955, current_train_items 295552.
I0302 19:02:37.247497 22447428132992 run.py:483] Algo bellman_ford step 9236 current loss 1.098512, current_train_items 295584.
I0302 19:02:37.270933 22447428132992 run.py:483] Algo bellman_ford step 9237 current loss 0.822339, current_train_items 295616.
I0302 19:02:37.303375 22447428132992 run.py:483] Algo bellman_ford step 9238 current loss 1.066135, current_train_items 295648.
I0302 19:02:37.337182 22447428132992 run.py:483] Algo bellman_ford step 9239 current loss 1.197520, current_train_items 295680.
I0302 19:02:37.356568 22447428132992 run.py:483] Algo bellman_ford step 9240 current loss 0.935456, current_train_items 295712.
I0302 19:02:37.373107 22447428132992 run.py:483] Algo bellman_ford step 9241 current loss 0.831481, current_train_items 295744.
I0302 19:02:37.397250 22447428132992 run.py:483] Algo bellman_ford step 9242 current loss 0.952722, current_train_items 295776.
I0302 19:02:37.428565 22447428132992 run.py:483] Algo bellman_ford step 9243 current loss 0.909854, current_train_items 295808.
I0302 19:02:37.460796 22447428132992 run.py:483] Algo bellman_ford step 9244 current loss 1.061044, current_train_items 295840.
I0302 19:02:37.480144 22447428132992 run.py:483] Algo bellman_ford step 9245 current loss 0.558564, current_train_items 295872.
I0302 19:02:37.496288 22447428132992 run.py:483] Algo bellman_ford step 9246 current loss 0.728658, current_train_items 295904.
I0302 19:02:37.519077 22447428132992 run.py:483] Algo bellman_ford step 9247 current loss 0.873833, current_train_items 295936.
I0302 19:02:37.550483 22447428132992 run.py:483] Algo bellman_ford step 9248 current loss 0.994349, current_train_items 295968.
I0302 19:02:37.584273 22447428132992 run.py:483] Algo bellman_ford step 9249 current loss 1.070313, current_train_items 296000.
I0302 19:02:37.603826 22447428132992 run.py:483] Algo bellman_ford step 9250 current loss 0.612157, current_train_items 296032.
I0302 19:02:37.612087 22447428132992 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:02:37.612193 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:02:37.628710 22447428132992 run.py:483] Algo bellman_ford step 9251 current loss 0.707891, current_train_items 296064.
I0302 19:02:37.653300 22447428132992 run.py:483] Algo bellman_ford step 9252 current loss 0.876283, current_train_items 296096.
I0302 19:02:37.684540 22447428132992 run.py:483] Algo bellman_ford step 9253 current loss 1.035999, current_train_items 296128.
I0302 19:02:37.716808 22447428132992 run.py:483] Algo bellman_ford step 9254 current loss 1.165702, current_train_items 296160.
I0302 19:02:37.736640 22447428132992 run.py:483] Algo bellman_ford step 9255 current loss 0.725708, current_train_items 296192.
I0302 19:02:37.752507 22447428132992 run.py:483] Algo bellman_ford step 9256 current loss 0.610475, current_train_items 296224.
I0302 19:02:37.776529 22447428132992 run.py:483] Algo bellman_ford step 9257 current loss 0.933071, current_train_items 296256.
I0302 19:02:37.808123 22447428132992 run.py:483] Algo bellman_ford step 9258 current loss 0.979896, current_train_items 296288.
I0302 19:02:37.841608 22447428132992 run.py:483] Algo bellman_ford step 9259 current loss 1.053362, current_train_items 296320.
I0302 19:02:37.861819 22447428132992 run.py:483] Algo bellman_ford step 9260 current loss 0.644416, current_train_items 296352.
I0302 19:02:37.878720 22447428132992 run.py:483] Algo bellman_ford step 9261 current loss 0.743106, current_train_items 296384.
I0302 19:02:37.901057 22447428132992 run.py:483] Algo bellman_ford step 9262 current loss 0.833539, current_train_items 296416.
I0302 19:02:37.932748 22447428132992 run.py:483] Algo bellman_ford step 9263 current loss 0.989720, current_train_items 296448.
I0302 19:02:37.966513 22447428132992 run.py:483] Algo bellman_ford step 9264 current loss 1.111817, current_train_items 296480.
I0302 19:02:37.986002 22447428132992 run.py:483] Algo bellman_ford step 9265 current loss 0.638187, current_train_items 296512.
I0302 19:02:38.002260 22447428132992 run.py:483] Algo bellman_ford step 9266 current loss 0.701205, current_train_items 296544.
I0302 19:02:38.024802 22447428132992 run.py:483] Algo bellman_ford step 9267 current loss 0.794091, current_train_items 296576.
I0302 19:02:38.056160 22447428132992 run.py:483] Algo bellman_ford step 9268 current loss 0.985775, current_train_items 296608.
I0302 19:02:38.089602 22447428132992 run.py:483] Algo bellman_ford step 9269 current loss 1.075701, current_train_items 296640.
I0302 19:02:38.109385 22447428132992 run.py:483] Algo bellman_ford step 9270 current loss 0.656924, current_train_items 296672.
I0302 19:02:38.125480 22447428132992 run.py:483] Algo bellman_ford step 9271 current loss 0.721689, current_train_items 296704.
I0302 19:02:38.148868 22447428132992 run.py:483] Algo bellman_ford step 9272 current loss 0.851204, current_train_items 296736.
I0302 19:02:38.181935 22447428132992 run.py:483] Algo bellman_ford step 9273 current loss 1.068337, current_train_items 296768.
I0302 19:02:38.216161 22447428132992 run.py:483] Algo bellman_ford step 9274 current loss 1.148933, current_train_items 296800.
I0302 19:02:38.236166 22447428132992 run.py:483] Algo bellman_ford step 9275 current loss 0.693991, current_train_items 296832.
I0302 19:02:38.252448 22447428132992 run.py:483] Algo bellman_ford step 9276 current loss 0.735942, current_train_items 296864.
I0302 19:02:38.275602 22447428132992 run.py:483] Algo bellman_ford step 9277 current loss 0.976854, current_train_items 296896.
I0302 19:02:38.307686 22447428132992 run.py:483] Algo bellman_ford step 9278 current loss 1.023036, current_train_items 296928.
I0302 19:02:38.341541 22447428132992 run.py:483] Algo bellman_ford step 9279 current loss 1.303386, current_train_items 296960.
I0302 19:02:38.361261 22447428132992 run.py:483] Algo bellman_ford step 9280 current loss 0.594943, current_train_items 296992.
I0302 19:02:38.377579 22447428132992 run.py:483] Algo bellman_ford step 9281 current loss 0.785706, current_train_items 297024.
I0302 19:02:38.401330 22447428132992 run.py:483] Algo bellman_ford step 9282 current loss 0.873667, current_train_items 297056.
I0302 19:02:38.431260 22447428132992 run.py:483] Algo bellman_ford step 9283 current loss 0.883327, current_train_items 297088.
I0302 19:02:38.463966 22447428132992 run.py:483] Algo bellman_ford step 9284 current loss 1.196659, current_train_items 297120.
I0302 19:02:38.484057 22447428132992 run.py:483] Algo bellman_ford step 9285 current loss 0.557833, current_train_items 297152.
I0302 19:02:38.499659 22447428132992 run.py:483] Algo bellman_ford step 9286 current loss 0.657199, current_train_items 297184.
I0302 19:02:38.524336 22447428132992 run.py:483] Algo bellman_ford step 9287 current loss 1.060601, current_train_items 297216.
I0302 19:02:38.555081 22447428132992 run.py:483] Algo bellman_ford step 9288 current loss 0.917032, current_train_items 297248.
I0302 19:02:38.590341 22447428132992 run.py:483] Algo bellman_ford step 9289 current loss 1.232626, current_train_items 297280.
I0302 19:02:38.610308 22447428132992 run.py:483] Algo bellman_ford step 9290 current loss 0.579849, current_train_items 297312.
I0302 19:02:38.626421 22447428132992 run.py:483] Algo bellman_ford step 9291 current loss 0.719201, current_train_items 297344.
I0302 19:02:38.650766 22447428132992 run.py:483] Algo bellman_ford step 9292 current loss 0.916194, current_train_items 297376.
I0302 19:02:38.681260 22447428132992 run.py:483] Algo bellman_ford step 9293 current loss 0.982838, current_train_items 297408.
I0302 19:02:38.716043 22447428132992 run.py:483] Algo bellman_ford step 9294 current loss 1.272320, current_train_items 297440.
I0302 19:02:38.735587 22447428132992 run.py:483] Algo bellman_ford step 9295 current loss 0.572261, current_train_items 297472.
I0302 19:02:38.751788 22447428132992 run.py:483] Algo bellman_ford step 9296 current loss 0.699443, current_train_items 297504.
I0302 19:02:38.775327 22447428132992 run.py:483] Algo bellman_ford step 9297 current loss 0.856157, current_train_items 297536.
I0302 19:02:38.807012 22447428132992 run.py:483] Algo bellman_ford step 9298 current loss 1.009796, current_train_items 297568.
I0302 19:02:38.841009 22447428132992 run.py:483] Algo bellman_ford step 9299 current loss 1.133730, current_train_items 297600.
I0302 19:02:38.861138 22447428132992 run.py:483] Algo bellman_ford step 9300 current loss 0.715811, current_train_items 297632.
I0302 19:02:38.868804 22447428132992 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.888671875, 'score': 0.888671875, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:02:38.868910 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.889, val scores are: bellman_ford: 0.889
I0302 19:02:38.885660 22447428132992 run.py:483] Algo bellman_ford step 9301 current loss 0.700202, current_train_items 297664.
I0302 19:02:38.910316 22447428132992 run.py:483] Algo bellman_ford step 9302 current loss 0.872140, current_train_items 297696.
I0302 19:02:38.942020 22447428132992 run.py:483] Algo bellman_ford step 9303 current loss 0.969519, current_train_items 297728.
I0302 19:02:38.976923 22447428132992 run.py:483] Algo bellman_ford step 9304 current loss 1.156117, current_train_items 297760.
I0302 19:02:38.996852 22447428132992 run.py:483] Algo bellman_ford step 9305 current loss 0.616430, current_train_items 297792.
I0302 19:02:39.012914 22447428132992 run.py:483] Algo bellman_ford step 9306 current loss 0.756145, current_train_items 297824.
I0302 19:02:39.037089 22447428132992 run.py:483] Algo bellman_ford step 9307 current loss 0.945729, current_train_items 297856.
I0302 19:02:39.068859 22447428132992 run.py:483] Algo bellman_ford step 9308 current loss 0.919288, current_train_items 297888.
I0302 19:02:39.102395 22447428132992 run.py:483] Algo bellman_ford step 9309 current loss 1.046063, current_train_items 297920.
I0302 19:02:39.122165 22447428132992 run.py:483] Algo bellman_ford step 9310 current loss 0.563338, current_train_items 297952.
I0302 19:02:39.138252 22447428132992 run.py:483] Algo bellman_ford step 9311 current loss 0.693435, current_train_items 297984.
I0302 19:02:39.161983 22447428132992 run.py:483] Algo bellman_ford step 9312 current loss 0.876700, current_train_items 298016.
I0302 19:02:39.192406 22447428132992 run.py:483] Algo bellman_ford step 9313 current loss 0.907442, current_train_items 298048.
I0302 19:02:39.226747 22447428132992 run.py:483] Algo bellman_ford step 9314 current loss 1.042777, current_train_items 298080.
I0302 19:02:39.246305 22447428132992 run.py:483] Algo bellman_ford step 9315 current loss 0.849293, current_train_items 298112.
I0302 19:02:39.262096 22447428132992 run.py:483] Algo bellman_ford step 9316 current loss 0.705501, current_train_items 298144.
I0302 19:02:39.286131 22447428132992 run.py:483] Algo bellman_ford step 9317 current loss 0.865602, current_train_items 298176.
I0302 19:02:39.318399 22447428132992 run.py:483] Algo bellman_ford step 9318 current loss 0.988685, current_train_items 298208.
I0302 19:02:39.352253 22447428132992 run.py:483] Algo bellman_ford step 9319 current loss 0.998759, current_train_items 298240.
I0302 19:02:39.371727 22447428132992 run.py:483] Algo bellman_ford step 9320 current loss 0.725169, current_train_items 298272.
I0302 19:02:39.387940 22447428132992 run.py:483] Algo bellman_ford step 9321 current loss 0.800908, current_train_items 298304.
I0302 19:02:39.410400 22447428132992 run.py:483] Algo bellman_ford step 9322 current loss 0.850814, current_train_items 298336.
I0302 19:02:39.441092 22447428132992 run.py:483] Algo bellman_ford step 9323 current loss 0.896037, current_train_items 298368.
I0302 19:02:39.476868 22447428132992 run.py:483] Algo bellman_ford step 9324 current loss 1.169557, current_train_items 298400.
I0302 19:02:39.496625 22447428132992 run.py:483] Algo bellman_ford step 9325 current loss 0.599732, current_train_items 298432.
I0302 19:02:39.512676 22447428132992 run.py:483] Algo bellman_ford step 9326 current loss 0.802979, current_train_items 298464.
I0302 19:02:39.536996 22447428132992 run.py:483] Algo bellman_ford step 9327 current loss 1.032875, current_train_items 298496.
I0302 19:02:39.569673 22447428132992 run.py:483] Algo bellman_ford step 9328 current loss 1.049056, current_train_items 298528.
I0302 19:02:39.600320 22447428132992 run.py:483] Algo bellman_ford step 9329 current loss 1.186547, current_train_items 298560.
I0302 19:02:39.620006 22447428132992 run.py:483] Algo bellman_ford step 9330 current loss 0.557817, current_train_items 298592.
I0302 19:02:39.636222 22447428132992 run.py:483] Algo bellman_ford step 9331 current loss 0.856947, current_train_items 298624.
I0302 19:02:39.659557 22447428132992 run.py:483] Algo bellman_ford step 9332 current loss 0.922905, current_train_items 298656.
I0302 19:02:39.690978 22447428132992 run.py:483] Algo bellman_ford step 9333 current loss 0.947629, current_train_items 298688.
I0302 19:02:39.723614 22447428132992 run.py:483] Algo bellman_ford step 9334 current loss 1.100257, current_train_items 298720.
I0302 19:02:39.743229 22447428132992 run.py:483] Algo bellman_ford step 9335 current loss 0.550070, current_train_items 298752.
I0302 19:02:39.759507 22447428132992 run.py:483] Algo bellman_ford step 9336 current loss 0.819801, current_train_items 298784.
I0302 19:02:39.784105 22447428132992 run.py:483] Algo bellman_ford step 9337 current loss 0.986263, current_train_items 298816.
I0302 19:02:39.815500 22447428132992 run.py:483] Algo bellman_ford step 9338 current loss 0.963274, current_train_items 298848.
I0302 19:02:39.849624 22447428132992 run.py:483] Algo bellman_ford step 9339 current loss 1.068864, current_train_items 298880.
I0302 19:02:39.869369 22447428132992 run.py:483] Algo bellman_ford step 9340 current loss 0.561947, current_train_items 298912.
I0302 19:02:39.885171 22447428132992 run.py:483] Algo bellman_ford step 9341 current loss 0.684568, current_train_items 298944.
I0302 19:02:39.908975 22447428132992 run.py:483] Algo bellman_ford step 9342 current loss 0.770664, current_train_items 298976.
I0302 19:02:39.940816 22447428132992 run.py:483] Algo bellman_ford step 9343 current loss 0.929005, current_train_items 299008.
I0302 19:02:39.975008 22447428132992 run.py:483] Algo bellman_ford step 9344 current loss 1.060706, current_train_items 299040.
I0302 19:02:39.994701 22447428132992 run.py:483] Algo bellman_ford step 9345 current loss 0.821541, current_train_items 299072.
I0302 19:02:40.010844 22447428132992 run.py:483] Algo bellman_ford step 9346 current loss 0.768742, current_train_items 299104.
I0302 19:02:40.034842 22447428132992 run.py:483] Algo bellman_ford step 9347 current loss 0.880562, current_train_items 299136.
I0302 19:02:40.066625 22447428132992 run.py:483] Algo bellman_ford step 9348 current loss 1.014910, current_train_items 299168.
I0302 19:02:40.098990 22447428132992 run.py:483] Algo bellman_ford step 9349 current loss 1.010526, current_train_items 299200.
I0302 19:02:40.118844 22447428132992 run.py:483] Algo bellman_ford step 9350 current loss 0.778481, current_train_items 299232.
I0302 19:02:40.126906 22447428132992 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:02:40.127012 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:02:40.143465 22447428132992 run.py:483] Algo bellman_ford step 9351 current loss 0.675786, current_train_items 299264.
I0302 19:02:40.168369 22447428132992 run.py:483] Algo bellman_ford step 9352 current loss 0.999520, current_train_items 299296.
I0302 19:02:40.200446 22447428132992 run.py:483] Algo bellman_ford step 9353 current loss 1.019611, current_train_items 299328.
I0302 19:02:40.236230 22447428132992 run.py:483] Algo bellman_ford step 9354 current loss 1.147646, current_train_items 299360.
I0302 19:02:40.256221 22447428132992 run.py:483] Algo bellman_ford step 9355 current loss 0.536703, current_train_items 299392.
I0302 19:02:40.271720 22447428132992 run.py:483] Algo bellman_ford step 9356 current loss 0.705173, current_train_items 299424.
I0302 19:02:40.295648 22447428132992 run.py:483] Algo bellman_ford step 9357 current loss 0.923828, current_train_items 299456.
I0302 19:02:40.327128 22447428132992 run.py:483] Algo bellman_ford step 9358 current loss 1.080814, current_train_items 299488.
I0302 19:02:40.360817 22447428132992 run.py:483] Algo bellman_ford step 9359 current loss 1.027122, current_train_items 299520.
I0302 19:02:40.380960 22447428132992 run.py:483] Algo bellman_ford step 9360 current loss 0.622702, current_train_items 299552.
I0302 19:02:40.397015 22447428132992 run.py:483] Algo bellman_ford step 9361 current loss 0.721993, current_train_items 299584.
I0302 19:02:40.420001 22447428132992 run.py:483] Algo bellman_ford step 9362 current loss 0.842722, current_train_items 299616.
I0302 19:02:40.452020 22447428132992 run.py:483] Algo bellman_ford step 9363 current loss 1.009809, current_train_items 299648.
I0302 19:02:40.486102 22447428132992 run.py:483] Algo bellman_ford step 9364 current loss 1.145708, current_train_items 299680.
I0302 19:02:40.505373 22447428132992 run.py:483] Algo bellman_ford step 9365 current loss 0.601007, current_train_items 299712.
I0302 19:02:40.521923 22447428132992 run.py:483] Algo bellman_ford step 9366 current loss 0.784275, current_train_items 299744.
I0302 19:02:40.546996 22447428132992 run.py:483] Algo bellman_ford step 9367 current loss 1.039765, current_train_items 299776.
I0302 19:02:40.579717 22447428132992 run.py:483] Algo bellman_ford step 9368 current loss 0.946915, current_train_items 299808.
I0302 19:02:40.612941 22447428132992 run.py:483] Algo bellman_ford step 9369 current loss 0.965305, current_train_items 299840.
I0302 19:02:40.632926 22447428132992 run.py:483] Algo bellman_ford step 9370 current loss 0.604766, current_train_items 299872.
I0302 19:02:40.649284 22447428132992 run.py:483] Algo bellman_ford step 9371 current loss 0.742791, current_train_items 299904.
I0302 19:02:40.673085 22447428132992 run.py:483] Algo bellman_ford step 9372 current loss 0.888087, current_train_items 299936.
I0302 19:02:40.705359 22447428132992 run.py:483] Algo bellman_ford step 9373 current loss 0.975214, current_train_items 299968.
I0302 19:02:40.738798 22447428132992 run.py:483] Algo bellman_ford step 9374 current loss 1.013190, current_train_items 300000.
I0302 19:02:40.758370 22447428132992 run.py:483] Algo bellman_ford step 9375 current loss 0.610532, current_train_items 300032.
I0302 19:02:40.774115 22447428132992 run.py:483] Algo bellman_ford step 9376 current loss 0.830566, current_train_items 300064.
I0302 19:02:40.797226 22447428132992 run.py:483] Algo bellman_ford step 9377 current loss 0.812889, current_train_items 300096.
I0302 19:02:40.828025 22447428132992 run.py:483] Algo bellman_ford step 9378 current loss 0.938287, current_train_items 300128.
I0302 19:02:40.861268 22447428132992 run.py:483] Algo bellman_ford step 9379 current loss 1.190672, current_train_items 300160.
I0302 19:02:40.880642 22447428132992 run.py:483] Algo bellman_ford step 9380 current loss 0.495513, current_train_items 300192.
I0302 19:02:40.896276 22447428132992 run.py:483] Algo bellman_ford step 9381 current loss 0.733967, current_train_items 300224.
I0302 19:02:40.920171 22447428132992 run.py:483] Algo bellman_ford step 9382 current loss 0.896169, current_train_items 300256.
I0302 19:02:40.950807 22447428132992 run.py:483] Algo bellman_ford step 9383 current loss 0.869699, current_train_items 300288.
I0302 19:02:40.984427 22447428132992 run.py:483] Algo bellman_ford step 9384 current loss 1.081525, current_train_items 300320.
I0302 19:02:41.004157 22447428132992 run.py:483] Algo bellman_ford step 9385 current loss 0.575805, current_train_items 300352.
I0302 19:02:41.020691 22447428132992 run.py:483] Algo bellman_ford step 9386 current loss 0.841559, current_train_items 300384.
I0302 19:02:41.043744 22447428132992 run.py:483] Algo bellman_ford step 9387 current loss 0.972204, current_train_items 300416.
I0302 19:02:41.074854 22447428132992 run.py:483] Algo bellman_ford step 9388 current loss 0.967076, current_train_items 300448.
I0302 19:02:41.110414 22447428132992 run.py:483] Algo bellman_ford step 9389 current loss 0.982600, current_train_items 300480.
I0302 19:02:41.130134 22447428132992 run.py:483] Algo bellman_ford step 9390 current loss 0.630702, current_train_items 300512.
I0302 19:02:41.146504 22447428132992 run.py:483] Algo bellman_ford step 9391 current loss 0.743424, current_train_items 300544.
I0302 19:02:41.169757 22447428132992 run.py:483] Algo bellman_ford step 9392 current loss 0.896072, current_train_items 300576.
I0302 19:02:41.200685 22447428132992 run.py:483] Algo bellman_ford step 9393 current loss 0.943341, current_train_items 300608.
I0302 19:02:41.233713 22447428132992 run.py:483] Algo bellman_ford step 9394 current loss 1.032598, current_train_items 300640.
I0302 19:02:41.253033 22447428132992 run.py:483] Algo bellman_ford step 9395 current loss 0.796092, current_train_items 300672.
I0302 19:02:41.269188 22447428132992 run.py:483] Algo bellman_ford step 9396 current loss 0.701935, current_train_items 300704.
I0302 19:02:41.293570 22447428132992 run.py:483] Algo bellman_ford step 9397 current loss 0.971947, current_train_items 300736.
I0302 19:02:41.324567 22447428132992 run.py:483] Algo bellman_ford step 9398 current loss 0.898337, current_train_items 300768.
I0302 19:02:41.358409 22447428132992 run.py:483] Algo bellman_ford step 9399 current loss 1.123481, current_train_items 300800.
I0302 19:02:41.378123 22447428132992 run.py:483] Algo bellman_ford step 9400 current loss 0.609961, current_train_items 300832.
I0302 19:02:41.385827 22447428132992 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:41.385932 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:02:41.402633 22447428132992 run.py:483] Algo bellman_ford step 9401 current loss 0.690713, current_train_items 300864.
I0302 19:02:41.427547 22447428132992 run.py:483] Algo bellman_ford step 9402 current loss 1.002045, current_train_items 300896.
I0302 19:02:41.459458 22447428132992 run.py:483] Algo bellman_ford step 9403 current loss 0.978657, current_train_items 300928.
I0302 19:02:41.493937 22447428132992 run.py:483] Algo bellman_ford step 9404 current loss 1.105030, current_train_items 300960.
I0302 19:02:41.513778 22447428132992 run.py:483] Algo bellman_ford step 9405 current loss 0.506105, current_train_items 300992.
I0302 19:02:41.529536 22447428132992 run.py:483] Algo bellman_ford step 9406 current loss 0.720343, current_train_items 301024.
I0302 19:02:41.554706 22447428132992 run.py:483] Algo bellman_ford step 9407 current loss 0.990083, current_train_items 301056.
I0302 19:02:41.585630 22447428132992 run.py:483] Algo bellman_ford step 9408 current loss 0.903459, current_train_items 301088.
I0302 19:02:41.620516 22447428132992 run.py:483] Algo bellman_ford step 9409 current loss 1.157784, current_train_items 301120.
I0302 19:02:41.640011 22447428132992 run.py:483] Algo bellman_ford step 9410 current loss 0.562074, current_train_items 301152.
I0302 19:02:41.655995 22447428132992 run.py:483] Algo bellman_ford step 9411 current loss 0.804501, current_train_items 301184.
I0302 19:02:41.680081 22447428132992 run.py:483] Algo bellman_ford step 9412 current loss 0.855912, current_train_items 301216.
I0302 19:02:41.711768 22447428132992 run.py:483] Algo bellman_ford step 9413 current loss 0.916559, current_train_items 301248.
I0302 19:02:41.744504 22447428132992 run.py:483] Algo bellman_ford step 9414 current loss 1.073560, current_train_items 301280.
I0302 19:02:41.764052 22447428132992 run.py:483] Algo bellman_ford step 9415 current loss 0.821060, current_train_items 301312.
I0302 19:02:41.780342 22447428132992 run.py:483] Algo bellman_ford step 9416 current loss 0.712701, current_train_items 301344.
I0302 19:02:41.804615 22447428132992 run.py:483] Algo bellman_ford step 9417 current loss 0.965786, current_train_items 301376.
I0302 19:02:41.835441 22447428132992 run.py:483] Algo bellman_ford step 9418 current loss 0.933913, current_train_items 301408.
I0302 19:02:41.867143 22447428132992 run.py:483] Algo bellman_ford step 9419 current loss 0.946147, current_train_items 301440.
I0302 19:02:41.886763 22447428132992 run.py:483] Algo bellman_ford step 9420 current loss 0.498013, current_train_items 301472.
I0302 19:02:41.903089 22447428132992 run.py:483] Algo bellman_ford step 9421 current loss 0.734772, current_train_items 301504.
I0302 19:02:41.927419 22447428132992 run.py:483] Algo bellman_ford step 9422 current loss 0.917374, current_train_items 301536.
I0302 19:02:41.958441 22447428132992 run.py:483] Algo bellman_ford step 9423 current loss 0.919713, current_train_items 301568.
I0302 19:02:41.991967 22447428132992 run.py:483] Algo bellman_ford step 9424 current loss 1.186452, current_train_items 301600.
I0302 19:02:42.011705 22447428132992 run.py:483] Algo bellman_ford step 9425 current loss 0.721890, current_train_items 301632.
I0302 19:02:42.027399 22447428132992 run.py:483] Algo bellman_ford step 9426 current loss 0.720783, current_train_items 301664.
I0302 19:02:42.050934 22447428132992 run.py:483] Algo bellman_ford step 9427 current loss 0.832580, current_train_items 301696.
I0302 19:02:42.083078 22447428132992 run.py:483] Algo bellman_ford step 9428 current loss 1.055166, current_train_items 301728.
I0302 19:02:42.114148 22447428132992 run.py:483] Algo bellman_ford step 9429 current loss 1.190608, current_train_items 301760.
I0302 19:02:42.133636 22447428132992 run.py:483] Algo bellman_ford step 9430 current loss 0.552722, current_train_items 301792.
I0302 19:02:42.149677 22447428132992 run.py:483] Algo bellman_ford step 9431 current loss 0.874742, current_train_items 301824.
I0302 19:02:42.173293 22447428132992 run.py:483] Algo bellman_ford step 9432 current loss 0.924759, current_train_items 301856.
I0302 19:02:42.204066 22447428132992 run.py:483] Algo bellman_ford step 9433 current loss 1.011291, current_train_items 301888.
I0302 19:02:42.238276 22447428132992 run.py:483] Algo bellman_ford step 9434 current loss 1.376429, current_train_items 301920.
I0302 19:02:42.257905 22447428132992 run.py:483] Algo bellman_ford step 9435 current loss 0.536530, current_train_items 301952.
I0302 19:02:42.274268 22447428132992 run.py:483] Algo bellman_ford step 9436 current loss 0.803410, current_train_items 301984.
I0302 19:02:42.298032 22447428132992 run.py:483] Algo bellman_ford step 9437 current loss 0.906621, current_train_items 302016.
I0302 19:02:42.329742 22447428132992 run.py:483] Algo bellman_ford step 9438 current loss 0.942769, current_train_items 302048.
I0302 19:02:42.362309 22447428132992 run.py:483] Algo bellman_ford step 9439 current loss 1.214122, current_train_items 302080.
I0302 19:02:42.382184 22447428132992 run.py:483] Algo bellman_ford step 9440 current loss 0.512033, current_train_items 302112.
I0302 19:02:42.398646 22447428132992 run.py:483] Algo bellman_ford step 9441 current loss 0.733622, current_train_items 302144.
I0302 19:02:42.422139 22447428132992 run.py:483] Algo bellman_ford step 9442 current loss 0.992215, current_train_items 302176.
I0302 19:02:42.454193 22447428132992 run.py:483] Algo bellman_ford step 9443 current loss 0.972930, current_train_items 302208.
I0302 19:02:42.489830 22447428132992 run.py:483] Algo bellman_ford step 9444 current loss 1.124179, current_train_items 302240.
I0302 19:02:42.509326 22447428132992 run.py:483] Algo bellman_ford step 9445 current loss 0.634338, current_train_items 302272.
I0302 19:02:42.525089 22447428132992 run.py:483] Algo bellman_ford step 9446 current loss 0.614865, current_train_items 302304.
I0302 19:02:42.548254 22447428132992 run.py:483] Algo bellman_ford step 9447 current loss 0.930872, current_train_items 302336.
I0302 19:02:42.579228 22447428132992 run.py:483] Algo bellman_ford step 9448 current loss 1.068726, current_train_items 302368.
I0302 19:02:42.611373 22447428132992 run.py:483] Algo bellman_ford step 9449 current loss 1.054245, current_train_items 302400.
I0302 19:02:42.630909 22447428132992 run.py:483] Algo bellman_ford step 9450 current loss 0.593651, current_train_items 302432.
I0302 19:02:42.639130 22447428132992 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:42.639237 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:02:42.655972 22447428132992 run.py:483] Algo bellman_ford step 9451 current loss 0.785324, current_train_items 302464.
I0302 19:02:42.678956 22447428132992 run.py:483] Algo bellman_ford step 9452 current loss 0.831667, current_train_items 302496.
I0302 19:02:42.710843 22447428132992 run.py:483] Algo bellman_ford step 9453 current loss 1.059023, current_train_items 302528.
I0302 19:02:42.745223 22447428132992 run.py:483] Algo bellman_ford step 9454 current loss 1.147939, current_train_items 302560.
I0302 19:02:42.765118 22447428132992 run.py:483] Algo bellman_ford step 9455 current loss 0.549160, current_train_items 302592.
I0302 19:02:42.780964 22447428132992 run.py:483] Algo bellman_ford step 9456 current loss 0.711060, current_train_items 302624.
I0302 19:02:42.804832 22447428132992 run.py:483] Algo bellman_ford step 9457 current loss 0.871720, current_train_items 302656.
I0302 19:02:42.836034 22447428132992 run.py:483] Algo bellman_ford step 9458 current loss 0.971032, current_train_items 302688.
I0302 19:02:42.867981 22447428132992 run.py:483] Algo bellman_ford step 9459 current loss 1.008094, current_train_items 302720.
I0302 19:02:42.887929 22447428132992 run.py:483] Algo bellman_ford step 9460 current loss 0.544750, current_train_items 302752.
I0302 19:02:42.904278 22447428132992 run.py:483] Algo bellman_ford step 9461 current loss 0.852770, current_train_items 302784.
I0302 19:02:42.926979 22447428132992 run.py:483] Algo bellman_ford step 9462 current loss 0.830202, current_train_items 302816.
I0302 19:02:42.959181 22447428132992 run.py:483] Algo bellman_ford step 9463 current loss 0.928850, current_train_items 302848.
I0302 19:02:42.992057 22447428132992 run.py:483] Algo bellman_ford step 9464 current loss 1.036121, current_train_items 302880.
I0302 19:02:43.011571 22447428132992 run.py:483] Algo bellman_ford step 9465 current loss 0.788391, current_train_items 302912.
I0302 19:02:43.028043 22447428132992 run.py:483] Algo bellman_ford step 9466 current loss 0.896507, current_train_items 302944.
I0302 19:02:43.052912 22447428132992 run.py:483] Algo bellman_ford step 9467 current loss 0.953091, current_train_items 302976.
I0302 19:02:43.084890 22447428132992 run.py:483] Algo bellman_ford step 9468 current loss 0.910573, current_train_items 303008.
I0302 19:02:43.119292 22447428132992 run.py:483] Algo bellman_ford step 9469 current loss 1.082252, current_train_items 303040.
I0302 19:02:43.139005 22447428132992 run.py:483] Algo bellman_ford step 9470 current loss 0.687160, current_train_items 303072.
I0302 19:02:43.154840 22447428132992 run.py:483] Algo bellman_ford step 9471 current loss 0.671977, current_train_items 303104.
I0302 19:02:43.178527 22447428132992 run.py:483] Algo bellman_ford step 9472 current loss 0.846336, current_train_items 303136.
I0302 19:02:43.210258 22447428132992 run.py:483] Algo bellman_ford step 9473 current loss 0.947668, current_train_items 303168.
I0302 19:02:43.242806 22447428132992 run.py:483] Algo bellman_ford step 9474 current loss 1.051023, current_train_items 303200.
I0302 19:02:43.262451 22447428132992 run.py:483] Algo bellman_ford step 9475 current loss 0.684931, current_train_items 303232.
I0302 19:02:43.278705 22447428132992 run.py:483] Algo bellman_ford step 9476 current loss 0.763123, current_train_items 303264.
I0302 19:02:43.301639 22447428132992 run.py:483] Algo bellman_ford step 9477 current loss 0.799471, current_train_items 303296.
I0302 19:02:43.332476 22447428132992 run.py:483] Algo bellman_ford step 9478 current loss 0.953131, current_train_items 303328.
I0302 19:02:43.365089 22447428132992 run.py:483] Algo bellman_ford step 9479 current loss 1.024427, current_train_items 303360.
I0302 19:02:43.384816 22447428132992 run.py:483] Algo bellman_ford step 9480 current loss 0.744421, current_train_items 303392.
I0302 19:02:43.400755 22447428132992 run.py:483] Algo bellman_ford step 9481 current loss 0.808457, current_train_items 303424.
I0302 19:02:43.425564 22447428132992 run.py:483] Algo bellman_ford step 9482 current loss 1.087347, current_train_items 303456.
I0302 19:02:43.455468 22447428132992 run.py:483] Algo bellman_ford step 9483 current loss 0.927017, current_train_items 303488.
I0302 19:02:43.487643 22447428132992 run.py:483] Algo bellman_ford step 9484 current loss 1.048148, current_train_items 303520.
I0302 19:02:43.507078 22447428132992 run.py:483] Algo bellman_ford step 9485 current loss 0.593939, current_train_items 303552.
I0302 19:02:43.523538 22447428132992 run.py:483] Algo bellman_ford step 9486 current loss 0.779572, current_train_items 303584.
I0302 19:02:43.547910 22447428132992 run.py:483] Algo bellman_ford step 9487 current loss 0.962951, current_train_items 303616.
I0302 19:02:43.580020 22447428132992 run.py:483] Algo bellman_ford step 9488 current loss 0.939222, current_train_items 303648.
I0302 19:02:43.612303 22447428132992 run.py:483] Algo bellman_ford step 9489 current loss 1.109189, current_train_items 303680.
I0302 19:02:43.631872 22447428132992 run.py:483] Algo bellman_ford step 9490 current loss 0.703795, current_train_items 303712.
I0302 19:02:43.647805 22447428132992 run.py:483] Algo bellman_ford step 9491 current loss 0.829581, current_train_items 303744.
I0302 19:02:43.671780 22447428132992 run.py:483] Algo bellman_ford step 9492 current loss 0.891378, current_train_items 303776.
I0302 19:02:43.702016 22447428132992 run.py:483] Algo bellman_ford step 9493 current loss 0.896967, current_train_items 303808.
I0302 19:02:43.735253 22447428132992 run.py:483] Algo bellman_ford step 9494 current loss 1.067768, current_train_items 303840.
I0302 19:02:43.754694 22447428132992 run.py:483] Algo bellman_ford step 9495 current loss 0.695667, current_train_items 303872.
I0302 19:02:43.771210 22447428132992 run.py:483] Algo bellman_ford step 9496 current loss 0.809945, current_train_items 303904.
I0302 19:02:43.794695 22447428132992 run.py:483] Algo bellman_ford step 9497 current loss 0.873310, current_train_items 303936.
I0302 19:02:43.824772 22447428132992 run.py:483] Algo bellman_ford step 9498 current loss 0.954893, current_train_items 303968.
I0302 19:02:43.858767 22447428132992 run.py:483] Algo bellman_ford step 9499 current loss 1.153542, current_train_items 304000.
I0302 19:02:43.878655 22447428132992 run.py:483] Algo bellman_ford step 9500 current loss 0.570634, current_train_items 304032.
I0302 19:02:43.886670 22447428132992 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:43.886776 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:02:43.903174 22447428132992 run.py:483] Algo bellman_ford step 9501 current loss 0.713487, current_train_items 304064.
I0302 19:02:43.927465 22447428132992 run.py:483] Algo bellman_ford step 9502 current loss 0.997291, current_train_items 304096.
I0302 19:02:43.959196 22447428132992 run.py:483] Algo bellman_ford step 9503 current loss 1.059479, current_train_items 304128.
I0302 19:02:43.991960 22447428132992 run.py:483] Algo bellman_ford step 9504 current loss 1.097804, current_train_items 304160.
I0302 19:02:44.011933 22447428132992 run.py:483] Algo bellman_ford step 9505 current loss 0.615334, current_train_items 304192.
I0302 19:02:44.028151 22447428132992 run.py:483] Algo bellman_ford step 9506 current loss 0.761611, current_train_items 304224.
I0302 19:02:44.052402 22447428132992 run.py:483] Algo bellman_ford step 9507 current loss 0.875332, current_train_items 304256.
I0302 19:02:44.082553 22447428132992 run.py:483] Algo bellman_ford step 9508 current loss 1.042679, current_train_items 304288.
I0302 19:02:44.118013 22447428132992 run.py:483] Algo bellman_ford step 9509 current loss 1.145257, current_train_items 304320.
I0302 19:02:44.137634 22447428132992 run.py:483] Algo bellman_ford step 9510 current loss 0.664766, current_train_items 304352.
I0302 19:02:44.154260 22447428132992 run.py:483] Algo bellman_ford step 9511 current loss 0.783814, current_train_items 304384.
I0302 19:02:44.178080 22447428132992 run.py:483] Algo bellman_ford step 9512 current loss 0.951561, current_train_items 304416.
I0302 19:02:44.209776 22447428132992 run.py:483] Algo bellman_ford step 9513 current loss 1.055209, current_train_items 304448.
I0302 19:02:44.242110 22447428132992 run.py:483] Algo bellman_ford step 9514 current loss 1.196011, current_train_items 304480.
I0302 19:02:44.261442 22447428132992 run.py:483] Algo bellman_ford step 9515 current loss 0.630969, current_train_items 304512.
I0302 19:02:44.277180 22447428132992 run.py:483] Algo bellman_ford step 9516 current loss 0.691023, current_train_items 304544.
I0302 19:02:44.300930 22447428132992 run.py:483] Algo bellman_ford step 9517 current loss 0.953654, current_train_items 304576.
I0302 19:02:44.332135 22447428132992 run.py:483] Algo bellman_ford step 9518 current loss 1.024205, current_train_items 304608.
I0302 19:02:44.364056 22447428132992 run.py:483] Algo bellman_ford step 9519 current loss 1.178908, current_train_items 304640.
I0302 19:02:44.383350 22447428132992 run.py:483] Algo bellman_ford step 9520 current loss 0.502921, current_train_items 304672.
I0302 19:02:44.398862 22447428132992 run.py:483] Algo bellman_ford step 9521 current loss 0.726145, current_train_items 304704.
I0302 19:02:44.421923 22447428132992 run.py:483] Algo bellman_ford step 9522 current loss 0.839167, current_train_items 304736.
I0302 19:02:44.451593 22447428132992 run.py:483] Algo bellman_ford step 9523 current loss 0.874930, current_train_items 304768.
I0302 19:02:44.483989 22447428132992 run.py:483] Algo bellman_ford step 9524 current loss 1.048751, current_train_items 304800.
I0302 19:02:44.502950 22447428132992 run.py:483] Algo bellman_ford step 9525 current loss 0.500099, current_train_items 304832.
I0302 19:02:44.518959 22447428132992 run.py:483] Algo bellman_ford step 9526 current loss 0.716198, current_train_items 304864.
I0302 19:02:44.542815 22447428132992 run.py:483] Algo bellman_ford step 9527 current loss 0.964468, current_train_items 304896.
I0302 19:02:44.574445 22447428132992 run.py:483] Algo bellman_ford step 9528 current loss 1.122185, current_train_items 304928.
I0302 19:02:44.608854 22447428132992 run.py:483] Algo bellman_ford step 9529 current loss 1.079227, current_train_items 304960.
I0302 19:02:44.628128 22447428132992 run.py:483] Algo bellman_ford step 9530 current loss 0.538154, current_train_items 304992.
I0302 19:02:44.644140 22447428132992 run.py:483] Algo bellman_ford step 9531 current loss 0.689459, current_train_items 305024.
I0302 19:02:44.667243 22447428132992 run.py:483] Algo bellman_ford step 9532 current loss 0.854484, current_train_items 305056.
I0302 19:02:44.699934 22447428132992 run.py:483] Algo bellman_ford step 9533 current loss 1.140980, current_train_items 305088.
I0302 19:02:44.732859 22447428132992 run.py:483] Algo bellman_ford step 9534 current loss 1.017192, current_train_items 305120.
I0302 19:02:44.752480 22447428132992 run.py:483] Algo bellman_ford step 9535 current loss 0.897328, current_train_items 305152.
I0302 19:02:44.768600 22447428132992 run.py:483] Algo bellman_ford step 9536 current loss 0.810096, current_train_items 305184.
I0302 19:02:44.792297 22447428132992 run.py:483] Algo bellman_ford step 9537 current loss 0.917333, current_train_items 305216.
I0302 19:02:44.824813 22447428132992 run.py:483] Algo bellman_ford step 9538 current loss 1.021244, current_train_items 305248.
I0302 19:02:44.855877 22447428132992 run.py:483] Algo bellman_ford step 9539 current loss 0.993975, current_train_items 305280.
I0302 19:02:44.875739 22447428132992 run.py:483] Algo bellman_ford step 9540 current loss 0.823460, current_train_items 305312.
I0302 19:02:44.891598 22447428132992 run.py:483] Algo bellman_ford step 9541 current loss 0.781529, current_train_items 305344.
I0302 19:02:44.914486 22447428132992 run.py:483] Algo bellman_ford step 9542 current loss 0.934469, current_train_items 305376.
I0302 19:02:44.947076 22447428132992 run.py:483] Algo bellman_ford step 9543 current loss 0.992827, current_train_items 305408.
I0302 19:02:44.980517 22447428132992 run.py:483] Algo bellman_ford step 9544 current loss 1.081319, current_train_items 305440.
I0302 19:02:45.000034 22447428132992 run.py:483] Algo bellman_ford step 9545 current loss 0.742060, current_train_items 305472.
I0302 19:02:45.015571 22447428132992 run.py:483] Algo bellman_ford step 9546 current loss 0.729113, current_train_items 305504.
I0302 19:02:45.036864 22447428132992 run.py:483] Algo bellman_ford step 9547 current loss 0.777452, current_train_items 305536.
I0302 19:02:45.067772 22447428132992 run.py:483] Algo bellman_ford step 9548 current loss 1.041938, current_train_items 305568.
I0302 19:02:45.101214 22447428132992 run.py:483] Algo bellman_ford step 9549 current loss 1.104905, current_train_items 305600.
I0302 19:02:45.120426 22447428132992 run.py:483] Algo bellman_ford step 9550 current loss 0.595981, current_train_items 305632.
I0302 19:02:45.128537 22447428132992 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:45.128641 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:45.145238 22447428132992 run.py:483] Algo bellman_ford step 9551 current loss 0.716480, current_train_items 305664.
I0302 19:02:45.169470 22447428132992 run.py:483] Algo bellman_ford step 9552 current loss 1.016213, current_train_items 305696.
I0302 19:02:45.202550 22447428132992 run.py:483] Algo bellman_ford step 9553 current loss 0.949229, current_train_items 305728.
I0302 19:02:45.235362 22447428132992 run.py:483] Algo bellman_ford step 9554 current loss 1.032576, current_train_items 305760.
I0302 19:02:45.255153 22447428132992 run.py:483] Algo bellman_ford step 9555 current loss 0.729377, current_train_items 305792.
I0302 19:02:45.270464 22447428132992 run.py:483] Algo bellman_ford step 9556 current loss 0.668808, current_train_items 305824.
I0302 19:02:45.294513 22447428132992 run.py:483] Algo bellman_ford step 9557 current loss 0.859567, current_train_items 305856.
I0302 19:02:45.325440 22447428132992 run.py:483] Algo bellman_ford step 9558 current loss 0.969830, current_train_items 305888.
I0302 19:02:45.357916 22447428132992 run.py:483] Algo bellman_ford step 9559 current loss 1.125000, current_train_items 305920.
I0302 19:02:45.377756 22447428132992 run.py:483] Algo bellman_ford step 9560 current loss 0.736283, current_train_items 305952.
I0302 19:02:45.393939 22447428132992 run.py:483] Algo bellman_ford step 9561 current loss 0.754423, current_train_items 305984.
I0302 19:02:45.416821 22447428132992 run.py:483] Algo bellman_ford step 9562 current loss 0.873583, current_train_items 306016.
I0302 19:02:45.447506 22447428132992 run.py:483] Algo bellman_ford step 9563 current loss 0.932352, current_train_items 306048.
I0302 19:02:45.480048 22447428132992 run.py:483] Algo bellman_ford step 9564 current loss 1.005513, current_train_items 306080.
I0302 19:02:45.499517 22447428132992 run.py:483] Algo bellman_ford step 9565 current loss 0.806716, current_train_items 306112.
I0302 19:02:45.516094 22447428132992 run.py:483] Algo bellman_ford step 9566 current loss 0.807368, current_train_items 306144.
I0302 19:02:45.540622 22447428132992 run.py:483] Algo bellman_ford step 9567 current loss 1.048806, current_train_items 306176.
I0302 19:02:45.572178 22447428132992 run.py:483] Algo bellman_ford step 9568 current loss 1.032219, current_train_items 306208.
I0302 19:02:45.605299 22447428132992 run.py:483] Algo bellman_ford step 9569 current loss 1.085055, current_train_items 306240.
I0302 19:02:45.625015 22447428132992 run.py:483] Algo bellman_ford step 9570 current loss 0.723954, current_train_items 306272.
I0302 19:02:45.641181 22447428132992 run.py:483] Algo bellman_ford step 9571 current loss 0.833868, current_train_items 306304.
I0302 19:02:45.665373 22447428132992 run.py:483] Algo bellman_ford step 9572 current loss 1.093531, current_train_items 306336.
I0302 19:02:45.695738 22447428132992 run.py:483] Algo bellman_ford step 9573 current loss 0.991055, current_train_items 306368.
I0302 19:02:45.729756 22447428132992 run.py:483] Algo bellman_ford step 9574 current loss 1.118721, current_train_items 306400.
I0302 19:02:45.749856 22447428132992 run.py:483] Algo bellman_ford step 9575 current loss 0.639793, current_train_items 306432.
I0302 19:02:45.766688 22447428132992 run.py:483] Algo bellman_ford step 9576 current loss 0.774762, current_train_items 306464.
I0302 19:02:45.791065 22447428132992 run.py:483] Algo bellman_ford step 9577 current loss 1.020576, current_train_items 306496.
I0302 19:02:45.821853 22447428132992 run.py:483] Algo bellman_ford step 9578 current loss 0.964809, current_train_items 306528.
I0302 19:02:45.855473 22447428132992 run.py:483] Algo bellman_ford step 9579 current loss 1.110068, current_train_items 306560.
I0302 19:02:45.874761 22447428132992 run.py:483] Algo bellman_ford step 9580 current loss 0.613306, current_train_items 306592.
I0302 19:02:45.890702 22447428132992 run.py:483] Algo bellman_ford step 9581 current loss 0.753730, current_train_items 306624.
I0302 19:02:45.915595 22447428132992 run.py:483] Algo bellman_ford step 9582 current loss 0.905653, current_train_items 306656.
I0302 19:02:45.947925 22447428132992 run.py:483] Algo bellman_ford step 9583 current loss 0.972920, current_train_items 306688.
I0302 19:02:45.982065 22447428132992 run.py:483] Algo bellman_ford step 9584 current loss 1.120878, current_train_items 306720.
I0302 19:02:46.002182 22447428132992 run.py:483] Algo bellman_ford step 9585 current loss 0.673360, current_train_items 306752.
I0302 19:02:46.018490 22447428132992 run.py:483] Algo bellman_ford step 9586 current loss 0.864365, current_train_items 306784.
I0302 19:02:46.042176 22447428132992 run.py:483] Algo bellman_ford step 9587 current loss 0.942831, current_train_items 306816.
I0302 19:02:46.073764 22447428132992 run.py:483] Algo bellman_ford step 9588 current loss 1.032254, current_train_items 306848.
I0302 19:02:46.108248 22447428132992 run.py:483] Algo bellman_ford step 9589 current loss 1.173179, current_train_items 306880.
I0302 19:02:46.127934 22447428132992 run.py:483] Algo bellman_ford step 9590 current loss 1.323793, current_train_items 306912.
I0302 19:02:46.144139 22447428132992 run.py:483] Algo bellman_ford step 9591 current loss 0.884681, current_train_items 306944.
I0302 19:02:46.167670 22447428132992 run.py:483] Algo bellman_ford step 9592 current loss 0.923674, current_train_items 306976.
I0302 19:02:46.199537 22447428132992 run.py:483] Algo bellman_ford step 9593 current loss 0.940915, current_train_items 307008.
I0302 19:02:46.232286 22447428132992 run.py:483] Algo bellman_ford step 9594 current loss 1.134408, current_train_items 307040.
I0302 19:02:46.251806 22447428132992 run.py:483] Algo bellman_ford step 9595 current loss 0.530894, current_train_items 307072.
I0302 19:02:46.268150 22447428132992 run.py:483] Algo bellman_ford step 9596 current loss 0.739808, current_train_items 307104.
I0302 19:02:46.292385 22447428132992 run.py:483] Algo bellman_ford step 9597 current loss 0.868337, current_train_items 307136.
I0302 19:02:46.322298 22447428132992 run.py:483] Algo bellman_ford step 9598 current loss 0.789715, current_train_items 307168.
I0302 19:02:46.355444 22447428132992 run.py:483] Algo bellman_ford step 9599 current loss 0.998541, current_train_items 307200.
I0302 19:02:46.375289 22447428132992 run.py:483] Algo bellman_ford step 9600 current loss 0.513698, current_train_items 307232.
I0302 19:02:46.383241 22447428132992 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:46.383345 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:02:46.400399 22447428132992 run.py:483] Algo bellman_ford step 9601 current loss 0.776160, current_train_items 307264.
I0302 19:02:46.425333 22447428132992 run.py:483] Algo bellman_ford step 9602 current loss 0.892205, current_train_items 307296.
I0302 19:02:46.456437 22447428132992 run.py:483] Algo bellman_ford step 9603 current loss 0.874260, current_train_items 307328.
I0302 19:02:46.492205 22447428132992 run.py:483] Algo bellman_ford step 9604 current loss 1.126015, current_train_items 307360.
I0302 19:02:46.512223 22447428132992 run.py:483] Algo bellman_ford step 9605 current loss 0.650001, current_train_items 307392.
I0302 19:02:46.527983 22447428132992 run.py:483] Algo bellman_ford step 9606 current loss 0.703495, current_train_items 307424.
I0302 19:02:46.551949 22447428132992 run.py:483] Algo bellman_ford step 9607 current loss 0.815039, current_train_items 307456.
I0302 19:02:46.582933 22447428132992 run.py:483] Algo bellman_ford step 9608 current loss 0.955426, current_train_items 307488.
I0302 19:02:46.616290 22447428132992 run.py:483] Algo bellman_ford step 9609 current loss 1.192941, current_train_items 307520.
I0302 19:02:46.636096 22447428132992 run.py:483] Algo bellman_ford step 9610 current loss 0.585676, current_train_items 307552.
I0302 19:02:46.652360 22447428132992 run.py:483] Algo bellman_ford step 9611 current loss 0.723120, current_train_items 307584.
I0302 19:02:46.676935 22447428132992 run.py:483] Algo bellman_ford step 9612 current loss 0.902327, current_train_items 307616.
I0302 19:02:46.708468 22447428132992 run.py:483] Algo bellman_ford step 9613 current loss 1.120007, current_train_items 307648.
I0302 19:02:46.742191 22447428132992 run.py:483] Algo bellman_ford step 9614 current loss 1.031257, current_train_items 307680.
I0302 19:02:46.761809 22447428132992 run.py:483] Algo bellman_ford step 9615 current loss 0.817485, current_train_items 307712.
I0302 19:02:46.778061 22447428132992 run.py:483] Algo bellman_ford step 9616 current loss 0.758643, current_train_items 307744.
I0302 19:02:46.801835 22447428132992 run.py:483] Algo bellman_ford step 9617 current loss 0.950030, current_train_items 307776.
I0302 19:02:46.833115 22447428132992 run.py:483] Algo bellman_ford step 9618 current loss 0.921287, current_train_items 307808.
I0302 19:02:46.867034 22447428132992 run.py:483] Algo bellman_ford step 9619 current loss 1.074465, current_train_items 307840.
I0302 19:02:46.886719 22447428132992 run.py:483] Algo bellman_ford step 9620 current loss 0.729766, current_train_items 307872.
I0302 19:02:46.902677 22447428132992 run.py:483] Algo bellman_ford step 9621 current loss 0.732345, current_train_items 307904.
I0302 19:02:46.927083 22447428132992 run.py:483] Algo bellman_ford step 9622 current loss 0.968028, current_train_items 307936.
I0302 19:02:46.958727 22447428132992 run.py:483] Algo bellman_ford step 9623 current loss 0.914404, current_train_items 307968.
I0302 19:02:46.992592 22447428132992 run.py:483] Algo bellman_ford step 9624 current loss 1.053776, current_train_items 308000.
I0302 19:02:47.012103 22447428132992 run.py:483] Algo bellman_ford step 9625 current loss 0.684959, current_train_items 308032.
I0302 19:02:47.028249 22447428132992 run.py:483] Algo bellman_ford step 9626 current loss 0.683283, current_train_items 308064.
I0302 19:02:47.052081 22447428132992 run.py:483] Algo bellman_ford step 9627 current loss 0.845868, current_train_items 308096.
I0302 19:02:47.082834 22447428132992 run.py:483] Algo bellman_ford step 9628 current loss 0.897842, current_train_items 308128.
I0302 19:02:47.117286 22447428132992 run.py:483] Algo bellman_ford step 9629 current loss 1.146573, current_train_items 308160.
I0302 19:02:47.137122 22447428132992 run.py:483] Algo bellman_ford step 9630 current loss 0.540732, current_train_items 308192.
I0302 19:02:47.153555 22447428132992 run.py:483] Algo bellman_ford step 9631 current loss 0.697715, current_train_items 308224.
I0302 19:02:47.178406 22447428132992 run.py:483] Algo bellman_ford step 9632 current loss 0.982164, current_train_items 308256.
I0302 19:02:47.209002 22447428132992 run.py:483] Algo bellman_ford step 9633 current loss 0.939452, current_train_items 308288.
I0302 19:02:47.243211 22447428132992 run.py:483] Algo bellman_ford step 9634 current loss 1.146046, current_train_items 308320.
I0302 19:02:47.262666 22447428132992 run.py:483] Algo bellman_ford step 9635 current loss 0.555716, current_train_items 308352.
I0302 19:02:47.278900 22447428132992 run.py:483] Algo bellman_ford step 9636 current loss 0.721909, current_train_items 308384.
I0302 19:02:47.303035 22447428132992 run.py:483] Algo bellman_ford step 9637 current loss 0.873112, current_train_items 308416.
I0302 19:02:47.335115 22447428132992 run.py:483] Algo bellman_ford step 9638 current loss 1.139036, current_train_items 308448.
I0302 19:02:47.368404 22447428132992 run.py:483] Algo bellman_ford step 9639 current loss 1.131490, current_train_items 308480.
I0302 19:02:47.387992 22447428132992 run.py:483] Algo bellman_ford step 9640 current loss 0.605292, current_train_items 308512.
I0302 19:02:47.403851 22447428132992 run.py:483] Algo bellman_ford step 9641 current loss 0.706783, current_train_items 308544.
I0302 19:02:47.428070 22447428132992 run.py:483] Algo bellman_ford step 9642 current loss 0.929091, current_train_items 308576.
I0302 19:02:47.461185 22447428132992 run.py:483] Algo bellman_ford step 9643 current loss 0.981472, current_train_items 308608.
I0302 19:02:47.492886 22447428132992 run.py:483] Algo bellman_ford step 9644 current loss 1.299614, current_train_items 308640.
I0302 19:02:47.512576 22447428132992 run.py:483] Algo bellman_ford step 9645 current loss 0.631630, current_train_items 308672.
I0302 19:02:47.528859 22447428132992 run.py:483] Algo bellman_ford step 9646 current loss 0.818676, current_train_items 308704.
I0302 19:02:47.552433 22447428132992 run.py:483] Algo bellman_ford step 9647 current loss 0.888833, current_train_items 308736.
I0302 19:02:47.584678 22447428132992 run.py:483] Algo bellman_ford step 9648 current loss 0.956124, current_train_items 308768.
I0302 19:02:47.618581 22447428132992 run.py:483] Algo bellman_ford step 9649 current loss 1.124538, current_train_items 308800.
I0302 19:02:47.638024 22447428132992 run.py:483] Algo bellman_ford step 9650 current loss 0.529593, current_train_items 308832.
I0302 19:02:47.646087 22447428132992 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:47.646229 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:47.663100 22447428132992 run.py:483] Algo bellman_ford step 9651 current loss 0.731981, current_train_items 308864.
I0302 19:02:47.688620 22447428132992 run.py:483] Algo bellman_ford step 9652 current loss 1.012088, current_train_items 308896.
I0302 19:02:47.720668 22447428132992 run.py:483] Algo bellman_ford step 9653 current loss 1.088491, current_train_items 308928.
I0302 19:02:47.756038 22447428132992 run.py:483] Algo bellman_ford step 9654 current loss 1.159037, current_train_items 308960.
I0302 19:02:47.776150 22447428132992 run.py:483] Algo bellman_ford step 9655 current loss 0.580203, current_train_items 308992.
I0302 19:02:47.792108 22447428132992 run.py:483] Algo bellman_ford step 9656 current loss 0.714701, current_train_items 309024.
I0302 19:02:47.815761 22447428132992 run.py:483] Algo bellman_ford step 9657 current loss 0.872819, current_train_items 309056.
I0302 19:02:47.848060 22447428132992 run.py:483] Algo bellman_ford step 9658 current loss 0.964132, current_train_items 309088.
I0302 19:02:47.880066 22447428132992 run.py:483] Algo bellman_ford step 9659 current loss 1.003053, current_train_items 309120.
I0302 19:02:47.899656 22447428132992 run.py:483] Algo bellman_ford step 9660 current loss 0.541509, current_train_items 309152.
I0302 19:02:47.916231 22447428132992 run.py:483] Algo bellman_ford step 9661 current loss 0.777395, current_train_items 309184.
I0302 19:02:47.939702 22447428132992 run.py:483] Algo bellman_ford step 9662 current loss 0.953526, current_train_items 309216.
I0302 19:02:47.970756 22447428132992 run.py:483] Algo bellman_ford step 9663 current loss 0.992984, current_train_items 309248.
I0302 19:02:48.004511 22447428132992 run.py:483] Algo bellman_ford step 9664 current loss 1.146208, current_train_items 309280.
I0302 19:02:48.023900 22447428132992 run.py:483] Algo bellman_ford step 9665 current loss 0.574317, current_train_items 309312.
I0302 19:02:48.040117 22447428132992 run.py:483] Algo bellman_ford step 9666 current loss 0.700797, current_train_items 309344.
I0302 19:02:48.063120 22447428132992 run.py:483] Algo bellman_ford step 9667 current loss 0.832207, current_train_items 309376.
I0302 19:02:48.094018 22447428132992 run.py:483] Algo bellman_ford step 9668 current loss 0.934594, current_train_items 309408.
I0302 19:02:48.127199 22447428132992 run.py:483] Algo bellman_ford step 9669 current loss 1.025991, current_train_items 309440.
I0302 19:02:48.147172 22447428132992 run.py:483] Algo bellman_ford step 9670 current loss 0.529258, current_train_items 309472.
I0302 19:02:48.163280 22447428132992 run.py:483] Algo bellman_ford step 9671 current loss 0.634651, current_train_items 309504.
I0302 19:02:48.186951 22447428132992 run.py:483] Algo bellman_ford step 9672 current loss 0.840925, current_train_items 309536.
I0302 19:02:48.217711 22447428132992 run.py:483] Algo bellman_ford step 9673 current loss 0.827850, current_train_items 309568.
I0302 19:02:48.250801 22447428132992 run.py:483] Algo bellman_ford step 9674 current loss 1.046757, current_train_items 309600.
I0302 19:02:48.270619 22447428132992 run.py:483] Algo bellman_ford step 9675 current loss 0.503171, current_train_items 309632.
I0302 19:02:48.287132 22447428132992 run.py:483] Algo bellman_ford step 9676 current loss 0.824153, current_train_items 309664.
I0302 19:02:48.310955 22447428132992 run.py:483] Algo bellman_ford step 9677 current loss 0.815907, current_train_items 309696.
I0302 19:02:48.340945 22447428132992 run.py:483] Algo bellman_ford step 9678 current loss 0.951225, current_train_items 309728.
I0302 19:02:48.374707 22447428132992 run.py:483] Algo bellman_ford step 9679 current loss 1.048443, current_train_items 309760.
I0302 19:02:48.394243 22447428132992 run.py:483] Algo bellman_ford step 9680 current loss 0.566993, current_train_items 309792.
I0302 19:02:48.410598 22447428132992 run.py:483] Algo bellman_ford step 9681 current loss 0.718464, current_train_items 309824.
I0302 19:02:48.434002 22447428132992 run.py:483] Algo bellman_ford step 9682 current loss 0.775911, current_train_items 309856.
I0302 19:02:48.464724 22447428132992 run.py:483] Algo bellman_ford step 9683 current loss 1.038944, current_train_items 309888.
I0302 19:02:48.496922 22447428132992 run.py:483] Algo bellman_ford step 9684 current loss 1.030399, current_train_items 309920.
I0302 19:02:48.516594 22447428132992 run.py:483] Algo bellman_ford step 9685 current loss 0.601380, current_train_items 309952.
I0302 19:02:48.532651 22447428132992 run.py:483] Algo bellman_ford step 9686 current loss 0.840018, current_train_items 309984.
I0302 19:02:48.556113 22447428132992 run.py:483] Algo bellman_ford step 9687 current loss 0.944763, current_train_items 310016.
I0302 19:02:48.587047 22447428132992 run.py:483] Algo bellman_ford step 9688 current loss 1.070152, current_train_items 310048.
I0302 19:02:48.620540 22447428132992 run.py:483] Algo bellman_ford step 9689 current loss 1.075612, current_train_items 310080.
I0302 19:02:48.640336 22447428132992 run.py:483] Algo bellman_ford step 9690 current loss 0.688493, current_train_items 310112.
I0302 19:02:48.656110 22447428132992 run.py:483] Algo bellman_ford step 9691 current loss 0.671142, current_train_items 310144.
I0302 19:02:48.679322 22447428132992 run.py:483] Algo bellman_ford step 9692 current loss 0.833365, current_train_items 310176.
I0302 19:02:48.709914 22447428132992 run.py:483] Algo bellman_ford step 9693 current loss 0.939088, current_train_items 310208.
I0302 19:02:48.743412 22447428132992 run.py:483] Algo bellman_ford step 9694 current loss 1.013893, current_train_items 310240.
I0302 19:02:48.762675 22447428132992 run.py:483] Algo bellman_ford step 9695 current loss 0.603892, current_train_items 310272.
I0302 19:02:48.778797 22447428132992 run.py:483] Algo bellman_ford step 9696 current loss 0.903269, current_train_items 310304.
I0302 19:02:48.801707 22447428132992 run.py:483] Algo bellman_ford step 9697 current loss 0.804949, current_train_items 310336.
I0302 19:02:48.835585 22447428132992 run.py:483] Algo bellman_ford step 9698 current loss 1.062679, current_train_items 310368.
I0302 19:02:48.870769 22447428132992 run.py:483] Algo bellman_ford step 9699 current loss 1.045939, current_train_items 310400.
I0302 19:02:48.890811 22447428132992 run.py:483] Algo bellman_ford step 9700 current loss 0.598801, current_train_items 310432.
I0302 19:02:48.898654 22447428132992 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:48.898760 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:02:48.915334 22447428132992 run.py:483] Algo bellman_ford step 9701 current loss 0.755505, current_train_items 310464.
I0302 19:02:48.939963 22447428132992 run.py:483] Algo bellman_ford step 9702 current loss 0.920939, current_train_items 310496.
I0302 19:02:48.972311 22447428132992 run.py:483] Algo bellman_ford step 9703 current loss 0.996459, current_train_items 310528.
I0302 19:02:49.005610 22447428132992 run.py:483] Algo bellman_ford step 9704 current loss 0.988434, current_train_items 310560.
I0302 19:02:49.025893 22447428132992 run.py:483] Algo bellman_ford step 9705 current loss 0.546096, current_train_items 310592.
I0302 19:02:49.041792 22447428132992 run.py:483] Algo bellman_ford step 9706 current loss 0.788197, current_train_items 310624.
I0302 19:02:49.066827 22447428132992 run.py:483] Algo bellman_ford step 9707 current loss 0.997571, current_train_items 310656.
I0302 19:02:49.098627 22447428132992 run.py:483] Algo bellman_ford step 9708 current loss 0.915873, current_train_items 310688.
I0302 19:02:49.129981 22447428132992 run.py:483] Algo bellman_ford step 9709 current loss 1.063485, current_train_items 310720.
I0302 19:02:49.149520 22447428132992 run.py:483] Algo bellman_ford step 9710 current loss 0.488482, current_train_items 310752.
I0302 19:02:49.165471 22447428132992 run.py:483] Algo bellman_ford step 9711 current loss 0.723824, current_train_items 310784.
I0302 19:02:49.189153 22447428132992 run.py:483] Algo bellman_ford step 9712 current loss 0.893135, current_train_items 310816.
I0302 19:02:49.220776 22447428132992 run.py:483] Algo bellman_ford step 9713 current loss 0.933266, current_train_items 310848.
I0302 19:02:49.254651 22447428132992 run.py:483] Algo bellman_ford step 9714 current loss 1.051418, current_train_items 310880.
I0302 19:02:49.274218 22447428132992 run.py:483] Algo bellman_ford step 9715 current loss 0.588113, current_train_items 310912.
I0302 19:02:49.290683 22447428132992 run.py:483] Algo bellman_ford step 9716 current loss 0.740493, current_train_items 310944.
I0302 19:02:49.313226 22447428132992 run.py:483] Algo bellman_ford step 9717 current loss 0.992613, current_train_items 310976.
I0302 19:02:49.344722 22447428132992 run.py:483] Algo bellman_ford step 9718 current loss 1.014306, current_train_items 311008.
I0302 19:02:49.375566 22447428132992 run.py:483] Algo bellman_ford step 9719 current loss 0.920376, current_train_items 311040.
I0302 19:02:49.394872 22447428132992 run.py:483] Algo bellman_ford step 9720 current loss 0.573525, current_train_items 311072.
I0302 19:02:49.410451 22447428132992 run.py:483] Algo bellman_ford step 9721 current loss 0.733089, current_train_items 311104.
I0302 19:02:49.433532 22447428132992 run.py:483] Algo bellman_ford step 9722 current loss 0.889478, current_train_items 311136.
I0302 19:02:49.465091 22447428132992 run.py:483] Algo bellman_ford step 9723 current loss 1.000702, current_train_items 311168.
I0302 19:02:49.498396 22447428132992 run.py:483] Algo bellman_ford step 9724 current loss 1.116154, current_train_items 311200.
I0302 19:02:49.517742 22447428132992 run.py:483] Algo bellman_ford step 9725 current loss 0.513718, current_train_items 311232.
I0302 19:02:49.533576 22447428132992 run.py:483] Algo bellman_ford step 9726 current loss 0.724368, current_train_items 311264.
I0302 19:02:49.556400 22447428132992 run.py:483] Algo bellman_ford step 9727 current loss 0.847425, current_train_items 311296.
I0302 19:02:49.587983 22447428132992 run.py:483] Algo bellman_ford step 9728 current loss 1.031913, current_train_items 311328.
I0302 19:02:49.621994 22447428132992 run.py:483] Algo bellman_ford step 9729 current loss 1.142311, current_train_items 311360.
I0302 19:02:49.641643 22447428132992 run.py:483] Algo bellman_ford step 9730 current loss 0.989044, current_train_items 311392.
I0302 19:02:49.658004 22447428132992 run.py:483] Algo bellman_ford step 9731 current loss 0.798718, current_train_items 311424.
I0302 19:02:49.681239 22447428132992 run.py:483] Algo bellman_ford step 9732 current loss 0.883033, current_train_items 311456.
I0302 19:02:49.712138 22447428132992 run.py:483] Algo bellman_ford step 9733 current loss 0.956515, current_train_items 311488.
I0302 19:02:49.744453 22447428132992 run.py:483] Algo bellman_ford step 9734 current loss 1.054077, current_train_items 311520.
I0302 19:02:49.763798 22447428132992 run.py:483] Algo bellman_ford step 9735 current loss 1.242546, current_train_items 311552.
I0302 19:02:49.779867 22447428132992 run.py:483] Algo bellman_ford step 9736 current loss 1.050554, current_train_items 311584.
I0302 19:02:49.803057 22447428132992 run.py:483] Algo bellman_ford step 9737 current loss 0.915198, current_train_items 311616.
I0302 19:02:49.835696 22447428132992 run.py:483] Algo bellman_ford step 9738 current loss 1.107241, current_train_items 311648.
I0302 19:02:49.869662 22447428132992 run.py:483] Algo bellman_ford step 9739 current loss 1.168415, current_train_items 311680.
I0302 19:02:49.888926 22447428132992 run.py:483] Algo bellman_ford step 9740 current loss 0.825340, current_train_items 311712.
I0302 19:02:49.905716 22447428132992 run.py:483] Algo bellman_ford step 9741 current loss 0.890312, current_train_items 311744.
I0302 19:02:49.930711 22447428132992 run.py:483] Algo bellman_ford step 9742 current loss 0.916802, current_train_items 311776.
I0302 19:02:49.962792 22447428132992 run.py:483] Algo bellman_ford step 9743 current loss 1.002998, current_train_items 311808.
I0302 19:02:49.996225 22447428132992 run.py:483] Algo bellman_ford step 9744 current loss 1.126956, current_train_items 311840.
I0302 19:02:50.015770 22447428132992 run.py:483] Algo bellman_ford step 9745 current loss 0.626360, current_train_items 311872.
I0302 19:02:50.032541 22447428132992 run.py:483] Algo bellman_ford step 9746 current loss 0.729467, current_train_items 311904.
I0302 19:02:50.055709 22447428132992 run.py:483] Algo bellman_ford step 9747 current loss 0.966529, current_train_items 311936.
I0302 19:02:50.084821 22447428132992 run.py:483] Algo bellman_ford step 9748 current loss 0.873879, current_train_items 311968.
I0302 19:02:50.116820 22447428132992 run.py:483] Algo bellman_ford step 9749 current loss 1.131537, current_train_items 312000.
I0302 19:02:50.136512 22447428132992 run.py:483] Algo bellman_ford step 9750 current loss 0.591430, current_train_items 312032.
I0302 19:02:50.144652 22447428132992 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:50.144757 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:02:50.161182 22447428132992 run.py:483] Algo bellman_ford step 9751 current loss 0.833881, current_train_items 312064.
I0302 19:02:50.186596 22447428132992 run.py:483] Algo bellman_ford step 9752 current loss 0.983192, current_train_items 312096.
I0302 19:02:50.217454 22447428132992 run.py:483] Algo bellman_ford step 9753 current loss 1.102594, current_train_items 312128.
I0302 19:02:50.250758 22447428132992 run.py:483] Algo bellman_ford step 9754 current loss 1.039228, current_train_items 312160.
I0302 19:02:50.271071 22447428132992 run.py:483] Algo bellman_ford step 9755 current loss 0.572680, current_train_items 312192.
I0302 19:02:50.287568 22447428132992 run.py:483] Algo bellman_ford step 9756 current loss 0.901096, current_train_items 312224.
I0302 19:02:50.310336 22447428132992 run.py:483] Algo bellman_ford step 9757 current loss 0.868251, current_train_items 312256.
I0302 19:02:50.342801 22447428132992 run.py:483] Algo bellman_ford step 9758 current loss 1.059967, current_train_items 312288.
I0302 19:02:50.376368 22447428132992 run.py:483] Algo bellman_ford step 9759 current loss 1.076615, current_train_items 312320.
I0302 19:02:50.396661 22447428132992 run.py:483] Algo bellman_ford step 9760 current loss 0.568090, current_train_items 312352.
I0302 19:02:50.413229 22447428132992 run.py:483] Algo bellman_ford step 9761 current loss 0.856344, current_train_items 312384.
I0302 19:02:50.437097 22447428132992 run.py:483] Algo bellman_ford step 9762 current loss 0.975543, current_train_items 312416.
I0302 19:02:50.469281 22447428132992 run.py:483] Algo bellman_ford step 9763 current loss 0.961916, current_train_items 312448.
I0302 19:02:50.499994 22447428132992 run.py:483] Algo bellman_ford step 9764 current loss 1.045666, current_train_items 312480.
I0302 19:02:50.519479 22447428132992 run.py:483] Algo bellman_ford step 9765 current loss 0.555751, current_train_items 312512.
I0302 19:02:50.535285 22447428132992 run.py:483] Algo bellman_ford step 9766 current loss 0.799675, current_train_items 312544.
I0302 19:02:50.559395 22447428132992 run.py:483] Algo bellman_ford step 9767 current loss 1.005502, current_train_items 312576.
I0302 19:02:50.590505 22447428132992 run.py:483] Algo bellman_ford step 9768 current loss 0.928221, current_train_items 312608.
I0302 19:02:50.624271 22447428132992 run.py:483] Algo bellman_ford step 9769 current loss 1.160290, current_train_items 312640.
I0302 19:02:50.644275 22447428132992 run.py:483] Algo bellman_ford step 9770 current loss 0.567486, current_train_items 312672.
I0302 19:02:50.660189 22447428132992 run.py:483] Algo bellman_ford step 9771 current loss 0.749905, current_train_items 312704.
I0302 19:02:50.683230 22447428132992 run.py:483] Algo bellman_ford step 9772 current loss 0.932020, current_train_items 312736.
I0302 19:02:50.715854 22447428132992 run.py:483] Algo bellman_ford step 9773 current loss 1.062985, current_train_items 312768.
I0302 19:02:50.749338 22447428132992 run.py:483] Algo bellman_ford step 9774 current loss 1.204749, current_train_items 312800.
I0302 19:02:50.769252 22447428132992 run.py:483] Algo bellman_ford step 9775 current loss 0.517110, current_train_items 312832.
I0302 19:02:50.785238 22447428132992 run.py:483] Algo bellman_ford step 9776 current loss 0.667472, current_train_items 312864.
I0302 19:02:50.809740 22447428132992 run.py:483] Algo bellman_ford step 9777 current loss 0.947907, current_train_items 312896.
I0302 19:02:50.841821 22447428132992 run.py:483] Algo bellman_ford step 9778 current loss 0.936154, current_train_items 312928.
I0302 19:02:50.875305 22447428132992 run.py:483] Algo bellman_ford step 9779 current loss 1.111702, current_train_items 312960.
I0302 19:02:50.895196 22447428132992 run.py:483] Algo bellman_ford step 9780 current loss 0.556172, current_train_items 312992.
I0302 19:02:50.911643 22447428132992 run.py:483] Algo bellman_ford step 9781 current loss 0.831449, current_train_items 313024.
I0302 19:02:50.934926 22447428132992 run.py:483] Algo bellman_ford step 9782 current loss 0.945737, current_train_items 313056.
I0302 19:02:50.968277 22447428132992 run.py:483] Algo bellman_ford step 9783 current loss 1.057859, current_train_items 313088.
I0302 19:02:51.000107 22447428132992 run.py:483] Algo bellman_ford step 9784 current loss 1.138988, current_train_items 313120.
I0302 19:02:51.020096 22447428132992 run.py:483] Algo bellman_ford step 9785 current loss 0.763266, current_train_items 313152.
I0302 19:02:51.036646 22447428132992 run.py:483] Algo bellman_ford step 9786 current loss 0.826577, current_train_items 313184.
I0302 19:02:51.059909 22447428132992 run.py:483] Algo bellman_ford step 9787 current loss 0.893113, current_train_items 313216.
I0302 19:02:51.090955 22447428132992 run.py:483] Algo bellman_ford step 9788 current loss 0.923556, current_train_items 313248.
I0302 19:02:51.124610 22447428132992 run.py:483] Algo bellman_ford step 9789 current loss 0.995837, current_train_items 313280.
I0302 19:02:51.144191 22447428132992 run.py:483] Algo bellman_ford step 9790 current loss 0.799002, current_train_items 313312.
I0302 19:02:51.160162 22447428132992 run.py:483] Algo bellman_ford step 9791 current loss 0.728379, current_train_items 313344.
I0302 19:02:51.183279 22447428132992 run.py:483] Algo bellman_ford step 9792 current loss 0.874676, current_train_items 313376.
I0302 19:02:51.214740 22447428132992 run.py:483] Algo bellman_ford step 9793 current loss 1.017373, current_train_items 313408.
I0302 19:02:51.246488 22447428132992 run.py:483] Algo bellman_ford step 9794 current loss 1.002984, current_train_items 313440.
I0302 19:02:51.266248 22447428132992 run.py:483] Algo bellman_ford step 9795 current loss 0.637695, current_train_items 313472.
I0302 19:02:51.282719 22447428132992 run.py:483] Algo bellman_ford step 9796 current loss 0.729777, current_train_items 313504.
I0302 19:02:51.306225 22447428132992 run.py:483] Algo bellman_ford step 9797 current loss 0.843849, current_train_items 313536.
I0302 19:02:51.337485 22447428132992 run.py:483] Algo bellman_ford step 9798 current loss 0.882131, current_train_items 313568.
I0302 19:02:51.370111 22447428132992 run.py:483] Algo bellman_ford step 9799 current loss 1.008649, current_train_items 313600.
I0302 19:02:51.389911 22447428132992 run.py:483] Algo bellman_ford step 9800 current loss 0.632800, current_train_items 313632.
I0302 19:02:51.397845 22447428132992 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:51.397950 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:02:51.414394 22447428132992 run.py:483] Algo bellman_ford step 9801 current loss 0.728259, current_train_items 313664.
I0302 19:02:51.438615 22447428132992 run.py:483] Algo bellman_ford step 9802 current loss 0.949044, current_train_items 313696.
I0302 19:02:51.470883 22447428132992 run.py:483] Algo bellman_ford step 9803 current loss 1.085312, current_train_items 313728.
I0302 19:02:51.503589 22447428132992 run.py:483] Algo bellman_ford step 9804 current loss 1.023234, current_train_items 313760.
I0302 19:02:51.523430 22447428132992 run.py:483] Algo bellman_ford step 9805 current loss 0.649884, current_train_items 313792.
I0302 19:02:51.539409 22447428132992 run.py:483] Algo bellman_ford step 9806 current loss 0.776974, current_train_items 313824.
I0302 19:02:51.562181 22447428132992 run.py:483] Algo bellman_ford step 9807 current loss 0.913825, current_train_items 313856.
I0302 19:02:51.593513 22447428132992 run.py:483] Algo bellman_ford step 9808 current loss 0.988100, current_train_items 313888.
I0302 19:02:51.628893 22447428132992 run.py:483] Algo bellman_ford step 9809 current loss 1.188144, current_train_items 313920.
I0302 19:02:51.648433 22447428132992 run.py:483] Algo bellman_ford step 9810 current loss 0.692385, current_train_items 313952.
I0302 19:02:51.664199 22447428132992 run.py:483] Algo bellman_ford step 9811 current loss 0.741343, current_train_items 313984.
I0302 19:02:51.688544 22447428132992 run.py:483] Algo bellman_ford step 9812 current loss 1.035478, current_train_items 314016.
I0302 19:02:51.717734 22447428132992 run.py:483] Algo bellman_ford step 9813 current loss 0.875892, current_train_items 314048.
I0302 19:02:51.751416 22447428132992 run.py:483] Algo bellman_ford step 9814 current loss 1.056399, current_train_items 314080.
I0302 19:02:51.770601 22447428132992 run.py:483] Algo bellman_ford step 9815 current loss 0.549106, current_train_items 314112.
I0302 19:02:51.786913 22447428132992 run.py:483] Algo bellman_ford step 9816 current loss 0.797761, current_train_items 314144.
I0302 19:02:51.809942 22447428132992 run.py:483] Algo bellman_ford step 9817 current loss 0.863474, current_train_items 314176.
I0302 19:02:51.842141 22447428132992 run.py:483] Algo bellman_ford step 9818 current loss 1.076600, current_train_items 314208.
I0302 19:02:51.874967 22447428132992 run.py:483] Algo bellman_ford step 9819 current loss 1.007154, current_train_items 314240.
I0302 19:02:51.894116 22447428132992 run.py:483] Algo bellman_ford step 9820 current loss 0.519441, current_train_items 314272.
I0302 19:02:51.910050 22447428132992 run.py:483] Algo bellman_ford step 9821 current loss 0.745599, current_train_items 314304.
I0302 19:02:51.934045 22447428132992 run.py:483] Algo bellman_ford step 9822 current loss 0.959661, current_train_items 314336.
I0302 19:02:51.964611 22447428132992 run.py:483] Algo bellman_ford step 9823 current loss 0.892637, current_train_items 314368.
I0302 19:02:51.997272 22447428132992 run.py:483] Algo bellman_ford step 9824 current loss 1.032091, current_train_items 314400.
I0302 19:02:52.016711 22447428132992 run.py:483] Algo bellman_ford step 9825 current loss 0.527488, current_train_items 314432.
I0302 19:02:52.032585 22447428132992 run.py:483] Algo bellman_ford step 9826 current loss 0.738115, current_train_items 314464.
I0302 19:02:52.055874 22447428132992 run.py:483] Algo bellman_ford step 9827 current loss 0.906361, current_train_items 314496.
I0302 19:02:52.086157 22447428132992 run.py:483] Algo bellman_ford step 9828 current loss 0.906778, current_train_items 314528.
I0302 19:02:52.120301 22447428132992 run.py:483] Algo bellman_ford step 9829 current loss 1.114722, current_train_items 314560.
I0302 19:02:52.139366 22447428132992 run.py:483] Algo bellman_ford step 9830 current loss 0.528711, current_train_items 314592.
I0302 19:02:52.156025 22447428132992 run.py:483] Algo bellman_ford step 9831 current loss 0.766137, current_train_items 314624.
I0302 19:02:52.179443 22447428132992 run.py:483] Algo bellman_ford step 9832 current loss 0.830567, current_train_items 314656.
I0302 19:02:52.211016 22447428132992 run.py:483] Algo bellman_ford step 9833 current loss 1.003891, current_train_items 314688.
I0302 19:02:52.242588 22447428132992 run.py:483] Algo bellman_ford step 9834 current loss 1.017275, current_train_items 314720.
I0302 19:02:52.261797 22447428132992 run.py:483] Algo bellman_ford step 9835 current loss 0.627997, current_train_items 314752.
I0302 19:02:52.277918 22447428132992 run.py:483] Algo bellman_ford step 9836 current loss 0.742006, current_train_items 314784.
I0302 19:02:52.301268 22447428132992 run.py:483] Algo bellman_ford step 9837 current loss 0.965351, current_train_items 314816.
I0302 19:02:52.333448 22447428132992 run.py:483] Algo bellman_ford step 9838 current loss 1.026279, current_train_items 314848.
I0302 19:02:52.368089 22447428132992 run.py:483] Algo bellman_ford step 9839 current loss 1.108416, current_train_items 314880.
I0302 19:02:52.387389 22447428132992 run.py:483] Algo bellman_ford step 9840 current loss 0.560579, current_train_items 314912.
I0302 19:02:52.403460 22447428132992 run.py:483] Algo bellman_ford step 9841 current loss 0.705099, current_train_items 314944.
I0302 19:02:52.427465 22447428132992 run.py:483] Algo bellman_ford step 9842 current loss 0.849855, current_train_items 314976.
I0302 19:02:52.458341 22447428132992 run.py:483] Algo bellman_ford step 9843 current loss 0.856296, current_train_items 315008.
I0302 19:02:52.489453 22447428132992 run.py:483] Algo bellman_ford step 9844 current loss 0.998581, current_train_items 315040.
I0302 19:02:52.508886 22447428132992 run.py:483] Algo bellman_ford step 9845 current loss 0.706993, current_train_items 315072.
I0302 19:02:52.525273 22447428132992 run.py:483] Algo bellman_ford step 9846 current loss 0.807630, current_train_items 315104.
I0302 19:02:52.549511 22447428132992 run.py:483] Algo bellman_ford step 9847 current loss 0.863222, current_train_items 315136.
I0302 19:02:52.580347 22447428132992 run.py:483] Algo bellman_ford step 9848 current loss 0.938402, current_train_items 315168.
I0302 19:02:52.613462 22447428132992 run.py:483] Algo bellman_ford step 9849 current loss 1.039224, current_train_items 315200.
I0302 19:02:52.633075 22447428132992 run.py:483] Algo bellman_ford step 9850 current loss 0.593689, current_train_items 315232.
I0302 19:02:52.641088 22447428132992 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:52.641193 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:02:52.658473 22447428132992 run.py:483] Algo bellman_ford step 9851 current loss 0.760263, current_train_items 315264.
I0302 19:02:52.684023 22447428132992 run.py:483] Algo bellman_ford step 9852 current loss 0.969552, current_train_items 315296.
I0302 19:02:52.715156 22447428132992 run.py:483] Algo bellman_ford step 9853 current loss 0.975528, current_train_items 315328.
I0302 19:02:52.749205 22447428132992 run.py:483] Algo bellman_ford step 9854 current loss 1.394234, current_train_items 315360.
I0302 19:02:52.768943 22447428132992 run.py:483] Algo bellman_ford step 9855 current loss 0.542196, current_train_items 315392.
I0302 19:02:52.784832 22447428132992 run.py:483] Algo bellman_ford step 9856 current loss 0.784427, current_train_items 315424.
I0302 19:02:52.808910 22447428132992 run.py:483] Algo bellman_ford step 9857 current loss 0.887542, current_train_items 315456.
I0302 19:02:52.839593 22447428132992 run.py:483] Algo bellman_ford step 9858 current loss 0.953491, current_train_items 315488.
I0302 19:02:52.872858 22447428132992 run.py:483] Algo bellman_ford step 9859 current loss 1.156834, current_train_items 315520.
I0302 19:02:52.892746 22447428132992 run.py:483] Algo bellman_ford step 9860 current loss 0.721576, current_train_items 315552.
I0302 19:02:52.908892 22447428132992 run.py:483] Algo bellman_ford step 9861 current loss 0.723694, current_train_items 315584.
I0302 19:02:52.932689 22447428132992 run.py:483] Algo bellman_ford step 9862 current loss 0.972220, current_train_items 315616.
I0302 19:02:52.964270 22447428132992 run.py:483] Algo bellman_ford step 9863 current loss 0.995037, current_train_items 315648.
I0302 19:02:52.997292 22447428132992 run.py:483] Algo bellman_ford step 9864 current loss 1.203603, current_train_items 315680.
I0302 19:02:53.017162 22447428132992 run.py:483] Algo bellman_ford step 9865 current loss 0.674678, current_train_items 315712.
I0302 19:02:53.033069 22447428132992 run.py:483] Algo bellman_ford step 9866 current loss 0.641429, current_train_items 315744.
I0302 19:02:53.056610 22447428132992 run.py:483] Algo bellman_ford step 9867 current loss 0.956141, current_train_items 315776.
I0302 19:02:53.088835 22447428132992 run.py:483] Algo bellman_ford step 9868 current loss 1.067695, current_train_items 315808.
I0302 19:02:53.122200 22447428132992 run.py:483] Algo bellman_ford step 9869 current loss 2.348281, current_train_items 315840.
I0302 19:02:53.142015 22447428132992 run.py:483] Algo bellman_ford step 9870 current loss 0.622122, current_train_items 315872.
I0302 19:02:53.158680 22447428132992 run.py:483] Algo bellman_ford step 9871 current loss 0.830639, current_train_items 315904.
I0302 19:02:53.182603 22447428132992 run.py:483] Algo bellman_ford step 9872 current loss 0.986380, current_train_items 315936.
I0302 19:02:53.214267 22447428132992 run.py:483] Algo bellman_ford step 9873 current loss 0.963098, current_train_items 315968.
I0302 19:02:53.246587 22447428132992 run.py:483] Algo bellman_ford step 9874 current loss 1.231552, current_train_items 316000.
I0302 19:02:53.266081 22447428132992 run.py:483] Algo bellman_ford step 9875 current loss 0.570862, current_train_items 316032.
I0302 19:02:53.282145 22447428132992 run.py:483] Algo bellman_ford step 9876 current loss 0.694692, current_train_items 316064.
I0302 19:02:53.304119 22447428132992 run.py:483] Algo bellman_ford step 9877 current loss 0.838408, current_train_items 316096.
I0302 19:02:53.335281 22447428132992 run.py:483] Algo bellman_ford step 9878 current loss 0.942921, current_train_items 316128.
I0302 19:02:53.367927 22447428132992 run.py:483] Algo bellman_ford step 9879 current loss 1.110810, current_train_items 316160.
I0302 19:02:53.387664 22447428132992 run.py:483] Algo bellman_ford step 9880 current loss 0.632037, current_train_items 316192.
I0302 19:02:53.403900 22447428132992 run.py:483] Algo bellman_ford step 9881 current loss 0.712738, current_train_items 316224.
I0302 19:02:53.427782 22447428132992 run.py:483] Algo bellman_ford step 9882 current loss 0.799690, current_train_items 316256.
I0302 19:02:53.459162 22447428132992 run.py:483] Algo bellman_ford step 9883 current loss 0.967806, current_train_items 316288.
I0302 19:02:53.492542 22447428132992 run.py:483] Algo bellman_ford step 9884 current loss 1.088384, current_train_items 316320.
I0302 19:02:53.512415 22447428132992 run.py:483] Algo bellman_ford step 9885 current loss 0.523292, current_train_items 316352.
I0302 19:02:53.528468 22447428132992 run.py:483] Algo bellman_ford step 9886 current loss 0.720786, current_train_items 316384.
I0302 19:02:53.551999 22447428132992 run.py:483] Algo bellman_ford step 9887 current loss 0.875822, current_train_items 316416.
I0302 19:02:53.582818 22447428132992 run.py:483] Algo bellman_ford step 9888 current loss 1.013977, current_train_items 316448.
I0302 19:02:53.614930 22447428132992 run.py:483] Algo bellman_ford step 9889 current loss 1.167048, current_train_items 316480.
I0302 19:02:53.634576 22447428132992 run.py:483] Algo bellman_ford step 9890 current loss 0.538426, current_train_items 316512.
I0302 19:02:53.650676 22447428132992 run.py:483] Algo bellman_ford step 9891 current loss 0.694906, current_train_items 316544.
I0302 19:02:53.673331 22447428132992 run.py:483] Algo bellman_ford step 9892 current loss 0.930918, current_train_items 316576.
I0302 19:02:53.705244 22447428132992 run.py:483] Algo bellman_ford step 9893 current loss 1.046404, current_train_items 316608.
I0302 19:02:53.739611 22447428132992 run.py:483] Algo bellman_ford step 9894 current loss 1.746930, current_train_items 316640.
I0302 19:02:53.759250 22447428132992 run.py:483] Algo bellman_ford step 9895 current loss 0.619623, current_train_items 316672.
I0302 19:02:53.775238 22447428132992 run.py:483] Algo bellman_ford step 9896 current loss 0.655500, current_train_items 316704.
I0302 19:02:53.799258 22447428132992 run.py:483] Algo bellman_ford step 9897 current loss 0.894298, current_train_items 316736.
I0302 19:02:53.830501 22447428132992 run.py:483] Algo bellman_ford step 9898 current loss 1.003020, current_train_items 316768.
I0302 19:02:53.865479 22447428132992 run.py:483] Algo bellman_ford step 9899 current loss 1.155190, current_train_items 316800.
I0302 19:02:53.885127 22447428132992 run.py:483] Algo bellman_ford step 9900 current loss 0.934508, current_train_items 316832.
I0302 19:02:53.892643 22447428132992 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:53.892748 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:53.909367 22447428132992 run.py:483] Algo bellman_ford step 9901 current loss 0.812448, current_train_items 316864.
I0302 19:02:53.933287 22447428132992 run.py:483] Algo bellman_ford step 9902 current loss 0.820354, current_train_items 316896.
I0302 19:02:53.964373 22447428132992 run.py:483] Algo bellman_ford step 9903 current loss 0.935192, current_train_items 316928.
I0302 19:02:53.998696 22447428132992 run.py:483] Algo bellman_ford step 9904 current loss 0.975861, current_train_items 316960.
I0302 19:02:54.018559 22447428132992 run.py:483] Algo bellman_ford step 9905 current loss 0.668762, current_train_items 316992.
I0302 19:02:54.034747 22447428132992 run.py:483] Algo bellman_ford step 9906 current loss 0.738922, current_train_items 317024.
I0302 19:02:54.058771 22447428132992 run.py:483] Algo bellman_ford step 9907 current loss 1.005545, current_train_items 317056.
I0302 19:02:54.090007 22447428132992 run.py:483] Algo bellman_ford step 9908 current loss 0.907581, current_train_items 317088.
I0302 19:02:54.121536 22447428132992 run.py:483] Algo bellman_ford step 9909 current loss 1.139271, current_train_items 317120.
I0302 19:02:54.140968 22447428132992 run.py:483] Algo bellman_ford step 9910 current loss 0.558458, current_train_items 317152.
I0302 19:02:54.157395 22447428132992 run.py:483] Algo bellman_ford step 9911 current loss 0.733713, current_train_items 317184.
I0302 19:02:54.181042 22447428132992 run.py:483] Algo bellman_ford step 9912 current loss 0.954610, current_train_items 317216.
I0302 19:02:54.212828 22447428132992 run.py:483] Algo bellman_ford step 9913 current loss 0.963843, current_train_items 317248.
I0302 19:02:54.245725 22447428132992 run.py:483] Algo bellman_ford step 9914 current loss 1.116473, current_train_items 317280.
I0302 19:02:54.265284 22447428132992 run.py:483] Algo bellman_ford step 9915 current loss 0.590345, current_train_items 317312.
I0302 19:02:54.281614 22447428132992 run.py:483] Algo bellman_ford step 9916 current loss 0.769593, current_train_items 317344.
I0302 19:02:54.305542 22447428132992 run.py:483] Algo bellman_ford step 9917 current loss 0.922160, current_train_items 317376.
I0302 19:02:54.336951 22447428132992 run.py:483] Algo bellman_ford step 9918 current loss 0.998211, current_train_items 317408.
I0302 19:02:54.369720 22447428132992 run.py:483] Algo bellman_ford step 9919 current loss 1.018047, current_train_items 317440.
I0302 19:02:54.388917 22447428132992 run.py:483] Algo bellman_ford step 9920 current loss 0.534020, current_train_items 317472.
I0302 19:02:54.405282 22447428132992 run.py:483] Algo bellman_ford step 9921 current loss 0.787155, current_train_items 317504.
I0302 19:02:54.428886 22447428132992 run.py:483] Algo bellman_ford step 9922 current loss 0.892188, current_train_items 317536.
I0302 19:02:54.458408 22447428132992 run.py:483] Algo bellman_ford step 9923 current loss 0.979398, current_train_items 317568.
I0302 19:02:54.493980 22447428132992 run.py:483] Algo bellman_ford step 9924 current loss 1.180052, current_train_items 317600.
I0302 19:02:54.513160 22447428132992 run.py:483] Algo bellman_ford step 9925 current loss 0.495802, current_train_items 317632.
I0302 19:02:54.529118 22447428132992 run.py:483] Algo bellman_ford step 9926 current loss 0.760410, current_train_items 317664.
I0302 19:02:54.553280 22447428132992 run.py:483] Algo bellman_ford step 9927 current loss 0.857085, current_train_items 317696.
I0302 19:02:54.585466 22447428132992 run.py:483] Algo bellman_ford step 9928 current loss 0.990898, current_train_items 317728.
I0302 19:02:54.616362 22447428132992 run.py:483] Algo bellman_ford step 9929 current loss 1.069126, current_train_items 317760.
I0302 19:02:54.635662 22447428132992 run.py:483] Algo bellman_ford step 9930 current loss 0.589634, current_train_items 317792.
I0302 19:02:54.651127 22447428132992 run.py:483] Algo bellman_ford step 9931 current loss 0.721015, current_train_items 317824.
I0302 19:02:54.674786 22447428132992 run.py:483] Algo bellman_ford step 9932 current loss 0.957497, current_train_items 317856.
I0302 19:02:54.705450 22447428132992 run.py:483] Algo bellman_ford step 9933 current loss 0.984016, current_train_items 317888.
I0302 19:02:54.737237 22447428132992 run.py:483] Algo bellman_ford step 9934 current loss 1.020718, current_train_items 317920.
I0302 19:02:54.756415 22447428132992 run.py:483] Algo bellman_ford step 9935 current loss 0.525813, current_train_items 317952.
I0302 19:02:54.772132 22447428132992 run.py:483] Algo bellman_ford step 9936 current loss 0.677828, current_train_items 317984.
I0302 19:02:54.795375 22447428132992 run.py:483] Algo bellman_ford step 9937 current loss 0.875362, current_train_items 318016.
I0302 19:02:54.826097 22447428132992 run.py:483] Algo bellman_ford step 9938 current loss 0.910274, current_train_items 318048.
I0302 19:02:54.859742 22447428132992 run.py:483] Algo bellman_ford step 9939 current loss 1.153867, current_train_items 318080.
I0302 19:02:54.879187 22447428132992 run.py:483] Algo bellman_ford step 9940 current loss 0.539473, current_train_items 318112.
I0302 19:02:54.895010 22447428132992 run.py:483] Algo bellman_ford step 9941 current loss 0.619223, current_train_items 318144.
I0302 19:02:54.918063 22447428132992 run.py:483] Algo bellman_ford step 9942 current loss 0.825874, current_train_items 318176.
I0302 19:02:54.949528 22447428132992 run.py:483] Algo bellman_ford step 9943 current loss 0.953735, current_train_items 318208.
I0302 19:02:54.981004 22447428132992 run.py:483] Algo bellman_ford step 9944 current loss 0.982248, current_train_items 318240.
I0302 19:02:55.000289 22447428132992 run.py:483] Algo bellman_ford step 9945 current loss 0.536436, current_train_items 318272.
I0302 19:02:55.015980 22447428132992 run.py:483] Algo bellman_ford step 9946 current loss 0.731962, current_train_items 318304.
I0302 19:02:55.039750 22447428132992 run.py:483] Algo bellman_ford step 9947 current loss 0.846101, current_train_items 318336.
I0302 19:02:55.070991 22447428132992 run.py:483] Algo bellman_ford step 9948 current loss 1.014478, current_train_items 318368.
I0302 19:02:55.105033 22447428132992 run.py:483] Algo bellman_ford step 9949 current loss 1.061249, current_train_items 318400.
I0302 19:02:55.124115 22447428132992 run.py:483] Algo bellman_ford step 9950 current loss 0.687032, current_train_items 318432.
I0302 19:02:55.132250 22447428132992 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.8564453125, 'score': 0.8564453125, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:55.132355 22447428132992 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.856, val scores are: bellman_ford: 0.856
I0302 19:02:55.149206 22447428132992 run.py:483] Algo bellman_ford step 9951 current loss 0.760982, current_train_items 318464.
I0302 19:02:55.173435 22447428132992 run.py:483] Algo bellman_ford step 9952 current loss 0.785353, current_train_items 318496.
I0302 19:02:55.206060 22447428132992 run.py:483] Algo bellman_ford step 9953 current loss 1.058292, current_train_items 318528.
I0302 19:02:55.241217 22447428132992 run.py:483] Algo bellman_ford step 9954 current loss 1.490930, current_train_items 318560.
I0302 19:02:55.261349 22447428132992 run.py:483] Algo bellman_ford step 9955 current loss 0.549015, current_train_items 318592.
I0302 19:02:55.277678 22447428132992 run.py:483] Algo bellman_ford step 9956 current loss 0.791503, current_train_items 318624.
I0302 19:02:55.301742 22447428132992 run.py:483] Algo bellman_ford step 9957 current loss 0.849085, current_train_items 318656.
I0302 19:02:55.333535 22447428132992 run.py:483] Algo bellman_ford step 9958 current loss 0.940810, current_train_items 318688.
I0302 19:02:55.367931 22447428132992 run.py:483] Algo bellman_ford step 9959 current loss 1.142536, current_train_items 318720.
I0302 19:02:55.388138 22447428132992 run.py:483] Algo bellman_ford step 9960 current loss 0.577730, current_train_items 318752.
I0302 19:02:55.403873 22447428132992 run.py:483] Algo bellman_ford step 9961 current loss 0.768272, current_train_items 318784.
I0302 19:02:55.428156 22447428132992 run.py:483] Algo bellman_ford step 9962 current loss 0.974968, current_train_items 318816.
I0302 19:02:55.460094 22447428132992 run.py:483] Algo bellman_ford step 9963 current loss 0.992364, current_train_items 318848.
I0302 19:02:55.493295 22447428132992 run.py:483] Algo bellman_ford step 9964 current loss 1.062255, current_train_items 318880.
I0302 19:02:55.512852 22447428132992 run.py:483] Algo bellman_ford step 9965 current loss 0.656055, current_train_items 318912.
I0302 19:02:55.529331 22447428132992 run.py:483] Algo bellman_ford step 9966 current loss 0.753302, current_train_items 318944.
I0302 19:02:55.553349 22447428132992 run.py:483] Algo bellman_ford step 9967 current loss 0.906216, current_train_items 318976.
I0302 19:02:55.584940 22447428132992 run.py:483] Algo bellman_ford step 9968 current loss 0.979611, current_train_items 319008.
I0302 19:02:55.618426 22447428132992 run.py:483] Algo bellman_ford step 9969 current loss 0.950886, current_train_items 319040.
I0302 19:02:55.638503 22447428132992 run.py:483] Algo bellman_ford step 9970 current loss 0.741398, current_train_items 319072.
I0302 19:02:55.654520 22447428132992 run.py:483] Algo bellman_ford step 9971 current loss 0.777386, current_train_items 319104.
I0302 19:02:55.678026 22447428132992 run.py:483] Algo bellman_ford step 9972 current loss 0.884887, current_train_items 319136.
I0302 19:02:55.708633 22447428132992 run.py:483] Algo bellman_ford step 9973 current loss 1.055968, current_train_items 319168.
I0302 19:02:55.742087 22447428132992 run.py:483] Algo bellman_ford step 9974 current loss 1.114978, current_train_items 319200.
I0302 19:02:55.761846 22447428132992 run.py:483] Algo bellman_ford step 9975 current loss 0.541513, current_train_items 319232.
I0302 19:02:55.778350 22447428132992 run.py:483] Algo bellman_ford step 9976 current loss 0.777102, current_train_items 319264.
I0302 19:02:55.801905 22447428132992 run.py:483] Algo bellman_ford step 9977 current loss 0.901708, current_train_items 319296.
I0302 19:02:55.833338 22447428132992 run.py:483] Algo bellman_ford step 9978 current loss 0.886774, current_train_items 319328.
I0302 19:02:55.865437 22447428132992 run.py:483] Algo bellman_ford step 9979 current loss 1.020905, current_train_items 319360.
I0302 19:02:55.884860 22447428132992 run.py:483] Algo bellman_ford step 9980 current loss 0.773174, current_train_items 319392.
I0302 19:02:55.900897 22447428132992 run.py:483] Algo bellman_ford step 9981 current loss 0.750076, current_train_items 319424.
I0302 19:02:55.925518 22447428132992 run.py:483] Algo bellman_ford step 9982 current loss 0.946777, current_train_items 319456.
I0302 19:02:55.958329 22447428132992 run.py:483] Algo bellman_ford step 9983 current loss 1.088695, current_train_items 319488.
I0302 19:02:55.992457 22447428132992 run.py:483] Algo bellman_ford step 9984 current loss 1.152753, current_train_items 319520.
I0302 19:02:56.012212 22447428132992 run.py:483] Algo bellman_ford step 9985 current loss 0.620026, current_train_items 319552.
I0302 19:02:56.028045 22447428132992 run.py:483] Algo bellman_ford step 9986 current loss 0.561333, current_train_items 319584.
I0302 19:02:56.051429 22447428132992 run.py:483] Algo bellman_ford step 9987 current loss 0.899404, current_train_items 319616.
I0302 19:02:56.083138 22447428132992 run.py:483] Algo bellman_ford step 9988 current loss 0.922908, current_train_items 319648.
I0302 19:02:56.116345 22447428132992 run.py:483] Algo bellman_ford step 9989 current loss 1.081028, current_train_items 319680.
I0302 19:02:56.136374 22447428132992 run.py:483] Algo bellman_ford step 9990 current loss 0.735956, current_train_items 319712.
I0302 19:02:56.152690 22447428132992 run.py:483] Algo bellman_ford step 9991 current loss 0.750972, current_train_items 319744.
I0302 19:02:56.176165 22447428132992 run.py:483] Algo bellman_ford step 9992 current loss 1.036770, current_train_items 319776.
I0302 19:02:56.206717 22447428132992 run.py:483] Algo bellman_ford step 9993 current loss 0.943269, current_train_items 319808.
I0302 19:02:56.239272 22447428132992 run.py:483] Algo bellman_ford step 9994 current loss 0.998225, current_train_items 319840.
I0302 19:02:56.258835 22447428132992 run.py:483] Algo bellman_ford step 9995 current loss 0.652386, current_train_items 319872.
I0302 19:02:56.275038 22447428132992 run.py:483] Algo bellman_ford step 9996 current loss 0.825183, current_train_items 319904.
I0302 19:02:56.298745 22447428132992 run.py:483] Algo bellman_ford step 9997 current loss 0.786089, current_train_items 319936.
I0302 19:02:56.330268 22447428132992 run.py:483] Algo bellman_ford step 9998 current loss 0.995110, current_train_items 319968.
I0302 19:02:56.361680 22447428132992 run.py:483] Algo bellman_ford step 9999 current loss 1.198137, current_train_items 320000.
I0302 19:02:56.367637 22447428132992 run.py:527] Restoring best model from checkpoint...
I0302 19:02:58.620302 22447428132992 run.py:542] (test) algo bellman_ford : {'pi': 0.22265625, 'score': 0.22265625, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:02:58.620550 22447428132992 run.py:544] Done!

Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:23.136944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:23.137245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:23.138835: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:24.389659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:27.652513 22760421793920 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:27.654289 22760421793920 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:27.978149 22760421793920 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:27.978688 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:27.979015 22760421793920 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.196073 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.196319 22760421793920 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.450972 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.451241 22760421793920 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.800632 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.800874 22760421793920 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.229337 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:29.229583 22760421793920 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.770841 22760421793920 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:29.771110 22760421793920 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:29.809611 22760421793920 run.py:166] Dataset not found in ./datasets_1/21/CLRS30_v1.0.0. Downloading...
I0302 18:56:44.825960 22760421793920 dataset_info.py:482] Load dataset info from ./datasets_1/21/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:44.829150 22760421793920 dataset_info.py:482] Load dataset info from ./datasets_1/21/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:44.830012 22760421793920 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/21/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:56:44.830101 22760421793920 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/21/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:58.815772 22760421793920 run.py:483] Algo bellman_ford step 0 current loss 4.253816, current_train_items 32.
I0302 18:57:01.635451 22760421793920 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.591796875, 'score': 0.591796875, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:01.635720 22760421793920 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.592, val scores are: bellman_ford: 0.592
I0302 18:57:10.269861 22760421793920 run.py:483] Algo bellman_ford step 1 current loss 4.586732, current_train_items 64.
I0302 18:57:19.266675 22760421793920 run.py:483] Algo bellman_ford step 2 current loss 3.588478, current_train_items 96.
I0302 18:57:28.310132 22760421793920 run.py:483] Algo bellman_ford step 3 current loss 3.199789, current_train_items 128.
I0302 18:57:36.032150 22760421793920 run.py:483] Algo bellman_ford step 4 current loss 3.655756, current_train_items 160.
I0302 18:57:36.049700 22760421793920 run.py:483] Algo bellman_ford step 5 current loss 1.040589, current_train_items 192.
I0302 18:57:36.066754 22760421793920 run.py:483] Algo bellman_ford step 6 current loss 1.668322, current_train_items 224.
I0302 18:57:36.088921 22760421793920 run.py:483] Algo bellman_ford step 7 current loss 2.080736, current_train_items 256.
I0302 18:57:36.117258 22760421793920 run.py:483] Algo bellman_ford step 8 current loss 2.452293, current_train_items 288.
I0302 18:57:36.147957 22760421793920 run.py:483] Algo bellman_ford step 9 current loss 2.608021, current_train_items 320.
I0302 18:57:36.165088 22760421793920 run.py:483] Algo bellman_ford step 10 current loss 0.929639, current_train_items 352.
I0302 18:57:36.181298 22760421793920 run.py:483] Algo bellman_ford step 11 current loss 1.509972, current_train_items 384.
I0302 18:57:36.202669 22760421793920 run.py:483] Algo bellman_ford step 12 current loss 1.803143, current_train_items 416.
I0302 18:57:36.232597 22760421793920 run.py:483] Algo bellman_ford step 13 current loss 2.212682, current_train_items 448.
I0302 18:57:36.259594 22760421793920 run.py:483] Algo bellman_ford step 14 current loss 1.915611, current_train_items 480.
I0302 18:57:36.276887 22760421793920 run.py:483] Algo bellman_ford step 15 current loss 0.854463, current_train_items 512.
I0302 18:57:36.292585 22760421793920 run.py:483] Algo bellman_ford step 16 current loss 1.245101, current_train_items 544.
I0302 18:57:36.316447 22760421793920 run.py:483] Algo bellman_ford step 17 current loss 1.967340, current_train_items 576.
I0302 18:57:36.344259 22760421793920 run.py:483] Algo bellman_ford step 18 current loss 1.803854, current_train_items 608.
I0302 18:57:36.375463 22760421793920 run.py:483] Algo bellman_ford step 19 current loss 2.180656, current_train_items 640.
I0302 18:57:36.392193 22760421793920 run.py:483] Algo bellman_ford step 20 current loss 0.650769, current_train_items 672.
I0302 18:57:36.407582 22760421793920 run.py:483] Algo bellman_ford step 21 current loss 0.897645, current_train_items 704.
I0302 18:57:36.430295 22760421793920 run.py:483] Algo bellman_ford step 22 current loss 1.582857, current_train_items 736.
I0302 18:57:36.458317 22760421793920 run.py:483] Algo bellman_ford step 23 current loss 1.704930, current_train_items 768.
I0302 18:57:36.488403 22760421793920 run.py:483] Algo bellman_ford step 24 current loss 1.942283, current_train_items 800.
I0302 18:57:36.505110 22760421793920 run.py:483] Algo bellman_ford step 25 current loss 0.620912, current_train_items 832.
I0302 18:57:36.521014 22760421793920 run.py:483] Algo bellman_ford step 26 current loss 0.997887, current_train_items 864.
I0302 18:57:36.543998 22760421793920 run.py:483] Algo bellman_ford step 27 current loss 1.573863, current_train_items 896.
I0302 18:57:36.572984 22760421793920 run.py:483] Algo bellman_ford step 28 current loss 1.581334, current_train_items 928.
I0302 18:57:36.604611 22760421793920 run.py:483] Algo bellman_ford step 29 current loss 1.864148, current_train_items 960.
I0302 18:57:36.621594 22760421793920 run.py:483] Algo bellman_ford step 30 current loss 0.464334, current_train_items 992.
I0302 18:57:36.636620 22760421793920 run.py:483] Algo bellman_ford step 31 current loss 0.808055, current_train_items 1024.
I0302 18:57:36.659047 22760421793920 run.py:483] Algo bellman_ford step 32 current loss 1.386862, current_train_items 1056.
I0302 18:57:36.688259 22760421793920 run.py:483] Algo bellman_ford step 33 current loss 1.568953, current_train_items 1088.
I0302 18:57:36.718379 22760421793920 run.py:483] Algo bellman_ford step 34 current loss 1.729047, current_train_items 1120.
I0302 18:57:36.735000 22760421793920 run.py:483] Algo bellman_ford step 35 current loss 0.523462, current_train_items 1152.
I0302 18:57:36.750274 22760421793920 run.py:483] Algo bellman_ford step 36 current loss 0.730152, current_train_items 1184.
I0302 18:57:36.772835 22760421793920 run.py:483] Algo bellman_ford step 37 current loss 1.146246, current_train_items 1216.
I0302 18:57:36.801516 22760421793920 run.py:483] Algo bellman_ford step 38 current loss 1.254302, current_train_items 1248.
W0302 18:57:36.823661 22760421793920 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:57:42.428109 22760421793920 run.py:483] Algo bellman_ford step 39 current loss 1.744439, current_train_items 1280.
I0302 18:57:42.446769 22760421793920 run.py:483] Algo bellman_ford step 40 current loss 0.520281, current_train_items 1312.
I0302 18:57:42.462993 22760421793920 run.py:483] Algo bellman_ford step 41 current loss 0.709964, current_train_items 1344.
I0302 18:57:42.485355 22760421793920 run.py:483] Algo bellman_ford step 42 current loss 1.061245, current_train_items 1376.
I0302 18:57:42.515026 22760421793920 run.py:483] Algo bellman_ford step 43 current loss 1.370602, current_train_items 1408.
I0302 18:57:42.545971 22760421793920 run.py:483] Algo bellman_ford step 44 current loss 1.523109, current_train_items 1440.
I0302 18:57:42.564428 22760421793920 run.py:483] Algo bellman_ford step 45 current loss 0.341434, current_train_items 1472.
I0302 18:57:42.580638 22760421793920 run.py:483] Algo bellman_ford step 46 current loss 0.737832, current_train_items 1504.
I0302 18:57:42.602214 22760421793920 run.py:483] Algo bellman_ford step 47 current loss 0.946146, current_train_items 1536.
I0302 18:57:42.628408 22760421793920 run.py:483] Algo bellman_ford step 48 current loss 0.868231, current_train_items 1568.
I0302 18:57:42.656646 22760421793920 run.py:483] Algo bellman_ford step 49 current loss 1.176627, current_train_items 1600.
I0302 18:57:42.674305 22760421793920 run.py:483] Algo bellman_ford step 50 current loss 0.299409, current_train_items 1632.
I0302 18:57:42.683309 22760421793920 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8125, 'score': 0.8125, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:57:42.683425 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.592, current avg val score is 0.812, val scores are: bellman_ford: 0.812
I0302 18:57:42.711670 22760421793920 run.py:483] Algo bellman_ford step 51 current loss 0.599273, current_train_items 1664.
I0302 18:57:42.734442 22760421793920 run.py:483] Algo bellman_ford step 52 current loss 1.003134, current_train_items 1696.
I0302 18:57:42.762334 22760421793920 run.py:483] Algo bellman_ford step 53 current loss 1.012200, current_train_items 1728.
I0302 18:57:42.793067 22760421793920 run.py:483] Algo bellman_ford step 54 current loss 1.289833, current_train_items 1760.
I0302 18:57:42.810956 22760421793920 run.py:483] Algo bellman_ford step 55 current loss 0.322957, current_train_items 1792.
I0302 18:57:42.826642 22760421793920 run.py:483] Algo bellman_ford step 56 current loss 0.480808, current_train_items 1824.
I0302 18:57:42.848644 22760421793920 run.py:483] Algo bellman_ford step 57 current loss 0.874805, current_train_items 1856.
I0302 18:57:42.875425 22760421793920 run.py:483] Algo bellman_ford step 58 current loss 0.760880, current_train_items 1888.
I0302 18:57:42.906641 22760421793920 run.py:483] Algo bellman_ford step 59 current loss 1.165930, current_train_items 1920.
I0302 18:57:42.924026 22760421793920 run.py:483] Algo bellman_ford step 60 current loss 0.218433, current_train_items 1952.
W0302 18:57:42.933454 22760421793920 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:57:48.303451 22760421793920 run.py:483] Algo bellman_ford step 61 current loss 0.457136, current_train_items 1984.
I0302 18:57:48.327604 22760421793920 run.py:483] Algo bellman_ford step 62 current loss 0.928123, current_train_items 2016.
I0302 18:57:48.356511 22760421793920 run.py:483] Algo bellman_ford step 63 current loss 1.090853, current_train_items 2048.
I0302 18:57:48.389399 22760421793920 run.py:483] Algo bellman_ford step 64 current loss 1.364743, current_train_items 2080.
I0302 18:57:48.407824 22760421793920 run.py:483] Algo bellman_ford step 65 current loss 0.220057, current_train_items 2112.
I0302 18:57:48.423763 22760421793920 run.py:483] Algo bellman_ford step 66 current loss 0.384600, current_train_items 2144.
I0302 18:57:48.447320 22760421793920 run.py:483] Algo bellman_ford step 67 current loss 0.964847, current_train_items 2176.
I0302 18:57:48.474696 22760421793920 run.py:483] Algo bellman_ford step 68 current loss 0.815183, current_train_items 2208.
I0302 18:57:48.505951 22760421793920 run.py:483] Algo bellman_ford step 69 current loss 1.027354, current_train_items 2240.
I0302 18:57:48.523510 22760421793920 run.py:483] Algo bellman_ford step 70 current loss 0.198716, current_train_items 2272.
I0302 18:57:48.539449 22760421793920 run.py:483] Algo bellman_ford step 71 current loss 0.425068, current_train_items 2304.
I0302 18:57:48.562355 22760421793920 run.py:483] Algo bellman_ford step 72 current loss 0.813878, current_train_items 2336.
I0302 18:57:48.591357 22760421793920 run.py:483] Algo bellman_ford step 73 current loss 0.790993, current_train_items 2368.
I0302 18:57:48.622671 22760421793920 run.py:483] Algo bellman_ford step 74 current loss 1.068100, current_train_items 2400.
I0302 18:57:48.640407 22760421793920 run.py:483] Algo bellman_ford step 75 current loss 0.148637, current_train_items 2432.
I0302 18:57:48.656716 22760421793920 run.py:483] Algo bellman_ford step 76 current loss 0.618209, current_train_items 2464.
I0302 18:57:48.679066 22760421793920 run.py:483] Algo bellman_ford step 77 current loss 0.932474, current_train_items 2496.
I0302 18:57:48.706752 22760421793920 run.py:483] Algo bellman_ford step 78 current loss 0.890214, current_train_items 2528.
I0302 18:57:48.735152 22760421793920 run.py:483] Algo bellman_ford step 79 current loss 1.017087, current_train_items 2560.
I0302 18:57:48.753065 22760421793920 run.py:483] Algo bellman_ford step 80 current loss 0.134188, current_train_items 2592.
I0302 18:57:48.768797 22760421793920 run.py:483] Algo bellman_ford step 81 current loss 0.421955, current_train_items 2624.
I0302 18:57:48.791644 22760421793920 run.py:483] Algo bellman_ford step 82 current loss 0.880170, current_train_items 2656.
I0302 18:57:48.819699 22760421793920 run.py:483] Algo bellman_ford step 83 current loss 0.929319, current_train_items 2688.
I0302 18:57:48.848708 22760421793920 run.py:483] Algo bellman_ford step 84 current loss 0.922590, current_train_items 2720.
I0302 18:57:48.865968 22760421793920 run.py:483] Algo bellman_ford step 85 current loss 0.149745, current_train_items 2752.
I0302 18:57:48.882020 22760421793920 run.py:483] Algo bellman_ford step 86 current loss 0.423102, current_train_items 2784.
I0302 18:57:48.905781 22760421793920 run.py:483] Algo bellman_ford step 87 current loss 0.849094, current_train_items 2816.
I0302 18:57:48.934483 22760421793920 run.py:483] Algo bellman_ford step 88 current loss 0.763404, current_train_items 2848.
I0302 18:57:48.965137 22760421793920 run.py:483] Algo bellman_ford step 89 current loss 0.925383, current_train_items 2880.
I0302 18:57:48.982630 22760421793920 run.py:483] Algo bellman_ford step 90 current loss 0.181784, current_train_items 2912.
I0302 18:57:48.998565 22760421793920 run.py:483] Algo bellman_ford step 91 current loss 0.464188, current_train_items 2944.
I0302 18:57:49.021008 22760421793920 run.py:483] Algo bellman_ford step 92 current loss 0.697430, current_train_items 2976.
I0302 18:57:49.050613 22760421793920 run.py:483] Algo bellman_ford step 93 current loss 0.803685, current_train_items 3008.
I0302 18:57:49.080801 22760421793920 run.py:483] Algo bellman_ford step 94 current loss 0.817672, current_train_items 3040.
I0302 18:57:49.098693 22760421793920 run.py:483] Algo bellman_ford step 95 current loss 0.198614, current_train_items 3072.
I0302 18:57:49.114399 22760421793920 run.py:483] Algo bellman_ford step 96 current loss 0.415836, current_train_items 3104.
I0302 18:57:49.136879 22760421793920 run.py:483] Algo bellman_ford step 97 current loss 0.719027, current_train_items 3136.
I0302 18:57:49.165547 22760421793920 run.py:483] Algo bellman_ford step 98 current loss 0.824739, current_train_items 3168.
I0302 18:57:49.196017 22760421793920 run.py:483] Algo bellman_ford step 99 current loss 0.936961, current_train_items 3200.
I0302 18:57:49.213482 22760421793920 run.py:483] Algo bellman_ford step 100 current loss 0.119656, current_train_items 3232.
I0302 18:57:49.222477 22760421793920 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8525390625, 'score': 0.8525390625, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:57:49.222591 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.812, current avg val score is 0.853, val scores are: bellman_ford: 0.853
I0302 18:57:49.251121 22760421793920 run.py:483] Algo bellman_ford step 101 current loss 0.421090, current_train_items 3264.
I0302 18:57:49.273872 22760421793920 run.py:483] Algo bellman_ford step 102 current loss 0.453893, current_train_items 3296.
I0302 18:57:49.301971 22760421793920 run.py:483] Algo bellman_ford step 103 current loss 0.661507, current_train_items 3328.
I0302 18:57:49.334820 22760421793920 run.py:483] Algo bellman_ford step 104 current loss 0.846099, current_train_items 3360.
I0302 18:57:49.352751 22760421793920 run.py:483] Algo bellman_ford step 105 current loss 0.104641, current_train_items 3392.
I0302 18:57:49.368678 22760421793920 run.py:483] Algo bellman_ford step 106 current loss 0.346002, current_train_items 3424.
I0302 18:57:49.391312 22760421793920 run.py:483] Algo bellman_ford step 107 current loss 0.531401, current_train_items 3456.
I0302 18:57:49.419240 22760421793920 run.py:483] Algo bellman_ford step 108 current loss 0.657889, current_train_items 3488.
I0302 18:57:49.448815 22760421793920 run.py:483] Algo bellman_ford step 109 current loss 0.650198, current_train_items 3520.
I0302 18:57:49.466637 22760421793920 run.py:483] Algo bellman_ford step 110 current loss 0.128613, current_train_items 3552.
I0302 18:57:49.482264 22760421793920 run.py:483] Algo bellman_ford step 111 current loss 0.315338, current_train_items 3584.
I0302 18:57:49.504428 22760421793920 run.py:483] Algo bellman_ford step 112 current loss 0.434748, current_train_items 3616.
I0302 18:57:49.533257 22760421793920 run.py:483] Algo bellman_ford step 113 current loss 0.684613, current_train_items 3648.
I0302 18:57:49.562902 22760421793920 run.py:483] Algo bellman_ford step 114 current loss 0.718796, current_train_items 3680.
I0302 18:57:49.580150 22760421793920 run.py:483] Algo bellman_ford step 115 current loss 0.113236, current_train_items 3712.
I0302 18:57:49.596085 22760421793920 run.py:483] Algo bellman_ford step 116 current loss 0.319792, current_train_items 3744.
I0302 18:57:49.619437 22760421793920 run.py:483] Algo bellman_ford step 117 current loss 0.557474, current_train_items 3776.
I0302 18:57:49.647275 22760421793920 run.py:483] Algo bellman_ford step 118 current loss 0.572234, current_train_items 3808.
I0302 18:57:49.675727 22760421793920 run.py:483] Algo bellman_ford step 119 current loss 0.603144, current_train_items 3840.
I0302 18:57:49.693241 22760421793920 run.py:483] Algo bellman_ford step 120 current loss 0.092476, current_train_items 3872.
I0302 18:57:49.709405 22760421793920 run.py:483] Algo bellman_ford step 121 current loss 0.304056, current_train_items 3904.
I0302 18:57:49.732759 22760421793920 run.py:483] Algo bellman_ford step 122 current loss 0.488366, current_train_items 3936.
I0302 18:57:49.761790 22760421793920 run.py:483] Algo bellman_ford step 123 current loss 0.589413, current_train_items 3968.
I0302 18:57:49.795886 22760421793920 run.py:483] Algo bellman_ford step 124 current loss 0.868574, current_train_items 4000.
I0302 18:57:49.813576 22760421793920 run.py:483] Algo bellman_ford step 125 current loss 0.209517, current_train_items 4032.
I0302 18:57:49.829608 22760421793920 run.py:483] Algo bellman_ford step 126 current loss 0.334043, current_train_items 4064.
I0302 18:57:49.852829 22760421793920 run.py:483] Algo bellman_ford step 127 current loss 0.498501, current_train_items 4096.
I0302 18:57:49.880980 22760421793920 run.py:483] Algo bellman_ford step 128 current loss 0.580474, current_train_items 4128.
I0302 18:57:49.911659 22760421793920 run.py:483] Algo bellman_ford step 129 current loss 0.685427, current_train_items 4160.
I0302 18:57:49.929381 22760421793920 run.py:483] Algo bellman_ford step 130 current loss 0.129288, current_train_items 4192.
I0302 18:57:49.945027 22760421793920 run.py:483] Algo bellman_ford step 131 current loss 0.240151, current_train_items 4224.
I0302 18:57:49.968294 22760421793920 run.py:483] Algo bellman_ford step 132 current loss 0.703962, current_train_items 4256.
I0302 18:57:49.996773 22760421793920 run.py:483] Algo bellman_ford step 133 current loss 0.665661, current_train_items 4288.
I0302 18:57:50.026515 22760421793920 run.py:483] Algo bellman_ford step 134 current loss 0.650240, current_train_items 4320.
I0302 18:57:50.043553 22760421793920 run.py:483] Algo bellman_ford step 135 current loss 0.070553, current_train_items 4352.
I0302 18:57:50.059182 22760421793920 run.py:483] Algo bellman_ford step 136 current loss 0.326661, current_train_items 4384.
I0302 18:57:50.082001 22760421793920 run.py:483] Algo bellman_ford step 137 current loss 0.517624, current_train_items 4416.
I0302 18:57:50.110139 22760421793920 run.py:483] Algo bellman_ford step 138 current loss 0.593049, current_train_items 4448.
I0302 18:57:50.141656 22760421793920 run.py:483] Algo bellman_ford step 139 current loss 0.729218, current_train_items 4480.
I0302 18:57:50.158703 22760421793920 run.py:483] Algo bellman_ford step 140 current loss 0.117229, current_train_items 4512.
I0302 18:57:50.174655 22760421793920 run.py:483] Algo bellman_ford step 141 current loss 0.450826, current_train_items 4544.
I0302 18:57:50.197194 22760421793920 run.py:483] Algo bellman_ford step 142 current loss 0.516770, current_train_items 4576.
I0302 18:57:50.225892 22760421793920 run.py:483] Algo bellman_ford step 143 current loss 0.682324, current_train_items 4608.
I0302 18:57:50.256080 22760421793920 run.py:483] Algo bellman_ford step 144 current loss 0.577387, current_train_items 4640.
I0302 18:57:50.273248 22760421793920 run.py:483] Algo bellman_ford step 145 current loss 0.105505, current_train_items 4672.
I0302 18:57:50.288919 22760421793920 run.py:483] Algo bellman_ford step 146 current loss 0.292569, current_train_items 4704.
I0302 18:57:50.310834 22760421793920 run.py:483] Algo bellman_ford step 147 current loss 0.420024, current_train_items 4736.
I0302 18:57:50.339078 22760421793920 run.py:483] Algo bellman_ford step 148 current loss 0.656944, current_train_items 4768.
I0302 18:57:50.367774 22760421793920 run.py:483] Algo bellman_ford step 149 current loss 0.625480, current_train_items 4800.
I0302 18:57:50.385379 22760421793920 run.py:483] Algo bellman_ford step 150 current loss 0.065247, current_train_items 4832.
I0302 18:57:50.392854 22760421793920 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:57:50.392981 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.853, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 18:57:50.420756 22760421793920 run.py:483] Algo bellman_ford step 151 current loss 0.152550, current_train_items 4864.
I0302 18:57:50.443773 22760421793920 run.py:483] Algo bellman_ford step 152 current loss 0.437262, current_train_items 4896.
I0302 18:57:50.472055 22760421793920 run.py:483] Algo bellman_ford step 153 current loss 0.627970, current_train_items 4928.
I0302 18:57:50.506238 22760421793920 run.py:483] Algo bellman_ford step 154 current loss 0.839913, current_train_items 4960.
I0302 18:57:50.524060 22760421793920 run.py:483] Algo bellman_ford step 155 current loss 0.055003, current_train_items 4992.
I0302 18:57:50.539957 22760421793920 run.py:483] Algo bellman_ford step 156 current loss 0.294850, current_train_items 5024.
I0302 18:57:50.561963 22760421793920 run.py:483] Algo bellman_ford step 157 current loss 0.497477, current_train_items 5056.
I0302 18:57:50.589467 22760421793920 run.py:483] Algo bellman_ford step 158 current loss 0.482390, current_train_items 5088.
I0302 18:57:50.619521 22760421793920 run.py:483] Algo bellman_ford step 159 current loss 0.479303, current_train_items 5120.
I0302 18:57:50.637336 22760421793920 run.py:483] Algo bellman_ford step 160 current loss 0.126043, current_train_items 5152.
I0302 18:57:50.653150 22760421793920 run.py:483] Algo bellman_ford step 161 current loss 0.206590, current_train_items 5184.
I0302 18:57:50.674679 22760421793920 run.py:483] Algo bellman_ford step 162 current loss 0.387662, current_train_items 5216.
I0302 18:57:50.703058 22760421793920 run.py:483] Algo bellman_ford step 163 current loss 0.476329, current_train_items 5248.
I0302 18:57:50.733243 22760421793920 run.py:483] Algo bellman_ford step 164 current loss 0.569428, current_train_items 5280.
I0302 18:57:50.750767 22760421793920 run.py:483] Algo bellman_ford step 165 current loss 0.096226, current_train_items 5312.
I0302 18:57:50.766751 22760421793920 run.py:483] Algo bellman_ford step 166 current loss 0.405502, current_train_items 5344.
I0302 18:57:50.788662 22760421793920 run.py:483] Algo bellman_ford step 167 current loss 0.417807, current_train_items 5376.
I0302 18:57:50.817177 22760421793920 run.py:483] Algo bellman_ford step 168 current loss 0.498619, current_train_items 5408.
I0302 18:57:50.847307 22760421793920 run.py:483] Algo bellman_ford step 169 current loss 0.582516, current_train_items 5440.
I0302 18:57:50.864801 22760421793920 run.py:483] Algo bellman_ford step 170 current loss 0.175789, current_train_items 5472.
I0302 18:57:50.880512 22760421793920 run.py:483] Algo bellman_ford step 171 current loss 0.272020, current_train_items 5504.
I0302 18:57:50.901990 22760421793920 run.py:483] Algo bellman_ford step 172 current loss 0.492571, current_train_items 5536.
I0302 18:57:50.931233 22760421793920 run.py:483] Algo bellman_ford step 173 current loss 0.601986, current_train_items 5568.
I0302 18:57:50.964522 22760421793920 run.py:483] Algo bellman_ford step 174 current loss 0.678646, current_train_items 5600.
I0302 18:57:50.981662 22760421793920 run.py:483] Algo bellman_ford step 175 current loss 0.137094, current_train_items 5632.
I0302 18:57:50.997295 22760421793920 run.py:483] Algo bellman_ford step 176 current loss 0.294753, current_train_items 5664.
I0302 18:57:51.020297 22760421793920 run.py:483] Algo bellman_ford step 177 current loss 0.669672, current_train_items 5696.
I0302 18:57:51.047140 22760421793920 run.py:483] Algo bellman_ford step 178 current loss 0.517945, current_train_items 5728.
I0302 18:57:51.076832 22760421793920 run.py:483] Algo bellman_ford step 179 current loss 0.713896, current_train_items 5760.
I0302 18:57:51.094393 22760421793920 run.py:483] Algo bellman_ford step 180 current loss 0.068054, current_train_items 5792.
I0302 18:57:51.110207 22760421793920 run.py:483] Algo bellman_ford step 181 current loss 0.221967, current_train_items 5824.
I0302 18:57:51.133905 22760421793920 run.py:483] Algo bellman_ford step 182 current loss 0.639916, current_train_items 5856.
I0302 18:57:51.161816 22760421793920 run.py:483] Algo bellman_ford step 183 current loss 0.593072, current_train_items 5888.
I0302 18:57:51.190401 22760421793920 run.py:483] Algo bellman_ford step 184 current loss 0.609288, current_train_items 5920.
I0302 18:57:51.207879 22760421793920 run.py:483] Algo bellman_ford step 185 current loss 0.050529, current_train_items 5952.
I0302 18:57:51.223815 22760421793920 run.py:483] Algo bellman_ford step 186 current loss 0.323599, current_train_items 5984.
I0302 18:57:51.245433 22760421793920 run.py:483] Algo bellman_ford step 187 current loss 0.330712, current_train_items 6016.
I0302 18:57:51.273353 22760421793920 run.py:483] Algo bellman_ford step 188 current loss 0.558530, current_train_items 6048.
I0302 18:57:51.305137 22760421793920 run.py:483] Algo bellman_ford step 189 current loss 0.758741, current_train_items 6080.
I0302 18:57:51.322466 22760421793920 run.py:483] Algo bellman_ford step 190 current loss 0.050265, current_train_items 6112.
I0302 18:57:51.338238 22760421793920 run.py:483] Algo bellman_ford step 191 current loss 0.270377, current_train_items 6144.
I0302 18:57:51.361349 22760421793920 run.py:483] Algo bellman_ford step 192 current loss 0.451772, current_train_items 6176.
I0302 18:57:51.389345 22760421793920 run.py:483] Algo bellman_ford step 193 current loss 0.411327, current_train_items 6208.
I0302 18:57:51.419597 22760421793920 run.py:483] Algo bellman_ford step 194 current loss 0.514800, current_train_items 6240.
I0302 18:57:51.437313 22760421793920 run.py:483] Algo bellman_ford step 195 current loss 0.059098, current_train_items 6272.
I0302 18:57:51.452995 22760421793920 run.py:483] Algo bellman_ford step 196 current loss 0.230714, current_train_items 6304.
I0302 18:57:51.475272 22760421793920 run.py:483] Algo bellman_ford step 197 current loss 0.317003, current_train_items 6336.
I0302 18:57:51.504296 22760421793920 run.py:483] Algo bellman_ford step 198 current loss 0.736831, current_train_items 6368.
I0302 18:57:51.535318 22760421793920 run.py:483] Algo bellman_ford step 199 current loss 0.668814, current_train_items 6400.
I0302 18:57:51.552891 22760421793920 run.py:483] Algo bellman_ford step 200 current loss 0.049731, current_train_items 6432.
I0302 18:57:51.560370 22760421793920 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:57:51.560482 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.900, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:57:51.576815 22760421793920 run.py:483] Algo bellman_ford step 201 current loss 0.161243, current_train_items 6464.
I0302 18:57:51.599847 22760421793920 run.py:483] Algo bellman_ford step 202 current loss 0.429896, current_train_items 6496.
I0302 18:57:51.630114 22760421793920 run.py:483] Algo bellman_ford step 203 current loss 0.435689, current_train_items 6528.
I0302 18:57:51.663735 22760421793920 run.py:483] Algo bellman_ford step 204 current loss 0.611129, current_train_items 6560.
I0302 18:57:51.681838 22760421793920 run.py:483] Algo bellman_ford step 205 current loss 0.082560, current_train_items 6592.
I0302 18:57:51.696911 22760421793920 run.py:483] Algo bellman_ford step 206 current loss 0.140031, current_train_items 6624.
I0302 18:57:51.720005 22760421793920 run.py:483] Algo bellman_ford step 207 current loss 0.499536, current_train_items 6656.
I0302 18:57:51.748655 22760421793920 run.py:483] Algo bellman_ford step 208 current loss 0.526435, current_train_items 6688.
I0302 18:57:51.778311 22760421793920 run.py:483] Algo bellman_ford step 209 current loss 0.627788, current_train_items 6720.
I0302 18:57:51.796100 22760421793920 run.py:483] Algo bellman_ford step 210 current loss 0.182503, current_train_items 6752.
I0302 18:57:51.811830 22760421793920 run.py:483] Algo bellman_ford step 211 current loss 0.208614, current_train_items 6784.
I0302 18:57:51.835003 22760421793920 run.py:483] Algo bellman_ford step 212 current loss 0.450668, current_train_items 6816.
I0302 18:57:51.863888 22760421793920 run.py:483] Algo bellman_ford step 213 current loss 0.362747, current_train_items 6848.
I0302 18:57:51.890844 22760421793920 run.py:483] Algo bellman_ford step 214 current loss 0.304386, current_train_items 6880.
I0302 18:57:51.908543 22760421793920 run.py:483] Algo bellman_ford step 215 current loss 0.030522, current_train_items 6912.
I0302 18:57:51.924153 22760421793920 run.py:483] Algo bellman_ford step 216 current loss 0.207966, current_train_items 6944.
I0302 18:57:51.946398 22760421793920 run.py:483] Algo bellman_ford step 217 current loss 0.282110, current_train_items 6976.
I0302 18:57:51.974400 22760421793920 run.py:483] Algo bellman_ford step 218 current loss 0.398506, current_train_items 7008.
I0302 18:57:52.003666 22760421793920 run.py:483] Algo bellman_ford step 219 current loss 0.529962, current_train_items 7040.
I0302 18:57:52.021303 22760421793920 run.py:483] Algo bellman_ford step 220 current loss 0.042707, current_train_items 7072.
I0302 18:57:52.037023 22760421793920 run.py:483] Algo bellman_ford step 221 current loss 0.162562, current_train_items 7104.
I0302 18:57:52.060431 22760421793920 run.py:483] Algo bellman_ford step 222 current loss 0.322133, current_train_items 7136.
I0302 18:57:52.088236 22760421793920 run.py:483] Algo bellman_ford step 223 current loss 0.363591, current_train_items 7168.
I0302 18:57:52.118860 22760421793920 run.py:483] Algo bellman_ford step 224 current loss 0.451019, current_train_items 7200.
I0302 18:57:52.136629 22760421793920 run.py:483] Algo bellman_ford step 225 current loss 0.071583, current_train_items 7232.
I0302 18:57:52.152511 22760421793920 run.py:483] Algo bellman_ford step 226 current loss 0.224945, current_train_items 7264.
I0302 18:57:52.175008 22760421793920 run.py:483] Algo bellman_ford step 227 current loss 0.480991, current_train_items 7296.
I0302 18:57:52.203666 22760421793920 run.py:483] Algo bellman_ford step 228 current loss 0.375236, current_train_items 7328.
I0302 18:57:52.234773 22760421793920 run.py:483] Algo bellman_ford step 229 current loss 0.474166, current_train_items 7360.
I0302 18:57:52.252002 22760421793920 run.py:483] Algo bellman_ford step 230 current loss 0.046416, current_train_items 7392.
I0302 18:57:52.267733 22760421793920 run.py:483] Algo bellman_ford step 231 current loss 0.220107, current_train_items 7424.
I0302 18:57:52.290812 22760421793920 run.py:483] Algo bellman_ford step 232 current loss 0.421505, current_train_items 7456.
I0302 18:57:52.319832 22760421793920 run.py:483] Algo bellman_ford step 233 current loss 0.532288, current_train_items 7488.
I0302 18:57:52.351554 22760421793920 run.py:483] Algo bellman_ford step 234 current loss 0.490267, current_train_items 7520.
I0302 18:57:52.369162 22760421793920 run.py:483] Algo bellman_ford step 235 current loss 0.113797, current_train_items 7552.
I0302 18:57:52.384591 22760421793920 run.py:483] Algo bellman_ford step 236 current loss 0.220178, current_train_items 7584.
I0302 18:57:52.407114 22760421793920 run.py:483] Algo bellman_ford step 237 current loss 0.306569, current_train_items 7616.
I0302 18:57:52.435467 22760421793920 run.py:483] Algo bellman_ford step 238 current loss 0.348437, current_train_items 7648.
I0302 18:57:52.465934 22760421793920 run.py:483] Algo bellman_ford step 239 current loss 0.373122, current_train_items 7680.
I0302 18:57:52.483103 22760421793920 run.py:483] Algo bellman_ford step 240 current loss 0.053860, current_train_items 7712.
I0302 18:57:52.498954 22760421793920 run.py:483] Algo bellman_ford step 241 current loss 0.216318, current_train_items 7744.
I0302 18:57:52.521713 22760421793920 run.py:483] Algo bellman_ford step 242 current loss 0.340519, current_train_items 7776.
I0302 18:57:52.549473 22760421793920 run.py:483] Algo bellman_ford step 243 current loss 0.355054, current_train_items 7808.
I0302 18:57:52.577214 22760421793920 run.py:483] Algo bellman_ford step 244 current loss 0.377921, current_train_items 7840.
I0302 18:57:52.594836 22760421793920 run.py:483] Algo bellman_ford step 245 current loss 0.066950, current_train_items 7872.
I0302 18:57:52.610368 22760421793920 run.py:483] Algo bellman_ford step 246 current loss 0.134664, current_train_items 7904.
I0302 18:57:52.632258 22760421793920 run.py:483] Algo bellman_ford step 247 current loss 0.256228, current_train_items 7936.
I0302 18:57:52.661604 22760421793920 run.py:483] Algo bellman_ford step 248 current loss 0.395506, current_train_items 7968.
I0302 18:57:52.692281 22760421793920 run.py:483] Algo bellman_ford step 249 current loss 0.393112, current_train_items 8000.
I0302 18:57:52.709787 22760421793920 run.py:483] Algo bellman_ford step 250 current loss 0.060268, current_train_items 8032.
I0302 18:57:52.717036 22760421793920 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:57:52.717148 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.900, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 18:57:52.745680 22760421793920 run.py:483] Algo bellman_ford step 251 current loss 0.238860, current_train_items 8064.
I0302 18:57:52.769040 22760421793920 run.py:483] Algo bellman_ford step 252 current loss 0.322393, current_train_items 8096.
I0302 18:57:52.798773 22760421793920 run.py:483] Algo bellman_ford step 253 current loss 0.414646, current_train_items 8128.
I0302 18:57:52.830799 22760421793920 run.py:483] Algo bellman_ford step 254 current loss 0.461584, current_train_items 8160.
I0302 18:57:52.849045 22760421793920 run.py:483] Algo bellman_ford step 255 current loss 0.160456, current_train_items 8192.
I0302 18:57:52.864920 22760421793920 run.py:483] Algo bellman_ford step 256 current loss 0.186159, current_train_items 8224.
I0302 18:57:52.886794 22760421793920 run.py:483] Algo bellman_ford step 257 current loss 0.252301, current_train_items 8256.
I0302 18:57:52.915296 22760421793920 run.py:483] Algo bellman_ford step 258 current loss 0.413997, current_train_items 8288.
I0302 18:57:52.944317 22760421793920 run.py:483] Algo bellman_ford step 259 current loss 0.471708, current_train_items 8320.
I0302 18:57:52.961809 22760421793920 run.py:483] Algo bellman_ford step 260 current loss 0.068060, current_train_items 8352.
I0302 18:57:52.977399 22760421793920 run.py:483] Algo bellman_ford step 261 current loss 0.154711, current_train_items 8384.
I0302 18:57:52.999493 22760421793920 run.py:483] Algo bellman_ford step 262 current loss 0.308389, current_train_items 8416.
I0302 18:57:53.027166 22760421793920 run.py:483] Algo bellman_ford step 263 current loss 0.369258, current_train_items 8448.
I0302 18:57:53.056311 22760421793920 run.py:483] Algo bellman_ford step 264 current loss 0.377366, current_train_items 8480.
I0302 18:57:53.073778 22760421793920 run.py:483] Algo bellman_ford step 265 current loss 0.044666, current_train_items 8512.
I0302 18:57:53.089595 22760421793920 run.py:483] Algo bellman_ford step 266 current loss 0.155511, current_train_items 8544.
I0302 18:57:53.112763 22760421793920 run.py:483] Algo bellman_ford step 267 current loss 0.476835, current_train_items 8576.
I0302 18:57:53.141773 22760421793920 run.py:483] Algo bellman_ford step 268 current loss 0.570640, current_train_items 8608.
I0302 18:57:53.170593 22760421793920 run.py:483] Algo bellman_ford step 269 current loss 0.415338, current_train_items 8640.
I0302 18:57:53.188127 22760421793920 run.py:483] Algo bellman_ford step 270 current loss 0.068623, current_train_items 8672.
I0302 18:57:53.203264 22760421793920 run.py:483] Algo bellman_ford step 271 current loss 0.128449, current_train_items 8704.
I0302 18:57:53.226741 22760421793920 run.py:483] Algo bellman_ford step 272 current loss 0.272459, current_train_items 8736.
I0302 18:57:53.254642 22760421793920 run.py:483] Algo bellman_ford step 273 current loss 0.419350, current_train_items 8768.
I0302 18:57:53.283618 22760421793920 run.py:483] Algo bellman_ford step 274 current loss 0.463226, current_train_items 8800.
I0302 18:57:53.301219 22760421793920 run.py:483] Algo bellman_ford step 275 current loss 0.068031, current_train_items 8832.
I0302 18:57:53.316797 22760421793920 run.py:483] Algo bellman_ford step 276 current loss 0.181043, current_train_items 8864.
I0302 18:57:53.340194 22760421793920 run.py:483] Algo bellman_ford step 277 current loss 0.382478, current_train_items 8896.
I0302 18:57:53.369273 22760421793920 run.py:483] Algo bellman_ford step 278 current loss 0.484674, current_train_items 8928.
I0302 18:57:53.399781 22760421793920 run.py:483] Algo bellman_ford step 279 current loss 0.394047, current_train_items 8960.
I0302 18:57:53.416974 22760421793920 run.py:483] Algo bellman_ford step 280 current loss 0.050488, current_train_items 8992.
I0302 18:57:53.432944 22760421793920 run.py:483] Algo bellman_ford step 281 current loss 0.277423, current_train_items 9024.
I0302 18:57:53.456099 22760421793920 run.py:483] Algo bellman_ford step 282 current loss 0.321969, current_train_items 9056.
I0302 18:57:53.483929 22760421793920 run.py:483] Algo bellman_ford step 283 current loss 0.444867, current_train_items 9088.
I0302 18:57:53.515622 22760421793920 run.py:483] Algo bellman_ford step 284 current loss 0.615393, current_train_items 9120.
I0302 18:57:53.533234 22760421793920 run.py:483] Algo bellman_ford step 285 current loss 0.095036, current_train_items 9152.
I0302 18:57:53.549387 22760421793920 run.py:483] Algo bellman_ford step 286 current loss 0.141148, current_train_items 9184.
I0302 18:57:53.571702 22760421793920 run.py:483] Algo bellman_ford step 287 current loss 0.256731, current_train_items 9216.
I0302 18:57:53.600575 22760421793920 run.py:483] Algo bellman_ford step 288 current loss 0.391568, current_train_items 9248.
I0302 18:57:53.631618 22760421793920 run.py:483] Algo bellman_ford step 289 current loss 0.437484, current_train_items 9280.
I0302 18:57:53.648736 22760421793920 run.py:483] Algo bellman_ford step 290 current loss 0.022326, current_train_items 9312.
I0302 18:57:53.664317 22760421793920 run.py:483] Algo bellman_ford step 291 current loss 0.096543, current_train_items 9344.
I0302 18:57:53.687402 22760421793920 run.py:483] Algo bellman_ford step 292 current loss 0.265548, current_train_items 9376.
I0302 18:57:53.715884 22760421793920 run.py:483] Algo bellman_ford step 293 current loss 0.369742, current_train_items 9408.
I0302 18:57:53.746213 22760421793920 run.py:483] Algo bellman_ford step 294 current loss 0.408270, current_train_items 9440.
I0302 18:57:53.763687 22760421793920 run.py:483] Algo bellman_ford step 295 current loss 0.059339, current_train_items 9472.
I0302 18:57:53.779213 22760421793920 run.py:483] Algo bellman_ford step 296 current loss 0.163699, current_train_items 9504.
I0302 18:57:53.802594 22760421793920 run.py:483] Algo bellman_ford step 297 current loss 0.339557, current_train_items 9536.
I0302 18:57:53.832019 22760421793920 run.py:483] Algo bellman_ford step 298 current loss 0.438690, current_train_items 9568.
I0302 18:57:53.861470 22760421793920 run.py:483] Algo bellman_ford step 299 current loss 0.398668, current_train_items 9600.
I0302 18:57:53.878754 22760421793920 run.py:483] Algo bellman_ford step 300 current loss 0.048445, current_train_items 9632.
I0302 18:57:53.886708 22760421793920 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:57:53.886820 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.916, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:57:53.903338 22760421793920 run.py:483] Algo bellman_ford step 301 current loss 0.114506, current_train_items 9664.
I0302 18:57:53.925543 22760421793920 run.py:483] Algo bellman_ford step 302 current loss 0.253997, current_train_items 9696.
I0302 18:57:53.953365 22760421793920 run.py:483] Algo bellman_ford step 303 current loss 0.284618, current_train_items 9728.
I0302 18:57:53.984879 22760421793920 run.py:483] Algo bellman_ford step 304 current loss 0.522675, current_train_items 9760.
I0302 18:57:54.002985 22760421793920 run.py:483] Algo bellman_ford step 305 current loss 0.022422, current_train_items 9792.
I0302 18:57:54.019168 22760421793920 run.py:483] Algo bellman_ford step 306 current loss 0.173545, current_train_items 9824.
I0302 18:57:54.042646 22760421793920 run.py:483] Algo bellman_ford step 307 current loss 0.304305, current_train_items 9856.
I0302 18:57:54.070971 22760421793920 run.py:483] Algo bellman_ford step 308 current loss 0.396308, current_train_items 9888.
I0302 18:57:54.101169 22760421793920 run.py:483] Algo bellman_ford step 309 current loss 0.421098, current_train_items 9920.
I0302 18:57:54.118811 22760421793920 run.py:483] Algo bellman_ford step 310 current loss 0.087883, current_train_items 9952.
I0302 18:57:54.134316 22760421793920 run.py:483] Algo bellman_ford step 311 current loss 0.147335, current_train_items 9984.
I0302 18:57:54.156871 22760421793920 run.py:483] Algo bellman_ford step 312 current loss 0.221437, current_train_items 10016.
I0302 18:57:54.185866 22760421793920 run.py:483] Algo bellman_ford step 313 current loss 0.364274, current_train_items 10048.
I0302 18:57:54.216345 22760421793920 run.py:483] Algo bellman_ford step 314 current loss 0.460537, current_train_items 10080.
I0302 18:57:54.233734 22760421793920 run.py:483] Algo bellman_ford step 315 current loss 0.079923, current_train_items 10112.
I0302 18:57:54.249511 22760421793920 run.py:483] Algo bellman_ford step 316 current loss 0.155162, current_train_items 10144.
I0302 18:57:54.271474 22760421793920 run.py:483] Algo bellman_ford step 317 current loss 0.274695, current_train_items 10176.
I0302 18:57:54.298904 22760421793920 run.py:483] Algo bellman_ford step 318 current loss 0.315132, current_train_items 10208.
I0302 18:57:54.328115 22760421793920 run.py:483] Algo bellman_ford step 319 current loss 0.397168, current_train_items 10240.
I0302 18:57:54.345623 22760421793920 run.py:483] Algo bellman_ford step 320 current loss 0.022553, current_train_items 10272.
I0302 18:57:54.361291 22760421793920 run.py:483] Algo bellman_ford step 321 current loss 0.146417, current_train_items 10304.
I0302 18:57:54.383973 22760421793920 run.py:483] Algo bellman_ford step 322 current loss 0.309397, current_train_items 10336.
I0302 18:57:54.411401 22760421793920 run.py:483] Algo bellman_ford step 323 current loss 0.234196, current_train_items 10368.
I0302 18:57:54.441944 22760421793920 run.py:483] Algo bellman_ford step 324 current loss 0.368602, current_train_items 10400.
I0302 18:57:54.459337 22760421793920 run.py:483] Algo bellman_ford step 325 current loss 0.075525, current_train_items 10432.
I0302 18:57:54.474715 22760421793920 run.py:483] Algo bellman_ford step 326 current loss 0.116465, current_train_items 10464.
I0302 18:57:54.497247 22760421793920 run.py:483] Algo bellman_ford step 327 current loss 0.336431, current_train_items 10496.
I0302 18:57:54.526433 22760421793920 run.py:483] Algo bellman_ford step 328 current loss 0.434727, current_train_items 10528.
I0302 18:57:54.557621 22760421793920 run.py:483] Algo bellman_ford step 329 current loss 0.414224, current_train_items 10560.
I0302 18:57:54.575283 22760421793920 run.py:483] Algo bellman_ford step 330 current loss 0.078573, current_train_items 10592.
I0302 18:57:54.590808 22760421793920 run.py:483] Algo bellman_ford step 331 current loss 0.164100, current_train_items 10624.
I0302 18:57:54.613547 22760421793920 run.py:483] Algo bellman_ford step 332 current loss 0.455837, current_train_items 10656.
I0302 18:57:54.642526 22760421793920 run.py:483] Algo bellman_ford step 333 current loss 0.518553, current_train_items 10688.
I0302 18:57:54.674060 22760421793920 run.py:483] Algo bellman_ford step 334 current loss 0.470943, current_train_items 10720.
I0302 18:57:54.691425 22760421793920 run.py:483] Algo bellman_ford step 335 current loss 0.057782, current_train_items 10752.
I0302 18:57:54.707540 22760421793920 run.py:483] Algo bellman_ford step 336 current loss 0.223363, current_train_items 10784.
I0302 18:57:54.731209 22760421793920 run.py:483] Algo bellman_ford step 337 current loss 0.405164, current_train_items 10816.
I0302 18:57:54.759863 22760421793920 run.py:483] Algo bellman_ford step 338 current loss 0.341880, current_train_items 10848.
I0302 18:57:54.791208 22760421793920 run.py:483] Algo bellman_ford step 339 current loss 0.437476, current_train_items 10880.
I0302 18:57:54.808568 22760421793920 run.py:483] Algo bellman_ford step 340 current loss 0.074617, current_train_items 10912.
I0302 18:57:54.824014 22760421793920 run.py:483] Algo bellman_ford step 341 current loss 0.203122, current_train_items 10944.
I0302 18:57:54.846184 22760421793920 run.py:483] Algo bellman_ford step 342 current loss 0.361877, current_train_items 10976.
I0302 18:57:54.873554 22760421793920 run.py:483] Algo bellman_ford step 343 current loss 0.339839, current_train_items 11008.
I0302 18:57:54.904618 22760421793920 run.py:483] Algo bellman_ford step 344 current loss 0.375681, current_train_items 11040.
I0302 18:57:54.922164 22760421793920 run.py:483] Algo bellman_ford step 345 current loss 0.046389, current_train_items 11072.
I0302 18:57:54.937808 22760421793920 run.py:483] Algo bellman_ford step 346 current loss 0.150151, current_train_items 11104.
I0302 18:57:54.961313 22760421793920 run.py:483] Algo bellman_ford step 347 current loss 0.322328, current_train_items 11136.
I0302 18:57:54.989334 22760421793920 run.py:483] Algo bellman_ford step 348 current loss 0.386851, current_train_items 11168.
I0302 18:57:55.020986 22760421793920 run.py:483] Algo bellman_ford step 349 current loss 0.374680, current_train_items 11200.
I0302 18:57:55.038628 22760421793920 run.py:483] Algo bellman_ford step 350 current loss 0.071975, current_train_items 11232.
I0302 18:57:55.045682 22760421793920 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:57:55.045791 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.916, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 18:57:55.073357 22760421793920 run.py:483] Algo bellman_ford step 351 current loss 0.217293, current_train_items 11264.
I0302 18:57:55.096204 22760421793920 run.py:483] Algo bellman_ford step 352 current loss 0.281611, current_train_items 11296.
I0302 18:57:55.124850 22760421793920 run.py:483] Algo bellman_ford step 353 current loss 0.290813, current_train_items 11328.
I0302 18:57:55.157310 22760421793920 run.py:483] Algo bellman_ford step 354 current loss 0.449836, current_train_items 11360.
I0302 18:57:55.175060 22760421793920 run.py:483] Algo bellman_ford step 355 current loss 0.094961, current_train_items 11392.
I0302 18:57:55.190986 22760421793920 run.py:483] Algo bellman_ford step 356 current loss 0.194776, current_train_items 11424.
I0302 18:57:55.212870 22760421793920 run.py:483] Algo bellman_ford step 357 current loss 0.212772, current_train_items 11456.
I0302 18:57:55.242502 22760421793920 run.py:483] Algo bellman_ford step 358 current loss 0.420406, current_train_items 11488.
I0302 18:57:55.274444 22760421793920 run.py:483] Algo bellman_ford step 359 current loss 0.432063, current_train_items 11520.
I0302 18:57:55.291815 22760421793920 run.py:483] Algo bellman_ford step 360 current loss 0.058437, current_train_items 11552.
I0302 18:57:55.307790 22760421793920 run.py:483] Algo bellman_ford step 361 current loss 0.121183, current_train_items 11584.
I0302 18:57:55.329321 22760421793920 run.py:483] Algo bellman_ford step 362 current loss 0.230066, current_train_items 11616.
I0302 18:57:55.357676 22760421793920 run.py:483] Algo bellman_ford step 363 current loss 0.274971, current_train_items 11648.
I0302 18:57:55.386429 22760421793920 run.py:483] Algo bellman_ford step 364 current loss 0.251152, current_train_items 11680.
I0302 18:57:55.403783 22760421793920 run.py:483] Algo bellman_ford step 365 current loss 0.043399, current_train_items 11712.
I0302 18:57:55.419319 22760421793920 run.py:483] Algo bellman_ford step 366 current loss 0.172044, current_train_items 11744.
I0302 18:57:55.440938 22760421793920 run.py:483] Algo bellman_ford step 367 current loss 0.285563, current_train_items 11776.
I0302 18:57:55.469996 22760421793920 run.py:483] Algo bellman_ford step 368 current loss 0.394406, current_train_items 11808.
I0302 18:57:55.498390 22760421793920 run.py:483] Algo bellman_ford step 369 current loss 0.313105, current_train_items 11840.
I0302 18:57:55.515586 22760421793920 run.py:483] Algo bellman_ford step 370 current loss 0.063650, current_train_items 11872.
I0302 18:57:55.531204 22760421793920 run.py:483] Algo bellman_ford step 371 current loss 0.136597, current_train_items 11904.
I0302 18:57:55.553308 22760421793920 run.py:483] Algo bellman_ford step 372 current loss 0.261816, current_train_items 11936.
I0302 18:57:55.582273 22760421793920 run.py:483] Algo bellman_ford step 373 current loss 0.419895, current_train_items 11968.
I0302 18:57:55.613975 22760421793920 run.py:483] Algo bellman_ford step 374 current loss 0.473019, current_train_items 12000.
I0302 18:57:55.631564 22760421793920 run.py:483] Algo bellman_ford step 375 current loss 0.043799, current_train_items 12032.
I0302 18:57:55.646833 22760421793920 run.py:483] Algo bellman_ford step 376 current loss 0.168768, current_train_items 12064.
I0302 18:57:55.669999 22760421793920 run.py:483] Algo bellman_ford step 377 current loss 0.352011, current_train_items 12096.
I0302 18:57:55.698167 22760421793920 run.py:483] Algo bellman_ford step 378 current loss 0.329033, current_train_items 12128.
I0302 18:57:55.729811 22760421793920 run.py:483] Algo bellman_ford step 379 current loss 0.432654, current_train_items 12160.
I0302 18:57:55.747246 22760421793920 run.py:483] Algo bellman_ford step 380 current loss 0.064816, current_train_items 12192.
I0302 18:57:55.763206 22760421793920 run.py:483] Algo bellman_ford step 381 current loss 0.200835, current_train_items 12224.
I0302 18:57:55.785618 22760421793920 run.py:483] Algo bellman_ford step 382 current loss 0.296402, current_train_items 12256.
I0302 18:57:55.814557 22760421793920 run.py:483] Algo bellman_ford step 383 current loss 0.334543, current_train_items 12288.
I0302 18:57:55.845808 22760421793920 run.py:483] Algo bellman_ford step 384 current loss 0.386719, current_train_items 12320.
I0302 18:57:55.863092 22760421793920 run.py:483] Algo bellman_ford step 385 current loss 0.038934, current_train_items 12352.
I0302 18:57:55.878832 22760421793920 run.py:483] Algo bellman_ford step 386 current loss 0.125852, current_train_items 12384.
I0302 18:57:55.901033 22760421793920 run.py:483] Algo bellman_ford step 387 current loss 0.392572, current_train_items 12416.
I0302 18:57:55.928805 22760421793920 run.py:483] Algo bellman_ford step 388 current loss 0.369262, current_train_items 12448.
I0302 18:57:55.959805 22760421793920 run.py:483] Algo bellman_ford step 389 current loss 0.490835, current_train_items 12480.
I0302 18:57:55.977108 22760421793920 run.py:483] Algo bellman_ford step 390 current loss 0.037225, current_train_items 12512.
I0302 18:57:55.992576 22760421793920 run.py:483] Algo bellman_ford step 391 current loss 0.144923, current_train_items 12544.
I0302 18:57:56.015234 22760421793920 run.py:483] Algo bellman_ford step 392 current loss 0.228127, current_train_items 12576.
I0302 18:57:56.045061 22760421793920 run.py:483] Algo bellman_ford step 393 current loss 0.385507, current_train_items 12608.
I0302 18:57:56.075281 22760421793920 run.py:483] Algo bellman_ford step 394 current loss 0.396793, current_train_items 12640.
I0302 18:57:56.092297 22760421793920 run.py:483] Algo bellman_ford step 395 current loss 0.032438, current_train_items 12672.
I0302 18:57:56.107982 22760421793920 run.py:483] Algo bellman_ford step 396 current loss 0.169769, current_train_items 12704.
I0302 18:57:56.131101 22760421793920 run.py:483] Algo bellman_ford step 397 current loss 0.511324, current_train_items 12736.
I0302 18:57:56.160387 22760421793920 run.py:483] Algo bellman_ford step 398 current loss 0.298468, current_train_items 12768.
I0302 18:57:56.188995 22760421793920 run.py:483] Algo bellman_ford step 399 current loss 0.318588, current_train_items 12800.
I0302 18:57:56.206164 22760421793920 run.py:483] Algo bellman_ford step 400 current loss 0.021695, current_train_items 12832.
I0302 18:57:56.213928 22760421793920 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:57:56.214070 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.939, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 18:57:56.230688 22760421793920 run.py:483] Algo bellman_ford step 401 current loss 0.147297, current_train_items 12864.
I0302 18:57:56.253916 22760421793920 run.py:483] Algo bellman_ford step 402 current loss 0.292321, current_train_items 12896.
I0302 18:57:56.283202 22760421793920 run.py:483] Algo bellman_ford step 403 current loss 0.366904, current_train_items 12928.
I0302 18:57:56.312813 22760421793920 run.py:483] Algo bellman_ford step 404 current loss 0.429262, current_train_items 12960.
I0302 18:57:56.330638 22760421793920 run.py:483] Algo bellman_ford step 405 current loss 0.091094, current_train_items 12992.
I0302 18:57:56.345654 22760421793920 run.py:483] Algo bellman_ford step 406 current loss 0.132636, current_train_items 13024.
I0302 18:57:56.368184 22760421793920 run.py:483] Algo bellman_ford step 407 current loss 0.358146, current_train_items 13056.
I0302 18:57:56.397673 22760421793920 run.py:483] Algo bellman_ford step 408 current loss 0.474306, current_train_items 13088.
I0302 18:57:56.428244 22760421793920 run.py:483] Algo bellman_ford step 409 current loss 0.387665, current_train_items 13120.
I0302 18:57:56.445927 22760421793920 run.py:483] Algo bellman_ford step 410 current loss 0.081040, current_train_items 13152.
I0302 18:57:56.461405 22760421793920 run.py:483] Algo bellman_ford step 411 current loss 0.147193, current_train_items 13184.
I0302 18:57:56.484221 22760421793920 run.py:483] Algo bellman_ford step 412 current loss 0.352542, current_train_items 13216.
I0302 18:57:56.512963 22760421793920 run.py:483] Algo bellman_ford step 413 current loss 0.477831, current_train_items 13248.
I0302 18:57:56.544085 22760421793920 run.py:483] Algo bellman_ford step 414 current loss 0.487565, current_train_items 13280.
I0302 18:57:56.561517 22760421793920 run.py:483] Algo bellman_ford step 415 current loss 0.094910, current_train_items 13312.
I0302 18:57:56.576910 22760421793920 run.py:483] Algo bellman_ford step 416 current loss 0.137735, current_train_items 13344.
I0302 18:57:56.599907 22760421793920 run.py:483] Algo bellman_ford step 417 current loss 0.340622, current_train_items 13376.
I0302 18:57:56.630572 22760421793920 run.py:483] Algo bellman_ford step 418 current loss 0.663621, current_train_items 13408.
I0302 18:57:56.661311 22760421793920 run.py:483] Algo bellman_ford step 419 current loss 0.547255, current_train_items 13440.
I0302 18:57:56.678644 22760421793920 run.py:483] Algo bellman_ford step 420 current loss 0.106828, current_train_items 13472.
I0302 18:57:56.694132 22760421793920 run.py:483] Algo bellman_ford step 421 current loss 0.142988, current_train_items 13504.
I0302 18:57:56.717283 22760421793920 run.py:483] Algo bellman_ford step 422 current loss 0.438233, current_train_items 13536.
I0302 18:57:56.745154 22760421793920 run.py:483] Algo bellman_ford step 423 current loss 0.488643, current_train_items 13568.
I0302 18:57:56.775677 22760421793920 run.py:483] Algo bellman_ford step 424 current loss 0.431848, current_train_items 13600.
I0302 18:57:56.793461 22760421793920 run.py:483] Algo bellman_ford step 425 current loss 0.107349, current_train_items 13632.
I0302 18:57:56.808755 22760421793920 run.py:483] Algo bellman_ford step 426 current loss 0.102308, current_train_items 13664.
I0302 18:57:56.830667 22760421793920 run.py:483] Algo bellman_ford step 427 current loss 0.290928, current_train_items 13696.
I0302 18:57:56.858810 22760421793920 run.py:483] Algo bellman_ford step 428 current loss 0.289183, current_train_items 13728.
I0302 18:57:56.889192 22760421793920 run.py:483] Algo bellman_ford step 429 current loss 0.372986, current_train_items 13760.
I0302 18:57:56.906919 22760421793920 run.py:483] Algo bellman_ford step 430 current loss 0.099634, current_train_items 13792.
I0302 18:57:56.922508 22760421793920 run.py:483] Algo bellman_ford step 431 current loss 0.169083, current_train_items 13824.
I0302 18:57:56.945586 22760421793920 run.py:483] Algo bellman_ford step 432 current loss 0.338272, current_train_items 13856.
I0302 18:57:56.973183 22760421793920 run.py:483] Algo bellman_ford step 433 current loss 0.280126, current_train_items 13888.
I0302 18:57:57.003843 22760421793920 run.py:483] Algo bellman_ford step 434 current loss 0.355658, current_train_items 13920.
I0302 18:57:57.021058 22760421793920 run.py:483] Algo bellman_ford step 435 current loss 0.053937, current_train_items 13952.
I0302 18:57:57.037018 22760421793920 run.py:483] Algo bellman_ford step 436 current loss 0.170552, current_train_items 13984.
I0302 18:57:57.058946 22760421793920 run.py:483] Algo bellman_ford step 437 current loss 0.246087, current_train_items 14016.
I0302 18:57:57.086831 22760421793920 run.py:483] Algo bellman_ford step 438 current loss 0.218524, current_train_items 14048.
I0302 18:57:57.117311 22760421793920 run.py:483] Algo bellman_ford step 439 current loss 0.320887, current_train_items 14080.
I0302 18:57:57.134793 22760421793920 run.py:483] Algo bellman_ford step 440 current loss 0.044855, current_train_items 14112.
I0302 18:57:57.150248 22760421793920 run.py:483] Algo bellman_ford step 441 current loss 0.144013, current_train_items 14144.
I0302 18:57:57.172149 22760421793920 run.py:483] Algo bellman_ford step 442 current loss 0.238416, current_train_items 14176.
I0302 18:57:57.200602 22760421793920 run.py:483] Algo bellman_ford step 443 current loss 0.306307, current_train_items 14208.
I0302 18:57:57.230414 22760421793920 run.py:483] Algo bellman_ford step 444 current loss 0.353627, current_train_items 14240.
I0302 18:57:57.247689 22760421793920 run.py:483] Algo bellman_ford step 445 current loss 0.075153, current_train_items 14272.
I0302 18:57:57.264167 22760421793920 run.py:483] Algo bellman_ford step 446 current loss 0.239174, current_train_items 14304.
I0302 18:57:57.287261 22760421793920 run.py:483] Algo bellman_ford step 447 current loss 0.261728, current_train_items 14336.
I0302 18:57:57.314336 22760421793920 run.py:483] Algo bellman_ford step 448 current loss 0.234531, current_train_items 14368.
I0302 18:57:57.343019 22760421793920 run.py:483] Algo bellman_ford step 449 current loss 0.259853, current_train_items 14400.
I0302 18:57:57.360717 22760421793920 run.py:483] Algo bellman_ford step 450 current loss 0.089601, current_train_items 14432.
I0302 18:57:57.368127 22760421793920 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:57:57.368239 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.939, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0302 18:57:57.396504 22760421793920 run.py:483] Algo bellman_ford step 451 current loss 0.162800, current_train_items 14464.
I0302 18:57:57.418955 22760421793920 run.py:483] Algo bellman_ford step 452 current loss 0.185594, current_train_items 14496.
I0302 18:57:57.449005 22760421793920 run.py:483] Algo bellman_ford step 453 current loss 0.317051, current_train_items 14528.
I0302 18:57:57.480225 22760421793920 run.py:483] Algo bellman_ford step 454 current loss 0.325569, current_train_items 14560.
I0302 18:57:57.497928 22760421793920 run.py:483] Algo bellman_ford step 455 current loss 0.066486, current_train_items 14592.
I0302 18:57:57.514170 22760421793920 run.py:483] Algo bellman_ford step 456 current loss 0.262006, current_train_items 14624.
I0302 18:57:57.536096 22760421793920 run.py:483] Algo bellman_ford step 457 current loss 0.429982, current_train_items 14656.
I0302 18:57:57.564282 22760421793920 run.py:483] Algo bellman_ford step 458 current loss 0.237150, current_train_items 14688.
I0302 18:57:57.593933 22760421793920 run.py:483] Algo bellman_ford step 459 current loss 0.398858, current_train_items 14720.
I0302 18:57:57.611295 22760421793920 run.py:483] Algo bellman_ford step 460 current loss 0.020664, current_train_items 14752.
I0302 18:57:57.627227 22760421793920 run.py:483] Algo bellman_ford step 461 current loss 0.211994, current_train_items 14784.
I0302 18:57:57.650099 22760421793920 run.py:483] Algo bellman_ford step 462 current loss 0.232350, current_train_items 14816.
I0302 18:57:57.678973 22760421793920 run.py:483] Algo bellman_ford step 463 current loss 0.317377, current_train_items 14848.
I0302 18:57:57.710728 22760421793920 run.py:483] Algo bellman_ford step 464 current loss 0.393116, current_train_items 14880.
I0302 18:57:57.728419 22760421793920 run.py:483] Algo bellman_ford step 465 current loss 0.045670, current_train_items 14912.
I0302 18:57:57.743605 22760421793920 run.py:483] Algo bellman_ford step 466 current loss 0.133211, current_train_items 14944.
I0302 18:57:57.766943 22760421793920 run.py:483] Algo bellman_ford step 467 current loss 0.236607, current_train_items 14976.
I0302 18:57:57.794353 22760421793920 run.py:483] Algo bellman_ford step 468 current loss 0.217327, current_train_items 15008.
I0302 18:57:57.826267 22760421793920 run.py:483] Algo bellman_ford step 469 current loss 0.406341, current_train_items 15040.
I0302 18:57:57.843670 22760421793920 run.py:483] Algo bellman_ford step 470 current loss 0.046744, current_train_items 15072.
I0302 18:57:57.859251 22760421793920 run.py:483] Algo bellman_ford step 471 current loss 0.114846, current_train_items 15104.
I0302 18:57:57.882535 22760421793920 run.py:483] Algo bellman_ford step 472 current loss 0.275525, current_train_items 15136.
I0302 18:57:57.911511 22760421793920 run.py:483] Algo bellman_ford step 473 current loss 0.353940, current_train_items 15168.
I0302 18:57:57.942996 22760421793920 run.py:483] Algo bellman_ford step 474 current loss 0.416814, current_train_items 15200.
I0302 18:57:57.960945 22760421793920 run.py:483] Algo bellman_ford step 475 current loss 0.023055, current_train_items 15232.
I0302 18:57:57.976210 22760421793920 run.py:483] Algo bellman_ford step 476 current loss 0.085072, current_train_items 15264.
I0302 18:57:57.999470 22760421793920 run.py:483] Algo bellman_ford step 477 current loss 0.254165, current_train_items 15296.
I0302 18:57:58.026943 22760421793920 run.py:483] Algo bellman_ford step 478 current loss 0.172703, current_train_items 15328.
I0302 18:57:58.058242 22760421793920 run.py:483] Algo bellman_ford step 479 current loss 0.436650, current_train_items 15360.
I0302 18:57:58.075763 22760421793920 run.py:483] Algo bellman_ford step 480 current loss 0.038028, current_train_items 15392.
I0302 18:57:58.091370 22760421793920 run.py:483] Algo bellman_ford step 481 current loss 0.074249, current_train_items 15424.
I0302 18:57:58.113528 22760421793920 run.py:483] Algo bellman_ford step 482 current loss 0.220629, current_train_items 15456.
I0302 18:57:58.142167 22760421793920 run.py:483] Algo bellman_ford step 483 current loss 0.255140, current_train_items 15488.
I0302 18:57:58.173075 22760421793920 run.py:483] Algo bellman_ford step 484 current loss 0.347256, current_train_items 15520.
I0302 18:57:58.190367 22760421793920 run.py:483] Algo bellman_ford step 485 current loss 0.038469, current_train_items 15552.
I0302 18:57:58.205993 22760421793920 run.py:483] Algo bellman_ford step 486 current loss 0.086070, current_train_items 15584.
I0302 18:57:58.229032 22760421793920 run.py:483] Algo bellman_ford step 487 current loss 0.328331, current_train_items 15616.
I0302 18:57:58.257307 22760421793920 run.py:483] Algo bellman_ford step 488 current loss 0.270695, current_train_items 15648.
I0302 18:57:58.286765 22760421793920 run.py:483] Algo bellman_ford step 489 current loss 0.379928, current_train_items 15680.
I0302 18:57:58.304335 22760421793920 run.py:483] Algo bellman_ford step 490 current loss 0.028837, current_train_items 15712.
I0302 18:57:58.320159 22760421793920 run.py:483] Algo bellman_ford step 491 current loss 0.124090, current_train_items 15744.
I0302 18:57:58.342674 22760421793920 run.py:483] Algo bellman_ford step 492 current loss 0.189811, current_train_items 15776.
I0302 18:57:58.370880 22760421793920 run.py:483] Algo bellman_ford step 493 current loss 0.237708, current_train_items 15808.
I0302 18:57:58.402743 22760421793920 run.py:483] Algo bellman_ford step 494 current loss 0.329472, current_train_items 15840.
I0302 18:57:58.420165 22760421793920 run.py:483] Algo bellman_ford step 495 current loss 0.014405, current_train_items 15872.
I0302 18:57:58.435605 22760421793920 run.py:483] Algo bellman_ford step 496 current loss 0.169751, current_train_items 15904.
I0302 18:57:58.459344 22760421793920 run.py:483] Algo bellman_ford step 497 current loss 0.346834, current_train_items 15936.
I0302 18:57:58.488776 22760421793920 run.py:483] Algo bellman_ford step 498 current loss 0.276357, current_train_items 15968.
I0302 18:57:58.519087 22760421793920 run.py:483] Algo bellman_ford step 499 current loss 0.242382, current_train_items 16000.
I0302 18:57:58.536802 22760421793920 run.py:483] Algo bellman_ford step 500 current loss 0.030823, current_train_items 16032.
I0302 18:57:58.544348 22760421793920 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:57:58.544457 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.949, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 18:57:58.560278 22760421793920 run.py:483] Algo bellman_ford step 501 current loss 0.096697, current_train_items 16064.
I0302 18:57:58.583383 22760421793920 run.py:483] Algo bellman_ford step 502 current loss 0.273375, current_train_items 16096.
I0302 18:57:58.613352 22760421793920 run.py:483] Algo bellman_ford step 503 current loss 0.291963, current_train_items 16128.
I0302 18:57:58.646684 22760421793920 run.py:483] Algo bellman_ford step 504 current loss 0.363509, current_train_items 16160.
I0302 18:57:58.664339 22760421793920 run.py:483] Algo bellman_ford step 505 current loss 0.045200, current_train_items 16192.
I0302 18:57:58.679361 22760421793920 run.py:483] Algo bellman_ford step 506 current loss 0.060932, current_train_items 16224.
I0302 18:57:58.701529 22760421793920 run.py:483] Algo bellman_ford step 507 current loss 0.180012, current_train_items 16256.
I0302 18:57:58.730650 22760421793920 run.py:483] Algo bellman_ford step 508 current loss 0.281979, current_train_items 16288.
I0302 18:57:58.762158 22760421793920 run.py:483] Algo bellman_ford step 509 current loss 0.393018, current_train_items 16320.
I0302 18:57:58.779753 22760421793920 run.py:483] Algo bellman_ford step 510 current loss 0.045343, current_train_items 16352.
I0302 18:57:58.795641 22760421793920 run.py:483] Algo bellman_ford step 511 current loss 0.141460, current_train_items 16384.
I0302 18:57:58.818534 22760421793920 run.py:483] Algo bellman_ford step 512 current loss 0.252186, current_train_items 16416.
I0302 18:57:58.847546 22760421793920 run.py:483] Algo bellman_ford step 513 current loss 0.267950, current_train_items 16448.
I0302 18:57:58.878217 22760421793920 run.py:483] Algo bellman_ford step 514 current loss 0.327921, current_train_items 16480.
I0302 18:57:58.895534 22760421793920 run.py:483] Algo bellman_ford step 515 current loss 0.042368, current_train_items 16512.
I0302 18:57:58.910992 22760421793920 run.py:483] Algo bellman_ford step 516 current loss 0.125282, current_train_items 16544.
I0302 18:57:58.934328 22760421793920 run.py:483] Algo bellman_ford step 517 current loss 0.221981, current_train_items 16576.
I0302 18:57:58.964181 22760421793920 run.py:483] Algo bellman_ford step 518 current loss 0.228182, current_train_items 16608.
I0302 18:57:58.995093 22760421793920 run.py:483] Algo bellman_ford step 519 current loss 0.285431, current_train_items 16640.
I0302 18:57:59.012188 22760421793920 run.py:483] Algo bellman_ford step 520 current loss 0.103201, current_train_items 16672.
I0302 18:57:59.028237 22760421793920 run.py:483] Algo bellman_ford step 521 current loss 0.233677, current_train_items 16704.
I0302 18:57:59.050819 22760421793920 run.py:483] Algo bellman_ford step 522 current loss 0.271979, current_train_items 16736.
I0302 18:57:59.079994 22760421793920 run.py:483] Algo bellman_ford step 523 current loss 0.381700, current_train_items 16768.
I0302 18:57:59.109946 22760421793920 run.py:483] Algo bellman_ford step 524 current loss 0.347186, current_train_items 16800.
I0302 18:57:59.127358 22760421793920 run.py:483] Algo bellman_ford step 525 current loss 0.054375, current_train_items 16832.
I0302 18:57:59.143317 22760421793920 run.py:483] Algo bellman_ford step 526 current loss 0.222002, current_train_items 16864.
I0302 18:57:59.166203 22760421793920 run.py:483] Algo bellman_ford step 527 current loss 0.351264, current_train_items 16896.
I0302 18:57:59.194094 22760421793920 run.py:483] Algo bellman_ford step 528 current loss 0.393899, current_train_items 16928.
I0302 18:57:59.225125 22760421793920 run.py:483] Algo bellman_ford step 529 current loss 0.407484, current_train_items 16960.
I0302 18:57:59.242731 22760421793920 run.py:483] Algo bellman_ford step 530 current loss 0.064109, current_train_items 16992.
I0302 18:57:59.258401 22760421793920 run.py:483] Algo bellman_ford step 531 current loss 0.197935, current_train_items 17024.
I0302 18:57:59.280429 22760421793920 run.py:483] Algo bellman_ford step 532 current loss 0.496412, current_train_items 17056.
I0302 18:57:59.309318 22760421793920 run.py:483] Algo bellman_ford step 533 current loss 0.670619, current_train_items 17088.
I0302 18:57:59.339989 22760421793920 run.py:483] Algo bellman_ford step 534 current loss 0.633078, current_train_items 17120.
I0302 18:57:59.357485 22760421793920 run.py:483] Algo bellman_ford step 535 current loss 0.060184, current_train_items 17152.
I0302 18:57:59.372721 22760421793920 run.py:483] Algo bellman_ford step 536 current loss 0.129381, current_train_items 17184.
I0302 18:57:59.394702 22760421793920 run.py:483] Algo bellman_ford step 537 current loss 0.293049, current_train_items 17216.
I0302 18:57:59.422460 22760421793920 run.py:483] Algo bellman_ford step 538 current loss 0.377199, current_train_items 17248.
I0302 18:57:59.451828 22760421793920 run.py:483] Algo bellman_ford step 539 current loss 0.483929, current_train_items 17280.
I0302 18:57:59.469305 22760421793920 run.py:483] Algo bellman_ford step 540 current loss 0.140853, current_train_items 17312.
I0302 18:57:59.484427 22760421793920 run.py:483] Algo bellman_ford step 541 current loss 0.151063, current_train_items 17344.
I0302 18:57:59.507432 22760421793920 run.py:483] Algo bellman_ford step 542 current loss 0.369660, current_train_items 17376.
I0302 18:57:59.536511 22760421793920 run.py:483] Algo bellman_ford step 543 current loss 0.311859, current_train_items 17408.
I0302 18:57:59.567538 22760421793920 run.py:483] Algo bellman_ford step 544 current loss 0.319508, current_train_items 17440.
I0302 18:57:59.585018 22760421793920 run.py:483] Algo bellman_ford step 545 current loss 0.018460, current_train_items 17472.
I0302 18:57:59.600733 22760421793920 run.py:483] Algo bellman_ford step 546 current loss 0.189930, current_train_items 17504.
I0302 18:57:59.623288 22760421793920 run.py:483] Algo bellman_ford step 547 current loss 0.404811, current_train_items 17536.
I0302 18:57:59.652263 22760421793920 run.py:483] Algo bellman_ford step 548 current loss 0.410142, current_train_items 17568.
I0302 18:57:59.680818 22760421793920 run.py:483] Algo bellman_ford step 549 current loss 0.253978, current_train_items 17600.
I0302 18:57:59.698364 22760421793920 run.py:483] Algo bellman_ford step 550 current loss 0.054457, current_train_items 17632.
I0302 18:57:59.705816 22760421793920 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:57:59.705935 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.949, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 18:57:59.722128 22760421793920 run.py:483] Algo bellman_ford step 551 current loss 0.107647, current_train_items 17664.
I0302 18:57:59.744625 22760421793920 run.py:483] Algo bellman_ford step 552 current loss 0.225850, current_train_items 17696.
I0302 18:57:59.774912 22760421793920 run.py:483] Algo bellman_ford step 553 current loss 0.360168, current_train_items 17728.
I0302 18:57:59.805982 22760421793920 run.py:483] Algo bellman_ford step 554 current loss 0.303120, current_train_items 17760.
I0302 18:57:59.823553 22760421793920 run.py:483] Algo bellman_ford step 555 current loss 0.052857, current_train_items 17792.
I0302 18:57:59.839401 22760421793920 run.py:483] Algo bellman_ford step 556 current loss 0.088273, current_train_items 17824.
I0302 18:57:59.862645 22760421793920 run.py:483] Algo bellman_ford step 557 current loss 0.274131, current_train_items 17856.
I0302 18:57:59.890721 22760421793920 run.py:483] Algo bellman_ford step 558 current loss 0.480876, current_train_items 17888.
I0302 18:57:59.923232 22760421793920 run.py:483] Algo bellman_ford step 559 current loss 0.428199, current_train_items 17920.
I0302 18:57:59.940939 22760421793920 run.py:483] Algo bellman_ford step 560 current loss 0.025938, current_train_items 17952.
I0302 18:57:59.956525 22760421793920 run.py:483] Algo bellman_ford step 561 current loss 0.155003, current_train_items 17984.
I0302 18:57:59.979570 22760421793920 run.py:483] Algo bellman_ford step 562 current loss 0.385567, current_train_items 18016.
I0302 18:58:00.007657 22760421793920 run.py:483] Algo bellman_ford step 563 current loss 0.316229, current_train_items 18048.
I0302 18:58:00.036275 22760421793920 run.py:483] Algo bellman_ford step 564 current loss 0.276348, current_train_items 18080.
I0302 18:58:00.053753 22760421793920 run.py:483] Algo bellman_ford step 565 current loss 0.041849, current_train_items 18112.
I0302 18:58:00.069638 22760421793920 run.py:483] Algo bellman_ford step 566 current loss 0.167024, current_train_items 18144.
I0302 18:58:00.093197 22760421793920 run.py:483] Algo bellman_ford step 567 current loss 0.447783, current_train_items 18176.
I0302 18:58:00.121581 22760421793920 run.py:483] Algo bellman_ford step 568 current loss 0.274401, current_train_items 18208.
I0302 18:58:00.151560 22760421793920 run.py:483] Algo bellman_ford step 569 current loss 0.371893, current_train_items 18240.
I0302 18:58:00.169032 22760421793920 run.py:483] Algo bellman_ford step 570 current loss 0.107408, current_train_items 18272.
I0302 18:58:00.184400 22760421793920 run.py:483] Algo bellman_ford step 571 current loss 0.194960, current_train_items 18304.
I0302 18:58:00.207962 22760421793920 run.py:483] Algo bellman_ford step 572 current loss 0.229480, current_train_items 18336.
I0302 18:58:00.237859 22760421793920 run.py:483] Algo bellman_ford step 573 current loss 0.252793, current_train_items 18368.
I0302 18:58:00.267437 22760421793920 run.py:483] Algo bellman_ford step 574 current loss 0.291546, current_train_items 18400.
I0302 18:58:00.285456 22760421793920 run.py:483] Algo bellman_ford step 575 current loss 0.036022, current_train_items 18432.
I0302 18:58:00.300829 22760421793920 run.py:483] Algo bellman_ford step 576 current loss 0.067041, current_train_items 18464.
I0302 18:58:00.323997 22760421793920 run.py:483] Algo bellman_ford step 577 current loss 0.183326, current_train_items 18496.
I0302 18:58:00.353284 22760421793920 run.py:483] Algo bellman_ford step 578 current loss 0.246275, current_train_items 18528.
I0302 18:58:00.384787 22760421793920 run.py:483] Algo bellman_ford step 579 current loss 0.281977, current_train_items 18560.
I0302 18:58:00.402532 22760421793920 run.py:483] Algo bellman_ford step 580 current loss 0.058175, current_train_items 18592.
I0302 18:58:00.418137 22760421793920 run.py:483] Algo bellman_ford step 581 current loss 0.074496, current_train_items 18624.
I0302 18:58:00.439603 22760421793920 run.py:483] Algo bellman_ford step 582 current loss 0.131061, current_train_items 18656.
I0302 18:58:00.467036 22760421793920 run.py:483] Algo bellman_ford step 583 current loss 0.129347, current_train_items 18688.
I0302 18:58:00.497661 22760421793920 run.py:483] Algo bellman_ford step 584 current loss 0.216963, current_train_items 18720.
I0302 18:58:00.514841 22760421793920 run.py:483] Algo bellman_ford step 585 current loss 0.052406, current_train_items 18752.
I0302 18:58:00.530569 22760421793920 run.py:483] Algo bellman_ford step 586 current loss 0.091342, current_train_items 18784.
I0302 18:58:00.552101 22760421793920 run.py:483] Algo bellman_ford step 587 current loss 0.100061, current_train_items 18816.
I0302 18:58:00.579947 22760421793920 run.py:483] Algo bellman_ford step 588 current loss 0.243620, current_train_items 18848.
I0302 18:58:00.612073 22760421793920 run.py:483] Algo bellman_ford step 589 current loss 0.378515, current_train_items 18880.
I0302 18:58:00.629923 22760421793920 run.py:483] Algo bellman_ford step 590 current loss 0.050638, current_train_items 18912.
I0302 18:58:00.645689 22760421793920 run.py:483] Algo bellman_ford step 591 current loss 0.152456, current_train_items 18944.
I0302 18:58:00.669012 22760421793920 run.py:483] Algo bellman_ford step 592 current loss 0.176775, current_train_items 18976.
I0302 18:58:00.697378 22760421793920 run.py:483] Algo bellman_ford step 593 current loss 0.191224, current_train_items 19008.
I0302 18:58:00.727657 22760421793920 run.py:483] Algo bellman_ford step 594 current loss 0.299947, current_train_items 19040.
I0302 18:58:00.745214 22760421793920 run.py:483] Algo bellman_ford step 595 current loss 0.022552, current_train_items 19072.
I0302 18:58:00.760855 22760421793920 run.py:483] Algo bellman_ford step 596 current loss 0.096525, current_train_items 19104.
I0302 18:58:00.784282 22760421793920 run.py:483] Algo bellman_ford step 597 current loss 0.403144, current_train_items 19136.
I0302 18:58:00.812801 22760421793920 run.py:483] Algo bellman_ford step 598 current loss 0.242096, current_train_items 19168.
I0302 18:58:00.843606 22760421793920 run.py:483] Algo bellman_ford step 599 current loss 0.373996, current_train_items 19200.
I0302 18:58:00.861306 22760421793920 run.py:483] Algo bellman_ford step 600 current loss 0.041109, current_train_items 19232.
I0302 18:58:00.868914 22760421793920 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:00.869022 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.949, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0302 18:58:00.885089 22760421793920 run.py:483] Algo bellman_ford step 601 current loss 0.081139, current_train_items 19264.
I0302 18:58:00.908174 22760421793920 run.py:483] Algo bellman_ford step 602 current loss 0.226440, current_train_items 19296.
I0302 18:58:00.936369 22760421793920 run.py:483] Algo bellman_ford step 603 current loss 0.251808, current_train_items 19328.
I0302 18:58:00.966325 22760421793920 run.py:483] Algo bellman_ford step 604 current loss 0.249275, current_train_items 19360.
I0302 18:58:00.984380 22760421793920 run.py:483] Algo bellman_ford step 605 current loss 0.026582, current_train_items 19392.
I0302 18:58:00.999552 22760421793920 run.py:483] Algo bellman_ford step 606 current loss 0.104621, current_train_items 19424.
I0302 18:58:01.022732 22760421793920 run.py:483] Algo bellman_ford step 607 current loss 0.322653, current_train_items 19456.
I0302 18:58:01.050364 22760421793920 run.py:483] Algo bellman_ford step 608 current loss 0.313496, current_train_items 19488.
I0302 18:58:01.080991 22760421793920 run.py:483] Algo bellman_ford step 609 current loss 0.281083, current_train_items 19520.
I0302 18:58:01.098697 22760421793920 run.py:483] Algo bellman_ford step 610 current loss 0.064863, current_train_items 19552.
I0302 18:58:01.114642 22760421793920 run.py:483] Algo bellman_ford step 611 current loss 0.082637, current_train_items 19584.
I0302 18:58:01.137495 22760421793920 run.py:483] Algo bellman_ford step 612 current loss 0.236842, current_train_items 19616.
I0302 18:58:01.164499 22760421793920 run.py:483] Algo bellman_ford step 613 current loss 0.211148, current_train_items 19648.
I0302 18:58:01.194807 22760421793920 run.py:483] Algo bellman_ford step 614 current loss 0.342954, current_train_items 19680.
I0302 18:58:01.212023 22760421793920 run.py:483] Algo bellman_ford step 615 current loss 0.023945, current_train_items 19712.
I0302 18:58:01.227991 22760421793920 run.py:483] Algo bellman_ford step 616 current loss 0.116074, current_train_items 19744.
I0302 18:58:01.250221 22760421793920 run.py:483] Algo bellman_ford step 617 current loss 0.147849, current_train_items 19776.
I0302 18:58:01.278213 22760421793920 run.py:483] Algo bellman_ford step 618 current loss 0.176373, current_train_items 19808.
I0302 18:58:01.308984 22760421793920 run.py:483] Algo bellman_ford step 619 current loss 0.279359, current_train_items 19840.
I0302 18:58:01.326527 22760421793920 run.py:483] Algo bellman_ford step 620 current loss 0.070793, current_train_items 19872.
I0302 18:58:01.342668 22760421793920 run.py:483] Algo bellman_ford step 621 current loss 0.160414, current_train_items 19904.
I0302 18:58:01.366088 22760421793920 run.py:483] Algo bellman_ford step 622 current loss 0.209634, current_train_items 19936.
I0302 18:58:01.394447 22760421793920 run.py:483] Algo bellman_ford step 623 current loss 0.217239, current_train_items 19968.
I0302 18:58:01.424662 22760421793920 run.py:483] Algo bellman_ford step 624 current loss 0.331442, current_train_items 20000.
I0302 18:58:01.442297 22760421793920 run.py:483] Algo bellman_ford step 625 current loss 0.042925, current_train_items 20032.
I0302 18:58:01.457724 22760421793920 run.py:483] Algo bellman_ford step 626 current loss 0.080757, current_train_items 20064.
I0302 18:58:01.481309 22760421793920 run.py:483] Algo bellman_ford step 627 current loss 0.153336, current_train_items 20096.
I0302 18:58:01.510557 22760421793920 run.py:483] Algo bellman_ford step 628 current loss 0.250989, current_train_items 20128.
I0302 18:58:01.541351 22760421793920 run.py:483] Algo bellman_ford step 629 current loss 0.345649, current_train_items 20160.
I0302 18:58:01.558940 22760421793920 run.py:483] Algo bellman_ford step 630 current loss 0.035473, current_train_items 20192.
I0302 18:58:01.574476 22760421793920 run.py:483] Algo bellman_ford step 631 current loss 0.178818, current_train_items 20224.
I0302 18:58:01.596934 22760421793920 run.py:483] Algo bellman_ford step 632 current loss 0.257402, current_train_items 20256.
I0302 18:58:01.625023 22760421793920 run.py:483] Algo bellman_ford step 633 current loss 0.217678, current_train_items 20288.
I0302 18:58:01.655244 22760421793920 run.py:483] Algo bellman_ford step 634 current loss 0.212227, current_train_items 20320.
I0302 18:58:01.672831 22760421793920 run.py:483] Algo bellman_ford step 635 current loss 0.030118, current_train_items 20352.
I0302 18:58:01.688434 22760421793920 run.py:483] Algo bellman_ford step 636 current loss 0.058574, current_train_items 20384.
I0302 18:58:01.711124 22760421793920 run.py:483] Algo bellman_ford step 637 current loss 0.161412, current_train_items 20416.
I0302 18:58:01.740396 22760421793920 run.py:483] Algo bellman_ford step 638 current loss 0.265131, current_train_items 20448.
I0302 18:58:01.768203 22760421793920 run.py:483] Algo bellman_ford step 639 current loss 0.215975, current_train_items 20480.
I0302 18:58:01.785926 22760421793920 run.py:483] Algo bellman_ford step 640 current loss 0.037156, current_train_items 20512.
I0302 18:58:01.801383 22760421793920 run.py:483] Algo bellman_ford step 641 current loss 0.083818, current_train_items 20544.
I0302 18:58:01.823195 22760421793920 run.py:483] Algo bellman_ford step 642 current loss 0.214557, current_train_items 20576.
I0302 18:58:01.851125 22760421793920 run.py:483] Algo bellman_ford step 643 current loss 0.245007, current_train_items 20608.
I0302 18:58:01.881458 22760421793920 run.py:483] Algo bellman_ford step 644 current loss 0.255063, current_train_items 20640.
I0302 18:58:01.898950 22760421793920 run.py:483] Algo bellman_ford step 645 current loss 0.022392, current_train_items 20672.
I0302 18:58:01.915504 22760421793920 run.py:483] Algo bellman_ford step 646 current loss 0.115917, current_train_items 20704.
I0302 18:58:01.937841 22760421793920 run.py:483] Algo bellman_ford step 647 current loss 0.150822, current_train_items 20736.
I0302 18:58:01.967440 22760421793920 run.py:483] Algo bellman_ford step 648 current loss 0.273329, current_train_items 20768.
I0302 18:58:01.998438 22760421793920 run.py:483] Algo bellman_ford step 649 current loss 0.255302, current_train_items 20800.
I0302 18:58:02.015924 22760421793920 run.py:483] Algo bellman_ford step 650 current loss 0.076257, current_train_items 20832.
I0302 18:58:02.023280 22760421793920 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.953125, 'score': 0.953125, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:02.023389 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.949, current avg val score is 0.953, val scores are: bellman_ford: 0.953
I0302 18:58:02.051757 22760421793920 run.py:483] Algo bellman_ford step 651 current loss 0.071545, current_train_items 20864.
I0302 18:58:02.073972 22760421793920 run.py:483] Algo bellman_ford step 652 current loss 0.183005, current_train_items 20896.
I0302 18:58:02.101544 22760421793920 run.py:483] Algo bellman_ford step 653 current loss 0.218453, current_train_items 20928.
I0302 18:58:02.132339 22760421793920 run.py:483] Algo bellman_ford step 654 current loss 0.242861, current_train_items 20960.
I0302 18:58:02.150089 22760421793920 run.py:483] Algo bellman_ford step 655 current loss 0.022992, current_train_items 20992.
I0302 18:58:02.165958 22760421793920 run.py:483] Algo bellman_ford step 656 current loss 0.181642, current_train_items 21024.
I0302 18:58:02.188650 22760421793920 run.py:483] Algo bellman_ford step 657 current loss 0.183361, current_train_items 21056.
I0302 18:58:02.217469 22760421793920 run.py:483] Algo bellman_ford step 658 current loss 0.171315, current_train_items 21088.
I0302 18:58:02.246930 22760421793920 run.py:483] Algo bellman_ford step 659 current loss 0.319921, current_train_items 21120.
I0302 18:58:02.264320 22760421793920 run.py:483] Algo bellman_ford step 660 current loss 0.074468, current_train_items 21152.
I0302 18:58:02.279791 22760421793920 run.py:483] Algo bellman_ford step 661 current loss 0.159444, current_train_items 21184.
I0302 18:58:02.301846 22760421793920 run.py:483] Algo bellman_ford step 662 current loss 0.168606, current_train_items 21216.
I0302 18:58:02.329978 22760421793920 run.py:483] Algo bellman_ford step 663 current loss 0.265559, current_train_items 21248.
I0302 18:58:02.362550 22760421793920 run.py:483] Algo bellman_ford step 664 current loss 0.343844, current_train_items 21280.
I0302 18:58:02.380224 22760421793920 run.py:483] Algo bellman_ford step 665 current loss 0.044879, current_train_items 21312.
I0302 18:58:02.396043 22760421793920 run.py:483] Algo bellman_ford step 666 current loss 0.130876, current_train_items 21344.
I0302 18:58:02.419166 22760421793920 run.py:483] Algo bellman_ford step 667 current loss 0.244188, current_train_items 21376.
I0302 18:58:02.447166 22760421793920 run.py:483] Algo bellman_ford step 668 current loss 0.233024, current_train_items 21408.
I0302 18:58:02.476812 22760421793920 run.py:483] Algo bellman_ford step 669 current loss 0.220682, current_train_items 21440.
I0302 18:58:02.494548 22760421793920 run.py:483] Algo bellman_ford step 670 current loss 0.014765, current_train_items 21472.
I0302 18:58:02.509996 22760421793920 run.py:483] Algo bellman_ford step 671 current loss 0.075364, current_train_items 21504.
I0302 18:58:02.533254 22760421793920 run.py:483] Algo bellman_ford step 672 current loss 0.299757, current_train_items 21536.
I0302 18:58:02.561437 22760421793920 run.py:483] Algo bellman_ford step 673 current loss 0.258856, current_train_items 21568.
I0302 18:58:02.592596 22760421793920 run.py:483] Algo bellman_ford step 674 current loss 0.267545, current_train_items 21600.
I0302 18:58:02.610205 22760421793920 run.py:483] Algo bellman_ford step 675 current loss 0.024804, current_train_items 21632.
I0302 18:58:02.625289 22760421793920 run.py:483] Algo bellman_ford step 676 current loss 0.103331, current_train_items 21664.
I0302 18:58:02.647508 22760421793920 run.py:483] Algo bellman_ford step 677 current loss 0.614644, current_train_items 21696.
I0302 18:58:02.676738 22760421793920 run.py:483] Algo bellman_ford step 678 current loss 0.757679, current_train_items 21728.
I0302 18:58:02.707847 22760421793920 run.py:483] Algo bellman_ford step 679 current loss 0.675367, current_train_items 21760.
I0302 18:58:02.725418 22760421793920 run.py:483] Algo bellman_ford step 680 current loss 0.143024, current_train_items 21792.
I0302 18:58:02.741121 22760421793920 run.py:483] Algo bellman_ford step 681 current loss 0.147039, current_train_items 21824.
I0302 18:58:02.763665 22760421793920 run.py:483] Algo bellman_ford step 682 current loss 0.294621, current_train_items 21856.
I0302 18:58:02.791315 22760421793920 run.py:483] Algo bellman_ford step 683 current loss 0.295729, current_train_items 21888.
I0302 18:58:02.822752 22760421793920 run.py:483] Algo bellman_ford step 684 current loss 0.398900, current_train_items 21920.
I0302 18:58:02.840185 22760421793920 run.py:483] Algo bellman_ford step 685 current loss 0.091602, current_train_items 21952.
I0302 18:58:02.855617 22760421793920 run.py:483] Algo bellman_ford step 686 current loss 0.152912, current_train_items 21984.
I0302 18:58:02.878452 22760421793920 run.py:483] Algo bellman_ford step 687 current loss 0.224404, current_train_items 22016.
I0302 18:58:02.905097 22760421793920 run.py:483] Algo bellman_ford step 688 current loss 0.179322, current_train_items 22048.
I0302 18:58:02.937321 22760421793920 run.py:483] Algo bellman_ford step 689 current loss 0.372248, current_train_items 22080.
I0302 18:58:02.954580 22760421793920 run.py:483] Algo bellman_ford step 690 current loss 0.046049, current_train_items 22112.
I0302 18:58:02.970606 22760421793920 run.py:483] Algo bellman_ford step 691 current loss 0.148758, current_train_items 22144.
I0302 18:58:02.993970 22760421793920 run.py:483] Algo bellman_ford step 692 current loss 0.205743, current_train_items 22176.
I0302 18:58:03.021555 22760421793920 run.py:483] Algo bellman_ford step 693 current loss 0.302802, current_train_items 22208.
I0302 18:58:03.053400 22760421793920 run.py:483] Algo bellman_ford step 694 current loss 0.364755, current_train_items 22240.
I0302 18:58:03.070688 22760421793920 run.py:483] Algo bellman_ford step 695 current loss 0.029951, current_train_items 22272.
I0302 18:58:03.086106 22760421793920 run.py:483] Algo bellman_ford step 696 current loss 0.090928, current_train_items 22304.
I0302 18:58:03.107164 22760421793920 run.py:483] Algo bellman_ford step 697 current loss 0.286618, current_train_items 22336.
I0302 18:58:03.135540 22760421793920 run.py:483] Algo bellman_ford step 698 current loss 0.197603, current_train_items 22368.
I0302 18:58:03.167022 22760421793920 run.py:483] Algo bellman_ford step 699 current loss 0.366496, current_train_items 22400.
I0302 18:58:03.184398 22760421793920 run.py:483] Algo bellman_ford step 700 current loss 0.025756, current_train_items 22432.
I0302 18:58:03.191914 22760421793920 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9453125, 'score': 0.9453125, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:03.192026 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.953, current avg val score is 0.945, val scores are: bellman_ford: 0.945
I0302 18:58:03.207919 22760421793920 run.py:483] Algo bellman_ford step 701 current loss 0.042076, current_train_items 22464.
I0302 18:58:03.230759 22760421793920 run.py:483] Algo bellman_ford step 702 current loss 0.180883, current_train_items 22496.
I0302 18:58:03.258256 22760421793920 run.py:483] Algo bellman_ford step 703 current loss 0.180759, current_train_items 22528.
I0302 18:58:03.288003 22760421793920 run.py:483] Algo bellman_ford step 704 current loss 0.219830, current_train_items 22560.
I0302 18:58:03.305994 22760421793920 run.py:483] Algo bellman_ford step 705 current loss 0.023053, current_train_items 22592.
I0302 18:58:03.321458 22760421793920 run.py:483] Algo bellman_ford step 706 current loss 0.065282, current_train_items 22624.
I0302 18:58:03.344232 22760421793920 run.py:483] Algo bellman_ford step 707 current loss 0.124529, current_train_items 22656.
I0302 18:58:03.372410 22760421793920 run.py:483] Algo bellman_ford step 708 current loss 0.192248, current_train_items 22688.
I0302 18:58:03.403912 22760421793920 run.py:483] Algo bellman_ford step 709 current loss 0.288267, current_train_items 22720.
I0302 18:58:03.421461 22760421793920 run.py:483] Algo bellman_ford step 710 current loss 0.020113, current_train_items 22752.
I0302 18:58:03.437116 22760421793920 run.py:483] Algo bellman_ford step 711 current loss 0.097646, current_train_items 22784.
I0302 18:58:03.460215 22760421793920 run.py:483] Algo bellman_ford step 712 current loss 0.204114, current_train_items 22816.
I0302 18:58:03.489632 22760421793920 run.py:483] Algo bellman_ford step 713 current loss 0.229319, current_train_items 22848.
I0302 18:58:03.518580 22760421793920 run.py:483] Algo bellman_ford step 714 current loss 0.241661, current_train_items 22880.
I0302 18:58:03.536303 22760421793920 run.py:483] Algo bellman_ford step 715 current loss 0.015874, current_train_items 22912.
I0302 18:58:03.552253 22760421793920 run.py:483] Algo bellman_ford step 716 current loss 0.175681, current_train_items 22944.
I0302 18:58:03.576091 22760421793920 run.py:483] Algo bellman_ford step 717 current loss 0.276992, current_train_items 22976.
I0302 18:58:03.603876 22760421793920 run.py:483] Algo bellman_ford step 718 current loss 0.147575, current_train_items 23008.
I0302 18:58:03.634659 22760421793920 run.py:483] Algo bellman_ford step 719 current loss 0.277625, current_train_items 23040.
I0302 18:58:03.652403 22760421793920 run.py:483] Algo bellman_ford step 720 current loss 0.030859, current_train_items 23072.
I0302 18:58:03.668442 22760421793920 run.py:483] Algo bellman_ford step 721 current loss 0.125982, current_train_items 23104.
I0302 18:58:03.690742 22760421793920 run.py:483] Algo bellman_ford step 722 current loss 0.134129, current_train_items 23136.
I0302 18:58:03.718703 22760421793920 run.py:483] Algo bellman_ford step 723 current loss 0.181839, current_train_items 23168.
I0302 18:58:03.748305 22760421793920 run.py:483] Algo bellman_ford step 724 current loss 0.278217, current_train_items 23200.
I0302 18:58:03.765873 22760421793920 run.py:483] Algo bellman_ford step 725 current loss 0.038841, current_train_items 23232.
I0302 18:58:03.781938 22760421793920 run.py:483] Algo bellman_ford step 726 current loss 0.068167, current_train_items 23264.
I0302 18:58:03.804931 22760421793920 run.py:483] Algo bellman_ford step 727 current loss 0.191698, current_train_items 23296.
I0302 18:58:03.832548 22760421793920 run.py:483] Algo bellman_ford step 728 current loss 0.163205, current_train_items 23328.
I0302 18:58:03.863128 22760421793920 run.py:483] Algo bellman_ford step 729 current loss 0.229580, current_train_items 23360.
I0302 18:58:03.880799 22760421793920 run.py:483] Algo bellman_ford step 730 current loss 0.014381, current_train_items 23392.
I0302 18:58:03.896468 22760421793920 run.py:483] Algo bellman_ford step 731 current loss 0.051796, current_train_items 23424.
I0302 18:58:03.919358 22760421793920 run.py:483] Algo bellman_ford step 732 current loss 0.175226, current_train_items 23456.
I0302 18:58:03.947721 22760421793920 run.py:483] Algo bellman_ford step 733 current loss 0.192913, current_train_items 23488.
I0302 18:58:03.978977 22760421793920 run.py:483] Algo bellman_ford step 734 current loss 0.224642, current_train_items 23520.
I0302 18:58:03.996497 22760421793920 run.py:483] Algo bellman_ford step 735 current loss 0.024528, current_train_items 23552.
I0302 18:58:04.012497 22760421793920 run.py:483] Algo bellman_ford step 736 current loss 0.056562, current_train_items 23584.
I0302 18:58:04.035086 22760421793920 run.py:483] Algo bellman_ford step 737 current loss 0.142199, current_train_items 23616.
I0302 18:58:04.063671 22760421793920 run.py:483] Algo bellman_ford step 738 current loss 0.173633, current_train_items 23648.
I0302 18:58:04.092536 22760421793920 run.py:483] Algo bellman_ford step 739 current loss 0.152048, current_train_items 23680.
I0302 18:58:04.110066 22760421793920 run.py:483] Algo bellman_ford step 740 current loss 0.030492, current_train_items 23712.
I0302 18:58:04.125825 22760421793920 run.py:483] Algo bellman_ford step 741 current loss 0.073937, current_train_items 23744.
I0302 18:58:04.148202 22760421793920 run.py:483] Algo bellman_ford step 742 current loss 0.151305, current_train_items 23776.
I0302 18:58:04.175811 22760421793920 run.py:483] Algo bellman_ford step 743 current loss 0.123088, current_train_items 23808.
I0302 18:58:04.208216 22760421793920 run.py:483] Algo bellman_ford step 744 current loss 0.259462, current_train_items 23840.
I0302 18:58:04.225894 22760421793920 run.py:483] Algo bellman_ford step 745 current loss 0.012560, current_train_items 23872.
I0302 18:58:04.241481 22760421793920 run.py:483] Algo bellman_ford step 746 current loss 0.101442, current_train_items 23904.
I0302 18:58:04.263877 22760421793920 run.py:483] Algo bellman_ford step 747 current loss 0.177346, current_train_items 23936.
I0302 18:58:04.292949 22760421793920 run.py:483] Algo bellman_ford step 748 current loss 0.312763, current_train_items 23968.
I0302 18:58:04.323166 22760421793920 run.py:483] Algo bellman_ford step 749 current loss 0.300533, current_train_items 24000.
I0302 18:58:04.340704 22760421793920 run.py:483] Algo bellman_ford step 750 current loss 0.023864, current_train_items 24032.
I0302 18:58:04.348198 22760421793920 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:04.348309 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.953, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 18:58:04.364786 22760421793920 run.py:483] Algo bellman_ford step 751 current loss 0.100239, current_train_items 24064.
I0302 18:58:04.386909 22760421793920 run.py:483] Algo bellman_ford step 752 current loss 0.210632, current_train_items 24096.
I0302 18:58:04.414885 22760421793920 run.py:483] Algo bellman_ford step 753 current loss 0.204895, current_train_items 24128.
I0302 18:58:04.446141 22760421793920 run.py:483] Algo bellman_ford step 754 current loss 0.255169, current_train_items 24160.
I0302 18:58:04.464017 22760421793920 run.py:483] Algo bellman_ford step 755 current loss 0.060905, current_train_items 24192.
I0302 18:58:04.479732 22760421793920 run.py:483] Algo bellman_ford step 756 current loss 0.076942, current_train_items 24224.
I0302 18:58:04.502957 22760421793920 run.py:483] Algo bellman_ford step 757 current loss 0.150472, current_train_items 24256.
I0302 18:58:04.530944 22760421793920 run.py:483] Algo bellman_ford step 758 current loss 0.190668, current_train_items 24288.
I0302 18:58:04.560808 22760421793920 run.py:483] Algo bellman_ford step 759 current loss 0.253281, current_train_items 24320.
I0302 18:58:04.578064 22760421793920 run.py:483] Algo bellman_ford step 760 current loss 0.038269, current_train_items 24352.
I0302 18:58:04.593435 22760421793920 run.py:483] Algo bellman_ford step 761 current loss 0.092682, current_train_items 24384.
I0302 18:58:04.616052 22760421793920 run.py:483] Algo bellman_ford step 762 current loss 0.238223, current_train_items 24416.
I0302 18:58:04.643656 22760421793920 run.py:483] Algo bellman_ford step 763 current loss 0.145903, current_train_items 24448.
I0302 18:58:04.674407 22760421793920 run.py:483] Algo bellman_ford step 764 current loss 0.256742, current_train_items 24480.
I0302 18:58:04.691754 22760421793920 run.py:483] Algo bellman_ford step 765 current loss 0.038608, current_train_items 24512.
I0302 18:58:04.707911 22760421793920 run.py:483] Algo bellman_ford step 766 current loss 0.148466, current_train_items 24544.
I0302 18:58:04.730453 22760421793920 run.py:483] Algo bellman_ford step 767 current loss 0.212963, current_train_items 24576.
I0302 18:58:04.758874 22760421793920 run.py:483] Algo bellman_ford step 768 current loss 0.171336, current_train_items 24608.
I0302 18:58:04.790175 22760421793920 run.py:483] Algo bellman_ford step 769 current loss 0.282588, current_train_items 24640.
I0302 18:58:04.807585 22760421793920 run.py:483] Algo bellman_ford step 770 current loss 0.026402, current_train_items 24672.
I0302 18:58:04.823379 22760421793920 run.py:483] Algo bellman_ford step 771 current loss 0.118910, current_train_items 24704.
I0302 18:58:04.845831 22760421793920 run.py:483] Algo bellman_ford step 772 current loss 0.257063, current_train_items 24736.
I0302 18:58:04.873739 22760421793920 run.py:483] Algo bellman_ford step 773 current loss 0.210489, current_train_items 24768.
I0302 18:58:04.905160 22760421793920 run.py:483] Algo bellman_ford step 774 current loss 0.319851, current_train_items 24800.
I0302 18:58:04.922676 22760421793920 run.py:483] Algo bellman_ford step 775 current loss 0.038342, current_train_items 24832.
I0302 18:58:04.938917 22760421793920 run.py:483] Algo bellman_ford step 776 current loss 0.076257, current_train_items 24864.
I0302 18:58:04.961861 22760421793920 run.py:483] Algo bellman_ford step 777 current loss 0.185087, current_train_items 24896.
I0302 18:58:04.989280 22760421793920 run.py:483] Algo bellman_ford step 778 current loss 0.223956, current_train_items 24928.
I0302 18:58:05.019316 22760421793920 run.py:483] Algo bellman_ford step 779 current loss 0.224075, current_train_items 24960.
I0302 18:58:05.036807 22760421793920 run.py:483] Algo bellman_ford step 780 current loss 0.073366, current_train_items 24992.
I0302 18:58:05.052723 22760421793920 run.py:483] Algo bellman_ford step 781 current loss 0.100909, current_train_items 25024.
I0302 18:58:05.074694 22760421793920 run.py:483] Algo bellman_ford step 782 current loss 0.092417, current_train_items 25056.
I0302 18:58:05.103049 22760421793920 run.py:483] Algo bellman_ford step 783 current loss 0.177162, current_train_items 25088.
I0302 18:58:05.134596 22760421793920 run.py:483] Algo bellman_ford step 784 current loss 0.226264, current_train_items 25120.
I0302 18:58:05.151812 22760421793920 run.py:483] Algo bellman_ford step 785 current loss 0.029723, current_train_items 25152.
I0302 18:58:05.167807 22760421793920 run.py:483] Algo bellman_ford step 786 current loss 0.071673, current_train_items 25184.
I0302 18:58:05.190212 22760421793920 run.py:483] Algo bellman_ford step 787 current loss 0.099780, current_train_items 25216.
I0302 18:58:05.218693 22760421793920 run.py:483] Algo bellman_ford step 788 current loss 0.181849, current_train_items 25248.
I0302 18:58:05.247201 22760421793920 run.py:483] Algo bellman_ford step 789 current loss 0.274187, current_train_items 25280.
I0302 18:58:05.264569 22760421793920 run.py:483] Algo bellman_ford step 790 current loss 0.041991, current_train_items 25312.
I0302 18:58:05.280558 22760421793920 run.py:483] Algo bellman_ford step 791 current loss 0.119103, current_train_items 25344.
I0302 18:58:05.301740 22760421793920 run.py:483] Algo bellman_ford step 792 current loss 0.128441, current_train_items 25376.
I0302 18:58:05.329931 22760421793920 run.py:483] Algo bellman_ford step 793 current loss 0.165227, current_train_items 25408.
I0302 18:58:05.359030 22760421793920 run.py:483] Algo bellman_ford step 794 current loss 0.204192, current_train_items 25440.
I0302 18:58:05.376710 22760421793920 run.py:483] Algo bellman_ford step 795 current loss 0.036940, current_train_items 25472.
I0302 18:58:05.392063 22760421793920 run.py:483] Algo bellman_ford step 796 current loss 0.107396, current_train_items 25504.
I0302 18:58:05.414907 22760421793920 run.py:483] Algo bellman_ford step 797 current loss 0.130777, current_train_items 25536.
I0302 18:58:05.442174 22760421793920 run.py:483] Algo bellman_ford step 798 current loss 0.140718, current_train_items 25568.
I0302 18:58:05.472054 22760421793920 run.py:483] Algo bellman_ford step 799 current loss 0.195332, current_train_items 25600.
I0302 18:58:05.489005 22760421793920 run.py:483] Algo bellman_ford step 800 current loss 0.009924, current_train_items 25632.
I0302 18:58:05.496535 22760421793920 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:05.496646 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.953, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0302 18:58:05.524452 22760421793920 run.py:483] Algo bellman_ford step 801 current loss 0.050532, current_train_items 25664.
I0302 18:58:05.548213 22760421793920 run.py:483] Algo bellman_ford step 802 current loss 0.168985, current_train_items 25696.
I0302 18:58:05.577352 22760421793920 run.py:483] Algo bellman_ford step 803 current loss 0.180455, current_train_items 25728.
I0302 18:58:05.608053 22760421793920 run.py:483] Algo bellman_ford step 804 current loss 0.241783, current_train_items 25760.
I0302 18:58:05.626461 22760421793920 run.py:483] Algo bellman_ford step 805 current loss 0.021254, current_train_items 25792.
I0302 18:58:05.642108 22760421793920 run.py:483] Algo bellman_ford step 806 current loss 0.061432, current_train_items 25824.
I0302 18:58:05.665472 22760421793920 run.py:483] Algo bellman_ford step 807 current loss 0.141511, current_train_items 25856.
I0302 18:58:05.693861 22760421793920 run.py:483] Algo bellman_ford step 808 current loss 0.185020, current_train_items 25888.
I0302 18:58:05.723495 22760421793920 run.py:483] Algo bellman_ford step 809 current loss 0.183853, current_train_items 25920.
I0302 18:58:05.741640 22760421793920 run.py:483] Algo bellman_ford step 810 current loss 0.025518, current_train_items 25952.
I0302 18:58:05.757927 22760421793920 run.py:483] Algo bellman_ford step 811 current loss 0.145128, current_train_items 25984.
I0302 18:58:05.780231 22760421793920 run.py:483] Algo bellman_ford step 812 current loss 0.130877, current_train_items 26016.
I0302 18:58:05.808026 22760421793920 run.py:483] Algo bellman_ford step 813 current loss 0.125739, current_train_items 26048.
I0302 18:58:05.838503 22760421793920 run.py:483] Algo bellman_ford step 814 current loss 0.258247, current_train_items 26080.
I0302 18:58:05.856226 22760421793920 run.py:483] Algo bellman_ford step 815 current loss 0.047751, current_train_items 26112.
I0302 18:58:05.872207 22760421793920 run.py:483] Algo bellman_ford step 816 current loss 0.108340, current_train_items 26144.
I0302 18:58:05.895210 22760421793920 run.py:483] Algo bellman_ford step 817 current loss 0.090530, current_train_items 26176.
I0302 18:58:05.924376 22760421793920 run.py:483] Algo bellman_ford step 818 current loss 0.211510, current_train_items 26208.
I0302 18:58:05.956927 22760421793920 run.py:483] Algo bellman_ford step 819 current loss 0.328270, current_train_items 26240.
I0302 18:58:05.974451 22760421793920 run.py:483] Algo bellman_ford step 820 current loss 0.029425, current_train_items 26272.
I0302 18:58:05.989804 22760421793920 run.py:483] Algo bellman_ford step 821 current loss 0.085762, current_train_items 26304.
I0302 18:58:06.012045 22760421793920 run.py:483] Algo bellman_ford step 822 current loss 0.119765, current_train_items 26336.
I0302 18:58:06.040720 22760421793920 run.py:483] Algo bellman_ford step 823 current loss 0.229526, current_train_items 26368.
I0302 18:58:06.070262 22760421793920 run.py:483] Algo bellman_ford step 824 current loss 0.363568, current_train_items 26400.
I0302 18:58:06.088199 22760421793920 run.py:483] Algo bellman_ford step 825 current loss 0.066808, current_train_items 26432.
I0302 18:58:06.104222 22760421793920 run.py:483] Algo bellman_ford step 826 current loss 0.074536, current_train_items 26464.
I0302 18:58:06.127713 22760421793920 run.py:483] Algo bellman_ford step 827 current loss 0.167920, current_train_items 26496.
I0302 18:58:06.156136 22760421793920 run.py:483] Algo bellman_ford step 828 current loss 0.228729, current_train_items 26528.
I0302 18:58:06.186305 22760421793920 run.py:483] Algo bellman_ford step 829 current loss 0.214311, current_train_items 26560.
I0302 18:58:06.203710 22760421793920 run.py:483] Algo bellman_ford step 830 current loss 0.013661, current_train_items 26592.
I0302 18:58:06.219263 22760421793920 run.py:483] Algo bellman_ford step 831 current loss 0.060745, current_train_items 26624.
I0302 18:58:06.242078 22760421793920 run.py:483] Algo bellman_ford step 832 current loss 0.170275, current_train_items 26656.
I0302 18:58:06.270495 22760421793920 run.py:483] Algo bellman_ford step 833 current loss 0.173663, current_train_items 26688.
I0302 18:58:06.299737 22760421793920 run.py:483] Algo bellman_ford step 834 current loss 0.150107, current_train_items 26720.
I0302 18:58:06.317580 22760421793920 run.py:483] Algo bellman_ford step 835 current loss 0.022363, current_train_items 26752.
I0302 18:58:06.333139 22760421793920 run.py:483] Algo bellman_ford step 836 current loss 0.078641, current_train_items 26784.
I0302 18:58:06.356091 22760421793920 run.py:483] Algo bellman_ford step 837 current loss 0.208181, current_train_items 26816.
I0302 18:58:06.384352 22760421793920 run.py:483] Algo bellman_ford step 838 current loss 0.110833, current_train_items 26848.
I0302 18:58:06.416075 22760421793920 run.py:483] Algo bellman_ford step 839 current loss 0.263130, current_train_items 26880.
I0302 18:58:06.433508 22760421793920 run.py:483] Algo bellman_ford step 840 current loss 0.130987, current_train_items 26912.
I0302 18:58:06.449263 22760421793920 run.py:483] Algo bellman_ford step 841 current loss 0.108099, current_train_items 26944.
I0302 18:58:06.472350 22760421793920 run.py:483] Algo bellman_ford step 842 current loss 0.162679, current_train_items 26976.
I0302 18:58:06.499530 22760421793920 run.py:483] Algo bellman_ford step 843 current loss 0.166907, current_train_items 27008.
I0302 18:58:06.529080 22760421793920 run.py:483] Algo bellman_ford step 844 current loss 0.194877, current_train_items 27040.
I0302 18:58:06.546395 22760421793920 run.py:483] Algo bellman_ford step 845 current loss 0.015574, current_train_items 27072.
I0302 18:58:06.562299 22760421793920 run.py:483] Algo bellman_ford step 846 current loss 0.052520, current_train_items 27104.
I0302 18:58:06.586538 22760421793920 run.py:483] Algo bellman_ford step 847 current loss 0.136057, current_train_items 27136.
I0302 18:58:06.614291 22760421793920 run.py:483] Algo bellman_ford step 848 current loss 0.177846, current_train_items 27168.
I0302 18:58:06.645281 22760421793920 run.py:483] Algo bellman_ford step 849 current loss 0.228491, current_train_items 27200.
I0302 18:58:06.662634 22760421793920 run.py:483] Algo bellman_ford step 850 current loss 0.012668, current_train_items 27232.
I0302 18:58:06.670272 22760421793920 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:06.670383 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.963, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 18:58:06.686300 22760421793920 run.py:483] Algo bellman_ford step 851 current loss 0.076247, current_train_items 27264.
I0302 18:58:06.709496 22760421793920 run.py:483] Algo bellman_ford step 852 current loss 0.146604, current_train_items 27296.
I0302 18:58:06.739450 22760421793920 run.py:483] Algo bellman_ford step 853 current loss 0.212023, current_train_items 27328.
I0302 18:58:06.769685 22760421793920 run.py:483] Algo bellman_ford step 854 current loss 0.225412, current_train_items 27360.
I0302 18:58:06.787459 22760421793920 run.py:483] Algo bellman_ford step 855 current loss 0.024189, current_train_items 27392.
I0302 18:58:06.803715 22760421793920 run.py:483] Algo bellman_ford step 856 current loss 0.044706, current_train_items 27424.
I0302 18:58:06.825469 22760421793920 run.py:483] Algo bellman_ford step 857 current loss 0.189075, current_train_items 27456.
I0302 18:58:06.855128 22760421793920 run.py:483] Algo bellman_ford step 858 current loss 0.277704, current_train_items 27488.
I0302 18:58:06.885034 22760421793920 run.py:483] Algo bellman_ford step 859 current loss 0.251570, current_train_items 27520.
I0302 18:58:06.902753 22760421793920 run.py:483] Algo bellman_ford step 860 current loss 0.021620, current_train_items 27552.
I0302 18:58:06.918612 22760421793920 run.py:483] Algo bellman_ford step 861 current loss 0.080123, current_train_items 27584.
I0302 18:58:06.941385 22760421793920 run.py:483] Algo bellman_ford step 862 current loss 0.198406, current_train_items 27616.
I0302 18:58:06.969919 22760421793920 run.py:483] Algo bellman_ford step 863 current loss 0.290739, current_train_items 27648.
I0302 18:58:06.999523 22760421793920 run.py:483] Algo bellman_ford step 864 current loss 0.318923, current_train_items 27680.
I0302 18:58:07.016553 22760421793920 run.py:483] Algo bellman_ford step 865 current loss 0.046467, current_train_items 27712.
I0302 18:58:07.031934 22760421793920 run.py:483] Algo bellman_ford step 866 current loss 0.088014, current_train_items 27744.
I0302 18:58:07.055328 22760421793920 run.py:483] Algo bellman_ford step 867 current loss 0.260311, current_train_items 27776.
I0302 18:58:07.083439 22760421793920 run.py:483] Algo bellman_ford step 868 current loss 0.172186, current_train_items 27808.
I0302 18:58:07.115275 22760421793920 run.py:483] Algo bellman_ford step 869 current loss 0.247975, current_train_items 27840.
I0302 18:58:07.133029 22760421793920 run.py:483] Algo bellman_ford step 870 current loss 0.050662, current_train_items 27872.
I0302 18:58:07.148892 22760421793920 run.py:483] Algo bellman_ford step 871 current loss 0.075616, current_train_items 27904.
I0302 18:58:07.170854 22760421793920 run.py:483] Algo bellman_ford step 872 current loss 0.145149, current_train_items 27936.
I0302 18:58:07.199991 22760421793920 run.py:483] Algo bellman_ford step 873 current loss 0.224792, current_train_items 27968.
I0302 18:58:07.229825 22760421793920 run.py:483] Algo bellman_ford step 874 current loss 0.246642, current_train_items 28000.
I0302 18:58:07.247435 22760421793920 run.py:483] Algo bellman_ford step 875 current loss 0.021608, current_train_items 28032.
I0302 18:58:07.263439 22760421793920 run.py:483] Algo bellman_ford step 876 current loss 0.076593, current_train_items 28064.
I0302 18:58:07.288069 22760421793920 run.py:483] Algo bellman_ford step 877 current loss 0.202532, current_train_items 28096.
I0302 18:58:07.315802 22760421793920 run.py:483] Algo bellman_ford step 878 current loss 0.170803, current_train_items 28128.
I0302 18:58:07.347475 22760421793920 run.py:483] Algo bellman_ford step 879 current loss 0.213254, current_train_items 28160.
I0302 18:58:07.364540 22760421793920 run.py:483] Algo bellman_ford step 880 current loss 0.008689, current_train_items 28192.
I0302 18:58:07.380060 22760421793920 run.py:483] Algo bellman_ford step 881 current loss 0.035724, current_train_items 28224.
I0302 18:58:07.402179 22760421793920 run.py:483] Algo bellman_ford step 882 current loss 0.093309, current_train_items 28256.
I0302 18:58:07.431151 22760421793920 run.py:483] Algo bellman_ford step 883 current loss 0.202710, current_train_items 28288.
I0302 18:58:07.462802 22760421793920 run.py:483] Algo bellman_ford step 884 current loss 0.190135, current_train_items 28320.
I0302 18:58:07.480355 22760421793920 run.py:483] Algo bellman_ford step 885 current loss 0.040553, current_train_items 28352.
I0302 18:58:07.495890 22760421793920 run.py:483] Algo bellman_ford step 886 current loss 0.083889, current_train_items 28384.
I0302 18:58:07.518821 22760421793920 run.py:483] Algo bellman_ford step 887 current loss 0.179911, current_train_items 28416.
I0302 18:58:07.546672 22760421793920 run.py:483] Algo bellman_ford step 888 current loss 0.183519, current_train_items 28448.
I0302 18:58:07.578308 22760421793920 run.py:483] Algo bellman_ford step 889 current loss 0.239253, current_train_items 28480.
I0302 18:58:07.596073 22760421793920 run.py:483] Algo bellman_ford step 890 current loss 0.038174, current_train_items 28512.
I0302 18:58:07.611811 22760421793920 run.py:483] Algo bellman_ford step 891 current loss 0.099406, current_train_items 28544.
I0302 18:58:07.634823 22760421793920 run.py:483] Algo bellman_ford step 892 current loss 0.260473, current_train_items 28576.
I0302 18:58:07.664541 22760421793920 run.py:483] Algo bellman_ford step 893 current loss 0.276936, current_train_items 28608.
I0302 18:58:07.694877 22760421793920 run.py:483] Algo bellman_ford step 894 current loss 0.190028, current_train_items 28640.
I0302 18:58:07.712404 22760421793920 run.py:483] Algo bellman_ford step 895 current loss 0.048510, current_train_items 28672.
I0302 18:58:07.727906 22760421793920 run.py:483] Algo bellman_ford step 896 current loss 0.036575, current_train_items 28704.
I0302 18:58:07.751137 22760421793920 run.py:483] Algo bellman_ford step 897 current loss 0.151680, current_train_items 28736.
I0302 18:58:07.780353 22760421793920 run.py:483] Algo bellman_ford step 898 current loss 0.184551, current_train_items 28768.
I0302 18:58:07.812762 22760421793920 run.py:483] Algo bellman_ford step 899 current loss 0.238173, current_train_items 28800.
I0302 18:58:07.830458 22760421793920 run.py:483] Algo bellman_ford step 900 current loss 0.021454, current_train_items 28832.
I0302 18:58:07.837893 22760421793920 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:07.838011 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.963, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0302 18:58:07.866055 22760421793920 run.py:483] Algo bellman_ford step 901 current loss 0.104669, current_train_items 28864.
I0302 18:58:07.889302 22760421793920 run.py:483] Algo bellman_ford step 902 current loss 0.168007, current_train_items 28896.
I0302 18:58:07.918800 22760421793920 run.py:483] Algo bellman_ford step 903 current loss 0.149586, current_train_items 28928.
I0302 18:58:07.949853 22760421793920 run.py:483] Algo bellman_ford step 904 current loss 0.192382, current_train_items 28960.
I0302 18:58:07.967673 22760421793920 run.py:483] Algo bellman_ford step 905 current loss 0.019608, current_train_items 28992.
I0302 18:58:07.983729 22760421793920 run.py:483] Algo bellman_ford step 906 current loss 0.105966, current_train_items 29024.
I0302 18:58:08.006430 22760421793920 run.py:483] Algo bellman_ford step 907 current loss 0.132568, current_train_items 29056.
I0302 18:58:08.035708 22760421793920 run.py:483] Algo bellman_ford step 908 current loss 0.231949, current_train_items 29088.
I0302 18:58:08.066631 22760421793920 run.py:483] Algo bellman_ford step 909 current loss 0.217623, current_train_items 29120.
I0302 18:58:08.084088 22760421793920 run.py:483] Algo bellman_ford step 910 current loss 0.024711, current_train_items 29152.
I0302 18:58:08.100462 22760421793920 run.py:483] Algo bellman_ford step 911 current loss 0.090477, current_train_items 29184.
I0302 18:58:08.123624 22760421793920 run.py:483] Algo bellman_ford step 912 current loss 0.171039, current_train_items 29216.
I0302 18:58:08.150470 22760421793920 run.py:483] Algo bellman_ford step 913 current loss 0.138655, current_train_items 29248.
I0302 18:58:08.182037 22760421793920 run.py:483] Algo bellman_ford step 914 current loss 0.247406, current_train_items 29280.
I0302 18:58:08.199061 22760421793920 run.py:483] Algo bellman_ford step 915 current loss 0.011664, current_train_items 29312.
I0302 18:58:08.215020 22760421793920 run.py:483] Algo bellman_ford step 916 current loss 0.075819, current_train_items 29344.
I0302 18:58:08.238023 22760421793920 run.py:483] Algo bellman_ford step 917 current loss 0.160209, current_train_items 29376.
I0302 18:58:08.266882 22760421793920 run.py:483] Algo bellman_ford step 918 current loss 0.208774, current_train_items 29408.
I0302 18:58:08.298464 22760421793920 run.py:483] Algo bellman_ford step 919 current loss 0.234426, current_train_items 29440.
I0302 18:58:08.316360 22760421793920 run.py:483] Algo bellman_ford step 920 current loss 0.023430, current_train_items 29472.
I0302 18:58:08.332439 22760421793920 run.py:483] Algo bellman_ford step 921 current loss 0.151950, current_train_items 29504.
I0302 18:58:08.355302 22760421793920 run.py:483] Algo bellman_ford step 922 current loss 0.122973, current_train_items 29536.
I0302 18:58:08.383059 22760421793920 run.py:483] Algo bellman_ford step 923 current loss 0.122425, current_train_items 29568.
I0302 18:58:08.413359 22760421793920 run.py:483] Algo bellman_ford step 924 current loss 0.194923, current_train_items 29600.
I0302 18:58:08.430604 22760421793920 run.py:483] Algo bellman_ford step 925 current loss 0.016704, current_train_items 29632.
I0302 18:58:08.446037 22760421793920 run.py:483] Algo bellman_ford step 926 current loss 0.051655, current_train_items 29664.
I0302 18:58:08.468906 22760421793920 run.py:483] Algo bellman_ford step 927 current loss 0.108732, current_train_items 29696.
I0302 18:58:08.498011 22760421793920 run.py:483] Algo bellman_ford step 928 current loss 0.151675, current_train_items 29728.
I0302 18:58:08.527398 22760421793920 run.py:483] Algo bellman_ford step 929 current loss 0.178911, current_train_items 29760.
I0302 18:58:08.544780 22760421793920 run.py:483] Algo bellman_ford step 930 current loss 0.014681, current_train_items 29792.
I0302 18:58:08.560476 22760421793920 run.py:483] Algo bellman_ford step 931 current loss 0.059655, current_train_items 29824.
I0302 18:58:08.583111 22760421793920 run.py:483] Algo bellman_ford step 932 current loss 0.143400, current_train_items 29856.
I0302 18:58:08.610645 22760421793920 run.py:483] Algo bellman_ford step 933 current loss 0.130547, current_train_items 29888.
I0302 18:58:08.641668 22760421793920 run.py:483] Algo bellman_ford step 934 current loss 0.199384, current_train_items 29920.
I0302 18:58:08.658833 22760421793920 run.py:483] Algo bellman_ford step 935 current loss 0.012730, current_train_items 29952.
I0302 18:58:08.674501 22760421793920 run.py:483] Algo bellman_ford step 936 current loss 0.055049, current_train_items 29984.
I0302 18:58:08.696407 22760421793920 run.py:483] Algo bellman_ford step 937 current loss 0.117373, current_train_items 30016.
I0302 18:58:08.724153 22760421793920 run.py:483] Algo bellman_ford step 938 current loss 0.134339, current_train_items 30048.
I0302 18:58:08.754693 22760421793920 run.py:483] Algo bellman_ford step 939 current loss 0.226107, current_train_items 30080.
I0302 18:58:08.771949 22760421793920 run.py:483] Algo bellman_ford step 940 current loss 0.038804, current_train_items 30112.
I0302 18:58:08.787761 22760421793920 run.py:483] Algo bellman_ford step 941 current loss 0.108136, current_train_items 30144.
I0302 18:58:08.810765 22760421793920 run.py:483] Algo bellman_ford step 942 current loss 0.194913, current_train_items 30176.
I0302 18:58:08.839529 22760421793920 run.py:483] Algo bellman_ford step 943 current loss 0.136483, current_train_items 30208.
I0302 18:58:08.870945 22760421793920 run.py:483] Algo bellman_ford step 944 current loss 0.220069, current_train_items 30240.
I0302 18:58:08.888304 22760421793920 run.py:483] Algo bellman_ford step 945 current loss 0.049380, current_train_items 30272.
I0302 18:58:08.903576 22760421793920 run.py:483] Algo bellman_ford step 946 current loss 0.127731, current_train_items 30304.
I0302 18:58:08.927461 22760421793920 run.py:483] Algo bellman_ford step 947 current loss 0.220923, current_train_items 30336.
I0302 18:58:08.955252 22760421793920 run.py:483] Algo bellman_ford step 948 current loss 0.202283, current_train_items 30368.
I0302 18:58:08.985285 22760421793920 run.py:483] Algo bellman_ford step 949 current loss 0.223501, current_train_items 30400.
I0302 18:58:09.002941 22760421793920 run.py:483] Algo bellman_ford step 950 current loss 0.061646, current_train_items 30432.
I0302 18:58:09.010443 22760421793920 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:09.010551 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.970, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:58:09.039134 22760421793920 run.py:483] Algo bellman_ford step 951 current loss 0.117691, current_train_items 30464.
I0302 18:58:09.062206 22760421793920 run.py:483] Algo bellman_ford step 952 current loss 0.149164, current_train_items 30496.
I0302 18:58:09.091220 22760421793920 run.py:483] Algo bellman_ford step 953 current loss 0.199069, current_train_items 30528.
I0302 18:58:09.119804 22760421793920 run.py:483] Algo bellman_ford step 954 current loss 0.197131, current_train_items 30560.
I0302 18:58:09.137826 22760421793920 run.py:483] Algo bellman_ford step 955 current loss 0.034470, current_train_items 30592.
I0302 18:58:09.154055 22760421793920 run.py:483] Algo bellman_ford step 956 current loss 0.062262, current_train_items 30624.
I0302 18:58:09.177022 22760421793920 run.py:483] Algo bellman_ford step 957 current loss 0.122567, current_train_items 30656.
I0302 18:58:09.204320 22760421793920 run.py:483] Algo bellman_ford step 958 current loss 0.187734, current_train_items 30688.
I0302 18:58:09.235414 22760421793920 run.py:483] Algo bellman_ford step 959 current loss 0.226167, current_train_items 30720.
I0302 18:58:09.253218 22760421793920 run.py:483] Algo bellman_ford step 960 current loss 0.045770, current_train_items 30752.
I0302 18:58:09.269428 22760421793920 run.py:483] Algo bellman_ford step 961 current loss 0.163250, current_train_items 30784.
I0302 18:58:09.291172 22760421793920 run.py:483] Algo bellman_ford step 962 current loss 0.161979, current_train_items 30816.
I0302 18:58:09.318110 22760421793920 run.py:483] Algo bellman_ford step 963 current loss 0.162929, current_train_items 30848.
I0302 18:58:09.349055 22760421793920 run.py:483] Algo bellman_ford step 964 current loss 0.269358, current_train_items 30880.
I0302 18:58:09.366699 22760421793920 run.py:483] Algo bellman_ford step 965 current loss 0.020674, current_train_items 30912.
I0302 18:58:09.382217 22760421793920 run.py:483] Algo bellman_ford step 966 current loss 0.051173, current_train_items 30944.
I0302 18:58:09.405374 22760421793920 run.py:483] Algo bellman_ford step 967 current loss 0.164328, current_train_items 30976.
I0302 18:58:09.434623 22760421793920 run.py:483] Algo bellman_ford step 968 current loss 0.158674, current_train_items 31008.
I0302 18:58:09.464950 22760421793920 run.py:483] Algo bellman_ford step 969 current loss 0.207888, current_train_items 31040.
I0302 18:58:09.482482 22760421793920 run.py:483] Algo bellman_ford step 970 current loss 0.018905, current_train_items 31072.
I0302 18:58:09.498409 22760421793920 run.py:483] Algo bellman_ford step 971 current loss 0.032481, current_train_items 31104.
I0302 18:58:09.521678 22760421793920 run.py:483] Algo bellman_ford step 972 current loss 0.104828, current_train_items 31136.
I0302 18:58:09.550953 22760421793920 run.py:483] Algo bellman_ford step 973 current loss 0.108521, current_train_items 31168.
I0302 18:58:09.583379 22760421793920 run.py:483] Algo bellman_ford step 974 current loss 0.164263, current_train_items 31200.
I0302 18:58:09.601044 22760421793920 run.py:483] Algo bellman_ford step 975 current loss 0.022913, current_train_items 31232.
I0302 18:58:09.616491 22760421793920 run.py:483] Algo bellman_ford step 976 current loss 0.018193, current_train_items 31264.
I0302 18:58:09.639383 22760421793920 run.py:483] Algo bellman_ford step 977 current loss 0.087474, current_train_items 31296.
I0302 18:58:09.667631 22760421793920 run.py:483] Algo bellman_ford step 978 current loss 0.198033, current_train_items 31328.
I0302 18:58:09.697833 22760421793920 run.py:483] Algo bellman_ford step 979 current loss 0.209520, current_train_items 31360.
I0302 18:58:09.715107 22760421793920 run.py:483] Algo bellman_ford step 980 current loss 0.033236, current_train_items 31392.
I0302 18:58:09.730996 22760421793920 run.py:483] Algo bellman_ford step 981 current loss 0.065987, current_train_items 31424.
I0302 18:58:09.753434 22760421793920 run.py:483] Algo bellman_ford step 982 current loss 0.164207, current_train_items 31456.
I0302 18:58:09.781379 22760421793920 run.py:483] Algo bellman_ford step 983 current loss 0.172259, current_train_items 31488.
I0302 18:58:09.812610 22760421793920 run.py:483] Algo bellman_ford step 984 current loss 0.187948, current_train_items 31520.
I0302 18:58:09.830311 22760421793920 run.py:483] Algo bellman_ford step 985 current loss 0.011842, current_train_items 31552.
I0302 18:58:09.846390 22760421793920 run.py:483] Algo bellman_ford step 986 current loss 0.077782, current_train_items 31584.
I0302 18:58:09.868697 22760421793920 run.py:483] Algo bellman_ford step 987 current loss 0.073890, current_train_items 31616.
I0302 18:58:09.897332 22760421793920 run.py:483] Algo bellman_ford step 988 current loss 0.132549, current_train_items 31648.
I0302 18:58:09.927211 22760421793920 run.py:483] Algo bellman_ford step 989 current loss 0.167741, current_train_items 31680.
I0302 18:58:09.944565 22760421793920 run.py:483] Algo bellman_ford step 990 current loss 0.017014, current_train_items 31712.
I0302 18:58:09.959939 22760421793920 run.py:483] Algo bellman_ford step 991 current loss 0.038001, current_train_items 31744.
I0302 18:58:09.983218 22760421793920 run.py:483] Algo bellman_ford step 992 current loss 0.153596, current_train_items 31776.
I0302 18:58:10.010440 22760421793920 run.py:483] Algo bellman_ford step 993 current loss 0.157178, current_train_items 31808.
I0302 18:58:10.043247 22760421793920 run.py:483] Algo bellman_ford step 994 current loss 0.262482, current_train_items 31840.
I0302 18:58:10.060715 22760421793920 run.py:483] Algo bellman_ford step 995 current loss 0.025012, current_train_items 31872.
I0302 18:58:10.076234 22760421793920 run.py:483] Algo bellman_ford step 996 current loss 0.036285, current_train_items 31904.
I0302 18:58:10.098922 22760421793920 run.py:483] Algo bellman_ford step 997 current loss 0.168769, current_train_items 31936.
I0302 18:58:10.128042 22760421793920 run.py:483] Algo bellman_ford step 998 current loss 0.184681, current_train_items 31968.
I0302 18:58:10.158329 22760421793920 run.py:483] Algo bellman_ford step 999 current loss 0.238583, current_train_items 32000.
I0302 18:58:10.175983 22760421793920 run.py:483] Algo bellman_ford step 1000 current loss 0.025686, current_train_items 32032.
I0302 18:58:10.183618 22760421793920 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:10.183728 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.972, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0302 18:58:10.200067 22760421793920 run.py:483] Algo bellman_ford step 1001 current loss 0.088984, current_train_items 32064.
I0302 18:58:10.223332 22760421793920 run.py:483] Algo bellman_ford step 1002 current loss 0.131892, current_train_items 32096.
I0302 18:58:10.250721 22760421793920 run.py:483] Algo bellman_ford step 1003 current loss 0.200786, current_train_items 32128.
I0302 18:58:10.284274 22760421793920 run.py:483] Algo bellman_ford step 1004 current loss 0.319629, current_train_items 32160.
I0302 18:58:10.302343 22760421793920 run.py:483] Algo bellman_ford step 1005 current loss 0.043303, current_train_items 32192.
I0302 18:58:10.317835 22760421793920 run.py:483] Algo bellman_ford step 1006 current loss 0.059162, current_train_items 32224.
I0302 18:58:10.341697 22760421793920 run.py:483] Algo bellman_ford step 1007 current loss 0.136752, current_train_items 32256.
I0302 18:58:10.369553 22760421793920 run.py:483] Algo bellman_ford step 1008 current loss 0.096932, current_train_items 32288.
I0302 18:58:10.401250 22760421793920 run.py:483] Algo bellman_ford step 1009 current loss 0.173357, current_train_items 32320.
I0302 18:58:10.418784 22760421793920 run.py:483] Algo bellman_ford step 1010 current loss 0.011447, current_train_items 32352.
I0302 18:58:10.434641 22760421793920 run.py:483] Algo bellman_ford step 1011 current loss 0.062773, current_train_items 32384.
I0302 18:58:10.458642 22760421793920 run.py:483] Algo bellman_ford step 1012 current loss 0.150206, current_train_items 32416.
I0302 18:58:10.487483 22760421793920 run.py:483] Algo bellman_ford step 1013 current loss 0.175339, current_train_items 32448.
I0302 18:58:10.518650 22760421793920 run.py:483] Algo bellman_ford step 1014 current loss 0.172229, current_train_items 32480.
I0302 18:58:10.536324 22760421793920 run.py:483] Algo bellman_ford step 1015 current loss 0.007198, current_train_items 32512.
I0302 18:58:10.552277 22760421793920 run.py:483] Algo bellman_ford step 1016 current loss 0.074400, current_train_items 32544.
I0302 18:58:10.575381 22760421793920 run.py:483] Algo bellman_ford step 1017 current loss 0.293214, current_train_items 32576.
I0302 18:58:10.604480 22760421793920 run.py:483] Algo bellman_ford step 1018 current loss 0.160719, current_train_items 32608.
I0302 18:58:10.637905 22760421793920 run.py:483] Algo bellman_ford step 1019 current loss 0.271614, current_train_items 32640.
I0302 18:58:10.655239 22760421793920 run.py:483] Algo bellman_ford step 1020 current loss 0.036486, current_train_items 32672.
I0302 18:58:10.670832 22760421793920 run.py:483] Algo bellman_ford step 1021 current loss 0.067847, current_train_items 32704.
I0302 18:58:10.693415 22760421793920 run.py:483] Algo bellman_ford step 1022 current loss 0.125202, current_train_items 32736.
I0302 18:58:10.721211 22760421793920 run.py:483] Algo bellman_ford step 1023 current loss 0.194940, current_train_items 32768.
I0302 18:58:10.750374 22760421793920 run.py:483] Algo bellman_ford step 1024 current loss 0.360612, current_train_items 32800.
I0302 18:58:10.768116 22760421793920 run.py:483] Algo bellman_ford step 1025 current loss 0.048895, current_train_items 32832.
I0302 18:58:10.784404 22760421793920 run.py:483] Algo bellman_ford step 1026 current loss 0.077273, current_train_items 32864.
I0302 18:58:10.807316 22760421793920 run.py:483] Algo bellman_ford step 1027 current loss 0.118434, current_train_items 32896.
I0302 18:58:10.836596 22760421793920 run.py:483] Algo bellman_ford step 1028 current loss 0.187600, current_train_items 32928.
I0302 18:58:10.866798 22760421793920 run.py:483] Algo bellman_ford step 1029 current loss 0.253278, current_train_items 32960.
I0302 18:58:10.884344 22760421793920 run.py:483] Algo bellman_ford step 1030 current loss 0.010882, current_train_items 32992.
I0302 18:58:10.899667 22760421793920 run.py:483] Algo bellman_ford step 1031 current loss 0.027820, current_train_items 33024.
I0302 18:58:10.921944 22760421793920 run.py:483] Algo bellman_ford step 1032 current loss 0.107845, current_train_items 33056.
I0302 18:58:10.949838 22760421793920 run.py:483] Algo bellman_ford step 1033 current loss 0.131797, current_train_items 33088.
I0302 18:58:10.981408 22760421793920 run.py:483] Algo bellman_ford step 1034 current loss 0.220491, current_train_items 33120.
I0302 18:58:10.998760 22760421793920 run.py:483] Algo bellman_ford step 1035 current loss 0.030133, current_train_items 33152.
I0302 18:58:11.014447 22760421793920 run.py:483] Algo bellman_ford step 1036 current loss 0.038426, current_train_items 33184.
I0302 18:58:11.037747 22760421793920 run.py:483] Algo bellman_ford step 1037 current loss 0.122493, current_train_items 33216.
I0302 18:58:11.066820 22760421793920 run.py:483] Algo bellman_ford step 1038 current loss 0.246342, current_train_items 33248.
I0302 18:58:11.099388 22760421793920 run.py:483] Algo bellman_ford step 1039 current loss 0.320969, current_train_items 33280.
I0302 18:58:11.116846 22760421793920 run.py:483] Algo bellman_ford step 1040 current loss 0.012489, current_train_items 33312.
I0302 18:58:11.133243 22760421793920 run.py:483] Algo bellman_ford step 1041 current loss 0.102070, current_train_items 33344.
I0302 18:58:11.156567 22760421793920 run.py:483] Algo bellman_ford step 1042 current loss 0.165257, current_train_items 33376.
I0302 18:58:11.184927 22760421793920 run.py:483] Algo bellman_ford step 1043 current loss 0.161808, current_train_items 33408.
I0302 18:58:11.215311 22760421793920 run.py:483] Algo bellman_ford step 1044 current loss 0.169077, current_train_items 33440.
I0302 18:58:11.232911 22760421793920 run.py:483] Algo bellman_ford step 1045 current loss 0.005590, current_train_items 33472.
I0302 18:58:11.248302 22760421793920 run.py:483] Algo bellman_ford step 1046 current loss 0.035337, current_train_items 33504.
I0302 18:58:11.271783 22760421793920 run.py:483] Algo bellman_ford step 1047 current loss 0.149088, current_train_items 33536.
I0302 18:58:11.301228 22760421793920 run.py:483] Algo bellman_ford step 1048 current loss 0.149362, current_train_items 33568.
I0302 18:58:11.332849 22760421793920 run.py:483] Algo bellman_ford step 1049 current loss 0.178463, current_train_items 33600.
I0302 18:58:11.350649 22760421793920 run.py:483] Algo bellman_ford step 1050 current loss 0.061942, current_train_items 33632.
I0302 18:58:11.358301 22760421793920 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:11.358414 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.972, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0302 18:58:11.374486 22760421793920 run.py:483] Algo bellman_ford step 1051 current loss 0.061466, current_train_items 33664.
I0302 18:58:11.398083 22760421793920 run.py:483] Algo bellman_ford step 1052 current loss 0.196807, current_train_items 33696.
I0302 18:58:11.425870 22760421793920 run.py:483] Algo bellman_ford step 1053 current loss 0.192798, current_train_items 33728.
I0302 18:58:11.457122 22760421793920 run.py:483] Algo bellman_ford step 1054 current loss 0.269638, current_train_items 33760.
I0302 18:58:11.475329 22760421793920 run.py:483] Algo bellman_ford step 1055 current loss 0.022377, current_train_items 33792.
I0302 18:58:11.491635 22760421793920 run.py:483] Algo bellman_ford step 1056 current loss 0.064981, current_train_items 33824.
I0302 18:58:11.514564 22760421793920 run.py:483] Algo bellman_ford step 1057 current loss 0.128021, current_train_items 33856.
I0302 18:58:11.544235 22760421793920 run.py:483] Algo bellman_ford step 1058 current loss 0.218716, current_train_items 33888.
I0302 18:58:11.572879 22760421793920 run.py:483] Algo bellman_ford step 1059 current loss 0.189365, current_train_items 33920.
I0302 18:58:11.590391 22760421793920 run.py:483] Algo bellman_ford step 1060 current loss 0.012521, current_train_items 33952.
I0302 18:58:11.606088 22760421793920 run.py:483] Algo bellman_ford step 1061 current loss 0.061450, current_train_items 33984.
I0302 18:58:11.627383 22760421793920 run.py:483] Algo bellman_ford step 1062 current loss 0.102929, current_train_items 34016.
I0302 18:58:11.655368 22760421793920 run.py:483] Algo bellman_ford step 1063 current loss 0.215311, current_train_items 34048.
I0302 18:58:11.686758 22760421793920 run.py:483] Algo bellman_ford step 1064 current loss 0.222564, current_train_items 34080.
I0302 18:58:11.704131 22760421793920 run.py:483] Algo bellman_ford step 1065 current loss 0.015577, current_train_items 34112.
I0302 18:58:11.719805 22760421793920 run.py:483] Algo bellman_ford step 1066 current loss 0.158282, current_train_items 34144.
I0302 18:58:11.742724 22760421793920 run.py:483] Algo bellman_ford step 1067 current loss 0.190953, current_train_items 34176.
I0302 18:58:11.771638 22760421793920 run.py:483] Algo bellman_ford step 1068 current loss 0.192274, current_train_items 34208.
I0302 18:58:11.803760 22760421793920 run.py:483] Algo bellman_ford step 1069 current loss 0.393424, current_train_items 34240.
I0302 18:58:11.821053 22760421793920 run.py:483] Algo bellman_ford step 1070 current loss 0.015055, current_train_items 34272.
I0302 18:58:11.837197 22760421793920 run.py:483] Algo bellman_ford step 1071 current loss 0.113084, current_train_items 34304.
I0302 18:58:11.860036 22760421793920 run.py:483] Algo bellman_ford step 1072 current loss 0.126662, current_train_items 34336.
I0302 18:58:11.888662 22760421793920 run.py:483] Algo bellman_ford step 1073 current loss 0.127257, current_train_items 34368.
I0302 18:58:11.918425 22760421793920 run.py:483] Algo bellman_ford step 1074 current loss 0.144975, current_train_items 34400.
I0302 18:58:11.935932 22760421793920 run.py:483] Algo bellman_ford step 1075 current loss 0.029686, current_train_items 34432.
I0302 18:58:11.951867 22760421793920 run.py:483] Algo bellman_ford step 1076 current loss 0.198430, current_train_items 34464.
I0302 18:58:11.975128 22760421793920 run.py:483] Algo bellman_ford step 1077 current loss 0.162445, current_train_items 34496.
I0302 18:58:12.004216 22760421793920 run.py:483] Algo bellman_ford step 1078 current loss 0.198956, current_train_items 34528.
I0302 18:58:12.034438 22760421793920 run.py:483] Algo bellman_ford step 1079 current loss 0.213645, current_train_items 34560.
I0302 18:58:12.051830 22760421793920 run.py:483] Algo bellman_ford step 1080 current loss 0.020102, current_train_items 34592.
I0302 18:58:12.067937 22760421793920 run.py:483] Algo bellman_ford step 1081 current loss 0.089311, current_train_items 34624.
I0302 18:58:12.090274 22760421793920 run.py:483] Algo bellman_ford step 1082 current loss 0.176871, current_train_items 34656.
I0302 18:58:12.118324 22760421793920 run.py:483] Algo bellman_ford step 1083 current loss 0.138863, current_train_items 34688.
I0302 18:58:12.149838 22760421793920 run.py:483] Algo bellman_ford step 1084 current loss 0.202108, current_train_items 34720.
I0302 18:58:12.167561 22760421793920 run.py:483] Algo bellman_ford step 1085 current loss 0.022541, current_train_items 34752.
I0302 18:58:12.183457 22760421793920 run.py:483] Algo bellman_ford step 1086 current loss 0.049796, current_train_items 34784.
I0302 18:58:12.206573 22760421793920 run.py:483] Algo bellman_ford step 1087 current loss 0.124127, current_train_items 34816.
I0302 18:58:12.234052 22760421793920 run.py:483] Algo bellman_ford step 1088 current loss 0.184561, current_train_items 34848.
I0302 18:58:12.263382 22760421793920 run.py:483] Algo bellman_ford step 1089 current loss 0.134420, current_train_items 34880.
I0302 18:58:12.280994 22760421793920 run.py:483] Algo bellman_ford step 1090 current loss 0.065046, current_train_items 34912.
I0302 18:58:12.297136 22760421793920 run.py:483] Algo bellman_ford step 1091 current loss 0.188962, current_train_items 34944.
I0302 18:58:12.319824 22760421793920 run.py:483] Algo bellman_ford step 1092 current loss 0.202319, current_train_items 34976.
I0302 18:58:12.349257 22760421793920 run.py:483] Algo bellman_ford step 1093 current loss 0.304617, current_train_items 35008.
I0302 18:58:12.381146 22760421793920 run.py:483] Algo bellman_ford step 1094 current loss 0.235821, current_train_items 35040.
I0302 18:58:12.398367 22760421793920 run.py:483] Algo bellman_ford step 1095 current loss 0.032769, current_train_items 35072.
I0302 18:58:12.414076 22760421793920 run.py:483] Algo bellman_ford step 1096 current loss 0.131190, current_train_items 35104.
I0302 18:58:12.436779 22760421793920 run.py:483] Algo bellman_ford step 1097 current loss 0.295004, current_train_items 35136.
I0302 18:58:12.465968 22760421793920 run.py:483] Algo bellman_ford step 1098 current loss 0.310260, current_train_items 35168.
I0302 18:58:12.496163 22760421793920 run.py:483] Algo bellman_ford step 1099 current loss 0.387367, current_train_items 35200.
I0302 18:58:12.513576 22760421793920 run.py:483] Algo bellman_ford step 1100 current loss 0.012979, current_train_items 35232.
I0302 18:58:12.521126 22760421793920 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:12.521235 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.972, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:58:12.549493 22760421793920 run.py:483] Algo bellman_ford step 1101 current loss 0.084042, current_train_items 35264.
I0302 18:58:12.571862 22760421793920 run.py:483] Algo bellman_ford step 1102 current loss 0.087073, current_train_items 35296.
I0302 18:58:12.601817 22760421793920 run.py:483] Algo bellman_ford step 1103 current loss 0.183097, current_train_items 35328.
I0302 18:58:12.634874 22760421793920 run.py:483] Algo bellman_ford step 1104 current loss 0.239671, current_train_items 35360.
I0302 18:58:12.652746 22760421793920 run.py:483] Algo bellman_ford step 1105 current loss 0.012633, current_train_items 35392.
I0302 18:58:12.668386 22760421793920 run.py:483] Algo bellman_ford step 1106 current loss 0.056618, current_train_items 35424.
I0302 18:58:12.691430 22760421793920 run.py:483] Algo bellman_ford step 1107 current loss 0.231583, current_train_items 35456.
I0302 18:58:12.719462 22760421793920 run.py:483] Algo bellman_ford step 1108 current loss 0.183494, current_train_items 35488.
I0302 18:58:12.748518 22760421793920 run.py:483] Algo bellman_ford step 1109 current loss 0.175880, current_train_items 35520.
I0302 18:58:12.766187 22760421793920 run.py:483] Algo bellman_ford step 1110 current loss 0.040875, current_train_items 35552.
I0302 18:58:12.782082 22760421793920 run.py:483] Algo bellman_ford step 1111 current loss 0.073216, current_train_items 35584.
I0302 18:58:12.805112 22760421793920 run.py:483] Algo bellman_ford step 1112 current loss 0.125208, current_train_items 35616.
I0302 18:58:12.832178 22760421793920 run.py:483] Algo bellman_ford step 1113 current loss 0.157302, current_train_items 35648.
I0302 18:58:12.864343 22760421793920 run.py:483] Algo bellman_ford step 1114 current loss 0.226196, current_train_items 35680.
I0302 18:58:12.881946 22760421793920 run.py:483] Algo bellman_ford step 1115 current loss 0.012789, current_train_items 35712.
I0302 18:58:12.897439 22760421793920 run.py:483] Algo bellman_ford step 1116 current loss 0.082677, current_train_items 35744.
I0302 18:58:12.920994 22760421793920 run.py:483] Algo bellman_ford step 1117 current loss 0.121682, current_train_items 35776.
I0302 18:58:12.950045 22760421793920 run.py:483] Algo bellman_ford step 1118 current loss 0.110211, current_train_items 35808.
I0302 18:58:12.981209 22760421793920 run.py:483] Algo bellman_ford step 1119 current loss 0.198178, current_train_items 35840.
I0302 18:58:12.998937 22760421793920 run.py:483] Algo bellman_ford step 1120 current loss 0.027313, current_train_items 35872.
I0302 18:58:13.014220 22760421793920 run.py:483] Algo bellman_ford step 1121 current loss 0.062012, current_train_items 35904.
I0302 18:58:13.037458 22760421793920 run.py:483] Algo bellman_ford step 1122 current loss 0.155382, current_train_items 35936.
I0302 18:58:13.064774 22760421793920 run.py:483] Algo bellman_ford step 1123 current loss 0.147212, current_train_items 35968.
I0302 18:58:13.097215 22760421793920 run.py:483] Algo bellman_ford step 1124 current loss 0.186095, current_train_items 36000.
I0302 18:58:13.114682 22760421793920 run.py:483] Algo bellman_ford step 1125 current loss 0.006406, current_train_items 36032.
I0302 18:58:13.130180 22760421793920 run.py:483] Algo bellman_ford step 1126 current loss 0.050851, current_train_items 36064.
I0302 18:58:13.153005 22760421793920 run.py:483] Algo bellman_ford step 1127 current loss 0.093733, current_train_items 36096.
I0302 18:58:13.181570 22760421793920 run.py:483] Algo bellman_ford step 1128 current loss 0.190657, current_train_items 36128.
I0302 18:58:13.209985 22760421793920 run.py:483] Algo bellman_ford step 1129 current loss 0.153738, current_train_items 36160.
I0302 18:58:13.227714 22760421793920 run.py:483] Algo bellman_ford step 1130 current loss 0.008912, current_train_items 36192.
I0302 18:58:13.243431 22760421793920 run.py:483] Algo bellman_ford step 1131 current loss 0.061045, current_train_items 36224.
I0302 18:58:13.266429 22760421793920 run.py:483] Algo bellman_ford step 1132 current loss 0.131836, current_train_items 36256.
I0302 18:58:13.295176 22760421793920 run.py:483] Algo bellman_ford step 1133 current loss 0.157297, current_train_items 36288.
I0302 18:58:13.325149 22760421793920 run.py:483] Algo bellman_ford step 1134 current loss 0.151370, current_train_items 36320.
I0302 18:58:13.342671 22760421793920 run.py:483] Algo bellman_ford step 1135 current loss 0.022649, current_train_items 36352.
I0302 18:58:13.358364 22760421793920 run.py:483] Algo bellman_ford step 1136 current loss 0.124509, current_train_items 36384.
I0302 18:58:13.380599 22760421793920 run.py:483] Algo bellman_ford step 1137 current loss 0.146757, current_train_items 36416.
I0302 18:58:13.410321 22760421793920 run.py:483] Algo bellman_ford step 1138 current loss 0.153938, current_train_items 36448.
I0302 18:58:13.440789 22760421793920 run.py:483] Algo bellman_ford step 1139 current loss 0.216822, current_train_items 36480.
I0302 18:58:13.458432 22760421793920 run.py:483] Algo bellman_ford step 1140 current loss 0.064590, current_train_items 36512.
I0302 18:58:13.474385 22760421793920 run.py:483] Algo bellman_ford step 1141 current loss 0.069264, current_train_items 36544.
I0302 18:58:13.496595 22760421793920 run.py:483] Algo bellman_ford step 1142 current loss 0.143030, current_train_items 36576.
I0302 18:58:13.524157 22760421793920 run.py:483] Algo bellman_ford step 1143 current loss 0.125350, current_train_items 36608.
I0302 18:58:13.554432 22760421793920 run.py:483] Algo bellman_ford step 1144 current loss 0.111758, current_train_items 36640.
I0302 18:58:13.572480 22760421793920 run.py:483] Algo bellman_ford step 1145 current loss 0.059253, current_train_items 36672.
I0302 18:58:13.588358 22760421793920 run.py:483] Algo bellman_ford step 1146 current loss 0.072872, current_train_items 36704.
I0302 18:58:13.611333 22760421793920 run.py:483] Algo bellman_ford step 1147 current loss 0.109196, current_train_items 36736.
I0302 18:58:13.640075 22760421793920 run.py:483] Algo bellman_ford step 1148 current loss 0.126437, current_train_items 36768.
I0302 18:58:13.668555 22760421793920 run.py:483] Algo bellman_ford step 1149 current loss 0.127927, current_train_items 36800.
I0302 18:58:13.685976 22760421793920 run.py:483] Algo bellman_ford step 1150 current loss 0.091698, current_train_items 36832.
I0302 18:58:13.693586 22760421793920 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:13.693697 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.973, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:58:13.721818 22760421793920 run.py:483] Algo bellman_ford step 1151 current loss 0.028199, current_train_items 36864.
I0302 18:58:13.745427 22760421793920 run.py:483] Algo bellman_ford step 1152 current loss 0.139308, current_train_items 36896.
I0302 18:58:13.775733 22760421793920 run.py:483] Algo bellman_ford step 1153 current loss 0.181758, current_train_items 36928.
W0302 18:58:13.796283 22760421793920 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:19.650152 22760421793920 run.py:483] Algo bellman_ford step 1154 current loss 0.163452, current_train_items 36960.
I0302 18:58:19.669721 22760421793920 run.py:483] Algo bellman_ford step 1155 current loss 0.019072, current_train_items 36992.
I0302 18:58:19.686489 22760421793920 run.py:483] Algo bellman_ford step 1156 current loss 0.059987, current_train_items 37024.
I0302 18:58:19.709308 22760421793920 run.py:483] Algo bellman_ford step 1157 current loss 0.109456, current_train_items 37056.
I0302 18:58:19.737805 22760421793920 run.py:483] Algo bellman_ford step 1158 current loss 0.137514, current_train_items 37088.
I0302 18:58:19.766979 22760421793920 run.py:483] Algo bellman_ford step 1159 current loss 0.175857, current_train_items 37120.
I0302 18:58:19.785333 22760421793920 run.py:483] Algo bellman_ford step 1160 current loss 0.034971, current_train_items 37152.
I0302 18:58:19.801164 22760421793920 run.py:483] Algo bellman_ford step 1161 current loss 0.052954, current_train_items 37184.
I0302 18:58:19.823187 22760421793920 run.py:483] Algo bellman_ford step 1162 current loss 0.076146, current_train_items 37216.
I0302 18:58:19.851668 22760421793920 run.py:483] Algo bellman_ford step 1163 current loss 0.146775, current_train_items 37248.
I0302 18:58:19.882917 22760421793920 run.py:483] Algo bellman_ford step 1164 current loss 0.178585, current_train_items 37280.
I0302 18:58:19.900484 22760421793920 run.py:483] Algo bellman_ford step 1165 current loss 0.011228, current_train_items 37312.
I0302 18:58:19.916296 22760421793920 run.py:483] Algo bellman_ford step 1166 current loss 0.052126, current_train_items 37344.
I0302 18:58:19.939180 22760421793920 run.py:483] Algo bellman_ford step 1167 current loss 0.179288, current_train_items 37376.
I0302 18:58:19.967924 22760421793920 run.py:483] Algo bellman_ford step 1168 current loss 0.138375, current_train_items 37408.
I0302 18:58:20.000325 22760421793920 run.py:483] Algo bellman_ford step 1169 current loss 0.199729, current_train_items 37440.
I0302 18:58:20.018463 22760421793920 run.py:483] Algo bellman_ford step 1170 current loss 0.018847, current_train_items 37472.
I0302 18:58:20.034624 22760421793920 run.py:483] Algo bellman_ford step 1171 current loss 0.059453, current_train_items 37504.
I0302 18:58:20.057367 22760421793920 run.py:483] Algo bellman_ford step 1172 current loss 0.130085, current_train_items 37536.
I0302 18:58:20.086732 22760421793920 run.py:483] Algo bellman_ford step 1173 current loss 0.163356, current_train_items 37568.
I0302 18:58:20.117720 22760421793920 run.py:483] Algo bellman_ford step 1174 current loss 0.233666, current_train_items 37600.
I0302 18:58:20.135665 22760421793920 run.py:483] Algo bellman_ford step 1175 current loss 0.013163, current_train_items 37632.
I0302 18:58:20.151400 22760421793920 run.py:483] Algo bellman_ford step 1176 current loss 0.033736, current_train_items 37664.
I0302 18:58:20.173766 22760421793920 run.py:483] Algo bellman_ford step 1177 current loss 0.160814, current_train_items 37696.
I0302 18:58:20.202037 22760421793920 run.py:483] Algo bellman_ford step 1178 current loss 0.176563, current_train_items 37728.
I0302 18:58:20.234952 22760421793920 run.py:483] Algo bellman_ford step 1179 current loss 0.340710, current_train_items 37760.
I0302 18:58:20.252883 22760421793920 run.py:483] Algo bellman_ford step 1180 current loss 0.019250, current_train_items 37792.
I0302 18:58:20.269003 22760421793920 run.py:483] Algo bellman_ford step 1181 current loss 0.083866, current_train_items 37824.
I0302 18:58:20.291697 22760421793920 run.py:483] Algo bellman_ford step 1182 current loss 0.108419, current_train_items 37856.
I0302 18:58:20.319555 22760421793920 run.py:483] Algo bellman_ford step 1183 current loss 0.208066, current_train_items 37888.
I0302 18:58:20.350806 22760421793920 run.py:483] Algo bellman_ford step 1184 current loss 0.214995, current_train_items 37920.
I0302 18:58:20.368630 22760421793920 run.py:483] Algo bellman_ford step 1185 current loss 0.059735, current_train_items 37952.
I0302 18:58:20.384132 22760421793920 run.py:483] Algo bellman_ford step 1186 current loss 0.057840, current_train_items 37984.
I0302 18:58:20.406803 22760421793920 run.py:483] Algo bellman_ford step 1187 current loss 0.151545, current_train_items 38016.
I0302 18:58:20.435288 22760421793920 run.py:483] Algo bellman_ford step 1188 current loss 0.135047, current_train_items 38048.
I0302 18:58:20.464889 22760421793920 run.py:483] Algo bellman_ford step 1189 current loss 0.204191, current_train_items 38080.
I0302 18:58:20.482625 22760421793920 run.py:483] Algo bellman_ford step 1190 current loss 0.028941, current_train_items 38112.
I0302 18:58:20.498603 22760421793920 run.py:483] Algo bellman_ford step 1191 current loss 0.105941, current_train_items 38144.
I0302 18:58:20.522445 22760421793920 run.py:483] Algo bellman_ford step 1192 current loss 0.166762, current_train_items 38176.
I0302 18:58:20.550857 22760421793920 run.py:483] Algo bellman_ford step 1193 current loss 0.155835, current_train_items 38208.
I0302 18:58:20.581863 22760421793920 run.py:483] Algo bellman_ford step 1194 current loss 0.178046, current_train_items 38240.
I0302 18:58:20.599855 22760421793920 run.py:483] Algo bellman_ford step 1195 current loss 0.032979, current_train_items 38272.
I0302 18:58:20.615723 22760421793920 run.py:483] Algo bellman_ford step 1196 current loss 0.081601, current_train_items 38304.
I0302 18:58:20.637821 22760421793920 run.py:483] Algo bellman_ford step 1197 current loss 0.165350, current_train_items 38336.
I0302 18:58:20.667609 22760421793920 run.py:483] Algo bellman_ford step 1198 current loss 0.196578, current_train_items 38368.
I0302 18:58:20.699517 22760421793920 run.py:483] Algo bellman_ford step 1199 current loss 0.255799, current_train_items 38400.
I0302 18:58:20.717360 22760421793920 run.py:483] Algo bellman_ford step 1200 current loss 0.039737, current_train_items 38432.
I0302 18:58:20.726472 22760421793920 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:20.726585 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:58:20.743254 22760421793920 run.py:483] Algo bellman_ford step 1201 current loss 0.113350, current_train_items 38464.
I0302 18:58:20.766157 22760421793920 run.py:483] Algo bellman_ford step 1202 current loss 0.128017, current_train_items 38496.
I0302 18:58:20.796040 22760421793920 run.py:483] Algo bellman_ford step 1203 current loss 0.161157, current_train_items 38528.
I0302 18:58:20.828475 22760421793920 run.py:483] Algo bellman_ford step 1204 current loss 0.189583, current_train_items 38560.
I0302 18:58:20.846819 22760421793920 run.py:483] Algo bellman_ford step 1205 current loss 0.045087, current_train_items 38592.
I0302 18:58:20.862533 22760421793920 run.py:483] Algo bellman_ford step 1206 current loss 0.114308, current_train_items 38624.
I0302 18:58:20.884970 22760421793920 run.py:483] Algo bellman_ford step 1207 current loss 0.206471, current_train_items 38656.
I0302 18:58:20.914132 22760421793920 run.py:483] Algo bellman_ford step 1208 current loss 0.334089, current_train_items 38688.
I0302 18:58:20.946488 22760421793920 run.py:483] Algo bellman_ford step 1209 current loss 0.209435, current_train_items 38720.
I0302 18:58:20.964403 22760421793920 run.py:483] Algo bellman_ford step 1210 current loss 0.009091, current_train_items 38752.
I0302 18:58:20.980232 22760421793920 run.py:483] Algo bellman_ford step 1211 current loss 0.094084, current_train_items 38784.
I0302 18:58:21.002176 22760421793920 run.py:483] Algo bellman_ford step 1212 current loss 0.126256, current_train_items 38816.
I0302 18:58:21.029712 22760421793920 run.py:483] Algo bellman_ford step 1213 current loss 0.119553, current_train_items 38848.
I0302 18:58:21.060923 22760421793920 run.py:483] Algo bellman_ford step 1214 current loss 0.161486, current_train_items 38880.
I0302 18:58:21.078734 22760421793920 run.py:483] Algo bellman_ford step 1215 current loss 0.016065, current_train_items 38912.
I0302 18:58:21.094150 22760421793920 run.py:483] Algo bellman_ford step 1216 current loss 0.043387, current_train_items 38944.
I0302 18:58:21.116827 22760421793920 run.py:483] Algo bellman_ford step 1217 current loss 0.121759, current_train_items 38976.
I0302 18:58:21.146277 22760421793920 run.py:483] Algo bellman_ford step 1218 current loss 0.213054, current_train_items 39008.
I0302 18:58:21.177701 22760421793920 run.py:483] Algo bellman_ford step 1219 current loss 0.162547, current_train_items 39040.
I0302 18:58:21.195791 22760421793920 run.py:483] Algo bellman_ford step 1220 current loss 0.015827, current_train_items 39072.
I0302 18:58:21.211352 22760421793920 run.py:483] Algo bellman_ford step 1221 current loss 0.038785, current_train_items 39104.
I0302 18:58:21.235023 22760421793920 run.py:483] Algo bellman_ford step 1222 current loss 0.165378, current_train_items 39136.
I0302 18:58:21.264273 22760421793920 run.py:483] Algo bellman_ford step 1223 current loss 0.162076, current_train_items 39168.
I0302 18:58:21.295160 22760421793920 run.py:483] Algo bellman_ford step 1224 current loss 0.132964, current_train_items 39200.
I0302 18:58:21.312950 22760421793920 run.py:483] Algo bellman_ford step 1225 current loss 0.006189, current_train_items 39232.
I0302 18:58:21.328764 22760421793920 run.py:483] Algo bellman_ford step 1226 current loss 0.053332, current_train_items 39264.
I0302 18:58:21.352494 22760421793920 run.py:483] Algo bellman_ford step 1227 current loss 0.160952, current_train_items 39296.
I0302 18:58:21.380689 22760421793920 run.py:483] Algo bellman_ford step 1228 current loss 0.128562, current_train_items 39328.
I0302 18:58:21.413297 22760421793920 run.py:483] Algo bellman_ford step 1229 current loss 0.179566, current_train_items 39360.
I0302 18:58:21.431276 22760421793920 run.py:483] Algo bellman_ford step 1230 current loss 0.014304, current_train_items 39392.
I0302 18:58:21.447134 22760421793920 run.py:483] Algo bellman_ford step 1231 current loss 0.088014, current_train_items 39424.
I0302 18:58:21.470479 22760421793920 run.py:483] Algo bellman_ford step 1232 current loss 0.191989, current_train_items 39456.
I0302 18:58:21.499068 22760421793920 run.py:483] Algo bellman_ford step 1233 current loss 0.126106, current_train_items 39488.
I0302 18:58:21.528869 22760421793920 run.py:483] Algo bellman_ford step 1234 current loss 0.153736, current_train_items 39520.
I0302 18:58:21.546555 22760421793920 run.py:483] Algo bellman_ford step 1235 current loss 0.035855, current_train_items 39552.
I0302 18:58:21.561935 22760421793920 run.py:483] Algo bellman_ford step 1236 current loss 0.055284, current_train_items 39584.
I0302 18:58:21.584583 22760421793920 run.py:483] Algo bellman_ford step 1237 current loss 0.129508, current_train_items 39616.
I0302 18:58:21.613468 22760421793920 run.py:483] Algo bellman_ford step 1238 current loss 0.112945, current_train_items 39648.
I0302 18:58:21.645470 22760421793920 run.py:483] Algo bellman_ford step 1239 current loss 0.178434, current_train_items 39680.
I0302 18:58:21.663557 22760421793920 run.py:483] Algo bellman_ford step 1240 current loss 0.037966, current_train_items 39712.
I0302 18:58:21.679379 22760421793920 run.py:483] Algo bellman_ford step 1241 current loss 0.056433, current_train_items 39744.
I0302 18:58:21.702624 22760421793920 run.py:483] Algo bellman_ford step 1242 current loss 0.082856, current_train_items 39776.
I0302 18:58:21.731276 22760421793920 run.py:483] Algo bellman_ford step 1243 current loss 0.113566, current_train_items 39808.
I0302 18:58:21.762905 22760421793920 run.py:483] Algo bellman_ford step 1244 current loss 0.199009, current_train_items 39840.
I0302 18:58:21.780683 22760421793920 run.py:483] Algo bellman_ford step 1245 current loss 0.029267, current_train_items 39872.
I0302 18:58:21.796380 22760421793920 run.py:483] Algo bellman_ford step 1246 current loss 0.066301, current_train_items 39904.
I0302 18:58:21.818583 22760421793920 run.py:483] Algo bellman_ford step 1247 current loss 0.090917, current_train_items 39936.
I0302 18:58:21.847805 22760421793920 run.py:483] Algo bellman_ford step 1248 current loss 0.181339, current_train_items 39968.
I0302 18:58:21.878259 22760421793920 run.py:483] Algo bellman_ford step 1249 current loss 0.178289, current_train_items 40000.
I0302 18:58:21.896154 22760421793920 run.py:483] Algo bellman_ford step 1250 current loss 0.033308, current_train_items 40032.
I0302 18:58:21.904064 22760421793920 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:21.904174 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.975, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0302 18:58:21.920560 22760421793920 run.py:483] Algo bellman_ford step 1251 current loss 0.084829, current_train_items 40064.
I0302 18:58:21.943530 22760421793920 run.py:483] Algo bellman_ford step 1252 current loss 0.155097, current_train_items 40096.
I0302 18:58:21.970982 22760421793920 run.py:483] Algo bellman_ford step 1253 current loss 0.184726, current_train_items 40128.
I0302 18:58:22.000144 22760421793920 run.py:483] Algo bellman_ford step 1254 current loss 0.264626, current_train_items 40160.
I0302 18:58:22.018288 22760421793920 run.py:483] Algo bellman_ford step 1255 current loss 0.007220, current_train_items 40192.
I0302 18:58:22.034653 22760421793920 run.py:483] Algo bellman_ford step 1256 current loss 0.133785, current_train_items 40224.
I0302 18:58:22.056883 22760421793920 run.py:483] Algo bellman_ford step 1257 current loss 0.146696, current_train_items 40256.
I0302 18:58:22.085289 22760421793920 run.py:483] Algo bellman_ford step 1258 current loss 0.140492, current_train_items 40288.
I0302 18:58:22.115676 22760421793920 run.py:483] Algo bellman_ford step 1259 current loss 0.158654, current_train_items 40320.
I0302 18:58:22.133581 22760421793920 run.py:483] Algo bellman_ford step 1260 current loss 0.021951, current_train_items 40352.
I0302 18:58:22.149827 22760421793920 run.py:483] Algo bellman_ford step 1261 current loss 0.063474, current_train_items 40384.
I0302 18:58:22.173151 22760421793920 run.py:483] Algo bellman_ford step 1262 current loss 0.135659, current_train_items 40416.
I0302 18:58:22.199537 22760421793920 run.py:483] Algo bellman_ford step 1263 current loss 0.136031, current_train_items 40448.
I0302 18:58:22.232554 22760421793920 run.py:483] Algo bellman_ford step 1264 current loss 0.234561, current_train_items 40480.
I0302 18:58:22.250505 22760421793920 run.py:483] Algo bellman_ford step 1265 current loss 0.019003, current_train_items 40512.
I0302 18:58:22.266302 22760421793920 run.py:483] Algo bellman_ford step 1266 current loss 0.074165, current_train_items 40544.
I0302 18:58:22.288360 22760421793920 run.py:483] Algo bellman_ford step 1267 current loss 0.076110, current_train_items 40576.
I0302 18:58:22.316699 22760421793920 run.py:483] Algo bellman_ford step 1268 current loss 0.098871, current_train_items 40608.
I0302 18:58:22.348265 22760421793920 run.py:483] Algo bellman_ford step 1269 current loss 0.137115, current_train_items 40640.
I0302 18:58:22.366253 22760421793920 run.py:483] Algo bellman_ford step 1270 current loss 0.025425, current_train_items 40672.
I0302 18:58:22.381789 22760421793920 run.py:483] Algo bellman_ford step 1271 current loss 0.050296, current_train_items 40704.
I0302 18:58:22.403684 22760421793920 run.py:483] Algo bellman_ford step 1272 current loss 0.100024, current_train_items 40736.
I0302 18:58:22.431229 22760421793920 run.py:483] Algo bellman_ford step 1273 current loss 0.129679, current_train_items 40768.
I0302 18:58:22.462712 22760421793920 run.py:483] Algo bellman_ford step 1274 current loss 0.209520, current_train_items 40800.
I0302 18:58:22.480443 22760421793920 run.py:483] Algo bellman_ford step 1275 current loss 0.009987, current_train_items 40832.
I0302 18:58:22.496312 22760421793920 run.py:483] Algo bellman_ford step 1276 current loss 0.050694, current_train_items 40864.
I0302 18:58:22.517827 22760421793920 run.py:483] Algo bellman_ford step 1277 current loss 0.086747, current_train_items 40896.
I0302 18:58:22.546321 22760421793920 run.py:483] Algo bellman_ford step 1278 current loss 0.131812, current_train_items 40928.
I0302 18:58:22.576714 22760421793920 run.py:483] Algo bellman_ford step 1279 current loss 0.148673, current_train_items 40960.
I0302 18:58:22.594586 22760421793920 run.py:483] Algo bellman_ford step 1280 current loss 0.010483, current_train_items 40992.
I0302 18:58:22.610195 22760421793920 run.py:483] Algo bellman_ford step 1281 current loss 0.040167, current_train_items 41024.
I0302 18:58:22.633163 22760421793920 run.py:483] Algo bellman_ford step 1282 current loss 0.103116, current_train_items 41056.
I0302 18:58:22.662671 22760421793920 run.py:483] Algo bellman_ford step 1283 current loss 0.166481, current_train_items 41088.
I0302 18:58:22.693181 22760421793920 run.py:483] Algo bellman_ford step 1284 current loss 0.152085, current_train_items 41120.
I0302 18:58:22.711013 22760421793920 run.py:483] Algo bellman_ford step 1285 current loss 0.067049, current_train_items 41152.
I0302 18:58:22.726667 22760421793920 run.py:483] Algo bellman_ford step 1286 current loss 0.024612, current_train_items 41184.
I0302 18:58:22.750584 22760421793920 run.py:483] Algo bellman_ford step 1287 current loss 0.126467, current_train_items 41216.
I0302 18:58:22.779652 22760421793920 run.py:483] Algo bellman_ford step 1288 current loss 0.162528, current_train_items 41248.
I0302 18:58:22.810824 22760421793920 run.py:483] Algo bellman_ford step 1289 current loss 0.168257, current_train_items 41280.
I0302 18:58:22.828938 22760421793920 run.py:483] Algo bellman_ford step 1290 current loss 0.016471, current_train_items 41312.
I0302 18:58:22.844817 22760421793920 run.py:483] Algo bellman_ford step 1291 current loss 0.061736, current_train_items 41344.
I0302 18:58:22.867036 22760421793920 run.py:483] Algo bellman_ford step 1292 current loss 0.095569, current_train_items 41376.
I0302 18:58:22.895401 22760421793920 run.py:483] Algo bellman_ford step 1293 current loss 0.113567, current_train_items 41408.
I0302 18:58:22.927478 22760421793920 run.py:483] Algo bellman_ford step 1294 current loss 0.183423, current_train_items 41440.
I0302 18:58:22.945280 22760421793920 run.py:483] Algo bellman_ford step 1295 current loss 0.019784, current_train_items 41472.
I0302 18:58:22.961124 22760421793920 run.py:483] Algo bellman_ford step 1296 current loss 0.038411, current_train_items 41504.
I0302 18:58:22.984076 22760421793920 run.py:483] Algo bellman_ford step 1297 current loss 0.097156, current_train_items 41536.
I0302 18:58:23.011275 22760421793920 run.py:483] Algo bellman_ford step 1298 current loss 0.099799, current_train_items 41568.
I0302 18:58:23.039994 22760421793920 run.py:483] Algo bellman_ford step 1299 current loss 0.171716, current_train_items 41600.
I0302 18:58:23.057459 22760421793920 run.py:483] Algo bellman_ford step 1300 current loss 0.019217, current_train_items 41632.
I0302 18:58:23.064980 22760421793920 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:23.065090 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.975, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:23.093281 22760421793920 run.py:483] Algo bellman_ford step 1301 current loss 0.040767, current_train_items 41664.
I0302 18:58:23.116272 22760421793920 run.py:483] Algo bellman_ford step 1302 current loss 0.165240, current_train_items 41696.
I0302 18:58:23.145600 22760421793920 run.py:483] Algo bellman_ford step 1303 current loss 0.147505, current_train_items 41728.
I0302 18:58:23.176108 22760421793920 run.py:483] Algo bellman_ford step 1304 current loss 0.124272, current_train_items 41760.
I0302 18:58:23.194498 22760421793920 run.py:483] Algo bellman_ford step 1305 current loss 0.007627, current_train_items 41792.
I0302 18:58:23.210261 22760421793920 run.py:483] Algo bellman_ford step 1306 current loss 0.060271, current_train_items 41824.
I0302 18:58:23.232488 22760421793920 run.py:483] Algo bellman_ford step 1307 current loss 0.091020, current_train_items 41856.
I0302 18:58:23.260688 22760421793920 run.py:483] Algo bellman_ford step 1308 current loss 0.122715, current_train_items 41888.
I0302 18:58:23.291073 22760421793920 run.py:483] Algo bellman_ford step 1309 current loss 0.194081, current_train_items 41920.
I0302 18:58:23.309072 22760421793920 run.py:483] Algo bellman_ford step 1310 current loss 0.031719, current_train_items 41952.
I0302 18:58:23.324736 22760421793920 run.py:483] Algo bellman_ford step 1311 current loss 0.100170, current_train_items 41984.
I0302 18:58:23.346736 22760421793920 run.py:483] Algo bellman_ford step 1312 current loss 0.167390, current_train_items 42016.
I0302 18:58:23.376053 22760421793920 run.py:483] Algo bellman_ford step 1313 current loss 0.193684, current_train_items 42048.
I0302 18:58:23.407587 22760421793920 run.py:483] Algo bellman_ford step 1314 current loss 0.128188, current_train_items 42080.
I0302 18:58:23.424935 22760421793920 run.py:483] Algo bellman_ford step 1315 current loss 0.006322, current_train_items 42112.
I0302 18:58:23.440885 22760421793920 run.py:483] Algo bellman_ford step 1316 current loss 0.085454, current_train_items 42144.
I0302 18:58:23.463465 22760421793920 run.py:483] Algo bellman_ford step 1317 current loss 0.152780, current_train_items 42176.
I0302 18:58:23.491770 22760421793920 run.py:483] Algo bellman_ford step 1318 current loss 0.116306, current_train_items 42208.
I0302 18:58:23.522647 22760421793920 run.py:483] Algo bellman_ford step 1319 current loss 0.247213, current_train_items 42240.
I0302 18:58:23.540276 22760421793920 run.py:483] Algo bellman_ford step 1320 current loss 0.025166, current_train_items 42272.
I0302 18:58:23.556494 22760421793920 run.py:483] Algo bellman_ford step 1321 current loss 0.074995, current_train_items 42304.
I0302 18:58:23.579874 22760421793920 run.py:483] Algo bellman_ford step 1322 current loss 0.258864, current_train_items 42336.
I0302 18:58:23.609105 22760421793920 run.py:483] Algo bellman_ford step 1323 current loss 0.209005, current_train_items 42368.
I0302 18:58:23.639446 22760421793920 run.py:483] Algo bellman_ford step 1324 current loss 0.214858, current_train_items 42400.
I0302 18:58:23.657762 22760421793920 run.py:483] Algo bellman_ford step 1325 current loss 0.092936, current_train_items 42432.
I0302 18:58:23.673759 22760421793920 run.py:483] Algo bellman_ford step 1326 current loss 0.080615, current_train_items 42464.
I0302 18:58:23.696307 22760421793920 run.py:483] Algo bellman_ford step 1327 current loss 0.143259, current_train_items 42496.
I0302 18:58:23.725626 22760421793920 run.py:483] Algo bellman_ford step 1328 current loss 0.291160, current_train_items 42528.
I0302 18:58:23.756756 22760421793920 run.py:483] Algo bellman_ford step 1329 current loss 0.316099, current_train_items 42560.
I0302 18:58:23.774609 22760421793920 run.py:483] Algo bellman_ford step 1330 current loss 0.020955, current_train_items 42592.
I0302 18:58:23.790305 22760421793920 run.py:483] Algo bellman_ford step 1331 current loss 0.088546, current_train_items 42624.
I0302 18:58:23.812684 22760421793920 run.py:483] Algo bellman_ford step 1332 current loss 0.222333, current_train_items 42656.
I0302 18:58:23.841221 22760421793920 run.py:483] Algo bellman_ford step 1333 current loss 0.210361, current_train_items 42688.
I0302 18:58:23.871618 22760421793920 run.py:483] Algo bellman_ford step 1334 current loss 0.195690, current_train_items 42720.
I0302 18:58:23.889876 22760421793920 run.py:483] Algo bellman_ford step 1335 current loss 0.012058, current_train_items 42752.
I0302 18:58:23.905148 22760421793920 run.py:483] Algo bellman_ford step 1336 current loss 0.034791, current_train_items 42784.
I0302 18:58:23.929171 22760421793920 run.py:483] Algo bellman_ford step 1337 current loss 0.105429, current_train_items 42816.
I0302 18:58:23.957407 22760421793920 run.py:483] Algo bellman_ford step 1338 current loss 0.153388, current_train_items 42848.
I0302 18:58:23.990526 22760421793920 run.py:483] Algo bellman_ford step 1339 current loss 0.231199, current_train_items 42880.
I0302 18:58:24.007977 22760421793920 run.py:483] Algo bellman_ford step 1340 current loss 0.119888, current_train_items 42912.
I0302 18:58:24.024001 22760421793920 run.py:483] Algo bellman_ford step 1341 current loss 0.032915, current_train_items 42944.
I0302 18:58:24.046686 22760421793920 run.py:483] Algo bellman_ford step 1342 current loss 0.095277, current_train_items 42976.
I0302 18:58:24.075104 22760421793920 run.py:483] Algo bellman_ford step 1343 current loss 0.165265, current_train_items 43008.
I0302 18:58:24.105620 22760421793920 run.py:483] Algo bellman_ford step 1344 current loss 0.171639, current_train_items 43040.
I0302 18:58:24.123539 22760421793920 run.py:483] Algo bellman_ford step 1345 current loss 0.021551, current_train_items 43072.
I0302 18:58:24.139194 22760421793920 run.py:483] Algo bellman_ford step 1346 current loss 0.071815, current_train_items 43104.
I0302 18:58:24.162972 22760421793920 run.py:483] Algo bellman_ford step 1347 current loss 0.143363, current_train_items 43136.
I0302 18:58:24.190498 22760421793920 run.py:483] Algo bellman_ford step 1348 current loss 0.158309, current_train_items 43168.
I0302 18:58:24.220048 22760421793920 run.py:483] Algo bellman_ford step 1349 current loss 0.159725, current_train_items 43200.
I0302 18:58:24.237848 22760421793920 run.py:483] Algo bellman_ford step 1350 current loss 0.080455, current_train_items 43232.
I0302 18:58:24.245560 22760421793920 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:24.245672 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.976, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0302 18:58:24.262133 22760421793920 run.py:483] Algo bellman_ford step 1351 current loss 0.040349, current_train_items 43264.
I0302 18:58:24.285891 22760421793920 run.py:483] Algo bellman_ford step 1352 current loss 0.072862, current_train_items 43296.
I0302 18:58:24.313326 22760421793920 run.py:483] Algo bellman_ford step 1353 current loss 0.140742, current_train_items 43328.
I0302 18:58:24.346452 22760421793920 run.py:483] Algo bellman_ford step 1354 current loss 0.197027, current_train_items 43360.
I0302 18:58:24.364662 22760421793920 run.py:483] Algo bellman_ford step 1355 current loss 0.008512, current_train_items 43392.
I0302 18:58:24.380573 22760421793920 run.py:483] Algo bellman_ford step 1356 current loss 0.061164, current_train_items 43424.
I0302 18:58:24.402609 22760421793920 run.py:483] Algo bellman_ford step 1357 current loss 0.080030, current_train_items 43456.
I0302 18:58:24.431021 22760421793920 run.py:483] Algo bellman_ford step 1358 current loss 0.118922, current_train_items 43488.
I0302 18:58:24.463682 22760421793920 run.py:483] Algo bellman_ford step 1359 current loss 0.239759, current_train_items 43520.
I0302 18:58:24.481626 22760421793920 run.py:483] Algo bellman_ford step 1360 current loss 0.006475, current_train_items 43552.
I0302 18:58:24.497843 22760421793920 run.py:483] Algo bellman_ford step 1361 current loss 0.091543, current_train_items 43584.
I0302 18:58:24.521208 22760421793920 run.py:483] Algo bellman_ford step 1362 current loss 0.119256, current_train_items 43616.
I0302 18:58:24.549637 22760421793920 run.py:483] Algo bellman_ford step 1363 current loss 0.143474, current_train_items 43648.
I0302 18:58:24.582144 22760421793920 run.py:483] Algo bellman_ford step 1364 current loss 0.162297, current_train_items 43680.
I0302 18:58:24.600325 22760421793920 run.py:483] Algo bellman_ford step 1365 current loss 0.012853, current_train_items 43712.
I0302 18:58:24.615768 22760421793920 run.py:483] Algo bellman_ford step 1366 current loss 0.059326, current_train_items 43744.
I0302 18:58:24.638159 22760421793920 run.py:483] Algo bellman_ford step 1367 current loss 0.094844, current_train_items 43776.
I0302 18:58:24.665626 22760421793920 run.py:483] Algo bellman_ford step 1368 current loss 0.098999, current_train_items 43808.
I0302 18:58:24.697251 22760421793920 run.py:483] Algo bellman_ford step 1369 current loss 0.194793, current_train_items 43840.
I0302 18:58:24.715161 22760421793920 run.py:483] Algo bellman_ford step 1370 current loss 0.014095, current_train_items 43872.
I0302 18:58:24.731080 22760421793920 run.py:483] Algo bellman_ford step 1371 current loss 0.088516, current_train_items 43904.
I0302 18:58:24.754307 22760421793920 run.py:483] Algo bellman_ford step 1372 current loss 0.117078, current_train_items 43936.
I0302 18:58:24.783979 22760421793920 run.py:483] Algo bellman_ford step 1373 current loss 0.163721, current_train_items 43968.
I0302 18:58:24.815781 22760421793920 run.py:483] Algo bellman_ford step 1374 current loss 0.153975, current_train_items 44000.
I0302 18:58:24.833689 22760421793920 run.py:483] Algo bellman_ford step 1375 current loss 0.038595, current_train_items 44032.
I0302 18:58:24.849013 22760421793920 run.py:483] Algo bellman_ford step 1376 current loss 0.156852, current_train_items 44064.
I0302 18:58:24.872044 22760421793920 run.py:483] Algo bellman_ford step 1377 current loss 0.286112, current_train_items 44096.
I0302 18:58:24.901155 22760421793920 run.py:483] Algo bellman_ford step 1378 current loss 0.260184, current_train_items 44128.
I0302 18:58:24.932631 22760421793920 run.py:483] Algo bellman_ford step 1379 current loss 0.193350, current_train_items 44160.
I0302 18:58:24.950862 22760421793920 run.py:483] Algo bellman_ford step 1380 current loss 0.043620, current_train_items 44192.
I0302 18:58:24.966946 22760421793920 run.py:483] Algo bellman_ford step 1381 current loss 0.096705, current_train_items 44224.
I0302 18:58:24.989536 22760421793920 run.py:483] Algo bellman_ford step 1382 current loss 0.229997, current_train_items 44256.
I0302 18:58:25.018413 22760421793920 run.py:483] Algo bellman_ford step 1383 current loss 0.388151, current_train_items 44288.
I0302 18:58:25.047274 22760421793920 run.py:483] Algo bellman_ford step 1384 current loss 0.374736, current_train_items 44320.
I0302 18:58:25.065116 22760421793920 run.py:483] Algo bellman_ford step 1385 current loss 0.006205, current_train_items 44352.
I0302 18:58:25.080660 22760421793920 run.py:483] Algo bellman_ford step 1386 current loss 0.086474, current_train_items 44384.
I0302 18:58:25.102478 22760421793920 run.py:483] Algo bellman_ford step 1387 current loss 0.098793, current_train_items 44416.
I0302 18:58:25.131316 22760421793920 run.py:483] Algo bellman_ford step 1388 current loss 0.155566, current_train_items 44448.
I0302 18:58:25.162517 22760421793920 run.py:483] Algo bellman_ford step 1389 current loss 0.275343, current_train_items 44480.
I0302 18:58:25.180453 22760421793920 run.py:483] Algo bellman_ford step 1390 current loss 0.017877, current_train_items 44512.
I0302 18:58:25.196087 22760421793920 run.py:483] Algo bellman_ford step 1391 current loss 0.103743, current_train_items 44544.
I0302 18:58:25.219717 22760421793920 run.py:483] Algo bellman_ford step 1392 current loss 0.174586, current_train_items 44576.
I0302 18:58:25.248003 22760421793920 run.py:483] Algo bellman_ford step 1393 current loss 0.184205, current_train_items 44608.
I0302 18:58:25.278843 22760421793920 run.py:483] Algo bellman_ford step 1394 current loss 0.262724, current_train_items 44640.
I0302 18:58:25.296451 22760421793920 run.py:483] Algo bellman_ford step 1395 current loss 0.021095, current_train_items 44672.
I0302 18:58:25.312083 22760421793920 run.py:483] Algo bellman_ford step 1396 current loss 0.125299, current_train_items 44704.
I0302 18:58:25.334978 22760421793920 run.py:483] Algo bellman_ford step 1397 current loss 0.205374, current_train_items 44736.
I0302 18:58:25.364331 22760421793920 run.py:483] Algo bellman_ford step 1398 current loss 0.268924, current_train_items 44768.
I0302 18:58:25.394421 22760421793920 run.py:483] Algo bellman_ford step 1399 current loss 0.193340, current_train_items 44800.
I0302 18:58:25.412130 22760421793920 run.py:483] Algo bellman_ford step 1400 current loss 0.049695, current_train_items 44832.
I0302 18:58:25.419268 22760421793920 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.9560546875, 'score': 0.9560546875, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:58:25.419379 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.976, current avg val score is 0.956, val scores are: bellman_ford: 0.956
I0302 18:58:25.435437 22760421793920 run.py:483] Algo bellman_ford step 1401 current loss 0.109795, current_train_items 44864.
I0302 18:58:25.458930 22760421793920 run.py:483] Algo bellman_ford step 1402 current loss 0.241238, current_train_items 44896.
I0302 18:58:25.488174 22760421793920 run.py:483] Algo bellman_ford step 1403 current loss 0.303773, current_train_items 44928.
I0302 18:58:25.520107 22760421793920 run.py:483] Algo bellman_ford step 1404 current loss 0.466798, current_train_items 44960.
I0302 18:58:25.537895 22760421793920 run.py:483] Algo bellman_ford step 1405 current loss 0.019395, current_train_items 44992.
I0302 18:58:25.553852 22760421793920 run.py:483] Algo bellman_ford step 1406 current loss 0.113182, current_train_items 45024.
I0302 18:58:25.576425 22760421793920 run.py:483] Algo bellman_ford step 1407 current loss 0.136507, current_train_items 45056.
I0302 18:58:25.604951 22760421793920 run.py:483] Algo bellman_ford step 1408 current loss 0.159716, current_train_items 45088.
I0302 18:58:25.635716 22760421793920 run.py:483] Algo bellman_ford step 1409 current loss 0.198143, current_train_items 45120.
I0302 18:58:25.653228 22760421793920 run.py:483] Algo bellman_ford step 1410 current loss 0.006334, current_train_items 45152.
I0302 18:58:25.669416 22760421793920 run.py:483] Algo bellman_ford step 1411 current loss 0.032330, current_train_items 45184.
I0302 18:58:25.691895 22760421793920 run.py:483] Algo bellman_ford step 1412 current loss 0.083802, current_train_items 45216.
I0302 18:58:25.718645 22760421793920 run.py:483] Algo bellman_ford step 1413 current loss 0.090507, current_train_items 45248.
I0302 18:58:25.748759 22760421793920 run.py:483] Algo bellman_ford step 1414 current loss 0.152464, current_train_items 45280.
I0302 18:58:25.766522 22760421793920 run.py:483] Algo bellman_ford step 1415 current loss 0.006895, current_train_items 45312.
I0302 18:58:25.782654 22760421793920 run.py:483] Algo bellman_ford step 1416 current loss 0.040437, current_train_items 45344.
I0302 18:58:25.804581 22760421793920 run.py:483] Algo bellman_ford step 1417 current loss 0.049244, current_train_items 45376.
I0302 18:58:25.832092 22760421793920 run.py:483] Algo bellman_ford step 1418 current loss 0.148640, current_train_items 45408.
I0302 18:58:25.863393 22760421793920 run.py:483] Algo bellman_ford step 1419 current loss 0.186657, current_train_items 45440.
I0302 18:58:25.881338 22760421793920 run.py:483] Algo bellman_ford step 1420 current loss 0.062390, current_train_items 45472.
I0302 18:58:25.897017 22760421793920 run.py:483] Algo bellman_ford step 1421 current loss 0.050770, current_train_items 45504.
I0302 18:58:25.920125 22760421793920 run.py:483] Algo bellman_ford step 1422 current loss 0.104900, current_train_items 45536.
I0302 18:58:25.949343 22760421793920 run.py:483] Algo bellman_ford step 1423 current loss 0.112546, current_train_items 45568.
I0302 18:58:25.979214 22760421793920 run.py:483] Algo bellman_ford step 1424 current loss 0.136624, current_train_items 45600.
I0302 18:58:25.996762 22760421793920 run.py:483] Algo bellman_ford step 1425 current loss 0.011082, current_train_items 45632.
I0302 18:58:26.012828 22760421793920 run.py:483] Algo bellman_ford step 1426 current loss 0.112989, current_train_items 45664.
I0302 18:58:26.036032 22760421793920 run.py:483] Algo bellman_ford step 1427 current loss 0.091978, current_train_items 45696.
I0302 18:58:26.064709 22760421793920 run.py:483] Algo bellman_ford step 1428 current loss 0.106106, current_train_items 45728.
I0302 18:58:26.093964 22760421793920 run.py:483] Algo bellman_ford step 1429 current loss 0.104059, current_train_items 45760.
I0302 18:58:26.111492 22760421793920 run.py:483] Algo bellman_ford step 1430 current loss 0.028652, current_train_items 45792.
I0302 18:58:26.127012 22760421793920 run.py:483] Algo bellman_ford step 1431 current loss 0.035832, current_train_items 45824.
I0302 18:58:26.150604 22760421793920 run.py:483] Algo bellman_ford step 1432 current loss 0.129194, current_train_items 45856.
I0302 18:58:26.177561 22760421793920 run.py:483] Algo bellman_ford step 1433 current loss 0.110851, current_train_items 45888.
I0302 18:58:26.207005 22760421793920 run.py:483] Algo bellman_ford step 1434 current loss 0.121599, current_train_items 45920.
I0302 18:58:26.224560 22760421793920 run.py:483] Algo bellman_ford step 1435 current loss 0.006614, current_train_items 45952.
I0302 18:58:26.240206 22760421793920 run.py:483] Algo bellman_ford step 1436 current loss 0.039032, current_train_items 45984.
I0302 18:58:26.262256 22760421793920 run.py:483] Algo bellman_ford step 1437 current loss 0.108332, current_train_items 46016.
I0302 18:58:26.289949 22760421793920 run.py:483] Algo bellman_ford step 1438 current loss 0.116093, current_train_items 46048.
I0302 18:58:26.321860 22760421793920 run.py:483] Algo bellman_ford step 1439 current loss 0.205433, current_train_items 46080.
I0302 18:58:26.339558 22760421793920 run.py:483] Algo bellman_ford step 1440 current loss 0.010574, current_train_items 46112.
I0302 18:58:26.355370 22760421793920 run.py:483] Algo bellman_ford step 1441 current loss 0.053664, current_train_items 46144.
I0302 18:58:26.377431 22760421793920 run.py:483] Algo bellman_ford step 1442 current loss 0.070441, current_train_items 46176.
I0302 18:58:26.405736 22760421793920 run.py:483] Algo bellman_ford step 1443 current loss 0.118212, current_train_items 46208.
I0302 18:58:26.440326 22760421793920 run.py:483] Algo bellman_ford step 1444 current loss 0.221846, current_train_items 46240.
I0302 18:58:26.458003 22760421793920 run.py:483] Algo bellman_ford step 1445 current loss 0.013458, current_train_items 46272.
I0302 18:58:26.473217 22760421793920 run.py:483] Algo bellman_ford step 1446 current loss 0.087350, current_train_items 46304.
I0302 18:58:26.496019 22760421793920 run.py:483] Algo bellman_ford step 1447 current loss 0.094559, current_train_items 46336.
I0302 18:58:26.523945 22760421793920 run.py:483] Algo bellman_ford step 1448 current loss 0.129608, current_train_items 46368.
I0302 18:58:26.553616 22760421793920 run.py:483] Algo bellman_ford step 1449 current loss 0.155244, current_train_items 46400.
I0302 18:58:26.571128 22760421793920 run.py:483] Algo bellman_ford step 1450 current loss 0.005799, current_train_items 46432.
I0302 18:58:26.578604 22760421793920 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:58:26.578715 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.976, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0302 18:58:26.594553 22760421793920 run.py:483] Algo bellman_ford step 1451 current loss 0.095283, current_train_items 46464.
I0302 18:58:26.617061 22760421793920 run.py:483] Algo bellman_ford step 1452 current loss 0.079918, current_train_items 46496.
I0302 18:58:26.646914 22760421793920 run.py:483] Algo bellman_ford step 1453 current loss 0.136684, current_train_items 46528.
I0302 18:58:26.679497 22760421793920 run.py:483] Algo bellman_ford step 1454 current loss 0.196905, current_train_items 46560.
I0302 18:58:26.697449 22760421793920 run.py:483] Algo bellman_ford step 1455 current loss 0.023848, current_train_items 46592.
I0302 18:58:26.713533 22760421793920 run.py:483] Algo bellman_ford step 1456 current loss 0.071735, current_train_items 46624.
I0302 18:58:26.734801 22760421793920 run.py:483] Algo bellman_ford step 1457 current loss 0.064074, current_train_items 46656.
I0302 18:58:26.764017 22760421793920 run.py:483] Algo bellman_ford step 1458 current loss 0.181206, current_train_items 46688.
I0302 18:58:26.794509 22760421793920 run.py:483] Algo bellman_ford step 1459 current loss 0.230752, current_train_items 46720.
I0302 18:58:26.812155 22760421793920 run.py:483] Algo bellman_ford step 1460 current loss 0.016308, current_train_items 46752.
I0302 18:58:26.827304 22760421793920 run.py:483] Algo bellman_ford step 1461 current loss 0.039992, current_train_items 46784.
I0302 18:58:26.849381 22760421793920 run.py:483] Algo bellman_ford step 1462 current loss 0.102378, current_train_items 46816.
I0302 18:58:26.877461 22760421793920 run.py:483] Algo bellman_ford step 1463 current loss 0.180268, current_train_items 46848.
I0302 18:58:26.907121 22760421793920 run.py:483] Algo bellman_ford step 1464 current loss 0.140642, current_train_items 46880.
I0302 18:58:26.924665 22760421793920 run.py:483] Algo bellman_ford step 1465 current loss 0.028739, current_train_items 46912.
I0302 18:58:26.940657 22760421793920 run.py:483] Algo bellman_ford step 1466 current loss 0.054931, current_train_items 46944.
I0302 18:58:26.962914 22760421793920 run.py:483] Algo bellman_ford step 1467 current loss 0.174004, current_train_items 46976.
I0302 18:58:26.989418 22760421793920 run.py:483] Algo bellman_ford step 1468 current loss 0.166081, current_train_items 47008.
I0302 18:58:27.018857 22760421793920 run.py:483] Algo bellman_ford step 1469 current loss 0.182606, current_train_items 47040.
I0302 18:58:27.036875 22760421793920 run.py:483] Algo bellman_ford step 1470 current loss 0.008557, current_train_items 47072.
I0302 18:58:27.052869 22760421793920 run.py:483] Algo bellman_ford step 1471 current loss 0.052173, current_train_items 47104.
I0302 18:58:27.075002 22760421793920 run.py:483] Algo bellman_ford step 1472 current loss 0.081086, current_train_items 47136.
I0302 18:58:27.103332 22760421793920 run.py:483] Algo bellman_ford step 1473 current loss 0.170532, current_train_items 47168.
I0302 18:58:27.134757 22760421793920 run.py:483] Algo bellman_ford step 1474 current loss 0.176142, current_train_items 47200.
I0302 18:58:27.152699 22760421793920 run.py:483] Algo bellman_ford step 1475 current loss 0.016990, current_train_items 47232.
I0302 18:58:27.168298 22760421793920 run.py:483] Algo bellman_ford step 1476 current loss 0.025543, current_train_items 47264.
I0302 18:58:27.190866 22760421793920 run.py:483] Algo bellman_ford step 1477 current loss 0.075719, current_train_items 47296.
I0302 18:58:27.219063 22760421793920 run.py:483] Algo bellman_ford step 1478 current loss 0.165190, current_train_items 47328.
I0302 18:58:27.251230 22760421793920 run.py:483] Algo bellman_ford step 1479 current loss 0.201182, current_train_items 47360.
I0302 18:58:27.268709 22760421793920 run.py:483] Algo bellman_ford step 1480 current loss 0.007763, current_train_items 47392.
I0302 18:58:27.284157 22760421793920 run.py:483] Algo bellman_ford step 1481 current loss 0.036869, current_train_items 47424.
I0302 18:58:27.305803 22760421793920 run.py:483] Algo bellman_ford step 1482 current loss 0.096218, current_train_items 47456.
I0302 18:58:27.334767 22760421793920 run.py:483] Algo bellman_ford step 1483 current loss 0.145475, current_train_items 47488.
I0302 18:58:27.364012 22760421793920 run.py:483] Algo bellman_ford step 1484 current loss 0.127858, current_train_items 47520.
I0302 18:58:27.381525 22760421793920 run.py:483] Algo bellman_ford step 1485 current loss 0.038774, current_train_items 47552.
I0302 18:58:27.396754 22760421793920 run.py:483] Algo bellman_ford step 1486 current loss 0.052181, current_train_items 47584.
I0302 18:58:27.420438 22760421793920 run.py:483] Algo bellman_ford step 1487 current loss 0.085807, current_train_items 47616.
I0302 18:58:27.447113 22760421793920 run.py:483] Algo bellman_ford step 1488 current loss 0.145370, current_train_items 47648.
I0302 18:58:27.478888 22760421793920 run.py:483] Algo bellman_ford step 1489 current loss 0.265486, current_train_items 47680.
I0302 18:58:27.496630 22760421793920 run.py:483] Algo bellman_ford step 1490 current loss 0.008988, current_train_items 47712.
I0302 18:58:27.512304 22760421793920 run.py:483] Algo bellman_ford step 1491 current loss 0.059119, current_train_items 47744.
I0302 18:58:27.535056 22760421793920 run.py:483] Algo bellman_ford step 1492 current loss 0.052560, current_train_items 47776.
I0302 18:58:27.563266 22760421793920 run.py:483] Algo bellman_ford step 1493 current loss 0.151609, current_train_items 47808.
I0302 18:58:27.595136 22760421793920 run.py:483] Algo bellman_ford step 1494 current loss 0.204140, current_train_items 47840.
I0302 18:58:27.613008 22760421793920 run.py:483] Algo bellman_ford step 1495 current loss 0.014384, current_train_items 47872.
I0302 18:58:27.629028 22760421793920 run.py:483] Algo bellman_ford step 1496 current loss 0.154285, current_train_items 47904.
I0302 18:58:27.651735 22760421793920 run.py:483] Algo bellman_ford step 1497 current loss 0.203831, current_train_items 47936.
I0302 18:58:27.680722 22760421793920 run.py:483] Algo bellman_ford step 1498 current loss 0.204000, current_train_items 47968.
I0302 18:58:27.713458 22760421793920 run.py:483] Algo bellman_ford step 1499 current loss 0.216737, current_train_items 48000.
I0302 18:58:27.730926 22760421793920 run.py:483] Algo bellman_ford step 1500 current loss 0.019846, current_train_items 48032.
I0302 18:58:27.738454 22760421793920 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:58:27.738565 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.976, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0302 18:58:27.755053 22760421793920 run.py:483] Algo bellman_ford step 1501 current loss 0.064708, current_train_items 48064.
I0302 18:58:27.778834 22760421793920 run.py:483] Algo bellman_ford step 1502 current loss 0.171848, current_train_items 48096.
I0302 18:58:27.807320 22760421793920 run.py:483] Algo bellman_ford step 1503 current loss 0.247562, current_train_items 48128.
I0302 18:58:27.838565 22760421793920 run.py:483] Algo bellman_ford step 1504 current loss 0.187343, current_train_items 48160.
I0302 18:58:27.856503 22760421793920 run.py:483] Algo bellman_ford step 1505 current loss 0.008806, current_train_items 48192.
I0302 18:58:27.872103 22760421793920 run.py:483] Algo bellman_ford step 1506 current loss 0.038315, current_train_items 48224.
I0302 18:58:27.894218 22760421793920 run.py:483] Algo bellman_ford step 1507 current loss 0.096855, current_train_items 48256.
I0302 18:58:27.922676 22760421793920 run.py:483] Algo bellman_ford step 1508 current loss 0.103467, current_train_items 48288.
I0302 18:58:27.953661 22760421793920 run.py:483] Algo bellman_ford step 1509 current loss 0.135627, current_train_items 48320.
I0302 18:58:27.971593 22760421793920 run.py:483] Algo bellman_ford step 1510 current loss 0.030739, current_train_items 48352.
I0302 18:58:27.987432 22760421793920 run.py:483] Algo bellman_ford step 1511 current loss 0.061051, current_train_items 48384.
I0302 18:58:28.010336 22760421793920 run.py:483] Algo bellman_ford step 1512 current loss 0.141661, current_train_items 48416.
I0302 18:58:28.037753 22760421793920 run.py:483] Algo bellman_ford step 1513 current loss 0.104760, current_train_items 48448.
I0302 18:58:28.070155 22760421793920 run.py:483] Algo bellman_ford step 1514 current loss 0.304119, current_train_items 48480.
I0302 18:58:28.087982 22760421793920 run.py:483] Algo bellman_ford step 1515 current loss 0.016706, current_train_items 48512.
I0302 18:58:28.103365 22760421793920 run.py:483] Algo bellman_ford step 1516 current loss 0.030046, current_train_items 48544.
I0302 18:58:28.126128 22760421793920 run.py:483] Algo bellman_ford step 1517 current loss 0.130873, current_train_items 48576.
I0302 18:58:28.155120 22760421793920 run.py:483] Algo bellman_ford step 1518 current loss 0.143218, current_train_items 48608.
I0302 18:58:28.185970 22760421793920 run.py:483] Algo bellman_ford step 1519 current loss 0.202528, current_train_items 48640.
I0302 18:58:28.203798 22760421793920 run.py:483] Algo bellman_ford step 1520 current loss 0.012289, current_train_items 48672.
I0302 18:58:28.219128 22760421793920 run.py:483] Algo bellman_ford step 1521 current loss 0.103953, current_train_items 48704.
I0302 18:58:28.242200 22760421793920 run.py:483] Algo bellman_ford step 1522 current loss 0.178092, current_train_items 48736.
I0302 18:58:28.270767 22760421793920 run.py:483] Algo bellman_ford step 1523 current loss 0.239764, current_train_items 48768.
I0302 18:58:28.301728 22760421793920 run.py:483] Algo bellman_ford step 1524 current loss 0.217312, current_train_items 48800.
I0302 18:58:28.319512 22760421793920 run.py:483] Algo bellman_ford step 1525 current loss 0.029583, current_train_items 48832.
I0302 18:58:28.334832 22760421793920 run.py:483] Algo bellman_ford step 1526 current loss 0.110899, current_train_items 48864.
I0302 18:58:28.358389 22760421793920 run.py:483] Algo bellman_ford step 1527 current loss 0.447925, current_train_items 48896.
I0302 18:58:28.386676 22760421793920 run.py:483] Algo bellman_ford step 1528 current loss 0.469382, current_train_items 48928.
I0302 18:58:28.418236 22760421793920 run.py:483] Algo bellman_ford step 1529 current loss 0.350581, current_train_items 48960.
I0302 18:58:28.436316 22760421793920 run.py:483] Algo bellman_ford step 1530 current loss 0.057616, current_train_items 48992.
I0302 18:58:28.451741 22760421793920 run.py:483] Algo bellman_ford step 1531 current loss 0.080136, current_train_items 49024.
I0302 18:58:28.474380 22760421793920 run.py:483] Algo bellman_ford step 1532 current loss 0.110670, current_train_items 49056.
I0302 18:58:28.503395 22760421793920 run.py:483] Algo bellman_ford step 1533 current loss 0.333527, current_train_items 49088.
I0302 18:58:28.535332 22760421793920 run.py:483] Algo bellman_ford step 1534 current loss 0.413884, current_train_items 49120.
I0302 18:58:28.553135 22760421793920 run.py:483] Algo bellman_ford step 1535 current loss 0.051412, current_train_items 49152.
I0302 18:58:28.569184 22760421793920 run.py:483] Algo bellman_ford step 1536 current loss 0.038729, current_train_items 49184.
I0302 18:58:28.592225 22760421793920 run.py:483] Algo bellman_ford step 1537 current loss 0.111249, current_train_items 49216.
I0302 18:58:28.620984 22760421793920 run.py:483] Algo bellman_ford step 1538 current loss 0.112142, current_train_items 49248.
I0302 18:58:28.655076 22760421793920 run.py:483] Algo bellman_ford step 1539 current loss 0.291326, current_train_items 49280.
I0302 18:58:28.673108 22760421793920 run.py:483] Algo bellman_ford step 1540 current loss 0.013157, current_train_items 49312.
I0302 18:58:28.688472 22760421793920 run.py:483] Algo bellman_ford step 1541 current loss 0.108255, current_train_items 49344.
I0302 18:58:28.711280 22760421793920 run.py:483] Algo bellman_ford step 1542 current loss 0.100535, current_train_items 49376.
I0302 18:58:28.739064 22760421793920 run.py:483] Algo bellman_ford step 1543 current loss 0.136521, current_train_items 49408.
I0302 18:58:28.769872 22760421793920 run.py:483] Algo bellman_ford step 1544 current loss 0.203325, current_train_items 49440.
I0302 18:58:28.788035 22760421793920 run.py:483] Algo bellman_ford step 1545 current loss 0.019535, current_train_items 49472.
I0302 18:58:28.803667 22760421793920 run.py:483] Algo bellman_ford step 1546 current loss 0.067314, current_train_items 49504.
I0302 18:58:28.826598 22760421793920 run.py:483] Algo bellman_ford step 1547 current loss 0.132602, current_train_items 49536.
I0302 18:58:28.855887 22760421793920 run.py:483] Algo bellman_ford step 1548 current loss 0.134964, current_train_items 49568.
I0302 18:58:28.888688 22760421793920 run.py:483] Algo bellman_ford step 1549 current loss 0.227672, current_train_items 49600.
I0302 18:58:28.906500 22760421793920 run.py:483] Algo bellman_ford step 1550 current loss 0.026058, current_train_items 49632.
I0302 18:58:28.914097 22760421793920 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:58:28.914206 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.976, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:58:28.943319 22760421793920 run.py:483] Algo bellman_ford step 1551 current loss 0.065301, current_train_items 49664.
I0302 18:58:28.965728 22760421793920 run.py:483] Algo bellman_ford step 1552 current loss 0.059364, current_train_items 49696.
I0302 18:58:28.994989 22760421793920 run.py:483] Algo bellman_ford step 1553 current loss 0.140410, current_train_items 49728.
I0302 18:58:29.025298 22760421793920 run.py:483] Algo bellman_ford step 1554 current loss 0.134937, current_train_items 49760.
I0302 18:58:29.043368 22760421793920 run.py:483] Algo bellman_ford step 1555 current loss 0.010311, current_train_items 49792.
I0302 18:58:29.059443 22760421793920 run.py:483] Algo bellman_ford step 1556 current loss 0.037774, current_train_items 49824.
I0302 18:58:29.082496 22760421793920 run.py:483] Algo bellman_ford step 1557 current loss 0.200056, current_train_items 49856.
I0302 18:58:29.111674 22760421793920 run.py:483] Algo bellman_ford step 1558 current loss 0.154875, current_train_items 49888.
I0302 18:58:29.143420 22760421793920 run.py:483] Algo bellman_ford step 1559 current loss 0.120318, current_train_items 49920.
I0302 18:58:29.161250 22760421793920 run.py:483] Algo bellman_ford step 1560 current loss 0.009959, current_train_items 49952.
I0302 18:58:29.177043 22760421793920 run.py:483] Algo bellman_ford step 1561 current loss 0.027296, current_train_items 49984.
I0302 18:58:29.199923 22760421793920 run.py:483] Algo bellman_ford step 1562 current loss 0.090453, current_train_items 50016.
I0302 18:58:29.229137 22760421793920 run.py:483] Algo bellman_ford step 1563 current loss 0.090127, current_train_items 50048.
I0302 18:58:29.261544 22760421793920 run.py:483] Algo bellman_ford step 1564 current loss 0.131469, current_train_items 50080.
I0302 18:58:29.279165 22760421793920 run.py:483] Algo bellman_ford step 1565 current loss 0.004744, current_train_items 50112.
I0302 18:58:29.295130 22760421793920 run.py:483] Algo bellman_ford step 1566 current loss 0.042854, current_train_items 50144.
I0302 18:58:29.317944 22760421793920 run.py:483] Algo bellman_ford step 1567 current loss 0.102361, current_train_items 50176.
I0302 18:58:29.346455 22760421793920 run.py:483] Algo bellman_ford step 1568 current loss 0.211864, current_train_items 50208.
I0302 18:58:29.379026 22760421793920 run.py:483] Algo bellman_ford step 1569 current loss 0.186666, current_train_items 50240.
I0302 18:58:29.397062 22760421793920 run.py:483] Algo bellman_ford step 1570 current loss 0.010383, current_train_items 50272.
I0302 18:58:29.412924 22760421793920 run.py:483] Algo bellman_ford step 1571 current loss 0.030136, current_train_items 50304.
I0302 18:58:29.436723 22760421793920 run.py:483] Algo bellman_ford step 1572 current loss 0.096908, current_train_items 50336.
I0302 18:58:29.465009 22760421793920 run.py:483] Algo bellman_ford step 1573 current loss 0.104714, current_train_items 50368.
I0302 18:58:29.494287 22760421793920 run.py:483] Algo bellman_ford step 1574 current loss 0.131484, current_train_items 50400.
I0302 18:58:29.512585 22760421793920 run.py:483] Algo bellman_ford step 1575 current loss 0.034209, current_train_items 50432.
I0302 18:58:29.528355 22760421793920 run.py:483] Algo bellman_ford step 1576 current loss 0.056153, current_train_items 50464.
I0302 18:58:29.550010 22760421793920 run.py:483] Algo bellman_ford step 1577 current loss 0.075159, current_train_items 50496.
I0302 18:58:29.579437 22760421793920 run.py:483] Algo bellman_ford step 1578 current loss 0.183036, current_train_items 50528.
I0302 18:58:29.610342 22760421793920 run.py:483] Algo bellman_ford step 1579 current loss 0.110385, current_train_items 50560.
I0302 18:58:29.628101 22760421793920 run.py:483] Algo bellman_ford step 1580 current loss 0.032400, current_train_items 50592.
I0302 18:58:29.643477 22760421793920 run.py:483] Algo bellman_ford step 1581 current loss 0.059764, current_train_items 50624.
I0302 18:58:29.665888 22760421793920 run.py:483] Algo bellman_ford step 1582 current loss 0.117637, current_train_items 50656.
I0302 18:58:29.694482 22760421793920 run.py:483] Algo bellman_ford step 1583 current loss 0.165884, current_train_items 50688.
I0302 18:58:29.726586 22760421793920 run.py:483] Algo bellman_ford step 1584 current loss 0.213087, current_train_items 50720.
I0302 18:58:29.743979 22760421793920 run.py:483] Algo bellman_ford step 1585 current loss 0.005918, current_train_items 50752.
I0302 18:58:29.759156 22760421793920 run.py:483] Algo bellman_ford step 1586 current loss 0.132969, current_train_items 50784.
I0302 18:58:29.782437 22760421793920 run.py:483] Algo bellman_ford step 1587 current loss 0.145104, current_train_items 50816.
I0302 18:58:29.810598 22760421793920 run.py:483] Algo bellman_ford step 1588 current loss 0.132246, current_train_items 50848.
I0302 18:58:29.841521 22760421793920 run.py:483] Algo bellman_ford step 1589 current loss 0.159209, current_train_items 50880.
I0302 18:58:29.859362 22760421793920 run.py:483] Algo bellman_ford step 1590 current loss 0.072820, current_train_items 50912.
I0302 18:58:29.875089 22760421793920 run.py:483] Algo bellman_ford step 1591 current loss 0.054717, current_train_items 50944.
I0302 18:58:29.897106 22760421793920 run.py:483] Algo bellman_ford step 1592 current loss 0.113041, current_train_items 50976.
I0302 18:58:29.926712 22760421793920 run.py:483] Algo bellman_ford step 1593 current loss 0.154405, current_train_items 51008.
I0302 18:58:29.959146 22760421793920 run.py:483] Algo bellman_ford step 1594 current loss 0.157271, current_train_items 51040.
I0302 18:58:29.977425 22760421793920 run.py:483] Algo bellman_ford step 1595 current loss 0.020706, current_train_items 51072.
I0302 18:58:29.992618 22760421793920 run.py:483] Algo bellman_ford step 1596 current loss 0.057783, current_train_items 51104.
I0302 18:58:30.016098 22760421793920 run.py:483] Algo bellman_ford step 1597 current loss 0.105567, current_train_items 51136.
I0302 18:58:30.043763 22760421793920 run.py:483] Algo bellman_ford step 1598 current loss 0.123159, current_train_items 51168.
I0302 18:58:30.075374 22760421793920 run.py:483] Algo bellman_ford step 1599 current loss 0.162127, current_train_items 51200.
I0302 18:58:30.093415 22760421793920 run.py:483] Algo bellman_ford step 1600 current loss 0.024178, current_train_items 51232.
I0302 18:58:30.100930 22760421793920 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:58:30.101040 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:58:30.117160 22760421793920 run.py:483] Algo bellman_ford step 1601 current loss 0.046261, current_train_items 51264.
I0302 18:58:30.140739 22760421793920 run.py:483] Algo bellman_ford step 1602 current loss 0.055624, current_train_items 51296.
I0302 18:58:30.169482 22760421793920 run.py:483] Algo bellman_ford step 1603 current loss 0.147972, current_train_items 51328.
I0302 18:58:30.201270 22760421793920 run.py:483] Algo bellman_ford step 1604 current loss 0.122368, current_train_items 51360.
I0302 18:58:30.219340 22760421793920 run.py:483] Algo bellman_ford step 1605 current loss 0.021406, current_train_items 51392.
I0302 18:58:30.234476 22760421793920 run.py:483] Algo bellman_ford step 1606 current loss 0.061230, current_train_items 51424.
I0302 18:58:30.256966 22760421793920 run.py:483] Algo bellman_ford step 1607 current loss 0.067073, current_train_items 51456.
I0302 18:58:30.286200 22760421793920 run.py:483] Algo bellman_ford step 1608 current loss 0.159476, current_train_items 51488.
I0302 18:58:30.318339 22760421793920 run.py:483] Algo bellman_ford step 1609 current loss 0.265351, current_train_items 51520.
I0302 18:58:30.335985 22760421793920 run.py:483] Algo bellman_ford step 1610 current loss 0.039620, current_train_items 51552.
I0302 18:58:30.351893 22760421793920 run.py:483] Algo bellman_ford step 1611 current loss 0.114280, current_train_items 51584.
I0302 18:58:30.374322 22760421793920 run.py:483] Algo bellman_ford step 1612 current loss 0.156809, current_train_items 51616.
I0302 18:58:30.403133 22760421793920 run.py:483] Algo bellman_ford step 1613 current loss 0.295477, current_train_items 51648.
I0302 18:58:30.433251 22760421793920 run.py:483] Algo bellman_ford step 1614 current loss 0.256434, current_train_items 51680.
I0302 18:58:30.450805 22760421793920 run.py:483] Algo bellman_ford step 1615 current loss 0.027397, current_train_items 51712.
I0302 18:58:30.466748 22760421793920 run.py:483] Algo bellman_ford step 1616 current loss 0.068224, current_train_items 51744.
I0302 18:58:30.489291 22760421793920 run.py:483] Algo bellman_ford step 1617 current loss 0.254066, current_train_items 51776.
I0302 18:58:30.516894 22760421793920 run.py:483] Algo bellman_ford step 1618 current loss 0.247264, current_train_items 51808.
I0302 18:58:30.549670 22760421793920 run.py:483] Algo bellman_ford step 1619 current loss 0.314779, current_train_items 51840.
I0302 18:58:30.567277 22760421793920 run.py:483] Algo bellman_ford step 1620 current loss 0.025126, current_train_items 51872.
I0302 18:58:30.582757 22760421793920 run.py:483] Algo bellman_ford step 1621 current loss 0.073299, current_train_items 51904.
I0302 18:58:30.605595 22760421793920 run.py:483] Algo bellman_ford step 1622 current loss 0.130706, current_train_items 51936.
I0302 18:58:30.634207 22760421793920 run.py:483] Algo bellman_ford step 1623 current loss 0.158769, current_train_items 51968.
I0302 18:58:30.665185 22760421793920 run.py:483] Algo bellman_ford step 1624 current loss 0.175642, current_train_items 52000.
I0302 18:58:30.682823 22760421793920 run.py:483] Algo bellman_ford step 1625 current loss 0.007521, current_train_items 52032.
I0302 18:58:30.698642 22760421793920 run.py:483] Algo bellman_ford step 1626 current loss 0.055543, current_train_items 52064.
I0302 18:58:30.720924 22760421793920 run.py:483] Algo bellman_ford step 1627 current loss 0.121777, current_train_items 52096.
I0302 18:58:30.750648 22760421793920 run.py:483] Algo bellman_ford step 1628 current loss 0.150028, current_train_items 52128.
I0302 18:58:30.780972 22760421793920 run.py:483] Algo bellman_ford step 1629 current loss 0.138934, current_train_items 52160.
I0302 18:58:30.798633 22760421793920 run.py:483] Algo bellman_ford step 1630 current loss 0.015315, current_train_items 52192.
I0302 18:58:30.814811 22760421793920 run.py:483] Algo bellman_ford step 1631 current loss 0.052858, current_train_items 52224.
I0302 18:58:30.838100 22760421793920 run.py:483] Algo bellman_ford step 1632 current loss 0.143743, current_train_items 52256.
I0302 18:58:30.865824 22760421793920 run.py:483] Algo bellman_ford step 1633 current loss 0.105717, current_train_items 52288.
I0302 18:58:30.897223 22760421793920 run.py:483] Algo bellman_ford step 1634 current loss 0.128958, current_train_items 52320.
I0302 18:58:30.915152 22760421793920 run.py:483] Algo bellman_ford step 1635 current loss 0.011440, current_train_items 52352.
I0302 18:58:30.931035 22760421793920 run.py:483] Algo bellman_ford step 1636 current loss 0.052605, current_train_items 52384.
I0302 18:58:30.953696 22760421793920 run.py:483] Algo bellman_ford step 1637 current loss 0.127527, current_train_items 52416.
I0302 18:58:30.982618 22760421793920 run.py:483] Algo bellman_ford step 1638 current loss 0.149477, current_train_items 52448.
I0302 18:58:31.015821 22760421793920 run.py:483] Algo bellman_ford step 1639 current loss 0.128231, current_train_items 52480.
I0302 18:58:31.033150 22760421793920 run.py:483] Algo bellman_ford step 1640 current loss 0.066029, current_train_items 52512.
I0302 18:58:31.048474 22760421793920 run.py:483] Algo bellman_ford step 1641 current loss 0.086173, current_train_items 52544.
I0302 18:58:31.071005 22760421793920 run.py:483] Algo bellman_ford step 1642 current loss 0.268408, current_train_items 52576.
I0302 18:58:31.099345 22760421793920 run.py:483] Algo bellman_ford step 1643 current loss 0.207318, current_train_items 52608.
I0302 18:58:31.129604 22760421793920 run.py:483] Algo bellman_ford step 1644 current loss 0.207536, current_train_items 52640.
I0302 18:58:31.147137 22760421793920 run.py:483] Algo bellman_ford step 1645 current loss 0.026236, current_train_items 52672.
I0302 18:58:31.162796 22760421793920 run.py:483] Algo bellman_ford step 1646 current loss 0.032599, current_train_items 52704.
I0302 18:58:31.186124 22760421793920 run.py:483] Algo bellman_ford step 1647 current loss 0.149003, current_train_items 52736.
I0302 18:58:31.213402 22760421793920 run.py:483] Algo bellman_ford step 1648 current loss 0.125359, current_train_items 52768.
I0302 18:58:31.245540 22760421793920 run.py:483] Algo bellman_ford step 1649 current loss 0.234629, current_train_items 52800.
I0302 18:58:31.263201 22760421793920 run.py:483] Algo bellman_ford step 1650 current loss 0.026189, current_train_items 52832.
I0302 18:58:31.270544 22760421793920 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:58:31.270653 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.979, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:31.287502 22760421793920 run.py:483] Algo bellman_ford step 1651 current loss 0.077397, current_train_items 52864.
I0302 18:58:31.311590 22760421793920 run.py:483] Algo bellman_ford step 1652 current loss 0.125168, current_train_items 52896.
I0302 18:58:31.340141 22760421793920 run.py:483] Algo bellman_ford step 1653 current loss 0.156526, current_train_items 52928.
I0302 18:58:31.370441 22760421793920 run.py:483] Algo bellman_ford step 1654 current loss 0.141720, current_train_items 52960.
I0302 18:58:31.388959 22760421793920 run.py:483] Algo bellman_ford step 1655 current loss 0.010801, current_train_items 52992.
I0302 18:58:31.405205 22760421793920 run.py:483] Algo bellman_ford step 1656 current loss 0.034136, current_train_items 53024.
I0302 18:58:31.427526 22760421793920 run.py:483] Algo bellman_ford step 1657 current loss 0.113965, current_train_items 53056.
I0302 18:58:31.454724 22760421793920 run.py:483] Algo bellman_ford step 1658 current loss 0.117708, current_train_items 53088.
I0302 18:58:31.484563 22760421793920 run.py:483] Algo bellman_ford step 1659 current loss 0.184715, current_train_items 53120.
I0302 18:58:31.502359 22760421793920 run.py:483] Algo bellman_ford step 1660 current loss 0.012471, current_train_items 53152.
I0302 18:58:31.518002 22760421793920 run.py:483] Algo bellman_ford step 1661 current loss 0.066437, current_train_items 53184.
I0302 18:58:31.540391 22760421793920 run.py:483] Algo bellman_ford step 1662 current loss 0.113072, current_train_items 53216.
I0302 18:58:31.570397 22760421793920 run.py:483] Algo bellman_ford step 1663 current loss 0.122442, current_train_items 53248.
I0302 18:58:31.602358 22760421793920 run.py:483] Algo bellman_ford step 1664 current loss 0.151190, current_train_items 53280.
I0302 18:58:31.620419 22760421793920 run.py:483] Algo bellman_ford step 1665 current loss 0.041799, current_train_items 53312.
I0302 18:58:31.636155 22760421793920 run.py:483] Algo bellman_ford step 1666 current loss 0.090769, current_train_items 53344.
I0302 18:58:31.658543 22760421793920 run.py:483] Algo bellman_ford step 1667 current loss 0.095246, current_train_items 53376.
I0302 18:58:31.686286 22760421793920 run.py:483] Algo bellman_ford step 1668 current loss 0.121335, current_train_items 53408.
I0302 18:58:31.716020 22760421793920 run.py:483] Algo bellman_ford step 1669 current loss 0.173198, current_train_items 53440.
I0302 18:58:31.733813 22760421793920 run.py:483] Algo bellman_ford step 1670 current loss 0.017432, current_train_items 53472.
I0302 18:58:31.749225 22760421793920 run.py:483] Algo bellman_ford step 1671 current loss 0.044760, current_train_items 53504.
I0302 18:58:31.771644 22760421793920 run.py:483] Algo bellman_ford step 1672 current loss 0.089075, current_train_items 53536.
I0302 18:58:31.801149 22760421793920 run.py:483] Algo bellman_ford step 1673 current loss 0.101108, current_train_items 53568.
I0302 18:58:31.832759 22760421793920 run.py:483] Algo bellman_ford step 1674 current loss 0.141093, current_train_items 53600.
I0302 18:58:31.850632 22760421793920 run.py:483] Algo bellman_ford step 1675 current loss 0.011017, current_train_items 53632.
I0302 18:58:31.866371 22760421793920 run.py:483] Algo bellman_ford step 1676 current loss 0.071906, current_train_items 53664.
I0302 18:58:31.889097 22760421793920 run.py:483] Algo bellman_ford step 1677 current loss 0.087309, current_train_items 53696.
I0302 18:58:31.917481 22760421793920 run.py:483] Algo bellman_ford step 1678 current loss 0.234831, current_train_items 53728.
I0302 18:58:31.949732 22760421793920 run.py:483] Algo bellman_ford step 1679 current loss 0.195664, current_train_items 53760.
I0302 18:58:31.967495 22760421793920 run.py:483] Algo bellman_ford step 1680 current loss 0.023859, current_train_items 53792.
I0302 18:58:31.982926 22760421793920 run.py:483] Algo bellman_ford step 1681 current loss 0.026571, current_train_items 53824.
I0302 18:58:32.004793 22760421793920 run.py:483] Algo bellman_ford step 1682 current loss 0.087644, current_train_items 53856.
I0302 18:58:32.032474 22760421793920 run.py:483] Algo bellman_ford step 1683 current loss 0.118663, current_train_items 53888.
I0302 18:58:32.063711 22760421793920 run.py:483] Algo bellman_ford step 1684 current loss 0.144833, current_train_items 53920.
I0302 18:58:32.081852 22760421793920 run.py:483] Algo bellman_ford step 1685 current loss 0.016459, current_train_items 53952.
I0302 18:58:32.097474 22760421793920 run.py:483] Algo bellman_ford step 1686 current loss 0.048502, current_train_items 53984.
I0302 18:58:32.120262 22760421793920 run.py:483] Algo bellman_ford step 1687 current loss 0.119803, current_train_items 54016.
I0302 18:58:32.148887 22760421793920 run.py:483] Algo bellman_ford step 1688 current loss 0.217957, current_train_items 54048.
I0302 18:58:32.182089 22760421793920 run.py:483] Algo bellman_ford step 1689 current loss 0.194118, current_train_items 54080.
I0302 18:58:32.199643 22760421793920 run.py:483] Algo bellman_ford step 1690 current loss 0.017773, current_train_items 54112.
I0302 18:58:32.215716 22760421793920 run.py:483] Algo bellman_ford step 1691 current loss 0.083561, current_train_items 54144.
I0302 18:58:32.238486 22760421793920 run.py:483] Algo bellman_ford step 1692 current loss 0.122694, current_train_items 54176.
I0302 18:58:32.268085 22760421793920 run.py:483] Algo bellman_ford step 1693 current loss 0.207407, current_train_items 54208.
I0302 18:58:32.299692 22760421793920 run.py:483] Algo bellman_ford step 1694 current loss 0.207728, current_train_items 54240.
I0302 18:58:32.317619 22760421793920 run.py:483] Algo bellman_ford step 1695 current loss 0.017081, current_train_items 54272.
I0302 18:58:32.333669 22760421793920 run.py:483] Algo bellman_ford step 1696 current loss 0.036635, current_train_items 54304.
I0302 18:58:32.354674 22760421793920 run.py:483] Algo bellman_ford step 1697 current loss 0.053040, current_train_items 54336.
I0302 18:58:32.382152 22760421793920 run.py:483] Algo bellman_ford step 1698 current loss 0.143497, current_train_items 54368.
I0302 18:58:32.414086 22760421793920 run.py:483] Algo bellman_ford step 1699 current loss 0.163465, current_train_items 54400.
I0302 18:58:32.432056 22760421793920 run.py:483] Algo bellman_ford step 1700 current loss 0.015164, current_train_items 54432.
I0302 18:58:32.439558 22760421793920 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:58:32.439667 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.979, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:58:32.467735 22760421793920 run.py:483] Algo bellman_ford step 1701 current loss 0.072907, current_train_items 54464.
I0302 18:58:32.489923 22760421793920 run.py:483] Algo bellman_ford step 1702 current loss 0.077335, current_train_items 54496.
I0302 18:58:32.518716 22760421793920 run.py:483] Algo bellman_ford step 1703 current loss 0.069782, current_train_items 54528.
I0302 18:58:32.550036 22760421793920 run.py:483] Algo bellman_ford step 1704 current loss 0.191732, current_train_items 54560.
I0302 18:58:32.568635 22760421793920 run.py:483] Algo bellman_ford step 1705 current loss 0.007370, current_train_items 54592.
I0302 18:58:32.584198 22760421793920 run.py:483] Algo bellman_ford step 1706 current loss 0.028826, current_train_items 54624.
I0302 18:58:32.606817 22760421793920 run.py:483] Algo bellman_ford step 1707 current loss 0.117001, current_train_items 54656.
I0302 18:58:32.635218 22760421793920 run.py:483] Algo bellman_ford step 1708 current loss 0.128524, current_train_items 54688.
I0302 18:58:32.667891 22760421793920 run.py:483] Algo bellman_ford step 1709 current loss 0.134470, current_train_items 54720.
I0302 18:58:32.685665 22760421793920 run.py:483] Algo bellman_ford step 1710 current loss 0.017117, current_train_items 54752.
I0302 18:58:32.701517 22760421793920 run.py:483] Algo bellman_ford step 1711 current loss 0.033919, current_train_items 54784.
I0302 18:58:32.724226 22760421793920 run.py:483] Algo bellman_ford step 1712 current loss 0.181035, current_train_items 54816.
I0302 18:58:32.752439 22760421793920 run.py:483] Algo bellman_ford step 1713 current loss 0.114546, current_train_items 54848.
I0302 18:58:32.783126 22760421793920 run.py:483] Algo bellman_ford step 1714 current loss 0.184315, current_train_items 54880.
I0302 18:58:32.800963 22760421793920 run.py:483] Algo bellman_ford step 1715 current loss 0.009833, current_train_items 54912.
I0302 18:58:32.816220 22760421793920 run.py:483] Algo bellman_ford step 1716 current loss 0.051485, current_train_items 54944.
I0302 18:58:32.839328 22760421793920 run.py:483] Algo bellman_ford step 1717 current loss 0.090386, current_train_items 54976.
I0302 18:58:32.866594 22760421793920 run.py:483] Algo bellman_ford step 1718 current loss 0.146497, current_train_items 55008.
I0302 18:58:32.897517 22760421793920 run.py:483] Algo bellman_ford step 1719 current loss 0.167015, current_train_items 55040.
I0302 18:58:32.915298 22760421793920 run.py:483] Algo bellman_ford step 1720 current loss 0.050121, current_train_items 55072.
I0302 18:58:32.930718 22760421793920 run.py:483] Algo bellman_ford step 1721 current loss 0.078214, current_train_items 55104.
I0302 18:58:32.953047 22760421793920 run.py:483] Algo bellman_ford step 1722 current loss 0.062198, current_train_items 55136.
I0302 18:58:32.981423 22760421793920 run.py:483] Algo bellman_ford step 1723 current loss 0.104888, current_train_items 55168.
I0302 18:58:33.012364 22760421793920 run.py:483] Algo bellman_ford step 1724 current loss 0.139000, current_train_items 55200.
I0302 18:58:33.029986 22760421793920 run.py:483] Algo bellman_ford step 1725 current loss 0.006139, current_train_items 55232.
I0302 18:58:33.045749 22760421793920 run.py:483] Algo bellman_ford step 1726 current loss 0.106109, current_train_items 55264.
I0302 18:58:33.069895 22760421793920 run.py:483] Algo bellman_ford step 1727 current loss 0.103470, current_train_items 55296.
I0302 18:58:33.098472 22760421793920 run.py:483] Algo bellman_ford step 1728 current loss 0.099563, current_train_items 55328.
I0302 18:58:33.129827 22760421793920 run.py:483] Algo bellman_ford step 1729 current loss 0.174401, current_train_items 55360.
I0302 18:58:33.147566 22760421793920 run.py:483] Algo bellman_ford step 1730 current loss 0.015678, current_train_items 55392.
I0302 18:58:33.163293 22760421793920 run.py:483] Algo bellman_ford step 1731 current loss 0.242057, current_train_items 55424.
I0302 18:58:33.185777 22760421793920 run.py:483] Algo bellman_ford step 1732 current loss 0.233306, current_train_items 55456.
I0302 18:58:33.214128 22760421793920 run.py:483] Algo bellman_ford step 1733 current loss 0.132920, current_train_items 55488.
I0302 18:58:33.243135 22760421793920 run.py:483] Algo bellman_ford step 1734 current loss 0.153421, current_train_items 55520.
I0302 18:58:33.261031 22760421793920 run.py:483] Algo bellman_ford step 1735 current loss 0.015455, current_train_items 55552.
I0302 18:58:33.276850 22760421793920 run.py:483] Algo bellman_ford step 1736 current loss 0.125446, current_train_items 55584.
I0302 18:58:33.299985 22760421793920 run.py:483] Algo bellman_ford step 1737 current loss 0.244793, current_train_items 55616.
I0302 18:58:33.328785 22760421793920 run.py:483] Algo bellman_ford step 1738 current loss 0.273887, current_train_items 55648.
I0302 18:58:33.360205 22760421793920 run.py:483] Algo bellman_ford step 1739 current loss 0.168568, current_train_items 55680.
I0302 18:58:33.377836 22760421793920 run.py:483] Algo bellman_ford step 1740 current loss 0.014464, current_train_items 55712.
I0302 18:58:33.393514 22760421793920 run.py:483] Algo bellman_ford step 1741 current loss 0.056707, current_train_items 55744.
I0302 18:58:33.416235 22760421793920 run.py:483] Algo bellman_ford step 1742 current loss 0.155193, current_train_items 55776.
I0302 18:58:33.445114 22760421793920 run.py:483] Algo bellman_ford step 1743 current loss 0.252963, current_train_items 55808.
I0302 18:58:33.475303 22760421793920 run.py:483] Algo bellman_ford step 1744 current loss 0.239824, current_train_items 55840.
I0302 18:58:33.493150 22760421793920 run.py:483] Algo bellman_ford step 1745 current loss 0.064087, current_train_items 55872.
I0302 18:58:33.509055 22760421793920 run.py:483] Algo bellman_ford step 1746 current loss 0.090878, current_train_items 55904.
I0302 18:58:33.531335 22760421793920 run.py:483] Algo bellman_ford step 1747 current loss 0.216948, current_train_items 55936.
I0302 18:58:33.559736 22760421793920 run.py:483] Algo bellman_ford step 1748 current loss 0.200950, current_train_items 55968.
I0302 18:58:33.589362 22760421793920 run.py:483] Algo bellman_ford step 1749 current loss 0.287887, current_train_items 56000.
I0302 18:58:33.607396 22760421793920 run.py:483] Algo bellman_ford step 1750 current loss 0.054902, current_train_items 56032.
I0302 18:58:33.614852 22760421793920 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:58:33.614970 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.980, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0302 18:58:33.631345 22760421793920 run.py:483] Algo bellman_ford step 1751 current loss 0.095178, current_train_items 56064.
I0302 18:58:33.654098 22760421793920 run.py:483] Algo bellman_ford step 1752 current loss 0.094414, current_train_items 56096.
I0302 18:58:33.684228 22760421793920 run.py:483] Algo bellman_ford step 1753 current loss 0.176719, current_train_items 56128.
I0302 18:58:33.717516 22760421793920 run.py:483] Algo bellman_ford step 1754 current loss 0.244350, current_train_items 56160.
I0302 18:58:33.735705 22760421793920 run.py:483] Algo bellman_ford step 1755 current loss 0.015252, current_train_items 56192.
I0302 18:58:33.751957 22760421793920 run.py:483] Algo bellman_ford step 1756 current loss 0.053222, current_train_items 56224.
I0302 18:58:33.774700 22760421793920 run.py:483] Algo bellman_ford step 1757 current loss 0.183369, current_train_items 56256.
I0302 18:58:33.804027 22760421793920 run.py:483] Algo bellman_ford step 1758 current loss 0.145963, current_train_items 56288.
I0302 18:58:33.833226 22760421793920 run.py:483] Algo bellman_ford step 1759 current loss 0.127961, current_train_items 56320.
I0302 18:58:33.850823 22760421793920 run.py:483] Algo bellman_ford step 1760 current loss 0.020172, current_train_items 56352.
I0302 18:58:33.866406 22760421793920 run.py:483] Algo bellman_ford step 1761 current loss 0.040874, current_train_items 56384.
I0302 18:58:33.889889 22760421793920 run.py:483] Algo bellman_ford step 1762 current loss 0.167511, current_train_items 56416.
I0302 18:58:33.920229 22760421793920 run.py:483] Algo bellman_ford step 1763 current loss 0.147650, current_train_items 56448.
I0302 18:58:33.953344 22760421793920 run.py:483] Algo bellman_ford step 1764 current loss 0.231279, current_train_items 56480.
I0302 18:58:33.971175 22760421793920 run.py:483] Algo bellman_ford step 1765 current loss 0.041018, current_train_items 56512.
I0302 18:58:33.986635 22760421793920 run.py:483] Algo bellman_ford step 1766 current loss 0.043799, current_train_items 56544.
I0302 18:58:34.008964 22760421793920 run.py:483] Algo bellman_ford step 1767 current loss 0.120332, current_train_items 56576.
I0302 18:58:34.037433 22760421793920 run.py:483] Algo bellman_ford step 1768 current loss 0.123236, current_train_items 56608.
I0302 18:58:34.067684 22760421793920 run.py:483] Algo bellman_ford step 1769 current loss 0.173403, current_train_items 56640.
I0302 18:58:34.085560 22760421793920 run.py:483] Algo bellman_ford step 1770 current loss 0.021399, current_train_items 56672.
I0302 18:58:34.101404 22760421793920 run.py:483] Algo bellman_ford step 1771 current loss 0.049753, current_train_items 56704.
I0302 18:58:34.125066 22760421793920 run.py:483] Algo bellman_ford step 1772 current loss 0.123998, current_train_items 56736.
I0302 18:58:34.154450 22760421793920 run.py:483] Algo bellman_ford step 1773 current loss 0.125166, current_train_items 56768.
I0302 18:58:34.184783 22760421793920 run.py:483] Algo bellman_ford step 1774 current loss 0.181019, current_train_items 56800.
I0302 18:58:34.202478 22760421793920 run.py:483] Algo bellman_ford step 1775 current loss 0.015821, current_train_items 56832.
I0302 18:58:34.218277 22760421793920 run.py:483] Algo bellman_ford step 1776 current loss 0.077409, current_train_items 56864.
I0302 18:58:34.241328 22760421793920 run.py:483] Algo bellman_ford step 1777 current loss 0.136432, current_train_items 56896.
I0302 18:58:34.269989 22760421793920 run.py:483] Algo bellman_ford step 1778 current loss 0.148689, current_train_items 56928.
I0302 18:58:34.299385 22760421793920 run.py:483] Algo bellman_ford step 1779 current loss 0.138957, current_train_items 56960.
I0302 18:58:34.317387 22760421793920 run.py:483] Algo bellman_ford step 1780 current loss 0.006611, current_train_items 56992.
I0302 18:58:34.333026 22760421793920 run.py:483] Algo bellman_ford step 1781 current loss 0.055756, current_train_items 57024.
I0302 18:58:34.355604 22760421793920 run.py:483] Algo bellman_ford step 1782 current loss 0.117942, current_train_items 57056.
I0302 18:58:34.384640 22760421793920 run.py:483] Algo bellman_ford step 1783 current loss 0.180629, current_train_items 57088.
I0302 18:58:34.415035 22760421793920 run.py:483] Algo bellman_ford step 1784 current loss 0.145176, current_train_items 57120.
I0302 18:58:34.432819 22760421793920 run.py:483] Algo bellman_ford step 1785 current loss 0.006209, current_train_items 57152.
I0302 18:58:34.448710 22760421793920 run.py:483] Algo bellman_ford step 1786 current loss 0.054117, current_train_items 57184.
I0302 18:58:34.470759 22760421793920 run.py:483] Algo bellman_ford step 1787 current loss 0.103038, current_train_items 57216.
I0302 18:58:34.498614 22760421793920 run.py:483] Algo bellman_ford step 1788 current loss 0.099414, current_train_items 57248.
I0302 18:58:34.529909 22760421793920 run.py:483] Algo bellman_ford step 1789 current loss 0.181253, current_train_items 57280.
I0302 18:58:34.547770 22760421793920 run.py:483] Algo bellman_ford step 1790 current loss 0.019025, current_train_items 57312.
I0302 18:58:34.564124 22760421793920 run.py:483] Algo bellman_ford step 1791 current loss 0.092332, current_train_items 57344.
I0302 18:58:34.587047 22760421793920 run.py:483] Algo bellman_ford step 1792 current loss 0.088271, current_train_items 57376.
I0302 18:58:34.614477 22760421793920 run.py:483] Algo bellman_ford step 1793 current loss 0.114052, current_train_items 57408.
I0302 18:58:34.645338 22760421793920 run.py:483] Algo bellman_ford step 1794 current loss 0.160817, current_train_items 57440.
I0302 18:58:34.663297 22760421793920 run.py:483] Algo bellman_ford step 1795 current loss 0.011285, current_train_items 57472.
I0302 18:58:34.678858 22760421793920 run.py:483] Algo bellman_ford step 1796 current loss 0.033060, current_train_items 57504.
I0302 18:58:34.702313 22760421793920 run.py:483] Algo bellman_ford step 1797 current loss 0.097216, current_train_items 57536.
I0302 18:58:34.731466 22760421793920 run.py:483] Algo bellman_ford step 1798 current loss 0.146792, current_train_items 57568.
I0302 18:58:34.763276 22760421793920 run.py:483] Algo bellman_ford step 1799 current loss 0.207861, current_train_items 57600.
I0302 18:58:34.781215 22760421793920 run.py:483] Algo bellman_ford step 1800 current loss 0.039024, current_train_items 57632.
I0302 18:58:34.788799 22760421793920 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:58:34.788916 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.980, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:34.804977 22760421793920 run.py:483] Algo bellman_ford step 1801 current loss 0.027704, current_train_items 57664.
I0302 18:58:34.828291 22760421793920 run.py:483] Algo bellman_ford step 1802 current loss 0.093121, current_train_items 57696.
I0302 18:58:34.856354 22760421793920 run.py:483] Algo bellman_ford step 1803 current loss 0.110217, current_train_items 57728.
I0302 18:58:34.886993 22760421793920 run.py:483] Algo bellman_ford step 1804 current loss 0.249668, current_train_items 57760.
I0302 18:58:34.904969 22760421793920 run.py:483] Algo bellman_ford step 1805 current loss 0.003970, current_train_items 57792.
I0302 18:58:34.920498 22760421793920 run.py:483] Algo bellman_ford step 1806 current loss 0.029883, current_train_items 57824.
I0302 18:58:34.942687 22760421793920 run.py:483] Algo bellman_ford step 1807 current loss 0.064003, current_train_items 57856.
I0302 18:58:34.970083 22760421793920 run.py:483] Algo bellman_ford step 1808 current loss 0.090961, current_train_items 57888.
I0302 18:58:35.002420 22760421793920 run.py:483] Algo bellman_ford step 1809 current loss 0.217097, current_train_items 57920.
I0302 18:58:35.019983 22760421793920 run.py:483] Algo bellman_ford step 1810 current loss 0.010440, current_train_items 57952.
I0302 18:58:35.035530 22760421793920 run.py:483] Algo bellman_ford step 1811 current loss 0.060360, current_train_items 57984.
I0302 18:58:35.058480 22760421793920 run.py:483] Algo bellman_ford step 1812 current loss 0.111176, current_train_items 58016.
I0302 18:58:35.085133 22760421793920 run.py:483] Algo bellman_ford step 1813 current loss 0.079271, current_train_items 58048.
I0302 18:58:35.115874 22760421793920 run.py:483] Algo bellman_ford step 1814 current loss 0.130087, current_train_items 58080.
I0302 18:58:35.133832 22760421793920 run.py:483] Algo bellman_ford step 1815 current loss 0.003362, current_train_items 58112.
I0302 18:58:35.149953 22760421793920 run.py:483] Algo bellman_ford step 1816 current loss 0.079323, current_train_items 58144.
I0302 18:58:35.172099 22760421793920 run.py:483] Algo bellman_ford step 1817 current loss 0.093790, current_train_items 58176.
I0302 18:58:35.200632 22760421793920 run.py:483] Algo bellman_ford step 1818 current loss 0.092356, current_train_items 58208.
I0302 18:58:35.231578 22760421793920 run.py:483] Algo bellman_ford step 1819 current loss 0.126259, current_train_items 58240.
I0302 18:58:35.249459 22760421793920 run.py:483] Algo bellman_ford step 1820 current loss 0.013042, current_train_items 58272.
I0302 18:58:35.265584 22760421793920 run.py:483] Algo bellman_ford step 1821 current loss 0.091215, current_train_items 58304.
I0302 18:58:35.290398 22760421793920 run.py:483] Algo bellman_ford step 1822 current loss 0.132313, current_train_items 58336.
I0302 18:58:35.319363 22760421793920 run.py:483] Algo bellman_ford step 1823 current loss 0.094125, current_train_items 58368.
I0302 18:58:35.349112 22760421793920 run.py:483] Algo bellman_ford step 1824 current loss 0.150599, current_train_items 58400.
I0302 18:58:35.366692 22760421793920 run.py:483] Algo bellman_ford step 1825 current loss 0.004703, current_train_items 58432.
I0302 18:58:35.382583 22760421793920 run.py:483] Algo bellman_ford step 1826 current loss 0.080680, current_train_items 58464.
I0302 18:58:35.404871 22760421793920 run.py:483] Algo bellman_ford step 1827 current loss 0.118682, current_train_items 58496.
I0302 18:58:35.434445 22760421793920 run.py:483] Algo bellman_ford step 1828 current loss 0.231995, current_train_items 58528.
I0302 18:58:35.467974 22760421793920 run.py:483] Algo bellman_ford step 1829 current loss 0.270818, current_train_items 58560.
I0302 18:58:35.485905 22760421793920 run.py:483] Algo bellman_ford step 1830 current loss 0.020418, current_train_items 58592.
I0302 18:58:35.501725 22760421793920 run.py:483] Algo bellman_ford step 1831 current loss 0.118935, current_train_items 58624.
I0302 18:58:35.525972 22760421793920 run.py:483] Algo bellman_ford step 1832 current loss 0.102791, current_train_items 58656.
I0302 18:58:35.554664 22760421793920 run.py:483] Algo bellman_ford step 1833 current loss 0.122240, current_train_items 58688.
I0302 18:58:35.585040 22760421793920 run.py:483] Algo bellman_ford step 1834 current loss 0.134093, current_train_items 58720.
I0302 18:58:35.602911 22760421793920 run.py:483] Algo bellman_ford step 1835 current loss 0.021243, current_train_items 58752.
I0302 18:58:35.618808 22760421793920 run.py:483] Algo bellman_ford step 1836 current loss 0.119829, current_train_items 58784.
I0302 18:58:35.642561 22760421793920 run.py:483] Algo bellman_ford step 1837 current loss 0.234313, current_train_items 58816.
I0302 18:58:35.670858 22760421793920 run.py:483] Algo bellman_ford step 1838 current loss 0.243001, current_train_items 58848.
I0302 18:58:35.702201 22760421793920 run.py:483] Algo bellman_ford step 1839 current loss 0.286473, current_train_items 58880.
I0302 18:58:35.720157 22760421793920 run.py:483] Algo bellman_ford step 1840 current loss 0.012457, current_train_items 58912.
I0302 18:58:35.736068 22760421793920 run.py:483] Algo bellman_ford step 1841 current loss 0.056591, current_train_items 58944.
I0302 18:58:35.759295 22760421793920 run.py:483] Algo bellman_ford step 1842 current loss 0.190491, current_train_items 58976.
I0302 18:58:35.788397 22760421793920 run.py:483] Algo bellman_ford step 1843 current loss 0.182349, current_train_items 59008.
I0302 18:58:35.819194 22760421793920 run.py:483] Algo bellman_ford step 1844 current loss 0.229176, current_train_items 59040.
I0302 18:58:35.837100 22760421793920 run.py:483] Algo bellman_ford step 1845 current loss 0.007495, current_train_items 59072.
I0302 18:58:35.852738 22760421793920 run.py:483] Algo bellman_ford step 1846 current loss 0.028034, current_train_items 59104.
I0302 18:58:35.875940 22760421793920 run.py:483] Algo bellman_ford step 1847 current loss 0.075921, current_train_items 59136.
I0302 18:58:35.904492 22760421793920 run.py:483] Algo bellman_ford step 1848 current loss 0.103850, current_train_items 59168.
I0302 18:58:35.936842 22760421793920 run.py:483] Algo bellman_ford step 1849 current loss 0.132931, current_train_items 59200.
I0302 18:58:35.954748 22760421793920 run.py:483] Algo bellman_ford step 1850 current loss 0.005940, current_train_items 59232.
I0302 18:58:35.962307 22760421793920 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:58:35.962417 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.980, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:58:35.978924 22760421793920 run.py:483] Algo bellman_ford step 1851 current loss 0.035039, current_train_items 59264.
I0302 18:58:36.002330 22760421793920 run.py:483] Algo bellman_ford step 1852 current loss 0.073920, current_train_items 59296.
I0302 18:58:36.031482 22760421793920 run.py:483] Algo bellman_ford step 1853 current loss 0.090660, current_train_items 59328.
I0302 18:58:36.063000 22760421793920 run.py:483] Algo bellman_ford step 1854 current loss 0.188306, current_train_items 59360.
I0302 18:58:36.081105 22760421793920 run.py:483] Algo bellman_ford step 1855 current loss 0.017349, current_train_items 59392.
I0302 18:58:36.096662 22760421793920 run.py:483] Algo bellman_ford step 1856 current loss 0.028956, current_train_items 59424.
I0302 18:58:36.119735 22760421793920 run.py:483] Algo bellman_ford step 1857 current loss 0.079673, current_train_items 59456.
I0302 18:58:36.147550 22760421793920 run.py:483] Algo bellman_ford step 1858 current loss 0.120944, current_train_items 59488.
I0302 18:58:36.178988 22760421793920 run.py:483] Algo bellman_ford step 1859 current loss 0.129822, current_train_items 59520.
I0302 18:58:36.197037 22760421793920 run.py:483] Algo bellman_ford step 1860 current loss 0.004959, current_train_items 59552.
I0302 18:58:36.213025 22760421793920 run.py:483] Algo bellman_ford step 1861 current loss 0.030456, current_train_items 59584.
I0302 18:58:36.235605 22760421793920 run.py:483] Algo bellman_ford step 1862 current loss 0.118128, current_train_items 59616.
I0302 18:58:36.263838 22760421793920 run.py:483] Algo bellman_ford step 1863 current loss 0.092640, current_train_items 59648.
I0302 18:58:36.295586 22760421793920 run.py:483] Algo bellman_ford step 1864 current loss 0.186976, current_train_items 59680.
I0302 18:58:36.313575 22760421793920 run.py:483] Algo bellman_ford step 1865 current loss 0.008254, current_train_items 59712.
I0302 18:58:36.329426 22760421793920 run.py:483] Algo bellman_ford step 1866 current loss 0.036385, current_train_items 59744.
I0302 18:58:36.353068 22760421793920 run.py:483] Algo bellman_ford step 1867 current loss 0.086772, current_train_items 59776.
I0302 18:58:36.382068 22760421793920 run.py:483] Algo bellman_ford step 1868 current loss 0.115236, current_train_items 59808.
I0302 18:58:36.413009 22760421793920 run.py:483] Algo bellman_ford step 1869 current loss 0.151718, current_train_items 59840.
I0302 18:58:36.430656 22760421793920 run.py:483] Algo bellman_ford step 1870 current loss 0.008484, current_train_items 59872.
I0302 18:58:36.446222 22760421793920 run.py:483] Algo bellman_ford step 1871 current loss 0.021816, current_train_items 59904.
I0302 18:58:36.468826 22760421793920 run.py:483] Algo bellman_ford step 1872 current loss 0.033290, current_train_items 59936.
I0302 18:58:36.497449 22760421793920 run.py:483] Algo bellman_ford step 1873 current loss 0.089468, current_train_items 59968.
I0302 18:58:36.528117 22760421793920 run.py:483] Algo bellman_ford step 1874 current loss 0.115597, current_train_items 60000.
I0302 18:58:36.545665 22760421793920 run.py:483] Algo bellman_ford step 1875 current loss 0.023327, current_train_items 60032.
I0302 18:58:36.561301 22760421793920 run.py:483] Algo bellman_ford step 1876 current loss 0.039099, current_train_items 60064.
I0302 18:58:36.584311 22760421793920 run.py:483] Algo bellman_ford step 1877 current loss 0.116335, current_train_items 60096.
I0302 18:58:36.611644 22760421793920 run.py:483] Algo bellman_ford step 1878 current loss 0.070712, current_train_items 60128.
I0302 18:58:36.645178 22760421793920 run.py:483] Algo bellman_ford step 1879 current loss 0.187400, current_train_items 60160.
I0302 18:58:36.663017 22760421793920 run.py:483] Algo bellman_ford step 1880 current loss 0.022231, current_train_items 60192.
I0302 18:58:36.679124 22760421793920 run.py:483] Algo bellman_ford step 1881 current loss 0.061792, current_train_items 60224.
I0302 18:58:36.701221 22760421793920 run.py:483] Algo bellman_ford step 1882 current loss 0.109996, current_train_items 60256.
I0302 18:58:36.729786 22760421793920 run.py:483] Algo bellman_ford step 1883 current loss 0.150559, current_train_items 60288.
I0302 18:58:36.761053 22760421793920 run.py:483] Algo bellman_ford step 1884 current loss 0.257451, current_train_items 60320.
I0302 18:58:36.778923 22760421793920 run.py:483] Algo bellman_ford step 1885 current loss 0.084120, current_train_items 60352.
I0302 18:58:36.794919 22760421793920 run.py:483] Algo bellman_ford step 1886 current loss 0.065334, current_train_items 60384.
I0302 18:58:36.817821 22760421793920 run.py:483] Algo bellman_ford step 1887 current loss 0.104766, current_train_items 60416.
I0302 18:58:36.845563 22760421793920 run.py:483] Algo bellman_ford step 1888 current loss 0.112029, current_train_items 60448.
I0302 18:58:36.878475 22760421793920 run.py:483] Algo bellman_ford step 1889 current loss 0.165543, current_train_items 60480.
I0302 18:58:36.896271 22760421793920 run.py:483] Algo bellman_ford step 1890 current loss 0.005556, current_train_items 60512.
I0302 18:58:36.912006 22760421793920 run.py:483] Algo bellman_ford step 1891 current loss 0.055822, current_train_items 60544.
I0302 18:58:36.934434 22760421793920 run.py:483] Algo bellman_ford step 1892 current loss 0.081209, current_train_items 60576.
I0302 18:58:36.963708 22760421793920 run.py:483] Algo bellman_ford step 1893 current loss 0.122618, current_train_items 60608.
I0302 18:58:36.997906 22760421793920 run.py:483] Algo bellman_ford step 1894 current loss 0.215058, current_train_items 60640.
I0302 18:58:37.015746 22760421793920 run.py:483] Algo bellman_ford step 1895 current loss 0.025614, current_train_items 60672.
I0302 18:58:37.031492 22760421793920 run.py:483] Algo bellman_ford step 1896 current loss 0.027451, current_train_items 60704.
I0302 18:58:37.054343 22760421793920 run.py:483] Algo bellman_ford step 1897 current loss 0.111375, current_train_items 60736.
I0302 18:58:37.083463 22760421793920 run.py:483] Algo bellman_ford step 1898 current loss 0.127998, current_train_items 60768.
I0302 18:58:37.114875 22760421793920 run.py:483] Algo bellman_ford step 1899 current loss 0.160147, current_train_items 60800.
I0302 18:58:37.133008 22760421793920 run.py:483] Algo bellman_ford step 1900 current loss 0.009559, current_train_items 60832.
I0302 18:58:37.140443 22760421793920 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:58:37.140554 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.980, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:58:37.169458 22760421793920 run.py:483] Algo bellman_ford step 1901 current loss 0.030658, current_train_items 60864.
I0302 18:58:37.192022 22760421793920 run.py:483] Algo bellman_ford step 1902 current loss 0.065276, current_train_items 60896.
I0302 18:58:37.219597 22760421793920 run.py:483] Algo bellman_ford step 1903 current loss 0.106216, current_train_items 60928.
I0302 18:58:37.250988 22760421793920 run.py:483] Algo bellman_ford step 1904 current loss 0.141430, current_train_items 60960.
I0302 18:58:37.269360 22760421793920 run.py:483] Algo bellman_ford step 1905 current loss 0.016787, current_train_items 60992.
I0302 18:58:37.284903 22760421793920 run.py:483] Algo bellman_ford step 1906 current loss 0.086176, current_train_items 61024.
I0302 18:58:37.306830 22760421793920 run.py:483] Algo bellman_ford step 1907 current loss 0.114386, current_train_items 61056.
I0302 18:58:37.335191 22760421793920 run.py:483] Algo bellman_ford step 1908 current loss 0.100358, current_train_items 61088.
I0302 18:58:37.365776 22760421793920 run.py:483] Algo bellman_ford step 1909 current loss 0.118930, current_train_items 61120.
I0302 18:58:37.383560 22760421793920 run.py:483] Algo bellman_ford step 1910 current loss 0.023707, current_train_items 61152.
I0302 18:58:37.399448 22760421793920 run.py:483] Algo bellman_ford step 1911 current loss 0.058713, current_train_items 61184.
I0302 18:58:37.422675 22760421793920 run.py:483] Algo bellman_ford step 1912 current loss 0.233866, current_train_items 61216.
I0302 18:58:37.450567 22760421793920 run.py:483] Algo bellman_ford step 1913 current loss 0.208067, current_train_items 61248.
I0302 18:58:37.481925 22760421793920 run.py:483] Algo bellman_ford step 1914 current loss 0.160626, current_train_items 61280.
I0302 18:58:37.499539 22760421793920 run.py:483] Algo bellman_ford step 1915 current loss 0.012153, current_train_items 61312.
I0302 18:58:37.515412 22760421793920 run.py:483] Algo bellman_ford step 1916 current loss 0.072337, current_train_items 61344.
I0302 18:58:37.538428 22760421793920 run.py:483] Algo bellman_ford step 1917 current loss 0.223369, current_train_items 61376.
I0302 18:58:37.567678 22760421793920 run.py:483] Algo bellman_ford step 1918 current loss 0.152399, current_train_items 61408.
I0302 18:58:37.600152 22760421793920 run.py:483] Algo bellman_ford step 1919 current loss 0.211063, current_train_items 61440.
I0302 18:58:37.617968 22760421793920 run.py:483] Algo bellman_ford step 1920 current loss 0.052914, current_train_items 61472.
I0302 18:58:37.633789 22760421793920 run.py:483] Algo bellman_ford step 1921 current loss 0.037891, current_train_items 61504.
I0302 18:58:37.656099 22760421793920 run.py:483] Algo bellman_ford step 1922 current loss 0.135173, current_train_items 61536.
I0302 18:58:37.683639 22760421793920 run.py:483] Algo bellman_ford step 1923 current loss 0.161135, current_train_items 61568.
I0302 18:58:37.714915 22760421793920 run.py:483] Algo bellman_ford step 1924 current loss 0.242935, current_train_items 61600.
I0302 18:58:37.732922 22760421793920 run.py:483] Algo bellman_ford step 1925 current loss 0.009560, current_train_items 61632.
I0302 18:58:37.748709 22760421793920 run.py:483] Algo bellman_ford step 1926 current loss 0.135945, current_train_items 61664.
I0302 18:58:37.770452 22760421793920 run.py:483] Algo bellman_ford step 1927 current loss 0.067381, current_train_items 61696.
I0302 18:58:37.798179 22760421793920 run.py:483] Algo bellman_ford step 1928 current loss 0.102417, current_train_items 61728.
I0302 18:58:37.827560 22760421793920 run.py:483] Algo bellman_ford step 1929 current loss 0.234671, current_train_items 61760.
I0302 18:58:37.845455 22760421793920 run.py:483] Algo bellman_ford step 1930 current loss 0.021864, current_train_items 61792.
I0302 18:58:37.860618 22760421793920 run.py:483] Algo bellman_ford step 1931 current loss 0.079833, current_train_items 61824.
I0302 18:58:37.884500 22760421793920 run.py:483] Algo bellman_ford step 1932 current loss 0.102753, current_train_items 61856.
I0302 18:58:37.912891 22760421793920 run.py:483] Algo bellman_ford step 1933 current loss 0.172494, current_train_items 61888.
I0302 18:58:37.942604 22760421793920 run.py:483] Algo bellman_ford step 1934 current loss 0.180234, current_train_items 61920.
I0302 18:58:37.960380 22760421793920 run.py:483] Algo bellman_ford step 1935 current loss 0.049098, current_train_items 61952.
I0302 18:58:37.976196 22760421793920 run.py:483] Algo bellman_ford step 1936 current loss 0.065974, current_train_items 61984.
I0302 18:58:37.999663 22760421793920 run.py:483] Algo bellman_ford step 1937 current loss 0.085955, current_train_items 62016.
I0302 18:58:38.027383 22760421793920 run.py:483] Algo bellman_ford step 1938 current loss 0.133706, current_train_items 62048.
I0302 18:58:38.058953 22760421793920 run.py:483] Algo bellman_ford step 1939 current loss 0.218698, current_train_items 62080.
I0302 18:58:38.076920 22760421793920 run.py:483] Algo bellman_ford step 1940 current loss 0.036396, current_train_items 62112.
I0302 18:58:38.092698 22760421793920 run.py:483] Algo bellman_ford step 1941 current loss 0.032040, current_train_items 62144.
I0302 18:58:38.115000 22760421793920 run.py:483] Algo bellman_ford step 1942 current loss 0.135997, current_train_items 62176.
I0302 18:58:38.142260 22760421793920 run.py:483] Algo bellman_ford step 1943 current loss 0.100458, current_train_items 62208.
I0302 18:58:38.173678 22760421793920 run.py:483] Algo bellman_ford step 1944 current loss 0.146673, current_train_items 62240.
I0302 18:58:38.191411 22760421793920 run.py:483] Algo bellman_ford step 1945 current loss 0.025481, current_train_items 62272.
I0302 18:58:38.207211 22760421793920 run.py:483] Algo bellman_ford step 1946 current loss 0.053214, current_train_items 62304.
I0302 18:58:38.230325 22760421793920 run.py:483] Algo bellman_ford step 1947 current loss 0.101184, current_train_items 62336.
I0302 18:58:38.259617 22760421793920 run.py:483] Algo bellman_ford step 1948 current loss 0.188997, current_train_items 62368.
I0302 18:58:38.290010 22760421793920 run.py:483] Algo bellman_ford step 1949 current loss 0.220568, current_train_items 62400.
I0302 18:58:38.308032 22760421793920 run.py:483] Algo bellman_ford step 1950 current loss 0.030256, current_train_items 62432.
I0302 18:58:38.315376 22760421793920 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:58:38.315489 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 18:58:38.331964 22760421793920 run.py:483] Algo bellman_ford step 1951 current loss 0.027734, current_train_items 62464.
I0302 18:58:38.355442 22760421793920 run.py:483] Algo bellman_ford step 1952 current loss 0.130374, current_train_items 62496.
I0302 18:58:38.386062 22760421793920 run.py:483] Algo bellman_ford step 1953 current loss 0.165165, current_train_items 62528.
I0302 18:58:38.417329 22760421793920 run.py:483] Algo bellman_ford step 1954 current loss 0.178473, current_train_items 62560.
I0302 18:58:38.435544 22760421793920 run.py:483] Algo bellman_ford step 1955 current loss 0.013396, current_train_items 62592.
I0302 18:58:38.450804 22760421793920 run.py:483] Algo bellman_ford step 1956 current loss 0.058704, current_train_items 62624.
I0302 18:58:38.473345 22760421793920 run.py:483] Algo bellman_ford step 1957 current loss 0.251448, current_train_items 62656.
I0302 18:58:38.502731 22760421793920 run.py:483] Algo bellman_ford step 1958 current loss 0.182875, current_train_items 62688.
I0302 18:58:38.534830 22760421793920 run.py:483] Algo bellman_ford step 1959 current loss 0.148562, current_train_items 62720.
I0302 18:58:38.552684 22760421793920 run.py:483] Algo bellman_ford step 1960 current loss 0.021038, current_train_items 62752.
I0302 18:58:38.568405 22760421793920 run.py:483] Algo bellman_ford step 1961 current loss 0.095175, current_train_items 62784.
I0302 18:58:38.590958 22760421793920 run.py:483] Algo bellman_ford step 1962 current loss 0.181686, current_train_items 62816.
I0302 18:58:38.618331 22760421793920 run.py:483] Algo bellman_ford step 1963 current loss 0.267567, current_train_items 62848.
I0302 18:58:38.651953 22760421793920 run.py:483] Algo bellman_ford step 1964 current loss 0.290015, current_train_items 62880.
I0302 18:58:38.670016 22760421793920 run.py:483] Algo bellman_ford step 1965 current loss 0.027289, current_train_items 62912.
I0302 18:58:38.685565 22760421793920 run.py:483] Algo bellman_ford step 1966 current loss 0.100204, current_train_items 62944.
I0302 18:58:38.708293 22760421793920 run.py:483] Algo bellman_ford step 1967 current loss 0.098716, current_train_items 62976.
I0302 18:58:38.736182 22760421793920 run.py:483] Algo bellman_ford step 1968 current loss 0.111794, current_train_items 63008.
I0302 18:58:38.766964 22760421793920 run.py:483] Algo bellman_ford step 1969 current loss 0.144196, current_train_items 63040.
I0302 18:58:38.784756 22760421793920 run.py:483] Algo bellman_ford step 1970 current loss 0.008964, current_train_items 63072.
I0302 18:58:38.800291 22760421793920 run.py:483] Algo bellman_ford step 1971 current loss 0.051779, current_train_items 63104.
I0302 18:58:38.822730 22760421793920 run.py:483] Algo bellman_ford step 1972 current loss 0.069585, current_train_items 63136.
I0302 18:58:38.851236 22760421793920 run.py:483] Algo bellman_ford step 1973 current loss 0.113696, current_train_items 63168.
I0302 18:58:38.885235 22760421793920 run.py:483] Algo bellman_ford step 1974 current loss 0.179692, current_train_items 63200.
I0302 18:58:38.903159 22760421793920 run.py:483] Algo bellman_ford step 1975 current loss 0.009561, current_train_items 63232.
I0302 18:58:38.918559 22760421793920 run.py:483] Algo bellman_ford step 1976 current loss 0.051880, current_train_items 63264.
I0302 18:58:38.940677 22760421793920 run.py:483] Algo bellman_ford step 1977 current loss 0.071263, current_train_items 63296.
I0302 18:58:38.968417 22760421793920 run.py:483] Algo bellman_ford step 1978 current loss 0.109742, current_train_items 63328.
I0302 18:58:39.000051 22760421793920 run.py:483] Algo bellman_ford step 1979 current loss 0.155879, current_train_items 63360.
I0302 18:58:39.017695 22760421793920 run.py:483] Algo bellman_ford step 1980 current loss 0.011839, current_train_items 63392.
I0302 18:58:39.033542 22760421793920 run.py:483] Algo bellman_ford step 1981 current loss 0.030960, current_train_items 63424.
I0302 18:58:39.055707 22760421793920 run.py:483] Algo bellman_ford step 1982 current loss 0.114940, current_train_items 63456.
I0302 18:58:39.083112 22760421793920 run.py:483] Algo bellman_ford step 1983 current loss 0.077522, current_train_items 63488.
I0302 18:58:39.117252 22760421793920 run.py:483] Algo bellman_ford step 1984 current loss 0.205290, current_train_items 63520.
I0302 18:58:39.134885 22760421793920 run.py:483] Algo bellman_ford step 1985 current loss 0.023690, current_train_items 63552.
I0302 18:58:39.150510 22760421793920 run.py:483] Algo bellman_ford step 1986 current loss 0.069537, current_train_items 63584.
I0302 18:58:39.172148 22760421793920 run.py:483] Algo bellman_ford step 1987 current loss 0.088431, current_train_items 63616.
I0302 18:58:39.199600 22760421793920 run.py:483] Algo bellman_ford step 1988 current loss 0.105829, current_train_items 63648.
I0302 18:58:39.231824 22760421793920 run.py:483] Algo bellman_ford step 1989 current loss 0.158953, current_train_items 63680.
I0302 18:58:39.249634 22760421793920 run.py:483] Algo bellman_ford step 1990 current loss 0.019784, current_train_items 63712.
I0302 18:58:39.265305 22760421793920 run.py:483] Algo bellman_ford step 1991 current loss 0.050537, current_train_items 63744.
I0302 18:58:39.288403 22760421793920 run.py:483] Algo bellman_ford step 1992 current loss 0.069668, current_train_items 63776.
I0302 18:58:39.316788 22760421793920 run.py:483] Algo bellman_ford step 1993 current loss 0.120450, current_train_items 63808.
I0302 18:58:39.344203 22760421793920 run.py:483] Algo bellman_ford step 1994 current loss 0.107965, current_train_items 63840.
I0302 18:58:39.362293 22760421793920 run.py:483] Algo bellman_ford step 1995 current loss 0.023174, current_train_items 63872.
I0302 18:58:39.378581 22760421793920 run.py:483] Algo bellman_ford step 1996 current loss 0.060784, current_train_items 63904.
I0302 18:58:39.400988 22760421793920 run.py:483] Algo bellman_ford step 1997 current loss 0.175261, current_train_items 63936.
I0302 18:58:39.430853 22760421793920 run.py:483] Algo bellman_ford step 1998 current loss 0.117649, current_train_items 63968.
I0302 18:58:39.461229 22760421793920 run.py:483] Algo bellman_ford step 1999 current loss 0.134412, current_train_items 64000.
I0302 18:58:39.479340 22760421793920 run.py:483] Algo bellman_ford step 2000 current loss 0.018486, current_train_items 64032.
I0302 18:58:39.486790 22760421793920 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:58:39.486907 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 18:58:39.503117 22760421793920 run.py:483] Algo bellman_ford step 2001 current loss 0.059967, current_train_items 64064.
I0302 18:58:39.526460 22760421793920 run.py:483] Algo bellman_ford step 2002 current loss 0.192733, current_train_items 64096.
I0302 18:58:39.555647 22760421793920 run.py:483] Algo bellman_ford step 2003 current loss 0.217547, current_train_items 64128.
I0302 18:58:39.587089 22760421793920 run.py:483] Algo bellman_ford step 2004 current loss 0.177047, current_train_items 64160.
I0302 18:58:39.605263 22760421793920 run.py:483] Algo bellman_ford step 2005 current loss 0.017627, current_train_items 64192.
I0302 18:58:39.620939 22760421793920 run.py:483] Algo bellman_ford step 2006 current loss 0.050638, current_train_items 64224.
I0302 18:58:39.643726 22760421793920 run.py:483] Algo bellman_ford step 2007 current loss 0.078009, current_train_items 64256.
I0302 18:58:39.673133 22760421793920 run.py:483] Algo bellman_ford step 2008 current loss 0.147454, current_train_items 64288.
I0302 18:58:39.703984 22760421793920 run.py:483] Algo bellman_ford step 2009 current loss 0.145893, current_train_items 64320.
I0302 18:58:39.721930 22760421793920 run.py:483] Algo bellman_ford step 2010 current loss 0.007416, current_train_items 64352.
I0302 18:58:39.737799 22760421793920 run.py:483] Algo bellman_ford step 2011 current loss 0.029046, current_train_items 64384.
I0302 18:58:39.760557 22760421793920 run.py:483] Algo bellman_ford step 2012 current loss 0.083801, current_train_items 64416.
I0302 18:58:39.788650 22760421793920 run.py:483] Algo bellman_ford step 2013 current loss 0.140760, current_train_items 64448.
I0302 18:58:39.819506 22760421793920 run.py:483] Algo bellman_ford step 2014 current loss 0.153405, current_train_items 64480.
I0302 18:58:39.837109 22760421793920 run.py:483] Algo bellman_ford step 2015 current loss 0.032181, current_train_items 64512.
I0302 18:58:39.853325 22760421793920 run.py:483] Algo bellman_ford step 2016 current loss 0.079442, current_train_items 64544.
I0302 18:58:39.876547 22760421793920 run.py:483] Algo bellman_ford step 2017 current loss 0.143578, current_train_items 64576.
I0302 18:58:39.903031 22760421793920 run.py:483] Algo bellman_ford step 2018 current loss 0.095237, current_train_items 64608.
I0302 18:58:39.935183 22760421793920 run.py:483] Algo bellman_ford step 2019 current loss 0.189956, current_train_items 64640.
I0302 18:58:39.953132 22760421793920 run.py:483] Algo bellman_ford step 2020 current loss 0.024317, current_train_items 64672.
I0302 18:58:39.968822 22760421793920 run.py:483] Algo bellman_ford step 2021 current loss 0.054568, current_train_items 64704.
I0302 18:58:39.991918 22760421793920 run.py:483] Algo bellman_ford step 2022 current loss 0.195091, current_train_items 64736.
I0302 18:58:40.020422 22760421793920 run.py:483] Algo bellman_ford step 2023 current loss 0.241603, current_train_items 64768.
I0302 18:58:40.054332 22760421793920 run.py:483] Algo bellman_ford step 2024 current loss 0.139718, current_train_items 64800.
I0302 18:58:40.072566 22760421793920 run.py:483] Algo bellman_ford step 2025 current loss 0.011793, current_train_items 64832.
I0302 18:58:40.088129 22760421793920 run.py:483] Algo bellman_ford step 2026 current loss 0.049777, current_train_items 64864.
I0302 18:58:40.111592 22760421793920 run.py:483] Algo bellman_ford step 2027 current loss 0.133900, current_train_items 64896.
I0302 18:58:40.139960 22760421793920 run.py:483] Algo bellman_ford step 2028 current loss 0.151045, current_train_items 64928.
I0302 18:58:40.170717 22760421793920 run.py:483] Algo bellman_ford step 2029 current loss 0.185938, current_train_items 64960.
I0302 18:58:40.188843 22760421793920 run.py:483] Algo bellman_ford step 2030 current loss 0.011777, current_train_items 64992.
I0302 18:58:40.205161 22760421793920 run.py:483] Algo bellman_ford step 2031 current loss 0.102685, current_train_items 65024.
I0302 18:58:40.227380 22760421793920 run.py:483] Algo bellman_ford step 2032 current loss 0.082551, current_train_items 65056.
I0302 18:58:40.256949 22760421793920 run.py:483] Algo bellman_ford step 2033 current loss 0.168198, current_train_items 65088.
I0302 18:58:40.287331 22760421793920 run.py:483] Algo bellman_ford step 2034 current loss 0.105341, current_train_items 65120.
I0302 18:58:40.305118 22760421793920 run.py:483] Algo bellman_ford step 2035 current loss 0.010161, current_train_items 65152.
I0302 18:58:40.320621 22760421793920 run.py:483] Algo bellman_ford step 2036 current loss 0.016713, current_train_items 65184.
I0302 18:58:40.342681 22760421793920 run.py:483] Algo bellman_ford step 2037 current loss 0.080609, current_train_items 65216.
I0302 18:58:40.371330 22760421793920 run.py:483] Algo bellman_ford step 2038 current loss 0.124124, current_train_items 65248.
I0302 18:58:40.403012 22760421793920 run.py:483] Algo bellman_ford step 2039 current loss 0.105052, current_train_items 65280.
I0302 18:58:40.421124 22760421793920 run.py:483] Algo bellman_ford step 2040 current loss 0.024795, current_train_items 65312.
I0302 18:58:40.436553 22760421793920 run.py:483] Algo bellman_ford step 2041 current loss 0.056955, current_train_items 65344.
I0302 18:58:40.460192 22760421793920 run.py:483] Algo bellman_ford step 2042 current loss 0.087470, current_train_items 65376.
I0302 18:58:40.489098 22760421793920 run.py:483] Algo bellman_ford step 2043 current loss 0.094640, current_train_items 65408.
I0302 18:58:40.515114 22760421793920 run.py:483] Algo bellman_ford step 2044 current loss 0.061563, current_train_items 65440.
I0302 18:58:40.533252 22760421793920 run.py:483] Algo bellman_ford step 2045 current loss 0.003559, current_train_items 65472.
I0302 18:58:40.549494 22760421793920 run.py:483] Algo bellman_ford step 2046 current loss 0.036919, current_train_items 65504.
I0302 18:58:40.571968 22760421793920 run.py:483] Algo bellman_ford step 2047 current loss 0.112069, current_train_items 65536.
I0302 18:58:40.600539 22760421793920 run.py:483] Algo bellman_ford step 2048 current loss 0.069902, current_train_items 65568.
I0302 18:58:40.629461 22760421793920 run.py:483] Algo bellman_ford step 2049 current loss 0.107710, current_train_items 65600.
I0302 18:58:40.647627 22760421793920 run.py:483] Algo bellman_ford step 2050 current loss 0.015690, current_train_items 65632.
I0302 18:58:40.655017 22760421793920 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:58:40.655172 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:58:40.672223 22760421793920 run.py:483] Algo bellman_ford step 2051 current loss 0.044459, current_train_items 65664.
I0302 18:58:40.694609 22760421793920 run.py:483] Algo bellman_ford step 2052 current loss 0.063386, current_train_items 65696.
I0302 18:58:40.723141 22760421793920 run.py:483] Algo bellman_ford step 2053 current loss 0.091914, current_train_items 65728.
I0302 18:58:40.757336 22760421793920 run.py:483] Algo bellman_ford step 2054 current loss 0.169139, current_train_items 65760.
I0302 18:58:40.775563 22760421793920 run.py:483] Algo bellman_ford step 2055 current loss 0.002856, current_train_items 65792.
I0302 18:58:40.791438 22760421793920 run.py:483] Algo bellman_ford step 2056 current loss 0.039512, current_train_items 65824.
I0302 18:58:40.813593 22760421793920 run.py:483] Algo bellman_ford step 2057 current loss 0.053089, current_train_items 65856.
I0302 18:58:40.841985 22760421793920 run.py:483] Algo bellman_ford step 2058 current loss 0.102548, current_train_items 65888.
I0302 18:58:40.873575 22760421793920 run.py:483] Algo bellman_ford step 2059 current loss 0.223483, current_train_items 65920.
I0302 18:58:40.891407 22760421793920 run.py:483] Algo bellman_ford step 2060 current loss 0.011973, current_train_items 65952.
I0302 18:58:40.907249 22760421793920 run.py:483] Algo bellman_ford step 2061 current loss 0.087468, current_train_items 65984.
I0302 18:58:40.930100 22760421793920 run.py:483] Algo bellman_ford step 2062 current loss 0.142163, current_train_items 66016.
I0302 18:58:40.958945 22760421793920 run.py:483] Algo bellman_ford step 2063 current loss 0.161407, current_train_items 66048.
I0302 18:58:40.991914 22760421793920 run.py:483] Algo bellman_ford step 2064 current loss 0.183779, current_train_items 66080.
I0302 18:58:41.009525 22760421793920 run.py:483] Algo bellman_ford step 2065 current loss 0.010704, current_train_items 66112.
I0302 18:58:41.025290 22760421793920 run.py:483] Algo bellman_ford step 2066 current loss 0.044071, current_train_items 66144.
I0302 18:58:41.048022 22760421793920 run.py:483] Algo bellman_ford step 2067 current loss 0.146020, current_train_items 66176.
I0302 18:58:41.077294 22760421793920 run.py:483] Algo bellman_ford step 2068 current loss 0.152240, current_train_items 66208.
I0302 18:58:41.108349 22760421793920 run.py:483] Algo bellman_ford step 2069 current loss 0.136599, current_train_items 66240.
I0302 18:58:41.126351 22760421793920 run.py:483] Algo bellman_ford step 2070 current loss 0.006467, current_train_items 66272.
I0302 18:58:41.142263 22760421793920 run.py:483] Algo bellman_ford step 2071 current loss 0.057767, current_train_items 66304.
I0302 18:58:41.165130 22760421793920 run.py:483] Algo bellman_ford step 2072 current loss 0.084849, current_train_items 66336.
I0302 18:58:41.194440 22760421793920 run.py:483] Algo bellman_ford step 2073 current loss 0.174322, current_train_items 66368.
I0302 18:58:41.225619 22760421793920 run.py:483] Algo bellman_ford step 2074 current loss 0.283725, current_train_items 66400.
I0302 18:58:41.243475 22760421793920 run.py:483] Algo bellman_ford step 2075 current loss 0.040113, current_train_items 66432.
I0302 18:58:41.259706 22760421793920 run.py:483] Algo bellman_ford step 2076 current loss 0.130366, current_train_items 66464.
I0302 18:58:41.282817 22760421793920 run.py:483] Algo bellman_ford step 2077 current loss 0.288135, current_train_items 66496.
I0302 18:58:41.309477 22760421793920 run.py:483] Algo bellman_ford step 2078 current loss 0.265042, current_train_items 66528.
I0302 18:58:41.340150 22760421793920 run.py:483] Algo bellman_ford step 2079 current loss 0.301741, current_train_items 66560.
I0302 18:58:41.357987 22760421793920 run.py:483] Algo bellman_ford step 2080 current loss 0.053356, current_train_items 66592.
I0302 18:58:41.373772 22760421793920 run.py:483] Algo bellman_ford step 2081 current loss 0.103814, current_train_items 66624.
I0302 18:58:41.396318 22760421793920 run.py:483] Algo bellman_ford step 2082 current loss 0.161160, current_train_items 66656.
I0302 18:58:41.426268 22760421793920 run.py:483] Algo bellman_ford step 2083 current loss 0.196279, current_train_items 66688.
I0302 18:58:41.458089 22760421793920 run.py:483] Algo bellman_ford step 2084 current loss 0.211966, current_train_items 66720.
I0302 18:58:41.476281 22760421793920 run.py:483] Algo bellman_ford step 2085 current loss 0.029262, current_train_items 66752.
I0302 18:58:41.492003 22760421793920 run.py:483] Algo bellman_ford step 2086 current loss 0.069974, current_train_items 66784.
I0302 18:58:41.514963 22760421793920 run.py:483] Algo bellman_ford step 2087 current loss 0.101186, current_train_items 66816.
I0302 18:58:41.541656 22760421793920 run.py:483] Algo bellman_ford step 2088 current loss 0.081960, current_train_items 66848.
I0302 18:58:41.573105 22760421793920 run.py:483] Algo bellman_ford step 2089 current loss 0.175529, current_train_items 66880.
I0302 18:58:41.591254 22760421793920 run.py:483] Algo bellman_ford step 2090 current loss 0.033911, current_train_items 66912.
I0302 18:58:41.606996 22760421793920 run.py:483] Algo bellman_ford step 2091 current loss 0.061726, current_train_items 66944.
I0302 18:58:41.629606 22760421793920 run.py:483] Algo bellman_ford step 2092 current loss 0.090645, current_train_items 66976.
I0302 18:58:41.657866 22760421793920 run.py:483] Algo bellman_ford step 2093 current loss 0.083217, current_train_items 67008.
I0302 18:58:41.689159 22760421793920 run.py:483] Algo bellman_ford step 2094 current loss 0.196158, current_train_items 67040.
I0302 18:58:41.706957 22760421793920 run.py:483] Algo bellman_ford step 2095 current loss 0.013707, current_train_items 67072.
I0302 18:58:41.722783 22760421793920 run.py:483] Algo bellman_ford step 2096 current loss 0.052442, current_train_items 67104.
I0302 18:58:41.745072 22760421793920 run.py:483] Algo bellman_ford step 2097 current loss 0.095414, current_train_items 67136.
I0302 18:58:41.772634 22760421793920 run.py:483] Algo bellman_ford step 2098 current loss 0.069842, current_train_items 67168.
I0302 18:58:41.803117 22760421793920 run.py:483] Algo bellman_ford step 2099 current loss 0.124257, current_train_items 67200.
I0302 18:58:41.821096 22760421793920 run.py:483] Algo bellman_ford step 2100 current loss 0.004357, current_train_items 67232.
I0302 18:58:41.828410 22760421793920 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:58:41.828520 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 18:58:41.844615 22760421793920 run.py:483] Algo bellman_ford step 2101 current loss 0.018011, current_train_items 67264.
I0302 18:58:41.867530 22760421793920 run.py:483] Algo bellman_ford step 2102 current loss 0.096763, current_train_items 67296.
I0302 18:58:41.896488 22760421793920 run.py:483] Algo bellman_ford step 2103 current loss 0.120843, current_train_items 67328.
I0302 18:58:41.927120 22760421793920 run.py:483] Algo bellman_ford step 2104 current loss 0.114636, current_train_items 67360.
I0302 18:58:41.945324 22760421793920 run.py:483] Algo bellman_ford step 2105 current loss 0.016415, current_train_items 67392.
I0302 18:58:41.960184 22760421793920 run.py:483] Algo bellman_ford step 2106 current loss 0.035451, current_train_items 67424.
I0302 18:58:41.983256 22760421793920 run.py:483] Algo bellman_ford step 2107 current loss 0.143313, current_train_items 67456.
I0302 18:58:42.013113 22760421793920 run.py:483] Algo bellman_ford step 2108 current loss 0.136393, current_train_items 67488.
I0302 18:58:42.042731 22760421793920 run.py:483] Algo bellman_ford step 2109 current loss 0.081773, current_train_items 67520.
I0302 18:58:42.060587 22760421793920 run.py:483] Algo bellman_ford step 2110 current loss 0.013501, current_train_items 67552.
I0302 18:58:42.076360 22760421793920 run.py:483] Algo bellman_ford step 2111 current loss 0.059712, current_train_items 67584.
I0302 18:58:42.098311 22760421793920 run.py:483] Algo bellman_ford step 2112 current loss 0.255835, current_train_items 67616.
I0302 18:58:42.125889 22760421793920 run.py:483] Algo bellman_ford step 2113 current loss 0.095599, current_train_items 67648.
I0302 18:58:42.155711 22760421793920 run.py:483] Algo bellman_ford step 2114 current loss 0.172176, current_train_items 67680.
I0302 18:58:42.173539 22760421793920 run.py:483] Algo bellman_ford step 2115 current loss 0.028725, current_train_items 67712.
I0302 18:58:42.189243 22760421793920 run.py:483] Algo bellman_ford step 2116 current loss 0.083627, current_train_items 67744.
I0302 18:58:42.212561 22760421793920 run.py:483] Algo bellman_ford step 2117 current loss 0.192838, current_train_items 67776.
I0302 18:58:42.241619 22760421793920 run.py:483] Algo bellman_ford step 2118 current loss 0.230387, current_train_items 67808.
I0302 18:58:42.271637 22760421793920 run.py:483] Algo bellman_ford step 2119 current loss 0.367478, current_train_items 67840.
I0302 18:58:42.289600 22760421793920 run.py:483] Algo bellman_ford step 2120 current loss 0.005529, current_train_items 67872.
I0302 18:58:42.305050 22760421793920 run.py:483] Algo bellman_ford step 2121 current loss 0.038651, current_train_items 67904.
I0302 18:58:42.327667 22760421793920 run.py:483] Algo bellman_ford step 2122 current loss 0.059261, current_train_items 67936.
I0302 18:58:42.356638 22760421793920 run.py:483] Algo bellman_ford step 2123 current loss 0.093817, current_train_items 67968.
I0302 18:58:42.384142 22760421793920 run.py:483] Algo bellman_ford step 2124 current loss 0.081642, current_train_items 68000.
I0302 18:58:42.401762 22760421793920 run.py:483] Algo bellman_ford step 2125 current loss 0.002714, current_train_items 68032.
I0302 18:58:42.417810 22760421793920 run.py:483] Algo bellman_ford step 2126 current loss 0.138373, current_train_items 68064.
I0302 18:58:42.441717 22760421793920 run.py:483] Algo bellman_ford step 2127 current loss 0.070426, current_train_items 68096.
I0302 18:58:42.469930 22760421793920 run.py:483] Algo bellman_ford step 2128 current loss 0.134028, current_train_items 68128.
I0302 18:58:42.500210 22760421793920 run.py:483] Algo bellman_ford step 2129 current loss 0.118400, current_train_items 68160.
I0302 18:58:42.518157 22760421793920 run.py:483] Algo bellman_ford step 2130 current loss 0.005956, current_train_items 68192.
I0302 18:58:42.534372 22760421793920 run.py:483] Algo bellman_ford step 2131 current loss 0.036258, current_train_items 68224.
I0302 18:58:42.557741 22760421793920 run.py:483] Algo bellman_ford step 2132 current loss 0.131613, current_train_items 68256.
I0302 18:58:42.586074 22760421793920 run.py:483] Algo bellman_ford step 2133 current loss 0.108364, current_train_items 68288.
I0302 18:58:42.618320 22760421793920 run.py:483] Algo bellman_ford step 2134 current loss 0.138613, current_train_items 68320.
I0302 18:58:42.636230 22760421793920 run.py:483] Algo bellman_ford step 2135 current loss 0.005227, current_train_items 68352.
I0302 18:58:42.652110 22760421793920 run.py:483] Algo bellman_ford step 2136 current loss 0.074505, current_train_items 68384.
I0302 18:58:42.674355 22760421793920 run.py:483] Algo bellman_ford step 2137 current loss 0.070322, current_train_items 68416.
I0302 18:58:42.703061 22760421793920 run.py:483] Algo bellman_ford step 2138 current loss 0.124002, current_train_items 68448.
I0302 18:58:42.737155 22760421793920 run.py:483] Algo bellman_ford step 2139 current loss 0.207781, current_train_items 68480.
I0302 18:58:42.754676 22760421793920 run.py:483] Algo bellman_ford step 2140 current loss 0.057264, current_train_items 68512.
I0302 18:58:42.770255 22760421793920 run.py:483] Algo bellman_ford step 2141 current loss 0.032469, current_train_items 68544.
I0302 18:58:42.791296 22760421793920 run.py:483] Algo bellman_ford step 2142 current loss 0.030476, current_train_items 68576.
I0302 18:58:42.819658 22760421793920 run.py:483] Algo bellman_ford step 2143 current loss 0.080390, current_train_items 68608.
I0302 18:58:42.850631 22760421793920 run.py:483] Algo bellman_ford step 2144 current loss 0.159062, current_train_items 68640.
I0302 18:58:42.868532 22760421793920 run.py:483] Algo bellman_ford step 2145 current loss 0.007871, current_train_items 68672.
I0302 18:58:42.884671 22760421793920 run.py:483] Algo bellman_ford step 2146 current loss 0.082288, current_train_items 68704.
I0302 18:58:42.907783 22760421793920 run.py:483] Algo bellman_ford step 2147 current loss 0.148020, current_train_items 68736.
I0302 18:58:42.936745 22760421793920 run.py:483] Algo bellman_ford step 2148 current loss 0.110531, current_train_items 68768.
I0302 18:58:42.966706 22760421793920 run.py:483] Algo bellman_ford step 2149 current loss 0.142998, current_train_items 68800.
I0302 18:58:42.984311 22760421793920 run.py:483] Algo bellman_ford step 2150 current loss 0.006548, current_train_items 68832.
I0302 18:58:42.991729 22760421793920 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:58:42.991842 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:58:43.008323 22760421793920 run.py:483] Algo bellman_ford step 2151 current loss 0.006420, current_train_items 68864.
I0302 18:58:43.031464 22760421793920 run.py:483] Algo bellman_ford step 2152 current loss 0.100657, current_train_items 68896.
I0302 18:58:43.059957 22760421793920 run.py:483] Algo bellman_ford step 2153 current loss 0.139685, current_train_items 68928.
I0302 18:58:43.090780 22760421793920 run.py:483] Algo bellman_ford step 2154 current loss 0.132746, current_train_items 68960.
I0302 18:58:43.108910 22760421793920 run.py:483] Algo bellman_ford step 2155 current loss 0.003348, current_train_items 68992.
I0302 18:58:43.124911 22760421793920 run.py:483] Algo bellman_ford step 2156 current loss 0.024372, current_train_items 69024.
I0302 18:58:43.147778 22760421793920 run.py:483] Algo bellman_ford step 2157 current loss 0.069225, current_train_items 69056.
I0302 18:58:43.177086 22760421793920 run.py:483] Algo bellman_ford step 2158 current loss 0.110615, current_train_items 69088.
I0302 18:58:43.208690 22760421793920 run.py:483] Algo bellman_ford step 2159 current loss 0.176245, current_train_items 69120.
I0302 18:58:43.226770 22760421793920 run.py:483] Algo bellman_ford step 2160 current loss 0.019281, current_train_items 69152.
I0302 18:58:43.242304 22760421793920 run.py:483] Algo bellman_ford step 2161 current loss 0.011379, current_train_items 69184.
I0302 18:58:43.265308 22760421793920 run.py:483] Algo bellman_ford step 2162 current loss 0.119924, current_train_items 69216.
I0302 18:58:43.292711 22760421793920 run.py:483] Algo bellman_ford step 2163 current loss 0.088294, current_train_items 69248.
I0302 18:58:43.324923 22760421793920 run.py:483] Algo bellman_ford step 2164 current loss 0.135348, current_train_items 69280.
I0302 18:58:43.342731 22760421793920 run.py:483] Algo bellman_ford step 2165 current loss 0.006331, current_train_items 69312.
I0302 18:58:43.358963 22760421793920 run.py:483] Algo bellman_ford step 2166 current loss 0.058665, current_train_items 69344.
I0302 18:58:43.381308 22760421793920 run.py:483] Algo bellman_ford step 2167 current loss 0.061240, current_train_items 69376.
I0302 18:58:43.408860 22760421793920 run.py:483] Algo bellman_ford step 2168 current loss 0.095722, current_train_items 69408.
I0302 18:58:43.439960 22760421793920 run.py:483] Algo bellman_ford step 2169 current loss 0.151414, current_train_items 69440.
I0302 18:58:43.457731 22760421793920 run.py:483] Algo bellman_ford step 2170 current loss 0.033312, current_train_items 69472.
I0302 18:58:43.473290 22760421793920 run.py:483] Algo bellman_ford step 2171 current loss 0.047524, current_train_items 69504.
I0302 18:58:43.495966 22760421793920 run.py:483] Algo bellman_ford step 2172 current loss 0.106847, current_train_items 69536.
I0302 18:58:43.525496 22760421793920 run.py:483] Algo bellman_ford step 2173 current loss 0.134086, current_train_items 69568.
I0302 18:58:43.554965 22760421793920 run.py:483] Algo bellman_ford step 2174 current loss 0.143104, current_train_items 69600.
I0302 18:58:43.572884 22760421793920 run.py:483] Algo bellman_ford step 2175 current loss 0.012888, current_train_items 69632.
I0302 18:58:43.588044 22760421793920 run.py:483] Algo bellman_ford step 2176 current loss 0.055445, current_train_items 69664.
I0302 18:58:43.611476 22760421793920 run.py:483] Algo bellman_ford step 2177 current loss 0.134166, current_train_items 69696.
I0302 18:58:43.639113 22760421793920 run.py:483] Algo bellman_ford step 2178 current loss 0.115627, current_train_items 69728.
I0302 18:58:43.670758 22760421793920 run.py:483] Algo bellman_ford step 2179 current loss 0.097003, current_train_items 69760.
I0302 18:58:43.688215 22760421793920 run.py:483] Algo bellman_ford step 2180 current loss 0.022777, current_train_items 69792.
I0302 18:58:43.703947 22760421793920 run.py:483] Algo bellman_ford step 2181 current loss 0.024238, current_train_items 69824.
I0302 18:58:43.726643 22760421793920 run.py:483] Algo bellman_ford step 2182 current loss 0.062491, current_train_items 69856.
I0302 18:58:43.754367 22760421793920 run.py:483] Algo bellman_ford step 2183 current loss 0.112226, current_train_items 69888.
I0302 18:58:43.786528 22760421793920 run.py:483] Algo bellman_ford step 2184 current loss 0.178095, current_train_items 69920.
I0302 18:58:43.804622 22760421793920 run.py:483] Algo bellman_ford step 2185 current loss 0.011494, current_train_items 69952.
I0302 18:58:43.819785 22760421793920 run.py:483] Algo bellman_ford step 2186 current loss 0.018659, current_train_items 69984.
I0302 18:58:43.842639 22760421793920 run.py:483] Algo bellman_ford step 2187 current loss 0.114626, current_train_items 70016.
I0302 18:58:43.871251 22760421793920 run.py:483] Algo bellman_ford step 2188 current loss 0.087483, current_train_items 70048.
W0302 18:58:43.895280 22760421793920 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:58:49.542619 22760421793920 run.py:483] Algo bellman_ford step 2189 current loss 0.144935, current_train_items 70080.
I0302 18:58:49.561609 22760421793920 run.py:483] Algo bellman_ford step 2190 current loss 0.004814, current_train_items 70112.
I0302 18:58:49.577573 22760421793920 run.py:483] Algo bellman_ford step 2191 current loss 0.081127, current_train_items 70144.
I0302 18:58:49.600372 22760421793920 run.py:483] Algo bellman_ford step 2192 current loss 0.282418, current_train_items 70176.
W0302 18:58:49.621589 22760421793920 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:58:55.615299 22760421793920 run.py:483] Algo bellman_ford step 2193 current loss 0.263184, current_train_items 70208.
I0302 18:58:55.646377 22760421793920 run.py:483] Algo bellman_ford step 2194 current loss 0.185216, current_train_items 70240.
I0302 18:58:55.665339 22760421793920 run.py:483] Algo bellman_ford step 2195 current loss 0.056201, current_train_items 70272.
I0302 18:58:55.681605 22760421793920 run.py:483] Algo bellman_ford step 2196 current loss 0.056537, current_train_items 70304.
I0302 18:58:55.704260 22760421793920 run.py:483] Algo bellman_ford step 2197 current loss 0.110488, current_train_items 70336.
I0302 18:58:55.732079 22760421793920 run.py:483] Algo bellman_ford step 2198 current loss 0.292569, current_train_items 70368.
I0302 18:58:55.763418 22760421793920 run.py:483] Algo bellman_ford step 2199 current loss 0.233990, current_train_items 70400.
I0302 18:58:55.781928 22760421793920 run.py:483] Algo bellman_ford step 2200 current loss 0.031536, current_train_items 70432.
I0302 18:58:55.791010 22760421793920 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.791122 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0302 18:58:55.807741 22760421793920 run.py:483] Algo bellman_ford step 2201 current loss 0.041711, current_train_items 70464.
I0302 18:58:55.830120 22760421793920 run.py:483] Algo bellman_ford step 2202 current loss 0.084713, current_train_items 70496.
I0302 18:58:55.858972 22760421793920 run.py:483] Algo bellman_ford step 2203 current loss 0.123165, current_train_items 70528.
I0302 18:58:55.891145 22760421793920 run.py:483] Algo bellman_ford step 2204 current loss 0.112788, current_train_items 70560.
I0302 18:58:55.909746 22760421793920 run.py:483] Algo bellman_ford step 2205 current loss 0.008473, current_train_items 70592.
I0302 18:58:55.925649 22760421793920 run.py:483] Algo bellman_ford step 2206 current loss 0.044130, current_train_items 70624.
I0302 18:58:55.947906 22760421793920 run.py:483] Algo bellman_ford step 2207 current loss 0.066792, current_train_items 70656.
I0302 18:58:55.976906 22760421793920 run.py:483] Algo bellman_ford step 2208 current loss 0.098596, current_train_items 70688.
I0302 18:58:56.009310 22760421793920 run.py:483] Algo bellman_ford step 2209 current loss 0.131171, current_train_items 70720.
I0302 18:58:56.027826 22760421793920 run.py:483] Algo bellman_ford step 2210 current loss 0.019478, current_train_items 70752.
I0302 18:58:56.043537 22760421793920 run.py:483] Algo bellman_ford step 2211 current loss 0.034893, current_train_items 70784.
I0302 18:58:56.066681 22760421793920 run.py:483] Algo bellman_ford step 2212 current loss 0.145231, current_train_items 70816.
I0302 18:58:56.095931 22760421793920 run.py:483] Algo bellman_ford step 2213 current loss 0.261113, current_train_items 70848.
I0302 18:58:56.128523 22760421793920 run.py:483] Algo bellman_ford step 2214 current loss 0.190588, current_train_items 70880.
I0302 18:58:56.147372 22760421793920 run.py:483] Algo bellman_ford step 2215 current loss 0.022162, current_train_items 70912.
I0302 18:58:56.163063 22760421793920 run.py:483] Algo bellman_ford step 2216 current loss 0.029058, current_train_items 70944.
I0302 18:58:56.184422 22760421793920 run.py:483] Algo bellman_ford step 2217 current loss 0.107775, current_train_items 70976.
I0302 18:58:56.214654 22760421793920 run.py:483] Algo bellman_ford step 2218 current loss 0.217198, current_train_items 71008.
I0302 18:58:56.245314 22760421793920 run.py:483] Algo bellman_ford step 2219 current loss 0.192298, current_train_items 71040.
I0302 18:58:56.263401 22760421793920 run.py:483] Algo bellman_ford step 2220 current loss 0.013720, current_train_items 71072.
I0302 18:58:56.279176 22760421793920 run.py:483] Algo bellman_ford step 2221 current loss 0.035427, current_train_items 71104.
I0302 18:58:56.301287 22760421793920 run.py:483] Algo bellman_ford step 2222 current loss 0.097819, current_train_items 71136.
I0302 18:58:56.330848 22760421793920 run.py:483] Algo bellman_ford step 2223 current loss 0.097076, current_train_items 71168.
I0302 18:58:56.363706 22760421793920 run.py:483] Algo bellman_ford step 2224 current loss 0.188833, current_train_items 71200.
I0302 18:58:56.382175 22760421793920 run.py:483] Algo bellman_ford step 2225 current loss 0.032687, current_train_items 71232.
I0302 18:58:56.398053 22760421793920 run.py:483] Algo bellman_ford step 2226 current loss 0.053643, current_train_items 71264.
I0302 18:58:56.420854 22760421793920 run.py:483] Algo bellman_ford step 2227 current loss 0.118666, current_train_items 71296.
I0302 18:58:56.450047 22760421793920 run.py:483] Algo bellman_ford step 2228 current loss 0.115501, current_train_items 71328.
I0302 18:58:56.479442 22760421793920 run.py:483] Algo bellman_ford step 2229 current loss 0.097384, current_train_items 71360.
I0302 18:58:56.497935 22760421793920 run.py:483] Algo bellman_ford step 2230 current loss 0.011966, current_train_items 71392.
I0302 18:58:56.513837 22760421793920 run.py:483] Algo bellman_ford step 2231 current loss 0.057231, current_train_items 71424.
I0302 18:58:56.536619 22760421793920 run.py:483] Algo bellman_ford step 2232 current loss 0.086498, current_train_items 71456.
I0302 18:58:56.565156 22760421793920 run.py:483] Algo bellman_ford step 2233 current loss 0.131792, current_train_items 71488.
I0302 18:58:56.596305 22760421793920 run.py:483] Algo bellman_ford step 2234 current loss 0.170771, current_train_items 71520.
I0302 18:58:56.614683 22760421793920 run.py:483] Algo bellman_ford step 2235 current loss 0.005839, current_train_items 71552.
I0302 18:58:56.630549 22760421793920 run.py:483] Algo bellman_ford step 2236 current loss 0.058921, current_train_items 71584.
I0302 18:58:56.654211 22760421793920 run.py:483] Algo bellman_ford step 2237 current loss 0.092827, current_train_items 71616.
I0302 18:58:56.683541 22760421793920 run.py:483] Algo bellman_ford step 2238 current loss 0.069777, current_train_items 71648.
I0302 18:58:56.714092 22760421793920 run.py:483] Algo bellman_ford step 2239 current loss 0.105322, current_train_items 71680.
I0302 18:58:56.732116 22760421793920 run.py:483] Algo bellman_ford step 2240 current loss 0.040995, current_train_items 71712.
I0302 18:58:56.748252 22760421793920 run.py:483] Algo bellman_ford step 2241 current loss 0.033614, current_train_items 71744.
I0302 18:58:56.771734 22760421793920 run.py:483] Algo bellman_ford step 2242 current loss 0.071292, current_train_items 71776.
I0302 18:58:56.801240 22760421793920 run.py:483] Algo bellman_ford step 2243 current loss 0.100081, current_train_items 71808.
I0302 18:58:56.830994 22760421793920 run.py:483] Algo bellman_ford step 2244 current loss 0.100694, current_train_items 71840.
I0302 18:58:56.849318 22760421793920 run.py:483] Algo bellman_ford step 2245 current loss 0.048961, current_train_items 71872.
I0302 18:58:56.865292 22760421793920 run.py:483] Algo bellman_ford step 2246 current loss 0.046196, current_train_items 71904.
I0302 18:58:56.887145 22760421793920 run.py:483] Algo bellman_ford step 2247 current loss 0.068978, current_train_items 71936.
I0302 18:58:56.917433 22760421793920 run.py:483] Algo bellman_ford step 2248 current loss 0.060657, current_train_items 71968.
I0302 18:58:56.949369 22760421793920 run.py:483] Algo bellman_ford step 2249 current loss 0.088833, current_train_items 72000.
I0302 18:58:56.967569 22760421793920 run.py:483] Algo bellman_ford step 2250 current loss 0.008695, current_train_items 72032.
I0302 18:58:56.975415 22760421793920 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.975526 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:56.991605 22760421793920 run.py:483] Algo bellman_ford step 2251 current loss 0.039146, current_train_items 72064.
I0302 18:58:57.014018 22760421793920 run.py:483] Algo bellman_ford step 2252 current loss 0.070592, current_train_items 72096.
I0302 18:58:57.045013 22760421793920 run.py:483] Algo bellman_ford step 2253 current loss 0.174077, current_train_items 72128.
I0302 18:58:57.077699 22760421793920 run.py:483] Algo bellman_ford step 2254 current loss 0.181308, current_train_items 72160.
I0302 18:58:57.096674 22760421793920 run.py:483] Algo bellman_ford step 2255 current loss 0.015568, current_train_items 72192.
I0302 18:58:57.112871 22760421793920 run.py:483] Algo bellman_ford step 2256 current loss 0.031241, current_train_items 72224.
I0302 18:58:57.135238 22760421793920 run.py:483] Algo bellman_ford step 2257 current loss 0.100149, current_train_items 72256.
I0302 18:58:57.163350 22760421793920 run.py:483] Algo bellman_ford step 2258 current loss 0.109969, current_train_items 72288.
I0302 18:58:57.196358 22760421793920 run.py:483] Algo bellman_ford step 2259 current loss 0.139935, current_train_items 72320.
I0302 18:58:57.214958 22760421793920 run.py:483] Algo bellman_ford step 2260 current loss 0.030431, current_train_items 72352.
I0302 18:58:57.230987 22760421793920 run.py:483] Algo bellman_ford step 2261 current loss 0.070537, current_train_items 72384.
I0302 18:58:57.255049 22760421793920 run.py:483] Algo bellman_ford step 2262 current loss 0.214891, current_train_items 72416.
I0302 18:58:57.283537 22760421793920 run.py:483] Algo bellman_ford step 2263 current loss 0.130005, current_train_items 72448.
I0302 18:58:57.315460 22760421793920 run.py:483] Algo bellman_ford step 2264 current loss 0.195383, current_train_items 72480.
I0302 18:58:57.334066 22760421793920 run.py:483] Algo bellman_ford step 2265 current loss 0.030121, current_train_items 72512.
I0302 18:58:57.349812 22760421793920 run.py:483] Algo bellman_ford step 2266 current loss 0.085187, current_train_items 72544.
I0302 18:58:57.373512 22760421793920 run.py:483] Algo bellman_ford step 2267 current loss 0.272339, current_train_items 72576.
I0302 18:58:57.402831 22760421793920 run.py:483] Algo bellman_ford step 2268 current loss 0.255691, current_train_items 72608.
I0302 18:58:57.434196 22760421793920 run.py:483] Algo bellman_ford step 2269 current loss 0.230499, current_train_items 72640.
I0302 18:58:57.452568 22760421793920 run.py:483] Algo bellman_ford step 2270 current loss 0.004180, current_train_items 72672.
I0302 18:58:57.468093 22760421793920 run.py:483] Algo bellman_ford step 2271 current loss 0.022207, current_train_items 72704.
I0302 18:58:57.489995 22760421793920 run.py:483] Algo bellman_ford step 2272 current loss 0.080888, current_train_items 72736.
I0302 18:58:57.520209 22760421793920 run.py:483] Algo bellman_ford step 2273 current loss 0.115074, current_train_items 72768.
I0302 18:58:57.554131 22760421793920 run.py:483] Algo bellman_ford step 2274 current loss 0.312170, current_train_items 72800.
I0302 18:58:57.572699 22760421793920 run.py:483] Algo bellman_ford step 2275 current loss 0.008134, current_train_items 72832.
I0302 18:58:57.588234 22760421793920 run.py:483] Algo bellman_ford step 2276 current loss 0.023606, current_train_items 72864.
I0302 18:58:57.609553 22760421793920 run.py:483] Algo bellman_ford step 2277 current loss 0.054204, current_train_items 72896.
I0302 18:58:57.638184 22760421793920 run.py:483] Algo bellman_ford step 2278 current loss 0.098342, current_train_items 72928.
I0302 18:58:57.671247 22760421793920 run.py:483] Algo bellman_ford step 2279 current loss 0.140910, current_train_items 72960.
I0302 18:58:57.689729 22760421793920 run.py:483] Algo bellman_ford step 2280 current loss 0.004052, current_train_items 72992.
I0302 18:58:57.705249 22760421793920 run.py:483] Algo bellman_ford step 2281 current loss 0.067157, current_train_items 73024.
I0302 18:58:57.728718 22760421793920 run.py:483] Algo bellman_ford step 2282 current loss 0.066863, current_train_items 73056.
I0302 18:58:57.760819 22760421793920 run.py:483] Algo bellman_ford step 2283 current loss 0.176597, current_train_items 73088.
I0302 18:58:57.793648 22760421793920 run.py:483] Algo bellman_ford step 2284 current loss 0.116471, current_train_items 73120.
I0302 18:58:57.812011 22760421793920 run.py:483] Algo bellman_ford step 2285 current loss 0.016393, current_train_items 73152.
I0302 18:58:57.827785 22760421793920 run.py:483] Algo bellman_ford step 2286 current loss 0.044478, current_train_items 73184.
I0302 18:58:57.850502 22760421793920 run.py:483] Algo bellman_ford step 2287 current loss 0.064895, current_train_items 73216.
I0302 18:58:57.879394 22760421793920 run.py:483] Algo bellman_ford step 2288 current loss 0.080229, current_train_items 73248.
I0302 18:58:57.913320 22760421793920 run.py:483] Algo bellman_ford step 2289 current loss 0.166847, current_train_items 73280.
I0302 18:58:57.931487 22760421793920 run.py:483] Algo bellman_ford step 2290 current loss 0.017657, current_train_items 73312.
I0302 18:58:57.946597 22760421793920 run.py:483] Algo bellman_ford step 2291 current loss 0.053418, current_train_items 73344.
I0302 18:58:57.969199 22760421793920 run.py:483] Algo bellman_ford step 2292 current loss 0.142032, current_train_items 73376.
I0302 18:58:57.999292 22760421793920 run.py:483] Algo bellman_ford step 2293 current loss 0.122658, current_train_items 73408.
I0302 18:58:58.031670 22760421793920 run.py:483] Algo bellman_ford step 2294 current loss 0.246833, current_train_items 73440.
I0302 18:58:58.049599 22760421793920 run.py:483] Algo bellman_ford step 2295 current loss 0.028628, current_train_items 73472.
I0302 18:58:58.065398 22760421793920 run.py:483] Algo bellman_ford step 2296 current loss 0.078764, current_train_items 73504.
I0302 18:58:58.087234 22760421793920 run.py:483] Algo bellman_ford step 2297 current loss 0.080255, current_train_items 73536.
I0302 18:58:58.115944 22760421793920 run.py:483] Algo bellman_ford step 2298 current loss 0.137847, current_train_items 73568.
I0302 18:58:58.147660 22760421793920 run.py:483] Algo bellman_ford step 2299 current loss 0.133236, current_train_items 73600.
I0302 18:58:58.165935 22760421793920 run.py:483] Algo bellman_ford step 2300 current loss 0.022621, current_train_items 73632.
I0302 18:58:58.173377 22760421793920 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:58:58.173489 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 18:58:58.190135 22760421793920 run.py:483] Algo bellman_ford step 2301 current loss 0.094705, current_train_items 73664.
I0302 18:58:58.213289 22760421793920 run.py:483] Algo bellman_ford step 2302 current loss 0.121182, current_train_items 73696.
I0302 18:58:58.244138 22760421793920 run.py:483] Algo bellman_ford step 2303 current loss 0.160742, current_train_items 73728.
I0302 18:58:58.276579 22760421793920 run.py:483] Algo bellman_ford step 2304 current loss 0.109596, current_train_items 73760.
I0302 18:58:58.295197 22760421793920 run.py:483] Algo bellman_ford step 2305 current loss 0.026899, current_train_items 73792.
I0302 18:58:58.311352 22760421793920 run.py:483] Algo bellman_ford step 2306 current loss 0.046826, current_train_items 73824.
I0302 18:58:58.333996 22760421793920 run.py:483] Algo bellman_ford step 2307 current loss 0.158650, current_train_items 73856.
I0302 18:58:58.362456 22760421793920 run.py:483] Algo bellman_ford step 2308 current loss 0.163054, current_train_items 73888.
I0302 18:58:58.394747 22760421793920 run.py:483] Algo bellman_ford step 2309 current loss 0.233315, current_train_items 73920.
I0302 18:58:58.413302 22760421793920 run.py:483] Algo bellman_ford step 2310 current loss 0.011481, current_train_items 73952.
I0302 18:58:58.429100 22760421793920 run.py:483] Algo bellman_ford step 2311 current loss 0.045758, current_train_items 73984.
I0302 18:58:58.451540 22760421793920 run.py:483] Algo bellman_ford step 2312 current loss 0.097285, current_train_items 74016.
I0302 18:58:58.480786 22760421793920 run.py:483] Algo bellman_ford step 2313 current loss 0.119670, current_train_items 74048.
I0302 18:58:58.513382 22760421793920 run.py:483] Algo bellman_ford step 2314 current loss 0.117488, current_train_items 74080.
I0302 18:58:58.531737 22760421793920 run.py:483] Algo bellman_ford step 2315 current loss 0.019984, current_train_items 74112.
I0302 18:58:58.547340 22760421793920 run.py:483] Algo bellman_ford step 2316 current loss 0.091612, current_train_items 74144.
I0302 18:58:58.569203 22760421793920 run.py:483] Algo bellman_ford step 2317 current loss 0.062743, current_train_items 74176.
I0302 18:58:58.599454 22760421793920 run.py:483] Algo bellman_ford step 2318 current loss 0.074599, current_train_items 74208.
I0302 18:58:58.632148 22760421793920 run.py:483] Algo bellman_ford step 2319 current loss 0.138617, current_train_items 74240.
I0302 18:58:58.650588 22760421793920 run.py:483] Algo bellman_ford step 2320 current loss 0.022515, current_train_items 74272.
I0302 18:58:58.666076 22760421793920 run.py:483] Algo bellman_ford step 2321 current loss 0.013881, current_train_items 74304.
I0302 18:58:58.689432 22760421793920 run.py:483] Algo bellman_ford step 2322 current loss 0.081580, current_train_items 74336.
I0302 18:58:58.718340 22760421793920 run.py:483] Algo bellman_ford step 2323 current loss 0.097307, current_train_items 74368.
I0302 18:58:58.750823 22760421793920 run.py:483] Algo bellman_ford step 2324 current loss 0.136098, current_train_items 74400.
I0302 18:58:58.769309 22760421793920 run.py:483] Algo bellman_ford step 2325 current loss 0.017739, current_train_items 74432.
I0302 18:58:58.784840 22760421793920 run.py:483] Algo bellman_ford step 2326 current loss 0.044821, current_train_items 74464.
I0302 18:58:58.806686 22760421793920 run.py:483] Algo bellman_ford step 2327 current loss 0.099187, current_train_items 74496.
I0302 18:58:58.834814 22760421793920 run.py:483] Algo bellman_ford step 2328 current loss 0.080923, current_train_items 74528.
I0302 18:58:58.865386 22760421793920 run.py:483] Algo bellman_ford step 2329 current loss 0.096158, current_train_items 74560.
I0302 18:58:58.883888 22760421793920 run.py:483] Algo bellman_ford step 2330 current loss 0.012705, current_train_items 74592.
I0302 18:58:58.899198 22760421793920 run.py:483] Algo bellman_ford step 2331 current loss 0.053792, current_train_items 74624.
I0302 18:58:58.922080 22760421793920 run.py:483] Algo bellman_ford step 2332 current loss 0.130044, current_train_items 74656.
I0302 18:58:58.952000 22760421793920 run.py:483] Algo bellman_ford step 2333 current loss 0.105716, current_train_items 74688.
I0302 18:58:58.985084 22760421793920 run.py:483] Algo bellman_ford step 2334 current loss 0.103570, current_train_items 74720.
I0302 18:58:59.003150 22760421793920 run.py:483] Algo bellman_ford step 2335 current loss 0.009908, current_train_items 74752.
I0302 18:58:59.019380 22760421793920 run.py:483] Algo bellman_ford step 2336 current loss 0.032516, current_train_items 74784.
I0302 18:58:59.042121 22760421793920 run.py:483] Algo bellman_ford step 2337 current loss 0.045534, current_train_items 74816.
I0302 18:58:59.071288 22760421793920 run.py:483] Algo bellman_ford step 2338 current loss 0.079227, current_train_items 74848.
I0302 18:58:59.102699 22760421793920 run.py:483] Algo bellman_ford step 2339 current loss 0.121006, current_train_items 74880.
I0302 18:58:59.121000 22760421793920 run.py:483] Algo bellman_ford step 2340 current loss 0.016428, current_train_items 74912.
I0302 18:58:59.137050 22760421793920 run.py:483] Algo bellman_ford step 2341 current loss 0.020651, current_train_items 74944.
I0302 18:58:59.158701 22760421793920 run.py:483] Algo bellman_ford step 2342 current loss 0.056109, current_train_items 74976.
I0302 18:58:59.186766 22760421793920 run.py:483] Algo bellman_ford step 2343 current loss 0.078412, current_train_items 75008.
I0302 18:58:59.216833 22760421793920 run.py:483] Algo bellman_ford step 2344 current loss 0.117537, current_train_items 75040.
I0302 18:58:59.234930 22760421793920 run.py:483] Algo bellman_ford step 2345 current loss 0.024188, current_train_items 75072.
I0302 18:58:59.250464 22760421793920 run.py:483] Algo bellman_ford step 2346 current loss 0.059332, current_train_items 75104.
I0302 18:58:59.272932 22760421793920 run.py:483] Algo bellman_ford step 2347 current loss 0.041047, current_train_items 75136.
I0302 18:58:59.300617 22760421793920 run.py:483] Algo bellman_ford step 2348 current loss 0.059820, current_train_items 75168.
I0302 18:58:59.330774 22760421793920 run.py:483] Algo bellman_ford step 2349 current loss 0.106201, current_train_items 75200.
I0302 18:58:59.348813 22760421793920 run.py:483] Algo bellman_ford step 2350 current loss 0.011570, current_train_items 75232.
I0302 18:58:59.356215 22760421793920 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:58:59.356324 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 18:58:59.372705 22760421793920 run.py:483] Algo bellman_ford step 2351 current loss 0.020663, current_train_items 75264.
I0302 18:58:59.395582 22760421793920 run.py:483] Algo bellman_ford step 2352 current loss 0.120039, current_train_items 75296.
I0302 18:58:59.423456 22760421793920 run.py:483] Algo bellman_ford step 2353 current loss 0.072455, current_train_items 75328.
I0302 18:58:59.457076 22760421793920 run.py:483] Algo bellman_ford step 2354 current loss 0.125849, current_train_items 75360.
I0302 18:58:59.475471 22760421793920 run.py:483] Algo bellman_ford step 2355 current loss 0.043228, current_train_items 75392.
I0302 18:58:59.491343 22760421793920 run.py:483] Algo bellman_ford step 2356 current loss 0.017664, current_train_items 75424.
I0302 18:58:59.514173 22760421793920 run.py:483] Algo bellman_ford step 2357 current loss 0.088129, current_train_items 75456.
I0302 18:58:59.542390 22760421793920 run.py:483] Algo bellman_ford step 2358 current loss 0.089779, current_train_items 75488.
I0302 18:58:59.575467 22760421793920 run.py:483] Algo bellman_ford step 2359 current loss 0.191893, current_train_items 75520.
I0302 18:58:59.593378 22760421793920 run.py:483] Algo bellman_ford step 2360 current loss 0.026711, current_train_items 75552.
I0302 18:58:59.609753 22760421793920 run.py:483] Algo bellman_ford step 2361 current loss 0.059030, current_train_items 75584.
I0302 18:58:59.633719 22760421793920 run.py:483] Algo bellman_ford step 2362 current loss 0.082324, current_train_items 75616.
I0302 18:58:59.662149 22760421793920 run.py:483] Algo bellman_ford step 2363 current loss 0.070734, current_train_items 75648.
I0302 18:58:59.693936 22760421793920 run.py:483] Algo bellman_ford step 2364 current loss 0.132984, current_train_items 75680.
I0302 18:58:59.711827 22760421793920 run.py:483] Algo bellman_ford step 2365 current loss 0.007774, current_train_items 75712.
I0302 18:58:59.727773 22760421793920 run.py:483] Algo bellman_ford step 2366 current loss 0.025974, current_train_items 75744.
I0302 18:58:59.750409 22760421793920 run.py:483] Algo bellman_ford step 2367 current loss 0.101247, current_train_items 75776.
I0302 18:58:59.780550 22760421793920 run.py:483] Algo bellman_ford step 2368 current loss 0.097498, current_train_items 75808.
I0302 18:58:59.813248 22760421793920 run.py:483] Algo bellman_ford step 2369 current loss 0.116904, current_train_items 75840.
I0302 18:58:59.831731 22760421793920 run.py:483] Algo bellman_ford step 2370 current loss 0.010649, current_train_items 75872.
I0302 18:58:59.847375 22760421793920 run.py:483] Algo bellman_ford step 2371 current loss 0.047108, current_train_items 75904.
I0302 18:58:59.869937 22760421793920 run.py:483] Algo bellman_ford step 2372 current loss 0.063655, current_train_items 75936.
I0302 18:58:59.898151 22760421793920 run.py:483] Algo bellman_ford step 2373 current loss 0.094636, current_train_items 75968.
I0302 18:58:59.928948 22760421793920 run.py:483] Algo bellman_ford step 2374 current loss 0.103143, current_train_items 76000.
I0302 18:58:59.946832 22760421793920 run.py:483] Algo bellman_ford step 2375 current loss 0.017725, current_train_items 76032.
I0302 18:58:59.962605 22760421793920 run.py:483] Algo bellman_ford step 2376 current loss 0.040063, current_train_items 76064.
I0302 18:58:59.985375 22760421793920 run.py:483] Algo bellman_ford step 2377 current loss 0.126713, current_train_items 76096.
I0302 18:59:00.015635 22760421793920 run.py:483] Algo bellman_ford step 2378 current loss 0.181294, current_train_items 76128.
I0302 18:59:00.048727 22760421793920 run.py:483] Algo bellman_ford step 2379 current loss 0.226886, current_train_items 76160.
I0302 18:59:00.067093 22760421793920 run.py:483] Algo bellman_ford step 2380 current loss 0.014885, current_train_items 76192.
I0302 18:59:00.083375 22760421793920 run.py:483] Algo bellman_ford step 2381 current loss 0.061724, current_train_items 76224.
I0302 18:59:00.106231 22760421793920 run.py:483] Algo bellman_ford step 2382 current loss 0.076706, current_train_items 76256.
I0302 18:59:00.134255 22760421793920 run.py:483] Algo bellman_ford step 2383 current loss 0.094239, current_train_items 76288.
I0302 18:59:00.166244 22760421793920 run.py:483] Algo bellman_ford step 2384 current loss 0.130487, current_train_items 76320.
I0302 18:59:00.184417 22760421793920 run.py:483] Algo bellman_ford step 2385 current loss 0.013423, current_train_items 76352.
I0302 18:59:00.199850 22760421793920 run.py:483] Algo bellman_ford step 2386 current loss 0.013660, current_train_items 76384.
I0302 18:59:00.221956 22760421793920 run.py:483] Algo bellman_ford step 2387 current loss 0.073594, current_train_items 76416.
I0302 18:59:00.251129 22760421793920 run.py:483] Algo bellman_ford step 2388 current loss 0.096994, current_train_items 76448.
I0302 18:59:00.284592 22760421793920 run.py:483] Algo bellman_ford step 2389 current loss 0.141005, current_train_items 76480.
I0302 18:59:00.302703 22760421793920 run.py:483] Algo bellman_ford step 2390 current loss 0.003984, current_train_items 76512.
I0302 18:59:00.317913 22760421793920 run.py:483] Algo bellman_ford step 2391 current loss 0.058446, current_train_items 76544.
I0302 18:59:00.338946 22760421793920 run.py:483] Algo bellman_ford step 2392 current loss 0.067409, current_train_items 76576.
I0302 18:59:00.367762 22760421793920 run.py:483] Algo bellman_ford step 2393 current loss 0.091460, current_train_items 76608.
I0302 18:59:00.402097 22760421793920 run.py:483] Algo bellman_ford step 2394 current loss 0.114477, current_train_items 76640.
I0302 18:59:00.420230 22760421793920 run.py:483] Algo bellman_ford step 2395 current loss 0.011426, current_train_items 76672.
I0302 18:59:00.435743 22760421793920 run.py:483] Algo bellman_ford step 2396 current loss 0.011924, current_train_items 76704.
I0302 18:59:00.457160 22760421793920 run.py:483] Algo bellman_ford step 2397 current loss 0.061309, current_train_items 76736.
I0302 18:59:00.486059 22760421793920 run.py:483] Algo bellman_ford step 2398 current loss 0.094530, current_train_items 76768.
I0302 18:59:00.518772 22760421793920 run.py:483] Algo bellman_ford step 2399 current loss 0.123019, current_train_items 76800.
I0302 18:59:00.537350 22760421793920 run.py:483] Algo bellman_ford step 2400 current loss 0.003953, current_train_items 76832.
I0302 18:59:00.544914 22760421793920 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:00.545026 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0302 18:59:00.561268 22760421793920 run.py:483] Algo bellman_ford step 2401 current loss 0.027929, current_train_items 76864.
I0302 18:59:00.584508 22760421793920 run.py:483] Algo bellman_ford step 2402 current loss 0.072509, current_train_items 76896.
I0302 18:59:00.615383 22760421793920 run.py:483] Algo bellman_ford step 2403 current loss 0.136117, current_train_items 76928.
I0302 18:59:00.650984 22760421793920 run.py:483] Algo bellman_ford step 2404 current loss 0.132144, current_train_items 76960.
I0302 18:59:00.669655 22760421793920 run.py:483] Algo bellman_ford step 2405 current loss 0.028577, current_train_items 76992.
I0302 18:59:00.685414 22760421793920 run.py:483] Algo bellman_ford step 2406 current loss 0.064846, current_train_items 77024.
I0302 18:59:00.708323 22760421793920 run.py:483] Algo bellman_ford step 2407 current loss 0.204741, current_train_items 77056.
I0302 18:59:00.736554 22760421793920 run.py:483] Algo bellman_ford step 2408 current loss 0.179367, current_train_items 77088.
I0302 18:59:00.767691 22760421793920 run.py:483] Algo bellman_ford step 2409 current loss 0.146857, current_train_items 77120.
I0302 18:59:00.785585 22760421793920 run.py:483] Algo bellman_ford step 2410 current loss 0.010551, current_train_items 77152.
I0302 18:59:00.801360 22760421793920 run.py:483] Algo bellman_ford step 2411 current loss 0.029722, current_train_items 77184.
I0302 18:59:00.825139 22760421793920 run.py:483] Algo bellman_ford step 2412 current loss 0.195703, current_train_items 77216.
I0302 18:59:00.854931 22760421793920 run.py:483] Algo bellman_ford step 2413 current loss 0.139753, current_train_items 77248.
I0302 18:59:00.887675 22760421793920 run.py:483] Algo bellman_ford step 2414 current loss 0.247212, current_train_items 77280.
I0302 18:59:00.905917 22760421793920 run.py:483] Algo bellman_ford step 2415 current loss 0.004962, current_train_items 77312.
I0302 18:59:00.921056 22760421793920 run.py:483] Algo bellman_ford step 2416 current loss 0.064097, current_train_items 77344.
I0302 18:59:00.943596 22760421793920 run.py:483] Algo bellman_ford step 2417 current loss 0.081600, current_train_items 77376.
I0302 18:59:00.973759 22760421793920 run.py:483] Algo bellman_ford step 2418 current loss 0.165581, current_train_items 77408.
I0302 18:59:01.003748 22760421793920 run.py:483] Algo bellman_ford step 2419 current loss 0.116568, current_train_items 77440.
I0302 18:59:01.021753 22760421793920 run.py:483] Algo bellman_ford step 2420 current loss 0.014597, current_train_items 77472.
I0302 18:59:01.037199 22760421793920 run.py:483] Algo bellman_ford step 2421 current loss 0.027782, current_train_items 77504.
I0302 18:59:01.060320 22760421793920 run.py:483] Algo bellman_ford step 2422 current loss 0.107604, current_train_items 77536.
I0302 18:59:01.089595 22760421793920 run.py:483] Algo bellman_ford step 2423 current loss 0.088570, current_train_items 77568.
I0302 18:59:01.122146 22760421793920 run.py:483] Algo bellman_ford step 2424 current loss 0.118931, current_train_items 77600.
I0302 18:59:01.140336 22760421793920 run.py:483] Algo bellman_ford step 2425 current loss 0.040564, current_train_items 77632.
I0302 18:59:01.155859 22760421793920 run.py:483] Algo bellman_ford step 2426 current loss 0.035015, current_train_items 77664.
I0302 18:59:01.179955 22760421793920 run.py:483] Algo bellman_ford step 2427 current loss 0.132671, current_train_items 77696.
I0302 18:59:01.208086 22760421793920 run.py:483] Algo bellman_ford step 2428 current loss 0.115324, current_train_items 77728.
I0302 18:59:01.239586 22760421793920 run.py:483] Algo bellman_ford step 2429 current loss 0.079844, current_train_items 77760.
I0302 18:59:01.257735 22760421793920 run.py:483] Algo bellman_ford step 2430 current loss 0.027152, current_train_items 77792.
I0302 18:59:01.273607 22760421793920 run.py:483] Algo bellman_ford step 2431 current loss 0.047930, current_train_items 77824.
I0302 18:59:01.296800 22760421793920 run.py:483] Algo bellman_ford step 2432 current loss 0.111490, current_train_items 77856.
I0302 18:59:01.325616 22760421793920 run.py:483] Algo bellman_ford step 2433 current loss 0.072973, current_train_items 77888.
I0302 18:59:01.357694 22760421793920 run.py:483] Algo bellman_ford step 2434 current loss 0.129293, current_train_items 77920.
I0302 18:59:01.375515 22760421793920 run.py:483] Algo bellman_ford step 2435 current loss 0.026114, current_train_items 77952.
I0302 18:59:01.391339 22760421793920 run.py:483] Algo bellman_ford step 2436 current loss 0.063269, current_train_items 77984.
I0302 18:59:01.413805 22760421793920 run.py:483] Algo bellman_ford step 2437 current loss 0.142824, current_train_items 78016.
I0302 18:59:01.441526 22760421793920 run.py:483] Algo bellman_ford step 2438 current loss 0.187913, current_train_items 78048.
I0302 18:59:01.472692 22760421793920 run.py:483] Algo bellman_ford step 2439 current loss 0.184662, current_train_items 78080.
I0302 18:59:01.490852 22760421793920 run.py:483] Algo bellman_ford step 2440 current loss 0.013858, current_train_items 78112.
I0302 18:59:01.506507 22760421793920 run.py:483] Algo bellman_ford step 2441 current loss 0.044417, current_train_items 78144.
I0302 18:59:01.530268 22760421793920 run.py:483] Algo bellman_ford step 2442 current loss 0.121128, current_train_items 78176.
I0302 18:59:01.559279 22760421793920 run.py:483] Algo bellman_ford step 2443 current loss 0.113136, current_train_items 78208.
I0302 18:59:01.592602 22760421793920 run.py:483] Algo bellman_ford step 2444 current loss 0.162800, current_train_items 78240.
I0302 18:59:01.611149 22760421793920 run.py:483] Algo bellman_ford step 2445 current loss 0.014960, current_train_items 78272.
I0302 18:59:01.626908 22760421793920 run.py:483] Algo bellman_ford step 2446 current loss 0.023636, current_train_items 78304.
I0302 18:59:01.650389 22760421793920 run.py:483] Algo bellman_ford step 2447 current loss 0.098808, current_train_items 78336.
I0302 18:59:01.678819 22760421793920 run.py:483] Algo bellman_ford step 2448 current loss 0.098608, current_train_items 78368.
I0302 18:59:01.710501 22760421793920 run.py:483] Algo bellman_ford step 2449 current loss 0.104468, current_train_items 78400.
I0302 18:59:01.728406 22760421793920 run.py:483] Algo bellman_ford step 2450 current loss 0.023755, current_train_items 78432.
I0302 18:59:01.736096 22760421793920 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:01.736206 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:59:01.752442 22760421793920 run.py:483] Algo bellman_ford step 2451 current loss 0.028361, current_train_items 78464.
I0302 18:59:01.775698 22760421793920 run.py:483] Algo bellman_ford step 2452 current loss 0.084342, current_train_items 78496.
I0302 18:59:01.804927 22760421793920 run.py:483] Algo bellman_ford step 2453 current loss 0.109141, current_train_items 78528.
I0302 18:59:01.837821 22760421793920 run.py:483] Algo bellman_ford step 2454 current loss 0.143414, current_train_items 78560.
I0302 18:59:01.856363 22760421793920 run.py:483] Algo bellman_ford step 2455 current loss 0.047256, current_train_items 78592.
I0302 18:59:01.872384 22760421793920 run.py:483] Algo bellman_ford step 2456 current loss 0.053215, current_train_items 78624.
I0302 18:59:01.893867 22760421793920 run.py:483] Algo bellman_ford step 2457 current loss 0.041532, current_train_items 78656.
I0302 18:59:01.923150 22760421793920 run.py:483] Algo bellman_ford step 2458 current loss 0.091495, current_train_items 78688.
I0302 18:59:01.956306 22760421793920 run.py:483] Algo bellman_ford step 2459 current loss 0.162855, current_train_items 78720.
I0302 18:59:01.974655 22760421793920 run.py:483] Algo bellman_ford step 2460 current loss 0.010992, current_train_items 78752.
I0302 18:59:01.990706 22760421793920 run.py:483] Algo bellman_ford step 2461 current loss 0.063866, current_train_items 78784.
I0302 18:59:02.013140 22760421793920 run.py:483] Algo bellman_ford step 2462 current loss 0.100038, current_train_items 78816.
I0302 18:59:02.043089 22760421793920 run.py:483] Algo bellman_ford step 2463 current loss 0.161048, current_train_items 78848.
I0302 18:59:02.075005 22760421793920 run.py:483] Algo bellman_ford step 2464 current loss 0.187508, current_train_items 78880.
I0302 18:59:02.092869 22760421793920 run.py:483] Algo bellman_ford step 2465 current loss 0.013035, current_train_items 78912.
I0302 18:59:02.108580 22760421793920 run.py:483] Algo bellman_ford step 2466 current loss 0.027966, current_train_items 78944.
I0302 18:59:02.131442 22760421793920 run.py:483] Algo bellman_ford step 2467 current loss 0.056196, current_train_items 78976.
I0302 18:59:02.161232 22760421793920 run.py:483] Algo bellman_ford step 2468 current loss 0.139489, current_train_items 79008.
I0302 18:59:02.194511 22760421793920 run.py:483] Algo bellman_ford step 2469 current loss 0.155773, current_train_items 79040.
I0302 18:59:02.212523 22760421793920 run.py:483] Algo bellman_ford step 2470 current loss 0.003873, current_train_items 79072.
I0302 18:59:02.228174 22760421793920 run.py:483] Algo bellman_ford step 2471 current loss 0.047185, current_train_items 79104.
I0302 18:59:02.251433 22760421793920 run.py:483] Algo bellman_ford step 2472 current loss 0.121217, current_train_items 79136.
I0302 18:59:02.281839 22760421793920 run.py:483] Algo bellman_ford step 2473 current loss 0.123441, current_train_items 79168.
I0302 18:59:02.312823 22760421793920 run.py:483] Algo bellman_ford step 2474 current loss 0.088948, current_train_items 79200.
I0302 18:59:02.330806 22760421793920 run.py:483] Algo bellman_ford step 2475 current loss 0.005122, current_train_items 79232.
I0302 18:59:02.346732 22760421793920 run.py:483] Algo bellman_ford step 2476 current loss 0.053575, current_train_items 79264.
I0302 18:59:02.368066 22760421793920 run.py:483] Algo bellman_ford step 2477 current loss 0.034179, current_train_items 79296.
I0302 18:59:02.396513 22760421793920 run.py:483] Algo bellman_ford step 2478 current loss 0.118523, current_train_items 79328.
I0302 18:59:02.429561 22760421793920 run.py:483] Algo bellman_ford step 2479 current loss 0.114597, current_train_items 79360.
I0302 18:59:02.447688 22760421793920 run.py:483] Algo bellman_ford step 2480 current loss 0.006561, current_train_items 79392.
I0302 18:59:02.463046 22760421793920 run.py:483] Algo bellman_ford step 2481 current loss 0.044217, current_train_items 79424.
I0302 18:59:02.486568 22760421793920 run.py:483] Algo bellman_ford step 2482 current loss 0.108349, current_train_items 79456.
I0302 18:59:02.515895 22760421793920 run.py:483] Algo bellman_ford step 2483 current loss 0.172458, current_train_items 79488.
I0302 18:59:02.549426 22760421793920 run.py:483] Algo bellman_ford step 2484 current loss 0.143272, current_train_items 79520.
I0302 18:59:02.567718 22760421793920 run.py:483] Algo bellman_ford step 2485 current loss 0.009773, current_train_items 79552.
I0302 18:59:02.583558 22760421793920 run.py:483] Algo bellman_ford step 2486 current loss 0.070282, current_train_items 79584.
I0302 18:59:02.607125 22760421793920 run.py:483] Algo bellman_ford step 2487 current loss 0.176899, current_train_items 79616.
I0302 18:59:02.633777 22760421793920 run.py:483] Algo bellman_ford step 2488 current loss 0.106402, current_train_items 79648.
I0302 18:59:02.666344 22760421793920 run.py:483] Algo bellman_ford step 2489 current loss 0.221238, current_train_items 79680.
I0302 18:59:02.684409 22760421793920 run.py:483] Algo bellman_ford step 2490 current loss 0.027401, current_train_items 79712.
I0302 18:59:02.700009 22760421793920 run.py:483] Algo bellman_ford step 2491 current loss 0.027057, current_train_items 79744.
I0302 18:59:02.722523 22760421793920 run.py:483] Algo bellman_ford step 2492 current loss 0.085506, current_train_items 79776.
I0302 18:59:02.751411 22760421793920 run.py:483] Algo bellman_ford step 2493 current loss 0.081838, current_train_items 79808.
I0302 18:59:02.782156 22760421793920 run.py:483] Algo bellman_ford step 2494 current loss 0.105025, current_train_items 79840.
I0302 18:59:02.800419 22760421793920 run.py:483] Algo bellman_ford step 2495 current loss 0.009670, current_train_items 79872.
I0302 18:59:02.816566 22760421793920 run.py:483] Algo bellman_ford step 2496 current loss 0.030072, current_train_items 79904.
I0302 18:59:02.838594 22760421793920 run.py:483] Algo bellman_ford step 2497 current loss 0.066929, current_train_items 79936.
I0302 18:59:02.866397 22760421793920 run.py:483] Algo bellman_ford step 2498 current loss 0.134914, current_train_items 79968.
I0302 18:59:02.898273 22760421793920 run.py:483] Algo bellman_ford step 2499 current loss 0.104355, current_train_items 80000.
I0302 18:59:02.916563 22760421793920 run.py:483] Algo bellman_ford step 2500 current loss 0.007853, current_train_items 80032.
I0302 18:59:02.924246 22760421793920 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:02.924355 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0302 18:59:02.940777 22760421793920 run.py:483] Algo bellman_ford step 2501 current loss 0.050388, current_train_items 80064.
I0302 18:59:02.964408 22760421793920 run.py:483] Algo bellman_ford step 2502 current loss 0.130501, current_train_items 80096.
I0302 18:59:02.993061 22760421793920 run.py:483] Algo bellman_ford step 2503 current loss 0.139054, current_train_items 80128.
I0302 18:59:03.026556 22760421793920 run.py:483] Algo bellman_ford step 2504 current loss 0.121098, current_train_items 80160.
I0302 18:59:03.045062 22760421793920 run.py:483] Algo bellman_ford step 2505 current loss 0.003545, current_train_items 80192.
I0302 18:59:03.060784 22760421793920 run.py:483] Algo bellman_ford step 2506 current loss 0.020646, current_train_items 80224.
I0302 18:59:03.084045 22760421793920 run.py:483] Algo bellman_ford step 2507 current loss 0.114546, current_train_items 80256.
I0302 18:59:03.112396 22760421793920 run.py:483] Algo bellman_ford step 2508 current loss 0.157030, current_train_items 80288.
I0302 18:59:03.145090 22760421793920 run.py:483] Algo bellman_ford step 2509 current loss 0.166737, current_train_items 80320.
I0302 18:59:03.163278 22760421793920 run.py:483] Algo bellman_ford step 2510 current loss 0.007112, current_train_items 80352.
I0302 18:59:03.179213 22760421793920 run.py:483] Algo bellman_ford step 2511 current loss 0.040688, current_train_items 80384.
I0302 18:59:03.201845 22760421793920 run.py:483] Algo bellman_ford step 2512 current loss 0.062093, current_train_items 80416.
I0302 18:59:03.230845 22760421793920 run.py:483] Algo bellman_ford step 2513 current loss 0.089425, current_train_items 80448.
I0302 18:59:03.264039 22760421793920 run.py:483] Algo bellman_ford step 2514 current loss 0.089575, current_train_items 80480.
I0302 18:59:03.282422 22760421793920 run.py:483] Algo bellman_ford step 2515 current loss 0.005359, current_train_items 80512.
I0302 18:59:03.298288 22760421793920 run.py:483] Algo bellman_ford step 2516 current loss 0.038574, current_train_items 80544.
I0302 18:59:03.321329 22760421793920 run.py:483] Algo bellman_ford step 2517 current loss 0.138379, current_train_items 80576.
I0302 18:59:03.350059 22760421793920 run.py:483] Algo bellman_ford step 2518 current loss 0.109021, current_train_items 80608.
I0302 18:59:03.382742 22760421793920 run.py:483] Algo bellman_ford step 2519 current loss 0.177897, current_train_items 80640.
I0302 18:59:03.400777 22760421793920 run.py:483] Algo bellman_ford step 2520 current loss 0.003285, current_train_items 80672.
I0302 18:59:03.416451 22760421793920 run.py:483] Algo bellman_ford step 2521 current loss 0.038635, current_train_items 80704.
I0302 18:59:03.437839 22760421793920 run.py:483] Algo bellman_ford step 2522 current loss 0.044424, current_train_items 80736.
I0302 18:59:03.466338 22760421793920 run.py:483] Algo bellman_ford step 2523 current loss 0.090729, current_train_items 80768.
I0302 18:59:03.497910 22760421793920 run.py:483] Algo bellman_ford step 2524 current loss 0.077581, current_train_items 80800.
I0302 18:59:03.516201 22760421793920 run.py:483] Algo bellman_ford step 2525 current loss 0.006924, current_train_items 80832.
I0302 18:59:03.531964 22760421793920 run.py:483] Algo bellman_ford step 2526 current loss 0.029100, current_train_items 80864.
I0302 18:59:03.554545 22760421793920 run.py:483] Algo bellman_ford step 2527 current loss 0.077380, current_train_items 80896.
I0302 18:59:03.583205 22760421793920 run.py:483] Algo bellman_ford step 2528 current loss 0.053565, current_train_items 80928.
I0302 18:59:03.616005 22760421793920 run.py:483] Algo bellman_ford step 2529 current loss 0.107942, current_train_items 80960.
I0302 18:59:03.634220 22760421793920 run.py:483] Algo bellman_ford step 2530 current loss 0.007608, current_train_items 80992.
I0302 18:59:03.649527 22760421793920 run.py:483] Algo bellman_ford step 2531 current loss 0.050221, current_train_items 81024.
I0302 18:59:03.672795 22760421793920 run.py:483] Algo bellman_ford step 2532 current loss 0.088651, current_train_items 81056.
I0302 18:59:03.702109 22760421793920 run.py:483] Algo bellman_ford step 2533 current loss 0.078350, current_train_items 81088.
I0302 18:59:03.733642 22760421793920 run.py:483] Algo bellman_ford step 2534 current loss 0.106116, current_train_items 81120.
I0302 18:59:03.752172 22760421793920 run.py:483] Algo bellman_ford step 2535 current loss 0.002341, current_train_items 81152.
I0302 18:59:03.767664 22760421793920 run.py:483] Algo bellman_ford step 2536 current loss 0.016496, current_train_items 81184.
W0302 18:59:03.784322 22760421793920 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:09.481581 22760421793920 run.py:483] Algo bellman_ford step 2537 current loss 0.114736, current_train_items 81216.
I0302 18:59:09.513472 22760421793920 run.py:483] Algo bellman_ford step 2538 current loss 0.055813, current_train_items 81248.
I0302 18:59:09.546801 22760421793920 run.py:483] Algo bellman_ford step 2539 current loss 0.099780, current_train_items 81280.
I0302 18:59:09.565724 22760421793920 run.py:483] Algo bellman_ford step 2540 current loss 0.013211, current_train_items 81312.
I0302 18:59:09.581335 22760421793920 run.py:483] Algo bellman_ford step 2541 current loss 0.034668, current_train_items 81344.
I0302 18:59:09.604416 22760421793920 run.py:483] Algo bellman_ford step 2542 current loss 0.098530, current_train_items 81376.
I0302 18:59:09.632990 22760421793920 run.py:483] Algo bellman_ford step 2543 current loss 0.110743, current_train_items 81408.
I0302 18:59:09.665290 22760421793920 run.py:483] Algo bellman_ford step 2544 current loss 0.111010, current_train_items 81440.
I0302 18:59:09.683760 22760421793920 run.py:483] Algo bellman_ford step 2545 current loss 0.002988, current_train_items 81472.
I0302 18:59:09.699403 22760421793920 run.py:483] Algo bellman_ford step 2546 current loss 0.042268, current_train_items 81504.
I0302 18:59:09.722620 22760421793920 run.py:483] Algo bellman_ford step 2547 current loss 0.121389, current_train_items 81536.
I0302 18:59:09.753376 22760421793920 run.py:483] Algo bellman_ford step 2548 current loss 0.123417, current_train_items 81568.
I0302 18:59:09.784699 22760421793920 run.py:483] Algo bellman_ford step 2549 current loss 0.102167, current_train_items 81600.
I0302 18:59:09.803121 22760421793920 run.py:483] Algo bellman_ford step 2550 current loss 0.006005, current_train_items 81632.
I0302 18:59:09.812015 22760421793920 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:09.812127 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0302 18:59:09.828564 22760421793920 run.py:483] Algo bellman_ford step 2551 current loss 0.044955, current_train_items 81664.
I0302 18:59:09.852475 22760421793920 run.py:483] Algo bellman_ford step 2552 current loss 0.109201, current_train_items 81696.
I0302 18:59:09.883368 22760421793920 run.py:483] Algo bellman_ford step 2553 current loss 0.110173, current_train_items 81728.
I0302 18:59:09.916121 22760421793920 run.py:483] Algo bellman_ford step 2554 current loss 0.073513, current_train_items 81760.
I0302 18:59:09.934715 22760421793920 run.py:483] Algo bellman_ford step 2555 current loss 0.003717, current_train_items 81792.
I0302 18:59:09.951143 22760421793920 run.py:483] Algo bellman_ford step 2556 current loss 0.057240, current_train_items 81824.
I0302 18:59:09.973963 22760421793920 run.py:483] Algo bellman_ford step 2557 current loss 0.080920, current_train_items 81856.
I0302 18:59:10.004436 22760421793920 run.py:483] Algo bellman_ford step 2558 current loss 0.149847, current_train_items 81888.
I0302 18:59:10.038135 22760421793920 run.py:483] Algo bellman_ford step 2559 current loss 0.114919, current_train_items 81920.
I0302 18:59:10.056081 22760421793920 run.py:483] Algo bellman_ford step 2560 current loss 0.019053, current_train_items 81952.
I0302 18:59:10.072085 22760421793920 run.py:483] Algo bellman_ford step 2561 current loss 0.041454, current_train_items 81984.
I0302 18:59:10.094930 22760421793920 run.py:483] Algo bellman_ford step 2562 current loss 0.219523, current_train_items 82016.
I0302 18:59:10.125716 22760421793920 run.py:483] Algo bellman_ford step 2563 current loss 0.341931, current_train_items 82048.
I0302 18:59:10.157817 22760421793920 run.py:483] Algo bellman_ford step 2564 current loss 0.327889, current_train_items 82080.
I0302 18:59:10.175769 22760421793920 run.py:483] Algo bellman_ford step 2565 current loss 0.005401, current_train_items 82112.
I0302 18:59:10.191616 22760421793920 run.py:483] Algo bellman_ford step 2566 current loss 0.048942, current_train_items 82144.
I0302 18:59:10.214121 22760421793920 run.py:483] Algo bellman_ford step 2567 current loss 0.050005, current_train_items 82176.
I0302 18:59:10.244586 22760421793920 run.py:483] Algo bellman_ford step 2568 current loss 0.137939, current_train_items 82208.
I0302 18:59:10.275982 22760421793920 run.py:483] Algo bellman_ford step 2569 current loss 0.137403, current_train_items 82240.
I0302 18:59:10.294215 22760421793920 run.py:483] Algo bellman_ford step 2570 current loss 0.013995, current_train_items 82272.
I0302 18:59:10.309807 22760421793920 run.py:483] Algo bellman_ford step 2571 current loss 0.026645, current_train_items 82304.
I0302 18:59:10.331449 22760421793920 run.py:483] Algo bellman_ford step 2572 current loss 0.045544, current_train_items 82336.
I0302 18:59:10.360894 22760421793920 run.py:483] Algo bellman_ford step 2573 current loss 0.052003, current_train_items 82368.
I0302 18:59:10.394035 22760421793920 run.py:483] Algo bellman_ford step 2574 current loss 0.141901, current_train_items 82400.
I0302 18:59:10.412161 22760421793920 run.py:483] Algo bellman_ford step 2575 current loss 0.015361, current_train_items 82432.
I0302 18:59:10.427598 22760421793920 run.py:483] Algo bellman_ford step 2576 current loss 0.036937, current_train_items 82464.
I0302 18:59:10.449920 22760421793920 run.py:483] Algo bellman_ford step 2577 current loss 0.065584, current_train_items 82496.
I0302 18:59:10.480806 22760421793920 run.py:483] Algo bellman_ford step 2578 current loss 0.077171, current_train_items 82528.
I0302 18:59:10.511646 22760421793920 run.py:483] Algo bellman_ford step 2579 current loss 0.143716, current_train_items 82560.
I0302 18:59:10.529975 22760421793920 run.py:483] Algo bellman_ford step 2580 current loss 0.029919, current_train_items 82592.
I0302 18:59:10.545781 22760421793920 run.py:483] Algo bellman_ford step 2581 current loss 0.029863, current_train_items 82624.
I0302 18:59:10.568395 22760421793920 run.py:483] Algo bellman_ford step 2582 current loss 0.058128, current_train_items 82656.
I0302 18:59:10.597693 22760421793920 run.py:483] Algo bellman_ford step 2583 current loss 0.078742, current_train_items 82688.
I0302 18:59:10.630659 22760421793920 run.py:483] Algo bellman_ford step 2584 current loss 0.142717, current_train_items 82720.
I0302 18:59:10.648743 22760421793920 run.py:483] Algo bellman_ford step 2585 current loss 0.011581, current_train_items 82752.
I0302 18:59:10.664471 22760421793920 run.py:483] Algo bellman_ford step 2586 current loss 0.045092, current_train_items 82784.
I0302 18:59:10.687091 22760421793920 run.py:483] Algo bellman_ford step 2587 current loss 0.037545, current_train_items 82816.
I0302 18:59:10.716315 22760421793920 run.py:483] Algo bellman_ford step 2588 current loss 0.070874, current_train_items 82848.
I0302 18:59:10.748670 22760421793920 run.py:483] Algo bellman_ford step 2589 current loss 0.151690, current_train_items 82880.
I0302 18:59:10.766690 22760421793920 run.py:483] Algo bellman_ford step 2590 current loss 0.012635, current_train_items 82912.
I0302 18:59:10.782610 22760421793920 run.py:483] Algo bellman_ford step 2591 current loss 0.029768, current_train_items 82944.
I0302 18:59:10.806041 22760421793920 run.py:483] Algo bellman_ford step 2592 current loss 0.076584, current_train_items 82976.
I0302 18:59:10.836503 22760421793920 run.py:483] Algo bellman_ford step 2593 current loss 0.103259, current_train_items 83008.
I0302 18:59:10.866732 22760421793920 run.py:483] Algo bellman_ford step 2594 current loss 0.104278, current_train_items 83040.
I0302 18:59:10.884769 22760421793920 run.py:483] Algo bellman_ford step 2595 current loss 0.012360, current_train_items 83072.
I0302 18:59:10.900779 22760421793920 run.py:483] Algo bellman_ford step 2596 current loss 0.043814, current_train_items 83104.
I0302 18:59:10.924418 22760421793920 run.py:483] Algo bellman_ford step 2597 current loss 0.103297, current_train_items 83136.
I0302 18:59:10.954185 22760421793920 run.py:483] Algo bellman_ford step 2598 current loss 0.089902, current_train_items 83168.
I0302 18:59:10.985936 22760421793920 run.py:483] Algo bellman_ford step 2599 current loss 0.195443, current_train_items 83200.
I0302 18:59:11.004064 22760421793920 run.py:483] Algo bellman_ford step 2600 current loss 0.019272, current_train_items 83232.
I0302 18:59:11.011938 22760421793920 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:11.012051 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.988, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 18:59:11.040000 22760421793920 run.py:483] Algo bellman_ford step 2601 current loss 0.019871, current_train_items 83264.
I0302 18:59:11.062662 22760421793920 run.py:483] Algo bellman_ford step 2602 current loss 0.073608, current_train_items 83296.
I0302 18:59:11.091457 22760421793920 run.py:483] Algo bellman_ford step 2603 current loss 0.122866, current_train_items 83328.
I0302 18:59:11.124212 22760421793920 run.py:483] Algo bellman_ford step 2604 current loss 0.104488, current_train_items 83360.
I0302 18:59:11.142699 22760421793920 run.py:483] Algo bellman_ford step 2605 current loss 0.017893, current_train_items 83392.
I0302 18:59:11.158281 22760421793920 run.py:483] Algo bellman_ford step 2606 current loss 0.013776, current_train_items 83424.
I0302 18:59:11.181405 22760421793920 run.py:483] Algo bellman_ford step 2607 current loss 0.031687, current_train_items 83456.
I0302 18:59:11.212199 22760421793920 run.py:483] Algo bellman_ford step 2608 current loss 0.063917, current_train_items 83488.
I0302 18:59:11.244750 22760421793920 run.py:483] Algo bellman_ford step 2609 current loss 0.147437, current_train_items 83520.
I0302 18:59:11.263037 22760421793920 run.py:483] Algo bellman_ford step 2610 current loss 0.031070, current_train_items 83552.
I0302 18:59:11.278659 22760421793920 run.py:483] Algo bellman_ford step 2611 current loss 0.060934, current_train_items 83584.
I0302 18:59:11.300985 22760421793920 run.py:483] Algo bellman_ford step 2612 current loss 0.045299, current_train_items 83616.
I0302 18:59:11.330401 22760421793920 run.py:483] Algo bellman_ford step 2613 current loss 0.099371, current_train_items 83648.
I0302 18:59:11.361481 22760421793920 run.py:483] Algo bellman_ford step 2614 current loss 0.083170, current_train_items 83680.
I0302 18:59:11.379891 22760421793920 run.py:483] Algo bellman_ford step 2615 current loss 0.003397, current_train_items 83712.
I0302 18:59:11.395347 22760421793920 run.py:483] Algo bellman_ford step 2616 current loss 0.016759, current_train_items 83744.
I0302 18:59:11.419393 22760421793920 run.py:483] Algo bellman_ford step 2617 current loss 0.111820, current_train_items 83776.
I0302 18:59:11.449937 22760421793920 run.py:483] Algo bellman_ford step 2618 current loss 0.087571, current_train_items 83808.
I0302 18:59:11.480516 22760421793920 run.py:483] Algo bellman_ford step 2619 current loss 0.079450, current_train_items 83840.
I0302 18:59:11.498996 22760421793920 run.py:483] Algo bellman_ford step 2620 current loss 0.008883, current_train_items 83872.
I0302 18:59:11.514739 22760421793920 run.py:483] Algo bellman_ford step 2621 current loss 0.191093, current_train_items 83904.
I0302 18:59:11.537935 22760421793920 run.py:483] Algo bellman_ford step 2622 current loss 0.140160, current_train_items 83936.
I0302 18:59:11.567755 22760421793920 run.py:483] Algo bellman_ford step 2623 current loss 0.171028, current_train_items 83968.
I0302 18:59:11.598917 22760421793920 run.py:483] Algo bellman_ford step 2624 current loss 0.154610, current_train_items 84000.
I0302 18:59:11.617319 22760421793920 run.py:483] Algo bellman_ford step 2625 current loss 0.008156, current_train_items 84032.
I0302 18:59:11.633010 22760421793920 run.py:483] Algo bellman_ford step 2626 current loss 0.024159, current_train_items 84064.
I0302 18:59:11.656151 22760421793920 run.py:483] Algo bellman_ford step 2627 current loss 0.105828, current_train_items 84096.
I0302 18:59:11.686377 22760421793920 run.py:483] Algo bellman_ford step 2628 current loss 0.323492, current_train_items 84128.
I0302 18:59:11.718355 22760421793920 run.py:483] Algo bellman_ford step 2629 current loss 0.243350, current_train_items 84160.
I0302 18:59:11.736647 22760421793920 run.py:483] Algo bellman_ford step 2630 current loss 0.022584, current_train_items 84192.
I0302 18:59:11.752409 22760421793920 run.py:483] Algo bellman_ford step 2631 current loss 0.060873, current_train_items 84224.
I0302 18:59:11.774852 22760421793920 run.py:483] Algo bellman_ford step 2632 current loss 0.097059, current_train_items 84256.
I0302 18:59:11.803917 22760421793920 run.py:483] Algo bellman_ford step 2633 current loss 0.090482, current_train_items 84288.
I0302 18:59:11.837590 22760421793920 run.py:483] Algo bellman_ford step 2634 current loss 0.171224, current_train_items 84320.
I0302 18:59:11.856039 22760421793920 run.py:483] Algo bellman_ford step 2635 current loss 0.006077, current_train_items 84352.
I0302 18:59:11.871506 22760421793920 run.py:483] Algo bellman_ford step 2636 current loss 0.040630, current_train_items 84384.
I0302 18:59:11.894838 22760421793920 run.py:483] Algo bellman_ford step 2637 current loss 0.108423, current_train_items 84416.
I0302 18:59:11.925438 22760421793920 run.py:483] Algo bellman_ford step 2638 current loss 0.089735, current_train_items 84448.
I0302 18:59:11.958150 22760421793920 run.py:483] Algo bellman_ford step 2639 current loss 0.124219, current_train_items 84480.
I0302 18:59:11.976600 22760421793920 run.py:483] Algo bellman_ford step 2640 current loss 0.021869, current_train_items 84512.
I0302 18:59:11.992209 22760421793920 run.py:483] Algo bellman_ford step 2641 current loss 0.026921, current_train_items 84544.
I0302 18:59:12.014369 22760421793920 run.py:483] Algo bellman_ford step 2642 current loss 0.065083, current_train_items 84576.
I0302 18:59:12.043813 22760421793920 run.py:483] Algo bellman_ford step 2643 current loss 0.112072, current_train_items 84608.
I0302 18:59:12.076984 22760421793920 run.py:483] Algo bellman_ford step 2644 current loss 0.124008, current_train_items 84640.
I0302 18:59:12.095289 22760421793920 run.py:483] Algo bellman_ford step 2645 current loss 0.020760, current_train_items 84672.
I0302 18:59:12.110812 22760421793920 run.py:483] Algo bellman_ford step 2646 current loss 0.052718, current_train_items 84704.
I0302 18:59:12.133718 22760421793920 run.py:483] Algo bellman_ford step 2647 current loss 0.114196, current_train_items 84736.
I0302 18:59:12.163947 22760421793920 run.py:483] Algo bellman_ford step 2648 current loss 0.111358, current_train_items 84768.
I0302 18:59:12.198152 22760421793920 run.py:483] Algo bellman_ford step 2649 current loss 0.115791, current_train_items 84800.
I0302 18:59:12.216539 22760421793920 run.py:483] Algo bellman_ford step 2650 current loss 0.028994, current_train_items 84832.
I0302 18:59:12.224196 22760421793920 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:12.224306 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0302 18:59:12.240398 22760421793920 run.py:483] Algo bellman_ford step 2651 current loss 0.058216, current_train_items 84864.
I0302 18:59:12.263535 22760421793920 run.py:483] Algo bellman_ford step 2652 current loss 0.065884, current_train_items 84896.
I0302 18:59:12.294302 22760421793920 run.py:483] Algo bellman_ford step 2653 current loss 0.208877, current_train_items 84928.
I0302 18:59:12.326743 22760421793920 run.py:483] Algo bellman_ford step 2654 current loss 0.107959, current_train_items 84960.
I0302 18:59:12.345437 22760421793920 run.py:483] Algo bellman_ford step 2655 current loss 0.011973, current_train_items 84992.
I0302 18:59:12.361610 22760421793920 run.py:483] Algo bellman_ford step 2656 current loss 0.046644, current_train_items 85024.
I0302 18:59:12.384171 22760421793920 run.py:483] Algo bellman_ford step 2657 current loss 0.083666, current_train_items 85056.
I0302 18:59:12.413085 22760421793920 run.py:483] Algo bellman_ford step 2658 current loss 0.100814, current_train_items 85088.
I0302 18:59:12.447408 22760421793920 run.py:483] Algo bellman_ford step 2659 current loss 0.186674, current_train_items 85120.
I0302 18:59:12.465911 22760421793920 run.py:483] Algo bellman_ford step 2660 current loss 0.005866, current_train_items 85152.
I0302 18:59:12.482148 22760421793920 run.py:483] Algo bellman_ford step 2661 current loss 0.062807, current_train_items 85184.
I0302 18:59:12.505368 22760421793920 run.py:483] Algo bellman_ford step 2662 current loss 0.077780, current_train_items 85216.
I0302 18:59:12.535351 22760421793920 run.py:483] Algo bellman_ford step 2663 current loss 0.148821, current_train_items 85248.
I0302 18:59:12.569771 22760421793920 run.py:483] Algo bellman_ford step 2664 current loss 0.138147, current_train_items 85280.
I0302 18:59:12.588080 22760421793920 run.py:483] Algo bellman_ford step 2665 current loss 0.015405, current_train_items 85312.
I0302 18:59:12.603847 22760421793920 run.py:483] Algo bellman_ford step 2666 current loss 0.038468, current_train_items 85344.
I0302 18:59:12.627570 22760421793920 run.py:483] Algo bellman_ford step 2667 current loss 0.050744, current_train_items 85376.
I0302 18:59:12.657921 22760421793920 run.py:483] Algo bellman_ford step 2668 current loss 0.138160, current_train_items 85408.
I0302 18:59:12.689438 22760421793920 run.py:483] Algo bellman_ford step 2669 current loss 0.065216, current_train_items 85440.
I0302 18:59:12.707493 22760421793920 run.py:483] Algo bellman_ford step 2670 current loss 0.006936, current_train_items 85472.
I0302 18:59:12.723403 22760421793920 run.py:483] Algo bellman_ford step 2671 current loss 0.020363, current_train_items 85504.
I0302 18:59:12.746526 22760421793920 run.py:483] Algo bellman_ford step 2672 current loss 0.051425, current_train_items 85536.
I0302 18:59:12.777397 22760421793920 run.py:483] Algo bellman_ford step 2673 current loss 0.081669, current_train_items 85568.
I0302 18:59:12.811180 22760421793920 run.py:483] Algo bellman_ford step 2674 current loss 0.077825, current_train_items 85600.
I0302 18:59:12.829479 22760421793920 run.py:483] Algo bellman_ford step 2675 current loss 0.013415, current_train_items 85632.
I0302 18:59:12.845174 22760421793920 run.py:483] Algo bellman_ford step 2676 current loss 0.092357, current_train_items 85664.
I0302 18:59:12.869309 22760421793920 run.py:483] Algo bellman_ford step 2677 current loss 0.214343, current_train_items 85696.
I0302 18:59:12.897977 22760421793920 run.py:483] Algo bellman_ford step 2678 current loss 0.085136, current_train_items 85728.
I0302 18:59:12.928573 22760421793920 run.py:483] Algo bellman_ford step 2679 current loss 0.081258, current_train_items 85760.
I0302 18:59:12.946857 22760421793920 run.py:483] Algo bellman_ford step 2680 current loss 0.004774, current_train_items 85792.
I0302 18:59:12.962520 22760421793920 run.py:483] Algo bellman_ford step 2681 current loss 0.022361, current_train_items 85824.
I0302 18:59:12.985600 22760421793920 run.py:483] Algo bellman_ford step 2682 current loss 0.061467, current_train_items 85856.
I0302 18:59:13.016752 22760421793920 run.py:483] Algo bellman_ford step 2683 current loss 0.091511, current_train_items 85888.
I0302 18:59:13.049423 22760421793920 run.py:483] Algo bellman_ford step 2684 current loss 0.099931, current_train_items 85920.
I0302 18:59:13.067837 22760421793920 run.py:483] Algo bellman_ford step 2685 current loss 0.009993, current_train_items 85952.
I0302 18:59:13.083422 22760421793920 run.py:483] Algo bellman_ford step 2686 current loss 0.050654, current_train_items 85984.
I0302 18:59:13.106866 22760421793920 run.py:483] Algo bellman_ford step 2687 current loss 0.117764, current_train_items 86016.
I0302 18:59:13.136721 22760421793920 run.py:483] Algo bellman_ford step 2688 current loss 0.153884, current_train_items 86048.
I0302 18:59:13.167742 22760421793920 run.py:483] Algo bellman_ford step 2689 current loss 0.118945, current_train_items 86080.
I0302 18:59:13.186299 22760421793920 run.py:483] Algo bellman_ford step 2690 current loss 0.025198, current_train_items 86112.
I0302 18:59:13.202052 22760421793920 run.py:483] Algo bellman_ford step 2691 current loss 0.028064, current_train_items 86144.
I0302 18:59:13.226308 22760421793920 run.py:483] Algo bellman_ford step 2692 current loss 0.137908, current_train_items 86176.
I0302 18:59:13.257440 22760421793920 run.py:483] Algo bellman_ford step 2693 current loss 0.227057, current_train_items 86208.
I0302 18:59:13.289725 22760421793920 run.py:483] Algo bellman_ford step 2694 current loss 0.225050, current_train_items 86240.
I0302 18:59:13.308469 22760421793920 run.py:483] Algo bellman_ford step 2695 current loss 0.001684, current_train_items 86272.
I0302 18:59:13.323877 22760421793920 run.py:483] Algo bellman_ford step 2696 current loss 0.017691, current_train_items 86304.
I0302 18:59:13.347087 22760421793920 run.py:483] Algo bellman_ford step 2697 current loss 0.062141, current_train_items 86336.
I0302 18:59:13.376283 22760421793920 run.py:483] Algo bellman_ford step 2698 current loss 0.104151, current_train_items 86368.
I0302 18:59:13.406828 22760421793920 run.py:483] Algo bellman_ford step 2699 current loss 0.165862, current_train_items 86400.
I0302 18:59:13.424938 22760421793920 run.py:483] Algo bellman_ford step 2700 current loss 0.012413, current_train_items 86432.
I0302 18:59:13.432522 22760421793920 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:13.432631 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 18:59:13.448800 22760421793920 run.py:483] Algo bellman_ford step 2701 current loss 0.020660, current_train_items 86464.
I0302 18:59:13.472221 22760421793920 run.py:483] Algo bellman_ford step 2702 current loss 0.038025, current_train_items 86496.
I0302 18:59:13.504264 22760421793920 run.py:483] Algo bellman_ford step 2703 current loss 0.092934, current_train_items 86528.
I0302 18:59:13.539042 22760421793920 run.py:483] Algo bellman_ford step 2704 current loss 0.116529, current_train_items 86560.
I0302 18:59:13.557640 22760421793920 run.py:483] Algo bellman_ford step 2705 current loss 0.005592, current_train_items 86592.
I0302 18:59:13.573116 22760421793920 run.py:483] Algo bellman_ford step 2706 current loss 0.040718, current_train_items 86624.
I0302 18:59:13.596507 22760421793920 run.py:483] Algo bellman_ford step 2707 current loss 0.145448, current_train_items 86656.
I0302 18:59:13.624819 22760421793920 run.py:483] Algo bellman_ford step 2708 current loss 0.119671, current_train_items 86688.
I0302 18:59:13.655349 22760421793920 run.py:483] Algo bellman_ford step 2709 current loss 0.075310, current_train_items 86720.
I0302 18:59:13.673863 22760421793920 run.py:483] Algo bellman_ford step 2710 current loss 0.005323, current_train_items 86752.
I0302 18:59:13.689921 22760421793920 run.py:483] Algo bellman_ford step 2711 current loss 0.043656, current_train_items 86784.
I0302 18:59:13.713529 22760421793920 run.py:483] Algo bellman_ford step 2712 current loss 0.170104, current_train_items 86816.
I0302 18:59:13.744139 22760421793920 run.py:483] Algo bellman_ford step 2713 current loss 0.237257, current_train_items 86848.
I0302 18:59:13.778381 22760421793920 run.py:483] Algo bellman_ford step 2714 current loss 0.220846, current_train_items 86880.
I0302 18:59:13.796634 22760421793920 run.py:483] Algo bellman_ford step 2715 current loss 0.013661, current_train_items 86912.
I0302 18:59:13.811836 22760421793920 run.py:483] Algo bellman_ford step 2716 current loss 0.015287, current_train_items 86944.
I0302 18:59:13.836085 22760421793920 run.py:483] Algo bellman_ford step 2717 current loss 0.115265, current_train_items 86976.
I0302 18:59:13.864310 22760421793920 run.py:483] Algo bellman_ford step 2718 current loss 0.116392, current_train_items 87008.
I0302 18:59:13.896736 22760421793920 run.py:483] Algo bellman_ford step 2719 current loss 0.132248, current_train_items 87040.
I0302 18:59:13.914778 22760421793920 run.py:483] Algo bellman_ford step 2720 current loss 0.004350, current_train_items 87072.
I0302 18:59:13.930383 22760421793920 run.py:483] Algo bellman_ford step 2721 current loss 0.066101, current_train_items 87104.
I0302 18:59:13.953174 22760421793920 run.py:483] Algo bellman_ford step 2722 current loss 0.063944, current_train_items 87136.
I0302 18:59:13.983995 22760421793920 run.py:483] Algo bellman_ford step 2723 current loss 0.083634, current_train_items 87168.
I0302 18:59:14.015795 22760421793920 run.py:483] Algo bellman_ford step 2724 current loss 0.081131, current_train_items 87200.
I0302 18:59:14.033868 22760421793920 run.py:483] Algo bellman_ford step 2725 current loss 0.002628, current_train_items 87232.
I0302 18:59:14.049472 22760421793920 run.py:483] Algo bellman_ford step 2726 current loss 0.029666, current_train_items 87264.
I0302 18:59:14.072451 22760421793920 run.py:483] Algo bellman_ford step 2727 current loss 0.072611, current_train_items 87296.
I0302 18:59:14.102375 22760421793920 run.py:483] Algo bellman_ford step 2728 current loss 0.075712, current_train_items 87328.
I0302 18:59:14.134413 22760421793920 run.py:483] Algo bellman_ford step 2729 current loss 0.080876, current_train_items 87360.
I0302 18:59:14.153115 22760421793920 run.py:483] Algo bellman_ford step 2730 current loss 0.002401, current_train_items 87392.
I0302 18:59:14.168745 22760421793920 run.py:483] Algo bellman_ford step 2731 current loss 0.035329, current_train_items 87424.
I0302 18:59:14.190753 22760421793920 run.py:483] Algo bellman_ford step 2732 current loss 0.030751, current_train_items 87456.
I0302 18:59:14.219403 22760421793920 run.py:483] Algo bellman_ford step 2733 current loss 0.052743, current_train_items 87488.
I0302 18:59:14.251598 22760421793920 run.py:483] Algo bellman_ford step 2734 current loss 0.081545, current_train_items 87520.
I0302 18:59:14.270322 22760421793920 run.py:483] Algo bellman_ford step 2735 current loss 0.020863, current_train_items 87552.
I0302 18:59:14.285591 22760421793920 run.py:483] Algo bellman_ford step 2736 current loss 0.013549, current_train_items 87584.
I0302 18:59:14.308556 22760421793920 run.py:483] Algo bellman_ford step 2737 current loss 0.084375, current_train_items 87616.
I0302 18:59:14.337374 22760421793920 run.py:483] Algo bellman_ford step 2738 current loss 0.083756, current_train_items 87648.
I0302 18:59:14.368445 22760421793920 run.py:483] Algo bellman_ford step 2739 current loss 0.082117, current_train_items 87680.
I0302 18:59:14.386734 22760421793920 run.py:483] Algo bellman_ford step 2740 current loss 0.003672, current_train_items 87712.
I0302 18:59:14.402212 22760421793920 run.py:483] Algo bellman_ford step 2741 current loss 0.026300, current_train_items 87744.
I0302 18:59:14.425295 22760421793920 run.py:483] Algo bellman_ford step 2742 current loss 0.060843, current_train_items 87776.
I0302 18:59:14.456926 22760421793920 run.py:483] Algo bellman_ford step 2743 current loss 0.308695, current_train_items 87808.
I0302 18:59:14.488411 22760421793920 run.py:483] Algo bellman_ford step 2744 current loss 0.148588, current_train_items 87840.
I0302 18:59:14.506537 22760421793920 run.py:483] Algo bellman_ford step 2745 current loss 0.020791, current_train_items 87872.
I0302 18:59:14.521970 22760421793920 run.py:483] Algo bellman_ford step 2746 current loss 0.043980, current_train_items 87904.
I0302 18:59:14.545304 22760421793920 run.py:483] Algo bellman_ford step 2747 current loss 0.136452, current_train_items 87936.
I0302 18:59:14.575798 22760421793920 run.py:483] Algo bellman_ford step 2748 current loss 0.131498, current_train_items 87968.
I0302 18:59:14.607269 22760421793920 run.py:483] Algo bellman_ford step 2749 current loss 0.158707, current_train_items 88000.
I0302 18:59:14.625704 22760421793920 run.py:483] Algo bellman_ford step 2750 current loss 0.019086, current_train_items 88032.
I0302 18:59:14.632970 22760421793920 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:14.633121 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:14.649609 22760421793920 run.py:483] Algo bellman_ford step 2751 current loss 0.031520, current_train_items 88064.
I0302 18:59:14.673381 22760421793920 run.py:483] Algo bellman_ford step 2752 current loss 0.107208, current_train_items 88096.
I0302 18:59:14.704360 22760421793920 run.py:483] Algo bellman_ford step 2753 current loss 0.160764, current_train_items 88128.
I0302 18:59:14.735695 22760421793920 run.py:483] Algo bellman_ford step 2754 current loss 0.125275, current_train_items 88160.
I0302 18:59:14.754139 22760421793920 run.py:483] Algo bellman_ford step 2755 current loss 0.011640, current_train_items 88192.
I0302 18:59:14.770517 22760421793920 run.py:483] Algo bellman_ford step 2756 current loss 0.049963, current_train_items 88224.
I0302 18:59:14.792586 22760421793920 run.py:483] Algo bellman_ford step 2757 current loss 0.035743, current_train_items 88256.
I0302 18:59:14.823033 22760421793920 run.py:483] Algo bellman_ford step 2758 current loss 0.125285, current_train_items 88288.
I0302 18:59:14.857478 22760421793920 run.py:483] Algo bellman_ford step 2759 current loss 0.247364, current_train_items 88320.
I0302 18:59:14.875507 22760421793920 run.py:483] Algo bellman_ford step 2760 current loss 0.010030, current_train_items 88352.
I0302 18:59:14.891547 22760421793920 run.py:483] Algo bellman_ford step 2761 current loss 0.042806, current_train_items 88384.
I0302 18:59:14.914839 22760421793920 run.py:483] Algo bellman_ford step 2762 current loss 0.160349, current_train_items 88416.
I0302 18:59:14.945052 22760421793920 run.py:483] Algo bellman_ford step 2763 current loss 0.158054, current_train_items 88448.
I0302 18:59:14.977275 22760421793920 run.py:483] Algo bellman_ford step 2764 current loss 0.176231, current_train_items 88480.
I0302 18:59:14.995637 22760421793920 run.py:483] Algo bellman_ford step 2765 current loss 0.006437, current_train_items 88512.
I0302 18:59:15.011536 22760421793920 run.py:483] Algo bellman_ford step 2766 current loss 0.031597, current_train_items 88544.
I0302 18:59:15.034538 22760421793920 run.py:483] Algo bellman_ford step 2767 current loss 0.055632, current_train_items 88576.
I0302 18:59:15.065689 22760421793920 run.py:483] Algo bellman_ford step 2768 current loss 0.118053, current_train_items 88608.
I0302 18:59:15.097623 22760421793920 run.py:483] Algo bellman_ford step 2769 current loss 0.109352, current_train_items 88640.
I0302 18:59:15.115882 22760421793920 run.py:483] Algo bellman_ford step 2770 current loss 0.002452, current_train_items 88672.
I0302 18:59:15.131656 22760421793920 run.py:483] Algo bellman_ford step 2771 current loss 0.027497, current_train_items 88704.
I0302 18:59:15.155099 22760421793920 run.py:483] Algo bellman_ford step 2772 current loss 0.056723, current_train_items 88736.
I0302 18:59:15.184410 22760421793920 run.py:483] Algo bellman_ford step 2773 current loss 0.067802, current_train_items 88768.
I0302 18:59:15.216728 22760421793920 run.py:483] Algo bellman_ford step 2774 current loss 0.078606, current_train_items 88800.
I0302 18:59:15.234924 22760421793920 run.py:483] Algo bellman_ford step 2775 current loss 0.005533, current_train_items 88832.
I0302 18:59:15.251113 22760421793920 run.py:483] Algo bellman_ford step 2776 current loss 0.161019, current_train_items 88864.
I0302 18:59:15.273973 22760421793920 run.py:483] Algo bellman_ford step 2777 current loss 0.056231, current_train_items 88896.
I0302 18:59:15.303671 22760421793920 run.py:483] Algo bellman_ford step 2778 current loss 0.095911, current_train_items 88928.
I0302 18:59:15.336517 22760421793920 run.py:483] Algo bellman_ford step 2779 current loss 0.146706, current_train_items 88960.
I0302 18:59:15.354744 22760421793920 run.py:483] Algo bellman_ford step 2780 current loss 0.008241, current_train_items 88992.
I0302 18:59:15.370437 22760421793920 run.py:483] Algo bellman_ford step 2781 current loss 0.053803, current_train_items 89024.
I0302 18:59:15.393621 22760421793920 run.py:483] Algo bellman_ford step 2782 current loss 0.163903, current_train_items 89056.
I0302 18:59:15.423174 22760421793920 run.py:483] Algo bellman_ford step 2783 current loss 0.088564, current_train_items 89088.
I0302 18:59:15.455859 22760421793920 run.py:483] Algo bellman_ford step 2784 current loss 0.089532, current_train_items 89120.
I0302 18:59:15.474501 22760421793920 run.py:483] Algo bellman_ford step 2785 current loss 0.006307, current_train_items 89152.
I0302 18:59:15.489949 22760421793920 run.py:483] Algo bellman_ford step 2786 current loss 0.039313, current_train_items 89184.
I0302 18:59:15.512074 22760421793920 run.py:483] Algo bellman_ford step 2787 current loss 0.053839, current_train_items 89216.
I0302 18:59:15.541366 22760421793920 run.py:483] Algo bellman_ford step 2788 current loss 0.126580, current_train_items 89248.
I0302 18:59:15.570654 22760421793920 run.py:483] Algo bellman_ford step 2789 current loss 0.117094, current_train_items 89280.
I0302 18:59:15.588656 22760421793920 run.py:483] Algo bellman_ford step 2790 current loss 0.017926, current_train_items 89312.
I0302 18:59:15.604270 22760421793920 run.py:483] Algo bellman_ford step 2791 current loss 0.059860, current_train_items 89344.
I0302 18:59:15.627496 22760421793920 run.py:483] Algo bellman_ford step 2792 current loss 0.143858, current_train_items 89376.
I0302 18:59:15.656774 22760421793920 run.py:483] Algo bellman_ford step 2793 current loss 0.194006, current_train_items 89408.
I0302 18:59:15.691060 22760421793920 run.py:483] Algo bellman_ford step 2794 current loss 0.223395, current_train_items 89440.
I0302 18:59:15.709317 22760421793920 run.py:483] Algo bellman_ford step 2795 current loss 0.003581, current_train_items 89472.
I0302 18:59:15.725576 22760421793920 run.py:483] Algo bellman_ford step 2796 current loss 0.061697, current_train_items 89504.
I0302 18:59:15.748296 22760421793920 run.py:483] Algo bellman_ford step 2797 current loss 0.128808, current_train_items 89536.
I0302 18:59:15.778844 22760421793920 run.py:483] Algo bellman_ford step 2798 current loss 0.138363, current_train_items 89568.
I0302 18:59:15.811053 22760421793920 run.py:483] Algo bellman_ford step 2799 current loss 0.217446, current_train_items 89600.
I0302 18:59:15.829341 22760421793920 run.py:483] Algo bellman_ford step 2800 current loss 0.019019, current_train_items 89632.
I0302 18:59:15.836756 22760421793920 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:15.836869 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0302 18:59:15.853388 22760421793920 run.py:483] Algo bellman_ford step 2801 current loss 0.047591, current_train_items 89664.
I0302 18:59:15.877524 22760421793920 run.py:483] Algo bellman_ford step 2802 current loss 0.257717, current_train_items 89696.
I0302 18:59:15.908572 22760421793920 run.py:483] Algo bellman_ford step 2803 current loss 0.123598, current_train_items 89728.
I0302 18:59:15.941230 22760421793920 run.py:483] Algo bellman_ford step 2804 current loss 0.114126, current_train_items 89760.
I0302 18:59:15.959789 22760421793920 run.py:483] Algo bellman_ford step 2805 current loss 0.033180, current_train_items 89792.
I0302 18:59:15.975848 22760421793920 run.py:483] Algo bellman_ford step 2806 current loss 0.113532, current_train_items 89824.
I0302 18:59:15.998396 22760421793920 run.py:483] Algo bellman_ford step 2807 current loss 0.142751, current_train_items 89856.
I0302 18:59:16.029212 22760421793920 run.py:483] Algo bellman_ford step 2808 current loss 0.280149, current_train_items 89888.
I0302 18:59:16.062146 22760421793920 run.py:483] Algo bellman_ford step 2809 current loss 0.274526, current_train_items 89920.
I0302 18:59:16.080772 22760421793920 run.py:483] Algo bellman_ford step 2810 current loss 0.013820, current_train_items 89952.
I0302 18:59:16.096865 22760421793920 run.py:483] Algo bellman_ford step 2811 current loss 0.078622, current_train_items 89984.
I0302 18:59:16.119241 22760421793920 run.py:483] Algo bellman_ford step 2812 current loss 0.042010, current_train_items 90016.
I0302 18:59:16.150082 22760421793920 run.py:483] Algo bellman_ford step 2813 current loss 0.139277, current_train_items 90048.
I0302 18:59:16.181277 22760421793920 run.py:483] Algo bellman_ford step 2814 current loss 0.162185, current_train_items 90080.
I0302 18:59:16.199497 22760421793920 run.py:483] Algo bellman_ford step 2815 current loss 0.015119, current_train_items 90112.
I0302 18:59:16.215448 22760421793920 run.py:483] Algo bellman_ford step 2816 current loss 0.041250, current_train_items 90144.
I0302 18:59:16.238957 22760421793920 run.py:483] Algo bellman_ford step 2817 current loss 0.078098, current_train_items 90176.
I0302 18:59:16.265944 22760421793920 run.py:483] Algo bellman_ford step 2818 current loss 0.052777, current_train_items 90208.
I0302 18:59:16.298007 22760421793920 run.py:483] Algo bellman_ford step 2819 current loss 0.074158, current_train_items 90240.
I0302 18:59:16.316139 22760421793920 run.py:483] Algo bellman_ford step 2820 current loss 0.003872, current_train_items 90272.
I0302 18:59:16.331686 22760421793920 run.py:483] Algo bellman_ford step 2821 current loss 0.037292, current_train_items 90304.
I0302 18:59:16.354551 22760421793920 run.py:483] Algo bellman_ford step 2822 current loss 0.079904, current_train_items 90336.
I0302 18:59:16.384256 22760421793920 run.py:483] Algo bellman_ford step 2823 current loss 0.144009, current_train_items 90368.
I0302 18:59:16.418714 22760421793920 run.py:483] Algo bellman_ford step 2824 current loss 0.127778, current_train_items 90400.
I0302 18:59:16.437342 22760421793920 run.py:483] Algo bellman_ford step 2825 current loss 0.011291, current_train_items 90432.
I0302 18:59:16.452729 22760421793920 run.py:483] Algo bellman_ford step 2826 current loss 0.039155, current_train_items 90464.
I0302 18:59:16.476080 22760421793920 run.py:483] Algo bellman_ford step 2827 current loss 0.104471, current_train_items 90496.
I0302 18:59:16.506136 22760421793920 run.py:483] Algo bellman_ford step 2828 current loss 0.087075, current_train_items 90528.
I0302 18:59:16.536861 22760421793920 run.py:483] Algo bellman_ford step 2829 current loss 0.063569, current_train_items 90560.
I0302 18:59:16.555279 22760421793920 run.py:483] Algo bellman_ford step 2830 current loss 0.014227, current_train_items 90592.
I0302 18:59:16.571265 22760421793920 run.py:483] Algo bellman_ford step 2831 current loss 0.048744, current_train_items 90624.
I0302 18:59:16.594042 22760421793920 run.py:483] Algo bellman_ford step 2832 current loss 0.078313, current_train_items 90656.
I0302 18:59:16.622893 22760421793920 run.py:483] Algo bellman_ford step 2833 current loss 0.067201, current_train_items 90688.
I0302 18:59:16.654584 22760421793920 run.py:483] Algo bellman_ford step 2834 current loss 0.099188, current_train_items 90720.
I0302 18:59:16.672865 22760421793920 run.py:483] Algo bellman_ford step 2835 current loss 0.023202, current_train_items 90752.
I0302 18:59:16.688179 22760421793920 run.py:483] Algo bellman_ford step 2836 current loss 0.050507, current_train_items 90784.
I0302 18:59:16.710818 22760421793920 run.py:483] Algo bellman_ford step 2837 current loss 0.067756, current_train_items 90816.
I0302 18:59:16.740768 22760421793920 run.py:483] Algo bellman_ford step 2838 current loss 0.101282, current_train_items 90848.
I0302 18:59:16.775372 22760421793920 run.py:483] Algo bellman_ford step 2839 current loss 0.111101, current_train_items 90880.
I0302 18:59:16.793950 22760421793920 run.py:483] Algo bellman_ford step 2840 current loss 0.029972, current_train_items 90912.
I0302 18:59:16.809350 22760421793920 run.py:483] Algo bellman_ford step 2841 current loss 0.019850, current_train_items 90944.
I0302 18:59:16.832649 22760421793920 run.py:483] Algo bellman_ford step 2842 current loss 0.094791, current_train_items 90976.
I0302 18:59:16.863101 22760421793920 run.py:483] Algo bellman_ford step 2843 current loss 0.134138, current_train_items 91008.
I0302 18:59:16.895639 22760421793920 run.py:483] Algo bellman_ford step 2844 current loss 0.134199, current_train_items 91040.
I0302 18:59:16.914348 22760421793920 run.py:483] Algo bellman_ford step 2845 current loss 0.004735, current_train_items 91072.
I0302 18:59:16.930491 22760421793920 run.py:483] Algo bellman_ford step 2846 current loss 0.086298, current_train_items 91104.
I0302 18:59:16.953377 22760421793920 run.py:483] Algo bellman_ford step 2847 current loss 0.103854, current_train_items 91136.
I0302 18:59:16.983216 22760421793920 run.py:483] Algo bellman_ford step 2848 current loss 0.094361, current_train_items 91168.
I0302 18:59:17.016653 22760421793920 run.py:483] Algo bellman_ford step 2849 current loss 0.093411, current_train_items 91200.
I0302 18:59:17.034744 22760421793920 run.py:483] Algo bellman_ford step 2850 current loss 0.004104, current_train_items 91232.
I0302 18:59:17.042105 22760421793920 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:17.042216 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0302 18:59:17.058027 22760421793920 run.py:483] Algo bellman_ford step 2851 current loss 0.033596, current_train_items 91264.
I0302 18:59:17.080835 22760421793920 run.py:483] Algo bellman_ford step 2852 current loss 0.063544, current_train_items 91296.
I0302 18:59:17.111442 22760421793920 run.py:483] Algo bellman_ford step 2853 current loss 0.162332, current_train_items 91328.
I0302 18:59:17.143603 22760421793920 run.py:483] Algo bellman_ford step 2854 current loss 0.181341, current_train_items 91360.
I0302 18:59:17.161907 22760421793920 run.py:483] Algo bellman_ford step 2855 current loss 0.006967, current_train_items 91392.
I0302 18:59:17.177881 22760421793920 run.py:483] Algo bellman_ford step 2856 current loss 0.057196, current_train_items 91424.
I0302 18:59:17.200723 22760421793920 run.py:483] Algo bellman_ford step 2857 current loss 0.085794, current_train_items 91456.
I0302 18:59:17.228702 22760421793920 run.py:483] Algo bellman_ford step 2858 current loss 0.095859, current_train_items 91488.
I0302 18:59:17.259252 22760421793920 run.py:483] Algo bellman_ford step 2859 current loss 0.076322, current_train_items 91520.
I0302 18:59:17.277938 22760421793920 run.py:483] Algo bellman_ford step 2860 current loss 0.005361, current_train_items 91552.
I0302 18:59:17.293560 22760421793920 run.py:483] Algo bellman_ford step 2861 current loss 0.037460, current_train_items 91584.
I0302 18:59:17.316611 22760421793920 run.py:483] Algo bellman_ford step 2862 current loss 0.046516, current_train_items 91616.
I0302 18:59:17.346859 22760421793920 run.py:483] Algo bellman_ford step 2863 current loss 0.115158, current_train_items 91648.
I0302 18:59:17.376042 22760421793920 run.py:483] Algo bellman_ford step 2864 current loss 0.092948, current_train_items 91680.
I0302 18:59:17.394312 22760421793920 run.py:483] Algo bellman_ford step 2865 current loss 0.023604, current_train_items 91712.
I0302 18:59:17.410033 22760421793920 run.py:483] Algo bellman_ford step 2866 current loss 0.051799, current_train_items 91744.
I0302 18:59:17.432564 22760421793920 run.py:483] Algo bellman_ford step 2867 current loss 0.104518, current_train_items 91776.
I0302 18:59:17.461634 22760421793920 run.py:483] Algo bellman_ford step 2868 current loss 0.107457, current_train_items 91808.
I0302 18:59:17.494107 22760421793920 run.py:483] Algo bellman_ford step 2869 current loss 0.119977, current_train_items 91840.
I0302 18:59:17.512202 22760421793920 run.py:483] Algo bellman_ford step 2870 current loss 0.005783, current_train_items 91872.
I0302 18:59:17.527874 22760421793920 run.py:483] Algo bellman_ford step 2871 current loss 0.028972, current_train_items 91904.
I0302 18:59:17.551302 22760421793920 run.py:483] Algo bellman_ford step 2872 current loss 0.047052, current_train_items 91936.
I0302 18:59:17.581984 22760421793920 run.py:483] Algo bellman_ford step 2873 current loss 0.113733, current_train_items 91968.
I0302 18:59:17.612829 22760421793920 run.py:483] Algo bellman_ford step 2874 current loss 0.125544, current_train_items 92000.
I0302 18:59:17.630855 22760421793920 run.py:483] Algo bellman_ford step 2875 current loss 0.002134, current_train_items 92032.
I0302 18:59:17.646573 22760421793920 run.py:483] Algo bellman_ford step 2876 current loss 0.015145, current_train_items 92064.
I0302 18:59:17.669102 22760421793920 run.py:483] Algo bellman_ford step 2877 current loss 0.025800, current_train_items 92096.
I0302 18:59:17.700084 22760421793920 run.py:483] Algo bellman_ford step 2878 current loss 0.099667, current_train_items 92128.
I0302 18:59:17.730966 22760421793920 run.py:483] Algo bellman_ford step 2879 current loss 0.090862, current_train_items 92160.
I0302 18:59:17.749193 22760421793920 run.py:483] Algo bellman_ford step 2880 current loss 0.033765, current_train_items 92192.
I0302 18:59:17.764866 22760421793920 run.py:483] Algo bellman_ford step 2881 current loss 0.008302, current_train_items 92224.
I0302 18:59:17.787655 22760421793920 run.py:483] Algo bellman_ford step 2882 current loss 0.063709, current_train_items 92256.
I0302 18:59:17.817719 22760421793920 run.py:483] Algo bellman_ford step 2883 current loss 0.075213, current_train_items 92288.
I0302 18:59:17.847977 22760421793920 run.py:483] Algo bellman_ford step 2884 current loss 0.081641, current_train_items 92320.
I0302 18:59:17.866004 22760421793920 run.py:483] Algo bellman_ford step 2885 current loss 0.003097, current_train_items 92352.
I0302 18:59:17.881592 22760421793920 run.py:483] Algo bellman_ford step 2886 current loss 0.094145, current_train_items 92384.
I0302 18:59:17.904786 22760421793920 run.py:483] Algo bellman_ford step 2887 current loss 0.059075, current_train_items 92416.
I0302 18:59:17.933629 22760421793920 run.py:483] Algo bellman_ford step 2888 current loss 0.055827, current_train_items 92448.
I0302 18:59:17.966029 22760421793920 run.py:483] Algo bellman_ford step 2889 current loss 0.120847, current_train_items 92480.
I0302 18:59:17.984159 22760421793920 run.py:483] Algo bellman_ford step 2890 current loss 0.006227, current_train_items 92512.
I0302 18:59:17.999762 22760421793920 run.py:483] Algo bellman_ford step 2891 current loss 0.042788, current_train_items 92544.
I0302 18:59:18.023847 22760421793920 run.py:483] Algo bellman_ford step 2892 current loss 0.063605, current_train_items 92576.
I0302 18:59:18.052403 22760421793920 run.py:483] Algo bellman_ford step 2893 current loss 0.061860, current_train_items 92608.
I0302 18:59:18.084342 22760421793920 run.py:483] Algo bellman_ford step 2894 current loss 0.091775, current_train_items 92640.
I0302 18:59:18.102566 22760421793920 run.py:483] Algo bellman_ford step 2895 current loss 0.006786, current_train_items 92672.
I0302 18:59:18.118827 22760421793920 run.py:483] Algo bellman_ford step 2896 current loss 0.041787, current_train_items 92704.
I0302 18:59:18.140322 22760421793920 run.py:483] Algo bellman_ford step 2897 current loss 0.084094, current_train_items 92736.
I0302 18:59:18.169560 22760421793920 run.py:483] Algo bellman_ford step 2898 current loss 0.052631, current_train_items 92768.
I0302 18:59:18.198834 22760421793920 run.py:483] Algo bellman_ford step 2899 current loss 0.093323, current_train_items 92800.
I0302 18:59:18.216620 22760421793920 run.py:483] Algo bellman_ford step 2900 current loss 0.006401, current_train_items 92832.
I0302 18:59:18.224028 22760421793920 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:18.224138 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0302 18:59:18.240023 22760421793920 run.py:483] Algo bellman_ford step 2901 current loss 0.054266, current_train_items 92864.
I0302 18:59:18.262623 22760421793920 run.py:483] Algo bellman_ford step 2902 current loss 0.178462, current_train_items 92896.
I0302 18:59:18.292405 22760421793920 run.py:483] Algo bellman_ford step 2903 current loss 0.153226, current_train_items 92928.
I0302 18:59:18.324971 22760421793920 run.py:483] Algo bellman_ford step 2904 current loss 0.172356, current_train_items 92960.
I0302 18:59:18.343500 22760421793920 run.py:483] Algo bellman_ford step 2905 current loss 0.031097, current_train_items 92992.
I0302 18:59:18.359148 22760421793920 run.py:483] Algo bellman_ford step 2906 current loss 0.159919, current_train_items 93024.
I0302 18:59:18.382189 22760421793920 run.py:483] Algo bellman_ford step 2907 current loss 0.244884, current_train_items 93056.
I0302 18:59:18.413378 22760421793920 run.py:483] Algo bellman_ford step 2908 current loss 0.373786, current_train_items 93088.
I0302 18:59:18.445828 22760421793920 run.py:483] Algo bellman_ford step 2909 current loss 0.247694, current_train_items 93120.
I0302 18:59:18.463875 22760421793920 run.py:483] Algo bellman_ford step 2910 current loss 0.011618, current_train_items 93152.
I0302 18:59:18.480143 22760421793920 run.py:483] Algo bellman_ford step 2911 current loss 0.029412, current_train_items 93184.
I0302 18:59:18.502911 22760421793920 run.py:483] Algo bellman_ford step 2912 current loss 0.088249, current_train_items 93216.
I0302 18:59:18.532296 22760421793920 run.py:483] Algo bellman_ford step 2913 current loss 0.178991, current_train_items 93248.
I0302 18:59:18.565513 22760421793920 run.py:483] Algo bellman_ford step 2914 current loss 0.306049, current_train_items 93280.
I0302 18:59:18.583457 22760421793920 run.py:483] Algo bellman_ford step 2915 current loss 0.011741, current_train_items 93312.
I0302 18:59:18.599096 22760421793920 run.py:483] Algo bellman_ford step 2916 current loss 0.031937, current_train_items 93344.
I0302 18:59:18.621384 22760421793920 run.py:483] Algo bellman_ford step 2917 current loss 0.150070, current_train_items 93376.
I0302 18:59:18.652433 22760421793920 run.py:483] Algo bellman_ford step 2918 current loss 0.170698, current_train_items 93408.
I0302 18:59:18.685827 22760421793920 run.py:483] Algo bellman_ford step 2919 current loss 0.137988, current_train_items 93440.
I0302 18:59:18.704019 22760421793920 run.py:483] Algo bellman_ford step 2920 current loss 0.014315, current_train_items 93472.
I0302 18:59:18.719716 22760421793920 run.py:483] Algo bellman_ford step 2921 current loss 0.057043, current_train_items 93504.
I0302 18:59:18.743106 22760421793920 run.py:483] Algo bellman_ford step 2922 current loss 0.239760, current_train_items 93536.
I0302 18:59:18.772007 22760421793920 run.py:483] Algo bellman_ford step 2923 current loss 0.131118, current_train_items 93568.
I0302 18:59:18.805700 22760421793920 run.py:483] Algo bellman_ford step 2924 current loss 0.302720, current_train_items 93600.
I0302 18:59:18.823874 22760421793920 run.py:483] Algo bellman_ford step 2925 current loss 0.046285, current_train_items 93632.
I0302 18:59:18.839373 22760421793920 run.py:483] Algo bellman_ford step 2926 current loss 0.087776, current_train_items 93664.
I0302 18:59:18.863029 22760421793920 run.py:483] Algo bellman_ford step 2927 current loss 0.172334, current_train_items 93696.
I0302 18:59:18.893473 22760421793920 run.py:483] Algo bellman_ford step 2928 current loss 0.183479, current_train_items 93728.
I0302 18:59:18.926028 22760421793920 run.py:483] Algo bellman_ford step 2929 current loss 0.233778, current_train_items 93760.
I0302 18:59:18.944130 22760421793920 run.py:483] Algo bellman_ford step 2930 current loss 0.011281, current_train_items 93792.
I0302 18:59:18.959654 22760421793920 run.py:483] Algo bellman_ford step 2931 current loss 0.061805, current_train_items 93824.
I0302 18:59:18.983967 22760421793920 run.py:483] Algo bellman_ford step 2932 current loss 0.093292, current_train_items 93856.
I0302 18:59:19.012770 22760421793920 run.py:483] Algo bellman_ford step 2933 current loss 0.084963, current_train_items 93888.
I0302 18:59:19.044232 22760421793920 run.py:483] Algo bellman_ford step 2934 current loss 0.148082, current_train_items 93920.
I0302 18:59:19.062583 22760421793920 run.py:483] Algo bellman_ford step 2935 current loss 0.033737, current_train_items 93952.
I0302 18:59:19.078092 22760421793920 run.py:483] Algo bellman_ford step 2936 current loss 0.122955, current_train_items 93984.
I0302 18:59:19.100955 22760421793920 run.py:483] Algo bellman_ford step 2937 current loss 0.199117, current_train_items 94016.
I0302 18:59:19.129060 22760421793920 run.py:483] Algo bellman_ford step 2938 current loss 0.195196, current_train_items 94048.
I0302 18:59:19.160830 22760421793920 run.py:483] Algo bellman_ford step 2939 current loss 0.119917, current_train_items 94080.
I0302 18:59:19.178959 22760421793920 run.py:483] Algo bellman_ford step 2940 current loss 0.031516, current_train_items 94112.
I0302 18:59:19.194872 22760421793920 run.py:483] Algo bellman_ford step 2941 current loss 0.058293, current_train_items 94144.
I0302 18:59:19.218428 22760421793920 run.py:483] Algo bellman_ford step 2942 current loss 0.179579, current_train_items 94176.
I0302 18:59:19.249125 22760421793920 run.py:483] Algo bellman_ford step 2943 current loss 0.292812, current_train_items 94208.
I0302 18:59:19.282848 22760421793920 run.py:483] Algo bellman_ford step 2944 current loss 0.251356, current_train_items 94240.
I0302 18:59:19.300814 22760421793920 run.py:483] Algo bellman_ford step 2945 current loss 0.021253, current_train_items 94272.
I0302 18:59:19.316788 22760421793920 run.py:483] Algo bellman_ford step 2946 current loss 0.044098, current_train_items 94304.
I0302 18:59:19.339636 22760421793920 run.py:483] Algo bellman_ford step 2947 current loss 0.057156, current_train_items 94336.
I0302 18:59:19.369278 22760421793920 run.py:483] Algo bellman_ford step 2948 current loss 0.127547, current_train_items 94368.
I0302 18:59:19.401017 22760421793920 run.py:483] Algo bellman_ford step 2949 current loss 0.192927, current_train_items 94400.
I0302 18:59:19.418946 22760421793920 run.py:483] Algo bellman_ford step 2950 current loss 0.008913, current_train_items 94432.
I0302 18:59:19.426509 22760421793920 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.947265625, 'score': 0.947265625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:19.426617 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.947, val scores are: bellman_ford: 0.947
I0302 18:59:19.442949 22760421793920 run.py:483] Algo bellman_ford step 2951 current loss 0.076503, current_train_items 94464.
I0302 18:59:19.464905 22760421793920 run.py:483] Algo bellman_ford step 2952 current loss 0.091608, current_train_items 94496.
I0302 18:59:19.494360 22760421793920 run.py:483] Algo bellman_ford step 2953 current loss 0.103003, current_train_items 94528.
I0302 18:59:19.527293 22760421793920 run.py:483] Algo bellman_ford step 2954 current loss 0.114521, current_train_items 94560.
I0302 18:59:19.546129 22760421793920 run.py:483] Algo bellman_ford step 2955 current loss 0.011136, current_train_items 94592.
I0302 18:59:19.561874 22760421793920 run.py:483] Algo bellman_ford step 2956 current loss 0.165973, current_train_items 94624.
I0302 18:59:19.585638 22760421793920 run.py:483] Algo bellman_ford step 2957 current loss 0.280673, current_train_items 94656.
I0302 18:59:19.616599 22760421793920 run.py:483] Algo bellman_ford step 2958 current loss 0.139792, current_train_items 94688.
I0302 18:59:19.648944 22760421793920 run.py:483] Algo bellman_ford step 2959 current loss 0.142988, current_train_items 94720.
I0302 18:59:19.667311 22760421793920 run.py:483] Algo bellman_ford step 2960 current loss 0.024592, current_train_items 94752.
I0302 18:59:19.683179 22760421793920 run.py:483] Algo bellman_ford step 2961 current loss 0.074955, current_train_items 94784.
I0302 18:59:19.706027 22760421793920 run.py:483] Algo bellman_ford step 2962 current loss 0.336430, current_train_items 94816.
I0302 18:59:19.734893 22760421793920 run.py:483] Algo bellman_ford step 2963 current loss 0.256345, current_train_items 94848.
I0302 18:59:19.767690 22760421793920 run.py:483] Algo bellman_ford step 2964 current loss 0.339087, current_train_items 94880.
I0302 18:59:19.785593 22760421793920 run.py:483] Algo bellman_ford step 2965 current loss 0.004219, current_train_items 94912.
I0302 18:59:19.801248 22760421793920 run.py:483] Algo bellman_ford step 2966 current loss 0.063358, current_train_items 94944.
I0302 18:59:19.824325 22760421793920 run.py:483] Algo bellman_ford step 2967 current loss 0.081235, current_train_items 94976.
I0302 18:59:19.853678 22760421793920 run.py:483] Algo bellman_ford step 2968 current loss 0.074371, current_train_items 95008.
I0302 18:59:19.886575 22760421793920 run.py:483] Algo bellman_ford step 2969 current loss 0.174737, current_train_items 95040.
I0302 18:59:19.904950 22760421793920 run.py:483] Algo bellman_ford step 2970 current loss 0.080452, current_train_items 95072.
I0302 18:59:19.920481 22760421793920 run.py:483] Algo bellman_ford step 2971 current loss 0.103448, current_train_items 95104.
I0302 18:59:19.943791 22760421793920 run.py:483] Algo bellman_ford step 2972 current loss 0.210032, current_train_items 95136.
I0302 18:59:19.973033 22760421793920 run.py:483] Algo bellman_ford step 2973 current loss 0.145017, current_train_items 95168.
I0302 18:59:20.007877 22760421793920 run.py:483] Algo bellman_ford step 2974 current loss 0.175115, current_train_items 95200.
I0302 18:59:20.026177 22760421793920 run.py:483] Algo bellman_ford step 2975 current loss 0.028817, current_train_items 95232.
I0302 18:59:20.042497 22760421793920 run.py:483] Algo bellman_ford step 2976 current loss 0.092070, current_train_items 95264.
I0302 18:59:20.065765 22760421793920 run.py:483] Algo bellman_ford step 2977 current loss 0.097442, current_train_items 95296.
I0302 18:59:20.095098 22760421793920 run.py:483] Algo bellman_ford step 2978 current loss 0.195802, current_train_items 95328.
I0302 18:59:20.128465 22760421793920 run.py:483] Algo bellman_ford step 2979 current loss 0.278683, current_train_items 95360.
I0302 18:59:20.146604 22760421793920 run.py:483] Algo bellman_ford step 2980 current loss 0.012262, current_train_items 95392.
I0302 18:59:20.162139 22760421793920 run.py:483] Algo bellman_ford step 2981 current loss 0.076706, current_train_items 95424.
I0302 18:59:20.185817 22760421793920 run.py:483] Algo bellman_ford step 2982 current loss 0.077482, current_train_items 95456.
I0302 18:59:20.217573 22760421793920 run.py:483] Algo bellman_ford step 2983 current loss 0.148942, current_train_items 95488.
I0302 18:59:20.249763 22760421793920 run.py:483] Algo bellman_ford step 2984 current loss 0.229930, current_train_items 95520.
I0302 18:59:20.267846 22760421793920 run.py:483] Algo bellman_ford step 2985 current loss 0.003546, current_train_items 95552.
I0302 18:59:20.283973 22760421793920 run.py:483] Algo bellman_ford step 2986 current loss 0.074547, current_train_items 95584.
I0302 18:59:20.308164 22760421793920 run.py:483] Algo bellman_ford step 2987 current loss 0.079980, current_train_items 95616.
I0302 18:59:20.337856 22760421793920 run.py:483] Algo bellman_ford step 2988 current loss 0.079346, current_train_items 95648.
I0302 18:59:20.370634 22760421793920 run.py:483] Algo bellman_ford step 2989 current loss 0.120535, current_train_items 95680.
I0302 18:59:20.388888 22760421793920 run.py:483] Algo bellman_ford step 2990 current loss 0.014930, current_train_items 95712.
I0302 18:59:20.405240 22760421793920 run.py:483] Algo bellman_ford step 2991 current loss 0.024557, current_train_items 95744.
I0302 18:59:20.429086 22760421793920 run.py:483] Algo bellman_ford step 2992 current loss 0.119932, current_train_items 95776.
I0302 18:59:20.459739 22760421793920 run.py:483] Algo bellman_ford step 2993 current loss 0.111452, current_train_items 95808.
I0302 18:59:20.493385 22760421793920 run.py:483] Algo bellman_ford step 2994 current loss 0.144686, current_train_items 95840.
I0302 18:59:20.511845 22760421793920 run.py:483] Algo bellman_ford step 2995 current loss 0.022832, current_train_items 95872.
I0302 18:59:20.527770 22760421793920 run.py:483] Algo bellman_ford step 2996 current loss 0.046592, current_train_items 95904.
I0302 18:59:20.552128 22760421793920 run.py:483] Algo bellman_ford step 2997 current loss 0.077350, current_train_items 95936.
I0302 18:59:20.583081 22760421793920 run.py:483] Algo bellman_ford step 2998 current loss 0.164523, current_train_items 95968.
I0302 18:59:20.614406 22760421793920 run.py:483] Algo bellman_ford step 2999 current loss 0.132806, current_train_items 96000.
I0302 18:59:20.632573 22760421793920 run.py:483] Algo bellman_ford step 3000 current loss 0.009727, current_train_items 96032.
I0302 18:59:20.640016 22760421793920 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:20.640127 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:20.656297 22760421793920 run.py:483] Algo bellman_ford step 3001 current loss 0.040042, current_train_items 96064.
I0302 18:59:20.678933 22760421793920 run.py:483] Algo bellman_ford step 3002 current loss 0.075601, current_train_items 96096.
I0302 18:59:20.708958 22760421793920 run.py:483] Algo bellman_ford step 3003 current loss 0.181617, current_train_items 96128.
I0302 18:59:20.740349 22760421793920 run.py:483] Algo bellman_ford step 3004 current loss 0.164835, current_train_items 96160.
I0302 18:59:20.758563 22760421793920 run.py:483] Algo bellman_ford step 3005 current loss 0.006332, current_train_items 96192.
I0302 18:59:20.774079 22760421793920 run.py:483] Algo bellman_ford step 3006 current loss 0.022893, current_train_items 96224.
I0302 18:59:20.798019 22760421793920 run.py:483] Algo bellman_ford step 3007 current loss 0.078425, current_train_items 96256.
I0302 18:59:20.828365 22760421793920 run.py:483] Algo bellman_ford step 3008 current loss 0.074471, current_train_items 96288.
I0302 18:59:20.862752 22760421793920 run.py:483] Algo bellman_ford step 3009 current loss 0.132785, current_train_items 96320.
I0302 18:59:20.880866 22760421793920 run.py:483] Algo bellman_ford step 3010 current loss 0.008208, current_train_items 96352.
I0302 18:59:20.896657 22760421793920 run.py:483] Algo bellman_ford step 3011 current loss 0.029783, current_train_items 96384.
I0302 18:59:20.919403 22760421793920 run.py:483] Algo bellman_ford step 3012 current loss 0.051706, current_train_items 96416.
I0302 18:59:20.948524 22760421793920 run.py:483] Algo bellman_ford step 3013 current loss 0.058068, current_train_items 96448.
I0302 18:59:20.981737 22760421793920 run.py:483] Algo bellman_ford step 3014 current loss 0.107407, current_train_items 96480.
I0302 18:59:20.999754 22760421793920 run.py:483] Algo bellman_ford step 3015 current loss 0.018578, current_train_items 96512.
I0302 18:59:21.015757 22760421793920 run.py:483] Algo bellman_ford step 3016 current loss 0.030368, current_train_items 96544.
I0302 18:59:21.038332 22760421793920 run.py:483] Algo bellman_ford step 3017 current loss 0.093691, current_train_items 96576.
I0302 18:59:21.067512 22760421793920 run.py:483] Algo bellman_ford step 3018 current loss 0.080360, current_train_items 96608.
I0302 18:59:21.098094 22760421793920 run.py:483] Algo bellman_ford step 3019 current loss 0.115219, current_train_items 96640.
I0302 18:59:21.116195 22760421793920 run.py:483] Algo bellman_ford step 3020 current loss 0.006982, current_train_items 96672.
I0302 18:59:21.131460 22760421793920 run.py:483] Algo bellman_ford step 3021 current loss 0.026410, current_train_items 96704.
I0302 18:59:21.155661 22760421793920 run.py:483] Algo bellman_ford step 3022 current loss 0.127695, current_train_items 96736.
I0302 18:59:21.185435 22760421793920 run.py:483] Algo bellman_ford step 3023 current loss 0.124286, current_train_items 96768.
I0302 18:59:21.217152 22760421793920 run.py:483] Algo bellman_ford step 3024 current loss 0.078854, current_train_items 96800.
I0302 18:59:21.235432 22760421793920 run.py:483] Algo bellman_ford step 3025 current loss 0.019583, current_train_items 96832.
I0302 18:59:21.250925 22760421793920 run.py:483] Algo bellman_ford step 3026 current loss 0.045891, current_train_items 96864.
I0302 18:59:21.272570 22760421793920 run.py:483] Algo bellman_ford step 3027 current loss 0.098011, current_train_items 96896.
I0302 18:59:21.302103 22760421793920 run.py:483] Algo bellman_ford step 3028 current loss 0.117691, current_train_items 96928.
I0302 18:59:21.332703 22760421793920 run.py:483] Algo bellman_ford step 3029 current loss 0.103832, current_train_items 96960.
I0302 18:59:21.350728 22760421793920 run.py:483] Algo bellman_ford step 3030 current loss 0.010113, current_train_items 96992.
I0302 18:59:21.366229 22760421793920 run.py:483] Algo bellman_ford step 3031 current loss 0.028607, current_train_items 97024.
I0302 18:59:21.389839 22760421793920 run.py:483] Algo bellman_ford step 3032 current loss 0.079680, current_train_items 97056.
I0302 18:59:21.420213 22760421793920 run.py:483] Algo bellman_ford step 3033 current loss 0.087317, current_train_items 97088.
I0302 18:59:21.452845 22760421793920 run.py:483] Algo bellman_ford step 3034 current loss 0.098857, current_train_items 97120.
I0302 18:59:21.470932 22760421793920 run.py:483] Algo bellman_ford step 3035 current loss 0.029057, current_train_items 97152.
I0302 18:59:21.486871 22760421793920 run.py:483] Algo bellman_ford step 3036 current loss 0.028938, current_train_items 97184.
I0302 18:59:21.509544 22760421793920 run.py:483] Algo bellman_ford step 3037 current loss 0.069100, current_train_items 97216.
I0302 18:59:21.539523 22760421793920 run.py:483] Algo bellman_ford step 3038 current loss 0.114182, current_train_items 97248.
I0302 18:59:21.570296 22760421793920 run.py:483] Algo bellman_ford step 3039 current loss 0.146969, current_train_items 97280.
I0302 18:59:21.588319 22760421793920 run.py:483] Algo bellman_ford step 3040 current loss 0.003187, current_train_items 97312.
I0302 18:59:21.603926 22760421793920 run.py:483] Algo bellman_ford step 3041 current loss 0.026241, current_train_items 97344.
I0302 18:59:21.626618 22760421793920 run.py:483] Algo bellman_ford step 3042 current loss 0.057250, current_train_items 97376.
I0302 18:59:21.656183 22760421793920 run.py:483] Algo bellman_ford step 3043 current loss 0.101350, current_train_items 97408.
I0302 18:59:21.687007 22760421793920 run.py:483] Algo bellman_ford step 3044 current loss 0.152189, current_train_items 97440.
I0302 18:59:21.705079 22760421793920 run.py:483] Algo bellman_ford step 3045 current loss 0.016327, current_train_items 97472.
I0302 18:59:21.720725 22760421793920 run.py:483] Algo bellman_ford step 3046 current loss 0.055189, current_train_items 97504.
I0302 18:59:21.743368 22760421793920 run.py:483] Algo bellman_ford step 3047 current loss 0.112499, current_train_items 97536.
I0302 18:59:21.774003 22760421793920 run.py:483] Algo bellman_ford step 3048 current loss 0.065698, current_train_items 97568.
I0302 18:59:21.805627 22760421793920 run.py:483] Algo bellman_ford step 3049 current loss 0.088563, current_train_items 97600.
I0302 18:59:21.823571 22760421793920 run.py:483] Algo bellman_ford step 3050 current loss 0.009242, current_train_items 97632.
I0302 18:59:21.830992 22760421793920 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 18:59:21.831100 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:59:21.847277 22760421793920 run.py:483] Algo bellman_ford step 3051 current loss 0.025306, current_train_items 97664.
I0302 18:59:21.870344 22760421793920 run.py:483] Algo bellman_ford step 3052 current loss 0.078064, current_train_items 97696.
I0302 18:59:21.901148 22760421793920 run.py:483] Algo bellman_ford step 3053 current loss 0.070406, current_train_items 97728.
I0302 18:59:21.931489 22760421793920 run.py:483] Algo bellman_ford step 3054 current loss 0.088836, current_train_items 97760.
I0302 18:59:21.949998 22760421793920 run.py:483] Algo bellman_ford step 3055 current loss 0.005850, current_train_items 97792.
I0302 18:59:21.965998 22760421793920 run.py:483] Algo bellman_ford step 3056 current loss 0.080431, current_train_items 97824.
I0302 18:59:21.988645 22760421793920 run.py:483] Algo bellman_ford step 3057 current loss 0.108411, current_train_items 97856.
I0302 18:59:22.018819 22760421793920 run.py:483] Algo bellman_ford step 3058 current loss 0.113244, current_train_items 97888.
I0302 18:59:22.049968 22760421793920 run.py:483] Algo bellman_ford step 3059 current loss 0.093250, current_train_items 97920.
I0302 18:59:22.068034 22760421793920 run.py:483] Algo bellman_ford step 3060 current loss 0.003728, current_train_items 97952.
I0302 18:59:22.083867 22760421793920 run.py:483] Algo bellman_ford step 3061 current loss 0.054788, current_train_items 97984.
I0302 18:59:22.106369 22760421793920 run.py:483] Algo bellman_ford step 3062 current loss 0.066089, current_train_items 98016.
I0302 18:59:22.137384 22760421793920 run.py:483] Algo bellman_ford step 3063 current loss 0.151832, current_train_items 98048.
I0302 18:59:22.170375 22760421793920 run.py:483] Algo bellman_ford step 3064 current loss 0.130775, current_train_items 98080.
I0302 18:59:22.188683 22760421793920 run.py:483] Algo bellman_ford step 3065 current loss 0.005372, current_train_items 98112.
I0302 18:59:22.204342 22760421793920 run.py:483] Algo bellman_ford step 3066 current loss 0.058785, current_train_items 98144.
I0302 18:59:22.228083 22760421793920 run.py:483] Algo bellman_ford step 3067 current loss 0.049592, current_train_items 98176.
I0302 18:59:22.257713 22760421793920 run.py:483] Algo bellman_ford step 3068 current loss 0.095777, current_train_items 98208.
I0302 18:59:22.292482 22760421793920 run.py:483] Algo bellman_ford step 3069 current loss 0.118629, current_train_items 98240.
I0302 18:59:22.310874 22760421793920 run.py:483] Algo bellman_ford step 3070 current loss 0.003864, current_train_items 98272.
I0302 18:59:22.326605 22760421793920 run.py:483] Algo bellman_ford step 3071 current loss 0.025124, current_train_items 98304.
I0302 18:59:22.349118 22760421793920 run.py:483] Algo bellman_ford step 3072 current loss 0.048841, current_train_items 98336.
I0302 18:59:22.378061 22760421793920 run.py:483] Algo bellman_ford step 3073 current loss 0.070848, current_train_items 98368.
I0302 18:59:22.412184 22760421793920 run.py:483] Algo bellman_ford step 3074 current loss 0.140758, current_train_items 98400.
I0302 18:59:22.430621 22760421793920 run.py:483] Algo bellman_ford step 3075 current loss 0.006835, current_train_items 98432.
I0302 18:59:22.446660 22760421793920 run.py:483] Algo bellman_ford step 3076 current loss 0.053778, current_train_items 98464.
I0302 18:59:22.469662 22760421793920 run.py:483] Algo bellman_ford step 3077 current loss 0.089207, current_train_items 98496.
I0302 18:59:22.500162 22760421793920 run.py:483] Algo bellman_ford step 3078 current loss 0.114890, current_train_items 98528.
I0302 18:59:22.530916 22760421793920 run.py:483] Algo bellman_ford step 3079 current loss 0.120632, current_train_items 98560.
I0302 18:59:22.549189 22760421793920 run.py:483] Algo bellman_ford step 3080 current loss 0.009289, current_train_items 98592.
I0302 18:59:22.565153 22760421793920 run.py:483] Algo bellman_ford step 3081 current loss 0.039339, current_train_items 98624.
I0302 18:59:22.588351 22760421793920 run.py:483] Algo bellman_ford step 3082 current loss 0.056921, current_train_items 98656.
I0302 18:59:22.618101 22760421793920 run.py:483] Algo bellman_ford step 3083 current loss 0.126685, current_train_items 98688.
I0302 18:59:22.650222 22760421793920 run.py:483] Algo bellman_ford step 3084 current loss 0.160599, current_train_items 98720.
I0302 18:59:22.668430 22760421793920 run.py:483] Algo bellman_ford step 3085 current loss 0.029665, current_train_items 98752.
I0302 18:59:22.684476 22760421793920 run.py:483] Algo bellman_ford step 3086 current loss 0.015336, current_train_items 98784.
I0302 18:59:22.706715 22760421793920 run.py:483] Algo bellman_ford step 3087 current loss 0.025442, current_train_items 98816.
I0302 18:59:22.736578 22760421793920 run.py:483] Algo bellman_ford step 3088 current loss 0.073570, current_train_items 98848.
I0302 18:59:22.770147 22760421793920 run.py:483] Algo bellman_ford step 3089 current loss 0.094433, current_train_items 98880.
I0302 18:59:22.788461 22760421793920 run.py:483] Algo bellman_ford step 3090 current loss 0.002937, current_train_items 98912.
I0302 18:59:22.804008 22760421793920 run.py:483] Algo bellman_ford step 3091 current loss 0.009606, current_train_items 98944.
I0302 18:59:22.826207 22760421793920 run.py:483] Algo bellman_ford step 3092 current loss 0.065957, current_train_items 98976.
I0302 18:59:22.856573 22760421793920 run.py:483] Algo bellman_ford step 3093 current loss 0.106703, current_train_items 99008.
I0302 18:59:22.888505 22760421793920 run.py:483] Algo bellman_ford step 3094 current loss 0.091407, current_train_items 99040.
I0302 18:59:22.906611 22760421793920 run.py:483] Algo bellman_ford step 3095 current loss 0.005775, current_train_items 99072.
I0302 18:59:22.922846 22760421793920 run.py:483] Algo bellman_ford step 3096 current loss 0.030838, current_train_items 99104.
I0302 18:59:22.947182 22760421793920 run.py:483] Algo bellman_ford step 3097 current loss 0.073463, current_train_items 99136.
I0302 18:59:22.976621 22760421793920 run.py:483] Algo bellman_ford step 3098 current loss 0.058318, current_train_items 99168.
I0302 18:59:23.008204 22760421793920 run.py:483] Algo bellman_ford step 3099 current loss 0.095424, current_train_items 99200.
I0302 18:59:23.026936 22760421793920 run.py:483] Algo bellman_ford step 3100 current loss 0.002974, current_train_items 99232.
I0302 18:59:23.034444 22760421793920 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 18:59:23.034553 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:23.051894 22760421793920 run.py:483] Algo bellman_ford step 3101 current loss 0.033418, current_train_items 99264.
I0302 18:59:23.075397 22760421793920 run.py:483] Algo bellman_ford step 3102 current loss 0.059775, current_train_items 99296.
I0302 18:59:23.105934 22760421793920 run.py:483] Algo bellman_ford step 3103 current loss 0.064821, current_train_items 99328.
I0302 18:59:23.138985 22760421793920 run.py:483] Algo bellman_ford step 3104 current loss 0.074714, current_train_items 99360.
I0302 18:59:23.157445 22760421793920 run.py:483] Algo bellman_ford step 3105 current loss 0.015688, current_train_items 99392.
I0302 18:59:23.172625 22760421793920 run.py:483] Algo bellman_ford step 3106 current loss 0.005469, current_train_items 99424.
I0302 18:59:23.195426 22760421793920 run.py:483] Algo bellman_ford step 3107 current loss 0.034263, current_train_items 99456.
I0302 18:59:23.226092 22760421793920 run.py:483] Algo bellman_ford step 3108 current loss 0.118289, current_train_items 99488.
I0302 18:59:23.256733 22760421793920 run.py:483] Algo bellman_ford step 3109 current loss 0.114857, current_train_items 99520.
I0302 18:59:23.274873 22760421793920 run.py:483] Algo bellman_ford step 3110 current loss 0.002687, current_train_items 99552.
I0302 18:59:23.290683 22760421793920 run.py:483] Algo bellman_ford step 3111 current loss 0.027963, current_train_items 99584.
I0302 18:59:23.313385 22760421793920 run.py:483] Algo bellman_ford step 3112 current loss 0.069514, current_train_items 99616.
I0302 18:59:23.342946 22760421793920 run.py:483] Algo bellman_ford step 3113 current loss 0.077639, current_train_items 99648.
I0302 18:59:23.375534 22760421793920 run.py:483] Algo bellman_ford step 3114 current loss 0.092592, current_train_items 99680.
I0302 18:59:23.393665 22760421793920 run.py:483] Algo bellman_ford step 3115 current loss 0.008484, current_train_items 99712.
I0302 18:59:23.409557 22760421793920 run.py:483] Algo bellman_ford step 3116 current loss 0.021046, current_train_items 99744.
I0302 18:59:23.432422 22760421793920 run.py:483] Algo bellman_ford step 3117 current loss 0.051402, current_train_items 99776.
I0302 18:59:23.462709 22760421793920 run.py:483] Algo bellman_ford step 3118 current loss 0.122237, current_train_items 99808.
I0302 18:59:23.493903 22760421793920 run.py:483] Algo bellman_ford step 3119 current loss 0.089928, current_train_items 99840.
I0302 18:59:23.511893 22760421793920 run.py:483] Algo bellman_ford step 3120 current loss 0.004736, current_train_items 99872.
I0302 18:59:23.527500 22760421793920 run.py:483] Algo bellman_ford step 3121 current loss 0.028590, current_train_items 99904.
I0302 18:59:23.550604 22760421793920 run.py:483] Algo bellman_ford step 3122 current loss 0.045386, current_train_items 99936.
I0302 18:59:23.581143 22760421793920 run.py:483] Algo bellman_ford step 3123 current loss 0.068138, current_train_items 99968.
I0302 18:59:23.612617 22760421793920 run.py:483] Algo bellman_ford step 3124 current loss 0.151012, current_train_items 100000.
I0302 18:59:23.630636 22760421793920 run.py:483] Algo bellman_ford step 3125 current loss 0.007291, current_train_items 100032.
I0302 18:59:23.646286 22760421793920 run.py:483] Algo bellman_ford step 3126 current loss 0.025018, current_train_items 100064.
I0302 18:59:23.668279 22760421793920 run.py:483] Algo bellman_ford step 3127 current loss 0.064998, current_train_items 100096.
I0302 18:59:23.699289 22760421793920 run.py:483] Algo bellman_ford step 3128 current loss 0.079115, current_train_items 100128.
I0302 18:59:23.729761 22760421793920 run.py:483] Algo bellman_ford step 3129 current loss 0.084485, current_train_items 100160.
I0302 18:59:23.748122 22760421793920 run.py:483] Algo bellman_ford step 3130 current loss 0.004681, current_train_items 100192.
I0302 18:59:23.764005 22760421793920 run.py:483] Algo bellman_ford step 3131 current loss 0.066894, current_train_items 100224.
I0302 18:59:23.787102 22760421793920 run.py:483] Algo bellman_ford step 3132 current loss 0.083670, current_train_items 100256.
I0302 18:59:23.816064 22760421793920 run.py:483] Algo bellman_ford step 3133 current loss 0.119669, current_train_items 100288.
I0302 18:59:23.850041 22760421793920 run.py:483] Algo bellman_ford step 3134 current loss 0.097133, current_train_items 100320.
I0302 18:59:23.868300 22760421793920 run.py:483] Algo bellman_ford step 3135 current loss 0.015585, current_train_items 100352.
I0302 18:59:23.884089 22760421793920 run.py:483] Algo bellman_ford step 3136 current loss 0.047078, current_train_items 100384.
I0302 18:59:23.907192 22760421793920 run.py:483] Algo bellman_ford step 3137 current loss 0.089465, current_train_items 100416.
I0302 18:59:23.935763 22760421793920 run.py:483] Algo bellman_ford step 3138 current loss 0.085429, current_train_items 100448.
I0302 18:59:23.968064 22760421793920 run.py:483] Algo bellman_ford step 3139 current loss 0.082443, current_train_items 100480.
I0302 18:59:23.986266 22760421793920 run.py:483] Algo bellman_ford step 3140 current loss 0.018230, current_train_items 100512.
I0302 18:59:24.002602 22760421793920 run.py:483] Algo bellman_ford step 3141 current loss 0.175930, current_train_items 100544.
I0302 18:59:24.024133 22760421793920 run.py:483] Algo bellman_ford step 3142 current loss 0.135280, current_train_items 100576.
I0302 18:59:24.054877 22760421793920 run.py:483] Algo bellman_ford step 3143 current loss 0.105931, current_train_items 100608.
I0302 18:59:24.086059 22760421793920 run.py:483] Algo bellman_ford step 3144 current loss 0.124778, current_train_items 100640.
I0302 18:59:24.104316 22760421793920 run.py:483] Algo bellman_ford step 3145 current loss 0.006937, current_train_items 100672.
I0302 18:59:24.120120 22760421793920 run.py:483] Algo bellman_ford step 3146 current loss 0.041140, current_train_items 100704.
I0302 18:59:24.141862 22760421793920 run.py:483] Algo bellman_ford step 3147 current loss 0.076379, current_train_items 100736.
I0302 18:59:24.169595 22760421793920 run.py:483] Algo bellman_ford step 3148 current loss 0.114341, current_train_items 100768.
I0302 18:59:24.203378 22760421793920 run.py:483] Algo bellman_ford step 3149 current loss 0.409978, current_train_items 100800.
I0302 18:59:24.221549 22760421793920 run.py:483] Algo bellman_ford step 3150 current loss 0.008113, current_train_items 100832.
I0302 18:59:24.229343 22760421793920 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 18:59:24.229454 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:59:24.245930 22760421793920 run.py:483] Algo bellman_ford step 3151 current loss 0.037522, current_train_items 100864.
I0302 18:59:24.270581 22760421793920 run.py:483] Algo bellman_ford step 3152 current loss 0.126326, current_train_items 100896.
I0302 18:59:24.301103 22760421793920 run.py:483] Algo bellman_ford step 3153 current loss 0.087978, current_train_items 100928.
I0302 18:59:24.334579 22760421793920 run.py:483] Algo bellman_ford step 3154 current loss 0.126998, current_train_items 100960.
I0302 18:59:24.353247 22760421793920 run.py:483] Algo bellman_ford step 3155 current loss 0.029359, current_train_items 100992.
I0302 18:59:24.369209 22760421793920 run.py:483] Algo bellman_ford step 3156 current loss 0.054463, current_train_items 101024.
I0302 18:59:24.392680 22760421793920 run.py:483] Algo bellman_ford step 3157 current loss 0.076800, current_train_items 101056.
I0302 18:59:24.421696 22760421793920 run.py:483] Algo bellman_ford step 3158 current loss 0.123104, current_train_items 101088.
I0302 18:59:24.453078 22760421793920 run.py:483] Algo bellman_ford step 3159 current loss 0.111476, current_train_items 101120.
I0302 18:59:24.471649 22760421793920 run.py:483] Algo bellman_ford step 3160 current loss 0.007050, current_train_items 101152.
I0302 18:59:24.487182 22760421793920 run.py:483] Algo bellman_ford step 3161 current loss 0.042986, current_train_items 101184.
I0302 18:59:24.510075 22760421793920 run.py:483] Algo bellman_ford step 3162 current loss 0.044296, current_train_items 101216.
I0302 18:59:24.539087 22760421793920 run.py:483] Algo bellman_ford step 3163 current loss 0.075606, current_train_items 101248.
I0302 18:59:24.572641 22760421793920 run.py:483] Algo bellman_ford step 3164 current loss 0.190645, current_train_items 101280.
I0302 18:59:24.591015 22760421793920 run.py:483] Algo bellman_ford step 3165 current loss 0.010662, current_train_items 101312.
I0302 18:59:24.607376 22760421793920 run.py:483] Algo bellman_ford step 3166 current loss 0.104517, current_train_items 101344.
I0302 18:59:24.630990 22760421793920 run.py:483] Algo bellman_ford step 3167 current loss 0.079141, current_train_items 101376.
I0302 18:59:24.661262 22760421793920 run.py:483] Algo bellman_ford step 3168 current loss 0.092455, current_train_items 101408.
I0302 18:59:24.692397 22760421793920 run.py:483] Algo bellman_ford step 3169 current loss 0.117102, current_train_items 101440.
I0302 18:59:24.711148 22760421793920 run.py:483] Algo bellman_ford step 3170 current loss 0.011957, current_train_items 101472.
I0302 18:59:24.727099 22760421793920 run.py:483] Algo bellman_ford step 3171 current loss 0.054290, current_train_items 101504.
I0302 18:59:24.750294 22760421793920 run.py:483] Algo bellman_ford step 3172 current loss 0.036995, current_train_items 101536.
I0302 18:59:24.780327 22760421793920 run.py:483] Algo bellman_ford step 3173 current loss 0.071894, current_train_items 101568.
I0302 18:59:24.810227 22760421793920 run.py:483] Algo bellman_ford step 3174 current loss 0.096062, current_train_items 101600.
I0302 18:59:24.828456 22760421793920 run.py:483] Algo bellman_ford step 3175 current loss 0.005743, current_train_items 101632.
I0302 18:59:24.844306 22760421793920 run.py:483] Algo bellman_ford step 3176 current loss 0.023976, current_train_items 101664.
I0302 18:59:24.866780 22760421793920 run.py:483] Algo bellman_ford step 3177 current loss 0.056107, current_train_items 101696.
I0302 18:59:24.894633 22760421793920 run.py:483] Algo bellman_ford step 3178 current loss 0.050294, current_train_items 101728.
I0302 18:59:24.927427 22760421793920 run.py:483] Algo bellman_ford step 3179 current loss 0.102524, current_train_items 101760.
I0302 18:59:24.945734 22760421793920 run.py:483] Algo bellman_ford step 3180 current loss 0.025357, current_train_items 101792.
I0302 18:59:24.961636 22760421793920 run.py:483] Algo bellman_ford step 3181 current loss 0.036292, current_train_items 101824.
I0302 18:59:24.985053 22760421793920 run.py:483] Algo bellman_ford step 3182 current loss 0.029920, current_train_items 101856.
I0302 18:59:25.015387 22760421793920 run.py:483] Algo bellman_ford step 3183 current loss 0.052032, current_train_items 101888.
I0302 18:59:25.050280 22760421793920 run.py:483] Algo bellman_ford step 3184 current loss 0.081545, current_train_items 101920.
I0302 18:59:25.068921 22760421793920 run.py:483] Algo bellman_ford step 3185 current loss 0.031509, current_train_items 101952.
I0302 18:59:25.084564 22760421793920 run.py:483] Algo bellman_ford step 3186 current loss 0.026704, current_train_items 101984.
I0302 18:59:25.108306 22760421793920 run.py:483] Algo bellman_ford step 3187 current loss 0.065944, current_train_items 102016.
I0302 18:59:25.137754 22760421793920 run.py:483] Algo bellman_ford step 3188 current loss 0.067969, current_train_items 102048.
I0302 18:59:25.169555 22760421793920 run.py:483] Algo bellman_ford step 3189 current loss 0.065228, current_train_items 102080.
I0302 18:59:25.187974 22760421793920 run.py:483] Algo bellman_ford step 3190 current loss 0.007919, current_train_items 102112.
I0302 18:59:25.203848 22760421793920 run.py:483] Algo bellman_ford step 3191 current loss 0.017928, current_train_items 102144.
I0302 18:59:25.227391 22760421793920 run.py:483] Algo bellman_ford step 3192 current loss 0.144907, current_train_items 102176.
I0302 18:59:25.256775 22760421793920 run.py:483] Algo bellman_ford step 3193 current loss 0.073732, current_train_items 102208.
I0302 18:59:25.289264 22760421793920 run.py:483] Algo bellman_ford step 3194 current loss 0.067951, current_train_items 102240.
I0302 18:59:25.307562 22760421793920 run.py:483] Algo bellman_ford step 3195 current loss 0.004198, current_train_items 102272.
I0302 18:59:25.322976 22760421793920 run.py:483] Algo bellman_ford step 3196 current loss 0.030633, current_train_items 102304.
I0302 18:59:25.345777 22760421793920 run.py:483] Algo bellman_ford step 3197 current loss 0.122670, current_train_items 102336.
I0302 18:59:25.375743 22760421793920 run.py:483] Algo bellman_ford step 3198 current loss 0.128809, current_train_items 102368.
I0302 18:59:25.408252 22760421793920 run.py:483] Algo bellman_ford step 3199 current loss 0.125118, current_train_items 102400.
I0302 18:59:25.426635 22760421793920 run.py:483] Algo bellman_ford step 3200 current loss 0.019377, current_train_items 102432.
I0302 18:59:25.434207 22760421793920 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 18:59:25.434315 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:25.450127 22760421793920 run.py:483] Algo bellman_ford step 3201 current loss 0.026680, current_train_items 102464.
I0302 18:59:25.473146 22760421793920 run.py:483] Algo bellman_ford step 3202 current loss 0.075294, current_train_items 102496.
I0302 18:59:25.505008 22760421793920 run.py:483] Algo bellman_ford step 3203 current loss 0.216363, current_train_items 102528.
I0302 18:59:25.537891 22760421793920 run.py:483] Algo bellman_ford step 3204 current loss 0.154327, current_train_items 102560.
I0302 18:59:25.556645 22760421793920 run.py:483] Algo bellman_ford step 3205 current loss 0.002680, current_train_items 102592.
I0302 18:59:25.572067 22760421793920 run.py:483] Algo bellman_ford step 3206 current loss 0.029020, current_train_items 102624.
I0302 18:59:25.594071 22760421793920 run.py:483] Algo bellman_ford step 3207 current loss 0.029370, current_train_items 102656.
I0302 18:59:25.625009 22760421793920 run.py:483] Algo bellman_ford step 3208 current loss 0.093830, current_train_items 102688.
I0302 18:59:25.657696 22760421793920 run.py:483] Algo bellman_ford step 3209 current loss 0.098946, current_train_items 102720.
I0302 18:59:25.676039 22760421793920 run.py:483] Algo bellman_ford step 3210 current loss 0.054309, current_train_items 102752.
I0302 18:59:25.691430 22760421793920 run.py:483] Algo bellman_ford step 3211 current loss 0.010403, current_train_items 102784.
I0302 18:59:25.714230 22760421793920 run.py:483] Algo bellman_ford step 3212 current loss 0.067363, current_train_items 102816.
I0302 18:59:25.744264 22760421793920 run.py:483] Algo bellman_ford step 3213 current loss 0.091272, current_train_items 102848.
I0302 18:59:25.776114 22760421793920 run.py:483] Algo bellman_ford step 3214 current loss 0.099785, current_train_items 102880.
I0302 18:59:25.794618 22760421793920 run.py:483] Algo bellman_ford step 3215 current loss 0.038704, current_train_items 102912.
I0302 18:59:25.810301 22760421793920 run.py:483] Algo bellman_ford step 3216 current loss 0.102704, current_train_items 102944.
I0302 18:59:25.834103 22760421793920 run.py:483] Algo bellman_ford step 3217 current loss 0.198630, current_train_items 102976.
I0302 18:59:25.863002 22760421793920 run.py:483] Algo bellman_ford step 3218 current loss 0.349828, current_train_items 103008.
I0302 18:59:25.894370 22760421793920 run.py:483] Algo bellman_ford step 3219 current loss 0.140986, current_train_items 103040.
I0302 18:59:25.912608 22760421793920 run.py:483] Algo bellman_ford step 3220 current loss 0.031320, current_train_items 103072.
I0302 18:59:25.928365 22760421793920 run.py:483] Algo bellman_ford step 3221 current loss 0.037209, current_train_items 103104.
I0302 18:59:25.951994 22760421793920 run.py:483] Algo bellman_ford step 3222 current loss 0.103553, current_train_items 103136.
I0302 18:59:25.981961 22760421793920 run.py:483] Algo bellman_ford step 3223 current loss 0.114193, current_train_items 103168.
I0302 18:59:26.012497 22760421793920 run.py:483] Algo bellman_ford step 3224 current loss 0.123379, current_train_items 103200.
I0302 18:59:26.030573 22760421793920 run.py:483] Algo bellman_ford step 3225 current loss 0.007280, current_train_items 103232.
I0302 18:59:26.045591 22760421793920 run.py:483] Algo bellman_ford step 3226 current loss 0.030548, current_train_items 103264.
I0302 18:59:26.068632 22760421793920 run.py:483] Algo bellman_ford step 3227 current loss 0.031652, current_train_items 103296.
I0302 18:59:26.098879 22760421793920 run.py:483] Algo bellman_ford step 3228 current loss 0.068498, current_train_items 103328.
I0302 18:59:26.134662 22760421793920 run.py:483] Algo bellman_ford step 3229 current loss 0.136501, current_train_items 103360.
I0302 18:59:26.152748 22760421793920 run.py:483] Algo bellman_ford step 3230 current loss 0.003759, current_train_items 103392.
I0302 18:59:26.168500 22760421793920 run.py:483] Algo bellman_ford step 3231 current loss 0.041885, current_train_items 103424.
I0302 18:59:26.190872 22760421793920 run.py:483] Algo bellman_ford step 3232 current loss 0.060186, current_train_items 103456.
I0302 18:59:26.221061 22760421793920 run.py:483] Algo bellman_ford step 3233 current loss 0.054423, current_train_items 103488.
I0302 18:59:26.254369 22760421793920 run.py:483] Algo bellman_ford step 3234 current loss 0.089541, current_train_items 103520.
I0302 18:59:26.272731 22760421793920 run.py:483] Algo bellman_ford step 3235 current loss 0.075721, current_train_items 103552.
I0302 18:59:26.288630 22760421793920 run.py:483] Algo bellman_ford step 3236 current loss 0.027692, current_train_items 103584.
I0302 18:59:26.311023 22760421793920 run.py:483] Algo bellman_ford step 3237 current loss 0.041962, current_train_items 103616.
I0302 18:59:26.342213 22760421793920 run.py:483] Algo bellman_ford step 3238 current loss 0.132504, current_train_items 103648.
I0302 18:59:26.374026 22760421793920 run.py:483] Algo bellman_ford step 3239 current loss 0.110688, current_train_items 103680.
I0302 18:59:26.392196 22760421793920 run.py:483] Algo bellman_ford step 3240 current loss 0.015784, current_train_items 103712.
I0302 18:59:26.408023 22760421793920 run.py:483] Algo bellman_ford step 3241 current loss 0.044533, current_train_items 103744.
I0302 18:59:26.430724 22760421793920 run.py:483] Algo bellman_ford step 3242 current loss 0.097475, current_train_items 103776.
I0302 18:59:26.459573 22760421793920 run.py:483] Algo bellman_ford step 3243 current loss 0.127984, current_train_items 103808.
I0302 18:59:26.492314 22760421793920 run.py:483] Algo bellman_ford step 3244 current loss 0.178474, current_train_items 103840.
I0302 18:59:26.510505 22760421793920 run.py:483] Algo bellman_ford step 3245 current loss 0.003147, current_train_items 103872.
I0302 18:59:26.526424 22760421793920 run.py:483] Algo bellman_ford step 3246 current loss 0.019136, current_train_items 103904.
I0302 18:59:26.549021 22760421793920 run.py:483] Algo bellman_ford step 3247 current loss 0.078293, current_train_items 103936.
I0302 18:59:26.579305 22760421793920 run.py:483] Algo bellman_ford step 3248 current loss 0.048829, current_train_items 103968.
I0302 18:59:26.610318 22760421793920 run.py:483] Algo bellman_ford step 3249 current loss 0.086817, current_train_items 104000.
I0302 18:59:26.628811 22760421793920 run.py:483] Algo bellman_ford step 3250 current loss 0.026456, current_train_items 104032.
I0302 18:59:26.636296 22760421793920 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 18:59:26.636406 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:26.652693 22760421793920 run.py:483] Algo bellman_ford step 3251 current loss 0.028432, current_train_items 104064.
I0302 18:59:26.674527 22760421793920 run.py:483] Algo bellman_ford step 3252 current loss 0.045603, current_train_items 104096.
I0302 18:59:26.706014 22760421793920 run.py:483] Algo bellman_ford step 3253 current loss 0.098347, current_train_items 104128.
I0302 18:59:26.738941 22760421793920 run.py:483] Algo bellman_ford step 3254 current loss 0.076180, current_train_items 104160.
I0302 18:59:26.757914 22760421793920 run.py:483] Algo bellman_ford step 3255 current loss 0.006686, current_train_items 104192.
I0302 18:59:26.773673 22760421793920 run.py:483] Algo bellman_ford step 3256 current loss 0.057343, current_train_items 104224.
I0302 18:59:26.795921 22760421793920 run.py:483] Algo bellman_ford step 3257 current loss 0.149759, current_train_items 104256.
I0302 18:59:26.827437 22760421793920 run.py:483] Algo bellman_ford step 3258 current loss 0.201548, current_train_items 104288.
I0302 18:59:26.861370 22760421793920 run.py:483] Algo bellman_ford step 3259 current loss 0.118822, current_train_items 104320.
I0302 18:59:26.879779 22760421793920 run.py:483] Algo bellman_ford step 3260 current loss 0.007336, current_train_items 104352.
I0302 18:59:26.895428 22760421793920 run.py:483] Algo bellman_ford step 3261 current loss 0.022935, current_train_items 104384.
I0302 18:59:26.918362 22760421793920 run.py:483] Algo bellman_ford step 3262 current loss 0.069456, current_train_items 104416.
I0302 18:59:26.948574 22760421793920 run.py:483] Algo bellman_ford step 3263 current loss 0.123761, current_train_items 104448.
I0302 18:59:26.981098 22760421793920 run.py:483] Algo bellman_ford step 3264 current loss 0.129822, current_train_items 104480.
I0302 18:59:26.999443 22760421793920 run.py:483] Algo bellman_ford step 3265 current loss 0.043587, current_train_items 104512.
I0302 18:59:27.015384 22760421793920 run.py:483] Algo bellman_ford step 3266 current loss 0.037139, current_train_items 104544.
I0302 18:59:27.037780 22760421793920 run.py:483] Algo bellman_ford step 3267 current loss 0.047520, current_train_items 104576.
I0302 18:59:27.066060 22760421793920 run.py:483] Algo bellman_ford step 3268 current loss 0.068587, current_train_items 104608.
I0302 18:59:27.100672 22760421793920 run.py:483] Algo bellman_ford step 3269 current loss 0.174162, current_train_items 104640.
I0302 18:59:27.118815 22760421793920 run.py:483] Algo bellman_ford step 3270 current loss 0.004621, current_train_items 104672.
I0302 18:59:27.134567 22760421793920 run.py:483] Algo bellman_ford step 3271 current loss 0.025795, current_train_items 104704.
I0302 18:59:27.157636 22760421793920 run.py:483] Algo bellman_ford step 3272 current loss 0.055689, current_train_items 104736.
I0302 18:59:27.186578 22760421793920 run.py:483] Algo bellman_ford step 3273 current loss 0.109541, current_train_items 104768.
I0302 18:59:27.216833 22760421793920 run.py:483] Algo bellman_ford step 3274 current loss 0.041844, current_train_items 104800.
I0302 18:59:27.235199 22760421793920 run.py:483] Algo bellman_ford step 3275 current loss 0.017005, current_train_items 104832.
I0302 18:59:27.250728 22760421793920 run.py:483] Algo bellman_ford step 3276 current loss 0.031283, current_train_items 104864.
I0302 18:59:27.273408 22760421793920 run.py:483] Algo bellman_ford step 3277 current loss 0.053376, current_train_items 104896.
I0302 18:59:27.304365 22760421793920 run.py:483] Algo bellman_ford step 3278 current loss 0.102551, current_train_items 104928.
I0302 18:59:27.337293 22760421793920 run.py:483] Algo bellman_ford step 3279 current loss 0.110700, current_train_items 104960.
I0302 18:59:27.355781 22760421793920 run.py:483] Algo bellman_ford step 3280 current loss 0.005683, current_train_items 104992.
I0302 18:59:27.371280 22760421793920 run.py:483] Algo bellman_ford step 3281 current loss 0.024711, current_train_items 105024.
I0302 18:59:27.394099 22760421793920 run.py:483] Algo bellman_ford step 3282 current loss 0.072210, current_train_items 105056.
I0302 18:59:27.424634 22760421793920 run.py:483] Algo bellman_ford step 3283 current loss 0.086371, current_train_items 105088.
I0302 18:59:27.457419 22760421793920 run.py:483] Algo bellman_ford step 3284 current loss 0.110881, current_train_items 105120.
I0302 18:59:27.475553 22760421793920 run.py:483] Algo bellman_ford step 3285 current loss 0.014427, current_train_items 105152.
I0302 18:59:27.491586 22760421793920 run.py:483] Algo bellman_ford step 3286 current loss 0.017916, current_train_items 105184.
I0302 18:59:27.515073 22760421793920 run.py:483] Algo bellman_ford step 3287 current loss 0.061784, current_train_items 105216.
I0302 18:59:27.544079 22760421793920 run.py:483] Algo bellman_ford step 3288 current loss 0.072442, current_train_items 105248.
I0302 18:59:27.574540 22760421793920 run.py:483] Algo bellman_ford step 3289 current loss 0.125493, current_train_items 105280.
I0302 18:59:27.592825 22760421793920 run.py:483] Algo bellman_ford step 3290 current loss 0.015137, current_train_items 105312.
I0302 18:59:27.609005 22760421793920 run.py:483] Algo bellman_ford step 3291 current loss 0.019415, current_train_items 105344.
I0302 18:59:27.632121 22760421793920 run.py:483] Algo bellman_ford step 3292 current loss 0.055408, current_train_items 105376.
I0302 18:59:27.662186 22760421793920 run.py:483] Algo bellman_ford step 3293 current loss 0.097976, current_train_items 105408.
I0302 18:59:27.694568 22760421793920 run.py:483] Algo bellman_ford step 3294 current loss 0.106743, current_train_items 105440.
I0302 18:59:27.712885 22760421793920 run.py:483] Algo bellman_ford step 3295 current loss 0.011559, current_train_items 105472.
I0302 18:59:27.728251 22760421793920 run.py:483] Algo bellman_ford step 3296 current loss 0.009608, current_train_items 105504.
I0302 18:59:27.752038 22760421793920 run.py:483] Algo bellman_ford step 3297 current loss 0.087759, current_train_items 105536.
I0302 18:59:27.783454 22760421793920 run.py:483] Algo bellman_ford step 3298 current loss 0.131014, current_train_items 105568.
I0302 18:59:27.816892 22760421793920 run.py:483] Algo bellman_ford step 3299 current loss 0.102173, current_train_items 105600.
I0302 18:59:27.834963 22760421793920 run.py:483] Algo bellman_ford step 3300 current loss 0.009883, current_train_items 105632.
I0302 18:59:27.842483 22760421793920 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 18:59:27.842591 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 18:59:27.858836 22760421793920 run.py:483] Algo bellman_ford step 3301 current loss 0.041953, current_train_items 105664.
I0302 18:59:27.881963 22760421793920 run.py:483] Algo bellman_ford step 3302 current loss 0.136683, current_train_items 105696.
I0302 18:59:27.910878 22760421793920 run.py:483] Algo bellman_ford step 3303 current loss 0.068373, current_train_items 105728.
I0302 18:59:27.941415 22760421793920 run.py:483] Algo bellman_ford step 3304 current loss 0.077811, current_train_items 105760.
I0302 18:59:27.960068 22760421793920 run.py:483] Algo bellman_ford step 3305 current loss 0.003243, current_train_items 105792.
I0302 18:59:27.975873 22760421793920 run.py:483] Algo bellman_ford step 3306 current loss 0.045874, current_train_items 105824.
I0302 18:59:27.999454 22760421793920 run.py:483] Algo bellman_ford step 3307 current loss 0.049080, current_train_items 105856.
I0302 18:59:28.029659 22760421793920 run.py:483] Algo bellman_ford step 3308 current loss 0.052152, current_train_items 105888.
I0302 18:59:28.063791 22760421793920 run.py:483] Algo bellman_ford step 3309 current loss 0.122153, current_train_items 105920.
I0302 18:59:28.082369 22760421793920 run.py:483] Algo bellman_ford step 3310 current loss 0.022871, current_train_items 105952.
I0302 18:59:28.098366 22760421793920 run.py:483] Algo bellman_ford step 3311 current loss 0.109350, current_train_items 105984.
I0302 18:59:28.121294 22760421793920 run.py:483] Algo bellman_ford step 3312 current loss 0.137529, current_train_items 106016.
I0302 18:59:28.152033 22760421793920 run.py:483] Algo bellman_ford step 3313 current loss 0.176104, current_train_items 106048.
I0302 18:59:28.183021 22760421793920 run.py:483] Algo bellman_ford step 3314 current loss 0.164388, current_train_items 106080.
I0302 18:59:28.201375 22760421793920 run.py:483] Algo bellman_ford step 3315 current loss 0.006715, current_train_items 106112.
I0302 18:59:28.217443 22760421793920 run.py:483] Algo bellman_ford step 3316 current loss 0.025162, current_train_items 106144.
I0302 18:59:28.239971 22760421793920 run.py:483] Algo bellman_ford step 3317 current loss 0.149792, current_train_items 106176.
I0302 18:59:28.269956 22760421793920 run.py:483] Algo bellman_ford step 3318 current loss 0.195910, current_train_items 106208.
I0302 18:59:28.302578 22760421793920 run.py:483] Algo bellman_ford step 3319 current loss 0.231584, current_train_items 106240.
I0302 18:59:28.320930 22760421793920 run.py:483] Algo bellman_ford step 3320 current loss 0.027454, current_train_items 106272.
I0302 18:59:28.336678 22760421793920 run.py:483] Algo bellman_ford step 3321 current loss 0.077146, current_train_items 106304.
I0302 18:59:28.360501 22760421793920 run.py:483] Algo bellman_ford step 3322 current loss 0.117246, current_train_items 106336.
I0302 18:59:28.390739 22760421793920 run.py:483] Algo bellman_ford step 3323 current loss 0.139011, current_train_items 106368.
I0302 18:59:28.421919 22760421793920 run.py:483] Algo bellman_ford step 3324 current loss 0.120833, current_train_items 106400.
I0302 18:59:28.439988 22760421793920 run.py:483] Algo bellman_ford step 3325 current loss 0.004861, current_train_items 106432.
I0302 18:59:28.455858 22760421793920 run.py:483] Algo bellman_ford step 3326 current loss 0.043477, current_train_items 106464.
I0302 18:59:28.478392 22760421793920 run.py:483] Algo bellman_ford step 3327 current loss 0.073167, current_train_items 106496.
I0302 18:59:28.507925 22760421793920 run.py:483] Algo bellman_ford step 3328 current loss 0.089859, current_train_items 106528.
I0302 18:59:28.539814 22760421793920 run.py:483] Algo bellman_ford step 3329 current loss 0.136261, current_train_items 106560.
I0302 18:59:28.558336 22760421793920 run.py:483] Algo bellman_ford step 3330 current loss 0.046097, current_train_items 106592.
I0302 18:59:28.573858 22760421793920 run.py:483] Algo bellman_ford step 3331 current loss 0.085925, current_train_items 106624.
I0302 18:59:28.596666 22760421793920 run.py:483] Algo bellman_ford step 3332 current loss 0.090695, current_train_items 106656.
I0302 18:59:28.625782 22760421793920 run.py:483] Algo bellman_ford step 3333 current loss 0.134293, current_train_items 106688.
I0302 18:59:28.656969 22760421793920 run.py:483] Algo bellman_ford step 3334 current loss 0.082359, current_train_items 106720.
I0302 18:59:28.675137 22760421793920 run.py:483] Algo bellman_ford step 3335 current loss 0.010308, current_train_items 106752.
I0302 18:59:28.690464 22760421793920 run.py:483] Algo bellman_ford step 3336 current loss 0.059426, current_train_items 106784.
I0302 18:59:28.713842 22760421793920 run.py:483] Algo bellman_ford step 3337 current loss 0.086883, current_train_items 106816.
I0302 18:59:28.744991 22760421793920 run.py:483] Algo bellman_ford step 3338 current loss 0.160212, current_train_items 106848.
I0302 18:59:28.778481 22760421793920 run.py:483] Algo bellman_ford step 3339 current loss 0.123296, current_train_items 106880.
I0302 18:59:28.796606 22760421793920 run.py:483] Algo bellman_ford step 3340 current loss 0.005177, current_train_items 106912.
I0302 18:59:28.812795 22760421793920 run.py:483] Algo bellman_ford step 3341 current loss 0.030367, current_train_items 106944.
I0302 18:59:28.835876 22760421793920 run.py:483] Algo bellman_ford step 3342 current loss 0.084986, current_train_items 106976.
I0302 18:59:28.866808 22760421793920 run.py:483] Algo bellman_ford step 3343 current loss 0.095583, current_train_items 107008.
I0302 18:59:28.897084 22760421793920 run.py:483] Algo bellman_ford step 3344 current loss 0.095218, current_train_items 107040.
I0302 18:59:28.915417 22760421793920 run.py:483] Algo bellman_ford step 3345 current loss 0.019216, current_train_items 107072.
I0302 18:59:28.930918 22760421793920 run.py:483] Algo bellman_ford step 3346 current loss 0.034921, current_train_items 107104.
I0302 18:59:28.953811 22760421793920 run.py:483] Algo bellman_ford step 3347 current loss 0.043831, current_train_items 107136.
I0302 18:59:28.983618 22760421793920 run.py:483] Algo bellman_ford step 3348 current loss 0.052539, current_train_items 107168.
I0302 18:59:29.013911 22760421793920 run.py:483] Algo bellman_ford step 3349 current loss 0.091621, current_train_items 107200.
I0302 18:59:29.032479 22760421793920 run.py:483] Algo bellman_ford step 3350 current loss 0.006737, current_train_items 107232.
I0302 18:59:29.040135 22760421793920 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 18:59:29.040245 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:59:29.057001 22760421793920 run.py:483] Algo bellman_ford step 3351 current loss 0.043300, current_train_items 107264.
I0302 18:59:29.080717 22760421793920 run.py:483] Algo bellman_ford step 3352 current loss 0.077035, current_train_items 107296.
I0302 18:59:29.111611 22760421793920 run.py:483] Algo bellman_ford step 3353 current loss 0.044708, current_train_items 107328.
I0302 18:59:29.144379 22760421793920 run.py:483] Algo bellman_ford step 3354 current loss 0.081022, current_train_items 107360.
I0302 18:59:29.162966 22760421793920 run.py:483] Algo bellman_ford step 3355 current loss 0.001469, current_train_items 107392.
I0302 18:59:29.178762 22760421793920 run.py:483] Algo bellman_ford step 3356 current loss 0.038075, current_train_items 107424.
I0302 18:59:29.201261 22760421793920 run.py:483] Algo bellman_ford step 3357 current loss 0.045281, current_train_items 107456.
I0302 18:59:29.231057 22760421793920 run.py:483] Algo bellman_ford step 3358 current loss 0.064671, current_train_items 107488.
I0302 18:59:29.264514 22760421793920 run.py:483] Algo bellman_ford step 3359 current loss 0.101161, current_train_items 107520.
I0302 18:59:29.282481 22760421793920 run.py:483] Algo bellman_ford step 3360 current loss 0.003482, current_train_items 107552.
I0302 18:59:29.298030 22760421793920 run.py:483] Algo bellman_ford step 3361 current loss 0.018700, current_train_items 107584.
I0302 18:59:29.321485 22760421793920 run.py:483] Algo bellman_ford step 3362 current loss 0.076670, current_train_items 107616.
I0302 18:59:29.351446 22760421793920 run.py:483] Algo bellman_ford step 3363 current loss 0.067501, current_train_items 107648.
I0302 18:59:29.382500 22760421793920 run.py:483] Algo bellman_ford step 3364 current loss 0.094042, current_train_items 107680.
I0302 18:59:29.400575 22760421793920 run.py:483] Algo bellman_ford step 3365 current loss 0.017591, current_train_items 107712.
I0302 18:59:29.415880 22760421793920 run.py:483] Algo bellman_ford step 3366 current loss 0.005004, current_train_items 107744.
I0302 18:59:29.439161 22760421793920 run.py:483] Algo bellman_ford step 3367 current loss 0.055877, current_train_items 107776.
I0302 18:59:29.467474 22760421793920 run.py:483] Algo bellman_ford step 3368 current loss 0.093198, current_train_items 107808.
I0302 18:59:29.498258 22760421793920 run.py:483] Algo bellman_ford step 3369 current loss 0.089772, current_train_items 107840.
I0302 18:59:29.516294 22760421793920 run.py:483] Algo bellman_ford step 3370 current loss 0.004090, current_train_items 107872.
I0302 18:59:29.532146 22760421793920 run.py:483] Algo bellman_ford step 3371 current loss 0.026584, current_train_items 107904.
I0302 18:59:29.554986 22760421793920 run.py:483] Algo bellman_ford step 3372 current loss 0.065276, current_train_items 107936.
I0302 18:59:29.586570 22760421793920 run.py:483] Algo bellman_ford step 3373 current loss 0.084494, current_train_items 107968.
I0302 18:59:29.617973 22760421793920 run.py:483] Algo bellman_ford step 3374 current loss 0.166584, current_train_items 108000.
I0302 18:59:29.636279 22760421793920 run.py:483] Algo bellman_ford step 3375 current loss 0.004187, current_train_items 108032.
I0302 18:59:29.651606 22760421793920 run.py:483] Algo bellman_ford step 3376 current loss 0.017790, current_train_items 108064.
I0302 18:59:29.674374 22760421793920 run.py:483] Algo bellman_ford step 3377 current loss 0.056154, current_train_items 108096.
I0302 18:59:29.703251 22760421793920 run.py:483] Algo bellman_ford step 3378 current loss 0.045083, current_train_items 108128.
I0302 18:59:29.736351 22760421793920 run.py:483] Algo bellman_ford step 3379 current loss 0.087930, current_train_items 108160.
I0302 18:59:29.754598 22760421793920 run.py:483] Algo bellman_ford step 3380 current loss 0.017971, current_train_items 108192.
I0302 18:59:29.770587 22760421793920 run.py:483] Algo bellman_ford step 3381 current loss 0.043547, current_train_items 108224.
I0302 18:59:29.794267 22760421793920 run.py:483] Algo bellman_ford step 3382 current loss 0.042025, current_train_items 108256.
I0302 18:59:29.823016 22760421793920 run.py:483] Algo bellman_ford step 3383 current loss 0.060626, current_train_items 108288.
I0302 18:59:29.856871 22760421793920 run.py:483] Algo bellman_ford step 3384 current loss 0.138765, current_train_items 108320.
I0302 18:59:29.874994 22760421793920 run.py:483] Algo bellman_ford step 3385 current loss 0.011339, current_train_items 108352.
I0302 18:59:29.890931 22760421793920 run.py:483] Algo bellman_ford step 3386 current loss 0.051779, current_train_items 108384.
I0302 18:59:29.913636 22760421793920 run.py:483] Algo bellman_ford step 3387 current loss 0.059291, current_train_items 108416.
I0302 18:59:29.942957 22760421793920 run.py:483] Algo bellman_ford step 3388 current loss 0.138632, current_train_items 108448.
I0302 18:59:29.973353 22760421793920 run.py:483] Algo bellman_ford step 3389 current loss 0.114514, current_train_items 108480.
I0302 18:59:29.991429 22760421793920 run.py:483] Algo bellman_ford step 3390 current loss 0.019899, current_train_items 108512.
I0302 18:59:30.007087 22760421793920 run.py:483] Algo bellman_ford step 3391 current loss 0.035759, current_train_items 108544.
I0302 18:59:30.029938 22760421793920 run.py:483] Algo bellman_ford step 3392 current loss 0.095691, current_train_items 108576.
I0302 18:59:30.058884 22760421793920 run.py:483] Algo bellman_ford step 3393 current loss 0.098273, current_train_items 108608.
I0302 18:59:30.089075 22760421793920 run.py:483] Algo bellman_ford step 3394 current loss 0.087322, current_train_items 108640.
I0302 18:59:30.107189 22760421793920 run.py:483] Algo bellman_ford step 3395 current loss 0.011131, current_train_items 108672.
I0302 18:59:30.123084 22760421793920 run.py:483] Algo bellman_ford step 3396 current loss 0.018320, current_train_items 108704.
I0302 18:59:30.145802 22760421793920 run.py:483] Algo bellman_ford step 3397 current loss 0.097992, current_train_items 108736.
I0302 18:59:30.175982 22760421793920 run.py:483] Algo bellman_ford step 3398 current loss 0.140981, current_train_items 108768.
I0302 18:59:30.207029 22760421793920 run.py:483] Algo bellman_ford step 3399 current loss 0.110075, current_train_items 108800.
I0302 18:59:30.225196 22760421793920 run.py:483] Algo bellman_ford step 3400 current loss 0.006169, current_train_items 108832.
I0302 18:59:30.232657 22760421793920 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 18:59:30.232768 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:30.249469 22760421793920 run.py:483] Algo bellman_ford step 3401 current loss 0.017217, current_train_items 108864.
I0302 18:59:30.273001 22760421793920 run.py:483] Algo bellman_ford step 3402 current loss 0.029804, current_train_items 108896.
I0302 18:59:30.303398 22760421793920 run.py:483] Algo bellman_ford step 3403 current loss 0.103569, current_train_items 108928.
I0302 18:59:30.336729 22760421793920 run.py:483] Algo bellman_ford step 3404 current loss 0.113217, current_train_items 108960.
I0302 18:59:30.355444 22760421793920 run.py:483] Algo bellman_ford step 3405 current loss 0.004545, current_train_items 108992.
I0302 18:59:30.370747 22760421793920 run.py:483] Algo bellman_ford step 3406 current loss 0.022611, current_train_items 109024.
I0302 18:59:30.393971 22760421793920 run.py:483] Algo bellman_ford step 3407 current loss 0.051634, current_train_items 109056.
I0302 18:59:30.425506 22760421793920 run.py:483] Algo bellman_ford step 3408 current loss 0.070323, current_train_items 109088.
I0302 18:59:30.456989 22760421793920 run.py:483] Algo bellman_ford step 3409 current loss 0.077899, current_train_items 109120.
I0302 18:59:30.474987 22760421793920 run.py:483] Algo bellman_ford step 3410 current loss 0.003550, current_train_items 109152.
I0302 18:59:30.491140 22760421793920 run.py:483] Algo bellman_ford step 3411 current loss 0.045243, current_train_items 109184.
I0302 18:59:30.515183 22760421793920 run.py:483] Algo bellman_ford step 3412 current loss 0.075496, current_train_items 109216.
I0302 18:59:30.546445 22760421793920 run.py:483] Algo bellman_ford step 3413 current loss 0.124276, current_train_items 109248.
I0302 18:59:30.581371 22760421793920 run.py:483] Algo bellman_ford step 3414 current loss 0.116902, current_train_items 109280.
I0302 18:59:30.599446 22760421793920 run.py:483] Algo bellman_ford step 3415 current loss 0.002561, current_train_items 109312.
I0302 18:59:30.615277 22760421793920 run.py:483] Algo bellman_ford step 3416 current loss 0.025040, current_train_items 109344.
I0302 18:59:30.638456 22760421793920 run.py:483] Algo bellman_ford step 3417 current loss 0.086696, current_train_items 109376.
I0302 18:59:30.668852 22760421793920 run.py:483] Algo bellman_ford step 3418 current loss 0.331283, current_train_items 109408.
I0302 18:59:30.700266 22760421793920 run.py:483] Algo bellman_ford step 3419 current loss 0.171729, current_train_items 109440.
I0302 18:59:30.718679 22760421793920 run.py:483] Algo bellman_ford step 3420 current loss 0.002336, current_train_items 109472.
I0302 18:59:30.734149 22760421793920 run.py:483] Algo bellman_ford step 3421 current loss 0.047189, current_train_items 109504.
I0302 18:59:30.757366 22760421793920 run.py:483] Algo bellman_ford step 3422 current loss 0.052616, current_train_items 109536.
I0302 18:59:30.787952 22760421793920 run.py:483] Algo bellman_ford step 3423 current loss 0.045645, current_train_items 109568.
I0302 18:59:30.819828 22760421793920 run.py:483] Algo bellman_ford step 3424 current loss 0.158832, current_train_items 109600.
I0302 18:59:30.837823 22760421793920 run.py:483] Algo bellman_ford step 3425 current loss 0.007301, current_train_items 109632.
I0302 18:59:30.854072 22760421793920 run.py:483] Algo bellman_ford step 3426 current loss 0.031651, current_train_items 109664.
I0302 18:59:30.876786 22760421793920 run.py:483] Algo bellman_ford step 3427 current loss 0.037718, current_train_items 109696.
I0302 18:59:30.907045 22760421793920 run.py:483] Algo bellman_ford step 3428 current loss 0.096378, current_train_items 109728.
I0302 18:59:30.936413 22760421793920 run.py:483] Algo bellman_ford step 3429 current loss 0.051640, current_train_items 109760.
I0302 18:59:30.954863 22760421793920 run.py:483] Algo bellman_ford step 3430 current loss 0.016086, current_train_items 109792.
I0302 18:59:30.970888 22760421793920 run.py:483] Algo bellman_ford step 3431 current loss 0.002624, current_train_items 109824.
I0302 18:59:30.993309 22760421793920 run.py:483] Algo bellman_ford step 3432 current loss 0.066988, current_train_items 109856.
I0302 18:59:31.025020 22760421793920 run.py:483] Algo bellman_ford step 3433 current loss 0.089454, current_train_items 109888.
I0302 18:59:31.055908 22760421793920 run.py:483] Algo bellman_ford step 3434 current loss 0.110469, current_train_items 109920.
I0302 18:59:31.074144 22760421793920 run.py:483] Algo bellman_ford step 3435 current loss 0.003973, current_train_items 109952.
I0302 18:59:31.089929 22760421793920 run.py:483] Algo bellman_ford step 3436 current loss 0.025359, current_train_items 109984.
I0302 18:59:31.113035 22760421793920 run.py:483] Algo bellman_ford step 3437 current loss 0.035739, current_train_items 110016.
I0302 18:59:31.142119 22760421793920 run.py:483] Algo bellman_ford step 3438 current loss 0.107752, current_train_items 110048.
I0302 18:59:31.172307 22760421793920 run.py:483] Algo bellman_ford step 3439 current loss 0.117019, current_train_items 110080.
I0302 18:59:31.190512 22760421793920 run.py:483] Algo bellman_ford step 3440 current loss 0.014423, current_train_items 110112.
I0302 18:59:31.206582 22760421793920 run.py:483] Algo bellman_ford step 3441 current loss 0.071362, current_train_items 110144.
I0302 18:59:31.229305 22760421793920 run.py:483] Algo bellman_ford step 3442 current loss 0.144710, current_train_items 110176.
I0302 18:59:31.260058 22760421793920 run.py:483] Algo bellman_ford step 3443 current loss 0.228540, current_train_items 110208.
I0302 18:59:31.293104 22760421793920 run.py:483] Algo bellman_ford step 3444 current loss 0.138009, current_train_items 110240.
I0302 18:59:31.311480 22760421793920 run.py:483] Algo bellman_ford step 3445 current loss 0.004760, current_train_items 110272.
I0302 18:59:31.327541 22760421793920 run.py:483] Algo bellman_ford step 3446 current loss 0.034436, current_train_items 110304.
I0302 18:59:31.350711 22760421793920 run.py:483] Algo bellman_ford step 3447 current loss 0.139965, current_train_items 110336.
I0302 18:59:31.380543 22760421793920 run.py:483] Algo bellman_ford step 3448 current loss 0.182479, current_train_items 110368.
I0302 18:59:31.412679 22760421793920 run.py:483] Algo bellman_ford step 3449 current loss 0.160374, current_train_items 110400.
I0302 18:59:31.430876 22760421793920 run.py:483] Algo bellman_ford step 3450 current loss 0.027120, current_train_items 110432.
I0302 18:59:31.438322 22760421793920 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 18:59:31.438430 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:31.454675 22760421793920 run.py:483] Algo bellman_ford step 3451 current loss 0.022580, current_train_items 110464.
I0302 18:59:31.478595 22760421793920 run.py:483] Algo bellman_ford step 3452 current loss 0.085012, current_train_items 110496.
I0302 18:59:31.507600 22760421793920 run.py:483] Algo bellman_ford step 3453 current loss 0.092223, current_train_items 110528.
I0302 18:59:31.542128 22760421793920 run.py:483] Algo bellman_ford step 3454 current loss 0.148056, current_train_items 110560.
I0302 18:59:31.561021 22760421793920 run.py:483] Algo bellman_ford step 3455 current loss 0.002173, current_train_items 110592.
I0302 18:59:31.576752 22760421793920 run.py:483] Algo bellman_ford step 3456 current loss 0.041889, current_train_items 110624.
I0302 18:59:31.599677 22760421793920 run.py:483] Algo bellman_ford step 3457 current loss 0.050115, current_train_items 110656.
I0302 18:59:31.630558 22760421793920 run.py:483] Algo bellman_ford step 3458 current loss 0.067273, current_train_items 110688.
I0302 18:59:31.661866 22760421793920 run.py:483] Algo bellman_ford step 3459 current loss 0.067587, current_train_items 110720.
I0302 18:59:31.680213 22760421793920 run.py:483] Algo bellman_ford step 3460 current loss 0.006759, current_train_items 110752.
I0302 18:59:31.696086 22760421793920 run.py:483] Algo bellman_ford step 3461 current loss 0.044656, current_train_items 110784.
I0302 18:59:31.719113 22760421793920 run.py:483] Algo bellman_ford step 3462 current loss 0.070964, current_train_items 110816.
I0302 18:59:31.748894 22760421793920 run.py:483] Algo bellman_ford step 3463 current loss 0.071007, current_train_items 110848.
I0302 18:59:31.780101 22760421793920 run.py:483] Algo bellman_ford step 3464 current loss 0.081126, current_train_items 110880.
I0302 18:59:31.798442 22760421793920 run.py:483] Algo bellman_ford step 3465 current loss 0.020914, current_train_items 110912.
I0302 18:59:31.814482 22760421793920 run.py:483] Algo bellman_ford step 3466 current loss 0.028363, current_train_items 110944.
I0302 18:59:31.838043 22760421793920 run.py:483] Algo bellman_ford step 3467 current loss 0.066040, current_train_items 110976.
I0302 18:59:31.869047 22760421793920 run.py:483] Algo bellman_ford step 3468 current loss 0.090251, current_train_items 111008.
I0302 18:59:31.903233 22760421793920 run.py:483] Algo bellman_ford step 3469 current loss 0.094710, current_train_items 111040.
I0302 18:59:31.921582 22760421793920 run.py:483] Algo bellman_ford step 3470 current loss 0.004571, current_train_items 111072.
I0302 18:59:31.937948 22760421793920 run.py:483] Algo bellman_ford step 3471 current loss 0.025662, current_train_items 111104.
I0302 18:59:31.960911 22760421793920 run.py:483] Algo bellman_ford step 3472 current loss 0.051855, current_train_items 111136.
I0302 18:59:31.992356 22760421793920 run.py:483] Algo bellman_ford step 3473 current loss 0.081380, current_train_items 111168.
I0302 18:59:32.024585 22760421793920 run.py:483] Algo bellman_ford step 3474 current loss 0.073576, current_train_items 111200.
I0302 18:59:32.042855 22760421793920 run.py:483] Algo bellman_ford step 3475 current loss 0.006429, current_train_items 111232.
I0302 18:59:32.058398 22760421793920 run.py:483] Algo bellman_ford step 3476 current loss 0.053229, current_train_items 111264.
I0302 18:59:32.080835 22760421793920 run.py:483] Algo bellman_ford step 3477 current loss 0.081903, current_train_items 111296.
I0302 18:59:32.110122 22760421793920 run.py:483] Algo bellman_ford step 3478 current loss 0.065903, current_train_items 111328.
I0302 18:59:32.143286 22760421793920 run.py:483] Algo bellman_ford step 3479 current loss 0.127621, current_train_items 111360.
I0302 18:59:32.161415 22760421793920 run.py:483] Algo bellman_ford step 3480 current loss 0.023380, current_train_items 111392.
I0302 18:59:32.177046 22760421793920 run.py:483] Algo bellman_ford step 3481 current loss 0.051098, current_train_items 111424.
I0302 18:59:32.199661 22760421793920 run.py:483] Algo bellman_ford step 3482 current loss 0.084058, current_train_items 111456.
I0302 18:59:32.228450 22760421793920 run.py:483] Algo bellman_ford step 3483 current loss 0.109970, current_train_items 111488.
I0302 18:59:32.259749 22760421793920 run.py:483] Algo bellman_ford step 3484 current loss 0.098131, current_train_items 111520.
I0302 18:59:32.278201 22760421793920 run.py:483] Algo bellman_ford step 3485 current loss 0.002566, current_train_items 111552.
I0302 18:59:32.294117 22760421793920 run.py:483] Algo bellman_ford step 3486 current loss 0.046954, current_train_items 111584.
I0302 18:59:32.317460 22760421793920 run.py:483] Algo bellman_ford step 3487 current loss 0.180110, current_train_items 111616.
I0302 18:59:32.346662 22760421793920 run.py:483] Algo bellman_ford step 3488 current loss 0.162476, current_train_items 111648.
I0302 18:59:32.376799 22760421793920 run.py:483] Algo bellman_ford step 3489 current loss 0.094127, current_train_items 111680.
I0302 18:59:32.395381 22760421793920 run.py:483] Algo bellman_ford step 3490 current loss 0.003502, current_train_items 111712.
I0302 18:59:32.410752 22760421793920 run.py:483] Algo bellman_ford step 3491 current loss 0.019722, current_train_items 111744.
I0302 18:59:32.433914 22760421793920 run.py:483] Algo bellman_ford step 3492 current loss 0.103597, current_train_items 111776.
I0302 18:59:32.464818 22760421793920 run.py:483] Algo bellman_ford step 3493 current loss 0.113346, current_train_items 111808.
I0302 18:59:32.496103 22760421793920 run.py:483] Algo bellman_ford step 3494 current loss 0.101211, current_train_items 111840.
I0302 18:59:32.514482 22760421793920 run.py:483] Algo bellman_ford step 3495 current loss 0.003725, current_train_items 111872.
I0302 18:59:32.530340 22760421793920 run.py:483] Algo bellman_ford step 3496 current loss 0.055992, current_train_items 111904.
I0302 18:59:32.552613 22760421793920 run.py:483] Algo bellman_ford step 3497 current loss 0.148208, current_train_items 111936.
I0302 18:59:32.582925 22760421793920 run.py:483] Algo bellman_ford step 3498 current loss 0.185473, current_train_items 111968.
I0302 18:59:32.615574 22760421793920 run.py:483] Algo bellman_ford step 3499 current loss 0.187847, current_train_items 112000.
I0302 18:59:32.633971 22760421793920 run.py:483] Algo bellman_ford step 3500 current loss 0.004146, current_train_items 112032.
I0302 18:59:32.641556 22760421793920 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 18:59:32.641666 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:32.657442 22760421793920 run.py:483] Algo bellman_ford step 3501 current loss 0.015238, current_train_items 112064.
I0302 18:59:32.680495 22760421793920 run.py:483] Algo bellman_ford step 3502 current loss 0.041921, current_train_items 112096.
I0302 18:59:32.710818 22760421793920 run.py:483] Algo bellman_ford step 3503 current loss 0.098284, current_train_items 112128.
I0302 18:59:32.745018 22760421793920 run.py:483] Algo bellman_ford step 3504 current loss 0.094194, current_train_items 112160.
I0302 18:59:32.763678 22760421793920 run.py:483] Algo bellman_ford step 3505 current loss 0.010435, current_train_items 112192.
I0302 18:59:32.778562 22760421793920 run.py:483] Algo bellman_ford step 3506 current loss 0.021042, current_train_items 112224.
I0302 18:59:32.801969 22760421793920 run.py:483] Algo bellman_ford step 3507 current loss 0.040444, current_train_items 112256.
I0302 18:59:32.830873 22760421793920 run.py:483] Algo bellman_ford step 3508 current loss 0.073984, current_train_items 112288.
I0302 18:59:32.860202 22760421793920 run.py:483] Algo bellman_ford step 3509 current loss 0.047368, current_train_items 112320.
I0302 18:59:32.878221 22760421793920 run.py:483] Algo bellman_ford step 3510 current loss 0.013348, current_train_items 112352.
I0302 18:59:32.893691 22760421793920 run.py:483] Algo bellman_ford step 3511 current loss 0.030007, current_train_items 112384.
I0302 18:59:32.915997 22760421793920 run.py:483] Algo bellman_ford step 3512 current loss 0.063749, current_train_items 112416.
I0302 18:59:32.944736 22760421793920 run.py:483] Algo bellman_ford step 3513 current loss 0.058904, current_train_items 112448.
I0302 18:59:32.975323 22760421793920 run.py:483] Algo bellman_ford step 3514 current loss 0.049147, current_train_items 112480.
I0302 18:59:32.993139 22760421793920 run.py:483] Algo bellman_ford step 3515 current loss 0.004478, current_train_items 112512.
I0302 18:59:33.008380 22760421793920 run.py:483] Algo bellman_ford step 3516 current loss 0.020594, current_train_items 112544.
I0302 18:59:33.031162 22760421793920 run.py:483] Algo bellman_ford step 3517 current loss 0.238651, current_train_items 112576.
I0302 18:59:33.061292 22760421793920 run.py:483] Algo bellman_ford step 3518 current loss 0.305415, current_train_items 112608.
I0302 18:59:33.095452 22760421793920 run.py:483] Algo bellman_ford step 3519 current loss 0.254853, current_train_items 112640.
I0302 18:59:33.113659 22760421793920 run.py:483] Algo bellman_ford step 3520 current loss 0.034243, current_train_items 112672.
I0302 18:59:33.129199 22760421793920 run.py:483] Algo bellman_ford step 3521 current loss 0.023640, current_train_items 112704.
I0302 18:59:33.151150 22760421793920 run.py:483] Algo bellman_ford step 3522 current loss 0.034336, current_train_items 112736.
I0302 18:59:33.180741 22760421793920 run.py:483] Algo bellman_ford step 3523 current loss 0.219154, current_train_items 112768.
I0302 18:59:33.212234 22760421793920 run.py:483] Algo bellman_ford step 3524 current loss 0.345821, current_train_items 112800.
I0302 18:59:33.230093 22760421793920 run.py:483] Algo bellman_ford step 3525 current loss 0.171453, current_train_items 112832.
I0302 18:59:33.245465 22760421793920 run.py:483] Algo bellman_ford step 3526 current loss 0.086295, current_train_items 112864.
I0302 18:59:33.268461 22760421793920 run.py:483] Algo bellman_ford step 3527 current loss 0.083047, current_train_items 112896.
I0302 18:59:33.299822 22760421793920 run.py:483] Algo bellman_ford step 3528 current loss 0.097165, current_train_items 112928.
I0302 18:59:33.330602 22760421793920 run.py:483] Algo bellman_ford step 3529 current loss 0.127916, current_train_items 112960.
I0302 18:59:33.348652 22760421793920 run.py:483] Algo bellman_ford step 3530 current loss 0.008795, current_train_items 112992.
I0302 18:59:33.364738 22760421793920 run.py:483] Algo bellman_ford step 3531 current loss 0.019039, current_train_items 113024.
I0302 18:59:33.388772 22760421793920 run.py:483] Algo bellman_ford step 3532 current loss 0.122347, current_train_items 113056.
I0302 18:59:33.421214 22760421793920 run.py:483] Algo bellman_ford step 3533 current loss 0.157278, current_train_items 113088.
I0302 18:59:33.453315 22760421793920 run.py:483] Algo bellman_ford step 3534 current loss 0.128464, current_train_items 113120.
I0302 18:59:33.471550 22760421793920 run.py:483] Algo bellman_ford step 3535 current loss 0.065516, current_train_items 113152.
I0302 18:59:33.487454 22760421793920 run.py:483] Algo bellman_ford step 3536 current loss 0.141177, current_train_items 113184.
I0302 18:59:33.510084 22760421793920 run.py:483] Algo bellman_ford step 3537 current loss 0.121322, current_train_items 113216.
I0302 18:59:33.540591 22760421793920 run.py:483] Algo bellman_ford step 3538 current loss 0.185244, current_train_items 113248.
I0302 18:59:33.574419 22760421793920 run.py:483] Algo bellman_ford step 3539 current loss 0.162310, current_train_items 113280.
I0302 18:59:33.592474 22760421793920 run.py:483] Algo bellman_ford step 3540 current loss 0.012022, current_train_items 113312.
I0302 18:59:33.608028 22760421793920 run.py:483] Algo bellman_ford step 3541 current loss 0.015494, current_train_items 113344.
I0302 18:59:33.631493 22760421793920 run.py:483] Algo bellman_ford step 3542 current loss 0.074706, current_train_items 113376.
I0302 18:59:33.660559 22760421793920 run.py:483] Algo bellman_ford step 3543 current loss 0.106432, current_train_items 113408.
I0302 18:59:33.691801 22760421793920 run.py:483] Algo bellman_ford step 3544 current loss 0.157859, current_train_items 113440.
I0302 18:59:33.709718 22760421793920 run.py:483] Algo bellman_ford step 3545 current loss 0.015805, current_train_items 113472.
I0302 18:59:33.725207 22760421793920 run.py:483] Algo bellman_ford step 3546 current loss 0.018462, current_train_items 113504.
I0302 18:59:33.748130 22760421793920 run.py:483] Algo bellman_ford step 3547 current loss 0.068227, current_train_items 113536.
I0302 18:59:33.776852 22760421793920 run.py:483] Algo bellman_ford step 3548 current loss 0.043079, current_train_items 113568.
I0302 18:59:33.808622 22760421793920 run.py:483] Algo bellman_ford step 3549 current loss 0.107432, current_train_items 113600.
I0302 18:59:33.826923 22760421793920 run.py:483] Algo bellman_ford step 3550 current loss 0.011635, current_train_items 113632.
I0302 18:59:33.834555 22760421793920 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 18:59:33.834676 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:59:33.851526 22760421793920 run.py:483] Algo bellman_ford step 3551 current loss 0.035843, current_train_items 113664.
I0302 18:59:33.876270 22760421793920 run.py:483] Algo bellman_ford step 3552 current loss 0.072185, current_train_items 113696.
I0302 18:59:33.906205 22760421793920 run.py:483] Algo bellman_ford step 3553 current loss 0.045688, current_train_items 113728.
I0302 18:59:33.938509 22760421793920 run.py:483] Algo bellman_ford step 3554 current loss 0.073595, current_train_items 113760.
I0302 18:59:33.957363 22760421793920 run.py:483] Algo bellman_ford step 3555 current loss 0.011737, current_train_items 113792.
I0302 18:59:33.973436 22760421793920 run.py:483] Algo bellman_ford step 3556 current loss 0.013430, current_train_items 113824.
I0302 18:59:33.996316 22760421793920 run.py:483] Algo bellman_ford step 3557 current loss 0.047222, current_train_items 113856.
I0302 18:59:34.026993 22760421793920 run.py:483] Algo bellman_ford step 3558 current loss 0.100577, current_train_items 113888.
I0302 18:59:34.058748 22760421793920 run.py:483] Algo bellman_ford step 3559 current loss 0.076285, current_train_items 113920.
I0302 18:59:34.076767 22760421793920 run.py:483] Algo bellman_ford step 3560 current loss 0.003194, current_train_items 113952.
I0302 18:59:34.092506 22760421793920 run.py:483] Algo bellman_ford step 3561 current loss 0.029837, current_train_items 113984.
I0302 18:59:34.116639 22760421793920 run.py:483] Algo bellman_ford step 3562 current loss 0.074553, current_train_items 114016.
I0302 18:59:34.145680 22760421793920 run.py:483] Algo bellman_ford step 3563 current loss 0.049862, current_train_items 114048.
I0302 18:59:34.180625 22760421793920 run.py:483] Algo bellman_ford step 3564 current loss 0.105250, current_train_items 114080.
I0302 18:59:34.198740 22760421793920 run.py:483] Algo bellman_ford step 3565 current loss 0.002870, current_train_items 114112.
I0302 18:59:34.214703 22760421793920 run.py:483] Algo bellman_ford step 3566 current loss 0.031321, current_train_items 114144.
I0302 18:59:34.237598 22760421793920 run.py:483] Algo bellman_ford step 3567 current loss 0.066384, current_train_items 114176.
I0302 18:59:34.267274 22760421793920 run.py:483] Algo bellman_ford step 3568 current loss 0.061516, current_train_items 114208.
I0302 18:59:34.299564 22760421793920 run.py:483] Algo bellman_ford step 3569 current loss 0.106419, current_train_items 114240.
I0302 18:59:34.317499 22760421793920 run.py:483] Algo bellman_ford step 3570 current loss 0.005822, current_train_items 114272.
I0302 18:59:34.332774 22760421793920 run.py:483] Algo bellman_ford step 3571 current loss 0.017034, current_train_items 114304.
I0302 18:59:34.355796 22760421793920 run.py:483] Algo bellman_ford step 3572 current loss 0.080564, current_train_items 114336.
I0302 18:59:34.384610 22760421793920 run.py:483] Algo bellman_ford step 3573 current loss 0.063713, current_train_items 114368.
I0302 18:59:34.418269 22760421793920 run.py:483] Algo bellman_ford step 3574 current loss 0.116003, current_train_items 114400.
I0302 18:59:34.436188 22760421793920 run.py:483] Algo bellman_ford step 3575 current loss 0.008332, current_train_items 114432.
I0302 18:59:34.451960 22760421793920 run.py:483] Algo bellman_ford step 3576 current loss 0.025922, current_train_items 114464.
I0302 18:59:34.474717 22760421793920 run.py:483] Algo bellman_ford step 3577 current loss 0.072165, current_train_items 114496.
I0302 18:59:34.504256 22760421793920 run.py:483] Algo bellman_ford step 3578 current loss 0.067833, current_train_items 114528.
I0302 18:59:34.535140 22760421793920 run.py:483] Algo bellman_ford step 3579 current loss 0.084705, current_train_items 114560.
I0302 18:59:34.553730 22760421793920 run.py:483] Algo bellman_ford step 3580 current loss 0.002574, current_train_items 114592.
I0302 18:59:34.569476 22760421793920 run.py:483] Algo bellman_ford step 3581 current loss 0.034507, current_train_items 114624.
I0302 18:59:34.593354 22760421793920 run.py:483] Algo bellman_ford step 3582 current loss 0.058156, current_train_items 114656.
I0302 18:59:34.623641 22760421793920 run.py:483] Algo bellman_ford step 3583 current loss 0.042839, current_train_items 114688.
I0302 18:59:34.656072 22760421793920 run.py:483] Algo bellman_ford step 3584 current loss 0.073049, current_train_items 114720.
I0302 18:59:34.674030 22760421793920 run.py:483] Algo bellman_ford step 3585 current loss 0.004464, current_train_items 114752.
I0302 18:59:34.689800 22760421793920 run.py:483] Algo bellman_ford step 3586 current loss 0.090507, current_train_items 114784.
I0302 18:59:34.712170 22760421793920 run.py:483] Algo bellman_ford step 3587 current loss 0.079460, current_train_items 114816.
I0302 18:59:34.742908 22760421793920 run.py:483] Algo bellman_ford step 3588 current loss 0.081670, current_train_items 114848.
I0302 18:59:34.775894 22760421793920 run.py:483] Algo bellman_ford step 3589 current loss 0.087891, current_train_items 114880.
I0302 18:59:34.793684 22760421793920 run.py:483] Algo bellman_ford step 3590 current loss 0.006273, current_train_items 114912.
I0302 18:59:34.809191 22760421793920 run.py:483] Algo bellman_ford step 3591 current loss 0.023407, current_train_items 114944.
I0302 18:59:34.833424 22760421793920 run.py:483] Algo bellman_ford step 3592 current loss 0.052935, current_train_items 114976.
I0302 18:59:34.864887 22760421793920 run.py:483] Algo bellman_ford step 3593 current loss 0.147304, current_train_items 115008.
I0302 18:59:34.897660 22760421793920 run.py:483] Algo bellman_ford step 3594 current loss 0.104586, current_train_items 115040.
I0302 18:59:34.915773 22760421793920 run.py:483] Algo bellman_ford step 3595 current loss 0.007895, current_train_items 115072.
I0302 18:59:34.931861 22760421793920 run.py:483] Algo bellman_ford step 3596 current loss 0.047071, current_train_items 115104.
I0302 18:59:34.954424 22760421793920 run.py:483] Algo bellman_ford step 3597 current loss 0.048176, current_train_items 115136.
I0302 18:59:34.983044 22760421793920 run.py:483] Algo bellman_ford step 3598 current loss 0.061745, current_train_items 115168.
I0302 18:59:35.015869 22760421793920 run.py:483] Algo bellman_ford step 3599 current loss 0.081312, current_train_items 115200.
I0302 18:59:35.034058 22760421793920 run.py:483] Algo bellman_ford step 3600 current loss 0.002326, current_train_items 115232.
I0302 18:59:35.041464 22760421793920 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 18:59:35.041610 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:59:35.058336 22760421793920 run.py:483] Algo bellman_ford step 3601 current loss 0.018081, current_train_items 115264.
I0302 18:59:35.082119 22760421793920 run.py:483] Algo bellman_ford step 3602 current loss 0.040926, current_train_items 115296.
I0302 18:59:35.111846 22760421793920 run.py:483] Algo bellman_ford step 3603 current loss 0.066993, current_train_items 115328.
I0302 18:59:35.146836 22760421793920 run.py:483] Algo bellman_ford step 3604 current loss 0.117751, current_train_items 115360.
I0302 18:59:35.165179 22760421793920 run.py:483] Algo bellman_ford step 3605 current loss 0.016131, current_train_items 115392.
I0302 18:59:35.181259 22760421793920 run.py:483] Algo bellman_ford step 3606 current loss 0.084129, current_train_items 115424.
I0302 18:59:35.203093 22760421793920 run.py:483] Algo bellman_ford step 3607 current loss 0.063858, current_train_items 115456.
I0302 18:59:35.231840 22760421793920 run.py:483] Algo bellman_ford step 3608 current loss 0.071026, current_train_items 115488.
I0302 18:59:35.262078 22760421793920 run.py:483] Algo bellman_ford step 3609 current loss 0.094994, current_train_items 115520.
I0302 18:59:35.280249 22760421793920 run.py:483] Algo bellman_ford step 3610 current loss 0.006424, current_train_items 115552.
I0302 18:59:35.296180 22760421793920 run.py:483] Algo bellman_ford step 3611 current loss 0.027128, current_train_items 115584.
I0302 18:59:35.319960 22760421793920 run.py:483] Algo bellman_ford step 3612 current loss 0.129699, current_train_items 115616.
I0302 18:59:35.349067 22760421793920 run.py:483] Algo bellman_ford step 3613 current loss 0.115297, current_train_items 115648.
I0302 18:59:35.379360 22760421793920 run.py:483] Algo bellman_ford step 3614 current loss 0.136373, current_train_items 115680.
I0302 18:59:35.397308 22760421793920 run.py:483] Algo bellman_ford step 3615 current loss 0.038472, current_train_items 115712.
I0302 18:59:35.413339 22760421793920 run.py:483] Algo bellman_ford step 3616 current loss 0.029290, current_train_items 115744.
I0302 18:59:35.435664 22760421793920 run.py:483] Algo bellman_ford step 3617 current loss 0.046570, current_train_items 115776.
I0302 18:59:35.465939 22760421793920 run.py:483] Algo bellman_ford step 3618 current loss 0.110173, current_train_items 115808.
I0302 18:59:35.494585 22760421793920 run.py:483] Algo bellman_ford step 3619 current loss 0.092663, current_train_items 115840.
I0302 18:59:35.512598 22760421793920 run.py:483] Algo bellman_ford step 3620 current loss 0.003513, current_train_items 115872.
I0302 18:59:35.528364 22760421793920 run.py:483] Algo bellman_ford step 3621 current loss 0.036332, current_train_items 115904.
I0302 18:59:35.550968 22760421793920 run.py:483] Algo bellman_ford step 3622 current loss 0.086643, current_train_items 115936.
I0302 18:59:35.581224 22760421793920 run.py:483] Algo bellman_ford step 3623 current loss 0.089354, current_train_items 115968.
I0302 18:59:35.612359 22760421793920 run.py:483] Algo bellman_ford step 3624 current loss 0.098006, current_train_items 116000.
I0302 18:59:35.630273 22760421793920 run.py:483] Algo bellman_ford step 3625 current loss 0.010974, current_train_items 116032.
I0302 18:59:35.646087 22760421793920 run.py:483] Algo bellman_ford step 3626 current loss 0.016379, current_train_items 116064.
I0302 18:59:35.669217 22760421793920 run.py:483] Algo bellman_ford step 3627 current loss 0.081007, current_train_items 116096.
I0302 18:59:35.699371 22760421793920 run.py:483] Algo bellman_ford step 3628 current loss 0.115086, current_train_items 116128.
I0302 18:59:35.730364 22760421793920 run.py:483] Algo bellman_ford step 3629 current loss 0.074698, current_train_items 116160.
I0302 18:59:35.748573 22760421793920 run.py:483] Algo bellman_ford step 3630 current loss 0.013719, current_train_items 116192.
I0302 18:59:35.764185 22760421793920 run.py:483] Algo bellman_ford step 3631 current loss 0.043520, current_train_items 116224.
I0302 18:59:35.787257 22760421793920 run.py:483] Algo bellman_ford step 3632 current loss 0.045906, current_train_items 116256.
I0302 18:59:35.817021 22760421793920 run.py:483] Algo bellman_ford step 3633 current loss 0.072688, current_train_items 116288.
I0302 18:59:35.848196 22760421793920 run.py:483] Algo bellman_ford step 3634 current loss 0.103487, current_train_items 116320.
I0302 18:59:35.866186 22760421793920 run.py:483] Algo bellman_ford step 3635 current loss 0.006329, current_train_items 116352.
I0302 18:59:35.881923 22760421793920 run.py:483] Algo bellman_ford step 3636 current loss 0.040247, current_train_items 116384.
I0302 18:59:35.903724 22760421793920 run.py:483] Algo bellman_ford step 3637 current loss 0.057328, current_train_items 116416.
I0302 18:59:35.933802 22760421793920 run.py:483] Algo bellman_ford step 3638 current loss 0.054821, current_train_items 116448.
I0302 18:59:35.964927 22760421793920 run.py:483] Algo bellman_ford step 3639 current loss 0.089278, current_train_items 116480.
I0302 18:59:35.982931 22760421793920 run.py:483] Algo bellman_ford step 3640 current loss 0.011257, current_train_items 116512.
I0302 18:59:35.998238 22760421793920 run.py:483] Algo bellman_ford step 3641 current loss 0.031214, current_train_items 116544.
I0302 18:59:36.021528 22760421793920 run.py:483] Algo bellman_ford step 3642 current loss 0.142542, current_train_items 116576.
I0302 18:59:36.054237 22760421793920 run.py:483] Algo bellman_ford step 3643 current loss 0.126259, current_train_items 116608.
I0302 18:59:36.086786 22760421793920 run.py:483] Algo bellman_ford step 3644 current loss 0.103733, current_train_items 116640.
I0302 18:59:36.104701 22760421793920 run.py:483] Algo bellman_ford step 3645 current loss 0.002366, current_train_items 116672.
I0302 18:59:36.120465 22760421793920 run.py:483] Algo bellman_ford step 3646 current loss 0.041687, current_train_items 116704.
I0302 18:59:36.144793 22760421793920 run.py:483] Algo bellman_ford step 3647 current loss 0.080162, current_train_items 116736.
I0302 18:59:36.174024 22760421793920 run.py:483] Algo bellman_ford step 3648 current loss 0.074629, current_train_items 116768.
I0302 18:59:36.206214 22760421793920 run.py:483] Algo bellman_ford step 3649 current loss 0.079285, current_train_items 116800.
I0302 18:59:36.224038 22760421793920 run.py:483] Algo bellman_ford step 3650 current loss 0.004819, current_train_items 116832.
I0302 18:59:36.231403 22760421793920 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 18:59:36.231511 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:36.248014 22760421793920 run.py:483] Algo bellman_ford step 3651 current loss 0.015765, current_train_items 116864.
I0302 18:59:36.270138 22760421793920 run.py:483] Algo bellman_ford step 3652 current loss 0.069906, current_train_items 116896.
I0302 18:59:36.300145 22760421793920 run.py:483] Algo bellman_ford step 3653 current loss 0.029847, current_train_items 116928.
I0302 18:59:36.331812 22760421793920 run.py:483] Algo bellman_ford step 3654 current loss 0.073190, current_train_items 116960.
I0302 18:59:36.350103 22760421793920 run.py:483] Algo bellman_ford step 3655 current loss 0.028445, current_train_items 116992.
I0302 18:59:36.365683 22760421793920 run.py:483] Algo bellman_ford step 3656 current loss 0.041432, current_train_items 117024.
I0302 18:59:36.389032 22760421793920 run.py:483] Algo bellman_ford step 3657 current loss 0.077187, current_train_items 117056.
I0302 18:59:36.418363 22760421793920 run.py:483] Algo bellman_ford step 3658 current loss 0.105653, current_train_items 117088.
I0302 18:59:36.450556 22760421793920 run.py:483] Algo bellman_ford step 3659 current loss 0.069941, current_train_items 117120.
I0302 18:59:36.469070 22760421793920 run.py:483] Algo bellman_ford step 3660 current loss 0.002149, current_train_items 117152.
I0302 18:59:36.484943 22760421793920 run.py:483] Algo bellman_ford step 3661 current loss 0.050754, current_train_items 117184.
I0302 18:59:36.507413 22760421793920 run.py:483] Algo bellman_ford step 3662 current loss 0.057106, current_train_items 117216.
I0302 18:59:36.538525 22760421793920 run.py:483] Algo bellman_ford step 3663 current loss 0.128329, current_train_items 117248.
I0302 18:59:36.571560 22760421793920 run.py:483] Algo bellman_ford step 3664 current loss 0.097543, current_train_items 117280.
I0302 18:59:36.589625 22760421793920 run.py:483] Algo bellman_ford step 3665 current loss 0.029199, current_train_items 117312.
I0302 18:59:36.604975 22760421793920 run.py:483] Algo bellman_ford step 3666 current loss 0.071672, current_train_items 117344.
I0302 18:59:36.628200 22760421793920 run.py:483] Algo bellman_ford step 3667 current loss 0.085967, current_train_items 117376.
I0302 18:59:36.658368 22760421793920 run.py:483] Algo bellman_ford step 3668 current loss 0.157366, current_train_items 117408.
I0302 18:59:36.691758 22760421793920 run.py:483] Algo bellman_ford step 3669 current loss 0.204879, current_train_items 117440.
I0302 18:59:36.709792 22760421793920 run.py:483] Algo bellman_ford step 3670 current loss 0.006975, current_train_items 117472.
I0302 18:59:36.725604 22760421793920 run.py:483] Algo bellman_ford step 3671 current loss 0.150625, current_train_items 117504.
I0302 18:59:36.748125 22760421793920 run.py:483] Algo bellman_ford step 3672 current loss 0.083830, current_train_items 117536.
I0302 18:59:36.777877 22760421793920 run.py:483] Algo bellman_ford step 3673 current loss 0.109123, current_train_items 117568.
I0302 18:59:36.810640 22760421793920 run.py:483] Algo bellman_ford step 3674 current loss 0.174837, current_train_items 117600.
I0302 18:59:36.828552 22760421793920 run.py:483] Algo bellman_ford step 3675 current loss 0.002060, current_train_items 117632.
I0302 18:59:36.843857 22760421793920 run.py:483] Algo bellman_ford step 3676 current loss 0.052072, current_train_items 117664.
I0302 18:59:36.866571 22760421793920 run.py:483] Algo bellman_ford step 3677 current loss 0.070431, current_train_items 117696.
I0302 18:59:36.896570 22760421793920 run.py:483] Algo bellman_ford step 3678 current loss 0.070187, current_train_items 117728.
I0302 18:59:36.928700 22760421793920 run.py:483] Algo bellman_ford step 3679 current loss 0.118751, current_train_items 117760.
I0302 18:59:36.946761 22760421793920 run.py:483] Algo bellman_ford step 3680 current loss 0.003828, current_train_items 117792.
I0302 18:59:36.962717 22760421793920 run.py:483] Algo bellman_ford step 3681 current loss 0.033459, current_train_items 117824.
I0302 18:59:36.986241 22760421793920 run.py:483] Algo bellman_ford step 3682 current loss 0.054987, current_train_items 117856.
I0302 18:59:37.015480 22760421793920 run.py:483] Algo bellman_ford step 3683 current loss 0.088317, current_train_items 117888.
I0302 18:59:37.047445 22760421793920 run.py:483] Algo bellman_ford step 3684 current loss 0.081616, current_train_items 117920.
I0302 18:59:37.065556 22760421793920 run.py:483] Algo bellman_ford step 3685 current loss 0.005489, current_train_items 117952.
I0302 18:59:37.081231 22760421793920 run.py:483] Algo bellman_ford step 3686 current loss 0.020075, current_train_items 117984.
I0302 18:59:37.103863 22760421793920 run.py:483] Algo bellman_ford step 3687 current loss 0.076040, current_train_items 118016.
I0302 18:59:37.134918 22760421793920 run.py:483] Algo bellman_ford step 3688 current loss 0.123153, current_train_items 118048.
I0302 18:59:37.168538 22760421793920 run.py:483] Algo bellman_ford step 3689 current loss 0.075743, current_train_items 118080.
I0302 18:59:37.186550 22760421793920 run.py:483] Algo bellman_ford step 3690 current loss 0.022678, current_train_items 118112.
I0302 18:59:37.202206 22760421793920 run.py:483] Algo bellman_ford step 3691 current loss 0.038455, current_train_items 118144.
I0302 18:59:37.223649 22760421793920 run.py:483] Algo bellman_ford step 3692 current loss 0.027682, current_train_items 118176.
I0302 18:59:37.254875 22760421793920 run.py:483] Algo bellman_ford step 3693 current loss 0.080693, current_train_items 118208.
I0302 18:59:37.285937 22760421793920 run.py:483] Algo bellman_ford step 3694 current loss 0.087018, current_train_items 118240.
I0302 18:59:37.304083 22760421793920 run.py:483] Algo bellman_ford step 3695 current loss 0.010016, current_train_items 118272.
I0302 18:59:37.320405 22760421793920 run.py:483] Algo bellman_ford step 3696 current loss 0.041553, current_train_items 118304.
I0302 18:59:37.343365 22760421793920 run.py:483] Algo bellman_ford step 3697 current loss 0.142242, current_train_items 118336.
I0302 18:59:37.373834 22760421793920 run.py:483] Algo bellman_ford step 3698 current loss 0.116026, current_train_items 118368.
I0302 18:59:37.404505 22760421793920 run.py:483] Algo bellman_ford step 3699 current loss 0.165534, current_train_items 118400.
I0302 18:59:37.422704 22760421793920 run.py:483] Algo bellman_ford step 3700 current loss 0.007107, current_train_items 118432.
I0302 18:59:37.430024 22760421793920 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 18:59:37.430133 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:59:37.446365 22760421793920 run.py:483] Algo bellman_ford step 3701 current loss 0.014680, current_train_items 118464.
I0302 18:59:37.469730 22760421793920 run.py:483] Algo bellman_ford step 3702 current loss 0.099067, current_train_items 118496.
I0302 18:59:37.501419 22760421793920 run.py:483] Algo bellman_ford step 3703 current loss 0.090544, current_train_items 118528.
I0302 18:59:37.533242 22760421793920 run.py:483] Algo bellman_ford step 3704 current loss 0.089123, current_train_items 118560.
I0302 18:59:37.551506 22760421793920 run.py:483] Algo bellman_ford step 3705 current loss 0.003820, current_train_items 118592.
I0302 18:59:37.567380 22760421793920 run.py:483] Algo bellman_ford step 3706 current loss 0.027179, current_train_items 118624.
I0302 18:59:37.590283 22760421793920 run.py:483] Algo bellman_ford step 3707 current loss 0.066493, current_train_items 118656.
I0302 18:59:37.619259 22760421793920 run.py:483] Algo bellman_ford step 3708 current loss 0.080006, current_train_items 118688.
I0302 18:59:37.651214 22760421793920 run.py:483] Algo bellman_ford step 3709 current loss 0.100840, current_train_items 118720.
I0302 18:59:37.669455 22760421793920 run.py:483] Algo bellman_ford step 3710 current loss 0.014424, current_train_items 118752.
I0302 18:59:37.685337 22760421793920 run.py:483] Algo bellman_ford step 3711 current loss 0.044122, current_train_items 118784.
I0302 18:59:37.708396 22760421793920 run.py:483] Algo bellman_ford step 3712 current loss 0.044535, current_train_items 118816.
I0302 18:59:37.739520 22760421793920 run.py:483] Algo bellman_ford step 3713 current loss 0.071904, current_train_items 118848.
I0302 18:59:37.772022 22760421793920 run.py:483] Algo bellman_ford step 3714 current loss 0.081843, current_train_items 118880.
I0302 18:59:37.789934 22760421793920 run.py:483] Algo bellman_ford step 3715 current loss 0.004904, current_train_items 118912.
I0302 18:59:37.805463 22760421793920 run.py:483] Algo bellman_ford step 3716 current loss 0.012246, current_train_items 118944.
I0302 18:59:37.828109 22760421793920 run.py:483] Algo bellman_ford step 3717 current loss 0.064629, current_train_items 118976.
I0302 18:59:37.858114 22760421793920 run.py:483] Algo bellman_ford step 3718 current loss 0.109824, current_train_items 119008.
I0302 18:59:37.888635 22760421793920 run.py:483] Algo bellman_ford step 3719 current loss 0.069725, current_train_items 119040.
I0302 18:59:37.906780 22760421793920 run.py:483] Algo bellman_ford step 3720 current loss 0.002648, current_train_items 119072.
I0302 18:59:37.922681 22760421793920 run.py:483] Algo bellman_ford step 3721 current loss 0.029570, current_train_items 119104.
I0302 18:59:37.945406 22760421793920 run.py:483] Algo bellman_ford step 3722 current loss 0.027113, current_train_items 119136.
I0302 18:59:37.974764 22760421793920 run.py:483] Algo bellman_ford step 3723 current loss 0.041718, current_train_items 119168.
I0302 18:59:38.006173 22760421793920 run.py:483] Algo bellman_ford step 3724 current loss 0.073381, current_train_items 119200.
I0302 18:59:38.024049 22760421793920 run.py:483] Algo bellman_ford step 3725 current loss 0.001760, current_train_items 119232.
I0302 18:59:38.040169 22760421793920 run.py:483] Algo bellman_ford step 3726 current loss 0.011292, current_train_items 119264.
I0302 18:59:38.063588 22760421793920 run.py:483] Algo bellman_ford step 3727 current loss 0.067490, current_train_items 119296.
I0302 18:59:38.093545 22760421793920 run.py:483] Algo bellman_ford step 3728 current loss 0.076470, current_train_items 119328.
I0302 18:59:38.125207 22760421793920 run.py:483] Algo bellman_ford step 3729 current loss 0.057218, current_train_items 119360.
I0302 18:59:38.143219 22760421793920 run.py:483] Algo bellman_ford step 3730 current loss 0.014403, current_train_items 119392.
I0302 18:59:38.158734 22760421793920 run.py:483] Algo bellman_ford step 3731 current loss 0.032002, current_train_items 119424.
I0302 18:59:38.181795 22760421793920 run.py:483] Algo bellman_ford step 3732 current loss 0.046330, current_train_items 119456.
I0302 18:59:38.209813 22760421793920 run.py:483] Algo bellman_ford step 3733 current loss 0.058449, current_train_items 119488.
I0302 18:59:38.242104 22760421793920 run.py:483] Algo bellman_ford step 3734 current loss 0.103807, current_train_items 119520.
I0302 18:59:38.260227 22760421793920 run.py:483] Algo bellman_ford step 3735 current loss 0.008190, current_train_items 119552.
I0302 18:59:38.275677 22760421793920 run.py:483] Algo bellman_ford step 3736 current loss 0.047814, current_train_items 119584.
I0302 18:59:38.297623 22760421793920 run.py:483] Algo bellman_ford step 3737 current loss 0.020783, current_train_items 119616.
I0302 18:59:38.327656 22760421793920 run.py:483] Algo bellman_ford step 3738 current loss 0.064780, current_train_items 119648.
I0302 18:59:38.359056 22760421793920 run.py:483] Algo bellman_ford step 3739 current loss 0.061315, current_train_items 119680.
I0302 18:59:38.377182 22760421793920 run.py:483] Algo bellman_ford step 3740 current loss 0.025633, current_train_items 119712.
I0302 18:59:38.392658 22760421793920 run.py:483] Algo bellman_ford step 3741 current loss 0.022159, current_train_items 119744.
I0302 18:59:38.414893 22760421793920 run.py:483] Algo bellman_ford step 3742 current loss 0.120006, current_train_items 119776.
I0302 18:59:38.444428 22760421793920 run.py:483] Algo bellman_ford step 3743 current loss 0.210595, current_train_items 119808.
I0302 18:59:38.474220 22760421793920 run.py:483] Algo bellman_ford step 3744 current loss 0.082978, current_train_items 119840.
I0302 18:59:38.492056 22760421793920 run.py:483] Algo bellman_ford step 3745 current loss 0.059117, current_train_items 119872.
I0302 18:59:38.507492 22760421793920 run.py:483] Algo bellman_ford step 3746 current loss 0.027488, current_train_items 119904.
I0302 18:59:38.530789 22760421793920 run.py:483] Algo bellman_ford step 3747 current loss 0.068131, current_train_items 119936.
I0302 18:59:38.561072 22760421793920 run.py:483] Algo bellman_ford step 3748 current loss 0.157072, current_train_items 119968.
I0302 18:59:38.594290 22760421793920 run.py:483] Algo bellman_ford step 3749 current loss 0.216827, current_train_items 120000.
I0302 18:59:38.612415 22760421793920 run.py:483] Algo bellman_ford step 3750 current loss 0.014135, current_train_items 120032.
I0302 18:59:38.620132 22760421793920 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 18:59:38.620279 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:59:38.636932 22760421793920 run.py:483] Algo bellman_ford step 3751 current loss 0.043016, current_train_items 120064.
I0302 18:59:38.660267 22760421793920 run.py:483] Algo bellman_ford step 3752 current loss 0.065757, current_train_items 120096.
I0302 18:59:38.690504 22760421793920 run.py:483] Algo bellman_ford step 3753 current loss 0.117079, current_train_items 120128.
I0302 18:59:38.725017 22760421793920 run.py:483] Algo bellman_ford step 3754 current loss 0.150123, current_train_items 120160.
I0302 18:59:38.743403 22760421793920 run.py:483] Algo bellman_ford step 3755 current loss 0.008896, current_train_items 120192.
I0302 18:59:38.759464 22760421793920 run.py:483] Algo bellman_ford step 3756 current loss 0.026845, current_train_items 120224.
I0302 18:59:38.782676 22760421793920 run.py:483] Algo bellman_ford step 3757 current loss 0.089844, current_train_items 120256.
I0302 18:59:38.812915 22760421793920 run.py:483] Algo bellman_ford step 3758 current loss 0.047187, current_train_items 120288.
I0302 18:59:38.843137 22760421793920 run.py:483] Algo bellman_ford step 3759 current loss 0.074820, current_train_items 120320.
I0302 18:59:38.861757 22760421793920 run.py:483] Algo bellman_ford step 3760 current loss 0.011478, current_train_items 120352.
I0302 18:59:38.877523 22760421793920 run.py:483] Algo bellman_ford step 3761 current loss 0.010389, current_train_items 120384.
I0302 18:59:38.899985 22760421793920 run.py:483] Algo bellman_ford step 3762 current loss 0.041247, current_train_items 120416.
I0302 18:59:38.929704 22760421793920 run.py:483] Algo bellman_ford step 3763 current loss 0.054020, current_train_items 120448.
I0302 18:59:38.962065 22760421793920 run.py:483] Algo bellman_ford step 3764 current loss 0.079199, current_train_items 120480.
I0302 18:59:38.980225 22760421793920 run.py:483] Algo bellman_ford step 3765 current loss 0.010471, current_train_items 120512.
I0302 18:59:38.996445 22760421793920 run.py:483] Algo bellman_ford step 3766 current loss 0.050507, current_train_items 120544.
I0302 18:59:39.019981 22760421793920 run.py:483] Algo bellman_ford step 3767 current loss 0.158083, current_train_items 120576.
I0302 18:59:39.051071 22760421793920 run.py:483] Algo bellman_ford step 3768 current loss 0.083477, current_train_items 120608.
I0302 18:59:39.082110 22760421793920 run.py:483] Algo bellman_ford step 3769 current loss 0.064811, current_train_items 120640.
I0302 18:59:39.100490 22760421793920 run.py:483] Algo bellman_ford step 3770 current loss 0.009367, current_train_items 120672.
I0302 18:59:39.116259 22760421793920 run.py:483] Algo bellman_ford step 3771 current loss 0.059082, current_train_items 120704.
I0302 18:59:39.138585 22760421793920 run.py:483] Algo bellman_ford step 3772 current loss 0.117212, current_train_items 120736.
I0302 18:59:39.167828 22760421793920 run.py:483] Algo bellman_ford step 3773 current loss 0.112719, current_train_items 120768.
I0302 18:59:39.197210 22760421793920 run.py:483] Algo bellman_ford step 3774 current loss 0.099341, current_train_items 120800.
I0302 18:59:39.215613 22760421793920 run.py:483] Algo bellman_ford step 3775 current loss 0.005000, current_train_items 120832.
I0302 18:59:39.231685 22760421793920 run.py:483] Algo bellman_ford step 3776 current loss 0.033984, current_train_items 120864.
I0302 18:59:39.254517 22760421793920 run.py:483] Algo bellman_ford step 3777 current loss 0.059040, current_train_items 120896.
I0302 18:59:39.285486 22760421793920 run.py:483] Algo bellman_ford step 3778 current loss 0.099887, current_train_items 120928.
I0302 18:59:39.317604 22760421793920 run.py:483] Algo bellman_ford step 3779 current loss 0.132092, current_train_items 120960.
I0302 18:59:39.335921 22760421793920 run.py:483] Algo bellman_ford step 3780 current loss 0.002715, current_train_items 120992.
I0302 18:59:39.351783 22760421793920 run.py:483] Algo bellman_ford step 3781 current loss 0.032592, current_train_items 121024.
I0302 18:59:39.374542 22760421793920 run.py:483] Algo bellman_ford step 3782 current loss 0.036347, current_train_items 121056.
I0302 18:59:39.403223 22760421793920 run.py:483] Algo bellman_ford step 3783 current loss 0.069241, current_train_items 121088.
I0302 18:59:39.438179 22760421793920 run.py:483] Algo bellman_ford step 3784 current loss 0.135016, current_train_items 121120.
I0302 18:59:39.456424 22760421793920 run.py:483] Algo bellman_ford step 3785 current loss 0.003783, current_train_items 121152.
I0302 18:59:39.471998 22760421793920 run.py:483] Algo bellman_ford step 3786 current loss 0.022294, current_train_items 121184.
I0302 18:59:39.495421 22760421793920 run.py:483] Algo bellman_ford step 3787 current loss 0.080171, current_train_items 121216.
I0302 18:59:39.525213 22760421793920 run.py:483] Algo bellman_ford step 3788 current loss 0.049121, current_train_items 121248.
I0302 18:59:39.557239 22760421793920 run.py:483] Algo bellman_ford step 3789 current loss 0.133090, current_train_items 121280.
I0302 18:59:39.575493 22760421793920 run.py:483] Algo bellman_ford step 3790 current loss 0.017381, current_train_items 121312.
I0302 18:59:39.591247 22760421793920 run.py:483] Algo bellman_ford step 3791 current loss 0.012346, current_train_items 121344.
I0302 18:59:39.613785 22760421793920 run.py:483] Algo bellman_ford step 3792 current loss 0.053970, current_train_items 121376.
I0302 18:59:39.644680 22760421793920 run.py:483] Algo bellman_ford step 3793 current loss 0.140128, current_train_items 121408.
I0302 18:59:39.676579 22760421793920 run.py:483] Algo bellman_ford step 3794 current loss 0.063633, current_train_items 121440.
I0302 18:59:39.694764 22760421793920 run.py:483] Algo bellman_ford step 3795 current loss 0.012231, current_train_items 121472.
I0302 18:59:39.710807 22760421793920 run.py:483] Algo bellman_ford step 3796 current loss 0.044706, current_train_items 121504.
I0302 18:59:39.734395 22760421793920 run.py:483] Algo bellman_ford step 3797 current loss 0.069339, current_train_items 121536.
I0302 18:59:39.765410 22760421793920 run.py:483] Algo bellman_ford step 3798 current loss 0.066450, current_train_items 121568.
I0302 18:59:39.796351 22760421793920 run.py:483] Algo bellman_ford step 3799 current loss 0.066963, current_train_items 121600.
I0302 18:59:39.814807 22760421793920 run.py:483] Algo bellman_ford step 3800 current loss 0.001797, current_train_items 121632.
I0302 18:59:39.822197 22760421793920 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 18:59:39.822308 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0302 18:59:39.838631 22760421793920 run.py:483] Algo bellman_ford step 3801 current loss 0.041412, current_train_items 121664.
I0302 18:59:39.862700 22760421793920 run.py:483] Algo bellman_ford step 3802 current loss 0.079098, current_train_items 121696.
I0302 18:59:39.892630 22760421793920 run.py:483] Algo bellman_ford step 3803 current loss 0.078191, current_train_items 121728.
I0302 18:59:39.922860 22760421793920 run.py:483] Algo bellman_ford step 3804 current loss 0.071712, current_train_items 121760.
I0302 18:59:39.941239 22760421793920 run.py:483] Algo bellman_ford step 3805 current loss 0.007288, current_train_items 121792.
I0302 18:59:39.956947 22760421793920 run.py:483] Algo bellman_ford step 3806 current loss 0.047241, current_train_items 121824.
I0302 18:59:39.979685 22760421793920 run.py:483] Algo bellman_ford step 3807 current loss 0.045923, current_train_items 121856.
I0302 18:59:40.009609 22760421793920 run.py:483] Algo bellman_ford step 3808 current loss 0.074931, current_train_items 121888.
I0302 18:59:40.043310 22760421793920 run.py:483] Algo bellman_ford step 3809 current loss 0.066076, current_train_items 121920.
I0302 18:59:40.061307 22760421793920 run.py:483] Algo bellman_ford step 3810 current loss 0.032562, current_train_items 121952.
I0302 18:59:40.077349 22760421793920 run.py:483] Algo bellman_ford step 3811 current loss 0.020860, current_train_items 121984.
I0302 18:59:40.100395 22760421793920 run.py:483] Algo bellman_ford step 3812 current loss 0.068240, current_train_items 122016.
I0302 18:59:40.132307 22760421793920 run.py:483] Algo bellman_ford step 3813 current loss 0.107682, current_train_items 122048.
I0302 18:59:40.165760 22760421793920 run.py:483] Algo bellman_ford step 3814 current loss 0.114150, current_train_items 122080.
I0302 18:59:40.184150 22760421793920 run.py:483] Algo bellman_ford step 3815 current loss 0.007586, current_train_items 122112.
I0302 18:59:40.200235 22760421793920 run.py:483] Algo bellman_ford step 3816 current loss 0.022695, current_train_items 122144.
I0302 18:59:40.223186 22760421793920 run.py:483] Algo bellman_ford step 3817 current loss 0.067894, current_train_items 122176.
I0302 18:59:40.253486 22760421793920 run.py:483] Algo bellman_ford step 3818 current loss 0.103830, current_train_items 122208.
I0302 18:59:40.285452 22760421793920 run.py:483] Algo bellman_ford step 3819 current loss 0.188190, current_train_items 122240.
I0302 18:59:40.303640 22760421793920 run.py:483] Algo bellman_ford step 3820 current loss 0.003004, current_train_items 122272.
I0302 18:59:40.318940 22760421793920 run.py:483] Algo bellman_ford step 3821 current loss 0.010480, current_train_items 122304.
I0302 18:59:40.341780 22760421793920 run.py:483] Algo bellman_ford step 3822 current loss 0.050951, current_train_items 122336.
I0302 18:59:40.372022 22760421793920 run.py:483] Algo bellman_ford step 3823 current loss 0.060981, current_train_items 122368.
I0302 18:59:40.404950 22760421793920 run.py:483] Algo bellman_ford step 3824 current loss 0.069028, current_train_items 122400.
I0302 18:59:40.423041 22760421793920 run.py:483] Algo bellman_ford step 3825 current loss 0.008139, current_train_items 122432.
I0302 18:59:40.438415 22760421793920 run.py:483] Algo bellman_ford step 3826 current loss 0.020172, current_train_items 122464.
I0302 18:59:40.461580 22760421793920 run.py:483] Algo bellman_ford step 3827 current loss 0.059661, current_train_items 122496.
I0302 18:59:40.492353 22760421793920 run.py:483] Algo bellman_ford step 3828 current loss 0.081784, current_train_items 122528.
I0302 18:59:40.522873 22760421793920 run.py:483] Algo bellman_ford step 3829 current loss 0.089264, current_train_items 122560.
I0302 18:59:40.540799 22760421793920 run.py:483] Algo bellman_ford step 3830 current loss 0.002023, current_train_items 122592.
I0302 18:59:40.556527 22760421793920 run.py:483] Algo bellman_ford step 3831 current loss 0.046449, current_train_items 122624.
I0302 18:59:40.578406 22760421793920 run.py:483] Algo bellman_ford step 3832 current loss 0.063793, current_train_items 122656.
I0302 18:59:40.609297 22760421793920 run.py:483] Algo bellman_ford step 3833 current loss 0.080527, current_train_items 122688.
I0302 18:59:40.640295 22760421793920 run.py:483] Algo bellman_ford step 3834 current loss 0.088556, current_train_items 122720.
I0302 18:59:40.658163 22760421793920 run.py:483] Algo bellman_ford step 3835 current loss 0.024853, current_train_items 122752.
I0302 18:59:40.673290 22760421793920 run.py:483] Algo bellman_ford step 3836 current loss 0.020917, current_train_items 122784.
I0302 18:59:40.696978 22760421793920 run.py:483] Algo bellman_ford step 3837 current loss 0.069681, current_train_items 122816.
I0302 18:59:40.727108 22760421793920 run.py:483] Algo bellman_ford step 3838 current loss 0.097403, current_train_items 122848.
I0302 18:59:40.758573 22760421793920 run.py:483] Algo bellman_ford step 3839 current loss 0.075960, current_train_items 122880.
I0302 18:59:40.776508 22760421793920 run.py:483] Algo bellman_ford step 3840 current loss 0.009578, current_train_items 122912.
I0302 18:59:40.792187 22760421793920 run.py:483] Algo bellman_ford step 3841 current loss 0.027267, current_train_items 122944.
I0302 18:59:40.814094 22760421793920 run.py:483] Algo bellman_ford step 3842 current loss 0.058113, current_train_items 122976.
I0302 18:59:40.843844 22760421793920 run.py:483] Algo bellman_ford step 3843 current loss 0.047053, current_train_items 123008.
I0302 18:59:40.876907 22760421793920 run.py:483] Algo bellman_ford step 3844 current loss 0.075225, current_train_items 123040.
I0302 18:59:40.895241 22760421793920 run.py:483] Algo bellman_ford step 3845 current loss 0.002976, current_train_items 123072.
I0302 18:59:40.910867 22760421793920 run.py:483] Algo bellman_ford step 3846 current loss 0.019812, current_train_items 123104.
I0302 18:59:40.935084 22760421793920 run.py:483] Algo bellman_ford step 3847 current loss 0.093783, current_train_items 123136.
I0302 18:59:40.964282 22760421793920 run.py:483] Algo bellman_ford step 3848 current loss 0.058100, current_train_items 123168.
I0302 18:59:40.996047 22760421793920 run.py:483] Algo bellman_ford step 3849 current loss 0.098523, current_train_items 123200.
I0302 18:59:41.014302 22760421793920 run.py:483] Algo bellman_ford step 3850 current loss 0.005028, current_train_items 123232.
I0302 18:59:41.021987 22760421793920 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 18:59:41.022095 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:59:41.038041 22760421793920 run.py:483] Algo bellman_ford step 3851 current loss 0.010533, current_train_items 123264.
I0302 18:59:41.060380 22760421793920 run.py:483] Algo bellman_ford step 3852 current loss 0.030924, current_train_items 123296.
I0302 18:59:41.091563 22760421793920 run.py:483] Algo bellman_ford step 3853 current loss 0.073510, current_train_items 123328.
I0302 18:59:41.125376 22760421793920 run.py:483] Algo bellman_ford step 3854 current loss 0.109795, current_train_items 123360.
I0302 18:59:41.144039 22760421793920 run.py:483] Algo bellman_ford step 3855 current loss 0.004564, current_train_items 123392.
I0302 18:59:41.159928 22760421793920 run.py:483] Algo bellman_ford step 3856 current loss 0.024780, current_train_items 123424.
I0302 18:59:41.181807 22760421793920 run.py:483] Algo bellman_ford step 3857 current loss 0.030462, current_train_items 123456.
I0302 18:59:41.212078 22760421793920 run.py:483] Algo bellman_ford step 3858 current loss 0.097697, current_train_items 123488.
I0302 18:59:41.246439 22760421793920 run.py:483] Algo bellman_ford step 3859 current loss 0.113885, current_train_items 123520.
I0302 18:59:41.264463 22760421793920 run.py:483] Algo bellman_ford step 3860 current loss 0.006454, current_train_items 123552.
I0302 18:59:41.280370 22760421793920 run.py:483] Algo bellman_ford step 3861 current loss 0.037144, current_train_items 123584.
I0302 18:59:41.303035 22760421793920 run.py:483] Algo bellman_ford step 3862 current loss 0.050031, current_train_items 123616.
I0302 18:59:41.332800 22760421793920 run.py:483] Algo bellman_ford step 3863 current loss 0.118634, current_train_items 123648.
I0302 18:59:41.367518 22760421793920 run.py:483] Algo bellman_ford step 3864 current loss 0.129234, current_train_items 123680.
I0302 18:59:41.385542 22760421793920 run.py:483] Algo bellman_ford step 3865 current loss 0.006737, current_train_items 123712.
I0302 18:59:41.401111 22760421793920 run.py:483] Algo bellman_ford step 3866 current loss 0.024406, current_train_items 123744.
I0302 18:59:41.423978 22760421793920 run.py:483] Algo bellman_ford step 3867 current loss 0.033174, current_train_items 123776.
I0302 18:59:41.453464 22760421793920 run.py:483] Algo bellman_ford step 3868 current loss 0.031364, current_train_items 123808.
I0302 18:59:41.485235 22760421793920 run.py:483] Algo bellman_ford step 3869 current loss 0.083212, current_train_items 123840.
I0302 18:59:41.503794 22760421793920 run.py:483] Algo bellman_ford step 3870 current loss 0.004876, current_train_items 123872.
I0302 18:59:41.519098 22760421793920 run.py:483] Algo bellman_ford step 3871 current loss 0.014329, current_train_items 123904.
I0302 18:59:41.542792 22760421793920 run.py:483] Algo bellman_ford step 3872 current loss 0.034631, current_train_items 123936.
I0302 18:59:41.571640 22760421793920 run.py:483] Algo bellman_ford step 3873 current loss 0.036904, current_train_items 123968.
I0302 18:59:41.602813 22760421793920 run.py:483] Algo bellman_ford step 3874 current loss 0.054137, current_train_items 124000.
I0302 18:59:41.621158 22760421793920 run.py:483] Algo bellman_ford step 3875 current loss 0.003086, current_train_items 124032.
I0302 18:59:41.637176 22760421793920 run.py:483] Algo bellman_ford step 3876 current loss 0.047828, current_train_items 124064.
I0302 18:59:41.660663 22760421793920 run.py:483] Algo bellman_ford step 3877 current loss 0.041621, current_train_items 124096.
I0302 18:59:41.690863 22760421793920 run.py:483] Algo bellman_ford step 3878 current loss 0.047251, current_train_items 124128.
I0302 18:59:41.722916 22760421793920 run.py:483] Algo bellman_ford step 3879 current loss 0.094668, current_train_items 124160.
I0302 18:59:41.741426 22760421793920 run.py:483] Algo bellman_ford step 3880 current loss 0.046645, current_train_items 124192.
I0302 18:59:41.757076 22760421793920 run.py:483] Algo bellman_ford step 3881 current loss 0.033136, current_train_items 124224.
I0302 18:59:41.780179 22760421793920 run.py:483] Algo bellman_ford step 3882 current loss 0.042637, current_train_items 124256.
I0302 18:59:41.810147 22760421793920 run.py:483] Algo bellman_ford step 3883 current loss 0.073267, current_train_items 124288.
I0302 18:59:41.841155 22760421793920 run.py:483] Algo bellman_ford step 3884 current loss 0.119063, current_train_items 124320.
I0302 18:59:41.859547 22760421793920 run.py:483] Algo bellman_ford step 3885 current loss 0.012152, current_train_items 124352.
I0302 18:59:41.875633 22760421793920 run.py:483] Algo bellman_ford step 3886 current loss 0.024210, current_train_items 124384.
I0302 18:59:41.898625 22760421793920 run.py:483] Algo bellman_ford step 3887 current loss 0.087226, current_train_items 124416.
I0302 18:59:41.928574 22760421793920 run.py:483] Algo bellman_ford step 3888 current loss 0.109317, current_train_items 124448.
I0302 18:59:41.961698 22760421793920 run.py:483] Algo bellman_ford step 3889 current loss 0.099922, current_train_items 124480.
I0302 18:59:41.979712 22760421793920 run.py:483] Algo bellman_ford step 3890 current loss 0.006325, current_train_items 124512.
I0302 18:59:41.995475 22760421793920 run.py:483] Algo bellman_ford step 3891 current loss 0.037443, current_train_items 124544.
I0302 18:59:42.019182 22760421793920 run.py:483] Algo bellman_ford step 3892 current loss 0.122966, current_train_items 124576.
I0302 18:59:42.047387 22760421793920 run.py:483] Algo bellman_ford step 3893 current loss 0.071501, current_train_items 124608.
I0302 18:59:42.080245 22760421793920 run.py:483] Algo bellman_ford step 3894 current loss 0.110310, current_train_items 124640.
I0302 18:59:42.098640 22760421793920 run.py:483] Algo bellman_ford step 3895 current loss 0.002329, current_train_items 124672.
I0302 18:59:42.114638 22760421793920 run.py:483] Algo bellman_ford step 3896 current loss 0.024053, current_train_items 124704.
I0302 18:59:42.138653 22760421793920 run.py:483] Algo bellman_ford step 3897 current loss 0.048774, current_train_items 124736.
I0302 18:59:42.169018 22760421793920 run.py:483] Algo bellman_ford step 3898 current loss 0.052718, current_train_items 124768.
I0302 18:59:42.200639 22760421793920 run.py:483] Algo bellman_ford step 3899 current loss 0.075964, current_train_items 124800.
I0302 18:59:42.219344 22760421793920 run.py:483] Algo bellman_ford step 3900 current loss 0.003152, current_train_items 124832.
I0302 18:59:42.226686 22760421793920 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 18:59:42.226793 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 18:59:42.243328 22760421793920 run.py:483] Algo bellman_ford step 3901 current loss 0.060138, current_train_items 124864.
I0302 18:59:42.266281 22760421793920 run.py:483] Algo bellman_ford step 3902 current loss 0.080446, current_train_items 124896.
I0302 18:59:42.296566 22760421793920 run.py:483] Algo bellman_ford step 3903 current loss 0.163671, current_train_items 124928.
I0302 18:59:42.328649 22760421793920 run.py:483] Algo bellman_ford step 3904 current loss 0.109380, current_train_items 124960.
I0302 18:59:42.347058 22760421793920 run.py:483] Algo bellman_ford step 3905 current loss 0.005239, current_train_items 124992.
I0302 18:59:42.362522 22760421793920 run.py:483] Algo bellman_ford step 3906 current loss 0.016611, current_train_items 125024.
I0302 18:59:42.384305 22760421793920 run.py:483] Algo bellman_ford step 3907 current loss 0.031284, current_train_items 125056.
I0302 18:59:42.414361 22760421793920 run.py:483] Algo bellman_ford step 3908 current loss 0.060067, current_train_items 125088.
I0302 18:59:42.446544 22760421793920 run.py:483] Algo bellman_ford step 3909 current loss 0.075120, current_train_items 125120.
I0302 18:59:42.464531 22760421793920 run.py:483] Algo bellman_ford step 3910 current loss 0.003858, current_train_items 125152.
I0302 18:59:42.480002 22760421793920 run.py:483] Algo bellman_ford step 3911 current loss 0.014937, current_train_items 125184.
I0302 18:59:42.502995 22760421793920 run.py:483] Algo bellman_ford step 3912 current loss 0.039545, current_train_items 125216.
I0302 18:59:42.532382 22760421793920 run.py:483] Algo bellman_ford step 3913 current loss 0.044696, current_train_items 125248.
I0302 18:59:42.561235 22760421793920 run.py:483] Algo bellman_ford step 3914 current loss 0.080140, current_train_items 125280.
I0302 18:59:42.579179 22760421793920 run.py:483] Algo bellman_ford step 3915 current loss 0.004123, current_train_items 125312.
I0302 18:59:42.594707 22760421793920 run.py:483] Algo bellman_ford step 3916 current loss 0.005141, current_train_items 125344.
I0302 18:59:42.618743 22760421793920 run.py:483] Algo bellman_ford step 3917 current loss 0.076787, current_train_items 125376.
I0302 18:59:42.647967 22760421793920 run.py:483] Algo bellman_ford step 3918 current loss 0.063832, current_train_items 125408.
I0302 18:59:42.679506 22760421793920 run.py:483] Algo bellman_ford step 3919 current loss 0.046064, current_train_items 125440.
I0302 18:59:42.697998 22760421793920 run.py:483] Algo bellman_ford step 3920 current loss 0.001575, current_train_items 125472.
I0302 18:59:42.713804 22760421793920 run.py:483] Algo bellman_ford step 3921 current loss 0.018417, current_train_items 125504.
I0302 18:59:42.737716 22760421793920 run.py:483] Algo bellman_ford step 3922 current loss 0.036260, current_train_items 125536.
I0302 18:59:42.768235 22760421793920 run.py:483] Algo bellman_ford step 3923 current loss 0.114118, current_train_items 125568.
I0302 18:59:42.802590 22760421793920 run.py:483] Algo bellman_ford step 3924 current loss 0.127586, current_train_items 125600.
I0302 18:59:42.821007 22760421793920 run.py:483] Algo bellman_ford step 3925 current loss 0.007615, current_train_items 125632.
I0302 18:59:42.836794 22760421793920 run.py:483] Algo bellman_ford step 3926 current loss 0.074722, current_train_items 125664.
I0302 18:59:42.859811 22760421793920 run.py:483] Algo bellman_ford step 3927 current loss 0.109278, current_train_items 125696.
I0302 18:59:42.889266 22760421793920 run.py:483] Algo bellman_ford step 3928 current loss 0.086351, current_train_items 125728.
I0302 18:59:42.921031 22760421793920 run.py:483] Algo bellman_ford step 3929 current loss 0.073615, current_train_items 125760.
I0302 18:59:42.939056 22760421793920 run.py:483] Algo bellman_ford step 3930 current loss 0.002559, current_train_items 125792.
I0302 18:59:42.954545 22760421793920 run.py:483] Algo bellman_ford step 3931 current loss 0.074287, current_train_items 125824.
I0302 18:59:42.977230 22760421793920 run.py:483] Algo bellman_ford step 3932 current loss 0.062387, current_train_items 125856.
I0302 18:59:43.008304 22760421793920 run.py:483] Algo bellman_ford step 3933 current loss 0.110786, current_train_items 125888.
I0302 18:59:43.040418 22760421793920 run.py:483] Algo bellman_ford step 3934 current loss 0.092203, current_train_items 125920.
I0302 18:59:43.058528 22760421793920 run.py:483] Algo bellman_ford step 3935 current loss 0.033823, current_train_items 125952.
I0302 18:59:43.074464 22760421793920 run.py:483] Algo bellman_ford step 3936 current loss 0.033206, current_train_items 125984.
I0302 18:59:43.096989 22760421793920 run.py:483] Algo bellman_ford step 3937 current loss 0.216104, current_train_items 126016.
I0302 18:59:43.127136 22760421793920 run.py:483] Algo bellman_ford step 3938 current loss 0.210325, current_train_items 126048.
I0302 18:59:43.160021 22760421793920 run.py:483] Algo bellman_ford step 3939 current loss 0.187389, current_train_items 126080.
I0302 18:59:43.177989 22760421793920 run.py:483] Algo bellman_ford step 3940 current loss 0.006812, current_train_items 126112.
I0302 18:59:43.193537 22760421793920 run.py:483] Algo bellman_ford step 3941 current loss 0.029261, current_train_items 126144.
I0302 18:59:43.216281 22760421793920 run.py:483] Algo bellman_ford step 3942 current loss 0.061311, current_train_items 126176.
I0302 18:59:43.245031 22760421793920 run.py:483] Algo bellman_ford step 3943 current loss 0.050110, current_train_items 126208.
I0302 18:59:43.277983 22760421793920 run.py:483] Algo bellman_ford step 3944 current loss 0.092773, current_train_items 126240.
I0302 18:59:43.295934 22760421793920 run.py:483] Algo bellman_ford step 3945 current loss 0.065708, current_train_items 126272.
I0302 18:59:43.311868 22760421793920 run.py:483] Algo bellman_ford step 3946 current loss 0.021060, current_train_items 126304.
I0302 18:59:43.334815 22760421793920 run.py:483] Algo bellman_ford step 3947 current loss 0.059778, current_train_items 126336.
I0302 18:59:43.362792 22760421793920 run.py:483] Algo bellman_ford step 3948 current loss 0.075310, current_train_items 126368.
I0302 18:59:43.395203 22760421793920 run.py:483] Algo bellman_ford step 3949 current loss 0.048893, current_train_items 126400.
I0302 18:59:43.413261 22760421793920 run.py:483] Algo bellman_ford step 3950 current loss 0.002541, current_train_items 126432.
I0302 18:59:43.420542 22760421793920 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 18:59:43.420653 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.989, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 18:59:43.450135 22760421793920 run.py:483] Algo bellman_ford step 3951 current loss 0.016048, current_train_items 126464.
I0302 18:59:43.473773 22760421793920 run.py:483] Algo bellman_ford step 3952 current loss 0.058321, current_train_items 126496.
I0302 18:59:43.503654 22760421793920 run.py:483] Algo bellman_ford step 3953 current loss 0.048092, current_train_items 126528.
I0302 18:59:43.534756 22760421793920 run.py:483] Algo bellman_ford step 3954 current loss 0.046932, current_train_items 126560.
I0302 18:59:43.553580 22760421793920 run.py:483] Algo bellman_ford step 3955 current loss 0.002963, current_train_items 126592.
I0302 18:59:43.569594 22760421793920 run.py:483] Algo bellman_ford step 3956 current loss 0.049236, current_train_items 126624.
I0302 18:59:43.592830 22760421793920 run.py:483] Algo bellman_ford step 3957 current loss 0.037022, current_train_items 126656.
I0302 18:59:43.623978 22760421793920 run.py:483] Algo bellman_ford step 3958 current loss 0.081101, current_train_items 126688.
I0302 18:59:43.657857 22760421793920 run.py:483] Algo bellman_ford step 3959 current loss 0.070436, current_train_items 126720.
I0302 18:59:43.676047 22760421793920 run.py:483] Algo bellman_ford step 3960 current loss 0.011125, current_train_items 126752.
I0302 18:59:43.691486 22760421793920 run.py:483] Algo bellman_ford step 3961 current loss 0.014737, current_train_items 126784.
I0302 18:59:43.713431 22760421793920 run.py:483] Algo bellman_ford step 3962 current loss 0.029917, current_train_items 126816.
I0302 18:59:43.743814 22760421793920 run.py:483] Algo bellman_ford step 3963 current loss 0.096231, current_train_items 126848.
I0302 18:59:43.776502 22760421793920 run.py:483] Algo bellman_ford step 3964 current loss 0.091969, current_train_items 126880.
I0302 18:59:43.794928 22760421793920 run.py:483] Algo bellman_ford step 3965 current loss 0.008045, current_train_items 126912.
I0302 18:59:43.810671 22760421793920 run.py:483] Algo bellman_ford step 3966 current loss 0.045559, current_train_items 126944.
I0302 18:59:43.835174 22760421793920 run.py:483] Algo bellman_ford step 3967 current loss 0.088870, current_train_items 126976.
I0302 18:59:43.865416 22760421793920 run.py:483] Algo bellman_ford step 3968 current loss 0.058460, current_train_items 127008.
I0302 18:59:43.896788 22760421793920 run.py:483] Algo bellman_ford step 3969 current loss 0.065116, current_train_items 127040.
I0302 18:59:43.915272 22760421793920 run.py:483] Algo bellman_ford step 3970 current loss 0.003022, current_train_items 127072.
I0302 18:59:43.930838 22760421793920 run.py:483] Algo bellman_ford step 3971 current loss 0.024773, current_train_items 127104.
I0302 18:59:43.954840 22760421793920 run.py:483] Algo bellman_ford step 3972 current loss 0.067910, current_train_items 127136.
I0302 18:59:43.985482 22760421793920 run.py:483] Algo bellman_ford step 3973 current loss 0.073714, current_train_items 127168.
I0302 18:59:44.015995 22760421793920 run.py:483] Algo bellman_ford step 3974 current loss 0.097467, current_train_items 127200.
I0302 18:59:44.034503 22760421793920 run.py:483] Algo bellman_ford step 3975 current loss 0.002515, current_train_items 127232.
I0302 18:59:44.050592 22760421793920 run.py:483] Algo bellman_ford step 3976 current loss 0.076309, current_train_items 127264.
I0302 18:59:44.073392 22760421793920 run.py:483] Algo bellman_ford step 3977 current loss 0.050995, current_train_items 127296.
I0302 18:59:44.104408 22760421793920 run.py:483] Algo bellman_ford step 3978 current loss 0.076395, current_train_items 127328.
I0302 18:59:44.136206 22760421793920 run.py:483] Algo bellman_ford step 3979 current loss 0.069138, current_train_items 127360.
I0302 18:59:44.154536 22760421793920 run.py:483] Algo bellman_ford step 3980 current loss 0.005740, current_train_items 127392.
I0302 18:59:44.170168 22760421793920 run.py:483] Algo bellman_ford step 3981 current loss 0.054569, current_train_items 127424.
I0302 18:59:44.192942 22760421793920 run.py:483] Algo bellman_ford step 3982 current loss 0.055593, current_train_items 127456.
I0302 18:59:44.223538 22760421793920 run.py:483] Algo bellman_ford step 3983 current loss 0.155662, current_train_items 127488.
I0302 18:59:44.254557 22760421793920 run.py:483] Algo bellman_ford step 3984 current loss 0.111996, current_train_items 127520.
I0302 18:59:44.273084 22760421793920 run.py:483] Algo bellman_ford step 3985 current loss 0.021593, current_train_items 127552.
I0302 18:59:44.288911 22760421793920 run.py:483] Algo bellman_ford step 3986 current loss 0.097264, current_train_items 127584.
I0302 18:59:44.311701 22760421793920 run.py:483] Algo bellman_ford step 3987 current loss 0.111915, current_train_items 127616.
I0302 18:59:44.342065 22760421793920 run.py:483] Algo bellman_ford step 3988 current loss 0.428986, current_train_items 127648.
I0302 18:59:44.374528 22760421793920 run.py:483] Algo bellman_ford step 3989 current loss 0.229095, current_train_items 127680.
I0302 18:59:44.393040 22760421793920 run.py:483] Algo bellman_ford step 3990 current loss 0.012515, current_train_items 127712.
I0302 18:59:44.408968 22760421793920 run.py:483] Algo bellman_ford step 3991 current loss 0.046779, current_train_items 127744.
I0302 18:59:44.431338 22760421793920 run.py:483] Algo bellman_ford step 3992 current loss 0.041024, current_train_items 127776.
I0302 18:59:44.461125 22760421793920 run.py:483] Algo bellman_ford step 3993 current loss 0.068312, current_train_items 127808.
I0302 18:59:44.493520 22760421793920 run.py:483] Algo bellman_ford step 3994 current loss 0.128232, current_train_items 127840.
I0302 18:59:44.511846 22760421793920 run.py:483] Algo bellman_ford step 3995 current loss 0.004332, current_train_items 127872.
I0302 18:59:44.527712 22760421793920 run.py:483] Algo bellman_ford step 3996 current loss 0.044662, current_train_items 127904.
I0302 18:59:44.550977 22760421793920 run.py:483] Algo bellman_ford step 3997 current loss 0.057362, current_train_items 127936.
I0302 18:59:44.580481 22760421793920 run.py:483] Algo bellman_ford step 3998 current loss 0.066639, current_train_items 127968.
I0302 18:59:44.613462 22760421793920 run.py:483] Algo bellman_ford step 3999 current loss 0.090582, current_train_items 128000.
I0302 18:59:44.631950 22760421793920 run.py:483] Algo bellman_ford step 4000 current loss 0.012198, current_train_items 128032.
I0302 18:59:44.639428 22760421793920 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 18:59:44.639537 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:59:44.655800 22760421793920 run.py:483] Algo bellman_ford step 4001 current loss 0.018082, current_train_items 128064.
I0302 18:59:44.679292 22760421793920 run.py:483] Algo bellman_ford step 4002 current loss 0.047028, current_train_items 128096.
I0302 18:59:44.709149 22760421793920 run.py:483] Algo bellman_ford step 4003 current loss 0.098896, current_train_items 128128.
I0302 18:59:44.740970 22760421793920 run.py:483] Algo bellman_ford step 4004 current loss 0.133763, current_train_items 128160.
I0302 18:59:44.759725 22760421793920 run.py:483] Algo bellman_ford step 4005 current loss 0.001944, current_train_items 128192.
I0302 18:59:44.775357 22760421793920 run.py:483] Algo bellman_ford step 4006 current loss 0.023022, current_train_items 128224.
I0302 18:59:44.798663 22760421793920 run.py:483] Algo bellman_ford step 4007 current loss 0.060768, current_train_items 128256.
I0302 18:59:44.829323 22760421793920 run.py:483] Algo bellman_ford step 4008 current loss 0.078769, current_train_items 128288.
I0302 18:59:44.859973 22760421793920 run.py:483] Algo bellman_ford step 4009 current loss 0.113910, current_train_items 128320.
I0302 18:59:44.878620 22760421793920 run.py:483] Algo bellman_ford step 4010 current loss 0.002734, current_train_items 128352.
I0302 18:59:44.894282 22760421793920 run.py:483] Algo bellman_ford step 4011 current loss 0.024226, current_train_items 128384.
I0302 18:59:44.916168 22760421793920 run.py:483] Algo bellman_ford step 4012 current loss 0.027207, current_train_items 128416.
I0302 18:59:44.946582 22760421793920 run.py:483] Algo bellman_ford step 4013 current loss 0.052199, current_train_items 128448.
I0302 18:59:44.978770 22760421793920 run.py:483] Algo bellman_ford step 4014 current loss 0.074512, current_train_items 128480.
I0302 18:59:44.997358 22760421793920 run.py:483] Algo bellman_ford step 4015 current loss 0.001343, current_train_items 128512.
I0302 18:59:45.013035 22760421793920 run.py:483] Algo bellman_ford step 4016 current loss 0.039901, current_train_items 128544.
I0302 18:59:45.036229 22760421793920 run.py:483] Algo bellman_ford step 4017 current loss 0.037392, current_train_items 128576.
I0302 18:59:45.066486 22760421793920 run.py:483] Algo bellman_ford step 4018 current loss 0.056850, current_train_items 128608.
I0302 18:59:45.097749 22760421793920 run.py:483] Algo bellman_ford step 4019 current loss 0.060562, current_train_items 128640.
I0302 18:59:45.115829 22760421793920 run.py:483] Algo bellman_ford step 4020 current loss 0.002997, current_train_items 128672.
I0302 18:59:45.131399 22760421793920 run.py:483] Algo bellman_ford step 4021 current loss 0.027273, current_train_items 128704.
I0302 18:59:45.154867 22760421793920 run.py:483] Algo bellman_ford step 4022 current loss 0.067775, current_train_items 128736.
I0302 18:59:45.184069 22760421793920 run.py:483] Algo bellman_ford step 4023 current loss 0.038781, current_train_items 128768.
I0302 18:59:45.218641 22760421793920 run.py:483] Algo bellman_ford step 4024 current loss 0.128619, current_train_items 128800.
I0302 18:59:45.237014 22760421793920 run.py:483] Algo bellman_ford step 4025 current loss 0.004220, current_train_items 128832.
I0302 18:59:45.252862 22760421793920 run.py:483] Algo bellman_ford step 4026 current loss 0.021854, current_train_items 128864.
I0302 18:59:45.275369 22760421793920 run.py:483] Algo bellman_ford step 4027 current loss 0.050857, current_train_items 128896.
I0302 18:59:45.304528 22760421793920 run.py:483] Algo bellman_ford step 4028 current loss 0.052193, current_train_items 128928.
I0302 18:59:45.336051 22760421793920 run.py:483] Algo bellman_ford step 4029 current loss 0.086401, current_train_items 128960.
I0302 18:59:45.354089 22760421793920 run.py:483] Algo bellman_ford step 4030 current loss 0.004200, current_train_items 128992.
I0302 18:59:45.369482 22760421793920 run.py:483] Algo bellman_ford step 4031 current loss 0.041694, current_train_items 129024.
I0302 18:59:45.392792 22760421793920 run.py:483] Algo bellman_ford step 4032 current loss 0.039274, current_train_items 129056.
I0302 18:59:45.423444 22760421793920 run.py:483] Algo bellman_ford step 4033 current loss 0.072362, current_train_items 129088.
I0302 18:59:45.454725 22760421793920 run.py:483] Algo bellman_ford step 4034 current loss 0.064451, current_train_items 129120.
I0302 18:59:45.473165 22760421793920 run.py:483] Algo bellman_ford step 4035 current loss 0.001430, current_train_items 129152.
I0302 18:59:45.488680 22760421793920 run.py:483] Algo bellman_ford step 4036 current loss 0.022480, current_train_items 129184.
I0302 18:59:45.511636 22760421793920 run.py:483] Algo bellman_ford step 4037 current loss 0.053929, current_train_items 129216.
I0302 18:59:45.542224 22760421793920 run.py:483] Algo bellman_ford step 4038 current loss 0.078406, current_train_items 129248.
I0302 18:59:45.574720 22760421793920 run.py:483] Algo bellman_ford step 4039 current loss 0.060032, current_train_items 129280.
I0302 18:59:45.592826 22760421793920 run.py:483] Algo bellman_ford step 4040 current loss 0.003413, current_train_items 129312.
I0302 18:59:45.608600 22760421793920 run.py:483] Algo bellman_ford step 4041 current loss 0.043611, current_train_items 129344.
I0302 18:59:45.631045 22760421793920 run.py:483] Algo bellman_ford step 4042 current loss 0.053060, current_train_items 129376.
I0302 18:59:45.662044 22760421793920 run.py:483] Algo bellman_ford step 4043 current loss 0.107063, current_train_items 129408.
I0302 18:59:45.693258 22760421793920 run.py:483] Algo bellman_ford step 4044 current loss 0.063478, current_train_items 129440.
I0302 18:59:45.711620 22760421793920 run.py:483] Algo bellman_ford step 4045 current loss 0.012163, current_train_items 129472.
I0302 18:59:45.727672 22760421793920 run.py:483] Algo bellman_ford step 4046 current loss 0.042986, current_train_items 129504.
I0302 18:59:45.750391 22760421793920 run.py:483] Algo bellman_ford step 4047 current loss 0.054465, current_train_items 129536.
I0302 18:59:45.779389 22760421793920 run.py:483] Algo bellman_ford step 4048 current loss 0.037994, current_train_items 129568.
I0302 18:59:45.812762 22760421793920 run.py:483] Algo bellman_ford step 4049 current loss 0.100444, current_train_items 129600.
I0302 18:59:45.831000 22760421793920 run.py:483] Algo bellman_ford step 4050 current loss 0.010072, current_train_items 129632.
I0302 18:59:45.838539 22760421793920 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 18:59:45.838647 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 18:59:45.855158 22760421793920 run.py:483] Algo bellman_ford step 4051 current loss 0.018626, current_train_items 129664.
I0302 18:59:45.877801 22760421793920 run.py:483] Algo bellman_ford step 4052 current loss 0.026602, current_train_items 129696.
I0302 18:59:45.907792 22760421793920 run.py:483] Algo bellman_ford step 4053 current loss 0.030508, current_train_items 129728.
I0302 18:59:45.941993 22760421793920 run.py:483] Algo bellman_ford step 4054 current loss 0.138861, current_train_items 129760.
I0302 18:59:45.960503 22760421793920 run.py:483] Algo bellman_ford step 4055 current loss 0.010499, current_train_items 129792.
I0302 18:59:45.976384 22760421793920 run.py:483] Algo bellman_ford step 4056 current loss 0.025921, current_train_items 129824.
I0302 18:59:45.998914 22760421793920 run.py:483] Algo bellman_ford step 4057 current loss 0.042037, current_train_items 129856.
I0302 18:59:46.027848 22760421793920 run.py:483] Algo bellman_ford step 4058 current loss 0.043531, current_train_items 129888.
I0302 18:59:46.059050 22760421793920 run.py:483] Algo bellman_ford step 4059 current loss 0.050135, current_train_items 129920.
I0302 18:59:46.077548 22760421793920 run.py:483] Algo bellman_ford step 4060 current loss 0.017850, current_train_items 129952.
I0302 18:59:46.093644 22760421793920 run.py:483] Algo bellman_ford step 4061 current loss 0.020535, current_train_items 129984.
I0302 18:59:46.116217 22760421793920 run.py:483] Algo bellman_ford step 4062 current loss 0.049809, current_train_items 130016.
I0302 18:59:46.144843 22760421793920 run.py:483] Algo bellman_ford step 4063 current loss 0.045079, current_train_items 130048.
I0302 18:59:46.178163 22760421793920 run.py:483] Algo bellman_ford step 4064 current loss 0.086962, current_train_items 130080.
I0302 18:59:46.196604 22760421793920 run.py:483] Algo bellman_ford step 4065 current loss 0.007485, current_train_items 130112.
I0302 18:59:46.211944 22760421793920 run.py:483] Algo bellman_ford step 4066 current loss 0.023432, current_train_items 130144.
I0302 18:59:46.236315 22760421793920 run.py:483] Algo bellman_ford step 4067 current loss 0.079895, current_train_items 130176.
I0302 18:59:46.265858 22760421793920 run.py:483] Algo bellman_ford step 4068 current loss 0.051227, current_train_items 130208.
I0302 18:59:46.299148 22760421793920 run.py:483] Algo bellman_ford step 4069 current loss 0.084428, current_train_items 130240.
I0302 18:59:46.317224 22760421793920 run.py:483] Algo bellman_ford step 4070 current loss 0.004538, current_train_items 130272.
I0302 18:59:46.332996 22760421793920 run.py:483] Algo bellman_ford step 4071 current loss 0.021762, current_train_items 130304.
I0302 18:59:46.355806 22760421793920 run.py:483] Algo bellman_ford step 4072 current loss 0.077167, current_train_items 130336.
I0302 18:59:46.384436 22760421793920 run.py:483] Algo bellman_ford step 4073 current loss 0.066155, current_train_items 130368.
I0302 18:59:46.415926 22760421793920 run.py:483] Algo bellman_ford step 4074 current loss 0.088347, current_train_items 130400.
I0302 18:59:46.434333 22760421793920 run.py:483] Algo bellman_ford step 4075 current loss 0.010330, current_train_items 130432.
I0302 18:59:46.450589 22760421793920 run.py:483] Algo bellman_ford step 4076 current loss 0.021193, current_train_items 130464.
I0302 18:59:46.474651 22760421793920 run.py:483] Algo bellman_ford step 4077 current loss 0.103592, current_train_items 130496.
I0302 18:59:46.503978 22760421793920 run.py:483] Algo bellman_ford step 4078 current loss 0.086920, current_train_items 130528.
I0302 18:59:46.535403 22760421793920 run.py:483] Algo bellman_ford step 4079 current loss 0.091484, current_train_items 130560.
I0302 18:59:46.553479 22760421793920 run.py:483] Algo bellman_ford step 4080 current loss 0.001180, current_train_items 130592.
I0302 18:59:46.569497 22760421793920 run.py:483] Algo bellman_ford step 4081 current loss 0.025014, current_train_items 130624.
I0302 18:59:46.593249 22760421793920 run.py:483] Algo bellman_ford step 4082 current loss 0.067623, current_train_items 130656.
I0302 18:59:46.624819 22760421793920 run.py:483] Algo bellman_ford step 4083 current loss 0.075766, current_train_items 130688.
I0302 18:59:46.654313 22760421793920 run.py:483] Algo bellman_ford step 4084 current loss 0.048460, current_train_items 130720.
I0302 18:59:46.672894 22760421793920 run.py:483] Algo bellman_ford step 4085 current loss 0.012619, current_train_items 130752.
I0302 18:59:46.688720 22760421793920 run.py:483] Algo bellman_ford step 4086 current loss 0.024760, current_train_items 130784.
I0302 18:59:46.711081 22760421793920 run.py:483] Algo bellman_ford step 4087 current loss 0.052996, current_train_items 130816.
I0302 18:59:46.740570 22760421793920 run.py:483] Algo bellman_ford step 4088 current loss 0.061220, current_train_items 130848.
I0302 18:59:46.770993 22760421793920 run.py:483] Algo bellman_ford step 4089 current loss 0.052476, current_train_items 130880.
I0302 18:59:46.788951 22760421793920 run.py:483] Algo bellman_ford step 4090 current loss 0.003237, current_train_items 130912.
I0302 18:59:46.804529 22760421793920 run.py:483] Algo bellman_ford step 4091 current loss 0.021787, current_train_items 130944.
I0302 18:59:46.828245 22760421793920 run.py:483] Algo bellman_ford step 4092 current loss 0.136608, current_train_items 130976.
I0302 18:59:46.858781 22760421793920 run.py:483] Algo bellman_ford step 4093 current loss 0.145711, current_train_items 131008.
I0302 18:59:46.891309 22760421793920 run.py:483] Algo bellman_ford step 4094 current loss 0.133581, current_train_items 131040.
I0302 18:59:46.909362 22760421793920 run.py:483] Algo bellman_ford step 4095 current loss 0.006440, current_train_items 131072.
I0302 18:59:46.925421 22760421793920 run.py:483] Algo bellman_ford step 4096 current loss 0.050775, current_train_items 131104.
I0302 18:59:46.949149 22760421793920 run.py:483] Algo bellman_ford step 4097 current loss 0.061633, current_train_items 131136.
I0302 18:59:46.979416 22760421793920 run.py:483] Algo bellman_ford step 4098 current loss 0.079016, current_train_items 131168.
I0302 18:59:47.012133 22760421793920 run.py:483] Algo bellman_ford step 4099 current loss 0.107587, current_train_items 131200.
I0302 18:59:47.030424 22760421793920 run.py:483] Algo bellman_ford step 4100 current loss 0.001989, current_train_items 131232.
I0302 18:59:47.038054 22760421793920 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 18:59:47.038163 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:59:47.054016 22760421793920 run.py:483] Algo bellman_ford step 4101 current loss 0.008316, current_train_items 131264.
I0302 18:59:47.075721 22760421793920 run.py:483] Algo bellman_ford step 4102 current loss 0.059733, current_train_items 131296.
I0302 18:59:47.107033 22760421793920 run.py:483] Algo bellman_ford step 4103 current loss 0.092083, current_train_items 131328.
I0302 18:59:47.140837 22760421793920 run.py:483] Algo bellman_ford step 4104 current loss 0.161497, current_train_items 131360.
I0302 18:59:47.159240 22760421793920 run.py:483] Algo bellman_ford step 4105 current loss 0.009256, current_train_items 131392.
I0302 18:59:47.175091 22760421793920 run.py:483] Algo bellman_ford step 4106 current loss 0.033388, current_train_items 131424.
I0302 18:59:47.198680 22760421793920 run.py:483] Algo bellman_ford step 4107 current loss 0.044646, current_train_items 131456.
I0302 18:59:47.228847 22760421793920 run.py:483] Algo bellman_ford step 4108 current loss 0.078263, current_train_items 131488.
I0302 18:59:47.262896 22760421793920 run.py:483] Algo bellman_ford step 4109 current loss 0.082416, current_train_items 131520.
I0302 18:59:47.281009 22760421793920 run.py:483] Algo bellman_ford step 4110 current loss 0.017063, current_train_items 131552.
I0302 18:59:47.297204 22760421793920 run.py:483] Algo bellman_ford step 4111 current loss 0.032990, current_train_items 131584.
I0302 18:59:47.319551 22760421793920 run.py:483] Algo bellman_ford step 4112 current loss 0.077168, current_train_items 131616.
I0302 18:59:47.350566 22760421793920 run.py:483] Algo bellman_ford step 4113 current loss 0.059089, current_train_items 131648.
I0302 18:59:47.383686 22760421793920 run.py:483] Algo bellman_ford step 4114 current loss 0.080641, current_train_items 131680.
I0302 18:59:47.401911 22760421793920 run.py:483] Algo bellman_ford step 4115 current loss 0.011339, current_train_items 131712.
I0302 18:59:47.417877 22760421793920 run.py:483] Algo bellman_ford step 4116 current loss 0.028854, current_train_items 131744.
I0302 18:59:47.442049 22760421793920 run.py:483] Algo bellman_ford step 4117 current loss 0.078772, current_train_items 131776.
I0302 18:59:47.471226 22760421793920 run.py:483] Algo bellman_ford step 4118 current loss 0.057607, current_train_items 131808.
I0302 18:59:47.503545 22760421793920 run.py:483] Algo bellman_ford step 4119 current loss 0.088780, current_train_items 131840.
I0302 18:59:47.521765 22760421793920 run.py:483] Algo bellman_ford step 4120 current loss 0.014205, current_train_items 131872.
I0302 18:59:47.537752 22760421793920 run.py:483] Algo bellman_ford step 4121 current loss 0.015154, current_train_items 131904.
I0302 18:59:47.561724 22760421793920 run.py:483] Algo bellman_ford step 4122 current loss 0.067336, current_train_items 131936.
I0302 18:59:47.592510 22760421793920 run.py:483] Algo bellman_ford step 4123 current loss 0.133063, current_train_items 131968.
I0302 18:59:47.624464 22760421793920 run.py:483] Algo bellman_ford step 4124 current loss 0.093476, current_train_items 132000.
I0302 18:59:47.642628 22760421793920 run.py:483] Algo bellman_ford step 4125 current loss 0.006372, current_train_items 132032.
I0302 18:59:47.658413 22760421793920 run.py:483] Algo bellman_ford step 4126 current loss 0.072048, current_train_items 132064.
I0302 18:59:47.682252 22760421793920 run.py:483] Algo bellman_ford step 4127 current loss 0.107758, current_train_items 132096.
I0302 18:59:47.712017 22760421793920 run.py:483] Algo bellman_ford step 4128 current loss 0.069205, current_train_items 132128.
I0302 18:59:47.743078 22760421793920 run.py:483] Algo bellman_ford step 4129 current loss 0.132583, current_train_items 132160.
I0302 18:59:47.761670 22760421793920 run.py:483] Algo bellman_ford step 4130 current loss 0.006186, current_train_items 132192.
I0302 18:59:47.777616 22760421793920 run.py:483] Algo bellman_ford step 4131 current loss 0.037855, current_train_items 132224.
I0302 18:59:47.800635 22760421793920 run.py:483] Algo bellman_ford step 4132 current loss 0.056696, current_train_items 132256.
I0302 18:59:47.831612 22760421793920 run.py:483] Algo bellman_ford step 4133 current loss 0.087867, current_train_items 132288.
I0302 18:59:47.864922 22760421793920 run.py:483] Algo bellman_ford step 4134 current loss 0.074844, current_train_items 132320.
I0302 18:59:47.883286 22760421793920 run.py:483] Algo bellman_ford step 4135 current loss 0.016165, current_train_items 132352.
I0302 18:59:47.898876 22760421793920 run.py:483] Algo bellman_ford step 4136 current loss 0.011659, current_train_items 132384.
I0302 18:59:47.921383 22760421793920 run.py:483] Algo bellman_ford step 4137 current loss 0.036341, current_train_items 132416.
I0302 18:59:47.950814 22760421793920 run.py:483] Algo bellman_ford step 4138 current loss 0.050466, current_train_items 132448.
I0302 18:59:47.983853 22760421793920 run.py:483] Algo bellman_ford step 4139 current loss 0.091651, current_train_items 132480.
I0302 18:59:48.002285 22760421793920 run.py:483] Algo bellman_ford step 4140 current loss 0.002551, current_train_items 132512.
I0302 18:59:48.017726 22760421793920 run.py:483] Algo bellman_ford step 4141 current loss 0.020108, current_train_items 132544.
I0302 18:59:48.042041 22760421793920 run.py:483] Algo bellman_ford step 4142 current loss 0.043676, current_train_items 132576.
I0302 18:59:48.071638 22760421793920 run.py:483] Algo bellman_ford step 4143 current loss 0.044130, current_train_items 132608.
I0302 18:59:48.103588 22760421793920 run.py:483] Algo bellman_ford step 4144 current loss 0.079169, current_train_items 132640.
I0302 18:59:48.121775 22760421793920 run.py:483] Algo bellman_ford step 4145 current loss 0.006149, current_train_items 132672.
I0302 18:59:48.137209 22760421793920 run.py:483] Algo bellman_ford step 4146 current loss 0.012803, current_train_items 132704.
I0302 18:59:48.159977 22760421793920 run.py:483] Algo bellman_ford step 4147 current loss 0.060190, current_train_items 132736.
I0302 18:59:48.188411 22760421793920 run.py:483] Algo bellman_ford step 4148 current loss 0.035028, current_train_items 132768.
I0302 18:59:48.219829 22760421793920 run.py:483] Algo bellman_ford step 4149 current loss 0.077706, current_train_items 132800.
I0302 18:59:48.238460 22760421793920 run.py:483] Algo bellman_ford step 4150 current loss 0.007035, current_train_items 132832.
I0302 18:59:48.245718 22760421793920 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 18:59:48.245828 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:59:48.262278 22760421793920 run.py:483] Algo bellman_ford step 4151 current loss 0.084310, current_train_items 132864.
I0302 18:59:48.285974 22760421793920 run.py:483] Algo bellman_ford step 4152 current loss 0.099109, current_train_items 132896.
I0302 18:59:48.316016 22760421793920 run.py:483] Algo bellman_ford step 4153 current loss 0.058767, current_train_items 132928.
I0302 18:59:48.349518 22760421793920 run.py:483] Algo bellman_ford step 4154 current loss 0.125428, current_train_items 132960.
I0302 18:59:48.368002 22760421793920 run.py:483] Algo bellman_ford step 4155 current loss 0.006007, current_train_items 132992.
I0302 18:59:48.384396 22760421793920 run.py:483] Algo bellman_ford step 4156 current loss 0.069891, current_train_items 133024.
I0302 18:59:48.406378 22760421793920 run.py:483] Algo bellman_ford step 4157 current loss 0.068917, current_train_items 133056.
I0302 18:59:48.435721 22760421793920 run.py:483] Algo bellman_ford step 4158 current loss 0.080036, current_train_items 133088.
I0302 18:59:48.467883 22760421793920 run.py:483] Algo bellman_ford step 4159 current loss 0.107982, current_train_items 133120.
I0302 18:59:48.486258 22760421793920 run.py:483] Algo bellman_ford step 4160 current loss 0.006106, current_train_items 133152.
I0302 18:59:48.501971 22760421793920 run.py:483] Algo bellman_ford step 4161 current loss 0.042083, current_train_items 133184.
I0302 18:59:48.523676 22760421793920 run.py:483] Algo bellman_ford step 4162 current loss 0.049273, current_train_items 133216.
I0302 18:59:48.553610 22760421793920 run.py:483] Algo bellman_ford step 4163 current loss 0.099067, current_train_items 133248.
I0302 18:59:48.585961 22760421793920 run.py:483] Algo bellman_ford step 4164 current loss 0.145798, current_train_items 133280.
I0302 18:59:48.604509 22760421793920 run.py:483] Algo bellman_ford step 4165 current loss 0.008103, current_train_items 133312.
I0302 18:59:48.620184 22760421793920 run.py:483] Algo bellman_ford step 4166 current loss 0.021235, current_train_items 133344.
I0302 18:59:48.643832 22760421793920 run.py:483] Algo bellman_ford step 4167 current loss 0.111548, current_train_items 133376.
I0302 18:59:48.673222 22760421793920 run.py:483] Algo bellman_ford step 4168 current loss 0.095581, current_train_items 133408.
I0302 18:59:48.703976 22760421793920 run.py:483] Algo bellman_ford step 4169 current loss 0.096444, current_train_items 133440.
I0302 18:59:48.722059 22760421793920 run.py:483] Algo bellman_ford step 4170 current loss 0.002422, current_train_items 133472.
I0302 18:59:48.737206 22760421793920 run.py:483] Algo bellman_ford step 4171 current loss 0.012978, current_train_items 133504.
I0302 18:59:48.759751 22760421793920 run.py:483] Algo bellman_ford step 4172 current loss 0.017590, current_train_items 133536.
I0302 18:59:48.789080 22760421793920 run.py:483] Algo bellman_ford step 4173 current loss 0.048728, current_train_items 133568.
I0302 18:59:48.822799 22760421793920 run.py:483] Algo bellman_ford step 4174 current loss 0.065468, current_train_items 133600.
I0302 18:59:48.840682 22760421793920 run.py:483] Algo bellman_ford step 4175 current loss 0.004464, current_train_items 133632.
I0302 18:59:48.856988 22760421793920 run.py:483] Algo bellman_ford step 4176 current loss 0.021575, current_train_items 133664.
I0302 18:59:48.880077 22760421793920 run.py:483] Algo bellman_ford step 4177 current loss 0.040737, current_train_items 133696.
I0302 18:59:48.910286 22760421793920 run.py:483] Algo bellman_ford step 4178 current loss 0.064166, current_train_items 133728.
I0302 18:59:48.942729 22760421793920 run.py:483] Algo bellman_ford step 4179 current loss 0.082521, current_train_items 133760.
I0302 18:59:48.960877 22760421793920 run.py:483] Algo bellman_ford step 4180 current loss 0.001381, current_train_items 133792.
I0302 18:59:48.976613 22760421793920 run.py:483] Algo bellman_ford step 4181 current loss 0.015975, current_train_items 133824.
I0302 18:59:49.000545 22760421793920 run.py:483] Algo bellman_ford step 4182 current loss 0.035920, current_train_items 133856.
I0302 18:59:49.028643 22760421793920 run.py:483] Algo bellman_ford step 4183 current loss 0.051158, current_train_items 133888.
I0302 18:59:49.061075 22760421793920 run.py:483] Algo bellman_ford step 4184 current loss 0.092344, current_train_items 133920.
I0302 18:59:49.079277 22760421793920 run.py:483] Algo bellman_ford step 4185 current loss 0.002821, current_train_items 133952.
I0302 18:59:49.094763 22760421793920 run.py:483] Algo bellman_ford step 4186 current loss 0.026907, current_train_items 133984.
I0302 18:59:49.117456 22760421793920 run.py:483] Algo bellman_ford step 4187 current loss 0.057618, current_train_items 134016.
I0302 18:59:49.146815 22760421793920 run.py:483] Algo bellman_ford step 4188 current loss 0.066491, current_train_items 134048.
I0302 18:59:49.177933 22760421793920 run.py:483] Algo bellman_ford step 4189 current loss 0.098420, current_train_items 134080.
I0302 18:59:49.196072 22760421793920 run.py:483] Algo bellman_ford step 4190 current loss 0.032371, current_train_items 134112.
I0302 18:59:49.212128 22760421793920 run.py:483] Algo bellman_ford step 4191 current loss 0.021459, current_train_items 134144.
I0302 18:59:49.232751 22760421793920 run.py:483] Algo bellman_ford step 4192 current loss 0.033350, current_train_items 134176.
I0302 18:59:49.262940 22760421793920 run.py:483] Algo bellman_ford step 4193 current loss 0.056500, current_train_items 134208.
I0302 18:59:49.297143 22760421793920 run.py:483] Algo bellman_ford step 4194 current loss 0.145667, current_train_items 134240.
I0302 18:59:49.315037 22760421793920 run.py:483] Algo bellman_ford step 4195 current loss 0.002311, current_train_items 134272.
I0302 18:59:49.330626 22760421793920 run.py:483] Algo bellman_ford step 4196 current loss 0.022363, current_train_items 134304.
I0302 18:59:49.353763 22760421793920 run.py:483] Algo bellman_ford step 4197 current loss 0.022861, current_train_items 134336.
I0302 18:59:49.382992 22760421793920 run.py:483] Algo bellman_ford step 4198 current loss 0.065351, current_train_items 134368.
I0302 18:59:49.414365 22760421793920 run.py:483] Algo bellman_ford step 4199 current loss 0.085784, current_train_items 134400.
I0302 18:59:49.432370 22760421793920 run.py:483] Algo bellman_ford step 4200 current loss 0.001732, current_train_items 134432.
I0302 18:59:49.439793 22760421793920 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 18:59:49.439913 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:49.456533 22760421793920 run.py:483] Algo bellman_ford step 4201 current loss 0.032409, current_train_items 134464.
I0302 18:59:49.479270 22760421793920 run.py:483] Algo bellman_ford step 4202 current loss 0.031456, current_train_items 134496.
I0302 18:59:49.509282 22760421793920 run.py:483] Algo bellman_ford step 4203 current loss 0.042716, current_train_items 134528.
I0302 18:59:49.541189 22760421793920 run.py:483] Algo bellman_ford step 4204 current loss 0.087588, current_train_items 134560.
I0302 18:59:49.559718 22760421793920 run.py:483] Algo bellman_ford step 4205 current loss 0.002664, current_train_items 134592.
I0302 18:59:49.575435 22760421793920 run.py:483] Algo bellman_ford step 4206 current loss 0.019421, current_train_items 134624.
I0302 18:59:49.598542 22760421793920 run.py:483] Algo bellman_ford step 4207 current loss 0.030793, current_train_items 134656.
I0302 18:59:49.629098 22760421793920 run.py:483] Algo bellman_ford step 4208 current loss 0.067647, current_train_items 134688.
I0302 18:59:49.662068 22760421793920 run.py:483] Algo bellman_ford step 4209 current loss 0.093369, current_train_items 134720.
I0302 18:59:49.680189 22760421793920 run.py:483] Algo bellman_ford step 4210 current loss 0.004006, current_train_items 134752.
I0302 18:59:49.695840 22760421793920 run.py:483] Algo bellman_ford step 4211 current loss 0.013322, current_train_items 134784.
I0302 18:59:49.718885 22760421793920 run.py:483] Algo bellman_ford step 4212 current loss 0.048781, current_train_items 134816.
I0302 18:59:49.749104 22760421793920 run.py:483] Algo bellman_ford step 4213 current loss 0.061273, current_train_items 134848.
I0302 18:59:49.781102 22760421793920 run.py:483] Algo bellman_ford step 4214 current loss 0.110276, current_train_items 134880.
I0302 18:59:49.799016 22760421793920 run.py:483] Algo bellman_ford step 4215 current loss 0.012950, current_train_items 134912.
I0302 18:59:49.814716 22760421793920 run.py:483] Algo bellman_ford step 4216 current loss 0.027332, current_train_items 134944.
I0302 18:59:49.837086 22760421793920 run.py:483] Algo bellman_ford step 4217 current loss 0.029942, current_train_items 134976.
I0302 18:59:49.866437 22760421793920 run.py:483] Algo bellman_ford step 4218 current loss 0.104950, current_train_items 135008.
I0302 18:59:49.898053 22760421793920 run.py:483] Algo bellman_ford step 4219 current loss 0.065091, current_train_items 135040.
I0302 18:59:49.916191 22760421793920 run.py:483] Algo bellman_ford step 4220 current loss 0.002535, current_train_items 135072.
I0302 18:59:49.932013 22760421793920 run.py:483] Algo bellman_ford step 4221 current loss 0.027774, current_train_items 135104.
I0302 18:59:49.955096 22760421793920 run.py:483] Algo bellman_ford step 4222 current loss 0.049024, current_train_items 135136.
I0302 18:59:49.985797 22760421793920 run.py:483] Algo bellman_ford step 4223 current loss 0.063179, current_train_items 135168.
I0302 18:59:50.019419 22760421793920 run.py:483] Algo bellman_ford step 4224 current loss 0.094641, current_train_items 135200.
I0302 18:59:50.037742 22760421793920 run.py:483] Algo bellman_ford step 4225 current loss 0.001667, current_train_items 135232.
I0302 18:59:50.053254 22760421793920 run.py:483] Algo bellman_ford step 4226 current loss 0.017156, current_train_items 135264.
I0302 18:59:50.075705 22760421793920 run.py:483] Algo bellman_ford step 4227 current loss 0.074363, current_train_items 135296.
I0302 18:59:50.104348 22760421793920 run.py:483] Algo bellman_ford step 4228 current loss 0.054345, current_train_items 135328.
I0302 18:59:50.134259 22760421793920 run.py:483] Algo bellman_ford step 4229 current loss 0.054696, current_train_items 135360.
I0302 18:59:50.152062 22760421793920 run.py:483] Algo bellman_ford step 4230 current loss 0.002633, current_train_items 135392.
I0302 18:59:50.167537 22760421793920 run.py:483] Algo bellman_ford step 4231 current loss 0.010603, current_train_items 135424.
I0302 18:59:50.190416 22760421793920 run.py:483] Algo bellman_ford step 4232 current loss 0.040655, current_train_items 135456.
I0302 18:59:50.220109 22760421793920 run.py:483] Algo bellman_ford step 4233 current loss 0.072244, current_train_items 135488.
I0302 18:59:50.251468 22760421793920 run.py:483] Algo bellman_ford step 4234 current loss 0.057445, current_train_items 135520.
I0302 18:59:50.269555 22760421793920 run.py:483] Algo bellman_ford step 4235 current loss 0.004827, current_train_items 135552.
I0302 18:59:50.285872 22760421793920 run.py:483] Algo bellman_ford step 4236 current loss 0.034827, current_train_items 135584.
I0302 18:59:50.308792 22760421793920 run.py:483] Algo bellman_ford step 4237 current loss 0.073659, current_train_items 135616.
I0302 18:59:50.338743 22760421793920 run.py:483] Algo bellman_ford step 4238 current loss 0.096583, current_train_items 135648.
I0302 18:59:50.372275 22760421793920 run.py:483] Algo bellman_ford step 4239 current loss 0.118647, current_train_items 135680.
I0302 18:59:50.390387 22760421793920 run.py:483] Algo bellman_ford step 4240 current loss 0.007621, current_train_items 135712.
I0302 18:59:50.405871 22760421793920 run.py:483] Algo bellman_ford step 4241 current loss 0.012562, current_train_items 135744.
I0302 18:59:50.428281 22760421793920 run.py:483] Algo bellman_ford step 4242 current loss 0.043818, current_train_items 135776.
I0302 18:59:50.459095 22760421793920 run.py:483] Algo bellman_ford step 4243 current loss 0.075533, current_train_items 135808.
I0302 18:59:50.490062 22760421793920 run.py:483] Algo bellman_ford step 4244 current loss 0.055028, current_train_items 135840.
I0302 18:59:50.507910 22760421793920 run.py:483] Algo bellman_ford step 4245 current loss 0.001940, current_train_items 135872.
I0302 18:59:50.523280 22760421793920 run.py:483] Algo bellman_ford step 4246 current loss 0.008870, current_train_items 135904.
I0302 18:59:50.547516 22760421793920 run.py:483] Algo bellman_ford step 4247 current loss 0.038978, current_train_items 135936.
I0302 18:59:50.578045 22760421793920 run.py:483] Algo bellman_ford step 4248 current loss 0.073137, current_train_items 135968.
I0302 18:59:50.611404 22760421793920 run.py:483] Algo bellman_ford step 4249 current loss 0.091795, current_train_items 136000.
I0302 18:59:50.629560 22760421793920 run.py:483] Algo bellman_ford step 4250 current loss 0.007705, current_train_items 136032.
I0302 18:59:50.637062 22760421793920 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 18:59:50.637171 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:50.653341 22760421793920 run.py:483] Algo bellman_ford step 4251 current loss 0.017899, current_train_items 136064.
I0302 18:59:50.676638 22760421793920 run.py:483] Algo bellman_ford step 4252 current loss 0.033706, current_train_items 136096.
I0302 18:59:50.707029 22760421793920 run.py:483] Algo bellman_ford step 4253 current loss 0.095606, current_train_items 136128.
I0302 18:59:50.739188 22760421793920 run.py:483] Algo bellman_ford step 4254 current loss 0.066740, current_train_items 136160.
I0302 18:59:50.757360 22760421793920 run.py:483] Algo bellman_ford step 4255 current loss 0.004402, current_train_items 136192.
I0302 18:59:50.773182 22760421793920 run.py:483] Algo bellman_ford step 4256 current loss 0.014709, current_train_items 136224.
I0302 18:59:50.795436 22760421793920 run.py:483] Algo bellman_ford step 4257 current loss 0.029518, current_train_items 136256.
I0302 18:59:50.825656 22760421793920 run.py:483] Algo bellman_ford step 4258 current loss 0.073550, current_train_items 136288.
I0302 18:59:50.858248 22760421793920 run.py:483] Algo bellman_ford step 4259 current loss 0.102205, current_train_items 136320.
I0302 18:59:50.876590 22760421793920 run.py:483] Algo bellman_ford step 4260 current loss 0.010603, current_train_items 136352.
I0302 18:59:50.892659 22760421793920 run.py:483] Algo bellman_ford step 4261 current loss 0.029032, current_train_items 136384.
I0302 18:59:50.915503 22760421793920 run.py:483] Algo bellman_ford step 4262 current loss 0.038900, current_train_items 136416.
I0302 18:59:50.945644 22760421793920 run.py:483] Algo bellman_ford step 4263 current loss 0.045334, current_train_items 136448.
I0302 18:59:50.977947 22760421793920 run.py:483] Algo bellman_ford step 4264 current loss 0.056458, current_train_items 136480.
I0302 18:59:50.996101 22760421793920 run.py:483] Algo bellman_ford step 4265 current loss 0.004448, current_train_items 136512.
I0302 18:59:51.011824 22760421793920 run.py:483] Algo bellman_ford step 4266 current loss 0.024480, current_train_items 136544.
I0302 18:59:51.034545 22760421793920 run.py:483] Algo bellman_ford step 4267 current loss 0.018685, current_train_items 136576.
I0302 18:59:51.064247 22760421793920 run.py:483] Algo bellman_ford step 4268 current loss 0.072894, current_train_items 136608.
I0302 18:59:51.095861 22760421793920 run.py:483] Algo bellman_ford step 4269 current loss 0.085995, current_train_items 136640.
I0302 18:59:51.114240 22760421793920 run.py:483] Algo bellman_ford step 4270 current loss 0.002212, current_train_items 136672.
I0302 18:59:51.129661 22760421793920 run.py:483] Algo bellman_ford step 4271 current loss 0.024600, current_train_items 136704.
I0302 18:59:51.151787 22760421793920 run.py:483] Algo bellman_ford step 4272 current loss 0.019983, current_train_items 136736.
I0302 18:59:51.181599 22760421793920 run.py:483] Algo bellman_ford step 4273 current loss 0.052809, current_train_items 136768.
I0302 18:59:51.213026 22760421793920 run.py:483] Algo bellman_ford step 4274 current loss 0.075644, current_train_items 136800.
I0302 18:59:51.231324 22760421793920 run.py:483] Algo bellman_ford step 4275 current loss 0.002956, current_train_items 136832.
I0302 18:59:51.246959 22760421793920 run.py:483] Algo bellman_ford step 4276 current loss 0.016458, current_train_items 136864.
I0302 18:59:51.269603 22760421793920 run.py:483] Algo bellman_ford step 4277 current loss 0.060873, current_train_items 136896.
I0302 18:59:51.301270 22760421793920 run.py:483] Algo bellman_ford step 4278 current loss 0.072305, current_train_items 136928.
I0302 18:59:51.334018 22760421793920 run.py:483] Algo bellman_ford step 4279 current loss 0.062843, current_train_items 136960.
I0302 18:59:51.352499 22760421793920 run.py:483] Algo bellman_ford step 4280 current loss 0.015592, current_train_items 136992.
I0302 18:59:51.368055 22760421793920 run.py:483] Algo bellman_ford step 4281 current loss 0.038580, current_train_items 137024.
I0302 18:59:51.391143 22760421793920 run.py:483] Algo bellman_ford step 4282 current loss 0.025161, current_train_items 137056.
I0302 18:59:51.420043 22760421793920 run.py:483] Algo bellman_ford step 4283 current loss 0.028858, current_train_items 137088.
I0302 18:59:51.454049 22760421793920 run.py:483] Algo bellman_ford step 4284 current loss 0.071681, current_train_items 137120.
I0302 18:59:51.472512 22760421793920 run.py:483] Algo bellman_ford step 4285 current loss 0.002264, current_train_items 137152.
I0302 18:59:51.488249 22760421793920 run.py:483] Algo bellman_ford step 4286 current loss 0.003163, current_train_items 137184.
I0302 18:59:51.511184 22760421793920 run.py:483] Algo bellman_ford step 4287 current loss 0.073826, current_train_items 137216.
I0302 18:59:51.542480 22760421793920 run.py:483] Algo bellman_ford step 4288 current loss 0.068794, current_train_items 137248.
I0302 18:59:51.575189 22760421793920 run.py:483] Algo bellman_ford step 4289 current loss 0.073098, current_train_items 137280.
I0302 18:59:51.593582 22760421793920 run.py:483] Algo bellman_ford step 4290 current loss 0.004346, current_train_items 137312.
I0302 18:59:51.609707 22760421793920 run.py:483] Algo bellman_ford step 4291 current loss 0.027499, current_train_items 137344.
I0302 18:59:51.633793 22760421793920 run.py:483] Algo bellman_ford step 4292 current loss 0.077254, current_train_items 137376.
I0302 18:59:51.665040 22760421793920 run.py:483] Algo bellman_ford step 4293 current loss 0.094000, current_train_items 137408.
I0302 18:59:51.699062 22760421793920 run.py:483] Algo bellman_ford step 4294 current loss 0.103429, current_train_items 137440.
I0302 18:59:51.717402 22760421793920 run.py:483] Algo bellman_ford step 4295 current loss 0.003801, current_train_items 137472.
I0302 18:59:51.732949 22760421793920 run.py:483] Algo bellman_ford step 4296 current loss 0.017836, current_train_items 137504.
I0302 18:59:51.755417 22760421793920 run.py:483] Algo bellman_ford step 4297 current loss 0.062610, current_train_items 137536.
I0302 18:59:51.785829 22760421793920 run.py:483] Algo bellman_ford step 4298 current loss 0.120605, current_train_items 137568.
I0302 18:59:51.814655 22760421793920 run.py:483] Algo bellman_ford step 4299 current loss 0.032996, current_train_items 137600.
I0302 18:59:51.833196 22760421793920 run.py:483] Algo bellman_ford step 4300 current loss 0.007568, current_train_items 137632.
I0302 18:59:51.840533 22760421793920 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 18:59:51.840644 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.990, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 18:59:51.856855 22760421793920 run.py:483] Algo bellman_ford step 4301 current loss 0.024829, current_train_items 137664.
I0302 18:59:51.879877 22760421793920 run.py:483] Algo bellman_ford step 4302 current loss 0.068383, current_train_items 137696.
I0302 18:59:51.911108 22760421793920 run.py:483] Algo bellman_ford step 4303 current loss 0.157965, current_train_items 137728.
I0302 18:59:51.945518 22760421793920 run.py:483] Algo bellman_ford step 4304 current loss 0.070559, current_train_items 137760.
I0302 18:59:51.964054 22760421793920 run.py:483] Algo bellman_ford step 4305 current loss 0.006522, current_train_items 137792.
I0302 18:59:51.979651 22760421793920 run.py:483] Algo bellman_ford step 4306 current loss 0.020249, current_train_items 137824.
I0302 18:59:52.003834 22760421793920 run.py:483] Algo bellman_ford step 4307 current loss 0.114511, current_train_items 137856.
I0302 18:59:52.034494 22760421793920 run.py:483] Algo bellman_ford step 4308 current loss 0.087401, current_train_items 137888.
I0302 18:59:52.068807 22760421793920 run.py:483] Algo bellman_ford step 4309 current loss 0.141587, current_train_items 137920.
I0302 18:59:52.086953 22760421793920 run.py:483] Algo bellman_ford step 4310 current loss 0.005895, current_train_items 137952.
I0302 18:59:52.102586 22760421793920 run.py:483] Algo bellman_ford step 4311 current loss 0.022346, current_train_items 137984.
I0302 18:59:52.124212 22760421793920 run.py:483] Algo bellman_ford step 4312 current loss 0.063110, current_train_items 138016.
I0302 18:59:52.154435 22760421793920 run.py:483] Algo bellman_ford step 4313 current loss 0.098926, current_train_items 138048.
I0302 18:59:52.186977 22760421793920 run.py:483] Algo bellman_ford step 4314 current loss 0.127007, current_train_items 138080.
I0302 18:59:52.204996 22760421793920 run.py:483] Algo bellman_ford step 4315 current loss 0.006183, current_train_items 138112.
I0302 18:59:52.220504 22760421793920 run.py:483] Algo bellman_ford step 4316 current loss 0.049877, current_train_items 138144.
I0302 18:59:52.243687 22760421793920 run.py:483] Algo bellman_ford step 4317 current loss 0.078137, current_train_items 138176.
I0302 18:59:52.272628 22760421793920 run.py:483] Algo bellman_ford step 4318 current loss 0.063638, current_train_items 138208.
I0302 18:59:52.304591 22760421793920 run.py:483] Algo bellman_ford step 4319 current loss 0.044076, current_train_items 138240.
I0302 18:59:52.322644 22760421793920 run.py:483] Algo bellman_ford step 4320 current loss 0.001598, current_train_items 138272.
I0302 18:59:52.338317 22760421793920 run.py:483] Algo bellman_ford step 4321 current loss 0.018628, current_train_items 138304.
I0302 18:59:52.360916 22760421793920 run.py:483] Algo bellman_ford step 4322 current loss 0.051838, current_train_items 138336.
I0302 18:59:52.389378 22760421793920 run.py:483] Algo bellman_ford step 4323 current loss 0.079600, current_train_items 138368.
I0302 18:59:52.421939 22760421793920 run.py:483] Algo bellman_ford step 4324 current loss 0.102022, current_train_items 138400.
I0302 18:59:52.439977 22760421793920 run.py:483] Algo bellman_ford step 4325 current loss 0.003429, current_train_items 138432.
I0302 18:59:52.455727 22760421793920 run.py:483] Algo bellman_ford step 4326 current loss 0.016719, current_train_items 138464.
I0302 18:59:52.478574 22760421793920 run.py:483] Algo bellman_ford step 4327 current loss 0.159704, current_train_items 138496.
I0302 18:59:52.509938 22760421793920 run.py:483] Algo bellman_ford step 4328 current loss 0.099224, current_train_items 138528.
I0302 18:59:52.540586 22760421793920 run.py:483] Algo bellman_ford step 4329 current loss 0.091709, current_train_items 138560.
I0302 18:59:52.558879 22760421793920 run.py:483] Algo bellman_ford step 4330 current loss 0.007329, current_train_items 138592.
I0302 18:59:52.575032 22760421793920 run.py:483] Algo bellman_ford step 4331 current loss 0.057494, current_train_items 138624.
I0302 18:59:52.598443 22760421793920 run.py:483] Algo bellman_ford step 4332 current loss 0.090537, current_train_items 138656.
I0302 18:59:52.627444 22760421793920 run.py:483] Algo bellman_ford step 4333 current loss 0.200222, current_train_items 138688.
I0302 18:59:52.657561 22760421793920 run.py:483] Algo bellman_ford step 4334 current loss 0.122503, current_train_items 138720.
I0302 18:59:52.675586 22760421793920 run.py:483] Algo bellman_ford step 4335 current loss 0.006216, current_train_items 138752.
I0302 18:59:52.691163 22760421793920 run.py:483] Algo bellman_ford step 4336 current loss 0.035219, current_train_items 138784.
I0302 18:59:52.714285 22760421793920 run.py:483] Algo bellman_ford step 4337 current loss 0.087777, current_train_items 138816.
I0302 18:59:52.745335 22760421793920 run.py:483] Algo bellman_ford step 4338 current loss 0.121969, current_train_items 138848.
I0302 18:59:52.776260 22760421793920 run.py:483] Algo bellman_ford step 4339 current loss 0.139419, current_train_items 138880.
I0302 18:59:52.794178 22760421793920 run.py:483] Algo bellman_ford step 4340 current loss 0.015131, current_train_items 138912.
I0302 18:59:52.809588 22760421793920 run.py:483] Algo bellman_ford step 4341 current loss 0.004872, current_train_items 138944.
I0302 18:59:52.832226 22760421793920 run.py:483] Algo bellman_ford step 4342 current loss 0.032536, current_train_items 138976.
I0302 18:59:52.862125 22760421793920 run.py:483] Algo bellman_ford step 4343 current loss 0.084235, current_train_items 139008.
I0302 18:59:52.894945 22760421793920 run.py:483] Algo bellman_ford step 4344 current loss 0.098208, current_train_items 139040.
I0302 18:59:52.913143 22760421793920 run.py:483] Algo bellman_ford step 4345 current loss 0.007290, current_train_items 139072.
I0302 18:59:52.929145 22760421793920 run.py:483] Algo bellman_ford step 4346 current loss 0.008596, current_train_items 139104.
I0302 18:59:52.952032 22760421793920 run.py:483] Algo bellman_ford step 4347 current loss 0.034867, current_train_items 139136.
I0302 18:59:52.981947 22760421793920 run.py:483] Algo bellman_ford step 4348 current loss 0.048011, current_train_items 139168.
I0302 18:59:53.013082 22760421793920 run.py:483] Algo bellman_ford step 4349 current loss 0.079451, current_train_items 139200.
I0302 18:59:53.031260 22760421793920 run.py:483] Algo bellman_ford step 4350 current loss 0.027732, current_train_items 139232.
I0302 18:59:53.038710 22760421793920 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 18:59:53.038819 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.990, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 18:59:53.067026 22760421793920 run.py:483] Algo bellman_ford step 4351 current loss 0.026264, current_train_items 139264.
I0302 18:59:53.090732 22760421793920 run.py:483] Algo bellman_ford step 4352 current loss 0.065681, current_train_items 139296.
I0302 18:59:53.120564 22760421793920 run.py:483] Algo bellman_ford step 4353 current loss 0.067231, current_train_items 139328.
I0302 18:59:53.153770 22760421793920 run.py:483] Algo bellman_ford step 4354 current loss 0.061049, current_train_items 139360.
I0302 18:59:53.172087 22760421793920 run.py:483] Algo bellman_ford step 4355 current loss 0.002027, current_train_items 139392.
I0302 18:59:53.188268 22760421793920 run.py:483] Algo bellman_ford step 4356 current loss 0.022773, current_train_items 139424.
I0302 18:59:53.211422 22760421793920 run.py:483] Algo bellman_ford step 4357 current loss 0.050904, current_train_items 139456.
I0302 18:59:53.239557 22760421793920 run.py:483] Algo bellman_ford step 4358 current loss 0.054684, current_train_items 139488.
I0302 18:59:53.270332 22760421793920 run.py:483] Algo bellman_ford step 4359 current loss 0.056239, current_train_items 139520.
I0302 18:59:53.288350 22760421793920 run.py:483] Algo bellman_ford step 4360 current loss 0.002404, current_train_items 139552.
I0302 18:59:53.304552 22760421793920 run.py:483] Algo bellman_ford step 4361 current loss 0.021168, current_train_items 139584.
I0302 18:59:53.326604 22760421793920 run.py:483] Algo bellman_ford step 4362 current loss 0.035246, current_train_items 139616.
I0302 18:59:53.357577 22760421793920 run.py:483] Algo bellman_ford step 4363 current loss 0.083824, current_train_items 139648.
I0302 18:59:53.390709 22760421793920 run.py:483] Algo bellman_ford step 4364 current loss 0.077132, current_train_items 139680.
I0302 18:59:53.408632 22760421793920 run.py:483] Algo bellman_ford step 4365 current loss 0.003246, current_train_items 139712.
I0302 18:59:53.424492 22760421793920 run.py:483] Algo bellman_ford step 4366 current loss 0.016826, current_train_items 139744.
I0302 18:59:53.447578 22760421793920 run.py:483] Algo bellman_ford step 4367 current loss 0.058632, current_train_items 139776.
I0302 18:59:53.476381 22760421793920 run.py:483] Algo bellman_ford step 4368 current loss 0.068647, current_train_items 139808.
I0302 18:59:53.508677 22760421793920 run.py:483] Algo bellman_ford step 4369 current loss 0.075322, current_train_items 139840.
I0302 18:59:53.526535 22760421793920 run.py:483] Algo bellman_ford step 4370 current loss 0.001476, current_train_items 139872.
I0302 18:59:53.542260 22760421793920 run.py:483] Algo bellman_ford step 4371 current loss 0.039983, current_train_items 139904.
I0302 18:59:53.564703 22760421793920 run.py:483] Algo bellman_ford step 4372 current loss 0.046773, current_train_items 139936.
I0302 18:59:53.594086 22760421793920 run.py:483] Algo bellman_ford step 4373 current loss 0.067282, current_train_items 139968.
I0302 18:59:53.625584 22760421793920 run.py:483] Algo bellman_ford step 4374 current loss 0.103758, current_train_items 140000.
I0302 18:59:53.643885 22760421793920 run.py:483] Algo bellman_ford step 4375 current loss 0.004821, current_train_items 140032.
I0302 18:59:53.659514 22760421793920 run.py:483] Algo bellman_ford step 4376 current loss 0.013101, current_train_items 140064.
I0302 18:59:53.682348 22760421793920 run.py:483] Algo bellman_ford step 4377 current loss 0.043982, current_train_items 140096.
I0302 18:59:53.711536 22760421793920 run.py:483] Algo bellman_ford step 4378 current loss 0.068816, current_train_items 140128.
I0302 18:59:53.743796 22760421793920 run.py:483] Algo bellman_ford step 4379 current loss 0.100478, current_train_items 140160.
I0302 18:59:53.761595 22760421793920 run.py:483] Algo bellman_ford step 4380 current loss 0.002245, current_train_items 140192.
I0302 18:59:53.777188 22760421793920 run.py:483] Algo bellman_ford step 4381 current loss 0.047722, current_train_items 140224.
I0302 18:59:53.800111 22760421793920 run.py:483] Algo bellman_ford step 4382 current loss 0.073049, current_train_items 140256.
I0302 18:59:53.830070 22760421793920 run.py:483] Algo bellman_ford step 4383 current loss 0.066675, current_train_items 140288.
I0302 18:59:53.862126 22760421793920 run.py:483] Algo bellman_ford step 4384 current loss 0.066652, current_train_items 140320.
I0302 18:59:53.880310 22760421793920 run.py:483] Algo bellman_ford step 4385 current loss 0.010562, current_train_items 140352.
I0302 18:59:53.895991 22760421793920 run.py:483] Algo bellman_ford step 4386 current loss 0.018561, current_train_items 140384.
I0302 18:59:53.918951 22760421793920 run.py:483] Algo bellman_ford step 4387 current loss 0.078740, current_train_items 140416.
I0302 18:59:53.949174 22760421793920 run.py:483] Algo bellman_ford step 4388 current loss 0.084403, current_train_items 140448.
I0302 18:59:53.979627 22760421793920 run.py:483] Algo bellman_ford step 4389 current loss 0.053350, current_train_items 140480.
I0302 18:59:53.997889 22760421793920 run.py:483] Algo bellman_ford step 4390 current loss 0.002682, current_train_items 140512.
I0302 18:59:54.013365 22760421793920 run.py:483] Algo bellman_ford step 4391 current loss 0.012240, current_train_items 140544.
I0302 18:59:54.035911 22760421793920 run.py:483] Algo bellman_ford step 4392 current loss 0.068493, current_train_items 140576.
I0302 18:59:54.065602 22760421793920 run.py:483] Algo bellman_ford step 4393 current loss 0.086528, current_train_items 140608.
I0302 18:59:54.097686 22760421793920 run.py:483] Algo bellman_ford step 4394 current loss 0.176668, current_train_items 140640.
I0302 18:59:54.115635 22760421793920 run.py:483] Algo bellman_ford step 4395 current loss 0.001629, current_train_items 140672.
I0302 18:59:54.131699 22760421793920 run.py:483] Algo bellman_ford step 4396 current loss 0.023856, current_train_items 140704.
I0302 18:59:54.154802 22760421793920 run.py:483] Algo bellman_ford step 4397 current loss 0.050914, current_train_items 140736.
I0302 18:59:54.184444 22760421793920 run.py:483] Algo bellman_ford step 4398 current loss 0.078006, current_train_items 140768.
I0302 18:59:54.215936 22760421793920 run.py:483] Algo bellman_ford step 4399 current loss 0.146960, current_train_items 140800.
I0302 18:59:54.233878 22760421793920 run.py:483] Algo bellman_ford step 4400 current loss 0.002565, current_train_items 140832.
I0302 18:59:54.241264 22760421793920 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 18:59:54.241374 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:54.258285 22760421793920 run.py:483] Algo bellman_ford step 4401 current loss 0.012807, current_train_items 140864.
I0302 18:59:54.281417 22760421793920 run.py:483] Algo bellman_ford step 4402 current loss 0.124538, current_train_items 140896.
I0302 18:59:54.311404 22760421793920 run.py:483] Algo bellman_ford step 4403 current loss 0.109664, current_train_items 140928.
I0302 18:59:54.348053 22760421793920 run.py:483] Algo bellman_ford step 4404 current loss 0.255479, current_train_items 140960.
I0302 18:59:54.366957 22760421793920 run.py:483] Algo bellman_ford step 4405 current loss 0.021061, current_train_items 140992.
I0302 18:59:54.382719 22760421793920 run.py:483] Algo bellman_ford step 4406 current loss 0.033785, current_train_items 141024.
I0302 18:59:54.405844 22760421793920 run.py:483] Algo bellman_ford step 4407 current loss 0.058394, current_train_items 141056.
I0302 18:59:54.436131 22760421793920 run.py:483] Algo bellman_ford step 4408 current loss 0.089767, current_train_items 141088.
I0302 18:59:54.469966 22760421793920 run.py:483] Algo bellman_ford step 4409 current loss 0.099530, current_train_items 141120.
I0302 18:59:54.488511 22760421793920 run.py:483] Algo bellman_ford step 4410 current loss 0.003457, current_train_items 141152.
I0302 18:59:54.504249 22760421793920 run.py:483] Algo bellman_ford step 4411 current loss 0.021663, current_train_items 141184.
I0302 18:59:54.527467 22760421793920 run.py:483] Algo bellman_ford step 4412 current loss 0.037019, current_train_items 141216.
I0302 18:59:54.558941 22760421793920 run.py:483] Algo bellman_ford step 4413 current loss 0.062651, current_train_items 141248.
I0302 18:59:54.590273 22760421793920 run.py:483] Algo bellman_ford step 4414 current loss 0.046452, current_train_items 141280.
I0302 18:59:54.608671 22760421793920 run.py:483] Algo bellman_ford step 4415 current loss 0.004244, current_train_items 141312.
I0302 18:59:54.624868 22760421793920 run.py:483] Algo bellman_ford step 4416 current loss 0.014531, current_train_items 141344.
I0302 18:59:54.647917 22760421793920 run.py:483] Algo bellman_ford step 4417 current loss 0.067490, current_train_items 141376.
I0302 18:59:54.677804 22760421793920 run.py:483] Algo bellman_ford step 4418 current loss 0.070632, current_train_items 141408.
I0302 18:59:54.709665 22760421793920 run.py:483] Algo bellman_ford step 4419 current loss 0.076201, current_train_items 141440.
I0302 18:59:54.727824 22760421793920 run.py:483] Algo bellman_ford step 4420 current loss 0.031457, current_train_items 141472.
I0302 18:59:54.743242 22760421793920 run.py:483] Algo bellman_ford step 4421 current loss 0.027686, current_train_items 141504.
I0302 18:59:54.766323 22760421793920 run.py:483] Algo bellman_ford step 4422 current loss 0.031826, current_train_items 141536.
I0302 18:59:54.796144 22760421793920 run.py:483] Algo bellman_ford step 4423 current loss 0.033195, current_train_items 141568.
I0302 18:59:54.829047 22760421793920 run.py:483] Algo bellman_ford step 4424 current loss 0.077097, current_train_items 141600.
I0302 18:59:54.847434 22760421793920 run.py:483] Algo bellman_ford step 4425 current loss 0.036289, current_train_items 141632.
I0302 18:59:54.862295 22760421793920 run.py:483] Algo bellman_ford step 4426 current loss 0.027558, current_train_items 141664.
I0302 18:59:54.885945 22760421793920 run.py:483] Algo bellman_ford step 4427 current loss 0.052547, current_train_items 141696.
I0302 18:59:54.916377 22760421793920 run.py:483] Algo bellman_ford step 4428 current loss 0.036797, current_train_items 141728.
I0302 18:59:54.948203 22760421793920 run.py:483] Algo bellman_ford step 4429 current loss 0.039167, current_train_items 141760.
I0302 18:59:54.966286 22760421793920 run.py:483] Algo bellman_ford step 4430 current loss 0.002767, current_train_items 141792.
I0302 18:59:54.981657 22760421793920 run.py:483] Algo bellman_ford step 4431 current loss 0.022634, current_train_items 141824.
I0302 18:59:55.005035 22760421793920 run.py:483] Algo bellman_ford step 4432 current loss 0.046058, current_train_items 141856.
I0302 18:59:55.033910 22760421793920 run.py:483] Algo bellman_ford step 4433 current loss 0.046005, current_train_items 141888.
I0302 18:59:55.066108 22760421793920 run.py:483] Algo bellman_ford step 4434 current loss 0.072539, current_train_items 141920.
I0302 18:59:55.084561 22760421793920 run.py:483] Algo bellman_ford step 4435 current loss 0.002501, current_train_items 141952.
I0302 18:59:55.100332 22760421793920 run.py:483] Algo bellman_ford step 4436 current loss 0.070218, current_train_items 141984.
I0302 18:59:55.123893 22760421793920 run.py:483] Algo bellman_ford step 4437 current loss 0.059895, current_train_items 142016.
I0302 18:59:55.152826 22760421793920 run.py:483] Algo bellman_ford step 4438 current loss 0.063953, current_train_items 142048.
I0302 18:59:55.186017 22760421793920 run.py:483] Algo bellman_ford step 4439 current loss 0.114384, current_train_items 142080.
I0302 18:59:55.204431 22760421793920 run.py:483] Algo bellman_ford step 4440 current loss 0.007051, current_train_items 142112.
I0302 18:59:55.220449 22760421793920 run.py:483] Algo bellman_ford step 4441 current loss 0.028889, current_train_items 142144.
I0302 18:59:55.243099 22760421793920 run.py:483] Algo bellman_ford step 4442 current loss 0.072151, current_train_items 142176.
I0302 18:59:55.272710 22760421793920 run.py:483] Algo bellman_ford step 4443 current loss 0.075355, current_train_items 142208.
I0302 18:59:55.306097 22760421793920 run.py:483] Algo bellman_ford step 4444 current loss 0.087824, current_train_items 142240.
I0302 18:59:55.324279 22760421793920 run.py:483] Algo bellman_ford step 4445 current loss 0.004994, current_train_items 142272.
I0302 18:59:55.339723 22760421793920 run.py:483] Algo bellman_ford step 4446 current loss 0.032228, current_train_items 142304.
I0302 18:59:55.362984 22760421793920 run.py:483] Algo bellman_ford step 4447 current loss 0.064036, current_train_items 142336.
I0302 18:59:55.393644 22760421793920 run.py:483] Algo bellman_ford step 4448 current loss 0.135690, current_train_items 142368.
I0302 18:59:55.425958 22760421793920 run.py:483] Algo bellman_ford step 4449 current loss 0.115113, current_train_items 142400.
I0302 18:59:55.444327 22760421793920 run.py:483] Algo bellman_ford step 4450 current loss 0.046447, current_train_items 142432.
I0302 18:59:55.451995 22760421793920 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 18:59:55.452105 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:55.467688 22760421793920 run.py:483] Algo bellman_ford step 4451 current loss 0.007416, current_train_items 142464.
I0302 18:59:55.491101 22760421793920 run.py:483] Algo bellman_ford step 4452 current loss 0.039789, current_train_items 142496.
I0302 18:59:55.520059 22760421793920 run.py:483] Algo bellman_ford step 4453 current loss 0.061627, current_train_items 142528.
I0302 18:59:55.550958 22760421793920 run.py:483] Algo bellman_ford step 4454 current loss 0.062301, current_train_items 142560.
I0302 18:59:55.569212 22760421793920 run.py:483] Algo bellman_ford step 4455 current loss 0.009182, current_train_items 142592.
I0302 18:59:55.585167 22760421793920 run.py:483] Algo bellman_ford step 4456 current loss 0.016739, current_train_items 142624.
I0302 18:59:55.607949 22760421793920 run.py:483] Algo bellman_ford step 4457 current loss 0.078200, current_train_items 142656.
I0302 18:59:55.636166 22760421793920 run.py:483] Algo bellman_ford step 4458 current loss 0.057411, current_train_items 142688.
I0302 18:59:55.670995 22760421793920 run.py:483] Algo bellman_ford step 4459 current loss 0.083199, current_train_items 142720.
I0302 18:59:55.689142 22760421793920 run.py:483] Algo bellman_ford step 4460 current loss 0.005852, current_train_items 142752.
I0302 18:59:55.704853 22760421793920 run.py:483] Algo bellman_ford step 4461 current loss 0.030159, current_train_items 142784.
I0302 18:59:55.728786 22760421793920 run.py:483] Algo bellman_ford step 4462 current loss 0.057922, current_train_items 142816.
I0302 18:59:55.757140 22760421793920 run.py:483] Algo bellman_ford step 4463 current loss 0.040182, current_train_items 142848.
I0302 18:59:55.789479 22760421793920 run.py:483] Algo bellman_ford step 4464 current loss 0.080823, current_train_items 142880.
I0302 18:59:55.807795 22760421793920 run.py:483] Algo bellman_ford step 4465 current loss 0.001855, current_train_items 142912.
I0302 18:59:55.823615 22760421793920 run.py:483] Algo bellman_ford step 4466 current loss 0.019535, current_train_items 142944.
I0302 18:59:55.846191 22760421793920 run.py:483] Algo bellman_ford step 4467 current loss 0.062993, current_train_items 142976.
I0302 18:59:55.877468 22760421793920 run.py:483] Algo bellman_ford step 4468 current loss 0.074055, current_train_items 143008.
I0302 18:59:55.910064 22760421793920 run.py:483] Algo bellman_ford step 4469 current loss 0.103218, current_train_items 143040.
I0302 18:59:55.928100 22760421793920 run.py:483] Algo bellman_ford step 4470 current loss 0.010399, current_train_items 143072.
I0302 18:59:55.943475 22760421793920 run.py:483] Algo bellman_ford step 4471 current loss 0.005466, current_train_items 143104.
I0302 18:59:55.966203 22760421793920 run.py:483] Algo bellman_ford step 4472 current loss 0.084900, current_train_items 143136.
I0302 18:59:55.996027 22760421793920 run.py:483] Algo bellman_ford step 4473 current loss 0.125844, current_train_items 143168.
I0302 18:59:56.030344 22760421793920 run.py:483] Algo bellman_ford step 4474 current loss 0.219999, current_train_items 143200.
I0302 18:59:56.048404 22760421793920 run.py:483] Algo bellman_ford step 4475 current loss 0.002744, current_train_items 143232.
I0302 18:59:56.063935 22760421793920 run.py:483] Algo bellman_ford step 4476 current loss 0.034545, current_train_items 143264.
I0302 18:59:56.086376 22760421793920 run.py:483] Algo bellman_ford step 4477 current loss 0.057438, current_train_items 143296.
I0302 18:59:56.116100 22760421793920 run.py:483] Algo bellman_ford step 4478 current loss 0.071164, current_train_items 143328.
I0302 18:59:56.149671 22760421793920 run.py:483] Algo bellman_ford step 4479 current loss 0.110265, current_train_items 143360.
I0302 18:59:56.167584 22760421793920 run.py:483] Algo bellman_ford step 4480 current loss 0.000810, current_train_items 143392.
I0302 18:59:56.183199 22760421793920 run.py:483] Algo bellman_ford step 4481 current loss 0.029845, current_train_items 143424.
I0302 18:59:56.206114 22760421793920 run.py:483] Algo bellman_ford step 4482 current loss 0.042009, current_train_items 143456.
I0302 18:59:56.236231 22760421793920 run.py:483] Algo bellman_ford step 4483 current loss 0.060060, current_train_items 143488.
I0302 18:59:56.266001 22760421793920 run.py:483] Algo bellman_ford step 4484 current loss 0.054941, current_train_items 143520.
I0302 18:59:56.284108 22760421793920 run.py:483] Algo bellman_ford step 4485 current loss 0.005798, current_train_items 143552.
I0302 18:59:56.299986 22760421793920 run.py:483] Algo bellman_ford step 4486 current loss 0.013171, current_train_items 143584.
I0302 18:59:56.324200 22760421793920 run.py:483] Algo bellman_ford step 4487 current loss 0.065634, current_train_items 143616.
I0302 18:59:56.354384 22760421793920 run.py:483] Algo bellman_ford step 4488 current loss 0.038396, current_train_items 143648.
I0302 18:59:56.385725 22760421793920 run.py:483] Algo bellman_ford step 4489 current loss 0.071031, current_train_items 143680.
I0302 18:59:56.403973 22760421793920 run.py:483] Algo bellman_ford step 4490 current loss 0.021516, current_train_items 143712.
I0302 18:59:56.419212 22760421793920 run.py:483] Algo bellman_ford step 4491 current loss 0.018770, current_train_items 143744.
I0302 18:59:56.442670 22760421793920 run.py:483] Algo bellman_ford step 4492 current loss 0.040109, current_train_items 143776.
I0302 18:59:56.472588 22760421793920 run.py:483] Algo bellman_ford step 4493 current loss 0.054830, current_train_items 143808.
I0302 18:59:56.505522 22760421793920 run.py:483] Algo bellman_ford step 4494 current loss 0.078038, current_train_items 143840.
I0302 18:59:56.523600 22760421793920 run.py:483] Algo bellman_ford step 4495 current loss 0.002938, current_train_items 143872.
I0302 18:59:56.539759 22760421793920 run.py:483] Algo bellman_ford step 4496 current loss 0.049848, current_train_items 143904.
I0302 18:59:56.562751 22760421793920 run.py:483] Algo bellman_ford step 4497 current loss 0.043409, current_train_items 143936.
I0302 18:59:56.591422 22760421793920 run.py:483] Algo bellman_ford step 4498 current loss 0.059244, current_train_items 143968.
I0302 18:59:56.621151 22760421793920 run.py:483] Algo bellman_ford step 4499 current loss 0.033829, current_train_items 144000.
I0302 18:59:56.639199 22760421793920 run.py:483] Algo bellman_ford step 4500 current loss 0.001915, current_train_items 144032.
I0302 18:59:56.646766 22760421793920 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 18:59:56.646878 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:59:56.663155 22760421793920 run.py:483] Algo bellman_ford step 4501 current loss 0.071434, current_train_items 144064.
I0302 18:59:56.687299 22760421793920 run.py:483] Algo bellman_ford step 4502 current loss 0.112611, current_train_items 144096.
I0302 18:59:56.717168 22760421793920 run.py:483] Algo bellman_ford step 4503 current loss 0.117562, current_train_items 144128.
I0302 18:59:56.749992 22760421793920 run.py:483] Algo bellman_ford step 4504 current loss 0.062234, current_train_items 144160.
I0302 18:59:56.768464 22760421793920 run.py:483] Algo bellman_ford step 4505 current loss 0.013177, current_train_items 144192.
I0302 18:59:56.784165 22760421793920 run.py:483] Algo bellman_ford step 4506 current loss 0.051624, current_train_items 144224.
I0302 18:59:56.806524 22760421793920 run.py:483] Algo bellman_ford step 4507 current loss 0.082100, current_train_items 144256.
I0302 18:59:56.835563 22760421793920 run.py:483] Algo bellman_ford step 4508 current loss 0.055239, current_train_items 144288.
I0302 18:59:56.866673 22760421793920 run.py:483] Algo bellman_ford step 4509 current loss 0.066358, current_train_items 144320.
I0302 18:59:56.885008 22760421793920 run.py:483] Algo bellman_ford step 4510 current loss 0.039710, current_train_items 144352.
I0302 18:59:56.900886 22760421793920 run.py:483] Algo bellman_ford step 4511 current loss 0.041261, current_train_items 144384.
I0302 18:59:56.924139 22760421793920 run.py:483] Algo bellman_ford step 4512 current loss 0.097597, current_train_items 144416.
I0302 18:59:56.953862 22760421793920 run.py:483] Algo bellman_ford step 4513 current loss 0.081427, current_train_items 144448.
I0302 18:59:56.985211 22760421793920 run.py:483] Algo bellman_ford step 4514 current loss 0.063408, current_train_items 144480.
I0302 18:59:57.003117 22760421793920 run.py:483] Algo bellman_ford step 4515 current loss 0.010666, current_train_items 144512.
I0302 18:59:57.019068 22760421793920 run.py:483] Algo bellman_ford step 4516 current loss 0.017772, current_train_items 144544.
I0302 18:59:57.040894 22760421793920 run.py:483] Algo bellman_ford step 4517 current loss 0.114176, current_train_items 144576.
I0302 18:59:57.071603 22760421793920 run.py:483] Algo bellman_ford step 4518 current loss 0.095180, current_train_items 144608.
I0302 18:59:57.106556 22760421793920 run.py:483] Algo bellman_ford step 4519 current loss 0.098998, current_train_items 144640.
I0302 18:59:57.124537 22760421793920 run.py:483] Algo bellman_ford step 4520 current loss 0.002341, current_train_items 144672.
I0302 18:59:57.140452 22760421793920 run.py:483] Algo bellman_ford step 4521 current loss 0.005522, current_train_items 144704.
I0302 18:59:57.162920 22760421793920 run.py:483] Algo bellman_ford step 4522 current loss 0.028280, current_train_items 144736.
I0302 18:59:57.194289 22760421793920 run.py:483] Algo bellman_ford step 4523 current loss 0.072036, current_train_items 144768.
I0302 18:59:57.226138 22760421793920 run.py:483] Algo bellman_ford step 4524 current loss 0.056061, current_train_items 144800.
I0302 18:59:57.244007 22760421793920 run.py:483] Algo bellman_ford step 4525 current loss 0.003237, current_train_items 144832.
I0302 18:59:57.259745 22760421793920 run.py:483] Algo bellman_ford step 4526 current loss 0.006390, current_train_items 144864.
I0302 18:59:57.283247 22760421793920 run.py:483] Algo bellman_ford step 4527 current loss 0.059513, current_train_items 144896.
I0302 18:59:57.313270 22760421793920 run.py:483] Algo bellman_ford step 4528 current loss 0.047083, current_train_items 144928.
I0302 18:59:57.345581 22760421793920 run.py:483] Algo bellman_ford step 4529 current loss 0.078436, current_train_items 144960.
I0302 18:59:57.363750 22760421793920 run.py:483] Algo bellman_ford step 4530 current loss 0.011111, current_train_items 144992.
I0302 18:59:57.379149 22760421793920 run.py:483] Algo bellman_ford step 4531 current loss 0.007770, current_train_items 145024.
I0302 18:59:57.401209 22760421793920 run.py:483] Algo bellman_ford step 4532 current loss 0.045281, current_train_items 145056.
I0302 18:59:57.429182 22760421793920 run.py:483] Algo bellman_ford step 4533 current loss 0.035074, current_train_items 145088.
I0302 18:59:57.462431 22760421793920 run.py:483] Algo bellman_ford step 4534 current loss 0.110602, current_train_items 145120.
I0302 18:59:57.480515 22760421793920 run.py:483] Algo bellman_ford step 4535 current loss 0.003149, current_train_items 145152.
I0302 18:59:57.496172 22760421793920 run.py:483] Algo bellman_ford step 4536 current loss 0.014014, current_train_items 145184.
I0302 18:59:57.519184 22760421793920 run.py:483] Algo bellman_ford step 4537 current loss 0.066735, current_train_items 145216.
I0302 18:59:57.548795 22760421793920 run.py:483] Algo bellman_ford step 4538 current loss 0.028566, current_train_items 145248.
I0302 18:59:57.582265 22760421793920 run.py:483] Algo bellman_ford step 4539 current loss 0.097639, current_train_items 145280.
I0302 18:59:57.600501 22760421793920 run.py:483] Algo bellman_ford step 4540 current loss 0.012556, current_train_items 145312.
I0302 18:59:57.616134 22760421793920 run.py:483] Algo bellman_ford step 4541 current loss 0.037500, current_train_items 145344.
I0302 18:59:57.638211 22760421793920 run.py:483] Algo bellman_ford step 4542 current loss 0.057053, current_train_items 145376.
I0302 18:59:57.667922 22760421793920 run.py:483] Algo bellman_ford step 4543 current loss 0.114424, current_train_items 145408.
I0302 18:59:57.700643 22760421793920 run.py:483] Algo bellman_ford step 4544 current loss 0.175066, current_train_items 145440.
I0302 18:59:57.718740 22760421793920 run.py:483] Algo bellman_ford step 4545 current loss 0.010031, current_train_items 145472.
I0302 18:59:57.734345 22760421793920 run.py:483] Algo bellman_ford step 4546 current loss 0.031509, current_train_items 145504.
I0302 18:59:57.756593 22760421793920 run.py:483] Algo bellman_ford step 4547 current loss 0.038289, current_train_items 145536.
I0302 18:59:57.785561 22760421793920 run.py:483] Algo bellman_ford step 4548 current loss 0.121220, current_train_items 145568.
I0302 18:59:57.818457 22760421793920 run.py:483] Algo bellman_ford step 4549 current loss 0.157413, current_train_items 145600.
I0302 18:59:57.836501 22760421793920 run.py:483] Algo bellman_ford step 4550 current loss 0.002845, current_train_items 145632.
I0302 18:59:57.843864 22760421793920 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 18:59:57.843984 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:59:57.860529 22760421793920 run.py:483] Algo bellman_ford step 4551 current loss 0.022241, current_train_items 145664.
I0302 18:59:57.884160 22760421793920 run.py:483] Algo bellman_ford step 4552 current loss 0.029701, current_train_items 145696.
I0302 18:59:57.915322 22760421793920 run.py:483] Algo bellman_ford step 4553 current loss 0.092120, current_train_items 145728.
I0302 18:59:57.947896 22760421793920 run.py:483] Algo bellman_ford step 4554 current loss 0.067767, current_train_items 145760.
I0302 18:59:57.966886 22760421793920 run.py:483] Algo bellman_ford step 4555 current loss 0.007108, current_train_items 145792.
I0302 18:59:57.982542 22760421793920 run.py:483] Algo bellman_ford step 4556 current loss 0.037364, current_train_items 145824.
I0302 18:59:58.004766 22760421793920 run.py:483] Algo bellman_ford step 4557 current loss 0.112675, current_train_items 145856.
I0302 18:59:58.036136 22760421793920 run.py:483] Algo bellman_ford step 4558 current loss 0.077122, current_train_items 145888.
I0302 18:59:58.069684 22760421793920 run.py:483] Algo bellman_ford step 4559 current loss 0.118432, current_train_items 145920.
I0302 18:59:58.088073 22760421793920 run.py:483] Algo bellman_ford step 4560 current loss 0.009312, current_train_items 145952.
I0302 18:59:58.103953 22760421793920 run.py:483] Algo bellman_ford step 4561 current loss 0.043191, current_train_items 145984.
I0302 18:59:58.126675 22760421793920 run.py:483] Algo bellman_ford step 4562 current loss 0.104919, current_train_items 146016.
I0302 18:59:58.155963 22760421793920 run.py:483] Algo bellman_ford step 4563 current loss 0.255809, current_train_items 146048.
I0302 18:59:58.189538 22760421793920 run.py:483] Algo bellman_ford step 4564 current loss 0.149276, current_train_items 146080.
I0302 18:59:58.207937 22760421793920 run.py:483] Algo bellman_ford step 4565 current loss 0.007848, current_train_items 146112.
I0302 18:59:58.223678 22760421793920 run.py:483] Algo bellman_ford step 4566 current loss 0.030825, current_train_items 146144.
I0302 18:59:58.247198 22760421793920 run.py:483] Algo bellman_ford step 4567 current loss 0.042647, current_train_items 146176.
I0302 18:59:58.278279 22760421793920 run.py:483] Algo bellman_ford step 4568 current loss 0.073486, current_train_items 146208.
I0302 18:59:58.309669 22760421793920 run.py:483] Algo bellman_ford step 4569 current loss 0.130251, current_train_items 146240.
I0302 18:59:58.327665 22760421793920 run.py:483] Algo bellman_ford step 4570 current loss 0.005683, current_train_items 146272.
I0302 18:59:58.343230 22760421793920 run.py:483] Algo bellman_ford step 4571 current loss 0.012257, current_train_items 146304.
I0302 18:59:58.366744 22760421793920 run.py:483] Algo bellman_ford step 4572 current loss 0.055473, current_train_items 146336.
I0302 18:59:58.396082 22760421793920 run.py:483] Algo bellman_ford step 4573 current loss 0.030921, current_train_items 146368.
I0302 18:59:58.426943 22760421793920 run.py:483] Algo bellman_ford step 4574 current loss 0.068196, current_train_items 146400.
I0302 18:59:58.444947 22760421793920 run.py:483] Algo bellman_ford step 4575 current loss 0.004741, current_train_items 146432.
I0302 18:59:58.460535 22760421793920 run.py:483] Algo bellman_ford step 4576 current loss 0.018810, current_train_items 146464.
I0302 18:59:58.483514 22760421793920 run.py:483] Algo bellman_ford step 4577 current loss 0.033205, current_train_items 146496.
I0302 18:59:58.513782 22760421793920 run.py:483] Algo bellman_ford step 4578 current loss 0.050530, current_train_items 146528.
I0302 18:59:58.546241 22760421793920 run.py:483] Algo bellman_ford step 4579 current loss 0.088002, current_train_items 146560.
I0302 18:59:58.564241 22760421793920 run.py:483] Algo bellman_ford step 4580 current loss 0.027905, current_train_items 146592.
I0302 18:59:58.580326 22760421793920 run.py:483] Algo bellman_ford step 4581 current loss 0.024979, current_train_items 146624.
I0302 18:59:58.603184 22760421793920 run.py:483] Algo bellman_ford step 4582 current loss 0.071611, current_train_items 146656.
I0302 18:59:58.633546 22760421793920 run.py:483] Algo bellman_ford step 4583 current loss 0.061089, current_train_items 146688.
I0302 18:59:58.665979 22760421793920 run.py:483] Algo bellman_ford step 4584 current loss 0.107444, current_train_items 146720.
I0302 18:59:58.684425 22760421793920 run.py:483] Algo bellman_ford step 4585 current loss 0.006038, current_train_items 146752.
I0302 18:59:58.700411 22760421793920 run.py:483] Algo bellman_ford step 4586 current loss 0.061694, current_train_items 146784.
I0302 18:59:58.722755 22760421793920 run.py:483] Algo bellman_ford step 4587 current loss 0.097048, current_train_items 146816.
I0302 18:59:58.752682 22760421793920 run.py:483] Algo bellman_ford step 4588 current loss 0.108405, current_train_items 146848.
I0302 18:59:58.783910 22760421793920 run.py:483] Algo bellman_ford step 4589 current loss 0.118108, current_train_items 146880.
I0302 18:59:58.802477 22760421793920 run.py:483] Algo bellman_ford step 4590 current loss 0.010768, current_train_items 146912.
I0302 18:59:58.818134 22760421793920 run.py:483] Algo bellman_ford step 4591 current loss 0.032233, current_train_items 146944.
I0302 18:59:58.840851 22760421793920 run.py:483] Algo bellman_ford step 4592 current loss 0.113541, current_train_items 146976.
I0302 18:59:58.871689 22760421793920 run.py:483] Algo bellman_ford step 4593 current loss 0.235761, current_train_items 147008.
I0302 18:59:58.904805 22760421793920 run.py:483] Algo bellman_ford step 4594 current loss 0.325594, current_train_items 147040.
I0302 18:59:58.923130 22760421793920 run.py:483] Algo bellman_ford step 4595 current loss 0.015541, current_train_items 147072.
I0302 18:59:58.938828 22760421793920 run.py:483] Algo bellman_ford step 4596 current loss 0.051284, current_train_items 147104.
I0302 18:59:58.963732 22760421793920 run.py:483] Algo bellman_ford step 4597 current loss 0.085643, current_train_items 147136.
I0302 18:59:58.993739 22760421793920 run.py:483] Algo bellman_ford step 4598 current loss 0.082657, current_train_items 147168.
I0302 18:59:59.024255 22760421793920 run.py:483] Algo bellman_ford step 4599 current loss 0.163773, current_train_items 147200.
I0302 18:59:59.042520 22760421793920 run.py:483] Algo bellman_ford step 4600 current loss 0.012807, current_train_items 147232.
I0302 18:59:59.050168 22760421793920 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 18:59:59.050278 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:59.066846 22760421793920 run.py:483] Algo bellman_ford step 4601 current loss 0.059538, current_train_items 147264.
I0302 18:59:59.090959 22760421793920 run.py:483] Algo bellman_ford step 4602 current loss 0.102353, current_train_items 147296.
I0302 18:59:59.120813 22760421793920 run.py:483] Algo bellman_ford step 4603 current loss 0.058085, current_train_items 147328.
I0302 18:59:59.153700 22760421793920 run.py:483] Algo bellman_ford step 4604 current loss 0.112828, current_train_items 147360.
I0302 18:59:59.171921 22760421793920 run.py:483] Algo bellman_ford step 4605 current loss 0.005766, current_train_items 147392.
I0302 18:59:59.187756 22760421793920 run.py:483] Algo bellman_ford step 4606 current loss 0.065508, current_train_items 147424.
I0302 18:59:59.210347 22760421793920 run.py:483] Algo bellman_ford step 4607 current loss 0.047292, current_train_items 147456.
I0302 18:59:59.241081 22760421793920 run.py:483] Algo bellman_ford step 4608 current loss 0.136343, current_train_items 147488.
I0302 18:59:59.271616 22760421793920 run.py:483] Algo bellman_ford step 4609 current loss 0.143576, current_train_items 147520.
I0302 18:59:59.289458 22760421793920 run.py:483] Algo bellman_ford step 4610 current loss 0.020490, current_train_items 147552.
I0302 18:59:59.304978 22760421793920 run.py:483] Algo bellman_ford step 4611 current loss 0.056825, current_train_items 147584.
I0302 18:59:59.328145 22760421793920 run.py:483] Algo bellman_ford step 4612 current loss 0.148521, current_train_items 147616.
I0302 18:59:59.358489 22760421793920 run.py:483] Algo bellman_ford step 4613 current loss 0.196994, current_train_items 147648.
I0302 18:59:59.391337 22760421793920 run.py:483] Algo bellman_ford step 4614 current loss 0.222296, current_train_items 147680.
I0302 18:59:59.409524 22760421793920 run.py:483] Algo bellman_ford step 4615 current loss 0.098522, current_train_items 147712.
I0302 18:59:59.425391 22760421793920 run.py:483] Algo bellman_ford step 4616 current loss 0.089502, current_train_items 147744.
I0302 18:59:59.448610 22760421793920 run.py:483] Algo bellman_ford step 4617 current loss 0.042824, current_train_items 147776.
I0302 18:59:59.477714 22760421793920 run.py:483] Algo bellman_ford step 4618 current loss 0.059333, current_train_items 147808.
I0302 18:59:59.509534 22760421793920 run.py:483] Algo bellman_ford step 4619 current loss 0.093598, current_train_items 147840.
I0302 18:59:59.527448 22760421793920 run.py:483] Algo bellman_ford step 4620 current loss 0.018383, current_train_items 147872.
I0302 18:59:59.542905 22760421793920 run.py:483] Algo bellman_ford step 4621 current loss 0.028573, current_train_items 147904.
I0302 18:59:59.565559 22760421793920 run.py:483] Algo bellman_ford step 4622 current loss 0.165617, current_train_items 147936.
I0302 18:59:59.595383 22760421793920 run.py:483] Algo bellman_ford step 4623 current loss 0.082214, current_train_items 147968.
I0302 18:59:59.626705 22760421793920 run.py:483] Algo bellman_ford step 4624 current loss 0.171533, current_train_items 148000.
I0302 18:59:59.644757 22760421793920 run.py:483] Algo bellman_ford step 4625 current loss 0.010758, current_train_items 148032.
I0302 18:59:59.660388 22760421793920 run.py:483] Algo bellman_ford step 4626 current loss 0.058985, current_train_items 148064.
I0302 18:59:59.683699 22760421793920 run.py:483] Algo bellman_ford step 4627 current loss 0.088568, current_train_items 148096.
I0302 18:59:59.714332 22760421793920 run.py:483] Algo bellman_ford step 4628 current loss 0.169783, current_train_items 148128.
I0302 18:59:59.747665 22760421793920 run.py:483] Algo bellman_ford step 4629 current loss 0.272530, current_train_items 148160.
I0302 18:59:59.765667 22760421793920 run.py:483] Algo bellman_ford step 4630 current loss 0.005745, current_train_items 148192.
I0302 18:59:59.781721 22760421793920 run.py:483] Algo bellman_ford step 4631 current loss 0.027173, current_train_items 148224.
I0302 18:59:59.805328 22760421793920 run.py:483] Algo bellman_ford step 4632 current loss 0.066258, current_train_items 148256.
I0302 18:59:59.833935 22760421793920 run.py:483] Algo bellman_ford step 4633 current loss 0.049473, current_train_items 148288.
I0302 18:59:59.866397 22760421793920 run.py:483] Algo bellman_ford step 4634 current loss 0.065006, current_train_items 148320.
I0302 18:59:59.884639 22760421793920 run.py:483] Algo bellman_ford step 4635 current loss 0.005598, current_train_items 148352.
I0302 18:59:59.900824 22760421793920 run.py:483] Algo bellman_ford step 4636 current loss 0.009686, current_train_items 148384.
I0302 18:59:59.924120 22760421793920 run.py:483] Algo bellman_ford step 4637 current loss 0.064594, current_train_items 148416.
I0302 18:59:59.955668 22760421793920 run.py:483] Algo bellman_ford step 4638 current loss 0.057504, current_train_items 148448.
I0302 18:59:59.988134 22760421793920 run.py:483] Algo bellman_ford step 4639 current loss 0.088359, current_train_items 148480.
I0302 19:00:00.006242 22760421793920 run.py:483] Algo bellman_ford step 4640 current loss 0.007934, current_train_items 148512.
I0302 19:00:00.021843 22760421793920 run.py:483] Algo bellman_ford step 4641 current loss 0.013680, current_train_items 148544.
I0302 19:00:00.044782 22760421793920 run.py:483] Algo bellman_ford step 4642 current loss 0.122344, current_train_items 148576.
I0302 19:00:00.074815 22760421793920 run.py:483] Algo bellman_ford step 4643 current loss 0.110737, current_train_items 148608.
I0302 19:00:00.107697 22760421793920 run.py:483] Algo bellman_ford step 4644 current loss 0.079935, current_train_items 148640.
I0302 19:00:00.125834 22760421793920 run.py:483] Algo bellman_ford step 4645 current loss 0.005185, current_train_items 148672.
I0302 19:00:00.141575 22760421793920 run.py:483] Algo bellman_ford step 4646 current loss 0.010363, current_train_items 148704.
I0302 19:00:00.163393 22760421793920 run.py:483] Algo bellman_ford step 4647 current loss 0.056748, current_train_items 148736.
I0302 19:00:00.191786 22760421793920 run.py:483] Algo bellman_ford step 4648 current loss 0.061199, current_train_items 148768.
I0302 19:00:00.221835 22760421793920 run.py:483] Algo bellman_ford step 4649 current loss 0.073200, current_train_items 148800.
I0302 19:00:00.239932 22760421793920 run.py:483] Algo bellman_ford step 4650 current loss 0.007375, current_train_items 148832.
I0302 19:00:00.247362 22760421793920 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:00.247472 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:00:00.263995 22760421793920 run.py:483] Algo bellman_ford step 4651 current loss 0.042978, current_train_items 148864.
I0302 19:00:00.287360 22760421793920 run.py:483] Algo bellman_ford step 4652 current loss 0.041134, current_train_items 148896.
I0302 19:00:00.318502 22760421793920 run.py:483] Algo bellman_ford step 4653 current loss 0.123954, current_train_items 148928.
I0302 19:00:00.351027 22760421793920 run.py:483] Algo bellman_ford step 4654 current loss 0.139139, current_train_items 148960.
I0302 19:00:00.369287 22760421793920 run.py:483] Algo bellman_ford step 4655 current loss 0.006850, current_train_items 148992.
I0302 19:00:00.385368 22760421793920 run.py:483] Algo bellman_ford step 4656 current loss 0.026109, current_train_items 149024.
I0302 19:00:00.408376 22760421793920 run.py:483] Algo bellman_ford step 4657 current loss 0.060347, current_train_items 149056.
I0302 19:00:00.438074 22760421793920 run.py:483] Algo bellman_ford step 4658 current loss 0.108907, current_train_items 149088.
I0302 19:00:00.469103 22760421793920 run.py:483] Algo bellman_ford step 4659 current loss 0.072253, current_train_items 149120.
I0302 19:00:00.487277 22760421793920 run.py:483] Algo bellman_ford step 4660 current loss 0.001342, current_train_items 149152.
I0302 19:00:00.503133 22760421793920 run.py:483] Algo bellman_ford step 4661 current loss 0.037627, current_train_items 149184.
I0302 19:00:00.526872 22760421793920 run.py:483] Algo bellman_ford step 4662 current loss 0.124355, current_train_items 149216.
I0302 19:00:00.556115 22760421793920 run.py:483] Algo bellman_ford step 4663 current loss 0.093606, current_train_items 149248.
I0302 19:00:00.591348 22760421793920 run.py:483] Algo bellman_ford step 4664 current loss 0.163547, current_train_items 149280.
I0302 19:00:00.609590 22760421793920 run.py:483] Algo bellman_ford step 4665 current loss 0.013825, current_train_items 149312.
I0302 19:00:00.625074 22760421793920 run.py:483] Algo bellman_ford step 4666 current loss 0.018553, current_train_items 149344.
I0302 19:00:00.647445 22760421793920 run.py:483] Algo bellman_ford step 4667 current loss 0.039772, current_train_items 149376.
I0302 19:00:00.678004 22760421793920 run.py:483] Algo bellman_ford step 4668 current loss 0.062531, current_train_items 149408.
I0302 19:00:00.710610 22760421793920 run.py:483] Algo bellman_ford step 4669 current loss 0.131038, current_train_items 149440.
I0302 19:00:00.728747 22760421793920 run.py:483] Algo bellman_ford step 4670 current loss 0.001927, current_train_items 149472.
I0302 19:00:00.744014 22760421793920 run.py:483] Algo bellman_ford step 4671 current loss 0.021116, current_train_items 149504.
I0302 19:00:00.766156 22760421793920 run.py:483] Algo bellman_ford step 4672 current loss 0.051041, current_train_items 149536.
I0302 19:00:00.794877 22760421793920 run.py:483] Algo bellman_ford step 4673 current loss 0.107234, current_train_items 149568.
I0302 19:00:00.826695 22760421793920 run.py:483] Algo bellman_ford step 4674 current loss 0.079754, current_train_items 149600.
I0302 19:00:00.844793 22760421793920 run.py:483] Algo bellman_ford step 4675 current loss 0.004709, current_train_items 149632.
I0302 19:00:00.860862 22760421793920 run.py:483] Algo bellman_ford step 4676 current loss 0.019954, current_train_items 149664.
I0302 19:00:00.884303 22760421793920 run.py:483] Algo bellman_ford step 4677 current loss 0.049286, current_train_items 149696.
I0302 19:00:00.915838 22760421793920 run.py:483] Algo bellman_ford step 4678 current loss 0.073484, current_train_items 149728.
I0302 19:00:00.947834 22760421793920 run.py:483] Algo bellman_ford step 4679 current loss 0.097414, current_train_items 149760.
I0302 19:00:00.966078 22760421793920 run.py:483] Algo bellman_ford step 4680 current loss 0.002578, current_train_items 149792.
I0302 19:00:00.981506 22760421793920 run.py:483] Algo bellman_ford step 4681 current loss 0.010303, current_train_items 149824.
I0302 19:00:01.003912 22760421793920 run.py:483] Algo bellman_ford step 4682 current loss 0.052050, current_train_items 149856.
I0302 19:00:01.033362 22760421793920 run.py:483] Algo bellman_ford step 4683 current loss 0.062536, current_train_items 149888.
I0302 19:00:01.066638 22760421793920 run.py:483] Algo bellman_ford step 4684 current loss 0.053861, current_train_items 149920.
I0302 19:00:01.084912 22760421793920 run.py:483] Algo bellman_ford step 4685 current loss 0.012925, current_train_items 149952.
I0302 19:00:01.100244 22760421793920 run.py:483] Algo bellman_ford step 4686 current loss 0.006072, current_train_items 149984.
I0302 19:00:01.123262 22760421793920 run.py:483] Algo bellman_ford step 4687 current loss 0.048678, current_train_items 150016.
I0302 19:00:01.153583 22760421793920 run.py:483] Algo bellman_ford step 4688 current loss 0.035502, current_train_items 150048.
I0302 19:00:01.187008 22760421793920 run.py:483] Algo bellman_ford step 4689 current loss 0.098522, current_train_items 150080.
I0302 19:00:01.205150 22760421793920 run.py:483] Algo bellman_ford step 4690 current loss 0.005273, current_train_items 150112.
I0302 19:00:01.220779 22760421793920 run.py:483] Algo bellman_ford step 4691 current loss 0.010062, current_train_items 150144.
I0302 19:00:01.243248 22760421793920 run.py:483] Algo bellman_ford step 4692 current loss 0.037203, current_train_items 150176.
I0302 19:00:01.273368 22760421793920 run.py:483] Algo bellman_ford step 4693 current loss 0.107091, current_train_items 150208.
I0302 19:00:01.304498 22760421793920 run.py:483] Algo bellman_ford step 4694 current loss 0.090246, current_train_items 150240.
I0302 19:00:01.322620 22760421793920 run.py:483] Algo bellman_ford step 4695 current loss 0.004633, current_train_items 150272.
I0302 19:00:01.338345 22760421793920 run.py:483] Algo bellman_ford step 4696 current loss 0.017553, current_train_items 150304.
I0302 19:00:01.361804 22760421793920 run.py:483] Algo bellman_ford step 4697 current loss 0.054514, current_train_items 150336.
I0302 19:00:01.391254 22760421793920 run.py:483] Algo bellman_ford step 4698 current loss 0.044929, current_train_items 150368.
I0302 19:00:01.421551 22760421793920 run.py:483] Algo bellman_ford step 4699 current loss 0.047983, current_train_items 150400.
I0302 19:00:01.439589 22760421793920 run.py:483] Algo bellman_ford step 4700 current loss 0.003438, current_train_items 150432.
I0302 19:00:01.447141 22760421793920 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:01.447250 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:01.463182 22760421793920 run.py:483] Algo bellman_ford step 4701 current loss 0.039855, current_train_items 150464.
I0302 19:00:01.487156 22760421793920 run.py:483] Algo bellman_ford step 4702 current loss 0.082305, current_train_items 150496.
I0302 19:00:01.518607 22760421793920 run.py:483] Algo bellman_ford step 4703 current loss 0.050094, current_train_items 150528.
I0302 19:00:01.552784 22760421793920 run.py:483] Algo bellman_ford step 4704 current loss 0.057971, current_train_items 150560.
I0302 19:00:01.571525 22760421793920 run.py:483] Algo bellman_ford step 4705 current loss 0.022741, current_train_items 150592.
I0302 19:00:01.586889 22760421793920 run.py:483] Algo bellman_ford step 4706 current loss 0.007359, current_train_items 150624.
I0302 19:00:01.610233 22760421793920 run.py:483] Algo bellman_ford step 4707 current loss 0.028699, current_train_items 150656.
I0302 19:00:01.641284 22760421793920 run.py:483] Algo bellman_ford step 4708 current loss 0.076416, current_train_items 150688.
I0302 19:00:01.673212 22760421793920 run.py:483] Algo bellman_ford step 4709 current loss 0.081533, current_train_items 150720.
I0302 19:00:01.691246 22760421793920 run.py:483] Algo bellman_ford step 4710 current loss 0.022403, current_train_items 150752.
I0302 19:00:01.706920 22760421793920 run.py:483] Algo bellman_ford step 4711 current loss 0.018028, current_train_items 150784.
I0302 19:00:01.730459 22760421793920 run.py:483] Algo bellman_ford step 4712 current loss 0.071536, current_train_items 150816.
I0302 19:00:01.760676 22760421793920 run.py:483] Algo bellman_ford step 4713 current loss 0.085358, current_train_items 150848.
I0302 19:00:01.791604 22760421793920 run.py:483] Algo bellman_ford step 4714 current loss 0.079978, current_train_items 150880.
I0302 19:00:01.809930 22760421793920 run.py:483] Algo bellman_ford step 4715 current loss 0.006665, current_train_items 150912.
I0302 19:00:01.825826 22760421793920 run.py:483] Algo bellman_ford step 4716 current loss 0.029612, current_train_items 150944.
I0302 19:00:01.847593 22760421793920 run.py:483] Algo bellman_ford step 4717 current loss 0.043024, current_train_items 150976.
I0302 19:00:01.877827 22760421793920 run.py:483] Algo bellman_ford step 4718 current loss 0.061693, current_train_items 151008.
I0302 19:00:01.912090 22760421793920 run.py:483] Algo bellman_ford step 4719 current loss 0.075236, current_train_items 151040.
I0302 19:00:01.930407 22760421793920 run.py:483] Algo bellman_ford step 4720 current loss 0.003924, current_train_items 151072.
I0302 19:00:01.945840 22760421793920 run.py:483] Algo bellman_ford step 4721 current loss 0.019048, current_train_items 151104.
I0302 19:00:01.969723 22760421793920 run.py:483] Algo bellman_ford step 4722 current loss 0.059110, current_train_items 151136.
I0302 19:00:01.998466 22760421793920 run.py:483] Algo bellman_ford step 4723 current loss 0.047281, current_train_items 151168.
I0302 19:00:02.031230 22760421793920 run.py:483] Algo bellman_ford step 4724 current loss 0.070586, current_train_items 151200.
I0302 19:00:02.049218 22760421793920 run.py:483] Algo bellman_ford step 4725 current loss 0.002278, current_train_items 151232.
I0302 19:00:02.065256 22760421793920 run.py:483] Algo bellman_ford step 4726 current loss 0.029217, current_train_items 151264.
I0302 19:00:02.089461 22760421793920 run.py:483] Algo bellman_ford step 4727 current loss 0.039012, current_train_items 151296.
I0302 19:00:02.118187 22760421793920 run.py:483] Algo bellman_ford step 4728 current loss 0.034654, current_train_items 151328.
I0302 19:00:02.147048 22760421793920 run.py:483] Algo bellman_ford step 4729 current loss 0.081295, current_train_items 151360.
I0302 19:00:02.165028 22760421793920 run.py:483] Algo bellman_ford step 4730 current loss 0.004548, current_train_items 151392.
I0302 19:00:02.180398 22760421793920 run.py:483] Algo bellman_ford step 4731 current loss 0.005737, current_train_items 151424.
I0302 19:00:02.202864 22760421793920 run.py:483] Algo bellman_ford step 4732 current loss 0.045965, current_train_items 151456.
I0302 19:00:02.232968 22760421793920 run.py:483] Algo bellman_ford step 4733 current loss 0.060738, current_train_items 151488.
I0302 19:00:02.265377 22760421793920 run.py:483] Algo bellman_ford step 4734 current loss 0.138498, current_train_items 151520.
I0302 19:00:02.283760 22760421793920 run.py:483] Algo bellman_ford step 4735 current loss 0.001100, current_train_items 151552.
I0302 19:00:02.299752 22760421793920 run.py:483] Algo bellman_ford step 4736 current loss 0.038362, current_train_items 151584.
I0302 19:00:02.322510 22760421793920 run.py:483] Algo bellman_ford step 4737 current loss 0.057577, current_train_items 151616.
I0302 19:00:02.351797 22760421793920 run.py:483] Algo bellman_ford step 4738 current loss 0.065334, current_train_items 151648.
I0302 19:00:02.383312 22760421793920 run.py:483] Algo bellman_ford step 4739 current loss 0.072546, current_train_items 151680.
I0302 19:00:02.401592 22760421793920 run.py:483] Algo bellman_ford step 4740 current loss 0.021849, current_train_items 151712.
I0302 19:00:02.417030 22760421793920 run.py:483] Algo bellman_ford step 4741 current loss 0.031954, current_train_items 151744.
I0302 19:00:02.439351 22760421793920 run.py:483] Algo bellman_ford step 4742 current loss 0.063670, current_train_items 151776.
I0302 19:00:02.469982 22760421793920 run.py:483] Algo bellman_ford step 4743 current loss 0.064661, current_train_items 151808.
I0302 19:00:02.500931 22760421793920 run.py:483] Algo bellman_ford step 4744 current loss 0.100004, current_train_items 151840.
I0302 19:00:02.519361 22760421793920 run.py:483] Algo bellman_ford step 4745 current loss 0.017055, current_train_items 151872.
I0302 19:00:02.535538 22760421793920 run.py:483] Algo bellman_ford step 4746 current loss 0.037816, current_train_items 151904.
I0302 19:00:02.559705 22760421793920 run.py:483] Algo bellman_ford step 4747 current loss 0.048715, current_train_items 151936.
I0302 19:00:02.589825 22760421793920 run.py:483] Algo bellman_ford step 4748 current loss 0.074656, current_train_items 151968.
I0302 19:00:02.621427 22760421793920 run.py:483] Algo bellman_ford step 4749 current loss 0.108340, current_train_items 152000.
I0302 19:00:02.639692 22760421793920 run.py:483] Algo bellman_ford step 4750 current loss 0.005888, current_train_items 152032.
I0302 19:00:02.647349 22760421793920 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:02.647493 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:02.663771 22760421793920 run.py:483] Algo bellman_ford step 4751 current loss 0.018942, current_train_items 152064.
I0302 19:00:02.687786 22760421793920 run.py:483] Algo bellman_ford step 4752 current loss 0.061804, current_train_items 152096.
I0302 19:00:02.719700 22760421793920 run.py:483] Algo bellman_ford step 4753 current loss 0.101960, current_train_items 152128.
I0302 19:00:02.751703 22760421793920 run.py:483] Algo bellman_ford step 4754 current loss 0.049278, current_train_items 152160.
I0302 19:00:02.770179 22760421793920 run.py:483] Algo bellman_ford step 4755 current loss 0.002968, current_train_items 152192.
I0302 19:00:02.786238 22760421793920 run.py:483] Algo bellman_ford step 4756 current loss 0.003199, current_train_items 152224.
I0302 19:00:02.809573 22760421793920 run.py:483] Algo bellman_ford step 4757 current loss 0.085295, current_train_items 152256.
I0302 19:00:02.839881 22760421793920 run.py:483] Algo bellman_ford step 4758 current loss 0.070521, current_train_items 152288.
I0302 19:00:02.872815 22760421793920 run.py:483] Algo bellman_ford step 4759 current loss 0.091265, current_train_items 152320.
I0302 19:00:02.891195 22760421793920 run.py:483] Algo bellman_ford step 4760 current loss 0.004251, current_train_items 152352.
I0302 19:00:02.907018 22760421793920 run.py:483] Algo bellman_ford step 4761 current loss 0.017132, current_train_items 152384.
I0302 19:00:02.930337 22760421793920 run.py:483] Algo bellman_ford step 4762 current loss 0.091243, current_train_items 152416.
I0302 19:00:02.960088 22760421793920 run.py:483] Algo bellman_ford step 4763 current loss 0.041602, current_train_items 152448.
I0302 19:00:02.992779 22760421793920 run.py:483] Algo bellman_ford step 4764 current loss 0.089451, current_train_items 152480.
I0302 19:00:03.011261 22760421793920 run.py:483] Algo bellman_ford step 4765 current loss 0.013478, current_train_items 152512.
I0302 19:00:03.026508 22760421793920 run.py:483] Algo bellman_ford step 4766 current loss 0.023634, current_train_items 152544.
I0302 19:00:03.050262 22760421793920 run.py:483] Algo bellman_ford step 4767 current loss 0.110712, current_train_items 152576.
I0302 19:00:03.079143 22760421793920 run.py:483] Algo bellman_ford step 4768 current loss 0.071688, current_train_items 152608.
I0302 19:00:03.110458 22760421793920 run.py:483] Algo bellman_ford step 4769 current loss 0.079940, current_train_items 152640.
I0302 19:00:03.128740 22760421793920 run.py:483] Algo bellman_ford step 4770 current loss 0.001787, current_train_items 152672.
I0302 19:00:03.144229 22760421793920 run.py:483] Algo bellman_ford step 4771 current loss 0.004048, current_train_items 152704.
I0302 19:00:03.166505 22760421793920 run.py:483] Algo bellman_ford step 4772 current loss 0.029697, current_train_items 152736.
I0302 19:00:03.196712 22760421793920 run.py:483] Algo bellman_ford step 4773 current loss 0.075636, current_train_items 152768.
I0302 19:00:03.229425 22760421793920 run.py:483] Algo bellman_ford step 4774 current loss 0.073961, current_train_items 152800.
I0302 19:00:03.247834 22760421793920 run.py:483] Algo bellman_ford step 4775 current loss 0.001662, current_train_items 152832.
I0302 19:00:03.263487 22760421793920 run.py:483] Algo bellman_ford step 4776 current loss 0.017401, current_train_items 152864.
I0302 19:00:03.286705 22760421793920 run.py:483] Algo bellman_ford step 4777 current loss 0.037986, current_train_items 152896.
I0302 19:00:03.316941 22760421793920 run.py:483] Algo bellman_ford step 4778 current loss 0.061590, current_train_items 152928.
I0302 19:00:03.347503 22760421793920 run.py:483] Algo bellman_ford step 4779 current loss 0.037380, current_train_items 152960.
I0302 19:00:03.366070 22760421793920 run.py:483] Algo bellman_ford step 4780 current loss 0.002588, current_train_items 152992.
I0302 19:00:03.381866 22760421793920 run.py:483] Algo bellman_ford step 4781 current loss 0.007210, current_train_items 153024.
I0302 19:00:03.405209 22760421793920 run.py:483] Algo bellman_ford step 4782 current loss 0.031498, current_train_items 153056.
I0302 19:00:03.435939 22760421793920 run.py:483] Algo bellman_ford step 4783 current loss 0.061586, current_train_items 153088.
I0302 19:00:03.469047 22760421793920 run.py:483] Algo bellman_ford step 4784 current loss 0.087393, current_train_items 153120.
I0302 19:00:03.487472 22760421793920 run.py:483] Algo bellman_ford step 4785 current loss 0.021546, current_train_items 153152.
I0302 19:00:03.503464 22760421793920 run.py:483] Algo bellman_ford step 4786 current loss 0.027408, current_train_items 153184.
I0302 19:00:03.527127 22760421793920 run.py:483] Algo bellman_ford step 4787 current loss 0.029920, current_train_items 153216.
I0302 19:00:03.556524 22760421793920 run.py:483] Algo bellman_ford step 4788 current loss 0.038789, current_train_items 153248.
I0302 19:00:03.587358 22760421793920 run.py:483] Algo bellman_ford step 4789 current loss 0.079680, current_train_items 153280.
I0302 19:00:03.605346 22760421793920 run.py:483] Algo bellman_ford step 4790 current loss 0.002036, current_train_items 153312.
I0302 19:00:03.621070 22760421793920 run.py:483] Algo bellman_ford step 4791 current loss 0.012324, current_train_items 153344.
I0302 19:00:03.644466 22760421793920 run.py:483] Algo bellman_ford step 4792 current loss 0.036472, current_train_items 153376.
I0302 19:00:03.674476 22760421793920 run.py:483] Algo bellman_ford step 4793 current loss 0.074838, current_train_items 153408.
I0302 19:00:03.708591 22760421793920 run.py:483] Algo bellman_ford step 4794 current loss 0.094869, current_train_items 153440.
I0302 19:00:03.726797 22760421793920 run.py:483] Algo bellman_ford step 4795 current loss 0.001885, current_train_items 153472.
I0302 19:00:03.742650 22760421793920 run.py:483] Algo bellman_ford step 4796 current loss 0.009192, current_train_items 153504.
I0302 19:00:03.765983 22760421793920 run.py:483] Algo bellman_ford step 4797 current loss 0.039787, current_train_items 153536.
I0302 19:00:03.795732 22760421793920 run.py:483] Algo bellman_ford step 4798 current loss 0.073396, current_train_items 153568.
I0302 19:00:03.827104 22760421793920 run.py:483] Algo bellman_ford step 4799 current loss 0.051637, current_train_items 153600.
I0302 19:00:03.845341 22760421793920 run.py:483] Algo bellman_ford step 4800 current loss 0.005438, current_train_items 153632.
I0302 19:00:03.852833 22760421793920 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.9697265625, 'score': 0.9697265625, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:03.852953 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.970, val scores are: bellman_ford: 0.970
I0302 19:00:03.869404 22760421793920 run.py:483] Algo bellman_ford step 4801 current loss 0.069308, current_train_items 153664.
I0302 19:00:03.893098 22760421793920 run.py:483] Algo bellman_ford step 4802 current loss 0.153598, current_train_items 153696.
I0302 19:00:03.922759 22760421793920 run.py:483] Algo bellman_ford step 4803 current loss 0.112789, current_train_items 153728.
I0302 19:00:03.957716 22760421793920 run.py:483] Algo bellman_ford step 4804 current loss 0.125176, current_train_items 153760.
I0302 19:00:03.976377 22760421793920 run.py:483] Algo bellman_ford step 4805 current loss 0.026428, current_train_items 153792.
I0302 19:00:03.992474 22760421793920 run.py:483] Algo bellman_ford step 4806 current loss 0.026599, current_train_items 153824.
I0302 19:00:04.015534 22760421793920 run.py:483] Algo bellman_ford step 4807 current loss 0.086779, current_train_items 153856.
I0302 19:00:04.045980 22760421793920 run.py:483] Algo bellman_ford step 4808 current loss 0.150389, current_train_items 153888.
I0302 19:00:04.078251 22760421793920 run.py:483] Algo bellman_ford step 4809 current loss 0.211021, current_train_items 153920.
I0302 19:00:04.096967 22760421793920 run.py:483] Algo bellman_ford step 4810 current loss 0.001388, current_train_items 153952.
I0302 19:00:04.112697 22760421793920 run.py:483] Algo bellman_ford step 4811 current loss 0.028136, current_train_items 153984.
I0302 19:00:04.135207 22760421793920 run.py:483] Algo bellman_ford step 4812 current loss 0.019430, current_train_items 154016.
I0302 19:00:04.164524 22760421793920 run.py:483] Algo bellman_ford step 4813 current loss 0.036767, current_train_items 154048.
I0302 19:00:04.198048 22760421793920 run.py:483] Algo bellman_ford step 4814 current loss 0.083598, current_train_items 154080.
I0302 19:00:04.216631 22760421793920 run.py:483] Algo bellman_ford step 4815 current loss 0.008694, current_train_items 154112.
I0302 19:00:04.232401 22760421793920 run.py:483] Algo bellman_ford step 4816 current loss 0.019901, current_train_items 154144.
I0302 19:00:04.256820 22760421793920 run.py:483] Algo bellman_ford step 4817 current loss 0.081794, current_train_items 154176.
I0302 19:00:04.287051 22760421793920 run.py:483] Algo bellman_ford step 4818 current loss 0.053486, current_train_items 154208.
I0302 19:00:04.315196 22760421793920 run.py:483] Algo bellman_ford step 4819 current loss 0.056339, current_train_items 154240.
I0302 19:00:04.333460 22760421793920 run.py:483] Algo bellman_ford step 4820 current loss 0.003773, current_train_items 154272.
I0302 19:00:04.349134 22760421793920 run.py:483] Algo bellman_ford step 4821 current loss 0.029977, current_train_items 154304.
I0302 19:00:04.372348 22760421793920 run.py:483] Algo bellman_ford step 4822 current loss 0.048746, current_train_items 154336.
I0302 19:00:04.402021 22760421793920 run.py:483] Algo bellman_ford step 4823 current loss 0.047473, current_train_items 154368.
I0302 19:00:04.435056 22760421793920 run.py:483] Algo bellman_ford step 4824 current loss 0.073951, current_train_items 154400.
I0302 19:00:04.453553 22760421793920 run.py:483] Algo bellman_ford step 4825 current loss 0.003061, current_train_items 154432.
I0302 19:00:04.470048 22760421793920 run.py:483] Algo bellman_ford step 4826 current loss 0.008677, current_train_items 154464.
I0302 19:00:04.492581 22760421793920 run.py:483] Algo bellman_ford step 4827 current loss 0.042378, current_train_items 154496.
I0302 19:00:04.522814 22760421793920 run.py:483] Algo bellman_ford step 4828 current loss 0.071132, current_train_items 154528.
I0302 19:00:04.555782 22760421793920 run.py:483] Algo bellman_ford step 4829 current loss 0.104934, current_train_items 154560.
I0302 19:00:04.574414 22760421793920 run.py:483] Algo bellman_ford step 4830 current loss 0.001316, current_train_items 154592.
I0302 19:00:04.590241 22760421793920 run.py:483] Algo bellman_ford step 4831 current loss 0.035955, current_train_items 154624.
I0302 19:00:04.613879 22760421793920 run.py:483] Algo bellman_ford step 4832 current loss 0.045853, current_train_items 154656.
I0302 19:00:04.644555 22760421793920 run.py:483] Algo bellman_ford step 4833 current loss 0.047762, current_train_items 154688.
I0302 19:00:04.676428 22760421793920 run.py:483] Algo bellman_ford step 4834 current loss 0.085195, current_train_items 154720.
I0302 19:00:04.694434 22760421793920 run.py:483] Algo bellman_ford step 4835 current loss 0.002340, current_train_items 154752.
I0302 19:00:04.710049 22760421793920 run.py:483] Algo bellman_ford step 4836 current loss 0.015227, current_train_items 154784.
I0302 19:00:04.732274 22760421793920 run.py:483] Algo bellman_ford step 4837 current loss 0.037587, current_train_items 154816.
I0302 19:00:04.762120 22760421793920 run.py:483] Algo bellman_ford step 4838 current loss 0.047350, current_train_items 154848.
I0302 19:00:04.791987 22760421793920 run.py:483] Algo bellman_ford step 4839 current loss 0.045917, current_train_items 154880.
I0302 19:00:04.810174 22760421793920 run.py:483] Algo bellman_ford step 4840 current loss 0.007252, current_train_items 154912.
I0302 19:00:04.825630 22760421793920 run.py:483] Algo bellman_ford step 4841 current loss 0.022362, current_train_items 154944.
I0302 19:00:04.848683 22760421793920 run.py:483] Algo bellman_ford step 4842 current loss 0.031046, current_train_items 154976.
I0302 19:00:04.878189 22760421793920 run.py:483] Algo bellman_ford step 4843 current loss 0.061137, current_train_items 155008.
I0302 19:00:04.911529 22760421793920 run.py:483] Algo bellman_ford step 4844 current loss 0.099521, current_train_items 155040.
I0302 19:00:04.930069 22760421793920 run.py:483] Algo bellman_ford step 4845 current loss 0.007427, current_train_items 155072.
I0302 19:00:04.946044 22760421793920 run.py:483] Algo bellman_ford step 4846 current loss 0.016363, current_train_items 155104.
I0302 19:00:04.968237 22760421793920 run.py:483] Algo bellman_ford step 4847 current loss 0.086866, current_train_items 155136.
I0302 19:00:04.997756 22760421793920 run.py:483] Algo bellman_ford step 4848 current loss 0.067469, current_train_items 155168.
I0302 19:00:05.029862 22760421793920 run.py:483] Algo bellman_ford step 4849 current loss 0.088390, current_train_items 155200.
I0302 19:00:05.048369 22760421793920 run.py:483] Algo bellman_ford step 4850 current loss 0.002282, current_train_items 155232.
I0302 19:00:05.055735 22760421793920 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:05.055847 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:05.072452 22760421793920 run.py:483] Algo bellman_ford step 4851 current loss 0.016068, current_train_items 155264.
I0302 19:00:05.094918 22760421793920 run.py:483] Algo bellman_ford step 4852 current loss 0.067528, current_train_items 155296.
I0302 19:00:05.124826 22760421793920 run.py:483] Algo bellman_ford step 4853 current loss 0.067306, current_train_items 155328.
I0302 19:00:05.157217 22760421793920 run.py:483] Algo bellman_ford step 4854 current loss 0.096219, current_train_items 155360.
I0302 19:00:05.175689 22760421793920 run.py:483] Algo bellman_ford step 4855 current loss 0.002460, current_train_items 155392.
I0302 19:00:05.192177 22760421793920 run.py:483] Algo bellman_ford step 4856 current loss 0.038147, current_train_items 155424.
I0302 19:00:05.216164 22760421793920 run.py:483] Algo bellman_ford step 4857 current loss 0.078905, current_train_items 155456.
I0302 19:00:05.246765 22760421793920 run.py:483] Algo bellman_ford step 4858 current loss 0.104209, current_train_items 155488.
I0302 19:00:05.280843 22760421793920 run.py:483] Algo bellman_ford step 4859 current loss 0.156043, current_train_items 155520.
I0302 19:00:05.299025 22760421793920 run.py:483] Algo bellman_ford step 4860 current loss 0.005528, current_train_items 155552.
I0302 19:00:05.315556 22760421793920 run.py:483] Algo bellman_ford step 4861 current loss 0.018270, current_train_items 155584.
I0302 19:00:05.338622 22760421793920 run.py:483] Algo bellman_ford step 4862 current loss 0.059889, current_train_items 155616.
I0302 19:00:05.369609 22760421793920 run.py:483] Algo bellman_ford step 4863 current loss 0.081926, current_train_items 155648.
I0302 19:00:05.400982 22760421793920 run.py:483] Algo bellman_ford step 4864 current loss 0.085122, current_train_items 155680.
I0302 19:00:05.419170 22760421793920 run.py:483] Algo bellman_ford step 4865 current loss 0.004462, current_train_items 155712.
I0302 19:00:05.434650 22760421793920 run.py:483] Algo bellman_ford step 4866 current loss 0.031320, current_train_items 155744.
I0302 19:00:05.458336 22760421793920 run.py:483] Algo bellman_ford step 4867 current loss 0.041759, current_train_items 155776.
I0302 19:00:05.489684 22760421793920 run.py:483] Algo bellman_ford step 4868 current loss 0.057976, current_train_items 155808.
I0302 19:00:05.520758 22760421793920 run.py:483] Algo bellman_ford step 4869 current loss 0.064159, current_train_items 155840.
I0302 19:00:05.539437 22760421793920 run.py:483] Algo bellman_ford step 4870 current loss 0.002794, current_train_items 155872.
I0302 19:00:05.555194 22760421793920 run.py:483] Algo bellman_ford step 4871 current loss 0.031433, current_train_items 155904.
I0302 19:00:05.578157 22760421793920 run.py:483] Algo bellman_ford step 4872 current loss 0.037718, current_train_items 155936.
I0302 19:00:05.606369 22760421793920 run.py:483] Algo bellman_ford step 4873 current loss 0.036104, current_train_items 155968.
I0302 19:00:05.637095 22760421793920 run.py:483] Algo bellman_ford step 4874 current loss 0.087265, current_train_items 156000.
I0302 19:00:05.655592 22760421793920 run.py:483] Algo bellman_ford step 4875 current loss 0.002541, current_train_items 156032.
I0302 19:00:05.671021 22760421793920 run.py:483] Algo bellman_ford step 4876 current loss 0.026998, current_train_items 156064.
I0302 19:00:05.693425 22760421793920 run.py:483] Algo bellman_ford step 4877 current loss 0.017479, current_train_items 156096.
I0302 19:00:05.722608 22760421793920 run.py:483] Algo bellman_ford step 4878 current loss 0.056128, current_train_items 156128.
I0302 19:00:05.754741 22760421793920 run.py:483] Algo bellman_ford step 4879 current loss 0.073108, current_train_items 156160.
I0302 19:00:05.773143 22760421793920 run.py:483] Algo bellman_ford step 4880 current loss 0.005548, current_train_items 156192.
I0302 19:00:05.788750 22760421793920 run.py:483] Algo bellman_ford step 4881 current loss 0.013239, current_train_items 156224.
I0302 19:00:05.812130 22760421793920 run.py:483] Algo bellman_ford step 4882 current loss 0.083722, current_train_items 156256.
I0302 19:00:05.842606 22760421793920 run.py:483] Algo bellman_ford step 4883 current loss 0.094802, current_train_items 156288.
I0302 19:00:05.875589 22760421793920 run.py:483] Algo bellman_ford step 4884 current loss 0.062434, current_train_items 156320.
I0302 19:00:05.893865 22760421793920 run.py:483] Algo bellman_ford step 4885 current loss 0.003745, current_train_items 156352.
I0302 19:00:05.909826 22760421793920 run.py:483] Algo bellman_ford step 4886 current loss 0.040872, current_train_items 156384.
I0302 19:00:05.932771 22760421793920 run.py:483] Algo bellman_ford step 4887 current loss 0.050149, current_train_items 156416.
I0302 19:00:05.962577 22760421793920 run.py:483] Algo bellman_ford step 4888 current loss 0.020135, current_train_items 156448.
I0302 19:00:05.994266 22760421793920 run.py:483] Algo bellman_ford step 4889 current loss 0.082132, current_train_items 156480.
I0302 19:00:06.012490 22760421793920 run.py:483] Algo bellman_ford step 4890 current loss 0.003467, current_train_items 156512.
I0302 19:00:06.028001 22760421793920 run.py:483] Algo bellman_ford step 4891 current loss 0.022074, current_train_items 156544.
I0302 19:00:06.050718 22760421793920 run.py:483] Algo bellman_ford step 4892 current loss 0.053842, current_train_items 156576.
I0302 19:00:06.079245 22760421793920 run.py:483] Algo bellman_ford step 4893 current loss 0.090309, current_train_items 156608.
I0302 19:00:06.110312 22760421793920 run.py:483] Algo bellman_ford step 4894 current loss 0.142708, current_train_items 156640.
I0302 19:00:06.128404 22760421793920 run.py:483] Algo bellman_ford step 4895 current loss 0.041732, current_train_items 156672.
I0302 19:00:06.144252 22760421793920 run.py:483] Algo bellman_ford step 4896 current loss 0.022678, current_train_items 156704.
I0302 19:00:06.166429 22760421793920 run.py:483] Algo bellman_ford step 4897 current loss 0.039995, current_train_items 156736.
I0302 19:00:06.195747 22760421793920 run.py:483] Algo bellman_ford step 4898 current loss 0.097239, current_train_items 156768.
I0302 19:00:06.228536 22760421793920 run.py:483] Algo bellman_ford step 4899 current loss 0.186371, current_train_items 156800.
I0302 19:00:06.246811 22760421793920 run.py:483] Algo bellman_ford step 4900 current loss 0.010890, current_train_items 156832.
I0302 19:00:06.254095 22760421793920 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:06.254235 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 19:00:06.270109 22760421793920 run.py:483] Algo bellman_ford step 4901 current loss 0.043161, current_train_items 156864.
I0302 19:00:06.293481 22760421793920 run.py:483] Algo bellman_ford step 4902 current loss 0.071865, current_train_items 156896.
I0302 19:00:06.325354 22760421793920 run.py:483] Algo bellman_ford step 4903 current loss 0.045746, current_train_items 156928.
I0302 19:00:06.357056 22760421793920 run.py:483] Algo bellman_ford step 4904 current loss 0.071125, current_train_items 156960.
I0302 19:00:06.375727 22760421793920 run.py:483] Algo bellman_ford step 4905 current loss 0.031269, current_train_items 156992.
I0302 19:00:06.391249 22760421793920 run.py:483] Algo bellman_ford step 4906 current loss 0.035806, current_train_items 157024.
I0302 19:00:06.414564 22760421793920 run.py:483] Algo bellman_ford step 4907 current loss 0.064404, current_train_items 157056.
I0302 19:00:06.444580 22760421793920 run.py:483] Algo bellman_ford step 4908 current loss 0.062360, current_train_items 157088.
I0302 19:00:06.475905 22760421793920 run.py:483] Algo bellman_ford step 4909 current loss 0.055516, current_train_items 157120.
I0302 19:00:06.494293 22760421793920 run.py:483] Algo bellman_ford step 4910 current loss 0.007494, current_train_items 157152.
I0302 19:00:06.510214 22760421793920 run.py:483] Algo bellman_ford step 4911 current loss 0.037591, current_train_items 157184.
I0302 19:00:06.532917 22760421793920 run.py:483] Algo bellman_ford step 4912 current loss 0.041063, current_train_items 157216.
I0302 19:00:06.564332 22760421793920 run.py:483] Algo bellman_ford step 4913 current loss 0.082514, current_train_items 157248.
I0302 19:00:06.595840 22760421793920 run.py:483] Algo bellman_ford step 4914 current loss 0.089772, current_train_items 157280.
I0302 19:00:06.613879 22760421793920 run.py:483] Algo bellman_ford step 4915 current loss 0.013600, current_train_items 157312.
I0302 19:00:06.629313 22760421793920 run.py:483] Algo bellman_ford step 4916 current loss 0.021578, current_train_items 157344.
I0302 19:00:06.652036 22760421793920 run.py:483] Algo bellman_ford step 4917 current loss 0.052254, current_train_items 157376.
I0302 19:00:06.680895 22760421793920 run.py:483] Algo bellman_ford step 4918 current loss 0.041492, current_train_items 157408.
I0302 19:00:06.712741 22760421793920 run.py:483] Algo bellman_ford step 4919 current loss 0.062946, current_train_items 157440.
I0302 19:00:06.730612 22760421793920 run.py:483] Algo bellman_ford step 4920 current loss 0.003624, current_train_items 157472.
I0302 19:00:06.745646 22760421793920 run.py:483] Algo bellman_ford step 4921 current loss 0.006050, current_train_items 157504.
I0302 19:00:06.768617 22760421793920 run.py:483] Algo bellman_ford step 4922 current loss 0.035249, current_train_items 157536.
I0302 19:00:06.798602 22760421793920 run.py:483] Algo bellman_ford step 4923 current loss 0.049339, current_train_items 157568.
I0302 19:00:06.830188 22760421793920 run.py:483] Algo bellman_ford step 4924 current loss 0.067710, current_train_items 157600.
I0302 19:00:06.848464 22760421793920 run.py:483] Algo bellman_ford step 4925 current loss 0.008775, current_train_items 157632.
I0302 19:00:06.864224 22760421793920 run.py:483] Algo bellman_ford step 4926 current loss 0.018248, current_train_items 157664.
I0302 19:00:06.887660 22760421793920 run.py:483] Algo bellman_ford step 4927 current loss 0.041458, current_train_items 157696.
I0302 19:00:06.916506 22760421793920 run.py:483] Algo bellman_ford step 4928 current loss 0.084213, current_train_items 157728.
I0302 19:00:06.948529 22760421793920 run.py:483] Algo bellman_ford step 4929 current loss 0.085798, current_train_items 157760.
I0302 19:00:06.966508 22760421793920 run.py:483] Algo bellman_ford step 4930 current loss 0.004713, current_train_items 157792.
I0302 19:00:06.981822 22760421793920 run.py:483] Algo bellman_ford step 4931 current loss 0.015394, current_train_items 157824.
I0302 19:00:07.004468 22760421793920 run.py:483] Algo bellman_ford step 4932 current loss 0.041453, current_train_items 157856.
I0302 19:00:07.033744 22760421793920 run.py:483] Algo bellman_ford step 4933 current loss 0.067080, current_train_items 157888.
I0302 19:00:07.065570 22760421793920 run.py:483] Algo bellman_ford step 4934 current loss 0.095060, current_train_items 157920.
I0302 19:00:07.083645 22760421793920 run.py:483] Algo bellman_ford step 4935 current loss 0.008325, current_train_items 157952.
I0302 19:00:07.099309 22760421793920 run.py:483] Algo bellman_ford step 4936 current loss 0.019264, current_train_items 157984.
I0302 19:00:07.123370 22760421793920 run.py:483] Algo bellman_ford step 4937 current loss 0.057297, current_train_items 158016.
I0302 19:00:07.153090 22760421793920 run.py:483] Algo bellman_ford step 4938 current loss 0.046567, current_train_items 158048.
I0302 19:00:07.186791 22760421793920 run.py:483] Algo bellman_ford step 4939 current loss 0.079648, current_train_items 158080.
I0302 19:00:07.204926 22760421793920 run.py:483] Algo bellman_ford step 4940 current loss 0.004169, current_train_items 158112.
I0302 19:00:07.220204 22760421793920 run.py:483] Algo bellman_ford step 4941 current loss 0.016560, current_train_items 158144.
I0302 19:00:07.242912 22760421793920 run.py:483] Algo bellman_ford step 4942 current loss 0.026646, current_train_items 158176.
I0302 19:00:07.273553 22760421793920 run.py:483] Algo bellman_ford step 4943 current loss 0.056354, current_train_items 158208.
I0302 19:00:07.303885 22760421793920 run.py:483] Algo bellman_ford step 4944 current loss 0.052549, current_train_items 158240.
I0302 19:00:07.321798 22760421793920 run.py:483] Algo bellman_ford step 4945 current loss 0.005003, current_train_items 158272.
I0302 19:00:07.337491 22760421793920 run.py:483] Algo bellman_ford step 4946 current loss 0.029527, current_train_items 158304.
I0302 19:00:07.359917 22760421793920 run.py:483] Algo bellman_ford step 4947 current loss 0.045806, current_train_items 158336.
I0302 19:00:07.389787 22760421793920 run.py:483] Algo bellman_ford step 4948 current loss 0.055794, current_train_items 158368.
I0302 19:00:07.422969 22760421793920 run.py:483] Algo bellman_ford step 4949 current loss 0.045713, current_train_items 158400.
I0302 19:00:07.441079 22760421793920 run.py:483] Algo bellman_ford step 4950 current loss 0.001353, current_train_items 158432.
I0302 19:00:07.448516 22760421793920 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:07.448626 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:00:07.464731 22760421793920 run.py:483] Algo bellman_ford step 4951 current loss 0.003407, current_train_items 158464.
I0302 19:00:07.487509 22760421793920 run.py:483] Algo bellman_ford step 4952 current loss 0.018990, current_train_items 158496.
I0302 19:00:07.518092 22760421793920 run.py:483] Algo bellman_ford step 4953 current loss 0.057150, current_train_items 158528.
I0302 19:00:07.552058 22760421793920 run.py:483] Algo bellman_ford step 4954 current loss 0.054016, current_train_items 158560.
I0302 19:00:07.570656 22760421793920 run.py:483] Algo bellman_ford step 4955 current loss 0.006721, current_train_items 158592.
I0302 19:00:07.586890 22760421793920 run.py:483] Algo bellman_ford step 4956 current loss 0.006126, current_train_items 158624.
I0302 19:00:07.610722 22760421793920 run.py:483] Algo bellman_ford step 4957 current loss 0.043954, current_train_items 158656.
I0302 19:00:07.641026 22760421793920 run.py:483] Algo bellman_ford step 4958 current loss 0.074581, current_train_items 158688.
I0302 19:00:07.670663 22760421793920 run.py:483] Algo bellman_ford step 4959 current loss 0.053664, current_train_items 158720.
I0302 19:00:07.689073 22760421793920 run.py:483] Algo bellman_ford step 4960 current loss 0.012913, current_train_items 158752.
I0302 19:00:07.704873 22760421793920 run.py:483] Algo bellman_ford step 4961 current loss 0.021116, current_train_items 158784.
I0302 19:00:07.727815 22760421793920 run.py:483] Algo bellman_ford step 4962 current loss 0.053359, current_train_items 158816.
I0302 19:00:07.756750 22760421793920 run.py:483] Algo bellman_ford step 4963 current loss 0.038215, current_train_items 158848.
I0302 19:00:07.788526 22760421793920 run.py:483] Algo bellman_ford step 4964 current loss 0.065755, current_train_items 158880.
I0302 19:00:07.806359 22760421793920 run.py:483] Algo bellman_ford step 4965 current loss 0.004686, current_train_items 158912.
I0302 19:00:07.822122 22760421793920 run.py:483] Algo bellman_ford step 4966 current loss 0.032137, current_train_items 158944.
I0302 19:00:07.845877 22760421793920 run.py:483] Algo bellman_ford step 4967 current loss 0.041993, current_train_items 158976.
I0302 19:00:07.876712 22760421793920 run.py:483] Algo bellman_ford step 4968 current loss 0.035201, current_train_items 159008.
I0302 19:00:07.906858 22760421793920 run.py:483] Algo bellman_ford step 4969 current loss 0.112416, current_train_items 159040.
I0302 19:00:07.925150 22760421793920 run.py:483] Algo bellman_ford step 4970 current loss 0.003544, current_train_items 159072.
I0302 19:00:07.940640 22760421793920 run.py:483] Algo bellman_ford step 4971 current loss 0.069940, current_train_items 159104.
I0302 19:00:07.963852 22760421793920 run.py:483] Algo bellman_ford step 4972 current loss 0.125892, current_train_items 159136.
I0302 19:00:07.994454 22760421793920 run.py:483] Algo bellman_ford step 4973 current loss 0.106850, current_train_items 159168.
I0302 19:00:08.026656 22760421793920 run.py:483] Algo bellman_ford step 4974 current loss 0.074295, current_train_items 159200.
I0302 19:00:08.044764 22760421793920 run.py:483] Algo bellman_ford step 4975 current loss 0.002246, current_train_items 159232.
I0302 19:00:08.060787 22760421793920 run.py:483] Algo bellman_ford step 4976 current loss 0.029383, current_train_items 159264.
I0302 19:00:08.084378 22760421793920 run.py:483] Algo bellman_ford step 4977 current loss 0.124518, current_train_items 159296.
I0302 19:00:08.115104 22760421793920 run.py:483] Algo bellman_ford step 4978 current loss 0.155814, current_train_items 159328.
I0302 19:00:08.147893 22760421793920 run.py:483] Algo bellman_ford step 4979 current loss 0.072231, current_train_items 159360.
I0302 19:00:08.166071 22760421793920 run.py:483] Algo bellman_ford step 4980 current loss 0.003954, current_train_items 159392.
I0302 19:00:08.181941 22760421793920 run.py:483] Algo bellman_ford step 4981 current loss 0.017002, current_train_items 159424.
I0302 19:00:08.205702 22760421793920 run.py:483] Algo bellman_ford step 4982 current loss 0.087291, current_train_items 159456.
I0302 19:00:08.233884 22760421793920 run.py:483] Algo bellman_ford step 4983 current loss 0.148657, current_train_items 159488.
I0302 19:00:08.265408 22760421793920 run.py:483] Algo bellman_ford step 4984 current loss 0.092932, current_train_items 159520.
I0302 19:00:08.283752 22760421793920 run.py:483] Algo bellman_ford step 4985 current loss 0.009673, current_train_items 159552.
I0302 19:00:08.299547 22760421793920 run.py:483] Algo bellman_ford step 4986 current loss 0.026300, current_train_items 159584.
I0302 19:00:08.321049 22760421793920 run.py:483] Algo bellman_ford step 4987 current loss 0.045741, current_train_items 159616.
I0302 19:00:08.350092 22760421793920 run.py:483] Algo bellman_ford step 4988 current loss 0.097618, current_train_items 159648.
I0302 19:00:08.384382 22760421793920 run.py:483] Algo bellman_ford step 4989 current loss 0.117967, current_train_items 159680.
I0302 19:00:08.402334 22760421793920 run.py:483] Algo bellman_ford step 4990 current loss 0.003860, current_train_items 159712.
I0302 19:00:08.417773 22760421793920 run.py:483] Algo bellman_ford step 4991 current loss 0.016719, current_train_items 159744.
I0302 19:00:08.440885 22760421793920 run.py:483] Algo bellman_ford step 4992 current loss 0.069328, current_train_items 159776.
I0302 19:00:08.470592 22760421793920 run.py:483] Algo bellman_ford step 4993 current loss 0.071172, current_train_items 159808.
I0302 19:00:08.503082 22760421793920 run.py:483] Algo bellman_ford step 4994 current loss 0.085317, current_train_items 159840.
I0302 19:00:08.520876 22760421793920 run.py:483] Algo bellman_ford step 4995 current loss 0.008002, current_train_items 159872.
I0302 19:00:08.536819 22760421793920 run.py:483] Algo bellman_ford step 4996 current loss 0.010316, current_train_items 159904.
I0302 19:00:08.559353 22760421793920 run.py:483] Algo bellman_ford step 4997 current loss 0.043554, current_train_items 159936.
I0302 19:00:08.589386 22760421793920 run.py:483] Algo bellman_ford step 4998 current loss 0.095033, current_train_items 159968.
I0302 19:00:08.622092 22760421793920 run.py:483] Algo bellman_ford step 4999 current loss 0.077147, current_train_items 160000.
I0302 19:00:08.640243 22760421793920 run.py:483] Algo bellman_ford step 5000 current loss 0.003782, current_train_items 160032.
I0302 19:00:08.647870 22760421793920 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:08.647988 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 19:00:08.664513 22760421793920 run.py:483] Algo bellman_ford step 5001 current loss 0.046740, current_train_items 160064.
I0302 19:00:08.687501 22760421793920 run.py:483] Algo bellman_ford step 5002 current loss 0.128835, current_train_items 160096.
I0302 19:00:08.716552 22760421793920 run.py:483] Algo bellman_ford step 5003 current loss 0.020631, current_train_items 160128.
I0302 19:00:08.748450 22760421793920 run.py:483] Algo bellman_ford step 5004 current loss 0.062350, current_train_items 160160.
I0302 19:00:08.767277 22760421793920 run.py:483] Algo bellman_ford step 5005 current loss 0.003305, current_train_items 160192.
I0302 19:00:08.783000 22760421793920 run.py:483] Algo bellman_ford step 5006 current loss 0.013421, current_train_items 160224.
I0302 19:00:08.806693 22760421793920 run.py:483] Algo bellman_ford step 5007 current loss 0.079418, current_train_items 160256.
I0302 19:00:08.838713 22760421793920 run.py:483] Algo bellman_ford step 5008 current loss 0.121122, current_train_items 160288.
I0302 19:00:08.869141 22760421793920 run.py:483] Algo bellman_ford step 5009 current loss 0.115883, current_train_items 160320.
I0302 19:00:08.887465 22760421793920 run.py:483] Algo bellman_ford step 5010 current loss 0.010764, current_train_items 160352.
I0302 19:00:08.902939 22760421793920 run.py:483] Algo bellman_ford step 5011 current loss 0.017008, current_train_items 160384.
I0302 19:00:08.926412 22760421793920 run.py:483] Algo bellman_ford step 5012 current loss 0.085579, current_train_items 160416.
I0302 19:00:08.956199 22760421793920 run.py:483] Algo bellman_ford step 5013 current loss 0.133107, current_train_items 160448.
I0302 19:00:08.989512 22760421793920 run.py:483] Algo bellman_ford step 5014 current loss 0.166193, current_train_items 160480.
I0302 19:00:09.007991 22760421793920 run.py:483] Algo bellman_ford step 5015 current loss 0.020505, current_train_items 160512.
I0302 19:00:09.024328 22760421793920 run.py:483] Algo bellman_ford step 5016 current loss 0.065823, current_train_items 160544.
I0302 19:00:09.047148 22760421793920 run.py:483] Algo bellman_ford step 5017 current loss 0.064296, current_train_items 160576.
I0302 19:00:09.077447 22760421793920 run.py:483] Algo bellman_ford step 5018 current loss 0.079709, current_train_items 160608.
I0302 19:00:09.111404 22760421793920 run.py:483] Algo bellman_ford step 5019 current loss 0.210424, current_train_items 160640.
I0302 19:00:09.129836 22760421793920 run.py:483] Algo bellman_ford step 5020 current loss 0.014369, current_train_items 160672.
I0302 19:00:09.145203 22760421793920 run.py:483] Algo bellman_ford step 5021 current loss 0.031666, current_train_items 160704.
I0302 19:00:09.169069 22760421793920 run.py:483] Algo bellman_ford step 5022 current loss 0.055555, current_train_items 160736.
I0302 19:00:09.199386 22760421793920 run.py:483] Algo bellman_ford step 5023 current loss 0.061894, current_train_items 160768.
I0302 19:00:09.231441 22760421793920 run.py:483] Algo bellman_ford step 5024 current loss 0.061212, current_train_items 160800.
I0302 19:00:09.249505 22760421793920 run.py:483] Algo bellman_ford step 5025 current loss 0.017529, current_train_items 160832.
I0302 19:00:09.264806 22760421793920 run.py:483] Algo bellman_ford step 5026 current loss 0.048649, current_train_items 160864.
I0302 19:00:09.287173 22760421793920 run.py:483] Algo bellman_ford step 5027 current loss 0.147932, current_train_items 160896.
I0302 19:00:09.315941 22760421793920 run.py:483] Algo bellman_ford step 5028 current loss 0.139448, current_train_items 160928.
I0302 19:00:09.347844 22760421793920 run.py:483] Algo bellman_ford step 5029 current loss 0.080780, current_train_items 160960.
I0302 19:00:09.366628 22760421793920 run.py:483] Algo bellman_ford step 5030 current loss 0.008221, current_train_items 160992.
I0302 19:00:09.382431 22760421793920 run.py:483] Algo bellman_ford step 5031 current loss 0.037187, current_train_items 161024.
I0302 19:00:09.405963 22760421793920 run.py:483] Algo bellman_ford step 5032 current loss 0.075866, current_train_items 161056.
I0302 19:00:09.434710 22760421793920 run.py:483] Algo bellman_ford step 5033 current loss 0.076944, current_train_items 161088.
I0302 19:00:09.467497 22760421793920 run.py:483] Algo bellman_ford step 5034 current loss 0.121255, current_train_items 161120.
I0302 19:00:09.486049 22760421793920 run.py:483] Algo bellman_ford step 5035 current loss 0.019899, current_train_items 161152.
I0302 19:00:09.501861 22760421793920 run.py:483] Algo bellman_ford step 5036 current loss 0.008273, current_train_items 161184.
I0302 19:00:09.524744 22760421793920 run.py:483] Algo bellman_ford step 5037 current loss 0.045578, current_train_items 161216.
I0302 19:00:09.556163 22760421793920 run.py:483] Algo bellman_ford step 5038 current loss 0.133471, current_train_items 161248.
I0302 19:00:09.588132 22760421793920 run.py:483] Algo bellman_ford step 5039 current loss 0.066301, current_train_items 161280.
I0302 19:00:09.606250 22760421793920 run.py:483] Algo bellman_ford step 5040 current loss 0.003383, current_train_items 161312.
I0302 19:00:09.621484 22760421793920 run.py:483] Algo bellman_ford step 5041 current loss 0.012792, current_train_items 161344.
I0302 19:00:09.645406 22760421793920 run.py:483] Algo bellman_ford step 5042 current loss 0.067140, current_train_items 161376.
I0302 19:00:09.674317 22760421793920 run.py:483] Algo bellman_ford step 5043 current loss 0.092818, current_train_items 161408.
I0302 19:00:09.705975 22760421793920 run.py:483] Algo bellman_ford step 5044 current loss 0.142901, current_train_items 161440.
I0302 19:00:09.724160 22760421793920 run.py:483] Algo bellman_ford step 5045 current loss 0.009486, current_train_items 161472.
I0302 19:00:09.739814 22760421793920 run.py:483] Algo bellman_ford step 5046 current loss 0.041673, current_train_items 161504.
I0302 19:00:09.761848 22760421793920 run.py:483] Algo bellman_ford step 5047 current loss 0.031040, current_train_items 161536.
I0302 19:00:09.791611 22760421793920 run.py:483] Algo bellman_ford step 5048 current loss 0.067043, current_train_items 161568.
I0302 19:00:09.823809 22760421793920 run.py:483] Algo bellman_ford step 5049 current loss 0.121534, current_train_items 161600.
I0302 19:00:09.842298 22760421793920 run.py:483] Algo bellman_ford step 5050 current loss 0.001975, current_train_items 161632.
I0302 19:00:09.849917 22760421793920 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:09.850027 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:09.866195 22760421793920 run.py:483] Algo bellman_ford step 5051 current loss 0.008635, current_train_items 161664.
I0302 19:00:09.890586 22760421793920 run.py:483] Algo bellman_ford step 5052 current loss 0.078372, current_train_items 161696.
I0302 19:00:09.920771 22760421793920 run.py:483] Algo bellman_ford step 5053 current loss 0.066781, current_train_items 161728.
I0302 19:00:09.951970 22760421793920 run.py:483] Algo bellman_ford step 5054 current loss 0.078258, current_train_items 161760.
I0302 19:00:09.970876 22760421793920 run.py:483] Algo bellman_ford step 5055 current loss 0.020973, current_train_items 161792.
I0302 19:00:09.987950 22760421793920 run.py:483] Algo bellman_ford step 5056 current loss 0.066887, current_train_items 161824.
I0302 19:00:10.012076 22760421793920 run.py:483] Algo bellman_ford step 5057 current loss 0.089774, current_train_items 161856.
I0302 19:00:10.041699 22760421793920 run.py:483] Algo bellman_ford step 5058 current loss 0.098287, current_train_items 161888.
I0302 19:00:10.074351 22760421793920 run.py:483] Algo bellman_ford step 5059 current loss 0.062833, current_train_items 161920.
I0302 19:00:10.092806 22760421793920 run.py:483] Algo bellman_ford step 5060 current loss 0.038302, current_train_items 161952.
I0302 19:00:10.108391 22760421793920 run.py:483] Algo bellman_ford step 5061 current loss 0.054859, current_train_items 161984.
I0302 19:00:10.130360 22760421793920 run.py:483] Algo bellman_ford step 5062 current loss 0.040566, current_train_items 162016.
I0302 19:00:10.160175 22760421793920 run.py:483] Algo bellman_ford step 5063 current loss 0.080445, current_train_items 162048.
I0302 19:00:10.190338 22760421793920 run.py:483] Algo bellman_ford step 5064 current loss 0.047345, current_train_items 162080.
I0302 19:00:10.208433 22760421793920 run.py:483] Algo bellman_ford step 5065 current loss 0.007774, current_train_items 162112.
I0302 19:00:10.224108 22760421793920 run.py:483] Algo bellman_ford step 5066 current loss 0.010695, current_train_items 162144.
I0302 19:00:10.248034 22760421793920 run.py:483] Algo bellman_ford step 5067 current loss 0.061838, current_train_items 162176.
I0302 19:00:10.278062 22760421793920 run.py:483] Algo bellman_ford step 5068 current loss 0.034456, current_train_items 162208.
I0302 19:00:10.311269 22760421793920 run.py:483] Algo bellman_ford step 5069 current loss 0.064688, current_train_items 162240.
I0302 19:00:10.329550 22760421793920 run.py:483] Algo bellman_ford step 5070 current loss 0.001193, current_train_items 162272.
I0302 19:00:10.345635 22760421793920 run.py:483] Algo bellman_ford step 5071 current loss 0.017297, current_train_items 162304.
I0302 19:00:10.367620 22760421793920 run.py:483] Algo bellman_ford step 5072 current loss 0.053386, current_train_items 162336.
I0302 19:00:10.398370 22760421793920 run.py:483] Algo bellman_ford step 5073 current loss 0.086982, current_train_items 162368.
I0302 19:00:10.427894 22760421793920 run.py:483] Algo bellman_ford step 5074 current loss 0.039342, current_train_items 162400.
I0302 19:00:10.446443 22760421793920 run.py:483] Algo bellman_ford step 5075 current loss 0.001685, current_train_items 162432.
I0302 19:00:10.462015 22760421793920 run.py:483] Algo bellman_ford step 5076 current loss 0.016398, current_train_items 162464.
I0302 19:00:10.485102 22760421793920 run.py:483] Algo bellman_ford step 5077 current loss 0.026340, current_train_items 162496.
I0302 19:00:10.514813 22760421793920 run.py:483] Algo bellman_ford step 5078 current loss 0.076527, current_train_items 162528.
I0302 19:00:10.545631 22760421793920 run.py:483] Algo bellman_ford step 5079 current loss 0.059379, current_train_items 162560.
I0302 19:00:10.563984 22760421793920 run.py:483] Algo bellman_ford step 5080 current loss 0.001891, current_train_items 162592.
I0302 19:00:10.579395 22760421793920 run.py:483] Algo bellman_ford step 5081 current loss 0.034699, current_train_items 162624.
I0302 19:00:10.602419 22760421793920 run.py:483] Algo bellman_ford step 5082 current loss 0.024310, current_train_items 162656.
I0302 19:00:10.632641 22760421793920 run.py:483] Algo bellman_ford step 5083 current loss 0.066683, current_train_items 162688.
I0302 19:00:10.664861 22760421793920 run.py:483] Algo bellman_ford step 5084 current loss 0.077426, current_train_items 162720.
I0302 19:00:10.683323 22760421793920 run.py:483] Algo bellman_ford step 5085 current loss 0.022218, current_train_items 162752.
I0302 19:00:10.698881 22760421793920 run.py:483] Algo bellman_ford step 5086 current loss 0.035777, current_train_items 162784.
I0302 19:00:10.722186 22760421793920 run.py:483] Algo bellman_ford step 5087 current loss 0.063976, current_train_items 162816.
I0302 19:00:10.752561 22760421793920 run.py:483] Algo bellman_ford step 5088 current loss 0.041506, current_train_items 162848.
I0302 19:00:10.785823 22760421793920 run.py:483] Algo bellman_ford step 5089 current loss 0.061503, current_train_items 162880.
I0302 19:00:10.804063 22760421793920 run.py:483] Algo bellman_ford step 5090 current loss 0.001475, current_train_items 162912.
I0302 19:00:10.819884 22760421793920 run.py:483] Algo bellman_ford step 5091 current loss 0.056239, current_train_items 162944.
I0302 19:00:10.841724 22760421793920 run.py:483] Algo bellman_ford step 5092 current loss 0.096490, current_train_items 162976.
I0302 19:00:10.870601 22760421793920 run.py:483] Algo bellman_ford step 5093 current loss 0.040511, current_train_items 163008.
I0302 19:00:10.901522 22760421793920 run.py:483] Algo bellman_ford step 5094 current loss 0.055092, current_train_items 163040.
I0302 19:00:10.919661 22760421793920 run.py:483] Algo bellman_ford step 5095 current loss 0.001248, current_train_items 163072.
I0302 19:00:10.935440 22760421793920 run.py:483] Algo bellman_ford step 5096 current loss 0.013244, current_train_items 163104.
I0302 19:00:10.959121 22760421793920 run.py:483] Algo bellman_ford step 5097 current loss 0.089951, current_train_items 163136.
I0302 19:00:10.989961 22760421793920 run.py:483] Algo bellman_ford step 5098 current loss 0.143827, current_train_items 163168.
I0302 19:00:11.023203 22760421793920 run.py:483] Algo bellman_ford step 5099 current loss 0.060700, current_train_items 163200.
I0302 19:00:11.041629 22760421793920 run.py:483] Algo bellman_ford step 5100 current loss 0.001141, current_train_items 163232.
I0302 19:00:11.049095 22760421793920 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:11.049204 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 19:00:11.065237 22760421793920 run.py:483] Algo bellman_ford step 5101 current loss 0.067498, current_train_items 163264.
I0302 19:00:11.086508 22760421793920 run.py:483] Algo bellman_ford step 5102 current loss 0.013716, current_train_items 163296.
I0302 19:00:11.117435 22760421793920 run.py:483] Algo bellman_ford step 5103 current loss 0.066418, current_train_items 163328.
I0302 19:00:11.150467 22760421793920 run.py:483] Algo bellman_ford step 5104 current loss 0.084822, current_train_items 163360.
I0302 19:00:11.169031 22760421793920 run.py:483] Algo bellman_ford step 5105 current loss 0.002336, current_train_items 163392.
I0302 19:00:11.184997 22760421793920 run.py:483] Algo bellman_ford step 5106 current loss 0.020388, current_train_items 163424.
I0302 19:00:11.207855 22760421793920 run.py:483] Algo bellman_ford step 5107 current loss 0.087834, current_train_items 163456.
I0302 19:00:11.236937 22760421793920 run.py:483] Algo bellman_ford step 5108 current loss 0.052712, current_train_items 163488.
I0302 19:00:11.269643 22760421793920 run.py:483] Algo bellman_ford step 5109 current loss 0.088446, current_train_items 163520.
I0302 19:00:11.288281 22760421793920 run.py:483] Algo bellman_ford step 5110 current loss 0.002577, current_train_items 163552.
I0302 19:00:11.304164 22760421793920 run.py:483] Algo bellman_ford step 5111 current loss 0.021591, current_train_items 163584.
I0302 19:00:11.326639 22760421793920 run.py:483] Algo bellman_ford step 5112 current loss 0.035208, current_train_items 163616.
I0302 19:00:11.355785 22760421793920 run.py:483] Algo bellman_ford step 5113 current loss 0.050597, current_train_items 163648.
I0302 19:00:11.387381 22760421793920 run.py:483] Algo bellman_ford step 5114 current loss 0.078595, current_train_items 163680.
I0302 19:00:11.405654 22760421793920 run.py:483] Algo bellman_ford step 5115 current loss 0.005227, current_train_items 163712.
I0302 19:00:11.421600 22760421793920 run.py:483] Algo bellman_ford step 5116 current loss 0.021128, current_train_items 163744.
I0302 19:00:11.445425 22760421793920 run.py:483] Algo bellman_ford step 5117 current loss 0.060524, current_train_items 163776.
I0302 19:00:11.474690 22760421793920 run.py:483] Algo bellman_ford step 5118 current loss 0.054171, current_train_items 163808.
I0302 19:00:11.507386 22760421793920 run.py:483] Algo bellman_ford step 5119 current loss 0.055796, current_train_items 163840.
I0302 19:00:11.525823 22760421793920 run.py:483] Algo bellman_ford step 5120 current loss 0.047555, current_train_items 163872.
I0302 19:00:11.541125 22760421793920 run.py:483] Algo bellman_ford step 5121 current loss 0.010556, current_train_items 163904.
I0302 19:00:11.563934 22760421793920 run.py:483] Algo bellman_ford step 5122 current loss 0.059488, current_train_items 163936.
I0302 19:00:11.592893 22760421793920 run.py:483] Algo bellman_ford step 5123 current loss 0.038319, current_train_items 163968.
I0302 19:00:11.624803 22760421793920 run.py:483] Algo bellman_ford step 5124 current loss 0.101244, current_train_items 164000.
I0302 19:00:11.642844 22760421793920 run.py:483] Algo bellman_ford step 5125 current loss 0.001882, current_train_items 164032.
I0302 19:00:11.658459 22760421793920 run.py:483] Algo bellman_ford step 5126 current loss 0.032664, current_train_items 164064.
I0302 19:00:11.681534 22760421793920 run.py:483] Algo bellman_ford step 5127 current loss 0.094482, current_train_items 164096.
I0302 19:00:11.711419 22760421793920 run.py:483] Algo bellman_ford step 5128 current loss 0.042554, current_train_items 164128.
I0302 19:00:11.744791 22760421793920 run.py:483] Algo bellman_ford step 5129 current loss 0.065869, current_train_items 164160.
I0302 19:00:11.762961 22760421793920 run.py:483] Algo bellman_ford step 5130 current loss 0.006268, current_train_items 164192.
I0302 19:00:11.778874 22760421793920 run.py:483] Algo bellman_ford step 5131 current loss 0.031649, current_train_items 164224.
I0302 19:00:11.800964 22760421793920 run.py:483] Algo bellman_ford step 5132 current loss 0.028750, current_train_items 164256.
I0302 19:00:11.831652 22760421793920 run.py:483] Algo bellman_ford step 5133 current loss 0.053509, current_train_items 164288.
I0302 19:00:11.863361 22760421793920 run.py:483] Algo bellman_ford step 5134 current loss 0.046067, current_train_items 164320.
I0302 19:00:11.881781 22760421793920 run.py:483] Algo bellman_ford step 5135 current loss 0.008179, current_train_items 164352.
I0302 19:00:11.897392 22760421793920 run.py:483] Algo bellman_ford step 5136 current loss 0.015591, current_train_items 164384.
I0302 19:00:11.920335 22760421793920 run.py:483] Algo bellman_ford step 5137 current loss 0.153088, current_train_items 164416.
I0302 19:00:11.950082 22760421793920 run.py:483] Algo bellman_ford step 5138 current loss 0.107012, current_train_items 164448.
I0302 19:00:11.982835 22760421793920 run.py:483] Algo bellman_ford step 5139 current loss 0.130471, current_train_items 164480.
I0302 19:00:12.001333 22760421793920 run.py:483] Algo bellman_ford step 5140 current loss 0.005771, current_train_items 164512.
I0302 19:00:12.017296 22760421793920 run.py:483] Algo bellman_ford step 5141 current loss 0.023258, current_train_items 164544.
I0302 19:00:12.040934 22760421793920 run.py:483] Algo bellman_ford step 5142 current loss 0.070253, current_train_items 164576.
I0302 19:00:12.070659 22760421793920 run.py:483] Algo bellman_ford step 5143 current loss 0.107639, current_train_items 164608.
I0302 19:00:12.102669 22760421793920 run.py:483] Algo bellman_ford step 5144 current loss 0.136722, current_train_items 164640.
I0302 19:00:12.120750 22760421793920 run.py:483] Algo bellman_ford step 5145 current loss 0.003967, current_train_items 164672.
I0302 19:00:12.136280 22760421793920 run.py:483] Algo bellman_ford step 5146 current loss 0.055388, current_train_items 164704.
I0302 19:00:12.158080 22760421793920 run.py:483] Algo bellman_ford step 5147 current loss 0.017152, current_train_items 164736.
I0302 19:00:12.187232 22760421793920 run.py:483] Algo bellman_ford step 5148 current loss 0.066487, current_train_items 164768.
I0302 19:00:12.218306 22760421793920 run.py:483] Algo bellman_ford step 5149 current loss 0.130945, current_train_items 164800.
I0302 19:00:12.236743 22760421793920 run.py:483] Algo bellman_ford step 5150 current loss 0.025906, current_train_items 164832.
I0302 19:00:12.244334 22760421793920 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:12.244443 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 19:00:12.260748 22760421793920 run.py:483] Algo bellman_ford step 5151 current loss 0.013462, current_train_items 164864.
I0302 19:00:12.283662 22760421793920 run.py:483] Algo bellman_ford step 5152 current loss 0.042072, current_train_items 164896.
I0302 19:00:12.312644 22760421793920 run.py:483] Algo bellman_ford step 5153 current loss 0.051848, current_train_items 164928.
I0302 19:00:12.343974 22760421793920 run.py:483] Algo bellman_ford step 5154 current loss 0.068224, current_train_items 164960.
I0302 19:00:12.362238 22760421793920 run.py:483] Algo bellman_ford step 5155 current loss 0.012073, current_train_items 164992.
I0302 19:00:12.378248 22760421793920 run.py:483] Algo bellman_ford step 5156 current loss 0.014757, current_train_items 165024.
I0302 19:00:12.401689 22760421793920 run.py:483] Algo bellman_ford step 5157 current loss 0.034432, current_train_items 165056.
I0302 19:00:12.432933 22760421793920 run.py:483] Algo bellman_ford step 5158 current loss 0.067880, current_train_items 165088.
I0302 19:00:12.464019 22760421793920 run.py:483] Algo bellman_ford step 5159 current loss 0.045901, current_train_items 165120.
I0302 19:00:12.482144 22760421793920 run.py:483] Algo bellman_ford step 5160 current loss 0.001185, current_train_items 165152.
I0302 19:00:12.497569 22760421793920 run.py:483] Algo bellman_ford step 5161 current loss 0.020921, current_train_items 165184.
I0302 19:00:12.521223 22760421793920 run.py:483] Algo bellman_ford step 5162 current loss 0.056221, current_train_items 165216.
I0302 19:00:12.552100 22760421793920 run.py:483] Algo bellman_ford step 5163 current loss 0.039235, current_train_items 165248.
I0302 19:00:12.582574 22760421793920 run.py:483] Algo bellman_ford step 5164 current loss 0.077832, current_train_items 165280.
I0302 19:00:12.600396 22760421793920 run.py:483] Algo bellman_ford step 5165 current loss 0.002538, current_train_items 165312.
I0302 19:00:12.615884 22760421793920 run.py:483] Algo bellman_ford step 5166 current loss 0.008761, current_train_items 165344.
I0302 19:00:12.637659 22760421793920 run.py:483] Algo bellman_ford step 5167 current loss 0.038802, current_train_items 165376.
I0302 19:00:12.667851 22760421793920 run.py:483] Algo bellman_ford step 5168 current loss 0.034522, current_train_items 165408.
I0302 19:00:12.700810 22760421793920 run.py:483] Algo bellman_ford step 5169 current loss 0.068244, current_train_items 165440.
I0302 19:00:12.718847 22760421793920 run.py:483] Algo bellman_ford step 5170 current loss 0.023682, current_train_items 165472.
I0302 19:00:12.734539 22760421793920 run.py:483] Algo bellman_ford step 5171 current loss 0.007237, current_train_items 165504.
I0302 19:00:12.757659 22760421793920 run.py:483] Algo bellman_ford step 5172 current loss 0.047565, current_train_items 165536.
I0302 19:00:12.788544 22760421793920 run.py:483] Algo bellman_ford step 5173 current loss 0.059982, current_train_items 165568.
I0302 19:00:12.820047 22760421793920 run.py:483] Algo bellman_ford step 5174 current loss 0.066007, current_train_items 165600.
I0302 19:00:12.837664 22760421793920 run.py:483] Algo bellman_ford step 5175 current loss 0.003274, current_train_items 165632.
I0302 19:00:12.853513 22760421793920 run.py:483] Algo bellman_ford step 5176 current loss 0.017541, current_train_items 165664.
I0302 19:00:12.876757 22760421793920 run.py:483] Algo bellman_ford step 5177 current loss 0.082787, current_train_items 165696.
I0302 19:00:12.906142 22760421793920 run.py:483] Algo bellman_ford step 5178 current loss 0.063451, current_train_items 165728.
I0302 19:00:12.936885 22760421793920 run.py:483] Algo bellman_ford step 5179 current loss 0.078816, current_train_items 165760.
I0302 19:00:12.954863 22760421793920 run.py:483] Algo bellman_ford step 5180 current loss 0.008628, current_train_items 165792.
I0302 19:00:12.970511 22760421793920 run.py:483] Algo bellman_ford step 5181 current loss 0.046359, current_train_items 165824.
I0302 19:00:12.993308 22760421793920 run.py:483] Algo bellman_ford step 5182 current loss 0.115373, current_train_items 165856.
I0302 19:00:13.022943 22760421793920 run.py:483] Algo bellman_ford step 5183 current loss 0.118533, current_train_items 165888.
I0302 19:00:13.053475 22760421793920 run.py:483] Algo bellman_ford step 5184 current loss 0.058763, current_train_items 165920.
I0302 19:00:13.071517 22760421793920 run.py:483] Algo bellman_ford step 5185 current loss 0.025299, current_train_items 165952.
I0302 19:00:13.087544 22760421793920 run.py:483] Algo bellman_ford step 5186 current loss 0.050025, current_train_items 165984.
I0302 19:00:13.109332 22760421793920 run.py:483] Algo bellman_ford step 5187 current loss 0.147677, current_train_items 166016.
I0302 19:00:13.139792 22760421793920 run.py:483] Algo bellman_ford step 5188 current loss 0.318469, current_train_items 166048.
I0302 19:00:13.170425 22760421793920 run.py:483] Algo bellman_ford step 5189 current loss 0.290464, current_train_items 166080.
I0302 19:00:13.188826 22760421793920 run.py:483] Algo bellman_ford step 5190 current loss 0.044396, current_train_items 166112.
I0302 19:00:13.204356 22760421793920 run.py:483] Algo bellman_ford step 5191 current loss 0.026544, current_train_items 166144.
I0302 19:00:13.227665 22760421793920 run.py:483] Algo bellman_ford step 5192 current loss 0.051314, current_train_items 166176.
I0302 19:00:13.256512 22760421793920 run.py:483] Algo bellman_ford step 5193 current loss 0.062739, current_train_items 166208.
I0302 19:00:13.288208 22760421793920 run.py:483] Algo bellman_ford step 5194 current loss 0.163821, current_train_items 166240.
I0302 19:00:13.306201 22760421793920 run.py:483] Algo bellman_ford step 5195 current loss 0.005386, current_train_items 166272.
I0302 19:00:13.321933 22760421793920 run.py:483] Algo bellman_ford step 5196 current loss 0.048755, current_train_items 166304.
I0302 19:00:13.345010 22760421793920 run.py:483] Algo bellman_ford step 5197 current loss 0.234905, current_train_items 166336.
I0302 19:00:13.375933 22760421793920 run.py:483] Algo bellman_ford step 5198 current loss 0.243898, current_train_items 166368.
I0302 19:00:13.407227 22760421793920 run.py:483] Algo bellman_ford step 5199 current loss 0.153493, current_train_items 166400.
I0302 19:00:13.425196 22760421793920 run.py:483] Algo bellman_ford step 5200 current loss 0.012695, current_train_items 166432.
I0302 19:00:13.432719 22760421793920 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.962890625, 'score': 0.962890625, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:13.432830 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.963, val scores are: bellman_ford: 0.963
I0302 19:00:13.449259 22760421793920 run.py:483] Algo bellman_ford step 5201 current loss 0.055581, current_train_items 166464.
I0302 19:00:13.473035 22760421793920 run.py:483] Algo bellman_ford step 5202 current loss 0.144302, current_train_items 166496.
I0302 19:00:13.504353 22760421793920 run.py:483] Algo bellman_ford step 5203 current loss 0.295050, current_train_items 166528.
I0302 19:00:13.537791 22760421793920 run.py:483] Algo bellman_ford step 5204 current loss 0.294919, current_train_items 166560.
I0302 19:00:13.556132 22760421793920 run.py:483] Algo bellman_ford step 5205 current loss 0.009684, current_train_items 166592.
I0302 19:00:13.571571 22760421793920 run.py:483] Algo bellman_ford step 5206 current loss 0.066696, current_train_items 166624.
I0302 19:00:13.594501 22760421793920 run.py:483] Algo bellman_ford step 5207 current loss 0.069312, current_train_items 166656.
I0302 19:00:13.623309 22760421793920 run.py:483] Algo bellman_ford step 5208 current loss 0.047929, current_train_items 166688.
I0302 19:00:13.657547 22760421793920 run.py:483] Algo bellman_ford step 5209 current loss 0.098839, current_train_items 166720.
I0302 19:00:13.675764 22760421793920 run.py:483] Algo bellman_ford step 5210 current loss 0.130000, current_train_items 166752.
I0302 19:00:13.691030 22760421793920 run.py:483] Algo bellman_ford step 5211 current loss 0.038124, current_train_items 166784.
I0302 19:00:13.714325 22760421793920 run.py:483] Algo bellman_ford step 5212 current loss 0.154377, current_train_items 166816.
I0302 19:00:13.745887 22760421793920 run.py:483] Algo bellman_ford step 5213 current loss 0.161553, current_train_items 166848.
I0302 19:00:13.777263 22760421793920 run.py:483] Algo bellman_ford step 5214 current loss 0.131781, current_train_items 166880.
I0302 19:00:13.795218 22760421793920 run.py:483] Algo bellman_ford step 5215 current loss 0.004993, current_train_items 166912.
I0302 19:00:13.811165 22760421793920 run.py:483] Algo bellman_ford step 5216 current loss 0.038881, current_train_items 166944.
I0302 19:00:13.833932 22760421793920 run.py:483] Algo bellman_ford step 5217 current loss 0.049008, current_train_items 166976.
I0302 19:00:13.863567 22760421793920 run.py:483] Algo bellman_ford step 5218 current loss 0.071052, current_train_items 167008.
I0302 19:00:13.895407 22760421793920 run.py:483] Algo bellman_ford step 5219 current loss 0.100255, current_train_items 167040.
I0302 19:00:13.913576 22760421793920 run.py:483] Algo bellman_ford step 5220 current loss 0.002914, current_train_items 167072.
I0302 19:00:13.929142 22760421793920 run.py:483] Algo bellman_ford step 5221 current loss 0.024657, current_train_items 167104.
I0302 19:00:13.951040 22760421793920 run.py:483] Algo bellman_ford step 5222 current loss 0.034476, current_train_items 167136.
I0302 19:00:13.981859 22760421793920 run.py:483] Algo bellman_ford step 5223 current loss 0.081407, current_train_items 167168.
I0302 19:00:14.013077 22760421793920 run.py:483] Algo bellman_ford step 5224 current loss 0.076853, current_train_items 167200.
I0302 19:00:14.031461 22760421793920 run.py:483] Algo bellman_ford step 5225 current loss 0.001256, current_train_items 167232.
I0302 19:00:14.046954 22760421793920 run.py:483] Algo bellman_ford step 5226 current loss 0.011205, current_train_items 167264.
I0302 19:00:14.070337 22760421793920 run.py:483] Algo bellman_ford step 5227 current loss 0.044939, current_train_items 167296.
I0302 19:00:14.101643 22760421793920 run.py:483] Algo bellman_ford step 5228 current loss 0.066386, current_train_items 167328.
I0302 19:00:14.134458 22760421793920 run.py:483] Algo bellman_ford step 5229 current loss 0.073407, current_train_items 167360.
I0302 19:00:14.152496 22760421793920 run.py:483] Algo bellman_ford step 5230 current loss 0.001037, current_train_items 167392.
I0302 19:00:14.168313 22760421793920 run.py:483] Algo bellman_ford step 5231 current loss 0.029570, current_train_items 167424.
I0302 19:00:14.191597 22760421793920 run.py:483] Algo bellman_ford step 5232 current loss 0.052432, current_train_items 167456.
I0302 19:00:14.221054 22760421793920 run.py:483] Algo bellman_ford step 5233 current loss 0.087890, current_train_items 167488.
I0302 19:00:14.254353 22760421793920 run.py:483] Algo bellman_ford step 5234 current loss 0.063895, current_train_items 167520.
I0302 19:00:14.272452 22760421793920 run.py:483] Algo bellman_ford step 5235 current loss 0.001752, current_train_items 167552.
I0302 19:00:14.287927 22760421793920 run.py:483] Algo bellman_ford step 5236 current loss 0.019072, current_train_items 167584.
I0302 19:00:14.310740 22760421793920 run.py:483] Algo bellman_ford step 5237 current loss 0.063178, current_train_items 167616.
I0302 19:00:14.342174 22760421793920 run.py:483] Algo bellman_ford step 5238 current loss 0.092068, current_train_items 167648.
I0302 19:00:14.374693 22760421793920 run.py:483] Algo bellman_ford step 5239 current loss 0.079514, current_train_items 167680.
I0302 19:00:14.393180 22760421793920 run.py:483] Algo bellman_ford step 5240 current loss 0.019584, current_train_items 167712.
I0302 19:00:14.408838 22760421793920 run.py:483] Algo bellman_ford step 5241 current loss 0.028843, current_train_items 167744.
I0302 19:00:14.431854 22760421793920 run.py:483] Algo bellman_ford step 5242 current loss 0.034817, current_train_items 167776.
I0302 19:00:14.462114 22760421793920 run.py:483] Algo bellman_ford step 5243 current loss 0.051422, current_train_items 167808.
I0302 19:00:14.491563 22760421793920 run.py:483] Algo bellman_ford step 5244 current loss 0.032045, current_train_items 167840.
I0302 19:00:14.509410 22760421793920 run.py:483] Algo bellman_ford step 5245 current loss 0.013065, current_train_items 167872.
I0302 19:00:14.525185 22760421793920 run.py:483] Algo bellman_ford step 5246 current loss 0.008640, current_train_items 167904.
I0302 19:00:14.548647 22760421793920 run.py:483] Algo bellman_ford step 5247 current loss 0.026085, current_train_items 167936.
I0302 19:00:14.579016 22760421793920 run.py:483] Algo bellman_ford step 5248 current loss 0.043750, current_train_items 167968.
I0302 19:00:14.611391 22760421793920 run.py:483] Algo bellman_ford step 5249 current loss 0.058625, current_train_items 168000.
I0302 19:00:14.629481 22760421793920 run.py:483] Algo bellman_ford step 5250 current loss 0.002958, current_train_items 168032.
I0302 19:00:14.636997 22760421793920 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:14.637109 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:00:14.653526 22760421793920 run.py:483] Algo bellman_ford step 5251 current loss 0.004389, current_train_items 168064.
I0302 19:00:14.677290 22760421793920 run.py:483] Algo bellman_ford step 5252 current loss 0.113068, current_train_items 168096.
I0302 19:00:14.708130 22760421793920 run.py:483] Algo bellman_ford step 5253 current loss 0.052603, current_train_items 168128.
I0302 19:00:14.738035 22760421793920 run.py:483] Algo bellman_ford step 5254 current loss 0.033406, current_train_items 168160.
I0302 19:00:14.756449 22760421793920 run.py:483] Algo bellman_ford step 5255 current loss 0.045048, current_train_items 168192.
I0302 19:00:14.772441 22760421793920 run.py:483] Algo bellman_ford step 5256 current loss 0.005446, current_train_items 168224.
I0302 19:00:14.794281 22760421793920 run.py:483] Algo bellman_ford step 5257 current loss 0.053550, current_train_items 168256.
I0302 19:00:14.824950 22760421793920 run.py:483] Algo bellman_ford step 5258 current loss 0.080107, current_train_items 168288.
I0302 19:00:14.854819 22760421793920 run.py:483] Algo bellman_ford step 5259 current loss 0.057106, current_train_items 168320.
I0302 19:00:14.873231 22760421793920 run.py:483] Algo bellman_ford step 5260 current loss 0.001771, current_train_items 168352.
I0302 19:00:14.888919 22760421793920 run.py:483] Algo bellman_ford step 5261 current loss 0.051923, current_train_items 168384.
I0302 19:00:14.911344 22760421793920 run.py:483] Algo bellman_ford step 5262 current loss 0.085715, current_train_items 168416.
I0302 19:00:14.942375 22760421793920 run.py:483] Algo bellman_ford step 5263 current loss 0.127524, current_train_items 168448.
I0302 19:00:14.973049 22760421793920 run.py:483] Algo bellman_ford step 5264 current loss 0.083908, current_train_items 168480.
I0302 19:00:14.991267 22760421793920 run.py:483] Algo bellman_ford step 5265 current loss 0.009539, current_train_items 168512.
I0302 19:00:15.006998 22760421793920 run.py:483] Algo bellman_ford step 5266 current loss 0.048432, current_train_items 168544.
I0302 19:00:15.030800 22760421793920 run.py:483] Algo bellman_ford step 5267 current loss 0.132396, current_train_items 168576.
I0302 19:00:15.060034 22760421793920 run.py:483] Algo bellman_ford step 5268 current loss 0.194010, current_train_items 168608.
I0302 19:00:15.090241 22760421793920 run.py:483] Algo bellman_ford step 5269 current loss 0.080003, current_train_items 168640.
I0302 19:00:15.108049 22760421793920 run.py:483] Algo bellman_ford step 5270 current loss 0.000998, current_train_items 168672.
I0302 19:00:15.123577 22760421793920 run.py:483] Algo bellman_ford step 5271 current loss 0.017581, current_train_items 168704.
I0302 19:00:15.145919 22760421793920 run.py:483] Algo bellman_ford step 5272 current loss 0.045861, current_train_items 168736.
I0302 19:00:15.175408 22760421793920 run.py:483] Algo bellman_ford step 5273 current loss 0.046784, current_train_items 168768.
I0302 19:00:15.206315 22760421793920 run.py:483] Algo bellman_ford step 5274 current loss 0.089780, current_train_items 168800.
I0302 19:00:15.224338 22760421793920 run.py:483] Algo bellman_ford step 5275 current loss 0.002338, current_train_items 168832.
I0302 19:00:15.239952 22760421793920 run.py:483] Algo bellman_ford step 5276 current loss 0.012258, current_train_items 168864.
I0302 19:00:15.262481 22760421793920 run.py:483] Algo bellman_ford step 5277 current loss 0.046478, current_train_items 168896.
I0302 19:00:15.292734 22760421793920 run.py:483] Algo bellman_ford step 5278 current loss 0.091324, current_train_items 168928.
I0302 19:00:15.325180 22760421793920 run.py:483] Algo bellman_ford step 5279 current loss 0.094004, current_train_items 168960.
I0302 19:00:15.343149 22760421793920 run.py:483] Algo bellman_ford step 5280 current loss 0.002680, current_train_items 168992.
I0302 19:00:15.359061 22760421793920 run.py:483] Algo bellman_ford step 5281 current loss 0.038445, current_train_items 169024.
I0302 19:00:15.383187 22760421793920 run.py:483] Algo bellman_ford step 5282 current loss 0.034865, current_train_items 169056.
I0302 19:00:15.413364 22760421793920 run.py:483] Algo bellman_ford step 5283 current loss 0.057003, current_train_items 169088.
I0302 19:00:15.444843 22760421793920 run.py:483] Algo bellman_ford step 5284 current loss 0.110511, current_train_items 169120.
I0302 19:00:15.463255 22760421793920 run.py:483] Algo bellman_ford step 5285 current loss 0.002405, current_train_items 169152.
I0302 19:00:15.479032 22760421793920 run.py:483] Algo bellman_ford step 5286 current loss 0.023693, current_train_items 169184.
I0302 19:00:15.503025 22760421793920 run.py:483] Algo bellman_ford step 5287 current loss 0.040204, current_train_items 169216.
I0302 19:00:15.532730 22760421793920 run.py:483] Algo bellman_ford step 5288 current loss 0.038638, current_train_items 169248.
I0302 19:00:15.565397 22760421793920 run.py:483] Algo bellman_ford step 5289 current loss 0.095781, current_train_items 169280.
I0302 19:00:15.583493 22760421793920 run.py:483] Algo bellman_ford step 5290 current loss 0.001942, current_train_items 169312.
I0302 19:00:15.599580 22760421793920 run.py:483] Algo bellman_ford step 5291 current loss 0.010950, current_train_items 169344.
I0302 19:00:15.622727 22760421793920 run.py:483] Algo bellman_ford step 5292 current loss 0.088142, current_train_items 169376.
I0302 19:00:15.652843 22760421793920 run.py:483] Algo bellman_ford step 5293 current loss 0.065787, current_train_items 169408.
I0302 19:00:15.684004 22760421793920 run.py:483] Algo bellman_ford step 5294 current loss 0.079371, current_train_items 169440.
I0302 19:00:15.701895 22760421793920 run.py:483] Algo bellman_ford step 5295 current loss 0.002374, current_train_items 169472.
I0302 19:00:15.717372 22760421793920 run.py:483] Algo bellman_ford step 5296 current loss 0.021281, current_train_items 169504.
I0302 19:00:15.739691 22760421793920 run.py:483] Algo bellman_ford step 5297 current loss 0.044084, current_train_items 169536.
I0302 19:00:15.770428 22760421793920 run.py:483] Algo bellman_ford step 5298 current loss 0.063687, current_train_items 169568.
I0302 19:00:15.804586 22760421793920 run.py:483] Algo bellman_ford step 5299 current loss 0.092998, current_train_items 169600.
I0302 19:00:15.822613 22760421793920 run.py:483] Algo bellman_ford step 5300 current loss 0.010902, current_train_items 169632.
I0302 19:00:15.830251 22760421793920 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:15.830361 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 19:00:15.846322 22760421793920 run.py:483] Algo bellman_ford step 5301 current loss 0.022710, current_train_items 169664.
I0302 19:00:15.870009 22760421793920 run.py:483] Algo bellman_ford step 5302 current loss 0.147203, current_train_items 169696.
I0302 19:00:15.901493 22760421793920 run.py:483] Algo bellman_ford step 5303 current loss 0.079840, current_train_items 169728.
I0302 19:00:15.934736 22760421793920 run.py:483] Algo bellman_ford step 5304 current loss 0.089903, current_train_items 169760.
I0302 19:00:15.953243 22760421793920 run.py:483] Algo bellman_ford step 5305 current loss 0.006253, current_train_items 169792.
I0302 19:00:15.969095 22760421793920 run.py:483] Algo bellman_ford step 5306 current loss 0.023532, current_train_items 169824.
I0302 19:00:15.993198 22760421793920 run.py:483] Algo bellman_ford step 5307 current loss 0.074280, current_train_items 169856.
I0302 19:00:16.024396 22760421793920 run.py:483] Algo bellman_ford step 5308 current loss 0.063286, current_train_items 169888.
I0302 19:00:16.054959 22760421793920 run.py:483] Algo bellman_ford step 5309 current loss 0.060892, current_train_items 169920.
I0302 19:00:16.073272 22760421793920 run.py:483] Algo bellman_ford step 5310 current loss 0.002635, current_train_items 169952.
I0302 19:00:16.088497 22760421793920 run.py:483] Algo bellman_ford step 5311 current loss 0.012380, current_train_items 169984.
I0302 19:00:16.111763 22760421793920 run.py:483] Algo bellman_ford step 5312 current loss 0.026473, current_train_items 170016.
I0302 19:00:16.141744 22760421793920 run.py:483] Algo bellman_ford step 5313 current loss 0.098527, current_train_items 170048.
I0302 19:00:16.175020 22760421793920 run.py:483] Algo bellman_ford step 5314 current loss 0.118554, current_train_items 170080.
I0302 19:00:16.193574 22760421793920 run.py:483] Algo bellman_ford step 5315 current loss 0.003548, current_train_items 170112.
I0302 19:00:16.209232 22760421793920 run.py:483] Algo bellman_ford step 5316 current loss 0.021708, current_train_items 170144.
I0302 19:00:16.232236 22760421793920 run.py:483] Algo bellman_ford step 5317 current loss 0.072063, current_train_items 170176.
I0302 19:00:16.261349 22760421793920 run.py:483] Algo bellman_ford step 5318 current loss 0.067420, current_train_items 170208.
I0302 19:00:16.294111 22760421793920 run.py:483] Algo bellman_ford step 5319 current loss 0.065689, current_train_items 170240.
I0302 19:00:16.312377 22760421793920 run.py:483] Algo bellman_ford step 5320 current loss 0.003764, current_train_items 170272.
I0302 19:00:16.328049 22760421793920 run.py:483] Algo bellman_ford step 5321 current loss 0.028907, current_train_items 170304.
I0302 19:00:16.352179 22760421793920 run.py:483] Algo bellman_ford step 5322 current loss 0.048264, current_train_items 170336.
I0302 19:00:16.380712 22760421793920 run.py:483] Algo bellman_ford step 5323 current loss 0.056298, current_train_items 170368.
I0302 19:00:16.413879 22760421793920 run.py:483] Algo bellman_ford step 5324 current loss 0.112774, current_train_items 170400.
I0302 19:00:16.432239 22760421793920 run.py:483] Algo bellman_ford step 5325 current loss 0.003119, current_train_items 170432.
I0302 19:00:16.447777 22760421793920 run.py:483] Algo bellman_ford step 5326 current loss 0.026190, current_train_items 170464.
I0302 19:00:16.470032 22760421793920 run.py:483] Algo bellman_ford step 5327 current loss 0.059960, current_train_items 170496.
I0302 19:00:16.501382 22760421793920 run.py:483] Algo bellman_ford step 5328 current loss 0.049347, current_train_items 170528.
I0302 19:00:16.532767 22760421793920 run.py:483] Algo bellman_ford step 5329 current loss 0.082019, current_train_items 170560.
I0302 19:00:16.551121 22760421793920 run.py:483] Algo bellman_ford step 5330 current loss 0.008650, current_train_items 170592.
I0302 19:00:16.566843 22760421793920 run.py:483] Algo bellman_ford step 5331 current loss 0.015454, current_train_items 170624.
I0302 19:00:16.589728 22760421793920 run.py:483] Algo bellman_ford step 5332 current loss 0.106162, current_train_items 170656.
I0302 19:00:16.618347 22760421793920 run.py:483] Algo bellman_ford step 5333 current loss 0.082026, current_train_items 170688.
I0302 19:00:16.649290 22760421793920 run.py:483] Algo bellman_ford step 5334 current loss 0.139362, current_train_items 170720.
I0302 19:00:16.667657 22760421793920 run.py:483] Algo bellman_ford step 5335 current loss 0.009845, current_train_items 170752.
I0302 19:00:16.683537 22760421793920 run.py:483] Algo bellman_ford step 5336 current loss 0.026082, current_train_items 170784.
I0302 19:00:16.706447 22760421793920 run.py:483] Algo bellman_ford step 5337 current loss 0.026912, current_train_items 170816.
I0302 19:00:16.735698 22760421793920 run.py:483] Algo bellman_ford step 5338 current loss 0.126977, current_train_items 170848.
I0302 19:00:16.768383 22760421793920 run.py:483] Algo bellman_ford step 5339 current loss 0.126187, current_train_items 170880.
I0302 19:00:16.786506 22760421793920 run.py:483] Algo bellman_ford step 5340 current loss 0.001365, current_train_items 170912.
I0302 19:00:16.802264 22760421793920 run.py:483] Algo bellman_ford step 5341 current loss 0.020633, current_train_items 170944.
I0302 19:00:16.826852 22760421793920 run.py:483] Algo bellman_ford step 5342 current loss 0.039245, current_train_items 170976.
I0302 19:00:16.857228 22760421793920 run.py:483] Algo bellman_ford step 5343 current loss 0.049035, current_train_items 171008.
I0302 19:00:16.889101 22760421793920 run.py:483] Algo bellman_ford step 5344 current loss 0.077010, current_train_items 171040.
I0302 19:00:16.907402 22760421793920 run.py:483] Algo bellman_ford step 5345 current loss 0.001585, current_train_items 171072.
I0302 19:00:16.922507 22760421793920 run.py:483] Algo bellman_ford step 5346 current loss 0.020547, current_train_items 171104.
I0302 19:00:16.945377 22760421793920 run.py:483] Algo bellman_ford step 5347 current loss 0.034512, current_train_items 171136.
I0302 19:00:16.976420 22760421793920 run.py:483] Algo bellman_ford step 5348 current loss 0.056017, current_train_items 171168.
I0302 19:00:17.008875 22760421793920 run.py:483] Algo bellman_ford step 5349 current loss 0.069842, current_train_items 171200.
I0302 19:00:17.027201 22760421793920 run.py:483] Algo bellman_ford step 5350 current loss 0.007044, current_train_items 171232.
I0302 19:00:17.034517 22760421793920 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:17.034629 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:00:17.050518 22760421793920 run.py:483] Algo bellman_ford step 5351 current loss 0.011053, current_train_items 171264.
I0302 19:00:17.074175 22760421793920 run.py:483] Algo bellman_ford step 5352 current loss 0.093796, current_train_items 171296.
I0302 19:00:17.103311 22760421793920 run.py:483] Algo bellman_ford step 5353 current loss 0.089360, current_train_items 171328.
I0302 19:00:17.135125 22760421793920 run.py:483] Algo bellman_ford step 5354 current loss 0.053338, current_train_items 171360.
I0302 19:00:17.153419 22760421793920 run.py:483] Algo bellman_ford step 5355 current loss 0.001264, current_train_items 171392.
I0302 19:00:17.169561 22760421793920 run.py:483] Algo bellman_ford step 5356 current loss 0.036435, current_train_items 171424.
I0302 19:00:17.192079 22760421793920 run.py:483] Algo bellman_ford step 5357 current loss 0.057175, current_train_items 171456.
I0302 19:00:17.221271 22760421793920 run.py:483] Algo bellman_ford step 5358 current loss 0.063128, current_train_items 171488.
I0302 19:00:17.252760 22760421793920 run.py:483] Algo bellman_ford step 5359 current loss 0.069704, current_train_items 171520.
I0302 19:00:17.270928 22760421793920 run.py:483] Algo bellman_ford step 5360 current loss 0.010590, current_train_items 171552.
I0302 19:00:17.286496 22760421793920 run.py:483] Algo bellman_ford step 5361 current loss 0.051876, current_train_items 171584.
I0302 19:00:17.309799 22760421793920 run.py:483] Algo bellman_ford step 5362 current loss 0.076435, current_train_items 171616.
I0302 19:00:17.339699 22760421793920 run.py:483] Algo bellman_ford step 5363 current loss 0.075547, current_train_items 171648.
I0302 19:00:17.371515 22760421793920 run.py:483] Algo bellman_ford step 5364 current loss 0.097973, current_train_items 171680.
I0302 19:00:17.390075 22760421793920 run.py:483] Algo bellman_ford step 5365 current loss 0.008755, current_train_items 171712.
I0302 19:00:17.405697 22760421793920 run.py:483] Algo bellman_ford step 5366 current loss 0.055050, current_train_items 171744.
I0302 19:00:17.428400 22760421793920 run.py:483] Algo bellman_ford step 5367 current loss 0.066667, current_train_items 171776.
I0302 19:00:17.457908 22760421793920 run.py:483] Algo bellman_ford step 5368 current loss 0.114122, current_train_items 171808.
I0302 19:00:17.488842 22760421793920 run.py:483] Algo bellman_ford step 5369 current loss 0.082851, current_train_items 171840.
I0302 19:00:17.507072 22760421793920 run.py:483] Algo bellman_ford step 5370 current loss 0.012762, current_train_items 171872.
I0302 19:00:17.522630 22760421793920 run.py:483] Algo bellman_ford step 5371 current loss 0.019118, current_train_items 171904.
I0302 19:00:17.545233 22760421793920 run.py:483] Algo bellman_ford step 5372 current loss 0.037894, current_train_items 171936.
I0302 19:00:17.574788 22760421793920 run.py:483] Algo bellman_ford step 5373 current loss 0.066889, current_train_items 171968.
I0302 19:00:17.604190 22760421793920 run.py:483] Algo bellman_ford step 5374 current loss 0.066270, current_train_items 172000.
I0302 19:00:17.622086 22760421793920 run.py:483] Algo bellman_ford step 5375 current loss 0.002426, current_train_items 172032.
I0302 19:00:17.637763 22760421793920 run.py:483] Algo bellman_ford step 5376 current loss 0.022476, current_train_items 172064.
I0302 19:00:17.660830 22760421793920 run.py:483] Algo bellman_ford step 5377 current loss 0.063786, current_train_items 172096.
I0302 19:00:17.689606 22760421793920 run.py:483] Algo bellman_ford step 5378 current loss 0.024300, current_train_items 172128.
I0302 19:00:17.718364 22760421793920 run.py:483] Algo bellman_ford step 5379 current loss 0.053662, current_train_items 172160.
I0302 19:00:17.736387 22760421793920 run.py:483] Algo bellman_ford step 5380 current loss 0.011557, current_train_items 172192.
I0302 19:00:17.752196 22760421793920 run.py:483] Algo bellman_ford step 5381 current loss 0.013396, current_train_items 172224.
I0302 19:00:17.775632 22760421793920 run.py:483] Algo bellman_ford step 5382 current loss 0.078738, current_train_items 172256.
I0302 19:00:17.805533 22760421793920 run.py:483] Algo bellman_ford step 5383 current loss 0.084602, current_train_items 172288.
I0302 19:00:17.835951 22760421793920 run.py:483] Algo bellman_ford step 5384 current loss 0.053504, current_train_items 172320.
I0302 19:00:17.854119 22760421793920 run.py:483] Algo bellman_ford step 5385 current loss 0.001770, current_train_items 172352.
I0302 19:00:17.869356 22760421793920 run.py:483] Algo bellman_ford step 5386 current loss 0.021117, current_train_items 172384.
I0302 19:00:17.891687 22760421793920 run.py:483] Algo bellman_ford step 5387 current loss 0.026410, current_train_items 172416.
I0302 19:00:17.920434 22760421793920 run.py:483] Algo bellman_ford step 5388 current loss 0.034769, current_train_items 172448.
I0302 19:00:17.952499 22760421793920 run.py:483] Algo bellman_ford step 5389 current loss 0.105628, current_train_items 172480.
I0302 19:00:17.970790 22760421793920 run.py:483] Algo bellman_ford step 5390 current loss 0.047561, current_train_items 172512.
I0302 19:00:17.986908 22760421793920 run.py:483] Algo bellman_ford step 5391 current loss 0.022855, current_train_items 172544.
I0302 19:00:18.009059 22760421793920 run.py:483] Algo bellman_ford step 5392 current loss 0.045671, current_train_items 172576.
I0302 19:00:18.040108 22760421793920 run.py:483] Algo bellman_ford step 5393 current loss 0.035178, current_train_items 172608.
I0302 19:00:18.072659 22760421793920 run.py:483] Algo bellman_ford step 5394 current loss 0.040639, current_train_items 172640.
I0302 19:00:18.091096 22760421793920 run.py:483] Algo bellman_ford step 5395 current loss 0.001280, current_train_items 172672.
I0302 19:00:18.107198 22760421793920 run.py:483] Algo bellman_ford step 5396 current loss 0.032750, current_train_items 172704.
I0302 19:00:18.130039 22760421793920 run.py:483] Algo bellman_ford step 5397 current loss 0.027961, current_train_items 172736.
I0302 19:00:18.159603 22760421793920 run.py:483] Algo bellman_ford step 5398 current loss 0.044387, current_train_items 172768.
I0302 19:00:18.192404 22760421793920 run.py:483] Algo bellman_ford step 5399 current loss 0.118424, current_train_items 172800.
I0302 19:00:18.210453 22760421793920 run.py:483] Algo bellman_ford step 5400 current loss 0.001393, current_train_items 172832.
I0302 19:00:18.218102 22760421793920 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:18.218211 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:18.235111 22760421793920 run.py:483] Algo bellman_ford step 5401 current loss 0.030184, current_train_items 172864.
I0302 19:00:18.259281 22760421793920 run.py:483] Algo bellman_ford step 5402 current loss 0.059320, current_train_items 172896.
I0302 19:00:18.290581 22760421793920 run.py:483] Algo bellman_ford step 5403 current loss 0.118523, current_train_items 172928.
I0302 19:00:18.323636 22760421793920 run.py:483] Algo bellman_ford step 5404 current loss 0.048943, current_train_items 172960.
I0302 19:00:18.341921 22760421793920 run.py:483] Algo bellman_ford step 5405 current loss 0.002926, current_train_items 172992.
I0302 19:00:18.357247 22760421793920 run.py:483] Algo bellman_ford step 5406 current loss 0.014921, current_train_items 173024.
I0302 19:00:18.379593 22760421793920 run.py:483] Algo bellman_ford step 5407 current loss 0.039162, current_train_items 173056.
I0302 19:00:18.407783 22760421793920 run.py:483] Algo bellman_ford step 5408 current loss 0.135479, current_train_items 173088.
I0302 19:00:18.438793 22760421793920 run.py:483] Algo bellman_ford step 5409 current loss 0.090687, current_train_items 173120.
I0302 19:00:18.456910 22760421793920 run.py:483] Algo bellman_ford step 5410 current loss 0.004156, current_train_items 173152.
I0302 19:00:18.473016 22760421793920 run.py:483] Algo bellman_ford step 5411 current loss 0.024847, current_train_items 173184.
I0302 19:00:18.496280 22760421793920 run.py:483] Algo bellman_ford step 5412 current loss 0.078750, current_train_items 173216.
I0302 19:00:18.526252 22760421793920 run.py:483] Algo bellman_ford step 5413 current loss 0.042243, current_train_items 173248.
I0302 19:00:18.558964 22760421793920 run.py:483] Algo bellman_ford step 5414 current loss 0.100105, current_train_items 173280.
I0302 19:00:18.577215 22760421793920 run.py:483] Algo bellman_ford step 5415 current loss 0.009598, current_train_items 173312.
I0302 19:00:18.593077 22760421793920 run.py:483] Algo bellman_ford step 5416 current loss 0.020561, current_train_items 173344.
I0302 19:00:18.616325 22760421793920 run.py:483] Algo bellman_ford step 5417 current loss 0.047954, current_train_items 173376.
I0302 19:00:18.647249 22760421793920 run.py:483] Algo bellman_ford step 5418 current loss 0.044078, current_train_items 173408.
I0302 19:00:18.680688 22760421793920 run.py:483] Algo bellman_ford step 5419 current loss 0.071991, current_train_items 173440.
I0302 19:00:18.698842 22760421793920 run.py:483] Algo bellman_ford step 5420 current loss 0.007764, current_train_items 173472.
I0302 19:00:18.714772 22760421793920 run.py:483] Algo bellman_ford step 5421 current loss 0.032051, current_train_items 173504.
I0302 19:00:18.737266 22760421793920 run.py:483] Algo bellman_ford step 5422 current loss 0.021779, current_train_items 173536.
I0302 19:00:18.766306 22760421793920 run.py:483] Algo bellman_ford step 5423 current loss 0.033332, current_train_items 173568.
I0302 19:00:18.796149 22760421793920 run.py:483] Algo bellman_ford step 5424 current loss 0.088903, current_train_items 173600.
I0302 19:00:18.814413 22760421793920 run.py:483] Algo bellman_ford step 5425 current loss 0.004622, current_train_items 173632.
I0302 19:00:18.829822 22760421793920 run.py:483] Algo bellman_ford step 5426 current loss 0.011959, current_train_items 173664.
I0302 19:00:18.853116 22760421793920 run.py:483] Algo bellman_ford step 5427 current loss 0.060247, current_train_items 173696.
I0302 19:00:18.882567 22760421793920 run.py:483] Algo bellman_ford step 5428 current loss 0.074877, current_train_items 173728.
I0302 19:00:18.915535 22760421793920 run.py:483] Algo bellman_ford step 5429 current loss 0.066901, current_train_items 173760.
I0302 19:00:18.933441 22760421793920 run.py:483] Algo bellman_ford step 5430 current loss 0.005320, current_train_items 173792.
I0302 19:00:18.949392 22760421793920 run.py:483] Algo bellman_ford step 5431 current loss 0.049232, current_train_items 173824.
I0302 19:00:18.972739 22760421793920 run.py:483] Algo bellman_ford step 5432 current loss 0.044554, current_train_items 173856.
I0302 19:00:19.004673 22760421793920 run.py:483] Algo bellman_ford step 5433 current loss 0.106885, current_train_items 173888.
I0302 19:00:19.038621 22760421793920 run.py:483] Algo bellman_ford step 5434 current loss 0.132851, current_train_items 173920.
I0302 19:00:19.056578 22760421793920 run.py:483] Algo bellman_ford step 5435 current loss 0.002594, current_train_items 173952.
I0302 19:00:19.072283 22760421793920 run.py:483] Algo bellman_ford step 5436 current loss 0.014273, current_train_items 173984.
I0302 19:00:19.095189 22760421793920 run.py:483] Algo bellman_ford step 5437 current loss 0.056221, current_train_items 174016.
I0302 19:00:19.124178 22760421793920 run.py:483] Algo bellman_ford step 5438 current loss 0.070728, current_train_items 174048.
I0302 19:00:19.159431 22760421793920 run.py:483] Algo bellman_ford step 5439 current loss 0.116046, current_train_items 174080.
I0302 19:00:19.177913 22760421793920 run.py:483] Algo bellman_ford step 5440 current loss 0.010281, current_train_items 174112.
I0302 19:00:19.193419 22760421793920 run.py:483] Algo bellman_ford step 5441 current loss 0.010063, current_train_items 174144.
I0302 19:00:19.215738 22760421793920 run.py:483] Algo bellman_ford step 5442 current loss 0.041886, current_train_items 174176.
I0302 19:00:19.244836 22760421793920 run.py:483] Algo bellman_ford step 5443 current loss 0.063616, current_train_items 174208.
I0302 19:00:19.276846 22760421793920 run.py:483] Algo bellman_ford step 5444 current loss 0.064099, current_train_items 174240.
I0302 19:00:19.294843 22760421793920 run.py:483] Algo bellman_ford step 5445 current loss 0.003234, current_train_items 174272.
I0302 19:00:19.310673 22760421793920 run.py:483] Algo bellman_ford step 5446 current loss 0.025636, current_train_items 174304.
I0302 19:00:19.334040 22760421793920 run.py:483] Algo bellman_ford step 5447 current loss 0.074420, current_train_items 174336.
I0302 19:00:19.364971 22760421793920 run.py:483] Algo bellman_ford step 5448 current loss 0.068800, current_train_items 174368.
I0302 19:00:19.396718 22760421793920 run.py:483] Algo bellman_ford step 5449 current loss 0.044028, current_train_items 174400.
I0302 19:00:19.414683 22760421793920 run.py:483] Algo bellman_ford step 5450 current loss 0.005160, current_train_items 174432.
I0302 19:00:19.422029 22760421793920 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:00:19.422137 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:19.438138 22760421793920 run.py:483] Algo bellman_ford step 5451 current loss 0.025040, current_train_items 174464.
I0302 19:00:19.461398 22760421793920 run.py:483] Algo bellman_ford step 5452 current loss 0.065809, current_train_items 174496.
I0302 19:00:19.490675 22760421793920 run.py:483] Algo bellman_ford step 5453 current loss 0.067106, current_train_items 174528.
I0302 19:00:19.522768 22760421793920 run.py:483] Algo bellman_ford step 5454 current loss 0.080063, current_train_items 174560.
I0302 19:00:19.541580 22760421793920 run.py:483] Algo bellman_ford step 5455 current loss 0.000851, current_train_items 174592.
I0302 19:00:19.557884 22760421793920 run.py:483] Algo bellman_ford step 5456 current loss 0.004368, current_train_items 174624.
I0302 19:00:19.581095 22760421793920 run.py:483] Algo bellman_ford step 5457 current loss 0.023270, current_train_items 174656.
I0302 19:00:19.611816 22760421793920 run.py:483] Algo bellman_ford step 5458 current loss 0.072544, current_train_items 174688.
I0302 19:00:19.641454 22760421793920 run.py:483] Algo bellman_ford step 5459 current loss 0.044511, current_train_items 174720.
I0302 19:00:19.659910 22760421793920 run.py:483] Algo bellman_ford step 5460 current loss 0.001330, current_train_items 174752.
I0302 19:00:19.675892 22760421793920 run.py:483] Algo bellman_ford step 5461 current loss 0.031801, current_train_items 174784.
I0302 19:00:19.699541 22760421793920 run.py:483] Algo bellman_ford step 5462 current loss 0.017363, current_train_items 174816.
I0302 19:00:19.728934 22760421793920 run.py:483] Algo bellman_ford step 5463 current loss 0.075532, current_train_items 174848.
I0302 19:00:19.761482 22760421793920 run.py:483] Algo bellman_ford step 5464 current loss 0.079924, current_train_items 174880.
I0302 19:00:19.780172 22760421793920 run.py:483] Algo bellman_ford step 5465 current loss 0.002356, current_train_items 174912.
I0302 19:00:19.795619 22760421793920 run.py:483] Algo bellman_ford step 5466 current loss 0.020293, current_train_items 174944.
I0302 19:00:19.818338 22760421793920 run.py:483] Algo bellman_ford step 5467 current loss 0.031360, current_train_items 174976.
I0302 19:00:19.847643 22760421793920 run.py:483] Algo bellman_ford step 5468 current loss 0.051698, current_train_items 175008.
I0302 19:00:19.879312 22760421793920 run.py:483] Algo bellman_ford step 5469 current loss 0.090274, current_train_items 175040.
I0302 19:00:19.897657 22760421793920 run.py:483] Algo bellman_ford step 5470 current loss 0.004126, current_train_items 175072.
I0302 19:00:19.913411 22760421793920 run.py:483] Algo bellman_ford step 5471 current loss 0.016088, current_train_items 175104.
I0302 19:00:19.936001 22760421793920 run.py:483] Algo bellman_ford step 5472 current loss 0.031188, current_train_items 175136.
I0302 19:00:19.965573 22760421793920 run.py:483] Algo bellman_ford step 5473 current loss 0.037914, current_train_items 175168.
I0302 19:00:19.998500 22760421793920 run.py:483] Algo bellman_ford step 5474 current loss 0.067394, current_train_items 175200.
I0302 19:00:20.016334 22760421793920 run.py:483] Algo bellman_ford step 5475 current loss 0.003927, current_train_items 175232.
I0302 19:00:20.032117 22760421793920 run.py:483] Algo bellman_ford step 5476 current loss 0.129655, current_train_items 175264.
I0302 19:00:20.055062 22760421793920 run.py:483] Algo bellman_ford step 5477 current loss 0.050669, current_train_items 175296.
I0302 19:00:20.085462 22760421793920 run.py:483] Algo bellman_ford step 5478 current loss 0.066407, current_train_items 175328.
I0302 19:00:20.117110 22760421793920 run.py:483] Algo bellman_ford step 5479 current loss 0.120707, current_train_items 175360.
I0302 19:00:20.135574 22760421793920 run.py:483] Algo bellman_ford step 5480 current loss 0.019429, current_train_items 175392.
I0302 19:00:20.151957 22760421793920 run.py:483] Algo bellman_ford step 5481 current loss 0.021520, current_train_items 175424.
I0302 19:00:20.176172 22760421793920 run.py:483] Algo bellman_ford step 5482 current loss 0.139953, current_train_items 175456.
I0302 19:00:20.207079 22760421793920 run.py:483] Algo bellman_ford step 5483 current loss 0.164481, current_train_items 175488.
I0302 19:00:20.241091 22760421793920 run.py:483] Algo bellman_ford step 5484 current loss 0.160971, current_train_items 175520.
I0302 19:00:20.259197 22760421793920 run.py:483] Algo bellman_ford step 5485 current loss 0.003208, current_train_items 175552.
I0302 19:00:20.274594 22760421793920 run.py:483] Algo bellman_ford step 5486 current loss 0.012423, current_train_items 175584.
I0302 19:00:20.297378 22760421793920 run.py:483] Algo bellman_ford step 5487 current loss 0.054900, current_train_items 175616.
I0302 19:00:20.328847 22760421793920 run.py:483] Algo bellman_ford step 5488 current loss 0.063737, current_train_items 175648.
I0302 19:00:20.359988 22760421793920 run.py:483] Algo bellman_ford step 5489 current loss 0.053597, current_train_items 175680.
I0302 19:00:20.378428 22760421793920 run.py:483] Algo bellman_ford step 5490 current loss 0.011781, current_train_items 175712.
I0302 19:00:20.394159 22760421793920 run.py:483] Algo bellman_ford step 5491 current loss 0.024271, current_train_items 175744.
I0302 19:00:20.417526 22760421793920 run.py:483] Algo bellman_ford step 5492 current loss 0.047912, current_train_items 175776.
I0302 19:00:20.447364 22760421793920 run.py:483] Algo bellman_ford step 5493 current loss 0.039426, current_train_items 175808.
I0302 19:00:20.479279 22760421793920 run.py:483] Algo bellman_ford step 5494 current loss 0.079559, current_train_items 175840.
I0302 19:00:20.497609 22760421793920 run.py:483] Algo bellman_ford step 5495 current loss 0.001875, current_train_items 175872.
I0302 19:00:20.513623 22760421793920 run.py:483] Algo bellman_ford step 5496 current loss 0.005011, current_train_items 175904.
I0302 19:00:20.536871 22760421793920 run.py:483] Algo bellman_ford step 5497 current loss 0.031306, current_train_items 175936.
I0302 19:00:20.567185 22760421793920 run.py:483] Algo bellman_ford step 5498 current loss 0.031759, current_train_items 175968.
I0302 19:00:20.600294 22760421793920 run.py:483] Algo bellman_ford step 5499 current loss 0.129747, current_train_items 176000.
I0302 19:00:20.618966 22760421793920 run.py:483] Algo bellman_ford step 5500 current loss 0.008212, current_train_items 176032.
I0302 19:00:20.626449 22760421793920 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:00:20.626557 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:20.643264 22760421793920 run.py:483] Algo bellman_ford step 5501 current loss 0.043783, current_train_items 176064.
I0302 19:00:20.666879 22760421793920 run.py:483] Algo bellman_ford step 5502 current loss 0.052820, current_train_items 176096.
I0302 19:00:20.696682 22760421793920 run.py:483] Algo bellman_ford step 5503 current loss 0.037621, current_train_items 176128.
I0302 19:00:20.729324 22760421793920 run.py:483] Algo bellman_ford step 5504 current loss 0.067164, current_train_items 176160.
I0302 19:00:20.748011 22760421793920 run.py:483] Algo bellman_ford step 5505 current loss 0.019614, current_train_items 176192.
I0302 19:00:20.763763 22760421793920 run.py:483] Algo bellman_ford step 5506 current loss 0.016776, current_train_items 176224.
I0302 19:00:20.786158 22760421793920 run.py:483] Algo bellman_ford step 5507 current loss 0.022934, current_train_items 176256.
I0302 19:00:20.814816 22760421793920 run.py:483] Algo bellman_ford step 5508 current loss 0.045123, current_train_items 176288.
I0302 19:00:20.846214 22760421793920 run.py:483] Algo bellman_ford step 5509 current loss 0.052215, current_train_items 176320.
I0302 19:00:20.864125 22760421793920 run.py:483] Algo bellman_ford step 5510 current loss 0.002584, current_train_items 176352.
I0302 19:00:20.880199 22760421793920 run.py:483] Algo bellman_ford step 5511 current loss 0.009049, current_train_items 176384.
I0302 19:00:20.904326 22760421793920 run.py:483] Algo bellman_ford step 5512 current loss 0.021936, current_train_items 176416.
I0302 19:00:20.933511 22760421793920 run.py:483] Algo bellman_ford step 5513 current loss 0.037640, current_train_items 176448.
I0302 19:00:20.963639 22760421793920 run.py:483] Algo bellman_ford step 5514 current loss 0.078745, current_train_items 176480.
I0302 19:00:20.981708 22760421793920 run.py:483] Algo bellman_ford step 5515 current loss 0.001993, current_train_items 176512.
I0302 19:00:20.997115 22760421793920 run.py:483] Algo bellman_ford step 5516 current loss 0.005304, current_train_items 176544.
I0302 19:00:21.020389 22760421793920 run.py:483] Algo bellman_ford step 5517 current loss 0.045855, current_train_items 176576.
I0302 19:00:21.049767 22760421793920 run.py:483] Algo bellman_ford step 5518 current loss 0.063490, current_train_items 176608.
I0302 19:00:21.083338 22760421793920 run.py:483] Algo bellman_ford step 5519 current loss 0.065637, current_train_items 176640.
I0302 19:00:21.101480 22760421793920 run.py:483] Algo bellman_ford step 5520 current loss 0.009325, current_train_items 176672.
I0302 19:00:21.117453 22760421793920 run.py:483] Algo bellman_ford step 5521 current loss 0.013016, current_train_items 176704.
I0302 19:00:21.139791 22760421793920 run.py:483] Algo bellman_ford step 5522 current loss 0.022802, current_train_items 176736.
I0302 19:00:21.170364 22760421793920 run.py:483] Algo bellman_ford step 5523 current loss 0.060464, current_train_items 176768.
I0302 19:00:21.203745 22760421793920 run.py:483] Algo bellman_ford step 5524 current loss 0.102035, current_train_items 176800.
I0302 19:00:21.221764 22760421793920 run.py:483] Algo bellman_ford step 5525 current loss 0.012172, current_train_items 176832.
I0302 19:00:21.237157 22760421793920 run.py:483] Algo bellman_ford step 5526 current loss 0.037435, current_train_items 176864.
I0302 19:00:21.260739 22760421793920 run.py:483] Algo bellman_ford step 5527 current loss 0.049451, current_train_items 176896.
I0302 19:00:21.289839 22760421793920 run.py:483] Algo bellman_ford step 5528 current loss 0.113099, current_train_items 176928.
I0302 19:00:21.320581 22760421793920 run.py:483] Algo bellman_ford step 5529 current loss 0.057257, current_train_items 176960.
I0302 19:00:21.338825 22760421793920 run.py:483] Algo bellman_ford step 5530 current loss 0.002297, current_train_items 176992.
I0302 19:00:21.354559 22760421793920 run.py:483] Algo bellman_ford step 5531 current loss 0.011363, current_train_items 177024.
I0302 19:00:21.377931 22760421793920 run.py:483] Algo bellman_ford step 5532 current loss 0.050364, current_train_items 177056.
I0302 19:00:21.407256 22760421793920 run.py:483] Algo bellman_ford step 5533 current loss 0.067719, current_train_items 177088.
I0302 19:00:21.441339 22760421793920 run.py:483] Algo bellman_ford step 5534 current loss 0.099956, current_train_items 177120.
I0302 19:00:21.459345 22760421793920 run.py:483] Algo bellman_ford step 5535 current loss 0.003856, current_train_items 177152.
I0302 19:00:21.475287 22760421793920 run.py:483] Algo bellman_ford step 5536 current loss 0.043535, current_train_items 177184.
I0302 19:00:21.497457 22760421793920 run.py:483] Algo bellman_ford step 5537 current loss 0.035442, current_train_items 177216.
I0302 19:00:21.526490 22760421793920 run.py:483] Algo bellman_ford step 5538 current loss 0.045871, current_train_items 177248.
I0302 19:00:21.560075 22760421793920 run.py:483] Algo bellman_ford step 5539 current loss 0.048482, current_train_items 177280.
I0302 19:00:21.578235 22760421793920 run.py:483] Algo bellman_ford step 5540 current loss 0.000893, current_train_items 177312.
I0302 19:00:21.594402 22760421793920 run.py:483] Algo bellman_ford step 5541 current loss 0.023571, current_train_items 177344.
I0302 19:00:21.617924 22760421793920 run.py:483] Algo bellman_ford step 5542 current loss 0.032850, current_train_items 177376.
I0302 19:00:21.647740 22760421793920 run.py:483] Algo bellman_ford step 5543 current loss 0.037489, current_train_items 177408.
I0302 19:00:21.679037 22760421793920 run.py:483] Algo bellman_ford step 5544 current loss 0.144690, current_train_items 177440.
I0302 19:00:21.696968 22760421793920 run.py:483] Algo bellman_ford step 5545 current loss 0.001909, current_train_items 177472.
I0302 19:00:21.713163 22760421793920 run.py:483] Algo bellman_ford step 5546 current loss 0.038054, current_train_items 177504.
I0302 19:00:21.736102 22760421793920 run.py:483] Algo bellman_ford step 5547 current loss 0.029495, current_train_items 177536.
I0302 19:00:21.765429 22760421793920 run.py:483] Algo bellman_ford step 5548 current loss 0.047053, current_train_items 177568.
I0302 19:00:21.796471 22760421793920 run.py:483] Algo bellman_ford step 5549 current loss 0.082863, current_train_items 177600.
I0302 19:00:21.814552 22760421793920 run.py:483] Algo bellman_ford step 5550 current loss 0.003638, current_train_items 177632.
I0302 19:00:21.822087 22760421793920 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:00:21.822199 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:00:21.838531 22760421793920 run.py:483] Algo bellman_ford step 5551 current loss 0.014731, current_train_items 177664.
I0302 19:00:21.863087 22760421793920 run.py:483] Algo bellman_ford step 5552 current loss 0.073263, current_train_items 177696.
I0302 19:00:21.892791 22760421793920 run.py:483] Algo bellman_ford step 5553 current loss 0.101113, current_train_items 177728.
I0302 19:00:21.927548 22760421793920 run.py:483] Algo bellman_ford step 5554 current loss 0.112662, current_train_items 177760.
I0302 19:00:21.946417 22760421793920 run.py:483] Algo bellman_ford step 5555 current loss 0.005484, current_train_items 177792.
I0302 19:00:21.962455 22760421793920 run.py:483] Algo bellman_ford step 5556 current loss 0.040921, current_train_items 177824.
I0302 19:00:21.985571 22760421793920 run.py:483] Algo bellman_ford step 5557 current loss 0.091598, current_train_items 177856.
I0302 19:00:22.015114 22760421793920 run.py:483] Algo bellman_ford step 5558 current loss 0.132600, current_train_items 177888.
I0302 19:00:22.048002 22760421793920 run.py:483] Algo bellman_ford step 5559 current loss 0.096924, current_train_items 177920.
I0302 19:00:22.066614 22760421793920 run.py:483] Algo bellman_ford step 5560 current loss 0.016917, current_train_items 177952.
I0302 19:00:22.082171 22760421793920 run.py:483] Algo bellman_ford step 5561 current loss 0.021586, current_train_items 177984.
I0302 19:00:22.105291 22760421793920 run.py:483] Algo bellman_ford step 5562 current loss 0.062064, current_train_items 178016.
I0302 19:00:22.135700 22760421793920 run.py:483] Algo bellman_ford step 5563 current loss 0.064883, current_train_items 178048.
I0302 19:00:22.168748 22760421793920 run.py:483] Algo bellman_ford step 5564 current loss 0.082058, current_train_items 178080.
I0302 19:00:22.187096 22760421793920 run.py:483] Algo bellman_ford step 5565 current loss 0.003953, current_train_items 178112.
I0302 19:00:22.202700 22760421793920 run.py:483] Algo bellman_ford step 5566 current loss 0.032967, current_train_items 178144.
I0302 19:00:22.226217 22760421793920 run.py:483] Algo bellman_ford step 5567 current loss 0.070202, current_train_items 178176.
I0302 19:00:22.257221 22760421793920 run.py:483] Algo bellman_ford step 5568 current loss 0.112596, current_train_items 178208.
I0302 19:00:22.289716 22760421793920 run.py:483] Algo bellman_ford step 5569 current loss 0.094685, current_train_items 178240.
I0302 19:00:22.308212 22760421793920 run.py:483] Algo bellman_ford step 5570 current loss 0.010230, current_train_items 178272.
I0302 19:00:22.323673 22760421793920 run.py:483] Algo bellman_ford step 5571 current loss 0.008580, current_train_items 178304.
I0302 19:00:22.346638 22760421793920 run.py:483] Algo bellman_ford step 5572 current loss 0.034348, current_train_items 178336.
I0302 19:00:22.376332 22760421793920 run.py:483] Algo bellman_ford step 5573 current loss 0.069859, current_train_items 178368.
I0302 19:00:22.408670 22760421793920 run.py:483] Algo bellman_ford step 5574 current loss 0.119892, current_train_items 178400.
I0302 19:00:22.427242 22760421793920 run.py:483] Algo bellman_ford step 5575 current loss 0.002706, current_train_items 178432.
I0302 19:00:22.442858 22760421793920 run.py:483] Algo bellman_ford step 5576 current loss 0.017753, current_train_items 178464.
I0302 19:00:22.465378 22760421793920 run.py:483] Algo bellman_ford step 5577 current loss 0.062179, current_train_items 178496.
I0302 19:00:22.495715 22760421793920 run.py:483] Algo bellman_ford step 5578 current loss 0.072915, current_train_items 178528.
I0302 19:00:22.529332 22760421793920 run.py:483] Algo bellman_ford step 5579 current loss 0.136633, current_train_items 178560.
I0302 19:00:22.547639 22760421793920 run.py:483] Algo bellman_ford step 5580 current loss 0.003301, current_train_items 178592.
I0302 19:00:22.562924 22760421793920 run.py:483] Algo bellman_ford step 5581 current loss 0.134681, current_train_items 178624.
I0302 19:00:22.586951 22760421793920 run.py:483] Algo bellman_ford step 5582 current loss 0.038296, current_train_items 178656.
I0302 19:00:22.616740 22760421793920 run.py:483] Algo bellman_ford step 5583 current loss 0.039325, current_train_items 178688.
I0302 19:00:22.648983 22760421793920 run.py:483] Algo bellman_ford step 5584 current loss 0.068073, current_train_items 178720.
I0302 19:00:22.667530 22760421793920 run.py:483] Algo bellman_ford step 5585 current loss 0.033039, current_train_items 178752.
I0302 19:00:22.683214 22760421793920 run.py:483] Algo bellman_ford step 5586 current loss 0.034400, current_train_items 178784.
I0302 19:00:22.707759 22760421793920 run.py:483] Algo bellman_ford step 5587 current loss 0.058318, current_train_items 178816.
I0302 19:00:22.738732 22760421793920 run.py:483] Algo bellman_ford step 5588 current loss 0.092759, current_train_items 178848.
I0302 19:00:22.768747 22760421793920 run.py:483] Algo bellman_ford step 5589 current loss 0.124508, current_train_items 178880.
I0302 19:00:22.787070 22760421793920 run.py:483] Algo bellman_ford step 5590 current loss 0.011946, current_train_items 178912.
I0302 19:00:22.802659 22760421793920 run.py:483] Algo bellman_ford step 5591 current loss 0.056315, current_train_items 178944.
I0302 19:00:22.824769 22760421793920 run.py:483] Algo bellman_ford step 5592 current loss 0.034297, current_train_items 178976.
I0302 19:00:22.854871 22760421793920 run.py:483] Algo bellman_ford step 5593 current loss 0.081860, current_train_items 179008.
I0302 19:00:22.888195 22760421793920 run.py:483] Algo bellman_ford step 5594 current loss 0.053098, current_train_items 179040.
I0302 19:00:22.906377 22760421793920 run.py:483] Algo bellman_ford step 5595 current loss 0.014706, current_train_items 179072.
I0302 19:00:22.921919 22760421793920 run.py:483] Algo bellman_ford step 5596 current loss 0.010469, current_train_items 179104.
I0302 19:00:22.944922 22760421793920 run.py:483] Algo bellman_ford step 5597 current loss 0.088716, current_train_items 179136.
I0302 19:00:22.974824 22760421793920 run.py:483] Algo bellman_ford step 5598 current loss 0.098661, current_train_items 179168.
I0302 19:00:23.003452 22760421793920 run.py:483] Algo bellman_ford step 5599 current loss 0.037070, current_train_items 179200.
I0302 19:00:23.021710 22760421793920 run.py:483] Algo bellman_ford step 5600 current loss 0.007212, current_train_items 179232.
I0302 19:00:23.029071 22760421793920 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:00:23.029179 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:23.045552 22760421793920 run.py:483] Algo bellman_ford step 5601 current loss 0.016511, current_train_items 179264.
I0302 19:00:23.069278 22760421793920 run.py:483] Algo bellman_ford step 5602 current loss 0.032273, current_train_items 179296.
I0302 19:00:23.099519 22760421793920 run.py:483] Algo bellman_ford step 5603 current loss 0.096173, current_train_items 179328.
I0302 19:00:23.130507 22760421793920 run.py:483] Algo bellman_ford step 5604 current loss 0.039652, current_train_items 179360.
I0302 19:00:23.148957 22760421793920 run.py:483] Algo bellman_ford step 5605 current loss 0.003263, current_train_items 179392.
I0302 19:00:23.164385 22760421793920 run.py:483] Algo bellman_ford step 5606 current loss 0.016807, current_train_items 179424.
I0302 19:00:23.187610 22760421793920 run.py:483] Algo bellman_ford step 5607 current loss 0.089821, current_train_items 179456.
I0302 19:00:23.216633 22760421793920 run.py:483] Algo bellman_ford step 5608 current loss 0.170239, current_train_items 179488.
I0302 19:00:23.249397 22760421793920 run.py:483] Algo bellman_ford step 5609 current loss 0.084735, current_train_items 179520.
I0302 19:00:23.267941 22760421793920 run.py:483] Algo bellman_ford step 5610 current loss 0.001453, current_train_items 179552.
I0302 19:00:23.283737 22760421793920 run.py:483] Algo bellman_ford step 5611 current loss 0.009515, current_train_items 179584.
I0302 19:00:23.306633 22760421793920 run.py:483] Algo bellman_ford step 5612 current loss 0.051370, current_train_items 179616.
I0302 19:00:23.337935 22760421793920 run.py:483] Algo bellman_ford step 5613 current loss 0.044216, current_train_items 179648.
I0302 19:00:23.368669 22760421793920 run.py:483] Algo bellman_ford step 5614 current loss 0.055594, current_train_items 179680.
I0302 19:00:23.387059 22760421793920 run.py:483] Algo bellman_ford step 5615 current loss 0.028192, current_train_items 179712.
I0302 19:00:23.403049 22760421793920 run.py:483] Algo bellman_ford step 5616 current loss 0.032559, current_train_items 179744.
I0302 19:00:23.426155 22760421793920 run.py:483] Algo bellman_ford step 5617 current loss 0.020521, current_train_items 179776.
I0302 19:00:23.456154 22760421793920 run.py:483] Algo bellman_ford step 5618 current loss 0.034740, current_train_items 179808.
I0302 19:00:23.489236 22760421793920 run.py:483] Algo bellman_ford step 5619 current loss 0.116427, current_train_items 179840.
I0302 19:00:23.507525 22760421793920 run.py:483] Algo bellman_ford step 5620 current loss 0.002546, current_train_items 179872.
I0302 19:00:23.523187 22760421793920 run.py:483] Algo bellman_ford step 5621 current loss 0.037262, current_train_items 179904.
I0302 19:00:23.546457 22760421793920 run.py:483] Algo bellman_ford step 5622 current loss 0.027328, current_train_items 179936.
I0302 19:00:23.576177 22760421793920 run.py:483] Algo bellman_ford step 5623 current loss 0.064881, current_train_items 179968.
I0302 19:00:23.608936 22760421793920 run.py:483] Algo bellman_ford step 5624 current loss 0.079164, current_train_items 180000.
I0302 19:00:23.627329 22760421793920 run.py:483] Algo bellman_ford step 5625 current loss 0.003446, current_train_items 180032.
I0302 19:00:23.643429 22760421793920 run.py:483] Algo bellman_ford step 5626 current loss 0.041808, current_train_items 180064.
I0302 19:00:23.667128 22760421793920 run.py:483] Algo bellman_ford step 5627 current loss 0.051844, current_train_items 180096.
I0302 19:00:23.697855 22760421793920 run.py:483] Algo bellman_ford step 5628 current loss 0.094633, current_train_items 180128.
I0302 19:00:23.730967 22760421793920 run.py:483] Algo bellman_ford step 5629 current loss 0.074528, current_train_items 180160.
I0302 19:00:23.749166 22760421793920 run.py:483] Algo bellman_ford step 5630 current loss 0.006407, current_train_items 180192.
I0302 19:00:23.765010 22760421793920 run.py:483] Algo bellman_ford step 5631 current loss 0.009588, current_train_items 180224.
I0302 19:00:23.789332 22760421793920 run.py:483] Algo bellman_ford step 5632 current loss 0.099142, current_train_items 180256.
I0302 19:00:23.818943 22760421793920 run.py:483] Algo bellman_ford step 5633 current loss 0.099995, current_train_items 180288.
I0302 19:00:23.851585 22760421793920 run.py:483] Algo bellman_ford step 5634 current loss 0.117953, current_train_items 180320.
I0302 19:00:23.870265 22760421793920 run.py:483] Algo bellman_ford step 5635 current loss 0.010804, current_train_items 180352.
I0302 19:00:23.886144 22760421793920 run.py:483] Algo bellman_ford step 5636 current loss 0.018366, current_train_items 180384.
I0302 19:00:23.909141 22760421793920 run.py:483] Algo bellman_ford step 5637 current loss 0.040472, current_train_items 180416.
I0302 19:00:23.939023 22760421793920 run.py:483] Algo bellman_ford step 5638 current loss 0.059937, current_train_items 180448.
I0302 19:00:23.970327 22760421793920 run.py:483] Algo bellman_ford step 5639 current loss 0.099031, current_train_items 180480.
I0302 19:00:23.989245 22760421793920 run.py:483] Algo bellman_ford step 5640 current loss 0.005203, current_train_items 180512.
I0302 19:00:24.004694 22760421793920 run.py:483] Algo bellman_ford step 5641 current loss 0.021783, current_train_items 180544.
I0302 19:00:24.027906 22760421793920 run.py:483] Algo bellman_ford step 5642 current loss 0.044977, current_train_items 180576.
I0302 19:00:24.057665 22760421793920 run.py:483] Algo bellman_ford step 5643 current loss 0.062645, current_train_items 180608.
I0302 19:00:24.089018 22760421793920 run.py:483] Algo bellman_ford step 5644 current loss 0.076566, current_train_items 180640.
I0302 19:00:24.107379 22760421793920 run.py:483] Algo bellman_ford step 5645 current loss 0.001060, current_train_items 180672.
I0302 19:00:24.123619 22760421793920 run.py:483] Algo bellman_ford step 5646 current loss 0.024182, current_train_items 180704.
I0302 19:00:24.146049 22760421793920 run.py:483] Algo bellman_ford step 5647 current loss 0.035776, current_train_items 180736.
I0302 19:00:24.176019 22760421793920 run.py:483] Algo bellman_ford step 5648 current loss 0.055207, current_train_items 180768.
I0302 19:00:24.209854 22760421793920 run.py:483] Algo bellman_ford step 5649 current loss 0.079014, current_train_items 180800.
I0302 19:00:24.227905 22760421793920 run.py:483] Algo bellman_ford step 5650 current loss 0.032144, current_train_items 180832.
I0302 19:00:24.235492 22760421793920 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:00:24.235603 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.991, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:00:24.264888 22760421793920 run.py:483] Algo bellman_ford step 5651 current loss 0.008834, current_train_items 180864.
I0302 19:00:24.287405 22760421793920 run.py:483] Algo bellman_ford step 5652 current loss 0.034648, current_train_items 180896.
I0302 19:00:24.319640 22760421793920 run.py:483] Algo bellman_ford step 5653 current loss 0.089655, current_train_items 180928.
I0302 19:00:24.353317 22760421793920 run.py:483] Algo bellman_ford step 5654 current loss 0.087793, current_train_items 180960.
I0302 19:00:24.372403 22760421793920 run.py:483] Algo bellman_ford step 5655 current loss 0.001792, current_train_items 180992.
I0302 19:00:24.388993 22760421793920 run.py:483] Algo bellman_ford step 5656 current loss 0.013597, current_train_items 181024.
I0302 19:00:24.410974 22760421793920 run.py:483] Algo bellman_ford step 5657 current loss 0.034920, current_train_items 181056.
I0302 19:00:24.442250 22760421793920 run.py:483] Algo bellman_ford step 5658 current loss 0.038135, current_train_items 181088.
I0302 19:00:24.474761 22760421793920 run.py:483] Algo bellman_ford step 5659 current loss 0.065981, current_train_items 181120.
I0302 19:00:24.493455 22760421793920 run.py:483] Algo bellman_ford step 5660 current loss 0.005864, current_train_items 181152.
I0302 19:00:24.508866 22760421793920 run.py:483] Algo bellman_ford step 5661 current loss 0.017902, current_train_items 181184.
I0302 19:00:24.531665 22760421793920 run.py:483] Algo bellman_ford step 5662 current loss 0.039691, current_train_items 181216.
I0302 19:00:24.563782 22760421793920 run.py:483] Algo bellman_ford step 5663 current loss 0.063262, current_train_items 181248.
I0302 19:00:24.596777 22760421793920 run.py:483] Algo bellman_ford step 5664 current loss 0.045369, current_train_items 181280.
I0302 19:00:24.615312 22760421793920 run.py:483] Algo bellman_ford step 5665 current loss 0.009515, current_train_items 181312.
I0302 19:00:24.631501 22760421793920 run.py:483] Algo bellman_ford step 5666 current loss 0.037239, current_train_items 181344.
I0302 19:00:24.654131 22760421793920 run.py:483] Algo bellman_ford step 5667 current loss 0.060257, current_train_items 181376.
I0302 19:00:24.684791 22760421793920 run.py:483] Algo bellman_ford step 5668 current loss 0.049089, current_train_items 181408.
I0302 19:00:24.716189 22760421793920 run.py:483] Algo bellman_ford step 5669 current loss 0.081704, current_train_items 181440.
I0302 19:00:24.734525 22760421793920 run.py:483] Algo bellman_ford step 5670 current loss 0.002807, current_train_items 181472.
I0302 19:00:24.750112 22760421793920 run.py:483] Algo bellman_ford step 5671 current loss 0.007022, current_train_items 181504.
I0302 19:00:24.772854 22760421793920 run.py:483] Algo bellman_ford step 5672 current loss 0.043876, current_train_items 181536.
I0302 19:00:24.803159 22760421793920 run.py:483] Algo bellman_ford step 5673 current loss 0.037709, current_train_items 181568.
I0302 19:00:24.834980 22760421793920 run.py:483] Algo bellman_ford step 5674 current loss 0.039950, current_train_items 181600.
I0302 19:00:24.852978 22760421793920 run.py:483] Algo bellman_ford step 5675 current loss 0.003880, current_train_items 181632.
I0302 19:00:24.868531 22760421793920 run.py:483] Algo bellman_ford step 5676 current loss 0.017873, current_train_items 181664.
I0302 19:00:24.891942 22760421793920 run.py:483] Algo bellman_ford step 5677 current loss 0.033653, current_train_items 181696.
I0302 19:00:24.923994 22760421793920 run.py:483] Algo bellman_ford step 5678 current loss 0.080682, current_train_items 181728.
I0302 19:00:24.956023 22760421793920 run.py:483] Algo bellman_ford step 5679 current loss 0.079664, current_train_items 181760.
I0302 19:00:24.974511 22760421793920 run.py:483] Algo bellman_ford step 5680 current loss 0.000995, current_train_items 181792.
I0302 19:00:24.990390 22760421793920 run.py:483] Algo bellman_ford step 5681 current loss 0.011005, current_train_items 181824.
I0302 19:00:25.013722 22760421793920 run.py:483] Algo bellman_ford step 5682 current loss 0.080047, current_train_items 181856.
I0302 19:00:25.043520 22760421793920 run.py:483] Algo bellman_ford step 5683 current loss 0.043078, current_train_items 181888.
I0302 19:00:25.074217 22760421793920 run.py:483] Algo bellman_ford step 5684 current loss 0.051743, current_train_items 181920.
I0302 19:00:25.092471 22760421793920 run.py:483] Algo bellman_ford step 5685 current loss 0.002650, current_train_items 181952.
I0302 19:00:25.108355 22760421793920 run.py:483] Algo bellman_ford step 5686 current loss 0.034656, current_train_items 181984.
I0302 19:00:25.131812 22760421793920 run.py:483] Algo bellman_ford step 5687 current loss 0.066677, current_train_items 182016.
I0302 19:00:25.162173 22760421793920 run.py:483] Algo bellman_ford step 5688 current loss 0.072997, current_train_items 182048.
I0302 19:00:25.193801 22760421793920 run.py:483] Algo bellman_ford step 5689 current loss 0.082257, current_train_items 182080.
I0302 19:00:25.212289 22760421793920 run.py:483] Algo bellman_ford step 5690 current loss 0.003377, current_train_items 182112.
I0302 19:00:25.228770 22760421793920 run.py:483] Algo bellman_ford step 5691 current loss 0.029047, current_train_items 182144.
I0302 19:00:25.252091 22760421793920 run.py:483] Algo bellman_ford step 5692 current loss 0.063627, current_train_items 182176.
I0302 19:00:25.281941 22760421793920 run.py:483] Algo bellman_ford step 5693 current loss 0.045063, current_train_items 182208.
I0302 19:00:25.312985 22760421793920 run.py:483] Algo bellman_ford step 5694 current loss 0.069573, current_train_items 182240.
I0302 19:00:25.331197 22760421793920 run.py:483] Algo bellman_ford step 5695 current loss 0.003252, current_train_items 182272.
I0302 19:00:25.347371 22760421793920 run.py:483] Algo bellman_ford step 5696 current loss 0.029323, current_train_items 182304.
I0302 19:00:25.370413 22760421793920 run.py:483] Algo bellman_ford step 5697 current loss 0.069074, current_train_items 182336.
I0302 19:00:25.401360 22760421793920 run.py:483] Algo bellman_ford step 5698 current loss 0.081765, current_train_items 182368.
I0302 19:00:25.434107 22760421793920 run.py:483] Algo bellman_ford step 5699 current loss 0.066733, current_train_items 182400.
I0302 19:00:25.452170 22760421793920 run.py:483] Algo bellman_ford step 5700 current loss 0.010325, current_train_items 182432.
I0302 19:00:25.459407 22760421793920 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:00:25.459519 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:00:25.475471 22760421793920 run.py:483] Algo bellman_ford step 5701 current loss 0.006235, current_train_items 182464.
I0302 19:00:25.497824 22760421793920 run.py:483] Algo bellman_ford step 5702 current loss 0.041496, current_train_items 182496.
I0302 19:00:25.528436 22760421793920 run.py:483] Algo bellman_ford step 5703 current loss 0.037571, current_train_items 182528.
I0302 19:00:25.562153 22760421793920 run.py:483] Algo bellman_ford step 5704 current loss 0.069486, current_train_items 182560.
I0302 19:00:25.581125 22760421793920 run.py:483] Algo bellman_ford step 5705 current loss 0.007441, current_train_items 182592.
I0302 19:00:25.596640 22760421793920 run.py:483] Algo bellman_ford step 5706 current loss 0.012012, current_train_items 182624.
I0302 19:00:25.620450 22760421793920 run.py:483] Algo bellman_ford step 5707 current loss 0.087117, current_train_items 182656.
I0302 19:00:25.650300 22760421793920 run.py:483] Algo bellman_ford step 5708 current loss 0.109577, current_train_items 182688.
I0302 19:00:25.682614 22760421793920 run.py:483] Algo bellman_ford step 5709 current loss 0.105306, current_train_items 182720.
I0302 19:00:25.700807 22760421793920 run.py:483] Algo bellman_ford step 5710 current loss 0.001159, current_train_items 182752.
I0302 19:00:25.716410 22760421793920 run.py:483] Algo bellman_ford step 5711 current loss 0.031812, current_train_items 182784.
I0302 19:00:25.738813 22760421793920 run.py:483] Algo bellman_ford step 5712 current loss 0.061841, current_train_items 182816.
I0302 19:00:25.768190 22760421793920 run.py:483] Algo bellman_ford step 5713 current loss 0.110984, current_train_items 182848.
I0302 19:00:25.799873 22760421793920 run.py:483] Algo bellman_ford step 5714 current loss 0.083167, current_train_items 182880.
I0302 19:00:25.818050 22760421793920 run.py:483] Algo bellman_ford step 5715 current loss 0.002242, current_train_items 182912.
I0302 19:00:25.833763 22760421793920 run.py:483] Algo bellman_ford step 5716 current loss 0.014848, current_train_items 182944.
I0302 19:00:25.857002 22760421793920 run.py:483] Algo bellman_ford step 5717 current loss 0.029516, current_train_items 182976.
I0302 19:00:25.886554 22760421793920 run.py:483] Algo bellman_ford step 5718 current loss 0.071135, current_train_items 183008.
I0302 19:00:25.919355 22760421793920 run.py:483] Algo bellman_ford step 5719 current loss 0.070410, current_train_items 183040.
I0302 19:00:25.937872 22760421793920 run.py:483] Algo bellman_ford step 5720 current loss 0.005769, current_train_items 183072.
I0302 19:00:25.953546 22760421793920 run.py:483] Algo bellman_ford step 5721 current loss 0.009993, current_train_items 183104.
I0302 19:00:25.976032 22760421793920 run.py:483] Algo bellman_ford step 5722 current loss 0.054675, current_train_items 183136.
I0302 19:00:26.005919 22760421793920 run.py:483] Algo bellman_ford step 5723 current loss 0.067536, current_train_items 183168.
I0302 19:00:26.037172 22760421793920 run.py:483] Algo bellman_ford step 5724 current loss 0.075110, current_train_items 183200.
I0302 19:00:26.055448 22760421793920 run.py:483] Algo bellman_ford step 5725 current loss 0.002115, current_train_items 183232.
I0302 19:00:26.071381 22760421793920 run.py:483] Algo bellman_ford step 5726 current loss 0.031072, current_train_items 183264.
I0302 19:00:26.095375 22760421793920 run.py:483] Algo bellman_ford step 5727 current loss 0.044345, current_train_items 183296.
I0302 19:00:26.124501 22760421793920 run.py:483] Algo bellman_ford step 5728 current loss 0.051870, current_train_items 183328.
I0302 19:00:26.156588 22760421793920 run.py:483] Algo bellman_ford step 5729 current loss 0.051886, current_train_items 183360.
I0302 19:00:26.174707 22760421793920 run.py:483] Algo bellman_ford step 5730 current loss 0.001918, current_train_items 183392.
I0302 19:00:26.190998 22760421793920 run.py:483] Algo bellman_ford step 5731 current loss 0.006270, current_train_items 183424.
I0302 19:00:26.214623 22760421793920 run.py:483] Algo bellman_ford step 5732 current loss 0.074573, current_train_items 183456.
I0302 19:00:26.244045 22760421793920 run.py:483] Algo bellman_ford step 5733 current loss 0.082475, current_train_items 183488.
I0302 19:00:26.277207 22760421793920 run.py:483] Algo bellman_ford step 5734 current loss 0.061059, current_train_items 183520.
I0302 19:00:26.295717 22760421793920 run.py:483] Algo bellman_ford step 5735 current loss 0.002551, current_train_items 183552.
I0302 19:00:26.311704 22760421793920 run.py:483] Algo bellman_ford step 5736 current loss 0.014146, current_train_items 183584.
I0302 19:00:26.334596 22760421793920 run.py:483] Algo bellman_ford step 5737 current loss 0.072855, current_train_items 183616.
I0302 19:00:26.364149 22760421793920 run.py:483] Algo bellman_ford step 5738 current loss 0.035924, current_train_items 183648.
I0302 19:00:26.396811 22760421793920 run.py:483] Algo bellman_ford step 5739 current loss 0.097347, current_train_items 183680.
I0302 19:00:26.415066 22760421793920 run.py:483] Algo bellman_ford step 5740 current loss 0.012686, current_train_items 183712.
I0302 19:00:26.431071 22760421793920 run.py:483] Algo bellman_ford step 5741 current loss 0.008631, current_train_items 183744.
I0302 19:00:26.455138 22760421793920 run.py:483] Algo bellman_ford step 5742 current loss 0.038881, current_train_items 183776.
I0302 19:00:26.485822 22760421793920 run.py:483] Algo bellman_ford step 5743 current loss 0.101367, current_train_items 183808.
I0302 19:00:26.518888 22760421793920 run.py:483] Algo bellman_ford step 5744 current loss 0.061390, current_train_items 183840.
I0302 19:00:26.537266 22760421793920 run.py:483] Algo bellman_ford step 5745 current loss 0.021255, current_train_items 183872.
I0302 19:00:26.553454 22760421793920 run.py:483] Algo bellman_ford step 5746 current loss 0.017250, current_train_items 183904.
I0302 19:00:26.577113 22760421793920 run.py:483] Algo bellman_ford step 5747 current loss 0.056297, current_train_items 183936.
I0302 19:00:26.607375 22760421793920 run.py:483] Algo bellman_ford step 5748 current loss 0.093200, current_train_items 183968.
I0302 19:00:26.638731 22760421793920 run.py:483] Algo bellman_ford step 5749 current loss 0.084402, current_train_items 184000.
I0302 19:00:26.657342 22760421793920 run.py:483] Algo bellman_ford step 5750 current loss 0.002364, current_train_items 184032.
I0302 19:00:26.664926 22760421793920 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:00:26.665039 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:00:26.681799 22760421793920 run.py:483] Algo bellman_ford step 5751 current loss 0.008649, current_train_items 184064.
I0302 19:00:26.704569 22760421793920 run.py:483] Algo bellman_ford step 5752 current loss 0.057213, current_train_items 184096.
I0302 19:00:26.733920 22760421793920 run.py:483] Algo bellman_ford step 5753 current loss 0.057150, current_train_items 184128.
I0302 19:00:26.767622 22760421793920 run.py:483] Algo bellman_ford step 5754 current loss 0.095147, current_train_items 184160.
I0302 19:00:26.786267 22760421793920 run.py:483] Algo bellman_ford step 5755 current loss 0.005658, current_train_items 184192.
I0302 19:00:26.802036 22760421793920 run.py:483] Algo bellman_ford step 5756 current loss 0.026993, current_train_items 184224.
I0302 19:00:26.825382 22760421793920 run.py:483] Algo bellman_ford step 5757 current loss 0.067559, current_train_items 184256.
I0302 19:00:26.856231 22760421793920 run.py:483] Algo bellman_ford step 5758 current loss 0.057578, current_train_items 184288.
I0302 19:00:26.888786 22760421793920 run.py:483] Algo bellman_ford step 5759 current loss 0.048052, current_train_items 184320.
I0302 19:00:26.906561 22760421793920 run.py:483] Algo bellman_ford step 5760 current loss 0.005728, current_train_items 184352.
I0302 19:00:26.922421 22760421793920 run.py:483] Algo bellman_ford step 5761 current loss 0.029926, current_train_items 184384.
I0302 19:00:26.945644 22760421793920 run.py:483] Algo bellman_ford step 5762 current loss 0.038711, current_train_items 184416.
I0302 19:00:26.974867 22760421793920 run.py:483] Algo bellman_ford step 5763 current loss 0.041817, current_train_items 184448.
I0302 19:00:27.007563 22760421793920 run.py:483] Algo bellman_ford step 5764 current loss 0.043357, current_train_items 184480.
I0302 19:00:27.026075 22760421793920 run.py:483] Algo bellman_ford step 5765 current loss 0.011562, current_train_items 184512.
I0302 19:00:27.041778 22760421793920 run.py:483] Algo bellman_ford step 5766 current loss 0.016590, current_train_items 184544.
I0302 19:00:27.064560 22760421793920 run.py:483] Algo bellman_ford step 5767 current loss 0.032554, current_train_items 184576.
I0302 19:00:27.095423 22760421793920 run.py:483] Algo bellman_ford step 5768 current loss 0.050776, current_train_items 184608.
I0302 19:00:27.126879 22760421793920 run.py:483] Algo bellman_ford step 5769 current loss 0.045862, current_train_items 184640.
I0302 19:00:27.145038 22760421793920 run.py:483] Algo bellman_ford step 5770 current loss 0.004213, current_train_items 184672.
I0302 19:00:27.160640 22760421793920 run.py:483] Algo bellman_ford step 5771 current loss 0.021895, current_train_items 184704.
I0302 19:00:27.182409 22760421793920 run.py:483] Algo bellman_ford step 5772 current loss 0.034938, current_train_items 184736.
I0302 19:00:27.211697 22760421793920 run.py:483] Algo bellman_ford step 5773 current loss 0.044860, current_train_items 184768.
I0302 19:00:27.242758 22760421793920 run.py:483] Algo bellman_ford step 5774 current loss 0.061968, current_train_items 184800.
I0302 19:00:27.261114 22760421793920 run.py:483] Algo bellman_ford step 5775 current loss 0.003234, current_train_items 184832.
I0302 19:00:27.277184 22760421793920 run.py:483] Algo bellman_ford step 5776 current loss 0.037773, current_train_items 184864.
I0302 19:00:27.299258 22760421793920 run.py:483] Algo bellman_ford step 5777 current loss 0.030026, current_train_items 184896.
I0302 19:00:27.329260 22760421793920 run.py:483] Algo bellman_ford step 5778 current loss 0.070317, current_train_items 184928.
I0302 19:00:27.361612 22760421793920 run.py:483] Algo bellman_ford step 5779 current loss 0.075194, current_train_items 184960.
I0302 19:00:27.379553 22760421793920 run.py:483] Algo bellman_ford step 5780 current loss 0.020978, current_train_items 184992.
I0302 19:00:27.395296 22760421793920 run.py:483] Algo bellman_ford step 5781 current loss 0.017174, current_train_items 185024.
I0302 19:00:27.418673 22760421793920 run.py:483] Algo bellman_ford step 5782 current loss 0.027656, current_train_items 185056.
I0302 19:00:27.447516 22760421793920 run.py:483] Algo bellman_ford step 5783 current loss 0.029492, current_train_items 185088.
I0302 19:00:27.477570 22760421793920 run.py:483] Algo bellman_ford step 5784 current loss 0.035012, current_train_items 185120.
I0302 19:00:27.495577 22760421793920 run.py:483] Algo bellman_ford step 5785 current loss 0.001790, current_train_items 185152.
I0302 19:00:27.510983 22760421793920 run.py:483] Algo bellman_ford step 5786 current loss 0.028103, current_train_items 185184.
I0302 19:00:27.532677 22760421793920 run.py:483] Algo bellman_ford step 5787 current loss 0.035164, current_train_items 185216.
I0302 19:00:27.563690 22760421793920 run.py:483] Algo bellman_ford step 5788 current loss 0.074346, current_train_items 185248.
I0302 19:00:27.594280 22760421793920 run.py:483] Algo bellman_ford step 5789 current loss 0.100942, current_train_items 185280.
I0302 19:00:27.612124 22760421793920 run.py:483] Algo bellman_ford step 5790 current loss 0.006182, current_train_items 185312.
I0302 19:00:27.628164 22760421793920 run.py:483] Algo bellman_ford step 5791 current loss 0.042121, current_train_items 185344.
I0302 19:00:27.650118 22760421793920 run.py:483] Algo bellman_ford step 5792 current loss 0.078858, current_train_items 185376.
I0302 19:00:27.679871 22760421793920 run.py:483] Algo bellman_ford step 5793 current loss 0.074473, current_train_items 185408.
I0302 19:00:27.711824 22760421793920 run.py:483] Algo bellman_ford step 5794 current loss 0.071992, current_train_items 185440.
I0302 19:00:27.730050 22760421793920 run.py:483] Algo bellman_ford step 5795 current loss 0.002750, current_train_items 185472.
I0302 19:00:27.745647 22760421793920 run.py:483] Algo bellman_ford step 5796 current loss 0.009119, current_train_items 185504.
I0302 19:00:27.768907 22760421793920 run.py:483] Algo bellman_ford step 5797 current loss 0.068191, current_train_items 185536.
I0302 19:00:27.798851 22760421793920 run.py:483] Algo bellman_ford step 5798 current loss 0.093764, current_train_items 185568.
I0302 19:00:27.831283 22760421793920 run.py:483] Algo bellman_ford step 5799 current loss 0.101936, current_train_items 185600.
I0302 19:00:27.849333 22760421793920 run.py:483] Algo bellman_ford step 5800 current loss 0.004259, current_train_items 185632.
I0302 19:00:27.857063 22760421793920 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:00:27.857173 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:27.873690 22760421793920 run.py:483] Algo bellman_ford step 5801 current loss 0.040797, current_train_items 185664.
I0302 19:00:27.896117 22760421793920 run.py:483] Algo bellman_ford step 5802 current loss 0.039659, current_train_items 185696.
I0302 19:00:27.926682 22760421793920 run.py:483] Algo bellman_ford step 5803 current loss 0.112260, current_train_items 185728.
I0302 19:00:27.960860 22760421793920 run.py:483] Algo bellman_ford step 5804 current loss 0.096878, current_train_items 185760.
I0302 19:00:27.979684 22760421793920 run.py:483] Algo bellman_ford step 5805 current loss 0.003097, current_train_items 185792.
I0302 19:00:27.995654 22760421793920 run.py:483] Algo bellman_ford step 5806 current loss 0.009549, current_train_items 185824.
I0302 19:00:28.019382 22760421793920 run.py:483] Algo bellman_ford step 5807 current loss 0.056289, current_train_items 185856.
I0302 19:00:28.049735 22760421793920 run.py:483] Algo bellman_ford step 5808 current loss 0.083009, current_train_items 185888.
I0302 19:00:28.081484 22760421793920 run.py:483] Algo bellman_ford step 5809 current loss 0.113701, current_train_items 185920.
I0302 19:00:28.100137 22760421793920 run.py:483] Algo bellman_ford step 5810 current loss 0.002710, current_train_items 185952.
I0302 19:00:28.115759 22760421793920 run.py:483] Algo bellman_ford step 5811 current loss 0.014817, current_train_items 185984.
I0302 19:00:28.139080 22760421793920 run.py:483] Algo bellman_ford step 5812 current loss 0.054747, current_train_items 186016.
I0302 19:00:28.170016 22760421793920 run.py:483] Algo bellman_ford step 5813 current loss 0.086814, current_train_items 186048.
I0302 19:00:28.203115 22760421793920 run.py:483] Algo bellman_ford step 5814 current loss 0.047683, current_train_items 186080.
I0302 19:00:28.221621 22760421793920 run.py:483] Algo bellman_ford step 5815 current loss 0.003023, current_train_items 186112.
I0302 19:00:28.237573 22760421793920 run.py:483] Algo bellman_ford step 5816 current loss 0.030421, current_train_items 186144.
I0302 19:00:28.260866 22760421793920 run.py:483] Algo bellman_ford step 5817 current loss 0.102280, current_train_items 186176.
I0302 19:00:28.289849 22760421793920 run.py:483] Algo bellman_ford step 5818 current loss 0.050051, current_train_items 186208.
I0302 19:00:28.322838 22760421793920 run.py:483] Algo bellman_ford step 5819 current loss 0.055591, current_train_items 186240.
I0302 19:00:28.341279 22760421793920 run.py:483] Algo bellman_ford step 5820 current loss 0.004017, current_train_items 186272.
I0302 19:00:28.357055 22760421793920 run.py:483] Algo bellman_ford step 5821 current loss 0.003462, current_train_items 186304.
I0302 19:00:28.380344 22760421793920 run.py:483] Algo bellman_ford step 5822 current loss 0.067822, current_train_items 186336.
I0302 19:00:28.410799 22760421793920 run.py:483] Algo bellman_ford step 5823 current loss 0.086597, current_train_items 186368.
I0302 19:00:28.442244 22760421793920 run.py:483] Algo bellman_ford step 5824 current loss 0.102163, current_train_items 186400.
I0302 19:00:28.460695 22760421793920 run.py:483] Algo bellman_ford step 5825 current loss 0.012938, current_train_items 186432.
I0302 19:00:28.476532 22760421793920 run.py:483] Algo bellman_ford step 5826 current loss 0.020989, current_train_items 186464.
I0302 19:00:28.500059 22760421793920 run.py:483] Algo bellman_ford step 5827 current loss 0.071435, current_train_items 186496.
I0302 19:00:28.530695 22760421793920 run.py:483] Algo bellman_ford step 5828 current loss 0.102428, current_train_items 186528.
I0302 19:00:28.564104 22760421793920 run.py:483] Algo bellman_ford step 5829 current loss 0.132632, current_train_items 186560.
I0302 19:00:28.582430 22760421793920 run.py:483] Algo bellman_ford step 5830 current loss 0.005024, current_train_items 186592.
I0302 19:00:28.598387 22760421793920 run.py:483] Algo bellman_ford step 5831 current loss 0.026627, current_train_items 186624.
I0302 19:00:28.621054 22760421793920 run.py:483] Algo bellman_ford step 5832 current loss 0.043478, current_train_items 186656.
I0302 19:00:28.651300 22760421793920 run.py:483] Algo bellman_ford step 5833 current loss 0.068058, current_train_items 186688.
I0302 19:00:28.680138 22760421793920 run.py:483] Algo bellman_ford step 5834 current loss 0.046024, current_train_items 186720.
I0302 19:00:28.698603 22760421793920 run.py:483] Algo bellman_ford step 5835 current loss 0.003840, current_train_items 186752.
I0302 19:00:28.713698 22760421793920 run.py:483] Algo bellman_ford step 5836 current loss 0.007009, current_train_items 186784.
I0302 19:00:28.738321 22760421793920 run.py:483] Algo bellman_ford step 5837 current loss 0.029347, current_train_items 186816.
I0302 19:00:28.766911 22760421793920 run.py:483] Algo bellman_ford step 5838 current loss 0.021404, current_train_items 186848.
I0302 19:00:28.800591 22760421793920 run.py:483] Algo bellman_ford step 5839 current loss 0.053190, current_train_items 186880.
I0302 19:00:28.818854 22760421793920 run.py:483] Algo bellman_ford step 5840 current loss 0.003379, current_train_items 186912.
I0302 19:00:28.834839 22760421793920 run.py:483] Algo bellman_ford step 5841 current loss 0.019323, current_train_items 186944.
I0302 19:00:28.857549 22760421793920 run.py:483] Algo bellman_ford step 5842 current loss 0.042968, current_train_items 186976.
I0302 19:00:28.886268 22760421793920 run.py:483] Algo bellman_ford step 5843 current loss 0.037234, current_train_items 187008.
I0302 19:00:28.917894 22760421793920 run.py:483] Algo bellman_ford step 5844 current loss 0.107038, current_train_items 187040.
I0302 19:00:28.936641 22760421793920 run.py:483] Algo bellman_ford step 5845 current loss 0.003655, current_train_items 187072.
I0302 19:00:28.952501 22760421793920 run.py:483] Algo bellman_ford step 5846 current loss 0.017549, current_train_items 187104.
I0302 19:00:28.976649 22760421793920 run.py:483] Algo bellman_ford step 5847 current loss 0.035635, current_train_items 187136.
I0302 19:00:29.005424 22760421793920 run.py:483] Algo bellman_ford step 5848 current loss 0.058330, current_train_items 187168.
I0302 19:00:29.037960 22760421793920 run.py:483] Algo bellman_ford step 5849 current loss 0.064311, current_train_items 187200.
I0302 19:00:29.055920 22760421793920 run.py:483] Algo bellman_ford step 5850 current loss 0.008039, current_train_items 187232.
I0302 19:00:29.063450 22760421793920 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:00:29.063561 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.992, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:00:29.091933 22760421793920 run.py:483] Algo bellman_ford step 5851 current loss 0.007124, current_train_items 187264.
I0302 19:00:29.114218 22760421793920 run.py:483] Algo bellman_ford step 5852 current loss 0.053482, current_train_items 187296.
I0302 19:00:29.143660 22760421793920 run.py:483] Algo bellman_ford step 5853 current loss 0.049149, current_train_items 187328.
I0302 19:00:29.176753 22760421793920 run.py:483] Algo bellman_ford step 5854 current loss 0.049925, current_train_items 187360.
I0302 19:00:29.194931 22760421793920 run.py:483] Algo bellman_ford step 5855 current loss 0.001900, current_train_items 187392.
I0302 19:00:29.211004 22760421793920 run.py:483] Algo bellman_ford step 5856 current loss 0.041863, current_train_items 187424.
I0302 19:00:29.234070 22760421793920 run.py:483] Algo bellman_ford step 5857 current loss 0.077936, current_train_items 187456.
I0302 19:00:29.262844 22760421793920 run.py:483] Algo bellman_ford step 5858 current loss 0.042552, current_train_items 187488.
I0302 19:00:29.295096 22760421793920 run.py:483] Algo bellman_ford step 5859 current loss 0.042043, current_train_items 187520.
I0302 19:00:29.313469 22760421793920 run.py:483] Algo bellman_ford step 5860 current loss 0.004391, current_train_items 187552.
I0302 19:00:29.329305 22760421793920 run.py:483] Algo bellman_ford step 5861 current loss 0.011313, current_train_items 187584.
I0302 19:00:29.351318 22760421793920 run.py:483] Algo bellman_ford step 5862 current loss 0.023257, current_train_items 187616.
I0302 19:00:29.381394 22760421793920 run.py:483] Algo bellman_ford step 5863 current loss 0.036207, current_train_items 187648.
I0302 19:00:29.414176 22760421793920 run.py:483] Algo bellman_ford step 5864 current loss 0.073072, current_train_items 187680.
I0302 19:00:29.432315 22760421793920 run.py:483] Algo bellman_ford step 5865 current loss 0.012969, current_train_items 187712.
I0302 19:00:29.448558 22760421793920 run.py:483] Algo bellman_ford step 5866 current loss 0.032613, current_train_items 187744.
I0302 19:00:29.471397 22760421793920 run.py:483] Algo bellman_ford step 5867 current loss 0.052988, current_train_items 187776.
I0302 19:00:29.500694 22760421793920 run.py:483] Algo bellman_ford step 5868 current loss 0.192914, current_train_items 187808.
I0302 19:00:29.534222 22760421793920 run.py:483] Algo bellman_ford step 5869 current loss 0.190465, current_train_items 187840.
I0302 19:00:29.552511 22760421793920 run.py:483] Algo bellman_ford step 5870 current loss 0.000907, current_train_items 187872.
I0302 19:00:29.568169 22760421793920 run.py:483] Algo bellman_ford step 5871 current loss 0.013419, current_train_items 187904.
I0302 19:00:29.590081 22760421793920 run.py:483] Algo bellman_ford step 5872 current loss 0.030035, current_train_items 187936.
I0302 19:00:29.619415 22760421793920 run.py:483] Algo bellman_ford step 5873 current loss 0.074345, current_train_items 187968.
I0302 19:00:29.651547 22760421793920 run.py:483] Algo bellman_ford step 5874 current loss 0.067040, current_train_items 188000.
I0302 19:00:29.669569 22760421793920 run.py:483] Algo bellman_ford step 5875 current loss 0.001681, current_train_items 188032.
I0302 19:00:29.685232 22760421793920 run.py:483] Algo bellman_ford step 5876 current loss 0.010323, current_train_items 188064.
I0302 19:00:29.707360 22760421793920 run.py:483] Algo bellman_ford step 5877 current loss 0.057934, current_train_items 188096.
I0302 19:00:29.736585 22760421793920 run.py:483] Algo bellman_ford step 5878 current loss 0.016152, current_train_items 188128.
I0302 19:00:29.769161 22760421793920 run.py:483] Algo bellman_ford step 5879 current loss 0.045937, current_train_items 188160.
I0302 19:00:29.787341 22760421793920 run.py:483] Algo bellman_ford step 5880 current loss 0.027968, current_train_items 188192.
I0302 19:00:29.802804 22760421793920 run.py:483] Algo bellman_ford step 5881 current loss 0.026971, current_train_items 188224.
I0302 19:00:29.825936 22760421793920 run.py:483] Algo bellman_ford step 5882 current loss 0.047455, current_train_items 188256.
I0302 19:00:29.855478 22760421793920 run.py:483] Algo bellman_ford step 5883 current loss 0.027977, current_train_items 188288.
I0302 19:00:29.888188 22760421793920 run.py:483] Algo bellman_ford step 5884 current loss 0.070280, current_train_items 188320.
I0302 19:00:29.906544 22760421793920 run.py:483] Algo bellman_ford step 5885 current loss 0.002983, current_train_items 188352.
I0302 19:00:29.921986 22760421793920 run.py:483] Algo bellman_ford step 5886 current loss 0.011644, current_train_items 188384.
I0302 19:00:29.944590 22760421793920 run.py:483] Algo bellman_ford step 5887 current loss 0.047655, current_train_items 188416.
I0302 19:00:29.973661 22760421793920 run.py:483] Algo bellman_ford step 5888 current loss 0.055320, current_train_items 188448.
I0302 19:00:30.004215 22760421793920 run.py:483] Algo bellman_ford step 5889 current loss 0.152705, current_train_items 188480.
I0302 19:00:30.022411 22760421793920 run.py:483] Algo bellman_ford step 5890 current loss 0.002385, current_train_items 188512.
I0302 19:00:30.038398 22760421793920 run.py:483] Algo bellman_ford step 5891 current loss 0.049339, current_train_items 188544.
I0302 19:00:30.060965 22760421793920 run.py:483] Algo bellman_ford step 5892 current loss 0.055773, current_train_items 188576.
I0302 19:00:30.089659 22760421793920 run.py:483] Algo bellman_ford step 5893 current loss 0.019220, current_train_items 188608.
I0302 19:00:30.119081 22760421793920 run.py:483] Algo bellman_ford step 5894 current loss 0.031648, current_train_items 188640.
I0302 19:00:30.137407 22760421793920 run.py:483] Algo bellman_ford step 5895 current loss 0.002170, current_train_items 188672.
I0302 19:00:30.153206 22760421793920 run.py:483] Algo bellman_ford step 5896 current loss 0.029410, current_train_items 188704.
I0302 19:00:30.176642 22760421793920 run.py:483] Algo bellman_ford step 5897 current loss 0.047881, current_train_items 188736.
I0302 19:00:30.206713 22760421793920 run.py:483] Algo bellman_ford step 5898 current loss 0.060524, current_train_items 188768.
I0302 19:00:30.238117 22760421793920 run.py:483] Algo bellman_ford step 5899 current loss 0.056867, current_train_items 188800.
I0302 19:00:30.256218 22760421793920 run.py:483] Algo bellman_ford step 5900 current loss 0.003581, current_train_items 188832.
I0302 19:00:30.263727 22760421793920 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:00:30.263838 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:00:30.279859 22760421793920 run.py:483] Algo bellman_ford step 5901 current loss 0.012726, current_train_items 188864.
I0302 19:00:30.303430 22760421793920 run.py:483] Algo bellman_ford step 5902 current loss 0.032954, current_train_items 188896.
I0302 19:00:30.334124 22760421793920 run.py:483] Algo bellman_ford step 5903 current loss 0.073203, current_train_items 188928.
I0302 19:00:30.364394 22760421793920 run.py:483] Algo bellman_ford step 5904 current loss 0.040383, current_train_items 188960.
I0302 19:00:30.382814 22760421793920 run.py:483] Algo bellman_ford step 5905 current loss 0.003820, current_train_items 188992.
I0302 19:00:30.398799 22760421793920 run.py:483] Algo bellman_ford step 5906 current loss 0.072349, current_train_items 189024.
I0302 19:00:30.421308 22760421793920 run.py:483] Algo bellman_ford step 5907 current loss 0.039360, current_train_items 189056.
I0302 19:00:30.451676 22760421793920 run.py:483] Algo bellman_ford step 5908 current loss 0.050452, current_train_items 189088.
I0302 19:00:30.481951 22760421793920 run.py:483] Algo bellman_ford step 5909 current loss 0.103491, current_train_items 189120.
I0302 19:00:30.500161 22760421793920 run.py:483] Algo bellman_ford step 5910 current loss 0.002303, current_train_items 189152.
I0302 19:00:30.516075 22760421793920 run.py:483] Algo bellman_ford step 5911 current loss 0.027524, current_train_items 189184.
I0302 19:00:30.539166 22760421793920 run.py:483] Algo bellman_ford step 5912 current loss 0.097421, current_train_items 189216.
I0302 19:00:30.568318 22760421793920 run.py:483] Algo bellman_ford step 5913 current loss 0.073441, current_train_items 189248.
I0302 19:00:30.599119 22760421793920 run.py:483] Algo bellman_ford step 5914 current loss 0.077592, current_train_items 189280.
I0302 19:00:30.617279 22760421793920 run.py:483] Algo bellman_ford step 5915 current loss 0.004256, current_train_items 189312.
I0302 19:00:30.633113 22760421793920 run.py:483] Algo bellman_ford step 5916 current loss 0.033505, current_train_items 189344.
I0302 19:00:30.655737 22760421793920 run.py:483] Algo bellman_ford step 5917 current loss 0.048702, current_train_items 189376.
I0302 19:00:30.685087 22760421793920 run.py:483] Algo bellman_ford step 5918 current loss 0.035794, current_train_items 189408.
I0302 19:00:30.717486 22760421793920 run.py:483] Algo bellman_ford step 5919 current loss 0.044649, current_train_items 189440.
I0302 19:00:30.735642 22760421793920 run.py:483] Algo bellman_ford step 5920 current loss 0.035660, current_train_items 189472.
I0302 19:00:30.751366 22760421793920 run.py:483] Algo bellman_ford step 5921 current loss 0.016696, current_train_items 189504.
I0302 19:00:30.773465 22760421793920 run.py:483] Algo bellman_ford step 5922 current loss 0.045590, current_train_items 189536.
I0302 19:00:30.801614 22760421793920 run.py:483] Algo bellman_ford step 5923 current loss 0.037285, current_train_items 189568.
I0302 19:00:30.833219 22760421793920 run.py:483] Algo bellman_ford step 5924 current loss 0.050824, current_train_items 189600.
I0302 19:00:30.851084 22760421793920 run.py:483] Algo bellman_ford step 5925 current loss 0.001405, current_train_items 189632.
I0302 19:00:30.866533 22760421793920 run.py:483] Algo bellman_ford step 5926 current loss 0.023722, current_train_items 189664.
I0302 19:00:30.888539 22760421793920 run.py:483] Algo bellman_ford step 5927 current loss 0.075404, current_train_items 189696.
I0302 19:00:30.918416 22760421793920 run.py:483] Algo bellman_ford step 5928 current loss 0.083970, current_train_items 189728.
I0302 19:00:30.948835 22760421793920 run.py:483] Algo bellman_ford step 5929 current loss 0.033330, current_train_items 189760.
I0302 19:00:30.967174 22760421793920 run.py:483] Algo bellman_ford step 5930 current loss 0.003892, current_train_items 189792.
I0302 19:00:30.982997 22760421793920 run.py:483] Algo bellman_ford step 5931 current loss 0.037048, current_train_items 189824.
I0302 19:00:31.005389 22760421793920 run.py:483] Algo bellman_ford step 5932 current loss 0.088372, current_train_items 189856.
I0302 19:00:31.034120 22760421793920 run.py:483] Algo bellman_ford step 5933 current loss 0.088174, current_train_items 189888.
I0302 19:00:31.066905 22760421793920 run.py:483] Algo bellman_ford step 5934 current loss 0.185450, current_train_items 189920.
I0302 19:00:31.084973 22760421793920 run.py:483] Algo bellman_ford step 5935 current loss 0.017512, current_train_items 189952.
I0302 19:00:31.100258 22760421793920 run.py:483] Algo bellman_ford step 5936 current loss 0.019752, current_train_items 189984.
I0302 19:00:31.122880 22760421793920 run.py:483] Algo bellman_ford step 5937 current loss 0.050039, current_train_items 190016.
I0302 19:00:31.153370 22760421793920 run.py:483] Algo bellman_ford step 5938 current loss 0.110631, current_train_items 190048.
I0302 19:00:31.183462 22760421793920 run.py:483] Algo bellman_ford step 5939 current loss 0.149776, current_train_items 190080.
I0302 19:00:31.201882 22760421793920 run.py:483] Algo bellman_ford step 5940 current loss 0.006608, current_train_items 190112.
I0302 19:00:31.217332 22760421793920 run.py:483] Algo bellman_ford step 5941 current loss 0.032062, current_train_items 190144.
I0302 19:00:31.240371 22760421793920 run.py:483] Algo bellman_ford step 5942 current loss 0.038534, current_train_items 190176.
I0302 19:00:31.270126 22760421793920 run.py:483] Algo bellman_ford step 5943 current loss 0.041985, current_train_items 190208.
I0302 19:00:31.302098 22760421793920 run.py:483] Algo bellman_ford step 5944 current loss 0.094721, current_train_items 190240.
I0302 19:00:31.320266 22760421793920 run.py:483] Algo bellman_ford step 5945 current loss 0.006073, current_train_items 190272.
I0302 19:00:31.336210 22760421793920 run.py:483] Algo bellman_ford step 5946 current loss 0.021355, current_train_items 190304.
I0302 19:00:31.358951 22760421793920 run.py:483] Algo bellman_ford step 5947 current loss 0.064906, current_train_items 190336.
I0302 19:00:31.387512 22760421793920 run.py:483] Algo bellman_ford step 5948 current loss 0.035402, current_train_items 190368.
I0302 19:00:31.419292 22760421793920 run.py:483] Algo bellman_ford step 5949 current loss 0.054454, current_train_items 190400.
I0302 19:00:31.437132 22760421793920 run.py:483] Algo bellman_ford step 5950 current loss 0.005497, current_train_items 190432.
I0302 19:00:31.444571 22760421793920 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:00:31.444681 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:31.460937 22760421793920 run.py:483] Algo bellman_ford step 5951 current loss 0.008153, current_train_items 190464.
I0302 19:00:31.484369 22760421793920 run.py:483] Algo bellman_ford step 5952 current loss 0.076357, current_train_items 190496.
I0302 19:00:31.514675 22760421793920 run.py:483] Algo bellman_ford step 5953 current loss 0.091057, current_train_items 190528.
I0302 19:00:31.546685 22760421793920 run.py:483] Algo bellman_ford step 5954 current loss 0.176875, current_train_items 190560.
I0302 19:00:31.565373 22760421793920 run.py:483] Algo bellman_ford step 5955 current loss 0.014713, current_train_items 190592.
I0302 19:00:31.581724 22760421793920 run.py:483] Algo bellman_ford step 5956 current loss 0.037324, current_train_items 190624.
I0302 19:00:31.603309 22760421793920 run.py:483] Algo bellman_ford step 5957 current loss 0.063111, current_train_items 190656.
I0302 19:00:31.632637 22760421793920 run.py:483] Algo bellman_ford step 5958 current loss 0.064124, current_train_items 190688.
I0302 19:00:31.664202 22760421793920 run.py:483] Algo bellman_ford step 5959 current loss 0.093971, current_train_items 190720.
I0302 19:00:31.682643 22760421793920 run.py:483] Algo bellman_ford step 5960 current loss 0.004484, current_train_items 190752.
I0302 19:00:31.698241 22760421793920 run.py:483] Algo bellman_ford step 5961 current loss 0.054537, current_train_items 190784.
I0302 19:00:31.721731 22760421793920 run.py:483] Algo bellman_ford step 5962 current loss 0.107330, current_train_items 190816.
I0302 19:00:31.750103 22760421793920 run.py:483] Algo bellman_ford step 5963 current loss 0.037205, current_train_items 190848.
I0302 19:00:31.781487 22760421793920 run.py:483] Algo bellman_ford step 5964 current loss 0.104914, current_train_items 190880.
I0302 19:00:31.799570 22760421793920 run.py:483] Algo bellman_ford step 5965 current loss 0.058024, current_train_items 190912.
I0302 19:00:31.815114 22760421793920 run.py:483] Algo bellman_ford step 5966 current loss 0.046210, current_train_items 190944.
I0302 19:00:31.838207 22760421793920 run.py:483] Algo bellman_ford step 5967 current loss 0.049506, current_train_items 190976.
I0302 19:00:31.867501 22760421793920 run.py:483] Algo bellman_ford step 5968 current loss 0.066987, current_train_items 191008.
I0302 19:00:31.900081 22760421793920 run.py:483] Algo bellman_ford step 5969 current loss 0.067083, current_train_items 191040.
I0302 19:00:31.918122 22760421793920 run.py:483] Algo bellman_ford step 5970 current loss 0.007049, current_train_items 191072.
I0302 19:00:31.933628 22760421793920 run.py:483] Algo bellman_ford step 5971 current loss 0.012194, current_train_items 191104.
I0302 19:00:31.956170 22760421793920 run.py:483] Algo bellman_ford step 5972 current loss 0.023287, current_train_items 191136.
I0302 19:00:31.986366 22760421793920 run.py:483] Algo bellman_ford step 5973 current loss 0.060492, current_train_items 191168.
I0302 19:00:32.017612 22760421793920 run.py:483] Algo bellman_ford step 5974 current loss 0.061712, current_train_items 191200.
I0302 19:00:32.035510 22760421793920 run.py:483] Algo bellman_ford step 5975 current loss 0.008419, current_train_items 191232.
I0302 19:00:32.050922 22760421793920 run.py:483] Algo bellman_ford step 5976 current loss 0.003320, current_train_items 191264.
I0302 19:00:32.073318 22760421793920 run.py:483] Algo bellman_ford step 5977 current loss 0.047843, current_train_items 191296.
I0302 19:00:32.103317 22760421793920 run.py:483] Algo bellman_ford step 5978 current loss 0.066865, current_train_items 191328.
I0302 19:00:32.133819 22760421793920 run.py:483] Algo bellman_ford step 5979 current loss 0.030238, current_train_items 191360.
I0302 19:00:32.151562 22760421793920 run.py:483] Algo bellman_ford step 5980 current loss 0.005485, current_train_items 191392.
I0302 19:00:32.167858 22760421793920 run.py:483] Algo bellman_ford step 5981 current loss 0.037007, current_train_items 191424.
I0302 19:00:32.189960 22760421793920 run.py:483] Algo bellman_ford step 5982 current loss 0.044551, current_train_items 191456.
I0302 19:00:32.219314 22760421793920 run.py:483] Algo bellman_ford step 5983 current loss 0.089306, current_train_items 191488.
I0302 19:00:32.250244 22760421793920 run.py:483] Algo bellman_ford step 5984 current loss 0.065472, current_train_items 191520.
I0302 19:00:32.268633 22760421793920 run.py:483] Algo bellman_ford step 5985 current loss 0.006341, current_train_items 191552.
I0302 19:00:32.284749 22760421793920 run.py:483] Algo bellman_ford step 5986 current loss 0.016535, current_train_items 191584.
I0302 19:00:32.307524 22760421793920 run.py:483] Algo bellman_ford step 5987 current loss 0.087100, current_train_items 191616.
I0302 19:00:32.336135 22760421793920 run.py:483] Algo bellman_ford step 5988 current loss 0.053752, current_train_items 191648.
I0302 19:00:32.367383 22760421793920 run.py:483] Algo bellman_ford step 5989 current loss 0.054401, current_train_items 191680.
I0302 19:00:32.385505 22760421793920 run.py:483] Algo bellman_ford step 5990 current loss 0.002080, current_train_items 191712.
I0302 19:00:32.400538 22760421793920 run.py:483] Algo bellman_ford step 5991 current loss 0.012661, current_train_items 191744.
I0302 19:00:32.423586 22760421793920 run.py:483] Algo bellman_ford step 5992 current loss 0.061088, current_train_items 191776.
I0302 19:00:32.454649 22760421793920 run.py:483] Algo bellman_ford step 5993 current loss 0.122278, current_train_items 191808.
I0302 19:00:32.486690 22760421793920 run.py:483] Algo bellman_ford step 5994 current loss 0.070518, current_train_items 191840.
I0302 19:00:32.504704 22760421793920 run.py:483] Algo bellman_ford step 5995 current loss 0.002344, current_train_items 191872.
I0302 19:00:32.520380 22760421793920 run.py:483] Algo bellman_ford step 5996 current loss 0.021811, current_train_items 191904.
I0302 19:00:32.543243 22760421793920 run.py:483] Algo bellman_ford step 5997 current loss 0.050356, current_train_items 191936.
I0302 19:00:32.572444 22760421793920 run.py:483] Algo bellman_ford step 5998 current loss 0.036891, current_train_items 191968.
I0302 19:00:32.603060 22760421793920 run.py:483] Algo bellman_ford step 5999 current loss 0.037764, current_train_items 192000.
I0302 19:00:32.621661 22760421793920 run.py:483] Algo bellman_ford step 6000 current loss 0.008117, current_train_items 192032.
I0302 19:00:32.629085 22760421793920 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:00:32.629195 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 19:00:32.645540 22760421793920 run.py:483] Algo bellman_ford step 6001 current loss 0.035291, current_train_items 192064.
I0302 19:00:32.668642 22760421793920 run.py:483] Algo bellman_ford step 6002 current loss 0.096659, current_train_items 192096.
I0302 19:00:32.697809 22760421793920 run.py:483] Algo bellman_ford step 6003 current loss 0.071966, current_train_items 192128.
I0302 19:00:32.729510 22760421793920 run.py:483] Algo bellman_ford step 6004 current loss 0.056166, current_train_items 192160.
I0302 19:00:32.747778 22760421793920 run.py:483] Algo bellman_ford step 6005 current loss 0.004779, current_train_items 192192.
I0302 19:00:32.763290 22760421793920 run.py:483] Algo bellman_ford step 6006 current loss 0.015256, current_train_items 192224.
I0302 19:00:32.787160 22760421793920 run.py:483] Algo bellman_ford step 6007 current loss 0.153053, current_train_items 192256.
I0302 19:00:32.818935 22760421793920 run.py:483] Algo bellman_ford step 6008 current loss 0.171588, current_train_items 192288.
I0302 19:00:32.851978 22760421793920 run.py:483] Algo bellman_ford step 6009 current loss 0.085911, current_train_items 192320.
I0302 19:00:32.870014 22760421793920 run.py:483] Algo bellman_ford step 6010 current loss 0.003135, current_train_items 192352.
I0302 19:00:32.885727 22760421793920 run.py:483] Algo bellman_ford step 6011 current loss 0.016897, current_train_items 192384.
I0302 19:00:32.908702 22760421793920 run.py:483] Algo bellman_ford step 6012 current loss 0.046064, current_train_items 192416.
I0302 19:00:32.937676 22760421793920 run.py:483] Algo bellman_ford step 6013 current loss 0.062094, current_train_items 192448.
I0302 19:00:32.971252 22760421793920 run.py:483] Algo bellman_ford step 6014 current loss 0.117682, current_train_items 192480.
I0302 19:00:32.989330 22760421793920 run.py:483] Algo bellman_ford step 6015 current loss 0.005942, current_train_items 192512.
I0302 19:00:33.005274 22760421793920 run.py:483] Algo bellman_ford step 6016 current loss 0.015955, current_train_items 192544.
I0302 19:00:33.027584 22760421793920 run.py:483] Algo bellman_ford step 6017 current loss 0.025165, current_train_items 192576.
I0302 19:00:33.057827 22760421793920 run.py:483] Algo bellman_ford step 6018 current loss 0.049160, current_train_items 192608.
I0302 19:00:33.089833 22760421793920 run.py:483] Algo bellman_ford step 6019 current loss 0.056493, current_train_items 192640.
I0302 19:00:33.108045 22760421793920 run.py:483] Algo bellman_ford step 6020 current loss 0.004529, current_train_items 192672.
I0302 19:00:33.124104 22760421793920 run.py:483] Algo bellman_ford step 6021 current loss 0.006795, current_train_items 192704.
I0302 19:00:33.147184 22760421793920 run.py:483] Algo bellman_ford step 6022 current loss 0.024051, current_train_items 192736.
I0302 19:00:33.178370 22760421793920 run.py:483] Algo bellman_ford step 6023 current loss 0.078679, current_train_items 192768.
I0302 19:00:33.210749 22760421793920 run.py:483] Algo bellman_ford step 6024 current loss 0.101927, current_train_items 192800.
I0302 19:00:33.228610 22760421793920 run.py:483] Algo bellman_ford step 6025 current loss 0.003119, current_train_items 192832.
I0302 19:00:33.244535 22760421793920 run.py:483] Algo bellman_ford step 6026 current loss 0.028187, current_train_items 192864.
I0302 19:00:33.267215 22760421793920 run.py:483] Algo bellman_ford step 6027 current loss 0.046549, current_train_items 192896.
I0302 19:00:33.296892 22760421793920 run.py:483] Algo bellman_ford step 6028 current loss 0.107494, current_train_items 192928.
I0302 19:00:33.328632 22760421793920 run.py:483] Algo bellman_ford step 6029 current loss 0.126988, current_train_items 192960.
I0302 19:00:33.346810 22760421793920 run.py:483] Algo bellman_ford step 6030 current loss 0.004113, current_train_items 192992.
I0302 19:00:33.362290 22760421793920 run.py:483] Algo bellman_ford step 6031 current loss 0.014475, current_train_items 193024.
I0302 19:00:33.385042 22760421793920 run.py:483] Algo bellman_ford step 6032 current loss 0.052132, current_train_items 193056.
I0302 19:00:33.416366 22760421793920 run.py:483] Algo bellman_ford step 6033 current loss 0.103435, current_train_items 193088.
I0302 19:00:33.447643 22760421793920 run.py:483] Algo bellman_ford step 6034 current loss 0.063141, current_train_items 193120.
I0302 19:00:33.465655 22760421793920 run.py:483] Algo bellman_ford step 6035 current loss 0.001955, current_train_items 193152.
I0302 19:00:33.481198 22760421793920 run.py:483] Algo bellman_ford step 6036 current loss 0.011528, current_train_items 193184.
I0302 19:00:33.504034 22760421793920 run.py:483] Algo bellman_ford step 6037 current loss 0.027580, current_train_items 193216.
I0302 19:00:33.534328 22760421793920 run.py:483] Algo bellman_ford step 6038 current loss 0.078203, current_train_items 193248.
I0302 19:00:33.565083 22760421793920 run.py:483] Algo bellman_ford step 6039 current loss 0.056439, current_train_items 193280.
I0302 19:00:33.583208 22760421793920 run.py:483] Algo bellman_ford step 6040 current loss 0.001258, current_train_items 193312.
I0302 19:00:33.598996 22760421793920 run.py:483] Algo bellman_ford step 6041 current loss 0.018122, current_train_items 193344.
I0302 19:00:33.622646 22760421793920 run.py:483] Algo bellman_ford step 6042 current loss 0.048264, current_train_items 193376.
I0302 19:00:33.652140 22760421793920 run.py:483] Algo bellman_ford step 6043 current loss 0.038208, current_train_items 193408.
I0302 19:00:33.686589 22760421793920 run.py:483] Algo bellman_ford step 6044 current loss 0.065664, current_train_items 193440.
I0302 19:00:33.704867 22760421793920 run.py:483] Algo bellman_ford step 6045 current loss 0.000965, current_train_items 193472.
I0302 19:00:33.720918 22760421793920 run.py:483] Algo bellman_ford step 6046 current loss 0.019508, current_train_items 193504.
I0302 19:00:33.743737 22760421793920 run.py:483] Algo bellman_ford step 6047 current loss 0.048343, current_train_items 193536.
I0302 19:00:33.774602 22760421793920 run.py:483] Algo bellman_ford step 6048 current loss 0.059893, current_train_items 193568.
I0302 19:00:33.807125 22760421793920 run.py:483] Algo bellman_ford step 6049 current loss 0.040368, current_train_items 193600.
I0302 19:00:33.825569 22760421793920 run.py:483] Algo bellman_ford step 6050 current loss 0.003106, current_train_items 193632.
I0302 19:00:33.833266 22760421793920 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:00:33.833408 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 19:00:33.849505 22760421793920 run.py:483] Algo bellman_ford step 6051 current loss 0.017497, current_train_items 193664.
I0302 19:00:33.873473 22760421793920 run.py:483] Algo bellman_ford step 6052 current loss 0.109218, current_train_items 193696.
I0302 19:00:33.903527 22760421793920 run.py:483] Algo bellman_ford step 6053 current loss 0.056363, current_train_items 193728.
I0302 19:00:33.936407 22760421793920 run.py:483] Algo bellman_ford step 6054 current loss 0.069620, current_train_items 193760.
I0302 19:00:33.955046 22760421793920 run.py:483] Algo bellman_ford step 6055 current loss 0.013203, current_train_items 193792.
I0302 19:00:33.970747 22760421793920 run.py:483] Algo bellman_ford step 6056 current loss 0.030059, current_train_items 193824.
I0302 19:00:33.992677 22760421793920 run.py:483] Algo bellman_ford step 6057 current loss 0.123328, current_train_items 193856.
I0302 19:00:34.022149 22760421793920 run.py:483] Algo bellman_ford step 6058 current loss 0.097425, current_train_items 193888.
I0302 19:00:34.051483 22760421793920 run.py:483] Algo bellman_ford step 6059 current loss 0.069530, current_train_items 193920.
I0302 19:00:34.069713 22760421793920 run.py:483] Algo bellman_ford step 6060 current loss 0.003575, current_train_items 193952.
I0302 19:00:34.085455 22760421793920 run.py:483] Algo bellman_ford step 6061 current loss 0.027668, current_train_items 193984.
I0302 19:00:34.106940 22760421793920 run.py:483] Algo bellman_ford step 6062 current loss 0.022741, current_train_items 194016.
I0302 19:00:34.135231 22760421793920 run.py:483] Algo bellman_ford step 6063 current loss 0.043099, current_train_items 194048.
I0302 19:00:34.167534 22760421793920 run.py:483] Algo bellman_ford step 6064 current loss 0.093779, current_train_items 194080.
I0302 19:00:34.185660 22760421793920 run.py:483] Algo bellman_ford step 6065 current loss 0.003463, current_train_items 194112.
I0302 19:00:34.201305 22760421793920 run.py:483] Algo bellman_ford step 6066 current loss 0.031084, current_train_items 194144.
I0302 19:00:34.224616 22760421793920 run.py:483] Algo bellman_ford step 6067 current loss 0.073952, current_train_items 194176.
I0302 19:00:34.254839 22760421793920 run.py:483] Algo bellman_ford step 6068 current loss 0.089135, current_train_items 194208.
I0302 19:00:34.285856 22760421793920 run.py:483] Algo bellman_ford step 6069 current loss 0.061540, current_train_items 194240.
I0302 19:00:34.303895 22760421793920 run.py:483] Algo bellman_ford step 6070 current loss 0.005498, current_train_items 194272.
I0302 19:00:34.319329 22760421793920 run.py:483] Algo bellman_ford step 6071 current loss 0.010088, current_train_items 194304.
I0302 19:00:34.342378 22760421793920 run.py:483] Algo bellman_ford step 6072 current loss 0.052928, current_train_items 194336.
I0302 19:00:34.371195 22760421793920 run.py:483] Algo bellman_ford step 6073 current loss 0.059304, current_train_items 194368.
I0302 19:00:34.403267 22760421793920 run.py:483] Algo bellman_ford step 6074 current loss 0.058707, current_train_items 194400.
I0302 19:00:34.421470 22760421793920 run.py:483] Algo bellman_ford step 6075 current loss 0.009936, current_train_items 194432.
I0302 19:00:34.437165 22760421793920 run.py:483] Algo bellman_ford step 6076 current loss 0.029225, current_train_items 194464.
I0302 19:00:34.459414 22760421793920 run.py:483] Algo bellman_ford step 6077 current loss 0.044852, current_train_items 194496.
I0302 19:00:34.488621 22760421793920 run.py:483] Algo bellman_ford step 6078 current loss 0.071433, current_train_items 194528.
I0302 19:00:34.521189 22760421793920 run.py:483] Algo bellman_ford step 6079 current loss 0.096455, current_train_items 194560.
I0302 19:00:34.539325 22760421793920 run.py:483] Algo bellman_ford step 6080 current loss 0.003175, current_train_items 194592.
I0302 19:00:34.555153 22760421793920 run.py:483] Algo bellman_ford step 6081 current loss 0.005024, current_train_items 194624.
I0302 19:00:34.577605 22760421793920 run.py:483] Algo bellman_ford step 6082 current loss 0.047513, current_train_items 194656.
I0302 19:00:34.608597 22760421793920 run.py:483] Algo bellman_ford step 6083 current loss 0.065385, current_train_items 194688.
I0302 19:00:34.639106 22760421793920 run.py:483] Algo bellman_ford step 6084 current loss 0.057059, current_train_items 194720.
I0302 19:00:34.657058 22760421793920 run.py:483] Algo bellman_ford step 6085 current loss 0.009994, current_train_items 194752.
I0302 19:00:34.672941 22760421793920 run.py:483] Algo bellman_ford step 6086 current loss 0.021851, current_train_items 194784.
I0302 19:00:34.695516 22760421793920 run.py:483] Algo bellman_ford step 6087 current loss 0.048565, current_train_items 194816.
I0302 19:00:34.724929 22760421793920 run.py:483] Algo bellman_ford step 6088 current loss 0.062600, current_train_items 194848.
I0302 19:00:34.757440 22760421793920 run.py:483] Algo bellman_ford step 6089 current loss 0.147080, current_train_items 194880.
I0302 19:00:34.775632 22760421793920 run.py:483] Algo bellman_ford step 6090 current loss 0.013714, current_train_items 194912.
I0302 19:00:34.791434 22760421793920 run.py:483] Algo bellman_ford step 6091 current loss 0.008966, current_train_items 194944.
I0302 19:00:34.814185 22760421793920 run.py:483] Algo bellman_ford step 6092 current loss 0.040813, current_train_items 194976.
I0302 19:00:34.843366 22760421793920 run.py:483] Algo bellman_ford step 6093 current loss 0.075119, current_train_items 195008.
I0302 19:00:34.874297 22760421793920 run.py:483] Algo bellman_ford step 6094 current loss 0.102384, current_train_items 195040.
I0302 19:00:34.892541 22760421793920 run.py:483] Algo bellman_ford step 6095 current loss 0.002954, current_train_items 195072.
I0302 19:00:34.908368 22760421793920 run.py:483] Algo bellman_ford step 6096 current loss 0.019436, current_train_items 195104.
I0302 19:00:34.931479 22760421793920 run.py:483] Algo bellman_ford step 6097 current loss 0.033629, current_train_items 195136.
I0302 19:00:34.960412 22760421793920 run.py:483] Algo bellman_ford step 6098 current loss 0.038732, current_train_items 195168.
I0302 19:00:34.992516 22760421793920 run.py:483] Algo bellman_ford step 6099 current loss 0.091432, current_train_items 195200.
I0302 19:00:35.010458 22760421793920 run.py:483] Algo bellman_ford step 6100 current loss 0.002374, current_train_items 195232.
I0302 19:00:35.017938 22760421793920 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:00:35.018053 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:35.033906 22760421793920 run.py:483] Algo bellman_ford step 6101 current loss 0.008252, current_train_items 195264.
I0302 19:00:35.058107 22760421793920 run.py:483] Algo bellman_ford step 6102 current loss 0.032068, current_train_items 195296.
I0302 19:00:35.089470 22760421793920 run.py:483] Algo bellman_ford step 6103 current loss 0.079210, current_train_items 195328.
I0302 19:00:35.120876 22760421793920 run.py:483] Algo bellman_ford step 6104 current loss 0.047747, current_train_items 195360.
I0302 19:00:35.139258 22760421793920 run.py:483] Algo bellman_ford step 6105 current loss 0.001411, current_train_items 195392.
I0302 19:00:35.154395 22760421793920 run.py:483] Algo bellman_ford step 6106 current loss 0.028565, current_train_items 195424.
I0302 19:00:35.177142 22760421793920 run.py:483] Algo bellman_ford step 6107 current loss 0.102617, current_train_items 195456.
I0302 19:00:35.207077 22760421793920 run.py:483] Algo bellman_ford step 6108 current loss 0.074040, current_train_items 195488.
I0302 19:00:35.238719 22760421793920 run.py:483] Algo bellman_ford step 6109 current loss 0.101742, current_train_items 195520.
I0302 19:00:35.256765 22760421793920 run.py:483] Algo bellman_ford step 6110 current loss 0.007005, current_train_items 195552.
I0302 19:00:35.272337 22760421793920 run.py:483] Algo bellman_ford step 6111 current loss 0.019291, current_train_items 195584.
I0302 19:00:35.295008 22760421793920 run.py:483] Algo bellman_ford step 6112 current loss 0.066002, current_train_items 195616.
I0302 19:00:35.324387 22760421793920 run.py:483] Algo bellman_ford step 6113 current loss 0.050432, current_train_items 195648.
I0302 19:00:35.356141 22760421793920 run.py:483] Algo bellman_ford step 6114 current loss 0.049438, current_train_items 195680.
I0302 19:00:35.374584 22760421793920 run.py:483] Algo bellman_ford step 6115 current loss 0.011221, current_train_items 195712.
I0302 19:00:35.390184 22760421793920 run.py:483] Algo bellman_ford step 6116 current loss 0.032471, current_train_items 195744.
I0302 19:00:35.413131 22760421793920 run.py:483] Algo bellman_ford step 6117 current loss 0.030212, current_train_items 195776.
I0302 19:00:35.443922 22760421793920 run.py:483] Algo bellman_ford step 6118 current loss 0.040078, current_train_items 195808.
I0302 19:00:35.475037 22760421793920 run.py:483] Algo bellman_ford step 6119 current loss 0.048261, current_train_items 195840.
I0302 19:00:35.493150 22760421793920 run.py:483] Algo bellman_ford step 6120 current loss 0.014337, current_train_items 195872.
I0302 19:00:35.508966 22760421793920 run.py:483] Algo bellman_ford step 6121 current loss 0.009046, current_train_items 195904.
I0302 19:00:35.531279 22760421793920 run.py:483] Algo bellman_ford step 6122 current loss 0.014889, current_train_items 195936.
I0302 19:00:35.562127 22760421793920 run.py:483] Algo bellman_ford step 6123 current loss 0.065683, current_train_items 195968.
I0302 19:00:35.593894 22760421793920 run.py:483] Algo bellman_ford step 6124 current loss 0.069775, current_train_items 196000.
I0302 19:00:35.611946 22760421793920 run.py:483] Algo bellman_ford step 6125 current loss 0.003666, current_train_items 196032.
I0302 19:00:35.627161 22760421793920 run.py:483] Algo bellman_ford step 6126 current loss 0.038760, current_train_items 196064.
I0302 19:00:35.651543 22760421793920 run.py:483] Algo bellman_ford step 6127 current loss 0.062239, current_train_items 196096.
I0302 19:00:35.681489 22760421793920 run.py:483] Algo bellman_ford step 6128 current loss 0.030748, current_train_items 196128.
I0302 19:00:35.712616 22760421793920 run.py:483] Algo bellman_ford step 6129 current loss 0.085019, current_train_items 196160.
I0302 19:00:35.730633 22760421793920 run.py:483] Algo bellman_ford step 6130 current loss 0.006789, current_train_items 196192.
I0302 19:00:35.746240 22760421793920 run.py:483] Algo bellman_ford step 6131 current loss 0.011666, current_train_items 196224.
I0302 19:00:35.769773 22760421793920 run.py:483] Algo bellman_ford step 6132 current loss 0.047211, current_train_items 196256.
I0302 19:00:35.798787 22760421793920 run.py:483] Algo bellman_ford step 6133 current loss 0.041065, current_train_items 196288.
I0302 19:00:35.829494 22760421793920 run.py:483] Algo bellman_ford step 6134 current loss 0.051730, current_train_items 196320.
I0302 19:00:35.847446 22760421793920 run.py:483] Algo bellman_ford step 6135 current loss 0.007353, current_train_items 196352.
I0302 19:00:35.862619 22760421793920 run.py:483] Algo bellman_ford step 6136 current loss 0.043944, current_train_items 196384.
I0302 19:00:35.885688 22760421793920 run.py:483] Algo bellman_ford step 6137 current loss 0.028364, current_train_items 196416.
I0302 19:00:35.916339 22760421793920 run.py:483] Algo bellman_ford step 6138 current loss 0.067817, current_train_items 196448.
I0302 19:00:35.948489 22760421793920 run.py:483] Algo bellman_ford step 6139 current loss 0.064053, current_train_items 196480.
I0302 19:00:35.966712 22760421793920 run.py:483] Algo bellman_ford step 6140 current loss 0.006311, current_train_items 196512.
I0302 19:00:35.982427 22760421793920 run.py:483] Algo bellman_ford step 6141 current loss 0.013619, current_train_items 196544.
I0302 19:00:36.005827 22760421793920 run.py:483] Algo bellman_ford step 6142 current loss 0.024457, current_train_items 196576.
I0302 19:00:36.035953 22760421793920 run.py:483] Algo bellman_ford step 6143 current loss 0.034652, current_train_items 196608.
I0302 19:00:36.067365 22760421793920 run.py:483] Algo bellman_ford step 6144 current loss 0.049970, current_train_items 196640.
I0302 19:00:36.085180 22760421793920 run.py:483] Algo bellman_ford step 6145 current loss 0.009742, current_train_items 196672.
I0302 19:00:36.100863 22760421793920 run.py:483] Algo bellman_ford step 6146 current loss 0.012703, current_train_items 196704.
I0302 19:00:36.123262 22760421793920 run.py:483] Algo bellman_ford step 6147 current loss 0.021809, current_train_items 196736.
I0302 19:00:36.152777 22760421793920 run.py:483] Algo bellman_ford step 6148 current loss 0.048406, current_train_items 196768.
I0302 19:00:36.185713 22760421793920 run.py:483] Algo bellman_ford step 6149 current loss 0.102920, current_train_items 196800.
I0302 19:00:36.204363 22760421793920 run.py:483] Algo bellman_ford step 6150 current loss 0.001657, current_train_items 196832.
I0302 19:00:36.211894 22760421793920 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:00:36.212011 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:00:36.228331 22760421793920 run.py:483] Algo bellman_ford step 6151 current loss 0.011789, current_train_items 196864.
I0302 19:00:36.250832 22760421793920 run.py:483] Algo bellman_ford step 6152 current loss 0.036343, current_train_items 196896.
I0302 19:00:36.279798 22760421793920 run.py:483] Algo bellman_ford step 6153 current loss 0.101807, current_train_items 196928.
I0302 19:00:36.311360 22760421793920 run.py:483] Algo bellman_ford step 6154 current loss 0.063955, current_train_items 196960.
I0302 19:00:36.329598 22760421793920 run.py:483] Algo bellman_ford step 6155 current loss 0.006813, current_train_items 196992.
I0302 19:00:36.345751 22760421793920 run.py:483] Algo bellman_ford step 6156 current loss 0.047553, current_train_items 197024.
I0302 19:00:36.368205 22760421793920 run.py:483] Algo bellman_ford step 6157 current loss 0.048680, current_train_items 197056.
I0302 19:00:36.397193 22760421793920 run.py:483] Algo bellman_ford step 6158 current loss 0.119310, current_train_items 197088.
I0302 19:00:36.428961 22760421793920 run.py:483] Algo bellman_ford step 6159 current loss 0.105088, current_train_items 197120.
I0302 19:00:36.447047 22760421793920 run.py:483] Algo bellman_ford step 6160 current loss 0.085248, current_train_items 197152.
I0302 19:00:36.462961 22760421793920 run.py:483] Algo bellman_ford step 6161 current loss 0.024373, current_train_items 197184.
I0302 19:00:36.485665 22760421793920 run.py:483] Algo bellman_ford step 6162 current loss 0.073762, current_train_items 197216.
I0302 19:00:36.517061 22760421793920 run.py:483] Algo bellman_ford step 6163 current loss 0.053122, current_train_items 197248.
I0302 19:00:36.549589 22760421793920 run.py:483] Algo bellman_ford step 6164 current loss 0.090992, current_train_items 197280.
I0302 19:00:36.567941 22760421793920 run.py:483] Algo bellman_ford step 6165 current loss 0.032783, current_train_items 197312.
I0302 19:00:36.583853 22760421793920 run.py:483] Algo bellman_ford step 6166 current loss 0.024740, current_train_items 197344.
I0302 19:00:36.607046 22760421793920 run.py:483] Algo bellman_ford step 6167 current loss 0.049561, current_train_items 197376.
I0302 19:00:36.637783 22760421793920 run.py:483] Algo bellman_ford step 6168 current loss 0.049281, current_train_items 197408.
I0302 19:00:36.670311 22760421793920 run.py:483] Algo bellman_ford step 6169 current loss 0.058206, current_train_items 197440.
I0302 19:00:36.688707 22760421793920 run.py:483] Algo bellman_ford step 6170 current loss 0.010188, current_train_items 197472.
I0302 19:00:36.704200 22760421793920 run.py:483] Algo bellman_ford step 6171 current loss 0.035052, current_train_items 197504.
I0302 19:00:36.727457 22760421793920 run.py:483] Algo bellman_ford step 6172 current loss 0.053258, current_train_items 197536.
I0302 19:00:36.758708 22760421793920 run.py:483] Algo bellman_ford step 6173 current loss 0.095699, current_train_items 197568.
I0302 19:00:36.790590 22760421793920 run.py:483] Algo bellman_ford step 6174 current loss 0.137320, current_train_items 197600.
I0302 19:00:36.808696 22760421793920 run.py:483] Algo bellman_ford step 6175 current loss 0.005965, current_train_items 197632.
I0302 19:00:36.824621 22760421793920 run.py:483] Algo bellman_ford step 6176 current loss 0.034413, current_train_items 197664.
I0302 19:00:36.847003 22760421793920 run.py:483] Algo bellman_ford step 6177 current loss 0.042339, current_train_items 197696.
I0302 19:00:36.877032 22760421793920 run.py:483] Algo bellman_ford step 6178 current loss 0.038757, current_train_items 197728.
I0302 19:00:36.909407 22760421793920 run.py:483] Algo bellman_ford step 6179 current loss 0.095499, current_train_items 197760.
I0302 19:00:36.927687 22760421793920 run.py:483] Algo bellman_ford step 6180 current loss 0.003954, current_train_items 197792.
I0302 19:00:36.943530 22760421793920 run.py:483] Algo bellman_ford step 6181 current loss 0.014254, current_train_items 197824.
I0302 19:00:36.966180 22760421793920 run.py:483] Algo bellman_ford step 6182 current loss 0.029922, current_train_items 197856.
I0302 19:00:36.996100 22760421793920 run.py:483] Algo bellman_ford step 6183 current loss 0.069247, current_train_items 197888.
I0302 19:00:37.030216 22760421793920 run.py:483] Algo bellman_ford step 6184 current loss 0.051629, current_train_items 197920.
I0302 19:00:37.048334 22760421793920 run.py:483] Algo bellman_ford step 6185 current loss 0.010746, current_train_items 197952.
I0302 19:00:37.064053 22760421793920 run.py:483] Algo bellman_ford step 6186 current loss 0.023435, current_train_items 197984.
I0302 19:00:37.086234 22760421793920 run.py:483] Algo bellman_ford step 6187 current loss 0.036634, current_train_items 198016.
I0302 19:00:37.115720 22760421793920 run.py:483] Algo bellman_ford step 6188 current loss 0.021267, current_train_items 198048.
I0302 19:00:37.148205 22760421793920 run.py:483] Algo bellman_ford step 6189 current loss 0.046481, current_train_items 198080.
I0302 19:00:37.166535 22760421793920 run.py:483] Algo bellman_ford step 6190 current loss 0.008840, current_train_items 198112.
I0302 19:00:37.182347 22760421793920 run.py:483] Algo bellman_ford step 6191 current loss 0.054642, current_train_items 198144.
I0302 19:00:37.205620 22760421793920 run.py:483] Algo bellman_ford step 6192 current loss 0.063355, current_train_items 198176.
I0302 19:00:37.236275 22760421793920 run.py:483] Algo bellman_ford step 6193 current loss 0.115124, current_train_items 198208.
I0302 19:00:37.267316 22760421793920 run.py:483] Algo bellman_ford step 6194 current loss 0.100551, current_train_items 198240.
I0302 19:00:37.285515 22760421793920 run.py:483] Algo bellman_ford step 6195 current loss 0.005122, current_train_items 198272.
I0302 19:00:37.300712 22760421793920 run.py:483] Algo bellman_ford step 6196 current loss 0.027858, current_train_items 198304.
I0302 19:00:37.324624 22760421793920 run.py:483] Algo bellman_ford step 6197 current loss 0.089797, current_train_items 198336.
I0302 19:00:37.353583 22760421793920 run.py:483] Algo bellman_ford step 6198 current loss 0.071536, current_train_items 198368.
I0302 19:00:37.386560 22760421793920 run.py:483] Algo bellman_ford step 6199 current loss 0.225535, current_train_items 198400.
I0302 19:00:37.405066 22760421793920 run.py:483] Algo bellman_ford step 6200 current loss 0.004524, current_train_items 198432.
I0302 19:00:37.412797 22760421793920 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:00:37.412918 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:37.429383 22760421793920 run.py:483] Algo bellman_ford step 6201 current loss 0.009965, current_train_items 198464.
I0302 19:00:37.453246 22760421793920 run.py:483] Algo bellman_ford step 6202 current loss 0.069145, current_train_items 198496.
I0302 19:00:37.482172 22760421793920 run.py:483] Algo bellman_ford step 6203 current loss 0.023314, current_train_items 198528.
I0302 19:00:37.513409 22760421793920 run.py:483] Algo bellman_ford step 6204 current loss 0.055600, current_train_items 198560.
I0302 19:00:37.531816 22760421793920 run.py:483] Algo bellman_ford step 6205 current loss 0.002941, current_train_items 198592.
I0302 19:00:37.546655 22760421793920 run.py:483] Algo bellman_ford step 6206 current loss 0.021777, current_train_items 198624.
I0302 19:00:37.569509 22760421793920 run.py:483] Algo bellman_ford step 6207 current loss 0.025776, current_train_items 198656.
I0302 19:00:37.599305 22760421793920 run.py:483] Algo bellman_ford step 6208 current loss 0.068521, current_train_items 198688.
I0302 19:00:37.632232 22760421793920 run.py:483] Algo bellman_ford step 6209 current loss 0.076223, current_train_items 198720.
I0302 19:00:37.650165 22760421793920 run.py:483] Algo bellman_ford step 6210 current loss 0.004912, current_train_items 198752.
I0302 19:00:37.665861 22760421793920 run.py:483] Algo bellman_ford step 6211 current loss 0.025214, current_train_items 198784.
I0302 19:00:37.688054 22760421793920 run.py:483] Algo bellman_ford step 6212 current loss 0.076007, current_train_items 198816.
I0302 19:00:37.716381 22760421793920 run.py:483] Algo bellman_ford step 6213 current loss 0.056070, current_train_items 198848.
I0302 19:00:37.749575 22760421793920 run.py:483] Algo bellman_ford step 6214 current loss 0.076896, current_train_items 198880.
I0302 19:00:37.767695 22760421793920 run.py:483] Algo bellman_ford step 6215 current loss 0.005669, current_train_items 198912.
I0302 19:00:37.783376 22760421793920 run.py:483] Algo bellman_ford step 6216 current loss 0.049983, current_train_items 198944.
I0302 19:00:37.805526 22760421793920 run.py:483] Algo bellman_ford step 6217 current loss 0.037348, current_train_items 198976.
I0302 19:00:37.835204 22760421793920 run.py:483] Algo bellman_ford step 6218 current loss 0.091191, current_train_items 199008.
I0302 19:00:37.865964 22760421793920 run.py:483] Algo bellman_ford step 6219 current loss 0.080944, current_train_items 199040.
I0302 19:00:37.884263 22760421793920 run.py:483] Algo bellman_ford step 6220 current loss 0.029336, current_train_items 199072.
I0302 19:00:37.899397 22760421793920 run.py:483] Algo bellman_ford step 6221 current loss 0.032664, current_train_items 199104.
I0302 19:00:37.922955 22760421793920 run.py:483] Algo bellman_ford step 6222 current loss 0.051304, current_train_items 199136.
I0302 19:00:37.951411 22760421793920 run.py:483] Algo bellman_ford step 6223 current loss 0.084593, current_train_items 199168.
I0302 19:00:37.984092 22760421793920 run.py:483] Algo bellman_ford step 6224 current loss 0.141336, current_train_items 199200.
I0302 19:00:38.002559 22760421793920 run.py:483] Algo bellman_ford step 6225 current loss 0.003957, current_train_items 199232.
I0302 19:00:38.018106 22760421793920 run.py:483] Algo bellman_ford step 6226 current loss 0.035622, current_train_items 199264.
I0302 19:00:38.039998 22760421793920 run.py:483] Algo bellman_ford step 6227 current loss 0.017959, current_train_items 199296.
I0302 19:00:38.068424 22760421793920 run.py:483] Algo bellman_ford step 6228 current loss 0.054161, current_train_items 199328.
I0302 19:00:38.098329 22760421793920 run.py:483] Algo bellman_ford step 6229 current loss 0.078135, current_train_items 199360.
I0302 19:00:38.116552 22760421793920 run.py:483] Algo bellman_ford step 6230 current loss 0.011041, current_train_items 199392.
I0302 19:00:38.132302 22760421793920 run.py:483] Algo bellman_ford step 6231 current loss 0.015054, current_train_items 199424.
I0302 19:00:38.155391 22760421793920 run.py:483] Algo bellman_ford step 6232 current loss 0.068596, current_train_items 199456.
I0302 19:00:38.183982 22760421793920 run.py:483] Algo bellman_ford step 6233 current loss 0.045599, current_train_items 199488.
I0302 19:00:38.215241 22760421793920 run.py:483] Algo bellman_ford step 6234 current loss 0.084223, current_train_items 199520.
I0302 19:00:38.233072 22760421793920 run.py:483] Algo bellman_ford step 6235 current loss 0.002743, current_train_items 199552.
I0302 19:00:38.248718 22760421793920 run.py:483] Algo bellman_ford step 6236 current loss 0.007295, current_train_items 199584.
I0302 19:00:38.270644 22760421793920 run.py:483] Algo bellman_ford step 6237 current loss 0.038313, current_train_items 199616.
I0302 19:00:38.300992 22760421793920 run.py:483] Algo bellman_ford step 6238 current loss 0.039483, current_train_items 199648.
I0302 19:00:38.330312 22760421793920 run.py:483] Algo bellman_ford step 6239 current loss 0.078278, current_train_items 199680.
I0302 19:00:38.348254 22760421793920 run.py:483] Algo bellman_ford step 6240 current loss 0.008103, current_train_items 199712.
I0302 19:00:38.363911 22760421793920 run.py:483] Algo bellman_ford step 6241 current loss 0.059112, current_train_items 199744.
I0302 19:00:38.387185 22760421793920 run.py:483] Algo bellman_ford step 6242 current loss 0.057680, current_train_items 199776.
I0302 19:00:38.417436 22760421793920 run.py:483] Algo bellman_ford step 6243 current loss 0.096809, current_train_items 199808.
I0302 19:00:38.447960 22760421793920 run.py:483] Algo bellman_ford step 6244 current loss 0.056281, current_train_items 199840.
I0302 19:00:38.466276 22760421793920 run.py:483] Algo bellman_ford step 6245 current loss 0.005051, current_train_items 199872.
I0302 19:00:38.481948 22760421793920 run.py:483] Algo bellman_ford step 6246 current loss 0.051512, current_train_items 199904.
I0302 19:00:38.504198 22760421793920 run.py:483] Algo bellman_ford step 6247 current loss 0.026854, current_train_items 199936.
I0302 19:00:38.534636 22760421793920 run.py:483] Algo bellman_ford step 6248 current loss 0.047997, current_train_items 199968.
I0302 19:00:38.564845 22760421793920 run.py:483] Algo bellman_ford step 6249 current loss 0.067967, current_train_items 200000.
I0302 19:00:38.582709 22760421793920 run.py:483] Algo bellman_ford step 6250 current loss 0.002111, current_train_items 200032.
I0302 19:00:38.590259 22760421793920 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:00:38.590368 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:00:38.606888 22760421793920 run.py:483] Algo bellman_ford step 6251 current loss 0.023322, current_train_items 200064.
I0302 19:00:38.629760 22760421793920 run.py:483] Algo bellman_ford step 6252 current loss 0.039352, current_train_items 200096.
I0302 19:00:38.660745 22760421793920 run.py:483] Algo bellman_ford step 6253 current loss 0.077388, current_train_items 200128.
I0302 19:00:38.691625 22760421793920 run.py:483] Algo bellman_ford step 6254 current loss 0.068578, current_train_items 200160.
I0302 19:00:38.709855 22760421793920 run.py:483] Algo bellman_ford step 6255 current loss 0.022927, current_train_items 200192.
I0302 19:00:38.725781 22760421793920 run.py:483] Algo bellman_ford step 6256 current loss 0.027108, current_train_items 200224.
I0302 19:00:38.749019 22760421793920 run.py:483] Algo bellman_ford step 6257 current loss 0.034321, current_train_items 200256.
I0302 19:00:38.778028 22760421793920 run.py:483] Algo bellman_ford step 6258 current loss 0.052247, current_train_items 200288.
I0302 19:00:38.808733 22760421793920 run.py:483] Algo bellman_ford step 6259 current loss 0.134617, current_train_items 200320.
I0302 19:00:38.826824 22760421793920 run.py:483] Algo bellman_ford step 6260 current loss 0.005257, current_train_items 200352.
I0302 19:00:38.842748 22760421793920 run.py:483] Algo bellman_ford step 6261 current loss 0.006718, current_train_items 200384.
I0302 19:00:38.865532 22760421793920 run.py:483] Algo bellman_ford step 6262 current loss 0.024316, current_train_items 200416.
I0302 19:00:38.895317 22760421793920 run.py:483] Algo bellman_ford step 6263 current loss 0.079712, current_train_items 200448.
I0302 19:00:38.927160 22760421793920 run.py:483] Algo bellman_ford step 6264 current loss 0.050925, current_train_items 200480.
I0302 19:00:38.945377 22760421793920 run.py:483] Algo bellman_ford step 6265 current loss 0.005014, current_train_items 200512.
I0302 19:00:38.961807 22760421793920 run.py:483] Algo bellman_ford step 6266 current loss 0.039444, current_train_items 200544.
I0302 19:00:38.985559 22760421793920 run.py:483] Algo bellman_ford step 6267 current loss 0.045065, current_train_items 200576.
I0302 19:00:39.015032 22760421793920 run.py:483] Algo bellman_ford step 6268 current loss 0.034659, current_train_items 200608.
I0302 19:00:39.044345 22760421793920 run.py:483] Algo bellman_ford step 6269 current loss 0.045571, current_train_items 200640.
I0302 19:00:39.062307 22760421793920 run.py:483] Algo bellman_ford step 6270 current loss 0.002141, current_train_items 200672.
I0302 19:00:39.078117 22760421793920 run.py:483] Algo bellman_ford step 6271 current loss 0.037394, current_train_items 200704.
I0302 19:00:39.100872 22760421793920 run.py:483] Algo bellman_ford step 6272 current loss 0.050780, current_train_items 200736.
I0302 19:00:39.129701 22760421793920 run.py:483] Algo bellman_ford step 6273 current loss 0.058004, current_train_items 200768.
I0302 19:00:39.163057 22760421793920 run.py:483] Algo bellman_ford step 6274 current loss 0.114283, current_train_items 200800.
I0302 19:00:39.181219 22760421793920 run.py:483] Algo bellman_ford step 6275 current loss 0.007352, current_train_items 200832.
I0302 19:00:39.197225 22760421793920 run.py:483] Algo bellman_ford step 6276 current loss 0.033864, current_train_items 200864.
I0302 19:00:39.220438 22760421793920 run.py:483] Algo bellman_ford step 6277 current loss 0.085401, current_train_items 200896.
I0302 19:00:39.249282 22760421793920 run.py:483] Algo bellman_ford step 6278 current loss 0.103150, current_train_items 200928.
I0302 19:00:39.282179 22760421793920 run.py:483] Algo bellman_ford step 6279 current loss 0.096003, current_train_items 200960.
I0302 19:00:39.300367 22760421793920 run.py:483] Algo bellman_ford step 6280 current loss 0.002737, current_train_items 200992.
I0302 19:00:39.315888 22760421793920 run.py:483] Algo bellman_ford step 6281 current loss 0.011402, current_train_items 201024.
I0302 19:00:39.338400 22760421793920 run.py:483] Algo bellman_ford step 6282 current loss 0.064811, current_train_items 201056.
I0302 19:00:39.368560 22760421793920 run.py:483] Algo bellman_ford step 6283 current loss 0.066391, current_train_items 201088.
I0302 19:00:39.401215 22760421793920 run.py:483] Algo bellman_ford step 6284 current loss 0.073670, current_train_items 201120.
I0302 19:00:39.419219 22760421793920 run.py:483] Algo bellman_ford step 6285 current loss 0.014855, current_train_items 201152.
I0302 19:00:39.435085 22760421793920 run.py:483] Algo bellman_ford step 6286 current loss 0.014672, current_train_items 201184.
I0302 19:00:39.458048 22760421793920 run.py:483] Algo bellman_ford step 6287 current loss 0.038056, current_train_items 201216.
I0302 19:00:39.488210 22760421793920 run.py:483] Algo bellman_ford step 6288 current loss 0.036514, current_train_items 201248.
I0302 19:00:39.520408 22760421793920 run.py:483] Algo bellman_ford step 6289 current loss 0.071119, current_train_items 201280.
I0302 19:00:39.538425 22760421793920 run.py:483] Algo bellman_ford step 6290 current loss 0.003645, current_train_items 201312.
I0302 19:00:39.554307 22760421793920 run.py:483] Algo bellman_ford step 6291 current loss 0.027108, current_train_items 201344.
I0302 19:00:39.576976 22760421793920 run.py:483] Algo bellman_ford step 6292 current loss 0.041733, current_train_items 201376.
I0302 19:00:39.607444 22760421793920 run.py:483] Algo bellman_ford step 6293 current loss 0.096555, current_train_items 201408.
I0302 19:00:39.640774 22760421793920 run.py:483] Algo bellman_ford step 6294 current loss 0.056368, current_train_items 201440.
I0302 19:00:39.658855 22760421793920 run.py:483] Algo bellman_ford step 6295 current loss 0.003293, current_train_items 201472.
I0302 19:00:39.675169 22760421793920 run.py:483] Algo bellman_ford step 6296 current loss 0.027977, current_train_items 201504.
I0302 19:00:39.697594 22760421793920 run.py:483] Algo bellman_ford step 6297 current loss 0.020198, current_train_items 201536.
I0302 19:00:39.727385 22760421793920 run.py:483] Algo bellman_ford step 6298 current loss 0.066908, current_train_items 201568.
I0302 19:00:39.758273 22760421793920 run.py:483] Algo bellman_ford step 6299 current loss 0.053449, current_train_items 201600.
I0302 19:00:39.776258 22760421793920 run.py:483] Algo bellman_ford step 6300 current loss 0.001134, current_train_items 201632.
I0302 19:00:39.783828 22760421793920 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:00:39.783946 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:39.800506 22760421793920 run.py:483] Algo bellman_ford step 6301 current loss 0.024315, current_train_items 201664.
I0302 19:00:39.823210 22760421793920 run.py:483] Algo bellman_ford step 6302 current loss 0.056089, current_train_items 201696.
I0302 19:00:39.852782 22760421793920 run.py:483] Algo bellman_ford step 6303 current loss 0.056886, current_train_items 201728.
I0302 19:00:39.882982 22760421793920 run.py:483] Algo bellman_ford step 6304 current loss 0.055993, current_train_items 201760.
I0302 19:00:39.901465 22760421793920 run.py:483] Algo bellman_ford step 6305 current loss 0.011114, current_train_items 201792.
I0302 19:00:39.916913 22760421793920 run.py:483] Algo bellman_ford step 6306 current loss 0.007925, current_train_items 201824.
I0302 19:00:39.940123 22760421793920 run.py:483] Algo bellman_ford step 6307 current loss 0.063386, current_train_items 201856.
I0302 19:00:39.970619 22760421793920 run.py:483] Algo bellman_ford step 6308 current loss 0.039790, current_train_items 201888.
I0302 19:00:40.001914 22760421793920 run.py:483] Algo bellman_ford step 6309 current loss 0.091011, current_train_items 201920.
I0302 19:00:40.020498 22760421793920 run.py:483] Algo bellman_ford step 6310 current loss 0.003271, current_train_items 201952.
I0302 19:00:40.036027 22760421793920 run.py:483] Algo bellman_ford step 6311 current loss 0.011151, current_train_items 201984.
I0302 19:00:40.057938 22760421793920 run.py:483] Algo bellman_ford step 6312 current loss 0.063963, current_train_items 202016.
I0302 19:00:40.088734 22760421793920 run.py:483] Algo bellman_ford step 6313 current loss 0.124138, current_train_items 202048.
I0302 19:00:40.121316 22760421793920 run.py:483] Algo bellman_ford step 6314 current loss 0.142168, current_train_items 202080.
I0302 19:00:40.139902 22760421793920 run.py:483] Algo bellman_ford step 6315 current loss 0.004167, current_train_items 202112.
I0302 19:00:40.155368 22760421793920 run.py:483] Algo bellman_ford step 6316 current loss 0.007629, current_train_items 202144.
I0302 19:00:40.178147 22760421793920 run.py:483] Algo bellman_ford step 6317 current loss 0.046112, current_train_items 202176.
I0302 19:00:40.207236 22760421793920 run.py:483] Algo bellman_ford step 6318 current loss 0.031675, current_train_items 202208.
I0302 19:00:40.238395 22760421793920 run.py:483] Algo bellman_ford step 6319 current loss 0.097632, current_train_items 202240.
I0302 19:00:40.256939 22760421793920 run.py:483] Algo bellman_ford step 6320 current loss 0.013137, current_train_items 202272.
I0302 19:00:40.272923 22760421793920 run.py:483] Algo bellman_ford step 6321 current loss 0.012214, current_train_items 202304.
I0302 19:00:40.296771 22760421793920 run.py:483] Algo bellman_ford step 6322 current loss 0.039803, current_train_items 202336.
I0302 19:00:40.327451 22760421793920 run.py:483] Algo bellman_ford step 6323 current loss 0.081751, current_train_items 202368.
I0302 19:00:40.359744 22760421793920 run.py:483] Algo bellman_ford step 6324 current loss 0.075927, current_train_items 202400.
I0302 19:00:40.378290 22760421793920 run.py:483] Algo bellman_ford step 6325 current loss 0.002918, current_train_items 202432.
I0302 19:00:40.393786 22760421793920 run.py:483] Algo bellman_ford step 6326 current loss 0.017564, current_train_items 202464.
I0302 19:00:40.417862 22760421793920 run.py:483] Algo bellman_ford step 6327 current loss 0.057841, current_train_items 202496.
I0302 19:00:40.447476 22760421793920 run.py:483] Algo bellman_ford step 6328 current loss 0.080755, current_train_items 202528.
I0302 19:00:40.479983 22760421793920 run.py:483] Algo bellman_ford step 6329 current loss 0.063480, current_train_items 202560.
I0302 19:00:40.498364 22760421793920 run.py:483] Algo bellman_ford step 6330 current loss 0.003116, current_train_items 202592.
I0302 19:00:40.514108 22760421793920 run.py:483] Algo bellman_ford step 6331 current loss 0.022392, current_train_items 202624.
I0302 19:00:40.536554 22760421793920 run.py:483] Algo bellman_ford step 6332 current loss 0.079378, current_train_items 202656.
I0302 19:00:40.566556 22760421793920 run.py:483] Algo bellman_ford step 6333 current loss 0.166244, current_train_items 202688.
I0302 19:00:40.597634 22760421793920 run.py:483] Algo bellman_ford step 6334 current loss 0.072008, current_train_items 202720.
I0302 19:00:40.616013 22760421793920 run.py:483] Algo bellman_ford step 6335 current loss 0.001704, current_train_items 202752.
I0302 19:00:40.631922 22760421793920 run.py:483] Algo bellman_ford step 6336 current loss 0.013019, current_train_items 202784.
I0302 19:00:40.655653 22760421793920 run.py:483] Algo bellman_ford step 6337 current loss 0.036796, current_train_items 202816.
I0302 19:00:40.686695 22760421793920 run.py:483] Algo bellman_ford step 6338 current loss 0.060229, current_train_items 202848.
I0302 19:00:40.718185 22760421793920 run.py:483] Algo bellman_ford step 6339 current loss 0.061504, current_train_items 202880.
I0302 19:00:40.736146 22760421793920 run.py:483] Algo bellman_ford step 6340 current loss 0.004111, current_train_items 202912.
I0302 19:00:40.751445 22760421793920 run.py:483] Algo bellman_ford step 6341 current loss 0.013211, current_train_items 202944.
I0302 19:00:40.775250 22760421793920 run.py:483] Algo bellman_ford step 6342 current loss 0.036747, current_train_items 202976.
I0302 19:00:40.804644 22760421793920 run.py:483] Algo bellman_ford step 6343 current loss 0.019234, current_train_items 203008.
I0302 19:00:40.834172 22760421793920 run.py:483] Algo bellman_ford step 6344 current loss 0.059189, current_train_items 203040.
I0302 19:00:40.852338 22760421793920 run.py:483] Algo bellman_ford step 6345 current loss 0.001615, current_train_items 203072.
I0302 19:00:40.868458 22760421793920 run.py:483] Algo bellman_ford step 6346 current loss 0.040544, current_train_items 203104.
I0302 19:00:40.891516 22760421793920 run.py:483] Algo bellman_ford step 6347 current loss 0.040145, current_train_items 203136.
I0302 19:00:40.920751 22760421793920 run.py:483] Algo bellman_ford step 6348 current loss 0.043831, current_train_items 203168.
I0302 19:00:40.954786 22760421793920 run.py:483] Algo bellman_ford step 6349 current loss 0.042866, current_train_items 203200.
I0302 19:00:40.973356 22760421793920 run.py:483] Algo bellman_ford step 6350 current loss 0.041092, current_train_items 203232.
I0302 19:00:40.981013 22760421793920 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:00:40.981124 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:00:40.997911 22760421793920 run.py:483] Algo bellman_ford step 6351 current loss 0.012749, current_train_items 203264.
I0302 19:00:41.020308 22760421793920 run.py:483] Algo bellman_ford step 6352 current loss 0.026336, current_train_items 203296.
I0302 19:00:41.050467 22760421793920 run.py:483] Algo bellman_ford step 6353 current loss 0.060759, current_train_items 203328.
I0302 19:00:41.082500 22760421793920 run.py:483] Algo bellman_ford step 6354 current loss 0.070875, current_train_items 203360.
I0302 19:00:41.100784 22760421793920 run.py:483] Algo bellman_ford step 6355 current loss 0.005372, current_train_items 203392.
I0302 19:00:41.116949 22760421793920 run.py:483] Algo bellman_ford step 6356 current loss 0.010208, current_train_items 203424.
I0302 19:00:41.139983 22760421793920 run.py:483] Algo bellman_ford step 6357 current loss 0.046278, current_train_items 203456.
I0302 19:00:41.169406 22760421793920 run.py:483] Algo bellman_ford step 6358 current loss 0.082037, current_train_items 203488.
I0302 19:00:41.201810 22760421793920 run.py:483] Algo bellman_ford step 6359 current loss 0.087459, current_train_items 203520.
I0302 19:00:41.219846 22760421793920 run.py:483] Algo bellman_ford step 6360 current loss 0.003124, current_train_items 203552.
I0302 19:00:41.235692 22760421793920 run.py:483] Algo bellman_ford step 6361 current loss 0.013451, current_train_items 203584.
I0302 19:00:41.257829 22760421793920 run.py:483] Algo bellman_ford step 6362 current loss 0.032673, current_train_items 203616.
I0302 19:00:41.287697 22760421793920 run.py:483] Algo bellman_ford step 6363 current loss 0.050653, current_train_items 203648.
I0302 19:00:41.318582 22760421793920 run.py:483] Algo bellman_ford step 6364 current loss 0.072540, current_train_items 203680.
I0302 19:00:41.336587 22760421793920 run.py:483] Algo bellman_ford step 6365 current loss 0.002134, current_train_items 203712.
I0302 19:00:41.352433 22760421793920 run.py:483] Algo bellman_ford step 6366 current loss 0.013167, current_train_items 203744.
I0302 19:00:41.374826 22760421793920 run.py:483] Algo bellman_ford step 6367 current loss 0.050184, current_train_items 203776.
I0302 19:00:41.406098 22760421793920 run.py:483] Algo bellman_ford step 6368 current loss 0.075401, current_train_items 203808.
I0302 19:00:41.436229 22760421793920 run.py:483] Algo bellman_ford step 6369 current loss 0.044134, current_train_items 203840.
I0302 19:00:41.454321 22760421793920 run.py:483] Algo bellman_ford step 6370 current loss 0.003988, current_train_items 203872.
I0302 19:00:41.470037 22760421793920 run.py:483] Algo bellman_ford step 6371 current loss 0.009857, current_train_items 203904.
I0302 19:00:41.493346 22760421793920 run.py:483] Algo bellman_ford step 6372 current loss 0.210350, current_train_items 203936.
I0302 19:00:41.522637 22760421793920 run.py:483] Algo bellman_ford step 6373 current loss 0.183522, current_train_items 203968.
I0302 19:00:41.554339 22760421793920 run.py:483] Algo bellman_ford step 6374 current loss 0.065244, current_train_items 204000.
I0302 19:00:41.572372 22760421793920 run.py:483] Algo bellman_ford step 6375 current loss 0.006101, current_train_items 204032.
I0302 19:00:41.587724 22760421793920 run.py:483] Algo bellman_ford step 6376 current loss 0.011604, current_train_items 204064.
I0302 19:00:41.611272 22760421793920 run.py:483] Algo bellman_ford step 6377 current loss 0.047159, current_train_items 204096.
I0302 19:00:41.640634 22760421793920 run.py:483] Algo bellman_ford step 6378 current loss 0.179282, current_train_items 204128.
I0302 19:00:41.674266 22760421793920 run.py:483] Algo bellman_ford step 6379 current loss 0.165218, current_train_items 204160.
I0302 19:00:41.692227 22760421793920 run.py:483] Algo bellman_ford step 6380 current loss 0.013034, current_train_items 204192.
I0302 19:00:41.707793 22760421793920 run.py:483] Algo bellman_ford step 6381 current loss 0.093307, current_train_items 204224.
I0302 19:00:41.730546 22760421793920 run.py:483] Algo bellman_ford step 6382 current loss 0.032775, current_train_items 204256.
I0302 19:00:41.759410 22760421793920 run.py:483] Algo bellman_ford step 6383 current loss 0.038860, current_train_items 204288.
I0302 19:00:41.791079 22760421793920 run.py:483] Algo bellman_ford step 6384 current loss 0.093183, current_train_items 204320.
I0302 19:00:41.809240 22760421793920 run.py:483] Algo bellman_ford step 6385 current loss 0.003822, current_train_items 204352.
I0302 19:00:41.824409 22760421793920 run.py:483] Algo bellman_ford step 6386 current loss 0.021911, current_train_items 204384.
I0302 19:00:41.847715 22760421793920 run.py:483] Algo bellman_ford step 6387 current loss 0.075324, current_train_items 204416.
I0302 19:00:41.876672 22760421793920 run.py:483] Algo bellman_ford step 6388 current loss 0.041877, current_train_items 204448.
I0302 19:00:41.908040 22760421793920 run.py:483] Algo bellman_ford step 6389 current loss 0.042659, current_train_items 204480.
I0302 19:00:41.925961 22760421793920 run.py:483] Algo bellman_ford step 6390 current loss 0.003539, current_train_items 204512.
I0302 19:00:41.941833 22760421793920 run.py:483] Algo bellman_ford step 6391 current loss 0.013036, current_train_items 204544.
I0302 19:00:41.964385 22760421793920 run.py:483] Algo bellman_ford step 6392 current loss 0.042089, current_train_items 204576.
I0302 19:00:41.992709 22760421793920 run.py:483] Algo bellman_ford step 6393 current loss 0.025355, current_train_items 204608.
I0302 19:00:42.024736 22760421793920 run.py:483] Algo bellman_ford step 6394 current loss 0.049135, current_train_items 204640.
I0302 19:00:42.043261 22760421793920 run.py:483] Algo bellman_ford step 6395 current loss 0.004567, current_train_items 204672.
I0302 19:00:42.058885 22760421793920 run.py:483] Algo bellman_ford step 6396 current loss 0.012598, current_train_items 204704.
I0302 19:00:42.081120 22760421793920 run.py:483] Algo bellman_ford step 6397 current loss 0.026122, current_train_items 204736.
I0302 19:00:42.110162 22760421793920 run.py:483] Algo bellman_ford step 6398 current loss 0.043918, current_train_items 204768.
I0302 19:00:42.139896 22760421793920 run.py:483] Algo bellman_ford step 6399 current loss 0.072700, current_train_items 204800.
I0302 19:00:42.158370 22760421793920 run.py:483] Algo bellman_ford step 6400 current loss 0.003684, current_train_items 204832.
I0302 19:00:42.165813 22760421793920 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:00:42.165933 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:42.181594 22760421793920 run.py:483] Algo bellman_ford step 6401 current loss 0.008332, current_train_items 204864.
I0302 19:00:42.204573 22760421793920 run.py:483] Algo bellman_ford step 6402 current loss 0.012965, current_train_items 204896.
I0302 19:00:42.234772 22760421793920 run.py:483] Algo bellman_ford step 6403 current loss 0.083458, current_train_items 204928.
I0302 19:00:42.267161 22760421793920 run.py:483] Algo bellman_ford step 6404 current loss 0.064124, current_train_items 204960.
I0302 19:00:42.285663 22760421793920 run.py:483] Algo bellman_ford step 6405 current loss 0.011012, current_train_items 204992.
I0302 19:00:42.301079 22760421793920 run.py:483] Algo bellman_ford step 6406 current loss 0.016383, current_train_items 205024.
I0302 19:00:42.323383 22760421793920 run.py:483] Algo bellman_ford step 6407 current loss 0.024798, current_train_items 205056.
I0302 19:00:42.353197 22760421793920 run.py:483] Algo bellman_ford step 6408 current loss 0.046039, current_train_items 205088.
I0302 19:00:42.386371 22760421793920 run.py:483] Algo bellman_ford step 6409 current loss 0.052678, current_train_items 205120.
I0302 19:00:42.404449 22760421793920 run.py:483] Algo bellman_ford step 6410 current loss 0.008218, current_train_items 205152.
I0302 19:00:42.419830 22760421793920 run.py:483] Algo bellman_ford step 6411 current loss 0.029752, current_train_items 205184.
I0302 19:00:42.441964 22760421793920 run.py:483] Algo bellman_ford step 6412 current loss 0.063258, current_train_items 205216.
I0302 19:00:42.472195 22760421793920 run.py:483] Algo bellman_ford step 6413 current loss 0.066130, current_train_items 205248.
I0302 19:00:42.504339 22760421793920 run.py:483] Algo bellman_ford step 6414 current loss 0.048774, current_train_items 205280.
I0302 19:00:42.522559 22760421793920 run.py:483] Algo bellman_ford step 6415 current loss 0.002043, current_train_items 205312.
I0302 19:00:42.538221 22760421793920 run.py:483] Algo bellman_ford step 6416 current loss 0.038567, current_train_items 205344.
I0302 19:00:42.561867 22760421793920 run.py:483] Algo bellman_ford step 6417 current loss 0.047640, current_train_items 205376.
I0302 19:00:42.593040 22760421793920 run.py:483] Algo bellman_ford step 6418 current loss 0.036047, current_train_items 205408.
I0302 19:00:42.625630 22760421793920 run.py:483] Algo bellman_ford step 6419 current loss 0.082019, current_train_items 205440.
I0302 19:00:42.643886 22760421793920 run.py:483] Algo bellman_ford step 6420 current loss 0.004530, current_train_items 205472.
I0302 19:00:42.659564 22760421793920 run.py:483] Algo bellman_ford step 6421 current loss 0.027585, current_train_items 205504.
I0302 19:00:42.682058 22760421793920 run.py:483] Algo bellman_ford step 6422 current loss 0.028499, current_train_items 205536.
I0302 19:00:42.711588 22760421793920 run.py:483] Algo bellman_ford step 6423 current loss 0.089422, current_train_items 205568.
I0302 19:00:42.743152 22760421793920 run.py:483] Algo bellman_ford step 6424 current loss 0.033266, current_train_items 205600.
I0302 19:00:42.761432 22760421793920 run.py:483] Algo bellman_ford step 6425 current loss 0.006642, current_train_items 205632.
I0302 19:00:42.777445 22760421793920 run.py:483] Algo bellman_ford step 6426 current loss 0.012134, current_train_items 205664.
I0302 19:00:42.800467 22760421793920 run.py:483] Algo bellman_ford step 6427 current loss 0.042256, current_train_items 205696.
I0302 19:00:42.830846 22760421793920 run.py:483] Algo bellman_ford step 6428 current loss 0.074019, current_train_items 205728.
I0302 19:00:42.864943 22760421793920 run.py:483] Algo bellman_ford step 6429 current loss 0.078869, current_train_items 205760.
I0302 19:00:42.883287 22760421793920 run.py:483] Algo bellman_ford step 6430 current loss 0.004240, current_train_items 205792.
I0302 19:00:42.898446 22760421793920 run.py:483] Algo bellman_ford step 6431 current loss 0.047118, current_train_items 205824.
I0302 19:00:42.921711 22760421793920 run.py:483] Algo bellman_ford step 6432 current loss 0.041325, current_train_items 205856.
I0302 19:00:42.951793 22760421793920 run.py:483] Algo bellman_ford step 6433 current loss 0.062614, current_train_items 205888.
I0302 19:00:42.983037 22760421793920 run.py:483] Algo bellman_ford step 6434 current loss 0.065301, current_train_items 205920.
I0302 19:00:43.001286 22760421793920 run.py:483] Algo bellman_ford step 6435 current loss 0.004686, current_train_items 205952.
I0302 19:00:43.016916 22760421793920 run.py:483] Algo bellman_ford step 6436 current loss 0.014654, current_train_items 205984.
I0302 19:00:43.039509 22760421793920 run.py:483] Algo bellman_ford step 6437 current loss 0.062978, current_train_items 206016.
I0302 19:00:43.070398 22760421793920 run.py:483] Algo bellman_ford step 6438 current loss 0.072409, current_train_items 206048.
I0302 19:00:43.102846 22760421793920 run.py:483] Algo bellman_ford step 6439 current loss 0.106117, current_train_items 206080.
I0302 19:00:43.121238 22760421793920 run.py:483] Algo bellman_ford step 6440 current loss 0.001624, current_train_items 206112.
I0302 19:00:43.137201 22760421793920 run.py:483] Algo bellman_ford step 6441 current loss 0.014645, current_train_items 206144.
I0302 19:00:43.159234 22760421793920 run.py:483] Algo bellman_ford step 6442 current loss 0.025197, current_train_items 206176.
I0302 19:00:43.187361 22760421793920 run.py:483] Algo bellman_ford step 6443 current loss 0.057887, current_train_items 206208.
I0302 19:00:43.219781 22760421793920 run.py:483] Algo bellman_ford step 6444 current loss 0.100079, current_train_items 206240.
I0302 19:00:43.238005 22760421793920 run.py:483] Algo bellman_ford step 6445 current loss 0.024383, current_train_items 206272.
I0302 19:00:43.253363 22760421793920 run.py:483] Algo bellman_ford step 6446 current loss 0.022084, current_train_items 206304.
I0302 19:00:43.277079 22760421793920 run.py:483] Algo bellman_ford step 6447 current loss 0.020307, current_train_items 206336.
I0302 19:00:43.307292 22760421793920 run.py:483] Algo bellman_ford step 6448 current loss 0.035341, current_train_items 206368.
I0302 19:00:43.339569 22760421793920 run.py:483] Algo bellman_ford step 6449 current loss 0.041761, current_train_items 206400.
I0302 19:00:43.358003 22760421793920 run.py:483] Algo bellman_ford step 6450 current loss 0.005418, current_train_items 206432.
I0302 19:00:43.365412 22760421793920 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:00:43.365520 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:43.381840 22760421793920 run.py:483] Algo bellman_ford step 6451 current loss 0.009850, current_train_items 206464.
I0302 19:00:43.406342 22760421793920 run.py:483] Algo bellman_ford step 6452 current loss 0.046675, current_train_items 206496.
I0302 19:00:43.435643 22760421793920 run.py:483] Algo bellman_ford step 6453 current loss 0.056483, current_train_items 206528.
I0302 19:00:43.466270 22760421793920 run.py:483] Algo bellman_ford step 6454 current loss 0.027122, current_train_items 206560.
I0302 19:00:43.484827 22760421793920 run.py:483] Algo bellman_ford step 6455 current loss 0.020599, current_train_items 206592.
I0302 19:00:43.500957 22760421793920 run.py:483] Algo bellman_ford step 6456 current loss 0.003830, current_train_items 206624.
I0302 19:00:43.522790 22760421793920 run.py:483] Algo bellman_ford step 6457 current loss 0.040000, current_train_items 206656.
I0302 19:00:43.552348 22760421793920 run.py:483] Algo bellman_ford step 6458 current loss 0.024400, current_train_items 206688.
I0302 19:00:43.582564 22760421793920 run.py:483] Algo bellman_ford step 6459 current loss 0.047883, current_train_items 206720.
I0302 19:00:43.600662 22760421793920 run.py:483] Algo bellman_ford step 6460 current loss 0.013854, current_train_items 206752.
I0302 19:00:43.616349 22760421793920 run.py:483] Algo bellman_ford step 6461 current loss 0.004002, current_train_items 206784.
I0302 19:00:43.638892 22760421793920 run.py:483] Algo bellman_ford step 6462 current loss 0.083348, current_train_items 206816.
I0302 19:00:43.668116 22760421793920 run.py:483] Algo bellman_ford step 6463 current loss 0.033980, current_train_items 206848.
I0302 19:00:43.702262 22760421793920 run.py:483] Algo bellman_ford step 6464 current loss 0.089172, current_train_items 206880.
I0302 19:00:43.720405 22760421793920 run.py:483] Algo bellman_ford step 6465 current loss 0.000957, current_train_items 206912.
I0302 19:00:43.736365 22760421793920 run.py:483] Algo bellman_ford step 6466 current loss 0.011968, current_train_items 206944.
I0302 19:00:43.760676 22760421793920 run.py:483] Algo bellman_ford step 6467 current loss 0.047549, current_train_items 206976.
I0302 19:00:43.791840 22760421793920 run.py:483] Algo bellman_ford step 6468 current loss 0.049730, current_train_items 207008.
I0302 19:00:43.824889 22760421793920 run.py:483] Algo bellman_ford step 6469 current loss 0.076349, current_train_items 207040.
I0302 19:00:43.843185 22760421793920 run.py:483] Algo bellman_ford step 6470 current loss 0.002985, current_train_items 207072.
I0302 19:00:43.859443 22760421793920 run.py:483] Algo bellman_ford step 6471 current loss 0.021055, current_train_items 207104.
I0302 19:00:43.881948 22760421793920 run.py:483] Algo bellman_ford step 6472 current loss 0.012048, current_train_items 207136.
I0302 19:00:43.912068 22760421793920 run.py:483] Algo bellman_ford step 6473 current loss 0.053910, current_train_items 207168.
I0302 19:00:43.942967 22760421793920 run.py:483] Algo bellman_ford step 6474 current loss 0.078976, current_train_items 207200.
I0302 19:00:43.961059 22760421793920 run.py:483] Algo bellman_ford step 6475 current loss 0.014110, current_train_items 207232.
I0302 19:00:43.976981 22760421793920 run.py:483] Algo bellman_ford step 6476 current loss 0.029548, current_train_items 207264.
I0302 19:00:43.999062 22760421793920 run.py:483] Algo bellman_ford step 6477 current loss 0.058820, current_train_items 207296.
I0302 19:00:44.029400 22760421793920 run.py:483] Algo bellman_ford step 6478 current loss 0.068873, current_train_items 207328.
I0302 19:00:44.062393 22760421793920 run.py:483] Algo bellman_ford step 6479 current loss 0.054554, current_train_items 207360.
I0302 19:00:44.080823 22760421793920 run.py:483] Algo bellman_ford step 6480 current loss 0.017787, current_train_items 207392.
I0302 19:00:44.096247 22760421793920 run.py:483] Algo bellman_ford step 6481 current loss 0.048768, current_train_items 207424.
I0302 19:00:44.118974 22760421793920 run.py:483] Algo bellman_ford step 6482 current loss 0.027758, current_train_items 207456.
I0302 19:00:44.149358 22760421793920 run.py:483] Algo bellman_ford step 6483 current loss 0.052452, current_train_items 207488.
I0302 19:00:44.182181 22760421793920 run.py:483] Algo bellman_ford step 6484 current loss 0.112519, current_train_items 207520.
I0302 19:00:44.200620 22760421793920 run.py:483] Algo bellman_ford step 6485 current loss 0.002210, current_train_items 207552.
I0302 19:00:44.216546 22760421793920 run.py:483] Algo bellman_ford step 6486 current loss 0.038820, current_train_items 207584.
I0302 19:00:44.241001 22760421793920 run.py:483] Algo bellman_ford step 6487 current loss 0.038726, current_train_items 207616.
I0302 19:00:44.269782 22760421793920 run.py:483] Algo bellman_ford step 6488 current loss 0.056314, current_train_items 207648.
I0302 19:00:44.302082 22760421793920 run.py:483] Algo bellman_ford step 6489 current loss 0.048726, current_train_items 207680.
I0302 19:00:44.320733 22760421793920 run.py:483] Algo bellman_ford step 6490 current loss 0.003060, current_train_items 207712.
I0302 19:00:44.336286 22760421793920 run.py:483] Algo bellman_ford step 6491 current loss 0.007626, current_train_items 207744.
I0302 19:00:44.358823 22760421793920 run.py:483] Algo bellman_ford step 6492 current loss 0.038343, current_train_items 207776.
I0302 19:00:44.388690 22760421793920 run.py:483] Algo bellman_ford step 6493 current loss 0.037806, current_train_items 207808.
I0302 19:00:44.420520 22760421793920 run.py:483] Algo bellman_ford step 6494 current loss 0.114634, current_train_items 207840.
I0302 19:00:44.438722 22760421793920 run.py:483] Algo bellman_ford step 6495 current loss 0.002824, current_train_items 207872.
I0302 19:00:44.454088 22760421793920 run.py:483] Algo bellman_ford step 6496 current loss 0.040701, current_train_items 207904.
I0302 19:00:44.477546 22760421793920 run.py:483] Algo bellman_ford step 6497 current loss 0.058104, current_train_items 207936.
I0302 19:00:44.506585 22760421793920 run.py:483] Algo bellman_ford step 6498 current loss 0.044822, current_train_items 207968.
I0302 19:00:44.540143 22760421793920 run.py:483] Algo bellman_ford step 6499 current loss 0.073194, current_train_items 208000.
I0302 19:00:44.558477 22760421793920 run.py:483] Algo bellman_ford step 6500 current loss 0.002190, current_train_items 208032.
I0302 19:00:44.565858 22760421793920 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:00:44.565976 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:44.582283 22760421793920 run.py:483] Algo bellman_ford step 6501 current loss 0.026718, current_train_items 208064.
I0302 19:00:44.606124 22760421793920 run.py:483] Algo bellman_ford step 6502 current loss 0.068437, current_train_items 208096.
I0302 19:00:44.635384 22760421793920 run.py:483] Algo bellman_ford step 6503 current loss 0.056069, current_train_items 208128.
I0302 19:00:44.668718 22760421793920 run.py:483] Algo bellman_ford step 6504 current loss 0.049114, current_train_items 208160.
I0302 19:00:44.687121 22760421793920 run.py:483] Algo bellman_ford step 6505 current loss 0.015396, current_train_items 208192.
I0302 19:00:44.702197 22760421793920 run.py:483] Algo bellman_ford step 6506 current loss 0.005635, current_train_items 208224.
I0302 19:00:44.725145 22760421793920 run.py:483] Algo bellman_ford step 6507 current loss 0.030700, current_train_items 208256.
I0302 19:00:44.755688 22760421793920 run.py:483] Algo bellman_ford step 6508 current loss 0.080197, current_train_items 208288.
I0302 19:00:44.785565 22760421793920 run.py:483] Algo bellman_ford step 6509 current loss 0.098867, current_train_items 208320.
I0302 19:00:44.803606 22760421793920 run.py:483] Algo bellman_ford step 6510 current loss 0.004748, current_train_items 208352.
I0302 19:00:44.819275 22760421793920 run.py:483] Algo bellman_ford step 6511 current loss 0.069242, current_train_items 208384.
I0302 19:00:44.842698 22760421793920 run.py:483] Algo bellman_ford step 6512 current loss 0.037778, current_train_items 208416.
I0302 19:00:44.872180 22760421793920 run.py:483] Algo bellman_ford step 6513 current loss 0.042297, current_train_items 208448.
I0302 19:00:44.905218 22760421793920 run.py:483] Algo bellman_ford step 6514 current loss 0.073128, current_train_items 208480.
I0302 19:00:44.923157 22760421793920 run.py:483] Algo bellman_ford step 6515 current loss 0.055827, current_train_items 208512.
I0302 19:00:44.938512 22760421793920 run.py:483] Algo bellman_ford step 6516 current loss 0.022859, current_train_items 208544.
I0302 19:00:44.961617 22760421793920 run.py:483] Algo bellman_ford step 6517 current loss 0.052876, current_train_items 208576.
I0302 19:00:44.991084 22760421793920 run.py:483] Algo bellman_ford step 6518 current loss 0.068800, current_train_items 208608.
I0302 19:00:45.022046 22760421793920 run.py:483] Algo bellman_ford step 6519 current loss 0.046966, current_train_items 208640.
I0302 19:00:45.040073 22760421793920 run.py:483] Algo bellman_ford step 6520 current loss 0.054472, current_train_items 208672.
I0302 19:00:45.055722 22760421793920 run.py:483] Algo bellman_ford step 6521 current loss 0.015101, current_train_items 208704.
I0302 19:00:45.079074 22760421793920 run.py:483] Algo bellman_ford step 6522 current loss 0.036276, current_train_items 208736.
I0302 19:00:45.108330 22760421793920 run.py:483] Algo bellman_ford step 6523 current loss 0.048692, current_train_items 208768.
I0302 19:00:45.137677 22760421793920 run.py:483] Algo bellman_ford step 6524 current loss 0.087910, current_train_items 208800.
I0302 19:00:45.156108 22760421793920 run.py:483] Algo bellman_ford step 6525 current loss 0.004595, current_train_items 208832.
I0302 19:00:45.171571 22760421793920 run.py:483] Algo bellman_ford step 6526 current loss 0.016962, current_train_items 208864.
I0302 19:00:45.193973 22760421793920 run.py:483] Algo bellman_ford step 6527 current loss 0.065238, current_train_items 208896.
I0302 19:00:45.222594 22760421793920 run.py:483] Algo bellman_ford step 6528 current loss 0.036744, current_train_items 208928.
I0302 19:00:45.254653 22760421793920 run.py:483] Algo bellman_ford step 6529 current loss 0.044348, current_train_items 208960.
I0302 19:00:45.272520 22760421793920 run.py:483] Algo bellman_ford step 6530 current loss 0.002319, current_train_items 208992.
I0302 19:00:45.288335 22760421793920 run.py:483] Algo bellman_ford step 6531 current loss 0.013544, current_train_items 209024.
I0302 19:00:45.312572 22760421793920 run.py:483] Algo bellman_ford step 6532 current loss 0.042668, current_train_items 209056.
I0302 19:00:45.341374 22760421793920 run.py:483] Algo bellman_ford step 6533 current loss 0.071344, current_train_items 209088.
I0302 19:00:45.372979 22760421793920 run.py:483] Algo bellman_ford step 6534 current loss 0.106231, current_train_items 209120.
I0302 19:00:45.390816 22760421793920 run.py:483] Algo bellman_ford step 6535 current loss 0.001473, current_train_items 209152.
I0302 19:00:45.406565 22760421793920 run.py:483] Algo bellman_ford step 6536 current loss 0.022342, current_train_items 209184.
I0302 19:00:45.428878 22760421793920 run.py:483] Algo bellman_ford step 6537 current loss 0.024251, current_train_items 209216.
I0302 19:00:45.457850 22760421793920 run.py:483] Algo bellman_ford step 6538 current loss 0.032974, current_train_items 209248.
I0302 19:00:45.489527 22760421793920 run.py:483] Algo bellman_ford step 6539 current loss 0.044154, current_train_items 209280.
I0302 19:00:45.507696 22760421793920 run.py:483] Algo bellman_ford step 6540 current loss 0.001150, current_train_items 209312.
I0302 19:00:45.523412 22760421793920 run.py:483] Algo bellman_ford step 6541 current loss 0.015188, current_train_items 209344.
I0302 19:00:45.545972 22760421793920 run.py:483] Algo bellman_ford step 6542 current loss 0.045960, current_train_items 209376.
I0302 19:00:45.576141 22760421793920 run.py:483] Algo bellman_ford step 6543 current loss 0.056585, current_train_items 209408.
I0302 19:00:45.609808 22760421793920 run.py:483] Algo bellman_ford step 6544 current loss 0.061511, current_train_items 209440.
I0302 19:00:45.628044 22760421793920 run.py:483] Algo bellman_ford step 6545 current loss 0.001735, current_train_items 209472.
I0302 19:00:45.643416 22760421793920 run.py:483] Algo bellman_ford step 6546 current loss 0.034373, current_train_items 209504.
I0302 19:00:45.665670 22760421793920 run.py:483] Algo bellman_ford step 6547 current loss 0.040230, current_train_items 209536.
I0302 19:00:45.696365 22760421793920 run.py:483] Algo bellman_ford step 6548 current loss 0.078336, current_train_items 209568.
I0302 19:00:45.728994 22760421793920 run.py:483] Algo bellman_ford step 6549 current loss 0.069474, current_train_items 209600.
I0302 19:00:45.747140 22760421793920 run.py:483] Algo bellman_ford step 6550 current loss 0.019363, current_train_items 209632.
I0302 19:00:45.754573 22760421793920 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:00:45.754681 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 19:00:45.771361 22760421793920 run.py:483] Algo bellman_ford step 6551 current loss 0.008784, current_train_items 209664.
I0302 19:00:45.794404 22760421793920 run.py:483] Algo bellman_ford step 6552 current loss 0.065819, current_train_items 209696.
I0302 19:00:45.825355 22760421793920 run.py:483] Algo bellman_ford step 6553 current loss 0.175520, current_train_items 209728.
I0302 19:00:45.859301 22760421793920 run.py:483] Algo bellman_ford step 6554 current loss 0.264925, current_train_items 209760.
I0302 19:00:45.877804 22760421793920 run.py:483] Algo bellman_ford step 6555 current loss 0.010141, current_train_items 209792.
I0302 19:00:45.893851 22760421793920 run.py:483] Algo bellman_ford step 6556 current loss 0.019096, current_train_items 209824.
I0302 19:00:45.916615 22760421793920 run.py:483] Algo bellman_ford step 6557 current loss 0.041206, current_train_items 209856.
I0302 19:00:45.947160 22760421793920 run.py:483] Algo bellman_ford step 6558 current loss 0.046975, current_train_items 209888.
I0302 19:00:45.980163 22760421793920 run.py:483] Algo bellman_ford step 6559 current loss 0.116743, current_train_items 209920.
I0302 19:00:45.998482 22760421793920 run.py:483] Algo bellman_ford step 6560 current loss 0.012018, current_train_items 209952.
I0302 19:00:46.013594 22760421793920 run.py:483] Algo bellman_ford step 6561 current loss 0.010144, current_train_items 209984.
I0302 19:00:46.035658 22760421793920 run.py:483] Algo bellman_ford step 6562 current loss 0.022472, current_train_items 210016.
I0302 19:00:46.065229 22760421793920 run.py:483] Algo bellman_ford step 6563 current loss 0.048372, current_train_items 210048.
I0302 19:00:46.094602 22760421793920 run.py:483] Algo bellman_ford step 6564 current loss 0.031477, current_train_items 210080.
I0302 19:00:46.113005 22760421793920 run.py:483] Algo bellman_ford step 6565 current loss 0.004210, current_train_items 210112.
I0302 19:00:46.128348 22760421793920 run.py:483] Algo bellman_ford step 6566 current loss 0.007646, current_train_items 210144.
I0302 19:00:46.151160 22760421793920 run.py:483] Algo bellman_ford step 6567 current loss 0.056271, current_train_items 210176.
I0302 19:00:46.179319 22760421793920 run.py:483] Algo bellman_ford step 6568 current loss 0.045798, current_train_items 210208.
I0302 19:00:46.211176 22760421793920 run.py:483] Algo bellman_ford step 6569 current loss 0.098885, current_train_items 210240.
I0302 19:00:46.229345 22760421793920 run.py:483] Algo bellman_ford step 6570 current loss 0.002713, current_train_items 210272.
I0302 19:00:46.244917 22760421793920 run.py:483] Algo bellman_ford step 6571 current loss 0.032718, current_train_items 210304.
I0302 19:00:46.267449 22760421793920 run.py:483] Algo bellman_ford step 6572 current loss 0.047682, current_train_items 210336.
I0302 19:00:46.296878 22760421793920 run.py:483] Algo bellman_ford step 6573 current loss 0.029887, current_train_items 210368.
I0302 19:00:46.328181 22760421793920 run.py:483] Algo bellman_ford step 6574 current loss 0.067116, current_train_items 210400.
I0302 19:00:46.346580 22760421793920 run.py:483] Algo bellman_ford step 6575 current loss 0.013134, current_train_items 210432.
I0302 19:00:46.362352 22760421793920 run.py:483] Algo bellman_ford step 6576 current loss 0.013604, current_train_items 210464.
I0302 19:00:46.384609 22760421793920 run.py:483] Algo bellman_ford step 6577 current loss 0.027450, current_train_items 210496.
I0302 19:00:46.414652 22760421793920 run.py:483] Algo bellman_ford step 6578 current loss 0.045352, current_train_items 210528.
I0302 19:00:46.446281 22760421793920 run.py:483] Algo bellman_ford step 6579 current loss 0.065899, current_train_items 210560.
I0302 19:00:46.464937 22760421793920 run.py:483] Algo bellman_ford step 6580 current loss 0.010042, current_train_items 210592.
I0302 19:00:46.480591 22760421793920 run.py:483] Algo bellman_ford step 6581 current loss 0.013158, current_train_items 210624.
I0302 19:00:46.503427 22760421793920 run.py:483] Algo bellman_ford step 6582 current loss 0.078823, current_train_items 210656.
I0302 19:00:46.534278 22760421793920 run.py:483] Algo bellman_ford step 6583 current loss 0.116072, current_train_items 210688.
I0302 19:00:46.565675 22760421793920 run.py:483] Algo bellman_ford step 6584 current loss 0.087458, current_train_items 210720.
I0302 19:00:46.583615 22760421793920 run.py:483] Algo bellman_ford step 6585 current loss 0.007611, current_train_items 210752.
I0302 19:00:46.598794 22760421793920 run.py:483] Algo bellman_ford step 6586 current loss 0.014291, current_train_items 210784.
I0302 19:00:46.620466 22760421793920 run.py:483] Algo bellman_ford step 6587 current loss 0.014186, current_train_items 210816.
I0302 19:00:46.649344 22760421793920 run.py:483] Algo bellman_ford step 6588 current loss 0.046214, current_train_items 210848.
I0302 19:00:46.679538 22760421793920 run.py:483] Algo bellman_ford step 6589 current loss 0.112334, current_train_items 210880.
I0302 19:00:46.697583 22760421793920 run.py:483] Algo bellman_ford step 6590 current loss 0.003143, current_train_items 210912.
I0302 19:00:46.712955 22760421793920 run.py:483] Algo bellman_ford step 6591 current loss 0.004159, current_train_items 210944.
I0302 19:00:46.736085 22760421793920 run.py:483] Algo bellman_ford step 6592 current loss 0.027245, current_train_items 210976.
I0302 19:00:46.765918 22760421793920 run.py:483] Algo bellman_ford step 6593 current loss 0.028450, current_train_items 211008.
I0302 19:00:46.796662 22760421793920 run.py:483] Algo bellman_ford step 6594 current loss 0.086164, current_train_items 211040.
I0302 19:00:46.814621 22760421793920 run.py:483] Algo bellman_ford step 6595 current loss 0.000708, current_train_items 211072.
I0302 19:00:46.830119 22760421793920 run.py:483] Algo bellman_ford step 6596 current loss 0.006381, current_train_items 211104.
I0302 19:00:46.853510 22760421793920 run.py:483] Algo bellman_ford step 6597 current loss 0.026007, current_train_items 211136.
I0302 19:00:46.884040 22760421793920 run.py:483] Algo bellman_ford step 6598 current loss 0.040811, current_train_items 211168.
I0302 19:00:46.912597 22760421793920 run.py:483] Algo bellman_ford step 6599 current loss 0.030167, current_train_items 211200.
I0302 19:00:46.930758 22760421793920 run.py:483] Algo bellman_ford step 6600 current loss 0.001841, current_train_items 211232.
I0302 19:00:46.938366 22760421793920 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:00:46.938474 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 19:00:46.954647 22760421793920 run.py:483] Algo bellman_ford step 6601 current loss 0.010371, current_train_items 211264.
I0302 19:00:46.976555 22760421793920 run.py:483] Algo bellman_ford step 6602 current loss 0.096687, current_train_items 211296.
I0302 19:00:47.006698 22760421793920 run.py:483] Algo bellman_ford step 6603 current loss 0.097251, current_train_items 211328.
I0302 19:00:47.039053 22760421793920 run.py:483] Algo bellman_ford step 6604 current loss 0.111807, current_train_items 211360.
I0302 19:00:47.057333 22760421793920 run.py:483] Algo bellman_ford step 6605 current loss 0.008064, current_train_items 211392.
I0302 19:00:47.072673 22760421793920 run.py:483] Algo bellman_ford step 6606 current loss 0.028100, current_train_items 211424.
I0302 19:00:47.095375 22760421793920 run.py:483] Algo bellman_ford step 6607 current loss 0.070109, current_train_items 211456.
I0302 19:00:47.125718 22760421793920 run.py:483] Algo bellman_ford step 6608 current loss 0.074077, current_train_items 211488.
I0302 19:00:47.155627 22760421793920 run.py:483] Algo bellman_ford step 6609 current loss 0.085582, current_train_items 211520.
I0302 19:00:47.174226 22760421793920 run.py:483] Algo bellman_ford step 6610 current loss 0.001290, current_train_items 211552.
I0302 19:00:47.189684 22760421793920 run.py:483] Algo bellman_ford step 6611 current loss 0.059678, current_train_items 211584.
I0302 19:00:47.211416 22760421793920 run.py:483] Algo bellman_ford step 6612 current loss 0.039128, current_train_items 211616.
I0302 19:00:47.241419 22760421793920 run.py:483] Algo bellman_ford step 6613 current loss 0.084675, current_train_items 211648.
I0302 19:00:47.275465 22760421793920 run.py:483] Algo bellman_ford step 6614 current loss 0.076040, current_train_items 211680.
I0302 19:00:47.293548 22760421793920 run.py:483] Algo bellman_ford step 6615 current loss 0.002711, current_train_items 211712.
I0302 19:00:47.308992 22760421793920 run.py:483] Algo bellman_ford step 6616 current loss 0.003480, current_train_items 211744.
I0302 19:00:47.331874 22760421793920 run.py:483] Algo bellman_ford step 6617 current loss 0.092082, current_train_items 211776.
I0302 19:00:47.361271 22760421793920 run.py:483] Algo bellman_ford step 6618 current loss 0.137573, current_train_items 211808.
I0302 19:00:47.393321 22760421793920 run.py:483] Algo bellman_ford step 6619 current loss 0.091566, current_train_items 211840.
I0302 19:00:47.411265 22760421793920 run.py:483] Algo bellman_ford step 6620 current loss 0.021664, current_train_items 211872.
I0302 19:00:47.426559 22760421793920 run.py:483] Algo bellman_ford step 6621 current loss 0.031604, current_train_items 211904.
I0302 19:00:47.447886 22760421793920 run.py:483] Algo bellman_ford step 6622 current loss 0.060798, current_train_items 211936.
I0302 19:00:47.477861 22760421793920 run.py:483] Algo bellman_ford step 6623 current loss 0.076203, current_train_items 211968.
I0302 19:00:47.508320 22760421793920 run.py:483] Algo bellman_ford step 6624 current loss 0.099653, current_train_items 212000.
I0302 19:00:47.526740 22760421793920 run.py:483] Algo bellman_ford step 6625 current loss 0.004506, current_train_items 212032.
I0302 19:00:47.542421 22760421793920 run.py:483] Algo bellman_ford step 6626 current loss 0.020317, current_train_items 212064.
I0302 19:00:47.565123 22760421793920 run.py:483] Algo bellman_ford step 6627 current loss 0.066679, current_train_items 212096.
I0302 19:00:47.593825 22760421793920 run.py:483] Algo bellman_ford step 6628 current loss 0.107944, current_train_items 212128.
I0302 19:00:47.623982 22760421793920 run.py:483] Algo bellman_ford step 6629 current loss 0.072887, current_train_items 212160.
I0302 19:00:47.641837 22760421793920 run.py:483] Algo bellman_ford step 6630 current loss 0.005754, current_train_items 212192.
I0302 19:00:47.657454 22760421793920 run.py:483] Algo bellman_ford step 6631 current loss 0.055995, current_train_items 212224.
I0302 19:00:47.680999 22760421793920 run.py:483] Algo bellman_ford step 6632 current loss 0.140970, current_train_items 212256.
I0302 19:00:47.711718 22760421793920 run.py:483] Algo bellman_ford step 6633 current loss 0.153845, current_train_items 212288.
I0302 19:00:47.742195 22760421793920 run.py:483] Algo bellman_ford step 6634 current loss 0.081991, current_train_items 212320.
I0302 19:00:47.760259 22760421793920 run.py:483] Algo bellman_ford step 6635 current loss 0.017979, current_train_items 212352.
I0302 19:00:47.775986 22760421793920 run.py:483] Algo bellman_ford step 6636 current loss 0.075518, current_train_items 212384.
I0302 19:00:47.798853 22760421793920 run.py:483] Algo bellman_ford step 6637 current loss 0.044477, current_train_items 212416.
I0302 19:00:47.828311 22760421793920 run.py:483] Algo bellman_ford step 6638 current loss 0.222930, current_train_items 212448.
I0302 19:00:47.860424 22760421793920 run.py:483] Algo bellman_ford step 6639 current loss 0.191908, current_train_items 212480.
I0302 19:00:47.878350 22760421793920 run.py:483] Algo bellman_ford step 6640 current loss 0.003917, current_train_items 212512.
I0302 19:00:47.894106 22760421793920 run.py:483] Algo bellman_ford step 6641 current loss 0.016262, current_train_items 212544.
I0302 19:00:47.917779 22760421793920 run.py:483] Algo bellman_ford step 6642 current loss 0.061532, current_train_items 212576.
I0302 19:00:47.947052 22760421793920 run.py:483] Algo bellman_ford step 6643 current loss 0.060827, current_train_items 212608.
I0302 19:00:47.979571 22760421793920 run.py:483] Algo bellman_ford step 6644 current loss 0.067509, current_train_items 212640.
I0302 19:00:47.997466 22760421793920 run.py:483] Algo bellman_ford step 6645 current loss 0.013976, current_train_items 212672.
I0302 19:00:48.012946 22760421793920 run.py:483] Algo bellman_ford step 6646 current loss 0.015439, current_train_items 212704.
I0302 19:00:48.035659 22760421793920 run.py:483] Algo bellman_ford step 6647 current loss 0.104265, current_train_items 212736.
I0302 19:00:48.065872 22760421793920 run.py:483] Algo bellman_ford step 6648 current loss 0.166612, current_train_items 212768.
I0302 19:00:48.098361 22760421793920 run.py:483] Algo bellman_ford step 6649 current loss 0.146842, current_train_items 212800.
I0302 19:00:48.116776 22760421793920 run.py:483] Algo bellman_ford step 6650 current loss 0.008101, current_train_items 212832.
I0302 19:00:48.124337 22760421793920 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:00:48.124447 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:48.141172 22760421793920 run.py:483] Algo bellman_ford step 6651 current loss 0.019108, current_train_items 212864.
I0302 19:00:48.165193 22760421793920 run.py:483] Algo bellman_ford step 6652 current loss 0.078102, current_train_items 212896.
I0302 19:00:48.195696 22760421793920 run.py:483] Algo bellman_ford step 6653 current loss 0.070011, current_train_items 212928.
I0302 19:00:48.223850 22760421793920 run.py:483] Algo bellman_ford step 6654 current loss 0.065281, current_train_items 212960.
I0302 19:00:48.242156 22760421793920 run.py:483] Algo bellman_ford step 6655 current loss 0.007531, current_train_items 212992.
I0302 19:00:48.257969 22760421793920 run.py:483] Algo bellman_ford step 6656 current loss 0.027146, current_train_items 213024.
I0302 19:00:48.280594 22760421793920 run.py:483] Algo bellman_ford step 6657 current loss 0.044736, current_train_items 213056.
I0302 19:00:48.310789 22760421793920 run.py:483] Algo bellman_ford step 6658 current loss 0.029827, current_train_items 213088.
I0302 19:00:48.344480 22760421793920 run.py:483] Algo bellman_ford step 6659 current loss 0.083855, current_train_items 213120.
I0302 19:00:48.362907 22760421793920 run.py:483] Algo bellman_ford step 6660 current loss 0.016469, current_train_items 213152.
I0302 19:00:48.378863 22760421793920 run.py:483] Algo bellman_ford step 6661 current loss 0.031827, current_train_items 213184.
I0302 19:00:48.403340 22760421793920 run.py:483] Algo bellman_ford step 6662 current loss 0.054403, current_train_items 213216.
I0302 19:00:48.432338 22760421793920 run.py:483] Algo bellman_ford step 6663 current loss 0.025480, current_train_items 213248.
I0302 19:00:48.464073 22760421793920 run.py:483] Algo bellman_ford step 6664 current loss 0.039973, current_train_items 213280.
I0302 19:00:48.482687 22760421793920 run.py:483] Algo bellman_ford step 6665 current loss 0.005823, current_train_items 213312.
I0302 19:00:48.498371 22760421793920 run.py:483] Algo bellman_ford step 6666 current loss 0.018199, current_train_items 213344.
I0302 19:00:48.523411 22760421793920 run.py:483] Algo bellman_ford step 6667 current loss 0.063317, current_train_items 213376.
I0302 19:00:48.554624 22760421793920 run.py:483] Algo bellman_ford step 6668 current loss 0.086498, current_train_items 213408.
I0302 19:00:48.588331 22760421793920 run.py:483] Algo bellman_ford step 6669 current loss 0.055788, current_train_items 213440.
I0302 19:00:48.606953 22760421793920 run.py:483] Algo bellman_ford step 6670 current loss 0.002014, current_train_items 213472.
I0302 19:00:48.622509 22760421793920 run.py:483] Algo bellman_ford step 6671 current loss 0.017785, current_train_items 213504.
I0302 19:00:48.646056 22760421793920 run.py:483] Algo bellman_ford step 6672 current loss 0.046300, current_train_items 213536.
I0302 19:00:48.676673 22760421793920 run.py:483] Algo bellman_ford step 6673 current loss 0.073579, current_train_items 213568.
I0302 19:00:48.707502 22760421793920 run.py:483] Algo bellman_ford step 6674 current loss 0.117715, current_train_items 213600.
I0302 19:00:48.726107 22760421793920 run.py:483] Algo bellman_ford step 6675 current loss 0.022430, current_train_items 213632.
I0302 19:00:48.741234 22760421793920 run.py:483] Algo bellman_ford step 6676 current loss 0.014226, current_train_items 213664.
I0302 19:00:48.764997 22760421793920 run.py:483] Algo bellman_ford step 6677 current loss 0.071750, current_train_items 213696.
I0302 19:00:48.796159 22760421793920 run.py:483] Algo bellman_ford step 6678 current loss 0.095611, current_train_items 213728.
I0302 19:00:48.830038 22760421793920 run.py:483] Algo bellman_ford step 6679 current loss 0.153020, current_train_items 213760.
I0302 19:00:48.848553 22760421793920 run.py:483] Algo bellman_ford step 6680 current loss 0.003731, current_train_items 213792.
I0302 19:00:48.864293 22760421793920 run.py:483] Algo bellman_ford step 6681 current loss 0.013797, current_train_items 213824.
I0302 19:00:48.888208 22760421793920 run.py:483] Algo bellman_ford step 6682 current loss 0.086622, current_train_items 213856.
I0302 19:00:48.918168 22760421793920 run.py:483] Algo bellman_ford step 6683 current loss 0.060153, current_train_items 213888.
I0302 19:00:48.953004 22760421793920 run.py:483] Algo bellman_ford step 6684 current loss 0.106696, current_train_items 213920.
I0302 19:00:48.971546 22760421793920 run.py:483] Algo bellman_ford step 6685 current loss 0.024440, current_train_items 213952.
I0302 19:00:48.986995 22760421793920 run.py:483] Algo bellman_ford step 6686 current loss 0.095233, current_train_items 213984.
I0302 19:00:49.010288 22760421793920 run.py:483] Algo bellman_ford step 6687 current loss 0.082543, current_train_items 214016.
I0302 19:00:49.040588 22760421793920 run.py:483] Algo bellman_ford step 6688 current loss 0.057579, current_train_items 214048.
I0302 19:00:49.072115 22760421793920 run.py:483] Algo bellman_ford step 6689 current loss 0.047486, current_train_items 214080.
I0302 19:00:49.090798 22760421793920 run.py:483] Algo bellman_ford step 6690 current loss 0.004822, current_train_items 214112.
I0302 19:00:49.106134 22760421793920 run.py:483] Algo bellman_ford step 6691 current loss 0.004766, current_train_items 214144.
I0302 19:00:49.129910 22760421793920 run.py:483] Algo bellman_ford step 6692 current loss 0.111297, current_train_items 214176.
I0302 19:00:49.160089 22760421793920 run.py:483] Algo bellman_ford step 6693 current loss 0.191233, current_train_items 214208.
I0302 19:00:49.192531 22760421793920 run.py:483] Algo bellman_ford step 6694 current loss 0.080998, current_train_items 214240.
I0302 19:00:49.211024 22760421793920 run.py:483] Algo bellman_ford step 6695 current loss 0.008725, current_train_items 214272.
I0302 19:00:49.226868 22760421793920 run.py:483] Algo bellman_ford step 6696 current loss 0.054571, current_train_items 214304.
I0302 19:00:49.250102 22760421793920 run.py:483] Algo bellman_ford step 6697 current loss 0.052445, current_train_items 214336.
I0302 19:00:49.280875 22760421793920 run.py:483] Algo bellman_ford step 6698 current loss 0.100455, current_train_items 214368.
I0302 19:00:49.314135 22760421793920 run.py:483] Algo bellman_ford step 6699 current loss 0.113382, current_train_items 214400.
I0302 19:00:49.332071 22760421793920 run.py:483] Algo bellman_ford step 6700 current loss 0.033773, current_train_items 214432.
I0302 19:00:49.339544 22760421793920 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:00:49.339652 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:49.356452 22760421793920 run.py:483] Algo bellman_ford step 6701 current loss 0.027522, current_train_items 214464.
I0302 19:00:49.379618 22760421793920 run.py:483] Algo bellman_ford step 6702 current loss 0.037567, current_train_items 214496.
I0302 19:00:49.410165 22760421793920 run.py:483] Algo bellman_ford step 6703 current loss 0.061565, current_train_items 214528.
I0302 19:00:49.441618 22760421793920 run.py:483] Algo bellman_ford step 6704 current loss 0.068339, current_train_items 214560.
I0302 19:00:49.460103 22760421793920 run.py:483] Algo bellman_ford step 6705 current loss 0.019682, current_train_items 214592.
I0302 19:00:49.475787 22760421793920 run.py:483] Algo bellman_ford step 6706 current loss 0.026582, current_train_items 214624.
I0302 19:00:49.499885 22760421793920 run.py:483] Algo bellman_ford step 6707 current loss 0.037927, current_train_items 214656.
I0302 19:00:49.529862 22760421793920 run.py:483] Algo bellman_ford step 6708 current loss 0.037007, current_train_items 214688.
I0302 19:00:49.562737 22760421793920 run.py:483] Algo bellman_ford step 6709 current loss 0.070037, current_train_items 214720.
I0302 19:00:49.580942 22760421793920 run.py:483] Algo bellman_ford step 6710 current loss 0.004268, current_train_items 214752.
I0302 19:00:49.596748 22760421793920 run.py:483] Algo bellman_ford step 6711 current loss 0.025789, current_train_items 214784.
I0302 19:00:49.618754 22760421793920 run.py:483] Algo bellman_ford step 6712 current loss 0.044958, current_train_items 214816.
I0302 19:00:49.649923 22760421793920 run.py:483] Algo bellman_ford step 6713 current loss 0.081825, current_train_items 214848.
I0302 19:00:49.681568 22760421793920 run.py:483] Algo bellman_ford step 6714 current loss 0.039508, current_train_items 214880.
I0302 19:00:49.699795 22760421793920 run.py:483] Algo bellman_ford step 6715 current loss 0.013366, current_train_items 214912.
I0302 19:00:49.715459 22760421793920 run.py:483] Algo bellman_ford step 6716 current loss 0.012694, current_train_items 214944.
I0302 19:00:49.738399 22760421793920 run.py:483] Algo bellman_ford step 6717 current loss 0.030629, current_train_items 214976.
I0302 19:00:49.768546 22760421793920 run.py:483] Algo bellman_ford step 6718 current loss 0.055098, current_train_items 215008.
I0302 19:00:49.802067 22760421793920 run.py:483] Algo bellman_ford step 6719 current loss 0.078035, current_train_items 215040.
I0302 19:00:49.820061 22760421793920 run.py:483] Algo bellman_ford step 6720 current loss 0.002376, current_train_items 215072.
I0302 19:00:49.835635 22760421793920 run.py:483] Algo bellman_ford step 6721 current loss 0.010289, current_train_items 215104.
I0302 19:00:49.857549 22760421793920 run.py:483] Algo bellman_ford step 6722 current loss 0.023287, current_train_items 215136.
I0302 19:00:49.888254 22760421793920 run.py:483] Algo bellman_ford step 6723 current loss 0.126721, current_train_items 215168.
I0302 19:00:49.918806 22760421793920 run.py:483] Algo bellman_ford step 6724 current loss 0.099845, current_train_items 215200.
I0302 19:00:49.936741 22760421793920 run.py:483] Algo bellman_ford step 6725 current loss 0.006167, current_train_items 215232.
I0302 19:00:49.952429 22760421793920 run.py:483] Algo bellman_ford step 6726 current loss 0.039188, current_train_items 215264.
I0302 19:00:49.974663 22760421793920 run.py:483] Algo bellman_ford step 6727 current loss 0.030466, current_train_items 215296.
I0302 19:00:50.004150 22760421793920 run.py:483] Algo bellman_ford step 6728 current loss 0.035761, current_train_items 215328.
I0302 19:00:50.034460 22760421793920 run.py:483] Algo bellman_ford step 6729 current loss 0.090310, current_train_items 215360.
I0302 19:00:50.052520 22760421793920 run.py:483] Algo bellman_ford step 6730 current loss 0.003204, current_train_items 215392.
I0302 19:00:50.067977 22760421793920 run.py:483] Algo bellman_ford step 6731 current loss 0.012025, current_train_items 215424.
I0302 19:00:50.091551 22760421793920 run.py:483] Algo bellman_ford step 6732 current loss 0.080776, current_train_items 215456.
I0302 19:00:50.119629 22760421793920 run.py:483] Algo bellman_ford step 6733 current loss 0.065316, current_train_items 215488.
I0302 19:00:50.149393 22760421793920 run.py:483] Algo bellman_ford step 6734 current loss 0.058195, current_train_items 215520.
I0302 19:00:50.167464 22760421793920 run.py:483] Algo bellman_ford step 6735 current loss 0.000911, current_train_items 215552.
I0302 19:00:50.183300 22760421793920 run.py:483] Algo bellman_ford step 6736 current loss 0.039772, current_train_items 215584.
I0302 19:00:50.206917 22760421793920 run.py:483] Algo bellman_ford step 6737 current loss 0.067337, current_train_items 215616.
I0302 19:00:50.235873 22760421793920 run.py:483] Algo bellman_ford step 6738 current loss 0.023148, current_train_items 215648.
I0302 19:00:50.269123 22760421793920 run.py:483] Algo bellman_ford step 6739 current loss 0.077176, current_train_items 215680.
I0302 19:00:50.287317 22760421793920 run.py:483] Algo bellman_ford step 6740 current loss 0.015554, current_train_items 215712.
I0302 19:00:50.303109 22760421793920 run.py:483] Algo bellman_ford step 6741 current loss 0.029163, current_train_items 215744.
I0302 19:00:50.326510 22760421793920 run.py:483] Algo bellman_ford step 6742 current loss 0.060826, current_train_items 215776.
I0302 19:00:50.355095 22760421793920 run.py:483] Algo bellman_ford step 6743 current loss 0.039409, current_train_items 215808.
I0302 19:00:50.387886 22760421793920 run.py:483] Algo bellman_ford step 6744 current loss 0.050868, current_train_items 215840.
I0302 19:00:50.406101 22760421793920 run.py:483] Algo bellman_ford step 6745 current loss 0.006347, current_train_items 215872.
I0302 19:00:50.421848 22760421793920 run.py:483] Algo bellman_ford step 6746 current loss 0.030103, current_train_items 215904.
I0302 19:00:50.444707 22760421793920 run.py:483] Algo bellman_ford step 6747 current loss 0.031551, current_train_items 215936.
I0302 19:00:50.473842 22760421793920 run.py:483] Algo bellman_ford step 6748 current loss 0.043753, current_train_items 215968.
I0302 19:00:50.506061 22760421793920 run.py:483] Algo bellman_ford step 6749 current loss 0.061069, current_train_items 216000.
I0302 19:00:50.524046 22760421793920 run.py:483] Algo bellman_ford step 6750 current loss 0.002272, current_train_items 216032.
I0302 19:00:50.531626 22760421793920 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:00:50.531736 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:50.548308 22760421793920 run.py:483] Algo bellman_ford step 6751 current loss 0.019084, current_train_items 216064.
I0302 19:00:50.572396 22760421793920 run.py:483] Algo bellman_ford step 6752 current loss 0.047911, current_train_items 216096.
I0302 19:00:50.603602 22760421793920 run.py:483] Algo bellman_ford step 6753 current loss 0.055770, current_train_items 216128.
I0302 19:00:50.637163 22760421793920 run.py:483] Algo bellman_ford step 6754 current loss 0.072037, current_train_items 216160.
I0302 19:00:50.655723 22760421793920 run.py:483] Algo bellman_ford step 6755 current loss 0.017598, current_train_items 216192.
I0302 19:00:50.672360 22760421793920 run.py:483] Algo bellman_ford step 6756 current loss 0.022569, current_train_items 216224.
I0302 19:00:50.695084 22760421793920 run.py:483] Algo bellman_ford step 6757 current loss 0.022054, current_train_items 216256.
I0302 19:00:50.725538 22760421793920 run.py:483] Algo bellman_ford step 6758 current loss 0.069626, current_train_items 216288.
I0302 19:00:50.759021 22760421793920 run.py:483] Algo bellman_ford step 6759 current loss 0.095460, current_train_items 216320.
I0302 19:00:50.777382 22760421793920 run.py:483] Algo bellman_ford step 6760 current loss 0.018832, current_train_items 216352.
I0302 19:00:50.793312 22760421793920 run.py:483] Algo bellman_ford step 6761 current loss 0.005232, current_train_items 216384.
I0302 19:00:50.816809 22760421793920 run.py:483] Algo bellman_ford step 6762 current loss 0.077110, current_train_items 216416.
I0302 19:00:50.847522 22760421793920 run.py:483] Algo bellman_ford step 6763 current loss 0.110978, current_train_items 216448.
I0302 19:00:50.879119 22760421793920 run.py:483] Algo bellman_ford step 6764 current loss 0.095985, current_train_items 216480.
I0302 19:00:50.897435 22760421793920 run.py:483] Algo bellman_ford step 6765 current loss 0.002191, current_train_items 216512.
I0302 19:00:50.913444 22760421793920 run.py:483] Algo bellman_ford step 6766 current loss 0.118495, current_train_items 216544.
I0302 19:00:50.936706 22760421793920 run.py:483] Algo bellman_ford step 6767 current loss 0.041539, current_train_items 216576.
I0302 19:00:50.965791 22760421793920 run.py:483] Algo bellman_ford step 6768 current loss 0.045442, current_train_items 216608.
I0302 19:00:50.997509 22760421793920 run.py:483] Algo bellman_ford step 6769 current loss 0.084020, current_train_items 216640.
I0302 19:00:51.015628 22760421793920 run.py:483] Algo bellman_ford step 6770 current loss 0.002207, current_train_items 216672.
I0302 19:00:51.031090 22760421793920 run.py:483] Algo bellman_ford step 6771 current loss 0.025333, current_train_items 216704.
I0302 19:00:51.053414 22760421793920 run.py:483] Algo bellman_ford step 6772 current loss 0.020168, current_train_items 216736.
I0302 19:00:51.083688 22760421793920 run.py:483] Algo bellman_ford step 6773 current loss 0.075086, current_train_items 216768.
I0302 19:00:51.113913 22760421793920 run.py:483] Algo bellman_ford step 6774 current loss 0.079033, current_train_items 216800.
I0302 19:00:51.132386 22760421793920 run.py:483] Algo bellman_ford step 6775 current loss 0.013583, current_train_items 216832.
I0302 19:00:51.148194 22760421793920 run.py:483] Algo bellman_ford step 6776 current loss 0.035418, current_train_items 216864.
I0302 19:00:51.171053 22760421793920 run.py:483] Algo bellman_ford step 6777 current loss 0.017810, current_train_items 216896.
I0302 19:00:51.202465 22760421793920 run.py:483] Algo bellman_ford step 6778 current loss 0.063791, current_train_items 216928.
I0302 19:00:51.234179 22760421793920 run.py:483] Algo bellman_ford step 6779 current loss 0.052171, current_train_items 216960.
I0302 19:00:51.252727 22760421793920 run.py:483] Algo bellman_ford step 6780 current loss 0.002798, current_train_items 216992.
I0302 19:00:51.268396 22760421793920 run.py:483] Algo bellman_ford step 6781 current loss 0.007299, current_train_items 217024.
I0302 19:00:51.290807 22760421793920 run.py:483] Algo bellman_ford step 6782 current loss 0.050713, current_train_items 217056.
I0302 19:00:51.322165 22760421793920 run.py:483] Algo bellman_ford step 6783 current loss 0.075053, current_train_items 217088.
I0302 19:00:51.353480 22760421793920 run.py:483] Algo bellman_ford step 6784 current loss 0.110181, current_train_items 217120.
I0302 19:00:51.371871 22760421793920 run.py:483] Algo bellman_ford step 6785 current loss 0.010042, current_train_items 217152.
I0302 19:00:51.387278 22760421793920 run.py:483] Algo bellman_ford step 6786 current loss 0.008662, current_train_items 217184.
I0302 19:00:51.410204 22760421793920 run.py:483] Algo bellman_ford step 6787 current loss 0.046696, current_train_items 217216.
I0302 19:00:51.439258 22760421793920 run.py:483] Algo bellman_ford step 6788 current loss 0.064222, current_train_items 217248.
I0302 19:00:51.471758 22760421793920 run.py:483] Algo bellman_ford step 6789 current loss 0.120910, current_train_items 217280.
I0302 19:00:51.490221 22760421793920 run.py:483] Algo bellman_ford step 6790 current loss 0.004945, current_train_items 217312.
I0302 19:00:51.505750 22760421793920 run.py:483] Algo bellman_ford step 6791 current loss 0.010462, current_train_items 217344.
I0302 19:00:51.529179 22760421793920 run.py:483] Algo bellman_ford step 6792 current loss 0.018274, current_train_items 217376.
I0302 19:00:51.558667 22760421793920 run.py:483] Algo bellman_ford step 6793 current loss 0.051544, current_train_items 217408.
I0302 19:00:51.590478 22760421793920 run.py:483] Algo bellman_ford step 6794 current loss 0.113646, current_train_items 217440.
I0302 19:00:51.608737 22760421793920 run.py:483] Algo bellman_ford step 6795 current loss 0.006596, current_train_items 217472.
I0302 19:00:51.624320 22760421793920 run.py:483] Algo bellman_ford step 6796 current loss 0.013534, current_train_items 217504.
I0302 19:00:51.648142 22760421793920 run.py:483] Algo bellman_ford step 6797 current loss 0.045430, current_train_items 217536.
I0302 19:00:51.677402 22760421793920 run.py:483] Algo bellman_ford step 6798 current loss 0.051148, current_train_items 217568.
I0302 19:00:51.709481 22760421793920 run.py:483] Algo bellman_ford step 6799 current loss 0.055008, current_train_items 217600.
I0302 19:00:51.727806 22760421793920 run.py:483] Algo bellman_ford step 6800 current loss 0.040625, current_train_items 217632.
I0302 19:00:51.735154 22760421793920 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:00:51.735262 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:51.751388 22760421793920 run.py:483] Algo bellman_ford step 6801 current loss 0.018176, current_train_items 217664.
I0302 19:00:51.774712 22760421793920 run.py:483] Algo bellman_ford step 6802 current loss 0.038578, current_train_items 217696.
I0302 19:00:51.806577 22760421793920 run.py:483] Algo bellman_ford step 6803 current loss 0.056448, current_train_items 217728.
I0302 19:00:51.839186 22760421793920 run.py:483] Algo bellman_ford step 6804 current loss 0.064659, current_train_items 217760.
I0302 19:00:51.857991 22760421793920 run.py:483] Algo bellman_ford step 6805 current loss 0.003767, current_train_items 217792.
I0302 19:00:51.873866 22760421793920 run.py:483] Algo bellman_ford step 6806 current loss 0.009406, current_train_items 217824.
I0302 19:00:51.897361 22760421793920 run.py:483] Algo bellman_ford step 6807 current loss 0.054502, current_train_items 217856.
I0302 19:00:51.926265 22760421793920 run.py:483] Algo bellman_ford step 6808 current loss 0.036619, current_train_items 217888.
I0302 19:00:51.958147 22760421793920 run.py:483] Algo bellman_ford step 6809 current loss 0.053866, current_train_items 217920.
I0302 19:00:51.976564 22760421793920 run.py:483] Algo bellman_ford step 6810 current loss 0.014559, current_train_items 217952.
I0302 19:00:51.991811 22760421793920 run.py:483] Algo bellman_ford step 6811 current loss 0.011920, current_train_items 217984.
I0302 19:00:52.015818 22760421793920 run.py:483] Algo bellman_ford step 6812 current loss 0.061152, current_train_items 218016.
I0302 19:00:52.045408 22760421793920 run.py:483] Algo bellman_ford step 6813 current loss 0.072846, current_train_items 218048.
I0302 19:00:52.075361 22760421793920 run.py:483] Algo bellman_ford step 6814 current loss 0.062747, current_train_items 218080.
I0302 19:00:52.093569 22760421793920 run.py:483] Algo bellman_ford step 6815 current loss 0.003976, current_train_items 218112.
I0302 19:00:52.109245 22760421793920 run.py:483] Algo bellman_ford step 6816 current loss 0.040981, current_train_items 218144.
I0302 19:00:52.131239 22760421793920 run.py:483] Algo bellman_ford step 6817 current loss 0.050186, current_train_items 218176.
I0302 19:00:52.161796 22760421793920 run.py:483] Algo bellman_ford step 6818 current loss 0.069583, current_train_items 218208.
I0302 19:00:52.195490 22760421793920 run.py:483] Algo bellman_ford step 6819 current loss 0.131791, current_train_items 218240.
I0302 19:00:52.213862 22760421793920 run.py:483] Algo bellman_ford step 6820 current loss 0.007710, current_train_items 218272.
I0302 19:00:52.229439 22760421793920 run.py:483] Algo bellman_ford step 6821 current loss 0.008372, current_train_items 218304.
I0302 19:00:52.252130 22760421793920 run.py:483] Algo bellman_ford step 6822 current loss 0.028807, current_train_items 218336.
I0302 19:00:52.282284 22760421793920 run.py:483] Algo bellman_ford step 6823 current loss 0.102195, current_train_items 218368.
I0302 19:00:52.315613 22760421793920 run.py:483] Algo bellman_ford step 6824 current loss 0.093236, current_train_items 218400.
I0302 19:00:52.333978 22760421793920 run.py:483] Algo bellman_ford step 6825 current loss 0.004647, current_train_items 218432.
I0302 19:00:52.349749 22760421793920 run.py:483] Algo bellman_ford step 6826 current loss 0.016224, current_train_items 218464.
I0302 19:00:52.372651 22760421793920 run.py:483] Algo bellman_ford step 6827 current loss 0.029270, current_train_items 218496.
I0302 19:00:52.401938 22760421793920 run.py:483] Algo bellman_ford step 6828 current loss 0.021579, current_train_items 218528.
I0302 19:00:52.435984 22760421793920 run.py:483] Algo bellman_ford step 6829 current loss 0.070563, current_train_items 218560.
I0302 19:00:52.454309 22760421793920 run.py:483] Algo bellman_ford step 6830 current loss 0.005924, current_train_items 218592.
I0302 19:00:52.469782 22760421793920 run.py:483] Algo bellman_ford step 6831 current loss 0.014248, current_train_items 218624.
I0302 19:00:52.494456 22760421793920 run.py:483] Algo bellman_ford step 6832 current loss 0.047747, current_train_items 218656.
I0302 19:00:52.524194 22760421793920 run.py:483] Algo bellman_ford step 6833 current loss 0.051551, current_train_items 218688.
I0302 19:00:52.556154 22760421793920 run.py:483] Algo bellman_ford step 6834 current loss 0.052504, current_train_items 218720.
I0302 19:00:52.574459 22760421793920 run.py:483] Algo bellman_ford step 6835 current loss 0.011579, current_train_items 218752.
I0302 19:00:52.590329 22760421793920 run.py:483] Algo bellman_ford step 6836 current loss 0.023935, current_train_items 218784.
I0302 19:00:52.613043 22760421793920 run.py:483] Algo bellman_ford step 6837 current loss 0.035408, current_train_items 218816.
I0302 19:00:52.643834 22760421793920 run.py:483] Algo bellman_ford step 6838 current loss 0.055569, current_train_items 218848.
I0302 19:00:52.673887 22760421793920 run.py:483] Algo bellman_ford step 6839 current loss 0.091774, current_train_items 218880.
I0302 19:00:52.692193 22760421793920 run.py:483] Algo bellman_ford step 6840 current loss 0.003911, current_train_items 218912.
I0302 19:00:52.708262 22760421793920 run.py:483] Algo bellman_ford step 6841 current loss 0.033302, current_train_items 218944.
I0302 19:00:52.732051 22760421793920 run.py:483] Algo bellman_ford step 6842 current loss 0.045724, current_train_items 218976.
I0302 19:00:52.761055 22760421793920 run.py:483] Algo bellman_ford step 6843 current loss 0.027724, current_train_items 219008.
I0302 19:00:52.792500 22760421793920 run.py:483] Algo bellman_ford step 6844 current loss 0.070384, current_train_items 219040.
I0302 19:00:52.810794 22760421793920 run.py:483] Algo bellman_ford step 6845 current loss 0.004656, current_train_items 219072.
I0302 19:00:52.827060 22760421793920 run.py:483] Algo bellman_ford step 6846 current loss 0.035016, current_train_items 219104.
I0302 19:00:52.851061 22760421793920 run.py:483] Algo bellman_ford step 6847 current loss 0.060162, current_train_items 219136.
I0302 19:00:52.881848 22760421793920 run.py:483] Algo bellman_ford step 6848 current loss 0.036301, current_train_items 219168.
I0302 19:00:52.914534 22760421793920 run.py:483] Algo bellman_ford step 6849 current loss 0.052673, current_train_items 219200.
I0302 19:00:52.932975 22760421793920 run.py:483] Algo bellman_ford step 6850 current loss 0.002967, current_train_items 219232.
I0302 19:00:52.940369 22760421793920 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:00:52.940479 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 19:00:52.957180 22760421793920 run.py:483] Algo bellman_ford step 6851 current loss 0.031401, current_train_items 219264.
I0302 19:00:52.981049 22760421793920 run.py:483] Algo bellman_ford step 6852 current loss 0.028389, current_train_items 219296.
I0302 19:00:53.011431 22760421793920 run.py:483] Algo bellman_ford step 6853 current loss 0.035983, current_train_items 219328.
I0302 19:00:53.044497 22760421793920 run.py:483] Algo bellman_ford step 6854 current loss 0.048780, current_train_items 219360.
I0302 19:00:53.063104 22760421793920 run.py:483] Algo bellman_ford step 6855 current loss 0.006379, current_train_items 219392.
I0302 19:00:53.078996 22760421793920 run.py:483] Algo bellman_ford step 6856 current loss 0.005631, current_train_items 219424.
I0302 19:00:53.102038 22760421793920 run.py:483] Algo bellman_ford step 6857 current loss 0.048472, current_train_items 219456.
I0302 19:00:53.131416 22760421793920 run.py:483] Algo bellman_ford step 6858 current loss 0.079136, current_train_items 219488.
I0302 19:00:53.163501 22760421793920 run.py:483] Algo bellman_ford step 6859 current loss 0.073937, current_train_items 219520.
I0302 19:00:53.181658 22760421793920 run.py:483] Algo bellman_ford step 6860 current loss 0.003007, current_train_items 219552.
I0302 19:00:53.197147 22760421793920 run.py:483] Algo bellman_ford step 6861 current loss 0.015323, current_train_items 219584.
I0302 19:00:53.219596 22760421793920 run.py:483] Algo bellman_ford step 6862 current loss 0.032244, current_train_items 219616.
I0302 19:00:53.248962 22760421793920 run.py:483] Algo bellman_ford step 6863 current loss 0.079336, current_train_items 219648.
I0302 19:00:53.279302 22760421793920 run.py:483] Algo bellman_ford step 6864 current loss 0.065584, current_train_items 219680.
I0302 19:00:53.297799 22760421793920 run.py:483] Algo bellman_ford step 6865 current loss 0.014884, current_train_items 219712.
I0302 19:00:53.313576 22760421793920 run.py:483] Algo bellman_ford step 6866 current loss 0.008705, current_train_items 219744.
I0302 19:00:53.337507 22760421793920 run.py:483] Algo bellman_ford step 6867 current loss 0.076053, current_train_items 219776.
I0302 19:00:53.367679 22760421793920 run.py:483] Algo bellman_ford step 6868 current loss 0.071691, current_train_items 219808.
I0302 19:00:53.401983 22760421793920 run.py:483] Algo bellman_ford step 6869 current loss 0.143216, current_train_items 219840.
I0302 19:00:53.420359 22760421793920 run.py:483] Algo bellman_ford step 6870 current loss 0.015847, current_train_items 219872.
I0302 19:00:53.436582 22760421793920 run.py:483] Algo bellman_ford step 6871 current loss 0.031836, current_train_items 219904.
I0302 19:00:53.459716 22760421793920 run.py:483] Algo bellman_ford step 6872 current loss 0.034548, current_train_items 219936.
I0302 19:00:53.488511 22760421793920 run.py:483] Algo bellman_ford step 6873 current loss 0.075042, current_train_items 219968.
I0302 19:00:53.520644 22760421793920 run.py:483] Algo bellman_ford step 6874 current loss 0.074150, current_train_items 220000.
I0302 19:00:53.539165 22760421793920 run.py:483] Algo bellman_ford step 6875 current loss 0.024997, current_train_items 220032.
I0302 19:00:53.554680 22760421793920 run.py:483] Algo bellman_ford step 6876 current loss 0.010029, current_train_items 220064.
I0302 19:00:53.577958 22760421793920 run.py:483] Algo bellman_ford step 6877 current loss 0.080593, current_train_items 220096.
I0302 19:00:53.607835 22760421793920 run.py:483] Algo bellman_ford step 6878 current loss 0.042836, current_train_items 220128.
I0302 19:00:53.640483 22760421793920 run.py:483] Algo bellman_ford step 6879 current loss 0.070413, current_train_items 220160.
I0302 19:00:53.658786 22760421793920 run.py:483] Algo bellman_ford step 6880 current loss 0.011672, current_train_items 220192.
I0302 19:00:53.674537 22760421793920 run.py:483] Algo bellman_ford step 6881 current loss 0.008462, current_train_items 220224.
I0302 19:00:53.698315 22760421793920 run.py:483] Algo bellman_ford step 6882 current loss 0.026519, current_train_items 220256.
I0302 19:00:53.727286 22760421793920 run.py:483] Algo bellman_ford step 6883 current loss 0.041336, current_train_items 220288.
I0302 19:00:53.758912 22760421793920 run.py:483] Algo bellman_ford step 6884 current loss 0.100847, current_train_items 220320.
I0302 19:00:53.777035 22760421793920 run.py:483] Algo bellman_ford step 6885 current loss 0.003434, current_train_items 220352.
I0302 19:00:53.792863 22760421793920 run.py:483] Algo bellman_ford step 6886 current loss 0.022570, current_train_items 220384.
I0302 19:00:53.816802 22760421793920 run.py:483] Algo bellman_ford step 6887 current loss 0.030997, current_train_items 220416.
I0302 19:00:53.847781 22760421793920 run.py:483] Algo bellman_ford step 6888 current loss 0.057621, current_train_items 220448.
I0302 19:00:53.879911 22760421793920 run.py:483] Algo bellman_ford step 6889 current loss 0.030728, current_train_items 220480.
I0302 19:00:53.898068 22760421793920 run.py:483] Algo bellman_ford step 6890 current loss 0.037394, current_train_items 220512.
I0302 19:00:53.914065 22760421793920 run.py:483] Algo bellman_ford step 6891 current loss 0.022563, current_train_items 220544.
I0302 19:00:53.937994 22760421793920 run.py:483] Algo bellman_ford step 6892 current loss 0.028993, current_train_items 220576.
I0302 19:00:53.968762 22760421793920 run.py:483] Algo bellman_ford step 6893 current loss 0.041113, current_train_items 220608.
I0302 19:00:54.000171 22760421793920 run.py:483] Algo bellman_ford step 6894 current loss 0.061878, current_train_items 220640.
I0302 19:00:54.018557 22760421793920 run.py:483] Algo bellman_ford step 6895 current loss 0.012581, current_train_items 220672.
I0302 19:00:54.034231 22760421793920 run.py:483] Algo bellman_ford step 6896 current loss 0.004918, current_train_items 220704.
I0302 19:00:54.057683 22760421793920 run.py:483] Algo bellman_ford step 6897 current loss 0.050413, current_train_items 220736.
I0302 19:00:54.086503 22760421793920 run.py:483] Algo bellman_ford step 6898 current loss 0.035104, current_train_items 220768.
I0302 19:00:54.119077 22760421793920 run.py:483] Algo bellman_ford step 6899 current loss 0.080228, current_train_items 220800.
I0302 19:00:54.137803 22760421793920 run.py:483] Algo bellman_ford step 6900 current loss 0.004831, current_train_items 220832.
I0302 19:00:54.145526 22760421793920 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:00:54.145635 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 19:00:54.161757 22760421793920 run.py:483] Algo bellman_ford step 6901 current loss 0.006676, current_train_items 220864.
I0302 19:00:54.184616 22760421793920 run.py:483] Algo bellman_ford step 6902 current loss 0.055648, current_train_items 220896.
I0302 19:00:54.215789 22760421793920 run.py:483] Algo bellman_ford step 6903 current loss 0.077236, current_train_items 220928.
I0302 19:00:54.249748 22760421793920 run.py:483] Algo bellman_ford step 6904 current loss 0.092779, current_train_items 220960.
I0302 19:00:54.267826 22760421793920 run.py:483] Algo bellman_ford step 6905 current loss 0.004548, current_train_items 220992.
I0302 19:00:54.282990 22760421793920 run.py:483] Algo bellman_ford step 6906 current loss 0.008658, current_train_items 221024.
I0302 19:00:54.305906 22760421793920 run.py:483] Algo bellman_ford step 6907 current loss 0.029318, current_train_items 221056.
I0302 19:00:54.336953 22760421793920 run.py:483] Algo bellman_ford step 6908 current loss 0.063940, current_train_items 221088.
I0302 19:00:54.369916 22760421793920 run.py:483] Algo bellman_ford step 6909 current loss 0.078477, current_train_items 221120.
I0302 19:00:54.388207 22760421793920 run.py:483] Algo bellman_ford step 6910 current loss 0.014147, current_train_items 221152.
I0302 19:00:54.404465 22760421793920 run.py:483] Algo bellman_ford step 6911 current loss 0.036243, current_train_items 221184.
I0302 19:00:54.426240 22760421793920 run.py:483] Algo bellman_ford step 6912 current loss 0.033835, current_train_items 221216.
I0302 19:00:54.456915 22760421793920 run.py:483] Algo bellman_ford step 6913 current loss 0.041827, current_train_items 221248.
I0302 19:00:54.490436 22760421793920 run.py:483] Algo bellman_ford step 6914 current loss 0.082141, current_train_items 221280.
I0302 19:00:54.508476 22760421793920 run.py:483] Algo bellman_ford step 6915 current loss 0.005755, current_train_items 221312.
I0302 19:00:54.523618 22760421793920 run.py:483] Algo bellman_ford step 6916 current loss 0.003585, current_train_items 221344.
I0302 19:00:54.547413 22760421793920 run.py:483] Algo bellman_ford step 6917 current loss 0.058207, current_train_items 221376.
I0302 19:00:54.577839 22760421793920 run.py:483] Algo bellman_ford step 6918 current loss 0.057041, current_train_items 221408.
I0302 19:00:54.611451 22760421793920 run.py:483] Algo bellman_ford step 6919 current loss 0.076875, current_train_items 221440.
I0302 19:00:54.629392 22760421793920 run.py:483] Algo bellman_ford step 6920 current loss 0.004562, current_train_items 221472.
I0302 19:00:54.645058 22760421793920 run.py:483] Algo bellman_ford step 6921 current loss 0.008711, current_train_items 221504.
I0302 19:00:54.667290 22760421793920 run.py:483] Algo bellman_ford step 6922 current loss 0.039220, current_train_items 221536.
I0302 19:00:54.697859 22760421793920 run.py:483] Algo bellman_ford step 6923 current loss 0.094732, current_train_items 221568.
I0302 19:00:54.729709 22760421793920 run.py:483] Algo bellman_ford step 6924 current loss 0.036853, current_train_items 221600.
I0302 19:00:54.747818 22760421793920 run.py:483] Algo bellman_ford step 6925 current loss 0.005448, current_train_items 221632.
I0302 19:00:54.763226 22760421793920 run.py:483] Algo bellman_ford step 6926 current loss 0.008796, current_train_items 221664.
I0302 19:00:54.786344 22760421793920 run.py:483] Algo bellman_ford step 6927 current loss 0.147315, current_train_items 221696.
I0302 19:00:54.816700 22760421793920 run.py:483] Algo bellman_ford step 6928 current loss 0.108433, current_train_items 221728.
I0302 19:00:54.848101 22760421793920 run.py:483] Algo bellman_ford step 6929 current loss 0.080649, current_train_items 221760.
I0302 19:00:54.866682 22760421793920 run.py:483] Algo bellman_ford step 6930 current loss 0.009413, current_train_items 221792.
I0302 19:00:54.882480 22760421793920 run.py:483] Algo bellman_ford step 6931 current loss 0.023234, current_train_items 221824.
I0302 19:00:54.904872 22760421793920 run.py:483] Algo bellman_ford step 6932 current loss 0.052439, current_train_items 221856.
I0302 19:00:54.934850 22760421793920 run.py:483] Algo bellman_ford step 6933 current loss 0.064993, current_train_items 221888.
I0302 19:00:54.967983 22760421793920 run.py:483] Algo bellman_ford step 6934 current loss 0.122360, current_train_items 221920.
I0302 19:00:54.985974 22760421793920 run.py:483] Algo bellman_ford step 6935 current loss 0.022255, current_train_items 221952.
I0302 19:00:55.001648 22760421793920 run.py:483] Algo bellman_ford step 6936 current loss 0.009811, current_train_items 221984.
I0302 19:00:55.024336 22760421793920 run.py:483] Algo bellman_ford step 6937 current loss 0.048710, current_train_items 222016.
I0302 19:00:55.054257 22760421793920 run.py:483] Algo bellman_ford step 6938 current loss 0.067376, current_train_items 222048.
I0302 19:00:55.085007 22760421793920 run.py:483] Algo bellman_ford step 6939 current loss 0.068680, current_train_items 222080.
I0302 19:00:55.103144 22760421793920 run.py:483] Algo bellman_ford step 6940 current loss 0.028925, current_train_items 222112.
I0302 19:00:55.119127 22760421793920 run.py:483] Algo bellman_ford step 6941 current loss 0.015676, current_train_items 222144.
I0302 19:00:55.142519 22760421793920 run.py:483] Algo bellman_ford step 6942 current loss 0.093323, current_train_items 222176.
I0302 19:00:55.172707 22760421793920 run.py:483] Algo bellman_ford step 6943 current loss 0.097131, current_train_items 222208.
I0302 19:00:55.205699 22760421793920 run.py:483] Algo bellman_ford step 6944 current loss 0.110858, current_train_items 222240.
I0302 19:00:55.224200 22760421793920 run.py:483] Algo bellman_ford step 6945 current loss 0.018386, current_train_items 222272.
I0302 19:00:55.240062 22760421793920 run.py:483] Algo bellman_ford step 6946 current loss 0.009369, current_train_items 222304.
I0302 19:00:55.263057 22760421793920 run.py:483] Algo bellman_ford step 6947 current loss 0.066057, current_train_items 222336.
I0302 19:00:55.290942 22760421793920 run.py:483] Algo bellman_ford step 6948 current loss 0.053072, current_train_items 222368.
I0302 19:00:55.323431 22760421793920 run.py:483] Algo bellman_ford step 6949 current loss 0.077161, current_train_items 222400.
I0302 19:00:55.341576 22760421793920 run.py:483] Algo bellman_ford step 6950 current loss 0.002166, current_train_items 222432.
I0302 19:00:55.349010 22760421793920 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:00:55.349120 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:55.365527 22760421793920 run.py:483] Algo bellman_ford step 6951 current loss 0.023254, current_train_items 222464.
I0302 19:00:55.389476 22760421793920 run.py:483] Algo bellman_ford step 6952 current loss 0.067510, current_train_items 222496.
I0302 19:00:55.419259 22760421793920 run.py:483] Algo bellman_ford step 6953 current loss 0.048207, current_train_items 222528.
I0302 19:00:55.452421 22760421793920 run.py:483] Algo bellman_ford step 6954 current loss 0.062832, current_train_items 222560.
I0302 19:00:55.471043 22760421793920 run.py:483] Algo bellman_ford step 6955 current loss 0.026121, current_train_items 222592.
I0302 19:00:55.487196 22760421793920 run.py:483] Algo bellman_ford step 6956 current loss 0.042320, current_train_items 222624.
I0302 19:00:55.511151 22760421793920 run.py:483] Algo bellman_ford step 6957 current loss 0.130655, current_train_items 222656.
I0302 19:00:55.540998 22760421793920 run.py:483] Algo bellman_ford step 6958 current loss 0.077709, current_train_items 222688.
I0302 19:00:55.571292 22760421793920 run.py:483] Algo bellman_ford step 6959 current loss 0.056997, current_train_items 222720.
I0302 19:00:55.589337 22760421793920 run.py:483] Algo bellman_ford step 6960 current loss 0.003331, current_train_items 222752.
I0302 19:00:55.605306 22760421793920 run.py:483] Algo bellman_ford step 6961 current loss 0.020298, current_train_items 222784.
I0302 19:00:55.628288 22760421793920 run.py:483] Algo bellman_ford step 6962 current loss 0.078977, current_train_items 222816.
I0302 19:00:55.658511 22760421793920 run.py:483] Algo bellman_ford step 6963 current loss 0.159047, current_train_items 222848.
I0302 19:00:55.691708 22760421793920 run.py:483] Algo bellman_ford step 6964 current loss 0.103252, current_train_items 222880.
I0302 19:00:55.710367 22760421793920 run.py:483] Algo bellman_ford step 6965 current loss 0.004070, current_train_items 222912.
I0302 19:00:55.726103 22760421793920 run.py:483] Algo bellman_ford step 6966 current loss 0.010215, current_train_items 222944.
I0302 19:00:55.749246 22760421793920 run.py:483] Algo bellman_ford step 6967 current loss 0.045909, current_train_items 222976.
I0302 19:00:55.778640 22760421793920 run.py:483] Algo bellman_ford step 6968 current loss 0.083064, current_train_items 223008.
I0302 19:00:55.810605 22760421793920 run.py:483] Algo bellman_ford step 6969 current loss 0.111916, current_train_items 223040.
I0302 19:00:55.828630 22760421793920 run.py:483] Algo bellman_ford step 6970 current loss 0.001839, current_train_items 223072.
I0302 19:00:55.844394 22760421793920 run.py:483] Algo bellman_ford step 6971 current loss 0.024149, current_train_items 223104.
I0302 19:00:55.867244 22760421793920 run.py:483] Algo bellman_ford step 6972 current loss 0.041721, current_train_items 223136.
I0302 19:00:55.896829 22760421793920 run.py:483] Algo bellman_ford step 6973 current loss 0.038156, current_train_items 223168.
I0302 19:00:55.929436 22760421793920 run.py:483] Algo bellman_ford step 6974 current loss 0.049854, current_train_items 223200.
I0302 19:00:55.947639 22760421793920 run.py:483] Algo bellman_ford step 6975 current loss 0.004865, current_train_items 223232.
I0302 19:00:55.963500 22760421793920 run.py:483] Algo bellman_ford step 6976 current loss 0.014975, current_train_items 223264.
I0302 19:00:55.986301 22760421793920 run.py:483] Algo bellman_ford step 6977 current loss 0.027336, current_train_items 223296.
I0302 19:00:56.015248 22760421793920 run.py:483] Algo bellman_ford step 6978 current loss 0.034827, current_train_items 223328.
I0302 19:00:56.048157 22760421793920 run.py:483] Algo bellman_ford step 6979 current loss 0.043505, current_train_items 223360.
I0302 19:00:56.066719 22760421793920 run.py:483] Algo bellman_ford step 6980 current loss 0.017562, current_train_items 223392.
I0302 19:00:56.082945 22760421793920 run.py:483] Algo bellman_ford step 6981 current loss 0.010814, current_train_items 223424.
I0302 19:00:56.104981 22760421793920 run.py:483] Algo bellman_ford step 6982 current loss 0.028801, current_train_items 223456.
I0302 19:00:56.136083 22760421793920 run.py:483] Algo bellman_ford step 6983 current loss 0.059743, current_train_items 223488.
I0302 19:00:56.168443 22760421793920 run.py:483] Algo bellman_ford step 6984 current loss 0.073267, current_train_items 223520.
I0302 19:00:56.186596 22760421793920 run.py:483] Algo bellman_ford step 6985 current loss 0.006986, current_train_items 223552.
I0302 19:00:56.202119 22760421793920 run.py:483] Algo bellman_ford step 6986 current loss 0.013207, current_train_items 223584.
I0302 19:00:56.224973 22760421793920 run.py:483] Algo bellman_ford step 6987 current loss 0.031808, current_train_items 223616.
I0302 19:00:56.254689 22760421793920 run.py:483] Algo bellman_ford step 6988 current loss 0.043275, current_train_items 223648.
I0302 19:00:56.289016 22760421793920 run.py:483] Algo bellman_ford step 6989 current loss 0.099045, current_train_items 223680.
I0302 19:00:56.307348 22760421793920 run.py:483] Algo bellman_ford step 6990 current loss 0.007259, current_train_items 223712.
I0302 19:00:56.322883 22760421793920 run.py:483] Algo bellman_ford step 6991 current loss 0.022744, current_train_items 223744.
I0302 19:00:56.345947 22760421793920 run.py:483] Algo bellman_ford step 6992 current loss 0.042457, current_train_items 223776.
I0302 19:00:56.376811 22760421793920 run.py:483] Algo bellman_ford step 6993 current loss 0.051576, current_train_items 223808.
I0302 19:00:56.411498 22760421793920 run.py:483] Algo bellman_ford step 6994 current loss 0.067903, current_train_items 223840.
I0302 19:00:56.429828 22760421793920 run.py:483] Algo bellman_ford step 6995 current loss 0.004095, current_train_items 223872.
I0302 19:00:56.445790 22760421793920 run.py:483] Algo bellman_ford step 6996 current loss 0.011672, current_train_items 223904.
I0302 19:00:56.467201 22760421793920 run.py:483] Algo bellman_ford step 6997 current loss 0.054680, current_train_items 223936.
I0302 19:00:56.496307 22760421793920 run.py:483] Algo bellman_ford step 6998 current loss 0.024088, current_train_items 223968.
I0302 19:00:56.528589 22760421793920 run.py:483] Algo bellman_ford step 6999 current loss 0.078100, current_train_items 224000.
I0302 19:00:56.546607 22760421793920 run.py:483] Algo bellman_ford step 7000 current loss 0.011581, current_train_items 224032.
I0302 19:00:56.554207 22760421793920 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:00:56.554314 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:00:56.570448 22760421793920 run.py:483] Algo bellman_ford step 7001 current loss 0.020765, current_train_items 224064.
I0302 19:00:56.592856 22760421793920 run.py:483] Algo bellman_ford step 7002 current loss 0.028856, current_train_items 224096.
I0302 19:00:56.621457 22760421793920 run.py:483] Algo bellman_ford step 7003 current loss 0.032969, current_train_items 224128.
I0302 19:00:56.652929 22760421793920 run.py:483] Algo bellman_ford step 7004 current loss 0.051242, current_train_items 224160.
I0302 19:00:56.671181 22760421793920 run.py:483] Algo bellman_ford step 7005 current loss 0.001586, current_train_items 224192.
I0302 19:00:56.686845 22760421793920 run.py:483] Algo bellman_ford step 7006 current loss 0.035203, current_train_items 224224.
I0302 19:00:56.710480 22760421793920 run.py:483] Algo bellman_ford step 7007 current loss 0.052187, current_train_items 224256.
I0302 19:00:56.739086 22760421793920 run.py:483] Algo bellman_ford step 7008 current loss 0.031141, current_train_items 224288.
I0302 19:00:56.773332 22760421793920 run.py:483] Algo bellman_ford step 7009 current loss 0.065266, current_train_items 224320.
I0302 19:00:56.791640 22760421793920 run.py:483] Algo bellman_ford step 7010 current loss 0.002130, current_train_items 224352.
I0302 19:00:56.807366 22760421793920 run.py:483] Algo bellman_ford step 7011 current loss 0.009272, current_train_items 224384.
I0302 19:00:56.829960 22760421793920 run.py:483] Algo bellman_ford step 7012 current loss 0.075675, current_train_items 224416.
I0302 19:00:56.859233 22760421793920 run.py:483] Algo bellman_ford step 7013 current loss 0.040276, current_train_items 224448.
I0302 19:00:56.891673 22760421793920 run.py:483] Algo bellman_ford step 7014 current loss 0.064758, current_train_items 224480.
I0302 19:00:56.909841 22760421793920 run.py:483] Algo bellman_ford step 7015 current loss 0.004233, current_train_items 224512.
I0302 19:00:56.925395 22760421793920 run.py:483] Algo bellman_ford step 7016 current loss 0.016147, current_train_items 224544.
I0302 19:00:56.948596 22760421793920 run.py:483] Algo bellman_ford step 7017 current loss 0.041234, current_train_items 224576.
I0302 19:00:56.977944 22760421793920 run.py:483] Algo bellman_ford step 7018 current loss 0.069812, current_train_items 224608.
I0302 19:00:57.009141 22760421793920 run.py:483] Algo bellman_ford step 7019 current loss 0.052326, current_train_items 224640.
I0302 19:00:57.027460 22760421793920 run.py:483] Algo bellman_ford step 7020 current loss 0.007786, current_train_items 224672.
I0302 19:00:57.043025 22760421793920 run.py:483] Algo bellman_ford step 7021 current loss 0.036237, current_train_items 224704.
I0302 19:00:57.065921 22760421793920 run.py:483] Algo bellman_ford step 7022 current loss 0.049822, current_train_items 224736.
I0302 19:00:57.095118 22760421793920 run.py:483] Algo bellman_ford step 7023 current loss 0.028278, current_train_items 224768.
I0302 19:00:57.127527 22760421793920 run.py:483] Algo bellman_ford step 7024 current loss 0.095696, current_train_items 224800.
I0302 19:00:57.145912 22760421793920 run.py:483] Algo bellman_ford step 7025 current loss 0.002716, current_train_items 224832.
I0302 19:00:57.161995 22760421793920 run.py:483] Algo bellman_ford step 7026 current loss 0.012880, current_train_items 224864.
I0302 19:00:57.186335 22760421793920 run.py:483] Algo bellman_ford step 7027 current loss 0.062066, current_train_items 224896.
I0302 19:00:57.218068 22760421793920 run.py:483] Algo bellman_ford step 7028 current loss 0.065611, current_train_items 224928.
I0302 19:00:57.248223 22760421793920 run.py:483] Algo bellman_ford step 7029 current loss 0.068672, current_train_items 224960.
I0302 19:00:57.266437 22760421793920 run.py:483] Algo bellman_ford step 7030 current loss 0.001010, current_train_items 224992.
I0302 19:00:57.282131 22760421793920 run.py:483] Algo bellman_ford step 7031 current loss 0.012333, current_train_items 225024.
I0302 19:00:57.305032 22760421793920 run.py:483] Algo bellman_ford step 7032 current loss 0.025973, current_train_items 225056.
I0302 19:00:57.334714 22760421793920 run.py:483] Algo bellman_ford step 7033 current loss 0.058901, current_train_items 225088.
I0302 19:00:57.368452 22760421793920 run.py:483] Algo bellman_ford step 7034 current loss 0.043167, current_train_items 225120.
I0302 19:00:57.386779 22760421793920 run.py:483] Algo bellman_ford step 7035 current loss 0.003510, current_train_items 225152.
I0302 19:00:57.402608 22760421793920 run.py:483] Algo bellman_ford step 7036 current loss 0.044293, current_train_items 225184.
I0302 19:00:57.425337 22760421793920 run.py:483] Algo bellman_ford step 7037 current loss 0.037815, current_train_items 225216.
I0302 19:00:57.456191 22760421793920 run.py:483] Algo bellman_ford step 7038 current loss 0.091798, current_train_items 225248.
I0302 19:00:57.487813 22760421793920 run.py:483] Algo bellman_ford step 7039 current loss 0.050727, current_train_items 225280.
I0302 19:00:57.506072 22760421793920 run.py:483] Algo bellman_ford step 7040 current loss 0.011130, current_train_items 225312.
I0302 19:00:57.521744 22760421793920 run.py:483] Algo bellman_ford step 7041 current loss 0.014938, current_train_items 225344.
I0302 19:00:57.544503 22760421793920 run.py:483] Algo bellman_ford step 7042 current loss 0.040216, current_train_items 225376.
I0302 19:00:57.575257 22760421793920 run.py:483] Algo bellman_ford step 7043 current loss 0.047980, current_train_items 225408.
I0302 19:00:57.607886 22760421793920 run.py:483] Algo bellman_ford step 7044 current loss 0.048790, current_train_items 225440.
I0302 19:00:57.626167 22760421793920 run.py:483] Algo bellman_ford step 7045 current loss 0.001175, current_train_items 225472.
I0302 19:00:57.642142 22760421793920 run.py:483] Algo bellman_ford step 7046 current loss 0.026464, current_train_items 225504.
I0302 19:00:57.664598 22760421793920 run.py:483] Algo bellman_ford step 7047 current loss 0.055333, current_train_items 225536.
I0302 19:00:57.695059 22760421793920 run.py:483] Algo bellman_ford step 7048 current loss 0.083858, current_train_items 225568.
I0302 19:00:57.726658 22760421793920 run.py:483] Algo bellman_ford step 7049 current loss 0.058328, current_train_items 225600.
I0302 19:00:57.745095 22760421793920 run.py:483] Algo bellman_ford step 7050 current loss 0.000819, current_train_items 225632.
I0302 19:00:57.752574 22760421793920 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:00:57.752682 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:00:57.768532 22760421793920 run.py:483] Algo bellman_ford step 7051 current loss 0.005817, current_train_items 225664.
I0302 19:00:57.792360 22760421793920 run.py:483] Algo bellman_ford step 7052 current loss 0.093296, current_train_items 225696.
I0302 19:00:57.823823 22760421793920 run.py:483] Algo bellman_ford step 7053 current loss 0.071975, current_train_items 225728.
I0302 19:00:57.857100 22760421793920 run.py:483] Algo bellman_ford step 7054 current loss 0.071419, current_train_items 225760.
I0302 19:00:57.875278 22760421793920 run.py:483] Algo bellman_ford step 7055 current loss 0.004944, current_train_items 225792.
I0302 19:00:57.891281 22760421793920 run.py:483] Algo bellman_ford step 7056 current loss 0.018076, current_train_items 225824.
I0302 19:00:57.914562 22760421793920 run.py:483] Algo bellman_ford step 7057 current loss 0.124346, current_train_items 225856.
I0302 19:00:57.944628 22760421793920 run.py:483] Algo bellman_ford step 7058 current loss 0.147756, current_train_items 225888.
I0302 19:00:57.978395 22760421793920 run.py:483] Algo bellman_ford step 7059 current loss 0.214681, current_train_items 225920.
I0302 19:00:57.996368 22760421793920 run.py:483] Algo bellman_ford step 7060 current loss 0.012303, current_train_items 225952.
I0302 19:00:58.012020 22760421793920 run.py:483] Algo bellman_ford step 7061 current loss 0.013070, current_train_items 225984.
I0302 19:00:58.035863 22760421793920 run.py:483] Algo bellman_ford step 7062 current loss 0.035125, current_train_items 226016.
I0302 19:00:58.063425 22760421793920 run.py:483] Algo bellman_ford step 7063 current loss 0.044203, current_train_items 226048.
I0302 19:00:58.095164 22760421793920 run.py:483] Algo bellman_ford step 7064 current loss 0.042020, current_train_items 226080.
I0302 19:00:58.113303 22760421793920 run.py:483] Algo bellman_ford step 7065 current loss 0.000805, current_train_items 226112.
I0302 19:00:58.129443 22760421793920 run.py:483] Algo bellman_ford step 7066 current loss 0.015792, current_train_items 226144.
I0302 19:00:58.151931 22760421793920 run.py:483] Algo bellman_ford step 7067 current loss 0.019277, current_train_items 226176.
I0302 19:00:58.183505 22760421793920 run.py:483] Algo bellman_ford step 7068 current loss 0.045236, current_train_items 226208.
I0302 19:00:58.217297 22760421793920 run.py:483] Algo bellman_ford step 7069 current loss 0.058286, current_train_items 226240.
I0302 19:00:58.235299 22760421793920 run.py:483] Algo bellman_ford step 7070 current loss 0.026651, current_train_items 226272.
I0302 19:00:58.250800 22760421793920 run.py:483] Algo bellman_ford step 7071 current loss 0.024267, current_train_items 226304.
I0302 19:00:58.272252 22760421793920 run.py:483] Algo bellman_ford step 7072 current loss 0.028604, current_train_items 226336.
I0302 19:00:58.301209 22760421793920 run.py:483] Algo bellman_ford step 7073 current loss 0.020306, current_train_items 226368.
I0302 19:00:58.333100 22760421793920 run.py:483] Algo bellman_ford step 7074 current loss 0.053725, current_train_items 226400.
I0302 19:00:58.351249 22760421793920 run.py:483] Algo bellman_ford step 7075 current loss 0.002689, current_train_items 226432.
I0302 19:00:58.366218 22760421793920 run.py:483] Algo bellman_ford step 7076 current loss 0.016045, current_train_items 226464.
I0302 19:00:58.388782 22760421793920 run.py:483] Algo bellman_ford step 7077 current loss 0.058275, current_train_items 226496.
I0302 19:00:58.418931 22760421793920 run.py:483] Algo bellman_ford step 7078 current loss 0.052972, current_train_items 226528.
I0302 19:00:58.451448 22760421793920 run.py:483] Algo bellman_ford step 7079 current loss 0.053564, current_train_items 226560.
I0302 19:00:58.469316 22760421793920 run.py:483] Algo bellman_ford step 7080 current loss 0.002480, current_train_items 226592.
I0302 19:00:58.484704 22760421793920 run.py:483] Algo bellman_ford step 7081 current loss 0.039051, current_train_items 226624.
I0302 19:00:58.507234 22760421793920 run.py:483] Algo bellman_ford step 7082 current loss 0.115560, current_train_items 226656.
I0302 19:00:58.536903 22760421793920 run.py:483] Algo bellman_ford step 7083 current loss 0.114772, current_train_items 226688.
I0302 19:00:58.568274 22760421793920 run.py:483] Algo bellman_ford step 7084 current loss 0.159418, current_train_items 226720.
I0302 19:00:58.586701 22760421793920 run.py:483] Algo bellman_ford step 7085 current loss 0.002477, current_train_items 226752.
I0302 19:00:58.602271 22760421793920 run.py:483] Algo bellman_ford step 7086 current loss 0.009098, current_train_items 226784.
I0302 19:00:58.624247 22760421793920 run.py:483] Algo bellman_ford step 7087 current loss 0.011675, current_train_items 226816.
I0302 19:00:58.654318 22760421793920 run.py:483] Algo bellman_ford step 7088 current loss 0.064226, current_train_items 226848.
I0302 19:00:58.685753 22760421793920 run.py:483] Algo bellman_ford step 7089 current loss 0.055213, current_train_items 226880.
I0302 19:00:58.704065 22760421793920 run.py:483] Algo bellman_ford step 7090 current loss 0.006317, current_train_items 226912.
I0302 19:00:58.719399 22760421793920 run.py:483] Algo bellman_ford step 7091 current loss 0.008385, current_train_items 226944.
I0302 19:00:58.742498 22760421793920 run.py:483] Algo bellman_ford step 7092 current loss 0.024830, current_train_items 226976.
I0302 19:00:58.773081 22760421793920 run.py:483] Algo bellman_ford step 7093 current loss 0.066676, current_train_items 227008.
I0302 19:00:58.802865 22760421793920 run.py:483] Algo bellman_ford step 7094 current loss 0.069467, current_train_items 227040.
I0302 19:00:58.821163 22760421793920 run.py:483] Algo bellman_ford step 7095 current loss 0.001465, current_train_items 227072.
I0302 19:00:58.836744 22760421793920 run.py:483] Algo bellman_ford step 7096 current loss 0.008400, current_train_items 227104.
I0302 19:00:58.858354 22760421793920 run.py:483] Algo bellman_ford step 7097 current loss 0.029077, current_train_items 227136.
I0302 19:00:58.888978 22760421793920 run.py:483] Algo bellman_ford step 7098 current loss 0.056431, current_train_items 227168.
I0302 19:00:58.919775 22760421793920 run.py:483] Algo bellman_ford step 7099 current loss 0.049492, current_train_items 227200.
I0302 19:00:58.938101 22760421793920 run.py:483] Algo bellman_ford step 7100 current loss 0.000696, current_train_items 227232.
I0302 19:00:58.945627 22760421793920 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:00:58.945735 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:58.962020 22760421793920 run.py:483] Algo bellman_ford step 7101 current loss 0.024255, current_train_items 227264.
I0302 19:00:58.985003 22760421793920 run.py:483] Algo bellman_ford step 7102 current loss 0.018485, current_train_items 227296.
I0302 19:00:59.015677 22760421793920 run.py:483] Algo bellman_ford step 7103 current loss 0.047222, current_train_items 227328.
I0302 19:00:59.050011 22760421793920 run.py:483] Algo bellman_ford step 7104 current loss 0.069973, current_train_items 227360.
I0302 19:00:59.068726 22760421793920 run.py:483] Algo bellman_ford step 7105 current loss 0.020537, current_train_items 227392.
I0302 19:00:59.084486 22760421793920 run.py:483] Algo bellman_ford step 7106 current loss 0.041477, current_train_items 227424.
I0302 19:00:59.107767 22760421793920 run.py:483] Algo bellman_ford step 7107 current loss 0.034119, current_train_items 227456.
I0302 19:00:59.137606 22760421793920 run.py:483] Algo bellman_ford step 7108 current loss 0.079964, current_train_items 227488.
I0302 19:00:59.171877 22760421793920 run.py:483] Algo bellman_ford step 7109 current loss 0.071552, current_train_items 227520.
I0302 19:00:59.190331 22760421793920 run.py:483] Algo bellman_ford step 7110 current loss 0.003647, current_train_items 227552.
I0302 19:00:59.206161 22760421793920 run.py:483] Algo bellman_ford step 7111 current loss 0.028159, current_train_items 227584.
I0302 19:00:59.229356 22760421793920 run.py:483] Algo bellman_ford step 7112 current loss 0.057666, current_train_items 227616.
I0302 19:00:59.260273 22760421793920 run.py:483] Algo bellman_ford step 7113 current loss 0.053538, current_train_items 227648.
I0302 19:00:59.292424 22760421793920 run.py:483] Algo bellman_ford step 7114 current loss 0.067110, current_train_items 227680.
I0302 19:00:59.310971 22760421793920 run.py:483] Algo bellman_ford step 7115 current loss 0.004034, current_train_items 227712.
I0302 19:00:59.326688 22760421793920 run.py:483] Algo bellman_ford step 7116 current loss 0.039154, current_train_items 227744.
I0302 19:00:59.350361 22760421793920 run.py:483] Algo bellman_ford step 7117 current loss 0.061358, current_train_items 227776.
I0302 19:00:59.380849 22760421793920 run.py:483] Algo bellman_ford step 7118 current loss 0.122049, current_train_items 227808.
I0302 19:00:59.413123 22760421793920 run.py:483] Algo bellman_ford step 7119 current loss 0.090010, current_train_items 227840.
I0302 19:00:59.431170 22760421793920 run.py:483] Algo bellman_ford step 7120 current loss 0.016845, current_train_items 227872.
I0302 19:00:59.447024 22760421793920 run.py:483] Algo bellman_ford step 7121 current loss 0.020297, current_train_items 227904.
I0302 19:00:59.470107 22760421793920 run.py:483] Algo bellman_ford step 7122 current loss 0.058903, current_train_items 227936.
I0302 19:00:59.499464 22760421793920 run.py:483] Algo bellman_ford step 7123 current loss 0.092160, current_train_items 227968.
I0302 19:00:59.532023 22760421793920 run.py:483] Algo bellman_ford step 7124 current loss 0.106990, current_train_items 228000.
I0302 19:00:59.550612 22760421793920 run.py:483] Algo bellman_ford step 7125 current loss 0.002850, current_train_items 228032.
I0302 19:00:59.565781 22760421793920 run.py:483] Algo bellman_ford step 7126 current loss 0.008311, current_train_items 228064.
I0302 19:00:59.588223 22760421793920 run.py:483] Algo bellman_ford step 7127 current loss 0.012394, current_train_items 228096.
I0302 19:00:59.617346 22760421793920 run.py:483] Algo bellman_ford step 7128 current loss 0.089041, current_train_items 228128.
I0302 19:00:59.651068 22760421793920 run.py:483] Algo bellman_ford step 7129 current loss 0.093360, current_train_items 228160.
I0302 19:00:59.669322 22760421793920 run.py:483] Algo bellman_ford step 7130 current loss 0.017183, current_train_items 228192.
I0302 19:00:59.685276 22760421793920 run.py:483] Algo bellman_ford step 7131 current loss 0.016003, current_train_items 228224.
I0302 19:00:59.708826 22760421793920 run.py:483] Algo bellman_ford step 7132 current loss 0.033862, current_train_items 228256.
I0302 19:00:59.738942 22760421793920 run.py:483] Algo bellman_ford step 7133 current loss 0.060389, current_train_items 228288.
I0302 19:00:59.770689 22760421793920 run.py:483] Algo bellman_ford step 7134 current loss 0.050538, current_train_items 228320.
I0302 19:00:59.789046 22760421793920 run.py:483] Algo bellman_ford step 7135 current loss 0.018517, current_train_items 228352.
I0302 19:00:59.804471 22760421793920 run.py:483] Algo bellman_ford step 7136 current loss 0.010611, current_train_items 228384.
I0302 19:00:59.826783 22760421793920 run.py:483] Algo bellman_ford step 7137 current loss 0.027395, current_train_items 228416.
I0302 19:00:59.855120 22760421793920 run.py:483] Algo bellman_ford step 7138 current loss 0.060124, current_train_items 228448.
I0302 19:00:59.889632 22760421793920 run.py:483] Algo bellman_ford step 7139 current loss 0.075086, current_train_items 228480.
I0302 19:00:59.908072 22760421793920 run.py:483] Algo bellman_ford step 7140 current loss 0.001586, current_train_items 228512.
I0302 19:00:59.923527 22760421793920 run.py:483] Algo bellman_ford step 7141 current loss 0.018668, current_train_items 228544.
I0302 19:00:59.946935 22760421793920 run.py:483] Algo bellman_ford step 7142 current loss 0.027924, current_train_items 228576.
I0302 19:00:59.975071 22760421793920 run.py:483] Algo bellman_ford step 7143 current loss 0.020007, current_train_items 228608.
I0302 19:01:00.006470 22760421793920 run.py:483] Algo bellman_ford step 7144 current loss 0.066321, current_train_items 228640.
I0302 19:01:00.024700 22760421793920 run.py:483] Algo bellman_ford step 7145 current loss 0.004015, current_train_items 228672.
I0302 19:01:00.040797 22760421793920 run.py:483] Algo bellman_ford step 7146 current loss 0.010860, current_train_items 228704.
I0302 19:01:00.064610 22760421793920 run.py:483] Algo bellman_ford step 7147 current loss 0.044190, current_train_items 228736.
I0302 19:01:00.093504 22760421793920 run.py:483] Algo bellman_ford step 7148 current loss 0.026707, current_train_items 228768.
I0302 19:01:00.123131 22760421793920 run.py:483] Algo bellman_ford step 7149 current loss 0.048926, current_train_items 228800.
I0302 19:01:00.141428 22760421793920 run.py:483] Algo bellman_ford step 7150 current loss 0.000551, current_train_items 228832.
I0302 19:01:00.148880 22760421793920 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:00.148996 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:00.165668 22760421793920 run.py:483] Algo bellman_ford step 7151 current loss 0.013794, current_train_items 228864.
I0302 19:01:00.189521 22760421793920 run.py:483] Algo bellman_ford step 7152 current loss 0.040041, current_train_items 228896.
I0302 19:01:00.219666 22760421793920 run.py:483] Algo bellman_ford step 7153 current loss 0.049359, current_train_items 228928.
I0302 19:01:00.251063 22760421793920 run.py:483] Algo bellman_ford step 7154 current loss 0.037956, current_train_items 228960.
I0302 19:01:00.269555 22760421793920 run.py:483] Algo bellman_ford step 7155 current loss 0.020426, current_train_items 228992.
I0302 19:01:00.285763 22760421793920 run.py:483] Algo bellman_ford step 7156 current loss 0.037546, current_train_items 229024.
I0302 19:01:00.308149 22760421793920 run.py:483] Algo bellman_ford step 7157 current loss 0.046151, current_train_items 229056.
I0302 19:01:00.337508 22760421793920 run.py:483] Algo bellman_ford step 7158 current loss 0.025522, current_train_items 229088.
I0302 19:01:00.370078 22760421793920 run.py:483] Algo bellman_ford step 7159 current loss 0.055656, current_train_items 229120.
I0302 19:01:00.388104 22760421793920 run.py:483] Algo bellman_ford step 7160 current loss 0.003045, current_train_items 229152.
I0302 19:01:00.403998 22760421793920 run.py:483] Algo bellman_ford step 7161 current loss 0.044519, current_train_items 229184.
W0302 19:01:00.418603 22760421793920 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:06.149198 22760421793920 run.py:483] Algo bellman_ford step 7162 current loss 0.049561, current_train_items 229216.
I0302 19:01:06.180571 22760421793920 run.py:483] Algo bellman_ford step 7163 current loss 0.098060, current_train_items 229248.
I0302 19:01:06.214006 22760421793920 run.py:483] Algo bellman_ford step 7164 current loss 0.081039, current_train_items 229280.
I0302 19:01:06.232685 22760421793920 run.py:483] Algo bellman_ford step 7165 current loss 0.001650, current_train_items 229312.
I0302 19:01:06.248507 22760421793920 run.py:483] Algo bellman_ford step 7166 current loss 0.023714, current_train_items 229344.
I0302 19:01:06.272019 22760421793920 run.py:483] Algo bellman_ford step 7167 current loss 0.047327, current_train_items 229376.
I0302 19:01:06.302582 22760421793920 run.py:483] Algo bellman_ford step 7168 current loss 0.035455, current_train_items 229408.
I0302 19:01:06.332591 22760421793920 run.py:483] Algo bellman_ford step 7169 current loss 0.042682, current_train_items 229440.
I0302 19:01:06.350803 22760421793920 run.py:483] Algo bellman_ford step 7170 current loss 0.001669, current_train_items 229472.
I0302 19:01:06.366788 22760421793920 run.py:483] Algo bellman_ford step 7171 current loss 0.008749, current_train_items 229504.
I0302 19:01:06.390278 22760421793920 run.py:483] Algo bellman_ford step 7172 current loss 0.075378, current_train_items 229536.
I0302 19:01:06.420772 22760421793920 run.py:483] Algo bellman_ford step 7173 current loss 0.036760, current_train_items 229568.
I0302 19:01:06.453664 22760421793920 run.py:483] Algo bellman_ford step 7174 current loss 0.051495, current_train_items 229600.
I0302 19:01:06.472064 22760421793920 run.py:483] Algo bellman_ford step 7175 current loss 0.003011, current_train_items 229632.
I0302 19:01:06.487506 22760421793920 run.py:483] Algo bellman_ford step 7176 current loss 0.010976, current_train_items 229664.
I0302 19:01:06.510973 22760421793920 run.py:483] Algo bellman_ford step 7177 current loss 0.037007, current_train_items 229696.
I0302 19:01:06.543260 22760421793920 run.py:483] Algo bellman_ford step 7178 current loss 0.042250, current_train_items 229728.
I0302 19:01:06.576822 22760421793920 run.py:483] Algo bellman_ford step 7179 current loss 0.054230, current_train_items 229760.
I0302 19:01:06.595213 22760421793920 run.py:483] Algo bellman_ford step 7180 current loss 0.002307, current_train_items 229792.
I0302 19:01:06.610916 22760421793920 run.py:483] Algo bellman_ford step 7181 current loss 0.045465, current_train_items 229824.
I0302 19:01:06.634635 22760421793920 run.py:483] Algo bellman_ford step 7182 current loss 0.104749, current_train_items 229856.
I0302 19:01:06.664432 22760421793920 run.py:483] Algo bellman_ford step 7183 current loss 0.054369, current_train_items 229888.
I0302 19:01:06.699866 22760421793920 run.py:483] Algo bellman_ford step 7184 current loss 0.056842, current_train_items 229920.
I0302 19:01:06.718236 22760421793920 run.py:483] Algo bellman_ford step 7185 current loss 0.003290, current_train_items 229952.
I0302 19:01:06.733889 22760421793920 run.py:483] Algo bellman_ford step 7186 current loss 0.040732, current_train_items 229984.
I0302 19:01:06.758111 22760421793920 run.py:483] Algo bellman_ford step 7187 current loss 0.090415, current_train_items 230016.
I0302 19:01:06.788494 22760421793920 run.py:483] Algo bellman_ford step 7188 current loss 0.117532, current_train_items 230048.
I0302 19:01:06.822184 22760421793920 run.py:483] Algo bellman_ford step 7189 current loss 0.124748, current_train_items 230080.
I0302 19:01:06.840573 22760421793920 run.py:483] Algo bellman_ford step 7190 current loss 0.001920, current_train_items 230112.
I0302 19:01:06.856616 22760421793920 run.py:483] Algo bellman_ford step 7191 current loss 0.007093, current_train_items 230144.
I0302 19:01:06.880239 22760421793920 run.py:483] Algo bellman_ford step 7192 current loss 0.019949, current_train_items 230176.
I0302 19:01:06.910906 22760421793920 run.py:483] Algo bellman_ford step 7193 current loss 0.035481, current_train_items 230208.
I0302 19:01:06.942971 22760421793920 run.py:483] Algo bellman_ford step 7194 current loss 0.064327, current_train_items 230240.
I0302 19:01:06.961607 22760421793920 run.py:483] Algo bellman_ford step 7195 current loss 0.001003, current_train_items 230272.
I0302 19:01:06.977087 22760421793920 run.py:483] Algo bellman_ford step 7196 current loss 0.006670, current_train_items 230304.
I0302 19:01:07.000337 22760421793920 run.py:483] Algo bellman_ford step 7197 current loss 0.023461, current_train_items 230336.
I0302 19:01:07.031152 22760421793920 run.py:483] Algo bellman_ford step 7198 current loss 0.086050, current_train_items 230368.
I0302 19:01:07.063917 22760421793920 run.py:483] Algo bellman_ford step 7199 current loss 0.058870, current_train_items 230400.
I0302 19:01:07.082248 22760421793920 run.py:483] Algo bellman_ford step 7200 current loss 0.001831, current_train_items 230432.
I0302 19:01:07.091237 22760421793920 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:07.091387 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:07.107732 22760421793920 run.py:483] Algo bellman_ford step 7201 current loss 0.043761, current_train_items 230464.
I0302 19:01:07.131079 22760421793920 run.py:483] Algo bellman_ford step 7202 current loss 0.021341, current_train_items 230496.
I0302 19:01:07.161998 22760421793920 run.py:483] Algo bellman_ford step 7203 current loss 0.049426, current_train_items 230528.
I0302 19:01:07.194336 22760421793920 run.py:483] Algo bellman_ford step 7204 current loss 0.036406, current_train_items 230560.
I0302 19:01:07.213104 22760421793920 run.py:483] Algo bellman_ford step 7205 current loss 0.003961, current_train_items 230592.
I0302 19:01:07.228497 22760421793920 run.py:483] Algo bellman_ford step 7206 current loss 0.043691, current_train_items 230624.
I0302 19:01:07.252243 22760421793920 run.py:483] Algo bellman_ford step 7207 current loss 0.046544, current_train_items 230656.
I0302 19:01:07.282414 22760421793920 run.py:483] Algo bellman_ford step 7208 current loss 0.028121, current_train_items 230688.
I0302 19:01:07.315778 22760421793920 run.py:483] Algo bellman_ford step 7209 current loss 0.062943, current_train_items 230720.
I0302 19:01:07.334342 22760421793920 run.py:483] Algo bellman_ford step 7210 current loss 0.001834, current_train_items 230752.
I0302 19:01:07.349891 22760421793920 run.py:483] Algo bellman_ford step 7211 current loss 0.006994, current_train_items 230784.
I0302 19:01:07.372721 22760421793920 run.py:483] Algo bellman_ford step 7212 current loss 0.080497, current_train_items 230816.
I0302 19:01:07.402950 22760421793920 run.py:483] Algo bellman_ford step 7213 current loss 0.072357, current_train_items 230848.
I0302 19:01:07.433501 22760421793920 run.py:483] Algo bellman_ford step 7214 current loss 0.049963, current_train_items 230880.
I0302 19:01:07.451735 22760421793920 run.py:483] Algo bellman_ford step 7215 current loss 0.004053, current_train_items 230912.
I0302 19:01:07.467463 22760421793920 run.py:483] Algo bellman_ford step 7216 current loss 0.029592, current_train_items 230944.
I0302 19:01:07.491032 22760421793920 run.py:483] Algo bellman_ford step 7217 current loss 0.058174, current_train_items 230976.
I0302 19:01:07.521600 22760421793920 run.py:483] Algo bellman_ford step 7218 current loss 0.073040, current_train_items 231008.
I0302 19:01:07.553160 22760421793920 run.py:483] Algo bellman_ford step 7219 current loss 0.053919, current_train_items 231040.
I0302 19:01:07.571660 22760421793920 run.py:483] Algo bellman_ford step 7220 current loss 0.008659, current_train_items 231072.
I0302 19:01:07.587709 22760421793920 run.py:483] Algo bellman_ford step 7221 current loss 0.009620, current_train_items 231104.
I0302 19:01:07.611813 22760421793920 run.py:483] Algo bellman_ford step 7222 current loss 0.037272, current_train_items 231136.
I0302 19:01:07.641836 22760421793920 run.py:483] Algo bellman_ford step 7223 current loss 0.071338, current_train_items 231168.
I0302 19:01:07.674746 22760421793920 run.py:483] Algo bellman_ford step 7224 current loss 0.107603, current_train_items 231200.
I0302 19:01:07.693458 22760421793920 run.py:483] Algo bellman_ford step 7225 current loss 0.001763, current_train_items 231232.
I0302 19:01:07.709313 22760421793920 run.py:483] Algo bellman_ford step 7226 current loss 0.035202, current_train_items 231264.
I0302 19:01:07.734092 22760421793920 run.py:483] Algo bellman_ford step 7227 current loss 0.036108, current_train_items 231296.
I0302 19:01:07.764750 22760421793920 run.py:483] Algo bellman_ford step 7228 current loss 0.028749, current_train_items 231328.
I0302 19:01:07.795365 22760421793920 run.py:483] Algo bellman_ford step 7229 current loss 0.063405, current_train_items 231360.
I0302 19:01:07.813568 22760421793920 run.py:483] Algo bellman_ford step 7230 current loss 0.001417, current_train_items 231392.
I0302 19:01:07.829537 22760421793920 run.py:483] Algo bellman_ford step 7231 current loss 0.021815, current_train_items 231424.
I0302 19:01:07.850826 22760421793920 run.py:483] Algo bellman_ford step 7232 current loss 0.012159, current_train_items 231456.
I0302 19:01:07.882060 22760421793920 run.py:483] Algo bellman_ford step 7233 current loss 0.050526, current_train_items 231488.
I0302 19:01:07.913521 22760421793920 run.py:483] Algo bellman_ford step 7234 current loss 0.039548, current_train_items 231520.
I0302 19:01:07.931863 22760421793920 run.py:483] Algo bellman_ford step 7235 current loss 0.002704, current_train_items 231552.
I0302 19:01:07.948021 22760421793920 run.py:483] Algo bellman_ford step 7236 current loss 0.011687, current_train_items 231584.
I0302 19:01:07.972218 22760421793920 run.py:483] Algo bellman_ford step 7237 current loss 0.053728, current_train_items 231616.
I0302 19:01:08.004078 22760421793920 run.py:483] Algo bellman_ford step 7238 current loss 0.127739, current_train_items 231648.
I0302 19:01:08.036468 22760421793920 run.py:483] Algo bellman_ford step 7239 current loss 0.036482, current_train_items 231680.
I0302 19:01:08.054997 22760421793920 run.py:483] Algo bellman_ford step 7240 current loss 0.012900, current_train_items 231712.
I0302 19:01:08.070613 22760421793920 run.py:483] Algo bellman_ford step 7241 current loss 0.014401, current_train_items 231744.
I0302 19:01:08.094778 22760421793920 run.py:483] Algo bellman_ford step 7242 current loss 0.110806, current_train_items 231776.
I0302 19:01:08.125637 22760421793920 run.py:483] Algo bellman_ford step 7243 current loss 0.073511, current_train_items 231808.
I0302 19:01:08.159678 22760421793920 run.py:483] Algo bellman_ford step 7244 current loss 0.095721, current_train_items 231840.
I0302 19:01:08.177951 22760421793920 run.py:483] Algo bellman_ford step 7245 current loss 0.002032, current_train_items 231872.
I0302 19:01:08.193638 22760421793920 run.py:483] Algo bellman_ford step 7246 current loss 0.007035, current_train_items 231904.
I0302 19:01:08.217521 22760421793920 run.py:483] Algo bellman_ford step 7247 current loss 0.030541, current_train_items 231936.
I0302 19:01:08.249359 22760421793920 run.py:483] Algo bellman_ford step 7248 current loss 0.072332, current_train_items 231968.
I0302 19:01:08.281886 22760421793920 run.py:483] Algo bellman_ford step 7249 current loss 0.083326, current_train_items 232000.
I0302 19:01:08.300533 22760421793920 run.py:483] Algo bellman_ford step 7250 current loss 0.021223, current_train_items 232032.
I0302 19:01:08.308430 22760421793920 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:08.308540 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:08.324991 22760421793920 run.py:483] Algo bellman_ford step 7251 current loss 0.026351, current_train_items 232064.
I0302 19:01:08.348543 22760421793920 run.py:483] Algo bellman_ford step 7252 current loss 0.045221, current_train_items 232096.
I0302 19:01:08.378129 22760421793920 run.py:483] Algo bellman_ford step 7253 current loss 0.029046, current_train_items 232128.
I0302 19:01:08.410887 22760421793920 run.py:483] Algo bellman_ford step 7254 current loss 0.054947, current_train_items 232160.
I0302 19:01:08.429486 22760421793920 run.py:483] Algo bellman_ford step 7255 current loss 0.002062, current_train_items 232192.
I0302 19:01:08.445250 22760421793920 run.py:483] Algo bellman_ford step 7256 current loss 0.012979, current_train_items 232224.
I0302 19:01:08.468258 22760421793920 run.py:483] Algo bellman_ford step 7257 current loss 0.047665, current_train_items 232256.
I0302 19:01:08.500924 22760421793920 run.py:483] Algo bellman_ford step 7258 current loss 0.058563, current_train_items 232288.
I0302 19:01:08.533771 22760421793920 run.py:483] Algo bellman_ford step 7259 current loss 0.056092, current_train_items 232320.
I0302 19:01:08.551998 22760421793920 run.py:483] Algo bellman_ford step 7260 current loss 0.002019, current_train_items 232352.
I0302 19:01:08.567645 22760421793920 run.py:483] Algo bellman_ford step 7261 current loss 0.007788, current_train_items 232384.
I0302 19:01:08.591040 22760421793920 run.py:483] Algo bellman_ford step 7262 current loss 0.021649, current_train_items 232416.
I0302 19:01:08.621818 22760421793920 run.py:483] Algo bellman_ford step 7263 current loss 0.046078, current_train_items 232448.
I0302 19:01:08.653773 22760421793920 run.py:483] Algo bellman_ford step 7264 current loss 0.061256, current_train_items 232480.
I0302 19:01:08.671979 22760421793920 run.py:483] Algo bellman_ford step 7265 current loss 0.003099, current_train_items 232512.
I0302 19:01:08.687659 22760421793920 run.py:483] Algo bellman_ford step 7266 current loss 0.025414, current_train_items 232544.
I0302 19:01:08.710638 22760421793920 run.py:483] Algo bellman_ford step 7267 current loss 0.022699, current_train_items 232576.
I0302 19:01:08.740996 22760421793920 run.py:483] Algo bellman_ford step 7268 current loss 0.028601, current_train_items 232608.
I0302 19:01:08.772233 22760421793920 run.py:483] Algo bellman_ford step 7269 current loss 0.064849, current_train_items 232640.
I0302 19:01:08.790241 22760421793920 run.py:483] Algo bellman_ford step 7270 current loss 0.003530, current_train_items 232672.
I0302 19:01:08.806172 22760421793920 run.py:483] Algo bellman_ford step 7271 current loss 0.027740, current_train_items 232704.
I0302 19:01:08.829424 22760421793920 run.py:483] Algo bellman_ford step 7272 current loss 0.016567, current_train_items 232736.
I0302 19:01:08.859833 22760421793920 run.py:483] Algo bellman_ford step 7273 current loss 0.024609, current_train_items 232768.
I0302 19:01:08.891813 22760421793920 run.py:483] Algo bellman_ford step 7274 current loss 0.069039, current_train_items 232800.
I0302 19:01:08.910375 22760421793920 run.py:483] Algo bellman_ford step 7275 current loss 0.002213, current_train_items 232832.
I0302 19:01:08.926104 22760421793920 run.py:483] Algo bellman_ford step 7276 current loss 0.010222, current_train_items 232864.
I0302 19:01:08.949646 22760421793920 run.py:483] Algo bellman_ford step 7277 current loss 0.039605, current_train_items 232896.
I0302 19:01:08.980698 22760421793920 run.py:483] Algo bellman_ford step 7278 current loss 0.043208, current_train_items 232928.
I0302 19:01:09.012854 22760421793920 run.py:483] Algo bellman_ford step 7279 current loss 0.045923, current_train_items 232960.
I0302 19:01:09.031490 22760421793920 run.py:483] Algo bellman_ford step 7280 current loss 0.020204, current_train_items 232992.
I0302 19:01:09.047577 22760421793920 run.py:483] Algo bellman_ford step 7281 current loss 0.032353, current_train_items 233024.
I0302 19:01:09.070209 22760421793920 run.py:483] Algo bellman_ford step 7282 current loss 0.028101, current_train_items 233056.
I0302 19:01:09.100763 22760421793920 run.py:483] Algo bellman_ford step 7283 current loss 0.053794, current_train_items 233088.
I0302 19:01:09.132112 22760421793920 run.py:483] Algo bellman_ford step 7284 current loss 0.035406, current_train_items 233120.
I0302 19:01:09.150621 22760421793920 run.py:483] Algo bellman_ford step 7285 current loss 0.009678, current_train_items 233152.
I0302 19:01:09.167234 22760421793920 run.py:483] Algo bellman_ford step 7286 current loss 0.033595, current_train_items 233184.
I0302 19:01:09.190338 22760421793920 run.py:483] Algo bellman_ford step 7287 current loss 0.094930, current_train_items 233216.
I0302 19:01:09.220634 22760421793920 run.py:483] Algo bellman_ford step 7288 current loss 0.119299, current_train_items 233248.
I0302 19:01:09.252012 22760421793920 run.py:483] Algo bellman_ford step 7289 current loss 0.089715, current_train_items 233280.
I0302 19:01:09.270414 22760421793920 run.py:483] Algo bellman_ford step 7290 current loss 0.011576, current_train_items 233312.
I0302 19:01:09.286311 22760421793920 run.py:483] Algo bellman_ford step 7291 current loss 0.040137, current_train_items 233344.
I0302 19:01:09.308965 22760421793920 run.py:483] Algo bellman_ford step 7292 current loss 0.015290, current_train_items 233376.
I0302 19:01:09.339839 22760421793920 run.py:483] Algo bellman_ford step 7293 current loss 0.057319, current_train_items 233408.
I0302 19:01:09.370410 22760421793920 run.py:483] Algo bellman_ford step 7294 current loss 0.063438, current_train_items 233440.
I0302 19:01:09.388457 22760421793920 run.py:483] Algo bellman_ford step 7295 current loss 0.009376, current_train_items 233472.
I0302 19:01:09.404350 22760421793920 run.py:483] Algo bellman_ford step 7296 current loss 0.007750, current_train_items 233504.
I0302 19:01:09.427568 22760421793920 run.py:483] Algo bellman_ford step 7297 current loss 0.071655, current_train_items 233536.
I0302 19:01:09.458338 22760421793920 run.py:483] Algo bellman_ford step 7298 current loss 0.029409, current_train_items 233568.
I0302 19:01:09.490103 22760421793920 run.py:483] Algo bellman_ford step 7299 current loss 0.056587, current_train_items 233600.
I0302 19:01:09.508460 22760421793920 run.py:483] Algo bellman_ford step 7300 current loss 0.009771, current_train_items 233632.
I0302 19:01:09.515823 22760421793920 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:09.515941 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 19:01:09.532644 22760421793920 run.py:483] Algo bellman_ford step 7301 current loss 0.025162, current_train_items 233664.
I0302 19:01:09.556208 22760421793920 run.py:483] Algo bellman_ford step 7302 current loss 0.051021, current_train_items 233696.
I0302 19:01:09.586802 22760421793920 run.py:483] Algo bellman_ford step 7303 current loss 0.094939, current_train_items 233728.
I0302 19:01:09.619924 22760421793920 run.py:483] Algo bellman_ford step 7304 current loss 0.084216, current_train_items 233760.
I0302 19:01:09.638554 22760421793920 run.py:483] Algo bellman_ford step 7305 current loss 0.002709, current_train_items 233792.
I0302 19:01:09.654059 22760421793920 run.py:483] Algo bellman_ford step 7306 current loss 0.008684, current_train_items 233824.
I0302 19:01:09.677089 22760421793920 run.py:483] Algo bellman_ford step 7307 current loss 0.030816, current_train_items 233856.
I0302 19:01:09.706434 22760421793920 run.py:483] Algo bellman_ford step 7308 current loss 0.028261, current_train_items 233888.
I0302 19:01:09.739669 22760421793920 run.py:483] Algo bellman_ford step 7309 current loss 0.061378, current_train_items 233920.
I0302 19:01:09.758063 22760421793920 run.py:483] Algo bellman_ford step 7310 current loss 0.004396, current_train_items 233952.
I0302 19:01:09.773600 22760421793920 run.py:483] Algo bellman_ford step 7311 current loss 0.003978, current_train_items 233984.
I0302 19:01:09.798363 22760421793920 run.py:483] Algo bellman_ford step 7312 current loss 0.059184, current_train_items 234016.
I0302 19:01:09.829002 22760421793920 run.py:483] Algo bellman_ford step 7313 current loss 0.037391, current_train_items 234048.
I0302 19:01:09.862003 22760421793920 run.py:483] Algo bellman_ford step 7314 current loss 0.032635, current_train_items 234080.
I0302 19:01:09.880436 22760421793920 run.py:483] Algo bellman_ford step 7315 current loss 0.007221, current_train_items 234112.
I0302 19:01:09.895958 22760421793920 run.py:483] Algo bellman_ford step 7316 current loss 0.007737, current_train_items 234144.
I0302 19:01:09.920018 22760421793920 run.py:483] Algo bellman_ford step 7317 current loss 0.050431, current_train_items 234176.
I0302 19:01:09.950206 22760421793920 run.py:483] Algo bellman_ford step 7318 current loss 0.028385, current_train_items 234208.
I0302 19:01:09.983966 22760421793920 run.py:483] Algo bellman_ford step 7319 current loss 0.090806, current_train_items 234240.
I0302 19:01:10.002433 22760421793920 run.py:483] Algo bellman_ford step 7320 current loss 0.003056, current_train_items 234272.
I0302 19:01:10.018688 22760421793920 run.py:483] Algo bellman_ford step 7321 current loss 0.041034, current_train_items 234304.
I0302 19:01:10.043500 22760421793920 run.py:483] Algo bellman_ford step 7322 current loss 0.043314, current_train_items 234336.
I0302 19:01:10.073694 22760421793920 run.py:483] Algo bellman_ford step 7323 current loss 0.037802, current_train_items 234368.
I0302 19:01:10.107150 22760421793920 run.py:483] Algo bellman_ford step 7324 current loss 0.056154, current_train_items 234400.
I0302 19:01:10.125528 22760421793920 run.py:483] Algo bellman_ford step 7325 current loss 0.003529, current_train_items 234432.
I0302 19:01:10.141612 22760421793920 run.py:483] Algo bellman_ford step 7326 current loss 0.021855, current_train_items 234464.
I0302 19:01:10.165005 22760421793920 run.py:483] Algo bellman_ford step 7327 current loss 0.058793, current_train_items 234496.
I0302 19:01:10.194962 22760421793920 run.py:483] Algo bellman_ford step 7328 current loss 0.061297, current_train_items 234528.
I0302 19:01:10.227677 22760421793920 run.py:483] Algo bellman_ford step 7329 current loss 0.056876, current_train_items 234560.
I0302 19:01:10.245741 22760421793920 run.py:483] Algo bellman_ford step 7330 current loss 0.003677, current_train_items 234592.
I0302 19:01:10.261785 22760421793920 run.py:483] Algo bellman_ford step 7331 current loss 0.063749, current_train_items 234624.
I0302 19:01:10.286027 22760421793920 run.py:483] Algo bellman_ford step 7332 current loss 0.076159, current_train_items 234656.
I0302 19:01:10.317196 22760421793920 run.py:483] Algo bellman_ford step 7333 current loss 0.071761, current_train_items 234688.
I0302 19:01:10.347760 22760421793920 run.py:483] Algo bellman_ford step 7334 current loss 0.049950, current_train_items 234720.
I0302 19:01:10.366092 22760421793920 run.py:483] Algo bellman_ford step 7335 current loss 0.001963, current_train_items 234752.
I0302 19:01:10.381539 22760421793920 run.py:483] Algo bellman_ford step 7336 current loss 0.017679, current_train_items 234784.
I0302 19:01:10.404855 22760421793920 run.py:483] Algo bellman_ford step 7337 current loss 0.073550, current_train_items 234816.
I0302 19:01:10.435461 22760421793920 run.py:483] Algo bellman_ford step 7338 current loss 0.073676, current_train_items 234848.
I0302 19:01:10.469264 22760421793920 run.py:483] Algo bellman_ford step 7339 current loss 0.095298, current_train_items 234880.
I0302 19:01:10.487516 22760421793920 run.py:483] Algo bellman_ford step 7340 current loss 0.002354, current_train_items 234912.
I0302 19:01:10.503222 22760421793920 run.py:483] Algo bellman_ford step 7341 current loss 0.021293, current_train_items 234944.
I0302 19:01:10.527446 22760421793920 run.py:483] Algo bellman_ford step 7342 current loss 0.051718, current_train_items 234976.
I0302 19:01:10.558741 22760421793920 run.py:483] Algo bellman_ford step 7343 current loss 0.039378, current_train_items 235008.
I0302 19:01:10.590280 22760421793920 run.py:483] Algo bellman_ford step 7344 current loss 0.056822, current_train_items 235040.
I0302 19:01:10.608852 22760421793920 run.py:483] Algo bellman_ford step 7345 current loss 0.007049, current_train_items 235072.
I0302 19:01:10.624319 22760421793920 run.py:483] Algo bellman_ford step 7346 current loss 0.008080, current_train_items 235104.
I0302 19:01:10.648133 22760421793920 run.py:483] Algo bellman_ford step 7347 current loss 0.041689, current_train_items 235136.
I0302 19:01:10.679116 22760421793920 run.py:483] Algo bellman_ford step 7348 current loss 0.036807, current_train_items 235168.
I0302 19:01:10.708456 22760421793920 run.py:483] Algo bellman_ford step 7349 current loss 0.023277, current_train_items 235200.
I0302 19:01:10.726795 22760421793920 run.py:483] Algo bellman_ford step 7350 current loss 0.001729, current_train_items 235232.
I0302 19:01:10.734320 22760421793920 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:10.734432 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:10.751323 22760421793920 run.py:483] Algo bellman_ford step 7351 current loss 0.031883, current_train_items 235264.
I0302 19:01:10.775877 22760421793920 run.py:483] Algo bellman_ford step 7352 current loss 0.029113, current_train_items 235296.
I0302 19:01:10.807468 22760421793920 run.py:483] Algo bellman_ford step 7353 current loss 0.036209, current_train_items 235328.
I0302 19:01:10.839459 22760421793920 run.py:483] Algo bellman_ford step 7354 current loss 0.048413, current_train_items 235360.
I0302 19:01:10.858059 22760421793920 run.py:483] Algo bellman_ford step 7355 current loss 0.005381, current_train_items 235392.
I0302 19:01:10.873873 22760421793920 run.py:483] Algo bellman_ford step 7356 current loss 0.007437, current_train_items 235424.
I0302 19:01:10.898155 22760421793920 run.py:483] Algo bellman_ford step 7357 current loss 0.046382, current_train_items 235456.
I0302 19:01:10.928605 22760421793920 run.py:483] Algo bellman_ford step 7358 current loss 0.046010, current_train_items 235488.
I0302 19:01:10.960914 22760421793920 run.py:483] Algo bellman_ford step 7359 current loss 0.057311, current_train_items 235520.
I0302 19:01:10.979364 22760421793920 run.py:483] Algo bellman_ford step 7360 current loss 0.002448, current_train_items 235552.
I0302 19:01:10.995500 22760421793920 run.py:483] Algo bellman_ford step 7361 current loss 0.034457, current_train_items 235584.
I0302 19:01:11.018219 22760421793920 run.py:483] Algo bellman_ford step 7362 current loss 0.031470, current_train_items 235616.
I0302 19:01:11.050784 22760421793920 run.py:483] Algo bellman_ford step 7363 current loss 0.059275, current_train_items 235648.
I0302 19:01:11.081435 22760421793920 run.py:483] Algo bellman_ford step 7364 current loss 0.053843, current_train_items 235680.
I0302 19:01:11.099904 22760421793920 run.py:483] Algo bellman_ford step 7365 current loss 0.016765, current_train_items 235712.
I0302 19:01:11.115708 22760421793920 run.py:483] Algo bellman_ford step 7366 current loss 0.022250, current_train_items 235744.
I0302 19:01:11.139951 22760421793920 run.py:483] Algo bellman_ford step 7367 current loss 0.033746, current_train_items 235776.
I0302 19:01:11.172662 22760421793920 run.py:483] Algo bellman_ford step 7368 current loss 0.032078, current_train_items 235808.
I0302 19:01:11.203603 22760421793920 run.py:483] Algo bellman_ford step 7369 current loss 0.030829, current_train_items 235840.
I0302 19:01:11.221823 22760421793920 run.py:483] Algo bellman_ford step 7370 current loss 0.002285, current_train_items 235872.
I0302 19:01:11.237627 22760421793920 run.py:483] Algo bellman_ford step 7371 current loss 0.031372, current_train_items 235904.
I0302 19:01:11.260053 22760421793920 run.py:483] Algo bellman_ford step 7372 current loss 0.016822, current_train_items 235936.
I0302 19:01:11.290512 22760421793920 run.py:483] Algo bellman_ford step 7373 current loss 0.028747, current_train_items 235968.
I0302 19:01:11.322259 22760421793920 run.py:483] Algo bellman_ford step 7374 current loss 0.067749, current_train_items 236000.
I0302 19:01:11.340961 22760421793920 run.py:483] Algo bellman_ford step 7375 current loss 0.001006, current_train_items 236032.
I0302 19:01:11.357356 22760421793920 run.py:483] Algo bellman_ford step 7376 current loss 0.050542, current_train_items 236064.
I0302 19:01:11.381398 22760421793920 run.py:483] Algo bellman_ford step 7377 current loss 0.029302, current_train_items 236096.
I0302 19:01:11.411910 22760421793920 run.py:483] Algo bellman_ford step 7378 current loss 0.051690, current_train_items 236128.
I0302 19:01:11.442702 22760421793920 run.py:483] Algo bellman_ford step 7379 current loss 0.059012, current_train_items 236160.
I0302 19:01:11.460918 22760421793920 run.py:483] Algo bellman_ford step 7380 current loss 0.000486, current_train_items 236192.
I0302 19:01:11.476358 22760421793920 run.py:483] Algo bellman_ford step 7381 current loss 0.025273, current_train_items 236224.
I0302 19:01:11.500555 22760421793920 run.py:483] Algo bellman_ford step 7382 current loss 0.051470, current_train_items 236256.
I0302 19:01:11.530391 22760421793920 run.py:483] Algo bellman_ford step 7383 current loss 0.033892, current_train_items 236288.
I0302 19:01:11.561392 22760421793920 run.py:483] Algo bellman_ford step 7384 current loss 0.060451, current_train_items 236320.
I0302 19:01:11.579637 22760421793920 run.py:483] Algo bellman_ford step 7385 current loss 0.000640, current_train_items 236352.
I0302 19:01:11.595485 22760421793920 run.py:483] Algo bellman_ford step 7386 current loss 0.034231, current_train_items 236384.
I0302 19:01:11.619248 22760421793920 run.py:483] Algo bellman_ford step 7387 current loss 0.022437, current_train_items 236416.
I0302 19:01:11.650433 22760421793920 run.py:483] Algo bellman_ford step 7388 current loss 0.030673, current_train_items 236448.
I0302 19:01:11.681484 22760421793920 run.py:483] Algo bellman_ford step 7389 current loss 0.036140, current_train_items 236480.
I0302 19:01:11.699938 22760421793920 run.py:483] Algo bellman_ford step 7390 current loss 0.003639, current_train_items 236512.
I0302 19:01:11.716087 22760421793920 run.py:483] Algo bellman_ford step 7391 current loss 0.017325, current_train_items 236544.
I0302 19:01:11.739585 22760421793920 run.py:483] Algo bellman_ford step 7392 current loss 0.148816, current_train_items 236576.
I0302 19:01:11.769053 22760421793920 run.py:483] Algo bellman_ford step 7393 current loss 0.223813, current_train_items 236608.
I0302 19:01:11.801574 22760421793920 run.py:483] Algo bellman_ford step 7394 current loss 0.375701, current_train_items 236640.
I0302 19:01:11.819718 22760421793920 run.py:483] Algo bellman_ford step 7395 current loss 0.003353, current_train_items 236672.
I0302 19:01:11.835826 22760421793920 run.py:483] Algo bellman_ford step 7396 current loss 0.035120, current_train_items 236704.
I0302 19:01:11.859718 22760421793920 run.py:483] Algo bellman_ford step 7397 current loss 0.098423, current_train_items 236736.
I0302 19:01:11.889363 22760421793920 run.py:483] Algo bellman_ford step 7398 current loss 0.089862, current_train_items 236768.
I0302 19:01:11.920464 22760421793920 run.py:483] Algo bellman_ford step 7399 current loss 0.083046, current_train_items 236800.
I0302 19:01:11.938716 22760421793920 run.py:483] Algo bellman_ford step 7400 current loss 0.008333, current_train_items 236832.
I0302 19:01:11.946210 22760421793920 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:11.946323 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:01:11.962883 22760421793920 run.py:483] Algo bellman_ford step 7401 current loss 0.011793, current_train_items 236864.
I0302 19:01:11.987194 22760421793920 run.py:483] Algo bellman_ford step 7402 current loss 0.068722, current_train_items 236896.
I0302 19:01:12.018700 22760421793920 run.py:483] Algo bellman_ford step 7403 current loss 0.106478, current_train_items 236928.
I0302 19:01:12.051568 22760421793920 run.py:483] Algo bellman_ford step 7404 current loss 0.088781, current_train_items 236960.
I0302 19:01:12.070162 22760421793920 run.py:483] Algo bellman_ford step 7405 current loss 0.003088, current_train_items 236992.
I0302 19:01:12.085424 22760421793920 run.py:483] Algo bellman_ford step 7406 current loss 0.014515, current_train_items 237024.
I0302 19:01:12.108987 22760421793920 run.py:483] Algo bellman_ford step 7407 current loss 0.052945, current_train_items 237056.
I0302 19:01:12.139849 22760421793920 run.py:483] Algo bellman_ford step 7408 current loss 0.111755, current_train_items 237088.
I0302 19:01:12.170228 22760421793920 run.py:483] Algo bellman_ford step 7409 current loss 0.095908, current_train_items 237120.
I0302 19:01:12.188702 22760421793920 run.py:483] Algo bellman_ford step 7410 current loss 0.009547, current_train_items 237152.
I0302 19:01:12.205049 22760421793920 run.py:483] Algo bellman_ford step 7411 current loss 0.014694, current_train_items 237184.
I0302 19:01:12.229588 22760421793920 run.py:483] Algo bellman_ford step 7412 current loss 0.039299, current_train_items 237216.
I0302 19:01:12.260534 22760421793920 run.py:483] Algo bellman_ford step 7413 current loss 0.066593, current_train_items 237248.
I0302 19:01:12.291285 22760421793920 run.py:483] Algo bellman_ford step 7414 current loss 0.058860, current_train_items 237280.
I0302 19:01:12.309455 22760421793920 run.py:483] Algo bellman_ford step 7415 current loss 0.001635, current_train_items 237312.
I0302 19:01:12.325061 22760421793920 run.py:483] Algo bellman_ford step 7416 current loss 0.005355, current_train_items 237344.
I0302 19:01:12.348439 22760421793920 run.py:483] Algo bellman_ford step 7417 current loss 0.053056, current_train_items 237376.
I0302 19:01:12.378626 22760421793920 run.py:483] Algo bellman_ford step 7418 current loss 0.050362, current_train_items 237408.
I0302 19:01:12.409986 22760421793920 run.py:483] Algo bellman_ford step 7419 current loss 0.051027, current_train_items 237440.
I0302 19:01:12.428295 22760421793920 run.py:483] Algo bellman_ford step 7420 current loss 0.006290, current_train_items 237472.
I0302 19:01:12.443817 22760421793920 run.py:483] Algo bellman_ford step 7421 current loss 0.016082, current_train_items 237504.
I0302 19:01:12.466902 22760421793920 run.py:483] Algo bellman_ford step 7422 current loss 0.051633, current_train_items 237536.
I0302 19:01:12.496921 22760421793920 run.py:483] Algo bellman_ford step 7423 current loss 0.062844, current_train_items 237568.
I0302 19:01:12.528729 22760421793920 run.py:483] Algo bellman_ford step 7424 current loss 0.034603, current_train_items 237600.
I0302 19:01:12.547046 22760421793920 run.py:483] Algo bellman_ford step 7425 current loss 0.000833, current_train_items 237632.
I0302 19:01:12.562624 22760421793920 run.py:483] Algo bellman_ford step 7426 current loss 0.011132, current_train_items 237664.
I0302 19:01:12.586563 22760421793920 run.py:483] Algo bellman_ford step 7427 current loss 0.037101, current_train_items 237696.
I0302 19:01:12.616471 22760421793920 run.py:483] Algo bellman_ford step 7428 current loss 0.032152, current_train_items 237728.
I0302 19:01:12.648677 22760421793920 run.py:483] Algo bellman_ford step 7429 current loss 0.048808, current_train_items 237760.
I0302 19:01:12.666804 22760421793920 run.py:483] Algo bellman_ford step 7430 current loss 0.000533, current_train_items 237792.
I0302 19:01:12.682272 22760421793920 run.py:483] Algo bellman_ford step 7431 current loss 0.011132, current_train_items 237824.
I0302 19:01:12.706236 22760421793920 run.py:483] Algo bellman_ford step 7432 current loss 0.089565, current_train_items 237856.
I0302 19:01:12.736953 22760421793920 run.py:483] Algo bellman_ford step 7433 current loss 0.077015, current_train_items 237888.
I0302 19:01:12.766594 22760421793920 run.py:483] Algo bellman_ford step 7434 current loss 0.037418, current_train_items 237920.
I0302 19:01:12.784997 22760421793920 run.py:483] Algo bellman_ford step 7435 current loss 0.000781, current_train_items 237952.
I0302 19:01:12.800693 22760421793920 run.py:483] Algo bellman_ford step 7436 current loss 0.009823, current_train_items 237984.
I0302 19:01:12.824707 22760421793920 run.py:483] Algo bellman_ford step 7437 current loss 0.099352, current_train_items 238016.
I0302 19:01:12.854637 22760421793920 run.py:483] Algo bellman_ford step 7438 current loss 0.055668, current_train_items 238048.
I0302 19:01:12.887474 22760421793920 run.py:483] Algo bellman_ford step 7439 current loss 0.071734, current_train_items 238080.
I0302 19:01:12.905742 22760421793920 run.py:483] Algo bellman_ford step 7440 current loss 0.001787, current_train_items 238112.
I0302 19:01:12.921846 22760421793920 run.py:483] Algo bellman_ford step 7441 current loss 0.028510, current_train_items 238144.
I0302 19:01:12.944521 22760421793920 run.py:483] Algo bellman_ford step 7442 current loss 0.022469, current_train_items 238176.
I0302 19:01:12.975743 22760421793920 run.py:483] Algo bellman_ford step 7443 current loss 0.079053, current_train_items 238208.
I0302 19:01:13.008423 22760421793920 run.py:483] Algo bellman_ford step 7444 current loss 0.062677, current_train_items 238240.
I0302 19:01:13.026603 22760421793920 run.py:483] Algo bellman_ford step 7445 current loss 0.001085, current_train_items 238272.
I0302 19:01:13.042355 22760421793920 run.py:483] Algo bellman_ford step 7446 current loss 0.017982, current_train_items 238304.
I0302 19:01:13.065444 22760421793920 run.py:483] Algo bellman_ford step 7447 current loss 0.029372, current_train_items 238336.
I0302 19:01:13.095738 22760421793920 run.py:483] Algo bellman_ford step 7448 current loss 0.070191, current_train_items 238368.
I0302 19:01:13.129251 22760421793920 run.py:483] Algo bellman_ford step 7449 current loss 0.150576, current_train_items 238400.
I0302 19:01:13.147443 22760421793920 run.py:483] Algo bellman_ford step 7450 current loss 0.003112, current_train_items 238432.
I0302 19:01:13.155105 22760421793920 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:13.155214 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:01:13.171491 22760421793920 run.py:483] Algo bellman_ford step 7451 current loss 0.010943, current_train_items 238464.
I0302 19:01:13.196049 22760421793920 run.py:483] Algo bellman_ford step 7452 current loss 0.039801, current_train_items 238496.
I0302 19:01:13.225948 22760421793920 run.py:483] Algo bellman_ford step 7453 current loss 0.030657, current_train_items 238528.
I0302 19:01:13.257123 22760421793920 run.py:483] Algo bellman_ford step 7454 current loss 0.042652, current_train_items 238560.
I0302 19:01:13.275392 22760421793920 run.py:483] Algo bellman_ford step 7455 current loss 0.019336, current_train_items 238592.
I0302 19:01:13.291911 22760421793920 run.py:483] Algo bellman_ford step 7456 current loss 0.013404, current_train_items 238624.
I0302 19:01:13.314538 22760421793920 run.py:483] Algo bellman_ford step 7457 current loss 0.026010, current_train_items 238656.
I0302 19:01:13.345445 22760421793920 run.py:483] Algo bellman_ford step 7458 current loss 0.060530, current_train_items 238688.
I0302 19:01:13.377354 22760421793920 run.py:483] Algo bellman_ford step 7459 current loss 0.040159, current_train_items 238720.
I0302 19:01:13.395586 22760421793920 run.py:483] Algo bellman_ford step 7460 current loss 0.002868, current_train_items 238752.
I0302 19:01:13.411920 22760421793920 run.py:483] Algo bellman_ford step 7461 current loss 0.024123, current_train_items 238784.
I0302 19:01:13.434720 22760421793920 run.py:483] Algo bellman_ford step 7462 current loss 0.033974, current_train_items 238816.
I0302 19:01:13.464431 22760421793920 run.py:483] Algo bellman_ford step 7463 current loss 0.037624, current_train_items 238848.
I0302 19:01:13.496194 22760421793920 run.py:483] Algo bellman_ford step 7464 current loss 0.085503, current_train_items 238880.
I0302 19:01:13.514461 22760421793920 run.py:483] Algo bellman_ford step 7465 current loss 0.001578, current_train_items 238912.
I0302 19:01:13.530104 22760421793920 run.py:483] Algo bellman_ford step 7466 current loss 0.002126, current_train_items 238944.
I0302 19:01:13.552812 22760421793920 run.py:483] Algo bellman_ford step 7467 current loss 0.055617, current_train_items 238976.
I0302 19:01:13.584075 22760421793920 run.py:483] Algo bellman_ford step 7468 current loss 0.099978, current_train_items 239008.
I0302 19:01:13.614339 22760421793920 run.py:483] Algo bellman_ford step 7469 current loss 0.064032, current_train_items 239040.
I0302 19:01:13.632568 22760421793920 run.py:483] Algo bellman_ford step 7470 current loss 0.003999, current_train_items 239072.
I0302 19:01:13.648303 22760421793920 run.py:483] Algo bellman_ford step 7471 current loss 0.040441, current_train_items 239104.
I0302 19:01:13.670799 22760421793920 run.py:483] Algo bellman_ford step 7472 current loss 0.045417, current_train_items 239136.
I0302 19:01:13.701270 22760421793920 run.py:483] Algo bellman_ford step 7473 current loss 0.041185, current_train_items 239168.
I0302 19:01:13.734605 22760421793920 run.py:483] Algo bellman_ford step 7474 current loss 0.031908, current_train_items 239200.
I0302 19:01:13.752720 22760421793920 run.py:483] Algo bellman_ford step 7475 current loss 0.001514, current_train_items 239232.
I0302 19:01:13.768626 22760421793920 run.py:483] Algo bellman_ford step 7476 current loss 0.005529, current_train_items 239264.
I0302 19:01:13.790979 22760421793920 run.py:483] Algo bellman_ford step 7477 current loss 0.040353, current_train_items 239296.
I0302 19:01:13.821612 22760421793920 run.py:483] Algo bellman_ford step 7478 current loss 0.023594, current_train_items 239328.
I0302 19:01:13.856255 22760421793920 run.py:483] Algo bellman_ford step 7479 current loss 0.093789, current_train_items 239360.
I0302 19:01:13.874614 22760421793920 run.py:483] Algo bellman_ford step 7480 current loss 0.001905, current_train_items 239392.
I0302 19:01:13.890566 22760421793920 run.py:483] Algo bellman_ford step 7481 current loss 0.020992, current_train_items 239424.
I0302 19:01:13.914935 22760421793920 run.py:483] Algo bellman_ford step 7482 current loss 0.074365, current_train_items 239456.
I0302 19:01:13.946155 22760421793920 run.py:483] Algo bellman_ford step 7483 current loss 0.051738, current_train_items 239488.
I0302 19:01:13.978638 22760421793920 run.py:483] Algo bellman_ford step 7484 current loss 0.080927, current_train_items 239520.
I0302 19:01:13.997042 22760421793920 run.py:483] Algo bellman_ford step 7485 current loss 0.002007, current_train_items 239552.
I0302 19:01:14.012686 22760421793920 run.py:483] Algo bellman_ford step 7486 current loss 0.019563, current_train_items 239584.
I0302 19:01:14.035326 22760421793920 run.py:483] Algo bellman_ford step 7487 current loss 0.072458, current_train_items 239616.
I0302 19:01:14.066049 22760421793920 run.py:483] Algo bellman_ford step 7488 current loss 0.106062, current_train_items 239648.
I0302 19:01:14.099637 22760421793920 run.py:483] Algo bellman_ford step 7489 current loss 0.067097, current_train_items 239680.
I0302 19:01:14.117618 22760421793920 run.py:483] Algo bellman_ford step 7490 current loss 0.004118, current_train_items 239712.
I0302 19:01:14.132948 22760421793920 run.py:483] Algo bellman_ford step 7491 current loss 0.051527, current_train_items 239744.
I0302 19:01:14.155596 22760421793920 run.py:483] Algo bellman_ford step 7492 current loss 0.034673, current_train_items 239776.
I0302 19:01:14.185509 22760421793920 run.py:483] Algo bellman_ford step 7493 current loss 0.052136, current_train_items 239808.
I0302 19:01:14.216681 22760421793920 run.py:483] Algo bellman_ford step 7494 current loss 0.095877, current_train_items 239840.
I0302 19:01:14.234819 22760421793920 run.py:483] Algo bellman_ford step 7495 current loss 0.001805, current_train_items 239872.
I0302 19:01:14.250828 22760421793920 run.py:483] Algo bellman_ford step 7496 current loss 0.013859, current_train_items 239904.
I0302 19:01:14.274024 22760421793920 run.py:483] Algo bellman_ford step 7497 current loss 0.054033, current_train_items 239936.
I0302 19:01:14.303650 22760421793920 run.py:483] Algo bellman_ford step 7498 current loss 0.083198, current_train_items 239968.
I0302 19:01:14.334941 22760421793920 run.py:483] Algo bellman_ford step 7499 current loss 0.078974, current_train_items 240000.
I0302 19:01:14.353399 22760421793920 run.py:483] Algo bellman_ford step 7500 current loss 0.003875, current_train_items 240032.
I0302 19:01:14.360856 22760421793920 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:14.360976 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:01:14.377183 22760421793920 run.py:483] Algo bellman_ford step 7501 current loss 0.011651, current_train_items 240064.
I0302 19:01:14.400571 22760421793920 run.py:483] Algo bellman_ford step 7502 current loss 0.038860, current_train_items 240096.
I0302 19:01:14.431315 22760421793920 run.py:483] Algo bellman_ford step 7503 current loss 0.028497, current_train_items 240128.
I0302 19:01:14.463273 22760421793920 run.py:483] Algo bellman_ford step 7504 current loss 0.049244, current_train_items 240160.
I0302 19:01:14.482088 22760421793920 run.py:483] Algo bellman_ford step 7505 current loss 0.010924, current_train_items 240192.
I0302 19:01:14.497805 22760421793920 run.py:483] Algo bellman_ford step 7506 current loss 0.008954, current_train_items 240224.
I0302 19:01:14.520737 22760421793920 run.py:483] Algo bellman_ford step 7507 current loss 0.033084, current_train_items 240256.
I0302 19:01:14.550858 22760421793920 run.py:483] Algo bellman_ford step 7508 current loss 0.027126, current_train_items 240288.
I0302 19:01:14.582188 22760421793920 run.py:483] Algo bellman_ford step 7509 current loss 0.045483, current_train_items 240320.
I0302 19:01:14.600493 22760421793920 run.py:483] Algo bellman_ford step 7510 current loss 0.005155, current_train_items 240352.
I0302 19:01:14.616557 22760421793920 run.py:483] Algo bellman_ford step 7511 current loss 0.029483, current_train_items 240384.
I0302 19:01:14.639100 22760421793920 run.py:483] Algo bellman_ford step 7512 current loss 0.026481, current_train_items 240416.
I0302 19:01:14.669166 22760421793920 run.py:483] Algo bellman_ford step 7513 current loss 0.043780, current_train_items 240448.
I0302 19:01:14.700026 22760421793920 run.py:483] Algo bellman_ford step 7514 current loss 0.061168, current_train_items 240480.
I0302 19:01:14.718490 22760421793920 run.py:483] Algo bellman_ford step 7515 current loss 0.007899, current_train_items 240512.
I0302 19:01:14.733572 22760421793920 run.py:483] Algo bellman_ford step 7516 current loss 0.032580, current_train_items 240544.
I0302 19:01:14.757527 22760421793920 run.py:483] Algo bellman_ford step 7517 current loss 0.057378, current_train_items 240576.
I0302 19:01:14.788414 22760421793920 run.py:483] Algo bellman_ford step 7518 current loss 0.044337, current_train_items 240608.
I0302 19:01:14.820872 22760421793920 run.py:483] Algo bellman_ford step 7519 current loss 0.069923, current_train_items 240640.
I0302 19:01:14.839425 22760421793920 run.py:483] Algo bellman_ford step 7520 current loss 0.005375, current_train_items 240672.
I0302 19:01:14.855147 22760421793920 run.py:483] Algo bellman_ford step 7521 current loss 0.040578, current_train_items 240704.
I0302 19:01:14.878313 22760421793920 run.py:483] Algo bellman_ford step 7522 current loss 0.045114, current_train_items 240736.
I0302 19:01:14.908884 22760421793920 run.py:483] Algo bellman_ford step 7523 current loss 0.081975, current_train_items 240768.
I0302 19:01:14.938338 22760421793920 run.py:483] Algo bellman_ford step 7524 current loss 0.062094, current_train_items 240800.
I0302 19:01:14.956606 22760421793920 run.py:483] Algo bellman_ford step 7525 current loss 0.019146, current_train_items 240832.
I0302 19:01:14.972285 22760421793920 run.py:483] Algo bellman_ford step 7526 current loss 0.023415, current_train_items 240864.
I0302 19:01:14.996542 22760421793920 run.py:483] Algo bellman_ford step 7527 current loss 0.079153, current_train_items 240896.
I0302 19:01:15.026094 22760421793920 run.py:483] Algo bellman_ford step 7528 current loss 0.088508, current_train_items 240928.
I0302 19:01:15.057833 22760421793920 run.py:483] Algo bellman_ford step 7529 current loss 0.112941, current_train_items 240960.
I0302 19:01:15.076141 22760421793920 run.py:483] Algo bellman_ford step 7530 current loss 0.003476, current_train_items 240992.
I0302 19:01:15.091673 22760421793920 run.py:483] Algo bellman_ford step 7531 current loss 0.005440, current_train_items 241024.
I0302 19:01:15.114214 22760421793920 run.py:483] Algo bellman_ford step 7532 current loss 0.034247, current_train_items 241056.
I0302 19:01:15.146057 22760421793920 run.py:483] Algo bellman_ford step 7533 current loss 0.068778, current_train_items 241088.
I0302 19:01:15.178851 22760421793920 run.py:483] Algo bellman_ford step 7534 current loss 0.084665, current_train_items 241120.
I0302 19:01:15.197333 22760421793920 run.py:483] Algo bellman_ford step 7535 current loss 0.007683, current_train_items 241152.
I0302 19:01:15.213110 22760421793920 run.py:483] Algo bellman_ford step 7536 current loss 0.008507, current_train_items 241184.
I0302 19:01:15.236205 22760421793920 run.py:483] Algo bellman_ford step 7537 current loss 0.079594, current_train_items 241216.
I0302 19:01:15.267107 22760421793920 run.py:483] Algo bellman_ford step 7538 current loss 0.034951, current_train_items 241248.
I0302 19:01:15.298715 22760421793920 run.py:483] Algo bellman_ford step 7539 current loss 0.045130, current_train_items 241280.
I0302 19:01:15.317493 22760421793920 run.py:483] Algo bellman_ford step 7540 current loss 0.002574, current_train_items 241312.
I0302 19:01:15.333658 22760421793920 run.py:483] Algo bellman_ford step 7541 current loss 0.014510, current_train_items 241344.
I0302 19:01:15.358293 22760421793920 run.py:483] Algo bellman_ford step 7542 current loss 0.083133, current_train_items 241376.
I0302 19:01:15.389682 22760421793920 run.py:483] Algo bellman_ford step 7543 current loss 0.029928, current_train_items 241408.
I0302 19:01:15.421664 22760421793920 run.py:483] Algo bellman_ford step 7544 current loss 0.048308, current_train_items 241440.
I0302 19:01:15.439622 22760421793920 run.py:483] Algo bellman_ford step 7545 current loss 0.005647, current_train_items 241472.
I0302 19:01:15.455331 22760421793920 run.py:483] Algo bellman_ford step 7546 current loss 0.022555, current_train_items 241504.
I0302 19:01:15.479468 22760421793920 run.py:483] Algo bellman_ford step 7547 current loss 0.047868, current_train_items 241536.
I0302 19:01:15.510648 22760421793920 run.py:483] Algo bellman_ford step 7548 current loss 0.094261, current_train_items 241568.
I0302 19:01:15.542798 22760421793920 run.py:483] Algo bellman_ford step 7549 current loss 0.066503, current_train_items 241600.
I0302 19:01:15.561267 22760421793920 run.py:483] Algo bellman_ford step 7550 current loss 0.012721, current_train_items 241632.
I0302 19:01:15.568840 22760421793920 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:15.568959 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:01:15.585670 22760421793920 run.py:483] Algo bellman_ford step 7551 current loss 0.021411, current_train_items 241664.
I0302 19:01:15.609740 22760421793920 run.py:483] Algo bellman_ford step 7552 current loss 0.014491, current_train_items 241696.
I0302 19:01:15.641183 22760421793920 run.py:483] Algo bellman_ford step 7553 current loss 0.071327, current_train_items 241728.
I0302 19:01:15.673883 22760421793920 run.py:483] Algo bellman_ford step 7554 current loss 0.111929, current_train_items 241760.
I0302 19:01:15.692443 22760421793920 run.py:483] Algo bellman_ford step 7555 current loss 0.006026, current_train_items 241792.
I0302 19:01:15.708724 22760421793920 run.py:483] Algo bellman_ford step 7556 current loss 0.035022, current_train_items 241824.
I0302 19:01:15.732165 22760421793920 run.py:483] Algo bellman_ford step 7557 current loss 0.038490, current_train_items 241856.
I0302 19:01:15.763884 22760421793920 run.py:483] Algo bellman_ford step 7558 current loss 0.045579, current_train_items 241888.
I0302 19:01:15.795192 22760421793920 run.py:483] Algo bellman_ford step 7559 current loss 0.043474, current_train_items 241920.
I0302 19:01:15.813560 22760421793920 run.py:483] Algo bellman_ford step 7560 current loss 0.011017, current_train_items 241952.
I0302 19:01:15.829999 22760421793920 run.py:483] Algo bellman_ford step 7561 current loss 0.044259, current_train_items 241984.
I0302 19:01:15.853168 22760421793920 run.py:483] Algo bellman_ford step 7562 current loss 0.024986, current_train_items 242016.
I0302 19:01:15.884078 22760421793920 run.py:483] Algo bellman_ford step 7563 current loss 0.077588, current_train_items 242048.
I0302 19:01:15.916669 22760421793920 run.py:483] Algo bellman_ford step 7564 current loss 0.089865, current_train_items 242080.
I0302 19:01:15.934965 22760421793920 run.py:483] Algo bellman_ford step 7565 current loss 0.001953, current_train_items 242112.
I0302 19:01:15.950479 22760421793920 run.py:483] Algo bellman_ford step 7566 current loss 0.107145, current_train_items 242144.
I0302 19:01:15.973926 22760421793920 run.py:483] Algo bellman_ford step 7567 current loss 0.048408, current_train_items 242176.
I0302 19:01:16.004194 22760421793920 run.py:483] Algo bellman_ford step 7568 current loss 0.047629, current_train_items 242208.
I0302 19:01:16.035667 22760421793920 run.py:483] Algo bellman_ford step 7569 current loss 0.101339, current_train_items 242240.
I0302 19:01:16.054088 22760421793920 run.py:483] Algo bellman_ford step 7570 current loss 0.001737, current_train_items 242272.
I0302 19:01:16.070038 22760421793920 run.py:483] Algo bellman_ford step 7571 current loss 0.024039, current_train_items 242304.
I0302 19:01:16.093377 22760421793920 run.py:483] Algo bellman_ford step 7572 current loss 0.024861, current_train_items 242336.
I0302 19:01:16.122852 22760421793920 run.py:483] Algo bellman_ford step 7573 current loss 0.021233, current_train_items 242368.
I0302 19:01:16.152515 22760421793920 run.py:483] Algo bellman_ford step 7574 current loss 0.027657, current_train_items 242400.
I0302 19:01:16.170915 22760421793920 run.py:483] Algo bellman_ford step 7575 current loss 0.001151, current_train_items 242432.
I0302 19:01:16.187231 22760421793920 run.py:483] Algo bellman_ford step 7576 current loss 0.007580, current_train_items 242464.
I0302 19:01:16.209592 22760421793920 run.py:483] Algo bellman_ford step 7577 current loss 0.037337, current_train_items 242496.
I0302 19:01:16.240430 22760421793920 run.py:483] Algo bellman_ford step 7578 current loss 0.032961, current_train_items 242528.
I0302 19:01:16.273747 22760421793920 run.py:483] Algo bellman_ford step 7579 current loss 0.090852, current_train_items 242560.
I0302 19:01:16.292326 22760421793920 run.py:483] Algo bellman_ford step 7580 current loss 0.000731, current_train_items 242592.
I0302 19:01:16.308332 22760421793920 run.py:483] Algo bellman_ford step 7581 current loss 0.036208, current_train_items 242624.
I0302 19:01:16.331785 22760421793920 run.py:483] Algo bellman_ford step 7582 current loss 0.027137, current_train_items 242656.
I0302 19:01:16.362067 22760421793920 run.py:483] Algo bellman_ford step 7583 current loss 0.052160, current_train_items 242688.
I0302 19:01:16.393392 22760421793920 run.py:483] Algo bellman_ford step 7584 current loss 0.074502, current_train_items 242720.
I0302 19:01:16.411723 22760421793920 run.py:483] Algo bellman_ford step 7585 current loss 0.002827, current_train_items 242752.
I0302 19:01:16.427091 22760421793920 run.py:483] Algo bellman_ford step 7586 current loss 0.008663, current_train_items 242784.
I0302 19:01:16.450443 22760421793920 run.py:483] Algo bellman_ford step 7587 current loss 0.035569, current_train_items 242816.
I0302 19:01:16.478703 22760421793920 run.py:483] Algo bellman_ford step 7588 current loss 0.008893, current_train_items 242848.
I0302 19:01:16.511175 22760421793920 run.py:483] Algo bellman_ford step 7589 current loss 0.064203, current_train_items 242880.
I0302 19:01:16.529453 22760421793920 run.py:483] Algo bellman_ford step 7590 current loss 0.012391, current_train_items 242912.
I0302 19:01:16.544884 22760421793920 run.py:483] Algo bellman_ford step 7591 current loss 0.011927, current_train_items 242944.
I0302 19:01:16.569176 22760421793920 run.py:483] Algo bellman_ford step 7592 current loss 0.118631, current_train_items 242976.
I0302 19:01:16.598385 22760421793920 run.py:483] Algo bellman_ford step 7593 current loss 0.038303, current_train_items 243008.
I0302 19:01:16.630719 22760421793920 run.py:483] Algo bellman_ford step 7594 current loss 0.041394, current_train_items 243040.
I0302 19:01:16.648982 22760421793920 run.py:483] Algo bellman_ford step 7595 current loss 0.002142, current_train_items 243072.
I0302 19:01:16.664541 22760421793920 run.py:483] Algo bellman_ford step 7596 current loss 0.011662, current_train_items 243104.
I0302 19:01:16.688277 22760421793920 run.py:483] Algo bellman_ford step 7597 current loss 0.043650, current_train_items 243136.
I0302 19:01:16.719652 22760421793920 run.py:483] Algo bellman_ford step 7598 current loss 0.128378, current_train_items 243168.
I0302 19:01:16.749143 22760421793920 run.py:483] Algo bellman_ford step 7599 current loss 0.078208, current_train_items 243200.
I0302 19:01:16.767280 22760421793920 run.py:483] Algo bellman_ford step 7600 current loss 0.013946, current_train_items 243232.
I0302 19:01:16.774664 22760421793920 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:01:16.774775 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:16.791303 22760421793920 run.py:483] Algo bellman_ford step 7601 current loss 0.017410, current_train_items 243264.
I0302 19:01:16.816607 22760421793920 run.py:483] Algo bellman_ford step 7602 current loss 0.086073, current_train_items 243296.
I0302 19:01:16.846675 22760421793920 run.py:483] Algo bellman_ford step 7603 current loss 0.033136, current_train_items 243328.
I0302 19:01:16.879537 22760421793920 run.py:483] Algo bellman_ford step 7604 current loss 0.064853, current_train_items 243360.
I0302 19:01:16.898038 22760421793920 run.py:483] Algo bellman_ford step 7605 current loss 0.034492, current_train_items 243392.
I0302 19:01:16.913015 22760421793920 run.py:483] Algo bellman_ford step 7606 current loss 0.007471, current_train_items 243424.
I0302 19:01:16.936160 22760421793920 run.py:483] Algo bellman_ford step 7607 current loss 0.022983, current_train_items 243456.
I0302 19:01:16.966888 22760421793920 run.py:483] Algo bellman_ford step 7608 current loss 0.040361, current_train_items 243488.
I0302 19:01:16.999507 22760421793920 run.py:483] Algo bellman_ford step 7609 current loss 0.069529, current_train_items 243520.
I0302 19:01:17.017724 22760421793920 run.py:483] Algo bellman_ford step 7610 current loss 0.001557, current_train_items 243552.
I0302 19:01:17.033920 22760421793920 run.py:483] Algo bellman_ford step 7611 current loss 0.006212, current_train_items 243584.
I0302 19:01:17.057176 22760421793920 run.py:483] Algo bellman_ford step 7612 current loss 0.023038, current_train_items 243616.
I0302 19:01:17.085432 22760421793920 run.py:483] Algo bellman_ford step 7613 current loss 0.031070, current_train_items 243648.
I0302 19:01:17.116569 22760421793920 run.py:483] Algo bellman_ford step 7614 current loss 0.091982, current_train_items 243680.
I0302 19:01:17.134912 22760421793920 run.py:483] Algo bellman_ford step 7615 current loss 0.001197, current_train_items 243712.
I0302 19:01:17.150716 22760421793920 run.py:483] Algo bellman_ford step 7616 current loss 0.007352, current_train_items 243744.
I0302 19:01:17.173547 22760421793920 run.py:483] Algo bellman_ford step 7617 current loss 0.031306, current_train_items 243776.
I0302 19:01:17.203021 22760421793920 run.py:483] Algo bellman_ford step 7618 current loss 0.033689, current_train_items 243808.
I0302 19:01:17.235083 22760421793920 run.py:483] Algo bellman_ford step 7619 current loss 0.038545, current_train_items 243840.
I0302 19:01:17.253146 22760421793920 run.py:483] Algo bellman_ford step 7620 current loss 0.005996, current_train_items 243872.
I0302 19:01:17.268840 22760421793920 run.py:483] Algo bellman_ford step 7621 current loss 0.008892, current_train_items 243904.
I0302 19:01:17.292124 22760421793920 run.py:483] Algo bellman_ford step 7622 current loss 0.031656, current_train_items 243936.
I0302 19:01:17.323512 22760421793920 run.py:483] Algo bellman_ford step 7623 current loss 0.049131, current_train_items 243968.
I0302 19:01:17.354509 22760421793920 run.py:483] Algo bellman_ford step 7624 current loss 0.041824, current_train_items 244000.
I0302 19:01:17.372596 22760421793920 run.py:483] Algo bellman_ford step 7625 current loss 0.004050, current_train_items 244032.
I0302 19:01:17.387620 22760421793920 run.py:483] Algo bellman_ford step 7626 current loss 0.006233, current_train_items 244064.
I0302 19:01:17.411662 22760421793920 run.py:483] Algo bellman_ford step 7627 current loss 0.027858, current_train_items 244096.
I0302 19:01:17.442885 22760421793920 run.py:483] Algo bellman_ford step 7628 current loss 0.028620, current_train_items 244128.
I0302 19:01:17.474029 22760421793920 run.py:483] Algo bellman_ford step 7629 current loss 0.043688, current_train_items 244160.
I0302 19:01:17.492223 22760421793920 run.py:483] Algo bellman_ford step 7630 current loss 0.028218, current_train_items 244192.
I0302 19:01:17.508207 22760421793920 run.py:483] Algo bellman_ford step 7631 current loss 0.010053, current_train_items 244224.
I0302 19:01:17.531209 22760421793920 run.py:483] Algo bellman_ford step 7632 current loss 0.030801, current_train_items 244256.
I0302 19:01:17.562186 22760421793920 run.py:483] Algo bellman_ford step 7633 current loss 0.019371, current_train_items 244288.
I0302 19:01:17.594447 22760421793920 run.py:483] Algo bellman_ford step 7634 current loss 0.046321, current_train_items 244320.
I0302 19:01:17.612710 22760421793920 run.py:483] Algo bellman_ford step 7635 current loss 0.004357, current_train_items 244352.
I0302 19:01:17.628252 22760421793920 run.py:483] Algo bellman_ford step 7636 current loss 0.005448, current_train_items 244384.
I0302 19:01:17.651270 22760421793920 run.py:483] Algo bellman_ford step 7637 current loss 0.019059, current_train_items 244416.
I0302 19:01:17.680516 22760421793920 run.py:483] Algo bellman_ford step 7638 current loss 0.029409, current_train_items 244448.
I0302 19:01:17.714890 22760421793920 run.py:483] Algo bellman_ford step 7639 current loss 0.039882, current_train_items 244480.
I0302 19:01:17.732818 22760421793920 run.py:483] Algo bellman_ford step 7640 current loss 0.001613, current_train_items 244512.
I0302 19:01:17.748399 22760421793920 run.py:483] Algo bellman_ford step 7641 current loss 0.017670, current_train_items 244544.
I0302 19:01:17.770908 22760421793920 run.py:483] Algo bellman_ford step 7642 current loss 0.018281, current_train_items 244576.
I0302 19:01:17.803320 22760421793920 run.py:483] Algo bellman_ford step 7643 current loss 0.056150, current_train_items 244608.
I0302 19:01:17.836738 22760421793920 run.py:483] Algo bellman_ford step 7644 current loss 0.040705, current_train_items 244640.
I0302 19:01:17.854812 22760421793920 run.py:483] Algo bellman_ford step 7645 current loss 0.014699, current_train_items 244672.
I0302 19:01:17.870575 22760421793920 run.py:483] Algo bellman_ford step 7646 current loss 0.010950, current_train_items 244704.
I0302 19:01:17.894584 22760421793920 run.py:483] Algo bellman_ford step 7647 current loss 0.045175, current_train_items 244736.
I0302 19:01:17.926535 22760421793920 run.py:483] Algo bellman_ford step 7648 current loss 0.064704, current_train_items 244768.
I0302 19:01:17.960111 22760421793920 run.py:483] Algo bellman_ford step 7649 current loss 0.114055, current_train_items 244800.
I0302 19:01:17.978374 22760421793920 run.py:483] Algo bellman_ford step 7650 current loss 0.004330, current_train_items 244832.
I0302 19:01:17.985927 22760421793920 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:01:17.986041 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:18.002034 22760421793920 run.py:483] Algo bellman_ford step 7651 current loss 0.017703, current_train_items 244864.
I0302 19:01:18.026434 22760421793920 run.py:483] Algo bellman_ford step 7652 current loss 0.038742, current_train_items 244896.
I0302 19:01:18.056858 22760421793920 run.py:483] Algo bellman_ford step 7653 current loss 0.045134, current_train_items 244928.
I0302 19:01:18.088894 22760421793920 run.py:483] Algo bellman_ford step 7654 current loss 0.076607, current_train_items 244960.
I0302 19:01:18.107352 22760421793920 run.py:483] Algo bellman_ford step 7655 current loss 0.009401, current_train_items 244992.
I0302 19:01:18.123407 22760421793920 run.py:483] Algo bellman_ford step 7656 current loss 0.022470, current_train_items 245024.
I0302 19:01:18.146974 22760421793920 run.py:483] Algo bellman_ford step 7657 current loss 0.030589, current_train_items 245056.
I0302 19:01:18.178118 22760421793920 run.py:483] Algo bellman_ford step 7658 current loss 0.051753, current_train_items 245088.
I0302 19:01:18.211158 22760421793920 run.py:483] Algo bellman_ford step 7659 current loss 0.062318, current_train_items 245120.
I0302 19:01:18.229225 22760421793920 run.py:483] Algo bellman_ford step 7660 current loss 0.011616, current_train_items 245152.
I0302 19:01:18.244893 22760421793920 run.py:483] Algo bellman_ford step 7661 current loss 0.004534, current_train_items 245184.
I0302 19:01:18.268251 22760421793920 run.py:483] Algo bellman_ford step 7662 current loss 0.050210, current_train_items 245216.
I0302 19:01:18.298077 22760421793920 run.py:483] Algo bellman_ford step 7663 current loss 0.034619, current_train_items 245248.
I0302 19:01:18.329796 22760421793920 run.py:483] Algo bellman_ford step 7664 current loss 0.064843, current_train_items 245280.
I0302 19:01:18.347884 22760421793920 run.py:483] Algo bellman_ford step 7665 current loss 0.016146, current_train_items 245312.
I0302 19:01:18.363574 22760421793920 run.py:483] Algo bellman_ford step 7666 current loss 0.013307, current_train_items 245344.
I0302 19:01:18.386460 22760421793920 run.py:483] Algo bellman_ford step 7667 current loss 0.032504, current_train_items 245376.
I0302 19:01:18.418993 22760421793920 run.py:483] Algo bellman_ford step 7668 current loss 0.059465, current_train_items 245408.
I0302 19:01:18.451659 22760421793920 run.py:483] Algo bellman_ford step 7669 current loss 0.045472, current_train_items 245440.
I0302 19:01:18.469584 22760421793920 run.py:483] Algo bellman_ford step 7670 current loss 0.005918, current_train_items 245472.
I0302 19:01:18.485628 22760421793920 run.py:483] Algo bellman_ford step 7671 current loss 0.011672, current_train_items 245504.
I0302 19:01:18.508723 22760421793920 run.py:483] Algo bellman_ford step 7672 current loss 0.026439, current_train_items 245536.
I0302 19:01:18.539757 22760421793920 run.py:483] Algo bellman_ford step 7673 current loss 0.026876, current_train_items 245568.
I0302 19:01:18.572325 22760421793920 run.py:483] Algo bellman_ford step 7674 current loss 0.065453, current_train_items 245600.
I0302 19:01:18.590278 22760421793920 run.py:483] Algo bellman_ford step 7675 current loss 0.001245, current_train_items 245632.
I0302 19:01:18.606515 22760421793920 run.py:483] Algo bellman_ford step 7676 current loss 0.034670, current_train_items 245664.
I0302 19:01:18.630242 22760421793920 run.py:483] Algo bellman_ford step 7677 current loss 0.019596, current_train_items 245696.
I0302 19:01:18.660130 22760421793920 run.py:483] Algo bellman_ford step 7678 current loss 0.051229, current_train_items 245728.
I0302 19:01:18.693950 22760421793920 run.py:483] Algo bellman_ford step 7679 current loss 0.061422, current_train_items 245760.
I0302 19:01:18.711957 22760421793920 run.py:483] Algo bellman_ford step 7680 current loss 0.001014, current_train_items 245792.
I0302 19:01:18.727857 22760421793920 run.py:483] Algo bellman_ford step 7681 current loss 0.010019, current_train_items 245824.
I0302 19:01:18.750550 22760421793920 run.py:483] Algo bellman_ford step 7682 current loss 0.023682, current_train_items 245856.
I0302 19:01:18.781297 22760421793920 run.py:483] Algo bellman_ford step 7683 current loss 0.039569, current_train_items 245888.
I0302 19:01:18.814106 22760421793920 run.py:483] Algo bellman_ford step 7684 current loss 0.060566, current_train_items 245920.
I0302 19:01:18.832175 22760421793920 run.py:483] Algo bellman_ford step 7685 current loss 0.006422, current_train_items 245952.
I0302 19:01:18.848073 22760421793920 run.py:483] Algo bellman_ford step 7686 current loss 0.017639, current_train_items 245984.
I0302 19:01:18.870445 22760421793920 run.py:483] Algo bellman_ford step 7687 current loss 0.070342, current_train_items 246016.
I0302 19:01:18.901651 22760421793920 run.py:483] Algo bellman_ford step 7688 current loss 0.019164, current_train_items 246048.
I0302 19:01:18.934083 22760421793920 run.py:483] Algo bellman_ford step 7689 current loss 0.056347, current_train_items 246080.
I0302 19:01:18.951982 22760421793920 run.py:483] Algo bellman_ford step 7690 current loss 0.012954, current_train_items 246112.
I0302 19:01:18.967721 22760421793920 run.py:483] Algo bellman_ford step 7691 current loss 0.010466, current_train_items 246144.
I0302 19:01:18.990511 22760421793920 run.py:483] Algo bellman_ford step 7692 current loss 0.030300, current_train_items 246176.
I0302 19:01:19.022409 22760421793920 run.py:483] Algo bellman_ford step 7693 current loss 0.074191, current_train_items 246208.
I0302 19:01:19.053798 22760421793920 run.py:483] Algo bellman_ford step 7694 current loss 0.054028, current_train_items 246240.
I0302 19:01:19.071807 22760421793920 run.py:483] Algo bellman_ford step 7695 current loss 0.003887, current_train_items 246272.
I0302 19:01:19.087779 22760421793920 run.py:483] Algo bellman_ford step 7696 current loss 0.010030, current_train_items 246304.
I0302 19:01:19.111574 22760421793920 run.py:483] Algo bellman_ford step 7697 current loss 0.049206, current_train_items 246336.
I0302 19:01:19.142867 22760421793920 run.py:483] Algo bellman_ford step 7698 current loss 0.050822, current_train_items 246368.
I0302 19:01:19.176521 22760421793920 run.py:483] Algo bellman_ford step 7699 current loss 0.054831, current_train_items 246400.
I0302 19:01:19.194868 22760421793920 run.py:483] Algo bellman_ford step 7700 current loss 0.006734, current_train_items 246432.
I0302 19:01:19.202304 22760421793920 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:01:19.202415 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:19.219191 22760421793920 run.py:483] Algo bellman_ford step 7701 current loss 0.011229, current_train_items 246464.
I0302 19:01:19.242753 22760421793920 run.py:483] Algo bellman_ford step 7702 current loss 0.020916, current_train_items 246496.
I0302 19:01:19.272527 22760421793920 run.py:483] Algo bellman_ford step 7703 current loss 0.024968, current_train_items 246528.
I0302 19:01:19.304035 22760421793920 run.py:483] Algo bellman_ford step 7704 current loss 0.055395, current_train_items 246560.
I0302 19:01:19.322586 22760421793920 run.py:483] Algo bellman_ford step 7705 current loss 0.021728, current_train_items 246592.
I0302 19:01:19.338129 22760421793920 run.py:483] Algo bellman_ford step 7706 current loss 0.010356, current_train_items 246624.
I0302 19:01:19.362397 22760421793920 run.py:483] Algo bellman_ford step 7707 current loss 0.023121, current_train_items 246656.
I0302 19:01:19.392058 22760421793920 run.py:483] Algo bellman_ford step 7708 current loss 0.033447, current_train_items 246688.
I0302 19:01:19.423093 22760421793920 run.py:483] Algo bellman_ford step 7709 current loss 0.038051, current_train_items 246720.
I0302 19:01:19.441710 22760421793920 run.py:483] Algo bellman_ford step 7710 current loss 0.001882, current_train_items 246752.
I0302 19:01:19.457635 22760421793920 run.py:483] Algo bellman_ford step 7711 current loss 0.005964, current_train_items 246784.
I0302 19:01:19.480775 22760421793920 run.py:483] Algo bellman_ford step 7712 current loss 0.047580, current_train_items 246816.
I0302 19:01:19.510234 22760421793920 run.py:483] Algo bellman_ford step 7713 current loss 0.032925, current_train_items 246848.
I0302 19:01:19.541946 22760421793920 run.py:483] Algo bellman_ford step 7714 current loss 0.030239, current_train_items 246880.
I0302 19:01:19.560161 22760421793920 run.py:483] Algo bellman_ford step 7715 current loss 0.027150, current_train_items 246912.
I0302 19:01:19.575787 22760421793920 run.py:483] Algo bellman_ford step 7716 current loss 0.007398, current_train_items 246944.
I0302 19:01:19.599263 22760421793920 run.py:483] Algo bellman_ford step 7717 current loss 0.089401, current_train_items 246976.
I0302 19:01:19.629265 22760421793920 run.py:483] Algo bellman_ford step 7718 current loss 0.062401, current_train_items 247008.
I0302 19:01:19.660039 22760421793920 run.py:483] Algo bellman_ford step 7719 current loss 0.077550, current_train_items 247040.
I0302 19:01:19.678441 22760421793920 run.py:483] Algo bellman_ford step 7720 current loss 0.002691, current_train_items 247072.
I0302 19:01:19.694101 22760421793920 run.py:483] Algo bellman_ford step 7721 current loss 0.009787, current_train_items 247104.
I0302 19:01:19.717972 22760421793920 run.py:483] Algo bellman_ford step 7722 current loss 0.052003, current_train_items 247136.
I0302 19:01:19.749533 22760421793920 run.py:483] Algo bellman_ford step 7723 current loss 0.083962, current_train_items 247168.
I0302 19:01:19.782377 22760421793920 run.py:483] Algo bellman_ford step 7724 current loss 0.062432, current_train_items 247200.
I0302 19:01:19.800777 22760421793920 run.py:483] Algo bellman_ford step 7725 current loss 0.002282, current_train_items 247232.
I0302 19:01:19.816496 22760421793920 run.py:483] Algo bellman_ford step 7726 current loss 0.015446, current_train_items 247264.
I0302 19:01:19.838505 22760421793920 run.py:483] Algo bellman_ford step 7727 current loss 0.015191, current_train_items 247296.
I0302 19:01:19.868694 22760421793920 run.py:483] Algo bellman_ford step 7728 current loss 0.047044, current_train_items 247328.
I0302 19:01:19.899806 22760421793920 run.py:483] Algo bellman_ford step 7729 current loss 0.047661, current_train_items 247360.
I0302 19:01:19.918025 22760421793920 run.py:483] Algo bellman_ford step 7730 current loss 0.009527, current_train_items 247392.
I0302 19:01:19.933406 22760421793920 run.py:483] Algo bellman_ford step 7731 current loss 0.005674, current_train_items 247424.
I0302 19:01:19.956113 22760421793920 run.py:483] Algo bellman_ford step 7732 current loss 0.053193, current_train_items 247456.
I0302 19:01:19.986419 22760421793920 run.py:483] Algo bellman_ford step 7733 current loss 0.095297, current_train_items 247488.
I0302 19:01:20.016760 22760421793920 run.py:483] Algo bellman_ford step 7734 current loss 0.074487, current_train_items 247520.
I0302 19:01:20.034769 22760421793920 run.py:483] Algo bellman_ford step 7735 current loss 0.001582, current_train_items 247552.
I0302 19:01:20.050610 22760421793920 run.py:483] Algo bellman_ford step 7736 current loss 0.013828, current_train_items 247584.
I0302 19:01:20.074234 22760421793920 run.py:483] Algo bellman_ford step 7737 current loss 0.034244, current_train_items 247616.
I0302 19:01:20.104065 22760421793920 run.py:483] Algo bellman_ford step 7738 current loss 0.069325, current_train_items 247648.
I0302 19:01:20.136434 22760421793920 run.py:483] Algo bellman_ford step 7739 current loss 0.068311, current_train_items 247680.
I0302 19:01:20.154578 22760421793920 run.py:483] Algo bellman_ford step 7740 current loss 0.011804, current_train_items 247712.
I0302 19:01:20.170509 22760421793920 run.py:483] Algo bellman_ford step 7741 current loss 0.019829, current_train_items 247744.
I0302 19:01:20.194853 22760421793920 run.py:483] Algo bellman_ford step 7742 current loss 0.108274, current_train_items 247776.
I0302 19:01:20.225859 22760421793920 run.py:483] Algo bellman_ford step 7743 current loss 0.088463, current_train_items 247808.
I0302 19:01:20.257884 22760421793920 run.py:483] Algo bellman_ford step 7744 current loss 0.144808, current_train_items 247840.
I0302 19:01:20.276051 22760421793920 run.py:483] Algo bellman_ford step 7745 current loss 0.025965, current_train_items 247872.
I0302 19:01:20.291632 22760421793920 run.py:483] Algo bellman_ford step 7746 current loss 0.005649, current_train_items 247904.
I0302 19:01:20.314551 22760421793920 run.py:483] Algo bellman_ford step 7747 current loss 0.028713, current_train_items 247936.
I0302 19:01:20.346301 22760421793920 run.py:483] Algo bellman_ford step 7748 current loss 0.102071, current_train_items 247968.
I0302 19:01:20.378718 22760421793920 run.py:483] Algo bellman_ford step 7749 current loss 0.048786, current_train_items 248000.
I0302 19:01:20.397459 22760421793920 run.py:483] Algo bellman_ford step 7750 current loss 0.012323, current_train_items 248032.
I0302 19:01:20.405081 22760421793920 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:01:20.405190 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:20.421925 22760421793920 run.py:483] Algo bellman_ford step 7751 current loss 0.028925, current_train_items 248064.
I0302 19:01:20.445832 22760421793920 run.py:483] Algo bellman_ford step 7752 current loss 0.037955, current_train_items 248096.
I0302 19:01:20.476521 22760421793920 run.py:483] Algo bellman_ford step 7753 current loss 0.066891, current_train_items 248128.
I0302 19:01:20.510993 22760421793920 run.py:483] Algo bellman_ford step 7754 current loss 0.066237, current_train_items 248160.
I0302 19:01:20.529743 22760421793920 run.py:483] Algo bellman_ford step 7755 current loss 0.000991, current_train_items 248192.
I0302 19:01:20.545608 22760421793920 run.py:483] Algo bellman_ford step 7756 current loss 0.023672, current_train_items 248224.
I0302 19:01:20.568764 22760421793920 run.py:483] Algo bellman_ford step 7757 current loss 0.024262, current_train_items 248256.
I0302 19:01:20.598477 22760421793920 run.py:483] Algo bellman_ford step 7758 current loss 0.044293, current_train_items 248288.
I0302 19:01:20.630271 22760421793920 run.py:483] Algo bellman_ford step 7759 current loss 0.055854, current_train_items 248320.
I0302 19:01:20.648875 22760421793920 run.py:483] Algo bellman_ford step 7760 current loss 0.001060, current_train_items 248352.
I0302 19:01:20.664510 22760421793920 run.py:483] Algo bellman_ford step 7761 current loss 0.046712, current_train_items 248384.
I0302 19:01:20.688360 22760421793920 run.py:483] Algo bellman_ford step 7762 current loss 0.038515, current_train_items 248416.
I0302 19:01:20.718694 22760421793920 run.py:483] Algo bellman_ford step 7763 current loss 0.038890, current_train_items 248448.
I0302 19:01:20.749516 22760421793920 run.py:483] Algo bellman_ford step 7764 current loss 0.030865, current_train_items 248480.
I0302 19:01:20.767891 22760421793920 run.py:483] Algo bellman_ford step 7765 current loss 0.002245, current_train_items 248512.
I0302 19:01:20.783652 22760421793920 run.py:483] Algo bellman_ford step 7766 current loss 0.018829, current_train_items 248544.
I0302 19:01:20.806591 22760421793920 run.py:483] Algo bellman_ford step 7767 current loss 0.035100, current_train_items 248576.
I0302 19:01:20.837108 22760421793920 run.py:483] Algo bellman_ford step 7768 current loss 0.043600, current_train_items 248608.
I0302 19:01:20.868987 22760421793920 run.py:483] Algo bellman_ford step 7769 current loss 0.066496, current_train_items 248640.
I0302 19:01:20.887107 22760421793920 run.py:483] Algo bellman_ford step 7770 current loss 0.003151, current_train_items 248672.
I0302 19:01:20.902972 22760421793920 run.py:483] Algo bellman_ford step 7771 current loss 0.023011, current_train_items 248704.
I0302 19:01:20.926076 22760421793920 run.py:483] Algo bellman_ford step 7772 current loss 0.015175, current_train_items 248736.
I0302 19:01:20.957407 22760421793920 run.py:483] Algo bellman_ford step 7773 current loss 0.041347, current_train_items 248768.
I0302 19:01:20.988945 22760421793920 run.py:483] Algo bellman_ford step 7774 current loss 0.065870, current_train_items 248800.
I0302 19:01:21.007469 22760421793920 run.py:483] Algo bellman_ford step 7775 current loss 0.002764, current_train_items 248832.
I0302 19:01:21.023291 22760421793920 run.py:483] Algo bellman_ford step 7776 current loss 0.049790, current_train_items 248864.
I0302 19:01:21.047649 22760421793920 run.py:483] Algo bellman_ford step 7777 current loss 0.040477, current_train_items 248896.
I0302 19:01:21.078769 22760421793920 run.py:483] Algo bellman_ford step 7778 current loss 0.061643, current_train_items 248928.
I0302 19:01:21.113029 22760421793920 run.py:483] Algo bellman_ford step 7779 current loss 0.069243, current_train_items 248960.
I0302 19:01:21.131329 22760421793920 run.py:483] Algo bellman_ford step 7780 current loss 0.002437, current_train_items 248992.
I0302 19:01:21.147424 22760421793920 run.py:483] Algo bellman_ford step 7781 current loss 0.019881, current_train_items 249024.
I0302 19:01:21.170606 22760421793920 run.py:483] Algo bellman_ford step 7782 current loss 0.063895, current_train_items 249056.
I0302 19:01:21.202226 22760421793920 run.py:483] Algo bellman_ford step 7783 current loss 0.059530, current_train_items 249088.
I0302 19:01:21.234963 22760421793920 run.py:483] Algo bellman_ford step 7784 current loss 0.061237, current_train_items 249120.
I0302 19:01:21.253655 22760421793920 run.py:483] Algo bellman_ford step 7785 current loss 0.006975, current_train_items 249152.
I0302 19:01:21.269421 22760421793920 run.py:483] Algo bellman_ford step 7786 current loss 0.037098, current_train_items 249184.
I0302 19:01:21.292489 22760421793920 run.py:483] Algo bellman_ford step 7787 current loss 0.031098, current_train_items 249216.
I0302 19:01:21.322732 22760421793920 run.py:483] Algo bellman_ford step 7788 current loss 0.058383, current_train_items 249248.
I0302 19:01:21.352740 22760421793920 run.py:483] Algo bellman_ford step 7789 current loss 0.047927, current_train_items 249280.
I0302 19:01:21.371133 22760421793920 run.py:483] Algo bellman_ford step 7790 current loss 0.005883, current_train_items 249312.
I0302 19:01:21.386446 22760421793920 run.py:483] Algo bellman_ford step 7791 current loss 0.037528, current_train_items 249344.
I0302 19:01:21.409865 22760421793920 run.py:483] Algo bellman_ford step 7792 current loss 0.059006, current_train_items 249376.
I0302 19:01:21.439616 22760421793920 run.py:483] Algo bellman_ford step 7793 current loss 0.087650, current_train_items 249408.
I0302 19:01:21.471478 22760421793920 run.py:483] Algo bellman_ford step 7794 current loss 0.079156, current_train_items 249440.
I0302 19:01:21.490067 22760421793920 run.py:483] Algo bellman_ford step 7795 current loss 0.007115, current_train_items 249472.
I0302 19:01:21.505398 22760421793920 run.py:483] Algo bellman_ford step 7796 current loss 0.010132, current_train_items 249504.
I0302 19:01:21.528093 22760421793920 run.py:483] Algo bellman_ford step 7797 current loss 0.019812, current_train_items 249536.
I0302 19:01:21.557725 22760421793920 run.py:483] Algo bellman_ford step 7798 current loss 0.021646, current_train_items 249568.
I0302 19:01:21.591281 22760421793920 run.py:483] Algo bellman_ford step 7799 current loss 0.065051, current_train_items 249600.
I0302 19:01:21.609465 22760421793920 run.py:483] Algo bellman_ford step 7800 current loss 0.001646, current_train_items 249632.
I0302 19:01:21.616693 22760421793920 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:01:21.616804 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:01:21.633149 22760421793920 run.py:483] Algo bellman_ford step 7801 current loss 0.028094, current_train_items 249664.
I0302 19:01:21.657126 22760421793920 run.py:483] Algo bellman_ford step 7802 current loss 0.018226, current_train_items 249696.
I0302 19:01:21.689251 22760421793920 run.py:483] Algo bellman_ford step 7803 current loss 0.034876, current_train_items 249728.
I0302 19:01:21.723702 22760421793920 run.py:483] Algo bellman_ford step 7804 current loss 0.047728, current_train_items 249760.
I0302 19:01:21.742676 22760421793920 run.py:483] Algo bellman_ford step 7805 current loss 0.003317, current_train_items 249792.
I0302 19:01:21.758655 22760421793920 run.py:483] Algo bellman_ford step 7806 current loss 0.008253, current_train_items 249824.
I0302 19:01:21.781028 22760421793920 run.py:483] Algo bellman_ford step 7807 current loss 0.033261, current_train_items 249856.
I0302 19:01:21.810608 22760421793920 run.py:483] Algo bellman_ford step 7808 current loss 0.057516, current_train_items 249888.
I0302 19:01:21.843095 22760421793920 run.py:483] Algo bellman_ford step 7809 current loss 0.131011, current_train_items 249920.
I0302 19:01:21.861432 22760421793920 run.py:483] Algo bellman_ford step 7810 current loss 0.001502, current_train_items 249952.
I0302 19:01:21.877544 22760421793920 run.py:483] Algo bellman_ford step 7811 current loss 0.017926, current_train_items 249984.
I0302 19:01:21.901125 22760421793920 run.py:483] Algo bellman_ford step 7812 current loss 0.041310, current_train_items 250016.
I0302 19:01:21.932684 22760421793920 run.py:483] Algo bellman_ford step 7813 current loss 0.054323, current_train_items 250048.
I0302 19:01:21.965240 22760421793920 run.py:483] Algo bellman_ford step 7814 current loss 0.054607, current_train_items 250080.
I0302 19:01:21.983296 22760421793920 run.py:483] Algo bellman_ford step 7815 current loss 0.005254, current_train_items 250112.
I0302 19:01:21.998807 22760421793920 run.py:483] Algo bellman_ford step 7816 current loss 0.018010, current_train_items 250144.
I0302 19:01:22.022454 22760421793920 run.py:483] Algo bellman_ford step 7817 current loss 0.033826, current_train_items 250176.
I0302 19:01:22.051810 22760421793920 run.py:483] Algo bellman_ford step 7818 current loss 0.051672, current_train_items 250208.
I0302 19:01:22.083068 22760421793920 run.py:483] Algo bellman_ford step 7819 current loss 0.052649, current_train_items 250240.
I0302 19:01:22.101405 22760421793920 run.py:483] Algo bellman_ford step 7820 current loss 0.002076, current_train_items 250272.
I0302 19:01:22.117459 22760421793920 run.py:483] Algo bellman_ford step 7821 current loss 0.011669, current_train_items 250304.
I0302 19:01:22.141933 22760421793920 run.py:483] Algo bellman_ford step 7822 current loss 0.032830, current_train_items 250336.
I0302 19:01:22.174005 22760421793920 run.py:483] Algo bellman_ford step 7823 current loss 0.079655, current_train_items 250368.
I0302 19:01:22.205478 22760421793920 run.py:483] Algo bellman_ford step 7824 current loss 0.059489, current_train_items 250400.
I0302 19:01:22.223769 22760421793920 run.py:483] Algo bellman_ford step 7825 current loss 0.002532, current_train_items 250432.
I0302 19:01:22.239861 22760421793920 run.py:483] Algo bellman_ford step 7826 current loss 0.012354, current_train_items 250464.
I0302 19:01:22.263918 22760421793920 run.py:483] Algo bellman_ford step 7827 current loss 0.016265, current_train_items 250496.
I0302 19:01:22.294816 22760421793920 run.py:483] Algo bellman_ford step 7828 current loss 0.079559, current_train_items 250528.
I0302 19:01:22.327101 22760421793920 run.py:483] Algo bellman_ford step 7829 current loss 0.060244, current_train_items 250560.
I0302 19:01:22.345578 22760421793920 run.py:483] Algo bellman_ford step 7830 current loss 0.007022, current_train_items 250592.
I0302 19:01:22.360959 22760421793920 run.py:483] Algo bellman_ford step 7831 current loss 0.015766, current_train_items 250624.
I0302 19:01:22.384541 22760421793920 run.py:483] Algo bellman_ford step 7832 current loss 0.073454, current_train_items 250656.
I0302 19:01:22.415952 22760421793920 run.py:483] Algo bellman_ford step 7833 current loss 0.085602, current_train_items 250688.
I0302 19:01:22.446146 22760421793920 run.py:483] Algo bellman_ford step 7834 current loss 0.079121, current_train_items 250720.
I0302 19:01:22.464323 22760421793920 run.py:483] Algo bellman_ford step 7835 current loss 0.003101, current_train_items 250752.
I0302 19:01:22.480275 22760421793920 run.py:483] Algo bellman_ford step 7836 current loss 0.006095, current_train_items 250784.
I0302 19:01:22.503879 22760421793920 run.py:483] Algo bellman_ford step 7837 current loss 0.020608, current_train_items 250816.
I0302 19:01:22.535759 22760421793920 run.py:483] Algo bellman_ford step 7838 current loss 0.062281, current_train_items 250848.
I0302 19:01:22.569364 22760421793920 run.py:483] Algo bellman_ford step 7839 current loss 0.126765, current_train_items 250880.
I0302 19:01:22.587571 22760421793920 run.py:483] Algo bellman_ford step 7840 current loss 0.010566, current_train_items 250912.
I0302 19:01:22.603480 22760421793920 run.py:483] Algo bellman_ford step 7841 current loss 0.029657, current_train_items 250944.
I0302 19:01:22.627083 22760421793920 run.py:483] Algo bellman_ford step 7842 current loss 0.023657, current_train_items 250976.
I0302 19:01:22.658985 22760421793920 run.py:483] Algo bellman_ford step 7843 current loss 0.060965, current_train_items 251008.
I0302 19:01:22.692470 22760421793920 run.py:483] Algo bellman_ford step 7844 current loss 0.064124, current_train_items 251040.
I0302 19:01:22.710669 22760421793920 run.py:483] Algo bellman_ford step 7845 current loss 0.009557, current_train_items 251072.
I0302 19:01:22.726375 22760421793920 run.py:483] Algo bellman_ford step 7846 current loss 0.019336, current_train_items 251104.
I0302 19:01:22.750012 22760421793920 run.py:483] Algo bellman_ford step 7847 current loss 0.049159, current_train_items 251136.
I0302 19:01:22.781118 22760421793920 run.py:483] Algo bellman_ford step 7848 current loss 0.053273, current_train_items 251168.
I0302 19:01:22.813847 22760421793920 run.py:483] Algo bellman_ford step 7849 current loss 0.080987, current_train_items 251200.
I0302 19:01:22.831907 22760421793920 run.py:483] Algo bellman_ford step 7850 current loss 0.006950, current_train_items 251232.
I0302 19:01:22.839383 22760421793920 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:01:22.839494 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.993, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:01:22.867571 22760421793920 run.py:483] Algo bellman_ford step 7851 current loss 0.031712, current_train_items 251264.
I0302 19:01:22.890821 22760421793920 run.py:483] Algo bellman_ford step 7852 current loss 0.035440, current_train_items 251296.
I0302 19:01:22.923218 22760421793920 run.py:483] Algo bellman_ford step 7853 current loss 0.045968, current_train_items 251328.
I0302 19:01:22.957930 22760421793920 run.py:483] Algo bellman_ford step 7854 current loss 0.079180, current_train_items 251360.
I0302 19:01:22.976807 22760421793920 run.py:483] Algo bellman_ford step 7855 current loss 0.008134, current_train_items 251392.
I0302 19:01:22.993467 22760421793920 run.py:483] Algo bellman_ford step 7856 current loss 0.025002, current_train_items 251424.
I0302 19:01:23.017444 22760421793920 run.py:483] Algo bellman_ford step 7857 current loss 0.128619, current_train_items 251456.
I0302 19:01:23.048199 22760421793920 run.py:483] Algo bellman_ford step 7858 current loss 0.026281, current_train_items 251488.
I0302 19:01:23.081586 22760421793920 run.py:483] Algo bellman_ford step 7859 current loss 0.054583, current_train_items 251520.
I0302 19:01:23.100121 22760421793920 run.py:483] Algo bellman_ford step 7860 current loss 0.010511, current_train_items 251552.
I0302 19:01:23.115561 22760421793920 run.py:483] Algo bellman_ford step 7861 current loss 0.013758, current_train_items 251584.
I0302 19:01:23.139300 22760421793920 run.py:483] Algo bellman_ford step 7862 current loss 0.102597, current_train_items 251616.
I0302 19:01:23.170120 22760421793920 run.py:483] Algo bellman_ford step 7863 current loss 0.323962, current_train_items 251648.
I0302 19:01:23.201124 22760421793920 run.py:483] Algo bellman_ford step 7864 current loss 0.099075, current_train_items 251680.
I0302 19:01:23.219681 22760421793920 run.py:483] Algo bellman_ford step 7865 current loss 0.002512, current_train_items 251712.
I0302 19:01:23.235689 22760421793920 run.py:483] Algo bellman_ford step 7866 current loss 0.028362, current_train_items 251744.
I0302 19:01:23.260016 22760421793920 run.py:483] Algo bellman_ford step 7867 current loss 0.028262, current_train_items 251776.
I0302 19:01:23.290090 22760421793920 run.py:483] Algo bellman_ford step 7868 current loss 0.037139, current_train_items 251808.
I0302 19:01:23.321542 22760421793920 run.py:483] Algo bellman_ford step 7869 current loss 0.067263, current_train_items 251840.
I0302 19:01:23.339787 22760421793920 run.py:483] Algo bellman_ford step 7870 current loss 0.005506, current_train_items 251872.
I0302 19:01:23.355488 22760421793920 run.py:483] Algo bellman_ford step 7871 current loss 0.058105, current_train_items 251904.
I0302 19:01:23.379233 22760421793920 run.py:483] Algo bellman_ford step 7872 current loss 0.038966, current_train_items 251936.
I0302 19:01:23.409179 22760421793920 run.py:483] Algo bellman_ford step 7873 current loss 0.068203, current_train_items 251968.
I0302 19:01:23.441610 22760421793920 run.py:483] Algo bellman_ford step 7874 current loss 0.071452, current_train_items 252000.
I0302 19:01:23.459755 22760421793920 run.py:483] Algo bellman_ford step 7875 current loss 0.005657, current_train_items 252032.
I0302 19:01:23.475011 22760421793920 run.py:483] Algo bellman_ford step 7876 current loss 0.020252, current_train_items 252064.
I0302 19:01:23.498034 22760421793920 run.py:483] Algo bellman_ford step 7877 current loss 0.024791, current_train_items 252096.
I0302 19:01:23.528766 22760421793920 run.py:483] Algo bellman_ford step 7878 current loss 0.034485, current_train_items 252128.
I0302 19:01:23.561451 22760421793920 run.py:483] Algo bellman_ford step 7879 current loss 0.066424, current_train_items 252160.
I0302 19:01:23.579863 22760421793920 run.py:483] Algo bellman_ford step 7880 current loss 0.004683, current_train_items 252192.
I0302 19:01:23.595184 22760421793920 run.py:483] Algo bellman_ford step 7881 current loss 0.009163, current_train_items 252224.
I0302 19:01:23.617972 22760421793920 run.py:483] Algo bellman_ford step 7882 current loss 0.050140, current_train_items 252256.
I0302 19:01:23.648775 22760421793920 run.py:483] Algo bellman_ford step 7883 current loss 0.027478, current_train_items 252288.
I0302 19:01:23.681587 22760421793920 run.py:483] Algo bellman_ford step 7884 current loss 0.068055, current_train_items 252320.
I0302 19:01:23.700275 22760421793920 run.py:483] Algo bellman_ford step 7885 current loss 0.004420, current_train_items 252352.
I0302 19:01:23.715956 22760421793920 run.py:483] Algo bellman_ford step 7886 current loss 0.014256, current_train_items 252384.
I0302 19:01:23.738528 22760421793920 run.py:483] Algo bellman_ford step 7887 current loss 0.021792, current_train_items 252416.
I0302 19:01:23.767409 22760421793920 run.py:483] Algo bellman_ford step 7888 current loss 0.013280, current_train_items 252448.
I0302 19:01:23.799363 22760421793920 run.py:483] Algo bellman_ford step 7889 current loss 0.023275, current_train_items 252480.
I0302 19:01:23.817705 22760421793920 run.py:483] Algo bellman_ford step 7890 current loss 0.002238, current_train_items 252512.
I0302 19:01:23.833266 22760421793920 run.py:483] Algo bellman_ford step 7891 current loss 0.001950, current_train_items 252544.
I0302 19:01:23.856791 22760421793920 run.py:483] Algo bellman_ford step 7892 current loss 0.031649, current_train_items 252576.
I0302 19:01:23.888060 22760421793920 run.py:483] Algo bellman_ford step 7893 current loss 0.053675, current_train_items 252608.
I0302 19:01:23.920189 22760421793920 run.py:483] Algo bellman_ford step 7894 current loss 0.048681, current_train_items 252640.
I0302 19:01:23.938507 22760421793920 run.py:483] Algo bellman_ford step 7895 current loss 0.001890, current_train_items 252672.
I0302 19:01:23.953870 22760421793920 run.py:483] Algo bellman_ford step 7896 current loss 0.009659, current_train_items 252704.
I0302 19:01:23.977702 22760421793920 run.py:483] Algo bellman_ford step 7897 current loss 0.072028, current_train_items 252736.
I0302 19:01:24.008817 22760421793920 run.py:483] Algo bellman_ford step 7898 current loss 0.040356, current_train_items 252768.
I0302 19:01:24.041627 22760421793920 run.py:483] Algo bellman_ford step 7899 current loss 0.051903, current_train_items 252800.
I0302 19:01:24.060160 22760421793920 run.py:483] Algo bellman_ford step 7900 current loss 0.003505, current_train_items 252832.
I0302 19:01:24.067559 22760421793920 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:01:24.067671 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:01:24.084050 22760421793920 run.py:483] Algo bellman_ford step 7901 current loss 0.017328, current_train_items 252864.
I0302 19:01:24.108423 22760421793920 run.py:483] Algo bellman_ford step 7902 current loss 0.034005, current_train_items 252896.
I0302 19:01:24.139929 22760421793920 run.py:483] Algo bellman_ford step 7903 current loss 0.053137, current_train_items 252928.
I0302 19:01:24.174826 22760421793920 run.py:483] Algo bellman_ford step 7904 current loss 0.091889, current_train_items 252960.
I0302 19:01:24.193429 22760421793920 run.py:483] Algo bellman_ford step 7905 current loss 0.001712, current_train_items 252992.
I0302 19:01:24.209849 22760421793920 run.py:483] Algo bellman_ford step 7906 current loss 0.018361, current_train_items 253024.
I0302 19:01:24.234152 22760421793920 run.py:483] Algo bellman_ford step 7907 current loss 0.069133, current_train_items 253056.
I0302 19:01:24.264166 22760421793920 run.py:483] Algo bellman_ford step 7908 current loss 0.065073, current_train_items 253088.
I0302 19:01:24.294185 22760421793920 run.py:483] Algo bellman_ford step 7909 current loss 0.065153, current_train_items 253120.
I0302 19:01:24.312754 22760421793920 run.py:483] Algo bellman_ford step 7910 current loss 0.003848, current_train_items 253152.
I0302 19:01:24.329155 22760421793920 run.py:483] Algo bellman_ford step 7911 current loss 0.024023, current_train_items 253184.
I0302 19:01:24.353165 22760421793920 run.py:483] Algo bellman_ford step 7912 current loss 0.035071, current_train_items 253216.
I0302 19:01:24.383966 22760421793920 run.py:483] Algo bellman_ford step 7913 current loss 0.063347, current_train_items 253248.
I0302 19:01:24.416239 22760421793920 run.py:483] Algo bellman_ford step 7914 current loss 0.136829, current_train_items 253280.
I0302 19:01:24.434823 22760421793920 run.py:483] Algo bellman_ford step 7915 current loss 0.009190, current_train_items 253312.
I0302 19:01:24.450810 22760421793920 run.py:483] Algo bellman_ford step 7916 current loss 0.003604, current_train_items 253344.
I0302 19:01:24.474049 22760421793920 run.py:483] Algo bellman_ford step 7917 current loss 0.047748, current_train_items 253376.
I0302 19:01:24.504855 22760421793920 run.py:483] Algo bellman_ford step 7918 current loss 0.027614, current_train_items 253408.
I0302 19:01:24.538694 22760421793920 run.py:483] Algo bellman_ford step 7919 current loss 0.045030, current_train_items 253440.
I0302 19:01:24.557199 22760421793920 run.py:483] Algo bellman_ford step 7920 current loss 0.001521, current_train_items 253472.
I0302 19:01:24.573225 22760421793920 run.py:483] Algo bellman_ford step 7921 current loss 0.005530, current_train_items 253504.
I0302 19:01:24.597143 22760421793920 run.py:483] Algo bellman_ford step 7922 current loss 0.048243, current_train_items 253536.
I0302 19:01:24.627450 22760421793920 run.py:483] Algo bellman_ford step 7923 current loss 0.027684, current_train_items 253568.
I0302 19:01:24.660332 22760421793920 run.py:483] Algo bellman_ford step 7924 current loss 0.054294, current_train_items 253600.
I0302 19:01:24.678294 22760421793920 run.py:483] Algo bellman_ford step 7925 current loss 0.005937, current_train_items 253632.
I0302 19:01:24.694342 22760421793920 run.py:483] Algo bellman_ford step 7926 current loss 0.011045, current_train_items 253664.
I0302 19:01:24.717462 22760421793920 run.py:483] Algo bellman_ford step 7927 current loss 0.038529, current_train_items 253696.
I0302 19:01:24.749334 22760421793920 run.py:483] Algo bellman_ford step 7928 current loss 0.049467, current_train_items 253728.
I0302 19:01:24.780766 22760421793920 run.py:483] Algo bellman_ford step 7929 current loss 0.034070, current_train_items 253760.
I0302 19:01:24.799468 22760421793920 run.py:483] Algo bellman_ford step 7930 current loss 0.004308, current_train_items 253792.
I0302 19:01:24.815259 22760421793920 run.py:483] Algo bellman_ford step 7931 current loss 0.017767, current_train_items 253824.
I0302 19:01:24.837717 22760421793920 run.py:483] Algo bellman_ford step 7932 current loss 0.035449, current_train_items 253856.
I0302 19:01:24.869010 22760421793920 run.py:483] Algo bellman_ford step 7933 current loss 0.025035, current_train_items 253888.
I0302 19:01:24.902107 22760421793920 run.py:483] Algo bellman_ford step 7934 current loss 0.058425, current_train_items 253920.
I0302 19:01:24.920541 22760421793920 run.py:483] Algo bellman_ford step 7935 current loss 0.000887, current_train_items 253952.
I0302 19:01:24.935967 22760421793920 run.py:483] Algo bellman_ford step 7936 current loss 0.011431, current_train_items 253984.
I0302 19:01:24.960142 22760421793920 run.py:483] Algo bellman_ford step 7937 current loss 0.027225, current_train_items 254016.
I0302 19:01:24.990360 22760421793920 run.py:483] Algo bellman_ford step 7938 current loss 0.051309, current_train_items 254048.
I0302 19:01:25.023618 22760421793920 run.py:483] Algo bellman_ford step 7939 current loss 0.102972, current_train_items 254080.
I0302 19:01:25.042017 22760421793920 run.py:483] Algo bellman_ford step 7940 current loss 0.004199, current_train_items 254112.
I0302 19:01:25.057275 22760421793920 run.py:483] Algo bellman_ford step 7941 current loss 0.004359, current_train_items 254144.
I0302 19:01:25.081527 22760421793920 run.py:483] Algo bellman_ford step 7942 current loss 0.097686, current_train_items 254176.
I0302 19:01:25.113581 22760421793920 run.py:483] Algo bellman_ford step 7943 current loss 0.182301, current_train_items 254208.
I0302 19:01:25.145278 22760421793920 run.py:483] Algo bellman_ford step 7944 current loss 0.199597, current_train_items 254240.
I0302 19:01:25.163376 22760421793920 run.py:483] Algo bellman_ford step 7945 current loss 0.003822, current_train_items 254272.
I0302 19:01:25.178640 22760421793920 run.py:483] Algo bellman_ford step 7946 current loss 0.021931, current_train_items 254304.
I0302 19:01:25.201215 22760421793920 run.py:483] Algo bellman_ford step 7947 current loss 0.064882, current_train_items 254336.
I0302 19:01:25.230759 22760421793920 run.py:483] Algo bellman_ford step 7948 current loss 0.024570, current_train_items 254368.
I0302 19:01:25.263791 22760421793920 run.py:483] Algo bellman_ford step 7949 current loss 0.081644, current_train_items 254400.
I0302 19:01:25.282233 22760421793920 run.py:483] Algo bellman_ford step 7950 current loss 0.002416, current_train_items 254432.
I0302 19:01:25.289685 22760421793920 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:01:25.289794 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 19:01:25.306415 22760421793920 run.py:483] Algo bellman_ford step 7951 current loss 0.011822, current_train_items 254464.
I0302 19:01:25.330581 22760421793920 run.py:483] Algo bellman_ford step 7952 current loss 0.104534, current_train_items 254496.
I0302 19:01:25.362542 22760421793920 run.py:483] Algo bellman_ford step 7953 current loss 0.073544, current_train_items 254528.
I0302 19:01:25.394978 22760421793920 run.py:483] Algo bellman_ford step 7954 current loss 0.098705, current_train_items 254560.
I0302 19:01:25.413708 22760421793920 run.py:483] Algo bellman_ford step 7955 current loss 0.006868, current_train_items 254592.
I0302 19:01:25.430037 22760421793920 run.py:483] Algo bellman_ford step 7956 current loss 0.042422, current_train_items 254624.
I0302 19:01:25.453724 22760421793920 run.py:483] Algo bellman_ford step 7957 current loss 0.054764, current_train_items 254656.
I0302 19:01:25.484682 22760421793920 run.py:483] Algo bellman_ford step 7958 current loss 0.112371, current_train_items 254688.
I0302 19:01:25.516815 22760421793920 run.py:483] Algo bellman_ford step 7959 current loss 0.028029, current_train_items 254720.
I0302 19:01:25.535154 22760421793920 run.py:483] Algo bellman_ford step 7960 current loss 0.010188, current_train_items 254752.
I0302 19:01:25.551206 22760421793920 run.py:483] Algo bellman_ford step 7961 current loss 0.017950, current_train_items 254784.
I0302 19:01:25.574781 22760421793920 run.py:483] Algo bellman_ford step 7962 current loss 0.048201, current_train_items 254816.
I0302 19:01:25.604367 22760421793920 run.py:483] Algo bellman_ford step 7963 current loss 0.051912, current_train_items 254848.
I0302 19:01:25.637320 22760421793920 run.py:483] Algo bellman_ford step 7964 current loss 0.058520, current_train_items 254880.
I0302 19:01:25.655813 22760421793920 run.py:483] Algo bellman_ford step 7965 current loss 0.001617, current_train_items 254912.
I0302 19:01:25.671937 22760421793920 run.py:483] Algo bellman_ford step 7966 current loss 0.009631, current_train_items 254944.
I0302 19:01:25.696317 22760421793920 run.py:483] Algo bellman_ford step 7967 current loss 0.068271, current_train_items 254976.
I0302 19:01:25.726772 22760421793920 run.py:483] Algo bellman_ford step 7968 current loss 0.045533, current_train_items 255008.
I0302 19:01:25.757645 22760421793920 run.py:483] Algo bellman_ford step 7969 current loss 0.055353, current_train_items 255040.
I0302 19:01:25.776136 22760421793920 run.py:483] Algo bellman_ford step 7970 current loss 0.012064, current_train_items 255072.
I0302 19:01:25.791754 22760421793920 run.py:483] Algo bellman_ford step 7971 current loss 0.046796, current_train_items 255104.
I0302 19:01:25.815856 22760421793920 run.py:483] Algo bellman_ford step 7972 current loss 0.074193, current_train_items 255136.
I0302 19:01:25.845914 22760421793920 run.py:483] Algo bellman_ford step 7973 current loss 0.034442, current_train_items 255168.
I0302 19:01:25.879731 22760421793920 run.py:483] Algo bellman_ford step 7974 current loss 0.058216, current_train_items 255200.
I0302 19:01:25.897933 22760421793920 run.py:483] Algo bellman_ford step 7975 current loss 0.012543, current_train_items 255232.
I0302 19:01:25.913829 22760421793920 run.py:483] Algo bellman_ford step 7976 current loss 0.034081, current_train_items 255264.
I0302 19:01:25.937804 22760421793920 run.py:483] Algo bellman_ford step 7977 current loss 0.031497, current_train_items 255296.
I0302 19:01:25.968613 22760421793920 run.py:483] Algo bellman_ford step 7978 current loss 0.046119, current_train_items 255328.
I0302 19:01:25.998934 22760421793920 run.py:483] Algo bellman_ford step 7979 current loss 0.041604, current_train_items 255360.
I0302 19:01:26.017214 22760421793920 run.py:483] Algo bellman_ford step 7980 current loss 0.003307, current_train_items 255392.
I0302 19:01:26.032857 22760421793920 run.py:483] Algo bellman_ford step 7981 current loss 0.017337, current_train_items 255424.
I0302 19:01:26.055335 22760421793920 run.py:483] Algo bellman_ford step 7982 current loss 0.049636, current_train_items 255456.
I0302 19:01:26.085416 22760421793920 run.py:483] Algo bellman_ford step 7983 current loss 0.041075, current_train_items 255488.
I0302 19:01:26.118240 22760421793920 run.py:483] Algo bellman_ford step 7984 current loss 0.072538, current_train_items 255520.
I0302 19:01:26.136725 22760421793920 run.py:483] Algo bellman_ford step 7985 current loss 0.006942, current_train_items 255552.
I0302 19:01:26.152637 22760421793920 run.py:483] Algo bellman_ford step 7986 current loss 0.022165, current_train_items 255584.
I0302 19:01:26.175388 22760421793920 run.py:483] Algo bellman_ford step 7987 current loss 0.036545, current_train_items 255616.
I0302 19:01:26.206628 22760421793920 run.py:483] Algo bellman_ford step 7988 current loss 0.026180, current_train_items 255648.
I0302 19:01:26.238111 22760421793920 run.py:483] Algo bellman_ford step 7989 current loss 0.047156, current_train_items 255680.
I0302 19:01:26.256375 22760421793920 run.py:483] Algo bellman_ford step 7990 current loss 0.003634, current_train_items 255712.
I0302 19:01:26.272170 22760421793920 run.py:483] Algo bellman_ford step 7991 current loss 0.019445, current_train_items 255744.
I0302 19:01:26.295823 22760421793920 run.py:483] Algo bellman_ford step 7992 current loss 0.031739, current_train_items 255776.
I0302 19:01:26.328062 22760421793920 run.py:483] Algo bellman_ford step 7993 current loss 0.047162, current_train_items 255808.
I0302 19:01:26.361348 22760421793920 run.py:483] Algo bellman_ford step 7994 current loss 0.037386, current_train_items 255840.
I0302 19:01:26.379536 22760421793920 run.py:483] Algo bellman_ford step 7995 current loss 0.002624, current_train_items 255872.
I0302 19:01:26.394894 22760421793920 run.py:483] Algo bellman_ford step 7996 current loss 0.006217, current_train_items 255904.
I0302 19:01:26.418167 22760421793920 run.py:483] Algo bellman_ford step 7997 current loss 0.038750, current_train_items 255936.
I0302 19:01:26.448568 22760421793920 run.py:483] Algo bellman_ford step 7998 current loss 0.024056, current_train_items 255968.
I0302 19:01:26.478920 22760421793920 run.py:483] Algo bellman_ford step 7999 current loss 0.053852, current_train_items 256000.
I0302 19:01:26.497378 22760421793920 run.py:483] Algo bellman_ford step 8000 current loss 0.001421, current_train_items 256032.
I0302 19:01:26.505092 22760421793920 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:01:26.505203 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:01:26.521811 22760421793920 run.py:483] Algo bellman_ford step 8001 current loss 0.007069, current_train_items 256064.
I0302 19:01:26.544873 22760421793920 run.py:483] Algo bellman_ford step 8002 current loss 0.062916, current_train_items 256096.
I0302 19:01:26.576454 22760421793920 run.py:483] Algo bellman_ford step 8003 current loss 0.053676, current_train_items 256128.
I0302 19:01:26.607838 22760421793920 run.py:483] Algo bellman_ford step 8004 current loss 0.061224, current_train_items 256160.
I0302 19:01:26.626645 22760421793920 run.py:483] Algo bellman_ford step 8005 current loss 0.001338, current_train_items 256192.
I0302 19:01:26.642490 22760421793920 run.py:483] Algo bellman_ford step 8006 current loss 0.005821, current_train_items 256224.
I0302 19:01:26.666025 22760421793920 run.py:483] Algo bellman_ford step 8007 current loss 0.067339, current_train_items 256256.
I0302 19:01:26.698265 22760421793920 run.py:483] Algo bellman_ford step 8008 current loss 0.067601, current_train_items 256288.
I0302 19:01:26.730740 22760421793920 run.py:483] Algo bellman_ford step 8009 current loss 0.112306, current_train_items 256320.
I0302 19:01:26.749164 22760421793920 run.py:483] Algo bellman_ford step 8010 current loss 0.001920, current_train_items 256352.
I0302 19:01:26.764659 22760421793920 run.py:483] Algo bellman_ford step 8011 current loss 0.015122, current_train_items 256384.
I0302 19:01:26.788755 22760421793920 run.py:483] Algo bellman_ford step 8012 current loss 0.025359, current_train_items 256416.
I0302 19:01:26.818965 22760421793920 run.py:483] Algo bellman_ford step 8013 current loss 0.050299, current_train_items 256448.
I0302 19:01:26.849435 22760421793920 run.py:483] Algo bellman_ford step 8014 current loss 0.064977, current_train_items 256480.
I0302 19:01:26.868133 22760421793920 run.py:483] Algo bellman_ford step 8015 current loss 0.023664, current_train_items 256512.
I0302 19:01:26.884453 22760421793920 run.py:483] Algo bellman_ford step 8016 current loss 0.030133, current_train_items 256544.
I0302 19:01:26.907059 22760421793920 run.py:483] Algo bellman_ford step 8017 current loss 0.040330, current_train_items 256576.
I0302 19:01:26.937302 22760421793920 run.py:483] Algo bellman_ford step 8018 current loss 0.110117, current_train_items 256608.
I0302 19:01:26.968345 22760421793920 run.py:483] Algo bellman_ford step 8019 current loss 0.028441, current_train_items 256640.
I0302 19:01:26.986569 22760421793920 run.py:483] Algo bellman_ford step 8020 current loss 0.001489, current_train_items 256672.
I0302 19:01:27.002371 22760421793920 run.py:483] Algo bellman_ford step 8021 current loss 0.005215, current_train_items 256704.
I0302 19:01:27.025183 22760421793920 run.py:483] Algo bellman_ford step 8022 current loss 0.044153, current_train_items 256736.
I0302 19:01:27.054806 22760421793920 run.py:483] Algo bellman_ford step 8023 current loss 0.062288, current_train_items 256768.
I0302 19:01:27.088284 22760421793920 run.py:483] Algo bellman_ford step 8024 current loss 0.062501, current_train_items 256800.
I0302 19:01:27.106646 22760421793920 run.py:483] Algo bellman_ford step 8025 current loss 0.007771, current_train_items 256832.
I0302 19:01:27.122662 22760421793920 run.py:483] Algo bellman_ford step 8026 current loss 0.004462, current_train_items 256864.
I0302 19:01:27.146072 22760421793920 run.py:483] Algo bellman_ford step 8027 current loss 0.021419, current_train_items 256896.
I0302 19:01:27.175825 22760421793920 run.py:483] Algo bellman_ford step 8028 current loss 0.026508, current_train_items 256928.
I0302 19:01:27.208105 22760421793920 run.py:483] Algo bellman_ford step 8029 current loss 0.115808, current_train_items 256960.
I0302 19:01:27.226252 22760421793920 run.py:483] Algo bellman_ford step 8030 current loss 0.005619, current_train_items 256992.
I0302 19:01:27.241892 22760421793920 run.py:483] Algo bellman_ford step 8031 current loss 0.039386, current_train_items 257024.
I0302 19:01:27.265660 22760421793920 run.py:483] Algo bellman_ford step 8032 current loss 0.049296, current_train_items 257056.
I0302 19:01:27.296697 22760421793920 run.py:483] Algo bellman_ford step 8033 current loss 0.053043, current_train_items 257088.
I0302 19:01:27.329918 22760421793920 run.py:483] Algo bellman_ford step 8034 current loss 0.049080, current_train_items 257120.
I0302 19:01:27.348416 22760421793920 run.py:483] Algo bellman_ford step 8035 current loss 0.002854, current_train_items 257152.
I0302 19:01:27.364593 22760421793920 run.py:483] Algo bellman_ford step 8036 current loss 0.027762, current_train_items 257184.
I0302 19:01:27.388116 22760421793920 run.py:483] Algo bellman_ford step 8037 current loss 0.048825, current_train_items 257216.
I0302 19:01:27.418331 22760421793920 run.py:483] Algo bellman_ford step 8038 current loss 0.027458, current_train_items 257248.
I0302 19:01:27.451821 22760421793920 run.py:483] Algo bellman_ford step 8039 current loss 0.068846, current_train_items 257280.
I0302 19:01:27.469949 22760421793920 run.py:483] Algo bellman_ford step 8040 current loss 0.001845, current_train_items 257312.
I0302 19:01:27.485975 22760421793920 run.py:483] Algo bellman_ford step 8041 current loss 0.023559, current_train_items 257344.
I0302 19:01:27.509492 22760421793920 run.py:483] Algo bellman_ford step 8042 current loss 0.058003, current_train_items 257376.
I0302 19:01:27.540479 22760421793920 run.py:483] Algo bellman_ford step 8043 current loss 0.052874, current_train_items 257408.
I0302 19:01:27.573000 22760421793920 run.py:483] Algo bellman_ford step 8044 current loss 0.076958, current_train_items 257440.
I0302 19:01:27.591831 22760421793920 run.py:483] Algo bellman_ford step 8045 current loss 0.003476, current_train_items 257472.
I0302 19:01:27.608100 22760421793920 run.py:483] Algo bellman_ford step 8046 current loss 0.024479, current_train_items 257504.
I0302 19:01:27.631358 22760421793920 run.py:483] Algo bellman_ford step 8047 current loss 0.031793, current_train_items 257536.
I0302 19:01:27.662264 22760421793920 run.py:483] Algo bellman_ford step 8048 current loss 0.087822, current_train_items 257568.
I0302 19:01:27.693150 22760421793920 run.py:483] Algo bellman_ford step 8049 current loss 0.058507, current_train_items 257600.
I0302 19:01:27.711226 22760421793920 run.py:483] Algo bellman_ford step 8050 current loss 0.008419, current_train_items 257632.
I0302 19:01:27.718751 22760421793920 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:01:27.718861 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:01:27.735380 22760421793920 run.py:483] Algo bellman_ford step 8051 current loss 0.032193, current_train_items 257664.
I0302 19:01:27.758758 22760421793920 run.py:483] Algo bellman_ford step 8052 current loss 0.015385, current_train_items 257696.
I0302 19:01:27.789646 22760421793920 run.py:483] Algo bellman_ford step 8053 current loss 0.060669, current_train_items 257728.
I0302 19:01:27.823305 22760421793920 run.py:483] Algo bellman_ford step 8054 current loss 0.164193, current_train_items 257760.
I0302 19:01:27.842069 22760421793920 run.py:483] Algo bellman_ford step 8055 current loss 0.009555, current_train_items 257792.
I0302 19:01:27.857486 22760421793920 run.py:483] Algo bellman_ford step 8056 current loss 0.002615, current_train_items 257824.
I0302 19:01:27.879443 22760421793920 run.py:483] Algo bellman_ford step 8057 current loss 0.031691, current_train_items 257856.
I0302 19:01:27.911541 22760421793920 run.py:483] Algo bellman_ford step 8058 current loss 0.039353, current_train_items 257888.
I0302 19:01:27.945573 22760421793920 run.py:483] Algo bellman_ford step 8059 current loss 0.063167, current_train_items 257920.
I0302 19:01:27.963632 22760421793920 run.py:483] Algo bellman_ford step 8060 current loss 0.013309, current_train_items 257952.
I0302 19:01:27.979059 22760421793920 run.py:483] Algo bellman_ford step 8061 current loss 0.016206, current_train_items 257984.
I0302 19:01:28.003799 22760421793920 run.py:483] Algo bellman_ford step 8062 current loss 0.059661, current_train_items 258016.
I0302 19:01:28.034693 22760421793920 run.py:483] Algo bellman_ford step 8063 current loss 0.097320, current_train_items 258048.
I0302 19:01:28.065696 22760421793920 run.py:483] Algo bellman_ford step 8064 current loss 0.112470, current_train_items 258080.
I0302 19:01:28.083818 22760421793920 run.py:483] Algo bellman_ford step 8065 current loss 0.002618, current_train_items 258112.
I0302 19:01:28.099331 22760421793920 run.py:483] Algo bellman_ford step 8066 current loss 0.042909, current_train_items 258144.
I0302 19:01:28.122654 22760421793920 run.py:483] Algo bellman_ford step 8067 current loss 0.044162, current_train_items 258176.
I0302 19:01:28.152971 22760421793920 run.py:483] Algo bellman_ford step 8068 current loss 0.044825, current_train_items 258208.
I0302 19:01:28.183819 22760421793920 run.py:483] Algo bellman_ford step 8069 current loss 0.069408, current_train_items 258240.
I0302 19:01:28.202174 22760421793920 run.py:483] Algo bellman_ford step 8070 current loss 0.019123, current_train_items 258272.
I0302 19:01:28.218626 22760421793920 run.py:483] Algo bellman_ford step 8071 current loss 0.054135, current_train_items 258304.
I0302 19:01:28.242126 22760421793920 run.py:483] Algo bellman_ford step 8072 current loss 0.057568, current_train_items 258336.
I0302 19:01:28.272966 22760421793920 run.py:483] Algo bellman_ford step 8073 current loss 0.043319, current_train_items 258368.
I0302 19:01:28.305960 22760421793920 run.py:483] Algo bellman_ford step 8074 current loss 0.056200, current_train_items 258400.
I0302 19:01:28.324280 22760421793920 run.py:483] Algo bellman_ford step 8075 current loss 0.003339, current_train_items 258432.
I0302 19:01:28.339944 22760421793920 run.py:483] Algo bellman_ford step 8076 current loss 0.044998, current_train_items 258464.
I0302 19:01:28.363008 22760421793920 run.py:483] Algo bellman_ford step 8077 current loss 0.025718, current_train_items 258496.
I0302 19:01:28.392974 22760421793920 run.py:483] Algo bellman_ford step 8078 current loss 0.117706, current_train_items 258528.
I0302 19:01:28.423257 22760421793920 run.py:483] Algo bellman_ford step 8079 current loss 0.086555, current_train_items 258560.
I0302 19:01:28.441585 22760421793920 run.py:483] Algo bellman_ford step 8080 current loss 0.011641, current_train_items 258592.
I0302 19:01:28.457404 22760421793920 run.py:483] Algo bellman_ford step 8081 current loss 0.018382, current_train_items 258624.
I0302 19:01:28.480440 22760421793920 run.py:483] Algo bellman_ford step 8082 current loss 0.088980, current_train_items 258656.
I0302 19:01:28.511518 22760421793920 run.py:483] Algo bellman_ford step 8083 current loss 0.079682, current_train_items 258688.
I0302 19:01:28.546548 22760421793920 run.py:483] Algo bellman_ford step 8084 current loss 0.106701, current_train_items 258720.
I0302 19:01:28.564722 22760421793920 run.py:483] Algo bellman_ford step 8085 current loss 0.005959, current_train_items 258752.
I0302 19:01:28.580849 22760421793920 run.py:483] Algo bellman_ford step 8086 current loss 0.024670, current_train_items 258784.
I0302 19:01:28.603886 22760421793920 run.py:483] Algo bellman_ford step 8087 current loss 0.026431, current_train_items 258816.
I0302 19:01:28.635946 22760421793920 run.py:483] Algo bellman_ford step 8088 current loss 0.102443, current_train_items 258848.
I0302 19:01:28.666541 22760421793920 run.py:483] Algo bellman_ford step 8089 current loss 0.090240, current_train_items 258880.
I0302 19:01:28.684841 22760421793920 run.py:483] Algo bellman_ford step 8090 current loss 0.003594, current_train_items 258912.
I0302 19:01:28.700098 22760421793920 run.py:483] Algo bellman_ford step 8091 current loss 0.010260, current_train_items 258944.
I0302 19:01:28.724228 22760421793920 run.py:483] Algo bellman_ford step 8092 current loss 0.023808, current_train_items 258976.
I0302 19:01:28.754212 22760421793920 run.py:483] Algo bellman_ford step 8093 current loss 0.028917, current_train_items 259008.
I0302 19:01:28.786768 22760421793920 run.py:483] Algo bellman_ford step 8094 current loss 0.042727, current_train_items 259040.
I0302 19:01:28.805304 22760421793920 run.py:483] Algo bellman_ford step 8095 current loss 0.011461, current_train_items 259072.
I0302 19:01:28.820842 22760421793920 run.py:483] Algo bellman_ford step 8096 current loss 0.017849, current_train_items 259104.
I0302 19:01:28.845257 22760421793920 run.py:483] Algo bellman_ford step 8097 current loss 0.073390, current_train_items 259136.
I0302 19:01:28.876799 22760421793920 run.py:483] Algo bellman_ford step 8098 current loss 0.059941, current_train_items 259168.
I0302 19:01:28.908794 22760421793920 run.py:483] Algo bellman_ford step 8099 current loss 0.034072, current_train_items 259200.
I0302 19:01:28.927003 22760421793920 run.py:483] Algo bellman_ford step 8100 current loss 0.004552, current_train_items 259232.
I0302 19:01:28.934546 22760421793920 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.9736328125, 'score': 0.9736328125, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:01:28.934657 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.994, current avg val score is 0.974, val scores are: bellman_ford: 0.974
I0302 19:01:28.951286 22760421793920 run.py:483] Algo bellman_ford step 8101 current loss 0.010163, current_train_items 259264.
I0302 19:01:28.976505 22760421793920 run.py:483] Algo bellman_ford step 8102 current loss 0.093443, current_train_items 259296.
I0302 19:01:29.007536 22760421793920 run.py:483] Algo bellman_ford step 8103 current loss 0.063067, current_train_items 259328.
I0302 19:01:29.039909 22760421793920 run.py:483] Algo bellman_ford step 8104 current loss 0.091742, current_train_items 259360.
I0302 19:01:29.058704 22760421793920 run.py:483] Algo bellman_ford step 8105 current loss 0.012159, current_train_items 259392.
I0302 19:01:29.074608 22760421793920 run.py:483] Algo bellman_ford step 8106 current loss 0.027286, current_train_items 259424.
I0302 19:01:29.097700 22760421793920 run.py:483] Algo bellman_ford step 8107 current loss 0.057962, current_train_items 259456.
I0302 19:01:29.128039 22760421793920 run.py:483] Algo bellman_ford step 8108 current loss 0.083883, current_train_items 259488.
I0302 19:01:29.161202 22760421793920 run.py:483] Algo bellman_ford step 8109 current loss 0.088206, current_train_items 259520.
I0302 19:01:29.179925 22760421793920 run.py:483] Algo bellman_ford step 8110 current loss 0.011353, current_train_items 259552.
I0302 19:01:29.195245 22760421793920 run.py:483] Algo bellman_ford step 8111 current loss 0.019773, current_train_items 259584.
I0302 19:01:29.218192 22760421793920 run.py:483] Algo bellman_ford step 8112 current loss 0.031676, current_train_items 259616.
I0302 19:01:29.247355 22760421793920 run.py:483] Algo bellman_ford step 8113 current loss 0.043697, current_train_items 259648.
I0302 19:01:29.281522 22760421793920 run.py:483] Algo bellman_ford step 8114 current loss 0.096057, current_train_items 259680.
I0302 19:01:29.300055 22760421793920 run.py:483] Algo bellman_ford step 8115 current loss 0.004627, current_train_items 259712.
I0302 19:01:29.315925 22760421793920 run.py:483] Algo bellman_ford step 8116 current loss 0.018335, current_train_items 259744.
I0302 19:01:29.339597 22760421793920 run.py:483] Algo bellman_ford step 8117 current loss 0.030458, current_train_items 259776.
I0302 19:01:29.370563 22760421793920 run.py:483] Algo bellman_ford step 8118 current loss 0.057476, current_train_items 259808.
I0302 19:01:29.402497 22760421793920 run.py:483] Algo bellman_ford step 8119 current loss 0.065175, current_train_items 259840.
I0302 19:01:29.420968 22760421793920 run.py:483] Algo bellman_ford step 8120 current loss 0.007078, current_train_items 259872.
I0302 19:01:29.436803 22760421793920 run.py:483] Algo bellman_ford step 8121 current loss 0.005153, current_train_items 259904.
I0302 19:01:29.460178 22760421793920 run.py:483] Algo bellman_ford step 8122 current loss 0.030368, current_train_items 259936.
I0302 19:01:29.491748 22760421793920 run.py:483] Algo bellman_ford step 8123 current loss 0.039379, current_train_items 259968.
I0302 19:01:29.522393 22760421793920 run.py:483] Algo bellman_ford step 8124 current loss 0.035656, current_train_items 260000.
I0302 19:01:29.540557 22760421793920 run.py:483] Algo bellman_ford step 8125 current loss 0.002476, current_train_items 260032.
I0302 19:01:29.556603 22760421793920 run.py:483] Algo bellman_ford step 8126 current loss 0.026146, current_train_items 260064.
I0302 19:01:29.580881 22760421793920 run.py:483] Algo bellman_ford step 8127 current loss 0.028497, current_train_items 260096.
I0302 19:01:29.612087 22760421793920 run.py:483] Algo bellman_ford step 8128 current loss 0.052339, current_train_items 260128.
I0302 19:01:29.646745 22760421793920 run.py:483] Algo bellman_ford step 8129 current loss 0.105951, current_train_items 260160.
I0302 19:01:29.665127 22760421793920 run.py:483] Algo bellman_ford step 8130 current loss 0.006302, current_train_items 260192.
I0302 19:01:29.680567 22760421793920 run.py:483] Algo bellman_ford step 8131 current loss 0.013528, current_train_items 260224.
I0302 19:01:29.703930 22760421793920 run.py:483] Algo bellman_ford step 8132 current loss 0.089795, current_train_items 260256.
I0302 19:01:29.734606 22760421793920 run.py:483] Algo bellman_ford step 8133 current loss 0.124350, current_train_items 260288.
I0302 19:01:29.767338 22760421793920 run.py:483] Algo bellman_ford step 8134 current loss 0.107838, current_train_items 260320.
I0302 19:01:29.785763 22760421793920 run.py:483] Algo bellman_ford step 8135 current loss 0.004012, current_train_items 260352.
I0302 19:01:29.801282 22760421793920 run.py:483] Algo bellman_ford step 8136 current loss 0.020776, current_train_items 260384.
I0302 19:01:29.824495 22760421793920 run.py:483] Algo bellman_ford step 8137 current loss 0.048243, current_train_items 260416.
I0302 19:01:29.855447 22760421793920 run.py:483] Algo bellman_ford step 8138 current loss 0.035502, current_train_items 260448.
I0302 19:01:29.888984 22760421793920 run.py:483] Algo bellman_ford step 8139 current loss 0.081938, current_train_items 260480.
I0302 19:01:29.907108 22760421793920 run.py:483] Algo bellman_ford step 8140 current loss 0.001799, current_train_items 260512.
I0302 19:01:29.922578 22760421793920 run.py:483] Algo bellman_ford step 8141 current loss 0.028933, current_train_items 260544.
I0302 19:01:29.946968 22760421793920 run.py:483] Algo bellman_ford step 8142 current loss 0.055051, current_train_items 260576.
I0302 19:01:29.978122 22760421793920 run.py:483] Algo bellman_ford step 8143 current loss 0.026100, current_train_items 260608.
I0302 19:01:30.011011 22760421793920 run.py:483] Algo bellman_ford step 8144 current loss 0.057804, current_train_items 260640.
I0302 19:01:30.029288 22760421793920 run.py:483] Algo bellman_ford step 8145 current loss 0.001302, current_train_items 260672.
I0302 19:01:30.045345 22760421793920 run.py:483] Algo bellman_ford step 8146 current loss 0.012384, current_train_items 260704.
I0302 19:01:30.068603 22760421793920 run.py:483] Algo bellman_ford step 8147 current loss 0.029568, current_train_items 260736.
I0302 19:01:30.099764 22760421793920 run.py:483] Algo bellman_ford step 8148 current loss 0.086759, current_train_items 260768.
I0302 19:01:30.130999 22760421793920 run.py:483] Algo bellman_ford step 8149 current loss 0.039801, current_train_items 260800.
I0302 19:01:30.149610 22760421793920 run.py:483] Algo bellman_ford step 8150 current loss 0.002879, current_train_items 260832.
I0302 19:01:30.157133 22760421793920 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:01:30.157242 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.994, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:01:30.185134 22760421793920 run.py:483] Algo bellman_ford step 8151 current loss 0.004833, current_train_items 260864.
I0302 19:01:30.209124 22760421793920 run.py:483] Algo bellman_ford step 8152 current loss 0.032989, current_train_items 260896.
I0302 19:01:30.238685 22760421793920 run.py:483] Algo bellman_ford step 8153 current loss 0.013279, current_train_items 260928.
I0302 19:01:30.273017 22760421793920 run.py:483] Algo bellman_ford step 8154 current loss 0.048253, current_train_items 260960.
I0302 19:01:30.292108 22760421793920 run.py:483] Algo bellman_ford step 8155 current loss 0.002669, current_train_items 260992.
I0302 19:01:30.307958 22760421793920 run.py:483] Algo bellman_ford step 8156 current loss 0.020615, current_train_items 261024.
I0302 19:01:30.331310 22760421793920 run.py:483] Algo bellman_ford step 8157 current loss 0.035811, current_train_items 261056.
I0302 19:01:30.361829 22760421793920 run.py:483] Algo bellman_ford step 8158 current loss 0.052170, current_train_items 261088.
I0302 19:01:30.393565 22760421793920 run.py:483] Algo bellman_ford step 8159 current loss 0.056499, current_train_items 261120.
I0302 19:01:30.411941 22760421793920 run.py:483] Algo bellman_ford step 8160 current loss 0.001168, current_train_items 261152.
I0302 19:01:30.427783 22760421793920 run.py:483] Algo bellman_ford step 8161 current loss 0.025734, current_train_items 261184.
I0302 19:01:30.451145 22760421793920 run.py:483] Algo bellman_ford step 8162 current loss 0.098847, current_train_items 261216.
I0302 19:01:30.482278 22760421793920 run.py:483] Algo bellman_ford step 8163 current loss 0.047828, current_train_items 261248.
I0302 19:01:30.511198 22760421793920 run.py:483] Algo bellman_ford step 8164 current loss 0.023877, current_train_items 261280.
I0302 19:01:30.529432 22760421793920 run.py:483] Algo bellman_ford step 8165 current loss 0.002001, current_train_items 261312.
I0302 19:01:30.545196 22760421793920 run.py:483] Algo bellman_ford step 8166 current loss 0.018745, current_train_items 261344.
I0302 19:01:30.568379 22760421793920 run.py:483] Algo bellman_ford step 8167 current loss 0.127392, current_train_items 261376.
I0302 19:01:30.598618 22760421793920 run.py:483] Algo bellman_ford step 8168 current loss 0.125436, current_train_items 261408.
I0302 19:01:30.631543 22760421793920 run.py:483] Algo bellman_ford step 8169 current loss 0.227553, current_train_items 261440.
I0302 19:01:30.649539 22760421793920 run.py:483] Algo bellman_ford step 8170 current loss 0.004473, current_train_items 261472.
I0302 19:01:30.665256 22760421793920 run.py:483] Algo bellman_ford step 8171 current loss 0.010110, current_train_items 261504.
I0302 19:01:30.688993 22760421793920 run.py:483] Algo bellman_ford step 8172 current loss 0.055318, current_train_items 261536.
I0302 19:01:30.719889 22760421793920 run.py:483] Algo bellman_ford step 8173 current loss 0.037975, current_train_items 261568.
I0302 19:01:30.750999 22760421793920 run.py:483] Algo bellman_ford step 8174 current loss 0.073932, current_train_items 261600.
I0302 19:01:30.769484 22760421793920 run.py:483] Algo bellman_ford step 8175 current loss 0.002844, current_train_items 261632.
I0302 19:01:30.785608 22760421793920 run.py:483] Algo bellman_ford step 8176 current loss 0.009003, current_train_items 261664.
I0302 19:01:30.808840 22760421793920 run.py:483] Algo bellman_ford step 8177 current loss 0.046513, current_train_items 261696.
I0302 19:01:30.840608 22760421793920 run.py:483] Algo bellman_ford step 8178 current loss 0.042023, current_train_items 261728.
I0302 19:01:30.870425 22760421793920 run.py:483] Algo bellman_ford step 8179 current loss 0.037014, current_train_items 261760.
I0302 19:01:30.888904 22760421793920 run.py:483] Algo bellman_ford step 8180 current loss 0.001316, current_train_items 261792.
I0302 19:01:30.904567 22760421793920 run.py:483] Algo bellman_ford step 8181 current loss 0.008987, current_train_items 261824.
I0302 19:01:30.927637 22760421793920 run.py:483] Algo bellman_ford step 8182 current loss 0.032807, current_train_items 261856.
I0302 19:01:30.958805 22760421793920 run.py:483] Algo bellman_ford step 8183 current loss 0.037739, current_train_items 261888.
I0302 19:01:30.992726 22760421793920 run.py:483] Algo bellman_ford step 8184 current loss 0.062969, current_train_items 261920.
I0302 19:01:31.011165 22760421793920 run.py:483] Algo bellman_ford step 8185 current loss 0.004532, current_train_items 261952.
I0302 19:01:31.026777 22760421793920 run.py:483] Algo bellman_ford step 8186 current loss 0.010267, current_train_items 261984.
I0302 19:01:31.050442 22760421793920 run.py:483] Algo bellman_ford step 8187 current loss 0.031980, current_train_items 262016.
I0302 19:01:31.081164 22760421793920 run.py:483] Algo bellman_ford step 8188 current loss 0.041775, current_train_items 262048.
I0302 19:01:31.112993 22760421793920 run.py:483] Algo bellman_ford step 8189 current loss 0.059266, current_train_items 262080.
I0302 19:01:31.131551 22760421793920 run.py:483] Algo bellman_ford step 8190 current loss 0.019901, current_train_items 262112.
I0302 19:01:31.147407 22760421793920 run.py:483] Algo bellman_ford step 8191 current loss 0.034422, current_train_items 262144.
I0302 19:01:31.170601 22760421793920 run.py:483] Algo bellman_ford step 8192 current loss 0.062047, current_train_items 262176.
I0302 19:01:31.201040 22760421793920 run.py:483] Algo bellman_ford step 8193 current loss 0.108174, current_train_items 262208.
I0302 19:01:31.233222 22760421793920 run.py:483] Algo bellman_ford step 8194 current loss 0.068457, current_train_items 262240.
I0302 19:01:31.251685 22760421793920 run.py:483] Algo bellman_ford step 8195 current loss 0.001748, current_train_items 262272.
I0302 19:01:31.267482 22760421793920 run.py:483] Algo bellman_ford step 8196 current loss 0.009960, current_train_items 262304.
I0302 19:01:31.290039 22760421793920 run.py:483] Algo bellman_ford step 8197 current loss 0.026747, current_train_items 262336.
I0302 19:01:31.320601 22760421793920 run.py:483] Algo bellman_ford step 8198 current loss 0.035050, current_train_items 262368.
I0302 19:01:31.352698 22760421793920 run.py:483] Algo bellman_ford step 8199 current loss 0.042197, current_train_items 262400.
I0302 19:01:31.371119 22760421793920 run.py:483] Algo bellman_ford step 8200 current loss 0.002464, current_train_items 262432.
I0302 19:01:31.378563 22760421793920 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:01:31.378675 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:01:31.394743 22760421793920 run.py:483] Algo bellman_ford step 8201 current loss 0.003619, current_train_items 262464.
I0302 19:01:31.418937 22760421793920 run.py:483] Algo bellman_ford step 8202 current loss 0.035217, current_train_items 262496.
I0302 19:01:31.449462 22760421793920 run.py:483] Algo bellman_ford step 8203 current loss 0.023271, current_train_items 262528.
I0302 19:01:31.483014 22760421793920 run.py:483] Algo bellman_ford step 8204 current loss 0.045044, current_train_items 262560.
I0302 19:01:31.501741 22760421793920 run.py:483] Algo bellman_ford step 8205 current loss 0.001856, current_train_items 262592.
I0302 19:01:31.517735 22760421793920 run.py:483] Algo bellman_ford step 8206 current loss 0.003714, current_train_items 262624.
I0302 19:01:31.541710 22760421793920 run.py:483] Algo bellman_ford step 8207 current loss 0.037064, current_train_items 262656.
I0302 19:01:31.572179 22760421793920 run.py:483] Algo bellman_ford step 8208 current loss 0.033127, current_train_items 262688.
I0302 19:01:31.601875 22760421793920 run.py:483] Algo bellman_ford step 8209 current loss 0.028642, current_train_items 262720.
I0302 19:01:31.620260 22760421793920 run.py:483] Algo bellman_ford step 8210 current loss 0.000934, current_train_items 262752.
I0302 19:01:31.635833 22760421793920 run.py:483] Algo bellman_ford step 8211 current loss 0.097398, current_train_items 262784.
I0302 19:01:31.658700 22760421793920 run.py:483] Algo bellman_ford step 8212 current loss 0.043005, current_train_items 262816.
I0302 19:01:31.689209 22760421793920 run.py:483] Algo bellman_ford step 8213 current loss 0.084987, current_train_items 262848.
I0302 19:01:31.721112 22760421793920 run.py:483] Algo bellman_ford step 8214 current loss 0.081099, current_train_items 262880.
I0302 19:01:31.739387 22760421793920 run.py:483] Algo bellman_ford step 8215 current loss 0.002377, current_train_items 262912.
I0302 19:01:31.754570 22760421793920 run.py:483] Algo bellman_ford step 8216 current loss 0.002902, current_train_items 262944.
I0302 19:01:31.776631 22760421793920 run.py:483] Algo bellman_ford step 8217 current loss 0.076997, current_train_items 262976.
I0302 19:01:31.808449 22760421793920 run.py:483] Algo bellman_ford step 8218 current loss 0.102348, current_train_items 263008.
I0302 19:01:31.841305 22760421793920 run.py:483] Algo bellman_ford step 8219 current loss 0.087216, current_train_items 263040.
I0302 19:01:31.859511 22760421793920 run.py:483] Algo bellman_ford step 8220 current loss 0.002651, current_train_items 263072.
I0302 19:01:31.875057 22760421793920 run.py:483] Algo bellman_ford step 8221 current loss 0.034089, current_train_items 263104.
I0302 19:01:31.899274 22760421793920 run.py:483] Algo bellman_ford step 8222 current loss 0.035964, current_train_items 263136.
I0302 19:01:31.930414 22760421793920 run.py:483] Algo bellman_ford step 8223 current loss 0.044928, current_train_items 263168.
I0302 19:01:31.960213 22760421793920 run.py:483] Algo bellman_ford step 8224 current loss 0.044360, current_train_items 263200.
I0302 19:01:31.978564 22760421793920 run.py:483] Algo bellman_ford step 8225 current loss 0.005519, current_train_items 263232.
I0302 19:01:31.994674 22760421793920 run.py:483] Algo bellman_ford step 8226 current loss 0.008826, current_train_items 263264.
I0302 19:01:32.018619 22760421793920 run.py:483] Algo bellman_ford step 8227 current loss 0.016569, current_train_items 263296.
I0302 19:01:32.048031 22760421793920 run.py:483] Algo bellman_ford step 8228 current loss 0.060055, current_train_items 263328.
I0302 19:01:32.080540 22760421793920 run.py:483] Algo bellman_ford step 8229 current loss 0.036302, current_train_items 263360.
I0302 19:01:32.098547 22760421793920 run.py:483] Algo bellman_ford step 8230 current loss 0.002954, current_train_items 263392.
I0302 19:01:32.114203 22760421793920 run.py:483] Algo bellman_ford step 8231 current loss 0.007172, current_train_items 263424.
I0302 19:01:32.138101 22760421793920 run.py:483] Algo bellman_ford step 8232 current loss 0.025336, current_train_items 263456.
I0302 19:01:32.168290 22760421793920 run.py:483] Algo bellman_ford step 8233 current loss 0.025296, current_train_items 263488.
I0302 19:01:32.199932 22760421793920 run.py:483] Algo bellman_ford step 8234 current loss 0.050040, current_train_items 263520.
I0302 19:01:32.218247 22760421793920 run.py:483] Algo bellman_ford step 8235 current loss 0.003490, current_train_items 263552.
I0302 19:01:32.234420 22760421793920 run.py:483] Algo bellman_ford step 8236 current loss 0.021024, current_train_items 263584.
I0302 19:01:32.258315 22760421793920 run.py:483] Algo bellman_ford step 8237 current loss 0.020869, current_train_items 263616.
I0302 19:01:32.289262 22760421793920 run.py:483] Algo bellman_ford step 8238 current loss 0.023748, current_train_items 263648.
I0302 19:01:32.321173 22760421793920 run.py:483] Algo bellman_ford step 8239 current loss 0.040991, current_train_items 263680.
I0302 19:01:32.339548 22760421793920 run.py:483] Algo bellman_ford step 8240 current loss 0.002649, current_train_items 263712.
I0302 19:01:32.355357 22760421793920 run.py:483] Algo bellman_ford step 8241 current loss 0.007632, current_train_items 263744.
I0302 19:01:32.378123 22760421793920 run.py:483] Algo bellman_ford step 8242 current loss 0.029618, current_train_items 263776.
I0302 19:01:32.408313 22760421793920 run.py:483] Algo bellman_ford step 8243 current loss 0.033107, current_train_items 263808.
I0302 19:01:32.442676 22760421793920 run.py:483] Algo bellman_ford step 8244 current loss 0.046656, current_train_items 263840.
I0302 19:01:32.461121 22760421793920 run.py:483] Algo bellman_ford step 8245 current loss 0.038206, current_train_items 263872.
I0302 19:01:32.476870 22760421793920 run.py:483] Algo bellman_ford step 8246 current loss 0.013858, current_train_items 263904.
I0302 19:01:32.499216 22760421793920 run.py:483] Algo bellman_ford step 8247 current loss 0.031201, current_train_items 263936.
I0302 19:01:32.529478 22760421793920 run.py:483] Algo bellman_ford step 8248 current loss 0.046144, current_train_items 263968.
I0302 19:01:32.562161 22760421793920 run.py:483] Algo bellman_ford step 8249 current loss 0.057689, current_train_items 264000.
I0302 19:01:32.580315 22760421793920 run.py:483] Algo bellman_ford step 8250 current loss 0.002327, current_train_items 264032.
I0302 19:01:32.587699 22760421793920 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:01:32.587813 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:32.604198 22760421793920 run.py:483] Algo bellman_ford step 8251 current loss 0.023494, current_train_items 264064.
I0302 19:01:32.628184 22760421793920 run.py:483] Algo bellman_ford step 8252 current loss 0.042814, current_train_items 264096.
I0302 19:01:32.660230 22760421793920 run.py:483] Algo bellman_ford step 8253 current loss 0.051780, current_train_items 264128.
I0302 19:01:32.692197 22760421793920 run.py:483] Algo bellman_ford step 8254 current loss 0.054225, current_train_items 264160.
I0302 19:01:32.710736 22760421793920 run.py:483] Algo bellman_ford step 8255 current loss 0.011292, current_train_items 264192.
I0302 19:01:32.727459 22760421793920 run.py:483] Algo bellman_ford step 8256 current loss 0.024234, current_train_items 264224.
I0302 19:01:32.751171 22760421793920 run.py:483] Algo bellman_ford step 8257 current loss 0.023855, current_train_items 264256.
I0302 19:01:32.782378 22760421793920 run.py:483] Algo bellman_ford step 8258 current loss 0.063420, current_train_items 264288.
I0302 19:01:32.815973 22760421793920 run.py:483] Algo bellman_ford step 8259 current loss 0.088176, current_train_items 264320.
I0302 19:01:32.834342 22760421793920 run.py:483] Algo bellman_ford step 8260 current loss 0.010154, current_train_items 264352.
I0302 19:01:32.850194 22760421793920 run.py:483] Algo bellman_ford step 8261 current loss 0.006304, current_train_items 264384.
I0302 19:01:32.874590 22760421793920 run.py:483] Algo bellman_ford step 8262 current loss 0.056459, current_train_items 264416.
I0302 19:01:32.905686 22760421793920 run.py:483] Algo bellman_ford step 8263 current loss 0.032024, current_train_items 264448.
I0302 19:01:32.939260 22760421793920 run.py:483] Algo bellman_ford step 8264 current loss 0.055616, current_train_items 264480.
I0302 19:01:32.957703 22760421793920 run.py:483] Algo bellman_ford step 8265 current loss 0.012829, current_train_items 264512.
I0302 19:01:32.973096 22760421793920 run.py:483] Algo bellman_ford step 8266 current loss 0.010339, current_train_items 264544.
I0302 19:01:32.996664 22760421793920 run.py:483] Algo bellman_ford step 8267 current loss 0.037077, current_train_items 264576.
I0302 19:01:33.027920 22760421793920 run.py:483] Algo bellman_ford step 8268 current loss 0.045826, current_train_items 264608.
I0302 19:01:33.058822 22760421793920 run.py:483] Algo bellman_ford step 8269 current loss 0.106676, current_train_items 264640.
I0302 19:01:33.077476 22760421793920 run.py:483] Algo bellman_ford step 8270 current loss 0.005341, current_train_items 264672.
I0302 19:01:33.092664 22760421793920 run.py:483] Algo bellman_ford step 8271 current loss 0.015614, current_train_items 264704.
I0302 19:01:33.116101 22760421793920 run.py:483] Algo bellman_ford step 8272 current loss 0.024611, current_train_items 264736.
I0302 19:01:33.145633 22760421793920 run.py:483] Algo bellman_ford step 8273 current loss 0.031483, current_train_items 264768.
I0302 19:01:33.179183 22760421793920 run.py:483] Algo bellman_ford step 8274 current loss 0.082035, current_train_items 264800.
I0302 19:01:33.197604 22760421793920 run.py:483] Algo bellman_ford step 8275 current loss 0.001375, current_train_items 264832.
I0302 19:01:33.213739 22760421793920 run.py:483] Algo bellman_ford step 8276 current loss 0.047119, current_train_items 264864.
I0302 19:01:33.237922 22760421793920 run.py:483] Algo bellman_ford step 8277 current loss 0.036669, current_train_items 264896.
I0302 19:01:33.268321 22760421793920 run.py:483] Algo bellman_ford step 8278 current loss 0.128626, current_train_items 264928.
I0302 19:01:33.299857 22760421793920 run.py:483] Algo bellman_ford step 8279 current loss 0.069449, current_train_items 264960.
I0302 19:01:33.318228 22760421793920 run.py:483] Algo bellman_ford step 8280 current loss 0.004589, current_train_items 264992.
I0302 19:01:33.334312 22760421793920 run.py:483] Algo bellman_ford step 8281 current loss 0.008678, current_train_items 265024.
I0302 19:01:33.357441 22760421793920 run.py:483] Algo bellman_ford step 8282 current loss 0.011981, current_train_items 265056.
I0302 19:01:33.387429 22760421793920 run.py:483] Algo bellman_ford step 8283 current loss 0.038427, current_train_items 265088.
I0302 19:01:33.418741 22760421793920 run.py:483] Algo bellman_ford step 8284 current loss 0.063227, current_train_items 265120.
I0302 19:01:33.437103 22760421793920 run.py:483] Algo bellman_ford step 8285 current loss 0.004361, current_train_items 265152.
I0302 19:01:33.453201 22760421793920 run.py:483] Algo bellman_ford step 8286 current loss 0.029001, current_train_items 265184.
I0302 19:01:33.476695 22760421793920 run.py:483] Algo bellman_ford step 8287 current loss 0.029403, current_train_items 265216.
I0302 19:01:33.508522 22760421793920 run.py:483] Algo bellman_ford step 8288 current loss 0.066512, current_train_items 265248.
I0302 19:01:33.541060 22760421793920 run.py:483] Algo bellman_ford step 8289 current loss 0.041484, current_train_items 265280.
I0302 19:01:33.559160 22760421793920 run.py:483] Algo bellman_ford step 8290 current loss 0.001896, current_train_items 265312.
I0302 19:01:33.574986 22760421793920 run.py:483] Algo bellman_ford step 8291 current loss 0.005297, current_train_items 265344.
I0302 19:01:33.599078 22760421793920 run.py:483] Algo bellman_ford step 8292 current loss 0.025183, current_train_items 265376.
I0302 19:01:33.629013 22760421793920 run.py:483] Algo bellman_ford step 8293 current loss 0.053414, current_train_items 265408.
I0302 19:01:33.662987 22760421793920 run.py:483] Algo bellman_ford step 8294 current loss 0.062510, current_train_items 265440.
I0302 19:01:33.681514 22760421793920 run.py:483] Algo bellman_ford step 8295 current loss 0.022405, current_train_items 265472.
I0302 19:01:33.697394 22760421793920 run.py:483] Algo bellman_ford step 8296 current loss 0.017508, current_train_items 265504.
I0302 19:01:33.721680 22760421793920 run.py:483] Algo bellman_ford step 8297 current loss 0.031322, current_train_items 265536.
I0302 19:01:33.752638 22760421793920 run.py:483] Algo bellman_ford step 8298 current loss 0.035805, current_train_items 265568.
I0302 19:01:33.783884 22760421793920 run.py:483] Algo bellman_ford step 8299 current loss 0.037801, current_train_items 265600.
I0302 19:01:33.802425 22760421793920 run.py:483] Algo bellman_ford step 8300 current loss 0.009881, current_train_items 265632.
I0302 19:01:33.810075 22760421793920 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:01:33.810186 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:33.826320 22760421793920 run.py:483] Algo bellman_ford step 8301 current loss 0.004346, current_train_items 265664.
I0302 19:01:33.851401 22760421793920 run.py:483] Algo bellman_ford step 8302 current loss 0.036269, current_train_items 265696.
I0302 19:01:33.882855 22760421793920 run.py:483] Algo bellman_ford step 8303 current loss 0.051470, current_train_items 265728.
I0302 19:01:33.915120 22760421793920 run.py:483] Algo bellman_ford step 8304 current loss 0.038040, current_train_items 265760.
I0302 19:01:33.933858 22760421793920 run.py:483] Algo bellman_ford step 8305 current loss 0.002084, current_train_items 265792.
I0302 19:01:33.949293 22760421793920 run.py:483] Algo bellman_ford step 8306 current loss 0.029227, current_train_items 265824.
I0302 19:01:33.973073 22760421793920 run.py:483] Algo bellman_ford step 8307 current loss 0.029750, current_train_items 265856.
I0302 19:01:34.001735 22760421793920 run.py:483] Algo bellman_ford step 8308 current loss 0.028635, current_train_items 265888.
I0302 19:01:34.036083 22760421793920 run.py:483] Algo bellman_ford step 8309 current loss 0.068255, current_train_items 265920.
I0302 19:01:34.054430 22760421793920 run.py:483] Algo bellman_ford step 8310 current loss 0.001728, current_train_items 265952.
I0302 19:01:34.070029 22760421793920 run.py:483] Algo bellman_ford step 8311 current loss 0.014184, current_train_items 265984.
I0302 19:01:34.093422 22760421793920 run.py:483] Algo bellman_ford step 8312 current loss 0.056398, current_train_items 266016.
I0302 19:01:34.123442 22760421793920 run.py:483] Algo bellman_ford step 8313 current loss 0.022957, current_train_items 266048.
I0302 19:01:34.154181 22760421793920 run.py:483] Algo bellman_ford step 8314 current loss 0.093788, current_train_items 266080.
I0302 19:01:34.172350 22760421793920 run.py:483] Algo bellman_ford step 8315 current loss 0.003495, current_train_items 266112.
I0302 19:01:34.188416 22760421793920 run.py:483] Algo bellman_ford step 8316 current loss 0.010642, current_train_items 266144.
I0302 19:01:34.211762 22760421793920 run.py:483] Algo bellman_ford step 8317 current loss 0.045973, current_train_items 266176.
I0302 19:01:34.240711 22760421793920 run.py:483] Algo bellman_ford step 8318 current loss 0.039428, current_train_items 266208.
I0302 19:01:34.272920 22760421793920 run.py:483] Algo bellman_ford step 8319 current loss 0.055080, current_train_items 266240.
I0302 19:01:34.291259 22760421793920 run.py:483] Algo bellman_ford step 8320 current loss 0.001820, current_train_items 266272.
I0302 19:01:34.306919 22760421793920 run.py:483] Algo bellman_ford step 8321 current loss 0.026255, current_train_items 266304.
I0302 19:01:34.331033 22760421793920 run.py:483] Algo bellman_ford step 8322 current loss 0.045690, current_train_items 266336.
I0302 19:01:34.361196 22760421793920 run.py:483] Algo bellman_ford step 8323 current loss 0.062913, current_train_items 266368.
I0302 19:01:34.394850 22760421793920 run.py:483] Algo bellman_ford step 8324 current loss 0.066474, current_train_items 266400.
I0302 19:01:34.413415 22760421793920 run.py:483] Algo bellman_ford step 8325 current loss 0.003497, current_train_items 266432.
I0302 19:01:34.428666 22760421793920 run.py:483] Algo bellman_ford step 8326 current loss 0.007602, current_train_items 266464.
I0302 19:01:34.451875 22760421793920 run.py:483] Algo bellman_ford step 8327 current loss 0.028719, current_train_items 266496.
I0302 19:01:34.481608 22760421793920 run.py:483] Algo bellman_ford step 8328 current loss 0.063901, current_train_items 266528.
I0302 19:01:34.514567 22760421793920 run.py:483] Algo bellman_ford step 8329 current loss 0.034411, current_train_items 266560.
I0302 19:01:34.532939 22760421793920 run.py:483] Algo bellman_ford step 8330 current loss 0.005497, current_train_items 266592.
I0302 19:01:34.548507 22760421793920 run.py:483] Algo bellman_ford step 8331 current loss 0.070612, current_train_items 266624.
I0302 19:01:34.571314 22760421793920 run.py:483] Algo bellman_ford step 8332 current loss 0.017818, current_train_items 266656.
I0302 19:01:34.602326 22760421793920 run.py:483] Algo bellman_ford step 8333 current loss 0.094765, current_train_items 266688.
I0302 19:01:34.634186 22760421793920 run.py:483] Algo bellman_ford step 8334 current loss 0.067379, current_train_items 266720.
I0302 19:01:34.652436 22760421793920 run.py:483] Algo bellman_ford step 8335 current loss 0.016734, current_train_items 266752.
I0302 19:01:34.668132 22760421793920 run.py:483] Algo bellman_ford step 8336 current loss 0.049963, current_train_items 266784.
I0302 19:01:34.692494 22760421793920 run.py:483] Algo bellman_ford step 8337 current loss 0.088536, current_train_items 266816.
I0302 19:01:34.722825 22760421793920 run.py:483] Algo bellman_ford step 8338 current loss 0.147192, current_train_items 266848.
I0302 19:01:34.754260 22760421793920 run.py:483] Algo bellman_ford step 8339 current loss 0.060570, current_train_items 266880.
I0302 19:01:34.772610 22760421793920 run.py:483] Algo bellman_ford step 8340 current loss 0.001962, current_train_items 266912.
I0302 19:01:34.788254 22760421793920 run.py:483] Algo bellman_ford step 8341 current loss 0.008296, current_train_items 266944.
I0302 19:01:34.810789 22760421793920 run.py:483] Algo bellman_ford step 8342 current loss 0.029200, current_train_items 266976.
I0302 19:01:34.840550 22760421793920 run.py:483] Algo bellman_ford step 8343 current loss 0.064281, current_train_items 267008.
I0302 19:01:34.873783 22760421793920 run.py:483] Algo bellman_ford step 8344 current loss 0.076753, current_train_items 267040.
I0302 19:01:34.891967 22760421793920 run.py:483] Algo bellman_ford step 8345 current loss 0.002037, current_train_items 267072.
I0302 19:01:34.907608 22760421793920 run.py:483] Algo bellman_ford step 8346 current loss 0.006867, current_train_items 267104.
I0302 19:01:34.931742 22760421793920 run.py:483] Algo bellman_ford step 8347 current loss 0.041229, current_train_items 267136.
I0302 19:01:34.963102 22760421793920 run.py:483] Algo bellman_ford step 8348 current loss 0.026765, current_train_items 267168.
I0302 19:01:34.996425 22760421793920 run.py:483] Algo bellman_ford step 8349 current loss 0.050436, current_train_items 267200.
I0302 19:01:35.014880 22760421793920 run.py:483] Algo bellman_ford step 8350 current loss 0.002352, current_train_items 267232.
I0302 19:01:35.022384 22760421793920 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:01:35.022541 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:35.039007 22760421793920 run.py:483] Algo bellman_ford step 8351 current loss 0.009313, current_train_items 267264.
I0302 19:01:35.062851 22760421793920 run.py:483] Algo bellman_ford step 8352 current loss 0.036114, current_train_items 267296.
I0302 19:01:35.093539 22760421793920 run.py:483] Algo bellman_ford step 8353 current loss 0.015637, current_train_items 267328.
I0302 19:01:35.127424 22760421793920 run.py:483] Algo bellman_ford step 8354 current loss 0.075081, current_train_items 267360.
I0302 19:01:35.145702 22760421793920 run.py:483] Algo bellman_ford step 8355 current loss 0.003701, current_train_items 267392.
I0302 19:01:35.161461 22760421793920 run.py:483] Algo bellman_ford step 8356 current loss 0.016789, current_train_items 267424.
I0302 19:01:35.184440 22760421793920 run.py:483] Algo bellman_ford step 8357 current loss 0.017770, current_train_items 267456.
I0302 19:01:35.215118 22760421793920 run.py:483] Algo bellman_ford step 8358 current loss 0.030981, current_train_items 267488.
I0302 19:01:35.246079 22760421793920 run.py:483] Algo bellman_ford step 8359 current loss 0.044739, current_train_items 267520.
I0302 19:01:35.264190 22760421793920 run.py:483] Algo bellman_ford step 8360 current loss 0.001263, current_train_items 267552.
I0302 19:01:35.279519 22760421793920 run.py:483] Algo bellman_ford step 8361 current loss 0.007914, current_train_items 267584.
I0302 19:01:35.303078 22760421793920 run.py:483] Algo bellman_ford step 8362 current loss 0.028512, current_train_items 267616.
I0302 19:01:35.332775 22760421793920 run.py:483] Algo bellman_ford step 8363 current loss 0.031374, current_train_items 267648.
I0302 19:01:35.365130 22760421793920 run.py:483] Algo bellman_ford step 8364 current loss 0.053514, current_train_items 267680.
I0302 19:01:35.383340 22760421793920 run.py:483] Algo bellman_ford step 8365 current loss 0.001061, current_train_items 267712.
I0302 19:01:35.399348 22760421793920 run.py:483] Algo bellman_ford step 8366 current loss 0.002240, current_train_items 267744.
I0302 19:01:35.421709 22760421793920 run.py:483] Algo bellman_ford step 8367 current loss 0.027938, current_train_items 267776.
I0302 19:01:35.452156 22760421793920 run.py:483] Algo bellman_ford step 8368 current loss 0.036490, current_train_items 267808.
I0302 19:01:35.486106 22760421793920 run.py:483] Algo bellman_ford step 8369 current loss 0.062932, current_train_items 267840.
I0302 19:01:35.504340 22760421793920 run.py:483] Algo bellman_ford step 8370 current loss 0.000824, current_train_items 267872.
I0302 19:01:35.520258 22760421793920 run.py:483] Algo bellman_ford step 8371 current loss 0.033176, current_train_items 267904.
I0302 19:01:35.542869 22760421793920 run.py:483] Algo bellman_ford step 8372 current loss 0.101144, current_train_items 267936.
I0302 19:01:35.573071 22760421793920 run.py:483] Algo bellman_ford step 8373 current loss 0.032703, current_train_items 267968.
I0302 19:01:35.605684 22760421793920 run.py:483] Algo bellman_ford step 8374 current loss 0.027945, current_train_items 268000.
I0302 19:01:35.624101 22760421793920 run.py:483] Algo bellman_ford step 8375 current loss 0.001478, current_train_items 268032.
I0302 19:01:35.639404 22760421793920 run.py:483] Algo bellman_ford step 8376 current loss 0.005008, current_train_items 268064.
I0302 19:01:35.662015 22760421793920 run.py:483] Algo bellman_ford step 8377 current loss 0.061609, current_train_items 268096.
I0302 19:01:35.692492 22760421793920 run.py:483] Algo bellman_ford step 8378 current loss 0.183329, current_train_items 268128.
I0302 19:01:35.724916 22760421793920 run.py:483] Algo bellman_ford step 8379 current loss 0.144888, current_train_items 268160.
I0302 19:01:35.743193 22760421793920 run.py:483] Algo bellman_ford step 8380 current loss 0.030640, current_train_items 268192.
I0302 19:01:35.759057 22760421793920 run.py:483] Algo bellman_ford step 8381 current loss 0.012604, current_train_items 268224.
I0302 19:01:35.781221 22760421793920 run.py:483] Algo bellman_ford step 8382 current loss 0.035062, current_train_items 268256.
I0302 19:01:35.812318 22760421793920 run.py:483] Algo bellman_ford step 8383 current loss 0.041694, current_train_items 268288.
I0302 19:01:35.843087 22760421793920 run.py:483] Algo bellman_ford step 8384 current loss 0.052418, current_train_items 268320.
I0302 19:01:35.861225 22760421793920 run.py:483] Algo bellman_ford step 8385 current loss 0.006396, current_train_items 268352.
I0302 19:01:35.876462 22760421793920 run.py:483] Algo bellman_ford step 8386 current loss 0.011063, current_train_items 268384.
I0302 19:01:35.899269 22760421793920 run.py:483] Algo bellman_ford step 8387 current loss 0.072002, current_train_items 268416.
I0302 19:01:35.929679 22760421793920 run.py:483] Algo bellman_ford step 8388 current loss 0.060127, current_train_items 268448.
I0302 19:01:35.961527 22760421793920 run.py:483] Algo bellman_ford step 8389 current loss 0.069864, current_train_items 268480.
I0302 19:01:35.979614 22760421793920 run.py:483] Algo bellman_ford step 8390 current loss 0.003649, current_train_items 268512.
I0302 19:01:35.995408 22760421793920 run.py:483] Algo bellman_ford step 8391 current loss 0.003913, current_train_items 268544.
I0302 19:01:36.017726 22760421793920 run.py:483] Algo bellman_ford step 8392 current loss 0.068983, current_train_items 268576.
I0302 19:01:36.049545 22760421793920 run.py:483] Algo bellman_ford step 8393 current loss 0.151620, current_train_items 268608.
I0302 19:01:36.080416 22760421793920 run.py:483] Algo bellman_ford step 8394 current loss 0.059872, current_train_items 268640.
I0302 19:01:36.098656 22760421793920 run.py:483] Algo bellman_ford step 8395 current loss 0.013468, current_train_items 268672.
I0302 19:01:36.114424 22760421793920 run.py:483] Algo bellman_ford step 8396 current loss 0.030883, current_train_items 268704.
I0302 19:01:36.137387 22760421793920 run.py:483] Algo bellman_ford step 8397 current loss 0.044645, current_train_items 268736.
I0302 19:01:36.168133 22760421793920 run.py:483] Algo bellman_ford step 8398 current loss 0.069803, current_train_items 268768.
I0302 19:01:36.200824 22760421793920 run.py:483] Algo bellman_ford step 8399 current loss 0.065104, current_train_items 268800.
I0302 19:01:36.218856 22760421793920 run.py:483] Algo bellman_ford step 8400 current loss 0.004561, current_train_items 268832.
I0302 19:01:36.226351 22760421793920 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:01:36.226463 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 19:01:36.243294 22760421793920 run.py:483] Algo bellman_ford step 8401 current loss 0.008033, current_train_items 268864.
I0302 19:01:36.267768 22760421793920 run.py:483] Algo bellman_ford step 8402 current loss 0.036953, current_train_items 268896.
I0302 19:01:36.299614 22760421793920 run.py:483] Algo bellman_ford step 8403 current loss 0.032726, current_train_items 268928.
I0302 19:01:36.334665 22760421793920 run.py:483] Algo bellman_ford step 8404 current loss 0.089101, current_train_items 268960.
I0302 19:01:36.353305 22760421793920 run.py:483] Algo bellman_ford step 8405 current loss 0.009349, current_train_items 268992.
I0302 19:01:36.369041 22760421793920 run.py:483] Algo bellman_ford step 8406 current loss 0.040567, current_train_items 269024.
I0302 19:01:36.391905 22760421793920 run.py:483] Algo bellman_ford step 8407 current loss 0.078874, current_train_items 269056.
I0302 19:01:36.421803 22760421793920 run.py:483] Algo bellman_ford step 8408 current loss 0.078039, current_train_items 269088.
I0302 19:01:36.455636 22760421793920 run.py:483] Algo bellman_ford step 8409 current loss 0.057378, current_train_items 269120.
I0302 19:01:36.473922 22760421793920 run.py:483] Algo bellman_ford step 8410 current loss 0.004968, current_train_items 269152.
I0302 19:01:36.489772 22760421793920 run.py:483] Algo bellman_ford step 8411 current loss 0.015873, current_train_items 269184.
I0302 19:01:36.513690 22760421793920 run.py:483] Algo bellman_ford step 8412 current loss 0.084329, current_train_items 269216.
I0302 19:01:36.545813 22760421793920 run.py:483] Algo bellman_ford step 8413 current loss 0.170801, current_train_items 269248.
I0302 19:01:36.578631 22760421793920 run.py:483] Algo bellman_ford step 8414 current loss 0.205512, current_train_items 269280.
I0302 19:01:36.597178 22760421793920 run.py:483] Algo bellman_ford step 8415 current loss 0.002129, current_train_items 269312.
I0302 19:01:36.613046 22760421793920 run.py:483] Algo bellman_ford step 8416 current loss 0.007585, current_train_items 269344.
I0302 19:01:36.636562 22760421793920 run.py:483] Algo bellman_ford step 8417 current loss 0.069557, current_train_items 269376.
I0302 19:01:36.668027 22760421793920 run.py:483] Algo bellman_ford step 8418 current loss 0.042293, current_train_items 269408.
I0302 19:01:36.703475 22760421793920 run.py:483] Algo bellman_ford step 8419 current loss 0.071912, current_train_items 269440.
I0302 19:01:36.721908 22760421793920 run.py:483] Algo bellman_ford step 8420 current loss 0.003717, current_train_items 269472.
I0302 19:01:36.737639 22760421793920 run.py:483] Algo bellman_ford step 8421 current loss 0.024072, current_train_items 269504.
I0302 19:01:36.762283 22760421793920 run.py:483] Algo bellman_ford step 8422 current loss 0.031843, current_train_items 269536.
I0302 19:01:36.793434 22760421793920 run.py:483] Algo bellman_ford step 8423 current loss 0.042273, current_train_items 269568.
I0302 19:01:36.826584 22760421793920 run.py:483] Algo bellman_ford step 8424 current loss 0.043283, current_train_items 269600.
I0302 19:01:36.845206 22760421793920 run.py:483] Algo bellman_ford step 8425 current loss 0.009091, current_train_items 269632.
I0302 19:01:36.860536 22760421793920 run.py:483] Algo bellman_ford step 8426 current loss 0.043938, current_train_items 269664.
I0302 19:01:36.884086 22760421793920 run.py:483] Algo bellman_ford step 8427 current loss 0.063602, current_train_items 269696.
I0302 19:01:36.914345 22760421793920 run.py:483] Algo bellman_ford step 8428 current loss 0.053074, current_train_items 269728.
I0302 19:01:36.946495 22760421793920 run.py:483] Algo bellman_ford step 8429 current loss 0.041193, current_train_items 269760.
I0302 19:01:36.964844 22760421793920 run.py:483] Algo bellman_ford step 8430 current loss 0.008266, current_train_items 269792.
I0302 19:01:36.981066 22760421793920 run.py:483] Algo bellman_ford step 8431 current loss 0.027010, current_train_items 269824.
I0302 19:01:37.004421 22760421793920 run.py:483] Algo bellman_ford step 8432 current loss 0.031288, current_train_items 269856.
I0302 19:01:37.036392 22760421793920 run.py:483] Algo bellman_ford step 8433 current loss 0.139182, current_train_items 269888.
I0302 19:01:37.067254 22760421793920 run.py:483] Algo bellman_ford step 8434 current loss 0.046415, current_train_items 269920.
I0302 19:01:37.085464 22760421793920 run.py:483] Algo bellman_ford step 8435 current loss 0.001331, current_train_items 269952.
I0302 19:01:37.101058 22760421793920 run.py:483] Algo bellman_ford step 8436 current loss 0.027871, current_train_items 269984.
I0302 19:01:37.124447 22760421793920 run.py:483] Algo bellman_ford step 8437 current loss 0.050024, current_train_items 270016.
I0302 19:01:37.154985 22760421793920 run.py:483] Algo bellman_ford step 8438 current loss 0.025556, current_train_items 270048.
I0302 19:01:37.185695 22760421793920 run.py:483] Algo bellman_ford step 8439 current loss 0.051220, current_train_items 270080.
I0302 19:01:37.204225 22760421793920 run.py:483] Algo bellman_ford step 8440 current loss 0.011614, current_train_items 270112.
I0302 19:01:37.220026 22760421793920 run.py:483] Algo bellman_ford step 8441 current loss 0.017424, current_train_items 270144.
I0302 19:01:37.243498 22760421793920 run.py:483] Algo bellman_ford step 8442 current loss 0.024084, current_train_items 270176.
I0302 19:01:37.274051 22760421793920 run.py:483] Algo bellman_ford step 8443 current loss 0.034839, current_train_items 270208.
I0302 19:01:37.306971 22760421793920 run.py:483] Algo bellman_ford step 8444 current loss 0.042805, current_train_items 270240.
I0302 19:01:37.325237 22760421793920 run.py:483] Algo bellman_ford step 8445 current loss 0.007217, current_train_items 270272.
I0302 19:01:37.340790 22760421793920 run.py:483] Algo bellman_ford step 8446 current loss 0.030158, current_train_items 270304.
I0302 19:01:37.364040 22760421793920 run.py:483] Algo bellman_ford step 8447 current loss 0.120229, current_train_items 270336.
I0302 19:01:37.394262 22760421793920 run.py:483] Algo bellman_ford step 8448 current loss 0.128022, current_train_items 270368.
I0302 19:01:37.426792 22760421793920 run.py:483] Algo bellman_ford step 8449 current loss 0.106816, current_train_items 270400.
I0302 19:01:37.445256 22760421793920 run.py:483] Algo bellman_ford step 8450 current loss 0.001716, current_train_items 270432.
I0302 19:01:37.452958 22760421793920 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:01:37.453069 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:37.469156 22760421793920 run.py:483] Algo bellman_ford step 8451 current loss 0.026298, current_train_items 270464.
I0302 19:01:37.493361 22760421793920 run.py:483] Algo bellman_ford step 8452 current loss 0.033351, current_train_items 270496.
I0302 19:01:37.525123 22760421793920 run.py:483] Algo bellman_ford step 8453 current loss 0.081826, current_train_items 270528.
I0302 19:01:37.558397 22760421793920 run.py:483] Algo bellman_ford step 8454 current loss 0.115882, current_train_items 270560.
I0302 19:01:37.577036 22760421793920 run.py:483] Algo bellman_ford step 8455 current loss 0.009524, current_train_items 270592.
I0302 19:01:37.593440 22760421793920 run.py:483] Algo bellman_ford step 8456 current loss 0.002453, current_train_items 270624.
I0302 19:01:37.617285 22760421793920 run.py:483] Algo bellman_ford step 8457 current loss 0.038421, current_train_items 270656.
I0302 19:01:37.646655 22760421793920 run.py:483] Algo bellman_ford step 8458 current loss 0.030949, current_train_items 270688.
I0302 19:01:37.676602 22760421793920 run.py:483] Algo bellman_ford step 8459 current loss 0.084115, current_train_items 270720.
I0302 19:01:37.694992 22760421793920 run.py:483] Algo bellman_ford step 8460 current loss 0.044909, current_train_items 270752.
I0302 19:01:37.710484 22760421793920 run.py:483] Algo bellman_ford step 8461 current loss 0.009575, current_train_items 270784.
I0302 19:01:37.732967 22760421793920 run.py:483] Algo bellman_ford step 8462 current loss 0.051644, current_train_items 270816.
I0302 19:01:37.762887 22760421793920 run.py:483] Algo bellman_ford step 8463 current loss 0.023018, current_train_items 270848.
I0302 19:01:37.795810 22760421793920 run.py:483] Algo bellman_ford step 8464 current loss 0.050632, current_train_items 270880.
I0302 19:01:37.814036 22760421793920 run.py:483] Algo bellman_ford step 8465 current loss 0.002562, current_train_items 270912.
I0302 19:01:37.830155 22760421793920 run.py:483] Algo bellman_ford step 8466 current loss 0.044057, current_train_items 270944.
I0302 19:01:37.852954 22760421793920 run.py:483] Algo bellman_ford step 8467 current loss 0.068607, current_train_items 270976.
I0302 19:01:37.882886 22760421793920 run.py:483] Algo bellman_ford step 8468 current loss 0.053566, current_train_items 271008.
I0302 19:01:37.914550 22760421793920 run.py:483] Algo bellman_ford step 8469 current loss 0.079940, current_train_items 271040.
I0302 19:01:37.933145 22760421793920 run.py:483] Algo bellman_ford step 8470 current loss 0.003749, current_train_items 271072.
I0302 19:01:37.949300 22760421793920 run.py:483] Algo bellman_ford step 8471 current loss 0.039483, current_train_items 271104.
I0302 19:01:37.972333 22760421793920 run.py:483] Algo bellman_ford step 8472 current loss 0.028177, current_train_items 271136.
I0302 19:01:38.000972 22760421793920 run.py:483] Algo bellman_ford step 8473 current loss 0.062561, current_train_items 271168.
I0302 19:01:38.033964 22760421793920 run.py:483] Algo bellman_ford step 8474 current loss 0.125859, current_train_items 271200.
I0302 19:01:38.052653 22760421793920 run.py:483] Algo bellman_ford step 8475 current loss 0.005081, current_train_items 271232.
I0302 19:01:38.068685 22760421793920 run.py:483] Algo bellman_ford step 8476 current loss 0.029384, current_train_items 271264.
I0302 19:01:38.093686 22760421793920 run.py:483] Algo bellman_ford step 8477 current loss 0.052392, current_train_items 271296.
I0302 19:01:38.124547 22760421793920 run.py:483] Algo bellman_ford step 8478 current loss 0.041452, current_train_items 271328.
I0302 19:01:38.157791 22760421793920 run.py:483] Algo bellman_ford step 8479 current loss 0.062751, current_train_items 271360.
I0302 19:01:38.175916 22760421793920 run.py:483] Algo bellman_ford step 8480 current loss 0.000997, current_train_items 271392.
I0302 19:01:38.191953 22760421793920 run.py:483] Algo bellman_ford step 8481 current loss 0.092705, current_train_items 271424.
I0302 19:01:38.215761 22760421793920 run.py:483] Algo bellman_ford step 8482 current loss 0.089107, current_train_items 271456.
I0302 19:01:38.246734 22760421793920 run.py:483] Algo bellman_ford step 8483 current loss 0.067208, current_train_items 271488.
I0302 19:01:38.277908 22760421793920 run.py:483] Algo bellman_ford step 8484 current loss 0.082307, current_train_items 271520.
I0302 19:01:38.296274 22760421793920 run.py:483] Algo bellman_ford step 8485 current loss 0.012604, current_train_items 271552.
I0302 19:01:38.311642 22760421793920 run.py:483] Algo bellman_ford step 8486 current loss 0.016185, current_train_items 271584.
I0302 19:01:38.335014 22760421793920 run.py:483] Algo bellman_ford step 8487 current loss 0.035771, current_train_items 271616.
I0302 19:01:38.366582 22760421793920 run.py:483] Algo bellman_ford step 8488 current loss 0.091232, current_train_items 271648.
I0302 19:01:38.399405 22760421793920 run.py:483] Algo bellman_ford step 8489 current loss 0.102111, current_train_items 271680.
I0302 19:01:38.417639 22760421793920 run.py:483] Algo bellman_ford step 8490 current loss 0.051120, current_train_items 271712.
I0302 19:01:38.433169 22760421793920 run.py:483] Algo bellman_ford step 8491 current loss 0.029227, current_train_items 271744.
I0302 19:01:38.457020 22760421793920 run.py:483] Algo bellman_ford step 8492 current loss 0.060654, current_train_items 271776.
I0302 19:01:38.486941 22760421793920 run.py:483] Algo bellman_ford step 8493 current loss 0.057294, current_train_items 271808.
I0302 19:01:38.518140 22760421793920 run.py:483] Algo bellman_ford step 8494 current loss 0.118815, current_train_items 271840.
I0302 19:01:38.536687 22760421793920 run.py:483] Algo bellman_ford step 8495 current loss 0.013161, current_train_items 271872.
I0302 19:01:38.552852 22760421793920 run.py:483] Algo bellman_ford step 8496 current loss 0.029704, current_train_items 271904.
I0302 19:01:38.576631 22760421793920 run.py:483] Algo bellman_ford step 8497 current loss 0.023615, current_train_items 271936.
I0302 19:01:38.607680 22760421793920 run.py:483] Algo bellman_ford step 8498 current loss 0.054917, current_train_items 271968.
I0302 19:01:38.638272 22760421793920 run.py:483] Algo bellman_ford step 8499 current loss 0.040036, current_train_items 272000.
I0302 19:01:38.656625 22760421793920 run.py:483] Algo bellman_ford step 8500 current loss 0.010160, current_train_items 272032.
I0302 19:01:38.664302 22760421793920 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:01:38.664454 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 19:01:38.681223 22760421793920 run.py:483] Algo bellman_ford step 8501 current loss 0.037221, current_train_items 272064.
I0302 19:01:38.705440 22760421793920 run.py:483] Algo bellman_ford step 8502 current loss 0.065727, current_train_items 272096.
I0302 19:01:38.735273 22760421793920 run.py:483] Algo bellman_ford step 8503 current loss 0.042079, current_train_items 272128.
I0302 19:01:38.766784 22760421793920 run.py:483] Algo bellman_ford step 8504 current loss 0.083820, current_train_items 272160.
I0302 19:01:38.785662 22760421793920 run.py:483] Algo bellman_ford step 8505 current loss 0.002200, current_train_items 272192.
I0302 19:01:38.800656 22760421793920 run.py:483] Algo bellman_ford step 8506 current loss 0.003961, current_train_items 272224.
I0302 19:01:38.822408 22760421793920 run.py:483] Algo bellman_ford step 8507 current loss 0.024973, current_train_items 272256.
I0302 19:01:38.853193 22760421793920 run.py:483] Algo bellman_ford step 8508 current loss 0.067015, current_train_items 272288.
I0302 19:01:38.886466 22760421793920 run.py:483] Algo bellman_ford step 8509 current loss 0.041129, current_train_items 272320.
I0302 19:01:38.904692 22760421793920 run.py:483] Algo bellman_ford step 8510 current loss 0.004752, current_train_items 272352.
I0302 19:01:38.919949 22760421793920 run.py:483] Algo bellman_ford step 8511 current loss 0.044341, current_train_items 272384.
I0302 19:01:38.944046 22760421793920 run.py:483] Algo bellman_ford step 8512 current loss 0.022081, current_train_items 272416.
I0302 19:01:38.973925 22760421793920 run.py:483] Algo bellman_ford step 8513 current loss 0.052437, current_train_items 272448.
I0302 19:01:39.006648 22760421793920 run.py:483] Algo bellman_ford step 8514 current loss 0.043341, current_train_items 272480.
I0302 19:01:39.025157 22760421793920 run.py:483] Algo bellman_ford step 8515 current loss 0.001611, current_train_items 272512.
I0302 19:01:39.040712 22760421793920 run.py:483] Algo bellman_ford step 8516 current loss 0.004774, current_train_items 272544.
I0302 19:01:39.063786 22760421793920 run.py:483] Algo bellman_ford step 8517 current loss 0.073868, current_train_items 272576.
I0302 19:01:39.095773 22760421793920 run.py:483] Algo bellman_ford step 8518 current loss 0.036165, current_train_items 272608.
I0302 19:01:39.126740 22760421793920 run.py:483] Algo bellman_ford step 8519 current loss 0.087351, current_train_items 272640.
I0302 19:01:39.144934 22760421793920 run.py:483] Algo bellman_ford step 8520 current loss 0.001099, current_train_items 272672.
I0302 19:01:39.160428 22760421793920 run.py:483] Algo bellman_ford step 8521 current loss 0.002538, current_train_items 272704.
I0302 19:01:39.183363 22760421793920 run.py:483] Algo bellman_ford step 8522 current loss 0.036582, current_train_items 272736.
I0302 19:01:39.213609 22760421793920 run.py:483] Algo bellman_ford step 8523 current loss 0.069038, current_train_items 272768.
I0302 19:01:39.246392 22760421793920 run.py:483] Algo bellman_ford step 8524 current loss 0.091683, current_train_items 272800.
I0302 19:01:39.264667 22760421793920 run.py:483] Algo bellman_ford step 8525 current loss 0.006694, current_train_items 272832.
I0302 19:01:39.280447 22760421793920 run.py:483] Algo bellman_ford step 8526 current loss 0.011395, current_train_items 272864.
I0302 19:01:39.303411 22760421793920 run.py:483] Algo bellman_ford step 8527 current loss 0.038184, current_train_items 272896.
I0302 19:01:39.333320 22760421793920 run.py:483] Algo bellman_ford step 8528 current loss 0.018854, current_train_items 272928.
I0302 19:01:39.366078 22760421793920 run.py:483] Algo bellman_ford step 8529 current loss 0.085842, current_train_items 272960.
I0302 19:01:39.384284 22760421793920 run.py:483] Algo bellman_ford step 8530 current loss 0.002260, current_train_items 272992.
I0302 19:01:39.400082 22760421793920 run.py:483] Algo bellman_ford step 8531 current loss 0.018839, current_train_items 273024.
I0302 19:01:39.422050 22760421793920 run.py:483] Algo bellman_ford step 8532 current loss 0.020088, current_train_items 273056.
I0302 19:01:39.452118 22760421793920 run.py:483] Algo bellman_ford step 8533 current loss 0.039451, current_train_items 273088.
I0302 19:01:39.482940 22760421793920 run.py:483] Algo bellman_ford step 8534 current loss 0.043753, current_train_items 273120.
I0302 19:01:39.501023 22760421793920 run.py:483] Algo bellman_ford step 8535 current loss 0.001549, current_train_items 273152.
I0302 19:01:39.516693 22760421793920 run.py:483] Algo bellman_ford step 8536 current loss 0.020490, current_train_items 273184.
I0302 19:01:39.539788 22760421793920 run.py:483] Algo bellman_ford step 8537 current loss 0.037787, current_train_items 273216.
I0302 19:01:39.570460 22760421793920 run.py:483] Algo bellman_ford step 8538 current loss 0.035818, current_train_items 273248.
I0302 19:01:39.604531 22760421793920 run.py:483] Algo bellman_ford step 8539 current loss 0.048335, current_train_items 273280.
I0302 19:01:39.622710 22760421793920 run.py:483] Algo bellman_ford step 8540 current loss 0.008984, current_train_items 273312.
I0302 19:01:39.638399 22760421793920 run.py:483] Algo bellman_ford step 8541 current loss 0.024852, current_train_items 273344.
I0302 19:01:39.661722 22760421793920 run.py:483] Algo bellman_ford step 8542 current loss 0.014419, current_train_items 273376.
I0302 19:01:39.693228 22760421793920 run.py:483] Algo bellman_ford step 8543 current loss 0.046837, current_train_items 273408.
I0302 19:01:39.724930 22760421793920 run.py:483] Algo bellman_ford step 8544 current loss 0.043693, current_train_items 273440.
I0302 19:01:39.743461 22760421793920 run.py:483] Algo bellman_ford step 8545 current loss 0.001153, current_train_items 273472.
I0302 19:01:39.758677 22760421793920 run.py:483] Algo bellman_ford step 8546 current loss 0.020511, current_train_items 273504.
I0302 19:01:39.782012 22760421793920 run.py:483] Algo bellman_ford step 8547 current loss 0.066564, current_train_items 273536.
I0302 19:01:39.812139 22760421793920 run.py:483] Algo bellman_ford step 8548 current loss 0.040116, current_train_items 273568.
I0302 19:01:39.844598 22760421793920 run.py:483] Algo bellman_ford step 8549 current loss 0.098435, current_train_items 273600.
I0302 19:01:39.862720 22760421793920 run.py:483] Algo bellman_ford step 8550 current loss 0.001994, current_train_items 273632.
I0302 19:01:39.870236 22760421793920 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:01:39.870348 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:39.887126 22760421793920 run.py:483] Algo bellman_ford step 8551 current loss 0.010525, current_train_items 273664.
I0302 19:01:39.911461 22760421793920 run.py:483] Algo bellman_ford step 8552 current loss 0.031643, current_train_items 273696.
I0302 19:01:39.942377 22760421793920 run.py:483] Algo bellman_ford step 8553 current loss 0.038643, current_train_items 273728.
I0302 19:01:39.972849 22760421793920 run.py:483] Algo bellman_ford step 8554 current loss 0.035910, current_train_items 273760.
I0302 19:01:39.991450 22760421793920 run.py:483] Algo bellman_ford step 8555 current loss 0.001595, current_train_items 273792.
I0302 19:01:40.007519 22760421793920 run.py:483] Algo bellman_ford step 8556 current loss 0.016413, current_train_items 273824.
I0302 19:01:40.030279 22760421793920 run.py:483] Algo bellman_ford step 8557 current loss 0.074018, current_train_items 273856.
I0302 19:01:40.061206 22760421793920 run.py:483] Algo bellman_ford step 8558 current loss 0.061822, current_train_items 273888.
I0302 19:01:40.094565 22760421793920 run.py:483] Algo bellman_ford step 8559 current loss 0.076480, current_train_items 273920.
I0302 19:01:40.112573 22760421793920 run.py:483] Algo bellman_ford step 8560 current loss 0.002553, current_train_items 273952.
I0302 19:01:40.128593 22760421793920 run.py:483] Algo bellman_ford step 8561 current loss 0.021796, current_train_items 273984.
I0302 19:01:40.152442 22760421793920 run.py:483] Algo bellman_ford step 8562 current loss 0.037235, current_train_items 274016.
I0302 19:01:40.184452 22760421793920 run.py:483] Algo bellman_ford step 8563 current loss 0.038288, current_train_items 274048.
I0302 19:01:40.215384 22760421793920 run.py:483] Algo bellman_ford step 8564 current loss 0.039774, current_train_items 274080.
I0302 19:01:40.233622 22760421793920 run.py:483] Algo bellman_ford step 8565 current loss 0.003041, current_train_items 274112.
I0302 19:01:40.249954 22760421793920 run.py:483] Algo bellman_ford step 8566 current loss 0.031392, current_train_items 274144.
I0302 19:01:40.274562 22760421793920 run.py:483] Algo bellman_ford step 8567 current loss 0.026701, current_train_items 274176.
I0302 19:01:40.305189 22760421793920 run.py:483] Algo bellman_ford step 8568 current loss 0.034562, current_train_items 274208.
I0302 19:01:40.338099 22760421793920 run.py:483] Algo bellman_ford step 8569 current loss 0.069783, current_train_items 274240.
I0302 19:01:40.356223 22760421793920 run.py:483] Algo bellman_ford step 8570 current loss 0.004631, current_train_items 274272.
I0302 19:01:40.371910 22760421793920 run.py:483] Algo bellman_ford step 8571 current loss 0.021653, current_train_items 274304.
I0302 19:01:40.394758 22760421793920 run.py:483] Algo bellman_ford step 8572 current loss 0.028038, current_train_items 274336.
I0302 19:01:40.425107 22760421793920 run.py:483] Algo bellman_ford step 8573 current loss 0.014144, current_train_items 274368.
I0302 19:01:40.455643 22760421793920 run.py:483] Algo bellman_ford step 8574 current loss 0.061906, current_train_items 274400.
I0302 19:01:40.474164 22760421793920 run.py:483] Algo bellman_ford step 8575 current loss 0.001542, current_train_items 274432.
I0302 19:01:40.489772 22760421793920 run.py:483] Algo bellman_ford step 8576 current loss 0.015566, current_train_items 274464.
I0302 19:01:40.513448 22760421793920 run.py:483] Algo bellman_ford step 8577 current loss 0.021886, current_train_items 274496.
I0302 19:01:40.544768 22760421793920 run.py:483] Algo bellman_ford step 8578 current loss 0.055700, current_train_items 274528.
I0302 19:01:40.577488 22760421793920 run.py:483] Algo bellman_ford step 8579 current loss 0.060526, current_train_items 274560.
I0302 19:01:40.595602 22760421793920 run.py:483] Algo bellman_ford step 8580 current loss 0.008231, current_train_items 274592.
I0302 19:01:40.611498 22760421793920 run.py:483] Algo bellman_ford step 8581 current loss 0.001961, current_train_items 274624.
I0302 19:01:40.634151 22760421793920 run.py:483] Algo bellman_ford step 8582 current loss 0.047633, current_train_items 274656.
I0302 19:01:40.664694 22760421793920 run.py:483] Algo bellman_ford step 8583 current loss 0.053029, current_train_items 274688.
I0302 19:01:40.698129 22760421793920 run.py:483] Algo bellman_ford step 8584 current loss 0.032528, current_train_items 274720.
I0302 19:01:40.716448 22760421793920 run.py:483] Algo bellman_ford step 8585 current loss 0.001840, current_train_items 274752.
I0302 19:01:40.732015 22760421793920 run.py:483] Algo bellman_ford step 8586 current loss 0.005960, current_train_items 274784.
I0302 19:01:40.756818 22760421793920 run.py:483] Algo bellman_ford step 8587 current loss 0.083089, current_train_items 274816.
I0302 19:01:40.787555 22760421793920 run.py:483] Algo bellman_ford step 8588 current loss 0.046838, current_train_items 274848.
I0302 19:01:40.818904 22760421793920 run.py:483] Algo bellman_ford step 8589 current loss 0.041559, current_train_items 274880.
I0302 19:01:40.837085 22760421793920 run.py:483] Algo bellman_ford step 8590 current loss 0.034011, current_train_items 274912.
I0302 19:01:40.852089 22760421793920 run.py:483] Algo bellman_ford step 8591 current loss 0.023469, current_train_items 274944.
I0302 19:01:40.875833 22760421793920 run.py:483] Algo bellman_ford step 8592 current loss 0.038737, current_train_items 274976.
I0302 19:01:40.907309 22760421793920 run.py:483] Algo bellman_ford step 8593 current loss 0.052679, current_train_items 275008.
I0302 19:01:40.939781 22760421793920 run.py:483] Algo bellman_ford step 8594 current loss 0.104547, current_train_items 275040.
I0302 19:01:40.958014 22760421793920 run.py:483] Algo bellman_ford step 8595 current loss 0.001790, current_train_items 275072.
I0302 19:01:40.973516 22760421793920 run.py:483] Algo bellman_ford step 8596 current loss 0.010334, current_train_items 275104.
I0302 19:01:40.997020 22760421793920 run.py:483] Algo bellman_ford step 8597 current loss 0.065519, current_train_items 275136.
I0302 19:01:41.026189 22760421793920 run.py:483] Algo bellman_ford step 8598 current loss 0.041179, current_train_items 275168.
I0302 19:01:41.059139 22760421793920 run.py:483] Algo bellman_ford step 8599 current loss 0.120863, current_train_items 275200.
I0302 19:01:41.077410 22760421793920 run.py:483] Algo bellman_ford step 8600 current loss 0.015551, current_train_items 275232.
I0302 19:01:41.084807 22760421793920 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:01:41.084925 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:41.101559 22760421793920 run.py:483] Algo bellman_ford step 8601 current loss 0.037440, current_train_items 275264.
I0302 19:01:41.125828 22760421793920 run.py:483] Algo bellman_ford step 8602 current loss 0.055172, current_train_items 275296.
I0302 19:01:41.157916 22760421793920 run.py:483] Algo bellman_ford step 8603 current loss 0.080517, current_train_items 275328.
I0302 19:01:41.191012 22760421793920 run.py:483] Algo bellman_ford step 8604 current loss 0.188124, current_train_items 275360.
I0302 19:01:41.209722 22760421793920 run.py:483] Algo bellman_ford step 8605 current loss 0.008962, current_train_items 275392.
I0302 19:01:41.224834 22760421793920 run.py:483] Algo bellman_ford step 8606 current loss 0.055216, current_train_items 275424.
I0302 19:01:41.248106 22760421793920 run.py:483] Algo bellman_ford step 8607 current loss 0.076547, current_train_items 275456.
I0302 19:01:41.278715 22760421793920 run.py:483] Algo bellman_ford step 8608 current loss 0.077554, current_train_items 275488.
I0302 19:01:41.312754 22760421793920 run.py:483] Algo bellman_ford step 8609 current loss 0.158627, current_train_items 275520.
I0302 19:01:41.331161 22760421793920 run.py:483] Algo bellman_ford step 8610 current loss 0.020675, current_train_items 275552.
I0302 19:01:41.346938 22760421793920 run.py:483] Algo bellman_ford step 8611 current loss 0.044005, current_train_items 275584.
I0302 19:01:41.371004 22760421793920 run.py:483] Algo bellman_ford step 8612 current loss 0.107742, current_train_items 275616.
I0302 19:01:41.402033 22760421793920 run.py:483] Algo bellman_ford step 8613 current loss 0.069939, current_train_items 275648.
I0302 19:01:41.432358 22760421793920 run.py:483] Algo bellman_ford step 8614 current loss 0.090268, current_train_items 275680.
I0302 19:01:41.450967 22760421793920 run.py:483] Algo bellman_ford step 8615 current loss 0.005649, current_train_items 275712.
I0302 19:01:41.466771 22760421793920 run.py:483] Algo bellman_ford step 8616 current loss 0.029735, current_train_items 275744.
I0302 19:01:41.490140 22760421793920 run.py:483] Algo bellman_ford step 8617 current loss 0.049806, current_train_items 275776.
I0302 19:01:41.521861 22760421793920 run.py:483] Algo bellman_ford step 8618 current loss 0.131311, current_train_items 275808.
I0302 19:01:41.554241 22760421793920 run.py:483] Algo bellman_ford step 8619 current loss 0.136249, current_train_items 275840.
I0302 19:01:41.572471 22760421793920 run.py:483] Algo bellman_ford step 8620 current loss 0.004401, current_train_items 275872.
I0302 19:01:41.588283 22760421793920 run.py:483] Algo bellman_ford step 8621 current loss 0.021727, current_train_items 275904.
I0302 19:01:41.612256 22760421793920 run.py:483] Algo bellman_ford step 8622 current loss 0.043362, current_train_items 275936.
I0302 19:01:41.643859 22760421793920 run.py:483] Algo bellman_ford step 8623 current loss 0.028110, current_train_items 275968.
I0302 19:01:41.676126 22760421793920 run.py:483] Algo bellman_ford step 8624 current loss 0.041742, current_train_items 276000.
I0302 19:01:41.694744 22760421793920 run.py:483] Algo bellman_ford step 8625 current loss 0.011608, current_train_items 276032.
I0302 19:01:41.710635 22760421793920 run.py:483] Algo bellman_ford step 8626 current loss 0.024848, current_train_items 276064.
I0302 19:01:41.734577 22760421793920 run.py:483] Algo bellman_ford step 8627 current loss 0.025860, current_train_items 276096.
I0302 19:01:41.766702 22760421793920 run.py:483] Algo bellman_ford step 8628 current loss 0.042846, current_train_items 276128.
I0302 19:01:41.798395 22760421793920 run.py:483] Algo bellman_ford step 8629 current loss 0.075889, current_train_items 276160.
I0302 19:01:41.816923 22760421793920 run.py:483] Algo bellman_ford step 8630 current loss 0.003121, current_train_items 276192.
I0302 19:01:41.832802 22760421793920 run.py:483] Algo bellman_ford step 8631 current loss 0.036766, current_train_items 276224.
I0302 19:01:41.855809 22760421793920 run.py:483] Algo bellman_ford step 8632 current loss 0.034456, current_train_items 276256.
I0302 19:01:41.886412 22760421793920 run.py:483] Algo bellman_ford step 8633 current loss 0.043419, current_train_items 276288.
I0302 19:01:41.918815 22760421793920 run.py:483] Algo bellman_ford step 8634 current loss 0.059426, current_train_items 276320.
I0302 19:01:41.937344 22760421793920 run.py:483] Algo bellman_ford step 8635 current loss 0.009122, current_train_items 276352.
I0302 19:01:41.953073 22760421793920 run.py:483] Algo bellman_ford step 8636 current loss 0.016345, current_train_items 276384.
I0302 19:01:41.977296 22760421793920 run.py:483] Algo bellman_ford step 8637 current loss 0.075387, current_train_items 276416.
I0302 19:01:42.007256 22760421793920 run.py:483] Algo bellman_ford step 8638 current loss 0.092219, current_train_items 276448.
I0302 19:01:42.040056 22760421793920 run.py:483] Algo bellman_ford step 8639 current loss 0.131540, current_train_items 276480.
I0302 19:01:42.058478 22760421793920 run.py:483] Algo bellman_ford step 8640 current loss 0.036838, current_train_items 276512.
I0302 19:01:42.074602 22760421793920 run.py:483] Algo bellman_ford step 8641 current loss 0.032518, current_train_items 276544.
I0302 19:01:42.098203 22760421793920 run.py:483] Algo bellman_ford step 8642 current loss 0.121269, current_train_items 276576.
I0302 19:01:42.129557 22760421793920 run.py:483] Algo bellman_ford step 8643 current loss 0.098182, current_train_items 276608.
I0302 19:01:42.162457 22760421793920 run.py:483] Algo bellman_ford step 8644 current loss 0.114407, current_train_items 276640.
I0302 19:01:42.180550 22760421793920 run.py:483] Algo bellman_ford step 8645 current loss 0.003059, current_train_items 276672.
I0302 19:01:42.196285 22760421793920 run.py:483] Algo bellman_ford step 8646 current loss 0.027304, current_train_items 276704.
I0302 19:01:42.219657 22760421793920 run.py:483] Algo bellman_ford step 8647 current loss 0.067624, current_train_items 276736.
I0302 19:01:42.251854 22760421793920 run.py:483] Algo bellman_ford step 8648 current loss 0.056406, current_train_items 276768.
I0302 19:01:42.283959 22760421793920 run.py:483] Algo bellman_ford step 8649 current loss 0.097943, current_train_items 276800.
I0302 19:01:42.302318 22760421793920 run.py:483] Algo bellman_ford step 8650 current loss 0.007220, current_train_items 276832.
I0302 19:01:42.309677 22760421793920 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:01:42.309788 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0302 19:01:42.326299 22760421793920 run.py:483] Algo bellman_ford step 8651 current loss 0.020892, current_train_items 276864.
I0302 19:01:42.350068 22760421793920 run.py:483] Algo bellman_ford step 8652 current loss 0.155131, current_train_items 276896.
I0302 19:01:42.381183 22760421793920 run.py:483] Algo bellman_ford step 8653 current loss 0.121891, current_train_items 276928.
I0302 19:01:42.414633 22760421793920 run.py:483] Algo bellman_ford step 8654 current loss 0.114201, current_train_items 276960.
I0302 19:01:42.433062 22760421793920 run.py:483] Algo bellman_ford step 8655 current loss 0.005342, current_train_items 276992.
I0302 19:01:42.449077 22760421793920 run.py:483] Algo bellman_ford step 8656 current loss 0.024716, current_train_items 277024.
I0302 19:01:42.473250 22760421793920 run.py:483] Algo bellman_ford step 8657 current loss 0.062944, current_train_items 277056.
I0302 19:01:42.504258 22760421793920 run.py:483] Algo bellman_ford step 8658 current loss 0.047701, current_train_items 277088.
I0302 19:01:42.536280 22760421793920 run.py:483] Algo bellman_ford step 8659 current loss 0.088736, current_train_items 277120.
I0302 19:01:42.554986 22760421793920 run.py:483] Algo bellman_ford step 8660 current loss 0.001924, current_train_items 277152.
I0302 19:01:42.570384 22760421793920 run.py:483] Algo bellman_ford step 8661 current loss 0.007628, current_train_items 277184.
I0302 19:01:42.593518 22760421793920 run.py:483] Algo bellman_ford step 8662 current loss 0.045891, current_train_items 277216.
I0302 19:01:42.623893 22760421793920 run.py:483] Algo bellman_ford step 8663 current loss 0.067244, current_train_items 277248.
I0302 19:01:42.657171 22760421793920 run.py:483] Algo bellman_ford step 8664 current loss 0.063360, current_train_items 277280.
I0302 19:01:42.675312 22760421793920 run.py:483] Algo bellman_ford step 8665 current loss 0.005594, current_train_items 277312.
I0302 19:01:42.690547 22760421793920 run.py:483] Algo bellman_ford step 8666 current loss 0.018325, current_train_items 277344.
I0302 19:01:42.714155 22760421793920 run.py:483] Algo bellman_ford step 8667 current loss 0.035341, current_train_items 277376.
I0302 19:01:42.743869 22760421793920 run.py:483] Algo bellman_ford step 8668 current loss 0.030473, current_train_items 277408.
I0302 19:01:42.775046 22760421793920 run.py:483] Algo bellman_ford step 8669 current loss 0.024544, current_train_items 277440.
I0302 19:01:42.793171 22760421793920 run.py:483] Algo bellman_ford step 8670 current loss 0.003945, current_train_items 277472.
I0302 19:01:42.808976 22760421793920 run.py:483] Algo bellman_ford step 8671 current loss 0.033823, current_train_items 277504.
I0302 19:01:42.831962 22760421793920 run.py:483] Algo bellman_ford step 8672 current loss 0.026238, current_train_items 277536.
I0302 19:01:42.862224 22760421793920 run.py:483] Algo bellman_ford step 8673 current loss 0.040783, current_train_items 277568.
I0302 19:01:42.892411 22760421793920 run.py:483] Algo bellman_ford step 8674 current loss 0.048994, current_train_items 277600.
I0302 19:01:42.910469 22760421793920 run.py:483] Algo bellman_ford step 8675 current loss 0.002229, current_train_items 277632.
I0302 19:01:42.925953 22760421793920 run.py:483] Algo bellman_ford step 8676 current loss 0.009694, current_train_items 277664.
I0302 19:01:42.949517 22760421793920 run.py:483] Algo bellman_ford step 8677 current loss 0.027853, current_train_items 277696.
I0302 19:01:42.980049 22760421793920 run.py:483] Algo bellman_ford step 8678 current loss 0.032139, current_train_items 277728.
I0302 19:01:43.011478 22760421793920 run.py:483] Algo bellman_ford step 8679 current loss 0.040897, current_train_items 277760.
I0302 19:01:43.029340 22760421793920 run.py:483] Algo bellman_ford step 8680 current loss 0.003154, current_train_items 277792.
I0302 19:01:43.045301 22760421793920 run.py:483] Algo bellman_ford step 8681 current loss 0.012495, current_train_items 277824.
I0302 19:01:43.068715 22760421793920 run.py:483] Algo bellman_ford step 8682 current loss 0.035033, current_train_items 277856.
I0302 19:01:43.098959 22760421793920 run.py:483] Algo bellman_ford step 8683 current loss 0.053059, current_train_items 277888.
I0302 19:01:43.130361 22760421793920 run.py:483] Algo bellman_ford step 8684 current loss 0.100519, current_train_items 277920.
I0302 19:01:43.148483 22760421793920 run.py:483] Algo bellman_ford step 8685 current loss 0.003280, current_train_items 277952.
I0302 19:01:43.163840 22760421793920 run.py:483] Algo bellman_ford step 8686 current loss 0.017496, current_train_items 277984.
I0302 19:01:43.188205 22760421793920 run.py:483] Algo bellman_ford step 8687 current loss 0.023872, current_train_items 278016.
I0302 19:01:43.219276 22760421793920 run.py:483] Algo bellman_ford step 8688 current loss 0.076832, current_train_items 278048.
I0302 19:01:43.250432 22760421793920 run.py:483] Algo bellman_ford step 8689 current loss 0.099209, current_train_items 278080.
I0302 19:01:43.268225 22760421793920 run.py:483] Algo bellman_ford step 8690 current loss 0.002257, current_train_items 278112.
I0302 19:01:43.284006 22760421793920 run.py:483] Algo bellman_ford step 8691 current loss 0.031028, current_train_items 278144.
I0302 19:01:43.306875 22760421793920 run.py:483] Algo bellman_ford step 8692 current loss 0.033812, current_train_items 278176.
I0302 19:01:43.337097 22760421793920 run.py:483] Algo bellman_ford step 8693 current loss 0.019876, current_train_items 278208.
I0302 19:01:43.368672 22760421793920 run.py:483] Algo bellman_ford step 8694 current loss 0.070499, current_train_items 278240.
I0302 19:01:43.386673 22760421793920 run.py:483] Algo bellman_ford step 8695 current loss 0.002292, current_train_items 278272.
I0302 19:01:43.402147 22760421793920 run.py:483] Algo bellman_ford step 8696 current loss 0.007000, current_train_items 278304.
I0302 19:01:43.425425 22760421793920 run.py:483] Algo bellman_ford step 8697 current loss 0.040857, current_train_items 278336.
I0302 19:01:43.456045 22760421793920 run.py:483] Algo bellman_ford step 8698 current loss 0.019438, current_train_items 278368.
I0302 19:01:43.489750 22760421793920 run.py:483] Algo bellman_ford step 8699 current loss 0.058972, current_train_items 278400.
I0302 19:01:43.507841 22760421793920 run.py:483] Algo bellman_ford step 8700 current loss 0.003998, current_train_items 278432.
I0302 19:01:43.515167 22760421793920 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:01:43.515291 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:01:43.531710 22760421793920 run.py:483] Algo bellman_ford step 8701 current loss 0.006410, current_train_items 278464.
I0302 19:01:43.554966 22760421793920 run.py:483] Algo bellman_ford step 8702 current loss 0.027301, current_train_items 278496.
I0302 19:01:43.586315 22760421793920 run.py:483] Algo bellman_ford step 8703 current loss 0.029515, current_train_items 278528.
I0302 19:01:43.620182 22760421793920 run.py:483] Algo bellman_ford step 8704 current loss 0.053828, current_train_items 278560.
I0302 19:01:43.639176 22760421793920 run.py:483] Algo bellman_ford step 8705 current loss 0.005634, current_train_items 278592.
I0302 19:01:43.654525 22760421793920 run.py:483] Algo bellman_ford step 8706 current loss 0.014949, current_train_items 278624.
I0302 19:01:43.677649 22760421793920 run.py:483] Algo bellman_ford step 8707 current loss 0.023040, current_train_items 278656.
I0302 19:01:43.708697 22760421793920 run.py:483] Algo bellman_ford step 8708 current loss 0.052782, current_train_items 278688.
I0302 19:01:43.740816 22760421793920 run.py:483] Algo bellman_ford step 8709 current loss 0.058863, current_train_items 278720.
I0302 19:01:43.758961 22760421793920 run.py:483] Algo bellman_ford step 8710 current loss 0.014088, current_train_items 278752.
I0302 19:01:43.774831 22760421793920 run.py:483] Algo bellman_ford step 8711 current loss 0.011372, current_train_items 278784.
I0302 19:01:43.797986 22760421793920 run.py:483] Algo bellman_ford step 8712 current loss 0.028700, current_train_items 278816.
I0302 19:01:43.830559 22760421793920 run.py:483] Algo bellman_ford step 8713 current loss 0.057189, current_train_items 278848.
I0302 19:01:43.861307 22760421793920 run.py:483] Algo bellman_ford step 8714 current loss 0.035162, current_train_items 278880.
I0302 19:01:43.879545 22760421793920 run.py:483] Algo bellman_ford step 8715 current loss 0.005414, current_train_items 278912.
I0302 19:01:43.895494 22760421793920 run.py:483] Algo bellman_ford step 8716 current loss 0.033624, current_train_items 278944.
I0302 19:01:43.918139 22760421793920 run.py:483] Algo bellman_ford step 8717 current loss 0.023276, current_train_items 278976.
I0302 19:01:43.947723 22760421793920 run.py:483] Algo bellman_ford step 8718 current loss 0.023468, current_train_items 279008.
I0302 19:01:43.978590 22760421793920 run.py:483] Algo bellman_ford step 8719 current loss 0.073685, current_train_items 279040.
I0302 19:01:43.996719 22760421793920 run.py:483] Algo bellman_ford step 8720 current loss 0.002185, current_train_items 279072.
I0302 19:01:44.012786 22760421793920 run.py:483] Algo bellman_ford step 8721 current loss 0.019255, current_train_items 279104.
I0302 19:01:44.036625 22760421793920 run.py:483] Algo bellman_ford step 8722 current loss 0.032319, current_train_items 279136.
I0302 19:01:44.066316 22760421793920 run.py:483] Algo bellman_ford step 8723 current loss 0.040074, current_train_items 279168.
I0302 19:01:44.097712 22760421793920 run.py:483] Algo bellman_ford step 8724 current loss 0.037684, current_train_items 279200.
I0302 19:01:44.115992 22760421793920 run.py:483] Algo bellman_ford step 8725 current loss 0.007371, current_train_items 279232.
I0302 19:01:44.131864 22760421793920 run.py:483] Algo bellman_ford step 8726 current loss 0.020497, current_train_items 279264.
I0302 19:01:44.155370 22760421793920 run.py:483] Algo bellman_ford step 8727 current loss 0.026582, current_train_items 279296.
I0302 19:01:44.185623 22760421793920 run.py:483] Algo bellman_ford step 8728 current loss 0.071335, current_train_items 279328.
I0302 19:01:44.217329 22760421793920 run.py:483] Algo bellman_ford step 8729 current loss 0.070562, current_train_items 279360.
I0302 19:01:44.235600 22760421793920 run.py:483] Algo bellman_ford step 8730 current loss 0.002850, current_train_items 279392.
I0302 19:01:44.250942 22760421793920 run.py:483] Algo bellman_ford step 8731 current loss 0.001968, current_train_items 279424.
I0302 19:01:44.274262 22760421793920 run.py:483] Algo bellman_ford step 8732 current loss 0.072418, current_train_items 279456.
I0302 19:01:44.305149 22760421793920 run.py:483] Algo bellman_ford step 8733 current loss 0.046272, current_train_items 279488.
I0302 19:01:44.338871 22760421793920 run.py:483] Algo bellman_ford step 8734 current loss 0.056088, current_train_items 279520.
I0302 19:01:44.357123 22760421793920 run.py:483] Algo bellman_ford step 8735 current loss 0.001963, current_train_items 279552.
I0302 19:01:44.372840 22760421793920 run.py:483] Algo bellman_ford step 8736 current loss 0.013406, current_train_items 279584.
I0302 19:01:44.397404 22760421793920 run.py:483] Algo bellman_ford step 8737 current loss 0.085697, current_train_items 279616.
I0302 19:01:44.427420 22760421793920 run.py:483] Algo bellman_ford step 8738 current loss 0.063467, current_train_items 279648.
I0302 19:01:44.460235 22760421793920 run.py:483] Algo bellman_ford step 8739 current loss 0.089980, current_train_items 279680.
I0302 19:01:44.478467 22760421793920 run.py:483] Algo bellman_ford step 8740 current loss 0.001792, current_train_items 279712.
I0302 19:01:44.494052 22760421793920 run.py:483] Algo bellman_ford step 8741 current loss 0.003889, current_train_items 279744.
I0302 19:01:44.517456 22760421793920 run.py:483] Algo bellman_ford step 8742 current loss 0.049598, current_train_items 279776.
I0302 19:01:44.548523 22760421793920 run.py:483] Algo bellman_ford step 8743 current loss 0.047252, current_train_items 279808.
I0302 19:01:44.580804 22760421793920 run.py:483] Algo bellman_ford step 8744 current loss 0.061508, current_train_items 279840.
I0302 19:01:44.598951 22760421793920 run.py:483] Algo bellman_ford step 8745 current loss 0.004252, current_train_items 279872.
I0302 19:01:44.614986 22760421793920 run.py:483] Algo bellman_ford step 8746 current loss 0.064583, current_train_items 279904.
I0302 19:01:44.638031 22760421793920 run.py:483] Algo bellman_ford step 8747 current loss 0.026595, current_train_items 279936.
I0302 19:01:44.669853 22760421793920 run.py:483] Algo bellman_ford step 8748 current loss 0.038840, current_train_items 279968.
I0302 19:01:44.703287 22760421793920 run.py:483] Algo bellman_ford step 8749 current loss 0.050783, current_train_items 280000.
I0302 19:01:44.721174 22760421793920 run.py:483] Algo bellman_ford step 8750 current loss 0.001438, current_train_items 280032.
I0302 19:01:44.728525 22760421793920 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:01:44.728635 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:44.745021 22760421793920 run.py:483] Algo bellman_ford step 8751 current loss 0.018131, current_train_items 280064.
I0302 19:01:44.768500 22760421793920 run.py:483] Algo bellman_ford step 8752 current loss 0.041614, current_train_items 280096.
I0302 19:01:44.799860 22760421793920 run.py:483] Algo bellman_ford step 8753 current loss 0.035646, current_train_items 280128.
I0302 19:01:44.832697 22760421793920 run.py:483] Algo bellman_ford step 8754 current loss 0.042796, current_train_items 280160.
I0302 19:01:44.851452 22760421793920 run.py:483] Algo bellman_ford step 8755 current loss 0.021423, current_train_items 280192.
I0302 19:01:44.867403 22760421793920 run.py:483] Algo bellman_ford step 8756 current loss 0.017872, current_train_items 280224.
I0302 19:01:44.891739 22760421793920 run.py:483] Algo bellman_ford step 8757 current loss 0.058820, current_train_items 280256.
I0302 19:01:44.922226 22760421793920 run.py:483] Algo bellman_ford step 8758 current loss 0.033846, current_train_items 280288.
I0302 19:01:44.954750 22760421793920 run.py:483] Algo bellman_ford step 8759 current loss 0.066277, current_train_items 280320.
I0302 19:01:44.973057 22760421793920 run.py:483] Algo bellman_ford step 8760 current loss 0.004295, current_train_items 280352.
I0302 19:01:44.988856 22760421793920 run.py:483] Algo bellman_ford step 8761 current loss 0.004049, current_train_items 280384.
I0302 19:01:45.013042 22760421793920 run.py:483] Algo bellman_ford step 8762 current loss 0.034943, current_train_items 280416.
I0302 19:01:45.043246 22760421793920 run.py:483] Algo bellman_ford step 8763 current loss 0.052500, current_train_items 280448.
I0302 19:01:45.077285 22760421793920 run.py:483] Algo bellman_ford step 8764 current loss 0.085577, current_train_items 280480.
I0302 19:01:45.095741 22760421793920 run.py:483] Algo bellman_ford step 8765 current loss 0.002072, current_train_items 280512.
I0302 19:01:45.111877 22760421793920 run.py:483] Algo bellman_ford step 8766 current loss 0.021979, current_train_items 280544.
I0302 19:01:45.134000 22760421793920 run.py:483] Algo bellman_ford step 8767 current loss 0.023845, current_train_items 280576.
I0302 19:01:45.164139 22760421793920 run.py:483] Algo bellman_ford step 8768 current loss 0.093347, current_train_items 280608.
I0302 19:01:45.194412 22760421793920 run.py:483] Algo bellman_ford step 8769 current loss 0.116468, current_train_items 280640.
I0302 19:01:45.213023 22760421793920 run.py:483] Algo bellman_ford step 8770 current loss 0.001828, current_train_items 280672.
I0302 19:01:45.228938 22760421793920 run.py:483] Algo bellman_ford step 8771 current loss 0.005652, current_train_items 280704.
I0302 19:01:45.252880 22760421793920 run.py:483] Algo bellman_ford step 8772 current loss 0.032090, current_train_items 280736.
I0302 19:01:45.283892 22760421793920 run.py:483] Algo bellman_ford step 8773 current loss 0.040063, current_train_items 280768.
I0302 19:01:45.315420 22760421793920 run.py:483] Algo bellman_ford step 8774 current loss 0.048885, current_train_items 280800.
I0302 19:01:45.333520 22760421793920 run.py:483] Algo bellman_ford step 8775 current loss 0.006611, current_train_items 280832.
I0302 19:01:45.349207 22760421793920 run.py:483] Algo bellman_ford step 8776 current loss 0.015125, current_train_items 280864.
I0302 19:01:45.371979 22760421793920 run.py:483] Algo bellman_ford step 8777 current loss 0.070760, current_train_items 280896.
I0302 19:01:45.402257 22760421793920 run.py:483] Algo bellman_ford step 8778 current loss 0.071141, current_train_items 280928.
I0302 19:01:45.435311 22760421793920 run.py:483] Algo bellman_ford step 8779 current loss 0.023204, current_train_items 280960.
I0302 19:01:45.453497 22760421793920 run.py:483] Algo bellman_ford step 8780 current loss 0.005654, current_train_items 280992.
I0302 19:01:45.469152 22760421793920 run.py:483] Algo bellman_ford step 8781 current loss 0.030185, current_train_items 281024.
I0302 19:01:45.492625 22760421793920 run.py:483] Algo bellman_ford step 8782 current loss 0.052534, current_train_items 281056.
I0302 19:01:45.524291 22760421793920 run.py:483] Algo bellman_ford step 8783 current loss 0.057784, current_train_items 281088.
I0302 19:01:45.557306 22760421793920 run.py:483] Algo bellman_ford step 8784 current loss 0.069712, current_train_items 281120.
I0302 19:01:45.575666 22760421793920 run.py:483] Algo bellman_ford step 8785 current loss 0.005832, current_train_items 281152.
I0302 19:01:45.591328 22760421793920 run.py:483] Algo bellman_ford step 8786 current loss 0.041832, current_train_items 281184.
I0302 19:01:45.615259 22760421793920 run.py:483] Algo bellman_ford step 8787 current loss 0.054451, current_train_items 281216.
I0302 19:01:45.645653 22760421793920 run.py:483] Algo bellman_ford step 8788 current loss 0.059810, current_train_items 281248.
I0302 19:01:45.678209 22760421793920 run.py:483] Algo bellman_ford step 8789 current loss 0.042975, current_train_items 281280.
I0302 19:01:45.696316 22760421793920 run.py:483] Algo bellman_ford step 8790 current loss 0.025700, current_train_items 281312.
I0302 19:01:45.712522 22760421793920 run.py:483] Algo bellman_ford step 8791 current loss 0.009634, current_train_items 281344.
I0302 19:01:45.735965 22760421793920 run.py:483] Algo bellman_ford step 8792 current loss 0.039297, current_train_items 281376.
I0302 19:01:45.766627 22760421793920 run.py:483] Algo bellman_ford step 8793 current loss 0.024009, current_train_items 281408.
I0302 19:01:45.799995 22760421793920 run.py:483] Algo bellman_ford step 8794 current loss 0.043945, current_train_items 281440.
I0302 19:01:45.818263 22760421793920 run.py:483] Algo bellman_ford step 8795 current loss 0.002026, current_train_items 281472.
I0302 19:01:45.834033 22760421793920 run.py:483] Algo bellman_ford step 8796 current loss 0.013674, current_train_items 281504.
I0302 19:01:45.857109 22760421793920 run.py:483] Algo bellman_ford step 8797 current loss 0.038087, current_train_items 281536.
I0302 19:01:45.888141 22760421793920 run.py:483] Algo bellman_ford step 8798 current loss 0.053557, current_train_items 281568.
I0302 19:01:45.922169 22760421793920 run.py:483] Algo bellman_ford step 8799 current loss 0.069110, current_train_items 281600.
I0302 19:01:45.940466 22760421793920 run.py:483] Algo bellman_ford step 8800 current loss 0.001430, current_train_items 281632.
I0302 19:01:45.947817 22760421793920 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:01:45.947938 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 19:01:45.963997 22760421793920 run.py:483] Algo bellman_ford step 8801 current loss 0.012953, current_train_items 281664.
I0302 19:01:45.987435 22760421793920 run.py:483] Algo bellman_ford step 8802 current loss 0.045348, current_train_items 281696.
I0302 19:01:46.019214 22760421793920 run.py:483] Algo bellman_ford step 8803 current loss 0.066704, current_train_items 281728.
I0302 19:01:46.051708 22760421793920 run.py:483] Algo bellman_ford step 8804 current loss 0.052883, current_train_items 281760.
I0302 19:01:46.070163 22760421793920 run.py:483] Algo bellman_ford step 8805 current loss 0.002333, current_train_items 281792.
I0302 19:01:46.085766 22760421793920 run.py:483] Algo bellman_ford step 8806 current loss 0.012870, current_train_items 281824.
I0302 19:01:46.109130 22760421793920 run.py:483] Algo bellman_ford step 8807 current loss 0.044973, current_train_items 281856.
I0302 19:01:46.139541 22760421793920 run.py:483] Algo bellman_ford step 8808 current loss 0.079712, current_train_items 281888.
I0302 19:01:46.174116 22760421793920 run.py:483] Algo bellman_ford step 8809 current loss 0.044318, current_train_items 281920.
I0302 19:01:46.192130 22760421793920 run.py:483] Algo bellman_ford step 8810 current loss 0.001732, current_train_items 281952.
I0302 19:01:46.208059 22760421793920 run.py:483] Algo bellman_ford step 8811 current loss 0.019421, current_train_items 281984.
I0302 19:01:46.231081 22760421793920 run.py:483] Algo bellman_ford step 8812 current loss 0.048783, current_train_items 282016.
I0302 19:01:46.261944 22760421793920 run.py:483] Algo bellman_ford step 8813 current loss 0.052665, current_train_items 282048.
I0302 19:01:46.294004 22760421793920 run.py:483] Algo bellman_ford step 8814 current loss 0.097330, current_train_items 282080.
I0302 19:01:46.311923 22760421793920 run.py:483] Algo bellman_ford step 8815 current loss 0.003609, current_train_items 282112.
I0302 19:01:46.327769 22760421793920 run.py:483] Algo bellman_ford step 8816 current loss 0.060671, current_train_items 282144.
I0302 19:01:46.350735 22760421793920 run.py:483] Algo bellman_ford step 8817 current loss 0.038220, current_train_items 282176.
I0302 19:01:46.380497 22760421793920 run.py:483] Algo bellman_ford step 8818 current loss 0.035609, current_train_items 282208.
I0302 19:01:46.415983 22760421793920 run.py:483] Algo bellman_ford step 8819 current loss 0.061842, current_train_items 282240.
I0302 19:01:46.434225 22760421793920 run.py:483] Algo bellman_ford step 8820 current loss 0.001049, current_train_items 282272.
I0302 19:01:46.449807 22760421793920 run.py:483] Algo bellman_ford step 8821 current loss 0.030324, current_train_items 282304.
I0302 19:01:46.472790 22760421793920 run.py:483] Algo bellman_ford step 8822 current loss 0.024284, current_train_items 282336.
I0302 19:01:46.503944 22760421793920 run.py:483] Algo bellman_ford step 8823 current loss 0.053956, current_train_items 282368.
I0302 19:01:46.535622 22760421793920 run.py:483] Algo bellman_ford step 8824 current loss 0.071918, current_train_items 282400.
I0302 19:01:46.553660 22760421793920 run.py:483] Algo bellman_ford step 8825 current loss 0.000612, current_train_items 282432.
I0302 19:01:46.569577 22760421793920 run.py:483] Algo bellman_ford step 8826 current loss 0.025257, current_train_items 282464.
I0302 19:01:46.593013 22760421793920 run.py:483] Algo bellman_ford step 8827 current loss 0.078428, current_train_items 282496.
I0302 19:01:46.623981 22760421793920 run.py:483] Algo bellman_ford step 8828 current loss 0.044969, current_train_items 282528.
I0302 19:01:46.655586 22760421793920 run.py:483] Algo bellman_ford step 8829 current loss 0.029147, current_train_items 282560.
I0302 19:01:46.673877 22760421793920 run.py:483] Algo bellman_ford step 8830 current loss 0.000866, current_train_items 282592.
I0302 19:01:46.689351 22760421793920 run.py:483] Algo bellman_ford step 8831 current loss 0.002998, current_train_items 282624.
I0302 19:01:46.713314 22760421793920 run.py:483] Algo bellman_ford step 8832 current loss 0.079489, current_train_items 282656.
I0302 19:01:46.742322 22760421793920 run.py:483] Algo bellman_ford step 8833 current loss 0.071389, current_train_items 282688.
I0302 19:01:46.773201 22760421793920 run.py:483] Algo bellman_ford step 8834 current loss 0.090081, current_train_items 282720.
I0302 19:01:46.791646 22760421793920 run.py:483] Algo bellman_ford step 8835 current loss 0.002696, current_train_items 282752.
I0302 19:01:46.807179 22760421793920 run.py:483] Algo bellman_ford step 8836 current loss 0.022671, current_train_items 282784.
I0302 19:01:46.831104 22760421793920 run.py:483] Algo bellman_ford step 8837 current loss 0.053450, current_train_items 282816.
I0302 19:01:46.862071 22760421793920 run.py:483] Algo bellman_ford step 8838 current loss 0.066444, current_train_items 282848.
I0302 19:01:46.894113 22760421793920 run.py:483] Algo bellman_ford step 8839 current loss 0.063811, current_train_items 282880.
I0302 19:01:46.912376 22760421793920 run.py:483] Algo bellman_ford step 8840 current loss 0.010843, current_train_items 282912.
I0302 19:01:46.928369 22760421793920 run.py:483] Algo bellman_ford step 8841 current loss 0.006039, current_train_items 282944.
I0302 19:01:46.951806 22760421793920 run.py:483] Algo bellman_ford step 8842 current loss 0.076873, current_train_items 282976.
I0302 19:01:46.982171 22760421793920 run.py:483] Algo bellman_ford step 8843 current loss 0.049543, current_train_items 283008.
I0302 19:01:47.015928 22760421793920 run.py:483] Algo bellman_ford step 8844 current loss 0.102310, current_train_items 283040.
I0302 19:01:47.034240 22760421793920 run.py:483] Algo bellman_ford step 8845 current loss 0.002412, current_train_items 283072.
I0302 19:01:47.049869 22760421793920 run.py:483] Algo bellman_ford step 8846 current loss 0.050935, current_train_items 283104.
I0302 19:01:47.073364 22760421793920 run.py:483] Algo bellman_ford step 8847 current loss 0.049069, current_train_items 283136.
I0302 19:01:47.102392 22760421793920 run.py:483] Algo bellman_ford step 8848 current loss 0.119074, current_train_items 283168.
I0302 19:01:47.134442 22760421793920 run.py:483] Algo bellman_ford step 8849 current loss 0.143773, current_train_items 283200.
I0302 19:01:47.152342 22760421793920 run.py:483] Algo bellman_ford step 8850 current loss 0.005012, current_train_items 283232.
I0302 19:01:47.159987 22760421793920 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:01:47.160097 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 19:01:47.176535 22760421793920 run.py:483] Algo bellman_ford step 8851 current loss 0.076691, current_train_items 283264.
I0302 19:01:47.201095 22760421793920 run.py:483] Algo bellman_ford step 8852 current loss 0.078349, current_train_items 283296.
I0302 19:01:47.232426 22760421793920 run.py:483] Algo bellman_ford step 8853 current loss 0.068586, current_train_items 283328.
I0302 19:01:47.265061 22760421793920 run.py:483] Algo bellman_ford step 8854 current loss 0.093478, current_train_items 283360.
I0302 19:01:47.283996 22760421793920 run.py:483] Algo bellman_ford step 8855 current loss 0.002727, current_train_items 283392.
I0302 19:01:47.300282 22760421793920 run.py:483] Algo bellman_ford step 8856 current loss 0.046454, current_train_items 283424.
I0302 19:01:47.322991 22760421793920 run.py:483] Algo bellman_ford step 8857 current loss 0.095689, current_train_items 283456.
I0302 19:01:47.354169 22760421793920 run.py:483] Algo bellman_ford step 8858 current loss 0.065162, current_train_items 283488.
I0302 19:01:47.384859 22760421793920 run.py:483] Algo bellman_ford step 8859 current loss 0.048712, current_train_items 283520.
I0302 19:01:47.403250 22760421793920 run.py:483] Algo bellman_ford step 8860 current loss 0.007994, current_train_items 283552.
I0302 19:01:47.419170 22760421793920 run.py:483] Algo bellman_ford step 8861 current loss 0.021152, current_train_items 283584.
I0302 19:01:47.443055 22760421793920 run.py:483] Algo bellman_ford step 8862 current loss 0.039979, current_train_items 283616.
I0302 19:01:47.472606 22760421793920 run.py:483] Algo bellman_ford step 8863 current loss 0.030196, current_train_items 283648.
I0302 19:01:47.503786 22760421793920 run.py:483] Algo bellman_ford step 8864 current loss 0.076107, current_train_items 283680.
I0302 19:01:47.522632 22760421793920 run.py:483] Algo bellman_ford step 8865 current loss 0.007768, current_train_items 283712.
I0302 19:01:47.538296 22760421793920 run.py:483] Algo bellman_ford step 8866 current loss 0.021933, current_train_items 283744.
I0302 19:01:47.560831 22760421793920 run.py:483] Algo bellman_ford step 8867 current loss 0.030408, current_train_items 283776.
I0302 19:01:47.592113 22760421793920 run.py:483] Algo bellman_ford step 8868 current loss 0.083712, current_train_items 283808.
I0302 19:01:47.623803 22760421793920 run.py:483] Algo bellman_ford step 8869 current loss 0.037218, current_train_items 283840.
I0302 19:01:47.642115 22760421793920 run.py:483] Algo bellman_ford step 8870 current loss 0.003836, current_train_items 283872.
I0302 19:01:47.657720 22760421793920 run.py:483] Algo bellman_ford step 8871 current loss 0.001881, current_train_items 283904.
I0302 19:01:47.681405 22760421793920 run.py:483] Algo bellman_ford step 8872 current loss 0.057387, current_train_items 283936.
I0302 19:01:47.711656 22760421793920 run.py:483] Algo bellman_ford step 8873 current loss 0.028865, current_train_items 283968.
I0302 19:01:47.745004 22760421793920 run.py:483] Algo bellman_ford step 8874 current loss 0.073101, current_train_items 284000.
I0302 19:01:47.763265 22760421793920 run.py:483] Algo bellman_ford step 8875 current loss 0.008075, current_train_items 284032.
I0302 19:01:47.779085 22760421793920 run.py:483] Algo bellman_ford step 8876 current loss 0.016370, current_train_items 284064.
I0302 19:01:47.802267 22760421793920 run.py:483] Algo bellman_ford step 8877 current loss 0.030855, current_train_items 284096.
I0302 19:01:47.832663 22760421793920 run.py:483] Algo bellman_ford step 8878 current loss 0.032588, current_train_items 284128.
I0302 19:01:47.865651 22760421793920 run.py:483] Algo bellman_ford step 8879 current loss 0.061242, current_train_items 284160.
I0302 19:01:47.884185 22760421793920 run.py:483] Algo bellman_ford step 8880 current loss 0.003220, current_train_items 284192.
I0302 19:01:47.899838 22760421793920 run.py:483] Algo bellman_ford step 8881 current loss 0.027228, current_train_items 284224.
I0302 19:01:47.922401 22760421793920 run.py:483] Algo bellman_ford step 8882 current loss 0.036047, current_train_items 284256.
I0302 19:01:47.954250 22760421793920 run.py:483] Algo bellman_ford step 8883 current loss 0.033456, current_train_items 284288.
I0302 19:01:47.986539 22760421793920 run.py:483] Algo bellman_ford step 8884 current loss 0.048384, current_train_items 284320.
I0302 19:01:48.004696 22760421793920 run.py:483] Algo bellman_ford step 8885 current loss 0.002345, current_train_items 284352.
I0302 19:01:48.020881 22760421793920 run.py:483] Algo bellman_ford step 8886 current loss 0.009335, current_train_items 284384.
I0302 19:01:48.044685 22760421793920 run.py:483] Algo bellman_ford step 8887 current loss 0.022526, current_train_items 284416.
I0302 19:01:48.074646 22760421793920 run.py:483] Algo bellman_ford step 8888 current loss 0.033203, current_train_items 284448.
I0302 19:01:48.107546 22760421793920 run.py:483] Algo bellman_ford step 8889 current loss 0.053271, current_train_items 284480.
I0302 19:01:48.125669 22760421793920 run.py:483] Algo bellman_ford step 8890 current loss 0.001252, current_train_items 284512.
I0302 19:01:48.140983 22760421793920 run.py:483] Algo bellman_ford step 8891 current loss 0.016407, current_train_items 284544.
I0302 19:01:48.164677 22760421793920 run.py:483] Algo bellman_ford step 8892 current loss 0.046873, current_train_items 284576.
I0302 19:01:48.196963 22760421793920 run.py:483] Algo bellman_ford step 8893 current loss 0.108667, current_train_items 284608.
I0302 19:01:48.229104 22760421793920 run.py:483] Algo bellman_ford step 8894 current loss 0.097553, current_train_items 284640.
I0302 19:01:48.247747 22760421793920 run.py:483] Algo bellman_ford step 8895 current loss 0.004644, current_train_items 284672.
I0302 19:01:48.263346 22760421793920 run.py:483] Algo bellman_ford step 8896 current loss 0.012824, current_train_items 284704.
I0302 19:01:48.285928 22760421793920 run.py:483] Algo bellman_ford step 8897 current loss 0.050111, current_train_items 284736.
I0302 19:01:48.317925 22760421793920 run.py:483] Algo bellman_ford step 8898 current loss 0.043948, current_train_items 284768.
I0302 19:01:48.351823 22760421793920 run.py:483] Algo bellman_ford step 8899 current loss 0.058387, current_train_items 284800.
I0302 19:01:48.370222 22760421793920 run.py:483] Algo bellman_ford step 8900 current loss 0.001449, current_train_items 284832.
I0302 19:01:48.377776 22760421793920 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:01:48.377890 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:01:48.394202 22760421793920 run.py:483] Algo bellman_ford step 8901 current loss 0.009011, current_train_items 284864.
I0302 19:01:48.417836 22760421793920 run.py:483] Algo bellman_ford step 8902 current loss 0.031407, current_train_items 284896.
I0302 19:01:48.447503 22760421793920 run.py:483] Algo bellman_ford step 8903 current loss 0.059251, current_train_items 284928.
I0302 19:01:48.481585 22760421793920 run.py:483] Algo bellman_ford step 8904 current loss 0.057633, current_train_items 284960.
I0302 19:01:48.500057 22760421793920 run.py:483] Algo bellman_ford step 8905 current loss 0.002707, current_train_items 284992.
I0302 19:01:48.515777 22760421793920 run.py:483] Algo bellman_ford step 8906 current loss 0.014592, current_train_items 285024.
I0302 19:01:48.538988 22760421793920 run.py:483] Algo bellman_ford step 8907 current loss 0.044584, current_train_items 285056.
I0302 19:01:48.569137 22760421793920 run.py:483] Algo bellman_ford step 8908 current loss 0.066255, current_train_items 285088.
I0302 19:01:48.599741 22760421793920 run.py:483] Algo bellman_ford step 8909 current loss 0.042833, current_train_items 285120.
I0302 19:01:48.617991 22760421793920 run.py:483] Algo bellman_ford step 8910 current loss 0.001599, current_train_items 285152.
I0302 19:01:48.633598 22760421793920 run.py:483] Algo bellman_ford step 8911 current loss 0.007478, current_train_items 285184.
I0302 19:01:48.657184 22760421793920 run.py:483] Algo bellman_ford step 8912 current loss 0.026174, current_train_items 285216.
I0302 19:01:48.686612 22760421793920 run.py:483] Algo bellman_ford step 8913 current loss 0.037100, current_train_items 285248.
I0302 19:01:48.720091 22760421793920 run.py:483] Algo bellman_ford step 8914 current loss 0.063764, current_train_items 285280.
I0302 19:01:48.738431 22760421793920 run.py:483] Algo bellman_ford step 8915 current loss 0.001807, current_train_items 285312.
I0302 19:01:48.753797 22760421793920 run.py:483] Algo bellman_ford step 8916 current loss 0.011032, current_train_items 285344.
I0302 19:01:48.777210 22760421793920 run.py:483] Algo bellman_ford step 8917 current loss 0.056490, current_train_items 285376.
I0302 19:01:48.807462 22760421793920 run.py:483] Algo bellman_ford step 8918 current loss 0.031055, current_train_items 285408.
I0302 19:01:48.839434 22760421793920 run.py:483] Algo bellman_ford step 8919 current loss 0.082949, current_train_items 285440.
I0302 19:01:48.857669 22760421793920 run.py:483] Algo bellman_ford step 8920 current loss 0.003610, current_train_items 285472.
I0302 19:01:48.873526 22760421793920 run.py:483] Algo bellman_ford step 8921 current loss 0.037155, current_train_items 285504.
I0302 19:01:48.897433 22760421793920 run.py:483] Algo bellman_ford step 8922 current loss 0.039169, current_train_items 285536.
I0302 19:01:48.927382 22760421793920 run.py:483] Algo bellman_ford step 8923 current loss 0.054390, current_train_items 285568.
I0302 19:01:48.959877 22760421793920 run.py:483] Algo bellman_ford step 8924 current loss 0.074169, current_train_items 285600.
I0302 19:01:48.978039 22760421793920 run.py:483] Algo bellman_ford step 8925 current loss 0.001404, current_train_items 285632.
I0302 19:01:48.993354 22760421793920 run.py:483] Algo bellman_ford step 8926 current loss 0.014289, current_train_items 285664.
I0302 19:01:49.016729 22760421793920 run.py:483] Algo bellman_ford step 8927 current loss 0.061249, current_train_items 285696.
I0302 19:01:49.045971 22760421793920 run.py:483] Algo bellman_ford step 8928 current loss 0.026228, current_train_items 285728.
I0302 19:01:49.078468 22760421793920 run.py:483] Algo bellman_ford step 8929 current loss 0.051672, current_train_items 285760.
I0302 19:01:49.096570 22760421793920 run.py:483] Algo bellman_ford step 8930 current loss 0.002341, current_train_items 285792.
I0302 19:01:49.112954 22760421793920 run.py:483] Algo bellman_ford step 8931 current loss 0.017807, current_train_items 285824.
I0302 19:01:49.136030 22760421793920 run.py:483] Algo bellman_ford step 8932 current loss 0.026456, current_train_items 285856.
I0302 19:01:49.167853 22760421793920 run.py:483] Algo bellman_ford step 8933 current loss 0.100626, current_train_items 285888.
I0302 19:01:49.200567 22760421793920 run.py:483] Algo bellman_ford step 8934 current loss 0.073640, current_train_items 285920.
I0302 19:01:49.218725 22760421793920 run.py:483] Algo bellman_ford step 8935 current loss 0.001011, current_train_items 285952.
I0302 19:01:49.234257 22760421793920 run.py:483] Algo bellman_ford step 8936 current loss 0.019614, current_train_items 285984.
I0302 19:01:49.257317 22760421793920 run.py:483] Algo bellman_ford step 8937 current loss 0.078946, current_train_items 286016.
I0302 19:01:49.287037 22760421793920 run.py:483] Algo bellman_ford step 8938 current loss 0.043282, current_train_items 286048.
I0302 19:01:49.317975 22760421793920 run.py:483] Algo bellman_ford step 8939 current loss 0.078241, current_train_items 286080.
I0302 19:01:49.336064 22760421793920 run.py:483] Algo bellman_ford step 8940 current loss 0.005907, current_train_items 286112.
I0302 19:01:49.351598 22760421793920 run.py:483] Algo bellman_ford step 8941 current loss 0.004450, current_train_items 286144.
I0302 19:01:49.375058 22760421793920 run.py:483] Algo bellman_ford step 8942 current loss 0.034531, current_train_items 286176.
I0302 19:01:49.406437 22760421793920 run.py:483] Algo bellman_ford step 8943 current loss 0.068693, current_train_items 286208.
I0302 19:01:49.439237 22760421793920 run.py:483] Algo bellman_ford step 8944 current loss 0.037378, current_train_items 286240.
I0302 19:01:49.457196 22760421793920 run.py:483] Algo bellman_ford step 8945 current loss 0.004271, current_train_items 286272.
I0302 19:01:49.472915 22760421793920 run.py:483] Algo bellman_ford step 8946 current loss 0.020803, current_train_items 286304.
I0302 19:01:49.495469 22760421793920 run.py:483] Algo bellman_ford step 8947 current loss 0.036874, current_train_items 286336.
I0302 19:01:49.526453 22760421793920 run.py:483] Algo bellman_ford step 8948 current loss 0.063515, current_train_items 286368.
I0302 19:01:49.558016 22760421793920 run.py:483] Algo bellman_ford step 8949 current loss 0.052812, current_train_items 286400.
I0302 19:01:49.576085 22760421793920 run.py:483] Algo bellman_ford step 8950 current loss 0.005726, current_train_items 286432.
I0302 19:01:49.583676 22760421793920 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:01:49.583787 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:01:49.600539 22760421793920 run.py:483] Algo bellman_ford step 8951 current loss 0.049552, current_train_items 286464.
I0302 19:01:49.623846 22760421793920 run.py:483] Algo bellman_ford step 8952 current loss 0.038468, current_train_items 286496.
I0302 19:01:49.657458 22760421793920 run.py:483] Algo bellman_ford step 8953 current loss 0.074475, current_train_items 286528.
I0302 19:01:49.691492 22760421793920 run.py:483] Algo bellman_ford step 8954 current loss 0.071333, current_train_items 286560.
I0302 19:01:49.710369 22760421793920 run.py:483] Algo bellman_ford step 8955 current loss 0.013500, current_train_items 286592.
I0302 19:01:49.727008 22760421793920 run.py:483] Algo bellman_ford step 8956 current loss 0.016438, current_train_items 286624.
I0302 19:01:49.750579 22760421793920 run.py:483] Algo bellman_ford step 8957 current loss 0.025505, current_train_items 286656.
I0302 19:01:49.780789 22760421793920 run.py:483] Algo bellman_ford step 8958 current loss 0.048024, current_train_items 286688.
I0302 19:01:49.814038 22760421793920 run.py:483] Algo bellman_ford step 8959 current loss 0.091792, current_train_items 286720.
I0302 19:01:49.832328 22760421793920 run.py:483] Algo bellman_ford step 8960 current loss 0.001849, current_train_items 286752.
I0302 19:01:49.847484 22760421793920 run.py:483] Algo bellman_ford step 8961 current loss 0.015424, current_train_items 286784.
I0302 19:01:49.871990 22760421793920 run.py:483] Algo bellman_ford step 8962 current loss 0.026937, current_train_items 286816.
I0302 19:01:49.903006 22760421793920 run.py:483] Algo bellman_ford step 8963 current loss 0.020643, current_train_items 286848.
I0302 19:01:49.933841 22760421793920 run.py:483] Algo bellman_ford step 8964 current loss 0.056617, current_train_items 286880.
I0302 19:01:49.952175 22760421793920 run.py:483] Algo bellman_ford step 8965 current loss 0.001096, current_train_items 286912.
I0302 19:01:49.968120 22760421793920 run.py:483] Algo bellman_ford step 8966 current loss 0.012442, current_train_items 286944.
I0302 19:01:49.991520 22760421793920 run.py:483] Algo bellman_ford step 8967 current loss 0.018188, current_train_items 286976.
I0302 19:01:50.022571 22760421793920 run.py:483] Algo bellman_ford step 8968 current loss 0.053895, current_train_items 287008.
I0302 19:01:50.054808 22760421793920 run.py:483] Algo bellman_ford step 8969 current loss 0.046140, current_train_items 287040.
I0302 19:01:50.073034 22760421793920 run.py:483] Algo bellman_ford step 8970 current loss 0.001345, current_train_items 287072.
I0302 19:01:50.088663 22760421793920 run.py:483] Algo bellman_ford step 8971 current loss 0.006601, current_train_items 287104.
I0302 19:01:50.112404 22760421793920 run.py:483] Algo bellman_ford step 8972 current loss 0.042983, current_train_items 287136.
I0302 19:01:50.142630 22760421793920 run.py:483] Algo bellman_ford step 8973 current loss 0.016023, current_train_items 287168.
I0302 19:01:50.174104 22760421793920 run.py:483] Algo bellman_ford step 8974 current loss 0.053138, current_train_items 287200.
I0302 19:01:50.192572 22760421793920 run.py:483] Algo bellman_ford step 8975 current loss 0.040022, current_train_items 287232.
I0302 19:01:50.208075 22760421793920 run.py:483] Algo bellman_ford step 8976 current loss 0.004600, current_train_items 287264.
I0302 19:01:50.230669 22760421793920 run.py:483] Algo bellman_ford step 8977 current loss 0.011030, current_train_items 287296.
I0302 19:01:50.261786 22760421793920 run.py:483] Algo bellman_ford step 8978 current loss 0.019498, current_train_items 287328.
I0302 19:01:50.293182 22760421793920 run.py:483] Algo bellman_ford step 8979 current loss 0.036615, current_train_items 287360.
I0302 19:01:50.311736 22760421793920 run.py:483] Algo bellman_ford step 8980 current loss 0.000395, current_train_items 287392.
I0302 19:01:50.327623 22760421793920 run.py:483] Algo bellman_ford step 8981 current loss 0.016347, current_train_items 287424.
I0302 19:01:50.351032 22760421793920 run.py:483] Algo bellman_ford step 8982 current loss 0.022148, current_train_items 287456.
I0302 19:01:50.382029 22760421793920 run.py:483] Algo bellman_ford step 8983 current loss 0.018016, current_train_items 287488.
I0302 19:01:50.414452 22760421793920 run.py:483] Algo bellman_ford step 8984 current loss 0.056360, current_train_items 287520.
I0302 19:01:50.432909 22760421793920 run.py:483] Algo bellman_ford step 8985 current loss 0.016841, current_train_items 287552.
I0302 19:01:50.448596 22760421793920 run.py:483] Algo bellman_ford step 8986 current loss 0.006704, current_train_items 287584.
I0302 19:01:50.471241 22760421793920 run.py:483] Algo bellman_ford step 8987 current loss 0.017464, current_train_items 287616.
I0302 19:01:50.500530 22760421793920 run.py:483] Algo bellman_ford step 8988 current loss 0.046159, current_train_items 287648.
I0302 19:01:50.534891 22760421793920 run.py:483] Algo bellman_ford step 8989 current loss 0.096603, current_train_items 287680.
I0302 19:01:50.553118 22760421793920 run.py:483] Algo bellman_ford step 8990 current loss 0.028332, current_train_items 287712.
I0302 19:01:50.568474 22760421793920 run.py:483] Algo bellman_ford step 8991 current loss 0.020229, current_train_items 287744.
I0302 19:01:50.591704 22760421793920 run.py:483] Algo bellman_ford step 8992 current loss 0.033803, current_train_items 287776.
I0302 19:01:50.622217 22760421793920 run.py:483] Algo bellman_ford step 8993 current loss 0.040733, current_train_items 287808.
I0302 19:01:50.654142 22760421793920 run.py:483] Algo bellman_ford step 8994 current loss 0.058543, current_train_items 287840.
I0302 19:01:50.672225 22760421793920 run.py:483] Algo bellman_ford step 8995 current loss 0.002246, current_train_items 287872.
I0302 19:01:50.688006 22760421793920 run.py:483] Algo bellman_ford step 8996 current loss 0.030940, current_train_items 287904.
I0302 19:01:50.711967 22760421793920 run.py:483] Algo bellman_ford step 8997 current loss 0.078858, current_train_items 287936.
I0302 19:01:50.744149 22760421793920 run.py:483] Algo bellman_ford step 8998 current loss 0.042252, current_train_items 287968.
I0302 19:01:50.775434 22760421793920 run.py:483] Algo bellman_ford step 8999 current loss 0.024885, current_train_items 288000.
I0302 19:01:50.793999 22760421793920 run.py:483] Algo bellman_ford step 9000 current loss 0.005390, current_train_items 288032.
I0302 19:01:50.801505 22760421793920 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:01:50.801615 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:01:50.818292 22760421793920 run.py:483] Algo bellman_ford step 9001 current loss 0.058112, current_train_items 288064.
I0302 19:01:50.841753 22760421793920 run.py:483] Algo bellman_ford step 9002 current loss 0.058373, current_train_items 288096.
I0302 19:01:50.872123 22760421793920 run.py:483] Algo bellman_ford step 9003 current loss 0.072621, current_train_items 288128.
I0302 19:01:50.905375 22760421793920 run.py:483] Algo bellman_ford step 9004 current loss 0.052332, current_train_items 288160.
I0302 19:01:50.924236 22760421793920 run.py:483] Algo bellman_ford step 9005 current loss 0.001183, current_train_items 288192.
I0302 19:01:50.940123 22760421793920 run.py:483] Algo bellman_ford step 9006 current loss 0.008334, current_train_items 288224.
I0302 19:01:50.963024 22760421793920 run.py:483] Algo bellman_ford step 9007 current loss 0.063218, current_train_items 288256.
I0302 19:01:50.994787 22760421793920 run.py:483] Algo bellman_ford step 9008 current loss 0.112297, current_train_items 288288.
I0302 19:01:51.027492 22760421793920 run.py:483] Algo bellman_ford step 9009 current loss 0.097494, current_train_items 288320.
I0302 19:01:51.046141 22760421793920 run.py:483] Algo bellman_ford step 9010 current loss 0.024802, current_train_items 288352.
I0302 19:01:51.062052 22760421793920 run.py:483] Algo bellman_ford step 9011 current loss 0.022509, current_train_items 288384.
I0302 19:01:51.084930 22760421793920 run.py:483] Algo bellman_ford step 9012 current loss 0.007730, current_train_items 288416.
I0302 19:01:51.114760 22760421793920 run.py:483] Algo bellman_ford step 9013 current loss 0.032144, current_train_items 288448.
I0302 19:01:51.149232 22760421793920 run.py:483] Algo bellman_ford step 9014 current loss 0.054671, current_train_items 288480.
I0302 19:01:51.167511 22760421793920 run.py:483] Algo bellman_ford step 9015 current loss 0.000984, current_train_items 288512.
I0302 19:01:51.183422 22760421793920 run.py:483] Algo bellman_ford step 9016 current loss 0.012442, current_train_items 288544.
I0302 19:01:51.207357 22760421793920 run.py:483] Algo bellman_ford step 9017 current loss 0.050515, current_train_items 288576.
I0302 19:01:51.238192 22760421793920 run.py:483] Algo bellman_ford step 9018 current loss 0.032861, current_train_items 288608.
I0302 19:01:51.271877 22760421793920 run.py:483] Algo bellman_ford step 9019 current loss 0.050132, current_train_items 288640.
I0302 19:01:51.290306 22760421793920 run.py:483] Algo bellman_ford step 9020 current loss 0.001024, current_train_items 288672.
I0302 19:01:51.306088 22760421793920 run.py:483] Algo bellman_ford step 9021 current loss 0.010467, current_train_items 288704.
I0302 19:01:51.330847 22760421793920 run.py:483] Algo bellman_ford step 9022 current loss 0.026979, current_train_items 288736.
I0302 19:01:51.362873 22760421793920 run.py:483] Algo bellman_ford step 9023 current loss 0.041600, current_train_items 288768.
I0302 19:01:51.395236 22760421793920 run.py:483] Algo bellman_ford step 9024 current loss 0.075957, current_train_items 288800.
I0302 19:01:51.413799 22760421793920 run.py:483] Algo bellman_ford step 9025 current loss 0.000577, current_train_items 288832.
I0302 19:01:51.429261 22760421793920 run.py:483] Algo bellman_ford step 9026 current loss 0.049326, current_train_items 288864.
I0302 19:01:51.453282 22760421793920 run.py:483] Algo bellman_ford step 9027 current loss 0.059105, current_train_items 288896.
I0302 19:01:51.484701 22760421793920 run.py:483] Algo bellman_ford step 9028 current loss 0.052860, current_train_items 288928.
I0302 19:01:51.517343 22760421793920 run.py:483] Algo bellman_ford step 9029 current loss 0.078848, current_train_items 288960.
I0302 19:01:51.536284 22760421793920 run.py:483] Algo bellman_ford step 9030 current loss 0.000669, current_train_items 288992.
I0302 19:01:51.552035 22760421793920 run.py:483] Algo bellman_ford step 9031 current loss 0.067102, current_train_items 289024.
I0302 19:01:51.576069 22760421793920 run.py:483] Algo bellman_ford step 9032 current loss 0.076159, current_train_items 289056.
I0302 19:01:51.607555 22760421793920 run.py:483] Algo bellman_ford step 9033 current loss 0.066490, current_train_items 289088.
I0302 19:01:51.642535 22760421793920 run.py:483] Algo bellman_ford step 9034 current loss 0.112992, current_train_items 289120.
I0302 19:01:51.661005 22760421793920 run.py:483] Algo bellman_ford step 9035 current loss 0.001712, current_train_items 289152.
I0302 19:01:51.676434 22760421793920 run.py:483] Algo bellman_ford step 9036 current loss 0.035486, current_train_items 289184.
I0302 19:01:51.700689 22760421793920 run.py:483] Algo bellman_ford step 9037 current loss 0.045909, current_train_items 289216.
I0302 19:01:51.731521 22760421793920 run.py:483] Algo bellman_ford step 9038 current loss 0.043518, current_train_items 289248.
I0302 19:01:51.763589 22760421793920 run.py:483] Algo bellman_ford step 9039 current loss 0.067158, current_train_items 289280.
I0302 19:01:51.781790 22760421793920 run.py:483] Algo bellman_ford step 9040 current loss 0.013114, current_train_items 289312.
I0302 19:01:51.797801 22760421793920 run.py:483] Algo bellman_ford step 9041 current loss 0.015016, current_train_items 289344.
I0302 19:01:51.820912 22760421793920 run.py:483] Algo bellman_ford step 9042 current loss 0.013654, current_train_items 289376.
I0302 19:01:51.851489 22760421793920 run.py:483] Algo bellman_ford step 9043 current loss 0.052159, current_train_items 289408.
I0302 19:01:51.884206 22760421793920 run.py:483] Algo bellman_ford step 9044 current loss 0.038983, current_train_items 289440.
I0302 19:01:51.902816 22760421793920 run.py:483] Algo bellman_ford step 9045 current loss 0.005581, current_train_items 289472.
I0302 19:01:51.918757 22760421793920 run.py:483] Algo bellman_ford step 9046 current loss 0.012688, current_train_items 289504.
I0302 19:01:51.940970 22760421793920 run.py:483] Algo bellman_ford step 9047 current loss 0.034875, current_train_items 289536.
I0302 19:01:51.972244 22760421793920 run.py:483] Algo bellman_ford step 9048 current loss 0.055772, current_train_items 289568.
I0302 19:01:52.004299 22760421793920 run.py:483] Algo bellman_ford step 9049 current loss 0.090170, current_train_items 289600.
I0302 19:01:52.022761 22760421793920 run.py:483] Algo bellman_ford step 9050 current loss 0.002106, current_train_items 289632.
I0302 19:01:52.030319 22760421793920 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:01:52.030428 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:52.046574 22760421793920 run.py:483] Algo bellman_ford step 9051 current loss 0.026485, current_train_items 289664.
I0302 19:01:52.070609 22760421793920 run.py:483] Algo bellman_ford step 9052 current loss 0.045329, current_train_items 289696.
I0302 19:01:52.102307 22760421793920 run.py:483] Algo bellman_ford step 9053 current loss 0.053817, current_train_items 289728.
I0302 19:01:52.134038 22760421793920 run.py:483] Algo bellman_ford step 9054 current loss 0.061974, current_train_items 289760.
I0302 19:01:52.152580 22760421793920 run.py:483] Algo bellman_ford step 9055 current loss 0.001249, current_train_items 289792.
I0302 19:01:52.168573 22760421793920 run.py:483] Algo bellman_ford step 9056 current loss 0.017171, current_train_items 289824.
I0302 19:01:52.191364 22760421793920 run.py:483] Algo bellman_ford step 9057 current loss 0.045336, current_train_items 289856.
I0302 19:01:52.220808 22760421793920 run.py:483] Algo bellman_ford step 9058 current loss 0.040018, current_train_items 289888.
I0302 19:01:52.251506 22760421793920 run.py:483] Algo bellman_ford step 9059 current loss 0.108968, current_train_items 289920.
I0302 19:01:52.269570 22760421793920 run.py:483] Algo bellman_ford step 9060 current loss 0.005469, current_train_items 289952.
I0302 19:01:52.285686 22760421793920 run.py:483] Algo bellman_ford step 9061 current loss 0.018101, current_train_items 289984.
I0302 19:01:52.310595 22760421793920 run.py:483] Algo bellman_ford step 9062 current loss 0.030336, current_train_items 290016.
I0302 19:01:52.340148 22760421793920 run.py:483] Algo bellman_ford step 9063 current loss 0.041983, current_train_items 290048.
I0302 19:01:52.373412 22760421793920 run.py:483] Algo bellman_ford step 9064 current loss 0.074066, current_train_items 290080.
I0302 19:01:52.392021 22760421793920 run.py:483] Algo bellman_ford step 9065 current loss 0.002413, current_train_items 290112.
I0302 19:01:52.407804 22760421793920 run.py:483] Algo bellman_ford step 9066 current loss 0.010190, current_train_items 290144.
I0302 19:01:52.429255 22760421793920 run.py:483] Algo bellman_ford step 9067 current loss 0.023785, current_train_items 290176.
I0302 19:01:52.460062 22760421793920 run.py:483] Algo bellman_ford step 9068 current loss 0.029663, current_train_items 290208.
I0302 19:01:52.492908 22760421793920 run.py:483] Algo bellman_ford step 9069 current loss 0.078099, current_train_items 290240.
I0302 19:01:52.511141 22760421793920 run.py:483] Algo bellman_ford step 9070 current loss 0.000794, current_train_items 290272.
I0302 19:01:52.527106 22760421793920 run.py:483] Algo bellman_ford step 9071 current loss 0.032165, current_train_items 290304.
I0302 19:01:52.551429 22760421793920 run.py:483] Algo bellman_ford step 9072 current loss 0.022062, current_train_items 290336.
I0302 19:01:52.582529 22760421793920 run.py:483] Algo bellman_ford step 9073 current loss 0.048328, current_train_items 290368.
I0302 19:01:52.614725 22760421793920 run.py:483] Algo bellman_ford step 9074 current loss 0.046968, current_train_items 290400.
I0302 19:01:52.632935 22760421793920 run.py:483] Algo bellman_ford step 9075 current loss 0.005075, current_train_items 290432.
I0302 19:01:52.648687 22760421793920 run.py:483] Algo bellman_ford step 9076 current loss 0.020220, current_train_items 290464.
I0302 19:01:52.672405 22760421793920 run.py:483] Algo bellman_ford step 9077 current loss 0.031766, current_train_items 290496.
I0302 19:01:52.702964 22760421793920 run.py:483] Algo bellman_ford step 9078 current loss 0.038232, current_train_items 290528.
I0302 19:01:52.736022 22760421793920 run.py:483] Algo bellman_ford step 9079 current loss 0.056770, current_train_items 290560.
I0302 19:01:52.754502 22760421793920 run.py:483] Algo bellman_ford step 9080 current loss 0.002926, current_train_items 290592.
I0302 19:01:52.770248 22760421793920 run.py:483] Algo bellman_ford step 9081 current loss 0.007457, current_train_items 290624.
I0302 19:01:52.793257 22760421793920 run.py:483] Algo bellman_ford step 9082 current loss 0.035872, current_train_items 290656.
I0302 19:01:52.823179 22760421793920 run.py:483] Algo bellman_ford step 9083 current loss 0.038242, current_train_items 290688.
I0302 19:01:52.854550 22760421793920 run.py:483] Algo bellman_ford step 9084 current loss 0.054845, current_train_items 290720.
I0302 19:01:52.872774 22760421793920 run.py:483] Algo bellman_ford step 9085 current loss 0.003282, current_train_items 290752.
I0302 19:01:52.888475 22760421793920 run.py:483] Algo bellman_ford step 9086 current loss 0.012362, current_train_items 290784.
I0302 19:01:52.912401 22760421793920 run.py:483] Algo bellman_ford step 9087 current loss 0.101294, current_train_items 290816.
I0302 19:01:52.942307 22760421793920 run.py:483] Algo bellman_ford step 9088 current loss 0.044326, current_train_items 290848.
I0302 19:01:52.976295 22760421793920 run.py:483] Algo bellman_ford step 9089 current loss 0.143623, current_train_items 290880.
I0302 19:01:52.994563 22760421793920 run.py:483] Algo bellman_ford step 9090 current loss 0.001741, current_train_items 290912.
I0302 19:01:53.010183 22760421793920 run.py:483] Algo bellman_ford step 9091 current loss 0.025236, current_train_items 290944.
I0302 19:01:53.033432 22760421793920 run.py:483] Algo bellman_ford step 9092 current loss 0.022538, current_train_items 290976.
I0302 19:01:53.062169 22760421793920 run.py:483] Algo bellman_ford step 9093 current loss 0.060746, current_train_items 291008.
I0302 19:01:53.096565 22760421793920 run.py:483] Algo bellman_ford step 9094 current loss 0.073642, current_train_items 291040.
I0302 19:01:53.115091 22760421793920 run.py:483] Algo bellman_ford step 9095 current loss 0.003790, current_train_items 291072.
I0302 19:01:53.130789 22760421793920 run.py:483] Algo bellman_ford step 9096 current loss 0.018491, current_train_items 291104.
I0302 19:01:53.153904 22760421793920 run.py:483] Algo bellman_ford step 9097 current loss 0.051659, current_train_items 291136.
I0302 19:01:53.184820 22760421793920 run.py:483] Algo bellman_ford step 9098 current loss 0.113114, current_train_items 291168.
I0302 19:01:53.216361 22760421793920 run.py:483] Algo bellman_ford step 9099 current loss 0.036100, current_train_items 291200.
I0302 19:01:53.234490 22760421793920 run.py:483] Algo bellman_ford step 9100 current loss 0.002418, current_train_items 291232.
I0302 19:01:53.241879 22760421793920 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:01:53.242003 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:01:53.258354 22760421793920 run.py:483] Algo bellman_ford step 9101 current loss 0.003813, current_train_items 291264.
I0302 19:01:53.282748 22760421793920 run.py:483] Algo bellman_ford step 9102 current loss 0.050323, current_train_items 291296.
I0302 19:01:53.313716 22760421793920 run.py:483] Algo bellman_ford step 9103 current loss 0.068927, current_train_items 291328.
I0302 19:01:53.347716 22760421793920 run.py:483] Algo bellman_ford step 9104 current loss 0.075609, current_train_items 291360.
I0302 19:01:53.366135 22760421793920 run.py:483] Algo bellman_ford step 9105 current loss 0.050551, current_train_items 291392.
I0302 19:01:53.382107 22760421793920 run.py:483] Algo bellman_ford step 9106 current loss 0.017677, current_train_items 291424.
I0302 19:01:53.405865 22760421793920 run.py:483] Algo bellman_ford step 9107 current loss 0.043915, current_train_items 291456.
I0302 19:01:53.437248 22760421793920 run.py:483] Algo bellman_ford step 9108 current loss 0.047505, current_train_items 291488.
I0302 19:01:53.469749 22760421793920 run.py:483] Algo bellman_ford step 9109 current loss 0.059290, current_train_items 291520.
I0302 19:01:53.487845 22760421793920 run.py:483] Algo bellman_ford step 9110 current loss 0.003484, current_train_items 291552.
I0302 19:01:53.503468 22760421793920 run.py:483] Algo bellman_ford step 9111 current loss 0.020943, current_train_items 291584.
I0302 19:01:53.526124 22760421793920 run.py:483] Algo bellman_ford step 9112 current loss 0.047327, current_train_items 291616.
I0302 19:01:53.558268 22760421793920 run.py:483] Algo bellman_ford step 9113 current loss 0.040089, current_train_items 291648.
I0302 19:01:53.591821 22760421793920 run.py:483] Algo bellman_ford step 9114 current loss 0.083191, current_train_items 291680.
I0302 19:01:53.610248 22760421793920 run.py:483] Algo bellman_ford step 9115 current loss 0.016493, current_train_items 291712.
I0302 19:01:53.626439 22760421793920 run.py:483] Algo bellman_ford step 9116 current loss 0.003774, current_train_items 291744.
I0302 19:01:53.649466 22760421793920 run.py:483] Algo bellman_ford step 9117 current loss 0.024065, current_train_items 291776.
I0302 19:01:53.679348 22760421793920 run.py:483] Algo bellman_ford step 9118 current loss 0.033229, current_train_items 291808.
I0302 19:01:53.712861 22760421793920 run.py:483] Algo bellman_ford step 9119 current loss 0.055763, current_train_items 291840.
I0302 19:01:53.731083 22760421793920 run.py:483] Algo bellman_ford step 9120 current loss 0.004349, current_train_items 291872.
I0302 19:01:53.746737 22760421793920 run.py:483] Algo bellman_ford step 9121 current loss 0.010105, current_train_items 291904.
I0302 19:01:53.769885 22760421793920 run.py:483] Algo bellman_ford step 9122 current loss 0.022216, current_train_items 291936.
I0302 19:01:53.801823 22760421793920 run.py:483] Algo bellman_ford step 9123 current loss 0.052631, current_train_items 291968.
I0302 19:01:53.836244 22760421793920 run.py:483] Algo bellman_ford step 9124 current loss 0.063835, current_train_items 292000.
I0302 19:01:53.854564 22760421793920 run.py:483] Algo bellman_ford step 9125 current loss 0.004013, current_train_items 292032.
I0302 19:01:53.870189 22760421793920 run.py:483] Algo bellman_ford step 9126 current loss 0.023812, current_train_items 292064.
I0302 19:01:53.892297 22760421793920 run.py:483] Algo bellman_ford step 9127 current loss 0.037050, current_train_items 292096.
I0302 19:01:53.923751 22760421793920 run.py:483] Algo bellman_ford step 9128 current loss 0.034680, current_train_items 292128.
I0302 19:01:53.955397 22760421793920 run.py:483] Algo bellman_ford step 9129 current loss 0.064577, current_train_items 292160.
I0302 19:01:53.973654 22760421793920 run.py:483] Algo bellman_ford step 9130 current loss 0.000904, current_train_items 292192.
I0302 19:01:53.989325 22760421793920 run.py:483] Algo bellman_ford step 9131 current loss 0.004480, current_train_items 292224.
I0302 19:01:54.013232 22760421793920 run.py:483] Algo bellman_ford step 9132 current loss 0.054320, current_train_items 292256.
I0302 19:01:54.044273 22760421793920 run.py:483] Algo bellman_ford step 9133 current loss 0.073958, current_train_items 292288.
I0302 19:01:54.075139 22760421793920 run.py:483] Algo bellman_ford step 9134 current loss 0.028375, current_train_items 292320.
I0302 19:01:54.093522 22760421793920 run.py:483] Algo bellman_ford step 9135 current loss 0.001353, current_train_items 292352.
I0302 19:01:54.108805 22760421793920 run.py:483] Algo bellman_ford step 9136 current loss 0.006809, current_train_items 292384.
I0302 19:01:54.131184 22760421793920 run.py:483] Algo bellman_ford step 9137 current loss 0.030922, current_train_items 292416.
I0302 19:01:54.161336 22760421793920 run.py:483] Algo bellman_ford step 9138 current loss 0.034712, current_train_items 292448.
I0302 19:01:54.192137 22760421793920 run.py:483] Algo bellman_ford step 9139 current loss 0.041074, current_train_items 292480.
I0302 19:01:54.210114 22760421793920 run.py:483] Algo bellman_ford step 9140 current loss 0.002393, current_train_items 292512.
I0302 19:01:54.225878 22760421793920 run.py:483] Algo bellman_ford step 9141 current loss 0.007284, current_train_items 292544.
I0302 19:01:54.248862 22760421793920 run.py:483] Algo bellman_ford step 9142 current loss 0.062856, current_train_items 292576.
I0302 19:01:54.278851 22760421793920 run.py:483] Algo bellman_ford step 9143 current loss 0.042197, current_train_items 292608.
I0302 19:01:54.311879 22760421793920 run.py:483] Algo bellman_ford step 9144 current loss 0.087230, current_train_items 292640.
I0302 19:01:54.329872 22760421793920 run.py:483] Algo bellman_ford step 9145 current loss 0.003869, current_train_items 292672.
I0302 19:01:54.345799 22760421793920 run.py:483] Algo bellman_ford step 9146 current loss 0.073406, current_train_items 292704.
I0302 19:01:54.369109 22760421793920 run.py:483] Algo bellman_ford step 9147 current loss 0.025261, current_train_items 292736.
I0302 19:01:54.399625 22760421793920 run.py:483] Algo bellman_ford step 9148 current loss 0.037753, current_train_items 292768.
I0302 19:01:54.428229 22760421793920 run.py:483] Algo bellman_ford step 9149 current loss 0.046197, current_train_items 292800.
I0302 19:01:54.446187 22760421793920 run.py:483] Algo bellman_ford step 9150 current loss 0.002363, current_train_items 292832.
I0302 19:01:54.453598 22760421793920 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:01:54.453710 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:54.470161 22760421793920 run.py:483] Algo bellman_ford step 9151 current loss 0.015653, current_train_items 292864.
I0302 19:01:54.493888 22760421793920 run.py:483] Algo bellman_ford step 9152 current loss 0.021631, current_train_items 292896.
I0302 19:01:54.525125 22760421793920 run.py:483] Algo bellman_ford step 9153 current loss 0.048461, current_train_items 292928.
I0302 19:01:54.555422 22760421793920 run.py:483] Algo bellman_ford step 9154 current loss 0.036083, current_train_items 292960.
I0302 19:01:54.573769 22760421793920 run.py:483] Algo bellman_ford step 9155 current loss 0.002633, current_train_items 292992.
I0302 19:01:54.590391 22760421793920 run.py:483] Algo bellman_ford step 9156 current loss 0.016371, current_train_items 293024.
I0302 19:01:54.614229 22760421793920 run.py:483] Algo bellman_ford step 9157 current loss 0.046115, current_train_items 293056.
I0302 19:01:54.644286 22760421793920 run.py:483] Algo bellman_ford step 9158 current loss 0.039220, current_train_items 293088.
I0302 19:01:54.677510 22760421793920 run.py:483] Algo bellman_ford step 9159 current loss 0.058607, current_train_items 293120.
I0302 19:01:54.695671 22760421793920 run.py:483] Algo bellman_ford step 9160 current loss 0.005753, current_train_items 293152.
I0302 19:01:54.711442 22760421793920 run.py:483] Algo bellman_ford step 9161 current loss 0.027380, current_train_items 293184.
I0302 19:01:54.734801 22760421793920 run.py:483] Algo bellman_ford step 9162 current loss 0.040322, current_train_items 293216.
I0302 19:01:54.764798 22760421793920 run.py:483] Algo bellman_ford step 9163 current loss 0.045943, current_train_items 293248.
I0302 19:01:54.798535 22760421793920 run.py:483] Algo bellman_ford step 9164 current loss 0.038889, current_train_items 293280.
I0302 19:01:54.816791 22760421793920 run.py:483] Algo bellman_ford step 9165 current loss 0.002388, current_train_items 293312.
I0302 19:01:54.832589 22760421793920 run.py:483] Algo bellman_ford step 9166 current loss 0.026378, current_train_items 293344.
I0302 19:01:54.855789 22760421793920 run.py:483] Algo bellman_ford step 9167 current loss 0.029910, current_train_items 293376.
I0302 19:01:54.886615 22760421793920 run.py:483] Algo bellman_ford step 9168 current loss 0.033997, current_train_items 293408.
I0302 19:01:54.917968 22760421793920 run.py:483] Algo bellman_ford step 9169 current loss 0.042993, current_train_items 293440.
I0302 19:01:54.936072 22760421793920 run.py:483] Algo bellman_ford step 9170 current loss 0.000581, current_train_items 293472.
I0302 19:01:54.951581 22760421793920 run.py:483] Algo bellman_ford step 9171 current loss 0.007871, current_train_items 293504.
I0302 19:01:54.976333 22760421793920 run.py:483] Algo bellman_ford step 9172 current loss 0.047598, current_train_items 293536.
I0302 19:01:55.006749 22760421793920 run.py:483] Algo bellman_ford step 9173 current loss 0.045039, current_train_items 293568.
I0302 19:01:55.039523 22760421793920 run.py:483] Algo bellman_ford step 9174 current loss 0.082036, current_train_items 293600.
I0302 19:01:55.057563 22760421793920 run.py:483] Algo bellman_ford step 9175 current loss 0.010673, current_train_items 293632.
I0302 19:01:55.073736 22760421793920 run.py:483] Algo bellman_ford step 9176 current loss 0.001627, current_train_items 293664.
I0302 19:01:55.096232 22760421793920 run.py:483] Algo bellman_ford step 9177 current loss 0.036249, current_train_items 293696.
I0302 19:01:55.125158 22760421793920 run.py:483] Algo bellman_ford step 9178 current loss 0.059316, current_train_items 293728.
I0302 19:01:55.158667 22760421793920 run.py:483] Algo bellman_ford step 9179 current loss 0.069026, current_train_items 293760.
I0302 19:01:55.176530 22760421793920 run.py:483] Algo bellman_ford step 9180 current loss 0.001394, current_train_items 293792.
I0302 19:01:55.192494 22760421793920 run.py:483] Algo bellman_ford step 9181 current loss 0.030613, current_train_items 293824.
I0302 19:01:55.216771 22760421793920 run.py:483] Algo bellman_ford step 9182 current loss 0.166781, current_train_items 293856.
I0302 19:01:55.246108 22760421793920 run.py:483] Algo bellman_ford step 9183 current loss 0.093340, current_train_items 293888.
I0302 19:01:55.277379 22760421793920 run.py:483] Algo bellman_ford step 9184 current loss 0.099387, current_train_items 293920.
I0302 19:01:55.295329 22760421793920 run.py:483] Algo bellman_ford step 9185 current loss 0.014254, current_train_items 293952.
I0302 19:01:55.311223 22760421793920 run.py:483] Algo bellman_ford step 9186 current loss 0.025580, current_train_items 293984.
I0302 19:01:55.335106 22760421793920 run.py:483] Algo bellman_ford step 9187 current loss 0.037723, current_train_items 294016.
I0302 19:01:55.366807 22760421793920 run.py:483] Algo bellman_ford step 9188 current loss 0.074818, current_train_items 294048.
I0302 19:01:55.401099 22760421793920 run.py:483] Algo bellman_ford step 9189 current loss 0.084785, current_train_items 294080.
I0302 19:01:55.419234 22760421793920 run.py:483] Algo bellman_ford step 9190 current loss 0.005244, current_train_items 294112.
I0302 19:01:55.434627 22760421793920 run.py:483] Algo bellman_ford step 9191 current loss 0.031739, current_train_items 294144.
I0302 19:01:55.458303 22760421793920 run.py:483] Algo bellman_ford step 9192 current loss 0.043323, current_train_items 294176.
I0302 19:01:55.488987 22760421793920 run.py:483] Algo bellman_ford step 9193 current loss 0.056609, current_train_items 294208.
I0302 19:01:55.520448 22760421793920 run.py:483] Algo bellman_ford step 9194 current loss 0.037376, current_train_items 294240.
I0302 19:01:55.538622 22760421793920 run.py:483] Algo bellman_ford step 9195 current loss 0.002671, current_train_items 294272.
I0302 19:01:55.554837 22760421793920 run.py:483] Algo bellman_ford step 9196 current loss 0.020622, current_train_items 294304.
I0302 19:01:55.578643 22760421793920 run.py:483] Algo bellman_ford step 9197 current loss 0.045129, current_train_items 294336.
I0302 19:01:55.608274 22760421793920 run.py:483] Algo bellman_ford step 9198 current loss 0.030012, current_train_items 294368.
I0302 19:01:55.639344 22760421793920 run.py:483] Algo bellman_ford step 9199 current loss 0.046331, current_train_items 294400.
I0302 19:01:55.657544 22760421793920 run.py:483] Algo bellman_ford step 9200 current loss 0.006387, current_train_items 294432.
I0302 19:01:55.664959 22760421793920 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:01:55.665068 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:01:55.681459 22760421793920 run.py:483] Algo bellman_ford step 9201 current loss 0.022808, current_train_items 294464.
I0302 19:01:55.704856 22760421793920 run.py:483] Algo bellman_ford step 9202 current loss 0.040971, current_train_items 294496.
I0302 19:01:55.737118 22760421793920 run.py:483] Algo bellman_ford step 9203 current loss 0.050811, current_train_items 294528.
I0302 19:01:55.770463 22760421793920 run.py:483] Algo bellman_ford step 9204 current loss 0.038045, current_train_items 294560.
I0302 19:01:55.789100 22760421793920 run.py:483] Algo bellman_ford step 9205 current loss 0.028216, current_train_items 294592.
I0302 19:01:55.804908 22760421793920 run.py:483] Algo bellman_ford step 9206 current loss 0.016991, current_train_items 294624.
I0302 19:01:55.828256 22760421793920 run.py:483] Algo bellman_ford step 9207 current loss 0.040321, current_train_items 294656.
I0302 19:01:55.859237 22760421793920 run.py:483] Algo bellman_ford step 9208 current loss 0.042486, current_train_items 294688.
I0302 19:01:55.889329 22760421793920 run.py:483] Algo bellman_ford step 9209 current loss 0.053584, current_train_items 294720.
I0302 19:01:55.907414 22760421793920 run.py:483] Algo bellman_ford step 9210 current loss 0.004641, current_train_items 294752.
I0302 19:01:55.923653 22760421793920 run.py:483] Algo bellman_ford step 9211 current loss 0.049346, current_train_items 294784.
I0302 19:01:55.947156 22760421793920 run.py:483] Algo bellman_ford step 9212 current loss 0.085155, current_train_items 294816.
I0302 19:01:55.977410 22760421793920 run.py:483] Algo bellman_ford step 9213 current loss 0.046732, current_train_items 294848.
I0302 19:01:56.008533 22760421793920 run.py:483] Algo bellman_ford step 9214 current loss 0.050838, current_train_items 294880.
I0302 19:01:56.026394 22760421793920 run.py:483] Algo bellman_ford step 9215 current loss 0.017644, current_train_items 294912.
I0302 19:01:56.042340 22760421793920 run.py:483] Algo bellman_ford step 9216 current loss 0.010794, current_train_items 294944.
I0302 19:01:56.065369 22760421793920 run.py:483] Algo bellman_ford step 9217 current loss 0.022205, current_train_items 294976.
I0302 19:01:56.095263 22760421793920 run.py:483] Algo bellman_ford step 9218 current loss 0.076610, current_train_items 295008.
I0302 19:01:56.127080 22760421793920 run.py:483] Algo bellman_ford step 9219 current loss 0.073421, current_train_items 295040.
I0302 19:01:56.145748 22760421793920 run.py:483] Algo bellman_ford step 9220 current loss 0.037963, current_train_items 295072.
I0302 19:01:56.161470 22760421793920 run.py:483] Algo bellman_ford step 9221 current loss 0.021024, current_train_items 295104.
I0302 19:01:56.185002 22760421793920 run.py:483] Algo bellman_ford step 9222 current loss 0.057434, current_train_items 295136.
I0302 19:01:56.214941 22760421793920 run.py:483] Algo bellman_ford step 9223 current loss 0.043208, current_train_items 295168.
I0302 19:01:56.247000 22760421793920 run.py:483] Algo bellman_ford step 9224 current loss 0.042446, current_train_items 295200.
I0302 19:01:56.265270 22760421793920 run.py:483] Algo bellman_ford step 9225 current loss 0.002584, current_train_items 295232.
I0302 19:01:56.280845 22760421793920 run.py:483] Algo bellman_ford step 9226 current loss 0.021679, current_train_items 295264.
I0302 19:01:56.304453 22760421793920 run.py:483] Algo bellman_ford step 9227 current loss 0.028952, current_train_items 295296.
I0302 19:01:56.335038 22760421793920 run.py:483] Algo bellman_ford step 9228 current loss 0.026777, current_train_items 295328.
I0302 19:01:56.364715 22760421793920 run.py:483] Algo bellman_ford step 9229 current loss 0.027942, current_train_items 295360.
I0302 19:01:56.382847 22760421793920 run.py:483] Algo bellman_ford step 9230 current loss 0.002072, current_train_items 295392.
I0302 19:01:56.399014 22760421793920 run.py:483] Algo bellman_ford step 9231 current loss 0.025518, current_train_items 295424.
I0302 19:01:56.422621 22760421793920 run.py:483] Algo bellman_ford step 9232 current loss 0.027055, current_train_items 295456.
I0302 19:01:56.455245 22760421793920 run.py:483] Algo bellman_ford step 9233 current loss 0.046271, current_train_items 295488.
I0302 19:01:56.487685 22760421793920 run.py:483] Algo bellman_ford step 9234 current loss 0.035193, current_train_items 295520.
I0302 19:01:56.505495 22760421793920 run.py:483] Algo bellman_ford step 9235 current loss 0.001187, current_train_items 295552.
I0302 19:01:56.521358 22760421793920 run.py:483] Algo bellman_ford step 9236 current loss 0.029292, current_train_items 295584.
I0302 19:01:56.544425 22760421793920 run.py:483] Algo bellman_ford step 9237 current loss 0.014243, current_train_items 295616.
I0302 19:01:56.576119 22760421793920 run.py:483] Algo bellman_ford step 9238 current loss 0.026193, current_train_items 295648.
I0302 19:01:56.608820 22760421793920 run.py:483] Algo bellman_ford step 9239 current loss 0.028059, current_train_items 295680.
I0302 19:01:56.626972 22760421793920 run.py:483] Algo bellman_ford step 9240 current loss 0.001531, current_train_items 295712.
I0302 19:01:56.643201 22760421793920 run.py:483] Algo bellman_ford step 9241 current loss 0.025272, current_train_items 295744.
I0302 19:01:56.666967 22760421793920 run.py:483] Algo bellman_ford step 9242 current loss 0.030763, current_train_items 295776.
I0302 19:01:56.697636 22760421793920 run.py:483] Algo bellman_ford step 9243 current loss 0.033285, current_train_items 295808.
I0302 19:01:56.728654 22760421793920 run.py:483] Algo bellman_ford step 9244 current loss 0.069234, current_train_items 295840.
I0302 19:01:56.746953 22760421793920 run.py:483] Algo bellman_ford step 9245 current loss 0.001541, current_train_items 295872.
I0302 19:01:56.762711 22760421793920 run.py:483] Algo bellman_ford step 9246 current loss 0.014406, current_train_items 295904.
I0302 19:01:56.785289 22760421793920 run.py:483] Algo bellman_ford step 9247 current loss 0.020057, current_train_items 295936.
I0302 19:01:56.815909 22760421793920 run.py:483] Algo bellman_ford step 9248 current loss 0.076665, current_train_items 295968.
I0302 19:01:56.848692 22760421793920 run.py:483] Algo bellman_ford step 9249 current loss 0.063407, current_train_items 296000.
I0302 19:01:56.866997 22760421793920 run.py:483] Algo bellman_ford step 9250 current loss 0.002446, current_train_items 296032.
I0302 19:01:56.874687 22760421793920 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:01:56.874797 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:01:56.891032 22760421793920 run.py:483] Algo bellman_ford step 9251 current loss 0.009806, current_train_items 296064.
I0302 19:01:56.915138 22760421793920 run.py:483] Algo bellman_ford step 9252 current loss 0.038836, current_train_items 296096.
I0302 19:01:56.945431 22760421793920 run.py:483] Algo bellman_ford step 9253 current loss 0.034523, current_train_items 296128.
I0302 19:01:56.976520 22760421793920 run.py:483] Algo bellman_ford step 9254 current loss 0.041309, current_train_items 296160.
I0302 19:01:56.995226 22760421793920 run.py:483] Algo bellman_ford step 9255 current loss 0.002109, current_train_items 296192.
I0302 19:01:57.011349 22760421793920 run.py:483] Algo bellman_ford step 9256 current loss 0.026243, current_train_items 296224.
I0302 19:01:57.034675 22760421793920 run.py:483] Algo bellman_ford step 9257 current loss 0.027968, current_train_items 296256.
I0302 19:01:57.065812 22760421793920 run.py:483] Algo bellman_ford step 9258 current loss 0.047296, current_train_items 296288.
I0302 19:01:57.098005 22760421793920 run.py:483] Algo bellman_ford step 9259 current loss 0.032346, current_train_items 296320.
I0302 19:01:57.116736 22760421793920 run.py:483] Algo bellman_ford step 9260 current loss 0.009089, current_train_items 296352.
I0302 19:01:57.133138 22760421793920 run.py:483] Algo bellman_ford step 9261 current loss 0.015038, current_train_items 296384.
I0302 19:01:57.155655 22760421793920 run.py:483] Algo bellman_ford step 9262 current loss 0.029152, current_train_items 296416.
I0302 19:01:57.186607 22760421793920 run.py:483] Algo bellman_ford step 9263 current loss 0.035722, current_train_items 296448.
I0302 19:01:57.218793 22760421793920 run.py:483] Algo bellman_ford step 9264 current loss 0.038545, current_train_items 296480.
I0302 19:01:57.237094 22760421793920 run.py:483] Algo bellman_ford step 9265 current loss 0.004855, current_train_items 296512.
I0302 19:01:57.253008 22760421793920 run.py:483] Algo bellman_ford step 9266 current loss 0.013816, current_train_items 296544.
I0302 19:01:57.274987 22760421793920 run.py:483] Algo bellman_ford step 9267 current loss 0.080038, current_train_items 296576.
I0302 19:01:57.305813 22760421793920 run.py:483] Algo bellman_ford step 9268 current loss 0.065934, current_train_items 296608.
I0302 19:01:57.337677 22760421793920 run.py:483] Algo bellman_ford step 9269 current loss 0.192259, current_train_items 296640.
I0302 19:01:57.355951 22760421793920 run.py:483] Algo bellman_ford step 9270 current loss 0.002449, current_train_items 296672.
I0302 19:01:57.371545 22760421793920 run.py:483] Algo bellman_ford step 9271 current loss 0.011237, current_train_items 296704.
I0302 19:01:57.394979 22760421793920 run.py:483] Algo bellman_ford step 9272 current loss 0.017787, current_train_items 296736.
I0302 19:01:57.427083 22760421793920 run.py:483] Algo bellman_ford step 9273 current loss 0.047414, current_train_items 296768.
I0302 19:01:57.460120 22760421793920 run.py:483] Algo bellman_ford step 9274 current loss 0.044101, current_train_items 296800.
I0302 19:01:57.478622 22760421793920 run.py:483] Algo bellman_ford step 9275 current loss 0.038229, current_train_items 296832.
I0302 19:01:57.494364 22760421793920 run.py:483] Algo bellman_ford step 9276 current loss 0.008028, current_train_items 296864.
I0302 19:01:57.517410 22760421793920 run.py:483] Algo bellman_ford step 9277 current loss 0.048429, current_train_items 296896.
I0302 19:01:57.548782 22760421793920 run.py:483] Algo bellman_ford step 9278 current loss 0.064007, current_train_items 296928.
I0302 19:01:57.581506 22760421793920 run.py:483] Algo bellman_ford step 9279 current loss 0.108224, current_train_items 296960.
I0302 19:01:57.599677 22760421793920 run.py:483] Algo bellman_ford step 9280 current loss 0.001659, current_train_items 296992.
I0302 19:01:57.615612 22760421793920 run.py:483] Algo bellman_ford step 9281 current loss 0.028158, current_train_items 297024.
I0302 19:01:57.638647 22760421793920 run.py:483] Algo bellman_ford step 9282 current loss 0.014439, current_train_items 297056.
I0302 19:01:57.667883 22760421793920 run.py:483] Algo bellman_ford step 9283 current loss 0.028936, current_train_items 297088.
I0302 19:01:57.699107 22760421793920 run.py:483] Algo bellman_ford step 9284 current loss 0.069895, current_train_items 297120.
I0302 19:01:57.717644 22760421793920 run.py:483] Algo bellman_ford step 9285 current loss 0.003419, current_train_items 297152.
I0302 19:01:57.732722 22760421793920 run.py:483] Algo bellman_ford step 9286 current loss 0.010027, current_train_items 297184.
I0302 19:01:57.757198 22760421793920 run.py:483] Algo bellman_ford step 9287 current loss 0.063167, current_train_items 297216.
I0302 19:01:57.787129 22760421793920 run.py:483] Algo bellman_ford step 9288 current loss 0.039285, current_train_items 297248.
I0302 19:01:57.820812 22760421793920 run.py:483] Algo bellman_ford step 9289 current loss 0.056106, current_train_items 297280.
I0302 19:01:57.838977 22760421793920 run.py:483] Algo bellman_ford step 9290 current loss 0.001410, current_train_items 297312.
I0302 19:01:57.854595 22760421793920 run.py:483] Algo bellman_ford step 9291 current loss 0.022954, current_train_items 297344.
I0302 19:01:57.878877 22760421793920 run.py:483] Algo bellman_ford step 9292 current loss 0.055719, current_train_items 297376.
I0302 19:01:57.908510 22760421793920 run.py:483] Algo bellman_ford step 9293 current loss 0.049293, current_train_items 297408.
I0302 19:01:57.941763 22760421793920 run.py:483] Algo bellman_ford step 9294 current loss 0.056651, current_train_items 297440.
I0302 19:01:57.960331 22760421793920 run.py:483] Algo bellman_ford step 9295 current loss 0.001747, current_train_items 297472.
I0302 19:01:57.976154 22760421793920 run.py:483] Algo bellman_ford step 9296 current loss 0.004302, current_train_items 297504.
I0302 19:01:57.999255 22760421793920 run.py:483] Algo bellman_ford step 9297 current loss 0.042258, current_train_items 297536.
I0302 19:01:58.030158 22760421793920 run.py:483] Algo bellman_ford step 9298 current loss 0.094725, current_train_items 297568.
I0302 19:01:58.063045 22760421793920 run.py:483] Algo bellman_ford step 9299 current loss 0.060780, current_train_items 297600.
I0302 19:01:58.081664 22760421793920 run.py:483] Algo bellman_ford step 9300 current loss 0.001260, current_train_items 297632.
I0302 19:01:58.089184 22760421793920 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:01:58.089295 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:01:58.105817 22760421793920 run.py:483] Algo bellman_ford step 9301 current loss 0.010297, current_train_items 297664.
I0302 19:01:58.129604 22760421793920 run.py:483] Algo bellman_ford step 9302 current loss 0.020523, current_train_items 297696.
I0302 19:01:58.160810 22760421793920 run.py:483] Algo bellman_ford step 9303 current loss 0.041286, current_train_items 297728.
I0302 19:01:58.194972 22760421793920 run.py:483] Algo bellman_ford step 9304 current loss 0.071494, current_train_items 297760.
I0302 19:01:58.213636 22760421793920 run.py:483] Algo bellman_ford step 9305 current loss 0.003594, current_train_items 297792.
I0302 19:01:58.229253 22760421793920 run.py:483] Algo bellman_ford step 9306 current loss 0.016912, current_train_items 297824.
I0302 19:01:58.253206 22760421793920 run.py:483] Algo bellman_ford step 9307 current loss 0.029083, current_train_items 297856.
I0302 19:01:58.283935 22760421793920 run.py:483] Algo bellman_ford step 9308 current loss 0.097071, current_train_items 297888.
I0302 19:01:58.315847 22760421793920 run.py:483] Algo bellman_ford step 9309 current loss 0.054090, current_train_items 297920.
I0302 19:01:58.334219 22760421793920 run.py:483] Algo bellman_ford step 9310 current loss 0.001613, current_train_items 297952.
I0302 19:01:58.349945 22760421793920 run.py:483] Algo bellman_ford step 9311 current loss 0.003853, current_train_items 297984.
I0302 19:01:58.373221 22760421793920 run.py:483] Algo bellman_ford step 9312 current loss 0.027159, current_train_items 298016.
I0302 19:01:58.403110 22760421793920 run.py:483] Algo bellman_ford step 9313 current loss 0.039978, current_train_items 298048.
I0302 19:01:58.435790 22760421793920 run.py:483] Algo bellman_ford step 9314 current loss 0.061412, current_train_items 298080.
I0302 19:01:58.454204 22760421793920 run.py:483] Algo bellman_ford step 9315 current loss 0.003987, current_train_items 298112.
I0302 19:01:58.469656 22760421793920 run.py:483] Algo bellman_ford step 9316 current loss 0.004794, current_train_items 298144.
I0302 19:01:58.493207 22760421793920 run.py:483] Algo bellman_ford step 9317 current loss 0.038258, current_train_items 298176.
I0302 19:01:58.524883 22760421793920 run.py:483] Algo bellman_ford step 9318 current loss 0.058059, current_train_items 298208.
I0302 19:01:58.557312 22760421793920 run.py:483] Algo bellman_ford step 9319 current loss 0.034782, current_train_items 298240.
I0302 19:01:58.575604 22760421793920 run.py:483] Algo bellman_ford step 9320 current loss 0.001496, current_train_items 298272.
I0302 19:01:58.591479 22760421793920 run.py:483] Algo bellman_ford step 9321 current loss 0.021850, current_train_items 298304.
I0302 19:01:58.613474 22760421793920 run.py:483] Algo bellman_ford step 9322 current loss 0.022924, current_train_items 298336.
I0302 19:01:58.643454 22760421793920 run.py:483] Algo bellman_ford step 9323 current loss 0.022754, current_train_items 298368.
I0302 19:01:58.678345 22760421793920 run.py:483] Algo bellman_ford step 9324 current loss 0.067057, current_train_items 298400.
I0302 19:01:58.696817 22760421793920 run.py:483] Algo bellman_ford step 9325 current loss 0.000761, current_train_items 298432.
I0302 19:01:58.712507 22760421793920 run.py:483] Algo bellman_ford step 9326 current loss 0.023931, current_train_items 298464.
I0302 19:01:58.736278 22760421793920 run.py:483] Algo bellman_ford step 9327 current loss 0.038471, current_train_items 298496.
I0302 19:01:58.767709 22760421793920 run.py:483] Algo bellman_ford step 9328 current loss 0.069575, current_train_items 298528.
I0302 19:01:58.797353 22760421793920 run.py:483] Algo bellman_ford step 9329 current loss 0.033248, current_train_items 298560.
I0302 19:01:58.815920 22760421793920 run.py:483] Algo bellman_ford step 9330 current loss 0.001582, current_train_items 298592.
I0302 19:01:58.831766 22760421793920 run.py:483] Algo bellman_ford step 9331 current loss 0.012172, current_train_items 298624.
I0302 19:01:58.854649 22760421793920 run.py:483] Algo bellman_ford step 9332 current loss 0.043652, current_train_items 298656.
I0302 19:01:58.885187 22760421793920 run.py:483] Algo bellman_ford step 9333 current loss 0.039360, current_train_items 298688.
I0302 19:01:58.916533 22760421793920 run.py:483] Algo bellman_ford step 9334 current loss 0.105182, current_train_items 298720.
I0302 19:01:58.935075 22760421793920 run.py:483] Algo bellman_ford step 9335 current loss 0.002495, current_train_items 298752.
I0302 19:01:58.950982 22760421793920 run.py:483] Algo bellman_ford step 9336 current loss 0.039866, current_train_items 298784.
I0302 19:01:58.975176 22760421793920 run.py:483] Algo bellman_ford step 9337 current loss 0.036929, current_train_items 298816.
I0302 19:01:59.006027 22760421793920 run.py:483] Algo bellman_ford step 9338 current loss 0.028717, current_train_items 298848.
I0302 19:01:59.038893 22760421793920 run.py:483] Algo bellman_ford step 9339 current loss 0.060004, current_train_items 298880.
I0302 19:01:59.057361 22760421793920 run.py:483] Algo bellman_ford step 9340 current loss 0.011934, current_train_items 298912.
I0302 19:01:59.072737 22760421793920 run.py:483] Algo bellman_ford step 9341 current loss 0.009663, current_train_items 298944.
I0302 19:01:59.096373 22760421793920 run.py:483] Algo bellman_ford step 9342 current loss 0.028055, current_train_items 298976.
I0302 19:01:59.127441 22760421793920 run.py:483] Algo bellman_ford step 9343 current loss 0.033961, current_train_items 299008.
I0302 19:01:59.160251 22760421793920 run.py:483] Algo bellman_ford step 9344 current loss 0.040317, current_train_items 299040.
I0302 19:01:59.178663 22760421793920 run.py:483] Algo bellman_ford step 9345 current loss 0.001262, current_train_items 299072.
I0302 19:01:59.194446 22760421793920 run.py:483] Algo bellman_ford step 9346 current loss 0.002925, current_train_items 299104.
I0302 19:01:59.217984 22760421793920 run.py:483] Algo bellman_ford step 9347 current loss 0.015026, current_train_items 299136.
I0302 19:01:59.249355 22760421793920 run.py:483] Algo bellman_ford step 9348 current loss 0.047378, current_train_items 299168.
I0302 19:01:59.280742 22760421793920 run.py:483] Algo bellman_ford step 9349 current loss 0.068834, current_train_items 299200.
I0302 19:01:59.299058 22760421793920 run.py:483] Algo bellman_ford step 9350 current loss 0.080627, current_train_items 299232.
I0302 19:01:59.306495 22760421793920 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:01:59.306604 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.995, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:59.322797 22760421793920 run.py:483] Algo bellman_ford step 9351 current loss 0.001322, current_train_items 299264.
I0302 19:01:59.347413 22760421793920 run.py:483] Algo bellman_ford step 9352 current loss 0.049629, current_train_items 299296.
I0302 19:01:59.379112 22760421793920 run.py:483] Algo bellman_ford step 9353 current loss 0.044387, current_train_items 299328.
I0302 19:01:59.413671 22760421793920 run.py:483] Algo bellman_ford step 9354 current loss 0.064905, current_train_items 299360.
I0302 19:01:59.432153 22760421793920 run.py:483] Algo bellman_ford step 9355 current loss 0.018784, current_train_items 299392.
I0302 19:01:59.447912 22760421793920 run.py:483] Algo bellman_ford step 9356 current loss 0.014190, current_train_items 299424.
I0302 19:01:59.471410 22760421793920 run.py:483] Algo bellman_ford step 9357 current loss 0.083448, current_train_items 299456.
I0302 19:01:59.502454 22760421793920 run.py:483] Algo bellman_ford step 9358 current loss 0.036527, current_train_items 299488.
I0302 19:01:59.534122 22760421793920 run.py:483] Algo bellman_ford step 9359 current loss 0.059282, current_train_items 299520.
I0302 19:01:59.552712 22760421793920 run.py:483] Algo bellman_ford step 9360 current loss 0.019225, current_train_items 299552.
I0302 19:01:59.568319 22760421793920 run.py:483] Algo bellman_ford step 9361 current loss 0.015903, current_train_items 299584.
I0302 19:01:59.591373 22760421793920 run.py:483] Algo bellman_ford step 9362 current loss 0.077065, current_train_items 299616.
I0302 19:01:59.622931 22760421793920 run.py:483] Algo bellman_ford step 9363 current loss 0.117118, current_train_items 299648.
I0302 19:01:59.656119 22760421793920 run.py:483] Algo bellman_ford step 9364 current loss 0.093780, current_train_items 299680.
I0302 19:01:59.674429 22760421793920 run.py:483] Algo bellman_ford step 9365 current loss 0.025718, current_train_items 299712.
I0302 19:01:59.690572 22760421793920 run.py:483] Algo bellman_ford step 9366 current loss 0.052969, current_train_items 299744.
I0302 19:01:59.715990 22760421793920 run.py:483] Algo bellman_ford step 9367 current loss 0.046488, current_train_items 299776.
I0302 19:01:59.747522 22760421793920 run.py:483] Algo bellman_ford step 9368 current loss 0.072533, current_train_items 299808.
I0302 19:01:59.779272 22760421793920 run.py:483] Algo bellman_ford step 9369 current loss 0.059851, current_train_items 299840.
I0302 19:01:59.798006 22760421793920 run.py:483] Algo bellman_ford step 9370 current loss 0.005619, current_train_items 299872.
I0302 19:01:59.814072 22760421793920 run.py:483] Algo bellman_ford step 9371 current loss 0.037442, current_train_items 299904.
I0302 19:01:59.837911 22760421793920 run.py:483] Algo bellman_ford step 9372 current loss 0.016543, current_train_items 299936.
I0302 19:01:59.869472 22760421793920 run.py:483] Algo bellman_ford step 9373 current loss 0.044339, current_train_items 299968.
I0302 19:01:59.902112 22760421793920 run.py:483] Algo bellman_ford step 9374 current loss 0.031336, current_train_items 300000.
I0302 19:01:59.920286 22760421793920 run.py:483] Algo bellman_ford step 9375 current loss 0.017467, current_train_items 300032.
I0302 19:01:59.935540 22760421793920 run.py:483] Algo bellman_ford step 9376 current loss 0.014053, current_train_items 300064.
I0302 19:01:59.958930 22760421793920 run.py:483] Algo bellman_ford step 9377 current loss 0.028091, current_train_items 300096.
I0302 19:01:59.988569 22760421793920 run.py:483] Algo bellman_ford step 9378 current loss 0.021905, current_train_items 300128.
I0302 19:02:00.020877 22760421793920 run.py:483] Algo bellman_ford step 9379 current loss 0.062415, current_train_items 300160.
I0302 19:02:00.039164 22760421793920 run.py:483] Algo bellman_ford step 9380 current loss 0.000985, current_train_items 300192.
I0302 19:02:00.054429 22760421793920 run.py:483] Algo bellman_ford step 9381 current loss 0.039552, current_train_items 300224.
I0302 19:02:00.078114 22760421793920 run.py:483] Algo bellman_ford step 9382 current loss 0.031038, current_train_items 300256.
I0302 19:02:00.107927 22760421793920 run.py:483] Algo bellman_ford step 9383 current loss 0.059891, current_train_items 300288.
I0302 19:02:00.140366 22760421793920 run.py:483] Algo bellman_ford step 9384 current loss 0.059025, current_train_items 300320.
I0302 19:02:00.158710 22760421793920 run.py:483] Algo bellman_ford step 9385 current loss 0.002330, current_train_items 300352.
I0302 19:02:00.174855 22760421793920 run.py:483] Algo bellman_ford step 9386 current loss 0.034388, current_train_items 300384.
I0302 19:02:00.198422 22760421793920 run.py:483] Algo bellman_ford step 9387 current loss 0.034245, current_train_items 300416.
I0302 19:02:00.229212 22760421793920 run.py:483] Algo bellman_ford step 9388 current loss 0.037651, current_train_items 300448.
I0302 19:02:00.263651 22760421793920 run.py:483] Algo bellman_ford step 9389 current loss 0.029029, current_train_items 300480.
I0302 19:02:00.281972 22760421793920 run.py:483] Algo bellman_ford step 9390 current loss 0.008622, current_train_items 300512.
I0302 19:02:00.297935 22760421793920 run.py:483] Algo bellman_ford step 9391 current loss 0.010477, current_train_items 300544.
I0302 19:02:00.321475 22760421793920 run.py:483] Algo bellman_ford step 9392 current loss 0.009859, current_train_items 300576.
I0302 19:02:00.351744 22760421793920 run.py:483] Algo bellman_ford step 9393 current loss 0.041932, current_train_items 300608.
I0302 19:02:00.383446 22760421793920 run.py:483] Algo bellman_ford step 9394 current loss 0.066310, current_train_items 300640.
I0302 19:02:00.401911 22760421793920 run.py:483] Algo bellman_ford step 9395 current loss 0.015747, current_train_items 300672.
I0302 19:02:00.417887 22760421793920 run.py:483] Algo bellman_ford step 9396 current loss 0.030104, current_train_items 300704.
I0302 19:02:00.442037 22760421793920 run.py:483] Algo bellman_ford step 9397 current loss 0.038443, current_train_items 300736.
I0302 19:02:00.472637 22760421793920 run.py:483] Algo bellman_ford step 9398 current loss 0.054048, current_train_items 300768.
I0302 19:02:00.505357 22760421793920 run.py:483] Algo bellman_ford step 9399 current loss 0.047858, current_train_items 300800.
I0302 19:02:00.524064 22760421793920 run.py:483] Algo bellman_ford step 9400 current loss 0.005332, current_train_items 300832.
I0302 19:02:00.531481 22760421793920 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:00.531591 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.995, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0302 19:02:00.560110 22760421793920 run.py:483] Algo bellman_ford step 9401 current loss 0.013259, current_train_items 300864.
I0302 19:02:00.584161 22760421793920 run.py:483] Algo bellman_ford step 9402 current loss 0.040211, current_train_items 300896.
I0302 19:02:00.615000 22760421793920 run.py:483] Algo bellman_ford step 9403 current loss 0.023213, current_train_items 300928.
I0302 19:02:00.648577 22760421793920 run.py:483] Algo bellman_ford step 9404 current loss 0.052315, current_train_items 300960.
I0302 19:02:00.667549 22760421793920 run.py:483] Algo bellman_ford step 9405 current loss 0.003374, current_train_items 300992.
I0302 19:02:00.683188 22760421793920 run.py:483] Algo bellman_ford step 9406 current loss 0.007950, current_train_items 301024.
I0302 19:02:00.707871 22760421793920 run.py:483] Algo bellman_ford step 9407 current loss 0.024945, current_train_items 301056.
I0302 19:02:00.738215 22760421793920 run.py:483] Algo bellman_ford step 9408 current loss 0.022217, current_train_items 301088.
I0302 19:02:00.771966 22760421793920 run.py:483] Algo bellman_ford step 9409 current loss 0.060048, current_train_items 301120.
I0302 19:02:00.790499 22760421793920 run.py:483] Algo bellman_ford step 9410 current loss 0.003821, current_train_items 301152.
I0302 19:02:00.806157 22760421793920 run.py:483] Algo bellman_ford step 9411 current loss 0.005155, current_train_items 301184.
I0302 19:02:00.829883 22760421793920 run.py:483] Algo bellman_ford step 9412 current loss 0.022518, current_train_items 301216.
I0302 19:02:00.860621 22760421793920 run.py:483] Algo bellman_ford step 9413 current loss 0.021657, current_train_items 301248.
I0302 19:02:00.892080 22760421793920 run.py:483] Algo bellman_ford step 9414 current loss 0.048145, current_train_items 301280.
I0302 19:02:00.910621 22760421793920 run.py:483] Algo bellman_ford step 9415 current loss 0.009501, current_train_items 301312.
I0302 19:02:00.926570 22760421793920 run.py:483] Algo bellman_ford step 9416 current loss 0.014933, current_train_items 301344.
I0302 19:02:00.950536 22760421793920 run.py:483] Algo bellman_ford step 9417 current loss 0.075571, current_train_items 301376.
I0302 19:02:00.980646 22760421793920 run.py:483] Algo bellman_ford step 9418 current loss 0.025638, current_train_items 301408.
I0302 19:02:01.011024 22760421793920 run.py:483] Algo bellman_ford step 9419 current loss 0.024134, current_train_items 301440.
I0302 19:02:01.029162 22760421793920 run.py:483] Algo bellman_ford step 9420 current loss 0.005952, current_train_items 301472.
I0302 19:02:01.045093 22760421793920 run.py:483] Algo bellman_ford step 9421 current loss 0.028776, current_train_items 301504.
I0302 19:02:01.068701 22760421793920 run.py:483] Algo bellman_ford step 9422 current loss 0.011267, current_train_items 301536.
I0302 19:02:01.099227 22760421793920 run.py:483] Algo bellman_ford step 9423 current loss 0.015207, current_train_items 301568.
I0302 19:02:01.131179 22760421793920 run.py:483] Algo bellman_ford step 9424 current loss 0.063003, current_train_items 301600.
I0302 19:02:01.149605 22760421793920 run.py:483] Algo bellman_ford step 9425 current loss 0.024892, current_train_items 301632.
I0302 19:02:01.164938 22760421793920 run.py:483] Algo bellman_ford step 9426 current loss 0.011067, current_train_items 301664.
I0302 19:02:01.187970 22760421793920 run.py:483] Algo bellman_ford step 9427 current loss 0.016353, current_train_items 301696.
I0302 19:02:01.219157 22760421793920 run.py:483] Algo bellman_ford step 9428 current loss 0.038908, current_train_items 301728.
I0302 19:02:01.248745 22760421793920 run.py:483] Algo bellman_ford step 9429 current loss 0.203094, current_train_items 301760.
I0302 19:02:01.267176 22760421793920 run.py:483] Algo bellman_ford step 9430 current loss 0.001917, current_train_items 301792.
I0302 19:02:01.282906 22760421793920 run.py:483] Algo bellman_ford step 9431 current loss 0.028087, current_train_items 301824.
I0302 19:02:01.306092 22760421793920 run.py:483] Algo bellman_ford step 9432 current loss 0.051871, current_train_items 301856.
I0302 19:02:01.335990 22760421793920 run.py:483] Algo bellman_ford step 9433 current loss 0.026489, current_train_items 301888.
I0302 19:02:01.369111 22760421793920 run.py:483] Algo bellman_ford step 9434 current loss 0.053446, current_train_items 301920.
I0302 19:02:01.387571 22760421793920 run.py:483] Algo bellman_ford step 9435 current loss 0.000714, current_train_items 301952.
I0302 19:02:01.403451 22760421793920 run.py:483] Algo bellman_ford step 9436 current loss 0.011274, current_train_items 301984.
I0302 19:02:01.426721 22760421793920 run.py:483] Algo bellman_ford step 9437 current loss 0.019894, current_train_items 302016.
I0302 19:02:01.457544 22760421793920 run.py:483] Algo bellman_ford step 9438 current loss 0.038773, current_train_items 302048.
I0302 19:02:01.488985 22760421793920 run.py:483] Algo bellman_ford step 9439 current loss 0.060582, current_train_items 302080.
I0302 19:02:01.507345 22760421793920 run.py:483] Algo bellman_ford step 9440 current loss 0.007876, current_train_items 302112.
I0302 19:02:01.523356 22760421793920 run.py:483] Algo bellman_ford step 9441 current loss 0.010418, current_train_items 302144.
I0302 19:02:01.546681 22760421793920 run.py:483] Algo bellman_ford step 9442 current loss 0.029413, current_train_items 302176.
I0302 19:02:01.578009 22760421793920 run.py:483] Algo bellman_ford step 9443 current loss 0.027345, current_train_items 302208.
I0302 19:02:01.612607 22760421793920 run.py:483] Algo bellman_ford step 9444 current loss 0.060104, current_train_items 302240.
I0302 19:02:01.631002 22760421793920 run.py:483] Algo bellman_ford step 9445 current loss 0.013414, current_train_items 302272.
I0302 19:02:01.646262 22760421793920 run.py:483] Algo bellman_ford step 9446 current loss 0.004740, current_train_items 302304.
I0302 19:02:01.669068 22760421793920 run.py:483] Algo bellman_ford step 9447 current loss 0.019228, current_train_items 302336.
I0302 19:02:01.699170 22760421793920 run.py:483] Algo bellman_ford step 9448 current loss 0.040091, current_train_items 302368.
I0302 19:02:01.729933 22760421793920 run.py:483] Algo bellman_ford step 9449 current loss 0.041313, current_train_items 302400.
I0302 19:02:01.748369 22760421793920 run.py:483] Algo bellman_ford step 9450 current loss 0.010619, current_train_items 302432.
I0302 19:02:01.755779 22760421793920 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:01.755892 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:02:01.772325 22760421793920 run.py:483] Algo bellman_ford step 9451 current loss 0.006533, current_train_items 302464.
I0302 19:02:01.795305 22760421793920 run.py:483] Algo bellman_ford step 9452 current loss 0.031102, current_train_items 302496.
I0302 19:02:01.826584 22760421793920 run.py:483] Algo bellman_ford step 9453 current loss 0.043142, current_train_items 302528.
I0302 19:02:01.859829 22760421793920 run.py:483] Algo bellman_ford step 9454 current loss 0.058817, current_train_items 302560.
I0302 19:02:01.878696 22760421793920 run.py:483] Algo bellman_ford step 9455 current loss 0.001361, current_train_items 302592.
I0302 19:02:01.894758 22760421793920 run.py:483] Algo bellman_ford step 9456 current loss 0.006345, current_train_items 302624.
I0302 19:02:01.917840 22760421793920 run.py:483] Algo bellman_ford step 9457 current loss 0.026835, current_train_items 302656.
I0302 19:02:01.948826 22760421793920 run.py:483] Algo bellman_ford step 9458 current loss 0.031158, current_train_items 302688.
I0302 19:02:01.979447 22760421793920 run.py:483] Algo bellman_ford step 9459 current loss 0.037884, current_train_items 302720.
I0302 19:02:01.998077 22760421793920 run.py:483] Algo bellman_ford step 9460 current loss 0.002528, current_train_items 302752.
I0302 19:02:02.013963 22760421793920 run.py:483] Algo bellman_ford step 9461 current loss 0.040743, current_train_items 302784.
I0302 19:02:02.036822 22760421793920 run.py:483] Algo bellman_ford step 9462 current loss 0.009970, current_train_items 302816.
I0302 19:02:02.068135 22760421793920 run.py:483] Algo bellman_ford step 9463 current loss 0.028666, current_train_items 302848.
I0302 19:02:02.099876 22760421793920 run.py:483] Algo bellman_ford step 9464 current loss 0.025465, current_train_items 302880.
I0302 19:02:02.118294 22760421793920 run.py:483] Algo bellman_ford step 9465 current loss 0.002392, current_train_items 302912.
I0302 19:02:02.134403 22760421793920 run.py:483] Algo bellman_ford step 9466 current loss 0.028316, current_train_items 302944.
I0302 19:02:02.159569 22760421793920 run.py:483] Algo bellman_ford step 9467 current loss 0.091923, current_train_items 302976.
I0302 19:02:02.190989 22760421793920 run.py:483] Algo bellman_ford step 9468 current loss 0.042933, current_train_items 303008.
I0302 19:02:02.224105 22760421793920 run.py:483] Algo bellman_ford step 9469 current loss 0.106329, current_train_items 303040.
I0302 19:02:02.242506 22760421793920 run.py:483] Algo bellman_ford step 9470 current loss 0.003507, current_train_items 303072.
I0302 19:02:02.257869 22760421793920 run.py:483] Algo bellman_ford step 9471 current loss 0.008912, current_train_items 303104.
I0302 19:02:02.281320 22760421793920 run.py:483] Algo bellman_ford step 9472 current loss 0.026192, current_train_items 303136.
I0302 19:02:02.312382 22760421793920 run.py:483] Algo bellman_ford step 9473 current loss 0.033930, current_train_items 303168.
I0302 19:02:02.343635 22760421793920 run.py:483] Algo bellman_ford step 9474 current loss 0.060546, current_train_items 303200.
I0302 19:02:02.361525 22760421793920 run.py:483] Algo bellman_ford step 9475 current loss 0.003513, current_train_items 303232.
I0302 19:02:02.377269 22760421793920 run.py:483] Algo bellman_ford step 9476 current loss 0.044087, current_train_items 303264.
I0302 19:02:02.400411 22760421793920 run.py:483] Algo bellman_ford step 9477 current loss 0.019214, current_train_items 303296.
I0302 19:02:02.430431 22760421793920 run.py:483] Algo bellman_ford step 9478 current loss 0.025957, current_train_items 303328.
I0302 19:02:02.462045 22760421793920 run.py:483] Algo bellman_ford step 9479 current loss 0.081149, current_train_items 303360.
I0302 19:02:02.480571 22760421793920 run.py:483] Algo bellman_ford step 9480 current loss 0.001456, current_train_items 303392.
I0302 19:02:02.496172 22760421793920 run.py:483] Algo bellman_ford step 9481 current loss 0.016581, current_train_items 303424.
I0302 19:02:02.521105 22760421793920 run.py:483] Algo bellman_ford step 9482 current loss 0.080719, current_train_items 303456.
I0302 19:02:02.550539 22760421793920 run.py:483] Algo bellman_ford step 9483 current loss 0.044839, current_train_items 303488.
I0302 19:02:02.581637 22760421793920 run.py:483] Algo bellman_ford step 9484 current loss 0.044278, current_train_items 303520.
I0302 19:02:02.599758 22760421793920 run.py:483] Algo bellman_ford step 9485 current loss 0.001591, current_train_items 303552.
I0302 19:02:02.615693 22760421793920 run.py:483] Algo bellman_ford step 9486 current loss 0.006703, current_train_items 303584.
I0302 19:02:02.640277 22760421793920 run.py:483] Algo bellman_ford step 9487 current loss 0.046302, current_train_items 303616.
I0302 19:02:02.672009 22760421793920 run.py:483] Algo bellman_ford step 9488 current loss 0.034641, current_train_items 303648.
I0302 19:02:02.703317 22760421793920 run.py:483] Algo bellman_ford step 9489 current loss 0.038062, current_train_items 303680.
I0302 19:02:02.721474 22760421793920 run.py:483] Algo bellman_ford step 9490 current loss 0.001544, current_train_items 303712.
I0302 19:02:02.737008 22760421793920 run.py:483] Algo bellman_ford step 9491 current loss 0.022218, current_train_items 303744.
I0302 19:02:02.761099 22760421793920 run.py:483] Algo bellman_ford step 9492 current loss 0.019875, current_train_items 303776.
I0302 19:02:02.790556 22760421793920 run.py:483] Algo bellman_ford step 9493 current loss 0.014383, current_train_items 303808.
I0302 19:02:02.822773 22760421793920 run.py:483] Algo bellman_ford step 9494 current loss 0.069463, current_train_items 303840.
I0302 19:02:02.841383 22760421793920 run.py:483] Algo bellman_ford step 9495 current loss 0.014064, current_train_items 303872.
I0302 19:02:02.857546 22760421793920 run.py:483] Algo bellman_ford step 9496 current loss 0.017398, current_train_items 303904.
I0302 19:02:02.880734 22760421793920 run.py:483] Algo bellman_ford step 9497 current loss 0.027253, current_train_items 303936.
I0302 19:02:02.910492 22760421793920 run.py:483] Algo bellman_ford step 9498 current loss 0.033276, current_train_items 303968.
I0302 19:02:02.943299 22760421793920 run.py:483] Algo bellman_ford step 9499 current loss 0.046337, current_train_items 304000.
I0302 19:02:02.962000 22760421793920 run.py:483] Algo bellman_ford step 9500 current loss 0.001796, current_train_items 304032.
I0302 19:02:02.969455 22760421793920 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:02.969563 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:02:02.985780 22760421793920 run.py:483] Algo bellman_ford step 9501 current loss 0.012470, current_train_items 304064.
I0302 19:02:03.010082 22760421793920 run.py:483] Algo bellman_ford step 9502 current loss 0.055895, current_train_items 304096.
I0302 19:02:03.041020 22760421793920 run.py:483] Algo bellman_ford step 9503 current loss 0.014377, current_train_items 304128.
I0302 19:02:03.072577 22760421793920 run.py:483] Algo bellman_ford step 9504 current loss 0.044265, current_train_items 304160.
I0302 19:02:03.091172 22760421793920 run.py:483] Algo bellman_ford step 9505 current loss 0.005464, current_train_items 304192.
I0302 19:02:03.107167 22760421793920 run.py:483] Algo bellman_ford step 9506 current loss 0.006732, current_train_items 304224.
I0302 19:02:03.131498 22760421793920 run.py:483] Algo bellman_ford step 9507 current loss 0.059256, current_train_items 304256.
I0302 19:02:03.161025 22760421793920 run.py:483] Algo bellman_ford step 9508 current loss 0.044403, current_train_items 304288.
I0302 19:02:03.195311 22760421793920 run.py:483] Algo bellman_ford step 9509 current loss 0.093118, current_train_items 304320.
I0302 19:02:03.213725 22760421793920 run.py:483] Algo bellman_ford step 9510 current loss 0.002502, current_train_items 304352.
I0302 19:02:03.230050 22760421793920 run.py:483] Algo bellman_ford step 9511 current loss 0.007650, current_train_items 304384.
I0302 19:02:03.253693 22760421793920 run.py:483] Algo bellman_ford step 9512 current loss 0.034984, current_train_items 304416.
I0302 19:02:03.284504 22760421793920 run.py:483] Algo bellman_ford step 9513 current loss 0.027124, current_train_items 304448.
I0302 19:02:03.315687 22760421793920 run.py:483] Algo bellman_ford step 9514 current loss 0.030023, current_train_items 304480.
I0302 19:02:03.334258 22760421793920 run.py:483] Algo bellman_ford step 9515 current loss 0.015706, current_train_items 304512.
I0302 19:02:03.349598 22760421793920 run.py:483] Algo bellman_ford step 9516 current loss 0.021671, current_train_items 304544.
I0302 19:02:03.373454 22760421793920 run.py:483] Algo bellman_ford step 9517 current loss 0.028472, current_train_items 304576.
I0302 19:02:03.404442 22760421793920 run.py:483] Algo bellman_ford step 9518 current loss 0.049104, current_train_items 304608.
I0302 19:02:03.435317 22760421793920 run.py:483] Algo bellman_ford step 9519 current loss 0.056058, current_train_items 304640.
I0302 19:02:03.453596 22760421793920 run.py:483] Algo bellman_ford step 9520 current loss 0.002333, current_train_items 304672.
I0302 19:02:03.468687 22760421793920 run.py:483] Algo bellman_ford step 9521 current loss 0.005911, current_train_items 304704.
I0302 19:02:03.491501 22760421793920 run.py:483] Algo bellman_ford step 9522 current loss 0.034031, current_train_items 304736.
I0302 19:02:03.520599 22760421793920 run.py:483] Algo bellman_ford step 9523 current loss 0.057844, current_train_items 304768.
I0302 19:02:03.551892 22760421793920 run.py:483] Algo bellman_ford step 9524 current loss 0.042028, current_train_items 304800.
I0302 19:02:03.569842 22760421793920 run.py:483] Algo bellman_ford step 9525 current loss 0.006066, current_train_items 304832.
I0302 19:02:03.585555 22760421793920 run.py:483] Algo bellman_ford step 9526 current loss 0.011272, current_train_items 304864.
I0302 19:02:03.609367 22760421793920 run.py:483] Algo bellman_ford step 9527 current loss 0.140535, current_train_items 304896.
I0302 19:02:03.640617 22760421793920 run.py:483] Algo bellman_ford step 9528 current loss 0.125397, current_train_items 304928.
I0302 19:02:03.673915 22760421793920 run.py:483] Algo bellman_ford step 9529 current loss 0.084785, current_train_items 304960.
I0302 19:02:03.692217 22760421793920 run.py:483] Algo bellman_ford step 9530 current loss 0.006709, current_train_items 304992.
I0302 19:02:03.707921 22760421793920 run.py:483] Algo bellman_ford step 9531 current loss 0.022196, current_train_items 305024.
I0302 19:02:03.730888 22760421793920 run.py:483] Algo bellman_ford step 9532 current loss 0.029232, current_train_items 305056.
I0302 19:02:03.762912 22760421793920 run.py:483] Algo bellman_ford step 9533 current loss 0.055747, current_train_items 305088.
I0302 19:02:03.794781 22760421793920 run.py:483] Algo bellman_ford step 9534 current loss 0.055871, current_train_items 305120.
I0302 19:02:03.813543 22760421793920 run.py:483] Algo bellman_ford step 9535 current loss 0.012338, current_train_items 305152.
I0302 19:02:03.829337 22760421793920 run.py:483] Algo bellman_ford step 9536 current loss 0.026806, current_train_items 305184.
I0302 19:02:03.852744 22760421793920 run.py:483] Algo bellman_ford step 9537 current loss 0.021999, current_train_items 305216.
I0302 19:02:03.884469 22760421793920 run.py:483] Algo bellman_ford step 9538 current loss 0.044921, current_train_items 305248.
I0302 19:02:03.914415 22760421793920 run.py:483] Algo bellman_ford step 9539 current loss 0.043795, current_train_items 305280.
I0302 19:02:03.932745 22760421793920 run.py:483] Algo bellman_ford step 9540 current loss 0.002348, current_train_items 305312.
I0302 19:02:03.948352 22760421793920 run.py:483] Algo bellman_ford step 9541 current loss 0.035012, current_train_items 305344.
I0302 19:02:03.971186 22760421793920 run.py:483] Algo bellman_ford step 9542 current loss 0.029651, current_train_items 305376.
I0302 19:02:04.003164 22760421793920 run.py:483] Algo bellman_ford step 9543 current loss 0.049873, current_train_items 305408.
I0302 19:02:04.035629 22760421793920 run.py:483] Algo bellman_ford step 9544 current loss 0.040826, current_train_items 305440.
I0302 19:02:04.054161 22760421793920 run.py:483] Algo bellman_ford step 9545 current loss 0.000661, current_train_items 305472.
I0302 19:02:04.069360 22760421793920 run.py:483] Algo bellman_ford step 9546 current loss 0.014595, current_train_items 305504.
I0302 19:02:04.090649 22760421793920 run.py:483] Algo bellman_ford step 9547 current loss 0.008141, current_train_items 305536.
I0302 19:02:04.120952 22760421793920 run.py:483] Algo bellman_ford step 9548 current loss 0.038275, current_train_items 305568.
I0302 19:02:04.153406 22760421793920 run.py:483] Algo bellman_ford step 9549 current loss 0.059015, current_train_items 305600.
I0302 19:02:04.171792 22760421793920 run.py:483] Algo bellman_ford step 9550 current loss 0.000902, current_train_items 305632.
I0302 19:02:04.179414 22760421793920 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:04.179524 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:02:04.195846 22760421793920 run.py:483] Algo bellman_ford step 9551 current loss 0.010695, current_train_items 305664.
I0302 19:02:04.220103 22760421793920 run.py:483] Algo bellman_ford step 9552 current loss 0.067869, current_train_items 305696.
I0302 19:02:04.252712 22760421793920 run.py:483] Algo bellman_ford step 9553 current loss 0.043591, current_train_items 305728.
I0302 19:02:04.284386 22760421793920 run.py:483] Algo bellman_ford step 9554 current loss 0.035137, current_train_items 305760.
I0302 19:02:04.303183 22760421793920 run.py:483] Algo bellman_ford step 9555 current loss 0.000676, current_train_items 305792.
I0302 19:02:04.318803 22760421793920 run.py:483] Algo bellman_ford step 9556 current loss 0.010283, current_train_items 305824.
I0302 19:02:04.342634 22760421793920 run.py:483] Algo bellman_ford step 9557 current loss 0.046049, current_train_items 305856.
I0302 19:02:04.373179 22760421793920 run.py:483] Algo bellman_ford step 9558 current loss 0.061531, current_train_items 305888.
I0302 19:02:04.404496 22760421793920 run.py:483] Algo bellman_ford step 9559 current loss 0.073985, current_train_items 305920.
I0302 19:02:04.422840 22760421793920 run.py:483] Algo bellman_ford step 9560 current loss 0.005446, current_train_items 305952.
I0302 19:02:04.438541 22760421793920 run.py:483] Algo bellman_ford step 9561 current loss 0.004023, current_train_items 305984.
I0302 19:02:04.461619 22760421793920 run.py:483] Algo bellman_ford step 9562 current loss 0.015474, current_train_items 306016.
I0302 19:02:04.492185 22760421793920 run.py:483] Algo bellman_ford step 9563 current loss 0.021529, current_train_items 306048.
I0302 19:02:04.523750 22760421793920 run.py:483] Algo bellman_ford step 9564 current loss 0.030469, current_train_items 306080.
I0302 19:02:04.541823 22760421793920 run.py:483] Algo bellman_ford step 9565 current loss 0.035241, current_train_items 306112.
I0302 19:02:04.558055 22760421793920 run.py:483] Algo bellman_ford step 9566 current loss 0.019674, current_train_items 306144.
I0302 19:02:04.582584 22760421793920 run.py:483] Algo bellman_ford step 9567 current loss 0.084071, current_train_items 306176.
I0302 19:02:04.613678 22760421793920 run.py:483] Algo bellman_ford step 9568 current loss 0.081313, current_train_items 306208.
I0302 19:02:04.645642 22760421793920 run.py:483] Algo bellman_ford step 9569 current loss 0.078149, current_train_items 306240.
I0302 19:02:04.664140 22760421793920 run.py:483] Algo bellman_ford step 9570 current loss 0.007404, current_train_items 306272.
I0302 19:02:04.679813 22760421793920 run.py:483] Algo bellman_ford step 9571 current loss 0.011179, current_train_items 306304.
I0302 19:02:04.704344 22760421793920 run.py:483] Algo bellman_ford step 9572 current loss 0.105238, current_train_items 306336.
I0302 19:02:04.734372 22760421793920 run.py:483] Algo bellman_ford step 9573 current loss 0.056787, current_train_items 306368.
I0302 19:02:04.766913 22760421793920 run.py:483] Algo bellman_ford step 9574 current loss 0.095440, current_train_items 306400.
I0302 19:02:04.785406 22760421793920 run.py:483] Algo bellman_ford step 9575 current loss 0.005456, current_train_items 306432.
I0302 19:02:04.801779 22760421793920 run.py:483] Algo bellman_ford step 9576 current loss 0.025599, current_train_items 306464.
I0302 19:02:04.826329 22760421793920 run.py:483] Algo bellman_ford step 9577 current loss 0.040506, current_train_items 306496.
I0302 19:02:04.856683 22760421793920 run.py:483] Algo bellman_ford step 9578 current loss 0.023445, current_train_items 306528.
I0302 19:02:04.889002 22760421793920 run.py:483] Algo bellman_ford step 9579 current loss 0.058247, current_train_items 306560.
I0302 19:02:04.907407 22760421793920 run.py:483] Algo bellman_ford step 9580 current loss 0.005227, current_train_items 306592.
I0302 19:02:04.923031 22760421793920 run.py:483] Algo bellman_ford step 9581 current loss 0.072446, current_train_items 306624.
I0302 19:02:04.947771 22760421793920 run.py:483] Algo bellman_ford step 9582 current loss 0.021344, current_train_items 306656.
I0302 19:02:04.979645 22760421793920 run.py:483] Algo bellman_ford step 9583 current loss 0.026563, current_train_items 306688.
I0302 19:02:05.012557 22760421793920 run.py:483] Algo bellman_ford step 9584 current loss 0.063251, current_train_items 306720.
I0302 19:02:05.030924 22760421793920 run.py:483] Algo bellman_ford step 9585 current loss 0.020865, current_train_items 306752.
I0302 19:02:05.046717 22760421793920 run.py:483] Algo bellman_ford step 9586 current loss 0.013654, current_train_items 306784.
I0302 19:02:05.070365 22760421793920 run.py:483] Algo bellman_ford step 9587 current loss 0.039216, current_train_items 306816.
I0302 19:02:05.101535 22760421793920 run.py:483] Algo bellman_ford step 9588 current loss 0.050682, current_train_items 306848.
I0302 19:02:05.135071 22760421793920 run.py:483] Algo bellman_ford step 9589 current loss 0.079419, current_train_items 306880.
I0302 19:02:05.153173 22760421793920 run.py:483] Algo bellman_ford step 9590 current loss 0.002245, current_train_items 306912.
I0302 19:02:05.168882 22760421793920 run.py:483] Algo bellman_ford step 9591 current loss 0.017658, current_train_items 306944.
I0302 19:02:05.192515 22760421793920 run.py:483] Algo bellman_ford step 9592 current loss 0.016607, current_train_items 306976.
I0302 19:02:05.223376 22760421793920 run.py:483] Algo bellman_ford step 9593 current loss 0.051746, current_train_items 307008.
I0302 19:02:05.255168 22760421793920 run.py:483] Algo bellman_ford step 9594 current loss 0.050231, current_train_items 307040.
I0302 19:02:05.273539 22760421793920 run.py:483] Algo bellman_ford step 9595 current loss 0.002371, current_train_items 307072.
I0302 19:02:05.289551 22760421793920 run.py:483] Algo bellman_ford step 9596 current loss 0.016824, current_train_items 307104.
I0302 19:02:05.313895 22760421793920 run.py:483] Algo bellman_ford step 9597 current loss 0.014795, current_train_items 307136.
I0302 19:02:05.343285 22760421793920 run.py:483] Algo bellman_ford step 9598 current loss 0.039840, current_train_items 307168.
I0302 19:02:05.375133 22760421793920 run.py:483] Algo bellman_ford step 9599 current loss 0.038654, current_train_items 307200.
I0302 19:02:05.393501 22760421793920 run.py:483] Algo bellman_ford step 9600 current loss 0.004751, current_train_items 307232.
I0302 19:02:05.401086 22760421793920 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:05.401196 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:02:05.417963 22760421793920 run.py:483] Algo bellman_ford step 9601 current loss 0.014028, current_train_items 307264.
I0302 19:02:05.442528 22760421793920 run.py:483] Algo bellman_ford step 9602 current loss 0.027932, current_train_items 307296.
I0302 19:02:05.472779 22760421793920 run.py:483] Algo bellman_ford step 9603 current loss 0.029758, current_train_items 307328.
I0302 19:02:05.506836 22760421793920 run.py:483] Algo bellman_ford step 9604 current loss 0.036613, current_train_items 307360.
I0302 19:02:05.525480 22760421793920 run.py:483] Algo bellman_ford step 9605 current loss 0.005838, current_train_items 307392.
I0302 19:02:05.540954 22760421793920 run.py:483] Algo bellman_ford step 9606 current loss 0.011491, current_train_items 307424.
I0302 19:02:05.564524 22760421793920 run.py:483] Algo bellman_ford step 9607 current loss 0.039309, current_train_items 307456.
I0302 19:02:05.594698 22760421793920 run.py:483] Algo bellman_ford step 9608 current loss 0.041039, current_train_items 307488.
I0302 19:02:05.626859 22760421793920 run.py:483] Algo bellman_ford step 9609 current loss 0.041676, current_train_items 307520.
I0302 19:02:05.645119 22760421793920 run.py:483] Algo bellman_ford step 9610 current loss 0.037600, current_train_items 307552.
I0302 19:02:05.661039 22760421793920 run.py:483] Algo bellman_ford step 9611 current loss 0.016767, current_train_items 307584.
I0302 19:02:05.685274 22760421793920 run.py:483] Algo bellman_ford step 9612 current loss 0.024988, current_train_items 307616.
I0302 19:02:05.715984 22760421793920 run.py:483] Algo bellman_ford step 9613 current loss 0.055265, current_train_items 307648.
I0302 19:02:05.748192 22760421793920 run.py:483] Algo bellman_ford step 9614 current loss 0.059207, current_train_items 307680.
I0302 19:02:05.766524 22760421793920 run.py:483] Algo bellman_ford step 9615 current loss 0.001314, current_train_items 307712.
I0302 19:02:05.782312 22760421793920 run.py:483] Algo bellman_ford step 9616 current loss 0.022216, current_train_items 307744.
I0302 19:02:05.805981 22760421793920 run.py:483] Algo bellman_ford step 9617 current loss 0.024125, current_train_items 307776.
I0302 19:02:05.836213 22760421793920 run.py:483] Algo bellman_ford step 9618 current loss 0.026908, current_train_items 307808.
I0302 19:02:05.868591 22760421793920 run.py:483] Algo bellman_ford step 9619 current loss 0.034954, current_train_items 307840.
I0302 19:02:05.887211 22760421793920 run.py:483] Algo bellman_ford step 9620 current loss 0.003858, current_train_items 307872.
I0302 19:02:05.902851 22760421793920 run.py:483] Algo bellman_ford step 9621 current loss 0.014560, current_train_items 307904.
I0302 19:02:05.926761 22760421793920 run.py:483] Algo bellman_ford step 9622 current loss 0.030329, current_train_items 307936.
I0302 19:02:05.957482 22760421793920 run.py:483] Algo bellman_ford step 9623 current loss 0.039116, current_train_items 307968.
I0302 19:02:05.990193 22760421793920 run.py:483] Algo bellman_ford step 9624 current loss 0.079983, current_train_items 308000.
I0302 19:02:06.008560 22760421793920 run.py:483] Algo bellman_ford step 9625 current loss 0.004737, current_train_items 308032.
I0302 19:02:06.024289 22760421793920 run.py:483] Algo bellman_ford step 9626 current loss 0.006294, current_train_items 308064.
I0302 19:02:06.047608 22760421793920 run.py:483] Algo bellman_ford step 9627 current loss 0.018939, current_train_items 308096.
I0302 19:02:06.078013 22760421793920 run.py:483] Algo bellman_ford step 9628 current loss 0.017004, current_train_items 308128.
I0302 19:02:06.111460 22760421793920 run.py:483] Algo bellman_ford step 9629 current loss 0.055887, current_train_items 308160.
I0302 19:02:06.129846 22760421793920 run.py:483] Algo bellman_ford step 9630 current loss 0.020894, current_train_items 308192.
I0302 19:02:06.145957 22760421793920 run.py:483] Algo bellman_ford step 9631 current loss 0.006984, current_train_items 308224.
I0302 19:02:06.170685 22760421793920 run.py:483] Algo bellman_ford step 9632 current loss 0.120649, current_train_items 308256.
I0302 19:02:06.200698 22760421793920 run.py:483] Algo bellman_ford step 9633 current loss 0.049098, current_train_items 308288.
I0302 19:02:06.233755 22760421793920 run.py:483] Algo bellman_ford step 9634 current loss 0.095273, current_train_items 308320.
I0302 19:02:06.252103 22760421793920 run.py:483] Algo bellman_ford step 9635 current loss 0.004215, current_train_items 308352.
I0302 19:02:06.267860 22760421793920 run.py:483] Algo bellman_ford step 9636 current loss 0.009660, current_train_items 308384.
I0302 19:02:06.291476 22760421793920 run.py:483] Algo bellman_ford step 9637 current loss 0.013842, current_train_items 308416.
I0302 19:02:06.322674 22760421793920 run.py:483] Algo bellman_ford step 9638 current loss 0.028628, current_train_items 308448.
I0302 19:02:06.354761 22760421793920 run.py:483] Algo bellman_ford step 9639 current loss 0.096171, current_train_items 308480.
I0302 19:02:06.373308 22760421793920 run.py:483] Algo bellman_ford step 9640 current loss 0.004833, current_train_items 308512.
I0302 19:02:06.388753 22760421793920 run.py:483] Algo bellman_ford step 9641 current loss 0.008397, current_train_items 308544.
I0302 19:02:06.412592 22760421793920 run.py:483] Algo bellman_ford step 9642 current loss 0.047832, current_train_items 308576.
I0302 19:02:06.444831 22760421793920 run.py:483] Algo bellman_ford step 9643 current loss 0.028842, current_train_items 308608.
I0302 19:02:06.475321 22760421793920 run.py:483] Algo bellman_ford step 9644 current loss 0.034350, current_train_items 308640.
I0302 19:02:06.493484 22760421793920 run.py:483] Algo bellman_ford step 9645 current loss 0.005944, current_train_items 308672.
I0302 19:02:06.509464 22760421793920 run.py:483] Algo bellman_ford step 9646 current loss 0.040895, current_train_items 308704.
I0302 19:02:06.532256 22760421793920 run.py:483] Algo bellman_ford step 9647 current loss 0.012644, current_train_items 308736.
I0302 19:02:06.563474 22760421793920 run.py:483] Algo bellman_ford step 9648 current loss 0.062070, current_train_items 308768.
I0302 19:02:06.596067 22760421793920 run.py:483] Algo bellman_ford step 9649 current loss 0.042677, current_train_items 308800.
I0302 19:02:06.614388 22760421793920 run.py:483] Algo bellman_ford step 9650 current loss 0.001770, current_train_items 308832.
I0302 19:02:06.622053 22760421793920 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:06.622209 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:02:06.638662 22760421793920 run.py:483] Algo bellman_ford step 9651 current loss 0.013763, current_train_items 308864.
I0302 19:02:06.663782 22760421793920 run.py:483] Algo bellman_ford step 9652 current loss 0.039698, current_train_items 308896.
I0302 19:02:06.694871 22760421793920 run.py:483] Algo bellman_ford step 9653 current loss 0.111713, current_train_items 308928.
I0302 19:02:06.729043 22760421793920 run.py:483] Algo bellman_ford step 9654 current loss 0.050220, current_train_items 308960.
I0302 19:02:06.747697 22760421793920 run.py:483] Algo bellman_ford step 9655 current loss 0.002882, current_train_items 308992.
I0302 19:02:06.763974 22760421793920 run.py:483] Algo bellman_ford step 9656 current loss 0.017223, current_train_items 309024.
I0302 19:02:06.787169 22760421793920 run.py:483] Algo bellman_ford step 9657 current loss 0.072478, current_train_items 309056.
I0302 19:02:06.818632 22760421793920 run.py:483] Algo bellman_ford step 9658 current loss 0.098084, current_train_items 309088.
I0302 19:02:06.849487 22760421793920 run.py:483] Algo bellman_ford step 9659 current loss 0.093929, current_train_items 309120.
I0302 19:02:06.867650 22760421793920 run.py:483] Algo bellman_ford step 9660 current loss 0.014865, current_train_items 309152.
I0302 19:02:06.883711 22760421793920 run.py:483] Algo bellman_ford step 9661 current loss 0.008338, current_train_items 309184.
I0302 19:02:06.907161 22760421793920 run.py:483] Algo bellman_ford step 9662 current loss 0.121673, current_train_items 309216.
I0302 19:02:06.937982 22760421793920 run.py:483] Algo bellman_ford step 9663 current loss 0.055037, current_train_items 309248.
I0302 19:02:06.970363 22760421793920 run.py:483] Algo bellman_ford step 9664 current loss 0.100035, current_train_items 309280.
I0302 19:02:06.988610 22760421793920 run.py:483] Algo bellman_ford step 9665 current loss 0.011892, current_train_items 309312.
I0302 19:02:07.004483 22760421793920 run.py:483] Algo bellman_ford step 9666 current loss 0.015811, current_train_items 309344.
I0302 19:02:07.026999 22760421793920 run.py:483] Algo bellman_ford step 9667 current loss 0.030039, current_train_items 309376.
I0302 19:02:07.057044 22760421793920 run.py:483] Algo bellman_ford step 9668 current loss 0.079510, current_train_items 309408.
I0302 19:02:07.088917 22760421793920 run.py:483] Algo bellman_ford step 9669 current loss 0.083894, current_train_items 309440.
I0302 19:02:07.107300 22760421793920 run.py:483] Algo bellman_ford step 9670 current loss 0.002433, current_train_items 309472.
I0302 19:02:07.122976 22760421793920 run.py:483] Algo bellman_ford step 9671 current loss 0.006649, current_train_items 309504.
I0302 19:02:07.146538 22760421793920 run.py:483] Algo bellman_ford step 9672 current loss 0.017011, current_train_items 309536.
I0302 19:02:07.176408 22760421793920 run.py:483] Algo bellman_ford step 9673 current loss 0.026131, current_train_items 309568.
I0302 19:02:07.208166 22760421793920 run.py:483] Algo bellman_ford step 9674 current loss 0.041919, current_train_items 309600.
I0302 19:02:07.226319 22760421793920 run.py:483] Algo bellman_ford step 9675 current loss 0.001131, current_train_items 309632.
I0302 19:02:07.242458 22760421793920 run.py:483] Algo bellman_ford step 9676 current loss 0.009223, current_train_items 309664.
I0302 19:02:07.266076 22760421793920 run.py:483] Algo bellman_ford step 9677 current loss 0.034117, current_train_items 309696.
I0302 19:02:07.295171 22760421793920 run.py:483] Algo bellman_ford step 9678 current loss 0.042837, current_train_items 309728.
I0302 19:02:07.327707 22760421793920 run.py:483] Algo bellman_ford step 9679 current loss 0.035022, current_train_items 309760.
I0302 19:02:07.345932 22760421793920 run.py:483] Algo bellman_ford step 9680 current loss 0.001535, current_train_items 309792.
I0302 19:02:07.361888 22760421793920 run.py:483] Algo bellman_ford step 9681 current loss 0.021591, current_train_items 309824.
I0302 19:02:07.384799 22760421793920 run.py:483] Algo bellman_ford step 9682 current loss 0.014971, current_train_items 309856.
I0302 19:02:07.414571 22760421793920 run.py:483] Algo bellman_ford step 9683 current loss 0.022496, current_train_items 309888.
I0302 19:02:07.445560 22760421793920 run.py:483] Algo bellman_ford step 9684 current loss 0.037908, current_train_items 309920.
I0302 19:02:07.463685 22760421793920 run.py:483] Algo bellman_ford step 9685 current loss 0.000916, current_train_items 309952.
I0302 19:02:07.479318 22760421793920 run.py:483] Algo bellman_ford step 9686 current loss 0.014032, current_train_items 309984.
I0302 19:02:07.502865 22760421793920 run.py:483] Algo bellman_ford step 9687 current loss 0.043739, current_train_items 310016.
I0302 19:02:07.533185 22760421793920 run.py:483] Algo bellman_ford step 9688 current loss 0.041961, current_train_items 310048.
I0302 19:02:07.565459 22760421793920 run.py:483] Algo bellman_ford step 9689 current loss 0.058772, current_train_items 310080.
I0302 19:02:07.583599 22760421793920 run.py:483] Algo bellman_ford step 9690 current loss 0.005332, current_train_items 310112.
I0302 19:02:07.598921 22760421793920 run.py:483] Algo bellman_ford step 9691 current loss 0.002772, current_train_items 310144.
I0302 19:02:07.622475 22760421793920 run.py:483] Algo bellman_ford step 9692 current loss 0.015395, current_train_items 310176.
I0302 19:02:07.652213 22760421793920 run.py:483] Algo bellman_ford step 9693 current loss 0.045073, current_train_items 310208.
I0302 19:02:07.684701 22760421793920 run.py:483] Algo bellman_ford step 9694 current loss 0.037244, current_train_items 310240.
I0302 19:02:07.702712 22760421793920 run.py:483] Algo bellman_ford step 9695 current loss 0.001601, current_train_items 310272.
I0302 19:02:07.718412 22760421793920 run.py:483] Algo bellman_ford step 9696 current loss 0.027685, current_train_items 310304.
I0302 19:02:07.740919 22760421793920 run.py:483] Algo bellman_ford step 9697 current loss 0.020334, current_train_items 310336.
I0302 19:02:07.773879 22760421793920 run.py:483] Algo bellman_ford step 9698 current loss 0.062039, current_train_items 310368.
I0302 19:02:07.807420 22760421793920 run.py:483] Algo bellman_ford step 9699 current loss 0.033519, current_train_items 310400.
I0302 19:02:07.825592 22760421793920 run.py:483] Algo bellman_ford step 9700 current loss 0.004619, current_train_items 310432.
I0302 19:02:07.833173 22760421793920 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:07.833287 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:02:07.849616 22760421793920 run.py:483] Algo bellman_ford step 9701 current loss 0.013008, current_train_items 310464.
I0302 19:02:07.873923 22760421793920 run.py:483] Algo bellman_ford step 9702 current loss 0.016889, current_train_items 310496.
I0302 19:02:07.905333 22760421793920 run.py:483] Algo bellman_ford step 9703 current loss 0.046193, current_train_items 310528.
I0302 19:02:07.937451 22760421793920 run.py:483] Algo bellman_ford step 9704 current loss 0.044425, current_train_items 310560.
I0302 19:02:07.956114 22760421793920 run.py:483] Algo bellman_ford step 9705 current loss 0.002374, current_train_items 310592.
I0302 19:02:07.971763 22760421793920 run.py:483] Algo bellman_ford step 9706 current loss 0.021649, current_train_items 310624.
I0302 19:02:07.996358 22760421793920 run.py:483] Algo bellman_ford step 9707 current loss 0.121589, current_train_items 310656.
I0302 19:02:08.027565 22760421793920 run.py:483] Algo bellman_ford step 9708 current loss 0.138145, current_train_items 310688.
I0302 19:02:08.057443 22760421793920 run.py:483] Algo bellman_ford step 9709 current loss 0.097199, current_train_items 310720.
I0302 19:02:08.075578 22760421793920 run.py:483] Algo bellman_ford step 9710 current loss 0.004302, current_train_items 310752.
I0302 19:02:08.091382 22760421793920 run.py:483] Algo bellman_ford step 9711 current loss 0.013029, current_train_items 310784.
I0302 19:02:08.114857 22760421793920 run.py:483] Algo bellman_ford step 9712 current loss 0.028364, current_train_items 310816.
I0302 19:02:08.145848 22760421793920 run.py:483] Algo bellman_ford step 9713 current loss 0.027329, current_train_items 310848.
I0302 19:02:08.178672 22760421793920 run.py:483] Algo bellman_ford step 9714 current loss 0.025858, current_train_items 310880.
I0302 19:02:08.196637 22760421793920 run.py:483] Algo bellman_ford step 9715 current loss 0.003234, current_train_items 310912.
I0302 19:02:08.212786 22760421793920 run.py:483] Algo bellman_ford step 9716 current loss 0.012580, current_train_items 310944.
I0302 19:02:08.235361 22760421793920 run.py:483] Algo bellman_ford step 9717 current loss 0.051230, current_train_items 310976.
I0302 19:02:08.266166 22760421793920 run.py:483] Algo bellman_ford step 9718 current loss 0.045097, current_train_items 311008.
I0302 19:02:08.295551 22760421793920 run.py:483] Algo bellman_ford step 9719 current loss 0.029179, current_train_items 311040.
I0302 19:02:08.313803 22760421793920 run.py:483] Algo bellman_ford step 9720 current loss 0.004252, current_train_items 311072.
I0302 19:02:08.329115 22760421793920 run.py:483] Algo bellman_ford step 9721 current loss 0.001823, current_train_items 311104.
I0302 19:02:08.351884 22760421793920 run.py:483] Algo bellman_ford step 9722 current loss 0.013986, current_train_items 311136.
I0302 19:02:08.382714 22760421793920 run.py:483] Algo bellman_ford step 9723 current loss 0.032776, current_train_items 311168.
I0302 19:02:08.414707 22760421793920 run.py:483] Algo bellman_ford step 9724 current loss 0.041338, current_train_items 311200.
I0302 19:02:08.432863 22760421793920 run.py:483] Algo bellman_ford step 9725 current loss 0.003900, current_train_items 311232.
I0302 19:02:08.448357 22760421793920 run.py:483] Algo bellman_ford step 9726 current loss 0.014732, current_train_items 311264.
I0302 19:02:08.471090 22760421793920 run.py:483] Algo bellman_ford step 9727 current loss 0.011397, current_train_items 311296.
I0302 19:02:08.502131 22760421793920 run.py:483] Algo bellman_ford step 9728 current loss 0.032505, current_train_items 311328.
I0302 19:02:08.534646 22760421793920 run.py:483] Algo bellman_ford step 9729 current loss 0.038437, current_train_items 311360.
I0302 19:02:08.553056 22760421793920 run.py:483] Algo bellman_ford step 9730 current loss 0.003081, current_train_items 311392.
I0302 19:02:08.569326 22760421793920 run.py:483] Algo bellman_ford step 9731 current loss 0.015356, current_train_items 311424.
I0302 19:02:08.592055 22760421793920 run.py:483] Algo bellman_ford step 9732 current loss 0.020719, current_train_items 311456.
I0302 19:02:08.622320 22760421793920 run.py:483] Algo bellman_ford step 9733 current loss 0.066860, current_train_items 311488.
I0302 19:02:08.653236 22760421793920 run.py:483] Algo bellman_ford step 9734 current loss 0.062967, current_train_items 311520.
I0302 19:02:08.671538 22760421793920 run.py:483] Algo bellman_ford step 9735 current loss 0.003291, current_train_items 311552.
I0302 19:02:08.687213 22760421793920 run.py:483] Algo bellman_ford step 9736 current loss 0.031334, current_train_items 311584.
I0302 19:02:08.710287 22760421793920 run.py:483] Algo bellman_ford step 9737 current loss 0.033151, current_train_items 311616.
I0302 19:02:08.742110 22760421793920 run.py:483] Algo bellman_ford step 9738 current loss 0.058924, current_train_items 311648.
I0302 19:02:08.775275 22760421793920 run.py:483] Algo bellman_ford step 9739 current loss 0.048625, current_train_items 311680.
I0302 19:02:08.793409 22760421793920 run.py:483] Algo bellman_ford step 9740 current loss 0.002478, current_train_items 311712.
I0302 19:02:08.809796 22760421793920 run.py:483] Algo bellman_ford step 9741 current loss 0.007434, current_train_items 311744.
I0302 19:02:08.834446 22760421793920 run.py:483] Algo bellman_ford step 9742 current loss 0.080940, current_train_items 311776.
I0302 19:02:08.865263 22760421793920 run.py:483] Algo bellman_ford step 9743 current loss 0.039237, current_train_items 311808.
I0302 19:02:08.897374 22760421793920 run.py:483] Algo bellman_ford step 9744 current loss 0.034740, current_train_items 311840.
I0302 19:02:08.915786 22760421793920 run.py:483] Algo bellman_ford step 9745 current loss 0.014145, current_train_items 311872.
I0302 19:02:08.932315 22760421793920 run.py:483] Algo bellman_ford step 9746 current loss 0.012127, current_train_items 311904.
I0302 19:02:08.955155 22760421793920 run.py:483] Algo bellman_ford step 9747 current loss 0.024079, current_train_items 311936.
I0302 19:02:08.983702 22760421793920 run.py:483] Algo bellman_ford step 9748 current loss 0.032848, current_train_items 311968.
I0302 19:02:09.014654 22760421793920 run.py:483] Algo bellman_ford step 9749 current loss 0.041555, current_train_items 312000.
I0302 19:02:09.032666 22760421793920 run.py:483] Algo bellman_ford step 9750 current loss 0.001493, current_train_items 312032.
I0302 19:02:09.040118 22760421793920 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:09.040229 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:02:09.056346 22760421793920 run.py:483] Algo bellman_ford step 9751 current loss 0.013427, current_train_items 312064.
I0302 19:02:09.081639 22760421793920 run.py:483] Algo bellman_ford step 9752 current loss 0.035160, current_train_items 312096.
I0302 19:02:09.111664 22760421793920 run.py:483] Algo bellman_ford step 9753 current loss 0.067437, current_train_items 312128.
I0302 19:02:09.143427 22760421793920 run.py:483] Algo bellman_ford step 9754 current loss 0.064873, current_train_items 312160.
I0302 19:02:09.162080 22760421793920 run.py:483] Algo bellman_ford step 9755 current loss 0.011450, current_train_items 312192.
I0302 19:02:09.178786 22760421793920 run.py:483] Algo bellman_ford step 9756 current loss 0.019617, current_train_items 312224.
I0302 19:02:09.200760 22760421793920 run.py:483] Algo bellman_ford step 9757 current loss 0.041287, current_train_items 312256.
I0302 19:02:09.232138 22760421793920 run.py:483] Algo bellman_ford step 9758 current loss 0.126359, current_train_items 312288.
I0302 19:02:09.264372 22760421793920 run.py:483] Algo bellman_ford step 9759 current loss 0.102292, current_train_items 312320.
I0302 19:02:09.282870 22760421793920 run.py:483] Algo bellman_ford step 9760 current loss 0.007320, current_train_items 312352.
I0302 19:02:09.298981 22760421793920 run.py:483] Algo bellman_ford step 9761 current loss 0.016404, current_train_items 312384.
I0302 19:02:09.323113 22760421793920 run.py:483] Algo bellman_ford step 9762 current loss 0.026690, current_train_items 312416.
I0302 19:02:09.354749 22760421793920 run.py:483] Algo bellman_ford step 9763 current loss 0.079199, current_train_items 312448.
I0302 19:02:09.384270 22760421793920 run.py:483] Algo bellman_ford step 9764 current loss 0.021956, current_train_items 312480.
I0302 19:02:09.402682 22760421793920 run.py:483] Algo bellman_ford step 9765 current loss 0.007000, current_train_items 312512.
I0302 19:02:09.418118 22760421793920 run.py:483] Algo bellman_ford step 9766 current loss 0.038122, current_train_items 312544.
I0302 19:02:09.442215 22760421793920 run.py:483] Algo bellman_ford step 9767 current loss 0.037572, current_train_items 312576.
I0302 19:02:09.473104 22760421793920 run.py:483] Algo bellman_ford step 9768 current loss 0.038685, current_train_items 312608.
I0302 19:02:09.505694 22760421793920 run.py:483] Algo bellman_ford step 9769 current loss 0.055532, current_train_items 312640.
I0302 19:02:09.524216 22760421793920 run.py:483] Algo bellman_ford step 9770 current loss 0.001663, current_train_items 312672.
I0302 19:02:09.539666 22760421793920 run.py:483] Algo bellman_ford step 9771 current loss 0.030623, current_train_items 312704.
I0302 19:02:09.562612 22760421793920 run.py:483] Algo bellman_ford step 9772 current loss 0.088669, current_train_items 312736.
I0302 19:02:09.594563 22760421793920 run.py:483] Algo bellman_ford step 9773 current loss 0.045644, current_train_items 312768.
I0302 19:02:09.626636 22760421793920 run.py:483] Algo bellman_ford step 9774 current loss 0.059973, current_train_items 312800.
I0302 19:02:09.645132 22760421793920 run.py:483] Algo bellman_ford step 9775 current loss 0.006266, current_train_items 312832.
I0302 19:02:09.660611 22760421793920 run.py:483] Algo bellman_ford step 9776 current loss 0.003404, current_train_items 312864.
I0302 19:02:09.685168 22760421793920 run.py:483] Algo bellman_ford step 9777 current loss 0.025716, current_train_items 312896.
I0302 19:02:09.716578 22760421793920 run.py:483] Algo bellman_ford step 9778 current loss 0.032350, current_train_items 312928.
I0302 19:02:09.748969 22760421793920 run.py:483] Algo bellman_ford step 9779 current loss 0.033398, current_train_items 312960.
I0302 19:02:09.767488 22760421793920 run.py:483] Algo bellman_ford step 9780 current loss 0.005794, current_train_items 312992.
I0302 19:02:09.783478 22760421793920 run.py:483] Algo bellman_ford step 9781 current loss 0.007308, current_train_items 313024.
I0302 19:02:09.806175 22760421793920 run.py:483] Algo bellman_ford step 9782 current loss 0.045385, current_train_items 313056.
I0302 19:02:09.838653 22760421793920 run.py:483] Algo bellman_ford step 9783 current loss 0.096991, current_train_items 313088.
I0302 19:02:09.869247 22760421793920 run.py:483] Algo bellman_ford step 9784 current loss 0.016021, current_train_items 313120.
I0302 19:02:09.887856 22760421793920 run.py:483] Algo bellman_ford step 9785 current loss 0.000745, current_train_items 313152.
I0302 19:02:09.903893 22760421793920 run.py:483] Algo bellman_ford step 9786 current loss 0.009386, current_train_items 313184.
I0302 19:02:09.927033 22760421793920 run.py:483] Algo bellman_ford step 9787 current loss 0.022496, current_train_items 313216.
I0302 19:02:09.957198 22760421793920 run.py:483] Algo bellman_ford step 9788 current loss 0.085573, current_train_items 313248.
I0302 19:02:09.989270 22760421793920 run.py:483] Algo bellman_ford step 9789 current loss 0.057884, current_train_items 313280.
I0302 19:02:10.007315 22760421793920 run.py:483] Algo bellman_ford step 9790 current loss 0.011607, current_train_items 313312.
I0302 19:02:10.022754 22760421793920 run.py:483] Algo bellman_ford step 9791 current loss 0.017583, current_train_items 313344.
I0302 19:02:10.045633 22760421793920 run.py:483] Algo bellman_ford step 9792 current loss 0.049915, current_train_items 313376.
I0302 19:02:10.076148 22760421793920 run.py:483] Algo bellman_ford step 9793 current loss 0.066842, current_train_items 313408.
I0302 19:02:10.106737 22760421793920 run.py:483] Algo bellman_ford step 9794 current loss 0.046241, current_train_items 313440.
I0302 19:02:10.125338 22760421793920 run.py:483] Algo bellman_ford step 9795 current loss 0.006037, current_train_items 313472.
I0302 19:02:10.141385 22760421793920 run.py:483] Algo bellman_ford step 9796 current loss 0.016579, current_train_items 313504.
I0302 19:02:10.164390 22760421793920 run.py:483] Algo bellman_ford step 9797 current loss 0.062609, current_train_items 313536.
I0302 19:02:10.194544 22760421793920 run.py:483] Algo bellman_ford step 9798 current loss 0.038217, current_train_items 313568.
I0302 19:02:10.225910 22760421793920 run.py:483] Algo bellman_ford step 9799 current loss 0.075798, current_train_items 313600.
I0302 19:02:10.244417 22760421793920 run.py:483] Algo bellman_ford step 9800 current loss 0.026986, current_train_items 313632.
I0302 19:02:10.251830 22760421793920 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:10.251954 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:02:10.268044 22760421793920 run.py:483] Algo bellman_ford step 9801 current loss 0.031448, current_train_items 313664.
I0302 19:02:10.291920 22760421793920 run.py:483] Algo bellman_ford step 9802 current loss 0.089490, current_train_items 313696.
I0302 19:02:10.323472 22760421793920 run.py:483] Algo bellman_ford step 9803 current loss 0.087736, current_train_items 313728.
I0302 19:02:10.354609 22760421793920 run.py:483] Algo bellman_ford step 9804 current loss 0.125049, current_train_items 313760.
I0302 19:02:10.373163 22760421793920 run.py:483] Algo bellman_ford step 9805 current loss 0.004154, current_train_items 313792.
I0302 19:02:10.389001 22760421793920 run.py:483] Algo bellman_ford step 9806 current loss 0.008115, current_train_items 313824.
I0302 19:02:10.411864 22760421793920 run.py:483] Algo bellman_ford step 9807 current loss 0.034016, current_train_items 313856.
I0302 19:02:10.442816 22760421793920 run.py:483] Algo bellman_ford step 9808 current loss 0.013636, current_train_items 313888.
I0302 19:02:10.476827 22760421793920 run.py:483] Algo bellman_ford step 9809 current loss 0.069675, current_train_items 313920.
I0302 19:02:10.495195 22760421793920 run.py:483] Algo bellman_ford step 9810 current loss 0.003501, current_train_items 313952.
I0302 19:02:10.510612 22760421793920 run.py:483] Algo bellman_ford step 9811 current loss 0.006722, current_train_items 313984.
I0302 19:02:10.534876 22760421793920 run.py:483] Algo bellman_ford step 9812 current loss 0.111999, current_train_items 314016.
I0302 19:02:10.563371 22760421793920 run.py:483] Algo bellman_ford step 9813 current loss 0.044533, current_train_items 314048.
I0302 19:02:10.595806 22760421793920 run.py:483] Algo bellman_ford step 9814 current loss 0.052419, current_train_items 314080.
I0302 19:02:10.614242 22760421793920 run.py:483] Algo bellman_ford step 9815 current loss 0.003018, current_train_items 314112.
I0302 19:02:10.630188 22760421793920 run.py:483] Algo bellman_ford step 9816 current loss 0.027524, current_train_items 314144.
I0302 19:02:10.652918 22760421793920 run.py:483] Algo bellman_ford step 9817 current loss 0.038290, current_train_items 314176.
I0302 19:02:10.684532 22760421793920 run.py:483] Algo bellman_ford step 9818 current loss 0.087711, current_train_items 314208.
I0302 19:02:10.716135 22760421793920 run.py:483] Algo bellman_ford step 9819 current loss 0.071504, current_train_items 314240.
I0302 19:02:10.734473 22760421793920 run.py:483] Algo bellman_ford step 9820 current loss 0.005346, current_train_items 314272.
I0302 19:02:10.750136 22760421793920 run.py:483] Algo bellman_ford step 9821 current loss 0.053387, current_train_items 314304.
I0302 19:02:10.773926 22760421793920 run.py:483] Algo bellman_ford step 9822 current loss 0.081662, current_train_items 314336.
I0302 19:02:10.803982 22760421793920 run.py:483] Algo bellman_ford step 9823 current loss 0.051096, current_train_items 314368.
I0302 19:02:10.835698 22760421793920 run.py:483] Algo bellman_ford step 9824 current loss 0.063283, current_train_items 314400.
I0302 19:02:10.854184 22760421793920 run.py:483] Algo bellman_ford step 9825 current loss 0.001202, current_train_items 314432.
I0302 19:02:10.869623 22760421793920 run.py:483] Algo bellman_ford step 9826 current loss 0.004987, current_train_items 314464.
I0302 19:02:10.892849 22760421793920 run.py:483] Algo bellman_ford step 9827 current loss 0.025515, current_train_items 314496.
I0302 19:02:10.923583 22760421793920 run.py:483] Algo bellman_ford step 9828 current loss 0.039447, current_train_items 314528.
I0302 19:02:10.957670 22760421793920 run.py:483] Algo bellman_ford step 9829 current loss 0.085117, current_train_items 314560.
I0302 19:02:10.975728 22760421793920 run.py:483] Algo bellman_ford step 9830 current loss 0.001853, current_train_items 314592.
I0302 19:02:10.991917 22760421793920 run.py:483] Algo bellman_ford step 9831 current loss 0.020490, current_train_items 314624.
I0302 19:02:11.015221 22760421793920 run.py:483] Algo bellman_ford step 9832 current loss 0.016110, current_train_items 314656.
I0302 19:02:11.046386 22760421793920 run.py:483] Algo bellman_ford step 9833 current loss 0.035753, current_train_items 314688.
I0302 19:02:11.076985 22760421793920 run.py:483] Algo bellman_ford step 9834 current loss 0.056535, current_train_items 314720.
I0302 19:02:11.095201 22760421793920 run.py:483] Algo bellman_ford step 9835 current loss 0.016579, current_train_items 314752.
I0302 19:02:11.110855 22760421793920 run.py:483] Algo bellman_ford step 9836 current loss 0.008323, current_train_items 314784.
I0302 19:02:11.133720 22760421793920 run.py:483] Algo bellman_ford step 9837 current loss 0.045600, current_train_items 314816.
I0302 19:02:11.165215 22760421793920 run.py:483] Algo bellman_ford step 9838 current loss 0.043968, current_train_items 314848.
I0302 19:02:11.198688 22760421793920 run.py:483] Algo bellman_ford step 9839 current loss 0.041448, current_train_items 314880.
I0302 19:02:11.217299 22760421793920 run.py:483] Algo bellman_ford step 9840 current loss 0.001919, current_train_items 314912.
I0302 19:02:11.233053 22760421793920 run.py:483] Algo bellman_ford step 9841 current loss 0.016743, current_train_items 314944.
I0302 19:02:11.256832 22760421793920 run.py:483] Algo bellman_ford step 9842 current loss 0.060752, current_train_items 314976.
I0302 19:02:11.287081 22760421793920 run.py:483] Algo bellman_ford step 9843 current loss 0.031673, current_train_items 315008.
I0302 19:02:11.317172 22760421793920 run.py:483] Algo bellman_ford step 9844 current loss 0.042951, current_train_items 315040.
I0302 19:02:11.335605 22760421793920 run.py:483] Algo bellman_ford step 9845 current loss 0.002082, current_train_items 315072.
I0302 19:02:11.351544 22760421793920 run.py:483] Algo bellman_ford step 9846 current loss 0.003473, current_train_items 315104.
I0302 19:02:11.375426 22760421793920 run.py:483] Algo bellman_ford step 9847 current loss 0.019225, current_train_items 315136.
I0302 19:02:11.405621 22760421793920 run.py:483] Algo bellman_ford step 9848 current loss 0.064846, current_train_items 315168.
I0302 19:02:11.437726 22760421793920 run.py:483] Algo bellman_ford step 9849 current loss 0.042292, current_train_items 315200.
I0302 19:02:11.456099 22760421793920 run.py:483] Algo bellman_ford step 9850 current loss 0.001831, current_train_items 315232.
I0302 19:02:11.463889 22760421793920 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:11.464006 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.996, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:02:11.481082 22760421793920 run.py:483] Algo bellman_ford step 9851 current loss 0.016297, current_train_items 315264.
I0302 19:02:11.506156 22760421793920 run.py:483] Algo bellman_ford step 9852 current loss 0.022736, current_train_items 315296.
I0302 19:02:11.536198 22760421793920 run.py:483] Algo bellman_ford step 9853 current loss 0.036268, current_train_items 315328.
I0302 19:02:11.569121 22760421793920 run.py:483] Algo bellman_ford step 9854 current loss 0.036528, current_train_items 315360.
I0302 19:02:11.587632 22760421793920 run.py:483] Algo bellman_ford step 9855 current loss 0.003309, current_train_items 315392.
I0302 19:02:11.603842 22760421793920 run.py:483] Algo bellman_ford step 9856 current loss 0.006337, current_train_items 315424.
I0302 19:02:11.627821 22760421793920 run.py:483] Algo bellman_ford step 9857 current loss 0.039315, current_train_items 315456.
I0302 19:02:11.657872 22760421793920 run.py:483] Algo bellman_ford step 9858 current loss 0.026955, current_train_items 315488.
I0302 19:02:11.690044 22760421793920 run.py:483] Algo bellman_ford step 9859 current loss 0.033904, current_train_items 315520.
I0302 19:02:11.708714 22760421793920 run.py:483] Algo bellman_ford step 9860 current loss 0.003103, current_train_items 315552.
I0302 19:02:11.724558 22760421793920 run.py:483] Algo bellman_ford step 9861 current loss 0.018235, current_train_items 315584.
I0302 19:02:11.748973 22760421793920 run.py:483] Algo bellman_ford step 9862 current loss 0.015487, current_train_items 315616.
I0302 19:02:11.779664 22760421793920 run.py:483] Algo bellman_ford step 9863 current loss 0.044856, current_train_items 315648.
I0302 19:02:11.811329 22760421793920 run.py:483] Algo bellman_ford step 9864 current loss 0.041667, current_train_items 315680.
I0302 19:02:11.829795 22760421793920 run.py:483] Algo bellman_ford step 9865 current loss 0.002277, current_train_items 315712.
I0302 19:02:11.845330 22760421793920 run.py:483] Algo bellman_ford step 9866 current loss 0.006401, current_train_items 315744.
I0302 19:02:11.868671 22760421793920 run.py:483] Algo bellman_ford step 9867 current loss 0.056089, current_train_items 315776.
I0302 19:02:11.900244 22760421793920 run.py:483] Algo bellman_ford step 9868 current loss 0.052160, current_train_items 315808.
I0302 19:02:11.932759 22760421793920 run.py:483] Algo bellman_ford step 9869 current loss 0.077672, current_train_items 315840.
I0302 19:02:11.951148 22760421793920 run.py:483] Algo bellman_ford step 9870 current loss 0.019251, current_train_items 315872.
I0302 19:02:11.967448 22760421793920 run.py:483] Algo bellman_ford step 9871 current loss 0.021884, current_train_items 315904.
I0302 19:02:11.991390 22760421793920 run.py:483] Algo bellman_ford step 9872 current loss 0.015866, current_train_items 315936.
I0302 19:02:12.022651 22760421793920 run.py:483] Algo bellman_ford step 9873 current loss 0.053073, current_train_items 315968.
I0302 19:02:12.053932 22760421793920 run.py:483] Algo bellman_ford step 9874 current loss 0.053528, current_train_items 316000.
I0302 19:02:12.072158 22760421793920 run.py:483] Algo bellman_ford step 9875 current loss 0.000423, current_train_items 316032.
I0302 19:02:12.087816 22760421793920 run.py:483] Algo bellman_ford step 9876 current loss 0.006926, current_train_items 316064.
I0302 19:02:12.109758 22760421793920 run.py:483] Algo bellman_ford step 9877 current loss 0.021376, current_train_items 316096.
I0302 19:02:12.139928 22760421793920 run.py:483] Algo bellman_ford step 9878 current loss 0.048775, current_train_items 316128.
I0302 19:02:12.171520 22760421793920 run.py:483] Algo bellman_ford step 9879 current loss 0.069177, current_train_items 316160.
I0302 19:02:12.190026 22760421793920 run.py:483] Algo bellman_ford step 9880 current loss 0.001367, current_train_items 316192.
I0302 19:02:12.205860 22760421793920 run.py:483] Algo bellman_ford step 9881 current loss 0.025760, current_train_items 316224.
I0302 19:02:12.229286 22760421793920 run.py:483] Algo bellman_ford step 9882 current loss 0.021009, current_train_items 316256.
I0302 19:02:12.260314 22760421793920 run.py:483] Algo bellman_ford step 9883 current loss 0.052998, current_train_items 316288.
I0302 19:02:12.292348 22760421793920 run.py:483] Algo bellman_ford step 9884 current loss 0.075879, current_train_items 316320.
I0302 19:02:12.310731 22760421793920 run.py:483] Algo bellman_ford step 9885 current loss 0.000836, current_train_items 316352.
I0302 19:02:12.326335 22760421793920 run.py:483] Algo bellman_ford step 9886 current loss 0.018329, current_train_items 316384.
I0302 19:02:12.349823 22760421793920 run.py:483] Algo bellman_ford step 9887 current loss 0.011654, current_train_items 316416.
I0302 19:02:12.380200 22760421793920 run.py:483] Algo bellman_ford step 9888 current loss 0.057895, current_train_items 316448.
I0302 19:02:12.411213 22760421793920 run.py:483] Algo bellman_ford step 9889 current loss 0.047283, current_train_items 316480.
I0302 19:02:12.429866 22760421793920 run.py:483] Algo bellman_ford step 9890 current loss 0.004718, current_train_items 316512.
I0302 19:02:12.445545 22760421793920 run.py:483] Algo bellman_ford step 9891 current loss 0.008583, current_train_items 316544.
I0302 19:02:12.468223 22760421793920 run.py:483] Algo bellman_ford step 9892 current loss 0.019890, current_train_items 316576.
I0302 19:02:12.499542 22760421793920 run.py:483] Algo bellman_ford step 9893 current loss 0.019414, current_train_items 316608.
I0302 19:02:12.532828 22760421793920 run.py:483] Algo bellman_ford step 9894 current loss 0.041102, current_train_items 316640.
I0302 19:02:12.551063 22760421793920 run.py:483] Algo bellman_ford step 9895 current loss 0.001207, current_train_items 316672.
I0302 19:02:12.566599 22760421793920 run.py:483] Algo bellman_ford step 9896 current loss 0.009298, current_train_items 316704.
I0302 19:02:12.590859 22760421793920 run.py:483] Algo bellman_ford step 9897 current loss 0.054599, current_train_items 316736.
I0302 19:02:12.621524 22760421793920 run.py:483] Algo bellman_ford step 9898 current loss 0.039571, current_train_items 316768.
I0302 19:02:12.655472 22760421793920 run.py:483] Algo bellman_ford step 9899 current loss 0.045589, current_train_items 316800.
I0302 19:02:12.673916 22760421793920 run.py:483] Algo bellman_ford step 9900 current loss 0.000813, current_train_items 316832.
I0302 19:02:12.681333 22760421793920 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9970703125, 'score': 0.9970703125, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:12.681442 22760421793920 run.py:519] Checkpointing best model, best avg val score was 0.996, current avg val score is 0.997, val scores are: bellman_ford: 0.997
I0302 19:02:12.709300 22760421793920 run.py:483] Algo bellman_ford step 9901 current loss 0.008497, current_train_items 316864.
I0302 19:02:12.732594 22760421793920 run.py:483] Algo bellman_ford step 9902 current loss 0.031371, current_train_items 316896.
I0302 19:02:12.763252 22760421793920 run.py:483] Algo bellman_ford step 9903 current loss 0.032806, current_train_items 316928.
I0302 19:02:12.796710 22760421793920 run.py:483] Algo bellman_ford step 9904 current loss 0.045711, current_train_items 316960.
I0302 19:02:12.815486 22760421793920 run.py:483] Algo bellman_ford step 9905 current loss 0.001095, current_train_items 316992.
I0302 19:02:12.831892 22760421793920 run.py:483] Algo bellman_ford step 9906 current loss 0.039786, current_train_items 317024.
I0302 19:02:12.856013 22760421793920 run.py:483] Algo bellman_ford step 9907 current loss 0.048719, current_train_items 317056.
I0302 19:02:12.886869 22760421793920 run.py:483] Algo bellman_ford step 9908 current loss 0.024906, current_train_items 317088.
I0302 19:02:12.917776 22760421793920 run.py:483] Algo bellman_ford step 9909 current loss 0.024889, current_train_items 317120.
I0302 19:02:12.936060 22760421793920 run.py:483] Algo bellman_ford step 9910 current loss 0.006492, current_train_items 317152.
I0302 19:02:12.952237 22760421793920 run.py:483] Algo bellman_ford step 9911 current loss 0.005145, current_train_items 317184.
I0302 19:02:12.975688 22760421793920 run.py:483] Algo bellman_ford step 9912 current loss 0.153835, current_train_items 317216.
I0302 19:02:13.007165 22760421793920 run.py:483] Algo bellman_ford step 9913 current loss 0.182509, current_train_items 317248.
I0302 19:02:13.039445 22760421793920 run.py:483] Algo bellman_ford step 9914 current loss 0.117093, current_train_items 317280.
I0302 19:02:13.058026 22760421793920 run.py:483] Algo bellman_ford step 9915 current loss 0.136287, current_train_items 317312.
I0302 19:02:13.074344 22760421793920 run.py:483] Algo bellman_ford step 9916 current loss 0.039872, current_train_items 317344.
I0302 19:02:13.098226 22760421793920 run.py:483] Algo bellman_ford step 9917 current loss 0.029526, current_train_items 317376.
I0302 19:02:13.129145 22760421793920 run.py:483] Algo bellman_ford step 9918 current loss 0.081813, current_train_items 317408.
I0302 19:02:13.160404 22760421793920 run.py:483] Algo bellman_ford step 9919 current loss 0.169686, current_train_items 317440.
I0302 19:02:13.179129 22760421793920 run.py:483] Algo bellman_ford step 9920 current loss 0.002270, current_train_items 317472.
I0302 19:02:13.195262 22760421793920 run.py:483] Algo bellman_ford step 9921 current loss 0.031079, current_train_items 317504.
I0302 19:02:13.218985 22760421793920 run.py:483] Algo bellman_ford step 9922 current loss 0.122440, current_train_items 317536.
I0302 19:02:13.248438 22760421793920 run.py:483] Algo bellman_ford step 9923 current loss 0.109248, current_train_items 317568.
I0302 19:02:13.282753 22760421793920 run.py:483] Algo bellman_ford step 9924 current loss 0.106416, current_train_items 317600.
I0302 19:02:13.301044 22760421793920 run.py:483] Algo bellman_ford step 9925 current loss 0.024798, current_train_items 317632.
I0302 19:02:13.316664 22760421793920 run.py:483] Algo bellman_ford step 9926 current loss 0.041272, current_train_items 317664.
I0302 19:02:13.341018 22760421793920 run.py:483] Algo bellman_ford step 9927 current loss 0.092842, current_train_items 317696.
I0302 19:02:13.372879 22760421793920 run.py:483] Algo bellman_ford step 9928 current loss 0.089781, current_train_items 317728.
I0302 19:02:13.403072 22760421793920 run.py:483] Algo bellman_ford step 9929 current loss 0.302926, current_train_items 317760.
I0302 19:02:13.421464 22760421793920 run.py:483] Algo bellman_ford step 9930 current loss 0.012977, current_train_items 317792.
I0302 19:02:13.436633 22760421793920 run.py:483] Algo bellman_ford step 9931 current loss 0.092514, current_train_items 317824.
I0302 19:02:13.460233 22760421793920 run.py:483] Algo bellman_ford step 9932 current loss 0.124045, current_train_items 317856.
I0302 19:02:13.490252 22760421793920 run.py:483] Algo bellman_ford step 9933 current loss 0.101502, current_train_items 317888.
I0302 19:02:13.521022 22760421793920 run.py:483] Algo bellman_ford step 9934 current loss 0.144821, current_train_items 317920.
I0302 19:02:13.539540 22760421793920 run.py:483] Algo bellman_ford step 9935 current loss 0.001759, current_train_items 317952.
I0302 19:02:13.554950 22760421793920 run.py:483] Algo bellman_ford step 9936 current loss 0.006471, current_train_items 317984.
I0302 19:02:13.578277 22760421793920 run.py:483] Algo bellman_ford step 9937 current loss 0.039575, current_train_items 318016.
I0302 19:02:13.608455 22760421793920 run.py:483] Algo bellman_ford step 9938 current loss 0.138209, current_train_items 318048.
I0302 19:02:13.641205 22760421793920 run.py:483] Algo bellman_ford step 9939 current loss 0.244682, current_train_items 318080.
I0302 19:02:13.659830 22760421793920 run.py:483] Algo bellman_ford step 9940 current loss 0.024325, current_train_items 318112.
I0302 19:02:13.675262 22760421793920 run.py:483] Algo bellman_ford step 9941 current loss 0.033244, current_train_items 318144.
I0302 19:02:13.698303 22760421793920 run.py:483] Algo bellman_ford step 9942 current loss 0.053995, current_train_items 318176.
I0302 19:02:13.729441 22760421793920 run.py:483] Algo bellman_ford step 9943 current loss 0.064481, current_train_items 318208.
I0302 19:02:13.760222 22760421793920 run.py:483] Algo bellman_ford step 9944 current loss 0.070230, current_train_items 318240.
I0302 19:02:13.778374 22760421793920 run.py:483] Algo bellman_ford step 9945 current loss 0.012380, current_train_items 318272.
I0302 19:02:13.793721 22760421793920 run.py:483] Algo bellman_ford step 9946 current loss 0.057503, current_train_items 318304.
I0302 19:02:13.817386 22760421793920 run.py:483] Algo bellman_ford step 9947 current loss 0.027518, current_train_items 318336.
I0302 19:02:13.848288 22760421793920 run.py:483] Algo bellman_ford step 9948 current loss 0.022417, current_train_items 318368.
I0302 19:02:13.881564 22760421793920 run.py:483] Algo bellman_ford step 9949 current loss 0.047432, current_train_items 318400.
I0302 19:02:13.899564 22760421793920 run.py:483] Algo bellman_ford step 9950 current loss 0.012069, current_train_items 318432.
I0302 19:02:13.907202 22760421793920 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:13.907309 22760421793920 run.py:522] Not saving new best model, best avg val score was 0.997, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0302 19:02:13.924007 22760421793920 run.py:483] Algo bellman_ford step 9951 current loss 0.018097, current_train_items 318464.
I0302 19:02:13.947664 22760421793920 run.py:483] Algo bellman_ford step 9952 current loss 0.021597, current_train_items 318496.
I0302 19:02:13.979249 22760421793920 run.py:483] Algo bellman_ford step 9953 current loss 0.036129, current_train_items 318528.
I0302 19:02:14.013252 22760421793920 run.py:483] Algo bellman_ford step 9954 current loss 0.053626, current_train_items 318560.
I0302 19:02:14.031755 22760421793920 run.py:483] Algo bellman_ford step 9955 current loss 0.002260, current_train_items 318592.
I0302 19:02:14.048305 22760421793920 run.py:483] Algo bellman_ford step 9956 current loss 0.013439, current_train_items 318624.
I0302 19:02:14.071885 22760421793920 run.py:483] Algo bellman_ford step 9957 current loss 0.015626, current_train_items 318656.
I0302 19:02:14.103214 22760421793920 run.py:483] Algo bellman_ford step 9958 current loss 0.044167, current_train_items 318688.
I0302 19:02:14.136726 22760421793920 run.py:483] Algo bellman_ford step 9959 current loss 0.048244, current_train_items 318720.
I0302 19:02:14.155128 22760421793920 run.py:483] Algo bellman_ford step 9960 current loss 0.004240, current_train_items 318752.
I0302 19:02:14.170280 22760421793920 run.py:483] Algo bellman_ford step 9961 current loss 0.016958, current_train_items 318784.
I0302 19:02:14.194870 22760421793920 run.py:483] Algo bellman_ford step 9962 current loss 0.025309, current_train_items 318816.
I0302 19:02:14.225585 22760421793920 run.py:483] Algo bellman_ford step 9963 current loss 0.043140, current_train_items 318848.
I0302 19:02:14.257488 22760421793920 run.py:483] Algo bellman_ford step 9964 current loss 0.061458, current_train_items 318880.
I0302 19:02:14.275928 22760421793920 run.py:483] Algo bellman_ford step 9965 current loss 0.002459, current_train_items 318912.
I0302 19:02:14.291953 22760421793920 run.py:483] Algo bellman_ford step 9966 current loss 0.016242, current_train_items 318944.
I0302 19:02:14.315569 22760421793920 run.py:483] Algo bellman_ford step 9967 current loss 0.042108, current_train_items 318976.
I0302 19:02:14.346217 22760421793920 run.py:483] Algo bellman_ford step 9968 current loss 0.037259, current_train_items 319008.
I0302 19:02:14.378433 22760421793920 run.py:483] Algo bellman_ford step 9969 current loss 0.034834, current_train_items 319040.
I0302 19:02:14.396712 22760421793920 run.py:483] Algo bellman_ford step 9970 current loss 0.001633, current_train_items 319072.
I0302 19:02:14.412343 22760421793920 run.py:483] Algo bellman_ford step 9971 current loss 0.013645, current_train_items 319104.
I0302 19:02:14.435500 22760421793920 run.py:483] Algo bellman_ford step 9972 current loss 0.043230, current_train_items 319136.
I0302 19:02:14.465498 22760421793920 run.py:483] Algo bellman_ford step 9973 current loss 0.065398, current_train_items 319168.
I0302 19:02:14.497388 22760421793920 run.py:483] Algo bellman_ford step 9974 current loss 0.057676, current_train_items 319200.
I0302 19:02:14.515572 22760421793920 run.py:483] Algo bellman_ford step 9975 current loss 0.001251, current_train_items 319232.
I0302 19:02:14.531546 22760421793920 run.py:483] Algo bellman_ford step 9976 current loss 0.032817, current_train_items 319264.
I0302 19:02:14.555089 22760421793920 run.py:483] Algo bellman_ford step 9977 current loss 0.056478, current_train_items 319296.
I0302 19:02:14.585676 22760421793920 run.py:483] Algo bellman_ford step 9978 current loss 0.029110, current_train_items 319328.
I0302 19:02:14.616605 22760421793920 run.py:483] Algo bellman_ford step 9979 current loss 0.018924, current_train_items 319360.
I0302 19:02:14.634929 22760421793920 run.py:483] Algo bellman_ford step 9980 current loss 0.000956, current_train_items 319392.
I0302 19:02:14.650615 22760421793920 run.py:483] Algo bellman_ford step 9981 current loss 0.009378, current_train_items 319424.
I0302 19:02:14.674738 22760421793920 run.py:483] Algo bellman_ford step 9982 current loss 0.041515, current_train_items 319456.
I0302 19:02:14.706755 22760421793920 run.py:483] Algo bellman_ford step 9983 current loss 0.051841, current_train_items 319488.
I0302 19:02:14.739632 22760421793920 run.py:483] Algo bellman_ford step 9984 current loss 0.081637, current_train_items 319520.
I0302 19:02:14.757970 22760421793920 run.py:483] Algo bellman_ford step 9985 current loss 0.001607, current_train_items 319552.
I0302 19:02:14.773280 22760421793920 run.py:483] Algo bellman_ford step 9986 current loss 0.004662, current_train_items 319584.
I0302 19:02:14.796995 22760421793920 run.py:483] Algo bellman_ford step 9987 current loss 0.020568, current_train_items 319616.
I0302 19:02:14.827656 22760421793920 run.py:483] Algo bellman_ford step 9988 current loss 0.044473, current_train_items 319648.
I0302 19:02:14.859582 22760421793920 run.py:483] Algo bellman_ford step 9989 current loss 0.072056, current_train_items 319680.
I0302 19:02:14.878106 22760421793920 run.py:483] Algo bellman_ford step 9990 current loss 0.052094, current_train_items 319712.
I0302 19:02:14.893908 22760421793920 run.py:483] Algo bellman_ford step 9991 current loss 0.011545, current_train_items 319744.
I0302 19:02:14.917336 22760421793920 run.py:483] Algo bellman_ford step 9992 current loss 0.043927, current_train_items 319776.
I0302 19:02:14.946932 22760421793920 run.py:483] Algo bellman_ford step 9993 current loss 0.019859, current_train_items 319808.
I0302 19:02:14.978139 22760421793920 run.py:483] Algo bellman_ford step 9994 current loss 0.035727, current_train_items 319840.
I0302 19:02:14.996840 22760421793920 run.py:483] Algo bellman_ford step 9995 current loss 0.002074, current_train_items 319872.
I0302 19:02:15.012752 22760421793920 run.py:483] Algo bellman_ford step 9996 current loss 0.044185, current_train_items 319904.
I0302 19:02:15.036454 22760421793920 run.py:483] Algo bellman_ford step 9997 current loss 0.020558, current_train_items 319936.
I0302 19:02:15.066823 22760421793920 run.py:483] Algo bellman_ford step 9998 current loss 0.082229, current_train_items 319968.
I0302 19:02:15.096724 22760421793920 run.py:483] Algo bellman_ford step 9999 current loss 0.055363, current_train_items 320000.
I0302 19:02:15.101492 22760421793920 run.py:527] Restoring best model from checkpoint...
I0302 19:02:17.365032 22760421793920 run.py:542] (test) algo bellman_ford : {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:02:17.365292 22760421793920 run.py:544] Done!

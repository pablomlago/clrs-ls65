Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:31.952531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:31.952829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:31.979728: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:36.425375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:47.198310 22535416901760 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:47.208403 22535416901760 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:47.598424 22535416901760 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:47.598872 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.599180 22535416901760 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:47.807764 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:47.808005 22535416901760 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.055627 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.055888 22535416901760 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.398269 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.398511 22535416901760 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:48.818539 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:48.818790 22535416901760 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:49.352811 22535416901760 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:49.353075 22535416901760 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:49.391196 22535416901760 run.py:166] Dataset not found in ./datasets_1/23/CLRS30_v1.0.0. Downloading...
I0302 18:57:05.303009 22535416901760 dataset_info.py:482] Load dataset info from ./datasets_1/23/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.305728 22535416901760 dataset_info.py:482] Load dataset info from ./datasets_1/23/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:05.306528 22535416901760 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/23/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:57:05.306610 22535416901760 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/23/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:57:21.572936 22535416901760 run.py:483] Algo bellman_ford step 0 current loss 3.645898, current_train_items 32.
I0302 18:57:24.694787 22535416901760 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.36328125, 'score': 0.36328125, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:24.695049 22535416901760 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.363, val scores are: bellman_ford: 0.363
I0302 18:57:34.767292 22535416901760 run.py:483] Algo bellman_ford step 1 current loss 298.057007, current_train_items 64.
I0302 18:57:45.862264 22535416901760 run.py:483] Algo bellman_ford step 2 current loss 633704704.000000, current_train_items 96.
I0302 18:57:56.878071 22535416901760 run.py:483] Algo bellman_ford step 3 current loss 24143077376.000000, current_train_items 128.
I0302 18:58:06.838654 22535416901760 run.py:483] Algo bellman_ford step 4 current loss 787404800.000000, current_train_items 160.
I0302 18:58:06.857276 22535416901760 run.py:483] Algo bellman_ford step 5 current loss 3.579791, current_train_items 192.
I0302 18:58:06.874641 22535416901760 run.py:483] Algo bellman_ford step 6 current loss 153.006546, current_train_items 224.
I0302 18:58:06.897056 22535416901760 run.py:483] Algo bellman_ford step 7 current loss 1070749.875000, current_train_items 256.
I0302 18:58:06.925906 22535416901760 run.py:483] Algo bellman_ford step 8 current loss 60864836.000000, current_train_items 288.
I0302 18:58:06.957581 22535416901760 run.py:483] Algo bellman_ford step 9 current loss 16591141.000000, current_train_items 320.
I0302 18:58:06.975246 22535416901760 run.py:483] Algo bellman_ford step 10 current loss 4.348547, current_train_items 352.
I0302 18:58:06.991820 22535416901760 run.py:483] Algo bellman_ford step 11 current loss 9.651734, current_train_items 384.
I0302 18:58:07.013581 22535416901760 run.py:483] Algo bellman_ford step 12 current loss 134555.343750, current_train_items 416.
I0302 18:58:07.044320 22535416901760 run.py:483] Algo bellman_ford step 13 current loss 5411654.000000, current_train_items 448.
I0302 18:58:07.072236 22535416901760 run.py:483] Algo bellman_ford step 14 current loss 2174976.500000, current_train_items 480.
I0302 18:58:07.090176 22535416901760 run.py:483] Algo bellman_ford step 15 current loss 1.646152, current_train_items 512.
I0302 18:58:07.106194 22535416901760 run.py:483] Algo bellman_ford step 16 current loss 4.818895, current_train_items 544.
I0302 18:58:07.130335 22535416901760 run.py:483] Algo bellman_ford step 17 current loss 6057.531250, current_train_items 576.
I0302 18:58:07.158949 22535416901760 run.py:483] Algo bellman_ford step 18 current loss 162606.437500, current_train_items 608.
I0302 18:58:07.191182 22535416901760 run.py:483] Algo bellman_ford step 19 current loss 92590.539062, current_train_items 640.
I0302 18:58:07.208987 22535416901760 run.py:483] Algo bellman_ford step 20 current loss 1.807946, current_train_items 672.
I0302 18:58:07.224591 22535416901760 run.py:483] Algo bellman_ford step 21 current loss 2.884176, current_train_items 704.
I0302 18:58:07.247904 22535416901760 run.py:483] Algo bellman_ford step 22 current loss 122.098137, current_train_items 736.
I0302 18:58:07.276901 22535416901760 run.py:483] Algo bellman_ford step 23 current loss 2912.955078, current_train_items 768.
I0302 18:58:07.308005 22535416901760 run.py:483] Algo bellman_ford step 24 current loss 5468.627930, current_train_items 800.
I0302 18:58:07.325651 22535416901760 run.py:483] Algo bellman_ford step 25 current loss 1.378874, current_train_items 832.
I0302 18:58:07.341705 22535416901760 run.py:483] Algo bellman_ford step 26 current loss 2.091409, current_train_items 864.
I0302 18:58:07.364887 22535416901760 run.py:483] Algo bellman_ford step 27 current loss 10.919992, current_train_items 896.
I0302 18:58:07.394330 22535416901760 run.py:483] Algo bellman_ford step 28 current loss 307.489746, current_train_items 928.
I0302 18:58:07.426577 22535416901760 run.py:483] Algo bellman_ford step 29 current loss 419.967163, current_train_items 960.
I0302 18:58:07.444109 22535416901760 run.py:483] Algo bellman_ford step 30 current loss 1.126187, current_train_items 992.
I0302 18:58:07.459415 22535416901760 run.py:483] Algo bellman_ford step 31 current loss 1.673290, current_train_items 1024.
I0302 18:58:07.481912 22535416901760 run.py:483] Algo bellman_ford step 32 current loss 6.285854, current_train_items 1056.
I0302 18:58:07.512049 22535416901760 run.py:483] Algo bellman_ford step 33 current loss 54.494789, current_train_items 1088.
I0302 18:58:07.542994 22535416901760 run.py:483] Algo bellman_ford step 34 current loss 77.072845, current_train_items 1120.
I0302 18:58:07.560567 22535416901760 run.py:483] Algo bellman_ford step 35 current loss 1.107823, current_train_items 1152.
I0302 18:58:07.576075 22535416901760 run.py:483] Algo bellman_ford step 36 current loss 1.565058, current_train_items 1184.
I0302 18:58:07.599235 22535416901760 run.py:483] Algo bellman_ford step 37 current loss 2.517659, current_train_items 1216.
I0302 18:58:07.628539 22535416901760 run.py:483] Algo bellman_ford step 38 current loss 3.532676, current_train_items 1248.
W0302 18:58:07.651641 22535416901760 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:58:14.234930 22535416901760 run.py:483] Algo bellman_ford step 39 current loss 1759.591553, current_train_items 1280.
I0302 18:58:14.254713 22535416901760 run.py:483] Algo bellman_ford step 40 current loss 1.094324, current_train_items 1312.
I0302 18:58:14.271243 22535416901760 run.py:483] Algo bellman_ford step 41 current loss 1.611571, current_train_items 1344.
I0302 18:58:14.293770 22535416901760 run.py:483] Algo bellman_ford step 42 current loss 2.468158, current_train_items 1376.
I0302 18:58:14.324335 22535416901760 run.py:483] Algo bellman_ford step 43 current loss 5.017614, current_train_items 1408.
I0302 18:58:14.356600 22535416901760 run.py:483] Algo bellman_ford step 44 current loss 178.713867, current_train_items 1440.
I0302 18:58:14.375728 22535416901760 run.py:483] Algo bellman_ford step 45 current loss 1.087755, current_train_items 1472.
I0302 18:58:14.392168 22535416901760 run.py:483] Algo bellman_ford step 46 current loss 1.640958, current_train_items 1504.
I0302 18:58:14.414220 22535416901760 run.py:483] Algo bellman_ford step 47 current loss 1.907344, current_train_items 1536.
I0302 18:58:14.441331 22535416901760 run.py:483] Algo bellman_ford step 48 current loss 1.903019, current_train_items 1568.
I0302 18:58:14.470587 22535416901760 run.py:483] Algo bellman_ford step 49 current loss 2.962393, current_train_items 1600.
I0302 18:58:14.489182 22535416901760 run.py:483] Algo bellman_ford step 50 current loss 0.850840, current_train_items 1632.
I0302 18:58:14.498123 22535416901760 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.5830078125, 'score': 0.5830078125, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:58:14.498247 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.363, current avg val score is 0.583, val scores are: bellman_ford: 0.583
I0302 18:58:14.527923 22535416901760 run.py:483] Algo bellman_ford step 51 current loss 1.457567, current_train_items 1664.
I0302 18:58:14.550879 22535416901760 run.py:483] Algo bellman_ford step 52 current loss 2.109385, current_train_items 1696.
I0302 18:58:14.580073 22535416901760 run.py:483] Algo bellman_ford step 53 current loss 2.166834, current_train_items 1728.
I0302 18:58:14.611954 22535416901760 run.py:483] Algo bellman_ford step 54 current loss 2.784627, current_train_items 1760.
I0302 18:58:14.630826 22535416901760 run.py:483] Algo bellman_ford step 55 current loss 0.911735, current_train_items 1792.
I0302 18:58:14.647313 22535416901760 run.py:483] Algo bellman_ford step 56 current loss 1.302691, current_train_items 1824.
I0302 18:58:14.669681 22535416901760 run.py:483] Algo bellman_ford step 57 current loss 1.907946, current_train_items 1856.
I0302 18:58:14.697418 22535416901760 run.py:483] Algo bellman_ford step 58 current loss 1.799793, current_train_items 1888.
I0302 18:58:14.729891 22535416901760 run.py:483] Algo bellman_ford step 59 current loss 2.244535, current_train_items 1920.
I0302 18:58:14.748732 22535416901760 run.py:483] Algo bellman_ford step 60 current loss 0.834414, current_train_items 1952.
W0302 18:58:14.758298 22535416901760 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:58:21.206688 22535416901760 run.py:483] Algo bellman_ford step 61 current loss 1.160037, current_train_items 1984.
I0302 18:58:21.231013 22535416901760 run.py:483] Algo bellman_ford step 62 current loss 1.657184, current_train_items 2016.
I0302 18:58:21.260945 22535416901760 run.py:483] Algo bellman_ford step 63 current loss 1.986055, current_train_items 2048.
I0302 18:58:21.294943 22535416901760 run.py:483] Algo bellman_ford step 64 current loss 2.508580, current_train_items 2080.
I0302 18:58:21.314653 22535416901760 run.py:483] Algo bellman_ford step 65 current loss 0.797989, current_train_items 2112.
I0302 18:58:21.330803 22535416901760 run.py:483] Algo bellman_ford step 66 current loss 1.100663, current_train_items 2144.
I0302 18:58:21.355350 22535416901760 run.py:483] Algo bellman_ford step 67 current loss 1.815295, current_train_items 2176.
I0302 18:58:21.383670 22535416901760 run.py:483] Algo bellman_ford step 68 current loss 1.719423, current_train_items 2208.
I0302 18:58:21.415729 22535416901760 run.py:483] Algo bellman_ford step 69 current loss 2.133335, current_train_items 2240.
I0302 18:58:21.434591 22535416901760 run.py:483] Algo bellman_ford step 70 current loss 0.705571, current_train_items 2272.
I0302 18:58:21.450930 22535416901760 run.py:483] Algo bellman_ford step 71 current loss 1.155603, current_train_items 2304.
I0302 18:58:21.473797 22535416901760 run.py:483] Algo bellman_ford step 72 current loss 1.548642, current_train_items 2336.
I0302 18:58:21.503232 22535416901760 run.py:483] Algo bellman_ford step 73 current loss 1.775358, current_train_items 2368.
I0302 18:58:21.536015 22535416901760 run.py:483] Algo bellman_ford step 74 current loss 1.925744, current_train_items 2400.
I0302 18:58:21.554416 22535416901760 run.py:483] Algo bellman_ford step 75 current loss 0.532595, current_train_items 2432.
I0302 18:58:21.571121 22535416901760 run.py:483] Algo bellman_ford step 76 current loss 1.247848, current_train_items 2464.
I0302 18:58:21.593626 22535416901760 run.py:483] Algo bellman_ford step 77 current loss 1.773316, current_train_items 2496.
I0302 18:58:21.622176 22535416901760 run.py:483] Algo bellman_ford step 78 current loss 1.726289, current_train_items 2528.
I0302 18:58:21.651515 22535416901760 run.py:483] Algo bellman_ford step 79 current loss 2.033378, current_train_items 2560.
I0302 18:58:21.670573 22535416901760 run.py:483] Algo bellman_ford step 80 current loss 0.649678, current_train_items 2592.
I0302 18:58:21.686597 22535416901760 run.py:483] Algo bellman_ford step 81 current loss 1.032789, current_train_items 2624.
I0302 18:58:21.709844 22535416901760 run.py:483] Algo bellman_ford step 82 current loss 1.568178, current_train_items 2656.
I0302 18:58:21.738609 22535416901760 run.py:483] Algo bellman_ford step 83 current loss 1.495458, current_train_items 2688.
I0302 18:58:21.768609 22535416901760 run.py:483] Algo bellman_ford step 84 current loss 1.668913, current_train_items 2720.
I0302 18:58:21.787002 22535416901760 run.py:483] Algo bellman_ford step 85 current loss 0.685435, current_train_items 2752.
I0302 18:58:21.803461 22535416901760 run.py:483] Algo bellman_ford step 86 current loss 1.047633, current_train_items 2784.
I0302 18:58:21.827641 22535416901760 run.py:483] Algo bellman_ford step 87 current loss 1.640195, current_train_items 2816.
I0302 18:58:21.857063 22535416901760 run.py:483] Algo bellman_ford step 88 current loss 1.706515, current_train_items 2848.
I0302 18:58:21.888972 22535416901760 run.py:483] Algo bellman_ford step 89 current loss 1.934081, current_train_items 2880.
I0302 18:58:21.907536 22535416901760 run.py:483] Algo bellman_ford step 90 current loss 0.650131, current_train_items 2912.
I0302 18:58:21.923868 22535416901760 run.py:483] Algo bellman_ford step 91 current loss 1.075856, current_train_items 2944.
I0302 18:58:21.946773 22535416901760 run.py:483] Algo bellman_ford step 92 current loss 1.384613, current_train_items 2976.
I0302 18:58:21.977151 22535416901760 run.py:483] Algo bellman_ford step 93 current loss 1.707592, current_train_items 3008.
I0302 18:58:22.008657 22535416901760 run.py:483] Algo bellman_ford step 94 current loss 1.870645, current_train_items 3040.
I0302 18:58:22.027614 22535416901760 run.py:483] Algo bellman_ford step 95 current loss 0.576585, current_train_items 3072.
I0302 18:58:22.043411 22535416901760 run.py:483] Algo bellman_ford step 96 current loss 0.932734, current_train_items 3104.
I0302 18:58:22.066314 22535416901760 run.py:483] Algo bellman_ford step 97 current loss 1.319168, current_train_items 3136.
I0302 18:58:22.095508 22535416901760 run.py:483] Algo bellman_ford step 98 current loss 1.431120, current_train_items 3168.
I0302 18:58:22.127383 22535416901760 run.py:483] Algo bellman_ford step 99 current loss 1.907371, current_train_items 3200.
I0302 18:58:22.145778 22535416901760 run.py:483] Algo bellman_ford step 100 current loss 0.542113, current_train_items 3232.
I0302 18:58:22.155410 22535416901760 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.7392578125, 'score': 0.7392578125, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:58:22.155525 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.583, current avg val score is 0.739, val scores are: bellman_ford: 0.739
I0302 18:58:22.185765 22535416901760 run.py:483] Algo bellman_ford step 101 current loss 1.016334, current_train_items 3264.
I0302 18:58:22.208850 22535416901760 run.py:483] Algo bellman_ford step 102 current loss 1.211508, current_train_items 3296.
I0302 18:58:22.237839 22535416901760 run.py:483] Algo bellman_ford step 103 current loss 1.447460, current_train_items 3328.
I0302 18:58:22.272341 22535416901760 run.py:483] Algo bellman_ford step 104 current loss 1.982406, current_train_items 3360.
I0302 18:58:22.291394 22535416901760 run.py:483] Algo bellman_ford step 105 current loss 0.507510, current_train_items 3392.
I0302 18:58:22.307580 22535416901760 run.py:483] Algo bellman_ford step 106 current loss 0.861864, current_train_items 3424.
I0302 18:58:22.330931 22535416901760 run.py:483] Algo bellman_ford step 107 current loss 1.175932, current_train_items 3456.
I0302 18:58:22.359372 22535416901760 run.py:483] Algo bellman_ford step 108 current loss 1.437265, current_train_items 3488.
I0302 18:58:22.389748 22535416901760 run.py:483] Algo bellman_ford step 109 current loss 1.561343, current_train_items 3520.
I0302 18:58:22.408494 22535416901760 run.py:483] Algo bellman_ford step 110 current loss 0.532200, current_train_items 3552.
I0302 18:58:22.424483 22535416901760 run.py:483] Algo bellman_ford step 111 current loss 0.855499, current_train_items 3584.
I0302 18:58:22.446687 22535416901760 run.py:483] Algo bellman_ford step 112 current loss 1.095969, current_train_items 3616.
I0302 18:58:22.475846 22535416901760 run.py:483] Algo bellman_ford step 113 current loss 1.630170, current_train_items 3648.
I0302 18:58:22.506623 22535416901760 run.py:483] Algo bellman_ford step 114 current loss 1.665398, current_train_items 3680.
I0302 18:58:22.524748 22535416901760 run.py:483] Algo bellman_ford step 115 current loss 0.492215, current_train_items 3712.
I0302 18:58:22.541183 22535416901760 run.py:483] Algo bellman_ford step 116 current loss 1.039565, current_train_items 3744.
I0302 18:58:22.565140 22535416901760 run.py:483] Algo bellman_ford step 117 current loss 1.561425, current_train_items 3776.
I0302 18:58:22.593696 22535416901760 run.py:483] Algo bellman_ford step 118 current loss 1.453346, current_train_items 3808.
I0302 18:58:22.622642 22535416901760 run.py:483] Algo bellman_ford step 119 current loss 1.820656, current_train_items 3840.
I0302 18:58:22.641196 22535416901760 run.py:483] Algo bellman_ford step 120 current loss 0.493173, current_train_items 3872.
I0302 18:58:22.657676 22535416901760 run.py:483] Algo bellman_ford step 121 current loss 0.847062, current_train_items 3904.
I0302 18:58:22.681074 22535416901760 run.py:483] Algo bellman_ford step 122 current loss 1.233116, current_train_items 3936.
I0302 18:58:22.710772 22535416901760 run.py:483] Algo bellman_ford step 123 current loss 1.343630, current_train_items 3968.
I0302 18:58:22.745519 22535416901760 run.py:483] Algo bellman_ford step 124 current loss 1.865981, current_train_items 4000.
I0302 18:58:22.764219 22535416901760 run.py:483] Algo bellman_ford step 125 current loss 0.591728, current_train_items 4032.
I0302 18:58:22.780764 22535416901760 run.py:483] Algo bellman_ford step 126 current loss 1.022179, current_train_items 4064.
I0302 18:58:22.803853 22535416901760 run.py:483] Algo bellman_ford step 127 current loss 1.331787, current_train_items 4096.
I0302 18:58:22.832626 22535416901760 run.py:483] Algo bellman_ford step 128 current loss 1.383274, current_train_items 4128.
I0302 18:58:22.864581 22535416901760 run.py:483] Algo bellman_ford step 129 current loss 1.448082, current_train_items 4160.
I0302 18:58:22.883326 22535416901760 run.py:483] Algo bellman_ford step 130 current loss 0.479681, current_train_items 4192.
I0302 18:58:22.899312 22535416901760 run.py:483] Algo bellman_ford step 131 current loss 0.709590, current_train_items 4224.
I0302 18:58:22.922623 22535416901760 run.py:483] Algo bellman_ford step 132 current loss 1.402029, current_train_items 4256.
I0302 18:58:22.951783 22535416901760 run.py:483] Algo bellman_ford step 133 current loss 1.437225, current_train_items 4288.
I0302 18:58:22.983418 22535416901760 run.py:483] Algo bellman_ford step 134 current loss 1.600948, current_train_items 4320.
I0302 18:58:23.001445 22535416901760 run.py:483] Algo bellman_ford step 135 current loss 0.510227, current_train_items 4352.
I0302 18:58:23.017435 22535416901760 run.py:483] Algo bellman_ford step 136 current loss 0.821685, current_train_items 4384.
I0302 18:58:23.040656 22535416901760 run.py:483] Algo bellman_ford step 137 current loss 1.327829, current_train_items 4416.
I0302 18:58:23.069313 22535416901760 run.py:483] Algo bellman_ford step 138 current loss 1.400495, current_train_items 4448.
I0302 18:58:23.101551 22535416901760 run.py:483] Algo bellman_ford step 139 current loss 1.837013, current_train_items 4480.
I0302 18:58:23.119931 22535416901760 run.py:483] Algo bellman_ford step 140 current loss 0.424993, current_train_items 4512.
I0302 18:58:23.136371 22535416901760 run.py:483] Algo bellman_ford step 141 current loss 1.030249, current_train_items 4544.
I0302 18:58:23.159233 22535416901760 run.py:483] Algo bellman_ford step 142 current loss 1.409443, current_train_items 4576.
I0302 18:58:23.188934 22535416901760 run.py:483] Algo bellman_ford step 143 current loss 1.592768, current_train_items 4608.
I0302 18:58:23.219868 22535416901760 run.py:483] Algo bellman_ford step 144 current loss 1.707974, current_train_items 4640.
I0302 18:58:23.238318 22535416901760 run.py:483] Algo bellman_ford step 145 current loss 0.574439, current_train_items 4672.
I0302 18:58:23.254362 22535416901760 run.py:483] Algo bellman_ford step 146 current loss 0.968090, current_train_items 4704.
I0302 18:58:23.276971 22535416901760 run.py:483] Algo bellman_ford step 147 current loss 1.374398, current_train_items 4736.
I0302 18:58:23.305587 22535416901760 run.py:483] Algo bellman_ford step 148 current loss 1.442040, current_train_items 4768.
I0302 18:58:23.335263 22535416901760 run.py:483] Algo bellman_ford step 149 current loss 1.606252, current_train_items 4800.
I0302 18:58:23.353718 22535416901760 run.py:483] Algo bellman_ford step 150 current loss 0.506790, current_train_items 4832.
I0302 18:58:23.361942 22535416901760 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8017578125, 'score': 0.8017578125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:58:23.362054 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.739, current avg val score is 0.802, val scores are: bellman_ford: 0.802
I0302 18:58:23.390766 22535416901760 run.py:483] Algo bellman_ford step 151 current loss 0.722364, current_train_items 4864.
I0302 18:58:23.413748 22535416901760 run.py:483] Algo bellman_ford step 152 current loss 1.220438, current_train_items 4896.
I0302 18:58:23.442824 22535416901760 run.py:483] Algo bellman_ford step 153 current loss 1.457769, current_train_items 4928.
I0302 18:58:23.477902 22535416901760 run.py:483] Algo bellman_ford step 154 current loss 1.814730, current_train_items 4960.
I0302 18:58:23.496890 22535416901760 run.py:483] Algo bellman_ford step 155 current loss 0.493204, current_train_items 4992.
I0302 18:58:23.513091 22535416901760 run.py:483] Algo bellman_ford step 156 current loss 0.886770, current_train_items 5024.
I0302 18:58:23.535827 22535416901760 run.py:483] Algo bellman_ford step 157 current loss 1.250202, current_train_items 5056.
I0302 18:58:23.563421 22535416901760 run.py:483] Algo bellman_ford step 158 current loss 1.330788, current_train_items 5088.
I0302 18:58:23.595259 22535416901760 run.py:483] Algo bellman_ford step 159 current loss 1.668942, current_train_items 5120.
I0302 18:58:23.613895 22535416901760 run.py:483] Algo bellman_ford step 160 current loss 0.518982, current_train_items 5152.
I0302 18:58:23.630179 22535416901760 run.py:483] Algo bellman_ford step 161 current loss 0.786022, current_train_items 5184.
I0302 18:58:23.652362 22535416901760 run.py:483] Algo bellman_ford step 162 current loss 1.244062, current_train_items 5216.
I0302 18:58:23.681529 22535416901760 run.py:483] Algo bellman_ford step 163 current loss 1.365894, current_train_items 5248.
I0302 18:58:23.712673 22535416901760 run.py:483] Algo bellman_ford step 164 current loss 1.605174, current_train_items 5280.
I0302 18:58:23.731238 22535416901760 run.py:483] Algo bellman_ford step 165 current loss 0.575548, current_train_items 5312.
I0302 18:58:23.747645 22535416901760 run.py:483] Algo bellman_ford step 166 current loss 1.207113, current_train_items 5344.
I0302 18:58:23.769888 22535416901760 run.py:483] Algo bellman_ford step 167 current loss 1.296534, current_train_items 5376.
I0302 18:58:23.799148 22535416901760 run.py:483] Algo bellman_ford step 168 current loss 1.394422, current_train_items 5408.
I0302 18:58:23.830446 22535416901760 run.py:483] Algo bellman_ford step 169 current loss 1.519483, current_train_items 5440.
I0302 18:58:23.849020 22535416901760 run.py:483] Algo bellman_ford step 170 current loss 0.559249, current_train_items 5472.
I0302 18:58:23.865197 22535416901760 run.py:483] Algo bellman_ford step 171 current loss 0.942912, current_train_items 5504.
I0302 18:58:23.886991 22535416901760 run.py:483] Algo bellman_ford step 172 current loss 1.506830, current_train_items 5536.
I0302 18:58:23.916322 22535416901760 run.py:483] Algo bellman_ford step 173 current loss 1.620150, current_train_items 5568.
I0302 18:58:23.951031 22535416901760 run.py:483] Algo bellman_ford step 174 current loss 1.954852, current_train_items 5600.
I0302 18:58:23.969461 22535416901760 run.py:483] Algo bellman_ford step 175 current loss 0.606463, current_train_items 5632.
I0302 18:58:23.985508 22535416901760 run.py:483] Algo bellman_ford step 176 current loss 0.801670, current_train_items 5664.
I0302 18:58:24.008677 22535416901760 run.py:483] Algo bellman_ford step 177 current loss 1.245828, current_train_items 5696.
I0302 18:58:24.035840 22535416901760 run.py:483] Algo bellman_ford step 178 current loss 1.285317, current_train_items 5728.
I0302 18:58:24.065811 22535416901760 run.py:483] Algo bellman_ford step 179 current loss 1.680857, current_train_items 5760.
I0302 18:58:24.084711 22535416901760 run.py:483] Algo bellman_ford step 180 current loss 0.588603, current_train_items 5792.
I0302 18:58:24.100930 22535416901760 run.py:483] Algo bellman_ford step 181 current loss 0.829348, current_train_items 5824.
I0302 18:58:24.124689 22535416901760 run.py:483] Algo bellman_ford step 182 current loss 1.317283, current_train_items 5856.
I0302 18:58:24.153376 22535416901760 run.py:483] Algo bellman_ford step 183 current loss 1.233834, current_train_items 5888.
I0302 18:58:24.183384 22535416901760 run.py:483] Algo bellman_ford step 184 current loss 1.801185, current_train_items 5920.
I0302 18:58:24.202200 22535416901760 run.py:483] Algo bellman_ford step 185 current loss 0.394995, current_train_items 5952.
I0302 18:58:24.218677 22535416901760 run.py:483] Algo bellman_ford step 186 current loss 0.909986, current_train_items 5984.
I0302 18:58:24.240653 22535416901760 run.py:483] Algo bellman_ford step 187 current loss 1.077319, current_train_items 6016.
I0302 18:58:24.269114 22535416901760 run.py:483] Algo bellman_ford step 188 current loss 1.475481, current_train_items 6048.
I0302 18:58:24.302202 22535416901760 run.py:483] Algo bellman_ford step 189 current loss 1.763414, current_train_items 6080.
I0302 18:58:24.320537 22535416901760 run.py:483] Algo bellman_ford step 190 current loss 0.435606, current_train_items 6112.
I0302 18:58:24.336885 22535416901760 run.py:483] Algo bellman_ford step 191 current loss 0.753101, current_train_items 6144.
I0302 18:58:24.360401 22535416901760 run.py:483] Algo bellman_ford step 192 current loss 1.386440, current_train_items 6176.
I0302 18:58:24.389061 22535416901760 run.py:483] Algo bellman_ford step 193 current loss 1.492783, current_train_items 6208.
I0302 18:58:24.420392 22535416901760 run.py:483] Algo bellman_ford step 194 current loss 1.625343, current_train_items 6240.
I0302 18:58:24.439309 22535416901760 run.py:483] Algo bellman_ford step 195 current loss 0.432132, current_train_items 6272.
I0302 18:58:24.455521 22535416901760 run.py:483] Algo bellman_ford step 196 current loss 0.786933, current_train_items 6304.
I0302 18:58:24.477998 22535416901760 run.py:483] Algo bellman_ford step 197 current loss 1.157734, current_train_items 6336.
I0302 18:58:24.507597 22535416901760 run.py:483] Algo bellman_ford step 198 current loss 1.431341, current_train_items 6368.
I0302 18:58:24.539541 22535416901760 run.py:483] Algo bellman_ford step 199 current loss 1.497007, current_train_items 6400.
I0302 18:58:24.558243 22535416901760 run.py:483] Algo bellman_ford step 200 current loss 0.484866, current_train_items 6432.
I0302 18:58:24.566075 22535416901760 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.822265625, 'score': 0.822265625, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:58:24.566193 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.802, current avg val score is 0.822, val scores are: bellman_ford: 0.822
I0302 18:58:24.596146 22535416901760 run.py:483] Algo bellman_ford step 201 current loss 0.763924, current_train_items 6464.
I0302 18:58:24.619771 22535416901760 run.py:483] Algo bellman_ford step 202 current loss 1.121222, current_train_items 6496.
I0302 18:58:24.651207 22535416901760 run.py:483] Algo bellman_ford step 203 current loss 1.437382, current_train_items 6528.
I0302 18:58:24.685942 22535416901760 run.py:483] Algo bellman_ford step 204 current loss 1.678683, current_train_items 6560.
I0302 18:58:24.705393 22535416901760 run.py:483] Algo bellman_ford step 205 current loss 0.521254, current_train_items 6592.
I0302 18:58:24.720660 22535416901760 run.py:483] Algo bellman_ford step 206 current loss 0.706747, current_train_items 6624.
I0302 18:58:24.744010 22535416901760 run.py:483] Algo bellman_ford step 207 current loss 1.308058, current_train_items 6656.
I0302 18:58:24.773516 22535416901760 run.py:483] Algo bellman_ford step 208 current loss 1.304572, current_train_items 6688.
I0302 18:58:24.803931 22535416901760 run.py:483] Algo bellman_ford step 209 current loss 1.588570, current_train_items 6720.
I0302 18:58:24.822477 22535416901760 run.py:483] Algo bellman_ford step 210 current loss 0.588887, current_train_items 6752.
I0302 18:58:24.838544 22535416901760 run.py:483] Algo bellman_ford step 211 current loss 0.773996, current_train_items 6784.
I0302 18:58:24.861742 22535416901760 run.py:483] Algo bellman_ford step 212 current loss 1.327071, current_train_items 6816.
I0302 18:58:24.890981 22535416901760 run.py:483] Algo bellman_ford step 213 current loss 1.341499, current_train_items 6848.
I0302 18:58:24.919046 22535416901760 run.py:483] Algo bellman_ford step 214 current loss 1.234686, current_train_items 6880.
I0302 18:58:24.937445 22535416901760 run.py:483] Algo bellman_ford step 215 current loss 0.396735, current_train_items 6912.
I0302 18:58:24.953334 22535416901760 run.py:483] Algo bellman_ford step 216 current loss 0.767561, current_train_items 6944.
I0302 18:58:24.975995 22535416901760 run.py:483] Algo bellman_ford step 217 current loss 1.549106, current_train_items 6976.
I0302 18:58:25.004297 22535416901760 run.py:483] Algo bellman_ford step 218 current loss 1.449247, current_train_items 7008.
I0302 18:58:25.034397 22535416901760 run.py:483] Algo bellman_ford step 219 current loss 1.636637, current_train_items 7040.
I0302 18:58:25.052633 22535416901760 run.py:483] Algo bellman_ford step 220 current loss 0.432628, current_train_items 7072.
I0302 18:58:25.068700 22535416901760 run.py:483] Algo bellman_ford step 221 current loss 0.822228, current_train_items 7104.
I0302 18:58:25.092349 22535416901760 run.py:483] Algo bellman_ford step 222 current loss 1.280601, current_train_items 7136.
I0302 18:58:25.120294 22535416901760 run.py:483] Algo bellman_ford step 223 current loss 1.452327, current_train_items 7168.
I0302 18:58:25.151933 22535416901760 run.py:483] Algo bellman_ford step 224 current loss 1.558220, current_train_items 7200.
I0302 18:58:25.170382 22535416901760 run.py:483] Algo bellman_ford step 225 current loss 0.483544, current_train_items 7232.
I0302 18:58:25.186545 22535416901760 run.py:483] Algo bellman_ford step 226 current loss 0.934652, current_train_items 7264.
I0302 18:58:25.209300 22535416901760 run.py:483] Algo bellman_ford step 227 current loss 1.290543, current_train_items 7296.
I0302 18:58:25.238696 22535416901760 run.py:483] Algo bellman_ford step 228 current loss 1.449231, current_train_items 7328.
I0302 18:58:25.270444 22535416901760 run.py:483] Algo bellman_ford step 229 current loss 1.880944, current_train_items 7360.
I0302 18:58:25.288765 22535416901760 run.py:483] Algo bellman_ford step 230 current loss 0.471869, current_train_items 7392.
I0302 18:58:25.304736 22535416901760 run.py:483] Algo bellman_ford step 231 current loss 0.830830, current_train_items 7424.
I0302 18:58:25.328027 22535416901760 run.py:483] Algo bellman_ford step 232 current loss 1.162314, current_train_items 7456.
I0302 18:58:25.357827 22535416901760 run.py:483] Algo bellman_ford step 233 current loss 1.272482, current_train_items 7488.
I0302 18:58:25.390328 22535416901760 run.py:483] Algo bellman_ford step 234 current loss 1.596435, current_train_items 7520.
I0302 18:58:25.409119 22535416901760 run.py:483] Algo bellman_ford step 235 current loss 0.480123, current_train_items 7552.
I0302 18:58:25.424966 22535416901760 run.py:483] Algo bellman_ford step 236 current loss 0.734267, current_train_items 7584.
I0302 18:58:25.447577 22535416901760 run.py:483] Algo bellman_ford step 237 current loss 1.245893, current_train_items 7616.
I0302 18:58:25.476679 22535416901760 run.py:483] Algo bellman_ford step 238 current loss 1.336327, current_train_items 7648.
I0302 18:58:25.508148 22535416901760 run.py:483] Algo bellman_ford step 239 current loss 1.527698, current_train_items 7680.
I0302 18:58:25.526504 22535416901760 run.py:483] Algo bellman_ford step 240 current loss 0.419279, current_train_items 7712.
I0302 18:58:25.542718 22535416901760 run.py:483] Algo bellman_ford step 241 current loss 0.922575, current_train_items 7744.
I0302 18:58:25.565512 22535416901760 run.py:483] Algo bellman_ford step 242 current loss 1.079325, current_train_items 7776.
I0302 18:58:25.594170 22535416901760 run.py:483] Algo bellman_ford step 243 current loss 1.329353, current_train_items 7808.
I0302 18:58:25.622935 22535416901760 run.py:483] Algo bellman_ford step 244 current loss 1.333075, current_train_items 7840.
I0302 18:58:25.641741 22535416901760 run.py:483] Algo bellman_ford step 245 current loss 0.418709, current_train_items 7872.
I0302 18:58:25.657670 22535416901760 run.py:483] Algo bellman_ford step 246 current loss 0.730929, current_train_items 7904.
I0302 18:58:25.680194 22535416901760 run.py:483] Algo bellman_ford step 247 current loss 1.146778, current_train_items 7936.
I0302 18:58:25.710145 22535416901760 run.py:483] Algo bellman_ford step 248 current loss 1.406401, current_train_items 7968.
I0302 18:58:25.741616 22535416901760 run.py:483] Algo bellman_ford step 249 current loss 1.343365, current_train_items 8000.
I0302 18:58:25.759755 22535416901760 run.py:483] Algo bellman_ford step 250 current loss 0.518395, current_train_items 8032.
I0302 18:58:25.767752 22535416901760 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.8173828125, 'score': 0.8173828125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:58:25.767860 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.822, current avg val score is 0.817, val scores are: bellman_ford: 0.817
I0302 18:58:25.785093 22535416901760 run.py:483] Algo bellman_ford step 251 current loss 0.890220, current_train_items 8064.
I0302 18:58:25.809043 22535416901760 run.py:483] Algo bellman_ford step 252 current loss 1.104856, current_train_items 8096.
I0302 18:58:25.838950 22535416901760 run.py:483] Algo bellman_ford step 253 current loss 1.243559, current_train_items 8128.
I0302 18:58:25.871606 22535416901760 run.py:483] Algo bellman_ford step 254 current loss 1.481817, current_train_items 8160.
I0302 18:58:25.890632 22535416901760 run.py:483] Algo bellman_ford step 255 current loss 0.578948, current_train_items 8192.
I0302 18:58:25.906221 22535416901760 run.py:483] Algo bellman_ford step 256 current loss 0.767471, current_train_items 8224.
I0302 18:58:25.928644 22535416901760 run.py:483] Algo bellman_ford step 257 current loss 0.928335, current_train_items 8256.
I0302 18:58:25.957364 22535416901760 run.py:483] Algo bellman_ford step 258 current loss 1.174207, current_train_items 8288.
I0302 18:58:25.987724 22535416901760 run.py:483] Algo bellman_ford step 259 current loss 1.447160, current_train_items 8320.
I0302 18:58:26.006603 22535416901760 run.py:483] Algo bellman_ford step 260 current loss 0.474541, current_train_items 8352.
I0302 18:58:26.022890 22535416901760 run.py:483] Algo bellman_ford step 261 current loss 0.746824, current_train_items 8384.
I0302 18:58:26.045459 22535416901760 run.py:483] Algo bellman_ford step 262 current loss 1.095969, current_train_items 8416.
I0302 18:58:26.073747 22535416901760 run.py:483] Algo bellman_ford step 263 current loss 1.174929, current_train_items 8448.
I0302 18:58:26.103771 22535416901760 run.py:483] Algo bellman_ford step 264 current loss 1.374539, current_train_items 8480.
I0302 18:58:26.122287 22535416901760 run.py:483] Algo bellman_ford step 265 current loss 0.371984, current_train_items 8512.
I0302 18:58:26.138582 22535416901760 run.py:483] Algo bellman_ford step 266 current loss 0.795542, current_train_items 8544.
I0302 18:58:26.162353 22535416901760 run.py:483] Algo bellman_ford step 267 current loss 1.221719, current_train_items 8576.
I0302 18:58:26.192117 22535416901760 run.py:483] Algo bellman_ford step 268 current loss 1.286441, current_train_items 8608.
I0302 18:58:26.222043 22535416901760 run.py:483] Algo bellman_ford step 269 current loss 1.322223, current_train_items 8640.
I0302 18:58:26.241260 22535416901760 run.py:483] Algo bellman_ford step 270 current loss 0.445218, current_train_items 8672.
I0302 18:58:26.257007 22535416901760 run.py:483] Algo bellman_ford step 271 current loss 0.636439, current_train_items 8704.
I0302 18:58:26.280425 22535416901760 run.py:483] Algo bellman_ford step 272 current loss 0.973933, current_train_items 8736.
I0302 18:58:26.308921 22535416901760 run.py:483] Algo bellman_ford step 273 current loss 1.227852, current_train_items 8768.
I0302 18:58:26.338828 22535416901760 run.py:483] Algo bellman_ford step 274 current loss 1.488432, current_train_items 8800.
I0302 18:58:26.357488 22535416901760 run.py:483] Algo bellman_ford step 275 current loss 0.452952, current_train_items 8832.
I0302 18:58:26.373669 22535416901760 run.py:483] Algo bellman_ford step 276 current loss 0.606641, current_train_items 8864.
I0302 18:58:26.397391 22535416901760 run.py:483] Algo bellman_ford step 277 current loss 1.213312, current_train_items 8896.
I0302 18:58:26.427437 22535416901760 run.py:483] Algo bellman_ford step 278 current loss 1.506071, current_train_items 8928.
I0302 18:58:26.458459 22535416901760 run.py:483] Algo bellman_ford step 279 current loss 1.473246, current_train_items 8960.
I0302 18:58:26.476811 22535416901760 run.py:483] Algo bellman_ford step 280 current loss 0.378904, current_train_items 8992.
I0302 18:58:26.493269 22535416901760 run.py:483] Algo bellman_ford step 281 current loss 0.810172, current_train_items 9024.
I0302 18:58:26.516737 22535416901760 run.py:483] Algo bellman_ford step 282 current loss 1.075580, current_train_items 9056.
I0302 18:58:26.545417 22535416901760 run.py:483] Algo bellman_ford step 283 current loss 1.451391, current_train_items 9088.
I0302 18:58:26.577973 22535416901760 run.py:483] Algo bellman_ford step 284 current loss 2.192015, current_train_items 9120.
I0302 18:58:26.596779 22535416901760 run.py:483] Algo bellman_ford step 285 current loss 0.457490, current_train_items 9152.
I0302 18:58:26.613125 22535416901760 run.py:483] Algo bellman_ford step 286 current loss 0.776502, current_train_items 9184.
I0302 18:58:26.635850 22535416901760 run.py:483] Algo bellman_ford step 287 current loss 1.094723, current_train_items 9216.
I0302 18:58:26.665680 22535416901760 run.py:483] Algo bellman_ford step 288 current loss 1.421188, current_train_items 9248.
I0302 18:58:26.698010 22535416901760 run.py:483] Algo bellman_ford step 289 current loss 1.685517, current_train_items 9280.
I0302 18:58:26.717008 22535416901760 run.py:483] Algo bellman_ford step 290 current loss 0.362450, current_train_items 9312.
I0302 18:58:26.733210 22535416901760 run.py:483] Algo bellman_ford step 291 current loss 0.805882, current_train_items 9344.
I0302 18:58:26.756024 22535416901760 run.py:483] Algo bellman_ford step 292 current loss 1.004140, current_train_items 9376.
I0302 18:58:26.784711 22535416901760 run.py:483] Algo bellman_ford step 293 current loss 1.283544, current_train_items 9408.
I0302 18:58:26.815666 22535416901760 run.py:483] Algo bellman_ford step 294 current loss 1.339016, current_train_items 9440.
I0302 18:58:26.833950 22535416901760 run.py:483] Algo bellman_ford step 295 current loss 0.420604, current_train_items 9472.
I0302 18:58:26.849796 22535416901760 run.py:483] Algo bellman_ford step 296 current loss 0.749956, current_train_items 9504.
I0302 18:58:26.873080 22535416901760 run.py:483] Algo bellman_ford step 297 current loss 1.071594, current_train_items 9536.
I0302 18:58:26.903102 22535416901760 run.py:483] Algo bellman_ford step 298 current loss 1.151608, current_train_items 9568.
I0302 18:58:26.933768 22535416901760 run.py:483] Algo bellman_ford step 299 current loss 1.287938, current_train_items 9600.
I0302 18:58:26.952447 22535416901760 run.py:483] Algo bellman_ford step 300 current loss 0.449641, current_train_items 9632.
I0302 18:58:26.960026 22535416901760 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.8349609375, 'score': 0.8349609375, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:58:26.960135 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.822, current avg val score is 0.835, val scores are: bellman_ford: 0.835
I0302 18:58:26.993734 22535416901760 run.py:483] Algo bellman_ford step 301 current loss 0.753456, current_train_items 9664.
I0302 18:58:27.016535 22535416901760 run.py:483] Algo bellman_ford step 302 current loss 1.024159, current_train_items 9696.
I0302 18:58:27.045108 22535416901760 run.py:483] Algo bellman_ford step 303 current loss 1.019819, current_train_items 9728.
I0302 18:58:27.078222 22535416901760 run.py:483] Algo bellman_ford step 304 current loss 1.506808, current_train_items 9760.
I0302 18:58:27.097416 22535416901760 run.py:483] Algo bellman_ford step 305 current loss 0.454325, current_train_items 9792.
I0302 18:58:27.113982 22535416901760 run.py:483] Algo bellman_ford step 306 current loss 0.998181, current_train_items 9824.
I0302 18:58:27.137886 22535416901760 run.py:483] Algo bellman_ford step 307 current loss 1.105856, current_train_items 9856.
I0302 18:58:27.167200 22535416901760 run.py:483] Algo bellman_ford step 308 current loss 1.241290, current_train_items 9888.
I0302 18:58:27.198640 22535416901760 run.py:483] Algo bellman_ford step 309 current loss 1.445430, current_train_items 9920.
I0302 18:58:27.217325 22535416901760 run.py:483] Algo bellman_ford step 310 current loss 0.471138, current_train_items 9952.
I0302 18:58:27.233557 22535416901760 run.py:483] Algo bellman_ford step 311 current loss 0.696927, current_train_items 9984.
I0302 18:58:27.256495 22535416901760 run.py:483] Algo bellman_ford step 312 current loss 1.022572, current_train_items 10016.
I0302 18:58:27.286257 22535416901760 run.py:483] Algo bellman_ford step 313 current loss 1.293766, current_train_items 10048.
I0302 18:58:27.317698 22535416901760 run.py:483] Algo bellman_ford step 314 current loss 1.402548, current_train_items 10080.
I0302 18:58:27.335919 22535416901760 run.py:483] Algo bellman_ford step 315 current loss 0.476012, current_train_items 10112.
I0302 18:58:27.352069 22535416901760 run.py:483] Algo bellman_ford step 316 current loss 0.724123, current_train_items 10144.
I0302 18:58:27.374745 22535416901760 run.py:483] Algo bellman_ford step 317 current loss 0.917781, current_train_items 10176.
I0302 18:58:27.402694 22535416901760 run.py:483] Algo bellman_ford step 318 current loss 1.065962, current_train_items 10208.
I0302 18:58:27.433140 22535416901760 run.py:483] Algo bellman_ford step 319 current loss 1.428565, current_train_items 10240.
I0302 18:58:27.451869 22535416901760 run.py:483] Algo bellman_ford step 320 current loss 0.407350, current_train_items 10272.
I0302 18:58:27.468243 22535416901760 run.py:483] Algo bellman_ford step 321 current loss 0.826164, current_train_items 10304.
I0302 18:58:27.491067 22535416901760 run.py:483] Algo bellman_ford step 322 current loss 1.017568, current_train_items 10336.
I0302 18:58:27.519412 22535416901760 run.py:483] Algo bellman_ford step 323 current loss 1.013378, current_train_items 10368.
I0302 18:58:27.551052 22535416901760 run.py:483] Algo bellman_ford step 324 current loss 1.488858, current_train_items 10400.
I0302 18:58:27.569390 22535416901760 run.py:483] Algo bellman_ford step 325 current loss 0.484688, current_train_items 10432.
I0302 18:58:27.585253 22535416901760 run.py:483] Algo bellman_ford step 326 current loss 0.768196, current_train_items 10464.
I0302 18:58:27.608124 22535416901760 run.py:483] Algo bellman_ford step 327 current loss 1.192863, current_train_items 10496.
I0302 18:58:27.637714 22535416901760 run.py:483] Algo bellman_ford step 328 current loss 1.537495, current_train_items 10528.
I0302 18:58:27.670029 22535416901760 run.py:483] Algo bellman_ford step 329 current loss 1.474372, current_train_items 10560.
I0302 18:58:27.688694 22535416901760 run.py:483] Algo bellman_ford step 330 current loss 0.418049, current_train_items 10592.
I0302 18:58:27.704813 22535416901760 run.py:483] Algo bellman_ford step 331 current loss 0.795861, current_train_items 10624.
I0302 18:58:27.727660 22535416901760 run.py:483] Algo bellman_ford step 332 current loss 1.182991, current_train_items 10656.
I0302 18:58:27.757282 22535416901760 run.py:483] Algo bellman_ford step 333 current loss 1.390444, current_train_items 10688.
I0302 18:58:27.789929 22535416901760 run.py:483] Algo bellman_ford step 334 current loss 1.895934, current_train_items 10720.
I0302 18:58:27.808910 22535416901760 run.py:483] Algo bellman_ford step 335 current loss 0.457244, current_train_items 10752.
I0302 18:58:27.825362 22535416901760 run.py:483] Algo bellman_ford step 336 current loss 0.953551, current_train_items 10784.
I0302 18:58:27.849462 22535416901760 run.py:483] Algo bellman_ford step 337 current loss 1.518546, current_train_items 10816.
I0302 18:58:27.878537 22535416901760 run.py:483] Algo bellman_ford step 338 current loss 1.564243, current_train_items 10848.
I0302 18:58:27.910707 22535416901760 run.py:483] Algo bellman_ford step 339 current loss 1.899371, current_train_items 10880.
I0302 18:58:27.929192 22535416901760 run.py:483] Algo bellman_ford step 340 current loss 0.403146, current_train_items 10912.
I0302 18:58:27.944999 22535416901760 run.py:483] Algo bellman_ford step 341 current loss 0.887140, current_train_items 10944.
I0302 18:58:27.967884 22535416901760 run.py:483] Algo bellman_ford step 342 current loss 1.212743, current_train_items 10976.
I0302 18:58:27.995865 22535416901760 run.py:483] Algo bellman_ford step 343 current loss 1.153355, current_train_items 11008.
I0302 18:58:28.027815 22535416901760 run.py:483] Algo bellman_ford step 344 current loss 1.409128, current_train_items 11040.
I0302 18:58:28.046487 22535416901760 run.py:483] Algo bellman_ford step 345 current loss 0.443733, current_train_items 11072.
I0302 18:58:28.062397 22535416901760 run.py:483] Algo bellman_ford step 346 current loss 0.787840, current_train_items 11104.
I0302 18:58:28.086393 22535416901760 run.py:483] Algo bellman_ford step 347 current loss 1.359993, current_train_items 11136.
I0302 18:58:28.115269 22535416901760 run.py:483] Algo bellman_ford step 348 current loss 1.226894, current_train_items 11168.
I0302 18:58:28.147513 22535416901760 run.py:483] Algo bellman_ford step 349 current loss 1.536539, current_train_items 11200.
I0302 18:58:28.166290 22535416901760 run.py:483] Algo bellman_ford step 350 current loss 0.514969, current_train_items 11232.
I0302 18:58:28.174201 22535416901760 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.80078125, 'score': 0.80078125, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:58:28.174308 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.835, current avg val score is 0.801, val scores are: bellman_ford: 0.801
I0302 18:58:28.190703 22535416901760 run.py:483] Algo bellman_ford step 351 current loss 0.724986, current_train_items 11264.
I0302 18:58:28.214474 22535416901760 run.py:483] Algo bellman_ford step 352 current loss 1.196188, current_train_items 11296.
I0302 18:58:28.243785 22535416901760 run.py:483] Algo bellman_ford step 353 current loss 1.800343, current_train_items 11328.
I0302 18:58:28.277296 22535416901760 run.py:483] Algo bellman_ford step 354 current loss 1.711368, current_train_items 11360.
I0302 18:58:28.296227 22535416901760 run.py:483] Algo bellman_ford step 355 current loss 0.411997, current_train_items 11392.
I0302 18:58:28.311978 22535416901760 run.py:483] Algo bellman_ford step 356 current loss 0.748090, current_train_items 11424.
I0302 18:58:28.334489 22535416901760 run.py:483] Algo bellman_ford step 357 current loss 0.964167, current_train_items 11456.
I0302 18:58:28.364820 22535416901760 run.py:483] Algo bellman_ford step 358 current loss 1.473616, current_train_items 11488.
I0302 18:58:28.397227 22535416901760 run.py:483] Algo bellman_ford step 359 current loss 1.644278, current_train_items 11520.
I0302 18:58:28.416008 22535416901760 run.py:483] Algo bellman_ford step 360 current loss 0.393283, current_train_items 11552.
I0302 18:58:28.432545 22535416901760 run.py:483] Algo bellman_ford step 361 current loss 0.728833, current_train_items 11584.
I0302 18:58:28.454346 22535416901760 run.py:483] Algo bellman_ford step 362 current loss 1.023196, current_train_items 11616.
I0302 18:58:28.483371 22535416901760 run.py:483] Algo bellman_ford step 363 current loss 1.227629, current_train_items 11648.
I0302 18:58:28.513217 22535416901760 run.py:483] Algo bellman_ford step 364 current loss 1.261687, current_train_items 11680.
I0302 18:58:28.531589 22535416901760 run.py:483] Algo bellman_ford step 365 current loss 0.415342, current_train_items 11712.
I0302 18:58:28.547730 22535416901760 run.py:483] Algo bellman_ford step 366 current loss 0.833847, current_train_items 11744.
I0302 18:58:28.569766 22535416901760 run.py:483] Algo bellman_ford step 367 current loss 1.058247, current_train_items 11776.
I0302 18:58:28.599738 22535416901760 run.py:483] Algo bellman_ford step 368 current loss 1.406480, current_train_items 11808.
I0302 18:58:28.629064 22535416901760 run.py:483] Algo bellman_ford step 369 current loss 1.265926, current_train_items 11840.
I0302 18:58:28.647590 22535416901760 run.py:483] Algo bellman_ford step 370 current loss 0.364187, current_train_items 11872.
I0302 18:58:28.663605 22535416901760 run.py:483] Algo bellman_ford step 371 current loss 0.711183, current_train_items 11904.
I0302 18:58:28.685833 22535416901760 run.py:483] Algo bellman_ford step 372 current loss 1.065679, current_train_items 11936.
I0302 18:58:28.715880 22535416901760 run.py:483] Algo bellman_ford step 373 current loss 1.201170, current_train_items 11968.
I0302 18:58:28.748926 22535416901760 run.py:483] Algo bellman_ford step 374 current loss 1.518141, current_train_items 12000.
I0302 18:58:28.767945 22535416901760 run.py:483] Algo bellman_ford step 375 current loss 0.443537, current_train_items 12032.
I0302 18:58:28.783794 22535416901760 run.py:483] Algo bellman_ford step 376 current loss 0.642083, current_train_items 12064.
I0302 18:58:28.806931 22535416901760 run.py:483] Algo bellman_ford step 377 current loss 1.006148, current_train_items 12096.
I0302 18:58:28.835717 22535416901760 run.py:483] Algo bellman_ford step 378 current loss 0.992246, current_train_items 12128.
I0302 18:58:28.868191 22535416901760 run.py:483] Algo bellman_ford step 379 current loss 1.410670, current_train_items 12160.
I0302 18:58:28.886541 22535416901760 run.py:483] Algo bellman_ford step 380 current loss 0.396449, current_train_items 12192.
I0302 18:58:28.902850 22535416901760 run.py:483] Algo bellman_ford step 381 current loss 0.767332, current_train_items 12224.
I0302 18:58:28.925747 22535416901760 run.py:483] Algo bellman_ford step 382 current loss 1.050080, current_train_items 12256.
I0302 18:58:28.955346 22535416901760 run.py:483] Algo bellman_ford step 383 current loss 1.312786, current_train_items 12288.
I0302 18:58:28.987745 22535416901760 run.py:483] Algo bellman_ford step 384 current loss 1.439388, current_train_items 12320.
I0302 18:58:29.006508 22535416901760 run.py:483] Algo bellman_ford step 385 current loss 0.473796, current_train_items 12352.
I0302 18:58:29.022914 22535416901760 run.py:483] Algo bellman_ford step 386 current loss 0.690353, current_train_items 12384.
I0302 18:58:29.045317 22535416901760 run.py:483] Algo bellman_ford step 387 current loss 1.099079, current_train_items 12416.
I0302 18:58:29.073913 22535416901760 run.py:483] Algo bellman_ford step 388 current loss 1.096875, current_train_items 12448.
I0302 18:58:29.106365 22535416901760 run.py:483] Algo bellman_ford step 389 current loss 1.404186, current_train_items 12480.
I0302 18:58:29.125089 22535416901760 run.py:483] Algo bellman_ford step 390 current loss 0.405189, current_train_items 12512.
I0302 18:58:29.141135 22535416901760 run.py:483] Algo bellman_ford step 391 current loss 0.768204, current_train_items 12544.
I0302 18:58:29.163862 22535416901760 run.py:483] Algo bellman_ford step 392 current loss 0.992014, current_train_items 12576.
I0302 18:58:29.194683 22535416901760 run.py:483] Algo bellman_ford step 393 current loss 1.216354, current_train_items 12608.
I0302 18:58:29.225835 22535416901760 run.py:483] Algo bellman_ford step 394 current loss 1.371787, current_train_items 12640.
I0302 18:58:29.244086 22535416901760 run.py:483] Algo bellman_ford step 395 current loss 0.360288, current_train_items 12672.
I0302 18:58:29.260284 22535416901760 run.py:483] Algo bellman_ford step 396 current loss 0.707202, current_train_items 12704.
I0302 18:58:29.283648 22535416901760 run.py:483] Algo bellman_ford step 397 current loss 1.176511, current_train_items 12736.
I0302 18:58:29.314042 22535416901760 run.py:483] Algo bellman_ford step 398 current loss 1.159908, current_train_items 12768.
I0302 18:58:29.343685 22535416901760 run.py:483] Algo bellman_ford step 399 current loss 1.335654, current_train_items 12800.
I0302 18:58:29.362291 22535416901760 run.py:483] Algo bellman_ford step 400 current loss 0.354287, current_train_items 12832.
I0302 18:58:29.370139 22535416901760 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:58:29.370258 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.835, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:29.400402 22535416901760 run.py:483] Algo bellman_ford step 401 current loss 0.706109, current_train_items 12864.
I0302 18:58:29.424148 22535416901760 run.py:483] Algo bellman_ford step 402 current loss 0.944188, current_train_items 12896.
I0302 18:58:29.454424 22535416901760 run.py:483] Algo bellman_ford step 403 current loss 1.357303, current_train_items 12928.
I0302 18:58:29.485086 22535416901760 run.py:483] Algo bellman_ford step 404 current loss 1.670735, current_train_items 12960.
I0302 18:58:29.504402 22535416901760 run.py:483] Algo bellman_ford step 405 current loss 0.435222, current_train_items 12992.
I0302 18:58:29.519771 22535416901760 run.py:483] Algo bellman_ford step 406 current loss 0.670179, current_train_items 13024.
I0302 18:58:29.542592 22535416901760 run.py:483] Algo bellman_ford step 407 current loss 1.040786, current_train_items 13056.
I0302 18:58:29.572742 22535416901760 run.py:483] Algo bellman_ford step 408 current loss 1.220123, current_train_items 13088.
I0302 18:58:29.604283 22535416901760 run.py:483] Algo bellman_ford step 409 current loss 1.468108, current_train_items 13120.
I0302 18:58:29.623036 22535416901760 run.py:483] Algo bellman_ford step 410 current loss 0.432757, current_train_items 13152.
I0302 18:58:29.638977 22535416901760 run.py:483] Algo bellman_ford step 411 current loss 0.609742, current_train_items 13184.
I0302 18:58:29.661836 22535416901760 run.py:483] Algo bellman_ford step 412 current loss 1.010281, current_train_items 13216.
I0302 18:58:29.691141 22535416901760 run.py:483] Algo bellman_ford step 413 current loss 1.105451, current_train_items 13248.
I0302 18:58:29.723142 22535416901760 run.py:483] Algo bellman_ford step 414 current loss 1.313002, current_train_items 13280.
I0302 18:58:29.741642 22535416901760 run.py:483] Algo bellman_ford step 415 current loss 0.459840, current_train_items 13312.
I0302 18:58:29.757583 22535416901760 run.py:483] Algo bellman_ford step 416 current loss 0.633311, current_train_items 13344.
I0302 18:58:29.781188 22535416901760 run.py:483] Algo bellman_ford step 417 current loss 1.115927, current_train_items 13376.
I0302 18:58:29.812051 22535416901760 run.py:483] Algo bellman_ford step 418 current loss 1.348612, current_train_items 13408.
I0302 18:58:29.843850 22535416901760 run.py:483] Algo bellman_ford step 419 current loss 1.393778, current_train_items 13440.
I0302 18:58:29.862223 22535416901760 run.py:483] Algo bellman_ford step 420 current loss 0.470695, current_train_items 13472.
I0302 18:58:29.878092 22535416901760 run.py:483] Algo bellman_ford step 421 current loss 0.779248, current_train_items 13504.
I0302 18:58:29.901448 22535416901760 run.py:483] Algo bellman_ford step 422 current loss 1.081977, current_train_items 13536.
I0302 18:58:29.929664 22535416901760 run.py:483] Algo bellman_ford step 423 current loss 1.123163, current_train_items 13568.
I0302 18:58:29.960973 22535416901760 run.py:483] Algo bellman_ford step 424 current loss 1.299017, current_train_items 13600.
I0302 18:58:29.979350 22535416901760 run.py:483] Algo bellman_ford step 425 current loss 0.453456, current_train_items 13632.
I0302 18:58:29.995041 22535416901760 run.py:483] Algo bellman_ford step 426 current loss 0.506628, current_train_items 13664.
I0302 18:58:30.017385 22535416901760 run.py:483] Algo bellman_ford step 427 current loss 1.031087, current_train_items 13696.
I0302 18:58:30.045939 22535416901760 run.py:483] Algo bellman_ford step 428 current loss 1.098819, current_train_items 13728.
I0302 18:58:30.077280 22535416901760 run.py:483] Algo bellman_ford step 429 current loss 1.299871, current_train_items 13760.
I0302 18:58:30.095962 22535416901760 run.py:483] Algo bellman_ford step 430 current loss 0.452370, current_train_items 13792.
I0302 18:58:30.112032 22535416901760 run.py:483] Algo bellman_ford step 431 current loss 0.825166, current_train_items 13824.
I0302 18:58:30.135422 22535416901760 run.py:483] Algo bellman_ford step 432 current loss 1.229027, current_train_items 13856.
I0302 18:58:30.163731 22535416901760 run.py:483] Algo bellman_ford step 433 current loss 1.174701, current_train_items 13888.
I0302 18:58:30.194800 22535416901760 run.py:483] Algo bellman_ford step 434 current loss 1.479910, current_train_items 13920.
I0302 18:58:30.212913 22535416901760 run.py:483] Algo bellman_ford step 435 current loss 0.407193, current_train_items 13952.
I0302 18:58:30.229246 22535416901760 run.py:483] Algo bellman_ford step 436 current loss 0.697710, current_train_items 13984.
I0302 18:58:30.251880 22535416901760 run.py:483] Algo bellman_ford step 437 current loss 1.027628, current_train_items 14016.
I0302 18:58:30.280144 22535416901760 run.py:483] Algo bellman_ford step 438 current loss 1.060630, current_train_items 14048.
I0302 18:58:30.311520 22535416901760 run.py:483] Algo bellman_ford step 439 current loss 1.227377, current_train_items 14080.
I0302 18:58:30.329526 22535416901760 run.py:483] Algo bellman_ford step 440 current loss 0.394628, current_train_items 14112.
I0302 18:58:30.345305 22535416901760 run.py:483] Algo bellman_ford step 441 current loss 0.636916, current_train_items 14144.
I0302 18:58:30.367284 22535416901760 run.py:483] Algo bellman_ford step 442 current loss 1.183339, current_train_items 14176.
I0302 18:58:30.395977 22535416901760 run.py:483] Algo bellman_ford step 443 current loss 1.145965, current_train_items 14208.
I0302 18:58:30.426228 22535416901760 run.py:483] Algo bellman_ford step 444 current loss 1.245572, current_train_items 14240.
I0302 18:58:30.444390 22535416901760 run.py:483] Algo bellman_ford step 445 current loss 0.407379, current_train_items 14272.
I0302 18:58:30.461092 22535416901760 run.py:483] Algo bellman_ford step 446 current loss 0.932159, current_train_items 14304.
I0302 18:58:30.484238 22535416901760 run.py:483] Algo bellman_ford step 447 current loss 1.104717, current_train_items 14336.
I0302 18:58:30.511611 22535416901760 run.py:483] Algo bellman_ford step 448 current loss 1.173377, current_train_items 14368.
I0302 18:58:30.541077 22535416901760 run.py:483] Algo bellman_ford step 449 current loss 1.243083, current_train_items 14400.
I0302 18:58:30.559540 22535416901760 run.py:483] Algo bellman_ford step 450 current loss 0.495147, current_train_items 14432.
I0302 18:58:30.567644 22535416901760 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.833984375, 'score': 0.833984375, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:58:30.567755 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.834, val scores are: bellman_ford: 0.834
I0302 18:58:30.584679 22535416901760 run.py:483] Algo bellman_ford step 451 current loss 0.802412, current_train_items 14464.
I0302 18:58:30.607867 22535416901760 run.py:483] Algo bellman_ford step 452 current loss 1.104830, current_train_items 14496.
I0302 18:58:30.638389 22535416901760 run.py:483] Algo bellman_ford step 453 current loss 1.360374, current_train_items 14528.
I0302 18:58:30.670058 22535416901760 run.py:483] Algo bellman_ford step 454 current loss 1.429200, current_train_items 14560.
I0302 18:58:30.688802 22535416901760 run.py:483] Algo bellman_ford step 455 current loss 0.514321, current_train_items 14592.
I0302 18:58:30.704835 22535416901760 run.py:483] Algo bellman_ford step 456 current loss 0.698746, current_train_items 14624.
I0302 18:58:30.727239 22535416901760 run.py:483] Algo bellman_ford step 457 current loss 1.133077, current_train_items 14656.
I0302 18:58:30.756046 22535416901760 run.py:483] Algo bellman_ford step 458 current loss 1.128684, current_train_items 14688.
I0302 18:58:30.786381 22535416901760 run.py:483] Algo bellman_ford step 459 current loss 1.366161, current_train_items 14720.
I0302 18:58:30.804772 22535416901760 run.py:483] Algo bellman_ford step 460 current loss 0.359229, current_train_items 14752.
I0302 18:58:30.821218 22535416901760 run.py:483] Algo bellman_ford step 461 current loss 0.769066, current_train_items 14784.
I0302 18:58:30.844187 22535416901760 run.py:483] Algo bellman_ford step 462 current loss 0.959209, current_train_items 14816.
I0302 18:58:30.873370 22535416901760 run.py:483] Algo bellman_ford step 463 current loss 1.338112, current_train_items 14848.
I0302 18:58:30.905852 22535416901760 run.py:483] Algo bellman_ford step 464 current loss 1.501038, current_train_items 14880.
I0302 18:58:30.924361 22535416901760 run.py:483] Algo bellman_ford step 465 current loss 0.421311, current_train_items 14912.
I0302 18:58:30.939954 22535416901760 run.py:483] Algo bellman_ford step 466 current loss 0.577390, current_train_items 14944.
I0302 18:58:30.963320 22535416901760 run.py:483] Algo bellman_ford step 467 current loss 1.056413, current_train_items 14976.
I0302 18:58:30.991064 22535416901760 run.py:483] Algo bellman_ford step 468 current loss 1.245561, current_train_items 15008.
I0302 18:58:31.023724 22535416901760 run.py:483] Algo bellman_ford step 469 current loss 1.497949, current_train_items 15040.
I0302 18:58:31.042329 22535416901760 run.py:483] Algo bellman_ford step 470 current loss 0.368960, current_train_items 15072.
I0302 18:58:31.058569 22535416901760 run.py:483] Algo bellman_ford step 471 current loss 0.674460, current_train_items 15104.
I0302 18:58:31.081682 22535416901760 run.py:483] Algo bellman_ford step 472 current loss 1.057223, current_train_items 15136.
I0302 18:58:31.111087 22535416901760 run.py:483] Algo bellman_ford step 473 current loss 1.209972, current_train_items 15168.
I0302 18:58:31.143653 22535416901760 run.py:483] Algo bellman_ford step 474 current loss 1.407340, current_train_items 15200.
I0302 18:58:31.162505 22535416901760 run.py:483] Algo bellman_ford step 475 current loss 0.480873, current_train_items 15232.
I0302 18:58:31.178240 22535416901760 run.py:483] Algo bellman_ford step 476 current loss 0.691902, current_train_items 15264.
I0302 18:58:31.201216 22535416901760 run.py:483] Algo bellman_ford step 477 current loss 0.998818, current_train_items 15296.
I0302 18:58:31.228869 22535416901760 run.py:483] Algo bellman_ford step 478 current loss 1.147201, current_train_items 15328.
I0302 18:58:31.261310 22535416901760 run.py:483] Algo bellman_ford step 479 current loss 1.645455, current_train_items 15360.
I0302 18:58:31.279349 22535416901760 run.py:483] Algo bellman_ford step 480 current loss 0.416490, current_train_items 15392.
I0302 18:58:31.295392 22535416901760 run.py:483] Algo bellman_ford step 481 current loss 0.759754, current_train_items 15424.
I0302 18:58:31.317634 22535416901760 run.py:483] Algo bellman_ford step 482 current loss 1.000077, current_train_items 15456.
I0302 18:58:31.346758 22535416901760 run.py:483] Algo bellman_ford step 483 current loss 1.160645, current_train_items 15488.
I0302 18:58:31.378650 22535416901760 run.py:483] Algo bellman_ford step 484 current loss 1.606714, current_train_items 15520.
I0302 18:58:31.397278 22535416901760 run.py:483] Algo bellman_ford step 485 current loss 0.419309, current_train_items 15552.
I0302 18:58:31.413398 22535416901760 run.py:483] Algo bellman_ford step 486 current loss 0.771961, current_train_items 15584.
I0302 18:58:31.436271 22535416901760 run.py:483] Algo bellman_ford step 487 current loss 1.085799, current_train_items 15616.
I0302 18:58:31.464933 22535416901760 run.py:483] Algo bellman_ford step 488 current loss 1.150535, current_train_items 15648.
I0302 18:58:31.495572 22535416901760 run.py:483] Algo bellman_ford step 489 current loss 1.289780, current_train_items 15680.
I0302 18:58:31.513983 22535416901760 run.py:483] Algo bellman_ford step 490 current loss 0.371555, current_train_items 15712.
I0302 18:58:31.530380 22535416901760 run.py:483] Algo bellman_ford step 491 current loss 0.709983, current_train_items 15744.
I0302 18:58:31.552821 22535416901760 run.py:483] Algo bellman_ford step 492 current loss 0.979629, current_train_items 15776.
I0302 18:58:31.581588 22535416901760 run.py:483] Algo bellman_ford step 493 current loss 1.162596, current_train_items 15808.
I0302 18:58:31.614387 22535416901760 run.py:483] Algo bellman_ford step 494 current loss 1.468163, current_train_items 15840.
I0302 18:58:31.632670 22535416901760 run.py:483] Algo bellman_ford step 495 current loss 0.367420, current_train_items 15872.
I0302 18:58:31.648447 22535416901760 run.py:483] Algo bellman_ford step 496 current loss 0.687243, current_train_items 15904.
I0302 18:58:31.672410 22535416901760 run.py:483] Algo bellman_ford step 497 current loss 1.147699, current_train_items 15936.
I0302 18:58:31.702252 22535416901760 run.py:483] Algo bellman_ford step 498 current loss 1.086455, current_train_items 15968.
I0302 18:58:31.733599 22535416901760 run.py:483] Algo bellman_ford step 499 current loss 1.256437, current_train_items 16000.
I0302 18:58:31.752335 22535416901760 run.py:483] Algo bellman_ford step 500 current loss 0.403386, current_train_items 16032.
I0302 18:58:31.760176 22535416901760 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.7978515625, 'score': 0.7978515625, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:58:31.760284 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.798, val scores are: bellman_ford: 0.798
I0302 18:58:31.776622 22535416901760 run.py:483] Algo bellman_ford step 501 current loss 0.580008, current_train_items 16064.
I0302 18:58:31.800295 22535416901760 run.py:483] Algo bellman_ford step 502 current loss 1.047657, current_train_items 16096.
I0302 18:58:31.831226 22535416901760 run.py:483] Algo bellman_ford step 503 current loss 1.255412, current_train_items 16128.
I0302 18:58:31.865527 22535416901760 run.py:483] Algo bellman_ford step 504 current loss 1.809867, current_train_items 16160.
I0302 18:58:31.884503 22535416901760 run.py:483] Algo bellman_ford step 505 current loss 0.457665, current_train_items 16192.
I0302 18:58:31.899773 22535416901760 run.py:483] Algo bellman_ford step 506 current loss 0.639090, current_train_items 16224.
I0302 18:58:31.921957 22535416901760 run.py:483] Algo bellman_ford step 507 current loss 0.986428, current_train_items 16256.
I0302 18:58:31.951689 22535416901760 run.py:483] Algo bellman_ford step 508 current loss 1.247881, current_train_items 16288.
I0302 18:58:31.983982 22535416901760 run.py:483] Algo bellman_ford step 509 current loss 1.467978, current_train_items 16320.
I0302 18:58:32.002613 22535416901760 run.py:483] Algo bellman_ford step 510 current loss 0.480886, current_train_items 16352.
I0302 18:58:32.018769 22535416901760 run.py:483] Algo bellman_ford step 511 current loss 0.745280, current_train_items 16384.
I0302 18:58:32.041906 22535416901760 run.py:483] Algo bellman_ford step 512 current loss 1.034475, current_train_items 16416.
I0302 18:58:32.071503 22535416901760 run.py:483] Algo bellman_ford step 513 current loss 1.345274, current_train_items 16448.
I0302 18:58:32.103010 22535416901760 run.py:483] Algo bellman_ford step 514 current loss 1.456842, current_train_items 16480.
I0302 18:58:32.121265 22535416901760 run.py:483] Algo bellman_ford step 515 current loss 0.392366, current_train_items 16512.
I0302 18:58:32.137185 22535416901760 run.py:483] Algo bellman_ford step 516 current loss 0.690065, current_train_items 16544.
I0302 18:58:32.160584 22535416901760 run.py:483] Algo bellman_ford step 517 current loss 1.287691, current_train_items 16576.
I0302 18:58:32.190779 22535416901760 run.py:483] Algo bellman_ford step 518 current loss 1.325727, current_train_items 16608.
I0302 18:58:32.222600 22535416901760 run.py:483] Algo bellman_ford step 519 current loss 1.604721, current_train_items 16640.
I0302 18:58:32.240816 22535416901760 run.py:483] Algo bellman_ford step 520 current loss 0.396972, current_train_items 16672.
I0302 18:58:32.257111 22535416901760 run.py:483] Algo bellman_ford step 521 current loss 0.671046, current_train_items 16704.
I0302 18:58:32.279931 22535416901760 run.py:483] Algo bellman_ford step 522 current loss 0.882313, current_train_items 16736.
I0302 18:58:32.309578 22535416901760 run.py:483] Algo bellman_ford step 523 current loss 1.169792, current_train_items 16768.
I0302 18:58:32.340431 22535416901760 run.py:483] Algo bellman_ford step 524 current loss 1.535091, current_train_items 16800.
I0302 18:58:32.358458 22535416901760 run.py:483] Algo bellman_ford step 525 current loss 0.349321, current_train_items 16832.
I0302 18:58:32.374857 22535416901760 run.py:483] Algo bellman_ford step 526 current loss 0.763295, current_train_items 16864.
I0302 18:58:32.397668 22535416901760 run.py:483] Algo bellman_ford step 527 current loss 1.051568, current_train_items 16896.
I0302 18:58:32.426080 22535416901760 run.py:483] Algo bellman_ford step 528 current loss 1.027156, current_train_items 16928.
I0302 18:58:32.458269 22535416901760 run.py:483] Algo bellman_ford step 529 current loss 1.443100, current_train_items 16960.
I0302 18:58:32.476736 22535416901760 run.py:483] Algo bellman_ford step 530 current loss 0.418539, current_train_items 16992.
I0302 18:58:32.492804 22535416901760 run.py:483] Algo bellman_ford step 531 current loss 0.760741, current_train_items 17024.
I0302 18:58:32.514890 22535416901760 run.py:483] Algo bellman_ford step 532 current loss 0.862544, current_train_items 17056.
I0302 18:58:32.544457 22535416901760 run.py:483] Algo bellman_ford step 533 current loss 1.212773, current_train_items 17088.
I0302 18:58:32.576082 22535416901760 run.py:483] Algo bellman_ford step 534 current loss 1.444424, current_train_items 17120.
I0302 18:58:32.594474 22535416901760 run.py:483] Algo bellman_ford step 535 current loss 0.415743, current_train_items 17152.
I0302 18:58:32.610063 22535416901760 run.py:483] Algo bellman_ford step 536 current loss 0.609531, current_train_items 17184.
I0302 18:58:32.631978 22535416901760 run.py:483] Algo bellman_ford step 537 current loss 0.950335, current_train_items 17216.
I0302 18:58:32.660317 22535416901760 run.py:483] Algo bellman_ford step 538 current loss 1.235503, current_train_items 17248.
I0302 18:58:32.690651 22535416901760 run.py:483] Algo bellman_ford step 539 current loss 1.284663, current_train_items 17280.
I0302 18:58:32.708705 22535416901760 run.py:483] Algo bellman_ford step 540 current loss 0.424363, current_train_items 17312.
I0302 18:58:32.724099 22535416901760 run.py:483] Algo bellman_ford step 541 current loss 0.616477, current_train_items 17344.
I0302 18:58:32.747559 22535416901760 run.py:483] Algo bellman_ford step 542 current loss 1.111027, current_train_items 17376.
I0302 18:58:32.777045 22535416901760 run.py:483] Algo bellman_ford step 543 current loss 1.146687, current_train_items 17408.
I0302 18:58:32.808965 22535416901760 run.py:483] Algo bellman_ford step 544 current loss 1.406544, current_train_items 17440.
I0302 18:58:32.827358 22535416901760 run.py:483] Algo bellman_ford step 545 current loss 0.434347, current_train_items 17472.
I0302 18:58:32.843371 22535416901760 run.py:483] Algo bellman_ford step 546 current loss 0.780759, current_train_items 17504.
I0302 18:58:32.866148 22535416901760 run.py:483] Algo bellman_ford step 547 current loss 1.025000, current_train_items 17536.
I0302 18:58:32.895686 22535416901760 run.py:483] Algo bellman_ford step 548 current loss 1.313147, current_train_items 17568.
I0302 18:58:32.924967 22535416901760 run.py:483] Algo bellman_ford step 549 current loss 1.174849, current_train_items 17600.
I0302 18:58:32.943267 22535416901760 run.py:483] Algo bellman_ford step 550 current loss 0.448291, current_train_items 17632.
I0302 18:58:32.951381 22535416901760 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.8486328125, 'score': 0.8486328125, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:32.951487 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.849, val scores are: bellman_ford: 0.849
I0302 18:58:32.968050 22535416901760 run.py:483] Algo bellman_ford step 551 current loss 0.610125, current_train_items 17664.
I0302 18:58:32.991375 22535416901760 run.py:483] Algo bellman_ford step 552 current loss 0.891462, current_train_items 17696.
I0302 18:58:33.022541 22535416901760 run.py:483] Algo bellman_ford step 553 current loss 1.242497, current_train_items 17728.
I0302 18:58:33.054011 22535416901760 run.py:483] Algo bellman_ford step 554 current loss 1.217975, current_train_items 17760.
I0302 18:58:33.072831 22535416901760 run.py:483] Algo bellman_ford step 555 current loss 0.435356, current_train_items 17792.
I0302 18:58:33.088487 22535416901760 run.py:483] Algo bellman_ford step 556 current loss 0.652355, current_train_items 17824.
I0302 18:58:33.112406 22535416901760 run.py:483] Algo bellman_ford step 557 current loss 1.264924, current_train_items 17856.
I0302 18:58:33.141046 22535416901760 run.py:483] Algo bellman_ford step 558 current loss 1.558617, current_train_items 17888.
I0302 18:58:33.174572 22535416901760 run.py:483] Algo bellman_ford step 559 current loss 1.626957, current_train_items 17920.
I0302 18:58:33.193283 22535416901760 run.py:483] Algo bellman_ford step 560 current loss 0.460694, current_train_items 17952.
I0302 18:58:33.209412 22535416901760 run.py:483] Algo bellman_ford step 561 current loss 0.561390, current_train_items 17984.
I0302 18:58:33.232834 22535416901760 run.py:483] Algo bellman_ford step 562 current loss 0.907186, current_train_items 18016.
I0302 18:58:33.261387 22535416901760 run.py:483] Algo bellman_ford step 563 current loss 0.992700, current_train_items 18048.
I0302 18:58:33.290965 22535416901760 run.py:483] Algo bellman_ford step 564 current loss 1.250530, current_train_items 18080.
I0302 18:58:33.309048 22535416901760 run.py:483] Algo bellman_ford step 565 current loss 0.390103, current_train_items 18112.
I0302 18:58:33.325307 22535416901760 run.py:483] Algo bellman_ford step 566 current loss 0.696949, current_train_items 18144.
I0302 18:58:33.349258 22535416901760 run.py:483] Algo bellman_ford step 567 current loss 1.176990, current_train_items 18176.
I0302 18:58:33.378366 22535416901760 run.py:483] Algo bellman_ford step 568 current loss 1.004423, current_train_items 18208.
I0302 18:58:33.408948 22535416901760 run.py:483] Algo bellman_ford step 569 current loss 1.213606, current_train_items 18240.
I0302 18:58:33.427490 22535416901760 run.py:483] Algo bellman_ford step 570 current loss 0.419433, current_train_items 18272.
I0302 18:58:33.443415 22535416901760 run.py:483] Algo bellman_ford step 571 current loss 0.710727, current_train_items 18304.
I0302 18:58:33.466837 22535416901760 run.py:483] Algo bellman_ford step 572 current loss 1.218377, current_train_items 18336.
I0302 18:58:33.497317 22535416901760 run.py:483] Algo bellman_ford step 573 current loss 1.200603, current_train_items 18368.
I0302 18:58:33.527734 22535416901760 run.py:483] Algo bellman_ford step 574 current loss 1.365896, current_train_items 18400.
I0302 18:58:33.546797 22535416901760 run.py:483] Algo bellman_ford step 575 current loss 0.439923, current_train_items 18432.
I0302 18:58:33.562697 22535416901760 run.py:483] Algo bellman_ford step 576 current loss 0.888916, current_train_items 18464.
I0302 18:58:33.585439 22535416901760 run.py:483] Algo bellman_ford step 577 current loss 1.036716, current_train_items 18496.
I0302 18:58:33.615197 22535416901760 run.py:483] Algo bellman_ford step 578 current loss 1.044119, current_train_items 18528.
I0302 18:58:33.648050 22535416901760 run.py:483] Algo bellman_ford step 579 current loss 1.436383, current_train_items 18560.
I0302 18:58:33.666853 22535416901760 run.py:483] Algo bellman_ford step 580 current loss 0.424961, current_train_items 18592.
I0302 18:58:33.682762 22535416901760 run.py:483] Algo bellman_ford step 581 current loss 0.668806, current_train_items 18624.
I0302 18:58:33.704322 22535416901760 run.py:483] Algo bellman_ford step 582 current loss 0.894215, current_train_items 18656.
I0302 18:58:33.732667 22535416901760 run.py:483] Algo bellman_ford step 583 current loss 0.892197, current_train_items 18688.
I0302 18:58:33.763994 22535416901760 run.py:483] Algo bellman_ford step 584 current loss 1.310567, current_train_items 18720.
I0302 18:58:33.782482 22535416901760 run.py:483] Algo bellman_ford step 585 current loss 0.362536, current_train_items 18752.
I0302 18:58:33.798806 22535416901760 run.py:483] Algo bellman_ford step 586 current loss 0.667644, current_train_items 18784.
I0302 18:58:33.820491 22535416901760 run.py:483] Algo bellman_ford step 587 current loss 0.731139, current_train_items 18816.
I0302 18:58:33.848919 22535416901760 run.py:483] Algo bellman_ford step 588 current loss 1.010430, current_train_items 18848.
I0302 18:58:33.881678 22535416901760 run.py:483] Algo bellman_ford step 589 current loss 1.494956, current_train_items 18880.
I0302 18:58:33.900714 22535416901760 run.py:483] Algo bellman_ford step 590 current loss 0.415152, current_train_items 18912.
I0302 18:58:33.916915 22535416901760 run.py:483] Algo bellman_ford step 591 current loss 0.723824, current_train_items 18944.
I0302 18:58:33.940281 22535416901760 run.py:483] Algo bellman_ford step 592 current loss 1.043826, current_train_items 18976.
I0302 18:58:33.969491 22535416901760 run.py:483] Algo bellman_ford step 593 current loss 1.180968, current_train_items 19008.
I0302 18:58:34.000399 22535416901760 run.py:483] Algo bellman_ford step 594 current loss 1.541426, current_train_items 19040.
I0302 18:58:34.018763 22535416901760 run.py:483] Algo bellman_ford step 595 current loss 0.410741, current_train_items 19072.
I0302 18:58:34.035023 22535416901760 run.py:483] Algo bellman_ford step 596 current loss 0.801558, current_train_items 19104.
I0302 18:58:34.058682 22535416901760 run.py:483] Algo bellman_ford step 597 current loss 1.067756, current_train_items 19136.
I0302 18:58:34.087882 22535416901760 run.py:483] Algo bellman_ford step 598 current loss 1.349929, current_train_items 19168.
I0302 18:58:34.119500 22535416901760 run.py:483] Algo bellman_ford step 599 current loss 1.350511, current_train_items 19200.
I0302 18:58:34.138178 22535416901760 run.py:483] Algo bellman_ford step 600 current loss 0.452615, current_train_items 19232.
I0302 18:58:34.146099 22535416901760 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.84375, 'score': 0.84375, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:34.146223 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.844, val scores are: bellman_ford: 0.844
I0302 18:58:34.162802 22535416901760 run.py:483] Algo bellman_ford step 601 current loss 0.578703, current_train_items 19264.
I0302 18:58:34.185957 22535416901760 run.py:483] Algo bellman_ford step 602 current loss 0.940229, current_train_items 19296.
I0302 18:58:34.214946 22535416901760 run.py:483] Algo bellman_ford step 603 current loss 1.088608, current_train_items 19328.
I0302 18:58:34.245799 22535416901760 run.py:483] Algo bellman_ford step 604 current loss 1.277473, current_train_items 19360.
I0302 18:58:34.264723 22535416901760 run.py:483] Algo bellman_ford step 605 current loss 0.420969, current_train_items 19392.
I0302 18:58:34.280075 22535416901760 run.py:483] Algo bellman_ford step 606 current loss 0.649895, current_train_items 19424.
I0302 18:58:34.303870 22535416901760 run.py:483] Algo bellman_ford step 607 current loss 1.008024, current_train_items 19456.
I0302 18:58:34.331808 22535416901760 run.py:483] Algo bellman_ford step 608 current loss 1.041639, current_train_items 19488.
I0302 18:58:34.363286 22535416901760 run.py:483] Algo bellman_ford step 609 current loss 1.338568, current_train_items 19520.
I0302 18:58:34.381665 22535416901760 run.py:483] Algo bellman_ford step 610 current loss 0.364654, current_train_items 19552.
I0302 18:58:34.397960 22535416901760 run.py:483] Algo bellman_ford step 611 current loss 0.689984, current_train_items 19584.
I0302 18:58:34.421177 22535416901760 run.py:483] Algo bellman_ford step 612 current loss 0.846638, current_train_items 19616.
I0302 18:58:34.448830 22535416901760 run.py:483] Algo bellman_ford step 613 current loss 1.069912, current_train_items 19648.
I0302 18:58:34.480001 22535416901760 run.py:483] Algo bellman_ford step 614 current loss 1.381454, current_train_items 19680.
I0302 18:58:34.498231 22535416901760 run.py:483] Algo bellman_ford step 615 current loss 0.302424, current_train_items 19712.
I0302 18:58:34.514607 22535416901760 run.py:483] Algo bellman_ford step 616 current loss 0.690122, current_train_items 19744.
I0302 18:58:34.537051 22535416901760 run.py:483] Algo bellman_ford step 617 current loss 0.981718, current_train_items 19776.
I0302 18:58:34.565569 22535416901760 run.py:483] Algo bellman_ford step 618 current loss 1.113541, current_train_items 19808.
I0302 18:58:34.596998 22535416901760 run.py:483] Algo bellman_ford step 619 current loss 1.236137, current_train_items 19840.
I0302 18:58:34.615815 22535416901760 run.py:483] Algo bellman_ford step 620 current loss 0.419660, current_train_items 19872.
I0302 18:58:34.632373 22535416901760 run.py:483] Algo bellman_ford step 621 current loss 0.861001, current_train_items 19904.
I0302 18:58:34.655610 22535416901760 run.py:483] Algo bellman_ford step 622 current loss 1.012421, current_train_items 19936.
I0302 18:58:34.684745 22535416901760 run.py:483] Algo bellman_ford step 623 current loss 1.322260, current_train_items 19968.
I0302 18:58:34.715576 22535416901760 run.py:483] Algo bellman_ford step 624 current loss 1.394713, current_train_items 20000.
I0302 18:58:34.733763 22535416901760 run.py:483] Algo bellman_ford step 625 current loss 0.478663, current_train_items 20032.
I0302 18:58:34.749591 22535416901760 run.py:483] Algo bellman_ford step 626 current loss 0.557403, current_train_items 20064.
I0302 18:58:34.773084 22535416901760 run.py:483] Algo bellman_ford step 627 current loss 0.973185, current_train_items 20096.
I0302 18:58:34.802840 22535416901760 run.py:483] Algo bellman_ford step 628 current loss 1.120771, current_train_items 20128.
I0302 18:58:34.834841 22535416901760 run.py:483] Algo bellman_ford step 629 current loss 1.255109, current_train_items 20160.
I0302 18:58:34.853132 22535416901760 run.py:483] Algo bellman_ford step 630 current loss 0.366306, current_train_items 20192.
I0302 18:58:34.868980 22535416901760 run.py:483] Algo bellman_ford step 631 current loss 0.600545, current_train_items 20224.
I0302 18:58:34.891578 22535416901760 run.py:483] Algo bellman_ford step 632 current loss 1.021024, current_train_items 20256.
I0302 18:58:34.920340 22535416901760 run.py:483] Algo bellman_ford step 633 current loss 1.038492, current_train_items 20288.
I0302 18:58:34.951063 22535416901760 run.py:483] Algo bellman_ford step 634 current loss 1.202757, current_train_items 20320.
I0302 18:58:34.969400 22535416901760 run.py:483] Algo bellman_ford step 635 current loss 0.444454, current_train_items 20352.
I0302 18:58:34.985414 22535416901760 run.py:483] Algo bellman_ford step 636 current loss 0.580081, current_train_items 20384.
I0302 18:58:35.008233 22535416901760 run.py:483] Algo bellman_ford step 637 current loss 0.924168, current_train_items 20416.
I0302 18:58:35.038083 22535416901760 run.py:483] Algo bellman_ford step 638 current loss 1.023203, current_train_items 20448.
I0302 18:58:35.066983 22535416901760 run.py:483] Algo bellman_ford step 639 current loss 1.138737, current_train_items 20480.
I0302 18:58:35.085418 22535416901760 run.py:483] Algo bellman_ford step 640 current loss 0.442633, current_train_items 20512.
I0302 18:58:35.101269 22535416901760 run.py:483] Algo bellman_ford step 641 current loss 0.670540, current_train_items 20544.
I0302 18:58:35.123060 22535416901760 run.py:483] Algo bellman_ford step 642 current loss 0.964469, current_train_items 20576.
I0302 18:58:35.151454 22535416901760 run.py:483] Algo bellman_ford step 643 current loss 1.077765, current_train_items 20608.
I0302 18:58:35.182728 22535416901760 run.py:483] Algo bellman_ford step 644 current loss 1.334180, current_train_items 20640.
I0302 18:58:35.200887 22535416901760 run.py:483] Algo bellman_ford step 645 current loss 0.392912, current_train_items 20672.
I0302 18:58:35.217823 22535416901760 run.py:483] Algo bellman_ford step 646 current loss 0.747020, current_train_items 20704.
I0302 18:58:35.239970 22535416901760 run.py:483] Algo bellman_ford step 647 current loss 0.856218, current_train_items 20736.
I0302 18:58:35.269794 22535416901760 run.py:483] Algo bellman_ford step 648 current loss 1.230772, current_train_items 20768.
I0302 18:58:35.301695 22535416901760 run.py:483] Algo bellman_ford step 649 current loss 1.233524, current_train_items 20800.
I0302 18:58:35.320200 22535416901760 run.py:483] Algo bellman_ford step 650 current loss 0.481993, current_train_items 20832.
I0302 18:58:35.328371 22535416901760 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.84765625, 'score': 0.84765625, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:35.328479 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.854, current avg val score is 0.848, val scores are: bellman_ford: 0.848
I0302 18:58:35.345438 22535416901760 run.py:483] Algo bellman_ford step 651 current loss 0.752248, current_train_items 20864.
I0302 18:58:35.368090 22535416901760 run.py:483] Algo bellman_ford step 652 current loss 0.886882, current_train_items 20896.
I0302 18:58:35.396609 22535416901760 run.py:483] Algo bellman_ford step 653 current loss 0.997631, current_train_items 20928.
I0302 18:58:35.427877 22535416901760 run.py:483] Algo bellman_ford step 654 current loss 1.222140, current_train_items 20960.
I0302 18:58:35.446579 22535416901760 run.py:483] Algo bellman_ford step 655 current loss 0.464795, current_train_items 20992.
I0302 18:58:35.462114 22535416901760 run.py:483] Algo bellman_ford step 656 current loss 0.758422, current_train_items 21024.
I0302 18:58:35.485374 22535416901760 run.py:483] Algo bellman_ford step 657 current loss 1.018718, current_train_items 21056.
I0302 18:58:35.514965 22535416901760 run.py:483] Algo bellman_ford step 658 current loss 1.169481, current_train_items 21088.
I0302 18:58:35.545300 22535416901760 run.py:483] Algo bellman_ford step 659 current loss 1.266513, current_train_items 21120.
I0302 18:58:35.564141 22535416901760 run.py:483] Algo bellman_ford step 660 current loss 0.430332, current_train_items 21152.
I0302 18:58:35.580135 22535416901760 run.py:483] Algo bellman_ford step 661 current loss 0.601000, current_train_items 21184.
I0302 18:58:35.602285 22535416901760 run.py:483] Algo bellman_ford step 662 current loss 0.880226, current_train_items 21216.
I0302 18:58:35.631140 22535416901760 run.py:483] Algo bellman_ford step 663 current loss 1.133684, current_train_items 21248.
I0302 18:58:35.664912 22535416901760 run.py:483] Algo bellman_ford step 664 current loss 1.384852, current_train_items 21280.
I0302 18:58:35.683843 22535416901760 run.py:483] Algo bellman_ford step 665 current loss 0.448941, current_train_items 21312.
I0302 18:58:35.699907 22535416901760 run.py:483] Algo bellman_ford step 666 current loss 0.647611, current_train_items 21344.
I0302 18:58:35.723498 22535416901760 run.py:483] Algo bellman_ford step 667 current loss 1.078783, current_train_items 21376.
I0302 18:58:35.752070 22535416901760 run.py:483] Algo bellman_ford step 668 current loss 1.326542, current_train_items 21408.
I0302 18:58:35.782824 22535416901760 run.py:483] Algo bellman_ford step 669 current loss 1.621228, current_train_items 21440.
I0302 18:58:35.801319 22535416901760 run.py:483] Algo bellman_ford step 670 current loss 0.348634, current_train_items 21472.
I0302 18:58:35.817125 22535416901760 run.py:483] Algo bellman_ford step 671 current loss 0.576652, current_train_items 21504.
I0302 18:58:35.840363 22535416901760 run.py:483] Algo bellman_ford step 672 current loss 0.925565, current_train_items 21536.
I0302 18:58:35.868889 22535416901760 run.py:483] Algo bellman_ford step 673 current loss 1.227691, current_train_items 21568.
I0302 18:58:35.901098 22535416901760 run.py:483] Algo bellman_ford step 674 current loss 1.446365, current_train_items 21600.
I0302 18:58:35.919947 22535416901760 run.py:483] Algo bellman_ford step 675 current loss 0.394680, current_train_items 21632.
I0302 18:58:35.935605 22535416901760 run.py:483] Algo bellman_ford step 676 current loss 0.543413, current_train_items 21664.
I0302 18:58:35.957966 22535416901760 run.py:483] Algo bellman_ford step 677 current loss 0.999738, current_train_items 21696.
I0302 18:58:35.987731 22535416901760 run.py:483] Algo bellman_ford step 678 current loss 1.079996, current_train_items 21728.
I0302 18:58:36.019947 22535416901760 run.py:483] Algo bellman_ford step 679 current loss 1.293383, current_train_items 21760.
I0302 18:58:36.038358 22535416901760 run.py:483] Algo bellman_ford step 680 current loss 0.449913, current_train_items 21792.
I0302 18:58:36.054431 22535416901760 run.py:483] Algo bellman_ford step 681 current loss 0.779472, current_train_items 21824.
I0302 18:58:36.077299 22535416901760 run.py:483] Algo bellman_ford step 682 current loss 0.957248, current_train_items 21856.
I0302 18:58:36.106163 22535416901760 run.py:483] Algo bellman_ford step 683 current loss 1.003027, current_train_items 21888.
I0302 18:58:36.138403 22535416901760 run.py:483] Algo bellman_ford step 684 current loss 1.424607, current_train_items 21920.
I0302 18:58:36.157059 22535416901760 run.py:483] Algo bellman_ford step 685 current loss 0.433865, current_train_items 21952.
I0302 18:58:36.172999 22535416901760 run.py:483] Algo bellman_ford step 686 current loss 0.599615, current_train_items 21984.
I0302 18:58:36.196039 22535416901760 run.py:483] Algo bellman_ford step 687 current loss 0.848943, current_train_items 22016.
I0302 18:58:36.223687 22535416901760 run.py:483] Algo bellman_ford step 688 current loss 0.903057, current_train_items 22048.
I0302 18:58:36.256882 22535416901760 run.py:483] Algo bellman_ford step 689 current loss 1.432173, current_train_items 22080.
I0302 18:58:36.275273 22535416901760 run.py:483] Algo bellman_ford step 690 current loss 0.470976, current_train_items 22112.
I0302 18:58:36.291800 22535416901760 run.py:483] Algo bellman_ford step 691 current loss 0.752563, current_train_items 22144.
I0302 18:58:36.315152 22535416901760 run.py:483] Algo bellman_ford step 692 current loss 0.945265, current_train_items 22176.
I0302 18:58:36.343480 22535416901760 run.py:483] Algo bellman_ford step 693 current loss 1.299256, current_train_items 22208.
I0302 18:58:36.376231 22535416901760 run.py:483] Algo bellman_ford step 694 current loss 1.361598, current_train_items 22240.
I0302 18:58:36.394729 22535416901760 run.py:483] Algo bellman_ford step 695 current loss 0.440244, current_train_items 22272.
I0302 18:58:36.410639 22535416901760 run.py:483] Algo bellman_ford step 696 current loss 0.703795, current_train_items 22304.
I0302 18:58:36.432192 22535416901760 run.py:483] Algo bellman_ford step 697 current loss 0.957215, current_train_items 22336.
I0302 18:58:36.461428 22535416901760 run.py:483] Algo bellman_ford step 698 current loss 1.045721, current_train_items 22368.
I0302 18:58:36.494187 22535416901760 run.py:483] Algo bellman_ford step 699 current loss 1.346168, current_train_items 22400.
I0302 18:58:36.512707 22535416901760 run.py:483] Algo bellman_ford step 700 current loss 0.373462, current_train_items 22432.
I0302 18:58:36.520488 22535416901760 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.8798828125, 'score': 0.8798828125, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:36.520592 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.854, current avg val score is 0.880, val scores are: bellman_ford: 0.880
I0302 18:58:36.551279 22535416901760 run.py:483] Algo bellman_ford step 701 current loss 0.525632, current_train_items 22464.
I0302 18:58:36.574351 22535416901760 run.py:483] Algo bellman_ford step 702 current loss 0.998843, current_train_items 22496.
I0302 18:58:36.602273 22535416901760 run.py:483] Algo bellman_ford step 703 current loss 1.068543, current_train_items 22528.
I0302 18:58:36.633579 22535416901760 run.py:483] Algo bellman_ford step 704 current loss 1.068522, current_train_items 22560.
I0302 18:58:36.653118 22535416901760 run.py:483] Algo bellman_ford step 705 current loss 0.411020, current_train_items 22592.
I0302 18:58:36.668910 22535416901760 run.py:483] Algo bellman_ford step 706 current loss 0.715702, current_train_items 22624.
I0302 18:58:36.692147 22535416901760 run.py:483] Algo bellman_ford step 707 current loss 0.781108, current_train_items 22656.
I0302 18:58:36.721336 22535416901760 run.py:483] Algo bellman_ford step 708 current loss 0.911430, current_train_items 22688.
I0302 18:58:36.753979 22535416901760 run.py:483] Algo bellman_ford step 709 current loss 1.262730, current_train_items 22720.
I0302 18:58:36.773006 22535416901760 run.py:483] Algo bellman_ford step 710 current loss 0.412450, current_train_items 22752.
I0302 18:58:36.789090 22535416901760 run.py:483] Algo bellman_ford step 711 current loss 0.669264, current_train_items 22784.
I0302 18:58:36.812881 22535416901760 run.py:483] Algo bellman_ford step 712 current loss 1.073751, current_train_items 22816.
I0302 18:58:36.843328 22535416901760 run.py:483] Algo bellman_ford step 713 current loss 1.213207, current_train_items 22848.
I0302 18:58:36.873278 22535416901760 run.py:483] Algo bellman_ford step 714 current loss 1.294776, current_train_items 22880.
I0302 18:58:36.891946 22535416901760 run.py:483] Algo bellman_ford step 715 current loss 0.368267, current_train_items 22912.
I0302 18:58:36.908402 22535416901760 run.py:483] Algo bellman_ford step 716 current loss 0.803254, current_train_items 22944.
I0302 18:58:36.932707 22535416901760 run.py:483] Algo bellman_ford step 717 current loss 1.062513, current_train_items 22976.
I0302 18:58:36.960956 22535416901760 run.py:483] Algo bellman_ford step 718 current loss 1.009682, current_train_items 23008.
I0302 18:58:36.992357 22535416901760 run.py:483] Algo bellman_ford step 719 current loss 1.264815, current_train_items 23040.
I0302 18:58:37.010984 22535416901760 run.py:483] Algo bellman_ford step 720 current loss 0.516564, current_train_items 23072.
I0302 18:58:37.027383 22535416901760 run.py:483] Algo bellman_ford step 721 current loss 0.741479, current_train_items 23104.
I0302 18:58:37.050004 22535416901760 run.py:483] Algo bellman_ford step 722 current loss 0.817938, current_train_items 23136.
I0302 18:58:37.078348 22535416901760 run.py:483] Algo bellman_ford step 723 current loss 0.982838, current_train_items 23168.
I0302 18:58:37.109029 22535416901760 run.py:483] Algo bellman_ford step 724 current loss 1.281863, current_train_items 23200.
I0302 18:58:37.127477 22535416901760 run.py:483] Algo bellman_ford step 725 current loss 0.369266, current_train_items 23232.
I0302 18:58:37.144059 22535416901760 run.py:483] Algo bellman_ford step 726 current loss 0.693571, current_train_items 23264.
I0302 18:58:37.167547 22535416901760 run.py:483] Algo bellman_ford step 727 current loss 1.288789, current_train_items 23296.
I0302 18:58:37.196656 22535416901760 run.py:483] Algo bellman_ford step 728 current loss 1.221141, current_train_items 23328.
I0302 18:58:37.228504 22535416901760 run.py:483] Algo bellman_ford step 729 current loss 1.423040, current_train_items 23360.
I0302 18:58:37.246841 22535416901760 run.py:483] Algo bellman_ford step 730 current loss 0.356558, current_train_items 23392.
I0302 18:58:37.262908 22535416901760 run.py:483] Algo bellman_ford step 731 current loss 0.693956, current_train_items 23424.
I0302 18:58:37.286365 22535416901760 run.py:483] Algo bellman_ford step 732 current loss 0.973279, current_train_items 23456.
I0302 18:58:37.315355 22535416901760 run.py:483] Algo bellman_ford step 733 current loss 1.112191, current_train_items 23488.
I0302 18:58:37.347818 22535416901760 run.py:483] Algo bellman_ford step 734 current loss 1.234494, current_train_items 23520.
I0302 18:58:37.366089 22535416901760 run.py:483] Algo bellman_ford step 735 current loss 0.429286, current_train_items 23552.
I0302 18:58:37.382576 22535416901760 run.py:483] Algo bellman_ford step 736 current loss 0.603193, current_train_items 23584.
I0302 18:58:37.405305 22535416901760 run.py:483] Algo bellman_ford step 737 current loss 0.960120, current_train_items 23616.
I0302 18:58:37.434573 22535416901760 run.py:483] Algo bellman_ford step 738 current loss 1.013730, current_train_items 23648.
I0302 18:58:37.464443 22535416901760 run.py:483] Algo bellman_ford step 739 current loss 1.121529, current_train_items 23680.
I0302 18:58:37.483122 22535416901760 run.py:483] Algo bellman_ford step 740 current loss 0.512319, current_train_items 23712.
I0302 18:58:37.499330 22535416901760 run.py:483] Algo bellman_ford step 741 current loss 0.707644, current_train_items 23744.
I0302 18:58:37.522092 22535416901760 run.py:483] Algo bellman_ford step 742 current loss 0.951862, current_train_items 23776.
I0302 18:58:37.550572 22535416901760 run.py:483] Algo bellman_ford step 743 current loss 0.989027, current_train_items 23808.
I0302 18:58:37.584045 22535416901760 run.py:483] Algo bellman_ford step 744 current loss 1.457874, current_train_items 23840.
I0302 18:58:37.602755 22535416901760 run.py:483] Algo bellman_ford step 745 current loss 0.403558, current_train_items 23872.
I0302 18:58:37.618768 22535416901760 run.py:483] Algo bellman_ford step 746 current loss 0.742159, current_train_items 23904.
I0302 18:58:37.641568 22535416901760 run.py:483] Algo bellman_ford step 747 current loss 1.039713, current_train_items 23936.
I0302 18:58:37.671219 22535416901760 run.py:483] Algo bellman_ford step 748 current loss 1.625345, current_train_items 23968.
I0302 18:58:37.702261 22535416901760 run.py:483] Algo bellman_ford step 749 current loss 1.578331, current_train_items 24000.
I0302 18:58:37.720642 22535416901760 run.py:483] Algo bellman_ford step 750 current loss 0.409854, current_train_items 24032.
I0302 18:58:37.728672 22535416901760 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.80859375, 'score': 0.80859375, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:37.728781 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.809, val scores are: bellman_ford: 0.809
I0302 18:58:37.745919 22535416901760 run.py:483] Algo bellman_ford step 751 current loss 0.857352, current_train_items 24064.
I0302 18:58:37.768653 22535416901760 run.py:483] Algo bellman_ford step 752 current loss 0.848750, current_train_items 24096.
I0302 18:58:37.797392 22535416901760 run.py:483] Algo bellman_ford step 753 current loss 1.029434, current_train_items 24128.
I0302 18:58:37.829755 22535416901760 run.py:483] Algo bellman_ford step 754 current loss 1.179233, current_train_items 24160.
I0302 18:58:37.849106 22535416901760 run.py:483] Algo bellman_ford step 755 current loss 0.398599, current_train_items 24192.
I0302 18:58:37.864701 22535416901760 run.py:483] Algo bellman_ford step 756 current loss 0.627276, current_train_items 24224.
I0302 18:58:37.888699 22535416901760 run.py:483] Algo bellman_ford step 757 current loss 0.869227, current_train_items 24256.
I0302 18:58:37.917459 22535416901760 run.py:483] Algo bellman_ford step 758 current loss 1.104240, current_train_items 24288.
I0302 18:58:37.948501 22535416901760 run.py:483] Algo bellman_ford step 759 current loss 1.342853, current_train_items 24320.
I0302 18:58:37.967443 22535416901760 run.py:483] Algo bellman_ford step 760 current loss 0.334628, current_train_items 24352.
I0302 18:58:37.983451 22535416901760 run.py:483] Algo bellman_ford step 761 current loss 0.717341, current_train_items 24384.
I0302 18:58:38.006587 22535416901760 run.py:483] Algo bellman_ford step 762 current loss 0.976507, current_train_items 24416.
I0302 18:58:38.034847 22535416901760 run.py:483] Algo bellman_ford step 763 current loss 0.886449, current_train_items 24448.
I0302 18:58:38.066974 22535416901760 run.py:483] Algo bellman_ford step 764 current loss 1.220385, current_train_items 24480.
I0302 18:58:38.085686 22535416901760 run.py:483] Algo bellman_ford step 765 current loss 0.393049, current_train_items 24512.
I0302 18:58:38.102221 22535416901760 run.py:483] Algo bellman_ford step 766 current loss 0.723761, current_train_items 24544.
I0302 18:58:38.125522 22535416901760 run.py:483] Algo bellman_ford step 767 current loss 0.957187, current_train_items 24576.
I0302 18:58:38.154777 22535416901760 run.py:483] Algo bellman_ford step 768 current loss 1.094416, current_train_items 24608.
I0302 18:58:38.187308 22535416901760 run.py:483] Algo bellman_ford step 769 current loss 1.107898, current_train_items 24640.
I0302 18:58:38.206097 22535416901760 run.py:483] Algo bellman_ford step 770 current loss 0.345768, current_train_items 24672.
I0302 18:58:38.222335 22535416901760 run.py:483] Algo bellman_ford step 771 current loss 0.702227, current_train_items 24704.
I0302 18:58:38.245015 22535416901760 run.py:483] Algo bellman_ford step 772 current loss 0.985711, current_train_items 24736.
I0302 18:58:38.273947 22535416901760 run.py:483] Algo bellman_ford step 773 current loss 0.888613, current_train_items 24768.
I0302 18:58:38.306297 22535416901760 run.py:483] Algo bellman_ford step 774 current loss 1.436171, current_train_items 24800.
I0302 18:58:38.325350 22535416901760 run.py:483] Algo bellman_ford step 775 current loss 0.346225, current_train_items 24832.
I0302 18:58:38.342094 22535416901760 run.py:483] Algo bellman_ford step 776 current loss 0.657694, current_train_items 24864.
I0302 18:58:38.365663 22535416901760 run.py:483] Algo bellman_ford step 777 current loss 0.974867, current_train_items 24896.
I0302 18:58:38.394328 22535416901760 run.py:483] Algo bellman_ford step 778 current loss 0.939563, current_train_items 24928.
I0302 18:58:38.425451 22535416901760 run.py:483] Algo bellman_ford step 779 current loss 1.051649, current_train_items 24960.
I0302 18:58:38.443845 22535416901760 run.py:483] Algo bellman_ford step 780 current loss 0.455367, current_train_items 24992.
I0302 18:58:38.460092 22535416901760 run.py:483] Algo bellman_ford step 781 current loss 0.754537, current_train_items 25024.
I0302 18:58:38.482870 22535416901760 run.py:483] Algo bellman_ford step 782 current loss 0.902446, current_train_items 25056.
I0302 18:58:38.512226 22535416901760 run.py:483] Algo bellman_ford step 783 current loss 1.074129, current_train_items 25088.
I0302 18:58:38.545051 22535416901760 run.py:483] Algo bellman_ford step 784 current loss 1.311466, current_train_items 25120.
I0302 18:58:38.563567 22535416901760 run.py:483] Algo bellman_ford step 785 current loss 0.389945, current_train_items 25152.
I0302 18:58:38.580044 22535416901760 run.py:483] Algo bellman_ford step 786 current loss 0.699392, current_train_items 25184.
I0302 18:58:38.602751 22535416901760 run.py:483] Algo bellman_ford step 787 current loss 0.924253, current_train_items 25216.
I0302 18:58:38.632208 22535416901760 run.py:483] Algo bellman_ford step 788 current loss 1.006588, current_train_items 25248.
I0302 18:58:38.661656 22535416901760 run.py:483] Algo bellman_ford step 789 current loss 1.155400, current_train_items 25280.
I0302 18:58:38.680793 22535416901760 run.py:483] Algo bellman_ford step 790 current loss 0.373810, current_train_items 25312.
I0302 18:58:38.697309 22535416901760 run.py:483] Algo bellman_ford step 791 current loss 0.797442, current_train_items 25344.
I0302 18:58:38.718926 22535416901760 run.py:483] Algo bellman_ford step 792 current loss 0.720492, current_train_items 25376.
I0302 18:58:38.747859 22535416901760 run.py:483] Algo bellman_ford step 793 current loss 0.935530, current_train_items 25408.
I0302 18:58:38.778252 22535416901760 run.py:483] Algo bellman_ford step 794 current loss 1.048739, current_train_items 25440.
I0302 18:58:38.796722 22535416901760 run.py:483] Algo bellman_ford step 795 current loss 0.424523, current_train_items 25472.
I0302 18:58:38.812553 22535416901760 run.py:483] Algo bellman_ford step 796 current loss 0.567522, current_train_items 25504.
I0302 18:58:38.836050 22535416901760 run.py:483] Algo bellman_ford step 797 current loss 0.877733, current_train_items 25536.
I0302 18:58:38.864346 22535416901760 run.py:483] Algo bellman_ford step 798 current loss 1.059424, current_train_items 25568.
I0302 18:58:38.895300 22535416901760 run.py:483] Algo bellman_ford step 799 current loss 1.295614, current_train_items 25600.
I0302 18:58:38.914037 22535416901760 run.py:483] Algo bellman_ford step 800 current loss 0.271850, current_train_items 25632.
I0302 18:58:38.921665 22535416901760 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.837890625, 'score': 0.837890625, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:38.921777 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.838, val scores are: bellman_ford: 0.838
I0302 18:58:38.938593 22535416901760 run.py:483] Algo bellman_ford step 801 current loss 0.749186, current_train_items 25664.
I0302 18:58:38.962923 22535416901760 run.py:483] Algo bellman_ford step 802 current loss 1.226043, current_train_items 25696.
I0302 18:58:38.992420 22535416901760 run.py:483] Algo bellman_ford step 803 current loss 1.227520, current_train_items 25728.
I0302 18:58:39.023614 22535416901760 run.py:483] Algo bellman_ford step 804 current loss 1.121434, current_train_items 25760.
I0302 18:58:39.042647 22535416901760 run.py:483] Algo bellman_ford step 805 current loss 0.378847, current_train_items 25792.
I0302 18:58:39.058372 22535416901760 run.py:483] Algo bellman_ford step 806 current loss 0.696630, current_train_items 25824.
I0302 18:58:39.081796 22535416901760 run.py:483] Algo bellman_ford step 807 current loss 1.159296, current_train_items 25856.
I0302 18:58:39.110566 22535416901760 run.py:483] Algo bellman_ford step 808 current loss 1.085084, current_train_items 25888.
I0302 18:58:39.141008 22535416901760 run.py:483] Algo bellman_ford step 809 current loss 1.420289, current_train_items 25920.
I0302 18:58:39.159483 22535416901760 run.py:483] Algo bellman_ford step 810 current loss 0.470356, current_train_items 25952.
I0302 18:58:39.176115 22535416901760 run.py:483] Algo bellman_ford step 811 current loss 0.893971, current_train_items 25984.
I0302 18:58:39.198295 22535416901760 run.py:483] Algo bellman_ford step 812 current loss 0.813506, current_train_items 26016.
I0302 18:58:39.226422 22535416901760 run.py:483] Algo bellman_ford step 813 current loss 1.209166, current_train_items 26048.
I0302 18:58:39.257743 22535416901760 run.py:483] Algo bellman_ford step 814 current loss 1.703870, current_train_items 26080.
I0302 18:58:39.276364 22535416901760 run.py:483] Algo bellman_ford step 815 current loss 0.464776, current_train_items 26112.
I0302 18:58:39.292725 22535416901760 run.py:483] Algo bellman_ford step 816 current loss 0.854913, current_train_items 26144.
I0302 18:58:39.315855 22535416901760 run.py:483] Algo bellman_ford step 817 current loss 0.916281, current_train_items 26176.
I0302 18:58:39.345783 22535416901760 run.py:483] Algo bellman_ford step 818 current loss 1.119709, current_train_items 26208.
I0302 18:58:39.379168 22535416901760 run.py:483] Algo bellman_ford step 819 current loss 1.399033, current_train_items 26240.
I0302 18:58:39.397696 22535416901760 run.py:483] Algo bellman_ford step 820 current loss 0.464306, current_train_items 26272.
I0302 18:58:39.413489 22535416901760 run.py:483] Algo bellman_ford step 821 current loss 0.636916, current_train_items 26304.
I0302 18:58:39.435893 22535416901760 run.py:483] Algo bellman_ford step 822 current loss 0.808480, current_train_items 26336.
I0302 18:58:39.464912 22535416901760 run.py:483] Algo bellman_ford step 823 current loss 1.013505, current_train_items 26368.
I0302 18:58:39.495390 22535416901760 run.py:483] Algo bellman_ford step 824 current loss 1.308843, current_train_items 26400.
I0302 18:58:39.513843 22535416901760 run.py:483] Algo bellman_ford step 825 current loss 0.399611, current_train_items 26432.
I0302 18:58:39.530246 22535416901760 run.py:483] Algo bellman_ford step 826 current loss 0.772559, current_train_items 26464.
I0302 18:58:39.553818 22535416901760 run.py:483] Algo bellman_ford step 827 current loss 0.973570, current_train_items 26496.
I0302 18:58:39.582829 22535416901760 run.py:483] Algo bellman_ford step 828 current loss 1.179872, current_train_items 26528.
I0302 18:58:39.613999 22535416901760 run.py:483] Algo bellman_ford step 829 current loss 1.171675, current_train_items 26560.
I0302 18:58:39.632148 22535416901760 run.py:483] Algo bellman_ford step 830 current loss 0.360706, current_train_items 26592.
I0302 18:58:39.648111 22535416901760 run.py:483] Algo bellman_ford step 831 current loss 0.650276, current_train_items 26624.
I0302 18:58:39.671236 22535416901760 run.py:483] Algo bellman_ford step 832 current loss 1.034137, current_train_items 26656.
I0302 18:58:39.700160 22535416901760 run.py:483] Algo bellman_ford step 833 current loss 1.139237, current_train_items 26688.
I0302 18:58:39.730277 22535416901760 run.py:483] Algo bellman_ford step 834 current loss 1.251550, current_train_items 26720.
I0302 18:58:39.748619 22535416901760 run.py:483] Algo bellman_ford step 835 current loss 0.409966, current_train_items 26752.
I0302 18:58:39.764619 22535416901760 run.py:483] Algo bellman_ford step 836 current loss 0.589723, current_train_items 26784.
I0302 18:58:39.787754 22535416901760 run.py:483] Algo bellman_ford step 837 current loss 1.000637, current_train_items 26816.
I0302 18:58:39.816717 22535416901760 run.py:483] Algo bellman_ford step 838 current loss 1.018928, current_train_items 26848.
I0302 18:58:39.849070 22535416901760 run.py:483] Algo bellman_ford step 839 current loss 1.346267, current_train_items 26880.
I0302 18:58:39.867002 22535416901760 run.py:483] Algo bellman_ford step 840 current loss 0.491027, current_train_items 26912.
I0302 18:58:39.882895 22535416901760 run.py:483] Algo bellman_ford step 841 current loss 0.696706, current_train_items 26944.
I0302 18:58:39.905858 22535416901760 run.py:483] Algo bellman_ford step 842 current loss 1.009328, current_train_items 26976.
I0302 18:58:39.933526 22535416901760 run.py:483] Algo bellman_ford step 843 current loss 0.953296, current_train_items 27008.
I0302 18:58:39.963569 22535416901760 run.py:483] Algo bellman_ford step 844 current loss 1.171340, current_train_items 27040.
I0302 18:58:39.981690 22535416901760 run.py:483] Algo bellman_ford step 845 current loss 0.536703, current_train_items 27072.
I0302 18:58:39.997821 22535416901760 run.py:483] Algo bellman_ford step 846 current loss 0.722770, current_train_items 27104.
I0302 18:58:40.021910 22535416901760 run.py:483] Algo bellman_ford step 847 current loss 1.004630, current_train_items 27136.
I0302 18:58:40.050203 22535416901760 run.py:483] Algo bellman_ford step 848 current loss 1.002211, current_train_items 27168.
I0302 18:58:40.082054 22535416901760 run.py:483] Algo bellman_ford step 849 current loss 1.508528, current_train_items 27200.
I0302 18:58:40.100151 22535416901760 run.py:483] Algo bellman_ford step 850 current loss 0.403784, current_train_items 27232.
I0302 18:58:40.108088 22535416901760 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.81640625, 'score': 0.81640625, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:40.108202 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.816, val scores are: bellman_ford: 0.816
I0302 18:58:40.124476 22535416901760 run.py:483] Algo bellman_ford step 851 current loss 0.585610, current_train_items 27264.
I0302 18:58:40.147862 22535416901760 run.py:483] Algo bellman_ford step 852 current loss 0.903187, current_train_items 27296.
I0302 18:58:40.178311 22535416901760 run.py:483] Algo bellman_ford step 853 current loss 1.128772, current_train_items 27328.
I0302 18:58:40.209123 22535416901760 run.py:483] Algo bellman_ford step 854 current loss 1.248905, current_train_items 27360.
I0302 18:58:40.227934 22535416901760 run.py:483] Algo bellman_ford step 855 current loss 0.434040, current_train_items 27392.
I0302 18:58:40.243905 22535416901760 run.py:483] Algo bellman_ford step 856 current loss 0.638274, current_train_items 27424.
I0302 18:58:40.265932 22535416901760 run.py:483] Algo bellman_ford step 857 current loss 0.872763, current_train_items 27456.
I0302 18:58:40.295791 22535416901760 run.py:483] Algo bellman_ford step 858 current loss 1.076166, current_train_items 27488.
I0302 18:58:40.326642 22535416901760 run.py:483] Algo bellman_ford step 859 current loss 1.136666, current_train_items 27520.
I0302 18:58:40.345337 22535416901760 run.py:483] Algo bellman_ford step 860 current loss 0.416778, current_train_items 27552.
I0302 18:58:40.361763 22535416901760 run.py:483] Algo bellman_ford step 861 current loss 0.739734, current_train_items 27584.
I0302 18:58:40.384397 22535416901760 run.py:483] Algo bellman_ford step 862 current loss 0.892082, current_train_items 27616.
I0302 18:58:40.413055 22535416901760 run.py:483] Algo bellman_ford step 863 current loss 1.066519, current_train_items 27648.
I0302 18:58:40.443719 22535416901760 run.py:483] Algo bellman_ford step 864 current loss 1.409376, current_train_items 27680.
I0302 18:58:40.462093 22535416901760 run.py:483] Algo bellman_ford step 865 current loss 0.376853, current_train_items 27712.
I0302 18:58:40.477859 22535416901760 run.py:483] Algo bellman_ford step 866 current loss 0.631939, current_train_items 27744.
I0302 18:58:40.501473 22535416901760 run.py:483] Algo bellman_ford step 867 current loss 1.162566, current_train_items 27776.
I0302 18:58:40.530176 22535416901760 run.py:483] Algo bellman_ford step 868 current loss 0.928809, current_train_items 27808.
I0302 18:58:40.562796 22535416901760 run.py:483] Algo bellman_ford step 869 current loss 1.244639, current_train_items 27840.
I0302 18:58:40.581394 22535416901760 run.py:483] Algo bellman_ford step 870 current loss 0.369242, current_train_items 27872.
I0302 18:58:40.597707 22535416901760 run.py:483] Algo bellman_ford step 871 current loss 0.627457, current_train_items 27904.
I0302 18:58:40.619565 22535416901760 run.py:483] Algo bellman_ford step 872 current loss 0.760167, current_train_items 27936.
I0302 18:58:40.649045 22535416901760 run.py:483] Algo bellman_ford step 873 current loss 1.087474, current_train_items 27968.
I0302 18:58:40.679471 22535416901760 run.py:483] Algo bellman_ford step 874 current loss 1.155629, current_train_items 28000.
I0302 18:58:40.698116 22535416901760 run.py:483] Algo bellman_ford step 875 current loss 0.386743, current_train_items 28032.
I0302 18:58:40.714561 22535416901760 run.py:483] Algo bellman_ford step 876 current loss 0.644145, current_train_items 28064.
I0302 18:58:40.739311 22535416901760 run.py:483] Algo bellman_ford step 877 current loss 1.120018, current_train_items 28096.
I0302 18:58:40.767729 22535416901760 run.py:483] Algo bellman_ford step 878 current loss 0.938053, current_train_items 28128.
I0302 18:58:40.800061 22535416901760 run.py:483] Algo bellman_ford step 879 current loss 1.253256, current_train_items 28160.
I0302 18:58:40.818470 22535416901760 run.py:483] Algo bellman_ford step 880 current loss 0.365886, current_train_items 28192.
I0302 18:58:40.834384 22535416901760 run.py:483] Algo bellman_ford step 881 current loss 0.602356, current_train_items 28224.
I0302 18:58:40.856626 22535416901760 run.py:483] Algo bellman_ford step 882 current loss 0.756125, current_train_items 28256.
I0302 18:58:40.885872 22535416901760 run.py:483] Algo bellman_ford step 883 current loss 1.030042, current_train_items 28288.
I0302 18:58:40.918256 22535416901760 run.py:483] Algo bellman_ford step 884 current loss 1.313164, current_train_items 28320.
I0302 18:58:40.937036 22535416901760 run.py:483] Algo bellman_ford step 885 current loss 0.329720, current_train_items 28352.
I0302 18:58:40.953071 22535416901760 run.py:483] Algo bellman_ford step 886 current loss 0.729932, current_train_items 28384.
I0302 18:58:40.976121 22535416901760 run.py:483] Algo bellman_ford step 887 current loss 0.982261, current_train_items 28416.
I0302 18:58:41.004542 22535416901760 run.py:483] Algo bellman_ford step 888 current loss 1.102838, current_train_items 28448.
I0302 18:58:41.037336 22535416901760 run.py:483] Algo bellman_ford step 889 current loss 1.288619, current_train_items 28480.
I0302 18:58:41.056105 22535416901760 run.py:483] Algo bellman_ford step 890 current loss 0.440542, current_train_items 28512.
I0302 18:58:41.072377 22535416901760 run.py:483] Algo bellman_ford step 891 current loss 0.589471, current_train_items 28544.
I0302 18:58:41.095287 22535416901760 run.py:483] Algo bellman_ford step 892 current loss 1.112731, current_train_items 28576.
I0302 18:58:41.125340 22535416901760 run.py:483] Algo bellman_ford step 893 current loss 1.054353, current_train_items 28608.
I0302 18:58:41.156339 22535416901760 run.py:483] Algo bellman_ford step 894 current loss 1.185071, current_train_items 28640.
I0302 18:58:41.175022 22535416901760 run.py:483] Algo bellman_ford step 895 current loss 0.390112, current_train_items 28672.
I0302 18:58:41.190837 22535416901760 run.py:483] Algo bellman_ford step 896 current loss 0.528121, current_train_items 28704.
I0302 18:58:41.214053 22535416901760 run.py:483] Algo bellman_ford step 897 current loss 1.026140, current_train_items 28736.
I0302 18:58:41.243714 22535416901760 run.py:483] Algo bellman_ford step 898 current loss 1.058524, current_train_items 28768.
I0302 18:58:41.277087 22535416901760 run.py:483] Algo bellman_ford step 899 current loss 1.432409, current_train_items 28800.
I0302 18:58:41.295633 22535416901760 run.py:483] Algo bellman_ford step 900 current loss 0.478351, current_train_items 28832.
I0302 18:58:41.303414 22535416901760 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.8583984375, 'score': 0.8583984375, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:41.303522 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.858, val scores are: bellman_ford: 0.858
I0302 18:58:41.320703 22535416901760 run.py:483] Algo bellman_ford step 901 current loss 0.700576, current_train_items 28864.
I0302 18:58:41.344549 22535416901760 run.py:483] Algo bellman_ford step 902 current loss 0.922258, current_train_items 28896.
I0302 18:58:41.375036 22535416901760 run.py:483] Algo bellman_ford step 903 current loss 0.992477, current_train_items 28928.
I0302 18:58:41.406870 22535416901760 run.py:483] Algo bellman_ford step 904 current loss 1.189194, current_train_items 28960.
I0302 18:58:41.425701 22535416901760 run.py:483] Algo bellman_ford step 905 current loss 0.313774, current_train_items 28992.
I0302 18:58:41.441947 22535416901760 run.py:483] Algo bellman_ford step 906 current loss 0.791881, current_train_items 29024.
I0302 18:58:41.464990 22535416901760 run.py:483] Algo bellman_ford step 907 current loss 0.871639, current_train_items 29056.
I0302 18:58:41.494890 22535416901760 run.py:483] Algo bellman_ford step 908 current loss 1.208142, current_train_items 29088.
I0302 18:58:41.526799 22535416901760 run.py:483] Algo bellman_ford step 909 current loss 1.242759, current_train_items 29120.
I0302 18:58:41.544911 22535416901760 run.py:483] Algo bellman_ford step 910 current loss 0.368467, current_train_items 29152.
I0302 18:58:41.561535 22535416901760 run.py:483] Algo bellman_ford step 911 current loss 0.755610, current_train_items 29184.
I0302 18:58:41.584915 22535416901760 run.py:483] Algo bellman_ford step 912 current loss 1.015442, current_train_items 29216.
I0302 18:58:41.612726 22535416901760 run.py:483] Algo bellman_ford step 913 current loss 0.931709, current_train_items 29248.
I0302 18:58:41.644889 22535416901760 run.py:483] Algo bellman_ford step 914 current loss 1.273784, current_train_items 29280.
I0302 18:58:41.662935 22535416901760 run.py:483] Algo bellman_ford step 915 current loss 0.355144, current_train_items 29312.
I0302 18:58:41.679333 22535416901760 run.py:483] Algo bellman_ford step 916 current loss 0.578596, current_train_items 29344.
I0302 18:58:41.702553 22535416901760 run.py:483] Algo bellman_ford step 917 current loss 0.959625, current_train_items 29376.
I0302 18:58:41.732068 22535416901760 run.py:483] Algo bellman_ford step 918 current loss 1.104171, current_train_items 29408.
I0302 18:58:41.764644 22535416901760 run.py:483] Algo bellman_ford step 919 current loss 1.351218, current_train_items 29440.
I0302 18:58:41.783265 22535416901760 run.py:483] Algo bellman_ford step 920 current loss 0.364959, current_train_items 29472.
I0302 18:58:41.799677 22535416901760 run.py:483] Algo bellman_ford step 921 current loss 0.678367, current_train_items 29504.
I0302 18:58:41.822961 22535416901760 run.py:483] Algo bellman_ford step 922 current loss 0.899069, current_train_items 29536.
I0302 18:58:41.851359 22535416901760 run.py:483] Algo bellman_ford step 923 current loss 0.891488, current_train_items 29568.
I0302 18:58:41.882458 22535416901760 run.py:483] Algo bellman_ford step 924 current loss 1.172830, current_train_items 29600.
I0302 18:58:41.900725 22535416901760 run.py:483] Algo bellman_ford step 925 current loss 0.421506, current_train_items 29632.
I0302 18:58:41.916880 22535416901760 run.py:483] Algo bellman_ford step 926 current loss 0.594517, current_train_items 29664.
I0302 18:58:41.940032 22535416901760 run.py:483] Algo bellman_ford step 927 current loss 0.949120, current_train_items 29696.
I0302 18:58:41.969853 22535416901760 run.py:483] Algo bellman_ford step 928 current loss 1.035328, current_train_items 29728.
I0302 18:58:42.000066 22535416901760 run.py:483] Algo bellman_ford step 929 current loss 1.067951, current_train_items 29760.
I0302 18:58:42.018728 22535416901760 run.py:483] Algo bellman_ford step 930 current loss 0.412249, current_train_items 29792.
I0302 18:58:42.034846 22535416901760 run.py:483] Algo bellman_ford step 931 current loss 0.742075, current_train_items 29824.
I0302 18:58:42.058189 22535416901760 run.py:483] Algo bellman_ford step 932 current loss 0.903876, current_train_items 29856.
I0302 18:58:42.086299 22535416901760 run.py:483] Algo bellman_ford step 933 current loss 0.975361, current_train_items 29888.
I0302 18:58:42.118290 22535416901760 run.py:483] Algo bellman_ford step 934 current loss 1.241122, current_train_items 29920.
I0302 18:58:42.136363 22535416901760 run.py:483] Algo bellman_ford step 935 current loss 0.417439, current_train_items 29952.
I0302 18:58:42.152385 22535416901760 run.py:483] Algo bellman_ford step 936 current loss 0.652611, current_train_items 29984.
I0302 18:58:42.174814 22535416901760 run.py:483] Algo bellman_ford step 937 current loss 0.797431, current_train_items 30016.
I0302 18:58:42.203277 22535416901760 run.py:483] Algo bellman_ford step 938 current loss 1.158571, current_train_items 30048.
I0302 18:58:42.234841 22535416901760 run.py:483] Algo bellman_ford step 939 current loss 1.573771, current_train_items 30080.
I0302 18:58:42.252953 22535416901760 run.py:483] Algo bellman_ford step 940 current loss 0.381963, current_train_items 30112.
I0302 18:58:42.269201 22535416901760 run.py:483] Algo bellman_ford step 941 current loss 0.764542, current_train_items 30144.
I0302 18:58:42.292965 22535416901760 run.py:483] Algo bellman_ford step 942 current loss 0.996997, current_train_items 30176.
I0302 18:58:42.322198 22535416901760 run.py:483] Algo bellman_ford step 943 current loss 0.986419, current_train_items 30208.
I0302 18:58:42.354417 22535416901760 run.py:483] Algo bellman_ford step 944 current loss 1.350173, current_train_items 30240.
I0302 18:58:42.372663 22535416901760 run.py:483] Algo bellman_ford step 945 current loss 0.490521, current_train_items 30272.
I0302 18:58:42.388330 22535416901760 run.py:483] Algo bellman_ford step 946 current loss 0.704736, current_train_items 30304.
I0302 18:58:42.412472 22535416901760 run.py:483] Algo bellman_ford step 947 current loss 1.004099, current_train_items 30336.
I0302 18:58:42.441044 22535416901760 run.py:483] Algo bellman_ford step 948 current loss 1.035217, current_train_items 30368.
I0302 18:58:42.472363 22535416901760 run.py:483] Algo bellman_ford step 949 current loss 1.361222, current_train_items 30400.
I0302 18:58:42.490999 22535416901760 run.py:483] Algo bellman_ford step 950 current loss 0.446957, current_train_items 30432.
I0302 18:58:42.498915 22535416901760 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.8505859375, 'score': 0.8505859375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:42.499024 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.851, val scores are: bellman_ford: 0.851
I0302 18:58:42.515877 22535416901760 run.py:483] Algo bellman_ford step 951 current loss 0.608409, current_train_items 30464.
I0302 18:58:42.539169 22535416901760 run.py:483] Algo bellman_ford step 952 current loss 0.897719, current_train_items 30496.
I0302 18:58:42.569221 22535416901760 run.py:483] Algo bellman_ford step 953 current loss 1.214554, current_train_items 30528.
I0302 18:58:42.598759 22535416901760 run.py:483] Algo bellman_ford step 954 current loss 1.063067, current_train_items 30560.
I0302 18:58:42.617615 22535416901760 run.py:483] Algo bellman_ford step 955 current loss 0.346163, current_train_items 30592.
I0302 18:58:42.633662 22535416901760 run.py:483] Algo bellman_ford step 956 current loss 0.660300, current_train_items 30624.
I0302 18:58:42.657060 22535416901760 run.py:483] Algo bellman_ford step 957 current loss 1.004500, current_train_items 30656.
I0302 18:58:42.684865 22535416901760 run.py:483] Algo bellman_ford step 958 current loss 0.990095, current_train_items 30688.
I0302 18:58:42.716541 22535416901760 run.py:483] Algo bellman_ford step 959 current loss 1.252477, current_train_items 30720.
I0302 18:58:42.735458 22535416901760 run.py:483] Algo bellman_ford step 960 current loss 0.345228, current_train_items 30752.
I0302 18:58:42.752283 22535416901760 run.py:483] Algo bellman_ford step 961 current loss 0.806938, current_train_items 30784.
I0302 18:58:42.773695 22535416901760 run.py:483] Algo bellman_ford step 962 current loss 0.707315, current_train_items 30816.
I0302 18:58:42.801094 22535416901760 run.py:483] Algo bellman_ford step 963 current loss 0.896079, current_train_items 30848.
I0302 18:58:42.833127 22535416901760 run.py:483] Algo bellman_ford step 964 current loss 1.329926, current_train_items 30880.
I0302 18:58:42.851635 22535416901760 run.py:483] Algo bellman_ford step 965 current loss 0.510072, current_train_items 30912.
I0302 18:58:42.867680 22535416901760 run.py:483] Algo bellman_ford step 966 current loss 0.569311, current_train_items 30944.
I0302 18:58:42.891126 22535416901760 run.py:483] Algo bellman_ford step 967 current loss 0.816623, current_train_items 30976.
I0302 18:58:42.920766 22535416901760 run.py:483] Algo bellman_ford step 968 current loss 0.978404, current_train_items 31008.
I0302 18:58:42.952125 22535416901760 run.py:483] Algo bellman_ford step 969 current loss 1.188750, current_train_items 31040.
I0302 18:58:42.970585 22535416901760 run.py:483] Algo bellman_ford step 970 current loss 0.408569, current_train_items 31072.
I0302 18:58:42.986966 22535416901760 run.py:483] Algo bellman_ford step 971 current loss 0.614872, current_train_items 31104.
I0302 18:58:43.010250 22535416901760 run.py:483] Algo bellman_ford step 972 current loss 0.975194, current_train_items 31136.
I0302 18:58:43.039838 22535416901760 run.py:483] Algo bellman_ford step 973 current loss 1.085256, current_train_items 31168.
I0302 18:58:43.072677 22535416901760 run.py:483] Algo bellman_ford step 974 current loss 1.313615, current_train_items 31200.
I0302 18:58:43.091421 22535416901760 run.py:483] Algo bellman_ford step 975 current loss 0.436498, current_train_items 31232.
I0302 18:58:43.107407 22535416901760 run.py:483] Algo bellman_ford step 976 current loss 0.479192, current_train_items 31264.
I0302 18:58:43.130340 22535416901760 run.py:483] Algo bellman_ford step 977 current loss 0.913166, current_train_items 31296.
I0302 18:58:43.158810 22535416901760 run.py:483] Algo bellman_ford step 978 current loss 0.824261, current_train_items 31328.
I0302 18:58:43.189626 22535416901760 run.py:483] Algo bellman_ford step 979 current loss 1.172877, current_train_items 31360.
I0302 18:58:43.207815 22535416901760 run.py:483] Algo bellman_ford step 980 current loss 0.349772, current_train_items 31392.
I0302 18:58:43.224042 22535416901760 run.py:483] Algo bellman_ford step 981 current loss 0.674475, current_train_items 31424.
I0302 18:58:43.246889 22535416901760 run.py:483] Algo bellman_ford step 982 current loss 0.809437, current_train_items 31456.
I0302 18:58:43.275445 22535416901760 run.py:483] Algo bellman_ford step 983 current loss 1.017928, current_train_items 31488.
I0302 18:58:43.307378 22535416901760 run.py:483] Algo bellman_ford step 984 current loss 1.243251, current_train_items 31520.
I0302 18:58:43.326257 22535416901760 run.py:483] Algo bellman_ford step 985 current loss 0.365118, current_train_items 31552.
I0302 18:58:43.342867 22535416901760 run.py:483] Algo bellman_ford step 986 current loss 0.669726, current_train_items 31584.
I0302 18:58:43.365246 22535416901760 run.py:483] Algo bellman_ford step 987 current loss 0.876722, current_train_items 31616.
I0302 18:58:43.394394 22535416901760 run.py:483] Algo bellman_ford step 988 current loss 0.969262, current_train_items 31648.
I0302 18:58:43.425083 22535416901760 run.py:483] Algo bellman_ford step 989 current loss 1.018672, current_train_items 31680.
I0302 18:58:43.444226 22535416901760 run.py:483] Algo bellman_ford step 990 current loss 0.324709, current_train_items 31712.
I0302 18:58:43.460174 22535416901760 run.py:483] Algo bellman_ford step 991 current loss 0.583173, current_train_items 31744.
I0302 18:58:43.483146 22535416901760 run.py:483] Algo bellman_ford step 992 current loss 0.871669, current_train_items 31776.
I0302 18:58:43.510569 22535416901760 run.py:483] Algo bellman_ford step 993 current loss 0.925179, current_train_items 31808.
I0302 18:58:43.544612 22535416901760 run.py:483] Algo bellman_ford step 994 current loss 1.307126, current_train_items 31840.
I0302 18:58:43.562784 22535416901760 run.py:483] Algo bellman_ford step 995 current loss 0.478881, current_train_items 31872.
I0302 18:58:43.578727 22535416901760 run.py:483] Algo bellman_ford step 996 current loss 0.626871, current_train_items 31904.
I0302 18:58:43.601826 22535416901760 run.py:483] Algo bellman_ford step 997 current loss 0.908173, current_train_items 31936.
I0302 18:58:43.631636 22535416901760 run.py:483] Algo bellman_ford step 998 current loss 1.060121, current_train_items 31968.
I0302 18:58:43.662481 22535416901760 run.py:483] Algo bellman_ford step 999 current loss 1.130446, current_train_items 32000.
I0302 18:58:43.681065 22535416901760 run.py:483] Algo bellman_ford step 1000 current loss 0.388239, current_train_items 32032.
I0302 18:58:43.688758 22535416901760 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.8642578125, 'score': 0.8642578125, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:43.688867 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.864, val scores are: bellman_ford: 0.864
I0302 18:58:43.705793 22535416901760 run.py:483] Algo bellman_ford step 1001 current loss 0.670291, current_train_items 32064.
I0302 18:58:43.729667 22535416901760 run.py:483] Algo bellman_ford step 1002 current loss 0.733995, current_train_items 32096.
I0302 18:58:43.758031 22535416901760 run.py:483] Algo bellman_ford step 1003 current loss 1.190405, current_train_items 32128.
I0302 18:58:43.793058 22535416901760 run.py:483] Algo bellman_ford step 1004 current loss 1.431107, current_train_items 32160.
I0302 18:58:43.812362 22535416901760 run.py:483] Algo bellman_ford step 1005 current loss 0.416510, current_train_items 32192.
I0302 18:58:43.828217 22535416901760 run.py:483] Algo bellman_ford step 1006 current loss 0.652060, current_train_items 32224.
I0302 18:58:43.852778 22535416901760 run.py:483] Algo bellman_ford step 1007 current loss 0.932083, current_train_items 32256.
I0302 18:58:43.881387 22535416901760 run.py:483] Algo bellman_ford step 1008 current loss 0.932723, current_train_items 32288.
I0302 18:58:43.914303 22535416901760 run.py:483] Algo bellman_ford step 1009 current loss 1.115196, current_train_items 32320.
I0302 18:58:43.932791 22535416901760 run.py:483] Algo bellman_ford step 1010 current loss 0.401605, current_train_items 32352.
I0302 18:58:43.949090 22535416901760 run.py:483] Algo bellman_ford step 1011 current loss 0.651158, current_train_items 32384.
I0302 18:58:43.973371 22535416901760 run.py:483] Algo bellman_ford step 1012 current loss 0.910323, current_train_items 32416.
I0302 18:58:44.003280 22535416901760 run.py:483] Algo bellman_ford step 1013 current loss 1.108513, current_train_items 32448.
I0302 18:58:44.035511 22535416901760 run.py:483] Algo bellman_ford step 1014 current loss 1.037200, current_train_items 32480.
I0302 18:58:44.053883 22535416901760 run.py:483] Algo bellman_ford step 1015 current loss 0.400511, current_train_items 32512.
I0302 18:58:44.070323 22535416901760 run.py:483] Algo bellman_ford step 1016 current loss 0.678285, current_train_items 32544.
I0302 18:58:44.093874 22535416901760 run.py:483] Algo bellman_ford step 1017 current loss 0.874425, current_train_items 32576.
I0302 18:58:44.123344 22535416901760 run.py:483] Algo bellman_ford step 1018 current loss 0.943248, current_train_items 32608.
I0302 18:58:44.157808 22535416901760 run.py:483] Algo bellman_ford step 1019 current loss 1.555126, current_train_items 32640.
I0302 18:58:44.176030 22535416901760 run.py:483] Algo bellman_ford step 1020 current loss 0.387433, current_train_items 32672.
I0302 18:58:44.192134 22535416901760 run.py:483] Algo bellman_ford step 1021 current loss 0.562110, current_train_items 32704.
I0302 18:58:44.215342 22535416901760 run.py:483] Algo bellman_ford step 1022 current loss 0.789074, current_train_items 32736.
I0302 18:58:44.243683 22535416901760 run.py:483] Algo bellman_ford step 1023 current loss 0.908979, current_train_items 32768.
I0302 18:58:44.273727 22535416901760 run.py:483] Algo bellman_ford step 1024 current loss 1.097022, current_train_items 32800.
I0302 18:58:44.292573 22535416901760 run.py:483] Algo bellman_ford step 1025 current loss 0.376862, current_train_items 32832.
I0302 18:58:44.309232 22535416901760 run.py:483] Algo bellman_ford step 1026 current loss 0.634117, current_train_items 32864.
I0302 18:58:44.332481 22535416901760 run.py:483] Algo bellman_ford step 1027 current loss 0.837214, current_train_items 32896.
I0302 18:58:44.362604 22535416901760 run.py:483] Algo bellman_ford step 1028 current loss 0.985632, current_train_items 32928.
I0302 18:58:44.393582 22535416901760 run.py:483] Algo bellman_ford step 1029 current loss 1.128754, current_train_items 32960.
I0302 18:58:44.412043 22535416901760 run.py:483] Algo bellman_ford step 1030 current loss 0.489502, current_train_items 32992.
I0302 18:58:44.427727 22535416901760 run.py:483] Algo bellman_ford step 1031 current loss 0.588403, current_train_items 33024.
I0302 18:58:44.450038 22535416901760 run.py:483] Algo bellman_ford step 1032 current loss 1.017453, current_train_items 33056.
I0302 18:58:44.478418 22535416901760 run.py:483] Algo bellman_ford step 1033 current loss 1.103976, current_train_items 33088.
I0302 18:58:44.511063 22535416901760 run.py:483] Algo bellman_ford step 1034 current loss 1.431076, current_train_items 33120.
I0302 18:58:44.529501 22535416901760 run.py:483] Algo bellman_ford step 1035 current loss 0.377975, current_train_items 33152.
I0302 18:58:44.545608 22535416901760 run.py:483] Algo bellman_ford step 1036 current loss 0.576203, current_train_items 33184.
I0302 18:58:44.569364 22535416901760 run.py:483] Algo bellman_ford step 1037 current loss 0.840881, current_train_items 33216.
I0302 18:58:44.599015 22535416901760 run.py:483] Algo bellman_ford step 1038 current loss 1.099669, current_train_items 33248.
I0302 18:58:44.632666 22535416901760 run.py:483] Algo bellman_ford step 1039 current loss 1.358309, current_train_items 33280.
I0302 18:58:44.651216 22535416901760 run.py:483] Algo bellman_ford step 1040 current loss 0.360569, current_train_items 33312.
I0302 18:58:44.668097 22535416901760 run.py:483] Algo bellman_ford step 1041 current loss 0.821242, current_train_items 33344.
I0302 18:58:44.691942 22535416901760 run.py:483] Algo bellman_ford step 1042 current loss 0.972007, current_train_items 33376.
I0302 18:58:44.721241 22535416901760 run.py:483] Algo bellman_ford step 1043 current loss 0.974186, current_train_items 33408.
I0302 18:58:44.752825 22535416901760 run.py:483] Algo bellman_ford step 1044 current loss 1.099114, current_train_items 33440.
I0302 18:58:44.771023 22535416901760 run.py:483] Algo bellman_ford step 1045 current loss 0.272560, current_train_items 33472.
I0302 18:58:44.786873 22535416901760 run.py:483] Algo bellman_ford step 1046 current loss 0.576644, current_train_items 33504.
I0302 18:58:44.810360 22535416901760 run.py:483] Algo bellman_ford step 1047 current loss 0.990894, current_train_items 33536.
I0302 18:58:44.840249 22535416901760 run.py:483] Algo bellman_ford step 1048 current loss 1.117579, current_train_items 33568.
I0302 18:58:44.872993 22535416901760 run.py:483] Algo bellman_ford step 1049 current loss 1.072300, current_train_items 33600.
I0302 18:58:44.891686 22535416901760 run.py:483] Algo bellman_ford step 1050 current loss 0.424309, current_train_items 33632.
I0302 18:58:44.899477 22535416901760 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.8671875, 'score': 0.8671875, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:44.899584 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.880, current avg val score is 0.867, val scores are: bellman_ford: 0.867
I0302 18:58:44.915949 22535416901760 run.py:483] Algo bellman_ford step 1051 current loss 0.602760, current_train_items 33664.
I0302 18:58:44.940231 22535416901760 run.py:483] Algo bellman_ford step 1052 current loss 0.958462, current_train_items 33696.
I0302 18:58:44.968933 22535416901760 run.py:483] Algo bellman_ford step 1053 current loss 0.985384, current_train_items 33728.
I0302 18:58:45.001378 22535416901760 run.py:483] Algo bellman_ford step 1054 current loss 1.278727, current_train_items 33760.
I0302 18:58:45.020591 22535416901760 run.py:483] Algo bellman_ford step 1055 current loss 0.396927, current_train_items 33792.
I0302 18:58:45.037229 22535416901760 run.py:483] Algo bellman_ford step 1056 current loss 0.636702, current_train_items 33824.
I0302 18:58:45.060837 22535416901760 run.py:483] Algo bellman_ford step 1057 current loss 0.909600, current_train_items 33856.
I0302 18:58:45.091277 22535416901760 run.py:483] Algo bellman_ford step 1058 current loss 1.045391, current_train_items 33888.
I0302 18:58:45.121009 22535416901760 run.py:483] Algo bellman_ford step 1059 current loss 1.059955, current_train_items 33920.
I0302 18:58:45.139636 22535416901760 run.py:483] Algo bellman_ford step 1060 current loss 0.361514, current_train_items 33952.
I0302 18:58:45.155969 22535416901760 run.py:483] Algo bellman_ford step 1061 current loss 0.662244, current_train_items 33984.
I0302 18:58:45.177832 22535416901760 run.py:483] Algo bellman_ford step 1062 current loss 0.691748, current_train_items 34016.
I0302 18:58:45.206483 22535416901760 run.py:483] Algo bellman_ford step 1063 current loss 1.004548, current_train_items 34048.
I0302 18:58:45.238786 22535416901760 run.py:483] Algo bellman_ford step 1064 current loss 1.252924, current_train_items 34080.
I0302 18:58:45.257251 22535416901760 run.py:483] Algo bellman_ford step 1065 current loss 0.343456, current_train_items 34112.
I0302 18:58:45.273392 22535416901760 run.py:483] Algo bellman_ford step 1066 current loss 0.630609, current_train_items 34144.
I0302 18:58:45.296787 22535416901760 run.py:483] Algo bellman_ford step 1067 current loss 0.854422, current_train_items 34176.
I0302 18:58:45.325999 22535416901760 run.py:483] Algo bellman_ford step 1068 current loss 0.847870, current_train_items 34208.
I0302 18:58:45.359169 22535416901760 run.py:483] Algo bellman_ford step 1069 current loss 1.126268, current_train_items 34240.
I0302 18:58:45.377996 22535416901760 run.py:483] Algo bellman_ford step 1070 current loss 0.333026, current_train_items 34272.
I0302 18:58:45.394627 22535416901760 run.py:483] Algo bellman_ford step 1071 current loss 0.675501, current_train_items 34304.
I0302 18:58:45.418007 22535416901760 run.py:483] Algo bellman_ford step 1072 current loss 0.848866, current_train_items 34336.
I0302 18:58:45.447257 22535416901760 run.py:483] Algo bellman_ford step 1073 current loss 0.816216, current_train_items 34368.
I0302 18:58:45.477772 22535416901760 run.py:483] Algo bellman_ford step 1074 current loss 0.941703, current_train_items 34400.
I0302 18:58:45.496340 22535416901760 run.py:483] Algo bellman_ford step 1075 current loss 0.335758, current_train_items 34432.
I0302 18:58:45.512972 22535416901760 run.py:483] Algo bellman_ford step 1076 current loss 0.708928, current_train_items 34464.
I0302 18:58:45.536180 22535416901760 run.py:483] Algo bellman_ford step 1077 current loss 0.866898, current_train_items 34496.
I0302 18:58:45.565592 22535416901760 run.py:483] Algo bellman_ford step 1078 current loss 0.939835, current_train_items 34528.
I0302 18:58:45.597376 22535416901760 run.py:483] Algo bellman_ford step 1079 current loss 1.038765, current_train_items 34560.
I0302 18:58:45.616039 22535416901760 run.py:483] Algo bellman_ford step 1080 current loss 0.364020, current_train_items 34592.
I0302 18:58:45.632579 22535416901760 run.py:483] Algo bellman_ford step 1081 current loss 0.593363, current_train_items 34624.
I0302 18:58:45.655306 22535416901760 run.py:483] Algo bellman_ford step 1082 current loss 1.060314, current_train_items 34656.
I0302 18:58:45.683803 22535416901760 run.py:483] Algo bellman_ford step 1083 current loss 0.943784, current_train_items 34688.
I0302 18:58:45.716540 22535416901760 run.py:483] Algo bellman_ford step 1084 current loss 1.269104, current_train_items 34720.
I0302 18:58:45.735377 22535416901760 run.py:483] Algo bellman_ford step 1085 current loss 0.375391, current_train_items 34752.
I0302 18:58:45.751667 22535416901760 run.py:483] Algo bellman_ford step 1086 current loss 0.646439, current_train_items 34784.
I0302 18:58:45.774767 22535416901760 run.py:483] Algo bellman_ford step 1087 current loss 0.790444, current_train_items 34816.
I0302 18:58:45.803091 22535416901760 run.py:483] Algo bellman_ford step 1088 current loss 0.740712, current_train_items 34848.
I0302 18:58:45.833178 22535416901760 run.py:483] Algo bellman_ford step 1089 current loss 0.996953, current_train_items 34880.
I0302 18:58:45.852009 22535416901760 run.py:483] Algo bellman_ford step 1090 current loss 0.430208, current_train_items 34912.
I0302 18:58:45.868719 22535416901760 run.py:483] Algo bellman_ford step 1091 current loss 0.694093, current_train_items 34944.
I0302 18:58:45.891317 22535416901760 run.py:483] Algo bellman_ford step 1092 current loss 0.943663, current_train_items 34976.
I0302 18:58:45.921121 22535416901760 run.py:483] Algo bellman_ford step 1093 current loss 1.189739, current_train_items 35008.
I0302 18:58:45.953472 22535416901760 run.py:483] Algo bellman_ford step 1094 current loss 1.255706, current_train_items 35040.
I0302 18:58:45.971746 22535416901760 run.py:483] Algo bellman_ford step 1095 current loss 0.354084, current_train_items 35072.
I0302 18:58:45.987841 22535416901760 run.py:483] Algo bellman_ford step 1096 current loss 0.674045, current_train_items 35104.
I0302 18:58:46.010756 22535416901760 run.py:483] Algo bellman_ford step 1097 current loss 0.989228, current_train_items 35136.
I0302 18:58:46.040123 22535416901760 run.py:483] Algo bellman_ford step 1098 current loss 0.955682, current_train_items 35168.
I0302 18:58:46.071332 22535416901760 run.py:483] Algo bellman_ford step 1099 current loss 1.085100, current_train_items 35200.
I0302 18:58:46.090062 22535416901760 run.py:483] Algo bellman_ford step 1100 current loss 0.393705, current_train_items 35232.
I0302 18:58:46.097963 22535416901760 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:46.098070 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.880, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 18:58:46.131433 22535416901760 run.py:483] Algo bellman_ford step 1101 current loss 0.675759, current_train_items 35264.
I0302 18:58:46.154096 22535416901760 run.py:483] Algo bellman_ford step 1102 current loss 0.786413, current_train_items 35296.
I0302 18:58:46.185020 22535416901760 run.py:483] Algo bellman_ford step 1103 current loss 0.939857, current_train_items 35328.
I0302 18:58:46.219701 22535416901760 run.py:483] Algo bellman_ford step 1104 current loss 1.262436, current_train_items 35360.
I0302 18:58:46.238897 22535416901760 run.py:483] Algo bellman_ford step 1105 current loss 0.366296, current_train_items 35392.
I0302 18:58:46.254947 22535416901760 run.py:483] Algo bellman_ford step 1106 current loss 0.630021, current_train_items 35424.
I0302 18:58:46.278299 22535416901760 run.py:483] Algo bellman_ford step 1107 current loss 0.889718, current_train_items 35456.
I0302 18:58:46.306546 22535416901760 run.py:483] Algo bellman_ford step 1108 current loss 1.132548, current_train_items 35488.
I0302 18:58:46.336499 22535416901760 run.py:483] Algo bellman_ford step 1109 current loss 1.177881, current_train_items 35520.
I0302 18:58:46.355122 22535416901760 run.py:483] Algo bellman_ford step 1110 current loss 0.417445, current_train_items 35552.
I0302 18:58:46.371416 22535416901760 run.py:483] Algo bellman_ford step 1111 current loss 0.697131, current_train_items 35584.
I0302 18:58:46.394476 22535416901760 run.py:483] Algo bellman_ford step 1112 current loss 0.974409, current_train_items 35616.
I0302 18:58:46.421965 22535416901760 run.py:483] Algo bellman_ford step 1113 current loss 0.975261, current_train_items 35648.
I0302 18:58:46.454936 22535416901760 run.py:483] Algo bellman_ford step 1114 current loss 1.223529, current_train_items 35680.
I0302 18:58:46.473170 22535416901760 run.py:483] Algo bellman_ford step 1115 current loss 0.350874, current_train_items 35712.
I0302 18:58:46.488985 22535416901760 run.py:483] Algo bellman_ford step 1116 current loss 0.623243, current_train_items 35744.
I0302 18:58:46.512703 22535416901760 run.py:483] Algo bellman_ford step 1117 current loss 0.862206, current_train_items 35776.
I0302 18:58:46.542330 22535416901760 run.py:483] Algo bellman_ford step 1118 current loss 0.953615, current_train_items 35808.
I0302 18:58:46.574263 22535416901760 run.py:483] Algo bellman_ford step 1119 current loss 1.322153, current_train_items 35840.
I0302 18:58:46.592765 22535416901760 run.py:483] Algo bellman_ford step 1120 current loss 0.401708, current_train_items 35872.
I0302 18:58:46.608354 22535416901760 run.py:483] Algo bellman_ford step 1121 current loss 0.489075, current_train_items 35904.
I0302 18:58:46.631893 22535416901760 run.py:483] Algo bellman_ford step 1122 current loss 0.797799, current_train_items 35936.
I0302 18:58:46.659813 22535416901760 run.py:483] Algo bellman_ford step 1123 current loss 0.942207, current_train_items 35968.
I0302 18:58:46.692994 22535416901760 run.py:483] Algo bellman_ford step 1124 current loss 1.241434, current_train_items 36000.
I0302 18:58:46.711576 22535416901760 run.py:483] Algo bellman_ford step 1125 current loss 0.409020, current_train_items 36032.
I0302 18:58:46.727586 22535416901760 run.py:483] Algo bellman_ford step 1126 current loss 0.554769, current_train_items 36064.
I0302 18:58:46.750520 22535416901760 run.py:483] Algo bellman_ford step 1127 current loss 1.089340, current_train_items 36096.
I0302 18:58:46.779753 22535416901760 run.py:483] Algo bellman_ford step 1128 current loss 1.050734, current_train_items 36128.
I0302 18:58:46.809087 22535416901760 run.py:483] Algo bellman_ford step 1129 current loss 1.162225, current_train_items 36160.
I0302 18:58:46.827592 22535416901760 run.py:483] Algo bellman_ford step 1130 current loss 0.389990, current_train_items 36192.
I0302 18:58:46.843690 22535416901760 run.py:483] Algo bellman_ford step 1131 current loss 0.552074, current_train_items 36224.
I0302 18:58:46.866976 22535416901760 run.py:483] Algo bellman_ford step 1132 current loss 0.855299, current_train_items 36256.
I0302 18:58:46.896062 22535416901760 run.py:483] Algo bellman_ford step 1133 current loss 0.960494, current_train_items 36288.
I0302 18:58:46.927139 22535416901760 run.py:483] Algo bellman_ford step 1134 current loss 1.154858, current_train_items 36320.
I0302 18:58:46.945491 22535416901760 run.py:483] Algo bellman_ford step 1135 current loss 0.497613, current_train_items 36352.
I0302 18:58:46.961701 22535416901760 run.py:483] Algo bellman_ford step 1136 current loss 0.632703, current_train_items 36384.
I0302 18:58:46.984007 22535416901760 run.py:483] Algo bellman_ford step 1137 current loss 0.727176, current_train_items 36416.
I0302 18:58:47.014062 22535416901760 run.py:483] Algo bellman_ford step 1138 current loss 1.039544, current_train_items 36448.
I0302 18:58:47.045302 22535416901760 run.py:483] Algo bellman_ford step 1139 current loss 1.107084, current_train_items 36480.
I0302 18:58:47.063630 22535416901760 run.py:483] Algo bellman_ford step 1140 current loss 0.399531, current_train_items 36512.
I0302 18:58:47.079986 22535416901760 run.py:483] Algo bellman_ford step 1141 current loss 0.620709, current_train_items 36544.
I0302 18:58:47.102187 22535416901760 run.py:483] Algo bellman_ford step 1142 current loss 0.878137, current_train_items 36576.
I0302 18:58:47.130422 22535416901760 run.py:483] Algo bellman_ford step 1143 current loss 1.003184, current_train_items 36608.
I0302 18:58:47.161654 22535416901760 run.py:483] Algo bellman_ford step 1144 current loss 1.101791, current_train_items 36640.
I0302 18:58:47.180414 22535416901760 run.py:483] Algo bellman_ford step 1145 current loss 0.510094, current_train_items 36672.
I0302 18:58:47.196639 22535416901760 run.py:483] Algo bellman_ford step 1146 current loss 0.743860, current_train_items 36704.
I0302 18:58:47.219886 22535416901760 run.py:483] Algo bellman_ford step 1147 current loss 0.922861, current_train_items 36736.
I0302 18:58:47.249090 22535416901760 run.py:483] Algo bellman_ford step 1148 current loss 0.956841, current_train_items 36768.
I0302 18:58:47.278424 22535416901760 run.py:483] Algo bellman_ford step 1149 current loss 0.952184, current_train_items 36800.
I0302 18:58:47.296735 22535416901760 run.py:483] Algo bellman_ford step 1150 current loss 0.407480, current_train_items 36832.
I0302 18:58:47.304905 22535416901760 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.8544921875, 'score': 0.8544921875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:47.305012 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.854, val scores are: bellman_ford: 0.854
I0302 18:58:47.321619 22535416901760 run.py:483] Algo bellman_ford step 1151 current loss 0.649226, current_train_items 36864.
I0302 18:58:47.345852 22535416901760 run.py:483] Algo bellman_ford step 1152 current loss 0.884406, current_train_items 36896.
I0302 18:58:47.377099 22535416901760 run.py:483] Algo bellman_ford step 1153 current loss 1.167031, current_train_items 36928.
W0302 18:58:47.398165 22535416901760 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:54.018042 22535416901760 run.py:483] Algo bellman_ford step 1154 current loss 1.250673, current_train_items 36960.
I0302 18:58:54.039020 22535416901760 run.py:483] Algo bellman_ford step 1155 current loss 0.400028, current_train_items 36992.
I0302 18:58:54.055320 22535416901760 run.py:483] Algo bellman_ford step 1156 current loss 0.613977, current_train_items 37024.
I0302 18:58:54.078689 22535416901760 run.py:483] Algo bellman_ford step 1157 current loss 0.785590, current_train_items 37056.
I0302 18:58:54.107984 22535416901760 run.py:483] Algo bellman_ford step 1158 current loss 1.170659, current_train_items 37088.
I0302 18:58:54.138603 22535416901760 run.py:483] Algo bellman_ford step 1159 current loss 1.177415, current_train_items 37120.
I0302 18:58:54.158625 22535416901760 run.py:483] Algo bellman_ford step 1160 current loss 0.510732, current_train_items 37152.
I0302 18:58:54.175265 22535416901760 run.py:483] Algo bellman_ford step 1161 current loss 0.611573, current_train_items 37184.
I0302 18:58:54.197849 22535416901760 run.py:483] Algo bellman_ford step 1162 current loss 0.800934, current_train_items 37216.
I0302 18:58:54.227193 22535416901760 run.py:483] Algo bellman_ford step 1163 current loss 0.909007, current_train_items 37248.
I0302 18:58:54.259365 22535416901760 run.py:483] Algo bellman_ford step 1164 current loss 1.162826, current_train_items 37280.
I0302 18:58:54.278295 22535416901760 run.py:483] Algo bellman_ford step 1165 current loss 0.329707, current_train_items 37312.
I0302 18:58:54.294575 22535416901760 run.py:483] Algo bellman_ford step 1166 current loss 0.647179, current_train_items 37344.
I0302 18:58:54.317802 22535416901760 run.py:483] Algo bellman_ford step 1167 current loss 0.942718, current_train_items 37376.
I0302 18:58:54.347113 22535416901760 run.py:483] Algo bellman_ford step 1168 current loss 0.967189, current_train_items 37408.
I0302 18:58:54.380496 22535416901760 run.py:483] Algo bellman_ford step 1169 current loss 1.091392, current_train_items 37440.
I0302 18:58:54.400054 22535416901760 run.py:483] Algo bellman_ford step 1170 current loss 0.385361, current_train_items 37472.
I0302 18:58:54.416790 22535416901760 run.py:483] Algo bellman_ford step 1171 current loss 0.750759, current_train_items 37504.
I0302 18:58:54.440087 22535416901760 run.py:483] Algo bellman_ford step 1172 current loss 1.015828, current_train_items 37536.
I0302 18:58:54.470423 22535416901760 run.py:483] Algo bellman_ford step 1173 current loss 1.089355, current_train_items 37568.
I0302 18:58:54.502569 22535416901760 run.py:483] Algo bellman_ford step 1174 current loss 1.107633, current_train_items 37600.
I0302 18:58:54.522128 22535416901760 run.py:483] Algo bellman_ford step 1175 current loss 0.316575, current_train_items 37632.
I0302 18:58:54.538509 22535416901760 run.py:483] Algo bellman_ford step 1176 current loss 0.675798, current_train_items 37664.
I0302 18:58:54.560605 22535416901760 run.py:483] Algo bellman_ford step 1177 current loss 1.028845, current_train_items 37696.
I0302 18:58:54.589283 22535416901760 run.py:483] Algo bellman_ford step 1178 current loss 0.953317, current_train_items 37728.
I0302 18:58:54.623234 22535416901760 run.py:483] Algo bellman_ford step 1179 current loss 1.408887, current_train_items 37760.
I0302 18:58:54.642617 22535416901760 run.py:483] Algo bellman_ford step 1180 current loss 0.356418, current_train_items 37792.
I0302 18:58:54.659088 22535416901760 run.py:483] Algo bellman_ford step 1181 current loss 0.729199, current_train_items 37824.
I0302 18:58:54.682113 22535416901760 run.py:483] Algo bellman_ford step 1182 current loss 0.767693, current_train_items 37856.
I0302 18:58:54.710712 22535416901760 run.py:483] Algo bellman_ford step 1183 current loss 0.960114, current_train_items 37888.
I0302 18:58:54.742940 22535416901760 run.py:483] Algo bellman_ford step 1184 current loss 1.176797, current_train_items 37920.
I0302 18:58:54.762567 22535416901760 run.py:483] Algo bellman_ford step 1185 current loss 0.401239, current_train_items 37952.
I0302 18:58:54.778575 22535416901760 run.py:483] Algo bellman_ford step 1186 current loss 0.655178, current_train_items 37984.
I0302 18:58:54.801563 22535416901760 run.py:483] Algo bellman_ford step 1187 current loss 0.845101, current_train_items 38016.
I0302 18:58:54.830635 22535416901760 run.py:483] Algo bellman_ford step 1188 current loss 0.915046, current_train_items 38048.
I0302 18:58:54.861599 22535416901760 run.py:483] Algo bellman_ford step 1189 current loss 1.021445, current_train_items 38080.
I0302 18:58:54.880861 22535416901760 run.py:483] Algo bellman_ford step 1190 current loss 0.360475, current_train_items 38112.
I0302 18:58:54.897365 22535416901760 run.py:483] Algo bellman_ford step 1191 current loss 0.555174, current_train_items 38144.
I0302 18:58:54.921265 22535416901760 run.py:483] Algo bellman_ford step 1192 current loss 0.931735, current_train_items 38176.
I0302 18:58:54.951037 22535416901760 run.py:483] Algo bellman_ford step 1193 current loss 1.028913, current_train_items 38208.
I0302 18:58:54.982937 22535416901760 run.py:483] Algo bellman_ford step 1194 current loss 0.976155, current_train_items 38240.
I0302 18:58:55.002320 22535416901760 run.py:483] Algo bellman_ford step 1195 current loss 0.394811, current_train_items 38272.
I0302 18:58:55.018606 22535416901760 run.py:483] Algo bellman_ford step 1196 current loss 0.658170, current_train_items 38304.
I0302 18:58:55.041331 22535416901760 run.py:483] Algo bellman_ford step 1197 current loss 0.887084, current_train_items 38336.
I0302 18:58:55.072024 22535416901760 run.py:483] Algo bellman_ford step 1198 current loss 0.969393, current_train_items 38368.
I0302 18:58:55.105081 22535416901760 run.py:483] Algo bellman_ford step 1199 current loss 1.179961, current_train_items 38400.
I0302 18:58:55.124377 22535416901760 run.py:483] Algo bellman_ford step 1200 current loss 0.480880, current_train_items 38432.
I0302 18:58:55.134032 22535416901760 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.890625, 'score': 0.890625, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:55.134142 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.891, val scores are: bellman_ford: 0.891
I0302 18:58:55.151221 22535416901760 run.py:483] Algo bellman_ford step 1201 current loss 0.660484, current_train_items 38464.
I0302 18:58:55.175045 22535416901760 run.py:483] Algo bellman_ford step 1202 current loss 0.744791, current_train_items 38496.
I0302 18:58:55.205665 22535416901760 run.py:483] Algo bellman_ford step 1203 current loss 0.964046, current_train_items 38528.
I0302 18:58:55.239291 22535416901760 run.py:483] Algo bellman_ford step 1204 current loss 1.387604, current_train_items 38560.
I0302 18:58:55.259132 22535416901760 run.py:483] Algo bellman_ford step 1205 current loss 0.318098, current_train_items 38592.
I0302 18:58:55.275297 22535416901760 run.py:483] Algo bellman_ford step 1206 current loss 0.546130, current_train_items 38624.
I0302 18:58:55.298237 22535416901760 run.py:483] Algo bellman_ford step 1207 current loss 0.891170, current_train_items 38656.
I0302 18:58:55.328230 22535416901760 run.py:483] Algo bellman_ford step 1208 current loss 0.965600, current_train_items 38688.
I0302 18:58:55.361706 22535416901760 run.py:483] Algo bellman_ford step 1209 current loss 1.175504, current_train_items 38720.
I0302 18:58:55.381004 22535416901760 run.py:483] Algo bellman_ford step 1210 current loss 0.339352, current_train_items 38752.
I0302 18:58:55.397363 22535416901760 run.py:483] Algo bellman_ford step 1211 current loss 0.682897, current_train_items 38784.
I0302 18:58:55.420018 22535416901760 run.py:483] Algo bellman_ford step 1212 current loss 0.958714, current_train_items 38816.
I0302 18:58:55.448609 22535416901760 run.py:483] Algo bellman_ford step 1213 current loss 0.901426, current_train_items 38848.
I0302 18:58:55.480705 22535416901760 run.py:483] Algo bellman_ford step 1214 current loss 1.235093, current_train_items 38880.
I0302 18:58:55.499730 22535416901760 run.py:483] Algo bellman_ford step 1215 current loss 0.364338, current_train_items 38912.
I0302 18:58:55.515659 22535416901760 run.py:483] Algo bellman_ford step 1216 current loss 0.606878, current_train_items 38944.
I0302 18:58:55.538610 22535416901760 run.py:483] Algo bellman_ford step 1217 current loss 0.880232, current_train_items 38976.
I0302 18:58:55.569038 22535416901760 run.py:483] Algo bellman_ford step 1218 current loss 1.168016, current_train_items 39008.
I0302 18:58:55.601469 22535416901760 run.py:483] Algo bellman_ford step 1219 current loss 1.362332, current_train_items 39040.
I0302 18:58:55.620501 22535416901760 run.py:483] Algo bellman_ford step 1220 current loss 0.350422, current_train_items 39072.
I0302 18:58:55.636516 22535416901760 run.py:483] Algo bellman_ford step 1221 current loss 0.623808, current_train_items 39104.
I0302 18:58:55.660702 22535416901760 run.py:483] Algo bellman_ford step 1222 current loss 0.957246, current_train_items 39136.
I0302 18:58:55.690598 22535416901760 run.py:483] Algo bellman_ford step 1223 current loss 1.027447, current_train_items 39168.
I0302 18:58:55.722165 22535416901760 run.py:483] Algo bellman_ford step 1224 current loss 1.157030, current_train_items 39200.
I0302 18:58:55.741070 22535416901760 run.py:483] Algo bellman_ford step 1225 current loss 0.358982, current_train_items 39232.
I0302 18:58:55.757356 22535416901760 run.py:483] Algo bellman_ford step 1226 current loss 0.639728, current_train_items 39264.
I0302 18:58:55.781203 22535416901760 run.py:483] Algo bellman_ford step 1227 current loss 0.801317, current_train_items 39296.
I0302 18:58:55.810503 22535416901760 run.py:483] Algo bellman_ford step 1228 current loss 0.890982, current_train_items 39328.
I0302 18:58:55.844070 22535416901760 run.py:483] Algo bellman_ford step 1229 current loss 1.335351, current_train_items 39360.
I0302 18:58:55.862930 22535416901760 run.py:483] Algo bellman_ford step 1230 current loss 0.333458, current_train_items 39392.
I0302 18:58:55.879254 22535416901760 run.py:483] Algo bellman_ford step 1231 current loss 0.531135, current_train_items 39424.
I0302 18:58:55.903283 22535416901760 run.py:483] Algo bellman_ford step 1232 current loss 0.980598, current_train_items 39456.
I0302 18:58:55.932631 22535416901760 run.py:483] Algo bellman_ford step 1233 current loss 0.944936, current_train_items 39488.
I0302 18:58:55.963351 22535416901760 run.py:483] Algo bellman_ford step 1234 current loss 0.950257, current_train_items 39520.
I0302 18:58:55.982474 22535416901760 run.py:483] Algo bellman_ford step 1235 current loss 0.336023, current_train_items 39552.
I0302 18:58:55.998294 22535416901760 run.py:483] Algo bellman_ford step 1236 current loss 0.595104, current_train_items 39584.
I0302 18:58:56.021561 22535416901760 run.py:483] Algo bellman_ford step 1237 current loss 0.785440, current_train_items 39616.
I0302 18:58:56.051325 22535416901760 run.py:483] Algo bellman_ford step 1238 current loss 0.894042, current_train_items 39648.
I0302 18:58:56.084361 22535416901760 run.py:483] Algo bellman_ford step 1239 current loss 1.059313, current_train_items 39680.
I0302 18:58:56.103375 22535416901760 run.py:483] Algo bellman_ford step 1240 current loss 0.324714, current_train_items 39712.
I0302 18:58:56.119665 22535416901760 run.py:483] Algo bellman_ford step 1241 current loss 0.642365, current_train_items 39744.
I0302 18:58:56.143590 22535416901760 run.py:483] Algo bellman_ford step 1242 current loss 0.731990, current_train_items 39776.
I0302 18:58:56.173004 22535416901760 run.py:483] Algo bellman_ford step 1243 current loss 0.969042, current_train_items 39808.
I0302 18:58:56.205455 22535416901760 run.py:483] Algo bellman_ford step 1244 current loss 1.193936, current_train_items 39840.
I0302 18:58:56.224523 22535416901760 run.py:483] Algo bellman_ford step 1245 current loss 0.322180, current_train_items 39872.
I0302 18:58:56.241139 22535416901760 run.py:483] Algo bellman_ford step 1246 current loss 0.474386, current_train_items 39904.
I0302 18:58:56.263537 22535416901760 run.py:483] Algo bellman_ford step 1247 current loss 0.779825, current_train_items 39936.
I0302 18:58:56.293214 22535416901760 run.py:483] Algo bellman_ford step 1248 current loss 0.890957, current_train_items 39968.
I0302 18:58:56.324663 22535416901760 run.py:483] Algo bellman_ford step 1249 current loss 1.190896, current_train_items 40000.
I0302 18:58:56.343804 22535416901760 run.py:483] Algo bellman_ford step 1250 current loss 0.365600, current_train_items 40032.
I0302 18:58:56.351920 22535416901760 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.8642578125, 'score': 0.8642578125, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:56.352029 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.864, val scores are: bellman_ford: 0.864
I0302 18:58:56.368979 22535416901760 run.py:483] Algo bellman_ford step 1251 current loss 0.539420, current_train_items 40064.
I0302 18:58:56.392510 22535416901760 run.py:483] Algo bellman_ford step 1252 current loss 0.836825, current_train_items 40096.
I0302 18:58:56.420784 22535416901760 run.py:483] Algo bellman_ford step 1253 current loss 0.811452, current_train_items 40128.
I0302 18:58:56.450543 22535416901760 run.py:483] Algo bellman_ford step 1254 current loss 0.876100, current_train_items 40160.
I0302 18:58:56.469886 22535416901760 run.py:483] Algo bellman_ford step 1255 current loss 0.313765, current_train_items 40192.
I0302 18:58:56.486264 22535416901760 run.py:483] Algo bellman_ford step 1256 current loss 0.608040, current_train_items 40224.
I0302 18:58:56.509240 22535416901760 run.py:483] Algo bellman_ford step 1257 current loss 0.864247, current_train_items 40256.
I0302 18:58:56.537985 22535416901760 run.py:483] Algo bellman_ford step 1258 current loss 0.854837, current_train_items 40288.
I0302 18:58:56.569273 22535416901760 run.py:483] Algo bellman_ford step 1259 current loss 1.089187, current_train_items 40320.
I0302 18:58:56.588596 22535416901760 run.py:483] Algo bellman_ford step 1260 current loss 0.383998, current_train_items 40352.
I0302 18:58:56.605467 22535416901760 run.py:483] Algo bellman_ford step 1261 current loss 0.614576, current_train_items 40384.
I0302 18:58:56.628492 22535416901760 run.py:483] Algo bellman_ford step 1262 current loss 0.933877, current_train_items 40416.
I0302 18:58:56.655343 22535416901760 run.py:483] Algo bellman_ford step 1263 current loss 0.748615, current_train_items 40448.
I0302 18:58:56.689190 22535416901760 run.py:483] Algo bellman_ford step 1264 current loss 1.221788, current_train_items 40480.
I0302 18:58:56.708116 22535416901760 run.py:483] Algo bellman_ford step 1265 current loss 0.315189, current_train_items 40512.
I0302 18:58:56.724377 22535416901760 run.py:483] Algo bellman_ford step 1266 current loss 0.723906, current_train_items 40544.
I0302 18:58:56.746510 22535416901760 run.py:483] Algo bellman_ford step 1267 current loss 0.907575, current_train_items 40576.
I0302 18:58:56.775140 22535416901760 run.py:483] Algo bellman_ford step 1268 current loss 0.903307, current_train_items 40608.
I0302 18:58:56.807743 22535416901760 run.py:483] Algo bellman_ford step 1269 current loss 1.059096, current_train_items 40640.
I0302 18:58:56.827148 22535416901760 run.py:483] Algo bellman_ford step 1270 current loss 0.381259, current_train_items 40672.
I0302 18:58:56.843407 22535416901760 run.py:483] Algo bellman_ford step 1271 current loss 0.575949, current_train_items 40704.
I0302 18:58:56.865230 22535416901760 run.py:483] Algo bellman_ford step 1272 current loss 0.770014, current_train_items 40736.
I0302 18:58:56.893244 22535416901760 run.py:483] Algo bellman_ford step 1273 current loss 0.741326, current_train_items 40768.
I0302 18:58:56.925598 22535416901760 run.py:483] Algo bellman_ford step 1274 current loss 1.102577, current_train_items 40800.
I0302 18:58:56.944797 22535416901760 run.py:483] Algo bellman_ford step 1275 current loss 0.380770, current_train_items 40832.
I0302 18:58:56.961201 22535416901760 run.py:483] Algo bellman_ford step 1276 current loss 0.631397, current_train_items 40864.
I0302 18:58:56.982834 22535416901760 run.py:483] Algo bellman_ford step 1277 current loss 0.767952, current_train_items 40896.
I0302 18:58:57.011600 22535416901760 run.py:483] Algo bellman_ford step 1278 current loss 0.840147, current_train_items 40928.
I0302 18:58:57.043126 22535416901760 run.py:483] Algo bellman_ford step 1279 current loss 1.087879, current_train_items 40960.
I0302 18:58:57.062140 22535416901760 run.py:483] Algo bellman_ford step 1280 current loss 0.413593, current_train_items 40992.
I0302 18:58:57.078246 22535416901760 run.py:483] Algo bellman_ford step 1281 current loss 0.618964, current_train_items 41024.
I0302 18:58:57.101554 22535416901760 run.py:483] Algo bellman_ford step 1282 current loss 1.019028, current_train_items 41056.
I0302 18:58:57.131447 22535416901760 run.py:483] Algo bellman_ford step 1283 current loss 1.267201, current_train_items 41088.
I0302 18:58:57.162376 22535416901760 run.py:483] Algo bellman_ford step 1284 current loss 1.368051, current_train_items 41120.
I0302 18:58:57.181811 22535416901760 run.py:483] Algo bellman_ford step 1285 current loss 0.422888, current_train_items 41152.
I0302 18:58:57.198127 22535416901760 run.py:483] Algo bellman_ford step 1286 current loss 0.680493, current_train_items 41184.
I0302 18:58:57.222177 22535416901760 run.py:483] Algo bellman_ford step 1287 current loss 1.107725, current_train_items 41216.
I0302 18:58:57.251564 22535416901760 run.py:483] Algo bellman_ford step 1288 current loss 1.114436, current_train_items 41248.
I0302 18:58:57.283746 22535416901760 run.py:483] Algo bellman_ford step 1289 current loss 1.171778, current_train_items 41280.
I0302 18:58:57.303182 22535416901760 run.py:483] Algo bellman_ford step 1290 current loss 0.442638, current_train_items 41312.
I0302 18:58:57.319645 22535416901760 run.py:483] Algo bellman_ford step 1291 current loss 0.836424, current_train_items 41344.
I0302 18:58:57.341830 22535416901760 run.py:483] Algo bellman_ford step 1292 current loss 0.841806, current_train_items 41376.
I0302 18:58:57.370947 22535416901760 run.py:483] Algo bellman_ford step 1293 current loss 0.965710, current_train_items 41408.
I0302 18:58:57.403934 22535416901760 run.py:483] Algo bellman_ford step 1294 current loss 1.217170, current_train_items 41440.
I0302 18:58:57.422837 22535416901760 run.py:483] Algo bellman_ford step 1295 current loss 0.433411, current_train_items 41472.
I0302 18:58:57.439033 22535416901760 run.py:483] Algo bellman_ford step 1296 current loss 0.577421, current_train_items 41504.
I0302 18:58:57.462435 22535416901760 run.py:483] Algo bellman_ford step 1297 current loss 0.928170, current_train_items 41536.
I0302 18:58:57.490167 22535416901760 run.py:483] Algo bellman_ford step 1298 current loss 0.775238, current_train_items 41568.
I0302 18:58:57.519744 22535416901760 run.py:483] Algo bellman_ford step 1299 current loss 1.029773, current_train_items 41600.
I0302 18:58:57.538846 22535416901760 run.py:483] Algo bellman_ford step 1300 current loss 0.357209, current_train_items 41632.
I0302 18:58:57.546882 22535416901760 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.875, 'score': 0.875, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:57.546990 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.875, val scores are: bellman_ford: 0.875
I0302 18:58:57.563658 22535416901760 run.py:483] Algo bellman_ford step 1301 current loss 0.582321, current_train_items 41664.
I0302 18:58:57.587331 22535416901760 run.py:483] Algo bellman_ford step 1302 current loss 0.913364, current_train_items 41696.
I0302 18:58:57.617418 22535416901760 run.py:483] Algo bellman_ford step 1303 current loss 1.041655, current_train_items 41728.
I0302 18:58:57.648180 22535416901760 run.py:483] Algo bellman_ford step 1304 current loss 1.046079, current_train_items 41760.
I0302 18:58:57.667227 22535416901760 run.py:483] Algo bellman_ford step 1305 current loss 0.336906, current_train_items 41792.
I0302 18:58:57.683105 22535416901760 run.py:483] Algo bellman_ford step 1306 current loss 0.631018, current_train_items 41824.
I0302 18:58:57.705402 22535416901760 run.py:483] Algo bellman_ford step 1307 current loss 0.730440, current_train_items 41856.
I0302 18:58:57.734214 22535416901760 run.py:483] Algo bellman_ford step 1308 current loss 0.837320, current_train_items 41888.
I0302 18:58:57.765419 22535416901760 run.py:483] Algo bellman_ford step 1309 current loss 0.971459, current_train_items 41920.
I0302 18:58:57.784447 22535416901760 run.py:483] Algo bellman_ford step 1310 current loss 0.300765, current_train_items 41952.
I0302 18:58:57.800565 22535416901760 run.py:483] Algo bellman_ford step 1311 current loss 0.651420, current_train_items 41984.
I0302 18:58:57.822803 22535416901760 run.py:483] Algo bellman_ford step 1312 current loss 0.843633, current_train_items 42016.
I0302 18:58:57.852609 22535416901760 run.py:483] Algo bellman_ford step 1313 current loss 1.058398, current_train_items 42048.
I0302 18:58:57.884716 22535416901760 run.py:483] Algo bellman_ford step 1314 current loss 1.055818, current_train_items 42080.
I0302 18:58:57.903595 22535416901760 run.py:483] Algo bellman_ford step 1315 current loss 0.323707, current_train_items 42112.
I0302 18:58:57.919594 22535416901760 run.py:483] Algo bellman_ford step 1316 current loss 0.594615, current_train_items 42144.
I0302 18:58:57.942465 22535416901760 run.py:483] Algo bellman_ford step 1317 current loss 0.818050, current_train_items 42176.
I0302 18:58:57.971640 22535416901760 run.py:483] Algo bellman_ford step 1318 current loss 0.879995, current_train_items 42208.
I0302 18:58:58.003314 22535416901760 run.py:483] Algo bellman_ford step 1319 current loss 1.169620, current_train_items 42240.
I0302 18:58:58.022092 22535416901760 run.py:483] Algo bellman_ford step 1320 current loss 0.377776, current_train_items 42272.
I0302 18:58:58.038376 22535416901760 run.py:483] Algo bellman_ford step 1321 current loss 0.534624, current_train_items 42304.
I0302 18:58:58.061853 22535416901760 run.py:483] Algo bellman_ford step 1322 current loss 1.051498, current_train_items 42336.
I0302 18:58:58.091440 22535416901760 run.py:483] Algo bellman_ford step 1323 current loss 0.883149, current_train_items 42368.
I0302 18:58:58.122772 22535416901760 run.py:483] Algo bellman_ford step 1324 current loss 1.066539, current_train_items 42400.
I0302 18:58:58.141788 22535416901760 run.py:483] Algo bellman_ford step 1325 current loss 0.425363, current_train_items 42432.
I0302 18:58:58.157796 22535416901760 run.py:483] Algo bellman_ford step 1326 current loss 0.579359, current_train_items 42464.
I0302 18:58:58.179953 22535416901760 run.py:483] Algo bellman_ford step 1327 current loss 0.658225, current_train_items 42496.
I0302 18:58:58.208814 22535416901760 run.py:483] Algo bellman_ford step 1328 current loss 0.860589, current_train_items 42528.
I0302 18:58:58.241050 22535416901760 run.py:483] Algo bellman_ford step 1329 current loss 1.268544, current_train_items 42560.
I0302 18:58:58.259740 22535416901760 run.py:483] Algo bellman_ford step 1330 current loss 0.370646, current_train_items 42592.
I0302 18:58:58.275763 22535416901760 run.py:483] Algo bellman_ford step 1331 current loss 0.569851, current_train_items 42624.
I0302 18:58:58.298013 22535416901760 run.py:483] Algo bellman_ford step 1332 current loss 0.773868, current_train_items 42656.
I0302 18:58:58.326731 22535416901760 run.py:483] Algo bellman_ford step 1333 current loss 1.046752, current_train_items 42688.
I0302 18:58:58.357613 22535416901760 run.py:483] Algo bellman_ford step 1334 current loss 1.136672, current_train_items 42720.
I0302 18:58:58.376631 22535416901760 run.py:483] Algo bellman_ford step 1335 current loss 0.368264, current_train_items 42752.
I0302 18:58:58.392470 22535416901760 run.py:483] Algo bellman_ford step 1336 current loss 0.561037, current_train_items 42784.
I0302 18:58:58.416317 22535416901760 run.py:483] Algo bellman_ford step 1337 current loss 0.825799, current_train_items 42816.
I0302 18:58:58.445310 22535416901760 run.py:483] Algo bellman_ford step 1338 current loss 1.028384, current_train_items 42848.
I0302 18:58:58.479549 22535416901760 run.py:483] Algo bellman_ford step 1339 current loss 1.095574, current_train_items 42880.
I0302 18:58:58.498094 22535416901760 run.py:483] Algo bellman_ford step 1340 current loss 0.364625, current_train_items 42912.
I0302 18:58:58.514268 22535416901760 run.py:483] Algo bellman_ford step 1341 current loss 0.765464, current_train_items 42944.
I0302 18:58:58.537339 22535416901760 run.py:483] Algo bellman_ford step 1342 current loss 1.169365, current_train_items 42976.
I0302 18:58:58.566146 22535416901760 run.py:483] Algo bellman_ford step 1343 current loss 1.076901, current_train_items 43008.
I0302 18:58:58.597617 22535416901760 run.py:483] Algo bellman_ford step 1344 current loss 1.267976, current_train_items 43040.
I0302 18:58:58.616704 22535416901760 run.py:483] Algo bellman_ford step 1345 current loss 0.393869, current_train_items 43072.
I0302 18:58:58.632918 22535416901760 run.py:483] Algo bellman_ford step 1346 current loss 0.597405, current_train_items 43104.
I0302 18:58:58.656847 22535416901760 run.py:483] Algo bellman_ford step 1347 current loss 0.999343, current_train_items 43136.
I0302 18:58:58.685317 22535416901760 run.py:483] Algo bellman_ford step 1348 current loss 0.957687, current_train_items 43168.
I0302 18:58:58.715916 22535416901760 run.py:483] Algo bellman_ford step 1349 current loss 1.202200, current_train_items 43200.
I0302 18:58:58.735003 22535416901760 run.py:483] Algo bellman_ford step 1350 current loss 0.408322, current_train_items 43232.
I0302 18:58:58.743376 22535416901760 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.837890625, 'score': 0.837890625, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:58.743486 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.838, val scores are: bellman_ford: 0.838
I0302 18:58:58.760555 22535416901760 run.py:483] Algo bellman_ford step 1351 current loss 0.649329, current_train_items 43264.
I0302 18:58:58.784895 22535416901760 run.py:483] Algo bellman_ford step 1352 current loss 0.773887, current_train_items 43296.
I0302 18:58:58.813287 22535416901760 run.py:483] Algo bellman_ford step 1353 current loss 0.944511, current_train_items 43328.
I0302 18:58:58.847372 22535416901760 run.py:483] Algo bellman_ford step 1354 current loss 1.211577, current_train_items 43360.
I0302 18:58:58.866781 22535416901760 run.py:483] Algo bellman_ford step 1355 current loss 0.397675, current_train_items 43392.
I0302 18:58:58.882632 22535416901760 run.py:483] Algo bellman_ford step 1356 current loss 0.684929, current_train_items 43424.
I0302 18:58:58.905245 22535416901760 run.py:483] Algo bellman_ford step 1357 current loss 0.843004, current_train_items 43456.
I0302 18:58:58.934168 22535416901760 run.py:483] Algo bellman_ford step 1358 current loss 0.893798, current_train_items 43488.
I0302 18:58:58.967413 22535416901760 run.py:483] Algo bellman_ford step 1359 current loss 1.215135, current_train_items 43520.
I0302 18:58:58.986966 22535416901760 run.py:483] Algo bellman_ford step 1360 current loss 0.398637, current_train_items 43552.
I0302 18:58:59.003925 22535416901760 run.py:483] Algo bellman_ford step 1361 current loss 0.657443, current_train_items 43584.
I0302 18:58:59.027456 22535416901760 run.py:483] Algo bellman_ford step 1362 current loss 0.761697, current_train_items 43616.
I0302 18:58:59.056480 22535416901760 run.py:483] Algo bellman_ford step 1363 current loss 1.037288, current_train_items 43648.
I0302 18:58:59.089760 22535416901760 run.py:483] Algo bellman_ford step 1364 current loss 1.193671, current_train_items 43680.
I0302 18:58:59.108924 22535416901760 run.py:483] Algo bellman_ford step 1365 current loss 0.343316, current_train_items 43712.
I0302 18:58:59.124915 22535416901760 run.py:483] Algo bellman_ford step 1366 current loss 0.560574, current_train_items 43744.
I0302 18:58:59.147136 22535416901760 run.py:483] Algo bellman_ford step 1367 current loss 0.718372, current_train_items 43776.
I0302 18:58:59.174763 22535416901760 run.py:483] Algo bellman_ford step 1368 current loss 0.771934, current_train_items 43808.
I0302 18:58:59.206732 22535416901760 run.py:483] Algo bellman_ford step 1369 current loss 1.154188, current_train_items 43840.
I0302 18:58:59.226077 22535416901760 run.py:483] Algo bellman_ford step 1370 current loss 0.358715, current_train_items 43872.
I0302 18:58:59.242607 22535416901760 run.py:483] Algo bellman_ford step 1371 current loss 0.642370, current_train_items 43904.
I0302 18:58:59.265641 22535416901760 run.py:483] Algo bellman_ford step 1372 current loss 0.756827, current_train_items 43936.
I0302 18:58:59.295881 22535416901760 run.py:483] Algo bellman_ford step 1373 current loss 0.962499, current_train_items 43968.
I0302 18:58:59.328730 22535416901760 run.py:483] Algo bellman_ford step 1374 current loss 1.150897, current_train_items 44000.
I0302 18:58:59.347982 22535416901760 run.py:483] Algo bellman_ford step 1375 current loss 0.362018, current_train_items 44032.
I0302 18:58:59.363832 22535416901760 run.py:483] Algo bellman_ford step 1376 current loss 0.540192, current_train_items 44064.
I0302 18:58:59.386328 22535416901760 run.py:483] Algo bellman_ford step 1377 current loss 0.871123, current_train_items 44096.
I0302 18:58:59.415890 22535416901760 run.py:483] Algo bellman_ford step 1378 current loss 0.868628, current_train_items 44128.
I0302 18:58:59.448633 22535416901760 run.py:483] Algo bellman_ford step 1379 current loss 0.994866, current_train_items 44160.
I0302 18:58:59.467805 22535416901760 run.py:483] Algo bellman_ford step 1380 current loss 0.410360, current_train_items 44192.
I0302 18:58:59.484254 22535416901760 run.py:483] Algo bellman_ford step 1381 current loss 0.679707, current_train_items 44224.
I0302 18:58:59.506908 22535416901760 run.py:483] Algo bellman_ford step 1382 current loss 0.763611, current_train_items 44256.
I0302 18:58:59.536107 22535416901760 run.py:483] Algo bellman_ford step 1383 current loss 0.831315, current_train_items 44288.
I0302 18:58:59.565935 22535416901760 run.py:483] Algo bellman_ford step 1384 current loss 0.919109, current_train_items 44320.
I0302 18:58:59.585116 22535416901760 run.py:483] Algo bellman_ford step 1385 current loss 0.291285, current_train_items 44352.
I0302 18:58:59.601106 22535416901760 run.py:483] Algo bellman_ford step 1386 current loss 0.649456, current_train_items 44384.
I0302 18:58:59.622771 22535416901760 run.py:483] Algo bellman_ford step 1387 current loss 0.625438, current_train_items 44416.
I0302 18:58:59.652225 22535416901760 run.py:483] Algo bellman_ford step 1388 current loss 0.780435, current_train_items 44448.
I0302 18:58:59.684104 22535416901760 run.py:483] Algo bellman_ford step 1389 current loss 0.948487, current_train_items 44480.
I0302 18:58:59.703391 22535416901760 run.py:483] Algo bellman_ford step 1390 current loss 0.328678, current_train_items 44512.
I0302 18:58:59.719347 22535416901760 run.py:483] Algo bellman_ford step 1391 current loss 0.568793, current_train_items 44544.
I0302 18:58:59.742792 22535416901760 run.py:483] Algo bellman_ford step 1392 current loss 0.752687, current_train_items 44576.
I0302 18:58:59.771824 22535416901760 run.py:483] Algo bellman_ford step 1393 current loss 0.888452, current_train_items 44608.
I0302 18:58:59.803623 22535416901760 run.py:483] Algo bellman_ford step 1394 current loss 1.015864, current_train_items 44640.
I0302 18:58:59.822299 22535416901760 run.py:483] Algo bellman_ford step 1395 current loss 0.333240, current_train_items 44672.
I0302 18:58:59.838212 22535416901760 run.py:483] Algo bellman_ford step 1396 current loss 0.596785, current_train_items 44704.
I0302 18:58:59.861213 22535416901760 run.py:483] Algo bellman_ford step 1397 current loss 0.884682, current_train_items 44736.
I0302 18:58:59.891280 22535416901760 run.py:483] Algo bellman_ford step 1398 current loss 1.090514, current_train_items 44768.
I0302 18:58:59.922261 22535416901760 run.py:483] Algo bellman_ford step 1399 current loss 0.950620, current_train_items 44800.
I0302 18:58:59.941527 22535416901760 run.py:483] Algo bellman_ford step 1400 current loss 0.422998, current_train_items 44832.
I0302 18:58:59.949373 22535416901760 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.884765625, 'score': 0.884765625, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:58:59.949482 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.885, val scores are: bellman_ford: 0.885
I0302 18:58:59.965831 22535416901760 run.py:483] Algo bellman_ford step 1401 current loss 0.530018, current_train_items 44864.
I0302 18:58:59.989804 22535416901760 run.py:483] Algo bellman_ford step 1402 current loss 0.859149, current_train_items 44896.
I0302 18:59:00.019925 22535416901760 run.py:483] Algo bellman_ford step 1403 current loss 0.904882, current_train_items 44928.
I0302 18:59:00.053468 22535416901760 run.py:483] Algo bellman_ford step 1404 current loss 1.022342, current_train_items 44960.
I0302 18:59:00.072831 22535416901760 run.py:483] Algo bellman_ford step 1405 current loss 0.381688, current_train_items 44992.
I0302 18:59:00.089331 22535416901760 run.py:483] Algo bellman_ford step 1406 current loss 0.640151, current_train_items 45024.
I0302 18:59:00.112589 22535416901760 run.py:483] Algo bellman_ford step 1407 current loss 0.767749, current_train_items 45056.
I0302 18:59:00.142070 22535416901760 run.py:483] Algo bellman_ford step 1408 current loss 0.958907, current_train_items 45088.
I0302 18:59:00.174033 22535416901760 run.py:483] Algo bellman_ford step 1409 current loss 1.112608, current_train_items 45120.
I0302 18:59:00.192881 22535416901760 run.py:483] Algo bellman_ford step 1410 current loss 0.371730, current_train_items 45152.
I0302 18:59:00.209416 22535416901760 run.py:483] Algo bellman_ford step 1411 current loss 0.532540, current_train_items 45184.
I0302 18:59:00.232404 22535416901760 run.py:483] Algo bellman_ford step 1412 current loss 0.815857, current_train_items 45216.
I0302 18:59:00.260078 22535416901760 run.py:483] Algo bellman_ford step 1413 current loss 0.753832, current_train_items 45248.
I0302 18:59:00.291496 22535416901760 run.py:483] Algo bellman_ford step 1414 current loss 1.069943, current_train_items 45280.
I0302 18:59:00.310684 22535416901760 run.py:483] Algo bellman_ford step 1415 current loss 0.303435, current_train_items 45312.
I0302 18:59:00.327223 22535416901760 run.py:483] Algo bellman_ford step 1416 current loss 0.631769, current_train_items 45344.
I0302 18:59:00.349595 22535416901760 run.py:483] Algo bellman_ford step 1417 current loss 0.692699, current_train_items 45376.
I0302 18:59:00.377931 22535416901760 run.py:483] Algo bellman_ford step 1418 current loss 0.950059, current_train_items 45408.
I0302 18:59:00.410432 22535416901760 run.py:483] Algo bellman_ford step 1419 current loss 1.034008, current_train_items 45440.
I0302 18:59:00.429775 22535416901760 run.py:483] Algo bellman_ford step 1420 current loss 0.430495, current_train_items 45472.
I0302 18:59:00.445983 22535416901760 run.py:483] Algo bellman_ford step 1421 current loss 0.589591, current_train_items 45504.
I0302 18:59:00.469625 22535416901760 run.py:483] Algo bellman_ford step 1422 current loss 0.742815, current_train_items 45536.
I0302 18:59:00.499870 22535416901760 run.py:483] Algo bellman_ford step 1423 current loss 0.929770, current_train_items 45568.
I0302 18:59:00.531015 22535416901760 run.py:483] Algo bellman_ford step 1424 current loss 1.028067, current_train_items 45600.
I0302 18:59:00.549931 22535416901760 run.py:483] Algo bellman_ford step 1425 current loss 0.369215, current_train_items 45632.
I0302 18:59:00.566665 22535416901760 run.py:483] Algo bellman_ford step 1426 current loss 0.598754, current_train_items 45664.
I0302 18:59:00.590502 22535416901760 run.py:483] Algo bellman_ford step 1427 current loss 0.780739, current_train_items 45696.
I0302 18:59:00.619732 22535416901760 run.py:483] Algo bellman_ford step 1428 current loss 0.937859, current_train_items 45728.
I0302 18:59:00.650076 22535416901760 run.py:483] Algo bellman_ford step 1429 current loss 0.870697, current_train_items 45760.
I0302 18:59:00.668923 22535416901760 run.py:483] Algo bellman_ford step 1430 current loss 0.335698, current_train_items 45792.
I0302 18:59:00.684944 22535416901760 run.py:483] Algo bellman_ford step 1431 current loss 0.592479, current_train_items 45824.
I0302 18:59:00.709203 22535416901760 run.py:483] Algo bellman_ford step 1432 current loss 0.827380, current_train_items 45856.
I0302 18:59:00.737016 22535416901760 run.py:483] Algo bellman_ford step 1433 current loss 0.782249, current_train_items 45888.
I0302 18:59:00.767533 22535416901760 run.py:483] Algo bellman_ford step 1434 current loss 0.966431, current_train_items 45920.
I0302 18:59:00.786167 22535416901760 run.py:483] Algo bellman_ford step 1435 current loss 0.348416, current_train_items 45952.
I0302 18:59:00.802305 22535416901760 run.py:483] Algo bellman_ford step 1436 current loss 0.621232, current_train_items 45984.
I0302 18:59:00.824864 22535416901760 run.py:483] Algo bellman_ford step 1437 current loss 0.838750, current_train_items 46016.
I0302 18:59:00.853296 22535416901760 run.py:483] Algo bellman_ford step 1438 current loss 0.817978, current_train_items 46048.
I0302 18:59:00.886205 22535416901760 run.py:483] Algo bellman_ford step 1439 current loss 1.041804, current_train_items 46080.
I0302 18:59:00.905294 22535416901760 run.py:483] Algo bellman_ford step 1440 current loss 0.372322, current_train_items 46112.
I0302 18:59:00.921625 22535416901760 run.py:483] Algo bellman_ford step 1441 current loss 0.664754, current_train_items 46144.
I0302 18:59:00.944366 22535416901760 run.py:483] Algo bellman_ford step 1442 current loss 0.724927, current_train_items 46176.
I0302 18:59:00.973573 22535416901760 run.py:483] Algo bellman_ford step 1443 current loss 0.905679, current_train_items 46208.
I0302 18:59:01.009306 22535416901760 run.py:483] Algo bellman_ford step 1444 current loss 1.296239, current_train_items 46240.
I0302 18:59:01.028294 22535416901760 run.py:483] Algo bellman_ford step 1445 current loss 0.387495, current_train_items 46272.
I0302 18:59:01.043899 22535416901760 run.py:483] Algo bellman_ford step 1446 current loss 0.589033, current_train_items 46304.
I0302 18:59:01.067276 22535416901760 run.py:483] Algo bellman_ford step 1447 current loss 0.894667, current_train_items 46336.
I0302 18:59:01.096388 22535416901760 run.py:483] Algo bellman_ford step 1448 current loss 0.927408, current_train_items 46368.
I0302 18:59:01.127465 22535416901760 run.py:483] Algo bellman_ford step 1449 current loss 1.037562, current_train_items 46400.
I0302 18:59:01.146366 22535416901760 run.py:483] Algo bellman_ford step 1450 current loss 0.357109, current_train_items 46432.
I0302 18:59:01.154850 22535416901760 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.8701171875, 'score': 0.8701171875, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:59:01.154958 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.907, current avg val score is 0.870, val scores are: bellman_ford: 0.870
I0302 18:59:01.171335 22535416901760 run.py:483] Algo bellman_ford step 1451 current loss 0.550991, current_train_items 46464.
I0302 18:59:01.194337 22535416901760 run.py:483] Algo bellman_ford step 1452 current loss 0.628119, current_train_items 46496.
I0302 18:59:01.224995 22535416901760 run.py:483] Algo bellman_ford step 1453 current loss 1.020235, current_train_items 46528.
I0302 18:59:01.258750 22535416901760 run.py:483] Algo bellman_ford step 1454 current loss 1.238626, current_train_items 46560.
I0302 18:59:01.278222 22535416901760 run.py:483] Algo bellman_ford step 1455 current loss 0.464230, current_train_items 46592.
I0302 18:59:01.294447 22535416901760 run.py:483] Algo bellman_ford step 1456 current loss 0.583351, current_train_items 46624.
I0302 18:59:01.316432 22535416901760 run.py:483] Algo bellman_ford step 1457 current loss 0.716812, current_train_items 46656.
I0302 18:59:01.346309 22535416901760 run.py:483] Algo bellman_ford step 1458 current loss 1.090554, current_train_items 46688.
I0302 18:59:01.377751 22535416901760 run.py:483] Algo bellman_ford step 1459 current loss 1.087946, current_train_items 46720.
I0302 18:59:01.397019 22535416901760 run.py:483] Algo bellman_ford step 1460 current loss 0.398263, current_train_items 46752.
I0302 18:59:01.412839 22535416901760 run.py:483] Algo bellman_ford step 1461 current loss 0.628515, current_train_items 46784.
I0302 18:59:01.435244 22535416901760 run.py:483] Algo bellman_ford step 1462 current loss 0.873535, current_train_items 46816.
I0302 18:59:01.464164 22535416901760 run.py:483] Algo bellman_ford step 1463 current loss 0.951402, current_train_items 46848.
I0302 18:59:01.495033 22535416901760 run.py:483] Algo bellman_ford step 1464 current loss 1.165051, current_train_items 46880.
I0302 18:59:01.514022 22535416901760 run.py:483] Algo bellman_ford step 1465 current loss 0.417373, current_train_items 46912.
I0302 18:59:01.530578 22535416901760 run.py:483] Algo bellman_ford step 1466 current loss 0.543656, current_train_items 46944.
I0302 18:59:01.553453 22535416901760 run.py:483] Algo bellman_ford step 1467 current loss 0.709687, current_train_items 46976.
I0302 18:59:01.580791 22535416901760 run.py:483] Algo bellman_ford step 1468 current loss 0.831057, current_train_items 47008.
I0302 18:59:01.611454 22535416901760 run.py:483] Algo bellman_ford step 1469 current loss 0.935688, current_train_items 47040.
I0302 18:59:01.630785 22535416901760 run.py:483] Algo bellman_ford step 1470 current loss 0.362943, current_train_items 47072.
I0302 18:59:01.647657 22535416901760 run.py:483] Algo bellman_ford step 1471 current loss 0.610894, current_train_items 47104.
I0302 18:59:01.670089 22535416901760 run.py:483] Algo bellman_ford step 1472 current loss 0.731315, current_train_items 47136.
I0302 18:59:01.699559 22535416901760 run.py:483] Algo bellman_ford step 1473 current loss 0.923347, current_train_items 47168.
I0302 18:59:01.732062 22535416901760 run.py:483] Algo bellman_ford step 1474 current loss 1.039620, current_train_items 47200.
I0302 18:59:01.751682 22535416901760 run.py:483] Algo bellman_ford step 1475 current loss 0.464729, current_train_items 47232.
I0302 18:59:01.767807 22535416901760 run.py:483] Algo bellman_ford step 1476 current loss 0.518449, current_train_items 47264.
I0302 18:59:01.790530 22535416901760 run.py:483] Algo bellman_ford step 1477 current loss 0.678919, current_train_items 47296.
I0302 18:59:01.819814 22535416901760 run.py:483] Algo bellman_ford step 1478 current loss 0.879765, current_train_items 47328.
I0302 18:59:01.853329 22535416901760 run.py:483] Algo bellman_ford step 1479 current loss 0.974542, current_train_items 47360.
I0302 18:59:01.872081 22535416901760 run.py:483] Algo bellman_ford step 1480 current loss 0.350986, current_train_items 47392.
I0302 18:59:01.887961 22535416901760 run.py:483] Algo bellman_ford step 1481 current loss 0.490921, current_train_items 47424.
I0302 18:59:01.910253 22535416901760 run.py:483] Algo bellman_ford step 1482 current loss 0.717703, current_train_items 47456.
I0302 18:59:01.939450 22535416901760 run.py:483] Algo bellman_ford step 1483 current loss 0.898914, current_train_items 47488.
I0302 18:59:01.969609 22535416901760 run.py:483] Algo bellman_ford step 1484 current loss 0.940312, current_train_items 47520.
I0302 18:59:01.988816 22535416901760 run.py:483] Algo bellman_ford step 1485 current loss 0.406002, current_train_items 47552.
I0302 18:59:02.004615 22535416901760 run.py:483] Algo bellman_ford step 1486 current loss 0.605852, current_train_items 47584.
I0302 18:59:02.027979 22535416901760 run.py:483] Algo bellman_ford step 1487 current loss 0.827409, current_train_items 47616.
I0302 18:59:02.055867 22535416901760 run.py:483] Algo bellman_ford step 1488 current loss 0.776437, current_train_items 47648.
I0302 18:59:02.088373 22535416901760 run.py:483] Algo bellman_ford step 1489 current loss 1.088577, current_train_items 47680.
I0302 18:59:02.107364 22535416901760 run.py:483] Algo bellman_ford step 1490 current loss 0.411738, current_train_items 47712.
I0302 18:59:02.123438 22535416901760 run.py:483] Algo bellman_ford step 1491 current loss 0.623370, current_train_items 47744.
I0302 18:59:02.146449 22535416901760 run.py:483] Algo bellman_ford step 1492 current loss 0.793927, current_train_items 47776.
I0302 18:59:02.175393 22535416901760 run.py:483] Algo bellman_ford step 1493 current loss 0.900016, current_train_items 47808.
I0302 18:59:02.208434 22535416901760 run.py:483] Algo bellman_ford step 1494 current loss 1.134501, current_train_items 47840.
I0302 18:59:02.227360 22535416901760 run.py:483] Algo bellman_ford step 1495 current loss 0.356989, current_train_items 47872.
I0302 18:59:02.243885 22535416901760 run.py:483] Algo bellman_ford step 1496 current loss 0.629746, current_train_items 47904.
I0302 18:59:02.267190 22535416901760 run.py:483] Algo bellman_ford step 1497 current loss 0.805473, current_train_items 47936.
I0302 18:59:02.297126 22535416901760 run.py:483] Algo bellman_ford step 1498 current loss 0.912336, current_train_items 47968.
I0302 18:59:02.330655 22535416901760 run.py:483] Algo bellman_ford step 1499 current loss 1.006727, current_train_items 48000.
I0302 18:59:02.349817 22535416901760 run.py:483] Algo bellman_ford step 1500 current loss 0.318881, current_train_items 48032.
I0302 18:59:02.357598 22535416901760 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:59:02.357741 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.907, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 18:59:02.388108 22535416901760 run.py:483] Algo bellman_ford step 1501 current loss 0.773621, current_train_items 48064.
I0302 18:59:02.412187 22535416901760 run.py:483] Algo bellman_ford step 1502 current loss 1.006971, current_train_items 48096.
I0302 18:59:02.441998 22535416901760 run.py:483] Algo bellman_ford step 1503 current loss 0.939060, current_train_items 48128.
I0302 18:59:02.474650 22535416901760 run.py:483] Algo bellman_ford step 1504 current loss 0.943742, current_train_items 48160.
I0302 18:59:02.494331 22535416901760 run.py:483] Algo bellman_ford step 1505 current loss 0.316520, current_train_items 48192.
I0302 18:59:02.510372 22535416901760 run.py:483] Algo bellman_ford step 1506 current loss 0.521837, current_train_items 48224.
I0302 18:59:02.532987 22535416901760 run.py:483] Algo bellman_ford step 1507 current loss 0.872787, current_train_items 48256.
I0302 18:59:02.561996 22535416901760 run.py:483] Algo bellman_ford step 1508 current loss 0.904326, current_train_items 48288.
I0302 18:59:02.594110 22535416901760 run.py:483] Algo bellman_ford step 1509 current loss 1.068690, current_train_items 48320.
I0302 18:59:02.613253 22535416901760 run.py:483] Algo bellman_ford step 1510 current loss 0.372652, current_train_items 48352.
I0302 18:59:02.629712 22535416901760 run.py:483] Algo bellman_ford step 1511 current loss 0.795641, current_train_items 48384.
I0302 18:59:02.652831 22535416901760 run.py:483] Algo bellman_ford step 1512 current loss 0.788668, current_train_items 48416.
I0302 18:59:02.680535 22535416901760 run.py:483] Algo bellman_ford step 1513 current loss 0.802517, current_train_items 48448.
I0302 18:59:02.714242 22535416901760 run.py:483] Algo bellman_ford step 1514 current loss 1.182967, current_train_items 48480.
I0302 18:59:02.733506 22535416901760 run.py:483] Algo bellman_ford step 1515 current loss 0.335310, current_train_items 48512.
I0302 18:59:02.749296 22535416901760 run.py:483] Algo bellman_ford step 1516 current loss 0.624194, current_train_items 48544.
I0302 18:59:02.772361 22535416901760 run.py:483] Algo bellman_ford step 1517 current loss 0.834364, current_train_items 48576.
I0302 18:59:02.802442 22535416901760 run.py:483] Algo bellman_ford step 1518 current loss 0.913226, current_train_items 48608.
I0302 18:59:02.834106 22535416901760 run.py:483] Algo bellman_ford step 1519 current loss 0.996852, current_train_items 48640.
I0302 18:59:02.853423 22535416901760 run.py:483] Algo bellman_ford step 1520 current loss 0.292598, current_train_items 48672.
I0302 18:59:02.869347 22535416901760 run.py:483] Algo bellman_ford step 1521 current loss 0.630969, current_train_items 48704.
I0302 18:59:02.893276 22535416901760 run.py:483] Algo bellman_ford step 1522 current loss 0.839030, current_train_items 48736.
I0302 18:59:02.922866 22535416901760 run.py:483] Algo bellman_ford step 1523 current loss 0.943096, current_train_items 48768.
I0302 18:59:02.954709 22535416901760 run.py:483] Algo bellman_ford step 1524 current loss 1.239099, current_train_items 48800.
I0302 18:59:02.973970 22535416901760 run.py:483] Algo bellman_ford step 1525 current loss 0.338906, current_train_items 48832.
I0302 18:59:02.989738 22535416901760 run.py:483] Algo bellman_ford step 1526 current loss 0.605687, current_train_items 48864.
I0302 18:59:03.013852 22535416901760 run.py:483] Algo bellman_ford step 1527 current loss 0.944287, current_train_items 48896.
I0302 18:59:03.043117 22535416901760 run.py:483] Algo bellman_ford step 1528 current loss 0.908160, current_train_items 48928.
I0302 18:59:03.075346 22535416901760 run.py:483] Algo bellman_ford step 1529 current loss 1.132106, current_train_items 48960.
I0302 18:59:03.094633 22535416901760 run.py:483] Algo bellman_ford step 1530 current loss 0.379699, current_train_items 48992.
I0302 18:59:03.110608 22535416901760 run.py:483] Algo bellman_ford step 1531 current loss 0.691184, current_train_items 49024.
I0302 18:59:03.133212 22535416901760 run.py:483] Algo bellman_ford step 1532 current loss 0.777467, current_train_items 49056.
I0302 18:59:03.162822 22535416901760 run.py:483] Algo bellman_ford step 1533 current loss 1.049368, current_train_items 49088.
I0302 18:59:03.195623 22535416901760 run.py:483] Algo bellman_ford step 1534 current loss 1.331714, current_train_items 49120.
I0302 18:59:03.215106 22535416901760 run.py:483] Algo bellman_ford step 1535 current loss 0.406012, current_train_items 49152.
I0302 18:59:03.231657 22535416901760 run.py:483] Algo bellman_ford step 1536 current loss 0.532121, current_train_items 49184.
I0302 18:59:03.255054 22535416901760 run.py:483] Algo bellman_ford step 1537 current loss 0.770603, current_train_items 49216.
I0302 18:59:03.284898 22535416901760 run.py:483] Algo bellman_ford step 1538 current loss 0.860399, current_train_items 49248.
I0302 18:59:03.319878 22535416901760 run.py:483] Algo bellman_ford step 1539 current loss 1.135144, current_train_items 49280.
I0302 18:59:03.339251 22535416901760 run.py:483] Algo bellman_ford step 1540 current loss 0.443543, current_train_items 49312.
I0302 18:59:03.355073 22535416901760 run.py:483] Algo bellman_ford step 1541 current loss 0.584818, current_train_items 49344.
I0302 18:59:03.377935 22535416901760 run.py:483] Algo bellman_ford step 1542 current loss 0.719900, current_train_items 49376.
I0302 18:59:03.406726 22535416901760 run.py:483] Algo bellman_ford step 1543 current loss 0.824729, current_train_items 49408.
I0302 18:59:03.438953 22535416901760 run.py:483] Algo bellman_ford step 1544 current loss 1.074388, current_train_items 49440.
I0302 18:59:03.457856 22535416901760 run.py:483] Algo bellman_ford step 1545 current loss 0.337452, current_train_items 49472.
I0302 18:59:03.473962 22535416901760 run.py:483] Algo bellman_ford step 1546 current loss 0.627438, current_train_items 49504.
I0302 18:59:03.496990 22535416901760 run.py:483] Algo bellman_ford step 1547 current loss 0.806881, current_train_items 49536.
I0302 18:59:03.527115 22535416901760 run.py:483] Algo bellman_ford step 1548 current loss 0.904428, current_train_items 49568.
I0302 18:59:03.560751 22535416901760 run.py:483] Algo bellman_ford step 1549 current loss 1.206780, current_train_items 49600.
I0302 18:59:03.579958 22535416901760 run.py:483] Algo bellman_ford step 1550 current loss 0.361532, current_train_items 49632.
I0302 18:59:03.588038 22535416901760 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.90234375, 'score': 0.90234375, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:59:03.588145 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.902, val scores are: bellman_ford: 0.902
I0302 18:59:03.605493 22535416901760 run.py:483] Algo bellman_ford step 1551 current loss 0.663875, current_train_items 49664.
I0302 18:59:03.628784 22535416901760 run.py:483] Algo bellman_ford step 1552 current loss 0.667379, current_train_items 49696.
I0302 18:59:03.658741 22535416901760 run.py:483] Algo bellman_ford step 1553 current loss 0.861633, current_train_items 49728.
I0302 18:59:03.690263 22535416901760 run.py:483] Algo bellman_ford step 1554 current loss 0.979299, current_train_items 49760.
I0302 18:59:03.709868 22535416901760 run.py:483] Algo bellman_ford step 1555 current loss 0.306110, current_train_items 49792.
I0302 18:59:03.726024 22535416901760 run.py:483] Algo bellman_ford step 1556 current loss 0.609798, current_train_items 49824.
I0302 18:59:03.749466 22535416901760 run.py:483] Algo bellman_ford step 1557 current loss 0.963892, current_train_items 49856.
I0302 18:59:03.779037 22535416901760 run.py:483] Algo bellman_ford step 1558 current loss 0.856579, current_train_items 49888.
I0302 18:59:03.811483 22535416901760 run.py:483] Algo bellman_ford step 1559 current loss 1.076379, current_train_items 49920.
I0302 18:59:03.830844 22535416901760 run.py:483] Algo bellman_ford step 1560 current loss 0.361139, current_train_items 49952.
I0302 18:59:03.847247 22535416901760 run.py:483] Algo bellman_ford step 1561 current loss 0.554785, current_train_items 49984.
I0302 18:59:03.870160 22535416901760 run.py:483] Algo bellman_ford step 1562 current loss 0.668847, current_train_items 50016.
I0302 18:59:03.900063 22535416901760 run.py:483] Algo bellman_ford step 1563 current loss 0.818236, current_train_items 50048.
I0302 18:59:03.933266 22535416901760 run.py:483] Algo bellman_ford step 1564 current loss 1.049495, current_train_items 50080.
I0302 18:59:03.952438 22535416901760 run.py:483] Algo bellman_ford step 1565 current loss 0.332101, current_train_items 50112.
I0302 18:59:03.968842 22535416901760 run.py:483] Algo bellman_ford step 1566 current loss 0.597680, current_train_items 50144.
I0302 18:59:03.992336 22535416901760 run.py:483] Algo bellman_ford step 1567 current loss 0.654965, current_train_items 50176.
I0302 18:59:04.021652 22535416901760 run.py:483] Algo bellman_ford step 1568 current loss 0.910610, current_train_items 50208.
I0302 18:59:04.055454 22535416901760 run.py:483] Algo bellman_ford step 1569 current loss 1.050152, current_train_items 50240.
I0302 18:59:04.074945 22535416901760 run.py:483] Algo bellman_ford step 1570 current loss 0.337210, current_train_items 50272.
I0302 18:59:04.091725 22535416901760 run.py:483] Algo bellman_ford step 1571 current loss 0.520808, current_train_items 50304.
I0302 18:59:04.115401 22535416901760 run.py:483] Algo bellman_ford step 1572 current loss 0.739513, current_train_items 50336.
I0302 18:59:04.144544 22535416901760 run.py:483] Algo bellman_ford step 1573 current loss 0.814830, current_train_items 50368.
I0302 18:59:04.174649 22535416901760 run.py:483] Algo bellman_ford step 1574 current loss 0.978223, current_train_items 50400.
I0302 18:59:04.194632 22535416901760 run.py:483] Algo bellman_ford step 1575 current loss 0.386273, current_train_items 50432.
I0302 18:59:04.211047 22535416901760 run.py:483] Algo bellman_ford step 1576 current loss 0.596759, current_train_items 50464.
I0302 18:59:04.232736 22535416901760 run.py:483] Algo bellman_ford step 1577 current loss 0.681147, current_train_items 50496.
I0302 18:59:04.262963 22535416901760 run.py:483] Algo bellman_ford step 1578 current loss 1.013475, current_train_items 50528.
I0302 18:59:04.294841 22535416901760 run.py:483] Algo bellman_ford step 1579 current loss 0.911515, current_train_items 50560.
I0302 18:59:04.314247 22535416901760 run.py:483] Algo bellman_ford step 1580 current loss 0.430722, current_train_items 50592.
I0302 18:59:04.330078 22535416901760 run.py:483] Algo bellman_ford step 1581 current loss 0.693765, current_train_items 50624.
I0302 18:59:04.353133 22535416901760 run.py:483] Algo bellman_ford step 1582 current loss 0.739953, current_train_items 50656.
I0302 18:59:04.382704 22535416901760 run.py:483] Algo bellman_ford step 1583 current loss 0.890877, current_train_items 50688.
I0302 18:59:04.415825 22535416901760 run.py:483] Algo bellman_ford step 1584 current loss 0.979986, current_train_items 50720.
I0302 18:59:04.435475 22535416901760 run.py:483] Algo bellman_ford step 1585 current loss 0.329323, current_train_items 50752.
I0302 18:59:04.451129 22535416901760 run.py:483] Algo bellman_ford step 1586 current loss 0.575727, current_train_items 50784.
I0302 18:59:04.474088 22535416901760 run.py:483] Algo bellman_ford step 1587 current loss 0.701379, current_train_items 50816.
I0302 18:59:04.503141 22535416901760 run.py:483] Algo bellman_ford step 1588 current loss 0.838829, current_train_items 50848.
I0302 18:59:04.534935 22535416901760 run.py:483] Algo bellman_ford step 1589 current loss 0.908592, current_train_items 50880.
I0302 18:59:04.554260 22535416901760 run.py:483] Algo bellman_ford step 1590 current loss 0.356500, current_train_items 50912.
I0302 18:59:04.570493 22535416901760 run.py:483] Algo bellman_ford step 1591 current loss 0.509144, current_train_items 50944.
I0302 18:59:04.592631 22535416901760 run.py:483] Algo bellman_ford step 1592 current loss 0.635969, current_train_items 50976.
I0302 18:59:04.623210 22535416901760 run.py:483] Algo bellman_ford step 1593 current loss 0.897260, current_train_items 51008.
I0302 18:59:04.656620 22535416901760 run.py:483] Algo bellman_ford step 1594 current loss 0.969371, current_train_items 51040.
I0302 18:59:04.676014 22535416901760 run.py:483] Algo bellman_ford step 1595 current loss 0.382829, current_train_items 51072.
I0302 18:59:04.691726 22535416901760 run.py:483] Algo bellman_ford step 1596 current loss 0.651538, current_train_items 51104.
I0302 18:59:04.715737 22535416901760 run.py:483] Algo bellman_ford step 1597 current loss 0.777988, current_train_items 51136.
I0302 18:59:04.743968 22535416901760 run.py:483] Algo bellman_ford step 1598 current loss 0.746273, current_train_items 51168.
I0302 18:59:04.776754 22535416901760 run.py:483] Algo bellman_ford step 1599 current loss 0.994712, current_train_items 51200.
I0302 18:59:04.796030 22535416901760 run.py:483] Algo bellman_ford step 1600 current loss 0.316692, current_train_items 51232.
I0302 18:59:04.803914 22535416901760 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:59:04.804020 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:04.820553 22535416901760 run.py:483] Algo bellman_ford step 1601 current loss 0.532678, current_train_items 51264.
I0302 18:59:04.845264 22535416901760 run.py:483] Algo bellman_ford step 1602 current loss 0.841450, current_train_items 51296.
I0302 18:59:04.874958 22535416901760 run.py:483] Algo bellman_ford step 1603 current loss 0.900592, current_train_items 51328.
I0302 18:59:04.908009 22535416901760 run.py:483] Algo bellman_ford step 1604 current loss 0.893988, current_train_items 51360.
I0302 18:59:04.927495 22535416901760 run.py:483] Algo bellman_ford step 1605 current loss 0.334599, current_train_items 51392.
I0302 18:59:04.943189 22535416901760 run.py:483] Algo bellman_ford step 1606 current loss 0.537552, current_train_items 51424.
I0302 18:59:04.965802 22535416901760 run.py:483] Algo bellman_ford step 1607 current loss 0.700378, current_train_items 51456.
I0302 18:59:04.996025 22535416901760 run.py:483] Algo bellman_ford step 1608 current loss 0.774918, current_train_items 51488.
I0302 18:59:05.029348 22535416901760 run.py:483] Algo bellman_ford step 1609 current loss 0.978966, current_train_items 51520.
I0302 18:59:05.048520 22535416901760 run.py:483] Algo bellman_ford step 1610 current loss 0.367888, current_train_items 51552.
I0302 18:59:05.064938 22535416901760 run.py:483] Algo bellman_ford step 1611 current loss 0.614005, current_train_items 51584.
I0302 18:59:05.088223 22535416901760 run.py:483] Algo bellman_ford step 1612 current loss 0.709014, current_train_items 51616.
I0302 18:59:05.117835 22535416901760 run.py:483] Algo bellman_ford step 1613 current loss 0.901874, current_train_items 51648.
I0302 18:59:05.149459 22535416901760 run.py:483] Algo bellman_ford step 1614 current loss 0.959346, current_train_items 51680.
I0302 18:59:05.168755 22535416901760 run.py:483] Algo bellman_ford step 1615 current loss 0.372324, current_train_items 51712.
I0302 18:59:05.185191 22535416901760 run.py:483] Algo bellman_ford step 1616 current loss 0.520653, current_train_items 51744.
I0302 18:59:05.208448 22535416901760 run.py:483] Algo bellman_ford step 1617 current loss 0.776048, current_train_items 51776.
I0302 18:59:05.237293 22535416901760 run.py:483] Algo bellman_ford step 1618 current loss 0.711341, current_train_items 51808.
I0302 18:59:05.271192 22535416901760 run.py:483] Algo bellman_ford step 1619 current loss 1.062853, current_train_items 51840.
I0302 18:59:05.290105 22535416901760 run.py:483] Algo bellman_ford step 1620 current loss 0.414329, current_train_items 51872.
I0302 18:59:05.306148 22535416901760 run.py:483] Algo bellman_ford step 1621 current loss 0.556104, current_train_items 51904.
I0302 18:59:05.329820 22535416901760 run.py:483] Algo bellman_ford step 1622 current loss 0.894524, current_train_items 51936.
I0302 18:59:05.359453 22535416901760 run.py:483] Algo bellman_ford step 1623 current loss 0.939944, current_train_items 51968.
I0302 18:59:05.392005 22535416901760 run.py:483] Algo bellman_ford step 1624 current loss 1.057838, current_train_items 52000.
I0302 18:59:05.411561 22535416901760 run.py:483] Algo bellman_ford step 1625 current loss 0.340661, current_train_items 52032.
I0302 18:59:05.427840 22535416901760 run.py:483] Algo bellman_ford step 1626 current loss 0.578138, current_train_items 52064.
I0302 18:59:05.450644 22535416901760 run.py:483] Algo bellman_ford step 1627 current loss 0.901618, current_train_items 52096.
I0302 18:59:05.481221 22535416901760 run.py:483] Algo bellman_ford step 1628 current loss 1.052854, current_train_items 52128.
I0302 18:59:05.512619 22535416901760 run.py:483] Algo bellman_ford step 1629 current loss 1.537251, current_train_items 52160.
I0302 18:59:05.531576 22535416901760 run.py:483] Algo bellman_ford step 1630 current loss 0.373498, current_train_items 52192.
I0302 18:59:05.548302 22535416901760 run.py:483] Algo bellman_ford step 1631 current loss 0.609881, current_train_items 52224.
I0302 18:59:05.571872 22535416901760 run.py:483] Algo bellman_ford step 1632 current loss 0.816359, current_train_items 52256.
I0302 18:59:05.600800 22535416901760 run.py:483] Algo bellman_ford step 1633 current loss 0.811170, current_train_items 52288.
I0302 18:59:05.633290 22535416901760 run.py:483] Algo bellman_ford step 1634 current loss 0.951026, current_train_items 52320.
I0302 18:59:05.652459 22535416901760 run.py:483] Algo bellman_ford step 1635 current loss 0.362751, current_train_items 52352.
I0302 18:59:05.668733 22535416901760 run.py:483] Algo bellman_ford step 1636 current loss 0.630584, current_train_items 52384.
I0302 18:59:05.692010 22535416901760 run.py:483] Algo bellman_ford step 1637 current loss 0.844108, current_train_items 52416.
I0302 18:59:05.721957 22535416901760 run.py:483] Algo bellman_ford step 1638 current loss 0.886489, current_train_items 52448.
I0302 18:59:05.756519 22535416901760 run.py:483] Algo bellman_ford step 1639 current loss 1.103771, current_train_items 52480.
I0302 18:59:05.775507 22535416901760 run.py:483] Algo bellman_ford step 1640 current loss 0.313214, current_train_items 52512.
I0302 18:59:05.791276 22535416901760 run.py:483] Algo bellman_ford step 1641 current loss 0.515529, current_train_items 52544.
I0302 18:59:05.814618 22535416901760 run.py:483] Algo bellman_ford step 1642 current loss 0.758830, current_train_items 52576.
I0302 18:59:05.843632 22535416901760 run.py:483] Algo bellman_ford step 1643 current loss 0.964677, current_train_items 52608.
I0302 18:59:05.875218 22535416901760 run.py:483] Algo bellman_ford step 1644 current loss 1.041352, current_train_items 52640.
I0302 18:59:05.894204 22535416901760 run.py:483] Algo bellman_ford step 1645 current loss 0.402366, current_train_items 52672.
I0302 18:59:05.910250 22535416901760 run.py:483] Algo bellman_ford step 1646 current loss 0.570735, current_train_items 52704.
I0302 18:59:05.934000 22535416901760 run.py:483] Algo bellman_ford step 1647 current loss 0.883017, current_train_items 52736.
I0302 18:59:05.962289 22535416901760 run.py:483] Algo bellman_ford step 1648 current loss 0.764273, current_train_items 52768.
I0302 18:59:05.995837 22535416901760 run.py:483] Algo bellman_ford step 1649 current loss 1.123592, current_train_items 52800.
I0302 18:59:06.014923 22535416901760 run.py:483] Algo bellman_ford step 1650 current loss 0.520238, current_train_items 52832.
I0302 18:59:06.022799 22535416901760 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.8779296875, 'score': 0.8779296875, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:59:06.022904 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.878, val scores are: bellman_ford: 0.878
I0302 18:59:06.040217 22535416901760 run.py:483] Algo bellman_ford step 1651 current loss 0.551440, current_train_items 52864.
I0302 18:59:06.064393 22535416901760 run.py:483] Algo bellman_ford step 1652 current loss 0.827876, current_train_items 52896.
I0302 18:59:06.093568 22535416901760 run.py:483] Algo bellman_ford step 1653 current loss 0.828814, current_train_items 52928.
I0302 18:59:06.124622 22535416901760 run.py:483] Algo bellman_ford step 1654 current loss 1.070096, current_train_items 52960.
I0302 18:59:06.143920 22535416901760 run.py:483] Algo bellman_ford step 1655 current loss 0.389761, current_train_items 52992.
I0302 18:59:06.159799 22535416901760 run.py:483] Algo bellman_ford step 1656 current loss 0.642196, current_train_items 53024.
I0302 18:59:06.182743 22535416901760 run.py:483] Algo bellman_ford step 1657 current loss 0.835752, current_train_items 53056.
I0302 18:59:06.210705 22535416901760 run.py:483] Algo bellman_ford step 1658 current loss 0.863502, current_train_items 53088.
I0302 18:59:06.240917 22535416901760 run.py:483] Algo bellman_ford step 1659 current loss 1.012566, current_train_items 53120.
I0302 18:59:06.259956 22535416901760 run.py:483] Algo bellman_ford step 1660 current loss 0.442837, current_train_items 53152.
I0302 18:59:06.276193 22535416901760 run.py:483] Algo bellman_ford step 1661 current loss 0.665956, current_train_items 53184.
I0302 18:59:06.298318 22535416901760 run.py:483] Algo bellman_ford step 1662 current loss 0.849602, current_train_items 53216.
I0302 18:59:06.328790 22535416901760 run.py:483] Algo bellman_ford step 1663 current loss 0.908945, current_train_items 53248.
I0302 18:59:06.361812 22535416901760 run.py:483] Algo bellman_ford step 1664 current loss 1.087375, current_train_items 53280.
I0302 18:59:06.380726 22535416901760 run.py:483] Algo bellman_ford step 1665 current loss 0.335456, current_train_items 53312.
I0302 18:59:06.396842 22535416901760 run.py:483] Algo bellman_ford step 1666 current loss 0.702739, current_train_items 53344.
I0302 18:59:06.419161 22535416901760 run.py:483] Algo bellman_ford step 1667 current loss 0.776284, current_train_items 53376.
I0302 18:59:06.447302 22535416901760 run.py:483] Algo bellman_ford step 1668 current loss 0.761133, current_train_items 53408.
I0302 18:59:06.477759 22535416901760 run.py:483] Algo bellman_ford step 1669 current loss 0.921949, current_train_items 53440.
I0302 18:59:06.497028 22535416901760 run.py:483] Algo bellman_ford step 1670 current loss 0.364990, current_train_items 53472.
I0302 18:59:06.512986 22535416901760 run.py:483] Algo bellman_ford step 1671 current loss 0.603103, current_train_items 53504.
I0302 18:59:06.535440 22535416901760 run.py:483] Algo bellman_ford step 1672 current loss 0.711062, current_train_items 53536.
I0302 18:59:06.565481 22535416901760 run.py:483] Algo bellman_ford step 1673 current loss 0.829476, current_train_items 53568.
I0302 18:59:06.598203 22535416901760 run.py:483] Algo bellman_ford step 1674 current loss 0.913352, current_train_items 53600.
I0302 18:59:06.617267 22535416901760 run.py:483] Algo bellman_ford step 1675 current loss 0.336552, current_train_items 53632.
I0302 18:59:06.633638 22535416901760 run.py:483] Algo bellman_ford step 1676 current loss 0.574991, current_train_items 53664.
I0302 18:59:06.655981 22535416901760 run.py:483] Algo bellman_ford step 1677 current loss 0.601906, current_train_items 53696.
I0302 18:59:06.685037 22535416901760 run.py:483] Algo bellman_ford step 1678 current loss 1.027797, current_train_items 53728.
I0302 18:59:06.718091 22535416901760 run.py:483] Algo bellman_ford step 1679 current loss 1.074462, current_train_items 53760.
I0302 18:59:06.736906 22535416901760 run.py:483] Algo bellman_ford step 1680 current loss 0.331334, current_train_items 53792.
I0302 18:59:06.752730 22535416901760 run.py:483] Algo bellman_ford step 1681 current loss 0.485839, current_train_items 53824.
I0302 18:59:06.774921 22535416901760 run.py:483] Algo bellman_ford step 1682 current loss 0.726341, current_train_items 53856.
I0302 18:59:06.802817 22535416901760 run.py:483] Algo bellman_ford step 1683 current loss 0.726857, current_train_items 53888.
I0302 18:59:06.834861 22535416901760 run.py:483] Algo bellman_ford step 1684 current loss 0.931388, current_train_items 53920.
I0302 18:59:06.854338 22535416901760 run.py:483] Algo bellman_ford step 1685 current loss 0.318134, current_train_items 53952.
I0302 18:59:06.870328 22535416901760 run.py:483] Algo bellman_ford step 1686 current loss 0.552329, current_train_items 53984.
I0302 18:59:06.893167 22535416901760 run.py:483] Algo bellman_ford step 1687 current loss 0.711431, current_train_items 54016.
I0302 18:59:06.922011 22535416901760 run.py:483] Algo bellman_ford step 1688 current loss 0.974111, current_train_items 54048.
I0302 18:59:06.955759 22535416901760 run.py:483] Algo bellman_ford step 1689 current loss 1.040388, current_train_items 54080.
I0302 18:59:06.974863 22535416901760 run.py:483] Algo bellman_ford step 1690 current loss 0.307959, current_train_items 54112.
I0302 18:59:06.991280 22535416901760 run.py:483] Algo bellman_ford step 1691 current loss 0.577948, current_train_items 54144.
I0302 18:59:07.014005 22535416901760 run.py:483] Algo bellman_ford step 1692 current loss 0.773046, current_train_items 54176.
I0302 18:59:07.043882 22535416901760 run.py:483] Algo bellman_ford step 1693 current loss 0.812980, current_train_items 54208.
I0302 18:59:07.076302 22535416901760 run.py:483] Algo bellman_ford step 1694 current loss 1.053858, current_train_items 54240.
I0302 18:59:07.095098 22535416901760 run.py:483] Algo bellman_ford step 1695 current loss 0.370201, current_train_items 54272.
I0302 18:59:07.111625 22535416901760 run.py:483] Algo bellman_ford step 1696 current loss 0.488098, current_train_items 54304.
I0302 18:59:07.132971 22535416901760 run.py:483] Algo bellman_ford step 1697 current loss 0.685718, current_train_items 54336.
I0302 18:59:07.160819 22535416901760 run.py:483] Algo bellman_ford step 1698 current loss 0.782519, current_train_items 54368.
I0302 18:59:07.193606 22535416901760 run.py:483] Algo bellman_ford step 1699 current loss 0.948690, current_train_items 54400.
I0302 18:59:07.212912 22535416901760 run.py:483] Algo bellman_ford step 1700 current loss 0.405681, current_train_items 54432.
I0302 18:59:07.220836 22535416901760 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.873046875, 'score': 0.873046875, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:59:07.220942 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.873, val scores are: bellman_ford: 0.873
I0302 18:59:07.237699 22535416901760 run.py:483] Algo bellman_ford step 1701 current loss 0.513787, current_train_items 54464.
I0302 18:59:07.260457 22535416901760 run.py:483] Algo bellman_ford step 1702 current loss 0.681982, current_train_items 54496.
I0302 18:59:07.289993 22535416901760 run.py:483] Algo bellman_ford step 1703 current loss 0.808733, current_train_items 54528.
I0302 18:59:07.322328 22535416901760 run.py:483] Algo bellman_ford step 1704 current loss 1.028343, current_train_items 54560.
I0302 18:59:07.341959 22535416901760 run.py:483] Algo bellman_ford step 1705 current loss 0.348695, current_train_items 54592.
I0302 18:59:07.357703 22535416901760 run.py:483] Algo bellman_ford step 1706 current loss 0.470035, current_train_items 54624.
I0302 18:59:07.380539 22535416901760 run.py:483] Algo bellman_ford step 1707 current loss 0.757386, current_train_items 54656.
I0302 18:59:07.409885 22535416901760 run.py:483] Algo bellman_ford step 1708 current loss 0.789857, current_train_items 54688.
I0302 18:59:07.443356 22535416901760 run.py:483] Algo bellman_ford step 1709 current loss 0.910101, current_train_items 54720.
I0302 18:59:07.462213 22535416901760 run.py:483] Algo bellman_ford step 1710 current loss 0.372123, current_train_items 54752.
I0302 18:59:07.478563 22535416901760 run.py:483] Algo bellman_ford step 1711 current loss 0.566383, current_train_items 54784.
I0302 18:59:07.501766 22535416901760 run.py:483] Algo bellman_ford step 1712 current loss 0.842228, current_train_items 54816.
I0302 18:59:07.530311 22535416901760 run.py:483] Algo bellman_ford step 1713 current loss 0.907085, current_train_items 54848.
I0302 18:59:07.562125 22535416901760 run.py:483] Algo bellman_ford step 1714 current loss 0.866266, current_train_items 54880.
I0302 18:59:07.581036 22535416901760 run.py:483] Algo bellman_ford step 1715 current loss 0.447614, current_train_items 54912.
I0302 18:59:07.596768 22535416901760 run.py:483] Algo bellman_ford step 1716 current loss 0.549030, current_train_items 54944.
I0302 18:59:07.620103 22535416901760 run.py:483] Algo bellman_ford step 1717 current loss 0.765194, current_train_items 54976.
I0302 18:59:07.647921 22535416901760 run.py:483] Algo bellman_ford step 1718 current loss 0.759673, current_train_items 55008.
I0302 18:59:07.679800 22535416901760 run.py:483] Algo bellman_ford step 1719 current loss 0.898392, current_train_items 55040.
I0302 18:59:07.698798 22535416901760 run.py:483] Algo bellman_ford step 1720 current loss 0.390500, current_train_items 55072.
I0302 18:59:07.714753 22535416901760 run.py:483] Algo bellman_ford step 1721 current loss 0.554630, current_train_items 55104.
I0302 18:59:07.737581 22535416901760 run.py:483] Algo bellman_ford step 1722 current loss 0.771292, current_train_items 55136.
I0302 18:59:07.767024 22535416901760 run.py:483] Algo bellman_ford step 1723 current loss 0.895369, current_train_items 55168.
I0302 18:59:07.798886 22535416901760 run.py:483] Algo bellman_ford step 1724 current loss 1.146815, current_train_items 55200.
I0302 18:59:07.817798 22535416901760 run.py:483] Algo bellman_ford step 1725 current loss 0.332148, current_train_items 55232.
I0302 18:59:07.834240 22535416901760 run.py:483] Algo bellman_ford step 1726 current loss 0.650871, current_train_items 55264.
I0302 18:59:07.858646 22535416901760 run.py:483] Algo bellman_ford step 1727 current loss 0.784227, current_train_items 55296.
I0302 18:59:07.888082 22535416901760 run.py:483] Algo bellman_ford step 1728 current loss 0.809340, current_train_items 55328.
I0302 18:59:07.920061 22535416901760 run.py:483] Algo bellman_ford step 1729 current loss 1.004620, current_train_items 55360.
I0302 18:59:07.939193 22535416901760 run.py:483] Algo bellman_ford step 1730 current loss 0.296576, current_train_items 55392.
I0302 18:59:07.955389 22535416901760 run.py:483] Algo bellman_ford step 1731 current loss 0.533310, current_train_items 55424.
I0302 18:59:07.978113 22535416901760 run.py:483] Algo bellman_ford step 1732 current loss 0.750908, current_train_items 55456.
I0302 18:59:08.007510 22535416901760 run.py:483] Algo bellman_ford step 1733 current loss 0.805268, current_train_items 55488.
I0302 18:59:08.037405 22535416901760 run.py:483] Algo bellman_ford step 1734 current loss 1.005672, current_train_items 55520.
I0302 18:59:08.056514 22535416901760 run.py:483] Algo bellman_ford step 1735 current loss 0.389952, current_train_items 55552.
I0302 18:59:08.072789 22535416901760 run.py:483] Algo bellman_ford step 1736 current loss 0.551399, current_train_items 55584.
I0302 18:59:08.095895 22535416901760 run.py:483] Algo bellman_ford step 1737 current loss 0.729711, current_train_items 55616.
I0302 18:59:08.124939 22535416901760 run.py:483] Algo bellman_ford step 1738 current loss 0.826162, current_train_items 55648.
I0302 18:59:08.157300 22535416901760 run.py:483] Algo bellman_ford step 1739 current loss 0.872414, current_train_items 55680.
I0302 18:59:08.175968 22535416901760 run.py:483] Algo bellman_ford step 1740 current loss 0.344737, current_train_items 55712.
I0302 18:59:08.192124 22535416901760 run.py:483] Algo bellman_ford step 1741 current loss 0.724225, current_train_items 55744.
I0302 18:59:08.215031 22535416901760 run.py:483] Algo bellman_ford step 1742 current loss 0.770843, current_train_items 55776.
I0302 18:59:08.244626 22535416901760 run.py:483] Algo bellman_ford step 1743 current loss 0.905105, current_train_items 55808.
I0302 18:59:08.275779 22535416901760 run.py:483] Algo bellman_ford step 1744 current loss 0.940706, current_train_items 55840.
I0302 18:59:08.294799 22535416901760 run.py:483] Algo bellman_ford step 1745 current loss 0.354162, current_train_items 55872.
I0302 18:59:08.311192 22535416901760 run.py:483] Algo bellman_ford step 1746 current loss 0.652640, current_train_items 55904.
I0302 18:59:08.333812 22535416901760 run.py:483] Algo bellman_ford step 1747 current loss 0.824144, current_train_items 55936.
I0302 18:59:08.362993 22535416901760 run.py:483] Algo bellman_ford step 1748 current loss 0.854045, current_train_items 55968.
I0302 18:59:08.393591 22535416901760 run.py:483] Algo bellman_ford step 1749 current loss 0.910061, current_train_items 56000.
I0302 18:59:08.412664 22535416901760 run.py:483] Algo bellman_ford step 1750 current loss 0.289141, current_train_items 56032.
I0302 18:59:08.420746 22535416901760 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:59:08.420853 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:59:08.437690 22535416901760 run.py:483] Algo bellman_ford step 1751 current loss 0.526341, current_train_items 56064.
I0302 18:59:08.460815 22535416901760 run.py:483] Algo bellman_ford step 1752 current loss 0.692619, current_train_items 56096.
I0302 18:59:08.491715 22535416901760 run.py:483] Algo bellman_ford step 1753 current loss 0.904683, current_train_items 56128.
I0302 18:59:08.525920 22535416901760 run.py:483] Algo bellman_ford step 1754 current loss 1.021602, current_train_items 56160.
I0302 18:59:08.545417 22535416901760 run.py:483] Algo bellman_ford step 1755 current loss 0.330100, current_train_items 56192.
I0302 18:59:08.561689 22535416901760 run.py:483] Algo bellman_ford step 1756 current loss 0.638776, current_train_items 56224.
I0302 18:59:08.585152 22535416901760 run.py:483] Algo bellman_ford step 1757 current loss 0.795957, current_train_items 56256.
I0302 18:59:08.615031 22535416901760 run.py:483] Algo bellman_ford step 1758 current loss 1.037003, current_train_items 56288.
I0302 18:59:08.644847 22535416901760 run.py:483] Algo bellman_ford step 1759 current loss 0.922824, current_train_items 56320.
I0302 18:59:08.663966 22535416901760 run.py:483] Algo bellman_ford step 1760 current loss 0.360459, current_train_items 56352.
I0302 18:59:08.680344 22535416901760 run.py:483] Algo bellman_ford step 1761 current loss 0.532060, current_train_items 56384.
I0302 18:59:08.703716 22535416901760 run.py:483] Algo bellman_ford step 1762 current loss 1.021678, current_train_items 56416.
I0302 18:59:08.734729 22535416901760 run.py:483] Algo bellman_ford step 1763 current loss 1.004270, current_train_items 56448.
I0302 18:59:08.768808 22535416901760 run.py:483] Algo bellman_ford step 1764 current loss 1.221168, current_train_items 56480.
I0302 18:59:08.787779 22535416901760 run.py:483] Algo bellman_ford step 1765 current loss 0.408638, current_train_items 56512.
I0302 18:59:08.803768 22535416901760 run.py:483] Algo bellman_ford step 1766 current loss 0.541241, current_train_items 56544.
I0302 18:59:08.826327 22535416901760 run.py:483] Algo bellman_ford step 1767 current loss 0.728635, current_train_items 56576.
I0302 18:59:08.855273 22535416901760 run.py:483] Algo bellman_ford step 1768 current loss 0.855308, current_train_items 56608.
I0302 18:59:08.886167 22535416901760 run.py:483] Algo bellman_ford step 1769 current loss 1.118946, current_train_items 56640.
I0302 18:59:08.905493 22535416901760 run.py:483] Algo bellman_ford step 1770 current loss 0.356214, current_train_items 56672.
I0302 18:59:08.921858 22535416901760 run.py:483] Algo bellman_ford step 1771 current loss 0.652129, current_train_items 56704.
I0302 18:59:08.945256 22535416901760 run.py:483] Algo bellman_ford step 1772 current loss 1.001771, current_train_items 56736.
I0302 18:59:08.975311 22535416901760 run.py:483] Algo bellman_ford step 1773 current loss 0.969016, current_train_items 56768.
I0302 18:59:09.006405 22535416901760 run.py:483] Algo bellman_ford step 1774 current loss 0.875811, current_train_items 56800.
I0302 18:59:09.025523 22535416901760 run.py:483] Algo bellman_ford step 1775 current loss 0.376190, current_train_items 56832.
I0302 18:59:09.041908 22535416901760 run.py:483] Algo bellman_ford step 1776 current loss 0.706235, current_train_items 56864.
I0302 18:59:09.065001 22535416901760 run.py:483] Algo bellman_ford step 1777 current loss 0.897740, current_train_items 56896.
I0302 18:59:09.094184 22535416901760 run.py:483] Algo bellman_ford step 1778 current loss 0.949328, current_train_items 56928.
I0302 18:59:09.124489 22535416901760 run.py:483] Algo bellman_ford step 1779 current loss 0.920894, current_train_items 56960.
I0302 18:59:09.143328 22535416901760 run.py:483] Algo bellman_ford step 1780 current loss 0.340138, current_train_items 56992.
I0302 18:59:09.159420 22535416901760 run.py:483] Algo bellman_ford step 1781 current loss 0.601698, current_train_items 57024.
I0302 18:59:09.182223 22535416901760 run.py:483] Algo bellman_ford step 1782 current loss 0.807583, current_train_items 57056.
I0302 18:59:09.211682 22535416901760 run.py:483] Algo bellman_ford step 1783 current loss 1.016057, current_train_items 57088.
I0302 18:59:09.242956 22535416901760 run.py:483] Algo bellman_ford step 1784 current loss 1.017045, current_train_items 57120.
I0302 18:59:09.262014 22535416901760 run.py:483] Algo bellman_ford step 1785 current loss 0.405050, current_train_items 57152.
I0302 18:59:09.278271 22535416901760 run.py:483] Algo bellman_ford step 1786 current loss 0.606433, current_train_items 57184.
I0302 18:59:09.300227 22535416901760 run.py:483] Algo bellman_ford step 1787 current loss 0.740075, current_train_items 57216.
I0302 18:59:09.328290 22535416901760 run.py:483] Algo bellman_ford step 1788 current loss 0.776488, current_train_items 57248.
I0302 18:59:09.360181 22535416901760 run.py:483] Algo bellman_ford step 1789 current loss 0.944103, current_train_items 57280.
I0302 18:59:09.379176 22535416901760 run.py:483] Algo bellman_ford step 1790 current loss 0.287615, current_train_items 57312.
I0302 18:59:09.395850 22535416901760 run.py:483] Algo bellman_ford step 1791 current loss 0.585069, current_train_items 57344.
I0302 18:59:09.418636 22535416901760 run.py:483] Algo bellman_ford step 1792 current loss 0.740783, current_train_items 57376.
I0302 18:59:09.446690 22535416901760 run.py:483] Algo bellman_ford step 1793 current loss 0.811894, current_train_items 57408.
I0302 18:59:09.478208 22535416901760 run.py:483] Algo bellman_ford step 1794 current loss 0.987886, current_train_items 57440.
I0302 18:59:09.496955 22535416901760 run.py:483] Algo bellman_ford step 1795 current loss 0.368476, current_train_items 57472.
I0302 18:59:09.512966 22535416901760 run.py:483] Algo bellman_ford step 1796 current loss 0.619956, current_train_items 57504.
I0302 18:59:09.536532 22535416901760 run.py:483] Algo bellman_ford step 1797 current loss 0.696069, current_train_items 57536.
I0302 18:59:09.566104 22535416901760 run.py:483] Algo bellman_ford step 1798 current loss 0.853484, current_train_items 57568.
I0302 18:59:09.599059 22535416901760 run.py:483] Algo bellman_ford step 1799 current loss 0.949169, current_train_items 57600.
I0302 18:59:09.618270 22535416901760 run.py:483] Algo bellman_ford step 1800 current loss 0.396200, current_train_items 57632.
I0302 18:59:09.625947 22535416901760 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:59:09.626055 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:09.642435 22535416901760 run.py:483] Algo bellman_ford step 1801 current loss 0.427874, current_train_items 57664.
I0302 18:59:09.666412 22535416901760 run.py:483] Algo bellman_ford step 1802 current loss 0.659184, current_train_items 57696.
I0302 18:59:09.695373 22535416901760 run.py:483] Algo bellman_ford step 1803 current loss 0.772101, current_train_items 57728.
I0302 18:59:09.727098 22535416901760 run.py:483] Algo bellman_ford step 1804 current loss 1.073799, current_train_items 57760.
I0302 18:59:09.746734 22535416901760 run.py:483] Algo bellman_ford step 1805 current loss 0.349923, current_train_items 57792.
I0302 18:59:09.762585 22535416901760 run.py:483] Algo bellman_ford step 1806 current loss 0.485418, current_train_items 57824.
I0302 18:59:09.785097 22535416901760 run.py:483] Algo bellman_ford step 1807 current loss 0.705203, current_train_items 57856.
I0302 18:59:09.813183 22535416901760 run.py:483] Algo bellman_ford step 1808 current loss 0.742020, current_train_items 57888.
I0302 18:59:09.846318 22535416901760 run.py:483] Algo bellman_ford step 1809 current loss 0.985910, current_train_items 57920.
I0302 18:59:09.865347 22535416901760 run.py:483] Algo bellman_ford step 1810 current loss 0.386476, current_train_items 57952.
I0302 18:59:09.881359 22535416901760 run.py:483] Algo bellman_ford step 1811 current loss 0.585995, current_train_items 57984.
I0302 18:59:09.904707 22535416901760 run.py:483] Algo bellman_ford step 1812 current loss 0.781709, current_train_items 58016.
I0302 18:59:09.932184 22535416901760 run.py:483] Algo bellman_ford step 1813 current loss 0.806952, current_train_items 58048.
I0302 18:59:09.964536 22535416901760 run.py:483] Algo bellman_ford step 1814 current loss 0.948779, current_train_items 58080.
I0302 18:59:09.983770 22535416901760 run.py:483] Algo bellman_ford step 1815 current loss 0.401332, current_train_items 58112.
I0302 18:59:10.000402 22535416901760 run.py:483] Algo bellman_ford step 1816 current loss 0.728381, current_train_items 58144.
I0302 18:59:10.022829 22535416901760 run.py:483] Algo bellman_ford step 1817 current loss 0.674029, current_train_items 58176.
I0302 18:59:10.052177 22535416901760 run.py:483] Algo bellman_ford step 1818 current loss 0.948917, current_train_items 58208.
I0302 18:59:10.084279 22535416901760 run.py:483] Algo bellman_ford step 1819 current loss 1.015231, current_train_items 58240.
I0302 18:59:10.103169 22535416901760 run.py:483] Algo bellman_ford step 1820 current loss 0.414690, current_train_items 58272.
I0302 18:59:10.119871 22535416901760 run.py:483] Algo bellman_ford step 1821 current loss 0.604362, current_train_items 58304.
I0302 18:59:10.144914 22535416901760 run.py:483] Algo bellman_ford step 1822 current loss 0.808803, current_train_items 58336.
I0302 18:59:10.174426 22535416901760 run.py:483] Algo bellman_ford step 1823 current loss 0.822541, current_train_items 58368.
I0302 18:59:10.205497 22535416901760 run.py:483] Algo bellman_ford step 1824 current loss 0.860694, current_train_items 58400.
I0302 18:59:10.224229 22535416901760 run.py:483] Algo bellman_ford step 1825 current loss 0.323186, current_train_items 58432.
I0302 18:59:10.240486 22535416901760 run.py:483] Algo bellman_ford step 1826 current loss 0.631086, current_train_items 58464.
I0302 18:59:10.263372 22535416901760 run.py:483] Algo bellman_ford step 1827 current loss 0.717436, current_train_items 58496.
I0302 18:59:10.293981 22535416901760 run.py:483] Algo bellman_ford step 1828 current loss 0.997128, current_train_items 58528.
I0302 18:59:10.328530 22535416901760 run.py:483] Algo bellman_ford step 1829 current loss 1.141902, current_train_items 58560.
I0302 18:59:10.347685 22535416901760 run.py:483] Algo bellman_ford step 1830 current loss 0.356236, current_train_items 58592.
I0302 18:59:10.364017 22535416901760 run.py:483] Algo bellman_ford step 1831 current loss 0.706008, current_train_items 58624.
I0302 18:59:10.388719 22535416901760 run.py:483] Algo bellman_ford step 1832 current loss 0.886931, current_train_items 58656.
I0302 18:59:10.418355 22535416901760 run.py:483] Algo bellman_ford step 1833 current loss 0.932161, current_train_items 58688.
I0302 18:59:10.449656 22535416901760 run.py:483] Algo bellman_ford step 1834 current loss 0.884425, current_train_items 58720.
I0302 18:59:10.468786 22535416901760 run.py:483] Algo bellman_ford step 1835 current loss 0.272817, current_train_items 58752.
I0302 18:59:10.485227 22535416901760 run.py:483] Algo bellman_ford step 1836 current loss 0.566215, current_train_items 58784.
I0302 18:59:10.509366 22535416901760 run.py:483] Algo bellman_ford step 1837 current loss 0.891671, current_train_items 58816.
I0302 18:59:10.538005 22535416901760 run.py:483] Algo bellman_ford step 1838 current loss 0.967331, current_train_items 58848.
I0302 18:59:10.570369 22535416901760 run.py:483] Algo bellman_ford step 1839 current loss 0.936502, current_train_items 58880.
I0302 18:59:10.589318 22535416901760 run.py:483] Algo bellman_ford step 1840 current loss 0.329977, current_train_items 58912.
I0302 18:59:10.605563 22535416901760 run.py:483] Algo bellman_ford step 1841 current loss 0.544706, current_train_items 58944.
I0302 18:59:10.629115 22535416901760 run.py:483] Algo bellman_ford step 1842 current loss 0.755998, current_train_items 58976.
I0302 18:59:10.659105 22535416901760 run.py:483] Algo bellman_ford step 1843 current loss 0.863287, current_train_items 59008.
I0302 18:59:10.690832 22535416901760 run.py:483] Algo bellman_ford step 1844 current loss 1.001082, current_train_items 59040.
I0302 18:59:10.710230 22535416901760 run.py:483] Algo bellman_ford step 1845 current loss 0.360322, current_train_items 59072.
I0302 18:59:10.726327 22535416901760 run.py:483] Algo bellman_ford step 1846 current loss 0.517362, current_train_items 59104.
I0302 18:59:10.749864 22535416901760 run.py:483] Algo bellman_ford step 1847 current loss 0.703831, current_train_items 59136.
I0302 18:59:10.779389 22535416901760 run.py:483] Algo bellman_ford step 1848 current loss 0.795905, current_train_items 59168.
I0302 18:59:10.812901 22535416901760 run.py:483] Algo bellman_ford step 1849 current loss 1.048847, current_train_items 59200.
I0302 18:59:10.831819 22535416901760 run.py:483] Algo bellman_ford step 1850 current loss 0.356313, current_train_items 59232.
I0302 18:59:10.839869 22535416901760 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.857421875, 'score': 0.857421875, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:59:10.839974 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.857, val scores are: bellman_ford: 0.857
I0302 18:59:10.856794 22535416901760 run.py:483] Algo bellman_ford step 1851 current loss 0.530580, current_train_items 59264.
I0302 18:59:10.880425 22535416901760 run.py:483] Algo bellman_ford step 1852 current loss 0.723300, current_train_items 59296.
I0302 18:59:10.910242 22535416901760 run.py:483] Algo bellman_ford step 1853 current loss 0.802304, current_train_items 59328.
I0302 18:59:10.942907 22535416901760 run.py:483] Algo bellman_ford step 1854 current loss 0.984222, current_train_items 59360.
I0302 18:59:10.962866 22535416901760 run.py:483] Algo bellman_ford step 1855 current loss 0.331401, current_train_items 59392.
I0302 18:59:10.978278 22535416901760 run.py:483] Algo bellman_ford step 1856 current loss 0.523382, current_train_items 59424.
I0302 18:59:11.001703 22535416901760 run.py:483] Algo bellman_ford step 1857 current loss 0.772768, current_train_items 59456.
I0302 18:59:11.030095 22535416901760 run.py:483] Algo bellman_ford step 1858 current loss 0.700614, current_train_items 59488.
I0302 18:59:11.062739 22535416901760 run.py:483] Algo bellman_ford step 1859 current loss 0.975819, current_train_items 59520.
I0302 18:59:11.082422 22535416901760 run.py:483] Algo bellman_ford step 1860 current loss 0.304517, current_train_items 59552.
I0302 18:59:11.099179 22535416901760 run.py:483] Algo bellman_ford step 1861 current loss 0.485366, current_train_items 59584.
I0302 18:59:11.121910 22535416901760 run.py:483] Algo bellman_ford step 1862 current loss 0.699362, current_train_items 59616.
I0302 18:59:11.151090 22535416901760 run.py:483] Algo bellman_ford step 1863 current loss 0.824850, current_train_items 59648.
I0302 18:59:11.183716 22535416901760 run.py:483] Algo bellman_ford step 1864 current loss 0.986915, current_train_items 59680.
I0302 18:59:11.203258 22535416901760 run.py:483] Algo bellman_ford step 1865 current loss 0.338143, current_train_items 59712.
I0302 18:59:11.219579 22535416901760 run.py:483] Algo bellman_ford step 1866 current loss 0.613991, current_train_items 59744.
I0302 18:59:11.243604 22535416901760 run.py:483] Algo bellman_ford step 1867 current loss 0.857426, current_train_items 59776.
I0302 18:59:11.273394 22535416901760 run.py:483] Algo bellman_ford step 1868 current loss 0.792161, current_train_items 59808.
I0302 18:59:11.305530 22535416901760 run.py:483] Algo bellman_ford step 1869 current loss 0.847372, current_train_items 59840.
I0302 18:59:11.324645 22535416901760 run.py:483] Algo bellman_ford step 1870 current loss 0.385063, current_train_items 59872.
I0302 18:59:11.340693 22535416901760 run.py:483] Algo bellman_ford step 1871 current loss 0.540648, current_train_items 59904.
I0302 18:59:11.363523 22535416901760 run.py:483] Algo bellman_ford step 1872 current loss 0.666029, current_train_items 59936.
I0302 18:59:11.393169 22535416901760 run.py:483] Algo bellman_ford step 1873 current loss 0.775158, current_train_items 59968.
I0302 18:59:11.424968 22535416901760 run.py:483] Algo bellman_ford step 1874 current loss 0.902612, current_train_items 60000.
I0302 18:59:11.444015 22535416901760 run.py:483] Algo bellman_ford step 1875 current loss 0.397211, current_train_items 60032.
I0302 18:59:11.460041 22535416901760 run.py:483] Algo bellman_ford step 1876 current loss 0.491703, current_train_items 60064.
I0302 18:59:11.483267 22535416901760 run.py:483] Algo bellman_ford step 1877 current loss 0.895714, current_train_items 60096.
I0302 18:59:11.511604 22535416901760 run.py:483] Algo bellman_ford step 1878 current loss 0.767215, current_train_items 60128.
I0302 18:59:11.546218 22535416901760 run.py:483] Algo bellman_ford step 1879 current loss 1.131643, current_train_items 60160.
I0302 18:59:11.565145 22535416901760 run.py:483] Algo bellman_ford step 1880 current loss 0.342290, current_train_items 60192.
I0302 18:59:11.581666 22535416901760 run.py:483] Algo bellman_ford step 1881 current loss 0.635770, current_train_items 60224.
I0302 18:59:11.604111 22535416901760 run.py:483] Algo bellman_ford step 1882 current loss 0.718428, current_train_items 60256.
I0302 18:59:11.633756 22535416901760 run.py:483] Algo bellman_ford step 1883 current loss 1.157743, current_train_items 60288.
I0302 18:59:11.666070 22535416901760 run.py:483] Algo bellman_ford step 1884 current loss 1.361868, current_train_items 60320.
I0302 18:59:11.685307 22535416901760 run.py:483] Algo bellman_ford step 1885 current loss 0.271601, current_train_items 60352.
I0302 18:59:11.701748 22535416901760 run.py:483] Algo bellman_ford step 1886 current loss 0.606700, current_train_items 60384.
I0302 18:59:11.724890 22535416901760 run.py:483] Algo bellman_ford step 1887 current loss 0.812750, current_train_items 60416.
I0302 18:59:11.752949 22535416901760 run.py:483] Algo bellman_ford step 1888 current loss 0.720333, current_train_items 60448.
I0302 18:59:11.786834 22535416901760 run.py:483] Algo bellman_ford step 1889 current loss 1.103102, current_train_items 60480.
I0302 18:59:11.806258 22535416901760 run.py:483] Algo bellman_ford step 1890 current loss 0.348790, current_train_items 60512.
I0302 18:59:11.822559 22535416901760 run.py:483] Algo bellman_ford step 1891 current loss 0.584857, current_train_items 60544.
I0302 18:59:11.845282 22535416901760 run.py:483] Algo bellman_ford step 1892 current loss 0.726999, current_train_items 60576.
I0302 18:59:11.875450 22535416901760 run.py:483] Algo bellman_ford step 1893 current loss 0.880662, current_train_items 60608.
I0302 18:59:11.910278 22535416901760 run.py:483] Algo bellman_ford step 1894 current loss 1.045272, current_train_items 60640.
I0302 18:59:11.929477 22535416901760 run.py:483] Algo bellman_ford step 1895 current loss 0.395191, current_train_items 60672.
I0302 18:59:11.945704 22535416901760 run.py:483] Algo bellman_ford step 1896 current loss 0.622385, current_train_items 60704.
I0302 18:59:11.968998 22535416901760 run.py:483] Algo bellman_ford step 1897 current loss 0.951141, current_train_items 60736.
I0302 18:59:11.998627 22535416901760 run.py:483] Algo bellman_ford step 1898 current loss 0.949994, current_train_items 60768.
I0302 18:59:12.030969 22535416901760 run.py:483] Algo bellman_ford step 1899 current loss 1.165316, current_train_items 60800.
I0302 18:59:12.050427 22535416901760 run.py:483] Algo bellman_ford step 1900 current loss 0.407567, current_train_items 60832.
I0302 18:59:12.058124 22535416901760 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.88671875, 'score': 0.88671875, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:59:12.058245 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.887, val scores are: bellman_ford: 0.887
I0302 18:59:12.074995 22535416901760 run.py:483] Algo bellman_ford step 1901 current loss 0.572475, current_train_items 60864.
I0302 18:59:12.098016 22535416901760 run.py:483] Algo bellman_ford step 1902 current loss 0.577717, current_train_items 60896.
I0302 18:59:12.126180 22535416901760 run.py:483] Algo bellman_ford step 1903 current loss 0.702467, current_train_items 60928.
I0302 18:59:12.158247 22535416901760 run.py:483] Algo bellman_ford step 1904 current loss 0.862053, current_train_items 60960.
I0302 18:59:12.177540 22535416901760 run.py:483] Algo bellman_ford step 1905 current loss 0.405837, current_train_items 60992.
I0302 18:59:12.193425 22535416901760 run.py:483] Algo bellman_ford step 1906 current loss 0.536864, current_train_items 61024.
I0302 18:59:12.215674 22535416901760 run.py:483] Algo bellman_ford step 1907 current loss 0.722938, current_train_items 61056.
I0302 18:59:12.244524 22535416901760 run.py:483] Algo bellman_ford step 1908 current loss 0.851813, current_train_items 61088.
I0302 18:59:12.276041 22535416901760 run.py:483] Algo bellman_ford step 1909 current loss 0.918086, current_train_items 61120.
I0302 18:59:12.294966 22535416901760 run.py:483] Algo bellman_ford step 1910 current loss 0.356077, current_train_items 61152.
I0302 18:59:12.311288 22535416901760 run.py:483] Algo bellman_ford step 1911 current loss 0.572617, current_train_items 61184.
I0302 18:59:12.335109 22535416901760 run.py:483] Algo bellman_ford step 1912 current loss 0.738565, current_train_items 61216.
I0302 18:59:12.364024 22535416901760 run.py:483] Algo bellman_ford step 1913 current loss 0.804331, current_train_items 61248.
I0302 18:59:12.396415 22535416901760 run.py:483] Algo bellman_ford step 1914 current loss 0.839330, current_train_items 61280.
I0302 18:59:12.415115 22535416901760 run.py:483] Algo bellman_ford step 1915 current loss 0.292754, current_train_items 61312.
I0302 18:59:12.431472 22535416901760 run.py:483] Algo bellman_ford step 1916 current loss 0.566762, current_train_items 61344.
I0302 18:59:12.454809 22535416901760 run.py:483] Algo bellman_ford step 1917 current loss 0.739545, current_train_items 61376.
I0302 18:59:12.484560 22535416901760 run.py:483] Algo bellman_ford step 1918 current loss 0.849484, current_train_items 61408.
I0302 18:59:12.517864 22535416901760 run.py:483] Algo bellman_ford step 1919 current loss 1.055888, current_train_items 61440.
I0302 18:59:12.536665 22535416901760 run.py:483] Algo bellman_ford step 1920 current loss 0.411652, current_train_items 61472.
I0302 18:59:12.553000 22535416901760 run.py:483] Algo bellman_ford step 1921 current loss 0.647688, current_train_items 61504.
I0302 18:59:12.575585 22535416901760 run.py:483] Algo bellman_ford step 1922 current loss 0.756284, current_train_items 61536.
I0302 18:59:12.603924 22535416901760 run.py:483] Algo bellman_ford step 1923 current loss 0.772447, current_train_items 61568.
I0302 18:59:12.636076 22535416901760 run.py:483] Algo bellman_ford step 1924 current loss 1.067348, current_train_items 61600.
I0302 18:59:12.654796 22535416901760 run.py:483] Algo bellman_ford step 1925 current loss 0.326029, current_train_items 61632.
I0302 18:59:12.670998 22535416901760 run.py:483] Algo bellman_ford step 1926 current loss 0.577115, current_train_items 61664.
I0302 18:59:12.692861 22535416901760 run.py:483] Algo bellman_ford step 1927 current loss 0.579471, current_train_items 61696.
I0302 18:59:12.721529 22535416901760 run.py:483] Algo bellman_ford step 1928 current loss 0.748050, current_train_items 61728.
I0302 18:59:12.751856 22535416901760 run.py:483] Algo bellman_ford step 1929 current loss 1.006380, current_train_items 61760.
I0302 18:59:12.770877 22535416901760 run.py:483] Algo bellman_ford step 1930 current loss 0.345408, current_train_items 61792.
I0302 18:59:12.786478 22535416901760 run.py:483] Algo bellman_ford step 1931 current loss 0.412774, current_train_items 61824.
I0302 18:59:12.810513 22535416901760 run.py:483] Algo bellman_ford step 1932 current loss 0.807803, current_train_items 61856.
I0302 18:59:12.839628 22535416901760 run.py:483] Algo bellman_ford step 1933 current loss 0.976664, current_train_items 61888.
I0302 18:59:12.870105 22535416901760 run.py:483] Algo bellman_ford step 1934 current loss 0.911578, current_train_items 61920.
I0302 18:59:12.888882 22535416901760 run.py:483] Algo bellman_ford step 1935 current loss 0.383783, current_train_items 61952.
I0302 18:59:12.905050 22535416901760 run.py:483] Algo bellman_ford step 1936 current loss 0.505631, current_train_items 61984.
I0302 18:59:12.928571 22535416901760 run.py:483] Algo bellman_ford step 1937 current loss 0.639638, current_train_items 62016.
I0302 18:59:12.956889 22535416901760 run.py:483] Algo bellman_ford step 1938 current loss 0.721215, current_train_items 62048.
I0302 18:59:12.989755 22535416901760 run.py:483] Algo bellman_ford step 1939 current loss 1.126969, current_train_items 62080.
I0302 18:59:13.008681 22535416901760 run.py:483] Algo bellman_ford step 1940 current loss 0.340080, current_train_items 62112.
I0302 18:59:13.024920 22535416901760 run.py:483] Algo bellman_ford step 1941 current loss 0.478690, current_train_items 62144.
I0302 18:59:13.047592 22535416901760 run.py:483] Algo bellman_ford step 1942 current loss 0.747400, current_train_items 62176.
I0302 18:59:13.075696 22535416901760 run.py:483] Algo bellman_ford step 1943 current loss 0.739697, current_train_items 62208.
I0302 18:59:13.107659 22535416901760 run.py:483] Algo bellman_ford step 1944 current loss 0.938078, current_train_items 62240.
I0302 18:59:13.126574 22535416901760 run.py:483] Algo bellman_ford step 1945 current loss 0.365763, current_train_items 62272.
I0302 18:59:13.142836 22535416901760 run.py:483] Algo bellman_ford step 1946 current loss 0.532609, current_train_items 62304.
I0302 18:59:13.165924 22535416901760 run.py:483] Algo bellman_ford step 1947 current loss 0.694769, current_train_items 62336.
I0302 18:59:13.195856 22535416901760 run.py:483] Algo bellman_ford step 1948 current loss 0.942600, current_train_items 62368.
I0302 18:59:13.226857 22535416901760 run.py:483] Algo bellman_ford step 1949 current loss 0.908906, current_train_items 62400.
I0302 18:59:13.245787 22535416901760 run.py:483] Algo bellman_ford step 1950 current loss 0.328668, current_train_items 62432.
I0302 18:59:13.253837 22535416901760 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.859375, 'score': 0.859375, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:59:13.253944 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.859, val scores are: bellman_ford: 0.859
I0302 18:59:13.270875 22535416901760 run.py:483] Algo bellman_ford step 1951 current loss 0.839651, current_train_items 62464.
I0302 18:59:13.294485 22535416901760 run.py:483] Algo bellman_ford step 1952 current loss 0.963511, current_train_items 62496.
I0302 18:59:13.325567 22535416901760 run.py:483] Algo bellman_ford step 1953 current loss 1.231495, current_train_items 62528.
I0302 18:59:13.357662 22535416901760 run.py:483] Algo bellman_ford step 1954 current loss 1.067183, current_train_items 62560.
I0302 18:59:13.377067 22535416901760 run.py:483] Algo bellman_ford step 1955 current loss 0.383386, current_train_items 62592.
I0302 18:59:13.392202 22535416901760 run.py:483] Algo bellman_ford step 1956 current loss 0.442800, current_train_items 62624.
I0302 18:59:13.415583 22535416901760 run.py:483] Algo bellman_ford step 1957 current loss 0.715997, current_train_items 62656.
I0302 18:59:13.446185 22535416901760 run.py:483] Algo bellman_ford step 1958 current loss 0.933012, current_train_items 62688.
I0302 18:59:13.478719 22535416901760 run.py:483] Algo bellman_ford step 1959 current loss 0.862416, current_train_items 62720.
I0302 18:59:13.498122 22535416901760 run.py:483] Algo bellman_ford step 1960 current loss 0.425512, current_train_items 62752.
I0302 18:59:13.514358 22535416901760 run.py:483] Algo bellman_ford step 1961 current loss 0.615262, current_train_items 62784.
I0302 18:59:13.536824 22535416901760 run.py:483] Algo bellman_ford step 1962 current loss 0.700621, current_train_items 62816.
I0302 18:59:13.565596 22535416901760 run.py:483] Algo bellman_ford step 1963 current loss 0.744927, current_train_items 62848.
I0302 18:59:13.599932 22535416901760 run.py:483] Algo bellman_ford step 1964 current loss 0.948339, current_train_items 62880.
I0302 18:59:13.618862 22535416901760 run.py:483] Algo bellman_ford step 1965 current loss 0.365693, current_train_items 62912.
I0302 18:59:13.634835 22535416901760 run.py:483] Algo bellman_ford step 1966 current loss 0.474208, current_train_items 62944.
I0302 18:59:13.657705 22535416901760 run.py:483] Algo bellman_ford step 1967 current loss 0.728334, current_train_items 62976.
I0302 18:59:13.687088 22535416901760 run.py:483] Algo bellman_ford step 1968 current loss 0.745528, current_train_items 63008.
I0302 18:59:13.718964 22535416901760 run.py:483] Algo bellman_ford step 1969 current loss 0.893864, current_train_items 63040.
I0302 18:59:13.738031 22535416901760 run.py:483] Algo bellman_ford step 1970 current loss 0.322484, current_train_items 63072.
I0302 18:59:13.754148 22535416901760 run.py:483] Algo bellman_ford step 1971 current loss 0.555383, current_train_items 63104.
I0302 18:59:13.776216 22535416901760 run.py:483] Algo bellman_ford step 1972 current loss 0.876426, current_train_items 63136.
I0302 18:59:13.805365 22535416901760 run.py:483] Algo bellman_ford step 1973 current loss 0.904112, current_train_items 63168.
I0302 18:59:13.841161 22535416901760 run.py:483] Algo bellman_ford step 1974 current loss 1.198172, current_train_items 63200.
I0302 18:59:13.860236 22535416901760 run.py:483] Algo bellman_ford step 1975 current loss 0.299013, current_train_items 63232.
I0302 18:59:13.876211 22535416901760 run.py:483] Algo bellman_ford step 1976 current loss 0.494191, current_train_items 63264.
I0302 18:59:13.897958 22535416901760 run.py:483] Algo bellman_ford step 1977 current loss 0.733658, current_train_items 63296.
I0302 18:59:13.926517 22535416901760 run.py:483] Algo bellman_ford step 1978 current loss 1.044582, current_train_items 63328.
I0302 18:59:13.959290 22535416901760 run.py:483] Algo bellman_ford step 1979 current loss 1.378485, current_train_items 63360.
I0302 18:59:13.978444 22535416901760 run.py:483] Algo bellman_ford step 1980 current loss 0.382268, current_train_items 63392.
I0302 18:59:13.994784 22535416901760 run.py:483] Algo bellman_ford step 1981 current loss 0.727130, current_train_items 63424.
I0302 18:59:14.017629 22535416901760 run.py:483] Algo bellman_ford step 1982 current loss 0.977150, current_train_items 63456.
I0302 18:59:14.045747 22535416901760 run.py:483] Algo bellman_ford step 1983 current loss 0.958326, current_train_items 63488.
I0302 18:59:14.081003 22535416901760 run.py:483] Algo bellman_ford step 1984 current loss 1.255630, current_train_items 63520.
I0302 18:59:14.100209 22535416901760 run.py:483] Algo bellman_ford step 1985 current loss 0.312878, current_train_items 63552.
I0302 18:59:14.116448 22535416901760 run.py:483] Algo bellman_ford step 1986 current loss 0.566849, current_train_items 63584.
I0302 18:59:14.138173 22535416901760 run.py:483] Algo bellman_ford step 1987 current loss 0.757005, current_train_items 63616.
I0302 18:59:14.166369 22535416901760 run.py:483] Algo bellman_ford step 1988 current loss 0.866983, current_train_items 63648.
I0302 18:59:14.199612 22535416901760 run.py:483] Algo bellman_ford step 1989 current loss 1.208732, current_train_items 63680.
I0302 18:59:14.218708 22535416901760 run.py:483] Algo bellman_ford step 1990 current loss 0.466596, current_train_items 63712.
I0302 18:59:14.234924 22535416901760 run.py:483] Algo bellman_ford step 1991 current loss 0.836197, current_train_items 63744.
I0302 18:59:14.257772 22535416901760 run.py:483] Algo bellman_ford step 1992 current loss 0.891807, current_train_items 63776.
I0302 18:59:14.286906 22535416901760 run.py:483] Algo bellman_ford step 1993 current loss 0.799116, current_train_items 63808.
I0302 18:59:14.315103 22535416901760 run.py:483] Algo bellman_ford step 1994 current loss 0.723392, current_train_items 63840.
I0302 18:59:14.334243 22535416901760 run.py:483] Algo bellman_ford step 1995 current loss 0.371816, current_train_items 63872.
I0302 18:59:14.350948 22535416901760 run.py:483] Algo bellman_ford step 1996 current loss 0.680595, current_train_items 63904.
I0302 18:59:14.373344 22535416901760 run.py:483] Algo bellman_ford step 1997 current loss 0.687886, current_train_items 63936.
I0302 18:59:14.404042 22535416901760 run.py:483] Algo bellman_ford step 1998 current loss 0.976448, current_train_items 63968.
I0302 18:59:14.435439 22535416901760 run.py:483] Algo bellman_ford step 1999 current loss 0.991526, current_train_items 64000.
I0302 18:59:14.454515 22535416901760 run.py:483] Algo bellman_ford step 2000 current loss 0.329186, current_train_items 64032.
I0302 18:59:14.462580 22535416901760 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:59:14.462687 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 18:59:14.479319 22535416901760 run.py:483] Algo bellman_ford step 2001 current loss 0.465366, current_train_items 64064.
I0302 18:59:14.503138 22535416901760 run.py:483] Algo bellman_ford step 2002 current loss 0.869778, current_train_items 64096.
I0302 18:59:14.532814 22535416901760 run.py:483] Algo bellman_ford step 2003 current loss 0.731025, current_train_items 64128.
I0302 18:59:14.565188 22535416901760 run.py:483] Algo bellman_ford step 2004 current loss 0.825626, current_train_items 64160.
I0302 18:59:14.584570 22535416901760 run.py:483] Algo bellman_ford step 2005 current loss 0.369211, current_train_items 64192.
I0302 18:59:14.600769 22535416901760 run.py:483] Algo bellman_ford step 2006 current loss 0.563056, current_train_items 64224.
I0302 18:59:14.623695 22535416901760 run.py:483] Algo bellman_ford step 2007 current loss 0.716259, current_train_items 64256.
I0302 18:59:14.653605 22535416901760 run.py:483] Algo bellman_ford step 2008 current loss 0.821660, current_train_items 64288.
I0302 18:59:14.684885 22535416901760 run.py:483] Algo bellman_ford step 2009 current loss 0.843192, current_train_items 64320.
I0302 18:59:14.703921 22535416901760 run.py:483] Algo bellman_ford step 2010 current loss 0.346041, current_train_items 64352.
I0302 18:59:14.720242 22535416901760 run.py:483] Algo bellman_ford step 2011 current loss 0.512074, current_train_items 64384.
I0302 18:59:14.743481 22535416901760 run.py:483] Algo bellman_ford step 2012 current loss 0.703219, current_train_items 64416.
I0302 18:59:14.772019 22535416901760 run.py:483] Algo bellman_ford step 2013 current loss 0.820097, current_train_items 64448.
I0302 18:59:14.803978 22535416901760 run.py:483] Algo bellman_ford step 2014 current loss 0.970026, current_train_items 64480.
I0302 18:59:14.822873 22535416901760 run.py:483] Algo bellman_ford step 2015 current loss 0.343449, current_train_items 64512.
I0302 18:59:14.839565 22535416901760 run.py:483] Algo bellman_ford step 2016 current loss 0.594468, current_train_items 64544.
I0302 18:59:14.862913 22535416901760 run.py:483] Algo bellman_ford step 2017 current loss 0.841799, current_train_items 64576.
I0302 18:59:14.890317 22535416901760 run.py:483] Algo bellman_ford step 2018 current loss 0.732942, current_train_items 64608.
I0302 18:59:14.923462 22535416901760 run.py:483] Algo bellman_ford step 2019 current loss 0.987381, current_train_items 64640.
I0302 18:59:14.942243 22535416901760 run.py:483] Algo bellman_ford step 2020 current loss 0.327870, current_train_items 64672.
I0302 18:59:14.958344 22535416901760 run.py:483] Algo bellman_ford step 2021 current loss 0.497461, current_train_items 64704.
I0302 18:59:14.981698 22535416901760 run.py:483] Algo bellman_ford step 2022 current loss 0.839872, current_train_items 64736.
I0302 18:59:15.010869 22535416901760 run.py:483] Algo bellman_ford step 2023 current loss 0.779356, current_train_items 64768.
I0302 18:59:15.046161 22535416901760 run.py:483] Algo bellman_ford step 2024 current loss 1.084423, current_train_items 64800.
I0302 18:59:15.065389 22535416901760 run.py:483] Algo bellman_ford step 2025 current loss 0.383059, current_train_items 64832.
I0302 18:59:15.081400 22535416901760 run.py:483] Algo bellman_ford step 2026 current loss 0.473742, current_train_items 64864.
I0302 18:59:15.104511 22535416901760 run.py:483] Algo bellman_ford step 2027 current loss 0.754911, current_train_items 64896.
I0302 18:59:15.133405 22535416901760 run.py:483] Algo bellman_ford step 2028 current loss 0.777484, current_train_items 64928.
I0302 18:59:15.165110 22535416901760 run.py:483] Algo bellman_ford step 2029 current loss 0.855183, current_train_items 64960.
I0302 18:59:15.184169 22535416901760 run.py:483] Algo bellman_ford step 2030 current loss 0.414397, current_train_items 64992.
I0302 18:59:15.200768 22535416901760 run.py:483] Algo bellman_ford step 2031 current loss 0.625239, current_train_items 65024.
I0302 18:59:15.223276 22535416901760 run.py:483] Algo bellman_ford step 2032 current loss 0.780616, current_train_items 65056.
I0302 18:59:15.253445 22535416901760 run.py:483] Algo bellman_ford step 2033 current loss 0.902335, current_train_items 65088.
I0302 18:59:15.284513 22535416901760 run.py:483] Algo bellman_ford step 2034 current loss 1.000777, current_train_items 65120.
I0302 18:59:15.303951 22535416901760 run.py:483] Algo bellman_ford step 2035 current loss 0.435987, current_train_items 65152.
I0302 18:59:15.319932 22535416901760 run.py:483] Algo bellman_ford step 2036 current loss 0.544646, current_train_items 65184.
I0302 18:59:15.342294 22535416901760 run.py:483] Algo bellman_ford step 2037 current loss 0.813889, current_train_items 65216.
I0302 18:59:15.371541 22535416901760 run.py:483] Algo bellman_ford step 2038 current loss 0.860129, current_train_items 65248.
I0302 18:59:15.404076 22535416901760 run.py:483] Algo bellman_ford step 2039 current loss 0.882276, current_train_items 65280.
I0302 18:59:15.423210 22535416901760 run.py:483] Algo bellman_ford step 2040 current loss 0.377428, current_train_items 65312.
I0302 18:59:15.439202 22535416901760 run.py:483] Algo bellman_ford step 2041 current loss 0.550299, current_train_items 65344.
I0302 18:59:15.463011 22535416901760 run.py:483] Algo bellman_ford step 2042 current loss 0.883212, current_train_items 65376.
I0302 18:59:15.492433 22535416901760 run.py:483] Algo bellman_ford step 2043 current loss 0.748872, current_train_items 65408.
I0302 18:59:15.519587 22535416901760 run.py:483] Algo bellman_ford step 2044 current loss 0.776733, current_train_items 65440.
I0302 18:59:15.538836 22535416901760 run.py:483] Algo bellman_ford step 2045 current loss 0.358078, current_train_items 65472.
I0302 18:59:15.555658 22535416901760 run.py:483] Algo bellman_ford step 2046 current loss 0.597399, current_train_items 65504.
I0302 18:59:15.578664 22535416901760 run.py:483] Algo bellman_ford step 2047 current loss 0.833128, current_train_items 65536.
I0302 18:59:15.607730 22535416901760 run.py:483] Algo bellman_ford step 2048 current loss 0.827203, current_train_items 65568.
I0302 18:59:15.637584 22535416901760 run.py:483] Algo bellman_ford step 2049 current loss 1.060298, current_train_items 65600.
I0302 18:59:15.656678 22535416901760 run.py:483] Algo bellman_ford step 2050 current loss 0.425129, current_train_items 65632.
I0302 18:59:15.664816 22535416901760 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:59:15.664950 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:59:15.682284 22535416901760 run.py:483] Algo bellman_ford step 2051 current loss 0.559951, current_train_items 65664.
I0302 18:59:15.705297 22535416901760 run.py:483] Algo bellman_ford step 2052 current loss 0.634677, current_train_items 65696.
I0302 18:59:15.734318 22535416901760 run.py:483] Algo bellman_ford step 2053 current loss 0.811031, current_train_items 65728.
I0302 18:59:15.769837 22535416901760 run.py:483] Algo bellman_ford step 2054 current loss 1.110057, current_train_items 65760.
I0302 18:59:15.789356 22535416901760 run.py:483] Algo bellman_ford step 2055 current loss 0.347079, current_train_items 65792.
I0302 18:59:15.805191 22535416901760 run.py:483] Algo bellman_ford step 2056 current loss 0.492797, current_train_items 65824.
I0302 18:59:15.827697 22535416901760 run.py:483] Algo bellman_ford step 2057 current loss 0.757376, current_train_items 65856.
I0302 18:59:15.856440 22535416901760 run.py:483] Algo bellman_ford step 2058 current loss 0.754119, current_train_items 65888.
I0302 18:59:15.889128 22535416901760 run.py:483] Algo bellman_ford step 2059 current loss 1.109555, current_train_items 65920.
I0302 18:59:15.908485 22535416901760 run.py:483] Algo bellman_ford step 2060 current loss 0.389427, current_train_items 65952.
I0302 18:59:15.924844 22535416901760 run.py:483] Algo bellman_ford step 2061 current loss 0.488569, current_train_items 65984.
I0302 18:59:15.947640 22535416901760 run.py:483] Algo bellman_ford step 2062 current loss 0.684406, current_train_items 66016.
I0302 18:59:15.977280 22535416901760 run.py:483] Algo bellman_ford step 2063 current loss 0.760621, current_train_items 66048.
I0302 18:59:16.011377 22535416901760 run.py:483] Algo bellman_ford step 2064 current loss 0.931311, current_train_items 66080.
I0302 18:59:16.030262 22535416901760 run.py:483] Algo bellman_ford step 2065 current loss 0.346781, current_train_items 66112.
I0302 18:59:16.046618 22535416901760 run.py:483] Algo bellman_ford step 2066 current loss 0.519826, current_train_items 66144.
I0302 18:59:16.069673 22535416901760 run.py:483] Algo bellman_ford step 2067 current loss 0.675792, current_train_items 66176.
I0302 18:59:16.099397 22535416901760 run.py:483] Algo bellman_ford step 2068 current loss 0.744792, current_train_items 66208.
I0302 18:59:16.131313 22535416901760 run.py:483] Algo bellman_ford step 2069 current loss 0.926721, current_train_items 66240.
I0302 18:59:16.150778 22535416901760 run.py:483] Algo bellman_ford step 2070 current loss 0.353411, current_train_items 66272.
I0302 18:59:16.167335 22535416901760 run.py:483] Algo bellman_ford step 2071 current loss 0.532592, current_train_items 66304.
I0302 18:59:16.190251 22535416901760 run.py:483] Algo bellman_ford step 2072 current loss 0.695055, current_train_items 66336.
I0302 18:59:16.219745 22535416901760 run.py:483] Algo bellman_ford step 2073 current loss 0.868846, current_train_items 66368.
I0302 18:59:16.252034 22535416901760 run.py:483] Algo bellman_ford step 2074 current loss 0.928852, current_train_items 66400.
I0302 18:59:16.271266 22535416901760 run.py:483] Algo bellman_ford step 2075 current loss 0.384739, current_train_items 66432.
I0302 18:59:16.288025 22535416901760 run.py:483] Algo bellman_ford step 2076 current loss 0.604372, current_train_items 66464.
I0302 18:59:16.311872 22535416901760 run.py:483] Algo bellman_ford step 2077 current loss 0.689501, current_train_items 66496.
I0302 18:59:16.339366 22535416901760 run.py:483] Algo bellman_ford step 2078 current loss 0.814704, current_train_items 66528.
I0302 18:59:16.370976 22535416901760 run.py:483] Algo bellman_ford step 2079 current loss 0.870815, current_train_items 66560.
I0302 18:59:16.389955 22535416901760 run.py:483] Algo bellman_ford step 2080 current loss 0.303670, current_train_items 66592.
I0302 18:59:16.406206 22535416901760 run.py:483] Algo bellman_ford step 2081 current loss 0.494496, current_train_items 66624.
I0302 18:59:16.429288 22535416901760 run.py:483] Algo bellman_ford step 2082 current loss 0.670387, current_train_items 66656.
I0302 18:59:16.459793 22535416901760 run.py:483] Algo bellman_ford step 2083 current loss 0.856200, current_train_items 66688.
I0302 18:59:16.492444 22535416901760 run.py:483] Algo bellman_ford step 2084 current loss 0.925406, current_train_items 66720.
I0302 18:59:16.511847 22535416901760 run.py:483] Algo bellman_ford step 2085 current loss 0.363814, current_train_items 66752.
I0302 18:59:16.528124 22535416901760 run.py:483] Algo bellman_ford step 2086 current loss 0.684022, current_train_items 66784.
I0302 18:59:16.551097 22535416901760 run.py:483] Algo bellman_ford step 2087 current loss 0.801656, current_train_items 66816.
I0302 18:59:16.578432 22535416901760 run.py:483] Algo bellman_ford step 2088 current loss 0.675853, current_train_items 66848.
I0302 18:59:16.610812 22535416901760 run.py:483] Algo bellman_ford step 2089 current loss 1.026520, current_train_items 66880.
I0302 18:59:16.630287 22535416901760 run.py:483] Algo bellman_ford step 2090 current loss 0.356421, current_train_items 66912.
I0302 18:59:16.646675 22535416901760 run.py:483] Algo bellman_ford step 2091 current loss 0.573357, current_train_items 66944.
I0302 18:59:16.669347 22535416901760 run.py:483] Algo bellman_ford step 2092 current loss 0.699900, current_train_items 66976.
I0302 18:59:16.698391 22535416901760 run.py:483] Algo bellman_ford step 2093 current loss 0.770762, current_train_items 67008.
I0302 18:59:16.730542 22535416901760 run.py:483] Algo bellman_ford step 2094 current loss 0.955607, current_train_items 67040.
I0302 18:59:16.749664 22535416901760 run.py:483] Algo bellman_ford step 2095 current loss 0.357194, current_train_items 67072.
I0302 18:59:16.766035 22535416901760 run.py:483] Algo bellman_ford step 2096 current loss 0.527142, current_train_items 67104.
I0302 18:59:16.788730 22535416901760 run.py:483] Algo bellman_ford step 2097 current loss 0.733326, current_train_items 67136.
I0302 18:59:16.817533 22535416901760 run.py:483] Algo bellman_ford step 2098 current loss 0.689576, current_train_items 67168.
I0302 18:59:16.849117 22535416901760 run.py:483] Algo bellman_ford step 2099 current loss 0.902890, current_train_items 67200.
I0302 18:59:16.868594 22535416901760 run.py:483] Algo bellman_ford step 2100 current loss 0.330624, current_train_items 67232.
I0302 18:59:16.876498 22535416901760 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:59:16.876604 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:16.893243 22535416901760 run.py:483] Algo bellman_ford step 2101 current loss 0.487717, current_train_items 67264.
I0302 18:59:16.916457 22535416901760 run.py:483] Algo bellman_ford step 2102 current loss 0.722451, current_train_items 67296.
I0302 18:59:16.946307 22535416901760 run.py:483] Algo bellman_ford step 2103 current loss 0.845598, current_train_items 67328.
I0302 18:59:16.978121 22535416901760 run.py:483] Algo bellman_ford step 2104 current loss 0.936965, current_train_items 67360.
I0302 18:59:16.997614 22535416901760 run.py:483] Algo bellman_ford step 2105 current loss 0.386168, current_train_items 67392.
I0302 18:59:17.012779 22535416901760 run.py:483] Algo bellman_ford step 2106 current loss 0.418872, current_train_items 67424.
I0302 18:59:17.036249 22535416901760 run.py:483] Algo bellman_ford step 2107 current loss 0.753012, current_train_items 67456.
I0302 18:59:17.066833 22535416901760 run.py:483] Algo bellman_ford step 2108 current loss 0.910327, current_train_items 67488.
I0302 18:59:17.097595 22535416901760 run.py:483] Algo bellman_ford step 2109 current loss 0.790169, current_train_items 67520.
I0302 18:59:17.116682 22535416901760 run.py:483] Algo bellman_ford step 2110 current loss 0.406129, current_train_items 67552.
I0302 18:59:17.132851 22535416901760 run.py:483] Algo bellman_ford step 2111 current loss 0.611355, current_train_items 67584.
I0302 18:59:17.155192 22535416901760 run.py:483] Algo bellman_ford step 2112 current loss 0.825243, current_train_items 67616.
I0302 18:59:17.183586 22535416901760 run.py:483] Algo bellman_ford step 2113 current loss 0.771099, current_train_items 67648.
I0302 18:59:17.214275 22535416901760 run.py:483] Algo bellman_ford step 2114 current loss 0.893254, current_train_items 67680.
I0302 18:59:17.233180 22535416901760 run.py:483] Algo bellman_ford step 2115 current loss 0.391781, current_train_items 67712.
I0302 18:59:17.249300 22535416901760 run.py:483] Algo bellman_ford step 2116 current loss 0.506154, current_train_items 67744.
I0302 18:59:17.272848 22535416901760 run.py:483] Algo bellman_ford step 2117 current loss 0.798241, current_train_items 67776.
I0302 18:59:17.302479 22535416901760 run.py:483] Algo bellman_ford step 2118 current loss 0.882351, current_train_items 67808.
I0302 18:59:17.333503 22535416901760 run.py:483] Algo bellman_ford step 2119 current loss 0.904510, current_train_items 67840.
I0302 18:59:17.352548 22535416901760 run.py:483] Algo bellman_ford step 2120 current loss 0.316903, current_train_items 67872.
I0302 18:59:17.368482 22535416901760 run.py:483] Algo bellman_ford step 2121 current loss 0.545767, current_train_items 67904.
I0302 18:59:17.391235 22535416901760 run.py:483] Algo bellman_ford step 2122 current loss 0.692084, current_train_items 67936.
I0302 18:59:17.420783 22535416901760 run.py:483] Algo bellman_ford step 2123 current loss 0.834009, current_train_items 67968.
I0302 18:59:17.449011 22535416901760 run.py:483] Algo bellman_ford step 2124 current loss 0.920623, current_train_items 68000.
I0302 18:59:17.467855 22535416901760 run.py:483] Algo bellman_ford step 2125 current loss 0.287309, current_train_items 68032.
I0302 18:59:17.484437 22535416901760 run.py:483] Algo bellman_ford step 2126 current loss 0.563577, current_train_items 68064.
I0302 18:59:17.508748 22535416901760 run.py:483] Algo bellman_ford step 2127 current loss 0.771329, current_train_items 68096.
I0302 18:59:17.537377 22535416901760 run.py:483] Algo bellman_ford step 2128 current loss 0.785510, current_train_items 68128.
I0302 18:59:17.568863 22535416901760 run.py:483] Algo bellman_ford step 2129 current loss 0.810786, current_train_items 68160.
I0302 18:59:17.588161 22535416901760 run.py:483] Algo bellman_ford step 2130 current loss 0.371423, current_train_items 68192.
I0302 18:59:17.604925 22535416901760 run.py:483] Algo bellman_ford step 2131 current loss 0.584498, current_train_items 68224.
I0302 18:59:17.628369 22535416901760 run.py:483] Algo bellman_ford step 2132 current loss 0.695249, current_train_items 68256.
I0302 18:59:17.657072 22535416901760 run.py:483] Algo bellman_ford step 2133 current loss 0.833693, current_train_items 68288.
I0302 18:59:17.690042 22535416901760 run.py:483] Algo bellman_ford step 2134 current loss 0.950541, current_train_items 68320.
I0302 18:59:17.709036 22535416901760 run.py:483] Algo bellman_ford step 2135 current loss 0.350685, current_train_items 68352.
I0302 18:59:17.725437 22535416901760 run.py:483] Algo bellman_ford step 2136 current loss 0.460791, current_train_items 68384.
I0302 18:59:17.748080 22535416901760 run.py:483] Algo bellman_ford step 2137 current loss 0.688870, current_train_items 68416.
I0302 18:59:17.777587 22535416901760 run.py:483] Algo bellman_ford step 2138 current loss 0.786826, current_train_items 68448.
I0302 18:59:17.812357 22535416901760 run.py:483] Algo bellman_ford step 2139 current loss 1.004933, current_train_items 68480.
I0302 18:59:17.831177 22535416901760 run.py:483] Algo bellman_ford step 2140 current loss 0.469979, current_train_items 68512.
I0302 18:59:17.847093 22535416901760 run.py:483] Algo bellman_ford step 2141 current loss 0.494807, current_train_items 68544.
I0302 18:59:17.868437 22535416901760 run.py:483] Algo bellman_ford step 2142 current loss 0.549277, current_train_items 68576.
I0302 18:59:17.897348 22535416901760 run.py:483] Algo bellman_ford step 2143 current loss 0.706149, current_train_items 68608.
I0302 18:59:17.929401 22535416901760 run.py:483] Algo bellman_ford step 2144 current loss 0.980885, current_train_items 68640.
I0302 18:59:17.948307 22535416901760 run.py:483] Algo bellman_ford step 2145 current loss 0.288639, current_train_items 68672.
I0302 18:59:17.964937 22535416901760 run.py:483] Algo bellman_ford step 2146 current loss 0.514513, current_train_items 68704.
I0302 18:59:17.988507 22535416901760 run.py:483] Algo bellman_ford step 2147 current loss 0.934278, current_train_items 68736.
I0302 18:59:18.018079 22535416901760 run.py:483] Algo bellman_ford step 2148 current loss 0.822382, current_train_items 68768.
I0302 18:59:18.049054 22535416901760 run.py:483] Algo bellman_ford step 2149 current loss 0.794730, current_train_items 68800.
I0302 18:59:18.068017 22535416901760 run.py:483] Algo bellman_ford step 2150 current loss 0.428850, current_train_items 68832.
I0302 18:59:18.075921 22535416901760 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:59:18.076028 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 18:59:18.093018 22535416901760 run.py:483] Algo bellman_ford step 2151 current loss 0.474984, current_train_items 68864.
I0302 18:59:18.116459 22535416901760 run.py:483] Algo bellman_ford step 2152 current loss 0.755167, current_train_items 68896.
I0302 18:59:18.145739 22535416901760 run.py:483] Algo bellman_ford step 2153 current loss 0.911526, current_train_items 68928.
I0302 18:59:18.177596 22535416901760 run.py:483] Algo bellman_ford step 2154 current loss 0.937842, current_train_items 68960.
I0302 18:59:18.197063 22535416901760 run.py:483] Algo bellman_ford step 2155 current loss 0.386390, current_train_items 68992.
I0302 18:59:18.212962 22535416901760 run.py:483] Algo bellman_ford step 2156 current loss 0.483926, current_train_items 69024.
I0302 18:59:18.236290 22535416901760 run.py:483] Algo bellman_ford step 2157 current loss 0.684613, current_train_items 69056.
I0302 18:59:18.265792 22535416901760 run.py:483] Algo bellman_ford step 2158 current loss 0.829903, current_train_items 69088.
I0302 18:59:18.298474 22535416901760 run.py:483] Algo bellman_ford step 2159 current loss 1.166178, current_train_items 69120.
I0302 18:59:18.317984 22535416901760 run.py:483] Algo bellman_ford step 2160 current loss 0.414272, current_train_items 69152.
I0302 18:59:18.334112 22535416901760 run.py:483] Algo bellman_ford step 2161 current loss 0.471452, current_train_items 69184.
I0302 18:59:18.357206 22535416901760 run.py:483] Algo bellman_ford step 2162 current loss 0.696409, current_train_items 69216.
I0302 18:59:18.385126 22535416901760 run.py:483] Algo bellman_ford step 2163 current loss 0.737112, current_train_items 69248.
I0302 18:59:18.418011 22535416901760 run.py:483] Algo bellman_ford step 2164 current loss 0.956582, current_train_items 69280.
I0302 18:59:18.436950 22535416901760 run.py:483] Algo bellman_ford step 2165 current loss 0.359226, current_train_items 69312.
I0302 18:59:18.453660 22535416901760 run.py:483] Algo bellman_ford step 2166 current loss 0.612999, current_train_items 69344.
I0302 18:59:18.475976 22535416901760 run.py:483] Algo bellman_ford step 2167 current loss 0.637527, current_train_items 69376.
I0302 18:59:18.504023 22535416901760 run.py:483] Algo bellman_ford step 2168 current loss 0.746564, current_train_items 69408.
I0302 18:59:18.536133 22535416901760 run.py:483] Algo bellman_ford step 2169 current loss 0.942239, current_train_items 69440.
I0302 18:59:18.555340 22535416901760 run.py:483] Algo bellman_ford step 2170 current loss 0.370474, current_train_items 69472.
I0302 18:59:18.571483 22535416901760 run.py:483] Algo bellman_ford step 2171 current loss 0.569461, current_train_items 69504.
I0302 18:59:18.594059 22535416901760 run.py:483] Algo bellman_ford step 2172 current loss 0.723461, current_train_items 69536.
I0302 18:59:18.624196 22535416901760 run.py:483] Algo bellman_ford step 2173 current loss 0.769356, current_train_items 69568.
I0302 18:59:18.654412 22535416901760 run.py:483] Algo bellman_ford step 2174 current loss 0.903793, current_train_items 69600.
I0302 18:59:18.673744 22535416901760 run.py:483] Algo bellman_ford step 2175 current loss 0.361895, current_train_items 69632.
I0302 18:59:18.689559 22535416901760 run.py:483] Algo bellman_ford step 2176 current loss 0.490983, current_train_items 69664.
I0302 18:59:18.713143 22535416901760 run.py:483] Algo bellman_ford step 2177 current loss 0.835519, current_train_items 69696.
I0302 18:59:18.741328 22535416901760 run.py:483] Algo bellman_ford step 2178 current loss 0.752340, current_train_items 69728.
I0302 18:59:18.774090 22535416901760 run.py:483] Algo bellman_ford step 2179 current loss 0.812703, current_train_items 69760.
I0302 18:59:18.792860 22535416901760 run.py:483] Algo bellman_ford step 2180 current loss 0.303658, current_train_items 69792.
I0302 18:59:18.809111 22535416901760 run.py:483] Algo bellman_ford step 2181 current loss 0.507312, current_train_items 69824.
I0302 18:59:18.832000 22535416901760 run.py:483] Algo bellman_ford step 2182 current loss 0.764656, current_train_items 69856.
I0302 18:59:18.860437 22535416901760 run.py:483] Algo bellman_ford step 2183 current loss 0.832449, current_train_items 69888.
I0302 18:59:18.893789 22535416901760 run.py:483] Algo bellman_ford step 2184 current loss 1.073127, current_train_items 69920.
I0302 18:59:18.913225 22535416901760 run.py:483] Algo bellman_ford step 2185 current loss 0.317006, current_train_items 69952.
I0302 18:59:18.928977 22535416901760 run.py:483] Algo bellman_ford step 2186 current loss 0.435295, current_train_items 69984.
I0302 18:59:18.951423 22535416901760 run.py:483] Algo bellman_ford step 2187 current loss 0.691133, current_train_items 70016.
I0302 18:59:18.980594 22535416901760 run.py:483] Algo bellman_ford step 2188 current loss 0.771910, current_train_items 70048.
W0302 18:59:19.005128 22535416901760 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:59:25.638395 22535416901760 run.py:483] Algo bellman_ford step 2189 current loss 1.058342, current_train_items 70080.
I0302 18:59:25.658982 22535416901760 run.py:483] Algo bellman_ford step 2190 current loss 0.326541, current_train_items 70112.
I0302 18:59:25.675750 22535416901760 run.py:483] Algo bellman_ford step 2191 current loss 0.475960, current_train_items 70144.
I0302 18:59:25.698727 22535416901760 run.py:483] Algo bellman_ford step 2192 current loss 0.680955, current_train_items 70176.
W0302 18:59:25.720185 22535416901760 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:59:32.656980 22535416901760 run.py:483] Algo bellman_ford step 2193 current loss 0.994882, current_train_items 70208.
I0302 18:59:32.689209 22535416901760 run.py:483] Algo bellman_ford step 2194 current loss 0.896509, current_train_items 70240.
I0302 18:59:32.709571 22535416901760 run.py:483] Algo bellman_ford step 2195 current loss 0.358407, current_train_items 70272.
I0302 18:59:32.726439 22535416901760 run.py:483] Algo bellman_ford step 2196 current loss 0.534129, current_train_items 70304.
I0302 18:59:32.749448 22535416901760 run.py:483] Algo bellman_ford step 2197 current loss 0.616304, current_train_items 70336.
I0302 18:59:32.778017 22535416901760 run.py:483] Algo bellman_ford step 2198 current loss 0.753222, current_train_items 70368.
I0302 18:59:32.810513 22535416901760 run.py:483] Algo bellman_ford step 2199 current loss 0.817365, current_train_items 70400.
I0302 18:59:32.830444 22535416901760 run.py:483] Algo bellman_ford step 2200 current loss 0.387072, current_train_items 70432.
I0302 18:59:32.839413 22535416901760 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:59:32.839554 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:59:32.856978 22535416901760 run.py:483] Algo bellman_ford step 2201 current loss 0.551882, current_train_items 70464.
I0302 18:59:32.880352 22535416901760 run.py:483] Algo bellman_ford step 2202 current loss 0.604106, current_train_items 70496.
I0302 18:59:32.910458 22535416901760 run.py:483] Algo bellman_ford step 2203 current loss 0.757642, current_train_items 70528.
I0302 18:59:32.944163 22535416901760 run.py:483] Algo bellman_ford step 2204 current loss 0.848239, current_train_items 70560.
I0302 18:59:32.964688 22535416901760 run.py:483] Algo bellman_ford step 2205 current loss 0.272868, current_train_items 70592.
I0302 18:59:32.980851 22535416901760 run.py:483] Algo bellman_ford step 2206 current loss 0.504186, current_train_items 70624.
I0302 18:59:33.004008 22535416901760 run.py:483] Algo bellman_ford step 2207 current loss 0.745035, current_train_items 70656.
I0302 18:59:33.034125 22535416901760 run.py:483] Algo bellman_ford step 2208 current loss 0.885312, current_train_items 70688.
I0302 18:59:33.067979 22535416901760 run.py:483] Algo bellman_ford step 2209 current loss 0.941774, current_train_items 70720.
I0302 18:59:33.087880 22535416901760 run.py:483] Algo bellman_ford step 2210 current loss 0.387670, current_train_items 70752.
I0302 18:59:33.104099 22535416901760 run.py:483] Algo bellman_ford step 2211 current loss 0.561393, current_train_items 70784.
I0302 18:59:33.127944 22535416901760 run.py:483] Algo bellman_ford step 2212 current loss 0.727547, current_train_items 70816.
I0302 18:59:33.158444 22535416901760 run.py:483] Algo bellman_ford step 2213 current loss 0.797785, current_train_items 70848.
I0302 18:59:33.192519 22535416901760 run.py:483] Algo bellman_ford step 2214 current loss 0.878687, current_train_items 70880.
I0302 18:59:33.212394 22535416901760 run.py:483] Algo bellman_ford step 2215 current loss 0.363333, current_train_items 70912.
I0302 18:59:33.228735 22535416901760 run.py:483] Algo bellman_ford step 2216 current loss 0.456367, current_train_items 70944.
I0302 18:59:33.250620 22535416901760 run.py:483] Algo bellman_ford step 2217 current loss 0.666471, current_train_items 70976.
I0302 18:59:33.281507 22535416901760 run.py:483] Algo bellman_ford step 2218 current loss 0.823254, current_train_items 71008.
I0302 18:59:33.313267 22535416901760 run.py:483] Algo bellman_ford step 2219 current loss 0.898941, current_train_items 71040.
I0302 18:59:33.332803 22535416901760 run.py:483] Algo bellman_ford step 2220 current loss 0.303480, current_train_items 71072.
I0302 18:59:33.348986 22535416901760 run.py:483] Algo bellman_ford step 2221 current loss 0.511113, current_train_items 71104.
I0302 18:59:33.371749 22535416901760 run.py:483] Algo bellman_ford step 2222 current loss 0.631417, current_train_items 71136.
I0302 18:59:33.402255 22535416901760 run.py:483] Algo bellman_ford step 2223 current loss 0.848282, current_train_items 71168.
I0302 18:59:33.436407 22535416901760 run.py:483] Algo bellman_ford step 2224 current loss 1.026380, current_train_items 71200.
I0302 18:59:33.455948 22535416901760 run.py:483] Algo bellman_ford step 2225 current loss 0.324880, current_train_items 71232.
I0302 18:59:33.472234 22535416901760 run.py:483] Algo bellman_ford step 2226 current loss 0.511065, current_train_items 71264.
I0302 18:59:33.495474 22535416901760 run.py:483] Algo bellman_ford step 2227 current loss 0.657577, current_train_items 71296.
I0302 18:59:33.525527 22535416901760 run.py:483] Algo bellman_ford step 2228 current loss 0.806594, current_train_items 71328.
I0302 18:59:33.556113 22535416901760 run.py:483] Algo bellman_ford step 2229 current loss 0.791907, current_train_items 71360.
I0302 18:59:33.576077 22535416901760 run.py:483] Algo bellman_ford step 2230 current loss 0.346956, current_train_items 71392.
I0302 18:59:33.592684 22535416901760 run.py:483] Algo bellman_ford step 2231 current loss 0.607110, current_train_items 71424.
I0302 18:59:33.615728 22535416901760 run.py:483] Algo bellman_ford step 2232 current loss 0.794737, current_train_items 71456.
I0302 18:59:33.645398 22535416901760 run.py:483] Algo bellman_ford step 2233 current loss 0.811270, current_train_items 71488.
I0302 18:59:33.677969 22535416901760 run.py:483] Algo bellman_ford step 2234 current loss 1.054581, current_train_items 71520.
I0302 18:59:33.697827 22535416901760 run.py:483] Algo bellman_ford step 2235 current loss 0.308670, current_train_items 71552.
I0302 18:59:33.714358 22535416901760 run.py:483] Algo bellman_ford step 2236 current loss 0.499026, current_train_items 71584.
I0302 18:59:33.738576 22535416901760 run.py:483] Algo bellman_ford step 2237 current loss 0.738417, current_train_items 71616.
I0302 18:59:33.768872 22535416901760 run.py:483] Algo bellman_ford step 2238 current loss 0.808558, current_train_items 71648.
I0302 18:59:33.800523 22535416901760 run.py:483] Algo bellman_ford step 2239 current loss 0.755546, current_train_items 71680.
I0302 18:59:33.820209 22535416901760 run.py:483] Algo bellman_ford step 2240 current loss 0.423206, current_train_items 71712.
I0302 18:59:33.836930 22535416901760 run.py:483] Algo bellman_ford step 2241 current loss 0.552933, current_train_items 71744.
I0302 18:59:33.860801 22535416901760 run.py:483] Algo bellman_ford step 2242 current loss 0.710244, current_train_items 71776.
I0302 18:59:33.891020 22535416901760 run.py:483] Algo bellman_ford step 2243 current loss 0.801737, current_train_items 71808.
I0302 18:59:33.922352 22535416901760 run.py:483] Algo bellman_ford step 2244 current loss 0.690307, current_train_items 71840.
I0302 18:59:33.942297 22535416901760 run.py:483] Algo bellman_ford step 2245 current loss 0.273674, current_train_items 71872.
I0302 18:59:33.958858 22535416901760 run.py:483] Algo bellman_ford step 2246 current loss 0.572591, current_train_items 71904.
I0302 18:59:33.981271 22535416901760 run.py:483] Algo bellman_ford step 2247 current loss 1.124806, current_train_items 71936.
I0302 18:59:34.012670 22535416901760 run.py:483] Algo bellman_ford step 2248 current loss 0.852576, current_train_items 71968.
I0302 18:59:34.045929 22535416901760 run.py:483] Algo bellman_ford step 2249 current loss 0.916571, current_train_items 72000.
I0302 18:59:34.065797 22535416901760 run.py:483] Algo bellman_ford step 2250 current loss 0.358245, current_train_items 72032.
I0302 18:59:34.074354 22535416901760 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.90625, 'score': 0.90625, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:59:34.074462 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.906, val scores are: bellman_ford: 0.906
I0302 18:59:34.091321 22535416901760 run.py:483] Algo bellman_ford step 2251 current loss 0.556630, current_train_items 72064.
I0302 18:59:34.113913 22535416901760 run.py:483] Algo bellman_ford step 2252 current loss 0.621873, current_train_items 72096.
I0302 18:59:34.145498 22535416901760 run.py:483] Algo bellman_ford step 2253 current loss 0.831950, current_train_items 72128.
I0302 18:59:34.179767 22535416901760 run.py:483] Algo bellman_ford step 2254 current loss 0.858989, current_train_items 72160.
I0302 18:59:34.199784 22535416901760 run.py:483] Algo bellman_ford step 2255 current loss 0.353330, current_train_items 72192.
I0302 18:59:34.215917 22535416901760 run.py:483] Algo bellman_ford step 2256 current loss 0.478307, current_train_items 72224.
I0302 18:59:34.238967 22535416901760 run.py:483] Algo bellman_ford step 2257 current loss 0.658449, current_train_items 72256.
I0302 18:59:34.267900 22535416901760 run.py:483] Algo bellman_ford step 2258 current loss 0.739635, current_train_items 72288.
I0302 18:59:34.301756 22535416901760 run.py:483] Algo bellman_ford step 2259 current loss 0.957108, current_train_items 72320.
I0302 18:59:34.321539 22535416901760 run.py:483] Algo bellman_ford step 2260 current loss 0.327052, current_train_items 72352.
I0302 18:59:34.338299 22535416901760 run.py:483] Algo bellman_ford step 2261 current loss 0.556621, current_train_items 72384.
I0302 18:59:34.362389 22535416901760 run.py:483] Algo bellman_ford step 2262 current loss 0.737737, current_train_items 72416.
I0302 18:59:34.391422 22535416901760 run.py:483] Algo bellman_ford step 2263 current loss 0.749001, current_train_items 72448.
I0302 18:59:34.424324 22535416901760 run.py:483] Algo bellman_ford step 2264 current loss 0.840735, current_train_items 72480.
I0302 18:59:34.444012 22535416901760 run.py:483] Algo bellman_ford step 2265 current loss 0.366169, current_train_items 72512.
I0302 18:59:34.460200 22535416901760 run.py:483] Algo bellman_ford step 2266 current loss 0.505357, current_train_items 72544.
I0302 18:59:34.484226 22535416901760 run.py:483] Algo bellman_ford step 2267 current loss 0.822416, current_train_items 72576.
I0302 18:59:34.514261 22535416901760 run.py:483] Algo bellman_ford step 2268 current loss 0.892226, current_train_items 72608.
I0302 18:59:34.546537 22535416901760 run.py:483] Algo bellman_ford step 2269 current loss 0.850210, current_train_items 72640.
I0302 18:59:34.566452 22535416901760 run.py:483] Algo bellman_ford step 2270 current loss 0.331099, current_train_items 72672.
I0302 18:59:34.582513 22535416901760 run.py:483] Algo bellman_ford step 2271 current loss 0.498406, current_train_items 72704.
I0302 18:59:34.604367 22535416901760 run.py:483] Algo bellman_ford step 2272 current loss 0.701382, current_train_items 72736.
I0302 18:59:34.634828 22535416901760 run.py:483] Algo bellman_ford step 2273 current loss 0.902483, current_train_items 72768.
I0302 18:59:34.670124 22535416901760 run.py:483] Algo bellman_ford step 2274 current loss 1.144338, current_train_items 72800.
I0302 18:59:34.690107 22535416901760 run.py:483] Algo bellman_ford step 2275 current loss 0.355284, current_train_items 72832.
I0302 18:59:34.706213 22535416901760 run.py:483] Algo bellman_ford step 2276 current loss 0.526526, current_train_items 72864.
I0302 18:59:34.727467 22535416901760 run.py:483] Algo bellman_ford step 2277 current loss 0.654086, current_train_items 72896.
I0302 18:59:34.756553 22535416901760 run.py:483] Algo bellman_ford step 2278 current loss 0.895102, current_train_items 72928.
I0302 18:59:34.790535 22535416901760 run.py:483] Algo bellman_ford step 2279 current loss 1.139792, current_train_items 72960.
I0302 18:59:34.809859 22535416901760 run.py:483] Algo bellman_ford step 2280 current loss 0.336838, current_train_items 72992.
I0302 18:59:34.825814 22535416901760 run.py:483] Algo bellman_ford step 2281 current loss 0.607019, current_train_items 73024.
I0302 18:59:34.849169 22535416901760 run.py:483] Algo bellman_ford step 2282 current loss 0.634175, current_train_items 73056.
I0302 18:59:34.881695 22535416901760 run.py:483] Algo bellman_ford step 2283 current loss 0.911322, current_train_items 73088.
I0302 18:59:34.915738 22535416901760 run.py:483] Algo bellman_ford step 2284 current loss 0.882894, current_train_items 73120.
I0302 18:59:34.935531 22535416901760 run.py:483] Algo bellman_ford step 2285 current loss 0.354685, current_train_items 73152.
I0302 18:59:34.951804 22535416901760 run.py:483] Algo bellman_ford step 2286 current loss 0.521043, current_train_items 73184.
I0302 18:59:34.974888 22535416901760 run.py:483] Algo bellman_ford step 2287 current loss 0.678564, current_train_items 73216.
I0302 18:59:35.004265 22535416901760 run.py:483] Algo bellman_ford step 2288 current loss 0.782961, current_train_items 73248.
I0302 18:59:35.039471 22535416901760 run.py:483] Algo bellman_ford step 2289 current loss 0.881592, current_train_items 73280.
I0302 18:59:35.059420 22535416901760 run.py:483] Algo bellman_ford step 2290 current loss 0.382666, current_train_items 73312.
I0302 18:59:35.075239 22535416901760 run.py:483] Algo bellman_ford step 2291 current loss 0.432386, current_train_items 73344.
I0302 18:59:35.097577 22535416901760 run.py:483] Algo bellman_ford step 2292 current loss 0.588634, current_train_items 73376.
I0302 18:59:35.128311 22535416901760 run.py:483] Algo bellman_ford step 2293 current loss 0.805757, current_train_items 73408.
I0302 18:59:35.161798 22535416901760 run.py:483] Algo bellman_ford step 2294 current loss 0.987474, current_train_items 73440.
I0302 18:59:35.181256 22535416901760 run.py:483] Algo bellman_ford step 2295 current loss 0.306566, current_train_items 73472.
I0302 18:59:35.197509 22535416901760 run.py:483] Algo bellman_ford step 2296 current loss 0.481960, current_train_items 73504.
I0302 18:59:35.219636 22535416901760 run.py:483] Algo bellman_ford step 2297 current loss 0.583982, current_train_items 73536.
I0302 18:59:35.248755 22535416901760 run.py:483] Algo bellman_ford step 2298 current loss 0.748558, current_train_items 73568.
I0302 18:59:35.281860 22535416901760 run.py:483] Algo bellman_ford step 2299 current loss 0.965070, current_train_items 73600.
I0302 18:59:35.301735 22535416901760 run.py:483] Algo bellman_ford step 2300 current loss 0.295954, current_train_items 73632.
I0302 18:59:35.309854 22535416901760 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:59:35.309961 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 18:59:35.326893 22535416901760 run.py:483] Algo bellman_ford step 2301 current loss 0.615970, current_train_items 73664.
I0302 18:59:35.350750 22535416901760 run.py:483] Algo bellman_ford step 2302 current loss 0.750459, current_train_items 73696.
I0302 18:59:35.382528 22535416901760 run.py:483] Algo bellman_ford step 2303 current loss 0.768783, current_train_items 73728.
I0302 18:59:35.416309 22535416901760 run.py:483] Algo bellman_ford step 2304 current loss 0.931718, current_train_items 73760.
I0302 18:59:35.436330 22535416901760 run.py:483] Algo bellman_ford step 2305 current loss 0.308866, current_train_items 73792.
I0302 18:59:35.452803 22535416901760 run.py:483] Algo bellman_ford step 2306 current loss 0.581181, current_train_items 73824.
I0302 18:59:35.475627 22535416901760 run.py:483] Algo bellman_ford step 2307 current loss 0.691087, current_train_items 73856.
I0302 18:59:35.504954 22535416901760 run.py:483] Algo bellman_ford step 2308 current loss 0.712818, current_train_items 73888.
I0302 18:59:35.538767 22535416901760 run.py:483] Algo bellman_ford step 2309 current loss 0.911844, current_train_items 73920.
I0302 18:59:35.558625 22535416901760 run.py:483] Algo bellman_ford step 2310 current loss 0.363870, current_train_items 73952.
I0302 18:59:35.574903 22535416901760 run.py:483] Algo bellman_ford step 2311 current loss 0.491399, current_train_items 73984.
I0302 18:59:35.598021 22535416901760 run.py:483] Algo bellman_ford step 2312 current loss 0.716968, current_train_items 74016.
I0302 18:59:35.628223 22535416901760 run.py:483] Algo bellman_ford step 2313 current loss 0.759153, current_train_items 74048.
I0302 18:59:35.661772 22535416901760 run.py:483] Algo bellman_ford step 2314 current loss 0.828709, current_train_items 74080.
I0302 18:59:35.681914 22535416901760 run.py:483] Algo bellman_ford step 2315 current loss 0.370593, current_train_items 74112.
I0302 18:59:35.697975 22535416901760 run.py:483] Algo bellman_ford step 2316 current loss 0.541392, current_train_items 74144.
I0302 18:59:35.720787 22535416901760 run.py:483] Algo bellman_ford step 2317 current loss 0.734751, current_train_items 74176.
I0302 18:59:35.751944 22535416901760 run.py:483] Algo bellman_ford step 2318 current loss 0.826577, current_train_items 74208.
I0302 18:59:35.786381 22535416901760 run.py:483] Algo bellman_ford step 2319 current loss 0.918604, current_train_items 74240.
I0302 18:59:35.806161 22535416901760 run.py:483] Algo bellman_ford step 2320 current loss 0.379217, current_train_items 74272.
I0302 18:59:35.822144 22535416901760 run.py:483] Algo bellman_ford step 2321 current loss 0.510305, current_train_items 74304.
I0302 18:59:35.845803 22535416901760 run.py:483] Algo bellman_ford step 2322 current loss 0.774224, current_train_items 74336.
I0302 18:59:35.875771 22535416901760 run.py:483] Algo bellman_ford step 2323 current loss 0.818588, current_train_items 74368.
I0302 18:59:35.909303 22535416901760 run.py:483] Algo bellman_ford step 2324 current loss 0.898004, current_train_items 74400.
I0302 18:59:35.929296 22535416901760 run.py:483] Algo bellman_ford step 2325 current loss 0.319784, current_train_items 74432.
I0302 18:59:35.945315 22535416901760 run.py:483] Algo bellman_ford step 2326 current loss 0.496650, current_train_items 74464.
I0302 18:59:35.967972 22535416901760 run.py:483] Algo bellman_ford step 2327 current loss 0.709011, current_train_items 74496.
I0302 18:59:35.996622 22535416901760 run.py:483] Algo bellman_ford step 2328 current loss 0.677470, current_train_items 74528.
I0302 18:59:36.028753 22535416901760 run.py:483] Algo bellman_ford step 2329 current loss 0.787842, current_train_items 74560.
I0302 18:59:36.048526 22535416901760 run.py:483] Algo bellman_ford step 2330 current loss 0.383897, current_train_items 74592.
I0302 18:59:36.064356 22535416901760 run.py:483] Algo bellman_ford step 2331 current loss 0.531277, current_train_items 74624.
I0302 18:59:36.087781 22535416901760 run.py:483] Algo bellman_ford step 2332 current loss 0.767353, current_train_items 74656.
I0302 18:59:36.118616 22535416901760 run.py:483] Algo bellman_ford step 2333 current loss 0.774999, current_train_items 74688.
I0302 18:59:36.153285 22535416901760 run.py:483] Algo bellman_ford step 2334 current loss 0.799915, current_train_items 74720.
I0302 18:59:36.172812 22535416901760 run.py:483] Algo bellman_ford step 2335 current loss 0.399454, current_train_items 74752.
I0302 18:59:36.189320 22535416901760 run.py:483] Algo bellman_ford step 2336 current loss 0.553035, current_train_items 74784.
I0302 18:59:36.212517 22535416901760 run.py:483] Algo bellman_ford step 2337 current loss 0.646551, current_train_items 74816.
I0302 18:59:36.242308 22535416901760 run.py:483] Algo bellman_ford step 2338 current loss 0.760949, current_train_items 74848.
I0302 18:59:36.274890 22535416901760 run.py:483] Algo bellman_ford step 2339 current loss 0.866083, current_train_items 74880.
I0302 18:59:36.294858 22535416901760 run.py:483] Algo bellman_ford step 2340 current loss 0.407485, current_train_items 74912.
I0302 18:59:36.311341 22535416901760 run.py:483] Algo bellman_ford step 2341 current loss 0.515869, current_train_items 74944.
I0302 18:59:36.333528 22535416901760 run.py:483] Algo bellman_ford step 2342 current loss 0.618826, current_train_items 74976.
I0302 18:59:36.362747 22535416901760 run.py:483] Algo bellman_ford step 2343 current loss 0.746733, current_train_items 75008.
I0302 18:59:36.394227 22535416901760 run.py:483] Algo bellman_ford step 2344 current loss 0.822919, current_train_items 75040.
I0302 18:59:36.413782 22535416901760 run.py:483] Algo bellman_ford step 2345 current loss 0.374501, current_train_items 75072.
I0302 18:59:36.429832 22535416901760 run.py:483] Algo bellman_ford step 2346 current loss 0.511393, current_train_items 75104.
I0302 18:59:36.452828 22535416901760 run.py:483] Algo bellman_ford step 2347 current loss 0.714702, current_train_items 75136.
I0302 18:59:36.481244 22535416901760 run.py:483] Algo bellman_ford step 2348 current loss 0.703038, current_train_items 75168.
I0302 18:59:36.512795 22535416901760 run.py:483] Algo bellman_ford step 2349 current loss 0.787718, current_train_items 75200.
I0302 18:59:36.532254 22535416901760 run.py:483] Algo bellman_ford step 2350 current loss 0.272184, current_train_items 75232.
I0302 18:59:36.540351 22535416901760 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.8583984375, 'score': 0.8583984375, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:59:36.540464 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.858, val scores are: bellman_ford: 0.858
I0302 18:59:36.557324 22535416901760 run.py:483] Algo bellman_ford step 2351 current loss 0.498061, current_train_items 75264.
I0302 18:59:36.580832 22535416901760 run.py:483] Algo bellman_ford step 2352 current loss 0.733451, current_train_items 75296.
I0302 18:59:36.609660 22535416901760 run.py:483] Algo bellman_ford step 2353 current loss 0.792795, current_train_items 75328.
I0302 18:59:36.645098 22535416901760 run.py:483] Algo bellman_ford step 2354 current loss 1.106936, current_train_items 75360.
I0302 18:59:36.665112 22535416901760 run.py:483] Algo bellman_ford step 2355 current loss 0.377547, current_train_items 75392.
I0302 18:59:36.680687 22535416901760 run.py:483] Algo bellman_ford step 2356 current loss 0.558609, current_train_items 75424.
I0302 18:59:36.704221 22535416901760 run.py:483] Algo bellman_ford step 2357 current loss 0.755311, current_train_items 75456.
I0302 18:59:36.732811 22535416901760 run.py:483] Algo bellman_ford step 2358 current loss 0.720031, current_train_items 75488.
I0302 18:59:36.767488 22535416901760 run.py:483] Algo bellman_ford step 2359 current loss 0.972799, current_train_items 75520.
I0302 18:59:36.787187 22535416901760 run.py:483] Algo bellman_ford step 2360 current loss 0.347328, current_train_items 75552.
I0302 18:59:36.804123 22535416901760 run.py:483] Algo bellman_ford step 2361 current loss 0.546041, current_train_items 75584.
I0302 18:59:36.828347 22535416901760 run.py:483] Algo bellman_ford step 2362 current loss 0.625925, current_train_items 75616.
I0302 18:59:36.857830 22535416901760 run.py:483] Algo bellman_ford step 2363 current loss 0.763324, current_train_items 75648.
I0302 18:59:36.890925 22535416901760 run.py:483] Algo bellman_ford step 2364 current loss 0.880326, current_train_items 75680.
I0302 18:59:36.910489 22535416901760 run.py:483] Algo bellman_ford step 2365 current loss 0.296916, current_train_items 75712.
I0302 18:59:36.926708 22535416901760 run.py:483] Algo bellman_ford step 2366 current loss 0.466787, current_train_items 75744.
I0302 18:59:36.950042 22535416901760 run.py:483] Algo bellman_ford step 2367 current loss 0.684858, current_train_items 75776.
I0302 18:59:36.981169 22535416901760 run.py:483] Algo bellman_ford step 2368 current loss 0.744835, current_train_items 75808.
I0302 18:59:37.015089 22535416901760 run.py:483] Algo bellman_ford step 2369 current loss 0.786219, current_train_items 75840.
I0302 18:59:37.035167 22535416901760 run.py:483] Algo bellman_ford step 2370 current loss 0.313976, current_train_items 75872.
I0302 18:59:37.051380 22535416901760 run.py:483] Algo bellman_ford step 2371 current loss 0.532198, current_train_items 75904.
I0302 18:59:37.074030 22535416901760 run.py:483] Algo bellman_ford step 2372 current loss 0.697551, current_train_items 75936.
I0302 18:59:37.103013 22535416901760 run.py:483] Algo bellman_ford step 2373 current loss 0.746429, current_train_items 75968.
I0302 18:59:37.135510 22535416901760 run.py:483] Algo bellman_ford step 2374 current loss 0.786079, current_train_items 76000.
I0302 18:59:37.155509 22535416901760 run.py:483] Algo bellman_ford step 2375 current loss 0.278914, current_train_items 76032.
I0302 18:59:37.171885 22535416901760 run.py:483] Algo bellman_ford step 2376 current loss 0.612912, current_train_items 76064.
I0302 18:59:37.195115 22535416901760 run.py:483] Algo bellman_ford step 2377 current loss 0.693446, current_train_items 76096.
I0302 18:59:37.226346 22535416901760 run.py:483] Algo bellman_ford step 2378 current loss 0.779861, current_train_items 76128.
I0302 18:59:37.260787 22535416901760 run.py:483] Algo bellman_ford step 2379 current loss 0.942616, current_train_items 76160.
I0302 18:59:37.280873 22535416901760 run.py:483] Algo bellman_ford step 2380 current loss 0.297263, current_train_items 76192.
I0302 18:59:37.297544 22535416901760 run.py:483] Algo bellman_ford step 2381 current loss 0.616288, current_train_items 76224.
I0302 18:59:37.321117 22535416901760 run.py:483] Algo bellman_ford step 2382 current loss 0.706628, current_train_items 76256.
I0302 18:59:37.350236 22535416901760 run.py:483] Algo bellman_ford step 2383 current loss 0.654166, current_train_items 76288.
I0302 18:59:37.383749 22535416901760 run.py:483] Algo bellman_ford step 2384 current loss 0.885136, current_train_items 76320.
I0302 18:59:37.403611 22535416901760 run.py:483] Algo bellman_ford step 2385 current loss 0.300218, current_train_items 76352.
I0302 18:59:37.419661 22535416901760 run.py:483] Algo bellman_ford step 2386 current loss 0.472714, current_train_items 76384.
I0302 18:59:37.441819 22535416901760 run.py:483] Algo bellman_ford step 2387 current loss 0.700238, current_train_items 76416.
I0302 18:59:37.472242 22535416901760 run.py:483] Algo bellman_ford step 2388 current loss 0.740089, current_train_items 76448.
I0302 18:59:37.507138 22535416901760 run.py:483] Algo bellman_ford step 2389 current loss 0.808766, current_train_items 76480.
I0302 18:59:37.527421 22535416901760 run.py:483] Algo bellman_ford step 2390 current loss 0.306032, current_train_items 76512.
I0302 18:59:37.543316 22535416901760 run.py:483] Algo bellman_ford step 2391 current loss 0.488604, current_train_items 76544.
I0302 18:59:37.564630 22535416901760 run.py:483] Algo bellman_ford step 2392 current loss 0.668952, current_train_items 76576.
I0302 18:59:37.594452 22535416901760 run.py:483] Algo bellman_ford step 2393 current loss 0.788894, current_train_items 76608.
I0302 18:59:37.630790 22535416901760 run.py:483] Algo bellman_ford step 2394 current loss 0.970602, current_train_items 76640.
I0302 18:59:37.650634 22535416901760 run.py:483] Algo bellman_ford step 2395 current loss 0.376130, current_train_items 76672.
I0302 18:59:37.666723 22535416901760 run.py:483] Algo bellman_ford step 2396 current loss 0.532982, current_train_items 76704.
I0302 18:59:37.688627 22535416901760 run.py:483] Algo bellman_ford step 2397 current loss 0.673572, current_train_items 76736.
I0302 18:59:37.718236 22535416901760 run.py:483] Algo bellman_ford step 2398 current loss 0.843611, current_train_items 76768.
I0302 18:59:37.752491 22535416901760 run.py:483] Algo bellman_ford step 2399 current loss 1.058339, current_train_items 76800.
I0302 18:59:37.772392 22535416901760 run.py:483] Algo bellman_ford step 2400 current loss 0.283519, current_train_items 76832.
I0302 18:59:37.780283 22535416901760 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:59:37.780392 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 18:59:37.796958 22535416901760 run.py:483] Algo bellman_ford step 2401 current loss 0.606871, current_train_items 76864.
I0302 18:59:37.820410 22535416901760 run.py:483] Algo bellman_ford step 2402 current loss 0.709347, current_train_items 76896.
I0302 18:59:37.852088 22535416901760 run.py:483] Algo bellman_ford step 2403 current loss 0.934973, current_train_items 76928.
I0302 18:59:37.888785 22535416901760 run.py:483] Algo bellman_ford step 2404 current loss 1.097134, current_train_items 76960.
I0302 18:59:37.908899 22535416901760 run.py:483] Algo bellman_ford step 2405 current loss 0.258288, current_train_items 76992.
I0302 18:59:37.924893 22535416901760 run.py:483] Algo bellman_ford step 2406 current loss 0.601071, current_train_items 77024.
I0302 18:59:37.948087 22535416901760 run.py:483] Algo bellman_ford step 2407 current loss 0.758718, current_train_items 77056.
I0302 18:59:37.977321 22535416901760 run.py:483] Algo bellman_ford step 2408 current loss 0.776295, current_train_items 77088.
I0302 18:59:38.009739 22535416901760 run.py:483] Algo bellman_ford step 2409 current loss 0.853615, current_train_items 77120.
I0302 18:59:38.028923 22535416901760 run.py:483] Algo bellman_ford step 2410 current loss 0.236421, current_train_items 77152.
I0302 18:59:38.045070 22535416901760 run.py:483] Algo bellman_ford step 2411 current loss 0.496174, current_train_items 77184.
I0302 18:59:38.069390 22535416901760 run.py:483] Algo bellman_ford step 2412 current loss 0.823715, current_train_items 77216.
I0302 18:59:38.100092 22535416901760 run.py:483] Algo bellman_ford step 2413 current loss 0.782586, current_train_items 77248.
I0302 18:59:38.134106 22535416901760 run.py:483] Algo bellman_ford step 2414 current loss 1.028320, current_train_items 77280.
I0302 18:59:38.153667 22535416901760 run.py:483] Algo bellman_ford step 2415 current loss 0.340935, current_train_items 77312.
I0302 18:59:38.169198 22535416901760 run.py:483] Algo bellman_ford step 2416 current loss 0.533480, current_train_items 77344.
I0302 18:59:38.192182 22535416901760 run.py:483] Algo bellman_ford step 2417 current loss 0.694392, current_train_items 77376.
I0302 18:59:38.223268 22535416901760 run.py:483] Algo bellman_ford step 2418 current loss 0.962727, current_train_items 77408.
I0302 18:59:38.254492 22535416901760 run.py:483] Algo bellman_ford step 2419 current loss 0.870543, current_train_items 77440.
I0302 18:59:38.273941 22535416901760 run.py:483] Algo bellman_ford step 2420 current loss 0.412875, current_train_items 77472.
I0302 18:59:38.289930 22535416901760 run.py:483] Algo bellman_ford step 2421 current loss 0.679202, current_train_items 77504.
I0302 18:59:38.313765 22535416901760 run.py:483] Algo bellman_ford step 2422 current loss 0.864978, current_train_items 77536.
I0302 18:59:38.344000 22535416901760 run.py:483] Algo bellman_ford step 2423 current loss 0.814307, current_train_items 77568.
I0302 18:59:38.378318 22535416901760 run.py:483] Algo bellman_ford step 2424 current loss 1.002460, current_train_items 77600.
I0302 18:59:38.397836 22535416901760 run.py:483] Algo bellman_ford step 2425 current loss 0.295822, current_train_items 77632.
I0302 18:59:38.413794 22535416901760 run.py:483] Algo bellman_ford step 2426 current loss 0.489259, current_train_items 77664.
I0302 18:59:38.438195 22535416901760 run.py:483] Algo bellman_ford step 2427 current loss 0.794665, current_train_items 77696.
I0302 18:59:38.467110 22535416901760 run.py:483] Algo bellman_ford step 2428 current loss 0.836530, current_train_items 77728.
I0302 18:59:38.499840 22535416901760 run.py:483] Algo bellman_ford step 2429 current loss 0.871630, current_train_items 77760.
I0302 18:59:38.519468 22535416901760 run.py:483] Algo bellman_ford step 2430 current loss 0.398966, current_train_items 77792.
I0302 18:59:38.535884 22535416901760 run.py:483] Algo bellman_ford step 2431 current loss 0.603068, current_train_items 77824.
I0302 18:59:38.559463 22535416901760 run.py:483] Algo bellman_ford step 2432 current loss 0.862261, current_train_items 77856.
I0302 18:59:38.589279 22535416901760 run.py:483] Algo bellman_ford step 2433 current loss 0.853244, current_train_items 77888.
I0302 18:59:38.623257 22535416901760 run.py:483] Algo bellman_ford step 2434 current loss 0.761809, current_train_items 77920.
I0302 18:59:38.642845 22535416901760 run.py:483] Algo bellman_ford step 2435 current loss 0.313906, current_train_items 77952.
I0302 18:59:38.659162 22535416901760 run.py:483] Algo bellman_ford step 2436 current loss 0.605369, current_train_items 77984.
I0302 18:59:38.682167 22535416901760 run.py:483] Algo bellman_ford step 2437 current loss 0.722360, current_train_items 78016.
I0302 18:59:38.710825 22535416901760 run.py:483] Algo bellman_ford step 2438 current loss 0.781699, current_train_items 78048.
I0302 18:59:38.743193 22535416901760 run.py:483] Algo bellman_ford step 2439 current loss 0.892281, current_train_items 78080.
I0302 18:59:38.762767 22535416901760 run.py:483] Algo bellman_ford step 2440 current loss 0.357767, current_train_items 78112.
I0302 18:59:38.778947 22535416901760 run.py:483] Algo bellman_ford step 2441 current loss 0.456328, current_train_items 78144.
I0302 18:59:38.803081 22535416901760 run.py:483] Algo bellman_ford step 2442 current loss 0.758813, current_train_items 78176.
I0302 18:59:38.833077 22535416901760 run.py:483] Algo bellman_ford step 2443 current loss 0.785736, current_train_items 78208.
I0302 18:59:38.867622 22535416901760 run.py:483] Algo bellman_ford step 2444 current loss 0.838463, current_train_items 78240.
I0302 18:59:38.887118 22535416901760 run.py:483] Algo bellman_ford step 2445 current loss 0.362066, current_train_items 78272.
I0302 18:59:38.903391 22535416901760 run.py:483] Algo bellman_ford step 2446 current loss 0.576476, current_train_items 78304.
I0302 18:59:38.927087 22535416901760 run.py:483] Algo bellman_ford step 2447 current loss 0.663560, current_train_items 78336.
I0302 18:59:38.956070 22535416901760 run.py:483] Algo bellman_ford step 2448 current loss 0.790390, current_train_items 78368.
I0302 18:59:38.989210 22535416901760 run.py:483] Algo bellman_ford step 2449 current loss 0.805184, current_train_items 78400.
I0302 18:59:39.008462 22535416901760 run.py:483] Algo bellman_ford step 2450 current loss 0.394648, current_train_items 78432.
I0302 18:59:39.016484 22535416901760 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:39.016594 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 18:59:39.033623 22535416901760 run.py:483] Algo bellman_ford step 2451 current loss 0.478711, current_train_items 78464.
I0302 18:59:39.057494 22535416901760 run.py:483] Algo bellman_ford step 2452 current loss 0.796789, current_train_items 78496.
I0302 18:59:39.087469 22535416901760 run.py:483] Algo bellman_ford step 2453 current loss 0.790220, current_train_items 78528.
I0302 18:59:39.121632 22535416901760 run.py:483] Algo bellman_ford step 2454 current loss 0.969531, current_train_items 78560.
I0302 18:59:39.141660 22535416901760 run.py:483] Algo bellman_ford step 2455 current loss 0.298628, current_train_items 78592.
I0302 18:59:39.157632 22535416901760 run.py:483] Algo bellman_ford step 2456 current loss 0.514184, current_train_items 78624.
I0302 18:59:39.179794 22535416901760 run.py:483] Algo bellman_ford step 2457 current loss 0.631932, current_train_items 78656.
I0302 18:59:39.209819 22535416901760 run.py:483] Algo bellman_ford step 2458 current loss 0.675966, current_train_items 78688.
I0302 18:59:39.244554 22535416901760 run.py:483] Algo bellman_ford step 2459 current loss 0.923104, current_train_items 78720.
I0302 18:59:39.264564 22535416901760 run.py:483] Algo bellman_ford step 2460 current loss 0.309782, current_train_items 78752.
I0302 18:59:39.281295 22535416901760 run.py:483] Algo bellman_ford step 2461 current loss 0.608941, current_train_items 78784.
I0302 18:59:39.303885 22535416901760 run.py:483] Algo bellman_ford step 2462 current loss 0.642451, current_train_items 78816.
I0302 18:59:39.334902 22535416901760 run.py:483] Algo bellman_ford step 2463 current loss 0.841077, current_train_items 78848.
I0302 18:59:39.368418 22535416901760 run.py:483] Algo bellman_ford step 2464 current loss 0.854630, current_train_items 78880.
I0302 18:59:39.387787 22535416901760 run.py:483] Algo bellman_ford step 2465 current loss 0.306443, current_train_items 78912.
I0302 18:59:39.403899 22535416901760 run.py:483] Algo bellman_ford step 2466 current loss 0.589773, current_train_items 78944.
I0302 18:59:39.427323 22535416901760 run.py:483] Algo bellman_ford step 2467 current loss 0.783200, current_train_items 78976.
I0302 18:59:39.457680 22535416901760 run.py:483] Algo bellman_ford step 2468 current loss 0.794485, current_train_items 79008.
I0302 18:59:39.492406 22535416901760 run.py:483] Algo bellman_ford step 2469 current loss 0.939895, current_train_items 79040.
I0302 18:59:39.512199 22535416901760 run.py:483] Algo bellman_ford step 2470 current loss 0.365960, current_train_items 79072.
I0302 18:59:39.528475 22535416901760 run.py:483] Algo bellman_ford step 2471 current loss 0.461256, current_train_items 79104.
I0302 18:59:39.551942 22535416901760 run.py:483] Algo bellman_ford step 2472 current loss 0.741179, current_train_items 79136.
I0302 18:59:39.583145 22535416901760 run.py:483] Algo bellman_ford step 2473 current loss 0.855439, current_train_items 79168.
I0302 18:59:39.615882 22535416901760 run.py:483] Algo bellman_ford step 2474 current loss 0.867923, current_train_items 79200.
I0302 18:59:39.635617 22535416901760 run.py:483] Algo bellman_ford step 2475 current loss 0.318945, current_train_items 79232.
I0302 18:59:39.651881 22535416901760 run.py:483] Algo bellman_ford step 2476 current loss 0.589905, current_train_items 79264.
I0302 18:59:39.673526 22535416901760 run.py:483] Algo bellman_ford step 2477 current loss 0.531605, current_train_items 79296.
I0302 18:59:39.703003 22535416901760 run.py:483] Algo bellman_ford step 2478 current loss 0.766842, current_train_items 79328.
I0302 18:59:39.737443 22535416901760 run.py:483] Algo bellman_ford step 2479 current loss 0.910077, current_train_items 79360.
I0302 18:59:39.756898 22535416901760 run.py:483] Algo bellman_ford step 2480 current loss 0.344826, current_train_items 79392.
I0302 18:59:39.772961 22535416901760 run.py:483] Algo bellman_ford step 2481 current loss 0.508381, current_train_items 79424.
I0302 18:59:39.796747 22535416901760 run.py:483] Algo bellman_ford step 2482 current loss 0.892152, current_train_items 79456.
I0302 18:59:39.826804 22535416901760 run.py:483] Algo bellman_ford step 2483 current loss 0.930339, current_train_items 79488.
I0302 18:59:39.861560 22535416901760 run.py:483] Algo bellman_ford step 2484 current loss 0.904740, current_train_items 79520.
I0302 18:59:39.881398 22535416901760 run.py:483] Algo bellman_ford step 2485 current loss 0.360900, current_train_items 79552.
I0302 18:59:39.897723 22535416901760 run.py:483] Algo bellman_ford step 2486 current loss 0.589288, current_train_items 79584.
I0302 18:59:39.921415 22535416901760 run.py:483] Algo bellman_ford step 2487 current loss 0.703180, current_train_items 79616.
I0302 18:59:39.948756 22535416901760 run.py:483] Algo bellman_ford step 2488 current loss 0.672074, current_train_items 79648.
I0302 18:59:39.982803 22535416901760 run.py:483] Algo bellman_ford step 2489 current loss 0.837963, current_train_items 79680.
I0302 18:59:40.002745 22535416901760 run.py:483] Algo bellman_ford step 2490 current loss 0.286308, current_train_items 79712.
I0302 18:59:40.018975 22535416901760 run.py:483] Algo bellman_ford step 2491 current loss 0.504404, current_train_items 79744.
I0302 18:59:40.041367 22535416901760 run.py:483] Algo bellman_ford step 2492 current loss 0.614240, current_train_items 79776.
I0302 18:59:40.071396 22535416901760 run.py:483] Algo bellman_ford step 2493 current loss 0.789571, current_train_items 79808.
I0302 18:59:40.103589 22535416901760 run.py:483] Algo bellman_ford step 2494 current loss 0.882965, current_train_items 79840.
I0302 18:59:40.123186 22535416901760 run.py:483] Algo bellman_ford step 2495 current loss 0.313226, current_train_items 79872.
I0302 18:59:40.139801 22535416901760 run.py:483] Algo bellman_ford step 2496 current loss 0.460151, current_train_items 79904.
I0302 18:59:40.162384 22535416901760 run.py:483] Algo bellman_ford step 2497 current loss 0.647461, current_train_items 79936.
I0302 18:59:40.191130 22535416901760 run.py:483] Algo bellman_ford step 2498 current loss 0.813982, current_train_items 79968.
I0302 18:59:40.224405 22535416901760 run.py:483] Algo bellman_ford step 2499 current loss 0.840835, current_train_items 80000.
I0302 18:59:40.244426 22535416901760 run.py:483] Algo bellman_ford step 2500 current loss 0.356535, current_train_items 80032.
I0302 18:59:40.252343 22535416901760 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:40.252449 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:40.269334 22535416901760 run.py:483] Algo bellman_ford step 2501 current loss 0.476354, current_train_items 80064.
I0302 18:59:40.293162 22535416901760 run.py:483] Algo bellman_ford step 2502 current loss 0.695528, current_train_items 80096.
I0302 18:59:40.322680 22535416901760 run.py:483] Algo bellman_ford step 2503 current loss 0.699786, current_train_items 80128.
I0302 18:59:40.357360 22535416901760 run.py:483] Algo bellman_ford step 2504 current loss 0.872003, current_train_items 80160.
I0302 18:59:40.377398 22535416901760 run.py:483] Algo bellman_ford step 2505 current loss 0.285928, current_train_items 80192.
I0302 18:59:40.393390 22535416901760 run.py:483] Algo bellman_ford step 2506 current loss 0.552952, current_train_items 80224.
I0302 18:59:40.416919 22535416901760 run.py:483] Algo bellman_ford step 2507 current loss 0.633373, current_train_items 80256.
I0302 18:59:40.445993 22535416901760 run.py:483] Algo bellman_ford step 2508 current loss 0.726607, current_train_items 80288.
I0302 18:59:40.480409 22535416901760 run.py:483] Algo bellman_ford step 2509 current loss 0.986671, current_train_items 80320.
I0302 18:59:40.499811 22535416901760 run.py:483] Algo bellman_ford step 2510 current loss 0.317880, current_train_items 80352.
I0302 18:59:40.516291 22535416901760 run.py:483] Algo bellman_ford step 2511 current loss 0.622119, current_train_items 80384.
I0302 18:59:40.539434 22535416901760 run.py:483] Algo bellman_ford step 2512 current loss 0.650390, current_train_items 80416.
I0302 18:59:40.568507 22535416901760 run.py:483] Algo bellman_ford step 2513 current loss 0.701675, current_train_items 80448.
I0302 18:59:40.602816 22535416901760 run.py:483] Algo bellman_ford step 2514 current loss 0.952730, current_train_items 80480.
I0302 18:59:40.622090 22535416901760 run.py:483] Algo bellman_ford step 2515 current loss 0.291443, current_train_items 80512.
I0302 18:59:40.638355 22535416901760 run.py:483] Algo bellman_ford step 2516 current loss 0.500155, current_train_items 80544.
I0302 18:59:40.661740 22535416901760 run.py:483] Algo bellman_ford step 2517 current loss 0.714902, current_train_items 80576.
I0302 18:59:40.690971 22535416901760 run.py:483] Algo bellman_ford step 2518 current loss 0.810789, current_train_items 80608.
I0302 18:59:40.724649 22535416901760 run.py:483] Algo bellman_ford step 2519 current loss 0.934013, current_train_items 80640.
I0302 18:59:40.744092 22535416901760 run.py:483] Algo bellman_ford step 2520 current loss 0.345931, current_train_items 80672.
I0302 18:59:40.760357 22535416901760 run.py:483] Algo bellman_ford step 2521 current loss 0.506303, current_train_items 80704.
I0302 18:59:40.782100 22535416901760 run.py:483] Algo bellman_ford step 2522 current loss 0.608812, current_train_items 80736.
I0302 18:59:40.811366 22535416901760 run.py:483] Algo bellman_ford step 2523 current loss 0.663735, current_train_items 80768.
I0302 18:59:40.844161 22535416901760 run.py:483] Algo bellman_ford step 2524 current loss 0.783813, current_train_items 80800.
I0302 18:59:40.863754 22535416901760 run.py:483] Algo bellman_ford step 2525 current loss 0.357674, current_train_items 80832.
I0302 18:59:40.879915 22535416901760 run.py:483] Algo bellman_ford step 2526 current loss 0.544567, current_train_items 80864.
I0302 18:59:40.902994 22535416901760 run.py:483] Algo bellman_ford step 2527 current loss 0.673572, current_train_items 80896.
I0302 18:59:40.932117 22535416901760 run.py:483] Algo bellman_ford step 2528 current loss 0.706089, current_train_items 80928.
I0302 18:59:40.966389 22535416901760 run.py:483] Algo bellman_ford step 2529 current loss 0.914376, current_train_items 80960.
I0302 18:59:40.985944 22535416901760 run.py:483] Algo bellman_ford step 2530 current loss 0.358485, current_train_items 80992.
I0302 18:59:41.001787 22535416901760 run.py:483] Algo bellman_ford step 2531 current loss 0.516585, current_train_items 81024.
I0302 18:59:41.025031 22535416901760 run.py:483] Algo bellman_ford step 2532 current loss 0.820781, current_train_items 81056.
I0302 18:59:41.054730 22535416901760 run.py:483] Algo bellman_ford step 2533 current loss 0.762991, current_train_items 81088.
I0302 18:59:41.087273 22535416901760 run.py:483] Algo bellman_ford step 2534 current loss 0.868449, current_train_items 81120.
I0302 18:59:41.106786 22535416901760 run.py:483] Algo bellman_ford step 2535 current loss 0.353991, current_train_items 81152.
I0302 18:59:41.122714 22535416901760 run.py:483] Algo bellman_ford step 2536 current loss 0.543217, current_train_items 81184.
W0302 18:59:41.138969 22535416901760 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:47.682370 22535416901760 run.py:483] Algo bellman_ford step 2537 current loss 0.910708, current_train_items 81216.
I0302 18:59:47.714783 22535416901760 run.py:483] Algo bellman_ford step 2538 current loss 0.981678, current_train_items 81248.
I0302 18:59:47.749558 22535416901760 run.py:483] Algo bellman_ford step 2539 current loss 1.360436, current_train_items 81280.
I0302 18:59:47.769571 22535416901760 run.py:483] Algo bellman_ford step 2540 current loss 0.383408, current_train_items 81312.
I0302 18:59:47.785684 22535416901760 run.py:483] Algo bellman_ford step 2541 current loss 0.436790, current_train_items 81344.
I0302 18:59:47.809340 22535416901760 run.py:483] Algo bellman_ford step 2542 current loss 0.841632, current_train_items 81376.
I0302 18:59:47.838346 22535416901760 run.py:483] Algo bellman_ford step 2543 current loss 0.863576, current_train_items 81408.
I0302 18:59:47.871358 22535416901760 run.py:483] Algo bellman_ford step 2544 current loss 1.084950, current_train_items 81440.
I0302 18:59:47.890913 22535416901760 run.py:483] Algo bellman_ford step 2545 current loss 0.367569, current_train_items 81472.
I0302 18:59:47.906950 22535416901760 run.py:483] Algo bellman_ford step 2546 current loss 0.644196, current_train_items 81504.
I0302 18:59:47.930757 22535416901760 run.py:483] Algo bellman_ford step 2547 current loss 0.815413, current_train_items 81536.
I0302 18:59:47.961856 22535416901760 run.py:483] Algo bellman_ford step 2548 current loss 0.917172, current_train_items 81568.
I0302 18:59:47.994067 22535416901760 run.py:483] Algo bellman_ford step 2549 current loss 1.015285, current_train_items 81600.
I0302 18:59:48.014031 22535416901760 run.py:483] Algo bellman_ford step 2550 current loss 0.310474, current_train_items 81632.
I0302 18:59:48.023289 22535416901760 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.896484375, 'score': 0.896484375, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:48.023400 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.922, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 18:59:48.040683 22535416901760 run.py:483] Algo bellman_ford step 2551 current loss 0.623133, current_train_items 81664.
I0302 18:59:48.065025 22535416901760 run.py:483] Algo bellman_ford step 2552 current loss 0.733361, current_train_items 81696.
I0302 18:59:48.097293 22535416901760 run.py:483] Algo bellman_ford step 2553 current loss 0.933968, current_train_items 81728.
I0302 18:59:48.131063 22535416901760 run.py:483] Algo bellman_ford step 2554 current loss 1.158613, current_train_items 81760.
I0302 18:59:48.151185 22535416901760 run.py:483] Algo bellman_ford step 2555 current loss 0.320885, current_train_items 81792.
I0302 18:59:48.167597 22535416901760 run.py:483] Algo bellman_ford step 2556 current loss 0.626253, current_train_items 81824.
I0302 18:59:48.191214 22535416901760 run.py:483] Algo bellman_ford step 2557 current loss 0.680767, current_train_items 81856.
I0302 18:59:48.222429 22535416901760 run.py:483] Algo bellman_ford step 2558 current loss 0.742568, current_train_items 81888.
I0302 18:59:48.257377 22535416901760 run.py:483] Algo bellman_ford step 2559 current loss 1.075264, current_train_items 81920.
I0302 18:59:48.277250 22535416901760 run.py:483] Algo bellman_ford step 2560 current loss 0.465706, current_train_items 81952.
I0302 18:59:48.293763 22535416901760 run.py:483] Algo bellman_ford step 2561 current loss 0.554066, current_train_items 81984.
I0302 18:59:48.316657 22535416901760 run.py:483] Algo bellman_ford step 2562 current loss 0.639332, current_train_items 82016.
I0302 18:59:48.348836 22535416901760 run.py:483] Algo bellman_ford step 2563 current loss 0.845807, current_train_items 82048.
I0302 18:59:48.382859 22535416901760 run.py:483] Algo bellman_ford step 2564 current loss 0.871668, current_train_items 82080.
I0302 18:59:48.402538 22535416901760 run.py:483] Algo bellman_ford step 2565 current loss 0.324924, current_train_items 82112.
I0302 18:59:48.418915 22535416901760 run.py:483] Algo bellman_ford step 2566 current loss 0.508646, current_train_items 82144.
I0302 18:59:48.442284 22535416901760 run.py:483] Algo bellman_ford step 2567 current loss 0.646390, current_train_items 82176.
I0302 18:59:48.474001 22535416901760 run.py:483] Algo bellman_ford step 2568 current loss 0.764157, current_train_items 82208.
I0302 18:59:48.506751 22535416901760 run.py:483] Algo bellman_ford step 2569 current loss 0.887841, current_train_items 82240.
I0302 18:59:48.526610 22535416901760 run.py:483] Algo bellman_ford step 2570 current loss 0.267596, current_train_items 82272.
I0302 18:59:48.542737 22535416901760 run.py:483] Algo bellman_ford step 2571 current loss 0.542049, current_train_items 82304.
I0302 18:59:48.565082 22535416901760 run.py:483] Algo bellman_ford step 2572 current loss 0.594505, current_train_items 82336.
I0302 18:59:48.595620 22535416901760 run.py:483] Algo bellman_ford step 2573 current loss 0.667939, current_train_items 82368.
I0302 18:59:48.630249 22535416901760 run.py:483] Algo bellman_ford step 2574 current loss 0.915437, current_train_items 82400.
I0302 18:59:48.650496 22535416901760 run.py:483] Algo bellman_ford step 2575 current loss 0.395081, current_train_items 82432.
I0302 18:59:48.666466 22535416901760 run.py:483] Algo bellman_ford step 2576 current loss 0.509156, current_train_items 82464.
I0302 18:59:48.688995 22535416901760 run.py:483] Algo bellman_ford step 2577 current loss 0.672909, current_train_items 82496.
I0302 18:59:48.720306 22535416901760 run.py:483] Algo bellman_ford step 2578 current loss 0.856363, current_train_items 82528.
I0302 18:59:48.752717 22535416901760 run.py:483] Algo bellman_ford step 2579 current loss 0.987343, current_train_items 82560.
I0302 18:59:48.772713 22535416901760 run.py:483] Algo bellman_ford step 2580 current loss 0.397662, current_train_items 82592.
I0302 18:59:48.789003 22535416901760 run.py:483] Algo bellman_ford step 2581 current loss 0.458579, current_train_items 82624.
I0302 18:59:48.812387 22535416901760 run.py:483] Algo bellman_ford step 2582 current loss 0.695305, current_train_items 82656.
I0302 18:59:48.842631 22535416901760 run.py:483] Algo bellman_ford step 2583 current loss 0.696405, current_train_items 82688.
I0302 18:59:48.876971 22535416901760 run.py:483] Algo bellman_ford step 2584 current loss 0.932660, current_train_items 82720.
I0302 18:59:48.896953 22535416901760 run.py:483] Algo bellman_ford step 2585 current loss 0.344066, current_train_items 82752.
I0302 18:59:48.913415 22535416901760 run.py:483] Algo bellman_ford step 2586 current loss 0.571130, current_train_items 82784.
I0302 18:59:48.936550 22535416901760 run.py:483] Algo bellman_ford step 2587 current loss 0.593040, current_train_items 82816.
I0302 18:59:48.966820 22535416901760 run.py:483] Algo bellman_ford step 2588 current loss 0.666900, current_train_items 82848.
I0302 18:59:49.000698 22535416901760 run.py:483] Algo bellman_ford step 2589 current loss 0.911631, current_train_items 82880.
I0302 18:59:49.020484 22535416901760 run.py:483] Algo bellman_ford step 2590 current loss 0.337531, current_train_items 82912.
I0302 18:59:49.036895 22535416901760 run.py:483] Algo bellman_ford step 2591 current loss 0.551867, current_train_items 82944.
I0302 18:59:49.060684 22535416901760 run.py:483] Algo bellman_ford step 2592 current loss 0.717122, current_train_items 82976.
I0302 18:59:49.092041 22535416901760 run.py:483] Algo bellman_ford step 2593 current loss 0.840733, current_train_items 83008.
I0302 18:59:49.123691 22535416901760 run.py:483] Algo bellman_ford step 2594 current loss 1.060734, current_train_items 83040.
I0302 18:59:49.143093 22535416901760 run.py:483] Algo bellman_ford step 2595 current loss 0.437168, current_train_items 83072.
I0302 18:59:49.159492 22535416901760 run.py:483] Algo bellman_ford step 2596 current loss 0.532714, current_train_items 83104.
I0302 18:59:49.183906 22535416901760 run.py:483] Algo bellman_ford step 2597 current loss 1.007391, current_train_items 83136.
I0302 18:59:49.214617 22535416901760 run.py:483] Algo bellman_ford step 2598 current loss 0.862264, current_train_items 83168.
I0302 18:59:49.248049 22535416901760 run.py:483] Algo bellman_ford step 2599 current loss 0.812577, current_train_items 83200.
I0302 18:59:49.267863 22535416901760 run.py:483] Algo bellman_ford step 2600 current loss 0.369926, current_train_items 83232.
I0302 18:59:49.276178 22535416901760 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:49.276289 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.922, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 18:59:49.305915 22535416901760 run.py:483] Algo bellman_ford step 2601 current loss 0.476393, current_train_items 83264.
I0302 18:59:49.329267 22535416901760 run.py:483] Algo bellman_ford step 2602 current loss 0.650607, current_train_items 83296.
I0302 18:59:49.358970 22535416901760 run.py:483] Algo bellman_ford step 2603 current loss 0.742978, current_train_items 83328.
I0302 18:59:49.392539 22535416901760 run.py:483] Algo bellman_ford step 2604 current loss 1.050901, current_train_items 83360.
I0302 18:59:49.412984 22535416901760 run.py:483] Algo bellman_ford step 2605 current loss 0.365335, current_train_items 83392.
I0302 18:59:49.428851 22535416901760 run.py:483] Algo bellman_ford step 2606 current loss 0.507444, current_train_items 83424.
I0302 18:59:49.452491 22535416901760 run.py:483] Algo bellman_ford step 2607 current loss 0.661588, current_train_items 83456.
I0302 18:59:49.483521 22535416901760 run.py:483] Algo bellman_ford step 2608 current loss 0.855277, current_train_items 83488.
I0302 18:59:49.517470 22535416901760 run.py:483] Algo bellman_ford step 2609 current loss 0.974280, current_train_items 83520.
I0302 18:59:49.537096 22535416901760 run.py:483] Algo bellman_ford step 2610 current loss 0.321922, current_train_items 83552.
I0302 18:59:49.553102 22535416901760 run.py:483] Algo bellman_ford step 2611 current loss 0.508124, current_train_items 83584.
I0302 18:59:49.576143 22535416901760 run.py:483] Algo bellman_ford step 2612 current loss 0.604262, current_train_items 83616.
I0302 18:59:49.606619 22535416901760 run.py:483] Algo bellman_ford step 2613 current loss 0.718695, current_train_items 83648.
I0302 18:59:49.638916 22535416901760 run.py:483] Algo bellman_ford step 2614 current loss 0.791439, current_train_items 83680.
I0302 18:59:49.658252 22535416901760 run.py:483] Algo bellman_ford step 2615 current loss 0.363957, current_train_items 83712.
I0302 18:59:49.674073 22535416901760 run.py:483] Algo bellman_ford step 2616 current loss 0.427105, current_train_items 83744.
I0302 18:59:49.698205 22535416901760 run.py:483] Algo bellman_ford step 2617 current loss 0.694814, current_train_items 83776.
I0302 18:59:49.729504 22535416901760 run.py:483] Algo bellman_ford step 2618 current loss 0.777400, current_train_items 83808.
I0302 18:59:49.761622 22535416901760 run.py:483] Algo bellman_ford step 2619 current loss 0.814854, current_train_items 83840.
I0302 18:59:49.781052 22535416901760 run.py:483] Algo bellman_ford step 2620 current loss 0.328358, current_train_items 83872.
I0302 18:59:49.797183 22535416901760 run.py:483] Algo bellman_ford step 2621 current loss 0.493440, current_train_items 83904.
I0302 18:59:49.820594 22535416901760 run.py:483] Algo bellman_ford step 2622 current loss 0.783919, current_train_items 83936.
I0302 18:59:49.851114 22535416901760 run.py:483] Algo bellman_ford step 2623 current loss 0.743719, current_train_items 83968.
I0302 18:59:49.883517 22535416901760 run.py:483] Algo bellman_ford step 2624 current loss 0.762082, current_train_items 84000.
I0302 18:59:49.902985 22535416901760 run.py:483] Algo bellman_ford step 2625 current loss 0.273555, current_train_items 84032.
I0302 18:59:49.918960 22535416901760 run.py:483] Algo bellman_ford step 2626 current loss 0.496428, current_train_items 84064.
I0302 18:59:49.942556 22535416901760 run.py:483] Algo bellman_ford step 2627 current loss 0.713736, current_train_items 84096.
I0302 18:59:49.973151 22535416901760 run.py:483] Algo bellman_ford step 2628 current loss 0.784193, current_train_items 84128.
I0302 18:59:50.006557 22535416901760 run.py:483] Algo bellman_ford step 2629 current loss 0.866341, current_train_items 84160.
I0302 18:59:50.026058 22535416901760 run.py:483] Algo bellman_ford step 2630 current loss 0.280325, current_train_items 84192.
I0302 18:59:50.042135 22535416901760 run.py:483] Algo bellman_ford step 2631 current loss 0.481881, current_train_items 84224.
I0302 18:59:50.064976 22535416901760 run.py:483] Algo bellman_ford step 2632 current loss 0.639119, current_train_items 84256.
I0302 18:59:50.094586 22535416901760 run.py:483] Algo bellman_ford step 2633 current loss 0.665584, current_train_items 84288.
I0302 18:59:50.129950 22535416901760 run.py:483] Algo bellman_ford step 2634 current loss 0.919919, current_train_items 84320.
I0302 18:59:50.149590 22535416901760 run.py:483] Algo bellman_ford step 2635 current loss 0.233187, current_train_items 84352.
I0302 18:59:50.165525 22535416901760 run.py:483] Algo bellman_ford step 2636 current loss 0.527526, current_train_items 84384.
I0302 18:59:50.189128 22535416901760 run.py:483] Algo bellman_ford step 2637 current loss 0.668979, current_train_items 84416.
I0302 18:59:50.220252 22535416901760 run.py:483] Algo bellman_ford step 2638 current loss 0.748772, current_train_items 84448.
I0302 18:59:50.253853 22535416901760 run.py:483] Algo bellman_ford step 2639 current loss 0.814141, current_train_items 84480.
I0302 18:59:50.273464 22535416901760 run.py:483] Algo bellman_ford step 2640 current loss 0.377373, current_train_items 84512.
I0302 18:59:50.289504 22535416901760 run.py:483] Algo bellman_ford step 2641 current loss 0.534085, current_train_items 84544.
I0302 18:59:50.311849 22535416901760 run.py:483] Algo bellman_ford step 2642 current loss 0.709954, current_train_items 84576.
I0302 18:59:50.342251 22535416901760 run.py:483] Algo bellman_ford step 2643 current loss 0.729993, current_train_items 84608.
I0302 18:59:50.376463 22535416901760 run.py:483] Algo bellman_ford step 2644 current loss 0.989345, current_train_items 84640.
I0302 18:59:50.395782 22535416901760 run.py:483] Algo bellman_ford step 2645 current loss 0.344532, current_train_items 84672.
I0302 18:59:50.411759 22535416901760 run.py:483] Algo bellman_ford step 2646 current loss 0.600241, current_train_items 84704.
I0302 18:59:50.435281 22535416901760 run.py:483] Algo bellman_ford step 2647 current loss 0.810500, current_train_items 84736.
I0302 18:59:50.466222 22535416901760 run.py:483] Algo bellman_ford step 2648 current loss 0.787378, current_train_items 84768.
I0302 18:59:50.501582 22535416901760 run.py:483] Algo bellman_ford step 2649 current loss 0.939473, current_train_items 84800.
I0302 18:59:50.521035 22535416901760 run.py:483] Algo bellman_ford step 2650 current loss 0.334413, current_train_items 84832.
I0302 18:59:50.529022 22535416901760 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:50.529130 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 18:59:50.545692 22535416901760 run.py:483] Algo bellman_ford step 2651 current loss 0.463050, current_train_items 84864.
I0302 18:59:50.569291 22535416901760 run.py:483] Algo bellman_ford step 2652 current loss 0.664866, current_train_items 84896.
I0302 18:59:50.600887 22535416901760 run.py:483] Algo bellman_ford step 2653 current loss 0.767685, current_train_items 84928.
I0302 18:59:50.634462 22535416901760 run.py:483] Algo bellman_ford step 2654 current loss 0.886001, current_train_items 84960.
I0302 18:59:50.654405 22535416901760 run.py:483] Algo bellman_ford step 2655 current loss 0.268731, current_train_items 84992.
I0302 18:59:50.670395 22535416901760 run.py:483] Algo bellman_ford step 2656 current loss 0.559592, current_train_items 85024.
I0302 18:59:50.693805 22535416901760 run.py:483] Algo bellman_ford step 2657 current loss 0.655650, current_train_items 85056.
I0302 18:59:50.723591 22535416901760 run.py:483] Algo bellman_ford step 2658 current loss 0.719404, current_train_items 85088.
I0302 18:59:50.758971 22535416901760 run.py:483] Algo bellman_ford step 2659 current loss 0.853911, current_train_items 85120.
I0302 18:59:50.778849 22535416901760 run.py:483] Algo bellman_ford step 2660 current loss 0.368250, current_train_items 85152.
I0302 18:59:50.795570 22535416901760 run.py:483] Algo bellman_ford step 2661 current loss 0.584916, current_train_items 85184.
I0302 18:59:50.818819 22535416901760 run.py:483] Algo bellman_ford step 2662 current loss 0.879895, current_train_items 85216.
I0302 18:59:50.849492 22535416901760 run.py:483] Algo bellman_ford step 2663 current loss 0.959291, current_train_items 85248.
I0302 18:59:50.885348 22535416901760 run.py:483] Algo bellman_ford step 2664 current loss 1.094129, current_train_items 85280.
I0302 18:59:50.904776 22535416901760 run.py:483] Algo bellman_ford step 2665 current loss 0.282804, current_train_items 85312.
I0302 18:59:50.921025 22535416901760 run.py:483] Algo bellman_ford step 2666 current loss 0.486798, current_train_items 85344.
I0302 18:59:50.945105 22535416901760 run.py:483] Algo bellman_ford step 2667 current loss 0.671427, current_train_items 85376.
I0302 18:59:50.976402 22535416901760 run.py:483] Algo bellman_ford step 2668 current loss 0.775466, current_train_items 85408.
I0302 18:59:51.008908 22535416901760 run.py:483] Algo bellman_ford step 2669 current loss 0.941914, current_train_items 85440.
I0302 18:59:51.028631 22535416901760 run.py:483] Algo bellman_ford step 2670 current loss 0.352066, current_train_items 85472.
I0302 18:59:51.045122 22535416901760 run.py:483] Algo bellman_ford step 2671 current loss 0.500813, current_train_items 85504.
I0302 18:59:51.068528 22535416901760 run.py:483] Algo bellman_ford step 2672 current loss 0.597628, current_train_items 85536.
I0302 18:59:51.099982 22535416901760 run.py:483] Algo bellman_ford step 2673 current loss 0.758876, current_train_items 85568.
I0302 18:59:51.134740 22535416901760 run.py:483] Algo bellman_ford step 2674 current loss 0.786504, current_train_items 85600.
I0302 18:59:51.154487 22535416901760 run.py:483] Algo bellman_ford step 2675 current loss 0.320905, current_train_items 85632.
I0302 18:59:51.170699 22535416901760 run.py:483] Algo bellman_ford step 2676 current loss 0.537882, current_train_items 85664.
I0302 18:59:51.194610 22535416901760 run.py:483] Algo bellman_ford step 2677 current loss 0.691186, current_train_items 85696.
I0302 18:59:51.223859 22535416901760 run.py:483] Algo bellman_ford step 2678 current loss 0.625806, current_train_items 85728.
I0302 18:59:51.256206 22535416901760 run.py:483] Algo bellman_ford step 2679 current loss 0.752209, current_train_items 85760.
I0302 18:59:51.275624 22535416901760 run.py:483] Algo bellman_ford step 2680 current loss 0.315008, current_train_items 85792.
I0302 18:59:51.291841 22535416901760 run.py:483] Algo bellman_ford step 2681 current loss 0.497059, current_train_items 85824.
I0302 18:59:51.315474 22535416901760 run.py:483] Algo bellman_ford step 2682 current loss 0.722680, current_train_items 85856.
I0302 18:59:51.347540 22535416901760 run.py:483] Algo bellman_ford step 2683 current loss 0.829273, current_train_items 85888.
I0302 18:59:51.381384 22535416901760 run.py:483] Algo bellman_ford step 2684 current loss 0.826390, current_train_items 85920.
I0302 18:59:51.401145 22535416901760 run.py:483] Algo bellman_ford step 2685 current loss 0.286793, current_train_items 85952.
I0302 18:59:51.417440 22535416901760 run.py:483] Algo bellman_ford step 2686 current loss 0.519002, current_train_items 85984.
I0302 18:59:51.440658 22535416901760 run.py:483] Algo bellman_ford step 2687 current loss 0.732693, current_train_items 86016.
I0302 18:59:51.471422 22535416901760 run.py:483] Algo bellman_ford step 2688 current loss 0.757913, current_train_items 86048.
I0302 18:59:51.503586 22535416901760 run.py:483] Algo bellman_ford step 2689 current loss 0.752064, current_train_items 86080.
I0302 18:59:51.523503 22535416901760 run.py:483] Algo bellman_ford step 2690 current loss 0.303173, current_train_items 86112.
I0302 18:59:51.539761 22535416901760 run.py:483] Algo bellman_ford step 2691 current loss 0.574183, current_train_items 86144.
I0302 18:59:51.563900 22535416901760 run.py:483] Algo bellman_ford step 2692 current loss 0.929446, current_train_items 86176.
I0302 18:59:51.595899 22535416901760 run.py:483] Algo bellman_ford step 2693 current loss 0.964242, current_train_items 86208.
I0302 18:59:51.629661 22535416901760 run.py:483] Algo bellman_ford step 2694 current loss 0.997471, current_train_items 86240.
I0302 18:59:51.649162 22535416901760 run.py:483] Algo bellman_ford step 2695 current loss 0.311744, current_train_items 86272.
I0302 18:59:51.665034 22535416901760 run.py:483] Algo bellman_ford step 2696 current loss 0.497391, current_train_items 86304.
I0302 18:59:51.689173 22535416901760 run.py:483] Algo bellman_ford step 2697 current loss 0.693447, current_train_items 86336.
I0302 18:59:51.719665 22535416901760 run.py:483] Algo bellman_ford step 2698 current loss 0.788380, current_train_items 86368.
I0302 18:59:51.751504 22535416901760 run.py:483] Algo bellman_ford step 2699 current loss 1.024099, current_train_items 86400.
I0302 18:59:51.771461 22535416901760 run.py:483] Algo bellman_ford step 2700 current loss 0.362988, current_train_items 86432.
I0302 18:59:51.778955 22535416901760 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.87890625, 'score': 0.87890625, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:51.779062 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.879, val scores are: bellman_ford: 0.879
I0302 18:59:51.795770 22535416901760 run.py:483] Algo bellman_ford step 2701 current loss 0.546691, current_train_items 86464.
I0302 18:59:51.819922 22535416901760 run.py:483] Algo bellman_ford step 2702 current loss 0.589362, current_train_items 86496.
I0302 18:59:51.852908 22535416901760 run.py:483] Algo bellman_ford step 2703 current loss 0.822982, current_train_items 86528.
I0302 18:59:51.888772 22535416901760 run.py:483] Algo bellman_ford step 2704 current loss 0.842172, current_train_items 86560.
I0302 18:59:51.908742 22535416901760 run.py:483] Algo bellman_ford step 2705 current loss 0.343195, current_train_items 86592.
I0302 18:59:51.924576 22535416901760 run.py:483] Algo bellman_ford step 2706 current loss 0.562378, current_train_items 86624.
I0302 18:59:51.948225 22535416901760 run.py:483] Algo bellman_ford step 2707 current loss 0.688127, current_train_items 86656.
I0302 18:59:51.977317 22535416901760 run.py:483] Algo bellman_ford step 2708 current loss 0.701962, current_train_items 86688.
I0302 18:59:52.008912 22535416901760 run.py:483] Algo bellman_ford step 2709 current loss 0.767670, current_train_items 86720.
I0302 18:59:52.028955 22535416901760 run.py:483] Algo bellman_ford step 2710 current loss 0.328158, current_train_items 86752.
I0302 18:59:52.045377 22535416901760 run.py:483] Algo bellman_ford step 2711 current loss 0.457788, current_train_items 86784.
I0302 18:59:52.069084 22535416901760 run.py:483] Algo bellman_ford step 2712 current loss 0.698976, current_train_items 86816.
I0302 18:59:52.100653 22535416901760 run.py:483] Algo bellman_ford step 2713 current loss 0.747787, current_train_items 86848.
I0302 18:59:52.136298 22535416901760 run.py:483] Algo bellman_ford step 2714 current loss 0.898638, current_train_items 86880.
I0302 18:59:52.156000 22535416901760 run.py:483] Algo bellman_ford step 2715 current loss 0.342959, current_train_items 86912.
I0302 18:59:52.171633 22535416901760 run.py:483] Algo bellman_ford step 2716 current loss 0.374203, current_train_items 86944.
I0302 18:59:52.196076 22535416901760 run.py:483] Algo bellman_ford step 2717 current loss 0.814121, current_train_items 86976.
I0302 18:59:52.225289 22535416901760 run.py:483] Algo bellman_ford step 2718 current loss 0.771707, current_train_items 87008.
I0302 18:59:52.259049 22535416901760 run.py:483] Algo bellman_ford step 2719 current loss 0.878120, current_train_items 87040.
I0302 18:59:52.278298 22535416901760 run.py:483] Algo bellman_ford step 2720 current loss 0.318388, current_train_items 87072.
I0302 18:59:52.294405 22535416901760 run.py:483] Algo bellman_ford step 2721 current loss 0.714397, current_train_items 87104.
I0302 18:59:52.317660 22535416901760 run.py:483] Algo bellman_ford step 2722 current loss 0.733248, current_train_items 87136.
I0302 18:59:52.349511 22535416901760 run.py:483] Algo bellman_ford step 2723 current loss 0.817478, current_train_items 87168.
I0302 18:59:52.383084 22535416901760 run.py:483] Algo bellman_ford step 2724 current loss 0.789268, current_train_items 87200.
I0302 18:59:52.402498 22535416901760 run.py:483] Algo bellman_ford step 2725 current loss 0.304223, current_train_items 87232.
I0302 18:59:52.418390 22535416901760 run.py:483] Algo bellman_ford step 2726 current loss 0.587943, current_train_items 87264.
I0302 18:59:52.442399 22535416901760 run.py:483] Algo bellman_ford step 2727 current loss 0.769919, current_train_items 87296.
I0302 18:59:52.473120 22535416901760 run.py:483] Algo bellman_ford step 2728 current loss 0.924850, current_train_items 87328.
I0302 18:59:52.506437 22535416901760 run.py:483] Algo bellman_ford step 2729 current loss 1.117313, current_train_items 87360.
I0302 18:59:52.525968 22535416901760 run.py:483] Algo bellman_ford step 2730 current loss 0.329479, current_train_items 87392.
I0302 18:59:52.541965 22535416901760 run.py:483] Algo bellman_ford step 2731 current loss 0.449668, current_train_items 87424.
I0302 18:59:52.564431 22535416901760 run.py:483] Algo bellman_ford step 2732 current loss 0.768879, current_train_items 87456.
I0302 18:59:52.594087 22535416901760 run.py:483] Algo bellman_ford step 2733 current loss 0.760232, current_train_items 87488.
I0302 18:59:52.627471 22535416901760 run.py:483] Algo bellman_ford step 2734 current loss 0.987732, current_train_items 87520.
I0302 18:59:52.647243 22535416901760 run.py:483] Algo bellman_ford step 2735 current loss 0.385852, current_train_items 87552.
I0302 18:59:52.662828 22535416901760 run.py:483] Algo bellman_ford step 2736 current loss 0.495401, current_train_items 87584.
I0302 18:59:52.686190 22535416901760 run.py:483] Algo bellman_ford step 2737 current loss 0.735839, current_train_items 87616.
I0302 18:59:52.715895 22535416901760 run.py:483] Algo bellman_ford step 2738 current loss 0.711570, current_train_items 87648.
I0302 18:59:52.747996 22535416901760 run.py:483] Algo bellman_ford step 2739 current loss 0.865734, current_train_items 87680.
I0302 18:59:52.767456 22535416901760 run.py:483] Algo bellman_ford step 2740 current loss 0.390549, current_train_items 87712.
I0302 18:59:52.783353 22535416901760 run.py:483] Algo bellman_ford step 2741 current loss 0.513144, current_train_items 87744.
I0302 18:59:52.806907 22535416901760 run.py:483] Algo bellman_ford step 2742 current loss 0.605962, current_train_items 87776.
I0302 18:59:52.839084 22535416901760 run.py:483] Algo bellman_ford step 2743 current loss 0.791197, current_train_items 87808.
I0302 18:59:52.871771 22535416901760 run.py:483] Algo bellman_ford step 2744 current loss 0.905581, current_train_items 87840.
I0302 18:59:52.891269 22535416901760 run.py:483] Algo bellman_ford step 2745 current loss 0.332670, current_train_items 87872.
I0302 18:59:52.907182 22535416901760 run.py:483] Algo bellman_ford step 2746 current loss 0.522747, current_train_items 87904.
I0302 18:59:52.930304 22535416901760 run.py:483] Algo bellman_ford step 2747 current loss 0.657095, current_train_items 87936.
I0302 18:59:52.961541 22535416901760 run.py:483] Algo bellman_ford step 2748 current loss 0.748177, current_train_items 87968.
I0302 18:59:52.994081 22535416901760 run.py:483] Algo bellman_ford step 2749 current loss 0.831428, current_train_items 88000.
I0302 18:59:53.013666 22535416901760 run.py:483] Algo bellman_ford step 2750 current loss 0.395670, current_train_items 88032.
I0302 18:59:53.021580 22535416901760 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:53.021685 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 18:59:53.038563 22535416901760 run.py:483] Algo bellman_ford step 2751 current loss 0.567303, current_train_items 88064.
I0302 18:59:53.062288 22535416901760 run.py:483] Algo bellman_ford step 2752 current loss 0.742749, current_train_items 88096.
I0302 18:59:53.094099 22535416901760 run.py:483] Algo bellman_ford step 2753 current loss 0.802557, current_train_items 88128.
I0302 18:59:53.126347 22535416901760 run.py:483] Algo bellman_ford step 2754 current loss 0.798226, current_train_items 88160.
I0302 18:59:53.146066 22535416901760 run.py:483] Algo bellman_ford step 2755 current loss 0.333850, current_train_items 88192.
I0302 18:59:53.162161 22535416901760 run.py:483] Algo bellman_ford step 2756 current loss 0.479599, current_train_items 88224.
I0302 18:59:53.184820 22535416901760 run.py:483] Algo bellman_ford step 2757 current loss 0.608803, current_train_items 88256.
I0302 18:59:53.215876 22535416901760 run.py:483] Algo bellman_ford step 2758 current loss 0.753792, current_train_items 88288.
I0302 18:59:53.251520 22535416901760 run.py:483] Algo bellman_ford step 2759 current loss 0.938093, current_train_items 88320.
I0302 18:59:53.271146 22535416901760 run.py:483] Algo bellman_ford step 2760 current loss 0.359283, current_train_items 88352.
I0302 18:59:53.287766 22535416901760 run.py:483] Algo bellman_ford step 2761 current loss 0.524652, current_train_items 88384.
I0302 18:59:53.311159 22535416901760 run.py:483] Algo bellman_ford step 2762 current loss 0.788579, current_train_items 88416.
I0302 18:59:53.342400 22535416901760 run.py:483] Algo bellman_ford step 2763 current loss 0.781443, current_train_items 88448.
I0302 18:59:53.375593 22535416901760 run.py:483] Algo bellman_ford step 2764 current loss 0.957380, current_train_items 88480.
I0302 18:59:53.394918 22535416901760 run.py:483] Algo bellman_ford step 2765 current loss 0.358533, current_train_items 88512.
I0302 18:59:53.411231 22535416901760 run.py:483] Algo bellman_ford step 2766 current loss 0.557498, current_train_items 88544.
I0302 18:59:53.434675 22535416901760 run.py:483] Algo bellman_ford step 2767 current loss 0.712072, current_train_items 88576.
I0302 18:59:53.466238 22535416901760 run.py:483] Algo bellman_ford step 2768 current loss 0.784838, current_train_items 88608.
I0302 18:59:53.499405 22535416901760 run.py:483] Algo bellman_ford step 2769 current loss 0.854574, current_train_items 88640.
I0302 18:59:53.518820 22535416901760 run.py:483] Algo bellman_ford step 2770 current loss 0.318734, current_train_items 88672.
I0302 18:59:53.535135 22535416901760 run.py:483] Algo bellman_ford step 2771 current loss 0.513412, current_train_items 88704.
I0302 18:59:53.558533 22535416901760 run.py:483] Algo bellman_ford step 2772 current loss 0.679916, current_train_items 88736.
I0302 18:59:53.588329 22535416901760 run.py:483] Algo bellman_ford step 2773 current loss 0.713841, current_train_items 88768.
I0302 18:59:53.621338 22535416901760 run.py:483] Algo bellman_ford step 2774 current loss 0.787027, current_train_items 88800.
I0302 18:59:53.640804 22535416901760 run.py:483] Algo bellman_ford step 2775 current loss 0.277503, current_train_items 88832.
I0302 18:59:53.657453 22535416901760 run.py:483] Algo bellman_ford step 2776 current loss 0.659667, current_train_items 88864.
I0302 18:59:53.680543 22535416901760 run.py:483] Algo bellman_ford step 2777 current loss 0.738080, current_train_items 88896.
I0302 18:59:53.710513 22535416901760 run.py:483] Algo bellman_ford step 2778 current loss 0.734559, current_train_items 88928.
I0302 18:59:53.744223 22535416901760 run.py:483] Algo bellman_ford step 2779 current loss 0.920635, current_train_items 88960.
I0302 18:59:53.763833 22535416901760 run.py:483] Algo bellman_ford step 2780 current loss 0.336341, current_train_items 88992.
I0302 18:59:53.779891 22535416901760 run.py:483] Algo bellman_ford step 2781 current loss 0.633980, current_train_items 89024.
I0302 18:59:53.803600 22535416901760 run.py:483] Algo bellman_ford step 2782 current loss 0.644193, current_train_items 89056.
I0302 18:59:53.833802 22535416901760 run.py:483] Algo bellman_ford step 2783 current loss 0.654009, current_train_items 89088.
I0302 18:59:53.867606 22535416901760 run.py:483] Algo bellman_ford step 2784 current loss 0.825881, current_train_items 89120.
I0302 18:59:53.887378 22535416901760 run.py:483] Algo bellman_ford step 2785 current loss 0.364253, current_train_items 89152.
I0302 18:59:53.903411 22535416901760 run.py:483] Algo bellman_ford step 2786 current loss 0.498010, current_train_items 89184.
I0302 18:59:53.925173 22535416901760 run.py:483] Algo bellman_ford step 2787 current loss 0.549783, current_train_items 89216.
I0302 18:59:53.955083 22535416901760 run.py:483] Algo bellman_ford step 2788 current loss 0.724891, current_train_items 89248.
I0302 18:59:53.985456 22535416901760 run.py:483] Algo bellman_ford step 2789 current loss 0.699304, current_train_items 89280.
I0302 18:59:54.005367 22535416901760 run.py:483] Algo bellman_ford step 2790 current loss 0.413894, current_train_items 89312.
I0302 18:59:54.021440 22535416901760 run.py:483] Algo bellman_ford step 2791 current loss 0.463406, current_train_items 89344.
I0302 18:59:54.044378 22535416901760 run.py:483] Algo bellman_ford step 2792 current loss 0.603432, current_train_items 89376.
I0302 18:59:54.073950 22535416901760 run.py:483] Algo bellman_ford step 2793 current loss 0.739684, current_train_items 89408.
I0302 18:59:54.109405 22535416901760 run.py:483] Algo bellman_ford step 2794 current loss 0.908688, current_train_items 89440.
I0302 18:59:54.128987 22535416901760 run.py:483] Algo bellman_ford step 2795 current loss 0.351906, current_train_items 89472.
I0302 18:59:54.145650 22535416901760 run.py:483] Algo bellman_ford step 2796 current loss 0.544604, current_train_items 89504.
I0302 18:59:54.168665 22535416901760 run.py:483] Algo bellman_ford step 2797 current loss 0.735504, current_train_items 89536.
I0302 18:59:54.199787 22535416901760 run.py:483] Algo bellman_ford step 2798 current loss 0.764087, current_train_items 89568.
I0302 18:59:54.232782 22535416901760 run.py:483] Algo bellman_ford step 2799 current loss 0.820214, current_train_items 89600.
I0302 18:59:54.252397 22535416901760 run.py:483] Algo bellman_ford step 2800 current loss 0.367814, current_train_items 89632.
I0302 18:59:54.260144 22535416901760 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:54.260259 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 18:59:54.276983 22535416901760 run.py:483] Algo bellman_ford step 2801 current loss 0.485700, current_train_items 89664.
I0302 18:59:54.301548 22535416901760 run.py:483] Algo bellman_ford step 2802 current loss 0.705701, current_train_items 89696.
I0302 18:59:54.333075 22535416901760 run.py:483] Algo bellman_ford step 2803 current loss 0.743824, current_train_items 89728.
I0302 18:59:54.366463 22535416901760 run.py:483] Algo bellman_ford step 2804 current loss 0.917527, current_train_items 89760.
I0302 18:59:54.386467 22535416901760 run.py:483] Algo bellman_ford step 2805 current loss 0.318541, current_train_items 89792.
I0302 18:59:54.402628 22535416901760 run.py:483] Algo bellman_ford step 2806 current loss 0.596557, current_train_items 89824.
I0302 18:59:54.425357 22535416901760 run.py:483] Algo bellman_ford step 2807 current loss 0.630745, current_train_items 89856.
I0302 18:59:54.456761 22535416901760 run.py:483] Algo bellman_ford step 2808 current loss 0.868674, current_train_items 89888.
I0302 18:59:54.490798 22535416901760 run.py:483] Algo bellman_ford step 2809 current loss 0.962451, current_train_items 89920.
I0302 18:59:54.510859 22535416901760 run.py:483] Algo bellman_ford step 2810 current loss 0.278987, current_train_items 89952.
I0302 18:59:54.527395 22535416901760 run.py:483] Algo bellman_ford step 2811 current loss 0.628985, current_train_items 89984.
I0302 18:59:54.550124 22535416901760 run.py:483] Algo bellman_ford step 2812 current loss 0.664977, current_train_items 90016.
I0302 18:59:54.581782 22535416901760 run.py:483] Algo bellman_ford step 2813 current loss 1.007435, current_train_items 90048.
I0302 18:59:54.614811 22535416901760 run.py:483] Algo bellman_ford step 2814 current loss 0.904854, current_train_items 90080.
I0302 18:59:54.634175 22535416901760 run.py:483] Algo bellman_ford step 2815 current loss 0.340569, current_train_items 90112.
I0302 18:59:54.650512 22535416901760 run.py:483] Algo bellman_ford step 2816 current loss 0.539678, current_train_items 90144.
I0302 18:59:54.674186 22535416901760 run.py:483] Algo bellman_ford step 2817 current loss 0.774700, current_train_items 90176.
I0302 18:59:54.702101 22535416901760 run.py:483] Algo bellman_ford step 2818 current loss 0.686214, current_train_items 90208.
I0302 18:59:54.735588 22535416901760 run.py:483] Algo bellman_ford step 2819 current loss 0.975180, current_train_items 90240.
I0302 18:59:54.754982 22535416901760 run.py:483] Algo bellman_ford step 2820 current loss 0.364449, current_train_items 90272.
I0302 18:59:54.770969 22535416901760 run.py:483] Algo bellman_ford step 2821 current loss 0.473470, current_train_items 90304.
I0302 18:59:54.794523 22535416901760 run.py:483] Algo bellman_ford step 2822 current loss 0.630595, current_train_items 90336.
I0302 18:59:54.825073 22535416901760 run.py:483] Algo bellman_ford step 2823 current loss 0.916459, current_train_items 90368.
I0302 18:59:54.861008 22535416901760 run.py:483] Algo bellman_ford step 2824 current loss 0.934749, current_train_items 90400.
I0302 18:59:54.880398 22535416901760 run.py:483] Algo bellman_ford step 2825 current loss 0.317009, current_train_items 90432.
I0302 18:59:54.896207 22535416901760 run.py:483] Algo bellman_ford step 2826 current loss 0.624343, current_train_items 90464.
I0302 18:59:54.920024 22535416901760 run.py:483] Algo bellman_ford step 2827 current loss 0.729705, current_train_items 90496.
I0302 18:59:54.950848 22535416901760 run.py:483] Algo bellman_ford step 2828 current loss 0.729499, current_train_items 90528.
I0302 18:59:54.982905 22535416901760 run.py:483] Algo bellman_ford step 2829 current loss 0.673249, current_train_items 90560.
I0302 18:59:55.002420 22535416901760 run.py:483] Algo bellman_ford step 2830 current loss 0.264574, current_train_items 90592.
I0302 18:59:55.018955 22535416901760 run.py:483] Algo bellman_ford step 2831 current loss 0.530186, current_train_items 90624.
I0302 18:59:55.041967 22535416901760 run.py:483] Algo bellman_ford step 2832 current loss 0.663530, current_train_items 90656.
I0302 18:59:55.071692 22535416901760 run.py:483] Algo bellman_ford step 2833 current loss 0.728614, current_train_items 90688.
I0302 18:59:55.104775 22535416901760 run.py:483] Algo bellman_ford step 2834 current loss 0.878912, current_train_items 90720.
I0302 18:59:55.124254 22535416901760 run.py:483] Algo bellman_ford step 2835 current loss 0.309365, current_train_items 90752.
I0302 18:59:55.139950 22535416901760 run.py:483] Algo bellman_ford step 2836 current loss 0.477214, current_train_items 90784.
I0302 18:59:55.162659 22535416901760 run.py:483] Algo bellman_ford step 2837 current loss 0.549781, current_train_items 90816.
I0302 18:59:55.193210 22535416901760 run.py:483] Algo bellman_ford step 2838 current loss 0.747360, current_train_items 90848.
I0302 18:59:55.229032 22535416901760 run.py:483] Algo bellman_ford step 2839 current loss 0.902236, current_train_items 90880.
I0302 18:59:55.248870 22535416901760 run.py:483] Algo bellman_ford step 2840 current loss 0.287236, current_train_items 90912.
I0302 18:59:55.264777 22535416901760 run.py:483] Algo bellman_ford step 2841 current loss 0.535787, current_train_items 90944.
I0302 18:59:55.288413 22535416901760 run.py:483] Algo bellman_ford step 2842 current loss 0.745629, current_train_items 90976.
I0302 18:59:55.320004 22535416901760 run.py:483] Algo bellman_ford step 2843 current loss 0.784640, current_train_items 91008.
I0302 18:59:55.353840 22535416901760 run.py:483] Algo bellman_ford step 2844 current loss 0.945918, current_train_items 91040.
I0302 18:59:55.373256 22535416901760 run.py:483] Algo bellman_ford step 2845 current loss 0.323733, current_train_items 91072.
I0302 18:59:55.389822 22535416901760 run.py:483] Algo bellman_ford step 2846 current loss 0.556660, current_train_items 91104.
I0302 18:59:55.412870 22535416901760 run.py:483] Algo bellman_ford step 2847 current loss 0.667484, current_train_items 91136.
I0302 18:59:55.443353 22535416901760 run.py:483] Algo bellman_ford step 2848 current loss 0.648730, current_train_items 91168.
I0302 18:59:55.477848 22535416901760 run.py:483] Algo bellman_ford step 2849 current loss 0.962822, current_train_items 91200.
I0302 18:59:55.497457 22535416901760 run.py:483] Algo bellman_ford step 2850 current loss 0.355002, current_train_items 91232.
I0302 18:59:55.505461 22535416901760 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:55.505570 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 18:59:55.521998 22535416901760 run.py:483] Algo bellman_ford step 2851 current loss 0.436562, current_train_items 91264.
I0302 18:59:55.545554 22535416901760 run.py:483] Algo bellman_ford step 2852 current loss 0.615606, current_train_items 91296.
I0302 18:59:55.577384 22535416901760 run.py:483] Algo bellman_ford step 2853 current loss 0.738253, current_train_items 91328.
I0302 18:59:55.610836 22535416901760 run.py:483] Algo bellman_ford step 2854 current loss 0.815254, current_train_items 91360.
I0302 18:59:55.630698 22535416901760 run.py:483] Algo bellman_ford step 2855 current loss 0.353629, current_train_items 91392.
I0302 18:59:55.646610 22535416901760 run.py:483] Algo bellman_ford step 2856 current loss 0.547037, current_train_items 91424.
I0302 18:59:55.670580 22535416901760 run.py:483] Algo bellman_ford step 2857 current loss 0.693914, current_train_items 91456.
I0302 18:59:55.699346 22535416901760 run.py:483] Algo bellman_ford step 2858 current loss 0.626677, current_train_items 91488.
I0302 18:59:55.731498 22535416901760 run.py:483] Algo bellman_ford step 2859 current loss 0.730955, current_train_items 91520.
I0302 18:59:55.751333 22535416901760 run.py:483] Algo bellman_ford step 2860 current loss 0.361492, current_train_items 91552.
I0302 18:59:55.767585 22535416901760 run.py:483] Algo bellman_ford step 2861 current loss 0.470639, current_train_items 91584.
I0302 18:59:55.790837 22535416901760 run.py:483] Algo bellman_ford step 2862 current loss 0.611338, current_train_items 91616.
I0302 18:59:55.822121 22535416901760 run.py:483] Algo bellman_ford step 2863 current loss 0.799498, current_train_items 91648.
I0302 18:59:55.852971 22535416901760 run.py:483] Algo bellman_ford step 2864 current loss 0.741405, current_train_items 91680.
I0302 18:59:55.872976 22535416901760 run.py:483] Algo bellman_ford step 2865 current loss 0.334912, current_train_items 91712.
I0302 18:59:55.889160 22535416901760 run.py:483] Algo bellman_ford step 2866 current loss 0.533941, current_train_items 91744.
I0302 18:59:55.912528 22535416901760 run.py:483] Algo bellman_ford step 2867 current loss 0.785833, current_train_items 91776.
I0302 18:59:55.942766 22535416901760 run.py:483] Algo bellman_ford step 2868 current loss 0.698624, current_train_items 91808.
I0302 18:59:55.976691 22535416901760 run.py:483] Algo bellman_ford step 2869 current loss 0.855897, current_train_items 91840.
I0302 18:59:55.996483 22535416901760 run.py:483] Algo bellman_ford step 2870 current loss 0.284414, current_train_items 91872.
I0302 18:59:56.012737 22535416901760 run.py:483] Algo bellman_ford step 2871 current loss 0.487229, current_train_items 91904.
I0302 18:59:56.036535 22535416901760 run.py:483] Algo bellman_ford step 2872 current loss 0.660909, current_train_items 91936.
I0302 18:59:56.068335 22535416901760 run.py:483] Algo bellman_ford step 2873 current loss 0.788740, current_train_items 91968.
I0302 18:59:56.100599 22535416901760 run.py:483] Algo bellman_ford step 2874 current loss 0.844126, current_train_items 92000.
I0302 18:59:56.120754 22535416901760 run.py:483] Algo bellman_ford step 2875 current loss 0.350544, current_train_items 92032.
I0302 18:59:56.136992 22535416901760 run.py:483] Algo bellman_ford step 2876 current loss 0.574515, current_train_items 92064.
I0302 18:59:56.160042 22535416901760 run.py:483] Algo bellman_ford step 2877 current loss 0.639169, current_train_items 92096.
I0302 18:59:56.192050 22535416901760 run.py:483] Algo bellman_ford step 2878 current loss 0.883579, current_train_items 92128.
I0302 18:59:56.224347 22535416901760 run.py:483] Algo bellman_ford step 2879 current loss 0.817899, current_train_items 92160.
I0302 18:59:56.243895 22535416901760 run.py:483] Algo bellman_ford step 2880 current loss 0.355077, current_train_items 92192.
I0302 18:59:56.259993 22535416901760 run.py:483] Algo bellman_ford step 2881 current loss 0.546445, current_train_items 92224.
I0302 18:59:56.283527 22535416901760 run.py:483] Algo bellman_ford step 2882 current loss 0.581591, current_train_items 92256.
I0302 18:59:56.314408 22535416901760 run.py:483] Algo bellman_ford step 2883 current loss 0.716536, current_train_items 92288.
I0302 18:59:56.345957 22535416901760 run.py:483] Algo bellman_ford step 2884 current loss 0.932940, current_train_items 92320.
I0302 18:59:56.365690 22535416901760 run.py:483] Algo bellman_ford step 2885 current loss 0.297950, current_train_items 92352.
I0302 18:59:56.381903 22535416901760 run.py:483] Algo bellman_ford step 2886 current loss 0.598661, current_train_items 92384.
I0302 18:59:56.405312 22535416901760 run.py:483] Algo bellman_ford step 2887 current loss 0.754278, current_train_items 92416.
I0302 18:59:56.435667 22535416901760 run.py:483] Algo bellman_ford step 2888 current loss 0.772488, current_train_items 92448.
I0302 18:59:56.469555 22535416901760 run.py:483] Algo bellman_ford step 2889 current loss 1.207288, current_train_items 92480.
I0302 18:59:56.489640 22535416901760 run.py:483] Algo bellman_ford step 2890 current loss 0.396694, current_train_items 92512.
I0302 18:59:56.505787 22535416901760 run.py:483] Algo bellman_ford step 2891 current loss 0.501008, current_train_items 92544.
I0302 18:59:56.530223 22535416901760 run.py:483] Algo bellman_ford step 2892 current loss 0.723133, current_train_items 92576.
I0302 18:59:56.559821 22535416901760 run.py:483] Algo bellman_ford step 2893 current loss 0.737823, current_train_items 92608.
I0302 18:59:56.593472 22535416901760 run.py:483] Algo bellman_ford step 2894 current loss 0.962862, current_train_items 92640.
I0302 18:59:56.613229 22535416901760 run.py:483] Algo bellman_ford step 2895 current loss 0.343378, current_train_items 92672.
I0302 18:59:56.630034 22535416901760 run.py:483] Algo bellman_ford step 2896 current loss 0.536918, current_train_items 92704.
I0302 18:59:56.651979 22535416901760 run.py:483] Algo bellman_ford step 2897 current loss 0.605116, current_train_items 92736.
I0302 18:59:56.681972 22535416901760 run.py:483] Algo bellman_ford step 2898 current loss 0.681115, current_train_items 92768.
I0302 18:59:56.712980 22535416901760 run.py:483] Algo bellman_ford step 2899 current loss 0.707090, current_train_items 92800.
I0302 18:59:56.732769 22535416901760 run.py:483] Algo bellman_ford step 2900 current loss 0.279297, current_train_items 92832.
I0302 18:59:56.740725 22535416901760 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:56.740829 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 18:59:56.757004 22535416901760 run.py:483] Algo bellman_ford step 2901 current loss 0.474658, current_train_items 92864.
I0302 18:59:56.780225 22535416901760 run.py:483] Algo bellman_ford step 2902 current loss 0.685116, current_train_items 92896.
I0302 18:59:56.811047 22535416901760 run.py:483] Algo bellman_ford step 2903 current loss 0.737485, current_train_items 92928.
I0302 18:59:56.845218 22535416901760 run.py:483] Algo bellman_ford step 2904 current loss 0.872698, current_train_items 92960.
I0302 18:59:56.865261 22535416901760 run.py:483] Algo bellman_ford step 2905 current loss 0.383903, current_train_items 92992.
I0302 18:59:56.881274 22535416901760 run.py:483] Algo bellman_ford step 2906 current loss 0.522092, current_train_items 93024.
I0302 18:59:56.905228 22535416901760 run.py:483] Algo bellman_ford step 2907 current loss 0.712377, current_train_items 93056.
I0302 18:59:56.937805 22535416901760 run.py:483] Algo bellman_ford step 2908 current loss 0.886247, current_train_items 93088.
I0302 18:59:56.971668 22535416901760 run.py:483] Algo bellman_ford step 2909 current loss 0.842370, current_train_items 93120.
I0302 18:59:56.991091 22535416901760 run.py:483] Algo bellman_ford step 2910 current loss 0.369078, current_train_items 93152.
I0302 18:59:57.007869 22535416901760 run.py:483] Algo bellman_ford step 2911 current loss 0.617709, current_train_items 93184.
I0302 18:59:57.031365 22535416901760 run.py:483] Algo bellman_ford step 2912 current loss 0.650553, current_train_items 93216.
I0302 18:59:57.062192 22535416901760 run.py:483] Algo bellman_ford step 2913 current loss 0.709805, current_train_items 93248.
I0302 18:59:57.096765 22535416901760 run.py:483] Algo bellman_ford step 2914 current loss 0.976269, current_train_items 93280.
I0302 18:59:57.116595 22535416901760 run.py:483] Algo bellman_ford step 2915 current loss 0.305556, current_train_items 93312.
I0302 18:59:57.132895 22535416901760 run.py:483] Algo bellman_ford step 2916 current loss 0.490607, current_train_items 93344.
I0302 18:59:57.155825 22535416901760 run.py:483] Algo bellman_ford step 2917 current loss 0.605196, current_train_items 93376.
I0302 18:59:57.187787 22535416901760 run.py:483] Algo bellman_ford step 2918 current loss 0.823644, current_train_items 93408.
I0302 18:59:57.222549 22535416901760 run.py:483] Algo bellman_ford step 2919 current loss 0.953396, current_train_items 93440.
I0302 18:59:57.242621 22535416901760 run.py:483] Algo bellman_ford step 2920 current loss 0.329664, current_train_items 93472.
I0302 18:59:57.258844 22535416901760 run.py:483] Algo bellman_ford step 2921 current loss 0.550996, current_train_items 93504.
I0302 18:59:57.283232 22535416901760 run.py:483] Algo bellman_ford step 2922 current loss 0.669110, current_train_items 93536.
I0302 18:59:57.313223 22535416901760 run.py:483] Algo bellman_ford step 2923 current loss 0.711295, current_train_items 93568.
I0302 18:59:57.348628 22535416901760 run.py:483] Algo bellman_ford step 2924 current loss 0.928300, current_train_items 93600.
I0302 18:59:57.368348 22535416901760 run.py:483] Algo bellman_ford step 2925 current loss 0.329474, current_train_items 93632.
I0302 18:59:57.384374 22535416901760 run.py:483] Algo bellman_ford step 2926 current loss 0.509416, current_train_items 93664.
I0302 18:59:57.408754 22535416901760 run.py:483] Algo bellman_ford step 2927 current loss 0.755435, current_train_items 93696.
I0302 18:59:57.440093 22535416901760 run.py:483] Algo bellman_ford step 2928 current loss 0.735726, current_train_items 93728.
I0302 18:59:57.474095 22535416901760 run.py:483] Algo bellman_ford step 2929 current loss 0.925841, current_train_items 93760.
I0302 18:59:57.493987 22535416901760 run.py:483] Algo bellman_ford step 2930 current loss 0.342297, current_train_items 93792.
I0302 18:59:57.509964 22535416901760 run.py:483] Algo bellman_ford step 2931 current loss 0.485668, current_train_items 93824.
I0302 18:59:57.535201 22535416901760 run.py:483] Algo bellman_ford step 2932 current loss 0.766741, current_train_items 93856.
I0302 18:59:57.564952 22535416901760 run.py:483] Algo bellman_ford step 2933 current loss 0.810160, current_train_items 93888.
I0302 18:59:57.597930 22535416901760 run.py:483] Algo bellman_ford step 2934 current loss 0.815550, current_train_items 93920.
I0302 18:59:57.617748 22535416901760 run.py:483] Algo bellman_ford step 2935 current loss 0.325403, current_train_items 93952.
I0302 18:59:57.633761 22535416901760 run.py:483] Algo bellman_ford step 2936 current loss 0.423509, current_train_items 93984.
I0302 18:59:57.657402 22535416901760 run.py:483] Algo bellman_ford step 2937 current loss 0.685440, current_train_items 94016.
I0302 18:59:57.686415 22535416901760 run.py:483] Algo bellman_ford step 2938 current loss 0.619197, current_train_items 94048.
I0302 18:59:57.719644 22535416901760 run.py:483] Algo bellman_ford step 2939 current loss 0.880781, current_train_items 94080.
I0302 18:59:57.739355 22535416901760 run.py:483] Algo bellman_ford step 2940 current loss 0.396100, current_train_items 94112.
I0302 18:59:57.755805 22535416901760 run.py:483] Algo bellman_ford step 2941 current loss 0.569992, current_train_items 94144.
I0302 18:59:57.779665 22535416901760 run.py:483] Algo bellman_ford step 2942 current loss 0.662426, current_train_items 94176.
I0302 18:59:57.811570 22535416901760 run.py:483] Algo bellman_ford step 2943 current loss 0.823482, current_train_items 94208.
I0302 18:59:57.846613 22535416901760 run.py:483] Algo bellman_ford step 2944 current loss 0.817157, current_train_items 94240.
I0302 18:59:57.866052 22535416901760 run.py:483] Algo bellman_ford step 2945 current loss 0.406539, current_train_items 94272.
I0302 18:59:57.882563 22535416901760 run.py:483] Algo bellman_ford step 2946 current loss 0.539517, current_train_items 94304.
I0302 18:59:57.906122 22535416901760 run.py:483] Algo bellman_ford step 2947 current loss 0.600590, current_train_items 94336.
I0302 18:59:57.936751 22535416901760 run.py:483] Algo bellman_ford step 2948 current loss 0.681313, current_train_items 94368.
I0302 18:59:57.969820 22535416901760 run.py:483] Algo bellman_ford step 2949 current loss 0.719081, current_train_items 94400.
I0302 18:59:57.989241 22535416901760 run.py:483] Algo bellman_ford step 2950 current loss 0.337596, current_train_items 94432.
I0302 18:59:57.997531 22535416901760 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:57.997637 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 18:59:58.014274 22535416901760 run.py:483] Algo bellman_ford step 2951 current loss 0.590233, current_train_items 94464.
I0302 18:59:58.036290 22535416901760 run.py:483] Algo bellman_ford step 2952 current loss 0.480103, current_train_items 94496.
I0302 18:59:58.066334 22535416901760 run.py:483] Algo bellman_ford step 2953 current loss 0.688717, current_train_items 94528.
I0302 18:59:58.100258 22535416901760 run.py:483] Algo bellman_ford step 2954 current loss 0.786424, current_train_items 94560.
I0302 18:59:58.120369 22535416901760 run.py:483] Algo bellman_ford step 2955 current loss 0.296133, current_train_items 94592.
I0302 18:59:58.135916 22535416901760 run.py:483] Algo bellman_ford step 2956 current loss 0.543509, current_train_items 94624.
I0302 18:59:58.159863 22535416901760 run.py:483] Algo bellman_ford step 2957 current loss 0.755427, current_train_items 94656.
I0302 18:59:58.191438 22535416901760 run.py:483] Algo bellman_ford step 2958 current loss 0.696148, current_train_items 94688.
I0302 18:59:58.225050 22535416901760 run.py:483] Algo bellman_ford step 2959 current loss 0.840350, current_train_items 94720.
I0302 18:59:58.244970 22535416901760 run.py:483] Algo bellman_ford step 2960 current loss 0.324197, current_train_items 94752.
I0302 18:59:58.261432 22535416901760 run.py:483] Algo bellman_ford step 2961 current loss 0.524727, current_train_items 94784.
I0302 18:59:58.284231 22535416901760 run.py:483] Algo bellman_ford step 2962 current loss 0.861961, current_train_items 94816.
I0302 18:59:58.313797 22535416901760 run.py:483] Algo bellman_ford step 2963 current loss 0.759141, current_train_items 94848.
I0302 18:59:58.348136 22535416901760 run.py:483] Algo bellman_ford step 2964 current loss 0.827510, current_train_items 94880.
I0302 18:59:58.367511 22535416901760 run.py:483] Algo bellman_ford step 2965 current loss 0.295835, current_train_items 94912.
I0302 18:59:58.383599 22535416901760 run.py:483] Algo bellman_ford step 2966 current loss 0.546808, current_train_items 94944.
I0302 18:59:58.406823 22535416901760 run.py:483] Algo bellman_ford step 2967 current loss 0.708334, current_train_items 94976.
I0302 18:59:58.437160 22535416901760 run.py:483] Algo bellman_ford step 2968 current loss 0.678528, current_train_items 95008.
I0302 18:59:58.471416 22535416901760 run.py:483] Algo bellman_ford step 2969 current loss 0.825705, current_train_items 95040.
I0302 18:59:58.491135 22535416901760 run.py:483] Algo bellman_ford step 2970 current loss 0.311583, current_train_items 95072.
I0302 18:59:58.507143 22535416901760 run.py:483] Algo bellman_ford step 2971 current loss 0.568725, current_train_items 95104.
I0302 18:59:58.530147 22535416901760 run.py:483] Algo bellman_ford step 2972 current loss 0.736870, current_train_items 95136.
I0302 18:59:58.559995 22535416901760 run.py:483] Algo bellman_ford step 2973 current loss 0.734817, current_train_items 95168.
I0302 18:59:58.595968 22535416901760 run.py:483] Algo bellman_ford step 2974 current loss 1.079956, current_train_items 95200.
I0302 18:59:58.615637 22535416901760 run.py:483] Algo bellman_ford step 2975 current loss 0.301792, current_train_items 95232.
I0302 18:59:58.632467 22535416901760 run.py:483] Algo bellman_ford step 2976 current loss 0.518324, current_train_items 95264.
I0302 18:59:58.655617 22535416901760 run.py:483] Algo bellman_ford step 2977 current loss 0.617586, current_train_items 95296.
I0302 18:59:58.685658 22535416901760 run.py:483] Algo bellman_ford step 2978 current loss 0.746336, current_train_items 95328.
I0302 18:59:58.720487 22535416901760 run.py:483] Algo bellman_ford step 2979 current loss 0.963417, current_train_items 95360.
I0302 18:59:58.739852 22535416901760 run.py:483] Algo bellman_ford step 2980 current loss 0.337508, current_train_items 95392.
I0302 18:59:58.755869 22535416901760 run.py:483] Algo bellman_ford step 2981 current loss 0.529419, current_train_items 95424.
I0302 18:59:58.779729 22535416901760 run.py:483] Algo bellman_ford step 2982 current loss 0.677072, current_train_items 95456.
I0302 18:59:58.812268 22535416901760 run.py:483] Algo bellman_ford step 2983 current loss 0.743723, current_train_items 95488.
I0302 18:59:58.845590 22535416901760 run.py:483] Algo bellman_ford step 2984 current loss 0.907481, current_train_items 95520.
I0302 18:59:58.865076 22535416901760 run.py:483] Algo bellman_ford step 2985 current loss 0.331087, current_train_items 95552.
I0302 18:59:58.881668 22535416901760 run.py:483] Algo bellman_ford step 2986 current loss 0.606798, current_train_items 95584.
I0302 18:59:58.905504 22535416901760 run.py:483] Algo bellman_ford step 2987 current loss 0.689013, current_train_items 95616.
I0302 18:59:58.935961 22535416901760 run.py:483] Algo bellman_ford step 2988 current loss 0.671917, current_train_items 95648.
I0302 18:59:58.970032 22535416901760 run.py:483] Algo bellman_ford step 2989 current loss 0.838952, current_train_items 95680.
I0302 18:59:58.989726 22535416901760 run.py:483] Algo bellman_ford step 2990 current loss 0.321604, current_train_items 95712.
I0302 18:59:59.006536 22535416901760 run.py:483] Algo bellman_ford step 2991 current loss 0.504553, current_train_items 95744.
I0302 18:59:59.030033 22535416901760 run.py:483] Algo bellman_ford step 2992 current loss 0.644975, current_train_items 95776.
I0302 18:59:59.061731 22535416901760 run.py:483] Algo bellman_ford step 2993 current loss 0.814516, current_train_items 95808.
I0302 18:59:59.096436 22535416901760 run.py:483] Algo bellman_ford step 2994 current loss 1.077742, current_train_items 95840.
I0302 18:59:59.115707 22535416901760 run.py:483] Algo bellman_ford step 2995 current loss 0.263818, current_train_items 95872.
I0302 18:59:59.131987 22535416901760 run.py:483] Algo bellman_ford step 2996 current loss 0.487246, current_train_items 95904.
I0302 18:59:59.156592 22535416901760 run.py:483] Algo bellman_ford step 2997 current loss 0.689031, current_train_items 95936.
I0302 18:59:59.187978 22535416901760 run.py:483] Algo bellman_ford step 2998 current loss 0.646553, current_train_items 95968.
I0302 18:59:59.220405 22535416901760 run.py:483] Algo bellman_ford step 2999 current loss 0.826235, current_train_items 96000.
I0302 18:59:59.240167 22535416901760 run.py:483] Algo bellman_ford step 3000 current loss 0.350630, current_train_items 96032.
I0302 18:59:59.248135 22535416901760 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:59.248252 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 18:59:59.264748 22535416901760 run.py:483] Algo bellman_ford step 3001 current loss 0.447588, current_train_items 96064.
I0302 18:59:59.287810 22535416901760 run.py:483] Algo bellman_ford step 3002 current loss 0.534801, current_train_items 96096.
I0302 18:59:59.318863 22535416901760 run.py:483] Algo bellman_ford step 3003 current loss 0.771176, current_train_items 96128.
I0302 18:59:59.351816 22535416901760 run.py:483] Algo bellman_ford step 3004 current loss 0.696160, current_train_items 96160.
I0302 18:59:59.371792 22535416901760 run.py:483] Algo bellman_ford step 3005 current loss 0.301022, current_train_items 96192.
I0302 18:59:59.387610 22535416901760 run.py:483] Algo bellman_ford step 3006 current loss 0.452386, current_train_items 96224.
I0302 18:59:59.412435 22535416901760 run.py:483] Algo bellman_ford step 3007 current loss 0.725675, current_train_items 96256.
I0302 18:59:59.443812 22535416901760 run.py:483] Algo bellman_ford step 3008 current loss 0.765667, current_train_items 96288.
I0302 18:59:59.479289 22535416901760 run.py:483] Algo bellman_ford step 3009 current loss 0.914160, current_train_items 96320.
I0302 18:59:59.498908 22535416901760 run.py:483] Algo bellman_ford step 3010 current loss 0.348398, current_train_items 96352.
I0302 18:59:59.515082 22535416901760 run.py:483] Algo bellman_ford step 3011 current loss 0.636149, current_train_items 96384.
I0302 18:59:59.538613 22535416901760 run.py:483] Algo bellman_ford step 3012 current loss 0.613727, current_train_items 96416.
I0302 18:59:59.569031 22535416901760 run.py:483] Algo bellman_ford step 3013 current loss 0.741986, current_train_items 96448.
I0302 18:59:59.603189 22535416901760 run.py:483] Algo bellman_ford step 3014 current loss 1.112829, current_train_items 96480.
I0302 18:59:59.623011 22535416901760 run.py:483] Algo bellman_ford step 3015 current loss 0.357879, current_train_items 96512.
I0302 18:59:59.639544 22535416901760 run.py:483] Algo bellman_ford step 3016 current loss 0.537730, current_train_items 96544.
I0302 18:59:59.662648 22535416901760 run.py:483] Algo bellman_ford step 3017 current loss 0.817636, current_train_items 96576.
I0302 18:59:59.692975 22535416901760 run.py:483] Algo bellman_ford step 3018 current loss 0.807801, current_train_items 96608.
I0302 18:59:59.724953 22535416901760 run.py:483] Algo bellman_ford step 3019 current loss 1.117921, current_train_items 96640.
I0302 18:59:59.744366 22535416901760 run.py:483] Algo bellman_ford step 3020 current loss 0.249651, current_train_items 96672.
I0302 18:59:59.760082 22535416901760 run.py:483] Algo bellman_ford step 3021 current loss 0.517055, current_train_items 96704.
I0302 18:59:59.784516 22535416901760 run.py:483] Algo bellman_ford step 3022 current loss 0.733315, current_train_items 96736.
I0302 18:59:59.815673 22535416901760 run.py:483] Algo bellman_ford step 3023 current loss 0.827007, current_train_items 96768.
I0302 18:59:59.849144 22535416901760 run.py:483] Algo bellman_ford step 3024 current loss 1.004157, current_train_items 96800.
I0302 18:59:59.868438 22535416901760 run.py:483] Algo bellman_ford step 3025 current loss 0.278564, current_train_items 96832.
I0302 18:59:59.884493 22535416901760 run.py:483] Algo bellman_ford step 3026 current loss 0.555506, current_train_items 96864.
I0302 18:59:59.906984 22535416901760 run.py:483] Algo bellman_ford step 3027 current loss 0.607423, current_train_items 96896.
I0302 18:59:59.937572 22535416901760 run.py:483] Algo bellman_ford step 3028 current loss 0.757024, current_train_items 96928.
I0302 18:59:59.969494 22535416901760 run.py:483] Algo bellman_ford step 3029 current loss 0.912160, current_train_items 96960.
I0302 18:59:59.988972 22535416901760 run.py:483] Algo bellman_ford step 3030 current loss 0.331235, current_train_items 96992.
I0302 19:00:00.004837 22535416901760 run.py:483] Algo bellman_ford step 3031 current loss 0.451079, current_train_items 97024.
I0302 19:00:00.028839 22535416901760 run.py:483] Algo bellman_ford step 3032 current loss 0.630541, current_train_items 97056.
I0302 19:00:00.059968 22535416901760 run.py:483] Algo bellman_ford step 3033 current loss 0.792210, current_train_items 97088.
I0302 19:00:00.094240 22535416901760 run.py:483] Algo bellman_ford step 3034 current loss 0.852444, current_train_items 97120.
I0302 19:00:00.113762 22535416901760 run.py:483] Algo bellman_ford step 3035 current loss 0.328703, current_train_items 97152.
I0302 19:00:00.130196 22535416901760 run.py:483] Algo bellman_ford step 3036 current loss 0.524734, current_train_items 97184.
I0302 19:00:00.153553 22535416901760 run.py:483] Algo bellman_ford step 3037 current loss 0.679532, current_train_items 97216.
I0302 19:00:00.184664 22535416901760 run.py:483] Algo bellman_ford step 3038 current loss 0.738056, current_train_items 97248.
I0302 19:00:00.216782 22535416901760 run.py:483] Algo bellman_ford step 3039 current loss 0.856128, current_train_items 97280.
I0302 19:00:00.236219 22535416901760 run.py:483] Algo bellman_ford step 3040 current loss 0.305793, current_train_items 97312.
I0302 19:00:00.252477 22535416901760 run.py:483] Algo bellman_ford step 3041 current loss 0.433091, current_train_items 97344.
I0302 19:00:00.275688 22535416901760 run.py:483] Algo bellman_ford step 3042 current loss 0.589017, current_train_items 97376.
I0302 19:00:00.306447 22535416901760 run.py:483] Algo bellman_ford step 3043 current loss 0.746285, current_train_items 97408.
I0302 19:00:00.338654 22535416901760 run.py:483] Algo bellman_ford step 3044 current loss 0.726555, current_train_items 97440.
I0302 19:00:00.358440 22535416901760 run.py:483] Algo bellman_ford step 3045 current loss 0.336810, current_train_items 97472.
I0302 19:00:00.374505 22535416901760 run.py:483] Algo bellman_ford step 3046 current loss 0.571421, current_train_items 97504.
I0302 19:00:00.397970 22535416901760 run.py:483] Algo bellman_ford step 3047 current loss 0.784848, current_train_items 97536.
I0302 19:00:00.429341 22535416901760 run.py:483] Algo bellman_ford step 3048 current loss 0.799679, current_train_items 97568.
I0302 19:00:00.462094 22535416901760 run.py:483] Algo bellman_ford step 3049 current loss 0.860782, current_train_items 97600.
I0302 19:00:00.481395 22535416901760 run.py:483] Algo bellman_ford step 3050 current loss 0.290566, current_train_items 97632.
I0302 19:00:00.489420 22535416901760 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 19:00:00.489524 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:00.505925 22535416901760 run.py:483] Algo bellman_ford step 3051 current loss 0.522619, current_train_items 97664.
I0302 19:00:00.529616 22535416901760 run.py:483] Algo bellman_ford step 3052 current loss 0.753916, current_train_items 97696.
I0302 19:00:00.561398 22535416901760 run.py:483] Algo bellman_ford step 3053 current loss 0.708943, current_train_items 97728.
I0302 19:00:00.593177 22535416901760 run.py:483] Algo bellman_ford step 3054 current loss 0.799585, current_train_items 97760.
I0302 19:00:00.613278 22535416901760 run.py:483] Algo bellman_ford step 3055 current loss 0.375205, current_train_items 97792.
I0302 19:00:00.628983 22535416901760 run.py:483] Algo bellman_ford step 3056 current loss 0.481031, current_train_items 97824.
I0302 19:00:00.652301 22535416901760 run.py:483] Algo bellman_ford step 3057 current loss 0.634196, current_train_items 97856.
I0302 19:00:00.682898 22535416901760 run.py:483] Algo bellman_ford step 3058 current loss 0.733953, current_train_items 97888.
I0302 19:00:00.715246 22535416901760 run.py:483] Algo bellman_ford step 3059 current loss 0.876311, current_train_items 97920.
I0302 19:00:00.735179 22535416901760 run.py:483] Algo bellman_ford step 3060 current loss 0.354603, current_train_items 97952.
I0302 19:00:00.751543 22535416901760 run.py:483] Algo bellman_ford step 3061 current loss 0.547131, current_train_items 97984.
I0302 19:00:00.774025 22535416901760 run.py:483] Algo bellman_ford step 3062 current loss 0.627970, current_train_items 98016.
I0302 19:00:00.805782 22535416901760 run.py:483] Algo bellman_ford step 3063 current loss 0.847455, current_train_items 98048.
I0302 19:00:00.840294 22535416901760 run.py:483] Algo bellman_ford step 3064 current loss 0.842877, current_train_items 98080.
I0302 19:00:00.859706 22535416901760 run.py:483] Algo bellman_ford step 3065 current loss 0.360633, current_train_items 98112.
I0302 19:00:00.875764 22535416901760 run.py:483] Algo bellman_ford step 3066 current loss 0.581864, current_train_items 98144.
I0302 19:00:00.900214 22535416901760 run.py:483] Algo bellman_ford step 3067 current loss 0.758542, current_train_items 98176.
I0302 19:00:00.930577 22535416901760 run.py:483] Algo bellman_ford step 3068 current loss 0.731480, current_train_items 98208.
I0302 19:00:00.966160 22535416901760 run.py:483] Algo bellman_ford step 3069 current loss 0.945152, current_train_items 98240.
I0302 19:00:00.986040 22535416901760 run.py:483] Algo bellman_ford step 3070 current loss 0.273225, current_train_items 98272.
I0302 19:00:01.002343 22535416901760 run.py:483] Algo bellman_ford step 3071 current loss 0.549855, current_train_items 98304.
I0302 19:00:01.024973 22535416901760 run.py:483] Algo bellman_ford step 3072 current loss 0.667066, current_train_items 98336.
I0302 19:00:01.054936 22535416901760 run.py:483] Algo bellman_ford step 3073 current loss 0.810530, current_train_items 98368.
I0302 19:00:01.090448 22535416901760 run.py:483] Algo bellman_ford step 3074 current loss 0.997886, current_train_items 98400.
I0302 19:00:01.110317 22535416901760 run.py:483] Algo bellman_ford step 3075 current loss 0.286664, current_train_items 98432.
I0302 19:00:01.126868 22535416901760 run.py:483] Algo bellman_ford step 3076 current loss 0.665032, current_train_items 98464.
I0302 19:00:01.149952 22535416901760 run.py:483] Algo bellman_ford step 3077 current loss 0.649900, current_train_items 98496.
I0302 19:00:01.180713 22535416901760 run.py:483] Algo bellman_ford step 3078 current loss 0.806295, current_train_items 98528.
I0302 19:00:01.212618 22535416901760 run.py:483] Algo bellman_ford step 3079 current loss 0.767612, current_train_items 98560.
I0302 19:00:01.232324 22535416901760 run.py:483] Algo bellman_ford step 3080 current loss 0.346888, current_train_items 98592.
I0302 19:00:01.248655 22535416901760 run.py:483] Algo bellman_ford step 3081 current loss 0.555474, current_train_items 98624.
I0302 19:00:01.272011 22535416901760 run.py:483] Algo bellman_ford step 3082 current loss 0.605578, current_train_items 98656.
I0302 19:00:01.302515 22535416901760 run.py:483] Algo bellman_ford step 3083 current loss 0.753005, current_train_items 98688.
I0302 19:00:01.336218 22535416901760 run.py:483] Algo bellman_ford step 3084 current loss 0.995943, current_train_items 98720.
I0302 19:00:01.356080 22535416901760 run.py:483] Algo bellman_ford step 3085 current loss 0.347207, current_train_items 98752.
I0302 19:00:01.372561 22535416901760 run.py:483] Algo bellman_ford step 3086 current loss 0.531813, current_train_items 98784.
I0302 19:00:01.394554 22535416901760 run.py:483] Algo bellman_ford step 3087 current loss 0.568832, current_train_items 98816.
I0302 19:00:01.425419 22535416901760 run.py:483] Algo bellman_ford step 3088 current loss 0.728769, current_train_items 98848.
I0302 19:00:01.460454 22535416901760 run.py:483] Algo bellman_ford step 3089 current loss 0.826320, current_train_items 98880.
I0302 19:00:01.480238 22535416901760 run.py:483] Algo bellman_ford step 3090 current loss 0.289521, current_train_items 98912.
I0302 19:00:01.496309 22535416901760 run.py:483] Algo bellman_ford step 3091 current loss 0.487572, current_train_items 98944.
I0302 19:00:01.518508 22535416901760 run.py:483] Algo bellman_ford step 3092 current loss 0.717341, current_train_items 98976.
I0302 19:00:01.549366 22535416901760 run.py:483] Algo bellman_ford step 3093 current loss 0.759490, current_train_items 99008.
I0302 19:00:01.582734 22535416901760 run.py:483] Algo bellman_ford step 3094 current loss 0.900529, current_train_items 99040.
I0302 19:00:01.602025 22535416901760 run.py:483] Algo bellman_ford step 3095 current loss 0.369977, current_train_items 99072.
I0302 19:00:01.618740 22535416901760 run.py:483] Algo bellman_ford step 3096 current loss 0.572286, current_train_items 99104.
I0302 19:00:01.643001 22535416901760 run.py:483] Algo bellman_ford step 3097 current loss 0.732422, current_train_items 99136.
I0302 19:00:01.673290 22535416901760 run.py:483] Algo bellman_ford step 3098 current loss 0.782010, current_train_items 99168.
I0302 19:00:01.706149 22535416901760 run.py:483] Algo bellman_ford step 3099 current loss 0.800311, current_train_items 99200.
I0302 19:00:01.726304 22535416901760 run.py:483] Algo bellman_ford step 3100 current loss 0.353529, current_train_items 99232.
I0302 19:00:01.734125 22535416901760 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.9013671875, 'score': 0.9013671875, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 19:00:01.734238 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.901, val scores are: bellman_ford: 0.901
I0302 19:00:01.751857 22535416901760 run.py:483] Algo bellman_ford step 3101 current loss 0.511052, current_train_items 99264.
I0302 19:00:01.776187 22535416901760 run.py:483] Algo bellman_ford step 3102 current loss 0.679588, current_train_items 99296.
I0302 19:00:01.807458 22535416901760 run.py:483] Algo bellman_ford step 3103 current loss 0.689137, current_train_items 99328.
I0302 19:00:01.842318 22535416901760 run.py:483] Algo bellman_ford step 3104 current loss 0.850583, current_train_items 99360.
I0302 19:00:01.862577 22535416901760 run.py:483] Algo bellman_ford step 3105 current loss 0.273573, current_train_items 99392.
I0302 19:00:01.878042 22535416901760 run.py:483] Algo bellman_ford step 3106 current loss 0.516853, current_train_items 99424.
I0302 19:00:01.901274 22535416901760 run.py:483] Algo bellman_ford step 3107 current loss 0.748689, current_train_items 99456.
I0302 19:00:01.932664 22535416901760 run.py:483] Algo bellman_ford step 3108 current loss 0.968375, current_train_items 99488.
I0302 19:00:01.965053 22535416901760 run.py:483] Algo bellman_ford step 3109 current loss 0.993535, current_train_items 99520.
I0302 19:00:01.984755 22535416901760 run.py:483] Algo bellman_ford step 3110 current loss 0.291605, current_train_items 99552.
I0302 19:00:02.000903 22535416901760 run.py:483] Algo bellman_ford step 3111 current loss 0.523966, current_train_items 99584.
I0302 19:00:02.024267 22535416901760 run.py:483] Algo bellman_ford step 3112 current loss 0.763514, current_train_items 99616.
I0302 19:00:02.055145 22535416901760 run.py:483] Algo bellman_ford step 3113 current loss 0.768727, current_train_items 99648.
I0302 19:00:02.088925 22535416901760 run.py:483] Algo bellman_ford step 3114 current loss 0.859881, current_train_items 99680.
I0302 19:00:02.108808 22535416901760 run.py:483] Algo bellman_ford step 3115 current loss 0.288101, current_train_items 99712.
I0302 19:00:02.125146 22535416901760 run.py:483] Algo bellman_ford step 3116 current loss 0.503170, current_train_items 99744.
I0302 19:00:02.148664 22535416901760 run.py:483] Algo bellman_ford step 3117 current loss 0.613059, current_train_items 99776.
I0302 19:00:02.179942 22535416901760 run.py:483] Algo bellman_ford step 3118 current loss 0.776290, current_train_items 99808.
I0302 19:00:02.212547 22535416901760 run.py:483] Algo bellman_ford step 3119 current loss 0.763713, current_train_items 99840.
I0302 19:00:02.232048 22535416901760 run.py:483] Algo bellman_ford step 3120 current loss 0.211927, current_train_items 99872.
I0302 19:00:02.248106 22535416901760 run.py:483] Algo bellman_ford step 3121 current loss 0.557861, current_train_items 99904.
I0302 19:00:02.271670 22535416901760 run.py:483] Algo bellman_ford step 3122 current loss 0.596491, current_train_items 99936.
I0302 19:00:02.302801 22535416901760 run.py:483] Algo bellman_ford step 3123 current loss 0.712052, current_train_items 99968.
I0302 19:00:02.335941 22535416901760 run.py:483] Algo bellman_ford step 3124 current loss 0.769909, current_train_items 100000.
I0302 19:00:02.355517 22535416901760 run.py:483] Algo bellman_ford step 3125 current loss 0.350369, current_train_items 100032.
I0302 19:00:02.371697 22535416901760 run.py:483] Algo bellman_ford step 3126 current loss 0.541602, current_train_items 100064.
I0302 19:00:02.394286 22535416901760 run.py:483] Algo bellman_ford step 3127 current loss 0.569890, current_train_items 100096.
I0302 19:00:02.425816 22535416901760 run.py:483] Algo bellman_ford step 3128 current loss 0.710445, current_train_items 100128.
I0302 19:00:02.457887 22535416901760 run.py:483] Algo bellman_ford step 3129 current loss 0.703018, current_train_items 100160.
I0302 19:00:02.477469 22535416901760 run.py:483] Algo bellman_ford step 3130 current loss 0.306610, current_train_items 100192.
I0302 19:00:02.493645 22535416901760 run.py:483] Algo bellman_ford step 3131 current loss 0.461295, current_train_items 100224.
I0302 19:00:02.517450 22535416901760 run.py:483] Algo bellman_ford step 3132 current loss 0.664422, current_train_items 100256.
I0302 19:00:02.547310 22535416901760 run.py:483] Algo bellman_ford step 3133 current loss 0.766966, current_train_items 100288.
I0302 19:00:02.582532 22535416901760 run.py:483] Algo bellman_ford step 3134 current loss 0.848520, current_train_items 100320.
I0302 19:00:02.601893 22535416901760 run.py:483] Algo bellman_ford step 3135 current loss 0.354496, current_train_items 100352.
I0302 19:00:02.617880 22535416901760 run.py:483] Algo bellman_ford step 3136 current loss 0.455469, current_train_items 100384.
I0302 19:00:02.641751 22535416901760 run.py:483] Algo bellman_ford step 3137 current loss 0.635747, current_train_items 100416.
I0302 19:00:02.671300 22535416901760 run.py:483] Algo bellman_ford step 3138 current loss 0.727110, current_train_items 100448.
I0302 19:00:02.705426 22535416901760 run.py:483] Algo bellman_ford step 3139 current loss 0.782186, current_train_items 100480.
I0302 19:00:02.725085 22535416901760 run.py:483] Algo bellman_ford step 3140 current loss 0.381828, current_train_items 100512.
I0302 19:00:02.741738 22535416901760 run.py:483] Algo bellman_ford step 3141 current loss 0.585629, current_train_items 100544.
I0302 19:00:02.763946 22535416901760 run.py:483] Algo bellman_ford step 3142 current loss 0.804673, current_train_items 100576.
I0302 19:00:02.795798 22535416901760 run.py:483] Algo bellman_ford step 3143 current loss 0.849379, current_train_items 100608.
I0302 19:00:02.828835 22535416901760 run.py:483] Algo bellman_ford step 3144 current loss 0.807983, current_train_items 100640.
I0302 19:00:02.848135 22535416901760 run.py:483] Algo bellman_ford step 3145 current loss 0.271876, current_train_items 100672.
I0302 19:00:02.864239 22535416901760 run.py:483] Algo bellman_ford step 3146 current loss 0.433502, current_train_items 100704.
I0302 19:00:02.887216 22535416901760 run.py:483] Algo bellman_ford step 3147 current loss 0.557090, current_train_items 100736.
I0302 19:00:02.915819 22535416901760 run.py:483] Algo bellman_ford step 3148 current loss 0.653758, current_train_items 100768.
I0302 19:00:02.951274 22535416901760 run.py:483] Algo bellman_ford step 3149 current loss 1.025772, current_train_items 100800.
I0302 19:00:02.970645 22535416901760 run.py:483] Algo bellman_ford step 3150 current loss 0.326074, current_train_items 100832.
I0302 19:00:02.978847 22535416901760 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 19:00:02.978954 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:00:02.995767 22535416901760 run.py:483] Algo bellman_ford step 3151 current loss 0.480811, current_train_items 100864.
I0302 19:00:03.020500 22535416901760 run.py:483] Algo bellman_ford step 3152 current loss 0.719551, current_train_items 100896.
I0302 19:00:03.051810 22535416901760 run.py:483] Algo bellman_ford step 3153 current loss 0.723802, current_train_items 100928.
I0302 19:00:03.086580 22535416901760 run.py:483] Algo bellman_ford step 3154 current loss 0.849317, current_train_items 100960.
I0302 19:00:03.106751 22535416901760 run.py:483] Algo bellman_ford step 3155 current loss 0.291163, current_train_items 100992.
I0302 19:00:03.122576 22535416901760 run.py:483] Algo bellman_ford step 3156 current loss 0.495635, current_train_items 101024.
I0302 19:00:03.146234 22535416901760 run.py:483] Algo bellman_ford step 3157 current loss 0.653790, current_train_items 101056.
I0302 19:00:03.176237 22535416901760 run.py:483] Algo bellman_ford step 3158 current loss 0.738737, current_train_items 101088.
I0302 19:00:03.209098 22535416901760 run.py:483] Algo bellman_ford step 3159 current loss 0.772348, current_train_items 101120.
I0302 19:00:03.229089 22535416901760 run.py:483] Algo bellman_ford step 3160 current loss 0.320310, current_train_items 101152.
I0302 19:00:03.245087 22535416901760 run.py:483] Algo bellman_ford step 3161 current loss 0.536691, current_train_items 101184.
I0302 19:00:03.267898 22535416901760 run.py:483] Algo bellman_ford step 3162 current loss 0.704390, current_train_items 101216.
I0302 19:00:03.297496 22535416901760 run.py:483] Algo bellman_ford step 3163 current loss 0.659069, current_train_items 101248.
I0302 19:00:03.332322 22535416901760 run.py:483] Algo bellman_ford step 3164 current loss 0.859702, current_train_items 101280.
I0302 19:00:03.351700 22535416901760 run.py:483] Algo bellman_ford step 3165 current loss 0.372895, current_train_items 101312.
I0302 19:00:03.368456 22535416901760 run.py:483] Algo bellman_ford step 3166 current loss 0.583981, current_train_items 101344.
I0302 19:00:03.392370 22535416901760 run.py:483] Algo bellman_ford step 3167 current loss 0.675052, current_train_items 101376.
I0302 19:00:03.423389 22535416901760 run.py:483] Algo bellman_ford step 3168 current loss 0.798666, current_train_items 101408.
I0302 19:00:03.455986 22535416901760 run.py:483] Algo bellman_ford step 3169 current loss 0.860825, current_train_items 101440.
I0302 19:00:03.475743 22535416901760 run.py:483] Algo bellman_ford step 3170 current loss 0.376436, current_train_items 101472.
I0302 19:00:03.492389 22535416901760 run.py:483] Algo bellman_ford step 3171 current loss 0.544344, current_train_items 101504.
I0302 19:00:03.515653 22535416901760 run.py:483] Algo bellman_ford step 3172 current loss 0.598520, current_train_items 101536.
I0302 19:00:03.546582 22535416901760 run.py:483] Algo bellman_ford step 3173 current loss 0.710623, current_train_items 101568.
I0302 19:00:03.577548 22535416901760 run.py:483] Algo bellman_ford step 3174 current loss 0.801486, current_train_items 101600.
I0302 19:00:03.597090 22535416901760 run.py:483] Algo bellman_ford step 3175 current loss 0.276770, current_train_items 101632.
I0302 19:00:03.613378 22535416901760 run.py:483] Algo bellman_ford step 3176 current loss 0.468378, current_train_items 101664.
I0302 19:00:03.635968 22535416901760 run.py:483] Algo bellman_ford step 3177 current loss 0.572542, current_train_items 101696.
I0302 19:00:03.664662 22535416901760 run.py:483] Algo bellman_ford step 3178 current loss 0.619976, current_train_items 101728.
I0302 19:00:03.698597 22535416901760 run.py:483] Algo bellman_ford step 3179 current loss 0.786402, current_train_items 101760.
I0302 19:00:03.717900 22535416901760 run.py:483] Algo bellman_ford step 3180 current loss 0.361341, current_train_items 101792.
I0302 19:00:03.734318 22535416901760 run.py:483] Algo bellman_ford step 3181 current loss 0.452430, current_train_items 101824.
I0302 19:00:03.757902 22535416901760 run.py:483] Algo bellman_ford step 3182 current loss 0.631771, current_train_items 101856.
I0302 19:00:03.788897 22535416901760 run.py:483] Algo bellman_ford step 3183 current loss 0.717122, current_train_items 101888.
I0302 19:00:03.824733 22535416901760 run.py:483] Algo bellman_ford step 3184 current loss 0.895250, current_train_items 101920.
I0302 19:00:03.844827 22535416901760 run.py:483] Algo bellman_ford step 3185 current loss 0.379165, current_train_items 101952.
I0302 19:00:03.860950 22535416901760 run.py:483] Algo bellman_ford step 3186 current loss 0.563267, current_train_items 101984.
I0302 19:00:03.884648 22535416901760 run.py:483] Algo bellman_ford step 3187 current loss 0.764720, current_train_items 102016.
I0302 19:00:03.914890 22535416901760 run.py:483] Algo bellman_ford step 3188 current loss 0.721200, current_train_items 102048.
I0302 19:00:03.948054 22535416901760 run.py:483] Algo bellman_ford step 3189 current loss 0.814982, current_train_items 102080.
I0302 19:00:03.967930 22535416901760 run.py:483] Algo bellman_ford step 3190 current loss 0.375932, current_train_items 102112.
I0302 19:00:03.984264 22535416901760 run.py:483] Algo bellman_ford step 3191 current loss 0.478717, current_train_items 102144.
I0302 19:00:04.007733 22535416901760 run.py:483] Algo bellman_ford step 3192 current loss 0.805651, current_train_items 102176.
I0302 19:00:04.038182 22535416901760 run.py:483] Algo bellman_ford step 3193 current loss 0.715110, current_train_items 102208.
I0302 19:00:04.071792 22535416901760 run.py:483] Algo bellman_ford step 3194 current loss 0.884326, current_train_items 102240.
I0302 19:00:04.091645 22535416901760 run.py:483] Algo bellman_ford step 3195 current loss 0.379747, current_train_items 102272.
I0302 19:00:04.107542 22535416901760 run.py:483] Algo bellman_ford step 3196 current loss 0.432939, current_train_items 102304.
I0302 19:00:04.130814 22535416901760 run.py:483] Algo bellman_ford step 3197 current loss 0.783295, current_train_items 102336.
I0302 19:00:04.161662 22535416901760 run.py:483] Algo bellman_ford step 3198 current loss 0.868813, current_train_items 102368.
I0302 19:00:04.195299 22535416901760 run.py:483] Algo bellman_ford step 3199 current loss 0.846128, current_train_items 102400.
I0302 19:00:04.215315 22535416901760 run.py:483] Algo bellman_ford step 3200 current loss 0.340704, current_train_items 102432.
I0302 19:00:04.223073 22535416901760 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 19:00:04.223188 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:00:04.239330 22535416901760 run.py:483] Algo bellman_ford step 3201 current loss 0.382639, current_train_items 102464.
I0302 19:00:04.262465 22535416901760 run.py:483] Algo bellman_ford step 3202 current loss 0.630854, current_train_items 102496.
I0302 19:00:04.294724 22535416901760 run.py:483] Algo bellman_ford step 3203 current loss 0.905551, current_train_items 102528.
I0302 19:00:04.328945 22535416901760 run.py:483] Algo bellman_ford step 3204 current loss 0.906178, current_train_items 102560.
I0302 19:00:04.349005 22535416901760 run.py:483] Algo bellman_ford step 3205 current loss 0.375419, current_train_items 102592.
I0302 19:00:04.364598 22535416901760 run.py:483] Algo bellman_ford step 3206 current loss 0.607320, current_train_items 102624.
I0302 19:00:04.386954 22535416901760 run.py:483] Algo bellman_ford step 3207 current loss 0.538592, current_train_items 102656.
I0302 19:00:04.418600 22535416901760 run.py:483] Algo bellman_ford step 3208 current loss 0.754586, current_train_items 102688.
I0302 19:00:04.452837 22535416901760 run.py:483] Algo bellman_ford step 3209 current loss 0.901160, current_train_items 102720.
I0302 19:00:04.472257 22535416901760 run.py:483] Algo bellman_ford step 3210 current loss 0.295714, current_train_items 102752.
I0302 19:00:04.488041 22535416901760 run.py:483] Algo bellman_ford step 3211 current loss 0.416595, current_train_items 102784.
I0302 19:00:04.510776 22535416901760 run.py:483] Algo bellman_ford step 3212 current loss 0.676159, current_train_items 102816.
I0302 19:00:04.541355 22535416901760 run.py:483] Algo bellman_ford step 3213 current loss 0.740708, current_train_items 102848.
I0302 19:00:04.574428 22535416901760 run.py:483] Algo bellman_ford step 3214 current loss 0.765319, current_train_items 102880.
I0302 19:00:04.593946 22535416901760 run.py:483] Algo bellman_ford step 3215 current loss 0.343562, current_train_items 102912.
I0302 19:00:04.609990 22535416901760 run.py:483] Algo bellman_ford step 3216 current loss 0.529842, current_train_items 102944.
I0302 19:00:04.633701 22535416901760 run.py:483] Algo bellman_ford step 3217 current loss 0.633807, current_train_items 102976.
I0302 19:00:04.663108 22535416901760 run.py:483] Algo bellman_ford step 3218 current loss 0.696856, current_train_items 103008.
I0302 19:00:04.695644 22535416901760 run.py:483] Algo bellman_ford step 3219 current loss 0.780151, current_train_items 103040.
I0302 19:00:04.715006 22535416901760 run.py:483] Algo bellman_ford step 3220 current loss 0.294441, current_train_items 103072.
I0302 19:00:04.731126 22535416901760 run.py:483] Algo bellman_ford step 3221 current loss 0.483420, current_train_items 103104.
I0302 19:00:04.754889 22535416901760 run.py:483] Algo bellman_ford step 3222 current loss 0.650061, current_train_items 103136.
I0302 19:00:04.785870 22535416901760 run.py:483] Algo bellman_ford step 3223 current loss 0.681738, current_train_items 103168.
I0302 19:00:04.817397 22535416901760 run.py:483] Algo bellman_ford step 3224 current loss 0.691847, current_train_items 103200.
I0302 19:00:04.836718 22535416901760 run.py:483] Algo bellman_ford step 3225 current loss 0.332976, current_train_items 103232.
I0302 19:00:04.852098 22535416901760 run.py:483] Algo bellman_ford step 3226 current loss 0.392848, current_train_items 103264.
I0302 19:00:04.875519 22535416901760 run.py:483] Algo bellman_ford step 3227 current loss 0.622881, current_train_items 103296.
I0302 19:00:04.906825 22535416901760 run.py:483] Algo bellman_ford step 3228 current loss 0.836537, current_train_items 103328.
I0302 19:00:04.943691 22535416901760 run.py:483] Algo bellman_ford step 3229 current loss 1.199074, current_train_items 103360.
I0302 19:00:04.963013 22535416901760 run.py:483] Algo bellman_ford step 3230 current loss 0.249704, current_train_items 103392.
I0302 19:00:04.979244 22535416901760 run.py:483] Algo bellman_ford step 3231 current loss 0.434982, current_train_items 103424.
I0302 19:00:05.001867 22535416901760 run.py:483] Algo bellman_ford step 3232 current loss 0.652091, current_train_items 103456.
I0302 19:00:05.032685 22535416901760 run.py:483] Algo bellman_ford step 3233 current loss 0.668103, current_train_items 103488.
I0302 19:00:05.067572 22535416901760 run.py:483] Algo bellman_ford step 3234 current loss 0.890676, current_train_items 103520.
I0302 19:00:05.087385 22535416901760 run.py:483] Algo bellman_ford step 3235 current loss 0.310825, current_train_items 103552.
I0302 19:00:05.103703 22535416901760 run.py:483] Algo bellman_ford step 3236 current loss 0.568516, current_train_items 103584.
I0302 19:00:05.126477 22535416901760 run.py:483] Algo bellman_ford step 3237 current loss 0.649831, current_train_items 103616.
I0302 19:00:05.158271 22535416901760 run.py:483] Algo bellman_ford step 3238 current loss 0.744620, current_train_items 103648.
I0302 19:00:05.191367 22535416901760 run.py:483] Algo bellman_ford step 3239 current loss 0.792588, current_train_items 103680.
I0302 19:00:05.210736 22535416901760 run.py:483] Algo bellman_ford step 3240 current loss 0.298452, current_train_items 103712.
I0302 19:00:05.227079 22535416901760 run.py:483] Algo bellman_ford step 3241 current loss 0.484162, current_train_items 103744.
I0302 19:00:05.250396 22535416901760 run.py:483] Algo bellman_ford step 3242 current loss 0.745502, current_train_items 103776.
I0302 19:00:05.279944 22535416901760 run.py:483] Algo bellman_ford step 3243 current loss 0.534360, current_train_items 103808.
I0302 19:00:05.314231 22535416901760 run.py:483] Algo bellman_ford step 3244 current loss 0.982733, current_train_items 103840.
I0302 19:00:05.333754 22535416901760 run.py:483] Algo bellman_ford step 3245 current loss 0.271105, current_train_items 103872.
I0302 19:00:05.350175 22535416901760 run.py:483] Algo bellman_ford step 3246 current loss 0.511993, current_train_items 103904.
I0302 19:00:05.373593 22535416901760 run.py:483] Algo bellman_ford step 3247 current loss 0.624104, current_train_items 103936.
I0302 19:00:05.404585 22535416901760 run.py:483] Algo bellman_ford step 3248 current loss 0.741677, current_train_items 103968.
I0302 19:00:05.436724 22535416901760 run.py:483] Algo bellman_ford step 3249 current loss 0.886555, current_train_items 104000.
I0302 19:00:05.456321 22535416901760 run.py:483] Algo bellman_ford step 3250 current loss 0.327830, current_train_items 104032.
I0302 19:00:05.464566 22535416901760 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.892578125, 'score': 0.892578125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 19:00:05.464673 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.893, val scores are: bellman_ford: 0.893
I0302 19:00:05.481352 22535416901760 run.py:483] Algo bellman_ford step 3251 current loss 0.630544, current_train_items 104064.
I0302 19:00:05.503443 22535416901760 run.py:483] Algo bellman_ford step 3252 current loss 0.667942, current_train_items 104096.
I0302 19:00:05.535611 22535416901760 run.py:483] Algo bellman_ford step 3253 current loss 0.842985, current_train_items 104128.
I0302 19:00:05.569812 22535416901760 run.py:483] Algo bellman_ford step 3254 current loss 0.801764, current_train_items 104160.
I0302 19:00:05.589924 22535416901760 run.py:483] Algo bellman_ford step 3255 current loss 0.429503, current_train_items 104192.
I0302 19:00:05.605602 22535416901760 run.py:483] Algo bellman_ford step 3256 current loss 0.447534, current_train_items 104224.
I0302 19:00:05.628639 22535416901760 run.py:483] Algo bellman_ford step 3257 current loss 0.646023, current_train_items 104256.
I0302 19:00:05.661048 22535416901760 run.py:483] Algo bellman_ford step 3258 current loss 0.793753, current_train_items 104288.
I0302 19:00:05.695707 22535416901760 run.py:483] Algo bellman_ford step 3259 current loss 0.914103, current_train_items 104320.
I0302 19:00:05.715448 22535416901760 run.py:483] Algo bellman_ford step 3260 current loss 0.361300, current_train_items 104352.
I0302 19:00:05.731748 22535416901760 run.py:483] Algo bellman_ford step 3261 current loss 0.483851, current_train_items 104384.
I0302 19:00:05.754275 22535416901760 run.py:483] Algo bellman_ford step 3262 current loss 0.601576, current_train_items 104416.
I0302 19:00:05.785336 22535416901760 run.py:483] Algo bellman_ford step 3263 current loss 0.760689, current_train_items 104448.
I0302 19:00:05.819504 22535416901760 run.py:483] Algo bellman_ford step 3264 current loss 0.890756, current_train_items 104480.
I0302 19:00:05.839166 22535416901760 run.py:483] Algo bellman_ford step 3265 current loss 0.364474, current_train_items 104512.
I0302 19:00:05.855591 22535416901760 run.py:483] Algo bellman_ford step 3266 current loss 0.509522, current_train_items 104544.
I0302 19:00:05.878459 22535416901760 run.py:483] Algo bellman_ford step 3267 current loss 0.593786, current_train_items 104576.
I0302 19:00:05.907847 22535416901760 run.py:483] Algo bellman_ford step 3268 current loss 0.624255, current_train_items 104608.
I0302 19:00:05.943627 22535416901760 run.py:483] Algo bellman_ford step 3269 current loss 0.940411, current_train_items 104640.
I0302 19:00:05.963627 22535416901760 run.py:483] Algo bellman_ford step 3270 current loss 0.342318, current_train_items 104672.
I0302 19:00:05.979969 22535416901760 run.py:483] Algo bellman_ford step 3271 current loss 0.480839, current_train_items 104704.
I0302 19:00:06.002950 22535416901760 run.py:483] Algo bellman_ford step 3272 current loss 0.716998, current_train_items 104736.
I0302 19:00:06.032687 22535416901760 run.py:483] Algo bellman_ford step 3273 current loss 0.714441, current_train_items 104768.
I0302 19:00:06.064522 22535416901760 run.py:483] Algo bellman_ford step 3274 current loss 0.664583, current_train_items 104800.
I0302 19:00:06.084573 22535416901760 run.py:483] Algo bellman_ford step 3275 current loss 0.356562, current_train_items 104832.
I0302 19:00:06.100651 22535416901760 run.py:483] Algo bellman_ford step 3276 current loss 0.467905, current_train_items 104864.
I0302 19:00:06.123818 22535416901760 run.py:483] Algo bellman_ford step 3277 current loss 0.638227, current_train_items 104896.
I0302 19:00:06.155758 22535416901760 run.py:483] Algo bellman_ford step 3278 current loss 0.697638, current_train_items 104928.
I0302 19:00:06.190344 22535416901760 run.py:483] Algo bellman_ford step 3279 current loss 0.768462, current_train_items 104960.
I0302 19:00:06.209907 22535416901760 run.py:483] Algo bellman_ford step 3280 current loss 0.334870, current_train_items 104992.
I0302 19:00:06.225932 22535416901760 run.py:483] Algo bellman_ford step 3281 current loss 0.549023, current_train_items 105024.
I0302 19:00:06.249069 22535416901760 run.py:483] Algo bellman_ford step 3282 current loss 0.673942, current_train_items 105056.
I0302 19:00:06.280486 22535416901760 run.py:483] Algo bellman_ford step 3283 current loss 0.846231, current_train_items 105088.
I0302 19:00:06.314507 22535416901760 run.py:483] Algo bellman_ford step 3284 current loss 0.884378, current_train_items 105120.
I0302 19:00:06.334256 22535416901760 run.py:483] Algo bellman_ford step 3285 current loss 0.320705, current_train_items 105152.
I0302 19:00:06.350733 22535416901760 run.py:483] Algo bellman_ford step 3286 current loss 0.530332, current_train_items 105184.
I0302 19:00:06.374330 22535416901760 run.py:483] Algo bellman_ford step 3287 current loss 0.647371, current_train_items 105216.
I0302 19:00:06.404195 22535416901760 run.py:483] Algo bellman_ford step 3288 current loss 0.706672, current_train_items 105248.
I0302 19:00:06.436222 22535416901760 run.py:483] Algo bellman_ford step 3289 current loss 0.873801, current_train_items 105280.
I0302 19:00:06.456488 22535416901760 run.py:483] Algo bellman_ford step 3290 current loss 0.358743, current_train_items 105312.
I0302 19:00:06.473241 22535416901760 run.py:483] Algo bellman_ford step 3291 current loss 0.456060, current_train_items 105344.
I0302 19:00:06.496388 22535416901760 run.py:483] Algo bellman_ford step 3292 current loss 0.632559, current_train_items 105376.
I0302 19:00:06.527509 22535416901760 run.py:483] Algo bellman_ford step 3293 current loss 0.720351, current_train_items 105408.
I0302 19:00:06.561410 22535416901760 run.py:483] Algo bellman_ford step 3294 current loss 0.850130, current_train_items 105440.
I0302 19:00:06.581092 22535416901760 run.py:483] Algo bellman_ford step 3295 current loss 0.344316, current_train_items 105472.
I0302 19:00:06.596915 22535416901760 run.py:483] Algo bellman_ford step 3296 current loss 0.485265, current_train_items 105504.
I0302 19:00:06.621002 22535416901760 run.py:483] Algo bellman_ford step 3297 current loss 0.701578, current_train_items 105536.
I0302 19:00:06.653249 22535416901760 run.py:483] Algo bellman_ford step 3298 current loss 0.730080, current_train_items 105568.
I0302 19:00:06.687703 22535416901760 run.py:483] Algo bellman_ford step 3299 current loss 0.882518, current_train_items 105600.
I0302 19:00:06.707824 22535416901760 run.py:483] Algo bellman_ford step 3300 current loss 0.373004, current_train_items 105632.
I0302 19:00:06.715543 22535416901760 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.8974609375, 'score': 0.8974609375, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 19:00:06.715653 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.897, val scores are: bellman_ford: 0.897
I0302 19:00:06.732592 22535416901760 run.py:483] Algo bellman_ford step 3301 current loss 0.453395, current_train_items 105664.
I0302 19:00:06.756459 22535416901760 run.py:483] Algo bellman_ford step 3302 current loss 0.600361, current_train_items 105696.
I0302 19:00:06.786831 22535416901760 run.py:483] Algo bellman_ford step 3303 current loss 0.662793, current_train_items 105728.
I0302 19:00:06.818894 22535416901760 run.py:483] Algo bellman_ford step 3304 current loss 0.802606, current_train_items 105760.
I0302 19:00:06.839178 22535416901760 run.py:483] Algo bellman_ford step 3305 current loss 0.232681, current_train_items 105792.
I0302 19:00:06.855357 22535416901760 run.py:483] Algo bellman_ford step 3306 current loss 0.529140, current_train_items 105824.
I0302 19:00:06.879281 22535416901760 run.py:483] Algo bellman_ford step 3307 current loss 0.656666, current_train_items 105856.
I0302 19:00:06.910286 22535416901760 run.py:483] Algo bellman_ford step 3308 current loss 0.704444, current_train_items 105888.
I0302 19:00:06.945430 22535416901760 run.py:483] Algo bellman_ford step 3309 current loss 0.873203, current_train_items 105920.
I0302 19:00:06.965240 22535416901760 run.py:483] Algo bellman_ford step 3310 current loss 0.322216, current_train_items 105952.
I0302 19:00:06.981806 22535416901760 run.py:483] Algo bellman_ford step 3311 current loss 0.468690, current_train_items 105984.
I0302 19:00:07.005226 22535416901760 run.py:483] Algo bellman_ford step 3312 current loss 0.665182, current_train_items 106016.
I0302 19:00:07.036512 22535416901760 run.py:483] Algo bellman_ford step 3313 current loss 0.777782, current_train_items 106048.
I0302 19:00:07.068698 22535416901760 run.py:483] Algo bellman_ford step 3314 current loss 0.883187, current_train_items 106080.
I0302 19:00:07.088243 22535416901760 run.py:483] Algo bellman_ford step 3315 current loss 0.339215, current_train_items 106112.
I0302 19:00:07.104823 22535416901760 run.py:483] Algo bellman_ford step 3316 current loss 0.514502, current_train_items 106144.
I0302 19:00:07.127837 22535416901760 run.py:483] Algo bellman_ford step 3317 current loss 0.653706, current_train_items 106176.
I0302 19:00:07.158767 22535416901760 run.py:483] Algo bellman_ford step 3318 current loss 0.680020, current_train_items 106208.
I0302 19:00:07.193048 22535416901760 run.py:483] Algo bellman_ford step 3319 current loss 0.849680, current_train_items 106240.
I0302 19:00:07.212811 22535416901760 run.py:483] Algo bellman_ford step 3320 current loss 0.349087, current_train_items 106272.
I0302 19:00:07.229089 22535416901760 run.py:483] Algo bellman_ford step 3321 current loss 0.570587, current_train_items 106304.
I0302 19:00:07.253317 22535416901760 run.py:483] Algo bellman_ford step 3322 current loss 0.705517, current_train_items 106336.
I0302 19:00:07.284404 22535416901760 run.py:483] Algo bellman_ford step 3323 current loss 0.707864, current_train_items 106368.
I0302 19:00:07.316718 22535416901760 run.py:483] Algo bellman_ford step 3324 current loss 0.792703, current_train_items 106400.
I0302 19:00:07.336189 22535416901760 run.py:483] Algo bellman_ford step 3325 current loss 0.342800, current_train_items 106432.
I0302 19:00:07.352518 22535416901760 run.py:483] Algo bellman_ford step 3326 current loss 0.524268, current_train_items 106464.
I0302 19:00:07.375558 22535416901760 run.py:483] Algo bellman_ford step 3327 current loss 0.708004, current_train_items 106496.
I0302 19:00:07.405928 22535416901760 run.py:483] Algo bellman_ford step 3328 current loss 0.692266, current_train_items 106528.
I0302 19:00:07.438694 22535416901760 run.py:483] Algo bellman_ford step 3329 current loss 0.708816, current_train_items 106560.
I0302 19:00:07.458222 22535416901760 run.py:483] Algo bellman_ford step 3330 current loss 0.403610, current_train_items 106592.
I0302 19:00:07.474198 22535416901760 run.py:483] Algo bellman_ford step 3331 current loss 0.500485, current_train_items 106624.
I0302 19:00:07.497465 22535416901760 run.py:483] Algo bellman_ford step 3332 current loss 0.691902, current_train_items 106656.
I0302 19:00:07.527640 22535416901760 run.py:483] Algo bellman_ford step 3333 current loss 0.610338, current_train_items 106688.
I0302 19:00:07.560048 22535416901760 run.py:483] Algo bellman_ford step 3334 current loss 0.724325, current_train_items 106720.
I0302 19:00:07.579173 22535416901760 run.py:483] Algo bellman_ford step 3335 current loss 0.326027, current_train_items 106752.
I0302 19:00:07.594830 22535416901760 run.py:483] Algo bellman_ford step 3336 current loss 0.543318, current_train_items 106784.
I0302 19:00:07.618623 22535416901760 run.py:483] Algo bellman_ford step 3337 current loss 0.648096, current_train_items 106816.
I0302 19:00:07.650429 22535416901760 run.py:483] Algo bellman_ford step 3338 current loss 0.715045, current_train_items 106848.
I0302 19:00:07.685452 22535416901760 run.py:483] Algo bellman_ford step 3339 current loss 0.794568, current_train_items 106880.
I0302 19:00:07.704682 22535416901760 run.py:483] Algo bellman_ford step 3340 current loss 0.313952, current_train_items 106912.
I0302 19:00:07.721262 22535416901760 run.py:483] Algo bellman_ford step 3341 current loss 0.466047, current_train_items 106944.
I0302 19:00:07.744740 22535416901760 run.py:483] Algo bellman_ford step 3342 current loss 0.669229, current_train_items 106976.
I0302 19:00:07.776516 22535416901760 run.py:483] Algo bellman_ford step 3343 current loss 0.742763, current_train_items 107008.
I0302 19:00:07.807871 22535416901760 run.py:483] Algo bellman_ford step 3344 current loss 0.688287, current_train_items 107040.
I0302 19:00:07.827411 22535416901760 run.py:483] Algo bellman_ford step 3345 current loss 0.334916, current_train_items 107072.
I0302 19:00:07.843230 22535416901760 run.py:483] Algo bellman_ford step 3346 current loss 0.473674, current_train_items 107104.
I0302 19:00:07.866556 22535416901760 run.py:483] Algo bellman_ford step 3347 current loss 0.633912, current_train_items 107136.
I0302 19:00:07.896780 22535416901760 run.py:483] Algo bellman_ford step 3348 current loss 0.768829, current_train_items 107168.
I0302 19:00:07.927887 22535416901760 run.py:483] Algo bellman_ford step 3349 current loss 0.819904, current_train_items 107200.
I0302 19:00:07.947618 22535416901760 run.py:483] Algo bellman_ford step 3350 current loss 0.332503, current_train_items 107232.
I0302 19:00:07.955637 22535416901760 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 19:00:07.955744 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 19:00:07.973025 22535416901760 run.py:483] Algo bellman_ford step 3351 current loss 0.584931, current_train_items 107264.
I0302 19:00:07.997525 22535416901760 run.py:483] Algo bellman_ford step 3352 current loss 0.824748, current_train_items 107296.
I0302 19:00:08.029377 22535416901760 run.py:483] Algo bellman_ford step 3353 current loss 0.767643, current_train_items 107328.
I0302 19:00:08.063420 22535416901760 run.py:483] Algo bellman_ford step 3354 current loss 0.967350, current_train_items 107360.
I0302 19:00:08.083224 22535416901760 run.py:483] Algo bellman_ford step 3355 current loss 0.296198, current_train_items 107392.
I0302 19:00:08.098992 22535416901760 run.py:483] Algo bellman_ford step 3356 current loss 0.475414, current_train_items 107424.
I0302 19:00:08.122360 22535416901760 run.py:483] Algo bellman_ford step 3357 current loss 0.621921, current_train_items 107456.
I0302 19:00:08.153289 22535416901760 run.py:483] Algo bellman_ford step 3358 current loss 0.834980, current_train_items 107488.
I0302 19:00:08.188125 22535416901760 run.py:483] Algo bellman_ford step 3359 current loss 0.830404, current_train_items 107520.
I0302 19:00:08.207980 22535416901760 run.py:483] Algo bellman_ford step 3360 current loss 0.312775, current_train_items 107552.
I0302 19:00:08.224072 22535416901760 run.py:483] Algo bellman_ford step 3361 current loss 0.551592, current_train_items 107584.
I0302 19:00:08.247968 22535416901760 run.py:483] Algo bellman_ford step 3362 current loss 0.710032, current_train_items 107616.
I0302 19:00:08.278336 22535416901760 run.py:483] Algo bellman_ford step 3363 current loss 0.635450, current_train_items 107648.
I0302 19:00:08.310848 22535416901760 run.py:483] Algo bellman_ford step 3364 current loss 0.823951, current_train_items 107680.
I0302 19:00:08.330667 22535416901760 run.py:483] Algo bellman_ford step 3365 current loss 0.349523, current_train_items 107712.
I0302 19:00:08.346498 22535416901760 run.py:483] Algo bellman_ford step 3366 current loss 0.427897, current_train_items 107744.
I0302 19:00:08.370548 22535416901760 run.py:483] Algo bellman_ford step 3367 current loss 0.694162, current_train_items 107776.
I0302 19:00:08.399847 22535416901760 run.py:483] Algo bellman_ford step 3368 current loss 0.678344, current_train_items 107808.
I0302 19:00:08.432565 22535416901760 run.py:483] Algo bellman_ford step 3369 current loss 0.719046, current_train_items 107840.
I0302 19:00:08.452503 22535416901760 run.py:483] Algo bellman_ford step 3370 current loss 0.340052, current_train_items 107872.
I0302 19:00:08.468786 22535416901760 run.py:483] Algo bellman_ford step 3371 current loss 0.482077, current_train_items 107904.
I0302 19:00:08.492138 22535416901760 run.py:483] Algo bellman_ford step 3372 current loss 0.665667, current_train_items 107936.
I0302 19:00:08.524575 22535416901760 run.py:483] Algo bellman_ford step 3373 current loss 0.791863, current_train_items 107968.
I0302 19:00:08.557363 22535416901760 run.py:483] Algo bellman_ford step 3374 current loss 0.807044, current_train_items 108000.
I0302 19:00:08.577348 22535416901760 run.py:483] Algo bellman_ford step 3375 current loss 0.337831, current_train_items 108032.
I0302 19:00:08.593240 22535416901760 run.py:483] Algo bellman_ford step 3376 current loss 0.516713, current_train_items 108064.
I0302 19:00:08.615717 22535416901760 run.py:483] Algo bellman_ford step 3377 current loss 0.699923, current_train_items 108096.
I0302 19:00:08.645712 22535416901760 run.py:483] Algo bellman_ford step 3378 current loss 0.677919, current_train_items 108128.
I0302 19:00:08.680196 22535416901760 run.py:483] Algo bellman_ford step 3379 current loss 0.821956, current_train_items 108160.
I0302 19:00:08.699575 22535416901760 run.py:483] Algo bellman_ford step 3380 current loss 0.309071, current_train_items 108192.
I0302 19:00:08.715987 22535416901760 run.py:483] Algo bellman_ford step 3381 current loss 0.526742, current_train_items 108224.
I0302 19:00:08.740260 22535416901760 run.py:483] Algo bellman_ford step 3382 current loss 0.596891, current_train_items 108256.
I0302 19:00:08.770147 22535416901760 run.py:483] Algo bellman_ford step 3383 current loss 0.737629, current_train_items 108288.
I0302 19:00:08.805294 22535416901760 run.py:483] Algo bellman_ford step 3384 current loss 1.022734, current_train_items 108320.
I0302 19:00:08.825306 22535416901760 run.py:483] Algo bellman_ford step 3385 current loss 0.348213, current_train_items 108352.
I0302 19:00:08.841702 22535416901760 run.py:483] Algo bellman_ford step 3386 current loss 0.568178, current_train_items 108384.
I0302 19:00:08.864568 22535416901760 run.py:483] Algo bellman_ford step 3387 current loss 0.677071, current_train_items 108416.
I0302 19:00:08.894672 22535416901760 run.py:483] Algo bellman_ford step 3388 current loss 0.877667, current_train_items 108448.
I0302 19:00:08.926479 22535416901760 run.py:483] Algo bellman_ford step 3389 current loss 0.935985, current_train_items 108480.
I0302 19:00:08.946261 22535416901760 run.py:483] Algo bellman_ford step 3390 current loss 0.335123, current_train_items 108512.
I0302 19:00:08.962507 22535416901760 run.py:483] Algo bellman_ford step 3391 current loss 0.528673, current_train_items 108544.
I0302 19:00:08.985640 22535416901760 run.py:483] Algo bellman_ford step 3392 current loss 0.639203, current_train_items 108576.
I0302 19:00:09.015560 22535416901760 run.py:483] Algo bellman_ford step 3393 current loss 0.640568, current_train_items 108608.
I0302 19:00:09.046951 22535416901760 run.py:483] Algo bellman_ford step 3394 current loss 0.775335, current_train_items 108640.
I0302 19:00:09.066451 22535416901760 run.py:483] Algo bellman_ford step 3395 current loss 0.366076, current_train_items 108672.
I0302 19:00:09.082768 22535416901760 run.py:483] Algo bellman_ford step 3396 current loss 0.519937, current_train_items 108704.
I0302 19:00:09.106055 22535416901760 run.py:483] Algo bellman_ford step 3397 current loss 0.608620, current_train_items 108736.
I0302 19:00:09.137109 22535416901760 run.py:483] Algo bellman_ford step 3398 current loss 0.783370, current_train_items 108768.
I0302 19:00:09.169395 22535416901760 run.py:483] Algo bellman_ford step 3399 current loss 0.770811, current_train_items 108800.
I0302 19:00:09.189516 22535416901760 run.py:483] Algo bellman_ford step 3400 current loss 0.340601, current_train_items 108832.
I0302 19:00:09.197533 22535416901760 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 19:00:09.197638 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:09.214689 22535416901760 run.py:483] Algo bellman_ford step 3401 current loss 0.529592, current_train_items 108864.
I0302 19:00:09.238433 22535416901760 run.py:483] Algo bellman_ford step 3402 current loss 0.574759, current_train_items 108896.
I0302 19:00:09.269658 22535416901760 run.py:483] Algo bellman_ford step 3403 current loss 0.713092, current_train_items 108928.
I0302 19:00:09.304910 22535416901760 run.py:483] Algo bellman_ford step 3404 current loss 0.956050, current_train_items 108960.
I0302 19:00:09.324708 22535416901760 run.py:483] Algo bellman_ford step 3405 current loss 0.330492, current_train_items 108992.
I0302 19:00:09.340332 22535416901760 run.py:483] Algo bellman_ford step 3406 current loss 0.572379, current_train_items 109024.
I0302 19:00:09.363929 22535416901760 run.py:483] Algo bellman_ford step 3407 current loss 0.638163, current_train_items 109056.
I0302 19:00:09.396391 22535416901760 run.py:483] Algo bellman_ford step 3408 current loss 0.702220, current_train_items 109088.
I0302 19:00:09.429136 22535416901760 run.py:483] Algo bellman_ford step 3409 current loss 0.819699, current_train_items 109120.
I0302 19:00:09.448741 22535416901760 run.py:483] Algo bellman_ford step 3410 current loss 0.319751, current_train_items 109152.
I0302 19:00:09.465126 22535416901760 run.py:483] Algo bellman_ford step 3411 current loss 0.571142, current_train_items 109184.
I0302 19:00:09.489550 22535416901760 run.py:483] Algo bellman_ford step 3412 current loss 0.607725, current_train_items 109216.
I0302 19:00:09.521625 22535416901760 run.py:483] Algo bellman_ford step 3413 current loss 0.836122, current_train_items 109248.
I0302 19:00:09.557764 22535416901760 run.py:483] Algo bellman_ford step 3414 current loss 0.894412, current_train_items 109280.
I0302 19:00:09.577340 22535416901760 run.py:483] Algo bellman_ford step 3415 current loss 0.303085, current_train_items 109312.
I0302 19:00:09.593602 22535416901760 run.py:483] Algo bellman_ford step 3416 current loss 0.529618, current_train_items 109344.
I0302 19:00:09.617462 22535416901760 run.py:483] Algo bellman_ford step 3417 current loss 0.694624, current_train_items 109376.
I0302 19:00:09.648457 22535416901760 run.py:483] Algo bellman_ford step 3418 current loss 0.832039, current_train_items 109408.
I0302 19:00:09.681147 22535416901760 run.py:483] Algo bellman_ford step 3419 current loss 0.709314, current_train_items 109440.
I0302 19:00:09.700836 22535416901760 run.py:483] Algo bellman_ford step 3420 current loss 0.393346, current_train_items 109472.
I0302 19:00:09.716863 22535416901760 run.py:483] Algo bellman_ford step 3421 current loss 0.507753, current_train_items 109504.
I0302 19:00:09.740611 22535416901760 run.py:483] Algo bellman_ford step 3422 current loss 0.666501, current_train_items 109536.
I0302 19:00:09.771941 22535416901760 run.py:483] Algo bellman_ford step 3423 current loss 0.672798, current_train_items 109568.
I0302 19:00:09.805033 22535416901760 run.py:483] Algo bellman_ford step 3424 current loss 0.877006, current_train_items 109600.
I0302 19:00:09.824411 22535416901760 run.py:483] Algo bellman_ford step 3425 current loss 0.334706, current_train_items 109632.
I0302 19:00:09.841126 22535416901760 run.py:483] Algo bellman_ford step 3426 current loss 0.582053, current_train_items 109664.
I0302 19:00:09.864232 22535416901760 run.py:483] Algo bellman_ford step 3427 current loss 0.608557, current_train_items 109696.
I0302 19:00:09.895137 22535416901760 run.py:483] Algo bellman_ford step 3428 current loss 0.743317, current_train_items 109728.
I0302 19:00:09.926113 22535416901760 run.py:483] Algo bellman_ford step 3429 current loss 0.722665, current_train_items 109760.
I0302 19:00:09.945603 22535416901760 run.py:483] Algo bellman_ford step 3430 current loss 0.319517, current_train_items 109792.
I0302 19:00:09.962090 22535416901760 run.py:483] Algo bellman_ford step 3431 current loss 0.492951, current_train_items 109824.
I0302 19:00:09.985239 22535416901760 run.py:483] Algo bellman_ford step 3432 current loss 0.617284, current_train_items 109856.
I0302 19:00:10.018049 22535416901760 run.py:483] Algo bellman_ford step 3433 current loss 0.897620, current_train_items 109888.
I0302 19:00:10.050580 22535416901760 run.py:483] Algo bellman_ford step 3434 current loss 0.757856, current_train_items 109920.
I0302 19:00:10.070035 22535416901760 run.py:483] Algo bellman_ford step 3435 current loss 0.337065, current_train_items 109952.
I0302 19:00:10.086313 22535416901760 run.py:483] Algo bellman_ford step 3436 current loss 0.466103, current_train_items 109984.
I0302 19:00:10.110056 22535416901760 run.py:483] Algo bellman_ford step 3437 current loss 0.790656, current_train_items 110016.
I0302 19:00:10.139961 22535416901760 run.py:483] Algo bellman_ford step 3438 current loss 0.781013, current_train_items 110048.
I0302 19:00:10.171516 22535416901760 run.py:483] Algo bellman_ford step 3439 current loss 0.800128, current_train_items 110080.
I0302 19:00:10.191218 22535416901760 run.py:483] Algo bellman_ford step 3440 current loss 0.346091, current_train_items 110112.
I0302 19:00:10.207803 22535416901760 run.py:483] Algo bellman_ford step 3441 current loss 0.583940, current_train_items 110144.
I0302 19:00:10.230645 22535416901760 run.py:483] Algo bellman_ford step 3442 current loss 0.747504, current_train_items 110176.
I0302 19:00:10.262247 22535416901760 run.py:483] Algo bellman_ford step 3443 current loss 0.932421, current_train_items 110208.
I0302 19:00:10.296281 22535416901760 run.py:483] Algo bellman_ford step 3444 current loss 0.810592, current_train_items 110240.
I0302 19:00:10.315862 22535416901760 run.py:483] Algo bellman_ford step 3445 current loss 0.341896, current_train_items 110272.
I0302 19:00:10.332459 22535416901760 run.py:483] Algo bellman_ford step 3446 current loss 0.579333, current_train_items 110304.
I0302 19:00:10.356454 22535416901760 run.py:483] Algo bellman_ford step 3447 current loss 0.643233, current_train_items 110336.
I0302 19:00:10.386918 22535416901760 run.py:483] Algo bellman_ford step 3448 current loss 0.712999, current_train_items 110368.
I0302 19:00:10.420592 22535416901760 run.py:483] Algo bellman_ford step 3449 current loss 0.938174, current_train_items 110400.
I0302 19:00:10.440164 22535416901760 run.py:483] Algo bellman_ford step 3450 current loss 0.347681, current_train_items 110432.
I0302 19:00:10.448456 22535416901760 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.8759765625, 'score': 0.8759765625, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 19:00:10.448562 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.876, val scores are: bellman_ford: 0.876
I0302 19:00:10.465292 22535416901760 run.py:483] Algo bellman_ford step 3451 current loss 0.550258, current_train_items 110464.
I0302 19:00:10.489894 22535416901760 run.py:483] Algo bellman_ford step 3452 current loss 0.701711, current_train_items 110496.
I0302 19:00:10.519960 22535416901760 run.py:483] Algo bellman_ford step 3453 current loss 0.777180, current_train_items 110528.
I0302 19:00:10.556073 22535416901760 run.py:483] Algo bellman_ford step 3454 current loss 0.910558, current_train_items 110560.
I0302 19:00:10.576178 22535416901760 run.py:483] Algo bellman_ford step 3455 current loss 0.283445, current_train_items 110592.
I0302 19:00:10.591728 22535416901760 run.py:483] Algo bellman_ford step 3456 current loss 0.497929, current_train_items 110624.
I0302 19:00:10.614961 22535416901760 run.py:483] Algo bellman_ford step 3457 current loss 0.715493, current_train_items 110656.
I0302 19:00:10.646681 22535416901760 run.py:483] Algo bellman_ford step 3458 current loss 0.755723, current_train_items 110688.
I0302 19:00:10.679190 22535416901760 run.py:483] Algo bellman_ford step 3459 current loss 0.812733, current_train_items 110720.
I0302 19:00:10.699284 22535416901760 run.py:483] Algo bellman_ford step 3460 current loss 0.289266, current_train_items 110752.
I0302 19:00:10.715784 22535416901760 run.py:483] Algo bellman_ford step 3461 current loss 0.623470, current_train_items 110784.
I0302 19:00:10.739227 22535416901760 run.py:483] Algo bellman_ford step 3462 current loss 0.734212, current_train_items 110816.
I0302 19:00:10.769592 22535416901760 run.py:483] Algo bellman_ford step 3463 current loss 0.784609, current_train_items 110848.
I0302 19:00:10.801993 22535416901760 run.py:483] Algo bellman_ford step 3464 current loss 0.835315, current_train_items 110880.
I0302 19:00:10.821721 22535416901760 run.py:483] Algo bellman_ford step 3465 current loss 0.323513, current_train_items 110912.
I0302 19:00:10.838289 22535416901760 run.py:483] Algo bellman_ford step 3466 current loss 0.670276, current_train_items 110944.
I0302 19:00:10.862453 22535416901760 run.py:483] Algo bellman_ford step 3467 current loss 0.986310, current_train_items 110976.
I0302 19:00:10.894612 22535416901760 run.py:483] Algo bellman_ford step 3468 current loss 1.065679, current_train_items 111008.
I0302 19:00:10.930047 22535416901760 run.py:483] Algo bellman_ford step 3469 current loss 1.135195, current_train_items 111040.
I0302 19:00:10.950180 22535416901760 run.py:483] Algo bellman_ford step 3470 current loss 0.331534, current_train_items 111072.
I0302 19:00:10.966992 22535416901760 run.py:483] Algo bellman_ford step 3471 current loss 0.526125, current_train_items 111104.
I0302 19:00:10.990398 22535416901760 run.py:483] Algo bellman_ford step 3472 current loss 0.603321, current_train_items 111136.
I0302 19:00:11.022357 22535416901760 run.py:483] Algo bellman_ford step 3473 current loss 0.877215, current_train_items 111168.
I0302 19:00:11.055635 22535416901760 run.py:483] Algo bellman_ford step 3474 current loss 0.966996, current_train_items 111200.
I0302 19:00:11.075551 22535416901760 run.py:483] Algo bellman_ford step 3475 current loss 0.296753, current_train_items 111232.
I0302 19:00:11.091679 22535416901760 run.py:483] Algo bellman_ford step 3476 current loss 0.423918, current_train_items 111264.
I0302 19:00:11.114127 22535416901760 run.py:483] Algo bellman_ford step 3477 current loss 0.620712, current_train_items 111296.
I0302 19:00:11.144131 22535416901760 run.py:483] Algo bellman_ford step 3478 current loss 0.562014, current_train_items 111328.
I0302 19:00:11.178900 22535416901760 run.py:483] Algo bellman_ford step 3479 current loss 0.912427, current_train_items 111360.
I0302 19:00:11.198359 22535416901760 run.py:483] Algo bellman_ford step 3480 current loss 0.299091, current_train_items 111392.
I0302 19:00:11.214479 22535416901760 run.py:483] Algo bellman_ford step 3481 current loss 0.501746, current_train_items 111424.
I0302 19:00:11.237493 22535416901760 run.py:483] Algo bellman_ford step 3482 current loss 0.583697, current_train_items 111456.
I0302 19:00:11.267436 22535416901760 run.py:483] Algo bellman_ford step 3483 current loss 0.641497, current_train_items 111488.
I0302 19:00:11.300462 22535416901760 run.py:483] Algo bellman_ford step 3484 current loss 0.812129, current_train_items 111520.
I0302 19:00:11.320331 22535416901760 run.py:483] Algo bellman_ford step 3485 current loss 0.274551, current_train_items 111552.
I0302 19:00:11.336998 22535416901760 run.py:483] Algo bellman_ford step 3486 current loss 0.508618, current_train_items 111584.
I0302 19:00:11.360635 22535416901760 run.py:483] Algo bellman_ford step 3487 current loss 0.672856, current_train_items 111616.
I0302 19:00:11.390877 22535416901760 run.py:483] Algo bellman_ford step 3488 current loss 0.655701, current_train_items 111648.
I0302 19:00:11.422954 22535416901760 run.py:483] Algo bellman_ford step 3489 current loss 0.748227, current_train_items 111680.
I0302 19:00:11.442775 22535416901760 run.py:483] Algo bellman_ford step 3490 current loss 0.314740, current_train_items 111712.
I0302 19:00:11.458833 22535416901760 run.py:483] Algo bellman_ford step 3491 current loss 0.433431, current_train_items 111744.
I0302 19:00:11.481920 22535416901760 run.py:483] Algo bellman_ford step 3492 current loss 0.737321, current_train_items 111776.
I0302 19:00:11.513986 22535416901760 run.py:483] Algo bellman_ford step 3493 current loss 0.810731, current_train_items 111808.
I0302 19:00:11.546914 22535416901760 run.py:483] Algo bellman_ford step 3494 current loss 0.801198, current_train_items 111840.
I0302 19:00:11.566702 22535416901760 run.py:483] Algo bellman_ford step 3495 current loss 0.308684, current_train_items 111872.
I0302 19:00:11.582912 22535416901760 run.py:483] Algo bellman_ford step 3496 current loss 0.494793, current_train_items 111904.
I0302 19:00:11.606320 22535416901760 run.py:483] Algo bellman_ford step 3497 current loss 0.694647, current_train_items 111936.
I0302 19:00:11.637446 22535416901760 run.py:483] Algo bellman_ford step 3498 current loss 0.773885, current_train_items 111968.
I0302 19:00:11.671622 22535416901760 run.py:483] Algo bellman_ford step 3499 current loss 0.979174, current_train_items 112000.
I0302 19:00:11.691565 22535416901760 run.py:483] Algo bellman_ford step 3500 current loss 0.321912, current_train_items 112032.
I0302 19:00:11.699346 22535416901760 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.8583984375, 'score': 0.8583984375, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 19:00:11.699454 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.858, val scores are: bellman_ford: 0.858
I0302 19:00:11.715728 22535416901760 run.py:483] Algo bellman_ford step 3501 current loss 0.451006, current_train_items 112064.
I0302 19:00:11.739501 22535416901760 run.py:483] Algo bellman_ford step 3502 current loss 0.633758, current_train_items 112096.
I0302 19:00:11.770927 22535416901760 run.py:483] Algo bellman_ford step 3503 current loss 0.784053, current_train_items 112128.
I0302 19:00:11.807186 22535416901760 run.py:483] Algo bellman_ford step 3504 current loss 0.776632, current_train_items 112160.
I0302 19:00:11.827795 22535416901760 run.py:483] Algo bellman_ford step 3505 current loss 0.408904, current_train_items 112192.
I0302 19:00:11.843108 22535416901760 run.py:483] Algo bellman_ford step 3506 current loss 0.506364, current_train_items 112224.
I0302 19:00:11.867022 22535416901760 run.py:483] Algo bellman_ford step 3507 current loss 0.763070, current_train_items 112256.
I0302 19:00:11.896865 22535416901760 run.py:483] Algo bellman_ford step 3508 current loss 0.823071, current_train_items 112288.
I0302 19:00:11.927891 22535416901760 run.py:483] Algo bellman_ford step 3509 current loss 0.757912, current_train_items 112320.
I0302 19:00:11.947404 22535416901760 run.py:483] Algo bellman_ford step 3510 current loss 0.406031, current_train_items 112352.
I0302 19:00:11.963535 22535416901760 run.py:483] Algo bellman_ford step 3511 current loss 0.414688, current_train_items 112384.
I0302 19:00:11.986791 22535416901760 run.py:483] Algo bellman_ford step 3512 current loss 0.640225, current_train_items 112416.
I0302 19:00:12.016848 22535416901760 run.py:483] Algo bellman_ford step 3513 current loss 0.788131, current_train_items 112448.
I0302 19:00:12.049323 22535416901760 run.py:483] Algo bellman_ford step 3514 current loss 0.827583, current_train_items 112480.
I0302 19:00:12.068722 22535416901760 run.py:483] Algo bellman_ford step 3515 current loss 0.302987, current_train_items 112512.
I0302 19:00:12.084498 22535416901760 run.py:483] Algo bellman_ford step 3516 current loss 0.471232, current_train_items 112544.
I0302 19:00:12.107923 22535416901760 run.py:483] Algo bellman_ford step 3517 current loss 0.609542, current_train_items 112576.
I0302 19:00:12.139235 22535416901760 run.py:483] Algo bellman_ford step 3518 current loss 0.676424, current_train_items 112608.
I0302 19:00:12.174763 22535416901760 run.py:483] Algo bellman_ford step 3519 current loss 0.894259, current_train_items 112640.
I0302 19:00:12.194356 22535416901760 run.py:483] Algo bellman_ford step 3520 current loss 0.325923, current_train_items 112672.
I0302 19:00:12.210408 22535416901760 run.py:483] Algo bellman_ford step 3521 current loss 0.435978, current_train_items 112704.
I0302 19:00:12.232758 22535416901760 run.py:483] Algo bellman_ford step 3522 current loss 0.539806, current_train_items 112736.
I0302 19:00:12.263462 22535416901760 run.py:483] Algo bellman_ford step 3523 current loss 0.728796, current_train_items 112768.
I0302 19:00:12.296714 22535416901760 run.py:483] Algo bellman_ford step 3524 current loss 0.896433, current_train_items 112800.
I0302 19:00:12.316027 22535416901760 run.py:483] Algo bellman_ford step 3525 current loss 0.368523, current_train_items 112832.
I0302 19:00:12.332025 22535416901760 run.py:483] Algo bellman_ford step 3526 current loss 0.475264, current_train_items 112864.
I0302 19:00:12.355544 22535416901760 run.py:483] Algo bellman_ford step 3527 current loss 0.651438, current_train_items 112896.
I0302 19:00:12.387763 22535416901760 run.py:483] Algo bellman_ford step 3528 current loss 0.678778, current_train_items 112928.
I0302 19:00:12.420172 22535416901760 run.py:483] Algo bellman_ford step 3529 current loss 0.731068, current_train_items 112960.
I0302 19:00:12.439583 22535416901760 run.py:483] Algo bellman_ford step 3530 current loss 0.296537, current_train_items 112992.
I0302 19:00:12.456100 22535416901760 run.py:483] Algo bellman_ford step 3531 current loss 0.496246, current_train_items 113024.
I0302 19:00:12.480797 22535416901760 run.py:483] Algo bellman_ford step 3532 current loss 0.664374, current_train_items 113056.
I0302 19:00:12.514693 22535416901760 run.py:483] Algo bellman_ford step 3533 current loss 0.880173, current_train_items 113088.
I0302 19:00:12.548606 22535416901760 run.py:483] Algo bellman_ford step 3534 current loss 0.858100, current_train_items 113120.
I0302 19:00:12.568021 22535416901760 run.py:483] Algo bellman_ford step 3535 current loss 0.328665, current_train_items 113152.
I0302 19:00:12.584484 22535416901760 run.py:483] Algo bellman_ford step 3536 current loss 0.503578, current_train_items 113184.
I0302 19:00:12.607677 22535416901760 run.py:483] Algo bellman_ford step 3537 current loss 0.631487, current_train_items 113216.
I0302 19:00:12.639077 22535416901760 run.py:483] Algo bellman_ford step 3538 current loss 0.747708, current_train_items 113248.
I0302 19:00:12.674883 22535416901760 run.py:483] Algo bellman_ford step 3539 current loss 0.863959, current_train_items 113280.
I0302 19:00:12.694622 22535416901760 run.py:483] Algo bellman_ford step 3540 current loss 0.348381, current_train_items 113312.
I0302 19:00:12.710704 22535416901760 run.py:483] Algo bellman_ford step 3541 current loss 0.662280, current_train_items 113344.
I0302 19:00:12.734701 22535416901760 run.py:483] Algo bellman_ford step 3542 current loss 0.641441, current_train_items 113376.
I0302 19:00:12.764852 22535416901760 run.py:483] Algo bellman_ford step 3543 current loss 0.638059, current_train_items 113408.
I0302 19:00:12.797672 22535416901760 run.py:483] Algo bellman_ford step 3544 current loss 0.868366, current_train_items 113440.
I0302 19:00:12.817311 22535416901760 run.py:483] Algo bellman_ford step 3545 current loss 0.273385, current_train_items 113472.
I0302 19:00:12.833352 22535416901760 run.py:483] Algo bellman_ford step 3546 current loss 0.603591, current_train_items 113504.
I0302 19:00:12.856970 22535416901760 run.py:483] Algo bellman_ford step 3547 current loss 0.882572, current_train_items 113536.
I0302 19:00:12.886730 22535416901760 run.py:483] Algo bellman_ford step 3548 current loss 0.767363, current_train_items 113568.
I0302 19:00:12.920261 22535416901760 run.py:483] Algo bellman_ford step 3549 current loss 0.921224, current_train_items 113600.
I0302 19:00:12.939907 22535416901760 run.py:483] Algo bellman_ford step 3550 current loss 0.355551, current_train_items 113632.
I0302 19:00:12.948091 22535416901760 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 19:00:12.948206 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:00:12.965286 22535416901760 run.py:483] Algo bellman_ford step 3551 current loss 0.502871, current_train_items 113664.
I0302 19:00:12.990593 22535416901760 run.py:483] Algo bellman_ford step 3552 current loss 0.765378, current_train_items 113696.
I0302 19:00:13.021487 22535416901760 run.py:483] Algo bellman_ford step 3553 current loss 0.772215, current_train_items 113728.
I0302 19:00:13.055517 22535416901760 run.py:483] Algo bellman_ford step 3554 current loss 0.902430, current_train_items 113760.
I0302 19:00:13.075591 22535416901760 run.py:483] Algo bellman_ford step 3555 current loss 0.339460, current_train_items 113792.
I0302 19:00:13.091634 22535416901760 run.py:483] Algo bellman_ford step 3556 current loss 0.482246, current_train_items 113824.
I0302 19:00:13.115174 22535416901760 run.py:483] Algo bellman_ford step 3557 current loss 0.628366, current_train_items 113856.
I0302 19:00:13.146452 22535416901760 run.py:483] Algo bellman_ford step 3558 current loss 0.804737, current_train_items 113888.
I0302 19:00:13.179164 22535416901760 run.py:483] Algo bellman_ford step 3559 current loss 0.772135, current_train_items 113920.
I0302 19:00:13.198916 22535416901760 run.py:483] Algo bellman_ford step 3560 current loss 0.394275, current_train_items 113952.
I0302 19:00:13.215195 22535416901760 run.py:483] Algo bellman_ford step 3561 current loss 0.691510, current_train_items 113984.
I0302 19:00:13.239629 22535416901760 run.py:483] Algo bellman_ford step 3562 current loss 0.775227, current_train_items 114016.
I0302 19:00:13.269334 22535416901760 run.py:483] Algo bellman_ford step 3563 current loss 0.730019, current_train_items 114048.
I0302 19:00:13.305698 22535416901760 run.py:483] Algo bellman_ford step 3564 current loss 0.918355, current_train_items 114080.
I0302 19:00:13.325087 22535416901760 run.py:483] Algo bellman_ford step 3565 current loss 0.258843, current_train_items 114112.
I0302 19:00:13.341329 22535416901760 run.py:483] Algo bellman_ford step 3566 current loss 0.543131, current_train_items 114144.
I0302 19:00:13.364700 22535416901760 run.py:483] Algo bellman_ford step 3567 current loss 0.517088, current_train_items 114176.
I0302 19:00:13.395180 22535416901760 run.py:483] Algo bellman_ford step 3568 current loss 0.686799, current_train_items 114208.
I0302 19:00:13.428656 22535416901760 run.py:483] Algo bellman_ford step 3569 current loss 0.858686, current_train_items 114240.
I0302 19:00:13.448484 22535416901760 run.py:483] Algo bellman_ford step 3570 current loss 0.240877, current_train_items 114272.
I0302 19:00:13.464403 22535416901760 run.py:483] Algo bellman_ford step 3571 current loss 0.522072, current_train_items 114304.
I0302 19:00:13.487054 22535416901760 run.py:483] Algo bellman_ford step 3572 current loss 0.794305, current_train_items 114336.
I0302 19:00:13.516714 22535416901760 run.py:483] Algo bellman_ford step 3573 current loss 0.844741, current_train_items 114368.
I0302 19:00:13.552675 22535416901760 run.py:483] Algo bellman_ford step 3574 current loss 1.006695, current_train_items 114400.
I0302 19:00:13.572389 22535416901760 run.py:483] Algo bellman_ford step 3575 current loss 0.301991, current_train_items 114432.
I0302 19:00:13.588997 22535416901760 run.py:483] Algo bellman_ford step 3576 current loss 0.517570, current_train_items 114464.
I0302 19:00:13.612352 22535416901760 run.py:483] Algo bellman_ford step 3577 current loss 0.686301, current_train_items 114496.
I0302 19:00:13.642620 22535416901760 run.py:483] Algo bellman_ford step 3578 current loss 0.640855, current_train_items 114528.
I0302 19:00:13.674749 22535416901760 run.py:483] Algo bellman_ford step 3579 current loss 0.700941, current_train_items 114560.
I0302 19:00:13.694172 22535416901760 run.py:483] Algo bellman_ford step 3580 current loss 0.358339, current_train_items 114592.
I0302 19:00:13.710358 22535416901760 run.py:483] Algo bellman_ford step 3581 current loss 0.472245, current_train_items 114624.
I0302 19:00:13.734917 22535416901760 run.py:483] Algo bellman_ford step 3582 current loss 0.686215, current_train_items 114656.
I0302 19:00:13.766506 22535416901760 run.py:483] Algo bellman_ford step 3583 current loss 0.665367, current_train_items 114688.
I0302 19:00:13.800374 22535416901760 run.py:483] Algo bellman_ford step 3584 current loss 0.764917, current_train_items 114720.
I0302 19:00:13.820204 22535416901760 run.py:483] Algo bellman_ford step 3585 current loss 0.283512, current_train_items 114752.
I0302 19:00:13.836705 22535416901760 run.py:483] Algo bellman_ford step 3586 current loss 0.611671, current_train_items 114784.
I0302 19:00:13.859497 22535416901760 run.py:483] Algo bellman_ford step 3587 current loss 0.733942, current_train_items 114816.
I0302 19:00:13.891275 22535416901760 run.py:483] Algo bellman_ford step 3588 current loss 0.746262, current_train_items 114848.
I0302 19:00:13.925843 22535416901760 run.py:483] Algo bellman_ford step 3589 current loss 0.775020, current_train_items 114880.
I0302 19:00:13.945713 22535416901760 run.py:483] Algo bellman_ford step 3590 current loss 0.313539, current_train_items 114912.
I0302 19:00:13.962141 22535416901760 run.py:483] Algo bellman_ford step 3591 current loss 0.454763, current_train_items 114944.
I0302 19:00:13.986425 22535416901760 run.py:483] Algo bellman_ford step 3592 current loss 0.717863, current_train_items 114976.
I0302 19:00:14.018490 22535416901760 run.py:483] Algo bellman_ford step 3593 current loss 0.749012, current_train_items 115008.
I0302 19:00:14.052330 22535416901760 run.py:483] Algo bellman_ford step 3594 current loss 0.812641, current_train_items 115040.
I0302 19:00:14.071742 22535416901760 run.py:483] Algo bellman_ford step 3595 current loss 0.280758, current_train_items 115072.
I0302 19:00:14.088304 22535416901760 run.py:483] Algo bellman_ford step 3596 current loss 0.484294, current_train_items 115104.
I0302 19:00:14.111428 22535416901760 run.py:483] Algo bellman_ford step 3597 current loss 0.546472, current_train_items 115136.
I0302 19:00:14.141069 22535416901760 run.py:483] Algo bellman_ford step 3598 current loss 0.571220, current_train_items 115168.
I0302 19:00:14.174911 22535416901760 run.py:483] Algo bellman_ford step 3599 current loss 0.718762, current_train_items 115200.
I0302 19:00:14.194725 22535416901760 run.py:483] Algo bellman_ford step 3600 current loss 0.331108, current_train_items 115232.
I0302 19:00:14.202542 22535416901760 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 19:00:14.202650 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:00:14.219864 22535416901760 run.py:483] Algo bellman_ford step 3601 current loss 0.498016, current_train_items 115264.
I0302 19:00:14.244287 22535416901760 run.py:483] Algo bellman_ford step 3602 current loss 0.620297, current_train_items 115296.
I0302 19:00:14.274936 22535416901760 run.py:483] Algo bellman_ford step 3603 current loss 0.647962, current_train_items 115328.
I0302 19:00:14.311828 22535416901760 run.py:483] Algo bellman_ford step 3604 current loss 0.833809, current_train_items 115360.
I0302 19:00:14.332019 22535416901760 run.py:483] Algo bellman_ford step 3605 current loss 0.340431, current_train_items 115392.
I0302 19:00:14.348447 22535416901760 run.py:483] Algo bellman_ford step 3606 current loss 0.537207, current_train_items 115424.
I0302 19:00:14.370847 22535416901760 run.py:483] Algo bellman_ford step 3607 current loss 0.634055, current_train_items 115456.
I0302 19:00:14.401101 22535416901760 run.py:483] Algo bellman_ford step 3608 current loss 0.767383, current_train_items 115488.
I0302 19:00:14.432793 22535416901760 run.py:483] Algo bellman_ford step 3609 current loss 0.738554, current_train_items 115520.
I0302 19:00:14.452632 22535416901760 run.py:483] Algo bellman_ford step 3610 current loss 0.325920, current_train_items 115552.
I0302 19:00:14.469040 22535416901760 run.py:483] Algo bellman_ford step 3611 current loss 0.495686, current_train_items 115584.
I0302 19:00:14.493812 22535416901760 run.py:483] Algo bellman_ford step 3612 current loss 0.651683, current_train_items 115616.
I0302 19:00:14.524126 22535416901760 run.py:483] Algo bellman_ford step 3613 current loss 0.676974, current_train_items 115648.
I0302 19:00:14.556126 22535416901760 run.py:483] Algo bellman_ford step 3614 current loss 0.709932, current_train_items 115680.
I0302 19:00:14.575714 22535416901760 run.py:483] Algo bellman_ford step 3615 current loss 0.324261, current_train_items 115712.
I0302 19:00:14.592521 22535416901760 run.py:483] Algo bellman_ford step 3616 current loss 0.550470, current_train_items 115744.
I0302 19:00:14.615421 22535416901760 run.py:483] Algo bellman_ford step 3617 current loss 0.610606, current_train_items 115776.
I0302 19:00:14.647069 22535416901760 run.py:483] Algo bellman_ford step 3618 current loss 0.752059, current_train_items 115808.
I0302 19:00:14.677591 22535416901760 run.py:483] Algo bellman_ford step 3619 current loss 0.614207, current_train_items 115840.
I0302 19:00:14.697470 22535416901760 run.py:483] Algo bellman_ford step 3620 current loss 0.298116, current_train_items 115872.
I0302 19:00:14.713795 22535416901760 run.py:483] Algo bellman_ford step 3621 current loss 0.491821, current_train_items 115904.
I0302 19:00:14.737674 22535416901760 run.py:483] Algo bellman_ford step 3622 current loss 0.758886, current_train_items 115936.
I0302 19:00:14.769278 22535416901760 run.py:483] Algo bellman_ford step 3623 current loss 0.721991, current_train_items 115968.
I0302 19:00:14.802341 22535416901760 run.py:483] Algo bellman_ford step 3624 current loss 0.703024, current_train_items 116000.
I0302 19:00:14.821701 22535416901760 run.py:483] Algo bellman_ford step 3625 current loss 0.358189, current_train_items 116032.
I0302 19:00:14.838059 22535416901760 run.py:483] Algo bellman_ford step 3626 current loss 0.582256, current_train_items 116064.
I0302 19:00:14.862237 22535416901760 run.py:483] Algo bellman_ford step 3627 current loss 0.676560, current_train_items 116096.
I0302 19:00:14.893810 22535416901760 run.py:483] Algo bellman_ford step 3628 current loss 0.752597, current_train_items 116128.
I0302 19:00:14.926523 22535416901760 run.py:483] Algo bellman_ford step 3629 current loss 0.696091, current_train_items 116160.
I0302 19:00:14.946232 22535416901760 run.py:483] Algo bellman_ford step 3630 current loss 0.311642, current_train_items 116192.
I0302 19:00:14.962381 22535416901760 run.py:483] Algo bellman_ford step 3631 current loss 0.412443, current_train_items 116224.
I0302 19:00:14.986244 22535416901760 run.py:483] Algo bellman_ford step 3632 current loss 0.609929, current_train_items 116256.
I0302 19:00:15.017021 22535416901760 run.py:483] Algo bellman_ford step 3633 current loss 0.798080, current_train_items 116288.
I0302 19:00:15.049628 22535416901760 run.py:483] Algo bellman_ford step 3634 current loss 0.863734, current_train_items 116320.
I0302 19:00:15.069023 22535416901760 run.py:483] Algo bellman_ford step 3635 current loss 0.272872, current_train_items 116352.
I0302 19:00:15.085311 22535416901760 run.py:483] Algo bellman_ford step 3636 current loss 0.494785, current_train_items 116384.
I0302 19:00:15.107950 22535416901760 run.py:483] Algo bellman_ford step 3637 current loss 0.618986, current_train_items 116416.
I0302 19:00:15.139329 22535416901760 run.py:483] Algo bellman_ford step 3638 current loss 0.690420, current_train_items 116448.
I0302 19:00:15.171793 22535416901760 run.py:483] Algo bellman_ford step 3639 current loss 0.755378, current_train_items 116480.
I0302 19:00:15.191353 22535416901760 run.py:483] Algo bellman_ford step 3640 current loss 0.299433, current_train_items 116512.
I0302 19:00:15.207411 22535416901760 run.py:483] Algo bellman_ford step 3641 current loss 0.504206, current_train_items 116544.
I0302 19:00:15.231464 22535416901760 run.py:483] Algo bellman_ford step 3642 current loss 0.723624, current_train_items 116576.
I0302 19:00:15.265003 22535416901760 run.py:483] Algo bellman_ford step 3643 current loss 0.801153, current_train_items 116608.
I0302 19:00:15.299507 22535416901760 run.py:483] Algo bellman_ford step 3644 current loss 0.976687, current_train_items 116640.
I0302 19:00:15.319004 22535416901760 run.py:483] Algo bellman_ford step 3645 current loss 0.300821, current_train_items 116672.
I0302 19:00:15.335244 22535416901760 run.py:483] Algo bellman_ford step 3646 current loss 0.475222, current_train_items 116704.
I0302 19:00:15.360414 22535416901760 run.py:483] Algo bellman_ford step 3647 current loss 0.623249, current_train_items 116736.
I0302 19:00:15.390696 22535416901760 run.py:483] Algo bellman_ford step 3648 current loss 0.605230, current_train_items 116768.
I0302 19:00:15.424150 22535416901760 run.py:483] Algo bellman_ford step 3649 current loss 0.792042, current_train_items 116800.
I0302 19:00:15.443432 22535416901760 run.py:483] Algo bellman_ford step 3650 current loss 0.375593, current_train_items 116832.
I0302 19:00:15.451678 22535416901760 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 19:00:15.451786 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:15.468744 22535416901760 run.py:483] Algo bellman_ford step 3651 current loss 0.476888, current_train_items 116864.
I0302 19:00:15.491561 22535416901760 run.py:483] Algo bellman_ford step 3652 current loss 0.596864, current_train_items 116896.
I0302 19:00:15.521963 22535416901760 run.py:483] Algo bellman_ford step 3653 current loss 0.582986, current_train_items 116928.
I0302 19:00:15.555098 22535416901760 run.py:483] Algo bellman_ford step 3654 current loss 0.769398, current_train_items 116960.
I0302 19:00:15.574982 22535416901760 run.py:483] Algo bellman_ford step 3655 current loss 0.312947, current_train_items 116992.
I0302 19:00:15.590521 22535416901760 run.py:483] Algo bellman_ford step 3656 current loss 0.535799, current_train_items 117024.
I0302 19:00:15.614665 22535416901760 run.py:483] Algo bellman_ford step 3657 current loss 0.593931, current_train_items 117056.
I0302 19:00:15.644941 22535416901760 run.py:483] Algo bellman_ford step 3658 current loss 0.557708, current_train_items 117088.
I0302 19:00:15.678734 22535416901760 run.py:483] Algo bellman_ford step 3659 current loss 0.744155, current_train_items 117120.
I0302 19:00:15.698741 22535416901760 run.py:483] Algo bellman_ford step 3660 current loss 0.270626, current_train_items 117152.
I0302 19:00:15.715305 22535416901760 run.py:483] Algo bellman_ford step 3661 current loss 0.458085, current_train_items 117184.
I0302 19:00:15.737927 22535416901760 run.py:483] Algo bellman_ford step 3662 current loss 0.573602, current_train_items 117216.
I0302 19:00:15.769432 22535416901760 run.py:483] Algo bellman_ford step 3663 current loss 0.734732, current_train_items 117248.
I0302 19:00:15.803820 22535416901760 run.py:483] Algo bellman_ford step 3664 current loss 0.754803, current_train_items 117280.
I0302 19:00:15.823324 22535416901760 run.py:483] Algo bellman_ford step 3665 current loss 0.325092, current_train_items 117312.
I0302 19:00:15.839283 22535416901760 run.py:483] Algo bellman_ford step 3666 current loss 0.536203, current_train_items 117344.
I0302 19:00:15.862970 22535416901760 run.py:483] Algo bellman_ford step 3667 current loss 0.597837, current_train_items 117376.
I0302 19:00:15.894385 22535416901760 run.py:483] Algo bellman_ford step 3668 current loss 0.672772, current_train_items 117408.
I0302 19:00:15.929181 22535416901760 run.py:483] Algo bellman_ford step 3669 current loss 0.858358, current_train_items 117440.
I0302 19:00:15.949066 22535416901760 run.py:483] Algo bellman_ford step 3670 current loss 0.333025, current_train_items 117472.
I0302 19:00:15.965674 22535416901760 run.py:483] Algo bellman_ford step 3671 current loss 0.620484, current_train_items 117504.
I0302 19:00:15.988409 22535416901760 run.py:483] Algo bellman_ford step 3672 current loss 0.546582, current_train_items 117536.
I0302 19:00:16.019033 22535416901760 run.py:483] Algo bellman_ford step 3673 current loss 0.707781, current_train_items 117568.
I0302 19:00:16.053147 22535416901760 run.py:483] Algo bellman_ford step 3674 current loss 0.883463, current_train_items 117600.
I0302 19:00:16.072653 22535416901760 run.py:483] Algo bellman_ford step 3675 current loss 0.254041, current_train_items 117632.
I0302 19:00:16.088461 22535416901760 run.py:483] Algo bellman_ford step 3676 current loss 0.445169, current_train_items 117664.
I0302 19:00:16.111160 22535416901760 run.py:483] Algo bellman_ford step 3677 current loss 0.664145, current_train_items 117696.
I0302 19:00:16.141892 22535416901760 run.py:483] Algo bellman_ford step 3678 current loss 0.721778, current_train_items 117728.
I0302 19:00:16.175279 22535416901760 run.py:483] Algo bellman_ford step 3679 current loss 0.788339, current_train_items 117760.
I0302 19:00:16.194502 22535416901760 run.py:483] Algo bellman_ford step 3680 current loss 0.295599, current_train_items 117792.
I0302 19:00:16.210860 22535416901760 run.py:483] Algo bellman_ford step 3681 current loss 0.558250, current_train_items 117824.
I0302 19:00:16.234439 22535416901760 run.py:483] Algo bellman_ford step 3682 current loss 0.661534, current_train_items 117856.
I0302 19:00:16.264517 22535416901760 run.py:483] Algo bellman_ford step 3683 current loss 0.649995, current_train_items 117888.
I0302 19:00:16.297867 22535416901760 run.py:483] Algo bellman_ford step 3684 current loss 0.728495, current_train_items 117920.
I0302 19:00:16.317555 22535416901760 run.py:483] Algo bellman_ford step 3685 current loss 0.251329, current_train_items 117952.
I0302 19:00:16.333710 22535416901760 run.py:483] Algo bellman_ford step 3686 current loss 0.547140, current_train_items 117984.
I0302 19:00:16.356538 22535416901760 run.py:483] Algo bellman_ford step 3687 current loss 0.532180, current_train_items 118016.
I0302 19:00:16.388709 22535416901760 run.py:483] Algo bellman_ford step 3688 current loss 0.742494, current_train_items 118048.
I0302 19:00:16.423699 22535416901760 run.py:483] Algo bellman_ford step 3689 current loss 0.862162, current_train_items 118080.
I0302 19:00:16.443707 22535416901760 run.py:483] Algo bellman_ford step 3690 current loss 0.310801, current_train_items 118112.
I0302 19:00:16.459933 22535416901760 run.py:483] Algo bellman_ford step 3691 current loss 0.444947, current_train_items 118144.
I0302 19:00:16.481522 22535416901760 run.py:483] Algo bellman_ford step 3692 current loss 0.662644, current_train_items 118176.
I0302 19:00:16.513581 22535416901760 run.py:483] Algo bellman_ford step 3693 current loss 0.723399, current_train_items 118208.
I0302 19:00:16.546040 22535416901760 run.py:483] Algo bellman_ford step 3694 current loss 0.750156, current_train_items 118240.
I0302 19:00:16.565675 22535416901760 run.py:483] Algo bellman_ford step 3695 current loss 0.334866, current_train_items 118272.
I0302 19:00:16.582521 22535416901760 run.py:483] Algo bellman_ford step 3696 current loss 0.538371, current_train_items 118304.
I0302 19:00:16.605954 22535416901760 run.py:483] Algo bellman_ford step 3697 current loss 0.629668, current_train_items 118336.
I0302 19:00:16.637238 22535416901760 run.py:483] Algo bellman_ford step 3698 current loss 0.680936, current_train_items 118368.
I0302 19:00:16.668908 22535416901760 run.py:483] Algo bellman_ford step 3699 current loss 0.773918, current_train_items 118400.
I0302 19:00:16.688674 22535416901760 run.py:483] Algo bellman_ford step 3700 current loss 0.376153, current_train_items 118432.
I0302 19:00:16.696401 22535416901760 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.8818359375, 'score': 0.8818359375, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 19:00:16.696508 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.882, val scores are: bellman_ford: 0.882
I0302 19:00:16.713233 22535416901760 run.py:483] Algo bellman_ford step 3701 current loss 0.422193, current_train_items 118464.
I0302 19:00:16.737324 22535416901760 run.py:483] Algo bellman_ford step 3702 current loss 0.695885, current_train_items 118496.
I0302 19:00:16.770443 22535416901760 run.py:483] Algo bellman_ford step 3703 current loss 0.872999, current_train_items 118528.
I0302 19:00:16.803821 22535416901760 run.py:483] Algo bellman_ford step 3704 current loss 0.702985, current_train_items 118560.
I0302 19:00:16.823544 22535416901760 run.py:483] Algo bellman_ford step 3705 current loss 0.287924, current_train_items 118592.
I0302 19:00:16.839707 22535416901760 run.py:483] Algo bellman_ford step 3706 current loss 0.693704, current_train_items 118624.
I0302 19:00:16.863436 22535416901760 run.py:483] Algo bellman_ford step 3707 current loss 0.881103, current_train_items 118656.
I0302 19:00:16.893059 22535416901760 run.py:483] Algo bellman_ford step 3708 current loss 0.908056, current_train_items 118688.
I0302 19:00:16.926847 22535416901760 run.py:483] Algo bellman_ford step 3709 current loss 0.907021, current_train_items 118720.
I0302 19:00:16.946411 22535416901760 run.py:483] Algo bellman_ford step 3710 current loss 0.333019, current_train_items 118752.
I0302 19:00:16.962719 22535416901760 run.py:483] Algo bellman_ford step 3711 current loss 0.482687, current_train_items 118784.
I0302 19:00:16.986057 22535416901760 run.py:483] Algo bellman_ford step 3712 current loss 0.689644, current_train_items 118816.
I0302 19:00:17.017916 22535416901760 run.py:483] Algo bellman_ford step 3713 current loss 0.822033, current_train_items 118848.
I0302 19:00:17.051920 22535416901760 run.py:483] Algo bellman_ford step 3714 current loss 1.082629, current_train_items 118880.
I0302 19:00:17.071243 22535416901760 run.py:483] Algo bellman_ford step 3715 current loss 0.246486, current_train_items 118912.
I0302 19:00:17.087317 22535416901760 run.py:483] Algo bellman_ford step 3716 current loss 0.460991, current_train_items 118944.
I0302 19:00:17.110604 22535416901760 run.py:483] Algo bellman_ford step 3717 current loss 0.609005, current_train_items 118976.
I0302 19:00:17.141650 22535416901760 run.py:483] Algo bellman_ford step 3718 current loss 0.708882, current_train_items 119008.
I0302 19:00:17.173862 22535416901760 run.py:483] Algo bellman_ford step 3719 current loss 0.855784, current_train_items 119040.
I0302 19:00:17.193542 22535416901760 run.py:483] Algo bellman_ford step 3720 current loss 0.329468, current_train_items 119072.
I0302 19:00:17.209972 22535416901760 run.py:483] Algo bellman_ford step 3721 current loss 0.535990, current_train_items 119104.
I0302 19:00:17.233325 22535416901760 run.py:483] Algo bellman_ford step 3722 current loss 0.544702, current_train_items 119136.
I0302 19:00:17.263493 22535416901760 run.py:483] Algo bellman_ford step 3723 current loss 0.683004, current_train_items 119168.
I0302 19:00:17.296142 22535416901760 run.py:483] Algo bellman_ford step 3724 current loss 0.806691, current_train_items 119200.
I0302 19:00:17.315500 22535416901760 run.py:483] Algo bellman_ford step 3725 current loss 0.376797, current_train_items 119232.
I0302 19:00:17.332148 22535416901760 run.py:483] Algo bellman_ford step 3726 current loss 0.396544, current_train_items 119264.
I0302 19:00:17.356108 22535416901760 run.py:483] Algo bellman_ford step 3727 current loss 0.657365, current_train_items 119296.
I0302 19:00:17.387048 22535416901760 run.py:483] Algo bellman_ford step 3728 current loss 0.854867, current_train_items 119328.
I0302 19:00:17.420274 22535416901760 run.py:483] Algo bellman_ford step 3729 current loss 0.783181, current_train_items 119360.
I0302 19:00:17.439739 22535416901760 run.py:483] Algo bellman_ford step 3730 current loss 0.296015, current_train_items 119392.
I0302 19:00:17.455770 22535416901760 run.py:483] Algo bellman_ford step 3731 current loss 0.455065, current_train_items 119424.
I0302 19:00:17.479290 22535416901760 run.py:483] Algo bellman_ford step 3732 current loss 0.588410, current_train_items 119456.
I0302 19:00:17.508301 22535416901760 run.py:483] Algo bellman_ford step 3733 current loss 0.695291, current_train_items 119488.
I0302 19:00:17.542145 22535416901760 run.py:483] Algo bellman_ford step 3734 current loss 0.915093, current_train_items 119520.
I0302 19:00:17.561586 22535416901760 run.py:483] Algo bellman_ford step 3735 current loss 0.306568, current_train_items 119552.
I0302 19:00:17.577566 22535416901760 run.py:483] Algo bellman_ford step 3736 current loss 0.478068, current_train_items 119584.
I0302 19:00:17.600228 22535416901760 run.py:483] Algo bellman_ford step 3737 current loss 0.556241, current_train_items 119616.
I0302 19:00:17.631148 22535416901760 run.py:483] Algo bellman_ford step 3738 current loss 0.786983, current_train_items 119648.
I0302 19:00:17.664166 22535416901760 run.py:483] Algo bellman_ford step 3739 current loss 0.779164, current_train_items 119680.
I0302 19:00:17.683932 22535416901760 run.py:483] Algo bellman_ford step 3740 current loss 0.325038, current_train_items 119712.
I0302 19:00:17.699940 22535416901760 run.py:483] Algo bellman_ford step 3741 current loss 0.453264, current_train_items 119744.
I0302 19:00:17.722794 22535416901760 run.py:483] Algo bellman_ford step 3742 current loss 0.591705, current_train_items 119776.
I0302 19:00:17.753381 22535416901760 run.py:483] Algo bellman_ford step 3743 current loss 0.792116, current_train_items 119808.
I0302 19:00:17.784245 22535416901760 run.py:483] Algo bellman_ford step 3744 current loss 0.809737, current_train_items 119840.
I0302 19:00:17.803574 22535416901760 run.py:483] Algo bellman_ford step 3745 current loss 0.254737, current_train_items 119872.
I0302 19:00:17.819550 22535416901760 run.py:483] Algo bellman_ford step 3746 current loss 0.480680, current_train_items 119904.
I0302 19:00:17.843341 22535416901760 run.py:483] Algo bellman_ford step 3747 current loss 0.608725, current_train_items 119936.
I0302 19:00:17.874706 22535416901760 run.py:483] Algo bellman_ford step 3748 current loss 0.727751, current_train_items 119968.
I0302 19:00:17.909277 22535416901760 run.py:483] Algo bellman_ford step 3749 current loss 0.887252, current_train_items 120000.
I0302 19:00:17.929173 22535416901760 run.py:483] Algo bellman_ford step 3750 current loss 0.309826, current_train_items 120032.
I0302 19:00:17.937329 22535416901760 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 19:00:17.937459 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:00:17.954533 22535416901760 run.py:483] Algo bellman_ford step 3751 current loss 0.603158, current_train_items 120064.
I0302 19:00:17.978835 22535416901760 run.py:483] Algo bellman_ford step 3752 current loss 0.672390, current_train_items 120096.
I0302 19:00:18.009774 22535416901760 run.py:483] Algo bellman_ford step 3753 current loss 0.778655, current_train_items 120128.
I0302 19:00:18.045452 22535416901760 run.py:483] Algo bellman_ford step 3754 current loss 0.996408, current_train_items 120160.
I0302 19:00:18.065127 22535416901760 run.py:483] Algo bellman_ford step 3755 current loss 0.292432, current_train_items 120192.
I0302 19:00:18.081111 22535416901760 run.py:483] Algo bellman_ford step 3756 current loss 0.633834, current_train_items 120224.
I0302 19:00:18.105284 22535416901760 run.py:483] Algo bellman_ford step 3757 current loss 0.727368, current_train_items 120256.
I0302 19:00:18.136421 22535416901760 run.py:483] Algo bellman_ford step 3758 current loss 0.684976, current_train_items 120288.
I0302 19:00:18.168312 22535416901760 run.py:483] Algo bellman_ford step 3759 current loss 0.688372, current_train_items 120320.
I0302 19:00:18.188479 22535416901760 run.py:483] Algo bellman_ford step 3760 current loss 0.303503, current_train_items 120352.
I0302 19:00:18.204747 22535416901760 run.py:483] Algo bellman_ford step 3761 current loss 0.543667, current_train_items 120384.
I0302 19:00:18.227604 22535416901760 run.py:483] Algo bellman_ford step 3762 current loss 0.563604, current_train_items 120416.
I0302 19:00:18.258053 22535416901760 run.py:483] Algo bellman_ford step 3763 current loss 0.696136, current_train_items 120448.
I0302 19:00:18.291874 22535416901760 run.py:483] Algo bellman_ford step 3764 current loss 0.930632, current_train_items 120480.
I0302 19:00:18.311536 22535416901760 run.py:483] Algo bellman_ford step 3765 current loss 0.376958, current_train_items 120512.
I0302 19:00:18.328167 22535416901760 run.py:483] Algo bellman_ford step 3766 current loss 0.390581, current_train_items 120544.
I0302 19:00:18.351961 22535416901760 run.py:483] Algo bellman_ford step 3767 current loss 0.700437, current_train_items 120576.
I0302 19:00:18.383833 22535416901760 run.py:483] Algo bellman_ford step 3768 current loss 0.740952, current_train_items 120608.
I0302 19:00:18.416506 22535416901760 run.py:483] Algo bellman_ford step 3769 current loss 0.718044, current_train_items 120640.
I0302 19:00:18.436501 22535416901760 run.py:483] Algo bellman_ford step 3770 current loss 0.297729, current_train_items 120672.
I0302 19:00:18.452830 22535416901760 run.py:483] Algo bellman_ford step 3771 current loss 0.522182, current_train_items 120704.
I0302 19:00:18.475431 22535416901760 run.py:483] Algo bellman_ford step 3772 current loss 0.605194, current_train_items 120736.
I0302 19:00:18.505501 22535416901760 run.py:483] Algo bellman_ford step 3773 current loss 0.640882, current_train_items 120768.
I0302 19:00:18.535650 22535416901760 run.py:483] Algo bellman_ford step 3774 current loss 0.715033, current_train_items 120800.
I0302 19:00:18.555499 22535416901760 run.py:483] Algo bellman_ford step 3775 current loss 0.328197, current_train_items 120832.
I0302 19:00:18.572128 22535416901760 run.py:483] Algo bellman_ford step 3776 current loss 0.541028, current_train_items 120864.
I0302 19:00:18.594589 22535416901760 run.py:483] Algo bellman_ford step 3777 current loss 0.599582, current_train_items 120896.
I0302 19:00:18.626038 22535416901760 run.py:483] Algo bellman_ford step 3778 current loss 0.692556, current_train_items 120928.
I0302 19:00:18.659368 22535416901760 run.py:483] Algo bellman_ford step 3779 current loss 0.740471, current_train_items 120960.
I0302 19:00:18.679132 22535416901760 run.py:483] Algo bellman_ford step 3780 current loss 0.279959, current_train_items 120992.
I0302 19:00:18.695380 22535416901760 run.py:483] Algo bellman_ford step 3781 current loss 0.609295, current_train_items 121024.
I0302 19:00:18.718481 22535416901760 run.py:483] Algo bellman_ford step 3782 current loss 0.605676, current_train_items 121056.
I0302 19:00:18.747936 22535416901760 run.py:483] Algo bellman_ford step 3783 current loss 0.659552, current_train_items 121088.
I0302 19:00:18.784254 22535416901760 run.py:483] Algo bellman_ford step 3784 current loss 0.850219, current_train_items 121120.
I0302 19:00:18.803907 22535416901760 run.py:483] Algo bellman_ford step 3785 current loss 0.299747, current_train_items 121152.
I0302 19:00:18.820005 22535416901760 run.py:483] Algo bellman_ford step 3786 current loss 0.377906, current_train_items 121184.
I0302 19:00:18.843515 22535416901760 run.py:483] Algo bellman_ford step 3787 current loss 0.699149, current_train_items 121216.
I0302 19:00:18.874247 22535416901760 run.py:483] Algo bellman_ford step 3788 current loss 0.874532, current_train_items 121248.
I0302 19:00:18.907586 22535416901760 run.py:483] Algo bellman_ford step 3789 current loss 0.872529, current_train_items 121280.
I0302 19:00:18.927412 22535416901760 run.py:483] Algo bellman_ford step 3790 current loss 0.283766, current_train_items 121312.
I0302 19:00:18.943756 22535416901760 run.py:483] Algo bellman_ford step 3791 current loss 0.513311, current_train_items 121344.
I0302 19:00:18.966484 22535416901760 run.py:483] Algo bellman_ford step 3792 current loss 0.599505, current_train_items 121376.
I0302 19:00:18.997979 22535416901760 run.py:483] Algo bellman_ford step 3793 current loss 0.839151, current_train_items 121408.
I0302 19:00:19.030881 22535416901760 run.py:483] Algo bellman_ford step 3794 current loss 0.882327, current_train_items 121440.
I0302 19:00:19.050426 22535416901760 run.py:483] Algo bellman_ford step 3795 current loss 0.431955, current_train_items 121472.
I0302 19:00:19.066897 22535416901760 run.py:483] Algo bellman_ford step 3796 current loss 0.601963, current_train_items 121504.
I0302 19:00:19.091056 22535416901760 run.py:483] Algo bellman_ford step 3797 current loss 0.621876, current_train_items 121536.
I0302 19:00:19.122498 22535416901760 run.py:483] Algo bellman_ford step 3798 current loss 0.826390, current_train_items 121568.
I0302 19:00:19.154834 22535416901760 run.py:483] Algo bellman_ford step 3799 current loss 0.718955, current_train_items 121600.
I0302 19:00:19.174621 22535416901760 run.py:483] Algo bellman_ford step 3800 current loss 0.338524, current_train_items 121632.
I0302 19:00:19.182435 22535416901760 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.8857421875, 'score': 0.8857421875, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 19:00:19.182539 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.886, val scores are: bellman_ford: 0.886
I0302 19:00:19.199185 22535416901760 run.py:483] Algo bellman_ford step 3801 current loss 0.485033, current_train_items 121664.
I0302 19:00:19.223563 22535416901760 run.py:483] Algo bellman_ford step 3802 current loss 0.605434, current_train_items 121696.
I0302 19:00:19.254352 22535416901760 run.py:483] Algo bellman_ford step 3803 current loss 0.765407, current_train_items 121728.
I0302 19:00:19.286061 22535416901760 run.py:483] Algo bellman_ford step 3804 current loss 0.733832, current_train_items 121760.
I0302 19:00:19.305958 22535416901760 run.py:483] Algo bellman_ford step 3805 current loss 0.303022, current_train_items 121792.
I0302 19:00:19.322005 22535416901760 run.py:483] Algo bellman_ford step 3806 current loss 0.572975, current_train_items 121824.
I0302 19:00:19.345288 22535416901760 run.py:483] Algo bellman_ford step 3807 current loss 0.597126, current_train_items 121856.
I0302 19:00:19.376070 22535416901760 run.py:483] Algo bellman_ford step 3808 current loss 0.722903, current_train_items 121888.
I0302 19:00:19.410699 22535416901760 run.py:483] Algo bellman_ford step 3809 current loss 0.842080, current_train_items 121920.
I0302 19:00:19.430026 22535416901760 run.py:483] Algo bellman_ford step 3810 current loss 0.424491, current_train_items 121952.
I0302 19:00:19.446469 22535416901760 run.py:483] Algo bellman_ford step 3811 current loss 0.458301, current_train_items 121984.
I0302 19:00:19.469758 22535416901760 run.py:483] Algo bellman_ford step 3812 current loss 0.536037, current_train_items 122016.
I0302 19:00:19.502344 22535416901760 run.py:483] Algo bellman_ford step 3813 current loss 0.749743, current_train_items 122048.
I0302 19:00:19.537456 22535416901760 run.py:483] Algo bellman_ford step 3814 current loss 0.978483, current_train_items 122080.
I0302 19:00:19.557094 22535416901760 run.py:483] Algo bellman_ford step 3815 current loss 0.329796, current_train_items 122112.
I0302 19:00:19.573651 22535416901760 run.py:483] Algo bellman_ford step 3816 current loss 0.473046, current_train_items 122144.
I0302 19:00:19.597008 22535416901760 run.py:483] Algo bellman_ford step 3817 current loss 0.616506, current_train_items 122176.
I0302 19:00:19.627592 22535416901760 run.py:483] Algo bellman_ford step 3818 current loss 0.598592, current_train_items 122208.
I0302 19:00:19.661018 22535416901760 run.py:483] Algo bellman_ford step 3819 current loss 0.779613, current_train_items 122240.
I0302 19:00:19.680285 22535416901760 run.py:483] Algo bellman_ford step 3820 current loss 0.320061, current_train_items 122272.
I0302 19:00:19.696024 22535416901760 run.py:483] Algo bellman_ford step 3821 current loss 0.427278, current_train_items 122304.
I0302 19:00:19.719125 22535416901760 run.py:483] Algo bellman_ford step 3822 current loss 0.649803, current_train_items 122336.
I0302 19:00:19.750007 22535416901760 run.py:483] Algo bellman_ford step 3823 current loss 0.716612, current_train_items 122368.
I0302 19:00:19.783824 22535416901760 run.py:483] Algo bellman_ford step 3824 current loss 0.851225, current_train_items 122400.
I0302 19:00:19.803631 22535416901760 run.py:483] Algo bellman_ford step 3825 current loss 0.299389, current_train_items 122432.
I0302 19:00:19.819475 22535416901760 run.py:483] Algo bellman_ford step 3826 current loss 0.454325, current_train_items 122464.
I0302 19:00:19.843305 22535416901760 run.py:483] Algo bellman_ford step 3827 current loss 0.600423, current_train_items 122496.
I0302 19:00:19.874310 22535416901760 run.py:483] Algo bellman_ford step 3828 current loss 0.695551, current_train_items 122528.
I0302 19:00:19.906193 22535416901760 run.py:483] Algo bellman_ford step 3829 current loss 0.809698, current_train_items 122560.
I0302 19:00:19.925506 22535416901760 run.py:483] Algo bellman_ford step 3830 current loss 0.280567, current_train_items 122592.
I0302 19:00:19.941715 22535416901760 run.py:483] Algo bellman_ford step 3831 current loss 0.563889, current_train_items 122624.
I0302 19:00:19.964167 22535416901760 run.py:483] Algo bellman_ford step 3832 current loss 0.552295, current_train_items 122656.
I0302 19:00:19.995704 22535416901760 run.py:483] Algo bellman_ford step 3833 current loss 0.818212, current_train_items 122688.
I0302 19:00:20.028115 22535416901760 run.py:483] Algo bellman_ford step 3834 current loss 0.739621, current_train_items 122720.
I0302 19:00:20.047455 22535416901760 run.py:483] Algo bellman_ford step 3835 current loss 0.375099, current_train_items 122752.
I0302 19:00:20.063050 22535416901760 run.py:483] Algo bellman_ford step 3836 current loss 0.407116, current_train_items 122784.
I0302 19:00:20.087552 22535416901760 run.py:483] Algo bellman_ford step 3837 current loss 0.727599, current_train_items 122816.
I0302 19:00:20.118409 22535416901760 run.py:483] Algo bellman_ford step 3838 current loss 0.751388, current_train_items 122848.
I0302 19:00:20.151563 22535416901760 run.py:483] Algo bellman_ford step 3839 current loss 0.714197, current_train_items 122880.
I0302 19:00:20.170874 22535416901760 run.py:483] Algo bellman_ford step 3840 current loss 0.324596, current_train_items 122912.
I0302 19:00:20.186831 22535416901760 run.py:483] Algo bellman_ford step 3841 current loss 0.556639, current_train_items 122944.
I0302 19:00:20.209297 22535416901760 run.py:483] Algo bellman_ford step 3842 current loss 0.566090, current_train_items 122976.
I0302 19:00:20.240132 22535416901760 run.py:483] Algo bellman_ford step 3843 current loss 0.749381, current_train_items 123008.
I0302 19:00:20.274384 22535416901760 run.py:483] Algo bellman_ford step 3844 current loss 0.892593, current_train_items 123040.
I0302 19:00:20.293855 22535416901760 run.py:483] Algo bellman_ford step 3845 current loss 0.353166, current_train_items 123072.
I0302 19:00:20.309925 22535416901760 run.py:483] Algo bellman_ford step 3846 current loss 0.490558, current_train_items 123104.
I0302 19:00:20.334613 22535416901760 run.py:483] Algo bellman_ford step 3847 current loss 0.700136, current_train_items 123136.
I0302 19:00:20.364642 22535416901760 run.py:483] Algo bellman_ford step 3848 current loss 0.628896, current_train_items 123168.
I0302 19:00:20.398163 22535416901760 run.py:483] Algo bellman_ford step 3849 current loss 0.744699, current_train_items 123200.
I0302 19:00:20.417766 22535416901760 run.py:483] Algo bellman_ford step 3850 current loss 0.336614, current_train_items 123232.
I0302 19:00:20.425807 22535416901760 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9150390625, 'score': 0.9150390625, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 19:00:20.425913 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.915, val scores are: bellman_ford: 0.915
I0302 19:00:20.442230 22535416901760 run.py:483] Algo bellman_ford step 3851 current loss 0.387844, current_train_items 123264.
I0302 19:00:20.465273 22535416901760 run.py:483] Algo bellman_ford step 3852 current loss 0.664609, current_train_items 123296.
I0302 19:00:20.497386 22535416901760 run.py:483] Algo bellman_ford step 3853 current loss 0.748740, current_train_items 123328.
I0302 19:00:20.532583 22535416901760 run.py:483] Algo bellman_ford step 3854 current loss 0.822693, current_train_items 123360.
I0302 19:00:20.552485 22535416901760 run.py:483] Algo bellman_ford step 3855 current loss 0.242813, current_train_items 123392.
I0302 19:00:20.568207 22535416901760 run.py:483] Algo bellman_ford step 3856 current loss 0.510870, current_train_items 123424.
I0302 19:00:20.590468 22535416901760 run.py:483] Algo bellman_ford step 3857 current loss 0.686105, current_train_items 123456.
I0302 19:00:20.621800 22535416901760 run.py:483] Algo bellman_ford step 3858 current loss 0.940589, current_train_items 123488.
I0302 19:00:20.657256 22535416901760 run.py:483] Algo bellman_ford step 3859 current loss 1.117411, current_train_items 123520.
I0302 19:00:20.676804 22535416901760 run.py:483] Algo bellman_ford step 3860 current loss 0.350699, current_train_items 123552.
I0302 19:00:20.693267 22535416901760 run.py:483] Algo bellman_ford step 3861 current loss 0.465858, current_train_items 123584.
I0302 19:00:20.716107 22535416901760 run.py:483] Algo bellman_ford step 3862 current loss 0.670678, current_train_items 123616.
I0302 19:00:20.746721 22535416901760 run.py:483] Algo bellman_ford step 3863 current loss 0.932354, current_train_items 123648.
I0302 19:00:20.782233 22535416901760 run.py:483] Algo bellman_ford step 3864 current loss 1.038777, current_train_items 123680.
I0302 19:00:20.801548 22535416901760 run.py:483] Algo bellman_ford step 3865 current loss 0.422366, current_train_items 123712.
I0302 19:00:20.817492 22535416901760 run.py:483] Algo bellman_ford step 3866 current loss 0.531036, current_train_items 123744.
I0302 19:00:20.840727 22535416901760 run.py:483] Algo bellman_ford step 3867 current loss 0.552843, current_train_items 123776.
I0302 19:00:20.871340 22535416901760 run.py:483] Algo bellman_ford step 3868 current loss 0.611940, current_train_items 123808.
I0302 19:00:20.904138 22535416901760 run.py:483] Algo bellman_ford step 3869 current loss 0.762201, current_train_items 123840.
I0302 19:00:20.924126 22535416901760 run.py:483] Algo bellman_ford step 3870 current loss 0.318376, current_train_items 123872.
I0302 19:00:20.939790 22535416901760 run.py:483] Algo bellman_ford step 3871 current loss 0.468360, current_train_items 123904.
I0302 19:00:20.963486 22535416901760 run.py:483] Algo bellman_ford step 3872 current loss 0.615589, current_train_items 123936.
I0302 19:00:20.992828 22535416901760 run.py:483] Algo bellman_ford step 3873 current loss 0.558480, current_train_items 123968.
I0302 19:00:21.025127 22535416901760 run.py:483] Algo bellman_ford step 3874 current loss 0.801676, current_train_items 124000.
I0302 19:00:21.045152 22535416901760 run.py:483] Algo bellman_ford step 3875 current loss 0.390377, current_train_items 124032.
I0302 19:00:21.061591 22535416901760 run.py:483] Algo bellman_ford step 3876 current loss 0.547488, current_train_items 124064.
I0302 19:00:21.085188 22535416901760 run.py:483] Algo bellman_ford step 3877 current loss 0.734642, current_train_items 124096.
I0302 19:00:21.116243 22535416901760 run.py:483] Algo bellman_ford step 3878 current loss 0.688415, current_train_items 124128.
I0302 19:00:21.149523 22535416901760 run.py:483] Algo bellman_ford step 3879 current loss 0.957127, current_train_items 124160.
I0302 19:00:21.168992 22535416901760 run.py:483] Algo bellman_ford step 3880 current loss 0.345844, current_train_items 124192.
I0302 19:00:21.184979 22535416901760 run.py:483] Algo bellman_ford step 3881 current loss 0.560168, current_train_items 124224.
I0302 19:00:21.208538 22535416901760 run.py:483] Algo bellman_ford step 3882 current loss 0.783830, current_train_items 124256.
I0302 19:00:21.239336 22535416901760 run.py:483] Algo bellman_ford step 3883 current loss 0.873298, current_train_items 124288.
I0302 19:00:21.271596 22535416901760 run.py:483] Algo bellman_ford step 3884 current loss 0.969666, current_train_items 124320.
I0302 19:00:21.291643 22535416901760 run.py:483] Algo bellman_ford step 3885 current loss 0.278421, current_train_items 124352.
I0302 19:00:21.308331 22535416901760 run.py:483] Algo bellman_ford step 3886 current loss 0.496122, current_train_items 124384.
I0302 19:00:21.331500 22535416901760 run.py:483] Algo bellman_ford step 3887 current loss 0.749450, current_train_items 124416.
I0302 19:00:21.362397 22535416901760 run.py:483] Algo bellman_ford step 3888 current loss 0.721745, current_train_items 124448.
I0302 19:00:21.396501 22535416901760 run.py:483] Algo bellman_ford step 3889 current loss 0.901526, current_train_items 124480.
I0302 19:00:21.416549 22535416901760 run.py:483] Algo bellman_ford step 3890 current loss 0.296718, current_train_items 124512.
I0302 19:00:21.432776 22535416901760 run.py:483] Algo bellman_ford step 3891 current loss 0.638511, current_train_items 124544.
I0302 19:00:21.456543 22535416901760 run.py:483] Algo bellman_ford step 3892 current loss 0.593978, current_train_items 124576.
I0302 19:00:21.485360 22535416901760 run.py:483] Algo bellman_ford step 3893 current loss 0.632786, current_train_items 124608.
I0302 19:00:21.519463 22535416901760 run.py:483] Algo bellman_ford step 3894 current loss 0.829217, current_train_items 124640.
I0302 19:00:21.538839 22535416901760 run.py:483] Algo bellman_ford step 3895 current loss 0.270252, current_train_items 124672.
I0302 19:00:21.555289 22535416901760 run.py:483] Algo bellman_ford step 3896 current loss 0.555553, current_train_items 124704.
I0302 19:00:21.579668 22535416901760 run.py:483] Algo bellman_ford step 3897 current loss 0.722347, current_train_items 124736.
I0302 19:00:21.610671 22535416901760 run.py:483] Algo bellman_ford step 3898 current loss 0.691597, current_train_items 124768.
I0302 19:00:21.643562 22535416901760 run.py:483] Algo bellman_ford step 3899 current loss 0.836439, current_train_items 124800.
I0302 19:00:21.663710 22535416901760 run.py:483] Algo bellman_ford step 3900 current loss 0.330753, current_train_items 124832.
I0302 19:00:21.671482 22535416901760 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 19:00:21.671586 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.931, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:00:21.688484 22535416901760 run.py:483] Algo bellman_ford step 3901 current loss 0.565987, current_train_items 124864.
I0302 19:00:21.712133 22535416901760 run.py:483] Algo bellman_ford step 3902 current loss 0.673859, current_train_items 124896.
I0302 19:00:21.743388 22535416901760 run.py:483] Algo bellman_ford step 3903 current loss 0.836140, current_train_items 124928.
I0302 19:00:21.776924 22535416901760 run.py:483] Algo bellman_ford step 3904 current loss 0.874708, current_train_items 124960.
I0302 19:00:21.796896 22535416901760 run.py:483] Algo bellman_ford step 3905 current loss 0.245964, current_train_items 124992.
I0302 19:00:21.812709 22535416901760 run.py:483] Algo bellman_ford step 3906 current loss 0.476563, current_train_items 125024.
I0302 19:00:21.835189 22535416901760 run.py:483] Algo bellman_ford step 3907 current loss 0.573595, current_train_items 125056.
I0302 19:00:21.866014 22535416901760 run.py:483] Algo bellman_ford step 3908 current loss 0.704202, current_train_items 125088.
I0302 19:00:21.899498 22535416901760 run.py:483] Algo bellman_ford step 3909 current loss 0.831324, current_train_items 125120.
I0302 19:00:21.918863 22535416901760 run.py:483] Algo bellman_ford step 3910 current loss 0.302547, current_train_items 125152.
I0302 19:00:21.934702 22535416901760 run.py:483] Algo bellman_ford step 3911 current loss 0.471335, current_train_items 125184.
I0302 19:00:21.958178 22535416901760 run.py:483] Algo bellman_ford step 3912 current loss 0.727063, current_train_items 125216.
I0302 19:00:21.988147 22535416901760 run.py:483] Algo bellman_ford step 3913 current loss 0.738065, current_train_items 125248.
I0302 19:00:22.018213 22535416901760 run.py:483] Algo bellman_ford step 3914 current loss 0.811704, current_train_items 125280.
I0302 19:00:22.037344 22535416901760 run.py:483] Algo bellman_ford step 3915 current loss 0.354538, current_train_items 125312.
I0302 19:00:22.053317 22535416901760 run.py:483] Algo bellman_ford step 3916 current loss 0.501864, current_train_items 125344.
I0302 19:00:22.077734 22535416901760 run.py:483] Algo bellman_ford step 3917 current loss 0.870938, current_train_items 125376.
I0302 19:00:22.107289 22535416901760 run.py:483] Algo bellman_ford step 3918 current loss 1.047077, current_train_items 125408.
I0302 19:00:22.140351 22535416901760 run.py:483] Algo bellman_ford step 3919 current loss 0.881148, current_train_items 125440.
I0302 19:00:22.159753 22535416901760 run.py:483] Algo bellman_ford step 3920 current loss 0.319647, current_train_items 125472.
I0302 19:00:22.176085 22535416901760 run.py:483] Algo bellman_ford step 3921 current loss 0.576602, current_train_items 125504.
I0302 19:00:22.200355 22535416901760 run.py:483] Algo bellman_ford step 3922 current loss 0.619130, current_train_items 125536.
I0302 19:00:22.232134 22535416901760 run.py:483] Algo bellman_ford step 3923 current loss 0.851232, current_train_items 125568.
I0302 19:00:22.267696 22535416901760 run.py:483] Algo bellman_ford step 3924 current loss 0.911811, current_train_items 125600.
I0302 19:00:22.287576 22535416901760 run.py:483] Algo bellman_ford step 3925 current loss 0.323033, current_train_items 125632.
I0302 19:00:22.303763 22535416901760 run.py:483] Algo bellman_ford step 3926 current loss 0.441877, current_train_items 125664.
I0302 19:00:22.327172 22535416901760 run.py:483] Algo bellman_ford step 3927 current loss 0.635753, current_train_items 125696.
I0302 19:00:22.357505 22535416901760 run.py:483] Algo bellman_ford step 3928 current loss 0.773996, current_train_items 125728.
I0302 19:00:22.390330 22535416901760 run.py:483] Algo bellman_ford step 3929 current loss 0.879425, current_train_items 125760.
I0302 19:00:22.409641 22535416901760 run.py:483] Algo bellman_ford step 3930 current loss 0.314520, current_train_items 125792.
I0302 19:00:22.425579 22535416901760 run.py:483] Algo bellman_ford step 3931 current loss 0.585126, current_train_items 125824.
I0302 19:00:22.448816 22535416901760 run.py:483] Algo bellman_ford step 3932 current loss 0.639007, current_train_items 125856.
I0302 19:00:22.480697 22535416901760 run.py:483] Algo bellman_ford step 3933 current loss 0.734022, current_train_items 125888.
I0302 19:00:22.514069 22535416901760 run.py:483] Algo bellman_ford step 3934 current loss 0.816743, current_train_items 125920.
I0302 19:00:22.533472 22535416901760 run.py:483] Algo bellman_ford step 3935 current loss 0.339469, current_train_items 125952.
I0302 19:00:22.549892 22535416901760 run.py:483] Algo bellman_ford step 3936 current loss 0.512935, current_train_items 125984.
I0302 19:00:22.572867 22535416901760 run.py:483] Algo bellman_ford step 3937 current loss 0.628394, current_train_items 126016.
I0302 19:00:22.603921 22535416901760 run.py:483] Algo bellman_ford step 3938 current loss 0.817027, current_train_items 126048.
I0302 19:00:22.638078 22535416901760 run.py:483] Algo bellman_ford step 3939 current loss 0.824348, current_train_items 126080.
I0302 19:00:22.657692 22535416901760 run.py:483] Algo bellman_ford step 3940 current loss 0.349819, current_train_items 126112.
I0302 19:00:22.673713 22535416901760 run.py:483] Algo bellman_ford step 3941 current loss 0.440915, current_train_items 126144.
I0302 19:00:22.696983 22535416901760 run.py:483] Algo bellman_ford step 3942 current loss 0.608283, current_train_items 126176.
I0302 19:00:22.726620 22535416901760 run.py:483] Algo bellman_ford step 3943 current loss 0.612369, current_train_items 126208.
I0302 19:00:22.760808 22535416901760 run.py:483] Algo bellman_ford step 3944 current loss 0.804900, current_train_items 126240.
I0302 19:00:22.780051 22535416901760 run.py:483] Algo bellman_ford step 3945 current loss 0.358111, current_train_items 126272.
I0302 19:00:22.796453 22535416901760 run.py:483] Algo bellman_ford step 3946 current loss 0.468508, current_train_items 126304.
I0302 19:00:22.819500 22535416901760 run.py:483] Algo bellman_ford step 3947 current loss 0.581713, current_train_items 126336.
I0302 19:00:22.848412 22535416901760 run.py:483] Algo bellman_ford step 3948 current loss 0.782592, current_train_items 126368.
I0302 19:00:22.882290 22535416901760 run.py:483] Algo bellman_ford step 3949 current loss 0.747164, current_train_items 126400.
I0302 19:00:22.901777 22535416901760 run.py:483] Algo bellman_ford step 3950 current loss 0.259008, current_train_items 126432.
I0302 19:00:22.909904 22535416901760 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 19:00:22.910009 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.931, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:22.939765 22535416901760 run.py:483] Algo bellman_ford step 3951 current loss 0.484352, current_train_items 126464.
I0302 19:00:22.963986 22535416901760 run.py:483] Algo bellman_ford step 3952 current loss 0.614619, current_train_items 126496.
I0302 19:00:22.994405 22535416901760 run.py:483] Algo bellman_ford step 3953 current loss 0.662374, current_train_items 126528.
I0302 19:00:23.026297 22535416901760 run.py:483] Algo bellman_ford step 3954 current loss 0.752142, current_train_items 126560.
I0302 19:00:23.046332 22535416901760 run.py:483] Algo bellman_ford step 3955 current loss 0.327856, current_train_items 126592.
I0302 19:00:23.062218 22535416901760 run.py:483] Algo bellman_ford step 3956 current loss 0.420509, current_train_items 126624.
I0302 19:00:23.085827 22535416901760 run.py:483] Algo bellman_ford step 3957 current loss 0.604126, current_train_items 126656.
I0302 19:00:23.117823 22535416901760 run.py:483] Algo bellman_ford step 3958 current loss 0.738448, current_train_items 126688.
I0302 19:00:23.153062 22535416901760 run.py:483] Algo bellman_ford step 3959 current loss 0.790688, current_train_items 126720.
I0302 19:00:23.173167 22535416901760 run.py:483] Algo bellman_ford step 3960 current loss 0.306686, current_train_items 126752.
I0302 19:00:23.189228 22535416901760 run.py:483] Algo bellman_ford step 3961 current loss 0.451406, current_train_items 126784.
I0302 19:00:23.211444 22535416901760 run.py:483] Algo bellman_ford step 3962 current loss 0.596833, current_train_items 126816.
I0302 19:00:23.242399 22535416901760 run.py:483] Algo bellman_ford step 3963 current loss 0.706186, current_train_items 126848.
I0302 19:00:23.276139 22535416901760 run.py:483] Algo bellman_ford step 3964 current loss 1.034086, current_train_items 126880.
I0302 19:00:23.295557 22535416901760 run.py:483] Algo bellman_ford step 3965 current loss 0.373773, current_train_items 126912.
I0302 19:00:23.311727 22535416901760 run.py:483] Algo bellman_ford step 3966 current loss 0.495201, current_train_items 126944.
I0302 19:00:23.336362 22535416901760 run.py:483] Algo bellman_ford step 3967 current loss 0.820953, current_train_items 126976.
I0302 19:00:23.367441 22535416901760 run.py:483] Algo bellman_ford step 3968 current loss 0.774388, current_train_items 127008.
I0302 19:00:23.400130 22535416901760 run.py:483] Algo bellman_ford step 3969 current loss 0.767938, current_train_items 127040.
I0302 19:00:23.419919 22535416901760 run.py:483] Algo bellman_ford step 3970 current loss 0.349324, current_train_items 127072.
I0302 19:00:23.436017 22535416901760 run.py:483] Algo bellman_ford step 3971 current loss 0.455985, current_train_items 127104.
I0302 19:00:23.459547 22535416901760 run.py:483] Algo bellman_ford step 3972 current loss 0.671795, current_train_items 127136.
I0302 19:00:23.491344 22535416901760 run.py:483] Algo bellman_ford step 3973 current loss 0.951987, current_train_items 127168.
I0302 19:00:23.522933 22535416901760 run.py:483] Algo bellman_ford step 3974 current loss 0.899120, current_train_items 127200.
I0302 19:00:23.542752 22535416901760 run.py:483] Algo bellman_ford step 3975 current loss 0.283579, current_train_items 127232.
I0302 19:00:23.559231 22535416901760 run.py:483] Algo bellman_ford step 3976 current loss 0.448597, current_train_items 127264.
I0302 19:00:23.581990 22535416901760 run.py:483] Algo bellman_ford step 3977 current loss 0.593504, current_train_items 127296.
I0302 19:00:23.613653 22535416901760 run.py:483] Algo bellman_ford step 3978 current loss 0.806663, current_train_items 127328.
I0302 19:00:23.646594 22535416901760 run.py:483] Algo bellman_ford step 3979 current loss 0.879551, current_train_items 127360.
I0302 19:00:23.665882 22535416901760 run.py:483] Algo bellman_ford step 3980 current loss 0.289868, current_train_items 127392.
I0302 19:00:23.681872 22535416901760 run.py:483] Algo bellman_ford step 3981 current loss 0.470452, current_train_items 127424.
I0302 19:00:23.704683 22535416901760 run.py:483] Algo bellman_ford step 3982 current loss 0.766230, current_train_items 127456.
I0302 19:00:23.735922 22535416901760 run.py:483] Algo bellman_ford step 3983 current loss 0.797912, current_train_items 127488.
I0302 19:00:23.767675 22535416901760 run.py:483] Algo bellman_ford step 3984 current loss 0.806796, current_train_items 127520.
I0302 19:00:23.787405 22535416901760 run.py:483] Algo bellman_ford step 3985 current loss 0.270406, current_train_items 127552.
I0302 19:00:23.803798 22535416901760 run.py:483] Algo bellman_ford step 3986 current loss 0.644324, current_train_items 127584.
I0302 19:00:23.826708 22535416901760 run.py:483] Algo bellman_ford step 3987 current loss 0.579442, current_train_items 127616.
I0302 19:00:23.858003 22535416901760 run.py:483] Algo bellman_ford step 3988 current loss 0.684955, current_train_items 127648.
I0302 19:00:23.891449 22535416901760 run.py:483] Algo bellman_ford step 3989 current loss 0.812786, current_train_items 127680.
I0302 19:00:23.911638 22535416901760 run.py:483] Algo bellman_ford step 3990 current loss 0.289885, current_train_items 127712.
I0302 19:00:23.928051 22535416901760 run.py:483] Algo bellman_ford step 3991 current loss 0.500487, current_train_items 127744.
I0302 19:00:23.950551 22535416901760 run.py:483] Algo bellman_ford step 3992 current loss 0.607226, current_train_items 127776.
I0302 19:00:23.981314 22535416901760 run.py:483] Algo bellman_ford step 3993 current loss 0.779175, current_train_items 127808.
I0302 19:00:24.014613 22535416901760 run.py:483] Algo bellman_ford step 3994 current loss 0.861900, current_train_items 127840.
I0302 19:00:24.034494 22535416901760 run.py:483] Algo bellman_ford step 3995 current loss 0.364626, current_train_items 127872.
I0302 19:00:24.050794 22535416901760 run.py:483] Algo bellman_ford step 3996 current loss 0.538217, current_train_items 127904.
I0302 19:00:24.074540 22535416901760 run.py:483] Algo bellman_ford step 3997 current loss 0.574712, current_train_items 127936.
I0302 19:00:24.104725 22535416901760 run.py:483] Algo bellman_ford step 3998 current loss 0.655675, current_train_items 127968.
I0302 19:00:24.139191 22535416901760 run.py:483] Algo bellman_ford step 3999 current loss 0.757028, current_train_items 128000.
I0302 19:00:24.159352 22535416901760 run.py:483] Algo bellman_ford step 4000 current loss 0.355840, current_train_items 128032.
I0302 19:00:24.167207 22535416901760 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 19:00:24.167311 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:24.183957 22535416901760 run.py:483] Algo bellman_ford step 4001 current loss 0.442712, current_train_items 128064.
I0302 19:00:24.207735 22535416901760 run.py:483] Algo bellman_ford step 4002 current loss 0.668943, current_train_items 128096.
I0302 19:00:24.238237 22535416901760 run.py:483] Algo bellman_ford step 4003 current loss 0.641223, current_train_items 128128.
I0302 19:00:24.271344 22535416901760 run.py:483] Algo bellman_ford step 4004 current loss 0.746762, current_train_items 128160.
I0302 19:00:24.291079 22535416901760 run.py:483] Algo bellman_ford step 4005 current loss 0.269967, current_train_items 128192.
I0302 19:00:24.306976 22535416901760 run.py:483] Algo bellman_ford step 4006 current loss 0.476756, current_train_items 128224.
I0302 19:00:24.330778 22535416901760 run.py:483] Algo bellman_ford step 4007 current loss 0.648870, current_train_items 128256.
I0302 19:00:24.362404 22535416901760 run.py:483] Algo bellman_ford step 4008 current loss 0.710967, current_train_items 128288.
I0302 19:00:24.394619 22535416901760 run.py:483] Algo bellman_ford step 4009 current loss 0.875053, current_train_items 128320.
I0302 19:00:24.414588 22535416901760 run.py:483] Algo bellman_ford step 4010 current loss 0.376279, current_train_items 128352.
I0302 19:00:24.430704 22535416901760 run.py:483] Algo bellman_ford step 4011 current loss 0.455394, current_train_items 128384.
I0302 19:00:24.453214 22535416901760 run.py:483] Algo bellman_ford step 4012 current loss 0.564261, current_train_items 128416.
I0302 19:00:24.483811 22535416901760 run.py:483] Algo bellman_ford step 4013 current loss 0.613590, current_train_items 128448.
I0302 19:00:24.517178 22535416901760 run.py:483] Algo bellman_ford step 4014 current loss 0.805522, current_train_items 128480.
I0302 19:00:24.536520 22535416901760 run.py:483] Algo bellman_ford step 4015 current loss 0.269180, current_train_items 128512.
I0302 19:00:24.552655 22535416901760 run.py:483] Algo bellman_ford step 4016 current loss 0.487033, current_train_items 128544.
I0302 19:00:24.576278 22535416901760 run.py:483] Algo bellman_ford step 4017 current loss 0.626255, current_train_items 128576.
I0302 19:00:24.606819 22535416901760 run.py:483] Algo bellman_ford step 4018 current loss 0.802395, current_train_items 128608.
I0302 19:00:24.639120 22535416901760 run.py:483] Algo bellman_ford step 4019 current loss 0.782702, current_train_items 128640.
I0302 19:00:24.658500 22535416901760 run.py:483] Algo bellman_ford step 4020 current loss 0.309749, current_train_items 128672.
I0302 19:00:24.674461 22535416901760 run.py:483] Algo bellman_ford step 4021 current loss 0.457288, current_train_items 128704.
I0302 19:00:24.698550 22535416901760 run.py:483] Algo bellman_ford step 4022 current loss 0.613315, current_train_items 128736.
I0302 19:00:24.728662 22535416901760 run.py:483] Algo bellman_ford step 4023 current loss 0.614362, current_train_items 128768.
I0302 19:00:24.764450 22535416901760 run.py:483] Algo bellman_ford step 4024 current loss 1.004628, current_train_items 128800.
I0302 19:00:24.784001 22535416901760 run.py:483] Algo bellman_ford step 4025 current loss 0.311549, current_train_items 128832.
I0302 19:00:24.800309 22535416901760 run.py:483] Algo bellman_ford step 4026 current loss 0.417484, current_train_items 128864.
I0302 19:00:24.823427 22535416901760 run.py:483] Algo bellman_ford step 4027 current loss 0.528112, current_train_items 128896.
I0302 19:00:24.853544 22535416901760 run.py:483] Algo bellman_ford step 4028 current loss 0.695791, current_train_items 128928.
I0302 19:00:24.886193 22535416901760 run.py:483] Algo bellman_ford step 4029 current loss 0.745642, current_train_items 128960.
I0302 19:00:24.905636 22535416901760 run.py:483] Algo bellman_ford step 4030 current loss 0.325989, current_train_items 128992.
I0302 19:00:24.921505 22535416901760 run.py:483] Algo bellman_ford step 4031 current loss 0.442846, current_train_items 129024.
I0302 19:00:24.945202 22535416901760 run.py:483] Algo bellman_ford step 4032 current loss 0.616176, current_train_items 129056.
I0302 19:00:24.976739 22535416901760 run.py:483] Algo bellman_ford step 4033 current loss 0.703027, current_train_items 129088.
I0302 19:00:25.009616 22535416901760 run.py:483] Algo bellman_ford step 4034 current loss 0.715051, current_train_items 129120.
I0302 19:00:25.029290 22535416901760 run.py:483] Algo bellman_ford step 4035 current loss 0.316703, current_train_items 129152.
I0302 19:00:25.045165 22535416901760 run.py:483] Algo bellman_ford step 4036 current loss 0.644023, current_train_items 129184.
I0302 19:00:25.068275 22535416901760 run.py:483] Algo bellman_ford step 4037 current loss 0.558795, current_train_items 129216.
I0302 19:00:25.099320 22535416901760 run.py:483] Algo bellman_ford step 4038 current loss 0.728053, current_train_items 129248.
I0302 19:00:25.133104 22535416901760 run.py:483] Algo bellman_ford step 4039 current loss 0.914155, current_train_items 129280.
I0302 19:00:25.152677 22535416901760 run.py:483] Algo bellman_ford step 4040 current loss 0.287777, current_train_items 129312.
I0302 19:00:25.168890 22535416901760 run.py:483] Algo bellman_ford step 4041 current loss 0.406778, current_train_items 129344.
I0302 19:00:25.191915 22535416901760 run.py:483] Algo bellman_ford step 4042 current loss 0.627887, current_train_items 129376.
I0302 19:00:25.224110 22535416901760 run.py:483] Algo bellman_ford step 4043 current loss 0.812617, current_train_items 129408.
I0302 19:00:25.256424 22535416901760 run.py:483] Algo bellman_ford step 4044 current loss 0.916204, current_train_items 129440.
I0302 19:00:25.275857 22535416901760 run.py:483] Algo bellman_ford step 4045 current loss 0.358525, current_train_items 129472.
I0302 19:00:25.292277 22535416901760 run.py:483] Algo bellman_ford step 4046 current loss 0.628202, current_train_items 129504.
I0302 19:00:25.315237 22535416901760 run.py:483] Algo bellman_ford step 4047 current loss 0.677299, current_train_items 129536.
I0302 19:00:25.345274 22535416901760 run.py:483] Algo bellman_ford step 4048 current loss 0.710219, current_train_items 129568.
I0302 19:00:25.379860 22535416901760 run.py:483] Algo bellman_ford step 4049 current loss 0.793109, current_train_items 129600.
I0302 19:00:25.399305 22535416901760 run.py:483] Algo bellman_ford step 4050 current loss 0.293349, current_train_items 129632.
I0302 19:00:25.407506 22535416901760 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 19:00:25.407612 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:00:25.424529 22535416901760 run.py:483] Algo bellman_ford step 4051 current loss 0.534942, current_train_items 129664.
I0302 19:00:25.447441 22535416901760 run.py:483] Algo bellman_ford step 4052 current loss 0.574993, current_train_items 129696.
I0302 19:00:25.478368 22535416901760 run.py:483] Algo bellman_ford step 4053 current loss 0.777279, current_train_items 129728.
I0302 19:00:25.513862 22535416901760 run.py:483] Algo bellman_ford step 4054 current loss 0.884602, current_train_items 129760.
I0302 19:00:25.533653 22535416901760 run.py:483] Algo bellman_ford step 4055 current loss 0.391761, current_train_items 129792.
I0302 19:00:25.549312 22535416901760 run.py:483] Algo bellman_ford step 4056 current loss 0.476753, current_train_items 129824.
I0302 19:00:25.572373 22535416901760 run.py:483] Algo bellman_ford step 4057 current loss 0.555477, current_train_items 129856.
I0302 19:00:25.602067 22535416901760 run.py:483] Algo bellman_ford step 4058 current loss 0.748423, current_train_items 129888.
I0302 19:00:25.633804 22535416901760 run.py:483] Algo bellman_ford step 4059 current loss 0.769357, current_train_items 129920.
I0302 19:00:25.653554 22535416901760 run.py:483] Algo bellman_ford step 4060 current loss 0.349666, current_train_items 129952.
I0302 19:00:25.670174 22535416901760 run.py:483] Algo bellman_ford step 4061 current loss 0.407862, current_train_items 129984.
I0302 19:00:25.692400 22535416901760 run.py:483] Algo bellman_ford step 4062 current loss 0.639224, current_train_items 130016.
I0302 19:00:25.721701 22535416901760 run.py:483] Algo bellman_ford step 4063 current loss 0.612595, current_train_items 130048.
I0302 19:00:25.756425 22535416901760 run.py:483] Algo bellman_ford step 4064 current loss 0.916302, current_train_items 130080.
I0302 19:00:25.775745 22535416901760 run.py:483] Algo bellman_ford step 4065 current loss 0.320462, current_train_items 130112.
I0302 19:00:25.791443 22535416901760 run.py:483] Algo bellman_ford step 4066 current loss 0.489750, current_train_items 130144.
I0302 19:00:25.815728 22535416901760 run.py:483] Algo bellman_ford step 4067 current loss 0.663665, current_train_items 130176.
I0302 19:00:25.845840 22535416901760 run.py:483] Algo bellman_ford step 4068 current loss 0.609657, current_train_items 130208.
I0302 19:00:25.880275 22535416901760 run.py:483] Algo bellman_ford step 4069 current loss 0.774983, current_train_items 130240.
I0302 19:00:25.900015 22535416901760 run.py:483] Algo bellman_ford step 4070 current loss 0.332206, current_train_items 130272.
I0302 19:00:25.916280 22535416901760 run.py:483] Algo bellman_ford step 4071 current loss 0.443584, current_train_items 130304.
I0302 19:00:25.939078 22535416901760 run.py:483] Algo bellman_ford step 4072 current loss 0.702222, current_train_items 130336.
I0302 19:00:25.968199 22535416901760 run.py:483] Algo bellman_ford step 4073 current loss 0.756969, current_train_items 130368.
I0302 19:00:26.000432 22535416901760 run.py:483] Algo bellman_ford step 4074 current loss 0.728021, current_train_items 130400.
I0302 19:00:26.020181 22535416901760 run.py:483] Algo bellman_ford step 4075 current loss 0.285028, current_train_items 130432.
I0302 19:00:26.036785 22535416901760 run.py:483] Algo bellman_ford step 4076 current loss 0.623651, current_train_items 130464.
I0302 19:00:26.060568 22535416901760 run.py:483] Algo bellman_ford step 4077 current loss 0.845880, current_train_items 130496.
I0302 19:00:26.090743 22535416901760 run.py:483] Algo bellman_ford step 4078 current loss 0.766444, current_train_items 130528.
I0302 19:00:26.123286 22535416901760 run.py:483] Algo bellman_ford step 4079 current loss 0.764430, current_train_items 130560.
I0302 19:00:26.142464 22535416901760 run.py:483] Algo bellman_ford step 4080 current loss 0.302722, current_train_items 130592.
I0302 19:00:26.158865 22535416901760 run.py:483] Algo bellman_ford step 4081 current loss 0.434792, current_train_items 130624.
I0302 19:00:26.182844 22535416901760 run.py:483] Algo bellman_ford step 4082 current loss 0.731312, current_train_items 130656.
I0302 19:00:26.214848 22535416901760 run.py:483] Algo bellman_ford step 4083 current loss 0.669435, current_train_items 130688.
I0302 19:00:26.245694 22535416901760 run.py:483] Algo bellman_ford step 4084 current loss 0.699374, current_train_items 130720.
I0302 19:00:26.265645 22535416901760 run.py:483] Algo bellman_ford step 4085 current loss 0.315123, current_train_items 130752.
I0302 19:00:26.281882 22535416901760 run.py:483] Algo bellman_ford step 4086 current loss 0.466677, current_train_items 130784.
I0302 19:00:26.304286 22535416901760 run.py:483] Algo bellman_ford step 4087 current loss 0.590036, current_train_items 130816.
I0302 19:00:26.334229 22535416901760 run.py:483] Algo bellman_ford step 4088 current loss 0.653135, current_train_items 130848.
I0302 19:00:26.366091 22535416901760 run.py:483] Algo bellman_ford step 4089 current loss 0.691921, current_train_items 130880.
I0302 19:00:26.385638 22535416901760 run.py:483] Algo bellman_ford step 4090 current loss 0.298299, current_train_items 130912.
I0302 19:00:26.401695 22535416901760 run.py:483] Algo bellman_ford step 4091 current loss 0.402723, current_train_items 130944.
I0302 19:00:26.425289 22535416901760 run.py:483] Algo bellman_ford step 4092 current loss 0.723787, current_train_items 130976.
I0302 19:00:26.456781 22535416901760 run.py:483] Algo bellman_ford step 4093 current loss 0.847530, current_train_items 131008.
I0302 19:00:26.490370 22535416901760 run.py:483] Algo bellman_ford step 4094 current loss 0.894194, current_train_items 131040.
I0302 19:00:26.509792 22535416901760 run.py:483] Algo bellman_ford step 4095 current loss 0.323917, current_train_items 131072.
I0302 19:00:26.526190 22535416901760 run.py:483] Algo bellman_ford step 4096 current loss 0.566373, current_train_items 131104.
I0302 19:00:26.550113 22535416901760 run.py:483] Algo bellman_ford step 4097 current loss 0.644670, current_train_items 131136.
I0302 19:00:26.580773 22535416901760 run.py:483] Algo bellman_ford step 4098 current loss 0.870083, current_train_items 131168.
I0302 19:00:26.614674 22535416901760 run.py:483] Algo bellman_ford step 4099 current loss 0.928402, current_train_items 131200.
I0302 19:00:26.634387 22535416901760 run.py:483] Algo bellman_ford step 4100 current loss 0.313293, current_train_items 131232.
I0302 19:00:26.642073 22535416901760 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 19:00:26.642187 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:26.658347 22535416901760 run.py:483] Algo bellman_ford step 4101 current loss 0.436037, current_train_items 131264.
I0302 19:00:26.679983 22535416901760 run.py:483] Algo bellman_ford step 4102 current loss 0.618493, current_train_items 131296.
I0302 19:00:26.711698 22535416901760 run.py:483] Algo bellman_ford step 4103 current loss 0.736596, current_train_items 131328.
I0302 19:00:26.746915 22535416901760 run.py:483] Algo bellman_ford step 4104 current loss 0.791569, current_train_items 131360.
I0302 19:00:26.766641 22535416901760 run.py:483] Algo bellman_ford step 4105 current loss 0.311349, current_train_items 131392.
I0302 19:00:26.782769 22535416901760 run.py:483] Algo bellman_ford step 4106 current loss 0.451687, current_train_items 131424.
I0302 19:00:26.806747 22535416901760 run.py:483] Algo bellman_ford step 4107 current loss 0.612970, current_train_items 131456.
I0302 19:00:26.837447 22535416901760 run.py:483] Algo bellman_ford step 4108 current loss 0.748346, current_train_items 131488.
I0302 19:00:26.872927 22535416901760 run.py:483] Algo bellman_ford step 4109 current loss 0.920122, current_train_items 131520.
I0302 19:00:26.892677 22535416901760 run.py:483] Algo bellman_ford step 4110 current loss 0.333098, current_train_items 131552.
I0302 19:00:26.909250 22535416901760 run.py:483] Algo bellman_ford step 4111 current loss 0.512300, current_train_items 131584.
I0302 19:00:26.931629 22535416901760 run.py:483] Algo bellman_ford step 4112 current loss 0.648067, current_train_items 131616.
I0302 19:00:26.962937 22535416901760 run.py:483] Algo bellman_ford step 4113 current loss 0.639285, current_train_items 131648.
I0302 19:00:26.997131 22535416901760 run.py:483] Algo bellman_ford step 4114 current loss 0.705228, current_train_items 131680.
I0302 19:00:27.016825 22535416901760 run.py:483] Algo bellman_ford step 4115 current loss 0.302359, current_train_items 131712.
I0302 19:00:27.033245 22535416901760 run.py:483] Algo bellman_ford step 4116 current loss 0.499443, current_train_items 131744.
I0302 19:00:27.057992 22535416901760 run.py:483] Algo bellman_ford step 4117 current loss 0.741265, current_train_items 131776.
I0302 19:00:27.087785 22535416901760 run.py:483] Algo bellman_ford step 4118 current loss 0.643343, current_train_items 131808.
I0302 19:00:27.121649 22535416901760 run.py:483] Algo bellman_ford step 4119 current loss 0.920082, current_train_items 131840.
I0302 19:00:27.140972 22535416901760 run.py:483] Algo bellman_ford step 4120 current loss 0.291871, current_train_items 131872.
I0302 19:00:27.157102 22535416901760 run.py:483] Algo bellman_ford step 4121 current loss 0.439379, current_train_items 131904.
I0302 19:00:27.181348 22535416901760 run.py:483] Algo bellman_ford step 4122 current loss 0.639691, current_train_items 131936.
I0302 19:00:27.213171 22535416901760 run.py:483] Algo bellman_ford step 4123 current loss 0.852478, current_train_items 131968.
I0302 19:00:27.246541 22535416901760 run.py:483] Algo bellman_ford step 4124 current loss 0.790516, current_train_items 132000.
I0302 19:00:27.265938 22535416901760 run.py:483] Algo bellman_ford step 4125 current loss 0.241676, current_train_items 132032.
I0302 19:00:27.282138 22535416901760 run.py:483] Algo bellman_ford step 4126 current loss 0.513286, current_train_items 132064.
I0302 19:00:27.306562 22535416901760 run.py:483] Algo bellman_ford step 4127 current loss 0.805864, current_train_items 132096.
I0302 19:00:27.336899 22535416901760 run.py:483] Algo bellman_ford step 4128 current loss 0.794057, current_train_items 132128.
I0302 19:00:27.369017 22535416901760 run.py:483] Algo bellman_ford step 4129 current loss 0.697392, current_train_items 132160.
I0302 19:00:27.388932 22535416901760 run.py:483] Algo bellman_ford step 4130 current loss 0.314137, current_train_items 132192.
I0302 19:00:27.405244 22535416901760 run.py:483] Algo bellman_ford step 4131 current loss 0.446416, current_train_items 132224.
I0302 19:00:27.428763 22535416901760 run.py:483] Algo bellman_ford step 4132 current loss 0.793355, current_train_items 132256.
I0302 19:00:27.460145 22535416901760 run.py:483] Algo bellman_ford step 4133 current loss 0.897043, current_train_items 132288.
I0302 19:00:27.495024 22535416901760 run.py:483] Algo bellman_ford step 4134 current loss 0.997213, current_train_items 132320.
I0302 19:00:27.514998 22535416901760 run.py:483] Algo bellman_ford step 4135 current loss 0.310011, current_train_items 132352.
I0302 19:00:27.531021 22535416901760 run.py:483] Algo bellman_ford step 4136 current loss 0.447672, current_train_items 132384.
I0302 19:00:27.554371 22535416901760 run.py:483] Algo bellman_ford step 4137 current loss 0.653689, current_train_items 132416.
I0302 19:00:27.584615 22535416901760 run.py:483] Algo bellman_ford step 4138 current loss 0.653044, current_train_items 132448.
I0302 19:00:27.618681 22535416901760 run.py:483] Algo bellman_ford step 4139 current loss 0.895347, current_train_items 132480.
I0302 19:00:27.638430 22535416901760 run.py:483] Algo bellman_ford step 4140 current loss 0.321108, current_train_items 132512.
I0302 19:00:27.654130 22535416901760 run.py:483] Algo bellman_ford step 4141 current loss 0.452550, current_train_items 132544.
I0302 19:00:27.678661 22535416901760 run.py:483] Algo bellman_ford step 4142 current loss 0.786523, current_train_items 132576.
I0302 19:00:27.708666 22535416901760 run.py:483] Algo bellman_ford step 4143 current loss 0.648474, current_train_items 132608.
I0302 19:00:27.741791 22535416901760 run.py:483] Algo bellman_ford step 4144 current loss 0.758133, current_train_items 132640.
I0302 19:00:27.761494 22535416901760 run.py:483] Algo bellman_ford step 4145 current loss 0.291744, current_train_items 132672.
I0302 19:00:27.777333 22535416901760 run.py:483] Algo bellman_ford step 4146 current loss 0.409008, current_train_items 132704.
I0302 19:00:27.800352 22535416901760 run.py:483] Algo bellman_ford step 4147 current loss 0.681422, current_train_items 132736.
I0302 19:00:27.829635 22535416901760 run.py:483] Algo bellman_ford step 4148 current loss 0.631467, current_train_items 132768.
I0302 19:00:27.862010 22535416901760 run.py:483] Algo bellman_ford step 4149 current loss 0.805004, current_train_items 132800.
I0302 19:00:27.881748 22535416901760 run.py:483] Algo bellman_ford step 4150 current loss 0.318542, current_train_items 132832.
I0302 19:00:27.889583 22535416901760 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 19:00:27.889691 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:27.906506 22535416901760 run.py:483] Algo bellman_ford step 4151 current loss 0.604685, current_train_items 132864.
I0302 19:00:27.930461 22535416901760 run.py:483] Algo bellman_ford step 4152 current loss 0.651381, current_train_items 132896.
I0302 19:00:27.961498 22535416901760 run.py:483] Algo bellman_ford step 4153 current loss 0.618737, current_train_items 132928.
I0302 19:00:27.996327 22535416901760 run.py:483] Algo bellman_ford step 4154 current loss 0.882223, current_train_items 132960.
I0302 19:00:28.016062 22535416901760 run.py:483] Algo bellman_ford step 4155 current loss 0.325018, current_train_items 132992.
I0302 19:00:28.032322 22535416901760 run.py:483] Algo bellman_ford step 4156 current loss 0.429978, current_train_items 133024.
I0302 19:00:28.054899 22535416901760 run.py:483] Algo bellman_ford step 4157 current loss 0.525385, current_train_items 133056.
I0302 19:00:28.084807 22535416901760 run.py:483] Algo bellman_ford step 4158 current loss 0.691345, current_train_items 133088.
I0302 19:00:28.118330 22535416901760 run.py:483] Algo bellman_ford step 4159 current loss 0.811241, current_train_items 133120.
I0302 19:00:28.138117 22535416901760 run.py:483] Algo bellman_ford step 4160 current loss 0.291476, current_train_items 133152.
I0302 19:00:28.154432 22535416901760 run.py:483] Algo bellman_ford step 4161 current loss 0.461957, current_train_items 133184.
I0302 19:00:28.176048 22535416901760 run.py:483] Algo bellman_ford step 4162 current loss 0.632669, current_train_items 133216.
I0302 19:00:28.206842 22535416901760 run.py:483] Algo bellman_ford step 4163 current loss 0.635107, current_train_items 133248.
I0302 19:00:28.240489 22535416901760 run.py:483] Algo bellman_ford step 4164 current loss 0.722308, current_train_items 133280.
I0302 19:00:28.260199 22535416901760 run.py:483] Algo bellman_ford step 4165 current loss 0.372892, current_train_items 133312.
I0302 19:00:28.276238 22535416901760 run.py:483] Algo bellman_ford step 4166 current loss 0.437063, current_train_items 133344.
I0302 19:00:28.300422 22535416901760 run.py:483] Algo bellman_ford step 4167 current loss 0.682472, current_train_items 133376.
I0302 19:00:28.330667 22535416901760 run.py:483] Algo bellman_ford step 4168 current loss 0.632311, current_train_items 133408.
I0302 19:00:28.362787 22535416901760 run.py:483] Algo bellman_ford step 4169 current loss 0.695411, current_train_items 133440.
I0302 19:00:28.382562 22535416901760 run.py:483] Algo bellman_ford step 4170 current loss 0.274651, current_train_items 133472.
I0302 19:00:28.398220 22535416901760 run.py:483] Algo bellman_ford step 4171 current loss 0.414484, current_train_items 133504.
I0302 19:00:28.421221 22535416901760 run.py:483] Algo bellman_ford step 4172 current loss 0.621687, current_train_items 133536.
I0302 19:00:28.451270 22535416901760 run.py:483] Algo bellman_ford step 4173 current loss 0.623141, current_train_items 133568.
I0302 19:00:28.486445 22535416901760 run.py:483] Algo bellman_ford step 4174 current loss 0.805114, current_train_items 133600.
I0302 19:00:28.505956 22535416901760 run.py:483] Algo bellman_ford step 4175 current loss 0.296252, current_train_items 133632.
I0302 19:00:28.522706 22535416901760 run.py:483] Algo bellman_ford step 4176 current loss 0.486544, current_train_items 133664.
I0302 19:00:28.545840 22535416901760 run.py:483] Algo bellman_ford step 4177 current loss 0.708209, current_train_items 133696.
I0302 19:00:28.576665 22535416901760 run.py:483] Algo bellman_ford step 4178 current loss 0.785627, current_train_items 133728.
I0302 19:00:28.610417 22535416901760 run.py:483] Algo bellman_ford step 4179 current loss 0.889275, current_train_items 133760.
I0302 19:00:28.629882 22535416901760 run.py:483] Algo bellman_ford step 4180 current loss 0.336167, current_train_items 133792.
I0302 19:00:28.645963 22535416901760 run.py:483] Algo bellman_ford step 4181 current loss 0.464819, current_train_items 133824.
I0302 19:00:28.669987 22535416901760 run.py:483] Algo bellman_ford step 4182 current loss 0.591735, current_train_items 133856.
I0302 19:00:28.699004 22535416901760 run.py:483] Algo bellman_ford step 4183 current loss 0.607841, current_train_items 133888.
I0302 19:00:28.732618 22535416901760 run.py:483] Algo bellman_ford step 4184 current loss 0.801894, current_train_items 133920.
I0302 19:00:28.752313 22535416901760 run.py:483] Algo bellman_ford step 4185 current loss 0.296953, current_train_items 133952.
I0302 19:00:28.768242 22535416901760 run.py:483] Algo bellman_ford step 4186 current loss 0.486667, current_train_items 133984.
I0302 19:00:28.791380 22535416901760 run.py:483] Algo bellman_ford step 4187 current loss 0.664338, current_train_items 134016.
I0302 19:00:28.821481 22535416901760 run.py:483] Algo bellman_ford step 4188 current loss 0.747782, current_train_items 134048.
I0302 19:00:28.853719 22535416901760 run.py:483] Algo bellman_ford step 4189 current loss 0.780533, current_train_items 134080.
I0302 19:00:28.873729 22535416901760 run.py:483] Algo bellman_ford step 4190 current loss 0.360549, current_train_items 134112.
I0302 19:00:28.890352 22535416901760 run.py:483] Algo bellman_ford step 4191 current loss 0.446629, current_train_items 134144.
I0302 19:00:28.911312 22535416901760 run.py:483] Algo bellman_ford step 4192 current loss 0.513355, current_train_items 134176.
I0302 19:00:28.942380 22535416901760 run.py:483] Algo bellman_ford step 4193 current loss 0.667860, current_train_items 134208.
I0302 19:00:28.977786 22535416901760 run.py:483] Algo bellman_ford step 4194 current loss 0.909807, current_train_items 134240.
I0302 19:00:28.996983 22535416901760 run.py:483] Algo bellman_ford step 4195 current loss 0.329327, current_train_items 134272.
I0302 19:00:29.012936 22535416901760 run.py:483] Algo bellman_ford step 4196 current loss 0.522457, current_train_items 134304.
I0302 19:00:29.036498 22535416901760 run.py:483] Algo bellman_ford step 4197 current loss 0.586005, current_train_items 134336.
I0302 19:00:29.066511 22535416901760 run.py:483] Algo bellman_ford step 4198 current loss 0.606824, current_train_items 134368.
I0302 19:00:29.099442 22535416901760 run.py:483] Algo bellman_ford step 4199 current loss 0.804747, current_train_items 134400.
I0302 19:00:29.118825 22535416901760 run.py:483] Algo bellman_ford step 4200 current loss 0.291049, current_train_items 134432.
I0302 19:00:29.126694 22535416901760 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 19:00:29.126799 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:29.143737 22535416901760 run.py:483] Algo bellman_ford step 4201 current loss 0.565053, current_train_items 134464.
I0302 19:00:29.167339 22535416901760 run.py:483] Algo bellman_ford step 4202 current loss 0.618520, current_train_items 134496.
I0302 19:00:29.198034 22535416901760 run.py:483] Algo bellman_ford step 4203 current loss 0.660133, current_train_items 134528.
I0302 19:00:29.231273 22535416901760 run.py:483] Algo bellman_ford step 4204 current loss 0.864461, current_train_items 134560.
I0302 19:00:29.250979 22535416901760 run.py:483] Algo bellman_ford step 4205 current loss 0.351202, current_train_items 134592.
I0302 19:00:29.267081 22535416901760 run.py:483] Algo bellman_ford step 4206 current loss 0.678776, current_train_items 134624.
I0302 19:00:29.290591 22535416901760 run.py:483] Algo bellman_ford step 4207 current loss 0.685199, current_train_items 134656.
I0302 19:00:29.322376 22535416901760 run.py:483] Algo bellman_ford step 4208 current loss 0.742352, current_train_items 134688.
I0302 19:00:29.356290 22535416901760 run.py:483] Algo bellman_ford step 4209 current loss 0.882710, current_train_items 134720.
I0302 19:00:29.375641 22535416901760 run.py:483] Algo bellman_ford step 4210 current loss 0.267307, current_train_items 134752.
I0302 19:00:29.391867 22535416901760 run.py:483] Algo bellman_ford step 4211 current loss 0.527445, current_train_items 134784.
I0302 19:00:29.415633 22535416901760 run.py:483] Algo bellman_ford step 4212 current loss 0.894270, current_train_items 134816.
I0302 19:00:29.446611 22535416901760 run.py:483] Algo bellman_ford step 4213 current loss 1.104895, current_train_items 134848.
I0302 19:00:29.479564 22535416901760 run.py:483] Algo bellman_ford step 4214 current loss 1.199435, current_train_items 134880.
I0302 19:00:29.498960 22535416901760 run.py:483] Algo bellman_ford step 4215 current loss 0.440454, current_train_items 134912.
I0302 19:00:29.515077 22535416901760 run.py:483] Algo bellman_ford step 4216 current loss 0.490941, current_train_items 134944.
I0302 19:00:29.537878 22535416901760 run.py:483] Algo bellman_ford step 4217 current loss 0.576169, current_train_items 134976.
I0302 19:00:29.568044 22535416901760 run.py:483] Algo bellman_ford step 4218 current loss 0.699910, current_train_items 135008.
I0302 19:00:29.601173 22535416901760 run.py:483] Algo bellman_ford step 4219 current loss 0.884592, current_train_items 135040.
I0302 19:00:29.620897 22535416901760 run.py:483] Algo bellman_ford step 4220 current loss 0.318699, current_train_items 135072.
I0302 19:00:29.637137 22535416901760 run.py:483] Algo bellman_ford step 4221 current loss 0.502084, current_train_items 135104.
I0302 19:00:29.660671 22535416901760 run.py:483] Algo bellman_ford step 4222 current loss 0.654109, current_train_items 135136.
I0302 19:00:29.692462 22535416901760 run.py:483] Algo bellman_ford step 4223 current loss 0.677061, current_train_items 135168.
I0302 19:00:29.727878 22535416901760 run.py:483] Algo bellman_ford step 4224 current loss 0.785041, current_train_items 135200.
I0302 19:00:29.747350 22535416901760 run.py:483] Algo bellman_ford step 4225 current loss 0.358474, current_train_items 135232.
I0302 19:00:29.763351 22535416901760 run.py:483] Algo bellman_ford step 4226 current loss 0.448617, current_train_items 135264.
I0302 19:00:29.786277 22535416901760 run.py:483] Algo bellman_ford step 4227 current loss 0.670657, current_train_items 135296.
I0302 19:00:29.815562 22535416901760 run.py:483] Algo bellman_ford step 4228 current loss 0.839547, current_train_items 135328.
I0302 19:00:29.847294 22535416901760 run.py:483] Algo bellman_ford step 4229 current loss 0.770078, current_train_items 135360.
I0302 19:00:29.866626 22535416901760 run.py:483] Algo bellman_ford step 4230 current loss 0.338931, current_train_items 135392.
I0302 19:00:29.882618 22535416901760 run.py:483] Algo bellman_ford step 4231 current loss 0.464170, current_train_items 135424.
I0302 19:00:29.906045 22535416901760 run.py:483] Algo bellman_ford step 4232 current loss 0.742629, current_train_items 135456.
I0302 19:00:29.936930 22535416901760 run.py:483] Algo bellman_ford step 4233 current loss 0.681258, current_train_items 135488.
I0302 19:00:29.969940 22535416901760 run.py:483] Algo bellman_ford step 4234 current loss 0.799952, current_train_items 135520.
I0302 19:00:29.989375 22535416901760 run.py:483] Algo bellman_ford step 4235 current loss 0.305692, current_train_items 135552.
I0302 19:00:30.006091 22535416901760 run.py:483] Algo bellman_ford step 4236 current loss 0.585698, current_train_items 135584.
I0302 19:00:30.029787 22535416901760 run.py:483] Algo bellman_ford step 4237 current loss 0.681830, current_train_items 135616.
I0302 19:00:30.060600 22535416901760 run.py:483] Algo bellman_ford step 4238 current loss 0.714892, current_train_items 135648.
I0302 19:00:30.095488 22535416901760 run.py:483] Algo bellman_ford step 4239 current loss 0.927705, current_train_items 135680.
I0302 19:00:30.115002 22535416901760 run.py:483] Algo bellman_ford step 4240 current loss 0.276881, current_train_items 135712.
I0302 19:00:30.130910 22535416901760 run.py:483] Algo bellman_ford step 4241 current loss 0.442898, current_train_items 135744.
I0302 19:00:30.154108 22535416901760 run.py:483] Algo bellman_ford step 4242 current loss 0.612368, current_train_items 135776.
I0302 19:00:30.185921 22535416901760 run.py:483] Algo bellman_ford step 4243 current loss 0.914500, current_train_items 135808.
I0302 19:00:30.218437 22535416901760 run.py:483] Algo bellman_ford step 4244 current loss 0.936613, current_train_items 135840.
I0302 19:00:30.238058 22535416901760 run.py:483] Algo bellman_ford step 4245 current loss 0.310907, current_train_items 135872.
I0302 19:00:30.253842 22535416901760 run.py:483] Algo bellman_ford step 4246 current loss 0.461549, current_train_items 135904.
I0302 19:00:30.278364 22535416901760 run.py:483] Algo bellman_ford step 4247 current loss 0.645415, current_train_items 135936.
I0302 19:00:30.309601 22535416901760 run.py:483] Algo bellman_ford step 4248 current loss 0.705616, current_train_items 135968.
I0302 19:00:30.344678 22535416901760 run.py:483] Algo bellman_ford step 4249 current loss 0.767560, current_train_items 136000.
I0302 19:00:30.364421 22535416901760 run.py:483] Algo bellman_ford step 4250 current loss 0.426399, current_train_items 136032.
I0302 19:00:30.372385 22535416901760 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 19:00:30.372492 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:00:30.389035 22535416901760 run.py:483] Algo bellman_ford step 4251 current loss 0.455846, current_train_items 136064.
I0302 19:00:30.413239 22535416901760 run.py:483] Algo bellman_ford step 4252 current loss 0.741967, current_train_items 136096.
I0302 19:00:30.444466 22535416901760 run.py:483] Algo bellman_ford step 4253 current loss 0.800435, current_train_items 136128.
I0302 19:00:30.478043 22535416901760 run.py:483] Algo bellman_ford step 4254 current loss 0.871324, current_train_items 136160.
I0302 19:00:30.497891 22535416901760 run.py:483] Algo bellman_ford step 4255 current loss 0.326328, current_train_items 136192.
I0302 19:00:30.513809 22535416901760 run.py:483] Algo bellman_ford step 4256 current loss 0.518345, current_train_items 136224.
I0302 19:00:30.536708 22535416901760 run.py:483] Algo bellman_ford step 4257 current loss 0.542654, current_train_items 136256.
I0302 19:00:30.567471 22535416901760 run.py:483] Algo bellman_ford step 4258 current loss 0.791084, current_train_items 136288.
I0302 19:00:30.601542 22535416901760 run.py:483] Algo bellman_ford step 4259 current loss 0.834187, current_train_items 136320.
I0302 19:00:30.621467 22535416901760 run.py:483] Algo bellman_ford step 4260 current loss 0.342028, current_train_items 136352.
I0302 19:00:30.638083 22535416901760 run.py:483] Algo bellman_ford step 4261 current loss 0.672070, current_train_items 136384.
I0302 19:00:30.660957 22535416901760 run.py:483] Algo bellman_ford step 4262 current loss 0.577928, current_train_items 136416.
I0302 19:00:30.691634 22535416901760 run.py:483] Algo bellman_ford step 4263 current loss 0.868520, current_train_items 136448.
I0302 19:00:30.725055 22535416901760 run.py:483] Algo bellman_ford step 4264 current loss 0.891478, current_train_items 136480.
I0302 19:00:30.744409 22535416901760 run.py:483] Algo bellman_ford step 4265 current loss 0.408718, current_train_items 136512.
I0302 19:00:30.760631 22535416901760 run.py:483] Algo bellman_ford step 4266 current loss 0.717286, current_train_items 136544.
I0302 19:00:30.783695 22535416901760 run.py:483] Algo bellman_ford step 4267 current loss 0.829121, current_train_items 136576.
I0302 19:00:30.813889 22535416901760 run.py:483] Algo bellman_ford step 4268 current loss 0.703695, current_train_items 136608.
I0302 19:00:30.846589 22535416901760 run.py:483] Algo bellman_ford step 4269 current loss 0.764088, current_train_items 136640.
I0302 19:00:30.866800 22535416901760 run.py:483] Algo bellman_ford step 4270 current loss 0.378218, current_train_items 136672.
I0302 19:00:30.882809 22535416901760 run.py:483] Algo bellman_ford step 4271 current loss 0.404749, current_train_items 136704.
I0302 19:00:30.904878 22535416901760 run.py:483] Algo bellman_ford step 4272 current loss 0.587186, current_train_items 136736.
I0302 19:00:30.935084 22535416901760 run.py:483] Algo bellman_ford step 4273 current loss 0.768909, current_train_items 136768.
I0302 19:00:30.967804 22535416901760 run.py:483] Algo bellman_ford step 4274 current loss 0.973863, current_train_items 136800.
I0302 19:00:30.987486 22535416901760 run.py:483] Algo bellman_ford step 4275 current loss 0.311025, current_train_items 136832.
I0302 19:00:31.003590 22535416901760 run.py:483] Algo bellman_ford step 4276 current loss 0.420909, current_train_items 136864.
I0302 19:00:31.026321 22535416901760 run.py:483] Algo bellman_ford step 4277 current loss 0.695025, current_train_items 136896.
I0302 19:00:31.058612 22535416901760 run.py:483] Algo bellman_ford step 4278 current loss 0.811622, current_train_items 136928.
I0302 19:00:31.092458 22535416901760 run.py:483] Algo bellman_ford step 4279 current loss 0.798807, current_train_items 136960.
I0302 19:00:31.111955 22535416901760 run.py:483] Algo bellman_ford step 4280 current loss 0.278909, current_train_items 136992.
I0302 19:00:31.127938 22535416901760 run.py:483] Algo bellman_ford step 4281 current loss 0.506379, current_train_items 137024.
I0302 19:00:31.151554 22535416901760 run.py:483] Algo bellman_ford step 4282 current loss 0.595439, current_train_items 137056.
I0302 19:00:31.181033 22535416901760 run.py:483] Algo bellman_ford step 4283 current loss 0.698100, current_train_items 137088.
I0302 19:00:31.215952 22535416901760 run.py:483] Algo bellman_ford step 4284 current loss 0.858172, current_train_items 137120.
I0302 19:00:31.235624 22535416901760 run.py:483] Algo bellman_ford step 4285 current loss 0.307703, current_train_items 137152.
I0302 19:00:31.251689 22535416901760 run.py:483] Algo bellman_ford step 4286 current loss 0.439486, current_train_items 137184.
I0302 19:00:31.274746 22535416901760 run.py:483] Algo bellman_ford step 4287 current loss 0.651988, current_train_items 137216.
I0302 19:00:31.307013 22535416901760 run.py:483] Algo bellman_ford step 4288 current loss 0.704959, current_train_items 137248.
I0302 19:00:31.340858 22535416901760 run.py:483] Algo bellman_ford step 4289 current loss 1.201174, current_train_items 137280.
I0302 19:00:31.360616 22535416901760 run.py:483] Algo bellman_ford step 4290 current loss 0.337326, current_train_items 137312.
I0302 19:00:31.377237 22535416901760 run.py:483] Algo bellman_ford step 4291 current loss 0.510306, current_train_items 137344.
I0302 19:00:31.401221 22535416901760 run.py:483] Algo bellman_ford step 4292 current loss 0.650520, current_train_items 137376.
I0302 19:00:31.433576 22535416901760 run.py:483] Algo bellman_ford step 4293 current loss 0.858572, current_train_items 137408.
I0302 19:00:31.468642 22535416901760 run.py:483] Algo bellman_ford step 4294 current loss 0.950217, current_train_items 137440.
I0302 19:00:31.488487 22535416901760 run.py:483] Algo bellman_ford step 4295 current loss 0.291092, current_train_items 137472.
I0302 19:00:31.504394 22535416901760 run.py:483] Algo bellman_ford step 4296 current loss 0.561271, current_train_items 137504.
I0302 19:00:31.527000 22535416901760 run.py:483] Algo bellman_ford step 4297 current loss 0.665778, current_train_items 137536.
I0302 19:00:31.558197 22535416901760 run.py:483] Algo bellman_ford step 4298 current loss 0.719163, current_train_items 137568.
I0302 19:00:31.588059 22535416901760 run.py:483] Algo bellman_ford step 4299 current loss 0.732380, current_train_items 137600.
I0302 19:00:31.608141 22535416901760 run.py:483] Algo bellman_ford step 4300 current loss 0.365964, current_train_items 137632.
I0302 19:00:31.615630 22535416901760 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 19:00:31.615736 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:00:31.632271 22535416901760 run.py:483] Algo bellman_ford step 4301 current loss 0.630487, current_train_items 137664.
I0302 19:00:31.655786 22535416901760 run.py:483] Algo bellman_ford step 4302 current loss 0.638648, current_train_items 137696.
I0302 19:00:31.688030 22535416901760 run.py:483] Algo bellman_ford step 4303 current loss 0.980832, current_train_items 137728.
I0302 19:00:31.724025 22535416901760 run.py:483] Algo bellman_ford step 4304 current loss 0.959004, current_train_items 137760.
I0302 19:00:31.744194 22535416901760 run.py:483] Algo bellman_ford step 4305 current loss 0.334303, current_train_items 137792.
I0302 19:00:31.760052 22535416901760 run.py:483] Algo bellman_ford step 4306 current loss 0.488626, current_train_items 137824.
I0302 19:00:31.785219 22535416901760 run.py:483] Algo bellman_ford step 4307 current loss 0.723587, current_train_items 137856.
I0302 19:00:31.817101 22535416901760 run.py:483] Algo bellman_ford step 4308 current loss 0.854980, current_train_items 137888.
I0302 19:00:31.852705 22535416901760 run.py:483] Algo bellman_ford step 4309 current loss 0.859627, current_train_items 137920.
I0302 19:00:31.872444 22535416901760 run.py:483] Algo bellman_ford step 4310 current loss 0.340646, current_train_items 137952.
I0302 19:00:31.888457 22535416901760 run.py:483] Algo bellman_ford step 4311 current loss 0.467947, current_train_items 137984.
I0302 19:00:31.910846 22535416901760 run.py:483] Algo bellman_ford step 4312 current loss 0.604175, current_train_items 138016.
I0302 19:00:31.942174 22535416901760 run.py:483] Algo bellman_ford step 4313 current loss 0.746152, current_train_items 138048.
I0302 19:00:31.975803 22535416901760 run.py:483] Algo bellman_ford step 4314 current loss 0.897419, current_train_items 138080.
I0302 19:00:31.995435 22535416901760 run.py:483] Algo bellman_ford step 4315 current loss 0.376531, current_train_items 138112.
I0302 19:00:32.011274 22535416901760 run.py:483] Algo bellman_ford step 4316 current loss 0.456508, current_train_items 138144.
I0302 19:00:32.035038 22535416901760 run.py:483] Algo bellman_ford step 4317 current loss 0.623220, current_train_items 138176.
I0302 19:00:32.065082 22535416901760 run.py:483] Algo bellman_ford step 4318 current loss 0.716431, current_train_items 138208.
I0302 19:00:32.098322 22535416901760 run.py:483] Algo bellman_ford step 4319 current loss 0.829599, current_train_items 138240.
I0302 19:00:32.117744 22535416901760 run.py:483] Algo bellman_ford step 4320 current loss 0.356038, current_train_items 138272.
I0302 19:00:32.133819 22535416901760 run.py:483] Algo bellman_ford step 4321 current loss 0.509775, current_train_items 138304.
I0302 19:00:32.157354 22535416901760 run.py:483] Algo bellman_ford step 4322 current loss 0.583506, current_train_items 138336.
I0302 19:00:32.186955 22535416901760 run.py:483] Algo bellman_ford step 4323 current loss 0.632234, current_train_items 138368.
I0302 19:00:32.220848 22535416901760 run.py:483] Algo bellman_ford step 4324 current loss 0.864429, current_train_items 138400.
I0302 19:00:32.240248 22535416901760 run.py:483] Algo bellman_ford step 4325 current loss 0.297673, current_train_items 138432.
I0302 19:00:32.256479 22535416901760 run.py:483] Algo bellman_ford step 4326 current loss 0.453304, current_train_items 138464.
I0302 19:00:32.279686 22535416901760 run.py:483] Algo bellman_ford step 4327 current loss 0.581164, current_train_items 138496.
I0302 19:00:32.311696 22535416901760 run.py:483] Algo bellman_ford step 4328 current loss 0.760345, current_train_items 138528.
I0302 19:00:32.343837 22535416901760 run.py:483] Algo bellman_ford step 4329 current loss 0.760527, current_train_items 138560.
I0302 19:00:32.363512 22535416901760 run.py:483] Algo bellman_ford step 4330 current loss 0.323321, current_train_items 138592.
I0302 19:00:32.380064 22535416901760 run.py:483] Algo bellman_ford step 4331 current loss 0.530118, current_train_items 138624.
I0302 19:00:32.403939 22535416901760 run.py:483] Algo bellman_ford step 4332 current loss 0.678027, current_train_items 138656.
I0302 19:00:32.434098 22535416901760 run.py:483] Algo bellman_ford step 4333 current loss 0.762438, current_train_items 138688.
I0302 19:00:32.465682 22535416901760 run.py:483] Algo bellman_ford step 4334 current loss 0.726545, current_train_items 138720.
I0302 19:00:32.485060 22535416901760 run.py:483] Algo bellman_ford step 4335 current loss 0.362186, current_train_items 138752.
I0302 19:00:32.501129 22535416901760 run.py:483] Algo bellman_ford step 4336 current loss 0.580760, current_train_items 138784.
I0302 19:00:32.524801 22535416901760 run.py:483] Algo bellman_ford step 4337 current loss 0.723617, current_train_items 138816.
I0302 19:00:32.556926 22535416901760 run.py:483] Algo bellman_ford step 4338 current loss 0.705867, current_train_items 138848.
I0302 19:00:32.589596 22535416901760 run.py:483] Algo bellman_ford step 4339 current loss 0.775858, current_train_items 138880.
I0302 19:00:32.608983 22535416901760 run.py:483] Algo bellman_ford step 4340 current loss 0.303360, current_train_items 138912.
I0302 19:00:32.624864 22535416901760 run.py:483] Algo bellman_ford step 4341 current loss 0.424942, current_train_items 138944.
I0302 19:00:32.648271 22535416901760 run.py:483] Algo bellman_ford step 4342 current loss 0.560253, current_train_items 138976.
I0302 19:00:32.679003 22535416901760 run.py:483] Algo bellman_ford step 4343 current loss 0.758988, current_train_items 139008.
I0302 19:00:32.713709 22535416901760 run.py:483] Algo bellman_ford step 4344 current loss 0.834476, current_train_items 139040.
I0302 19:00:32.733309 22535416901760 run.py:483] Algo bellman_ford step 4345 current loss 0.311288, current_train_items 139072.
I0302 19:00:32.749782 22535416901760 run.py:483] Algo bellman_ford step 4346 current loss 0.476104, current_train_items 139104.
I0302 19:00:32.773108 22535416901760 run.py:483] Algo bellman_ford step 4347 current loss 0.640545, current_train_items 139136.
I0302 19:00:32.804338 22535416901760 run.py:483] Algo bellman_ford step 4348 current loss 0.636200, current_train_items 139168.
I0302 19:00:32.837104 22535416901760 run.py:483] Algo bellman_ford step 4349 current loss 0.801473, current_train_items 139200.
I0302 19:00:32.856525 22535416901760 run.py:483] Algo bellman_ford step 4350 current loss 0.317107, current_train_items 139232.
I0302 19:00:32.864552 22535416901760 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 19:00:32.864658 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:00:32.881530 22535416901760 run.py:483] Algo bellman_ford step 4351 current loss 0.506882, current_train_items 139264.
I0302 19:00:32.905792 22535416901760 run.py:483] Algo bellman_ford step 4352 current loss 0.718286, current_train_items 139296.
I0302 19:00:32.936302 22535416901760 run.py:483] Algo bellman_ford step 4353 current loss 0.708730, current_train_items 139328.
I0302 19:00:32.970540 22535416901760 run.py:483] Algo bellman_ford step 4354 current loss 0.845412, current_train_items 139360.
I0302 19:00:32.990262 22535416901760 run.py:483] Algo bellman_ford step 4355 current loss 0.307635, current_train_items 139392.
I0302 19:00:33.006148 22535416901760 run.py:483] Algo bellman_ford step 4356 current loss 0.423193, current_train_items 139424.
I0302 19:00:33.029853 22535416901760 run.py:483] Algo bellman_ford step 4357 current loss 0.595882, current_train_items 139456.
I0302 19:00:33.058822 22535416901760 run.py:483] Algo bellman_ford step 4358 current loss 0.605509, current_train_items 139488.
I0302 19:00:33.090872 22535416901760 run.py:483] Algo bellman_ford step 4359 current loss 0.685720, current_train_items 139520.
I0302 19:00:33.110625 22535416901760 run.py:483] Algo bellman_ford step 4360 current loss 0.310089, current_train_items 139552.
I0302 19:00:33.127252 22535416901760 run.py:483] Algo bellman_ford step 4361 current loss 0.558753, current_train_items 139584.
I0302 19:00:33.149369 22535416901760 run.py:483] Algo bellman_ford step 4362 current loss 0.609020, current_train_items 139616.
I0302 19:00:33.180803 22535416901760 run.py:483] Algo bellman_ford step 4363 current loss 0.772240, current_train_items 139648.
I0302 19:00:33.215443 22535416901760 run.py:483] Algo bellman_ford step 4364 current loss 0.816667, current_train_items 139680.
I0302 19:00:33.234582 22535416901760 run.py:483] Algo bellman_ford step 4365 current loss 0.330317, current_train_items 139712.
I0302 19:00:33.250831 22535416901760 run.py:483] Algo bellman_ford step 4366 current loss 0.529592, current_train_items 139744.
I0302 19:00:33.274165 22535416901760 run.py:483] Algo bellman_ford step 4367 current loss 0.560113, current_train_items 139776.
I0302 19:00:33.303854 22535416901760 run.py:483] Algo bellman_ford step 4368 current loss 0.654305, current_train_items 139808.
I0302 19:00:33.337496 22535416901760 run.py:483] Algo bellman_ford step 4369 current loss 0.727253, current_train_items 139840.
I0302 19:00:33.356961 22535416901760 run.py:483] Algo bellman_ford step 4370 current loss 0.346660, current_train_items 139872.
I0302 19:00:33.373053 22535416901760 run.py:483] Algo bellman_ford step 4371 current loss 0.561514, current_train_items 139904.
I0302 19:00:33.395466 22535416901760 run.py:483] Algo bellman_ford step 4372 current loss 0.628066, current_train_items 139936.
I0302 19:00:33.425835 22535416901760 run.py:483] Algo bellman_ford step 4373 current loss 0.701121, current_train_items 139968.
I0302 19:00:33.458853 22535416901760 run.py:483] Algo bellman_ford step 4374 current loss 0.667043, current_train_items 140000.
I0302 19:00:33.478781 22535416901760 run.py:483] Algo bellman_ford step 4375 current loss 0.340718, current_train_items 140032.
I0302 19:00:33.494791 22535416901760 run.py:483] Algo bellman_ford step 4376 current loss 0.426755, current_train_items 140064.
I0302 19:00:33.517459 22535416901760 run.py:483] Algo bellman_ford step 4377 current loss 0.590009, current_train_items 140096.
I0302 19:00:33.547181 22535416901760 run.py:483] Algo bellman_ford step 4378 current loss 0.656920, current_train_items 140128.
I0302 19:00:33.580822 22535416901760 run.py:483] Algo bellman_ford step 4379 current loss 0.797894, current_train_items 140160.
I0302 19:00:33.600029 22535416901760 run.py:483] Algo bellman_ford step 4380 current loss 0.315498, current_train_items 140192.
I0302 19:00:33.616075 22535416901760 run.py:483] Algo bellman_ford step 4381 current loss 0.428568, current_train_items 140224.
I0302 19:00:33.639340 22535416901760 run.py:483] Algo bellman_ford step 4382 current loss 0.659857, current_train_items 140256.
I0302 19:00:33.669862 22535416901760 run.py:483] Algo bellman_ford step 4383 current loss 0.660867, current_train_items 140288.
I0302 19:00:33.703367 22535416901760 run.py:483] Algo bellman_ford step 4384 current loss 0.776500, current_train_items 140320.
I0302 19:00:33.723365 22535416901760 run.py:483] Algo bellman_ford step 4385 current loss 0.303769, current_train_items 140352.
I0302 19:00:33.739243 22535416901760 run.py:483] Algo bellman_ford step 4386 current loss 0.423217, current_train_items 140384.
I0302 19:00:33.762482 22535416901760 run.py:483] Algo bellman_ford step 4387 current loss 0.618055, current_train_items 140416.
I0302 19:00:33.793136 22535416901760 run.py:483] Algo bellman_ford step 4388 current loss 0.679413, current_train_items 140448.
I0302 19:00:33.824953 22535416901760 run.py:483] Algo bellman_ford step 4389 current loss 0.709698, current_train_items 140480.
I0302 19:00:33.844710 22535416901760 run.py:483] Algo bellman_ford step 4390 current loss 0.246300, current_train_items 140512.
I0302 19:00:33.860664 22535416901760 run.py:483] Algo bellman_ford step 4391 current loss 0.466581, current_train_items 140544.
I0302 19:00:33.883479 22535416901760 run.py:483] Algo bellman_ford step 4392 current loss 0.651907, current_train_items 140576.
I0302 19:00:33.914151 22535416901760 run.py:483] Algo bellman_ford step 4393 current loss 0.702571, current_train_items 140608.
I0302 19:00:33.947762 22535416901760 run.py:483] Algo bellman_ford step 4394 current loss 0.807030, current_train_items 140640.
I0302 19:00:33.967072 22535416901760 run.py:483] Algo bellman_ford step 4395 current loss 0.297866, current_train_items 140672.
I0302 19:00:33.983526 22535416901760 run.py:483] Algo bellman_ford step 4396 current loss 0.612517, current_train_items 140704.
I0302 19:00:34.006974 22535416901760 run.py:483] Algo bellman_ford step 4397 current loss 0.646199, current_train_items 140736.
I0302 19:00:34.037618 22535416901760 run.py:483] Algo bellman_ford step 4398 current loss 0.734545, current_train_items 140768.
I0302 19:00:34.070447 22535416901760 run.py:483] Algo bellman_ford step 4399 current loss 0.775855, current_train_items 140800.
I0302 19:00:34.090028 22535416901760 run.py:483] Algo bellman_ford step 4400 current loss 0.338621, current_train_items 140832.
I0302 19:00:34.097715 22535416901760 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 19:00:34.097822 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:34.114773 22535416901760 run.py:483] Algo bellman_ford step 4401 current loss 0.446135, current_train_items 140864.
I0302 19:00:34.138179 22535416901760 run.py:483] Algo bellman_ford step 4402 current loss 0.648616, current_train_items 140896.
I0302 19:00:34.168658 22535416901760 run.py:483] Algo bellman_ford step 4403 current loss 0.753790, current_train_items 140928.
I0302 19:00:34.206696 22535416901760 run.py:483] Algo bellman_ford step 4404 current loss 1.302440, current_train_items 140960.
I0302 19:00:34.227030 22535416901760 run.py:483] Algo bellman_ford step 4405 current loss 0.304902, current_train_items 140992.
I0302 19:00:34.243125 22535416901760 run.py:483] Algo bellman_ford step 4406 current loss 0.501612, current_train_items 141024.
I0302 19:00:34.266332 22535416901760 run.py:483] Algo bellman_ford step 4407 current loss 0.616003, current_train_items 141056.
I0302 19:00:34.297073 22535416901760 run.py:483] Algo bellman_ford step 4408 current loss 0.681677, current_train_items 141088.
I0302 19:00:34.332548 22535416901760 run.py:483] Algo bellman_ford step 4409 current loss 0.801713, current_train_items 141120.
I0302 19:00:34.352065 22535416901760 run.py:483] Algo bellman_ford step 4410 current loss 0.334894, current_train_items 141152.
I0302 19:00:34.368239 22535416901760 run.py:483] Algo bellman_ford step 4411 current loss 0.528965, current_train_items 141184.
I0302 19:00:34.392050 22535416901760 run.py:483] Algo bellman_ford step 4412 current loss 0.636737, current_train_items 141216.
I0302 19:00:34.424167 22535416901760 run.py:483] Algo bellman_ford step 4413 current loss 0.857078, current_train_items 141248.
I0302 19:00:34.456376 22535416901760 run.py:483] Algo bellman_ford step 4414 current loss 0.735984, current_train_items 141280.
I0302 19:00:34.476171 22535416901760 run.py:483] Algo bellman_ford step 4415 current loss 0.309712, current_train_items 141312.
I0302 19:00:34.492761 22535416901760 run.py:483] Algo bellman_ford step 4416 current loss 0.534609, current_train_items 141344.
I0302 19:00:34.516032 22535416901760 run.py:483] Algo bellman_ford step 4417 current loss 0.641417, current_train_items 141376.
I0302 19:00:34.546478 22535416901760 run.py:483] Algo bellman_ford step 4418 current loss 0.700079, current_train_items 141408.
I0302 19:00:34.579550 22535416901760 run.py:483] Algo bellman_ford step 4419 current loss 0.785794, current_train_items 141440.
I0302 19:00:34.599022 22535416901760 run.py:483] Algo bellman_ford step 4420 current loss 0.330920, current_train_items 141472.
I0302 19:00:34.614870 22535416901760 run.py:483] Algo bellman_ford step 4421 current loss 0.533804, current_train_items 141504.
I0302 19:00:34.638346 22535416901760 run.py:483] Algo bellman_ford step 4422 current loss 0.612310, current_train_items 141536.
I0302 19:00:34.668903 22535416901760 run.py:483] Algo bellman_ford step 4423 current loss 0.665867, current_train_items 141568.
I0302 19:00:34.703351 22535416901760 run.py:483] Algo bellman_ford step 4424 current loss 0.795329, current_train_items 141600.
I0302 19:00:34.723085 22535416901760 run.py:483] Algo bellman_ford step 4425 current loss 0.305276, current_train_items 141632.
I0302 19:00:34.738374 22535416901760 run.py:483] Algo bellman_ford step 4426 current loss 0.442659, current_train_items 141664.
I0302 19:00:34.762142 22535416901760 run.py:483] Algo bellman_ford step 4427 current loss 0.663934, current_train_items 141696.
I0302 19:00:34.793355 22535416901760 run.py:483] Algo bellman_ford step 4428 current loss 0.746263, current_train_items 141728.
I0302 19:00:34.826640 22535416901760 run.py:483] Algo bellman_ford step 4429 current loss 0.876306, current_train_items 141760.
I0302 19:00:34.846032 22535416901760 run.py:483] Algo bellman_ford step 4430 current loss 0.249038, current_train_items 141792.
I0302 19:00:34.861708 22535416901760 run.py:483] Algo bellman_ford step 4431 current loss 0.404637, current_train_items 141824.
I0302 19:00:34.885098 22535416901760 run.py:483] Algo bellman_ford step 4432 current loss 0.582405, current_train_items 141856.
I0302 19:00:34.915102 22535416901760 run.py:483] Algo bellman_ford step 4433 current loss 0.698451, current_train_items 141888.
I0302 19:00:34.948229 22535416901760 run.py:483] Algo bellman_ford step 4434 current loss 0.710078, current_train_items 141920.
I0302 19:00:34.967713 22535416901760 run.py:483] Algo bellman_ford step 4435 current loss 0.351853, current_train_items 141952.
I0302 19:00:34.983986 22535416901760 run.py:483] Algo bellman_ford step 4436 current loss 0.490082, current_train_items 141984.
I0302 19:00:35.007770 22535416901760 run.py:483] Algo bellman_ford step 4437 current loss 0.710221, current_train_items 142016.
I0302 19:00:35.037418 22535416901760 run.py:483] Algo bellman_ford step 4438 current loss 0.584035, current_train_items 142048.
I0302 19:00:35.071761 22535416901760 run.py:483] Algo bellman_ford step 4439 current loss 0.748279, current_train_items 142080.
I0302 19:00:35.091085 22535416901760 run.py:483] Algo bellman_ford step 4440 current loss 0.392038, current_train_items 142112.
I0302 19:00:35.107561 22535416901760 run.py:483] Algo bellman_ford step 4441 current loss 0.571715, current_train_items 142144.
I0302 19:00:35.130494 22535416901760 run.py:483] Algo bellman_ford step 4442 current loss 0.709173, current_train_items 142176.
I0302 19:00:35.160744 22535416901760 run.py:483] Algo bellman_ford step 4443 current loss 0.809872, current_train_items 142208.
I0302 19:00:35.195565 22535416901760 run.py:483] Algo bellman_ford step 4444 current loss 0.810494, current_train_items 142240.
I0302 19:00:35.215123 22535416901760 run.py:483] Algo bellman_ford step 4445 current loss 0.341236, current_train_items 142272.
I0302 19:00:35.230991 22535416901760 run.py:483] Algo bellman_ford step 4446 current loss 0.595266, current_train_items 142304.
I0302 19:00:35.254473 22535416901760 run.py:483] Algo bellman_ford step 4447 current loss 0.673566, current_train_items 142336.
I0302 19:00:35.285874 22535416901760 run.py:483] Algo bellman_ford step 4448 current loss 0.838972, current_train_items 142368.
I0302 19:00:35.319113 22535416901760 run.py:483] Algo bellman_ford step 4449 current loss 0.846529, current_train_items 142400.
I0302 19:00:35.338877 22535416901760 run.py:483] Algo bellman_ford step 4450 current loss 0.328930, current_train_items 142432.
I0302 19:00:35.346925 22535416901760 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.9375, 'score': 0.9375, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 19:00:35.347032 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:00:35.363023 22535416901760 run.py:483] Algo bellman_ford step 4451 current loss 0.445410, current_train_items 142464.
I0302 19:00:35.386925 22535416901760 run.py:483] Algo bellman_ford step 4452 current loss 0.522618, current_train_items 142496.
I0302 19:00:35.417111 22535416901760 run.py:483] Algo bellman_ford step 4453 current loss 0.682719, current_train_items 142528.
I0302 19:00:35.449446 22535416901760 run.py:483] Algo bellman_ford step 4454 current loss 0.783074, current_train_items 142560.
I0302 19:00:35.469188 22535416901760 run.py:483] Algo bellman_ford step 4455 current loss 0.269377, current_train_items 142592.
I0302 19:00:35.484890 22535416901760 run.py:483] Algo bellman_ford step 4456 current loss 0.474348, current_train_items 142624.
I0302 19:00:35.508258 22535416901760 run.py:483] Algo bellman_ford step 4457 current loss 0.590494, current_train_items 142656.
I0302 19:00:35.537356 22535416901760 run.py:483] Algo bellman_ford step 4458 current loss 0.695223, current_train_items 142688.
I0302 19:00:35.573582 22535416901760 run.py:483] Algo bellman_ford step 4459 current loss 0.838864, current_train_items 142720.
I0302 19:00:35.593442 22535416901760 run.py:483] Algo bellman_ford step 4460 current loss 0.397397, current_train_items 142752.
I0302 19:00:35.609663 22535416901760 run.py:483] Algo bellman_ford step 4461 current loss 0.475798, current_train_items 142784.
I0302 19:00:35.633908 22535416901760 run.py:483] Algo bellman_ford step 4462 current loss 0.684118, current_train_items 142816.
I0302 19:00:35.663057 22535416901760 run.py:483] Algo bellman_ford step 4463 current loss 0.571176, current_train_items 142848.
I0302 19:00:35.696424 22535416901760 run.py:483] Algo bellman_ford step 4464 current loss 0.880039, current_train_items 142880.
I0302 19:00:35.715695 22535416901760 run.py:483] Algo bellman_ford step 4465 current loss 0.283076, current_train_items 142912.
I0302 19:00:35.731925 22535416901760 run.py:483] Algo bellman_ford step 4466 current loss 0.574610, current_train_items 142944.
I0302 19:00:35.755063 22535416901760 run.py:483] Algo bellman_ford step 4467 current loss 0.745791, current_train_items 142976.
I0302 19:00:35.787398 22535416901760 run.py:483] Algo bellman_ford step 4468 current loss 0.684641, current_train_items 143008.
I0302 19:00:35.821371 22535416901760 run.py:483] Algo bellman_ford step 4469 current loss 0.871478, current_train_items 143040.
I0302 19:00:35.841283 22535416901760 run.py:483] Algo bellman_ford step 4470 current loss 0.294887, current_train_items 143072.
I0302 19:00:35.857239 22535416901760 run.py:483] Algo bellman_ford step 4471 current loss 0.585020, current_train_items 143104.
I0302 19:00:35.880072 22535416901760 run.py:483] Algo bellman_ford step 4472 current loss 0.590307, current_train_items 143136.
I0302 19:00:35.910552 22535416901760 run.py:483] Algo bellman_ford step 4473 current loss 0.658323, current_train_items 143168.
I0302 19:00:35.946556 22535416901760 run.py:483] Algo bellman_ford step 4474 current loss 0.873895, current_train_items 143200.
I0302 19:00:35.966606 22535416901760 run.py:483] Algo bellman_ford step 4475 current loss 0.309912, current_train_items 143232.
I0302 19:00:35.982645 22535416901760 run.py:483] Algo bellman_ford step 4476 current loss 0.479902, current_train_items 143264.
I0302 19:00:36.005321 22535416901760 run.py:483] Algo bellman_ford step 4477 current loss 0.628606, current_train_items 143296.
I0302 19:00:36.035730 22535416901760 run.py:483] Algo bellman_ford step 4478 current loss 0.671720, current_train_items 143328.
I0302 19:00:36.070606 22535416901760 run.py:483] Algo bellman_ford step 4479 current loss 0.879885, current_train_items 143360.
I0302 19:00:36.089869 22535416901760 run.py:483] Algo bellman_ford step 4480 current loss 0.259259, current_train_items 143392.
I0302 19:00:36.105992 22535416901760 run.py:483] Algo bellman_ford step 4481 current loss 0.485942, current_train_items 143424.
I0302 19:00:36.129365 22535416901760 run.py:483] Algo bellman_ford step 4482 current loss 0.669381, current_train_items 143456.
I0302 19:00:36.160223 22535416901760 run.py:483] Algo bellman_ford step 4483 current loss 0.766766, current_train_items 143488.
I0302 19:00:36.191030 22535416901760 run.py:483] Algo bellman_ford step 4484 current loss 0.709534, current_train_items 143520.
I0302 19:00:36.211171 22535416901760 run.py:483] Algo bellman_ford step 4485 current loss 0.325170, current_train_items 143552.
I0302 19:00:36.227530 22535416901760 run.py:483] Algo bellman_ford step 4486 current loss 0.431617, current_train_items 143584.
I0302 19:00:36.251637 22535416901760 run.py:483] Algo bellman_ford step 4487 current loss 0.747380, current_train_items 143616.
I0302 19:00:36.282571 22535416901760 run.py:483] Algo bellman_ford step 4488 current loss 0.743328, current_train_items 143648.
I0302 19:00:36.315044 22535416901760 run.py:483] Algo bellman_ford step 4489 current loss 0.920404, current_train_items 143680.
I0302 19:00:36.334883 22535416901760 run.py:483] Algo bellman_ford step 4490 current loss 0.349422, current_train_items 143712.
I0302 19:00:36.350656 22535416901760 run.py:483] Algo bellman_ford step 4491 current loss 0.453100, current_train_items 143744.
I0302 19:00:36.374345 22535416901760 run.py:483] Algo bellman_ford step 4492 current loss 0.568800, current_train_items 143776.
I0302 19:00:36.405147 22535416901760 run.py:483] Algo bellman_ford step 4493 current loss 0.732936, current_train_items 143808.
I0302 19:00:36.439049 22535416901760 run.py:483] Algo bellman_ford step 4494 current loss 0.897726, current_train_items 143840.
I0302 19:00:36.458556 22535416901760 run.py:483] Algo bellman_ford step 4495 current loss 0.440134, current_train_items 143872.
I0302 19:00:36.475248 22535416901760 run.py:483] Algo bellman_ford step 4496 current loss 0.520178, current_train_items 143904.
I0302 19:00:36.498558 22535416901760 run.py:483] Algo bellman_ford step 4497 current loss 0.776074, current_train_items 143936.
I0302 19:00:36.528198 22535416901760 run.py:483] Algo bellman_ford step 4498 current loss 0.663855, current_train_items 143968.
I0302 19:00:36.559200 22535416901760 run.py:483] Algo bellman_ford step 4499 current loss 0.696658, current_train_items 144000.
I0302 19:00:36.578661 22535416901760 run.py:483] Algo bellman_ford step 4500 current loss 0.377122, current_train_items 144032.
I0302 19:00:36.585862 22535416901760 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 19:00:36.585967 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:00:36.602527 22535416901760 run.py:483] Algo bellman_ford step 4501 current loss 0.545491, current_train_items 144064.
I0302 19:00:36.627004 22535416901760 run.py:483] Algo bellman_ford step 4502 current loss 0.813018, current_train_items 144096.
I0302 19:00:36.658326 22535416901760 run.py:483] Algo bellman_ford step 4503 current loss 0.757306, current_train_items 144128.
I0302 19:00:36.692814 22535416901760 run.py:483] Algo bellman_ford step 4504 current loss 0.713497, current_train_items 144160.
I0302 19:00:36.712891 22535416901760 run.py:483] Algo bellman_ford step 4505 current loss 0.287058, current_train_items 144192.
I0302 19:00:36.728954 22535416901760 run.py:483] Algo bellman_ford step 4506 current loss 0.579582, current_train_items 144224.
I0302 19:00:36.752012 22535416901760 run.py:483] Algo bellman_ford step 4507 current loss 0.565506, current_train_items 144256.
I0302 19:00:36.782165 22535416901760 run.py:483] Algo bellman_ford step 4508 current loss 0.741659, current_train_items 144288.
I0302 19:00:36.814710 22535416901760 run.py:483] Algo bellman_ford step 4509 current loss 0.761526, current_train_items 144320.
I0302 19:00:36.834573 22535416901760 run.py:483] Algo bellman_ford step 4510 current loss 0.293413, current_train_items 144352.
I0302 19:00:36.850883 22535416901760 run.py:483] Algo bellman_ford step 4511 current loss 0.457513, current_train_items 144384.
I0302 19:00:36.874661 22535416901760 run.py:483] Algo bellman_ford step 4512 current loss 0.597061, current_train_items 144416.
I0302 19:00:36.905084 22535416901760 run.py:483] Algo bellman_ford step 4513 current loss 0.636841, current_train_items 144448.
I0302 19:00:36.937665 22535416901760 run.py:483] Algo bellman_ford step 4514 current loss 0.736451, current_train_items 144480.
I0302 19:00:36.956825 22535416901760 run.py:483] Algo bellman_ford step 4515 current loss 0.398347, current_train_items 144512.
I0302 19:00:36.973140 22535416901760 run.py:483] Algo bellman_ford step 4516 current loss 0.475255, current_train_items 144544.
I0302 19:00:36.995483 22535416901760 run.py:483] Algo bellman_ford step 4517 current loss 0.538539, current_train_items 144576.
I0302 19:00:37.026944 22535416901760 run.py:483] Algo bellman_ford step 4518 current loss 0.687315, current_train_items 144608.
I0302 19:00:37.063401 22535416901760 run.py:483] Algo bellman_ford step 4519 current loss 0.916408, current_train_items 144640.
I0302 19:00:37.082837 22535416901760 run.py:483] Algo bellman_ford step 4520 current loss 0.300258, current_train_items 144672.
I0302 19:00:37.099211 22535416901760 run.py:483] Algo bellman_ford step 4521 current loss 0.475852, current_train_items 144704.
I0302 19:00:37.122661 22535416901760 run.py:483] Algo bellman_ford step 4522 current loss 0.595333, current_train_items 144736.
I0302 19:00:37.155125 22535416901760 run.py:483] Algo bellman_ford step 4523 current loss 0.689806, current_train_items 144768.
I0302 19:00:37.188135 22535416901760 run.py:483] Algo bellman_ford step 4524 current loss 0.689686, current_train_items 144800.
I0302 19:00:37.207523 22535416901760 run.py:483] Algo bellman_ford step 4525 current loss 0.288602, current_train_items 144832.
I0302 19:00:37.223637 22535416901760 run.py:483] Algo bellman_ford step 4526 current loss 0.448427, current_train_items 144864.
I0302 19:00:37.247772 22535416901760 run.py:483] Algo bellman_ford step 4527 current loss 0.646650, current_train_items 144896.
I0302 19:00:37.278707 22535416901760 run.py:483] Algo bellman_ford step 4528 current loss 0.604233, current_train_items 144928.
I0302 19:00:37.312865 22535416901760 run.py:483] Algo bellman_ford step 4529 current loss 0.739224, current_train_items 144960.
I0302 19:00:37.332298 22535416901760 run.py:483] Algo bellman_ford step 4530 current loss 0.353097, current_train_items 144992.
I0302 19:00:37.348079 22535416901760 run.py:483] Algo bellman_ford step 4531 current loss 0.425818, current_train_items 145024.
I0302 19:00:37.370423 22535416901760 run.py:483] Algo bellman_ford step 4532 current loss 0.519502, current_train_items 145056.
I0302 19:00:37.399461 22535416901760 run.py:483] Algo bellman_ford step 4533 current loss 0.554721, current_train_items 145088.
I0302 19:00:37.434224 22535416901760 run.py:483] Algo bellman_ford step 4534 current loss 0.722342, current_train_items 145120.
I0302 19:00:37.453977 22535416901760 run.py:483] Algo bellman_ford step 4535 current loss 0.259950, current_train_items 145152.
I0302 19:00:37.470078 22535416901760 run.py:483] Algo bellman_ford step 4536 current loss 0.428705, current_train_items 145184.
I0302 19:00:37.493929 22535416901760 run.py:483] Algo bellman_ford step 4537 current loss 0.612419, current_train_items 145216.
I0302 19:00:37.524253 22535416901760 run.py:483] Algo bellman_ford step 4538 current loss 0.599227, current_train_items 145248.
I0302 19:00:37.559451 22535416901760 run.py:483] Algo bellman_ford step 4539 current loss 0.948061, current_train_items 145280.
I0302 19:00:37.578904 22535416901760 run.py:483] Algo bellman_ford step 4540 current loss 0.266392, current_train_items 145312.
I0302 19:00:37.594962 22535416901760 run.py:483] Algo bellman_ford step 4541 current loss 0.494958, current_train_items 145344.
I0302 19:00:37.617580 22535416901760 run.py:483] Algo bellman_ford step 4542 current loss 0.597138, current_train_items 145376.
I0302 19:00:37.647952 22535416901760 run.py:483] Algo bellman_ford step 4543 current loss 0.634520, current_train_items 145408.
I0302 19:00:37.681963 22535416901760 run.py:483] Algo bellman_ford step 4544 current loss 0.995697, current_train_items 145440.
I0302 19:00:37.701210 22535416901760 run.py:483] Algo bellman_ford step 4545 current loss 0.360968, current_train_items 145472.
I0302 19:00:37.717247 22535416901760 run.py:483] Algo bellman_ford step 4546 current loss 0.444676, current_train_items 145504.
I0302 19:00:37.739990 22535416901760 run.py:483] Algo bellman_ford step 4547 current loss 0.568781, current_train_items 145536.
I0302 19:00:37.769704 22535416901760 run.py:483] Algo bellman_ford step 4548 current loss 0.638664, current_train_items 145568.
I0302 19:00:37.804329 22535416901760 run.py:483] Algo bellman_ford step 4549 current loss 0.867588, current_train_items 145600.
I0302 19:00:37.823792 22535416901760 run.py:483] Algo bellman_ford step 4550 current loss 0.284179, current_train_items 145632.
I0302 19:00:37.831507 22535416901760 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.8896484375, 'score': 0.8896484375, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 19:00:37.831614 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.890, val scores are: bellman_ford: 0.890
I0302 19:00:37.848726 22535416901760 run.py:483] Algo bellman_ford step 4551 current loss 0.556405, current_train_items 145664.
I0302 19:00:37.872630 22535416901760 run.py:483] Algo bellman_ford step 4552 current loss 0.671181, current_train_items 145696.
I0302 19:00:37.904667 22535416901760 run.py:483] Algo bellman_ford step 4553 current loss 0.814799, current_train_items 145728.
I0302 19:00:37.938722 22535416901760 run.py:483] Algo bellman_ford step 4554 current loss 0.800167, current_train_items 145760.
I0302 19:00:37.958838 22535416901760 run.py:483] Algo bellman_ford step 4555 current loss 0.327648, current_train_items 145792.
I0302 19:00:37.974404 22535416901760 run.py:483] Algo bellman_ford step 4556 current loss 0.472507, current_train_items 145824.
I0302 19:00:37.996964 22535416901760 run.py:483] Algo bellman_ford step 4557 current loss 0.659124, current_train_items 145856.
I0302 19:00:38.028675 22535416901760 run.py:483] Algo bellman_ford step 4558 current loss 0.707437, current_train_items 145888.
I0302 19:00:38.063361 22535416901760 run.py:483] Algo bellman_ford step 4559 current loss 0.965621, current_train_items 145920.
I0302 19:00:38.083076 22535416901760 run.py:483] Algo bellman_ford step 4560 current loss 0.263149, current_train_items 145952.
I0302 19:00:38.099570 22535416901760 run.py:483] Algo bellman_ford step 4561 current loss 0.494650, current_train_items 145984.
I0302 19:00:38.122322 22535416901760 run.py:483] Algo bellman_ford step 4562 current loss 0.621803, current_train_items 146016.
I0302 19:00:38.152285 22535416901760 run.py:483] Algo bellman_ford step 4563 current loss 0.687775, current_train_items 146048.
I0302 19:00:38.186773 22535416901760 run.py:483] Algo bellman_ford step 4564 current loss 0.872856, current_train_items 146080.
I0302 19:00:38.206381 22535416901760 run.py:483] Algo bellman_ford step 4565 current loss 0.318073, current_train_items 146112.
I0302 19:00:38.222504 22535416901760 run.py:483] Algo bellman_ford step 4566 current loss 0.548997, current_train_items 146144.
I0302 19:00:38.246263 22535416901760 run.py:483] Algo bellman_ford step 4567 current loss 0.582747, current_train_items 146176.
I0302 19:00:38.277708 22535416901760 run.py:483] Algo bellman_ford step 4568 current loss 0.746757, current_train_items 146208.
I0302 19:00:38.310508 22535416901760 run.py:483] Algo bellman_ford step 4569 current loss 0.765224, current_train_items 146240.
I0302 19:00:38.330070 22535416901760 run.py:483] Algo bellman_ford step 4570 current loss 0.284655, current_train_items 146272.
I0302 19:00:38.346078 22535416901760 run.py:483] Algo bellman_ford step 4571 current loss 0.467502, current_train_items 146304.
I0302 19:00:38.369413 22535416901760 run.py:483] Algo bellman_ford step 4572 current loss 0.666077, current_train_items 146336.
I0302 19:00:38.399126 22535416901760 run.py:483] Algo bellman_ford step 4573 current loss 0.670763, current_train_items 146368.
I0302 19:00:38.431631 22535416901760 run.py:483] Algo bellman_ford step 4574 current loss 0.773984, current_train_items 146400.
I0302 19:00:38.451093 22535416901760 run.py:483] Algo bellman_ford step 4575 current loss 0.323284, current_train_items 146432.
I0302 19:00:38.467206 22535416901760 run.py:483] Algo bellman_ford step 4576 current loss 0.512318, current_train_items 146464.
I0302 19:00:38.490128 22535416901760 run.py:483] Algo bellman_ford step 4577 current loss 0.591846, current_train_items 146496.
I0302 19:00:38.521131 22535416901760 run.py:483] Algo bellman_ford step 4578 current loss 0.765796, current_train_items 146528.
I0302 19:00:38.554849 22535416901760 run.py:483] Algo bellman_ford step 4579 current loss 0.831526, current_train_items 146560.
I0302 19:00:38.574084 22535416901760 run.py:483] Algo bellman_ford step 4580 current loss 0.309444, current_train_items 146592.
I0302 19:00:38.590598 22535416901760 run.py:483] Algo bellman_ford step 4581 current loss 0.592203, current_train_items 146624.
I0302 19:00:38.613436 22535416901760 run.py:483] Algo bellman_ford step 4582 current loss 0.563385, current_train_items 146656.
I0302 19:00:38.644307 22535416901760 run.py:483] Algo bellman_ford step 4583 current loss 0.692204, current_train_items 146688.
I0302 19:00:38.678057 22535416901760 run.py:483] Algo bellman_ford step 4584 current loss 0.887959, current_train_items 146720.
I0302 19:00:38.697860 22535416901760 run.py:483] Algo bellman_ford step 4585 current loss 0.317168, current_train_items 146752.
I0302 19:00:38.714288 22535416901760 run.py:483] Algo bellman_ford step 4586 current loss 0.502599, current_train_items 146784.
I0302 19:00:38.736599 22535416901760 run.py:483] Algo bellman_ford step 4587 current loss 0.547556, current_train_items 146816.
I0302 19:00:38.767560 22535416901760 run.py:483] Algo bellman_ford step 4588 current loss 0.635534, current_train_items 146848.
I0302 19:00:38.800122 22535416901760 run.py:483] Algo bellman_ford step 4589 current loss 0.784456, current_train_items 146880.
I0302 19:00:38.819885 22535416901760 run.py:483] Algo bellman_ford step 4590 current loss 0.311481, current_train_items 146912.
I0302 19:00:38.836037 22535416901760 run.py:483] Algo bellman_ford step 4591 current loss 0.596574, current_train_items 146944.
I0302 19:00:38.858623 22535416901760 run.py:483] Algo bellman_ford step 4592 current loss 0.648182, current_train_items 146976.
I0302 19:00:38.890315 22535416901760 run.py:483] Algo bellman_ford step 4593 current loss 0.693782, current_train_items 147008.
I0302 19:00:38.924359 22535416901760 run.py:483] Algo bellman_ford step 4594 current loss 0.837632, current_train_items 147040.
I0302 19:00:38.943753 22535416901760 run.py:483] Algo bellman_ford step 4595 current loss 0.339991, current_train_items 147072.
I0302 19:00:38.959727 22535416901760 run.py:483] Algo bellman_ford step 4596 current loss 0.470566, current_train_items 147104.
I0302 19:00:38.984975 22535416901760 run.py:483] Algo bellman_ford step 4597 current loss 0.751671, current_train_items 147136.
I0302 19:00:39.015508 22535416901760 run.py:483] Algo bellman_ford step 4598 current loss 0.730359, current_train_items 147168.
I0302 19:00:39.046997 22535416901760 run.py:483] Algo bellman_ford step 4599 current loss 0.731716, current_train_items 147200.
I0302 19:00:39.066877 22535416901760 run.py:483] Algo bellman_ford step 4600 current loss 0.348514, current_train_items 147232.
I0302 19:00:39.074370 22535416901760 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.8984375, 'score': 0.8984375, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 19:00:39.074477 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.898, val scores are: bellman_ford: 0.898
I0302 19:00:39.091731 22535416901760 run.py:483] Algo bellman_ford step 4601 current loss 0.537271, current_train_items 147264.
I0302 19:00:39.116691 22535416901760 run.py:483] Algo bellman_ford step 4602 current loss 0.624001, current_train_items 147296.
I0302 19:00:39.147657 22535416901760 run.py:483] Algo bellman_ford step 4603 current loss 0.755962, current_train_items 147328.
I0302 19:00:39.181852 22535416901760 run.py:483] Algo bellman_ford step 4604 current loss 0.844178, current_train_items 147360.
I0302 19:00:39.201653 22535416901760 run.py:483] Algo bellman_ford step 4605 current loss 0.317136, current_train_items 147392.
I0302 19:00:39.217734 22535416901760 run.py:483] Algo bellman_ford step 4606 current loss 0.521623, current_train_items 147424.
I0302 19:00:39.240946 22535416901760 run.py:483] Algo bellman_ford step 4607 current loss 0.550520, current_train_items 147456.
I0302 19:00:39.272687 22535416901760 run.py:483] Algo bellman_ford step 4608 current loss 0.610135, current_train_items 147488.
I0302 19:00:39.304560 22535416901760 run.py:483] Algo bellman_ford step 4609 current loss 0.672876, current_train_items 147520.
I0302 19:00:39.323849 22535416901760 run.py:483] Algo bellman_ford step 4610 current loss 0.277906, current_train_items 147552.
I0302 19:00:39.339842 22535416901760 run.py:483] Algo bellman_ford step 4611 current loss 0.518859, current_train_items 147584.
I0302 19:00:39.363414 22535416901760 run.py:483] Algo bellman_ford step 4612 current loss 0.571788, current_train_items 147616.
I0302 19:00:39.394597 22535416901760 run.py:483] Algo bellman_ford step 4613 current loss 0.741684, current_train_items 147648.
I0302 19:00:39.428806 22535416901760 run.py:483] Algo bellman_ford step 4614 current loss 0.878406, current_train_items 147680.
I0302 19:00:39.448381 22535416901760 run.py:483] Algo bellman_ford step 4615 current loss 0.370460, current_train_items 147712.
I0302 19:00:39.464596 22535416901760 run.py:483] Algo bellman_ford step 4616 current loss 0.542877, current_train_items 147744.
I0302 19:00:39.488218 22535416901760 run.py:483] Algo bellman_ford step 4617 current loss 0.545951, current_train_items 147776.
I0302 19:00:39.518333 22535416901760 run.py:483] Algo bellman_ford step 4618 current loss 0.567926, current_train_items 147808.
I0302 19:00:39.551552 22535416901760 run.py:483] Algo bellman_ford step 4619 current loss 0.729608, current_train_items 147840.
I0302 19:00:39.570774 22535416901760 run.py:483] Algo bellman_ford step 4620 current loss 0.305603, current_train_items 147872.
I0302 19:00:39.586602 22535416901760 run.py:483] Algo bellman_ford step 4621 current loss 0.405333, current_train_items 147904.
I0302 19:00:39.609564 22535416901760 run.py:483] Algo bellman_ford step 4622 current loss 0.566960, current_train_items 147936.
I0302 19:00:39.640560 22535416901760 run.py:483] Algo bellman_ford step 4623 current loss 0.631566, current_train_items 147968.
I0302 19:00:39.673224 22535416901760 run.py:483] Algo bellman_ford step 4624 current loss 1.068753, current_train_items 148000.
I0302 19:00:39.692929 22535416901760 run.py:483] Algo bellman_ford step 4625 current loss 0.300237, current_train_items 148032.
I0302 19:00:39.709010 22535416901760 run.py:483] Algo bellman_ford step 4626 current loss 0.543792, current_train_items 148064.
I0302 19:00:39.732862 22535416901760 run.py:483] Algo bellman_ford step 4627 current loss 0.646678, current_train_items 148096.
I0302 19:00:39.764442 22535416901760 run.py:483] Algo bellman_ford step 4628 current loss 0.718725, current_train_items 148128.
I0302 19:00:39.799797 22535416901760 run.py:483] Algo bellman_ford step 4629 current loss 0.944709, current_train_items 148160.
I0302 19:00:39.819333 22535416901760 run.py:483] Algo bellman_ford step 4630 current loss 0.293939, current_train_items 148192.
I0302 19:00:39.835637 22535416901760 run.py:483] Algo bellman_ford step 4631 current loss 0.454676, current_train_items 148224.
I0302 19:00:39.859681 22535416901760 run.py:483] Algo bellman_ford step 4632 current loss 0.699221, current_train_items 148256.
I0302 19:00:39.889057 22535416901760 run.py:483] Algo bellman_ford step 4633 current loss 0.668709, current_train_items 148288.
I0302 19:00:39.922954 22535416901760 run.py:483] Algo bellman_ford step 4634 current loss 0.760918, current_train_items 148320.
I0302 19:00:39.942468 22535416901760 run.py:483] Algo bellman_ford step 4635 current loss 0.339397, current_train_items 148352.
I0302 19:00:39.959174 22535416901760 run.py:483] Algo bellman_ford step 4636 current loss 0.481509, current_train_items 148384.
I0302 19:00:39.983020 22535416901760 run.py:483] Algo bellman_ford step 4637 current loss 0.627302, current_train_items 148416.
I0302 19:00:40.015582 22535416901760 run.py:483] Algo bellman_ford step 4638 current loss 0.673076, current_train_items 148448.
I0302 19:00:40.049608 22535416901760 run.py:483] Algo bellman_ford step 4639 current loss 0.763911, current_train_items 148480.
I0302 19:00:40.069122 22535416901760 run.py:483] Algo bellman_ford step 4640 current loss 0.307465, current_train_items 148512.
I0302 19:00:40.085140 22535416901760 run.py:483] Algo bellman_ford step 4641 current loss 0.468318, current_train_items 148544.
I0302 19:00:40.108847 22535416901760 run.py:483] Algo bellman_ford step 4642 current loss 0.610716, current_train_items 148576.
I0302 19:00:40.139876 22535416901760 run.py:483] Algo bellman_ford step 4643 current loss 0.695437, current_train_items 148608.
I0302 19:00:40.174238 22535416901760 run.py:483] Algo bellman_ford step 4644 current loss 0.928834, current_train_items 148640.
I0302 19:00:40.194105 22535416901760 run.py:483] Algo bellman_ford step 4645 current loss 0.325802, current_train_items 148672.
I0302 19:00:40.210307 22535416901760 run.py:483] Algo bellman_ford step 4646 current loss 0.437663, current_train_items 148704.
I0302 19:00:40.232622 22535416901760 run.py:483] Algo bellman_ford step 4647 current loss 0.562946, current_train_items 148736.
I0302 19:00:40.261896 22535416901760 run.py:483] Algo bellman_ford step 4648 current loss 0.663333, current_train_items 148768.
I0302 19:00:40.293095 22535416901760 run.py:483] Algo bellman_ford step 4649 current loss 0.725125, current_train_items 148800.
I0302 19:00:40.312600 22535416901760 run.py:483] Algo bellman_ford step 4650 current loss 0.261646, current_train_items 148832.
I0302 19:00:40.320317 22535416901760 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 19:00:40.320422 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:00:40.337581 22535416901760 run.py:483] Algo bellman_ford step 4651 current loss 0.522589, current_train_items 148864.
I0302 19:00:40.361539 22535416901760 run.py:483] Algo bellman_ford step 4652 current loss 0.629594, current_train_items 148896.
I0302 19:00:40.393869 22535416901760 run.py:483] Algo bellman_ford step 4653 current loss 0.716541, current_train_items 148928.
I0302 19:00:40.427823 22535416901760 run.py:483] Algo bellman_ford step 4654 current loss 0.917984, current_train_items 148960.
I0302 19:00:40.447691 22535416901760 run.py:483] Algo bellman_ford step 4655 current loss 0.335684, current_train_items 148992.
I0302 19:00:40.463635 22535416901760 run.py:483] Algo bellman_ford step 4656 current loss 0.547692, current_train_items 149024.
I0302 19:00:40.487740 22535416901760 run.py:483] Algo bellman_ford step 4657 current loss 0.559731, current_train_items 149056.
I0302 19:00:40.518549 22535416901760 run.py:483] Algo bellman_ford step 4658 current loss 0.682418, current_train_items 149088.
I0302 19:00:40.550692 22535416901760 run.py:483] Algo bellman_ford step 4659 current loss 0.734035, current_train_items 149120.
I0302 19:00:40.570497 22535416901760 run.py:483] Algo bellman_ford step 4660 current loss 0.348107, current_train_items 149152.
I0302 19:00:40.586827 22535416901760 run.py:483] Algo bellman_ford step 4661 current loss 0.488096, current_train_items 149184.
I0302 19:00:40.610620 22535416901760 run.py:483] Algo bellman_ford step 4662 current loss 0.543219, current_train_items 149216.
I0302 19:00:40.640880 22535416901760 run.py:483] Algo bellman_ford step 4663 current loss 0.637244, current_train_items 149248.
I0302 19:00:40.677356 22535416901760 run.py:483] Algo bellman_ford step 4664 current loss 0.835995, current_train_items 149280.
I0302 19:00:40.697165 22535416901760 run.py:483] Algo bellman_ford step 4665 current loss 0.335499, current_train_items 149312.
I0302 19:00:40.713099 22535416901760 run.py:483] Algo bellman_ford step 4666 current loss 0.417894, current_train_items 149344.
I0302 19:00:40.736404 22535416901760 run.py:483] Algo bellman_ford step 4667 current loss 0.608374, current_train_items 149376.
I0302 19:00:40.768335 22535416901760 run.py:483] Algo bellman_ford step 4668 current loss 0.658307, current_train_items 149408.
I0302 19:00:40.802436 22535416901760 run.py:483] Algo bellman_ford step 4669 current loss 0.778163, current_train_items 149440.
I0302 19:00:40.822479 22535416901760 run.py:483] Algo bellman_ford step 4670 current loss 0.337385, current_train_items 149472.
I0302 19:00:40.838265 22535416901760 run.py:483] Algo bellman_ford step 4671 current loss 0.372941, current_train_items 149504.
I0302 19:00:40.860548 22535416901760 run.py:483] Algo bellman_ford step 4672 current loss 0.691159, current_train_items 149536.
I0302 19:00:40.890271 22535416901760 run.py:483] Algo bellman_ford step 4673 current loss 0.625685, current_train_items 149568.
I0302 19:00:40.923228 22535416901760 run.py:483] Algo bellman_ford step 4674 current loss 0.692707, current_train_items 149600.
I0302 19:00:40.943027 22535416901760 run.py:483] Algo bellman_ford step 4675 current loss 0.274041, current_train_items 149632.
I0302 19:00:40.959628 22535416901760 run.py:483] Algo bellman_ford step 4676 current loss 0.553695, current_train_items 149664.
I0302 19:00:40.983206 22535416901760 run.py:483] Algo bellman_ford step 4677 current loss 0.679784, current_train_items 149696.
I0302 19:00:41.015338 22535416901760 run.py:483] Algo bellman_ford step 4678 current loss 0.688172, current_train_items 149728.
I0302 19:00:41.048839 22535416901760 run.py:483] Algo bellman_ford step 4679 current loss 0.756428, current_train_items 149760.
I0302 19:00:41.068670 22535416901760 run.py:483] Algo bellman_ford step 4680 current loss 0.317786, current_train_items 149792.
I0302 19:00:41.084462 22535416901760 run.py:483] Algo bellman_ford step 4681 current loss 0.468169, current_train_items 149824.
I0302 19:00:41.107440 22535416901760 run.py:483] Algo bellman_ford step 4682 current loss 0.607599, current_train_items 149856.
I0302 19:00:41.137953 22535416901760 run.py:483] Algo bellman_ford step 4683 current loss 0.728491, current_train_items 149888.
I0302 19:00:41.172477 22535416901760 run.py:483] Algo bellman_ford step 4684 current loss 0.760359, current_train_items 149920.
I0302 19:00:41.192360 22535416901760 run.py:483] Algo bellman_ford step 4685 current loss 0.328820, current_train_items 149952.
I0302 19:00:41.208113 22535416901760 run.py:483] Algo bellman_ford step 4686 current loss 0.439491, current_train_items 149984.
I0302 19:00:41.231233 22535416901760 run.py:483] Algo bellman_ford step 4687 current loss 0.697107, current_train_items 150016.
I0302 19:00:41.262717 22535416901760 run.py:483] Algo bellman_ford step 4688 current loss 0.720819, current_train_items 150048.
I0302 19:00:41.297459 22535416901760 run.py:483] Algo bellman_ford step 4689 current loss 0.850893, current_train_items 150080.
I0302 19:00:41.317634 22535416901760 run.py:483] Algo bellman_ford step 4690 current loss 0.318742, current_train_items 150112.
I0302 19:00:41.333840 22535416901760 run.py:483] Algo bellman_ford step 4691 current loss 0.488433, current_train_items 150144.
I0302 19:00:41.356774 22535416901760 run.py:483] Algo bellman_ford step 4692 current loss 0.578193, current_train_items 150176.
I0302 19:00:41.387963 22535416901760 run.py:483] Algo bellman_ford step 4693 current loss 0.681819, current_train_items 150208.
I0302 19:00:41.420823 22535416901760 run.py:483] Algo bellman_ford step 4694 current loss 0.819354, current_train_items 150240.
I0302 19:00:41.440613 22535416901760 run.py:483] Algo bellman_ford step 4695 current loss 0.305745, current_train_items 150272.
I0302 19:00:41.456763 22535416901760 run.py:483] Algo bellman_ford step 4696 current loss 0.439960, current_train_items 150304.
I0302 19:00:41.480999 22535416901760 run.py:483] Algo bellman_ford step 4697 current loss 0.709313, current_train_items 150336.
I0302 19:00:41.511307 22535416901760 run.py:483] Algo bellman_ford step 4698 current loss 0.674591, current_train_items 150368.
I0302 19:00:41.543209 22535416901760 run.py:483] Algo bellman_ford step 4699 current loss 0.792084, current_train_items 150400.
I0302 19:00:41.563147 22535416901760 run.py:483] Algo bellman_ford step 4700 current loss 0.387107, current_train_items 150432.
I0302 19:00:41.570595 22535416901760 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 19:00:41.570701 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:00:41.587011 22535416901760 run.py:483] Algo bellman_ford step 4701 current loss 0.470058, current_train_items 150464.
I0302 19:00:41.611018 22535416901760 run.py:483] Algo bellman_ford step 4702 current loss 0.656804, current_train_items 150496.
I0302 19:00:41.643142 22535416901760 run.py:483] Algo bellman_ford step 4703 current loss 0.747913, current_train_items 150528.
I0302 19:00:41.678073 22535416901760 run.py:483] Algo bellman_ford step 4704 current loss 0.810965, current_train_items 150560.
I0302 19:00:41.697830 22535416901760 run.py:483] Algo bellman_ford step 4705 current loss 0.348221, current_train_items 150592.
I0302 19:00:41.713439 22535416901760 run.py:483] Algo bellman_ford step 4706 current loss 0.429865, current_train_items 150624.
I0302 19:00:41.737065 22535416901760 run.py:483] Algo bellman_ford step 4707 current loss 0.641920, current_train_items 150656.
I0302 19:00:41.768561 22535416901760 run.py:483] Algo bellman_ford step 4708 current loss 0.730654, current_train_items 150688.
I0302 19:00:41.801660 22535416901760 run.py:483] Algo bellman_ford step 4709 current loss 0.743249, current_train_items 150720.
I0302 19:00:41.820936 22535416901760 run.py:483] Algo bellman_ford step 4710 current loss 0.357369, current_train_items 150752.
I0302 19:00:41.837065 22535416901760 run.py:483] Algo bellman_ford step 4711 current loss 0.460381, current_train_items 150784.
I0302 19:00:41.860818 22535416901760 run.py:483] Algo bellman_ford step 4712 current loss 0.655430, current_train_items 150816.
I0302 19:00:41.891676 22535416901760 run.py:483] Algo bellman_ford step 4713 current loss 0.677642, current_train_items 150848.
I0302 19:00:41.923593 22535416901760 run.py:483] Algo bellman_ford step 4714 current loss 0.746641, current_train_items 150880.
I0302 19:00:41.942850 22535416901760 run.py:483] Algo bellman_ford step 4715 current loss 0.286183, current_train_items 150912.
I0302 19:00:41.959026 22535416901760 run.py:483] Algo bellman_ford step 4716 current loss 0.492326, current_train_items 150944.
I0302 19:00:41.980846 22535416901760 run.py:483] Algo bellman_ford step 4717 current loss 0.526779, current_train_items 150976.
I0302 19:00:42.011411 22535416901760 run.py:483] Algo bellman_ford step 4718 current loss 0.627750, current_train_items 151008.
I0302 19:00:42.046654 22535416901760 run.py:483] Algo bellman_ford step 4719 current loss 0.790229, current_train_items 151040.
I0302 19:00:42.066019 22535416901760 run.py:483] Algo bellman_ford step 4720 current loss 0.333174, current_train_items 151072.
I0302 19:00:42.081783 22535416901760 run.py:483] Algo bellman_ford step 4721 current loss 0.529401, current_train_items 151104.
I0302 19:00:42.105791 22535416901760 run.py:483] Algo bellman_ford step 4722 current loss 0.627890, current_train_items 151136.
I0302 19:00:42.134832 22535416901760 run.py:483] Algo bellman_ford step 4723 current loss 0.664154, current_train_items 151168.
I0302 19:00:42.168437 22535416901760 run.py:483] Algo bellman_ford step 4724 current loss 0.810354, current_train_items 151200.
I0302 19:00:42.187590 22535416901760 run.py:483] Algo bellman_ford step 4725 current loss 0.363578, current_train_items 151232.
I0302 19:00:42.204202 22535416901760 run.py:483] Algo bellman_ford step 4726 current loss 0.510692, current_train_items 151264.
I0302 19:00:42.228513 22535416901760 run.py:483] Algo bellman_ford step 4727 current loss 0.682413, current_train_items 151296.
I0302 19:00:42.257470 22535416901760 run.py:483] Algo bellman_ford step 4728 current loss 0.550934, current_train_items 151328.
I0302 19:00:42.287037 22535416901760 run.py:483] Algo bellman_ford step 4729 current loss 0.593709, current_train_items 151360.
I0302 19:00:42.306104 22535416901760 run.py:483] Algo bellman_ford step 4730 current loss 0.319709, current_train_items 151392.
I0302 19:00:42.321845 22535416901760 run.py:483] Algo bellman_ford step 4731 current loss 0.419494, current_train_items 151424.
I0302 19:00:42.344307 22535416901760 run.py:483] Algo bellman_ford step 4732 current loss 0.646986, current_train_items 151456.
I0302 19:00:42.374817 22535416901760 run.py:483] Algo bellman_ford step 4733 current loss 0.767053, current_train_items 151488.
I0302 19:00:42.408266 22535416901760 run.py:483] Algo bellman_ford step 4734 current loss 0.787081, current_train_items 151520.
I0302 19:00:42.427295 22535416901760 run.py:483] Algo bellman_ford step 4735 current loss 0.359149, current_train_items 151552.
I0302 19:00:42.443721 22535416901760 run.py:483] Algo bellman_ford step 4736 current loss 0.559371, current_train_items 151584.
I0302 19:00:42.466647 22535416901760 run.py:483] Algo bellman_ford step 4737 current loss 0.772424, current_train_items 151616.
I0302 19:00:42.496643 22535416901760 run.py:483] Algo bellman_ford step 4738 current loss 0.869849, current_train_items 151648.
I0302 19:00:42.529203 22535416901760 run.py:483] Algo bellman_ford step 4739 current loss 0.805915, current_train_items 151680.
I0302 19:00:42.548533 22535416901760 run.py:483] Algo bellman_ford step 4740 current loss 0.311531, current_train_items 151712.
I0302 19:00:42.564351 22535416901760 run.py:483] Algo bellman_ford step 4741 current loss 0.504294, current_train_items 151744.
I0302 19:00:42.586769 22535416901760 run.py:483] Algo bellman_ford step 4742 current loss 0.631335, current_train_items 151776.
I0302 19:00:42.618030 22535416901760 run.py:483] Algo bellman_ford step 4743 current loss 0.748625, current_train_items 151808.
I0302 19:00:42.649955 22535416901760 run.py:483] Algo bellman_ford step 4744 current loss 0.749628, current_train_items 151840.
I0302 19:00:42.669436 22535416901760 run.py:483] Algo bellman_ford step 4745 current loss 0.336376, current_train_items 151872.
I0302 19:00:42.685964 22535416901760 run.py:483] Algo bellman_ford step 4746 current loss 0.443376, current_train_items 151904.
I0302 19:00:42.710754 22535416901760 run.py:483] Algo bellman_ford step 4747 current loss 0.722694, current_train_items 151936.
I0302 19:00:42.741201 22535416901760 run.py:483] Algo bellman_ford step 4748 current loss 0.764053, current_train_items 151968.
I0302 19:00:42.774003 22535416901760 run.py:483] Algo bellman_ford step 4749 current loss 0.848101, current_train_items 152000.
I0302 19:00:42.793371 22535416901760 run.py:483] Algo bellman_ford step 4750 current loss 0.288624, current_train_items 152032.
I0302 19:00:42.801223 22535416901760 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.8876953125, 'score': 0.8876953125, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 19:00:42.801328 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.888, val scores are: bellman_ford: 0.888
I0302 19:00:42.818268 22535416901760 run.py:483] Algo bellman_ford step 4751 current loss 0.500298, current_train_items 152064.
I0302 19:00:42.842435 22535416901760 run.py:483] Algo bellman_ford step 4752 current loss 0.684143, current_train_items 152096.
I0302 19:00:42.875246 22535416901760 run.py:483] Algo bellman_ford step 4753 current loss 0.800050, current_train_items 152128.
I0302 19:00:42.908322 22535416901760 run.py:483] Algo bellman_ford step 4754 current loss 0.891804, current_train_items 152160.
I0302 19:00:42.928370 22535416901760 run.py:483] Algo bellman_ford step 4755 current loss 0.340309, current_train_items 152192.
I0302 19:00:42.944255 22535416901760 run.py:483] Algo bellman_ford step 4756 current loss 0.566228, current_train_items 152224.
I0302 19:00:42.968682 22535416901760 run.py:483] Algo bellman_ford step 4757 current loss 0.611728, current_train_items 152256.
I0302 19:00:42.999784 22535416901760 run.py:483] Algo bellman_ford step 4758 current loss 0.660655, current_train_items 152288.
I0302 19:00:43.034269 22535416901760 run.py:483] Algo bellman_ford step 4759 current loss 0.902774, current_train_items 152320.
I0302 19:00:43.054110 22535416901760 run.py:483] Algo bellman_ford step 4760 current loss 0.370203, current_train_items 152352.
I0302 19:00:43.070431 22535416901760 run.py:483] Algo bellman_ford step 4761 current loss 0.458016, current_train_items 152384.
I0302 19:00:43.093910 22535416901760 run.py:483] Algo bellman_ford step 4762 current loss 0.685085, current_train_items 152416.
I0302 19:00:43.124423 22535416901760 run.py:483] Algo bellman_ford step 4763 current loss 0.767379, current_train_items 152448.
I0302 19:00:43.158291 22535416901760 run.py:483] Algo bellman_ford step 4764 current loss 0.911216, current_train_items 152480.
I0302 19:00:43.177620 22535416901760 run.py:483] Algo bellman_ford step 4765 current loss 0.329806, current_train_items 152512.
I0302 19:00:43.193170 22535416901760 run.py:483] Algo bellman_ford step 4766 current loss 0.417163, current_train_items 152544.
I0302 19:00:43.217477 22535416901760 run.py:483] Algo bellman_ford step 4767 current loss 0.630359, current_train_items 152576.
I0302 19:00:43.247072 22535416901760 run.py:483] Algo bellman_ford step 4768 current loss 0.729278, current_train_items 152608.
I0302 19:00:43.279450 22535416901760 run.py:483] Algo bellman_ford step 4769 current loss 0.786760, current_train_items 152640.
I0302 19:00:43.299318 22535416901760 run.py:483] Algo bellman_ford step 4770 current loss 0.402470, current_train_items 152672.
I0302 19:00:43.315278 22535416901760 run.py:483] Algo bellman_ford step 4771 current loss 0.386305, current_train_items 152704.
I0302 19:00:43.337972 22535416901760 run.py:483] Algo bellman_ford step 4772 current loss 0.550867, current_train_items 152736.
I0302 19:00:43.368406 22535416901760 run.py:483] Algo bellman_ford step 4773 current loss 0.772213, current_train_items 152768.
I0302 19:00:43.402462 22535416901760 run.py:483] Algo bellman_ford step 4774 current loss 0.784022, current_train_items 152800.
I0302 19:00:43.422402 22535416901760 run.py:483] Algo bellman_ford step 4775 current loss 0.316547, current_train_items 152832.
I0302 19:00:43.439081 22535416901760 run.py:483] Algo bellman_ford step 4776 current loss 0.441153, current_train_items 152864.
I0302 19:00:43.462023 22535416901760 run.py:483] Algo bellman_ford step 4777 current loss 0.549358, current_train_items 152896.
I0302 19:00:43.492635 22535416901760 run.py:483] Algo bellman_ford step 4778 current loss 0.717619, current_train_items 152928.
I0302 19:00:43.524776 22535416901760 run.py:483] Algo bellman_ford step 4779 current loss 0.657598, current_train_items 152960.
I0302 19:00:43.544555 22535416901760 run.py:483] Algo bellman_ford step 4780 current loss 0.308136, current_train_items 152992.
I0302 19:00:43.560741 22535416901760 run.py:483] Algo bellman_ford step 4781 current loss 0.519185, current_train_items 153024.
I0302 19:00:43.584614 22535416901760 run.py:483] Algo bellman_ford step 4782 current loss 0.609030, current_train_items 153056.
I0302 19:00:43.616318 22535416901760 run.py:483] Algo bellman_ford step 4783 current loss 0.739454, current_train_items 153088.
I0302 19:00:43.650570 22535416901760 run.py:483] Algo bellman_ford step 4784 current loss 0.783626, current_train_items 153120.
I0302 19:00:43.670338 22535416901760 run.py:483] Algo bellman_ford step 4785 current loss 0.505817, current_train_items 153152.
I0302 19:00:43.686873 22535416901760 run.py:483] Algo bellman_ford step 4786 current loss 0.446009, current_train_items 153184.
I0302 19:00:43.710603 22535416901760 run.py:483] Algo bellman_ford step 4787 current loss 0.597514, current_train_items 153216.
I0302 19:00:43.740669 22535416901760 run.py:483] Algo bellman_ford step 4788 current loss 0.665757, current_train_items 153248.
I0302 19:00:43.772714 22535416901760 run.py:483] Algo bellman_ford step 4789 current loss 0.691839, current_train_items 153280.
I0302 19:00:43.791993 22535416901760 run.py:483] Algo bellman_ford step 4790 current loss 0.378317, current_train_items 153312.
I0302 19:00:43.808170 22535416901760 run.py:483] Algo bellman_ford step 4791 current loss 0.490235, current_train_items 153344.
I0302 19:00:43.831544 22535416901760 run.py:483] Algo bellman_ford step 4792 current loss 0.487257, current_train_items 153376.
I0302 19:00:43.862178 22535416901760 run.py:483] Algo bellman_ford step 4793 current loss 0.750615, current_train_items 153408.
I0302 19:00:43.897282 22535416901760 run.py:483] Algo bellman_ford step 4794 current loss 0.870642, current_train_items 153440.
I0302 19:00:43.916899 22535416901760 run.py:483] Algo bellman_ford step 4795 current loss 0.236078, current_train_items 153472.
I0302 19:00:43.933098 22535416901760 run.py:483] Algo bellman_ford step 4796 current loss 0.449860, current_train_items 153504.
I0302 19:00:43.956972 22535416901760 run.py:483] Algo bellman_ford step 4797 current loss 0.699317, current_train_items 153536.
I0302 19:00:43.987558 22535416901760 run.py:483] Algo bellman_ford step 4798 current loss 0.687276, current_train_items 153568.
I0302 19:00:44.020872 22535416901760 run.py:483] Algo bellman_ford step 4799 current loss 0.727764, current_train_items 153600.
I0302 19:00:44.040581 22535416901760 run.py:483] Algo bellman_ford step 4800 current loss 0.292741, current_train_items 153632.
I0302 19:00:44.048284 22535416901760 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.912109375, 'score': 0.912109375, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:44.048391 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.938, current avg val score is 0.912, val scores are: bellman_ford: 0.912
I0302 19:00:44.065423 22535416901760 run.py:483] Algo bellman_ford step 4801 current loss 0.579982, current_train_items 153664.
I0302 19:00:44.089326 22535416901760 run.py:483] Algo bellman_ford step 4802 current loss 0.662095, current_train_items 153696.
I0302 19:00:44.120016 22535416901760 run.py:483] Algo bellman_ford step 4803 current loss 0.629141, current_train_items 153728.
I0302 19:00:44.156055 22535416901760 run.py:483] Algo bellman_ford step 4804 current loss 0.817542, current_train_items 153760.
I0302 19:00:44.175758 22535416901760 run.py:483] Algo bellman_ford step 4805 current loss 0.351320, current_train_items 153792.
I0302 19:00:44.192238 22535416901760 run.py:483] Algo bellman_ford step 4806 current loss 0.468644, current_train_items 153824.
I0302 19:00:44.215513 22535416901760 run.py:483] Algo bellman_ford step 4807 current loss 0.612379, current_train_items 153856.
I0302 19:00:44.246390 22535416901760 run.py:483] Algo bellman_ford step 4808 current loss 0.719386, current_train_items 153888.
I0302 19:00:44.279625 22535416901760 run.py:483] Algo bellman_ford step 4809 current loss 0.765383, current_train_items 153920.
I0302 19:00:44.299274 22535416901760 run.py:483] Algo bellman_ford step 4810 current loss 0.319010, current_train_items 153952.
I0302 19:00:44.315457 22535416901760 run.py:483] Algo bellman_ford step 4811 current loss 0.496143, current_train_items 153984.
I0302 19:00:44.338326 22535416901760 run.py:483] Algo bellman_ford step 4812 current loss 0.604949, current_train_items 154016.
I0302 19:00:44.368398 22535416901760 run.py:483] Algo bellman_ford step 4813 current loss 0.706501, current_train_items 154048.
I0302 19:00:44.402899 22535416901760 run.py:483] Algo bellman_ford step 4814 current loss 0.873663, current_train_items 154080.
I0302 19:00:44.422353 22535416901760 run.py:483] Algo bellman_ford step 4815 current loss 0.344727, current_train_items 154112.
I0302 19:00:44.438558 22535416901760 run.py:483] Algo bellman_ford step 4816 current loss 0.432850, current_train_items 154144.
I0302 19:00:44.462926 22535416901760 run.py:483] Algo bellman_ford step 4817 current loss 0.784477, current_train_items 154176.
I0302 19:00:44.493604 22535416901760 run.py:483] Algo bellman_ford step 4818 current loss 0.754216, current_train_items 154208.
I0302 19:00:44.522953 22535416901760 run.py:483] Algo bellman_ford step 4819 current loss 0.687275, current_train_items 154240.
I0302 19:00:44.542462 22535416901760 run.py:483] Algo bellman_ford step 4820 current loss 0.313515, current_train_items 154272.
I0302 19:00:44.558453 22535416901760 run.py:483] Algo bellman_ford step 4821 current loss 0.522662, current_train_items 154304.
I0302 19:00:44.582268 22535416901760 run.py:483] Algo bellman_ford step 4822 current loss 0.702577, current_train_items 154336.
I0302 19:00:44.612419 22535416901760 run.py:483] Algo bellman_ford step 4823 current loss 0.770468, current_train_items 154368.
I0302 19:00:44.646290 22535416901760 run.py:483] Algo bellman_ford step 4824 current loss 0.800342, current_train_items 154400.
I0302 19:00:44.665794 22535416901760 run.py:483] Algo bellman_ford step 4825 current loss 0.307248, current_train_items 154432.
I0302 19:00:44.682596 22535416901760 run.py:483] Algo bellman_ford step 4826 current loss 0.414951, current_train_items 154464.
I0302 19:00:44.705460 22535416901760 run.py:483] Algo bellman_ford step 4827 current loss 0.661147, current_train_items 154496.
I0302 19:00:44.736312 22535416901760 run.py:483] Algo bellman_ford step 4828 current loss 0.830249, current_train_items 154528.
I0302 19:00:44.770061 22535416901760 run.py:483] Algo bellman_ford step 4829 current loss 1.052861, current_train_items 154560.
I0302 19:00:44.789402 22535416901760 run.py:483] Algo bellman_ford step 4830 current loss 0.268017, current_train_items 154592.
I0302 19:00:44.805634 22535416901760 run.py:483] Algo bellman_ford step 4831 current loss 0.501743, current_train_items 154624.
I0302 19:00:44.829344 22535416901760 run.py:483] Algo bellman_ford step 4832 current loss 0.568214, current_train_items 154656.
I0302 19:00:44.860509 22535416901760 run.py:483] Algo bellman_ford step 4833 current loss 0.736786, current_train_items 154688.
I0302 19:00:44.893317 22535416901760 run.py:483] Algo bellman_ford step 4834 current loss 0.713468, current_train_items 154720.
I0302 19:00:44.912419 22535416901760 run.py:483] Algo bellman_ford step 4835 current loss 0.228188, current_train_items 154752.
I0302 19:00:44.928481 22535416901760 run.py:483] Algo bellman_ford step 4836 current loss 0.460321, current_train_items 154784.
I0302 19:00:44.950745 22535416901760 run.py:483] Algo bellman_ford step 4837 current loss 0.573108, current_train_items 154816.
I0302 19:00:44.981307 22535416901760 run.py:483] Algo bellman_ford step 4838 current loss 0.729803, current_train_items 154848.
I0302 19:00:45.012544 22535416901760 run.py:483] Algo bellman_ford step 4839 current loss 0.799410, current_train_items 154880.
I0302 19:00:45.031893 22535416901760 run.py:483] Algo bellman_ford step 4840 current loss 0.310926, current_train_items 154912.
I0302 19:00:45.047875 22535416901760 run.py:483] Algo bellman_ford step 4841 current loss 0.440702, current_train_items 154944.
I0302 19:00:45.070976 22535416901760 run.py:483] Algo bellman_ford step 4842 current loss 0.609893, current_train_items 154976.
I0302 19:00:45.100769 22535416901760 run.py:483] Algo bellman_ford step 4843 current loss 0.783007, current_train_items 155008.
I0302 19:00:45.135200 22535416901760 run.py:483] Algo bellman_ford step 4844 current loss 0.906863, current_train_items 155040.
I0302 19:00:45.154575 22535416901760 run.py:483] Algo bellman_ford step 4845 current loss 0.368005, current_train_items 155072.
I0302 19:00:45.170931 22535416901760 run.py:483] Algo bellman_ford step 4846 current loss 0.507878, current_train_items 155104.
I0302 19:00:45.193400 22535416901760 run.py:483] Algo bellman_ford step 4847 current loss 0.553292, current_train_items 155136.
I0302 19:00:45.223114 22535416901760 run.py:483] Algo bellman_ford step 4848 current loss 0.714243, current_train_items 155168.
I0302 19:00:45.256275 22535416901760 run.py:483] Algo bellman_ford step 4849 current loss 0.794559, current_train_items 155200.
I0302 19:00:45.275598 22535416901760 run.py:483] Algo bellman_ford step 4850 current loss 0.348127, current_train_items 155232.
I0302 19:00:45.283522 22535416901760 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:45.283628 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.938, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:00:45.314024 22535416901760 run.py:483] Algo bellman_ford step 4851 current loss 0.496965, current_train_items 155264.
I0302 19:00:45.336778 22535416901760 run.py:483] Algo bellman_ford step 4852 current loss 0.666107, current_train_items 155296.
I0302 19:00:45.367444 22535416901760 run.py:483] Algo bellman_ford step 4853 current loss 0.648323, current_train_items 155328.
I0302 19:00:45.400988 22535416901760 run.py:483] Algo bellman_ford step 4854 current loss 0.804775, current_train_items 155360.
I0302 19:00:45.421413 22535416901760 run.py:483] Algo bellman_ford step 4855 current loss 0.343849, current_train_items 155392.
I0302 19:00:45.437938 22535416901760 run.py:483] Algo bellman_ford step 4856 current loss 0.500961, current_train_items 155424.
I0302 19:00:45.462245 22535416901760 run.py:483] Algo bellman_ford step 4857 current loss 0.639103, current_train_items 155456.
I0302 19:00:45.493964 22535416901760 run.py:483] Algo bellman_ford step 4858 current loss 0.698119, current_train_items 155488.
I0302 19:00:45.529422 22535416901760 run.py:483] Algo bellman_ford step 4859 current loss 0.796852, current_train_items 155520.
I0302 19:00:45.549119 22535416901760 run.py:483] Algo bellman_ford step 4860 current loss 0.308960, current_train_items 155552.
I0302 19:00:45.566214 22535416901760 run.py:483] Algo bellman_ford step 4861 current loss 0.592502, current_train_items 155584.
I0302 19:00:45.589341 22535416901760 run.py:483] Algo bellman_ford step 4862 current loss 0.836095, current_train_items 155616.
I0302 19:00:45.620873 22535416901760 run.py:483] Algo bellman_ford step 4863 current loss 0.691566, current_train_items 155648.
I0302 19:00:45.653213 22535416901760 run.py:483] Algo bellman_ford step 4864 current loss 0.634565, current_train_items 155680.
I0302 19:00:45.672713 22535416901760 run.py:483] Algo bellman_ford step 4865 current loss 0.286474, current_train_items 155712.
I0302 19:00:45.688655 22535416901760 run.py:483] Algo bellman_ford step 4866 current loss 0.487453, current_train_items 155744.
I0302 19:00:45.712728 22535416901760 run.py:483] Algo bellman_ford step 4867 current loss 0.730540, current_train_items 155776.
I0302 19:00:45.745068 22535416901760 run.py:483] Algo bellman_ford step 4868 current loss 0.734966, current_train_items 155808.
I0302 19:00:45.777288 22535416901760 run.py:483] Algo bellman_ford step 4869 current loss 0.735876, current_train_items 155840.
I0302 19:00:45.797123 22535416901760 run.py:483] Algo bellman_ford step 4870 current loss 0.311859, current_train_items 155872.
I0302 19:00:45.813366 22535416901760 run.py:483] Algo bellman_ford step 4871 current loss 0.479467, current_train_items 155904.
I0302 19:00:45.836754 22535416901760 run.py:483] Algo bellman_ford step 4872 current loss 0.581357, current_train_items 155936.
I0302 19:00:45.865549 22535416901760 run.py:483] Algo bellman_ford step 4873 current loss 0.542444, current_train_items 155968.
I0302 19:00:45.897606 22535416901760 run.py:483] Algo bellman_ford step 4874 current loss 0.899392, current_train_items 156000.
I0302 19:00:45.917371 22535416901760 run.py:483] Algo bellman_ford step 4875 current loss 0.305540, current_train_items 156032.
I0302 19:00:45.933298 22535416901760 run.py:483] Algo bellman_ford step 4876 current loss 0.504235, current_train_items 156064.
I0302 19:00:45.955736 22535416901760 run.py:483] Algo bellman_ford step 4877 current loss 0.571211, current_train_items 156096.
I0302 19:00:45.985385 22535416901760 run.py:483] Algo bellman_ford step 4878 current loss 0.603377, current_train_items 156128.
I0302 19:00:46.019034 22535416901760 run.py:483] Algo bellman_ford step 4879 current loss 0.683007, current_train_items 156160.
I0302 19:00:46.038458 22535416901760 run.py:483] Algo bellman_ford step 4880 current loss 0.299388, current_train_items 156192.
I0302 19:00:46.054524 22535416901760 run.py:483] Algo bellman_ford step 4881 current loss 0.435405, current_train_items 156224.
I0302 19:00:46.078276 22535416901760 run.py:483] Algo bellman_ford step 4882 current loss 0.652524, current_train_items 156256.
I0302 19:00:46.109446 22535416901760 run.py:483] Algo bellman_ford step 4883 current loss 0.751655, current_train_items 156288.
I0302 19:00:46.143455 22535416901760 run.py:483] Algo bellman_ford step 4884 current loss 0.760039, current_train_items 156320.
I0302 19:00:46.163250 22535416901760 run.py:483] Algo bellman_ford step 4885 current loss 0.319228, current_train_items 156352.
I0302 19:00:46.179721 22535416901760 run.py:483] Algo bellman_ford step 4886 current loss 0.627284, current_train_items 156384.
I0302 19:00:46.203091 22535416901760 run.py:483] Algo bellman_ford step 4887 current loss 0.677692, current_train_items 156416.
I0302 19:00:46.233528 22535416901760 run.py:483] Algo bellman_ford step 4888 current loss 0.652352, current_train_items 156448.
I0302 19:00:46.266048 22535416901760 run.py:483] Algo bellman_ford step 4889 current loss 0.721091, current_train_items 156480.
I0302 19:00:46.286027 22535416901760 run.py:483] Algo bellman_ford step 4890 current loss 0.262493, current_train_items 156512.
I0302 19:00:46.302139 22535416901760 run.py:483] Algo bellman_ford step 4891 current loss 0.438832, current_train_items 156544.
I0302 19:00:46.325222 22535416901760 run.py:483] Algo bellman_ford step 4892 current loss 0.675959, current_train_items 156576.
I0302 19:00:46.354839 22535416901760 run.py:483] Algo bellman_ford step 4893 current loss 0.740994, current_train_items 156608.
I0302 19:00:46.386683 22535416901760 run.py:483] Algo bellman_ford step 4894 current loss 0.874042, current_train_items 156640.
I0302 19:00:46.406185 22535416901760 run.py:483] Algo bellman_ford step 4895 current loss 0.316791, current_train_items 156672.
I0302 19:00:46.422333 22535416901760 run.py:483] Algo bellman_ford step 4896 current loss 0.449325, current_train_items 156704.
I0302 19:00:46.445025 22535416901760 run.py:483] Algo bellman_ford step 4897 current loss 0.549213, current_train_items 156736.
I0302 19:00:46.475097 22535416901760 run.py:483] Algo bellman_ford step 4898 current loss 0.814363, current_train_items 156768.
I0302 19:00:46.509060 22535416901760 run.py:483] Algo bellman_ford step 4899 current loss 0.845367, current_train_items 156800.
I0302 19:00:46.528929 22535416901760 run.py:483] Algo bellman_ford step 4900 current loss 0.319760, current_train_items 156832.
I0302 19:00:46.536591 22535416901760 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:46.536731 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:00:46.553004 22535416901760 run.py:483] Algo bellman_ford step 4901 current loss 0.461727, current_train_items 156864.
I0302 19:00:46.576970 22535416901760 run.py:483] Algo bellman_ford step 4902 current loss 0.596645, current_train_items 156896.
I0302 19:00:46.609822 22535416901760 run.py:483] Algo bellman_ford step 4903 current loss 0.705699, current_train_items 156928.
I0302 19:00:46.642992 22535416901760 run.py:483] Algo bellman_ford step 4904 current loss 0.717264, current_train_items 156960.
I0302 19:00:46.663313 22535416901760 run.py:483] Algo bellman_ford step 4905 current loss 0.369429, current_train_items 156992.
I0302 19:00:46.679170 22535416901760 run.py:483] Algo bellman_ford step 4906 current loss 0.526419, current_train_items 157024.
I0302 19:00:46.702830 22535416901760 run.py:483] Algo bellman_ford step 4907 current loss 0.578247, current_train_items 157056.
I0302 19:00:46.733934 22535416901760 run.py:483] Algo bellman_ford step 4908 current loss 0.613496, current_train_items 157088.
I0302 19:00:46.766995 22535416901760 run.py:483] Algo bellman_ford step 4909 current loss 0.750463, current_train_items 157120.
I0302 19:00:46.786673 22535416901760 run.py:483] Algo bellman_ford step 4910 current loss 0.324290, current_train_items 157152.
I0302 19:00:46.802929 22535416901760 run.py:483] Algo bellman_ford step 4911 current loss 0.467280, current_train_items 157184.
I0302 19:00:46.826179 22535416901760 run.py:483] Algo bellman_ford step 4912 current loss 0.526043, current_train_items 157216.
I0302 19:00:46.858071 22535416901760 run.py:483] Algo bellman_ford step 4913 current loss 0.713841, current_train_items 157248.
I0302 19:00:46.890918 22535416901760 run.py:483] Algo bellman_ford step 4914 current loss 0.752347, current_train_items 157280.
I0302 19:00:46.910688 22535416901760 run.py:483] Algo bellman_ford step 4915 current loss 0.272243, current_train_items 157312.
I0302 19:00:46.926538 22535416901760 run.py:483] Algo bellman_ford step 4916 current loss 0.479434, current_train_items 157344.
I0302 19:00:46.949752 22535416901760 run.py:483] Algo bellman_ford step 4917 current loss 0.689423, current_train_items 157376.
I0302 19:00:46.979329 22535416901760 run.py:483] Algo bellman_ford step 4918 current loss 0.669341, current_train_items 157408.
I0302 19:00:47.012761 22535416901760 run.py:483] Algo bellman_ford step 4919 current loss 0.724820, current_train_items 157440.
I0302 19:00:47.032168 22535416901760 run.py:483] Algo bellman_ford step 4920 current loss 0.346472, current_train_items 157472.
I0302 19:00:47.047735 22535416901760 run.py:483] Algo bellman_ford step 4921 current loss 0.372942, current_train_items 157504.
I0302 19:00:47.071274 22535416901760 run.py:483] Algo bellman_ford step 4922 current loss 0.582086, current_train_items 157536.
I0302 19:00:47.102216 22535416901760 run.py:483] Algo bellman_ford step 4923 current loss 0.723315, current_train_items 157568.
I0302 19:00:47.135087 22535416901760 run.py:483] Algo bellman_ford step 4924 current loss 0.849018, current_train_items 157600.
I0302 19:00:47.154662 22535416901760 run.py:483] Algo bellman_ford step 4925 current loss 0.277723, current_train_items 157632.
I0302 19:00:47.170855 22535416901760 run.py:483] Algo bellman_ford step 4926 current loss 0.528361, current_train_items 157664.
I0302 19:00:47.194670 22535416901760 run.py:483] Algo bellman_ford step 4927 current loss 0.610940, current_train_items 157696.
I0302 19:00:47.224485 22535416901760 run.py:483] Algo bellman_ford step 4928 current loss 0.594136, current_train_items 157728.
I0302 19:00:47.257807 22535416901760 run.py:483] Algo bellman_ford step 4929 current loss 0.821323, current_train_items 157760.
I0302 19:00:47.277190 22535416901760 run.py:483] Algo bellman_ford step 4930 current loss 0.277096, current_train_items 157792.
I0302 19:00:47.293012 22535416901760 run.py:483] Algo bellman_ford step 4931 current loss 0.573015, current_train_items 157824.
I0302 19:00:47.316387 22535416901760 run.py:483] Algo bellman_ford step 4932 current loss 0.611802, current_train_items 157856.
I0302 19:00:47.346570 22535416901760 run.py:483] Algo bellman_ford step 4933 current loss 0.556083, current_train_items 157888.
I0302 19:00:47.379702 22535416901760 run.py:483] Algo bellman_ford step 4934 current loss 0.779323, current_train_items 157920.
I0302 19:00:47.399005 22535416901760 run.py:483] Algo bellman_ford step 4935 current loss 0.322404, current_train_items 157952.
I0302 19:00:47.415174 22535416901760 run.py:483] Algo bellman_ford step 4936 current loss 0.455921, current_train_items 157984.
I0302 19:00:47.439903 22535416901760 run.py:483] Algo bellman_ford step 4937 current loss 0.603293, current_train_items 158016.
I0302 19:00:47.470104 22535416901760 run.py:483] Algo bellman_ford step 4938 current loss 0.645944, current_train_items 158048.
I0302 19:00:47.505324 22535416901760 run.py:483] Algo bellman_ford step 4939 current loss 0.829457, current_train_items 158080.
I0302 19:00:47.524764 22535416901760 run.py:483] Algo bellman_ford step 4940 current loss 0.328730, current_train_items 158112.
I0302 19:00:47.540525 22535416901760 run.py:483] Algo bellman_ford step 4941 current loss 0.399639, current_train_items 158144.
I0302 19:00:47.563883 22535416901760 run.py:483] Algo bellman_ford step 4942 current loss 0.547053, current_train_items 158176.
I0302 19:00:47.595414 22535416901760 run.py:483] Algo bellman_ford step 4943 current loss 0.695987, current_train_items 158208.
I0302 19:00:47.627323 22535416901760 run.py:483] Algo bellman_ford step 4944 current loss 0.679505, current_train_items 158240.
I0302 19:00:47.646777 22535416901760 run.py:483] Algo bellman_ford step 4945 current loss 0.371397, current_train_items 158272.
I0302 19:00:47.662998 22535416901760 run.py:483] Algo bellman_ford step 4946 current loss 0.488589, current_train_items 158304.
I0302 19:00:47.686013 22535416901760 run.py:483] Algo bellman_ford step 4947 current loss 0.559677, current_train_items 158336.
I0302 19:00:47.716763 22535416901760 run.py:483] Algo bellman_ford step 4948 current loss 0.791722, current_train_items 158368.
I0302 19:00:47.751700 22535416901760 run.py:483] Algo bellman_ford step 4949 current loss 0.713314, current_train_items 158400.
I0302 19:00:47.771197 22535416901760 run.py:483] Algo bellman_ford step 4950 current loss 0.288910, current_train_items 158432.
I0302 19:00:47.779395 22535416901760 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:47.779502 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:00:47.795892 22535416901760 run.py:483] Algo bellman_ford step 4951 current loss 0.425237, current_train_items 158464.
I0302 19:00:47.819140 22535416901760 run.py:483] Algo bellman_ford step 4952 current loss 0.533661, current_train_items 158496.
I0302 19:00:47.850072 22535416901760 run.py:483] Algo bellman_ford step 4953 current loss 0.580632, current_train_items 158528.
I0302 19:00:47.885666 22535416901760 run.py:483] Algo bellman_ford step 4954 current loss 0.692602, current_train_items 158560.
I0302 19:00:47.905985 22535416901760 run.py:483] Algo bellman_ford step 4955 current loss 0.337200, current_train_items 158592.
I0302 19:00:47.922099 22535416901760 run.py:483] Algo bellman_ford step 4956 current loss 0.513006, current_train_items 158624.
I0302 19:00:47.946836 22535416901760 run.py:483] Algo bellman_ford step 4957 current loss 0.646173, current_train_items 158656.
I0302 19:00:47.977819 22535416901760 run.py:483] Algo bellman_ford step 4958 current loss 0.783103, current_train_items 158688.
I0302 19:00:48.008985 22535416901760 run.py:483] Algo bellman_ford step 4959 current loss 0.829487, current_train_items 158720.
I0302 19:00:48.028967 22535416901760 run.py:483] Algo bellman_ford step 4960 current loss 0.258356, current_train_items 158752.
I0302 19:00:48.045382 22535416901760 run.py:483] Algo bellman_ford step 4961 current loss 0.401212, current_train_items 158784.
I0302 19:00:48.068828 22535416901760 run.py:483] Algo bellman_ford step 4962 current loss 0.599545, current_train_items 158816.
I0302 19:00:48.098342 22535416901760 run.py:483] Algo bellman_ford step 4963 current loss 0.601154, current_train_items 158848.
I0302 19:00:48.131608 22535416901760 run.py:483] Algo bellman_ford step 4964 current loss 0.666343, current_train_items 158880.
I0302 19:00:48.150887 22535416901760 run.py:483] Algo bellman_ford step 4965 current loss 0.276969, current_train_items 158912.
I0302 19:00:48.166963 22535416901760 run.py:483] Algo bellman_ford step 4966 current loss 0.467563, current_train_items 158944.
I0302 19:00:48.191076 22535416901760 run.py:483] Algo bellman_ford step 4967 current loss 0.767097, current_train_items 158976.
I0302 19:00:48.222596 22535416901760 run.py:483] Algo bellman_ford step 4968 current loss 0.691416, current_train_items 159008.
I0302 19:00:48.254502 22535416901760 run.py:483] Algo bellman_ford step 4969 current loss 0.641455, current_train_items 159040.
I0302 19:00:48.274747 22535416901760 run.py:483] Algo bellman_ford step 4970 current loss 0.330050, current_train_items 159072.
I0302 19:00:48.290775 22535416901760 run.py:483] Algo bellman_ford step 4971 current loss 0.477443, current_train_items 159104.
I0302 19:00:48.314457 22535416901760 run.py:483] Algo bellman_ford step 4972 current loss 0.631989, current_train_items 159136.
I0302 19:00:48.345901 22535416901760 run.py:483] Algo bellman_ford step 4973 current loss 0.908162, current_train_items 159168.
I0302 19:00:48.379123 22535416901760 run.py:483] Algo bellman_ford step 4974 current loss 0.851453, current_train_items 159200.
I0302 19:00:48.399127 22535416901760 run.py:483] Algo bellman_ford step 4975 current loss 0.385141, current_train_items 159232.
I0302 19:00:48.415826 22535416901760 run.py:483] Algo bellman_ford step 4976 current loss 0.402099, current_train_items 159264.
I0302 19:00:48.439541 22535416901760 run.py:483] Algo bellman_ford step 4977 current loss 0.641244, current_train_items 159296.
I0302 19:00:48.471080 22535416901760 run.py:483] Algo bellman_ford step 4978 current loss 0.749602, current_train_items 159328.
I0302 19:00:48.505574 22535416901760 run.py:483] Algo bellman_ford step 4979 current loss 0.791179, current_train_items 159360.
I0302 19:00:48.524963 22535416901760 run.py:483] Algo bellman_ford step 4980 current loss 0.290119, current_train_items 159392.
I0302 19:00:48.541227 22535416901760 run.py:483] Algo bellman_ford step 4981 current loss 0.451353, current_train_items 159424.
I0302 19:00:48.565730 22535416901760 run.py:483] Algo bellman_ford step 4982 current loss 0.565600, current_train_items 159456.
I0302 19:00:48.594787 22535416901760 run.py:483] Algo bellman_ford step 4983 current loss 0.638646, current_train_items 159488.
I0302 19:00:48.627881 22535416901760 run.py:483] Algo bellman_ford step 4984 current loss 0.758924, current_train_items 159520.
I0302 19:00:48.647627 22535416901760 run.py:483] Algo bellman_ford step 4985 current loss 0.347730, current_train_items 159552.
I0302 19:00:48.663938 22535416901760 run.py:483] Algo bellman_ford step 4986 current loss 0.414582, current_train_items 159584.
I0302 19:00:48.685786 22535416901760 run.py:483] Algo bellman_ford step 4987 current loss 0.553559, current_train_items 159616.
I0302 19:00:48.715894 22535416901760 run.py:483] Algo bellman_ford step 4988 current loss 0.625670, current_train_items 159648.
I0302 19:00:48.751931 22535416901760 run.py:483] Algo bellman_ford step 4989 current loss 0.806153, current_train_items 159680.
I0302 19:00:48.771532 22535416901760 run.py:483] Algo bellman_ford step 4990 current loss 0.302550, current_train_items 159712.
I0302 19:00:48.787498 22535416901760 run.py:483] Algo bellman_ford step 4991 current loss 0.469426, current_train_items 159744.
I0302 19:00:48.810932 22535416901760 run.py:483] Algo bellman_ford step 4992 current loss 0.615556, current_train_items 159776.
I0302 19:00:48.841259 22535416901760 run.py:483] Algo bellman_ford step 4993 current loss 0.683942, current_train_items 159808.
I0302 19:00:48.875129 22535416901760 run.py:483] Algo bellman_ford step 4994 current loss 0.829417, current_train_items 159840.
I0302 19:00:48.895067 22535416901760 run.py:483] Algo bellman_ford step 4995 current loss 0.280941, current_train_items 159872.
I0302 19:00:48.911520 22535416901760 run.py:483] Algo bellman_ford step 4996 current loss 0.449399, current_train_items 159904.
I0302 19:00:48.934612 22535416901760 run.py:483] Algo bellman_ford step 4997 current loss 0.632489, current_train_items 159936.
I0302 19:00:48.965504 22535416901760 run.py:483] Algo bellman_ford step 4998 current loss 0.755135, current_train_items 159968.
I0302 19:00:48.999737 22535416901760 run.py:483] Algo bellman_ford step 4999 current loss 0.881731, current_train_items 160000.
I0302 19:00:49.019623 22535416901760 run.py:483] Algo bellman_ford step 5000 current loss 0.352013, current_train_items 160032.
I0302 19:00:49.027335 22535416901760 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:49.027443 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:00:49.044334 22535416901760 run.py:483] Algo bellman_ford step 5001 current loss 0.464459, current_train_items 160064.
I0302 19:00:49.067741 22535416901760 run.py:483] Algo bellman_ford step 5002 current loss 0.651990, current_train_items 160096.
I0302 19:00:49.097767 22535416901760 run.py:483] Algo bellman_ford step 5003 current loss 0.546186, current_train_items 160128.
I0302 19:00:49.130903 22535416901760 run.py:483] Algo bellman_ford step 5004 current loss 0.705188, current_train_items 160160.
I0302 19:00:49.151186 22535416901760 run.py:483] Algo bellman_ford step 5005 current loss 0.338374, current_train_items 160192.
I0302 19:00:49.167164 22535416901760 run.py:483] Algo bellman_ford step 5006 current loss 0.490961, current_train_items 160224.
I0302 19:00:49.191332 22535416901760 run.py:483] Algo bellman_ford step 5007 current loss 0.688106, current_train_items 160256.
I0302 19:00:49.224309 22535416901760 run.py:483] Algo bellman_ford step 5008 current loss 0.774474, current_train_items 160288.
I0302 19:00:49.255931 22535416901760 run.py:483] Algo bellman_ford step 5009 current loss 0.719431, current_train_items 160320.
I0302 19:00:49.275357 22535416901760 run.py:483] Algo bellman_ford step 5010 current loss 0.312623, current_train_items 160352.
I0302 19:00:49.291171 22535416901760 run.py:483] Algo bellman_ford step 5011 current loss 0.461446, current_train_items 160384.
I0302 19:00:49.314624 22535416901760 run.py:483] Algo bellman_ford step 5012 current loss 0.567355, current_train_items 160416.
I0302 19:00:49.345086 22535416901760 run.py:483] Algo bellman_ford step 5013 current loss 0.677030, current_train_items 160448.
I0302 19:00:49.379246 22535416901760 run.py:483] Algo bellman_ford step 5014 current loss 0.750268, current_train_items 160480.
I0302 19:00:49.398908 22535416901760 run.py:483] Algo bellman_ford step 5015 current loss 0.367393, current_train_items 160512.
I0302 19:00:49.415685 22535416901760 run.py:483] Algo bellman_ford step 5016 current loss 0.562023, current_train_items 160544.
I0302 19:00:49.438664 22535416901760 run.py:483] Algo bellman_ford step 5017 current loss 0.622586, current_train_items 160576.
I0302 19:00:49.469543 22535416901760 run.py:483] Algo bellman_ford step 5018 current loss 0.632707, current_train_items 160608.
I0302 19:00:49.504539 22535416901760 run.py:483] Algo bellman_ford step 5019 current loss 0.793682, current_train_items 160640.
I0302 19:00:49.523807 22535416901760 run.py:483] Algo bellman_ford step 5020 current loss 0.295567, current_train_items 160672.
I0302 19:00:49.539648 22535416901760 run.py:483] Algo bellman_ford step 5021 current loss 0.484298, current_train_items 160704.
I0302 19:00:49.563856 22535416901760 run.py:483] Algo bellman_ford step 5022 current loss 0.757479, current_train_items 160736.
I0302 19:00:49.594531 22535416901760 run.py:483] Algo bellman_ford step 5023 current loss 0.744144, current_train_items 160768.
I0302 19:00:49.628201 22535416901760 run.py:483] Algo bellman_ford step 5024 current loss 0.797804, current_train_items 160800.
I0302 19:00:49.647520 22535416901760 run.py:483] Algo bellman_ford step 5025 current loss 0.310170, current_train_items 160832.
I0302 19:00:49.663205 22535416901760 run.py:483] Algo bellman_ford step 5026 current loss 0.439835, current_train_items 160864.
I0302 19:00:49.685739 22535416901760 run.py:483] Algo bellman_ford step 5027 current loss 0.542131, current_train_items 160896.
I0302 19:00:49.715040 22535416901760 run.py:483] Algo bellman_ford step 5028 current loss 0.546385, current_train_items 160928.
I0302 19:00:49.747839 22535416901760 run.py:483] Algo bellman_ford step 5029 current loss 0.725076, current_train_items 160960.
I0302 19:00:49.767648 22535416901760 run.py:483] Algo bellman_ford step 5030 current loss 0.272947, current_train_items 160992.
I0302 19:00:49.783756 22535416901760 run.py:483] Algo bellman_ford step 5031 current loss 0.461195, current_train_items 161024.
I0302 19:00:49.807715 22535416901760 run.py:483] Algo bellman_ford step 5032 current loss 0.578768, current_train_items 161056.
I0302 19:00:49.837183 22535416901760 run.py:483] Algo bellman_ford step 5033 current loss 0.779024, current_train_items 161088.
I0302 19:00:49.871419 22535416901760 run.py:483] Algo bellman_ford step 5034 current loss 0.828814, current_train_items 161120.
I0302 19:00:49.890902 22535416901760 run.py:483] Algo bellman_ford step 5035 current loss 0.296413, current_train_items 161152.
I0302 19:00:49.907118 22535416901760 run.py:483] Algo bellman_ford step 5036 current loss 0.447875, current_train_items 161184.
I0302 19:00:49.930547 22535416901760 run.py:483] Algo bellman_ford step 5037 current loss 0.621431, current_train_items 161216.
I0302 19:00:49.962385 22535416901760 run.py:483] Algo bellman_ford step 5038 current loss 0.727396, current_train_items 161248.
I0302 19:00:49.995745 22535416901760 run.py:483] Algo bellman_ford step 5039 current loss 0.824939, current_train_items 161280.
I0302 19:00:50.015545 22535416901760 run.py:483] Algo bellman_ford step 5040 current loss 0.313856, current_train_items 161312.
I0302 19:00:50.031132 22535416901760 run.py:483] Algo bellman_ford step 5041 current loss 0.447838, current_train_items 161344.
I0302 19:00:50.055235 22535416901760 run.py:483] Algo bellman_ford step 5042 current loss 0.709416, current_train_items 161376.
I0302 19:00:50.084727 22535416901760 run.py:483] Algo bellman_ford step 5043 current loss 0.617801, current_train_items 161408.
I0302 19:00:50.117777 22535416901760 run.py:483] Algo bellman_ford step 5044 current loss 0.791816, current_train_items 161440.
I0302 19:00:50.137138 22535416901760 run.py:483] Algo bellman_ford step 5045 current loss 0.292574, current_train_items 161472.
I0302 19:00:50.153172 22535416901760 run.py:483] Algo bellman_ford step 5046 current loss 0.538805, current_train_items 161504.
I0302 19:00:50.175826 22535416901760 run.py:483] Algo bellman_ford step 5047 current loss 0.538967, current_train_items 161536.
I0302 19:00:50.206170 22535416901760 run.py:483] Algo bellman_ford step 5048 current loss 0.729391, current_train_items 161568.
I0302 19:00:50.239652 22535416901760 run.py:483] Algo bellman_ford step 5049 current loss 0.794561, current_train_items 161600.
I0302 19:00:50.258972 22535416901760 run.py:483] Algo bellman_ford step 5050 current loss 0.346316, current_train_items 161632.
I0302 19:00:50.266997 22535416901760 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:50.267104 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:00:50.283592 22535416901760 run.py:483] Algo bellman_ford step 5051 current loss 0.468307, current_train_items 161664.
I0302 19:00:50.307985 22535416901760 run.py:483] Algo bellman_ford step 5052 current loss 0.652267, current_train_items 161696.
I0302 19:00:50.338932 22535416901760 run.py:483] Algo bellman_ford step 5053 current loss 0.697043, current_train_items 161728.
I0302 19:00:50.371198 22535416901760 run.py:483] Algo bellman_ford step 5054 current loss 0.834166, current_train_items 161760.
I0302 19:00:50.391382 22535416901760 run.py:483] Algo bellman_ford step 5055 current loss 0.391299, current_train_items 161792.
I0302 19:00:50.408307 22535416901760 run.py:483] Algo bellman_ford step 5056 current loss 0.526497, current_train_items 161824.
I0302 19:00:50.432913 22535416901760 run.py:483] Algo bellman_ford step 5057 current loss 0.632370, current_train_items 161856.
I0302 19:00:50.463037 22535416901760 run.py:483] Algo bellman_ford step 5058 current loss 0.672461, current_train_items 161888.
I0302 19:00:50.496711 22535416901760 run.py:483] Algo bellman_ford step 5059 current loss 0.735758, current_train_items 161920.
I0302 19:00:50.516405 22535416901760 run.py:483] Algo bellman_ford step 5060 current loss 0.272537, current_train_items 161952.
I0302 19:00:50.532579 22535416901760 run.py:483] Algo bellman_ford step 5061 current loss 0.438323, current_train_items 161984.
I0302 19:00:50.554254 22535416901760 run.py:483] Algo bellman_ford step 5062 current loss 0.617495, current_train_items 162016.
I0302 19:00:50.584782 22535416901760 run.py:483] Algo bellman_ford step 5063 current loss 0.737689, current_train_items 162048.
I0302 19:00:50.616043 22535416901760 run.py:483] Algo bellman_ford step 5064 current loss 0.701051, current_train_items 162080.
I0302 19:00:50.635466 22535416901760 run.py:483] Algo bellman_ford step 5065 current loss 0.335020, current_train_items 162112.
I0302 19:00:50.651554 22535416901760 run.py:483] Algo bellman_ford step 5066 current loss 0.456582, current_train_items 162144.
I0302 19:00:50.675745 22535416901760 run.py:483] Algo bellman_ford step 5067 current loss 0.610202, current_train_items 162176.
I0302 19:00:50.706050 22535416901760 run.py:483] Algo bellman_ford step 5068 current loss 0.673913, current_train_items 162208.
I0302 19:00:50.740088 22535416901760 run.py:483] Algo bellman_ford step 5069 current loss 0.826460, current_train_items 162240.
I0302 19:00:50.759680 22535416901760 run.py:483] Algo bellman_ford step 5070 current loss 0.311751, current_train_items 162272.
I0302 19:00:50.776305 22535416901760 run.py:483] Algo bellman_ford step 5071 current loss 0.436376, current_train_items 162304.
I0302 19:00:50.798135 22535416901760 run.py:483] Algo bellman_ford step 5072 current loss 0.591524, current_train_items 162336.
I0302 19:00:50.829421 22535416901760 run.py:483] Algo bellman_ford step 5073 current loss 0.659144, current_train_items 162368.
I0302 19:00:50.860193 22535416901760 run.py:483] Algo bellman_ford step 5074 current loss 0.666151, current_train_items 162400.
I0302 19:00:50.879804 22535416901760 run.py:483] Algo bellman_ford step 5075 current loss 0.339702, current_train_items 162432.
I0302 19:00:50.895904 22535416901760 run.py:483] Algo bellman_ford step 5076 current loss 0.463362, current_train_items 162464.
I0302 19:00:50.919113 22535416901760 run.py:483] Algo bellman_ford step 5077 current loss 0.524276, current_train_items 162496.
I0302 19:00:50.949508 22535416901760 run.py:483] Algo bellman_ford step 5078 current loss 0.695556, current_train_items 162528.
I0302 19:00:50.981470 22535416901760 run.py:483] Algo bellman_ford step 5079 current loss 0.891203, current_train_items 162560.
I0302 19:00:51.000814 22535416901760 run.py:483] Algo bellman_ford step 5080 current loss 0.298096, current_train_items 162592.
I0302 19:00:51.016635 22535416901760 run.py:483] Algo bellman_ford step 5081 current loss 0.374443, current_train_items 162624.
I0302 19:00:51.039909 22535416901760 run.py:483] Algo bellman_ford step 5082 current loss 0.598683, current_train_items 162656.
I0302 19:00:51.070888 22535416901760 run.py:483] Algo bellman_ford step 5083 current loss 0.639758, current_train_items 162688.
I0302 19:00:51.104354 22535416901760 run.py:483] Algo bellman_ford step 5084 current loss 0.722127, current_train_items 162720.
I0302 19:00:51.124167 22535416901760 run.py:483] Algo bellman_ford step 5085 current loss 0.310306, current_train_items 162752.
I0302 19:00:51.140219 22535416901760 run.py:483] Algo bellman_ford step 5086 current loss 0.500127, current_train_items 162784.
I0302 19:00:51.163541 22535416901760 run.py:483] Algo bellman_ford step 5087 current loss 0.661852, current_train_items 162816.
I0302 19:00:51.194636 22535416901760 run.py:483] Algo bellman_ford step 5088 current loss 0.581703, current_train_items 162848.
I0302 19:00:51.228929 22535416901760 run.py:483] Algo bellman_ford step 5089 current loss 0.708252, current_train_items 162880.
I0302 19:00:51.248349 22535416901760 run.py:483] Algo bellman_ford step 5090 current loss 0.279161, current_train_items 162912.
I0302 19:00:51.264713 22535416901760 run.py:483] Algo bellman_ford step 5091 current loss 0.468535, current_train_items 162944.
I0302 19:00:51.286435 22535416901760 run.py:483] Algo bellman_ford step 5092 current loss 0.616993, current_train_items 162976.
I0302 19:00:51.316147 22535416901760 run.py:483] Algo bellman_ford step 5093 current loss 0.726336, current_train_items 163008.
I0302 19:00:51.348391 22535416901760 run.py:483] Algo bellman_ford step 5094 current loss 0.712196, current_train_items 163040.
I0302 19:00:51.368126 22535416901760 run.py:483] Algo bellman_ford step 5095 current loss 0.297192, current_train_items 163072.
I0302 19:00:51.384266 22535416901760 run.py:483] Algo bellman_ford step 5096 current loss 0.530894, current_train_items 163104.
I0302 19:00:51.408332 22535416901760 run.py:483] Algo bellman_ford step 5097 current loss 0.668216, current_train_items 163136.
I0302 19:00:51.439965 22535416901760 run.py:483] Algo bellman_ford step 5098 current loss 0.744212, current_train_items 163168.
I0302 19:00:51.474529 22535416901760 run.py:483] Algo bellman_ford step 5099 current loss 0.818158, current_train_items 163200.
I0302 19:00:51.494239 22535416901760 run.py:483] Algo bellman_ford step 5100 current loss 0.262881, current_train_items 163232.
I0302 19:00:51.502164 22535416901760 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:51.502270 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:00:51.518642 22535416901760 run.py:483] Algo bellman_ford step 5101 current loss 0.471880, current_train_items 163264.
I0302 19:00:51.540617 22535416901760 run.py:483] Algo bellman_ford step 5102 current loss 0.501311, current_train_items 163296.
I0302 19:00:51.572145 22535416901760 run.py:483] Algo bellman_ford step 5103 current loss 0.641991, current_train_items 163328.
I0302 19:00:51.606701 22535416901760 run.py:483] Algo bellman_ford step 5104 current loss 0.816048, current_train_items 163360.
I0302 19:00:51.626679 22535416901760 run.py:483] Algo bellman_ford step 5105 current loss 0.285767, current_train_items 163392.
I0302 19:00:51.642911 22535416901760 run.py:483] Algo bellman_ford step 5106 current loss 0.519528, current_train_items 163424.
I0302 19:00:51.666586 22535416901760 run.py:483] Algo bellman_ford step 5107 current loss 0.574543, current_train_items 163456.
I0302 19:00:51.696225 22535416901760 run.py:483] Algo bellman_ford step 5108 current loss 0.648936, current_train_items 163488.
I0302 19:00:51.730257 22535416901760 run.py:483] Algo bellman_ford step 5109 current loss 0.807590, current_train_items 163520.
I0302 19:00:51.750076 22535416901760 run.py:483] Algo bellman_ford step 5110 current loss 0.301694, current_train_items 163552.
I0302 19:00:51.766310 22535416901760 run.py:483] Algo bellman_ford step 5111 current loss 0.486442, current_train_items 163584.
I0302 19:00:51.789233 22535416901760 run.py:483] Algo bellman_ford step 5112 current loss 0.540468, current_train_items 163616.
I0302 19:00:51.819398 22535416901760 run.py:483] Algo bellman_ford step 5113 current loss 0.593292, current_train_items 163648.
I0302 19:00:51.852207 22535416901760 run.py:483] Algo bellman_ford step 5114 current loss 0.651607, current_train_items 163680.
I0302 19:00:51.871729 22535416901760 run.py:483] Algo bellman_ford step 5115 current loss 0.308916, current_train_items 163712.
I0302 19:00:51.888052 22535416901760 run.py:483] Algo bellman_ford step 5116 current loss 0.428253, current_train_items 163744.
I0302 19:00:51.911939 22535416901760 run.py:483] Algo bellman_ford step 5117 current loss 0.631359, current_train_items 163776.
I0302 19:00:51.941667 22535416901760 run.py:483] Algo bellman_ford step 5118 current loss 0.632486, current_train_items 163808.
I0302 19:00:51.975937 22535416901760 run.py:483] Algo bellman_ford step 5119 current loss 0.687544, current_train_items 163840.
I0302 19:00:51.995402 22535416901760 run.py:483] Algo bellman_ford step 5120 current loss 0.293408, current_train_items 163872.
I0302 19:00:52.011168 22535416901760 run.py:483] Algo bellman_ford step 5121 current loss 0.427222, current_train_items 163904.
I0302 19:00:52.034461 22535416901760 run.py:483] Algo bellman_ford step 5122 current loss 0.547109, current_train_items 163936.
I0302 19:00:52.063946 22535416901760 run.py:483] Algo bellman_ford step 5123 current loss 0.629221, current_train_items 163968.
I0302 19:00:52.097404 22535416901760 run.py:483] Algo bellman_ford step 5124 current loss 0.706978, current_train_items 164000.
I0302 19:00:52.116637 22535416901760 run.py:483] Algo bellman_ford step 5125 current loss 0.333227, current_train_items 164032.
I0302 19:00:52.132776 22535416901760 run.py:483] Algo bellman_ford step 5126 current loss 0.468597, current_train_items 164064.
I0302 19:00:52.156378 22535416901760 run.py:483] Algo bellman_ford step 5127 current loss 0.740679, current_train_items 164096.
I0302 19:00:52.187352 22535416901760 run.py:483] Algo bellman_ford step 5128 current loss 0.748433, current_train_items 164128.
I0302 19:00:52.221945 22535416901760 run.py:483] Algo bellman_ford step 5129 current loss 0.883880, current_train_items 164160.
I0302 19:00:52.241568 22535416901760 run.py:483] Algo bellman_ford step 5130 current loss 0.350435, current_train_items 164192.
I0302 19:00:52.257899 22535416901760 run.py:483] Algo bellman_ford step 5131 current loss 0.489517, current_train_items 164224.
I0302 19:00:52.280228 22535416901760 run.py:483] Algo bellman_ford step 5132 current loss 0.708198, current_train_items 164256.
I0302 19:00:52.311821 22535416901760 run.py:483] Algo bellman_ford step 5133 current loss 0.702479, current_train_items 164288.
I0302 19:00:52.344560 22535416901760 run.py:483] Algo bellman_ford step 5134 current loss 0.763742, current_train_items 164320.
I0302 19:00:52.364467 22535416901760 run.py:483] Algo bellman_ford step 5135 current loss 0.336030, current_train_items 164352.
I0302 19:00:52.380480 22535416901760 run.py:483] Algo bellman_ford step 5136 current loss 0.461941, current_train_items 164384.
I0302 19:00:52.404083 22535416901760 run.py:483] Algo bellman_ford step 5137 current loss 0.687580, current_train_items 164416.
I0302 19:00:52.434327 22535416901760 run.py:483] Algo bellman_ford step 5138 current loss 0.721092, current_train_items 164448.
I0302 19:00:52.468003 22535416901760 run.py:483] Algo bellman_ford step 5139 current loss 0.785309, current_train_items 164480.
I0302 19:00:52.487560 22535416901760 run.py:483] Algo bellman_ford step 5140 current loss 0.324503, current_train_items 164512.
I0302 19:00:52.503928 22535416901760 run.py:483] Algo bellman_ford step 5141 current loss 0.477168, current_train_items 164544.
I0302 19:00:52.527901 22535416901760 run.py:483] Algo bellman_ford step 5142 current loss 0.610263, current_train_items 164576.
I0302 19:00:52.558643 22535416901760 run.py:483] Algo bellman_ford step 5143 current loss 0.682314, current_train_items 164608.
I0302 19:00:52.591575 22535416901760 run.py:483] Algo bellman_ford step 5144 current loss 0.807060, current_train_items 164640.
I0302 19:00:52.610818 22535416901760 run.py:483] Algo bellman_ford step 5145 current loss 0.273896, current_train_items 164672.
I0302 19:00:52.626690 22535416901760 run.py:483] Algo bellman_ford step 5146 current loss 0.561560, current_train_items 164704.
I0302 19:00:52.648989 22535416901760 run.py:483] Algo bellman_ford step 5147 current loss 0.742245, current_train_items 164736.
I0302 19:00:52.678964 22535416901760 run.py:483] Algo bellman_ford step 5148 current loss 0.705143, current_train_items 164768.
I0302 19:00:52.711814 22535416901760 run.py:483] Algo bellman_ford step 5149 current loss 0.813627, current_train_items 164800.
I0302 19:00:52.731219 22535416901760 run.py:483] Algo bellman_ford step 5150 current loss 0.245547, current_train_items 164832.
I0302 19:00:52.739143 22535416901760 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:52.739258 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:00:52.756046 22535416901760 run.py:483] Algo bellman_ford step 5151 current loss 0.497319, current_train_items 164864.
I0302 19:00:52.779533 22535416901760 run.py:483] Algo bellman_ford step 5152 current loss 0.653866, current_train_items 164896.
I0302 19:00:52.809281 22535416901760 run.py:483] Algo bellman_ford step 5153 current loss 0.895266, current_train_items 164928.
I0302 19:00:52.841966 22535416901760 run.py:483] Algo bellman_ford step 5154 current loss 0.965478, current_train_items 164960.
I0302 19:00:52.861855 22535416901760 run.py:483] Algo bellman_ford step 5155 current loss 0.326729, current_train_items 164992.
I0302 19:00:52.877655 22535416901760 run.py:483] Algo bellman_ford step 5156 current loss 0.460227, current_train_items 165024.
I0302 19:00:52.901777 22535416901760 run.py:483] Algo bellman_ford step 5157 current loss 0.609991, current_train_items 165056.
I0302 19:00:52.934047 22535416901760 run.py:483] Algo bellman_ford step 5158 current loss 0.765462, current_train_items 165088.
I0302 19:00:52.966246 22535416901760 run.py:483] Algo bellman_ford step 5159 current loss 0.800501, current_train_items 165120.
I0302 19:00:52.985951 22535416901760 run.py:483] Algo bellman_ford step 5160 current loss 0.278100, current_train_items 165152.
I0302 19:00:53.001880 22535416901760 run.py:483] Algo bellman_ford step 5161 current loss 0.385944, current_train_items 165184.
I0302 19:00:53.025946 22535416901760 run.py:483] Algo bellman_ford step 5162 current loss 0.686000, current_train_items 165216.
I0302 19:00:53.057713 22535416901760 run.py:483] Algo bellman_ford step 5163 current loss 0.711676, current_train_items 165248.
I0302 19:00:53.089657 22535416901760 run.py:483] Algo bellman_ford step 5164 current loss 0.736122, current_train_items 165280.
I0302 19:00:53.108905 22535416901760 run.py:483] Algo bellman_ford step 5165 current loss 0.296674, current_train_items 165312.
I0302 19:00:53.124812 22535416901760 run.py:483] Algo bellman_ford step 5166 current loss 0.450965, current_train_items 165344.
I0302 19:00:53.147030 22535416901760 run.py:483] Algo bellman_ford step 5167 current loss 0.602756, current_train_items 165376.
I0302 19:00:53.178106 22535416901760 run.py:483] Algo bellman_ford step 5168 current loss 0.641813, current_train_items 165408.
I0302 19:00:53.212730 22535416901760 run.py:483] Algo bellman_ford step 5169 current loss 0.723557, current_train_items 165440.
I0302 19:00:53.232386 22535416901760 run.py:483] Algo bellman_ford step 5170 current loss 0.322030, current_train_items 165472.
I0302 19:00:53.248652 22535416901760 run.py:483] Algo bellman_ford step 5171 current loss 0.423732, current_train_items 165504.
I0302 19:00:53.272096 22535416901760 run.py:483] Algo bellman_ford step 5172 current loss 0.639194, current_train_items 165536.
I0302 19:00:53.303799 22535416901760 run.py:483] Algo bellman_ford step 5173 current loss 0.739453, current_train_items 165568.
I0302 19:00:53.336212 22535416901760 run.py:483] Algo bellman_ford step 5174 current loss 0.677202, current_train_items 165600.
I0302 19:00:53.355592 22535416901760 run.py:483] Algo bellman_ford step 5175 current loss 0.268180, current_train_items 165632.
I0302 19:00:53.372112 22535416901760 run.py:483] Algo bellman_ford step 5176 current loss 0.386252, current_train_items 165664.
I0302 19:00:53.395555 22535416901760 run.py:483] Algo bellman_ford step 5177 current loss 0.567256, current_train_items 165696.
I0302 19:00:53.425886 22535416901760 run.py:483] Algo bellman_ford step 5178 current loss 0.642456, current_train_items 165728.
I0302 19:00:53.457831 22535416901760 run.py:483] Algo bellman_ford step 5179 current loss 0.815479, current_train_items 165760.
I0302 19:00:53.477055 22535416901760 run.py:483] Algo bellman_ford step 5180 current loss 0.330022, current_train_items 165792.
I0302 19:00:53.493044 22535416901760 run.py:483] Algo bellman_ford step 5181 current loss 0.445675, current_train_items 165824.
I0302 19:00:53.516194 22535416901760 run.py:483] Algo bellman_ford step 5182 current loss 0.573943, current_train_items 165856.
I0302 19:00:53.546514 22535416901760 run.py:483] Algo bellman_ford step 5183 current loss 0.662417, current_train_items 165888.
I0302 19:00:53.578383 22535416901760 run.py:483] Algo bellman_ford step 5184 current loss 0.680824, current_train_items 165920.
I0302 19:00:53.598075 22535416901760 run.py:483] Algo bellman_ford step 5185 current loss 0.336310, current_train_items 165952.
I0302 19:00:53.614613 22535416901760 run.py:483] Algo bellman_ford step 5186 current loss 0.490808, current_train_items 165984.
I0302 19:00:53.636573 22535416901760 run.py:483] Algo bellman_ford step 5187 current loss 0.581760, current_train_items 166016.
I0302 19:00:53.667634 22535416901760 run.py:483] Algo bellman_ford step 5188 current loss 0.719175, current_train_items 166048.
I0302 19:00:53.699599 22535416901760 run.py:483] Algo bellman_ford step 5189 current loss 0.781988, current_train_items 166080.
I0302 19:00:53.719534 22535416901760 run.py:483] Algo bellman_ford step 5190 current loss 0.324854, current_train_items 166112.
I0302 19:00:53.735513 22535416901760 run.py:483] Algo bellman_ford step 5191 current loss 0.504258, current_train_items 166144.
I0302 19:00:53.758987 22535416901760 run.py:483] Algo bellman_ford step 5192 current loss 0.702742, current_train_items 166176.
I0302 19:00:53.788686 22535416901760 run.py:483] Algo bellman_ford step 5193 current loss 0.744373, current_train_items 166208.
I0302 19:00:53.821705 22535416901760 run.py:483] Algo bellman_ford step 5194 current loss 0.830824, current_train_items 166240.
I0302 19:00:53.841034 22535416901760 run.py:483] Algo bellman_ford step 5195 current loss 0.308468, current_train_items 166272.
I0302 19:00:53.857135 22535416901760 run.py:483] Algo bellman_ford step 5196 current loss 0.458113, current_train_items 166304.
I0302 19:00:53.880670 22535416901760 run.py:483] Algo bellman_ford step 5197 current loss 0.660196, current_train_items 166336.
I0302 19:00:53.912570 22535416901760 run.py:483] Algo bellman_ford step 5198 current loss 0.788821, current_train_items 166368.
I0302 19:00:53.944960 22535416901760 run.py:483] Algo bellman_ford step 5199 current loss 0.868987, current_train_items 166400.
I0302 19:00:53.964571 22535416901760 run.py:483] Algo bellman_ford step 5200 current loss 0.334502, current_train_items 166432.
I0302 19:00:53.972375 22535416901760 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:53.972481 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:00:53.989455 22535416901760 run.py:483] Algo bellman_ford step 5201 current loss 0.470664, current_train_items 166464.
I0302 19:00:54.014014 22535416901760 run.py:483] Algo bellman_ford step 5202 current loss 0.665114, current_train_items 166496.
I0302 19:00:54.046527 22535416901760 run.py:483] Algo bellman_ford step 5203 current loss 0.757191, current_train_items 166528.
I0302 19:00:54.081558 22535416901760 run.py:483] Algo bellman_ford step 5204 current loss 0.696486, current_train_items 166560.
I0302 19:00:54.101401 22535416901760 run.py:483] Algo bellman_ford step 5205 current loss 0.266092, current_train_items 166592.
I0302 19:00:54.117112 22535416901760 run.py:483] Algo bellman_ford step 5206 current loss 0.494370, current_train_items 166624.
I0302 19:00:54.140607 22535416901760 run.py:483] Algo bellman_ford step 5207 current loss 0.641145, current_train_items 166656.
I0302 19:00:54.170178 22535416901760 run.py:483] Algo bellman_ford step 5208 current loss 0.573178, current_train_items 166688.
I0302 19:00:54.206025 22535416901760 run.py:483] Algo bellman_ford step 5209 current loss 0.845703, current_train_items 166720.
I0302 19:00:54.225564 22535416901760 run.py:483] Algo bellman_ford step 5210 current loss 0.361774, current_train_items 166752.
I0302 19:00:54.241178 22535416901760 run.py:483] Algo bellman_ford step 5211 current loss 0.439302, current_train_items 166784.
I0302 19:00:54.265010 22535416901760 run.py:483] Algo bellman_ford step 5212 current loss 0.562037, current_train_items 166816.
I0302 19:00:54.297827 22535416901760 run.py:483] Algo bellman_ford step 5213 current loss 0.699495, current_train_items 166848.
I0302 19:00:54.330602 22535416901760 run.py:483] Algo bellman_ford step 5214 current loss 0.720306, current_train_items 166880.
I0302 19:00:54.349742 22535416901760 run.py:483] Algo bellman_ford step 5215 current loss 0.329241, current_train_items 166912.
I0302 19:00:54.366109 22535416901760 run.py:483] Algo bellman_ford step 5216 current loss 0.463325, current_train_items 166944.
I0302 19:00:54.389708 22535416901760 run.py:483] Algo bellman_ford step 5217 current loss 0.627272, current_train_items 166976.
I0302 19:00:54.419590 22535416901760 run.py:483] Algo bellman_ford step 5218 current loss 0.661760, current_train_items 167008.
I0302 19:00:54.453090 22535416901760 run.py:483] Algo bellman_ford step 5219 current loss 0.826770, current_train_items 167040.
I0302 19:00:54.472421 22535416901760 run.py:483] Algo bellman_ford step 5220 current loss 0.351006, current_train_items 167072.
I0302 19:00:54.488391 22535416901760 run.py:483] Algo bellman_ford step 5221 current loss 0.448699, current_train_items 167104.
I0302 19:00:54.511108 22535416901760 run.py:483] Algo bellman_ford step 5222 current loss 0.612504, current_train_items 167136.
I0302 19:00:54.542798 22535416901760 run.py:483] Algo bellman_ford step 5223 current loss 0.723740, current_train_items 167168.
I0302 19:00:54.575587 22535416901760 run.py:483] Algo bellman_ford step 5224 current loss 0.728879, current_train_items 167200.
I0302 19:00:54.595412 22535416901760 run.py:483] Algo bellman_ford step 5225 current loss 0.325341, current_train_items 167232.
I0302 19:00:54.611331 22535416901760 run.py:483] Algo bellman_ford step 5226 current loss 0.426175, current_train_items 167264.
I0302 19:00:54.635197 22535416901760 run.py:483] Algo bellman_ford step 5227 current loss 0.604186, current_train_items 167296.
I0302 19:00:54.667141 22535416901760 run.py:483] Algo bellman_ford step 5228 current loss 0.689840, current_train_items 167328.
I0302 19:00:54.701802 22535416901760 run.py:483] Algo bellman_ford step 5229 current loss 0.842277, current_train_items 167360.
I0302 19:00:54.721607 22535416901760 run.py:483] Algo bellman_ford step 5230 current loss 0.305194, current_train_items 167392.
I0302 19:00:54.737804 22535416901760 run.py:483] Algo bellman_ford step 5231 current loss 0.464689, current_train_items 167424.
I0302 19:00:54.762110 22535416901760 run.py:483] Algo bellman_ford step 5232 current loss 0.554819, current_train_items 167456.
I0302 19:00:54.792708 22535416901760 run.py:483] Algo bellman_ford step 5233 current loss 0.600136, current_train_items 167488.
I0302 19:00:54.827490 22535416901760 run.py:483] Algo bellman_ford step 5234 current loss 0.769956, current_train_items 167520.
I0302 19:00:54.847090 22535416901760 run.py:483] Algo bellman_ford step 5235 current loss 0.327777, current_train_items 167552.
I0302 19:00:54.863057 22535416901760 run.py:483] Algo bellman_ford step 5236 current loss 0.502283, current_train_items 167584.
I0302 19:00:54.886906 22535416901760 run.py:483] Algo bellman_ford step 5237 current loss 0.541415, current_train_items 167616.
I0302 19:00:54.919354 22535416901760 run.py:483] Algo bellman_ford step 5238 current loss 0.671475, current_train_items 167648.
I0302 19:00:54.953875 22535416901760 run.py:483] Algo bellman_ford step 5239 current loss 0.865739, current_train_items 167680.
I0302 19:00:54.973678 22535416901760 run.py:483] Algo bellman_ford step 5240 current loss 0.345617, current_train_items 167712.
I0302 19:00:54.989889 22535416901760 run.py:483] Algo bellman_ford step 5241 current loss 0.450230, current_train_items 167744.
I0302 19:00:55.013407 22535416901760 run.py:483] Algo bellman_ford step 5242 current loss 0.738255, current_train_items 167776.
I0302 19:00:55.044617 22535416901760 run.py:483] Algo bellman_ford step 5243 current loss 0.819269, current_train_items 167808.
I0302 19:00:55.075731 22535416901760 run.py:483] Algo bellman_ford step 5244 current loss 0.728675, current_train_items 167840.
I0302 19:00:55.095204 22535416901760 run.py:483] Algo bellman_ford step 5245 current loss 0.325868, current_train_items 167872.
I0302 19:00:55.111394 22535416901760 run.py:483] Algo bellman_ford step 5246 current loss 0.525527, current_train_items 167904.
I0302 19:00:55.135759 22535416901760 run.py:483] Algo bellman_ford step 5247 current loss 0.573861, current_train_items 167936.
I0302 19:00:55.167490 22535416901760 run.py:483] Algo bellman_ford step 5248 current loss 0.666703, current_train_items 167968.
I0302 19:00:55.201441 22535416901760 run.py:483] Algo bellman_ford step 5249 current loss 0.778793, current_train_items 168000.
I0302 19:00:55.221047 22535416901760 run.py:483] Algo bellman_ford step 5250 current loss 0.242748, current_train_items 168032.
I0302 19:00:55.229294 22535416901760 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9072265625, 'score': 0.9072265625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:55.229399 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.907, val scores are: bellman_ford: 0.907
I0302 19:00:55.246054 22535416901760 run.py:483] Algo bellman_ford step 5251 current loss 0.464503, current_train_items 168064.
I0302 19:00:55.270380 22535416901760 run.py:483] Algo bellman_ford step 5252 current loss 0.679406, current_train_items 168096.
I0302 19:00:55.302561 22535416901760 run.py:483] Algo bellman_ford step 5253 current loss 0.649916, current_train_items 168128.
I0302 19:00:55.333944 22535416901760 run.py:483] Algo bellman_ford step 5254 current loss 0.657339, current_train_items 168160.
I0302 19:00:55.353958 22535416901760 run.py:483] Algo bellman_ford step 5255 current loss 0.308399, current_train_items 168192.
I0302 19:00:55.369894 22535416901760 run.py:483] Algo bellman_ford step 5256 current loss 0.443716, current_train_items 168224.
I0302 19:00:55.392815 22535416901760 run.py:483] Algo bellman_ford step 5257 current loss 0.582161, current_train_items 168256.
I0302 19:00:55.424469 22535416901760 run.py:483] Algo bellman_ford step 5258 current loss 0.784738, current_train_items 168288.
I0302 19:00:55.455952 22535416901760 run.py:483] Algo bellman_ford step 5259 current loss 0.629663, current_train_items 168320.
I0302 19:00:55.475786 22535416901760 run.py:483] Algo bellman_ford step 5260 current loss 0.288831, current_train_items 168352.
I0302 19:00:55.492215 22535416901760 run.py:483] Algo bellman_ford step 5261 current loss 0.478157, current_train_items 168384.
I0302 19:00:55.514952 22535416901760 run.py:483] Algo bellman_ford step 5262 current loss 0.587148, current_train_items 168416.
I0302 19:00:55.546971 22535416901760 run.py:483] Algo bellman_ford step 5263 current loss 0.670868, current_train_items 168448.
I0302 19:00:55.579346 22535416901760 run.py:483] Algo bellman_ford step 5264 current loss 0.681592, current_train_items 168480.
I0302 19:00:55.599295 22535416901760 run.py:483] Algo bellman_ford step 5265 current loss 0.214182, current_train_items 168512.
I0302 19:00:55.615528 22535416901760 run.py:483] Algo bellman_ford step 5266 current loss 0.524144, current_train_items 168544.
I0302 19:00:55.640056 22535416901760 run.py:483] Algo bellman_ford step 5267 current loss 0.625374, current_train_items 168576.
I0302 19:00:55.670310 22535416901760 run.py:483] Algo bellman_ford step 5268 current loss 0.628902, current_train_items 168608.
I0302 19:00:55.702005 22535416901760 run.py:483] Algo bellman_ford step 5269 current loss 0.685690, current_train_items 168640.
I0302 19:00:55.721507 22535416901760 run.py:483] Algo bellman_ford step 5270 current loss 0.328027, current_train_items 168672.
I0302 19:00:55.737476 22535416901760 run.py:483] Algo bellman_ford step 5271 current loss 0.527357, current_train_items 168704.
I0302 19:00:55.759975 22535416901760 run.py:483] Algo bellman_ford step 5272 current loss 0.578138, current_train_items 168736.
I0302 19:00:55.790746 22535416901760 run.py:483] Algo bellman_ford step 5273 current loss 0.600024, current_train_items 168768.
I0302 19:00:55.822930 22535416901760 run.py:483] Algo bellman_ford step 5274 current loss 0.636778, current_train_items 168800.
I0302 19:00:55.842981 22535416901760 run.py:483] Algo bellman_ford step 5275 current loss 0.343336, current_train_items 168832.
I0302 19:00:55.859308 22535416901760 run.py:483] Algo bellman_ford step 5276 current loss 0.524025, current_train_items 168864.
I0302 19:00:55.882151 22535416901760 run.py:483] Algo bellman_ford step 5277 current loss 0.659840, current_train_items 168896.
I0302 19:00:55.912826 22535416901760 run.py:483] Algo bellman_ford step 5278 current loss 0.684449, current_train_items 168928.
I0302 19:00:55.946671 22535416901760 run.py:483] Algo bellman_ford step 5279 current loss 0.915616, current_train_items 168960.
I0302 19:00:55.966033 22535416901760 run.py:483] Algo bellman_ford step 5280 current loss 0.362820, current_train_items 168992.
I0302 19:00:55.982472 22535416901760 run.py:483] Algo bellman_ford step 5281 current loss 0.553612, current_train_items 169024.
I0302 19:00:56.007149 22535416901760 run.py:483] Algo bellman_ford step 5282 current loss 0.660591, current_train_items 169056.
I0302 19:00:56.038144 22535416901760 run.py:483] Algo bellman_ford step 5283 current loss 0.713639, current_train_items 169088.
I0302 19:00:56.070819 22535416901760 run.py:483] Algo bellman_ford step 5284 current loss 0.870838, current_train_items 169120.
I0302 19:00:56.090812 22535416901760 run.py:483] Algo bellman_ford step 5285 current loss 0.276878, current_train_items 169152.
I0302 19:00:56.107059 22535416901760 run.py:483] Algo bellman_ford step 5286 current loss 0.449708, current_train_items 169184.
I0302 19:00:56.131448 22535416901760 run.py:483] Algo bellman_ford step 5287 current loss 0.634620, current_train_items 169216.
I0302 19:00:56.162171 22535416901760 run.py:483] Algo bellman_ford step 5288 current loss 0.655902, current_train_items 169248.
I0302 19:00:56.196267 22535416901760 run.py:483] Algo bellman_ford step 5289 current loss 0.795887, current_train_items 169280.
I0302 19:00:56.215820 22535416901760 run.py:483] Algo bellman_ford step 5290 current loss 0.344319, current_train_items 169312.
I0302 19:00:56.232422 22535416901760 run.py:483] Algo bellman_ford step 5291 current loss 0.446110, current_train_items 169344.
I0302 19:00:56.255448 22535416901760 run.py:483] Algo bellman_ford step 5292 current loss 0.566157, current_train_items 169376.
I0302 19:00:56.286297 22535416901760 run.py:483] Algo bellman_ford step 5293 current loss 0.593642, current_train_items 169408.
I0302 19:00:56.318897 22535416901760 run.py:483] Algo bellman_ford step 5294 current loss 0.730074, current_train_items 169440.
I0302 19:00:56.338329 22535416901760 run.py:483] Algo bellman_ford step 5295 current loss 0.260659, current_train_items 169472.
I0302 19:00:56.354286 22535416901760 run.py:483] Algo bellman_ford step 5296 current loss 0.485952, current_train_items 169504.
I0302 19:00:56.377148 22535416901760 run.py:483] Algo bellman_ford step 5297 current loss 0.549100, current_train_items 169536.
I0302 19:00:56.409197 22535416901760 run.py:483] Algo bellman_ford step 5298 current loss 0.632810, current_train_items 169568.
I0302 19:00:56.444846 22535416901760 run.py:483] Algo bellman_ford step 5299 current loss 0.819735, current_train_items 169600.
I0302 19:00:56.464730 22535416901760 run.py:483] Algo bellman_ford step 5300 current loss 0.361862, current_train_items 169632.
I0302 19:00:56.472471 22535416901760 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.900390625, 'score': 0.900390625, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:56.472575 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.900, val scores are: bellman_ford: 0.900
I0302 19:00:56.488978 22535416901760 run.py:483] Algo bellman_ford step 5301 current loss 0.399316, current_train_items 169664.
I0302 19:00:56.513369 22535416901760 run.py:483] Algo bellman_ford step 5302 current loss 0.633430, current_train_items 169696.
I0302 19:00:56.545422 22535416901760 run.py:483] Algo bellman_ford step 5303 current loss 0.630100, current_train_items 169728.
I0302 19:00:56.579768 22535416901760 run.py:483] Algo bellman_ford step 5304 current loss 0.777600, current_train_items 169760.
I0302 19:00:56.599702 22535416901760 run.py:483] Algo bellman_ford step 5305 current loss 0.296589, current_train_items 169792.
I0302 19:00:56.615701 22535416901760 run.py:483] Algo bellman_ford step 5306 current loss 0.494147, current_train_items 169824.
I0302 19:00:56.640231 22535416901760 run.py:483] Algo bellman_ford step 5307 current loss 0.590408, current_train_items 169856.
I0302 19:00:56.671871 22535416901760 run.py:483] Algo bellman_ford step 5308 current loss 0.689607, current_train_items 169888.
I0302 19:00:56.703867 22535416901760 run.py:483] Algo bellman_ford step 5309 current loss 0.705915, current_train_items 169920.
I0302 19:00:56.723225 22535416901760 run.py:483] Algo bellman_ford step 5310 current loss 0.307085, current_train_items 169952.
I0302 19:00:56.738815 22535416901760 run.py:483] Algo bellman_ford step 5311 current loss 0.386056, current_train_items 169984.
I0302 19:00:56.762295 22535416901760 run.py:483] Algo bellman_ford step 5312 current loss 0.549385, current_train_items 170016.
I0302 19:00:56.793121 22535416901760 run.py:483] Algo bellman_ford step 5313 current loss 0.675682, current_train_items 170048.
I0302 19:00:56.827276 22535416901760 run.py:483] Algo bellman_ford step 5314 current loss 0.717230, current_train_items 170080.
I0302 19:00:56.847074 22535416901760 run.py:483] Algo bellman_ford step 5315 current loss 0.339429, current_train_items 170112.
I0302 19:00:56.863150 22535416901760 run.py:483] Algo bellman_ford step 5316 current loss 0.435926, current_train_items 170144.
I0302 19:00:56.886509 22535416901760 run.py:483] Algo bellman_ford step 5317 current loss 0.598707, current_train_items 170176.
I0302 19:00:56.915844 22535416901760 run.py:483] Algo bellman_ford step 5318 current loss 0.570804, current_train_items 170208.
I0302 19:00:56.949583 22535416901760 run.py:483] Algo bellman_ford step 5319 current loss 0.755246, current_train_items 170240.
I0302 19:00:56.968959 22535416901760 run.py:483] Algo bellman_ford step 5320 current loss 0.342815, current_train_items 170272.
I0302 19:00:56.984967 22535416901760 run.py:483] Algo bellman_ford step 5321 current loss 0.451299, current_train_items 170304.
I0302 19:00:57.009690 22535416901760 run.py:483] Algo bellman_ford step 5322 current loss 0.584735, current_train_items 170336.
I0302 19:00:57.039175 22535416901760 run.py:483] Algo bellman_ford step 5323 current loss 0.636660, current_train_items 170368.
I0302 19:00:57.073462 22535416901760 run.py:483] Algo bellman_ford step 5324 current loss 0.753476, current_train_items 170400.
I0302 19:00:57.093248 22535416901760 run.py:483] Algo bellman_ford step 5325 current loss 0.320277, current_train_items 170432.
I0302 19:00:57.109326 22535416901760 run.py:483] Algo bellman_ford step 5326 current loss 0.457803, current_train_items 170464.
I0302 19:00:57.132313 22535416901760 run.py:483] Algo bellman_ford step 5327 current loss 0.544000, current_train_items 170496.
I0302 19:00:57.164363 22535416901760 run.py:483] Algo bellman_ford step 5328 current loss 0.685365, current_train_items 170528.
I0302 19:00:57.196823 22535416901760 run.py:483] Algo bellman_ford step 5329 current loss 0.800448, current_train_items 170560.
I0302 19:00:57.216454 22535416901760 run.py:483] Algo bellman_ford step 5330 current loss 0.361044, current_train_items 170592.
I0302 19:00:57.232554 22535416901760 run.py:483] Algo bellman_ford step 5331 current loss 0.428916, current_train_items 170624.
I0302 19:00:57.256173 22535416901760 run.py:483] Algo bellman_ford step 5332 current loss 0.809272, current_train_items 170656.
I0302 19:00:57.285666 22535416901760 run.py:483] Algo bellman_ford step 5333 current loss 0.669388, current_train_items 170688.
I0302 19:00:57.317778 22535416901760 run.py:483] Algo bellman_ford step 5334 current loss 0.794529, current_train_items 170720.
I0302 19:00:57.337602 22535416901760 run.py:483] Algo bellman_ford step 5335 current loss 0.376048, current_train_items 170752.
I0302 19:00:57.353972 22535416901760 run.py:483] Algo bellman_ford step 5336 current loss 0.471611, current_train_items 170784.
I0302 19:00:57.377226 22535416901760 run.py:483] Algo bellman_ford step 5337 current loss 0.701385, current_train_items 170816.
I0302 19:00:57.407270 22535416901760 run.py:483] Algo bellman_ford step 5338 current loss 0.666521, current_train_items 170848.
I0302 19:00:57.441221 22535416901760 run.py:483] Algo bellman_ford step 5339 current loss 0.712307, current_train_items 170880.
I0302 19:00:57.460500 22535416901760 run.py:483] Algo bellman_ford step 5340 current loss 0.268010, current_train_items 170912.
I0302 19:00:57.476674 22535416901760 run.py:483] Algo bellman_ford step 5341 current loss 0.548047, current_train_items 170944.
I0302 19:00:57.501507 22535416901760 run.py:483] Algo bellman_ford step 5342 current loss 0.741067, current_train_items 170976.
I0302 19:00:57.532608 22535416901760 run.py:483] Algo bellman_ford step 5343 current loss 0.708213, current_train_items 171008.
I0302 19:00:57.565825 22535416901760 run.py:483] Algo bellman_ford step 5344 current loss 0.696121, current_train_items 171040.
I0302 19:00:57.585071 22535416901760 run.py:483] Algo bellman_ford step 5345 current loss 0.224955, current_train_items 171072.
I0302 19:00:57.600603 22535416901760 run.py:483] Algo bellman_ford step 5346 current loss 0.444467, current_train_items 171104.
I0302 19:00:57.623922 22535416901760 run.py:483] Algo bellman_ford step 5347 current loss 0.574595, current_train_items 171136.
I0302 19:00:57.656032 22535416901760 run.py:483] Algo bellman_ford step 5348 current loss 0.696209, current_train_items 171168.
I0302 19:00:57.689821 22535416901760 run.py:483] Algo bellman_ford step 5349 current loss 0.787570, current_train_items 171200.
I0302 19:00:57.709510 22535416901760 run.py:483] Algo bellman_ford step 5350 current loss 0.290079, current_train_items 171232.
I0302 19:00:57.717679 22535416901760 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:57.717785 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:00:57.734133 22535416901760 run.py:483] Algo bellman_ford step 5351 current loss 0.455180, current_train_items 171264.
I0302 19:00:57.758707 22535416901760 run.py:483] Algo bellman_ford step 5352 current loss 0.654756, current_train_items 171296.
I0302 19:00:57.788866 22535416901760 run.py:483] Algo bellman_ford step 5353 current loss 0.718997, current_train_items 171328.
I0302 19:00:57.822447 22535416901760 run.py:483] Algo bellman_ford step 5354 current loss 0.714272, current_train_items 171360.
I0302 19:00:57.842579 22535416901760 run.py:483] Algo bellman_ford step 5355 current loss 0.300619, current_train_items 171392.
I0302 19:00:57.858561 22535416901760 run.py:483] Algo bellman_ford step 5356 current loss 0.505684, current_train_items 171424.
I0302 19:00:57.881911 22535416901760 run.py:483] Algo bellman_ford step 5357 current loss 0.749376, current_train_items 171456.
I0302 19:00:57.911757 22535416901760 run.py:483] Algo bellman_ford step 5358 current loss 0.609059, current_train_items 171488.
I0302 19:00:57.944601 22535416901760 run.py:483] Algo bellman_ford step 5359 current loss 0.782502, current_train_items 171520.
I0302 19:00:57.964306 22535416901760 run.py:483] Algo bellman_ford step 5360 current loss 0.329523, current_train_items 171552.
I0302 19:00:57.980424 22535416901760 run.py:483] Algo bellman_ford step 5361 current loss 0.495841, current_train_items 171584.
I0302 19:00:58.004122 22535416901760 run.py:483] Algo bellman_ford step 5362 current loss 0.754059, current_train_items 171616.
I0302 19:00:58.035032 22535416901760 run.py:483] Algo bellman_ford step 5363 current loss 0.838965, current_train_items 171648.
I0302 19:00:58.068027 22535416901760 run.py:483] Algo bellman_ford step 5364 current loss 0.851963, current_train_items 171680.
I0302 19:00:58.087564 22535416901760 run.py:483] Algo bellman_ford step 5365 current loss 0.324254, current_train_items 171712.
I0302 19:00:58.103762 22535416901760 run.py:483] Algo bellman_ford step 5366 current loss 0.518058, current_train_items 171744.
I0302 19:00:58.127068 22535416901760 run.py:483] Algo bellman_ford step 5367 current loss 0.602341, current_train_items 171776.
I0302 19:00:58.157695 22535416901760 run.py:483] Algo bellman_ford step 5368 current loss 0.644860, current_train_items 171808.
I0302 19:00:58.190356 22535416901760 run.py:483] Algo bellman_ford step 5369 current loss 0.771448, current_train_items 171840.
I0302 19:00:58.210084 22535416901760 run.py:483] Algo bellman_ford step 5370 current loss 0.322939, current_train_items 171872.
I0302 19:00:58.226240 22535416901760 run.py:483] Algo bellman_ford step 5371 current loss 0.429249, current_train_items 171904.
I0302 19:00:58.249117 22535416901760 run.py:483] Algo bellman_ford step 5372 current loss 0.590996, current_train_items 171936.
I0302 19:00:58.279601 22535416901760 run.py:483] Algo bellman_ford step 5373 current loss 0.682787, current_train_items 171968.
I0302 19:00:58.310650 22535416901760 run.py:483] Algo bellman_ford step 5374 current loss 0.692817, current_train_items 172000.
I0302 19:00:58.330365 22535416901760 run.py:483] Algo bellman_ford step 5375 current loss 0.292287, current_train_items 172032.
I0302 19:00:58.346666 22535416901760 run.py:483] Algo bellman_ford step 5376 current loss 0.530759, current_train_items 172064.
I0302 19:00:58.369701 22535416901760 run.py:483] Algo bellman_ford step 5377 current loss 0.614284, current_train_items 172096.
I0302 19:00:58.399344 22535416901760 run.py:483] Algo bellman_ford step 5378 current loss 0.610769, current_train_items 172128.
I0302 19:00:58.429841 22535416901760 run.py:483] Algo bellman_ford step 5379 current loss 0.660130, current_train_items 172160.
I0302 19:00:58.449665 22535416901760 run.py:483] Algo bellman_ford step 5380 current loss 0.372223, current_train_items 172192.
I0302 19:00:58.465961 22535416901760 run.py:483] Algo bellman_ford step 5381 current loss 0.463173, current_train_items 172224.
I0302 19:00:58.490353 22535416901760 run.py:483] Algo bellman_ford step 5382 current loss 0.657096, current_train_items 172256.
I0302 19:00:58.521113 22535416901760 run.py:483] Algo bellman_ford step 5383 current loss 0.755055, current_train_items 172288.
I0302 19:00:58.552944 22535416901760 run.py:483] Algo bellman_ford step 5384 current loss 0.742077, current_train_items 172320.
I0302 19:00:58.572563 22535416901760 run.py:483] Algo bellman_ford step 5385 current loss 0.305529, current_train_items 172352.
I0302 19:00:58.588319 22535416901760 run.py:483] Algo bellman_ford step 5386 current loss 0.473502, current_train_items 172384.
I0302 19:00:58.611288 22535416901760 run.py:483] Algo bellman_ford step 5387 current loss 0.560817, current_train_items 172416.
I0302 19:00:58.641292 22535416901760 run.py:483] Algo bellman_ford step 5388 current loss 0.687561, current_train_items 172448.
I0302 19:00:58.675100 22535416901760 run.py:483] Algo bellman_ford step 5389 current loss 0.789982, current_train_items 172480.
I0302 19:00:58.695075 22535416901760 run.py:483] Algo bellman_ford step 5390 current loss 0.319676, current_train_items 172512.
I0302 19:00:58.711675 22535416901760 run.py:483] Algo bellman_ford step 5391 current loss 0.497781, current_train_items 172544.
I0302 19:00:58.734533 22535416901760 run.py:483] Algo bellman_ford step 5392 current loss 0.538062, current_train_items 172576.
I0302 19:00:58.766537 22535416901760 run.py:483] Algo bellman_ford step 5393 current loss 0.685302, current_train_items 172608.
I0302 19:00:58.801007 22535416901760 run.py:483] Algo bellman_ford step 5394 current loss 0.808937, current_train_items 172640.
I0302 19:00:58.820508 22535416901760 run.py:483] Algo bellman_ford step 5395 current loss 0.384411, current_train_items 172672.
I0302 19:00:58.837051 22535416901760 run.py:483] Algo bellman_ford step 5396 current loss 0.548173, current_train_items 172704.
I0302 19:00:58.860627 22535416901760 run.py:483] Algo bellman_ford step 5397 current loss 0.554371, current_train_items 172736.
I0302 19:00:58.891516 22535416901760 run.py:483] Algo bellman_ford step 5398 current loss 0.628785, current_train_items 172768.
I0302 19:00:58.925635 22535416901760 run.py:483] Algo bellman_ford step 5399 current loss 0.872827, current_train_items 172800.
I0302 19:00:58.945191 22535416901760 run.py:483] Algo bellman_ford step 5400 current loss 0.233301, current_train_items 172832.
I0302 19:00:58.952826 22535416901760 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:58.952935 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:00:58.970133 22535416901760 run.py:483] Algo bellman_ford step 5401 current loss 0.534355, current_train_items 172864.
I0302 19:00:58.994417 22535416901760 run.py:483] Algo bellman_ford step 5402 current loss 0.647469, current_train_items 172896.
I0302 19:00:59.026751 22535416901760 run.py:483] Algo bellman_ford step 5403 current loss 0.774521, current_train_items 172928.
I0302 19:00:59.061366 22535416901760 run.py:483] Algo bellman_ford step 5404 current loss 0.675782, current_train_items 172960.
I0302 19:00:59.081199 22535416901760 run.py:483] Algo bellman_ford step 5405 current loss 0.281937, current_train_items 172992.
I0302 19:00:59.096822 22535416901760 run.py:483] Algo bellman_ford step 5406 current loss 0.456999, current_train_items 173024.
I0302 19:00:59.119538 22535416901760 run.py:483] Algo bellman_ford step 5407 current loss 0.554655, current_train_items 173056.
I0302 19:00:59.148625 22535416901760 run.py:483] Algo bellman_ford step 5408 current loss 0.647726, current_train_items 173088.
I0302 19:00:59.180970 22535416901760 run.py:483] Algo bellman_ford step 5409 current loss 0.666624, current_train_items 173120.
I0302 19:00:59.200520 22535416901760 run.py:483] Algo bellman_ford step 5410 current loss 0.267379, current_train_items 173152.
I0302 19:00:59.217105 22535416901760 run.py:483] Algo bellman_ford step 5411 current loss 0.481834, current_train_items 173184.
I0302 19:00:59.240703 22535416901760 run.py:483] Algo bellman_ford step 5412 current loss 0.606506, current_train_items 173216.
I0302 19:00:59.271657 22535416901760 run.py:483] Algo bellman_ford step 5413 current loss 0.575144, current_train_items 173248.
I0302 19:00:59.305224 22535416901760 run.py:483] Algo bellman_ford step 5414 current loss 0.729617, current_train_items 173280.
I0302 19:00:59.324743 22535416901760 run.py:483] Algo bellman_ford step 5415 current loss 0.240196, current_train_items 173312.
I0302 19:00:59.341103 22535416901760 run.py:483] Algo bellman_ford step 5416 current loss 0.436569, current_train_items 173344.
I0302 19:00:59.364812 22535416901760 run.py:483] Algo bellman_ford step 5417 current loss 0.598864, current_train_items 173376.
I0302 19:00:59.396632 22535416901760 run.py:483] Algo bellman_ford step 5418 current loss 0.648729, current_train_items 173408.
I0302 19:00:59.431372 22535416901760 run.py:483] Algo bellman_ford step 5419 current loss 0.774033, current_train_items 173440.
I0302 19:00:59.450786 22535416901760 run.py:483] Algo bellman_ford step 5420 current loss 0.322306, current_train_items 173472.
I0302 19:00:59.466999 22535416901760 run.py:483] Algo bellman_ford step 5421 current loss 0.456220, current_train_items 173504.
I0302 19:00:59.489903 22535416901760 run.py:483] Algo bellman_ford step 5422 current loss 0.485507, current_train_items 173536.
I0302 19:00:59.519890 22535416901760 run.py:483] Algo bellman_ford step 5423 current loss 0.647810, current_train_items 173568.
I0302 19:00:59.551167 22535416901760 run.py:483] Algo bellman_ford step 5424 current loss 0.768066, current_train_items 173600.
I0302 19:00:59.570794 22535416901760 run.py:483] Algo bellman_ford step 5425 current loss 0.289638, current_train_items 173632.
I0302 19:00:59.586633 22535416901760 run.py:483] Algo bellman_ford step 5426 current loss 0.401907, current_train_items 173664.
I0302 19:00:59.610263 22535416901760 run.py:483] Algo bellman_ford step 5427 current loss 0.698749, current_train_items 173696.
I0302 19:00:59.641072 22535416901760 run.py:483] Algo bellman_ford step 5428 current loss 0.781578, current_train_items 173728.
I0302 19:00:59.675286 22535416901760 run.py:483] Algo bellman_ford step 5429 current loss 0.795417, current_train_items 173760.
I0302 19:00:59.694568 22535416901760 run.py:483] Algo bellman_ford step 5430 current loss 0.318209, current_train_items 173792.
I0302 19:00:59.710930 22535416901760 run.py:483] Algo bellman_ford step 5431 current loss 0.543725, current_train_items 173824.
I0302 19:00:59.734405 22535416901760 run.py:483] Algo bellman_ford step 5432 current loss 0.637368, current_train_items 173856.
I0302 19:00:59.767262 22535416901760 run.py:483] Algo bellman_ford step 5433 current loss 0.839497, current_train_items 173888.
I0302 19:00:59.802453 22535416901760 run.py:483] Algo bellman_ford step 5434 current loss 0.941249, current_train_items 173920.
I0302 19:00:59.821720 22535416901760 run.py:483] Algo bellman_ford step 5435 current loss 0.283513, current_train_items 173952.
I0302 19:00:59.837768 22535416901760 run.py:483] Algo bellman_ford step 5436 current loss 0.492234, current_train_items 173984.
I0302 19:00:59.861246 22535416901760 run.py:483] Algo bellman_ford step 5437 current loss 0.626769, current_train_items 174016.
I0302 19:00:59.890993 22535416901760 run.py:483] Algo bellman_ford step 5438 current loss 0.893947, current_train_items 174048.
I0302 19:00:59.927744 22535416901760 run.py:483] Algo bellman_ford step 5439 current loss 0.952655, current_train_items 174080.
I0302 19:00:59.947193 22535416901760 run.py:483] Algo bellman_ford step 5440 current loss 0.361257, current_train_items 174112.
I0302 19:00:59.963095 22535416901760 run.py:483] Algo bellman_ford step 5441 current loss 0.398418, current_train_items 174144.
I0302 19:00:59.985796 22535416901760 run.py:483] Algo bellman_ford step 5442 current loss 0.586876, current_train_items 174176.
I0302 19:01:00.016100 22535416901760 run.py:483] Algo bellman_ford step 5443 current loss 0.700693, current_train_items 174208.
I0302 19:01:00.049657 22535416901760 run.py:483] Algo bellman_ford step 5444 current loss 0.770187, current_train_items 174240.
I0302 19:01:00.068900 22535416901760 run.py:483] Algo bellman_ford step 5445 current loss 0.240524, current_train_items 174272.
I0302 19:01:00.084954 22535416901760 run.py:483] Algo bellman_ford step 5446 current loss 0.492060, current_train_items 174304.
I0302 19:01:00.108775 22535416901760 run.py:483] Algo bellman_ford step 5447 current loss 0.623866, current_train_items 174336.
I0302 19:01:00.140291 22535416901760 run.py:483] Algo bellman_ford step 5448 current loss 0.909521, current_train_items 174368.
I0302 19:01:00.173420 22535416901760 run.py:483] Algo bellman_ford step 5449 current loss 0.747230, current_train_items 174400.
I0302 19:01:00.192707 22535416901760 run.py:483] Algo bellman_ford step 5450 current loss 0.288028, current_train_items 174432.
I0302 19:01:00.200859 22535416901760 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9130859375, 'score': 0.9130859375, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:01:00.200963 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.913, val scores are: bellman_ford: 0.913
I0302 19:01:00.217267 22535416901760 run.py:483] Algo bellman_ford step 5451 current loss 0.526345, current_train_items 174464.
I0302 19:01:00.241116 22535416901760 run.py:483] Algo bellman_ford step 5452 current loss 0.572063, current_train_items 174496.
I0302 19:01:00.271061 22535416901760 run.py:483] Algo bellman_ford step 5453 current loss 0.669865, current_train_items 174528.
I0302 19:01:00.304362 22535416901760 run.py:483] Algo bellman_ford step 5454 current loss 0.838078, current_train_items 174560.
I0302 19:01:00.324262 22535416901760 run.py:483] Algo bellman_ford step 5455 current loss 0.284810, current_train_items 174592.
I0302 19:01:00.340375 22535416901760 run.py:483] Algo bellman_ford step 5456 current loss 0.519782, current_train_items 174624.
I0302 19:01:00.364164 22535416901760 run.py:483] Algo bellman_ford step 5457 current loss 0.548869, current_train_items 174656.
I0302 19:01:00.395445 22535416901760 run.py:483] Algo bellman_ford step 5458 current loss 0.701955, current_train_items 174688.
I0302 19:01:00.426096 22535416901760 run.py:483] Algo bellman_ford step 5459 current loss 0.735754, current_train_items 174720.
I0302 19:01:00.445672 22535416901760 run.py:483] Algo bellman_ford step 5460 current loss 0.324266, current_train_items 174752.
I0302 19:01:00.462177 22535416901760 run.py:483] Algo bellman_ford step 5461 current loss 0.460522, current_train_items 174784.
I0302 19:01:00.485660 22535416901760 run.py:483] Algo bellman_ford step 5462 current loss 0.583862, current_train_items 174816.
I0302 19:01:00.515693 22535416901760 run.py:483] Algo bellman_ford step 5463 current loss 0.640545, current_train_items 174848.
I0302 19:01:00.549014 22535416901760 run.py:483] Algo bellman_ford step 5464 current loss 0.697073, current_train_items 174880.
I0302 19:01:00.568501 22535416901760 run.py:483] Algo bellman_ford step 5465 current loss 0.332035, current_train_items 174912.
I0302 19:01:00.584377 22535416901760 run.py:483] Algo bellman_ford step 5466 current loss 0.434767, current_train_items 174944.
I0302 19:01:00.607334 22535416901760 run.py:483] Algo bellman_ford step 5467 current loss 0.503674, current_train_items 174976.
I0302 19:01:00.636962 22535416901760 run.py:483] Algo bellman_ford step 5468 current loss 0.536984, current_train_items 175008.
I0302 19:01:00.670002 22535416901760 run.py:483] Algo bellman_ford step 5469 current loss 0.728728, current_train_items 175040.
I0302 19:01:00.690258 22535416901760 run.py:483] Algo bellman_ford step 5470 current loss 0.375341, current_train_items 175072.
I0302 19:01:00.706513 22535416901760 run.py:483] Algo bellman_ford step 5471 current loss 0.641509, current_train_items 175104.
I0302 19:01:00.729546 22535416901760 run.py:483] Algo bellman_ford step 5472 current loss 0.522231, current_train_items 175136.
I0302 19:01:00.759896 22535416901760 run.py:483] Algo bellman_ford step 5473 current loss 0.641272, current_train_items 175168.
I0302 19:01:00.794019 22535416901760 run.py:483] Algo bellman_ford step 5474 current loss 0.923131, current_train_items 175200.
I0302 19:01:00.813568 22535416901760 run.py:483] Algo bellman_ford step 5475 current loss 0.309047, current_train_items 175232.
I0302 19:01:00.829835 22535416901760 run.py:483] Algo bellman_ford step 5476 current loss 0.508329, current_train_items 175264.
I0302 19:01:00.852792 22535416901760 run.py:483] Algo bellman_ford step 5477 current loss 0.626096, current_train_items 175296.
I0302 19:01:00.883820 22535416901760 run.py:483] Algo bellman_ford step 5478 current loss 0.717363, current_train_items 175328.
I0302 19:01:00.916777 22535416901760 run.py:483] Algo bellman_ford step 5479 current loss 0.946020, current_train_items 175360.
I0302 19:01:00.936501 22535416901760 run.py:483] Algo bellman_ford step 5480 current loss 0.347157, current_train_items 175392.
I0302 19:01:00.953175 22535416901760 run.py:483] Algo bellman_ford step 5481 current loss 0.478625, current_train_items 175424.
I0302 19:01:00.977676 22535416901760 run.py:483] Algo bellman_ford step 5482 current loss 0.716911, current_train_items 175456.
I0302 19:01:01.009079 22535416901760 run.py:483] Algo bellman_ford step 5483 current loss 0.732850, current_train_items 175488.
I0302 19:01:01.044313 22535416901760 run.py:483] Algo bellman_ford step 5484 current loss 0.752667, current_train_items 175520.
I0302 19:01:01.063903 22535416901760 run.py:483] Algo bellman_ford step 5485 current loss 0.300916, current_train_items 175552.
I0302 19:01:01.079772 22535416901760 run.py:483] Algo bellman_ford step 5486 current loss 0.438711, current_train_items 175584.
I0302 19:01:01.102226 22535416901760 run.py:483] Algo bellman_ford step 5487 current loss 0.565175, current_train_items 175616.
I0302 19:01:01.134617 22535416901760 run.py:483] Algo bellman_ford step 5488 current loss 0.725745, current_train_items 175648.
I0302 19:01:01.166646 22535416901760 run.py:483] Algo bellman_ford step 5489 current loss 0.643728, current_train_items 175680.
I0302 19:01:01.186366 22535416901760 run.py:483] Algo bellman_ford step 5490 current loss 0.290588, current_train_items 175712.
I0302 19:01:01.202573 22535416901760 run.py:483] Algo bellman_ford step 5491 current loss 0.484227, current_train_items 175744.
I0302 19:01:01.226204 22535416901760 run.py:483] Algo bellman_ford step 5492 current loss 0.611792, current_train_items 175776.
I0302 19:01:01.256649 22535416901760 run.py:483] Algo bellman_ford step 5493 current loss 0.623570, current_train_items 175808.
I0302 19:01:01.289493 22535416901760 run.py:483] Algo bellman_ford step 5494 current loss 0.709128, current_train_items 175840.
I0302 19:01:01.309122 22535416901760 run.py:483] Algo bellman_ford step 5495 current loss 0.304389, current_train_items 175872.
I0302 19:01:01.325753 22535416901760 run.py:483] Algo bellman_ford step 5496 current loss 0.424954, current_train_items 175904.
I0302 19:01:01.349532 22535416901760 run.py:483] Algo bellman_ford step 5497 current loss 0.620420, current_train_items 175936.
I0302 19:01:01.380327 22535416901760 run.py:483] Algo bellman_ford step 5498 current loss 0.589214, current_train_items 175968.
I0302 19:01:01.414370 22535416901760 run.py:483] Algo bellman_ford step 5499 current loss 0.757774, current_train_items 176000.
I0302 19:01:01.434241 22535416901760 run.py:483] Algo bellman_ford step 5500 current loss 0.305890, current_train_items 176032.
I0302 19:01:01.441999 22535416901760 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:01:01.442107 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:01.459230 22535416901760 run.py:483] Algo bellman_ford step 5501 current loss 0.534100, current_train_items 176064.
I0302 19:01:01.483920 22535416901760 run.py:483] Algo bellman_ford step 5502 current loss 0.572049, current_train_items 176096.
I0302 19:01:01.514728 22535416901760 run.py:483] Algo bellman_ford step 5503 current loss 0.686207, current_train_items 176128.
I0302 19:01:01.548573 22535416901760 run.py:483] Algo bellman_ford step 5504 current loss 0.824050, current_train_items 176160.
I0302 19:01:01.568762 22535416901760 run.py:483] Algo bellman_ford step 5505 current loss 0.266443, current_train_items 176192.
I0302 19:01:01.584833 22535416901760 run.py:483] Algo bellman_ford step 5506 current loss 0.434058, current_train_items 176224.
I0302 19:01:01.608068 22535416901760 run.py:483] Algo bellman_ford step 5507 current loss 0.545464, current_train_items 176256.
I0302 19:01:01.637998 22535416901760 run.py:483] Algo bellman_ford step 5508 current loss 0.613026, current_train_items 176288.
I0302 19:01:01.670941 22535416901760 run.py:483] Algo bellman_ford step 5509 current loss 0.620907, current_train_items 176320.
I0302 19:01:01.690721 22535416901760 run.py:483] Algo bellman_ford step 5510 current loss 0.339135, current_train_items 176352.
I0302 19:01:01.707095 22535416901760 run.py:483] Algo bellman_ford step 5511 current loss 0.407567, current_train_items 176384.
I0302 19:01:01.732059 22535416901760 run.py:483] Algo bellman_ford step 5512 current loss 0.716173, current_train_items 176416.
I0302 19:01:01.761958 22535416901760 run.py:483] Algo bellman_ford step 5513 current loss 0.671181, current_train_items 176448.
I0302 19:01:01.793502 22535416901760 run.py:483] Algo bellman_ford step 5514 current loss 0.755171, current_train_items 176480.
I0302 19:01:01.813117 22535416901760 run.py:483] Algo bellman_ford step 5515 current loss 0.311864, current_train_items 176512.
I0302 19:01:01.828924 22535416901760 run.py:483] Algo bellman_ford step 5516 current loss 0.384238, current_train_items 176544.
I0302 19:01:01.852642 22535416901760 run.py:483] Algo bellman_ford step 5517 current loss 0.609404, current_train_items 176576.
I0302 19:01:01.883142 22535416901760 run.py:483] Algo bellman_ford step 5518 current loss 0.688944, current_train_items 176608.
I0302 19:01:01.918102 22535416901760 run.py:483] Algo bellman_ford step 5519 current loss 0.654870, current_train_items 176640.
I0302 19:01:01.937675 22535416901760 run.py:483] Algo bellman_ford step 5520 current loss 0.347067, current_train_items 176672.
I0302 19:01:01.954164 22535416901760 run.py:483] Algo bellman_ford step 5521 current loss 0.510394, current_train_items 176704.
I0302 19:01:01.977035 22535416901760 run.py:483] Algo bellman_ford step 5522 current loss 0.593203, current_train_items 176736.
I0302 19:01:02.008534 22535416901760 run.py:483] Algo bellman_ford step 5523 current loss 0.631617, current_train_items 176768.
I0302 19:01:02.042869 22535416901760 run.py:483] Algo bellman_ford step 5524 current loss 0.843723, current_train_items 176800.
I0302 19:01:02.062296 22535416901760 run.py:483] Algo bellman_ford step 5525 current loss 0.293672, current_train_items 176832.
I0302 19:01:02.078116 22535416901760 run.py:483] Algo bellman_ford step 5526 current loss 0.480561, current_train_items 176864.
I0302 19:01:02.102446 22535416901760 run.py:483] Algo bellman_ford step 5527 current loss 0.770067, current_train_items 176896.
I0302 19:01:02.132270 22535416901760 run.py:483] Algo bellman_ford step 5528 current loss 0.781746, current_train_items 176928.
I0302 19:01:02.164425 22535416901760 run.py:483] Algo bellman_ford step 5529 current loss 0.784304, current_train_items 176960.
I0302 19:01:02.183985 22535416901760 run.py:483] Algo bellman_ford step 5530 current loss 0.332772, current_train_items 176992.
I0302 19:01:02.200227 22535416901760 run.py:483] Algo bellman_ford step 5531 current loss 0.441411, current_train_items 177024.
I0302 19:01:02.224353 22535416901760 run.py:483] Algo bellman_ford step 5532 current loss 0.649195, current_train_items 177056.
I0302 19:01:02.254742 22535416901760 run.py:483] Algo bellman_ford step 5533 current loss 0.727314, current_train_items 177088.
I0302 19:01:02.290210 22535416901760 run.py:483] Algo bellman_ford step 5534 current loss 0.898374, current_train_items 177120.
I0302 19:01:02.309916 22535416901760 run.py:483] Algo bellman_ford step 5535 current loss 0.282720, current_train_items 177152.
I0302 19:01:02.326274 22535416901760 run.py:483] Algo bellman_ford step 5536 current loss 0.530632, current_train_items 177184.
I0302 19:01:02.349235 22535416901760 run.py:483] Algo bellman_ford step 5537 current loss 0.552357, current_train_items 177216.
I0302 19:01:02.379432 22535416901760 run.py:483] Algo bellman_ford step 5538 current loss 0.623327, current_train_items 177248.
I0302 19:01:02.414777 22535416901760 run.py:483] Algo bellman_ford step 5539 current loss 0.781542, current_train_items 177280.
I0302 19:01:02.434219 22535416901760 run.py:483] Algo bellman_ford step 5540 current loss 0.294007, current_train_items 177312.
I0302 19:01:02.450836 22535416901760 run.py:483] Algo bellman_ford step 5541 current loss 0.428227, current_train_items 177344.
I0302 19:01:02.475070 22535416901760 run.py:483] Algo bellman_ford step 5542 current loss 0.602197, current_train_items 177376.
I0302 19:01:02.505761 22535416901760 run.py:483] Algo bellman_ford step 5543 current loss 0.632085, current_train_items 177408.
I0302 19:01:02.538438 22535416901760 run.py:483] Algo bellman_ford step 5544 current loss 0.710875, current_train_items 177440.
I0302 19:01:02.557840 22535416901760 run.py:483] Algo bellman_ford step 5545 current loss 0.266706, current_train_items 177472.
I0302 19:01:02.574502 22535416901760 run.py:483] Algo bellman_ford step 5546 current loss 0.572773, current_train_items 177504.
I0302 19:01:02.597846 22535416901760 run.py:483] Algo bellman_ford step 5547 current loss 0.682853, current_train_items 177536.
I0302 19:01:02.628170 22535416901760 run.py:483] Algo bellman_ford step 5548 current loss 0.626155, current_train_items 177568.
I0302 19:01:02.660188 22535416901760 run.py:483] Algo bellman_ford step 5549 current loss 0.831401, current_train_items 177600.
I0302 19:01:02.679479 22535416901760 run.py:483] Algo bellman_ford step 5550 current loss 0.279125, current_train_items 177632.
I0302 19:01:02.687433 22535416901760 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.923828125, 'score': 0.923828125, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:01:02.687538 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.924, val scores are: bellman_ford: 0.924
I0302 19:01:02.704282 22535416901760 run.py:483] Algo bellman_ford step 5551 current loss 0.405320, current_train_items 177664.
I0302 19:01:02.729005 22535416901760 run.py:483] Algo bellman_ford step 5552 current loss 0.715072, current_train_items 177696.
I0302 19:01:02.759557 22535416901760 run.py:483] Algo bellman_ford step 5553 current loss 0.758396, current_train_items 177728.
I0302 19:01:02.795663 22535416901760 run.py:483] Algo bellman_ford step 5554 current loss 1.062624, current_train_items 177760.
I0302 19:01:02.815755 22535416901760 run.py:483] Algo bellman_ford step 5555 current loss 0.287455, current_train_items 177792.
I0302 19:01:02.831496 22535416901760 run.py:483] Algo bellman_ford step 5556 current loss 0.378420, current_train_items 177824.
I0302 19:01:02.855102 22535416901760 run.py:483] Algo bellman_ford step 5557 current loss 0.648792, current_train_items 177856.
I0302 19:01:02.885126 22535416901760 run.py:483] Algo bellman_ford step 5558 current loss 0.657989, current_train_items 177888.
I0302 19:01:02.919125 22535416901760 run.py:483] Algo bellman_ford step 5559 current loss 0.709654, current_train_items 177920.
I0302 19:01:02.939029 22535416901760 run.py:483] Algo bellman_ford step 5560 current loss 0.329641, current_train_items 177952.
I0302 19:01:02.955054 22535416901760 run.py:483] Algo bellman_ford step 5561 current loss 0.567746, current_train_items 177984.
I0302 19:01:02.978023 22535416901760 run.py:483] Algo bellman_ford step 5562 current loss 0.586958, current_train_items 178016.
I0302 19:01:03.009397 22535416901760 run.py:483] Algo bellman_ford step 5563 current loss 0.605486, current_train_items 178048.
I0302 19:01:03.043594 22535416901760 run.py:483] Algo bellman_ford step 5564 current loss 0.726366, current_train_items 178080.
I0302 19:01:03.063183 22535416901760 run.py:483] Algo bellman_ford step 5565 current loss 0.334399, current_train_items 178112.
I0302 19:01:03.079326 22535416901760 run.py:483] Algo bellman_ford step 5566 current loss 0.444750, current_train_items 178144.
I0302 19:01:03.102802 22535416901760 run.py:483] Algo bellman_ford step 5567 current loss 0.602523, current_train_items 178176.
I0302 19:01:03.134605 22535416901760 run.py:483] Algo bellman_ford step 5568 current loss 0.749576, current_train_items 178208.
I0302 19:01:03.168227 22535416901760 run.py:483] Algo bellman_ford step 5569 current loss 0.806964, current_train_items 178240.
I0302 19:01:03.188006 22535416901760 run.py:483] Algo bellman_ford step 5570 current loss 0.377017, current_train_items 178272.
I0302 19:01:03.203915 22535416901760 run.py:483] Algo bellman_ford step 5571 current loss 0.454579, current_train_items 178304.
I0302 19:01:03.226687 22535416901760 run.py:483] Algo bellman_ford step 5572 current loss 0.655940, current_train_items 178336.
I0302 19:01:03.257403 22535416901760 run.py:483] Algo bellman_ford step 5573 current loss 0.686537, current_train_items 178368.
I0302 19:01:03.291265 22535416901760 run.py:483] Algo bellman_ford step 5574 current loss 0.786091, current_train_items 178400.
I0302 19:01:03.311109 22535416901760 run.py:483] Algo bellman_ford step 5575 current loss 0.350219, current_train_items 178432.
I0302 19:01:03.327252 22535416901760 run.py:483] Algo bellman_ford step 5576 current loss 0.670845, current_train_items 178464.
I0302 19:01:03.349822 22535416901760 run.py:483] Algo bellman_ford step 5577 current loss 0.570359, current_train_items 178496.
I0302 19:01:03.380772 22535416901760 run.py:483] Algo bellman_ford step 5578 current loss 0.693780, current_train_items 178528.
I0302 19:01:03.415730 22535416901760 run.py:483] Algo bellman_ford step 5579 current loss 0.768971, current_train_items 178560.
I0302 19:01:03.435167 22535416901760 run.py:483] Algo bellman_ford step 5580 current loss 0.305433, current_train_items 178592.
I0302 19:01:03.450832 22535416901760 run.py:483] Algo bellman_ford step 5581 current loss 0.405288, current_train_items 178624.
I0302 19:01:03.475182 22535416901760 run.py:483] Algo bellman_ford step 5582 current loss 0.566406, current_train_items 178656.
I0302 19:01:03.505526 22535416901760 run.py:483] Algo bellman_ford step 5583 current loss 0.640661, current_train_items 178688.
I0302 19:01:03.539073 22535416901760 run.py:483] Algo bellman_ford step 5584 current loss 0.720905, current_train_items 178720.
I0302 19:01:03.558788 22535416901760 run.py:483] Algo bellman_ford step 5585 current loss 0.367146, current_train_items 178752.
I0302 19:01:03.574992 22535416901760 run.py:483] Algo bellman_ford step 5586 current loss 0.466640, current_train_items 178784.
I0302 19:01:03.599371 22535416901760 run.py:483] Algo bellman_ford step 5587 current loss 0.650492, current_train_items 178816.
I0302 19:01:03.630713 22535416901760 run.py:483] Algo bellman_ford step 5588 current loss 0.697451, current_train_items 178848.
I0302 19:01:03.661797 22535416901760 run.py:483] Algo bellman_ford step 5589 current loss 0.681668, current_train_items 178880.
I0302 19:01:03.681736 22535416901760 run.py:483] Algo bellman_ford step 5590 current loss 0.354976, current_train_items 178912.
I0302 19:01:03.697996 22535416901760 run.py:483] Algo bellman_ford step 5591 current loss 0.420380, current_train_items 178944.
I0302 19:01:03.720235 22535416901760 run.py:483] Algo bellman_ford step 5592 current loss 0.571410, current_train_items 178976.
I0302 19:01:03.750852 22535416901760 run.py:483] Algo bellman_ford step 5593 current loss 0.701251, current_train_items 179008.
I0302 19:01:03.785099 22535416901760 run.py:483] Algo bellman_ford step 5594 current loss 0.640293, current_train_items 179040.
I0302 19:01:03.804486 22535416901760 run.py:483] Algo bellman_ford step 5595 current loss 0.343749, current_train_items 179072.
I0302 19:01:03.820401 22535416901760 run.py:483] Algo bellman_ford step 5596 current loss 0.479818, current_train_items 179104.
I0302 19:01:03.843531 22535416901760 run.py:483] Algo bellman_ford step 5597 current loss 0.667480, current_train_items 179136.
I0302 19:01:03.874491 22535416901760 run.py:483] Algo bellman_ford step 5598 current loss 0.687863, current_train_items 179168.
I0302 19:01:03.904032 22535416901760 run.py:483] Algo bellman_ford step 5599 current loss 0.684783, current_train_items 179200.
I0302 19:01:03.923899 22535416901760 run.py:483] Algo bellman_ford step 5600 current loss 0.344265, current_train_items 179232.
I0302 19:01:03.931925 22535416901760 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:01:03.932032 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:03.948790 22535416901760 run.py:483] Algo bellman_ford step 5601 current loss 0.511779, current_train_items 179264.
I0302 19:01:03.973175 22535416901760 run.py:483] Algo bellman_ford step 5602 current loss 0.603860, current_train_items 179296.
I0302 19:01:04.004401 22535416901760 run.py:483] Algo bellman_ford step 5603 current loss 0.755823, current_train_items 179328.
I0302 19:01:04.036805 22535416901760 run.py:483] Algo bellman_ford step 5604 current loss 0.704038, current_train_items 179360.
I0302 19:01:04.056565 22535416901760 run.py:483] Algo bellman_ford step 5605 current loss 0.281511, current_train_items 179392.
I0302 19:01:04.072111 22535416901760 run.py:483] Algo bellman_ford step 5606 current loss 0.480716, current_train_items 179424.
I0302 19:01:04.095483 22535416901760 run.py:483] Algo bellman_ford step 5607 current loss 0.564312, current_train_items 179456.
I0302 19:01:04.124816 22535416901760 run.py:483] Algo bellman_ford step 5608 current loss 0.596094, current_train_items 179488.
I0302 19:01:04.158973 22535416901760 run.py:483] Algo bellman_ford step 5609 current loss 0.716401, current_train_items 179520.
I0302 19:01:04.178823 22535416901760 run.py:483] Algo bellman_ford step 5610 current loss 0.257339, current_train_items 179552.
I0302 19:01:04.195047 22535416901760 run.py:483] Algo bellman_ford step 5611 current loss 0.471041, current_train_items 179584.
I0302 19:01:04.217945 22535416901760 run.py:483] Algo bellman_ford step 5612 current loss 0.557651, current_train_items 179616.
I0302 19:01:04.249817 22535416901760 run.py:483] Algo bellman_ford step 5613 current loss 0.675460, current_train_items 179648.
I0302 19:01:04.281786 22535416901760 run.py:483] Algo bellman_ford step 5614 current loss 0.723959, current_train_items 179680.
I0302 19:01:04.301227 22535416901760 run.py:483] Algo bellman_ford step 5615 current loss 0.294734, current_train_items 179712.
I0302 19:01:04.317689 22535416901760 run.py:483] Algo bellman_ford step 5616 current loss 0.512722, current_train_items 179744.
I0302 19:01:04.341131 22535416901760 run.py:483] Algo bellman_ford step 5617 current loss 0.518560, current_train_items 179776.
I0302 19:01:04.371727 22535416901760 run.py:483] Algo bellman_ford step 5618 current loss 0.617499, current_train_items 179808.
I0302 19:01:04.406193 22535416901760 run.py:483] Algo bellman_ford step 5619 current loss 0.796282, current_train_items 179840.
I0302 19:01:04.425945 22535416901760 run.py:483] Algo bellman_ford step 5620 current loss 0.299144, current_train_items 179872.
I0302 19:01:04.442046 22535416901760 run.py:483] Algo bellman_ford step 5621 current loss 0.535061, current_train_items 179904.
I0302 19:01:04.465317 22535416901760 run.py:483] Algo bellman_ford step 5622 current loss 0.536510, current_train_items 179936.
I0302 19:01:04.495921 22535416901760 run.py:483] Algo bellman_ford step 5623 current loss 0.669536, current_train_items 179968.
I0302 19:01:04.529978 22535416901760 run.py:483] Algo bellman_ford step 5624 current loss 0.658401, current_train_items 180000.
I0302 19:01:04.549497 22535416901760 run.py:483] Algo bellman_ford step 5625 current loss 0.267302, current_train_items 180032.
I0302 19:01:04.565987 22535416901760 run.py:483] Algo bellman_ford step 5626 current loss 0.513843, current_train_items 180064.
I0302 19:01:04.589696 22535416901760 run.py:483] Algo bellman_ford step 5627 current loss 0.662521, current_train_items 180096.
I0302 19:01:04.620842 22535416901760 run.py:483] Algo bellman_ford step 5628 current loss 0.699247, current_train_items 180128.
I0302 19:01:04.655164 22535416901760 run.py:483] Algo bellman_ford step 5629 current loss 0.781693, current_train_items 180160.
I0302 19:01:04.674606 22535416901760 run.py:483] Algo bellman_ford step 5630 current loss 0.352006, current_train_items 180192.
I0302 19:01:04.690614 22535416901760 run.py:483] Algo bellman_ford step 5631 current loss 0.404913, current_train_items 180224.
I0302 19:01:04.714952 22535416901760 run.py:483] Algo bellman_ford step 5632 current loss 0.653866, current_train_items 180256.
I0302 19:01:04.745666 22535416901760 run.py:483] Algo bellman_ford step 5633 current loss 0.640914, current_train_items 180288.
I0302 19:01:04.779418 22535416901760 run.py:483] Algo bellman_ford step 5634 current loss 0.762986, current_train_items 180320.
I0302 19:01:04.798999 22535416901760 run.py:483] Algo bellman_ford step 5635 current loss 0.320380, current_train_items 180352.
I0302 19:01:04.815256 22535416901760 run.py:483] Algo bellman_ford step 5636 current loss 0.465035, current_train_items 180384.
I0302 19:01:04.838618 22535416901760 run.py:483] Algo bellman_ford step 5637 current loss 0.740737, current_train_items 180416.
I0302 19:01:04.868326 22535416901760 run.py:483] Algo bellman_ford step 5638 current loss 0.701362, current_train_items 180448.
I0302 19:01:04.900689 22535416901760 run.py:483] Algo bellman_ford step 5639 current loss 0.738130, current_train_items 180480.
I0302 19:01:04.920114 22535416901760 run.py:483] Algo bellman_ford step 5640 current loss 0.299440, current_train_items 180512.
I0302 19:01:04.935993 22535416901760 run.py:483] Algo bellman_ford step 5641 current loss 0.462384, current_train_items 180544.
I0302 19:01:04.959196 22535416901760 run.py:483] Algo bellman_ford step 5642 current loss 0.605162, current_train_items 180576.
I0302 19:01:04.989798 22535416901760 run.py:483] Algo bellman_ford step 5643 current loss 0.843182, current_train_items 180608.
I0302 19:01:05.022700 22535416901760 run.py:483] Algo bellman_ford step 5644 current loss 0.855215, current_train_items 180640.
I0302 19:01:05.042376 22535416901760 run.py:483] Algo bellman_ford step 5645 current loss 0.303100, current_train_items 180672.
I0302 19:01:05.059027 22535416901760 run.py:483] Algo bellman_ford step 5646 current loss 0.477222, current_train_items 180704.
I0302 19:01:05.081942 22535416901760 run.py:483] Algo bellman_ford step 5647 current loss 0.587175, current_train_items 180736.
I0302 19:01:05.112649 22535416901760 run.py:483] Algo bellman_ford step 5648 current loss 0.658939, current_train_items 180768.
I0302 19:01:05.147856 22535416901760 run.py:483] Algo bellman_ford step 5649 current loss 0.926588, current_train_items 180800.
I0302 19:01:05.167501 22535416901760 run.py:483] Algo bellman_ford step 5650 current loss 0.293331, current_train_items 180832.
I0302 19:01:05.175523 22535416901760 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:01:05.175631 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:01:05.192467 22535416901760 run.py:483] Algo bellman_ford step 5651 current loss 0.504438, current_train_items 180864.
I0302 19:01:05.215994 22535416901760 run.py:483] Algo bellman_ford step 5652 current loss 0.611888, current_train_items 180896.
I0302 19:01:05.248664 22535416901760 run.py:483] Algo bellman_ford step 5653 current loss 0.725735, current_train_items 180928.
I0302 19:01:05.283887 22535416901760 run.py:483] Algo bellman_ford step 5654 current loss 0.747926, current_train_items 180960.
I0302 19:01:05.304013 22535416901760 run.py:483] Algo bellman_ford step 5655 current loss 0.314989, current_train_items 180992.
I0302 19:01:05.320186 22535416901760 run.py:483] Algo bellman_ford step 5656 current loss 0.516694, current_train_items 181024.
I0302 19:01:05.342583 22535416901760 run.py:483] Algo bellman_ford step 5657 current loss 0.657353, current_train_items 181056.
I0302 19:01:05.374256 22535416901760 run.py:483] Algo bellman_ford step 5658 current loss 0.712037, current_train_items 181088.
I0302 19:01:05.408209 22535416901760 run.py:483] Algo bellman_ford step 5659 current loss 0.747786, current_train_items 181120.
I0302 19:01:05.428059 22535416901760 run.py:483] Algo bellman_ford step 5660 current loss 0.315782, current_train_items 181152.
I0302 19:01:05.444037 22535416901760 run.py:483] Algo bellman_ford step 5661 current loss 0.417753, current_train_items 181184.
I0302 19:01:05.466373 22535416901760 run.py:483] Algo bellman_ford step 5662 current loss 0.589611, current_train_items 181216.
I0302 19:01:05.499136 22535416901760 run.py:483] Algo bellman_ford step 5663 current loss 0.752718, current_train_items 181248.
I0302 19:01:05.532780 22535416901760 run.py:483] Algo bellman_ford step 5664 current loss 0.756779, current_train_items 181280.
I0302 19:01:05.552389 22535416901760 run.py:483] Algo bellman_ford step 5665 current loss 0.335483, current_train_items 181312.
I0302 19:01:05.568928 22535416901760 run.py:483] Algo bellman_ford step 5666 current loss 0.431077, current_train_items 181344.
I0302 19:01:05.592006 22535416901760 run.py:483] Algo bellman_ford step 5667 current loss 0.541267, current_train_items 181376.
I0302 19:01:05.623362 22535416901760 run.py:483] Algo bellman_ford step 5668 current loss 0.730761, current_train_items 181408.
I0302 19:01:05.655951 22535416901760 run.py:483] Algo bellman_ford step 5669 current loss 0.655241, current_train_items 181440.
I0302 19:01:05.675855 22535416901760 run.py:483] Algo bellman_ford step 5670 current loss 0.242920, current_train_items 181472.
I0302 19:01:05.691943 22535416901760 run.py:483] Algo bellman_ford step 5671 current loss 0.380293, current_train_items 181504.
I0302 19:01:05.714841 22535416901760 run.py:483] Algo bellman_ford step 5672 current loss 0.591664, current_train_items 181536.
I0302 19:01:05.745533 22535416901760 run.py:483] Algo bellman_ford step 5673 current loss 0.666883, current_train_items 181568.
I0302 19:01:05.778401 22535416901760 run.py:483] Algo bellman_ford step 5674 current loss 0.657468, current_train_items 181600.
I0302 19:01:05.797905 22535416901760 run.py:483] Algo bellman_ford step 5675 current loss 0.302548, current_train_items 181632.
I0302 19:01:05.813936 22535416901760 run.py:483] Algo bellman_ford step 5676 current loss 0.430068, current_train_items 181664.
I0302 19:01:05.837225 22535416901760 run.py:483] Algo bellman_ford step 5677 current loss 0.610717, current_train_items 181696.
I0302 19:01:05.869554 22535416901760 run.py:483] Algo bellman_ford step 5678 current loss 0.729472, current_train_items 181728.
I0302 19:01:05.902561 22535416901760 run.py:483] Algo bellman_ford step 5679 current loss 0.668739, current_train_items 181760.
I0302 19:01:05.922279 22535416901760 run.py:483] Algo bellman_ford step 5680 current loss 0.290539, current_train_items 181792.
I0302 19:01:05.938545 22535416901760 run.py:483] Algo bellman_ford step 5681 current loss 0.403848, current_train_items 181824.
I0302 19:01:05.961971 22535416901760 run.py:483] Algo bellman_ford step 5682 current loss 0.691326, current_train_items 181856.
I0302 19:01:05.992496 22535416901760 run.py:483] Algo bellman_ford step 5683 current loss 0.594455, current_train_items 181888.
I0302 19:01:06.024835 22535416901760 run.py:483] Algo bellman_ford step 5684 current loss 0.616234, current_train_items 181920.
I0302 19:01:06.044544 22535416901760 run.py:483] Algo bellman_ford step 5685 current loss 0.269343, current_train_items 181952.
I0302 19:01:06.060763 22535416901760 run.py:483] Algo bellman_ford step 5686 current loss 0.533469, current_train_items 181984.
I0302 19:01:06.084188 22535416901760 run.py:483] Algo bellman_ford step 5687 current loss 0.644193, current_train_items 182016.
I0302 19:01:06.115290 22535416901760 run.py:483] Algo bellman_ford step 5688 current loss 0.718019, current_train_items 182048.
I0302 19:01:06.147723 22535416901760 run.py:483] Algo bellman_ford step 5689 current loss 0.759422, current_train_items 182080.
I0302 19:01:06.167294 22535416901760 run.py:483] Algo bellman_ford step 5690 current loss 0.294670, current_train_items 182112.
I0302 19:01:06.184051 22535416901760 run.py:483] Algo bellman_ford step 5691 current loss 0.447583, current_train_items 182144.
I0302 19:01:06.207241 22535416901760 run.py:483] Algo bellman_ford step 5692 current loss 0.665786, current_train_items 182176.
I0302 19:01:06.237741 22535416901760 run.py:483] Algo bellman_ford step 5693 current loss 0.697713, current_train_items 182208.
I0302 19:01:06.270469 22535416901760 run.py:483] Algo bellman_ford step 5694 current loss 0.721658, current_train_items 182240.
I0302 19:01:06.290021 22535416901760 run.py:483] Algo bellman_ford step 5695 current loss 0.276804, current_train_items 182272.
I0302 19:01:06.306467 22535416901760 run.py:483] Algo bellman_ford step 5696 current loss 0.531169, current_train_items 182304.
I0302 19:01:06.329554 22535416901760 run.py:483] Algo bellman_ford step 5697 current loss 0.621516, current_train_items 182336.
I0302 19:01:06.361282 22535416901760 run.py:483] Algo bellman_ford step 5698 current loss 0.672712, current_train_items 182368.
I0302 19:01:06.395250 22535416901760 run.py:483] Algo bellman_ford step 5699 current loss 0.751650, current_train_items 182400.
I0302 19:01:06.414843 22535416901760 run.py:483] Algo bellman_ford step 5700 current loss 0.337253, current_train_items 182432.
I0302 19:01:06.422732 22535416901760 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:01:06.422838 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:06.439121 22535416901760 run.py:483] Algo bellman_ford step 5701 current loss 0.437828, current_train_items 182464.
I0302 19:01:06.461789 22535416901760 run.py:483] Algo bellman_ford step 5702 current loss 0.506334, current_train_items 182496.
I0302 19:01:06.493261 22535416901760 run.py:483] Algo bellman_ford step 5703 current loss 0.641431, current_train_items 182528.
I0302 19:01:06.528337 22535416901760 run.py:483] Algo bellman_ford step 5704 current loss 0.783459, current_train_items 182560.
I0302 19:01:06.548286 22535416901760 run.py:483] Algo bellman_ford step 5705 current loss 0.305406, current_train_items 182592.
I0302 19:01:06.564099 22535416901760 run.py:483] Algo bellman_ford step 5706 current loss 0.407203, current_train_items 182624.
I0302 19:01:06.588323 22535416901760 run.py:483] Algo bellman_ford step 5707 current loss 0.626219, current_train_items 182656.
I0302 19:01:06.618938 22535416901760 run.py:483] Algo bellman_ford step 5708 current loss 0.660277, current_train_items 182688.
I0302 19:01:06.652469 22535416901760 run.py:483] Algo bellman_ford step 5709 current loss 0.674349, current_train_items 182720.
I0302 19:01:06.672291 22535416901760 run.py:483] Algo bellman_ford step 5710 current loss 0.251070, current_train_items 182752.
I0302 19:01:06.688238 22535416901760 run.py:483] Algo bellman_ford step 5711 current loss 0.563311, current_train_items 182784.
I0302 19:01:06.711007 22535416901760 run.py:483] Algo bellman_ford step 5712 current loss 0.599617, current_train_items 182816.
I0302 19:01:06.741242 22535416901760 run.py:483] Algo bellman_ford step 5713 current loss 0.621436, current_train_items 182848.
I0302 19:01:06.773850 22535416901760 run.py:483] Algo bellman_ford step 5714 current loss 0.662619, current_train_items 182880.
I0302 19:01:06.793569 22535416901760 run.py:483] Algo bellman_ford step 5715 current loss 0.364008, current_train_items 182912.
I0302 19:01:06.809726 22535416901760 run.py:483] Algo bellman_ford step 5716 current loss 0.476037, current_train_items 182944.
I0302 19:01:06.833708 22535416901760 run.py:483] Algo bellman_ford step 5717 current loss 0.647932, current_train_items 182976.
I0302 19:01:06.863601 22535416901760 run.py:483] Algo bellman_ford step 5718 current loss 0.674776, current_train_items 183008.
I0302 19:01:06.897322 22535416901760 run.py:483] Algo bellman_ford step 5719 current loss 0.817684, current_train_items 183040.
I0302 19:01:06.916755 22535416901760 run.py:483] Algo bellman_ford step 5720 current loss 0.299384, current_train_items 183072.
I0302 19:01:06.932863 22535416901760 run.py:483] Algo bellman_ford step 5721 current loss 0.462257, current_train_items 183104.
I0302 19:01:06.955923 22535416901760 run.py:483] Algo bellman_ford step 5722 current loss 0.585082, current_train_items 183136.
I0302 19:01:06.986125 22535416901760 run.py:483] Algo bellman_ford step 5723 current loss 0.674812, current_train_items 183168.
I0302 19:01:07.018910 22535416901760 run.py:483] Algo bellman_ford step 5724 current loss 0.693853, current_train_items 183200.
I0302 19:01:07.038142 22535416901760 run.py:483] Algo bellman_ford step 5725 current loss 0.351018, current_train_items 183232.
I0302 19:01:07.054312 22535416901760 run.py:483] Algo bellman_ford step 5726 current loss 0.500107, current_train_items 183264.
I0302 19:01:07.078662 22535416901760 run.py:483] Algo bellman_ford step 5727 current loss 0.559162, current_train_items 183296.
I0302 19:01:07.108928 22535416901760 run.py:483] Algo bellman_ford step 5728 current loss 0.657113, current_train_items 183328.
I0302 19:01:07.142255 22535416901760 run.py:483] Algo bellman_ford step 5729 current loss 0.768737, current_train_items 183360.
I0302 19:01:07.161713 22535416901760 run.py:483] Algo bellman_ford step 5730 current loss 0.331067, current_train_items 183392.
I0302 19:01:07.178275 22535416901760 run.py:483] Algo bellman_ford step 5731 current loss 0.559678, current_train_items 183424.
I0302 19:01:07.202537 22535416901760 run.py:483] Algo bellman_ford step 5732 current loss 0.696212, current_train_items 183456.
I0302 19:01:07.233015 22535416901760 run.py:483] Algo bellman_ford step 5733 current loss 0.739339, current_train_items 183488.
I0302 19:01:07.266987 22535416901760 run.py:483] Algo bellman_ford step 5734 current loss 0.791017, current_train_items 183520.
I0302 19:01:07.286724 22535416901760 run.py:483] Algo bellman_ford step 5735 current loss 0.297978, current_train_items 183552.
I0302 19:01:07.303132 22535416901760 run.py:483] Algo bellman_ford step 5736 current loss 0.490184, current_train_items 183584.
I0302 19:01:07.326620 22535416901760 run.py:483] Algo bellman_ford step 5737 current loss 0.638951, current_train_items 183616.
I0302 19:01:07.356512 22535416901760 run.py:483] Algo bellman_ford step 5738 current loss 0.599082, current_train_items 183648.
I0302 19:01:07.390234 22535416901760 run.py:483] Algo bellman_ford step 5739 current loss 0.693510, current_train_items 183680.
I0302 19:01:07.409679 22535416901760 run.py:483] Algo bellman_ford step 5740 current loss 0.358089, current_train_items 183712.
I0302 19:01:07.426165 22535416901760 run.py:483] Algo bellman_ford step 5741 current loss 0.426200, current_train_items 183744.
I0302 19:01:07.450427 22535416901760 run.py:483] Algo bellman_ford step 5742 current loss 0.553731, current_train_items 183776.
I0302 19:01:07.481985 22535416901760 run.py:483] Algo bellman_ford step 5743 current loss 0.639933, current_train_items 183808.
I0302 19:01:07.516530 22535416901760 run.py:483] Algo bellman_ford step 5744 current loss 0.766952, current_train_items 183840.
I0302 19:01:07.536254 22535416901760 run.py:483] Algo bellman_ford step 5745 current loss 0.329694, current_train_items 183872.
I0302 19:01:07.552885 22535416901760 run.py:483] Algo bellman_ford step 5746 current loss 0.490860, current_train_items 183904.
I0302 19:01:07.576675 22535416901760 run.py:483] Algo bellman_ford step 5747 current loss 0.587543, current_train_items 183936.
I0302 19:01:07.607527 22535416901760 run.py:483] Algo bellman_ford step 5748 current loss 0.756008, current_train_items 183968.
I0302 19:01:07.640325 22535416901760 run.py:483] Algo bellman_ford step 5749 current loss 0.742439, current_train_items 184000.
I0302 19:01:07.659889 22535416901760 run.py:483] Algo bellman_ford step 5750 current loss 0.354606, current_train_items 184032.
I0302 19:01:07.668270 22535416901760 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:01:07.668375 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:07.685510 22535416901760 run.py:483] Algo bellman_ford step 5751 current loss 0.453344, current_train_items 184064.
I0302 19:01:07.708671 22535416901760 run.py:483] Algo bellman_ford step 5752 current loss 0.666283, current_train_items 184096.
I0302 19:01:07.738664 22535416901760 run.py:483] Algo bellman_ford step 5753 current loss 0.625111, current_train_items 184128.
I0302 19:01:07.773580 22535416901760 run.py:483] Algo bellman_ford step 5754 current loss 0.868982, current_train_items 184160.
I0302 19:01:07.793678 22535416901760 run.py:483] Algo bellman_ford step 5755 current loss 0.289333, current_train_items 184192.
I0302 19:01:07.809265 22535416901760 run.py:483] Algo bellman_ford step 5756 current loss 0.397445, current_train_items 184224.
I0302 19:01:07.832937 22535416901760 run.py:483] Algo bellman_ford step 5757 current loss 0.719486, current_train_items 184256.
I0302 19:01:07.864540 22535416901760 run.py:483] Algo bellman_ford step 5758 current loss 0.773562, current_train_items 184288.
I0302 19:01:07.897989 22535416901760 run.py:483] Algo bellman_ford step 5759 current loss 0.809259, current_train_items 184320.
I0302 19:01:07.917501 22535416901760 run.py:483] Algo bellman_ford step 5760 current loss 0.409661, current_train_items 184352.
I0302 19:01:07.933960 22535416901760 run.py:483] Algo bellman_ford step 5761 current loss 0.449469, current_train_items 184384.
I0302 19:01:07.957126 22535416901760 run.py:483] Algo bellman_ford step 5762 current loss 0.551162, current_train_items 184416.
I0302 19:01:07.986914 22535416901760 run.py:483] Algo bellman_ford step 5763 current loss 0.656979, current_train_items 184448.
I0302 19:01:08.020915 22535416901760 run.py:483] Algo bellman_ford step 5764 current loss 0.784470, current_train_items 184480.
I0302 19:01:08.040635 22535416901760 run.py:483] Algo bellman_ford step 5765 current loss 0.351817, current_train_items 184512.
I0302 19:01:08.056766 22535416901760 run.py:483] Algo bellman_ford step 5766 current loss 0.485901, current_train_items 184544.
I0302 19:01:08.079889 22535416901760 run.py:483] Algo bellman_ford step 5767 current loss 0.581970, current_train_items 184576.
I0302 19:01:08.111455 22535416901760 run.py:483] Algo bellman_ford step 5768 current loss 0.656133, current_train_items 184608.
I0302 19:01:08.144545 22535416901760 run.py:483] Algo bellman_ford step 5769 current loss 0.717615, current_train_items 184640.
I0302 19:01:08.164329 22535416901760 run.py:483] Algo bellman_ford step 5770 current loss 0.315129, current_train_items 184672.
I0302 19:01:08.180413 22535416901760 run.py:483] Algo bellman_ford step 5771 current loss 0.485994, current_train_items 184704.
I0302 19:01:08.202197 22535416901760 run.py:483] Algo bellman_ford step 5772 current loss 0.637559, current_train_items 184736.
I0302 19:01:08.232195 22535416901760 run.py:483] Algo bellman_ford step 5773 current loss 0.767772, current_train_items 184768.
I0302 19:01:08.264479 22535416901760 run.py:483] Algo bellman_ford step 5774 current loss 0.751550, current_train_items 184800.
I0302 19:01:08.284049 22535416901760 run.py:483] Algo bellman_ford step 5775 current loss 0.320196, current_train_items 184832.
I0302 19:01:08.300555 22535416901760 run.py:483] Algo bellman_ford step 5776 current loss 0.431218, current_train_items 184864.
I0302 19:01:08.322650 22535416901760 run.py:483] Algo bellman_ford step 5777 current loss 0.581148, current_train_items 184896.
I0302 19:01:08.353594 22535416901760 run.py:483] Algo bellman_ford step 5778 current loss 0.704273, current_train_items 184928.
I0302 19:01:08.387108 22535416901760 run.py:483] Algo bellman_ford step 5779 current loss 0.869120, current_train_items 184960.
I0302 19:01:08.406447 22535416901760 run.py:483] Algo bellman_ford step 5780 current loss 0.266065, current_train_items 184992.
I0302 19:01:08.422494 22535416901760 run.py:483] Algo bellman_ford step 5781 current loss 0.443917, current_train_items 185024.
I0302 19:01:08.446309 22535416901760 run.py:483] Algo bellman_ford step 5782 current loss 0.611797, current_train_items 185056.
I0302 19:01:08.476032 22535416901760 run.py:483] Algo bellman_ford step 5783 current loss 0.582791, current_train_items 185088.
I0302 19:01:08.507498 22535416901760 run.py:483] Algo bellman_ford step 5784 current loss 0.716748, current_train_items 185120.
I0302 19:01:08.527120 22535416901760 run.py:483] Algo bellman_ford step 5785 current loss 0.326520, current_train_items 185152.
I0302 19:01:08.543100 22535416901760 run.py:483] Algo bellman_ford step 5786 current loss 0.428062, current_train_items 185184.
I0302 19:01:08.565017 22535416901760 run.py:483] Algo bellman_ford step 5787 current loss 0.509540, current_train_items 185216.
I0302 19:01:08.596397 22535416901760 run.py:483] Algo bellman_ford step 5788 current loss 0.667750, current_train_items 185248.
I0302 19:01:08.628345 22535416901760 run.py:483] Algo bellman_ford step 5789 current loss 0.776271, current_train_items 185280.
I0302 19:01:08.648017 22535416901760 run.py:483] Algo bellman_ford step 5790 current loss 0.248971, current_train_items 185312.
I0302 19:01:08.664553 22535416901760 run.py:483] Algo bellman_ford step 5791 current loss 0.513572, current_train_items 185344.
I0302 19:01:08.686671 22535416901760 run.py:483] Algo bellman_ford step 5792 current loss 0.606154, current_train_items 185376.
I0302 19:01:08.717278 22535416901760 run.py:483] Algo bellman_ford step 5793 current loss 0.637204, current_train_items 185408.
I0302 19:01:08.750283 22535416901760 run.py:483] Algo bellman_ford step 5794 current loss 0.678352, current_train_items 185440.
I0302 19:01:08.769648 22535416901760 run.py:483] Algo bellman_ford step 5795 current loss 0.329120, current_train_items 185472.
I0302 19:01:08.785612 22535416901760 run.py:483] Algo bellman_ford step 5796 current loss 0.491321, current_train_items 185504.
I0302 19:01:08.809323 22535416901760 run.py:483] Algo bellman_ford step 5797 current loss 0.670483, current_train_items 185536.
I0302 19:01:08.839907 22535416901760 run.py:483] Algo bellman_ford step 5798 current loss 0.665122, current_train_items 185568.
I0302 19:01:08.873646 22535416901760 run.py:483] Algo bellman_ford step 5799 current loss 0.823725, current_train_items 185600.
I0302 19:01:08.893252 22535416901760 run.py:483] Algo bellman_ford step 5800 current loss 0.325354, current_train_items 185632.
I0302 19:01:08.900931 22535416901760 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:01:08.901037 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.942, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:08.917990 22535416901760 run.py:483] Algo bellman_ford step 5801 current loss 0.562265, current_train_items 185664.
I0302 19:01:08.940853 22535416901760 run.py:483] Algo bellman_ford step 5802 current loss 0.678836, current_train_items 185696.
I0302 19:01:08.972198 22535416901760 run.py:483] Algo bellman_ford step 5803 current loss 0.825463, current_train_items 185728.
I0302 19:01:09.007293 22535416901760 run.py:483] Algo bellman_ford step 5804 current loss 0.961614, current_train_items 185760.
I0302 19:01:09.027196 22535416901760 run.py:483] Algo bellman_ford step 5805 current loss 0.337582, current_train_items 185792.
I0302 19:01:09.043284 22535416901760 run.py:483] Algo bellman_ford step 5806 current loss 0.398107, current_train_items 185824.
I0302 19:01:09.067367 22535416901760 run.py:483] Algo bellman_ford step 5807 current loss 0.578393, current_train_items 185856.
I0302 19:01:09.098700 22535416901760 run.py:483] Algo bellman_ford step 5808 current loss 0.735758, current_train_items 185888.
I0302 19:01:09.131581 22535416901760 run.py:483] Algo bellman_ford step 5809 current loss 0.771798, current_train_items 185920.
I0302 19:01:09.151320 22535416901760 run.py:483] Algo bellman_ford step 5810 current loss 0.280659, current_train_items 185952.
I0302 19:01:09.167240 22535416901760 run.py:483] Algo bellman_ford step 5811 current loss 0.463051, current_train_items 185984.
I0302 19:01:09.191104 22535416901760 run.py:483] Algo bellman_ford step 5812 current loss 0.653466, current_train_items 186016.
I0302 19:01:09.222492 22535416901760 run.py:483] Algo bellman_ford step 5813 current loss 0.720120, current_train_items 186048.
I0302 19:01:09.256534 22535416901760 run.py:483] Algo bellman_ford step 5814 current loss 0.845331, current_train_items 186080.
I0302 19:01:09.276427 22535416901760 run.py:483] Algo bellman_ford step 5815 current loss 0.322692, current_train_items 186112.
I0302 19:01:09.292750 22535416901760 run.py:483] Algo bellman_ford step 5816 current loss 0.461045, current_train_items 186144.
I0302 19:01:09.316320 22535416901760 run.py:483] Algo bellman_ford step 5817 current loss 0.723444, current_train_items 186176.
I0302 19:01:09.346068 22535416901760 run.py:483] Algo bellman_ford step 5818 current loss 0.679731, current_train_items 186208.
I0302 19:01:09.379956 22535416901760 run.py:483] Algo bellman_ford step 5819 current loss 0.800182, current_train_items 186240.
I0302 19:01:09.399322 22535416901760 run.py:483] Algo bellman_ford step 5820 current loss 0.297848, current_train_items 186272.
I0302 19:01:09.415386 22535416901760 run.py:483] Algo bellman_ford step 5821 current loss 0.459750, current_train_items 186304.
I0302 19:01:09.438637 22535416901760 run.py:483] Algo bellman_ford step 5822 current loss 0.553604, current_train_items 186336.
I0302 19:01:09.469395 22535416901760 run.py:483] Algo bellman_ford step 5823 current loss 0.680451, current_train_items 186368.
I0302 19:01:09.502409 22535416901760 run.py:483] Algo bellman_ford step 5824 current loss 0.768426, current_train_items 186400.
I0302 19:01:09.521811 22535416901760 run.py:483] Algo bellman_ford step 5825 current loss 0.334955, current_train_items 186432.
I0302 19:01:09.538006 22535416901760 run.py:483] Algo bellman_ford step 5826 current loss 0.463341, current_train_items 186464.
I0302 19:01:09.562124 22535416901760 run.py:483] Algo bellman_ford step 5827 current loss 0.622156, current_train_items 186496.
I0302 19:01:09.593797 22535416901760 run.py:483] Algo bellman_ford step 5828 current loss 0.660495, current_train_items 186528.
I0302 19:01:09.628429 22535416901760 run.py:483] Algo bellman_ford step 5829 current loss 0.841978, current_train_items 186560.
I0302 19:01:09.647979 22535416901760 run.py:483] Algo bellman_ford step 5830 current loss 0.328431, current_train_items 186592.
I0302 19:01:09.664321 22535416901760 run.py:483] Algo bellman_ford step 5831 current loss 0.467773, current_train_items 186624.
I0302 19:01:09.687296 22535416901760 run.py:483] Algo bellman_ford step 5832 current loss 0.523798, current_train_items 186656.
I0302 19:01:09.718244 22535416901760 run.py:483] Algo bellman_ford step 5833 current loss 0.642853, current_train_items 186688.
I0302 19:01:09.748247 22535416901760 run.py:483] Algo bellman_ford step 5834 current loss 0.581510, current_train_items 186720.
I0302 19:01:09.768262 22535416901760 run.py:483] Algo bellman_ford step 5835 current loss 0.284019, current_train_items 186752.
I0302 19:01:09.783766 22535416901760 run.py:483] Algo bellman_ford step 5836 current loss 0.436743, current_train_items 186784.
I0302 19:01:09.808646 22535416901760 run.py:483] Algo bellman_ford step 5837 current loss 0.653136, current_train_items 186816.
I0302 19:01:09.838064 22535416901760 run.py:483] Algo bellman_ford step 5838 current loss 0.537534, current_train_items 186848.
I0302 19:01:09.872439 22535416901760 run.py:483] Algo bellman_ford step 5839 current loss 0.764473, current_train_items 186880.
I0302 19:01:09.892364 22535416901760 run.py:483] Algo bellman_ford step 5840 current loss 0.321405, current_train_items 186912.
I0302 19:01:09.908746 22535416901760 run.py:483] Algo bellman_ford step 5841 current loss 0.438345, current_train_items 186944.
I0302 19:01:09.931873 22535416901760 run.py:483] Algo bellman_ford step 5842 current loss 0.624800, current_train_items 186976.
I0302 19:01:09.961482 22535416901760 run.py:483] Algo bellman_ford step 5843 current loss 0.600181, current_train_items 187008.
I0302 19:01:09.993913 22535416901760 run.py:483] Algo bellman_ford step 5844 current loss 0.722626, current_train_items 187040.
I0302 19:01:10.013833 22535416901760 run.py:483] Algo bellman_ford step 5845 current loss 0.345425, current_train_items 187072.
I0302 19:01:10.030146 22535416901760 run.py:483] Algo bellman_ford step 5846 current loss 0.401188, current_train_items 187104.
I0302 19:01:10.054602 22535416901760 run.py:483] Algo bellman_ford step 5847 current loss 0.584488, current_train_items 187136.
I0302 19:01:10.083957 22535416901760 run.py:483] Algo bellman_ford step 5848 current loss 0.596952, current_train_items 187168.
I0302 19:01:10.117352 22535416901760 run.py:483] Algo bellman_ford step 5849 current loss 0.673899, current_train_items 187200.
I0302 19:01:10.136819 22535416901760 run.py:483] Algo bellman_ford step 5850 current loss 0.260759, current_train_items 187232.
I0302 19:01:10.145095 22535416901760 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:01:10.145211 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.942, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0302 19:01:10.175674 22535416901760 run.py:483] Algo bellman_ford step 5851 current loss 0.490390, current_train_items 187264.
I0302 19:01:10.198919 22535416901760 run.py:483] Algo bellman_ford step 5852 current loss 0.630688, current_train_items 187296.
I0302 19:01:10.229378 22535416901760 run.py:483] Algo bellman_ford step 5853 current loss 0.578234, current_train_items 187328.
I0302 19:01:10.264064 22535416901760 run.py:483] Algo bellman_ford step 5854 current loss 0.753598, current_train_items 187360.
I0302 19:01:10.284309 22535416901760 run.py:483] Algo bellman_ford step 5855 current loss 0.347324, current_train_items 187392.
I0302 19:01:10.300329 22535416901760 run.py:483] Algo bellman_ford step 5856 current loss 0.600603, current_train_items 187424.
I0302 19:01:10.324566 22535416901760 run.py:483] Algo bellman_ford step 5857 current loss 0.559244, current_train_items 187456.
I0302 19:01:10.354552 22535416901760 run.py:483] Algo bellman_ford step 5858 current loss 0.543716, current_train_items 187488.
I0302 19:01:10.388373 22535416901760 run.py:483] Algo bellman_ford step 5859 current loss 0.701389, current_train_items 187520.
I0302 19:01:10.408327 22535416901760 run.py:483] Algo bellman_ford step 5860 current loss 0.286133, current_train_items 187552.
I0302 19:01:10.424808 22535416901760 run.py:483] Algo bellman_ford step 5861 current loss 0.423950, current_train_items 187584.
I0302 19:01:10.447173 22535416901760 run.py:483] Algo bellman_ford step 5862 current loss 0.617233, current_train_items 187616.
I0302 19:01:10.478282 22535416901760 run.py:483] Algo bellman_ford step 5863 current loss 0.668326, current_train_items 187648.
I0302 19:01:10.512332 22535416901760 run.py:483] Algo bellman_ford step 5864 current loss 0.869388, current_train_items 187680.
I0302 19:01:10.531831 22535416901760 run.py:483] Algo bellman_ford step 5865 current loss 0.357413, current_train_items 187712.
I0302 19:01:10.548511 22535416901760 run.py:483] Algo bellman_ford step 5866 current loss 0.464165, current_train_items 187744.
I0302 19:01:10.571922 22535416901760 run.py:483] Algo bellman_ford step 5867 current loss 0.576219, current_train_items 187776.
I0302 19:01:10.602428 22535416901760 run.py:483] Algo bellman_ford step 5868 current loss 0.683310, current_train_items 187808.
I0302 19:01:10.637395 22535416901760 run.py:483] Algo bellman_ford step 5869 current loss 0.755586, current_train_items 187840.
I0302 19:01:10.657537 22535416901760 run.py:483] Algo bellman_ford step 5870 current loss 0.275556, current_train_items 187872.
I0302 19:01:10.673775 22535416901760 run.py:483] Algo bellman_ford step 5871 current loss 0.422663, current_train_items 187904.
I0302 19:01:10.695990 22535416901760 run.py:483] Algo bellman_ford step 5872 current loss 0.530435, current_train_items 187936.
I0302 19:01:10.726234 22535416901760 run.py:483] Algo bellman_ford step 5873 current loss 0.616530, current_train_items 187968.
I0302 19:01:10.760152 22535416901760 run.py:483] Algo bellman_ford step 5874 current loss 0.794531, current_train_items 188000.
I0302 19:01:10.780314 22535416901760 run.py:483] Algo bellman_ford step 5875 current loss 0.333673, current_train_items 188032.
I0302 19:01:10.796416 22535416901760 run.py:483] Algo bellman_ford step 5876 current loss 0.468836, current_train_items 188064.
I0302 19:01:10.819052 22535416901760 run.py:483] Algo bellman_ford step 5877 current loss 0.561800, current_train_items 188096.
I0302 19:01:10.849546 22535416901760 run.py:483] Algo bellman_ford step 5878 current loss 0.668696, current_train_items 188128.
I0302 19:01:10.883843 22535416901760 run.py:483] Algo bellman_ford step 5879 current loss 0.868549, current_train_items 188160.
I0302 19:01:10.903668 22535416901760 run.py:483] Algo bellman_ford step 5880 current loss 0.366969, current_train_items 188192.
I0302 19:01:10.919651 22535416901760 run.py:483] Algo bellman_ford step 5881 current loss 0.498070, current_train_items 188224.
I0302 19:01:10.943360 22535416901760 run.py:483] Algo bellman_ford step 5882 current loss 0.602743, current_train_items 188256.
I0302 19:01:10.973855 22535416901760 run.py:483] Algo bellman_ford step 5883 current loss 0.670089, current_train_items 188288.
I0302 19:01:11.008349 22535416901760 run.py:483] Algo bellman_ford step 5884 current loss 0.774225, current_train_items 188320.
I0302 19:01:11.028434 22535416901760 run.py:483] Algo bellman_ford step 5885 current loss 0.326069, current_train_items 188352.
I0302 19:01:11.044421 22535416901760 run.py:483] Algo bellman_ford step 5886 current loss 0.469635, current_train_items 188384.
I0302 19:01:11.067218 22535416901760 run.py:483] Algo bellman_ford step 5887 current loss 0.555119, current_train_items 188416.
I0302 19:01:11.097087 22535416901760 run.py:483] Algo bellman_ford step 5888 current loss 0.589339, current_train_items 188448.
I0302 19:01:11.129141 22535416901760 run.py:483] Algo bellman_ford step 5889 current loss 0.644244, current_train_items 188480.
I0302 19:01:11.149314 22535416901760 run.py:483] Algo bellman_ford step 5890 current loss 0.342458, current_train_items 188512.
I0302 19:01:11.165512 22535416901760 run.py:483] Algo bellman_ford step 5891 current loss 0.503581, current_train_items 188544.
I0302 19:01:11.188665 22535416901760 run.py:483] Algo bellman_ford step 5892 current loss 0.576393, current_train_items 188576.
I0302 19:01:11.218487 22535416901760 run.py:483] Algo bellman_ford step 5893 current loss 0.553785, current_train_items 188608.
I0302 19:01:11.249733 22535416901760 run.py:483] Algo bellman_ford step 5894 current loss 0.644481, current_train_items 188640.
I0302 19:01:11.269420 22535416901760 run.py:483] Algo bellman_ford step 5895 current loss 0.269968, current_train_items 188672.
I0302 19:01:11.285650 22535416901760 run.py:483] Algo bellman_ford step 5896 current loss 0.447710, current_train_items 188704.
I0302 19:01:11.309440 22535416901760 run.py:483] Algo bellman_ford step 5897 current loss 0.625217, current_train_items 188736.
I0302 19:01:11.340997 22535416901760 run.py:483] Algo bellman_ford step 5898 current loss 0.643190, current_train_items 188768.
I0302 19:01:11.374072 22535416901760 run.py:483] Algo bellman_ford step 5899 current loss 0.775520, current_train_items 188800.
I0302 19:01:11.393881 22535416901760 run.py:483] Algo bellman_ford step 5900 current loss 0.303720, current_train_items 188832.
I0302 19:01:11.401625 22535416901760 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:01:11.401734 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:11.418226 22535416901760 run.py:483] Algo bellman_ford step 5901 current loss 0.442679, current_train_items 188864.
I0302 19:01:11.442736 22535416901760 run.py:483] Algo bellman_ford step 5902 current loss 0.582950, current_train_items 188896.
I0302 19:01:11.474045 22535416901760 run.py:483] Algo bellman_ford step 5903 current loss 0.667755, current_train_items 188928.
I0302 19:01:11.505668 22535416901760 run.py:483] Algo bellman_ford step 5904 current loss 0.711418, current_train_items 188960.
I0302 19:01:11.525790 22535416901760 run.py:483] Algo bellman_ford step 5905 current loss 0.311556, current_train_items 188992.
I0302 19:01:11.541977 22535416901760 run.py:483] Algo bellman_ford step 5906 current loss 0.539214, current_train_items 189024.
I0302 19:01:11.565101 22535416901760 run.py:483] Algo bellman_ford step 5907 current loss 0.657587, current_train_items 189056.
I0302 19:01:11.595719 22535416901760 run.py:483] Algo bellman_ford step 5908 current loss 0.543506, current_train_items 189088.
I0302 19:01:11.627437 22535416901760 run.py:483] Algo bellman_ford step 5909 current loss 0.775838, current_train_items 189120.
I0302 19:01:11.646858 22535416901760 run.py:483] Algo bellman_ford step 5910 current loss 0.307473, current_train_items 189152.
I0302 19:01:11.663192 22535416901760 run.py:483] Algo bellman_ford step 5911 current loss 0.623426, current_train_items 189184.
I0302 19:01:11.686820 22535416901760 run.py:483] Algo bellman_ford step 5912 current loss 0.801345, current_train_items 189216.
I0302 19:01:11.716745 22535416901760 run.py:483] Algo bellman_ford step 5913 current loss 0.672633, current_train_items 189248.
I0302 19:01:11.748966 22535416901760 run.py:483] Algo bellman_ford step 5914 current loss 0.783545, current_train_items 189280.
I0302 19:01:11.768280 22535416901760 run.py:483] Algo bellman_ford step 5915 current loss 0.317876, current_train_items 189312.
I0302 19:01:11.784502 22535416901760 run.py:483] Algo bellman_ford step 5916 current loss 0.421657, current_train_items 189344.
I0302 19:01:11.807677 22535416901760 run.py:483] Algo bellman_ford step 5917 current loss 0.625214, current_train_items 189376.
I0302 19:01:11.837347 22535416901760 run.py:483] Algo bellman_ford step 5918 current loss 0.803431, current_train_items 189408.
I0302 19:01:11.871371 22535416901760 run.py:483] Algo bellman_ford step 5919 current loss 1.119219, current_train_items 189440.
I0302 19:01:11.890981 22535416901760 run.py:483] Algo bellman_ford step 5920 current loss 0.292057, current_train_items 189472.
I0302 19:01:11.906982 22535416901760 run.py:483] Algo bellman_ford step 5921 current loss 0.523727, current_train_items 189504.
I0302 19:01:11.929540 22535416901760 run.py:483] Algo bellman_ford step 5922 current loss 0.628383, current_train_items 189536.
I0302 19:01:11.958937 22535416901760 run.py:483] Algo bellman_ford step 5923 current loss 0.595150, current_train_items 189568.
I0302 19:01:11.991870 22535416901760 run.py:483] Algo bellman_ford step 5924 current loss 0.737576, current_train_items 189600.
I0302 19:01:12.010966 22535416901760 run.py:483] Algo bellman_ford step 5925 current loss 0.286465, current_train_items 189632.
I0302 19:01:12.026717 22535416901760 run.py:483] Algo bellman_ford step 5926 current loss 0.500642, current_train_items 189664.
I0302 19:01:12.049144 22535416901760 run.py:483] Algo bellman_ford step 5927 current loss 0.629091, current_train_items 189696.
I0302 19:01:12.080163 22535416901760 run.py:483] Algo bellman_ford step 5928 current loss 0.838390, current_train_items 189728.
I0302 19:01:12.112229 22535416901760 run.py:483] Algo bellman_ford step 5929 current loss 0.763669, current_train_items 189760.
I0302 19:01:12.131840 22535416901760 run.py:483] Algo bellman_ford step 5930 current loss 0.307352, current_train_items 189792.
I0302 19:01:12.148073 22535416901760 run.py:483] Algo bellman_ford step 5931 current loss 0.462970, current_train_items 189824.
I0302 19:01:12.171190 22535416901760 run.py:483] Algo bellman_ford step 5932 current loss 0.627481, current_train_items 189856.
I0302 19:01:12.200680 22535416901760 run.py:483] Algo bellman_ford step 5933 current loss 0.631039, current_train_items 189888.
I0302 19:01:12.234948 22535416901760 run.py:483] Algo bellman_ford step 5934 current loss 0.847984, current_train_items 189920.
I0302 19:01:12.254677 22535416901760 run.py:483] Algo bellman_ford step 5935 current loss 0.369113, current_train_items 189952.
I0302 19:01:12.270426 22535416901760 run.py:483] Algo bellman_ford step 5936 current loss 0.495123, current_train_items 189984.
I0302 19:01:12.293697 22535416901760 run.py:483] Algo bellman_ford step 5937 current loss 0.659512, current_train_items 190016.
I0302 19:01:12.325132 22535416901760 run.py:483] Algo bellman_ford step 5938 current loss 0.678825, current_train_items 190048.
I0302 19:01:12.356454 22535416901760 run.py:483] Algo bellman_ford step 5939 current loss 0.789670, current_train_items 190080.
I0302 19:01:12.376127 22535416901760 run.py:483] Algo bellman_ford step 5940 current loss 0.295200, current_train_items 190112.
I0302 19:01:12.391926 22535416901760 run.py:483] Algo bellman_ford step 5941 current loss 0.450377, current_train_items 190144.
I0302 19:01:12.415577 22535416901760 run.py:483] Algo bellman_ford step 5942 current loss 0.566758, current_train_items 190176.
I0302 19:01:12.446321 22535416901760 run.py:483] Algo bellman_ford step 5943 current loss 0.610682, current_train_items 190208.
I0302 19:01:12.479676 22535416901760 run.py:483] Algo bellman_ford step 5944 current loss 0.704844, current_train_items 190240.
I0302 19:01:12.499364 22535416901760 run.py:483] Algo bellman_ford step 5945 current loss 0.245068, current_train_items 190272.
I0302 19:01:12.515725 22535416901760 run.py:483] Algo bellman_ford step 5946 current loss 0.508657, current_train_items 190304.
I0302 19:01:12.539148 22535416901760 run.py:483] Algo bellman_ford step 5947 current loss 0.601105, current_train_items 190336.
I0302 19:01:12.568659 22535416901760 run.py:483] Algo bellman_ford step 5948 current loss 0.587627, current_train_items 190368.
I0302 19:01:12.601819 22535416901760 run.py:483] Algo bellman_ford step 5949 current loss 0.775671, current_train_items 190400.
I0302 19:01:12.621443 22535416901760 run.py:483] Algo bellman_ford step 5950 current loss 0.297745, current_train_items 190432.
I0302 19:01:12.629561 22535416901760 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:01:12.629667 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0302 19:01:12.646409 22535416901760 run.py:483] Algo bellman_ford step 5951 current loss 0.449132, current_train_items 190464.
I0302 19:01:12.670720 22535416901760 run.py:483] Algo bellman_ford step 5952 current loss 0.625322, current_train_items 190496.
I0302 19:01:12.701869 22535416901760 run.py:483] Algo bellman_ford step 5953 current loss 0.659073, current_train_items 190528.
I0302 19:01:12.735705 22535416901760 run.py:483] Algo bellman_ford step 5954 current loss 0.716681, current_train_items 190560.
I0302 19:01:12.755981 22535416901760 run.py:483] Algo bellman_ford step 5955 current loss 0.286041, current_train_items 190592.
I0302 19:01:12.772289 22535416901760 run.py:483] Algo bellman_ford step 5956 current loss 0.451003, current_train_items 190624.
I0302 19:01:12.794910 22535416901760 run.py:483] Algo bellman_ford step 5957 current loss 0.541400, current_train_items 190656.
I0302 19:01:12.825256 22535416901760 run.py:483] Algo bellman_ford step 5958 current loss 0.566177, current_train_items 190688.
I0302 19:01:12.858676 22535416901760 run.py:483] Algo bellman_ford step 5959 current loss 0.692325, current_train_items 190720.
I0302 19:01:12.878819 22535416901760 run.py:483] Algo bellman_ford step 5960 current loss 0.261077, current_train_items 190752.
I0302 19:01:12.894965 22535416901760 run.py:483] Algo bellman_ford step 5961 current loss 0.503029, current_train_items 190784.
I0302 19:01:12.918591 22535416901760 run.py:483] Algo bellman_ford step 5962 current loss 0.585292, current_train_items 190816.
I0302 19:01:12.947480 22535416901760 run.py:483] Algo bellman_ford step 5963 current loss 0.624686, current_train_items 190848.
I0302 19:01:12.980523 22535416901760 run.py:483] Algo bellman_ford step 5964 current loss 0.752307, current_train_items 190880.
I0302 19:01:13.000319 22535416901760 run.py:483] Algo bellman_ford step 5965 current loss 0.311859, current_train_items 190912.
I0302 19:01:13.016238 22535416901760 run.py:483] Algo bellman_ford step 5966 current loss 0.465457, current_train_items 190944.
I0302 19:01:13.039877 22535416901760 run.py:483] Algo bellman_ford step 5967 current loss 0.618381, current_train_items 190976.
I0302 19:01:13.070049 22535416901760 run.py:483] Algo bellman_ford step 5968 current loss 0.616702, current_train_items 191008.
I0302 19:01:13.104215 22535416901760 run.py:483] Algo bellman_ford step 5969 current loss 0.721125, current_train_items 191040.
I0302 19:01:13.124109 22535416901760 run.py:483] Algo bellman_ford step 5970 current loss 0.288673, current_train_items 191072.
I0302 19:01:13.140321 22535416901760 run.py:483] Algo bellman_ford step 5971 current loss 0.513705, current_train_items 191104.
I0302 19:01:13.163205 22535416901760 run.py:483] Algo bellman_ford step 5972 current loss 0.538366, current_train_items 191136.
I0302 19:01:13.194204 22535416901760 run.py:483] Algo bellman_ford step 5973 current loss 0.696250, current_train_items 191168.
I0302 19:01:13.227219 22535416901760 run.py:483] Algo bellman_ford step 5974 current loss 0.706072, current_train_items 191200.
I0302 19:01:13.247092 22535416901760 run.py:483] Algo bellman_ford step 5975 current loss 0.352330, current_train_items 191232.
I0302 19:01:13.262897 22535416901760 run.py:483] Algo bellman_ford step 5976 current loss 0.402728, current_train_items 191264.
I0302 19:01:13.285320 22535416901760 run.py:483] Algo bellman_ford step 5977 current loss 0.583030, current_train_items 191296.
I0302 19:01:13.316132 22535416901760 run.py:483] Algo bellman_ford step 5978 current loss 0.676095, current_train_items 191328.
I0302 19:01:13.348424 22535416901760 run.py:483] Algo bellman_ford step 5979 current loss 0.672051, current_train_items 191360.
I0302 19:01:13.367643 22535416901760 run.py:483] Algo bellman_ford step 5980 current loss 0.302668, current_train_items 191392.
I0302 19:01:13.384372 22535416901760 run.py:483] Algo bellman_ford step 5981 current loss 0.535276, current_train_items 191424.
I0302 19:01:13.407238 22535416901760 run.py:483] Algo bellman_ford step 5982 current loss 0.593318, current_train_items 191456.
I0302 19:01:13.437529 22535416901760 run.py:483] Algo bellman_ford step 5983 current loss 0.676530, current_train_items 191488.
I0302 19:01:13.470241 22535416901760 run.py:483] Algo bellman_ford step 5984 current loss 0.955747, current_train_items 191520.
I0302 19:01:13.489919 22535416901760 run.py:483] Algo bellman_ford step 5985 current loss 0.304072, current_train_items 191552.
I0302 19:01:13.506642 22535416901760 run.py:483] Algo bellman_ford step 5986 current loss 0.428782, current_train_items 191584.
I0302 19:01:13.530015 22535416901760 run.py:483] Algo bellman_ford step 5987 current loss 0.566344, current_train_items 191616.
I0302 19:01:13.560073 22535416901760 run.py:483] Algo bellman_ford step 5988 current loss 0.589725, current_train_items 191648.
I0302 19:01:13.593040 22535416901760 run.py:483] Algo bellman_ford step 5989 current loss 0.717603, current_train_items 191680.
I0302 19:01:13.612636 22535416901760 run.py:483] Algo bellman_ford step 5990 current loss 0.279189, current_train_items 191712.
I0302 19:01:13.628188 22535416901760 run.py:483] Algo bellman_ford step 5991 current loss 0.519532, current_train_items 191744.
I0302 19:01:13.651483 22535416901760 run.py:483] Algo bellman_ford step 5992 current loss 0.686001, current_train_items 191776.
I0302 19:01:13.684043 22535416901760 run.py:483] Algo bellman_ford step 5993 current loss 0.775296, current_train_items 191808.
I0302 19:01:13.718024 22535416901760 run.py:483] Algo bellman_ford step 5994 current loss 0.759420, current_train_items 191840.
I0302 19:01:13.737456 22535416901760 run.py:483] Algo bellman_ford step 5995 current loss 0.308126, current_train_items 191872.
I0302 19:01:13.754131 22535416901760 run.py:483] Algo bellman_ford step 5996 current loss 0.414788, current_train_items 191904.
I0302 19:01:13.778110 22535416901760 run.py:483] Algo bellman_ford step 5997 current loss 0.629154, current_train_items 191936.
I0302 19:01:13.808693 22535416901760 run.py:483] Algo bellman_ford step 5998 current loss 0.761814, current_train_items 191968.
I0302 19:01:13.841299 22535416901760 run.py:483] Algo bellman_ford step 5999 current loss 0.748116, current_train_items 192000.
I0302 19:01:13.861768 22535416901760 run.py:483] Algo bellman_ford step 6000 current loss 0.316975, current_train_items 192032.
I0302 19:01:13.869844 22535416901760 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:01:13.869950 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:01:13.886742 22535416901760 run.py:483] Algo bellman_ford step 6001 current loss 0.485578, current_train_items 192064.
I0302 19:01:13.910625 22535416901760 run.py:483] Algo bellman_ford step 6002 current loss 0.686638, current_train_items 192096.
I0302 19:01:13.940163 22535416901760 run.py:483] Algo bellman_ford step 6003 current loss 0.668879, current_train_items 192128.
I0302 19:01:13.973330 22535416901760 run.py:483] Algo bellman_ford step 6004 current loss 0.745542, current_train_items 192160.
I0302 19:01:13.993750 22535416901760 run.py:483] Algo bellman_ford step 6005 current loss 0.254089, current_train_items 192192.
I0302 19:01:14.009599 22535416901760 run.py:483] Algo bellman_ford step 6006 current loss 0.442950, current_train_items 192224.
I0302 19:01:14.034078 22535416901760 run.py:483] Algo bellman_ford step 6007 current loss 0.737578, current_train_items 192256.
I0302 19:01:14.066319 22535416901760 run.py:483] Algo bellman_ford step 6008 current loss 0.909343, current_train_items 192288.
I0302 19:01:14.100815 22535416901760 run.py:483] Algo bellman_ford step 6009 current loss 0.843603, current_train_items 192320.
I0302 19:01:14.120553 22535416901760 run.py:483] Algo bellman_ford step 6010 current loss 0.297331, current_train_items 192352.
I0302 19:01:14.136676 22535416901760 run.py:483] Algo bellman_ford step 6011 current loss 0.502345, current_train_items 192384.
I0302 19:01:14.160209 22535416901760 run.py:483] Algo bellman_ford step 6012 current loss 0.594727, current_train_items 192416.
I0302 19:01:14.190103 22535416901760 run.py:483] Algo bellman_ford step 6013 current loss 0.728117, current_train_items 192448.
I0302 19:01:14.225661 22535416901760 run.py:483] Algo bellman_ford step 6014 current loss 0.862013, current_train_items 192480.
I0302 19:01:14.244998 22535416901760 run.py:483] Algo bellman_ford step 6015 current loss 0.317948, current_train_items 192512.
I0302 19:01:14.261270 22535416901760 run.py:483] Algo bellman_ford step 6016 current loss 0.504039, current_train_items 192544.
I0302 19:01:14.284267 22535416901760 run.py:483] Algo bellman_ford step 6017 current loss 0.586356, current_train_items 192576.
I0302 19:01:14.315100 22535416901760 run.py:483] Algo bellman_ford step 6018 current loss 0.729532, current_train_items 192608.
I0302 19:01:14.348126 22535416901760 run.py:483] Algo bellman_ford step 6019 current loss 0.832859, current_train_items 192640.
I0302 19:01:14.367901 22535416901760 run.py:483] Algo bellman_ford step 6020 current loss 0.315845, current_train_items 192672.
I0302 19:01:14.384302 22535416901760 run.py:483] Algo bellman_ford step 6021 current loss 0.445936, current_train_items 192704.
I0302 19:01:14.408043 22535416901760 run.py:483] Algo bellman_ford step 6022 current loss 0.568623, current_train_items 192736.
I0302 19:01:14.440191 22535416901760 run.py:483] Algo bellman_ford step 6023 current loss 0.802648, current_train_items 192768.
I0302 19:01:14.474101 22535416901760 run.py:483] Algo bellman_ford step 6024 current loss 0.731504, current_train_items 192800.
I0302 19:01:14.493950 22535416901760 run.py:483] Algo bellman_ford step 6025 current loss 0.290626, current_train_items 192832.
I0302 19:01:14.510342 22535416901760 run.py:483] Algo bellman_ford step 6026 current loss 0.641162, current_train_items 192864.
I0302 19:01:14.533578 22535416901760 run.py:483] Algo bellman_ford step 6027 current loss 0.638154, current_train_items 192896.
I0302 19:01:14.564304 22535416901760 run.py:483] Algo bellman_ford step 6028 current loss 0.708644, current_train_items 192928.
I0302 19:01:14.597644 22535416901760 run.py:483] Algo bellman_ford step 6029 current loss 0.740965, current_train_items 192960.
I0302 19:01:14.617204 22535416901760 run.py:483] Algo bellman_ford step 6030 current loss 0.278106, current_train_items 192992.
I0302 19:01:14.633054 22535416901760 run.py:483] Algo bellman_ford step 6031 current loss 0.425780, current_train_items 193024.
I0302 19:01:14.656256 22535416901760 run.py:483] Algo bellman_ford step 6032 current loss 0.538428, current_train_items 193056.
I0302 19:01:14.688675 22535416901760 run.py:483] Algo bellman_ford step 6033 current loss 0.856509, current_train_items 193088.
I0302 19:01:14.721495 22535416901760 run.py:483] Algo bellman_ford step 6034 current loss 0.767992, current_train_items 193120.
I0302 19:01:14.740793 22535416901760 run.py:483] Algo bellman_ford step 6035 current loss 0.270269, current_train_items 193152.
I0302 19:01:14.756784 22535416901760 run.py:483] Algo bellman_ford step 6036 current loss 0.442985, current_train_items 193184.
I0302 19:01:14.780338 22535416901760 run.py:483] Algo bellman_ford step 6037 current loss 0.652354, current_train_items 193216.
I0302 19:01:14.811641 22535416901760 run.py:483] Algo bellman_ford step 6038 current loss 0.850653, current_train_items 193248.
I0302 19:01:14.843811 22535416901760 run.py:483] Algo bellman_ford step 6039 current loss 0.887462, current_train_items 193280.
I0302 19:01:14.862927 22535416901760 run.py:483] Algo bellman_ford step 6040 current loss 0.402486, current_train_items 193312.
I0302 19:01:14.878997 22535416901760 run.py:483] Algo bellman_ford step 6041 current loss 0.492621, current_train_items 193344.
I0302 19:01:14.903209 22535416901760 run.py:483] Algo bellman_ford step 6042 current loss 0.705278, current_train_items 193376.
I0302 19:01:14.933866 22535416901760 run.py:483] Algo bellman_ford step 6043 current loss 0.665860, current_train_items 193408.
I0302 19:01:14.969269 22535416901760 run.py:483] Algo bellman_ford step 6044 current loss 0.811222, current_train_items 193440.
I0302 19:01:14.989100 22535416901760 run.py:483] Algo bellman_ford step 6045 current loss 0.266115, current_train_items 193472.
I0302 19:01:15.005496 22535416901760 run.py:483] Algo bellman_ford step 6046 current loss 0.506473, current_train_items 193504.
I0302 19:01:15.028729 22535416901760 run.py:483] Algo bellman_ford step 6047 current loss 0.680370, current_train_items 193536.
I0302 19:01:15.060258 22535416901760 run.py:483] Algo bellman_ford step 6048 current loss 0.763939, current_train_items 193568.
I0302 19:01:15.094300 22535416901760 run.py:483] Algo bellman_ford step 6049 current loss 0.736367, current_train_items 193600.
I0302 19:01:15.113723 22535416901760 run.py:483] Algo bellman_ford step 6050 current loss 0.275810, current_train_items 193632.
I0302 19:01:15.121920 22535416901760 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:01:15.122050 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:15.138642 22535416901760 run.py:483] Algo bellman_ford step 6051 current loss 0.452834, current_train_items 193664.
I0302 19:01:15.162899 22535416901760 run.py:483] Algo bellman_ford step 6052 current loss 0.631402, current_train_items 193696.
I0302 19:01:15.193577 22535416901760 run.py:483] Algo bellman_ford step 6053 current loss 0.844942, current_train_items 193728.
I0302 19:01:15.227700 22535416901760 run.py:483] Algo bellman_ford step 6054 current loss 1.061345, current_train_items 193760.
I0302 19:01:15.247707 22535416901760 run.py:483] Algo bellman_ford step 6055 current loss 0.438441, current_train_items 193792.
I0302 19:01:15.263423 22535416901760 run.py:483] Algo bellman_ford step 6056 current loss 0.571967, current_train_items 193824.
I0302 19:01:15.285936 22535416901760 run.py:483] Algo bellman_ford step 6057 current loss 0.734911, current_train_items 193856.
I0302 19:01:15.316230 22535416901760 run.py:483] Algo bellman_ford step 6058 current loss 0.668016, current_train_items 193888.
I0302 19:01:15.347186 22535416901760 run.py:483] Algo bellman_ford step 6059 current loss 0.618801, current_train_items 193920.
I0302 19:01:15.367357 22535416901760 run.py:483] Algo bellman_ford step 6060 current loss 0.283023, current_train_items 193952.
I0302 19:01:15.383563 22535416901760 run.py:483] Algo bellman_ford step 6061 current loss 0.495236, current_train_items 193984.
I0302 19:01:15.405204 22535416901760 run.py:483] Algo bellman_ford step 6062 current loss 0.567636, current_train_items 194016.
I0302 19:01:15.434501 22535416901760 run.py:483] Algo bellman_ford step 6063 current loss 0.694964, current_train_items 194048.
I0302 19:01:15.468062 22535416901760 run.py:483] Algo bellman_ford step 6064 current loss 0.852725, current_train_items 194080.
I0302 19:01:15.487429 22535416901760 run.py:483] Algo bellman_ford step 6065 current loss 0.313605, current_train_items 194112.
I0302 19:01:15.503516 22535416901760 run.py:483] Algo bellman_ford step 6066 current loss 0.471464, current_train_items 194144.
I0302 19:01:15.527318 22535416901760 run.py:483] Algo bellman_ford step 6067 current loss 0.644814, current_train_items 194176.
I0302 19:01:15.558547 22535416901760 run.py:483] Algo bellman_ford step 6068 current loss 0.769508, current_train_items 194208.
I0302 19:01:15.590893 22535416901760 run.py:483] Algo bellman_ford step 6069 current loss 0.824247, current_train_items 194240.
I0302 19:01:15.610844 22535416901760 run.py:483] Algo bellman_ford step 6070 current loss 0.334208, current_train_items 194272.
I0302 19:01:15.626823 22535416901760 run.py:483] Algo bellman_ford step 6071 current loss 0.522316, current_train_items 194304.
I0302 19:01:15.650026 22535416901760 run.py:483] Algo bellman_ford step 6072 current loss 0.589961, current_train_items 194336.
I0302 19:01:15.679764 22535416901760 run.py:483] Algo bellman_ford step 6073 current loss 0.607573, current_train_items 194368.
I0302 19:01:15.713745 22535416901760 run.py:483] Algo bellman_ford step 6074 current loss 0.797417, current_train_items 194400.
I0302 19:01:15.733468 22535416901760 run.py:483] Algo bellman_ford step 6075 current loss 0.306131, current_train_items 194432.
I0302 19:01:15.749653 22535416901760 run.py:483] Algo bellman_ford step 6076 current loss 0.436429, current_train_items 194464.
I0302 19:01:15.771743 22535416901760 run.py:483] Algo bellman_ford step 6077 current loss 0.530172, current_train_items 194496.
I0302 19:01:15.801868 22535416901760 run.py:483] Algo bellman_ford step 6078 current loss 0.655022, current_train_items 194528.
I0302 19:01:15.836000 22535416901760 run.py:483] Algo bellman_ford step 6079 current loss 0.910724, current_train_items 194560.
I0302 19:01:15.855315 22535416901760 run.py:483] Algo bellman_ford step 6080 current loss 0.276515, current_train_items 194592.
I0302 19:01:15.871528 22535416901760 run.py:483] Algo bellman_ford step 6081 current loss 0.411897, current_train_items 194624.
I0302 19:01:15.894330 22535416901760 run.py:483] Algo bellman_ford step 6082 current loss 0.527806, current_train_items 194656.
I0302 19:01:15.926545 22535416901760 run.py:483] Algo bellman_ford step 6083 current loss 0.660358, current_train_items 194688.
I0302 19:01:15.958605 22535416901760 run.py:483] Algo bellman_ford step 6084 current loss 0.664278, current_train_items 194720.
I0302 19:01:15.978222 22535416901760 run.py:483] Algo bellman_ford step 6085 current loss 0.290538, current_train_items 194752.
I0302 19:01:15.994524 22535416901760 run.py:483] Algo bellman_ford step 6086 current loss 0.527494, current_train_items 194784.
I0302 19:01:16.017248 22535416901760 run.py:483] Algo bellman_ford step 6087 current loss 0.679461, current_train_items 194816.
I0302 19:01:16.047416 22535416901760 run.py:483] Algo bellman_ford step 6088 current loss 0.862187, current_train_items 194848.
I0302 19:01:16.081266 22535416901760 run.py:483] Algo bellman_ford step 6089 current loss 0.916825, current_train_items 194880.
I0302 19:01:16.101214 22535416901760 run.py:483] Algo bellman_ford step 6090 current loss 0.379855, current_train_items 194912.
I0302 19:01:16.117450 22535416901760 run.py:483] Algo bellman_ford step 6091 current loss 0.579078, current_train_items 194944.
I0302 19:01:16.140055 22535416901760 run.py:483] Algo bellman_ford step 6092 current loss 0.569794, current_train_items 194976.
I0302 19:01:16.170221 22535416901760 run.py:483] Algo bellman_ford step 6093 current loss 0.690391, current_train_items 195008.
I0302 19:01:16.202401 22535416901760 run.py:483] Algo bellman_ford step 6094 current loss 0.881016, current_train_items 195040.
I0302 19:01:16.221830 22535416901760 run.py:483] Algo bellman_ford step 6095 current loss 0.293700, current_train_items 195072.
I0302 19:01:16.237985 22535416901760 run.py:483] Algo bellman_ford step 6096 current loss 0.396758, current_train_items 195104.
I0302 19:01:16.261368 22535416901760 run.py:483] Algo bellman_ford step 6097 current loss 0.663538, current_train_items 195136.
I0302 19:01:16.291004 22535416901760 run.py:483] Algo bellman_ford step 6098 current loss 0.619219, current_train_items 195168.
I0302 19:01:16.324230 22535416901760 run.py:483] Algo bellman_ford step 6099 current loss 0.802369, current_train_items 195200.
I0302 19:01:16.343954 22535416901760 run.py:483] Algo bellman_ford step 6100 current loss 0.213174, current_train_items 195232.
I0302 19:01:16.351773 22535416901760 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:01:16.351878 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:01:16.367993 22535416901760 run.py:483] Algo bellman_ford step 6101 current loss 0.439779, current_train_items 195264.
I0302 19:01:16.392682 22535416901760 run.py:483] Algo bellman_ford step 6102 current loss 0.668811, current_train_items 195296.
I0302 19:01:16.425253 22535416901760 run.py:483] Algo bellman_ford step 6103 current loss 0.838679, current_train_items 195328.
I0302 19:01:16.457972 22535416901760 run.py:483] Algo bellman_ford step 6104 current loss 0.707251, current_train_items 195360.
I0302 19:01:16.478115 22535416901760 run.py:483] Algo bellman_ford step 6105 current loss 0.307189, current_train_items 195392.
I0302 19:01:16.493460 22535416901760 run.py:483] Algo bellman_ford step 6106 current loss 0.498633, current_train_items 195424.
I0302 19:01:16.517040 22535416901760 run.py:483] Algo bellman_ford step 6107 current loss 0.679337, current_train_items 195456.
I0302 19:01:16.547520 22535416901760 run.py:483] Algo bellman_ford step 6108 current loss 0.631237, current_train_items 195488.
I0302 19:01:16.580398 22535416901760 run.py:483] Algo bellman_ford step 6109 current loss 0.772762, current_train_items 195520.
I0302 19:01:16.600213 22535416901760 run.py:483] Algo bellman_ford step 6110 current loss 0.280984, current_train_items 195552.
I0302 19:01:16.616130 22535416901760 run.py:483] Algo bellman_ford step 6111 current loss 0.428157, current_train_items 195584.
I0302 19:01:16.639115 22535416901760 run.py:483] Algo bellman_ford step 6112 current loss 0.671315, current_train_items 195616.
I0302 19:01:16.669238 22535416901760 run.py:483] Algo bellman_ford step 6113 current loss 0.705432, current_train_items 195648.
I0302 19:01:16.702378 22535416901760 run.py:483] Algo bellman_ford step 6114 current loss 0.698235, current_train_items 195680.
I0302 19:01:16.722055 22535416901760 run.py:483] Algo bellman_ford step 6115 current loss 0.300642, current_train_items 195712.
I0302 19:01:16.738185 22535416901760 run.py:483] Algo bellman_ford step 6116 current loss 0.422786, current_train_items 195744.
I0302 19:01:16.761416 22535416901760 run.py:483] Algo bellman_ford step 6117 current loss 0.546124, current_train_items 195776.
I0302 19:01:16.793018 22535416901760 run.py:483] Algo bellman_ford step 6118 current loss 0.715348, current_train_items 195808.
I0302 19:01:16.825663 22535416901760 run.py:483] Algo bellman_ford step 6119 current loss 0.743240, current_train_items 195840.
I0302 19:01:16.845497 22535416901760 run.py:483] Algo bellman_ford step 6120 current loss 0.356830, current_train_items 195872.
I0302 19:01:16.861769 22535416901760 run.py:483] Algo bellman_ford step 6121 current loss 0.452863, current_train_items 195904.
I0302 19:01:16.885076 22535416901760 run.py:483] Algo bellman_ford step 6122 current loss 0.557356, current_train_items 195936.
I0302 19:01:16.916706 22535416901760 run.py:483] Algo bellman_ford step 6123 current loss 0.690136, current_train_items 195968.
I0302 19:01:16.949544 22535416901760 run.py:483] Algo bellman_ford step 6124 current loss 0.832320, current_train_items 196000.
I0302 19:01:16.969038 22535416901760 run.py:483] Algo bellman_ford step 6125 current loss 0.267356, current_train_items 196032.
I0302 19:01:16.984646 22535416901760 run.py:483] Algo bellman_ford step 6126 current loss 0.446239, current_train_items 196064.
I0302 19:01:17.009664 22535416901760 run.py:483] Algo bellman_ford step 6127 current loss 0.606739, current_train_items 196096.
I0302 19:01:17.040387 22535416901760 run.py:483] Algo bellman_ford step 6128 current loss 0.563488, current_train_items 196128.
I0302 19:01:17.073369 22535416901760 run.py:483] Algo bellman_ford step 6129 current loss 0.663326, current_train_items 196160.
I0302 19:01:17.093164 22535416901760 run.py:483] Algo bellman_ford step 6130 current loss 0.329059, current_train_items 196192.
I0302 19:01:17.109142 22535416901760 run.py:483] Algo bellman_ford step 6131 current loss 0.396369, current_train_items 196224.
I0302 19:01:17.133516 22535416901760 run.py:483] Algo bellman_ford step 6132 current loss 0.580190, current_train_items 196256.
I0302 19:01:17.163536 22535416901760 run.py:483] Algo bellman_ford step 6133 current loss 0.583362, current_train_items 196288.
I0302 19:01:17.196088 22535416901760 run.py:483] Algo bellman_ford step 6134 current loss 0.630625, current_train_items 196320.
I0302 19:01:17.215428 22535416901760 run.py:483] Algo bellman_ford step 6135 current loss 0.282216, current_train_items 196352.
I0302 19:01:17.231029 22535416901760 run.py:483] Algo bellman_ford step 6136 current loss 0.468788, current_train_items 196384.
I0302 19:01:17.254752 22535416901760 run.py:483] Algo bellman_ford step 6137 current loss 0.639948, current_train_items 196416.
I0302 19:01:17.286333 22535416901760 run.py:483] Algo bellman_ford step 6138 current loss 0.621296, current_train_items 196448.
I0302 19:01:17.319350 22535416901760 run.py:483] Algo bellman_ford step 6139 current loss 0.721943, current_train_items 196480.
I0302 19:01:17.339091 22535416901760 run.py:483] Algo bellman_ford step 6140 current loss 0.341425, current_train_items 196512.
I0302 19:01:17.355244 22535416901760 run.py:483] Algo bellman_ford step 6141 current loss 0.362108, current_train_items 196544.
I0302 19:01:17.378998 22535416901760 run.py:483] Algo bellman_ford step 6142 current loss 0.520635, current_train_items 196576.
I0302 19:01:17.410060 22535416901760 run.py:483] Algo bellman_ford step 6143 current loss 0.625368, current_train_items 196608.
I0302 19:01:17.442850 22535416901760 run.py:483] Algo bellman_ford step 6144 current loss 0.794072, current_train_items 196640.
I0302 19:01:17.462463 22535416901760 run.py:483] Algo bellman_ford step 6145 current loss 0.246088, current_train_items 196672.
I0302 19:01:17.478677 22535416901760 run.py:483] Algo bellman_ford step 6146 current loss 0.581509, current_train_items 196704.
I0302 19:01:17.501434 22535416901760 run.py:483] Algo bellman_ford step 6147 current loss 0.567847, current_train_items 196736.
I0302 19:01:17.531861 22535416901760 run.py:483] Algo bellman_ford step 6148 current loss 0.695956, current_train_items 196768.
I0302 19:01:17.566280 22535416901760 run.py:483] Algo bellman_ford step 6149 current loss 0.841031, current_train_items 196800.
I0302 19:01:17.585942 22535416901760 run.py:483] Algo bellman_ford step 6150 current loss 0.310896, current_train_items 196832.
I0302 19:01:17.593962 22535416901760 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:01:17.594075 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:17.610808 22535416901760 run.py:483] Algo bellman_ford step 6151 current loss 0.468862, current_train_items 196864.
I0302 19:01:17.633626 22535416901760 run.py:483] Algo bellman_ford step 6152 current loss 0.571185, current_train_items 196896.
I0302 19:01:17.663584 22535416901760 run.py:483] Algo bellman_ford step 6153 current loss 0.661196, current_train_items 196928.
I0302 19:01:17.697067 22535416901760 run.py:483] Algo bellman_ford step 6154 current loss 0.670980, current_train_items 196960.
I0302 19:01:17.716976 22535416901760 run.py:483] Algo bellman_ford step 6155 current loss 0.258226, current_train_items 196992.
I0302 19:01:17.733107 22535416901760 run.py:483] Algo bellman_ford step 6156 current loss 0.452586, current_train_items 197024.
I0302 19:01:17.756170 22535416901760 run.py:483] Algo bellman_ford step 6157 current loss 0.476056, current_train_items 197056.
I0302 19:01:17.785957 22535416901760 run.py:483] Algo bellman_ford step 6158 current loss 0.599912, current_train_items 197088.
I0302 19:01:17.818896 22535416901760 run.py:483] Algo bellman_ford step 6159 current loss 0.659321, current_train_items 197120.
I0302 19:01:17.838822 22535416901760 run.py:483] Algo bellman_ford step 6160 current loss 0.342585, current_train_items 197152.
I0302 19:01:17.855419 22535416901760 run.py:483] Algo bellman_ford step 6161 current loss 0.473026, current_train_items 197184.
I0302 19:01:17.878169 22535416901760 run.py:483] Algo bellman_ford step 6162 current loss 0.609639, current_train_items 197216.
I0302 19:01:17.910268 22535416901760 run.py:483] Algo bellman_ford step 6163 current loss 0.719976, current_train_items 197248.
I0302 19:01:17.944081 22535416901760 run.py:483] Algo bellman_ford step 6164 current loss 0.724899, current_train_items 197280.
I0302 19:01:17.963790 22535416901760 run.py:483] Algo bellman_ford step 6165 current loss 0.264874, current_train_items 197312.
I0302 19:01:17.980010 22535416901760 run.py:483] Algo bellman_ford step 6166 current loss 0.465673, current_train_items 197344.
I0302 19:01:18.003818 22535416901760 run.py:483] Algo bellman_ford step 6167 current loss 0.630531, current_train_items 197376.
I0302 19:01:18.034842 22535416901760 run.py:483] Algo bellman_ford step 6168 current loss 0.754972, current_train_items 197408.
I0302 19:01:18.068052 22535416901760 run.py:483] Algo bellman_ford step 6169 current loss 0.775743, current_train_items 197440.
I0302 19:01:18.088470 22535416901760 run.py:483] Algo bellman_ford step 6170 current loss 0.307516, current_train_items 197472.
I0302 19:01:18.104440 22535416901760 run.py:483] Algo bellman_ford step 6171 current loss 0.418632, current_train_items 197504.
I0302 19:01:18.128135 22535416901760 run.py:483] Algo bellman_ford step 6172 current loss 0.567386, current_train_items 197536.
I0302 19:01:18.159862 22535416901760 run.py:483] Algo bellman_ford step 6173 current loss 0.689618, current_train_items 197568.
I0302 19:01:18.193126 22535416901760 run.py:483] Algo bellman_ford step 6174 current loss 0.777734, current_train_items 197600.
I0302 19:01:18.212906 22535416901760 run.py:483] Algo bellman_ford step 6175 current loss 0.271488, current_train_items 197632.
I0302 19:01:18.229413 22535416901760 run.py:483] Algo bellman_ford step 6176 current loss 0.661121, current_train_items 197664.
I0302 19:01:18.251926 22535416901760 run.py:483] Algo bellman_ford step 6177 current loss 0.595592, current_train_items 197696.
I0302 19:01:18.282870 22535416901760 run.py:483] Algo bellman_ford step 6178 current loss 0.695044, current_train_items 197728.
I0302 19:01:18.316634 22535416901760 run.py:483] Algo bellman_ford step 6179 current loss 0.894565, current_train_items 197760.
I0302 19:01:18.336420 22535416901760 run.py:483] Algo bellman_ford step 6180 current loss 0.290001, current_train_items 197792.
I0302 19:01:18.352575 22535416901760 run.py:483] Algo bellman_ford step 6181 current loss 0.454850, current_train_items 197824.
I0302 19:01:18.375991 22535416901760 run.py:483] Algo bellman_ford step 6182 current loss 0.640487, current_train_items 197856.
I0302 19:01:18.406932 22535416901760 run.py:483] Algo bellman_ford step 6183 current loss 0.676159, current_train_items 197888.
I0302 19:01:18.442487 22535416901760 run.py:483] Algo bellman_ford step 6184 current loss 0.822777, current_train_items 197920.
I0302 19:01:18.462141 22535416901760 run.py:483] Algo bellman_ford step 6185 current loss 0.344382, current_train_items 197952.
I0302 19:01:18.478398 22535416901760 run.py:483] Algo bellman_ford step 6186 current loss 0.479667, current_train_items 197984.
I0302 19:01:18.500759 22535416901760 run.py:483] Algo bellman_ford step 6187 current loss 0.632850, current_train_items 198016.
I0302 19:01:18.530800 22535416901760 run.py:483] Algo bellman_ford step 6188 current loss 0.554642, current_train_items 198048.
I0302 19:01:18.564178 22535416901760 run.py:483] Algo bellman_ford step 6189 current loss 0.788656, current_train_items 198080.
I0302 19:01:18.584474 22535416901760 run.py:483] Algo bellman_ford step 6190 current loss 0.323688, current_train_items 198112.
I0302 19:01:18.600945 22535416901760 run.py:483] Algo bellman_ford step 6191 current loss 0.445362, current_train_items 198144.
I0302 19:01:18.624303 22535416901760 run.py:483] Algo bellman_ford step 6192 current loss 0.589831, current_train_items 198176.
I0302 19:01:18.656239 22535416901760 run.py:483] Algo bellman_ford step 6193 current loss 0.646691, current_train_items 198208.
I0302 19:01:18.688548 22535416901760 run.py:483] Algo bellman_ford step 6194 current loss 0.685354, current_train_items 198240.
I0302 19:01:18.708250 22535416901760 run.py:483] Algo bellman_ford step 6195 current loss 0.318765, current_train_items 198272.
I0302 19:01:18.723809 22535416901760 run.py:483] Algo bellman_ford step 6196 current loss 0.402240, current_train_items 198304.
I0302 19:01:18.747838 22535416901760 run.py:483] Algo bellman_ford step 6197 current loss 0.642126, current_train_items 198336.
I0302 19:01:18.777542 22535416901760 run.py:483] Algo bellman_ford step 6198 current loss 0.658218, current_train_items 198368.
I0302 19:01:18.811654 22535416901760 run.py:483] Algo bellman_ford step 6199 current loss 0.778086, current_train_items 198400.
I0302 19:01:18.831624 22535416901760 run.py:483] Algo bellman_ford step 6200 current loss 0.305375, current_train_items 198432.
I0302 19:01:18.839627 22535416901760 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:01:18.839733 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:01:18.856682 22535416901760 run.py:483] Algo bellman_ford step 6201 current loss 0.417292, current_train_items 198464.
I0302 19:01:18.881047 22535416901760 run.py:483] Algo bellman_ford step 6202 current loss 0.562425, current_train_items 198496.
I0302 19:01:18.911289 22535416901760 run.py:483] Algo bellman_ford step 6203 current loss 0.600674, current_train_items 198528.
I0302 19:01:18.943916 22535416901760 run.py:483] Algo bellman_ford step 6204 current loss 0.664192, current_train_items 198560.
I0302 19:01:18.964131 22535416901760 run.py:483] Algo bellman_ford step 6205 current loss 0.337764, current_train_items 198592.
I0302 19:01:18.979452 22535416901760 run.py:483] Algo bellman_ford step 6206 current loss 0.429784, current_train_items 198624.
I0302 19:01:19.002815 22535416901760 run.py:483] Algo bellman_ford step 6207 current loss 0.590405, current_train_items 198656.
I0302 19:01:19.033722 22535416901760 run.py:483] Algo bellman_ford step 6208 current loss 0.701366, current_train_items 198688.
I0302 19:01:19.067801 22535416901760 run.py:483] Algo bellman_ford step 6209 current loss 0.738469, current_train_items 198720.
I0302 19:01:19.087368 22535416901760 run.py:483] Algo bellman_ford step 6210 current loss 0.284970, current_train_items 198752.
I0302 19:01:19.103522 22535416901760 run.py:483] Algo bellman_ford step 6211 current loss 0.525681, current_train_items 198784.
I0302 19:01:19.126228 22535416901760 run.py:483] Algo bellman_ford step 6212 current loss 0.620236, current_train_items 198816.
I0302 19:01:19.155726 22535416901760 run.py:483] Algo bellman_ford step 6213 current loss 0.694344, current_train_items 198848.
I0302 19:01:19.190530 22535416901760 run.py:483] Algo bellman_ford step 6214 current loss 0.753935, current_train_items 198880.
I0302 19:01:19.210083 22535416901760 run.py:483] Algo bellman_ford step 6215 current loss 0.283533, current_train_items 198912.
I0302 19:01:19.226128 22535416901760 run.py:483] Algo bellman_ford step 6216 current loss 0.441109, current_train_items 198944.
I0302 19:01:19.249178 22535416901760 run.py:483] Algo bellman_ford step 6217 current loss 0.567547, current_train_items 198976.
I0302 19:01:19.279445 22535416901760 run.py:483] Algo bellman_ford step 6218 current loss 0.621460, current_train_items 199008.
I0302 19:01:19.311889 22535416901760 run.py:483] Algo bellman_ford step 6219 current loss 0.814226, current_train_items 199040.
I0302 19:01:19.331459 22535416901760 run.py:483] Algo bellman_ford step 6220 current loss 0.283654, current_train_items 199072.
I0302 19:01:19.346960 22535416901760 run.py:483] Algo bellman_ford step 6221 current loss 0.427988, current_train_items 199104.
I0302 19:01:19.370836 22535416901760 run.py:483] Algo bellman_ford step 6222 current loss 0.622749, current_train_items 199136.
I0302 19:01:19.400602 22535416901760 run.py:483] Algo bellman_ford step 6223 current loss 0.670940, current_train_items 199168.
I0302 19:01:19.434435 22535416901760 run.py:483] Algo bellman_ford step 6224 current loss 0.864635, current_train_items 199200.
I0302 19:01:19.453979 22535416901760 run.py:483] Algo bellman_ford step 6225 current loss 0.328090, current_train_items 199232.
I0302 19:01:19.469990 22535416901760 run.py:483] Algo bellman_ford step 6226 current loss 0.444390, current_train_items 199264.
I0302 19:01:19.492310 22535416901760 run.py:483] Algo bellman_ford step 6227 current loss 0.549589, current_train_items 199296.
I0302 19:01:19.521835 22535416901760 run.py:483] Algo bellman_ford step 6228 current loss 0.569786, current_train_items 199328.
I0302 19:01:19.552924 22535416901760 run.py:483] Algo bellman_ford step 6229 current loss 0.681171, current_train_items 199360.
I0302 19:01:19.572531 22535416901760 run.py:483] Algo bellman_ford step 6230 current loss 0.291581, current_train_items 199392.
I0302 19:01:19.588720 22535416901760 run.py:483] Algo bellman_ford step 6231 current loss 0.514439, current_train_items 199424.
I0302 19:01:19.612243 22535416901760 run.py:483] Algo bellman_ford step 6232 current loss 0.568844, current_train_items 199456.
I0302 19:01:19.642128 22535416901760 run.py:483] Algo bellman_ford step 6233 current loss 0.644394, current_train_items 199488.
I0302 19:01:19.675337 22535416901760 run.py:483] Algo bellman_ford step 6234 current loss 0.631303, current_train_items 199520.
I0302 19:01:19.694906 22535416901760 run.py:483] Algo bellman_ford step 6235 current loss 0.216576, current_train_items 199552.
I0302 19:01:19.711075 22535416901760 run.py:483] Algo bellman_ford step 6236 current loss 0.434607, current_train_items 199584.
I0302 19:01:19.733997 22535416901760 run.py:483] Algo bellman_ford step 6237 current loss 0.522910, current_train_items 199616.
I0302 19:01:19.765367 22535416901760 run.py:483] Algo bellman_ford step 6238 current loss 0.559627, current_train_items 199648.
I0302 19:01:19.796488 22535416901760 run.py:483] Algo bellman_ford step 6239 current loss 0.672710, current_train_items 199680.
I0302 19:01:19.816145 22535416901760 run.py:483] Algo bellman_ford step 6240 current loss 0.278174, current_train_items 199712.
I0302 19:01:19.832337 22535416901760 run.py:483] Algo bellman_ford step 6241 current loss 0.472469, current_train_items 199744.
I0302 19:01:19.856269 22535416901760 run.py:483] Algo bellman_ford step 6242 current loss 0.597611, current_train_items 199776.
I0302 19:01:19.887346 22535416901760 run.py:483] Algo bellman_ford step 6243 current loss 0.680572, current_train_items 199808.
I0302 19:01:19.919178 22535416901760 run.py:483] Algo bellman_ford step 6244 current loss 0.698986, current_train_items 199840.
I0302 19:01:19.938878 22535416901760 run.py:483] Algo bellman_ford step 6245 current loss 0.338570, current_train_items 199872.
I0302 19:01:19.955029 22535416901760 run.py:483] Algo bellman_ford step 6246 current loss 0.440138, current_train_items 199904.
I0302 19:01:19.977721 22535416901760 run.py:483] Algo bellman_ford step 6247 current loss 0.537295, current_train_items 199936.
I0302 19:01:20.009440 22535416901760 run.py:483] Algo bellman_ford step 6248 current loss 0.659127, current_train_items 199968.
I0302 19:01:20.041037 22535416901760 run.py:483] Algo bellman_ford step 6249 current loss 0.636853, current_train_items 200000.
I0302 19:01:20.060682 22535416901760 run.py:483] Algo bellman_ford step 6250 current loss 0.278638, current_train_items 200032.
I0302 19:01:20.068875 22535416901760 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:01:20.068982 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:01:20.085964 22535416901760 run.py:483] Algo bellman_ford step 6251 current loss 0.480242, current_train_items 200064.
I0302 19:01:20.109343 22535416901760 run.py:483] Algo bellman_ford step 6252 current loss 0.575731, current_train_items 200096.
I0302 19:01:20.141131 22535416901760 run.py:483] Algo bellman_ford step 6253 current loss 0.771482, current_train_items 200128.
I0302 19:01:20.173562 22535416901760 run.py:483] Algo bellman_ford step 6254 current loss 0.848164, current_train_items 200160.
I0302 19:01:20.193330 22535416901760 run.py:483] Algo bellman_ford step 6255 current loss 0.303227, current_train_items 200192.
I0302 19:01:20.209137 22535416901760 run.py:483] Algo bellman_ford step 6256 current loss 0.393747, current_train_items 200224.
I0302 19:01:20.232937 22535416901760 run.py:483] Algo bellman_ford step 6257 current loss 0.639966, current_train_items 200256.
I0302 19:01:20.262937 22535416901760 run.py:483] Algo bellman_ford step 6258 current loss 0.683955, current_train_items 200288.
I0302 19:01:20.295133 22535416901760 run.py:483] Algo bellman_ford step 6259 current loss 0.864904, current_train_items 200320.
I0302 19:01:20.314919 22535416901760 run.py:483] Algo bellman_ford step 6260 current loss 0.387965, current_train_items 200352.
I0302 19:01:20.331444 22535416901760 run.py:483] Algo bellman_ford step 6261 current loss 0.455560, current_train_items 200384.
I0302 19:01:20.354244 22535416901760 run.py:483] Algo bellman_ford step 6262 current loss 0.663701, current_train_items 200416.
I0302 19:01:20.384646 22535416901760 run.py:483] Algo bellman_ford step 6263 current loss 0.630142, current_train_items 200448.
I0302 19:01:20.418169 22535416901760 run.py:483] Algo bellman_ford step 6264 current loss 0.790958, current_train_items 200480.
I0302 19:01:20.437726 22535416901760 run.py:483] Algo bellman_ford step 6265 current loss 0.271332, current_train_items 200512.
I0302 19:01:20.454622 22535416901760 run.py:483] Algo bellman_ford step 6266 current loss 0.627488, current_train_items 200544.
I0302 19:01:20.478927 22535416901760 run.py:483] Algo bellman_ford step 6267 current loss 0.646660, current_train_items 200576.
I0302 19:01:20.509222 22535416901760 run.py:483] Algo bellman_ford step 6268 current loss 0.575208, current_train_items 200608.
I0302 19:01:20.539498 22535416901760 run.py:483] Algo bellman_ford step 6269 current loss 0.638006, current_train_items 200640.
I0302 19:01:20.558970 22535416901760 run.py:483] Algo bellman_ford step 6270 current loss 0.269256, current_train_items 200672.
I0302 19:01:20.575381 22535416901760 run.py:483] Algo bellman_ford step 6271 current loss 0.431561, current_train_items 200704.
I0302 19:01:20.598264 22535416901760 run.py:483] Algo bellman_ford step 6272 current loss 0.540411, current_train_items 200736.
I0302 19:01:20.627753 22535416901760 run.py:483] Algo bellman_ford step 6273 current loss 0.639314, current_train_items 200768.
I0302 19:01:20.661979 22535416901760 run.py:483] Algo bellman_ford step 6274 current loss 0.754616, current_train_items 200800.
I0302 19:01:20.681915 22535416901760 run.py:483] Algo bellman_ford step 6275 current loss 0.330103, current_train_items 200832.
I0302 19:01:20.698482 22535416901760 run.py:483] Algo bellman_ford step 6276 current loss 0.477816, current_train_items 200864.
I0302 19:01:20.721530 22535416901760 run.py:483] Algo bellman_ford step 6277 current loss 0.585431, current_train_items 200896.
I0302 19:01:20.751383 22535416901760 run.py:483] Algo bellman_ford step 6278 current loss 0.651430, current_train_items 200928.
I0302 19:01:20.785579 22535416901760 run.py:483] Algo bellman_ford step 6279 current loss 0.766212, current_train_items 200960.
I0302 19:01:20.805304 22535416901760 run.py:483] Algo bellman_ford step 6280 current loss 0.285500, current_train_items 200992.
I0302 19:01:20.821346 22535416901760 run.py:483] Algo bellman_ford step 6281 current loss 0.420694, current_train_items 201024.
I0302 19:01:20.844449 22535416901760 run.py:483] Algo bellman_ford step 6282 current loss 0.646949, current_train_items 201056.
I0302 19:01:20.875319 22535416901760 run.py:483] Algo bellman_ford step 6283 current loss 0.635342, current_train_items 201088.
I0302 19:01:20.909151 22535416901760 run.py:483] Algo bellman_ford step 6284 current loss 0.712074, current_train_items 201120.
I0302 19:01:20.928673 22535416901760 run.py:483] Algo bellman_ford step 6285 current loss 0.300824, current_train_items 201152.
I0302 19:01:20.945027 22535416901760 run.py:483] Algo bellman_ford step 6286 current loss 0.455052, current_train_items 201184.
I0302 19:01:20.968122 22535416901760 run.py:483] Algo bellman_ford step 6287 current loss 0.561222, current_train_items 201216.
I0302 19:01:20.999003 22535416901760 run.py:483] Algo bellman_ford step 6288 current loss 0.615107, current_train_items 201248.
I0302 19:01:21.032371 22535416901760 run.py:483] Algo bellman_ford step 6289 current loss 0.795353, current_train_items 201280.
I0302 19:01:21.052020 22535416901760 run.py:483] Algo bellman_ford step 6290 current loss 0.300451, current_train_items 201312.
I0302 19:01:21.068539 22535416901760 run.py:483] Algo bellman_ford step 6291 current loss 0.425407, current_train_items 201344.
I0302 19:01:21.091406 22535416901760 run.py:483] Algo bellman_ford step 6292 current loss 0.572871, current_train_items 201376.
I0302 19:01:21.122730 22535416901760 run.py:483] Algo bellman_ford step 6293 current loss 0.709032, current_train_items 201408.
I0302 19:01:21.157227 22535416901760 run.py:483] Algo bellman_ford step 6294 current loss 0.774420, current_train_items 201440.
I0302 19:01:21.176657 22535416901760 run.py:483] Algo bellman_ford step 6295 current loss 0.287634, current_train_items 201472.
I0302 19:01:21.193393 22535416901760 run.py:483] Algo bellman_ford step 6296 current loss 0.493923, current_train_items 201504.
I0302 19:01:21.216028 22535416901760 run.py:483] Algo bellman_ford step 6297 current loss 0.620935, current_train_items 201536.
I0302 19:01:21.246636 22535416901760 run.py:483] Algo bellman_ford step 6298 current loss 0.710246, current_train_items 201568.
I0302 19:01:21.278636 22535416901760 run.py:483] Algo bellman_ford step 6299 current loss 0.674380, current_train_items 201600.
I0302 19:01:21.298089 22535416901760 run.py:483] Algo bellman_ford step 6300 current loss 0.291427, current_train_items 201632.
I0302 19:01:21.305771 22535416901760 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.9111328125, 'score': 0.9111328125, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:01:21.305878 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.911, val scores are: bellman_ford: 0.911
I0302 19:01:21.322741 22535416901760 run.py:483] Algo bellman_ford step 6301 current loss 0.447689, current_train_items 201664.
I0302 19:01:21.346134 22535416901760 run.py:483] Algo bellman_ford step 6302 current loss 0.595103, current_train_items 201696.
I0302 19:01:21.376625 22535416901760 run.py:483] Algo bellman_ford step 6303 current loss 0.667113, current_train_items 201728.
I0302 19:01:21.407872 22535416901760 run.py:483] Algo bellman_ford step 6304 current loss 0.582552, current_train_items 201760.
I0302 19:01:21.427797 22535416901760 run.py:483] Algo bellman_ford step 6305 current loss 0.312916, current_train_items 201792.
I0302 19:01:21.443391 22535416901760 run.py:483] Algo bellman_ford step 6306 current loss 0.442609, current_train_items 201824.
I0302 19:01:21.467112 22535416901760 run.py:483] Algo bellman_ford step 6307 current loss 0.565099, current_train_items 201856.
I0302 19:01:21.498628 22535416901760 run.py:483] Algo bellman_ford step 6308 current loss 0.668693, current_train_items 201888.
I0302 19:01:21.531356 22535416901760 run.py:483] Algo bellman_ford step 6309 current loss 0.769095, current_train_items 201920.
I0302 19:01:21.550959 22535416901760 run.py:483] Algo bellman_ford step 6310 current loss 0.327559, current_train_items 201952.
I0302 19:01:21.566923 22535416901760 run.py:483] Algo bellman_ford step 6311 current loss 0.443484, current_train_items 201984.
I0302 19:01:21.589228 22535416901760 run.py:483] Algo bellman_ford step 6312 current loss 0.553589, current_train_items 202016.
I0302 19:01:21.620609 22535416901760 run.py:483] Algo bellman_ford step 6313 current loss 0.687860, current_train_items 202048.
I0302 19:01:21.654542 22535416901760 run.py:483] Algo bellman_ford step 6314 current loss 0.878156, current_train_items 202080.
I0302 19:01:21.674169 22535416901760 run.py:483] Algo bellman_ford step 6315 current loss 0.281365, current_train_items 202112.
I0302 19:01:21.689997 22535416901760 run.py:483] Algo bellman_ford step 6316 current loss 0.469277, current_train_items 202144.
I0302 19:01:21.713284 22535416901760 run.py:483] Algo bellman_ford step 6317 current loss 0.571526, current_train_items 202176.
I0302 19:01:21.743302 22535416901760 run.py:483] Algo bellman_ford step 6318 current loss 0.553121, current_train_items 202208.
I0302 19:01:21.775700 22535416901760 run.py:483] Algo bellman_ford step 6319 current loss 0.645524, current_train_items 202240.
I0302 19:01:21.795247 22535416901760 run.py:483] Algo bellman_ford step 6320 current loss 0.324380, current_train_items 202272.
I0302 19:01:21.811545 22535416901760 run.py:483] Algo bellman_ford step 6321 current loss 0.453459, current_train_items 202304.
I0302 19:01:21.835866 22535416901760 run.py:483] Algo bellman_ford step 6322 current loss 0.577655, current_train_items 202336.
I0302 19:01:21.867411 22535416901760 run.py:483] Algo bellman_ford step 6323 current loss 0.682504, current_train_items 202368.
I0302 19:01:21.901104 22535416901760 run.py:483] Algo bellman_ford step 6324 current loss 0.753417, current_train_items 202400.
I0302 19:01:21.920665 22535416901760 run.py:483] Algo bellman_ford step 6325 current loss 0.331015, current_train_items 202432.
I0302 19:01:21.936569 22535416901760 run.py:483] Algo bellman_ford step 6326 current loss 0.440466, current_train_items 202464.
I0302 19:01:21.961053 22535416901760 run.py:483] Algo bellman_ford step 6327 current loss 0.613804, current_train_items 202496.
I0302 19:01:21.991468 22535416901760 run.py:483] Algo bellman_ford step 6328 current loss 0.645283, current_train_items 202528.
I0302 19:01:22.025124 22535416901760 run.py:483] Algo bellman_ford step 6329 current loss 0.647240, current_train_items 202560.
I0302 19:01:22.044418 22535416901760 run.py:483] Algo bellman_ford step 6330 current loss 0.289591, current_train_items 202592.
I0302 19:01:22.060442 22535416901760 run.py:483] Algo bellman_ford step 6331 current loss 0.485483, current_train_items 202624.
I0302 19:01:22.083015 22535416901760 run.py:483] Algo bellman_ford step 6332 current loss 0.600018, current_train_items 202656.
I0302 19:01:22.113799 22535416901760 run.py:483] Algo bellman_ford step 6333 current loss 0.647157, current_train_items 202688.
I0302 19:01:22.146437 22535416901760 run.py:483] Algo bellman_ford step 6334 current loss 0.712810, current_train_items 202720.
I0302 19:01:22.166215 22535416901760 run.py:483] Algo bellman_ford step 6335 current loss 0.263797, current_train_items 202752.
I0302 19:01:22.182499 22535416901760 run.py:483] Algo bellman_ford step 6336 current loss 0.417928, current_train_items 202784.
I0302 19:01:22.206178 22535416901760 run.py:483] Algo bellman_ford step 6337 current loss 0.524787, current_train_items 202816.
I0302 19:01:22.237820 22535416901760 run.py:483] Algo bellman_ford step 6338 current loss 0.644736, current_train_items 202848.
I0302 19:01:22.270235 22535416901760 run.py:483] Algo bellman_ford step 6339 current loss 0.708536, current_train_items 202880.
I0302 19:01:22.289769 22535416901760 run.py:483] Algo bellman_ford step 6340 current loss 0.348792, current_train_items 202912.
I0302 19:01:22.305509 22535416901760 run.py:483] Algo bellman_ford step 6341 current loss 0.497192, current_train_items 202944.
I0302 19:01:22.329558 22535416901760 run.py:483] Algo bellman_ford step 6342 current loss 0.578474, current_train_items 202976.
I0302 19:01:22.359542 22535416901760 run.py:483] Algo bellman_ford step 6343 current loss 0.635288, current_train_items 203008.
I0302 19:01:22.390384 22535416901760 run.py:483] Algo bellman_ford step 6344 current loss 0.667375, current_train_items 203040.
I0302 19:01:22.410129 22535416901760 run.py:483] Algo bellman_ford step 6345 current loss 0.300075, current_train_items 203072.
I0302 19:01:22.426393 22535416901760 run.py:483] Algo bellman_ford step 6346 current loss 0.530935, current_train_items 203104.
I0302 19:01:22.449817 22535416901760 run.py:483] Algo bellman_ford step 6347 current loss 0.551367, current_train_items 203136.
I0302 19:01:22.479735 22535416901760 run.py:483] Algo bellman_ford step 6348 current loss 0.657985, current_train_items 203168.
I0302 19:01:22.514834 22535416901760 run.py:483] Algo bellman_ford step 6349 current loss 0.748739, current_train_items 203200.
I0302 19:01:22.534421 22535416901760 run.py:483] Algo bellman_ford step 6350 current loss 0.345066, current_train_items 203232.
I0302 19:01:22.542379 22535416901760 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:01:22.542486 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:01:22.559446 22535416901760 run.py:483] Algo bellman_ford step 6351 current loss 0.494429, current_train_items 203264.
I0302 19:01:22.582608 22535416901760 run.py:483] Algo bellman_ford step 6352 current loss 0.649058, current_train_items 203296.
I0302 19:01:22.613882 22535416901760 run.py:483] Algo bellman_ford step 6353 current loss 0.584621, current_train_items 203328.
I0302 19:01:22.646996 22535416901760 run.py:483] Algo bellman_ford step 6354 current loss 0.733477, current_train_items 203360.
I0302 19:01:22.667097 22535416901760 run.py:483] Algo bellman_ford step 6355 current loss 0.319320, current_train_items 203392.
I0302 19:01:22.683060 22535416901760 run.py:483] Algo bellman_ford step 6356 current loss 0.514031, current_train_items 203424.
I0302 19:01:22.706928 22535416901760 run.py:483] Algo bellman_ford step 6357 current loss 0.618547, current_train_items 203456.
I0302 19:01:22.737596 22535416901760 run.py:483] Algo bellman_ford step 6358 current loss 0.668325, current_train_items 203488.
I0302 19:01:22.771373 22535416901760 run.py:483] Algo bellman_ford step 6359 current loss 0.678939, current_train_items 203520.
I0302 19:01:22.791283 22535416901760 run.py:483] Algo bellman_ford step 6360 current loss 0.225417, current_train_items 203552.
I0302 19:01:22.807653 22535416901760 run.py:483] Algo bellman_ford step 6361 current loss 0.447244, current_train_items 203584.
I0302 19:01:22.830204 22535416901760 run.py:483] Algo bellman_ford step 6362 current loss 0.634543, current_train_items 203616.
I0302 19:01:22.860749 22535416901760 run.py:483] Algo bellman_ford step 6363 current loss 0.497498, current_train_items 203648.
I0302 19:01:22.893364 22535416901760 run.py:483] Algo bellman_ford step 6364 current loss 0.703925, current_train_items 203680.
I0302 19:01:22.913007 22535416901760 run.py:483] Algo bellman_ford step 6365 current loss 0.274553, current_train_items 203712.
I0302 19:01:22.929265 22535416901760 run.py:483] Algo bellman_ford step 6366 current loss 0.410977, current_train_items 203744.
I0302 19:01:22.952543 22535416901760 run.py:483] Algo bellman_ford step 6367 current loss 0.613767, current_train_items 203776.
I0302 19:01:22.984570 22535416901760 run.py:483] Algo bellman_ford step 6368 current loss 0.742093, current_train_items 203808.
I0302 19:01:23.016617 22535416901760 run.py:483] Algo bellman_ford step 6369 current loss 0.557137, current_train_items 203840.
I0302 19:01:23.036458 22535416901760 run.py:483] Algo bellman_ford step 6370 current loss 0.296028, current_train_items 203872.
I0302 19:01:23.052756 22535416901760 run.py:483] Algo bellman_ford step 6371 current loss 0.494267, current_train_items 203904.
I0302 19:01:23.076670 22535416901760 run.py:483] Algo bellman_ford step 6372 current loss 0.625671, current_train_items 203936.
I0302 19:01:23.106709 22535416901760 run.py:483] Algo bellman_ford step 6373 current loss 0.676937, current_train_items 203968.
I0302 19:01:23.139823 22535416901760 run.py:483] Algo bellman_ford step 6374 current loss 0.704871, current_train_items 204000.
I0302 19:01:23.159538 22535416901760 run.py:483] Algo bellman_ford step 6375 current loss 0.300572, current_train_items 204032.
I0302 19:01:23.175470 22535416901760 run.py:483] Algo bellman_ford step 6376 current loss 0.459684, current_train_items 204064.
I0302 19:01:23.199223 22535416901760 run.py:483] Algo bellman_ford step 6377 current loss 0.724971, current_train_items 204096.
I0302 19:01:23.229606 22535416901760 run.py:483] Algo bellman_ford step 6378 current loss 0.562382, current_train_items 204128.
I0302 19:01:23.264679 22535416901760 run.py:483] Algo bellman_ford step 6379 current loss 0.792622, current_train_items 204160.
I0302 19:01:23.283879 22535416901760 run.py:483] Algo bellman_ford step 6380 current loss 0.306332, current_train_items 204192.
I0302 19:01:23.299906 22535416901760 run.py:483] Algo bellman_ford step 6381 current loss 0.470478, current_train_items 204224.
I0302 19:01:23.323332 22535416901760 run.py:483] Algo bellman_ford step 6382 current loss 0.528264, current_train_items 204256.
I0302 19:01:23.353347 22535416901760 run.py:483] Algo bellman_ford step 6383 current loss 0.684464, current_train_items 204288.
I0302 19:01:23.386695 22535416901760 run.py:483] Algo bellman_ford step 6384 current loss 0.688999, current_train_items 204320.
I0302 19:01:23.406363 22535416901760 run.py:483] Algo bellman_ford step 6385 current loss 0.345705, current_train_items 204352.
I0302 19:01:23.422032 22535416901760 run.py:483] Algo bellman_ford step 6386 current loss 0.512472, current_train_items 204384.
I0302 19:01:23.445611 22535416901760 run.py:483] Algo bellman_ford step 6387 current loss 0.538868, current_train_items 204416.
I0302 19:01:23.475553 22535416901760 run.py:483] Algo bellman_ford step 6388 current loss 0.579309, current_train_items 204448.
I0302 19:01:23.509013 22535416901760 run.py:483] Algo bellman_ford step 6389 current loss 0.694281, current_train_items 204480.
I0302 19:01:23.528731 22535416901760 run.py:483] Algo bellman_ford step 6390 current loss 0.305052, current_train_items 204512.
I0302 19:01:23.545071 22535416901760 run.py:483] Algo bellman_ford step 6391 current loss 0.429161, current_train_items 204544.
I0302 19:01:23.567944 22535416901760 run.py:483] Algo bellman_ford step 6392 current loss 0.572865, current_train_items 204576.
I0302 19:01:23.597113 22535416901760 run.py:483] Algo bellman_ford step 6393 current loss 0.647513, current_train_items 204608.
I0302 19:01:23.630465 22535416901760 run.py:483] Algo bellman_ford step 6394 current loss 0.795005, current_train_items 204640.
I0302 19:01:23.650099 22535416901760 run.py:483] Algo bellman_ford step 6395 current loss 0.265324, current_train_items 204672.
I0302 19:01:23.666197 22535416901760 run.py:483] Algo bellman_ford step 6396 current loss 0.402643, current_train_items 204704.
I0302 19:01:23.688939 22535416901760 run.py:483] Algo bellman_ford step 6397 current loss 0.601040, current_train_items 204736.
I0302 19:01:23.718830 22535416901760 run.py:483] Algo bellman_ford step 6398 current loss 0.594467, current_train_items 204768.
I0302 19:01:23.749936 22535416901760 run.py:483] Algo bellman_ford step 6399 current loss 0.668975, current_train_items 204800.
I0302 19:01:23.769877 22535416901760 run.py:483] Algo bellman_ford step 6400 current loss 0.291598, current_train_items 204832.
I0302 19:01:23.777778 22535416901760 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:01:23.777882 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:01:23.794012 22535416901760 run.py:483] Algo bellman_ford step 6401 current loss 0.501140, current_train_items 204864.
I0302 19:01:23.817516 22535416901760 run.py:483] Algo bellman_ford step 6402 current loss 0.598250, current_train_items 204896.
I0302 19:01:23.848454 22535416901760 run.py:483] Algo bellman_ford step 6403 current loss 0.695376, current_train_items 204928.
I0302 19:01:23.882080 22535416901760 run.py:483] Algo bellman_ford step 6404 current loss 0.748325, current_train_items 204960.
I0302 19:01:23.902186 22535416901760 run.py:483] Algo bellman_ford step 6405 current loss 0.326414, current_train_items 204992.
I0302 19:01:23.917956 22535416901760 run.py:483] Algo bellman_ford step 6406 current loss 0.478903, current_train_items 205024.
I0302 19:01:23.940876 22535416901760 run.py:483] Algo bellman_ford step 6407 current loss 0.584041, current_train_items 205056.
I0302 19:01:23.971681 22535416901760 run.py:483] Algo bellman_ford step 6408 current loss 0.695871, current_train_items 205088.
I0302 19:01:24.006557 22535416901760 run.py:483] Algo bellman_ford step 6409 current loss 0.845157, current_train_items 205120.
I0302 19:01:24.026151 22535416901760 run.py:483] Algo bellman_ford step 6410 current loss 0.282446, current_train_items 205152.
I0302 19:01:24.042053 22535416901760 run.py:483] Algo bellman_ford step 6411 current loss 0.461360, current_train_items 205184.
I0302 19:01:24.064516 22535416901760 run.py:483] Algo bellman_ford step 6412 current loss 0.594321, current_train_items 205216.
I0302 19:01:24.095496 22535416901760 run.py:483] Algo bellman_ford step 6413 current loss 0.736200, current_train_items 205248.
I0302 19:01:24.128877 22535416901760 run.py:483] Algo bellman_ford step 6414 current loss 0.648773, current_train_items 205280.
I0302 19:01:24.148424 22535416901760 run.py:483] Algo bellman_ford step 6415 current loss 0.237793, current_train_items 205312.
I0302 19:01:24.164521 22535416901760 run.py:483] Algo bellman_ford step 6416 current loss 0.415685, current_train_items 205344.
I0302 19:01:24.188632 22535416901760 run.py:483] Algo bellman_ford step 6417 current loss 0.572761, current_train_items 205376.
I0302 19:01:24.220471 22535416901760 run.py:483] Algo bellman_ford step 6418 current loss 0.675458, current_train_items 205408.
I0302 19:01:24.254372 22535416901760 run.py:483] Algo bellman_ford step 6419 current loss 0.842758, current_train_items 205440.
I0302 19:01:24.273911 22535416901760 run.py:483] Algo bellman_ford step 6420 current loss 0.305367, current_train_items 205472.
I0302 19:01:24.290125 22535416901760 run.py:483] Algo bellman_ford step 6421 current loss 0.445245, current_train_items 205504.
I0302 19:01:24.313187 22535416901760 run.py:483] Algo bellman_ford step 6422 current loss 0.592985, current_train_items 205536.
I0302 19:01:24.343847 22535416901760 run.py:483] Algo bellman_ford step 6423 current loss 0.711989, current_train_items 205568.
I0302 19:01:24.376586 22535416901760 run.py:483] Algo bellman_ford step 6424 current loss 0.658786, current_train_items 205600.
I0302 19:01:24.396097 22535416901760 run.py:483] Algo bellman_ford step 6425 current loss 0.319238, current_train_items 205632.
I0302 19:01:24.412602 22535416901760 run.py:483] Algo bellman_ford step 6426 current loss 0.498413, current_train_items 205664.
I0302 19:01:24.436131 22535416901760 run.py:483] Algo bellman_ford step 6427 current loss 0.601633, current_train_items 205696.
I0302 19:01:24.467226 22535416901760 run.py:483] Algo bellman_ford step 6428 current loss 0.738813, current_train_items 205728.
I0302 19:01:24.502563 22535416901760 run.py:483] Algo bellman_ford step 6429 current loss 0.842374, current_train_items 205760.
I0302 19:01:24.522348 22535416901760 run.py:483] Algo bellman_ford step 6430 current loss 0.306244, current_train_items 205792.
I0302 19:01:24.537963 22535416901760 run.py:483] Algo bellman_ford step 6431 current loss 0.530514, current_train_items 205824.
I0302 19:01:24.561506 22535416901760 run.py:483] Algo bellman_ford step 6432 current loss 0.715070, current_train_items 205856.
I0302 19:01:24.592922 22535416901760 run.py:483] Algo bellman_ford step 6433 current loss 0.628067, current_train_items 205888.
I0302 19:01:24.625428 22535416901760 run.py:483] Algo bellman_ford step 6434 current loss 0.727039, current_train_items 205920.
I0302 19:01:24.644925 22535416901760 run.py:483] Algo bellman_ford step 6435 current loss 0.293937, current_train_items 205952.
I0302 19:01:24.661005 22535416901760 run.py:483] Algo bellman_ford step 6436 current loss 0.610526, current_train_items 205984.
I0302 19:01:24.684103 22535416901760 run.py:483] Algo bellman_ford step 6437 current loss 0.601614, current_train_items 206016.
I0302 19:01:24.715790 22535416901760 run.py:483] Algo bellman_ford step 6438 current loss 0.702576, current_train_items 206048.
I0302 19:01:24.749914 22535416901760 run.py:483] Algo bellman_ford step 6439 current loss 0.689239, current_train_items 206080.
I0302 19:01:24.769672 22535416901760 run.py:483] Algo bellman_ford step 6440 current loss 0.226451, current_train_items 206112.
I0302 19:01:24.786058 22535416901760 run.py:483] Algo bellman_ford step 6441 current loss 0.484350, current_train_items 206144.
I0302 19:01:24.808477 22535416901760 run.py:483] Algo bellman_ford step 6442 current loss 0.527273, current_train_items 206176.
I0302 19:01:24.837669 22535416901760 run.py:483] Algo bellman_ford step 6443 current loss 0.586701, current_train_items 206208.
I0302 19:01:24.871016 22535416901760 run.py:483] Algo bellman_ford step 6444 current loss 0.613466, current_train_items 206240.
I0302 19:01:24.890866 22535416901760 run.py:483] Algo bellman_ford step 6445 current loss 0.268291, current_train_items 206272.
I0302 19:01:24.906799 22535416901760 run.py:483] Algo bellman_ford step 6446 current loss 0.499292, current_train_items 206304.
I0302 19:01:24.930979 22535416901760 run.py:483] Algo bellman_ford step 6447 current loss 0.564721, current_train_items 206336.
I0302 19:01:24.961881 22535416901760 run.py:483] Algo bellman_ford step 6448 current loss 0.725352, current_train_items 206368.
I0302 19:01:24.995334 22535416901760 run.py:483] Algo bellman_ford step 6449 current loss 0.772254, current_train_items 206400.
I0302 19:01:25.015238 22535416901760 run.py:483] Algo bellman_ford step 6450 current loss 0.310230, current_train_items 206432.
I0302 19:01:25.023232 22535416901760 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:01:25.023340 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:25.040182 22535416901760 run.py:483] Algo bellman_ford step 6451 current loss 0.418973, current_train_items 206464.
I0302 19:01:25.065043 22535416901760 run.py:483] Algo bellman_ford step 6452 current loss 0.696451, current_train_items 206496.
I0302 19:01:25.095299 22535416901760 run.py:483] Algo bellman_ford step 6453 current loss 0.689162, current_train_items 206528.
I0302 19:01:25.127162 22535416901760 run.py:483] Algo bellman_ford step 6454 current loss 0.668230, current_train_items 206560.
I0302 19:01:25.146964 22535416901760 run.py:483] Algo bellman_ford step 6455 current loss 0.307845, current_train_items 206592.
I0302 19:01:25.162979 22535416901760 run.py:483] Algo bellman_ford step 6456 current loss 0.403659, current_train_items 206624.
I0302 19:01:25.185241 22535416901760 run.py:483] Algo bellman_ford step 6457 current loss 0.628030, current_train_items 206656.
I0302 19:01:25.215423 22535416901760 run.py:483] Algo bellman_ford step 6458 current loss 0.638107, current_train_items 206688.
I0302 19:01:25.246678 22535416901760 run.py:483] Algo bellman_ford step 6459 current loss 0.734203, current_train_items 206720.
I0302 19:01:25.266542 22535416901760 run.py:483] Algo bellman_ford step 6460 current loss 0.345289, current_train_items 206752.
I0302 19:01:25.282877 22535416901760 run.py:483] Algo bellman_ford step 6461 current loss 0.454485, current_train_items 206784.
I0302 19:01:25.305351 22535416901760 run.py:483] Algo bellman_ford step 6462 current loss 0.534756, current_train_items 206816.
I0302 19:01:25.335186 22535416901760 run.py:483] Algo bellman_ford step 6463 current loss 0.572916, current_train_items 206848.
I0302 19:01:25.370377 22535416901760 run.py:483] Algo bellman_ford step 6464 current loss 0.774529, current_train_items 206880.
I0302 19:01:25.389612 22535416901760 run.py:483] Algo bellman_ford step 6465 current loss 0.224703, current_train_items 206912.
I0302 19:01:25.405932 22535416901760 run.py:483] Algo bellman_ford step 6466 current loss 0.435515, current_train_items 206944.
I0302 19:01:25.430817 22535416901760 run.py:483] Algo bellman_ford step 6467 current loss 0.666160, current_train_items 206976.
I0302 19:01:25.462528 22535416901760 run.py:483] Algo bellman_ford step 6468 current loss 0.661119, current_train_items 207008.
I0302 19:01:25.496727 22535416901760 run.py:483] Algo bellman_ford step 6469 current loss 0.743254, current_train_items 207040.
I0302 19:01:25.516556 22535416901760 run.py:483] Algo bellman_ford step 6470 current loss 0.343922, current_train_items 207072.
I0302 19:01:25.533250 22535416901760 run.py:483] Algo bellman_ford step 6471 current loss 0.429993, current_train_items 207104.
I0302 19:01:25.555480 22535416901760 run.py:483] Algo bellman_ford step 6472 current loss 0.494097, current_train_items 207136.
I0302 19:01:25.586209 22535416901760 run.py:483] Algo bellman_ford step 6473 current loss 0.710102, current_train_items 207168.
I0302 19:01:25.618307 22535416901760 run.py:483] Algo bellman_ford step 6474 current loss 0.624642, current_train_items 207200.
I0302 19:01:25.637755 22535416901760 run.py:483] Algo bellman_ford step 6475 current loss 0.449005, current_train_items 207232.
I0302 19:01:25.654162 22535416901760 run.py:483] Algo bellman_ford step 6476 current loss 0.506395, current_train_items 207264.
I0302 19:01:25.676172 22535416901760 run.py:483] Algo bellman_ford step 6477 current loss 0.544364, current_train_items 207296.
I0302 19:01:25.706784 22535416901760 run.py:483] Algo bellman_ford step 6478 current loss 0.614346, current_train_items 207328.
I0302 19:01:25.741152 22535416901760 run.py:483] Algo bellman_ford step 6479 current loss 0.745446, current_train_items 207360.
I0302 19:01:25.760713 22535416901760 run.py:483] Algo bellman_ford step 6480 current loss 0.341473, current_train_items 207392.
I0302 19:01:25.776463 22535416901760 run.py:483] Algo bellman_ford step 6481 current loss 0.482426, current_train_items 207424.
I0302 19:01:25.799381 22535416901760 run.py:483] Algo bellman_ford step 6482 current loss 0.641009, current_train_items 207456.
I0302 19:01:25.830603 22535416901760 run.py:483] Algo bellman_ford step 6483 current loss 0.698456, current_train_items 207488.
I0302 19:01:25.864601 22535416901760 run.py:483] Algo bellman_ford step 6484 current loss 0.715915, current_train_items 207520.
I0302 19:01:25.884379 22535416901760 run.py:483] Algo bellman_ford step 6485 current loss 0.323482, current_train_items 207552.
I0302 19:01:25.900756 22535416901760 run.py:483] Algo bellman_ford step 6486 current loss 0.396232, current_train_items 207584.
I0302 19:01:25.925089 22535416901760 run.py:483] Algo bellman_ford step 6487 current loss 0.638951, current_train_items 207616.
I0302 19:01:25.954632 22535416901760 run.py:483] Algo bellman_ford step 6488 current loss 0.640049, current_train_items 207648.
I0302 19:01:25.988069 22535416901760 run.py:483] Algo bellman_ford step 6489 current loss 0.685752, current_train_items 207680.
I0302 19:01:26.007984 22535416901760 run.py:483] Algo bellman_ford step 6490 current loss 0.421733, current_train_items 207712.
I0302 19:01:26.024085 22535416901760 run.py:483] Algo bellman_ford step 6491 current loss 0.437743, current_train_items 207744.
I0302 19:01:26.046359 22535416901760 run.py:483] Algo bellman_ford step 6492 current loss 0.650428, current_train_items 207776.
I0302 19:01:26.076788 22535416901760 run.py:483] Algo bellman_ford step 6493 current loss 0.675509, current_train_items 207808.
I0302 19:01:26.109460 22535416901760 run.py:483] Algo bellman_ford step 6494 current loss 0.873311, current_train_items 207840.
I0302 19:01:26.129039 22535416901760 run.py:483] Algo bellman_ford step 6495 current loss 0.348868, current_train_items 207872.
I0302 19:01:26.144835 22535416901760 run.py:483] Algo bellman_ford step 6496 current loss 0.434828, current_train_items 207904.
I0302 19:01:26.168524 22535416901760 run.py:483] Algo bellman_ford step 6497 current loss 0.647110, current_train_items 207936.
I0302 19:01:26.198713 22535416901760 run.py:483] Algo bellman_ford step 6498 current loss 0.640759, current_train_items 207968.
I0302 19:01:26.233215 22535416901760 run.py:483] Algo bellman_ford step 6499 current loss 0.752553, current_train_items 208000.
I0302 19:01:26.253112 22535416901760 run.py:483] Algo bellman_ford step 6500 current loss 0.378128, current_train_items 208032.
I0302 19:01:26.260792 22535416901760 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:01:26.260898 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:01:26.277679 22535416901760 run.py:483] Algo bellman_ford step 6501 current loss 0.395109, current_train_items 208064.
I0302 19:01:26.302086 22535416901760 run.py:483] Algo bellman_ford step 6502 current loss 0.669073, current_train_items 208096.
I0302 19:01:26.332401 22535416901760 run.py:483] Algo bellman_ford step 6503 current loss 0.649032, current_train_items 208128.
I0302 19:01:26.366645 22535416901760 run.py:483] Algo bellman_ford step 6504 current loss 0.698849, current_train_items 208160.
I0302 19:01:26.386636 22535416901760 run.py:483] Algo bellman_ford step 6505 current loss 0.304287, current_train_items 208192.
I0302 19:01:26.402109 22535416901760 run.py:483] Algo bellman_ford step 6506 current loss 0.331411, current_train_items 208224.
I0302 19:01:26.425624 22535416901760 run.py:483] Algo bellman_ford step 6507 current loss 0.673562, current_train_items 208256.
I0302 19:01:26.456957 22535416901760 run.py:483] Algo bellman_ford step 6508 current loss 0.647070, current_train_items 208288.
I0302 19:01:26.488179 22535416901760 run.py:483] Algo bellman_ford step 6509 current loss 0.786323, current_train_items 208320.
I0302 19:01:26.507833 22535416901760 run.py:483] Algo bellman_ford step 6510 current loss 0.307499, current_train_items 208352.
I0302 19:01:26.524006 22535416901760 run.py:483] Algo bellman_ford step 6511 current loss 0.449851, current_train_items 208384.
I0302 19:01:26.548090 22535416901760 run.py:483] Algo bellman_ford step 6512 current loss 0.640188, current_train_items 208416.
I0302 19:01:26.578320 22535416901760 run.py:483] Algo bellman_ford step 6513 current loss 0.619473, current_train_items 208448.
I0302 19:01:26.612706 22535416901760 run.py:483] Algo bellman_ford step 6514 current loss 0.824956, current_train_items 208480.
I0302 19:01:26.631853 22535416901760 run.py:483] Algo bellman_ford step 6515 current loss 0.294730, current_train_items 208512.
I0302 19:01:26.647708 22535416901760 run.py:483] Algo bellman_ford step 6516 current loss 0.487299, current_train_items 208544.
I0302 19:01:26.671428 22535416901760 run.py:483] Algo bellman_ford step 6517 current loss 0.576469, current_train_items 208576.
I0302 19:01:26.701931 22535416901760 run.py:483] Algo bellman_ford step 6518 current loss 0.655761, current_train_items 208608.
I0302 19:01:26.734273 22535416901760 run.py:483] Algo bellman_ford step 6519 current loss 0.733205, current_train_items 208640.
I0302 19:01:26.753529 22535416901760 run.py:483] Algo bellman_ford step 6520 current loss 0.284885, current_train_items 208672.
I0302 19:01:26.769716 22535416901760 run.py:483] Algo bellman_ford step 6521 current loss 0.455538, current_train_items 208704.
I0302 19:01:26.793659 22535416901760 run.py:483] Algo bellman_ford step 6522 current loss 0.628894, current_train_items 208736.
I0302 19:01:26.823892 22535416901760 run.py:483] Algo bellman_ford step 6523 current loss 0.630660, current_train_items 208768.
I0302 19:01:26.854822 22535416901760 run.py:483] Algo bellman_ford step 6524 current loss 0.762007, current_train_items 208800.
I0302 19:01:26.874620 22535416901760 run.py:483] Algo bellman_ford step 6525 current loss 0.258525, current_train_items 208832.
I0302 19:01:26.890548 22535416901760 run.py:483] Algo bellman_ford step 6526 current loss 0.417425, current_train_items 208864.
I0302 19:01:26.913477 22535416901760 run.py:483] Algo bellman_ford step 6527 current loss 0.549978, current_train_items 208896.
I0302 19:01:26.943191 22535416901760 run.py:483] Algo bellman_ford step 6528 current loss 0.611058, current_train_items 208928.
I0302 19:01:26.976915 22535416901760 run.py:483] Algo bellman_ford step 6529 current loss 0.729953, current_train_items 208960.
I0302 19:01:26.996371 22535416901760 run.py:483] Algo bellman_ford step 6530 current loss 0.311623, current_train_items 208992.
I0302 19:01:27.012522 22535416901760 run.py:483] Algo bellman_ford step 6531 current loss 0.438594, current_train_items 209024.
I0302 19:01:27.037259 22535416901760 run.py:483] Algo bellman_ford step 6532 current loss 0.612413, current_train_items 209056.
I0302 19:01:27.067227 22535416901760 run.py:483] Algo bellman_ford step 6533 current loss 0.701947, current_train_items 209088.
I0302 19:01:27.100178 22535416901760 run.py:483] Algo bellman_ford step 6534 current loss 0.868583, current_train_items 209120.
I0302 19:01:27.119533 22535416901760 run.py:483] Algo bellman_ford step 6535 current loss 0.279588, current_train_items 209152.
I0302 19:01:27.135639 22535416901760 run.py:483] Algo bellman_ford step 6536 current loss 0.464630, current_train_items 209184.
I0302 19:01:27.158601 22535416901760 run.py:483] Algo bellman_ford step 6537 current loss 0.558857, current_train_items 209216.
I0302 19:01:27.188651 22535416901760 run.py:483] Algo bellman_ford step 6538 current loss 0.685758, current_train_items 209248.
I0302 19:01:27.221774 22535416901760 run.py:483] Algo bellman_ford step 6539 current loss 0.745826, current_train_items 209280.
I0302 19:01:27.241200 22535416901760 run.py:483] Algo bellman_ford step 6540 current loss 0.306381, current_train_items 209312.
I0302 19:01:27.257192 22535416901760 run.py:483] Algo bellman_ford step 6541 current loss 0.480941, current_train_items 209344.
I0302 19:01:27.280181 22535416901760 run.py:483] Algo bellman_ford step 6542 current loss 0.602867, current_train_items 209376.
I0302 19:01:27.311145 22535416901760 run.py:483] Algo bellman_ford step 6543 current loss 0.705315, current_train_items 209408.
I0302 19:01:27.346149 22535416901760 run.py:483] Algo bellman_ford step 6544 current loss 0.776978, current_train_items 209440.
I0302 19:01:27.365600 22535416901760 run.py:483] Algo bellman_ford step 6545 current loss 0.279146, current_train_items 209472.
I0302 19:01:27.381447 22535416901760 run.py:483] Algo bellman_ford step 6546 current loss 0.399678, current_train_items 209504.
I0302 19:01:27.403977 22535416901760 run.py:483] Algo bellman_ford step 6547 current loss 0.523492, current_train_items 209536.
I0302 19:01:27.435919 22535416901760 run.py:483] Algo bellman_ford step 6548 current loss 0.752285, current_train_items 209568.
I0302 19:01:27.470083 22535416901760 run.py:483] Algo bellman_ford step 6549 current loss 0.788742, current_train_items 209600.
I0302 19:01:27.489827 22535416901760 run.py:483] Algo bellman_ford step 6550 current loss 0.320895, current_train_items 209632.
I0302 19:01:27.497782 22535416901760 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:01:27.497888 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:01:27.514936 22535416901760 run.py:483] Algo bellman_ford step 6551 current loss 0.433264, current_train_items 209664.
I0302 19:01:27.538380 22535416901760 run.py:483] Algo bellman_ford step 6552 current loss 0.558047, current_train_items 209696.
I0302 19:01:27.570054 22535416901760 run.py:483] Algo bellman_ford step 6553 current loss 0.666046, current_train_items 209728.
I0302 19:01:27.605393 22535416901760 run.py:483] Algo bellman_ford step 6554 current loss 0.878870, current_train_items 209760.
I0302 19:01:27.625152 22535416901760 run.py:483] Algo bellman_ford step 6555 current loss 0.299694, current_train_items 209792.
I0302 19:01:27.640834 22535416901760 run.py:483] Algo bellman_ford step 6556 current loss 0.495452, current_train_items 209824.
I0302 19:01:27.664244 22535416901760 run.py:483] Algo bellman_ford step 6557 current loss 0.606982, current_train_items 209856.
I0302 19:01:27.695323 22535416901760 run.py:483] Algo bellman_ford step 6558 current loss 0.717555, current_train_items 209888.
I0302 19:01:27.729092 22535416901760 run.py:483] Algo bellman_ford step 6559 current loss 0.779748, current_train_items 209920.
I0302 19:01:27.749089 22535416901760 run.py:483] Algo bellman_ford step 6560 current loss 0.305332, current_train_items 209952.
I0302 19:01:27.764795 22535416901760 run.py:483] Algo bellman_ford step 6561 current loss 0.502511, current_train_items 209984.
I0302 19:01:27.787041 22535416901760 run.py:483] Algo bellman_ford step 6562 current loss 0.553870, current_train_items 210016.
I0302 19:01:27.817222 22535416901760 run.py:483] Algo bellman_ford step 6563 current loss 0.650243, current_train_items 210048.
I0302 19:01:27.847943 22535416901760 run.py:483] Algo bellman_ford step 6564 current loss 0.739322, current_train_items 210080.
I0302 19:01:27.867500 22535416901760 run.py:483] Algo bellman_ford step 6565 current loss 0.304410, current_train_items 210112.
I0302 19:01:27.883182 22535416901760 run.py:483] Algo bellman_ford step 6566 current loss 0.417048, current_train_items 210144.
I0302 19:01:27.906464 22535416901760 run.py:483] Algo bellman_ford step 6567 current loss 0.564170, current_train_items 210176.
I0302 19:01:27.935530 22535416901760 run.py:483] Algo bellman_ford step 6568 current loss 0.659961, current_train_items 210208.
I0302 19:01:27.968685 22535416901760 run.py:483] Algo bellman_ford step 6569 current loss 0.706373, current_train_items 210240.
I0302 19:01:27.988413 22535416901760 run.py:483] Algo bellman_ford step 6570 current loss 0.327978, current_train_items 210272.
I0302 19:01:28.004544 22535416901760 run.py:483] Algo bellman_ford step 6571 current loss 0.468846, current_train_items 210304.
I0302 19:01:28.027137 22535416901760 run.py:483] Algo bellman_ford step 6572 current loss 0.546387, current_train_items 210336.
I0302 19:01:28.057025 22535416901760 run.py:483] Algo bellman_ford step 6573 current loss 0.520091, current_train_items 210368.
I0302 19:01:28.089634 22535416901760 run.py:483] Algo bellman_ford step 6574 current loss 0.708432, current_train_items 210400.
I0302 19:01:28.109354 22535416901760 run.py:483] Algo bellman_ford step 6575 current loss 0.315985, current_train_items 210432.
I0302 19:01:28.125546 22535416901760 run.py:483] Algo bellman_ford step 6576 current loss 0.394011, current_train_items 210464.
I0302 19:01:28.148191 22535416901760 run.py:483] Algo bellman_ford step 6577 current loss 0.483812, current_train_items 210496.
I0302 19:01:28.178607 22535416901760 run.py:483] Algo bellman_ford step 6578 current loss 0.606410, current_train_items 210528.
I0302 19:01:28.211695 22535416901760 run.py:483] Algo bellman_ford step 6579 current loss 0.823937, current_train_items 210560.
I0302 19:01:28.231446 22535416901760 run.py:483] Algo bellman_ford step 6580 current loss 0.344276, current_train_items 210592.
I0302 19:01:28.247494 22535416901760 run.py:483] Algo bellman_ford step 6581 current loss 0.411035, current_train_items 210624.
I0302 19:01:28.270814 22535416901760 run.py:483] Algo bellman_ford step 6582 current loss 0.609815, current_train_items 210656.
I0302 19:01:28.302480 22535416901760 run.py:483] Algo bellman_ford step 6583 current loss 0.695901, current_train_items 210688.
I0302 19:01:28.334735 22535416901760 run.py:483] Algo bellman_ford step 6584 current loss 0.697272, current_train_items 210720.
I0302 19:01:28.354378 22535416901760 run.py:483] Algo bellman_ford step 6585 current loss 0.282888, current_train_items 210752.
I0302 19:01:28.370069 22535416901760 run.py:483] Algo bellman_ford step 6586 current loss 0.467650, current_train_items 210784.
I0302 19:01:28.391823 22535416901760 run.py:483] Algo bellman_ford step 6587 current loss 0.581398, current_train_items 210816.
I0302 19:01:28.421848 22535416901760 run.py:483] Algo bellman_ford step 6588 current loss 0.710602, current_train_items 210848.
I0302 19:01:28.453311 22535416901760 run.py:483] Algo bellman_ford step 6589 current loss 0.632114, current_train_items 210880.
I0302 19:01:28.472905 22535416901760 run.py:483] Algo bellman_ford step 6590 current loss 0.336151, current_train_items 210912.
I0302 19:01:28.488772 22535416901760 run.py:483] Algo bellman_ford step 6591 current loss 0.440620, current_train_items 210944.
I0302 19:01:28.511971 22535416901760 run.py:483] Algo bellman_ford step 6592 current loss 0.618934, current_train_items 210976.
I0302 19:01:28.542261 22535416901760 run.py:483] Algo bellman_ford step 6593 current loss 0.586480, current_train_items 211008.
I0302 19:01:28.574476 22535416901760 run.py:483] Algo bellman_ford step 6594 current loss 0.827897, current_train_items 211040.
I0302 19:01:28.593755 22535416901760 run.py:483] Algo bellman_ford step 6595 current loss 0.286593, current_train_items 211072.
I0302 19:01:28.609695 22535416901760 run.py:483] Algo bellman_ford step 6596 current loss 0.464844, current_train_items 211104.
I0302 19:01:28.633368 22535416901760 run.py:483] Algo bellman_ford step 6597 current loss 0.583758, current_train_items 211136.
I0302 19:01:28.664443 22535416901760 run.py:483] Algo bellman_ford step 6598 current loss 0.752776, current_train_items 211168.
I0302 19:01:28.694472 22535416901760 run.py:483] Algo bellman_ford step 6599 current loss 0.738601, current_train_items 211200.
I0302 19:01:28.714439 22535416901760 run.py:483] Algo bellman_ford step 6600 current loss 0.255252, current_train_items 211232.
I0302 19:01:28.722205 22535416901760 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:01:28.722313 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:28.738893 22535416901760 run.py:483] Algo bellman_ford step 6601 current loss 0.479705, current_train_items 211264.
I0302 19:01:28.761328 22535416901760 run.py:483] Algo bellman_ford step 6602 current loss 0.558811, current_train_items 211296.
I0302 19:01:28.792420 22535416901760 run.py:483] Algo bellman_ford step 6603 current loss 0.713051, current_train_items 211328.
I0302 19:01:28.826205 22535416901760 run.py:483] Algo bellman_ford step 6604 current loss 0.757662, current_train_items 211360.
I0302 19:01:28.845994 22535416901760 run.py:483] Algo bellman_ford step 6605 current loss 0.296698, current_train_items 211392.
I0302 19:01:28.861545 22535416901760 run.py:483] Algo bellman_ford step 6606 current loss 0.382239, current_train_items 211424.
I0302 19:01:28.884798 22535416901760 run.py:483] Algo bellman_ford step 6607 current loss 0.622561, current_train_items 211456.
I0302 19:01:28.915950 22535416901760 run.py:483] Algo bellman_ford step 6608 current loss 0.530117, current_train_items 211488.
I0302 19:01:28.947235 22535416901760 run.py:483] Algo bellman_ford step 6609 current loss 0.610847, current_train_items 211520.
I0302 19:01:28.966710 22535416901760 run.py:483] Algo bellman_ford step 6610 current loss 0.287750, current_train_items 211552.
I0302 19:01:28.982481 22535416901760 run.py:483] Algo bellman_ford step 6611 current loss 0.483405, current_train_items 211584.
I0302 19:01:29.004543 22535416901760 run.py:483] Algo bellman_ford step 6612 current loss 0.496052, current_train_items 211616.
I0302 19:01:29.035365 22535416901760 run.py:483] Algo bellman_ford step 6613 current loss 0.666075, current_train_items 211648.
I0302 19:01:29.071000 22535416901760 run.py:483] Algo bellman_ford step 6614 current loss 0.817387, current_train_items 211680.
I0302 19:01:29.090471 22535416901760 run.py:483] Algo bellman_ford step 6615 current loss 0.347288, current_train_items 211712.
I0302 19:01:29.106317 22535416901760 run.py:483] Algo bellman_ford step 6616 current loss 0.358328, current_train_items 211744.
I0302 19:01:29.129702 22535416901760 run.py:483] Algo bellman_ford step 6617 current loss 0.573220, current_train_items 211776.
I0302 19:01:29.160147 22535416901760 run.py:483] Algo bellman_ford step 6618 current loss 0.619840, current_train_items 211808.
I0302 19:01:29.193627 22535416901760 run.py:483] Algo bellman_ford step 6619 current loss 0.768098, current_train_items 211840.
I0302 19:01:29.212849 22535416901760 run.py:483] Algo bellman_ford step 6620 current loss 0.333977, current_train_items 211872.
I0302 19:01:29.228515 22535416901760 run.py:483] Algo bellman_ford step 6621 current loss 0.420271, current_train_items 211904.
I0302 19:01:29.250405 22535416901760 run.py:483] Algo bellman_ford step 6622 current loss 0.579886, current_train_items 211936.
I0302 19:01:29.281667 22535416901760 run.py:483] Algo bellman_ford step 6623 current loss 0.581966, current_train_items 211968.
I0302 19:01:29.313382 22535416901760 run.py:483] Algo bellman_ford step 6624 current loss 0.690420, current_train_items 212000.
I0302 19:01:29.332837 22535416901760 run.py:483] Algo bellman_ford step 6625 current loss 0.305949, current_train_items 212032.
I0302 19:01:29.348822 22535416901760 run.py:483] Algo bellman_ford step 6626 current loss 0.412603, current_train_items 212064.
I0302 19:01:29.371936 22535416901760 run.py:483] Algo bellman_ford step 6627 current loss 0.548784, current_train_items 212096.
I0302 19:01:29.401422 22535416901760 run.py:483] Algo bellman_ford step 6628 current loss 0.646598, current_train_items 212128.
I0302 19:01:29.432738 22535416901760 run.py:483] Algo bellman_ford step 6629 current loss 0.719471, current_train_items 212160.
I0302 19:01:29.452353 22535416901760 run.py:483] Algo bellman_ford step 6630 current loss 0.337861, current_train_items 212192.
I0302 19:01:29.468422 22535416901760 run.py:483] Algo bellman_ford step 6631 current loss 0.501358, current_train_items 212224.
I0302 19:01:29.492237 22535416901760 run.py:483] Algo bellman_ford step 6632 current loss 0.610613, current_train_items 212256.
I0302 19:01:29.523639 22535416901760 run.py:483] Algo bellman_ford step 6633 current loss 0.632877, current_train_items 212288.
I0302 19:01:29.555320 22535416901760 run.py:483] Algo bellman_ford step 6634 current loss 0.712305, current_train_items 212320.
I0302 19:01:29.574727 22535416901760 run.py:483] Algo bellman_ford step 6635 current loss 0.273903, current_train_items 212352.
I0302 19:01:29.590924 22535416901760 run.py:483] Algo bellman_ford step 6636 current loss 0.502208, current_train_items 212384.
I0302 19:01:29.614083 22535416901760 run.py:483] Algo bellman_ford step 6637 current loss 0.547589, current_train_items 212416.
I0302 19:01:29.644391 22535416901760 run.py:483] Algo bellman_ford step 6638 current loss 0.664653, current_train_items 212448.
I0302 19:01:29.677846 22535416901760 run.py:483] Algo bellman_ford step 6639 current loss 0.854331, current_train_items 212480.
I0302 19:01:29.697057 22535416901760 run.py:483] Algo bellman_ford step 6640 current loss 0.300254, current_train_items 212512.
I0302 19:01:29.713340 22535416901760 run.py:483] Algo bellman_ford step 6641 current loss 0.453020, current_train_items 212544.
I0302 19:01:29.737474 22535416901760 run.py:483] Algo bellman_ford step 6642 current loss 0.654998, current_train_items 212576.
I0302 19:01:29.767507 22535416901760 run.py:483] Algo bellman_ford step 6643 current loss 0.644515, current_train_items 212608.
I0302 19:01:29.801363 22535416901760 run.py:483] Algo bellman_ford step 6644 current loss 0.691238, current_train_items 212640.
I0302 19:01:29.820657 22535416901760 run.py:483] Algo bellman_ford step 6645 current loss 0.338510, current_train_items 212672.
I0302 19:01:29.836534 22535416901760 run.py:483] Algo bellman_ford step 6646 current loss 0.537641, current_train_items 212704.
I0302 19:01:29.859538 22535416901760 run.py:483] Algo bellman_ford step 6647 current loss 0.666282, current_train_items 212736.
I0302 19:01:29.890332 22535416901760 run.py:483] Algo bellman_ford step 6648 current loss 0.782419, current_train_items 212768.
I0302 19:01:29.923991 22535416901760 run.py:483] Algo bellman_ford step 6649 current loss 0.729923, current_train_items 212800.
I0302 19:01:29.943567 22535416901760 run.py:483] Algo bellman_ford step 6650 current loss 0.332210, current_train_items 212832.
I0302 19:01:29.951462 22535416901760 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:01:29.951569 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:01:29.968659 22535416901760 run.py:483] Algo bellman_ford step 6651 current loss 0.490764, current_train_items 212864.
I0302 19:01:29.993014 22535416901760 run.py:483] Algo bellman_ford step 6652 current loss 0.684998, current_train_items 212896.
I0302 19:01:30.024340 22535416901760 run.py:483] Algo bellman_ford step 6653 current loss 0.899821, current_train_items 212928.
I0302 19:01:30.053633 22535416901760 run.py:483] Algo bellman_ford step 6654 current loss 0.637488, current_train_items 212960.
I0302 19:01:30.073407 22535416901760 run.py:483] Algo bellman_ford step 6655 current loss 0.323343, current_train_items 212992.
I0302 19:01:30.089066 22535416901760 run.py:483] Algo bellman_ford step 6656 current loss 0.604228, current_train_items 213024.
I0302 19:01:30.112043 22535416901760 run.py:483] Algo bellman_ford step 6657 current loss 0.590046, current_train_items 213056.
I0302 19:01:30.143292 22535416901760 run.py:483] Algo bellman_ford step 6658 current loss 0.730877, current_train_items 213088.
I0302 19:01:30.177846 22535416901760 run.py:483] Algo bellman_ford step 6659 current loss 0.814243, current_train_items 213120.
I0302 19:01:30.197702 22535416901760 run.py:483] Algo bellman_ford step 6660 current loss 0.327484, current_train_items 213152.
I0302 19:01:30.214179 22535416901760 run.py:483] Algo bellman_ford step 6661 current loss 0.547654, current_train_items 213184.
I0302 19:01:30.238495 22535416901760 run.py:483] Algo bellman_ford step 6662 current loss 0.763893, current_train_items 213216.
I0302 19:01:30.267959 22535416901760 run.py:483] Algo bellman_ford step 6663 current loss 0.667108, current_train_items 213248.
I0302 19:01:30.300746 22535416901760 run.py:483] Algo bellman_ford step 6664 current loss 0.766813, current_train_items 213280.
I0302 19:01:30.320280 22535416901760 run.py:483] Algo bellman_ford step 6665 current loss 0.278871, current_train_items 213312.
I0302 19:01:30.336242 22535416901760 run.py:483] Algo bellman_ford step 6666 current loss 0.431000, current_train_items 213344.
I0302 19:01:30.361476 22535416901760 run.py:483] Algo bellman_ford step 6667 current loss 0.672046, current_train_items 213376.
I0302 19:01:30.393271 22535416901760 run.py:483] Algo bellman_ford step 6668 current loss 0.746564, current_train_items 213408.
I0302 19:01:30.427845 22535416901760 run.py:483] Algo bellman_ford step 6669 current loss 0.878894, current_train_items 213440.
I0302 19:01:30.447678 22535416901760 run.py:483] Algo bellman_ford step 6670 current loss 0.324559, current_train_items 213472.
I0302 19:01:30.463733 22535416901760 run.py:483] Algo bellman_ford step 6671 current loss 0.425998, current_train_items 213504.
I0302 19:01:30.486878 22535416901760 run.py:483] Algo bellman_ford step 6672 current loss 0.571130, current_train_items 213536.
I0302 19:01:30.518006 22535416901760 run.py:483] Algo bellman_ford step 6673 current loss 0.772926, current_train_items 213568.
I0302 19:01:30.549768 22535416901760 run.py:483] Algo bellman_ford step 6674 current loss 0.701796, current_train_items 213600.
I0302 19:01:30.569696 22535416901760 run.py:483] Algo bellman_ford step 6675 current loss 0.423138, current_train_items 213632.
I0302 19:01:30.585294 22535416901760 run.py:483] Algo bellman_ford step 6676 current loss 0.422965, current_train_items 213664.
I0302 19:01:30.609196 22535416901760 run.py:483] Algo bellman_ford step 6677 current loss 0.585550, current_train_items 213696.
I0302 19:01:30.640720 22535416901760 run.py:483] Algo bellman_ford step 6678 current loss 0.771782, current_train_items 213728.
I0302 19:01:30.675681 22535416901760 run.py:483] Algo bellman_ford step 6679 current loss 0.919018, current_train_items 213760.
I0302 19:01:30.695394 22535416901760 run.py:483] Algo bellman_ford step 6680 current loss 0.308393, current_train_items 213792.
I0302 19:01:30.711451 22535416901760 run.py:483] Algo bellman_ford step 6681 current loss 0.517075, current_train_items 213824.
I0302 19:01:30.735626 22535416901760 run.py:483] Algo bellman_ford step 6682 current loss 0.574604, current_train_items 213856.
I0302 19:01:30.765793 22535416901760 run.py:483] Algo bellman_ford step 6683 current loss 0.756263, current_train_items 213888.
I0302 19:01:30.801707 22535416901760 run.py:483] Algo bellman_ford step 6684 current loss 0.847362, current_train_items 213920.
I0302 19:01:30.821466 22535416901760 run.py:483] Algo bellman_ford step 6685 current loss 0.305077, current_train_items 213952.
I0302 19:01:30.837368 22535416901760 run.py:483] Algo bellman_ford step 6686 current loss 0.480749, current_train_items 213984.
I0302 19:01:30.860214 22535416901760 run.py:483] Algo bellman_ford step 6687 current loss 0.565117, current_train_items 214016.
I0302 19:01:30.891392 22535416901760 run.py:483] Algo bellman_ford step 6688 current loss 0.651166, current_train_items 214048.
I0302 19:01:30.924049 22535416901760 run.py:483] Algo bellman_ford step 6689 current loss 0.727922, current_train_items 214080.
I0302 19:01:30.943750 22535416901760 run.py:483] Algo bellman_ford step 6690 current loss 0.315214, current_train_items 214112.
I0302 19:01:30.959448 22535416901760 run.py:483] Algo bellman_ford step 6691 current loss 0.459271, current_train_items 214144.
I0302 19:01:30.982947 22535416901760 run.py:483] Algo bellman_ford step 6692 current loss 0.682450, current_train_items 214176.
I0302 19:01:31.013626 22535416901760 run.py:483] Algo bellman_ford step 6693 current loss 0.661678, current_train_items 214208.
I0302 19:01:31.047216 22535416901760 run.py:483] Algo bellman_ford step 6694 current loss 0.663373, current_train_items 214240.
I0302 19:01:31.066498 22535416901760 run.py:483] Algo bellman_ford step 6695 current loss 0.237406, current_train_items 214272.
I0302 19:01:31.082749 22535416901760 run.py:483] Algo bellman_ford step 6696 current loss 0.599538, current_train_items 214304.
I0302 19:01:31.106307 22535416901760 run.py:483] Algo bellman_ford step 6697 current loss 0.701929, current_train_items 214336.
I0302 19:01:31.137547 22535416901760 run.py:483] Algo bellman_ford step 6698 current loss 0.853957, current_train_items 214368.
I0302 19:01:31.171452 22535416901760 run.py:483] Algo bellman_ford step 6699 current loss 0.964342, current_train_items 214400.
I0302 19:01:31.190879 22535416901760 run.py:483] Algo bellman_ford step 6700 current loss 0.398630, current_train_items 214432.
I0302 19:01:31.198589 22535416901760 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.91015625, 'score': 0.91015625, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:01:31.198694 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.910, val scores are: bellman_ford: 0.910
I0302 19:01:31.215754 22535416901760 run.py:483] Algo bellman_ford step 6701 current loss 0.560345, current_train_items 214464.
I0302 19:01:31.239434 22535416901760 run.py:483] Algo bellman_ford step 6702 current loss 0.679178, current_train_items 214496.
I0302 19:01:31.270786 22535416901760 run.py:483] Algo bellman_ford step 6703 current loss 0.649661, current_train_items 214528.
I0302 19:01:31.303513 22535416901760 run.py:483] Algo bellman_ford step 6704 current loss 0.687825, current_train_items 214560.
I0302 19:01:31.323315 22535416901760 run.py:483] Algo bellman_ford step 6705 current loss 0.279069, current_train_items 214592.
I0302 19:01:31.339128 22535416901760 run.py:483] Algo bellman_ford step 6706 current loss 0.534129, current_train_items 214624.
I0302 19:01:31.363543 22535416901760 run.py:483] Algo bellman_ford step 6707 current loss 0.626365, current_train_items 214656.
I0302 19:01:31.394467 22535416901760 run.py:483] Algo bellman_ford step 6708 current loss 0.620697, current_train_items 214688.
I0302 19:01:31.428578 22535416901760 run.py:483] Algo bellman_ford step 6709 current loss 0.794887, current_train_items 214720.
I0302 19:01:31.448168 22535416901760 run.py:483] Algo bellman_ford step 6710 current loss 0.355499, current_train_items 214752.
I0302 19:01:31.464391 22535416901760 run.py:483] Algo bellman_ford step 6711 current loss 0.443113, current_train_items 214784.
I0302 19:01:31.487004 22535416901760 run.py:483] Algo bellman_ford step 6712 current loss 0.542619, current_train_items 214816.
I0302 19:01:31.519225 22535416901760 run.py:483] Algo bellman_ford step 6713 current loss 0.814947, current_train_items 214848.
I0302 19:01:31.552195 22535416901760 run.py:483] Algo bellman_ford step 6714 current loss 0.749686, current_train_items 214880.
I0302 19:01:31.571433 22535416901760 run.py:483] Algo bellman_ford step 6715 current loss 0.282498, current_train_items 214912.
I0302 19:01:31.587511 22535416901760 run.py:483] Algo bellman_ford step 6716 current loss 0.433253, current_train_items 214944.
I0302 19:01:31.610862 22535416901760 run.py:483] Algo bellman_ford step 6717 current loss 0.559533, current_train_items 214976.
I0302 19:01:31.641747 22535416901760 run.py:483] Algo bellman_ford step 6718 current loss 0.545824, current_train_items 215008.
I0302 19:01:31.676495 22535416901760 run.py:483] Algo bellman_ford step 6719 current loss 0.700835, current_train_items 215040.
I0302 19:01:31.695677 22535416901760 run.py:483] Algo bellman_ford step 6720 current loss 0.289504, current_train_items 215072.
I0302 19:01:31.711692 22535416901760 run.py:483] Algo bellman_ford step 6721 current loss 0.473771, current_train_items 215104.
I0302 19:01:31.733984 22535416901760 run.py:483] Algo bellman_ford step 6722 current loss 0.606980, current_train_items 215136.
I0302 19:01:31.765475 22535416901760 run.py:483] Algo bellman_ford step 6723 current loss 0.791372, current_train_items 215168.
I0302 19:01:31.797568 22535416901760 run.py:483] Algo bellman_ford step 6724 current loss 0.707005, current_train_items 215200.
I0302 19:01:31.816734 22535416901760 run.py:483] Algo bellman_ford step 6725 current loss 0.232926, current_train_items 215232.
I0302 19:01:31.832851 22535416901760 run.py:483] Algo bellman_ford step 6726 current loss 0.452883, current_train_items 215264.
I0302 19:01:31.855597 22535416901760 run.py:483] Algo bellman_ford step 6727 current loss 0.514227, current_train_items 215296.
I0302 19:01:31.886307 22535416901760 run.py:483] Algo bellman_ford step 6728 current loss 0.677948, current_train_items 215328.
I0302 19:01:31.917833 22535416901760 run.py:483] Algo bellman_ford step 6729 current loss 0.749840, current_train_items 215360.
I0302 19:01:31.937187 22535416901760 run.py:483] Algo bellman_ford step 6730 current loss 0.308323, current_train_items 215392.
I0302 19:01:31.953116 22535416901760 run.py:483] Algo bellman_ford step 6731 current loss 0.493354, current_train_items 215424.
I0302 19:01:31.976892 22535416901760 run.py:483] Algo bellman_ford step 6732 current loss 0.652612, current_train_items 215456.
I0302 19:01:32.005947 22535416901760 run.py:483] Algo bellman_ford step 6733 current loss 0.592834, current_train_items 215488.
I0302 19:01:32.037100 22535416901760 run.py:483] Algo bellman_ford step 6734 current loss 0.652462, current_train_items 215520.
I0302 19:01:32.056441 22535416901760 run.py:483] Algo bellman_ford step 6735 current loss 0.321308, current_train_items 215552.
I0302 19:01:32.072632 22535416901760 run.py:483] Algo bellman_ford step 6736 current loss 0.500014, current_train_items 215584.
I0302 19:01:32.096943 22535416901760 run.py:483] Algo bellman_ford step 6737 current loss 0.611108, current_train_items 215616.
I0302 19:01:32.126412 22535416901760 run.py:483] Algo bellman_ford step 6738 current loss 0.585921, current_train_items 215648.
I0302 19:01:32.161167 22535416901760 run.py:483] Algo bellman_ford step 6739 current loss 0.737982, current_train_items 215680.
I0302 19:01:32.180739 22535416901760 run.py:483] Algo bellman_ford step 6740 current loss 0.307799, current_train_items 215712.
I0302 19:01:32.197001 22535416901760 run.py:483] Algo bellman_ford step 6741 current loss 0.438429, current_train_items 215744.
I0302 19:01:32.221256 22535416901760 run.py:483] Algo bellman_ford step 6742 current loss 0.543191, current_train_items 215776.
I0302 19:01:32.250781 22535416901760 run.py:483] Algo bellman_ford step 6743 current loss 0.593531, current_train_items 215808.
I0302 19:01:32.284735 22535416901760 run.py:483] Algo bellman_ford step 6744 current loss 0.788156, current_train_items 215840.
I0302 19:01:32.304196 22535416901760 run.py:483] Algo bellman_ford step 6745 current loss 0.293793, current_train_items 215872.
I0302 19:01:32.320303 22535416901760 run.py:483] Algo bellman_ford step 6746 current loss 0.429986, current_train_items 215904.
I0302 19:01:32.343641 22535416901760 run.py:483] Algo bellman_ford step 6747 current loss 0.488322, current_train_items 215936.
I0302 19:01:32.373633 22535416901760 run.py:483] Algo bellman_ford step 6748 current loss 0.564500, current_train_items 215968.
I0302 19:01:32.407256 22535416901760 run.py:483] Algo bellman_ford step 6749 current loss 0.804078, current_train_items 216000.
I0302 19:01:32.426603 22535416901760 run.py:483] Algo bellman_ford step 6750 current loss 0.347072, current_train_items 216032.
I0302 19:01:32.434869 22535416901760 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:01:32.434974 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:01:32.451629 22535416901760 run.py:483] Algo bellman_ford step 6751 current loss 0.418956, current_train_items 216064.
I0302 19:01:32.475758 22535416901760 run.py:483] Algo bellman_ford step 6752 current loss 0.638868, current_train_items 216096.
I0302 19:01:32.507714 22535416901760 run.py:483] Algo bellman_ford step 6753 current loss 0.659176, current_train_items 216128.
I0302 19:01:32.542342 22535416901760 run.py:483] Algo bellman_ford step 6754 current loss 0.748071, current_train_items 216160.
I0302 19:01:32.562536 22535416901760 run.py:483] Algo bellman_ford step 6755 current loss 0.319540, current_train_items 216192.
I0302 19:01:32.578878 22535416901760 run.py:483] Algo bellman_ford step 6756 current loss 0.508711, current_train_items 216224.
I0302 19:01:32.602195 22535416901760 run.py:483] Algo bellman_ford step 6757 current loss 0.508014, current_train_items 216256.
I0302 19:01:32.633699 22535416901760 run.py:483] Algo bellman_ford step 6758 current loss 0.695203, current_train_items 216288.
I0302 19:01:32.668229 22535416901760 run.py:483] Algo bellman_ford step 6759 current loss 0.837360, current_train_items 216320.
I0302 19:01:32.688076 22535416901760 run.py:483] Algo bellman_ford step 6760 current loss 0.285437, current_train_items 216352.
I0302 19:01:32.704493 22535416901760 run.py:483] Algo bellman_ford step 6761 current loss 0.367244, current_train_items 216384.
I0302 19:01:32.727654 22535416901760 run.py:483] Algo bellman_ford step 6762 current loss 0.548717, current_train_items 216416.
I0302 19:01:32.758880 22535416901760 run.py:483] Algo bellman_ford step 6763 current loss 0.637145, current_train_items 216448.
I0302 19:01:32.791362 22535416901760 run.py:483] Algo bellman_ford step 6764 current loss 0.708731, current_train_items 216480.
I0302 19:01:32.810875 22535416901760 run.py:483] Algo bellman_ford step 6765 current loss 0.274192, current_train_items 216512.
I0302 19:01:32.827315 22535416901760 run.py:483] Algo bellman_ford step 6766 current loss 0.467603, current_train_items 216544.
I0302 19:01:32.850543 22535416901760 run.py:483] Algo bellman_ford step 6767 current loss 0.522048, current_train_items 216576.
I0302 19:01:32.880518 22535416901760 run.py:483] Algo bellman_ford step 6768 current loss 0.608005, current_train_items 216608.
I0302 19:01:32.913254 22535416901760 run.py:483] Algo bellman_ford step 6769 current loss 0.755270, current_train_items 216640.
I0302 19:01:32.933395 22535416901760 run.py:483] Algo bellman_ford step 6770 current loss 0.264646, current_train_items 216672.
I0302 19:01:32.949425 22535416901760 run.py:483] Algo bellman_ford step 6771 current loss 0.457048, current_train_items 216704.
I0302 19:01:32.971514 22535416901760 run.py:483] Algo bellman_ford step 6772 current loss 0.548435, current_train_items 216736.
I0302 19:01:33.002329 22535416901760 run.py:483] Algo bellman_ford step 6773 current loss 0.627110, current_train_items 216768.
I0302 19:01:33.033939 22535416901760 run.py:483] Algo bellman_ford step 6774 current loss 0.691941, current_train_items 216800.
I0302 19:01:33.053794 22535416901760 run.py:483] Algo bellman_ford step 6775 current loss 0.293945, current_train_items 216832.
I0302 19:01:33.070022 22535416901760 run.py:483] Algo bellman_ford step 6776 current loss 0.470502, current_train_items 216864.
I0302 19:01:33.092881 22535416901760 run.py:483] Algo bellman_ford step 6777 current loss 0.535561, current_train_items 216896.
I0302 19:01:33.125073 22535416901760 run.py:483] Algo bellman_ford step 6778 current loss 0.699226, current_train_items 216928.
I0302 19:01:33.157629 22535416901760 run.py:483] Algo bellman_ford step 6779 current loss 0.693408, current_train_items 216960.
I0302 19:01:33.176994 22535416901760 run.py:483] Algo bellman_ford step 6780 current loss 0.314984, current_train_items 216992.
I0302 19:01:33.192926 22535416901760 run.py:483] Algo bellman_ford step 6781 current loss 0.465799, current_train_items 217024.
I0302 19:01:33.215729 22535416901760 run.py:483] Algo bellman_ford step 6782 current loss 0.601363, current_train_items 217056.
I0302 19:01:33.247595 22535416901760 run.py:483] Algo bellman_ford step 6783 current loss 0.675202, current_train_items 217088.
I0302 19:01:33.280063 22535416901760 run.py:483] Algo bellman_ford step 6784 current loss 0.845657, current_train_items 217120.
I0302 19:01:33.299716 22535416901760 run.py:483] Algo bellman_ford step 6785 current loss 0.278100, current_train_items 217152.
I0302 19:01:33.315616 22535416901760 run.py:483] Algo bellman_ford step 6786 current loss 0.446797, current_train_items 217184.
I0302 19:01:33.338148 22535416901760 run.py:483] Algo bellman_ford step 6787 current loss 0.488973, current_train_items 217216.
I0302 19:01:33.368115 22535416901760 run.py:483] Algo bellman_ford step 6788 current loss 0.612308, current_train_items 217248.
I0302 19:01:33.401853 22535416901760 run.py:483] Algo bellman_ford step 6789 current loss 0.742419, current_train_items 217280.
I0302 19:01:33.421828 22535416901760 run.py:483] Algo bellman_ford step 6790 current loss 0.389889, current_train_items 217312.
I0302 19:01:33.437908 22535416901760 run.py:483] Algo bellman_ford step 6791 current loss 0.418924, current_train_items 217344.
I0302 19:01:33.461162 22535416901760 run.py:483] Algo bellman_ford step 6792 current loss 0.526759, current_train_items 217376.
I0302 19:01:33.491217 22535416901760 run.py:483] Algo bellman_ford step 6793 current loss 0.606970, current_train_items 217408.
I0302 19:01:33.524164 22535416901760 run.py:483] Algo bellman_ford step 6794 current loss 0.720482, current_train_items 217440.
I0302 19:01:33.543570 22535416901760 run.py:483] Algo bellman_ford step 6795 current loss 0.263152, current_train_items 217472.
I0302 19:01:33.559552 22535416901760 run.py:483] Algo bellman_ford step 6796 current loss 0.418889, current_train_items 217504.
I0302 19:01:33.584105 22535416901760 run.py:483] Algo bellman_ford step 6797 current loss 0.648244, current_train_items 217536.
I0302 19:01:33.614407 22535416901760 run.py:483] Algo bellman_ford step 6798 current loss 0.653625, current_train_items 217568.
I0302 19:01:33.647440 22535416901760 run.py:483] Algo bellman_ford step 6799 current loss 0.730805, current_train_items 217600.
I0302 19:01:33.667358 22535416901760 run.py:483] Algo bellman_ford step 6800 current loss 0.331877, current_train_items 217632.
I0302 19:01:33.675314 22535416901760 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:01:33.675419 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:01:33.691970 22535416901760 run.py:483] Algo bellman_ford step 6801 current loss 0.455737, current_train_items 217664.
I0302 19:01:33.715925 22535416901760 run.py:483] Algo bellman_ford step 6802 current loss 0.596182, current_train_items 217696.
I0302 19:01:33.748518 22535416901760 run.py:483] Algo bellman_ford step 6803 current loss 0.703162, current_train_items 217728.
I0302 19:01:33.782621 22535416901760 run.py:483] Algo bellman_ford step 6804 current loss 0.713959, current_train_items 217760.
I0302 19:01:33.802713 22535416901760 run.py:483] Algo bellman_ford step 6805 current loss 0.271231, current_train_items 217792.
I0302 19:01:33.818886 22535416901760 run.py:483] Algo bellman_ford step 6806 current loss 0.374679, current_train_items 217824.
I0302 19:01:33.843137 22535416901760 run.py:483] Algo bellman_ford step 6807 current loss 0.704008, current_train_items 217856.
I0302 19:01:33.873212 22535416901760 run.py:483] Algo bellman_ford step 6808 current loss 0.624470, current_train_items 217888.
I0302 19:01:33.906740 22535416901760 run.py:483] Algo bellman_ford step 6809 current loss 0.716280, current_train_items 217920.
I0302 19:01:33.926699 22535416901760 run.py:483] Algo bellman_ford step 6810 current loss 0.351971, current_train_items 217952.
I0302 19:01:33.942412 22535416901760 run.py:483] Algo bellman_ford step 6811 current loss 0.374188, current_train_items 217984.
I0302 19:01:33.967034 22535416901760 run.py:483] Algo bellman_ford step 6812 current loss 0.639493, current_train_items 218016.
I0302 19:01:33.997291 22535416901760 run.py:483] Algo bellman_ford step 6813 current loss 0.694373, current_train_items 218048.
I0302 19:01:34.028501 22535416901760 run.py:483] Algo bellman_ford step 6814 current loss 0.614262, current_train_items 218080.
I0302 19:01:34.048299 22535416901760 run.py:483] Algo bellman_ford step 6815 current loss 0.337908, current_train_items 218112.
I0302 19:01:34.064412 22535416901760 run.py:483] Algo bellman_ford step 6816 current loss 0.403926, current_train_items 218144.
I0302 19:01:34.086665 22535416901760 run.py:483] Algo bellman_ford step 6817 current loss 0.529365, current_train_items 218176.
I0302 19:01:34.118027 22535416901760 run.py:483] Algo bellman_ford step 6818 current loss 0.609542, current_train_items 218208.
I0302 19:01:34.152899 22535416901760 run.py:483] Algo bellman_ford step 6819 current loss 0.822953, current_train_items 218240.
I0302 19:01:34.172373 22535416901760 run.py:483] Algo bellman_ford step 6820 current loss 0.272730, current_train_items 218272.
I0302 19:01:34.188291 22535416901760 run.py:483] Algo bellman_ford step 6821 current loss 0.424333, current_train_items 218304.
I0302 19:01:34.211338 22535416901760 run.py:483] Algo bellman_ford step 6822 current loss 0.598553, current_train_items 218336.
I0302 19:01:34.242100 22535416901760 run.py:483] Algo bellman_ford step 6823 current loss 0.720830, current_train_items 218368.
I0302 19:01:34.276561 22535416901760 run.py:483] Algo bellman_ford step 6824 current loss 0.762654, current_train_items 218400.
I0302 19:01:34.295943 22535416901760 run.py:483] Algo bellman_ford step 6825 current loss 0.278731, current_train_items 218432.
I0302 19:01:34.312094 22535416901760 run.py:483] Algo bellman_ford step 6826 current loss 0.417267, current_train_items 218464.
I0302 19:01:34.335276 22535416901760 run.py:483] Algo bellman_ford step 6827 current loss 0.559739, current_train_items 218496.
I0302 19:01:34.365401 22535416901760 run.py:483] Algo bellman_ford step 6828 current loss 0.584840, current_train_items 218528.
I0302 19:01:34.400591 22535416901760 run.py:483] Algo bellman_ford step 6829 current loss 0.818882, current_train_items 218560.
I0302 19:01:34.420027 22535416901760 run.py:483] Algo bellman_ford step 6830 current loss 0.323229, current_train_items 218592.
I0302 19:01:34.435832 22535416901760 run.py:483] Algo bellman_ford step 6831 current loss 0.451083, current_train_items 218624.
I0302 19:01:34.460673 22535416901760 run.py:483] Algo bellman_ford step 6832 current loss 0.686625, current_train_items 218656.
I0302 19:01:34.490949 22535416901760 run.py:483] Algo bellman_ford step 6833 current loss 0.628988, current_train_items 218688.
I0302 19:01:34.524506 22535416901760 run.py:483] Algo bellman_ford step 6834 current loss 0.653324, current_train_items 218720.
I0302 19:01:34.543947 22535416901760 run.py:483] Algo bellman_ford step 6835 current loss 0.277144, current_train_items 218752.
I0302 19:01:34.560120 22535416901760 run.py:483] Algo bellman_ford step 6836 current loss 0.430506, current_train_items 218784.
I0302 19:01:34.583119 22535416901760 run.py:483] Algo bellman_ford step 6837 current loss 0.577488, current_train_items 218816.
I0302 19:01:34.614680 22535416901760 run.py:483] Algo bellman_ford step 6838 current loss 0.681322, current_train_items 218848.
I0302 19:01:34.646252 22535416901760 run.py:483] Algo bellman_ford step 6839 current loss 0.731552, current_train_items 218880.
I0302 19:01:34.665438 22535416901760 run.py:483] Algo bellman_ford step 6840 current loss 0.255117, current_train_items 218912.
I0302 19:01:34.681865 22535416901760 run.py:483] Algo bellman_ford step 6841 current loss 0.501415, current_train_items 218944.
I0302 19:01:34.706184 22535416901760 run.py:483] Algo bellman_ford step 6842 current loss 0.568763, current_train_items 218976.
I0302 19:01:34.736034 22535416901760 run.py:483] Algo bellman_ford step 6843 current loss 0.558320, current_train_items 219008.
I0302 19:01:34.769136 22535416901760 run.py:483] Algo bellman_ford step 6844 current loss 0.818441, current_train_items 219040.
I0302 19:01:34.788343 22535416901760 run.py:483] Algo bellman_ford step 6845 current loss 0.326448, current_train_items 219072.
I0302 19:01:34.804931 22535416901760 run.py:483] Algo bellman_ford step 6846 current loss 0.411954, current_train_items 219104.
I0302 19:01:34.829252 22535416901760 run.py:483] Algo bellman_ford step 6847 current loss 0.601151, current_train_items 219136.
I0302 19:01:34.860770 22535416901760 run.py:483] Algo bellman_ford step 6848 current loss 0.629280, current_train_items 219168.
I0302 19:01:34.894488 22535416901760 run.py:483] Algo bellman_ford step 6849 current loss 0.696253, current_train_items 219200.
I0302 19:01:34.914122 22535416901760 run.py:483] Algo bellman_ford step 6850 current loss 0.326254, current_train_items 219232.
I0302 19:01:34.922315 22535416901760 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.9033203125, 'score': 0.9033203125, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:01:34.922417 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.903, val scores are: bellman_ford: 0.903
I0302 19:01:34.939220 22535416901760 run.py:483] Algo bellman_ford step 6851 current loss 0.470565, current_train_items 219264.
I0302 19:01:34.963340 22535416901760 run.py:483] Algo bellman_ford step 6852 current loss 0.647012, current_train_items 219296.
I0302 19:01:34.994376 22535416901760 run.py:483] Algo bellman_ford step 6853 current loss 0.615186, current_train_items 219328.
I0302 19:01:35.028725 22535416901760 run.py:483] Algo bellman_ford step 6854 current loss 0.727679, current_train_items 219360.
I0302 19:01:35.048594 22535416901760 run.py:483] Algo bellman_ford step 6855 current loss 0.336662, current_train_items 219392.
I0302 19:01:35.064262 22535416901760 run.py:483] Algo bellman_ford step 6856 current loss 0.406683, current_train_items 219424.
I0302 19:01:35.088094 22535416901760 run.py:483] Algo bellman_ford step 6857 current loss 0.611221, current_train_items 219456.
I0302 19:01:35.118095 22535416901760 run.py:483] Algo bellman_ford step 6858 current loss 0.623352, current_train_items 219488.
I0302 19:01:35.151288 22535416901760 run.py:483] Algo bellman_ford step 6859 current loss 0.750288, current_train_items 219520.
I0302 19:01:35.170906 22535416901760 run.py:483] Algo bellman_ford step 6860 current loss 0.348581, current_train_items 219552.
I0302 19:01:35.186918 22535416901760 run.py:483] Algo bellman_ford step 6861 current loss 0.468121, current_train_items 219584.
I0302 19:01:35.209615 22535416901760 run.py:483] Algo bellman_ford step 6862 current loss 0.512959, current_train_items 219616.
I0302 19:01:35.240142 22535416901760 run.py:483] Algo bellman_ford step 6863 current loss 0.532347, current_train_items 219648.
I0302 19:01:35.272101 22535416901760 run.py:483] Algo bellman_ford step 6864 current loss 0.703475, current_train_items 219680.
I0302 19:01:35.291854 22535416901760 run.py:483] Algo bellman_ford step 6865 current loss 0.286307, current_train_items 219712.
I0302 19:01:35.307935 22535416901760 run.py:483] Algo bellman_ford step 6866 current loss 0.381480, current_train_items 219744.
I0302 19:01:35.332194 22535416901760 run.py:483] Algo bellman_ford step 6867 current loss 0.593905, current_train_items 219776.
I0302 19:01:35.362954 22535416901760 run.py:483] Algo bellman_ford step 6868 current loss 0.695163, current_train_items 219808.
I0302 19:01:35.398261 22535416901760 run.py:483] Algo bellman_ford step 6869 current loss 0.824374, current_train_items 219840.
I0302 19:01:35.418183 22535416901760 run.py:483] Algo bellman_ford step 6870 current loss 0.339936, current_train_items 219872.
I0302 19:01:35.434489 22535416901760 run.py:483] Algo bellman_ford step 6871 current loss 0.466212, current_train_items 219904.
I0302 19:01:35.457400 22535416901760 run.py:483] Algo bellman_ford step 6872 current loss 0.570542, current_train_items 219936.
I0302 19:01:35.486896 22535416901760 run.py:483] Algo bellman_ford step 6873 current loss 0.579856, current_train_items 219968.
I0302 19:01:35.519994 22535416901760 run.py:483] Algo bellman_ford step 6874 current loss 0.670822, current_train_items 220000.
I0302 19:01:35.539759 22535416901760 run.py:483] Algo bellman_ford step 6875 current loss 0.320257, current_train_items 220032.
I0302 19:01:35.555504 22535416901760 run.py:483] Algo bellman_ford step 6876 current loss 0.417513, current_train_items 220064.
I0302 19:01:35.578670 22535416901760 run.py:483] Algo bellman_ford step 6877 current loss 0.643562, current_train_items 220096.
I0302 19:01:35.609573 22535416901760 run.py:483] Algo bellman_ford step 6878 current loss 0.622343, current_train_items 220128.
I0302 19:01:35.643489 22535416901760 run.py:483] Algo bellman_ford step 6879 current loss 0.764019, current_train_items 220160.
I0302 19:01:35.663286 22535416901760 run.py:483] Algo bellman_ford step 6880 current loss 0.319408, current_train_items 220192.
I0302 19:01:35.679150 22535416901760 run.py:483] Algo bellman_ford step 6881 current loss 0.485867, current_train_items 220224.
I0302 19:01:35.703269 22535416901760 run.py:483] Algo bellman_ford step 6882 current loss 0.596398, current_train_items 220256.
I0302 19:01:35.732700 22535416901760 run.py:483] Algo bellman_ford step 6883 current loss 0.580373, current_train_items 220288.
I0302 19:01:35.765968 22535416901760 run.py:483] Algo bellman_ford step 6884 current loss 0.697809, current_train_items 220320.
I0302 19:01:35.786116 22535416901760 run.py:483] Algo bellman_ford step 6885 current loss 0.298419, current_train_items 220352.
I0302 19:01:35.802416 22535416901760 run.py:483] Algo bellman_ford step 6886 current loss 0.457298, current_train_items 220384.
I0302 19:01:35.826133 22535416901760 run.py:483] Algo bellman_ford step 6887 current loss 0.656408, current_train_items 220416.
I0302 19:01:35.857960 22535416901760 run.py:483] Algo bellman_ford step 6888 current loss 0.661231, current_train_items 220448.
I0302 19:01:35.891520 22535416901760 run.py:483] Algo bellman_ford step 6889 current loss 0.672841, current_train_items 220480.
I0302 19:01:35.911078 22535416901760 run.py:483] Algo bellman_ford step 6890 current loss 0.323424, current_train_items 220512.
I0302 19:01:35.927594 22535416901760 run.py:483] Algo bellman_ford step 6891 current loss 0.484163, current_train_items 220544.
I0302 19:01:35.951038 22535416901760 run.py:483] Algo bellman_ford step 6892 current loss 0.613240, current_train_items 220576.
I0302 19:01:35.982597 22535416901760 run.py:483] Algo bellman_ford step 6893 current loss 0.730929, current_train_items 220608.
I0302 19:01:36.014713 22535416901760 run.py:483] Algo bellman_ford step 6894 current loss 0.812747, current_train_items 220640.
I0302 19:01:36.034226 22535416901760 run.py:483] Algo bellman_ford step 6895 current loss 0.291439, current_train_items 220672.
I0302 19:01:36.050165 22535416901760 run.py:483] Algo bellman_ford step 6896 current loss 0.426989, current_train_items 220704.
I0302 19:01:36.074450 22535416901760 run.py:483] Algo bellman_ford step 6897 current loss 0.627572, current_train_items 220736.
I0302 19:01:36.104367 22535416901760 run.py:483] Algo bellman_ford step 6898 current loss 0.502416, current_train_items 220768.
I0302 19:01:36.138198 22535416901760 run.py:483] Algo bellman_ford step 6899 current loss 0.706083, current_train_items 220800.
I0302 19:01:36.157992 22535416901760 run.py:483] Algo bellman_ford step 6900 current loss 0.327186, current_train_items 220832.
I0302 19:01:36.165722 22535416901760 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.9287109375, 'score': 0.9287109375, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:01:36.165828 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.929, val scores are: bellman_ford: 0.929
I0302 19:01:36.182203 22535416901760 run.py:483] Algo bellman_ford step 6901 current loss 0.388896, current_train_items 220864.
I0302 19:01:36.205723 22535416901760 run.py:483] Algo bellman_ford step 6902 current loss 0.610589, current_train_items 220896.
I0302 19:01:36.237797 22535416901760 run.py:483] Algo bellman_ford step 6903 current loss 0.673795, current_train_items 220928.
I0302 19:01:36.273116 22535416901760 run.py:483] Algo bellman_ford step 6904 current loss 0.617779, current_train_items 220960.
I0302 19:01:36.292677 22535416901760 run.py:483] Algo bellman_ford step 6905 current loss 0.350637, current_train_items 220992.
I0302 19:01:36.308143 22535416901760 run.py:483] Algo bellman_ford step 6906 current loss 0.386387, current_train_items 221024.
I0302 19:01:36.331429 22535416901760 run.py:483] Algo bellman_ford step 6907 current loss 0.584613, current_train_items 221056.
I0302 19:01:36.363019 22535416901760 run.py:483] Algo bellman_ford step 6908 current loss 0.720364, current_train_items 221088.
I0302 19:01:36.397715 22535416901760 run.py:483] Algo bellman_ford step 6909 current loss 0.797498, current_train_items 221120.
I0302 19:01:36.417062 22535416901760 run.py:483] Algo bellman_ford step 6910 current loss 0.289972, current_train_items 221152.
I0302 19:01:36.433727 22535416901760 run.py:483] Algo bellman_ford step 6911 current loss 0.574604, current_train_items 221184.
I0302 19:01:36.455932 22535416901760 run.py:483] Algo bellman_ford step 6912 current loss 0.592622, current_train_items 221216.
I0302 19:01:36.487598 22535416901760 run.py:483] Algo bellman_ford step 6913 current loss 0.658231, current_train_items 221248.
I0302 19:01:36.522084 22535416901760 run.py:483] Algo bellman_ford step 6914 current loss 0.754429, current_train_items 221280.
I0302 19:01:36.541336 22535416901760 run.py:483] Algo bellman_ford step 6915 current loss 0.382308, current_train_items 221312.
I0302 19:01:36.556854 22535416901760 run.py:483] Algo bellman_ford step 6916 current loss 0.419301, current_train_items 221344.
I0302 19:01:36.581066 22535416901760 run.py:483] Algo bellman_ford step 6917 current loss 0.691129, current_train_items 221376.
I0302 19:01:36.612322 22535416901760 run.py:483] Algo bellman_ford step 6918 current loss 0.706182, current_train_items 221408.
I0302 19:01:36.647257 22535416901760 run.py:483] Algo bellman_ford step 6919 current loss 0.681250, current_train_items 221440.
I0302 19:01:36.666496 22535416901760 run.py:483] Algo bellman_ford step 6920 current loss 0.296798, current_train_items 221472.
I0302 19:01:36.682519 22535416901760 run.py:483] Algo bellman_ford step 6921 current loss 0.507322, current_train_items 221504.
I0302 19:01:36.705227 22535416901760 run.py:483] Algo bellman_ford step 6922 current loss 0.597992, current_train_items 221536.
I0302 19:01:36.736239 22535416901760 run.py:483] Algo bellman_ford step 6923 current loss 0.645626, current_train_items 221568.
I0302 19:01:36.769354 22535416901760 run.py:483] Algo bellman_ford step 6924 current loss 0.647987, current_train_items 221600.
I0302 19:01:36.788723 22535416901760 run.py:483] Algo bellman_ford step 6925 current loss 0.279789, current_train_items 221632.
I0302 19:01:36.804644 22535416901760 run.py:483] Algo bellman_ford step 6926 current loss 0.355782, current_train_items 221664.
I0302 19:01:36.828576 22535416901760 run.py:483] Algo bellman_ford step 6927 current loss 0.608749, current_train_items 221696.
I0302 19:01:36.859607 22535416901760 run.py:483] Algo bellman_ford step 6928 current loss 0.666504, current_train_items 221728.
I0302 19:01:36.892470 22535416901760 run.py:483] Algo bellman_ford step 6929 current loss 0.773443, current_train_items 221760.
I0302 19:01:36.911973 22535416901760 run.py:483] Algo bellman_ford step 6930 current loss 0.298255, current_train_items 221792.
I0302 19:01:36.928318 22535416901760 run.py:483] Algo bellman_ford step 6931 current loss 0.458038, current_train_items 221824.
I0302 19:01:36.951112 22535416901760 run.py:483] Algo bellman_ford step 6932 current loss 0.600590, current_train_items 221856.
I0302 19:01:36.982116 22535416901760 run.py:483] Algo bellman_ford step 6933 current loss 0.762090, current_train_items 221888.
I0302 19:01:37.016664 22535416901760 run.py:483] Algo bellman_ford step 6934 current loss 0.843894, current_train_items 221920.
I0302 19:01:37.035986 22535416901760 run.py:483] Algo bellman_ford step 6935 current loss 0.330240, current_train_items 221952.
I0302 19:01:37.051832 22535416901760 run.py:483] Algo bellman_ford step 6936 current loss 0.477856, current_train_items 221984.
I0302 19:01:37.074947 22535416901760 run.py:483] Algo bellman_ford step 6937 current loss 0.534796, current_train_items 222016.
I0302 19:01:37.105671 22535416901760 run.py:483] Algo bellman_ford step 6938 current loss 0.615370, current_train_items 222048.
I0302 19:01:37.137968 22535416901760 run.py:483] Algo bellman_ford step 6939 current loss 0.731660, current_train_items 222080.
I0302 19:01:37.157376 22535416901760 run.py:483] Algo bellman_ford step 6940 current loss 0.307963, current_train_items 222112.
I0302 19:01:37.173826 22535416901760 run.py:483] Algo bellman_ford step 6941 current loss 0.637882, current_train_items 222144.
I0302 19:01:37.197622 22535416901760 run.py:483] Algo bellman_ford step 6942 current loss 0.654187, current_train_items 222176.
I0302 19:01:37.228736 22535416901760 run.py:483] Algo bellman_ford step 6943 current loss 0.619615, current_train_items 222208.
I0302 19:01:37.262926 22535416901760 run.py:483] Algo bellman_ford step 6944 current loss 0.900885, current_train_items 222240.
I0302 19:01:37.282356 22535416901760 run.py:483] Algo bellman_ford step 6945 current loss 0.297674, current_train_items 222272.
I0302 19:01:37.298687 22535416901760 run.py:483] Algo bellman_ford step 6946 current loss 0.537216, current_train_items 222304.
I0302 19:01:37.322266 22535416901760 run.py:483] Algo bellman_ford step 6947 current loss 0.667965, current_train_items 222336.
I0302 19:01:37.351241 22535416901760 run.py:483] Algo bellman_ford step 6948 current loss 0.579021, current_train_items 222368.
I0302 19:01:37.385188 22535416901760 run.py:483] Algo bellman_ford step 6949 current loss 0.792744, current_train_items 222400.
I0302 19:01:37.404551 22535416901760 run.py:483] Algo bellman_ford step 6950 current loss 0.369308, current_train_items 222432.
I0302 19:01:37.412617 22535416901760 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:01:37.412724 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:01:37.430041 22535416901760 run.py:483] Algo bellman_ford step 6951 current loss 0.505741, current_train_items 222464.
I0302 19:01:37.454477 22535416901760 run.py:483] Algo bellman_ford step 6952 current loss 0.655256, current_train_items 222496.
I0302 19:01:37.484997 22535416901760 run.py:483] Algo bellman_ford step 6953 current loss 0.661614, current_train_items 222528.
I0302 19:01:37.520009 22535416901760 run.py:483] Algo bellman_ford step 6954 current loss 0.803508, current_train_items 222560.
I0302 19:01:37.540540 22535416901760 run.py:483] Algo bellman_ford step 6955 current loss 0.363233, current_train_items 222592.
I0302 19:01:37.556578 22535416901760 run.py:483] Algo bellman_ford step 6956 current loss 0.440104, current_train_items 222624.
I0302 19:01:37.581526 22535416901760 run.py:483] Algo bellman_ford step 6957 current loss 0.711321, current_train_items 222656.
I0302 19:01:37.612415 22535416901760 run.py:483] Algo bellman_ford step 6958 current loss 0.720961, current_train_items 222688.
I0302 19:01:37.644395 22535416901760 run.py:483] Algo bellman_ford step 6959 current loss 0.723869, current_train_items 222720.
I0302 19:01:37.664469 22535416901760 run.py:483] Algo bellman_ford step 6960 current loss 0.295866, current_train_items 222752.
I0302 19:01:37.681090 22535416901760 run.py:483] Algo bellman_ford step 6961 current loss 0.440000, current_train_items 222784.
I0302 19:01:37.703973 22535416901760 run.py:483] Algo bellman_ford step 6962 current loss 0.543683, current_train_items 222816.
I0302 19:01:37.734967 22535416901760 run.py:483] Algo bellman_ford step 6963 current loss 0.754478, current_train_items 222848.
I0302 19:01:37.769743 22535416901760 run.py:483] Algo bellman_ford step 6964 current loss 0.753419, current_train_items 222880.
I0302 19:01:37.789697 22535416901760 run.py:483] Algo bellman_ford step 6965 current loss 0.327915, current_train_items 222912.
I0302 19:01:37.806000 22535416901760 run.py:483] Algo bellman_ford step 6966 current loss 0.520351, current_train_items 222944.
I0302 19:01:37.829753 22535416901760 run.py:483] Algo bellman_ford step 6967 current loss 0.528386, current_train_items 222976.
I0302 19:01:37.860054 22535416901760 run.py:483] Algo bellman_ford step 6968 current loss 0.590172, current_train_items 223008.
I0302 19:01:37.893433 22535416901760 run.py:483] Algo bellman_ford step 6969 current loss 0.708282, current_train_items 223040.
I0302 19:01:37.913102 22535416901760 run.py:483] Algo bellman_ford step 6970 current loss 0.263552, current_train_items 223072.
I0302 19:01:37.929374 22535416901760 run.py:483] Algo bellman_ford step 6971 current loss 0.398881, current_train_items 223104.
I0302 19:01:37.952585 22535416901760 run.py:483] Algo bellman_ford step 6972 current loss 0.648555, current_train_items 223136.
I0302 19:01:37.983288 22535416901760 run.py:483] Algo bellman_ford step 6973 current loss 0.642546, current_train_items 223168.
I0302 19:01:38.017430 22535416901760 run.py:483] Algo bellman_ford step 6974 current loss 0.771751, current_train_items 223200.
I0302 19:01:38.037004 22535416901760 run.py:483] Algo bellman_ford step 6975 current loss 0.335263, current_train_items 223232.
I0302 19:01:38.053424 22535416901760 run.py:483] Algo bellman_ford step 6976 current loss 0.455800, current_train_items 223264.
I0302 19:01:38.076703 22535416901760 run.py:483] Algo bellman_ford step 6977 current loss 0.576418, current_train_items 223296.
I0302 19:01:38.106673 22535416901760 run.py:483] Algo bellman_ford step 6978 current loss 0.666816, current_train_items 223328.
I0302 19:01:38.140716 22535416901760 run.py:483] Algo bellman_ford step 6979 current loss 0.699678, current_train_items 223360.
I0302 19:01:38.160403 22535416901760 run.py:483] Algo bellman_ford step 6980 current loss 0.380698, current_train_items 223392.
I0302 19:01:38.177227 22535416901760 run.py:483] Algo bellman_ford step 6981 current loss 0.513856, current_train_items 223424.
I0302 19:01:38.200085 22535416901760 run.py:483] Algo bellman_ford step 6982 current loss 0.550669, current_train_items 223456.
I0302 19:01:38.231912 22535416901760 run.py:483] Algo bellman_ford step 6983 current loss 0.716495, current_train_items 223488.
I0302 19:01:38.265601 22535416901760 run.py:483] Algo bellman_ford step 6984 current loss 0.765187, current_train_items 223520.
I0302 19:01:38.285522 22535416901760 run.py:483] Algo bellman_ford step 6985 current loss 0.336649, current_train_items 223552.
I0302 19:01:38.301532 22535416901760 run.py:483] Algo bellman_ford step 6986 current loss 0.383813, current_train_items 223584.
I0302 19:01:38.324580 22535416901760 run.py:483] Algo bellman_ford step 6987 current loss 0.563607, current_train_items 223616.
I0302 19:01:38.355626 22535416901760 run.py:483] Algo bellman_ford step 6988 current loss 0.605028, current_train_items 223648.
I0302 19:01:38.390618 22535416901760 run.py:483] Algo bellman_ford step 6989 current loss 0.738071, current_train_items 223680.
I0302 19:01:38.410230 22535416901760 run.py:483] Algo bellman_ford step 6990 current loss 0.276350, current_train_items 223712.
I0302 19:01:38.426323 22535416901760 run.py:483] Algo bellman_ford step 6991 current loss 0.450377, current_train_items 223744.
I0302 19:01:38.449338 22535416901760 run.py:483] Algo bellman_ford step 6992 current loss 0.609441, current_train_items 223776.
I0302 19:01:38.480981 22535416901760 run.py:483] Algo bellman_ford step 6993 current loss 0.708601, current_train_items 223808.
I0302 19:01:38.517095 22535416901760 run.py:483] Algo bellman_ford step 6994 current loss 0.718747, current_train_items 223840.
I0302 19:01:38.536960 22535416901760 run.py:483] Algo bellman_ford step 6995 current loss 0.331732, current_train_items 223872.
I0302 19:01:38.553417 22535416901760 run.py:483] Algo bellman_ford step 6996 current loss 0.415969, current_train_items 223904.
I0302 19:01:38.575318 22535416901760 run.py:483] Algo bellman_ford step 6997 current loss 0.497359, current_train_items 223936.
I0302 19:01:38.605549 22535416901760 run.py:483] Algo bellman_ford step 6998 current loss 0.558530, current_train_items 223968.
I0302 19:01:38.639071 22535416901760 run.py:483] Algo bellman_ford step 6999 current loss 0.764935, current_train_items 224000.
I0302 19:01:38.658902 22535416901760 run.py:483] Algo bellman_ford step 7000 current loss 0.295190, current_train_items 224032.
I0302 19:01:38.666760 22535416901760 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.89453125, 'score': 0.89453125, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:01:38.666867 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.895, val scores are: bellman_ford: 0.895
I0302 19:01:38.683469 22535416901760 run.py:483] Algo bellman_ford step 7001 current loss 0.453807, current_train_items 224064.
I0302 19:01:38.706300 22535416901760 run.py:483] Algo bellman_ford step 7002 current loss 0.598005, current_train_items 224096.
I0302 19:01:38.735814 22535416901760 run.py:483] Algo bellman_ford step 7003 current loss 0.595756, current_train_items 224128.
I0302 19:01:38.768520 22535416901760 run.py:483] Algo bellman_ford step 7004 current loss 0.839330, current_train_items 224160.
I0302 19:01:38.788285 22535416901760 run.py:483] Algo bellman_ford step 7005 current loss 0.276102, current_train_items 224192.
I0302 19:01:38.804378 22535416901760 run.py:483] Algo bellman_ford step 7006 current loss 0.482585, current_train_items 224224.
I0302 19:01:38.828348 22535416901760 run.py:483] Algo bellman_ford step 7007 current loss 0.569348, current_train_items 224256.
I0302 19:01:38.857548 22535416901760 run.py:483] Algo bellman_ford step 7008 current loss 0.639190, current_train_items 224288.
I0302 19:01:38.892917 22535416901760 run.py:483] Algo bellman_ford step 7009 current loss 0.885941, current_train_items 224320.
I0302 19:01:38.912480 22535416901760 run.py:483] Algo bellman_ford step 7010 current loss 0.341841, current_train_items 224352.
I0302 19:01:38.928646 22535416901760 run.py:483] Algo bellman_ford step 7011 current loss 0.488584, current_train_items 224384.
I0302 19:01:38.951661 22535416901760 run.py:483] Algo bellman_ford step 7012 current loss 0.576009, current_train_items 224416.
I0302 19:01:38.981600 22535416901760 run.py:483] Algo bellman_ford step 7013 current loss 0.615291, current_train_items 224448.
I0302 19:01:39.014924 22535416901760 run.py:483] Algo bellman_ford step 7014 current loss 0.706943, current_train_items 224480.
I0302 19:01:39.034419 22535416901760 run.py:483] Algo bellman_ford step 7015 current loss 0.283375, current_train_items 224512.
I0302 19:01:39.050362 22535416901760 run.py:483] Algo bellman_ford step 7016 current loss 0.419602, current_train_items 224544.
I0302 19:01:39.073743 22535416901760 run.py:483] Algo bellman_ford step 7017 current loss 0.500962, current_train_items 224576.
I0302 19:01:39.103299 22535416901760 run.py:483] Algo bellman_ford step 7018 current loss 0.580275, current_train_items 224608.
I0302 19:01:39.135713 22535416901760 run.py:483] Algo bellman_ford step 7019 current loss 0.668273, current_train_items 224640.
I0302 19:01:39.154824 22535416901760 run.py:483] Algo bellman_ford step 7020 current loss 0.292293, current_train_items 224672.
I0302 19:01:39.170747 22535416901760 run.py:483] Algo bellman_ford step 7021 current loss 0.534866, current_train_items 224704.
I0302 19:01:39.193480 22535416901760 run.py:483] Algo bellman_ford step 7022 current loss 0.516864, current_train_items 224736.
I0302 19:01:39.223081 22535416901760 run.py:483] Algo bellman_ford step 7023 current loss 0.563127, current_train_items 224768.
I0302 19:01:39.256639 22535416901760 run.py:483] Algo bellman_ford step 7024 current loss 0.714808, current_train_items 224800.
I0302 19:01:39.276015 22535416901760 run.py:483] Algo bellman_ford step 7025 current loss 0.336466, current_train_items 224832.
I0302 19:01:39.292375 22535416901760 run.py:483] Algo bellman_ford step 7026 current loss 0.432837, current_train_items 224864.
I0302 19:01:39.317059 22535416901760 run.py:483] Algo bellman_ford step 7027 current loss 0.748685, current_train_items 224896.
I0302 19:01:39.349512 22535416901760 run.py:483] Algo bellman_ford step 7028 current loss 0.730530, current_train_items 224928.
I0302 19:01:39.380934 22535416901760 run.py:483] Algo bellman_ford step 7029 current loss 0.656372, current_train_items 224960.
I0302 19:01:39.400295 22535416901760 run.py:483] Algo bellman_ford step 7030 current loss 0.346956, current_train_items 224992.
I0302 19:01:39.416383 22535416901760 run.py:483] Algo bellman_ford step 7031 current loss 0.427032, current_train_items 225024.
I0302 19:01:39.439135 22535416901760 run.py:483] Algo bellman_ford step 7032 current loss 0.563229, current_train_items 225056.
I0302 19:01:39.469339 22535416901760 run.py:483] Algo bellman_ford step 7033 current loss 0.657628, current_train_items 225088.
I0302 19:01:39.504078 22535416901760 run.py:483] Algo bellman_ford step 7034 current loss 0.727596, current_train_items 225120.
I0302 19:01:39.523627 22535416901760 run.py:483] Algo bellman_ford step 7035 current loss 0.234432, current_train_items 225152.
I0302 19:01:39.539755 22535416901760 run.py:483] Algo bellman_ford step 7036 current loss 0.479690, current_train_items 225184.
I0302 19:01:39.562862 22535416901760 run.py:483] Algo bellman_ford step 7037 current loss 0.523532, current_train_items 225216.
I0302 19:01:39.594016 22535416901760 run.py:483] Algo bellman_ford step 7038 current loss 0.639076, current_train_items 225248.
I0302 19:01:39.626404 22535416901760 run.py:483] Algo bellman_ford step 7039 current loss 0.692541, current_train_items 225280.
I0302 19:01:39.645925 22535416901760 run.py:483] Algo bellman_ford step 7040 current loss 0.288108, current_train_items 225312.
I0302 19:01:39.662032 22535416901760 run.py:483] Algo bellman_ford step 7041 current loss 0.452526, current_train_items 225344.
I0302 19:01:39.685060 22535416901760 run.py:483] Algo bellman_ford step 7042 current loss 0.570660, current_train_items 225376.
I0302 19:01:39.716113 22535416901760 run.py:483] Algo bellman_ford step 7043 current loss 0.695693, current_train_items 225408.
I0302 19:01:39.749503 22535416901760 run.py:483] Algo bellman_ford step 7044 current loss 0.646288, current_train_items 225440.
I0302 19:01:39.769198 22535416901760 run.py:483] Algo bellman_ford step 7045 current loss 0.312015, current_train_items 225472.
I0302 19:01:39.785523 22535416901760 run.py:483] Algo bellman_ford step 7046 current loss 0.556470, current_train_items 225504.
I0302 19:01:39.808378 22535416901760 run.py:483] Algo bellman_ford step 7047 current loss 0.695095, current_train_items 225536.
I0302 19:01:39.839542 22535416901760 run.py:483] Algo bellman_ford step 7048 current loss 0.649436, current_train_items 225568.
I0302 19:01:39.872476 22535416901760 run.py:483] Algo bellman_ford step 7049 current loss 0.723638, current_train_items 225600.
I0302 19:01:39.891975 22535416901760 run.py:483] Algo bellman_ford step 7050 current loss 0.282817, current_train_items 225632.
I0302 19:01:39.900294 22535416901760 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:01:39.900401 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:01:39.916725 22535416901760 run.py:483] Algo bellman_ford step 7051 current loss 0.449447, current_train_items 225664.
I0302 19:01:39.941041 22535416901760 run.py:483] Algo bellman_ford step 7052 current loss 0.674067, current_train_items 225696.
I0302 19:01:39.973275 22535416901760 run.py:483] Algo bellman_ford step 7053 current loss 0.837169, current_train_items 225728.
I0302 19:01:40.007902 22535416901760 run.py:483] Algo bellman_ford step 7054 current loss 0.763556, current_train_items 225760.
I0302 19:01:40.027643 22535416901760 run.py:483] Algo bellman_ford step 7055 current loss 0.334613, current_train_items 225792.
I0302 19:01:40.043562 22535416901760 run.py:483] Algo bellman_ford step 7056 current loss 0.447623, current_train_items 225824.
I0302 19:01:40.067458 22535416901760 run.py:483] Algo bellman_ford step 7057 current loss 0.605350, current_train_items 225856.
I0302 19:01:40.098293 22535416901760 run.py:483] Algo bellman_ford step 7058 current loss 0.706303, current_train_items 225888.
I0302 19:01:40.133569 22535416901760 run.py:483] Algo bellman_ford step 7059 current loss 0.697442, current_train_items 225920.
I0302 19:01:40.153398 22535416901760 run.py:483] Algo bellman_ford step 7060 current loss 0.302883, current_train_items 225952.
I0302 19:01:40.169614 22535416901760 run.py:483] Algo bellman_ford step 7061 current loss 0.466616, current_train_items 225984.
I0302 19:01:40.193462 22535416901760 run.py:483] Algo bellman_ford step 7062 current loss 0.538244, current_train_items 226016.
I0302 19:01:40.221835 22535416901760 run.py:483] Algo bellman_ford step 7063 current loss 0.482110, current_train_items 226048.
I0302 19:01:40.255134 22535416901760 run.py:483] Algo bellman_ford step 7064 current loss 0.669649, current_train_items 226080.
I0302 19:01:40.274521 22535416901760 run.py:483] Algo bellman_ford step 7065 current loss 0.321133, current_train_items 226112.
I0302 19:01:40.290987 22535416901760 run.py:483] Algo bellman_ford step 7066 current loss 0.414873, current_train_items 226144.
I0302 19:01:40.313852 22535416901760 run.py:483] Algo bellman_ford step 7067 current loss 0.517151, current_train_items 226176.
I0302 19:01:40.346376 22535416901760 run.py:483] Algo bellman_ford step 7068 current loss 0.694219, current_train_items 226208.
I0302 19:01:40.381290 22535416901760 run.py:483] Algo bellman_ford step 7069 current loss 0.782590, current_train_items 226240.
I0302 19:01:40.401041 22535416901760 run.py:483] Algo bellman_ford step 7070 current loss 0.295759, current_train_items 226272.
I0302 19:01:40.417062 22535416901760 run.py:483] Algo bellman_ford step 7071 current loss 0.578057, current_train_items 226304.
I0302 19:01:40.438592 22535416901760 run.py:483] Algo bellman_ford step 7072 current loss 0.557577, current_train_items 226336.
I0302 19:01:40.468863 22535416901760 run.py:483] Algo bellman_ford step 7073 current loss 0.552573, current_train_items 226368.
I0302 19:01:40.502168 22535416901760 run.py:483] Algo bellman_ford step 7074 current loss 0.660402, current_train_items 226400.
I0302 19:01:40.522071 22535416901760 run.py:483] Algo bellman_ford step 7075 current loss 0.311713, current_train_items 226432.
I0302 19:01:40.537540 22535416901760 run.py:483] Algo bellman_ford step 7076 current loss 0.414389, current_train_items 226464.
I0302 19:01:40.560323 22535416901760 run.py:483] Algo bellman_ford step 7077 current loss 0.643236, current_train_items 226496.
I0302 19:01:40.591418 22535416901760 run.py:483] Algo bellman_ford step 7078 current loss 0.686708, current_train_items 226528.
I0302 19:01:40.625371 22535416901760 run.py:483] Algo bellman_ford step 7079 current loss 0.796557, current_train_items 226560.
I0302 19:01:40.644794 22535416901760 run.py:483] Algo bellman_ford step 7080 current loss 0.237858, current_train_items 226592.
I0302 19:01:40.660540 22535416901760 run.py:483] Algo bellman_ford step 7081 current loss 0.384599, current_train_items 226624.
I0302 19:01:40.683704 22535416901760 run.py:483] Algo bellman_ford step 7082 current loss 0.616520, current_train_items 226656.
I0302 19:01:40.714638 22535416901760 run.py:483] Algo bellman_ford step 7083 current loss 0.696017, current_train_items 226688.
I0302 19:01:40.747562 22535416901760 run.py:483] Algo bellman_ford step 7084 current loss 0.715925, current_train_items 226720.
I0302 19:01:40.767228 22535416901760 run.py:483] Algo bellman_ford step 7085 current loss 0.292484, current_train_items 226752.
I0302 19:01:40.783115 22535416901760 run.py:483] Algo bellman_ford step 7086 current loss 0.451875, current_train_items 226784.
I0302 19:01:40.805241 22535416901760 run.py:483] Algo bellman_ford step 7087 current loss 0.510586, current_train_items 226816.
I0302 19:01:40.836309 22535416901760 run.py:483] Algo bellman_ford step 7088 current loss 0.636081, current_train_items 226848.
I0302 19:01:40.869110 22535416901760 run.py:483] Algo bellman_ford step 7089 current loss 0.637504, current_train_items 226880.
I0302 19:01:40.889102 22535416901760 run.py:483] Algo bellman_ford step 7090 current loss 0.280567, current_train_items 226912.
I0302 19:01:40.904993 22535416901760 run.py:483] Algo bellman_ford step 7091 current loss 0.411476, current_train_items 226944.
I0302 19:01:40.928382 22535416901760 run.py:483] Algo bellman_ford step 7092 current loss 0.557604, current_train_items 226976.
I0302 19:01:40.959871 22535416901760 run.py:483] Algo bellman_ford step 7093 current loss 0.680663, current_train_items 227008.
I0302 19:01:40.991537 22535416901760 run.py:483] Algo bellman_ford step 7094 current loss 0.631571, current_train_items 227040.
I0302 19:01:41.011026 22535416901760 run.py:483] Algo bellman_ford step 7095 current loss 0.372269, current_train_items 227072.
I0302 19:01:41.027027 22535416901760 run.py:483] Algo bellman_ford step 7096 current loss 0.468580, current_train_items 227104.
I0302 19:01:41.048998 22535416901760 run.py:483] Algo bellman_ford step 7097 current loss 0.507192, current_train_items 227136.
I0302 19:01:41.080546 22535416901760 run.py:483] Algo bellman_ford step 7098 current loss 0.742512, current_train_items 227168.
I0302 19:01:41.112862 22535416901760 run.py:483] Algo bellman_ford step 7099 current loss 0.668792, current_train_items 227200.
I0302 19:01:41.133193 22535416901760 run.py:483] Algo bellman_ford step 7100 current loss 0.290682, current_train_items 227232.
I0302 19:01:41.141109 22535416901760 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.916015625, 'score': 0.916015625, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:01:41.141229 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.916, val scores are: bellman_ford: 0.916
I0302 19:01:41.157859 22535416901760 run.py:483] Algo bellman_ford step 7101 current loss 0.498958, current_train_items 227264.
I0302 19:01:41.181510 22535416901760 run.py:483] Algo bellman_ford step 7102 current loss 0.585181, current_train_items 227296.
I0302 19:01:41.213130 22535416901760 run.py:483] Algo bellman_ford step 7103 current loss 0.638453, current_train_items 227328.
I0302 19:01:41.248220 22535416901760 run.py:483] Algo bellman_ford step 7104 current loss 0.782867, current_train_items 227360.
I0302 19:01:41.267931 22535416901760 run.py:483] Algo bellman_ford step 7105 current loss 0.340226, current_train_items 227392.
I0302 19:01:41.283999 22535416901760 run.py:483] Algo bellman_ford step 7106 current loss 0.460951, current_train_items 227424.
I0302 19:01:41.307440 22535416901760 run.py:483] Algo bellman_ford step 7107 current loss 0.620301, current_train_items 227456.
I0302 19:01:41.338037 22535416901760 run.py:483] Algo bellman_ford step 7108 current loss 0.732473, current_train_items 227488.
I0302 19:01:41.373376 22535416901760 run.py:483] Algo bellman_ford step 7109 current loss 0.730467, current_train_items 227520.
I0302 19:01:41.393331 22535416901760 run.py:483] Algo bellman_ford step 7110 current loss 0.317286, current_train_items 227552.
I0302 19:01:41.409852 22535416901760 run.py:483] Algo bellman_ford step 7111 current loss 0.477006, current_train_items 227584.
I0302 19:01:41.433332 22535416901760 run.py:483] Algo bellman_ford step 7112 current loss 0.693596, current_train_items 227616.
I0302 19:01:41.464663 22535416901760 run.py:483] Algo bellman_ford step 7113 current loss 0.904758, current_train_items 227648.
I0302 19:01:41.498694 22535416901760 run.py:483] Algo bellman_ford step 7114 current loss 1.051136, current_train_items 227680.
I0302 19:01:41.518231 22535416901760 run.py:483] Algo bellman_ford step 7115 current loss 0.334222, current_train_items 227712.
I0302 19:01:41.534372 22535416901760 run.py:483] Algo bellman_ford step 7116 current loss 0.646884, current_train_items 227744.
I0302 19:01:41.557917 22535416901760 run.py:483] Algo bellman_ford step 7117 current loss 0.609018, current_train_items 227776.
I0302 19:01:41.589015 22535416901760 run.py:483] Algo bellman_ford step 7118 current loss 0.694680, current_train_items 227808.
I0302 19:01:41.622691 22535416901760 run.py:483] Algo bellman_ford step 7119 current loss 0.700334, current_train_items 227840.
I0302 19:01:41.641805 22535416901760 run.py:483] Algo bellman_ford step 7120 current loss 0.333208, current_train_items 227872.
I0302 19:01:41.657978 22535416901760 run.py:483] Algo bellman_ford step 7121 current loss 0.526474, current_train_items 227904.
I0302 19:01:41.681470 22535416901760 run.py:483] Algo bellman_ford step 7122 current loss 0.633400, current_train_items 227936.
I0302 19:01:41.711850 22535416901760 run.py:483] Algo bellman_ford step 7123 current loss 0.658608, current_train_items 227968.
I0302 19:01:41.745528 22535416901760 run.py:483] Algo bellman_ford step 7124 current loss 0.852440, current_train_items 228000.
I0302 19:01:41.764806 22535416901760 run.py:483] Algo bellman_ford step 7125 current loss 0.329392, current_train_items 228032.
I0302 19:01:41.780333 22535416901760 run.py:483] Algo bellman_ford step 7126 current loss 0.425965, current_train_items 228064.
I0302 19:01:41.803020 22535416901760 run.py:483] Algo bellman_ford step 7127 current loss 0.597740, current_train_items 228096.
I0302 19:01:41.832939 22535416901760 run.py:483] Algo bellman_ford step 7128 current loss 0.609175, current_train_items 228128.
I0302 19:01:41.867336 22535416901760 run.py:483] Algo bellman_ford step 7129 current loss 0.721145, current_train_items 228160.
I0302 19:01:41.887145 22535416901760 run.py:483] Algo bellman_ford step 7130 current loss 0.319134, current_train_items 228192.
I0302 19:01:41.903458 22535416901760 run.py:483] Algo bellman_ford step 7131 current loss 0.427824, current_train_items 228224.
I0302 19:01:41.927346 22535416901760 run.py:483] Algo bellman_ford step 7132 current loss 0.590415, current_train_items 228256.
I0302 19:01:41.958055 22535416901760 run.py:483] Algo bellman_ford step 7133 current loss 0.681783, current_train_items 228288.
I0302 19:01:41.990564 22535416901760 run.py:483] Algo bellman_ford step 7134 current loss 0.710735, current_train_items 228320.
I0302 19:01:42.010347 22535416901760 run.py:483] Algo bellman_ford step 7135 current loss 0.313979, current_train_items 228352.
I0302 19:01:42.026123 22535416901760 run.py:483] Algo bellman_ford step 7136 current loss 0.382808, current_train_items 228384.
I0302 19:01:42.048757 22535416901760 run.py:483] Algo bellman_ford step 7137 current loss 0.558448, current_train_items 228416.
I0302 19:01:42.078331 22535416901760 run.py:483] Algo bellman_ford step 7138 current loss 0.597783, current_train_items 228448.
I0302 19:01:42.114039 22535416901760 run.py:483] Algo bellman_ford step 7139 current loss 0.795370, current_train_items 228480.
I0302 19:01:42.133825 22535416901760 run.py:483] Algo bellman_ford step 7140 current loss 0.294012, current_train_items 228512.
I0302 19:01:42.149755 22535416901760 run.py:483] Algo bellman_ford step 7141 current loss 0.546217, current_train_items 228544.
I0302 19:01:42.173581 22535416901760 run.py:483] Algo bellman_ford step 7142 current loss 0.547829, current_train_items 228576.
I0302 19:01:42.202661 22535416901760 run.py:483] Algo bellman_ford step 7143 current loss 0.562460, current_train_items 228608.
I0302 19:01:42.234966 22535416901760 run.py:483] Algo bellman_ford step 7144 current loss 0.672774, current_train_items 228640.
I0302 19:01:42.254779 22535416901760 run.py:483] Algo bellman_ford step 7145 current loss 0.295417, current_train_items 228672.
I0302 19:01:42.271170 22535416901760 run.py:483] Algo bellman_ford step 7146 current loss 0.470978, current_train_items 228704.
I0302 19:01:42.295278 22535416901760 run.py:483] Algo bellman_ford step 7147 current loss 0.587427, current_train_items 228736.
I0302 19:01:42.324664 22535416901760 run.py:483] Algo bellman_ford step 7148 current loss 0.607111, current_train_items 228768.
I0302 19:01:42.355703 22535416901760 run.py:483] Algo bellman_ford step 7149 current loss 0.695382, current_train_items 228800.
I0302 19:01:42.374885 22535416901760 run.py:483] Algo bellman_ford step 7150 current loss 0.337431, current_train_items 228832.
I0302 19:01:42.383164 22535416901760 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:01:42.383269 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:42.400347 22535416901760 run.py:483] Algo bellman_ford step 7151 current loss 0.444281, current_train_items 228864.
I0302 19:01:42.424831 22535416901760 run.py:483] Algo bellman_ford step 7152 current loss 0.684743, current_train_items 228896.
I0302 19:01:42.456084 22535416901760 run.py:483] Algo bellman_ford step 7153 current loss 0.893827, current_train_items 228928.
I0302 19:01:42.489381 22535416901760 run.py:483] Algo bellman_ford step 7154 current loss 0.754686, current_train_items 228960.
I0302 19:01:42.509364 22535416901760 run.py:483] Algo bellman_ford step 7155 current loss 0.312077, current_train_items 228992.
I0302 19:01:42.525459 22535416901760 run.py:483] Algo bellman_ford step 7156 current loss 0.572235, current_train_items 229024.
I0302 19:01:42.548899 22535416901760 run.py:483] Algo bellman_ford step 7157 current loss 0.628736, current_train_items 229056.
I0302 19:01:42.579202 22535416901760 run.py:483] Algo bellman_ford step 7158 current loss 0.593337, current_train_items 229088.
I0302 19:01:42.612741 22535416901760 run.py:483] Algo bellman_ford step 7159 current loss 0.727679, current_train_items 229120.
I0302 19:01:42.632857 22535416901760 run.py:483] Algo bellman_ford step 7160 current loss 0.279169, current_train_items 229152.
I0302 19:01:42.649383 22535416901760 run.py:483] Algo bellman_ford step 7161 current loss 0.596719, current_train_items 229184.
W0302 19:01:42.663960 22535416901760 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:49.478449 22535416901760 run.py:483] Algo bellman_ford step 7162 current loss 0.679666, current_train_items 229216.
I0302 19:01:49.510538 22535416901760 run.py:483] Algo bellman_ford step 7163 current loss 0.657409, current_train_items 229248.
I0302 19:01:49.545652 22535416901760 run.py:483] Algo bellman_ford step 7164 current loss 0.795382, current_train_items 229280.
I0302 19:01:49.565400 22535416901760 run.py:483] Algo bellman_ford step 7165 current loss 0.329401, current_train_items 229312.
I0302 19:01:49.582001 22535416901760 run.py:483] Algo bellman_ford step 7166 current loss 0.446120, current_train_items 229344.
I0302 19:01:49.605968 22535416901760 run.py:483] Algo bellman_ford step 7167 current loss 0.616492, current_train_items 229376.
I0302 19:01:49.637074 22535416901760 run.py:483] Algo bellman_ford step 7168 current loss 0.559868, current_train_items 229408.
I0302 19:01:49.668050 22535416901760 run.py:483] Algo bellman_ford step 7169 current loss 0.732855, current_train_items 229440.
I0302 19:01:49.688006 22535416901760 run.py:483] Algo bellman_ford step 7170 current loss 0.317678, current_train_items 229472.
I0302 19:01:49.704761 22535416901760 run.py:483] Algo bellman_ford step 7171 current loss 0.467821, current_train_items 229504.
I0302 19:01:49.727926 22535416901760 run.py:483] Algo bellman_ford step 7172 current loss 0.551923, current_train_items 229536.
I0302 19:01:49.758957 22535416901760 run.py:483] Algo bellman_ford step 7173 current loss 0.619075, current_train_items 229568.
I0302 19:01:49.793012 22535416901760 run.py:483] Algo bellman_ford step 7174 current loss 0.714833, current_train_items 229600.
I0302 19:01:49.813073 22535416901760 run.py:483] Algo bellman_ford step 7175 current loss 0.312927, current_train_items 229632.
I0302 19:01:49.829201 22535416901760 run.py:483] Algo bellman_ford step 7176 current loss 0.535997, current_train_items 229664.
I0302 19:01:49.852534 22535416901760 run.py:483] Algo bellman_ford step 7177 current loss 0.635796, current_train_items 229696.
I0302 19:01:49.885296 22535416901760 run.py:483] Algo bellman_ford step 7178 current loss 0.649385, current_train_items 229728.
I0302 19:01:49.919577 22535416901760 run.py:483] Algo bellman_ford step 7179 current loss 0.691835, current_train_items 229760.
I0302 19:01:49.939171 22535416901760 run.py:483] Algo bellman_ford step 7180 current loss 0.278708, current_train_items 229792.
I0302 19:01:49.955220 22535416901760 run.py:483] Algo bellman_ford step 7181 current loss 0.517852, current_train_items 229824.
I0302 19:01:49.979223 22535416901760 run.py:483] Algo bellman_ford step 7182 current loss 0.700534, current_train_items 229856.
I0302 19:01:50.009761 22535416901760 run.py:483] Algo bellman_ford step 7183 current loss 0.642272, current_train_items 229888.
I0302 19:01:50.046161 22535416901760 run.py:483] Algo bellman_ford step 7184 current loss 0.889620, current_train_items 229920.
I0302 19:01:50.065786 22535416901760 run.py:483] Algo bellman_ford step 7185 current loss 0.331802, current_train_items 229952.
I0302 19:01:50.081872 22535416901760 run.py:483] Algo bellman_ford step 7186 current loss 0.530393, current_train_items 229984.
I0302 19:01:50.105904 22535416901760 run.py:483] Algo bellman_ford step 7187 current loss 0.610515, current_train_items 230016.
I0302 19:01:50.137097 22535416901760 run.py:483] Algo bellman_ford step 7188 current loss 0.637967, current_train_items 230048.
I0302 19:01:50.172139 22535416901760 run.py:483] Algo bellman_ford step 7189 current loss 0.748495, current_train_items 230080.
I0302 19:01:50.191904 22535416901760 run.py:483] Algo bellman_ford step 7190 current loss 0.295113, current_train_items 230112.
I0302 19:01:50.208431 22535416901760 run.py:483] Algo bellman_ford step 7191 current loss 0.424124, current_train_items 230144.
I0302 19:01:50.231731 22535416901760 run.py:483] Algo bellman_ford step 7192 current loss 0.570245, current_train_items 230176.
I0302 19:01:50.263256 22535416901760 run.py:483] Algo bellman_ford step 7193 current loss 0.586401, current_train_items 230208.
I0302 19:01:50.296168 22535416901760 run.py:483] Algo bellman_ford step 7194 current loss 0.674516, current_train_items 230240.
I0302 19:01:50.315826 22535416901760 run.py:483] Algo bellman_ford step 7195 current loss 0.275469, current_train_items 230272.
I0302 19:01:50.331695 22535416901760 run.py:483] Algo bellman_ford step 7196 current loss 0.523040, current_train_items 230304.
I0302 19:01:50.354987 22535416901760 run.py:483] Algo bellman_ford step 7197 current loss 0.558189, current_train_items 230336.
I0302 19:01:50.386503 22535416901760 run.py:483] Algo bellman_ford step 7198 current loss 0.703784, current_train_items 230368.
I0302 19:01:50.419872 22535416901760 run.py:483] Algo bellman_ford step 7199 current loss 0.734546, current_train_items 230400.
I0302 19:01:50.439771 22535416901760 run.py:483] Algo bellman_ford step 7200 current loss 0.323650, current_train_items 230432.
I0302 19:01:50.449375 22535416901760 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.9306640625, 'score': 0.9306640625, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:50.449488 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.931, val scores are: bellman_ford: 0.931
I0302 19:01:50.466399 22535416901760 run.py:483] Algo bellman_ford step 7201 current loss 0.543476, current_train_items 230464.
I0302 19:01:50.490093 22535416901760 run.py:483] Algo bellman_ford step 7202 current loss 0.625872, current_train_items 230496.
I0302 19:01:50.522047 22535416901760 run.py:483] Algo bellman_ford step 7203 current loss 0.682681, current_train_items 230528.
I0302 19:01:50.555514 22535416901760 run.py:483] Algo bellman_ford step 7204 current loss 0.677217, current_train_items 230560.
I0302 19:01:50.575860 22535416901760 run.py:483] Algo bellman_ford step 7205 current loss 0.329418, current_train_items 230592.
I0302 19:01:50.591559 22535416901760 run.py:483] Algo bellman_ford step 7206 current loss 0.430299, current_train_items 230624.
I0302 19:01:50.615301 22535416901760 run.py:483] Algo bellman_ford step 7207 current loss 0.573374, current_train_items 230656.
I0302 19:01:50.646391 22535416901760 run.py:483] Algo bellman_ford step 7208 current loss 0.623707, current_train_items 230688.
I0302 19:01:50.681021 22535416901760 run.py:483] Algo bellman_ford step 7209 current loss 0.771760, current_train_items 230720.
I0302 19:01:50.700924 22535416901760 run.py:483] Algo bellman_ford step 7210 current loss 0.327827, current_train_items 230752.
I0302 19:01:50.716840 22535416901760 run.py:483] Algo bellman_ford step 7211 current loss 0.411298, current_train_items 230784.
I0302 19:01:50.739886 22535416901760 run.py:483] Algo bellman_ford step 7212 current loss 0.619099, current_train_items 230816.
I0302 19:01:50.770935 22535416901760 run.py:483] Algo bellman_ford step 7213 current loss 0.587418, current_train_items 230848.
I0302 19:01:50.802623 22535416901760 run.py:483] Algo bellman_ford step 7214 current loss 0.771659, current_train_items 230880.
I0302 19:01:50.822232 22535416901760 run.py:483] Algo bellman_ford step 7215 current loss 0.384930, current_train_items 230912.
I0302 19:01:50.838344 22535416901760 run.py:483] Algo bellman_ford step 7216 current loss 0.463236, current_train_items 230944.
I0302 19:01:50.862382 22535416901760 run.py:483] Algo bellman_ford step 7217 current loss 0.576495, current_train_items 230976.
I0302 19:01:50.893668 22535416901760 run.py:483] Algo bellman_ford step 7218 current loss 0.630538, current_train_items 231008.
I0302 19:01:50.926286 22535416901760 run.py:483] Algo bellman_ford step 7219 current loss 0.655210, current_train_items 231040.
I0302 19:01:50.945723 22535416901760 run.py:483] Algo bellman_ford step 7220 current loss 0.359840, current_train_items 231072.
I0302 19:01:50.961869 22535416901760 run.py:483] Algo bellman_ford step 7221 current loss 0.537112, current_train_items 231104.
I0302 19:01:50.986225 22535416901760 run.py:483] Algo bellman_ford step 7222 current loss 0.700408, current_train_items 231136.
I0302 19:01:51.017467 22535416901760 run.py:483] Algo bellman_ford step 7223 current loss 0.654205, current_train_items 231168.
I0302 19:01:51.051678 22535416901760 run.py:483] Algo bellman_ford step 7224 current loss 0.812024, current_train_items 231200.
I0302 19:01:51.071549 22535416901760 run.py:483] Algo bellman_ford step 7225 current loss 0.274257, current_train_items 231232.
I0302 19:01:51.087765 22535416901760 run.py:483] Algo bellman_ford step 7226 current loss 0.444962, current_train_items 231264.
I0302 19:01:51.112914 22535416901760 run.py:483] Algo bellman_ford step 7227 current loss 0.658526, current_train_items 231296.
I0302 19:01:51.143912 22535416901760 run.py:483] Algo bellman_ford step 7228 current loss 0.634124, current_train_items 231328.
I0302 19:01:51.175453 22535416901760 run.py:483] Algo bellman_ford step 7229 current loss 0.805032, current_train_items 231360.
I0302 19:01:51.194907 22535416901760 run.py:483] Algo bellman_ford step 7230 current loss 0.281184, current_train_items 231392.
I0302 19:01:51.211208 22535416901760 run.py:483] Algo bellman_ford step 7231 current loss 0.435974, current_train_items 231424.
I0302 19:01:51.232927 22535416901760 run.py:483] Algo bellman_ford step 7232 current loss 0.500967, current_train_items 231456.
I0302 19:01:51.264611 22535416901760 run.py:483] Algo bellman_ford step 7233 current loss 0.661718, current_train_items 231488.
I0302 19:01:51.297095 22535416901760 run.py:483] Algo bellman_ford step 7234 current loss 0.651417, current_train_items 231520.
I0302 19:01:51.316481 22535416901760 run.py:483] Algo bellman_ford step 7235 current loss 0.271690, current_train_items 231552.
I0302 19:01:51.332928 22535416901760 run.py:483] Algo bellman_ford step 7236 current loss 0.541170, current_train_items 231584.
I0302 19:01:51.357222 22535416901760 run.py:483] Algo bellman_ford step 7237 current loss 0.535142, current_train_items 231616.
I0302 19:01:51.389966 22535416901760 run.py:483] Algo bellman_ford step 7238 current loss 0.692996, current_train_items 231648.
I0302 19:01:51.423549 22535416901760 run.py:483] Algo bellman_ford step 7239 current loss 0.674088, current_train_items 231680.
I0302 19:01:51.443198 22535416901760 run.py:483] Algo bellman_ford step 7240 current loss 0.318761, current_train_items 231712.
I0302 19:01:51.459260 22535416901760 run.py:483] Algo bellman_ford step 7241 current loss 0.445118, current_train_items 231744.
I0302 19:01:51.483801 22535416901760 run.py:483] Algo bellman_ford step 7242 current loss 0.693784, current_train_items 231776.
I0302 19:01:51.515566 22535416901760 run.py:483] Algo bellman_ford step 7243 current loss 0.640476, current_train_items 231808.
I0302 19:01:51.550642 22535416901760 run.py:483] Algo bellman_ford step 7244 current loss 0.719989, current_train_items 231840.
I0302 19:01:51.570049 22535416901760 run.py:483] Algo bellman_ford step 7245 current loss 0.302018, current_train_items 231872.
I0302 19:01:51.586098 22535416901760 run.py:483] Algo bellman_ford step 7246 current loss 0.441505, current_train_items 231904.
I0302 19:01:51.610032 22535416901760 run.py:483] Algo bellman_ford step 7247 current loss 0.607412, current_train_items 231936.
I0302 19:01:51.642905 22535416901760 run.py:483] Algo bellman_ford step 7248 current loss 0.742143, current_train_items 231968.
I0302 19:01:51.676792 22535416901760 run.py:483] Algo bellman_ford step 7249 current loss 0.726165, current_train_items 232000.
I0302 19:01:51.696433 22535416901760 run.py:483] Algo bellman_ford step 7250 current loss 0.336973, current_train_items 232032.
I0302 19:01:51.704911 22535416901760 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:51.705018 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:01:51.721743 22535416901760 run.py:483] Algo bellman_ford step 7251 current loss 0.416474, current_train_items 232064.
I0302 19:01:51.745481 22535416901760 run.py:483] Algo bellman_ford step 7252 current loss 0.554290, current_train_items 232096.
I0302 19:01:51.776066 22535416901760 run.py:483] Algo bellman_ford step 7253 current loss 0.546577, current_train_items 232128.
I0302 19:01:51.809821 22535416901760 run.py:483] Algo bellman_ford step 7254 current loss 0.671308, current_train_items 232160.
I0302 19:01:51.829722 22535416901760 run.py:483] Algo bellman_ford step 7255 current loss 0.359339, current_train_items 232192.
I0302 19:01:51.845231 22535416901760 run.py:483] Algo bellman_ford step 7256 current loss 0.440388, current_train_items 232224.
I0302 19:01:51.868699 22535416901760 run.py:483] Algo bellman_ford step 7257 current loss 0.565949, current_train_items 232256.
I0302 19:01:51.902183 22535416901760 run.py:483] Algo bellman_ford step 7258 current loss 0.666623, current_train_items 232288.
I0302 19:01:51.935892 22535416901760 run.py:483] Algo bellman_ford step 7259 current loss 0.743396, current_train_items 232320.
I0302 19:01:51.955632 22535416901760 run.py:483] Algo bellman_ford step 7260 current loss 0.286674, current_train_items 232352.
I0302 19:01:51.971856 22535416901760 run.py:483] Algo bellman_ford step 7261 current loss 0.440535, current_train_items 232384.
I0302 19:01:51.995357 22535416901760 run.py:483] Algo bellman_ford step 7262 current loss 0.552060, current_train_items 232416.
I0302 19:01:52.026770 22535416901760 run.py:483] Algo bellman_ford step 7263 current loss 0.656194, current_train_items 232448.
I0302 19:01:52.060199 22535416901760 run.py:483] Algo bellman_ford step 7264 current loss 0.688829, current_train_items 232480.
I0302 19:01:52.079479 22535416901760 run.py:483] Algo bellman_ford step 7265 current loss 0.313129, current_train_items 232512.
I0302 19:01:52.095470 22535416901760 run.py:483] Algo bellman_ford step 7266 current loss 0.499185, current_train_items 232544.
I0302 19:01:52.118492 22535416901760 run.py:483] Algo bellman_ford step 7267 current loss 0.635791, current_train_items 232576.
I0302 19:01:52.149579 22535416901760 run.py:483] Algo bellman_ford step 7268 current loss 0.588102, current_train_items 232608.
I0302 19:01:52.181978 22535416901760 run.py:483] Algo bellman_ford step 7269 current loss 0.717213, current_train_items 232640.
I0302 19:01:52.201673 22535416901760 run.py:483] Algo bellman_ford step 7270 current loss 0.302095, current_train_items 232672.
I0302 19:01:52.218111 22535416901760 run.py:483] Algo bellman_ford step 7271 current loss 0.487205, current_train_items 232704.
I0302 19:01:52.241634 22535416901760 run.py:483] Algo bellman_ford step 7272 current loss 0.481408, current_train_items 232736.
I0302 19:01:52.272578 22535416901760 run.py:483] Algo bellman_ford step 7273 current loss 0.570562, current_train_items 232768.
I0302 19:01:52.305575 22535416901760 run.py:483] Algo bellman_ford step 7274 current loss 0.745669, current_train_items 232800.
I0302 19:01:52.325212 22535416901760 run.py:483] Algo bellman_ford step 7275 current loss 0.298593, current_train_items 232832.
I0302 19:01:52.341383 22535416901760 run.py:483] Algo bellman_ford step 7276 current loss 0.455137, current_train_items 232864.
I0302 19:01:52.364885 22535416901760 run.py:483] Algo bellman_ford step 7277 current loss 0.563097, current_train_items 232896.
I0302 19:01:52.396413 22535416901760 run.py:483] Algo bellman_ford step 7278 current loss 0.656302, current_train_items 232928.
I0302 19:01:52.429950 22535416901760 run.py:483] Algo bellman_ford step 7279 current loss 0.665832, current_train_items 232960.
I0302 19:01:52.449255 22535416901760 run.py:483] Algo bellman_ford step 7280 current loss 0.287219, current_train_items 232992.
I0302 19:01:52.465690 22535416901760 run.py:483] Algo bellman_ford step 7281 current loss 0.447785, current_train_items 233024.
I0302 19:01:52.488690 22535416901760 run.py:483] Algo bellman_ford step 7282 current loss 0.587671, current_train_items 233056.
I0302 19:01:52.520064 22535416901760 run.py:483] Algo bellman_ford step 7283 current loss 0.663001, current_train_items 233088.
I0302 19:01:52.552508 22535416901760 run.py:483] Algo bellman_ford step 7284 current loss 0.683209, current_train_items 233120.
I0302 19:01:52.572115 22535416901760 run.py:483] Algo bellman_ford step 7285 current loss 0.270211, current_train_items 233152.
I0302 19:01:52.589225 22535416901760 run.py:483] Algo bellman_ford step 7286 current loss 0.641269, current_train_items 233184.
I0302 19:01:52.612078 22535416901760 run.py:483] Algo bellman_ford step 7287 current loss 0.467003, current_train_items 233216.
I0302 19:01:52.643308 22535416901760 run.py:483] Algo bellman_ford step 7288 current loss 0.612143, current_train_items 233248.
I0302 19:01:52.675615 22535416901760 run.py:483] Algo bellman_ford step 7289 current loss 0.635711, current_train_items 233280.
I0302 19:01:52.695363 22535416901760 run.py:483] Algo bellman_ford step 7290 current loss 0.336021, current_train_items 233312.
I0302 19:01:52.711589 22535416901760 run.py:483] Algo bellman_ford step 7291 current loss 0.480429, current_train_items 233344.
I0302 19:01:52.733973 22535416901760 run.py:483] Algo bellman_ford step 7292 current loss 0.417160, current_train_items 233376.
I0302 19:01:52.765611 22535416901760 run.py:483] Algo bellman_ford step 7293 current loss 0.579430, current_train_items 233408.
I0302 19:01:52.797249 22535416901760 run.py:483] Algo bellman_ford step 7294 current loss 0.713412, current_train_items 233440.
I0302 19:01:52.816501 22535416901760 run.py:483] Algo bellman_ford step 7295 current loss 0.313875, current_train_items 233472.
I0302 19:01:52.832630 22535416901760 run.py:483] Algo bellman_ford step 7296 current loss 0.445138, current_train_items 233504.
I0302 19:01:52.855960 22535416901760 run.py:483] Algo bellman_ford step 7297 current loss 0.592106, current_train_items 233536.
I0302 19:01:52.887174 22535416901760 run.py:483] Algo bellman_ford step 7298 current loss 0.578748, current_train_items 233568.
I0302 19:01:52.919962 22535416901760 run.py:483] Algo bellman_ford step 7299 current loss 0.720519, current_train_items 233600.
I0302 19:01:52.939627 22535416901760 run.py:483] Algo bellman_ford step 7300 current loss 0.328721, current_train_items 233632.
I0302 19:01:52.947521 22535416901760 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.8955078125, 'score': 0.8955078125, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:52.947628 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.896, val scores are: bellman_ford: 0.896
I0302 19:01:52.964707 22535416901760 run.py:483] Algo bellman_ford step 7301 current loss 0.416104, current_train_items 233664.
I0302 19:01:52.988243 22535416901760 run.py:483] Algo bellman_ford step 7302 current loss 0.566479, current_train_items 233696.
I0302 19:01:53.019816 22535416901760 run.py:483] Algo bellman_ford step 7303 current loss 0.671087, current_train_items 233728.
I0302 19:01:53.054073 22535416901760 run.py:483] Algo bellman_ford step 7304 current loss 0.695061, current_train_items 233760.
I0302 19:01:53.073880 22535416901760 run.py:483] Algo bellman_ford step 7305 current loss 0.329929, current_train_items 233792.
I0302 19:01:53.089566 22535416901760 run.py:483] Algo bellman_ford step 7306 current loss 0.410992, current_train_items 233824.
I0302 19:01:53.112961 22535416901760 run.py:483] Algo bellman_ford step 7307 current loss 0.615261, current_train_items 233856.
I0302 19:01:53.143245 22535416901760 run.py:483] Algo bellman_ford step 7308 current loss 0.560316, current_train_items 233888.
I0302 19:01:53.177612 22535416901760 run.py:483] Algo bellman_ford step 7309 current loss 0.764090, current_train_items 233920.
I0302 19:01:53.197092 22535416901760 run.py:483] Algo bellman_ford step 7310 current loss 0.259732, current_train_items 233952.
I0302 19:01:53.212968 22535416901760 run.py:483] Algo bellman_ford step 7311 current loss 0.411316, current_train_items 233984.
I0302 19:01:53.238243 22535416901760 run.py:483] Algo bellman_ford step 7312 current loss 0.616215, current_train_items 234016.
I0302 19:01:53.269767 22535416901760 run.py:483] Algo bellman_ford step 7313 current loss 0.662397, current_train_items 234048.
I0302 19:01:53.303853 22535416901760 run.py:483] Algo bellman_ford step 7314 current loss 0.674718, current_train_items 234080.
I0302 19:01:53.323348 22535416901760 run.py:483] Algo bellman_ford step 7315 current loss 0.258317, current_train_items 234112.
I0302 19:01:53.339317 22535416901760 run.py:483] Algo bellman_ford step 7316 current loss 0.401843, current_train_items 234144.
I0302 19:01:53.363886 22535416901760 run.py:483] Algo bellman_ford step 7317 current loss 0.662549, current_train_items 234176.
I0302 19:01:53.394875 22535416901760 run.py:483] Algo bellman_ford step 7318 current loss 0.623925, current_train_items 234208.
I0302 19:01:53.429524 22535416901760 run.py:483] Algo bellman_ford step 7319 current loss 0.776703, current_train_items 234240.
I0302 19:01:53.449303 22535416901760 run.py:483] Algo bellman_ford step 7320 current loss 0.302429, current_train_items 234272.
I0302 19:01:53.465922 22535416901760 run.py:483] Algo bellman_ford step 7321 current loss 0.485742, current_train_items 234304.
I0302 19:01:53.490496 22535416901760 run.py:483] Algo bellman_ford step 7322 current loss 0.591651, current_train_items 234336.
I0302 19:01:53.521168 22535416901760 run.py:483] Algo bellman_ford step 7323 current loss 0.610061, current_train_items 234368.
I0302 19:01:53.555910 22535416901760 run.py:483] Algo bellman_ford step 7324 current loss 0.796978, current_train_items 234400.
I0302 19:01:53.575448 22535416901760 run.py:483] Algo bellman_ford step 7325 current loss 0.291417, current_train_items 234432.
I0302 19:01:53.591951 22535416901760 run.py:483] Algo bellman_ford step 7326 current loss 0.501077, current_train_items 234464.
I0302 19:01:53.615596 22535416901760 run.py:483] Algo bellman_ford step 7327 current loss 0.545407, current_train_items 234496.
I0302 19:01:53.646247 22535416901760 run.py:483] Algo bellman_ford step 7328 current loss 0.667226, current_train_items 234528.
I0302 19:01:53.680338 22535416901760 run.py:483] Algo bellman_ford step 7329 current loss 0.705257, current_train_items 234560.
I0302 19:01:53.699484 22535416901760 run.py:483] Algo bellman_ford step 7330 current loss 0.285453, current_train_items 234592.
I0302 19:01:53.715930 22535416901760 run.py:483] Algo bellman_ford step 7331 current loss 0.491447, current_train_items 234624.
I0302 19:01:53.740947 22535416901760 run.py:483] Algo bellman_ford step 7332 current loss 0.624619, current_train_items 234656.
I0302 19:01:53.772633 22535416901760 run.py:483] Algo bellman_ford step 7333 current loss 0.725994, current_train_items 234688.
I0302 19:01:53.804663 22535416901760 run.py:483] Algo bellman_ford step 7334 current loss 0.659310, current_train_items 234720.
I0302 19:01:53.824126 22535416901760 run.py:483] Algo bellman_ford step 7335 current loss 0.301080, current_train_items 234752.
I0302 19:01:53.840004 22535416901760 run.py:483] Algo bellman_ford step 7336 current loss 0.425706, current_train_items 234784.
I0302 19:01:53.863810 22535416901760 run.py:483] Algo bellman_ford step 7337 current loss 0.553955, current_train_items 234816.
I0302 19:01:53.895100 22535416901760 run.py:483] Algo bellman_ford step 7338 current loss 0.580389, current_train_items 234848.
I0302 19:01:53.930137 22535416901760 run.py:483] Algo bellman_ford step 7339 current loss 0.655308, current_train_items 234880.
I0302 19:01:53.949995 22535416901760 run.py:483] Algo bellman_ford step 7340 current loss 0.254665, current_train_items 234912.
I0302 19:01:53.965949 22535416901760 run.py:483] Algo bellman_ford step 7341 current loss 0.452753, current_train_items 234944.
I0302 19:01:53.990529 22535416901760 run.py:483] Algo bellman_ford step 7342 current loss 0.592556, current_train_items 234976.
I0302 19:01:54.022644 22535416901760 run.py:483] Algo bellman_ford step 7343 current loss 0.596461, current_train_items 235008.
I0302 19:01:54.055886 22535416901760 run.py:483] Algo bellman_ford step 7344 current loss 0.707214, current_train_items 235040.
I0302 19:01:54.075396 22535416901760 run.py:483] Algo bellman_ford step 7345 current loss 0.328880, current_train_items 235072.
I0302 19:01:54.091253 22535416901760 run.py:483] Algo bellman_ford step 7346 current loss 0.386563, current_train_items 235104.
I0302 19:01:54.115502 22535416901760 run.py:483] Algo bellman_ford step 7347 current loss 0.608196, current_train_items 235136.
I0302 19:01:54.146983 22535416901760 run.py:483] Algo bellman_ford step 7348 current loss 0.556341, current_train_items 235168.
I0302 19:01:54.177354 22535416901760 run.py:483] Algo bellman_ford step 7349 current loss 0.568734, current_train_items 235200.
I0302 19:01:54.196591 22535416901760 run.py:483] Algo bellman_ford step 7350 current loss 0.275245, current_train_items 235232.
I0302 19:01:54.204652 22535416901760 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.953125, 'score': 0.953125, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:54.204792 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.954, current avg val score is 0.953, val scores are: bellman_ford: 0.953
I0302 19:01:54.221934 22535416901760 run.py:483] Algo bellman_ford step 7351 current loss 0.501045, current_train_items 235264.
I0302 19:01:54.246897 22535416901760 run.py:483] Algo bellman_ford step 7352 current loss 0.622572, current_train_items 235296.
I0302 19:01:54.279258 22535416901760 run.py:483] Algo bellman_ford step 7353 current loss 0.599928, current_train_items 235328.
I0302 19:01:54.312579 22535416901760 run.py:483] Algo bellman_ford step 7354 current loss 0.656435, current_train_items 235360.
I0302 19:01:54.332796 22535416901760 run.py:483] Algo bellman_ford step 7355 current loss 0.297622, current_train_items 235392.
I0302 19:01:54.348500 22535416901760 run.py:483] Algo bellman_ford step 7356 current loss 0.380290, current_train_items 235424.
I0302 19:01:54.373357 22535416901760 run.py:483] Algo bellman_ford step 7357 current loss 0.641832, current_train_items 235456.
I0302 19:01:54.404612 22535416901760 run.py:483] Algo bellman_ford step 7358 current loss 0.710643, current_train_items 235488.
I0302 19:01:54.438107 22535416901760 run.py:483] Algo bellman_ford step 7359 current loss 0.702268, current_train_items 235520.
I0302 19:01:54.457933 22535416901760 run.py:483] Algo bellman_ford step 7360 current loss 0.353125, current_train_items 235552.
I0302 19:01:54.474708 22535416901760 run.py:483] Algo bellman_ford step 7361 current loss 0.527235, current_train_items 235584.
I0302 19:01:54.497364 22535416901760 run.py:483] Algo bellman_ford step 7362 current loss 0.609010, current_train_items 235616.
I0302 19:01:54.530422 22535416901760 run.py:483] Algo bellman_ford step 7363 current loss 0.799430, current_train_items 235648.
I0302 19:01:54.562224 22535416901760 run.py:483] Algo bellman_ford step 7364 current loss 0.781914, current_train_items 235680.
I0302 19:01:54.581839 22535416901760 run.py:483] Algo bellman_ford step 7365 current loss 0.334203, current_train_items 235712.
I0302 19:01:54.598051 22535416901760 run.py:483] Algo bellman_ford step 7366 current loss 0.406765, current_train_items 235744.
I0302 19:01:54.622377 22535416901760 run.py:483] Algo bellman_ford step 7367 current loss 0.475914, current_train_items 235776.
I0302 19:01:54.655618 22535416901760 run.py:483] Algo bellman_ford step 7368 current loss 0.633937, current_train_items 235808.
I0302 19:01:54.687593 22535416901760 run.py:483] Algo bellman_ford step 7369 current loss 0.603565, current_train_items 235840.
I0302 19:01:54.707295 22535416901760 run.py:483] Algo bellman_ford step 7370 current loss 0.267924, current_train_items 235872.
I0302 19:01:54.723556 22535416901760 run.py:483] Algo bellman_ford step 7371 current loss 0.570923, current_train_items 235904.
I0302 19:01:54.746039 22535416901760 run.py:483] Algo bellman_ford step 7372 current loss 0.584703, current_train_items 235936.
I0302 19:01:54.777453 22535416901760 run.py:483] Algo bellman_ford step 7373 current loss 0.600212, current_train_items 235968.
I0302 19:01:54.810272 22535416901760 run.py:483] Algo bellman_ford step 7374 current loss 0.807905, current_train_items 236000.
I0302 19:01:54.830117 22535416901760 run.py:483] Algo bellman_ford step 7375 current loss 0.254242, current_train_items 236032.
I0302 19:01:54.846845 22535416901760 run.py:483] Algo bellman_ford step 7376 current loss 0.415811, current_train_items 236064.
I0302 19:01:54.870697 22535416901760 run.py:483] Algo bellman_ford step 7377 current loss 0.531127, current_train_items 236096.
I0302 19:01:54.901793 22535416901760 run.py:483] Algo bellman_ford step 7378 current loss 0.608683, current_train_items 236128.
I0302 19:01:54.933828 22535416901760 run.py:483] Algo bellman_ford step 7379 current loss 0.670218, current_train_items 236160.
I0302 19:01:54.952986 22535416901760 run.py:483] Algo bellman_ford step 7380 current loss 0.312005, current_train_items 236192.
I0302 19:01:54.968769 22535416901760 run.py:483] Algo bellman_ford step 7381 current loss 0.377867, current_train_items 236224.
I0302 19:01:54.993705 22535416901760 run.py:483] Algo bellman_ford step 7382 current loss 0.633362, current_train_items 236256.
I0302 19:01:55.024486 22535416901760 run.py:483] Algo bellman_ford step 7383 current loss 0.646641, current_train_items 236288.
I0302 19:01:55.056775 22535416901760 run.py:483] Algo bellman_ford step 7384 current loss 0.644163, current_train_items 236320.
I0302 19:01:55.076742 22535416901760 run.py:483] Algo bellman_ford step 7385 current loss 0.265937, current_train_items 236352.
I0302 19:01:55.093162 22535416901760 run.py:483] Algo bellman_ford step 7386 current loss 0.462502, current_train_items 236384.
I0302 19:01:55.116860 22535416901760 run.py:483] Algo bellman_ford step 7387 current loss 0.519679, current_train_items 236416.
I0302 19:01:55.149210 22535416901760 run.py:483] Algo bellman_ford step 7388 current loss 0.569523, current_train_items 236448.
I0302 19:01:55.181312 22535416901760 run.py:483] Algo bellman_ford step 7389 current loss 0.658538, current_train_items 236480.
I0302 19:01:55.201251 22535416901760 run.py:483] Algo bellman_ford step 7390 current loss 0.280013, current_train_items 236512.
I0302 19:01:55.217736 22535416901760 run.py:483] Algo bellman_ford step 7391 current loss 0.496329, current_train_items 236544.
I0302 19:01:55.240908 22535416901760 run.py:483] Algo bellman_ford step 7392 current loss 0.556960, current_train_items 236576.
I0302 19:01:55.271754 22535416901760 run.py:483] Algo bellman_ford step 7393 current loss 0.536820, current_train_items 236608.
I0302 19:01:55.305175 22535416901760 run.py:483] Algo bellman_ford step 7394 current loss 0.736671, current_train_items 236640.
I0302 19:01:55.324609 22535416901760 run.py:483] Algo bellman_ford step 7395 current loss 0.352090, current_train_items 236672.
I0302 19:01:55.341010 22535416901760 run.py:483] Algo bellman_ford step 7396 current loss 0.449935, current_train_items 236704.
I0302 19:01:55.365573 22535416901760 run.py:483] Algo bellman_ford step 7397 current loss 0.611156, current_train_items 236736.
I0302 19:01:55.396421 22535416901760 run.py:483] Algo bellman_ford step 7398 current loss 0.536695, current_train_items 236768.
I0302 19:01:55.428195 22535416901760 run.py:483] Algo bellman_ford step 7399 current loss 0.636465, current_train_items 236800.
I0302 19:01:55.447986 22535416901760 run.py:483] Algo bellman_ford step 7400 current loss 0.312598, current_train_items 236832.
I0302 19:01:55.455994 22535416901760 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:55.456100 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.954, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0302 19:01:55.486193 22535416901760 run.py:483] Algo bellman_ford step 7401 current loss 0.452349, current_train_items 236864.
I0302 19:01:55.511206 22535416901760 run.py:483] Algo bellman_ford step 7402 current loss 0.577732, current_train_items 236896.
I0302 19:01:55.544106 22535416901760 run.py:483] Algo bellman_ford step 7403 current loss 0.649912, current_train_items 236928.
I0302 19:01:55.579007 22535416901760 run.py:483] Algo bellman_ford step 7404 current loss 0.697227, current_train_items 236960.
I0302 19:01:55.599487 22535416901760 run.py:483] Algo bellman_ford step 7405 current loss 0.321015, current_train_items 236992.
I0302 19:01:55.615107 22535416901760 run.py:483] Algo bellman_ford step 7406 current loss 0.396661, current_train_items 237024.
I0302 19:01:55.639168 22535416901760 run.py:483] Algo bellman_ford step 7407 current loss 0.501331, current_train_items 237056.
I0302 19:01:55.671478 22535416901760 run.py:483] Algo bellman_ford step 7408 current loss 0.630208, current_train_items 237088.
I0302 19:01:55.703276 22535416901760 run.py:483] Algo bellman_ford step 7409 current loss 0.705085, current_train_items 237120.
I0302 19:01:55.722806 22535416901760 run.py:483] Algo bellman_ford step 7410 current loss 0.287823, current_train_items 237152.
I0302 19:01:55.739610 22535416901760 run.py:483] Algo bellman_ford step 7411 current loss 0.453013, current_train_items 237184.
I0302 19:01:55.764854 22535416901760 run.py:483] Algo bellman_ford step 7412 current loss 0.622694, current_train_items 237216.
I0302 19:01:55.796799 22535416901760 run.py:483] Algo bellman_ford step 7413 current loss 0.697125, current_train_items 237248.
I0302 19:01:55.828667 22535416901760 run.py:483] Algo bellman_ford step 7414 current loss 0.698036, current_train_items 237280.
I0302 19:01:55.848253 22535416901760 run.py:483] Algo bellman_ford step 7415 current loss 0.352493, current_train_items 237312.
I0302 19:01:55.864194 22535416901760 run.py:483] Algo bellman_ford step 7416 current loss 0.405851, current_train_items 237344.
I0302 19:01:55.888289 22535416901760 run.py:483] Algo bellman_ford step 7417 current loss 0.624649, current_train_items 237376.
I0302 19:01:55.919387 22535416901760 run.py:483] Algo bellman_ford step 7418 current loss 0.710710, current_train_items 237408.
I0302 19:01:55.952058 22535416901760 run.py:483] Algo bellman_ford step 7419 current loss 0.717511, current_train_items 237440.
I0302 19:01:55.971896 22535416901760 run.py:483] Algo bellman_ford step 7420 current loss 0.318805, current_train_items 237472.
I0302 19:01:55.987824 22535416901760 run.py:483] Algo bellman_ford step 7421 current loss 0.473953, current_train_items 237504.
I0302 19:01:56.011391 22535416901760 run.py:483] Algo bellman_ford step 7422 current loss 0.561089, current_train_items 237536.
I0302 19:01:56.042753 22535416901760 run.py:483] Algo bellman_ford step 7423 current loss 0.634879, current_train_items 237568.
I0302 19:01:56.075700 22535416901760 run.py:483] Algo bellman_ford step 7424 current loss 0.623605, current_train_items 237600.
I0302 19:01:56.095590 22535416901760 run.py:483] Algo bellman_ford step 7425 current loss 0.271440, current_train_items 237632.
I0302 19:01:56.111598 22535416901760 run.py:483] Algo bellman_ford step 7426 current loss 0.396701, current_train_items 237664.
I0302 19:01:56.136028 22535416901760 run.py:483] Algo bellman_ford step 7427 current loss 0.549897, current_train_items 237696.
I0302 19:01:56.166954 22535416901760 run.py:483] Algo bellman_ford step 7428 current loss 0.670888, current_train_items 237728.
I0302 19:01:56.200732 22535416901760 run.py:483] Algo bellman_ford step 7429 current loss 0.920805, current_train_items 237760.
I0302 19:01:56.220220 22535416901760 run.py:483] Algo bellman_ford step 7430 current loss 0.267655, current_train_items 237792.
I0302 19:01:56.236083 22535416901760 run.py:483] Algo bellman_ford step 7431 current loss 0.448867, current_train_items 237824.
I0302 19:01:56.260451 22535416901760 run.py:483] Algo bellman_ford step 7432 current loss 0.707867, current_train_items 237856.
I0302 19:01:56.292219 22535416901760 run.py:483] Algo bellman_ford step 7433 current loss 0.747676, current_train_items 237888.
I0302 19:01:56.323380 22535416901760 run.py:483] Algo bellman_ford step 7434 current loss 0.843619, current_train_items 237920.
I0302 19:01:56.343029 22535416901760 run.py:483] Algo bellman_ford step 7435 current loss 0.273923, current_train_items 237952.
I0302 19:01:56.359088 22535416901760 run.py:483] Algo bellman_ford step 7436 current loss 0.480818, current_train_items 237984.
I0302 19:01:56.383727 22535416901760 run.py:483] Algo bellman_ford step 7437 current loss 0.527681, current_train_items 238016.
I0302 19:01:56.414350 22535416901760 run.py:483] Algo bellman_ford step 7438 current loss 0.629466, current_train_items 238048.
I0302 19:01:56.448837 22535416901760 run.py:483] Algo bellman_ford step 7439 current loss 0.752605, current_train_items 238080.
I0302 19:01:56.468384 22535416901760 run.py:483] Algo bellman_ford step 7440 current loss 0.349074, current_train_items 238112.
I0302 19:01:56.484909 22535416901760 run.py:483] Algo bellman_ford step 7441 current loss 0.506983, current_train_items 238144.
I0302 19:01:56.508046 22535416901760 run.py:483] Algo bellman_ford step 7442 current loss 0.555292, current_train_items 238176.
I0302 19:01:56.540273 22535416901760 run.py:483] Algo bellman_ford step 7443 current loss 0.716040, current_train_items 238208.
I0302 19:01:56.574380 22535416901760 run.py:483] Algo bellman_ford step 7444 current loss 0.664915, current_train_items 238240.
I0302 19:01:56.593864 22535416901760 run.py:483] Algo bellman_ford step 7445 current loss 0.294473, current_train_items 238272.
I0302 19:01:56.609919 22535416901760 run.py:483] Algo bellman_ford step 7446 current loss 0.525509, current_train_items 238304.
I0302 19:01:56.633502 22535416901760 run.py:483] Algo bellman_ford step 7447 current loss 0.570130, current_train_items 238336.
I0302 19:01:56.664217 22535416901760 run.py:483] Algo bellman_ford step 7448 current loss 0.645261, current_train_items 238368.
I0302 19:01:56.698942 22535416901760 run.py:483] Algo bellman_ford step 7449 current loss 0.706627, current_train_items 238400.
I0302 19:01:56.718677 22535416901760 run.py:483] Algo bellman_ford step 7450 current loss 0.319327, current_train_items 238432.
I0302 19:01:56.726659 22535416901760 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:56.726766 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:01:56.743551 22535416901760 run.py:483] Algo bellman_ford step 7451 current loss 0.458443, current_train_items 238464.
I0302 19:01:56.768783 22535416901760 run.py:483] Algo bellman_ford step 7452 current loss 0.604839, current_train_items 238496.
I0302 19:01:56.799633 22535416901760 run.py:483] Algo bellman_ford step 7453 current loss 0.590484, current_train_items 238528.
I0302 19:01:56.832347 22535416901760 run.py:483] Algo bellman_ford step 7454 current loss 0.704996, current_train_items 238560.
I0302 19:01:56.852252 22535416901760 run.py:483] Algo bellman_ford step 7455 current loss 0.299633, current_train_items 238592.
I0302 19:01:56.868613 22535416901760 run.py:483] Algo bellman_ford step 7456 current loss 0.403940, current_train_items 238624.
I0302 19:01:56.892429 22535416901760 run.py:483] Algo bellman_ford step 7457 current loss 0.524710, current_train_items 238656.
I0302 19:01:56.924201 22535416901760 run.py:483] Algo bellman_ford step 7458 current loss 0.660551, current_train_items 238688.
I0302 19:01:56.957059 22535416901760 run.py:483] Algo bellman_ford step 7459 current loss 0.731034, current_train_items 238720.
I0302 19:01:56.976885 22535416901760 run.py:483] Algo bellman_ford step 7460 current loss 0.227718, current_train_items 238752.
I0302 19:01:56.993637 22535416901760 run.py:483] Algo bellman_ford step 7461 current loss 0.477249, current_train_items 238784.
I0302 19:01:57.016336 22535416901760 run.py:483] Algo bellman_ford step 7462 current loss 0.557150, current_train_items 238816.
I0302 19:01:57.047381 22535416901760 run.py:483] Algo bellman_ford step 7463 current loss 0.610961, current_train_items 238848.
I0302 19:01:57.080436 22535416901760 run.py:483] Algo bellman_ford step 7464 current loss 0.753696, current_train_items 238880.
I0302 19:01:57.099869 22535416901760 run.py:483] Algo bellman_ford step 7465 current loss 0.297498, current_train_items 238912.
I0302 19:01:57.115728 22535416901760 run.py:483] Algo bellman_ford step 7466 current loss 0.388397, current_train_items 238944.
I0302 19:01:57.138836 22535416901760 run.py:483] Algo bellman_ford step 7467 current loss 0.565872, current_train_items 238976.
I0302 19:01:57.170908 22535416901760 run.py:483] Algo bellman_ford step 7468 current loss 0.579865, current_train_items 239008.
I0302 19:01:57.202440 22535416901760 run.py:483] Algo bellman_ford step 7469 current loss 0.667377, current_train_items 239040.
I0302 19:01:57.222296 22535416901760 run.py:483] Algo bellman_ford step 7470 current loss 0.274293, current_train_items 239072.
I0302 19:01:57.238510 22535416901760 run.py:483] Algo bellman_ford step 7471 current loss 0.450303, current_train_items 239104.
I0302 19:01:57.261236 22535416901760 run.py:483] Algo bellman_ford step 7472 current loss 0.699009, current_train_items 239136.
I0302 19:01:57.292698 22535416901760 run.py:483] Algo bellman_ford step 7473 current loss 0.667282, current_train_items 239168.
I0302 19:01:57.327441 22535416901760 run.py:483] Algo bellman_ford step 7474 current loss 0.821270, current_train_items 239200.
I0302 19:01:57.347175 22535416901760 run.py:483] Algo bellman_ford step 7475 current loss 0.256297, current_train_items 239232.
I0302 19:01:57.363444 22535416901760 run.py:483] Algo bellman_ford step 7476 current loss 0.519598, current_train_items 239264.
I0302 19:01:57.385948 22535416901760 run.py:483] Algo bellman_ford step 7477 current loss 0.573418, current_train_items 239296.
I0302 19:01:57.417630 22535416901760 run.py:483] Algo bellman_ford step 7478 current loss 0.627945, current_train_items 239328.
I0302 19:01:57.453727 22535416901760 run.py:483] Algo bellman_ford step 7479 current loss 0.808461, current_train_items 239360.
I0302 19:01:57.472963 22535416901760 run.py:483] Algo bellman_ford step 7480 current loss 0.302165, current_train_items 239392.
I0302 19:01:57.489300 22535416901760 run.py:483] Algo bellman_ford step 7481 current loss 0.443210, current_train_items 239424.
I0302 19:01:57.513845 22535416901760 run.py:483] Algo bellman_ford step 7482 current loss 0.711836, current_train_items 239456.
I0302 19:01:57.545842 22535416901760 run.py:483] Algo bellman_ford step 7483 current loss 0.766811, current_train_items 239488.
I0302 19:01:57.579951 22535416901760 run.py:483] Algo bellman_ford step 7484 current loss 0.830982, current_train_items 239520.
I0302 19:01:57.599625 22535416901760 run.py:483] Algo bellman_ford step 7485 current loss 0.287788, current_train_items 239552.
I0302 19:01:57.615710 22535416901760 run.py:483] Algo bellman_ford step 7486 current loss 0.549856, current_train_items 239584.
I0302 19:01:57.638276 22535416901760 run.py:483] Algo bellman_ford step 7487 current loss 0.566445, current_train_items 239616.
I0302 19:01:57.669697 22535416901760 run.py:483] Algo bellman_ford step 7488 current loss 0.797366, current_train_items 239648.
I0302 19:01:57.704743 22535416901760 run.py:483] Algo bellman_ford step 7489 current loss 0.963378, current_train_items 239680.
I0302 19:01:57.724372 22535416901760 run.py:483] Algo bellman_ford step 7490 current loss 0.391522, current_train_items 239712.
I0302 19:01:57.740219 22535416901760 run.py:483] Algo bellman_ford step 7491 current loss 0.488879, current_train_items 239744.
I0302 19:01:57.762686 22535416901760 run.py:483] Algo bellman_ford step 7492 current loss 0.572897, current_train_items 239776.
I0302 19:01:57.793693 22535416901760 run.py:483] Algo bellman_ford step 7493 current loss 0.681010, current_train_items 239808.
I0302 19:01:57.826812 22535416901760 run.py:483] Algo bellman_ford step 7494 current loss 0.742669, current_train_items 239840.
I0302 19:01:57.846329 22535416901760 run.py:483] Algo bellman_ford step 7495 current loss 0.410909, current_train_items 239872.
I0302 19:01:57.862610 22535416901760 run.py:483] Algo bellman_ford step 7496 current loss 0.456990, current_train_items 239904.
I0302 19:01:57.886349 22535416901760 run.py:483] Algo bellman_ford step 7497 current loss 0.553363, current_train_items 239936.
I0302 19:01:57.918188 22535416901760 run.py:483] Algo bellman_ford step 7498 current loss 0.660557, current_train_items 239968.
I0302 19:01:57.951027 22535416901760 run.py:483] Algo bellman_ford step 7499 current loss 0.704944, current_train_items 240000.
I0302 19:01:57.971081 22535416901760 run.py:483] Algo bellman_ford step 7500 current loss 0.343912, current_train_items 240032.
I0302 19:01:57.978723 22535416901760 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.9169921875, 'score': 0.9169921875, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:57.978844 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.917, val scores are: bellman_ford: 0.917
I0302 19:01:57.995868 22535416901760 run.py:483] Algo bellman_ford step 7501 current loss 0.356465, current_train_items 240064.
I0302 19:01:58.020498 22535416901760 run.py:483] Algo bellman_ford step 7502 current loss 0.614538, current_train_items 240096.
I0302 19:01:58.052189 22535416901760 run.py:483] Algo bellman_ford step 7503 current loss 0.656880, current_train_items 240128.
I0302 19:01:58.085559 22535416901760 run.py:483] Algo bellman_ford step 7504 current loss 0.723188, current_train_items 240160.
I0302 19:01:58.105511 22535416901760 run.py:483] Algo bellman_ford step 7505 current loss 0.291461, current_train_items 240192.
I0302 19:01:58.121546 22535416901760 run.py:483] Algo bellman_ford step 7506 current loss 0.465674, current_train_items 240224.
I0302 19:01:58.144944 22535416901760 run.py:483] Algo bellman_ford step 7507 current loss 0.527421, current_train_items 240256.
I0302 19:01:58.176008 22535416901760 run.py:483] Algo bellman_ford step 7508 current loss 0.594988, current_train_items 240288.
I0302 19:01:58.208352 22535416901760 run.py:483] Algo bellman_ford step 7509 current loss 0.663370, current_train_items 240320.
I0302 19:01:58.227918 22535416901760 run.py:483] Algo bellman_ford step 7510 current loss 0.307203, current_train_items 240352.
I0302 19:01:58.244426 22535416901760 run.py:483] Algo bellman_ford step 7511 current loss 0.512346, current_train_items 240384.
I0302 19:01:58.267420 22535416901760 run.py:483] Algo bellman_ford step 7512 current loss 0.626612, current_train_items 240416.
I0302 19:01:58.298389 22535416901760 run.py:483] Algo bellman_ford step 7513 current loss 0.626654, current_train_items 240448.
I0302 19:01:58.330452 22535416901760 run.py:483] Algo bellman_ford step 7514 current loss 0.687652, current_train_items 240480.
I0302 19:01:58.349921 22535416901760 run.py:483] Algo bellman_ford step 7515 current loss 0.323407, current_train_items 240512.
I0302 19:01:58.365371 22535416901760 run.py:483] Algo bellman_ford step 7516 current loss 0.515754, current_train_items 240544.
I0302 19:01:58.389429 22535416901760 run.py:483] Algo bellman_ford step 7517 current loss 0.571746, current_train_items 240576.
I0302 19:01:58.420828 22535416901760 run.py:483] Algo bellman_ford step 7518 current loss 0.632336, current_train_items 240608.
I0302 19:01:58.454825 22535416901760 run.py:483] Algo bellman_ford step 7519 current loss 0.754891, current_train_items 240640.
I0302 19:01:58.474595 22535416901760 run.py:483] Algo bellman_ford step 7520 current loss 0.275327, current_train_items 240672.
I0302 19:01:58.490649 22535416901760 run.py:483] Algo bellman_ford step 7521 current loss 0.421411, current_train_items 240704.
I0302 19:01:58.514011 22535416901760 run.py:483] Algo bellman_ford step 7522 current loss 0.560090, current_train_items 240736.
I0302 19:01:58.545266 22535416901760 run.py:483] Algo bellman_ford step 7523 current loss 0.713185, current_train_items 240768.
I0302 19:01:58.576256 22535416901760 run.py:483] Algo bellman_ford step 7524 current loss 0.588283, current_train_items 240800.
I0302 19:01:58.595610 22535416901760 run.py:483] Algo bellman_ford step 7525 current loss 0.197728, current_train_items 240832.
I0302 19:01:58.611574 22535416901760 run.py:483] Algo bellman_ford step 7526 current loss 0.484031, current_train_items 240864.
I0302 19:01:58.635902 22535416901760 run.py:483] Algo bellman_ford step 7527 current loss 0.652634, current_train_items 240896.
I0302 19:01:58.666371 22535416901760 run.py:483] Algo bellman_ford step 7528 current loss 0.591882, current_train_items 240928.
I0302 19:01:58.699569 22535416901760 run.py:483] Algo bellman_ford step 7529 current loss 0.645874, current_train_items 240960.
I0302 19:01:58.719023 22535416901760 run.py:483] Algo bellman_ford step 7530 current loss 0.264390, current_train_items 240992.
I0302 19:01:58.735116 22535416901760 run.py:483] Algo bellman_ford step 7531 current loss 0.416330, current_train_items 241024.
I0302 19:01:58.757720 22535416901760 run.py:483] Algo bellman_ford step 7532 current loss 0.536745, current_train_items 241056.
I0302 19:01:58.790509 22535416901760 run.py:483] Algo bellman_ford step 7533 current loss 0.609664, current_train_items 241088.
I0302 19:01:58.824529 22535416901760 run.py:483] Algo bellman_ford step 7534 current loss 0.752031, current_train_items 241120.
I0302 19:01:58.844472 22535416901760 run.py:483] Algo bellman_ford step 7535 current loss 0.327890, current_train_items 241152.
I0302 19:01:58.860602 22535416901760 run.py:483] Algo bellman_ford step 7536 current loss 0.482870, current_train_items 241184.
I0302 19:01:58.884296 22535416901760 run.py:483] Algo bellman_ford step 7537 current loss 0.675964, current_train_items 241216.
I0302 19:01:58.916209 22535416901760 run.py:483] Algo bellman_ford step 7538 current loss 0.724280, current_train_items 241248.
I0302 19:01:58.948636 22535416901760 run.py:483] Algo bellman_ford step 7539 current loss 0.738783, current_train_items 241280.
I0302 19:01:58.968319 22535416901760 run.py:483] Algo bellman_ford step 7540 current loss 0.280643, current_train_items 241312.
I0302 19:01:58.984899 22535416901760 run.py:483] Algo bellman_ford step 7541 current loss 0.503746, current_train_items 241344.
I0302 19:01:59.009798 22535416901760 run.py:483] Algo bellman_ford step 7542 current loss 0.549957, current_train_items 241376.
I0302 19:01:59.041950 22535416901760 run.py:483] Algo bellman_ford step 7543 current loss 0.649864, current_train_items 241408.
I0302 19:01:59.075149 22535416901760 run.py:483] Algo bellman_ford step 7544 current loss 0.690511, current_train_items 241440.
I0302 19:01:59.094669 22535416901760 run.py:483] Algo bellman_ford step 7545 current loss 0.243780, current_train_items 241472.
I0302 19:01:59.110821 22535416901760 run.py:483] Algo bellman_ford step 7546 current loss 0.575805, current_train_items 241504.
I0302 19:01:59.135215 22535416901760 run.py:483] Algo bellman_ford step 7547 current loss 0.740521, current_train_items 241536.
I0302 19:01:59.167118 22535416901760 run.py:483] Algo bellman_ford step 7548 current loss 0.716535, current_train_items 241568.
I0302 19:01:59.200451 22535416901760 run.py:483] Algo bellman_ford step 7549 current loss 0.727115, current_train_items 241600.
I0302 19:01:59.220277 22535416901760 run.py:483] Algo bellman_ford step 7550 current loss 0.292840, current_train_items 241632.
I0302 19:01:59.228327 22535416901760 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:59.228435 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:01:59.245610 22535416901760 run.py:483] Algo bellman_ford step 7551 current loss 0.466555, current_train_items 241664.
I0302 19:01:59.269697 22535416901760 run.py:483] Algo bellman_ford step 7552 current loss 0.525332, current_train_items 241696.
I0302 19:01:59.301700 22535416901760 run.py:483] Algo bellman_ford step 7553 current loss 0.659029, current_train_items 241728.
I0302 19:01:59.335363 22535416901760 run.py:483] Algo bellman_ford step 7554 current loss 0.777073, current_train_items 241760.
I0302 19:01:59.355367 22535416901760 run.py:483] Algo bellman_ford step 7555 current loss 0.280298, current_train_items 241792.
I0302 19:01:59.371544 22535416901760 run.py:483] Algo bellman_ford step 7556 current loss 0.476385, current_train_items 241824.
I0302 19:01:59.395365 22535416901760 run.py:483] Algo bellman_ford step 7557 current loss 0.549875, current_train_items 241856.
I0302 19:01:59.428072 22535416901760 run.py:483] Algo bellman_ford step 7558 current loss 0.690043, current_train_items 241888.
I0302 19:01:59.460291 22535416901760 run.py:483] Algo bellman_ford step 7559 current loss 0.713509, current_train_items 241920.
I0302 19:01:59.480128 22535416901760 run.py:483] Algo bellman_ford step 7560 current loss 0.306543, current_train_items 241952.
I0302 19:01:59.497108 22535416901760 run.py:483] Algo bellman_ford step 7561 current loss 0.511562, current_train_items 241984.
I0302 19:01:59.520009 22535416901760 run.py:483] Algo bellman_ford step 7562 current loss 0.620620, current_train_items 242016.
I0302 19:01:59.551668 22535416901760 run.py:483] Algo bellman_ford step 7563 current loss 0.577825, current_train_items 242048.
I0302 19:01:59.584746 22535416901760 run.py:483] Algo bellman_ford step 7564 current loss 0.716994, current_train_items 242080.
I0302 19:01:59.604012 22535416901760 run.py:483] Algo bellman_ford step 7565 current loss 0.338285, current_train_items 242112.
I0302 19:01:59.619931 22535416901760 run.py:483] Algo bellman_ford step 7566 current loss 0.480408, current_train_items 242144.
I0302 19:01:59.643780 22535416901760 run.py:483] Algo bellman_ford step 7567 current loss 0.550175, current_train_items 242176.
I0302 19:01:59.674673 22535416901760 run.py:483] Algo bellman_ford step 7568 current loss 0.617220, current_train_items 242208.
I0302 19:01:59.707059 22535416901760 run.py:483] Algo bellman_ford step 7569 current loss 0.756585, current_train_items 242240.
I0302 19:01:59.726727 22535416901760 run.py:483] Algo bellman_ford step 7570 current loss 0.340554, current_train_items 242272.
I0302 19:01:59.743120 22535416901760 run.py:483] Algo bellman_ford step 7571 current loss 0.448494, current_train_items 242304.
I0302 19:01:59.765949 22535416901760 run.py:483] Algo bellman_ford step 7572 current loss 0.634182, current_train_items 242336.
I0302 19:01:59.795877 22535416901760 run.py:483] Algo bellman_ford step 7573 current loss 0.551369, current_train_items 242368.
I0302 19:01:59.826693 22535416901760 run.py:483] Algo bellman_ford step 7574 current loss 0.615821, current_train_items 242400.
I0302 19:01:59.846297 22535416901760 run.py:483] Algo bellman_ford step 7575 current loss 0.254517, current_train_items 242432.
I0302 19:01:59.862962 22535416901760 run.py:483] Algo bellman_ford step 7576 current loss 0.447907, current_train_items 242464.
I0302 19:01:59.885380 22535416901760 run.py:483] Algo bellman_ford step 7577 current loss 0.502966, current_train_items 242496.
I0302 19:01:59.916701 22535416901760 run.py:483] Algo bellman_ford step 7578 current loss 0.597929, current_train_items 242528.
I0302 19:01:59.951532 22535416901760 run.py:483] Algo bellman_ford step 7579 current loss 0.918866, current_train_items 242560.
I0302 19:01:59.970893 22535416901760 run.py:483] Algo bellman_ford step 7580 current loss 0.303642, current_train_items 242592.
I0302 19:01:59.987261 22535416901760 run.py:483] Algo bellman_ford step 7581 current loss 0.468423, current_train_items 242624.
I0302 19:02:00.011136 22535416901760 run.py:483] Algo bellman_ford step 7582 current loss 0.555666, current_train_items 242656.
I0302 19:02:00.042076 22535416901760 run.py:483] Algo bellman_ford step 7583 current loss 0.592949, current_train_items 242688.
I0302 19:02:00.074404 22535416901760 run.py:483] Algo bellman_ford step 7584 current loss 0.704496, current_train_items 242720.
I0302 19:02:00.094230 22535416901760 run.py:483] Algo bellman_ford step 7585 current loss 0.288053, current_train_items 242752.
I0302 19:02:00.110008 22535416901760 run.py:483] Algo bellman_ford step 7586 current loss 0.461528, current_train_items 242784.
I0302 19:02:00.132939 22535416901760 run.py:483] Algo bellman_ford step 7587 current loss 0.587065, current_train_items 242816.
I0302 19:02:00.161990 22535416901760 run.py:483] Algo bellman_ford step 7588 current loss 0.504494, current_train_items 242848.
I0302 19:02:00.195500 22535416901760 run.py:483] Algo bellman_ford step 7589 current loss 0.724003, current_train_items 242880.
I0302 19:02:00.215033 22535416901760 run.py:483] Algo bellman_ford step 7590 current loss 0.358169, current_train_items 242912.
I0302 19:02:00.230928 22535416901760 run.py:483] Algo bellman_ford step 7591 current loss 0.405037, current_train_items 242944.
I0302 19:02:00.255127 22535416901760 run.py:483] Algo bellman_ford step 7592 current loss 0.633619, current_train_items 242976.
I0302 19:02:00.284845 22535416901760 run.py:483] Algo bellman_ford step 7593 current loss 0.612022, current_train_items 243008.
I0302 19:02:00.318163 22535416901760 run.py:483] Algo bellman_ford step 7594 current loss 0.654567, current_train_items 243040.
I0302 19:02:00.337460 22535416901760 run.py:483] Algo bellman_ford step 7595 current loss 0.339228, current_train_items 243072.
I0302 19:02:00.353425 22535416901760 run.py:483] Algo bellman_ford step 7596 current loss 0.477403, current_train_items 243104.
I0302 19:02:00.377315 22535416901760 run.py:483] Algo bellman_ford step 7597 current loss 0.615596, current_train_items 243136.
I0302 19:02:00.409236 22535416901760 run.py:483] Algo bellman_ford step 7598 current loss 0.574036, current_train_items 243168.
I0302 19:02:00.439638 22535416901760 run.py:483] Algo bellman_ford step 7599 current loss 0.702177, current_train_items 243200.
I0302 19:02:00.459357 22535416901760 run.py:483] Algo bellman_ford step 7600 current loss 0.286291, current_train_items 243232.
I0302 19:02:00.467123 22535416901760 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:02:00.467241 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0302 19:02:00.484239 22535416901760 run.py:483] Algo bellman_ford step 7601 current loss 0.475978, current_train_items 243264.
I0302 19:02:00.510296 22535416901760 run.py:483] Algo bellman_ford step 7602 current loss 0.631254, current_train_items 243296.
I0302 19:02:00.541497 22535416901760 run.py:483] Algo bellman_ford step 7603 current loss 0.568995, current_train_items 243328.
I0302 19:02:00.576415 22535416901760 run.py:483] Algo bellman_ford step 7604 current loss 0.748037, current_train_items 243360.
I0302 19:02:00.596385 22535416901760 run.py:483] Algo bellman_ford step 7605 current loss 0.306827, current_train_items 243392.
I0302 19:02:00.611662 22535416901760 run.py:483] Algo bellman_ford step 7606 current loss 0.443963, current_train_items 243424.
I0302 19:02:00.634865 22535416901760 run.py:483] Algo bellman_ford step 7607 current loss 0.495168, current_train_items 243456.
I0302 19:02:00.666609 22535416901760 run.py:483] Algo bellman_ford step 7608 current loss 0.584288, current_train_items 243488.
I0302 19:02:00.700757 22535416901760 run.py:483] Algo bellman_ford step 7609 current loss 0.728801, current_train_items 243520.
I0302 19:02:00.720160 22535416901760 run.py:483] Algo bellman_ford step 7610 current loss 0.290058, current_train_items 243552.
I0302 19:02:00.736671 22535416901760 run.py:483] Algo bellman_ford step 7611 current loss 0.507699, current_train_items 243584.
I0302 19:02:00.760356 22535416901760 run.py:483] Algo bellman_ford step 7612 current loss 0.545505, current_train_items 243616.
I0302 19:02:00.789632 22535416901760 run.py:483] Algo bellman_ford step 7613 current loss 0.572277, current_train_items 243648.
I0302 19:02:00.822276 22535416901760 run.py:483] Algo bellman_ford step 7614 current loss 0.784527, current_train_items 243680.
I0302 19:02:00.842013 22535416901760 run.py:483] Algo bellman_ford step 7615 current loss 0.305343, current_train_items 243712.
I0302 19:02:00.858207 22535416901760 run.py:483] Algo bellman_ford step 7616 current loss 0.433336, current_train_items 243744.
I0302 19:02:00.881779 22535416901760 run.py:483] Algo bellman_ford step 7617 current loss 0.537512, current_train_items 243776.
I0302 19:02:00.912159 22535416901760 run.py:483] Algo bellman_ford step 7618 current loss 0.615444, current_train_items 243808.
I0302 19:02:00.945914 22535416901760 run.py:483] Algo bellman_ford step 7619 current loss 0.752605, current_train_items 243840.
I0302 19:02:00.965552 22535416901760 run.py:483] Algo bellman_ford step 7620 current loss 0.373284, current_train_items 243872.
I0302 19:02:00.981696 22535416901760 run.py:483] Algo bellman_ford step 7621 current loss 0.493117, current_train_items 243904.
I0302 19:02:01.005845 22535416901760 run.py:483] Algo bellman_ford step 7622 current loss 0.530997, current_train_items 243936.
I0302 19:02:01.038240 22535416901760 run.py:483] Algo bellman_ford step 7623 current loss 0.617801, current_train_items 243968.
I0302 19:02:01.070823 22535416901760 run.py:483] Algo bellman_ford step 7624 current loss 0.664038, current_train_items 244000.
I0302 19:02:01.090758 22535416901760 run.py:483] Algo bellman_ford step 7625 current loss 0.264425, current_train_items 244032.
I0302 19:02:01.106149 22535416901760 run.py:483] Algo bellman_ford step 7626 current loss 0.393733, current_train_items 244064.
I0302 19:02:01.130545 22535416901760 run.py:483] Algo bellman_ford step 7627 current loss 0.685145, current_train_items 244096.
I0302 19:02:01.162768 22535416901760 run.py:483] Algo bellman_ford step 7628 current loss 0.614539, current_train_items 244128.
I0302 19:02:01.195202 22535416901760 run.py:483] Algo bellman_ford step 7629 current loss 0.734257, current_train_items 244160.
I0302 19:02:01.214863 22535416901760 run.py:483] Algo bellman_ford step 7630 current loss 0.306767, current_train_items 244192.
I0302 19:02:01.231249 22535416901760 run.py:483] Algo bellman_ford step 7631 current loss 0.553406, current_train_items 244224.
I0302 19:02:01.254574 22535416901760 run.py:483] Algo bellman_ford step 7632 current loss 0.587682, current_train_items 244256.
I0302 19:02:01.286688 22535416901760 run.py:483] Algo bellman_ford step 7633 current loss 0.632335, current_train_items 244288.
I0302 19:02:01.320305 22535416901760 run.py:483] Algo bellman_ford step 7634 current loss 0.660062, current_train_items 244320.
I0302 19:02:01.340105 22535416901760 run.py:483] Algo bellman_ford step 7635 current loss 0.285048, current_train_items 244352.
I0302 19:02:01.356075 22535416901760 run.py:483] Algo bellman_ford step 7636 current loss 0.476189, current_train_items 244384.
I0302 19:02:01.379520 22535416901760 run.py:483] Algo bellman_ford step 7637 current loss 0.559989, current_train_items 244416.
I0302 19:02:01.409547 22535416901760 run.py:483] Algo bellman_ford step 7638 current loss 0.538882, current_train_items 244448.
I0302 19:02:01.444931 22535416901760 run.py:483] Algo bellman_ford step 7639 current loss 0.665721, current_train_items 244480.
I0302 19:02:01.464448 22535416901760 run.py:483] Algo bellman_ford step 7640 current loss 0.288727, current_train_items 244512.
I0302 19:02:01.480398 22535416901760 run.py:483] Algo bellman_ford step 7641 current loss 0.552912, current_train_items 244544.
I0302 19:02:01.503043 22535416901760 run.py:483] Algo bellman_ford step 7642 current loss 0.514697, current_train_items 244576.
I0302 19:02:01.536931 22535416901760 run.py:483] Algo bellman_ford step 7643 current loss 0.652886, current_train_items 244608.
I0302 19:02:01.571897 22535416901760 run.py:483] Algo bellman_ford step 7644 current loss 0.684793, current_train_items 244640.
I0302 19:02:01.591571 22535416901760 run.py:483] Algo bellman_ford step 7645 current loss 0.260835, current_train_items 244672.
I0302 19:02:01.607712 22535416901760 run.py:483] Algo bellman_ford step 7646 current loss 0.557048, current_train_items 244704.
I0302 19:02:01.632087 22535416901760 run.py:483] Algo bellman_ford step 7647 current loss 0.527568, current_train_items 244736.
I0302 19:02:01.665327 22535416901760 run.py:483] Algo bellman_ford step 7648 current loss 0.638977, current_train_items 244768.
I0302 19:02:01.700304 22535416901760 run.py:483] Algo bellman_ford step 7649 current loss 0.750715, current_train_items 244800.
I0302 19:02:01.720089 22535416901760 run.py:483] Algo bellman_ford step 7650 current loss 0.376480, current_train_items 244832.
I0302 19:02:01.728240 22535416901760 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:02:01.728349 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:02:01.744843 22535416901760 run.py:483] Algo bellman_ford step 7651 current loss 0.410908, current_train_items 244864.
I0302 19:02:01.769727 22535416901760 run.py:483] Algo bellman_ford step 7652 current loss 0.611432, current_train_items 244896.
I0302 19:02:01.800999 22535416901760 run.py:483] Algo bellman_ford step 7653 current loss 0.580967, current_train_items 244928.
I0302 19:02:01.834710 22535416901760 run.py:483] Algo bellman_ford step 7654 current loss 0.726838, current_train_items 244960.
I0302 19:02:01.854551 22535416901760 run.py:483] Algo bellman_ford step 7655 current loss 0.264213, current_train_items 244992.
I0302 19:02:01.870389 22535416901760 run.py:483] Algo bellman_ford step 7656 current loss 0.466203, current_train_items 245024.
I0302 19:02:01.894350 22535416901760 run.py:483] Algo bellman_ford step 7657 current loss 0.646457, current_train_items 245056.
I0302 19:02:01.926449 22535416901760 run.py:483] Algo bellman_ford step 7658 current loss 0.601945, current_train_items 245088.
I0302 19:02:01.960924 22535416901760 run.py:483] Algo bellman_ford step 7659 current loss 0.937947, current_train_items 245120.
I0302 19:02:01.980816 22535416901760 run.py:483] Algo bellman_ford step 7660 current loss 0.326789, current_train_items 245152.
I0302 19:02:01.997092 22535416901760 run.py:483] Algo bellman_ford step 7661 current loss 0.417239, current_train_items 245184.
I0302 19:02:02.020388 22535416901760 run.py:483] Algo bellman_ford step 7662 current loss 0.530080, current_train_items 245216.
I0302 19:02:02.051184 22535416901760 run.py:483] Algo bellman_ford step 7663 current loss 0.585500, current_train_items 245248.
I0302 19:02:02.084689 22535416901760 run.py:483] Algo bellman_ford step 7664 current loss 0.712919, current_train_items 245280.
I0302 19:02:02.104137 22535416901760 run.py:483] Algo bellman_ford step 7665 current loss 0.340198, current_train_items 245312.
I0302 19:02:02.120252 22535416901760 run.py:483] Algo bellman_ford step 7666 current loss 0.452856, current_train_items 245344.
I0302 19:02:02.143398 22535416901760 run.py:483] Algo bellman_ford step 7667 current loss 0.581770, current_train_items 245376.
I0302 19:02:02.176990 22535416901760 run.py:483] Algo bellman_ford step 7668 current loss 0.598539, current_train_items 245408.
I0302 19:02:02.211147 22535416901760 run.py:483] Algo bellman_ford step 7669 current loss 0.702445, current_train_items 245440.
I0302 19:02:02.230981 22535416901760 run.py:483] Algo bellman_ford step 7670 current loss 0.300518, current_train_items 245472.
I0302 19:02:02.247501 22535416901760 run.py:483] Algo bellman_ford step 7671 current loss 0.445735, current_train_items 245504.
I0302 19:02:02.270584 22535416901760 run.py:483] Algo bellman_ford step 7672 current loss 0.604154, current_train_items 245536.
I0302 19:02:02.302387 22535416901760 run.py:483] Algo bellman_ford step 7673 current loss 0.723338, current_train_items 245568.
I0302 19:02:02.336579 22535416901760 run.py:483] Algo bellman_ford step 7674 current loss 0.789028, current_train_items 245600.
I0302 19:02:02.356302 22535416901760 run.py:483] Algo bellman_ford step 7675 current loss 0.276321, current_train_items 245632.
I0302 19:02:02.372954 22535416901760 run.py:483] Algo bellman_ford step 7676 current loss 0.487571, current_train_items 245664.
I0302 19:02:02.397148 22535416901760 run.py:483] Algo bellman_ford step 7677 current loss 0.603444, current_train_items 245696.
I0302 19:02:02.428120 22535416901760 run.py:483] Algo bellman_ford step 7678 current loss 0.556701, current_train_items 245728.
I0302 19:02:02.462607 22535416901760 run.py:483] Algo bellman_ford step 7679 current loss 0.702632, current_train_items 245760.
I0302 19:02:02.482330 22535416901760 run.py:483] Algo bellman_ford step 7680 current loss 0.291950, current_train_items 245792.
I0302 19:02:02.498594 22535416901760 run.py:483] Algo bellman_ford step 7681 current loss 0.558988, current_train_items 245824.
I0302 19:02:02.521842 22535416901760 run.py:483] Algo bellman_ford step 7682 current loss 0.552094, current_train_items 245856.
I0302 19:02:02.553737 22535416901760 run.py:483] Algo bellman_ford step 7683 current loss 0.699209, current_train_items 245888.
I0302 19:02:02.588414 22535416901760 run.py:483] Algo bellman_ford step 7684 current loss 0.745679, current_train_items 245920.
I0302 19:02:02.608332 22535416901760 run.py:483] Algo bellman_ford step 7685 current loss 0.291056, current_train_items 245952.
I0302 19:02:02.624684 22535416901760 run.py:483] Algo bellman_ford step 7686 current loss 0.515669, current_train_items 245984.
I0302 19:02:02.647324 22535416901760 run.py:483] Algo bellman_ford step 7687 current loss 0.528057, current_train_items 246016.
I0302 19:02:02.679497 22535416901760 run.py:483] Algo bellman_ford step 7688 current loss 0.589212, current_train_items 246048.
I0302 19:02:02.713115 22535416901760 run.py:483] Algo bellman_ford step 7689 current loss 0.678028, current_train_items 246080.
I0302 19:02:02.733065 22535416901760 run.py:483] Algo bellman_ford step 7690 current loss 0.255032, current_train_items 246112.
I0302 19:02:02.749307 22535416901760 run.py:483] Algo bellman_ford step 7691 current loss 0.405254, current_train_items 246144.
I0302 19:02:02.772583 22535416901760 run.py:483] Algo bellman_ford step 7692 current loss 0.559539, current_train_items 246176.
I0302 19:02:02.805459 22535416901760 run.py:483] Algo bellman_ford step 7693 current loss 0.618109, current_train_items 246208.
I0302 19:02:02.838541 22535416901760 run.py:483] Algo bellman_ford step 7694 current loss 0.647738, current_train_items 246240.
I0302 19:02:02.858183 22535416901760 run.py:483] Algo bellman_ford step 7695 current loss 0.275241, current_train_items 246272.
I0302 19:02:02.874415 22535416901760 run.py:483] Algo bellman_ford step 7696 current loss 0.484681, current_train_items 246304.
I0302 19:02:02.898861 22535416901760 run.py:483] Algo bellman_ford step 7697 current loss 0.602226, current_train_items 246336.
I0302 19:02:02.931208 22535416901760 run.py:483] Algo bellman_ford step 7698 current loss 0.754564, current_train_items 246368.
I0302 19:02:02.966403 22535416901760 run.py:483] Algo bellman_ford step 7699 current loss 0.830171, current_train_items 246400.
I0302 19:02:02.986821 22535416901760 run.py:483] Algo bellman_ford step 7700 current loss 0.365601, current_train_items 246432.
I0302 19:02:02.994511 22535416901760 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.9091796875, 'score': 0.9091796875, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:02:02.994619 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.909, val scores are: bellman_ford: 0.909
I0302 19:02:03.011840 22535416901760 run.py:483] Algo bellman_ford step 7701 current loss 0.518012, current_train_items 246464.
I0302 19:02:03.035940 22535416901760 run.py:483] Algo bellman_ford step 7702 current loss 0.609182, current_train_items 246496.
I0302 19:02:03.066842 22535416901760 run.py:483] Algo bellman_ford step 7703 current loss 0.591718, current_train_items 246528.
I0302 19:02:03.100149 22535416901760 run.py:483] Algo bellman_ford step 7704 current loss 0.845998, current_train_items 246560.
I0302 19:02:03.120093 22535416901760 run.py:483] Algo bellman_ford step 7705 current loss 0.290279, current_train_items 246592.
I0302 19:02:03.135997 22535416901760 run.py:483] Algo bellman_ford step 7706 current loss 0.377047, current_train_items 246624.
I0302 19:02:03.161098 22535416901760 run.py:483] Algo bellman_ford step 7707 current loss 0.561464, current_train_items 246656.
I0302 19:02:03.191916 22535416901760 run.py:483] Algo bellman_ford step 7708 current loss 0.580272, current_train_items 246688.
I0302 19:02:03.224284 22535416901760 run.py:483] Algo bellman_ford step 7709 current loss 0.656474, current_train_items 246720.
I0302 19:02:03.243868 22535416901760 run.py:483] Algo bellman_ford step 7710 current loss 0.330637, current_train_items 246752.
I0302 19:02:03.260086 22535416901760 run.py:483] Algo bellman_ford step 7711 current loss 0.407883, current_train_items 246784.
I0302 19:02:03.283984 22535416901760 run.py:483] Algo bellman_ford step 7712 current loss 0.631926, current_train_items 246816.
I0302 19:02:03.314379 22535416901760 run.py:483] Algo bellman_ford step 7713 current loss 0.705938, current_train_items 246848.
I0302 19:02:03.347716 22535416901760 run.py:483] Algo bellman_ford step 7714 current loss 0.630605, current_train_items 246880.
I0302 19:02:03.367340 22535416901760 run.py:483] Algo bellman_ford step 7715 current loss 0.325583, current_train_items 246912.
I0302 19:02:03.383341 22535416901760 run.py:483] Algo bellman_ford step 7716 current loss 0.408951, current_train_items 246944.
I0302 19:02:03.407561 22535416901760 run.py:483] Algo bellman_ford step 7717 current loss 0.584909, current_train_items 246976.
I0302 19:02:03.438833 22535416901760 run.py:483] Algo bellman_ford step 7718 current loss 0.725426, current_train_items 247008.
I0302 19:02:03.471052 22535416901760 run.py:483] Algo bellman_ford step 7719 current loss 0.755035, current_train_items 247040.
I0302 19:02:03.490674 22535416901760 run.py:483] Algo bellman_ford step 7720 current loss 0.320052, current_train_items 247072.
I0302 19:02:03.506669 22535416901760 run.py:483] Algo bellman_ford step 7721 current loss 0.423054, current_train_items 247104.
I0302 19:02:03.531203 22535416901760 run.py:483] Algo bellman_ford step 7722 current loss 0.564945, current_train_items 247136.
I0302 19:02:03.563661 22535416901760 run.py:483] Algo bellman_ford step 7723 current loss 0.624890, current_train_items 247168.
I0302 19:02:03.597925 22535416901760 run.py:483] Algo bellman_ford step 7724 current loss 0.701430, current_train_items 247200.
I0302 19:02:03.617670 22535416901760 run.py:483] Algo bellman_ford step 7725 current loss 0.294898, current_train_items 247232.
I0302 19:02:03.633769 22535416901760 run.py:483] Algo bellman_ford step 7726 current loss 0.470925, current_train_items 247264.
I0302 19:02:03.656497 22535416901760 run.py:483] Algo bellman_ford step 7727 current loss 0.555451, current_train_items 247296.
I0302 19:02:03.687595 22535416901760 run.py:483] Algo bellman_ford step 7728 current loss 0.638325, current_train_items 247328.
I0302 19:02:03.720131 22535416901760 run.py:483] Algo bellman_ford step 7729 current loss 0.700418, current_train_items 247360.
I0302 19:02:03.739913 22535416901760 run.py:483] Algo bellman_ford step 7730 current loss 0.281583, current_train_items 247392.
I0302 19:02:03.755787 22535416901760 run.py:483] Algo bellman_ford step 7731 current loss 0.439291, current_train_items 247424.
I0302 19:02:03.779342 22535416901760 run.py:483] Algo bellman_ford step 7732 current loss 0.511050, current_train_items 247456.
I0302 19:02:03.810536 22535416901760 run.py:483] Algo bellman_ford step 7733 current loss 0.599708, current_train_items 247488.
I0302 19:02:03.842449 22535416901760 run.py:483] Algo bellman_ford step 7734 current loss 0.640776, current_train_items 247520.
I0302 19:02:03.862080 22535416901760 run.py:483] Algo bellman_ford step 7735 current loss 0.255834, current_train_items 247552.
I0302 19:02:03.878260 22535416901760 run.py:483] Algo bellman_ford step 7736 current loss 0.583593, current_train_items 247584.
I0302 19:02:03.902195 22535416901760 run.py:483] Algo bellman_ford step 7737 current loss 0.525273, current_train_items 247616.
I0302 19:02:03.932944 22535416901760 run.py:483] Algo bellman_ford step 7738 current loss 0.685359, current_train_items 247648.
I0302 19:02:03.966812 22535416901760 run.py:483] Algo bellman_ford step 7739 current loss 0.741279, current_train_items 247680.
I0302 19:02:03.986256 22535416901760 run.py:483] Algo bellman_ford step 7740 current loss 0.233550, current_train_items 247712.
I0302 19:02:04.002619 22535416901760 run.py:483] Algo bellman_ford step 7741 current loss 0.424876, current_train_items 247744.
I0302 19:02:04.027666 22535416901760 run.py:483] Algo bellman_ford step 7742 current loss 0.716863, current_train_items 247776.
I0302 19:02:04.059576 22535416901760 run.py:483] Algo bellman_ford step 7743 current loss 0.779624, current_train_items 247808.
I0302 19:02:04.093391 22535416901760 run.py:483] Algo bellman_ford step 7744 current loss 0.817927, current_train_items 247840.
I0302 19:02:04.112832 22535416901760 run.py:483] Algo bellman_ford step 7745 current loss 0.283118, current_train_items 247872.
I0302 19:02:04.128785 22535416901760 run.py:483] Algo bellman_ford step 7746 current loss 0.416736, current_train_items 247904.
I0302 19:02:04.152474 22535416901760 run.py:483] Algo bellman_ford step 7747 current loss 0.658395, current_train_items 247936.
I0302 19:02:04.185298 22535416901760 run.py:483] Algo bellman_ford step 7748 current loss 0.793662, current_train_items 247968.
I0302 19:02:04.219386 22535416901760 run.py:483] Algo bellman_ford step 7749 current loss 0.709374, current_train_items 248000.
I0302 19:02:04.239182 22535416901760 run.py:483] Algo bellman_ford step 7750 current loss 0.293337, current_train_items 248032.
I0302 19:02:04.247375 22535416901760 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.935546875, 'score': 0.935546875, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:02:04.247481 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.936, val scores are: bellman_ford: 0.936
I0302 19:02:04.264444 22535416901760 run.py:483] Algo bellman_ford step 7751 current loss 0.493243, current_train_items 248064.
I0302 19:02:04.288757 22535416901760 run.py:483] Algo bellman_ford step 7752 current loss 0.648973, current_train_items 248096.
I0302 19:02:04.320459 22535416901760 run.py:483] Algo bellman_ford step 7753 current loss 0.606568, current_train_items 248128.
I0302 19:02:04.355893 22535416901760 run.py:483] Algo bellman_ford step 7754 current loss 0.846589, current_train_items 248160.
I0302 19:02:04.376022 22535416901760 run.py:483] Algo bellman_ford step 7755 current loss 0.304705, current_train_items 248192.
I0302 19:02:04.391439 22535416901760 run.py:483] Algo bellman_ford step 7756 current loss 0.474839, current_train_items 248224.
I0302 19:02:04.415349 22535416901760 run.py:483] Algo bellman_ford step 7757 current loss 0.572145, current_train_items 248256.
I0302 19:02:04.445628 22535416901760 run.py:483] Algo bellman_ford step 7758 current loss 0.624996, current_train_items 248288.
I0302 19:02:04.478928 22535416901760 run.py:483] Algo bellman_ford step 7759 current loss 0.705699, current_train_items 248320.
I0302 19:02:04.498744 22535416901760 run.py:483] Algo bellman_ford step 7760 current loss 0.320394, current_train_items 248352.
I0302 19:02:04.514851 22535416901760 run.py:483] Algo bellman_ford step 7761 current loss 0.530086, current_train_items 248384.
I0302 19:02:04.538931 22535416901760 run.py:483] Algo bellman_ford step 7762 current loss 0.540508, current_train_items 248416.
I0302 19:02:04.570078 22535416901760 run.py:483] Algo bellman_ford step 7763 current loss 0.728670, current_train_items 248448.
I0302 19:02:04.602152 22535416901760 run.py:483] Algo bellman_ford step 7764 current loss 0.683807, current_train_items 248480.
I0302 19:02:04.621706 22535416901760 run.py:483] Algo bellman_ford step 7765 current loss 0.334758, current_train_items 248512.
I0302 19:02:04.637841 22535416901760 run.py:483] Algo bellman_ford step 7766 current loss 0.425915, current_train_items 248544.
I0302 19:02:04.660993 22535416901760 run.py:483] Algo bellman_ford step 7767 current loss 0.546184, current_train_items 248576.
I0302 19:02:04.692345 22535416901760 run.py:483] Algo bellman_ford step 7768 current loss 0.721794, current_train_items 248608.
I0302 19:02:04.725723 22535416901760 run.py:483] Algo bellman_ford step 7769 current loss 0.835123, current_train_items 248640.
I0302 19:02:04.745806 22535416901760 run.py:483] Algo bellman_ford step 7770 current loss 0.318688, current_train_items 248672.
I0302 19:02:04.762160 22535416901760 run.py:483] Algo bellman_ford step 7771 current loss 0.426319, current_train_items 248704.
I0302 19:02:04.785474 22535416901760 run.py:483] Algo bellman_ford step 7772 current loss 0.527796, current_train_items 248736.
I0302 19:02:04.817455 22535416901760 run.py:483] Algo bellman_ford step 7773 current loss 0.642850, current_train_items 248768.
I0302 19:02:04.850508 22535416901760 run.py:483] Algo bellman_ford step 7774 current loss 0.683742, current_train_items 248800.
I0302 19:02:04.870536 22535416901760 run.py:483] Algo bellman_ford step 7775 current loss 0.302598, current_train_items 248832.
I0302 19:02:04.886739 22535416901760 run.py:483] Algo bellman_ford step 7776 current loss 0.583462, current_train_items 248864.
I0302 19:02:04.911123 22535416901760 run.py:483] Algo bellman_ford step 7777 current loss 0.699898, current_train_items 248896.
I0302 19:02:04.942845 22535416901760 run.py:483] Algo bellman_ford step 7778 current loss 0.614075, current_train_items 248928.
I0302 19:02:04.978085 22535416901760 run.py:483] Algo bellman_ford step 7779 current loss 0.723008, current_train_items 248960.
I0302 19:02:04.997297 22535416901760 run.py:483] Algo bellman_ford step 7780 current loss 0.249358, current_train_items 248992.
I0302 19:02:05.013734 22535416901760 run.py:483] Algo bellman_ford step 7781 current loss 0.493305, current_train_items 249024.
I0302 19:02:05.037191 22535416901760 run.py:483] Algo bellman_ford step 7782 current loss 0.577130, current_train_items 249056.
I0302 19:02:05.069174 22535416901760 run.py:483] Algo bellman_ford step 7783 current loss 0.707451, current_train_items 249088.
I0302 19:02:05.103146 22535416901760 run.py:483] Algo bellman_ford step 7784 current loss 0.856275, current_train_items 249120.
I0302 19:02:05.122979 22535416901760 run.py:483] Algo bellman_ford step 7785 current loss 0.308201, current_train_items 249152.
I0302 19:02:05.139361 22535416901760 run.py:483] Algo bellman_ford step 7786 current loss 0.457031, current_train_items 249184.
I0302 19:02:05.162416 22535416901760 run.py:483] Algo bellman_ford step 7787 current loss 0.488406, current_train_items 249216.
I0302 19:02:05.193601 22535416901760 run.py:483] Algo bellman_ford step 7788 current loss 0.529081, current_train_items 249248.
I0302 19:02:05.224834 22535416901760 run.py:483] Algo bellman_ford step 7789 current loss 0.654861, current_train_items 249280.
I0302 19:02:05.245126 22535416901760 run.py:483] Algo bellman_ford step 7790 current loss 0.305304, current_train_items 249312.
I0302 19:02:05.261049 22535416901760 run.py:483] Algo bellman_ford step 7791 current loss 0.464251, current_train_items 249344.
I0302 19:02:05.284760 22535416901760 run.py:483] Algo bellman_ford step 7792 current loss 0.613949, current_train_items 249376.
I0302 19:02:05.315598 22535416901760 run.py:483] Algo bellman_ford step 7793 current loss 0.683497, current_train_items 249408.
I0302 19:02:05.348583 22535416901760 run.py:483] Algo bellman_ford step 7794 current loss 0.816671, current_train_items 249440.
I0302 19:02:05.368133 22535416901760 run.py:483] Algo bellman_ford step 7795 current loss 0.320710, current_train_items 249472.
I0302 19:02:05.383872 22535416901760 run.py:483] Algo bellman_ford step 7796 current loss 0.463309, current_train_items 249504.
I0302 19:02:05.406944 22535416901760 run.py:483] Algo bellman_ford step 7797 current loss 0.561007, current_train_items 249536.
I0302 19:02:05.437384 22535416901760 run.py:483] Algo bellman_ford step 7798 current loss 0.592187, current_train_items 249568.
I0302 19:02:05.472335 22535416901760 run.py:483] Algo bellman_ford step 7799 current loss 0.768013, current_train_items 249600.
I0302 19:02:05.492394 22535416901760 run.py:483] Algo bellman_ford step 7800 current loss 0.330586, current_train_items 249632.
I0302 19:02:05.500278 22535416901760 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:02:05.500387 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:02:05.517190 22535416901760 run.py:483] Algo bellman_ford step 7801 current loss 0.448623, current_train_items 249664.
I0302 19:02:05.541075 22535416901760 run.py:483] Algo bellman_ford step 7802 current loss 0.565937, current_train_items 249696.
I0302 19:02:05.573857 22535416901760 run.py:483] Algo bellman_ford step 7803 current loss 0.634498, current_train_items 249728.
I0302 19:02:05.609831 22535416901760 run.py:483] Algo bellman_ford step 7804 current loss 0.733793, current_train_items 249760.
I0302 19:02:05.629979 22535416901760 run.py:483] Algo bellman_ford step 7805 current loss 0.266835, current_train_items 249792.
I0302 19:02:05.646036 22535416901760 run.py:483] Algo bellman_ford step 7806 current loss 0.454133, current_train_items 249824.
I0302 19:02:05.668627 22535416901760 run.py:483] Algo bellman_ford step 7807 current loss 0.453824, current_train_items 249856.
I0302 19:02:05.698651 22535416901760 run.py:483] Algo bellman_ford step 7808 current loss 0.559811, current_train_items 249888.
I0302 19:02:05.732025 22535416901760 run.py:483] Algo bellman_ford step 7809 current loss 0.671341, current_train_items 249920.
I0302 19:02:05.751601 22535416901760 run.py:483] Algo bellman_ford step 7810 current loss 0.357098, current_train_items 249952.
I0302 19:02:05.768037 22535416901760 run.py:483] Algo bellman_ford step 7811 current loss 0.470692, current_train_items 249984.
I0302 19:02:05.791803 22535416901760 run.py:483] Algo bellman_ford step 7812 current loss 0.554219, current_train_items 250016.
I0302 19:02:05.823851 22535416901760 run.py:483] Algo bellman_ford step 7813 current loss 0.712813, current_train_items 250048.
I0302 19:02:05.857112 22535416901760 run.py:483] Algo bellman_ford step 7814 current loss 0.698909, current_train_items 250080.
I0302 19:02:05.876333 22535416901760 run.py:483] Algo bellman_ford step 7815 current loss 0.312847, current_train_items 250112.
I0302 19:02:05.892177 22535416901760 run.py:483] Algo bellman_ford step 7816 current loss 0.493085, current_train_items 250144.
I0302 19:02:05.916004 22535416901760 run.py:483] Algo bellman_ford step 7817 current loss 0.584775, current_train_items 250176.
I0302 19:02:05.945666 22535416901760 run.py:483] Algo bellman_ford step 7818 current loss 0.603981, current_train_items 250208.
I0302 19:02:05.977896 22535416901760 run.py:483] Algo bellman_ford step 7819 current loss 0.705151, current_train_items 250240.
I0302 19:02:05.997181 22535416901760 run.py:483] Algo bellman_ford step 7820 current loss 0.330131, current_train_items 250272.
I0302 19:02:06.013504 22535416901760 run.py:483] Algo bellman_ford step 7821 current loss 0.516127, current_train_items 250304.
I0302 19:02:06.038328 22535416901760 run.py:483] Algo bellman_ford step 7822 current loss 0.517961, current_train_items 250336.
I0302 19:02:06.071334 22535416901760 run.py:483] Algo bellman_ford step 7823 current loss 0.639755, current_train_items 250368.
I0302 19:02:06.104225 22535416901760 run.py:483] Algo bellman_ford step 7824 current loss 0.793855, current_train_items 250400.
I0302 19:02:06.123484 22535416901760 run.py:483] Algo bellman_ford step 7825 current loss 0.266289, current_train_items 250432.
I0302 19:02:06.139894 22535416901760 run.py:483] Algo bellman_ford step 7826 current loss 0.511389, current_train_items 250464.
I0302 19:02:06.163777 22535416901760 run.py:483] Algo bellman_ford step 7827 current loss 0.639462, current_train_items 250496.
I0302 19:02:06.195599 22535416901760 run.py:483] Algo bellman_ford step 7828 current loss 0.679770, current_train_items 250528.
I0302 19:02:06.229312 22535416901760 run.py:483] Algo bellman_ford step 7829 current loss 0.697734, current_train_items 250560.
I0302 19:02:06.248476 22535416901760 run.py:483] Algo bellman_ford step 7830 current loss 0.241804, current_train_items 250592.
I0302 19:02:06.264373 22535416901760 run.py:483] Algo bellman_ford step 7831 current loss 0.521798, current_train_items 250624.
I0302 19:02:06.287685 22535416901760 run.py:483] Algo bellman_ford step 7832 current loss 0.638361, current_train_items 250656.
I0302 19:02:06.319639 22535416901760 run.py:483] Algo bellman_ford step 7833 current loss 0.658994, current_train_items 250688.
I0302 19:02:06.351048 22535416901760 run.py:483] Algo bellman_ford step 7834 current loss 0.753295, current_train_items 250720.
I0302 19:02:06.370469 22535416901760 run.py:483] Algo bellman_ford step 7835 current loss 0.303463, current_train_items 250752.
I0302 19:02:06.386707 22535416901760 run.py:483] Algo bellman_ford step 7836 current loss 0.431660, current_train_items 250784.
I0302 19:02:06.410432 22535416901760 run.py:483] Algo bellman_ford step 7837 current loss 0.563595, current_train_items 250816.
I0302 19:02:06.442982 22535416901760 run.py:483] Algo bellman_ford step 7838 current loss 0.630182, current_train_items 250848.
I0302 19:02:06.477618 22535416901760 run.py:483] Algo bellman_ford step 7839 current loss 0.756333, current_train_items 250880.
I0302 19:02:06.496902 22535416901760 run.py:483] Algo bellman_ford step 7840 current loss 0.259765, current_train_items 250912.
I0302 19:02:06.513028 22535416901760 run.py:483] Algo bellman_ford step 7841 current loss 0.403868, current_train_items 250944.
I0302 19:02:06.536662 22535416901760 run.py:483] Algo bellman_ford step 7842 current loss 0.552176, current_train_items 250976.
I0302 19:02:06.569262 22535416901760 run.py:483] Algo bellman_ford step 7843 current loss 0.603071, current_train_items 251008.
I0302 19:02:06.603868 22535416901760 run.py:483] Algo bellman_ford step 7844 current loss 0.714362, current_train_items 251040.
I0302 19:02:06.623080 22535416901760 run.py:483] Algo bellman_ford step 7845 current loss 0.428745, current_train_items 251072.
I0302 19:02:06.639092 22535416901760 run.py:483] Algo bellman_ford step 7846 current loss 0.521561, current_train_items 251104.
I0302 19:02:06.662867 22535416901760 run.py:483] Algo bellman_ford step 7847 current loss 0.575081, current_train_items 251136.
I0302 19:02:06.694707 22535416901760 run.py:483] Algo bellman_ford step 7848 current loss 0.622959, current_train_items 251168.
I0302 19:02:06.728508 22535416901760 run.py:483] Algo bellman_ford step 7849 current loss 0.764795, current_train_items 251200.
I0302 19:02:06.747798 22535416901760 run.py:483] Algo bellman_ford step 7850 current loss 0.277626, current_train_items 251232.
I0302 19:02:06.756206 22535416901760 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:02:06.756316 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 19:02:06.773077 22535416901760 run.py:483] Algo bellman_ford step 7851 current loss 0.457616, current_train_items 251264.
I0302 19:02:06.797182 22535416901760 run.py:483] Algo bellman_ford step 7852 current loss 0.532052, current_train_items 251296.
I0302 19:02:06.829890 22535416901760 run.py:483] Algo bellman_ford step 7853 current loss 0.665402, current_train_items 251328.
I0302 19:02:06.866286 22535416901760 run.py:483] Algo bellman_ford step 7854 current loss 0.809592, current_train_items 251360.
I0302 19:02:06.885931 22535416901760 run.py:483] Algo bellman_ford step 7855 current loss 0.336661, current_train_items 251392.
I0302 19:02:06.902352 22535416901760 run.py:483] Algo bellman_ford step 7856 current loss 0.523655, current_train_items 251424.
I0302 19:02:06.926319 22535416901760 run.py:483] Algo bellman_ford step 7857 current loss 0.571712, current_train_items 251456.
I0302 19:02:06.957801 22535416901760 run.py:483] Algo bellman_ford step 7858 current loss 0.629219, current_train_items 251488.
I0302 19:02:06.992424 22535416901760 run.py:483] Algo bellman_ford step 7859 current loss 0.785278, current_train_items 251520.
I0302 19:02:07.012332 22535416901760 run.py:483] Algo bellman_ford step 7860 current loss 0.340085, current_train_items 251552.
I0302 19:02:07.028301 22535416901760 run.py:483] Algo bellman_ford step 7861 current loss 0.408809, current_train_items 251584.
I0302 19:02:07.051763 22535416901760 run.py:483] Algo bellman_ford step 7862 current loss 0.709610, current_train_items 251616.
I0302 19:02:07.083143 22535416901760 run.py:483] Algo bellman_ford step 7863 current loss 0.724826, current_train_items 251648.
I0302 19:02:07.115411 22535416901760 run.py:483] Algo bellman_ford step 7864 current loss 0.665899, current_train_items 251680.
I0302 19:02:07.135327 22535416901760 run.py:483] Algo bellman_ford step 7865 current loss 0.300682, current_train_items 251712.
I0302 19:02:07.151834 22535416901760 run.py:483] Algo bellman_ford step 7866 current loss 0.596221, current_train_items 251744.
I0302 19:02:07.176061 22535416901760 run.py:483] Algo bellman_ford step 7867 current loss 0.616986, current_train_items 251776.
I0302 19:02:07.206926 22535416901760 run.py:483] Algo bellman_ford step 7868 current loss 0.620558, current_train_items 251808.
I0302 19:02:07.239483 22535416901760 run.py:483] Algo bellman_ford step 7869 current loss 0.733942, current_train_items 251840.
I0302 19:02:07.259433 22535416901760 run.py:483] Algo bellman_ford step 7870 current loss 0.341656, current_train_items 251872.
I0302 19:02:07.275541 22535416901760 run.py:483] Algo bellman_ford step 7871 current loss 0.424761, current_train_items 251904.
I0302 19:02:07.299028 22535416901760 run.py:483] Algo bellman_ford step 7872 current loss 0.595977, current_train_items 251936.
I0302 19:02:07.329775 22535416901760 run.py:483] Algo bellman_ford step 7873 current loss 0.625255, current_train_items 251968.
I0302 19:02:07.363034 22535416901760 run.py:483] Algo bellman_ford step 7874 current loss 0.677169, current_train_items 252000.
I0302 19:02:07.382997 22535416901760 run.py:483] Algo bellman_ford step 7875 current loss 0.293164, current_train_items 252032.
I0302 19:02:07.398713 22535416901760 run.py:483] Algo bellman_ford step 7876 current loss 0.397041, current_train_items 252064.
I0302 19:02:07.421795 22535416901760 run.py:483] Algo bellman_ford step 7877 current loss 0.601698, current_train_items 252096.
I0302 19:02:07.452876 22535416901760 run.py:483] Algo bellman_ford step 7878 current loss 0.679505, current_train_items 252128.
I0302 19:02:07.486470 22535416901760 run.py:483] Algo bellman_ford step 7879 current loss 0.732707, current_train_items 252160.
I0302 19:02:07.505984 22535416901760 run.py:483] Algo bellman_ford step 7880 current loss 0.278591, current_train_items 252192.
I0302 19:02:07.521706 22535416901760 run.py:483] Algo bellman_ford step 7881 current loss 0.412560, current_train_items 252224.
I0302 19:02:07.544992 22535416901760 run.py:483] Algo bellman_ford step 7882 current loss 0.548714, current_train_items 252256.
I0302 19:02:07.576684 22535416901760 run.py:483] Algo bellman_ford step 7883 current loss 0.650257, current_train_items 252288.
I0302 19:02:07.610595 22535416901760 run.py:483] Algo bellman_ford step 7884 current loss 0.748835, current_train_items 252320.
I0302 19:02:07.630713 22535416901760 run.py:483] Algo bellman_ford step 7885 current loss 0.325711, current_train_items 252352.
I0302 19:02:07.646848 22535416901760 run.py:483] Algo bellman_ford step 7886 current loss 0.436177, current_train_items 252384.
I0302 19:02:07.669469 22535416901760 run.py:483] Algo bellman_ford step 7887 current loss 0.492981, current_train_items 252416.
I0302 19:02:07.699306 22535416901760 run.py:483] Algo bellman_ford step 7888 current loss 0.602803, current_train_items 252448.
I0302 19:02:07.732197 22535416901760 run.py:483] Algo bellman_ford step 7889 current loss 0.666204, current_train_items 252480.
I0302 19:02:07.751994 22535416901760 run.py:483] Algo bellman_ford step 7890 current loss 0.281856, current_train_items 252512.
I0302 19:02:07.768031 22535416901760 run.py:483] Algo bellman_ford step 7891 current loss 0.435353, current_train_items 252544.
I0302 19:02:07.791520 22535416901760 run.py:483] Algo bellman_ford step 7892 current loss 0.572174, current_train_items 252576.
I0302 19:02:07.823262 22535416901760 run.py:483] Algo bellman_ford step 7893 current loss 0.677225, current_train_items 252608.
I0302 19:02:07.856108 22535416901760 run.py:483] Algo bellman_ford step 7894 current loss 0.729865, current_train_items 252640.
I0302 19:02:07.875322 22535416901760 run.py:483] Algo bellman_ford step 7895 current loss 0.297543, current_train_items 252672.
I0302 19:02:07.891114 22535416901760 run.py:483] Algo bellman_ford step 7896 current loss 0.425248, current_train_items 252704.
I0302 19:02:07.915232 22535416901760 run.py:483] Algo bellman_ford step 7897 current loss 0.667721, current_train_items 252736.
I0302 19:02:07.947420 22535416901760 run.py:483] Algo bellman_ford step 7898 current loss 0.691696, current_train_items 252768.
I0302 19:02:07.981224 22535416901760 run.py:483] Algo bellman_ford step 7899 current loss 0.846350, current_train_items 252800.
I0302 19:02:08.001208 22535416901760 run.py:483] Algo bellman_ford step 7900 current loss 0.330199, current_train_items 252832.
I0302 19:02:08.009113 22535416901760 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:02:08.009232 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 19:02:08.025900 22535416901760 run.py:483] Algo bellman_ford step 7901 current loss 0.427382, current_train_items 252864.
I0302 19:02:08.050283 22535416901760 run.py:483] Algo bellman_ford step 7902 current loss 0.553471, current_train_items 252896.
I0302 19:02:08.082710 22535416901760 run.py:483] Algo bellman_ford step 7903 current loss 0.658917, current_train_items 252928.
I0302 19:02:08.118924 22535416901760 run.py:483] Algo bellman_ford step 7904 current loss 0.745543, current_train_items 252960.
I0302 19:02:08.138754 22535416901760 run.py:483] Algo bellman_ford step 7905 current loss 0.314484, current_train_items 252992.
I0302 19:02:08.155558 22535416901760 run.py:483] Algo bellman_ford step 7906 current loss 0.512274, current_train_items 253024.
I0302 19:02:08.180033 22535416901760 run.py:483] Algo bellman_ford step 7907 current loss 0.561470, current_train_items 253056.
I0302 19:02:08.210959 22535416901760 run.py:483] Algo bellman_ford step 7908 current loss 0.598048, current_train_items 253088.
I0302 19:02:08.242024 22535416901760 run.py:483] Algo bellman_ford step 7909 current loss 0.585789, current_train_items 253120.
I0302 19:02:08.261488 22535416901760 run.py:483] Algo bellman_ford step 7910 current loss 0.294506, current_train_items 253152.
I0302 19:02:08.278119 22535416901760 run.py:483] Algo bellman_ford step 7911 current loss 0.491700, current_train_items 253184.
I0302 19:02:08.302123 22535416901760 run.py:483] Algo bellman_ford step 7912 current loss 0.669758, current_train_items 253216.
I0302 19:02:08.333562 22535416901760 run.py:483] Algo bellman_ford step 7913 current loss 0.568890, current_train_items 253248.
I0302 19:02:08.367115 22535416901760 run.py:483] Algo bellman_ford step 7914 current loss 0.632781, current_train_items 253280.
I0302 19:02:08.386829 22535416901760 run.py:483] Algo bellman_ford step 7915 current loss 0.270076, current_train_items 253312.
I0302 19:02:08.403197 22535416901760 run.py:483] Algo bellman_ford step 7916 current loss 0.502399, current_train_items 253344.
I0302 19:02:08.426870 22535416901760 run.py:483] Algo bellman_ford step 7917 current loss 0.588131, current_train_items 253376.
I0302 19:02:08.458119 22535416901760 run.py:483] Algo bellman_ford step 7918 current loss 0.614561, current_train_items 253408.
I0302 19:02:08.493234 22535416901760 run.py:483] Algo bellman_ford step 7919 current loss 0.729849, current_train_items 253440.
I0302 19:02:08.512695 22535416901760 run.py:483] Algo bellman_ford step 7920 current loss 0.265313, current_train_items 253472.
I0302 19:02:08.529040 22535416901760 run.py:483] Algo bellman_ford step 7921 current loss 0.451851, current_train_items 253504.
I0302 19:02:08.553012 22535416901760 run.py:483] Algo bellman_ford step 7922 current loss 0.624093, current_train_items 253536.
I0302 19:02:08.583594 22535416901760 run.py:483] Algo bellman_ford step 7923 current loss 0.560074, current_train_items 253568.
I0302 19:02:08.617255 22535416901760 run.py:483] Algo bellman_ford step 7924 current loss 0.744163, current_train_items 253600.
I0302 19:02:08.636413 22535416901760 run.py:483] Algo bellman_ford step 7925 current loss 0.339391, current_train_items 253632.
I0302 19:02:08.652618 22535416901760 run.py:483] Algo bellman_ford step 7926 current loss 0.446202, current_train_items 253664.
I0302 19:02:08.676143 22535416901760 run.py:483] Algo bellman_ford step 7927 current loss 0.631452, current_train_items 253696.
I0302 19:02:08.708600 22535416901760 run.py:483] Algo bellman_ford step 7928 current loss 0.841244, current_train_items 253728.
I0302 19:02:08.741244 22535416901760 run.py:483] Algo bellman_ford step 7929 current loss 0.801571, current_train_items 253760.
I0302 19:02:08.760810 22535416901760 run.py:483] Algo bellman_ford step 7930 current loss 0.312907, current_train_items 253792.
I0302 19:02:08.776969 22535416901760 run.py:483] Algo bellman_ford step 7931 current loss 0.405838, current_train_items 253824.
I0302 19:02:08.799572 22535416901760 run.py:483] Algo bellman_ford step 7932 current loss 0.466905, current_train_items 253856.
I0302 19:02:08.831170 22535416901760 run.py:483] Algo bellman_ford step 7933 current loss 0.648166, current_train_items 253888.
I0302 19:02:08.865084 22535416901760 run.py:483] Algo bellman_ford step 7934 current loss 0.681554, current_train_items 253920.
I0302 19:02:08.884524 22535416901760 run.py:483] Algo bellman_ford step 7935 current loss 0.301312, current_train_items 253952.
I0302 19:02:08.900309 22535416901760 run.py:483] Algo bellman_ford step 7936 current loss 0.429142, current_train_items 253984.
I0302 19:02:08.924430 22535416901760 run.py:483] Algo bellman_ford step 7937 current loss 0.472845, current_train_items 254016.
I0302 19:02:08.955091 22535416901760 run.py:483] Algo bellman_ford step 7938 current loss 0.586141, current_train_items 254048.
I0302 19:02:08.989381 22535416901760 run.py:483] Algo bellman_ford step 7939 current loss 0.785034, current_train_items 254080.
I0302 19:02:09.008736 22535416901760 run.py:483] Algo bellman_ford step 7940 current loss 0.305760, current_train_items 254112.
I0302 19:02:09.024312 22535416901760 run.py:483] Algo bellman_ford step 7941 current loss 0.445378, current_train_items 254144.
I0302 19:02:09.048789 22535416901760 run.py:483] Algo bellman_ford step 7942 current loss 0.551361, current_train_items 254176.
I0302 19:02:09.081542 22535416901760 run.py:483] Algo bellman_ford step 7943 current loss 0.668598, current_train_items 254208.
I0302 19:02:09.114198 22535416901760 run.py:483] Algo bellman_ford step 7944 current loss 0.784140, current_train_items 254240.
I0302 19:02:09.133511 22535416901760 run.py:483] Algo bellman_ford step 7945 current loss 0.279104, current_train_items 254272.
I0302 19:02:09.149192 22535416901760 run.py:483] Algo bellman_ford step 7946 current loss 0.449063, current_train_items 254304.
I0302 19:02:09.171880 22535416901760 run.py:483] Algo bellman_ford step 7947 current loss 0.639905, current_train_items 254336.
I0302 19:02:09.201705 22535416901760 run.py:483] Algo bellman_ford step 7948 current loss 0.654583, current_train_items 254368.
I0302 19:02:09.235665 22535416901760 run.py:483] Algo bellman_ford step 7949 current loss 0.840329, current_train_items 254400.
I0302 19:02:09.254875 22535416901760 run.py:483] Algo bellman_ford step 7950 current loss 0.303132, current_train_items 254432.
I0302 19:02:09.263003 22535416901760 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:02:09.263110 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:09.280043 22535416901760 run.py:483] Algo bellman_ford step 7951 current loss 0.540306, current_train_items 254464.
I0302 19:02:09.304721 22535416901760 run.py:483] Algo bellman_ford step 7952 current loss 0.566541, current_train_items 254496.
I0302 19:02:09.337457 22535416901760 run.py:483] Algo bellman_ford step 7953 current loss 0.672792, current_train_items 254528.
I0302 19:02:09.370778 22535416901760 run.py:483] Algo bellman_ford step 7954 current loss 0.590799, current_train_items 254560.
I0302 19:02:09.390823 22535416901760 run.py:483] Algo bellman_ford step 7955 current loss 0.384892, current_train_items 254592.
I0302 19:02:09.406970 22535416901760 run.py:483] Algo bellman_ford step 7956 current loss 0.577136, current_train_items 254624.
I0302 19:02:09.431117 22535416901760 run.py:483] Algo bellman_ford step 7957 current loss 0.559760, current_train_items 254656.
I0302 19:02:09.462543 22535416901760 run.py:483] Algo bellman_ford step 7958 current loss 0.658748, current_train_items 254688.
I0302 19:02:09.495643 22535416901760 run.py:483] Algo bellman_ford step 7959 current loss 0.642320, current_train_items 254720.
I0302 19:02:09.515427 22535416901760 run.py:483] Algo bellman_ford step 7960 current loss 0.276049, current_train_items 254752.
I0302 19:02:09.531937 22535416901760 run.py:483] Algo bellman_ford step 7961 current loss 0.411843, current_train_items 254784.
I0302 19:02:09.555143 22535416901760 run.py:483] Algo bellman_ford step 7962 current loss 0.569821, current_train_items 254816.
I0302 19:02:09.585953 22535416901760 run.py:483] Algo bellman_ford step 7963 current loss 0.583512, current_train_items 254848.
I0302 19:02:09.619840 22535416901760 run.py:483] Algo bellman_ford step 7964 current loss 0.682396, current_train_items 254880.
I0302 19:02:09.639214 22535416901760 run.py:483] Algo bellman_ford step 7965 current loss 0.277237, current_train_items 254912.
I0302 19:02:09.655642 22535416901760 run.py:483] Algo bellman_ford step 7966 current loss 0.538366, current_train_items 254944.
I0302 19:02:09.680411 22535416901760 run.py:483] Algo bellman_ford step 7967 current loss 0.673635, current_train_items 254976.
I0302 19:02:09.711956 22535416901760 run.py:483] Algo bellman_ford step 7968 current loss 0.589159, current_train_items 255008.
I0302 19:02:09.744058 22535416901760 run.py:483] Algo bellman_ford step 7969 current loss 0.743026, current_train_items 255040.
I0302 19:02:09.763745 22535416901760 run.py:483] Algo bellman_ford step 7970 current loss 0.414736, current_train_items 255072.
I0302 19:02:09.779840 22535416901760 run.py:483] Algo bellman_ford step 7971 current loss 0.573563, current_train_items 255104.
I0302 19:02:09.803883 22535416901760 run.py:483] Algo bellman_ford step 7972 current loss 0.617515, current_train_items 255136.
I0302 19:02:09.834866 22535416901760 run.py:483] Algo bellman_ford step 7973 current loss 0.571410, current_train_items 255168.
I0302 19:02:09.869693 22535416901760 run.py:483] Algo bellman_ford step 7974 current loss 0.782553, current_train_items 255200.
I0302 19:02:09.889735 22535416901760 run.py:483] Algo bellman_ford step 7975 current loss 0.315449, current_train_items 255232.
I0302 19:02:09.906043 22535416901760 run.py:483] Algo bellman_ford step 7976 current loss 0.417368, current_train_items 255264.
I0302 19:02:09.930093 22535416901760 run.py:483] Algo bellman_ford step 7977 current loss 0.619391, current_train_items 255296.
I0302 19:02:09.961424 22535416901760 run.py:483] Algo bellman_ford step 7978 current loss 0.692323, current_train_items 255328.
I0302 19:02:09.993031 22535416901760 run.py:483] Algo bellman_ford step 7979 current loss 0.675298, current_train_items 255360.
I0302 19:02:10.012535 22535416901760 run.py:483] Algo bellman_ford step 7980 current loss 0.295690, current_train_items 255392.
I0302 19:02:10.028490 22535416901760 run.py:483] Algo bellman_ford step 7981 current loss 0.475566, current_train_items 255424.
I0302 19:02:10.051168 22535416901760 run.py:483] Algo bellman_ford step 7982 current loss 0.510382, current_train_items 255456.
I0302 19:02:10.081899 22535416901760 run.py:483] Algo bellman_ford step 7983 current loss 0.551520, current_train_items 255488.
I0302 19:02:10.115870 22535416901760 run.py:483] Algo bellman_ford step 7984 current loss 0.718531, current_train_items 255520.
I0302 19:02:10.135436 22535416901760 run.py:483] Algo bellman_ford step 7985 current loss 0.234900, current_train_items 255552.
I0302 19:02:10.151846 22535416901760 run.py:483] Algo bellman_ford step 7986 current loss 0.507049, current_train_items 255584.
I0302 19:02:10.174638 22535416901760 run.py:483] Algo bellman_ford step 7987 current loss 0.493932, current_train_items 255616.
I0302 19:02:10.206271 22535416901760 run.py:483] Algo bellman_ford step 7988 current loss 0.553587, current_train_items 255648.
I0302 19:02:10.238986 22535416901760 run.py:483] Algo bellman_ford step 7989 current loss 0.815574, current_train_items 255680.
I0302 19:02:10.258784 22535416901760 run.py:483] Algo bellman_ford step 7990 current loss 0.345684, current_train_items 255712.
I0302 19:02:10.275055 22535416901760 run.py:483] Algo bellman_ford step 7991 current loss 0.538461, current_train_items 255744.
I0302 19:02:10.298747 22535416901760 run.py:483] Algo bellman_ford step 7992 current loss 0.562472, current_train_items 255776.
I0302 19:02:10.332240 22535416901760 run.py:483] Algo bellman_ford step 7993 current loss 0.672696, current_train_items 255808.
I0302 19:02:10.366762 22535416901760 run.py:483] Algo bellman_ford step 7994 current loss 0.711055, current_train_items 255840.
I0302 19:02:10.386371 22535416901760 run.py:483] Algo bellman_ford step 7995 current loss 0.339915, current_train_items 255872.
I0302 19:02:10.402060 22535416901760 run.py:483] Algo bellman_ford step 7996 current loss 0.391609, current_train_items 255904.
I0302 19:02:10.425473 22535416901760 run.py:483] Algo bellman_ford step 7997 current loss 0.534189, current_train_items 255936.
I0302 19:02:10.456634 22535416901760 run.py:483] Algo bellman_ford step 7998 current loss 0.616273, current_train_items 255968.
I0302 19:02:10.488033 22535416901760 run.py:483] Algo bellman_ford step 7999 current loss 0.726479, current_train_items 256000.
I0302 19:02:10.507935 22535416901760 run.py:483] Algo bellman_ford step 8000 current loss 0.325192, current_train_items 256032.
I0302 19:02:10.515961 22535416901760 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:02:10.516072 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:02:10.533086 22535416901760 run.py:483] Algo bellman_ford step 8001 current loss 0.460996, current_train_items 256064.
I0302 19:02:10.556455 22535416901760 run.py:483] Algo bellman_ford step 8002 current loss 0.616630, current_train_items 256096.
I0302 19:02:10.588677 22535416901760 run.py:483] Algo bellman_ford step 8003 current loss 0.586586, current_train_items 256128.
I0302 19:02:10.621297 22535416901760 run.py:483] Algo bellman_ford step 8004 current loss 0.544305, current_train_items 256160.
I0302 19:02:10.641220 22535416901760 run.py:483] Algo bellman_ford step 8005 current loss 0.272204, current_train_items 256192.
I0302 19:02:10.657244 22535416901760 run.py:483] Algo bellman_ford step 8006 current loss 0.438727, current_train_items 256224.
I0302 19:02:10.681087 22535416901760 run.py:483] Algo bellman_ford step 8007 current loss 0.609654, current_train_items 256256.
I0302 19:02:10.713712 22535416901760 run.py:483] Algo bellman_ford step 8008 current loss 0.645529, current_train_items 256288.
I0302 19:02:10.747261 22535416901760 run.py:483] Algo bellman_ford step 8009 current loss 0.728912, current_train_items 256320.
I0302 19:02:10.766664 22535416901760 run.py:483] Algo bellman_ford step 8010 current loss 0.303640, current_train_items 256352.
I0302 19:02:10.782663 22535416901760 run.py:483] Algo bellman_ford step 8011 current loss 0.437440, current_train_items 256384.
I0302 19:02:10.806887 22535416901760 run.py:483] Algo bellman_ford step 8012 current loss 0.538309, current_train_items 256416.
I0302 19:02:10.837707 22535416901760 run.py:483] Algo bellman_ford step 8013 current loss 0.644293, current_train_items 256448.
I0302 19:02:10.869000 22535416901760 run.py:483] Algo bellman_ford step 8014 current loss 0.616530, current_train_items 256480.
I0302 19:02:10.888338 22535416901760 run.py:483] Algo bellman_ford step 8015 current loss 0.352293, current_train_items 256512.
I0302 19:02:10.904952 22535416901760 run.py:483] Algo bellman_ford step 8016 current loss 0.457603, current_train_items 256544.
I0302 19:02:10.927972 22535416901760 run.py:483] Algo bellman_ford step 8017 current loss 0.526131, current_train_items 256576.
I0302 19:02:10.958992 22535416901760 run.py:483] Algo bellman_ford step 8018 current loss 0.500757, current_train_items 256608.
I0302 19:02:10.990853 22535416901760 run.py:483] Algo bellman_ford step 8019 current loss 0.635684, current_train_items 256640.
I0302 19:02:11.010029 22535416901760 run.py:483] Algo bellman_ford step 8020 current loss 0.319042, current_train_items 256672.
I0302 19:02:11.026201 22535416901760 run.py:483] Algo bellman_ford step 8021 current loss 0.478274, current_train_items 256704.
I0302 19:02:11.049317 22535416901760 run.py:483] Algo bellman_ford step 8022 current loss 0.524616, current_train_items 256736.
I0302 19:02:11.079883 22535416901760 run.py:483] Algo bellman_ford step 8023 current loss 0.626501, current_train_items 256768.
I0302 19:02:11.114620 22535416901760 run.py:483] Algo bellman_ford step 8024 current loss 0.739263, current_train_items 256800.
I0302 19:02:11.133793 22535416901760 run.py:483] Algo bellman_ford step 8025 current loss 0.413134, current_train_items 256832.
I0302 19:02:11.150144 22535416901760 run.py:483] Algo bellman_ford step 8026 current loss 0.478260, current_train_items 256864.
I0302 19:02:11.173794 22535416901760 run.py:483] Algo bellman_ford step 8027 current loss 0.610989, current_train_items 256896.
I0302 19:02:11.203937 22535416901760 run.py:483] Algo bellman_ford step 8028 current loss 0.589282, current_train_items 256928.
I0302 19:02:11.237209 22535416901760 run.py:483] Algo bellman_ford step 8029 current loss 0.598078, current_train_items 256960.
I0302 19:02:11.256561 22535416901760 run.py:483] Algo bellman_ford step 8030 current loss 0.305734, current_train_items 256992.
I0302 19:02:11.272553 22535416901760 run.py:483] Algo bellman_ford step 8031 current loss 0.361311, current_train_items 257024.
I0302 19:02:11.296546 22535416901760 run.py:483] Algo bellman_ford step 8032 current loss 0.589728, current_train_items 257056.
I0302 19:02:11.328575 22535416901760 run.py:483] Algo bellman_ford step 8033 current loss 0.621385, current_train_items 257088.
I0302 19:02:11.362851 22535416901760 run.py:483] Algo bellman_ford step 8034 current loss 0.678635, current_train_items 257120.
I0302 19:02:11.382317 22535416901760 run.py:483] Algo bellman_ford step 8035 current loss 0.286466, current_train_items 257152.
I0302 19:02:11.398945 22535416901760 run.py:483] Algo bellman_ford step 8036 current loss 0.434964, current_train_items 257184.
I0302 19:02:11.422524 22535416901760 run.py:483] Algo bellman_ford step 8037 current loss 0.535933, current_train_items 257216.
I0302 19:02:11.453370 22535416901760 run.py:483] Algo bellman_ford step 8038 current loss 0.618984, current_train_items 257248.
I0302 19:02:11.487960 22535416901760 run.py:483] Algo bellman_ford step 8039 current loss 0.765551, current_train_items 257280.
I0302 19:02:11.507338 22535416901760 run.py:483] Algo bellman_ford step 8040 current loss 0.302800, current_train_items 257312.
I0302 19:02:11.523635 22535416901760 run.py:483] Algo bellman_ford step 8041 current loss 0.487501, current_train_items 257344.
I0302 19:02:11.547407 22535416901760 run.py:483] Algo bellman_ford step 8042 current loss 0.557598, current_train_items 257376.
I0302 19:02:11.578801 22535416901760 run.py:483] Algo bellman_ford step 8043 current loss 0.587972, current_train_items 257408.
I0302 19:02:11.612641 22535416901760 run.py:483] Algo bellman_ford step 8044 current loss 0.631008, current_train_items 257440.
I0302 19:02:11.632273 22535416901760 run.py:483] Algo bellman_ford step 8045 current loss 0.258153, current_train_items 257472.
I0302 19:02:11.648859 22535416901760 run.py:483] Algo bellman_ford step 8046 current loss 0.551489, current_train_items 257504.
I0302 19:02:11.671994 22535416901760 run.py:483] Algo bellman_ford step 8047 current loss 0.592855, current_train_items 257536.
I0302 19:02:11.703483 22535416901760 run.py:483] Algo bellman_ford step 8048 current loss 0.783995, current_train_items 257568.
I0302 19:02:11.735321 22535416901760 run.py:483] Algo bellman_ford step 8049 current loss 0.595876, current_train_items 257600.
I0302 19:02:11.754631 22535416901760 run.py:483] Algo bellman_ford step 8050 current loss 0.288190, current_train_items 257632.
I0302 19:02:11.762874 22535416901760 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:02:11.762981 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:11.779944 22535416901760 run.py:483] Algo bellman_ford step 8051 current loss 0.457911, current_train_items 257664.
I0302 19:02:11.803865 22535416901760 run.py:483] Algo bellman_ford step 8052 current loss 0.513434, current_train_items 257696.
I0302 19:02:11.835676 22535416901760 run.py:483] Algo bellman_ford step 8053 current loss 0.544059, current_train_items 257728.
I0302 19:02:11.870454 22535416901760 run.py:483] Algo bellman_ford step 8054 current loss 0.749882, current_train_items 257760.
I0302 19:02:11.890486 22535416901760 run.py:483] Algo bellman_ford step 8055 current loss 0.385895, current_train_items 257792.
I0302 19:02:11.905676 22535416901760 run.py:483] Algo bellman_ford step 8056 current loss 0.342404, current_train_items 257824.
I0302 19:02:11.928051 22535416901760 run.py:483] Algo bellman_ford step 8057 current loss 0.482618, current_train_items 257856.
I0302 19:02:11.960861 22535416901760 run.py:483] Algo bellman_ford step 8058 current loss 0.613875, current_train_items 257888.
I0302 19:02:11.996374 22535416901760 run.py:483] Algo bellman_ford step 8059 current loss 0.654648, current_train_items 257920.
I0302 19:02:12.016016 22535416901760 run.py:483] Algo bellman_ford step 8060 current loss 0.283219, current_train_items 257952.
I0302 19:02:12.031946 22535416901760 run.py:483] Algo bellman_ford step 8061 current loss 0.510734, current_train_items 257984.
I0302 19:02:12.057086 22535416901760 run.py:483] Algo bellman_ford step 8062 current loss 0.672301, current_train_items 258016.
I0302 19:02:12.088847 22535416901760 run.py:483] Algo bellman_ford step 8063 current loss 0.591484, current_train_items 258048.
I0302 19:02:12.121370 22535416901760 run.py:483] Algo bellman_ford step 8064 current loss 0.729021, current_train_items 258080.
I0302 19:02:12.140946 22535416901760 run.py:483] Algo bellman_ford step 8065 current loss 0.282727, current_train_items 258112.
I0302 19:02:12.156971 22535416901760 run.py:483] Algo bellman_ford step 8066 current loss 0.453165, current_train_items 258144.
I0302 19:02:12.180968 22535416901760 run.py:483] Algo bellman_ford step 8067 current loss 0.635087, current_train_items 258176.
I0302 19:02:12.212392 22535416901760 run.py:483] Algo bellman_ford step 8068 current loss 0.857122, current_train_items 258208.
I0302 19:02:12.244507 22535416901760 run.py:483] Algo bellman_ford step 8069 current loss 0.949240, current_train_items 258240.
I0302 19:02:12.264396 22535416901760 run.py:483] Algo bellman_ford step 8070 current loss 0.319255, current_train_items 258272.
I0302 19:02:12.281444 22535416901760 run.py:483] Algo bellman_ford step 8071 current loss 0.531524, current_train_items 258304.
I0302 19:02:12.305299 22535416901760 run.py:483] Algo bellman_ford step 8072 current loss 0.605010, current_train_items 258336.
I0302 19:02:12.337293 22535416901760 run.py:483] Algo bellman_ford step 8073 current loss 0.646178, current_train_items 258368.
I0302 19:02:12.372007 22535416901760 run.py:483] Algo bellman_ford step 8074 current loss 0.898924, current_train_items 258400.
I0302 19:02:12.391866 22535416901760 run.py:483] Algo bellman_ford step 8075 current loss 0.338820, current_train_items 258432.
I0302 19:02:12.408195 22535416901760 run.py:483] Algo bellman_ford step 8076 current loss 0.534630, current_train_items 258464.
I0302 19:02:12.431591 22535416901760 run.py:483] Algo bellman_ford step 8077 current loss 0.523897, current_train_items 258496.
I0302 19:02:12.462970 22535416901760 run.py:483] Algo bellman_ford step 8078 current loss 0.728551, current_train_items 258528.
I0302 19:02:12.494677 22535416901760 run.py:483] Algo bellman_ford step 8079 current loss 0.690768, current_train_items 258560.
I0302 19:02:12.514480 22535416901760 run.py:483] Algo bellman_ford step 8080 current loss 0.349203, current_train_items 258592.
I0302 19:02:12.530697 22535416901760 run.py:483] Algo bellman_ford step 8081 current loss 0.511269, current_train_items 258624.
I0302 19:02:12.554044 22535416901760 run.py:483] Algo bellman_ford step 8082 current loss 0.615389, current_train_items 258656.
I0302 19:02:12.585952 22535416901760 run.py:483] Algo bellman_ford step 8083 current loss 0.630767, current_train_items 258688.
I0302 19:02:12.622490 22535416901760 run.py:483] Algo bellman_ford step 8084 current loss 0.834782, current_train_items 258720.
I0302 19:02:12.642246 22535416901760 run.py:483] Algo bellman_ford step 8085 current loss 0.344277, current_train_items 258752.
I0302 19:02:12.658872 22535416901760 run.py:483] Algo bellman_ford step 8086 current loss 0.455617, current_train_items 258784.
I0302 19:02:12.681704 22535416901760 run.py:483] Algo bellman_ford step 8087 current loss 0.581658, current_train_items 258816.
I0302 19:02:12.714487 22535416901760 run.py:483] Algo bellman_ford step 8088 current loss 0.608274, current_train_items 258848.
I0302 19:02:12.746549 22535416901760 run.py:483] Algo bellman_ford step 8089 current loss 0.655985, current_train_items 258880.
I0302 19:02:12.766457 22535416901760 run.py:483] Algo bellman_ford step 8090 current loss 0.314678, current_train_items 258912.
I0302 19:02:12.782232 22535416901760 run.py:483] Algo bellman_ford step 8091 current loss 0.412898, current_train_items 258944.
I0302 19:02:12.806395 22535416901760 run.py:483] Algo bellman_ford step 8092 current loss 0.600427, current_train_items 258976.
I0302 19:02:12.837189 22535416901760 run.py:483] Algo bellman_ford step 8093 current loss 0.558766, current_train_items 259008.
I0302 19:02:12.871518 22535416901760 run.py:483] Algo bellman_ford step 8094 current loss 0.714928, current_train_items 259040.
I0302 19:02:12.891368 22535416901760 run.py:483] Algo bellman_ford step 8095 current loss 0.297179, current_train_items 259072.
I0302 19:02:12.907283 22535416901760 run.py:483] Algo bellman_ford step 8096 current loss 0.549204, current_train_items 259104.
I0302 19:02:12.932350 22535416901760 run.py:483] Algo bellman_ford step 8097 current loss 0.573415, current_train_items 259136.
I0302 19:02:12.964561 22535416901760 run.py:483] Algo bellman_ford step 8098 current loss 0.653518, current_train_items 259168.
I0302 19:02:12.998037 22535416901760 run.py:483] Algo bellman_ford step 8099 current loss 0.702737, current_train_items 259200.
I0302 19:02:13.017541 22535416901760 run.py:483] Algo bellman_ford step 8100 current loss 0.269537, current_train_items 259232.
I0302 19:02:13.025588 22535416901760 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:02:13.025693 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:13.042527 22535416901760 run.py:483] Algo bellman_ford step 8101 current loss 0.480155, current_train_items 259264.
I0302 19:02:13.067995 22535416901760 run.py:483] Algo bellman_ford step 8102 current loss 0.647827, current_train_items 259296.
I0302 19:02:13.099734 22535416901760 run.py:483] Algo bellman_ford step 8103 current loss 0.587770, current_train_items 259328.
I0302 19:02:13.133438 22535416901760 run.py:483] Algo bellman_ford step 8104 current loss 0.744931, current_train_items 259360.
I0302 19:02:13.153570 22535416901760 run.py:483] Algo bellman_ford step 8105 current loss 0.303771, current_train_items 259392.
I0302 19:02:13.169817 22535416901760 run.py:483] Algo bellman_ford step 8106 current loss 0.431373, current_train_items 259424.
I0302 19:02:13.193593 22535416901760 run.py:483] Algo bellman_ford step 8107 current loss 0.583923, current_train_items 259456.
I0302 19:02:13.224848 22535416901760 run.py:483] Algo bellman_ford step 8108 current loss 0.761006, current_train_items 259488.
I0302 19:02:13.258946 22535416901760 run.py:483] Algo bellman_ford step 8109 current loss 0.676668, current_train_items 259520.
I0302 19:02:13.279007 22535416901760 run.py:483] Algo bellman_ford step 8110 current loss 0.268252, current_train_items 259552.
I0302 19:02:13.294780 22535416901760 run.py:483] Algo bellman_ford step 8111 current loss 0.509221, current_train_items 259584.
I0302 19:02:13.317956 22535416901760 run.py:483] Algo bellman_ford step 8112 current loss 0.500966, current_train_items 259616.
I0302 19:02:13.347804 22535416901760 run.py:483] Algo bellman_ford step 8113 current loss 0.510733, current_train_items 259648.
I0302 19:02:13.383122 22535416901760 run.py:483] Algo bellman_ford step 8114 current loss 0.699744, current_train_items 259680.
I0302 19:02:13.403070 22535416901760 run.py:483] Algo bellman_ford step 8115 current loss 0.313440, current_train_items 259712.
I0302 19:02:13.419245 22535416901760 run.py:483] Algo bellman_ford step 8116 current loss 0.444628, current_train_items 259744.
I0302 19:02:13.443945 22535416901760 run.py:483] Algo bellman_ford step 8117 current loss 0.547761, current_train_items 259776.
I0302 19:02:13.475717 22535416901760 run.py:483] Algo bellman_ford step 8118 current loss 0.649660, current_train_items 259808.
I0302 19:02:13.508836 22535416901760 run.py:483] Algo bellman_ford step 8119 current loss 0.693987, current_train_items 259840.
I0302 19:02:13.528469 22535416901760 run.py:483] Algo bellman_ford step 8120 current loss 0.307093, current_train_items 259872.
I0302 19:02:13.545358 22535416901760 run.py:483] Algo bellman_ford step 8121 current loss 0.433499, current_train_items 259904.
I0302 19:02:13.569675 22535416901760 run.py:483] Algo bellman_ford step 8122 current loss 0.558564, current_train_items 259936.
I0302 19:02:13.602130 22535416901760 run.py:483] Algo bellman_ford step 8123 current loss 0.601703, current_train_items 259968.
I0302 19:02:13.633943 22535416901760 run.py:483] Algo bellman_ford step 8124 current loss 0.652263, current_train_items 260000.
I0302 19:02:13.653595 22535416901760 run.py:483] Algo bellman_ford step 8125 current loss 0.269296, current_train_items 260032.
I0302 19:02:13.670022 22535416901760 run.py:483] Algo bellman_ford step 8126 current loss 0.632440, current_train_items 260064.
I0302 19:02:13.694537 22535416901760 run.py:483] Algo bellman_ford step 8127 current loss 0.554196, current_train_items 260096.
I0302 19:02:13.726547 22535416901760 run.py:483] Algo bellman_ford step 8128 current loss 0.700479, current_train_items 260128.
I0302 19:02:13.762178 22535416901760 run.py:483] Algo bellman_ford step 8129 current loss 0.884740, current_train_items 260160.
I0302 19:02:13.781758 22535416901760 run.py:483] Algo bellman_ford step 8130 current loss 0.305611, current_train_items 260192.
I0302 19:02:13.797816 22535416901760 run.py:483] Algo bellman_ford step 8131 current loss 0.396025, current_train_items 260224.
I0302 19:02:13.821962 22535416901760 run.py:483] Algo bellman_ford step 8132 current loss 0.651594, current_train_items 260256.
I0302 19:02:13.853190 22535416901760 run.py:483] Algo bellman_ford step 8133 current loss 0.614303, current_train_items 260288.
I0302 19:02:13.887445 22535416901760 run.py:483] Algo bellman_ford step 8134 current loss 0.790126, current_train_items 260320.
I0302 19:02:13.906840 22535416901760 run.py:483] Algo bellman_ford step 8135 current loss 0.274817, current_train_items 260352.
I0302 19:02:13.922816 22535416901760 run.py:483] Algo bellman_ford step 8136 current loss 0.442979, current_train_items 260384.
I0302 19:02:13.946289 22535416901760 run.py:483] Algo bellman_ford step 8137 current loss 0.576934, current_train_items 260416.
I0302 19:02:13.977875 22535416901760 run.py:483] Algo bellman_ford step 8138 current loss 0.680359, current_train_items 260448.
I0302 19:02:14.013340 22535416901760 run.py:483] Algo bellman_ford step 8139 current loss 0.779424, current_train_items 260480.
I0302 19:02:14.032823 22535416901760 run.py:483] Algo bellman_ford step 8140 current loss 0.314588, current_train_items 260512.
I0302 19:02:14.048710 22535416901760 run.py:483] Algo bellman_ford step 8141 current loss 0.531936, current_train_items 260544.
I0302 19:02:14.073053 22535416901760 run.py:483] Algo bellman_ford step 8142 current loss 0.604176, current_train_items 260576.
I0302 19:02:14.105247 22535416901760 run.py:483] Algo bellman_ford step 8143 current loss 0.700158, current_train_items 260608.
I0302 19:02:14.139450 22535416901760 run.py:483] Algo bellman_ford step 8144 current loss 0.729820, current_train_items 260640.
I0302 19:02:14.158627 22535416901760 run.py:483] Algo bellman_ford step 8145 current loss 0.336304, current_train_items 260672.
I0302 19:02:14.174997 22535416901760 run.py:483] Algo bellman_ford step 8146 current loss 0.449084, current_train_items 260704.
I0302 19:02:14.198740 22535416901760 run.py:483] Algo bellman_ford step 8147 current loss 0.529407, current_train_items 260736.
I0302 19:02:14.230380 22535416901760 run.py:483] Algo bellman_ford step 8148 current loss 0.699610, current_train_items 260768.
I0302 19:02:14.262894 22535416901760 run.py:483] Algo bellman_ford step 8149 current loss 0.683914, current_train_items 260800.
I0302 19:02:14.282851 22535416901760 run.py:483] Algo bellman_ford step 8150 current loss 0.344324, current_train_items 260832.
I0302 19:02:14.290842 22535416901760 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:02:14.290950 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:02:14.307632 22535416901760 run.py:483] Algo bellman_ford step 8151 current loss 0.364672, current_train_items 260864.
I0302 19:02:14.331895 22535416901760 run.py:483] Algo bellman_ford step 8152 current loss 0.605024, current_train_items 260896.
I0302 19:02:14.362245 22535416901760 run.py:483] Algo bellman_ford step 8153 current loss 0.561210, current_train_items 260928.
I0302 19:02:14.397521 22535416901760 run.py:483] Algo bellman_ford step 8154 current loss 0.700028, current_train_items 260960.
I0302 19:02:14.417573 22535416901760 run.py:483] Algo bellman_ford step 8155 current loss 0.310317, current_train_items 260992.
I0302 19:02:14.433015 22535416901760 run.py:483] Algo bellman_ford step 8156 current loss 0.410212, current_train_items 261024.
I0302 19:02:14.457071 22535416901760 run.py:483] Algo bellman_ford step 8157 current loss 0.553224, current_train_items 261056.
I0302 19:02:14.488276 22535416901760 run.py:483] Algo bellman_ford step 8158 current loss 0.677552, current_train_items 261088.
I0302 19:02:14.521032 22535416901760 run.py:483] Algo bellman_ford step 8159 current loss 0.694320, current_train_items 261120.
I0302 19:02:14.540692 22535416901760 run.py:483] Algo bellman_ford step 8160 current loss 0.272180, current_train_items 261152.
I0302 19:02:14.557029 22535416901760 run.py:483] Algo bellman_ford step 8161 current loss 0.466202, current_train_items 261184.
I0302 19:02:14.580578 22535416901760 run.py:483] Algo bellman_ford step 8162 current loss 0.710970, current_train_items 261216.
I0302 19:02:14.612756 22535416901760 run.py:483] Algo bellman_ford step 8163 current loss 0.832465, current_train_items 261248.
I0302 19:02:14.643287 22535416901760 run.py:483] Algo bellman_ford step 8164 current loss 0.735402, current_train_items 261280.
I0302 19:02:14.662665 22535416901760 run.py:483] Algo bellman_ford step 8165 current loss 0.257231, current_train_items 261312.
I0302 19:02:14.678827 22535416901760 run.py:483] Algo bellman_ford step 8166 current loss 0.514387, current_train_items 261344.
I0302 19:02:14.702605 22535416901760 run.py:483] Algo bellman_ford step 8167 current loss 0.642544, current_train_items 261376.
I0302 19:02:14.733466 22535416901760 run.py:483] Algo bellman_ford step 8168 current loss 0.654140, current_train_items 261408.
I0302 19:02:14.767800 22535416901760 run.py:483] Algo bellman_ford step 8169 current loss 0.923079, current_train_items 261440.
I0302 19:02:14.787665 22535416901760 run.py:483] Algo bellman_ford step 8170 current loss 0.261146, current_train_items 261472.
I0302 19:02:14.803863 22535416901760 run.py:483] Algo bellman_ford step 8171 current loss 0.481504, current_train_items 261504.
I0302 19:02:14.827610 22535416901760 run.py:483] Algo bellman_ford step 8172 current loss 0.617430, current_train_items 261536.
I0302 19:02:14.859152 22535416901760 run.py:483] Algo bellman_ford step 8173 current loss 0.622780, current_train_items 261568.
I0302 19:02:14.891976 22535416901760 run.py:483] Algo bellman_ford step 8174 current loss 0.764010, current_train_items 261600.
I0302 19:02:14.911804 22535416901760 run.py:483] Algo bellman_ford step 8175 current loss 0.308280, current_train_items 261632.
I0302 19:02:14.928354 22535416901760 run.py:483] Algo bellman_ford step 8176 current loss 0.446873, current_train_items 261664.
I0302 19:02:14.951670 22535416901760 run.py:483] Algo bellman_ford step 8177 current loss 0.546600, current_train_items 261696.
I0302 19:02:14.984194 22535416901760 run.py:483] Algo bellman_ford step 8178 current loss 0.606309, current_train_items 261728.
I0302 19:02:15.014937 22535416901760 run.py:483] Algo bellman_ford step 8179 current loss 0.580243, current_train_items 261760.
I0302 19:02:15.034187 22535416901760 run.py:483] Algo bellman_ford step 8180 current loss 0.324594, current_train_items 261792.
I0302 19:02:15.050261 22535416901760 run.py:483] Algo bellman_ford step 8181 current loss 0.406168, current_train_items 261824.
I0302 19:02:15.074041 22535416901760 run.py:483] Algo bellman_ford step 8182 current loss 0.536193, current_train_items 261856.
I0302 19:02:15.105982 22535416901760 run.py:483] Algo bellman_ford step 8183 current loss 0.625311, current_train_items 261888.
I0302 19:02:15.141429 22535416901760 run.py:483] Algo bellman_ford step 8184 current loss 0.866544, current_train_items 261920.
I0302 19:02:15.161372 22535416901760 run.py:483] Algo bellman_ford step 8185 current loss 0.312438, current_train_items 261952.
I0302 19:02:15.177493 22535416901760 run.py:483] Algo bellman_ford step 8186 current loss 0.487968, current_train_items 261984.
I0302 19:02:15.201117 22535416901760 run.py:483] Algo bellman_ford step 8187 current loss 0.577320, current_train_items 262016.
I0302 19:02:15.232767 22535416901760 run.py:483] Algo bellman_ford step 8188 current loss 0.567958, current_train_items 262048.
I0302 19:02:15.265949 22535416901760 run.py:483] Algo bellman_ford step 8189 current loss 0.731607, current_train_items 262080.
I0302 19:02:15.286148 22535416901760 run.py:483] Algo bellman_ford step 8190 current loss 0.312736, current_train_items 262112.
I0302 19:02:15.302475 22535416901760 run.py:483] Algo bellman_ford step 8191 current loss 0.449449, current_train_items 262144.
I0302 19:02:15.325803 22535416901760 run.py:483] Algo bellman_ford step 8192 current loss 0.526640, current_train_items 262176.
I0302 19:02:15.356862 22535416901760 run.py:483] Algo bellman_ford step 8193 current loss 0.516620, current_train_items 262208.
I0302 19:02:15.389962 22535416901760 run.py:483] Algo bellman_ford step 8194 current loss 0.722705, current_train_items 262240.
I0302 19:02:15.409656 22535416901760 run.py:483] Algo bellman_ford step 8195 current loss 0.355401, current_train_items 262272.
I0302 19:02:15.425741 22535416901760 run.py:483] Algo bellman_ford step 8196 current loss 0.376972, current_train_items 262304.
I0302 19:02:15.448607 22535416901760 run.py:483] Algo bellman_ford step 8197 current loss 0.474742, current_train_items 262336.
I0302 19:02:15.480098 22535416901760 run.py:483] Algo bellman_ford step 8198 current loss 0.617791, current_train_items 262368.
I0302 19:02:15.513168 22535416901760 run.py:483] Algo bellman_ford step 8199 current loss 0.695891, current_train_items 262400.
I0302 19:02:15.533413 22535416901760 run.py:483] Algo bellman_ford step 8200 current loss 0.285775, current_train_items 262432.
I0302 19:02:15.541296 22535416901760 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:02:15.541402 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 19:02:15.557687 22535416901760 run.py:483] Algo bellman_ford step 8201 current loss 0.398978, current_train_items 262464.
I0302 19:02:15.582273 22535416901760 run.py:483] Algo bellman_ford step 8202 current loss 0.499965, current_train_items 262496.
I0302 19:02:15.613801 22535416901760 run.py:483] Algo bellman_ford step 8203 current loss 0.654943, current_train_items 262528.
I0302 19:02:15.648604 22535416901760 run.py:483] Algo bellman_ford step 8204 current loss 0.698442, current_train_items 262560.
I0302 19:02:15.668428 22535416901760 run.py:483] Algo bellman_ford step 8205 current loss 0.259898, current_train_items 262592.
I0302 19:02:15.684612 22535416901760 run.py:483] Algo bellman_ford step 8206 current loss 0.423117, current_train_items 262624.
I0302 19:02:15.708817 22535416901760 run.py:483] Algo bellman_ford step 8207 current loss 0.619236, current_train_items 262656.
I0302 19:02:15.740291 22535416901760 run.py:483] Algo bellman_ford step 8208 current loss 0.591077, current_train_items 262688.
I0302 19:02:15.771366 22535416901760 run.py:483] Algo bellman_ford step 8209 current loss 0.622371, current_train_items 262720.
I0302 19:02:15.790830 22535416901760 run.py:483] Algo bellman_ford step 8210 current loss 0.270393, current_train_items 262752.
I0302 19:02:15.806831 22535416901760 run.py:483] Algo bellman_ford step 8211 current loss 0.437967, current_train_items 262784.
I0302 19:02:15.830150 22535416901760 run.py:483] Algo bellman_ford step 8212 current loss 0.548277, current_train_items 262816.
I0302 19:02:15.861676 22535416901760 run.py:483] Algo bellman_ford step 8213 current loss 0.617980, current_train_items 262848.
I0302 19:02:15.894775 22535416901760 run.py:483] Algo bellman_ford step 8214 current loss 0.763546, current_train_items 262880.
I0302 19:02:15.914031 22535416901760 run.py:483] Algo bellman_ford step 8215 current loss 0.281270, current_train_items 262912.
I0302 19:02:15.929527 22535416901760 run.py:483] Algo bellman_ford step 8216 current loss 0.434906, current_train_items 262944.
I0302 19:02:15.952110 22535416901760 run.py:483] Algo bellman_ford step 8217 current loss 0.627724, current_train_items 262976.
I0302 19:02:15.984371 22535416901760 run.py:483] Algo bellman_ford step 8218 current loss 0.574712, current_train_items 263008.
I0302 19:02:16.018594 22535416901760 run.py:483] Algo bellman_ford step 8219 current loss 0.629139, current_train_items 263040.
I0302 19:02:16.037956 22535416901760 run.py:483] Algo bellman_ford step 8220 current loss 0.276799, current_train_items 263072.
I0302 19:02:16.053751 22535416901760 run.py:483] Algo bellman_ford step 8221 current loss 0.471646, current_train_items 263104.
I0302 19:02:16.078137 22535416901760 run.py:483] Algo bellman_ford step 8222 current loss 0.669397, current_train_items 263136.
I0302 19:02:16.109937 22535416901760 run.py:483] Algo bellman_ford step 8223 current loss 0.650651, current_train_items 263168.
I0302 19:02:16.140810 22535416901760 run.py:483] Algo bellman_ford step 8224 current loss 0.729330, current_train_items 263200.
I0302 19:02:16.160632 22535416901760 run.py:483] Algo bellman_ford step 8225 current loss 0.291573, current_train_items 263232.
I0302 19:02:16.176956 22535416901760 run.py:483] Algo bellman_ford step 8226 current loss 0.430991, current_train_items 263264.
I0302 19:02:16.201220 22535416901760 run.py:483] Algo bellman_ford step 8227 current loss 0.624789, current_train_items 263296.
I0302 19:02:16.231477 22535416901760 run.py:483] Algo bellman_ford step 8228 current loss 0.541156, current_train_items 263328.
I0302 19:02:16.264985 22535416901760 run.py:483] Algo bellman_ford step 8229 current loss 0.732730, current_train_items 263360.
I0302 19:02:16.284316 22535416901760 run.py:483] Algo bellman_ford step 8230 current loss 0.258427, current_train_items 263392.
I0302 19:02:16.300472 22535416901760 run.py:483] Algo bellman_ford step 8231 current loss 0.539739, current_train_items 263424.
I0302 19:02:16.324601 22535416901760 run.py:483] Algo bellman_ford step 8232 current loss 0.509114, current_train_items 263456.
I0302 19:02:16.355713 22535416901760 run.py:483] Algo bellman_ford step 8233 current loss 0.554826, current_train_items 263488.
I0302 19:02:16.388404 22535416901760 run.py:483] Algo bellman_ford step 8234 current loss 0.680185, current_train_items 263520.
I0302 19:02:16.407878 22535416901760 run.py:483] Algo bellman_ford step 8235 current loss 0.282560, current_train_items 263552.
I0302 19:02:16.424394 22535416901760 run.py:483] Algo bellman_ford step 8236 current loss 0.463937, current_train_items 263584.
I0302 19:02:16.448717 22535416901760 run.py:483] Algo bellman_ford step 8237 current loss 0.594599, current_train_items 263616.
I0302 19:02:16.480558 22535416901760 run.py:483] Algo bellman_ford step 8238 current loss 0.592182, current_train_items 263648.
I0302 19:02:16.513520 22535416901760 run.py:483] Algo bellman_ford step 8239 current loss 0.590523, current_train_items 263680.
I0302 19:02:16.532811 22535416901760 run.py:483] Algo bellman_ford step 8240 current loss 0.280802, current_train_items 263712.
I0302 19:02:16.548933 22535416901760 run.py:483] Algo bellman_ford step 8241 current loss 0.411713, current_train_items 263744.
I0302 19:02:16.571853 22535416901760 run.py:483] Algo bellman_ford step 8242 current loss 0.561431, current_train_items 263776.
I0302 19:02:16.602880 22535416901760 run.py:483] Algo bellman_ford step 8243 current loss 0.568539, current_train_items 263808.
I0302 19:02:16.638284 22535416901760 run.py:483] Algo bellman_ford step 8244 current loss 0.715981, current_train_items 263840.
I0302 19:02:16.658267 22535416901760 run.py:483] Algo bellman_ford step 8245 current loss 0.303171, current_train_items 263872.
I0302 19:02:16.674315 22535416901760 run.py:483] Algo bellman_ford step 8246 current loss 0.465444, current_train_items 263904.
I0302 19:02:16.697099 22535416901760 run.py:483] Algo bellman_ford step 8247 current loss 0.503387, current_train_items 263936.
I0302 19:02:16.728098 22535416901760 run.py:483] Algo bellman_ford step 8248 current loss 0.530188, current_train_items 263968.
I0302 19:02:16.761786 22535416901760 run.py:483] Algo bellman_ford step 8249 current loss 0.691345, current_train_items 264000.
I0302 19:02:16.781447 22535416901760 run.py:483] Algo bellman_ford step 8250 current loss 0.237940, current_train_items 264032.
I0302 19:02:16.789400 22535416901760 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:02:16.789507 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:16.806080 22535416901760 run.py:483] Algo bellman_ford step 8251 current loss 0.504162, current_train_items 264064.
I0302 19:02:16.830628 22535416901760 run.py:483] Algo bellman_ford step 8252 current loss 0.574798, current_train_items 264096.
I0302 19:02:16.863368 22535416901760 run.py:483] Algo bellman_ford step 8253 current loss 0.676545, current_train_items 264128.
I0302 19:02:16.896292 22535416901760 run.py:483] Algo bellman_ford step 8254 current loss 0.614980, current_train_items 264160.
I0302 19:02:16.916136 22535416901760 run.py:483] Algo bellman_ford step 8255 current loss 0.391059, current_train_items 264192.
I0302 19:02:16.932494 22535416901760 run.py:483] Algo bellman_ford step 8256 current loss 0.402751, current_train_items 264224.
I0302 19:02:16.956453 22535416901760 run.py:483] Algo bellman_ford step 8257 current loss 0.534511, current_train_items 264256.
I0302 19:02:16.988325 22535416901760 run.py:483] Algo bellman_ford step 8258 current loss 0.576672, current_train_items 264288.
I0302 19:02:17.022810 22535416901760 run.py:483] Algo bellman_ford step 8259 current loss 0.721147, current_train_items 264320.
I0302 19:02:17.042314 22535416901760 run.py:483] Algo bellman_ford step 8260 current loss 0.304423, current_train_items 264352.
I0302 19:02:17.058575 22535416901760 run.py:483] Algo bellman_ford step 8261 current loss 0.449504, current_train_items 264384.
I0302 19:02:17.082912 22535416901760 run.py:483] Algo bellman_ford step 8262 current loss 0.598847, current_train_items 264416.
I0302 19:02:17.114978 22535416901760 run.py:483] Algo bellman_ford step 8263 current loss 0.743113, current_train_items 264448.
I0302 19:02:17.149528 22535416901760 run.py:483] Algo bellman_ford step 8264 current loss 0.780220, current_train_items 264480.
I0302 19:02:17.168900 22535416901760 run.py:483] Algo bellman_ford step 8265 current loss 0.296042, current_train_items 264512.
I0302 19:02:17.184654 22535416901760 run.py:483] Algo bellman_ford step 8266 current loss 0.561637, current_train_items 264544.
I0302 19:02:17.208594 22535416901760 run.py:483] Algo bellman_ford step 8267 current loss 0.621792, current_train_items 264576.
I0302 19:02:17.240523 22535416901760 run.py:483] Algo bellman_ford step 8268 current loss 0.779712, current_train_items 264608.
I0302 19:02:17.272612 22535416901760 run.py:483] Algo bellman_ford step 8269 current loss 0.858541, current_train_items 264640.
I0302 19:02:17.292586 22535416901760 run.py:483] Algo bellman_ford step 8270 current loss 0.313647, current_train_items 264672.
I0302 19:02:17.308350 22535416901760 run.py:483] Algo bellman_ford step 8271 current loss 0.363124, current_train_items 264704.
I0302 19:02:17.331480 22535416901760 run.py:483] Algo bellman_ford step 8272 current loss 0.539694, current_train_items 264736.
I0302 19:02:17.361902 22535416901760 run.py:483] Algo bellman_ford step 8273 current loss 0.600196, current_train_items 264768.
I0302 19:02:17.396759 22535416901760 run.py:483] Algo bellman_ford step 8274 current loss 0.757040, current_train_items 264800.
I0302 19:02:17.416580 22535416901760 run.py:483] Algo bellman_ford step 8275 current loss 0.271848, current_train_items 264832.
I0302 19:02:17.433241 22535416901760 run.py:483] Algo bellman_ford step 8276 current loss 0.498734, current_train_items 264864.
I0302 19:02:17.457111 22535416901760 run.py:483] Algo bellman_ford step 8277 current loss 0.663216, current_train_items 264896.
I0302 19:02:17.488121 22535416901760 run.py:483] Algo bellman_ford step 8278 current loss 0.679001, current_train_items 264928.
I0302 19:02:17.520828 22535416901760 run.py:483] Algo bellman_ford step 8279 current loss 0.766899, current_train_items 264960.
I0302 19:02:17.539931 22535416901760 run.py:483] Algo bellman_ford step 8280 current loss 0.325187, current_train_items 264992.
I0302 19:02:17.556538 22535416901760 run.py:483] Algo bellman_ford step 8281 current loss 0.520822, current_train_items 265024.
I0302 19:02:17.579884 22535416901760 run.py:483] Algo bellman_ford step 8282 current loss 0.593014, current_train_items 265056.
I0302 19:02:17.610680 22535416901760 run.py:483] Algo bellman_ford step 8283 current loss 0.604617, current_train_items 265088.
I0302 19:02:17.643307 22535416901760 run.py:483] Algo bellman_ford step 8284 current loss 0.770270, current_train_items 265120.
I0302 19:02:17.663439 22535416901760 run.py:483] Algo bellman_ford step 8285 current loss 0.287647, current_train_items 265152.
I0302 19:02:17.679775 22535416901760 run.py:483] Algo bellman_ford step 8286 current loss 0.443798, current_train_items 265184.
I0302 19:02:17.703061 22535416901760 run.py:483] Algo bellman_ford step 8287 current loss 0.532676, current_train_items 265216.
I0302 19:02:17.735701 22535416901760 run.py:483] Algo bellman_ford step 8288 current loss 0.694717, current_train_items 265248.
I0302 19:02:17.769380 22535416901760 run.py:483] Algo bellman_ford step 8289 current loss 0.714513, current_train_items 265280.
I0302 19:02:17.788757 22535416901760 run.py:483] Algo bellman_ford step 8290 current loss 0.305294, current_train_items 265312.
I0302 19:02:17.804887 22535416901760 run.py:483] Algo bellman_ford step 8291 current loss 0.449035, current_train_items 265344.
I0302 19:02:17.828895 22535416901760 run.py:483] Algo bellman_ford step 8292 current loss 0.573616, current_train_items 265376.
I0302 19:02:17.859126 22535416901760 run.py:483] Algo bellman_ford step 8293 current loss 0.540059, current_train_items 265408.
I0302 19:02:17.893893 22535416901760 run.py:483] Algo bellman_ford step 8294 current loss 0.715361, current_train_items 265440.
I0302 19:02:17.913513 22535416901760 run.py:483] Algo bellman_ford step 8295 current loss 0.269207, current_train_items 265472.
I0302 19:02:17.929768 22535416901760 run.py:483] Algo bellman_ford step 8296 current loss 0.514358, current_train_items 265504.
I0302 19:02:17.954489 22535416901760 run.py:483] Algo bellman_ford step 8297 current loss 0.746084, current_train_items 265536.
I0302 19:02:17.985872 22535416901760 run.py:483] Algo bellman_ford step 8298 current loss 0.641707, current_train_items 265568.
I0302 19:02:18.018391 22535416901760 run.py:483] Algo bellman_ford step 8299 current loss 0.708745, current_train_items 265600.
I0302 19:02:18.038188 22535416901760 run.py:483] Algo bellman_ford step 8300 current loss 0.270817, current_train_items 265632.
I0302 19:02:18.046022 22535416901760 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.93359375, 'score': 0.93359375, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:02:18.046130 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.934, val scores are: bellman_ford: 0.934
I0302 19:02:18.062483 22535416901760 run.py:483] Algo bellman_ford step 8301 current loss 0.538522, current_train_items 265664.
I0302 19:02:18.088267 22535416901760 run.py:483] Algo bellman_ford step 8302 current loss 0.676066, current_train_items 265696.
I0302 19:02:18.120630 22535416901760 run.py:483] Algo bellman_ford step 8303 current loss 0.650816, current_train_items 265728.
I0302 19:02:18.154050 22535416901760 run.py:483] Algo bellman_ford step 8304 current loss 0.672491, current_train_items 265760.
I0302 19:02:18.174069 22535416901760 run.py:483] Algo bellman_ford step 8305 current loss 0.297556, current_train_items 265792.
I0302 19:02:18.189776 22535416901760 run.py:483] Algo bellman_ford step 8306 current loss 0.530486, current_train_items 265824.
I0302 19:02:18.213738 22535416901760 run.py:483] Algo bellman_ford step 8307 current loss 0.581822, current_train_items 265856.
I0302 19:02:18.242922 22535416901760 run.py:483] Algo bellman_ford step 8308 current loss 0.571013, current_train_items 265888.
I0302 19:02:18.278610 22535416901760 run.py:483] Algo bellman_ford step 8309 current loss 0.986925, current_train_items 265920.
I0302 19:02:18.298440 22535416901760 run.py:483] Algo bellman_ford step 8310 current loss 0.293526, current_train_items 265952.
I0302 19:02:18.314395 22535416901760 run.py:483] Algo bellman_ford step 8311 current loss 0.411759, current_train_items 265984.
I0302 19:02:18.338488 22535416901760 run.py:483] Algo bellman_ford step 8312 current loss 0.613486, current_train_items 266016.
I0302 19:02:18.369270 22535416901760 run.py:483] Algo bellman_ford step 8313 current loss 0.627676, current_train_items 266048.
I0302 19:02:18.401268 22535416901760 run.py:483] Algo bellman_ford step 8314 current loss 0.652789, current_train_items 266080.
I0302 19:02:18.420805 22535416901760 run.py:483] Algo bellman_ford step 8315 current loss 0.233364, current_train_items 266112.
I0302 19:02:18.437224 22535416901760 run.py:483] Algo bellman_ford step 8316 current loss 0.487774, current_train_items 266144.
I0302 19:02:18.460499 22535416901760 run.py:483] Algo bellman_ford step 8317 current loss 0.609840, current_train_items 266176.
I0302 19:02:18.490465 22535416901760 run.py:483] Algo bellman_ford step 8318 current loss 0.605076, current_train_items 266208.
I0302 19:02:18.523694 22535416901760 run.py:483] Algo bellman_ford step 8319 current loss 0.693290, current_train_items 266240.
I0302 19:02:18.543137 22535416901760 run.py:483] Algo bellman_ford step 8320 current loss 0.258900, current_train_items 266272.
I0302 19:02:18.559246 22535416901760 run.py:483] Algo bellman_ford step 8321 current loss 0.439838, current_train_items 266304.
I0302 19:02:18.583805 22535416901760 run.py:483] Algo bellman_ford step 8322 current loss 0.534281, current_train_items 266336.
I0302 19:02:18.614950 22535416901760 run.py:483] Algo bellman_ford step 8323 current loss 0.611272, current_train_items 266368.
I0302 19:02:18.649679 22535416901760 run.py:483] Algo bellman_ford step 8324 current loss 0.706902, current_train_items 266400.
I0302 19:02:18.669202 22535416901760 run.py:483] Algo bellman_ford step 8325 current loss 0.336501, current_train_items 266432.
I0302 19:02:18.684832 22535416901760 run.py:483] Algo bellman_ford step 8326 current loss 0.407490, current_train_items 266464.
I0302 19:02:18.708447 22535416901760 run.py:483] Algo bellman_ford step 8327 current loss 0.509582, current_train_items 266496.
I0302 19:02:18.739145 22535416901760 run.py:483] Algo bellman_ford step 8328 current loss 0.660440, current_train_items 266528.
I0302 19:02:18.773252 22535416901760 run.py:483] Algo bellman_ford step 8329 current loss 0.702756, current_train_items 266560.
I0302 19:02:18.792635 22535416901760 run.py:483] Algo bellman_ford step 8330 current loss 0.321868, current_train_items 266592.
I0302 19:02:18.808774 22535416901760 run.py:483] Algo bellman_ford step 8331 current loss 0.475101, current_train_items 266624.
I0302 19:02:18.832026 22535416901760 run.py:483] Algo bellman_ford step 8332 current loss 0.546872, current_train_items 266656.
I0302 19:02:18.864026 22535416901760 run.py:483] Algo bellman_ford step 8333 current loss 0.680513, current_train_items 266688.
I0302 19:02:18.896616 22535416901760 run.py:483] Algo bellman_ford step 8334 current loss 0.716582, current_train_items 266720.
I0302 19:02:18.916571 22535416901760 run.py:483] Algo bellman_ford step 8335 current loss 0.307691, current_train_items 266752.
I0302 19:02:18.932632 22535416901760 run.py:483] Algo bellman_ford step 8336 current loss 0.530829, current_train_items 266784.
I0302 19:02:18.957643 22535416901760 run.py:483] Algo bellman_ford step 8337 current loss 0.626669, current_train_items 266816.
I0302 19:02:18.988570 22535416901760 run.py:483] Algo bellman_ford step 8338 current loss 0.638867, current_train_items 266848.
I0302 19:02:19.020906 22535416901760 run.py:483] Algo bellman_ford step 8339 current loss 0.786280, current_train_items 266880.
I0302 19:02:19.040764 22535416901760 run.py:483] Algo bellman_ford step 8340 current loss 0.353758, current_train_items 266912.
I0302 19:02:19.056751 22535416901760 run.py:483] Algo bellman_ford step 8341 current loss 0.416697, current_train_items 266944.
I0302 19:02:19.080043 22535416901760 run.py:483] Algo bellman_ford step 8342 current loss 0.534151, current_train_items 266976.
I0302 19:02:19.110191 22535416901760 run.py:483] Algo bellman_ford step 8343 current loss 0.576806, current_train_items 267008.
I0302 19:02:19.145089 22535416901760 run.py:483] Algo bellman_ford step 8344 current loss 0.711577, current_train_items 267040.
I0302 19:02:19.164559 22535416901760 run.py:483] Algo bellman_ford step 8345 current loss 0.314915, current_train_items 267072.
I0302 19:02:19.180523 22535416901760 run.py:483] Algo bellman_ford step 8346 current loss 0.450188, current_train_items 267104.
I0302 19:02:19.204818 22535416901760 run.py:483] Algo bellman_ford step 8347 current loss 0.725745, current_train_items 267136.
I0302 19:02:19.236620 22535416901760 run.py:483] Algo bellman_ford step 8348 current loss 0.736186, current_train_items 267168.
I0302 19:02:19.271038 22535416901760 run.py:483] Algo bellman_ford step 8349 current loss 0.817314, current_train_items 267200.
I0302 19:02:19.290560 22535416901760 run.py:483] Algo bellman_ford step 8350 current loss 0.274451, current_train_items 267232.
I0302 19:02:19.298733 22535416901760 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.9189453125, 'score': 0.9189453125, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:02:19.298842 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.919, val scores are: bellman_ford: 0.919
I0302 19:02:19.315877 22535416901760 run.py:483] Algo bellman_ford step 8351 current loss 0.430482, current_train_items 267264.
I0302 19:02:19.340377 22535416901760 run.py:483] Algo bellman_ford step 8352 current loss 0.618490, current_train_items 267296.
I0302 19:02:19.372137 22535416901760 run.py:483] Algo bellman_ford step 8353 current loss 0.594481, current_train_items 267328.
I0302 19:02:19.407492 22535416901760 run.py:483] Algo bellman_ford step 8354 current loss 0.853871, current_train_items 267360.
I0302 19:02:19.427359 22535416901760 run.py:483] Algo bellman_ford step 8355 current loss 0.315154, current_train_items 267392.
I0302 19:02:19.442886 22535416901760 run.py:483] Algo bellman_ford step 8356 current loss 0.494800, current_train_items 267424.
I0302 19:02:19.466432 22535416901760 run.py:483] Algo bellman_ford step 8357 current loss 0.492167, current_train_items 267456.
I0302 19:02:19.497844 22535416901760 run.py:483] Algo bellman_ford step 8358 current loss 0.587197, current_train_items 267488.
I0302 19:02:19.530407 22535416901760 run.py:483] Algo bellman_ford step 8359 current loss 0.771737, current_train_items 267520.
I0302 19:02:19.550230 22535416901760 run.py:483] Algo bellman_ford step 8360 current loss 0.308245, current_train_items 267552.
I0302 19:02:19.566174 22535416901760 run.py:483] Algo bellman_ford step 8361 current loss 0.442132, current_train_items 267584.
I0302 19:02:19.589885 22535416901760 run.py:483] Algo bellman_ford step 8362 current loss 0.535839, current_train_items 267616.
I0302 19:02:19.620352 22535416901760 run.py:483] Algo bellman_ford step 8363 current loss 0.615776, current_train_items 267648.
I0302 19:02:19.654093 22535416901760 run.py:483] Algo bellman_ford step 8364 current loss 0.825495, current_train_items 267680.
I0302 19:02:19.673531 22535416901760 run.py:483] Algo bellman_ford step 8365 current loss 0.286296, current_train_items 267712.
I0302 19:02:19.689953 22535416901760 run.py:483] Algo bellman_ford step 8366 current loss 0.397072, current_train_items 267744.
I0302 19:02:19.712882 22535416901760 run.py:483] Algo bellman_ford step 8367 current loss 0.506025, current_train_items 267776.
I0302 19:02:19.744043 22535416901760 run.py:483] Algo bellman_ford step 8368 current loss 0.626590, current_train_items 267808.
I0302 19:02:19.779388 22535416901760 run.py:483] Algo bellman_ford step 8369 current loss 0.816765, current_train_items 267840.
I0302 19:02:19.799146 22535416901760 run.py:483] Algo bellman_ford step 8370 current loss 0.254158, current_train_items 267872.
I0302 19:02:19.815478 22535416901760 run.py:483] Algo bellman_ford step 8371 current loss 0.539957, current_train_items 267904.
I0302 19:02:19.838045 22535416901760 run.py:483] Algo bellman_ford step 8372 current loss 0.624283, current_train_items 267936.
I0302 19:02:19.869040 22535416901760 run.py:483] Algo bellman_ford step 8373 current loss 0.591876, current_train_items 267968.
I0302 19:02:19.903099 22535416901760 run.py:483] Algo bellman_ford step 8374 current loss 0.781588, current_train_items 268000.
I0302 19:02:19.923098 22535416901760 run.py:483] Algo bellman_ford step 8375 current loss 0.268616, current_train_items 268032.
I0302 19:02:19.938890 22535416901760 run.py:483] Algo bellman_ford step 8376 current loss 0.398288, current_train_items 268064.
I0302 19:02:19.961524 22535416901760 run.py:483] Algo bellman_ford step 8377 current loss 0.555655, current_train_items 268096.
I0302 19:02:19.992804 22535416901760 run.py:483] Algo bellman_ford step 8378 current loss 0.545773, current_train_items 268128.
I0302 19:02:20.026649 22535416901760 run.py:483] Algo bellman_ford step 8379 current loss 0.738054, current_train_items 268160.
I0302 19:02:20.046121 22535416901760 run.py:483] Algo bellman_ford step 8380 current loss 0.335193, current_train_items 268192.
I0302 19:02:20.062490 22535416901760 run.py:483] Algo bellman_ford step 8381 current loss 0.520716, current_train_items 268224.
I0302 19:02:20.085012 22535416901760 run.py:483] Algo bellman_ford step 8382 current loss 0.495450, current_train_items 268256.
I0302 19:02:20.117383 22535416901760 run.py:483] Algo bellman_ford step 8383 current loss 0.667166, current_train_items 268288.
I0302 19:02:20.149420 22535416901760 run.py:483] Algo bellman_ford step 8384 current loss 0.682579, current_train_items 268320.
I0302 19:02:20.169162 22535416901760 run.py:483] Algo bellman_ford step 8385 current loss 0.341285, current_train_items 268352.
I0302 19:02:20.184872 22535416901760 run.py:483] Algo bellman_ford step 8386 current loss 0.386576, current_train_items 268384.
I0302 19:02:20.207706 22535416901760 run.py:483] Algo bellman_ford step 8387 current loss 0.506887, current_train_items 268416.
I0302 19:02:20.239024 22535416901760 run.py:483] Algo bellman_ford step 8388 current loss 0.649459, current_train_items 268448.
I0302 19:02:20.271955 22535416901760 run.py:483] Algo bellman_ford step 8389 current loss 0.716859, current_train_items 268480.
I0302 19:02:20.291641 22535416901760 run.py:483] Algo bellman_ford step 8390 current loss 0.410734, current_train_items 268512.
I0302 19:02:20.307857 22535416901760 run.py:483] Algo bellman_ford step 8391 current loss 0.393394, current_train_items 268544.
I0302 19:02:20.330339 22535416901760 run.py:483] Algo bellman_ford step 8392 current loss 0.506141, current_train_items 268576.
I0302 19:02:20.362819 22535416901760 run.py:483] Algo bellman_ford step 8393 current loss 0.666401, current_train_items 268608.
I0302 19:02:20.395035 22535416901760 run.py:483] Algo bellman_ford step 8394 current loss 0.790883, current_train_items 268640.
I0302 19:02:20.414732 22535416901760 run.py:483] Algo bellman_ford step 8395 current loss 0.336303, current_train_items 268672.
I0302 19:02:20.430963 22535416901760 run.py:483] Algo bellman_ford step 8396 current loss 0.482191, current_train_items 268704.
I0302 19:02:20.454313 22535416901760 run.py:483] Algo bellman_ford step 8397 current loss 0.551545, current_train_items 268736.
I0302 19:02:20.485882 22535416901760 run.py:483] Algo bellman_ford step 8398 current loss 0.603355, current_train_items 268768.
I0302 19:02:20.520211 22535416901760 run.py:483] Algo bellman_ford step 8399 current loss 0.760950, current_train_items 268800.
I0302 19:02:20.539923 22535416901760 run.py:483] Algo bellman_ford step 8400 current loss 0.350441, current_train_items 268832.
I0302 19:02:20.547770 22535416901760 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.9296875, 'score': 0.9296875, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:02:20.547877 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.930, val scores are: bellman_ford: 0.930
I0302 19:02:20.564978 22535416901760 run.py:483] Algo bellman_ford step 8401 current loss 0.432148, current_train_items 268864.
I0302 19:02:20.589656 22535416901760 run.py:483] Algo bellman_ford step 8402 current loss 0.548387, current_train_items 268896.
I0302 19:02:20.622418 22535416901760 run.py:483] Algo bellman_ford step 8403 current loss 0.619841, current_train_items 268928.
I0302 19:02:20.658975 22535416901760 run.py:483] Algo bellman_ford step 8404 current loss 0.843187, current_train_items 268960.
I0302 19:02:20.678780 22535416901760 run.py:483] Algo bellman_ford step 8405 current loss 0.317202, current_train_items 268992.
I0302 19:02:20.694861 22535416901760 run.py:483] Algo bellman_ford step 8406 current loss 0.438501, current_train_items 269024.
I0302 19:02:20.718121 22535416901760 run.py:483] Algo bellman_ford step 8407 current loss 0.590623, current_train_items 269056.
I0302 19:02:20.748804 22535416901760 run.py:483] Algo bellman_ford step 8408 current loss 0.627904, current_train_items 269088.
I0302 19:02:20.783450 22535416901760 run.py:483] Algo bellman_ford step 8409 current loss 0.757700, current_train_items 269120.
I0302 19:02:20.803365 22535416901760 run.py:483] Algo bellman_ford step 8410 current loss 0.299905, current_train_items 269152.
I0302 19:02:20.819544 22535416901760 run.py:483] Algo bellman_ford step 8411 current loss 0.461786, current_train_items 269184.
I0302 19:02:20.843768 22535416901760 run.py:483] Algo bellman_ford step 8412 current loss 0.535614, current_train_items 269216.
I0302 19:02:20.876496 22535416901760 run.py:483] Algo bellman_ford step 8413 current loss 0.678387, current_train_items 269248.
I0302 19:02:20.911009 22535416901760 run.py:483] Algo bellman_ford step 8414 current loss 0.744551, current_train_items 269280.
I0302 19:02:20.930809 22535416901760 run.py:483] Algo bellman_ford step 8415 current loss 0.398806, current_train_items 269312.
I0302 19:02:20.947262 22535416901760 run.py:483] Algo bellman_ford step 8416 current loss 0.402310, current_train_items 269344.
I0302 19:02:20.970809 22535416901760 run.py:483] Algo bellman_ford step 8417 current loss 0.609984, current_train_items 269376.
I0302 19:02:21.003304 22535416901760 run.py:483] Algo bellman_ford step 8418 current loss 0.539290, current_train_items 269408.
I0302 19:02:21.039514 22535416901760 run.py:483] Algo bellman_ford step 8419 current loss 0.823632, current_train_items 269440.
I0302 19:02:21.059424 22535416901760 run.py:483] Algo bellman_ford step 8420 current loss 0.250614, current_train_items 269472.
I0302 19:02:21.075550 22535416901760 run.py:483] Algo bellman_ford step 8421 current loss 0.484275, current_train_items 269504.
I0302 19:02:21.100193 22535416901760 run.py:483] Algo bellman_ford step 8422 current loss 0.540223, current_train_items 269536.
I0302 19:02:21.131835 22535416901760 run.py:483] Algo bellman_ford step 8423 current loss 0.575518, current_train_items 269568.
I0302 19:02:21.166482 22535416901760 run.py:483] Algo bellman_ford step 8424 current loss 0.778688, current_train_items 269600.
I0302 19:02:21.185832 22535416901760 run.py:483] Algo bellman_ford step 8425 current loss 0.303116, current_train_items 269632.
I0302 19:02:21.201649 22535416901760 run.py:483] Algo bellman_ford step 8426 current loss 0.463999, current_train_items 269664.
I0302 19:02:21.225097 22535416901760 run.py:483] Algo bellman_ford step 8427 current loss 0.563240, current_train_items 269696.
I0302 19:02:21.256190 22535416901760 run.py:483] Algo bellman_ford step 8428 current loss 0.614581, current_train_items 269728.
I0302 19:02:21.289546 22535416901760 run.py:483] Algo bellman_ford step 8429 current loss 0.676082, current_train_items 269760.
I0302 19:02:21.309354 22535416901760 run.py:483] Algo bellman_ford step 8430 current loss 0.307548, current_train_items 269792.
I0302 19:02:21.325980 22535416901760 run.py:483] Algo bellman_ford step 8431 current loss 0.525576, current_train_items 269824.
I0302 19:02:21.349409 22535416901760 run.py:483] Algo bellman_ford step 8432 current loss 0.454257, current_train_items 269856.
I0302 19:02:21.382010 22535416901760 run.py:483] Algo bellman_ford step 8433 current loss 0.675874, current_train_items 269888.
I0302 19:02:21.414005 22535416901760 run.py:483] Algo bellman_ford step 8434 current loss 0.726160, current_train_items 269920.
I0302 19:02:21.433209 22535416901760 run.py:483] Algo bellman_ford step 8435 current loss 0.296621, current_train_items 269952.
I0302 19:02:21.449050 22535416901760 run.py:483] Algo bellman_ford step 8436 current loss 0.485793, current_train_items 269984.
I0302 19:02:21.472355 22535416901760 run.py:483] Algo bellman_ford step 8437 current loss 0.509466, current_train_items 270016.
I0302 19:02:21.503734 22535416901760 run.py:483] Algo bellman_ford step 8438 current loss 0.582442, current_train_items 270048.
I0302 19:02:21.536068 22535416901760 run.py:483] Algo bellman_ford step 8439 current loss 0.727812, current_train_items 270080.
I0302 19:02:21.555423 22535416901760 run.py:483] Algo bellman_ford step 8440 current loss 0.315814, current_train_items 270112.
I0302 19:02:21.571674 22535416901760 run.py:483] Algo bellman_ford step 8441 current loss 0.447448, current_train_items 270144.
I0302 19:02:21.595565 22535416901760 run.py:483] Algo bellman_ford step 8442 current loss 0.553556, current_train_items 270176.
I0302 19:02:21.626726 22535416901760 run.py:483] Algo bellman_ford step 8443 current loss 0.607857, current_train_items 270208.
I0302 19:02:21.660989 22535416901760 run.py:483] Algo bellman_ford step 8444 current loss 0.730578, current_train_items 270240.
I0302 19:02:21.680692 22535416901760 run.py:483] Algo bellman_ford step 8445 current loss 0.333523, current_train_items 270272.
I0302 19:02:21.696695 22535416901760 run.py:483] Algo bellman_ford step 8446 current loss 0.434354, current_train_items 270304.
I0302 19:02:21.720124 22535416901760 run.py:483] Algo bellman_ford step 8447 current loss 0.492891, current_train_items 270336.
I0302 19:02:21.751126 22535416901760 run.py:483] Algo bellman_ford step 8448 current loss 0.589324, current_train_items 270368.
I0302 19:02:21.784900 22535416901760 run.py:483] Algo bellman_ford step 8449 current loss 0.785739, current_train_items 270400.
I0302 19:02:21.804374 22535416901760 run.py:483] Algo bellman_ford step 8450 current loss 0.280611, current_train_items 270432.
I0302 19:02:21.812479 22535416901760 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:02:21.812587 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:02:21.829005 22535416901760 run.py:483] Algo bellman_ford step 8451 current loss 0.469220, current_train_items 270464.
I0302 19:02:21.853284 22535416901760 run.py:483] Algo bellman_ford step 8452 current loss 0.572387, current_train_items 270496.
I0302 19:02:21.885767 22535416901760 run.py:483] Algo bellman_ford step 8453 current loss 0.654288, current_train_items 270528.
I0302 19:02:21.920355 22535416901760 run.py:483] Algo bellman_ford step 8454 current loss 0.758676, current_train_items 270560.
I0302 19:02:21.940344 22535416901760 run.py:483] Algo bellman_ford step 8455 current loss 0.288203, current_train_items 270592.
I0302 19:02:21.956567 22535416901760 run.py:483] Algo bellman_ford step 8456 current loss 0.530770, current_train_items 270624.
I0302 19:02:21.980763 22535416901760 run.py:483] Algo bellman_ford step 8457 current loss 0.628243, current_train_items 270656.
I0302 19:02:22.010865 22535416901760 run.py:483] Algo bellman_ford step 8458 current loss 0.628550, current_train_items 270688.
I0302 19:02:22.041898 22535416901760 run.py:483] Algo bellman_ford step 8459 current loss 0.672072, current_train_items 270720.
I0302 19:02:22.061797 22535416901760 run.py:483] Algo bellman_ford step 8460 current loss 0.334627, current_train_items 270752.
I0302 19:02:22.077827 22535416901760 run.py:483] Algo bellman_ford step 8461 current loss 0.462986, current_train_items 270784.
I0302 19:02:22.100183 22535416901760 run.py:483] Algo bellman_ford step 8462 current loss 0.576066, current_train_items 270816.
I0302 19:02:22.130708 22535416901760 run.py:483] Algo bellman_ford step 8463 current loss 0.657388, current_train_items 270848.
I0302 19:02:22.164818 22535416901760 run.py:483] Algo bellman_ford step 8464 current loss 0.753921, current_train_items 270880.
I0302 19:02:22.184128 22535416901760 run.py:483] Algo bellman_ford step 8465 current loss 0.310940, current_train_items 270912.
I0302 19:02:22.200598 22535416901760 run.py:483] Algo bellman_ford step 8466 current loss 0.536546, current_train_items 270944.
I0302 19:02:22.223567 22535416901760 run.py:483] Algo bellman_ford step 8467 current loss 0.534791, current_train_items 270976.
I0302 19:02:22.254332 22535416901760 run.py:483] Algo bellman_ford step 8468 current loss 0.816242, current_train_items 271008.
I0302 19:02:22.287301 22535416901760 run.py:483] Algo bellman_ford step 8469 current loss 0.697039, current_train_items 271040.
I0302 19:02:22.307162 22535416901760 run.py:483] Algo bellman_ford step 8470 current loss 0.323295, current_train_items 271072.
I0302 19:02:22.323797 22535416901760 run.py:483] Algo bellman_ford step 8471 current loss 0.570048, current_train_items 271104.
I0302 19:02:22.346776 22535416901760 run.py:483] Algo bellman_ford step 8472 current loss 0.519879, current_train_items 271136.
I0302 19:02:22.375671 22535416901760 run.py:483] Algo bellman_ford step 8473 current loss 0.555377, current_train_items 271168.
I0302 19:02:22.409527 22535416901760 run.py:483] Algo bellman_ford step 8474 current loss 0.690779, current_train_items 271200.
I0302 19:02:22.429393 22535416901760 run.py:483] Algo bellman_ford step 8475 current loss 0.266410, current_train_items 271232.
I0302 19:02:22.445747 22535416901760 run.py:483] Algo bellman_ford step 8476 current loss 0.430786, current_train_items 271264.
I0302 19:02:22.470451 22535416901760 run.py:483] Algo bellman_ford step 8477 current loss 0.608404, current_train_items 271296.
I0302 19:02:22.501672 22535416901760 run.py:483] Algo bellman_ford step 8478 current loss 0.579177, current_train_items 271328.
I0302 19:02:22.536245 22535416901760 run.py:483] Algo bellman_ford step 8479 current loss 0.644901, current_train_items 271360.
I0302 19:02:22.555516 22535416901760 run.py:483] Algo bellman_ford step 8480 current loss 0.296613, current_train_items 271392.
I0302 19:02:22.571817 22535416901760 run.py:483] Algo bellman_ford step 8481 current loss 0.526483, current_train_items 271424.
I0302 19:02:22.595754 22535416901760 run.py:483] Algo bellman_ford step 8482 current loss 0.621544, current_train_items 271456.
I0302 19:02:22.627161 22535416901760 run.py:483] Algo bellman_ford step 8483 current loss 0.730370, current_train_items 271488.
I0302 19:02:22.659296 22535416901760 run.py:483] Algo bellman_ford step 8484 current loss 0.713012, current_train_items 271520.
I0302 19:02:22.679183 22535416901760 run.py:483] Algo bellman_ford step 8485 current loss 0.347091, current_train_items 271552.
I0302 19:02:22.694999 22535416901760 run.py:483] Algo bellman_ford step 8486 current loss 0.369190, current_train_items 271584.
I0302 19:02:22.718195 22535416901760 run.py:483] Algo bellman_ford step 8487 current loss 0.559882, current_train_items 271616.
I0302 19:02:22.750247 22535416901760 run.py:483] Algo bellman_ford step 8488 current loss 0.679085, current_train_items 271648.
I0302 19:02:22.784264 22535416901760 run.py:483] Algo bellman_ford step 8489 current loss 0.744954, current_train_items 271680.
I0302 19:02:22.803994 22535416901760 run.py:483] Algo bellman_ford step 8490 current loss 0.301421, current_train_items 271712.
I0302 19:02:22.819867 22535416901760 run.py:483] Algo bellman_ford step 8491 current loss 0.425588, current_train_items 271744.
I0302 19:02:22.843696 22535416901760 run.py:483] Algo bellman_ford step 8492 current loss 0.669513, current_train_items 271776.
I0302 19:02:22.874717 22535416901760 run.py:483] Algo bellman_ford step 8493 current loss 0.623791, current_train_items 271808.
I0302 19:02:22.907019 22535416901760 run.py:483] Algo bellman_ford step 8494 current loss 0.778212, current_train_items 271840.
I0302 19:02:22.926446 22535416901760 run.py:483] Algo bellman_ford step 8495 current loss 0.266778, current_train_items 271872.
I0302 19:02:22.942936 22535416901760 run.py:483] Algo bellman_ford step 8496 current loss 0.570091, current_train_items 271904.
I0302 19:02:22.966659 22535416901760 run.py:483] Algo bellman_ford step 8497 current loss 0.451498, current_train_items 271936.
I0302 19:02:22.998210 22535416901760 run.py:483] Algo bellman_ford step 8498 current loss 0.626774, current_train_items 271968.
I0302 19:02:23.030012 22535416901760 run.py:483] Algo bellman_ford step 8499 current loss 0.653493, current_train_items 272000.
I0302 19:02:23.049654 22535416901760 run.py:483] Algo bellman_ford step 8500 current loss 0.296430, current_train_items 272032.
I0302 19:02:23.057599 22535416901760 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9140625, 'score': 0.9140625, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:02:23.057747 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.914, val scores are: bellman_ford: 0.914
I0302 19:02:23.074720 22535416901760 run.py:483] Algo bellman_ford step 8501 current loss 0.417949, current_train_items 272064.
I0302 19:02:23.099406 22535416901760 run.py:483] Algo bellman_ford step 8502 current loss 0.610996, current_train_items 272096.
I0302 19:02:23.130104 22535416901760 run.py:483] Algo bellman_ford step 8503 current loss 0.603978, current_train_items 272128.
I0302 19:02:23.162698 22535416901760 run.py:483] Algo bellman_ford step 8504 current loss 0.741064, current_train_items 272160.
I0302 19:02:23.182515 22535416901760 run.py:483] Algo bellman_ford step 8505 current loss 0.321304, current_train_items 272192.
I0302 19:02:23.197769 22535416901760 run.py:483] Algo bellman_ford step 8506 current loss 0.408358, current_train_items 272224.
I0302 19:02:23.220031 22535416901760 run.py:483] Algo bellman_ford step 8507 current loss 0.554740, current_train_items 272256.
I0302 19:02:23.251724 22535416901760 run.py:483] Algo bellman_ford step 8508 current loss 0.567242, current_train_items 272288.
I0302 19:02:23.286350 22535416901760 run.py:483] Algo bellman_ford step 8509 current loss 0.733014, current_train_items 272320.
I0302 19:02:23.305651 22535416901760 run.py:483] Algo bellman_ford step 8510 current loss 0.295570, current_train_items 272352.
I0302 19:02:23.321211 22535416901760 run.py:483] Algo bellman_ford step 8511 current loss 0.431949, current_train_items 272384.
I0302 19:02:23.345701 22535416901760 run.py:483] Algo bellman_ford step 8512 current loss 0.528745, current_train_items 272416.
I0302 19:02:23.376353 22535416901760 run.py:483] Algo bellman_ford step 8513 current loss 0.657395, current_train_items 272448.
I0302 19:02:23.410438 22535416901760 run.py:483] Algo bellman_ford step 8514 current loss 0.754769, current_train_items 272480.
I0302 19:02:23.429915 22535416901760 run.py:483] Algo bellman_ford step 8515 current loss 0.285664, current_train_items 272512.
I0302 19:02:23.445842 22535416901760 run.py:483] Algo bellman_ford step 8516 current loss 0.425202, current_train_items 272544.
I0302 19:02:23.469303 22535416901760 run.py:483] Algo bellman_ford step 8517 current loss 0.542279, current_train_items 272576.
I0302 19:02:23.501868 22535416901760 run.py:483] Algo bellman_ford step 8518 current loss 0.595751, current_train_items 272608.
I0302 19:02:23.534337 22535416901760 run.py:483] Algo bellman_ford step 8519 current loss 0.702268, current_train_items 272640.
I0302 19:02:23.553793 22535416901760 run.py:483] Algo bellman_ford step 8520 current loss 0.286410, current_train_items 272672.
I0302 19:02:23.569636 22535416901760 run.py:483] Algo bellman_ford step 8521 current loss 0.458508, current_train_items 272704.
I0302 19:02:23.592910 22535416901760 run.py:483] Algo bellman_ford step 8522 current loss 0.498595, current_train_items 272736.
I0302 19:02:23.624252 22535416901760 run.py:483] Algo bellman_ford step 8523 current loss 0.706796, current_train_items 272768.
I0302 19:02:23.658460 22535416901760 run.py:483] Algo bellman_ford step 8524 current loss 0.781427, current_train_items 272800.
I0302 19:02:23.677880 22535416901760 run.py:483] Algo bellman_ford step 8525 current loss 0.309211, current_train_items 272832.
I0302 19:02:23.694021 22535416901760 run.py:483] Algo bellman_ford step 8526 current loss 0.459727, current_train_items 272864.
I0302 19:02:23.717385 22535416901760 run.py:483] Algo bellman_ford step 8527 current loss 0.632682, current_train_items 272896.
I0302 19:02:23.748376 22535416901760 run.py:483] Algo bellman_ford step 8528 current loss 0.547449, current_train_items 272928.
I0302 19:02:23.782381 22535416901760 run.py:483] Algo bellman_ford step 8529 current loss 0.733477, current_train_items 272960.
I0302 19:02:23.801858 22535416901760 run.py:483] Algo bellman_ford step 8530 current loss 0.251931, current_train_items 272992.
I0302 19:02:23.817974 22535416901760 run.py:483] Algo bellman_ford step 8531 current loss 0.523328, current_train_items 273024.
I0302 19:02:23.840486 22535416901760 run.py:483] Algo bellman_ford step 8532 current loss 0.534977, current_train_items 273056.
I0302 19:02:23.871094 22535416901760 run.py:483] Algo bellman_ford step 8533 current loss 0.664174, current_train_items 273088.
I0302 19:02:23.903475 22535416901760 run.py:483] Algo bellman_ford step 8534 current loss 0.694107, current_train_items 273120.
I0302 19:02:23.923049 22535416901760 run.py:483] Algo bellman_ford step 8535 current loss 0.237698, current_train_items 273152.
I0302 19:02:23.939322 22535416901760 run.py:483] Algo bellman_ford step 8536 current loss 0.457887, current_train_items 273184.
I0302 19:02:23.962752 22535416901760 run.py:483] Algo bellman_ford step 8537 current loss 0.602032, current_train_items 273216.
I0302 19:02:23.994239 22535416901760 run.py:483] Algo bellman_ford step 8538 current loss 0.591982, current_train_items 273248.
I0302 19:02:24.029335 22535416901760 run.py:483] Algo bellman_ford step 8539 current loss 0.827428, current_train_items 273280.
I0302 19:02:24.049316 22535416901760 run.py:483] Algo bellman_ford step 8540 current loss 0.264392, current_train_items 273312.
I0302 19:02:24.065311 22535416901760 run.py:483] Algo bellman_ford step 8541 current loss 0.440492, current_train_items 273344.
I0302 19:02:24.089215 22535416901760 run.py:483] Algo bellman_ford step 8542 current loss 0.550907, current_train_items 273376.
I0302 19:02:24.121084 22535416901760 run.py:483] Algo bellman_ford step 8543 current loss 0.612334, current_train_items 273408.
I0302 19:02:24.154078 22535416901760 run.py:483] Algo bellman_ford step 8544 current loss 0.749521, current_train_items 273440.
I0302 19:02:24.173317 22535416901760 run.py:483] Algo bellman_ford step 8545 current loss 0.250412, current_train_items 273472.
I0302 19:02:24.188854 22535416901760 run.py:483] Algo bellman_ford step 8546 current loss 0.371687, current_train_items 273504.
I0302 19:02:24.212730 22535416901760 run.py:483] Algo bellman_ford step 8547 current loss 0.601111, current_train_items 273536.
I0302 19:02:24.243733 22535416901760 run.py:483] Algo bellman_ford step 8548 current loss 0.609054, current_train_items 273568.
I0302 19:02:24.277594 22535416901760 run.py:483] Algo bellman_ford step 8549 current loss 0.844096, current_train_items 273600.
I0302 19:02:24.297052 22535416901760 run.py:483] Algo bellman_ford step 8550 current loss 0.292191, current_train_items 273632.
I0302 19:02:24.305210 22535416901760 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:02:24.305317 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 19:02:24.322345 22535416901760 run.py:483] Algo bellman_ford step 8551 current loss 0.468085, current_train_items 273664.
I0302 19:02:24.347091 22535416901760 run.py:483] Algo bellman_ford step 8552 current loss 0.575236, current_train_items 273696.
I0302 19:02:24.379056 22535416901760 run.py:483] Algo bellman_ford step 8553 current loss 0.694875, current_train_items 273728.
I0302 19:02:24.411021 22535416901760 run.py:483] Algo bellman_ford step 8554 current loss 0.695939, current_train_items 273760.
I0302 19:02:24.431170 22535416901760 run.py:483] Algo bellman_ford step 8555 current loss 0.361991, current_train_items 273792.
I0302 19:02:24.446904 22535416901760 run.py:483] Algo bellman_ford step 8556 current loss 0.489773, current_train_items 273824.
I0302 19:02:24.470109 22535416901760 run.py:483] Algo bellman_ford step 8557 current loss 0.662822, current_train_items 273856.
I0302 19:02:24.501579 22535416901760 run.py:483] Algo bellman_ford step 8558 current loss 0.758819, current_train_items 273888.
I0302 19:02:24.536073 22535416901760 run.py:483] Algo bellman_ford step 8559 current loss 0.955420, current_train_items 273920.
I0302 19:02:24.555819 22535416901760 run.py:483] Algo bellman_ford step 8560 current loss 0.298953, current_train_items 273952.
I0302 19:02:24.572339 22535416901760 run.py:483] Algo bellman_ford step 8561 current loss 0.457694, current_train_items 273984.
I0302 19:02:24.595821 22535416901760 run.py:483] Algo bellman_ford step 8562 current loss 0.612592, current_train_items 274016.
I0302 19:02:24.628469 22535416901760 run.py:483] Algo bellman_ford step 8563 current loss 0.770307, current_train_items 274048.
I0302 19:02:24.660666 22535416901760 run.py:483] Algo bellman_ford step 8564 current loss 0.804154, current_train_items 274080.
I0302 19:02:24.680390 22535416901760 run.py:483] Algo bellman_ford step 8565 current loss 0.284387, current_train_items 274112.
I0302 19:02:24.697045 22535416901760 run.py:483] Algo bellman_ford step 8566 current loss 0.468299, current_train_items 274144.
I0302 19:02:24.721985 22535416901760 run.py:483] Algo bellman_ford step 8567 current loss 0.568327, current_train_items 274176.
I0302 19:02:24.753706 22535416901760 run.py:483] Algo bellman_ford step 8568 current loss 0.678635, current_train_items 274208.
I0302 19:02:24.787792 22535416901760 run.py:483] Algo bellman_ford step 8569 current loss 0.753466, current_train_items 274240.
I0302 19:02:24.807519 22535416901760 run.py:483] Algo bellman_ford step 8570 current loss 0.302740, current_train_items 274272.
I0302 19:02:24.823579 22535416901760 run.py:483] Algo bellman_ford step 8571 current loss 0.576972, current_train_items 274304.
I0302 19:02:24.846600 22535416901760 run.py:483] Algo bellman_ford step 8572 current loss 0.553021, current_train_items 274336.
I0302 19:02:24.877863 22535416901760 run.py:483] Algo bellman_ford step 8573 current loss 0.618796, current_train_items 274368.
I0302 19:02:24.909704 22535416901760 run.py:483] Algo bellman_ford step 8574 current loss 0.735116, current_train_items 274400.
I0302 19:02:24.929630 22535416901760 run.py:483] Algo bellman_ford step 8575 current loss 0.340569, current_train_items 274432.
I0302 19:02:24.945687 22535416901760 run.py:483] Algo bellman_ford step 8576 current loss 0.468276, current_train_items 274464.
I0302 19:02:24.969303 22535416901760 run.py:483] Algo bellman_ford step 8577 current loss 0.662706, current_train_items 274496.
I0302 19:02:25.001446 22535416901760 run.py:483] Algo bellman_ford step 8578 current loss 0.773892, current_train_items 274528.
I0302 19:02:25.035402 22535416901760 run.py:483] Algo bellman_ford step 8579 current loss 0.792670, current_train_items 274560.
I0302 19:02:25.055163 22535416901760 run.py:483] Algo bellman_ford step 8580 current loss 0.293431, current_train_items 274592.
I0302 19:02:25.071265 22535416901760 run.py:483] Algo bellman_ford step 8581 current loss 0.430912, current_train_items 274624.
I0302 19:02:25.093998 22535416901760 run.py:483] Algo bellman_ford step 8582 current loss 0.561457, current_train_items 274656.
I0302 19:02:25.125370 22535416901760 run.py:483] Algo bellman_ford step 8583 current loss 0.711282, current_train_items 274688.
I0302 19:02:25.159737 22535416901760 run.py:483] Algo bellman_ford step 8584 current loss 0.798339, current_train_items 274720.
I0302 19:02:25.179465 22535416901760 run.py:483] Algo bellman_ford step 8585 current loss 0.297563, current_train_items 274752.
I0302 19:02:25.195474 22535416901760 run.py:483] Algo bellman_ford step 8586 current loss 0.469449, current_train_items 274784.
I0302 19:02:25.220131 22535416901760 run.py:483] Algo bellman_ford step 8587 current loss 0.688707, current_train_items 274816.
I0302 19:02:25.251285 22535416901760 run.py:483] Algo bellman_ford step 8588 current loss 0.549976, current_train_items 274848.
I0302 19:02:25.283900 22535416901760 run.py:483] Algo bellman_ford step 8589 current loss 0.775393, current_train_items 274880.
I0302 19:02:25.303668 22535416901760 run.py:483] Algo bellman_ford step 8590 current loss 0.336486, current_train_items 274912.
I0302 19:02:25.319069 22535416901760 run.py:483] Algo bellman_ford step 8591 current loss 0.416880, current_train_items 274944.
I0302 19:02:25.342788 22535416901760 run.py:483] Algo bellman_ford step 8592 current loss 0.620600, current_train_items 274976.
I0302 19:02:25.374727 22535416901760 run.py:483] Algo bellman_ford step 8593 current loss 0.799073, current_train_items 275008.
I0302 19:02:25.408335 22535416901760 run.py:483] Algo bellman_ford step 8594 current loss 0.794080, current_train_items 275040.
I0302 19:02:25.427654 22535416901760 run.py:483] Algo bellman_ford step 8595 current loss 0.255661, current_train_items 275072.
I0302 19:02:25.443519 22535416901760 run.py:483] Algo bellman_ford step 8596 current loss 0.468897, current_train_items 275104.
I0302 19:02:25.467030 22535416901760 run.py:483] Algo bellman_ford step 8597 current loss 0.495197, current_train_items 275136.
I0302 19:02:25.497177 22535416901760 run.py:483] Algo bellman_ford step 8598 current loss 0.554099, current_train_items 275168.
I0302 19:02:25.531332 22535416901760 run.py:483] Algo bellman_ford step 8599 current loss 0.765267, current_train_items 275200.
I0302 19:02:25.551192 22535416901760 run.py:483] Algo bellman_ford step 8600 current loss 0.349525, current_train_items 275232.
I0302 19:02:25.558979 22535416901760 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.931640625, 'score': 0.931640625, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:02:25.559086 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.932, val scores are: bellman_ford: 0.932
I0302 19:02:25.576092 22535416901760 run.py:483] Algo bellman_ford step 8601 current loss 0.476394, current_train_items 275264.
I0302 19:02:25.601008 22535416901760 run.py:483] Algo bellman_ford step 8602 current loss 0.685730, current_train_items 275296.
I0302 19:02:25.633652 22535416901760 run.py:483] Algo bellman_ford step 8603 current loss 0.622515, current_train_items 275328.
I0302 19:02:25.667600 22535416901760 run.py:483] Algo bellman_ford step 8604 current loss 0.745361, current_train_items 275360.
I0302 19:02:25.687700 22535416901760 run.py:483] Algo bellman_ford step 8605 current loss 0.308378, current_train_items 275392.
I0302 19:02:25.702989 22535416901760 run.py:483] Algo bellman_ford step 8606 current loss 0.358841, current_train_items 275424.
I0302 19:02:25.726205 22535416901760 run.py:483] Algo bellman_ford step 8607 current loss 0.533222, current_train_items 275456.
I0302 19:02:25.757467 22535416901760 run.py:483] Algo bellman_ford step 8608 current loss 0.572953, current_train_items 275488.
I0302 19:02:25.792737 22535416901760 run.py:483] Algo bellman_ford step 8609 current loss 0.738448, current_train_items 275520.
I0302 19:02:25.812382 22535416901760 run.py:483] Algo bellman_ford step 8610 current loss 0.276903, current_train_items 275552.
I0302 19:02:25.828625 22535416901760 run.py:483] Algo bellman_ford step 8611 current loss 0.417026, current_train_items 275584.
I0302 19:02:25.853235 22535416901760 run.py:483] Algo bellman_ford step 8612 current loss 0.597250, current_train_items 275616.
I0302 19:02:25.884750 22535416901760 run.py:483] Algo bellman_ford step 8613 current loss 0.617125, current_train_items 275648.
I0302 19:02:25.916508 22535416901760 run.py:483] Algo bellman_ford step 8614 current loss 0.623551, current_train_items 275680.
I0302 19:02:25.936076 22535416901760 run.py:483] Algo bellman_ford step 8615 current loss 0.232912, current_train_items 275712.
I0302 19:02:25.952204 22535416901760 run.py:483] Algo bellman_ford step 8616 current loss 0.432402, current_train_items 275744.
I0302 19:02:25.976058 22535416901760 run.py:483] Algo bellman_ford step 8617 current loss 0.529930, current_train_items 275776.
I0302 19:02:26.008195 22535416901760 run.py:483] Algo bellman_ford step 8618 current loss 0.645079, current_train_items 275808.
I0302 19:02:26.042150 22535416901760 run.py:483] Algo bellman_ford step 8619 current loss 0.723823, current_train_items 275840.
I0302 19:02:26.061911 22535416901760 run.py:483] Algo bellman_ford step 8620 current loss 0.287809, current_train_items 275872.
I0302 19:02:26.078162 22535416901760 run.py:483] Algo bellman_ford step 8621 current loss 0.426834, current_train_items 275904.
I0302 19:02:26.102744 22535416901760 run.py:483] Algo bellman_ford step 8622 current loss 0.554842, current_train_items 275936.
I0302 19:02:26.135169 22535416901760 run.py:483] Algo bellman_ford step 8623 current loss 0.559140, current_train_items 275968.
I0302 19:02:26.168453 22535416901760 run.py:483] Algo bellman_ford step 8624 current loss 0.612653, current_train_items 276000.
I0302 19:02:26.188281 22535416901760 run.py:483] Algo bellman_ford step 8625 current loss 0.313623, current_train_items 276032.
I0302 19:02:26.204436 22535416901760 run.py:483] Algo bellman_ford step 8626 current loss 0.398604, current_train_items 276064.
I0302 19:02:26.228336 22535416901760 run.py:483] Algo bellman_ford step 8627 current loss 0.537958, current_train_items 276096.
I0302 19:02:26.261007 22535416901760 run.py:483] Algo bellman_ford step 8628 current loss 0.684235, current_train_items 276128.
I0302 19:02:26.294165 22535416901760 run.py:483] Algo bellman_ford step 8629 current loss 0.896998, current_train_items 276160.
I0302 19:02:26.313650 22535416901760 run.py:483] Algo bellman_ford step 8630 current loss 0.336789, current_train_items 276192.
I0302 19:02:26.330106 22535416901760 run.py:483] Algo bellman_ford step 8631 current loss 0.427307, current_train_items 276224.
I0302 19:02:26.353528 22535416901760 run.py:483] Algo bellman_ford step 8632 current loss 0.658834, current_train_items 276256.
I0302 19:02:26.384734 22535416901760 run.py:483] Algo bellman_ford step 8633 current loss 0.567419, current_train_items 276288.
I0302 19:02:26.417975 22535416901760 run.py:483] Algo bellman_ford step 8634 current loss 0.746998, current_train_items 276320.
I0302 19:02:26.437794 22535416901760 run.py:483] Algo bellman_ford step 8635 current loss 0.346945, current_train_items 276352.
I0302 19:02:26.453875 22535416901760 run.py:483] Algo bellman_ford step 8636 current loss 0.395448, current_train_items 276384.
I0302 19:02:26.478544 22535416901760 run.py:483] Algo bellman_ford step 8637 current loss 0.562638, current_train_items 276416.
I0302 19:02:26.509468 22535416901760 run.py:483] Algo bellman_ford step 8638 current loss 0.570178, current_train_items 276448.
I0302 19:02:26.543669 22535416901760 run.py:483] Algo bellman_ford step 8639 current loss 0.703704, current_train_items 276480.
I0302 19:02:26.563031 22535416901760 run.py:483] Algo bellman_ford step 8640 current loss 0.294124, current_train_items 276512.
I0302 19:02:26.579502 22535416901760 run.py:483] Algo bellman_ford step 8641 current loss 0.459419, current_train_items 276544.
I0302 19:02:26.603474 22535416901760 run.py:483] Algo bellman_ford step 8642 current loss 0.487221, current_train_items 276576.
I0302 19:02:26.635752 22535416901760 run.py:483] Algo bellman_ford step 8643 current loss 0.614337, current_train_items 276608.
I0302 19:02:26.669579 22535416901760 run.py:483] Algo bellman_ford step 8644 current loss 0.808995, current_train_items 276640.
I0302 19:02:26.689166 22535416901760 run.py:483] Algo bellman_ford step 8645 current loss 0.348887, current_train_items 276672.
I0302 19:02:26.705344 22535416901760 run.py:483] Algo bellman_ford step 8646 current loss 0.413256, current_train_items 276704.
I0302 19:02:26.729510 22535416901760 run.py:483] Algo bellman_ford step 8647 current loss 0.614400, current_train_items 276736.
I0302 19:02:26.761903 22535416901760 run.py:483] Algo bellman_ford step 8648 current loss 0.668189, current_train_items 276768.
I0302 19:02:26.795106 22535416901760 run.py:483] Algo bellman_ford step 8649 current loss 0.732730, current_train_items 276800.
I0302 19:02:26.814619 22535416901760 run.py:483] Algo bellman_ford step 8650 current loss 0.404874, current_train_items 276832.
I0302 19:02:26.822647 22535416901760 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:02:26.822753 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:02:26.839495 22535416901760 run.py:483] Algo bellman_ford step 8651 current loss 0.373470, current_train_items 276864.
I0302 19:02:26.863720 22535416901760 run.py:483] Algo bellman_ford step 8652 current loss 0.530042, current_train_items 276896.
I0302 19:02:26.895813 22535416901760 run.py:483] Algo bellman_ford step 8653 current loss 0.681484, current_train_items 276928.
I0302 19:02:26.930706 22535416901760 run.py:483] Algo bellman_ford step 8654 current loss 0.821999, current_train_items 276960.
I0302 19:02:26.950668 22535416901760 run.py:483] Algo bellman_ford step 8655 current loss 0.249657, current_train_items 276992.
I0302 19:02:26.966444 22535416901760 run.py:483] Algo bellman_ford step 8656 current loss 0.499203, current_train_items 277024.
I0302 19:02:26.991271 22535416901760 run.py:483] Algo bellman_ford step 8657 current loss 0.613329, current_train_items 277056.
I0302 19:02:27.023389 22535416901760 run.py:483] Algo bellman_ford step 8658 current loss 0.658489, current_train_items 277088.
I0302 19:02:27.056771 22535416901760 run.py:483] Algo bellman_ford step 8659 current loss 0.710613, current_train_items 277120.
I0302 19:02:27.076432 22535416901760 run.py:483] Algo bellman_ford step 8660 current loss 0.261814, current_train_items 277152.
I0302 19:02:27.092432 22535416901760 run.py:483] Algo bellman_ford step 8661 current loss 0.446158, current_train_items 277184.
I0302 19:02:27.115778 22535416901760 run.py:483] Algo bellman_ford step 8662 current loss 0.595821, current_train_items 277216.
I0302 19:02:27.146666 22535416901760 run.py:483] Algo bellman_ford step 8663 current loss 0.660428, current_train_items 277248.
I0302 19:02:27.181286 22535416901760 run.py:483] Algo bellman_ford step 8664 current loss 0.676641, current_train_items 277280.
I0302 19:02:27.200823 22535416901760 run.py:483] Algo bellman_ford step 8665 current loss 0.300039, current_train_items 277312.
I0302 19:02:27.216552 22535416901760 run.py:483] Algo bellman_ford step 8666 current loss 0.377249, current_train_items 277344.
I0302 19:02:27.240179 22535416901760 run.py:483] Algo bellman_ford step 8667 current loss 0.554526, current_train_items 277376.
I0302 19:02:27.270881 22535416901760 run.py:483] Algo bellman_ford step 8668 current loss 0.651724, current_train_items 277408.
I0302 19:02:27.303398 22535416901760 run.py:483] Algo bellman_ford step 8669 current loss 0.688981, current_train_items 277440.
I0302 19:02:27.322992 22535416901760 run.py:483] Algo bellman_ford step 8670 current loss 0.282768, current_train_items 277472.
I0302 19:02:27.339287 22535416901760 run.py:483] Algo bellman_ford step 8671 current loss 0.468934, current_train_items 277504.
I0302 19:02:27.362202 22535416901760 run.py:483] Algo bellman_ford step 8672 current loss 0.541064, current_train_items 277536.
I0302 19:02:27.393710 22535416901760 run.py:483] Algo bellman_ford step 8673 current loss 0.622997, current_train_items 277568.
I0302 19:02:27.424937 22535416901760 run.py:483] Algo bellman_ford step 8674 current loss 0.798951, current_train_items 277600.
I0302 19:02:27.444638 22535416901760 run.py:483] Algo bellman_ford step 8675 current loss 0.293606, current_train_items 277632.
I0302 19:02:27.460688 22535416901760 run.py:483] Algo bellman_ford step 8676 current loss 0.485978, current_train_items 277664.
I0302 19:02:27.484145 22535416901760 run.py:483] Algo bellman_ford step 8677 current loss 0.624202, current_train_items 277696.
I0302 19:02:27.515572 22535416901760 run.py:483] Algo bellman_ford step 8678 current loss 0.662807, current_train_items 277728.
I0302 19:02:27.548547 22535416901760 run.py:483] Algo bellman_ford step 8679 current loss 0.721954, current_train_items 277760.
I0302 19:02:27.567701 22535416901760 run.py:483] Algo bellman_ford step 8680 current loss 0.380249, current_train_items 277792.
I0302 19:02:27.583948 22535416901760 run.py:483] Algo bellman_ford step 8681 current loss 0.499359, current_train_items 277824.
I0302 19:02:27.607913 22535416901760 run.py:483] Algo bellman_ford step 8682 current loss 0.526793, current_train_items 277856.
I0302 19:02:27.639016 22535416901760 run.py:483] Algo bellman_ford step 8683 current loss 0.580096, current_train_items 277888.
I0302 19:02:27.671711 22535416901760 run.py:483] Algo bellman_ford step 8684 current loss 0.740979, current_train_items 277920.
I0302 19:02:27.691627 22535416901760 run.py:483] Algo bellman_ford step 8685 current loss 0.321295, current_train_items 277952.
I0302 19:02:27.707454 22535416901760 run.py:483] Algo bellman_ford step 8686 current loss 0.476878, current_train_items 277984.
I0302 19:02:27.731527 22535416901760 run.py:483] Algo bellman_ford step 8687 current loss 0.646770, current_train_items 278016.
I0302 19:02:27.763647 22535416901760 run.py:483] Algo bellman_ford step 8688 current loss 0.646014, current_train_items 278048.
I0302 19:02:27.796087 22535416901760 run.py:483] Algo bellman_ford step 8689 current loss 0.654974, current_train_items 278080.
I0302 19:02:27.815489 22535416901760 run.py:483] Algo bellman_ford step 8690 current loss 0.251703, current_train_items 278112.
I0302 19:02:27.831713 22535416901760 run.py:483] Algo bellman_ford step 8691 current loss 0.471808, current_train_items 278144.
I0302 19:02:27.854687 22535416901760 run.py:483] Algo bellman_ford step 8692 current loss 0.597282, current_train_items 278176.
I0302 19:02:27.885782 22535416901760 run.py:483] Algo bellman_ford step 8693 current loss 0.628993, current_train_items 278208.
I0302 19:02:27.918773 22535416901760 run.py:483] Algo bellman_ford step 8694 current loss 0.790278, current_train_items 278240.
I0302 19:02:27.938170 22535416901760 run.py:483] Algo bellman_ford step 8695 current loss 0.333874, current_train_items 278272.
I0302 19:02:27.954007 22535416901760 run.py:483] Algo bellman_ford step 8696 current loss 0.464574, current_train_items 278304.
I0302 19:02:27.977806 22535416901760 run.py:483] Algo bellman_ford step 8697 current loss 0.659524, current_train_items 278336.
I0302 19:02:28.009389 22535416901760 run.py:483] Algo bellman_ford step 8698 current loss 0.672671, current_train_items 278368.
I0302 19:02:28.044470 22535416901760 run.py:483] Algo bellman_ford step 8699 current loss 0.894643, current_train_items 278400.
I0302 19:02:28.064095 22535416901760 run.py:483] Algo bellman_ford step 8700 current loss 0.272254, current_train_items 278432.
I0302 19:02:28.071777 22535416901760 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.9482421875, 'score': 0.9482421875, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:02:28.071883 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.948, val scores are: bellman_ford: 0.948
I0302 19:02:28.088732 22535416901760 run.py:483] Algo bellman_ford step 8701 current loss 0.426749, current_train_items 278464.
I0302 19:02:28.112602 22535416901760 run.py:483] Algo bellman_ford step 8702 current loss 0.548149, current_train_items 278496.
I0302 19:02:28.145107 22535416901760 run.py:483] Algo bellman_ford step 8703 current loss 0.610820, current_train_items 278528.
I0302 19:02:28.180373 22535416901760 run.py:483] Algo bellman_ford step 8704 current loss 0.699029, current_train_items 278560.
I0302 19:02:28.200353 22535416901760 run.py:483] Algo bellman_ford step 8705 current loss 0.310305, current_train_items 278592.
I0302 19:02:28.216049 22535416901760 run.py:483] Algo bellman_ford step 8706 current loss 0.439322, current_train_items 278624.
I0302 19:02:28.239620 22535416901760 run.py:483] Algo bellman_ford step 8707 current loss 0.574059, current_train_items 278656.
I0302 19:02:28.271847 22535416901760 run.py:483] Algo bellman_ford step 8708 current loss 0.602585, current_train_items 278688.
I0302 19:02:28.305301 22535416901760 run.py:483] Algo bellman_ford step 8709 current loss 0.731698, current_train_items 278720.
I0302 19:02:28.324534 22535416901760 run.py:483] Algo bellman_ford step 8710 current loss 0.332725, current_train_items 278752.
I0302 19:02:28.340744 22535416901760 run.py:483] Algo bellman_ford step 8711 current loss 0.506350, current_train_items 278784.
I0302 19:02:28.364386 22535416901760 run.py:483] Algo bellman_ford step 8712 current loss 0.561244, current_train_items 278816.
I0302 19:02:28.397910 22535416901760 run.py:483] Algo bellman_ford step 8713 current loss 0.784388, current_train_items 278848.
I0302 19:02:28.430075 22535416901760 run.py:483] Algo bellman_ford step 8714 current loss 0.741934, current_train_items 278880.
I0302 19:02:28.449395 22535416901760 run.py:483] Algo bellman_ford step 8715 current loss 0.271959, current_train_items 278912.
I0302 19:02:28.465853 22535416901760 run.py:483] Algo bellman_ford step 8716 current loss 0.570628, current_train_items 278944.
I0302 19:02:28.489251 22535416901760 run.py:483] Algo bellman_ford step 8717 current loss 0.531359, current_train_items 278976.
I0302 19:02:28.519712 22535416901760 run.py:483] Algo bellman_ford step 8718 current loss 0.611115, current_train_items 279008.
I0302 19:02:28.551930 22535416901760 run.py:483] Algo bellman_ford step 8719 current loss 0.797316, current_train_items 279040.
I0302 19:02:28.571380 22535416901760 run.py:483] Algo bellman_ford step 8720 current loss 0.324195, current_train_items 279072.
I0302 19:02:28.587812 22535416901760 run.py:483] Algo bellman_ford step 8721 current loss 0.495083, current_train_items 279104.
I0302 19:02:28.612409 22535416901760 run.py:483] Algo bellman_ford step 8722 current loss 0.523762, current_train_items 279136.
I0302 19:02:28.643664 22535416901760 run.py:483] Algo bellman_ford step 8723 current loss 0.585082, current_train_items 279168.
I0302 19:02:28.676409 22535416901760 run.py:483] Algo bellman_ford step 8724 current loss 0.666445, current_train_items 279200.
I0302 19:02:28.696040 22535416901760 run.py:483] Algo bellman_ford step 8725 current loss 0.303388, current_train_items 279232.
I0302 19:02:28.712368 22535416901760 run.py:483] Algo bellman_ford step 8726 current loss 0.450007, current_train_items 279264.
I0302 19:02:28.736469 22535416901760 run.py:483] Algo bellman_ford step 8727 current loss 0.529474, current_train_items 279296.
I0302 19:02:28.767755 22535416901760 run.py:483] Algo bellman_ford step 8728 current loss 0.566738, current_train_items 279328.
I0302 19:02:28.800742 22535416901760 run.py:483] Algo bellman_ford step 8729 current loss 0.745437, current_train_items 279360.
I0302 19:02:28.820184 22535416901760 run.py:483] Algo bellman_ford step 8730 current loss 0.287219, current_train_items 279392.
I0302 19:02:28.835896 22535416901760 run.py:483] Algo bellman_ford step 8731 current loss 0.369970, current_train_items 279424.
I0302 19:02:28.859762 22535416901760 run.py:483] Algo bellman_ford step 8732 current loss 0.572821, current_train_items 279456.
I0302 19:02:28.891530 22535416901760 run.py:483] Algo bellman_ford step 8733 current loss 0.642514, current_train_items 279488.
I0302 19:02:28.926378 22535416901760 run.py:483] Algo bellman_ford step 8734 current loss 0.769487, current_train_items 279520.
I0302 19:02:28.945642 22535416901760 run.py:483] Algo bellman_ford step 8735 current loss 0.340734, current_train_items 279552.
I0302 19:02:28.961621 22535416901760 run.py:483] Algo bellman_ford step 8736 current loss 0.430440, current_train_items 279584.
I0302 19:02:28.986322 22535416901760 run.py:483] Algo bellman_ford step 8737 current loss 0.586107, current_train_items 279616.
I0302 19:02:29.017305 22535416901760 run.py:483] Algo bellman_ford step 8738 current loss 0.757810, current_train_items 279648.
I0302 19:02:29.051470 22535416901760 run.py:483] Algo bellman_ford step 8739 current loss 1.029709, current_train_items 279680.
I0302 19:02:29.070940 22535416901760 run.py:483] Algo bellman_ford step 8740 current loss 0.350693, current_train_items 279712.
I0302 19:02:29.086845 22535416901760 run.py:483] Algo bellman_ford step 8741 current loss 0.500753, current_train_items 279744.
I0302 19:02:29.110691 22535416901760 run.py:483] Algo bellman_ford step 8742 current loss 0.652285, current_train_items 279776.
I0302 19:02:29.142709 22535416901760 run.py:483] Algo bellman_ford step 8743 current loss 0.703599, current_train_items 279808.
I0302 19:02:29.176292 22535416901760 run.py:483] Algo bellman_ford step 8744 current loss 0.686571, current_train_items 279840.
I0302 19:02:29.195756 22535416901760 run.py:483] Algo bellman_ford step 8745 current loss 0.306999, current_train_items 279872.
I0302 19:02:29.212074 22535416901760 run.py:483] Algo bellman_ford step 8746 current loss 0.507349, current_train_items 279904.
I0302 19:02:29.235380 22535416901760 run.py:483] Algo bellman_ford step 8747 current loss 0.619206, current_train_items 279936.
I0302 19:02:29.267815 22535416901760 run.py:483] Algo bellman_ford step 8748 current loss 0.673268, current_train_items 279968.
I0302 19:02:29.302312 22535416901760 run.py:483] Algo bellman_ford step 8749 current loss 0.795609, current_train_items 280000.
I0302 19:02:29.321646 22535416901760 run.py:483] Algo bellman_ford step 8750 current loss 0.302407, current_train_items 280032.
I0302 19:02:29.329961 22535416901760 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.919921875, 'score': 0.919921875, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:02:29.330068 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.920, val scores are: bellman_ford: 0.920
I0302 19:02:29.346868 22535416901760 run.py:483] Algo bellman_ford step 8751 current loss 0.373508, current_train_items 280064.
I0302 19:02:29.370562 22535416901760 run.py:483] Algo bellman_ford step 8752 current loss 0.528137, current_train_items 280096.
I0302 19:02:29.402987 22535416901760 run.py:483] Algo bellman_ford step 8753 current loss 0.704858, current_train_items 280128.
I0302 19:02:29.436933 22535416901760 run.py:483] Algo bellman_ford step 8754 current loss 0.686254, current_train_items 280160.
I0302 19:02:29.456902 22535416901760 run.py:483] Algo bellman_ford step 8755 current loss 0.301088, current_train_items 280192.
I0302 19:02:29.472626 22535416901760 run.py:483] Algo bellman_ford step 8756 current loss 0.472241, current_train_items 280224.
I0302 19:02:29.497731 22535416901760 run.py:483] Algo bellman_ford step 8757 current loss 0.598638, current_train_items 280256.
I0302 19:02:29.528747 22535416901760 run.py:483] Algo bellman_ford step 8758 current loss 0.621193, current_train_items 280288.
I0302 19:02:29.562114 22535416901760 run.py:483] Algo bellman_ford step 8759 current loss 0.699436, current_train_items 280320.
I0302 19:02:29.582171 22535416901760 run.py:483] Algo bellman_ford step 8760 current loss 0.330742, current_train_items 280352.
I0302 19:02:29.598440 22535416901760 run.py:483] Algo bellman_ford step 8761 current loss 0.442878, current_train_items 280384.
I0302 19:02:29.622529 22535416901760 run.py:483] Algo bellman_ford step 8762 current loss 0.567370, current_train_items 280416.
I0302 19:02:29.653390 22535416901760 run.py:483] Algo bellman_ford step 8763 current loss 0.594419, current_train_items 280448.
I0302 19:02:29.688615 22535416901760 run.py:483] Algo bellman_ford step 8764 current loss 0.844152, current_train_items 280480.
I0302 19:02:29.708057 22535416901760 run.py:483] Algo bellman_ford step 8765 current loss 0.286386, current_train_items 280512.
I0302 19:02:29.724525 22535416901760 run.py:483] Algo bellman_ford step 8766 current loss 0.418156, current_train_items 280544.
I0302 19:02:29.746949 22535416901760 run.py:483] Algo bellman_ford step 8767 current loss 0.476491, current_train_items 280576.
I0302 19:02:29.777766 22535416901760 run.py:483] Algo bellman_ford step 8768 current loss 0.622913, current_train_items 280608.
I0302 19:02:29.809507 22535416901760 run.py:483] Algo bellman_ford step 8769 current loss 0.573302, current_train_items 280640.
I0302 19:02:29.829608 22535416901760 run.py:483] Algo bellman_ford step 8770 current loss 0.271653, current_train_items 280672.
I0302 19:02:29.846063 22535416901760 run.py:483] Algo bellman_ford step 8771 current loss 0.399208, current_train_items 280704.
I0302 19:02:29.870018 22535416901760 run.py:483] Algo bellman_ford step 8772 current loss 0.591121, current_train_items 280736.
I0302 19:02:29.901993 22535416901760 run.py:483] Algo bellman_ford step 8773 current loss 0.736472, current_train_items 280768.
I0302 19:02:29.934787 22535416901760 run.py:483] Algo bellman_ford step 8774 current loss 0.688939, current_train_items 280800.
I0302 19:02:29.954684 22535416901760 run.py:483] Algo bellman_ford step 8775 current loss 0.318729, current_train_items 280832.
I0302 19:02:29.970816 22535416901760 run.py:483] Algo bellman_ford step 8776 current loss 0.434513, current_train_items 280864.
I0302 19:02:29.993629 22535416901760 run.py:483] Algo bellman_ford step 8777 current loss 0.526411, current_train_items 280896.
I0302 19:02:30.024803 22535416901760 run.py:483] Algo bellman_ford step 8778 current loss 0.539363, current_train_items 280928.
I0302 19:02:30.059424 22535416901760 run.py:483] Algo bellman_ford step 8779 current loss 0.624859, current_train_items 280960.
I0302 19:02:30.078784 22535416901760 run.py:483] Algo bellman_ford step 8780 current loss 0.348970, current_train_items 280992.
I0302 19:02:30.094801 22535416901760 run.py:483] Algo bellman_ford step 8781 current loss 0.511125, current_train_items 281024.
I0302 19:02:30.118593 22535416901760 run.py:483] Algo bellman_ford step 8782 current loss 0.621354, current_train_items 281056.
I0302 19:02:30.150857 22535416901760 run.py:483] Algo bellman_ford step 8783 current loss 0.654354, current_train_items 281088.
I0302 19:02:30.184968 22535416901760 run.py:483] Algo bellman_ford step 8784 current loss 0.784133, current_train_items 281120.
I0302 19:02:30.204587 22535416901760 run.py:483] Algo bellman_ford step 8785 current loss 0.290100, current_train_items 281152.
I0302 19:02:30.220752 22535416901760 run.py:483] Algo bellman_ford step 8786 current loss 0.434447, current_train_items 281184.
I0302 19:02:30.244588 22535416901760 run.py:483] Algo bellman_ford step 8787 current loss 0.570295, current_train_items 281216.
I0302 19:02:30.276018 22535416901760 run.py:483] Algo bellman_ford step 8788 current loss 0.620103, current_train_items 281248.
I0302 19:02:30.309535 22535416901760 run.py:483] Algo bellman_ford step 8789 current loss 0.705928, current_train_items 281280.
I0302 19:02:30.329480 22535416901760 run.py:483] Algo bellman_ford step 8790 current loss 0.300231, current_train_items 281312.
I0302 19:02:30.346040 22535416901760 run.py:483] Algo bellman_ford step 8791 current loss 0.603187, current_train_items 281344.
I0302 19:02:30.369212 22535416901760 run.py:483] Algo bellman_ford step 8792 current loss 0.583695, current_train_items 281376.
I0302 19:02:30.400898 22535416901760 run.py:483] Algo bellman_ford step 8793 current loss 0.657890, current_train_items 281408.
I0302 19:02:30.435179 22535416901760 run.py:483] Algo bellman_ford step 8794 current loss 0.754550, current_train_items 281440.
I0302 19:02:30.454706 22535416901760 run.py:483] Algo bellman_ford step 8795 current loss 0.267083, current_train_items 281472.
I0302 19:02:30.470821 22535416901760 run.py:483] Algo bellman_ford step 8796 current loss 0.427155, current_train_items 281504.
I0302 19:02:30.494098 22535416901760 run.py:483] Algo bellman_ford step 8797 current loss 0.520545, current_train_items 281536.
I0302 19:02:30.525910 22535416901760 run.py:483] Algo bellman_ford step 8798 current loss 0.619276, current_train_items 281568.
I0302 19:02:30.560921 22535416901760 run.py:483] Algo bellman_ford step 8799 current loss 0.731526, current_train_items 281600.
I0302 19:02:30.580544 22535416901760 run.py:483] Algo bellman_ford step 8800 current loss 0.255768, current_train_items 281632.
I0302 19:02:30.588350 22535416901760 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.8828125, 'score': 0.8828125, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:02:30.588456 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.883, val scores are: bellman_ford: 0.883
I0302 19:02:30.605014 22535416901760 run.py:483] Algo bellman_ford step 8801 current loss 0.398329, current_train_items 281664.
I0302 19:02:30.629267 22535416901760 run.py:483] Algo bellman_ford step 8802 current loss 0.576592, current_train_items 281696.
I0302 19:02:30.662507 22535416901760 run.py:483] Algo bellman_ford step 8803 current loss 0.664258, current_train_items 281728.
I0302 19:02:30.696170 22535416901760 run.py:483] Algo bellman_ford step 8804 current loss 0.663143, current_train_items 281760.
I0302 19:02:30.716050 22535416901760 run.py:483] Algo bellman_ford step 8805 current loss 0.284068, current_train_items 281792.
I0302 19:02:30.731858 22535416901760 run.py:483] Algo bellman_ford step 8806 current loss 0.415091, current_train_items 281824.
I0302 19:02:30.755738 22535416901760 run.py:483] Algo bellman_ford step 8807 current loss 0.553004, current_train_items 281856.
I0302 19:02:30.787269 22535416901760 run.py:483] Algo bellman_ford step 8808 current loss 0.631701, current_train_items 281888.
I0302 19:02:30.822961 22535416901760 run.py:483] Algo bellman_ford step 8809 current loss 0.697479, current_train_items 281920.
I0302 19:02:30.842425 22535416901760 run.py:483] Algo bellman_ford step 8810 current loss 0.346438, current_train_items 281952.
I0302 19:02:30.858723 22535416901760 run.py:483] Algo bellman_ford step 8811 current loss 0.441623, current_train_items 281984.
I0302 19:02:30.882312 22535416901760 run.py:483] Algo bellman_ford step 8812 current loss 0.550387, current_train_items 282016.
I0302 19:02:30.913979 22535416901760 run.py:483] Algo bellman_ford step 8813 current loss 0.563577, current_train_items 282048.
I0302 19:02:30.947680 22535416901760 run.py:483] Algo bellman_ford step 8814 current loss 0.773580, current_train_items 282080.
I0302 19:02:30.967285 22535416901760 run.py:483] Algo bellman_ford step 8815 current loss 0.307865, current_train_items 282112.
I0302 19:02:30.983383 22535416901760 run.py:483] Algo bellman_ford step 8816 current loss 0.426030, current_train_items 282144.
I0302 19:02:31.006826 22535416901760 run.py:483] Algo bellman_ford step 8817 current loss 0.458576, current_train_items 282176.
I0302 19:02:31.037697 22535416901760 run.py:483] Algo bellman_ford step 8818 current loss 0.591006, current_train_items 282208.
I0302 19:02:31.074936 22535416901760 run.py:483] Algo bellman_ford step 8819 current loss 0.702675, current_train_items 282240.
I0302 19:02:31.094715 22535416901760 run.py:483] Algo bellman_ford step 8820 current loss 0.382004, current_train_items 282272.
I0302 19:02:31.110701 22535416901760 run.py:483] Algo bellman_ford step 8821 current loss 0.445430, current_train_items 282304.
I0302 19:02:31.134265 22535416901760 run.py:483] Algo bellman_ford step 8822 current loss 0.545123, current_train_items 282336.
I0302 19:02:31.166594 22535416901760 run.py:483] Algo bellman_ford step 8823 current loss 0.682914, current_train_items 282368.
I0302 19:02:31.199118 22535416901760 run.py:483] Algo bellman_ford step 8824 current loss 0.702470, current_train_items 282400.
I0302 19:02:31.218801 22535416901760 run.py:483] Algo bellman_ford step 8825 current loss 0.255690, current_train_items 282432.
I0302 19:02:31.235125 22535416901760 run.py:483] Algo bellman_ford step 8826 current loss 0.501656, current_train_items 282464.
I0302 19:02:31.258944 22535416901760 run.py:483] Algo bellman_ford step 8827 current loss 0.729219, current_train_items 282496.
I0302 19:02:31.290704 22535416901760 run.py:483] Algo bellman_ford step 8828 current loss 0.817519, current_train_items 282528.
I0302 19:02:31.323927 22535416901760 run.py:483] Algo bellman_ford step 8829 current loss 0.871217, current_train_items 282560.
I0302 19:02:31.343191 22535416901760 run.py:483] Algo bellman_ford step 8830 current loss 0.333602, current_train_items 282592.
I0302 19:02:31.359104 22535416901760 run.py:483] Algo bellman_ford step 8831 current loss 0.449828, current_train_items 282624.
I0302 19:02:31.383734 22535416901760 run.py:483] Algo bellman_ford step 8832 current loss 0.601515, current_train_items 282656.
I0302 19:02:31.413804 22535416901760 run.py:483] Algo bellman_ford step 8833 current loss 0.556270, current_train_items 282688.
I0302 19:02:31.446219 22535416901760 run.py:483] Algo bellman_ford step 8834 current loss 0.774626, current_train_items 282720.
I0302 19:02:31.465958 22535416901760 run.py:483] Algo bellman_ford step 8835 current loss 0.332910, current_train_items 282752.
I0302 19:02:31.481868 22535416901760 run.py:483] Algo bellman_ford step 8836 current loss 0.453304, current_train_items 282784.
I0302 19:02:31.506456 22535416901760 run.py:483] Algo bellman_ford step 8837 current loss 0.599853, current_train_items 282816.
I0302 19:02:31.537857 22535416901760 run.py:483] Algo bellman_ford step 8838 current loss 0.736784, current_train_items 282848.
I0302 19:02:31.571301 22535416901760 run.py:483] Algo bellman_ford step 8839 current loss 0.798223, current_train_items 282880.
I0302 19:02:31.590970 22535416901760 run.py:483] Algo bellman_ford step 8840 current loss 0.375603, current_train_items 282912.
I0302 19:02:31.607516 22535416901760 run.py:483] Algo bellman_ford step 8841 current loss 0.442949, current_train_items 282944.
I0302 19:02:31.631384 22535416901760 run.py:483] Algo bellman_ford step 8842 current loss 0.558809, current_train_items 282976.
I0302 19:02:31.662930 22535416901760 run.py:483] Algo bellman_ford step 8843 current loss 0.661356, current_train_items 283008.
I0302 19:02:31.698315 22535416901760 run.py:483] Algo bellman_ford step 8844 current loss 0.815275, current_train_items 283040.
I0302 19:02:31.718217 22535416901760 run.py:483] Algo bellman_ford step 8845 current loss 0.309544, current_train_items 283072.
I0302 19:02:31.734200 22535416901760 run.py:483] Algo bellman_ford step 8846 current loss 0.452526, current_train_items 283104.
I0302 19:02:31.758041 22535416901760 run.py:483] Algo bellman_ford step 8847 current loss 0.548589, current_train_items 283136.
I0302 19:02:31.788063 22535416901760 run.py:483] Algo bellman_ford step 8848 current loss 0.570878, current_train_items 283168.
I0302 19:02:31.821442 22535416901760 run.py:483] Algo bellman_ford step 8849 current loss 0.725947, current_train_items 283200.
I0302 19:02:31.840902 22535416901760 run.py:483] Algo bellman_ford step 8850 current loss 0.314433, current_train_items 283232.
I0302 19:02:31.849124 22535416901760 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:02:31.849239 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.957, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 19:02:31.866076 22535416901760 run.py:483] Algo bellman_ford step 8851 current loss 0.481051, current_train_items 283264.
I0302 19:02:31.890684 22535416901760 run.py:483] Algo bellman_ford step 8852 current loss 0.579095, current_train_items 283296.
I0302 19:02:31.922636 22535416901760 run.py:483] Algo bellman_ford step 8853 current loss 0.599733, current_train_items 283328.
I0302 19:02:31.956761 22535416901760 run.py:483] Algo bellman_ford step 8854 current loss 0.723286, current_train_items 283360.
I0302 19:02:31.976839 22535416901760 run.py:483] Algo bellman_ford step 8855 current loss 0.400479, current_train_items 283392.
I0302 19:02:31.992857 22535416901760 run.py:483] Algo bellman_ford step 8856 current loss 0.468832, current_train_items 283424.
I0302 19:02:32.016124 22535416901760 run.py:483] Algo bellman_ford step 8857 current loss 0.566895, current_train_items 283456.
I0302 19:02:32.048281 22535416901760 run.py:483] Algo bellman_ford step 8858 current loss 0.696906, current_train_items 283488.
I0302 19:02:32.080224 22535416901760 run.py:483] Algo bellman_ford step 8859 current loss 0.709989, current_train_items 283520.
I0302 19:02:32.099821 22535416901760 run.py:483] Algo bellman_ford step 8860 current loss 0.297235, current_train_items 283552.
I0302 19:02:32.116227 22535416901760 run.py:483] Algo bellman_ford step 8861 current loss 0.489819, current_train_items 283584.
I0302 19:02:32.140026 22535416901760 run.py:483] Algo bellman_ford step 8862 current loss 0.565816, current_train_items 283616.
I0302 19:02:32.170666 22535416901760 run.py:483] Algo bellman_ford step 8863 current loss 0.614689, current_train_items 283648.
I0302 19:02:32.203085 22535416901760 run.py:483] Algo bellman_ford step 8864 current loss 0.708879, current_train_items 283680.
I0302 19:02:32.222747 22535416901760 run.py:483] Algo bellman_ford step 8865 current loss 0.294320, current_train_items 283712.
I0302 19:02:32.238740 22535416901760 run.py:483] Algo bellman_ford step 8866 current loss 0.464360, current_train_items 283744.
I0302 19:02:32.261287 22535416901760 run.py:483] Algo bellman_ford step 8867 current loss 0.570130, current_train_items 283776.
I0302 19:02:32.293035 22535416901760 run.py:483] Algo bellman_ford step 8868 current loss 0.611529, current_train_items 283808.
I0302 19:02:32.325850 22535416901760 run.py:483] Algo bellman_ford step 8869 current loss 0.676551, current_train_items 283840.
I0302 19:02:32.345819 22535416901760 run.py:483] Algo bellman_ford step 8870 current loss 0.334676, current_train_items 283872.
I0302 19:02:32.361889 22535416901760 run.py:483] Algo bellman_ford step 8871 current loss 0.407943, current_train_items 283904.
I0302 19:02:32.385898 22535416901760 run.py:483] Algo bellman_ford step 8872 current loss 0.523655, current_train_items 283936.
I0302 19:02:32.416952 22535416901760 run.py:483] Algo bellman_ford step 8873 current loss 0.544300, current_train_items 283968.
I0302 19:02:32.451569 22535416901760 run.py:483] Algo bellman_ford step 8874 current loss 0.813974, current_train_items 284000.
I0302 19:02:32.471385 22535416901760 run.py:483] Algo bellman_ford step 8875 current loss 0.312477, current_train_items 284032.
I0302 19:02:32.487642 22535416901760 run.py:483] Algo bellman_ford step 8876 current loss 0.436585, current_train_items 284064.
I0302 19:02:32.510894 22535416901760 run.py:483] Algo bellman_ford step 8877 current loss 0.640104, current_train_items 284096.
I0302 19:02:32.541862 22535416901760 run.py:483] Algo bellman_ford step 8878 current loss 0.591471, current_train_items 284128.
I0302 19:02:32.575886 22535416901760 run.py:483] Algo bellman_ford step 8879 current loss 0.744834, current_train_items 284160.
I0302 19:02:32.595375 22535416901760 run.py:483] Algo bellman_ford step 8880 current loss 0.308998, current_train_items 284192.
I0302 19:02:32.611475 22535416901760 run.py:483] Algo bellman_ford step 8881 current loss 0.480959, current_train_items 284224.
I0302 19:02:32.634427 22535416901760 run.py:483] Algo bellman_ford step 8882 current loss 0.578336, current_train_items 284256.
I0302 19:02:32.667024 22535416901760 run.py:483] Algo bellman_ford step 8883 current loss 0.691819, current_train_items 284288.
I0302 19:02:32.700792 22535416901760 run.py:483] Algo bellman_ford step 8884 current loss 0.977740, current_train_items 284320.
I0302 19:02:32.720671 22535416901760 run.py:483] Algo bellman_ford step 8885 current loss 0.324385, current_train_items 284352.
I0302 19:02:32.737271 22535416901760 run.py:483] Algo bellman_ford step 8886 current loss 0.450694, current_train_items 284384.
I0302 19:02:32.761131 22535416901760 run.py:483] Algo bellman_ford step 8887 current loss 0.579847, current_train_items 284416.
I0302 19:02:32.792037 22535416901760 run.py:483] Algo bellman_ford step 8888 current loss 0.730320, current_train_items 284448.
I0302 19:02:32.826469 22535416901760 run.py:483] Algo bellman_ford step 8889 current loss 0.779505, current_train_items 284480.
I0302 19:02:32.846247 22535416901760 run.py:483] Algo bellman_ford step 8890 current loss 0.284984, current_train_items 284512.
I0302 19:02:32.862029 22535416901760 run.py:483] Algo bellman_ford step 8891 current loss 0.453507, current_train_items 284544.
I0302 19:02:32.885625 22535416901760 run.py:483] Algo bellman_ford step 8892 current loss 0.535478, current_train_items 284576.
I0302 19:02:32.918707 22535416901760 run.py:483] Algo bellman_ford step 8893 current loss 0.660447, current_train_items 284608.
I0302 19:02:32.952256 22535416901760 run.py:483] Algo bellman_ford step 8894 current loss 0.752454, current_train_items 284640.
I0302 19:02:32.971753 22535416901760 run.py:483] Algo bellman_ford step 8895 current loss 0.446116, current_train_items 284672.
I0302 19:02:32.987705 22535416901760 run.py:483] Algo bellman_ford step 8896 current loss 0.476667, current_train_items 284704.
I0302 19:02:33.010579 22535416901760 run.py:483] Algo bellman_ford step 8897 current loss 0.543136, current_train_items 284736.
I0302 19:02:33.042946 22535416901760 run.py:483] Algo bellman_ford step 8898 current loss 0.630558, current_train_items 284768.
I0302 19:02:33.077831 22535416901760 run.py:483] Algo bellman_ford step 8899 current loss 0.631156, current_train_items 284800.
I0302 19:02:33.097688 22535416901760 run.py:483] Algo bellman_ford step 8900 current loss 0.290774, current_train_items 284832.
I0302 19:02:33.105424 22535416901760 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9580078125, 'score': 0.9580078125, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:02:33.105534 22535416901760 run.py:519] Checkpointing best model, best avg val score was 0.957, current avg val score is 0.958, val scores are: bellman_ford: 0.958
I0302 19:02:33.135703 22535416901760 run.py:483] Algo bellman_ford step 8901 current loss 0.399420, current_train_items 284864.
I0302 19:02:33.159949 22535416901760 run.py:483] Algo bellman_ford step 8902 current loss 0.543236, current_train_items 284896.
I0302 19:02:33.190895 22535416901760 run.py:483] Algo bellman_ford step 8903 current loss 0.597239, current_train_items 284928.
I0302 19:02:33.226480 22535416901760 run.py:483] Algo bellman_ford step 8904 current loss 0.750639, current_train_items 284960.
I0302 19:02:33.246560 22535416901760 run.py:483] Algo bellman_ford step 8905 current loss 0.335598, current_train_items 284992.
I0302 19:02:33.262569 22535416901760 run.py:483] Algo bellman_ford step 8906 current loss 0.410956, current_train_items 285024.
I0302 19:02:33.286192 22535416901760 run.py:483] Algo bellman_ford step 8907 current loss 0.520622, current_train_items 285056.
I0302 19:02:33.317191 22535416901760 run.py:483] Algo bellman_ford step 8908 current loss 0.620136, current_train_items 285088.
I0302 19:02:33.349020 22535416901760 run.py:483] Algo bellman_ford step 8909 current loss 0.655705, current_train_items 285120.
I0302 19:02:33.368735 22535416901760 run.py:483] Algo bellman_ford step 8910 current loss 0.336561, current_train_items 285152.
I0302 19:02:33.384654 22535416901760 run.py:483] Algo bellman_ford step 8911 current loss 0.420630, current_train_items 285184.
I0302 19:02:33.408541 22535416901760 run.py:483] Algo bellman_ford step 8912 current loss 0.523332, current_train_items 285216.
I0302 19:02:33.438845 22535416901760 run.py:483] Algo bellman_ford step 8913 current loss 0.548940, current_train_items 285248.
I0302 19:02:33.473678 22535416901760 run.py:483] Algo bellman_ford step 8914 current loss 0.795230, current_train_items 285280.
I0302 19:02:33.493197 22535416901760 run.py:483] Algo bellman_ford step 8915 current loss 0.300788, current_train_items 285312.
I0302 19:02:33.508918 22535416901760 run.py:483] Algo bellman_ford step 8916 current loss 0.439527, current_train_items 285344.
I0302 19:02:33.532738 22535416901760 run.py:483] Algo bellman_ford step 8917 current loss 0.604184, current_train_items 285376.
I0302 19:02:33.564001 22535416901760 run.py:483] Algo bellman_ford step 8918 current loss 0.582227, current_train_items 285408.
I0302 19:02:33.596884 22535416901760 run.py:483] Algo bellman_ford step 8919 current loss 0.655543, current_train_items 285440.
I0302 19:02:33.616337 22535416901760 run.py:483] Algo bellman_ford step 8920 current loss 0.315990, current_train_items 285472.
I0302 19:02:33.632576 22535416901760 run.py:483] Algo bellman_ford step 8921 current loss 0.418048, current_train_items 285504.
I0302 19:02:33.656925 22535416901760 run.py:483] Algo bellman_ford step 8922 current loss 0.529237, current_train_items 285536.
I0302 19:02:33.688180 22535416901760 run.py:483] Algo bellman_ford step 8923 current loss 0.637235, current_train_items 285568.
I0302 19:02:33.722212 22535416901760 run.py:483] Algo bellman_ford step 8924 current loss 0.685759, current_train_items 285600.
I0302 19:02:33.741611 22535416901760 run.py:483] Algo bellman_ford step 8925 current loss 0.251422, current_train_items 285632.
I0302 19:02:33.757297 22535416901760 run.py:483] Algo bellman_ford step 8926 current loss 0.533062, current_train_items 285664.
I0302 19:02:33.781054 22535416901760 run.py:483] Algo bellman_ford step 8927 current loss 0.586183, current_train_items 285696.
I0302 19:02:33.811267 22535416901760 run.py:483] Algo bellman_ford step 8928 current loss 0.555030, current_train_items 285728.
I0302 19:02:33.844758 22535416901760 run.py:483] Algo bellman_ford step 8929 current loss 0.740937, current_train_items 285760.
I0302 19:02:33.864135 22535416901760 run.py:483] Algo bellman_ford step 8930 current loss 0.305490, current_train_items 285792.
I0302 19:02:33.880945 22535416901760 run.py:483] Algo bellman_ford step 8931 current loss 0.473773, current_train_items 285824.
I0302 19:02:33.904492 22535416901760 run.py:483] Algo bellman_ford step 8932 current loss 0.573774, current_train_items 285856.
I0302 19:02:33.937592 22535416901760 run.py:483] Algo bellman_ford step 8933 current loss 0.837839, current_train_items 285888.
I0302 19:02:33.971547 22535416901760 run.py:483] Algo bellman_ford step 8934 current loss 0.901682, current_train_items 285920.
I0302 19:02:33.991084 22535416901760 run.py:483] Algo bellman_ford step 8935 current loss 0.292063, current_train_items 285952.
I0302 19:02:34.006958 22535416901760 run.py:483] Algo bellman_ford step 8936 current loss 0.428902, current_train_items 285984.
I0302 19:02:34.030415 22535416901760 run.py:483] Algo bellman_ford step 8937 current loss 0.576260, current_train_items 286016.
I0302 19:02:34.061149 22535416901760 run.py:483] Algo bellman_ford step 8938 current loss 0.647779, current_train_items 286048.
I0302 19:02:34.093383 22535416901760 run.py:483] Algo bellman_ford step 8939 current loss 0.709076, current_train_items 286080.
I0302 19:02:34.112840 22535416901760 run.py:483] Algo bellman_ford step 8940 current loss 0.322749, current_train_items 286112.
I0302 19:02:34.128765 22535416901760 run.py:483] Algo bellman_ford step 8941 current loss 0.424236, current_train_items 286144.
I0302 19:02:34.152946 22535416901760 run.py:483] Algo bellman_ford step 8942 current loss 0.592443, current_train_items 286176.
I0302 19:02:34.185214 22535416901760 run.py:483] Algo bellman_ford step 8943 current loss 0.679254, current_train_items 286208.
I0302 19:02:34.218941 22535416901760 run.py:483] Algo bellman_ford step 8944 current loss 0.677521, current_train_items 286240.
I0302 19:02:34.238314 22535416901760 run.py:483] Algo bellman_ford step 8945 current loss 0.350739, current_train_items 286272.
I0302 19:02:34.254294 22535416901760 run.py:483] Algo bellman_ford step 8946 current loss 0.419535, current_train_items 286304.
I0302 19:02:34.277110 22535416901760 run.py:483] Algo bellman_ford step 8947 current loss 0.544554, current_train_items 286336.
I0302 19:02:34.308913 22535416901760 run.py:483] Algo bellman_ford step 8948 current loss 0.602959, current_train_items 286368.
I0302 19:02:34.341875 22535416901760 run.py:483] Algo bellman_ford step 8949 current loss 0.664666, current_train_items 286400.
I0302 19:02:34.361228 22535416901760 run.py:483] Algo bellman_ford step 8950 current loss 0.319872, current_train_items 286432.
I0302 19:02:34.369445 22535416901760 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.9365234375, 'score': 0.9365234375, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:02:34.369557 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.937, val scores are: bellman_ford: 0.937
I0302 19:02:34.386691 22535416901760 run.py:483] Algo bellman_ford step 8951 current loss 0.496278, current_train_items 286464.
I0302 19:02:34.410284 22535416901760 run.py:483] Algo bellman_ford step 8952 current loss 0.518508, current_train_items 286496.
I0302 19:02:34.444590 22535416901760 run.py:483] Algo bellman_ford step 8953 current loss 0.676511, current_train_items 286528.
I0302 19:02:34.479489 22535416901760 run.py:483] Algo bellman_ford step 8954 current loss 0.695679, current_train_items 286560.
I0302 19:02:34.499339 22535416901760 run.py:483] Algo bellman_ford step 8955 current loss 0.350465, current_train_items 286592.
I0302 19:02:34.515652 22535416901760 run.py:483] Algo bellman_ford step 8956 current loss 0.545274, current_train_items 286624.
I0302 19:02:34.539862 22535416901760 run.py:483] Algo bellman_ford step 8957 current loss 0.663973, current_train_items 286656.
I0302 19:02:34.570758 22535416901760 run.py:483] Algo bellman_ford step 8958 current loss 0.715671, current_train_items 286688.
I0302 19:02:34.605208 22535416901760 run.py:483] Algo bellman_ford step 8959 current loss 0.783385, current_train_items 286720.
I0302 19:02:34.624995 22535416901760 run.py:483] Algo bellman_ford step 8960 current loss 0.297547, current_train_items 286752.
I0302 19:02:34.640732 22535416901760 run.py:483] Algo bellman_ford step 8961 current loss 0.390526, current_train_items 286784.
I0302 19:02:34.665102 22535416901760 run.py:483] Algo bellman_ford step 8962 current loss 0.541847, current_train_items 286816.
I0302 19:02:34.696695 22535416901760 run.py:483] Algo bellman_ford step 8963 current loss 0.643621, current_train_items 286848.
I0302 19:02:34.728710 22535416901760 run.py:483] Algo bellman_ford step 8964 current loss 0.725000, current_train_items 286880.
I0302 19:02:34.748140 22535416901760 run.py:483] Algo bellman_ford step 8965 current loss 0.298004, current_train_items 286912.
I0302 19:02:34.764522 22535416901760 run.py:483] Algo bellman_ford step 8966 current loss 0.464104, current_train_items 286944.
I0302 19:02:34.788095 22535416901760 run.py:483] Algo bellman_ford step 8967 current loss 0.601756, current_train_items 286976.
I0302 19:02:34.819634 22535416901760 run.py:483] Algo bellman_ford step 8968 current loss 0.596957, current_train_items 287008.
I0302 19:02:34.852620 22535416901760 run.py:483] Algo bellman_ford step 8969 current loss 0.663792, current_train_items 287040.
I0302 19:02:34.872017 22535416901760 run.py:483] Algo bellman_ford step 8970 current loss 0.308116, current_train_items 287072.
I0302 19:02:34.888023 22535416901760 run.py:483] Algo bellman_ford step 8971 current loss 0.389550, current_train_items 287104.
I0302 19:02:34.911322 22535416901760 run.py:483] Algo bellman_ford step 8972 current loss 0.561190, current_train_items 287136.
I0302 19:02:34.942039 22535416901760 run.py:483] Algo bellman_ford step 8973 current loss 0.579352, current_train_items 287168.
I0302 19:02:34.974657 22535416901760 run.py:483] Algo bellman_ford step 8974 current loss 0.661274, current_train_items 287200.
I0302 19:02:34.994338 22535416901760 run.py:483] Algo bellman_ford step 8975 current loss 0.318425, current_train_items 287232.
I0302 19:02:35.010143 22535416901760 run.py:483] Algo bellman_ford step 8976 current loss 0.493656, current_train_items 287264.
I0302 19:02:35.032437 22535416901760 run.py:483] Algo bellman_ford step 8977 current loss 0.532500, current_train_items 287296.
I0302 19:02:35.064082 22535416901760 run.py:483] Algo bellman_ford step 8978 current loss 0.611933, current_train_items 287328.
I0302 19:02:35.096722 22535416901760 run.py:483] Algo bellman_ford step 8979 current loss 0.705948, current_train_items 287360.
I0302 19:02:35.116047 22535416901760 run.py:483] Algo bellman_ford step 8980 current loss 0.249216, current_train_items 287392.
I0302 19:02:35.132304 22535416901760 run.py:483] Algo bellman_ford step 8981 current loss 0.424741, current_train_items 287424.
I0302 19:02:35.156102 22535416901760 run.py:483] Algo bellman_ford step 8982 current loss 0.529897, current_train_items 287456.
I0302 19:02:35.187596 22535416901760 run.py:483] Algo bellman_ford step 8983 current loss 0.633124, current_train_items 287488.
I0302 19:02:35.220957 22535416901760 run.py:483] Algo bellman_ford step 8984 current loss 0.673309, current_train_items 287520.
I0302 19:02:35.240781 22535416901760 run.py:483] Algo bellman_ford step 8985 current loss 0.260491, current_train_items 287552.
I0302 19:02:35.257061 22535416901760 run.py:483] Algo bellman_ford step 8986 current loss 0.423979, current_train_items 287584.
I0302 19:02:35.279782 22535416901760 run.py:483] Algo bellman_ford step 8987 current loss 0.481795, current_train_items 287616.
I0302 19:02:35.309625 22535416901760 run.py:483] Algo bellman_ford step 8988 current loss 0.662281, current_train_items 287648.
I0302 19:02:35.344763 22535416901760 run.py:483] Algo bellman_ford step 8989 current loss 0.773703, current_train_items 287680.
I0302 19:02:35.364467 22535416901760 run.py:483] Algo bellman_ford step 8990 current loss 0.296681, current_train_items 287712.
I0302 19:02:35.380406 22535416901760 run.py:483] Algo bellman_ford step 8991 current loss 0.405347, current_train_items 287744.
I0302 19:02:35.403314 22535416901760 run.py:483] Algo bellman_ford step 8992 current loss 0.533097, current_train_items 287776.
I0302 19:02:35.434565 22535416901760 run.py:483] Algo bellman_ford step 8993 current loss 0.622091, current_train_items 287808.
I0302 19:02:35.467721 22535416901760 run.py:483] Algo bellman_ford step 8994 current loss 0.747788, current_train_items 287840.
I0302 19:02:35.486853 22535416901760 run.py:483] Algo bellman_ford step 8995 current loss 0.286068, current_train_items 287872.
I0302 19:02:35.503057 22535416901760 run.py:483] Algo bellman_ford step 8996 current loss 0.416791, current_train_items 287904.
I0302 19:02:35.527346 22535416901760 run.py:483] Algo bellman_ford step 8997 current loss 0.546659, current_train_items 287936.
I0302 19:02:35.560304 22535416901760 run.py:483] Algo bellman_ford step 8998 current loss 0.623458, current_train_items 287968.
I0302 19:02:35.592551 22535416901760 run.py:483] Algo bellman_ford step 8999 current loss 0.685401, current_train_items 288000.
I0302 19:02:35.612535 22535416901760 run.py:483] Algo bellman_ford step 9000 current loss 0.274026, current_train_items 288032.
I0302 19:02:35.620457 22535416901760 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:02:35.620564 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:02:35.637667 22535416901760 run.py:483] Algo bellman_ford step 9001 current loss 0.415268, current_train_items 288064.
I0302 19:02:35.661331 22535416901760 run.py:483] Algo bellman_ford step 9002 current loss 0.575380, current_train_items 288096.
I0302 19:02:35.692772 22535416901760 run.py:483] Algo bellman_ford step 9003 current loss 0.647166, current_train_items 288128.
I0302 19:02:35.727342 22535416901760 run.py:483] Algo bellman_ford step 9004 current loss 0.684911, current_train_items 288160.
I0302 19:02:35.747730 22535416901760 run.py:483] Algo bellman_ford step 9005 current loss 0.307997, current_train_items 288192.
I0302 19:02:35.763780 22535416901760 run.py:483] Algo bellman_ford step 9006 current loss 0.413992, current_train_items 288224.
I0302 19:02:35.786624 22535416901760 run.py:483] Algo bellman_ford step 9007 current loss 0.568231, current_train_items 288256.
I0302 19:02:35.818737 22535416901760 run.py:483] Algo bellman_ford step 9008 current loss 0.696504, current_train_items 288288.
I0302 19:02:35.852406 22535416901760 run.py:483] Algo bellman_ford step 9009 current loss 0.678482, current_train_items 288320.
I0302 19:02:35.872058 22535416901760 run.py:483] Algo bellman_ford step 9010 current loss 0.317765, current_train_items 288352.
I0302 19:02:35.888209 22535416901760 run.py:483] Algo bellman_ford step 9011 current loss 0.425028, current_train_items 288384.
I0302 19:02:35.911431 22535416901760 run.py:483] Algo bellman_ford step 9012 current loss 0.552519, current_train_items 288416.
I0302 19:02:35.942359 22535416901760 run.py:483] Algo bellman_ford step 9013 current loss 0.605778, current_train_items 288448.
I0302 19:02:35.978031 22535416901760 run.py:483] Algo bellman_ford step 9014 current loss 0.780580, current_train_items 288480.
I0302 19:02:35.997400 22535416901760 run.py:483] Algo bellman_ford step 9015 current loss 0.276455, current_train_items 288512.
I0302 19:02:36.013545 22535416901760 run.py:483] Algo bellman_ford step 9016 current loss 0.505793, current_train_items 288544.
I0302 19:02:36.037647 22535416901760 run.py:483] Algo bellman_ford step 9017 current loss 0.608039, current_train_items 288576.
I0302 19:02:36.069183 22535416901760 run.py:483] Algo bellman_ford step 9018 current loss 0.670833, current_train_items 288608.
I0302 19:02:36.104002 22535416901760 run.py:483] Algo bellman_ford step 9019 current loss 0.733902, current_train_items 288640.
I0302 19:02:36.123106 22535416901760 run.py:483] Algo bellman_ford step 9020 current loss 0.371975, current_train_items 288672.
I0302 19:02:36.139165 22535416901760 run.py:483] Algo bellman_ford step 9021 current loss 0.468307, current_train_items 288704.
I0302 19:02:36.164204 22535416901760 run.py:483] Algo bellman_ford step 9022 current loss 0.571743, current_train_items 288736.
I0302 19:02:36.196684 22535416901760 run.py:483] Algo bellman_ford step 9023 current loss 0.666584, current_train_items 288768.
I0302 19:02:36.230385 22535416901760 run.py:483] Algo bellman_ford step 9024 current loss 0.915564, current_train_items 288800.
I0302 19:02:36.249834 22535416901760 run.py:483] Algo bellman_ford step 9025 current loss 0.315589, current_train_items 288832.
I0302 19:02:36.265646 22535416901760 run.py:483] Algo bellman_ford step 9026 current loss 0.340238, current_train_items 288864.
I0302 19:02:36.289921 22535416901760 run.py:483] Algo bellman_ford step 9027 current loss 0.515226, current_train_items 288896.
I0302 19:02:36.321709 22535416901760 run.py:483] Algo bellman_ford step 9028 current loss 0.567927, current_train_items 288928.
I0302 19:02:36.355550 22535416901760 run.py:483] Algo bellman_ford step 9029 current loss 0.689843, current_train_items 288960.
I0302 19:02:36.375315 22535416901760 run.py:483] Algo bellman_ford step 9030 current loss 0.300782, current_train_items 288992.
I0302 19:02:36.391367 22535416901760 run.py:483] Algo bellman_ford step 9031 current loss 0.421115, current_train_items 289024.
I0302 19:02:36.415847 22535416901760 run.py:483] Algo bellman_ford step 9032 current loss 0.624350, current_train_items 289056.
I0302 19:02:36.447612 22535416901760 run.py:483] Algo bellman_ford step 9033 current loss 0.629373, current_train_items 289088.
I0302 19:02:36.483356 22535416901760 run.py:483] Algo bellman_ford step 9034 current loss 0.834086, current_train_items 289120.
I0302 19:02:36.502800 22535416901760 run.py:483] Algo bellman_ford step 9035 current loss 0.321575, current_train_items 289152.
I0302 19:02:36.518624 22535416901760 run.py:483] Algo bellman_ford step 9036 current loss 0.532232, current_train_items 289184.
I0302 19:02:36.543362 22535416901760 run.py:483] Algo bellman_ford step 9037 current loss 0.590849, current_train_items 289216.
I0302 19:02:36.574758 22535416901760 run.py:483] Algo bellman_ford step 9038 current loss 0.648087, current_train_items 289248.
I0302 19:02:36.608405 22535416901760 run.py:483] Algo bellman_ford step 9039 current loss 0.738161, current_train_items 289280.
I0302 19:02:36.627952 22535416901760 run.py:483] Algo bellman_ford step 9040 current loss 0.301898, current_train_items 289312.
I0302 19:02:36.644428 22535416901760 run.py:483] Algo bellman_ford step 9041 current loss 0.434664, current_train_items 289344.
I0302 19:02:36.667936 22535416901760 run.py:483] Algo bellman_ford step 9042 current loss 0.536368, current_train_items 289376.
I0302 19:02:36.699240 22535416901760 run.py:483] Algo bellman_ford step 9043 current loss 0.687657, current_train_items 289408.
I0302 19:02:36.733180 22535416901760 run.py:483] Algo bellman_ford step 9044 current loss 0.770392, current_train_items 289440.
I0302 19:02:36.752849 22535416901760 run.py:483] Algo bellman_ford step 9045 current loss 0.319620, current_train_items 289472.
I0302 19:02:36.769233 22535416901760 run.py:483] Algo bellman_ford step 9046 current loss 0.441533, current_train_items 289504.
I0302 19:02:36.791865 22535416901760 run.py:483] Algo bellman_ford step 9047 current loss 0.615261, current_train_items 289536.
I0302 19:02:36.823737 22535416901760 run.py:483] Algo bellman_ford step 9048 current loss 0.625428, current_train_items 289568.
I0302 19:02:36.857678 22535416901760 run.py:483] Algo bellman_ford step 9049 current loss 0.671179, current_train_items 289600.
I0302 19:02:36.877235 22535416901760 run.py:483] Algo bellman_ford step 9050 current loss 0.314333, current_train_items 289632.
I0302 19:02:36.885518 22535416901760 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:02:36.885625 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:02:36.902287 22535416901760 run.py:483] Algo bellman_ford step 9051 current loss 0.383725, current_train_items 289664.
I0302 19:02:36.927218 22535416901760 run.py:483] Algo bellman_ford step 9052 current loss 0.600339, current_train_items 289696.
I0302 19:02:36.959640 22535416901760 run.py:483] Algo bellman_ford step 9053 current loss 0.648015, current_train_items 289728.
I0302 19:02:36.992680 22535416901760 run.py:483] Algo bellman_ford step 9054 current loss 0.674599, current_train_items 289760.
I0302 19:02:37.012702 22535416901760 run.py:483] Algo bellman_ford step 9055 current loss 0.291144, current_train_items 289792.
I0302 19:02:37.028557 22535416901760 run.py:483] Algo bellman_ford step 9056 current loss 0.380215, current_train_items 289824.
I0302 19:02:37.051921 22535416901760 run.py:483] Algo bellman_ford step 9057 current loss 0.542449, current_train_items 289856.
I0302 19:02:37.081814 22535416901760 run.py:483] Algo bellman_ford step 9058 current loss 0.593427, current_train_items 289888.
I0302 19:02:37.113529 22535416901760 run.py:483] Algo bellman_ford step 9059 current loss 0.627251, current_train_items 289920.
I0302 19:02:37.133121 22535416901760 run.py:483] Algo bellman_ford step 9060 current loss 0.365897, current_train_items 289952.
I0302 19:02:37.149841 22535416901760 run.py:483] Algo bellman_ford step 9061 current loss 0.550231, current_train_items 289984.
I0302 19:02:37.174566 22535416901760 run.py:483] Algo bellman_ford step 9062 current loss 0.664381, current_train_items 290016.
I0302 19:02:37.204724 22535416901760 run.py:483] Algo bellman_ford step 9063 current loss 0.638067, current_train_items 290048.
I0302 19:02:37.238921 22535416901760 run.py:483] Algo bellman_ford step 9064 current loss 0.757316, current_train_items 290080.
I0302 19:02:37.258643 22535416901760 run.py:483] Algo bellman_ford step 9065 current loss 0.329644, current_train_items 290112.
I0302 19:02:37.274789 22535416901760 run.py:483] Algo bellman_ford step 9066 current loss 0.474861, current_train_items 290144.
I0302 19:02:37.296784 22535416901760 run.py:483] Algo bellman_ford step 9067 current loss 0.574687, current_train_items 290176.
I0302 19:02:37.328444 22535416901760 run.py:483] Algo bellman_ford step 9068 current loss 0.779281, current_train_items 290208.
I0302 19:02:37.362693 22535416901760 run.py:483] Algo bellman_ford step 9069 current loss 0.823734, current_train_items 290240.
I0302 19:02:37.382529 22535416901760 run.py:483] Algo bellman_ford step 9070 current loss 0.251731, current_train_items 290272.
I0302 19:02:37.399287 22535416901760 run.py:483] Algo bellman_ford step 9071 current loss 0.485464, current_train_items 290304.
I0302 19:02:37.423534 22535416901760 run.py:483] Algo bellman_ford step 9072 current loss 0.546652, current_train_items 290336.
I0302 19:02:37.455430 22535416901760 run.py:483] Algo bellman_ford step 9073 current loss 0.601011, current_train_items 290368.
I0302 19:02:37.488752 22535416901760 run.py:483] Algo bellman_ford step 9074 current loss 0.682013, current_train_items 290400.
I0302 19:02:37.508271 22535416901760 run.py:483] Algo bellman_ford step 9075 current loss 0.236922, current_train_items 290432.
I0302 19:02:37.524425 22535416901760 run.py:483] Algo bellman_ford step 9076 current loss 0.582417, current_train_items 290464.
I0302 19:02:37.547910 22535416901760 run.py:483] Algo bellman_ford step 9077 current loss 0.496830, current_train_items 290496.
I0302 19:02:37.578888 22535416901760 run.py:483] Algo bellman_ford step 9078 current loss 0.586942, current_train_items 290528.
I0302 19:02:37.612946 22535416901760 run.py:483] Algo bellman_ford step 9079 current loss 0.872107, current_train_items 290560.
I0302 19:02:37.632363 22535416901760 run.py:483] Algo bellman_ford step 9080 current loss 0.329617, current_train_items 290592.
I0302 19:02:37.648291 22535416901760 run.py:483] Algo bellman_ford step 9081 current loss 0.478271, current_train_items 290624.
I0302 19:02:37.671473 22535416901760 run.py:483] Algo bellman_ford step 9082 current loss 0.537466, current_train_items 290656.
I0302 19:02:37.702184 22535416901760 run.py:483] Algo bellman_ford step 9083 current loss 0.602304, current_train_items 290688.
I0302 19:02:37.734504 22535416901760 run.py:483] Algo bellman_ford step 9084 current loss 0.680099, current_train_items 290720.
I0302 19:02:37.754216 22535416901760 run.py:483] Algo bellman_ford step 9085 current loss 0.328266, current_train_items 290752.
I0302 19:02:37.770447 22535416901760 run.py:483] Algo bellman_ford step 9086 current loss 0.417319, current_train_items 290784.
I0302 19:02:37.794162 22535416901760 run.py:483] Algo bellman_ford step 9087 current loss 0.648792, current_train_items 290816.
I0302 19:02:37.824596 22535416901760 run.py:483] Algo bellman_ford step 9088 current loss 0.524274, current_train_items 290848.
I0302 19:02:37.859615 22535416901760 run.py:483] Algo bellman_ford step 9089 current loss 0.800584, current_train_items 290880.
I0302 19:02:37.879517 22535416901760 run.py:483] Algo bellman_ford step 9090 current loss 0.249441, current_train_items 290912.
I0302 19:02:37.895484 22535416901760 run.py:483] Algo bellman_ford step 9091 current loss 0.461718, current_train_items 290944.
I0302 19:02:37.918379 22535416901760 run.py:483] Algo bellman_ford step 9092 current loss 0.582455, current_train_items 290976.
I0302 19:02:37.947775 22535416901760 run.py:483] Algo bellman_ford step 9093 current loss 0.658737, current_train_items 291008.
I0302 19:02:37.983091 22535416901760 run.py:483] Algo bellman_ford step 9094 current loss 0.960167, current_train_items 291040.
I0302 19:02:38.002510 22535416901760 run.py:483] Algo bellman_ford step 9095 current loss 0.371258, current_train_items 291072.
I0302 19:02:38.018519 22535416901760 run.py:483] Algo bellman_ford step 9096 current loss 0.490381, current_train_items 291104.
I0302 19:02:38.041699 22535416901760 run.py:483] Algo bellman_ford step 9097 current loss 0.598743, current_train_items 291136.
I0302 19:02:38.073405 22535416901760 run.py:483] Algo bellman_ford step 9098 current loss 0.628110, current_train_items 291168.
I0302 19:02:38.106163 22535416901760 run.py:483] Algo bellman_ford step 9099 current loss 0.691388, current_train_items 291200.
I0302 19:02:38.125932 22535416901760 run.py:483] Algo bellman_ford step 9100 current loss 0.278700, current_train_items 291232.
I0302 19:02:38.133690 22535416901760 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.927734375, 'score': 0.927734375, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:02:38.133796 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.928, val scores are: bellman_ford: 0.928
I0302 19:02:38.150583 22535416901760 run.py:483] Algo bellman_ford step 9101 current loss 0.391958, current_train_items 291264.
I0302 19:02:38.175818 22535416901760 run.py:483] Algo bellman_ford step 9102 current loss 0.568876, current_train_items 291296.
I0302 19:02:38.207913 22535416901760 run.py:483] Algo bellman_ford step 9103 current loss 0.616559, current_train_items 291328.
I0302 19:02:38.243392 22535416901760 run.py:483] Algo bellman_ford step 9104 current loss 0.777061, current_train_items 291360.
I0302 19:02:38.263611 22535416901760 run.py:483] Algo bellman_ford step 9105 current loss 0.287190, current_train_items 291392.
I0302 19:02:38.279818 22535416901760 run.py:483] Algo bellman_ford step 9106 current loss 0.478795, current_train_items 291424.
I0302 19:02:38.303879 22535416901760 run.py:483] Algo bellman_ford step 9107 current loss 0.563191, current_train_items 291456.
I0302 19:02:38.336064 22535416901760 run.py:483] Algo bellman_ford step 9108 current loss 0.611823, current_train_items 291488.
I0302 19:02:38.369775 22535416901760 run.py:483] Algo bellman_ford step 9109 current loss 0.646074, current_train_items 291520.
I0302 19:02:38.389401 22535416901760 run.py:483] Algo bellman_ford step 9110 current loss 0.317015, current_train_items 291552.
I0302 19:02:38.405540 22535416901760 run.py:483] Algo bellman_ford step 9111 current loss 0.456638, current_train_items 291584.
I0302 19:02:38.428891 22535416901760 run.py:483] Algo bellman_ford step 9112 current loss 0.680974, current_train_items 291616.
I0302 19:02:38.462203 22535416901760 run.py:483] Algo bellman_ford step 9113 current loss 0.586898, current_train_items 291648.
I0302 19:02:38.497279 22535416901760 run.py:483] Algo bellman_ford step 9114 current loss 0.719752, current_train_items 291680.
I0302 19:02:38.517083 22535416901760 run.py:483] Algo bellman_ford step 9115 current loss 0.335578, current_train_items 291712.
I0302 19:02:38.533510 22535416901760 run.py:483] Algo bellman_ford step 9116 current loss 0.410626, current_train_items 291744.
I0302 19:02:38.557009 22535416901760 run.py:483] Algo bellman_ford step 9117 current loss 0.646472, current_train_items 291776.
I0302 19:02:38.587926 22535416901760 run.py:483] Algo bellman_ford step 9118 current loss 0.699600, current_train_items 291808.
I0302 19:02:38.622965 22535416901760 run.py:483] Algo bellman_ford step 9119 current loss 0.777186, current_train_items 291840.
I0302 19:02:38.642457 22535416901760 run.py:483] Algo bellman_ford step 9120 current loss 0.262975, current_train_items 291872.
I0302 19:02:38.658672 22535416901760 run.py:483] Algo bellman_ford step 9121 current loss 0.467115, current_train_items 291904.
I0302 19:02:38.681992 22535416901760 run.py:483] Algo bellman_ford step 9122 current loss 0.590251, current_train_items 291936.
I0302 19:02:38.715015 22535416901760 run.py:483] Algo bellman_ford step 9123 current loss 0.750181, current_train_items 291968.
I0302 19:02:38.750571 22535416901760 run.py:483] Algo bellman_ford step 9124 current loss 0.714240, current_train_items 292000.
I0302 19:02:38.770308 22535416901760 run.py:483] Algo bellman_ford step 9125 current loss 0.298790, current_train_items 292032.
I0302 19:02:38.786115 22535416901760 run.py:483] Algo bellman_ford step 9126 current loss 0.399947, current_train_items 292064.
I0302 19:02:38.808942 22535416901760 run.py:483] Algo bellman_ford step 9127 current loss 0.594688, current_train_items 292096.
I0302 19:02:38.841400 22535416901760 run.py:483] Algo bellman_ford step 9128 current loss 0.748725, current_train_items 292128.
I0302 19:02:38.875152 22535416901760 run.py:483] Algo bellman_ford step 9129 current loss 0.742202, current_train_items 292160.
I0302 19:02:38.895006 22535416901760 run.py:483] Algo bellman_ford step 9130 current loss 0.284219, current_train_items 292192.
I0302 19:02:38.911148 22535416901760 run.py:483] Algo bellman_ford step 9131 current loss 0.527519, current_train_items 292224.
I0302 19:02:38.935764 22535416901760 run.py:483] Algo bellman_ford step 9132 current loss 0.559418, current_train_items 292256.
I0302 19:02:38.967816 22535416901760 run.py:483] Algo bellman_ford step 9133 current loss 0.635914, current_train_items 292288.
I0302 19:02:39.000244 22535416901760 run.py:483] Algo bellman_ford step 9134 current loss 0.680524, current_train_items 292320.
I0302 19:02:39.019761 22535416901760 run.py:483] Algo bellman_ford step 9135 current loss 0.299729, current_train_items 292352.
I0302 19:02:39.035474 22535416901760 run.py:483] Algo bellman_ford step 9136 current loss 0.403912, current_train_items 292384.
I0302 19:02:39.058418 22535416901760 run.py:483] Algo bellman_ford step 9137 current loss 0.538121, current_train_items 292416.
I0302 19:02:39.089836 22535416901760 run.py:483] Algo bellman_ford step 9138 current loss 0.646132, current_train_items 292448.
I0302 19:02:39.122268 22535416901760 run.py:483] Algo bellman_ford step 9139 current loss 0.682507, current_train_items 292480.
I0302 19:02:39.141632 22535416901760 run.py:483] Algo bellman_ford step 9140 current loss 0.294587, current_train_items 292512.
I0302 19:02:39.157588 22535416901760 run.py:483] Algo bellman_ford step 9141 current loss 0.472556, current_train_items 292544.
I0302 19:02:39.180938 22535416901760 run.py:483] Algo bellman_ford step 9142 current loss 0.467330, current_train_items 292576.
I0302 19:02:39.211858 22535416901760 run.py:483] Algo bellman_ford step 9143 current loss 0.607573, current_train_items 292608.
I0302 19:02:39.246425 22535416901760 run.py:483] Algo bellman_ford step 9144 current loss 0.680432, current_train_items 292640.
I0302 19:02:39.265689 22535416901760 run.py:483] Algo bellman_ford step 9145 current loss 0.295361, current_train_items 292672.
I0302 19:02:39.281908 22535416901760 run.py:483] Algo bellman_ford step 9146 current loss 0.417851, current_train_items 292704.
I0302 19:02:39.305876 22535416901760 run.py:483] Algo bellman_ford step 9147 current loss 0.650697, current_train_items 292736.
I0302 19:02:39.337560 22535416901760 run.py:483] Algo bellman_ford step 9148 current loss 0.782873, current_train_items 292768.
I0302 19:02:39.367781 22535416901760 run.py:483] Algo bellman_ford step 9149 current loss 0.687562, current_train_items 292800.
I0302 19:02:39.387147 22535416901760 run.py:483] Algo bellman_ford step 9150 current loss 0.284483, current_train_items 292832.
I0302 19:02:39.395409 22535416901760 run.py:503] (val) algo bellman_ford step 9150: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:02:39.395518 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:02:39.412384 22535416901760 run.py:483] Algo bellman_ford step 9151 current loss 0.448507, current_train_items 292864.
I0302 19:02:39.436627 22535416901760 run.py:483] Algo bellman_ford step 9152 current loss 0.482715, current_train_items 292896.
I0302 19:02:39.469066 22535416901760 run.py:483] Algo bellman_ford step 9153 current loss 0.677509, current_train_items 292928.
I0302 19:02:39.501136 22535416901760 run.py:483] Algo bellman_ford step 9154 current loss 0.657337, current_train_items 292960.
I0302 19:02:39.520820 22535416901760 run.py:483] Algo bellman_ford step 9155 current loss 0.271560, current_train_items 292992.
I0302 19:02:39.537386 22535416901760 run.py:483] Algo bellman_ford step 9156 current loss 0.512210, current_train_items 293024.
I0302 19:02:39.561839 22535416901760 run.py:483] Algo bellman_ford step 9157 current loss 0.587802, current_train_items 293056.
I0302 19:02:39.592864 22535416901760 run.py:483] Algo bellman_ford step 9158 current loss 0.581266, current_train_items 293088.
I0302 19:02:39.627514 22535416901760 run.py:483] Algo bellman_ford step 9159 current loss 0.738111, current_train_items 293120.
I0302 19:02:39.647194 22535416901760 run.py:483] Algo bellman_ford step 9160 current loss 0.256581, current_train_items 293152.
I0302 19:02:39.663659 22535416901760 run.py:483] Algo bellman_ford step 9161 current loss 0.412649, current_train_items 293184.
I0302 19:02:39.687285 22535416901760 run.py:483] Algo bellman_ford step 9162 current loss 0.578981, current_train_items 293216.
I0302 19:02:39.718346 22535416901760 run.py:483] Algo bellman_ford step 9163 current loss 0.569630, current_train_items 293248.
I0302 19:02:39.753723 22535416901760 run.py:483] Algo bellman_ford step 9164 current loss 0.669657, current_train_items 293280.
I0302 19:02:39.773445 22535416901760 run.py:483] Algo bellman_ford step 9165 current loss 0.363594, current_train_items 293312.
I0302 19:02:39.789764 22535416901760 run.py:483] Algo bellman_ford step 9166 current loss 0.414981, current_train_items 293344.
I0302 19:02:39.813553 22535416901760 run.py:483] Algo bellman_ford step 9167 current loss 0.553933, current_train_items 293376.
I0302 19:02:39.845840 22535416901760 run.py:483] Algo bellman_ford step 9168 current loss 0.570178, current_train_items 293408.
I0302 19:02:39.878690 22535416901760 run.py:483] Algo bellman_ford step 9169 current loss 0.693696, current_train_items 293440.
I0302 19:02:39.898458 22535416901760 run.py:483] Algo bellman_ford step 9170 current loss 0.262272, current_train_items 293472.
I0302 19:02:39.914461 22535416901760 run.py:483] Algo bellman_ford step 9171 current loss 0.431417, current_train_items 293504.
I0302 19:02:39.939532 22535416901760 run.py:483] Algo bellman_ford step 9172 current loss 0.644330, current_train_items 293536.
I0302 19:02:39.971261 22535416901760 run.py:483] Algo bellman_ford step 9173 current loss 0.544166, current_train_items 293568.
I0302 19:02:40.005895 22535416901760 run.py:483] Algo bellman_ford step 9174 current loss 0.724126, current_train_items 293600.
I0302 19:02:40.025945 22535416901760 run.py:483] Algo bellman_ford step 9175 current loss 0.299835, current_train_items 293632.
I0302 19:02:40.042506 22535416901760 run.py:483] Algo bellman_ford step 9176 current loss 0.349812, current_train_items 293664.
I0302 19:02:40.065355 22535416901760 run.py:483] Algo bellman_ford step 9177 current loss 0.505111, current_train_items 293696.
I0302 19:02:40.095519 22535416901760 run.py:483] Algo bellman_ford step 9178 current loss 0.588301, current_train_items 293728.
I0302 19:02:40.130692 22535416901760 run.py:483] Algo bellman_ford step 9179 current loss 0.769412, current_train_items 293760.
I0302 19:02:40.149902 22535416901760 run.py:483] Algo bellman_ford step 9180 current loss 0.329600, current_train_items 293792.
I0302 19:02:40.166187 22535416901760 run.py:483] Algo bellman_ford step 9181 current loss 0.511402, current_train_items 293824.
I0302 19:02:40.190721 22535416901760 run.py:483] Algo bellman_ford step 9182 current loss 0.617521, current_train_items 293856.
I0302 19:02:40.221096 22535416901760 run.py:483] Algo bellman_ford step 9183 current loss 0.565317, current_train_items 293888.
I0302 19:02:40.253904 22535416901760 run.py:483] Algo bellman_ford step 9184 current loss 0.668485, current_train_items 293920.
I0302 19:02:40.273691 22535416901760 run.py:483] Algo bellman_ford step 9185 current loss 0.292852, current_train_items 293952.
I0302 19:02:40.290169 22535416901760 run.py:483] Algo bellman_ford step 9186 current loss 0.440671, current_train_items 293984.
I0302 19:02:40.314066 22535416901760 run.py:483] Algo bellman_ford step 9187 current loss 0.506716, current_train_items 294016.
I0302 19:02:40.346924 22535416901760 run.py:483] Algo bellman_ford step 9188 current loss 0.601893, current_train_items 294048.
I0302 19:02:40.382412 22535416901760 run.py:483] Algo bellman_ford step 9189 current loss 0.728011, current_train_items 294080.
I0302 19:02:40.402704 22535416901760 run.py:483] Algo bellman_ford step 9190 current loss 0.358672, current_train_items 294112.
I0302 19:02:40.418720 22535416901760 run.py:483] Algo bellman_ford step 9191 current loss 0.563772, current_train_items 294144.
I0302 19:02:40.442783 22535416901760 run.py:483] Algo bellman_ford step 9192 current loss 0.542663, current_train_items 294176.
I0302 19:02:40.474557 22535416901760 run.py:483] Algo bellman_ford step 9193 current loss 0.621460, current_train_items 294208.
I0302 19:02:40.507261 22535416901760 run.py:483] Algo bellman_ford step 9194 current loss 0.623289, current_train_items 294240.
I0302 19:02:40.526899 22535416901760 run.py:483] Algo bellman_ford step 9195 current loss 0.319010, current_train_items 294272.
I0302 19:02:40.543580 22535416901760 run.py:483] Algo bellman_ford step 9196 current loss 0.452141, current_train_items 294304.
I0302 19:02:40.567936 22535416901760 run.py:483] Algo bellman_ford step 9197 current loss 0.670746, current_train_items 294336.
I0302 19:02:40.598666 22535416901760 run.py:483] Algo bellman_ford step 9198 current loss 0.605216, current_train_items 294368.
I0302 19:02:40.630955 22535416901760 run.py:483] Algo bellman_ford step 9199 current loss 0.655802, current_train_items 294400.
I0302 19:02:40.650842 22535416901760 run.py:483] Algo bellman_ford step 9200 current loss 0.297929, current_train_items 294432.
I0302 19:02:40.658750 22535416901760 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.9248046875, 'score': 0.9248046875, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:02:40.658856 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.925, val scores are: bellman_ford: 0.925
I0302 19:02:40.675640 22535416901760 run.py:483] Algo bellman_ford step 9201 current loss 0.375453, current_train_items 294464.
I0302 19:02:40.699684 22535416901760 run.py:483] Algo bellman_ford step 9202 current loss 0.509987, current_train_items 294496.
I0302 19:02:40.732557 22535416901760 run.py:483] Algo bellman_ford step 9203 current loss 0.687285, current_train_items 294528.
I0302 19:02:40.767341 22535416901760 run.py:483] Algo bellman_ford step 9204 current loss 0.693796, current_train_items 294560.
I0302 19:02:40.787379 22535416901760 run.py:483] Algo bellman_ford step 9205 current loss 0.260256, current_train_items 294592.
I0302 19:02:40.803450 22535416901760 run.py:483] Algo bellman_ford step 9206 current loss 0.385612, current_train_items 294624.
I0302 19:02:40.827189 22535416901760 run.py:483] Algo bellman_ford step 9207 current loss 0.489275, current_train_items 294656.
I0302 19:02:40.858678 22535416901760 run.py:483] Algo bellman_ford step 9208 current loss 0.603411, current_train_items 294688.
I0302 19:02:40.889919 22535416901760 run.py:483] Algo bellman_ford step 9209 current loss 0.662077, current_train_items 294720.
I0302 19:02:40.909212 22535416901760 run.py:483] Algo bellman_ford step 9210 current loss 0.300125, current_train_items 294752.
I0302 19:02:40.925746 22535416901760 run.py:483] Algo bellman_ford step 9211 current loss 0.491220, current_train_items 294784.
I0302 19:02:40.949104 22535416901760 run.py:483] Algo bellman_ford step 9212 current loss 0.569583, current_train_items 294816.
I0302 19:02:40.980312 22535416901760 run.py:483] Algo bellman_ford step 9213 current loss 0.644087, current_train_items 294848.
I0302 19:02:41.012542 22535416901760 run.py:483] Algo bellman_ford step 9214 current loss 0.699275, current_train_items 294880.
I0302 19:02:41.031812 22535416901760 run.py:483] Algo bellman_ford step 9215 current loss 0.352329, current_train_items 294912.
I0302 19:02:41.048084 22535416901760 run.py:483] Algo bellman_ford step 9216 current loss 0.519522, current_train_items 294944.
I0302 19:02:41.071228 22535416901760 run.py:483] Algo bellman_ford step 9217 current loss 0.641614, current_train_items 294976.
I0302 19:02:41.102016 22535416901760 run.py:483] Algo bellman_ford step 9218 current loss 0.611750, current_train_items 295008.
I0302 19:02:41.135267 22535416901760 run.py:483] Algo bellman_ford step 9219 current loss 0.619866, current_train_items 295040.
I0302 19:02:41.154702 22535416901760 run.py:483] Algo bellman_ford step 9220 current loss 0.283375, current_train_items 295072.
I0302 19:02:41.170635 22535416901760 run.py:483] Algo bellman_ford step 9221 current loss 0.372981, current_train_items 295104.
I0302 19:02:41.194783 22535416901760 run.py:483] Algo bellman_ford step 9222 current loss 0.598289, current_train_items 295136.
I0302 19:02:41.225584 22535416901760 run.py:483] Algo bellman_ford step 9223 current loss 0.629752, current_train_items 295168.
I0302 19:02:41.258997 22535416901760 run.py:483] Algo bellman_ford step 9224 current loss 0.688160, current_train_items 295200.
I0302 19:02:41.278624 22535416901760 run.py:483] Algo bellman_ford step 9225 current loss 0.303778, current_train_items 295232.
I0302 19:02:41.294771 22535416901760 run.py:483] Algo bellman_ford step 9226 current loss 0.399210, current_train_items 295264.
I0302 19:02:41.318802 22535416901760 run.py:483] Algo bellman_ford step 9227 current loss 0.584945, current_train_items 295296.
I0302 19:02:41.350221 22535416901760 run.py:483] Algo bellman_ford step 9228 current loss 0.636181, current_train_items 295328.
I0302 19:02:41.381093 22535416901760 run.py:483] Algo bellman_ford step 9229 current loss 0.723293, current_train_items 295360.
I0302 19:02:41.400421 22535416901760 run.py:483] Algo bellman_ford step 9230 current loss 0.296922, current_train_items 295392.
I0302 19:02:41.416944 22535416901760 run.py:483] Algo bellman_ford step 9231 current loss 0.446353, current_train_items 295424.
I0302 19:02:41.441015 22535416901760 run.py:483] Algo bellman_ford step 9232 current loss 0.636644, current_train_items 295456.
I0302 19:02:41.474647 22535416901760 run.py:483] Algo bellman_ford step 9233 current loss 0.618685, current_train_items 295488.
I0302 19:02:41.508741 22535416901760 run.py:483] Algo bellman_ford step 9234 current loss 0.719556, current_train_items 295520.
I0302 19:02:41.527974 22535416901760 run.py:483] Algo bellman_ford step 9235 current loss 0.253462, current_train_items 295552.
I0302 19:02:41.544303 22535416901760 run.py:483] Algo bellman_ford step 9236 current loss 0.453560, current_train_items 295584.
I0302 19:02:41.567726 22535416901760 run.py:483] Algo bellman_ford step 9237 current loss 0.518720, current_train_items 295616.
I0302 19:02:41.600311 22535416901760 run.py:483] Algo bellman_ford step 9238 current loss 0.726151, current_train_items 295648.
I0302 19:02:41.634294 22535416901760 run.py:483] Algo bellman_ford step 9239 current loss 0.624901, current_train_items 295680.
I0302 19:02:41.653722 22535416901760 run.py:483] Algo bellman_ford step 9240 current loss 0.272076, current_train_items 295712.
I0302 19:02:41.670413 22535416901760 run.py:483] Algo bellman_ford step 9241 current loss 0.503546, current_train_items 295744.
I0302 19:02:41.694667 22535416901760 run.py:483] Algo bellman_ford step 9242 current loss 0.650167, current_train_items 295776.
I0302 19:02:41.726052 22535416901760 run.py:483] Algo bellman_ford step 9243 current loss 0.659189, current_train_items 295808.
I0302 19:02:41.758461 22535416901760 run.py:483] Algo bellman_ford step 9244 current loss 0.660757, current_train_items 295840.
I0302 19:02:41.778034 22535416901760 run.py:483] Algo bellman_ford step 9245 current loss 0.299752, current_train_items 295872.
I0302 19:02:41.794319 22535416901760 run.py:483] Algo bellman_ford step 9246 current loss 0.445323, current_train_items 295904.
I0302 19:02:41.817217 22535416901760 run.py:483] Algo bellman_ford step 9247 current loss 0.552876, current_train_items 295936.
I0302 19:02:41.848558 22535416901760 run.py:483] Algo bellman_ford step 9248 current loss 0.707813, current_train_items 295968.
I0302 19:02:41.882492 22535416901760 run.py:483] Algo bellman_ford step 9249 current loss 0.812477, current_train_items 296000.
I0302 19:02:41.902134 22535416901760 run.py:483] Algo bellman_ford step 9250 current loss 0.313403, current_train_items 296032.
I0302 19:02:41.910164 22535416901760 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:02:41.910274 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:41.926955 22535416901760 run.py:483] Algo bellman_ford step 9251 current loss 0.489769, current_train_items 296064.
I0302 19:02:41.951375 22535416901760 run.py:483] Algo bellman_ford step 9252 current loss 0.611504, current_train_items 296096.
I0302 19:02:41.982525 22535416901760 run.py:483] Algo bellman_ford step 9253 current loss 0.634344, current_train_items 296128.
I0302 19:02:42.014901 22535416901760 run.py:483] Algo bellman_ford step 9254 current loss 0.642820, current_train_items 296160.
I0302 19:02:42.034820 22535416901760 run.py:483] Algo bellman_ford step 9255 current loss 0.291269, current_train_items 296192.
I0302 19:02:42.050737 22535416901760 run.py:483] Algo bellman_ford step 9256 current loss 0.366642, current_train_items 296224.
I0302 19:02:42.074780 22535416901760 run.py:483] Algo bellman_ford step 9257 current loss 0.733339, current_train_items 296256.
I0302 19:02:42.106706 22535416901760 run.py:483] Algo bellman_ford step 9258 current loss 0.883165, current_train_items 296288.
I0302 19:02:42.140042 22535416901760 run.py:483] Algo bellman_ford step 9259 current loss 0.830365, current_train_items 296320.
I0302 19:02:42.160270 22535416901760 run.py:483] Algo bellman_ford step 9260 current loss 0.338677, current_train_items 296352.
I0302 19:02:42.177271 22535416901760 run.py:483] Algo bellman_ford step 9261 current loss 0.539792, current_train_items 296384.
I0302 19:02:42.200053 22535416901760 run.py:483] Algo bellman_ford step 9262 current loss 0.531926, current_train_items 296416.
I0302 19:02:42.231743 22535416901760 run.py:483] Algo bellman_ford step 9263 current loss 0.636984, current_train_items 296448.
I0302 19:02:42.265438 22535416901760 run.py:483] Algo bellman_ford step 9264 current loss 0.676370, current_train_items 296480.
I0302 19:02:42.285197 22535416901760 run.py:483] Algo bellman_ford step 9265 current loss 0.325108, current_train_items 296512.
I0302 19:02:42.301551 22535416901760 run.py:483] Algo bellman_ford step 9266 current loss 0.442076, current_train_items 296544.
I0302 19:02:42.323961 22535416901760 run.py:483] Algo bellman_ford step 9267 current loss 0.544665, current_train_items 296576.
I0302 19:02:42.355238 22535416901760 run.py:483] Algo bellman_ford step 9268 current loss 0.628237, current_train_items 296608.
I0302 19:02:42.388848 22535416901760 run.py:483] Algo bellman_ford step 9269 current loss 0.689086, current_train_items 296640.
I0302 19:02:42.408683 22535416901760 run.py:483] Algo bellman_ford step 9270 current loss 0.309646, current_train_items 296672.
I0302 19:02:42.424860 22535416901760 run.py:483] Algo bellman_ford step 9271 current loss 0.448974, current_train_items 296704.
I0302 19:02:42.448101 22535416901760 run.py:483] Algo bellman_ford step 9272 current loss 0.556862, current_train_items 296736.
I0302 19:02:42.481385 22535416901760 run.py:483] Algo bellman_ford step 9273 current loss 0.663064, current_train_items 296768.
I0302 19:02:42.515800 22535416901760 run.py:483] Algo bellman_ford step 9274 current loss 0.715161, current_train_items 296800.
I0302 19:02:42.536103 22535416901760 run.py:483] Algo bellman_ford step 9275 current loss 0.351696, current_train_items 296832.
I0302 19:02:42.552464 22535416901760 run.py:483] Algo bellman_ford step 9276 current loss 0.458011, current_train_items 296864.
I0302 19:02:42.575658 22535416901760 run.py:483] Algo bellman_ford step 9277 current loss 0.573344, current_train_items 296896.
I0302 19:02:42.607410 22535416901760 run.py:483] Algo bellman_ford step 9278 current loss 0.615802, current_train_items 296928.
I0302 19:02:42.641136 22535416901760 run.py:483] Algo bellman_ford step 9279 current loss 0.806786, current_train_items 296960.
I0302 19:02:42.660723 22535416901760 run.py:483] Algo bellman_ford step 9280 current loss 0.350032, current_train_items 296992.
I0302 19:02:42.677130 22535416901760 run.py:483] Algo bellman_ford step 9281 current loss 0.420824, current_train_items 297024.
I0302 19:02:42.700722 22535416901760 run.py:483] Algo bellman_ford step 9282 current loss 0.537596, current_train_items 297056.
I0302 19:02:42.730648 22535416901760 run.py:483] Algo bellman_ford step 9283 current loss 0.536036, current_train_items 297088.
I0302 19:02:42.763451 22535416901760 run.py:483] Algo bellman_ford step 9284 current loss 0.762128, current_train_items 297120.
I0302 19:02:42.783311 22535416901760 run.py:483] Algo bellman_ford step 9285 current loss 0.244311, current_train_items 297152.
I0302 19:02:42.798981 22535416901760 run.py:483] Algo bellman_ford step 9286 current loss 0.457662, current_train_items 297184.
I0302 19:02:42.823599 22535416901760 run.py:483] Algo bellman_ford step 9287 current loss 0.653661, current_train_items 297216.
I0302 19:02:42.854215 22535416901760 run.py:483] Algo bellman_ford step 9288 current loss 0.585232, current_train_items 297248.
I0302 19:02:42.889613 22535416901760 run.py:483] Algo bellman_ford step 9289 current loss 0.703449, current_train_items 297280.
I0302 19:02:42.909317 22535416901760 run.py:483] Algo bellman_ford step 9290 current loss 0.338073, current_train_items 297312.
I0302 19:02:42.925453 22535416901760 run.py:483] Algo bellman_ford step 9291 current loss 0.452300, current_train_items 297344.
I0302 19:02:42.949575 22535416901760 run.py:483] Algo bellman_ford step 9292 current loss 0.604688, current_train_items 297376.
I0302 19:02:42.980148 22535416901760 run.py:483] Algo bellman_ford step 9293 current loss 0.593759, current_train_items 297408.
I0302 19:02:43.014594 22535416901760 run.py:483] Algo bellman_ford step 9294 current loss 0.791225, current_train_items 297440.
I0302 19:02:43.034240 22535416901760 run.py:483] Algo bellman_ford step 9295 current loss 0.270699, current_train_items 297472.
I0302 19:02:43.050451 22535416901760 run.py:483] Algo bellman_ford step 9296 current loss 0.370937, current_train_items 297504.
I0302 19:02:43.074356 22535416901760 run.py:483] Algo bellman_ford step 9297 current loss 0.561130, current_train_items 297536.
I0302 19:02:43.106077 22535416901760 run.py:483] Algo bellman_ford step 9298 current loss 0.599339, current_train_items 297568.
I0302 19:02:43.140264 22535416901760 run.py:483] Algo bellman_ford step 9299 current loss 0.741946, current_train_items 297600.
I0302 19:02:43.160246 22535416901760 run.py:483] Algo bellman_ford step 9300 current loss 0.367877, current_train_items 297632.
I0302 19:02:43.168082 22535416901760 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.9208984375, 'score': 0.9208984375, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:02:43.168204 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.921, val scores are: bellman_ford: 0.921
I0302 19:02:43.185240 22535416901760 run.py:483] Algo bellman_ford step 9301 current loss 0.424090, current_train_items 297664.
I0302 19:02:43.209665 22535416901760 run.py:483] Algo bellman_ford step 9302 current loss 0.520352, current_train_items 297696.
I0302 19:02:43.241800 22535416901760 run.py:483] Algo bellman_ford step 9303 current loss 0.590426, current_train_items 297728.
I0302 19:02:43.276895 22535416901760 run.py:483] Algo bellman_ford step 9304 current loss 0.724571, current_train_items 297760.
I0302 19:02:43.297071 22535416901760 run.py:483] Algo bellman_ford step 9305 current loss 0.283759, current_train_items 297792.
I0302 19:02:43.312927 22535416901760 run.py:483] Algo bellman_ford step 9306 current loss 0.463721, current_train_items 297824.
I0302 19:02:43.337392 22535416901760 run.py:483] Algo bellman_ford step 9307 current loss 0.620012, current_train_items 297856.
I0302 19:02:43.369212 22535416901760 run.py:483] Algo bellman_ford step 9308 current loss 0.639371, current_train_items 297888.
I0302 19:02:43.402292 22535416901760 run.py:483] Algo bellman_ford step 9309 current loss 0.669538, current_train_items 297920.
I0302 19:02:43.422008 22535416901760 run.py:483] Algo bellman_ford step 9310 current loss 0.312620, current_train_items 297952.
I0302 19:02:43.438730 22535416901760 run.py:483] Algo bellman_ford step 9311 current loss 0.377202, current_train_items 297984.
I0302 19:02:43.462267 22535416901760 run.py:483] Algo bellman_ford step 9312 current loss 0.555973, current_train_items 298016.
I0302 19:02:43.492663 22535416901760 run.py:483] Algo bellman_ford step 9313 current loss 0.642822, current_train_items 298048.
I0302 19:02:43.526563 22535416901760 run.py:483] Algo bellman_ford step 9314 current loss 0.670979, current_train_items 298080.
I0302 19:02:43.546396 22535416901760 run.py:483] Algo bellman_ford step 9315 current loss 0.347005, current_train_items 298112.
I0302 19:02:43.562168 22535416901760 run.py:483] Algo bellman_ford step 9316 current loss 0.399016, current_train_items 298144.
I0302 19:02:43.585887 22535416901760 run.py:483] Algo bellman_ford step 9317 current loss 0.520214, current_train_items 298176.
I0302 19:02:43.618104 22535416901760 run.py:483] Algo bellman_ford step 9318 current loss 0.659509, current_train_items 298208.
I0302 19:02:43.651635 22535416901760 run.py:483] Algo bellman_ford step 9319 current loss 0.602295, current_train_items 298240.
I0302 19:02:43.671178 22535416901760 run.py:483] Algo bellman_ford step 9320 current loss 0.334229, current_train_items 298272.
I0302 19:02:43.687403 22535416901760 run.py:483] Algo bellman_ford step 9321 current loss 0.444491, current_train_items 298304.
I0302 19:02:43.709699 22535416901760 run.py:483] Algo bellman_ford step 9322 current loss 0.514315, current_train_items 298336.
I0302 19:02:43.740490 22535416901760 run.py:483] Algo bellman_ford step 9323 current loss 0.626787, current_train_items 298368.
I0302 19:02:43.776875 22535416901760 run.py:483] Algo bellman_ford step 9324 current loss 0.927930, current_train_items 298400.
I0302 19:02:43.796463 22535416901760 run.py:483] Algo bellman_ford step 9325 current loss 0.399471, current_train_items 298432.
I0302 19:02:43.812458 22535416901760 run.py:483] Algo bellman_ford step 9326 current loss 0.487618, current_train_items 298464.
I0302 19:02:43.836354 22535416901760 run.py:483] Algo bellman_ford step 9327 current loss 0.686042, current_train_items 298496.
I0302 19:02:43.868801 22535416901760 run.py:483] Algo bellman_ford step 9328 current loss 0.721644, current_train_items 298528.
I0302 19:02:43.899568 22535416901760 run.py:483] Algo bellman_ford step 9329 current loss 0.842812, current_train_items 298560.
I0302 19:02:43.919266 22535416901760 run.py:483] Algo bellman_ford step 9330 current loss 0.364194, current_train_items 298592.
I0302 19:02:43.935516 22535416901760 run.py:483] Algo bellman_ford step 9331 current loss 0.519871, current_train_items 298624.
I0302 19:02:43.958774 22535416901760 run.py:483] Algo bellman_ford step 9332 current loss 0.586627, current_train_items 298656.
I0302 19:02:43.990509 22535416901760 run.py:483] Algo bellman_ford step 9333 current loss 0.693061, current_train_items 298688.
I0302 19:02:44.023494 22535416901760 run.py:483] Algo bellman_ford step 9334 current loss 0.819126, current_train_items 298720.
I0302 19:02:44.042968 22535416901760 run.py:483] Algo bellman_ford step 9335 current loss 0.274582, current_train_items 298752.
I0302 19:02:44.059128 22535416901760 run.py:483] Algo bellman_ford step 9336 current loss 0.472375, current_train_items 298784.
I0302 19:02:44.083637 22535416901760 run.py:483] Algo bellman_ford step 9337 current loss 0.712265, current_train_items 298816.
I0302 19:02:44.115144 22535416901760 run.py:483] Algo bellman_ford step 9338 current loss 0.698289, current_train_items 298848.
I0302 19:02:44.149186 22535416901760 run.py:483] Algo bellman_ford step 9339 current loss 0.711226, current_train_items 298880.
I0302 19:02:44.168601 22535416901760 run.py:483] Algo bellman_ford step 9340 current loss 0.329417, current_train_items 298912.
I0302 19:02:44.184376 22535416901760 run.py:483] Algo bellman_ford step 9341 current loss 0.452604, current_train_items 298944.
I0302 19:02:44.207957 22535416901760 run.py:483] Algo bellman_ford step 9342 current loss 0.486426, current_train_items 298976.
I0302 19:02:44.239837 22535416901760 run.py:483] Algo bellman_ford step 9343 current loss 0.601290, current_train_items 299008.
I0302 19:02:44.274071 22535416901760 run.py:483] Algo bellman_ford step 9344 current loss 0.680468, current_train_items 299040.
I0302 19:02:44.293734 22535416901760 run.py:483] Algo bellman_ford step 9345 current loss 0.340285, current_train_items 299072.
I0302 19:02:44.309851 22535416901760 run.py:483] Algo bellman_ford step 9346 current loss 0.473282, current_train_items 299104.
I0302 19:02:44.333656 22535416901760 run.py:483] Algo bellman_ford step 9347 current loss 0.585338, current_train_items 299136.
I0302 19:02:44.365787 22535416901760 run.py:483] Algo bellman_ford step 9348 current loss 0.659310, current_train_items 299168.
I0302 19:02:44.398233 22535416901760 run.py:483] Algo bellman_ford step 9349 current loss 0.669130, current_train_items 299200.
I0302 19:02:44.418012 22535416901760 run.py:483] Algo bellman_ford step 9350 current loss 0.327742, current_train_items 299232.
I0302 19:02:44.426226 22535416901760 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9404296875, 'score': 0.9404296875, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:02:44.426333 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.940, val scores are: bellman_ford: 0.940
I0302 19:02:44.442809 22535416901760 run.py:483] Algo bellman_ford step 9351 current loss 0.439620, current_train_items 299264.
I0302 19:02:44.467751 22535416901760 run.py:483] Algo bellman_ford step 9352 current loss 0.662920, current_train_items 299296.
I0302 19:02:44.500068 22535416901760 run.py:483] Algo bellman_ford step 9353 current loss 0.666945, current_train_items 299328.
I0302 19:02:44.535893 22535416901760 run.py:483] Algo bellman_ford step 9354 current loss 0.744647, current_train_items 299360.
I0302 19:02:44.555735 22535416901760 run.py:483] Algo bellman_ford step 9355 current loss 0.336429, current_train_items 299392.
I0302 19:02:44.571246 22535416901760 run.py:483] Algo bellman_ford step 9356 current loss 0.389967, current_train_items 299424.
I0302 19:02:44.595482 22535416901760 run.py:483] Algo bellman_ford step 9357 current loss 0.666813, current_train_items 299456.
I0302 19:02:44.627113 22535416901760 run.py:483] Algo bellman_ford step 9358 current loss 0.673733, current_train_items 299488.
I0302 19:02:44.660328 22535416901760 run.py:483] Algo bellman_ford step 9359 current loss 0.671472, current_train_items 299520.
I0302 19:02:44.680337 22535416901760 run.py:483] Algo bellman_ford step 9360 current loss 0.331590, current_train_items 299552.
I0302 19:02:44.696574 22535416901760 run.py:483] Algo bellman_ford step 9361 current loss 0.439208, current_train_items 299584.
I0302 19:02:44.719688 22535416901760 run.py:483] Algo bellman_ford step 9362 current loss 0.508180, current_train_items 299616.
I0302 19:02:44.751665 22535416901760 run.py:483] Algo bellman_ford step 9363 current loss 0.585316, current_train_items 299648.
I0302 19:02:44.785705 22535416901760 run.py:483] Algo bellman_ford step 9364 current loss 0.704286, current_train_items 299680.
I0302 19:02:44.805553 22535416901760 run.py:483] Algo bellman_ford step 9365 current loss 0.345700, current_train_items 299712.
I0302 19:02:44.822031 22535416901760 run.py:483] Algo bellman_ford step 9366 current loss 0.474849, current_train_items 299744.
I0302 19:02:44.847511 22535416901760 run.py:483] Algo bellman_ford step 9367 current loss 0.598606, current_train_items 299776.
I0302 19:02:44.879950 22535416901760 run.py:483] Algo bellman_ford step 9368 current loss 0.629602, current_train_items 299808.
I0302 19:02:44.913321 22535416901760 run.py:483] Algo bellman_ford step 9369 current loss 0.676238, current_train_items 299840.
I0302 19:02:44.933505 22535416901760 run.py:483] Algo bellman_ford step 9370 current loss 0.303090, current_train_items 299872.
I0302 19:02:44.949892 22535416901760 run.py:483] Algo bellman_ford step 9371 current loss 0.471627, current_train_items 299904.
I0302 19:02:44.973411 22535416901760 run.py:483] Algo bellman_ford step 9372 current loss 0.613811, current_train_items 299936.
I0302 19:02:45.006278 22535416901760 run.py:483] Algo bellman_ford step 9373 current loss 0.603444, current_train_items 299968.
I0302 19:02:45.040121 22535416901760 run.py:483] Algo bellman_ford step 9374 current loss 0.638579, current_train_items 300000.
I0302 19:02:45.059821 22535416901760 run.py:483] Algo bellman_ford step 9375 current loss 0.277800, current_train_items 300032.
I0302 19:02:45.075539 22535416901760 run.py:483] Algo bellman_ford step 9376 current loss 0.481761, current_train_items 300064.
I0302 19:02:45.099138 22535416901760 run.py:483] Algo bellman_ford step 9377 current loss 0.513062, current_train_items 300096.
I0302 19:02:45.129954 22535416901760 run.py:483] Algo bellman_ford step 9378 current loss 0.613910, current_train_items 300128.
I0302 19:02:45.163225 22535416901760 run.py:483] Algo bellman_ford step 9379 current loss 0.756019, current_train_items 300160.
I0302 19:02:45.182638 22535416901760 run.py:483] Algo bellman_ford step 9380 current loss 0.284855, current_train_items 300192.
I0302 19:02:45.198277 22535416901760 run.py:483] Algo bellman_ford step 9381 current loss 0.457085, current_train_items 300224.
I0302 19:02:45.221887 22535416901760 run.py:483] Algo bellman_ford step 9382 current loss 0.603602, current_train_items 300256.
I0302 19:02:45.252963 22535416901760 run.py:483] Algo bellman_ford step 9383 current loss 0.555545, current_train_items 300288.
I0302 19:02:45.286499 22535416901760 run.py:483] Algo bellman_ford step 9384 current loss 0.719495, current_train_items 300320.
I0302 19:02:45.306380 22535416901760 run.py:483] Algo bellman_ford step 9385 current loss 0.319171, current_train_items 300352.
I0302 19:02:45.322964 22535416901760 run.py:483] Algo bellman_ford step 9386 current loss 0.511284, current_train_items 300384.
I0302 19:02:45.346457 22535416901760 run.py:483] Algo bellman_ford step 9387 current loss 0.584977, current_train_items 300416.
I0302 19:02:45.377874 22535416901760 run.py:483] Algo bellman_ford step 9388 current loss 0.652853, current_train_items 300448.
I0302 19:02:45.412974 22535416901760 run.py:483] Algo bellman_ford step 9389 current loss 0.700216, current_train_items 300480.
I0302 19:02:45.433210 22535416901760 run.py:483] Algo bellman_ford step 9390 current loss 0.315448, current_train_items 300512.
I0302 19:02:45.449593 22535416901760 run.py:483] Algo bellman_ford step 9391 current loss 0.376543, current_train_items 300544.
I0302 19:02:45.472819 22535416901760 run.py:483] Algo bellman_ford step 9392 current loss 0.539665, current_train_items 300576.
I0302 19:02:45.504052 22535416901760 run.py:483] Algo bellman_ford step 9393 current loss 0.629539, current_train_items 300608.
I0302 19:02:45.537118 22535416901760 run.py:483] Algo bellman_ford step 9394 current loss 0.696043, current_train_items 300640.
I0302 19:02:45.556697 22535416901760 run.py:483] Algo bellman_ford step 9395 current loss 0.596337, current_train_items 300672.
I0302 19:02:45.572961 22535416901760 run.py:483] Algo bellman_ford step 9396 current loss 0.574890, current_train_items 300704.
I0302 19:02:45.597533 22535416901760 run.py:483] Algo bellman_ford step 9397 current loss 0.611418, current_train_items 300736.
I0302 19:02:45.628941 22535416901760 run.py:483] Algo bellman_ford step 9398 current loss 0.510801, current_train_items 300768.
I0302 19:02:45.662656 22535416901760 run.py:483] Algo bellman_ford step 9399 current loss 0.650168, current_train_items 300800.
I0302 19:02:45.682792 22535416901760 run.py:483] Algo bellman_ford step 9400 current loss 0.285792, current_train_items 300832.
I0302 19:02:45.690567 22535416901760 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.92578125, 'score': 0.92578125, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:02:45.690672 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.926, val scores are: bellman_ford: 0.926
I0302 19:02:45.707471 22535416901760 run.py:483] Algo bellman_ford step 9401 current loss 0.489717, current_train_items 300864.
I0302 19:02:45.732275 22535416901760 run.py:483] Algo bellman_ford step 9402 current loss 0.697074, current_train_items 300896.
I0302 19:02:45.764030 22535416901760 run.py:483] Algo bellman_ford step 9403 current loss 0.612408, current_train_items 300928.
I0302 19:02:45.798614 22535416901760 run.py:483] Algo bellman_ford step 9404 current loss 0.722421, current_train_items 300960.
I0302 19:02:45.818463 22535416901760 run.py:483] Algo bellman_ford step 9405 current loss 0.340949, current_train_items 300992.
I0302 19:02:45.834353 22535416901760 run.py:483] Algo bellman_ford step 9406 current loss 0.457679, current_train_items 301024.
I0302 19:02:45.859388 22535416901760 run.py:483] Algo bellman_ford step 9407 current loss 0.691056, current_train_items 301056.
I0302 19:02:45.890517 22535416901760 run.py:483] Algo bellman_ford step 9408 current loss 0.659638, current_train_items 301088.
I0302 19:02:45.925125 22535416901760 run.py:483] Algo bellman_ford step 9409 current loss 0.852455, current_train_items 301120.
I0302 19:02:45.944512 22535416901760 run.py:483] Algo bellman_ford step 9410 current loss 0.327592, current_train_items 301152.
I0302 19:02:45.960518 22535416901760 run.py:483] Algo bellman_ford step 9411 current loss 0.482550, current_train_items 301184.
I0302 19:02:45.984618 22535416901760 run.py:483] Algo bellman_ford step 9412 current loss 0.578015, current_train_items 301216.
I0302 19:02:46.016082 22535416901760 run.py:483] Algo bellman_ford step 9413 current loss 0.555330, current_train_items 301248.
I0302 19:02:46.048926 22535416901760 run.py:483] Algo bellman_ford step 9414 current loss 0.666453, current_train_items 301280.
I0302 19:02:46.068337 22535416901760 run.py:483] Algo bellman_ford step 9415 current loss 0.349508, current_train_items 301312.
I0302 19:02:46.084550 22535416901760 run.py:483] Algo bellman_ford step 9416 current loss 0.402230, current_train_items 301344.
I0302 19:02:46.108777 22535416901760 run.py:483] Algo bellman_ford step 9417 current loss 0.576859, current_train_items 301376.
I0302 19:02:46.139846 22535416901760 run.py:483] Algo bellman_ford step 9418 current loss 0.649146, current_train_items 301408.
I0302 19:02:46.171472 22535416901760 run.py:483] Algo bellman_ford step 9419 current loss 0.602269, current_train_items 301440.
I0302 19:02:46.190982 22535416901760 run.py:483] Algo bellman_ford step 9420 current loss 0.325683, current_train_items 301472.
I0302 19:02:46.207264 22535416901760 run.py:483] Algo bellman_ford step 9421 current loss 0.380512, current_train_items 301504.
I0302 19:02:46.231698 22535416901760 run.py:483] Algo bellman_ford step 9422 current loss 0.541092, current_train_items 301536.
I0302 19:02:46.263218 22535416901760 run.py:483] Algo bellman_ford step 9423 current loss 0.622044, current_train_items 301568.
I0302 19:02:46.296530 22535416901760 run.py:483] Algo bellman_ford step 9424 current loss 0.710791, current_train_items 301600.
I0302 19:02:46.316426 22535416901760 run.py:483] Algo bellman_ford step 9425 current loss 0.338988, current_train_items 301632.
I0302 19:02:46.332089 22535416901760 run.py:483] Algo bellman_ford step 9426 current loss 0.414597, current_train_items 301664.
I0302 19:02:46.355467 22535416901760 run.py:483] Algo bellman_ford step 9427 current loss 0.516405, current_train_items 301696.
I0302 19:02:46.387450 22535416901760 run.py:483] Algo bellman_ford step 9428 current loss 0.679456, current_train_items 301728.
I0302 19:02:46.418083 22535416901760 run.py:483] Algo bellman_ford step 9429 current loss 0.786224, current_train_items 301760.
I0302 19:02:46.437686 22535416901760 run.py:483] Algo bellman_ford step 9430 current loss 0.299315, current_train_items 301792.
I0302 19:02:46.453976 22535416901760 run.py:483] Algo bellman_ford step 9431 current loss 0.502845, current_train_items 301824.
I0302 19:02:46.477375 22535416901760 run.py:483] Algo bellman_ford step 9432 current loss 0.565020, current_train_items 301856.
I0302 19:02:46.508255 22535416901760 run.py:483] Algo bellman_ford step 9433 current loss 0.630716, current_train_items 301888.
I0302 19:02:46.542302 22535416901760 run.py:483] Algo bellman_ford step 9434 current loss 0.750117, current_train_items 301920.
I0302 19:02:46.562048 22535416901760 run.py:483] Algo bellman_ford step 9435 current loss 0.276154, current_train_items 301952.
I0302 19:02:46.578346 22535416901760 run.py:483] Algo bellman_ford step 9436 current loss 0.481484, current_train_items 301984.
I0302 19:02:46.602292 22535416901760 run.py:483] Algo bellman_ford step 9437 current loss 0.567688, current_train_items 302016.
I0302 19:02:46.633977 22535416901760 run.py:483] Algo bellman_ford step 9438 current loss 0.727366, current_train_items 302048.
I0302 19:02:46.666302 22535416901760 run.py:483] Algo bellman_ford step 9439 current loss 0.831495, current_train_items 302080.
I0302 19:02:46.686222 22535416901760 run.py:483] Algo bellman_ford step 9440 current loss 0.274586, current_train_items 302112.
I0302 19:02:46.702657 22535416901760 run.py:483] Algo bellman_ford step 9441 current loss 0.418904, current_train_items 302144.
I0302 19:02:46.726436 22535416901760 run.py:483] Algo bellman_ford step 9442 current loss 0.478075, current_train_items 302176.
I0302 19:02:46.758290 22535416901760 run.py:483] Algo bellman_ford step 9443 current loss 0.524266, current_train_items 302208.
I0302 19:02:46.794162 22535416901760 run.py:483] Algo bellman_ford step 9444 current loss 0.682023, current_train_items 302240.
I0302 19:02:46.813652 22535416901760 run.py:483] Algo bellman_ford step 9445 current loss 0.361707, current_train_items 302272.
I0302 19:02:46.829387 22535416901760 run.py:483] Algo bellman_ford step 9446 current loss 0.380373, current_train_items 302304.
I0302 19:02:46.852630 22535416901760 run.py:483] Algo bellman_ford step 9447 current loss 0.598946, current_train_items 302336.
I0302 19:02:46.883548 22535416901760 run.py:483] Algo bellman_ford step 9448 current loss 0.652229, current_train_items 302368.
I0302 19:02:46.915633 22535416901760 run.py:483] Algo bellman_ford step 9449 current loss 0.695189, current_train_items 302400.
I0302 19:02:46.934949 22535416901760 run.py:483] Algo bellman_ford step 9450 current loss 0.358410, current_train_items 302432.
I0302 19:02:46.943204 22535416901760 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.9052734375, 'score': 0.9052734375, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:02:46.943313 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.905, val scores are: bellman_ford: 0.905
I0302 19:02:46.960115 22535416901760 run.py:483] Algo bellman_ford step 9451 current loss 0.436160, current_train_items 302464.
I0302 19:02:46.983253 22535416901760 run.py:483] Algo bellman_ford step 9452 current loss 0.553011, current_train_items 302496.
I0302 19:02:47.015149 22535416901760 run.py:483] Algo bellman_ford step 9453 current loss 0.586543, current_train_items 302528.
I0302 19:02:47.049410 22535416901760 run.py:483] Algo bellman_ford step 9454 current loss 0.710034, current_train_items 302560.
I0302 19:02:47.069236 22535416901760 run.py:483] Algo bellman_ford step 9455 current loss 0.320649, current_train_items 302592.
I0302 19:02:47.085106 22535416901760 run.py:483] Algo bellman_ford step 9456 current loss 0.436602, current_train_items 302624.
I0302 19:02:47.109110 22535416901760 run.py:483] Algo bellman_ford step 9457 current loss 0.501072, current_train_items 302656.
I0302 19:02:47.140618 22535416901760 run.py:483] Algo bellman_ford step 9458 current loss 0.614206, current_train_items 302688.
I0302 19:02:47.172769 22535416901760 run.py:483] Algo bellman_ford step 9459 current loss 0.653510, current_train_items 302720.
I0302 19:02:47.192762 22535416901760 run.py:483] Algo bellman_ford step 9460 current loss 0.276910, current_train_items 302752.
I0302 19:02:47.209068 22535416901760 run.py:483] Algo bellman_ford step 9461 current loss 0.436655, current_train_items 302784.
I0302 19:02:47.232003 22535416901760 run.py:483] Algo bellman_ford step 9462 current loss 0.456263, current_train_items 302816.
I0302 19:02:47.264298 22535416901760 run.py:483] Algo bellman_ford step 9463 current loss 0.601276, current_train_items 302848.
I0302 19:02:47.297432 22535416901760 run.py:483] Algo bellman_ford step 9464 current loss 0.673312, current_train_items 302880.
I0302 19:02:47.316831 22535416901760 run.py:483] Algo bellman_ford step 9465 current loss 0.343663, current_train_items 302912.
I0302 19:02:47.333304 22535416901760 run.py:483] Algo bellman_ford step 9466 current loss 0.515526, current_train_items 302944.
I0302 19:02:47.358431 22535416901760 run.py:483] Algo bellman_ford step 9467 current loss 0.630477, current_train_items 302976.
I0302 19:02:47.390405 22535416901760 run.py:483] Algo bellman_ford step 9468 current loss 0.619221, current_train_items 303008.
I0302 19:02:47.424594 22535416901760 run.py:483] Algo bellman_ford step 9469 current loss 0.713652, current_train_items 303040.
I0302 19:02:47.444315 22535416901760 run.py:483] Algo bellman_ford step 9470 current loss 0.298029, current_train_items 303072.
I0302 19:02:47.460144 22535416901760 run.py:483] Algo bellman_ford step 9471 current loss 0.461682, current_train_items 303104.
I0302 19:02:47.483717 22535416901760 run.py:483] Algo bellman_ford step 9472 current loss 0.536056, current_train_items 303136.
I0302 19:02:47.515532 22535416901760 run.py:483] Algo bellman_ford step 9473 current loss 0.599533, current_train_items 303168.
I0302 19:02:47.548245 22535416901760 run.py:483] Algo bellman_ford step 9474 current loss 0.721244, current_train_items 303200.
I0302 19:02:47.567790 22535416901760 run.py:483] Algo bellman_ford step 9475 current loss 0.305704, current_train_items 303232.
I0302 19:02:47.584015 22535416901760 run.py:483] Algo bellman_ford step 9476 current loss 0.431569, current_train_items 303264.
I0302 19:02:47.607167 22535416901760 run.py:483] Algo bellman_ford step 9477 current loss 0.516029, current_train_items 303296.
I0302 19:02:47.637840 22535416901760 run.py:483] Algo bellman_ford step 9478 current loss 0.618463, current_train_items 303328.
I0302 19:02:47.670697 22535416901760 run.py:483] Algo bellman_ford step 9479 current loss 0.644112, current_train_items 303360.
I0302 19:02:47.690223 22535416901760 run.py:483] Algo bellman_ford step 9480 current loss 0.379524, current_train_items 303392.
I0302 19:02:47.706140 22535416901760 run.py:483] Algo bellman_ford step 9481 current loss 0.452038, current_train_items 303424.
I0302 19:02:47.731537 22535416901760 run.py:483] Algo bellman_ford step 9482 current loss 0.735041, current_train_items 303456.
I0302 19:02:47.762051 22535416901760 run.py:483] Algo bellman_ford step 9483 current loss 0.562927, current_train_items 303488.
I0302 19:02:47.794183 22535416901760 run.py:483] Algo bellman_ford step 9484 current loss 0.699628, current_train_items 303520.
I0302 19:02:47.813862 22535416901760 run.py:483] Algo bellman_ford step 9485 current loss 0.337030, current_train_items 303552.
I0302 19:02:47.830336 22535416901760 run.py:483] Algo bellman_ford step 9486 current loss 0.419944, current_train_items 303584.
I0302 19:02:47.854911 22535416901760 run.py:483] Algo bellman_ford step 9487 current loss 0.581021, current_train_items 303616.
I0302 19:02:47.887591 22535416901760 run.py:483] Algo bellman_ford step 9488 current loss 0.708179, current_train_items 303648.
I0302 19:02:47.919904 22535416901760 run.py:483] Algo bellman_ford step 9489 current loss 0.744154, current_train_items 303680.
I0302 19:02:47.939842 22535416901760 run.py:483] Algo bellman_ford step 9490 current loss 0.276542, current_train_items 303712.
I0302 19:02:47.955759 22535416901760 run.py:483] Algo bellman_ford step 9491 current loss 0.432602, current_train_items 303744.
I0302 19:02:47.979835 22535416901760 run.py:483] Algo bellman_ford step 9492 current loss 0.575129, current_train_items 303776.
I0302 19:02:48.010033 22535416901760 run.py:483] Algo bellman_ford step 9493 current loss 0.555709, current_train_items 303808.
I0302 19:02:48.043609 22535416901760 run.py:483] Algo bellman_ford step 9494 current loss 0.787059, current_train_items 303840.
I0302 19:02:48.063189 22535416901760 run.py:483] Algo bellman_ford step 9495 current loss 0.309456, current_train_items 303872.
I0302 19:02:48.079880 22535416901760 run.py:483] Algo bellman_ford step 9496 current loss 0.469685, current_train_items 303904.
I0302 19:02:48.103928 22535416901760 run.py:483] Algo bellman_ford step 9497 current loss 0.562275, current_train_items 303936.
I0302 19:02:48.134248 22535416901760 run.py:483] Algo bellman_ford step 9498 current loss 0.552840, current_train_items 303968.
I0302 19:02:48.168564 22535416901760 run.py:483] Algo bellman_ford step 9499 current loss 0.704745, current_train_items 304000.
I0302 19:02:48.188499 22535416901760 run.py:483] Algo bellman_ford step 9500 current loss 0.257564, current_train_items 304032.
I0302 19:02:48.196479 22535416901760 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:02:48.196587 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 19:02:48.212959 22535416901760 run.py:483] Algo bellman_ford step 9501 current loss 0.395595, current_train_items 304064.
I0302 19:02:48.237432 22535416901760 run.py:483] Algo bellman_ford step 9502 current loss 0.564593, current_train_items 304096.
I0302 19:02:48.269328 22535416901760 run.py:483] Algo bellman_ford step 9503 current loss 0.598519, current_train_items 304128.
I0302 19:02:48.302239 22535416901760 run.py:483] Algo bellman_ford step 9504 current loss 0.669301, current_train_items 304160.
I0302 19:02:48.322520 22535416901760 run.py:483] Algo bellman_ford step 9505 current loss 0.310838, current_train_items 304192.
I0302 19:02:48.338654 22535416901760 run.py:483] Algo bellman_ford step 9506 current loss 0.431557, current_train_items 304224.
I0302 19:02:48.363059 22535416901760 run.py:483] Algo bellman_ford step 9507 current loss 0.509563, current_train_items 304256.
I0302 19:02:48.393417 22535416901760 run.py:483] Algo bellman_ford step 9508 current loss 0.632752, current_train_items 304288.
I0302 19:02:48.428771 22535416901760 run.py:483] Algo bellman_ford step 9509 current loss 0.774882, current_train_items 304320.
I0302 19:02:48.448337 22535416901760 run.py:483] Algo bellman_ford step 9510 current loss 0.326818, current_train_items 304352.
I0302 19:02:48.464929 22535416901760 run.py:483] Algo bellman_ford step 9511 current loss 0.448632, current_train_items 304384.
I0302 19:02:48.489119 22535416901760 run.py:483] Algo bellman_ford step 9512 current loss 0.611185, current_train_items 304416.
I0302 19:02:48.520970 22535416901760 run.py:483] Algo bellman_ford step 9513 current loss 0.628632, current_train_items 304448.
I0302 19:02:48.553467 22535416901760 run.py:483] Algo bellman_ford step 9514 current loss 0.646247, current_train_items 304480.
I0302 19:02:48.572983 22535416901760 run.py:483] Algo bellman_ford step 9515 current loss 0.325451, current_train_items 304512.
I0302 19:02:48.588738 22535416901760 run.py:483] Algo bellman_ford step 9516 current loss 0.406662, current_train_items 304544.
I0302 19:02:48.612525 22535416901760 run.py:483] Algo bellman_ford step 9517 current loss 0.579186, current_train_items 304576.
I0302 19:02:48.644032 22535416901760 run.py:483] Algo bellman_ford step 9518 current loss 0.638733, current_train_items 304608.
I0302 19:02:48.676073 22535416901760 run.py:483] Algo bellman_ford step 9519 current loss 0.617356, current_train_items 304640.
I0302 19:02:48.695958 22535416901760 run.py:483] Algo bellman_ford step 9520 current loss 0.306661, current_train_items 304672.
I0302 19:02:48.711307 22535416901760 run.py:483] Algo bellman_ford step 9521 current loss 0.414373, current_train_items 304704.
I0302 19:02:48.734446 22535416901760 run.py:483] Algo bellman_ford step 9522 current loss 0.551676, current_train_items 304736.
I0302 19:02:48.764785 22535416901760 run.py:483] Algo bellman_ford step 9523 current loss 0.543305, current_train_items 304768.
I0302 19:02:48.797585 22535416901760 run.py:483] Algo bellman_ford step 9524 current loss 0.618287, current_train_items 304800.
I0302 19:02:48.816735 22535416901760 run.py:483] Algo bellman_ford step 9525 current loss 0.229735, current_train_items 304832.
I0302 19:02:48.832855 22535416901760 run.py:483] Algo bellman_ford step 9526 current loss 0.410808, current_train_items 304864.
I0302 19:02:48.857127 22535416901760 run.py:483] Algo bellman_ford step 9527 current loss 0.698609, current_train_items 304896.
I0302 19:02:48.889040 22535416901760 run.py:483] Algo bellman_ford step 9528 current loss 0.674031, current_train_items 304928.
I0302 19:02:48.923543 22535416901760 run.py:483] Algo bellman_ford step 9529 current loss 0.704572, current_train_items 304960.
I0302 19:02:48.943388 22535416901760 run.py:483] Algo bellman_ford step 9530 current loss 0.292721, current_train_items 304992.
I0302 19:02:48.959451 22535416901760 run.py:483] Algo bellman_ford step 9531 current loss 0.408685, current_train_items 305024.
I0302 19:02:48.983278 22535416901760 run.py:483] Algo bellman_ford step 9532 current loss 0.540250, current_train_items 305056.
I0302 19:02:49.016273 22535416901760 run.py:483] Algo bellman_ford step 9533 current loss 0.649936, current_train_items 305088.
I0302 19:02:49.049575 22535416901760 run.py:483] Algo bellman_ford step 9534 current loss 0.604561, current_train_items 305120.
I0302 19:02:49.069451 22535416901760 run.py:483] Algo bellman_ford step 9535 current loss 0.311338, current_train_items 305152.
I0302 19:02:49.085577 22535416901760 run.py:483] Algo bellman_ford step 9536 current loss 0.474226, current_train_items 305184.
I0302 19:02:49.110085 22535416901760 run.py:483] Algo bellman_ford step 9537 current loss 0.537132, current_train_items 305216.
I0302 19:02:49.143013 22535416901760 run.py:483] Algo bellman_ford step 9538 current loss 0.616003, current_train_items 305248.
I0302 19:02:49.174682 22535416901760 run.py:483] Algo bellman_ford step 9539 current loss 0.675939, current_train_items 305280.
I0302 19:02:49.194462 22535416901760 run.py:483] Algo bellman_ford step 9540 current loss 0.300161, current_train_items 305312.
I0302 19:02:49.210835 22535416901760 run.py:483] Algo bellman_ford step 9541 current loss 0.442585, current_train_items 305344.
I0302 19:02:49.234292 22535416901760 run.py:483] Algo bellman_ford step 9542 current loss 0.537926, current_train_items 305376.
I0302 19:02:49.267363 22535416901760 run.py:483] Algo bellman_ford step 9543 current loss 0.655799, current_train_items 305408.
I0302 19:02:49.301437 22535416901760 run.py:483] Algo bellman_ford step 9544 current loss 0.692075, current_train_items 305440.
I0302 19:02:49.321644 22535416901760 run.py:483] Algo bellman_ford step 9545 current loss 0.289686, current_train_items 305472.
I0302 19:02:49.337360 22535416901760 run.py:483] Algo bellman_ford step 9546 current loss 0.489092, current_train_items 305504.
I0302 19:02:49.359275 22535416901760 run.py:483] Algo bellman_ford step 9547 current loss 0.549508, current_train_items 305536.
I0302 19:02:49.390520 22535416901760 run.py:483] Algo bellman_ford step 9548 current loss 0.579925, current_train_items 305568.
I0302 19:02:49.424333 22535416901760 run.py:483] Algo bellman_ford step 9549 current loss 0.651999, current_train_items 305600.
I0302 19:02:49.443799 22535416901760 run.py:483] Algo bellman_ford step 9550 current loss 0.253822, current_train_items 305632.
I0302 19:02:49.451863 22535416901760 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:02:49.451971 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:02:49.468684 22535416901760 run.py:483] Algo bellman_ford step 9551 current loss 0.484609, current_train_items 305664.
I0302 19:02:49.493008 22535416901760 run.py:483] Algo bellman_ford step 9552 current loss 0.616220, current_train_items 305696.
I0302 19:02:49.526337 22535416901760 run.py:483] Algo bellman_ford step 9553 current loss 0.640472, current_train_items 305728.
I0302 19:02:49.559164 22535416901760 run.py:483] Algo bellman_ford step 9554 current loss 0.648594, current_train_items 305760.
I0302 19:02:49.578924 22535416901760 run.py:483] Algo bellman_ford step 9555 current loss 0.302743, current_train_items 305792.
I0302 19:02:49.594250 22535416901760 run.py:483] Algo bellman_ford step 9556 current loss 0.463058, current_train_items 305824.
I0302 19:02:49.618281 22535416901760 run.py:483] Algo bellman_ford step 9557 current loss 0.560805, current_train_items 305856.
I0302 19:02:49.649263 22535416901760 run.py:483] Algo bellman_ford step 9558 current loss 0.711244, current_train_items 305888.
I0302 19:02:49.681760 22535416901760 run.py:483] Algo bellman_ford step 9559 current loss 0.735771, current_train_items 305920.
I0302 19:02:49.701515 22535416901760 run.py:483] Algo bellman_ford step 9560 current loss 0.321073, current_train_items 305952.
I0302 19:02:49.717642 22535416901760 run.py:483] Algo bellman_ford step 9561 current loss 0.461407, current_train_items 305984.
I0302 19:02:49.740691 22535416901760 run.py:483] Algo bellman_ford step 9562 current loss 0.511335, current_train_items 306016.
I0302 19:02:49.771627 22535416901760 run.py:483] Algo bellman_ford step 9563 current loss 0.610081, current_train_items 306048.
I0302 19:02:49.804319 22535416901760 run.py:483] Algo bellman_ford step 9564 current loss 0.639785, current_train_items 306080.
I0302 19:02:49.823805 22535416901760 run.py:483] Algo bellman_ford step 9565 current loss 0.365083, current_train_items 306112.
I0302 19:02:49.840293 22535416901760 run.py:483] Algo bellman_ford step 9566 current loss 0.467302, current_train_items 306144.
I0302 19:02:49.864873 22535416901760 run.py:483] Algo bellman_ford step 9567 current loss 0.667748, current_train_items 306176.
I0302 19:02:49.896642 22535416901760 run.py:483] Algo bellman_ford step 9568 current loss 0.753564, current_train_items 306208.
I0302 19:02:49.929878 22535416901760 run.py:483] Algo bellman_ford step 9569 current loss 0.644448, current_train_items 306240.
I0302 19:02:49.949565 22535416901760 run.py:483] Algo bellman_ford step 9570 current loss 0.366979, current_train_items 306272.
I0302 19:02:49.965702 22535416901760 run.py:483] Algo bellman_ford step 9571 current loss 0.449876, current_train_items 306304.
I0302 19:02:49.990139 22535416901760 run.py:483] Algo bellman_ford step 9572 current loss 0.597162, current_train_items 306336.
I0302 19:02:50.020655 22535416901760 run.py:483] Algo bellman_ford step 9573 current loss 0.536158, current_train_items 306368.
I0302 19:02:50.054193 22535416901760 run.py:483] Algo bellman_ford step 9574 current loss 0.796877, current_train_items 306400.
I0302 19:02:50.074080 22535416901760 run.py:483] Algo bellman_ford step 9575 current loss 0.309878, current_train_items 306432.
I0302 19:02:50.090876 22535416901760 run.py:483] Algo bellman_ford step 9576 current loss 0.407142, current_train_items 306464.
I0302 19:02:50.115140 22535416901760 run.py:483] Algo bellman_ford step 9577 current loss 0.675957, current_train_items 306496.
I0302 19:02:50.146071 22535416901760 run.py:483] Algo bellman_ford step 9578 current loss 0.525210, current_train_items 306528.
I0302 19:02:50.179558 22535416901760 run.py:483] Algo bellman_ford step 9579 current loss 0.619447, current_train_items 306560.
I0302 19:02:50.199113 22535416901760 run.py:483] Algo bellman_ford step 9580 current loss 0.385966, current_train_items 306592.
I0302 19:02:50.214970 22535416901760 run.py:483] Algo bellman_ford step 9581 current loss 0.497846, current_train_items 306624.
I0302 19:02:50.239933 22535416901760 run.py:483] Algo bellman_ford step 9582 current loss 0.555481, current_train_items 306656.
I0302 19:02:50.272451 22535416901760 run.py:483] Algo bellman_ford step 9583 current loss 0.648873, current_train_items 306688.
I0302 19:02:50.306532 22535416901760 run.py:483] Algo bellman_ford step 9584 current loss 0.658388, current_train_items 306720.
I0302 19:02:50.326626 22535416901760 run.py:483] Algo bellman_ford step 9585 current loss 0.343973, current_train_items 306752.
I0302 19:02:50.342948 22535416901760 run.py:483] Algo bellman_ford step 9586 current loss 0.409572, current_train_items 306784.
I0302 19:02:50.366666 22535416901760 run.py:483] Algo bellman_ford step 9587 current loss 0.531873, current_train_items 306816.
I0302 19:02:50.398493 22535416901760 run.py:483] Algo bellman_ford step 9588 current loss 0.635739, current_train_items 306848.
I0302 19:02:50.433032 22535416901760 run.py:483] Algo bellman_ford step 9589 current loss 0.804189, current_train_items 306880.
I0302 19:02:50.452618 22535416901760 run.py:483] Algo bellman_ford step 9590 current loss 0.241778, current_train_items 306912.
I0302 19:02:50.468800 22535416901760 run.py:483] Algo bellman_ford step 9591 current loss 0.387520, current_train_items 306944.
I0302 19:02:50.492300 22535416901760 run.py:483] Algo bellman_ford step 9592 current loss 0.606920, current_train_items 306976.
I0302 19:02:50.523832 22535416901760 run.py:483] Algo bellman_ford step 9593 current loss 0.639402, current_train_items 307008.
I0302 19:02:50.556543 22535416901760 run.py:483] Algo bellman_ford step 9594 current loss 0.705541, current_train_items 307040.
I0302 19:02:50.576165 22535416901760 run.py:483] Algo bellman_ford step 9595 current loss 0.321753, current_train_items 307072.
I0302 19:02:50.592478 22535416901760 run.py:483] Algo bellman_ford step 9596 current loss 0.501653, current_train_items 307104.
I0302 19:02:50.616719 22535416901760 run.py:483] Algo bellman_ford step 9597 current loss 0.587649, current_train_items 307136.
I0302 19:02:50.646916 22535416901760 run.py:483] Algo bellman_ford step 9598 current loss 0.544774, current_train_items 307168.
I0302 19:02:50.680023 22535416901760 run.py:483] Algo bellman_ford step 9599 current loss 0.668423, current_train_items 307200.
I0302 19:02:50.699856 22535416901760 run.py:483] Algo bellman_ford step 9600 current loss 0.286667, current_train_items 307232.
I0302 19:02:50.707582 22535416901760 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:02:50.707690 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 19:02:50.724881 22535416901760 run.py:483] Algo bellman_ford step 9601 current loss 0.428944, current_train_items 307264.
I0302 19:02:50.749948 22535416901760 run.py:483] Algo bellman_ford step 9602 current loss 0.508390, current_train_items 307296.
I0302 19:02:50.781343 22535416901760 run.py:483] Algo bellman_ford step 9603 current loss 0.549716, current_train_items 307328.
I0302 19:02:50.816437 22535416901760 run.py:483] Algo bellman_ford step 9604 current loss 0.686255, current_train_items 307360.
I0302 19:02:50.836411 22535416901760 run.py:483] Algo bellman_ford step 9605 current loss 0.313233, current_train_items 307392.
I0302 19:02:50.852213 22535416901760 run.py:483] Algo bellman_ford step 9606 current loss 0.417563, current_train_items 307424.
I0302 19:02:50.876227 22535416901760 run.py:483] Algo bellman_ford step 9607 current loss 0.525299, current_train_items 307456.
I0302 19:02:50.907272 22535416901760 run.py:483] Algo bellman_ford step 9608 current loss 0.584592, current_train_items 307488.
I0302 19:02:50.940751 22535416901760 run.py:483] Algo bellman_ford step 9609 current loss 0.678211, current_train_items 307520.
I0302 19:02:50.960471 22535416901760 run.py:483] Algo bellman_ford step 9610 current loss 0.295535, current_train_items 307552.
I0302 19:02:50.976628 22535416901760 run.py:483] Algo bellman_ford step 9611 current loss 0.458254, current_train_items 307584.
I0302 19:02:51.001191 22535416901760 run.py:483] Algo bellman_ford step 9612 current loss 0.594883, current_train_items 307616.
I0302 19:02:51.032645 22535416901760 run.py:483] Algo bellman_ford step 9613 current loss 0.745360, current_train_items 307648.
I0302 19:02:51.066490 22535416901760 run.py:483] Algo bellman_ford step 9614 current loss 0.697717, current_train_items 307680.
I0302 19:02:51.086116 22535416901760 run.py:483] Algo bellman_ford step 9615 current loss 0.320055, current_train_items 307712.
I0302 19:02:51.102253 22535416901760 run.py:483] Algo bellman_ford step 9616 current loss 0.497395, current_train_items 307744.
I0302 19:02:51.125931 22535416901760 run.py:483] Algo bellman_ford step 9617 current loss 0.694486, current_train_items 307776.
I0302 19:02:51.157451 22535416901760 run.py:483] Algo bellman_ford step 9618 current loss 0.658682, current_train_items 307808.
I0302 19:02:51.191425 22535416901760 run.py:483] Algo bellman_ford step 9619 current loss 0.884063, current_train_items 307840.
I0302 19:02:51.210833 22535416901760 run.py:483] Algo bellman_ford step 9620 current loss 0.316461, current_train_items 307872.
I0302 19:02:51.226773 22535416901760 run.py:483] Algo bellman_ford step 9621 current loss 0.385735, current_train_items 307904.
I0302 19:02:51.250909 22535416901760 run.py:483] Algo bellman_ford step 9622 current loss 0.622803, current_train_items 307936.
I0302 19:02:51.282453 22535416901760 run.py:483] Algo bellman_ford step 9623 current loss 0.600657, current_train_items 307968.
I0302 19:02:51.316469 22535416901760 run.py:483] Algo bellman_ford step 9624 current loss 0.704387, current_train_items 308000.
I0302 19:02:51.336131 22535416901760 run.py:483] Algo bellman_ford step 9625 current loss 0.340953, current_train_items 308032.
I0302 19:02:51.352277 22535416901760 run.py:483] Algo bellman_ford step 9626 current loss 0.483538, current_train_items 308064.
I0302 19:02:51.375912 22535416901760 run.py:483] Algo bellman_ford step 9627 current loss 0.592824, current_train_items 308096.
I0302 19:02:51.406968 22535416901760 run.py:483] Algo bellman_ford step 9628 current loss 0.625458, current_train_items 308128.
I0302 19:02:51.441338 22535416901760 run.py:483] Algo bellman_ford step 9629 current loss 0.886036, current_train_items 308160.
I0302 19:02:51.461225 22535416901760 run.py:483] Algo bellman_ford step 9630 current loss 0.280743, current_train_items 308192.
I0302 19:02:51.477766 22535416901760 run.py:483] Algo bellman_ford step 9631 current loss 0.408682, current_train_items 308224.
I0302 19:02:51.502730 22535416901760 run.py:483] Algo bellman_ford step 9632 current loss 0.554810, current_train_items 308256.
I0302 19:02:51.533861 22535416901760 run.py:483] Algo bellman_ford step 9633 current loss 0.547757, current_train_items 308288.
I0302 19:02:51.567970 22535416901760 run.py:483] Algo bellman_ford step 9634 current loss 0.769139, current_train_items 308320.
I0302 19:02:51.587704 22535416901760 run.py:483] Algo bellman_ford step 9635 current loss 0.304433, current_train_items 308352.
I0302 19:02:51.603745 22535416901760 run.py:483] Algo bellman_ford step 9636 current loss 0.420363, current_train_items 308384.
I0302 19:02:51.627749 22535416901760 run.py:483] Algo bellman_ford step 9637 current loss 0.575195, current_train_items 308416.
I0302 19:02:51.659675 22535416901760 run.py:483] Algo bellman_ford step 9638 current loss 0.637946, current_train_items 308448.
I0302 19:02:51.692784 22535416901760 run.py:483] Algo bellman_ford step 9639 current loss 0.727413, current_train_items 308480.
I0302 19:02:51.712772 22535416901760 run.py:483] Algo bellman_ford step 9640 current loss 0.243859, current_train_items 308512.
I0302 19:02:51.728581 22535416901760 run.py:483] Algo bellman_ford step 9641 current loss 0.378308, current_train_items 308544.
I0302 19:02:51.752817 22535416901760 run.py:483] Algo bellman_ford step 9642 current loss 0.523537, current_train_items 308576.
I0302 19:02:51.785721 22535416901760 run.py:483] Algo bellman_ford step 9643 current loss 0.644097, current_train_items 308608.
I0302 19:02:51.817171 22535416901760 run.py:483] Algo bellman_ford step 9644 current loss 0.825768, current_train_items 308640.
I0302 19:02:51.836624 22535416901760 run.py:483] Algo bellman_ford step 9645 current loss 0.261304, current_train_items 308672.
I0302 19:02:51.852838 22535416901760 run.py:483] Algo bellman_ford step 9646 current loss 0.527688, current_train_items 308704.
I0302 19:02:51.876073 22535416901760 run.py:483] Algo bellman_ford step 9647 current loss 0.517543, current_train_items 308736.
I0302 19:02:51.908261 22535416901760 run.py:483] Algo bellman_ford step 9648 current loss 0.608454, current_train_items 308768.
I0302 19:02:51.942168 22535416901760 run.py:483] Algo bellman_ford step 9649 current loss 0.700664, current_train_items 308800.
I0302 19:02:51.961446 22535416901760 run.py:483] Algo bellman_ford step 9650 current loss 0.402802, current_train_items 308832.
I0302 19:02:51.969585 22535416901760 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9384765625, 'score': 0.9384765625, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:51.969724 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.938, val scores are: bellman_ford: 0.938
I0302 19:02:51.986626 22535416901760 run.py:483] Algo bellman_ford step 9651 current loss 0.429800, current_train_items 308864.
I0302 19:02:52.012163 22535416901760 run.py:483] Algo bellman_ford step 9652 current loss 0.629639, current_train_items 308896.
I0302 19:02:52.044249 22535416901760 run.py:483] Algo bellman_ford step 9653 current loss 0.685770, current_train_items 308928.
I0302 19:02:52.079948 22535416901760 run.py:483] Algo bellman_ford step 9654 current loss 0.740228, current_train_items 308960.
I0302 19:02:52.100007 22535416901760 run.py:483] Algo bellman_ford step 9655 current loss 0.323955, current_train_items 308992.
I0302 19:02:52.115989 22535416901760 run.py:483] Algo bellman_ford step 9656 current loss 0.441811, current_train_items 309024.
I0302 19:02:52.140108 22535416901760 run.py:483] Algo bellman_ford step 9657 current loss 0.504090, current_train_items 309056.
I0302 19:02:52.172801 22535416901760 run.py:483] Algo bellman_ford step 9658 current loss 0.543431, current_train_items 309088.
I0302 19:02:52.204739 22535416901760 run.py:483] Algo bellman_ford step 9659 current loss 0.647124, current_train_items 309120.
I0302 19:02:52.224423 22535416901760 run.py:483] Algo bellman_ford step 9660 current loss 0.301846, current_train_items 309152.
I0302 19:02:52.240955 22535416901760 run.py:483] Algo bellman_ford step 9661 current loss 0.444532, current_train_items 309184.
I0302 19:02:52.264522 22535416901760 run.py:483] Algo bellman_ford step 9662 current loss 0.606379, current_train_items 309216.
I0302 19:02:52.295873 22535416901760 run.py:483] Algo bellman_ford step 9663 current loss 0.610341, current_train_items 309248.
I0302 19:02:52.329546 22535416901760 run.py:483] Algo bellman_ford step 9664 current loss 0.741069, current_train_items 309280.
I0302 19:02:52.349313 22535416901760 run.py:483] Algo bellman_ford step 9665 current loss 0.328400, current_train_items 309312.
I0302 19:02:52.365475 22535416901760 run.py:483] Algo bellman_ford step 9666 current loss 0.404341, current_train_items 309344.
I0302 19:02:52.388533 22535416901760 run.py:483] Algo bellman_ford step 9667 current loss 0.586113, current_train_items 309376.
I0302 19:02:52.419955 22535416901760 run.py:483] Algo bellman_ford step 9668 current loss 0.574134, current_train_items 309408.
I0302 19:02:52.453241 22535416901760 run.py:483] Algo bellman_ford step 9669 current loss 0.718699, current_train_items 309440.
I0302 19:02:52.473386 22535416901760 run.py:483] Algo bellman_ford step 9670 current loss 0.269945, current_train_items 309472.
I0302 19:02:52.489528 22535416901760 run.py:483] Algo bellman_ford step 9671 current loss 0.387613, current_train_items 309504.
I0302 19:02:52.513213 22535416901760 run.py:483] Algo bellman_ford step 9672 current loss 0.495767, current_train_items 309536.
I0302 19:02:52.544131 22535416901760 run.py:483] Algo bellman_ford step 9673 current loss 0.515954, current_train_items 309568.
I0302 19:02:52.577107 22535416901760 run.py:483] Algo bellman_ford step 9674 current loss 0.635594, current_train_items 309600.
I0302 19:02:52.596916 22535416901760 run.py:483] Algo bellman_ford step 9675 current loss 0.378095, current_train_items 309632.
I0302 19:02:52.613463 22535416901760 run.py:483] Algo bellman_ford step 9676 current loss 0.410867, current_train_items 309664.
I0302 19:02:52.637268 22535416901760 run.py:483] Algo bellman_ford step 9677 current loss 0.472897, current_train_items 309696.
I0302 19:02:52.667497 22535416901760 run.py:483] Algo bellman_ford step 9678 current loss 0.534186, current_train_items 309728.
I0302 19:02:52.701579 22535416901760 run.py:483] Algo bellman_ford step 9679 current loss 0.666126, current_train_items 309760.
I0302 19:02:52.721139 22535416901760 run.py:483] Algo bellman_ford step 9680 current loss 0.269256, current_train_items 309792.
I0302 19:02:52.737540 22535416901760 run.py:483] Algo bellman_ford step 9681 current loss 0.396114, current_train_items 309824.
I0302 19:02:52.760824 22535416901760 run.py:483] Algo bellman_ford step 9682 current loss 0.438492, current_train_items 309856.
I0302 19:02:52.791507 22535416901760 run.py:483] Algo bellman_ford step 9683 current loss 0.520969, current_train_items 309888.
I0302 19:02:52.823943 22535416901760 run.py:483] Algo bellman_ford step 9684 current loss 0.619899, current_train_items 309920.
I0302 19:02:52.844035 22535416901760 run.py:483] Algo bellman_ford step 9685 current loss 0.297111, current_train_items 309952.
I0302 19:02:52.860169 22535416901760 run.py:483] Algo bellman_ford step 9686 current loss 0.510085, current_train_items 309984.
I0302 19:02:52.883775 22535416901760 run.py:483] Algo bellman_ford step 9687 current loss 0.534869, current_train_items 310016.
I0302 19:02:52.915008 22535416901760 run.py:483] Algo bellman_ford step 9688 current loss 0.742192, current_train_items 310048.
I0302 19:02:52.948549 22535416901760 run.py:483] Algo bellman_ford step 9689 current loss 0.707790, current_train_items 310080.
I0302 19:02:52.968726 22535416901760 run.py:483] Algo bellman_ford step 9690 current loss 0.272010, current_train_items 310112.
I0302 19:02:52.984509 22535416901760 run.py:483] Algo bellman_ford step 9691 current loss 0.394586, current_train_items 310144.
I0302 19:02:53.008287 22535416901760 run.py:483] Algo bellman_ford step 9692 current loss 0.467682, current_train_items 310176.
I0302 19:02:53.038869 22535416901760 run.py:483] Algo bellman_ford step 9693 current loss 0.591664, current_train_items 310208.
I0302 19:02:53.072519 22535416901760 run.py:483] Algo bellman_ford step 9694 current loss 0.649639, current_train_items 310240.
I0302 19:02:53.092138 22535416901760 run.py:483] Algo bellman_ford step 9695 current loss 0.309476, current_train_items 310272.
I0302 19:02:53.108294 22535416901760 run.py:483] Algo bellman_ford step 9696 current loss 0.503649, current_train_items 310304.
I0302 19:02:53.131263 22535416901760 run.py:483] Algo bellman_ford step 9697 current loss 0.458566, current_train_items 310336.
I0302 19:02:53.165011 22535416901760 run.py:483] Algo bellman_ford step 9698 current loss 0.665269, current_train_items 310368.
I0302 19:02:53.200080 22535416901760 run.py:483] Algo bellman_ford step 9699 current loss 0.708132, current_train_items 310400.
I0302 19:02:53.220162 22535416901760 run.py:483] Algo bellman_ford step 9700 current loss 0.298566, current_train_items 310432.
I0302 19:02:53.228062 22535416901760 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:53.228178 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:02:53.244946 22535416901760 run.py:483] Algo bellman_ford step 9701 current loss 0.486350, current_train_items 310464.
I0302 19:02:53.269913 22535416901760 run.py:483] Algo bellman_ford step 9702 current loss 0.551947, current_train_items 310496.
I0302 19:02:53.302659 22535416901760 run.py:483] Algo bellman_ford step 9703 current loss 0.641701, current_train_items 310528.
I0302 19:02:53.336214 22535416901760 run.py:483] Algo bellman_ford step 9704 current loss 0.674463, current_train_items 310560.
I0302 19:02:53.356388 22535416901760 run.py:483] Algo bellman_ford step 9705 current loss 0.272072, current_train_items 310592.
I0302 19:02:53.372308 22535416901760 run.py:483] Algo bellman_ford step 9706 current loss 0.406719, current_train_items 310624.
I0302 19:02:53.397255 22535416901760 run.py:483] Algo bellman_ford step 9707 current loss 0.609241, current_train_items 310656.
I0302 19:02:53.429773 22535416901760 run.py:483] Algo bellman_ford step 9708 current loss 0.648278, current_train_items 310688.
I0302 19:02:53.461305 22535416901760 run.py:483] Algo bellman_ford step 9709 current loss 0.688208, current_train_items 310720.
I0302 19:02:53.481187 22535416901760 run.py:483] Algo bellman_ford step 9710 current loss 0.267040, current_train_items 310752.
I0302 19:02:53.497050 22535416901760 run.py:483] Algo bellman_ford step 9711 current loss 0.411671, current_train_items 310784.
I0302 19:02:53.520781 22535416901760 run.py:483] Algo bellman_ford step 9712 current loss 0.631485, current_train_items 310816.
I0302 19:02:53.552862 22535416901760 run.py:483] Algo bellman_ford step 9713 current loss 0.522445, current_train_items 310848.
I0302 19:02:53.586868 22535416901760 run.py:483] Algo bellman_ford step 9714 current loss 0.705789, current_train_items 310880.
I0302 19:02:53.606235 22535416901760 run.py:483] Algo bellman_ford step 9715 current loss 0.323156, current_train_items 310912.
I0302 19:02:53.622715 22535416901760 run.py:483] Algo bellman_ford step 9716 current loss 0.425920, current_train_items 310944.
I0302 19:02:53.645606 22535416901760 run.py:483] Algo bellman_ford step 9717 current loss 0.608455, current_train_items 310976.
I0302 19:02:53.677401 22535416901760 run.py:483] Algo bellman_ford step 9718 current loss 0.680343, current_train_items 311008.
I0302 19:02:53.708082 22535416901760 run.py:483] Algo bellman_ford step 9719 current loss 0.551195, current_train_items 311040.
I0302 19:02:53.727654 22535416901760 run.py:483] Algo bellman_ford step 9720 current loss 0.302149, current_train_items 311072.
I0302 19:02:53.743291 22535416901760 run.py:483] Algo bellman_ford step 9721 current loss 0.391165, current_train_items 311104.
I0302 19:02:53.766339 22535416901760 run.py:483] Algo bellman_ford step 9722 current loss 0.578903, current_train_items 311136.
I0302 19:02:53.797913 22535416901760 run.py:483] Algo bellman_ford step 9723 current loss 0.668508, current_train_items 311168.
I0302 19:02:53.831760 22535416901760 run.py:483] Algo bellman_ford step 9724 current loss 0.661952, current_train_items 311200.
I0302 19:02:53.851240 22535416901760 run.py:483] Algo bellman_ford step 9725 current loss 0.287017, current_train_items 311232.
I0302 19:02:53.867233 22535416901760 run.py:483] Algo bellman_ford step 9726 current loss 0.471285, current_train_items 311264.
I0302 19:02:53.890339 22535416901760 run.py:483] Algo bellman_ford step 9727 current loss 0.479644, current_train_items 311296.
I0302 19:02:53.922183 22535416901760 run.py:483] Algo bellman_ford step 9728 current loss 0.613471, current_train_items 311328.
I0302 19:02:53.956495 22535416901760 run.py:483] Algo bellman_ford step 9729 current loss 0.701502, current_train_items 311360.
I0302 19:02:53.976065 22535416901760 run.py:483] Algo bellman_ford step 9730 current loss 0.307723, current_train_items 311392.
I0302 19:02:53.992615 22535416901760 run.py:483] Algo bellman_ford step 9731 current loss 0.424135, current_train_items 311424.
I0302 19:02:54.016078 22535416901760 run.py:483] Algo bellman_ford step 9732 current loss 0.564257, current_train_items 311456.
I0302 19:02:54.047075 22535416901760 run.py:483] Algo bellman_ford step 9733 current loss 0.596595, current_train_items 311488.
I0302 19:02:54.079336 22535416901760 run.py:483] Algo bellman_ford step 9734 current loss 0.725693, current_train_items 311520.
I0302 19:02:54.098950 22535416901760 run.py:483] Algo bellman_ford step 9735 current loss 0.304366, current_train_items 311552.
I0302 19:02:54.115070 22535416901760 run.py:483] Algo bellman_ford step 9736 current loss 0.503211, current_train_items 311584.
I0302 19:02:54.138331 22535416901760 run.py:483] Algo bellman_ford step 9737 current loss 0.687738, current_train_items 311616.
I0302 19:02:54.171322 22535416901760 run.py:483] Algo bellman_ford step 9738 current loss 0.661597, current_train_items 311648.
I0302 19:02:54.205534 22535416901760 run.py:483] Algo bellman_ford step 9739 current loss 0.675098, current_train_items 311680.
I0302 19:02:54.224977 22535416901760 run.py:483] Algo bellman_ford step 9740 current loss 0.301100, current_train_items 311712.
I0302 19:02:54.241822 22535416901760 run.py:483] Algo bellman_ford step 9741 current loss 0.379407, current_train_items 311744.
I0302 19:02:54.267033 22535416901760 run.py:483] Algo bellman_ford step 9742 current loss 0.647816, current_train_items 311776.
I0302 19:02:54.299337 22535416901760 run.py:483] Algo bellman_ford step 9743 current loss 0.648657, current_train_items 311808.
I0302 19:02:54.333265 22535416901760 run.py:483] Algo bellman_ford step 9744 current loss 0.724981, current_train_items 311840.
I0302 19:02:54.353023 22535416901760 run.py:483] Algo bellman_ford step 9745 current loss 0.265268, current_train_items 311872.
I0302 19:02:54.369764 22535416901760 run.py:483] Algo bellman_ford step 9746 current loss 0.414452, current_train_items 311904.
I0302 19:02:54.393471 22535416901760 run.py:483] Algo bellman_ford step 9747 current loss 0.566663, current_train_items 311936.
I0302 19:02:54.423104 22535416901760 run.py:483] Algo bellman_ford step 9748 current loss 0.558960, current_train_items 311968.
I0302 19:02:54.455201 22535416901760 run.py:483] Algo bellman_ford step 9749 current loss 0.823377, current_train_items 312000.
I0302 19:02:54.474670 22535416901760 run.py:483] Algo bellman_ford step 9750 current loss 0.287498, current_train_items 312032.
I0302 19:02:54.482822 22535416901760 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.921875, 'score': 0.921875, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:54.482929 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.922, val scores are: bellman_ford: 0.922
I0302 19:02:54.499464 22535416901760 run.py:483] Algo bellman_ford step 9751 current loss 0.524282, current_train_items 312064.
I0302 19:02:54.525015 22535416901760 run.py:483] Algo bellman_ford step 9752 current loss 0.642855, current_train_items 312096.
I0302 19:02:54.556331 22535416901760 run.py:483] Algo bellman_ford step 9753 current loss 0.670623, current_train_items 312128.
I0302 19:02:54.589365 22535416901760 run.py:483] Algo bellman_ford step 9754 current loss 0.645389, current_train_items 312160.
I0302 19:02:54.609159 22535416901760 run.py:483] Algo bellman_ford step 9755 current loss 0.366505, current_train_items 312192.
I0302 19:02:54.625641 22535416901760 run.py:483] Algo bellman_ford step 9756 current loss 0.526788, current_train_items 312224.
I0302 19:02:54.648490 22535416901760 run.py:483] Algo bellman_ford step 9757 current loss 0.459290, current_train_items 312256.
I0302 19:02:54.680615 22535416901760 run.py:483] Algo bellman_ford step 9758 current loss 0.643247, current_train_items 312288.
I0302 19:02:54.714106 22535416901760 run.py:483] Algo bellman_ford step 9759 current loss 0.712775, current_train_items 312320.
I0302 19:02:54.734250 22535416901760 run.py:483] Algo bellman_ford step 9760 current loss 0.338109, current_train_items 312352.
I0302 19:02:54.750853 22535416901760 run.py:483] Algo bellman_ford step 9761 current loss 0.495915, current_train_items 312384.
I0302 19:02:54.775071 22535416901760 run.py:483] Algo bellman_ford step 9762 current loss 0.652604, current_train_items 312416.
I0302 19:02:54.807259 22535416901760 run.py:483] Algo bellman_ford step 9763 current loss 0.601397, current_train_items 312448.
I0302 19:02:54.838054 22535416901760 run.py:483] Algo bellman_ford step 9764 current loss 0.665044, current_train_items 312480.
I0302 19:02:54.857695 22535416901760 run.py:483] Algo bellman_ford step 9765 current loss 0.289133, current_train_items 312512.
I0302 19:02:54.873505 22535416901760 run.py:483] Algo bellman_ford step 9766 current loss 0.457223, current_train_items 312544.
I0302 19:02:54.897431 22535416901760 run.py:483] Algo bellman_ford step 9767 current loss 0.680793, current_train_items 312576.
I0302 19:02:54.928642 22535416901760 run.py:483] Algo bellman_ford step 9768 current loss 0.635778, current_train_items 312608.
I0302 19:02:54.962591 22535416901760 run.py:483] Algo bellman_ford step 9769 current loss 0.700971, current_train_items 312640.
I0302 19:02:54.982653 22535416901760 run.py:483] Algo bellman_ford step 9770 current loss 0.284681, current_train_items 312672.
I0302 19:02:54.998587 22535416901760 run.py:483] Algo bellman_ford step 9771 current loss 0.490363, current_train_items 312704.
I0302 19:02:55.021714 22535416901760 run.py:483] Algo bellman_ford step 9772 current loss 0.602782, current_train_items 312736.
I0302 19:02:55.054133 22535416901760 run.py:483] Algo bellman_ford step 9773 current loss 0.623397, current_train_items 312768.
I0302 19:02:55.087607 22535416901760 run.py:483] Algo bellman_ford step 9774 current loss 0.766055, current_train_items 312800.
I0302 19:02:55.107566 22535416901760 run.py:483] Algo bellman_ford step 9775 current loss 0.277618, current_train_items 312832.
I0302 19:02:55.123501 22535416901760 run.py:483] Algo bellman_ford step 9776 current loss 0.392715, current_train_items 312864.
I0302 19:02:55.148178 22535416901760 run.py:483] Algo bellman_ford step 9777 current loss 0.586164, current_train_items 312896.
I0302 19:02:55.180331 22535416901760 run.py:483] Algo bellman_ford step 9778 current loss 0.630950, current_train_items 312928.
I0302 19:02:55.213782 22535416901760 run.py:483] Algo bellman_ford step 9779 current loss 0.650711, current_train_items 312960.
I0302 19:02:55.233514 22535416901760 run.py:483] Algo bellman_ford step 9780 current loss 0.306396, current_train_items 312992.
I0302 19:02:55.250022 22535416901760 run.py:483] Algo bellman_ford step 9781 current loss 0.410511, current_train_items 313024.
I0302 19:02:55.273180 22535416901760 run.py:483] Algo bellman_ford step 9782 current loss 0.590029, current_train_items 313056.
I0302 19:02:55.306728 22535416901760 run.py:483] Algo bellman_ford step 9783 current loss 0.646514, current_train_items 313088.
I0302 19:02:55.338931 22535416901760 run.py:483] Algo bellman_ford step 9784 current loss 0.686573, current_train_items 313120.
I0302 19:02:55.359117 22535416901760 run.py:483] Algo bellman_ford step 9785 current loss 0.261947, current_train_items 313152.
I0302 19:02:55.375580 22535416901760 run.py:483] Algo bellman_ford step 9786 current loss 0.460664, current_train_items 313184.
I0302 19:02:55.398929 22535416901760 run.py:483] Algo bellman_ford step 9787 current loss 0.512136, current_train_items 313216.
I0302 19:02:55.429571 22535416901760 run.py:483] Algo bellman_ford step 9788 current loss 0.577067, current_train_items 313248.
I0302 19:02:55.462836 22535416901760 run.py:483] Algo bellman_ford step 9789 current loss 0.639866, current_train_items 313280.
I0302 19:02:55.482371 22535416901760 run.py:483] Algo bellman_ford step 9790 current loss 0.371209, current_train_items 313312.
I0302 19:02:55.498278 22535416901760 run.py:483] Algo bellman_ford step 9791 current loss 0.401555, current_train_items 313344.
I0302 19:02:55.521523 22535416901760 run.py:483] Algo bellman_ford step 9792 current loss 0.537990, current_train_items 313376.
I0302 19:02:55.552849 22535416901760 run.py:483] Algo bellman_ford step 9793 current loss 0.583264, current_train_items 313408.
I0302 19:02:55.584105 22535416901760 run.py:483] Algo bellman_ford step 9794 current loss 0.587066, current_train_items 313440.
I0302 19:02:55.603718 22535416901760 run.py:483] Algo bellman_ford step 9795 current loss 0.311198, current_train_items 313472.
I0302 19:02:55.620213 22535416901760 run.py:483] Algo bellman_ford step 9796 current loss 0.384693, current_train_items 313504.
I0302 19:02:55.643819 22535416901760 run.py:483] Algo bellman_ford step 9797 current loss 0.488341, current_train_items 313536.
I0302 19:02:55.674606 22535416901760 run.py:483] Algo bellman_ford step 9798 current loss 0.529822, current_train_items 313568.
I0302 19:02:55.707327 22535416901760 run.py:483] Algo bellman_ford step 9799 current loss 0.661488, current_train_items 313600.
I0302 19:02:55.726878 22535416901760 run.py:483] Algo bellman_ford step 9800 current loss 0.334847, current_train_items 313632.
I0302 19:02:55.734592 22535416901760 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:55.734702 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:02:55.751278 22535416901760 run.py:483] Algo bellman_ford step 9801 current loss 0.434827, current_train_items 313664.
I0302 19:02:55.776225 22535416901760 run.py:483] Algo bellman_ford step 9802 current loss 0.558555, current_train_items 313696.
I0302 19:02:55.808593 22535416901760 run.py:483] Algo bellman_ford step 9803 current loss 0.771581, current_train_items 313728.
I0302 19:02:55.841009 22535416901760 run.py:483] Algo bellman_ford step 9804 current loss 0.653219, current_train_items 313760.
I0302 19:02:55.861125 22535416901760 run.py:483] Algo bellman_ford step 9805 current loss 0.298779, current_train_items 313792.
I0302 19:02:55.877178 22535416901760 run.py:483] Algo bellman_ford step 9806 current loss 0.480900, current_train_items 313824.
I0302 19:02:55.900102 22535416901760 run.py:483] Algo bellman_ford step 9807 current loss 0.519630, current_train_items 313856.
I0302 19:02:55.931621 22535416901760 run.py:483] Algo bellman_ford step 9808 current loss 0.579896, current_train_items 313888.
I0302 19:02:55.966905 22535416901760 run.py:483] Algo bellman_ford step 9809 current loss 0.681072, current_train_items 313920.
I0302 19:02:55.986460 22535416901760 run.py:483] Algo bellman_ford step 9810 current loss 0.238226, current_train_items 313952.
I0302 19:02:56.002248 22535416901760 run.py:483] Algo bellman_ford step 9811 current loss 0.418949, current_train_items 313984.
I0302 19:02:56.026652 22535416901760 run.py:483] Algo bellman_ford step 9812 current loss 0.569561, current_train_items 314016.
I0302 19:02:56.056226 22535416901760 run.py:483] Algo bellman_ford step 9813 current loss 0.530359, current_train_items 314048.
I0302 19:02:56.090019 22535416901760 run.py:483] Algo bellman_ford step 9814 current loss 0.641914, current_train_items 314080.
I0302 19:02:56.109324 22535416901760 run.py:483] Algo bellman_ford step 9815 current loss 0.281685, current_train_items 314112.
I0302 19:02:56.125674 22535416901760 run.py:483] Algo bellman_ford step 9816 current loss 0.468097, current_train_items 314144.
I0302 19:02:56.148983 22535416901760 run.py:483] Algo bellman_ford step 9817 current loss 0.502472, current_train_items 314176.
I0302 19:02:56.181555 22535416901760 run.py:483] Algo bellman_ford step 9818 current loss 0.634964, current_train_items 314208.
I0302 19:02:56.214481 22535416901760 run.py:483] Algo bellman_ford step 9819 current loss 0.689056, current_train_items 314240.
I0302 19:02:56.234127 22535416901760 run.py:483] Algo bellman_ford step 9820 current loss 0.309802, current_train_items 314272.
I0302 19:02:56.250066 22535416901760 run.py:483] Algo bellman_ford step 9821 current loss 0.532001, current_train_items 314304.
I0302 19:02:56.273973 22535416901760 run.py:483] Algo bellman_ford step 9822 current loss 0.605228, current_train_items 314336.
I0302 19:02:56.304786 22535416901760 run.py:483] Algo bellman_ford step 9823 current loss 0.577588, current_train_items 314368.
I0302 19:02:56.337978 22535416901760 run.py:483] Algo bellman_ford step 9824 current loss 0.771289, current_train_items 314400.
I0302 19:02:56.357638 22535416901760 run.py:483] Algo bellman_ford step 9825 current loss 0.281536, current_train_items 314432.
I0302 19:02:56.373405 22535416901760 run.py:483] Algo bellman_ford step 9826 current loss 0.403779, current_train_items 314464.
I0302 19:02:56.396807 22535416901760 run.py:483] Algo bellman_ford step 9827 current loss 0.594976, current_train_items 314496.
I0302 19:02:56.427164 22535416901760 run.py:483] Algo bellman_ford step 9828 current loss 0.621803, current_train_items 314528.
I0302 19:02:56.461749 22535416901760 run.py:483] Algo bellman_ford step 9829 current loss 0.819016, current_train_items 314560.
I0302 19:02:56.481110 22535416901760 run.py:483] Algo bellman_ford step 9830 current loss 0.238721, current_train_items 314592.
I0302 19:02:56.497733 22535416901760 run.py:483] Algo bellman_ford step 9831 current loss 0.461407, current_train_items 314624.
I0302 19:02:56.521162 22535416901760 run.py:483] Algo bellman_ford step 9832 current loss 0.547439, current_train_items 314656.
I0302 19:02:56.553434 22535416901760 run.py:483] Algo bellman_ford step 9833 current loss 0.599268, current_train_items 314688.
I0302 19:02:56.585535 22535416901760 run.py:483] Algo bellman_ford step 9834 current loss 0.700115, current_train_items 314720.
I0302 19:02:56.605143 22535416901760 run.py:483] Algo bellman_ford step 9835 current loss 0.269963, current_train_items 314752.
I0302 19:02:56.621232 22535416901760 run.py:483] Algo bellman_ford step 9836 current loss 0.448318, current_train_items 314784.
I0302 19:02:56.644961 22535416901760 run.py:483] Algo bellman_ford step 9837 current loss 0.611880, current_train_items 314816.
I0302 19:02:56.677240 22535416901760 run.py:483] Algo bellman_ford step 9838 current loss 0.608255, current_train_items 314848.
I0302 19:02:56.712007 22535416901760 run.py:483] Algo bellman_ford step 9839 current loss 0.706228, current_train_items 314880.
I0302 19:02:56.731396 22535416901760 run.py:483] Algo bellman_ford step 9840 current loss 0.264161, current_train_items 314912.
I0302 19:02:56.747427 22535416901760 run.py:483] Algo bellman_ford step 9841 current loss 0.498471, current_train_items 314944.
I0302 19:02:56.771692 22535416901760 run.py:483] Algo bellman_ford step 9842 current loss 0.510329, current_train_items 314976.
I0302 19:02:56.802560 22535416901760 run.py:483] Algo bellman_ford step 9843 current loss 0.571379, current_train_items 315008.
I0302 19:02:56.833979 22535416901760 run.py:483] Algo bellman_ford step 9844 current loss 0.654763, current_train_items 315040.
I0302 19:02:56.853674 22535416901760 run.py:483] Algo bellman_ford step 9845 current loss 0.280057, current_train_items 315072.
I0302 19:02:56.870064 22535416901760 run.py:483] Algo bellman_ford step 9846 current loss 0.426300, current_train_items 315104.
I0302 19:02:56.894252 22535416901760 run.py:483] Algo bellman_ford step 9847 current loss 0.613457, current_train_items 315136.
I0302 19:02:56.925311 22535416901760 run.py:483] Algo bellman_ford step 9848 current loss 0.709600, current_train_items 315168.
I0302 19:02:56.958844 22535416901760 run.py:483] Algo bellman_ford step 9849 current loss 0.762780, current_train_items 315200.
I0302 19:02:56.978523 22535416901760 run.py:483] Algo bellman_ford step 9850 current loss 0.273178, current_train_items 315232.
I0302 19:02:56.986671 22535416901760 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.939453125, 'score': 0.939453125, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:56.986778 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.939, val scores are: bellman_ford: 0.939
I0302 19:02:57.004104 22535416901760 run.py:483] Algo bellman_ford step 9851 current loss 0.443847, current_train_items 315264.
I0302 19:02:57.029622 22535416901760 run.py:483] Algo bellman_ford step 9852 current loss 0.716973, current_train_items 315296.
I0302 19:02:57.060680 22535416901760 run.py:483] Algo bellman_ford step 9853 current loss 0.609708, current_train_items 315328.
I0302 19:02:57.095210 22535416901760 run.py:483] Algo bellman_ford step 9854 current loss 0.864657, current_train_items 315360.
I0302 19:02:57.114986 22535416901760 run.py:483] Algo bellman_ford step 9855 current loss 0.309760, current_train_items 315392.
I0302 19:02:57.130898 22535416901760 run.py:483] Algo bellman_ford step 9856 current loss 0.456151, current_train_items 315424.
I0302 19:02:57.155097 22535416901760 run.py:483] Algo bellman_ford step 9857 current loss 0.586696, current_train_items 315456.
I0302 19:02:57.185873 22535416901760 run.py:483] Algo bellman_ford step 9858 current loss 0.591378, current_train_items 315488.
I0302 19:02:57.219145 22535416901760 run.py:483] Algo bellman_ford step 9859 current loss 0.705763, current_train_items 315520.
I0302 19:02:57.239237 22535416901760 run.py:483] Algo bellman_ford step 9860 current loss 0.314633, current_train_items 315552.
I0302 19:02:57.255458 22535416901760 run.py:483] Algo bellman_ford step 9861 current loss 0.449375, current_train_items 315584.
I0302 19:02:57.279433 22535416901760 run.py:483] Algo bellman_ford step 9862 current loss 0.634732, current_train_items 315616.
I0302 19:02:57.310881 22535416901760 run.py:483] Algo bellman_ford step 9863 current loss 0.614611, current_train_items 315648.
I0302 19:02:57.344134 22535416901760 run.py:483] Algo bellman_ford step 9864 current loss 0.739228, current_train_items 315680.
I0302 19:02:57.363557 22535416901760 run.py:483] Algo bellman_ford step 9865 current loss 0.285000, current_train_items 315712.
I0302 19:02:57.379446 22535416901760 run.py:483] Algo bellman_ford step 9866 current loss 0.478102, current_train_items 315744.
I0302 19:02:57.403539 22535416901760 run.py:483] Algo bellman_ford step 9867 current loss 0.642540, current_train_items 315776.
I0302 19:02:57.435469 22535416901760 run.py:483] Algo bellman_ford step 9868 current loss 0.661884, current_train_items 315808.
I0302 19:02:57.469011 22535416901760 run.py:483] Algo bellman_ford step 9869 current loss 0.688674, current_train_items 315840.
I0302 19:02:57.489243 22535416901760 run.py:483] Algo bellman_ford step 9870 current loss 0.315988, current_train_items 315872.
I0302 19:02:57.505982 22535416901760 run.py:483] Algo bellman_ford step 9871 current loss 0.417295, current_train_items 315904.
I0302 19:02:57.530372 22535416901760 run.py:483] Algo bellman_ford step 9872 current loss 0.536979, current_train_items 315936.
I0302 19:02:57.562477 22535416901760 run.py:483] Algo bellman_ford step 9873 current loss 0.574030, current_train_items 315968.
I0302 19:02:57.594645 22535416901760 run.py:483] Algo bellman_ford step 9874 current loss 0.612256, current_train_items 316000.
I0302 19:02:57.614445 22535416901760 run.py:483] Algo bellman_ford step 9875 current loss 0.254085, current_train_items 316032.
I0302 19:02:57.630572 22535416901760 run.py:483] Algo bellman_ford step 9876 current loss 0.374204, current_train_items 316064.
I0302 19:02:57.653068 22535416901760 run.py:483] Algo bellman_ford step 9877 current loss 0.520933, current_train_items 316096.
I0302 19:02:57.684082 22535416901760 run.py:483] Algo bellman_ford step 9878 current loss 0.570317, current_train_items 316128.
I0302 19:02:57.716935 22535416901760 run.py:483] Algo bellman_ford step 9879 current loss 0.640546, current_train_items 316160.
I0302 19:02:57.736454 22535416901760 run.py:483] Algo bellman_ford step 9880 current loss 0.313300, current_train_items 316192.
I0302 19:02:57.752695 22535416901760 run.py:483] Algo bellman_ford step 9881 current loss 0.429964, current_train_items 316224.
I0302 19:02:57.776431 22535416901760 run.py:483] Algo bellman_ford step 9882 current loss 0.494832, current_train_items 316256.
I0302 19:02:57.808080 22535416901760 run.py:483] Algo bellman_ford step 9883 current loss 0.676945, current_train_items 316288.
I0302 19:02:57.841536 22535416901760 run.py:483] Algo bellman_ford step 9884 current loss 0.612906, current_train_items 316320.
I0302 19:02:57.861748 22535416901760 run.py:483] Algo bellman_ford step 9885 current loss 0.288012, current_train_items 316352.
I0302 19:02:57.877803 22535416901760 run.py:483] Algo bellman_ford step 9886 current loss 0.445548, current_train_items 316384.
I0302 19:02:57.901462 22535416901760 run.py:483] Algo bellman_ford step 9887 current loss 0.524505, current_train_items 316416.
I0302 19:02:57.932784 22535416901760 run.py:483] Algo bellman_ford step 9888 current loss 0.602991, current_train_items 316448.
I0302 19:02:57.964887 22535416901760 run.py:483] Algo bellman_ford step 9889 current loss 0.661826, current_train_items 316480.
I0302 19:02:57.984805 22535416901760 run.py:483] Algo bellman_ford step 9890 current loss 0.287870, current_train_items 316512.
I0302 19:02:58.000875 22535416901760 run.py:483] Algo bellman_ford step 9891 current loss 0.460098, current_train_items 316544.
I0302 19:02:58.023472 22535416901760 run.py:483] Algo bellman_ford step 9892 current loss 0.550720, current_train_items 316576.
I0302 19:02:58.055729 22535416901760 run.py:483] Algo bellman_ford step 9893 current loss 0.618311, current_train_items 316608.
I0302 19:02:58.090035 22535416901760 run.py:483] Algo bellman_ford step 9894 current loss 0.675001, current_train_items 316640.
I0302 19:02:58.110044 22535416901760 run.py:483] Algo bellman_ford step 9895 current loss 0.252327, current_train_items 316672.
I0302 19:02:58.126008 22535416901760 run.py:483] Algo bellman_ford step 9896 current loss 0.412049, current_train_items 316704.
I0302 19:02:58.150265 22535416901760 run.py:483] Algo bellman_ford step 9897 current loss 0.575405, current_train_items 316736.
I0302 19:02:58.181999 22535416901760 run.py:483] Algo bellman_ford step 9898 current loss 0.603248, current_train_items 316768.
I0302 19:02:58.217129 22535416901760 run.py:483] Algo bellman_ford step 9899 current loss 0.734898, current_train_items 316800.
I0302 19:02:58.236872 22535416901760 run.py:483] Algo bellman_ford step 9900 current loss 0.269826, current_train_items 316832.
I0302 19:02:58.244672 22535416901760 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.943359375, 'score': 0.943359375, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:58.244778 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.943, val scores are: bellman_ford: 0.943
I0302 19:02:58.261636 22535416901760 run.py:483] Algo bellman_ford step 9901 current loss 0.391409, current_train_items 316864.
I0302 19:02:58.285586 22535416901760 run.py:483] Algo bellman_ford step 9902 current loss 0.575409, current_train_items 316896.
I0302 19:02:58.317002 22535416901760 run.py:483] Algo bellman_ford step 9903 current loss 0.639357, current_train_items 316928.
I0302 19:02:58.351341 22535416901760 run.py:483] Algo bellman_ford step 9904 current loss 0.654413, current_train_items 316960.
I0302 19:02:58.371431 22535416901760 run.py:483] Algo bellman_ford step 9905 current loss 0.271882, current_train_items 316992.
I0302 19:02:58.387742 22535416901760 run.py:483] Algo bellman_ford step 9906 current loss 0.409231, current_train_items 317024.
I0302 19:02:58.412237 22535416901760 run.py:483] Algo bellman_ford step 9907 current loss 0.664175, current_train_items 317056.
I0302 19:02:58.443644 22535416901760 run.py:483] Algo bellman_ford step 9908 current loss 0.614650, current_train_items 317088.
I0302 19:02:58.475279 22535416901760 run.py:483] Algo bellman_ford step 9909 current loss 0.734949, current_train_items 317120.
I0302 19:02:58.494883 22535416901760 run.py:483] Algo bellman_ford step 9910 current loss 0.272875, current_train_items 317152.
I0302 19:02:58.511408 22535416901760 run.py:483] Algo bellman_ford step 9911 current loss 0.479997, current_train_items 317184.
I0302 19:02:58.535133 22535416901760 run.py:483] Algo bellman_ford step 9912 current loss 0.584820, current_train_items 317216.
I0302 19:02:58.567518 22535416901760 run.py:483] Algo bellman_ford step 9913 current loss 0.616659, current_train_items 317248.
I0302 19:02:58.600579 22535416901760 run.py:483] Algo bellman_ford step 9914 current loss 0.707012, current_train_items 317280.
I0302 19:02:58.620363 22535416901760 run.py:483] Algo bellman_ford step 9915 current loss 0.310217, current_train_items 317312.
I0302 19:02:58.636752 22535416901760 run.py:483] Algo bellman_ford step 9916 current loss 0.454321, current_train_items 317344.
I0302 19:02:58.660807 22535416901760 run.py:483] Algo bellman_ford step 9917 current loss 0.555548, current_train_items 317376.
I0302 19:02:58.692611 22535416901760 run.py:483] Algo bellman_ford step 9918 current loss 0.625631, current_train_items 317408.
I0302 19:02:58.725141 22535416901760 run.py:483] Algo bellman_ford step 9919 current loss 0.647484, current_train_items 317440.
I0302 19:02:58.745010 22535416901760 run.py:483] Algo bellman_ford step 9920 current loss 0.296778, current_train_items 317472.
I0302 19:02:58.761468 22535416901760 run.py:483] Algo bellman_ford step 9921 current loss 0.534041, current_train_items 317504.
I0302 19:02:58.785381 22535416901760 run.py:483] Algo bellman_ford step 9922 current loss 0.656141, current_train_items 317536.
I0302 19:02:58.815331 22535416901760 run.py:483] Algo bellman_ford step 9923 current loss 0.647031, current_train_items 317568.
I0302 19:02:58.851115 22535416901760 run.py:483] Algo bellman_ford step 9924 current loss 0.702271, current_train_items 317600.
I0302 19:02:58.870440 22535416901760 run.py:483] Algo bellman_ford step 9925 current loss 0.312966, current_train_items 317632.
I0302 19:02:58.886390 22535416901760 run.py:483] Algo bellman_ford step 9926 current loss 0.461707, current_train_items 317664.
I0302 19:02:58.910847 22535416901760 run.py:483] Algo bellman_ford step 9927 current loss 0.592145, current_train_items 317696.
I0302 19:02:58.943827 22535416901760 run.py:483] Algo bellman_ford step 9928 current loss 0.737661, current_train_items 317728.
I0302 19:02:58.974945 22535416901760 run.py:483] Algo bellman_ford step 9929 current loss 0.667871, current_train_items 317760.
I0302 19:02:58.994250 22535416901760 run.py:483] Algo bellman_ford step 9930 current loss 0.267210, current_train_items 317792.
I0302 19:02:59.009745 22535416901760 run.py:483] Algo bellman_ford step 9931 current loss 0.430470, current_train_items 317824.
I0302 19:02:59.033685 22535416901760 run.py:483] Algo bellman_ford step 9932 current loss 0.568742, current_train_items 317856.
I0302 19:02:59.064588 22535416901760 run.py:483] Algo bellman_ford step 9933 current loss 0.600961, current_train_items 317888.
I0302 19:02:59.096537 22535416901760 run.py:483] Algo bellman_ford step 9934 current loss 0.667127, current_train_items 317920.
I0302 19:02:59.115993 22535416901760 run.py:483] Algo bellman_ford step 9935 current loss 0.249701, current_train_items 317952.
I0302 19:02:59.131826 22535416901760 run.py:483] Algo bellman_ford step 9936 current loss 0.483452, current_train_items 317984.
I0302 19:02:59.155591 22535416901760 run.py:483] Algo bellman_ford step 9937 current loss 0.521074, current_train_items 318016.
I0302 19:02:59.186473 22535416901760 run.py:483] Algo bellman_ford step 9938 current loss 0.583147, current_train_items 318048.
I0302 19:02:59.220393 22535416901760 run.py:483] Algo bellman_ford step 9939 current loss 0.706362, current_train_items 318080.
I0302 19:02:59.240144 22535416901760 run.py:483] Algo bellman_ford step 9940 current loss 0.319201, current_train_items 318112.
I0302 19:02:59.255979 22535416901760 run.py:483] Algo bellman_ford step 9941 current loss 0.363807, current_train_items 318144.
I0302 19:02:59.279168 22535416901760 run.py:483] Algo bellman_ford step 9942 current loss 0.514517, current_train_items 318176.
I0302 19:02:59.310742 22535416901760 run.py:483] Algo bellman_ford step 9943 current loss 0.537759, current_train_items 318208.
I0302 19:02:59.343182 22535416901760 run.py:483] Algo bellman_ford step 9944 current loss 0.684277, current_train_items 318240.
I0302 19:02:59.363303 22535416901760 run.py:483] Algo bellman_ford step 9945 current loss 0.327819, current_train_items 318272.
I0302 19:02:59.379373 22535416901760 run.py:483] Algo bellman_ford step 9946 current loss 0.442147, current_train_items 318304.
I0302 19:02:59.404006 22535416901760 run.py:483] Algo bellman_ford step 9947 current loss 0.589728, current_train_items 318336.
I0302 19:02:59.436150 22535416901760 run.py:483] Algo bellman_ford step 9948 current loss 0.595109, current_train_items 318368.
I0302 19:02:59.470988 22535416901760 run.py:483] Algo bellman_ford step 9949 current loss 0.746157, current_train_items 318400.
I0302 19:02:59.490485 22535416901760 run.py:483] Algo bellman_ford step 9950 current loss 0.269739, current_train_items 318432.
I0302 19:02:59.498610 22535416901760 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9228515625, 'score': 0.9228515625, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:59.498721 22535416901760 run.py:522] Not saving new best model, best avg val score was 0.958, current avg val score is 0.923, val scores are: bellman_ford: 0.923
I0302 19:02:59.515849 22535416901760 run.py:483] Algo bellman_ford step 9951 current loss 0.432567, current_train_items 318464.
I0302 19:02:59.539910 22535416901760 run.py:483] Algo bellman_ford step 9952 current loss 0.515183, current_train_items 318496.
I0302 19:02:59.572683 22535416901760 run.py:483] Algo bellman_ford step 9953 current loss 0.733176, current_train_items 318528.
I0302 19:02:59.607874 22535416901760 run.py:483] Algo bellman_ford step 9954 current loss 0.643023, current_train_items 318560.
I0302 19:02:59.627669 22535416901760 run.py:483] Algo bellman_ford step 9955 current loss 0.297570, current_train_items 318592.
I0302 19:02:59.644002 22535416901760 run.py:483] Algo bellman_ford step 9956 current loss 0.489970, current_train_items 318624.
I0302 19:02:59.668024 22535416901760 run.py:483] Algo bellman_ford step 9957 current loss 0.572702, current_train_items 318656.
I0302 19:02:59.700019 22535416901760 run.py:483] Algo bellman_ford step 9958 current loss 0.676679, current_train_items 318688.
I0302 19:02:59.734302 22535416901760 run.py:483] Algo bellman_ford step 9959 current loss 0.871192, current_train_items 318720.
I0302 19:02:59.754112 22535416901760 run.py:483] Algo bellman_ford step 9960 current loss 0.324931, current_train_items 318752.
I0302 19:02:59.769725 22535416901760 run.py:483] Algo bellman_ford step 9961 current loss 0.440328, current_train_items 318784.
I0302 19:02:59.794183 22535416901760 run.py:483] Algo bellman_ford step 9962 current loss 0.571752, current_train_items 318816.
I0302 19:02:59.825654 22535416901760 run.py:483] Algo bellman_ford step 9963 current loss 0.592518, current_train_items 318848.
I0302 19:02:59.858543 22535416901760 run.py:483] Algo bellman_ford step 9964 current loss 0.764091, current_train_items 318880.
I0302 19:02:59.877837 22535416901760 run.py:483] Algo bellman_ford step 9965 current loss 0.314734, current_train_items 318912.
I0302 19:02:59.894193 22535416901760 run.py:483] Algo bellman_ford step 9966 current loss 0.494282, current_train_items 318944.
I0302 19:02:59.918105 22535416901760 run.py:483] Algo bellman_ford step 9967 current loss 0.555042, current_train_items 318976.
I0302 19:02:59.949665 22535416901760 run.py:483] Algo bellman_ford step 9968 current loss 0.613607, current_train_items 319008.
I0302 19:02:59.983360 22535416901760 run.py:483] Algo bellman_ford step 9969 current loss 0.648247, current_train_items 319040.
I0302 19:03:00.003077 22535416901760 run.py:483] Algo bellman_ford step 9970 current loss 0.288037, current_train_items 319072.
I0302 19:03:00.019097 22535416901760 run.py:483] Algo bellman_ford step 9971 current loss 0.495307, current_train_items 319104.
I0302 19:03:00.042503 22535416901760 run.py:483] Algo bellman_ford step 9972 current loss 0.535767, current_train_items 319136.
I0302 19:03:00.072686 22535416901760 run.py:483] Algo bellman_ford step 9973 current loss 0.578253, current_train_items 319168.
I0302 19:03:00.105677 22535416901760 run.py:483] Algo bellman_ford step 9974 current loss 0.621846, current_train_items 319200.
I0302 19:03:00.125173 22535416901760 run.py:483] Algo bellman_ford step 9975 current loss 0.313409, current_train_items 319232.
I0302 19:03:00.141805 22535416901760 run.py:483] Algo bellman_ford step 9976 current loss 0.456160, current_train_items 319264.
I0302 19:03:00.165067 22535416901760 run.py:483] Algo bellman_ford step 9977 current loss 0.500893, current_train_items 319296.
I0302 19:03:00.196733 22535416901760 run.py:483] Algo bellman_ford step 9978 current loss 0.545710, current_train_items 319328.
I0302 19:03:00.228623 22535416901760 run.py:483] Algo bellman_ford step 9979 current loss 0.616180, current_train_items 319360.
I0302 19:03:00.248066 22535416901760 run.py:483] Algo bellman_ford step 9980 current loss 0.291562, current_train_items 319392.
I0302 19:03:00.264037 22535416901760 run.py:483] Algo bellman_ford step 9981 current loss 0.529949, current_train_items 319424.
I0302 19:03:00.288478 22535416901760 run.py:483] Algo bellman_ford step 9982 current loss 0.537941, current_train_items 319456.
I0302 19:03:00.321643 22535416901760 run.py:483] Algo bellman_ford step 9983 current loss 0.663155, current_train_items 319488.
I0302 19:03:00.355344 22535416901760 run.py:483] Algo bellman_ford step 9984 current loss 0.709527, current_train_items 319520.
I0302 19:03:00.374943 22535416901760 run.py:483] Algo bellman_ford step 9985 current loss 0.301688, current_train_items 319552.
I0302 19:03:00.390745 22535416901760 run.py:483] Algo bellman_ford step 9986 current loss 0.360115, current_train_items 319584.
I0302 19:03:00.414039 22535416901760 run.py:483] Algo bellman_ford step 9987 current loss 0.586252, current_train_items 319616.
I0302 19:03:00.445540 22535416901760 run.py:483] Algo bellman_ford step 9988 current loss 0.650979, current_train_items 319648.
I0302 19:03:00.478348 22535416901760 run.py:483] Algo bellman_ford step 9989 current loss 0.610480, current_train_items 319680.
I0302 19:03:00.498021 22535416901760 run.py:483] Algo bellman_ford step 9990 current loss 0.267084, current_train_items 319712.
I0302 19:03:00.514341 22535416901760 run.py:483] Algo bellman_ford step 9991 current loss 0.406954, current_train_items 319744.
I0302 19:03:00.537538 22535416901760 run.py:483] Algo bellman_ford step 9992 current loss 0.628222, current_train_items 319776.
I0302 19:03:00.568145 22535416901760 run.py:483] Algo bellman_ford step 9993 current loss 0.612238, current_train_items 319808.
I0302 19:03:00.600477 22535416901760 run.py:483] Algo bellman_ford step 9994 current loss 0.585764, current_train_items 319840.
I0302 19:03:00.620041 22535416901760 run.py:483] Algo bellman_ford step 9995 current loss 0.310986, current_train_items 319872.
I0302 19:03:00.636280 22535416901760 run.py:483] Algo bellman_ford step 9996 current loss 0.449890, current_train_items 319904.
I0302 19:03:00.659806 22535416901760 run.py:483] Algo bellman_ford step 9997 current loss 0.469448, current_train_items 319936.
I0302 19:03:00.690897 22535416901760 run.py:483] Algo bellman_ford step 9998 current loss 0.549533, current_train_items 319968.
I0302 19:03:00.721834 22535416901760 run.py:483] Algo bellman_ford step 9999 current loss 0.718505, current_train_items 320000.
I0302 19:03:00.727818 22535416901760 run.py:527] Restoring best model from checkpoint...
I0302 19:03:03.286265 22535416901760 run.py:542] (test) algo bellman_ford : {'pi': 0.10595703125, 'score': 0.10595703125, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:03:03.286531 22535416901760 run.py:544] Done!

Loading rhel8/default-amp
  Loading requirement: dot rhel8/slurm singularity/current rhel8/global
    cuda/11.4 libpciaccess/0.16/gcc-9.4.0-6fonbj6
    libiconv/1.16/gcc-9.4.0-ahebbov libxml2/2.9.12/gcc-9.4.0-gnknt5e
    ncurses/6.2/gcc-9.4.0-aiirok7 hwloc/2.5.0/gcc-9.4.0-7sqomga
    libevent/2.1.12/gcc-9.4.0-hgny7cm numactl/2.0.14/gcc-9.4.0-52dwc6n
    cuda/11.4.0/gcc-9.4.0-3hnxhjt gdrcopy/2.2/gcc-9.4.0-e4igtfp
    knem/1.1.4/gcc-9.4.0-bpbxgva libnl/3.3.0/gcc-9.4.0-whwhrwb
    rdma-core/34.0/gcc-9.4.0-5eo5n2u ucx/1.11.1/gcc-9.4.0-lktqyl4
    openmpi/4.1.1/gcc-9.4.0-epagguv
2024-03-02 18:56:23.099132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-02 18:56:23.099397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-02 18:56:23.100542: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-02 18:56:24.353815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I0302 18:56:27.638507 22683591385216 xla_bridge.py:638] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0302 18:56:27.640167 22683591385216 xla_bridge.py:638] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0302 18:56:27.939918 22683591385216 run.py:307] Creating samplers for algo bellman_ford
W0302 18:56:27.940345 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:27.940608 22683591385216 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.149371 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.149608 22683591385216 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.399771 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.400029 22683591385216 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:28.744444 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:28.744668 22683591385216 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.165818 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
W0302 18:56:29.166068 22683591385216 samplers.py:100] Sampling dataset on-the-fly, unlimited samples.
W0302 18:56:29.700975 22683591385216 samplers.py:277] Ignoring kwargs {'length_needle'} when building sampler class <class 'clrs._src.samplers.BellmanFordSampler'>
I0302 18:56:29.701220 22683591385216 samplers.py:112] Creating a dataset with 64 samples.
I0302 18:56:29.739518 22683591385216 run.py:166] Dataset not found in ./datasets_1/20/CLRS30_v1.0.0. Downloading...
I0302 18:56:45.189496 22683591385216 dataset_info.py:482] Load dataset info from ./datasets_1/20/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:45.192789 22683591385216 dataset_info.py:482] Load dataset info from ./datasets_1/20/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:45.193805 22683591385216 dataset_builder.py:366] Reusing dataset clrs_dataset (./datasets_1/20/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0)
I0302 18:56:45.193919 22683591385216 logging_logger.py:44] Constructing tf.data.Dataset clrs_dataset for split test, from ./datasets_1/20/CLRS30_v1.0.0/clrs_dataset/bellman_ford_test/1.0.0
I0302 18:56:58.976086 22683591385216 run.py:483] Algo bellman_ford step 0 current loss 3.808173, current_train_items 32.
I0302 18:57:01.770606 22683591385216 run.py:503] (val) algo bellman_ford step 0: {'pi': 0.6201171875, 'score': 0.6201171875, 'examples_seen': 32, 'step': 0, 'algorithm': 'bellman_ford'}
I0302 18:57:01.770797 22683591385216 run.py:519] Checkpointing best model, best avg val score was -1.000, current avg val score is 0.620, val scores are: bellman_ford: 0.620
I0302 18:57:10.745231 22683591385216 run.py:483] Algo bellman_ford step 1 current loss 4.373810, current_train_items 64.
I0302 18:57:20.204679 22683591385216 run.py:483] Algo bellman_ford step 2 current loss 3.785630, current_train_items 96.
I0302 18:57:29.737178 22683591385216 run.py:483] Algo bellman_ford step 3 current loss 3.371275, current_train_items 128.
I0302 18:57:37.349694 22683591385216 run.py:483] Algo bellman_ford step 4 current loss 3.660047, current_train_items 160.
I0302 18:57:37.366958 22683591385216 run.py:483] Algo bellman_ford step 5 current loss 1.118337, current_train_items 192.
I0302 18:57:37.383238 22683591385216 run.py:483] Algo bellman_ford step 6 current loss 1.831795, current_train_items 224.
I0302 18:57:37.404796 22683591385216 run.py:483] Algo bellman_ford step 7 current loss 2.183954, current_train_items 256.
I0302 18:57:37.432857 22683591385216 run.py:483] Algo bellman_ford step 8 current loss 2.550467, current_train_items 288.
I0302 18:57:37.462990 22683591385216 run.py:483] Algo bellman_ford step 9 current loss 2.725612, current_train_items 320.
I0302 18:57:37.479665 22683591385216 run.py:483] Algo bellman_ford step 10 current loss 1.041028, current_train_items 352.
I0302 18:57:37.495746 22683591385216 run.py:483] Algo bellman_ford step 11 current loss 1.550904, current_train_items 384.
I0302 18:57:37.516825 22683591385216 run.py:483] Algo bellman_ford step 12 current loss 1.651413, current_train_items 416.
I0302 18:57:37.546339 22683591385216 run.py:483] Algo bellman_ford step 13 current loss 2.086736, current_train_items 448.
I0302 18:57:37.572643 22683591385216 run.py:483] Algo bellman_ford step 14 current loss 1.912559, current_train_items 480.
I0302 18:57:37.589676 22683591385216 run.py:483] Algo bellman_ford step 15 current loss 0.815167, current_train_items 512.
I0302 18:57:37.605107 22683591385216 run.py:483] Algo bellman_ford step 16 current loss 1.168647, current_train_items 544.
I0302 18:57:37.628599 22683591385216 run.py:483] Algo bellman_ford step 17 current loss 1.917745, current_train_items 576.
I0302 18:57:37.655943 22683591385216 run.py:483] Algo bellman_ford step 18 current loss 1.719084, current_train_items 608.
I0302 18:57:37.687005 22683591385216 run.py:483] Algo bellman_ford step 19 current loss 2.002029, current_train_items 640.
I0302 18:57:37.703572 22683591385216 run.py:483] Algo bellman_ford step 20 current loss 0.582332, current_train_items 672.
I0302 18:57:37.718709 22683591385216 run.py:483] Algo bellman_ford step 21 current loss 0.871552, current_train_items 704.
I0302 18:57:37.741161 22683591385216 run.py:483] Algo bellman_ford step 22 current loss 1.510061, current_train_items 736.
I0302 18:57:37.768659 22683591385216 run.py:483] Algo bellman_ford step 23 current loss 1.531404, current_train_items 768.
I0302 18:57:37.798152 22683591385216 run.py:483] Algo bellman_ford step 24 current loss 1.801200, current_train_items 800.
I0302 18:57:37.815025 22683591385216 run.py:483] Algo bellman_ford step 25 current loss 0.509194, current_train_items 832.
I0302 18:57:37.830624 22683591385216 run.py:483] Algo bellman_ford step 26 current loss 0.910242, current_train_items 864.
I0302 18:57:37.853350 22683591385216 run.py:483] Algo bellman_ford step 27 current loss 1.463254, current_train_items 896.
I0302 18:57:37.881710 22683591385216 run.py:483] Algo bellman_ford step 28 current loss 1.377482, current_train_items 928.
I0302 18:57:37.912396 22683591385216 run.py:483] Algo bellman_ford step 29 current loss 1.677651, current_train_items 960.
I0302 18:57:37.929221 22683591385216 run.py:483] Algo bellman_ford step 30 current loss 0.398119, current_train_items 992.
I0302 18:57:37.944016 22683591385216 run.py:483] Algo bellman_ford step 31 current loss 0.693278, current_train_items 1024.
I0302 18:57:37.965811 22683591385216 run.py:483] Algo bellman_ford step 32 current loss 1.255437, current_train_items 1056.
I0302 18:57:37.994870 22683591385216 run.py:483] Algo bellman_ford step 33 current loss 1.362206, current_train_items 1088.
I0302 18:57:38.024616 22683591385216 run.py:483] Algo bellman_ford step 34 current loss 1.510767, current_train_items 1120.
I0302 18:57:38.041314 22683591385216 run.py:483] Algo bellman_ford step 35 current loss 0.338594, current_train_items 1152.
I0302 18:57:38.056371 22683591385216 run.py:483] Algo bellman_ford step 36 current loss 0.566789, current_train_items 1184.
I0302 18:57:38.078616 22683591385216 run.py:483] Algo bellman_ford step 37 current loss 1.130372, current_train_items 1216.
I0302 18:57:38.106542 22683591385216 run.py:483] Algo bellman_ford step 38 current loss 1.128667, current_train_items 1248.
W0302 18:57:38.128578 22683591385216 samplers.py:155] Increasing hint lengh from 9 to 11
I0302 18:57:43.419172 22683591385216 run.py:483] Algo bellman_ford step 39 current loss 1.521996, current_train_items 1280.
I0302 18:57:43.438018 22683591385216 run.py:483] Algo bellman_ford step 40 current loss 0.434146, current_train_items 1312.
I0302 18:57:43.453976 22683591385216 run.py:483] Algo bellman_ford step 41 current loss 0.633419, current_train_items 1344.
I0302 18:57:43.475705 22683591385216 run.py:483] Algo bellman_ford step 42 current loss 0.907553, current_train_items 1376.
I0302 18:57:43.505120 22683591385216 run.py:483] Algo bellman_ford step 43 current loss 1.066149, current_train_items 1408.
I0302 18:57:43.535739 22683591385216 run.py:483] Algo bellman_ford step 44 current loss 1.170229, current_train_items 1440.
I0302 18:57:43.554069 22683591385216 run.py:483] Algo bellman_ford step 45 current loss 0.272215, current_train_items 1472.
I0302 18:57:43.570103 22683591385216 run.py:483] Algo bellman_ford step 46 current loss 0.609312, current_train_items 1504.
I0302 18:57:43.591475 22683591385216 run.py:483] Algo bellman_ford step 47 current loss 0.800517, current_train_items 1536.
I0302 18:57:43.617437 22683591385216 run.py:483] Algo bellman_ford step 48 current loss 0.730524, current_train_items 1568.
I0302 18:57:43.645034 22683591385216 run.py:483] Algo bellman_ford step 49 current loss 0.944415, current_train_items 1600.
I0302 18:57:43.662425 22683591385216 run.py:483] Algo bellman_ford step 50 current loss 0.191240, current_train_items 1632.
I0302 18:57:43.671614 22683591385216 run.py:503] (val) algo bellman_ford step 50: {'pi': 0.8271484375, 'score': 0.8271484375, 'examples_seen': 1632, 'step': 50, 'algorithm': 'bellman_ford'}
I0302 18:57:43.671728 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.620, current avg val score is 0.827, val scores are: bellman_ford: 0.827
I0302 18:57:43.699700 22683591385216 run.py:483] Algo bellman_ford step 51 current loss 0.466632, current_train_items 1664.
I0302 18:57:43.721658 22683591385216 run.py:483] Algo bellman_ford step 52 current loss 0.842221, current_train_items 1696.
I0302 18:57:43.749371 22683591385216 run.py:483] Algo bellman_ford step 53 current loss 0.778528, current_train_items 1728.
I0302 18:57:43.779892 22683591385216 run.py:483] Algo bellman_ford step 54 current loss 1.011339, current_train_items 1760.
I0302 18:57:43.797780 22683591385216 run.py:483] Algo bellman_ford step 55 current loss 0.238649, current_train_items 1792.
I0302 18:57:43.813221 22683591385216 run.py:483] Algo bellman_ford step 56 current loss 0.369875, current_train_items 1824.
I0302 18:57:43.835083 22683591385216 run.py:483] Algo bellman_ford step 57 current loss 0.753405, current_train_items 1856.
I0302 18:57:43.861535 22683591385216 run.py:483] Algo bellman_ford step 58 current loss 0.622752, current_train_items 1888.
I0302 18:57:43.892569 22683591385216 run.py:483] Algo bellman_ford step 59 current loss 0.962463, current_train_items 1920.
I0302 18:57:43.910005 22683591385216 run.py:483] Algo bellman_ford step 60 current loss 0.147120, current_train_items 1952.
W0302 18:57:43.919296 22683591385216 samplers.py:155] Increasing hint lengh from 6 to 7
I0302 18:57:49.144180 22683591385216 run.py:483] Algo bellman_ford step 61 current loss 0.390311, current_train_items 1984.
I0302 18:57:49.166974 22683591385216 run.py:483] Algo bellman_ford step 62 current loss 0.635495, current_train_items 2016.
I0302 18:57:49.194995 22683591385216 run.py:483] Algo bellman_ford step 63 current loss 0.737900, current_train_items 2048.
I0302 18:57:49.227915 22683591385216 run.py:483] Algo bellman_ford step 64 current loss 0.952111, current_train_items 2080.
I0302 18:57:49.245781 22683591385216 run.py:483] Algo bellman_ford step 65 current loss 0.146607, current_train_items 2112.
I0302 18:57:49.261174 22683591385216 run.py:483] Algo bellman_ford step 66 current loss 0.254839, current_train_items 2144.
I0302 18:57:49.284232 22683591385216 run.py:483] Algo bellman_ford step 67 current loss 0.647613, current_train_items 2176.
I0302 18:57:49.311031 22683591385216 run.py:483] Algo bellman_ford step 68 current loss 0.626441, current_train_items 2208.
I0302 18:57:49.341165 22683591385216 run.py:483] Algo bellman_ford step 69 current loss 0.774810, current_train_items 2240.
I0302 18:57:49.358297 22683591385216 run.py:483] Algo bellman_ford step 70 current loss 0.118452, current_train_items 2272.
I0302 18:57:49.374013 22683591385216 run.py:483] Algo bellman_ford step 71 current loss 0.234830, current_train_items 2304.
I0302 18:57:49.396110 22683591385216 run.py:483] Algo bellman_ford step 72 current loss 0.545997, current_train_items 2336.
I0302 18:57:49.424062 22683591385216 run.py:483] Algo bellman_ford step 73 current loss 0.626263, current_train_items 2368.
I0302 18:57:49.454621 22683591385216 run.py:483] Algo bellman_ford step 74 current loss 0.818981, current_train_items 2400.
I0302 18:57:49.472022 22683591385216 run.py:483] Algo bellman_ford step 75 current loss 0.075654, current_train_items 2432.
I0302 18:57:49.487934 22683591385216 run.py:483] Algo bellman_ford step 76 current loss 0.301215, current_train_items 2464.
I0302 18:57:49.509533 22683591385216 run.py:483] Algo bellman_ford step 77 current loss 0.515874, current_train_items 2496.
I0302 18:57:49.536398 22683591385216 run.py:483] Algo bellman_ford step 78 current loss 0.529142, current_train_items 2528.
I0302 18:57:49.564023 22683591385216 run.py:483] Algo bellman_ford step 79 current loss 0.608034, current_train_items 2560.
I0302 18:57:49.581415 22683591385216 run.py:483] Algo bellman_ford step 80 current loss 0.088043, current_train_items 2592.
I0302 18:57:49.596857 22683591385216 run.py:483] Algo bellman_ford step 81 current loss 0.320407, current_train_items 2624.
I0302 18:57:49.618812 22683591385216 run.py:483] Algo bellman_ford step 82 current loss 0.711973, current_train_items 2656.
I0302 18:57:49.646046 22683591385216 run.py:483] Algo bellman_ford step 83 current loss 0.566632, current_train_items 2688.
I0302 18:57:49.674398 22683591385216 run.py:483] Algo bellman_ford step 84 current loss 0.730252, current_train_items 2720.
I0302 18:57:49.690967 22683591385216 run.py:483] Algo bellman_ford step 85 current loss 0.178673, current_train_items 2752.
I0302 18:57:49.706606 22683591385216 run.py:483] Algo bellman_ford step 86 current loss 0.367300, current_train_items 2784.
I0302 18:57:49.729924 22683591385216 run.py:483] Algo bellman_ford step 87 current loss 0.675730, current_train_items 2816.
I0302 18:57:49.757962 22683591385216 run.py:483] Algo bellman_ford step 88 current loss 0.456204, current_train_items 2848.
I0302 18:57:49.788070 22683591385216 run.py:483] Algo bellman_ford step 89 current loss 0.637375, current_train_items 2880.
I0302 18:57:49.805386 22683591385216 run.py:483] Algo bellman_ford step 90 current loss 0.106614, current_train_items 2912.
I0302 18:57:49.820877 22683591385216 run.py:483] Algo bellman_ford step 91 current loss 0.357524, current_train_items 2944.
I0302 18:57:49.842784 22683591385216 run.py:483] Algo bellman_ford step 92 current loss 0.493998, current_train_items 2976.
I0302 18:57:49.871329 22683591385216 run.py:483] Algo bellman_ford step 93 current loss 0.689021, current_train_items 3008.
I0302 18:57:49.900774 22683591385216 run.py:483] Algo bellman_ford step 94 current loss 0.631786, current_train_items 3040.
I0302 18:57:49.918225 22683591385216 run.py:483] Algo bellman_ford step 95 current loss 0.099629, current_train_items 3072.
I0302 18:57:49.933466 22683591385216 run.py:483] Algo bellman_ford step 96 current loss 0.253106, current_train_items 3104.
I0302 18:57:49.955241 22683591385216 run.py:483] Algo bellman_ford step 97 current loss 0.437810, current_train_items 3136.
I0302 18:57:49.983159 22683591385216 run.py:483] Algo bellman_ford step 98 current loss 0.502072, current_train_items 3168.
I0302 18:57:50.013158 22683591385216 run.py:483] Algo bellman_ford step 99 current loss 0.720487, current_train_items 3200.
I0302 18:57:50.029968 22683591385216 run.py:483] Algo bellman_ford step 100 current loss 0.090223, current_train_items 3232.
I0302 18:57:50.039435 22683591385216 run.py:503] (val) algo bellman_ford step 100: {'pi': 0.8994140625, 'score': 0.8994140625, 'examples_seen': 3232, 'step': 100, 'algorithm': 'bellman_ford'}
I0302 18:57:50.039547 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.827, current avg val score is 0.899, val scores are: bellman_ford: 0.899
I0302 18:57:50.067962 22683591385216 run.py:483] Algo bellman_ford step 101 current loss 0.260174, current_train_items 3264.
I0302 18:57:50.089722 22683591385216 run.py:483] Algo bellman_ford step 102 current loss 0.324490, current_train_items 3296.
I0302 18:57:50.116877 22683591385216 run.py:483] Algo bellman_ford step 103 current loss 0.464582, current_train_items 3328.
I0302 18:57:50.149097 22683591385216 run.py:483] Algo bellman_ford step 104 current loss 0.592326, current_train_items 3360.
I0302 18:57:50.166843 22683591385216 run.py:483] Algo bellman_ford step 105 current loss 0.060238, current_train_items 3392.
I0302 18:57:50.182289 22683591385216 run.py:483] Algo bellman_ford step 106 current loss 0.249709, current_train_items 3424.
I0302 18:57:50.204263 22683591385216 run.py:483] Algo bellman_ford step 107 current loss 0.395528, current_train_items 3456.
I0302 18:57:50.230993 22683591385216 run.py:483] Algo bellman_ford step 108 current loss 0.413370, current_train_items 3488.
I0302 18:57:50.259578 22683591385216 run.py:483] Algo bellman_ford step 109 current loss 0.463128, current_train_items 3520.
I0302 18:57:50.277168 22683591385216 run.py:483] Algo bellman_ford step 110 current loss 0.110926, current_train_items 3552.
I0302 18:57:50.292364 22683591385216 run.py:483] Algo bellman_ford step 111 current loss 0.171719, current_train_items 3584.
I0302 18:57:50.313632 22683591385216 run.py:483] Algo bellman_ford step 112 current loss 0.273602, current_train_items 3616.
I0302 18:57:50.341560 22683591385216 run.py:483] Algo bellman_ford step 113 current loss 0.416795, current_train_items 3648.
I0302 18:57:50.370535 22683591385216 run.py:483] Algo bellman_ford step 114 current loss 0.623712, current_train_items 3680.
I0302 18:57:50.387654 22683591385216 run.py:483] Algo bellman_ford step 115 current loss 0.046244, current_train_items 3712.
I0302 18:57:50.403314 22683591385216 run.py:483] Algo bellman_ford step 116 current loss 0.199313, current_train_items 3744.
I0302 18:57:50.425816 22683591385216 run.py:483] Algo bellman_ford step 117 current loss 0.360163, current_train_items 3776.
I0302 18:57:50.452690 22683591385216 run.py:483] Algo bellman_ford step 118 current loss 0.570998, current_train_items 3808.
I0302 18:57:50.479942 22683591385216 run.py:483] Algo bellman_ford step 119 current loss 0.737370, current_train_items 3840.
I0302 18:57:50.497305 22683591385216 run.py:483] Algo bellman_ford step 120 current loss 0.094516, current_train_items 3872.
I0302 18:57:50.512826 22683591385216 run.py:483] Algo bellman_ford step 121 current loss 0.254473, current_train_items 3904.
I0302 18:57:50.535166 22683591385216 run.py:483] Algo bellman_ford step 122 current loss 0.387418, current_train_items 3936.
I0302 18:57:50.563314 22683591385216 run.py:483] Algo bellman_ford step 123 current loss 0.586665, current_train_items 3968.
I0302 18:57:50.596486 22683591385216 run.py:483] Algo bellman_ford step 124 current loss 0.797198, current_train_items 4000.
I0302 18:57:50.613737 22683591385216 run.py:483] Algo bellman_ford step 125 current loss 0.137712, current_train_items 4032.
I0302 18:57:50.629239 22683591385216 run.py:483] Algo bellman_ford step 126 current loss 0.227824, current_train_items 4064.
I0302 18:57:50.651191 22683591385216 run.py:483] Algo bellman_ford step 127 current loss 0.336889, current_train_items 4096.
I0302 18:57:50.678622 22683591385216 run.py:483] Algo bellman_ford step 128 current loss 0.407326, current_train_items 4128.
I0302 18:57:50.708402 22683591385216 run.py:483] Algo bellman_ford step 129 current loss 0.399774, current_train_items 4160.
I0302 18:57:50.725521 22683591385216 run.py:483] Algo bellman_ford step 130 current loss 0.094018, current_train_items 4192.
I0302 18:57:50.740655 22683591385216 run.py:483] Algo bellman_ford step 131 current loss 0.174144, current_train_items 4224.
I0302 18:57:50.762824 22683591385216 run.py:483] Algo bellman_ford step 132 current loss 0.399032, current_train_items 4256.
I0302 18:57:50.790243 22683591385216 run.py:483] Algo bellman_ford step 133 current loss 0.449088, current_train_items 4288.
I0302 18:57:50.819389 22683591385216 run.py:483] Algo bellman_ford step 134 current loss 0.501814, current_train_items 4320.
I0302 18:57:50.836269 22683591385216 run.py:483] Algo bellman_ford step 135 current loss 0.053650, current_train_items 4352.
I0302 18:57:50.851465 22683591385216 run.py:483] Algo bellman_ford step 136 current loss 0.196126, current_train_items 4384.
I0302 18:57:50.873652 22683591385216 run.py:483] Algo bellman_ford step 137 current loss 0.392718, current_train_items 4416.
I0302 18:57:50.900929 22683591385216 run.py:483] Algo bellman_ford step 138 current loss 0.348153, current_train_items 4448.
I0302 18:57:50.931359 22683591385216 run.py:483] Algo bellman_ford step 139 current loss 0.529527, current_train_items 4480.
I0302 18:57:50.948223 22683591385216 run.py:483] Algo bellman_ford step 140 current loss 0.043300, current_train_items 4512.
I0302 18:57:50.963732 22683591385216 run.py:483] Algo bellman_ford step 141 current loss 0.237349, current_train_items 4544.
I0302 18:57:50.985374 22683591385216 run.py:483] Algo bellman_ford step 142 current loss 0.443557, current_train_items 4576.
I0302 18:57:51.013473 22683591385216 run.py:483] Algo bellman_ford step 143 current loss 0.591177, current_train_items 4608.
I0302 18:57:51.042480 22683591385216 run.py:483] Algo bellman_ford step 144 current loss 0.587859, current_train_items 4640.
I0302 18:57:51.059495 22683591385216 run.py:483] Algo bellman_ford step 145 current loss 0.046383, current_train_items 4672.
I0302 18:57:51.074690 22683591385216 run.py:483] Algo bellman_ford step 146 current loss 0.186358, current_train_items 4704.
I0302 18:57:51.096039 22683591385216 run.py:483] Algo bellman_ford step 147 current loss 0.280196, current_train_items 4736.
I0302 18:57:51.123107 22683591385216 run.py:483] Algo bellman_ford step 148 current loss 0.392669, current_train_items 4768.
I0302 18:57:51.151031 22683591385216 run.py:483] Algo bellman_ford step 149 current loss 0.491421, current_train_items 4800.
I0302 18:57:51.168173 22683591385216 run.py:483] Algo bellman_ford step 150 current loss 0.034020, current_train_items 4832.
I0302 18:57:51.175769 22683591385216 run.py:503] (val) algo bellman_ford step 150: {'pi': 0.8720703125, 'score': 0.8720703125, 'examples_seen': 4832, 'step': 150, 'algorithm': 'bellman_ford'}
I0302 18:57:51.175885 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.899, current avg val score is 0.872, val scores are: bellman_ford: 0.872
I0302 18:57:51.191271 22683591385216 run.py:483] Algo bellman_ford step 151 current loss 0.196669, current_train_items 4864.
I0302 18:57:51.213574 22683591385216 run.py:483] Algo bellman_ford step 152 current loss 0.433730, current_train_items 4896.
I0302 18:57:51.241065 22683591385216 run.py:483] Algo bellman_ford step 153 current loss 0.468032, current_train_items 4928.
I0302 18:57:51.274527 22683591385216 run.py:483] Algo bellman_ford step 154 current loss 0.669590, current_train_items 4960.
I0302 18:57:51.292182 22683591385216 run.py:483] Algo bellman_ford step 155 current loss 0.132925, current_train_items 4992.
I0302 18:57:51.307847 22683591385216 run.py:483] Algo bellman_ford step 156 current loss 0.364066, current_train_items 5024.
I0302 18:57:51.329833 22683591385216 run.py:483] Algo bellman_ford step 157 current loss 0.433350, current_train_items 5056.
I0302 18:57:51.356104 22683591385216 run.py:483] Algo bellman_ford step 158 current loss 0.368697, current_train_items 5088.
I0302 18:57:51.386168 22683591385216 run.py:483] Algo bellman_ford step 159 current loss 0.477161, current_train_items 5120.
I0302 18:57:51.403759 22683591385216 run.py:483] Algo bellman_ford step 160 current loss 0.134609, current_train_items 5152.
I0302 18:57:51.419382 22683591385216 run.py:483] Algo bellman_ford step 161 current loss 0.316468, current_train_items 5184.
I0302 18:57:51.440515 22683591385216 run.py:483] Algo bellman_ford step 162 current loss 0.549026, current_train_items 5216.
I0302 18:57:51.468228 22683591385216 run.py:483] Algo bellman_ford step 163 current loss 0.434295, current_train_items 5248.
I0302 18:57:51.497462 22683591385216 run.py:483] Algo bellman_ford step 164 current loss 0.580626, current_train_items 5280.
I0302 18:57:51.514976 22683591385216 run.py:483] Algo bellman_ford step 165 current loss 0.056269, current_train_items 5312.
I0302 18:57:51.530734 22683591385216 run.py:483] Algo bellman_ford step 166 current loss 0.488834, current_train_items 5344.
I0302 18:57:51.552128 22683591385216 run.py:483] Algo bellman_ford step 167 current loss 0.580650, current_train_items 5376.
I0302 18:57:51.580249 22683591385216 run.py:483] Algo bellman_ford step 168 current loss 0.450429, current_train_items 5408.
I0302 18:57:51.610088 22683591385216 run.py:483] Algo bellman_ford step 169 current loss 0.450401, current_train_items 5440.
I0302 18:57:51.627462 22683591385216 run.py:483] Algo bellman_ford step 170 current loss 0.091730, current_train_items 5472.
I0302 18:57:51.643104 22683591385216 run.py:483] Algo bellman_ford step 171 current loss 0.272374, current_train_items 5504.
I0302 18:57:51.664254 22683591385216 run.py:483] Algo bellman_ford step 172 current loss 0.404029, current_train_items 5536.
I0302 18:57:51.692427 22683591385216 run.py:483] Algo bellman_ford step 173 current loss 0.470506, current_train_items 5568.
I0302 18:57:51.725753 22683591385216 run.py:483] Algo bellman_ford step 174 current loss 0.637378, current_train_items 5600.
I0302 18:57:51.742980 22683591385216 run.py:483] Algo bellman_ford step 175 current loss 0.090145, current_train_items 5632.
I0302 18:57:51.758491 22683591385216 run.py:483] Algo bellman_ford step 176 current loss 0.297382, current_train_items 5664.
I0302 18:57:51.781284 22683591385216 run.py:483] Algo bellman_ford step 177 current loss 0.479977, current_train_items 5696.
I0302 18:57:51.807456 22683591385216 run.py:483] Algo bellman_ford step 178 current loss 0.283322, current_train_items 5728.
I0302 18:57:51.835888 22683591385216 run.py:483] Algo bellman_ford step 179 current loss 0.476447, current_train_items 5760.
I0302 18:57:51.853635 22683591385216 run.py:483] Algo bellman_ford step 180 current loss 0.089202, current_train_items 5792.
I0302 18:57:51.869246 22683591385216 run.py:483] Algo bellman_ford step 181 current loss 0.211563, current_train_items 5824.
I0302 18:57:51.892169 22683591385216 run.py:483] Algo bellman_ford step 182 current loss 0.401872, current_train_items 5856.
I0302 18:57:51.919941 22683591385216 run.py:483] Algo bellman_ford step 183 current loss 0.386577, current_train_items 5888.
I0302 18:57:51.948339 22683591385216 run.py:483] Algo bellman_ford step 184 current loss 0.511941, current_train_items 5920.
I0302 18:57:51.965606 22683591385216 run.py:483] Algo bellman_ford step 185 current loss 0.076344, current_train_items 5952.
I0302 18:57:51.981482 22683591385216 run.py:483] Algo bellman_ford step 186 current loss 0.212313, current_train_items 5984.
I0302 18:57:52.002659 22683591385216 run.py:483] Algo bellman_ford step 187 current loss 0.326048, current_train_items 6016.
I0302 18:57:52.029946 22683591385216 run.py:483] Algo bellman_ford step 188 current loss 0.464895, current_train_items 6048.
I0302 18:57:52.061472 22683591385216 run.py:483] Algo bellman_ford step 189 current loss 0.472645, current_train_items 6080.
I0302 18:57:52.078929 22683591385216 run.py:483] Algo bellman_ford step 190 current loss 0.080457, current_train_items 6112.
I0302 18:57:52.094647 22683591385216 run.py:483] Algo bellman_ford step 191 current loss 0.343136, current_train_items 6144.
I0302 18:57:52.117449 22683591385216 run.py:483] Algo bellman_ford step 192 current loss 0.567914, current_train_items 6176.
I0302 18:57:52.145213 22683591385216 run.py:483] Algo bellman_ford step 193 current loss 0.409288, current_train_items 6208.
I0302 18:57:52.174791 22683591385216 run.py:483] Algo bellman_ford step 194 current loss 0.450355, current_train_items 6240.
I0302 18:57:52.192654 22683591385216 run.py:483] Algo bellman_ford step 195 current loss 0.101676, current_train_items 6272.
I0302 18:57:52.208288 22683591385216 run.py:483] Algo bellman_ford step 196 current loss 0.227609, current_train_items 6304.
I0302 18:57:52.229746 22683591385216 run.py:483] Algo bellman_ford step 197 current loss 0.393160, current_train_items 6336.
I0302 18:57:52.258282 22683591385216 run.py:483] Algo bellman_ford step 198 current loss 0.622761, current_train_items 6368.
I0302 18:57:52.288589 22683591385216 run.py:483] Algo bellman_ford step 199 current loss 0.505140, current_train_items 6400.
I0302 18:57:52.306149 22683591385216 run.py:483] Algo bellman_ford step 200 current loss 0.080930, current_train_items 6432.
I0302 18:57:52.313462 22683591385216 run.py:503] (val) algo bellman_ford step 200: {'pi': 0.9267578125, 'score': 0.9267578125, 'examples_seen': 6432, 'step': 200, 'algorithm': 'bellman_ford'}
I0302 18:57:52.313569 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.899, current avg val score is 0.927, val scores are: bellman_ford: 0.927
I0302 18:57:52.340532 22683591385216 run.py:483] Algo bellman_ford step 201 current loss 0.190787, current_train_items 6464.
I0302 18:57:52.363438 22683591385216 run.py:483] Algo bellman_ford step 202 current loss 0.567882, current_train_items 6496.
I0302 18:57:52.393444 22683591385216 run.py:483] Algo bellman_ford step 203 current loss 0.539315, current_train_items 6528.
I0302 18:57:52.426538 22683591385216 run.py:483] Algo bellman_ford step 204 current loss 0.535300, current_train_items 6560.
I0302 18:57:52.444450 22683591385216 run.py:483] Algo bellman_ford step 205 current loss 0.078828, current_train_items 6592.
I0302 18:57:52.459120 22683591385216 run.py:483] Algo bellman_ford step 206 current loss 0.100552, current_train_items 6624.
I0302 18:57:52.481798 22683591385216 run.py:483] Algo bellman_ford step 207 current loss 0.549268, current_train_items 6656.
I0302 18:57:52.509798 22683591385216 run.py:483] Algo bellman_ford step 208 current loss 0.533732, current_train_items 6688.
I0302 18:57:52.538882 22683591385216 run.py:483] Algo bellman_ford step 209 current loss 0.539857, current_train_items 6720.
I0302 18:57:52.556380 22683591385216 run.py:483] Algo bellman_ford step 210 current loss 0.147404, current_train_items 6752.
I0302 18:57:52.571741 22683591385216 run.py:483] Algo bellman_ford step 211 current loss 0.216730, current_train_items 6784.
I0302 18:57:52.594521 22683591385216 run.py:483] Algo bellman_ford step 212 current loss 0.679809, current_train_items 6816.
I0302 18:57:52.622696 22683591385216 run.py:483] Algo bellman_ford step 213 current loss 0.514562, current_train_items 6848.
I0302 18:57:52.649287 22683591385216 run.py:483] Algo bellman_ford step 214 current loss 0.306355, current_train_items 6880.
I0302 18:57:52.666697 22683591385216 run.py:483] Algo bellman_ford step 215 current loss 0.032636, current_train_items 6912.
I0302 18:57:52.681831 22683591385216 run.py:483] Algo bellman_ford step 216 current loss 0.201744, current_train_items 6944.
I0302 18:57:52.703499 22683591385216 run.py:483] Algo bellman_ford step 217 current loss 0.332334, current_train_items 6976.
I0302 18:57:52.730973 22683591385216 run.py:483] Algo bellman_ford step 218 current loss 0.580555, current_train_items 7008.
I0302 18:57:52.759927 22683591385216 run.py:483] Algo bellman_ford step 219 current loss 0.553651, current_train_items 7040.
I0302 18:57:52.777218 22683591385216 run.py:483] Algo bellman_ford step 220 current loss 0.039840, current_train_items 7072.
I0302 18:57:52.792513 22683591385216 run.py:483] Algo bellman_ford step 221 current loss 0.150236, current_train_items 7104.
I0302 18:57:52.815316 22683591385216 run.py:483] Algo bellman_ford step 222 current loss 0.369647, current_train_items 7136.
I0302 18:57:52.842373 22683591385216 run.py:483] Algo bellman_ford step 223 current loss 0.365028, current_train_items 7168.
I0302 18:57:52.872632 22683591385216 run.py:483] Algo bellman_ford step 224 current loss 0.448797, current_train_items 7200.
I0302 18:57:52.890092 22683591385216 run.py:483] Algo bellman_ford step 225 current loss 0.065983, current_train_items 7232.
I0302 18:57:52.905534 22683591385216 run.py:483] Algo bellman_ford step 226 current loss 0.166580, current_train_items 7264.
I0302 18:57:52.927840 22683591385216 run.py:483] Algo bellman_ford step 227 current loss 0.405179, current_train_items 7296.
I0302 18:57:52.956130 22683591385216 run.py:483] Algo bellman_ford step 228 current loss 0.302422, current_train_items 7328.
I0302 18:57:52.986582 22683591385216 run.py:483] Algo bellman_ford step 229 current loss 0.394386, current_train_items 7360.
I0302 18:57:53.003923 22683591385216 run.py:483] Algo bellman_ford step 230 current loss 0.023678, current_train_items 7392.
I0302 18:57:53.019150 22683591385216 run.py:483] Algo bellman_ford step 231 current loss 0.142892, current_train_items 7424.
I0302 18:57:53.041526 22683591385216 run.py:483] Algo bellman_ford step 232 current loss 0.309446, current_train_items 7456.
I0302 18:57:53.069444 22683591385216 run.py:483] Algo bellman_ford step 233 current loss 0.496715, current_train_items 7488.
I0302 18:57:53.100887 22683591385216 run.py:483] Algo bellman_ford step 234 current loss 0.387784, current_train_items 7520.
I0302 18:57:53.118592 22683591385216 run.py:483] Algo bellman_ford step 235 current loss 0.075321, current_train_items 7552.
I0302 18:57:53.133616 22683591385216 run.py:483] Algo bellman_ford step 236 current loss 0.160078, current_train_items 7584.
I0302 18:57:53.155242 22683591385216 run.py:483] Algo bellman_ford step 237 current loss 0.265325, current_train_items 7616.
I0302 18:57:53.182976 22683591385216 run.py:483] Algo bellman_ford step 238 current loss 0.317994, current_train_items 7648.
I0302 18:57:53.212939 22683591385216 run.py:483] Algo bellman_ford step 239 current loss 0.350067, current_train_items 7680.
I0302 18:57:53.230384 22683591385216 run.py:483] Algo bellman_ford step 240 current loss 0.077355, current_train_items 7712.
I0302 18:57:53.245785 22683591385216 run.py:483] Algo bellman_ford step 241 current loss 0.184754, current_train_items 7744.
I0302 18:57:53.267547 22683591385216 run.py:483] Algo bellman_ford step 242 current loss 0.287477, current_train_items 7776.
I0302 18:57:53.295022 22683591385216 run.py:483] Algo bellman_ford step 243 current loss 0.237919, current_train_items 7808.
I0302 18:57:53.322469 22683591385216 run.py:483] Algo bellman_ford step 244 current loss 0.289247, current_train_items 7840.
I0302 18:57:53.340009 22683591385216 run.py:483] Algo bellman_ford step 245 current loss 0.051507, current_train_items 7872.
I0302 18:57:53.355111 22683591385216 run.py:483] Algo bellman_ford step 246 current loss 0.092127, current_train_items 7904.
I0302 18:57:53.376429 22683591385216 run.py:483] Algo bellman_ford step 247 current loss 0.196759, current_train_items 7936.
I0302 18:57:53.405179 22683591385216 run.py:483] Algo bellman_ford step 248 current loss 0.347306, current_train_items 7968.
I0302 18:57:53.435415 22683591385216 run.py:483] Algo bellman_ford step 249 current loss 0.294922, current_train_items 8000.
I0302 18:57:53.453009 22683591385216 run.py:483] Algo bellman_ford step 250 current loss 0.053513, current_train_items 8032.
I0302 18:57:53.461321 22683591385216 run.py:503] (val) algo bellman_ford step 250: {'pi': 0.9345703125, 'score': 0.9345703125, 'examples_seen': 8032, 'step': 250, 'algorithm': 'bellman_ford'}
I0302 18:57:53.461431 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.927, current avg val score is 0.935, val scores are: bellman_ford: 0.935
I0302 18:57:53.489221 22683591385216 run.py:483] Algo bellman_ford step 251 current loss 0.160715, current_train_items 8064.
I0302 18:57:53.511856 22683591385216 run.py:483] Algo bellman_ford step 252 current loss 0.247569, current_train_items 8096.
I0302 18:57:53.540776 22683591385216 run.py:483] Algo bellman_ford step 253 current loss 0.406053, current_train_items 8128.
I0302 18:57:53.572195 22683591385216 run.py:483] Algo bellman_ford step 254 current loss 0.461464, current_train_items 8160.
I0302 18:57:53.590560 22683591385216 run.py:483] Algo bellman_ford step 255 current loss 0.088200, current_train_items 8192.
I0302 18:57:53.606142 22683591385216 run.py:483] Algo bellman_ford step 256 current loss 0.108971, current_train_items 8224.
I0302 18:57:53.627442 22683591385216 run.py:483] Algo bellman_ford step 257 current loss 0.210652, current_train_items 8256.
I0302 18:57:53.655170 22683591385216 run.py:483] Algo bellman_ford step 258 current loss 0.213933, current_train_items 8288.
I0302 18:57:53.683966 22683591385216 run.py:483] Algo bellman_ford step 259 current loss 0.274425, current_train_items 8320.
I0302 18:57:53.702251 22683591385216 run.py:483] Algo bellman_ford step 260 current loss 0.032385, current_train_items 8352.
I0302 18:57:53.718118 22683591385216 run.py:483] Algo bellman_ford step 261 current loss 0.140103, current_train_items 8384.
I0302 18:57:53.739437 22683591385216 run.py:483] Algo bellman_ford step 262 current loss 0.184568, current_train_items 8416.
I0302 18:57:53.766669 22683591385216 run.py:483] Algo bellman_ford step 263 current loss 0.306655, current_train_items 8448.
I0302 18:57:53.795306 22683591385216 run.py:483] Algo bellman_ford step 264 current loss 0.274018, current_train_items 8480.
I0302 18:57:53.812705 22683591385216 run.py:483] Algo bellman_ford step 265 current loss 0.049796, current_train_items 8512.
I0302 18:57:53.828152 22683591385216 run.py:483] Algo bellman_ford step 266 current loss 0.202368, current_train_items 8544.
I0302 18:57:53.851259 22683591385216 run.py:483] Algo bellman_ford step 267 current loss 0.418071, current_train_items 8576.
I0302 18:57:53.879889 22683591385216 run.py:483] Algo bellman_ford step 268 current loss 0.395978, current_train_items 8608.
I0302 18:57:53.908387 22683591385216 run.py:483] Algo bellman_ford step 269 current loss 0.285439, current_train_items 8640.
I0302 18:57:53.926481 22683591385216 run.py:483] Algo bellman_ford step 270 current loss 0.064600, current_train_items 8672.
I0302 18:57:53.941581 22683591385216 run.py:483] Algo bellman_ford step 271 current loss 0.146326, current_train_items 8704.
I0302 18:57:53.964312 22683591385216 run.py:483] Algo bellman_ford step 272 current loss 0.320917, current_train_items 8736.
I0302 18:57:53.992038 22683591385216 run.py:483] Algo bellman_ford step 273 current loss 0.420703, current_train_items 8768.
I0302 18:57:54.020486 22683591385216 run.py:483] Algo bellman_ford step 274 current loss 0.374488, current_train_items 8800.
I0302 18:57:54.038604 22683591385216 run.py:483] Algo bellman_ford step 275 current loss 0.064590, current_train_items 8832.
I0302 18:57:54.054096 22683591385216 run.py:483] Algo bellman_ford step 276 current loss 0.204151, current_train_items 8864.
I0302 18:57:54.077106 22683591385216 run.py:483] Algo bellman_ford step 277 current loss 0.514017, current_train_items 8896.
I0302 18:57:54.105706 22683591385216 run.py:483] Algo bellman_ford step 278 current loss 0.470718, current_train_items 8928.
I0302 18:57:54.135273 22683591385216 run.py:483] Algo bellman_ford step 279 current loss 0.313219, current_train_items 8960.
I0302 18:57:54.152496 22683591385216 run.py:483] Algo bellman_ford step 280 current loss 0.082530, current_train_items 8992.
I0302 18:57:54.168054 22683591385216 run.py:483] Algo bellman_ford step 281 current loss 0.286233, current_train_items 9024.
I0302 18:57:54.190748 22683591385216 run.py:483] Algo bellman_ford step 282 current loss 0.466011, current_train_items 9056.
I0302 18:57:54.218150 22683591385216 run.py:483] Algo bellman_ford step 283 current loss 0.463060, current_train_items 9088.
I0302 18:57:54.249224 22683591385216 run.py:483] Algo bellman_ford step 284 current loss 0.494474, current_train_items 9120.
I0302 18:57:54.267030 22683591385216 run.py:483] Algo bellman_ford step 285 current loss 0.091553, current_train_items 9152.
I0302 18:57:54.282703 22683591385216 run.py:483] Algo bellman_ford step 286 current loss 0.196176, current_train_items 9184.
I0302 18:57:54.304221 22683591385216 run.py:483] Algo bellman_ford step 287 current loss 0.407482, current_train_items 9216.
I0302 18:57:54.332883 22683591385216 run.py:483] Algo bellman_ford step 288 current loss 0.567502, current_train_items 9248.
I0302 18:57:54.363667 22683591385216 run.py:483] Algo bellman_ford step 289 current loss 0.493280, current_train_items 9280.
I0302 18:57:54.381315 22683591385216 run.py:483] Algo bellman_ford step 290 current loss 0.021190, current_train_items 9312.
I0302 18:57:54.396812 22683591385216 run.py:483] Algo bellman_ford step 291 current loss 0.098665, current_train_items 9344.
I0302 18:57:54.418768 22683591385216 run.py:483] Algo bellman_ford step 292 current loss 0.191612, current_train_items 9376.
I0302 18:57:54.446247 22683591385216 run.py:483] Algo bellman_ford step 293 current loss 0.345261, current_train_items 9408.
I0302 18:57:54.475987 22683591385216 run.py:483] Algo bellman_ford step 294 current loss 0.304756, current_train_items 9440.
I0302 18:57:54.493081 22683591385216 run.py:483] Algo bellman_ford step 295 current loss 0.034551, current_train_items 9472.
I0302 18:57:54.508021 22683591385216 run.py:483] Algo bellman_ford step 296 current loss 0.129227, current_train_items 9504.
I0302 18:57:54.530713 22683591385216 run.py:483] Algo bellman_ford step 297 current loss 0.323053, current_train_items 9536.
I0302 18:57:54.559880 22683591385216 run.py:483] Algo bellman_ford step 298 current loss 0.345162, current_train_items 9568.
I0302 18:57:54.588830 22683591385216 run.py:483] Algo bellman_ford step 299 current loss 0.346482, current_train_items 9600.
I0302 18:57:54.606747 22683591385216 run.py:483] Algo bellman_ford step 300 current loss 0.053484, current_train_items 9632.
I0302 18:57:54.614543 22683591385216 run.py:503] (val) algo bellman_ford step 300: {'pi': 0.908203125, 'score': 0.908203125, 'examples_seen': 9632, 'step': 300, 'algorithm': 'bellman_ford'}
I0302 18:57:54.614649 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.935, current avg val score is 0.908, val scores are: bellman_ford: 0.908
I0302 18:57:54.630648 22683591385216 run.py:483] Algo bellman_ford step 301 current loss 0.145038, current_train_items 9664.
I0302 18:57:54.652607 22683591385216 run.py:483] Algo bellman_ford step 302 current loss 0.276402, current_train_items 9696.
I0302 18:57:54.679982 22683591385216 run.py:483] Algo bellman_ford step 303 current loss 0.242090, current_train_items 9728.
I0302 18:57:54.710573 22683591385216 run.py:483] Algo bellman_ford step 304 current loss 0.526632, current_train_items 9760.
I0302 18:57:54.728412 22683591385216 run.py:483] Algo bellman_ford step 305 current loss 0.065894, current_train_items 9792.
I0302 18:57:54.744231 22683591385216 run.py:483] Algo bellman_ford step 306 current loss 0.255480, current_train_items 9824.
I0302 18:57:54.767322 22683591385216 run.py:483] Algo bellman_ford step 307 current loss 0.479547, current_train_items 9856.
I0302 18:57:54.795454 22683591385216 run.py:483] Algo bellman_ford step 308 current loss 0.539264, current_train_items 9888.
I0302 18:57:54.825000 22683591385216 run.py:483] Algo bellman_ford step 309 current loss 0.421060, current_train_items 9920.
I0302 18:57:54.842686 22683591385216 run.py:483] Algo bellman_ford step 310 current loss 0.121089, current_train_items 9952.
I0302 18:57:54.857886 22683591385216 run.py:483] Algo bellman_ford step 311 current loss 0.161183, current_train_items 9984.
I0302 18:57:54.879884 22683591385216 run.py:483] Algo bellman_ford step 312 current loss 0.283041, current_train_items 10016.
I0302 18:57:54.908500 22683591385216 run.py:483] Algo bellman_ford step 313 current loss 0.499919, current_train_items 10048.
I0302 18:57:54.938431 22683591385216 run.py:483] Algo bellman_ford step 314 current loss 0.535739, current_train_items 10080.
I0302 18:57:54.955831 22683591385216 run.py:483] Algo bellman_ford step 315 current loss 0.051081, current_train_items 10112.
I0302 18:57:54.971273 22683591385216 run.py:483] Algo bellman_ford step 316 current loss 0.099679, current_train_items 10144.
I0302 18:57:54.992998 22683591385216 run.py:483] Algo bellman_ford step 317 current loss 0.212809, current_train_items 10176.
I0302 18:57:55.019906 22683591385216 run.py:483] Algo bellman_ford step 318 current loss 0.280207, current_train_items 10208.
I0302 18:57:55.049074 22683591385216 run.py:483] Algo bellman_ford step 319 current loss 0.409337, current_train_items 10240.
I0302 18:57:55.066217 22683591385216 run.py:483] Algo bellman_ford step 320 current loss 0.047815, current_train_items 10272.
I0302 18:57:55.081645 22683591385216 run.py:483] Algo bellman_ford step 321 current loss 0.189612, current_train_items 10304.
I0302 18:57:55.103840 22683591385216 run.py:483] Algo bellman_ford step 322 current loss 0.269625, current_train_items 10336.
I0302 18:57:55.130965 22683591385216 run.py:483] Algo bellman_ford step 323 current loss 0.222320, current_train_items 10368.
I0302 18:57:55.160811 22683591385216 run.py:483] Algo bellman_ford step 324 current loss 0.418969, current_train_items 10400.
I0302 18:57:55.178404 22683591385216 run.py:483] Algo bellman_ford step 325 current loss 0.069657, current_train_items 10432.
I0302 18:57:55.193392 22683591385216 run.py:483] Algo bellman_ford step 326 current loss 0.069940, current_train_items 10464.
I0302 18:57:55.215337 22683591385216 run.py:483] Algo bellman_ford step 327 current loss 0.197965, current_train_items 10496.
I0302 18:57:55.243871 22683591385216 run.py:483] Algo bellman_ford step 328 current loss 0.283040, current_train_items 10528.
I0302 18:57:55.274490 22683591385216 run.py:483] Algo bellman_ford step 329 current loss 0.361202, current_train_items 10560.
I0302 18:57:55.291830 22683591385216 run.py:483] Algo bellman_ford step 330 current loss 0.081431, current_train_items 10592.
I0302 18:57:55.307078 22683591385216 run.py:483] Algo bellman_ford step 331 current loss 0.144464, current_train_items 10624.
I0302 18:57:55.329379 22683591385216 run.py:483] Algo bellman_ford step 332 current loss 0.235957, current_train_items 10656.
I0302 18:57:55.357944 22683591385216 run.py:483] Algo bellman_ford step 333 current loss 0.253261, current_train_items 10688.
I0302 18:57:55.389529 22683591385216 run.py:483] Algo bellman_ford step 334 current loss 0.320659, current_train_items 10720.
I0302 18:57:55.407072 22683591385216 run.py:483] Algo bellman_ford step 335 current loss 0.041283, current_train_items 10752.
I0302 18:57:55.422821 22683591385216 run.py:483] Algo bellman_ford step 336 current loss 0.154839, current_train_items 10784.
I0302 18:57:55.445997 22683591385216 run.py:483] Algo bellman_ford step 337 current loss 0.236738, current_train_items 10816.
I0302 18:57:55.473819 22683591385216 run.py:483] Algo bellman_ford step 338 current loss 0.194107, current_train_items 10848.
I0302 18:57:55.504611 22683591385216 run.py:483] Algo bellman_ford step 339 current loss 0.332807, current_train_items 10880.
I0302 18:57:55.522215 22683591385216 run.py:483] Algo bellman_ford step 340 current loss 0.038695, current_train_items 10912.
I0302 18:57:55.537289 22683591385216 run.py:483] Algo bellman_ford step 341 current loss 0.174454, current_train_items 10944.
I0302 18:57:55.559336 22683591385216 run.py:483] Algo bellman_ford step 342 current loss 0.305786, current_train_items 10976.
I0302 18:57:55.586056 22683591385216 run.py:483] Algo bellman_ford step 343 current loss 0.204078, current_train_items 11008.
I0302 18:57:55.616314 22683591385216 run.py:483] Algo bellman_ford step 344 current loss 0.314257, current_train_items 11040.
I0302 18:57:55.634243 22683591385216 run.py:483] Algo bellman_ford step 345 current loss 0.074376, current_train_items 11072.
I0302 18:57:55.649565 22683591385216 run.py:483] Algo bellman_ford step 346 current loss 0.180071, current_train_items 11104.
I0302 18:57:55.673060 22683591385216 run.py:483] Algo bellman_ford step 347 current loss 0.333265, current_train_items 11136.
I0302 18:57:55.700833 22683591385216 run.py:483] Algo bellman_ford step 348 current loss 0.278278, current_train_items 11168.
I0302 18:57:55.731860 22683591385216 run.py:483] Algo bellman_ford step 349 current loss 0.330036, current_train_items 11200.
I0302 18:57:55.749477 22683591385216 run.py:483] Algo bellman_ford step 350 current loss 0.060028, current_train_items 11232.
I0302 18:57:55.757220 22683591385216 run.py:503] (val) algo bellman_ford step 350: {'pi': 0.9326171875, 'score': 0.9326171875, 'examples_seen': 11232, 'step': 350, 'algorithm': 'bellman_ford'}
I0302 18:57:55.757326 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.935, current avg val score is 0.933, val scores are: bellman_ford: 0.933
I0302 18:57:55.772864 22683591385216 run.py:483] Algo bellman_ford step 351 current loss 0.206463, current_train_items 11264.
I0302 18:57:55.795563 22683591385216 run.py:483] Algo bellman_ford step 352 current loss 0.288407, current_train_items 11296.
I0302 18:57:55.824276 22683591385216 run.py:483] Algo bellman_ford step 353 current loss 0.206633, current_train_items 11328.
I0302 18:57:55.856451 22683591385216 run.py:483] Algo bellman_ford step 354 current loss 0.345440, current_train_items 11360.
I0302 18:57:55.874387 22683591385216 run.py:483] Algo bellman_ford step 355 current loss 0.065988, current_train_items 11392.
I0302 18:57:55.890097 22683591385216 run.py:483] Algo bellman_ford step 356 current loss 0.272021, current_train_items 11424.
I0302 18:57:55.912023 22683591385216 run.py:483] Algo bellman_ford step 357 current loss 0.288042, current_train_items 11456.
I0302 18:57:55.940884 22683591385216 run.py:483] Algo bellman_ford step 358 current loss 0.344488, current_train_items 11488.
I0302 18:57:55.972455 22683591385216 run.py:483] Algo bellman_ford step 359 current loss 0.261650, current_train_items 11520.
I0302 18:57:55.989711 22683591385216 run.py:483] Algo bellman_ford step 360 current loss 0.060698, current_train_items 11552.
I0302 18:57:56.005484 22683591385216 run.py:483] Algo bellman_ford step 361 current loss 0.106018, current_train_items 11584.
I0302 18:57:56.027179 22683591385216 run.py:483] Algo bellman_ford step 362 current loss 0.325458, current_train_items 11616.
I0302 18:57:56.055329 22683591385216 run.py:483] Algo bellman_ford step 363 current loss 0.332920, current_train_items 11648.
I0302 18:57:56.083171 22683591385216 run.py:483] Algo bellman_ford step 364 current loss 0.261301, current_train_items 11680.
I0302 18:57:56.100908 22683591385216 run.py:483] Algo bellman_ford step 365 current loss 0.047738, current_train_items 11712.
I0302 18:57:56.116399 22683591385216 run.py:483] Algo bellman_ford step 366 current loss 0.227545, current_train_items 11744.
I0302 18:57:56.137943 22683591385216 run.py:483] Algo bellman_ford step 367 current loss 0.261401, current_train_items 11776.
I0302 18:57:56.166449 22683591385216 run.py:483] Algo bellman_ford step 368 current loss 0.438498, current_train_items 11808.
I0302 18:57:56.194444 22683591385216 run.py:483] Algo bellman_ford step 369 current loss 0.250181, current_train_items 11840.
I0302 18:57:56.211885 22683591385216 run.py:483] Algo bellman_ford step 370 current loss 0.037508, current_train_items 11872.
I0302 18:57:56.227176 22683591385216 run.py:483] Algo bellman_ford step 371 current loss 0.136266, current_train_items 11904.
I0302 18:57:56.249260 22683591385216 run.py:483] Algo bellman_ford step 372 current loss 0.285487, current_train_items 11936.
I0302 18:57:56.277752 22683591385216 run.py:483] Algo bellman_ford step 373 current loss 0.547855, current_train_items 11968.
I0302 18:57:56.310211 22683591385216 run.py:483] Algo bellman_ford step 374 current loss 0.659083, current_train_items 12000.
I0302 18:57:56.327695 22683591385216 run.py:483] Algo bellman_ford step 375 current loss 0.008787, current_train_items 12032.
I0302 18:57:56.342823 22683591385216 run.py:483] Algo bellman_ford step 376 current loss 0.045824, current_train_items 12064.
I0302 18:57:56.365992 22683591385216 run.py:483] Algo bellman_ford step 377 current loss 0.227455, current_train_items 12096.
I0302 18:57:56.393846 22683591385216 run.py:483] Algo bellman_ford step 378 current loss 0.212371, current_train_items 12128.
I0302 18:57:56.424617 22683591385216 run.py:483] Algo bellman_ford step 379 current loss 0.358942, current_train_items 12160.
I0302 18:57:56.442420 22683591385216 run.py:483] Algo bellman_ford step 380 current loss 0.037052, current_train_items 12192.
I0302 18:57:56.458252 22683591385216 run.py:483] Algo bellman_ford step 381 current loss 0.138160, current_train_items 12224.
I0302 18:57:56.480846 22683591385216 run.py:483] Algo bellman_ford step 382 current loss 0.212495, current_train_items 12256.
I0302 18:57:56.509210 22683591385216 run.py:483] Algo bellman_ford step 383 current loss 0.249989, current_train_items 12288.
I0302 18:57:56.539959 22683591385216 run.py:483] Algo bellman_ford step 384 current loss 0.270054, current_train_items 12320.
I0302 18:57:56.557531 22683591385216 run.py:483] Algo bellman_ford step 385 current loss 0.039326, current_train_items 12352.
I0302 18:57:56.573243 22683591385216 run.py:483] Algo bellman_ford step 386 current loss 0.071330, current_train_items 12384.
I0302 18:57:56.595265 22683591385216 run.py:483] Algo bellman_ford step 387 current loss 0.187893, current_train_items 12416.
I0302 18:57:56.622419 22683591385216 run.py:483] Algo bellman_ford step 388 current loss 0.377367, current_train_items 12448.
I0302 18:57:56.653631 22683591385216 run.py:483] Algo bellman_ford step 389 current loss 0.379651, current_train_items 12480.
I0302 18:57:56.670818 22683591385216 run.py:483] Algo bellman_ford step 390 current loss 0.031778, current_train_items 12512.
I0302 18:57:56.686203 22683591385216 run.py:483] Algo bellman_ford step 391 current loss 0.091621, current_train_items 12544.
I0302 18:57:56.708790 22683591385216 run.py:483] Algo bellman_ford step 392 current loss 0.195390, current_train_items 12576.
I0302 18:57:56.738419 22683591385216 run.py:483] Algo bellman_ford step 393 current loss 0.262019, current_train_items 12608.
I0302 18:57:56.767720 22683591385216 run.py:483] Algo bellman_ford step 394 current loss 0.282374, current_train_items 12640.
I0302 18:57:56.785542 22683591385216 run.py:483] Algo bellman_ford step 395 current loss 0.015470, current_train_items 12672.
I0302 18:57:56.801152 22683591385216 run.py:483] Algo bellman_ford step 396 current loss 0.109541, current_train_items 12704.
I0302 18:57:56.824001 22683591385216 run.py:483] Algo bellman_ford step 397 current loss 0.255000, current_train_items 12736.
I0302 18:57:56.852977 22683591385216 run.py:483] Algo bellman_ford step 398 current loss 0.257903, current_train_items 12768.
I0302 18:57:56.881216 22683591385216 run.py:483] Algo bellman_ford step 399 current loss 0.293423, current_train_items 12800.
I0302 18:57:56.898785 22683591385216 run.py:483] Algo bellman_ford step 400 current loss 0.024356, current_train_items 12832.
I0302 18:57:56.906410 22683591385216 run.py:503] (val) algo bellman_ford step 400: {'pi': 0.951171875, 'score': 0.951171875, 'examples_seen': 12832, 'step': 400, 'algorithm': 'bellman_ford'}
I0302 18:57:56.906538 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.935, current avg val score is 0.951, val scores are: bellman_ford: 0.951
I0302 18:57:56.934657 22683591385216 run.py:483] Algo bellman_ford step 401 current loss 0.106016, current_train_items 12864.
I0302 18:57:56.957045 22683591385216 run.py:483] Algo bellman_ford step 402 current loss 0.192224, current_train_items 12896.
I0302 18:57:56.985676 22683591385216 run.py:483] Algo bellman_ford step 403 current loss 0.198515, current_train_items 12928.
I0302 18:57:57.015031 22683591385216 run.py:483] Algo bellman_ford step 404 current loss 0.302222, current_train_items 12960.
I0302 18:57:57.033279 22683591385216 run.py:483] Algo bellman_ford step 405 current loss 0.053596, current_train_items 12992.
I0302 18:57:57.047993 22683591385216 run.py:483] Algo bellman_ford step 406 current loss 0.094254, current_train_items 13024.
I0302 18:57:57.069806 22683591385216 run.py:483] Algo bellman_ford step 407 current loss 0.202164, current_train_items 13056.
I0302 18:57:57.098651 22683591385216 run.py:483] Algo bellman_ford step 408 current loss 0.241032, current_train_items 13088.
I0302 18:57:57.128497 22683591385216 run.py:483] Algo bellman_ford step 409 current loss 0.259951, current_train_items 13120.
I0302 18:57:57.145994 22683591385216 run.py:483] Algo bellman_ford step 410 current loss 0.032745, current_train_items 13152.
I0302 18:57:57.161129 22683591385216 run.py:483] Algo bellman_ford step 411 current loss 0.070055, current_train_items 13184.
I0302 18:57:57.183073 22683591385216 run.py:483] Algo bellman_ford step 412 current loss 0.175967, current_train_items 13216.
I0302 18:57:57.211121 22683591385216 run.py:483] Algo bellman_ford step 413 current loss 0.176005, current_train_items 13248.
I0302 18:57:57.241433 22683591385216 run.py:483] Algo bellman_ford step 414 current loss 0.245915, current_train_items 13280.
I0302 18:57:57.259120 22683591385216 run.py:483] Algo bellman_ford step 415 current loss 0.027678, current_train_items 13312.
I0302 18:57:57.274168 22683591385216 run.py:483] Algo bellman_ford step 416 current loss 0.043515, current_train_items 13344.
I0302 18:57:57.296354 22683591385216 run.py:483] Algo bellman_ford step 417 current loss 0.141869, current_train_items 13376.
I0302 18:57:57.326333 22683591385216 run.py:483] Algo bellman_ford step 418 current loss 0.277191, current_train_items 13408.
I0302 18:57:57.355914 22683591385216 run.py:483] Algo bellman_ford step 419 current loss 0.326688, current_train_items 13440.
I0302 18:57:57.373015 22683591385216 run.py:483] Algo bellman_ford step 420 current loss 0.090817, current_train_items 13472.
I0302 18:57:57.388096 22683591385216 run.py:483] Algo bellman_ford step 421 current loss 0.067545, current_train_items 13504.
I0302 18:57:57.410582 22683591385216 run.py:483] Algo bellman_ford step 422 current loss 0.103853, current_train_items 13536.
I0302 18:57:57.437503 22683591385216 run.py:483] Algo bellman_ford step 423 current loss 0.201272, current_train_items 13568.
I0302 18:57:57.467256 22683591385216 run.py:483] Algo bellman_ford step 424 current loss 0.290527, current_train_items 13600.
I0302 18:57:57.484450 22683591385216 run.py:483] Algo bellman_ford step 425 current loss 0.097882, current_train_items 13632.
I0302 18:57:57.499326 22683591385216 run.py:483] Algo bellman_ford step 426 current loss 0.046587, current_train_items 13664.
I0302 18:57:57.520929 22683591385216 run.py:483] Algo bellman_ford step 427 current loss 0.287386, current_train_items 13696.
I0302 18:57:57.548090 22683591385216 run.py:483] Algo bellman_ford step 428 current loss 0.292150, current_train_items 13728.
I0302 18:57:57.577921 22683591385216 run.py:483] Algo bellman_ford step 429 current loss 0.230677, current_train_items 13760.
I0302 18:57:57.594892 22683591385216 run.py:483] Algo bellman_ford step 430 current loss 0.043241, current_train_items 13792.
I0302 18:57:57.610114 22683591385216 run.py:483] Algo bellman_ford step 431 current loss 0.131196, current_train_items 13824.
I0302 18:57:57.632423 22683591385216 run.py:483] Algo bellman_ford step 432 current loss 0.360835, current_train_items 13856.
I0302 18:57:57.659087 22683591385216 run.py:483] Algo bellman_ford step 433 current loss 0.251155, current_train_items 13888.
I0302 18:57:57.688551 22683591385216 run.py:483] Algo bellman_ford step 434 current loss 0.280283, current_train_items 13920.
I0302 18:57:57.705538 22683591385216 run.py:483] Algo bellman_ford step 435 current loss 0.064537, current_train_items 13952.
I0302 18:57:57.721142 22683591385216 run.py:483] Algo bellman_ford step 436 current loss 0.130687, current_train_items 13984.
I0302 18:57:57.742385 22683591385216 run.py:483] Algo bellman_ford step 437 current loss 0.254975, current_train_items 14016.
I0302 18:57:57.769291 22683591385216 run.py:483] Algo bellman_ford step 438 current loss 0.204181, current_train_items 14048.
I0302 18:57:57.798893 22683591385216 run.py:483] Algo bellman_ford step 439 current loss 0.238267, current_train_items 14080.
I0302 18:57:57.815813 22683591385216 run.py:483] Algo bellman_ford step 440 current loss 0.036631, current_train_items 14112.
I0302 18:57:57.830759 22683591385216 run.py:483] Algo bellman_ford step 441 current loss 0.141514, current_train_items 14144.
I0302 18:57:57.852177 22683591385216 run.py:483] Algo bellman_ford step 442 current loss 0.192957, current_train_items 14176.
I0302 18:57:57.879674 22683591385216 run.py:483] Algo bellman_ford step 443 current loss 0.263170, current_train_items 14208.
I0302 18:57:57.908448 22683591385216 run.py:483] Algo bellman_ford step 444 current loss 0.238164, current_train_items 14240.
I0302 18:57:57.925801 22683591385216 run.py:483] Algo bellman_ford step 445 current loss 0.046070, current_train_items 14272.
I0302 18:57:57.941918 22683591385216 run.py:483] Algo bellman_ford step 446 current loss 0.270721, current_train_items 14304.
I0302 18:57:57.964301 22683591385216 run.py:483] Algo bellman_ford step 447 current loss 0.364856, current_train_items 14336.
I0302 18:57:57.990484 22683591385216 run.py:483] Algo bellman_ford step 448 current loss 0.291180, current_train_items 14368.
I0302 18:57:58.018280 22683591385216 run.py:483] Algo bellman_ford step 449 current loss 0.224185, current_train_items 14400.
I0302 18:57:58.035805 22683591385216 run.py:483] Algo bellman_ford step 450 current loss 0.055882, current_train_items 14432.
I0302 18:57:58.043742 22683591385216 run.py:503] (val) algo bellman_ford step 450: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 14432, 'step': 450, 'algorithm': 'bellman_ford'}
I0302 18:57:58.043848 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0302 18:57:58.060061 22683591385216 run.py:483] Algo bellman_ford step 451 current loss 0.120312, current_train_items 14464.
I0302 18:57:58.082099 22683591385216 run.py:483] Algo bellman_ford step 452 current loss 0.156394, current_train_items 14496.
I0302 18:57:58.111442 22683591385216 run.py:483] Algo bellman_ford step 453 current loss 0.261093, current_train_items 14528.
I0302 18:57:58.141689 22683591385216 run.py:483] Algo bellman_ford step 454 current loss 0.272409, current_train_items 14560.
I0302 18:57:58.159139 22683591385216 run.py:483] Algo bellman_ford step 455 current loss 0.080063, current_train_items 14592.
I0302 18:57:58.175064 22683591385216 run.py:483] Algo bellman_ford step 456 current loss 0.308451, current_train_items 14624.
I0302 18:57:58.197076 22683591385216 run.py:483] Algo bellman_ford step 457 current loss 0.363082, current_train_items 14656.
I0302 18:57:58.225089 22683591385216 run.py:483] Algo bellman_ford step 458 current loss 0.330112, current_train_items 14688.
I0302 18:57:58.253928 22683591385216 run.py:483] Algo bellman_ford step 459 current loss 0.328201, current_train_items 14720.
I0302 18:57:58.271287 22683591385216 run.py:483] Algo bellman_ford step 460 current loss 0.015478, current_train_items 14752.
I0302 18:57:58.287145 22683591385216 run.py:483] Algo bellman_ford step 461 current loss 0.100978, current_train_items 14784.
I0302 18:57:58.309885 22683591385216 run.py:483] Algo bellman_ford step 462 current loss 0.340600, current_train_items 14816.
I0302 18:57:58.338119 22683591385216 run.py:483] Algo bellman_ford step 463 current loss 0.434257, current_train_items 14848.
I0302 18:57:58.369647 22683591385216 run.py:483] Algo bellman_ford step 464 current loss 0.686314, current_train_items 14880.
I0302 18:57:58.387127 22683591385216 run.py:483] Algo bellman_ford step 465 current loss 0.054286, current_train_items 14912.
I0302 18:57:58.402230 22683591385216 run.py:483] Algo bellman_ford step 466 current loss 0.100676, current_train_items 14944.
I0302 18:57:58.425061 22683591385216 run.py:483] Algo bellman_ford step 467 current loss 0.202422, current_train_items 14976.
I0302 18:57:58.451764 22683591385216 run.py:483] Algo bellman_ford step 468 current loss 0.217114, current_train_items 15008.
I0302 18:57:58.483118 22683591385216 run.py:483] Algo bellman_ford step 469 current loss 0.392524, current_train_items 15040.
I0302 18:57:58.500135 22683591385216 run.py:483] Algo bellman_ford step 470 current loss 0.132909, current_train_items 15072.
I0302 18:57:58.515584 22683591385216 run.py:483] Algo bellman_ford step 471 current loss 0.100640, current_train_items 15104.
I0302 18:57:58.538477 22683591385216 run.py:483] Algo bellman_ford step 472 current loss 0.205601, current_train_items 15136.
I0302 18:57:58.567276 22683591385216 run.py:483] Algo bellman_ford step 473 current loss 0.327427, current_train_items 15168.
I0302 18:57:58.598598 22683591385216 run.py:483] Algo bellman_ford step 474 current loss 0.606259, current_train_items 15200.
I0302 18:57:58.616347 22683591385216 run.py:483] Algo bellman_ford step 475 current loss 0.018607, current_train_items 15232.
I0302 18:57:58.631621 22683591385216 run.py:483] Algo bellman_ford step 476 current loss 0.118429, current_train_items 15264.
I0302 18:57:58.654572 22683591385216 run.py:483] Algo bellman_ford step 477 current loss 0.211982, current_train_items 15296.
I0302 18:57:58.681159 22683591385216 run.py:483] Algo bellman_ford step 478 current loss 0.164160, current_train_items 15328.
I0302 18:57:58.712113 22683591385216 run.py:483] Algo bellman_ford step 479 current loss 0.309438, current_train_items 15360.
I0302 18:57:58.729459 22683591385216 run.py:483] Algo bellman_ford step 480 current loss 0.041911, current_train_items 15392.
I0302 18:57:58.744990 22683591385216 run.py:483] Algo bellman_ford step 481 current loss 0.091678, current_train_items 15424.
I0302 18:57:58.766731 22683591385216 run.py:483] Algo bellman_ford step 482 current loss 0.151248, current_train_items 15456.
I0302 18:57:58.794892 22683591385216 run.py:483] Algo bellman_ford step 483 current loss 0.269126, current_train_items 15488.
I0302 18:57:58.825602 22683591385216 run.py:483] Algo bellman_ford step 484 current loss 0.321138, current_train_items 15520.
I0302 18:57:58.842573 22683591385216 run.py:483] Algo bellman_ford step 485 current loss 0.028154, current_train_items 15552.
I0302 18:57:58.858092 22683591385216 run.py:483] Algo bellman_ford step 486 current loss 0.085782, current_train_items 15584.
I0302 18:57:58.880810 22683591385216 run.py:483] Algo bellman_ford step 487 current loss 0.221201, current_train_items 15616.
I0302 18:57:58.908563 22683591385216 run.py:483] Algo bellman_ford step 488 current loss 0.171041, current_train_items 15648.
I0302 18:57:58.937783 22683591385216 run.py:483] Algo bellman_ford step 489 current loss 0.226426, current_train_items 15680.
I0302 18:57:58.955018 22683591385216 run.py:483] Algo bellman_ford step 490 current loss 0.026196, current_train_items 15712.
I0302 18:57:58.970706 22683591385216 run.py:483] Algo bellman_ford step 491 current loss 0.065676, current_train_items 15744.
I0302 18:57:58.992806 22683591385216 run.py:483] Algo bellman_ford step 492 current loss 0.142349, current_train_items 15776.
I0302 18:57:59.020611 22683591385216 run.py:483] Algo bellman_ford step 493 current loss 0.188619, current_train_items 15808.
I0302 18:57:59.051753 22683591385216 run.py:483] Algo bellman_ford step 494 current loss 0.298532, current_train_items 15840.
I0302 18:57:59.069188 22683591385216 run.py:483] Algo bellman_ford step 495 current loss 0.010509, current_train_items 15872.
I0302 18:57:59.084449 22683591385216 run.py:483] Algo bellman_ford step 496 current loss 0.059251, current_train_items 15904.
I0302 18:57:59.107996 22683591385216 run.py:483] Algo bellman_ford step 497 current loss 0.299754, current_train_items 15936.
I0302 18:57:59.137121 22683591385216 run.py:483] Algo bellman_ford step 498 current loss 0.214802, current_train_items 15968.
I0302 18:57:59.167144 22683591385216 run.py:483] Algo bellman_ford step 499 current loss 0.213469, current_train_items 16000.
I0302 18:57:59.184337 22683591385216 run.py:483] Algo bellman_ford step 500 current loss 0.025240, current_train_items 16032.
I0302 18:57:59.191994 22683591385216 run.py:503] (val) algo bellman_ford step 500: {'pi': 0.9443359375, 'score': 0.9443359375, 'examples_seen': 16032, 'step': 500, 'algorithm': 'bellman_ford'}
I0302 18:57:59.192100 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.944, val scores are: bellman_ford: 0.944
I0302 18:57:59.207410 22683591385216 run.py:483] Algo bellman_ford step 501 current loss 0.080809, current_train_items 16064.
I0302 18:57:59.229908 22683591385216 run.py:483] Algo bellman_ford step 502 current loss 0.222824, current_train_items 16096.
I0302 18:57:59.259112 22683591385216 run.py:483] Algo bellman_ford step 503 current loss 0.225000, current_train_items 16128.
I0302 18:57:59.291964 22683591385216 run.py:483] Algo bellman_ford step 504 current loss 0.278356, current_train_items 16160.
I0302 18:57:59.309475 22683591385216 run.py:483] Algo bellman_ford step 505 current loss 0.017272, current_train_items 16192.
I0302 18:57:59.324102 22683591385216 run.py:483] Algo bellman_ford step 506 current loss 0.037890, current_train_items 16224.
I0302 18:57:59.345155 22683591385216 run.py:483] Algo bellman_ford step 507 current loss 0.080416, current_train_items 16256.
I0302 18:57:59.373795 22683591385216 run.py:483] Algo bellman_ford step 508 current loss 0.173847, current_train_items 16288.
I0302 18:57:59.404409 22683591385216 run.py:483] Algo bellman_ford step 509 current loss 0.307547, current_train_items 16320.
I0302 18:57:59.421736 22683591385216 run.py:483] Algo bellman_ford step 510 current loss 0.022310, current_train_items 16352.
I0302 18:57:59.437114 22683591385216 run.py:483] Algo bellman_ford step 511 current loss 0.108983, current_train_items 16384.
I0302 18:57:59.459029 22683591385216 run.py:483] Algo bellman_ford step 512 current loss 0.146479, current_train_items 16416.
I0302 18:57:59.487534 22683591385216 run.py:483] Algo bellman_ford step 513 current loss 0.232636, current_train_items 16448.
I0302 18:57:59.517713 22683591385216 run.py:483] Algo bellman_ford step 514 current loss 0.232760, current_train_items 16480.
I0302 18:57:59.534733 22683591385216 run.py:483] Algo bellman_ford step 515 current loss 0.013220, current_train_items 16512.
I0302 18:57:59.549717 22683591385216 run.py:483] Algo bellman_ford step 516 current loss 0.095201, current_train_items 16544.
I0302 18:57:59.572084 22683591385216 run.py:483] Algo bellman_ford step 517 current loss 0.168609, current_train_items 16576.
I0302 18:57:59.600905 22683591385216 run.py:483] Algo bellman_ford step 518 current loss 0.205105, current_train_items 16608.
I0302 18:57:59.630910 22683591385216 run.py:483] Algo bellman_ford step 519 current loss 0.216209, current_train_items 16640.
I0302 18:57:59.648011 22683591385216 run.py:483] Algo bellman_ford step 520 current loss 0.022189, current_train_items 16672.
I0302 18:57:59.663466 22683591385216 run.py:483] Algo bellman_ford step 521 current loss 0.084561, current_train_items 16704.
I0302 18:57:59.685183 22683591385216 run.py:483] Algo bellman_ford step 522 current loss 0.102454, current_train_items 16736.
I0302 18:57:59.713702 22683591385216 run.py:483] Algo bellman_ford step 523 current loss 0.158203, current_train_items 16768.
I0302 18:57:59.743076 22683591385216 run.py:483] Algo bellman_ford step 524 current loss 0.247239, current_train_items 16800.
I0302 18:57:59.759979 22683591385216 run.py:483] Algo bellman_ford step 525 current loss 0.027552, current_train_items 16832.
I0302 18:57:59.775536 22683591385216 run.py:483] Algo bellman_ford step 526 current loss 0.097120, current_train_items 16864.
I0302 18:57:59.797445 22683591385216 run.py:483] Algo bellman_ford step 527 current loss 0.167112, current_train_items 16896.
I0302 18:57:59.824283 22683591385216 run.py:483] Algo bellman_ford step 528 current loss 0.173434, current_train_items 16928.
I0302 18:57:59.854618 22683591385216 run.py:483] Algo bellman_ford step 529 current loss 0.211982, current_train_items 16960.
I0302 18:57:59.871999 22683591385216 run.py:483] Algo bellman_ford step 530 current loss 0.051171, current_train_items 16992.
I0302 18:57:59.887285 22683591385216 run.py:483] Algo bellman_ford step 531 current loss 0.086906, current_train_items 17024.
I0302 18:57:59.908325 22683591385216 run.py:483] Algo bellman_ford step 532 current loss 0.196396, current_train_items 17056.
I0302 18:57:59.936644 22683591385216 run.py:483] Algo bellman_ford step 533 current loss 0.276875, current_train_items 17088.
I0302 18:57:59.966949 22683591385216 run.py:483] Algo bellman_ford step 534 current loss 0.222075, current_train_items 17120.
I0302 18:57:59.984351 22683591385216 run.py:483] Algo bellman_ford step 535 current loss 0.030691, current_train_items 17152.
I0302 18:57:59.999169 22683591385216 run.py:483] Algo bellman_ford step 536 current loss 0.121922, current_train_items 17184.
I0302 18:58:00.020206 22683591385216 run.py:483] Algo bellman_ford step 537 current loss 0.282985, current_train_items 17216.
I0302 18:58:00.047100 22683591385216 run.py:483] Algo bellman_ford step 538 current loss 0.236589, current_train_items 17248.
I0302 18:58:00.075860 22683591385216 run.py:483] Algo bellman_ford step 539 current loss 0.205324, current_train_items 17280.
I0302 18:58:00.092852 22683591385216 run.py:483] Algo bellman_ford step 540 current loss 0.027722, current_train_items 17312.
I0302 18:58:00.107532 22683591385216 run.py:483] Algo bellman_ford step 541 current loss 0.048007, current_train_items 17344.
I0302 18:58:00.129830 22683591385216 run.py:483] Algo bellman_ford step 542 current loss 0.276751, current_train_items 17376.
I0302 18:58:00.157920 22683591385216 run.py:483] Algo bellman_ford step 543 current loss 0.382397, current_train_items 17408.
I0302 18:58:00.188618 22683591385216 run.py:483] Algo bellman_ford step 544 current loss 0.412559, current_train_items 17440.
I0302 18:58:00.205821 22683591385216 run.py:483] Algo bellman_ford step 545 current loss 0.023139, current_train_items 17472.
I0302 18:58:00.221239 22683591385216 run.py:483] Algo bellman_ford step 546 current loss 0.076706, current_train_items 17504.
I0302 18:58:00.243028 22683591385216 run.py:483] Algo bellman_ford step 547 current loss 0.162431, current_train_items 17536.
I0302 18:58:00.271372 22683591385216 run.py:483] Algo bellman_ford step 548 current loss 0.345636, current_train_items 17568.
I0302 18:58:00.298997 22683591385216 run.py:483] Algo bellman_ford step 549 current loss 0.178210, current_train_items 17600.
I0302 18:58:00.316140 22683591385216 run.py:483] Algo bellman_ford step 550 current loss 0.021832, current_train_items 17632.
I0302 18:58:00.323996 22683591385216 run.py:503] (val) algo bellman_ford step 550: {'pi': 0.94921875, 'score': 0.94921875, 'examples_seen': 17632, 'step': 550, 'algorithm': 'bellman_ford'}
I0302 18:58:00.324100 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.951, current avg val score is 0.949, val scores are: bellman_ford: 0.949
I0302 18:58:00.339770 22683591385216 run.py:483] Algo bellman_ford step 551 current loss 0.056404, current_train_items 17664.
I0302 18:58:00.361505 22683591385216 run.py:483] Algo bellman_ford step 552 current loss 0.204158, current_train_items 17696.
I0302 18:58:00.391089 22683591385216 run.py:483] Algo bellman_ford step 553 current loss 0.625729, current_train_items 17728.
I0302 18:58:00.421292 22683591385216 run.py:483] Algo bellman_ford step 554 current loss 0.477666, current_train_items 17760.
I0302 18:58:00.438848 22683591385216 run.py:483] Algo bellman_ford step 555 current loss 0.043441, current_train_items 17792.
I0302 18:58:00.454390 22683591385216 run.py:483] Algo bellman_ford step 556 current loss 0.075942, current_train_items 17824.
I0302 18:58:00.477308 22683591385216 run.py:483] Algo bellman_ford step 557 current loss 0.236060, current_train_items 17856.
I0302 18:58:00.504465 22683591385216 run.py:483] Algo bellman_ford step 558 current loss 0.217510, current_train_items 17888.
I0302 18:58:00.536065 22683591385216 run.py:483] Algo bellman_ford step 559 current loss 0.295012, current_train_items 17920.
I0302 18:58:00.553500 22683591385216 run.py:483] Algo bellman_ford step 560 current loss 0.013470, current_train_items 17952.
I0302 18:58:00.569013 22683591385216 run.py:483] Algo bellman_ford step 561 current loss 0.062935, current_train_items 17984.
I0302 18:58:00.591863 22683591385216 run.py:483] Algo bellman_ford step 562 current loss 0.126093, current_train_items 18016.
I0302 18:58:00.619346 22683591385216 run.py:483] Algo bellman_ford step 563 current loss 0.131584, current_train_items 18048.
I0302 18:58:00.647463 22683591385216 run.py:483] Algo bellman_ford step 564 current loss 0.217944, current_train_items 18080.
I0302 18:58:00.664664 22683591385216 run.py:483] Algo bellman_ford step 565 current loss 0.013456, current_train_items 18112.
I0302 18:58:00.680447 22683591385216 run.py:483] Algo bellman_ford step 566 current loss 0.057733, current_train_items 18144.
I0302 18:58:00.703526 22683591385216 run.py:483] Algo bellman_ford step 567 current loss 0.161708, current_train_items 18176.
I0302 18:58:00.731308 22683591385216 run.py:483] Algo bellman_ford step 568 current loss 0.228394, current_train_items 18208.
I0302 18:58:00.760332 22683591385216 run.py:483] Algo bellman_ford step 569 current loss 0.169485, current_train_items 18240.
I0302 18:58:00.777340 22683591385216 run.py:483] Algo bellman_ford step 570 current loss 0.032041, current_train_items 18272.
I0302 18:58:00.792623 22683591385216 run.py:483] Algo bellman_ford step 571 current loss 0.114806, current_train_items 18304.
I0302 18:58:00.815539 22683591385216 run.py:483] Algo bellman_ford step 572 current loss 0.186156, current_train_items 18336.
I0302 18:58:00.844437 22683591385216 run.py:483] Algo bellman_ford step 573 current loss 0.254557, current_train_items 18368.
I0302 18:58:00.873539 22683591385216 run.py:483] Algo bellman_ford step 574 current loss 0.192485, current_train_items 18400.
I0302 18:58:00.890908 22683591385216 run.py:483] Algo bellman_ford step 575 current loss 0.020760, current_train_items 18432.
I0302 18:58:00.906122 22683591385216 run.py:483] Algo bellman_ford step 576 current loss 0.071645, current_train_items 18464.
I0302 18:58:00.928558 22683591385216 run.py:483] Algo bellman_ford step 577 current loss 0.227270, current_train_items 18496.
I0302 18:58:00.956885 22683591385216 run.py:483] Algo bellman_ford step 578 current loss 0.247230, current_train_items 18528.
I0302 18:58:00.987888 22683591385216 run.py:483] Algo bellman_ford step 579 current loss 0.182251, current_train_items 18560.
I0302 18:58:01.005286 22683591385216 run.py:483] Algo bellman_ford step 580 current loss 0.034874, current_train_items 18592.
I0302 18:58:01.020672 22683591385216 run.py:483] Algo bellman_ford step 581 current loss 0.072798, current_train_items 18624.
I0302 18:58:01.041412 22683591385216 run.py:483] Algo bellman_ford step 582 current loss 0.298906, current_train_items 18656.
I0302 18:58:01.068255 22683591385216 run.py:483] Algo bellman_ford step 583 current loss 0.202439, current_train_items 18688.
I0302 18:58:01.098062 22683591385216 run.py:483] Algo bellman_ford step 584 current loss 0.205677, current_train_items 18720.
I0302 18:58:01.114879 22683591385216 run.py:483] Algo bellman_ford step 585 current loss 0.047589, current_train_items 18752.
I0302 18:58:01.130323 22683591385216 run.py:483] Algo bellman_ford step 586 current loss 0.133010, current_train_items 18784.
I0302 18:58:01.151524 22683591385216 run.py:483] Algo bellman_ford step 587 current loss 0.144653, current_train_items 18816.
I0302 18:58:01.178656 22683591385216 run.py:483] Algo bellman_ford step 588 current loss 0.349494, current_train_items 18848.
I0302 18:58:01.210014 22683591385216 run.py:483] Algo bellman_ford step 589 current loss 0.323409, current_train_items 18880.
I0302 18:58:01.227263 22683591385216 run.py:483] Algo bellman_ford step 590 current loss 0.033396, current_train_items 18912.
I0302 18:58:01.242783 22683591385216 run.py:483] Algo bellman_ford step 591 current loss 0.096696, current_train_items 18944.
I0302 18:58:01.265874 22683591385216 run.py:483] Algo bellman_ford step 592 current loss 0.340085, current_train_items 18976.
I0302 18:58:01.293703 22683591385216 run.py:483] Algo bellman_ford step 593 current loss 0.366835, current_train_items 19008.
I0302 18:58:01.323294 22683591385216 run.py:483] Algo bellman_ford step 594 current loss 0.385654, current_train_items 19040.
I0302 18:58:01.340463 22683591385216 run.py:483] Algo bellman_ford step 595 current loss 0.027604, current_train_items 19072.
I0302 18:58:01.355978 22683591385216 run.py:483] Algo bellman_ford step 596 current loss 0.067241, current_train_items 19104.
I0302 18:58:01.378852 22683591385216 run.py:483] Algo bellman_ford step 597 current loss 0.207416, current_train_items 19136.
I0302 18:58:01.406495 22683591385216 run.py:483] Algo bellman_ford step 598 current loss 0.184932, current_train_items 19168.
I0302 18:58:01.436252 22683591385216 run.py:483] Algo bellman_ford step 599 current loss 0.297203, current_train_items 19200.
I0302 18:58:01.453349 22683591385216 run.py:483] Algo bellman_ford step 600 current loss 0.058413, current_train_items 19232.
I0302 18:58:01.460818 22683591385216 run.py:503] (val) algo bellman_ford step 600: {'pi': 0.955078125, 'score': 0.955078125, 'examples_seen': 19232, 'step': 600, 'algorithm': 'bellman_ford'}
I0302 18:58:01.460933 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.951, current avg val score is 0.955, val scores are: bellman_ford: 0.955
I0302 18:58:01.489109 22683591385216 run.py:483] Algo bellman_ford step 601 current loss 0.067182, current_train_items 19264.
I0302 18:58:01.511278 22683591385216 run.py:483] Algo bellman_ford step 602 current loss 0.192301, current_train_items 19296.
I0302 18:58:01.538986 22683591385216 run.py:483] Algo bellman_ford step 603 current loss 0.180690, current_train_items 19328.
I0302 18:58:01.568417 22683591385216 run.py:483] Algo bellman_ford step 604 current loss 0.184518, current_train_items 19360.
I0302 18:58:01.586515 22683591385216 run.py:483] Algo bellman_ford step 605 current loss 0.028963, current_train_items 19392.
I0302 18:58:01.601504 22683591385216 run.py:483] Algo bellman_ford step 606 current loss 0.089529, current_train_items 19424.
I0302 18:58:01.624317 22683591385216 run.py:483] Algo bellman_ford step 607 current loss 0.250874, current_train_items 19456.
I0302 18:58:01.651589 22683591385216 run.py:483] Algo bellman_ford step 608 current loss 0.226778, current_train_items 19488.
I0302 18:58:01.681695 22683591385216 run.py:483] Algo bellman_ford step 609 current loss 0.226231, current_train_items 19520.
I0302 18:58:01.698979 22683591385216 run.py:483] Algo bellman_ford step 610 current loss 0.048564, current_train_items 19552.
I0302 18:58:01.714568 22683591385216 run.py:483] Algo bellman_ford step 611 current loss 0.051005, current_train_items 19584.
I0302 18:58:01.736917 22683591385216 run.py:483] Algo bellman_ford step 612 current loss 0.147409, current_train_items 19616.
I0302 18:58:01.763159 22683591385216 run.py:483] Algo bellman_ford step 613 current loss 0.159736, current_train_items 19648.
I0302 18:58:01.793284 22683591385216 run.py:483] Algo bellman_ford step 614 current loss 0.229644, current_train_items 19680.
I0302 18:58:01.810619 22683591385216 run.py:483] Algo bellman_ford step 615 current loss 0.014733, current_train_items 19712.
I0302 18:58:01.826233 22683591385216 run.py:483] Algo bellman_ford step 616 current loss 0.052969, current_train_items 19744.
I0302 18:58:01.848231 22683591385216 run.py:483] Algo bellman_ford step 617 current loss 0.118456, current_train_items 19776.
I0302 18:58:01.875357 22683591385216 run.py:483] Algo bellman_ford step 618 current loss 0.152005, current_train_items 19808.
I0302 18:58:01.905534 22683591385216 run.py:483] Algo bellman_ford step 619 current loss 0.176142, current_train_items 19840.
I0302 18:58:01.923137 22683591385216 run.py:483] Algo bellman_ford step 620 current loss 0.042550, current_train_items 19872.
I0302 18:58:01.938928 22683591385216 run.py:483] Algo bellman_ford step 621 current loss 0.202510, current_train_items 19904.
I0302 18:58:01.961448 22683591385216 run.py:483] Algo bellman_ford step 622 current loss 0.219073, current_train_items 19936.
I0302 18:58:01.988881 22683591385216 run.py:483] Algo bellman_ford step 623 current loss 0.203206, current_train_items 19968.
I0302 18:58:02.018442 22683591385216 run.py:483] Algo bellman_ford step 624 current loss 0.210357, current_train_items 20000.
I0302 18:58:02.036165 22683591385216 run.py:483] Algo bellman_ford step 625 current loss 0.021178, current_train_items 20032.
I0302 18:58:02.051254 22683591385216 run.py:483] Algo bellman_ford step 626 current loss 0.052662, current_train_items 20064.
I0302 18:58:02.074049 22683591385216 run.py:483] Algo bellman_ford step 627 current loss 0.161531, current_train_items 20096.
I0302 18:58:02.102851 22683591385216 run.py:483] Algo bellman_ford step 628 current loss 0.176604, current_train_items 20128.
I0302 18:58:02.133362 22683591385216 run.py:483] Algo bellman_ford step 629 current loss 0.237292, current_train_items 20160.
I0302 18:58:02.150716 22683591385216 run.py:483] Algo bellman_ford step 630 current loss 0.023495, current_train_items 20192.
I0302 18:58:02.165888 22683591385216 run.py:483] Algo bellman_ford step 631 current loss 0.168567, current_train_items 20224.
I0302 18:58:02.188063 22683591385216 run.py:483] Algo bellman_ford step 632 current loss 0.278700, current_train_items 20256.
I0302 18:58:02.215485 22683591385216 run.py:483] Algo bellman_ford step 633 current loss 0.349072, current_train_items 20288.
I0302 18:58:02.244935 22683591385216 run.py:483] Algo bellman_ford step 634 current loss 0.191881, current_train_items 20320.
I0302 18:58:02.262594 22683591385216 run.py:483] Algo bellman_ford step 635 current loss 0.028618, current_train_items 20352.
I0302 18:58:02.277815 22683591385216 run.py:483] Algo bellman_ford step 636 current loss 0.061856, current_train_items 20384.
I0302 18:58:02.299832 22683591385216 run.py:483] Algo bellman_ford step 637 current loss 0.245286, current_train_items 20416.
I0302 18:58:02.328674 22683591385216 run.py:483] Algo bellman_ford step 638 current loss 0.424563, current_train_items 20448.
I0302 18:58:02.355732 22683591385216 run.py:483] Algo bellman_ford step 639 current loss 0.238342, current_train_items 20480.
I0302 18:58:02.373137 22683591385216 run.py:483] Algo bellman_ford step 640 current loss 0.024356, current_train_items 20512.
I0302 18:58:02.388280 22683591385216 run.py:483] Algo bellman_ford step 641 current loss 0.067393, current_train_items 20544.
I0302 18:58:02.409314 22683591385216 run.py:483] Algo bellman_ford step 642 current loss 0.249344, current_train_items 20576.
I0302 18:58:02.436576 22683591385216 run.py:483] Algo bellman_ford step 643 current loss 0.174074, current_train_items 20608.
I0302 18:58:02.466409 22683591385216 run.py:483] Algo bellman_ford step 644 current loss 0.226188, current_train_items 20640.
I0302 18:58:02.483906 22683591385216 run.py:483] Algo bellman_ford step 645 current loss 0.015247, current_train_items 20672.
I0302 18:58:02.500221 22683591385216 run.py:483] Algo bellman_ford step 646 current loss 0.064753, current_train_items 20704.
I0302 18:58:02.521622 22683591385216 run.py:483] Algo bellman_ford step 647 current loss 0.071921, current_train_items 20736.
I0302 18:58:02.550189 22683591385216 run.py:483] Algo bellman_ford step 648 current loss 0.208957, current_train_items 20768.
I0302 18:58:02.580709 22683591385216 run.py:483] Algo bellman_ford step 649 current loss 0.170817, current_train_items 20800.
I0302 18:58:02.598363 22683591385216 run.py:483] Algo bellman_ford step 650 current loss 0.040249, current_train_items 20832.
I0302 18:58:02.606162 22683591385216 run.py:503] (val) algo bellman_ford step 650: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 20832, 'step': 650, 'algorithm': 'bellman_ford'}
I0302 18:58:02.606270 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.955, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0302 18:58:02.635237 22683591385216 run.py:483] Algo bellman_ford step 651 current loss 0.058770, current_train_items 20864.
I0302 18:58:02.656989 22683591385216 run.py:483] Algo bellman_ford step 652 current loss 0.113682, current_train_items 20896.
I0302 18:58:02.684202 22683591385216 run.py:483] Algo bellman_ford step 653 current loss 0.112688, current_train_items 20928.
I0302 18:58:02.714416 22683591385216 run.py:483] Algo bellman_ford step 654 current loss 0.178047, current_train_items 20960.
I0302 18:58:02.732462 22683591385216 run.py:483] Algo bellman_ford step 655 current loss 0.011767, current_train_items 20992.
I0302 18:58:02.748046 22683591385216 run.py:483] Algo bellman_ford step 656 current loss 0.070554, current_train_items 21024.
I0302 18:58:02.770731 22683591385216 run.py:483] Algo bellman_ford step 657 current loss 0.157957, current_train_items 21056.
I0302 18:58:02.798981 22683591385216 run.py:483] Algo bellman_ford step 658 current loss 0.153905, current_train_items 21088.
I0302 18:58:02.828785 22683591385216 run.py:483] Algo bellman_ford step 659 current loss 0.189778, current_train_items 21120.
I0302 18:58:02.846383 22683591385216 run.py:483] Algo bellman_ford step 660 current loss 0.014738, current_train_items 21152.
I0302 18:58:02.861773 22683591385216 run.py:483] Algo bellman_ford step 661 current loss 0.068035, current_train_items 21184.
I0302 18:58:02.883833 22683591385216 run.py:483] Algo bellman_ford step 662 current loss 0.076559, current_train_items 21216.
I0302 18:58:02.912052 22683591385216 run.py:483] Algo bellman_ford step 663 current loss 0.161337, current_train_items 21248.
I0302 18:58:02.943871 22683591385216 run.py:483] Algo bellman_ford step 664 current loss 0.259828, current_train_items 21280.
I0302 18:58:02.962033 22683591385216 run.py:483] Algo bellman_ford step 665 current loss 0.021355, current_train_items 21312.
I0302 18:58:02.977610 22683591385216 run.py:483] Algo bellman_ford step 666 current loss 0.070804, current_train_items 21344.
I0302 18:58:03.000826 22683591385216 run.py:483] Algo bellman_ford step 667 current loss 0.167627, current_train_items 21376.
I0302 18:58:03.028274 22683591385216 run.py:483] Algo bellman_ford step 668 current loss 0.156164, current_train_items 21408.
I0302 18:58:03.057557 22683591385216 run.py:483] Algo bellman_ford step 669 current loss 0.139102, current_train_items 21440.
I0302 18:58:03.074934 22683591385216 run.py:483] Algo bellman_ford step 670 current loss 0.006948, current_train_items 21472.
I0302 18:58:03.090231 22683591385216 run.py:483] Algo bellman_ford step 671 current loss 0.037663, current_train_items 21504.
I0302 18:58:03.113508 22683591385216 run.py:483] Algo bellman_ford step 672 current loss 0.155283, current_train_items 21536.
I0302 18:58:03.140774 22683591385216 run.py:483] Algo bellman_ford step 673 current loss 0.147426, current_train_items 21568.
I0302 18:58:03.172195 22683591385216 run.py:483] Algo bellman_ford step 674 current loss 0.196183, current_train_items 21600.
I0302 18:58:03.190111 22683591385216 run.py:483] Algo bellman_ford step 675 current loss 0.020441, current_train_items 21632.
I0302 18:58:03.205055 22683591385216 run.py:483] Algo bellman_ford step 676 current loss 0.055237, current_train_items 21664.
I0302 18:58:03.227409 22683591385216 run.py:483] Algo bellman_ford step 677 current loss 0.170714, current_train_items 21696.
I0302 18:58:03.256407 22683591385216 run.py:483] Algo bellman_ford step 678 current loss 0.187004, current_train_items 21728.
I0302 18:58:03.287026 22683591385216 run.py:483] Algo bellman_ford step 679 current loss 0.235979, current_train_items 21760.
I0302 18:58:03.305165 22683591385216 run.py:483] Algo bellman_ford step 680 current loss 0.111600, current_train_items 21792.
I0302 18:58:03.320697 22683591385216 run.py:483] Algo bellman_ford step 681 current loss 0.081744, current_train_items 21824.
I0302 18:58:03.343208 22683591385216 run.py:483] Algo bellman_ford step 682 current loss 0.184090, current_train_items 21856.
I0302 18:58:03.370572 22683591385216 run.py:483] Algo bellman_ford step 683 current loss 0.210831, current_train_items 21888.
I0302 18:58:03.401774 22683591385216 run.py:483] Algo bellman_ford step 684 current loss 0.211061, current_train_items 21920.
I0302 18:58:03.419244 22683591385216 run.py:483] Algo bellman_ford step 685 current loss 0.023066, current_train_items 21952.
I0302 18:58:03.434584 22683591385216 run.py:483] Algo bellman_ford step 686 current loss 0.081663, current_train_items 21984.
I0302 18:58:03.457585 22683591385216 run.py:483] Algo bellman_ford step 687 current loss 0.235237, current_train_items 22016.
I0302 18:58:03.483435 22683591385216 run.py:483] Algo bellman_ford step 688 current loss 0.197875, current_train_items 22048.
I0302 18:58:03.515997 22683591385216 run.py:483] Algo bellman_ford step 689 current loss 0.255554, current_train_items 22080.
I0302 18:58:03.533237 22683591385216 run.py:483] Algo bellman_ford step 690 current loss 0.032487, current_train_items 22112.
I0302 18:58:03.549103 22683591385216 run.py:483] Algo bellman_ford step 691 current loss 0.191828, current_train_items 22144.
I0302 18:58:03.572321 22683591385216 run.py:483] Algo bellman_ford step 692 current loss 0.206041, current_train_items 22176.
I0302 18:58:03.599973 22683591385216 run.py:483] Algo bellman_ford step 693 current loss 0.251626, current_train_items 22208.
I0302 18:58:03.631194 22683591385216 run.py:483] Algo bellman_ford step 694 current loss 0.457758, current_train_items 22240.
I0302 18:58:03.648992 22683591385216 run.py:483] Algo bellman_ford step 695 current loss 0.017599, current_train_items 22272.
I0302 18:58:03.664310 22683591385216 run.py:483] Algo bellman_ford step 696 current loss 0.052894, current_train_items 22304.
I0302 18:58:03.685230 22683591385216 run.py:483] Algo bellman_ford step 697 current loss 0.231837, current_train_items 22336.
I0302 18:58:03.713210 22683591385216 run.py:483] Algo bellman_ford step 698 current loss 0.151521, current_train_items 22368.
I0302 18:58:03.745035 22683591385216 run.py:483] Algo bellman_ford step 699 current loss 0.228609, current_train_items 22400.
I0302 18:58:03.762364 22683591385216 run.py:483] Algo bellman_ford step 700 current loss 0.010742, current_train_items 22432.
I0302 18:58:03.770283 22683591385216 run.py:503] (val) algo bellman_ford step 700: {'pi': 0.9541015625, 'score': 0.9541015625, 'examples_seen': 22432, 'step': 700, 'algorithm': 'bellman_ford'}
I0302 18:58:03.770393 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.954, val scores are: bellman_ford: 0.954
I0302 18:58:03.785894 22683591385216 run.py:483] Algo bellman_ford step 701 current loss 0.042967, current_train_items 22464.
I0302 18:58:03.808305 22683591385216 run.py:483] Algo bellman_ford step 702 current loss 0.252270, current_train_items 22496.
I0302 18:58:03.834867 22683591385216 run.py:483] Algo bellman_ford step 703 current loss 0.158756, current_train_items 22528.
I0302 18:58:03.864064 22683591385216 run.py:483] Algo bellman_ford step 704 current loss 0.153396, current_train_items 22560.
I0302 18:58:03.881944 22683591385216 run.py:483] Algo bellman_ford step 705 current loss 0.033144, current_train_items 22592.
I0302 18:58:03.897000 22683591385216 run.py:483] Algo bellman_ford step 706 current loss 0.070989, current_train_items 22624.
I0302 18:58:03.919003 22683591385216 run.py:483] Algo bellman_ford step 707 current loss 0.103339, current_train_items 22656.
I0302 18:58:03.946748 22683591385216 run.py:483] Algo bellman_ford step 708 current loss 0.119600, current_train_items 22688.
I0302 18:58:03.977540 22683591385216 run.py:483] Algo bellman_ford step 709 current loss 0.156047, current_train_items 22720.
I0302 18:58:03.995111 22683591385216 run.py:483] Algo bellman_ford step 710 current loss 0.009285, current_train_items 22752.
I0302 18:58:04.010424 22683591385216 run.py:483] Algo bellman_ford step 711 current loss 0.042034, current_train_items 22784.
I0302 18:58:04.033244 22683591385216 run.py:483] Algo bellman_ford step 712 current loss 0.122184, current_train_items 22816.
I0302 18:58:04.061923 22683591385216 run.py:483] Algo bellman_ford step 713 current loss 0.134796, current_train_items 22848.
I0302 18:58:04.090178 22683591385216 run.py:483] Algo bellman_ford step 714 current loss 0.150362, current_train_items 22880.
I0302 18:58:04.107462 22683591385216 run.py:483] Algo bellman_ford step 715 current loss 0.007363, current_train_items 22912.
I0302 18:58:04.123110 22683591385216 run.py:483] Algo bellman_ford step 716 current loss 0.070217, current_train_items 22944.
I0302 18:58:04.146573 22683591385216 run.py:483] Algo bellman_ford step 717 current loss 0.153629, current_train_items 22976.
I0302 18:58:04.173380 22683591385216 run.py:483] Algo bellman_ford step 718 current loss 0.067723, current_train_items 23008.
I0302 18:58:04.203248 22683591385216 run.py:483] Algo bellman_ford step 719 current loss 0.149348, current_train_items 23040.
I0302 18:58:04.220611 22683591385216 run.py:483] Algo bellman_ford step 720 current loss 0.026692, current_train_items 23072.
I0302 18:58:04.236248 22683591385216 run.py:483] Algo bellman_ford step 721 current loss 0.134594, current_train_items 23104.
I0302 18:58:04.258055 22683591385216 run.py:483] Algo bellman_ford step 722 current loss 0.070833, current_train_items 23136.
I0302 18:58:04.285619 22683591385216 run.py:483] Algo bellman_ford step 723 current loss 0.118404, current_train_items 23168.
I0302 18:58:04.314806 22683591385216 run.py:483] Algo bellman_ford step 724 current loss 0.168999, current_train_items 23200.
I0302 18:58:04.332292 22683591385216 run.py:483] Algo bellman_ford step 725 current loss 0.066155, current_train_items 23232.
I0302 18:58:04.348183 22683591385216 run.py:483] Algo bellman_ford step 726 current loss 0.039243, current_train_items 23264.
I0302 18:58:04.370949 22683591385216 run.py:483] Algo bellman_ford step 727 current loss 0.191578, current_train_items 23296.
I0302 18:58:04.398262 22683591385216 run.py:483] Algo bellman_ford step 728 current loss 0.198287, current_train_items 23328.
I0302 18:58:04.428418 22683591385216 run.py:483] Algo bellman_ford step 729 current loss 0.268076, current_train_items 23360.
I0302 18:58:04.445935 22683591385216 run.py:483] Algo bellman_ford step 730 current loss 0.008331, current_train_items 23392.
I0302 18:58:04.461264 22683591385216 run.py:483] Algo bellman_ford step 731 current loss 0.057563, current_train_items 23424.
I0302 18:58:04.483731 22683591385216 run.py:483] Algo bellman_ford step 732 current loss 0.151320, current_train_items 23456.
I0302 18:58:04.511688 22683591385216 run.py:483] Algo bellman_ford step 733 current loss 0.262076, current_train_items 23488.
I0302 18:58:04.542562 22683591385216 run.py:483] Algo bellman_ford step 734 current loss 0.228518, current_train_items 23520.
I0302 18:58:04.560142 22683591385216 run.py:483] Algo bellman_ford step 735 current loss 0.019674, current_train_items 23552.
I0302 18:58:04.575690 22683591385216 run.py:483] Algo bellman_ford step 736 current loss 0.042211, current_train_items 23584.
I0302 18:58:04.597560 22683591385216 run.py:483] Algo bellman_ford step 737 current loss 0.122686, current_train_items 23616.
I0302 18:58:04.625382 22683591385216 run.py:483] Algo bellman_ford step 738 current loss 0.201755, current_train_items 23648.
I0302 18:58:04.653697 22683591385216 run.py:483] Algo bellman_ford step 739 current loss 0.121596, current_train_items 23680.
I0302 18:58:04.671264 22683591385216 run.py:483] Algo bellman_ford step 740 current loss 0.004533, current_train_items 23712.
I0302 18:58:04.686705 22683591385216 run.py:483] Algo bellman_ford step 741 current loss 0.047887, current_train_items 23744.
I0302 18:58:04.708521 22683591385216 run.py:483] Algo bellman_ford step 742 current loss 0.107854, current_train_items 23776.
I0302 18:58:04.735338 22683591385216 run.py:483] Algo bellman_ford step 743 current loss 0.125213, current_train_items 23808.
I0302 18:58:04.766933 22683591385216 run.py:483] Algo bellman_ford step 744 current loss 0.210308, current_train_items 23840.
I0302 18:58:04.784551 22683591385216 run.py:483] Algo bellman_ford step 745 current loss 0.023454, current_train_items 23872.
I0302 18:58:04.799707 22683591385216 run.py:483] Algo bellman_ford step 746 current loss 0.079631, current_train_items 23904.
I0302 18:58:04.821274 22683591385216 run.py:483] Algo bellman_ford step 747 current loss 0.132578, current_train_items 23936.
I0302 18:58:04.849887 22683591385216 run.py:483] Algo bellman_ford step 748 current loss 0.207588, current_train_items 23968.
I0302 18:58:04.879723 22683591385216 run.py:483] Algo bellman_ford step 749 current loss 0.234330, current_train_items 24000.
I0302 18:58:04.896823 22683591385216 run.py:483] Algo bellman_ford step 750 current loss 0.009666, current_train_items 24032.
I0302 18:58:04.904705 22683591385216 run.py:503] (val) algo bellman_ford step 750: {'pi': 0.9423828125, 'score': 0.9423828125, 'examples_seen': 24032, 'step': 750, 'algorithm': 'bellman_ford'}
I0302 18:58:04.904811 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.942, val scores are: bellman_ford: 0.942
I0302 18:58:04.920928 22683591385216 run.py:483] Algo bellman_ford step 751 current loss 0.105288, current_train_items 24064.
I0302 18:58:04.942509 22683591385216 run.py:483] Algo bellman_ford step 752 current loss 0.163014, current_train_items 24096.
I0302 18:58:04.969689 22683591385216 run.py:483] Algo bellman_ford step 753 current loss 0.132651, current_train_items 24128.
I0302 18:58:05.000632 22683591385216 run.py:483] Algo bellman_ford step 754 current loss 0.209902, current_train_items 24160.
I0302 18:58:05.018000 22683591385216 run.py:483] Algo bellman_ford step 755 current loss 0.060081, current_train_items 24192.
I0302 18:58:05.032908 22683591385216 run.py:483] Algo bellman_ford step 756 current loss 0.098730, current_train_items 24224.
I0302 18:58:05.055830 22683591385216 run.py:483] Algo bellman_ford step 757 current loss 0.182725, current_train_items 24256.
I0302 18:58:05.083497 22683591385216 run.py:483] Algo bellman_ford step 758 current loss 0.303560, current_train_items 24288.
I0302 18:58:05.112612 22683591385216 run.py:483] Algo bellman_ford step 759 current loss 0.198965, current_train_items 24320.
I0302 18:58:05.130633 22683591385216 run.py:483] Algo bellman_ford step 760 current loss 0.015096, current_train_items 24352.
I0302 18:58:05.146198 22683591385216 run.py:483] Algo bellman_ford step 761 current loss 0.082171, current_train_items 24384.
I0302 18:58:05.167925 22683591385216 run.py:483] Algo bellman_ford step 762 current loss 0.234783, current_train_items 24416.
I0302 18:58:05.194708 22683591385216 run.py:483] Algo bellman_ford step 763 current loss 0.182508, current_train_items 24448.
I0302 18:58:05.224860 22683591385216 run.py:483] Algo bellman_ford step 764 current loss 0.204680, current_train_items 24480.
I0302 18:58:05.242412 22683591385216 run.py:483] Algo bellman_ford step 765 current loss 0.024853, current_train_items 24512.
I0302 18:58:05.258333 22683591385216 run.py:483] Algo bellman_ford step 766 current loss 0.081777, current_train_items 24544.
I0302 18:58:05.280253 22683591385216 run.py:483] Algo bellman_ford step 767 current loss 0.203376, current_train_items 24576.
I0302 18:58:05.308101 22683591385216 run.py:483] Algo bellman_ford step 768 current loss 0.199480, current_train_items 24608.
I0302 18:58:05.338865 22683591385216 run.py:483] Algo bellman_ford step 769 current loss 0.293975, current_train_items 24640.
I0302 18:58:05.356577 22683591385216 run.py:483] Algo bellman_ford step 770 current loss 0.008009, current_train_items 24672.
I0302 18:58:05.372219 22683591385216 run.py:483] Algo bellman_ford step 771 current loss 0.101467, current_train_items 24704.
I0302 18:58:05.393749 22683591385216 run.py:483] Algo bellman_ford step 772 current loss 0.122279, current_train_items 24736.
I0302 18:58:05.420968 22683591385216 run.py:483] Algo bellman_ford step 773 current loss 0.094373, current_train_items 24768.
I0302 18:58:05.452059 22683591385216 run.py:483] Algo bellman_ford step 774 current loss 0.225818, current_train_items 24800.
I0302 18:58:05.469834 22683591385216 run.py:483] Algo bellman_ford step 775 current loss 0.016595, current_train_items 24832.
I0302 18:58:05.485925 22683591385216 run.py:483] Algo bellman_ford step 776 current loss 0.055259, current_train_items 24864.
I0302 18:58:05.508265 22683591385216 run.py:483] Algo bellman_ford step 777 current loss 0.124535, current_train_items 24896.
I0302 18:58:05.534857 22683591385216 run.py:483] Algo bellman_ford step 778 current loss 0.237248, current_train_items 24928.
I0302 18:58:05.563942 22683591385216 run.py:483] Algo bellman_ford step 779 current loss 0.153915, current_train_items 24960.
I0302 18:58:05.581093 22683591385216 run.py:483] Algo bellman_ford step 780 current loss 0.024418, current_train_items 24992.
I0302 18:58:05.596572 22683591385216 run.py:483] Algo bellman_ford step 781 current loss 0.043457, current_train_items 25024.
I0302 18:58:05.617942 22683591385216 run.py:483] Algo bellman_ford step 782 current loss 0.129060, current_train_items 25056.
I0302 18:58:05.645630 22683591385216 run.py:483] Algo bellman_ford step 783 current loss 0.247069, current_train_items 25088.
I0302 18:58:05.676582 22683591385216 run.py:483] Algo bellman_ford step 784 current loss 0.215328, current_train_items 25120.
I0302 18:58:05.694260 22683591385216 run.py:483] Algo bellman_ford step 785 current loss 0.008866, current_train_items 25152.
I0302 18:58:05.710053 22683591385216 run.py:483] Algo bellman_ford step 786 current loss 0.082001, current_train_items 25184.
I0302 18:58:05.731594 22683591385216 run.py:483] Algo bellman_ford step 787 current loss 0.177199, current_train_items 25216.
I0302 18:58:05.759245 22683591385216 run.py:483] Algo bellman_ford step 788 current loss 0.178414, current_train_items 25248.
I0302 18:58:05.786803 22683591385216 run.py:483] Algo bellman_ford step 789 current loss 0.346165, current_train_items 25280.
I0302 18:58:05.804527 22683591385216 run.py:483] Algo bellman_ford step 790 current loss 0.020450, current_train_items 25312.
I0302 18:58:05.820363 22683591385216 run.py:483] Algo bellman_ford step 791 current loss 0.123815, current_train_items 25344.
I0302 18:58:05.840754 22683591385216 run.py:483] Algo bellman_ford step 792 current loss 0.183781, current_train_items 25376.
I0302 18:58:05.868339 22683591385216 run.py:483] Algo bellman_ford step 793 current loss 0.243861, current_train_items 25408.
I0302 18:58:05.896756 22683591385216 run.py:483] Algo bellman_ford step 794 current loss 0.209405, current_train_items 25440.
I0302 18:58:05.913942 22683591385216 run.py:483] Algo bellman_ford step 795 current loss 0.012855, current_train_items 25472.
I0302 18:58:05.928819 22683591385216 run.py:483] Algo bellman_ford step 796 current loss 0.086867, current_train_items 25504.
I0302 18:58:05.951049 22683591385216 run.py:483] Algo bellman_ford step 797 current loss 0.159197, current_train_items 25536.
I0302 18:58:05.977752 22683591385216 run.py:483] Algo bellman_ford step 798 current loss 0.204236, current_train_items 25568.
I0302 18:58:06.006647 22683591385216 run.py:483] Algo bellman_ford step 799 current loss 0.189222, current_train_items 25600.
I0302 18:58:06.023983 22683591385216 run.py:483] Algo bellman_ford step 800 current loss 0.005380, current_train_items 25632.
I0302 18:58:06.031647 22683591385216 run.py:503] (val) algo bellman_ford step 800: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 25632, 'step': 800, 'algorithm': 'bellman_ford'}
I0302 18:58:06.031753 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0302 18:58:06.047716 22683591385216 run.py:483] Algo bellman_ford step 801 current loss 0.054639, current_train_items 25664.
I0302 18:58:06.071134 22683591385216 run.py:483] Algo bellman_ford step 802 current loss 0.161393, current_train_items 25696.
I0302 18:58:06.099341 22683591385216 run.py:483] Algo bellman_ford step 803 current loss 0.207487, current_train_items 25728.
I0302 18:58:06.129186 22683591385216 run.py:483] Algo bellman_ford step 804 current loss 0.213593, current_train_items 25760.
I0302 18:58:06.147254 22683591385216 run.py:483] Algo bellman_ford step 805 current loss 0.005547, current_train_items 25792.
I0302 18:58:06.162411 22683591385216 run.py:483] Algo bellman_ford step 806 current loss 0.037968, current_train_items 25824.
I0302 18:58:06.185409 22683591385216 run.py:483] Algo bellman_ford step 807 current loss 0.108865, current_train_items 25856.
I0302 18:58:06.213120 22683591385216 run.py:483] Algo bellman_ford step 808 current loss 0.152009, current_train_items 25888.
I0302 18:58:06.242094 22683591385216 run.py:483] Algo bellman_ford step 809 current loss 0.170885, current_train_items 25920.
I0302 18:58:06.259678 22683591385216 run.py:483] Algo bellman_ford step 810 current loss 0.020991, current_train_items 25952.
I0302 18:58:06.275555 22683591385216 run.py:483] Algo bellman_ford step 811 current loss 0.092851, current_train_items 25984.
I0302 18:58:06.297397 22683591385216 run.py:483] Algo bellman_ford step 812 current loss 0.105427, current_train_items 26016.
I0302 18:58:06.324314 22683591385216 run.py:483] Algo bellman_ford step 813 current loss 0.136592, current_train_items 26048.
I0302 18:58:06.354112 22683591385216 run.py:483] Algo bellman_ford step 814 current loss 0.201275, current_train_items 26080.
I0302 18:58:06.371701 22683591385216 run.py:483] Algo bellman_ford step 815 current loss 0.031502, current_train_items 26112.
I0302 18:58:06.387289 22683591385216 run.py:483] Algo bellman_ford step 816 current loss 0.096969, current_train_items 26144.
I0302 18:58:06.409625 22683591385216 run.py:483] Algo bellman_ford step 817 current loss 0.063917, current_train_items 26176.
I0302 18:58:06.438426 22683591385216 run.py:483] Algo bellman_ford step 818 current loss 0.163212, current_train_items 26208.
I0302 18:58:06.470064 22683591385216 run.py:483] Algo bellman_ford step 819 current loss 0.191358, current_train_items 26240.
I0302 18:58:06.487797 22683591385216 run.py:483] Algo bellman_ford step 820 current loss 0.020912, current_train_items 26272.
I0302 18:58:06.502872 22683591385216 run.py:483] Algo bellman_ford step 821 current loss 0.102585, current_train_items 26304.
I0302 18:58:06.524498 22683591385216 run.py:483] Algo bellman_ford step 822 current loss 0.157373, current_train_items 26336.
I0302 18:58:06.552605 22683591385216 run.py:483] Algo bellman_ford step 823 current loss 0.145432, current_train_items 26368.
I0302 18:58:06.581640 22683591385216 run.py:483] Algo bellman_ford step 824 current loss 0.312401, current_train_items 26400.
I0302 18:58:06.599202 22683591385216 run.py:483] Algo bellman_ford step 825 current loss 0.023621, current_train_items 26432.
I0302 18:58:06.614972 22683591385216 run.py:483] Algo bellman_ford step 826 current loss 0.050860, current_train_items 26464.
I0302 18:58:06.637797 22683591385216 run.py:483] Algo bellman_ford step 827 current loss 0.113393, current_train_items 26496.
I0302 18:58:06.665888 22683591385216 run.py:483] Algo bellman_ford step 828 current loss 0.154215, current_train_items 26528.
I0302 18:58:06.695727 22683591385216 run.py:483] Algo bellman_ford step 829 current loss 0.137271, current_train_items 26560.
I0302 18:58:06.713162 22683591385216 run.py:483] Algo bellman_ford step 830 current loss 0.010765, current_train_items 26592.
I0302 18:58:06.728379 22683591385216 run.py:483] Algo bellman_ford step 831 current loss 0.044558, current_train_items 26624.
I0302 18:58:06.750956 22683591385216 run.py:483] Algo bellman_ford step 832 current loss 0.136717, current_train_items 26656.
I0302 18:58:06.778877 22683591385216 run.py:483] Algo bellman_ford step 833 current loss 0.173341, current_train_items 26688.
I0302 18:58:06.807637 22683591385216 run.py:483] Algo bellman_ford step 834 current loss 0.134243, current_train_items 26720.
I0302 18:58:06.825336 22683591385216 run.py:483] Algo bellman_ford step 835 current loss 0.009778, current_train_items 26752.
I0302 18:58:06.840532 22683591385216 run.py:483] Algo bellman_ford step 836 current loss 0.059420, current_train_items 26784.
I0302 18:58:06.862720 22683591385216 run.py:483] Algo bellman_ford step 837 current loss 0.244630, current_train_items 26816.
I0302 18:58:06.890418 22683591385216 run.py:483] Algo bellman_ford step 838 current loss 0.136624, current_train_items 26848.
I0302 18:58:06.921252 22683591385216 run.py:483] Algo bellman_ford step 839 current loss 0.300399, current_train_items 26880.
I0302 18:58:06.938404 22683591385216 run.py:483] Algo bellman_ford step 840 current loss 0.024360, current_train_items 26912.
I0302 18:58:06.953711 22683591385216 run.py:483] Algo bellman_ford step 841 current loss 0.091145, current_train_items 26944.
I0302 18:58:06.975910 22683591385216 run.py:483] Algo bellman_ford step 842 current loss 0.085015, current_train_items 26976.
I0302 18:58:07.002369 22683591385216 run.py:483] Algo bellman_ford step 843 current loss 0.107522, current_train_items 27008.
I0302 18:58:07.031266 22683591385216 run.py:483] Algo bellman_ford step 844 current loss 0.148176, current_train_items 27040.
I0302 18:58:07.048690 22683591385216 run.py:483] Algo bellman_ford step 845 current loss 0.055069, current_train_items 27072.
I0302 18:58:07.064214 22683591385216 run.py:483] Algo bellman_ford step 846 current loss 0.090371, current_train_items 27104.
I0302 18:58:07.087934 22683591385216 run.py:483] Algo bellman_ford step 847 current loss 0.061273, current_train_items 27136.
I0302 18:58:07.115065 22683591385216 run.py:483] Algo bellman_ford step 848 current loss 0.131114, current_train_items 27168.
I0302 18:58:07.145761 22683591385216 run.py:483] Algo bellman_ford step 849 current loss 0.154435, current_train_items 27200.
I0302 18:58:07.163150 22683591385216 run.py:483] Algo bellman_ford step 850 current loss 0.006484, current_train_items 27232.
I0302 18:58:07.171056 22683591385216 run.py:503] (val) algo bellman_ford step 850: {'pi': 0.958984375, 'score': 0.958984375, 'examples_seen': 27232, 'step': 850, 'algorithm': 'bellman_ford'}
I0302 18:58:07.171162 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.959, val scores are: bellman_ford: 0.959
I0302 18:58:07.186632 22683591385216 run.py:483] Algo bellman_ford step 851 current loss 0.032115, current_train_items 27264.
I0302 18:58:07.208961 22683591385216 run.py:483] Algo bellman_ford step 852 current loss 0.102611, current_train_items 27296.
I0302 18:58:07.238062 22683591385216 run.py:483] Algo bellman_ford step 853 current loss 0.162411, current_train_items 27328.
I0302 18:58:07.267418 22683591385216 run.py:483] Algo bellman_ford step 854 current loss 0.185925, current_train_items 27360.
I0302 18:58:07.284968 22683591385216 run.py:483] Algo bellman_ford step 855 current loss 0.030614, current_train_items 27392.
I0302 18:58:07.300322 22683591385216 run.py:483] Algo bellman_ford step 856 current loss 0.030524, current_train_items 27424.
I0302 18:58:07.321585 22683591385216 run.py:483] Algo bellman_ford step 857 current loss 0.158428, current_train_items 27456.
I0302 18:58:07.350184 22683591385216 run.py:483] Algo bellman_ford step 858 current loss 0.203767, current_train_items 27488.
I0302 18:58:07.379109 22683591385216 run.py:483] Algo bellman_ford step 859 current loss 0.178681, current_train_items 27520.
I0302 18:58:07.396653 22683591385216 run.py:483] Algo bellman_ford step 860 current loss 0.011025, current_train_items 27552.
I0302 18:58:07.412396 22683591385216 run.py:483] Algo bellman_ford step 861 current loss 0.076457, current_train_items 27584.
I0302 18:58:07.434162 22683591385216 run.py:483] Algo bellman_ford step 862 current loss 0.174168, current_train_items 27616.
I0302 18:58:07.461895 22683591385216 run.py:483] Algo bellman_ford step 863 current loss 0.236655, current_train_items 27648.
I0302 18:58:07.490978 22683591385216 run.py:483] Algo bellman_ford step 864 current loss 0.229213, current_train_items 27680.
I0302 18:58:07.508080 22683591385216 run.py:483] Algo bellman_ford step 865 current loss 0.017383, current_train_items 27712.
I0302 18:58:07.523038 22683591385216 run.py:483] Algo bellman_ford step 866 current loss 0.076068, current_train_items 27744.
I0302 18:58:07.546087 22683591385216 run.py:483] Algo bellman_ford step 867 current loss 0.229221, current_train_items 27776.
I0302 18:58:07.573189 22683591385216 run.py:483] Algo bellman_ford step 868 current loss 0.162084, current_train_items 27808.
I0302 18:58:07.604585 22683591385216 run.py:483] Algo bellman_ford step 869 current loss 0.204607, current_train_items 27840.
I0302 18:58:07.622124 22683591385216 run.py:483] Algo bellman_ford step 870 current loss 0.041934, current_train_items 27872.
I0302 18:58:07.637863 22683591385216 run.py:483] Algo bellman_ford step 871 current loss 0.049443, current_train_items 27904.
I0302 18:58:07.658660 22683591385216 run.py:483] Algo bellman_ford step 872 current loss 0.115101, current_train_items 27936.
I0302 18:58:07.686707 22683591385216 run.py:483] Algo bellman_ford step 873 current loss 0.195371, current_train_items 27968.
I0302 18:58:07.715903 22683591385216 run.py:483] Algo bellman_ford step 874 current loss 0.225158, current_train_items 28000.
I0302 18:58:07.733372 22683591385216 run.py:483] Algo bellman_ford step 875 current loss 0.007890, current_train_items 28032.
I0302 18:58:07.749197 22683591385216 run.py:483] Algo bellman_ford step 876 current loss 0.061573, current_train_items 28064.
I0302 18:58:07.772773 22683591385216 run.py:483] Algo bellman_ford step 877 current loss 0.148802, current_train_items 28096.
I0302 18:58:07.799562 22683591385216 run.py:483] Algo bellman_ford step 878 current loss 0.111681, current_train_items 28128.
I0302 18:58:07.830420 22683591385216 run.py:483] Algo bellman_ford step 879 current loss 0.144262, current_train_items 28160.
I0302 18:58:07.847768 22683591385216 run.py:483] Algo bellman_ford step 880 current loss 0.004508, current_train_items 28192.
I0302 18:58:07.862886 22683591385216 run.py:483] Algo bellman_ford step 881 current loss 0.016407, current_train_items 28224.
I0302 18:58:07.884079 22683591385216 run.py:483] Algo bellman_ford step 882 current loss 0.084443, current_train_items 28256.
I0302 18:58:07.912286 22683591385216 run.py:483] Algo bellman_ford step 883 current loss 0.102990, current_train_items 28288.
I0302 18:58:07.943179 22683591385216 run.py:483] Algo bellman_ford step 884 current loss 0.140444, current_train_items 28320.
I0302 18:58:07.961162 22683591385216 run.py:483] Algo bellman_ford step 885 current loss 0.009276, current_train_items 28352.
I0302 18:58:07.976604 22683591385216 run.py:483] Algo bellman_ford step 886 current loss 0.068472, current_train_items 28384.
I0302 18:58:07.998812 22683591385216 run.py:483] Algo bellman_ford step 887 current loss 0.099586, current_train_items 28416.
I0302 18:58:08.026003 22683591385216 run.py:483] Algo bellman_ford step 888 current loss 0.100609, current_train_items 28448.
I0302 18:58:08.056885 22683591385216 run.py:483] Algo bellman_ford step 889 current loss 0.156243, current_train_items 28480.
I0302 18:58:08.074558 22683591385216 run.py:483] Algo bellman_ford step 890 current loss 0.021751, current_train_items 28512.
I0302 18:58:08.090216 22683591385216 run.py:483] Algo bellman_ford step 891 current loss 0.061329, current_train_items 28544.
I0302 18:58:08.112006 22683591385216 run.py:483] Algo bellman_ford step 892 current loss 0.108505, current_train_items 28576.
I0302 18:58:08.140738 22683591385216 run.py:483] Algo bellman_ford step 893 current loss 0.133945, current_train_items 28608.
I0302 18:58:08.170164 22683591385216 run.py:483] Algo bellman_ford step 894 current loss 0.127272, current_train_items 28640.
I0302 18:58:08.187369 22683591385216 run.py:483] Algo bellman_ford step 895 current loss 0.025406, current_train_items 28672.
I0302 18:58:08.202445 22683591385216 run.py:483] Algo bellman_ford step 896 current loss 0.015343, current_train_items 28704.
I0302 18:58:08.224936 22683591385216 run.py:483] Algo bellman_ford step 897 current loss 0.113272, current_train_items 28736.
I0302 18:58:08.253114 22683591385216 run.py:483] Algo bellman_ford step 898 current loss 0.114784, current_train_items 28768.
I0302 18:58:08.284811 22683591385216 run.py:483] Algo bellman_ford step 899 current loss 0.191507, current_train_items 28800.
I0302 18:58:08.302363 22683591385216 run.py:483] Algo bellman_ford step 900 current loss 0.010770, current_train_items 28832.
I0302 18:58:08.310168 22683591385216 run.py:503] (val) algo bellman_ford step 900: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 28832, 'step': 900, 'algorithm': 'bellman_ford'}
I0302 18:58:08.310273 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0302 18:58:08.326483 22683591385216 run.py:483] Algo bellman_ford step 901 current loss 0.114922, current_train_items 28864.
I0302 18:58:08.349483 22683591385216 run.py:483] Algo bellman_ford step 902 current loss 0.088864, current_train_items 28896.
I0302 18:58:08.378802 22683591385216 run.py:483] Algo bellman_ford step 903 current loss 0.101795, current_train_items 28928.
I0302 18:58:08.409220 22683591385216 run.py:483] Algo bellman_ford step 904 current loss 0.154262, current_train_items 28960.
I0302 18:58:08.426846 22683591385216 run.py:483] Algo bellman_ford step 905 current loss 0.008575, current_train_items 28992.
I0302 18:58:08.442604 22683591385216 run.py:483] Algo bellman_ford step 906 current loss 0.073318, current_train_items 29024.
I0302 18:58:08.464799 22683591385216 run.py:483] Algo bellman_ford step 907 current loss 0.078259, current_train_items 29056.
I0302 18:58:08.493460 22683591385216 run.py:483] Algo bellman_ford step 908 current loss 0.132274, current_train_items 29088.
I0302 18:58:08.523997 22683591385216 run.py:483] Algo bellman_ford step 909 current loss 0.220983, current_train_items 29120.
I0302 18:58:08.541571 22683591385216 run.py:483] Algo bellman_ford step 910 current loss 0.009770, current_train_items 29152.
I0302 18:58:08.557641 22683591385216 run.py:483] Algo bellman_ford step 911 current loss 0.075043, current_train_items 29184.
I0302 18:58:08.580404 22683591385216 run.py:483] Algo bellman_ford step 912 current loss 0.178914, current_train_items 29216.
I0302 18:58:08.606814 22683591385216 run.py:483] Algo bellman_ford step 913 current loss 0.096130, current_train_items 29248.
I0302 18:58:08.637501 22683591385216 run.py:483] Algo bellman_ford step 914 current loss 0.194159, current_train_items 29280.
I0302 18:58:08.654746 22683591385216 run.py:483] Algo bellman_ford step 915 current loss 0.003399, current_train_items 29312.
I0302 18:58:08.670351 22683591385216 run.py:483] Algo bellman_ford step 916 current loss 0.046254, current_train_items 29344.
I0302 18:58:08.693180 22683591385216 run.py:483] Algo bellman_ford step 917 current loss 0.077267, current_train_items 29376.
I0302 18:58:08.721384 22683591385216 run.py:483] Algo bellman_ford step 918 current loss 0.120048, current_train_items 29408.
I0302 18:58:08.752777 22683591385216 run.py:483] Algo bellman_ford step 919 current loss 0.141593, current_train_items 29440.
I0302 18:58:08.770505 22683591385216 run.py:483] Algo bellman_ford step 920 current loss 0.017438, current_train_items 29472.
I0302 18:58:08.786259 22683591385216 run.py:483] Algo bellman_ford step 921 current loss 0.103888, current_train_items 29504.
I0302 18:58:08.808654 22683591385216 run.py:483] Algo bellman_ford step 922 current loss 0.103864, current_train_items 29536.
I0302 18:58:08.836361 22683591385216 run.py:483] Algo bellman_ford step 923 current loss 0.090848, current_train_items 29568.
I0302 18:58:08.866514 22683591385216 run.py:483] Algo bellman_ford step 924 current loss 0.154600, current_train_items 29600.
I0302 18:58:08.884095 22683591385216 run.py:483] Algo bellman_ford step 925 current loss 0.007625, current_train_items 29632.
I0302 18:58:08.899166 22683591385216 run.py:483] Algo bellman_ford step 926 current loss 0.058786, current_train_items 29664.
I0302 18:58:08.921586 22683591385216 run.py:483] Algo bellman_ford step 927 current loss 0.156566, current_train_items 29696.
I0302 18:58:08.950378 22683591385216 run.py:483] Algo bellman_ford step 928 current loss 0.170742, current_train_items 29728.
I0302 18:58:08.979048 22683591385216 run.py:483] Algo bellman_ford step 929 current loss 0.141266, current_train_items 29760.
I0302 18:58:08.996644 22683591385216 run.py:483] Algo bellman_ford step 930 current loss 0.031309, current_train_items 29792.
I0302 18:58:09.012180 22683591385216 run.py:483] Algo bellman_ford step 931 current loss 0.068919, current_train_items 29824.
I0302 18:58:09.034569 22683591385216 run.py:483] Algo bellman_ford step 932 current loss 0.078359, current_train_items 29856.
I0302 18:58:09.061819 22683591385216 run.py:483] Algo bellman_ford step 933 current loss 0.082779, current_train_items 29888.
I0302 18:58:09.092884 22683591385216 run.py:483] Algo bellman_ford step 934 current loss 0.207432, current_train_items 29920.
I0302 18:58:09.110309 22683591385216 run.py:483] Algo bellman_ford step 935 current loss 0.029533, current_train_items 29952.
I0302 18:58:09.125593 22683591385216 run.py:483] Algo bellman_ford step 936 current loss 0.037588, current_train_items 29984.
I0302 18:58:09.147015 22683591385216 run.py:483] Algo bellman_ford step 937 current loss 0.063790, current_train_items 30016.
I0302 18:58:09.174433 22683591385216 run.py:483] Algo bellman_ford step 938 current loss 0.101541, current_train_items 30048.
I0302 18:58:09.204637 22683591385216 run.py:483] Algo bellman_ford step 939 current loss 0.196606, current_train_items 30080.
I0302 18:58:09.222023 22683591385216 run.py:483] Algo bellman_ford step 940 current loss 0.011652, current_train_items 30112.
I0302 18:58:09.237418 22683591385216 run.py:483] Algo bellman_ford step 941 current loss 0.050725, current_train_items 30144.
I0302 18:58:09.260302 22683591385216 run.py:483] Algo bellman_ford step 942 current loss 0.142112, current_train_items 30176.
I0302 18:58:09.288358 22683591385216 run.py:483] Algo bellman_ford step 943 current loss 0.072278, current_train_items 30208.
I0302 18:58:09.319420 22683591385216 run.py:483] Algo bellman_ford step 944 current loss 0.152959, current_train_items 30240.
I0302 18:58:09.337085 22683591385216 run.py:483] Algo bellman_ford step 945 current loss 0.012691, current_train_items 30272.
I0302 18:58:09.351949 22683591385216 run.py:483] Algo bellman_ford step 946 current loss 0.065055, current_train_items 30304.
I0302 18:58:09.375320 22683591385216 run.py:483] Algo bellman_ford step 947 current loss 0.179838, current_train_items 30336.
I0302 18:58:09.402827 22683591385216 run.py:483] Algo bellman_ford step 948 current loss 0.129723, current_train_items 30368.
I0302 18:58:09.432487 22683591385216 run.py:483] Algo bellman_ford step 949 current loss 0.144601, current_train_items 30400.
I0302 18:58:09.450294 22683591385216 run.py:483] Algo bellman_ford step 950 current loss 0.019988, current_train_items 30432.
I0302 18:58:09.458153 22683591385216 run.py:503] (val) algo bellman_ford step 950: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 30432, 'step': 950, 'algorithm': 'bellman_ford'}
I0302 18:58:09.458259 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.968, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0302 18:58:09.474130 22683591385216 run.py:483] Algo bellman_ford step 951 current loss 0.052063, current_train_items 30464.
I0302 18:58:09.496406 22683591385216 run.py:483] Algo bellman_ford step 952 current loss 0.123295, current_train_items 30496.
I0302 18:58:09.524762 22683591385216 run.py:483] Algo bellman_ford step 953 current loss 0.200924, current_train_items 30528.
I0302 18:58:09.552317 22683591385216 run.py:483] Algo bellman_ford step 954 current loss 0.191819, current_train_items 30560.
I0302 18:58:09.569935 22683591385216 run.py:483] Algo bellman_ford step 955 current loss 0.015461, current_train_items 30592.
I0302 18:58:09.585227 22683591385216 run.py:483] Algo bellman_ford step 956 current loss 0.024891, current_train_items 30624.
I0302 18:58:09.607618 22683591385216 run.py:483] Algo bellman_ford step 957 current loss 0.072249, current_train_items 30656.
I0302 18:58:09.634138 22683591385216 run.py:483] Algo bellman_ford step 958 current loss 0.108608, current_train_items 30688.
I0302 18:58:09.664566 22683591385216 run.py:483] Algo bellman_ford step 959 current loss 0.149121, current_train_items 30720.
I0302 18:58:09.682229 22683591385216 run.py:483] Algo bellman_ford step 960 current loss 0.020011, current_train_items 30752.
I0302 18:58:09.698333 22683591385216 run.py:483] Algo bellman_ford step 961 current loss 0.086238, current_train_items 30784.
I0302 18:58:09.718931 22683591385216 run.py:483] Algo bellman_ford step 962 current loss 0.062440, current_train_items 30816.
I0302 18:58:09.745099 22683591385216 run.py:483] Algo bellman_ford step 963 current loss 0.076983, current_train_items 30848.
I0302 18:58:09.775669 22683591385216 run.py:483] Algo bellman_ford step 964 current loss 0.181436, current_train_items 30880.
I0302 18:58:09.792935 22683591385216 run.py:483] Algo bellman_ford step 965 current loss 0.031011, current_train_items 30912.
I0302 18:58:09.808075 22683591385216 run.py:483] Algo bellman_ford step 966 current loss 0.039658, current_train_items 30944.
I0302 18:58:09.830391 22683591385216 run.py:483] Algo bellman_ford step 967 current loss 0.090466, current_train_items 30976.
I0302 18:58:09.858677 22683591385216 run.py:483] Algo bellman_ford step 968 current loss 0.108981, current_train_items 31008.
I0302 18:58:09.888389 22683591385216 run.py:483] Algo bellman_ford step 969 current loss 0.167671, current_train_items 31040.
I0302 18:58:09.905883 22683591385216 run.py:483] Algo bellman_ford step 970 current loss 0.006910, current_train_items 31072.
I0302 18:58:09.921636 22683591385216 run.py:483] Algo bellman_ford step 971 current loss 0.019720, current_train_items 31104.
I0302 18:58:09.943947 22683591385216 run.py:483] Algo bellman_ford step 972 current loss 0.114700, current_train_items 31136.
I0302 18:58:09.972664 22683591385216 run.py:483] Algo bellman_ford step 973 current loss 0.125414, current_train_items 31168.
I0302 18:58:10.003940 22683591385216 run.py:483] Algo bellman_ford step 974 current loss 0.147922, current_train_items 31200.
I0302 18:58:10.021585 22683591385216 run.py:483] Algo bellman_ford step 975 current loss 0.018292, current_train_items 31232.
I0302 18:58:10.036942 22683591385216 run.py:483] Algo bellman_ford step 976 current loss 0.018876, current_train_items 31264.
I0302 18:58:10.059094 22683591385216 run.py:483] Algo bellman_ford step 977 current loss 0.046469, current_train_items 31296.
I0302 18:58:10.086275 22683591385216 run.py:483] Algo bellman_ford step 978 current loss 0.131077, current_train_items 31328.
I0302 18:58:10.115505 22683591385216 run.py:483] Algo bellman_ford step 979 current loss 0.162084, current_train_items 31360.
I0302 18:58:10.132448 22683591385216 run.py:483] Algo bellman_ford step 980 current loss 0.014872, current_train_items 31392.
I0302 18:58:10.147945 22683591385216 run.py:483] Algo bellman_ford step 981 current loss 0.051532, current_train_items 31424.
I0302 18:58:10.169780 22683591385216 run.py:483] Algo bellman_ford step 982 current loss 0.108323, current_train_items 31456.
I0302 18:58:10.196527 22683591385216 run.py:483] Algo bellman_ford step 983 current loss 0.084804, current_train_items 31488.
I0302 18:58:10.226812 22683591385216 run.py:483] Algo bellman_ford step 984 current loss 0.157178, current_train_items 31520.
I0302 18:58:10.244422 22683591385216 run.py:483] Algo bellman_ford step 985 current loss 0.008253, current_train_items 31552.
I0302 18:58:10.260342 22683591385216 run.py:483] Algo bellman_ford step 986 current loss 0.041428, current_train_items 31584.
I0302 18:58:10.281752 22683591385216 run.py:483] Algo bellman_ford step 987 current loss 0.061551, current_train_items 31616.
I0302 18:58:10.309693 22683591385216 run.py:483] Algo bellman_ford step 988 current loss 0.114346, current_train_items 31648.
I0302 18:58:10.338760 22683591385216 run.py:483] Algo bellman_ford step 989 current loss 0.097781, current_train_items 31680.
I0302 18:58:10.356164 22683591385216 run.py:483] Algo bellman_ford step 990 current loss 0.009300, current_train_items 31712.
I0302 18:58:10.371379 22683591385216 run.py:483] Algo bellman_ford step 991 current loss 0.021341, current_train_items 31744.
I0302 18:58:10.393254 22683591385216 run.py:483] Algo bellman_ford step 992 current loss 0.089021, current_train_items 31776.
I0302 18:58:10.419259 22683591385216 run.py:483] Algo bellman_ford step 993 current loss 0.105172, current_train_items 31808.
I0302 18:58:10.451493 22683591385216 run.py:483] Algo bellman_ford step 994 current loss 0.184679, current_train_items 31840.
I0302 18:58:10.468948 22683591385216 run.py:483] Algo bellman_ford step 995 current loss 0.019205, current_train_items 31872.
I0302 18:58:10.484047 22683591385216 run.py:483] Algo bellman_ford step 996 current loss 0.028163, current_train_items 31904.
I0302 18:58:10.505858 22683591385216 run.py:483] Algo bellman_ford step 997 current loss 0.084085, current_train_items 31936.
I0302 18:58:10.534137 22683591385216 run.py:483] Algo bellman_ford step 998 current loss 0.081998, current_train_items 31968.
I0302 18:58:10.563268 22683591385216 run.py:483] Algo bellman_ford step 999 current loss 0.121830, current_train_items 32000.
I0302 18:58:10.580775 22683591385216 run.py:483] Algo bellman_ford step 1000 current loss 0.007997, current_train_items 32032.
I0302 18:58:10.588337 22683591385216 run.py:503] (val) algo bellman_ford step 1000: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 32032, 'step': 1000, 'algorithm': 'bellman_ford'}
I0302 18:58:10.588442 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.968, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:58:10.616785 22683591385216 run.py:483] Algo bellman_ford step 1001 current loss 0.072288, current_train_items 32064.
I0302 18:58:10.639452 22683591385216 run.py:483] Algo bellman_ford step 1002 current loss 0.068838, current_train_items 32096.
I0302 18:58:10.666163 22683591385216 run.py:483] Algo bellman_ford step 1003 current loss 0.116205, current_train_items 32128.
I0302 18:58:10.698976 22683591385216 run.py:483] Algo bellman_ford step 1004 current loss 0.252887, current_train_items 32160.
I0302 18:58:10.717154 22683591385216 run.py:483] Algo bellman_ford step 1005 current loss 0.034832, current_train_items 32192.
I0302 18:58:10.732271 22683591385216 run.py:483] Algo bellman_ford step 1006 current loss 0.059190, current_train_items 32224.
I0302 18:58:10.755742 22683591385216 run.py:483] Algo bellman_ford step 1007 current loss 0.114442, current_train_items 32256.
I0302 18:58:10.782906 22683591385216 run.py:483] Algo bellman_ford step 1008 current loss 0.094557, current_train_items 32288.
I0302 18:58:10.813972 22683591385216 run.py:483] Algo bellman_ford step 1009 current loss 0.148306, current_train_items 32320.
I0302 18:58:10.831360 22683591385216 run.py:483] Algo bellman_ford step 1010 current loss 0.012564, current_train_items 32352.
I0302 18:58:10.846844 22683591385216 run.py:483] Algo bellman_ford step 1011 current loss 0.076251, current_train_items 32384.
I0302 18:58:10.870208 22683591385216 run.py:483] Algo bellman_ford step 1012 current loss 0.100237, current_train_items 32416.
I0302 18:58:10.898532 22683591385216 run.py:483] Algo bellman_ford step 1013 current loss 0.171715, current_train_items 32448.
I0302 18:58:10.929277 22683591385216 run.py:483] Algo bellman_ford step 1014 current loss 0.111435, current_train_items 32480.
I0302 18:58:10.947065 22683591385216 run.py:483] Algo bellman_ford step 1015 current loss 0.009241, current_train_items 32512.
I0302 18:58:10.962634 22683591385216 run.py:483] Algo bellman_ford step 1016 current loss 0.052135, current_train_items 32544.
I0302 18:58:10.985425 22683591385216 run.py:483] Algo bellman_ford step 1017 current loss 0.178468, current_train_items 32576.
I0302 18:58:11.013598 22683591385216 run.py:483] Algo bellman_ford step 1018 current loss 0.146442, current_train_items 32608.
I0302 18:58:11.046594 22683591385216 run.py:483] Algo bellman_ford step 1019 current loss 0.287496, current_train_items 32640.
I0302 18:58:11.064014 22683591385216 run.py:483] Algo bellman_ford step 1020 current loss 0.010986, current_train_items 32672.
I0302 18:58:11.079115 22683591385216 run.py:483] Algo bellman_ford step 1021 current loss 0.057940, current_train_items 32704.
I0302 18:58:11.101286 22683591385216 run.py:483] Algo bellman_ford step 1022 current loss 0.100056, current_train_items 32736.
I0302 18:58:11.128348 22683591385216 run.py:483] Algo bellman_ford step 1023 current loss 0.134265, current_train_items 32768.
I0302 18:58:11.156715 22683591385216 run.py:483] Algo bellman_ford step 1024 current loss 0.132919, current_train_items 32800.
I0302 18:58:11.174446 22683591385216 run.py:483] Algo bellman_ford step 1025 current loss 0.032166, current_train_items 32832.
I0302 18:58:11.190359 22683591385216 run.py:483] Algo bellman_ford step 1026 current loss 0.060419, current_train_items 32864.
I0302 18:58:11.212646 22683591385216 run.py:483] Algo bellman_ford step 1027 current loss 0.129839, current_train_items 32896.
I0302 18:58:11.241421 22683591385216 run.py:483] Algo bellman_ford step 1028 current loss 0.103633, current_train_items 32928.
I0302 18:58:11.270903 22683591385216 run.py:483] Algo bellman_ford step 1029 current loss 0.198809, current_train_items 32960.
I0302 18:58:11.288103 22683591385216 run.py:483] Algo bellman_ford step 1030 current loss 0.004382, current_train_items 32992.
I0302 18:58:11.303032 22683591385216 run.py:483] Algo bellman_ford step 1031 current loss 0.020979, current_train_items 33024.
I0302 18:58:11.324468 22683591385216 run.py:483] Algo bellman_ford step 1032 current loss 0.046093, current_train_items 33056.
I0302 18:58:11.351819 22683591385216 run.py:483] Algo bellman_ford step 1033 current loss 0.096994, current_train_items 33088.
I0302 18:58:11.382590 22683591385216 run.py:483] Algo bellman_ford step 1034 current loss 0.160321, current_train_items 33120.
I0302 18:58:11.399985 22683591385216 run.py:483] Algo bellman_ford step 1035 current loss 0.009667, current_train_items 33152.
I0302 18:58:11.415306 22683591385216 run.py:483] Algo bellman_ford step 1036 current loss 0.055728, current_train_items 33184.
I0302 18:58:11.438455 22683591385216 run.py:483] Algo bellman_ford step 1037 current loss 0.094076, current_train_items 33216.
I0302 18:58:11.466844 22683591385216 run.py:483] Algo bellman_ford step 1038 current loss 0.305548, current_train_items 33248.
I0302 18:58:11.498584 22683591385216 run.py:483] Algo bellman_ford step 1039 current loss 0.204835, current_train_items 33280.
I0302 18:58:11.516147 22683591385216 run.py:483] Algo bellman_ford step 1040 current loss 0.020070, current_train_items 33312.
I0302 18:58:11.532192 22683591385216 run.py:483] Algo bellman_ford step 1041 current loss 0.073108, current_train_items 33344.
I0302 18:58:11.554804 22683591385216 run.py:483] Algo bellman_ford step 1042 current loss 0.307352, current_train_items 33376.
I0302 18:58:11.582330 22683591385216 run.py:483] Algo bellman_ford step 1043 current loss 0.228218, current_train_items 33408.
I0302 18:58:11.612100 22683591385216 run.py:483] Algo bellman_ford step 1044 current loss 0.117949, current_train_items 33440.
I0302 18:58:11.629414 22683591385216 run.py:483] Algo bellman_ford step 1045 current loss 0.005779, current_train_items 33472.
I0302 18:58:11.644343 22683591385216 run.py:483] Algo bellman_ford step 1046 current loss 0.043318, current_train_items 33504.
I0302 18:58:11.666958 22683591385216 run.py:483] Algo bellman_ford step 1047 current loss 0.221527, current_train_items 33536.
I0302 18:58:11.695614 22683591385216 run.py:483] Algo bellman_ford step 1048 current loss 0.253403, current_train_items 33568.
I0302 18:58:11.726214 22683591385216 run.py:483] Algo bellman_ford step 1049 current loss 0.232484, current_train_items 33600.
I0302 18:58:11.743818 22683591385216 run.py:483] Algo bellman_ford step 1050 current loss 0.032079, current_train_items 33632.
I0302 18:58:11.751781 22683591385216 run.py:503] (val) algo bellman_ford step 1050: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 33632, 'step': 1050, 'algorithm': 'bellman_ford'}
I0302 18:58:11.751886 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:58:11.767433 22683591385216 run.py:483] Algo bellman_ford step 1051 current loss 0.044726, current_train_items 33664.
I0302 18:58:11.790735 22683591385216 run.py:483] Algo bellman_ford step 1052 current loss 0.194733, current_train_items 33696.
I0302 18:58:11.818192 22683591385216 run.py:483] Algo bellman_ford step 1053 current loss 0.176969, current_train_items 33728.
I0302 18:58:11.848852 22683591385216 run.py:483] Algo bellman_ford step 1054 current loss 0.251893, current_train_items 33760.
I0302 18:58:11.866933 22683591385216 run.py:483] Algo bellman_ford step 1055 current loss 0.018900, current_train_items 33792.
I0302 18:58:11.882516 22683591385216 run.py:483] Algo bellman_ford step 1056 current loss 0.037641, current_train_items 33824.
I0302 18:58:11.905353 22683591385216 run.py:483] Algo bellman_ford step 1057 current loss 0.150357, current_train_items 33856.
I0302 18:58:11.934081 22683591385216 run.py:483] Algo bellman_ford step 1058 current loss 0.130600, current_train_items 33888.
I0302 18:58:11.961812 22683591385216 run.py:483] Algo bellman_ford step 1059 current loss 0.108220, current_train_items 33920.
I0302 18:58:11.979361 22683591385216 run.py:483] Algo bellman_ford step 1060 current loss 0.002931, current_train_items 33952.
I0302 18:58:11.994893 22683591385216 run.py:483] Algo bellman_ford step 1061 current loss 0.020864, current_train_items 33984.
I0302 18:58:12.015705 22683591385216 run.py:483] Algo bellman_ford step 1062 current loss 0.054147, current_train_items 34016.
I0302 18:58:12.042604 22683591385216 run.py:483] Algo bellman_ford step 1063 current loss 0.119183, current_train_items 34048.
I0302 18:58:12.073513 22683591385216 run.py:483] Algo bellman_ford step 1064 current loss 0.143670, current_train_items 34080.
I0302 18:58:12.091112 22683591385216 run.py:483] Algo bellman_ford step 1065 current loss 0.004895, current_train_items 34112.
I0302 18:58:12.106398 22683591385216 run.py:483] Algo bellman_ford step 1066 current loss 0.031118, current_train_items 34144.
I0302 18:58:12.128746 22683591385216 run.py:483] Algo bellman_ford step 1067 current loss 0.087501, current_train_items 34176.
I0302 18:58:12.156540 22683591385216 run.py:483] Algo bellman_ford step 1068 current loss 0.087555, current_train_items 34208.
I0302 18:58:12.188111 22683591385216 run.py:483] Algo bellman_ford step 1069 current loss 0.226786, current_train_items 34240.
I0302 18:58:12.205734 22683591385216 run.py:483] Algo bellman_ford step 1070 current loss 0.003383, current_train_items 34272.
I0302 18:58:12.221744 22683591385216 run.py:483] Algo bellman_ford step 1071 current loss 0.083730, current_train_items 34304.
I0302 18:58:12.243869 22683591385216 run.py:483] Algo bellman_ford step 1072 current loss 0.111271, current_train_items 34336.
I0302 18:58:12.271520 22683591385216 run.py:483] Algo bellman_ford step 1073 current loss 0.069589, current_train_items 34368.
I0302 18:58:12.300546 22683591385216 run.py:483] Algo bellman_ford step 1074 current loss 0.091430, current_train_items 34400.
I0302 18:58:12.318029 22683591385216 run.py:483] Algo bellman_ford step 1075 current loss 0.005768, current_train_items 34432.
I0302 18:58:12.333781 22683591385216 run.py:483] Algo bellman_ford step 1076 current loss 0.058490, current_train_items 34464.
I0302 18:58:12.356101 22683591385216 run.py:483] Algo bellman_ford step 1077 current loss 0.079992, current_train_items 34496.
I0302 18:58:12.384398 22683591385216 run.py:483] Algo bellman_ford step 1078 current loss 0.102367, current_train_items 34528.
I0302 18:58:12.414197 22683591385216 run.py:483] Algo bellman_ford step 1079 current loss 0.111140, current_train_items 34560.
I0302 18:58:12.431757 22683591385216 run.py:483] Algo bellman_ford step 1080 current loss 0.012128, current_train_items 34592.
I0302 18:58:12.447470 22683591385216 run.py:483] Algo bellman_ford step 1081 current loss 0.041878, current_train_items 34624.
I0302 18:58:12.469153 22683591385216 run.py:483] Algo bellman_ford step 1082 current loss 0.137666, current_train_items 34656.
I0302 18:58:12.496408 22683591385216 run.py:483] Algo bellman_ford step 1083 current loss 0.075382, current_train_items 34688.
I0302 18:58:12.527672 22683591385216 run.py:483] Algo bellman_ford step 1084 current loss 0.128305, current_train_items 34720.
I0302 18:58:12.545467 22683591385216 run.py:483] Algo bellman_ford step 1085 current loss 0.010930, current_train_items 34752.
I0302 18:58:12.561130 22683591385216 run.py:483] Algo bellman_ford step 1086 current loss 0.034972, current_train_items 34784.
I0302 18:58:12.583394 22683591385216 run.py:483] Algo bellman_ford step 1087 current loss 0.102499, current_train_items 34816.
I0302 18:58:12.610004 22683591385216 run.py:483] Algo bellman_ford step 1088 current loss 0.048807, current_train_items 34848.
I0302 18:58:12.638453 22683591385216 run.py:483] Algo bellman_ford step 1089 current loss 0.096071, current_train_items 34880.
I0302 18:58:12.656157 22683591385216 run.py:483] Algo bellman_ford step 1090 current loss 0.020541, current_train_items 34912.
I0302 18:58:12.672077 22683591385216 run.py:483] Algo bellman_ford step 1091 current loss 0.137539, current_train_items 34944.
I0302 18:58:12.693837 22683591385216 run.py:483] Algo bellman_ford step 1092 current loss 0.158447, current_train_items 34976.
I0302 18:58:12.722224 22683591385216 run.py:483] Algo bellman_ford step 1093 current loss 0.203245, current_train_items 35008.
I0302 18:58:12.753325 22683591385216 run.py:483] Algo bellman_ford step 1094 current loss 0.120324, current_train_items 35040.
I0302 18:58:12.770474 22683591385216 run.py:483] Algo bellman_ford step 1095 current loss 0.021734, current_train_items 35072.
I0302 18:58:12.785745 22683591385216 run.py:483] Algo bellman_ford step 1096 current loss 0.126577, current_train_items 35104.
I0302 18:58:12.807645 22683591385216 run.py:483] Algo bellman_ford step 1097 current loss 0.373925, current_train_items 35136.
I0302 18:58:12.835763 22683591385216 run.py:483] Algo bellman_ford step 1098 current loss 0.381619, current_train_items 35168.
I0302 18:58:12.865202 22683591385216 run.py:483] Algo bellman_ford step 1099 current loss 0.331204, current_train_items 35200.
I0302 18:58:12.883108 22683591385216 run.py:483] Algo bellman_ford step 1100 current loss 0.003995, current_train_items 35232.
I0302 18:58:12.891021 22683591385216 run.py:503] (val) algo bellman_ford step 1100: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 35232, 'step': 1100, 'algorithm': 'bellman_ford'}
I0302 18:58:12.891126 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:58:12.907044 22683591385216 run.py:483] Algo bellman_ford step 1101 current loss 0.050855, current_train_items 35264.
I0302 18:58:12.929095 22683591385216 run.py:483] Algo bellman_ford step 1102 current loss 0.109925, current_train_items 35296.
I0302 18:58:12.958404 22683591385216 run.py:483] Algo bellman_ford step 1103 current loss 0.140592, current_train_items 35328.
I0302 18:58:12.990483 22683591385216 run.py:483] Algo bellman_ford step 1104 current loss 0.222032, current_train_items 35360.
I0302 18:58:13.008044 22683591385216 run.py:483] Algo bellman_ford step 1105 current loss 0.008574, current_train_items 35392.
I0302 18:58:13.023231 22683591385216 run.py:483] Algo bellman_ford step 1106 current loss 0.018059, current_train_items 35424.
I0302 18:58:13.045508 22683591385216 run.py:483] Algo bellman_ford step 1107 current loss 0.125937, current_train_items 35456.
I0302 18:58:13.072461 22683591385216 run.py:483] Algo bellman_ford step 1108 current loss 0.127487, current_train_items 35488.
I0302 18:58:13.101208 22683591385216 run.py:483] Algo bellman_ford step 1109 current loss 0.122193, current_train_items 35520.
I0302 18:58:13.118593 22683591385216 run.py:483] Algo bellman_ford step 1110 current loss 0.026915, current_train_items 35552.
I0302 18:58:13.134000 22683591385216 run.py:483] Algo bellman_ford step 1111 current loss 0.051102, current_train_items 35584.
I0302 18:58:13.156420 22683591385216 run.py:483] Algo bellman_ford step 1112 current loss 0.108431, current_train_items 35616.
I0302 18:58:13.182725 22683591385216 run.py:483] Algo bellman_ford step 1113 current loss 0.103148, current_train_items 35648.
I0302 18:58:13.214368 22683591385216 run.py:483] Algo bellman_ford step 1114 current loss 0.149121, current_train_items 35680.
I0302 18:58:13.231616 22683591385216 run.py:483] Algo bellman_ford step 1115 current loss 0.010348, current_train_items 35712.
I0302 18:58:13.246617 22683591385216 run.py:483] Algo bellman_ford step 1116 current loss 0.030988, current_train_items 35744.
I0302 18:58:13.269423 22683591385216 run.py:483] Algo bellman_ford step 1117 current loss 0.086091, current_train_items 35776.
I0302 18:58:13.297646 22683591385216 run.py:483] Algo bellman_ford step 1118 current loss 0.076822, current_train_items 35808.
I0302 18:58:13.328330 22683591385216 run.py:483] Algo bellman_ford step 1119 current loss 0.188395, current_train_items 35840.
I0302 18:58:13.346061 22683591385216 run.py:483] Algo bellman_ford step 1120 current loss 0.010942, current_train_items 35872.
I0302 18:58:13.360910 22683591385216 run.py:483] Algo bellman_ford step 1121 current loss 0.042644, current_train_items 35904.
I0302 18:58:13.383330 22683591385216 run.py:483] Algo bellman_ford step 1122 current loss 0.098215, current_train_items 35936.
I0302 18:58:13.410238 22683591385216 run.py:483] Algo bellman_ford step 1123 current loss 0.082991, current_train_items 35968.
I0302 18:58:13.442036 22683591385216 run.py:483] Algo bellman_ford step 1124 current loss 0.159259, current_train_items 36000.
I0302 18:58:13.459615 22683591385216 run.py:483] Algo bellman_ford step 1125 current loss 0.005127, current_train_items 36032.
I0302 18:58:13.474633 22683591385216 run.py:483] Algo bellman_ford step 1126 current loss 0.052890, current_train_items 36064.
I0302 18:58:13.496828 22683591385216 run.py:483] Algo bellman_ford step 1127 current loss 0.156283, current_train_items 36096.
I0302 18:58:13.524811 22683591385216 run.py:483] Algo bellman_ford step 1128 current loss 0.121757, current_train_items 36128.
I0302 18:58:13.552816 22683591385216 run.py:483] Algo bellman_ford step 1129 current loss 0.136476, current_train_items 36160.
I0302 18:58:13.570296 22683591385216 run.py:483] Algo bellman_ford step 1130 current loss 0.007712, current_train_items 36192.
I0302 18:58:13.585619 22683591385216 run.py:483] Algo bellman_ford step 1131 current loss 0.030173, current_train_items 36224.
I0302 18:58:13.608453 22683591385216 run.py:483] Algo bellman_ford step 1132 current loss 0.185600, current_train_items 36256.
I0302 18:58:13.636345 22683591385216 run.py:483] Algo bellman_ford step 1133 current loss 0.137648, current_train_items 36288.
I0302 18:58:13.665371 22683591385216 run.py:483] Algo bellman_ford step 1134 current loss 0.125660, current_train_items 36320.
I0302 18:58:13.682470 22683591385216 run.py:483] Algo bellman_ford step 1135 current loss 0.028846, current_train_items 36352.
I0302 18:58:13.698000 22683591385216 run.py:483] Algo bellman_ford step 1136 current loss 0.038025, current_train_items 36384.
I0302 18:58:13.719661 22683591385216 run.py:483] Algo bellman_ford step 1137 current loss 0.067381, current_train_items 36416.
I0302 18:58:13.748469 22683591385216 run.py:483] Algo bellman_ford step 1138 current loss 0.094836, current_train_items 36448.
I0302 18:58:13.778042 22683591385216 run.py:483] Algo bellman_ford step 1139 current loss 0.135292, current_train_items 36480.
I0302 18:58:13.795603 22683591385216 run.py:483] Algo bellman_ford step 1140 current loss 0.007704, current_train_items 36512.
I0302 18:58:13.811192 22683591385216 run.py:483] Algo bellman_ford step 1141 current loss 0.039623, current_train_items 36544.
I0302 18:58:13.832821 22683591385216 run.py:483] Algo bellman_ford step 1142 current loss 0.165244, current_train_items 36576.
I0302 18:58:13.859835 22683591385216 run.py:483] Algo bellman_ford step 1143 current loss 0.201064, current_train_items 36608.
I0302 18:58:13.889839 22683591385216 run.py:483] Algo bellman_ford step 1144 current loss 0.123273, current_train_items 36640.
I0302 18:58:13.907349 22683591385216 run.py:483] Algo bellman_ford step 1145 current loss 0.031481, current_train_items 36672.
I0302 18:58:13.922781 22683591385216 run.py:483] Algo bellman_ford step 1146 current loss 0.108257, current_train_items 36704.
I0302 18:58:13.945094 22683591385216 run.py:483] Algo bellman_ford step 1147 current loss 0.113434, current_train_items 36736.
I0302 18:58:13.972973 22683591385216 run.py:483] Algo bellman_ford step 1148 current loss 0.102356, current_train_items 36768.
I0302 18:58:14.000579 22683591385216 run.py:483] Algo bellman_ford step 1149 current loss 0.087677, current_train_items 36800.
I0302 18:58:14.018142 22683591385216 run.py:483] Algo bellman_ford step 1150 current loss 0.037717, current_train_items 36832.
I0302 18:58:14.025786 22683591385216 run.py:503] (val) algo bellman_ford step 1150: {'pi': 0.9638671875, 'score': 0.9638671875, 'examples_seen': 36832, 'step': 1150, 'algorithm': 'bellman_ford'}
I0302 18:58:14.025893 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.964, val scores are: bellman_ford: 0.964
I0302 18:58:14.041692 22683591385216 run.py:483] Algo bellman_ford step 1151 current loss 0.033158, current_train_items 36864.
I0302 18:58:14.064725 22683591385216 run.py:483] Algo bellman_ford step 1152 current loss 0.209324, current_train_items 36896.
I0302 18:58:14.094163 22683591385216 run.py:483] Algo bellman_ford step 1153 current loss 0.192427, current_train_items 36928.
W0302 18:58:14.114591 22683591385216 samplers.py:155] Increasing hint lengh from 11 to 12
I0302 18:58:19.433349 22683591385216 run.py:483] Algo bellman_ford step 1154 current loss 0.119803, current_train_items 36960.
I0302 18:58:19.452415 22683591385216 run.py:483] Algo bellman_ford step 1155 current loss 0.007626, current_train_items 36992.
I0302 18:58:19.467968 22683591385216 run.py:483] Algo bellman_ford step 1156 current loss 0.069347, current_train_items 37024.
I0302 18:58:19.490309 22683591385216 run.py:483] Algo bellman_ford step 1157 current loss 0.197593, current_train_items 37056.
I0302 18:58:19.518216 22683591385216 run.py:483] Algo bellman_ford step 1158 current loss 0.304664, current_train_items 37088.
I0302 18:58:19.546854 22683591385216 run.py:483] Algo bellman_ford step 1159 current loss 0.357920, current_train_items 37120.
I0302 18:58:19.565483 22683591385216 run.py:483] Algo bellman_ford step 1160 current loss 0.032611, current_train_items 37152.
I0302 18:58:19.581308 22683591385216 run.py:483] Algo bellman_ford step 1161 current loss 0.034474, current_train_items 37184.
I0302 18:58:19.602632 22683591385216 run.py:483] Algo bellman_ford step 1162 current loss 0.051547, current_train_items 37216.
I0302 18:58:19.630241 22683591385216 run.py:483] Algo bellman_ford step 1163 current loss 0.105890, current_train_items 37248.
I0302 18:58:19.661007 22683591385216 run.py:483] Algo bellman_ford step 1164 current loss 0.187614, current_train_items 37280.
I0302 18:58:19.678999 22683591385216 run.py:483] Algo bellman_ford step 1165 current loss 0.004142, current_train_items 37312.
I0302 18:58:19.694698 22683591385216 run.py:483] Algo bellman_ford step 1166 current loss 0.067151, current_train_items 37344.
I0302 18:58:19.716913 22683591385216 run.py:483] Algo bellman_ford step 1167 current loss 0.123529, current_train_items 37376.
I0302 18:58:19.745264 22683591385216 run.py:483] Algo bellman_ford step 1168 current loss 0.106909, current_train_items 37408.
I0302 18:58:19.776996 22683591385216 run.py:483] Algo bellman_ford step 1169 current loss 0.118499, current_train_items 37440.
I0302 18:58:19.795434 22683591385216 run.py:483] Algo bellman_ford step 1170 current loss 0.013991, current_train_items 37472.
I0302 18:58:19.811477 22683591385216 run.py:483] Algo bellman_ford step 1171 current loss 0.059785, current_train_items 37504.
I0302 18:58:19.833752 22683591385216 run.py:483] Algo bellman_ford step 1172 current loss 0.092746, current_train_items 37536.
I0302 18:58:19.862590 22683591385216 run.py:483] Algo bellman_ford step 1173 current loss 0.113164, current_train_items 37568.
I0302 18:58:19.893343 22683591385216 run.py:483] Algo bellman_ford step 1174 current loss 0.153611, current_train_items 37600.
I0302 18:58:19.911555 22683591385216 run.py:483] Algo bellman_ford step 1175 current loss 0.003723, current_train_items 37632.
I0302 18:58:19.927252 22683591385216 run.py:483] Algo bellman_ford step 1176 current loss 0.017021, current_train_items 37664.
I0302 18:58:19.948645 22683591385216 run.py:483] Algo bellman_ford step 1177 current loss 0.100555, current_train_items 37696.
I0302 18:58:19.975840 22683591385216 run.py:483] Algo bellman_ford step 1178 current loss 0.079489, current_train_items 37728.
I0302 18:58:20.007953 22683591385216 run.py:483] Algo bellman_ford step 1179 current loss 0.161406, current_train_items 37760.
I0302 18:58:20.025539 22683591385216 run.py:483] Algo bellman_ford step 1180 current loss 0.003973, current_train_items 37792.
I0302 18:58:20.041287 22683591385216 run.py:483] Algo bellman_ford step 1181 current loss 0.070975, current_train_items 37824.
I0302 18:58:20.063416 22683591385216 run.py:483] Algo bellman_ford step 1182 current loss 0.073966, current_train_items 37856.
I0302 18:58:20.090471 22683591385216 run.py:483] Algo bellman_ford step 1183 current loss 0.155106, current_train_items 37888.
I0302 18:58:20.120713 22683591385216 run.py:483] Algo bellman_ford step 1184 current loss 0.153040, current_train_items 37920.
I0302 18:58:20.139136 22683591385216 run.py:483] Algo bellman_ford step 1185 current loss 0.020235, current_train_items 37952.
I0302 18:58:20.154508 22683591385216 run.py:483] Algo bellman_ford step 1186 current loss 0.029163, current_train_items 37984.
I0302 18:58:20.176500 22683591385216 run.py:483] Algo bellman_ford step 1187 current loss 0.107753, current_train_items 38016.
I0302 18:58:20.204275 22683591385216 run.py:483] Algo bellman_ford step 1188 current loss 0.075929, current_train_items 38048.
I0302 18:58:20.233267 22683591385216 run.py:483] Algo bellman_ford step 1189 current loss 0.139177, current_train_items 38080.
I0302 18:58:20.251359 22683591385216 run.py:483] Algo bellman_ford step 1190 current loss 0.025725, current_train_items 38112.
I0302 18:58:20.267113 22683591385216 run.py:483] Algo bellman_ford step 1191 current loss 0.065622, current_train_items 38144.
I0302 18:58:20.290212 22683591385216 run.py:483] Algo bellman_ford step 1192 current loss 0.100933, current_train_items 38176.
I0302 18:58:20.318120 22683591385216 run.py:483] Algo bellman_ford step 1193 current loss 0.083650, current_train_items 38208.
I0302 18:58:20.348653 22683591385216 run.py:483] Algo bellman_ford step 1194 current loss 0.110590, current_train_items 38240.
I0302 18:58:20.366365 22683591385216 run.py:483] Algo bellman_ford step 1195 current loss 0.016174, current_train_items 38272.
I0302 18:58:20.381712 22683591385216 run.py:483] Algo bellman_ford step 1196 current loss 0.040273, current_train_items 38304.
I0302 18:58:20.403417 22683591385216 run.py:483] Algo bellman_ford step 1197 current loss 0.081586, current_train_items 38336.
I0302 18:58:20.432231 22683591385216 run.py:483] Algo bellman_ford step 1198 current loss 0.125679, current_train_items 38368.
I0302 18:58:20.463497 22683591385216 run.py:483] Algo bellman_ford step 1199 current loss 0.171031, current_train_items 38400.
I0302 18:58:20.481853 22683591385216 run.py:483] Algo bellman_ford step 1200 current loss 0.021278, current_train_items 38432.
I0302 18:58:20.491250 22683591385216 run.py:503] (val) algo bellman_ford step 1200: {'pi': 0.970703125, 'score': 0.970703125, 'examples_seen': 38432, 'step': 1200, 'algorithm': 'bellman_ford'}
I0302 18:58:20.491357 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.971, val scores are: bellman_ford: 0.971
I0302 18:58:20.507504 22683591385216 run.py:483] Algo bellman_ford step 1201 current loss 0.072805, current_train_items 38464.
I0302 18:58:20.529854 22683591385216 run.py:483] Algo bellman_ford step 1202 current loss 0.062120, current_train_items 38496.
I0302 18:58:20.558805 22683591385216 run.py:483] Algo bellman_ford step 1203 current loss 0.122327, current_train_items 38528.
I0302 18:58:20.590349 22683591385216 run.py:483] Algo bellman_ford step 1204 current loss 0.138369, current_train_items 38560.
I0302 18:58:20.608233 22683591385216 run.py:483] Algo bellman_ford step 1205 current loss 0.010886, current_train_items 38592.
I0302 18:58:20.623684 22683591385216 run.py:483] Algo bellman_ford step 1206 current loss 0.016789, current_train_items 38624.
I0302 18:58:20.645138 22683591385216 run.py:483] Algo bellman_ford step 1207 current loss 0.059027, current_train_items 38656.
I0302 18:58:20.673473 22683591385216 run.py:483] Algo bellman_ford step 1208 current loss 0.083966, current_train_items 38688.
I0302 18:58:20.705034 22683591385216 run.py:483] Algo bellman_ford step 1209 current loss 0.102133, current_train_items 38720.
I0302 18:58:20.722777 22683591385216 run.py:483] Algo bellman_ford step 1210 current loss 0.003919, current_train_items 38752.
I0302 18:58:20.738131 22683591385216 run.py:483] Algo bellman_ford step 1211 current loss 0.064748, current_train_items 38784.
I0302 18:58:20.759442 22683591385216 run.py:483] Algo bellman_ford step 1212 current loss 0.068134, current_train_items 38816.
I0302 18:58:20.786169 22683591385216 run.py:483] Algo bellman_ford step 1213 current loss 0.071478, current_train_items 38848.
I0302 18:58:20.816357 22683591385216 run.py:483] Algo bellman_ford step 1214 current loss 0.110692, current_train_items 38880.
I0302 18:58:20.834014 22683591385216 run.py:483] Algo bellman_ford step 1215 current loss 0.010212, current_train_items 38912.
I0302 18:58:20.849023 22683591385216 run.py:483] Algo bellman_ford step 1216 current loss 0.019594, current_train_items 38944.
I0302 18:58:20.870810 22683591385216 run.py:483] Algo bellman_ford step 1217 current loss 0.108819, current_train_items 38976.
I0302 18:58:20.899526 22683591385216 run.py:483] Algo bellman_ford step 1218 current loss 0.135540, current_train_items 39008.
I0302 18:58:20.930100 22683591385216 run.py:483] Algo bellman_ford step 1219 current loss 0.116187, current_train_items 39040.
I0302 18:58:20.947908 22683591385216 run.py:483] Algo bellman_ford step 1220 current loss 0.005061, current_train_items 39072.
I0302 18:58:20.963029 22683591385216 run.py:483] Algo bellman_ford step 1221 current loss 0.028952, current_train_items 39104.
I0302 18:58:20.985797 22683591385216 run.py:483] Algo bellman_ford step 1222 current loss 0.128722, current_train_items 39136.
I0302 18:58:21.014134 22683591385216 run.py:483] Algo bellman_ford step 1223 current loss 0.111590, current_train_items 39168.
I0302 18:58:21.044059 22683591385216 run.py:483] Algo bellman_ford step 1224 current loss 0.076774, current_train_items 39200.
I0302 18:58:21.061730 22683591385216 run.py:483] Algo bellman_ford step 1225 current loss 0.020050, current_train_items 39232.
I0302 18:58:21.077265 22683591385216 run.py:483] Algo bellman_ford step 1226 current loss 0.036623, current_train_items 39264.
I0302 18:58:21.100036 22683591385216 run.py:483] Algo bellman_ford step 1227 current loss 0.115383, current_train_items 39296.
I0302 18:58:21.127609 22683591385216 run.py:483] Algo bellman_ford step 1228 current loss 0.110300, current_train_items 39328.
I0302 18:58:21.159374 22683591385216 run.py:483] Algo bellman_ford step 1229 current loss 0.146155, current_train_items 39360.
I0302 18:58:21.177108 22683591385216 run.py:483] Algo bellman_ford step 1230 current loss 0.010815, current_train_items 39392.
I0302 18:58:21.192574 22683591385216 run.py:483] Algo bellman_ford step 1231 current loss 0.076019, current_train_items 39424.
I0302 18:58:21.215269 22683591385216 run.py:483] Algo bellman_ford step 1232 current loss 0.169645, current_train_items 39456.
I0302 18:58:21.243080 22683591385216 run.py:483] Algo bellman_ford step 1233 current loss 0.075510, current_train_items 39488.
I0302 18:58:21.272039 22683591385216 run.py:483] Algo bellman_ford step 1234 current loss 0.088542, current_train_items 39520.
I0302 18:58:21.289529 22683591385216 run.py:483] Algo bellman_ford step 1235 current loss 0.006671, current_train_items 39552.
I0302 18:58:21.304443 22683591385216 run.py:483] Algo bellman_ford step 1236 current loss 0.037447, current_train_items 39584.
I0302 18:58:21.326179 22683591385216 run.py:483] Algo bellman_ford step 1237 current loss 0.131905, current_train_items 39616.
I0302 18:58:21.354070 22683591385216 run.py:483] Algo bellman_ford step 1238 current loss 0.145540, current_train_items 39648.
I0302 18:58:21.385354 22683591385216 run.py:483] Algo bellman_ford step 1239 current loss 0.121167, current_train_items 39680.
I0302 18:58:21.403025 22683591385216 run.py:483] Algo bellman_ford step 1240 current loss 0.015395, current_train_items 39712.
I0302 18:58:21.418375 22683591385216 run.py:483] Algo bellman_ford step 1241 current loss 0.028081, current_train_items 39744.
I0302 18:58:21.441041 22683591385216 run.py:483] Algo bellman_ford step 1242 current loss 0.061944, current_train_items 39776.
I0302 18:58:21.468814 22683591385216 run.py:483] Algo bellman_ford step 1243 current loss 0.100469, current_train_items 39808.
I0302 18:58:21.499400 22683591385216 run.py:483] Algo bellman_ford step 1244 current loss 0.193486, current_train_items 39840.
I0302 18:58:21.517000 22683591385216 run.py:483] Algo bellman_ford step 1245 current loss 0.004351, current_train_items 39872.
I0302 18:58:21.532149 22683591385216 run.py:483] Algo bellman_ford step 1246 current loss 0.022206, current_train_items 39904.
I0302 18:58:21.553395 22683591385216 run.py:483] Algo bellman_ford step 1247 current loss 0.061440, current_train_items 39936.
I0302 18:58:21.581643 22683591385216 run.py:483] Algo bellman_ford step 1248 current loss 0.087438, current_train_items 39968.
I0302 18:58:21.611391 22683591385216 run.py:483] Algo bellman_ford step 1249 current loss 0.109398, current_train_items 40000.
I0302 18:58:21.629264 22683591385216 run.py:483] Algo bellman_ford step 1250 current loss 0.001683, current_train_items 40032.
I0302 18:58:21.637734 22683591385216 run.py:503] (val) algo bellman_ford step 1250: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 40032, 'step': 1250, 'algorithm': 'bellman_ford'}
I0302 18:58:21.637840 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:58:21.653775 22683591385216 run.py:483] Algo bellman_ford step 1251 current loss 0.058438, current_train_items 40064.
I0302 18:58:21.676163 22683591385216 run.py:483] Algo bellman_ford step 1252 current loss 0.068485, current_train_items 40096.
I0302 18:58:21.702890 22683591385216 run.py:483] Algo bellman_ford step 1253 current loss 0.064142, current_train_items 40128.
I0302 18:58:21.730766 22683591385216 run.py:483] Algo bellman_ford step 1254 current loss 0.124259, current_train_items 40160.
I0302 18:58:21.748980 22683591385216 run.py:483] Algo bellman_ford step 1255 current loss 0.005533, current_train_items 40192.
I0302 18:58:21.764587 22683591385216 run.py:483] Algo bellman_ford step 1256 current loss 0.065869, current_train_items 40224.
I0302 18:58:21.786592 22683591385216 run.py:483] Algo bellman_ford step 1257 current loss 0.102369, current_train_items 40256.
I0302 18:58:21.813731 22683591385216 run.py:483] Algo bellman_ford step 1258 current loss 0.116535, current_train_items 40288.
I0302 18:58:21.843341 22683591385216 run.py:483] Algo bellman_ford step 1259 current loss 0.090375, current_train_items 40320.
I0302 18:58:21.861324 22683591385216 run.py:483] Algo bellman_ford step 1260 current loss 0.017407, current_train_items 40352.
I0302 18:58:21.877541 22683591385216 run.py:483] Algo bellman_ford step 1261 current loss 0.035172, current_train_items 40384.
I0302 18:58:21.900184 22683591385216 run.py:483] Algo bellman_ford step 1262 current loss 0.072640, current_train_items 40416.
I0302 18:58:21.925241 22683591385216 run.py:483] Algo bellman_ford step 1263 current loss 0.075994, current_train_items 40448.
I0302 18:58:21.957917 22683591385216 run.py:483] Algo bellman_ford step 1264 current loss 0.121906, current_train_items 40480.
I0302 18:58:21.975431 22683591385216 run.py:483] Algo bellman_ford step 1265 current loss 0.004353, current_train_items 40512.
I0302 18:58:21.990790 22683591385216 run.py:483] Algo bellman_ford step 1266 current loss 0.042592, current_train_items 40544.
I0302 18:58:22.012010 22683591385216 run.py:483] Algo bellman_ford step 1267 current loss 0.107904, current_train_items 40576.
I0302 18:58:22.039510 22683591385216 run.py:483] Algo bellman_ford step 1268 current loss 0.072284, current_train_items 40608.
I0302 18:58:22.070444 22683591385216 run.py:483] Algo bellman_ford step 1269 current loss 0.083208, current_train_items 40640.
I0302 18:58:22.088802 22683591385216 run.py:483] Algo bellman_ford step 1270 current loss 0.011197, current_train_items 40672.
I0302 18:58:22.104230 22683591385216 run.py:483] Algo bellman_ford step 1271 current loss 0.067426, current_train_items 40704.
I0302 18:58:22.124948 22683591385216 run.py:483] Algo bellman_ford step 1272 current loss 0.137431, current_train_items 40736.
I0302 18:58:22.151827 22683591385216 run.py:483] Algo bellman_ford step 1273 current loss 0.058915, current_train_items 40768.
I0302 18:58:22.182491 22683591385216 run.py:483] Algo bellman_ford step 1274 current loss 0.126915, current_train_items 40800.
I0302 18:58:22.200513 22683591385216 run.py:483] Algo bellman_ford step 1275 current loss 0.004691, current_train_items 40832.
I0302 18:58:22.216271 22683591385216 run.py:483] Algo bellman_ford step 1276 current loss 0.033181, current_train_items 40864.
I0302 18:58:22.237011 22683591385216 run.py:483] Algo bellman_ford step 1277 current loss 0.081047, current_train_items 40896.
I0302 18:58:22.264508 22683591385216 run.py:483] Algo bellman_ford step 1278 current loss 0.117161, current_train_items 40928.
I0302 18:58:22.294177 22683591385216 run.py:483] Algo bellman_ford step 1279 current loss 0.157856, current_train_items 40960.
I0302 18:58:22.311817 22683591385216 run.py:483] Algo bellman_ford step 1280 current loss 0.010225, current_train_items 40992.
I0302 18:58:22.326968 22683591385216 run.py:483] Algo bellman_ford step 1281 current loss 0.041537, current_train_items 41024.
I0302 18:58:22.349170 22683591385216 run.py:483] Algo bellman_ford step 1282 current loss 0.117840, current_train_items 41056.
I0302 18:58:22.377543 22683591385216 run.py:483] Algo bellman_ford step 1283 current loss 0.132443, current_train_items 41088.
I0302 18:58:22.406847 22683591385216 run.py:483] Algo bellman_ford step 1284 current loss 0.132295, current_train_items 41120.
I0302 18:58:22.424989 22683591385216 run.py:483] Algo bellman_ford step 1285 current loss 0.020338, current_train_items 41152.
I0302 18:58:22.440563 22683591385216 run.py:483] Algo bellman_ford step 1286 current loss 0.069453, current_train_items 41184.
I0302 18:58:22.463560 22683591385216 run.py:483] Algo bellman_ford step 1287 current loss 0.293453, current_train_items 41216.
I0302 18:58:22.491979 22683591385216 run.py:483] Algo bellman_ford step 1288 current loss 0.245217, current_train_items 41248.
I0302 18:58:22.522386 22683591385216 run.py:483] Algo bellman_ford step 1289 current loss 0.163933, current_train_items 41280.
I0302 18:58:22.540731 22683591385216 run.py:483] Algo bellman_ford step 1290 current loss 0.010604, current_train_items 41312.
I0302 18:58:22.556416 22683591385216 run.py:483] Algo bellman_ford step 1291 current loss 0.078179, current_train_items 41344.
I0302 18:58:22.577535 22683591385216 run.py:483] Algo bellman_ford step 1292 current loss 0.139403, current_train_items 41376.
I0302 18:58:22.605140 22683591385216 run.py:483] Algo bellman_ford step 1293 current loss 0.165455, current_train_items 41408.
I0302 18:58:22.636402 22683591385216 run.py:483] Algo bellman_ford step 1294 current loss 0.224315, current_train_items 41440.
I0302 18:58:22.654065 22683591385216 run.py:483] Algo bellman_ford step 1295 current loss 0.018747, current_train_items 41472.
I0302 18:58:22.669402 22683591385216 run.py:483] Algo bellman_ford step 1296 current loss 0.022329, current_train_items 41504.
I0302 18:58:22.691657 22683591385216 run.py:483] Algo bellman_ford step 1297 current loss 0.076942, current_train_items 41536.
I0302 18:58:22.717973 22683591385216 run.py:483] Algo bellman_ford step 1298 current loss 0.113810, current_train_items 41568.
I0302 18:58:22.746059 22683591385216 run.py:483] Algo bellman_ford step 1299 current loss 0.165233, current_train_items 41600.
I0302 18:58:22.763830 22683591385216 run.py:483] Algo bellman_ford step 1300 current loss 0.009864, current_train_items 41632.
I0302 18:58:22.771490 22683591385216 run.py:503] (val) algo bellman_ford step 1300: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 41632, 'step': 1300, 'algorithm': 'bellman_ford'}
I0302 18:58:22.771595 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 18:58:22.787405 22683591385216 run.py:483] Algo bellman_ford step 1301 current loss 0.057334, current_train_items 41664.
I0302 18:58:22.810348 22683591385216 run.py:483] Algo bellman_ford step 1302 current loss 0.133983, current_train_items 41696.
I0302 18:58:22.838825 22683591385216 run.py:483] Algo bellman_ford step 1303 current loss 0.114967, current_train_items 41728.
I0302 18:58:22.868123 22683591385216 run.py:483] Algo bellman_ford step 1304 current loss 0.100835, current_train_items 41760.
I0302 18:58:22.886360 22683591385216 run.py:483] Algo bellman_ford step 1305 current loss 0.002648, current_train_items 41792.
I0302 18:58:22.901569 22683591385216 run.py:483] Algo bellman_ford step 1306 current loss 0.044293, current_train_items 41824.
I0302 18:58:22.922942 22683591385216 run.py:483] Algo bellman_ford step 1307 current loss 0.056328, current_train_items 41856.
I0302 18:58:22.950220 22683591385216 run.py:483] Algo bellman_ford step 1308 current loss 0.084222, current_train_items 41888.
I0302 18:58:22.979760 22683591385216 run.py:483] Algo bellman_ford step 1309 current loss 0.082094, current_train_items 41920.
I0302 18:58:22.997632 22683591385216 run.py:483] Algo bellman_ford step 1310 current loss 0.003088, current_train_items 41952.
I0302 18:58:23.013017 22683591385216 run.py:483] Algo bellman_ford step 1311 current loss 0.037947, current_train_items 41984.
I0302 18:58:23.034309 22683591385216 run.py:483] Algo bellman_ford step 1312 current loss 0.060101, current_train_items 42016.
I0302 18:58:23.062614 22683591385216 run.py:483] Algo bellman_ford step 1313 current loss 0.114267, current_train_items 42048.
I0302 18:58:23.093345 22683591385216 run.py:483] Algo bellman_ford step 1314 current loss 0.091949, current_train_items 42080.
I0302 18:58:23.110852 22683591385216 run.py:483] Algo bellman_ford step 1315 current loss 0.002952, current_train_items 42112.
I0302 18:58:23.126060 22683591385216 run.py:483] Algo bellman_ford step 1316 current loss 0.027036, current_train_items 42144.
I0302 18:58:23.148013 22683591385216 run.py:483] Algo bellman_ford step 1317 current loss 0.143483, current_train_items 42176.
I0302 18:58:23.176212 22683591385216 run.py:483] Algo bellman_ford step 1318 current loss 0.088524, current_train_items 42208.
I0302 18:58:23.206164 22683591385216 run.py:483] Algo bellman_ford step 1319 current loss 0.173661, current_train_items 42240.
I0302 18:58:23.223767 22683591385216 run.py:483] Algo bellman_ford step 1320 current loss 0.006379, current_train_items 42272.
I0302 18:58:23.239178 22683591385216 run.py:483] Algo bellman_ford step 1321 current loss 0.039813, current_train_items 42304.
I0302 18:58:23.261668 22683591385216 run.py:483] Algo bellman_ford step 1322 current loss 0.097733, current_train_items 42336.
I0302 18:58:23.289913 22683591385216 run.py:483] Algo bellman_ford step 1323 current loss 0.086833, current_train_items 42368.
I0302 18:58:23.319563 22683591385216 run.py:483] Algo bellman_ford step 1324 current loss 0.165794, current_train_items 42400.
I0302 18:58:23.337793 22683591385216 run.py:483] Algo bellman_ford step 1325 current loss 0.021383, current_train_items 42432.
I0302 18:58:23.352960 22683591385216 run.py:483] Algo bellman_ford step 1326 current loss 0.094146, current_train_items 42464.
I0302 18:58:23.374207 22683591385216 run.py:483] Algo bellman_ford step 1327 current loss 0.120550, current_train_items 42496.
I0302 18:58:23.402204 22683591385216 run.py:483] Algo bellman_ford step 1328 current loss 0.181359, current_train_items 42528.
I0302 18:58:23.432677 22683591385216 run.py:483] Algo bellman_ford step 1329 current loss 0.153936, current_train_items 42560.
I0302 18:58:23.450727 22683591385216 run.py:483] Algo bellman_ford step 1330 current loss 0.004453, current_train_items 42592.
I0302 18:58:23.465992 22683591385216 run.py:483] Algo bellman_ford step 1331 current loss 0.048071, current_train_items 42624.
I0302 18:58:23.487291 22683591385216 run.py:483] Algo bellman_ford step 1332 current loss 0.149182, current_train_items 42656.
I0302 18:58:23.514777 22683591385216 run.py:483] Algo bellman_ford step 1333 current loss 0.323683, current_train_items 42688.
I0302 18:58:23.544063 22683591385216 run.py:483] Algo bellman_ford step 1334 current loss 0.136767, current_train_items 42720.
I0302 18:58:23.562299 22683591385216 run.py:483] Algo bellman_ford step 1335 current loss 0.006528, current_train_items 42752.
I0302 18:58:23.577241 22683591385216 run.py:483] Algo bellman_ford step 1336 current loss 0.040005, current_train_items 42784.
I0302 18:58:23.600180 22683591385216 run.py:483] Algo bellman_ford step 1337 current loss 0.095440, current_train_items 42816.
I0302 18:58:23.627894 22683591385216 run.py:483] Algo bellman_ford step 1338 current loss 0.163690, current_train_items 42848.
I0302 18:58:23.660973 22683591385216 run.py:483] Algo bellman_ford step 1339 current loss 0.133135, current_train_items 42880.
I0302 18:58:23.679140 22683591385216 run.py:483] Algo bellman_ford step 1340 current loss 0.008222, current_train_items 42912.
I0302 18:58:23.694630 22683591385216 run.py:483] Algo bellman_ford step 1341 current loss 0.045466, current_train_items 42944.
I0302 18:58:23.717636 22683591385216 run.py:483] Algo bellman_ford step 1342 current loss 0.209714, current_train_items 42976.
I0302 18:58:23.745415 22683591385216 run.py:483] Algo bellman_ford step 1343 current loss 0.226625, current_train_items 43008.
I0302 18:58:23.775344 22683591385216 run.py:483] Algo bellman_ford step 1344 current loss 0.343230, current_train_items 43040.
I0302 18:58:23.793643 22683591385216 run.py:483] Algo bellman_ford step 1345 current loss 0.006115, current_train_items 43072.
I0302 18:58:23.809079 22683591385216 run.py:483] Algo bellman_ford step 1346 current loss 0.062734, current_train_items 43104.
I0302 18:58:23.832248 22683591385216 run.py:483] Algo bellman_ford step 1347 current loss 0.136476, current_train_items 43136.
I0302 18:58:23.859403 22683591385216 run.py:483] Algo bellman_ford step 1348 current loss 0.163423, current_train_items 43168.
I0302 18:58:23.888624 22683591385216 run.py:483] Algo bellman_ford step 1349 current loss 0.123849, current_train_items 43200.
I0302 18:58:23.906852 22683591385216 run.py:483] Algo bellman_ford step 1350 current loss 0.015564, current_train_items 43232.
I0302 18:58:23.915206 22683591385216 run.py:503] (val) algo bellman_ford step 1350: {'pi': 0.9658203125, 'score': 0.9658203125, 'examples_seen': 43232, 'step': 1350, 'algorithm': 'bellman_ford'}
I0302 18:58:23.915316 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.966, val scores are: bellman_ford: 0.966
I0302 18:58:23.931555 22683591385216 run.py:483] Algo bellman_ford step 1351 current loss 0.033570, current_train_items 43264.
I0302 18:58:23.955215 22683591385216 run.py:483] Algo bellman_ford step 1352 current loss 0.048400, current_train_items 43296.
I0302 18:58:23.982133 22683591385216 run.py:483] Algo bellman_ford step 1353 current loss 0.104024, current_train_items 43328.
I0302 18:58:24.014797 22683591385216 run.py:483] Algo bellman_ford step 1354 current loss 0.168388, current_train_items 43360.
I0302 18:58:24.033303 22683591385216 run.py:483] Algo bellman_ford step 1355 current loss 0.006827, current_train_items 43392.
I0302 18:58:24.048561 22683591385216 run.py:483] Algo bellman_ford step 1356 current loss 0.056394, current_train_items 43424.
I0302 18:58:24.070214 22683591385216 run.py:483] Algo bellman_ford step 1357 current loss 0.067247, current_train_items 43456.
I0302 18:58:24.097935 22683591385216 run.py:483] Algo bellman_ford step 1358 current loss 0.105867, current_train_items 43488.
I0302 18:58:24.129569 22683591385216 run.py:483] Algo bellman_ford step 1359 current loss 0.137260, current_train_items 43520.
I0302 18:58:24.148060 22683591385216 run.py:483] Algo bellman_ford step 1360 current loss 0.004036, current_train_items 43552.
I0302 18:58:24.164352 22683591385216 run.py:483] Algo bellman_ford step 1361 current loss 0.060016, current_train_items 43584.
I0302 18:58:24.186875 22683591385216 run.py:483] Algo bellman_ford step 1362 current loss 0.072244, current_train_items 43616.
I0302 18:58:24.214862 22683591385216 run.py:483] Algo bellman_ford step 1363 current loss 0.109513, current_train_items 43648.
I0302 18:58:24.246818 22683591385216 run.py:483] Algo bellman_ford step 1364 current loss 0.136667, current_train_items 43680.
I0302 18:58:24.264781 22683591385216 run.py:483] Algo bellman_ford step 1365 current loss 0.004492, current_train_items 43712.
I0302 18:58:24.279896 22683591385216 run.py:483] Algo bellman_ford step 1366 current loss 0.026593, current_train_items 43744.
I0302 18:58:24.301529 22683591385216 run.py:483] Algo bellman_ford step 1367 current loss 0.071747, current_train_items 43776.
I0302 18:58:24.327769 22683591385216 run.py:483] Algo bellman_ford step 1368 current loss 0.052575, current_train_items 43808.
I0302 18:58:24.358408 22683591385216 run.py:483] Algo bellman_ford step 1369 current loss 0.124210, current_train_items 43840.
I0302 18:58:24.376855 22683591385216 run.py:483] Algo bellman_ford step 1370 current loss 0.005929, current_train_items 43872.
I0302 18:58:24.392873 22683591385216 run.py:483] Algo bellman_ford step 1371 current loss 0.068911, current_train_items 43904.
I0302 18:58:24.415337 22683591385216 run.py:483] Algo bellman_ford step 1372 current loss 0.103493, current_train_items 43936.
I0302 18:58:24.444527 22683591385216 run.py:483] Algo bellman_ford step 1373 current loss 0.073253, current_train_items 43968.
I0302 18:58:24.475751 22683591385216 run.py:483] Algo bellman_ford step 1374 current loss 0.131139, current_train_items 44000.
I0302 18:58:24.494102 22683591385216 run.py:483] Algo bellman_ford step 1375 current loss 0.013246, current_train_items 44032.
I0302 18:58:24.509318 22683591385216 run.py:483] Algo bellman_ford step 1376 current loss 0.030894, current_train_items 44064.
I0302 18:58:24.531029 22683591385216 run.py:483] Algo bellman_ford step 1377 current loss 0.116295, current_train_items 44096.
I0302 18:58:24.559385 22683591385216 run.py:483] Algo bellman_ford step 1378 current loss 0.098327, current_train_items 44128.
I0302 18:58:24.590099 22683591385216 run.py:483] Algo bellman_ford step 1379 current loss 0.099482, current_train_items 44160.
I0302 18:58:24.608215 22683591385216 run.py:483] Algo bellman_ford step 1380 current loss 0.019814, current_train_items 44192.
I0302 18:58:24.623822 22683591385216 run.py:483] Algo bellman_ford step 1381 current loss 0.041107, current_train_items 44224.
I0302 18:58:24.645932 22683591385216 run.py:483] Algo bellman_ford step 1382 current loss 0.103272, current_train_items 44256.
I0302 18:58:24.674591 22683591385216 run.py:483] Algo bellman_ford step 1383 current loss 0.141226, current_train_items 44288.
I0302 18:58:24.703001 22683591385216 run.py:483] Algo bellman_ford step 1384 current loss 0.126154, current_train_items 44320.
I0302 18:58:24.721488 22683591385216 run.py:483] Algo bellman_ford step 1385 current loss 0.005266, current_train_items 44352.
I0302 18:58:24.736726 22683591385216 run.py:483] Algo bellman_ford step 1386 current loss 0.017448, current_train_items 44384.
I0302 18:58:24.758115 22683591385216 run.py:483] Algo bellman_ford step 1387 current loss 0.065468, current_train_items 44416.
I0302 18:58:24.786538 22683591385216 run.py:483] Algo bellman_ford step 1388 current loss 0.140099, current_train_items 44448.
I0302 18:58:24.817041 22683591385216 run.py:483] Algo bellman_ford step 1389 current loss 0.143245, current_train_items 44480.
I0302 18:58:24.835141 22683591385216 run.py:483] Algo bellman_ford step 1390 current loss 0.009411, current_train_items 44512.
I0302 18:58:24.850370 22683591385216 run.py:483] Algo bellman_ford step 1391 current loss 0.036531, current_train_items 44544.
I0302 18:58:24.873340 22683591385216 run.py:483] Algo bellman_ford step 1392 current loss 0.091153, current_train_items 44576.
I0302 18:58:24.901408 22683591385216 run.py:483] Algo bellman_ford step 1393 current loss 0.161712, current_train_items 44608.
I0302 18:58:24.931683 22683591385216 run.py:483] Algo bellman_ford step 1394 current loss 0.118023, current_train_items 44640.
I0302 18:58:24.949383 22683591385216 run.py:483] Algo bellman_ford step 1395 current loss 0.007127, current_train_items 44672.
I0302 18:58:24.964528 22683591385216 run.py:483] Algo bellman_ford step 1396 current loss 0.032744, current_train_items 44704.
I0302 18:58:24.986815 22683591385216 run.py:483] Algo bellman_ford step 1397 current loss 0.114902, current_train_items 44736.
I0302 18:58:25.015574 22683591385216 run.py:483] Algo bellman_ford step 1398 current loss 0.185263, current_train_items 44768.
I0302 18:58:25.044912 22683591385216 run.py:483] Algo bellman_ford step 1399 current loss 0.168070, current_train_items 44800.
I0302 18:58:25.063393 22683591385216 run.py:483] Algo bellman_ford step 1400 current loss 0.011001, current_train_items 44832.
I0302 18:58:25.071311 22683591385216 run.py:503] (val) algo bellman_ford step 1400: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 44832, 'step': 1400, 'algorithm': 'bellman_ford'}
I0302 18:58:25.071419 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:58:25.087258 22683591385216 run.py:483] Algo bellman_ford step 1401 current loss 0.050101, current_train_items 44864.
I0302 18:58:25.110424 22683591385216 run.py:483] Algo bellman_ford step 1402 current loss 0.139799, current_train_items 44896.
I0302 18:58:25.139389 22683591385216 run.py:483] Algo bellman_ford step 1403 current loss 0.150140, current_train_items 44928.
I0302 18:58:25.170886 22683591385216 run.py:483] Algo bellman_ford step 1404 current loss 0.159216, current_train_items 44960.
I0302 18:58:25.189212 22683591385216 run.py:483] Algo bellman_ford step 1405 current loss 0.004788, current_train_items 44992.
I0302 18:58:25.204961 22683591385216 run.py:483] Algo bellman_ford step 1406 current loss 0.076094, current_train_items 45024.
I0302 18:58:25.227396 22683591385216 run.py:483] Algo bellman_ford step 1407 current loss 0.080128, current_train_items 45056.
I0302 18:58:25.255545 22683591385216 run.py:483] Algo bellman_ford step 1408 current loss 0.101343, current_train_items 45088.
I0302 18:58:25.286137 22683591385216 run.py:483] Algo bellman_ford step 1409 current loss 0.116457, current_train_items 45120.
I0302 18:58:25.304162 22683591385216 run.py:483] Algo bellman_ford step 1410 current loss 0.005389, current_train_items 45152.
I0302 18:58:25.319784 22683591385216 run.py:483] Algo bellman_ford step 1411 current loss 0.019066, current_train_items 45184.
I0302 18:58:25.342247 22683591385216 run.py:483] Algo bellman_ford step 1412 current loss 0.081349, current_train_items 45216.
I0302 18:58:25.368584 22683591385216 run.py:483] Algo bellman_ford step 1413 current loss 0.059979, current_train_items 45248.
I0302 18:58:25.398457 22683591385216 run.py:483] Algo bellman_ford step 1414 current loss 0.103060, current_train_items 45280.
I0302 18:58:25.416561 22683591385216 run.py:483] Algo bellman_ford step 1415 current loss 0.012530, current_train_items 45312.
I0302 18:58:25.432352 22683591385216 run.py:483] Algo bellman_ford step 1416 current loss 0.035748, current_train_items 45344.
I0302 18:58:25.454059 22683591385216 run.py:483] Algo bellman_ford step 1417 current loss 0.033944, current_train_items 45376.
I0302 18:58:25.480936 22683591385216 run.py:483] Algo bellman_ford step 1418 current loss 0.142173, current_train_items 45408.
I0302 18:58:25.512196 22683591385216 run.py:483] Algo bellman_ford step 1419 current loss 0.159536, current_train_items 45440.
I0302 18:58:25.530344 22683591385216 run.py:483] Algo bellman_ford step 1420 current loss 0.032442, current_train_items 45472.
I0302 18:58:25.545742 22683591385216 run.py:483] Algo bellman_ford step 1421 current loss 0.035781, current_train_items 45504.
I0302 18:58:25.568689 22683591385216 run.py:483] Algo bellman_ford step 1422 current loss 0.069194, current_train_items 45536.
I0302 18:58:25.597642 22683591385216 run.py:483] Algo bellman_ford step 1423 current loss 0.082179, current_train_items 45568.
I0302 18:58:25.627400 22683591385216 run.py:483] Algo bellman_ford step 1424 current loss 0.084782, current_train_items 45600.
I0302 18:58:25.645166 22683591385216 run.py:483] Algo bellman_ford step 1425 current loss 0.005321, current_train_items 45632.
I0302 18:58:25.661049 22683591385216 run.py:483] Algo bellman_ford step 1426 current loss 0.079454, current_train_items 45664.
I0302 18:58:25.684066 22683591385216 run.py:483] Algo bellman_ford step 1427 current loss 0.066054, current_train_items 45696.
I0302 18:58:25.712296 22683591385216 run.py:483] Algo bellman_ford step 1428 current loss 0.069445, current_train_items 45728.
I0302 18:58:25.741420 22683591385216 run.py:483] Algo bellman_ford step 1429 current loss 0.091829, current_train_items 45760.
I0302 18:58:25.759214 22683591385216 run.py:483] Algo bellman_ford step 1430 current loss 0.006443, current_train_items 45792.
I0302 18:58:25.774552 22683591385216 run.py:483] Algo bellman_ford step 1431 current loss 0.049599, current_train_items 45824.
I0302 18:58:25.798373 22683591385216 run.py:483] Algo bellman_ford step 1432 current loss 0.126408, current_train_items 45856.
I0302 18:58:25.824777 22683591385216 run.py:483] Algo bellman_ford step 1433 current loss 0.063499, current_train_items 45888.
I0302 18:58:25.853976 22683591385216 run.py:483] Algo bellman_ford step 1434 current loss 0.083618, current_train_items 45920.
I0302 18:58:25.871488 22683591385216 run.py:483] Algo bellman_ford step 1435 current loss 0.004688, current_train_items 45952.
I0302 18:58:25.886817 22683591385216 run.py:483] Algo bellman_ford step 1436 current loss 0.049683, current_train_items 45984.
I0302 18:58:25.908767 22683591385216 run.py:483] Algo bellman_ford step 1437 current loss 0.097213, current_train_items 46016.
I0302 18:58:25.936059 22683591385216 run.py:483] Algo bellman_ford step 1438 current loss 0.115811, current_train_items 46048.
I0302 18:58:25.967507 22683591385216 run.py:483] Algo bellman_ford step 1439 current loss 0.226532, current_train_items 46080.
I0302 18:58:25.985651 22683591385216 run.py:483] Algo bellman_ford step 1440 current loss 0.004352, current_train_items 46112.
I0302 18:58:26.001102 22683591385216 run.py:483] Algo bellman_ford step 1441 current loss 0.047340, current_train_items 46144.
I0302 18:58:26.023176 22683591385216 run.py:483] Algo bellman_ford step 1442 current loss 0.077150, current_train_items 46176.
I0302 18:58:26.051040 22683591385216 run.py:483] Algo bellman_ford step 1443 current loss 0.083198, current_train_items 46208.
I0302 18:58:26.085315 22683591385216 run.py:483] Algo bellman_ford step 1444 current loss 0.158937, current_train_items 46240.
I0302 18:58:26.103233 22683591385216 run.py:483] Algo bellman_ford step 1445 current loss 0.010037, current_train_items 46272.
I0302 18:58:26.118093 22683591385216 run.py:483] Algo bellman_ford step 1446 current loss 0.124544, current_train_items 46304.
I0302 18:58:26.141022 22683591385216 run.py:483] Algo bellman_ford step 1447 current loss 0.095168, current_train_items 46336.
I0302 18:58:26.168639 22683591385216 run.py:483] Algo bellman_ford step 1448 current loss 0.098479, current_train_items 46368.
I0302 18:58:26.198041 22683591385216 run.py:483] Algo bellman_ford step 1449 current loss 0.101249, current_train_items 46400.
I0302 18:58:26.216106 22683591385216 run.py:483] Algo bellman_ford step 1450 current loss 0.005453, current_train_items 46432.
I0302 18:58:26.223946 22683591385216 run.py:503] (val) algo bellman_ford step 1450: {'pi': 0.97265625, 'score': 0.97265625, 'examples_seen': 46432, 'step': 1450, 'algorithm': 'bellman_ford'}
I0302 18:58:26.224052 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.973, val scores are: bellman_ford: 0.973
I0302 18:58:26.239529 22683591385216 run.py:483] Algo bellman_ford step 1451 current loss 0.045752, current_train_items 46464.
I0302 18:58:26.261396 22683591385216 run.py:483] Algo bellman_ford step 1452 current loss 0.105366, current_train_items 46496.
I0302 18:58:26.290537 22683591385216 run.py:483] Algo bellman_ford step 1453 current loss 0.147626, current_train_items 46528.
I0302 18:58:26.322809 22683591385216 run.py:483] Algo bellman_ford step 1454 current loss 0.196231, current_train_items 46560.
I0302 18:58:26.340886 22683591385216 run.py:483] Algo bellman_ford step 1455 current loss 0.014271, current_train_items 46592.
I0302 18:58:26.356194 22683591385216 run.py:483] Algo bellman_ford step 1456 current loss 0.054716, current_train_items 46624.
I0302 18:58:26.377121 22683591385216 run.py:483] Algo bellman_ford step 1457 current loss 0.119657, current_train_items 46656.
I0302 18:58:26.405468 22683591385216 run.py:483] Algo bellman_ford step 1458 current loss 0.231895, current_train_items 46688.
I0302 18:58:26.434924 22683591385216 run.py:483] Algo bellman_ford step 1459 current loss 0.195153, current_train_items 46720.
I0302 18:58:26.452887 22683591385216 run.py:483] Algo bellman_ford step 1460 current loss 0.014305, current_train_items 46752.
I0302 18:58:26.468051 22683591385216 run.py:483] Algo bellman_ford step 1461 current loss 0.070244, current_train_items 46784.
I0302 18:58:26.489195 22683591385216 run.py:483] Algo bellman_ford step 1462 current loss 0.113183, current_train_items 46816.
I0302 18:58:26.516566 22683591385216 run.py:483] Algo bellman_ford step 1463 current loss 0.131582, current_train_items 46848.
I0302 18:58:26.545568 22683591385216 run.py:483] Algo bellman_ford step 1464 current loss 0.142578, current_train_items 46880.
I0302 18:58:26.563284 22683591385216 run.py:483] Algo bellman_ford step 1465 current loss 0.034987, current_train_items 46912.
I0302 18:58:26.578864 22683591385216 run.py:483] Algo bellman_ford step 1466 current loss 0.051936, current_train_items 46944.
I0302 18:58:26.600539 22683591385216 run.py:483] Algo bellman_ford step 1467 current loss 0.112045, current_train_items 46976.
I0302 18:58:26.626444 22683591385216 run.py:483] Algo bellman_ford step 1468 current loss 0.115198, current_train_items 47008.
I0302 18:58:26.655490 22683591385216 run.py:483] Algo bellman_ford step 1469 current loss 0.143193, current_train_items 47040.
I0302 18:58:26.673506 22683591385216 run.py:483] Algo bellman_ford step 1470 current loss 0.007239, current_train_items 47072.
I0302 18:58:26.689520 22683591385216 run.py:483] Algo bellman_ford step 1471 current loss 0.080185, current_train_items 47104.
I0302 18:58:26.710983 22683591385216 run.py:483] Algo bellman_ford step 1472 current loss 0.077262, current_train_items 47136.
I0302 18:58:26.738492 22683591385216 run.py:483] Algo bellman_ford step 1473 current loss 0.340447, current_train_items 47168.
I0302 18:58:26.769593 22683591385216 run.py:483] Algo bellman_ford step 1474 current loss 0.199078, current_train_items 47200.
I0302 18:58:26.787846 22683591385216 run.py:483] Algo bellman_ford step 1475 current loss 0.023636, current_train_items 47232.
I0302 18:58:26.803244 22683591385216 run.py:483] Algo bellman_ford step 1476 current loss 0.032976, current_train_items 47264.
I0302 18:58:26.824894 22683591385216 run.py:483] Algo bellman_ford step 1477 current loss 0.103452, current_train_items 47296.
I0302 18:58:26.852439 22683591385216 run.py:483] Algo bellman_ford step 1478 current loss 0.321926, current_train_items 47328.
I0302 18:58:26.884289 22683591385216 run.py:483] Algo bellman_ford step 1479 current loss 0.234299, current_train_items 47360.
I0302 18:58:26.901850 22683591385216 run.py:483] Algo bellman_ford step 1480 current loss 0.007590, current_train_items 47392.
I0302 18:58:26.916875 22683591385216 run.py:483] Algo bellman_ford step 1481 current loss 0.029864, current_train_items 47424.
I0302 18:58:26.938121 22683591385216 run.py:483] Algo bellman_ford step 1482 current loss 0.090199, current_train_items 47456.
I0302 18:58:26.965939 22683591385216 run.py:483] Algo bellman_ford step 1483 current loss 0.177790, current_train_items 47488.
I0302 18:58:26.994599 22683591385216 run.py:483] Algo bellman_ford step 1484 current loss 0.124577, current_train_items 47520.
I0302 18:58:27.012633 22683591385216 run.py:483] Algo bellman_ford step 1485 current loss 0.015498, current_train_items 47552.
I0302 18:58:27.027753 22683591385216 run.py:483] Algo bellman_ford step 1486 current loss 0.041398, current_train_items 47584.
I0302 18:58:27.050056 22683591385216 run.py:483] Algo bellman_ford step 1487 current loss 0.081900, current_train_items 47616.
I0302 18:58:27.075917 22683591385216 run.py:483] Algo bellman_ford step 1488 current loss 0.090052, current_train_items 47648.
I0302 18:58:27.107220 22683591385216 run.py:483] Algo bellman_ford step 1489 current loss 0.126434, current_train_items 47680.
I0302 18:58:27.125118 22683591385216 run.py:483] Algo bellman_ford step 1490 current loss 0.003893, current_train_items 47712.
I0302 18:58:27.140651 22683591385216 run.py:483] Algo bellman_ford step 1491 current loss 0.027476, current_train_items 47744.
I0302 18:58:27.162534 22683591385216 run.py:483] Algo bellman_ford step 1492 current loss 0.054783, current_train_items 47776.
I0302 18:58:27.190426 22683591385216 run.py:483] Algo bellman_ford step 1493 current loss 0.132840, current_train_items 47808.
I0302 18:58:27.221753 22683591385216 run.py:483] Algo bellman_ford step 1494 current loss 0.109703, current_train_items 47840.
I0302 18:58:27.239405 22683591385216 run.py:483] Algo bellman_ford step 1495 current loss 0.003728, current_train_items 47872.
I0302 18:58:27.255056 22683591385216 run.py:483] Algo bellman_ford step 1496 current loss 0.091294, current_train_items 47904.
I0302 18:58:27.277428 22683591385216 run.py:483] Algo bellman_ford step 1497 current loss 0.118308, current_train_items 47936.
I0302 18:58:27.305663 22683591385216 run.py:483] Algo bellman_ford step 1498 current loss 0.216501, current_train_items 47968.
I0302 18:58:27.337576 22683591385216 run.py:483] Algo bellman_ford step 1499 current loss 0.136384, current_train_items 48000.
I0302 18:58:27.355570 22683591385216 run.py:483] Algo bellman_ford step 1500 current loss 0.004004, current_train_items 48032.
I0302 18:58:27.363287 22683591385216 run.py:503] (val) algo bellman_ford step 1500: {'pi': 0.96484375, 'score': 0.96484375, 'examples_seen': 48032, 'step': 1500, 'algorithm': 'bellman_ford'}
I0302 18:58:27.363393 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.965, val scores are: bellman_ford: 0.965
I0302 18:58:27.379484 22683591385216 run.py:483] Algo bellman_ford step 1501 current loss 0.035991, current_train_items 48064.
I0302 18:58:27.402613 22683591385216 run.py:483] Algo bellman_ford step 1502 current loss 0.079629, current_train_items 48096.
I0302 18:58:27.430800 22683591385216 run.py:483] Algo bellman_ford step 1503 current loss 0.112516, current_train_items 48128.
I0302 18:58:27.461265 22683591385216 run.py:483] Algo bellman_ford step 1504 current loss 0.118146, current_train_items 48160.
I0302 18:58:27.479303 22683591385216 run.py:483] Algo bellman_ford step 1505 current loss 0.007375, current_train_items 48192.
I0302 18:58:27.494584 22683591385216 run.py:483] Algo bellman_ford step 1506 current loss 0.032220, current_train_items 48224.
I0302 18:58:27.516023 22683591385216 run.py:483] Algo bellman_ford step 1507 current loss 0.067740, current_train_items 48256.
I0302 18:58:27.543667 22683591385216 run.py:483] Algo bellman_ford step 1508 current loss 0.132705, current_train_items 48288.
I0302 18:58:27.573708 22683591385216 run.py:483] Algo bellman_ford step 1509 current loss 0.138538, current_train_items 48320.
I0302 18:58:27.591702 22683591385216 run.py:483] Algo bellman_ford step 1510 current loss 0.035601, current_train_items 48352.
I0302 18:58:27.607223 22683591385216 run.py:483] Algo bellman_ford step 1511 current loss 0.057029, current_train_items 48384.
I0302 18:58:27.629816 22683591385216 run.py:483] Algo bellman_ford step 1512 current loss 0.099173, current_train_items 48416.
I0302 18:58:27.656130 22683591385216 run.py:483] Algo bellman_ford step 1513 current loss 0.085270, current_train_items 48448.
I0302 18:58:27.688121 22683591385216 run.py:483] Algo bellman_ford step 1514 current loss 0.251839, current_train_items 48480.
I0302 18:58:27.706042 22683591385216 run.py:483] Algo bellman_ford step 1515 current loss 0.008235, current_train_items 48512.
I0302 18:58:27.721147 22683591385216 run.py:483] Algo bellman_ford step 1516 current loss 0.027777, current_train_items 48544.
I0302 18:58:27.743519 22683591385216 run.py:483] Algo bellman_ford step 1517 current loss 0.144806, current_train_items 48576.
I0302 18:58:27.772248 22683591385216 run.py:483] Algo bellman_ford step 1518 current loss 0.218109, current_train_items 48608.
I0302 18:58:27.802592 22683591385216 run.py:483] Algo bellman_ford step 1519 current loss 0.169642, current_train_items 48640.
I0302 18:58:27.820321 22683591385216 run.py:483] Algo bellman_ford step 1520 current loss 0.008620, current_train_items 48672.
I0302 18:58:27.835495 22683591385216 run.py:483] Algo bellman_ford step 1521 current loss 0.030199, current_train_items 48704.
I0302 18:58:27.858380 22683591385216 run.py:483] Algo bellman_ford step 1522 current loss 0.115245, current_train_items 48736.
I0302 18:58:27.886218 22683591385216 run.py:483] Algo bellman_ford step 1523 current loss 0.246234, current_train_items 48768.
I0302 18:58:27.916557 22683591385216 run.py:483] Algo bellman_ford step 1524 current loss 0.140516, current_train_items 48800.
I0302 18:58:27.934544 22683591385216 run.py:483] Algo bellman_ford step 1525 current loss 0.005494, current_train_items 48832.
I0302 18:58:27.949539 22683591385216 run.py:483] Algo bellman_ford step 1526 current loss 0.021021, current_train_items 48864.
I0302 18:58:27.972361 22683591385216 run.py:483] Algo bellman_ford step 1527 current loss 0.106383, current_train_items 48896.
I0302 18:58:28.000252 22683591385216 run.py:483] Algo bellman_ford step 1528 current loss 0.149172, current_train_items 48928.
I0302 18:58:28.031308 22683591385216 run.py:483] Algo bellman_ford step 1529 current loss 0.187781, current_train_items 48960.
I0302 18:58:28.049470 22683591385216 run.py:483] Algo bellman_ford step 1530 current loss 0.014024, current_train_items 48992.
I0302 18:58:28.064523 22683591385216 run.py:483] Algo bellman_ford step 1531 current loss 0.027755, current_train_items 49024.
I0302 18:58:28.086333 22683591385216 run.py:483] Algo bellman_ford step 1532 current loss 0.056563, current_train_items 49056.
I0302 18:58:28.114728 22683591385216 run.py:483] Algo bellman_ford step 1533 current loss 0.160036, current_train_items 49088.
I0302 18:58:28.146273 22683591385216 run.py:483] Algo bellman_ford step 1534 current loss 0.158130, current_train_items 49120.
I0302 18:58:28.164242 22683591385216 run.py:483] Algo bellman_ford step 1535 current loss 0.022597, current_train_items 49152.
I0302 18:58:28.180022 22683591385216 run.py:483] Algo bellman_ford step 1536 current loss 0.042756, current_train_items 49184.
I0302 18:58:28.202701 22683591385216 run.py:483] Algo bellman_ford step 1537 current loss 0.128536, current_train_items 49216.
I0302 18:58:28.231157 22683591385216 run.py:483] Algo bellman_ford step 1538 current loss 0.131533, current_train_items 49248.
I0302 18:58:28.264738 22683591385216 run.py:483] Algo bellman_ford step 1539 current loss 0.238619, current_train_items 49280.
I0302 18:58:28.282922 22683591385216 run.py:483] Algo bellman_ford step 1540 current loss 0.009669, current_train_items 49312.
I0302 18:58:28.297934 22683591385216 run.py:483] Algo bellman_ford step 1541 current loss 0.039276, current_train_items 49344.
I0302 18:58:28.319860 22683591385216 run.py:483] Algo bellman_ford step 1542 current loss 0.113741, current_train_items 49376.
I0302 18:58:28.347409 22683591385216 run.py:483] Algo bellman_ford step 1543 current loss 0.173488, current_train_items 49408.
I0302 18:58:28.377824 22683591385216 run.py:483] Algo bellman_ford step 1544 current loss 0.249907, current_train_items 49440.
I0302 18:58:28.395626 22683591385216 run.py:483] Algo bellman_ford step 1545 current loss 0.017124, current_train_items 49472.
I0302 18:58:28.410930 22683591385216 run.py:483] Algo bellman_ford step 1546 current loss 0.044615, current_train_items 49504.
I0302 18:58:28.433017 22683591385216 run.py:483] Algo bellman_ford step 1547 current loss 0.113258, current_train_items 49536.
I0302 18:58:28.462152 22683591385216 run.py:483] Algo bellman_ford step 1548 current loss 0.087575, current_train_items 49568.
I0302 18:58:28.494051 22683591385216 run.py:483] Algo bellman_ford step 1549 current loss 0.233400, current_train_items 49600.
I0302 18:58:28.511727 22683591385216 run.py:483] Algo bellman_ford step 1550 current loss 0.044762, current_train_items 49632.
I0302 18:58:28.519536 22683591385216 run.py:503] (val) algo bellman_ford step 1550: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 49632, 'step': 1550, 'algorithm': 'bellman_ford'}
I0302 18:58:28.519642 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:58:28.536281 22683591385216 run.py:483] Algo bellman_ford step 1551 current loss 0.057050, current_train_items 49664.
I0302 18:58:28.558580 22683591385216 run.py:483] Algo bellman_ford step 1552 current loss 0.049157, current_train_items 49696.
I0302 18:58:28.587115 22683591385216 run.py:483] Algo bellman_ford step 1553 current loss 0.091832, current_train_items 49728.
I0302 18:58:28.616301 22683591385216 run.py:483] Algo bellman_ford step 1554 current loss 0.079696, current_train_items 49760.
I0302 18:58:28.634374 22683591385216 run.py:483] Algo bellman_ford step 1555 current loss 0.003737, current_train_items 49792.
I0302 18:58:28.649704 22683591385216 run.py:483] Algo bellman_ford step 1556 current loss 0.015501, current_train_items 49824.
I0302 18:58:28.672530 22683591385216 run.py:483] Algo bellman_ford step 1557 current loss 0.102662, current_train_items 49856.
I0302 18:58:28.701133 22683591385216 run.py:483] Algo bellman_ford step 1558 current loss 0.097712, current_train_items 49888.
I0302 18:58:28.732380 22683591385216 run.py:483] Algo bellman_ford step 1559 current loss 0.100441, current_train_items 49920.
I0302 18:58:28.750657 22683591385216 run.py:483] Algo bellman_ford step 1560 current loss 0.009014, current_train_items 49952.
I0302 18:58:28.766415 22683591385216 run.py:483] Algo bellman_ford step 1561 current loss 0.024421, current_train_items 49984.
I0302 18:58:28.788156 22683591385216 run.py:483] Algo bellman_ford step 1562 current loss 0.062122, current_train_items 50016.
I0302 18:58:28.816752 22683591385216 run.py:483] Algo bellman_ford step 1563 current loss 0.090314, current_train_items 50048.
I0302 18:58:28.848361 22683591385216 run.py:483] Algo bellman_ford step 1564 current loss 0.100726, current_train_items 50080.
I0302 18:58:28.866055 22683591385216 run.py:483] Algo bellman_ford step 1565 current loss 0.003820, current_train_items 50112.
I0302 18:58:28.881536 22683591385216 run.py:483] Algo bellman_ford step 1566 current loss 0.062080, current_train_items 50144.
I0302 18:58:28.904005 22683591385216 run.py:483] Algo bellman_ford step 1567 current loss 0.110289, current_train_items 50176.
I0302 18:58:28.931931 22683591385216 run.py:483] Algo bellman_ford step 1568 current loss 0.094808, current_train_items 50208.
I0302 18:58:28.963874 22683591385216 run.py:483] Algo bellman_ford step 1569 current loss 0.107653, current_train_items 50240.
I0302 18:58:28.981920 22683591385216 run.py:483] Algo bellman_ford step 1570 current loss 0.005456, current_train_items 50272.
I0302 18:58:28.997769 22683591385216 run.py:483] Algo bellman_ford step 1571 current loss 0.020251, current_train_items 50304.
I0302 18:58:29.020603 22683591385216 run.py:483] Algo bellman_ford step 1572 current loss 0.130149, current_train_items 50336.
I0302 18:58:29.048531 22683591385216 run.py:483] Algo bellman_ford step 1573 current loss 0.099384, current_train_items 50368.
I0302 18:58:29.076826 22683591385216 run.py:483] Algo bellman_ford step 1574 current loss 0.118961, current_train_items 50400.
I0302 18:58:29.095347 22683591385216 run.py:483] Algo bellman_ford step 1575 current loss 0.010571, current_train_items 50432.
I0302 18:58:29.111024 22683591385216 run.py:483] Algo bellman_ford step 1576 current loss 0.055603, current_train_items 50464.
I0302 18:58:29.131840 22683591385216 run.py:483] Algo bellman_ford step 1577 current loss 0.072832, current_train_items 50496.
I0302 18:58:29.160914 22683591385216 run.py:483] Algo bellman_ford step 1578 current loss 0.272562, current_train_items 50528.
I0302 18:58:29.191603 22683591385216 run.py:483] Algo bellman_ford step 1579 current loss 0.103750, current_train_items 50560.
I0302 18:58:29.209699 22683591385216 run.py:483] Algo bellman_ford step 1580 current loss 0.043873, current_train_items 50592.
I0302 18:58:29.224752 22683591385216 run.py:483] Algo bellman_ford step 1581 current loss 0.106602, current_train_items 50624.
I0302 18:58:29.246623 22683591385216 run.py:483] Algo bellman_ford step 1582 current loss 0.067730, current_train_items 50656.
I0302 18:58:29.274595 22683591385216 run.py:483] Algo bellman_ford step 1583 current loss 0.079831, current_train_items 50688.
I0302 18:58:29.305754 22683591385216 run.py:483] Algo bellman_ford step 1584 current loss 0.187347, current_train_items 50720.
I0302 18:58:29.324162 22683591385216 run.py:483] Algo bellman_ford step 1585 current loss 0.004509, current_train_items 50752.
I0302 18:58:29.339322 22683591385216 run.py:483] Algo bellman_ford step 1586 current loss 0.063297, current_train_items 50784.
I0302 18:58:29.361306 22683591385216 run.py:483] Algo bellman_ford step 1587 current loss 0.118516, current_train_items 50816.
I0302 18:58:29.389014 22683591385216 run.py:483] Algo bellman_ford step 1588 current loss 0.092725, current_train_items 50848.
I0302 18:58:29.419326 22683591385216 run.py:483] Algo bellman_ford step 1589 current loss 0.120335, current_train_items 50880.
I0302 18:58:29.437665 22683591385216 run.py:483] Algo bellman_ford step 1590 current loss 0.069687, current_train_items 50912.
I0302 18:58:29.453275 22683591385216 run.py:483] Algo bellman_ford step 1591 current loss 0.023040, current_train_items 50944.
I0302 18:58:29.474419 22683591385216 run.py:483] Algo bellman_ford step 1592 current loss 0.062731, current_train_items 50976.
I0302 18:58:29.503216 22683591385216 run.py:483] Algo bellman_ford step 1593 current loss 0.073410, current_train_items 51008.
I0302 18:58:29.534641 22683591385216 run.py:483] Algo bellman_ford step 1594 current loss 0.071181, current_train_items 51040.
I0302 18:58:29.552639 22683591385216 run.py:483] Algo bellman_ford step 1595 current loss 0.013918, current_train_items 51072.
I0302 18:58:29.567485 22683591385216 run.py:483] Algo bellman_ford step 1596 current loss 0.042177, current_train_items 51104.
I0302 18:58:29.590050 22683591385216 run.py:483] Algo bellman_ford step 1597 current loss 0.057225, current_train_items 51136.
I0302 18:58:29.617154 22683591385216 run.py:483] Algo bellman_ford step 1598 current loss 0.080011, current_train_items 51168.
I0302 18:58:29.648217 22683591385216 run.py:483] Algo bellman_ford step 1599 current loss 0.094372, current_train_items 51200.
I0302 18:58:29.666316 22683591385216 run.py:483] Algo bellman_ford step 1600 current loss 0.015443, current_train_items 51232.
I0302 18:58:29.673917 22683591385216 run.py:503] (val) algo bellman_ford step 1600: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 51232, 'step': 1600, 'algorithm': 'bellman_ford'}
I0302 18:58:29.674022 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.984, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0302 18:58:29.689853 22683591385216 run.py:483] Algo bellman_ford step 1601 current loss 0.024082, current_train_items 51264.
I0302 18:58:29.713279 22683591385216 run.py:483] Algo bellman_ford step 1602 current loss 0.037380, current_train_items 51296.
I0302 18:58:29.741682 22683591385216 run.py:483] Algo bellman_ford step 1603 current loss 0.081706, current_train_items 51328.
I0302 18:58:29.773035 22683591385216 run.py:483] Algo bellman_ford step 1604 current loss 0.120164, current_train_items 51360.
I0302 18:58:29.791185 22683591385216 run.py:483] Algo bellman_ford step 1605 current loss 0.014055, current_train_items 51392.
I0302 18:58:29.805994 22683591385216 run.py:483] Algo bellman_ford step 1606 current loss 0.074329, current_train_items 51424.
I0302 18:58:29.827687 22683591385216 run.py:483] Algo bellman_ford step 1607 current loss 0.058472, current_train_items 51456.
I0302 18:58:29.856701 22683591385216 run.py:483] Algo bellman_ford step 1608 current loss 0.136959, current_train_items 51488.
I0302 18:58:29.888327 22683591385216 run.py:483] Algo bellman_ford step 1609 current loss 0.170038, current_train_items 51520.
I0302 18:58:29.906329 22683591385216 run.py:483] Algo bellman_ford step 1610 current loss 0.050441, current_train_items 51552.
I0302 18:58:29.921866 22683591385216 run.py:483] Algo bellman_ford step 1611 current loss 0.045298, current_train_items 51584.
I0302 18:58:29.944098 22683591385216 run.py:483] Algo bellman_ford step 1612 current loss 0.103242, current_train_items 51616.
I0302 18:58:29.972508 22683591385216 run.py:483] Algo bellman_ford step 1613 current loss 0.242468, current_train_items 51648.
I0302 18:58:30.002495 22683591385216 run.py:483] Algo bellman_ford step 1614 current loss 0.244554, current_train_items 51680.
I0302 18:58:30.020374 22683591385216 run.py:483] Algo bellman_ford step 1615 current loss 0.006878, current_train_items 51712.
I0302 18:58:30.035929 22683591385216 run.py:483] Algo bellman_ford step 1616 current loss 0.031039, current_train_items 51744.
I0302 18:58:30.058261 22683591385216 run.py:483] Algo bellman_ford step 1617 current loss 0.094795, current_train_items 51776.
I0302 18:58:30.085425 22683591385216 run.py:483] Algo bellman_ford step 1618 current loss 0.055848, current_train_items 51808.
I0302 18:58:30.117395 22683591385216 run.py:483] Algo bellman_ford step 1619 current loss 0.116457, current_train_items 51840.
I0302 18:58:30.135499 22683591385216 run.py:483] Algo bellman_ford step 1620 current loss 0.012063, current_train_items 51872.
I0302 18:58:30.150643 22683591385216 run.py:483] Algo bellman_ford step 1621 current loss 0.028634, current_train_items 51904.
I0302 18:58:30.173022 22683591385216 run.py:483] Algo bellman_ford step 1622 current loss 0.070209, current_train_items 51936.
I0302 18:58:30.201437 22683591385216 run.py:483] Algo bellman_ford step 1623 current loss 0.097265, current_train_items 51968.
I0302 18:58:30.232002 22683591385216 run.py:483] Algo bellman_ford step 1624 current loss 0.104969, current_train_items 52000.
I0302 18:58:30.249963 22683591385216 run.py:483] Algo bellman_ford step 1625 current loss 0.003895, current_train_items 52032.
I0302 18:58:30.265480 22683591385216 run.py:483] Algo bellman_ford step 1626 current loss 0.060344, current_train_items 52064.
I0302 18:58:30.287595 22683591385216 run.py:483] Algo bellman_ford step 1627 current loss 0.082634, current_train_items 52096.
I0302 18:58:30.317240 22683591385216 run.py:483] Algo bellman_ford step 1628 current loss 0.089831, current_train_items 52128.
I0302 18:58:30.347352 22683591385216 run.py:483] Algo bellman_ford step 1629 current loss 0.120791, current_train_items 52160.
I0302 18:58:30.365226 22683591385216 run.py:483] Algo bellman_ford step 1630 current loss 0.007069, current_train_items 52192.
I0302 18:58:30.381143 22683591385216 run.py:483] Algo bellman_ford step 1631 current loss 0.059061, current_train_items 52224.
I0302 18:58:30.403847 22683591385216 run.py:483] Algo bellman_ford step 1632 current loss 0.114056, current_train_items 52256.
I0302 18:58:30.431480 22683591385216 run.py:483] Algo bellman_ford step 1633 current loss 0.075514, current_train_items 52288.
I0302 18:58:30.462513 22683591385216 run.py:483] Algo bellman_ford step 1634 current loss 0.109000, current_train_items 52320.
I0302 18:58:30.480339 22683591385216 run.py:483] Algo bellman_ford step 1635 current loss 0.008283, current_train_items 52352.
I0302 18:58:30.495649 22683591385216 run.py:483] Algo bellman_ford step 1636 current loss 0.036757, current_train_items 52384.
I0302 18:58:30.518254 22683591385216 run.py:483] Algo bellman_ford step 1637 current loss 0.087571, current_train_items 52416.
I0302 18:58:30.546982 22683591385216 run.py:483] Algo bellman_ford step 1638 current loss 0.126014, current_train_items 52448.
I0302 18:58:30.580729 22683591385216 run.py:483] Algo bellman_ford step 1639 current loss 0.107645, current_train_items 52480.
I0302 18:58:30.598357 22683591385216 run.py:483] Algo bellman_ford step 1640 current loss 0.031334, current_train_items 52512.
I0302 18:58:30.613308 22683591385216 run.py:483] Algo bellman_ford step 1641 current loss 0.026257, current_train_items 52544.
I0302 18:58:30.635673 22683591385216 run.py:483] Algo bellman_ford step 1642 current loss 0.085778, current_train_items 52576.
I0302 18:58:30.663509 22683591385216 run.py:483] Algo bellman_ford step 1643 current loss 0.143101, current_train_items 52608.
I0302 18:58:30.693339 22683591385216 run.py:483] Algo bellman_ford step 1644 current loss 0.162454, current_train_items 52640.
I0302 18:58:30.711439 22683591385216 run.py:483] Algo bellman_ford step 1645 current loss 0.022464, current_train_items 52672.
I0302 18:58:30.726745 22683591385216 run.py:483] Algo bellman_ford step 1646 current loss 0.024137, current_train_items 52704.
I0302 18:58:30.749744 22683591385216 run.py:483] Algo bellman_ford step 1647 current loss 0.095950, current_train_items 52736.
I0302 18:58:30.776324 22683591385216 run.py:483] Algo bellman_ford step 1648 current loss 0.052234, current_train_items 52768.
I0302 18:58:30.808057 22683591385216 run.py:483] Algo bellman_ford step 1649 current loss 0.167507, current_train_items 52800.
I0302 18:58:30.825960 22683591385216 run.py:483] Algo bellman_ford step 1650 current loss 0.016978, current_train_items 52832.
I0302 18:58:30.833812 22683591385216 run.py:503] (val) algo bellman_ford step 1650: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 52832, 'step': 1650, 'algorithm': 'bellman_ford'}
I0302 18:58:30.833926 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.984, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:58:30.862706 22683591385216 run.py:483] Algo bellman_ford step 1651 current loss 0.039559, current_train_items 52864.
I0302 18:58:30.885800 22683591385216 run.py:483] Algo bellman_ford step 1652 current loss 0.097609, current_train_items 52896.
I0302 18:58:30.913871 22683591385216 run.py:483] Algo bellman_ford step 1653 current loss 0.069437, current_train_items 52928.
I0302 18:58:30.943682 22683591385216 run.py:483] Algo bellman_ford step 1654 current loss 0.161008, current_train_items 52960.
I0302 18:58:30.961973 22683591385216 run.py:483] Algo bellman_ford step 1655 current loss 0.004099, current_train_items 52992.
I0302 18:58:30.977460 22683591385216 run.py:483] Algo bellman_ford step 1656 current loss 0.039195, current_train_items 53024.
I0302 18:58:30.999574 22683591385216 run.py:483] Algo bellman_ford step 1657 current loss 0.076809, current_train_items 53056.
I0302 18:58:31.026167 22683591385216 run.py:483] Algo bellman_ford step 1658 current loss 0.123964, current_train_items 53088.
I0302 18:58:31.055079 22683591385216 run.py:483] Algo bellman_ford step 1659 current loss 0.132276, current_train_items 53120.
I0302 18:58:31.073221 22683591385216 run.py:483] Algo bellman_ford step 1660 current loss 0.012687, current_train_items 53152.
I0302 18:58:31.088905 22683591385216 run.py:483] Algo bellman_ford step 1661 current loss 0.040925, current_train_items 53184.
I0302 18:58:31.110150 22683591385216 run.py:483] Algo bellman_ford step 1662 current loss 0.148006, current_train_items 53216.
I0302 18:58:31.139271 22683591385216 run.py:483] Algo bellman_ford step 1663 current loss 0.196183, current_train_items 53248.
I0302 18:58:31.170686 22683591385216 run.py:483] Algo bellman_ford step 1664 current loss 0.186431, current_train_items 53280.
I0302 18:58:31.188427 22683591385216 run.py:483] Algo bellman_ford step 1665 current loss 0.017001, current_train_items 53312.
I0302 18:58:31.203746 22683591385216 run.py:483] Algo bellman_ford step 1666 current loss 0.060402, current_train_items 53344.
I0302 18:58:31.225346 22683591385216 run.py:483] Algo bellman_ford step 1667 current loss 0.156652, current_train_items 53376.
I0302 18:58:31.252484 22683591385216 run.py:483] Algo bellman_ford step 1668 current loss 0.171835, current_train_items 53408.
I0302 18:58:31.281436 22683591385216 run.py:483] Algo bellman_ford step 1669 current loss 0.285336, current_train_items 53440.
I0302 18:58:31.299637 22683591385216 run.py:483] Algo bellman_ford step 1670 current loss 0.004076, current_train_items 53472.
I0302 18:58:31.314916 22683591385216 run.py:483] Algo bellman_ford step 1671 current loss 0.039731, current_train_items 53504.
I0302 18:58:31.336395 22683591385216 run.py:483] Algo bellman_ford step 1672 current loss 0.051426, current_train_items 53536.
I0302 18:58:31.364952 22683591385216 run.py:483] Algo bellman_ford step 1673 current loss 0.084575, current_train_items 53568.
I0302 18:58:31.395934 22683591385216 run.py:483] Algo bellman_ford step 1674 current loss 0.110763, current_train_items 53600.
I0302 18:58:31.413801 22683591385216 run.py:483] Algo bellman_ford step 1675 current loss 0.002050, current_train_items 53632.
I0302 18:58:31.429457 22683591385216 run.py:483] Algo bellman_ford step 1676 current loss 0.082501, current_train_items 53664.
I0302 18:58:31.450980 22683591385216 run.py:483] Algo bellman_ford step 1677 current loss 0.063573, current_train_items 53696.
I0302 18:58:31.478554 22683591385216 run.py:483] Algo bellman_ford step 1678 current loss 0.079028, current_train_items 53728.
I0302 18:58:31.510300 22683591385216 run.py:483] Algo bellman_ford step 1679 current loss 0.118343, current_train_items 53760.
I0302 18:58:31.527879 22683591385216 run.py:483] Algo bellman_ford step 1680 current loss 0.014279, current_train_items 53792.
I0302 18:58:31.543077 22683591385216 run.py:483] Algo bellman_ford step 1681 current loss 0.012603, current_train_items 53824.
I0302 18:58:31.564303 22683591385216 run.py:483] Algo bellman_ford step 1682 current loss 0.069236, current_train_items 53856.
I0302 18:58:31.590775 22683591385216 run.py:483] Algo bellman_ford step 1683 current loss 0.089403, current_train_items 53888.
I0302 18:58:31.621161 22683591385216 run.py:483] Algo bellman_ford step 1684 current loss 0.088766, current_train_items 53920.
I0302 18:58:31.639520 22683591385216 run.py:483] Algo bellman_ford step 1685 current loss 0.023599, current_train_items 53952.
I0302 18:58:31.655053 22683591385216 run.py:483] Algo bellman_ford step 1686 current loss 0.065048, current_train_items 53984.
I0302 18:58:31.676845 22683591385216 run.py:483] Algo bellman_ford step 1687 current loss 0.055578, current_train_items 54016.
I0302 18:58:31.704406 22683591385216 run.py:483] Algo bellman_ford step 1688 current loss 0.096035, current_train_items 54048.
I0302 18:58:31.736733 22683591385216 run.py:483] Algo bellman_ford step 1689 current loss 0.103106, current_train_items 54080.
I0302 18:58:31.754629 22683591385216 run.py:483] Algo bellman_ford step 1690 current loss 0.010844, current_train_items 54112.
I0302 18:58:31.770543 22683591385216 run.py:483] Algo bellman_ford step 1691 current loss 0.034897, current_train_items 54144.
I0302 18:58:31.792508 22683591385216 run.py:483] Algo bellman_ford step 1692 current loss 0.049612, current_train_items 54176.
I0302 18:58:31.821087 22683591385216 run.py:483] Algo bellman_ford step 1693 current loss 0.097021, current_train_items 54208.
I0302 18:58:31.851890 22683591385216 run.py:483] Algo bellman_ford step 1694 current loss 0.139661, current_train_items 54240.
I0302 18:58:31.869439 22683591385216 run.py:483] Algo bellman_ford step 1695 current loss 0.008985, current_train_items 54272.
I0302 18:58:31.885011 22683591385216 run.py:483] Algo bellman_ford step 1696 current loss 0.032257, current_train_items 54304.
I0302 18:58:31.905298 22683591385216 run.py:483] Algo bellman_ford step 1697 current loss 0.050262, current_train_items 54336.
I0302 18:58:31.931789 22683591385216 run.py:483] Algo bellman_ford step 1698 current loss 0.076414, current_train_items 54368.
I0302 18:58:31.962803 22683591385216 run.py:483] Algo bellman_ford step 1699 current loss 0.121579, current_train_items 54400.
I0302 18:58:31.981311 22683591385216 run.py:483] Algo bellman_ford step 1700 current loss 0.008644, current_train_items 54432.
I0302 18:58:31.989131 22683591385216 run.py:503] (val) algo bellman_ford step 1700: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 54432, 'step': 1700, 'algorithm': 'bellman_ford'}
I0302 18:58:31.989238 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:32.005060 22683591385216 run.py:483] Algo bellman_ford step 1701 current loss 0.056712, current_train_items 54464.
I0302 18:58:32.026850 22683591385216 run.py:483] Algo bellman_ford step 1702 current loss 0.074822, current_train_items 54496.
I0302 18:58:32.054914 22683591385216 run.py:483] Algo bellman_ford step 1703 current loss 0.077453, current_train_items 54528.
I0302 18:58:32.085561 22683591385216 run.py:483] Algo bellman_ford step 1704 current loss 0.123771, current_train_items 54560.
I0302 18:58:32.103715 22683591385216 run.py:483] Algo bellman_ford step 1705 current loss 0.003790, current_train_items 54592.
I0302 18:58:32.118709 22683591385216 run.py:483] Algo bellman_ford step 1706 current loss 0.008659, current_train_items 54624.
I0302 18:58:32.140702 22683591385216 run.py:483] Algo bellman_ford step 1707 current loss 0.097446, current_train_items 54656.
I0302 18:58:32.168381 22683591385216 run.py:483] Algo bellman_ford step 1708 current loss 0.132875, current_train_items 54688.
I0302 18:58:32.200197 22683591385216 run.py:483] Algo bellman_ford step 1709 current loss 0.104902, current_train_items 54720.
I0302 18:58:32.218010 22683591385216 run.py:483] Algo bellman_ford step 1710 current loss 0.010605, current_train_items 54752.
I0302 18:58:32.233507 22683591385216 run.py:483] Algo bellman_ford step 1711 current loss 0.024541, current_train_items 54784.
I0302 18:58:32.255546 22683591385216 run.py:483] Algo bellman_ford step 1712 current loss 0.086445, current_train_items 54816.
I0302 18:58:32.282977 22683591385216 run.py:483] Algo bellman_ford step 1713 current loss 0.081663, current_train_items 54848.
I0302 18:58:32.313009 22683591385216 run.py:483] Algo bellman_ford step 1714 current loss 0.089574, current_train_items 54880.
I0302 18:58:32.330799 22683591385216 run.py:483] Algo bellman_ford step 1715 current loss 0.008719, current_train_items 54912.
I0302 18:58:32.345708 22683591385216 run.py:483] Algo bellman_ford step 1716 current loss 0.021331, current_train_items 54944.
I0302 18:58:32.368014 22683591385216 run.py:483] Algo bellman_ford step 1717 current loss 0.053892, current_train_items 54976.
I0302 18:58:32.394447 22683591385216 run.py:483] Algo bellman_ford step 1718 current loss 0.137716, current_train_items 55008.
I0302 18:58:32.424569 22683591385216 run.py:483] Algo bellman_ford step 1719 current loss 0.105792, current_train_items 55040.
I0302 18:58:32.442593 22683591385216 run.py:483] Algo bellman_ford step 1720 current loss 0.027447, current_train_items 55072.
I0302 18:58:32.457638 22683591385216 run.py:483] Algo bellman_ford step 1721 current loss 0.048039, current_train_items 55104.
I0302 18:58:32.479311 22683591385216 run.py:483] Algo bellman_ford step 1722 current loss 0.050843, current_train_items 55136.
I0302 18:58:32.506998 22683591385216 run.py:483] Algo bellman_ford step 1723 current loss 0.097666, current_train_items 55168.
I0302 18:58:32.537116 22683591385216 run.py:483] Algo bellman_ford step 1724 current loss 0.127259, current_train_items 55200.
I0302 18:58:32.554545 22683591385216 run.py:483] Algo bellman_ford step 1725 current loss 0.002724, current_train_items 55232.
I0302 18:58:32.569998 22683591385216 run.py:483] Algo bellman_ford step 1726 current loss 0.054972, current_train_items 55264.
I0302 18:58:32.593488 22683591385216 run.py:483] Algo bellman_ford step 1727 current loss 0.073169, current_train_items 55296.
I0302 18:58:32.620882 22683591385216 run.py:483] Algo bellman_ford step 1728 current loss 0.099341, current_train_items 55328.
I0302 18:58:32.650933 22683591385216 run.py:483] Algo bellman_ford step 1729 current loss 0.106128, current_train_items 55360.
I0302 18:58:32.668703 22683591385216 run.py:483] Algo bellman_ford step 1730 current loss 0.008051, current_train_items 55392.
I0302 18:58:32.684251 22683591385216 run.py:483] Algo bellman_ford step 1731 current loss 0.020451, current_train_items 55424.
I0302 18:58:32.706200 22683591385216 run.py:483] Algo bellman_ford step 1732 current loss 0.067792, current_train_items 55456.
I0302 18:58:32.734119 22683591385216 run.py:483] Algo bellman_ford step 1733 current loss 0.072866, current_train_items 55488.
I0302 18:58:32.762237 22683591385216 run.py:483] Algo bellman_ford step 1734 current loss 0.087540, current_train_items 55520.
I0302 18:58:32.780157 22683591385216 run.py:483] Algo bellman_ford step 1735 current loss 0.004638, current_train_items 55552.
I0302 18:58:32.795719 22683591385216 run.py:483] Algo bellman_ford step 1736 current loss 0.027612, current_train_items 55584.
I0302 18:58:32.817721 22683591385216 run.py:483] Algo bellman_ford step 1737 current loss 0.122484, current_train_items 55616.
I0302 18:58:32.845511 22683591385216 run.py:483] Algo bellman_ford step 1738 current loss 0.140924, current_train_items 55648.
I0302 18:58:32.876239 22683591385216 run.py:483] Algo bellman_ford step 1739 current loss 0.105548, current_train_items 55680.
I0302 18:58:32.893882 22683591385216 run.py:483] Algo bellman_ford step 1740 current loss 0.006272, current_train_items 55712.
I0302 18:58:32.909327 22683591385216 run.py:483] Algo bellman_ford step 1741 current loss 0.042731, current_train_items 55744.
I0302 18:58:32.931252 22683591385216 run.py:483] Algo bellman_ford step 1742 current loss 0.072147, current_train_items 55776.
I0302 18:58:32.959622 22683591385216 run.py:483] Algo bellman_ford step 1743 current loss 0.062693, current_train_items 55808.
I0302 18:58:32.988773 22683591385216 run.py:483] Algo bellman_ford step 1744 current loss 0.118722, current_train_items 55840.
I0302 18:58:33.006490 22683591385216 run.py:483] Algo bellman_ford step 1745 current loss 0.003033, current_train_items 55872.
I0302 18:58:33.021986 22683591385216 run.py:483] Algo bellman_ford step 1746 current loss 0.028544, current_train_items 55904.
I0302 18:58:33.043596 22683591385216 run.py:483] Algo bellman_ford step 1747 current loss 0.083778, current_train_items 55936.
I0302 18:58:33.071695 22683591385216 run.py:483] Algo bellman_ford step 1748 current loss 0.067137, current_train_items 55968.
I0302 18:58:33.100633 22683591385216 run.py:483] Algo bellman_ford step 1749 current loss 0.073254, current_train_items 56000.
I0302 18:58:33.118529 22683591385216 run.py:483] Algo bellman_ford step 1750 current loss 0.042740, current_train_items 56032.
I0302 18:58:33.126285 22683591385216 run.py:503] (val) algo bellman_ford step 1750: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 56032, 'step': 1750, 'algorithm': 'bellman_ford'}
I0302 18:58:33.126388 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:58:33.142432 22683591385216 run.py:483] Algo bellman_ford step 1751 current loss 0.059122, current_train_items 56064.
I0302 18:58:33.164524 22683591385216 run.py:483] Algo bellman_ford step 1752 current loss 0.065881, current_train_items 56096.
I0302 18:58:33.194324 22683591385216 run.py:483] Algo bellman_ford step 1753 current loss 0.119702, current_train_items 56128.
I0302 18:58:33.226441 22683591385216 run.py:483] Algo bellman_ford step 1754 current loss 0.166697, current_train_items 56160.
I0302 18:58:33.244490 22683591385216 run.py:483] Algo bellman_ford step 1755 current loss 0.005321, current_train_items 56192.
I0302 18:58:33.259965 22683591385216 run.py:483] Algo bellman_ford step 1756 current loss 0.031096, current_train_items 56224.
I0302 18:58:33.282684 22683591385216 run.py:483] Algo bellman_ford step 1757 current loss 0.109377, current_train_items 56256.
I0302 18:58:33.311240 22683591385216 run.py:483] Algo bellman_ford step 1758 current loss 0.079024, current_train_items 56288.
I0302 18:58:33.339253 22683591385216 run.py:483] Algo bellman_ford step 1759 current loss 0.067131, current_train_items 56320.
I0302 18:58:33.357348 22683591385216 run.py:483] Algo bellman_ford step 1760 current loss 0.007695, current_train_items 56352.
I0302 18:58:33.372767 22683591385216 run.py:483] Algo bellman_ford step 1761 current loss 0.025397, current_train_items 56384.
I0302 18:58:33.395047 22683591385216 run.py:483] Algo bellman_ford step 1762 current loss 0.130153, current_train_items 56416.
I0302 18:58:33.424683 22683591385216 run.py:483] Algo bellman_ford step 1763 current loss 0.115419, current_train_items 56448.
I0302 18:58:33.456853 22683591385216 run.py:483] Algo bellman_ford step 1764 current loss 0.133361, current_train_items 56480.
I0302 18:58:33.474557 22683591385216 run.py:483] Algo bellman_ford step 1765 current loss 0.010898, current_train_items 56512.
I0302 18:58:33.489620 22683591385216 run.py:483] Algo bellman_ford step 1766 current loss 0.016594, current_train_items 56544.
I0302 18:58:33.511127 22683591385216 run.py:483] Algo bellman_ford step 1767 current loss 0.109775, current_train_items 56576.
I0302 18:58:33.538864 22683591385216 run.py:483] Algo bellman_ford step 1768 current loss 0.101034, current_train_items 56608.
I0302 18:58:33.568002 22683591385216 run.py:483] Algo bellman_ford step 1769 current loss 0.146187, current_train_items 56640.
I0302 18:58:33.586188 22683591385216 run.py:483] Algo bellman_ford step 1770 current loss 0.026085, current_train_items 56672.
I0302 18:58:33.601821 22683591385216 run.py:483] Algo bellman_ford step 1771 current loss 0.044375, current_train_items 56704.
I0302 18:58:33.624519 22683591385216 run.py:483] Algo bellman_ford step 1772 current loss 0.109017, current_train_items 56736.
I0302 18:58:33.652803 22683591385216 run.py:483] Algo bellman_ford step 1773 current loss 0.130205, current_train_items 56768.
I0302 18:58:33.682166 22683591385216 run.py:483] Algo bellman_ford step 1774 current loss 0.115350, current_train_items 56800.
I0302 18:58:33.700492 22683591385216 run.py:483] Algo bellman_ford step 1775 current loss 0.007600, current_train_items 56832.
I0302 18:58:33.716313 22683591385216 run.py:483] Algo bellman_ford step 1776 current loss 0.051848, current_train_items 56864.
I0302 18:58:33.738314 22683591385216 run.py:483] Algo bellman_ford step 1777 current loss 0.146250, current_train_items 56896.
I0302 18:58:33.766055 22683591385216 run.py:483] Algo bellman_ford step 1778 current loss 0.167891, current_train_items 56928.
I0302 18:58:33.794561 22683591385216 run.py:483] Algo bellman_ford step 1779 current loss 0.135031, current_train_items 56960.
I0302 18:58:33.812431 22683591385216 run.py:483] Algo bellman_ford step 1780 current loss 0.006418, current_train_items 56992.
I0302 18:58:33.827631 22683591385216 run.py:483] Algo bellman_ford step 1781 current loss 0.016112, current_train_items 57024.
I0302 18:58:33.849171 22683591385216 run.py:483] Algo bellman_ford step 1782 current loss 0.118430, current_train_items 57056.
I0302 18:58:33.877493 22683591385216 run.py:483] Algo bellman_ford step 1783 current loss 0.157479, current_train_items 57088.
I0302 18:58:33.906924 22683591385216 run.py:483] Algo bellman_ford step 1784 current loss 0.099541, current_train_items 57120.
I0302 18:58:33.924878 22683591385216 run.py:483] Algo bellman_ford step 1785 current loss 0.002608, current_train_items 57152.
I0302 18:58:33.940691 22683591385216 run.py:483] Algo bellman_ford step 1786 current loss 0.019356, current_train_items 57184.
I0302 18:58:33.961668 22683591385216 run.py:483] Algo bellman_ford step 1787 current loss 0.124204, current_train_items 57216.
I0302 18:58:33.988436 22683591385216 run.py:483] Algo bellman_ford step 1788 current loss 0.082864, current_train_items 57248.
I0302 18:58:34.018540 22683591385216 run.py:483] Algo bellman_ford step 1789 current loss 0.134566, current_train_items 57280.
I0302 18:58:34.036347 22683591385216 run.py:483] Algo bellman_ford step 1790 current loss 0.005675, current_train_items 57312.
I0302 18:58:34.052391 22683591385216 run.py:483] Algo bellman_ford step 1791 current loss 0.038737, current_train_items 57344.
I0302 18:58:34.074596 22683591385216 run.py:483] Algo bellman_ford step 1792 current loss 0.049495, current_train_items 57376.
I0302 18:58:34.101248 22683591385216 run.py:483] Algo bellman_ford step 1793 current loss 0.140881, current_train_items 57408.
I0302 18:58:34.131087 22683591385216 run.py:483] Algo bellman_ford step 1794 current loss 0.125380, current_train_items 57440.
I0302 18:58:34.148691 22683591385216 run.py:483] Algo bellman_ford step 1795 current loss 0.004696, current_train_items 57472.
I0302 18:58:34.163904 22683591385216 run.py:483] Algo bellman_ford step 1796 current loss 0.060268, current_train_items 57504.
I0302 18:58:34.186427 22683591385216 run.py:483] Algo bellman_ford step 1797 current loss 0.056840, current_train_items 57536.
I0302 18:58:34.214516 22683591385216 run.py:483] Algo bellman_ford step 1798 current loss 0.140083, current_train_items 57568.
I0302 18:58:34.245641 22683591385216 run.py:483] Algo bellman_ford step 1799 current loss 0.086279, current_train_items 57600.
I0302 18:58:34.263710 22683591385216 run.py:483] Algo bellman_ford step 1800 current loss 0.009300, current_train_items 57632.
I0302 18:58:34.271461 22683591385216 run.py:503] (val) algo bellman_ford step 1800: {'pi': 0.9599609375, 'score': 0.9599609375, 'examples_seen': 57632, 'step': 1800, 'algorithm': 'bellman_ford'}
I0302 18:58:34.271566 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.960, val scores are: bellman_ford: 0.960
I0302 18:58:34.287167 22683591385216 run.py:483] Algo bellman_ford step 1801 current loss 0.011562, current_train_items 57664.
I0302 18:58:34.309849 22683591385216 run.py:483] Algo bellman_ford step 1802 current loss 0.115222, current_train_items 57696.
I0302 18:58:34.337234 22683591385216 run.py:483] Algo bellman_ford step 1803 current loss 0.130732, current_train_items 57728.
I0302 18:58:34.366830 22683591385216 run.py:483] Algo bellman_ford step 1804 current loss 0.193618, current_train_items 57760.
I0302 18:58:34.384908 22683591385216 run.py:483] Algo bellman_ford step 1805 current loss 0.003000, current_train_items 57792.
I0302 18:58:34.399970 22683591385216 run.py:483] Algo bellman_ford step 1806 current loss 0.013407, current_train_items 57824.
I0302 18:58:34.421457 22683591385216 run.py:483] Algo bellman_ford step 1807 current loss 0.044738, current_train_items 57856.
I0302 18:58:34.447990 22683591385216 run.py:483] Algo bellman_ford step 1808 current loss 0.084333, current_train_items 57888.
I0302 18:58:34.479068 22683591385216 run.py:483] Algo bellman_ford step 1809 current loss 0.111066, current_train_items 57920.
I0302 18:58:34.496520 22683591385216 run.py:483] Algo bellman_ford step 1810 current loss 0.004385, current_train_items 57952.
I0302 18:58:34.511743 22683591385216 run.py:483] Algo bellman_ford step 1811 current loss 0.038303, current_train_items 57984.
I0302 18:58:34.533818 22683591385216 run.py:483] Algo bellman_ford step 1812 current loss 0.067781, current_train_items 58016.
I0302 18:58:34.559740 22683591385216 run.py:483] Algo bellman_ford step 1813 current loss 0.058719, current_train_items 58048.
I0302 18:58:34.589995 22683591385216 run.py:483] Algo bellman_ford step 1814 current loss 0.112335, current_train_items 58080.
I0302 18:58:34.607618 22683591385216 run.py:483] Algo bellman_ford step 1815 current loss 0.002823, current_train_items 58112.
I0302 18:58:34.623416 22683591385216 run.py:483] Algo bellman_ford step 1816 current loss 0.110843, current_train_items 58144.
I0302 18:58:34.645005 22683591385216 run.py:483] Algo bellman_ford step 1817 current loss 0.069598, current_train_items 58176.
I0302 18:58:34.672872 22683591385216 run.py:483] Algo bellman_ford step 1818 current loss 0.056984, current_train_items 58208.
I0302 18:58:34.702804 22683591385216 run.py:483] Algo bellman_ford step 1819 current loss 0.107401, current_train_items 58240.
I0302 18:58:34.720361 22683591385216 run.py:483] Algo bellman_ford step 1820 current loss 0.008062, current_train_items 58272.
I0302 18:58:34.736223 22683591385216 run.py:483] Algo bellman_ford step 1821 current loss 0.052174, current_train_items 58304.
I0302 18:58:34.760075 22683591385216 run.py:483] Algo bellman_ford step 1822 current loss 0.060253, current_train_items 58336.
I0302 18:58:34.788449 22683591385216 run.py:483] Algo bellman_ford step 1823 current loss 0.075063, current_train_items 58368.
I0302 18:58:34.817678 22683591385216 run.py:483] Algo bellman_ford step 1824 current loss 0.097062, current_train_items 58400.
I0302 18:58:34.835177 22683591385216 run.py:483] Algo bellman_ford step 1825 current loss 0.003965, current_train_items 58432.
I0302 18:58:34.850633 22683591385216 run.py:483] Algo bellman_ford step 1826 current loss 0.057056, current_train_items 58464.
I0302 18:58:34.872305 22683591385216 run.py:483] Algo bellman_ford step 1827 current loss 0.051021, current_train_items 58496.
I0302 18:58:34.901588 22683591385216 run.py:483] Algo bellman_ford step 1828 current loss 0.099682, current_train_items 58528.
I0302 18:58:34.934439 22683591385216 run.py:483] Algo bellman_ford step 1829 current loss 0.125249, current_train_items 58560.
I0302 18:58:34.952556 22683591385216 run.py:483] Algo bellman_ford step 1830 current loss 0.006773, current_train_items 58592.
I0302 18:58:34.968038 22683591385216 run.py:483] Algo bellman_ford step 1831 current loss 0.067328, current_train_items 58624.
I0302 18:58:34.991940 22683591385216 run.py:483] Algo bellman_ford step 1832 current loss 0.048933, current_train_items 58656.
I0302 18:58:35.019741 22683591385216 run.py:483] Algo bellman_ford step 1833 current loss 0.081232, current_train_items 58688.
I0302 18:58:35.049244 22683591385216 run.py:483] Algo bellman_ford step 1834 current loss 0.097614, current_train_items 58720.
I0302 18:58:35.066775 22683591385216 run.py:483] Algo bellman_ford step 1835 current loss 0.004735, current_train_items 58752.
I0302 18:58:35.082375 22683591385216 run.py:483] Algo bellman_ford step 1836 current loss 0.032502, current_train_items 58784.
I0302 18:58:35.105617 22683591385216 run.py:483] Algo bellman_ford step 1837 current loss 0.061507, current_train_items 58816.
I0302 18:58:35.132953 22683591385216 run.py:483] Algo bellman_ford step 1838 current loss 0.090007, current_train_items 58848.
I0302 18:58:35.163415 22683591385216 run.py:483] Algo bellman_ford step 1839 current loss 0.128707, current_train_items 58880.
I0302 18:58:35.181147 22683591385216 run.py:483] Algo bellman_ford step 1840 current loss 0.010924, current_train_items 58912.
I0302 18:58:35.196551 22683591385216 run.py:483] Algo bellman_ford step 1841 current loss 0.027425, current_train_items 58944.
I0302 18:58:35.219086 22683591385216 run.py:483] Algo bellman_ford step 1842 current loss 0.079868, current_train_items 58976.
I0302 18:58:35.247365 22683591385216 run.py:483] Algo bellman_ford step 1843 current loss 0.096309, current_train_items 59008.
I0302 18:58:35.277520 22683591385216 run.py:483] Algo bellman_ford step 1844 current loss 0.102559, current_train_items 59040.
I0302 18:58:35.295212 22683591385216 run.py:483] Algo bellman_ford step 1845 current loss 0.004514, current_train_items 59072.
I0302 18:58:35.310348 22683591385216 run.py:483] Algo bellman_ford step 1846 current loss 0.024724, current_train_items 59104.
I0302 18:58:35.332618 22683591385216 run.py:483] Algo bellman_ford step 1847 current loss 0.133164, current_train_items 59136.
I0302 18:58:35.360688 22683591385216 run.py:483] Algo bellman_ford step 1848 current loss 0.094423, current_train_items 59168.
I0302 18:58:35.392239 22683591385216 run.py:483] Algo bellman_ford step 1849 current loss 0.138865, current_train_items 59200.
I0302 18:58:35.410123 22683591385216 run.py:483] Algo bellman_ford step 1850 current loss 0.017025, current_train_items 59232.
I0302 18:58:35.418017 22683591385216 run.py:503] (val) algo bellman_ford step 1850: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 59232, 'step': 1850, 'algorithm': 'bellman_ford'}
I0302 18:58:35.418124 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0302 18:58:35.434087 22683591385216 run.py:483] Algo bellman_ford step 1851 current loss 0.035053, current_train_items 59264.
I0302 18:58:35.456563 22683591385216 run.py:483] Algo bellman_ford step 1852 current loss 0.133965, current_train_items 59296.
I0302 18:58:35.484781 22683591385216 run.py:483] Algo bellman_ford step 1853 current loss 0.227005, current_train_items 59328.
I0302 18:58:35.515547 22683591385216 run.py:483] Algo bellman_ford step 1854 current loss 0.215626, current_train_items 59360.
I0302 18:58:35.533472 22683591385216 run.py:483] Algo bellman_ford step 1855 current loss 0.023824, current_train_items 59392.
I0302 18:58:35.548229 22683591385216 run.py:483] Algo bellman_ford step 1856 current loss 0.036490, current_train_items 59424.
I0302 18:58:35.570654 22683591385216 run.py:483] Algo bellman_ford step 1857 current loss 0.110730, current_train_items 59456.
I0302 18:58:35.597264 22683591385216 run.py:483] Algo bellman_ford step 1858 current loss 0.113158, current_train_items 59488.
I0302 18:58:35.627619 22683591385216 run.py:483] Algo bellman_ford step 1859 current loss 0.207864, current_train_items 59520.
I0302 18:58:35.645730 22683591385216 run.py:483] Algo bellman_ford step 1860 current loss 0.010486, current_train_items 59552.
I0302 18:58:35.661628 22683591385216 run.py:483] Algo bellman_ford step 1861 current loss 0.033041, current_train_items 59584.
I0302 18:58:35.683237 22683591385216 run.py:483] Algo bellman_ford step 1862 current loss 0.041913, current_train_items 59616.
I0302 18:58:35.710567 22683591385216 run.py:483] Algo bellman_ford step 1863 current loss 0.088385, current_train_items 59648.
I0302 18:58:35.741565 22683591385216 run.py:483] Algo bellman_ford step 1864 current loss 0.146935, current_train_items 59680.
I0302 18:58:35.759210 22683591385216 run.py:483] Algo bellman_ford step 1865 current loss 0.007395, current_train_items 59712.
I0302 18:58:35.774573 22683591385216 run.py:483] Algo bellman_ford step 1866 current loss 0.035499, current_train_items 59744.
I0302 18:58:35.797509 22683591385216 run.py:483] Algo bellman_ford step 1867 current loss 0.070866, current_train_items 59776.
I0302 18:58:35.825446 22683591385216 run.py:483] Algo bellman_ford step 1868 current loss 0.088823, current_train_items 59808.
I0302 18:58:35.855411 22683591385216 run.py:483] Algo bellman_ford step 1869 current loss 0.115508, current_train_items 59840.
I0302 18:58:35.873513 22683591385216 run.py:483] Algo bellman_ford step 1870 current loss 0.005515, current_train_items 59872.
I0302 18:58:35.888957 22683591385216 run.py:483] Algo bellman_ford step 1871 current loss 0.012205, current_train_items 59904.
I0302 18:58:35.910417 22683591385216 run.py:483] Algo bellman_ford step 1872 current loss 0.025940, current_train_items 59936.
I0302 18:58:35.938200 22683591385216 run.py:483] Algo bellman_ford step 1873 current loss 0.105308, current_train_items 59968.
I0302 18:58:35.968084 22683591385216 run.py:483] Algo bellman_ford step 1874 current loss 0.074972, current_train_items 60000.
I0302 18:58:35.986061 22683591385216 run.py:483] Algo bellman_ford step 1875 current loss 0.026323, current_train_items 60032.
I0302 18:58:36.001454 22683591385216 run.py:483] Algo bellman_ford step 1876 current loss 0.027071, current_train_items 60064.
I0302 18:58:36.023322 22683591385216 run.py:483] Algo bellman_ford step 1877 current loss 0.107141, current_train_items 60096.
I0302 18:58:36.049887 22683591385216 run.py:483] Algo bellman_ford step 1878 current loss 0.088565, current_train_items 60128.
I0302 18:58:36.082886 22683591385216 run.py:483] Algo bellman_ford step 1879 current loss 0.143300, current_train_items 60160.
I0302 18:58:36.100440 22683591385216 run.py:483] Algo bellman_ford step 1880 current loss 0.003024, current_train_items 60192.
I0302 18:58:36.116156 22683591385216 run.py:483] Algo bellman_ford step 1881 current loss 0.031128, current_train_items 60224.
I0302 18:58:36.137540 22683591385216 run.py:483] Algo bellman_ford step 1882 current loss 0.060286, current_train_items 60256.
I0302 18:58:36.165333 22683591385216 run.py:483] Algo bellman_ford step 1883 current loss 0.076288, current_train_items 60288.
I0302 18:58:36.195649 22683591385216 run.py:483] Algo bellman_ford step 1884 current loss 0.119113, current_train_items 60320.
I0302 18:58:36.213412 22683591385216 run.py:483] Algo bellman_ford step 1885 current loss 0.051275, current_train_items 60352.
I0302 18:58:36.229258 22683591385216 run.py:483] Algo bellman_ford step 1886 current loss 0.032189, current_train_items 60384.
I0302 18:58:36.250980 22683591385216 run.py:483] Algo bellman_ford step 1887 current loss 0.063709, current_train_items 60416.
I0302 18:58:36.277735 22683591385216 run.py:483] Algo bellman_ford step 1888 current loss 0.064943, current_train_items 60448.
I0302 18:58:36.309762 22683591385216 run.py:483] Algo bellman_ford step 1889 current loss 0.128928, current_train_items 60480.
I0302 18:58:36.327416 22683591385216 run.py:483] Algo bellman_ford step 1890 current loss 0.002891, current_train_items 60512.
I0302 18:58:36.342990 22683591385216 run.py:483] Algo bellman_ford step 1891 current loss 0.033004, current_train_items 60544.
I0302 18:58:36.364418 22683591385216 run.py:483] Algo bellman_ford step 1892 current loss 0.043291, current_train_items 60576.
I0302 18:58:36.392616 22683591385216 run.py:483] Algo bellman_ford step 1893 current loss 0.073903, current_train_items 60608.
I0302 18:58:36.425718 22683591385216 run.py:483] Algo bellman_ford step 1894 current loss 0.103741, current_train_items 60640.
I0302 18:58:36.443171 22683591385216 run.py:483] Algo bellman_ford step 1895 current loss 0.006074, current_train_items 60672.
I0302 18:58:36.458364 22683591385216 run.py:483] Algo bellman_ford step 1896 current loss 0.019610, current_train_items 60704.
I0302 18:58:36.480396 22683591385216 run.py:483] Algo bellman_ford step 1897 current loss 0.097534, current_train_items 60736.
I0302 18:58:36.508504 22683591385216 run.py:483] Algo bellman_ford step 1898 current loss 0.080225, current_train_items 60768.
I0302 18:58:36.539004 22683591385216 run.py:483] Algo bellman_ford step 1899 current loss 0.135211, current_train_items 60800.
I0302 18:58:36.557045 22683591385216 run.py:483] Algo bellman_ford step 1900 current loss 0.004474, current_train_items 60832.
I0302 18:58:36.564815 22683591385216 run.py:503] (val) algo bellman_ford step 1900: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 60832, 'step': 1900, 'algorithm': 'bellman_ford'}
I0302 18:58:36.564934 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:58:36.580755 22683591385216 run.py:483] Algo bellman_ford step 1901 current loss 0.026469, current_train_items 60864.
I0302 18:58:36.602743 22683591385216 run.py:483] Algo bellman_ford step 1902 current loss 0.056020, current_train_items 60896.
I0302 18:58:36.629185 22683591385216 run.py:483] Algo bellman_ford step 1903 current loss 0.079148, current_train_items 60928.
I0302 18:58:36.659360 22683591385216 run.py:483] Algo bellman_ford step 1904 current loss 0.109272, current_train_items 60960.
I0302 18:58:36.677445 22683591385216 run.py:483] Algo bellman_ford step 1905 current loss 0.021941, current_train_items 60992.
I0302 18:58:36.692562 22683591385216 run.py:483] Algo bellman_ford step 1906 current loss 0.026210, current_train_items 61024.
I0302 18:58:36.713604 22683591385216 run.py:483] Algo bellman_ford step 1907 current loss 0.089833, current_train_items 61056.
I0302 18:58:36.740929 22683591385216 run.py:483] Algo bellman_ford step 1908 current loss 0.108030, current_train_items 61088.
I0302 18:58:36.770552 22683591385216 run.py:483] Algo bellman_ford step 1909 current loss 0.079521, current_train_items 61120.
I0302 18:58:36.788173 22683591385216 run.py:483] Algo bellman_ford step 1910 current loss 0.008262, current_train_items 61152.
I0302 18:58:36.803690 22683591385216 run.py:483] Algo bellman_ford step 1911 current loss 0.032307, current_train_items 61184.
I0302 18:58:36.826437 22683591385216 run.py:483] Algo bellman_ford step 1912 current loss 0.140610, current_train_items 61216.
I0302 18:58:36.853692 22683591385216 run.py:483] Algo bellman_ford step 1913 current loss 0.125986, current_train_items 61248.
I0302 18:58:36.884447 22683591385216 run.py:483] Algo bellman_ford step 1914 current loss 0.096522, current_train_items 61280.
I0302 18:58:36.902038 22683591385216 run.py:483] Algo bellman_ford step 1915 current loss 0.008080, current_train_items 61312.
I0302 18:58:36.917618 22683591385216 run.py:483] Algo bellman_ford step 1916 current loss 0.041328, current_train_items 61344.
I0302 18:58:36.939985 22683591385216 run.py:483] Algo bellman_ford step 1917 current loss 0.142574, current_train_items 61376.
I0302 18:58:36.968336 22683591385216 run.py:483] Algo bellman_ford step 1918 current loss 0.090580, current_train_items 61408.
I0302 18:58:37.000264 22683591385216 run.py:483] Algo bellman_ford step 1919 current loss 0.113823, current_train_items 61440.
I0302 18:58:37.018009 22683591385216 run.py:483] Algo bellman_ford step 1920 current loss 0.008926, current_train_items 61472.
I0302 18:58:37.033555 22683591385216 run.py:483] Algo bellman_ford step 1921 current loss 0.060448, current_train_items 61504.
I0302 18:58:37.055153 22683591385216 run.py:483] Algo bellman_ford step 1922 current loss 0.147409, current_train_items 61536.
I0302 18:58:37.082071 22683591385216 run.py:483] Algo bellman_ford step 1923 current loss 0.182903, current_train_items 61568.
I0302 18:58:37.112568 22683591385216 run.py:483] Algo bellman_ford step 1924 current loss 0.202354, current_train_items 61600.
I0302 18:58:37.130267 22683591385216 run.py:483] Algo bellman_ford step 1925 current loss 0.003018, current_train_items 61632.
I0302 18:58:37.145809 22683591385216 run.py:483] Algo bellman_ford step 1926 current loss 0.073222, current_train_items 61664.
I0302 18:58:37.166652 22683591385216 run.py:483] Algo bellman_ford step 1927 current loss 0.031507, current_train_items 61696.
I0302 18:58:37.193872 22683591385216 run.py:483] Algo bellman_ford step 1928 current loss 0.085456, current_train_items 61728.
I0302 18:58:37.222651 22683591385216 run.py:483] Algo bellman_ford step 1929 current loss 0.139629, current_train_items 61760.
I0302 18:58:37.240516 22683591385216 run.py:483] Algo bellman_ford step 1930 current loss 0.012788, current_train_items 61792.
I0302 18:58:37.255505 22683591385216 run.py:483] Algo bellman_ford step 1931 current loss 0.045779, current_train_items 61824.
I0302 18:58:37.278722 22683591385216 run.py:483] Algo bellman_ford step 1932 current loss 0.086065, current_train_items 61856.
I0302 18:58:37.306526 22683591385216 run.py:483] Algo bellman_ford step 1933 current loss 0.107431, current_train_items 61888.
I0302 18:58:37.335302 22683591385216 run.py:483] Algo bellman_ford step 1934 current loss 0.083014, current_train_items 61920.
I0302 18:58:37.352864 22683591385216 run.py:483] Algo bellman_ford step 1935 current loss 0.024426, current_train_items 61952.
I0302 18:58:37.368233 22683591385216 run.py:483] Algo bellman_ford step 1936 current loss 0.044485, current_train_items 61984.
I0302 18:58:37.390931 22683591385216 run.py:483] Algo bellman_ford step 1937 current loss 0.052480, current_train_items 62016.
I0302 18:58:37.418006 22683591385216 run.py:483] Algo bellman_ford step 1938 current loss 0.086237, current_train_items 62048.
I0302 18:58:37.448774 22683591385216 run.py:483] Algo bellman_ford step 1939 current loss 0.163338, current_train_items 62080.
I0302 18:58:37.466523 22683591385216 run.py:483] Algo bellman_ford step 1940 current loss 0.010359, current_train_items 62112.
I0302 18:58:37.481939 22683591385216 run.py:483] Algo bellman_ford step 1941 current loss 0.018580, current_train_items 62144.
I0302 18:58:37.503673 22683591385216 run.py:483] Algo bellman_ford step 1942 current loss 0.070133, current_train_items 62176.
I0302 18:58:37.530206 22683591385216 run.py:483] Algo bellman_ford step 1943 current loss 0.077549, current_train_items 62208.
I0302 18:58:37.560889 22683591385216 run.py:483] Algo bellman_ford step 1944 current loss 0.173375, current_train_items 62240.
I0302 18:58:37.578625 22683591385216 run.py:483] Algo bellman_ford step 1945 current loss 0.011879, current_train_items 62272.
I0302 18:58:37.594096 22683591385216 run.py:483] Algo bellman_ford step 1946 current loss 0.104063, current_train_items 62304.
I0302 18:58:37.616276 22683591385216 run.py:483] Algo bellman_ford step 1947 current loss 0.099561, current_train_items 62336.
I0302 18:58:37.644817 22683591385216 run.py:483] Algo bellman_ford step 1948 current loss 0.116191, current_train_items 62368.
I0302 18:58:37.673992 22683591385216 run.py:483] Algo bellman_ford step 1949 current loss 0.087356, current_train_items 62400.
I0302 18:58:37.691609 22683591385216 run.py:483] Algo bellman_ford step 1950 current loss 0.031285, current_train_items 62432.
I0302 18:58:37.699601 22683591385216 run.py:503] (val) algo bellman_ford step 1950: {'pi': 0.95703125, 'score': 0.95703125, 'examples_seen': 62432, 'step': 1950, 'algorithm': 'bellman_ford'}
I0302 18:58:37.699707 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.957, val scores are: bellman_ford: 0.957
I0302 18:58:37.715817 22683591385216 run.py:483] Algo bellman_ford step 1951 current loss 0.017694, current_train_items 62464.
I0302 18:58:37.738420 22683591385216 run.py:483] Algo bellman_ford step 1952 current loss 0.182871, current_train_items 62496.
I0302 18:58:37.768015 22683591385216 run.py:483] Algo bellman_ford step 1953 current loss 0.234593, current_train_items 62528.
I0302 18:58:37.798429 22683591385216 run.py:483] Algo bellman_ford step 1954 current loss 0.192379, current_train_items 62560.
I0302 18:58:37.816544 22683591385216 run.py:483] Algo bellman_ford step 1955 current loss 0.009993, current_train_items 62592.
I0302 18:58:37.831055 22683591385216 run.py:483] Algo bellman_ford step 1956 current loss 0.021879, current_train_items 62624.
I0302 18:58:37.853255 22683591385216 run.py:483] Algo bellman_ford step 1957 current loss 0.100996, current_train_items 62656.
I0302 18:58:37.882005 22683591385216 run.py:483] Algo bellman_ford step 1958 current loss 0.111004, current_train_items 62688.
I0302 18:58:37.913057 22683591385216 run.py:483] Algo bellman_ford step 1959 current loss 0.132541, current_train_items 62720.
I0302 18:58:37.931157 22683591385216 run.py:483] Algo bellman_ford step 1960 current loss 0.004436, current_train_items 62752.
I0302 18:58:37.946802 22683591385216 run.py:483] Algo bellman_ford step 1961 current loss 0.046573, current_train_items 62784.
I0302 18:58:37.968426 22683591385216 run.py:483] Algo bellman_ford step 1962 current loss 0.074151, current_train_items 62816.
I0302 18:58:37.995109 22683591385216 run.py:483] Algo bellman_ford step 1963 current loss 0.098513, current_train_items 62848.
I0302 18:58:38.028078 22683591385216 run.py:483] Algo bellman_ford step 1964 current loss 0.114585, current_train_items 62880.
I0302 18:58:38.045810 22683591385216 run.py:483] Algo bellman_ford step 1965 current loss 0.002003, current_train_items 62912.
I0302 18:58:38.060984 22683591385216 run.py:483] Algo bellman_ford step 1966 current loss 0.034615, current_train_items 62944.
I0302 18:58:38.082810 22683591385216 run.py:483] Algo bellman_ford step 1967 current loss 0.057332, current_train_items 62976.
I0302 18:58:38.109792 22683591385216 run.py:483] Algo bellman_ford step 1968 current loss 0.079987, current_train_items 63008.
I0302 18:58:38.139556 22683591385216 run.py:483] Algo bellman_ford step 1969 current loss 0.089936, current_train_items 63040.
I0302 18:58:38.157554 22683591385216 run.py:483] Algo bellman_ford step 1970 current loss 0.001580, current_train_items 63072.
I0302 18:58:38.173025 22683591385216 run.py:483] Algo bellman_ford step 1971 current loss 0.040104, current_train_items 63104.
I0302 18:58:38.194323 22683591385216 run.py:483] Algo bellman_ford step 1972 current loss 0.037035, current_train_items 63136.
I0302 18:58:38.221887 22683591385216 run.py:483] Algo bellman_ford step 1973 current loss 0.075354, current_train_items 63168.
I0302 18:58:38.255349 22683591385216 run.py:483] Algo bellman_ford step 1974 current loss 0.141574, current_train_items 63200.
I0302 18:58:38.273325 22683591385216 run.py:483] Algo bellman_ford step 1975 current loss 0.002492, current_train_items 63232.
I0302 18:58:38.288634 22683591385216 run.py:483] Algo bellman_ford step 1976 current loss 0.047574, current_train_items 63264.
I0302 18:58:38.309354 22683591385216 run.py:483] Algo bellman_ford step 1977 current loss 0.046907, current_train_items 63296.
I0302 18:58:38.336203 22683591385216 run.py:483] Algo bellman_ford step 1978 current loss 0.064583, current_train_items 63328.
I0302 18:58:38.367168 22683591385216 run.py:483] Algo bellman_ford step 1979 current loss 0.100667, current_train_items 63360.
I0302 18:58:38.384724 22683591385216 run.py:483] Algo bellman_ford step 1980 current loss 0.002365, current_train_items 63392.
I0302 18:58:38.400229 22683591385216 run.py:483] Algo bellman_ford step 1981 current loss 0.011160, current_train_items 63424.
I0302 18:58:38.422079 22683591385216 run.py:483] Algo bellman_ford step 1982 current loss 0.063895, current_train_items 63456.
I0302 18:58:38.448554 22683591385216 run.py:483] Algo bellman_ford step 1983 current loss 0.043632, current_train_items 63488.
I0302 18:58:38.481688 22683591385216 run.py:483] Algo bellman_ford step 1984 current loss 0.127422, current_train_items 63520.
I0302 18:58:38.499721 22683591385216 run.py:483] Algo bellman_ford step 1985 current loss 0.002444, current_train_items 63552.
I0302 18:58:38.515343 22683591385216 run.py:483] Algo bellman_ford step 1986 current loss 0.106045, current_train_items 63584.
I0302 18:58:38.535923 22683591385216 run.py:483] Algo bellman_ford step 1987 current loss 0.059370, current_train_items 63616.
I0302 18:58:38.562700 22683591385216 run.py:483] Algo bellman_ford step 1988 current loss 0.070439, current_train_items 63648.
I0302 18:58:38.593818 22683591385216 run.py:483] Algo bellman_ford step 1989 current loss 0.094141, current_train_items 63680.
I0302 18:58:38.611845 22683591385216 run.py:483] Algo bellman_ford step 1990 current loss 0.012080, current_train_items 63712.
I0302 18:58:38.627279 22683591385216 run.py:483] Algo bellman_ford step 1991 current loss 0.030502, current_train_items 63744.
I0302 18:58:38.649153 22683591385216 run.py:483] Algo bellman_ford step 1992 current loss 0.043856, current_train_items 63776.
I0302 18:58:38.676910 22683591385216 run.py:483] Algo bellman_ford step 1993 current loss 0.087459, current_train_items 63808.
I0302 18:58:38.703177 22683591385216 run.py:483] Algo bellman_ford step 1994 current loss 0.062879, current_train_items 63840.
I0302 18:58:38.720860 22683591385216 run.py:483] Algo bellman_ford step 1995 current loss 0.012849, current_train_items 63872.
I0302 18:58:38.736624 22683591385216 run.py:483] Algo bellman_ford step 1996 current loss 0.017060, current_train_items 63904.
I0302 18:58:38.758062 22683591385216 run.py:483] Algo bellman_ford step 1997 current loss 0.032957, current_train_items 63936.
I0302 18:58:38.787107 22683591385216 run.py:483] Algo bellman_ford step 1998 current loss 0.066395, current_train_items 63968.
I0302 18:58:38.816596 22683591385216 run.py:483] Algo bellman_ford step 1999 current loss 0.152880, current_train_items 64000.
I0302 18:58:38.834368 22683591385216 run.py:483] Algo bellman_ford step 2000 current loss 0.002069, current_train_items 64032.
I0302 18:58:38.842037 22683591385216 run.py:503] (val) algo bellman_ford step 2000: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 64032, 'step': 2000, 'algorithm': 'bellman_ford'}
I0302 18:58:38.842140 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:38.857933 22683591385216 run.py:483] Algo bellman_ford step 2001 current loss 0.013822, current_train_items 64064.
I0302 18:58:38.881272 22683591385216 run.py:483] Algo bellman_ford step 2002 current loss 0.067830, current_train_items 64096.
I0302 18:58:38.909814 22683591385216 run.py:483] Algo bellman_ford step 2003 current loss 0.071606, current_train_items 64128.
I0302 18:58:38.940546 22683591385216 run.py:483] Algo bellman_ford step 2004 current loss 0.069701, current_train_items 64160.
I0302 18:58:38.958803 22683591385216 run.py:483] Algo bellman_ford step 2005 current loss 0.004597, current_train_items 64192.
I0302 18:58:38.974158 22683591385216 run.py:483] Algo bellman_ford step 2006 current loss 0.016646, current_train_items 64224.
I0302 18:58:38.996149 22683591385216 run.py:483] Algo bellman_ford step 2007 current loss 0.032265, current_train_items 64256.
I0302 18:58:39.025039 22683591385216 run.py:483] Algo bellman_ford step 2008 current loss 0.065607, current_train_items 64288.
I0302 18:58:39.055191 22683591385216 run.py:483] Algo bellman_ford step 2009 current loss 0.063369, current_train_items 64320.
I0302 18:58:39.073371 22683591385216 run.py:483] Algo bellman_ford step 2010 current loss 0.003112, current_train_items 64352.
I0302 18:58:39.088892 22683591385216 run.py:483] Algo bellman_ford step 2011 current loss 0.022277, current_train_items 64384.
I0302 18:58:39.111549 22683591385216 run.py:483] Algo bellman_ford step 2012 current loss 0.052152, current_train_items 64416.
I0302 18:58:39.138864 22683591385216 run.py:483] Algo bellman_ford step 2013 current loss 0.080564, current_train_items 64448.
I0302 18:58:39.169380 22683591385216 run.py:483] Algo bellman_ford step 2014 current loss 0.069067, current_train_items 64480.
I0302 18:58:39.187330 22683591385216 run.py:483] Algo bellman_ford step 2015 current loss 0.012177, current_train_items 64512.
I0302 18:58:39.203216 22683591385216 run.py:483] Algo bellman_ford step 2016 current loss 0.027720, current_train_items 64544.
I0302 18:58:39.225991 22683591385216 run.py:483] Algo bellman_ford step 2017 current loss 0.105704, current_train_items 64576.
I0302 18:58:39.252050 22683591385216 run.py:483] Algo bellman_ford step 2018 current loss 0.060696, current_train_items 64608.
I0302 18:58:39.283928 22683591385216 run.py:483] Algo bellman_ford step 2019 current loss 0.106068, current_train_items 64640.
I0302 18:58:39.301839 22683591385216 run.py:483] Algo bellman_ford step 2020 current loss 0.003496, current_train_items 64672.
I0302 18:58:39.317111 22683591385216 run.py:483] Algo bellman_ford step 2021 current loss 0.012624, current_train_items 64704.
I0302 18:58:39.340030 22683591385216 run.py:483] Algo bellman_ford step 2022 current loss 0.105488, current_train_items 64736.
I0302 18:58:39.367774 22683591385216 run.py:483] Algo bellman_ford step 2023 current loss 0.063565, current_train_items 64768.
I0302 18:58:39.401065 22683591385216 run.py:483] Algo bellman_ford step 2024 current loss 0.085355, current_train_items 64800.
I0302 18:58:39.419283 22683591385216 run.py:483] Algo bellman_ford step 2025 current loss 0.008694, current_train_items 64832.
I0302 18:58:39.434407 22683591385216 run.py:483] Algo bellman_ford step 2026 current loss 0.033597, current_train_items 64864.
I0302 18:58:39.457035 22683591385216 run.py:483] Algo bellman_ford step 2027 current loss 0.065423, current_train_items 64896.
I0302 18:58:39.485079 22683591385216 run.py:483] Algo bellman_ford step 2028 current loss 0.044643, current_train_items 64928.
I0302 18:58:39.515215 22683591385216 run.py:483] Algo bellman_ford step 2029 current loss 0.090919, current_train_items 64960.
I0302 18:58:39.533446 22683591385216 run.py:483] Algo bellman_ford step 2030 current loss 0.009963, current_train_items 64992.
I0302 18:58:39.549158 22683591385216 run.py:483] Algo bellman_ford step 2031 current loss 0.053878, current_train_items 65024.
I0302 18:58:39.570632 22683591385216 run.py:483] Algo bellman_ford step 2032 current loss 0.036750, current_train_items 65056.
I0302 18:58:39.599471 22683591385216 run.py:483] Algo bellman_ford step 2033 current loss 0.089097, current_train_items 65088.
I0302 18:58:39.629096 22683591385216 run.py:483] Algo bellman_ford step 2034 current loss 0.062606, current_train_items 65120.
I0302 18:58:39.646834 22683591385216 run.py:483] Algo bellman_ford step 2035 current loss 0.003662, current_train_items 65152.
I0302 18:58:39.661973 22683591385216 run.py:483] Algo bellman_ford step 2036 current loss 0.009646, current_train_items 65184.
I0302 18:58:39.683653 22683591385216 run.py:483] Algo bellman_ford step 2037 current loss 0.069576, current_train_items 65216.
I0302 18:58:39.711709 22683591385216 run.py:483] Algo bellman_ford step 2038 current loss 0.097863, current_train_items 65248.
I0302 18:58:39.742378 22683591385216 run.py:483] Algo bellman_ford step 2039 current loss 0.091232, current_train_items 65280.
I0302 18:58:39.760329 22683591385216 run.py:483] Algo bellman_ford step 2040 current loss 0.017399, current_train_items 65312.
I0302 18:58:39.775492 22683591385216 run.py:483] Algo bellman_ford step 2041 current loss 0.041343, current_train_items 65344.
I0302 18:58:39.798575 22683591385216 run.py:483] Algo bellman_ford step 2042 current loss 0.113868, current_train_items 65376.
I0302 18:58:39.827013 22683591385216 run.py:483] Algo bellman_ford step 2043 current loss 0.080089, current_train_items 65408.
I0302 18:58:39.852816 22683591385216 run.py:483] Algo bellman_ford step 2044 current loss 0.057202, current_train_items 65440.
I0302 18:58:39.870982 22683591385216 run.py:483] Algo bellman_ford step 2045 current loss 0.012412, current_train_items 65472.
I0302 18:58:39.886907 22683591385216 run.py:483] Algo bellman_ford step 2046 current loss 0.048051, current_train_items 65504.
I0302 18:58:39.908954 22683591385216 run.py:483] Algo bellman_ford step 2047 current loss 0.108604, current_train_items 65536.
I0302 18:58:39.936661 22683591385216 run.py:483] Algo bellman_ford step 2048 current loss 0.118496, current_train_items 65568.
I0302 18:58:39.965055 22683591385216 run.py:483] Algo bellman_ford step 2049 current loss 0.073771, current_train_items 65600.
I0302 18:58:39.983262 22683591385216 run.py:483] Algo bellman_ford step 2050 current loss 0.013004, current_train_items 65632.
I0302 18:58:39.991293 22683591385216 run.py:503] (val) algo bellman_ford step 2050: {'pi': 0.966796875, 'score': 0.966796875, 'examples_seen': 65632, 'step': 2050, 'algorithm': 'bellman_ford'}
I0302 18:58:39.991420 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.988, current avg val score is 0.967, val scores are: bellman_ford: 0.967
I0302 18:58:40.007881 22683591385216 run.py:483] Algo bellman_ford step 2051 current loss 0.043838, current_train_items 65664.
I0302 18:58:40.030132 22683591385216 run.py:483] Algo bellman_ford step 2052 current loss 0.151663, current_train_items 65696.
I0302 18:58:40.057577 22683591385216 run.py:483] Algo bellman_ford step 2053 current loss 0.200235, current_train_items 65728.
I0302 18:58:40.091342 22683591385216 run.py:483] Algo bellman_ford step 2054 current loss 0.294604, current_train_items 65760.
I0302 18:58:40.109419 22683591385216 run.py:483] Algo bellman_ford step 2055 current loss 0.003736, current_train_items 65792.
I0302 18:58:40.124557 22683591385216 run.py:483] Algo bellman_ford step 2056 current loss 0.014298, current_train_items 65824.
I0302 18:58:40.145985 22683591385216 run.py:483] Algo bellman_ford step 2057 current loss 0.069534, current_train_items 65856.
I0302 18:58:40.173454 22683591385216 run.py:483] Algo bellman_ford step 2058 current loss 0.084837, current_train_items 65888.
I0302 18:58:40.204146 22683591385216 run.py:483] Algo bellman_ford step 2059 current loss 0.178099, current_train_items 65920.
I0302 18:58:40.222521 22683591385216 run.py:483] Algo bellman_ford step 2060 current loss 0.006881, current_train_items 65952.
I0302 18:58:40.238207 22683591385216 run.py:483] Algo bellman_ford step 2061 current loss 0.028775, current_train_items 65984.
I0302 18:58:40.259981 22683591385216 run.py:483] Algo bellman_ford step 2062 current loss 0.066344, current_train_items 66016.
I0302 18:58:40.288173 22683591385216 run.py:483] Algo bellman_ford step 2063 current loss 0.087366, current_train_items 66048.
I0302 18:58:40.320298 22683591385216 run.py:483] Algo bellman_ford step 2064 current loss 0.083042, current_train_items 66080.
I0302 18:58:40.337734 22683591385216 run.py:483] Algo bellman_ford step 2065 current loss 0.003063, current_train_items 66112.
I0302 18:58:40.353033 22683591385216 run.py:483] Algo bellman_ford step 2066 current loss 0.023073, current_train_items 66144.
I0302 18:58:40.375017 22683591385216 run.py:483] Algo bellman_ford step 2067 current loss 0.088090, current_train_items 66176.
I0302 18:58:40.403359 22683591385216 run.py:483] Algo bellman_ford step 2068 current loss 0.154790, current_train_items 66208.
I0302 18:58:40.433693 22683591385216 run.py:483] Algo bellman_ford step 2069 current loss 0.130736, current_train_items 66240.
I0302 18:58:40.451815 22683591385216 run.py:483] Algo bellman_ford step 2070 current loss 0.001592, current_train_items 66272.
I0302 18:58:40.467697 22683591385216 run.py:483] Algo bellman_ford step 2071 current loss 0.019780, current_train_items 66304.
I0302 18:58:40.489680 22683591385216 run.py:483] Algo bellman_ford step 2072 current loss 0.127192, current_train_items 66336.
I0302 18:58:40.518336 22683591385216 run.py:483] Algo bellman_ford step 2073 current loss 0.238047, current_train_items 66368.
I0302 18:58:40.548879 22683591385216 run.py:483] Algo bellman_ford step 2074 current loss 0.208352, current_train_items 66400.
I0302 18:58:40.566638 22683591385216 run.py:483] Algo bellman_ford step 2075 current loss 0.006359, current_train_items 66432.
I0302 18:58:40.582699 22683591385216 run.py:483] Algo bellman_ford step 2076 current loss 0.031846, current_train_items 66464.
I0302 18:58:40.604923 22683591385216 run.py:483] Algo bellman_ford step 2077 current loss 0.078619, current_train_items 66496.
I0302 18:58:40.631141 22683591385216 run.py:483] Algo bellman_ford step 2078 current loss 0.065037, current_train_items 66528.
I0302 18:58:40.660841 22683591385216 run.py:483] Algo bellman_ford step 2079 current loss 0.097516, current_train_items 66560.
I0302 18:58:40.678370 22683591385216 run.py:483] Algo bellman_ford step 2080 current loss 0.003678, current_train_items 66592.
I0302 18:58:40.693643 22683591385216 run.py:483] Algo bellman_ford step 2081 current loss 0.026031, current_train_items 66624.
I0302 18:58:40.715390 22683591385216 run.py:483] Algo bellman_ford step 2082 current loss 0.070750, current_train_items 66656.
I0302 18:58:40.744807 22683591385216 run.py:483] Algo bellman_ford step 2083 current loss 0.117514, current_train_items 66688.
I0302 18:58:40.775757 22683591385216 run.py:483] Algo bellman_ford step 2084 current loss 0.113149, current_train_items 66720.
I0302 18:58:40.793962 22683591385216 run.py:483] Algo bellman_ford step 2085 current loss 0.004908, current_train_items 66752.
I0302 18:58:40.809494 22683591385216 run.py:483] Algo bellman_ford step 2086 current loss 0.047173, current_train_items 66784.
I0302 18:58:40.831455 22683591385216 run.py:483] Algo bellman_ford step 2087 current loss 0.106160, current_train_items 66816.
I0302 18:58:40.857190 22683591385216 run.py:483] Algo bellman_ford step 2088 current loss 0.093284, current_train_items 66848.
I0302 18:58:40.887637 22683591385216 run.py:483] Algo bellman_ford step 2089 current loss 0.129067, current_train_items 66880.
I0302 18:58:40.905736 22683591385216 run.py:483] Algo bellman_ford step 2090 current loss 0.002769, current_train_items 66912.
I0302 18:58:40.921327 22683591385216 run.py:483] Algo bellman_ford step 2091 current loss 0.025656, current_train_items 66944.
I0302 18:58:40.942826 22683591385216 run.py:483] Algo bellman_ford step 2092 current loss 0.066646, current_train_items 66976.
I0302 18:58:40.970313 22683591385216 run.py:483] Algo bellman_ford step 2093 current loss 0.077303, current_train_items 67008.
I0302 18:58:41.001075 22683591385216 run.py:483] Algo bellman_ford step 2094 current loss 0.203824, current_train_items 67040.
I0302 18:58:41.018836 22683591385216 run.py:483] Algo bellman_ford step 2095 current loss 0.020291, current_train_items 67072.
I0302 18:58:41.034208 22683591385216 run.py:483] Algo bellman_ford step 2096 current loss 0.017770, current_train_items 67104.
I0302 18:58:41.055706 22683591385216 run.py:483] Algo bellman_ford step 2097 current loss 0.052763, current_train_items 67136.
I0302 18:58:41.082558 22683591385216 run.py:483] Algo bellman_ford step 2098 current loss 0.063567, current_train_items 67168.
I0302 18:58:41.112370 22683591385216 run.py:483] Algo bellman_ford step 2099 current loss 0.109630, current_train_items 67200.
I0302 18:58:41.130666 22683591385216 run.py:483] Algo bellman_ford step 2100 current loss 0.005751, current_train_items 67232.
I0302 18:58:41.138395 22683591385216 run.py:503] (val) algo bellman_ford step 2100: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 67232, 'step': 2100, 'algorithm': 'bellman_ford'}
I0302 18:58:41.138499 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.988, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 18:58:41.167193 22683591385216 run.py:483] Algo bellman_ford step 2101 current loss 0.020056, current_train_items 67264.
I0302 18:58:41.189185 22683591385216 run.py:483] Algo bellman_ford step 2102 current loss 0.088032, current_train_items 67296.
I0302 18:58:41.217340 22683591385216 run.py:483] Algo bellman_ford step 2103 current loss 0.061375, current_train_items 67328.
I0302 18:58:41.247760 22683591385216 run.py:483] Algo bellman_ford step 2104 current loss 0.073571, current_train_items 67360.
I0302 18:58:41.266056 22683591385216 run.py:483] Algo bellman_ford step 2105 current loss 0.003901, current_train_items 67392.
I0302 18:58:41.280661 22683591385216 run.py:483] Algo bellman_ford step 2106 current loss 0.011897, current_train_items 67424.
I0302 18:58:41.303148 22683591385216 run.py:483] Algo bellman_ford step 2107 current loss 0.070630, current_train_items 67456.
I0302 18:58:41.332447 22683591385216 run.py:483] Algo bellman_ford step 2108 current loss 0.136570, current_train_items 67488.
I0302 18:58:41.361320 22683591385216 run.py:483] Algo bellman_ford step 2109 current loss 0.072498, current_train_items 67520.
I0302 18:58:41.379088 22683591385216 run.py:483] Algo bellman_ford step 2110 current loss 0.012134, current_train_items 67552.
I0302 18:58:41.394374 22683591385216 run.py:483] Algo bellman_ford step 2111 current loss 0.016873, current_train_items 67584.
I0302 18:58:41.415812 22683591385216 run.py:483] Algo bellman_ford step 2112 current loss 0.060676, current_train_items 67616.
I0302 18:58:41.442593 22683591385216 run.py:483] Algo bellman_ford step 2113 current loss 0.074688, current_train_items 67648.
I0302 18:58:41.471667 22683591385216 run.py:483] Algo bellman_ford step 2114 current loss 0.082081, current_train_items 67680.
I0302 18:58:41.489495 22683591385216 run.py:483] Algo bellman_ford step 2115 current loss 0.014695, current_train_items 67712.
I0302 18:58:41.504719 22683591385216 run.py:483] Algo bellman_ford step 2116 current loss 0.061405, current_train_items 67744.
I0302 18:58:41.527288 22683591385216 run.py:483] Algo bellman_ford step 2117 current loss 0.074200, current_train_items 67776.
I0302 18:58:41.555560 22683591385216 run.py:483] Algo bellman_ford step 2118 current loss 0.098085, current_train_items 67808.
I0302 18:58:41.584875 22683591385216 run.py:483] Algo bellman_ford step 2119 current loss 0.068538, current_train_items 67840.
I0302 18:58:41.602607 22683591385216 run.py:483] Algo bellman_ford step 2120 current loss 0.002794, current_train_items 67872.
I0302 18:58:41.617654 22683591385216 run.py:483] Algo bellman_ford step 2121 current loss 0.023852, current_train_items 67904.
I0302 18:58:41.639401 22683591385216 run.py:483] Algo bellman_ford step 2122 current loss 0.056581, current_train_items 67936.
I0302 18:58:41.667623 22683591385216 run.py:483] Algo bellman_ford step 2123 current loss 0.060880, current_train_items 67968.
I0302 18:58:41.694340 22683591385216 run.py:483] Algo bellman_ford step 2124 current loss 0.082115, current_train_items 68000.
I0302 18:58:41.711813 22683591385216 run.py:483] Algo bellman_ford step 2125 current loss 0.003744, current_train_items 68032.
I0302 18:58:41.727485 22683591385216 run.py:483] Algo bellman_ford step 2126 current loss 0.091483, current_train_items 68064.
I0302 18:58:41.750802 22683591385216 run.py:483] Algo bellman_ford step 2127 current loss 0.046468, current_train_items 68096.
I0302 18:58:41.777890 22683591385216 run.py:483] Algo bellman_ford step 2128 current loss 0.107909, current_train_items 68128.
I0302 18:58:41.807362 22683591385216 run.py:483] Algo bellman_ford step 2129 current loss 0.087499, current_train_items 68160.
I0302 18:58:41.825047 22683591385216 run.py:483] Algo bellman_ford step 2130 current loss 0.003946, current_train_items 68192.
I0302 18:58:41.840886 22683591385216 run.py:483] Algo bellman_ford step 2131 current loss 0.057657, current_train_items 68224.
I0302 18:58:41.863431 22683591385216 run.py:483] Algo bellman_ford step 2132 current loss 0.133979, current_train_items 68256.
I0302 18:58:41.890611 22683591385216 run.py:483] Algo bellman_ford step 2133 current loss 0.138740, current_train_items 68288.
I0302 18:58:41.922064 22683591385216 run.py:483] Algo bellman_ford step 2134 current loss 0.094320, current_train_items 68320.
I0302 18:58:41.939638 22683591385216 run.py:483] Algo bellman_ford step 2135 current loss 0.013413, current_train_items 68352.
I0302 18:58:41.955124 22683591385216 run.py:483] Algo bellman_ford step 2136 current loss 0.089291, current_train_items 68384.
I0302 18:58:41.976805 22683591385216 run.py:483] Algo bellman_ford step 2137 current loss 0.094874, current_train_items 68416.
I0302 18:58:42.004632 22683591385216 run.py:483] Algo bellman_ford step 2138 current loss 0.176064, current_train_items 68448.
I0302 18:58:42.037712 22683591385216 run.py:483] Algo bellman_ford step 2139 current loss 0.147858, current_train_items 68480.
I0302 18:58:42.055259 22683591385216 run.py:483] Algo bellman_ford step 2140 current loss 0.069223, current_train_items 68512.
I0302 18:58:42.070444 22683591385216 run.py:483] Algo bellman_ford step 2141 current loss 0.031826, current_train_items 68544.
I0302 18:58:42.090782 22683591385216 run.py:483] Algo bellman_ford step 2142 current loss 0.040086, current_train_items 68576.
I0302 18:58:42.118162 22683591385216 run.py:483] Algo bellman_ford step 2143 current loss 0.128732, current_train_items 68608.
I0302 18:58:42.148263 22683591385216 run.py:483] Algo bellman_ford step 2144 current loss 0.235764, current_train_items 68640.
I0302 18:58:42.165817 22683591385216 run.py:483] Algo bellman_ford step 2145 current loss 0.005424, current_train_items 68672.
I0302 18:58:42.181473 22683591385216 run.py:483] Algo bellman_ford step 2146 current loss 0.063233, current_train_items 68704.
I0302 18:58:42.203750 22683591385216 run.py:483] Algo bellman_ford step 2147 current loss 0.097854, current_train_items 68736.
I0302 18:58:42.231796 22683591385216 run.py:483] Algo bellman_ford step 2148 current loss 0.102585, current_train_items 68768.
I0302 18:58:42.261023 22683591385216 run.py:483] Algo bellman_ford step 2149 current loss 0.111290, current_train_items 68800.
I0302 18:58:42.278546 22683591385216 run.py:483] Algo bellman_ford step 2150 current loss 0.013184, current_train_items 68832.
I0302 18:58:42.286617 22683591385216 run.py:503] (val) algo bellman_ford step 2150: {'pi': 0.9677734375, 'score': 0.9677734375, 'examples_seen': 68832, 'step': 2150, 'algorithm': 'bellman_ford'}
I0302 18:58:42.286723 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.968, val scores are: bellman_ford: 0.968
I0302 18:58:42.302741 22683591385216 run.py:483] Algo bellman_ford step 2151 current loss 0.020850, current_train_items 68864.
I0302 18:58:42.325300 22683591385216 run.py:483] Algo bellman_ford step 2152 current loss 0.129750, current_train_items 68896.
I0302 18:58:42.352918 22683591385216 run.py:483] Algo bellman_ford step 2153 current loss 0.086312, current_train_items 68928.
I0302 18:58:42.383178 22683591385216 run.py:483] Algo bellman_ford step 2154 current loss 0.135117, current_train_items 68960.
I0302 18:58:42.401458 22683591385216 run.py:483] Algo bellman_ford step 2155 current loss 0.009463, current_train_items 68992.
I0302 18:58:42.416710 22683591385216 run.py:483] Algo bellman_ford step 2156 current loss 0.073875, current_train_items 69024.
I0302 18:58:42.439055 22683591385216 run.py:483] Algo bellman_ford step 2157 current loss 0.095154, current_train_items 69056.
I0302 18:58:42.467228 22683591385216 run.py:483] Algo bellman_ford step 2158 current loss 0.083785, current_train_items 69088.
I0302 18:58:42.498277 22683591385216 run.py:483] Algo bellman_ford step 2159 current loss 0.159165, current_train_items 69120.
I0302 18:58:42.516760 22683591385216 run.py:483] Algo bellman_ford step 2160 current loss 0.017943, current_train_items 69152.
I0302 18:58:42.532290 22683591385216 run.py:483] Algo bellman_ford step 2161 current loss 0.021887, current_train_items 69184.
I0302 18:58:42.554318 22683591385216 run.py:483] Algo bellman_ford step 2162 current loss 0.217240, current_train_items 69216.
I0302 18:58:42.580860 22683591385216 run.py:483] Algo bellman_ford step 2163 current loss 0.232732, current_train_items 69248.
I0302 18:58:42.612072 22683591385216 run.py:483] Algo bellman_ford step 2164 current loss 0.234101, current_train_items 69280.
I0302 18:58:42.630016 22683591385216 run.py:483] Algo bellman_ford step 2165 current loss 0.021858, current_train_items 69312.
I0302 18:58:42.645806 22683591385216 run.py:483] Algo bellman_ford step 2166 current loss 0.054192, current_train_items 69344.
I0302 18:58:42.667341 22683591385216 run.py:483] Algo bellman_ford step 2167 current loss 0.052091, current_train_items 69376.
I0302 18:58:42.694081 22683591385216 run.py:483] Algo bellman_ford step 2168 current loss 0.115636, current_train_items 69408.
I0302 18:58:42.724459 22683591385216 run.py:483] Algo bellman_ford step 2169 current loss 0.216267, current_train_items 69440.
I0302 18:58:42.742671 22683591385216 run.py:483] Algo bellman_ford step 2170 current loss 0.035797, current_train_items 69472.
I0302 18:58:42.758159 22683591385216 run.py:483] Algo bellman_ford step 2171 current loss 0.027292, current_train_items 69504.
I0302 18:58:42.779934 22683591385216 run.py:483] Algo bellman_ford step 2172 current loss 0.090602, current_train_items 69536.
I0302 18:58:42.808510 22683591385216 run.py:483] Algo bellman_ford step 2173 current loss 0.081441, current_train_items 69568.
I0302 18:58:42.837105 22683591385216 run.py:483] Algo bellman_ford step 2174 current loss 0.115135, current_train_items 69600.
I0302 18:58:42.855149 22683591385216 run.py:483] Algo bellman_ford step 2175 current loss 0.005534, current_train_items 69632.
I0302 18:58:42.870254 22683591385216 run.py:483] Algo bellman_ford step 2176 current loss 0.034603, current_train_items 69664.
I0302 18:58:42.892961 22683591385216 run.py:483] Algo bellman_ford step 2177 current loss 0.085173, current_train_items 69696.
I0302 18:58:42.919974 22683591385216 run.py:483] Algo bellman_ford step 2178 current loss 0.094455, current_train_items 69728.
I0302 18:58:42.951246 22683591385216 run.py:483] Algo bellman_ford step 2179 current loss 0.072685, current_train_items 69760.
I0302 18:58:42.968671 22683591385216 run.py:483] Algo bellman_ford step 2180 current loss 0.016742, current_train_items 69792.
I0302 18:58:42.984119 22683591385216 run.py:483] Algo bellman_ford step 2181 current loss 0.042262, current_train_items 69824.
I0302 18:58:43.006166 22683591385216 run.py:483] Algo bellman_ford step 2182 current loss 0.126976, current_train_items 69856.
I0302 18:58:43.033347 22683591385216 run.py:483] Algo bellman_ford step 2183 current loss 0.143219, current_train_items 69888.
I0302 18:58:43.065353 22683591385216 run.py:483] Algo bellman_ford step 2184 current loss 0.220740, current_train_items 69920.
I0302 18:58:43.083780 22683591385216 run.py:483] Algo bellman_ford step 2185 current loss 0.016438, current_train_items 69952.
I0302 18:58:43.098821 22683591385216 run.py:483] Algo bellman_ford step 2186 current loss 0.026155, current_train_items 69984.
I0302 18:58:43.120672 22683591385216 run.py:483] Algo bellman_ford step 2187 current loss 0.149724, current_train_items 70016.
I0302 18:58:43.148773 22683591385216 run.py:483] Algo bellman_ford step 2188 current loss 0.162931, current_train_items 70048.
W0302 18:58:43.172567 22683591385216 samplers.py:155] Increasing hint lengh from 12 to 13
I0302 18:58:48.520723 22683591385216 run.py:483] Algo bellman_ford step 2189 current loss 0.184893, current_train_items 70080.
I0302 18:58:48.539890 22683591385216 run.py:483] Algo bellman_ford step 2190 current loss 0.004617, current_train_items 70112.
I0302 18:58:48.555957 22683591385216 run.py:483] Algo bellman_ford step 2191 current loss 0.034850, current_train_items 70144.
I0302 18:58:48.578318 22683591385216 run.py:483] Algo bellman_ford step 2192 current loss 0.069153, current_train_items 70176.
W0302 18:58:48.599320 22683591385216 samplers.py:155] Increasing hint lengh from 10 to 12
I0302 18:58:54.308949 22683591385216 run.py:483] Algo bellman_ford step 2193 current loss 0.123031, current_train_items 70208.
I0302 18:58:54.338607 22683591385216 run.py:483] Algo bellman_ford step 2194 current loss 0.127751, current_train_items 70240.
I0302 18:58:54.357199 22683591385216 run.py:483] Algo bellman_ford step 2195 current loss 0.047919, current_train_items 70272.
I0302 18:58:54.372807 22683591385216 run.py:483] Algo bellman_ford step 2196 current loss 0.027046, current_train_items 70304.
I0302 18:58:54.394531 22683591385216 run.py:483] Algo bellman_ford step 2197 current loss 0.060546, current_train_items 70336.
I0302 18:58:54.421428 22683591385216 run.py:483] Algo bellman_ford step 2198 current loss 0.087626, current_train_items 70368.
I0302 18:58:54.451991 22683591385216 run.py:483] Algo bellman_ford step 2199 current loss 0.115617, current_train_items 70400.
I0302 18:58:54.470360 22683591385216 run.py:483] Algo bellman_ford step 2200 current loss 0.016831, current_train_items 70432.
I0302 18:58:54.479470 22683591385216 run.py:503] (val) algo bellman_ford step 2200: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 70432, 'step': 2200, 'algorithm': 'bellman_ford'}
I0302 18:58:54.479575 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:58:54.495895 22683591385216 run.py:483] Algo bellman_ford step 2201 current loss 0.050171, current_train_items 70464.
I0302 18:58:54.517923 22683591385216 run.py:483] Algo bellman_ford step 2202 current loss 0.054210, current_train_items 70496.
I0302 18:58:54.546485 22683591385216 run.py:483] Algo bellman_ford step 2203 current loss 0.099365, current_train_items 70528.
I0302 18:58:54.577914 22683591385216 run.py:483] Algo bellman_ford step 2204 current loss 0.074073, current_train_items 70560.
I0302 18:58:54.596849 22683591385216 run.py:483] Algo bellman_ford step 2205 current loss 0.006964, current_train_items 70592.
I0302 18:58:54.612560 22683591385216 run.py:483] Algo bellman_ford step 2206 current loss 0.044976, current_train_items 70624.
I0302 18:58:54.634652 22683591385216 run.py:483] Algo bellman_ford step 2207 current loss 0.101534, current_train_items 70656.
I0302 18:58:54.663255 22683591385216 run.py:483] Algo bellman_ford step 2208 current loss 0.262479, current_train_items 70688.
I0302 18:58:54.695128 22683591385216 run.py:483] Algo bellman_ford step 2209 current loss 0.165578, current_train_items 70720.
I0302 18:58:54.713584 22683591385216 run.py:483] Algo bellman_ford step 2210 current loss 0.010792, current_train_items 70752.
I0302 18:58:54.729143 22683591385216 run.py:483] Algo bellman_ford step 2211 current loss 0.024592, current_train_items 70784.
I0302 18:58:54.751705 22683591385216 run.py:483] Algo bellman_ford step 2212 current loss 0.093010, current_train_items 70816.
I0302 18:58:54.780884 22683591385216 run.py:483] Algo bellman_ford step 2213 current loss 0.218350, current_train_items 70848.
I0302 18:58:54.813085 22683591385216 run.py:483] Algo bellman_ford step 2214 current loss 0.181863, current_train_items 70880.
I0302 18:58:54.831925 22683591385216 run.py:483] Algo bellman_ford step 2215 current loss 0.010746, current_train_items 70912.
I0302 18:58:54.847268 22683591385216 run.py:483] Algo bellman_ford step 2216 current loss 0.009545, current_train_items 70944.
I0302 18:58:54.868191 22683591385216 run.py:483] Algo bellman_ford step 2217 current loss 0.036780, current_train_items 70976.
I0302 18:58:54.897629 22683591385216 run.py:483] Algo bellman_ford step 2218 current loss 0.127090, current_train_items 71008.
I0302 18:58:54.927377 22683591385216 run.py:483] Algo bellman_ford step 2219 current loss 0.116003, current_train_items 71040.
I0302 18:58:54.945735 22683591385216 run.py:483] Algo bellman_ford step 2220 current loss 0.008072, current_train_items 71072.
I0302 18:58:54.961087 22683591385216 run.py:483] Algo bellman_ford step 2221 current loss 0.017296, current_train_items 71104.
I0302 18:58:54.982999 22683591385216 run.py:483] Algo bellman_ford step 2222 current loss 0.068348, current_train_items 71136.
I0302 18:58:55.011677 22683591385216 run.py:483] Algo bellman_ford step 2223 current loss 0.056398, current_train_items 71168.
I0302 18:58:55.043643 22683591385216 run.py:483] Algo bellman_ford step 2224 current loss 0.119586, current_train_items 71200.
I0302 18:58:55.062019 22683591385216 run.py:483] Algo bellman_ford step 2225 current loss 0.002889, current_train_items 71232.
I0302 18:58:55.077529 22683591385216 run.py:483] Algo bellman_ford step 2226 current loss 0.019254, current_train_items 71264.
I0302 18:58:55.099945 22683591385216 run.py:483] Algo bellman_ford step 2227 current loss 0.054704, current_train_items 71296.
I0302 18:58:55.128573 22683591385216 run.py:483] Algo bellman_ford step 2228 current loss 0.116144, current_train_items 71328.
I0302 18:58:55.157374 22683591385216 run.py:483] Algo bellman_ford step 2229 current loss 0.095613, current_train_items 71360.
I0302 18:58:55.176045 22683591385216 run.py:483] Algo bellman_ford step 2230 current loss 0.003883, current_train_items 71392.
I0302 18:58:55.191674 22683591385216 run.py:483] Algo bellman_ford step 2231 current loss 0.060045, current_train_items 71424.
I0302 18:58:55.213761 22683591385216 run.py:483] Algo bellman_ford step 2232 current loss 0.139176, current_train_items 71456.
I0302 18:58:55.242038 22683591385216 run.py:483] Algo bellman_ford step 2233 current loss 0.096390, current_train_items 71488.
I0302 18:58:55.272447 22683591385216 run.py:483] Algo bellman_ford step 2234 current loss 0.079058, current_train_items 71520.
I0302 18:58:55.291031 22683591385216 run.py:483] Algo bellman_ford step 2235 current loss 0.004395, current_train_items 71552.
I0302 18:58:55.306700 22683591385216 run.py:483] Algo bellman_ford step 2236 current loss 0.109541, current_train_items 71584.
I0302 18:58:55.329821 22683591385216 run.py:483] Algo bellman_ford step 2237 current loss 0.114658, current_train_items 71616.
I0302 18:58:55.358880 22683591385216 run.py:483] Algo bellman_ford step 2238 current loss 0.076420, current_train_items 71648.
I0302 18:58:55.388907 22683591385216 run.py:483] Algo bellman_ford step 2239 current loss 0.186245, current_train_items 71680.
I0302 18:58:55.407246 22683591385216 run.py:483] Algo bellman_ford step 2240 current loss 0.014038, current_train_items 71712.
I0302 18:58:55.423117 22683591385216 run.py:483] Algo bellman_ford step 2241 current loss 0.030879, current_train_items 71744.
I0302 18:58:55.446429 22683591385216 run.py:483] Algo bellman_ford step 2242 current loss 0.115079, current_train_items 71776.
I0302 18:58:55.475342 22683591385216 run.py:483] Algo bellman_ford step 2243 current loss 0.176719, current_train_items 71808.
I0302 18:58:55.504621 22683591385216 run.py:483] Algo bellman_ford step 2244 current loss 0.153378, current_train_items 71840.
I0302 18:58:55.522783 22683591385216 run.py:483] Algo bellman_ford step 2245 current loss 0.021514, current_train_items 71872.
I0302 18:58:55.538421 22683591385216 run.py:483] Algo bellman_ford step 2246 current loss 0.056655, current_train_items 71904.
I0302 18:58:55.559970 22683591385216 run.py:483] Algo bellman_ford step 2247 current loss 0.063428, current_train_items 71936.
I0302 18:58:55.589595 22683591385216 run.py:483] Algo bellman_ford step 2248 current loss 0.067000, current_train_items 71968.
I0302 18:58:55.620995 22683591385216 run.py:483] Algo bellman_ford step 2249 current loss 0.088614, current_train_items 72000.
I0302 18:58:55.639205 22683591385216 run.py:483] Algo bellman_ford step 2250 current loss 0.009402, current_train_items 72032.
I0302 18:58:55.647559 22683591385216 run.py:503] (val) algo bellman_ford step 2250: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 72032, 'step': 2250, 'algorithm': 'bellman_ford'}
I0302 18:58:55.647666 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.989, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:58:55.663585 22683591385216 run.py:483] Algo bellman_ford step 2251 current loss 0.047304, current_train_items 72064.
I0302 18:58:55.685658 22683591385216 run.py:483] Algo bellman_ford step 2252 current loss 0.029827, current_train_items 72096.
I0302 18:58:55.715921 22683591385216 run.py:483] Algo bellman_ford step 2253 current loss 0.121934, current_train_items 72128.
I0302 18:58:55.748190 22683591385216 run.py:483] Algo bellman_ford step 2254 current loss 0.072518, current_train_items 72160.
I0302 18:58:55.767237 22683591385216 run.py:483] Algo bellman_ford step 2255 current loss 0.005423, current_train_items 72192.
I0302 18:58:55.782773 22683591385216 run.py:483] Algo bellman_ford step 2256 current loss 0.023482, current_train_items 72224.
I0302 18:58:55.805190 22683591385216 run.py:483] Algo bellman_ford step 2257 current loss 0.043654, current_train_items 72256.
I0302 18:58:55.832358 22683591385216 run.py:483] Algo bellman_ford step 2258 current loss 0.101751, current_train_items 72288.
I0302 18:58:55.864307 22683591385216 run.py:483] Algo bellman_ford step 2259 current loss 0.123215, current_train_items 72320.
I0302 18:58:55.882976 22683591385216 run.py:483] Algo bellman_ford step 2260 current loss 0.014485, current_train_items 72352.
I0302 18:58:55.899226 22683591385216 run.py:483] Algo bellman_ford step 2261 current loss 0.021571, current_train_items 72384.
I0302 18:58:55.922703 22683591385216 run.py:483] Algo bellman_ford step 2262 current loss 0.069569, current_train_items 72416.
I0302 18:58:55.950390 22683591385216 run.py:483] Algo bellman_ford step 2263 current loss 0.050888, current_train_items 72448.
I0302 18:58:55.981458 22683591385216 run.py:483] Algo bellman_ford step 2264 current loss 0.134322, current_train_items 72480.
I0302 18:58:55.999756 22683591385216 run.py:483] Algo bellman_ford step 2265 current loss 0.028802, current_train_items 72512.
I0302 18:58:56.015133 22683591385216 run.py:483] Algo bellman_ford step 2266 current loss 0.046920, current_train_items 72544.
I0302 18:58:56.038245 22683591385216 run.py:483] Algo bellman_ford step 2267 current loss 0.059532, current_train_items 72576.
I0302 18:58:56.066886 22683591385216 run.py:483] Algo bellman_ford step 2268 current loss 0.164927, current_train_items 72608.
I0302 18:58:56.097112 22683591385216 run.py:483] Algo bellman_ford step 2269 current loss 0.099734, current_train_items 72640.
I0302 18:58:56.115968 22683591385216 run.py:483] Algo bellman_ford step 2270 current loss 0.006537, current_train_items 72672.
I0302 18:58:56.131427 22683591385216 run.py:483] Algo bellman_ford step 2271 current loss 0.017086, current_train_items 72704.
I0302 18:58:56.152708 22683591385216 run.py:483] Algo bellman_ford step 2272 current loss 0.087812, current_train_items 72736.
I0302 18:58:56.181912 22683591385216 run.py:483] Algo bellman_ford step 2273 current loss 0.112320, current_train_items 72768.
I0302 18:58:56.215226 22683591385216 run.py:483] Algo bellman_ford step 2274 current loss 0.139479, current_train_items 72800.
I0302 18:58:56.234306 22683591385216 run.py:483] Algo bellman_ford step 2275 current loss 0.004189, current_train_items 72832.
I0302 18:58:56.249788 22683591385216 run.py:483] Algo bellman_ford step 2276 current loss 0.025034, current_train_items 72864.
I0302 18:58:56.270497 22683591385216 run.py:483] Algo bellman_ford step 2277 current loss 0.025299, current_train_items 72896.
I0302 18:58:56.298218 22683591385216 run.py:483] Algo bellman_ford step 2278 current loss 0.143120, current_train_items 72928.
I0302 18:58:56.330150 22683591385216 run.py:483] Algo bellman_ford step 2279 current loss 0.142716, current_train_items 72960.
I0302 18:58:56.348408 22683591385216 run.py:483] Algo bellman_ford step 2280 current loss 0.002091, current_train_items 72992.
I0302 18:58:56.363423 22683591385216 run.py:483] Algo bellman_ford step 2281 current loss 0.058925, current_train_items 73024.
I0302 18:58:56.386376 22683591385216 run.py:483] Algo bellman_ford step 2282 current loss 0.046927, current_train_items 73056.
I0302 18:58:56.417442 22683591385216 run.py:483] Algo bellman_ford step 2283 current loss 0.096418, current_train_items 73088.
I0302 18:58:56.449444 22683591385216 run.py:483] Algo bellman_ford step 2284 current loss 0.105665, current_train_items 73120.
I0302 18:58:56.467998 22683591385216 run.py:483] Algo bellman_ford step 2285 current loss 0.002399, current_train_items 73152.
I0302 18:58:56.483660 22683591385216 run.py:483] Algo bellman_ford step 2286 current loss 0.015230, current_train_items 73184.
I0302 18:58:56.505445 22683591385216 run.py:483] Algo bellman_ford step 2287 current loss 0.041414, current_train_items 73216.
I0302 18:58:56.533483 22683591385216 run.py:483] Algo bellman_ford step 2288 current loss 0.064190, current_train_items 73248.
I0302 18:58:56.566763 22683591385216 run.py:483] Algo bellman_ford step 2289 current loss 0.137053, current_train_items 73280.
I0302 18:58:56.585184 22683591385216 run.py:483] Algo bellman_ford step 2290 current loss 0.001968, current_train_items 73312.
I0302 18:58:56.600198 22683591385216 run.py:483] Algo bellman_ford step 2291 current loss 0.014336, current_train_items 73344.
I0302 18:58:56.621945 22683591385216 run.py:483] Algo bellman_ford step 2292 current loss 0.094878, current_train_items 73376.
I0302 18:58:56.650721 22683591385216 run.py:483] Algo bellman_ford step 2293 current loss 0.083858, current_train_items 73408.
I0302 18:58:56.682081 22683591385216 run.py:483] Algo bellman_ford step 2294 current loss 0.185016, current_train_items 73440.
I0302 18:58:56.700534 22683591385216 run.py:483] Algo bellman_ford step 2295 current loss 0.002748, current_train_items 73472.
I0302 18:58:56.715846 22683591385216 run.py:483] Algo bellman_ford step 2296 current loss 0.032397, current_train_items 73504.
I0302 18:58:56.736973 22683591385216 run.py:483] Algo bellman_ford step 2297 current loss 0.028735, current_train_items 73536.
I0302 18:58:56.764624 22683591385216 run.py:483] Algo bellman_ford step 2298 current loss 0.081545, current_train_items 73568.
I0302 18:58:56.796144 22683591385216 run.py:483] Algo bellman_ford step 2299 current loss 0.121611, current_train_items 73600.
I0302 18:58:56.814738 22683591385216 run.py:483] Algo bellman_ford step 2300 current loss 0.003785, current_train_items 73632.
I0302 18:58:56.822582 22683591385216 run.py:503] (val) algo bellman_ford step 2300: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 73632, 'step': 2300, 'algorithm': 'bellman_ford'}
I0302 18:58:56.822697 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.989, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 18:58:56.851170 22683591385216 run.py:483] Algo bellman_ford step 2301 current loss 0.026827, current_train_items 73664.
I0302 18:58:56.873565 22683591385216 run.py:483] Algo bellman_ford step 2302 current loss 0.059440, current_train_items 73696.
I0302 18:58:56.903933 22683591385216 run.py:483] Algo bellman_ford step 2303 current loss 0.058212, current_train_items 73728.
I0302 18:58:56.936003 22683591385216 run.py:483] Algo bellman_ford step 2304 current loss 0.117325, current_train_items 73760.
I0302 18:58:56.954774 22683591385216 run.py:483] Algo bellman_ford step 2305 current loss 0.045969, current_train_items 73792.
I0302 18:58:56.970563 22683591385216 run.py:483] Algo bellman_ford step 2306 current loss 0.054525, current_train_items 73824.
I0302 18:58:56.992660 22683591385216 run.py:483] Algo bellman_ford step 2307 current loss 0.074412, current_train_items 73856.
I0302 18:58:57.020373 22683591385216 run.py:483] Algo bellman_ford step 2308 current loss 0.071196, current_train_items 73888.
I0302 18:58:57.052174 22683591385216 run.py:483] Algo bellman_ford step 2309 current loss 0.081523, current_train_items 73920.
I0302 18:58:57.070706 22683591385216 run.py:483] Algo bellman_ford step 2310 current loss 0.006004, current_train_items 73952.
I0302 18:58:57.086195 22683591385216 run.py:483] Algo bellman_ford step 2311 current loss 0.040062, current_train_items 73984.
I0302 18:58:57.108493 22683591385216 run.py:483] Algo bellman_ford step 2312 current loss 0.064801, current_train_items 74016.
I0302 18:58:57.137229 22683591385216 run.py:483] Algo bellman_ford step 2313 current loss 0.104400, current_train_items 74048.
I0302 18:58:57.168967 22683591385216 run.py:483] Algo bellman_ford step 2314 current loss 0.096095, current_train_items 74080.
I0302 18:58:57.187436 22683591385216 run.py:483] Algo bellman_ford step 2315 current loss 0.008709, current_train_items 74112.
I0302 18:58:57.202684 22683591385216 run.py:483] Algo bellman_ford step 2316 current loss 0.071101, current_train_items 74144.
I0302 18:58:57.224379 22683591385216 run.py:483] Algo bellman_ford step 2317 current loss 0.091976, current_train_items 74176.
I0302 18:58:57.253902 22683591385216 run.py:483] Algo bellman_ford step 2318 current loss 0.143745, current_train_items 74208.
I0302 18:58:57.285928 22683591385216 run.py:483] Algo bellman_ford step 2319 current loss 0.141248, current_train_items 74240.
I0302 18:58:57.304276 22683591385216 run.py:483] Algo bellman_ford step 2320 current loss 0.004330, current_train_items 74272.
I0302 18:58:57.319425 22683591385216 run.py:483] Algo bellman_ford step 2321 current loss 0.009428, current_train_items 74304.
I0302 18:58:57.342288 22683591385216 run.py:483] Algo bellman_ford step 2322 current loss 0.058217, current_train_items 74336.
I0302 18:58:57.370181 22683591385216 run.py:483] Algo bellman_ford step 2323 current loss 0.111936, current_train_items 74368.
I0302 18:58:57.401885 22683591385216 run.py:483] Algo bellman_ford step 2324 current loss 0.223855, current_train_items 74400.
I0302 18:58:57.420392 22683591385216 run.py:483] Algo bellman_ford step 2325 current loss 0.004393, current_train_items 74432.
I0302 18:58:57.435547 22683591385216 run.py:483] Algo bellman_ford step 2326 current loss 0.025730, current_train_items 74464.
I0302 18:58:57.457160 22683591385216 run.py:483] Algo bellman_ford step 2327 current loss 0.066405, current_train_items 74496.
I0302 18:58:57.484498 22683591385216 run.py:483] Algo bellman_ford step 2328 current loss 0.039537, current_train_items 74528.
I0302 18:58:57.514266 22683591385216 run.py:483] Algo bellman_ford step 2329 current loss 0.082628, current_train_items 74560.
I0302 18:58:57.532482 22683591385216 run.py:483] Algo bellman_ford step 2330 current loss 0.004391, current_train_items 74592.
I0302 18:58:57.547435 22683591385216 run.py:483] Algo bellman_ford step 2331 current loss 0.030578, current_train_items 74624.
I0302 18:58:57.569862 22683591385216 run.py:483] Algo bellman_ford step 2332 current loss 0.105840, current_train_items 74656.
I0302 18:58:57.599030 22683591385216 run.py:483] Algo bellman_ford step 2333 current loss 0.060578, current_train_items 74688.
I0302 18:58:57.631486 22683591385216 run.py:483] Algo bellman_ford step 2334 current loss 0.078427, current_train_items 74720.
I0302 18:58:57.649593 22683591385216 run.py:483] Algo bellman_ford step 2335 current loss 0.001877, current_train_items 74752.
I0302 18:58:57.665237 22683591385216 run.py:483] Algo bellman_ford step 2336 current loss 0.023389, current_train_items 74784.
I0302 18:58:57.687673 22683591385216 run.py:483] Algo bellman_ford step 2337 current loss 0.039681, current_train_items 74816.
I0302 18:58:57.715874 22683591385216 run.py:483] Algo bellman_ford step 2338 current loss 0.080321, current_train_items 74848.
I0302 18:58:57.746239 22683591385216 run.py:483] Algo bellman_ford step 2339 current loss 0.093250, current_train_items 74880.
I0302 18:58:57.764811 22683591385216 run.py:483] Algo bellman_ford step 2340 current loss 0.003524, current_train_items 74912.
I0302 18:58:57.780368 22683591385216 run.py:483] Algo bellman_ford step 2341 current loss 0.016615, current_train_items 74944.
I0302 18:58:57.801754 22683591385216 run.py:483] Algo bellman_ford step 2342 current loss 0.037191, current_train_items 74976.
I0302 18:58:57.829541 22683591385216 run.py:483] Algo bellman_ford step 2343 current loss 0.065379, current_train_items 75008.
I0302 18:58:57.859256 22683591385216 run.py:483] Algo bellman_ford step 2344 current loss 0.086315, current_train_items 75040.
I0302 18:58:57.877785 22683591385216 run.py:483] Algo bellman_ford step 2345 current loss 0.006556, current_train_items 75072.
I0302 18:58:57.892929 22683591385216 run.py:483] Algo bellman_ford step 2346 current loss 0.028105, current_train_items 75104.
I0302 18:58:57.915050 22683591385216 run.py:483] Algo bellman_ford step 2347 current loss 0.025002, current_train_items 75136.
I0302 18:58:57.942336 22683591385216 run.py:483] Algo bellman_ford step 2348 current loss 0.032310, current_train_items 75168.
I0302 18:58:57.971842 22683591385216 run.py:483] Algo bellman_ford step 2349 current loss 0.079740, current_train_items 75200.
I0302 18:58:57.990238 22683591385216 run.py:483] Algo bellman_ford step 2350 current loss 0.003078, current_train_items 75232.
I0302 18:58:57.998271 22683591385216 run.py:503] (val) algo bellman_ford step 2350: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 75232, 'step': 2350, 'algorithm': 'bellman_ford'}
I0302 18:58:57.998378 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:58:58.014264 22683591385216 run.py:483] Algo bellman_ford step 2351 current loss 0.024142, current_train_items 75264.
I0302 18:58:58.036478 22683591385216 run.py:483] Algo bellman_ford step 2352 current loss 0.100083, current_train_items 75296.
I0302 18:58:58.063547 22683591385216 run.py:483] Algo bellman_ford step 2353 current loss 0.074431, current_train_items 75328.
I0302 18:58:58.096636 22683591385216 run.py:483] Algo bellman_ford step 2354 current loss 0.106278, current_train_items 75360.
I0302 18:58:58.115166 22683591385216 run.py:483] Algo bellman_ford step 2355 current loss 0.031027, current_train_items 75392.
I0302 18:58:58.130010 22683591385216 run.py:483] Algo bellman_ford step 2356 current loss 0.016893, current_train_items 75424.
I0302 18:58:58.152144 22683591385216 run.py:483] Algo bellman_ford step 2357 current loss 0.078331, current_train_items 75456.
I0302 18:58:58.179294 22683591385216 run.py:483] Algo bellman_ford step 2358 current loss 0.101259, current_train_items 75488.
I0302 18:58:58.212171 22683591385216 run.py:483] Algo bellman_ford step 2359 current loss 0.096255, current_train_items 75520.
I0302 18:58:58.230587 22683591385216 run.py:483] Algo bellman_ford step 2360 current loss 0.013747, current_train_items 75552.
I0302 18:58:58.246810 22683591385216 run.py:483] Algo bellman_ford step 2361 current loss 0.042087, current_train_items 75584.
I0302 18:58:58.269607 22683591385216 run.py:483] Algo bellman_ford step 2362 current loss 0.075373, current_train_items 75616.
I0302 18:58:58.297057 22683591385216 run.py:483] Algo bellman_ford step 2363 current loss 0.061542, current_train_items 75648.
I0302 18:58:58.327734 22683591385216 run.py:483] Algo bellman_ford step 2364 current loss 0.097715, current_train_items 75680.
I0302 18:58:58.345559 22683591385216 run.py:483] Algo bellman_ford step 2365 current loss 0.003883, current_train_items 75712.
I0302 18:58:58.361009 22683591385216 run.py:483] Algo bellman_ford step 2366 current loss 0.034287, current_train_items 75744.
I0302 18:58:58.382988 22683591385216 run.py:483] Algo bellman_ford step 2367 current loss 0.226440, current_train_items 75776.
I0302 18:58:58.412370 22683591385216 run.py:483] Algo bellman_ford step 2368 current loss 0.152117, current_train_items 75808.
I0302 18:58:58.444085 22683591385216 run.py:483] Algo bellman_ford step 2369 current loss 0.116447, current_train_items 75840.
I0302 18:58:58.462692 22683591385216 run.py:483] Algo bellman_ford step 2370 current loss 0.006619, current_train_items 75872.
I0302 18:58:58.478156 22683591385216 run.py:483] Algo bellman_ford step 2371 current loss 0.093943, current_train_items 75904.
I0302 18:58:58.499735 22683591385216 run.py:483] Algo bellman_ford step 2372 current loss 0.082429, current_train_items 75936.
I0302 18:58:58.527025 22683591385216 run.py:483] Algo bellman_ford step 2373 current loss 0.123530, current_train_items 75968.
I0302 18:58:58.556906 22683591385216 run.py:483] Algo bellman_ford step 2374 current loss 0.131714, current_train_items 76000.
I0302 18:58:58.575162 22683591385216 run.py:483] Algo bellman_ford step 2375 current loss 0.003503, current_train_items 76032.
I0302 18:58:58.590850 22683591385216 run.py:483] Algo bellman_ford step 2376 current loss 0.017305, current_train_items 76064.
I0302 18:58:58.612607 22683591385216 run.py:483] Algo bellman_ford step 2377 current loss 0.071529, current_train_items 76096.
I0302 18:58:58.642047 22683591385216 run.py:483] Algo bellman_ford step 2378 current loss 0.093155, current_train_items 76128.
I0302 18:58:58.674332 22683591385216 run.py:483] Algo bellman_ford step 2379 current loss 0.114344, current_train_items 76160.
I0302 18:58:58.692466 22683591385216 run.py:483] Algo bellman_ford step 2380 current loss 0.015198, current_train_items 76192.
I0302 18:58:58.708245 22683591385216 run.py:483] Algo bellman_ford step 2381 current loss 0.049377, current_train_items 76224.
I0302 18:58:58.730616 22683591385216 run.py:483] Algo bellman_ford step 2382 current loss 0.051655, current_train_items 76256.
I0302 18:58:58.757828 22683591385216 run.py:483] Algo bellman_ford step 2383 current loss 0.068544, current_train_items 76288.
I0302 18:58:58.789099 22683591385216 run.py:483] Algo bellman_ford step 2384 current loss 0.095387, current_train_items 76320.
I0302 18:58:58.807642 22683591385216 run.py:483] Algo bellman_ford step 2385 current loss 0.002705, current_train_items 76352.
I0302 18:58:58.822992 22683591385216 run.py:483] Algo bellman_ford step 2386 current loss 0.016218, current_train_items 76384.
I0302 18:58:58.843918 22683591385216 run.py:483] Algo bellman_ford step 2387 current loss 0.053013, current_train_items 76416.
I0302 18:58:58.872682 22683591385216 run.py:483] Algo bellman_ford step 2388 current loss 0.060682, current_train_items 76448.
I0302 18:58:58.905124 22683591385216 run.py:483] Algo bellman_ford step 2389 current loss 0.090123, current_train_items 76480.
I0302 18:58:58.923800 22683591385216 run.py:483] Algo bellman_ford step 2390 current loss 0.001282, current_train_items 76512.
I0302 18:58:58.939165 22683591385216 run.py:483] Algo bellman_ford step 2391 current loss 0.030376, current_train_items 76544.
I0302 18:58:58.959441 22683591385216 run.py:483] Algo bellman_ford step 2392 current loss 0.045332, current_train_items 76576.
I0302 18:58:58.987620 22683591385216 run.py:483] Algo bellman_ford step 2393 current loss 0.084737, current_train_items 76608.
I0302 18:58:59.021284 22683591385216 run.py:483] Algo bellman_ford step 2394 current loss 0.097279, current_train_items 76640.
I0302 18:58:59.039399 22683591385216 run.py:483] Algo bellman_ford step 2395 current loss 0.005925, current_train_items 76672.
I0302 18:58:59.054634 22683591385216 run.py:483] Algo bellman_ford step 2396 current loss 0.010865, current_train_items 76704.
I0302 18:58:59.075565 22683591385216 run.py:483] Algo bellman_ford step 2397 current loss 0.078007, current_train_items 76736.
I0302 18:58:59.103758 22683591385216 run.py:483] Algo bellman_ford step 2398 current loss 0.163619, current_train_items 76768.
I0302 18:58:59.136354 22683591385216 run.py:483] Algo bellman_ford step 2399 current loss 0.092616, current_train_items 76800.
I0302 18:58:59.155244 22683591385216 run.py:483] Algo bellman_ford step 2400 current loss 0.001647, current_train_items 76832.
I0302 18:58:59.162867 22683591385216 run.py:503] (val) algo bellman_ford step 2400: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 76832, 'step': 2400, 'algorithm': 'bellman_ford'}
I0302 18:58:59.162985 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 18:58:59.178750 22683591385216 run.py:483] Algo bellman_ford step 2401 current loss 0.020778, current_train_items 76864.
I0302 18:58:59.201325 22683591385216 run.py:483] Algo bellman_ford step 2402 current loss 0.068138, current_train_items 76896.
I0302 18:58:59.230656 22683591385216 run.py:483] Algo bellman_ford step 2403 current loss 0.155514, current_train_items 76928.
I0302 18:58:59.265145 22683591385216 run.py:483] Algo bellman_ford step 2404 current loss 0.118524, current_train_items 76960.
I0302 18:58:59.283483 22683591385216 run.py:483] Algo bellman_ford step 2405 current loss 0.002966, current_train_items 76992.
I0302 18:58:59.298764 22683591385216 run.py:483] Algo bellman_ford step 2406 current loss 0.029865, current_train_items 77024.
I0302 18:58:59.320778 22683591385216 run.py:483] Algo bellman_ford step 2407 current loss 0.053988, current_train_items 77056.
I0302 18:58:59.348232 22683591385216 run.py:483] Algo bellman_ford step 2408 current loss 0.057081, current_train_items 77088.
I0302 18:58:59.378658 22683591385216 run.py:483] Algo bellman_ford step 2409 current loss 0.148399, current_train_items 77120.
I0302 18:58:59.396517 22683591385216 run.py:483] Algo bellman_ford step 2410 current loss 0.001274, current_train_items 77152.
I0302 18:58:59.411786 22683591385216 run.py:483] Algo bellman_ford step 2411 current loss 0.015470, current_train_items 77184.
I0302 18:58:59.435071 22683591385216 run.py:483] Algo bellman_ford step 2412 current loss 0.042859, current_train_items 77216.
I0302 18:58:59.464336 22683591385216 run.py:483] Algo bellman_ford step 2413 current loss 0.077431, current_train_items 77248.
I0302 18:58:59.496534 22683591385216 run.py:483] Algo bellman_ford step 2414 current loss 0.093774, current_train_items 77280.
I0302 18:58:59.515087 22683591385216 run.py:483] Algo bellman_ford step 2415 current loss 0.002490, current_train_items 77312.
I0302 18:58:59.530234 22683591385216 run.py:483] Algo bellman_ford step 2416 current loss 0.034740, current_train_items 77344.
I0302 18:58:59.552312 22683591385216 run.py:483] Algo bellman_ford step 2417 current loss 0.061785, current_train_items 77376.
I0302 18:58:59.582003 22683591385216 run.py:483] Algo bellman_ford step 2418 current loss 0.110917, current_train_items 77408.
I0302 18:58:59.611535 22683591385216 run.py:483] Algo bellman_ford step 2419 current loss 0.067910, current_train_items 77440.
I0302 18:58:59.629649 22683591385216 run.py:483] Algo bellman_ford step 2420 current loss 0.004090, current_train_items 77472.
I0302 18:58:59.645022 22683591385216 run.py:483] Algo bellman_ford step 2421 current loss 0.014711, current_train_items 77504.
I0302 18:58:59.668172 22683591385216 run.py:483] Algo bellman_ford step 2422 current loss 0.055102, current_train_items 77536.
I0302 18:58:59.697104 22683591385216 run.py:483] Algo bellman_ford step 2423 current loss 0.042425, current_train_items 77568.
I0302 18:58:59.729353 22683591385216 run.py:483] Algo bellman_ford step 2424 current loss 0.094875, current_train_items 77600.
I0302 18:58:59.747511 22683591385216 run.py:483] Algo bellman_ford step 2425 current loss 0.039135, current_train_items 77632.
I0302 18:58:59.762861 22683591385216 run.py:483] Algo bellman_ford step 2426 current loss 0.031396, current_train_items 77664.
I0302 18:58:59.786594 22683591385216 run.py:483] Algo bellman_ford step 2427 current loss 0.111152, current_train_items 77696.
I0302 18:58:59.813893 22683591385216 run.py:483] Algo bellman_ford step 2428 current loss 0.078173, current_train_items 77728.
I0302 18:58:59.844505 22683591385216 run.py:483] Algo bellman_ford step 2429 current loss 0.064938, current_train_items 77760.
I0302 18:58:59.862833 22683591385216 run.py:483] Algo bellman_ford step 2430 current loss 0.009333, current_train_items 77792.
I0302 18:58:59.878419 22683591385216 run.py:483] Algo bellman_ford step 2431 current loss 0.017394, current_train_items 77824.
I0302 18:58:59.901261 22683591385216 run.py:483] Algo bellman_ford step 2432 current loss 0.061163, current_train_items 77856.
I0302 18:58:59.929311 22683591385216 run.py:483] Algo bellman_ford step 2433 current loss 0.072634, current_train_items 77888.
I0302 18:58:59.960910 22683591385216 run.py:483] Algo bellman_ford step 2434 current loss 0.107934, current_train_items 77920.
I0302 18:58:59.978826 22683591385216 run.py:483] Algo bellman_ford step 2435 current loss 0.011032, current_train_items 77952.
I0302 18:58:59.996954 22683591385216 run.py:483] Algo bellman_ford step 2436 current loss 0.030872, current_train_items 77984.
I0302 18:59:00.019073 22683591385216 run.py:483] Algo bellman_ford step 2437 current loss 0.074500, current_train_items 78016.
I0302 18:59:00.046285 22683591385216 run.py:483] Algo bellman_ford step 2438 current loss 0.106626, current_train_items 78048.
I0302 18:59:00.076612 22683591385216 run.py:483] Algo bellman_ford step 2439 current loss 0.120233, current_train_items 78080.
I0302 18:59:00.095116 22683591385216 run.py:483] Algo bellman_ford step 2440 current loss 0.008821, current_train_items 78112.
I0302 18:59:00.110489 22683591385216 run.py:483] Algo bellman_ford step 2441 current loss 0.029515, current_train_items 78144.
I0302 18:59:00.135106 22683591385216 run.py:483] Algo bellman_ford step 2442 current loss 0.092505, current_train_items 78176.
I0302 18:59:00.164492 22683591385216 run.py:483] Algo bellman_ford step 2443 current loss 0.074081, current_train_items 78208.
I0302 18:59:00.197068 22683591385216 run.py:483] Algo bellman_ford step 2444 current loss 0.137511, current_train_items 78240.
I0302 18:59:00.215504 22683591385216 run.py:483] Algo bellman_ford step 2445 current loss 0.017457, current_train_items 78272.
I0302 18:59:00.230951 22683591385216 run.py:483] Algo bellman_ford step 2446 current loss 0.023005, current_train_items 78304.
I0302 18:59:00.253987 22683591385216 run.py:483] Algo bellman_ford step 2447 current loss 0.047015, current_train_items 78336.
I0302 18:59:00.281320 22683591385216 run.py:483] Algo bellman_ford step 2448 current loss 0.052764, current_train_items 78368.
I0302 18:59:00.311985 22683591385216 run.py:483] Algo bellman_ford step 2449 current loss 0.085067, current_train_items 78400.
I0302 18:59:00.329943 22683591385216 run.py:483] Algo bellman_ford step 2450 current loss 0.013954, current_train_items 78432.
I0302 18:59:00.337627 22683591385216 run.py:503] (val) algo bellman_ford step 2450: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 78432, 'step': 2450, 'algorithm': 'bellman_ford'}
I0302 18:59:00.337735 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:00.353701 22683591385216 run.py:483] Algo bellman_ford step 2451 current loss 0.012426, current_train_items 78464.
I0302 18:59:00.376648 22683591385216 run.py:483] Algo bellman_ford step 2452 current loss 0.045148, current_train_items 78496.
I0302 18:59:00.404753 22683591385216 run.py:483] Algo bellman_ford step 2453 current loss 0.086762, current_train_items 78528.
I0302 18:59:00.437026 22683591385216 run.py:483] Algo bellman_ford step 2454 current loss 0.084333, current_train_items 78560.
I0302 18:59:00.455644 22683591385216 run.py:483] Algo bellman_ford step 2455 current loss 0.005440, current_train_items 78592.
I0302 18:59:00.471003 22683591385216 run.py:483] Algo bellman_ford step 2456 current loss 0.017815, current_train_items 78624.
I0302 18:59:00.492272 22683591385216 run.py:483] Algo bellman_ford step 2457 current loss 0.037007, current_train_items 78656.
I0302 18:59:00.520807 22683591385216 run.py:483] Algo bellman_ford step 2458 current loss 0.070880, current_train_items 78688.
I0302 18:59:00.553471 22683591385216 run.py:483] Algo bellman_ford step 2459 current loss 0.304257, current_train_items 78720.
I0302 18:59:00.572166 22683591385216 run.py:483] Algo bellman_ford step 2460 current loss 0.005221, current_train_items 78752.
I0302 18:59:00.588282 22683591385216 run.py:483] Algo bellman_ford step 2461 current loss 0.045899, current_train_items 78784.
I0302 18:59:00.609846 22683591385216 run.py:483] Algo bellman_ford step 2462 current loss 0.063968, current_train_items 78816.
I0302 18:59:00.639089 22683591385216 run.py:483] Algo bellman_ford step 2463 current loss 0.098555, current_train_items 78848.
I0302 18:59:00.670245 22683591385216 run.py:483] Algo bellman_ford step 2464 current loss 0.115495, current_train_items 78880.
I0302 18:59:00.688268 22683591385216 run.py:483] Algo bellman_ford step 2465 current loss 0.009994, current_train_items 78912.
I0302 18:59:00.703846 22683591385216 run.py:483] Algo bellman_ford step 2466 current loss 0.029619, current_train_items 78944.
I0302 18:59:00.726002 22683591385216 run.py:483] Algo bellman_ford step 2467 current loss 0.059427, current_train_items 78976.
I0302 18:59:00.754909 22683591385216 run.py:483] Algo bellman_ford step 2468 current loss 0.097268, current_train_items 79008.
I0302 18:59:00.787437 22683591385216 run.py:483] Algo bellman_ford step 2469 current loss 0.121276, current_train_items 79040.
I0302 18:59:00.805849 22683591385216 run.py:483] Algo bellman_ford step 2470 current loss 0.004252, current_train_items 79072.
I0302 18:59:00.821459 22683591385216 run.py:483] Algo bellman_ford step 2471 current loss 0.042963, current_train_items 79104.
I0302 18:59:00.843707 22683591385216 run.py:483] Algo bellman_ford step 2472 current loss 0.135556, current_train_items 79136.
I0302 18:59:00.873094 22683591385216 run.py:483] Algo bellman_ford step 2473 current loss 0.111362, current_train_items 79168.
I0302 18:59:00.903695 22683591385216 run.py:483] Algo bellman_ford step 2474 current loss 0.076161, current_train_items 79200.
I0302 18:59:00.922112 22683591385216 run.py:483] Algo bellman_ford step 2475 current loss 0.009633, current_train_items 79232.
I0302 18:59:00.937623 22683591385216 run.py:483] Algo bellman_ford step 2476 current loss 0.049936, current_train_items 79264.
I0302 18:59:00.958266 22683591385216 run.py:483] Algo bellman_ford step 2477 current loss 0.054935, current_train_items 79296.
I0302 18:59:00.985760 22683591385216 run.py:483] Algo bellman_ford step 2478 current loss 0.127487, current_train_items 79328.
I0302 18:59:01.018043 22683591385216 run.py:483] Algo bellman_ford step 2479 current loss 0.170063, current_train_items 79360.
I0302 18:59:01.035872 22683591385216 run.py:483] Algo bellman_ford step 2480 current loss 0.002057, current_train_items 79392.
I0302 18:59:01.050812 22683591385216 run.py:483] Algo bellman_ford step 2481 current loss 0.037803, current_train_items 79424.
I0302 18:59:01.073614 22683591385216 run.py:483] Algo bellman_ford step 2482 current loss 0.088865, current_train_items 79456.
I0302 18:59:01.102065 22683591385216 run.py:483] Algo bellman_ford step 2483 current loss 0.154498, current_train_items 79488.
I0302 18:59:01.134525 22683591385216 run.py:483] Algo bellman_ford step 2484 current loss 0.120825, current_train_items 79520.
I0302 18:59:01.153242 22683591385216 run.py:483] Algo bellman_ford step 2485 current loss 0.009372, current_train_items 79552.
I0302 18:59:01.168941 22683591385216 run.py:483] Algo bellman_ford step 2486 current loss 0.046542, current_train_items 79584.
I0302 18:59:01.191645 22683591385216 run.py:483] Algo bellman_ford step 2487 current loss 0.104258, current_train_items 79616.
I0302 18:59:01.217242 22683591385216 run.py:483] Algo bellman_ford step 2488 current loss 0.078793, current_train_items 79648.
I0302 18:59:01.249015 22683591385216 run.py:483] Algo bellman_ford step 2489 current loss 0.161468, current_train_items 79680.
I0302 18:59:01.267477 22683591385216 run.py:483] Algo bellman_ford step 2490 current loss 0.040213, current_train_items 79712.
I0302 18:59:01.283034 22683591385216 run.py:483] Algo bellman_ford step 2491 current loss 0.032935, current_train_items 79744.
I0302 18:59:01.304458 22683591385216 run.py:483] Algo bellman_ford step 2492 current loss 0.131458, current_train_items 79776.
I0302 18:59:01.332461 22683591385216 run.py:483] Algo bellman_ford step 2493 current loss 0.076791, current_train_items 79808.
I0302 18:59:01.362435 22683591385216 run.py:483] Algo bellman_ford step 2494 current loss 0.092820, current_train_items 79840.
I0302 18:59:01.380387 22683591385216 run.py:483] Algo bellman_ford step 2495 current loss 0.004913, current_train_items 79872.
I0302 18:59:01.395989 22683591385216 run.py:483] Algo bellman_ford step 2496 current loss 0.038509, current_train_items 79904.
I0302 18:59:01.417558 22683591385216 run.py:483] Algo bellman_ford step 2497 current loss 0.069584, current_train_items 79936.
I0302 18:59:01.444363 22683591385216 run.py:483] Algo bellman_ford step 2498 current loss 0.151473, current_train_items 79968.
I0302 18:59:01.475501 22683591385216 run.py:483] Algo bellman_ford step 2499 current loss 0.134053, current_train_items 80000.
I0302 18:59:01.494063 22683591385216 run.py:483] Algo bellman_ford step 2500 current loss 0.004579, current_train_items 80032.
I0302 18:59:01.501568 22683591385216 run.py:503] (val) algo bellman_ford step 2500: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 80032, 'step': 2500, 'algorithm': 'bellman_ford'}
I0302 18:59:01.501679 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:01.517819 22683591385216 run.py:483] Algo bellman_ford step 2501 current loss 0.039335, current_train_items 80064.
I0302 18:59:01.540779 22683591385216 run.py:483] Algo bellman_ford step 2502 current loss 0.078669, current_train_items 80096.
I0302 18:59:01.568711 22683591385216 run.py:483] Algo bellman_ford step 2503 current loss 0.072432, current_train_items 80128.
I0302 18:59:01.601486 22683591385216 run.py:483] Algo bellman_ford step 2504 current loss 0.111692, current_train_items 80160.
I0302 18:59:01.619911 22683591385216 run.py:483] Algo bellman_ford step 2505 current loss 0.003335, current_train_items 80192.
I0302 18:59:01.635270 22683591385216 run.py:483] Algo bellman_ford step 2506 current loss 0.028091, current_train_items 80224.
I0302 18:59:01.657852 22683591385216 run.py:483] Algo bellman_ford step 2507 current loss 0.066297, current_train_items 80256.
I0302 18:59:01.685696 22683591385216 run.py:483] Algo bellman_ford step 2508 current loss 0.094155, current_train_items 80288.
I0302 18:59:01.717985 22683591385216 run.py:483] Algo bellman_ford step 2509 current loss 0.093841, current_train_items 80320.
I0302 18:59:01.736190 22683591385216 run.py:483] Algo bellman_ford step 2510 current loss 0.015502, current_train_items 80352.
I0302 18:59:01.751788 22683591385216 run.py:483] Algo bellman_ford step 2511 current loss 0.042431, current_train_items 80384.
I0302 18:59:01.773800 22683591385216 run.py:483] Algo bellman_ford step 2512 current loss 0.074948, current_train_items 80416.
I0302 18:59:01.801333 22683591385216 run.py:483] Algo bellman_ford step 2513 current loss 0.111985, current_train_items 80448.
I0302 18:59:01.833629 22683591385216 run.py:483] Algo bellman_ford step 2514 current loss 0.086820, current_train_items 80480.
I0302 18:59:01.851596 22683591385216 run.py:483] Algo bellman_ford step 2515 current loss 0.006678, current_train_items 80512.
I0302 18:59:01.867156 22683591385216 run.py:483] Algo bellman_ford step 2516 current loss 0.032620, current_train_items 80544.
I0302 18:59:01.889420 22683591385216 run.py:483] Algo bellman_ford step 2517 current loss 0.151426, current_train_items 80576.
I0302 18:59:01.917393 22683591385216 run.py:483] Algo bellman_ford step 2518 current loss 0.071672, current_train_items 80608.
I0302 18:59:01.949505 22683591385216 run.py:483] Algo bellman_ford step 2519 current loss 0.089139, current_train_items 80640.
I0302 18:59:01.967534 22683591385216 run.py:483] Algo bellman_ford step 2520 current loss 0.004529, current_train_items 80672.
I0302 18:59:01.982873 22683591385216 run.py:483] Algo bellman_ford step 2521 current loss 0.034906, current_train_items 80704.
I0302 18:59:02.003601 22683591385216 run.py:483] Algo bellman_ford step 2522 current loss 0.034973, current_train_items 80736.
I0302 18:59:02.031603 22683591385216 run.py:483] Algo bellman_ford step 2523 current loss 0.025931, current_train_items 80768.
I0302 18:59:02.062160 22683591385216 run.py:483] Algo bellman_ford step 2524 current loss 0.060275, current_train_items 80800.
I0302 18:59:02.080286 22683591385216 run.py:483] Algo bellman_ford step 2525 current loss 0.004786, current_train_items 80832.
I0302 18:59:02.095668 22683591385216 run.py:483] Algo bellman_ford step 2526 current loss 0.021824, current_train_items 80864.
I0302 18:59:02.117653 22683591385216 run.py:483] Algo bellman_ford step 2527 current loss 0.093198, current_train_items 80896.
I0302 18:59:02.145559 22683591385216 run.py:483] Algo bellman_ford step 2528 current loss 0.037376, current_train_items 80928.
I0302 18:59:02.178168 22683591385216 run.py:483] Algo bellman_ford step 2529 current loss 0.073432, current_train_items 80960.
I0302 18:59:02.196151 22683591385216 run.py:483] Algo bellman_ford step 2530 current loss 0.005541, current_train_items 80992.
I0302 18:59:02.211061 22683591385216 run.py:483] Algo bellman_ford step 2531 current loss 0.044997, current_train_items 81024.
I0302 18:59:02.233308 22683591385216 run.py:483] Algo bellman_ford step 2532 current loss 0.076567, current_train_items 81056.
I0302 18:59:02.261698 22683591385216 run.py:483] Algo bellman_ford step 2533 current loss 0.092994, current_train_items 81088.
I0302 18:59:02.292438 22683591385216 run.py:483] Algo bellman_ford step 2534 current loss 0.076843, current_train_items 81120.
I0302 18:59:02.310508 22683591385216 run.py:483] Algo bellman_ford step 2535 current loss 0.003588, current_train_items 81152.
I0302 18:59:02.325472 22683591385216 run.py:483] Algo bellman_ford step 2536 current loss 0.027587, current_train_items 81184.
W0302 18:59:02.341481 22683591385216 samplers.py:155] Increasing hint lengh from 9 to 10
I0302 18:59:07.730153 22683591385216 run.py:483] Algo bellman_ford step 2537 current loss 0.142853, current_train_items 81216.
I0302 18:59:07.761006 22683591385216 run.py:483] Algo bellman_ford step 2538 current loss 0.145881, current_train_items 81248.
I0302 18:59:07.793401 22683591385216 run.py:483] Algo bellman_ford step 2539 current loss 0.160996, current_train_items 81280.
I0302 18:59:07.812372 22683591385216 run.py:483] Algo bellman_ford step 2540 current loss 0.051701, current_train_items 81312.
I0302 18:59:07.827603 22683591385216 run.py:483] Algo bellman_ford step 2541 current loss 0.015313, current_train_items 81344.
I0302 18:59:07.850370 22683591385216 run.py:483] Algo bellman_ford step 2542 current loss 0.098650, current_train_items 81376.
I0302 18:59:07.877997 22683591385216 run.py:483] Algo bellman_ford step 2543 current loss 0.131703, current_train_items 81408.
I0302 18:59:07.909325 22683591385216 run.py:483] Algo bellman_ford step 2544 current loss 0.127675, current_train_items 81440.
I0302 18:59:07.927943 22683591385216 run.py:483] Algo bellman_ford step 2545 current loss 0.002673, current_train_items 81472.
I0302 18:59:07.943135 22683591385216 run.py:483] Algo bellman_ford step 2546 current loss 0.030429, current_train_items 81504.
I0302 18:59:07.965749 22683591385216 run.py:483] Algo bellman_ford step 2547 current loss 0.039099, current_train_items 81536.
I0302 18:59:07.995496 22683591385216 run.py:483] Algo bellman_ford step 2548 current loss 0.066148, current_train_items 81568.
I0302 18:59:08.025838 22683591385216 run.py:483] Algo bellman_ford step 2549 current loss 0.078876, current_train_items 81600.
I0302 18:59:08.044382 22683591385216 run.py:483] Algo bellman_ford step 2550 current loss 0.002304, current_train_items 81632.
I0302 18:59:08.053997 22683591385216 run.py:503] (val) algo bellman_ford step 2550: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 81632, 'step': 2550, 'algorithm': 'bellman_ford'}
I0302 18:59:08.054105 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 18:59:08.070194 22683591385216 run.py:483] Algo bellman_ford step 2551 current loss 0.015495, current_train_items 81664.
I0302 18:59:08.093578 22683591385216 run.py:483] Algo bellman_ford step 2552 current loss 0.060301, current_train_items 81696.
I0302 18:59:08.124050 22683591385216 run.py:483] Algo bellman_ford step 2553 current loss 0.077510, current_train_items 81728.
I0302 18:59:08.155825 22683591385216 run.py:483] Algo bellman_ford step 2554 current loss 0.108343, current_train_items 81760.
I0302 18:59:08.174504 22683591385216 run.py:483] Algo bellman_ford step 2555 current loss 0.001425, current_train_items 81792.
I0302 18:59:08.190107 22683591385216 run.py:483] Algo bellman_ford step 2556 current loss 0.029380, current_train_items 81824.
I0302 18:59:08.212807 22683591385216 run.py:483] Algo bellman_ford step 2557 current loss 0.040548, current_train_items 81856.
I0302 18:59:08.242578 22683591385216 run.py:483] Algo bellman_ford step 2558 current loss 0.101138, current_train_items 81888.
I0302 18:59:08.275792 22683591385216 run.py:483] Algo bellman_ford step 2559 current loss 0.082309, current_train_items 81920.
I0302 18:59:08.294318 22683591385216 run.py:483] Algo bellman_ford step 2560 current loss 0.007275, current_train_items 81952.
I0302 18:59:08.310248 22683591385216 run.py:483] Algo bellman_ford step 2561 current loss 0.010342, current_train_items 81984.
I0302 18:59:08.332379 22683591385216 run.py:483] Algo bellman_ford step 2562 current loss 0.108630, current_train_items 82016.
I0302 18:59:08.362692 22683591385216 run.py:483] Algo bellman_ford step 2563 current loss 0.154031, current_train_items 82048.
I0302 18:59:08.394238 22683591385216 run.py:483] Algo bellman_ford step 2564 current loss 0.088736, current_train_items 82080.
I0302 18:59:08.412298 22683591385216 run.py:483] Algo bellman_ford step 2565 current loss 0.001871, current_train_items 82112.
I0302 18:59:08.428030 22683591385216 run.py:483] Algo bellman_ford step 2566 current loss 0.032641, current_train_items 82144.
I0302 18:59:08.449987 22683591385216 run.py:483] Algo bellman_ford step 2567 current loss 0.069500, current_train_items 82176.
I0302 18:59:08.479993 22683591385216 run.py:483] Algo bellman_ford step 2568 current loss 0.139152, current_train_items 82208.
I0302 18:59:08.510768 22683591385216 run.py:483] Algo bellman_ford step 2569 current loss 0.106848, current_train_items 82240.
I0302 18:59:08.529599 22683591385216 run.py:483] Algo bellman_ford step 2570 current loss 0.002345, current_train_items 82272.
I0302 18:59:08.545110 22683591385216 run.py:483] Algo bellman_ford step 2571 current loss 0.012499, current_train_items 82304.
I0302 18:59:08.566263 22683591385216 run.py:483] Algo bellman_ford step 2572 current loss 0.062640, current_train_items 82336.
I0302 18:59:08.595313 22683591385216 run.py:483] Algo bellman_ford step 2573 current loss 0.057372, current_train_items 82368.
I0302 18:59:08.627971 22683591385216 run.py:483] Algo bellman_ford step 2574 current loss 0.180256, current_train_items 82400.
I0302 18:59:08.646787 22683591385216 run.py:483] Algo bellman_ford step 2575 current loss 0.016745, current_train_items 82432.
I0302 18:59:08.662096 22683591385216 run.py:483] Algo bellman_ford step 2576 current loss 0.033790, current_train_items 82464.
I0302 18:59:08.683834 22683591385216 run.py:483] Algo bellman_ford step 2577 current loss 0.038329, current_train_items 82496.
I0302 18:59:08.713591 22683591385216 run.py:483] Algo bellman_ford step 2578 current loss 0.057135, current_train_items 82528.
I0302 18:59:08.743741 22683591385216 run.py:483] Algo bellman_ford step 2579 current loss 0.087263, current_train_items 82560.
I0302 18:59:08.761856 22683591385216 run.py:483] Algo bellman_ford step 2580 current loss 0.003408, current_train_items 82592.
I0302 18:59:08.777328 22683591385216 run.py:483] Algo bellman_ford step 2581 current loss 0.018134, current_train_items 82624.
I0302 18:59:08.799218 22683591385216 run.py:483] Algo bellman_ford step 2582 current loss 0.059426, current_train_items 82656.
I0302 18:59:08.827794 22683591385216 run.py:483] Algo bellman_ford step 2583 current loss 0.062546, current_train_items 82688.
I0302 18:59:08.859991 22683591385216 run.py:483] Algo bellman_ford step 2584 current loss 0.075639, current_train_items 82720.
I0302 18:59:08.878430 22683591385216 run.py:483] Algo bellman_ford step 2585 current loss 0.002338, current_train_items 82752.
I0302 18:59:08.894148 22683591385216 run.py:483] Algo bellman_ford step 2586 current loss 0.029816, current_train_items 82784.
I0302 18:59:08.915993 22683591385216 run.py:483] Algo bellman_ford step 2587 current loss 0.047924, current_train_items 82816.
I0302 18:59:08.944745 22683591385216 run.py:483] Algo bellman_ford step 2588 current loss 0.098126, current_train_items 82848.
I0302 18:59:08.976584 22683591385216 run.py:483] Algo bellman_ford step 2589 current loss 0.143941, current_train_items 82880.
I0302 18:59:08.995455 22683591385216 run.py:483] Algo bellman_ford step 2590 current loss 0.001264, current_train_items 82912.
I0302 18:59:09.011287 22683591385216 run.py:483] Algo bellman_ford step 2591 current loss 0.018232, current_train_items 82944.
I0302 18:59:09.033930 22683591385216 run.py:483] Algo bellman_ford step 2592 current loss 0.118070, current_train_items 82976.
I0302 18:59:09.064125 22683591385216 run.py:483] Algo bellman_ford step 2593 current loss 0.135158, current_train_items 83008.
I0302 18:59:09.093783 22683591385216 run.py:483] Algo bellman_ford step 2594 current loss 0.142919, current_train_items 83040.
I0302 18:59:09.111626 22683591385216 run.py:483] Algo bellman_ford step 2595 current loss 0.009977, current_train_items 83072.
I0302 18:59:09.127239 22683591385216 run.py:483] Algo bellman_ford step 2596 current loss 0.065565, current_train_items 83104.
I0302 18:59:09.150183 22683591385216 run.py:483] Algo bellman_ford step 2597 current loss 0.057126, current_train_items 83136.
I0302 18:59:09.179677 22683591385216 run.py:483] Algo bellman_ford step 2598 current loss 0.070200, current_train_items 83168.
I0302 18:59:09.210806 22683591385216 run.py:483] Algo bellman_ford step 2599 current loss 0.149131, current_train_items 83200.
I0302 18:59:09.229794 22683591385216 run.py:483] Algo bellman_ford step 2600 current loss 0.014440, current_train_items 83232.
I0302 18:59:09.237966 22683591385216 run.py:503] (val) algo bellman_ford step 2600: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 83232, 'step': 2600, 'algorithm': 'bellman_ford'}
I0302 18:59:09.238070 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:09.253912 22683591385216 run.py:483] Algo bellman_ford step 2601 current loss 0.012951, current_train_items 83264.
I0302 18:59:09.276018 22683591385216 run.py:483] Algo bellman_ford step 2602 current loss 0.034085, current_train_items 83296.
I0302 18:59:09.303421 22683591385216 run.py:483] Algo bellman_ford step 2603 current loss 0.061723, current_train_items 83328.
I0302 18:59:09.334695 22683591385216 run.py:483] Algo bellman_ford step 2604 current loss 0.060083, current_train_items 83360.
I0302 18:59:09.352995 22683591385216 run.py:483] Algo bellman_ford step 2605 current loss 0.022889, current_train_items 83392.
I0302 18:59:09.368165 22683591385216 run.py:483] Algo bellman_ford step 2606 current loss 0.010177, current_train_items 83424.
I0302 18:59:09.390635 22683591385216 run.py:483] Algo bellman_ford step 2607 current loss 0.034349, current_train_items 83456.
I0302 18:59:09.420632 22683591385216 run.py:483] Algo bellman_ford step 2608 current loss 0.119427, current_train_items 83488.
I0302 18:59:09.452199 22683591385216 run.py:483] Algo bellman_ford step 2609 current loss 0.128630, current_train_items 83520.
I0302 18:59:09.470341 22683591385216 run.py:483] Algo bellman_ford step 2610 current loss 0.011411, current_train_items 83552.
I0302 18:59:09.485606 22683591385216 run.py:483] Algo bellman_ford step 2611 current loss 0.018596, current_train_items 83584.
I0302 18:59:09.507294 22683591385216 run.py:483] Algo bellman_ford step 2612 current loss 0.051364, current_train_items 83616.
I0302 18:59:09.536081 22683591385216 run.py:483] Algo bellman_ford step 2613 current loss 0.094707, current_train_items 83648.
I0302 18:59:09.566036 22683591385216 run.py:483] Algo bellman_ford step 2614 current loss 0.096919, current_train_items 83680.
I0302 18:59:09.584127 22683591385216 run.py:483] Algo bellman_ford step 2615 current loss 0.006649, current_train_items 83712.
I0302 18:59:09.599095 22683591385216 run.py:483] Algo bellman_ford step 2616 current loss 0.019593, current_train_items 83744.
I0302 18:59:09.622140 22683591385216 run.py:483] Algo bellman_ford step 2617 current loss 0.070314, current_train_items 83776.
I0302 18:59:09.652186 22683591385216 run.py:483] Algo bellman_ford step 2618 current loss 0.089455, current_train_items 83808.
I0302 18:59:09.681815 22683591385216 run.py:483] Algo bellman_ford step 2619 current loss 0.083478, current_train_items 83840.
I0302 18:59:09.699976 22683591385216 run.py:483] Algo bellman_ford step 2620 current loss 0.004389, current_train_items 83872.
I0302 18:59:09.715281 22683591385216 run.py:483] Algo bellman_ford step 2621 current loss 0.012446, current_train_items 83904.
I0302 18:59:09.737542 22683591385216 run.py:483] Algo bellman_ford step 2622 current loss 0.046180, current_train_items 83936.
I0302 18:59:09.766279 22683591385216 run.py:483] Algo bellman_ford step 2623 current loss 0.061021, current_train_items 83968.
I0302 18:59:09.796532 22683591385216 run.py:483] Algo bellman_ford step 2624 current loss 0.064138, current_train_items 84000.
I0302 18:59:09.814648 22683591385216 run.py:483] Algo bellman_ford step 2625 current loss 0.006488, current_train_items 84032.
I0302 18:59:09.829880 22683591385216 run.py:483] Algo bellman_ford step 2626 current loss 0.017937, current_train_items 84064.
I0302 18:59:09.852441 22683591385216 run.py:483] Algo bellman_ford step 2627 current loss 0.035002, current_train_items 84096.
I0302 18:59:09.881460 22683591385216 run.py:483] Algo bellman_ford step 2628 current loss 0.108056, current_train_items 84128.
I0302 18:59:09.912784 22683591385216 run.py:483] Algo bellman_ford step 2629 current loss 0.080770, current_train_items 84160.
I0302 18:59:09.930728 22683591385216 run.py:483] Algo bellman_ford step 2630 current loss 0.017155, current_train_items 84192.
I0302 18:59:09.946118 22683591385216 run.py:483] Algo bellman_ford step 2631 current loss 0.027229, current_train_items 84224.
I0302 18:59:09.967959 22683591385216 run.py:483] Algo bellman_ford step 2632 current loss 0.047316, current_train_items 84256.
I0302 18:59:09.996013 22683591385216 run.py:483] Algo bellman_ford step 2633 current loss 0.033665, current_train_items 84288.
I0302 18:59:10.029105 22683591385216 run.py:483] Algo bellman_ford step 2634 current loss 0.119651, current_train_items 84320.
I0302 18:59:10.047274 22683591385216 run.py:483] Algo bellman_ford step 2635 current loss 0.004495, current_train_items 84352.
I0302 18:59:10.062363 22683591385216 run.py:483] Algo bellman_ford step 2636 current loss 0.038512, current_train_items 84384.
I0302 18:59:10.085445 22683591385216 run.py:483] Algo bellman_ford step 2637 current loss 0.063884, current_train_items 84416.
I0302 18:59:10.115092 22683591385216 run.py:483] Algo bellman_ford step 2638 current loss 0.061450, current_train_items 84448.
I0302 18:59:10.146643 22683591385216 run.py:483] Algo bellman_ford step 2639 current loss 0.155300, current_train_items 84480.
I0302 18:59:10.165090 22683591385216 run.py:483] Algo bellman_ford step 2640 current loss 0.002272, current_train_items 84512.
I0302 18:59:10.180239 22683591385216 run.py:483] Algo bellman_ford step 2641 current loss 0.024365, current_train_items 84544.
I0302 18:59:10.201405 22683591385216 run.py:483] Algo bellman_ford step 2642 current loss 0.052876, current_train_items 84576.
I0302 18:59:10.230020 22683591385216 run.py:483] Algo bellman_ford step 2643 current loss 0.073133, current_train_items 84608.
I0302 18:59:10.262260 22683591385216 run.py:483] Algo bellman_ford step 2644 current loss 0.108769, current_train_items 84640.
I0302 18:59:10.280533 22683591385216 run.py:483] Algo bellman_ford step 2645 current loss 0.009168, current_train_items 84672.
I0302 18:59:10.295649 22683591385216 run.py:483] Algo bellman_ford step 2646 current loss 0.042513, current_train_items 84704.
I0302 18:59:10.317511 22683591385216 run.py:483] Algo bellman_ford step 2647 current loss 0.071285, current_train_items 84736.
I0302 18:59:10.346744 22683591385216 run.py:483] Algo bellman_ford step 2648 current loss 0.049436, current_train_items 84768.
I0302 18:59:10.380177 22683591385216 run.py:483] Algo bellman_ford step 2649 current loss 0.094428, current_train_items 84800.
I0302 18:59:10.398687 22683591385216 run.py:483] Algo bellman_ford step 2650 current loss 0.005960, current_train_items 84832.
I0302 18:59:10.406627 22683591385216 run.py:503] (val) algo bellman_ford step 2650: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 84832, 'step': 2650, 'algorithm': 'bellman_ford'}
I0302 18:59:10.406733 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:10.422454 22683591385216 run.py:483] Algo bellman_ford step 2651 current loss 0.033297, current_train_items 84864.
I0302 18:59:10.445080 22683591385216 run.py:483] Algo bellman_ford step 2652 current loss 0.088104, current_train_items 84896.
I0302 18:59:10.475663 22683591385216 run.py:483] Algo bellman_ford step 2653 current loss 0.119848, current_train_items 84928.
I0302 18:59:10.506786 22683591385216 run.py:483] Algo bellman_ford step 2654 current loss 0.085252, current_train_items 84960.
I0302 18:59:10.525263 22683591385216 run.py:483] Algo bellman_ford step 2655 current loss 0.023787, current_train_items 84992.
I0302 18:59:10.540589 22683591385216 run.py:483] Algo bellman_ford step 2656 current loss 0.048837, current_train_items 85024.
I0302 18:59:10.562730 22683591385216 run.py:483] Algo bellman_ford step 2657 current loss 0.068574, current_train_items 85056.
I0302 18:59:10.590771 22683591385216 run.py:483] Algo bellman_ford step 2658 current loss 0.071666, current_train_items 85088.
I0302 18:59:10.624323 22683591385216 run.py:483] Algo bellman_ford step 2659 current loss 0.167943, current_train_items 85120.
I0302 18:59:10.642939 22683591385216 run.py:483] Algo bellman_ford step 2660 current loss 0.003721, current_train_items 85152.
I0302 18:59:10.659009 22683591385216 run.py:483] Algo bellman_ford step 2661 current loss 0.020081, current_train_items 85184.
I0302 18:59:10.681051 22683591385216 run.py:483] Algo bellman_ford step 2662 current loss 0.062667, current_train_items 85216.
I0302 18:59:10.710397 22683591385216 run.py:483] Algo bellman_ford step 2663 current loss 0.080211, current_train_items 85248.
I0302 18:59:10.744125 22683591385216 run.py:483] Algo bellman_ford step 2664 current loss 0.117406, current_train_items 85280.
I0302 18:59:10.762332 22683591385216 run.py:483] Algo bellman_ford step 2665 current loss 0.010817, current_train_items 85312.
I0302 18:59:10.777714 22683591385216 run.py:483] Algo bellman_ford step 2666 current loss 0.055528, current_train_items 85344.
I0302 18:59:10.801074 22683591385216 run.py:483] Algo bellman_ford step 2667 current loss 0.066132, current_train_items 85376.
I0302 18:59:10.830508 22683591385216 run.py:483] Algo bellman_ford step 2668 current loss 0.107021, current_train_items 85408.
I0302 18:59:10.861197 22683591385216 run.py:483] Algo bellman_ford step 2669 current loss 0.063024, current_train_items 85440.
I0302 18:59:10.879887 22683591385216 run.py:483] Algo bellman_ford step 2670 current loss 0.004227, current_train_items 85472.
I0302 18:59:10.895642 22683591385216 run.py:483] Algo bellman_ford step 2671 current loss 0.017360, current_train_items 85504.
I0302 18:59:10.917920 22683591385216 run.py:483] Algo bellman_ford step 2672 current loss 0.075373, current_train_items 85536.
I0302 18:59:10.948018 22683591385216 run.py:483] Algo bellman_ford step 2673 current loss 0.084639, current_train_items 85568.
I0302 18:59:10.981034 22683591385216 run.py:483] Algo bellman_ford step 2674 current loss 0.061042, current_train_items 85600.
I0302 18:59:10.999674 22683591385216 run.py:483] Algo bellman_ford step 2675 current loss 0.012267, current_train_items 85632.
I0302 18:59:11.015161 22683591385216 run.py:483] Algo bellman_ford step 2676 current loss 0.031571, current_train_items 85664.
I0302 18:59:11.038132 22683591385216 run.py:483] Algo bellman_ford step 2677 current loss 0.101946, current_train_items 85696.
I0302 18:59:11.065748 22683591385216 run.py:483] Algo bellman_ford step 2678 current loss 0.033636, current_train_items 85728.
I0302 18:59:11.096093 22683591385216 run.py:483] Algo bellman_ford step 2679 current loss 0.059989, current_train_items 85760.
I0302 18:59:11.114142 22683591385216 run.py:483] Algo bellman_ford step 2680 current loss 0.005221, current_train_items 85792.
I0302 18:59:11.129517 22683591385216 run.py:483] Algo bellman_ford step 2681 current loss 0.025074, current_train_items 85824.
I0302 18:59:11.152082 22683591385216 run.py:483] Algo bellman_ford step 2682 current loss 0.043785, current_train_items 85856.
I0302 18:59:11.182398 22683591385216 run.py:483] Algo bellman_ford step 2683 current loss 0.088173, current_train_items 85888.
I0302 18:59:11.214057 22683591385216 run.py:483] Algo bellman_ford step 2684 current loss 0.091640, current_train_items 85920.
I0302 18:59:11.232678 22683591385216 run.py:483] Algo bellman_ford step 2685 current loss 0.002507, current_train_items 85952.
I0302 18:59:11.248098 22683591385216 run.py:483] Algo bellman_ford step 2686 current loss 0.028733, current_train_items 85984.
I0302 18:59:11.270595 22683591385216 run.py:483] Algo bellman_ford step 2687 current loss 0.064564, current_train_items 86016.
I0302 18:59:11.299666 22683591385216 run.py:483] Algo bellman_ford step 2688 current loss 0.094090, current_train_items 86048.
I0302 18:59:11.329751 22683591385216 run.py:483] Algo bellman_ford step 2689 current loss 0.083631, current_train_items 86080.
I0302 18:59:11.348758 22683591385216 run.py:483] Algo bellman_ford step 2690 current loss 0.017758, current_train_items 86112.
I0302 18:59:11.364343 22683591385216 run.py:483] Algo bellman_ford step 2691 current loss 0.022112, current_train_items 86144.
I0302 18:59:11.387740 22683591385216 run.py:483] Algo bellman_ford step 2692 current loss 0.075933, current_train_items 86176.
I0302 18:59:11.418035 22683591385216 run.py:483] Algo bellman_ford step 2693 current loss 0.088019, current_train_items 86208.
I0302 18:59:11.449799 22683591385216 run.py:483] Algo bellman_ford step 2694 current loss 0.080776, current_train_items 86240.
I0302 18:59:11.468092 22683591385216 run.py:483] Algo bellman_ford step 2695 current loss 0.004217, current_train_items 86272.
I0302 18:59:11.483138 22683591385216 run.py:483] Algo bellman_ford step 2696 current loss 0.012612, current_train_items 86304.
I0302 18:59:11.505956 22683591385216 run.py:483] Algo bellman_ford step 2697 current loss 0.085064, current_train_items 86336.
I0302 18:59:11.534608 22683591385216 run.py:483] Algo bellman_ford step 2698 current loss 0.091306, current_train_items 86368.
I0302 18:59:11.564418 22683591385216 run.py:483] Algo bellman_ford step 2699 current loss 0.103332, current_train_items 86400.
I0302 18:59:11.583182 22683591385216 run.py:483] Algo bellman_ford step 2700 current loss 0.013967, current_train_items 86432.
I0302 18:59:11.590748 22683591385216 run.py:503] (val) algo bellman_ford step 2700: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 86432, 'step': 2700, 'algorithm': 'bellman_ford'}
I0302 18:59:11.590857 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:11.606771 22683591385216 run.py:483] Algo bellman_ford step 2701 current loss 0.020920, current_train_items 86464.
I0302 18:59:11.629584 22683591385216 run.py:483] Algo bellman_ford step 2702 current loss 0.040738, current_train_items 86496.
I0302 18:59:11.661083 22683591385216 run.py:483] Algo bellman_ford step 2703 current loss 0.046973, current_train_items 86528.
I0302 18:59:11.694970 22683591385216 run.py:483] Algo bellman_ford step 2704 current loss 0.106034, current_train_items 86560.
I0302 18:59:11.713056 22683591385216 run.py:483] Algo bellman_ford step 2705 current loss 0.003370, current_train_items 86592.
I0302 18:59:11.728089 22683591385216 run.py:483] Algo bellman_ford step 2706 current loss 0.049190, current_train_items 86624.
I0302 18:59:11.750639 22683591385216 run.py:483] Algo bellman_ford step 2707 current loss 0.060517, current_train_items 86656.
I0302 18:59:11.778160 22683591385216 run.py:483] Algo bellman_ford step 2708 current loss 0.071156, current_train_items 86688.
I0302 18:59:11.807923 22683591385216 run.py:483] Algo bellman_ford step 2709 current loss 0.064731, current_train_items 86720.
I0302 18:59:11.826349 22683591385216 run.py:483] Algo bellman_ford step 2710 current loss 0.005431, current_train_items 86752.
I0302 18:59:11.841986 22683591385216 run.py:483] Algo bellman_ford step 2711 current loss 0.029331, current_train_items 86784.
I0302 18:59:11.864784 22683591385216 run.py:483] Algo bellman_ford step 2712 current loss 0.062445, current_train_items 86816.
I0302 18:59:11.894672 22683591385216 run.py:483] Algo bellman_ford step 2713 current loss 0.074235, current_train_items 86848.
I0302 18:59:11.928061 22683591385216 run.py:483] Algo bellman_ford step 2714 current loss 0.119189, current_train_items 86880.
I0302 18:59:11.946523 22683591385216 run.py:483] Algo bellman_ford step 2715 current loss 0.026251, current_train_items 86912.
I0302 18:59:11.961306 22683591385216 run.py:483] Algo bellman_ford step 2716 current loss 0.011452, current_train_items 86944.
I0302 18:59:11.984445 22683591385216 run.py:483] Algo bellman_ford step 2717 current loss 0.127967, current_train_items 86976.
I0302 18:59:12.011735 22683591385216 run.py:483] Algo bellman_ford step 2718 current loss 0.074608, current_train_items 87008.
I0302 18:59:12.042972 22683591385216 run.py:483] Algo bellman_ford step 2719 current loss 0.099637, current_train_items 87040.
I0302 18:59:12.060808 22683591385216 run.py:483] Algo bellman_ford step 2720 current loss 0.007215, current_train_items 87072.
I0302 18:59:12.076077 22683591385216 run.py:483] Algo bellman_ford step 2721 current loss 0.066809, current_train_items 87104.
I0302 18:59:12.098016 22683591385216 run.py:483] Algo bellman_ford step 2722 current loss 0.072076, current_train_items 87136.
I0302 18:59:12.128368 22683591385216 run.py:483] Algo bellman_ford step 2723 current loss 0.112016, current_train_items 87168.
I0302 18:59:12.159468 22683591385216 run.py:483] Algo bellman_ford step 2724 current loss 0.090056, current_train_items 87200.
I0302 18:59:12.177774 22683591385216 run.py:483] Algo bellman_ford step 2725 current loss 0.004116, current_train_items 87232.
I0302 18:59:12.192852 22683591385216 run.py:483] Algo bellman_ford step 2726 current loss 0.033981, current_train_items 87264.
I0302 18:59:12.215389 22683591385216 run.py:483] Algo bellman_ford step 2727 current loss 0.080404, current_train_items 87296.
I0302 18:59:12.244773 22683591385216 run.py:483] Algo bellman_ford step 2728 current loss 0.111134, current_train_items 87328.
I0302 18:59:12.275692 22683591385216 run.py:483] Algo bellman_ford step 2729 current loss 0.084283, current_train_items 87360.
I0302 18:59:12.293911 22683591385216 run.py:483] Algo bellman_ford step 2730 current loss 0.002177, current_train_items 87392.
I0302 18:59:12.309113 22683591385216 run.py:483] Algo bellman_ford step 2731 current loss 0.019326, current_train_items 87424.
I0302 18:59:12.330379 22683591385216 run.py:483] Algo bellman_ford step 2732 current loss 0.028332, current_train_items 87456.
I0302 18:59:12.358160 22683591385216 run.py:483] Algo bellman_ford step 2733 current loss 0.057788, current_train_items 87488.
I0302 18:59:12.389683 22683591385216 run.py:483] Algo bellman_ford step 2734 current loss 0.095398, current_train_items 87520.
I0302 18:59:12.407782 22683591385216 run.py:483] Algo bellman_ford step 2735 current loss 0.017573, current_train_items 87552.
I0302 18:59:12.422532 22683591385216 run.py:483] Algo bellman_ford step 2736 current loss 0.025009, current_train_items 87584.
I0302 18:59:12.444392 22683591385216 run.py:483] Algo bellman_ford step 2737 current loss 0.052514, current_train_items 87616.
I0302 18:59:12.472583 22683591385216 run.py:483] Algo bellman_ford step 2738 current loss 0.047562, current_train_items 87648.
I0302 18:59:12.502667 22683591385216 run.py:483] Algo bellman_ford step 2739 current loss 0.086566, current_train_items 87680.
I0302 18:59:12.520721 22683591385216 run.py:483] Algo bellman_ford step 2740 current loss 0.004537, current_train_items 87712.
I0302 18:59:12.535783 22683591385216 run.py:483] Algo bellman_ford step 2741 current loss 0.020089, current_train_items 87744.
I0302 18:59:12.557960 22683591385216 run.py:483] Algo bellman_ford step 2742 current loss 0.061922, current_train_items 87776.
I0302 18:59:12.588427 22683591385216 run.py:483] Algo bellman_ford step 2743 current loss 0.092970, current_train_items 87808.
I0302 18:59:12.619181 22683591385216 run.py:483] Algo bellman_ford step 2744 current loss 0.088133, current_train_items 87840.
I0302 18:59:12.637135 22683591385216 run.py:483] Algo bellman_ford step 2745 current loss 0.004828, current_train_items 87872.
I0302 18:59:12.652160 22683591385216 run.py:483] Algo bellman_ford step 2746 current loss 0.021458, current_train_items 87904.
I0302 18:59:12.674352 22683591385216 run.py:483] Algo bellman_ford step 2747 current loss 0.071604, current_train_items 87936.
I0302 18:59:12.704117 22683591385216 run.py:483] Algo bellman_ford step 2748 current loss 0.066291, current_train_items 87968.
I0302 18:59:12.734792 22683591385216 run.py:483] Algo bellman_ford step 2749 current loss 0.062670, current_train_items 88000.
I0302 18:59:12.752862 22683591385216 run.py:483] Algo bellman_ford step 2750 current loss 0.007868, current_train_items 88032.
I0302 18:59:12.760824 22683591385216 run.py:503] (val) algo bellman_ford step 2750: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 88032, 'step': 2750, 'algorithm': 'bellman_ford'}
I0302 18:59:12.760940 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:12.776916 22683591385216 run.py:483] Algo bellman_ford step 2751 current loss 0.021234, current_train_items 88064.
I0302 18:59:12.799993 22683591385216 run.py:483] Algo bellman_ford step 2752 current loss 0.105708, current_train_items 88096.
I0302 18:59:12.830086 22683591385216 run.py:483] Algo bellman_ford step 2753 current loss 0.143423, current_train_items 88128.
I0302 18:59:12.860605 22683591385216 run.py:483] Algo bellman_ford step 2754 current loss 0.087068, current_train_items 88160.
I0302 18:59:12.878785 22683591385216 run.py:483] Algo bellman_ford step 2755 current loss 0.006116, current_train_items 88192.
I0302 18:59:12.894332 22683591385216 run.py:483] Algo bellman_ford step 2756 current loss 0.036696, current_train_items 88224.
I0302 18:59:12.916074 22683591385216 run.py:483] Algo bellman_ford step 2757 current loss 0.037113, current_train_items 88256.
I0302 18:59:12.945674 22683591385216 run.py:483] Algo bellman_ford step 2758 current loss 0.121185, current_train_items 88288.
I0302 18:59:12.979203 22683591385216 run.py:483] Algo bellman_ford step 2759 current loss 0.175687, current_train_items 88320.
I0302 18:59:12.997808 22683591385216 run.py:483] Algo bellman_ford step 2760 current loss 0.004384, current_train_items 88352.
I0302 18:59:13.013753 22683591385216 run.py:483] Algo bellman_ford step 2761 current loss 0.022448, current_train_items 88384.
I0302 18:59:13.036248 22683591385216 run.py:483] Algo bellman_ford step 2762 current loss 0.172858, current_train_items 88416.
I0302 18:59:13.065400 22683591385216 run.py:483] Algo bellman_ford step 2763 current loss 0.172412, current_train_items 88448.
I0302 18:59:13.096607 22683591385216 run.py:483] Algo bellman_ford step 2764 current loss 0.165892, current_train_items 88480.
I0302 18:59:13.114740 22683591385216 run.py:483] Algo bellman_ford step 2765 current loss 0.015522, current_train_items 88512.
I0302 18:59:13.130291 22683591385216 run.py:483] Algo bellman_ford step 2766 current loss 0.044378, current_train_items 88544.
I0302 18:59:13.152554 22683591385216 run.py:483] Algo bellman_ford step 2767 current loss 0.047169, current_train_items 88576.
I0302 18:59:13.182546 22683591385216 run.py:483] Algo bellman_ford step 2768 current loss 0.156649, current_train_items 88608.
I0302 18:59:13.213571 22683591385216 run.py:483] Algo bellman_ford step 2769 current loss 0.104222, current_train_items 88640.
I0302 18:59:13.232476 22683591385216 run.py:483] Algo bellman_ford step 2770 current loss 0.002228, current_train_items 88672.
I0302 18:59:13.248243 22683591385216 run.py:483] Algo bellman_ford step 2771 current loss 0.025574, current_train_items 88704.
I0302 18:59:13.270806 22683591385216 run.py:483] Algo bellman_ford step 2772 current loss 0.056585, current_train_items 88736.
I0302 18:59:13.299098 22683591385216 run.py:483] Algo bellman_ford step 2773 current loss 0.068588, current_train_items 88768.
I0302 18:59:13.330260 22683591385216 run.py:483] Algo bellman_ford step 2774 current loss 0.059683, current_train_items 88800.
I0302 18:59:13.348690 22683591385216 run.py:483] Algo bellman_ford step 2775 current loss 0.001952, current_train_items 88832.
I0302 18:59:13.364749 22683591385216 run.py:483] Algo bellman_ford step 2776 current loss 0.029427, current_train_items 88864.
I0302 18:59:13.386541 22683591385216 run.py:483] Algo bellman_ford step 2777 current loss 0.060505, current_train_items 88896.
I0302 18:59:13.415414 22683591385216 run.py:483] Algo bellman_ford step 2778 current loss 0.133490, current_train_items 88928.
I0302 18:59:13.447569 22683591385216 run.py:483] Algo bellman_ford step 2779 current loss 0.097896, current_train_items 88960.
I0302 18:59:13.465752 22683591385216 run.py:483] Algo bellman_ford step 2780 current loss 0.007210, current_train_items 88992.
I0302 18:59:13.481001 22683591385216 run.py:483] Algo bellman_ford step 2781 current loss 0.059223, current_train_items 89024.
I0302 18:59:13.503631 22683591385216 run.py:483] Algo bellman_ford step 2782 current loss 0.177551, current_train_items 89056.
I0302 18:59:13.532776 22683591385216 run.py:483] Algo bellman_ford step 2783 current loss 0.127098, current_train_items 89088.
I0302 18:59:13.564596 22683591385216 run.py:483] Algo bellman_ford step 2784 current loss 0.152379, current_train_items 89120.
I0302 18:59:13.583258 22683591385216 run.py:483] Algo bellman_ford step 2785 current loss 0.003750, current_train_items 89152.
I0302 18:59:13.598721 22683591385216 run.py:483] Algo bellman_ford step 2786 current loss 0.050122, current_train_items 89184.
I0302 18:59:13.619566 22683591385216 run.py:483] Algo bellman_ford step 2787 current loss 0.031930, current_train_items 89216.
I0302 18:59:13.647808 22683591385216 run.py:483] Algo bellman_ford step 2788 current loss 0.074526, current_train_items 89248.
I0302 18:59:13.676212 22683591385216 run.py:483] Algo bellman_ford step 2789 current loss 0.079139, current_train_items 89280.
I0302 18:59:13.694825 22683591385216 run.py:483] Algo bellman_ford step 2790 current loss 0.005427, current_train_items 89312.
I0302 18:59:13.710293 22683591385216 run.py:483] Algo bellman_ford step 2791 current loss 0.024120, current_train_items 89344.
I0302 18:59:13.732291 22683591385216 run.py:483] Algo bellman_ford step 2792 current loss 0.046629, current_train_items 89376.
I0302 18:59:13.760579 22683591385216 run.py:483] Algo bellman_ford step 2793 current loss 0.084401, current_train_items 89408.
I0302 18:59:13.794255 22683591385216 run.py:483] Algo bellman_ford step 2794 current loss 0.097639, current_train_items 89440.
I0302 18:59:13.812426 22683591385216 run.py:483] Algo bellman_ford step 2795 current loss 0.004259, current_train_items 89472.
I0302 18:59:13.828179 22683591385216 run.py:483] Algo bellman_ford step 2796 current loss 0.038528, current_train_items 89504.
I0302 18:59:13.850120 22683591385216 run.py:483] Algo bellman_ford step 2797 current loss 0.144549, current_train_items 89536.
I0302 18:59:13.880200 22683591385216 run.py:483] Algo bellman_ford step 2798 current loss 0.124717, current_train_items 89568.
I0302 18:59:13.911547 22683591385216 run.py:483] Algo bellman_ford step 2799 current loss 0.099858, current_train_items 89600.
I0302 18:59:13.929940 22683591385216 run.py:483] Algo bellman_ford step 2800 current loss 0.003213, current_train_items 89632.
I0302 18:59:13.937493 22683591385216 run.py:503] (val) algo bellman_ford step 2800: {'pi': 0.974609375, 'score': 0.974609375, 'examples_seen': 89632, 'step': 2800, 'algorithm': 'bellman_ford'}
I0302 18:59:13.937600 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.975, val scores are: bellman_ford: 0.975
I0302 18:59:13.953631 22683591385216 run.py:483] Algo bellman_ford step 2801 current loss 0.026950, current_train_items 89664.
I0302 18:59:13.977127 22683591385216 run.py:483] Algo bellman_ford step 2802 current loss 0.108766, current_train_items 89696.
I0302 18:59:14.007274 22683591385216 run.py:483] Algo bellman_ford step 2803 current loss 0.124189, current_train_items 89728.
I0302 18:59:14.038726 22683591385216 run.py:483] Algo bellman_ford step 2804 current loss 0.076748, current_train_items 89760.
I0302 18:59:14.057502 22683591385216 run.py:483] Algo bellman_ford step 2805 current loss 0.003824, current_train_items 89792.
I0302 18:59:14.073053 22683591385216 run.py:483] Algo bellman_ford step 2806 current loss 0.050724, current_train_items 89824.
I0302 18:59:14.095012 22683591385216 run.py:483] Algo bellman_ford step 2807 current loss 0.077980, current_train_items 89856.
I0302 18:59:14.124830 22683591385216 run.py:483] Algo bellman_ford step 2808 current loss 0.128073, current_train_items 89888.
I0302 18:59:14.156762 22683591385216 run.py:483] Algo bellman_ford step 2809 current loss 0.121676, current_train_items 89920.
I0302 18:59:14.175052 22683591385216 run.py:483] Algo bellman_ford step 2810 current loss 0.005483, current_train_items 89952.
I0302 18:59:14.190765 22683591385216 run.py:483] Algo bellman_ford step 2811 current loss 0.069642, current_train_items 89984.
I0302 18:59:14.212205 22683591385216 run.py:483] Algo bellman_ford step 2812 current loss 0.041406, current_train_items 90016.
I0302 18:59:14.242217 22683591385216 run.py:483] Algo bellman_ford step 2813 current loss 0.121662, current_train_items 90048.
I0302 18:59:14.272844 22683591385216 run.py:483] Algo bellman_ford step 2814 current loss 0.201902, current_train_items 90080.
I0302 18:59:14.290889 22683591385216 run.py:483] Algo bellman_ford step 2815 current loss 0.005804, current_train_items 90112.
I0302 18:59:14.306441 22683591385216 run.py:483] Algo bellman_ford step 2816 current loss 0.026358, current_train_items 90144.
I0302 18:59:14.329366 22683591385216 run.py:483] Algo bellman_ford step 2817 current loss 0.043830, current_train_items 90176.
I0302 18:59:14.355515 22683591385216 run.py:483] Algo bellman_ford step 2818 current loss 0.040966, current_train_items 90208.
I0302 18:59:14.386602 22683591385216 run.py:483] Algo bellman_ford step 2819 current loss 0.076957, current_train_items 90240.
I0302 18:59:14.404874 22683591385216 run.py:483] Algo bellman_ford step 2820 current loss 0.010226, current_train_items 90272.
I0302 18:59:14.420023 22683591385216 run.py:483] Algo bellman_ford step 2821 current loss 0.023138, current_train_items 90304.
I0302 18:59:14.442270 22683591385216 run.py:483] Algo bellman_ford step 2822 current loss 0.073431, current_train_items 90336.
I0302 18:59:14.471082 22683591385216 run.py:483] Algo bellman_ford step 2823 current loss 0.179906, current_train_items 90368.
I0302 18:59:14.505018 22683591385216 run.py:483] Algo bellman_ford step 2824 current loss 0.132169, current_train_items 90400.
I0302 18:59:14.523433 22683591385216 run.py:483] Algo bellman_ford step 2825 current loss 0.003400, current_train_items 90432.
I0302 18:59:14.538337 22683591385216 run.py:483] Algo bellman_ford step 2826 current loss 0.038354, current_train_items 90464.
I0302 18:59:14.561043 22683591385216 run.py:483] Algo bellman_ford step 2827 current loss 0.099897, current_train_items 90496.
I0302 18:59:14.590261 22683591385216 run.py:483] Algo bellman_ford step 2828 current loss 0.088206, current_train_items 90528.
I0302 18:59:14.620442 22683591385216 run.py:483] Algo bellman_ford step 2829 current loss 0.068430, current_train_items 90560.
I0302 18:59:14.638664 22683591385216 run.py:483] Algo bellman_ford step 2830 current loss 0.008837, current_train_items 90592.
I0302 18:59:14.654255 22683591385216 run.py:483] Algo bellman_ford step 2831 current loss 0.043864, current_train_items 90624.
I0302 18:59:14.676474 22683591385216 run.py:483] Algo bellman_ford step 2832 current loss 0.074697, current_train_items 90656.
I0302 18:59:14.704618 22683591385216 run.py:483] Algo bellman_ford step 2833 current loss 0.056252, current_train_items 90688.
I0302 18:59:14.736001 22683591385216 run.py:483] Algo bellman_ford step 2834 current loss 0.122225, current_train_items 90720.
I0302 18:59:14.754061 22683591385216 run.py:483] Algo bellman_ford step 2835 current loss 0.017823, current_train_items 90752.
I0302 18:59:14.768921 22683591385216 run.py:483] Algo bellman_ford step 2836 current loss 0.045112, current_train_items 90784.
I0302 18:59:14.790574 22683591385216 run.py:483] Algo bellman_ford step 2837 current loss 0.061980, current_train_items 90816.
I0302 18:59:14.819652 22683591385216 run.py:483] Algo bellman_ford step 2838 current loss 0.065038, current_train_items 90848.
I0302 18:59:14.853559 22683591385216 run.py:483] Algo bellman_ford step 2839 current loss 0.100855, current_train_items 90880.
I0302 18:59:14.872039 22683591385216 run.py:483] Algo bellman_ford step 2840 current loss 0.010706, current_train_items 90912.
I0302 18:59:14.886987 22683591385216 run.py:483] Algo bellman_ford step 2841 current loss 0.018730, current_train_items 90944.
I0302 18:59:14.909452 22683591385216 run.py:483] Algo bellman_ford step 2842 current loss 0.067138, current_train_items 90976.
I0302 18:59:14.939406 22683591385216 run.py:483] Algo bellman_ford step 2843 current loss 0.101439, current_train_items 91008.
I0302 18:59:14.970960 22683591385216 run.py:483] Algo bellman_ford step 2844 current loss 0.099801, current_train_items 91040.
I0302 18:59:14.989456 22683591385216 run.py:483] Algo bellman_ford step 2845 current loss 0.002069, current_train_items 91072.
I0302 18:59:15.005163 22683591385216 run.py:483] Algo bellman_ford step 2846 current loss 0.030911, current_train_items 91104.
I0302 18:59:15.027204 22683591385216 run.py:483] Algo bellman_ford step 2847 current loss 0.028697, current_train_items 91136.
I0302 18:59:15.056018 22683591385216 run.py:483] Algo bellman_ford step 2848 current loss 0.081288, current_train_items 91168.
I0302 18:59:15.088984 22683591385216 run.py:483] Algo bellman_ford step 2849 current loss 0.125620, current_train_items 91200.
I0302 18:59:15.107429 22683591385216 run.py:483] Algo bellman_ford step 2850 current loss 0.002396, current_train_items 91232.
I0302 18:59:15.115379 22683591385216 run.py:503] (val) algo bellman_ford step 2850: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 91232, 'step': 2850, 'algorithm': 'bellman_ford'}
I0302 18:59:15.115485 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:59:15.130861 22683591385216 run.py:483] Algo bellman_ford step 2851 current loss 0.008731, current_train_items 91264.
I0302 18:59:15.153270 22683591385216 run.py:483] Algo bellman_ford step 2852 current loss 0.033262, current_train_items 91296.
I0302 18:59:15.183444 22683591385216 run.py:483] Algo bellman_ford step 2853 current loss 0.057973, current_train_items 91328.
I0302 18:59:15.215201 22683591385216 run.py:483] Algo bellman_ford step 2854 current loss 0.096655, current_train_items 91360.
I0302 18:59:15.233575 22683591385216 run.py:483] Algo bellman_ford step 2855 current loss 0.002105, current_train_items 91392.
I0302 18:59:15.248877 22683591385216 run.py:483] Algo bellman_ford step 2856 current loss 0.042276, current_train_items 91424.
I0302 18:59:15.271419 22683591385216 run.py:483] Algo bellman_ford step 2857 current loss 0.072837, current_train_items 91456.
I0302 18:59:15.298611 22683591385216 run.py:483] Algo bellman_ford step 2858 current loss 0.066047, current_train_items 91488.
I0302 18:59:15.328843 22683591385216 run.py:483] Algo bellman_ford step 2859 current loss 0.064693, current_train_items 91520.
I0302 18:59:15.347582 22683591385216 run.py:483] Algo bellman_ford step 2860 current loss 0.002139, current_train_items 91552.
I0302 18:59:15.363256 22683591385216 run.py:483] Algo bellman_ford step 2861 current loss 0.021743, current_train_items 91584.
I0302 18:59:15.385621 22683591385216 run.py:483] Algo bellman_ford step 2862 current loss 0.074419, current_train_items 91616.
I0302 18:59:15.415173 22683591385216 run.py:483] Algo bellman_ford step 2863 current loss 0.107121, current_train_items 91648.
I0302 18:59:15.443807 22683591385216 run.py:483] Algo bellman_ford step 2864 current loss 0.116759, current_train_items 91680.
I0302 18:59:15.462450 22683591385216 run.py:483] Algo bellman_ford step 2865 current loss 0.004003, current_train_items 91712.
I0302 18:59:15.477758 22683591385216 run.py:483] Algo bellman_ford step 2866 current loss 0.029597, current_train_items 91744.
I0302 18:59:15.500062 22683591385216 run.py:483] Algo bellman_ford step 2867 current loss 0.067650, current_train_items 91776.
I0302 18:59:15.528313 22683591385216 run.py:483] Algo bellman_ford step 2868 current loss 0.052139, current_train_items 91808.
I0302 18:59:15.560284 22683591385216 run.py:483] Algo bellman_ford step 2869 current loss 0.115957, current_train_items 91840.
I0302 18:59:15.578933 22683591385216 run.py:483] Algo bellman_ford step 2870 current loss 0.001215, current_train_items 91872.
I0302 18:59:15.594549 22683591385216 run.py:483] Algo bellman_ford step 2871 current loss 0.023949, current_train_items 91904.
I0302 18:59:15.617315 22683591385216 run.py:483] Algo bellman_ford step 2872 current loss 0.054748, current_train_items 91936.
I0302 18:59:15.647093 22683591385216 run.py:483] Algo bellman_ford step 2873 current loss 0.045876, current_train_items 91968.
I0302 18:59:15.677705 22683591385216 run.py:483] Algo bellman_ford step 2874 current loss 0.077785, current_train_items 92000.
I0302 18:59:15.696343 22683591385216 run.py:483] Algo bellman_ford step 2875 current loss 0.002269, current_train_items 92032.
I0302 18:59:15.711831 22683591385216 run.py:483] Algo bellman_ford step 2876 current loss 0.027015, current_train_items 92064.
I0302 18:59:15.733501 22683591385216 run.py:483] Algo bellman_ford step 2877 current loss 0.024333, current_train_items 92096.
I0302 18:59:15.763401 22683591385216 run.py:483] Algo bellman_ford step 2878 current loss 0.129584, current_train_items 92128.
I0302 18:59:15.793987 22683591385216 run.py:483] Algo bellman_ford step 2879 current loss 0.056753, current_train_items 92160.
I0302 18:59:15.812151 22683591385216 run.py:483] Algo bellman_ford step 2880 current loss 0.031665, current_train_items 92192.
I0302 18:59:15.827353 22683591385216 run.py:483] Algo bellman_ford step 2881 current loss 0.011411, current_train_items 92224.
I0302 18:59:15.849856 22683591385216 run.py:483] Algo bellman_ford step 2882 current loss 0.048093, current_train_items 92256.
I0302 18:59:15.879378 22683591385216 run.py:483] Algo bellman_ford step 2883 current loss 0.077673, current_train_items 92288.
I0302 18:59:15.908701 22683591385216 run.py:483] Algo bellman_ford step 2884 current loss 0.069020, current_train_items 92320.
I0302 18:59:15.927154 22683591385216 run.py:483] Algo bellman_ford step 2885 current loss 0.003229, current_train_items 92352.
I0302 18:59:15.942600 22683591385216 run.py:483] Algo bellman_ford step 2886 current loss 0.017483, current_train_items 92384.
I0302 18:59:15.964818 22683591385216 run.py:483] Algo bellman_ford step 2887 current loss 0.052395, current_train_items 92416.
I0302 18:59:15.993068 22683591385216 run.py:483] Algo bellman_ford step 2888 current loss 0.044274, current_train_items 92448.
I0302 18:59:16.025005 22683591385216 run.py:483] Algo bellman_ford step 2889 current loss 0.095622, current_train_items 92480.
I0302 18:59:16.043832 22683591385216 run.py:483] Algo bellman_ford step 2890 current loss 0.003195, current_train_items 92512.
I0302 18:59:16.059311 22683591385216 run.py:483] Algo bellman_ford step 2891 current loss 0.010923, current_train_items 92544.
I0302 18:59:16.082808 22683591385216 run.py:483] Algo bellman_ford step 2892 current loss 0.037163, current_train_items 92576.
I0302 18:59:16.110770 22683591385216 run.py:483] Algo bellman_ford step 2893 current loss 0.062093, current_train_items 92608.
I0302 18:59:16.142358 22683591385216 run.py:483] Algo bellman_ford step 2894 current loss 0.117288, current_train_items 92640.
I0302 18:59:16.160547 22683591385216 run.py:483] Algo bellman_ford step 2895 current loss 0.004429, current_train_items 92672.
I0302 18:59:16.176373 22683591385216 run.py:483] Algo bellman_ford step 2896 current loss 0.071096, current_train_items 92704.
I0302 18:59:16.197460 22683591385216 run.py:483] Algo bellman_ford step 2897 current loss 0.076232, current_train_items 92736.
I0302 18:59:16.225846 22683591385216 run.py:483] Algo bellman_ford step 2898 current loss 0.038061, current_train_items 92768.
I0302 18:59:16.254633 22683591385216 run.py:483] Algo bellman_ford step 2899 current loss 0.059541, current_train_items 92800.
I0302 18:59:16.273095 22683591385216 run.py:483] Algo bellman_ford step 2900 current loss 0.001709, current_train_items 92832.
I0302 18:59:16.280734 22683591385216 run.py:503] (val) algo bellman_ford step 2900: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 92832, 'step': 2900, 'algorithm': 'bellman_ford'}
I0302 18:59:16.280842 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 18:59:16.296233 22683591385216 run.py:483] Algo bellman_ford step 2901 current loss 0.004483, current_train_items 92864.
I0302 18:59:16.318304 22683591385216 run.py:483] Algo bellman_ford step 2902 current loss 0.068664, current_train_items 92896.
I0302 18:59:16.347660 22683591385216 run.py:483] Algo bellman_ford step 2903 current loss 0.053661, current_train_items 92928.
I0302 18:59:16.379634 22683591385216 run.py:483] Algo bellman_ford step 2904 current loss 0.082884, current_train_items 92960.
I0302 18:59:16.397873 22683591385216 run.py:483] Algo bellman_ford step 2905 current loss 0.008291, current_train_items 92992.
I0302 18:59:16.413138 22683591385216 run.py:483] Algo bellman_ford step 2906 current loss 0.015654, current_train_items 93024.
I0302 18:59:16.435595 22683591385216 run.py:483] Algo bellman_ford step 2907 current loss 0.071428, current_train_items 93056.
I0302 18:59:16.465950 22683591385216 run.py:483] Algo bellman_ford step 2908 current loss 0.108236, current_train_items 93088.
I0302 18:59:16.498035 22683591385216 run.py:483] Algo bellman_ford step 2909 current loss 0.095862, current_train_items 93120.
I0302 18:59:16.516283 22683591385216 run.py:483] Algo bellman_ford step 2910 current loss 0.020380, current_train_items 93152.
I0302 18:59:16.532081 22683591385216 run.py:483] Algo bellman_ford step 2911 current loss 0.039220, current_train_items 93184.
I0302 18:59:16.554282 22683591385216 run.py:483] Algo bellman_ford step 2912 current loss 0.077242, current_train_items 93216.
I0302 18:59:16.583063 22683591385216 run.py:483] Algo bellman_ford step 2913 current loss 0.095528, current_train_items 93248.
I0302 18:59:16.615597 22683591385216 run.py:483] Algo bellman_ford step 2914 current loss 0.138240, current_train_items 93280.
I0302 18:59:16.633566 22683591385216 run.py:483] Algo bellman_ford step 2915 current loss 0.004232, current_train_items 93312.
I0302 18:59:16.648774 22683591385216 run.py:483] Algo bellman_ford step 2916 current loss 0.009326, current_train_items 93344.
I0302 18:59:16.670701 22683591385216 run.py:483] Algo bellman_ford step 2917 current loss 0.110654, current_train_items 93376.
I0302 18:59:16.701219 22683591385216 run.py:483] Algo bellman_ford step 2918 current loss 0.124710, current_train_items 93408.
I0302 18:59:16.734180 22683591385216 run.py:483] Algo bellman_ford step 2919 current loss 0.124580, current_train_items 93440.
I0302 18:59:16.752483 22683591385216 run.py:483] Algo bellman_ford step 2920 current loss 0.002180, current_train_items 93472.
I0302 18:59:16.767994 22683591385216 run.py:483] Algo bellman_ford step 2921 current loss 0.040862, current_train_items 93504.
I0302 18:59:16.790893 22683591385216 run.py:483] Algo bellman_ford step 2922 current loss 0.072401, current_train_items 93536.
I0302 18:59:16.819337 22683591385216 run.py:483] Algo bellman_ford step 2923 current loss 0.063039, current_train_items 93568.
I0302 18:59:16.852771 22683591385216 run.py:483] Algo bellman_ford step 2924 current loss 0.083701, current_train_items 93600.
I0302 18:59:16.871010 22683591385216 run.py:483] Algo bellman_ford step 2925 current loss 0.011342, current_train_items 93632.
I0302 18:59:16.886096 22683591385216 run.py:483] Algo bellman_ford step 2926 current loss 0.042958, current_train_items 93664.
I0302 18:59:16.909107 22683591385216 run.py:483] Algo bellman_ford step 2927 current loss 0.107656, current_train_items 93696.
I0302 18:59:16.938991 22683591385216 run.py:483] Algo bellman_ford step 2928 current loss 0.179556, current_train_items 93728.
I0302 18:59:16.970804 22683591385216 run.py:483] Algo bellman_ford step 2929 current loss 0.078250, current_train_items 93760.
I0302 18:59:16.988985 22683591385216 run.py:483] Algo bellman_ford step 2930 current loss 0.002186, current_train_items 93792.
I0302 18:59:17.004107 22683591385216 run.py:483] Algo bellman_ford step 2931 current loss 0.010975, current_train_items 93824.
I0302 18:59:17.027768 22683591385216 run.py:483] Algo bellman_ford step 2932 current loss 0.144399, current_train_items 93856.
I0302 18:59:17.055999 22683591385216 run.py:483] Algo bellman_ford step 2933 current loss 0.066271, current_train_items 93888.
I0302 18:59:17.086671 22683591385216 run.py:483] Algo bellman_ford step 2934 current loss 0.076002, current_train_items 93920.
I0302 18:59:17.105077 22683591385216 run.py:483] Algo bellman_ford step 2935 current loss 0.002405, current_train_items 93952.
I0302 18:59:17.120148 22683591385216 run.py:483] Algo bellman_ford step 2936 current loss 0.024560, current_train_items 93984.
I0302 18:59:17.142577 22683591385216 run.py:483] Algo bellman_ford step 2937 current loss 0.079097, current_train_items 94016.
I0302 18:59:17.170044 22683591385216 run.py:483] Algo bellman_ford step 2938 current loss 0.033626, current_train_items 94048.
I0302 18:59:17.201151 22683591385216 run.py:483] Algo bellman_ford step 2939 current loss 0.064702, current_train_items 94080.
I0302 18:59:17.219119 22683591385216 run.py:483] Algo bellman_ford step 2940 current loss 0.004995, current_train_items 94112.
I0302 18:59:17.234616 22683591385216 run.py:483] Algo bellman_ford step 2941 current loss 0.038191, current_train_items 94144.
I0302 18:59:17.257473 22683591385216 run.py:483] Algo bellman_ford step 2942 current loss 0.089948, current_train_items 94176.
I0302 18:59:17.287407 22683591385216 run.py:483] Algo bellman_ford step 2943 current loss 0.113506, current_train_items 94208.
I0302 18:59:17.320294 22683591385216 run.py:483] Algo bellman_ford step 2944 current loss 0.078881, current_train_items 94240.
I0302 18:59:17.338325 22683591385216 run.py:483] Algo bellman_ford step 2945 current loss 0.021624, current_train_items 94272.
I0302 18:59:17.353818 22683591385216 run.py:483] Algo bellman_ford step 2946 current loss 0.030674, current_train_items 94304.
I0302 18:59:17.376134 22683591385216 run.py:483] Algo bellman_ford step 2947 current loss 0.074246, current_train_items 94336.
I0302 18:59:17.405222 22683591385216 run.py:483] Algo bellman_ford step 2948 current loss 0.105032, current_train_items 94368.
I0302 18:59:17.436118 22683591385216 run.py:483] Algo bellman_ford step 2949 current loss 0.071602, current_train_items 94400.
I0302 18:59:17.454232 22683591385216 run.py:483] Algo bellman_ford step 2950 current loss 0.007036, current_train_items 94432.
I0302 18:59:17.462184 22683591385216 run.py:503] (val) algo bellman_ford step 2950: {'pi': 0.9609375, 'score': 0.9609375, 'examples_seen': 94432, 'step': 2950, 'algorithm': 'bellman_ford'}
I0302 18:59:17.462289 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.961, val scores are: bellman_ford: 0.961
I0302 18:59:17.478248 22683591385216 run.py:483] Algo bellman_ford step 2951 current loss 0.035117, current_train_items 94464.
I0302 18:59:17.499451 22683591385216 run.py:483] Algo bellman_ford step 2952 current loss 0.056343, current_train_items 94496.
I0302 18:59:17.528461 22683591385216 run.py:483] Algo bellman_ford step 2953 current loss 0.077344, current_train_items 94528.
I0302 18:59:17.560731 22683591385216 run.py:483] Algo bellman_ford step 2954 current loss 0.088085, current_train_items 94560.
I0302 18:59:17.579062 22683591385216 run.py:483] Algo bellman_ford step 2955 current loss 0.002382, current_train_items 94592.
I0302 18:59:17.593893 22683591385216 run.py:483] Algo bellman_ford step 2956 current loss 0.047958, current_train_items 94624.
I0302 18:59:17.617265 22683591385216 run.py:483] Algo bellman_ford step 2957 current loss 0.111834, current_train_items 94656.
I0302 18:59:17.647606 22683591385216 run.py:483] Algo bellman_ford step 2958 current loss 0.076745, current_train_items 94688.
I0302 18:59:17.679378 22683591385216 run.py:483] Algo bellman_ford step 2959 current loss 0.156347, current_train_items 94720.
I0302 18:59:17.697913 22683591385216 run.py:483] Algo bellman_ford step 2960 current loss 0.015095, current_train_items 94752.
I0302 18:59:17.713697 22683591385216 run.py:483] Algo bellman_ford step 2961 current loss 0.040452, current_train_items 94784.
I0302 18:59:17.735555 22683591385216 run.py:483] Algo bellman_ford step 2962 current loss 0.160787, current_train_items 94816.
I0302 18:59:17.763823 22683591385216 run.py:483] Algo bellman_ford step 2963 current loss 0.078919, current_train_items 94848.
I0302 18:59:17.796130 22683591385216 run.py:483] Algo bellman_ford step 2964 current loss 0.086438, current_train_items 94880.
I0302 18:59:17.814256 22683591385216 run.py:483] Algo bellman_ford step 2965 current loss 0.002881, current_train_items 94912.
I0302 18:59:17.829503 22683591385216 run.py:483] Algo bellman_ford step 2966 current loss 0.032392, current_train_items 94944.
I0302 18:59:17.851842 22683591385216 run.py:483] Algo bellman_ford step 2967 current loss 0.056981, current_train_items 94976.
I0302 18:59:17.880601 22683591385216 run.py:483] Algo bellman_ford step 2968 current loss 0.090197, current_train_items 95008.
I0302 18:59:17.913043 22683591385216 run.py:483] Algo bellman_ford step 2969 current loss 0.148089, current_train_items 95040.
I0302 18:59:17.931607 22683591385216 run.py:483] Algo bellman_ford step 2970 current loss 0.009203, current_train_items 95072.
I0302 18:59:17.947002 22683591385216 run.py:483] Algo bellman_ford step 2971 current loss 0.022823, current_train_items 95104.
I0302 18:59:17.969196 22683591385216 run.py:483] Algo bellman_ford step 2972 current loss 0.077809, current_train_items 95136.
I0302 18:59:17.997546 22683591385216 run.py:483] Algo bellman_ford step 2973 current loss 0.081695, current_train_items 95168.
I0302 18:59:18.032244 22683591385216 run.py:483] Algo bellman_ford step 2974 current loss 0.162757, current_train_items 95200.
I0302 18:59:18.050850 22683591385216 run.py:483] Algo bellman_ford step 2975 current loss 0.005243, current_train_items 95232.
I0302 18:59:18.066981 22683591385216 run.py:483] Algo bellman_ford step 2976 current loss 0.035342, current_train_items 95264.
I0302 18:59:18.089259 22683591385216 run.py:483] Algo bellman_ford step 2977 current loss 0.112930, current_train_items 95296.
I0302 18:59:18.117972 22683591385216 run.py:483] Algo bellman_ford step 2978 current loss 0.136027, current_train_items 95328.
I0302 18:59:18.150618 22683591385216 run.py:483] Algo bellman_ford step 2979 current loss 0.179102, current_train_items 95360.
I0302 18:59:18.168590 22683591385216 run.py:483] Algo bellman_ford step 2980 current loss 0.002027, current_train_items 95392.
I0302 18:59:18.183663 22683591385216 run.py:483] Algo bellman_ford step 2981 current loss 0.021503, current_train_items 95424.
I0302 18:59:18.206844 22683591385216 run.py:483] Algo bellman_ford step 2982 current loss 0.082202, current_train_items 95456.
I0302 18:59:18.237789 22683591385216 run.py:483] Algo bellman_ford step 2983 current loss 0.082918, current_train_items 95488.
I0302 18:59:18.269041 22683591385216 run.py:483] Algo bellman_ford step 2984 current loss 0.085783, current_train_items 95520.
I0302 18:59:18.287391 22683591385216 run.py:483] Algo bellman_ford step 2985 current loss 0.000898, current_train_items 95552.
I0302 18:59:18.303416 22683591385216 run.py:483] Algo bellman_ford step 2986 current loss 0.042130, current_train_items 95584.
I0302 18:59:18.326573 22683591385216 run.py:483] Algo bellman_ford step 2987 current loss 0.070228, current_train_items 95616.
I0302 18:59:18.355782 22683591385216 run.py:483] Algo bellman_ford step 2988 current loss 0.075810, current_train_items 95648.
I0302 18:59:18.387723 22683591385216 run.py:483] Algo bellman_ford step 2989 current loss 0.090596, current_train_items 95680.
I0302 18:59:18.406256 22683591385216 run.py:483] Algo bellman_ford step 2990 current loss 0.013662, current_train_items 95712.
I0302 18:59:18.422400 22683591385216 run.py:483] Algo bellman_ford step 2991 current loss 0.026554, current_train_items 95744.
I0302 18:59:18.445338 22683591385216 run.py:483] Algo bellman_ford step 2992 current loss 0.045932, current_train_items 95776.
I0302 18:59:18.475144 22683591385216 run.py:483] Algo bellman_ford step 2993 current loss 0.146054, current_train_items 95808.
I0302 18:59:18.507947 22683591385216 run.py:483] Algo bellman_ford step 2994 current loss 0.087625, current_train_items 95840.
I0302 18:59:18.525998 22683591385216 run.py:483] Algo bellman_ford step 2995 current loss 0.005654, current_train_items 95872.
I0302 18:59:18.541358 22683591385216 run.py:483] Algo bellman_ford step 2996 current loss 0.045519, current_train_items 95904.
I0302 18:59:18.565307 22683591385216 run.py:483] Algo bellman_ford step 2997 current loss 0.054854, current_train_items 95936.
I0302 18:59:18.595273 22683591385216 run.py:483] Algo bellman_ford step 2998 current loss 0.062741, current_train_items 95968.
I0302 18:59:18.625594 22683591385216 run.py:483] Algo bellman_ford step 2999 current loss 0.086594, current_train_items 96000.
I0302 18:59:18.644378 22683591385216 run.py:483] Algo bellman_ford step 3000 current loss 0.003727, current_train_items 96032.
I0302 18:59:18.652236 22683591385216 run.py:503] (val) algo bellman_ford step 3000: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 96032, 'step': 3000, 'algorithm': 'bellman_ford'}
I0302 18:59:18.652343 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:18.668048 22683591385216 run.py:483] Algo bellman_ford step 3001 current loss 0.040181, current_train_items 96064.
I0302 18:59:18.690618 22683591385216 run.py:483] Algo bellman_ford step 3002 current loss 0.028127, current_train_items 96096.
I0302 18:59:18.720548 22683591385216 run.py:483] Algo bellman_ford step 3003 current loss 0.116420, current_train_items 96128.
I0302 18:59:18.751490 22683591385216 run.py:483] Algo bellman_ford step 3004 current loss 0.099434, current_train_items 96160.
I0302 18:59:18.770154 22683591385216 run.py:483] Algo bellman_ford step 3005 current loss 0.005572, current_train_items 96192.
I0302 18:59:18.785289 22683591385216 run.py:483] Algo bellman_ford step 3006 current loss 0.010279, current_train_items 96224.
I0302 18:59:18.808770 22683591385216 run.py:483] Algo bellman_ford step 3007 current loss 0.046777, current_train_items 96256.
I0302 18:59:18.838617 22683591385216 run.py:483] Algo bellman_ford step 3008 current loss 0.087109, current_train_items 96288.
I0302 18:59:18.872625 22683591385216 run.py:483] Algo bellman_ford step 3009 current loss 0.131746, current_train_items 96320.
I0302 18:59:18.890799 22683591385216 run.py:483] Algo bellman_ford step 3010 current loss 0.003426, current_train_items 96352.
I0302 18:59:18.906147 22683591385216 run.py:483] Algo bellman_ford step 3011 current loss 0.020629, current_train_items 96384.
I0302 18:59:18.928833 22683591385216 run.py:483] Algo bellman_ford step 3012 current loss 0.045930, current_train_items 96416.
I0302 18:59:18.957781 22683591385216 run.py:483] Algo bellman_ford step 3013 current loss 0.061594, current_train_items 96448.
I0302 18:59:18.990475 22683591385216 run.py:483] Algo bellman_ford step 3014 current loss 0.080544, current_train_items 96480.
I0302 18:59:19.008908 22683591385216 run.py:483] Algo bellman_ford step 3015 current loss 0.010243, current_train_items 96512.
I0302 18:59:19.024563 22683591385216 run.py:483] Algo bellman_ford step 3016 current loss 0.021185, current_train_items 96544.
I0302 18:59:19.046981 22683591385216 run.py:483] Algo bellman_ford step 3017 current loss 0.087819, current_train_items 96576.
I0302 18:59:19.075708 22683591385216 run.py:483] Algo bellman_ford step 3018 current loss 0.053620, current_train_items 96608.
I0302 18:59:19.106164 22683591385216 run.py:483] Algo bellman_ford step 3019 current loss 0.063517, current_train_items 96640.
I0302 18:59:19.123991 22683591385216 run.py:483] Algo bellman_ford step 3020 current loss 0.002770, current_train_items 96672.
I0302 18:59:19.138884 22683591385216 run.py:483] Algo bellman_ford step 3021 current loss 0.024449, current_train_items 96704.
I0302 18:59:19.162403 22683591385216 run.py:483] Algo bellman_ford step 3022 current loss 0.071291, current_train_items 96736.
I0302 18:59:19.191941 22683591385216 run.py:483] Algo bellman_ford step 3023 current loss 0.114466, current_train_items 96768.
I0302 18:59:19.223248 22683591385216 run.py:483] Algo bellman_ford step 3024 current loss 0.070911, current_train_items 96800.
I0302 18:59:19.241445 22683591385216 run.py:483] Algo bellman_ford step 3025 current loss 0.004391, current_train_items 96832.
I0302 18:59:19.256536 22683591385216 run.py:483] Algo bellman_ford step 3026 current loss 0.011588, current_train_items 96864.
I0302 18:59:19.277637 22683591385216 run.py:483] Algo bellman_ford step 3027 current loss 0.047597, current_train_items 96896.
I0302 18:59:19.306611 22683591385216 run.py:483] Algo bellman_ford step 3028 current loss 0.059280, current_train_items 96928.
I0302 18:59:19.336769 22683591385216 run.py:483] Algo bellman_ford step 3029 current loss 0.064207, current_train_items 96960.
I0302 18:59:19.354934 22683591385216 run.py:483] Algo bellman_ford step 3030 current loss 0.002760, current_train_items 96992.
I0302 18:59:19.369987 22683591385216 run.py:483] Algo bellman_ford step 3031 current loss 0.042418, current_train_items 97024.
I0302 18:59:19.393380 22683591385216 run.py:483] Algo bellman_ford step 3032 current loss 0.056667, current_train_items 97056.
I0302 18:59:19.422935 22683591385216 run.py:483] Algo bellman_ford step 3033 current loss 0.040434, current_train_items 97088.
I0302 18:59:19.454963 22683591385216 run.py:483] Algo bellman_ford step 3034 current loss 0.050214, current_train_items 97120.
I0302 18:59:19.473165 22683591385216 run.py:483] Algo bellman_ford step 3035 current loss 0.018476, current_train_items 97152.
I0302 18:59:19.488752 22683591385216 run.py:483] Algo bellman_ford step 3036 current loss 0.016776, current_train_items 97184.
I0302 18:59:19.511045 22683591385216 run.py:483] Algo bellman_ford step 3037 current loss 0.052359, current_train_items 97216.
I0302 18:59:19.540583 22683591385216 run.py:483] Algo bellman_ford step 3038 current loss 0.080460, current_train_items 97248.
I0302 18:59:19.570904 22683591385216 run.py:483] Algo bellman_ford step 3039 current loss 0.101283, current_train_items 97280.
I0302 18:59:19.588804 22683591385216 run.py:483] Algo bellman_ford step 3040 current loss 0.005815, current_train_items 97312.
I0302 18:59:19.604032 22683591385216 run.py:483] Algo bellman_ford step 3041 current loss 0.014042, current_train_items 97344.
I0302 18:59:19.626359 22683591385216 run.py:483] Algo bellman_ford step 3042 current loss 0.053736, current_train_items 97376.
I0302 18:59:19.655564 22683591385216 run.py:483] Algo bellman_ford step 3043 current loss 0.117054, current_train_items 97408.
I0302 18:59:19.685752 22683591385216 run.py:483] Algo bellman_ford step 3044 current loss 0.193946, current_train_items 97440.
I0302 18:59:19.703876 22683591385216 run.py:483] Algo bellman_ford step 3045 current loss 0.004664, current_train_items 97472.
I0302 18:59:19.719089 22683591385216 run.py:483] Algo bellman_ford step 3046 current loss 0.070376, current_train_items 97504.
I0302 18:59:19.741224 22683591385216 run.py:483] Algo bellman_ford step 3047 current loss 0.073132, current_train_items 97536.
I0302 18:59:19.771118 22683591385216 run.py:483] Algo bellman_ford step 3048 current loss 0.075492, current_train_items 97568.
I0302 18:59:19.802468 22683591385216 run.py:483] Algo bellman_ford step 3049 current loss 0.121407, current_train_items 97600.
I0302 18:59:19.820603 22683591385216 run.py:483] Algo bellman_ford step 3050 current loss 0.005416, current_train_items 97632.
I0302 18:59:19.828528 22683591385216 run.py:503] (val) algo bellman_ford step 3050: {'pi': 0.96875, 'score': 0.96875, 'examples_seen': 97632, 'step': 3050, 'algorithm': 'bellman_ford'}
I0302 18:59:19.828633 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.969, val scores are: bellman_ford: 0.969
I0302 18:59:19.844265 22683591385216 run.py:483] Algo bellman_ford step 3051 current loss 0.023918, current_train_items 97664.
I0302 18:59:19.867047 22683591385216 run.py:483] Algo bellman_ford step 3052 current loss 0.094743, current_train_items 97696.
I0302 18:59:19.896870 22683591385216 run.py:483] Algo bellman_ford step 3053 current loss 0.079129, current_train_items 97728.
I0302 18:59:19.926834 22683591385216 run.py:483] Algo bellman_ford step 3054 current loss 0.066344, current_train_items 97760.
I0302 18:59:19.945581 22683591385216 run.py:483] Algo bellman_ford step 3055 current loss 0.008035, current_train_items 97792.
I0302 18:59:19.960787 22683591385216 run.py:483] Algo bellman_ford step 3056 current loss 0.079574, current_train_items 97824.
I0302 18:59:19.983193 22683591385216 run.py:483] Algo bellman_ford step 3057 current loss 0.065910, current_train_items 97856.
I0302 18:59:20.012036 22683591385216 run.py:483] Algo bellman_ford step 3058 current loss 0.045673, current_train_items 97888.
I0302 18:59:20.042083 22683591385216 run.py:483] Algo bellman_ford step 3059 current loss 0.080586, current_train_items 97920.
I0302 18:59:20.060406 22683591385216 run.py:483] Algo bellman_ford step 3060 current loss 0.005218, current_train_items 97952.
I0302 18:59:20.076237 22683591385216 run.py:483] Algo bellman_ford step 3061 current loss 0.030859, current_train_items 97984.
I0302 18:59:20.097763 22683591385216 run.py:483] Algo bellman_ford step 3062 current loss 0.037611, current_train_items 98016.
I0302 18:59:20.127835 22683591385216 run.py:483] Algo bellman_ford step 3063 current loss 0.067124, current_train_items 98048.
I0302 18:59:20.160252 22683591385216 run.py:483] Algo bellman_ford step 3064 current loss 0.059258, current_train_items 98080.
I0302 18:59:20.178446 22683591385216 run.py:483] Algo bellman_ford step 3065 current loss 0.004544, current_train_items 98112.
I0302 18:59:20.193622 22683591385216 run.py:483] Algo bellman_ford step 3066 current loss 0.020634, current_train_items 98144.
I0302 18:59:20.216965 22683591385216 run.py:483] Algo bellman_ford step 3067 current loss 0.030667, current_train_items 98176.
I0302 18:59:20.245355 22683591385216 run.py:483] Algo bellman_ford step 3068 current loss 0.042485, current_train_items 98208.
I0302 18:59:20.279195 22683591385216 run.py:483] Algo bellman_ford step 3069 current loss 0.087605, current_train_items 98240.
I0302 18:59:20.297980 22683591385216 run.py:483] Algo bellman_ford step 3070 current loss 0.003736, current_train_items 98272.
I0302 18:59:20.313603 22683591385216 run.py:483] Algo bellman_ford step 3071 current loss 0.039765, current_train_items 98304.
I0302 18:59:20.335283 22683591385216 run.py:483] Algo bellman_ford step 3072 current loss 0.056867, current_train_items 98336.
I0302 18:59:20.363798 22683591385216 run.py:483] Algo bellman_ford step 3073 current loss 0.038457, current_train_items 98368.
I0302 18:59:20.396739 22683591385216 run.py:483] Algo bellman_ford step 3074 current loss 0.100620, current_train_items 98400.
I0302 18:59:20.414988 22683591385216 run.py:483] Algo bellman_ford step 3075 current loss 0.002686, current_train_items 98432.
I0302 18:59:20.430864 22683591385216 run.py:483] Algo bellman_ford step 3076 current loss 0.017355, current_train_items 98464.
I0302 18:59:20.452894 22683591385216 run.py:483] Algo bellman_ford step 3077 current loss 0.039938, current_train_items 98496.
I0302 18:59:20.482391 22683591385216 run.py:483] Algo bellman_ford step 3078 current loss 0.073569, current_train_items 98528.
I0302 18:59:20.512246 22683591385216 run.py:483] Algo bellman_ford step 3079 current loss 0.058388, current_train_items 98560.
I0302 18:59:20.530615 22683591385216 run.py:483] Algo bellman_ford step 3080 current loss 0.002703, current_train_items 98592.
I0302 18:59:20.546102 22683591385216 run.py:483] Algo bellman_ford step 3081 current loss 0.026793, current_train_items 98624.
I0302 18:59:20.568037 22683591385216 run.py:483] Algo bellman_ford step 3082 current loss 0.026675, current_train_items 98656.
I0302 18:59:20.597164 22683591385216 run.py:483] Algo bellman_ford step 3083 current loss 0.073105, current_train_items 98688.
I0302 18:59:20.628698 22683591385216 run.py:483] Algo bellman_ford step 3084 current loss 0.078980, current_train_items 98720.
I0302 18:59:20.647674 22683591385216 run.py:483] Algo bellman_ford step 3085 current loss 0.025487, current_train_items 98752.
I0302 18:59:20.663514 22683591385216 run.py:483] Algo bellman_ford step 3086 current loss 0.010981, current_train_items 98784.
I0302 18:59:20.684426 22683591385216 run.py:483] Algo bellman_ford step 3087 current loss 0.036054, current_train_items 98816.
I0302 18:59:20.713477 22683591385216 run.py:483] Algo bellman_ford step 3088 current loss 0.095630, current_train_items 98848.
I0302 18:59:20.746409 22683591385216 run.py:483] Algo bellman_ford step 3089 current loss 0.142694, current_train_items 98880.
I0302 18:59:20.764879 22683591385216 run.py:483] Algo bellman_ford step 3090 current loss 0.003076, current_train_items 98912.
I0302 18:59:20.780246 22683591385216 run.py:483] Algo bellman_ford step 3091 current loss 0.016855, current_train_items 98944.
I0302 18:59:20.801519 22683591385216 run.py:483] Algo bellman_ford step 3092 current loss 0.084806, current_train_items 98976.
I0302 18:59:20.830703 22683591385216 run.py:483] Algo bellman_ford step 3093 current loss 0.193910, current_train_items 99008.
I0302 18:59:20.861660 22683591385216 run.py:483] Algo bellman_ford step 3094 current loss 0.131015, current_train_items 99040.
I0302 18:59:20.879629 22683591385216 run.py:483] Algo bellman_ford step 3095 current loss 0.007236, current_train_items 99072.
I0302 18:59:20.895477 22683591385216 run.py:483] Algo bellman_ford step 3096 current loss 0.026455, current_train_items 99104.
I0302 18:59:20.918976 22683591385216 run.py:483] Algo bellman_ford step 3097 current loss 0.105975, current_train_items 99136.
I0302 18:59:20.947668 22683591385216 run.py:483] Algo bellman_ford step 3098 current loss 0.098599, current_train_items 99168.
I0302 18:59:20.978450 22683591385216 run.py:483] Algo bellman_ford step 3099 current loss 0.078725, current_train_items 99200.
I0302 18:59:20.997104 22683591385216 run.py:483] Algo bellman_ford step 3100 current loss 0.002004, current_train_items 99232.
I0302 18:59:21.004919 22683591385216 run.py:503] (val) algo bellman_ford step 3100: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 99232, 'step': 3100, 'algorithm': 'bellman_ford'}
I0302 18:59:21.005025 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 18:59:21.021916 22683591385216 run.py:483] Algo bellman_ford step 3101 current loss 0.039100, current_train_items 99264.
I0302 18:59:21.044821 22683591385216 run.py:483] Algo bellman_ford step 3102 current loss 0.055486, current_train_items 99296.
I0302 18:59:21.074672 22683591385216 run.py:483] Algo bellman_ford step 3103 current loss 0.038660, current_train_items 99328.
I0302 18:59:21.107141 22683591385216 run.py:483] Algo bellman_ford step 3104 current loss 0.060023, current_train_items 99360.
I0302 18:59:21.125638 22683591385216 run.py:483] Algo bellman_ford step 3105 current loss 0.027664, current_train_items 99392.
I0302 18:59:21.140420 22683591385216 run.py:483] Algo bellman_ford step 3106 current loss 0.003868, current_train_items 99424.
I0302 18:59:21.162560 22683591385216 run.py:483] Algo bellman_ford step 3107 current loss 0.027882, current_train_items 99456.
I0302 18:59:21.192560 22683591385216 run.py:483] Algo bellman_ford step 3108 current loss 0.074061, current_train_items 99488.
I0302 18:59:21.223034 22683591385216 run.py:483] Algo bellman_ford step 3109 current loss 0.097832, current_train_items 99520.
I0302 18:59:21.241073 22683591385216 run.py:483] Algo bellman_ford step 3110 current loss 0.001781, current_train_items 99552.
I0302 18:59:21.256415 22683591385216 run.py:483] Algo bellman_ford step 3111 current loss 0.018394, current_train_items 99584.
I0302 18:59:21.278309 22683591385216 run.py:483] Algo bellman_ford step 3112 current loss 0.048100, current_train_items 99616.
I0302 18:59:21.307685 22683591385216 run.py:483] Algo bellman_ford step 3113 current loss 0.063454, current_train_items 99648.
I0302 18:59:21.339183 22683591385216 run.py:483] Algo bellman_ford step 3114 current loss 0.045969, current_train_items 99680.
I0302 18:59:21.357267 22683591385216 run.py:483] Algo bellman_ford step 3115 current loss 0.008178, current_train_items 99712.
I0302 18:59:21.372771 22683591385216 run.py:483] Algo bellman_ford step 3116 current loss 0.021776, current_train_items 99744.
I0302 18:59:21.394953 22683591385216 run.py:483] Algo bellman_ford step 3117 current loss 0.049366, current_train_items 99776.
I0302 18:59:21.424594 22683591385216 run.py:483] Algo bellman_ford step 3118 current loss 0.088853, current_train_items 99808.
I0302 18:59:21.455200 22683591385216 run.py:483] Algo bellman_ford step 3119 current loss 0.068646, current_train_items 99840.
I0302 18:59:21.473509 22683591385216 run.py:483] Algo bellman_ford step 3120 current loss 0.001733, current_train_items 99872.
I0302 18:59:21.488764 22683591385216 run.py:483] Algo bellman_ford step 3121 current loss 0.029710, current_train_items 99904.
I0302 18:59:21.510959 22683591385216 run.py:483] Algo bellman_ford step 3122 current loss 0.067104, current_train_items 99936.
I0302 18:59:21.540274 22683591385216 run.py:483] Algo bellman_ford step 3123 current loss 0.117849, current_train_items 99968.
I0302 18:59:21.571369 22683591385216 run.py:483] Algo bellman_ford step 3124 current loss 0.044758, current_train_items 100000.
I0302 18:59:21.589740 22683591385216 run.py:483] Algo bellman_ford step 3125 current loss 0.002714, current_train_items 100032.
I0302 18:59:21.604990 22683591385216 run.py:483] Algo bellman_ford step 3126 current loss 0.032125, current_train_items 100064.
I0302 18:59:21.626303 22683591385216 run.py:483] Algo bellman_ford step 3127 current loss 0.065038, current_train_items 100096.
I0302 18:59:21.656262 22683591385216 run.py:483] Algo bellman_ford step 3128 current loss 0.099931, current_train_items 100128.
I0302 18:59:21.686096 22683591385216 run.py:483] Algo bellman_ford step 3129 current loss 0.090525, current_train_items 100160.
I0302 18:59:21.704422 22683591385216 run.py:483] Algo bellman_ford step 3130 current loss 0.001782, current_train_items 100192.
I0302 18:59:21.719684 22683591385216 run.py:483] Algo bellman_ford step 3131 current loss 0.023528, current_train_items 100224.
I0302 18:59:21.742066 22683591385216 run.py:483] Algo bellman_ford step 3132 current loss 0.047486, current_train_items 100256.
I0302 18:59:21.770556 22683591385216 run.py:483] Algo bellman_ford step 3133 current loss 0.115012, current_train_items 100288.
I0302 18:59:21.803267 22683591385216 run.py:483] Algo bellman_ford step 3134 current loss 0.114319, current_train_items 100320.
I0302 18:59:21.821150 22683591385216 run.py:483] Algo bellman_ford step 3135 current loss 0.004198, current_train_items 100352.
I0302 18:59:21.836232 22683591385216 run.py:483] Algo bellman_ford step 3136 current loss 0.018882, current_train_items 100384.
I0302 18:59:21.858829 22683591385216 run.py:483] Algo bellman_ford step 3137 current loss 0.051226, current_train_items 100416.
I0302 18:59:21.886785 22683591385216 run.py:483] Algo bellman_ford step 3138 current loss 0.077736, current_train_items 100448.
I0302 18:59:21.918329 22683591385216 run.py:483] Algo bellman_ford step 3139 current loss 0.071191, current_train_items 100480.
I0302 18:59:21.936625 22683591385216 run.py:483] Algo bellman_ford step 3140 current loss 0.002905, current_train_items 100512.
I0302 18:59:21.952457 22683591385216 run.py:483] Algo bellman_ford step 3141 current loss 0.065083, current_train_items 100544.
I0302 18:59:21.973424 22683591385216 run.py:483] Algo bellman_ford step 3142 current loss 0.068893, current_train_items 100576.
I0302 18:59:22.004181 22683591385216 run.py:483] Algo bellman_ford step 3143 current loss 0.078523, current_train_items 100608.
I0302 18:59:22.035085 22683591385216 run.py:483] Algo bellman_ford step 3144 current loss 0.062619, current_train_items 100640.
I0302 18:59:22.053235 22683591385216 run.py:483] Algo bellman_ford step 3145 current loss 0.001448, current_train_items 100672.
I0302 18:59:22.068414 22683591385216 run.py:483] Algo bellman_ford step 3146 current loss 0.027093, current_train_items 100704.
I0302 18:59:22.089926 22683591385216 run.py:483] Algo bellman_ford step 3147 current loss 0.042775, current_train_items 100736.
I0302 18:59:22.117256 22683591385216 run.py:483] Algo bellman_ford step 3148 current loss 0.048130, current_train_items 100768.
I0302 18:59:22.150396 22683591385216 run.py:483] Algo bellman_ford step 3149 current loss 0.144358, current_train_items 100800.
I0302 18:59:22.168313 22683591385216 run.py:483] Algo bellman_ford step 3150 current loss 0.001513, current_train_items 100832.
I0302 18:59:22.176299 22683591385216 run.py:503] (val) algo bellman_ford step 3150: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 100832, 'step': 3150, 'algorithm': 'bellman_ford'}
I0302 18:59:22.176406 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:22.192397 22683591385216 run.py:483] Algo bellman_ford step 3151 current loss 0.028898, current_train_items 100864.
I0302 18:59:22.216036 22683591385216 run.py:483] Algo bellman_ford step 3152 current loss 0.118066, current_train_items 100896.
I0302 18:59:22.245610 22683591385216 run.py:483] Algo bellman_ford step 3153 current loss 0.073943, current_train_items 100928.
I0302 18:59:22.278157 22683591385216 run.py:483] Algo bellman_ford step 3154 current loss 0.075860, current_train_items 100960.
I0302 18:59:22.296915 22683591385216 run.py:483] Algo bellman_ford step 3155 current loss 0.004933, current_train_items 100992.
I0302 18:59:22.312133 22683591385216 run.py:483] Algo bellman_ford step 3156 current loss 0.011097, current_train_items 101024.
I0302 18:59:22.335030 22683591385216 run.py:483] Algo bellman_ford step 3157 current loss 0.060541, current_train_items 101056.
I0302 18:59:22.363126 22683591385216 run.py:483] Algo bellman_ford step 3158 current loss 0.086878, current_train_items 101088.
I0302 18:59:22.393797 22683591385216 run.py:483] Algo bellman_ford step 3159 current loss 0.073778, current_train_items 101120.
I0302 18:59:22.412762 22683591385216 run.py:483] Algo bellman_ford step 3160 current loss 0.001753, current_train_items 101152.
I0302 18:59:22.428121 22683591385216 run.py:483] Algo bellman_ford step 3161 current loss 0.012122, current_train_items 101184.
I0302 18:59:22.450313 22683591385216 run.py:483] Algo bellman_ford step 3162 current loss 0.042682, current_train_items 101216.
I0302 18:59:22.478740 22683591385216 run.py:483] Algo bellman_ford step 3163 current loss 0.055920, current_train_items 101248.
I0302 18:59:22.511196 22683591385216 run.py:483] Algo bellman_ford step 3164 current loss 0.129578, current_train_items 101280.
I0302 18:59:22.529642 22683591385216 run.py:483] Algo bellman_ford step 3165 current loss 0.005901, current_train_items 101312.
I0302 18:59:22.545588 22683591385216 run.py:483] Algo bellman_ford step 3166 current loss 0.050763, current_train_items 101344.
I0302 18:59:22.568210 22683591385216 run.py:483] Algo bellman_ford step 3167 current loss 0.077750, current_train_items 101376.
I0302 18:59:22.597203 22683591385216 run.py:483] Algo bellman_ford step 3168 current loss 0.052843, current_train_items 101408.
I0302 18:59:22.627926 22683591385216 run.py:483] Algo bellman_ford step 3169 current loss 0.098006, current_train_items 101440.
I0302 18:59:22.646524 22683591385216 run.py:483] Algo bellman_ford step 3170 current loss 0.004669, current_train_items 101472.
I0302 18:59:22.662427 22683591385216 run.py:483] Algo bellman_ford step 3171 current loss 0.056243, current_train_items 101504.
I0302 18:59:22.685158 22683591385216 run.py:483] Algo bellman_ford step 3172 current loss 0.069618, current_train_items 101536.
I0302 18:59:22.714488 22683591385216 run.py:483] Algo bellman_ford step 3173 current loss 0.068425, current_train_items 101568.
I0302 18:59:22.743554 22683591385216 run.py:483] Algo bellman_ford step 3174 current loss 0.072964, current_train_items 101600.
I0302 18:59:22.762411 22683591385216 run.py:483] Algo bellman_ford step 3175 current loss 0.001186, current_train_items 101632.
I0302 18:59:22.777949 22683591385216 run.py:483] Algo bellman_ford step 3176 current loss 0.035518, current_train_items 101664.
I0302 18:59:22.799715 22683591385216 run.py:483] Algo bellman_ford step 3177 current loss 0.092098, current_train_items 101696.
I0302 18:59:22.826384 22683591385216 run.py:483] Algo bellman_ford step 3178 current loss 0.058242, current_train_items 101728.
I0302 18:59:22.858611 22683591385216 run.py:483] Algo bellman_ford step 3179 current loss 0.090818, current_train_items 101760.
I0302 18:59:22.876842 22683591385216 run.py:483] Algo bellman_ford step 3180 current loss 0.009638, current_train_items 101792.
I0302 18:59:22.892359 22683591385216 run.py:483] Algo bellman_ford step 3181 current loss 0.029823, current_train_items 101824.
I0302 18:59:22.914773 22683591385216 run.py:483] Algo bellman_ford step 3182 current loss 0.060632, current_train_items 101856.
I0302 18:59:22.944316 22683591385216 run.py:483] Algo bellman_ford step 3183 current loss 0.106325, current_train_items 101888.
I0302 18:59:22.978254 22683591385216 run.py:483] Algo bellman_ford step 3184 current loss 0.084776, current_train_items 101920.
I0302 18:59:22.997138 22683591385216 run.py:483] Algo bellman_ford step 3185 current loss 0.037295, current_train_items 101952.
I0302 18:59:23.012656 22683591385216 run.py:483] Algo bellman_ford step 3186 current loss 0.021440, current_train_items 101984.
I0302 18:59:23.035442 22683591385216 run.py:483] Algo bellman_ford step 3187 current loss 0.124787, current_train_items 102016.
I0302 18:59:23.064460 22683591385216 run.py:483] Algo bellman_ford step 3188 current loss 0.109655, current_train_items 102048.
I0302 18:59:23.095789 22683591385216 run.py:483] Algo bellman_ford step 3189 current loss 0.120040, current_train_items 102080.
I0302 18:59:23.114343 22683591385216 run.py:483] Algo bellman_ford step 3190 current loss 0.008087, current_train_items 102112.
I0302 18:59:23.130046 22683591385216 run.py:483] Algo bellman_ford step 3191 current loss 0.015473, current_train_items 102144.
I0302 18:59:23.152579 22683591385216 run.py:483] Algo bellman_ford step 3192 current loss 0.060796, current_train_items 102176.
I0302 18:59:23.180987 22683591385216 run.py:483] Algo bellman_ford step 3193 current loss 0.058580, current_train_items 102208.
I0302 18:59:23.212621 22683591385216 run.py:483] Algo bellman_ford step 3194 current loss 0.083344, current_train_items 102240.
I0302 18:59:23.231089 22683591385216 run.py:483] Algo bellman_ford step 3195 current loss 0.002854, current_train_items 102272.
I0302 18:59:23.246108 22683591385216 run.py:483] Algo bellman_ford step 3196 current loss 0.026953, current_train_items 102304.
I0302 18:59:23.268088 22683591385216 run.py:483] Algo bellman_ford step 3197 current loss 0.067850, current_train_items 102336.
I0302 18:59:23.297554 22683591385216 run.py:483] Algo bellman_ford step 3198 current loss 0.082874, current_train_items 102368.
I0302 18:59:23.329509 22683591385216 run.py:483] Algo bellman_ford step 3199 current loss 0.115700, current_train_items 102400.
I0302 18:59:23.348463 22683591385216 run.py:483] Algo bellman_ford step 3200 current loss 0.013818, current_train_items 102432.
I0302 18:59:23.356229 22683591385216 run.py:503] (val) algo bellman_ford step 3200: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 102432, 'step': 3200, 'algorithm': 'bellman_ford'}
I0302 18:59:23.356336 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:23.371666 22683591385216 run.py:483] Algo bellman_ford step 3201 current loss 0.012837, current_train_items 102464.
I0302 18:59:23.393560 22683591385216 run.py:483] Algo bellman_ford step 3202 current loss 0.041499, current_train_items 102496.
I0302 18:59:23.424434 22683591385216 run.py:483] Algo bellman_ford step 3203 current loss 0.131523, current_train_items 102528.
I0302 18:59:23.456669 22683591385216 run.py:483] Algo bellman_ford step 3204 current loss 0.097430, current_train_items 102560.
I0302 18:59:23.475359 22683591385216 run.py:483] Algo bellman_ford step 3205 current loss 0.001905, current_train_items 102592.
I0302 18:59:23.490232 22683591385216 run.py:483] Algo bellman_ford step 3206 current loss 0.025846, current_train_items 102624.
I0302 18:59:23.511259 22683591385216 run.py:483] Algo bellman_ford step 3207 current loss 0.034442, current_train_items 102656.
I0302 18:59:23.541185 22683591385216 run.py:483] Algo bellman_ford step 3208 current loss 0.086606, current_train_items 102688.
I0302 18:59:23.573721 22683591385216 run.py:483] Algo bellman_ford step 3209 current loss 0.113348, current_train_items 102720.
I0302 18:59:23.591933 22683591385216 run.py:483] Algo bellman_ford step 3210 current loss 0.062193, current_train_items 102752.
I0302 18:59:23.606887 22683591385216 run.py:483] Algo bellman_ford step 3211 current loss 0.005692, current_train_items 102784.
I0302 18:59:23.628538 22683591385216 run.py:483] Algo bellman_ford step 3212 current loss 0.033218, current_train_items 102816.
I0302 18:59:23.658090 22683591385216 run.py:483] Algo bellman_ford step 3213 current loss 0.080949, current_train_items 102848.
I0302 18:59:23.688961 22683591385216 run.py:483] Algo bellman_ford step 3214 current loss 0.084719, current_train_items 102880.
I0302 18:59:23.707103 22683591385216 run.py:483] Algo bellman_ford step 3215 current loss 0.021936, current_train_items 102912.
I0302 18:59:23.722301 22683591385216 run.py:483] Algo bellman_ford step 3216 current loss 0.050397, current_train_items 102944.
I0302 18:59:23.745615 22683591385216 run.py:483] Algo bellman_ford step 3217 current loss 0.111893, current_train_items 102976.
I0302 18:59:23.773478 22683591385216 run.py:483] Algo bellman_ford step 3218 current loss 0.144112, current_train_items 103008.
I0302 18:59:23.804141 22683591385216 run.py:483] Algo bellman_ford step 3219 current loss 0.071908, current_train_items 103040.
I0302 18:59:23.822154 22683591385216 run.py:483] Algo bellman_ford step 3220 current loss 0.004945, current_train_items 103072.
I0302 18:59:23.837468 22683591385216 run.py:483] Algo bellman_ford step 3221 current loss 0.037136, current_train_items 103104.
I0302 18:59:23.860317 22683591385216 run.py:483] Algo bellman_ford step 3222 current loss 0.110952, current_train_items 103136.
I0302 18:59:23.889966 22683591385216 run.py:483] Algo bellman_ford step 3223 current loss 0.060662, current_train_items 103168.
I0302 18:59:23.919567 22683591385216 run.py:483] Algo bellman_ford step 3224 current loss 0.061634, current_train_items 103200.
I0302 18:59:23.937590 22683591385216 run.py:483] Algo bellman_ford step 3225 current loss 0.004680, current_train_items 103232.
I0302 18:59:23.952125 22683591385216 run.py:483] Algo bellman_ford step 3226 current loss 0.028347, current_train_items 103264.
I0302 18:59:23.974382 22683591385216 run.py:483] Algo bellman_ford step 3227 current loss 0.027774, current_train_items 103296.
I0302 18:59:24.004535 22683591385216 run.py:483] Algo bellman_ford step 3228 current loss 0.094817, current_train_items 103328.
I0302 18:59:24.039524 22683591385216 run.py:483] Algo bellman_ford step 3229 current loss 0.132538, current_train_items 103360.
I0302 18:59:24.057290 22683591385216 run.py:483] Algo bellman_ford step 3230 current loss 0.011140, current_train_items 103392.
I0302 18:59:24.072739 22683591385216 run.py:483] Algo bellman_ford step 3231 current loss 0.038121, current_train_items 103424.
I0302 18:59:24.094166 22683591385216 run.py:483] Algo bellman_ford step 3232 current loss 0.051259, current_train_items 103456.
I0302 18:59:24.123171 22683591385216 run.py:483] Algo bellman_ford step 3233 current loss 0.034325, current_train_items 103488.
I0302 18:59:24.155676 22683591385216 run.py:483] Algo bellman_ford step 3234 current loss 0.069777, current_train_items 103520.
I0302 18:59:24.173565 22683591385216 run.py:483] Algo bellman_ford step 3235 current loss 0.017830, current_train_items 103552.
I0302 18:59:24.189017 22683591385216 run.py:483] Algo bellman_ford step 3236 current loss 0.020730, current_train_items 103584.
I0302 18:59:24.210539 22683591385216 run.py:483] Algo bellman_ford step 3237 current loss 0.034998, current_train_items 103616.
I0302 18:59:24.240747 22683591385216 run.py:483] Algo bellman_ford step 3238 current loss 0.061012, current_train_items 103648.
I0302 18:59:24.271810 22683591385216 run.py:483] Algo bellman_ford step 3239 current loss 0.070409, current_train_items 103680.
I0302 18:59:24.289738 22683591385216 run.py:483] Algo bellman_ford step 3240 current loss 0.033838, current_train_items 103712.
I0302 18:59:24.305202 22683591385216 run.py:483] Algo bellman_ford step 3241 current loss 0.037878, current_train_items 103744.
I0302 18:59:24.327069 22683591385216 run.py:483] Algo bellman_ford step 3242 current loss 0.089170, current_train_items 103776.
I0302 18:59:24.354981 22683591385216 run.py:483] Algo bellman_ford step 3243 current loss 0.085567, current_train_items 103808.
I0302 18:59:24.387184 22683591385216 run.py:483] Algo bellman_ford step 3244 current loss 0.133848, current_train_items 103840.
I0302 18:59:24.405183 22683591385216 run.py:483] Algo bellman_ford step 3245 current loss 0.001716, current_train_items 103872.
I0302 18:59:24.420614 22683591385216 run.py:483] Algo bellman_ford step 3246 current loss 0.013500, current_train_items 103904.
I0302 18:59:24.442749 22683591385216 run.py:483] Algo bellman_ford step 3247 current loss 0.053509, current_train_items 103936.
I0302 18:59:24.472064 22683591385216 run.py:483] Algo bellman_ford step 3248 current loss 0.035945, current_train_items 103968.
I0302 18:59:24.502027 22683591385216 run.py:483] Algo bellman_ford step 3249 current loss 0.066879, current_train_items 104000.
I0302 18:59:24.520547 22683591385216 run.py:483] Algo bellman_ford step 3250 current loss 0.008615, current_train_items 104032.
I0302 18:59:24.528596 22683591385216 run.py:503] (val) algo bellman_ford step 3250: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 104032, 'step': 3250, 'algorithm': 'bellman_ford'}
I0302 18:59:24.528702 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:59:24.544785 22683591385216 run.py:483] Algo bellman_ford step 3251 current loss 0.011666, current_train_items 104064.
I0302 18:59:24.565711 22683591385216 run.py:483] Algo bellman_ford step 3252 current loss 0.016574, current_train_items 104096.
I0302 18:59:24.596312 22683591385216 run.py:483] Algo bellman_ford step 3253 current loss 0.084843, current_train_items 104128.
I0302 18:59:24.627954 22683591385216 run.py:483] Algo bellman_ford step 3254 current loss 0.098081, current_train_items 104160.
I0302 18:59:24.646973 22683591385216 run.py:483] Algo bellman_ford step 3255 current loss 0.006793, current_train_items 104192.
I0302 18:59:24.661840 22683591385216 run.py:483] Algo bellman_ford step 3256 current loss 0.020302, current_train_items 104224.
I0302 18:59:24.683232 22683591385216 run.py:483] Algo bellman_ford step 3257 current loss 0.042430, current_train_items 104256.
I0302 18:59:24.713557 22683591385216 run.py:483] Algo bellman_ford step 3258 current loss 0.117575, current_train_items 104288.
I0302 18:59:24.746266 22683591385216 run.py:483] Algo bellman_ford step 3259 current loss 0.082951, current_train_items 104320.
I0302 18:59:24.764739 22683591385216 run.py:483] Algo bellman_ford step 3260 current loss 0.004290, current_train_items 104352.
I0302 18:59:24.780269 22683591385216 run.py:483] Algo bellman_ford step 3261 current loss 0.016256, current_train_items 104384.
I0302 18:59:24.801689 22683591385216 run.py:483] Algo bellman_ford step 3262 current loss 0.047850, current_train_items 104416.
I0302 18:59:24.830717 22683591385216 run.py:483] Algo bellman_ford step 3263 current loss 0.051804, current_train_items 104448.
I0302 18:59:24.862614 22683591385216 run.py:483] Algo bellman_ford step 3264 current loss 0.085398, current_train_items 104480.
I0302 18:59:24.880726 22683591385216 run.py:483] Algo bellman_ford step 3265 current loss 0.023701, current_train_items 104512.
I0302 18:59:24.896201 22683591385216 run.py:483] Algo bellman_ford step 3266 current loss 0.038385, current_train_items 104544.
I0302 18:59:24.917792 22683591385216 run.py:483] Algo bellman_ford step 3267 current loss 0.089624, current_train_items 104576.
I0302 18:59:24.945173 22683591385216 run.py:483] Algo bellman_ford step 3268 current loss 0.062663, current_train_items 104608.
I0302 18:59:24.978991 22683591385216 run.py:483] Algo bellman_ford step 3269 current loss 0.092774, current_train_items 104640.
I0302 18:59:24.997368 22683591385216 run.py:483] Algo bellman_ford step 3270 current loss 0.003895, current_train_items 104672.
I0302 18:59:25.013060 22683591385216 run.py:483] Algo bellman_ford step 3271 current loss 0.040186, current_train_items 104704.
I0302 18:59:25.034888 22683591385216 run.py:483] Algo bellman_ford step 3272 current loss 0.094712, current_train_items 104736.
I0302 18:59:25.062565 22683591385216 run.py:483] Algo bellman_ford step 3273 current loss 0.114851, current_train_items 104768.
I0302 18:59:25.091720 22683591385216 run.py:483] Algo bellman_ford step 3274 current loss 0.069049, current_train_items 104800.
I0302 18:59:25.110526 22683591385216 run.py:483] Algo bellman_ford step 3275 current loss 0.008924, current_train_items 104832.
I0302 18:59:25.125810 22683591385216 run.py:483] Algo bellman_ford step 3276 current loss 0.027893, current_train_items 104864.
I0302 18:59:25.147202 22683591385216 run.py:483] Algo bellman_ford step 3277 current loss 0.073012, current_train_items 104896.
I0302 18:59:25.176908 22683591385216 run.py:483] Algo bellman_ford step 3278 current loss 0.086882, current_train_items 104928.
I0302 18:59:25.208619 22683591385216 run.py:483] Algo bellman_ford step 3279 current loss 0.080067, current_train_items 104960.
I0302 18:59:25.226522 22683591385216 run.py:483] Algo bellman_ford step 3280 current loss 0.010992, current_train_items 104992.
I0302 18:59:25.241551 22683591385216 run.py:483] Algo bellman_ford step 3281 current loss 0.021588, current_train_items 105024.
I0302 18:59:25.263207 22683591385216 run.py:483] Algo bellman_ford step 3282 current loss 0.081964, current_train_items 105056.
I0302 18:59:25.292526 22683591385216 run.py:483] Algo bellman_ford step 3283 current loss 0.156072, current_train_items 105088.
I0302 18:59:25.324204 22683591385216 run.py:483] Algo bellman_ford step 3284 current loss 0.142848, current_train_items 105120.
I0302 18:59:25.342522 22683591385216 run.py:483] Algo bellman_ford step 3285 current loss 0.002644, current_train_items 105152.
I0302 18:59:25.358358 22683591385216 run.py:483] Algo bellman_ford step 3286 current loss 0.015430, current_train_items 105184.
I0302 18:59:25.380446 22683591385216 run.py:483] Algo bellman_ford step 3287 current loss 0.046662, current_train_items 105216.
I0302 18:59:25.408171 22683591385216 run.py:483] Algo bellman_ford step 3288 current loss 0.032851, current_train_items 105248.
I0302 18:59:25.437604 22683591385216 run.py:483] Algo bellman_ford step 3289 current loss 0.097842, current_train_items 105280.
I0302 18:59:25.456156 22683591385216 run.py:483] Algo bellman_ford step 3290 current loss 0.016382, current_train_items 105312.
I0302 18:59:25.472114 22683591385216 run.py:483] Algo bellman_ford step 3291 current loss 0.012779, current_train_items 105344.
I0302 18:59:25.493979 22683591385216 run.py:483] Algo bellman_ford step 3292 current loss 0.063772, current_train_items 105376.
I0302 18:59:25.522974 22683591385216 run.py:483] Algo bellman_ford step 3293 current loss 0.099476, current_train_items 105408.
I0302 18:59:25.554541 22683591385216 run.py:483] Algo bellman_ford step 3294 current loss 0.077382, current_train_items 105440.
I0302 18:59:25.572705 22683591385216 run.py:483] Algo bellman_ford step 3295 current loss 0.012901, current_train_items 105472.
I0302 18:59:25.587640 22683591385216 run.py:483] Algo bellman_ford step 3296 current loss 0.005764, current_train_items 105504.
I0302 18:59:25.610382 22683591385216 run.py:483] Algo bellman_ford step 3297 current loss 0.165514, current_train_items 105536.
I0302 18:59:25.640647 22683591385216 run.py:483] Algo bellman_ford step 3298 current loss 0.281312, current_train_items 105568.
I0302 18:59:25.672838 22683591385216 run.py:483] Algo bellman_ford step 3299 current loss 0.208703, current_train_items 105600.
I0302 18:59:25.691020 22683591385216 run.py:483] Algo bellman_ford step 3300 current loss 0.008762, current_train_items 105632.
I0302 18:59:25.698534 22683591385216 run.py:503] (val) algo bellman_ford step 3300: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 105632, 'step': 3300, 'algorithm': 'bellman_ford'}
I0302 18:59:25.698640 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:25.714485 22683591385216 run.py:483] Algo bellman_ford step 3301 current loss 0.016177, current_train_items 105664.
I0302 18:59:25.736811 22683591385216 run.py:483] Algo bellman_ford step 3302 current loss 0.072893, current_train_items 105696.
I0302 18:59:25.764807 22683591385216 run.py:483] Algo bellman_ford step 3303 current loss 0.069178, current_train_items 105728.
I0302 18:59:25.794481 22683591385216 run.py:483] Algo bellman_ford step 3304 current loss 0.064330, current_train_items 105760.
I0302 18:59:25.812651 22683591385216 run.py:483] Algo bellman_ford step 3305 current loss 0.003028, current_train_items 105792.
I0302 18:59:25.828088 22683591385216 run.py:483] Algo bellman_ford step 3306 current loss 0.036618, current_train_items 105824.
I0302 18:59:25.850707 22683591385216 run.py:483] Algo bellman_ford step 3307 current loss 0.056734, current_train_items 105856.
I0302 18:59:25.879614 22683591385216 run.py:483] Algo bellman_ford step 3308 current loss 0.048627, current_train_items 105888.
I0302 18:59:25.912622 22683591385216 run.py:483] Algo bellman_ford step 3309 current loss 0.121967, current_train_items 105920.
I0302 18:59:25.930725 22683591385216 run.py:483] Algo bellman_ford step 3310 current loss 0.040913, current_train_items 105952.
I0302 18:59:25.946238 22683591385216 run.py:483] Algo bellman_ford step 3311 current loss 0.046804, current_train_items 105984.
I0302 18:59:25.968492 22683591385216 run.py:483] Algo bellman_ford step 3312 current loss 0.047958, current_train_items 106016.
I0302 18:59:25.997965 22683591385216 run.py:483] Algo bellman_ford step 3313 current loss 0.051655, current_train_items 106048.
I0302 18:59:26.027967 22683591385216 run.py:483] Algo bellman_ford step 3314 current loss 0.081556, current_train_items 106080.
I0302 18:59:26.045916 22683591385216 run.py:483] Algo bellman_ford step 3315 current loss 0.002039, current_train_items 106112.
I0302 18:59:26.061826 22683591385216 run.py:483] Algo bellman_ford step 3316 current loss 0.023997, current_train_items 106144.
I0302 18:59:26.083496 22683591385216 run.py:483] Algo bellman_ford step 3317 current loss 0.025214, current_train_items 106176.
I0302 18:59:26.112410 22683591385216 run.py:483] Algo bellman_ford step 3318 current loss 0.045371, current_train_items 106208.
I0302 18:59:26.144273 22683591385216 run.py:483] Algo bellman_ford step 3319 current loss 0.114643, current_train_items 106240.
I0302 18:59:26.162373 22683591385216 run.py:483] Algo bellman_ford step 3320 current loss 0.002434, current_train_items 106272.
I0302 18:59:26.177654 22683591385216 run.py:483] Algo bellman_ford step 3321 current loss 0.016349, current_train_items 106304.
I0302 18:59:26.200538 22683591385216 run.py:483] Algo bellman_ford step 3322 current loss 0.056679, current_train_items 106336.
I0302 18:59:26.230176 22683591385216 run.py:483] Algo bellman_ford step 3323 current loss 0.067551, current_train_items 106368.
I0302 18:59:26.260500 22683591385216 run.py:483] Algo bellman_ford step 3324 current loss 0.063410, current_train_items 106400.
I0302 18:59:26.278389 22683591385216 run.py:483] Algo bellman_ford step 3325 current loss 0.003738, current_train_items 106432.
I0302 18:59:26.293776 22683591385216 run.py:483] Algo bellman_ford step 3326 current loss 0.043296, current_train_items 106464.
I0302 18:59:26.315486 22683591385216 run.py:483] Algo bellman_ford step 3327 current loss 0.045192, current_train_items 106496.
I0302 18:59:26.343810 22683591385216 run.py:483] Algo bellman_ford step 3328 current loss 0.062018, current_train_items 106528.
I0302 18:59:26.374620 22683591385216 run.py:483] Algo bellman_ford step 3329 current loss 0.093079, current_train_items 106560.
I0302 18:59:26.392734 22683591385216 run.py:483] Algo bellman_ford step 3330 current loss 0.013706, current_train_items 106592.
I0302 18:59:26.407902 22683591385216 run.py:483] Algo bellman_ford step 3331 current loss 0.038475, current_train_items 106624.
I0302 18:59:26.429708 22683591385216 run.py:483] Algo bellman_ford step 3332 current loss 0.058019, current_train_items 106656.
I0302 18:59:26.457837 22683591385216 run.py:483] Algo bellman_ford step 3333 current loss 0.050872, current_train_items 106688.
I0302 18:59:26.487783 22683591385216 run.py:483] Algo bellman_ford step 3334 current loss 0.061748, current_train_items 106720.
I0302 18:59:26.505552 22683591385216 run.py:483] Algo bellman_ford step 3335 current loss 0.002914, current_train_items 106752.
I0302 18:59:26.520365 22683591385216 run.py:483] Algo bellman_ford step 3336 current loss 0.043140, current_train_items 106784.
I0302 18:59:26.542849 22683591385216 run.py:483] Algo bellman_ford step 3337 current loss 0.066022, current_train_items 106816.
I0302 18:59:26.572988 22683591385216 run.py:483] Algo bellman_ford step 3338 current loss 0.086014, current_train_items 106848.
I0302 18:59:26.605653 22683591385216 run.py:483] Algo bellman_ford step 3339 current loss 0.076056, current_train_items 106880.
I0302 18:59:26.623468 22683591385216 run.py:483] Algo bellman_ford step 3340 current loss 0.003250, current_train_items 106912.
I0302 18:59:26.639276 22683591385216 run.py:483] Algo bellman_ford step 3341 current loss 0.015687, current_train_items 106944.
I0302 18:59:26.661662 22683591385216 run.py:483] Algo bellman_ford step 3342 current loss 0.091848, current_train_items 106976.
I0302 18:59:26.691472 22683591385216 run.py:483] Algo bellman_ford step 3343 current loss 0.137502, current_train_items 107008.
I0302 18:59:26.720664 22683591385216 run.py:483] Algo bellman_ford step 3344 current loss 0.090006, current_train_items 107040.
I0302 18:59:26.738855 22683591385216 run.py:483] Algo bellman_ford step 3345 current loss 0.016240, current_train_items 107072.
I0302 18:59:26.753863 22683591385216 run.py:483] Algo bellman_ford step 3346 current loss 0.007845, current_train_items 107104.
I0302 18:59:26.776024 22683591385216 run.py:483] Algo bellman_ford step 3347 current loss 0.020963, current_train_items 107136.
I0302 18:59:26.804380 22683591385216 run.py:483] Algo bellman_ford step 3348 current loss 0.029191, current_train_items 107168.
I0302 18:59:26.833627 22683591385216 run.py:483] Algo bellman_ford step 3349 current loss 0.094263, current_train_items 107200.
I0302 18:59:26.851778 22683591385216 run.py:483] Algo bellman_ford step 3350 current loss 0.038313, current_train_items 107232.
I0302 18:59:26.859904 22683591385216 run.py:503] (val) algo bellman_ford step 3350: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 107232, 'step': 3350, 'algorithm': 'bellman_ford'}
I0302 18:59:26.860011 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:26.876382 22683591385216 run.py:483] Algo bellman_ford step 3351 current loss 0.020230, current_train_items 107264.
I0302 18:59:26.899507 22683591385216 run.py:483] Algo bellman_ford step 3352 current loss 0.062311, current_train_items 107296.
I0302 18:59:26.929915 22683591385216 run.py:483] Algo bellman_ford step 3353 current loss 0.047391, current_train_items 107328.
I0302 18:59:26.962205 22683591385216 run.py:483] Algo bellman_ford step 3354 current loss 0.066311, current_train_items 107360.
I0302 18:59:26.980625 22683591385216 run.py:483] Algo bellman_ford step 3355 current loss 0.002759, current_train_items 107392.
I0302 18:59:26.995702 22683591385216 run.py:483] Algo bellman_ford step 3356 current loss 0.083539, current_train_items 107424.
I0302 18:59:27.018087 22683591385216 run.py:483] Algo bellman_ford step 3357 current loss 0.038657, current_train_items 107456.
I0302 18:59:27.047238 22683591385216 run.py:483] Algo bellman_ford step 3358 current loss 0.076209, current_train_items 107488.
I0302 18:59:27.080171 22683591385216 run.py:483] Algo bellman_ford step 3359 current loss 0.063396, current_train_items 107520.
I0302 18:59:27.098582 22683591385216 run.py:483] Algo bellman_ford step 3360 current loss 0.002224, current_train_items 107552.
I0302 18:59:27.113965 22683591385216 run.py:483] Algo bellman_ford step 3361 current loss 0.015298, current_train_items 107584.
I0302 18:59:27.136804 22683591385216 run.py:483] Algo bellman_ford step 3362 current loss 0.083779, current_train_items 107616.
I0302 18:59:27.165695 22683591385216 run.py:483] Algo bellman_ford step 3363 current loss 0.101241, current_train_items 107648.
I0302 18:59:27.196158 22683591385216 run.py:483] Algo bellman_ford step 3364 current loss 0.114161, current_train_items 107680.
I0302 18:59:27.214470 22683591385216 run.py:483] Algo bellman_ford step 3365 current loss 0.064046, current_train_items 107712.
I0302 18:59:27.229358 22683591385216 run.py:483] Algo bellman_ford step 3366 current loss 0.016090, current_train_items 107744.
I0302 18:59:27.252295 22683591385216 run.py:483] Algo bellman_ford step 3367 current loss 0.061817, current_train_items 107776.
I0302 18:59:27.279827 22683591385216 run.py:483] Algo bellman_ford step 3368 current loss 0.058480, current_train_items 107808.
I0302 18:59:27.309838 22683591385216 run.py:483] Algo bellman_ford step 3369 current loss 0.092507, current_train_items 107840.
I0302 18:59:27.328334 22683591385216 run.py:483] Algo bellman_ford step 3370 current loss 0.004603, current_train_items 107872.
I0302 18:59:27.344044 22683591385216 run.py:483] Algo bellman_ford step 3371 current loss 0.032901, current_train_items 107904.
I0302 18:59:27.365863 22683591385216 run.py:483] Algo bellman_ford step 3372 current loss 0.069659, current_train_items 107936.
I0302 18:59:27.396211 22683591385216 run.py:483] Algo bellman_ford step 3373 current loss 0.082729, current_train_items 107968.
I0302 18:59:27.426515 22683591385216 run.py:483] Algo bellman_ford step 3374 current loss 0.112648, current_train_items 108000.
I0302 18:59:27.444854 22683591385216 run.py:483] Algo bellman_ford step 3375 current loss 0.005583, current_train_items 108032.
I0302 18:59:27.460000 22683591385216 run.py:483] Algo bellman_ford step 3376 current loss 0.017110, current_train_items 108064.
I0302 18:59:27.481247 22683591385216 run.py:483] Algo bellman_ford step 3377 current loss 0.067208, current_train_items 108096.
I0302 18:59:27.509149 22683591385216 run.py:483] Algo bellman_ford step 3378 current loss 0.069007, current_train_items 108128.
I0302 18:59:27.541329 22683591385216 run.py:483] Algo bellman_ford step 3379 current loss 0.132129, current_train_items 108160.
I0302 18:59:27.559240 22683591385216 run.py:483] Algo bellman_ford step 3380 current loss 0.002046, current_train_items 108192.
I0302 18:59:27.574824 22683591385216 run.py:483] Algo bellman_ford step 3381 current loss 0.050321, current_train_items 108224.
I0302 18:59:27.597907 22683591385216 run.py:483] Algo bellman_ford step 3382 current loss 0.047242, current_train_items 108256.
I0302 18:59:27.625766 22683591385216 run.py:483] Algo bellman_ford step 3383 current loss 0.038205, current_train_items 108288.
I0302 18:59:27.658632 22683591385216 run.py:483] Algo bellman_ford step 3384 current loss 0.153271, current_train_items 108320.
I0302 18:59:27.677441 22683591385216 run.py:483] Algo bellman_ford step 3385 current loss 0.002932, current_train_items 108352.
I0302 18:59:27.693289 22683591385216 run.py:483] Algo bellman_ford step 3386 current loss 0.021225, current_train_items 108384.
I0302 18:59:27.715031 22683591385216 run.py:483] Algo bellman_ford step 3387 current loss 0.049208, current_train_items 108416.
I0302 18:59:27.743289 22683591385216 run.py:483] Algo bellman_ford step 3388 current loss 0.074349, current_train_items 108448.
I0302 18:59:27.772794 22683591385216 run.py:483] Algo bellman_ford step 3389 current loss 0.107861, current_train_items 108480.
I0302 18:59:27.791023 22683591385216 run.py:483] Algo bellman_ford step 3390 current loss 0.034642, current_train_items 108512.
I0302 18:59:27.806470 22683591385216 run.py:483] Algo bellman_ford step 3391 current loss 0.012089, current_train_items 108544.
I0302 18:59:27.828473 22683591385216 run.py:483] Algo bellman_ford step 3392 current loss 0.047041, current_train_items 108576.
I0302 18:59:27.856935 22683591385216 run.py:483] Algo bellman_ford step 3393 current loss 0.052542, current_train_items 108608.
I0302 18:59:27.886094 22683591385216 run.py:483] Algo bellman_ford step 3394 current loss 0.047801, current_train_items 108640.
I0302 18:59:27.904193 22683591385216 run.py:483] Algo bellman_ford step 3395 current loss 0.003535, current_train_items 108672.
I0302 18:59:27.919609 22683591385216 run.py:483] Algo bellman_ford step 3396 current loss 0.030896, current_train_items 108704.
I0302 18:59:27.942027 22683591385216 run.py:483] Algo bellman_ford step 3397 current loss 0.029076, current_train_items 108736.
I0302 18:59:27.971281 22683591385216 run.py:483] Algo bellman_ford step 3398 current loss 0.083005, current_train_items 108768.
I0302 18:59:28.001212 22683591385216 run.py:483] Algo bellman_ford step 3399 current loss 0.092475, current_train_items 108800.
I0302 18:59:28.019613 22683591385216 run.py:483] Algo bellman_ford step 3400 current loss 0.001453, current_train_items 108832.
I0302 18:59:28.027398 22683591385216 run.py:503] (val) algo bellman_ford step 3400: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 108832, 'step': 3400, 'algorithm': 'bellman_ford'}
I0302 18:59:28.027503 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 18:59:28.043709 22683591385216 run.py:483] Algo bellman_ford step 3401 current loss 0.013117, current_train_items 108864.
I0302 18:59:28.066429 22683591385216 run.py:483] Algo bellman_ford step 3402 current loss 0.032995, current_train_items 108896.
I0302 18:59:28.096102 22683591385216 run.py:483] Algo bellman_ford step 3403 current loss 0.071174, current_train_items 108928.
I0302 18:59:28.129204 22683591385216 run.py:483] Algo bellman_ford step 3404 current loss 0.105539, current_train_items 108960.
I0302 18:59:28.147813 22683591385216 run.py:483] Algo bellman_ford step 3405 current loss 0.003177, current_train_items 108992.
I0302 18:59:28.162699 22683591385216 run.py:483] Algo bellman_ford step 3406 current loss 0.026600, current_train_items 109024.
I0302 18:59:28.185677 22683591385216 run.py:483] Algo bellman_ford step 3407 current loss 0.074875, current_train_items 109056.
I0302 18:59:28.216264 22683591385216 run.py:483] Algo bellman_ford step 3408 current loss 0.124103, current_train_items 109088.
I0302 18:59:28.246982 22683591385216 run.py:483] Algo bellman_ford step 3409 current loss 0.053141, current_train_items 109120.
I0302 18:59:28.265264 22683591385216 run.py:483] Algo bellman_ford step 3410 current loss 0.003385, current_train_items 109152.
I0302 18:59:28.280823 22683591385216 run.py:483] Algo bellman_ford step 3411 current loss 0.044254, current_train_items 109184.
I0302 18:59:28.304220 22683591385216 run.py:483] Algo bellman_ford step 3412 current loss 0.075894, current_train_items 109216.
I0302 18:59:28.334687 22683591385216 run.py:483] Algo bellman_ford step 3413 current loss 0.067113, current_train_items 109248.
I0302 18:59:28.368695 22683591385216 run.py:483] Algo bellman_ford step 3414 current loss 0.118089, current_train_items 109280.
I0302 18:59:28.386888 22683591385216 run.py:483] Algo bellman_ford step 3415 current loss 0.016522, current_train_items 109312.
I0302 18:59:28.402319 22683591385216 run.py:483] Algo bellman_ford step 3416 current loss 0.024133, current_train_items 109344.
I0302 18:59:28.425163 22683591385216 run.py:483] Algo bellman_ford step 3417 current loss 0.104422, current_train_items 109376.
I0302 18:59:28.454561 22683591385216 run.py:483] Algo bellman_ford step 3418 current loss 0.075589, current_train_items 109408.
I0302 18:59:28.484893 22683591385216 run.py:483] Algo bellman_ford step 3419 current loss 0.069762, current_train_items 109440.
I0302 18:59:28.503271 22683591385216 run.py:483] Algo bellman_ford step 3420 current loss 0.004146, current_train_items 109472.
I0302 18:59:28.518334 22683591385216 run.py:483] Algo bellman_ford step 3421 current loss 0.025172, current_train_items 109504.
I0302 18:59:28.541156 22683591385216 run.py:483] Algo bellman_ford step 3422 current loss 0.079590, current_train_items 109536.
I0302 18:59:28.570862 22683591385216 run.py:483] Algo bellman_ford step 3423 current loss 0.055110, current_train_items 109568.
I0302 18:59:28.601960 22683591385216 run.py:483] Algo bellman_ford step 3424 current loss 0.080328, current_train_items 109600.
I0302 18:59:28.620118 22683591385216 run.py:483] Algo bellman_ford step 3425 current loss 0.009073, current_train_items 109632.
I0302 18:59:28.635907 22683591385216 run.py:483] Algo bellman_ford step 3426 current loss 0.038837, current_train_items 109664.
I0302 18:59:28.657995 22683591385216 run.py:483] Algo bellman_ford step 3427 current loss 0.049343, current_train_items 109696.
I0302 18:59:28.687275 22683591385216 run.py:483] Algo bellman_ford step 3428 current loss 0.086111, current_train_items 109728.
I0302 18:59:28.716131 22683591385216 run.py:483] Algo bellman_ford step 3429 current loss 0.049276, current_train_items 109760.
I0302 18:59:28.734508 22683591385216 run.py:483] Algo bellman_ford step 3430 current loss 0.005890, current_train_items 109792.
I0302 18:59:28.750140 22683591385216 run.py:483] Algo bellman_ford step 3431 current loss 0.006078, current_train_items 109824.
I0302 18:59:28.772095 22683591385216 run.py:483] Algo bellman_ford step 3432 current loss 0.022432, current_train_items 109856.
I0302 18:59:28.802869 22683591385216 run.py:483] Algo bellman_ford step 3433 current loss 0.090577, current_train_items 109888.
I0302 18:59:28.832932 22683591385216 run.py:483] Algo bellman_ford step 3434 current loss 0.108557, current_train_items 109920.
I0302 18:59:28.851417 22683591385216 run.py:483] Algo bellman_ford step 3435 current loss 0.004923, current_train_items 109952.
I0302 18:59:28.866725 22683591385216 run.py:483] Algo bellman_ford step 3436 current loss 0.025674, current_train_items 109984.
I0302 18:59:28.889188 22683591385216 run.py:483] Algo bellman_ford step 3437 current loss 0.043251, current_train_items 110016.
I0302 18:59:28.917273 22683591385216 run.py:483] Algo bellman_ford step 3438 current loss 0.068486, current_train_items 110048.
I0302 18:59:28.947017 22683591385216 run.py:483] Algo bellman_ford step 3439 current loss 0.054551, current_train_items 110080.
I0302 18:59:28.965043 22683591385216 run.py:483] Algo bellman_ford step 3440 current loss 0.016709, current_train_items 110112.
I0302 18:59:28.980674 22683591385216 run.py:483] Algo bellman_ford step 3441 current loss 0.026437, current_train_items 110144.
I0302 18:59:29.002532 22683591385216 run.py:483] Algo bellman_ford step 3442 current loss 0.036411, current_train_items 110176.
I0302 18:59:29.032301 22683591385216 run.py:483] Algo bellman_ford step 3443 current loss 0.076660, current_train_items 110208.
I0302 18:59:29.064884 22683591385216 run.py:483] Algo bellman_ford step 3444 current loss 0.104359, current_train_items 110240.
I0302 18:59:29.083113 22683591385216 run.py:483] Algo bellman_ford step 3445 current loss 0.001579, current_train_items 110272.
I0302 18:59:29.098811 22683591385216 run.py:483] Algo bellman_ford step 3446 current loss 0.020652, current_train_items 110304.
I0302 18:59:29.121300 22683591385216 run.py:483] Algo bellman_ford step 3447 current loss 0.065316, current_train_items 110336.
I0302 18:59:29.150335 22683591385216 run.py:483] Algo bellman_ford step 3448 current loss 0.056322, current_train_items 110368.
I0302 18:59:29.181575 22683591385216 run.py:483] Algo bellman_ford step 3449 current loss 0.101776, current_train_items 110400.
I0302 18:59:29.200053 22683591385216 run.py:483] Algo bellman_ford step 3450 current loss 0.006011, current_train_items 110432.
I0302 18:59:29.208168 22683591385216 run.py:503] (val) algo bellman_ford step 3450: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 110432, 'step': 3450, 'algorithm': 'bellman_ford'}
I0302 18:59:29.208272 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:29.224051 22683591385216 run.py:483] Algo bellman_ford step 3451 current loss 0.020745, current_train_items 110464.
I0302 18:59:29.247397 22683591385216 run.py:483] Algo bellman_ford step 3452 current loss 0.092748, current_train_items 110496.
I0302 18:59:29.275616 22683591385216 run.py:483] Algo bellman_ford step 3453 current loss 0.070147, current_train_items 110528.
I0302 18:59:29.309730 22683591385216 run.py:483] Algo bellman_ford step 3454 current loss 0.110410, current_train_items 110560.
I0302 18:59:29.328359 22683591385216 run.py:483] Algo bellman_ford step 3455 current loss 0.007157, current_train_items 110592.
I0302 18:59:29.343221 22683591385216 run.py:483] Algo bellman_ford step 3456 current loss 0.029153, current_train_items 110624.
I0302 18:59:29.365610 22683591385216 run.py:483] Algo bellman_ford step 3457 current loss 0.075933, current_train_items 110656.
I0302 18:59:29.396002 22683591385216 run.py:483] Algo bellman_ford step 3458 current loss 0.123145, current_train_items 110688.
I0302 18:59:29.426136 22683591385216 run.py:483] Algo bellman_ford step 3459 current loss 0.110792, current_train_items 110720.
I0302 18:59:29.444855 22683591385216 run.py:483] Algo bellman_ford step 3460 current loss 0.016549, current_train_items 110752.
I0302 18:59:29.460670 22683591385216 run.py:483] Algo bellman_ford step 3461 current loss 0.050322, current_train_items 110784.
I0302 18:59:29.482599 22683591385216 run.py:483] Algo bellman_ford step 3462 current loss 0.063017, current_train_items 110816.
I0302 18:59:29.511416 22683591385216 run.py:483] Algo bellman_ford step 3463 current loss 0.047190, current_train_items 110848.
I0302 18:59:29.542060 22683591385216 run.py:483] Algo bellman_ford step 3464 current loss 0.097799, current_train_items 110880.
I0302 18:59:29.560137 22683591385216 run.py:483] Algo bellman_ford step 3465 current loss 0.011706, current_train_items 110912.
I0302 18:59:29.575698 22683591385216 run.py:483] Algo bellman_ford step 3466 current loss 0.031035, current_train_items 110944.
I0302 18:59:29.598397 22683591385216 run.py:483] Algo bellman_ford step 3467 current loss 0.096948, current_train_items 110976.
I0302 18:59:29.628970 22683591385216 run.py:483] Algo bellman_ford step 3468 current loss 0.111880, current_train_items 111008.
I0302 18:59:29.662516 22683591385216 run.py:483] Algo bellman_ford step 3469 current loss 0.113768, current_train_items 111040.
I0302 18:59:29.681384 22683591385216 run.py:483] Algo bellman_ford step 3470 current loss 0.005592, current_train_items 111072.
I0302 18:59:29.697452 22683591385216 run.py:483] Algo bellman_ford step 3471 current loss 0.027701, current_train_items 111104.
I0302 18:59:29.719218 22683591385216 run.py:483] Algo bellman_ford step 3472 current loss 0.111972, current_train_items 111136.
I0302 18:59:29.749740 22683591385216 run.py:483] Algo bellman_ford step 3473 current loss 0.165671, current_train_items 111168.
I0302 18:59:29.781013 22683591385216 run.py:483] Algo bellman_ford step 3474 current loss 0.110327, current_train_items 111200.
I0302 18:59:29.799671 22683591385216 run.py:483] Algo bellman_ford step 3475 current loss 0.004562, current_train_items 111232.
I0302 18:59:29.815185 22683591385216 run.py:483] Algo bellman_ford step 3476 current loss 0.041473, current_train_items 111264.
I0302 18:59:29.836786 22683591385216 run.py:483] Algo bellman_ford step 3477 current loss 0.083466, current_train_items 111296.
I0302 18:59:29.864940 22683591385216 run.py:483] Algo bellman_ford step 3478 current loss 0.074412, current_train_items 111328.
I0302 18:59:29.897288 22683591385216 run.py:483] Algo bellman_ford step 3479 current loss 0.124592, current_train_items 111360.
I0302 18:59:29.915529 22683591385216 run.py:483] Algo bellman_ford step 3480 current loss 0.004721, current_train_items 111392.
I0302 18:59:29.930673 22683591385216 run.py:483] Algo bellman_ford step 3481 current loss 0.017107, current_train_items 111424.
I0302 18:59:29.952619 22683591385216 run.py:483] Algo bellman_ford step 3482 current loss 0.025795, current_train_items 111456.
I0302 18:59:29.980859 22683591385216 run.py:483] Algo bellman_ford step 3483 current loss 0.172587, current_train_items 111488.
I0302 18:59:30.011141 22683591385216 run.py:483] Algo bellman_ford step 3484 current loss 0.084892, current_train_items 111520.
I0302 18:59:30.029956 22683591385216 run.py:483] Algo bellman_ford step 3485 current loss 0.001776, current_train_items 111552.
I0302 18:59:30.045745 22683591385216 run.py:483] Algo bellman_ford step 3486 current loss 0.015170, current_train_items 111584.
I0302 18:59:30.068158 22683591385216 run.py:483] Algo bellman_ford step 3487 current loss 0.077387, current_train_items 111616.
I0302 18:59:30.096595 22683591385216 run.py:483] Algo bellman_ford step 3488 current loss 0.071392, current_train_items 111648.
I0302 18:59:30.126132 22683591385216 run.py:483] Algo bellman_ford step 3489 current loss 0.075993, current_train_items 111680.
I0302 18:59:30.144679 22683591385216 run.py:483] Algo bellman_ford step 3490 current loss 0.004173, current_train_items 111712.
I0302 18:59:30.159915 22683591385216 run.py:483] Algo bellman_ford step 3491 current loss 0.027556, current_train_items 111744.
I0302 18:59:30.181912 22683591385216 run.py:483] Algo bellman_ford step 3492 current loss 0.043713, current_train_items 111776.
I0302 18:59:30.211860 22683591385216 run.py:483] Algo bellman_ford step 3493 current loss 0.070571, current_train_items 111808.
I0302 18:59:30.242434 22683591385216 run.py:483] Algo bellman_ford step 3494 current loss 0.050096, current_train_items 111840.
I0302 18:59:30.260421 22683591385216 run.py:483] Algo bellman_ford step 3495 current loss 0.002587, current_train_items 111872.
I0302 18:59:30.275674 22683591385216 run.py:483] Algo bellman_ford step 3496 current loss 0.062777, current_train_items 111904.
I0302 18:59:30.297753 22683591385216 run.py:483] Algo bellman_ford step 3497 current loss 0.061680, current_train_items 111936.
I0302 18:59:30.326692 22683591385216 run.py:483] Algo bellman_ford step 3498 current loss 0.097777, current_train_items 111968.
I0302 18:59:30.358431 22683591385216 run.py:483] Algo bellman_ford step 3499 current loss 0.081940, current_train_items 112000.
I0302 18:59:30.376978 22683591385216 run.py:483] Algo bellman_ford step 3500 current loss 0.001578, current_train_items 112032.
I0302 18:59:30.384827 22683591385216 run.py:503] (val) algo bellman_ford step 3500: {'pi': 0.9619140625, 'score': 0.9619140625, 'examples_seen': 112032, 'step': 3500, 'algorithm': 'bellman_ford'}
I0302 18:59:30.384943 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.962, val scores are: bellman_ford: 0.962
I0302 18:59:30.400254 22683591385216 run.py:483] Algo bellman_ford step 3501 current loss 0.019152, current_train_items 112064.
I0302 18:59:30.423168 22683591385216 run.py:483] Algo bellman_ford step 3502 current loss 0.147581, current_train_items 112096.
I0302 18:59:30.453062 22683591385216 run.py:483] Algo bellman_ford step 3503 current loss 0.145387, current_train_items 112128.
I0302 18:59:30.486505 22683591385216 run.py:483] Algo bellman_ford step 3504 current loss 0.157242, current_train_items 112160.
I0302 18:59:30.505190 22683591385216 run.py:483] Algo bellman_ford step 3505 current loss 0.027211, current_train_items 112192.
I0302 18:59:30.519748 22683591385216 run.py:483] Algo bellman_ford step 3506 current loss 0.024024, current_train_items 112224.
I0302 18:59:30.542598 22683591385216 run.py:483] Algo bellman_ford step 3507 current loss 0.072892, current_train_items 112256.
I0302 18:59:30.570895 22683591385216 run.py:483] Algo bellman_ford step 3508 current loss 0.100519, current_train_items 112288.
I0302 18:59:30.599542 22683591385216 run.py:483] Algo bellman_ford step 3509 current loss 0.100162, current_train_items 112320.
I0302 18:59:30.617885 22683591385216 run.py:483] Algo bellman_ford step 3510 current loss 0.016198, current_train_items 112352.
I0302 18:59:30.633023 22683591385216 run.py:483] Algo bellman_ford step 3511 current loss 0.021574, current_train_items 112384.
I0302 18:59:30.654693 22683591385216 run.py:483] Algo bellman_ford step 3512 current loss 0.018568, current_train_items 112416.
I0302 18:59:30.682854 22683591385216 run.py:483] Algo bellman_ford step 3513 current loss 0.050028, current_train_items 112448.
I0302 18:59:30.712919 22683591385216 run.py:483] Algo bellman_ford step 3514 current loss 0.075968, current_train_items 112480.
I0302 18:59:30.730716 22683591385216 run.py:483] Algo bellman_ford step 3515 current loss 0.003323, current_train_items 112512.
I0302 18:59:30.745525 22683591385216 run.py:483] Algo bellman_ford step 3516 current loss 0.007640, current_train_items 112544.
I0302 18:59:30.768372 22683591385216 run.py:483] Algo bellman_ford step 3517 current loss 0.024909, current_train_items 112576.
I0302 18:59:30.797953 22683591385216 run.py:483] Algo bellman_ford step 3518 current loss 0.052089, current_train_items 112608.
I0302 18:59:30.831714 22683591385216 run.py:483] Algo bellman_ford step 3519 current loss 0.089708, current_train_items 112640.
I0302 18:59:30.850200 22683591385216 run.py:483] Algo bellman_ford step 3520 current loss 0.004857, current_train_items 112672.
I0302 18:59:30.865397 22683591385216 run.py:483] Algo bellman_ford step 3521 current loss 0.020152, current_train_items 112704.
I0302 18:59:30.886799 22683591385216 run.py:483] Algo bellman_ford step 3522 current loss 0.026679, current_train_items 112736.
I0302 18:59:30.916039 22683591385216 run.py:483] Algo bellman_ford step 3523 current loss 0.061063, current_train_items 112768.
I0302 18:59:30.947200 22683591385216 run.py:483] Algo bellman_ford step 3524 current loss 0.116750, current_train_items 112800.
I0302 18:59:30.965436 22683591385216 run.py:483] Algo bellman_ford step 3525 current loss 0.026660, current_train_items 112832.
I0302 18:59:30.980515 22683591385216 run.py:483] Algo bellman_ford step 3526 current loss 0.013685, current_train_items 112864.
I0302 18:59:31.003037 22683591385216 run.py:483] Algo bellman_ford step 3527 current loss 0.046796, current_train_items 112896.
I0302 18:59:31.033975 22683591385216 run.py:483] Algo bellman_ford step 3528 current loss 0.057251, current_train_items 112928.
I0302 18:59:31.064030 22683591385216 run.py:483] Algo bellman_ford step 3529 current loss 0.041699, current_train_items 112960.
I0302 18:59:31.082047 22683591385216 run.py:483] Algo bellman_ford step 3530 current loss 0.002565, current_train_items 112992.
I0302 18:59:31.097589 22683591385216 run.py:483] Algo bellman_ford step 3531 current loss 0.018213, current_train_items 113024.
I0302 18:59:31.121326 22683591385216 run.py:483] Algo bellman_ford step 3532 current loss 0.046142, current_train_items 113056.
I0302 18:59:31.153390 22683591385216 run.py:483] Algo bellman_ford step 3533 current loss 0.096623, current_train_items 113088.
I0302 18:59:31.185225 22683591385216 run.py:483] Algo bellman_ford step 3534 current loss 0.107710, current_train_items 113120.
I0302 18:59:31.203370 22683591385216 run.py:483] Algo bellman_ford step 3535 current loss 0.019057, current_train_items 113152.
I0302 18:59:31.218778 22683591385216 run.py:483] Algo bellman_ford step 3536 current loss 0.033277, current_train_items 113184.
I0302 18:59:31.241165 22683591385216 run.py:483] Algo bellman_ford step 3537 current loss 0.037186, current_train_items 113216.
I0302 18:59:31.271248 22683591385216 run.py:483] Algo bellman_ford step 3538 current loss 0.104479, current_train_items 113248.
I0302 18:59:31.304638 22683591385216 run.py:483] Algo bellman_ford step 3539 current loss 0.079759, current_train_items 113280.
I0302 18:59:31.323075 22683591385216 run.py:483] Algo bellman_ford step 3540 current loss 0.002741, current_train_items 113312.
I0302 18:59:31.338242 22683591385216 run.py:483] Algo bellman_ford step 3541 current loss 0.010116, current_train_items 113344.
I0302 18:59:31.361146 22683591385216 run.py:483] Algo bellman_ford step 3542 current loss 0.090230, current_train_items 113376.
I0302 18:59:31.389782 22683591385216 run.py:483] Algo bellman_ford step 3543 current loss 0.067390, current_train_items 113408.
I0302 18:59:31.420601 22683591385216 run.py:483] Algo bellman_ford step 3544 current loss 0.077226, current_train_items 113440.
I0302 18:59:31.438751 22683591385216 run.py:483] Algo bellman_ford step 3545 current loss 0.001443, current_train_items 113472.
I0302 18:59:31.453855 22683591385216 run.py:483] Algo bellman_ford step 3546 current loss 0.015697, current_train_items 113504.
I0302 18:59:31.476855 22683591385216 run.py:483] Algo bellman_ford step 3547 current loss 0.082264, current_train_items 113536.
I0302 18:59:31.504997 22683591385216 run.py:483] Algo bellman_ford step 3548 current loss 0.054457, current_train_items 113568.
I0302 18:59:31.536350 22683591385216 run.py:483] Algo bellman_ford step 3549 current loss 0.116523, current_train_items 113600.
I0302 18:59:31.554770 22683591385216 run.py:483] Algo bellman_ford step 3550 current loss 0.004132, current_train_items 113632.
I0302 18:59:31.562746 22683591385216 run.py:503] (val) algo bellman_ford step 3550: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 113632, 'step': 3550, 'algorithm': 'bellman_ford'}
I0302 18:59:31.562853 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 18:59:31.579113 22683591385216 run.py:483] Algo bellman_ford step 3551 current loss 0.024683, current_train_items 113664.
I0302 18:59:31.603246 22683591385216 run.py:483] Algo bellman_ford step 3552 current loss 0.066352, current_train_items 113696.
I0302 18:59:31.632825 22683591385216 run.py:483] Algo bellman_ford step 3553 current loss 0.046802, current_train_items 113728.
I0302 18:59:31.664920 22683591385216 run.py:483] Algo bellman_ford step 3554 current loss 0.081144, current_train_items 113760.
I0302 18:59:31.683772 22683591385216 run.py:483] Algo bellman_ford step 3555 current loss 0.010506, current_train_items 113792.
I0302 18:59:31.699084 22683591385216 run.py:483] Algo bellman_ford step 3556 current loss 0.014131, current_train_items 113824.
I0302 18:59:31.721505 22683591385216 run.py:483] Algo bellman_ford step 3557 current loss 0.059407, current_train_items 113856.
I0302 18:59:31.751302 22683591385216 run.py:483] Algo bellman_ford step 3558 current loss 0.063504, current_train_items 113888.
I0302 18:59:31.781923 22683591385216 run.py:483] Algo bellman_ford step 3559 current loss 0.071344, current_train_items 113920.
I0302 18:59:31.800574 22683591385216 run.py:483] Algo bellman_ford step 3560 current loss 0.003931, current_train_items 113952.
I0302 18:59:31.816115 22683591385216 run.py:483] Algo bellman_ford step 3561 current loss 0.026134, current_train_items 113984.
I0302 18:59:31.839424 22683591385216 run.py:483] Algo bellman_ford step 3562 current loss 0.136573, current_train_items 114016.
I0302 18:59:31.867851 22683591385216 run.py:483] Algo bellman_ford step 3563 current loss 0.116574, current_train_items 114048.
I0302 18:59:31.901993 22683591385216 run.py:483] Algo bellman_ford step 3564 current loss 0.154152, current_train_items 114080.
I0302 18:59:31.920388 22683591385216 run.py:483] Algo bellman_ford step 3565 current loss 0.004533, current_train_items 114112.
I0302 18:59:31.936009 22683591385216 run.py:483] Algo bellman_ford step 3566 current loss 0.023170, current_train_items 114144.
I0302 18:59:31.958594 22683591385216 run.py:483] Algo bellman_ford step 3567 current loss 0.057632, current_train_items 114176.
I0302 18:59:31.987936 22683591385216 run.py:483] Algo bellman_ford step 3568 current loss 0.059369, current_train_items 114208.
I0302 18:59:32.019669 22683591385216 run.py:483] Algo bellman_ford step 3569 current loss 0.068444, current_train_items 114240.
I0302 18:59:32.038027 22683591385216 run.py:483] Algo bellman_ford step 3570 current loss 0.004517, current_train_items 114272.
I0302 18:59:32.053296 22683591385216 run.py:483] Algo bellman_ford step 3571 current loss 0.020237, current_train_items 114304.
I0302 18:59:32.075386 22683591385216 run.py:483] Algo bellman_ford step 3572 current loss 0.101151, current_train_items 114336.
I0302 18:59:32.103586 22683591385216 run.py:483] Algo bellman_ford step 3573 current loss 0.107946, current_train_items 114368.
I0302 18:59:32.136710 22683591385216 run.py:483] Algo bellman_ford step 3574 current loss 0.119206, current_train_items 114400.
I0302 18:59:32.155209 22683591385216 run.py:483] Algo bellman_ford step 3575 current loss 0.001677, current_train_items 114432.
I0302 18:59:32.170923 22683591385216 run.py:483] Algo bellman_ford step 3576 current loss 0.040925, current_train_items 114464.
I0302 18:59:32.193035 22683591385216 run.py:483] Algo bellman_ford step 3577 current loss 0.038748, current_train_items 114496.
I0302 18:59:32.222073 22683591385216 run.py:483] Algo bellman_ford step 3578 current loss 0.039063, current_train_items 114528.
I0302 18:59:32.252719 22683591385216 run.py:483] Algo bellman_ford step 3579 current loss 0.045089, current_train_items 114560.
I0302 18:59:32.271154 22683591385216 run.py:483] Algo bellman_ford step 3580 current loss 0.001979, current_train_items 114592.
I0302 18:59:32.286476 22683591385216 run.py:483] Algo bellman_ford step 3581 current loss 0.059138, current_train_items 114624.
I0302 18:59:32.310076 22683591385216 run.py:483] Algo bellman_ford step 3582 current loss 0.066108, current_train_items 114656.
I0302 18:59:32.340178 22683591385216 run.py:483] Algo bellman_ford step 3583 current loss 0.035527, current_train_items 114688.
I0302 18:59:32.372014 22683591385216 run.py:483] Algo bellman_ford step 3584 current loss 0.068685, current_train_items 114720.
I0302 18:59:32.390657 22683591385216 run.py:483] Algo bellman_ford step 3585 current loss 0.001057, current_train_items 114752.
I0302 18:59:32.406263 22683591385216 run.py:483] Algo bellman_ford step 3586 current loss 0.029723, current_train_items 114784.
I0302 18:59:32.427778 22683591385216 run.py:483] Algo bellman_ford step 3587 current loss 0.080781, current_train_items 114816.
I0302 18:59:32.457718 22683591385216 run.py:483] Algo bellman_ford step 3588 current loss 0.068779, current_train_items 114848.
I0302 18:59:32.490125 22683591385216 run.py:483] Algo bellman_ford step 3589 current loss 0.080926, current_train_items 114880.
I0302 18:59:32.508618 22683591385216 run.py:483] Algo bellman_ford step 3590 current loss 0.005108, current_train_items 114912.
I0302 18:59:32.523958 22683591385216 run.py:483] Algo bellman_ford step 3591 current loss 0.050364, current_train_items 114944.
I0302 18:59:32.547443 22683591385216 run.py:483] Algo bellman_ford step 3592 current loss 0.082544, current_train_items 114976.
I0302 18:59:32.578058 22683591385216 run.py:483] Algo bellman_ford step 3593 current loss 0.105588, current_train_items 115008.
I0302 18:59:32.610312 22683591385216 run.py:483] Algo bellman_ford step 3594 current loss 0.059215, current_train_items 115040.
I0302 18:59:32.628686 22683591385216 run.py:483] Algo bellman_ford step 3595 current loss 0.001526, current_train_items 115072.
I0302 18:59:32.644346 22683591385216 run.py:483] Algo bellman_ford step 3596 current loss 0.050643, current_train_items 115104.
I0302 18:59:32.666274 22683591385216 run.py:483] Algo bellman_ford step 3597 current loss 0.051416, current_train_items 115136.
I0302 18:59:32.694185 22683591385216 run.py:483] Algo bellman_ford step 3598 current loss 0.084773, current_train_items 115168.
I0302 18:59:32.725929 22683591385216 run.py:483] Algo bellman_ford step 3599 current loss 0.090266, current_train_items 115200.
I0302 18:59:32.744739 22683591385216 run.py:483] Algo bellman_ford step 3600 current loss 0.001323, current_train_items 115232.
I0302 18:59:32.752578 22683591385216 run.py:503] (val) algo bellman_ford step 3600: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 115232, 'step': 3600, 'algorithm': 'bellman_ford'}
I0302 18:59:32.752685 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:32.768923 22683591385216 run.py:483] Algo bellman_ford step 3601 current loss 0.015846, current_train_items 115264.
I0302 18:59:32.792380 22683591385216 run.py:483] Algo bellman_ford step 3602 current loss 0.060934, current_train_items 115296.
I0302 18:59:32.821524 22683591385216 run.py:483] Algo bellman_ford step 3603 current loss 0.080221, current_train_items 115328.
I0302 18:59:32.856068 22683591385216 run.py:483] Algo bellman_ford step 3604 current loss 0.121395, current_train_items 115360.
I0302 18:59:32.874328 22683591385216 run.py:483] Algo bellman_ford step 3605 current loss 0.003320, current_train_items 115392.
I0302 18:59:32.890072 22683591385216 run.py:483] Algo bellman_ford step 3606 current loss 0.030917, current_train_items 115424.
I0302 18:59:32.911617 22683591385216 run.py:483] Algo bellman_ford step 3607 current loss 0.023113, current_train_items 115456.
I0302 18:59:32.939689 22683591385216 run.py:483] Algo bellman_ford step 3608 current loss 0.061229, current_train_items 115488.
I0302 18:59:32.969109 22683591385216 run.py:483] Algo bellman_ford step 3609 current loss 0.058944, current_train_items 115520.
I0302 18:59:32.987622 22683591385216 run.py:483] Algo bellman_ford step 3610 current loss 0.001546, current_train_items 115552.
I0302 18:59:33.003175 22683591385216 run.py:483] Algo bellman_ford step 3611 current loss 0.033270, current_train_items 115584.
I0302 18:59:33.026587 22683591385216 run.py:483] Algo bellman_ford step 3612 current loss 0.044718, current_train_items 115616.
I0302 18:59:33.055393 22683591385216 run.py:483] Algo bellman_ford step 3613 current loss 0.062467, current_train_items 115648.
I0302 18:59:33.085106 22683591385216 run.py:483] Algo bellman_ford step 3614 current loss 0.064668, current_train_items 115680.
I0302 18:59:33.103507 22683591385216 run.py:483] Algo bellman_ford step 3615 current loss 0.018193, current_train_items 115712.
I0302 18:59:33.119311 22683591385216 run.py:483] Algo bellman_ford step 3616 current loss 0.018829, current_train_items 115744.
I0302 18:59:33.141511 22683591385216 run.py:483] Algo bellman_ford step 3617 current loss 0.027836, current_train_items 115776.
I0302 18:59:33.171144 22683591385216 run.py:483] Algo bellman_ford step 3618 current loss 0.084174, current_train_items 115808.
I0302 18:59:33.199342 22683591385216 run.py:483] Algo bellman_ford step 3619 current loss 0.042844, current_train_items 115840.
I0302 18:59:33.217533 22683591385216 run.py:483] Algo bellman_ford step 3620 current loss 0.001628, current_train_items 115872.
I0302 18:59:33.232987 22683591385216 run.py:483] Algo bellman_ford step 3621 current loss 0.056957, current_train_items 115904.
I0302 18:59:33.255102 22683591385216 run.py:483] Algo bellman_ford step 3622 current loss 0.061161, current_train_items 115936.
I0302 18:59:33.284795 22683591385216 run.py:483] Algo bellman_ford step 3623 current loss 0.125301, current_train_items 115968.
I0302 18:59:33.315405 22683591385216 run.py:483] Algo bellman_ford step 3624 current loss 0.090978, current_train_items 116000.
I0302 18:59:33.333757 22683591385216 run.py:483] Algo bellman_ford step 3625 current loss 0.002013, current_train_items 116032.
I0302 18:59:33.349256 22683591385216 run.py:483] Algo bellman_ford step 3626 current loss 0.032836, current_train_items 116064.
I0302 18:59:33.371903 22683591385216 run.py:483] Algo bellman_ford step 3627 current loss 0.086090, current_train_items 116096.
I0302 18:59:33.401540 22683591385216 run.py:483] Algo bellman_ford step 3628 current loss 0.129411, current_train_items 116128.
I0302 18:59:33.431987 22683591385216 run.py:483] Algo bellman_ford step 3629 current loss 0.058711, current_train_items 116160.
I0302 18:59:33.450453 22683591385216 run.py:483] Algo bellman_ford step 3630 current loss 0.019024, current_train_items 116192.
I0302 18:59:33.465623 22683591385216 run.py:483] Algo bellman_ford step 3631 current loss 0.057500, current_train_items 116224.
I0302 18:59:33.488507 22683591385216 run.py:483] Algo bellman_ford step 3632 current loss 0.064219, current_train_items 116256.
I0302 18:59:33.517591 22683591385216 run.py:483] Algo bellman_ford step 3633 current loss 0.092624, current_train_items 116288.
I0302 18:59:33.548121 22683591385216 run.py:483] Algo bellman_ford step 3634 current loss 0.100444, current_train_items 116320.
I0302 18:59:33.566390 22683591385216 run.py:483] Algo bellman_ford step 3635 current loss 0.001890, current_train_items 116352.
I0302 18:59:33.581690 22683591385216 run.py:483] Algo bellman_ford step 3636 current loss 0.024487, current_train_items 116384.
I0302 18:59:33.603064 22683591385216 run.py:483] Algo bellman_ford step 3637 current loss 0.074562, current_train_items 116416.
I0302 18:59:33.632515 22683591385216 run.py:483] Algo bellman_ford step 3638 current loss 0.135237, current_train_items 116448.
I0302 18:59:33.663003 22683591385216 run.py:483] Algo bellman_ford step 3639 current loss 0.084141, current_train_items 116480.
I0302 18:59:33.681169 22683591385216 run.py:483] Algo bellman_ford step 3640 current loss 0.020526, current_train_items 116512.
I0302 18:59:33.696109 22683591385216 run.py:483] Algo bellman_ford step 3641 current loss 0.019258, current_train_items 116544.
I0302 18:59:33.719295 22683591385216 run.py:483] Algo bellman_ford step 3642 current loss 0.078866, current_train_items 116576.
I0302 18:59:33.751432 22683591385216 run.py:483] Algo bellman_ford step 3643 current loss 0.078103, current_train_items 116608.
I0302 18:59:33.783791 22683591385216 run.py:483] Algo bellman_ford step 3644 current loss 0.118191, current_train_items 116640.
I0302 18:59:33.801999 22683591385216 run.py:483] Algo bellman_ford step 3645 current loss 0.002006, current_train_items 116672.
I0302 18:59:33.817368 22683591385216 run.py:483] Algo bellman_ford step 3646 current loss 0.022734, current_train_items 116704.
I0302 18:59:33.841662 22683591385216 run.py:483] Algo bellman_ford step 3647 current loss 0.044705, current_train_items 116736.
I0302 18:59:33.870225 22683591385216 run.py:483] Algo bellman_ford step 3648 current loss 0.034966, current_train_items 116768.
I0302 18:59:33.902016 22683591385216 run.py:483] Algo bellman_ford step 3649 current loss 0.088662, current_train_items 116800.
I0302 18:59:33.919971 22683591385216 run.py:483] Algo bellman_ford step 3650 current loss 0.001999, current_train_items 116832.
I0302 18:59:33.928136 22683591385216 run.py:503] (val) algo bellman_ford step 3650: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 116832, 'step': 3650, 'algorithm': 'bellman_ford'}
I0302 18:59:33.928242 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:59:33.944287 22683591385216 run.py:483] Algo bellman_ford step 3651 current loss 0.017048, current_train_items 116864.
I0302 18:59:33.965847 22683591385216 run.py:483] Algo bellman_ford step 3652 current loss 0.076679, current_train_items 116896.
I0302 18:59:33.994799 22683591385216 run.py:483] Algo bellman_ford step 3653 current loss 0.046716, current_train_items 116928.
I0302 18:59:34.025745 22683591385216 run.py:483] Algo bellman_ford step 3654 current loss 0.090766, current_train_items 116960.
I0302 18:59:34.043891 22683591385216 run.py:483] Algo bellman_ford step 3655 current loss 0.002714, current_train_items 116992.
I0302 18:59:34.058730 22683591385216 run.py:483] Algo bellman_ford step 3656 current loss 0.012579, current_train_items 117024.
I0302 18:59:34.081325 22683591385216 run.py:483] Algo bellman_ford step 3657 current loss 0.055867, current_train_items 117056.
I0302 18:59:34.109890 22683591385216 run.py:483] Algo bellman_ford step 3658 current loss 0.076883, current_train_items 117088.
I0302 18:59:34.140850 22683591385216 run.py:483] Algo bellman_ford step 3659 current loss 0.060041, current_train_items 117120.
I0302 18:59:34.159244 22683591385216 run.py:483] Algo bellman_ford step 3660 current loss 0.002707, current_train_items 117152.
I0302 18:59:34.175042 22683591385216 run.py:483] Algo bellman_ford step 3661 current loss 0.018778, current_train_items 117184.
I0302 18:59:34.196372 22683591385216 run.py:483] Algo bellman_ford step 3662 current loss 0.019051, current_train_items 117216.
I0302 18:59:34.226353 22683591385216 run.py:483] Algo bellman_ford step 3663 current loss 0.078564, current_train_items 117248.
I0302 18:59:34.258584 22683591385216 run.py:483] Algo bellman_ford step 3664 current loss 0.065963, current_train_items 117280.
I0302 18:59:34.276713 22683591385216 run.py:483] Algo bellman_ford step 3665 current loss 0.007711, current_train_items 117312.
I0302 18:59:34.291722 22683591385216 run.py:483] Algo bellman_ford step 3666 current loss 0.016627, current_train_items 117344.
I0302 18:59:34.314205 22683591385216 run.py:483] Algo bellman_ford step 3667 current loss 0.056958, current_train_items 117376.
I0302 18:59:34.343676 22683591385216 run.py:483] Algo bellman_ford step 3668 current loss 0.079431, current_train_items 117408.
I0302 18:59:34.376424 22683591385216 run.py:483] Algo bellman_ford step 3669 current loss 0.074316, current_train_items 117440.
I0302 18:59:34.394867 22683591385216 run.py:483] Algo bellman_ford step 3670 current loss 0.012064, current_train_items 117472.
I0302 18:59:34.410606 22683591385216 run.py:483] Algo bellman_ford step 3671 current loss 0.107681, current_train_items 117504.
I0302 18:59:34.432031 22683591385216 run.py:483] Algo bellman_ford step 3672 current loss 0.079646, current_train_items 117536.
I0302 18:59:34.461310 22683591385216 run.py:483] Algo bellman_ford step 3673 current loss 0.081425, current_train_items 117568.
I0302 18:59:34.493142 22683591385216 run.py:483] Algo bellman_ford step 3674 current loss 0.089900, current_train_items 117600.
I0302 18:59:34.511365 22683591385216 run.py:483] Algo bellman_ford step 3675 current loss 0.001010, current_train_items 117632.
I0302 18:59:34.526485 22683591385216 run.py:483] Algo bellman_ford step 3676 current loss 0.015545, current_train_items 117664.
I0302 18:59:34.548206 22683591385216 run.py:483] Algo bellman_ford step 3677 current loss 0.047209, current_train_items 117696.
I0302 18:59:34.577330 22683591385216 run.py:483] Algo bellman_ford step 3678 current loss 0.111981, current_train_items 117728.
I0302 18:59:34.609052 22683591385216 run.py:483] Algo bellman_ford step 3679 current loss 0.134566, current_train_items 117760.
I0302 18:59:34.626938 22683591385216 run.py:483] Algo bellman_ford step 3680 current loss 0.001731, current_train_items 117792.
I0302 18:59:34.642491 22683591385216 run.py:483] Algo bellman_ford step 3681 current loss 0.010703, current_train_items 117824.
I0302 18:59:34.664956 22683591385216 run.py:483] Algo bellman_ford step 3682 current loss 0.031067, current_train_items 117856.
I0302 18:59:34.693354 22683591385216 run.py:483] Algo bellman_ford step 3683 current loss 0.056797, current_train_items 117888.
I0302 18:59:34.724720 22683591385216 run.py:483] Algo bellman_ford step 3684 current loss 0.060820, current_train_items 117920.
I0302 18:59:34.743023 22683591385216 run.py:483] Algo bellman_ford step 3685 current loss 0.001226, current_train_items 117952.
I0302 18:59:34.758517 22683591385216 run.py:483] Algo bellman_ford step 3686 current loss 0.024184, current_train_items 117984.
I0302 18:59:34.780502 22683591385216 run.py:483] Algo bellman_ford step 3687 current loss 0.032204, current_train_items 118016.
I0302 18:59:34.810709 22683591385216 run.py:483] Algo bellman_ford step 3688 current loss 0.062225, current_train_items 118048.
I0302 18:59:34.843642 22683591385216 run.py:483] Algo bellman_ford step 3689 current loss 0.042585, current_train_items 118080.
I0302 18:59:34.862127 22683591385216 run.py:483] Algo bellman_ford step 3690 current loss 0.008327, current_train_items 118112.
I0302 18:59:34.877613 22683591385216 run.py:483] Algo bellman_ford step 3691 current loss 0.012538, current_train_items 118144.
I0302 18:59:34.898094 22683591385216 run.py:483] Algo bellman_ford step 3692 current loss 0.013737, current_train_items 118176.
I0302 18:59:34.928369 22683591385216 run.py:483] Algo bellman_ford step 3693 current loss 0.039794, current_train_items 118208.
I0302 18:59:34.958609 22683591385216 run.py:483] Algo bellman_ford step 3694 current loss 0.035633, current_train_items 118240.
I0302 18:59:34.976826 22683591385216 run.py:483] Algo bellman_ford step 3695 current loss 0.001951, current_train_items 118272.
I0302 18:59:34.992701 22683591385216 run.py:483] Algo bellman_ford step 3696 current loss 0.038136, current_train_items 118304.
I0302 18:59:35.015053 22683591385216 run.py:483] Algo bellman_ford step 3697 current loss 0.022487, current_train_items 118336.
I0302 18:59:35.044870 22683591385216 run.py:483] Algo bellman_ford step 3698 current loss 0.037147, current_train_items 118368.
I0302 18:59:35.074918 22683591385216 run.py:483] Algo bellman_ford step 3699 current loss 0.076359, current_train_items 118400.
I0302 18:59:35.093489 22683591385216 run.py:483] Algo bellman_ford step 3700 current loss 0.002206, current_train_items 118432.
I0302 18:59:35.101203 22683591385216 run.py:503] (val) algo bellman_ford step 3700: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 118432, 'step': 3700, 'algorithm': 'bellman_ford'}
I0302 18:59:35.101310 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 18:59:35.117005 22683591385216 run.py:483] Algo bellman_ford step 3701 current loss 0.015534, current_train_items 118464.
I0302 18:59:35.140074 22683591385216 run.py:483] Algo bellman_ford step 3702 current loss 0.059298, current_train_items 118496.
I0302 18:59:35.171590 22683591385216 run.py:483] Algo bellman_ford step 3703 current loss 0.058072, current_train_items 118528.
I0302 18:59:35.203147 22683591385216 run.py:483] Algo bellman_ford step 3704 current loss 0.043774, current_train_items 118560.
I0302 18:59:35.221499 22683591385216 run.py:483] Algo bellman_ford step 3705 current loss 0.000810, current_train_items 118592.
I0302 18:59:35.237015 22683591385216 run.py:483] Algo bellman_ford step 3706 current loss 0.023998, current_train_items 118624.
I0302 18:59:35.259728 22683591385216 run.py:483] Algo bellman_ford step 3707 current loss 0.040821, current_train_items 118656.
I0302 18:59:35.288180 22683591385216 run.py:483] Algo bellman_ford step 3708 current loss 0.049634, current_train_items 118688.
I0302 18:59:35.319636 22683591385216 run.py:483] Algo bellman_ford step 3709 current loss 0.053965, current_train_items 118720.
I0302 18:59:35.338164 22683591385216 run.py:483] Algo bellman_ford step 3710 current loss 0.006490, current_train_items 118752.
I0302 18:59:35.353632 22683591385216 run.py:483] Algo bellman_ford step 3711 current loss 0.013475, current_train_items 118784.
I0302 18:59:35.376195 22683591385216 run.py:483] Algo bellman_ford step 3712 current loss 0.050466, current_train_items 118816.
I0302 18:59:35.406963 22683591385216 run.py:483] Algo bellman_ford step 3713 current loss 0.065986, current_train_items 118848.
I0302 18:59:35.439222 22683591385216 run.py:483] Algo bellman_ford step 3714 current loss 0.079183, current_train_items 118880.
I0302 18:59:35.457136 22683591385216 run.py:483] Algo bellman_ford step 3715 current loss 0.002714, current_train_items 118912.
I0302 18:59:35.472235 22683591385216 run.py:483] Algo bellman_ford step 3716 current loss 0.018590, current_train_items 118944.
I0302 18:59:35.494406 22683591385216 run.py:483] Algo bellman_ford step 3717 current loss 0.047624, current_train_items 118976.
I0302 18:59:35.523911 22683591385216 run.py:483] Algo bellman_ford step 3718 current loss 0.143287, current_train_items 119008.
I0302 18:59:35.554185 22683591385216 run.py:483] Algo bellman_ford step 3719 current loss 0.054327, current_train_items 119040.
I0302 18:59:35.572288 22683591385216 run.py:483] Algo bellman_ford step 3720 current loss 0.002207, current_train_items 119072.
I0302 18:59:35.587780 22683591385216 run.py:483] Algo bellman_ford step 3721 current loss 0.037227, current_train_items 119104.
I0302 18:59:35.610261 22683591385216 run.py:483] Algo bellman_ford step 3722 current loss 0.022633, current_train_items 119136.
I0302 18:59:35.639267 22683591385216 run.py:483] Algo bellman_ford step 3723 current loss 0.083075, current_train_items 119168.
I0302 18:59:35.670258 22683591385216 run.py:483] Algo bellman_ford step 3724 current loss 0.058677, current_train_items 119200.
I0302 18:59:35.688414 22683591385216 run.py:483] Algo bellman_ford step 3725 current loss 0.001179, current_train_items 119232.
I0302 18:59:35.704095 22683591385216 run.py:483] Algo bellman_ford step 3726 current loss 0.010860, current_train_items 119264.
I0302 18:59:35.727422 22683591385216 run.py:483] Algo bellman_ford step 3727 current loss 0.063362, current_train_items 119296.
I0302 18:59:35.756982 22683591385216 run.py:483] Algo bellman_ford step 3728 current loss 0.057071, current_train_items 119328.
I0302 18:59:35.787992 22683591385216 run.py:483] Algo bellman_ford step 3729 current loss 0.057202, current_train_items 119360.
I0302 18:59:35.806044 22683591385216 run.py:483] Algo bellman_ford step 3730 current loss 0.008208, current_train_items 119392.
I0302 18:59:35.821218 22683591385216 run.py:483] Algo bellman_ford step 3731 current loss 0.022663, current_train_items 119424.
I0302 18:59:35.844025 22683591385216 run.py:483] Algo bellman_ford step 3732 current loss 0.114081, current_train_items 119456.
I0302 18:59:35.871527 22683591385216 run.py:483] Algo bellman_ford step 3733 current loss 0.086440, current_train_items 119488.
I0302 18:59:35.903782 22683591385216 run.py:483] Algo bellman_ford step 3734 current loss 0.117797, current_train_items 119520.
I0302 18:59:35.922068 22683591385216 run.py:483] Algo bellman_ford step 3735 current loss 0.010869, current_train_items 119552.
I0302 18:59:35.937157 22683591385216 run.py:483] Algo bellman_ford step 3736 current loss 0.026795, current_train_items 119584.
I0302 18:59:35.958803 22683591385216 run.py:483] Algo bellman_ford step 3737 current loss 0.020339, current_train_items 119616.
I0302 18:59:35.988106 22683591385216 run.py:483] Algo bellman_ford step 3738 current loss 0.080634, current_train_items 119648.
I0302 18:59:36.019002 22683591385216 run.py:483] Algo bellman_ford step 3739 current loss 0.079653, current_train_items 119680.
I0302 18:59:36.037418 22683591385216 run.py:483] Algo bellman_ford step 3740 current loss 0.016214, current_train_items 119712.
I0302 18:59:36.052626 22683591385216 run.py:483] Algo bellman_ford step 3741 current loss 0.022242, current_train_items 119744.
I0302 18:59:36.074651 22683591385216 run.py:483] Algo bellman_ford step 3742 current loss 0.072067, current_train_items 119776.
I0302 18:59:36.103725 22683591385216 run.py:483] Algo bellman_ford step 3743 current loss 0.071886, current_train_items 119808.
I0302 18:59:36.132856 22683591385216 run.py:483] Algo bellman_ford step 3744 current loss 0.077819, current_train_items 119840.
I0302 18:59:36.151227 22683591385216 run.py:483] Algo bellman_ford step 3745 current loss 0.029419, current_train_items 119872.
I0302 18:59:36.166255 22683591385216 run.py:483] Algo bellman_ford step 3746 current loss 0.005874, current_train_items 119904.
I0302 18:59:36.189074 22683591385216 run.py:483] Algo bellman_ford step 3747 current loss 0.048004, current_train_items 119936.
I0302 18:59:36.218713 22683591385216 run.py:483] Algo bellman_ford step 3748 current loss 0.088857, current_train_items 119968.
I0302 18:59:36.251622 22683591385216 run.py:483] Algo bellman_ford step 3749 current loss 0.128280, current_train_items 120000.
I0302 18:59:36.269844 22683591385216 run.py:483] Algo bellman_ford step 3750 current loss 0.004523, current_train_items 120032.
I0302 18:59:36.277663 22683591385216 run.py:503] (val) algo bellman_ford step 3750: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 120032, 'step': 3750, 'algorithm': 'bellman_ford'}
I0302 18:59:36.277796 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 18:59:36.293928 22683591385216 run.py:483] Algo bellman_ford step 3751 current loss 0.036006, current_train_items 120064.
I0302 18:59:36.316575 22683591385216 run.py:483] Algo bellman_ford step 3752 current loss 0.044978, current_train_items 120096.
I0302 18:59:36.345669 22683591385216 run.py:483] Algo bellman_ford step 3753 current loss 0.059638, current_train_items 120128.
I0302 18:59:36.379227 22683591385216 run.py:483] Algo bellman_ford step 3754 current loss 0.073424, current_train_items 120160.
I0302 18:59:36.397470 22683591385216 run.py:483] Algo bellman_ford step 3755 current loss 0.002557, current_train_items 120192.
I0302 18:59:36.412727 22683591385216 run.py:483] Algo bellman_ford step 3756 current loss 0.025133, current_train_items 120224.
I0302 18:59:36.435112 22683591385216 run.py:483] Algo bellman_ford step 3757 current loss 0.057513, current_train_items 120256.
I0302 18:59:36.464262 22683591385216 run.py:483] Algo bellman_ford step 3758 current loss 0.061378, current_train_items 120288.
I0302 18:59:36.493598 22683591385216 run.py:483] Algo bellman_ford step 3759 current loss 0.057820, current_train_items 120320.
I0302 18:59:36.512032 22683591385216 run.py:483] Algo bellman_ford step 3760 current loss 0.002068, current_train_items 120352.
I0302 18:59:36.527640 22683591385216 run.py:483] Algo bellman_ford step 3761 current loss 0.006690, current_train_items 120384.
I0302 18:59:36.548828 22683591385216 run.py:483] Algo bellman_ford step 3762 current loss 0.030777, current_train_items 120416.
I0302 18:59:36.577392 22683591385216 run.py:483] Algo bellman_ford step 3763 current loss 0.056706, current_train_items 120448.
I0302 18:59:36.608471 22683591385216 run.py:483] Algo bellman_ford step 3764 current loss 0.078138, current_train_items 120480.
I0302 18:59:36.626327 22683591385216 run.py:483] Algo bellman_ford step 3765 current loss 0.001599, current_train_items 120512.
I0302 18:59:36.642093 22683591385216 run.py:483] Algo bellman_ford step 3766 current loss 0.019002, current_train_items 120544.
I0302 18:59:36.664572 22683591385216 run.py:483] Algo bellman_ford step 3767 current loss 0.063009, current_train_items 120576.
I0302 18:59:36.694670 22683591385216 run.py:483] Algo bellman_ford step 3768 current loss 0.086626, current_train_items 120608.
I0302 18:59:36.724729 22683591385216 run.py:483] Algo bellman_ford step 3769 current loss 0.069697, current_train_items 120640.
I0302 18:59:36.743206 22683591385216 run.py:483] Algo bellman_ford step 3770 current loss 0.001884, current_train_items 120672.
I0302 18:59:36.758868 22683591385216 run.py:483] Algo bellman_ford step 3771 current loss 0.046105, current_train_items 120704.
I0302 18:59:36.780371 22683591385216 run.py:483] Algo bellman_ford step 3772 current loss 0.064028, current_train_items 120736.
I0302 18:59:36.808345 22683591385216 run.py:483] Algo bellman_ford step 3773 current loss 0.041679, current_train_items 120768.
I0302 18:59:36.836496 22683591385216 run.py:483] Algo bellman_ford step 3774 current loss 0.050412, current_train_items 120800.
I0302 18:59:36.854896 22683591385216 run.py:483] Algo bellman_ford step 3775 current loss 0.003880, current_train_items 120832.
I0302 18:59:36.870831 22683591385216 run.py:483] Algo bellman_ford step 3776 current loss 0.026424, current_train_items 120864.
I0302 18:59:36.892158 22683591385216 run.py:483] Algo bellman_ford step 3777 current loss 0.031680, current_train_items 120896.
I0302 18:59:36.921713 22683591385216 run.py:483] Algo bellman_ford step 3778 current loss 0.071863, current_train_items 120928.
I0302 18:59:36.953049 22683591385216 run.py:483] Algo bellman_ford step 3779 current loss 0.080365, current_train_items 120960.
I0302 18:59:36.970896 22683591385216 run.py:483] Algo bellman_ford step 3780 current loss 0.003403, current_train_items 120992.
I0302 18:59:36.986257 22683591385216 run.py:483] Algo bellman_ford step 3781 current loss 0.038121, current_train_items 121024.
I0302 18:59:37.008250 22683591385216 run.py:483] Algo bellman_ford step 3782 current loss 0.069234, current_train_items 121056.
I0302 18:59:37.035832 22683591385216 run.py:483] Algo bellman_ford step 3783 current loss 0.068547, current_train_items 121088.
I0302 18:59:37.069489 22683591385216 run.py:483] Algo bellman_ford step 3784 current loss 0.131072, current_train_items 121120.
I0302 18:59:37.088080 22683591385216 run.py:483] Algo bellman_ford step 3785 current loss 0.002535, current_train_items 121152.
I0302 18:59:37.103539 22683591385216 run.py:483] Algo bellman_ford step 3786 current loss 0.013187, current_train_items 121184.
I0302 18:59:37.125857 22683591385216 run.py:483] Algo bellman_ford step 3787 current loss 0.091666, current_train_items 121216.
I0302 18:59:37.154605 22683591385216 run.py:483] Algo bellman_ford step 3788 current loss 0.067360, current_train_items 121248.
I0302 18:59:37.185652 22683591385216 run.py:483] Algo bellman_ford step 3789 current loss 0.139525, current_train_items 121280.
I0302 18:59:37.203926 22683591385216 run.py:483] Algo bellman_ford step 3790 current loss 0.003489, current_train_items 121312.
I0302 18:59:37.219543 22683591385216 run.py:483] Algo bellman_ford step 3791 current loss 0.009321, current_train_items 121344.
I0302 18:59:37.240835 22683591385216 run.py:483] Algo bellman_ford step 3792 current loss 0.052429, current_train_items 121376.
I0302 18:59:37.270674 22683591385216 run.py:483] Algo bellman_ford step 3793 current loss 0.099715, current_train_items 121408.
I0302 18:59:37.301190 22683591385216 run.py:483] Algo bellman_ford step 3794 current loss 0.035717, current_train_items 121440.
I0302 18:59:37.319245 22683591385216 run.py:483] Algo bellman_ford step 3795 current loss 0.011489, current_train_items 121472.
I0302 18:59:37.334814 22683591385216 run.py:483] Algo bellman_ford step 3796 current loss 0.030677, current_train_items 121504.
I0302 18:59:37.357284 22683591385216 run.py:483] Algo bellman_ford step 3797 current loss 0.047398, current_train_items 121536.
I0302 18:59:37.387033 22683591385216 run.py:483] Algo bellman_ford step 3798 current loss 0.070842, current_train_items 121568.
I0302 18:59:37.417120 22683591385216 run.py:483] Algo bellman_ford step 3799 current loss 0.092275, current_train_items 121600.
I0302 18:59:37.435512 22683591385216 run.py:483] Algo bellman_ford step 3800 current loss 0.002812, current_train_items 121632.
I0302 18:59:37.443347 22683591385216 run.py:503] (val) algo bellman_ford step 3800: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 121632, 'step': 3800, 'algorithm': 'bellman_ford'}
I0302 18:59:37.443454 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 18:59:37.459295 22683591385216 run.py:483] Algo bellman_ford step 3801 current loss 0.017169, current_train_items 121664.
I0302 18:59:37.482650 22683591385216 run.py:483] Algo bellman_ford step 3802 current loss 0.068323, current_train_items 121696.
I0302 18:59:37.511792 22683591385216 run.py:483] Algo bellman_ford step 3803 current loss 0.088441, current_train_items 121728.
I0302 18:59:37.541493 22683591385216 run.py:483] Algo bellman_ford step 3804 current loss 0.064084, current_train_items 121760.
I0302 18:59:37.560038 22683591385216 run.py:483] Algo bellman_ford step 3805 current loss 0.004251, current_train_items 121792.
I0302 18:59:37.575425 22683591385216 run.py:483] Algo bellman_ford step 3806 current loss 0.053530, current_train_items 121824.
I0302 18:59:37.598164 22683591385216 run.py:483] Algo bellman_ford step 3807 current loss 0.079685, current_train_items 121856.
I0302 18:59:37.627537 22683591385216 run.py:483] Algo bellman_ford step 3808 current loss 0.070620, current_train_items 121888.
I0302 18:59:37.660121 22683591385216 run.py:483] Algo bellman_ford step 3809 current loss 0.069867, current_train_items 121920.
I0302 18:59:37.678267 22683591385216 run.py:483] Algo bellman_ford step 3810 current loss 0.035676, current_train_items 121952.
I0302 18:59:37.693799 22683591385216 run.py:483] Algo bellman_ford step 3811 current loss 0.009243, current_train_items 121984.
I0302 18:59:37.716510 22683591385216 run.py:483] Algo bellman_ford step 3812 current loss 0.035922, current_train_items 122016.
I0302 18:59:37.747817 22683591385216 run.py:483] Algo bellman_ford step 3813 current loss 0.068556, current_train_items 122048.
I0302 18:59:37.781012 22683591385216 run.py:483] Algo bellman_ford step 3814 current loss 0.104579, current_train_items 122080.
I0302 18:59:37.799665 22683591385216 run.py:483] Algo bellman_ford step 3815 current loss 0.007446, current_train_items 122112.
I0302 18:59:37.815434 22683591385216 run.py:483] Algo bellman_ford step 3816 current loss 0.042725, current_train_items 122144.
I0302 18:59:37.838061 22683591385216 run.py:483] Algo bellman_ford step 3817 current loss 0.060738, current_train_items 122176.
I0302 18:59:37.867451 22683591385216 run.py:483] Algo bellman_ford step 3818 current loss 0.085907, current_train_items 122208.
I0302 18:59:37.898962 22683591385216 run.py:483] Algo bellman_ford step 3819 current loss 0.112762, current_train_items 122240.
I0302 18:59:37.916952 22683591385216 run.py:483] Algo bellman_ford step 3820 current loss 0.002352, current_train_items 122272.
I0302 18:59:37.931916 22683591385216 run.py:483] Algo bellman_ford step 3821 current loss 0.012119, current_train_items 122304.
I0302 18:59:37.954180 22683591385216 run.py:483] Algo bellman_ford step 3822 current loss 0.046541, current_train_items 122336.
I0302 18:59:37.983484 22683591385216 run.py:483] Algo bellman_ford step 3823 current loss 0.049023, current_train_items 122368.
I0302 18:59:38.015745 22683591385216 run.py:483] Algo bellman_ford step 3824 current loss 0.091772, current_train_items 122400.
I0302 18:59:38.034122 22683591385216 run.py:483] Algo bellman_ford step 3825 current loss 0.007631, current_train_items 122432.
I0302 18:59:38.049084 22683591385216 run.py:483] Algo bellman_ford step 3826 current loss 0.020382, current_train_items 122464.
I0302 18:59:38.071817 22683591385216 run.py:483] Algo bellman_ford step 3827 current loss 0.026691, current_train_items 122496.
I0302 18:59:38.101504 22683591385216 run.py:483] Algo bellman_ford step 3828 current loss 0.053892, current_train_items 122528.
I0302 18:59:38.131527 22683591385216 run.py:483] Algo bellman_ford step 3829 current loss 0.093881, current_train_items 122560.
I0302 18:59:38.149635 22683591385216 run.py:483] Algo bellman_ford step 3830 current loss 0.000922, current_train_items 122592.
I0302 18:59:38.165127 22683591385216 run.py:483] Algo bellman_ford step 3831 current loss 0.029830, current_train_items 122624.
I0302 18:59:38.186525 22683591385216 run.py:483] Algo bellman_ford step 3832 current loss 0.062611, current_train_items 122656.
I0302 18:59:38.216758 22683591385216 run.py:483] Algo bellman_ford step 3833 current loss 0.068805, current_train_items 122688.
I0302 18:59:38.247853 22683591385216 run.py:483] Algo bellman_ford step 3834 current loss 0.051246, current_train_items 122720.
I0302 18:59:38.266199 22683591385216 run.py:483] Algo bellman_ford step 3835 current loss 0.003672, current_train_items 122752.
I0302 18:59:38.280989 22683591385216 run.py:483] Algo bellman_ford step 3836 current loss 0.007941, current_train_items 122784.
I0302 18:59:38.304656 22683591385216 run.py:483] Algo bellman_ford step 3837 current loss 0.064841, current_train_items 122816.
I0302 18:59:38.334262 22683591385216 run.py:483] Algo bellman_ford step 3838 current loss 0.072996, current_train_items 122848.
I0302 18:59:38.365397 22683591385216 run.py:483] Algo bellman_ford step 3839 current loss 0.071547, current_train_items 122880.
I0302 18:59:38.383605 22683591385216 run.py:483] Algo bellman_ford step 3840 current loss 0.012679, current_train_items 122912.
I0302 18:59:38.398562 22683591385216 run.py:483] Algo bellman_ford step 3841 current loss 0.029453, current_train_items 122944.
I0302 18:59:38.420181 22683591385216 run.py:483] Algo bellman_ford step 3842 current loss 0.030925, current_train_items 122976.
I0302 18:59:38.449461 22683591385216 run.py:483] Algo bellman_ford step 3843 current loss 0.045113, current_train_items 123008.
I0302 18:59:38.482210 22683591385216 run.py:483] Algo bellman_ford step 3844 current loss 0.082307, current_train_items 123040.
I0302 18:59:38.500639 22683591385216 run.py:483] Algo bellman_ford step 3845 current loss 0.001169, current_train_items 123072.
I0302 18:59:38.515873 22683591385216 run.py:483] Algo bellman_ford step 3846 current loss 0.021732, current_train_items 123104.
I0302 18:59:38.539621 22683591385216 run.py:483] Algo bellman_ford step 3847 current loss 0.098998, current_train_items 123136.
I0302 18:59:38.568133 22683591385216 run.py:483] Algo bellman_ford step 3848 current loss 0.041291, current_train_items 123168.
I0302 18:59:38.599220 22683591385216 run.py:483] Algo bellman_ford step 3849 current loss 0.062655, current_train_items 123200.
I0302 18:59:38.617586 22683591385216 run.py:483] Algo bellman_ford step 3850 current loss 0.007203, current_train_items 123232.
I0302 18:59:38.625465 22683591385216 run.py:503] (val) algo bellman_ford step 3850: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 123232, 'step': 3850, 'algorithm': 'bellman_ford'}
I0302 18:59:38.625574 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:38.641068 22683591385216 run.py:483] Algo bellman_ford step 3851 current loss 0.005546, current_train_items 123264.
I0302 18:59:38.662916 22683591385216 run.py:483] Algo bellman_ford step 3852 current loss 0.042596, current_train_items 123296.
I0302 18:59:38.693179 22683591385216 run.py:483] Algo bellman_ford step 3853 current loss 0.049745, current_train_items 123328.
I0302 18:59:38.726971 22683591385216 run.py:483] Algo bellman_ford step 3854 current loss 0.088562, current_train_items 123360.
I0302 18:59:38.745352 22683591385216 run.py:483] Algo bellman_ford step 3855 current loss 0.003774, current_train_items 123392.
I0302 18:59:38.760574 22683591385216 run.py:483] Algo bellman_ford step 3856 current loss 0.014402, current_train_items 123424.
I0302 18:59:38.781459 22683591385216 run.py:483] Algo bellman_ford step 3857 current loss 0.049206, current_train_items 123456.
I0302 18:59:38.811061 22683591385216 run.py:483] Algo bellman_ford step 3858 current loss 0.084673, current_train_items 123488.
I0302 18:59:38.844711 22683591385216 run.py:483] Algo bellman_ford step 3859 current loss 0.118171, current_train_items 123520.
I0302 18:59:38.863242 22683591385216 run.py:483] Algo bellman_ford step 3860 current loss 0.003759, current_train_items 123552.
I0302 18:59:38.879082 22683591385216 run.py:483] Algo bellman_ford step 3861 current loss 0.062762, current_train_items 123584.
I0302 18:59:38.900828 22683591385216 run.py:483] Algo bellman_ford step 3862 current loss 0.045175, current_train_items 123616.
I0302 18:59:38.930137 22683591385216 run.py:483] Algo bellman_ford step 3863 current loss 0.065024, current_train_items 123648.
I0302 18:59:38.964028 22683591385216 run.py:483] Algo bellman_ford step 3864 current loss 0.069250, current_train_items 123680.
I0302 18:59:38.981916 22683591385216 run.py:483] Algo bellman_ford step 3865 current loss 0.004023, current_train_items 123712.
I0302 18:59:38.996953 22683591385216 run.py:483] Algo bellman_ford step 3866 current loss 0.024344, current_train_items 123744.
I0302 18:59:39.018991 22683591385216 run.py:483] Algo bellman_ford step 3867 current loss 0.036284, current_train_items 123776.
I0302 18:59:39.047628 22683591385216 run.py:483] Algo bellman_ford step 3868 current loss 0.032420, current_train_items 123808.
I0302 18:59:39.078599 22683591385216 run.py:483] Algo bellman_ford step 3869 current loss 0.093581, current_train_items 123840.
I0302 18:59:39.097519 22683591385216 run.py:483] Algo bellman_ford step 3870 current loss 0.007010, current_train_items 123872.
I0302 18:59:39.112623 22683591385216 run.py:483] Algo bellman_ford step 3871 current loss 0.013138, current_train_items 123904.
I0302 18:59:39.135351 22683591385216 run.py:483] Algo bellman_ford step 3872 current loss 0.033552, current_train_items 123936.
I0302 18:59:39.163145 22683591385216 run.py:483] Algo bellman_ford step 3873 current loss 0.032037, current_train_items 123968.
I0302 18:59:39.193525 22683591385216 run.py:483] Algo bellman_ford step 3874 current loss 0.074773, current_train_items 124000.
I0302 18:59:39.212391 22683591385216 run.py:483] Algo bellman_ford step 3875 current loss 0.002837, current_train_items 124032.
I0302 18:59:39.228236 22683591385216 run.py:483] Algo bellman_ford step 3876 current loss 0.032776, current_train_items 124064.
I0302 18:59:39.250913 22683591385216 run.py:483] Algo bellman_ford step 3877 current loss 0.032576, current_train_items 124096.
I0302 18:59:39.280319 22683591385216 run.py:483] Algo bellman_ford step 3878 current loss 0.068275, current_train_items 124128.
I0302 18:59:39.311550 22683591385216 run.py:483] Algo bellman_ford step 3879 current loss 0.099552, current_train_items 124160.
I0302 18:59:39.329796 22683591385216 run.py:483] Algo bellman_ford step 3880 current loss 0.041473, current_train_items 124192.
I0302 18:59:39.344975 22683591385216 run.py:483] Algo bellman_ford step 3881 current loss 0.024584, current_train_items 124224.
I0302 18:59:39.367700 22683591385216 run.py:483] Algo bellman_ford step 3882 current loss 0.040028, current_train_items 124256.
I0302 18:59:39.396947 22683591385216 run.py:483] Algo bellman_ford step 3883 current loss 0.106743, current_train_items 124288.
I0302 18:59:39.426803 22683591385216 run.py:483] Algo bellman_ford step 3884 current loss 0.110389, current_train_items 124320.
I0302 18:59:39.445371 22683591385216 run.py:483] Algo bellman_ford step 3885 current loss 0.006809, current_train_items 124352.
I0302 18:59:39.461347 22683591385216 run.py:483] Algo bellman_ford step 3886 current loss 0.022349, current_train_items 124384.
I0302 18:59:39.483781 22683591385216 run.py:483] Algo bellman_ford step 3887 current loss 0.070048, current_train_items 124416.
I0302 18:59:39.513039 22683591385216 run.py:483] Algo bellman_ford step 3888 current loss 0.092720, current_train_items 124448.
I0302 18:59:39.545349 22683591385216 run.py:483] Algo bellman_ford step 3889 current loss 0.081300, current_train_items 124480.
I0302 18:59:39.563764 22683591385216 run.py:483] Algo bellman_ford step 3890 current loss 0.002493, current_train_items 124512.
I0302 18:59:39.579404 22683591385216 run.py:483] Algo bellman_ford step 3891 current loss 0.034656, current_train_items 124544.
I0302 18:59:39.602056 22683591385216 run.py:483] Algo bellman_ford step 3892 current loss 0.034616, current_train_items 124576.
I0302 18:59:39.629444 22683591385216 run.py:483] Algo bellman_ford step 3893 current loss 0.042068, current_train_items 124608.
I0302 18:59:39.661365 22683591385216 run.py:483] Algo bellman_ford step 3894 current loss 0.069824, current_train_items 124640.
I0302 18:59:39.679501 22683591385216 run.py:483] Algo bellman_ford step 3895 current loss 0.001360, current_train_items 124672.
I0302 18:59:39.695022 22683591385216 run.py:483] Algo bellman_ford step 3896 current loss 0.039994, current_train_items 124704.
I0302 18:59:39.718361 22683591385216 run.py:483] Algo bellman_ford step 3897 current loss 0.051508, current_train_items 124736.
I0302 18:59:39.748036 22683591385216 run.py:483] Algo bellman_ford step 3898 current loss 0.092193, current_train_items 124768.
I0302 18:59:39.779053 22683591385216 run.py:483] Algo bellman_ford step 3899 current loss 0.120401, current_train_items 124800.
I0302 18:59:39.797930 22683591385216 run.py:483] Algo bellman_ford step 3900 current loss 0.003644, current_train_items 124832.
I0302 18:59:39.805643 22683591385216 run.py:503] (val) algo bellman_ford step 3900: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 124832, 'step': 3900, 'algorithm': 'bellman_ford'}
I0302 18:59:39.805749 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.991, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:59:39.822089 22683591385216 run.py:483] Algo bellman_ford step 3901 current loss 0.048622, current_train_items 124864.
I0302 18:59:39.844994 22683591385216 run.py:483] Algo bellman_ford step 3902 current loss 0.056543, current_train_items 124896.
I0302 18:59:39.874991 22683591385216 run.py:483] Algo bellman_ford step 3903 current loss 0.157753, current_train_items 124928.
I0302 18:59:39.906496 22683591385216 run.py:483] Algo bellman_ford step 3904 current loss 0.092118, current_train_items 124960.
I0302 18:59:39.925347 22683591385216 run.py:483] Algo bellman_ford step 3905 current loss 0.006041, current_train_items 124992.
I0302 18:59:39.940516 22683591385216 run.py:483] Algo bellman_ford step 3906 current loss 0.021111, current_train_items 125024.
I0302 18:59:39.961949 22683591385216 run.py:483] Algo bellman_ford step 3907 current loss 0.043000, current_train_items 125056.
I0302 18:59:39.991415 22683591385216 run.py:483] Algo bellman_ford step 3908 current loss 0.040100, current_train_items 125088.
I0302 18:59:40.023222 22683591385216 run.py:483] Algo bellman_ford step 3909 current loss 0.063083, current_train_items 125120.
I0302 18:59:40.041571 22683591385216 run.py:483] Algo bellman_ford step 3910 current loss 0.001407, current_train_items 125152.
I0302 18:59:40.056674 22683591385216 run.py:483] Algo bellman_ford step 3911 current loss 0.008817, current_train_items 125184.
I0302 18:59:40.079267 22683591385216 run.py:483] Algo bellman_ford step 3912 current loss 0.029938, current_train_items 125216.
I0302 18:59:40.107962 22683591385216 run.py:483] Algo bellman_ford step 3913 current loss 0.050364, current_train_items 125248.
I0302 18:59:40.136177 22683591385216 run.py:483] Algo bellman_ford step 3914 current loss 0.052108, current_train_items 125280.
I0302 18:59:40.154056 22683591385216 run.py:483] Algo bellman_ford step 3915 current loss 0.010535, current_train_items 125312.
I0302 18:59:40.169134 22683591385216 run.py:483] Algo bellman_ford step 3916 current loss 0.007559, current_train_items 125344.
I0302 18:59:40.192449 22683591385216 run.py:483] Algo bellman_ford step 3917 current loss 0.081140, current_train_items 125376.
I0302 18:59:40.220684 22683591385216 run.py:483] Algo bellman_ford step 3918 current loss 0.080914, current_train_items 125408.
I0302 18:59:40.252094 22683591385216 run.py:483] Algo bellman_ford step 3919 current loss 0.069630, current_train_items 125440.
I0302 18:59:40.270303 22683591385216 run.py:483] Algo bellman_ford step 3920 current loss 0.001452, current_train_items 125472.
I0302 18:59:40.285815 22683591385216 run.py:483] Algo bellman_ford step 3921 current loss 0.010211, current_train_items 125504.
I0302 18:59:40.309430 22683591385216 run.py:483] Algo bellman_ford step 3922 current loss 0.050627, current_train_items 125536.
I0302 18:59:40.339441 22683591385216 run.py:483] Algo bellman_ford step 3923 current loss 0.089204, current_train_items 125568.
I0302 18:59:40.373255 22683591385216 run.py:483] Algo bellman_ford step 3924 current loss 0.094435, current_train_items 125600.
I0302 18:59:40.391725 22683591385216 run.py:483] Algo bellman_ford step 3925 current loss 0.001678, current_train_items 125632.
I0302 18:59:40.407077 22683591385216 run.py:483] Algo bellman_ford step 3926 current loss 0.019288, current_train_items 125664.
I0302 18:59:40.429737 22683591385216 run.py:483] Algo bellman_ford step 3927 current loss 0.073062, current_train_items 125696.
I0302 18:59:40.458754 22683591385216 run.py:483] Algo bellman_ford step 3928 current loss 0.122787, current_train_items 125728.
I0302 18:59:40.489684 22683591385216 run.py:483] Algo bellman_ford step 3929 current loss 0.094199, current_train_items 125760.
I0302 18:59:40.507688 22683591385216 run.py:483] Algo bellman_ford step 3930 current loss 0.001064, current_train_items 125792.
I0302 18:59:40.522706 22683591385216 run.py:483] Algo bellman_ford step 3931 current loss 0.031667, current_train_items 125824.
I0302 18:59:40.545019 22683591385216 run.py:483] Algo bellman_ford step 3932 current loss 0.106212, current_train_items 125856.
I0302 18:59:40.575613 22683591385216 run.py:483] Algo bellman_ford step 3933 current loss 0.147731, current_train_items 125888.
I0302 18:59:40.607246 22683591385216 run.py:483] Algo bellman_ford step 3934 current loss 0.106060, current_train_items 125920.
I0302 18:59:40.625414 22683591385216 run.py:483] Algo bellman_ford step 3935 current loss 0.034010, current_train_items 125952.
I0302 18:59:40.640990 22683591385216 run.py:483] Algo bellman_ford step 3936 current loss 0.026262, current_train_items 125984.
I0302 18:59:40.663095 22683591385216 run.py:483] Algo bellman_ford step 3937 current loss 0.044733, current_train_items 126016.
I0302 18:59:40.692682 22683591385216 run.py:483] Algo bellman_ford step 3938 current loss 0.094004, current_train_items 126048.
I0302 18:59:40.725208 22683591385216 run.py:483] Algo bellman_ford step 3939 current loss 0.097239, current_train_items 126080.
I0302 18:59:40.743111 22683591385216 run.py:483] Algo bellman_ford step 3940 current loss 0.002910, current_train_items 126112.
I0302 18:59:40.758285 22683591385216 run.py:483] Algo bellman_ford step 3941 current loss 0.014994, current_train_items 126144.
I0302 18:59:40.780460 22683591385216 run.py:483] Algo bellman_ford step 3942 current loss 0.043396, current_train_items 126176.
I0302 18:59:40.808834 22683591385216 run.py:483] Algo bellman_ford step 3943 current loss 0.034641, current_train_items 126208.
I0302 18:59:40.841184 22683591385216 run.py:483] Algo bellman_ford step 3944 current loss 0.072779, current_train_items 126240.
I0302 18:59:40.859301 22683591385216 run.py:483] Algo bellman_ford step 3945 current loss 0.013041, current_train_items 126272.
I0302 18:59:40.874809 22683591385216 run.py:483] Algo bellman_ford step 3946 current loss 0.014313, current_train_items 126304.
I0302 18:59:40.897114 22683591385216 run.py:483] Algo bellman_ford step 3947 current loss 0.050784, current_train_items 126336.
I0302 18:59:40.924469 22683591385216 run.py:483] Algo bellman_ford step 3948 current loss 0.082057, current_train_items 126368.
I0302 18:59:40.956403 22683591385216 run.py:483] Algo bellman_ford step 3949 current loss 0.050617, current_train_items 126400.
I0302 18:59:40.974646 22683591385216 run.py:483] Algo bellman_ford step 3950 current loss 0.002471, current_train_items 126432.
I0302 18:59:40.982601 22683591385216 run.py:503] (val) algo bellman_ford step 3950: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 126432, 'step': 3950, 'algorithm': 'bellman_ford'}
I0302 18:59:40.982706 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.991, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 18:59:41.011711 22683591385216 run.py:483] Algo bellman_ford step 3951 current loss 0.015416, current_train_items 126464.
I0302 18:59:41.034760 22683591385216 run.py:483] Algo bellman_ford step 3952 current loss 0.048285, current_train_items 126496.
I0302 18:59:41.063791 22683591385216 run.py:483] Algo bellman_ford step 3953 current loss 0.032447, current_train_items 126528.
I0302 18:59:41.093528 22683591385216 run.py:483] Algo bellman_ford step 3954 current loss 0.047596, current_train_items 126560.
I0302 18:59:41.111749 22683591385216 run.py:483] Algo bellman_ford step 3955 current loss 0.001213, current_train_items 126592.
I0302 18:59:41.126942 22683591385216 run.py:483] Algo bellman_ford step 3956 current loss 0.028880, current_train_items 126624.
I0302 18:59:41.149285 22683591385216 run.py:483] Algo bellman_ford step 3957 current loss 0.037830, current_train_items 126656.
I0302 18:59:41.179423 22683591385216 run.py:483] Algo bellman_ford step 3958 current loss 0.062410, current_train_items 126688.
I0302 18:59:41.212498 22683591385216 run.py:483] Algo bellman_ford step 3959 current loss 0.059456, current_train_items 126720.
I0302 18:59:41.231088 22683591385216 run.py:483] Algo bellman_ford step 3960 current loss 0.019023, current_train_items 126752.
I0302 18:59:41.246607 22683591385216 run.py:483] Algo bellman_ford step 3961 current loss 0.015895, current_train_items 126784.
I0302 18:59:41.267183 22683591385216 run.py:483] Algo bellman_ford step 3962 current loss 0.030600, current_train_items 126816.
I0302 18:59:41.296545 22683591385216 run.py:483] Algo bellman_ford step 3963 current loss 0.058847, current_train_items 126848.
I0302 18:59:41.327848 22683591385216 run.py:483] Algo bellman_ford step 3964 current loss 0.093313, current_train_items 126880.
I0302 18:59:41.345792 22683591385216 run.py:483] Algo bellman_ford step 3965 current loss 0.011925, current_train_items 126912.
I0302 18:59:41.361070 22683591385216 run.py:483] Algo bellman_ford step 3966 current loss 0.026720, current_train_items 126944.
I0302 18:59:41.384473 22683591385216 run.py:483] Algo bellman_ford step 3967 current loss 0.051837, current_train_items 126976.
I0302 18:59:41.413511 22683591385216 run.py:483] Algo bellman_ford step 3968 current loss 0.057164, current_train_items 127008.
I0302 18:59:41.444161 22683591385216 run.py:483] Algo bellman_ford step 3969 current loss 0.048932, current_train_items 127040.
I0302 18:59:41.462872 22683591385216 run.py:483] Algo bellman_ford step 3970 current loss 0.002381, current_train_items 127072.
I0302 18:59:41.478334 22683591385216 run.py:483] Algo bellman_ford step 3971 current loss 0.011497, current_train_items 127104.
I0302 18:59:41.500846 22683591385216 run.py:483] Algo bellman_ford step 3972 current loss 0.066708, current_train_items 127136.
I0302 18:59:41.530575 22683591385216 run.py:483] Algo bellman_ford step 3973 current loss 0.138813, current_train_items 127168.
I0302 18:59:41.560123 22683591385216 run.py:483] Algo bellman_ford step 3974 current loss 0.074263, current_train_items 127200.
I0302 18:59:41.578626 22683591385216 run.py:483] Algo bellman_ford step 3975 current loss 0.002662, current_train_items 127232.
I0302 18:59:41.594412 22683591385216 run.py:483] Algo bellman_ford step 3976 current loss 0.041187, current_train_items 127264.
I0302 18:59:41.616133 22683591385216 run.py:483] Algo bellman_ford step 3977 current loss 0.042783, current_train_items 127296.
I0302 18:59:41.646093 22683591385216 run.py:483] Algo bellman_ford step 3978 current loss 0.072233, current_train_items 127328.
I0302 18:59:41.676755 22683591385216 run.py:483] Algo bellman_ford step 3979 current loss 0.063867, current_train_items 127360.
I0302 18:59:41.694561 22683591385216 run.py:483] Algo bellman_ford step 3980 current loss 0.001235, current_train_items 127392.
I0302 18:59:41.709748 22683591385216 run.py:483] Algo bellman_ford step 3981 current loss 0.020292, current_train_items 127424.
I0302 18:59:41.731300 22683591385216 run.py:483] Algo bellman_ford step 3982 current loss 0.039775, current_train_items 127456.
I0302 18:59:41.760773 22683591385216 run.py:483] Algo bellman_ford step 3983 current loss 0.072779, current_train_items 127488.
I0302 18:59:41.790910 22683591385216 run.py:483] Algo bellman_ford step 3984 current loss 0.076358, current_train_items 127520.
I0302 18:59:41.809408 22683591385216 run.py:483] Algo bellman_ford step 3985 current loss 0.002178, current_train_items 127552.
I0302 18:59:41.825125 22683591385216 run.py:483] Algo bellman_ford step 3986 current loss 0.021661, current_train_items 127584.
I0302 18:59:41.846746 22683591385216 run.py:483] Algo bellman_ford step 3987 current loss 0.060275, current_train_items 127616.
I0302 18:59:41.876079 22683591385216 run.py:483] Algo bellman_ford step 3988 current loss 0.074591, current_train_items 127648.
I0302 18:59:41.907478 22683591385216 run.py:483] Algo bellman_ford step 3989 current loss 0.085162, current_train_items 127680.
I0302 18:59:41.925895 22683591385216 run.py:483] Algo bellman_ford step 3990 current loss 0.006143, current_train_items 127712.
I0302 18:59:41.941803 22683591385216 run.py:483] Algo bellman_ford step 3991 current loss 0.031527, current_train_items 127744.
I0302 18:59:41.962813 22683591385216 run.py:483] Algo bellman_ford step 3992 current loss 0.021304, current_train_items 127776.
I0302 18:59:41.991380 22683591385216 run.py:483] Algo bellman_ford step 3993 current loss 0.029579, current_train_items 127808.
I0302 18:59:42.022736 22683591385216 run.py:483] Algo bellman_ford step 3994 current loss 0.064673, current_train_items 127840.
I0302 18:59:42.041043 22683591385216 run.py:483] Algo bellman_ford step 3995 current loss 0.002191, current_train_items 127872.
I0302 18:59:42.056450 22683591385216 run.py:483] Algo bellman_ford step 3996 current loss 0.018336, current_train_items 127904.
I0302 18:59:42.078577 22683591385216 run.py:483] Algo bellman_ford step 3997 current loss 0.027128, current_train_items 127936.
I0302 18:59:42.107044 22683591385216 run.py:483] Algo bellman_ford step 3998 current loss 0.049675, current_train_items 127968.
I0302 18:59:42.138808 22683591385216 run.py:483] Algo bellman_ford step 3999 current loss 0.077638, current_train_items 128000.
I0302 18:59:42.157347 22683591385216 run.py:483] Algo bellman_ford step 4000 current loss 0.004861, current_train_items 128032.
I0302 18:59:42.164917 22683591385216 run.py:503] (val) algo bellman_ford step 4000: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 128032, 'step': 4000, 'algorithm': 'bellman_ford'}
I0302 18:59:42.165024 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:42.180866 22683591385216 run.py:483] Algo bellman_ford step 4001 current loss 0.018044, current_train_items 128064.
I0302 18:59:42.203529 22683591385216 run.py:483] Algo bellman_ford step 4002 current loss 0.097253, current_train_items 128096.
I0302 18:59:42.232775 22683591385216 run.py:483] Algo bellman_ford step 4003 current loss 0.089606, current_train_items 128128.
I0302 18:59:42.263475 22683591385216 run.py:483] Algo bellman_ford step 4004 current loss 0.125988, current_train_items 128160.
I0302 18:59:42.281759 22683591385216 run.py:483] Algo bellman_ford step 4005 current loss 0.002293, current_train_items 128192.
I0302 18:59:42.297074 22683591385216 run.py:483] Algo bellman_ford step 4006 current loss 0.020852, current_train_items 128224.
I0302 18:59:42.319311 22683591385216 run.py:483] Algo bellman_ford step 4007 current loss 0.131742, current_train_items 128256.
I0302 18:59:42.349338 22683591385216 run.py:483] Algo bellman_ford step 4008 current loss 0.055897, current_train_items 128288.
I0302 18:59:42.379153 22683591385216 run.py:483] Algo bellman_ford step 4009 current loss 0.094117, current_train_items 128320.
I0302 18:59:42.397423 22683591385216 run.py:483] Algo bellman_ford step 4010 current loss 0.003848, current_train_items 128352.
I0302 18:59:42.412599 22683591385216 run.py:483] Algo bellman_ford step 4011 current loss 0.014279, current_train_items 128384.
I0302 18:59:42.433890 22683591385216 run.py:483] Algo bellman_ford step 4012 current loss 0.055940, current_train_items 128416.
I0302 18:59:42.463151 22683591385216 run.py:483] Algo bellman_ford step 4013 current loss 0.122000, current_train_items 128448.
I0302 18:59:42.494570 22683591385216 run.py:483] Algo bellman_ford step 4014 current loss 0.122364, current_train_items 128480.
I0302 18:59:42.512599 22683591385216 run.py:483] Algo bellman_ford step 4015 current loss 0.002350, current_train_items 128512.
I0302 18:59:42.527872 22683591385216 run.py:483] Algo bellman_ford step 4016 current loss 0.034892, current_train_items 128544.
I0302 18:59:42.550236 22683591385216 run.py:483] Algo bellman_ford step 4017 current loss 0.043021, current_train_items 128576.
I0302 18:59:42.579351 22683591385216 run.py:483] Algo bellman_ford step 4018 current loss 0.081340, current_train_items 128608.
I0302 18:59:42.609602 22683591385216 run.py:483] Algo bellman_ford step 4019 current loss 0.076988, current_train_items 128640.
I0302 18:59:42.627731 22683591385216 run.py:483] Algo bellman_ford step 4020 current loss 0.002098, current_train_items 128672.
I0302 18:59:42.642868 22683591385216 run.py:483] Algo bellman_ford step 4021 current loss 0.019415, current_train_items 128704.
I0302 18:59:42.665815 22683591385216 run.py:483] Algo bellman_ford step 4022 current loss 0.093588, current_train_items 128736.
I0302 18:59:42.694016 22683591385216 run.py:483] Algo bellman_ford step 4023 current loss 0.041038, current_train_items 128768.
I0302 18:59:42.727669 22683591385216 run.py:483] Algo bellman_ford step 4024 current loss 0.155520, current_train_items 128800.
I0302 18:59:42.745763 22683591385216 run.py:483] Algo bellman_ford step 4025 current loss 0.004707, current_train_items 128832.
I0302 18:59:42.761238 22683591385216 run.py:483] Algo bellman_ford step 4026 current loss 0.026043, current_train_items 128864.
I0302 18:59:42.783140 22683591385216 run.py:483] Algo bellman_ford step 4027 current loss 0.047356, current_train_items 128896.
I0302 18:59:42.811336 22683591385216 run.py:483] Algo bellman_ford step 4028 current loss 0.056239, current_train_items 128928.
I0302 18:59:42.841680 22683591385216 run.py:483] Algo bellman_ford step 4029 current loss 0.054724, current_train_items 128960.
I0302 18:59:42.859498 22683591385216 run.py:483] Algo bellman_ford step 4030 current loss 0.002180, current_train_items 128992.
I0302 18:59:42.874514 22683591385216 run.py:483] Algo bellman_ford step 4031 current loss 0.025419, current_train_items 129024.
I0302 18:59:42.896936 22683591385216 run.py:483] Algo bellman_ford step 4032 current loss 0.072254, current_train_items 129056.
I0302 18:59:42.926739 22683591385216 run.py:483] Algo bellman_ford step 4033 current loss 0.106169, current_train_items 129088.
I0302 18:59:42.957473 22683591385216 run.py:483] Algo bellman_ford step 4034 current loss 0.055454, current_train_items 129120.
I0302 18:59:42.975434 22683591385216 run.py:483] Algo bellman_ford step 4035 current loss 0.002708, current_train_items 129152.
I0302 18:59:42.990468 22683591385216 run.py:483] Algo bellman_ford step 4036 current loss 0.033677, current_train_items 129184.
I0302 18:59:43.012314 22683591385216 run.py:483] Algo bellman_ford step 4037 current loss 0.067486, current_train_items 129216.
I0302 18:59:43.041985 22683591385216 run.py:483] Algo bellman_ford step 4038 current loss 0.074463, current_train_items 129248.
I0302 18:59:43.073573 22683591385216 run.py:483] Algo bellman_ford step 4039 current loss 0.075523, current_train_items 129280.
I0302 18:59:43.091684 22683591385216 run.py:483] Algo bellman_ford step 4040 current loss 0.002493, current_train_items 129312.
I0302 18:59:43.107040 22683591385216 run.py:483] Algo bellman_ford step 4041 current loss 0.008854, current_train_items 129344.
I0302 18:59:43.128791 22683591385216 run.py:483] Algo bellman_ford step 4042 current loss 0.094063, current_train_items 129376.
I0302 18:59:43.159245 22683591385216 run.py:483] Algo bellman_ford step 4043 current loss 0.144501, current_train_items 129408.
I0302 18:59:43.189342 22683591385216 run.py:483] Algo bellman_ford step 4044 current loss 0.091859, current_train_items 129440.
I0302 18:59:43.207329 22683591385216 run.py:483] Algo bellman_ford step 4045 current loss 0.002976, current_train_items 129472.
I0302 18:59:43.223008 22683591385216 run.py:483] Algo bellman_ford step 4046 current loss 0.032510, current_train_items 129504.
I0302 18:59:43.244774 22683591385216 run.py:483] Algo bellman_ford step 4047 current loss 0.098158, current_train_items 129536.
I0302 18:59:43.272944 22683591385216 run.py:483] Algo bellman_ford step 4048 current loss 0.106194, current_train_items 129568.
I0302 18:59:43.305015 22683591385216 run.py:483] Algo bellman_ford step 4049 current loss 0.103359, current_train_items 129600.
I0302 18:59:43.322857 22683591385216 run.py:483] Algo bellman_ford step 4050 current loss 0.002006, current_train_items 129632.
I0302 18:59:43.330825 22683591385216 run.py:503] (val) algo bellman_ford step 4050: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 129632, 'step': 4050, 'algorithm': 'bellman_ford'}
I0302 18:59:43.330936 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:43.346992 22683591385216 run.py:483] Algo bellman_ford step 4051 current loss 0.022845, current_train_items 129664.
I0302 18:59:43.368824 22683591385216 run.py:483] Algo bellman_ford step 4052 current loss 0.047365, current_train_items 129696.
I0302 18:59:43.397909 22683591385216 run.py:483] Algo bellman_ford step 4053 current loss 0.069539, current_train_items 129728.
I0302 18:59:43.430980 22683591385216 run.py:483] Algo bellman_ford step 4054 current loss 0.093291, current_train_items 129760.
I0302 18:59:43.449460 22683591385216 run.py:483] Algo bellman_ford step 4055 current loss 0.007472, current_train_items 129792.
I0302 18:59:43.464533 22683591385216 run.py:483] Algo bellman_ford step 4056 current loss 0.017607, current_train_items 129824.
I0302 18:59:43.486616 22683591385216 run.py:483] Algo bellman_ford step 4057 current loss 0.038124, current_train_items 129856.
I0302 18:59:43.514642 22683591385216 run.py:483] Algo bellman_ford step 4058 current loss 0.046041, current_train_items 129888.
I0302 18:59:43.544213 22683591385216 run.py:483] Algo bellman_ford step 4059 current loss 0.049309, current_train_items 129920.
I0302 18:59:43.562678 22683591385216 run.py:483] Algo bellman_ford step 4060 current loss 0.008960, current_train_items 129952.
I0302 18:59:43.578601 22683591385216 run.py:483] Algo bellman_ford step 4061 current loss 0.017502, current_train_items 129984.
I0302 18:59:43.599822 22683591385216 run.py:483] Algo bellman_ford step 4062 current loss 0.040777, current_train_items 130016.
I0302 18:59:43.627598 22683591385216 run.py:483] Algo bellman_ford step 4063 current loss 0.049100, current_train_items 130048.
I0302 18:59:43.660218 22683591385216 run.py:483] Algo bellman_ford step 4064 current loss 0.083044, current_train_items 130080.
I0302 18:59:43.678155 22683591385216 run.py:483] Algo bellman_ford step 4065 current loss 0.001396, current_train_items 130112.
I0302 18:59:43.693020 22683591385216 run.py:483] Algo bellman_ford step 4066 current loss 0.073309, current_train_items 130144.
I0302 18:59:43.716431 22683591385216 run.py:483] Algo bellman_ford step 4067 current loss 0.078231, current_train_items 130176.
I0302 18:59:43.744790 22683591385216 run.py:483] Algo bellman_ford step 4068 current loss 0.036686, current_train_items 130208.
I0302 18:59:43.777245 22683591385216 run.py:483] Algo bellman_ford step 4069 current loss 0.068509, current_train_items 130240.
I0302 18:59:43.795814 22683591385216 run.py:483] Algo bellman_ford step 4070 current loss 0.003097, current_train_items 130272.
I0302 18:59:43.811572 22683591385216 run.py:483] Algo bellman_ford step 4071 current loss 0.092354, current_train_items 130304.
I0302 18:59:43.833330 22683591385216 run.py:483] Algo bellman_ford step 4072 current loss 0.093713, current_train_items 130336.
I0302 18:59:43.860748 22683591385216 run.py:483] Algo bellman_ford step 4073 current loss 0.075785, current_train_items 130368.
I0302 18:59:43.891085 22683591385216 run.py:483] Algo bellman_ford step 4074 current loss 0.084596, current_train_items 130400.
I0302 18:59:43.909675 22683591385216 run.py:483] Algo bellman_ford step 4075 current loss 0.007044, current_train_items 130432.
I0302 18:59:43.925765 22683591385216 run.py:483] Algo bellman_ford step 4076 current loss 0.025283, current_train_items 130464.
I0302 18:59:43.948727 22683591385216 run.py:483] Algo bellman_ford step 4077 current loss 0.057864, current_train_items 130496.
I0302 18:59:43.976881 22683591385216 run.py:483] Algo bellman_ford step 4078 current loss 0.055732, current_train_items 130528.
I0302 18:59:44.007068 22683591385216 run.py:483] Algo bellman_ford step 4079 current loss 0.100746, current_train_items 130560.
I0302 18:59:44.024835 22683591385216 run.py:483] Algo bellman_ford step 4080 current loss 0.001793, current_train_items 130592.
I0302 18:59:44.040413 22683591385216 run.py:483] Algo bellman_ford step 4081 current loss 0.013595, current_train_items 130624.
I0302 18:59:44.063350 22683591385216 run.py:483] Algo bellman_ford step 4082 current loss 0.051443, current_train_items 130656.
I0302 18:59:44.093703 22683591385216 run.py:483] Algo bellman_ford step 4083 current loss 0.083126, current_train_items 130688.
I0302 18:59:44.122339 22683591385216 run.py:483] Algo bellman_ford step 4084 current loss 0.047867, current_train_items 130720.
I0302 18:59:44.140975 22683591385216 run.py:483] Algo bellman_ford step 4085 current loss 0.016045, current_train_items 130752.
I0302 18:59:44.156594 22683591385216 run.py:483] Algo bellman_ford step 4086 current loss 0.012707, current_train_items 130784.
I0302 18:59:44.178008 22683591385216 run.py:483] Algo bellman_ford step 4087 current loss 0.081072, current_train_items 130816.
I0302 18:59:44.206350 22683591385216 run.py:483] Algo bellman_ford step 4088 current loss 0.042984, current_train_items 130848.
I0302 18:59:44.236117 22683591385216 run.py:483] Algo bellman_ford step 4089 current loss 0.040793, current_train_items 130880.
I0302 18:59:44.254434 22683591385216 run.py:483] Algo bellman_ford step 4090 current loss 0.002002, current_train_items 130912.
I0302 18:59:44.269871 22683591385216 run.py:483] Algo bellman_ford step 4091 current loss 0.013153, current_train_items 130944.
I0302 18:59:44.292490 22683591385216 run.py:483] Algo bellman_ford step 4092 current loss 0.039953, current_train_items 130976.
I0302 18:59:44.321806 22683591385216 run.py:483] Algo bellman_ford step 4093 current loss 0.070162, current_train_items 131008.
I0302 18:59:44.353311 22683591385216 run.py:483] Algo bellman_ford step 4094 current loss 0.063621, current_train_items 131040.
I0302 18:59:44.371283 22683591385216 run.py:483] Algo bellman_ford step 4095 current loss 0.002660, current_train_items 131072.
I0302 18:59:44.386824 22683591385216 run.py:483] Algo bellman_ford step 4096 current loss 0.019930, current_train_items 131104.
I0302 18:59:44.409401 22683591385216 run.py:483] Algo bellman_ford step 4097 current loss 0.069814, current_train_items 131136.
I0302 18:59:44.438358 22683591385216 run.py:483] Algo bellman_ford step 4098 current loss 0.057173, current_train_items 131168.
I0302 18:59:44.470154 22683591385216 run.py:483] Algo bellman_ford step 4099 current loss 0.121451, current_train_items 131200.
I0302 18:59:44.488600 22683591385216 run.py:483] Algo bellman_ford step 4100 current loss 0.001113, current_train_items 131232.
I0302 18:59:44.496337 22683591385216 run.py:503] (val) algo bellman_ford step 4100: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 131232, 'step': 4100, 'algorithm': 'bellman_ford'}
I0302 18:59:44.496443 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 18:59:44.511774 22683591385216 run.py:483] Algo bellman_ford step 4101 current loss 0.005152, current_train_items 131264.
I0302 18:59:44.532476 22683591385216 run.py:483] Algo bellman_ford step 4102 current loss 0.038288, current_train_items 131296.
I0302 18:59:44.562564 22683591385216 run.py:483] Algo bellman_ford step 4103 current loss 0.084349, current_train_items 131328.
I0302 18:59:44.596001 22683591385216 run.py:483] Algo bellman_ford step 4104 current loss 0.105950, current_train_items 131360.
I0302 18:59:44.614334 22683591385216 run.py:483] Algo bellman_ford step 4105 current loss 0.001936, current_train_items 131392.
I0302 18:59:44.629806 22683591385216 run.py:483] Algo bellman_ford step 4106 current loss 0.031801, current_train_items 131424.
I0302 18:59:44.653017 22683591385216 run.py:483] Algo bellman_ford step 4107 current loss 0.044585, current_train_items 131456.
I0302 18:59:44.682055 22683591385216 run.py:483] Algo bellman_ford step 4108 current loss 0.077966, current_train_items 131488.
I0302 18:59:44.715732 22683591385216 run.py:483] Algo bellman_ford step 4109 current loss 0.094079, current_train_items 131520.
I0302 18:59:44.733788 22683591385216 run.py:483] Algo bellman_ford step 4110 current loss 0.007374, current_train_items 131552.
I0302 18:59:44.749572 22683591385216 run.py:483] Algo bellman_ford step 4111 current loss 0.026544, current_train_items 131584.
I0302 18:59:44.771124 22683591385216 run.py:483] Algo bellman_ford step 4112 current loss 0.066508, current_train_items 131616.
I0302 18:59:44.801194 22683591385216 run.py:483] Algo bellman_ford step 4113 current loss 0.089862, current_train_items 131648.
I0302 18:59:44.833524 22683591385216 run.py:483] Algo bellman_ford step 4114 current loss 0.083185, current_train_items 131680.
I0302 18:59:44.851770 22683591385216 run.py:483] Algo bellman_ford step 4115 current loss 0.001948, current_train_items 131712.
I0302 18:59:44.867332 22683591385216 run.py:483] Algo bellman_ford step 4116 current loss 0.028800, current_train_items 131744.
I0302 18:59:44.890991 22683591385216 run.py:483] Algo bellman_ford step 4117 current loss 0.050910, current_train_items 131776.
I0302 18:59:44.919484 22683591385216 run.py:483] Algo bellman_ford step 4118 current loss 0.040859, current_train_items 131808.
I0302 18:59:44.951452 22683591385216 run.py:483] Algo bellman_ford step 4119 current loss 0.067937, current_train_items 131840.
I0302 18:59:44.969791 22683591385216 run.py:483] Algo bellman_ford step 4120 current loss 0.017058, current_train_items 131872.
I0302 18:59:44.985193 22683591385216 run.py:483] Algo bellman_ford step 4121 current loss 0.010573, current_train_items 131904.
I0302 18:59:45.008303 22683591385216 run.py:483] Algo bellman_ford step 4122 current loss 0.054104, current_train_items 131936.
I0302 18:59:45.038581 22683591385216 run.py:483] Algo bellman_ford step 4123 current loss 0.082420, current_train_items 131968.
I0302 18:59:45.069912 22683591385216 run.py:483] Algo bellman_ford step 4124 current loss 0.103056, current_train_items 132000.
I0302 18:59:45.088108 22683591385216 run.py:483] Algo bellman_ford step 4125 current loss 0.002745, current_train_items 132032.
I0302 18:59:45.103463 22683591385216 run.py:483] Algo bellman_ford step 4126 current loss 0.067956, current_train_items 132064.
I0302 18:59:45.126702 22683591385216 run.py:483] Algo bellman_ford step 4127 current loss 0.079852, current_train_items 132096.
I0302 18:59:45.155488 22683591385216 run.py:483] Algo bellman_ford step 4128 current loss 0.065399, current_train_items 132128.
I0302 18:59:45.185600 22683591385216 run.py:483] Algo bellman_ford step 4129 current loss 0.054213, current_train_items 132160.
I0302 18:59:45.203907 22683591385216 run.py:483] Algo bellman_ford step 4130 current loss 0.003526, current_train_items 132192.
I0302 18:59:45.219434 22683591385216 run.py:483] Algo bellman_ford step 4131 current loss 0.018264, current_train_items 132224.
I0302 18:59:45.241880 22683591385216 run.py:483] Algo bellman_ford step 4132 current loss 0.079871, current_train_items 132256.
I0302 18:59:45.271982 22683591385216 run.py:483] Algo bellman_ford step 4133 current loss 0.100893, current_train_items 132288.
I0302 18:59:45.304607 22683591385216 run.py:483] Algo bellman_ford step 4134 current loss 0.081854, current_train_items 132320.
I0302 18:59:45.323031 22683591385216 run.py:483] Algo bellman_ford step 4135 current loss 0.005652, current_train_items 132352.
I0302 18:59:45.338226 22683591385216 run.py:483] Algo bellman_ford step 4136 current loss 0.012578, current_train_items 132384.
I0302 18:59:45.360348 22683591385216 run.py:483] Algo bellman_ford step 4137 current loss 0.023977, current_train_items 132416.
I0302 18:59:45.389182 22683591385216 run.py:483] Algo bellman_ford step 4138 current loss 0.083102, current_train_items 132448.
I0302 18:59:45.421406 22683591385216 run.py:483] Algo bellman_ford step 4139 current loss 0.167967, current_train_items 132480.
I0302 18:59:45.439941 22683591385216 run.py:483] Algo bellman_ford step 4140 current loss 0.002475, current_train_items 132512.
I0302 18:59:45.454864 22683591385216 run.py:483] Algo bellman_ford step 4141 current loss 0.008648, current_train_items 132544.
I0302 18:59:45.478549 22683591385216 run.py:483] Algo bellman_ford step 4142 current loss 0.034872, current_train_items 132576.
I0302 18:59:45.507067 22683591385216 run.py:483] Algo bellman_ford step 4143 current loss 0.048036, current_train_items 132608.
I0302 18:59:45.538067 22683591385216 run.py:483] Algo bellman_ford step 4144 current loss 0.077648, current_train_items 132640.
I0302 18:59:45.556277 22683591385216 run.py:483] Algo bellman_ford step 4145 current loss 0.002298, current_train_items 132672.
I0302 18:59:45.571244 22683591385216 run.py:483] Algo bellman_ford step 4146 current loss 0.008935, current_train_items 132704.
I0302 18:59:45.593338 22683591385216 run.py:483] Algo bellman_ford step 4147 current loss 0.033146, current_train_items 132736.
I0302 18:59:45.621221 22683591385216 run.py:483] Algo bellman_ford step 4148 current loss 0.026692, current_train_items 132768.
I0302 18:59:45.651884 22683591385216 run.py:483] Algo bellman_ford step 4149 current loss 0.052127, current_train_items 132800.
I0302 18:59:45.670371 22683591385216 run.py:483] Algo bellman_ford step 4150 current loss 0.002618, current_train_items 132832.
I0302 18:59:45.678262 22683591385216 run.py:503] (val) algo bellman_ford step 4150: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 132832, 'step': 4150, 'algorithm': 'bellman_ford'}
I0302 18:59:45.678367 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 18:59:45.694327 22683591385216 run.py:483] Algo bellman_ford step 4151 current loss 0.037634, current_train_items 132864.
I0302 18:59:45.717633 22683591385216 run.py:483] Algo bellman_ford step 4152 current loss 0.055501, current_train_items 132896.
I0302 18:59:45.746923 22683591385216 run.py:483] Algo bellman_ford step 4153 current loss 0.056109, current_train_items 132928.
I0302 18:59:45.780095 22683591385216 run.py:483] Algo bellman_ford step 4154 current loss 0.097890, current_train_items 132960.
I0302 18:59:45.798759 22683591385216 run.py:483] Algo bellman_ford step 4155 current loss 0.005698, current_train_items 132992.
I0302 18:59:45.814414 22683591385216 run.py:483] Algo bellman_ford step 4156 current loss 0.029834, current_train_items 133024.
I0302 18:59:45.836430 22683591385216 run.py:483] Algo bellman_ford step 4157 current loss 0.036382, current_train_items 133056.
I0302 18:59:45.864779 22683591385216 run.py:483] Algo bellman_ford step 4158 current loss 0.062281, current_train_items 133088.
I0302 18:59:45.896299 22683591385216 run.py:483] Algo bellman_ford step 4159 current loss 0.070969, current_train_items 133120.
I0302 18:59:45.915220 22683591385216 run.py:483] Algo bellman_ford step 4160 current loss 0.004575, current_train_items 133152.
I0302 18:59:45.930948 22683591385216 run.py:483] Algo bellman_ford step 4161 current loss 0.030777, current_train_items 133184.
I0302 18:59:45.951963 22683591385216 run.py:483] Algo bellman_ford step 4162 current loss 0.069176, current_train_items 133216.
I0302 18:59:45.981691 22683591385216 run.py:483] Algo bellman_ford step 4163 current loss 0.057756, current_train_items 133248.
I0302 18:59:46.013747 22683591385216 run.py:483] Algo bellman_ford step 4164 current loss 0.030588, current_train_items 133280.
I0302 18:59:46.032278 22683591385216 run.py:483] Algo bellman_ford step 4165 current loss 0.002545, current_train_items 133312.
I0302 18:59:46.047499 22683591385216 run.py:483] Algo bellman_ford step 4166 current loss 0.016587, current_train_items 133344.
I0302 18:59:46.070956 22683591385216 run.py:483] Algo bellman_ford step 4167 current loss 0.084540, current_train_items 133376.
I0302 18:59:46.099718 22683591385216 run.py:483] Algo bellman_ford step 4168 current loss 0.041036, current_train_items 133408.
I0302 18:59:46.130369 22683591385216 run.py:483] Algo bellman_ford step 4169 current loss 0.044778, current_train_items 133440.
I0302 18:59:46.148988 22683591385216 run.py:483] Algo bellman_ford step 4170 current loss 0.001644, current_train_items 133472.
I0302 18:59:46.164088 22683591385216 run.py:483] Algo bellman_ford step 4171 current loss 0.005851, current_train_items 133504.
I0302 18:59:46.185905 22683591385216 run.py:483] Algo bellman_ford step 4172 current loss 0.053624, current_train_items 133536.
I0302 18:59:46.214442 22683591385216 run.py:483] Algo bellman_ford step 4173 current loss 0.066627, current_train_items 133568.
I0302 18:59:46.247723 22683591385216 run.py:483] Algo bellman_ford step 4174 current loss 0.105118, current_train_items 133600.
I0302 18:59:46.266102 22683591385216 run.py:483] Algo bellman_ford step 4175 current loss 0.003769, current_train_items 133632.
I0302 18:59:46.282311 22683591385216 run.py:483] Algo bellman_ford step 4176 current loss 0.011560, current_train_items 133664.
I0302 18:59:46.304787 22683591385216 run.py:483] Algo bellman_ford step 4177 current loss 0.101470, current_train_items 133696.
I0302 18:59:46.334168 22683591385216 run.py:483] Algo bellman_ford step 4178 current loss 0.090299, current_train_items 133728.
I0302 18:59:46.366458 22683591385216 run.py:483] Algo bellman_ford step 4179 current loss 0.195514, current_train_items 133760.
I0302 18:59:46.384802 22683591385216 run.py:483] Algo bellman_ford step 4180 current loss 0.002206, current_train_items 133792.
I0302 18:59:46.400141 22683591385216 run.py:483] Algo bellman_ford step 4181 current loss 0.012919, current_train_items 133824.
I0302 18:59:46.423593 22683591385216 run.py:483] Algo bellman_ford step 4182 current loss 0.047845, current_train_items 133856.
I0302 18:59:46.450999 22683591385216 run.py:483] Algo bellman_ford step 4183 current loss 0.029671, current_train_items 133888.
I0302 18:59:46.483140 22683591385216 run.py:483] Algo bellman_ford step 4184 current loss 0.098541, current_train_items 133920.
I0302 18:59:46.501635 22683591385216 run.py:483] Algo bellman_ford step 4185 current loss 0.002778, current_train_items 133952.
I0302 18:59:46.517094 22683591385216 run.py:483] Algo bellman_ford step 4186 current loss 0.054892, current_train_items 133984.
I0302 18:59:46.539190 22683591385216 run.py:483] Algo bellman_ford step 4187 current loss 0.032452, current_train_items 134016.
I0302 18:59:46.568059 22683591385216 run.py:483] Algo bellman_ford step 4188 current loss 0.066629, current_train_items 134048.
I0302 18:59:46.598501 22683591385216 run.py:483] Algo bellman_ford step 4189 current loss 0.054017, current_train_items 134080.
I0302 18:59:46.617086 22683591385216 run.py:483] Algo bellman_ford step 4190 current loss 0.007874, current_train_items 134112.
I0302 18:59:46.633029 22683591385216 run.py:483] Algo bellman_ford step 4191 current loss 0.009464, current_train_items 134144.
I0302 18:59:46.653056 22683591385216 run.py:483] Algo bellman_ford step 4192 current loss 0.008685, current_train_items 134176.
I0302 18:59:46.682556 22683591385216 run.py:483] Algo bellman_ford step 4193 current loss 0.028532, current_train_items 134208.
I0302 18:59:46.715965 22683591385216 run.py:483] Algo bellman_ford step 4194 current loss 0.107648, current_train_items 134240.
I0302 18:59:46.734182 22683591385216 run.py:483] Algo bellman_ford step 4195 current loss 0.003086, current_train_items 134272.
I0302 18:59:46.749382 22683591385216 run.py:483] Algo bellman_ford step 4196 current loss 0.025470, current_train_items 134304.
I0302 18:59:46.772030 22683591385216 run.py:483] Algo bellman_ford step 4197 current loss 0.036255, current_train_items 134336.
I0302 18:59:46.800649 22683591385216 run.py:483] Algo bellman_ford step 4198 current loss 0.051888, current_train_items 134368.
I0302 18:59:46.831675 22683591385216 run.py:483] Algo bellman_ford step 4199 current loss 0.066445, current_train_items 134400.
I0302 18:59:46.849908 22683591385216 run.py:483] Algo bellman_ford step 4200 current loss 0.002803, current_train_items 134432.
I0302 18:59:46.857765 22683591385216 run.py:503] (val) algo bellman_ford step 4200: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 134432, 'step': 4200, 'algorithm': 'bellman_ford'}
I0302 18:59:46.857871 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 18:59:46.874038 22683591385216 run.py:483] Algo bellman_ford step 4201 current loss 0.025779, current_train_items 134464.
I0302 18:59:46.896053 22683591385216 run.py:483] Algo bellman_ford step 4202 current loss 0.046806, current_train_items 134496.
I0302 18:59:46.925440 22683591385216 run.py:483] Algo bellman_ford step 4203 current loss 0.030678, current_train_items 134528.
I0302 18:59:46.956740 22683591385216 run.py:483] Algo bellman_ford step 4204 current loss 0.064854, current_train_items 134560.
I0302 18:59:46.975438 22683591385216 run.py:483] Algo bellman_ford step 4205 current loss 0.003161, current_train_items 134592.
I0302 18:59:46.990946 22683591385216 run.py:483] Algo bellman_ford step 4206 current loss 0.031785, current_train_items 134624.
I0302 18:59:47.013521 22683591385216 run.py:483] Algo bellman_ford step 4207 current loss 0.026974, current_train_items 134656.
I0302 18:59:47.043617 22683591385216 run.py:483] Algo bellman_ford step 4208 current loss 0.039055, current_train_items 134688.
I0302 18:59:47.076200 22683591385216 run.py:483] Algo bellman_ford step 4209 current loss 0.091597, current_train_items 134720.
I0302 18:59:47.094404 22683591385216 run.py:483] Algo bellman_ford step 4210 current loss 0.004310, current_train_items 134752.
I0302 18:59:47.109710 22683591385216 run.py:483] Algo bellman_ford step 4211 current loss 0.018660, current_train_items 134784.
I0302 18:59:47.132185 22683591385216 run.py:483] Algo bellman_ford step 4212 current loss 0.047000, current_train_items 134816.
I0302 18:59:47.161739 22683591385216 run.py:483] Algo bellman_ford step 4213 current loss 0.037765, current_train_items 134848.
I0302 18:59:47.193085 22683591385216 run.py:483] Algo bellman_ford step 4214 current loss 0.085353, current_train_items 134880.
I0302 18:59:47.211259 22683591385216 run.py:483] Algo bellman_ford step 4215 current loss 0.012602, current_train_items 134912.
I0302 18:59:47.226571 22683591385216 run.py:483] Algo bellman_ford step 4216 current loss 0.025768, current_train_items 134944.
I0302 18:59:47.248246 22683591385216 run.py:483] Algo bellman_ford step 4217 current loss 0.029152, current_train_items 134976.
I0302 18:59:47.277014 22683591385216 run.py:483] Algo bellman_ford step 4218 current loss 0.088237, current_train_items 135008.
I0302 18:59:47.308373 22683591385216 run.py:483] Algo bellman_ford step 4219 current loss 0.045450, current_train_items 135040.
I0302 18:59:47.326601 22683591385216 run.py:483] Algo bellman_ford step 4220 current loss 0.001186, current_train_items 135072.
I0302 18:59:47.342092 22683591385216 run.py:483] Algo bellman_ford step 4221 current loss 0.024528, current_train_items 135104.
I0302 18:59:47.364963 22683591385216 run.py:483] Algo bellman_ford step 4222 current loss 0.046198, current_train_items 135136.
I0302 18:59:47.395031 22683591385216 run.py:483] Algo bellman_ford step 4223 current loss 0.044343, current_train_items 135168.
I0302 18:59:47.428216 22683591385216 run.py:483] Algo bellman_ford step 4224 current loss 0.073256, current_train_items 135200.
I0302 18:59:47.446562 22683591385216 run.py:483] Algo bellman_ford step 4225 current loss 0.002140, current_train_items 135232.
I0302 18:59:47.461659 22683591385216 run.py:483] Algo bellman_ford step 4226 current loss 0.026462, current_train_items 135264.
I0302 18:59:47.483698 22683591385216 run.py:483] Algo bellman_ford step 4227 current loss 0.069573, current_train_items 135296.
I0302 18:59:47.511607 22683591385216 run.py:483] Algo bellman_ford step 4228 current loss 0.070120, current_train_items 135328.
I0302 18:59:47.541520 22683591385216 run.py:483] Algo bellman_ford step 4229 current loss 0.041667, current_train_items 135360.
I0302 18:59:47.559773 22683591385216 run.py:483] Algo bellman_ford step 4230 current loss 0.002773, current_train_items 135392.
I0302 18:59:47.574879 22683591385216 run.py:483] Algo bellman_ford step 4231 current loss 0.040977, current_train_items 135424.
I0302 18:59:47.597061 22683591385216 run.py:483] Algo bellman_ford step 4232 current loss 0.056177, current_train_items 135456.
I0302 18:59:47.626371 22683591385216 run.py:483] Algo bellman_ford step 4233 current loss 0.044082, current_train_items 135488.
I0302 18:59:47.657066 22683591385216 run.py:483] Algo bellman_ford step 4234 current loss 0.078374, current_train_items 135520.
I0302 18:59:47.675354 22683591385216 run.py:483] Algo bellman_ford step 4235 current loss 0.005149, current_train_items 135552.
I0302 18:59:47.691236 22683591385216 run.py:483] Algo bellman_ford step 4236 current loss 0.022842, current_train_items 135584.
I0302 18:59:47.713681 22683591385216 run.py:483] Algo bellman_ford step 4237 current loss 0.037138, current_train_items 135616.
I0302 18:59:47.743192 22683591385216 run.py:483] Algo bellman_ford step 4238 current loss 0.038823, current_train_items 135648.
I0302 18:59:47.776344 22683591385216 run.py:483] Algo bellman_ford step 4239 current loss 0.110779, current_train_items 135680.
I0302 18:59:47.794565 22683591385216 run.py:483] Algo bellman_ford step 4240 current loss 0.005308, current_train_items 135712.
I0302 18:59:47.809758 22683591385216 run.py:483] Algo bellman_ford step 4241 current loss 0.011703, current_train_items 135744.
I0302 18:59:47.831905 22683591385216 run.py:483] Algo bellman_ford step 4242 current loss 0.043930, current_train_items 135776.
I0302 18:59:47.861917 22683591385216 run.py:483] Algo bellman_ford step 4243 current loss 0.098374, current_train_items 135808.
I0302 18:59:47.892449 22683591385216 run.py:483] Algo bellman_ford step 4244 current loss 0.063701, current_train_items 135840.
I0302 18:59:47.910656 22683591385216 run.py:483] Algo bellman_ford step 4245 current loss 0.003638, current_train_items 135872.
I0302 18:59:47.925582 22683591385216 run.py:483] Algo bellman_ford step 4246 current loss 0.023495, current_train_items 135904.
I0302 18:59:47.949259 22683591385216 run.py:483] Algo bellman_ford step 4247 current loss 0.036646, current_train_items 135936.
I0302 18:59:47.978872 22683591385216 run.py:483] Algo bellman_ford step 4248 current loss 0.074667, current_train_items 135968.
I0302 18:59:48.012022 22683591385216 run.py:483] Algo bellman_ford step 4249 current loss 0.098924, current_train_items 136000.
I0302 18:59:48.030027 22683591385216 run.py:483] Algo bellman_ford step 4250 current loss 0.005164, current_train_items 136032.
I0302 18:59:48.037589 22683591385216 run.py:503] (val) algo bellman_ford step 4250: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 136032, 'step': 4250, 'algorithm': 'bellman_ford'}
I0302 18:59:48.037694 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 18:59:48.053338 22683591385216 run.py:483] Algo bellman_ford step 4251 current loss 0.020114, current_train_items 136064.
I0302 18:59:48.075630 22683591385216 run.py:483] Algo bellman_ford step 4252 current loss 0.038155, current_train_items 136096.
I0302 18:59:48.104961 22683591385216 run.py:483] Algo bellman_ford step 4253 current loss 0.082669, current_train_items 136128.
I0302 18:59:48.136579 22683591385216 run.py:483] Algo bellman_ford step 4254 current loss 0.068862, current_train_items 136160.
I0302 18:59:48.154572 22683591385216 run.py:483] Algo bellman_ford step 4255 current loss 0.004751, current_train_items 136192.
I0302 18:59:48.169702 22683591385216 run.py:483] Algo bellman_ford step 4256 current loss 0.009831, current_train_items 136224.
I0302 18:59:48.191261 22683591385216 run.py:483] Algo bellman_ford step 4257 current loss 0.038583, current_train_items 136256.
I0302 18:59:48.220195 22683591385216 run.py:483] Algo bellman_ford step 4258 current loss 0.044106, current_train_items 136288.
I0302 18:59:48.251860 22683591385216 run.py:483] Algo bellman_ford step 4259 current loss 0.062915, current_train_items 136320.
I0302 18:59:48.270337 22683591385216 run.py:483] Algo bellman_ford step 4260 current loss 0.002093, current_train_items 136352.
I0302 18:59:48.286310 22683591385216 run.py:483] Algo bellman_ford step 4261 current loss 0.052550, current_train_items 136384.
I0302 18:59:48.308034 22683591385216 run.py:483] Algo bellman_ford step 4262 current loss 0.046671, current_train_items 136416.
I0302 18:59:48.337027 22683591385216 run.py:483] Algo bellman_ford step 4263 current loss 0.038291, current_train_items 136448.
I0302 18:59:48.368332 22683591385216 run.py:483] Algo bellman_ford step 4264 current loss 0.047183, current_train_items 136480.
I0302 18:59:48.386329 22683591385216 run.py:483] Algo bellman_ford step 4265 current loss 0.003761, current_train_items 136512.
I0302 18:59:48.401662 22683591385216 run.py:483] Algo bellman_ford step 4266 current loss 0.044910, current_train_items 136544.
I0302 18:59:48.423567 22683591385216 run.py:483] Algo bellman_ford step 4267 current loss 0.024498, current_train_items 136576.
I0302 18:59:48.452013 22683591385216 run.py:483] Algo bellman_ford step 4268 current loss 0.037626, current_train_items 136608.
I0302 18:59:48.482330 22683591385216 run.py:483] Algo bellman_ford step 4269 current loss 0.071584, current_train_items 136640.
I0302 18:59:48.500847 22683591385216 run.py:483] Algo bellman_ford step 4270 current loss 0.002414, current_train_items 136672.
I0302 18:59:48.516192 22683591385216 run.py:483] Algo bellman_ford step 4271 current loss 0.019435, current_train_items 136704.
I0302 18:59:48.536892 22683591385216 run.py:483] Algo bellman_ford step 4272 current loss 0.015360, current_train_items 136736.
I0302 18:59:48.565463 22683591385216 run.py:483] Algo bellman_ford step 4273 current loss 0.054550, current_train_items 136768.
I0302 18:59:48.595762 22683591385216 run.py:483] Algo bellman_ford step 4274 current loss 0.052727, current_train_items 136800.
I0302 18:59:48.614394 22683591385216 run.py:483] Algo bellman_ford step 4275 current loss 0.001122, current_train_items 136832.
I0302 18:59:48.629825 22683591385216 run.py:483] Algo bellman_ford step 4276 current loss 0.016166, current_train_items 136864.
I0302 18:59:48.651651 22683591385216 run.py:483] Algo bellman_ford step 4277 current loss 0.046594, current_train_items 136896.
I0302 18:59:48.682276 22683591385216 run.py:483] Algo bellman_ford step 4278 current loss 0.100473, current_train_items 136928.
I0302 18:59:48.713930 22683591385216 run.py:483] Algo bellman_ford step 4279 current loss 0.058860, current_train_items 136960.
I0302 18:59:48.732253 22683591385216 run.py:483] Algo bellman_ford step 4280 current loss 0.017434, current_train_items 136992.
I0302 18:59:48.747439 22683591385216 run.py:483] Algo bellman_ford step 4281 current loss 0.009792, current_train_items 137024.
I0302 18:59:48.769387 22683591385216 run.py:483] Algo bellman_ford step 4282 current loss 0.016913, current_train_items 137056.
I0302 18:59:48.797356 22683591385216 run.py:483] Algo bellman_ford step 4283 current loss 0.028381, current_train_items 137088.
I0302 18:59:48.830257 22683591385216 run.py:483] Algo bellman_ford step 4284 current loss 0.075806, current_train_items 137120.
I0302 18:59:48.848670 22683591385216 run.py:483] Algo bellman_ford step 4285 current loss 0.000846, current_train_items 137152.
I0302 18:59:48.864082 22683591385216 run.py:483] Algo bellman_ford step 4286 current loss 0.001734, current_train_items 137184.
I0302 18:59:48.885908 22683591385216 run.py:483] Algo bellman_ford step 4287 current loss 0.043565, current_train_items 137216.
I0302 18:59:48.916182 22683591385216 run.py:483] Algo bellman_ford step 4288 current loss 0.053388, current_train_items 137248.
I0302 18:59:48.947951 22683591385216 run.py:483] Algo bellman_ford step 4289 current loss 0.062734, current_train_items 137280.
I0302 18:59:48.966332 22683591385216 run.py:483] Algo bellman_ford step 4290 current loss 0.015747, current_train_items 137312.
I0302 18:59:48.982314 22683591385216 run.py:483] Algo bellman_ford step 4291 current loss 0.030389, current_train_items 137344.
I0302 18:59:49.005401 22683591385216 run.py:483] Algo bellman_ford step 4292 current loss 0.068039, current_train_items 137376.
I0302 18:59:49.035858 22683591385216 run.py:483] Algo bellman_ford step 4293 current loss 0.083415, current_train_items 137408.
I0302 18:59:49.068734 22683591385216 run.py:483] Algo bellman_ford step 4294 current loss 0.058562, current_train_items 137440.
I0302 18:59:49.086776 22683591385216 run.py:483] Algo bellman_ford step 4295 current loss 0.001292, current_train_items 137472.
I0302 18:59:49.101874 22683591385216 run.py:483] Algo bellman_ford step 4296 current loss 0.016270, current_train_items 137504.
I0302 18:59:49.123318 22683591385216 run.py:483] Algo bellman_ford step 4297 current loss 0.100819, current_train_items 137536.
I0302 18:59:49.152589 22683591385216 run.py:483] Algo bellman_ford step 4298 current loss 0.060707, current_train_items 137568.
I0302 18:59:49.180308 22683591385216 run.py:483] Algo bellman_ford step 4299 current loss 0.057526, current_train_items 137600.
I0302 18:59:49.198628 22683591385216 run.py:483] Algo bellman_ford step 4300 current loss 0.001210, current_train_items 137632.
I0302 18:59:49.206018 22683591385216 run.py:503] (val) algo bellman_ford step 4300: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 137632, 'step': 4300, 'algorithm': 'bellman_ford'}
I0302 18:59:49.206125 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:49.221727 22683591385216 run.py:483] Algo bellman_ford step 4301 current loss 0.028456, current_train_items 137664.
I0302 18:59:49.244323 22683591385216 run.py:483] Algo bellman_ford step 4302 current loss 0.056151, current_train_items 137696.
I0302 18:59:49.275173 22683591385216 run.py:483] Algo bellman_ford step 4303 current loss 0.186548, current_train_items 137728.
I0302 18:59:49.309003 22683591385216 run.py:483] Algo bellman_ford step 4304 current loss 0.055548, current_train_items 137760.
I0302 18:59:49.327577 22683591385216 run.py:483] Algo bellman_ford step 4305 current loss 0.001836, current_train_items 137792.
I0302 18:59:49.342859 22683591385216 run.py:483] Algo bellman_ford step 4306 current loss 0.013371, current_train_items 137824.
I0302 18:59:49.367041 22683591385216 run.py:483] Algo bellman_ford step 4307 current loss 0.078368, current_train_items 137856.
I0302 18:59:49.397131 22683591385216 run.py:483] Algo bellman_ford step 4308 current loss 0.227691, current_train_items 137888.
I0302 18:59:49.430996 22683591385216 run.py:483] Algo bellman_ford step 4309 current loss 0.109927, current_train_items 137920.
I0302 18:59:49.449527 22683591385216 run.py:483] Algo bellman_ford step 4310 current loss 0.009070, current_train_items 137952.
I0302 18:59:49.464819 22683591385216 run.py:483] Algo bellman_ford step 4311 current loss 0.036007, current_train_items 137984.
I0302 18:59:49.485929 22683591385216 run.py:483] Algo bellman_ford step 4312 current loss 0.053010, current_train_items 138016.
I0302 18:59:49.515680 22683591385216 run.py:483] Algo bellman_ford step 4313 current loss 0.036396, current_train_items 138048.
I0302 18:59:49.547790 22683591385216 run.py:483] Algo bellman_ford step 4314 current loss 0.083546, current_train_items 138080.
I0302 18:59:49.565930 22683591385216 run.py:483] Algo bellman_ford step 4315 current loss 0.002130, current_train_items 138112.
I0302 18:59:49.581013 22683591385216 run.py:483] Algo bellman_ford step 4316 current loss 0.024310, current_train_items 138144.
I0302 18:59:49.603613 22683591385216 run.py:483] Algo bellman_ford step 4317 current loss 0.039516, current_train_items 138176.
I0302 18:59:49.631860 22683591385216 run.py:483] Algo bellman_ford step 4318 current loss 0.049443, current_train_items 138208.
I0302 18:59:49.663327 22683591385216 run.py:483] Algo bellman_ford step 4319 current loss 0.048134, current_train_items 138240.
I0302 18:59:49.681684 22683591385216 run.py:483] Algo bellman_ford step 4320 current loss 0.001529, current_train_items 138272.
I0302 18:59:49.696942 22683591385216 run.py:483] Algo bellman_ford step 4321 current loss 0.023774, current_train_items 138304.
I0302 18:59:49.719304 22683591385216 run.py:483] Algo bellman_ford step 4322 current loss 0.031969, current_train_items 138336.
I0302 18:59:49.747551 22683591385216 run.py:483] Algo bellman_ford step 4323 current loss 0.042280, current_train_items 138368.
I0302 18:59:49.779858 22683591385216 run.py:483] Algo bellman_ford step 4324 current loss 0.059183, current_train_items 138400.
I0302 18:59:49.798176 22683591385216 run.py:483] Algo bellman_ford step 4325 current loss 0.001077, current_train_items 138432.
I0302 18:59:49.813601 22683591385216 run.py:483] Algo bellman_ford step 4326 current loss 0.020085, current_train_items 138464.
I0302 18:59:49.836298 22683591385216 run.py:483] Algo bellman_ford step 4327 current loss 0.047329, current_train_items 138496.
I0302 18:59:49.867178 22683591385216 run.py:483] Algo bellman_ford step 4328 current loss 0.050671, current_train_items 138528.
I0302 18:59:49.897762 22683591385216 run.py:483] Algo bellman_ford step 4329 current loss 0.059620, current_train_items 138560.
I0302 18:59:49.916390 22683591385216 run.py:483] Algo bellman_ford step 4330 current loss 0.002797, current_train_items 138592.
I0302 18:59:49.932101 22683591385216 run.py:483] Algo bellman_ford step 4331 current loss 0.012195, current_train_items 138624.
I0302 18:59:49.955149 22683591385216 run.py:483] Algo bellman_ford step 4332 current loss 0.070288, current_train_items 138656.
I0302 18:59:49.983838 22683591385216 run.py:483] Algo bellman_ford step 4333 current loss 0.058473, current_train_items 138688.
I0302 18:59:50.013184 22683591385216 run.py:483] Algo bellman_ford step 4334 current loss 0.061511, current_train_items 138720.
I0302 18:59:50.031505 22683591385216 run.py:483] Algo bellman_ford step 4335 current loss 0.004523, current_train_items 138752.
I0302 18:59:50.046784 22683591385216 run.py:483] Algo bellman_ford step 4336 current loss 0.026167, current_train_items 138784.
I0302 18:59:50.069311 22683591385216 run.py:483] Algo bellman_ford step 4337 current loss 0.122359, current_train_items 138816.
I0302 18:59:50.099854 22683591385216 run.py:483] Algo bellman_ford step 4338 current loss 0.102717, current_train_items 138848.
I0302 18:59:50.130367 22683591385216 run.py:483] Algo bellman_ford step 4339 current loss 0.118513, current_train_items 138880.
I0302 18:59:50.148725 22683591385216 run.py:483] Algo bellman_ford step 4340 current loss 0.011117, current_train_items 138912.
I0302 18:59:50.163840 22683591385216 run.py:483] Algo bellman_ford step 4341 current loss 0.005668, current_train_items 138944.
I0302 18:59:50.186066 22683591385216 run.py:483] Algo bellman_ford step 4342 current loss 0.026448, current_train_items 138976.
I0302 18:59:50.215212 22683591385216 run.py:483] Algo bellman_ford step 4343 current loss 0.098572, current_train_items 139008.
I0302 18:59:50.247542 22683591385216 run.py:483] Algo bellman_ford step 4344 current loss 0.100604, current_train_items 139040.
I0302 18:59:50.266145 22683591385216 run.py:483] Algo bellman_ford step 4345 current loss 0.013881, current_train_items 139072.
I0302 18:59:50.281826 22683591385216 run.py:483] Algo bellman_ford step 4346 current loss 0.009666, current_train_items 139104.
I0302 18:59:50.304573 22683591385216 run.py:483] Algo bellman_ford step 4347 current loss 0.055998, current_train_items 139136.
I0302 18:59:50.334249 22683591385216 run.py:483] Algo bellman_ford step 4348 current loss 0.035351, current_train_items 139168.
I0302 18:59:50.364765 22683591385216 run.py:483] Algo bellman_ford step 4349 current loss 0.077810, current_train_items 139200.
I0302 18:59:50.382799 22683591385216 run.py:483] Algo bellman_ford step 4350 current loss 0.002850, current_train_items 139232.
I0302 18:59:50.390440 22683591385216 run.py:503] (val) algo bellman_ford step 4350: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 139232, 'step': 4350, 'algorithm': 'bellman_ford'}
I0302 18:59:50.390546 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 18:59:50.406517 22683591385216 run.py:483] Algo bellman_ford step 4351 current loss 0.038881, current_train_items 139264.
I0302 18:59:50.429641 22683591385216 run.py:483] Algo bellman_ford step 4352 current loss 0.019516, current_train_items 139296.
I0302 18:59:50.458890 22683591385216 run.py:483] Algo bellman_ford step 4353 current loss 0.035459, current_train_items 139328.
I0302 18:59:50.491055 22683591385216 run.py:483] Algo bellman_ford step 4354 current loss 0.066736, current_train_items 139360.
I0302 18:59:50.509232 22683591385216 run.py:483] Algo bellman_ford step 4355 current loss 0.001012, current_train_items 139392.
I0302 18:59:50.524544 22683591385216 run.py:483] Algo bellman_ford step 4356 current loss 0.005402, current_train_items 139424.
I0302 18:59:50.547326 22683591385216 run.py:483] Algo bellman_ford step 4357 current loss 0.049600, current_train_items 139456.
I0302 18:59:50.574506 22683591385216 run.py:483] Algo bellman_ford step 4358 current loss 0.044404, current_train_items 139488.
I0302 18:59:50.604819 22683591385216 run.py:483] Algo bellman_ford step 4359 current loss 0.049630, current_train_items 139520.
I0302 18:59:50.623106 22683591385216 run.py:483] Algo bellman_ford step 4360 current loss 0.001501, current_train_items 139552.
I0302 18:59:50.639094 22683591385216 run.py:483] Algo bellman_ford step 4361 current loss 0.036781, current_train_items 139584.
I0302 18:59:50.659952 22683591385216 run.py:483] Algo bellman_ford step 4362 current loss 0.033141, current_train_items 139616.
I0302 18:59:50.690063 22683591385216 run.py:483] Algo bellman_ford step 4363 current loss 0.046429, current_train_items 139648.
I0302 18:59:50.722530 22683591385216 run.py:483] Algo bellman_ford step 4364 current loss 0.090100, current_train_items 139680.
I0302 18:59:50.740329 22683591385216 run.py:483] Algo bellman_ford step 4365 current loss 0.003573, current_train_items 139712.
I0302 18:59:50.755664 22683591385216 run.py:483] Algo bellman_ford step 4366 current loss 0.018480, current_train_items 139744.
I0302 18:59:50.778047 22683591385216 run.py:483] Algo bellman_ford step 4367 current loss 0.018517, current_train_items 139776.
I0302 18:59:50.806097 22683591385216 run.py:483] Algo bellman_ford step 4368 current loss 0.042908, current_train_items 139808.
I0302 18:59:50.837784 22683591385216 run.py:483] Algo bellman_ford step 4369 current loss 0.078302, current_train_items 139840.
I0302 18:59:50.855915 22683591385216 run.py:483] Algo bellman_ford step 4370 current loss 0.002250, current_train_items 139872.
I0302 18:59:50.871414 22683591385216 run.py:483] Algo bellman_ford step 4371 current loss 0.032653, current_train_items 139904.
I0302 18:59:50.892780 22683591385216 run.py:483] Algo bellman_ford step 4372 current loss 0.030949, current_train_items 139936.
I0302 18:59:50.921223 22683591385216 run.py:483] Algo bellman_ford step 4373 current loss 0.049581, current_train_items 139968.
I0302 18:59:50.951843 22683591385216 run.py:483] Algo bellman_ford step 4374 current loss 0.096490, current_train_items 140000.
I0302 18:59:50.970522 22683591385216 run.py:483] Algo bellman_ford step 4375 current loss 0.003909, current_train_items 140032.
I0302 18:59:50.985922 22683591385216 run.py:483] Algo bellman_ford step 4376 current loss 0.005643, current_train_items 140064.
I0302 18:59:51.007377 22683591385216 run.py:483] Algo bellman_ford step 4377 current loss 0.052409, current_train_items 140096.
I0302 18:59:51.035764 22683591385216 run.py:483] Algo bellman_ford step 4378 current loss 0.047222, current_train_items 140128.
I0302 18:59:51.066906 22683591385216 run.py:483] Algo bellman_ford step 4379 current loss 0.037941, current_train_items 140160.
I0302 18:59:51.084564 22683591385216 run.py:483] Algo bellman_ford step 4380 current loss 0.002153, current_train_items 140192.
I0302 18:59:51.099762 22683591385216 run.py:483] Algo bellman_ford step 4381 current loss 0.046023, current_train_items 140224.
I0302 18:59:51.121962 22683591385216 run.py:483] Algo bellman_ford step 4382 current loss 0.048813, current_train_items 140256.
I0302 18:59:51.150938 22683591385216 run.py:483] Algo bellman_ford step 4383 current loss 0.060629, current_train_items 140288.
I0302 18:59:51.182250 22683591385216 run.py:483] Algo bellman_ford step 4384 current loss 0.070544, current_train_items 140320.
I0302 18:59:51.200829 22683591385216 run.py:483] Algo bellman_ford step 4385 current loss 0.010413, current_train_items 140352.
I0302 18:59:51.216113 22683591385216 run.py:483] Algo bellman_ford step 4386 current loss 0.034599, current_train_items 140384.
I0302 18:59:51.238077 22683591385216 run.py:483] Algo bellman_ford step 4387 current loss 0.084940, current_train_items 140416.
I0302 18:59:51.267028 22683591385216 run.py:483] Algo bellman_ford step 4388 current loss 0.055459, current_train_items 140448.
I0302 18:59:51.296570 22683591385216 run.py:483] Algo bellman_ford step 4389 current loss 0.036060, current_train_items 140480.
I0302 18:59:51.315131 22683591385216 run.py:483] Algo bellman_ford step 4390 current loss 0.003463, current_train_items 140512.
I0302 18:59:51.330463 22683591385216 run.py:483] Algo bellman_ford step 4391 current loss 0.006659, current_train_items 140544.
I0302 18:59:51.352016 22683591385216 run.py:483] Algo bellman_ford step 4392 current loss 0.075138, current_train_items 140576.
I0302 18:59:51.380895 22683591385216 run.py:483] Algo bellman_ford step 4393 current loss 0.102298, current_train_items 140608.
I0302 18:59:51.412467 22683591385216 run.py:483] Algo bellman_ford step 4394 current loss 0.152105, current_train_items 140640.
I0302 18:59:51.430305 22683591385216 run.py:483] Algo bellman_ford step 4395 current loss 0.009863, current_train_items 140672.
I0302 18:59:51.445785 22683591385216 run.py:483] Algo bellman_ford step 4396 current loss 0.042478, current_train_items 140704.
I0302 18:59:51.468501 22683591385216 run.py:483] Algo bellman_ford step 4397 current loss 0.043849, current_train_items 140736.
I0302 18:59:51.497770 22683591385216 run.py:483] Algo bellman_ford step 4398 current loss 0.064752, current_train_items 140768.
I0302 18:59:51.528296 22683591385216 run.py:483] Algo bellman_ford step 4399 current loss 0.121948, current_train_items 140800.
I0302 18:59:51.546574 22683591385216 run.py:483] Algo bellman_ford step 4400 current loss 0.001933, current_train_items 140832.
I0302 18:59:51.554148 22683591385216 run.py:503] (val) algo bellman_ford step 4400: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 140832, 'step': 4400, 'algorithm': 'bellman_ford'}
I0302 18:59:51.554254 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:51.570300 22683591385216 run.py:483] Algo bellman_ford step 4401 current loss 0.020972, current_train_items 140864.
I0302 18:59:51.592890 22683591385216 run.py:483] Algo bellman_ford step 4402 current loss 0.053282, current_train_items 140896.
I0302 18:59:51.621674 22683591385216 run.py:483] Algo bellman_ford step 4403 current loss 0.040479, current_train_items 140928.
I0302 18:59:51.657710 22683591385216 run.py:483] Algo bellman_ford step 4404 current loss 0.127163, current_train_items 140960.
I0302 18:59:51.676173 22683591385216 run.py:483] Algo bellman_ford step 4405 current loss 0.031401, current_train_items 140992.
I0302 18:59:51.691530 22683591385216 run.py:483] Algo bellman_ford step 4406 current loss 0.021881, current_train_items 141024.
I0302 18:59:51.713946 22683591385216 run.py:483] Algo bellman_ford step 4407 current loss 0.067760, current_train_items 141056.
I0302 18:59:51.742849 22683591385216 run.py:483] Algo bellman_ford step 4408 current loss 0.076498, current_train_items 141088.
I0302 18:59:51.775917 22683591385216 run.py:483] Algo bellman_ford step 4409 current loss 0.070759, current_train_items 141120.
I0302 18:59:51.794081 22683591385216 run.py:483] Algo bellman_ford step 4410 current loss 0.002087, current_train_items 141152.
I0302 18:59:51.809432 22683591385216 run.py:483] Algo bellman_ford step 4411 current loss 0.023207, current_train_items 141184.
I0302 18:59:51.831817 22683591385216 run.py:483] Algo bellman_ford step 4412 current loss 0.042249, current_train_items 141216.
I0302 18:59:51.862117 22683591385216 run.py:483] Algo bellman_ford step 4413 current loss 0.062638, current_train_items 141248.
I0302 18:59:51.892246 22683591385216 run.py:483] Algo bellman_ford step 4414 current loss 0.033460, current_train_items 141280.
I0302 18:59:51.910243 22683591385216 run.py:483] Algo bellman_ford step 4415 current loss 0.001736, current_train_items 141312.
I0302 18:59:51.925978 22683591385216 run.py:483] Algo bellman_ford step 4416 current loss 0.076460, current_train_items 141344.
I0302 18:59:51.948095 22683591385216 run.py:483] Algo bellman_ford step 4417 current loss 0.098728, current_train_items 141376.
I0302 18:59:51.976847 22683591385216 run.py:483] Algo bellman_ford step 4418 current loss 0.132335, current_train_items 141408.
I0302 18:59:52.008414 22683591385216 run.py:483] Algo bellman_ford step 4419 current loss 0.056462, current_train_items 141440.
I0302 18:59:52.026331 22683591385216 run.py:483] Algo bellman_ford step 4420 current loss 0.009632, current_train_items 141472.
I0302 18:59:52.041363 22683591385216 run.py:483] Algo bellman_ford step 4421 current loss 0.012986, current_train_items 141504.
I0302 18:59:52.063640 22683591385216 run.py:483] Algo bellman_ford step 4422 current loss 0.085288, current_train_items 141536.
I0302 18:59:52.092554 22683591385216 run.py:483] Algo bellman_ford step 4423 current loss 0.106750, current_train_items 141568.
I0302 18:59:52.124579 22683591385216 run.py:483] Algo bellman_ford step 4424 current loss 0.128095, current_train_items 141600.
I0302 18:59:52.142791 22683591385216 run.py:483] Algo bellman_ford step 4425 current loss 0.010236, current_train_items 141632.
I0302 18:59:52.157267 22683591385216 run.py:483] Algo bellman_ford step 4426 current loss 0.020824, current_train_items 141664.
I0302 18:59:52.179922 22683591385216 run.py:483] Algo bellman_ford step 4427 current loss 0.080267, current_train_items 141696.
I0302 18:59:52.209218 22683591385216 run.py:483] Algo bellman_ford step 4428 current loss 0.060604, current_train_items 141728.
I0302 18:59:52.240483 22683591385216 run.py:483] Algo bellman_ford step 4429 current loss 0.056605, current_train_items 141760.
I0302 18:59:52.258690 22683591385216 run.py:483] Algo bellman_ford step 4430 current loss 0.001319, current_train_items 141792.
I0302 18:59:52.273543 22683591385216 run.py:483] Algo bellman_ford step 4431 current loss 0.025263, current_train_items 141824.
I0302 18:59:52.295805 22683591385216 run.py:483] Algo bellman_ford step 4432 current loss 0.075742, current_train_items 141856.
I0302 18:59:52.324070 22683591385216 run.py:483] Algo bellman_ford step 4433 current loss 0.047008, current_train_items 141888.
I0302 18:59:52.355060 22683591385216 run.py:483] Algo bellman_ford step 4434 current loss 0.128055, current_train_items 141920.
I0302 18:59:52.373299 22683591385216 run.py:483] Algo bellman_ford step 4435 current loss 0.001575, current_train_items 141952.
I0302 18:59:52.388615 22683591385216 run.py:483] Algo bellman_ford step 4436 current loss 0.045078, current_train_items 141984.
I0302 18:59:52.411366 22683591385216 run.py:483] Algo bellman_ford step 4437 current loss 0.055987, current_train_items 142016.
I0302 18:59:52.439327 22683591385216 run.py:483] Algo bellman_ford step 4438 current loss 0.075659, current_train_items 142048.
I0302 18:59:52.471719 22683591385216 run.py:483] Algo bellman_ford step 4439 current loss 0.179291, current_train_items 142080.
I0302 18:59:52.489751 22683591385216 run.py:483] Algo bellman_ford step 4440 current loss 0.005099, current_train_items 142112.
I0302 18:59:52.505346 22683591385216 run.py:483] Algo bellman_ford step 4441 current loss 0.050626, current_train_items 142144.
I0302 18:59:52.527204 22683591385216 run.py:483] Algo bellman_ford step 4442 current loss 0.048511, current_train_items 142176.
I0302 18:59:52.555688 22683591385216 run.py:483] Algo bellman_ford step 4443 current loss 0.057375, current_train_items 142208.
I0302 18:59:52.587912 22683591385216 run.py:483] Algo bellman_ford step 4444 current loss 0.074476, current_train_items 142240.
I0302 18:59:52.606078 22683591385216 run.py:483] Algo bellman_ford step 4445 current loss 0.024839, current_train_items 142272.
I0302 18:59:52.621168 22683591385216 run.py:483] Algo bellman_ford step 4446 current loss 0.031595, current_train_items 142304.
I0302 18:59:52.643931 22683591385216 run.py:483] Algo bellman_ford step 4447 current loss 0.048440, current_train_items 142336.
I0302 18:59:52.673506 22683591385216 run.py:483] Algo bellman_ford step 4448 current loss 0.104742, current_train_items 142368.
I0302 18:59:52.704662 22683591385216 run.py:483] Algo bellman_ford step 4449 current loss 0.096469, current_train_items 142400.
I0302 18:59:52.722648 22683591385216 run.py:483] Algo bellman_ford step 4450 current loss 0.021897, current_train_items 142432.
I0302 18:59:52.730142 22683591385216 run.py:503] (val) algo bellman_ford step 4450: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 142432, 'step': 4450, 'algorithm': 'bellman_ford'}
I0302 18:59:52.730248 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 18:59:52.745426 22683591385216 run.py:483] Algo bellman_ford step 4451 current loss 0.016268, current_train_items 142464.
I0302 18:59:52.768578 22683591385216 run.py:483] Algo bellman_ford step 4452 current loss 0.048655, current_train_items 142496.
I0302 18:59:52.796677 22683591385216 run.py:483] Algo bellman_ford step 4453 current loss 0.052855, current_train_items 142528.
I0302 18:59:52.826775 22683591385216 run.py:483] Algo bellman_ford step 4454 current loss 0.056323, current_train_items 142560.
I0302 18:59:52.844953 22683591385216 run.py:483] Algo bellman_ford step 4455 current loss 0.037556, current_train_items 142592.
I0302 18:59:52.860096 22683591385216 run.py:483] Algo bellman_ford step 4456 current loss 0.023049, current_train_items 142624.
I0302 18:59:52.882412 22683591385216 run.py:483] Algo bellman_ford step 4457 current loss 0.114095, current_train_items 142656.
I0302 18:59:52.909893 22683591385216 run.py:483] Algo bellman_ford step 4458 current loss 0.133405, current_train_items 142688.
I0302 18:59:52.944175 22683591385216 run.py:483] Algo bellman_ford step 4459 current loss 0.138541, current_train_items 142720.
I0302 18:59:52.962941 22683591385216 run.py:483] Algo bellman_ford step 4460 current loss 0.009065, current_train_items 142752.
I0302 18:59:52.978640 22683591385216 run.py:483] Algo bellman_ford step 4461 current loss 0.020862, current_train_items 142784.
I0302 18:59:53.001391 22683591385216 run.py:483] Algo bellman_ford step 4462 current loss 0.070212, current_train_items 142816.
I0302 18:59:53.028853 22683591385216 run.py:483] Algo bellman_ford step 4463 current loss 0.053948, current_train_items 142848.
I0302 18:59:53.060272 22683591385216 run.py:483] Algo bellman_ford step 4464 current loss 0.117994, current_train_items 142880.
I0302 18:59:53.078490 22683591385216 run.py:483] Algo bellman_ford step 4465 current loss 0.002694, current_train_items 142912.
I0302 18:59:53.093851 22683591385216 run.py:483] Algo bellman_ford step 4466 current loss 0.015251, current_train_items 142944.
I0302 18:59:53.115939 22683591385216 run.py:483] Algo bellman_ford step 4467 current loss 0.038475, current_train_items 142976.
I0302 18:59:53.146400 22683591385216 run.py:483] Algo bellman_ford step 4468 current loss 0.042929, current_train_items 143008.
I0302 18:59:53.178122 22683591385216 run.py:483] Algo bellman_ford step 4469 current loss 0.083275, current_train_items 143040.
I0302 18:59:53.196997 22683591385216 run.py:483] Algo bellman_ford step 4470 current loss 0.003546, current_train_items 143072.
I0302 18:59:53.212371 22683591385216 run.py:483] Algo bellman_ford step 4471 current loss 0.002922, current_train_items 143104.
I0302 18:59:53.234532 22683591385216 run.py:483] Algo bellman_ford step 4472 current loss 0.041246, current_train_items 143136.
I0302 18:59:53.263151 22683591385216 run.py:483] Algo bellman_ford step 4473 current loss 0.064026, current_train_items 143168.
I0302 18:59:53.296669 22683591385216 run.py:483] Algo bellman_ford step 4474 current loss 0.086847, current_train_items 143200.
I0302 18:59:53.315433 22683591385216 run.py:483] Algo bellman_ford step 4475 current loss 0.004816, current_train_items 143232.
I0302 18:59:53.330836 22683591385216 run.py:483] Algo bellman_ford step 4476 current loss 0.037952, current_train_items 143264.
I0302 18:59:53.352323 22683591385216 run.py:483] Algo bellman_ford step 4477 current loss 0.036098, current_train_items 143296.
I0302 18:59:53.381061 22683591385216 run.py:483] Algo bellman_ford step 4478 current loss 0.041850, current_train_items 143328.
I0302 18:59:53.413884 22683591385216 run.py:483] Algo bellman_ford step 4479 current loss 0.121058, current_train_items 143360.
I0302 18:59:53.431724 22683591385216 run.py:483] Algo bellman_ford step 4480 current loss 0.002274, current_train_items 143392.
I0302 18:59:53.447315 22683591385216 run.py:483] Algo bellman_ford step 4481 current loss 0.025298, current_train_items 143424.
I0302 18:59:53.469522 22683591385216 run.py:483] Algo bellman_ford step 4482 current loss 0.043561, current_train_items 143456.
I0302 18:59:53.498893 22683591385216 run.py:483] Algo bellman_ford step 4483 current loss 0.061500, current_train_items 143488.
I0302 18:59:53.527731 22683591385216 run.py:483] Algo bellman_ford step 4484 current loss 0.052252, current_train_items 143520.
I0302 18:59:53.546456 22683591385216 run.py:483] Algo bellman_ford step 4485 current loss 0.004222, current_train_items 143552.
I0302 18:59:53.562472 22683591385216 run.py:483] Algo bellman_ford step 4486 current loss 0.010143, current_train_items 143584.
I0302 18:59:53.586030 22683591385216 run.py:483] Algo bellman_ford step 4487 current loss 0.047850, current_train_items 143616.
I0302 18:59:53.615299 22683591385216 run.py:483] Algo bellman_ford step 4488 current loss 0.041956, current_train_items 143648.
I0302 18:59:53.646015 22683591385216 run.py:483] Algo bellman_ford step 4489 current loss 0.067366, current_train_items 143680.
I0302 18:59:53.664566 22683591385216 run.py:483] Algo bellman_ford step 4490 current loss 0.014395, current_train_items 143712.
I0302 18:59:53.679956 22683591385216 run.py:483] Algo bellman_ford step 4491 current loss 0.003818, current_train_items 143744.
I0302 18:59:53.702635 22683591385216 run.py:483] Algo bellman_ford step 4492 current loss 0.030197, current_train_items 143776.
I0302 18:59:53.731887 22683591385216 run.py:483] Algo bellman_ford step 4493 current loss 0.081348, current_train_items 143808.
I0302 18:59:53.763824 22683591385216 run.py:483] Algo bellman_ford step 4494 current loss 0.102532, current_train_items 143840.
I0302 18:59:53.781719 22683591385216 run.py:483] Algo bellman_ford step 4495 current loss 0.002840, current_train_items 143872.
I0302 18:59:53.797308 22683591385216 run.py:483] Algo bellman_ford step 4496 current loss 0.033747, current_train_items 143904.
I0302 18:59:53.819687 22683591385216 run.py:483] Algo bellman_ford step 4497 current loss 0.039827, current_train_items 143936.
I0302 18:59:53.847459 22683591385216 run.py:483] Algo bellman_ford step 4498 current loss 0.052713, current_train_items 143968.
I0302 18:59:53.876789 22683591385216 run.py:483] Algo bellman_ford step 4499 current loss 0.032620, current_train_items 144000.
I0302 18:59:53.895513 22683591385216 run.py:483] Algo bellman_ford step 4500 current loss 0.003262, current_train_items 144032.
I0302 18:59:53.903409 22683591385216 run.py:503] (val) algo bellman_ford step 4500: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 144032, 'step': 4500, 'algorithm': 'bellman_ford'}
I0302 18:59:53.903515 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 18:59:53.919351 22683591385216 run.py:483] Algo bellman_ford step 4501 current loss 0.067627, current_train_items 144064.
I0302 18:59:53.942747 22683591385216 run.py:483] Algo bellman_ford step 4502 current loss 0.077051, current_train_items 144096.
I0302 18:59:53.972212 22683591385216 run.py:483] Algo bellman_ford step 4503 current loss 0.110465, current_train_items 144128.
I0302 18:59:54.004414 22683591385216 run.py:483] Algo bellman_ford step 4504 current loss 0.034737, current_train_items 144160.
I0302 18:59:54.022934 22683591385216 run.py:483] Algo bellman_ford step 4505 current loss 0.003210, current_train_items 144192.
I0302 18:59:54.038405 22683591385216 run.py:483] Algo bellman_ford step 4506 current loss 0.032104, current_train_items 144224.
I0302 18:59:54.060065 22683591385216 run.py:483] Algo bellman_ford step 4507 current loss 0.027557, current_train_items 144256.
I0302 18:59:54.088176 22683591385216 run.py:483] Algo bellman_ford step 4508 current loss 0.039656, current_train_items 144288.
I0302 18:59:54.118775 22683591385216 run.py:483] Algo bellman_ford step 4509 current loss 0.068518, current_train_items 144320.
I0302 18:59:54.136962 22683591385216 run.py:483] Algo bellman_ford step 4510 current loss 0.033681, current_train_items 144352.
I0302 18:59:54.152375 22683591385216 run.py:483] Algo bellman_ford step 4511 current loss 0.026038, current_train_items 144384.
I0302 18:59:54.174933 22683591385216 run.py:483] Algo bellman_ford step 4512 current loss 0.061863, current_train_items 144416.
I0302 18:59:54.203754 22683591385216 run.py:483] Algo bellman_ford step 4513 current loss 0.112985, current_train_items 144448.
I0302 18:59:54.234358 22683591385216 run.py:483] Algo bellman_ford step 4514 current loss 0.087748, current_train_items 144480.
I0302 18:59:54.252356 22683591385216 run.py:483] Algo bellman_ford step 4515 current loss 0.003981, current_train_items 144512.
I0302 18:59:54.268053 22683591385216 run.py:483] Algo bellman_ford step 4516 current loss 0.013229, current_train_items 144544.
I0302 18:59:54.289347 22683591385216 run.py:483] Algo bellman_ford step 4517 current loss 0.059178, current_train_items 144576.
I0302 18:59:54.319083 22683591385216 run.py:483] Algo bellman_ford step 4518 current loss 0.045709, current_train_items 144608.
I0302 18:59:54.353465 22683591385216 run.py:483] Algo bellman_ford step 4519 current loss 0.069131, current_train_items 144640.
I0302 18:59:54.371544 22683591385216 run.py:483] Algo bellman_ford step 4520 current loss 0.002698, current_train_items 144672.
I0302 18:59:54.387015 22683591385216 run.py:483] Algo bellman_ford step 4521 current loss 0.008052, current_train_items 144704.
I0302 18:59:54.408780 22683591385216 run.py:483] Algo bellman_ford step 4522 current loss 0.031701, current_train_items 144736.
I0302 18:59:54.439726 22683591385216 run.py:483] Algo bellman_ford step 4523 current loss 0.072785, current_train_items 144768.
I0302 18:59:54.470930 22683591385216 run.py:483] Algo bellman_ford step 4524 current loss 0.040949, current_train_items 144800.
I0302 18:59:54.488708 22683591385216 run.py:483] Algo bellman_ford step 4525 current loss 0.001474, current_train_items 144832.
I0302 18:59:54.503985 22683591385216 run.py:483] Algo bellman_ford step 4526 current loss 0.007429, current_train_items 144864.
I0302 18:59:54.527100 22683591385216 run.py:483] Algo bellman_ford step 4527 current loss 0.061441, current_train_items 144896.
I0302 18:59:54.556322 22683591385216 run.py:483] Algo bellman_ford step 4528 current loss 0.037491, current_train_items 144928.
I0302 18:59:54.587907 22683591385216 run.py:483] Algo bellman_ford step 4529 current loss 0.073531, current_train_items 144960.
I0302 18:59:54.606306 22683591385216 run.py:483] Algo bellman_ford step 4530 current loss 0.020292, current_train_items 144992.
I0302 18:59:54.621394 22683591385216 run.py:483] Algo bellman_ford step 4531 current loss 0.005355, current_train_items 145024.
I0302 18:59:54.642597 22683591385216 run.py:483] Algo bellman_ford step 4532 current loss 0.025347, current_train_items 145056.
I0302 18:59:54.670140 22683591385216 run.py:483] Algo bellman_ford step 4533 current loss 0.042084, current_train_items 145088.
I0302 18:59:54.702628 22683591385216 run.py:483] Algo bellman_ford step 4534 current loss 0.107160, current_train_items 145120.
I0302 18:59:54.720591 22683591385216 run.py:483] Algo bellman_ford step 4535 current loss 0.000855, current_train_items 145152.
I0302 18:59:54.735793 22683591385216 run.py:483] Algo bellman_ford step 4536 current loss 0.010495, current_train_items 145184.
I0302 18:59:54.758274 22683591385216 run.py:483] Algo bellman_ford step 4537 current loss 0.047326, current_train_items 145216.
I0302 18:59:54.786804 22683591385216 run.py:483] Algo bellman_ford step 4538 current loss 0.018941, current_train_items 145248.
I0302 18:59:54.819634 22683591385216 run.py:483] Algo bellman_ford step 4539 current loss 0.097800, current_train_items 145280.
I0302 18:59:54.837609 22683591385216 run.py:483] Algo bellman_ford step 4540 current loss 0.023491, current_train_items 145312.
I0302 18:59:54.852814 22683591385216 run.py:483] Algo bellman_ford step 4541 current loss 0.018844, current_train_items 145344.
I0302 18:59:54.874122 22683591385216 run.py:483] Algo bellman_ford step 4542 current loss 0.027553, current_train_items 145376.
I0302 18:59:54.902887 22683591385216 run.py:483] Algo bellman_ford step 4543 current loss 0.049200, current_train_items 145408.
I0302 18:59:54.934721 22683591385216 run.py:483] Algo bellman_ford step 4544 current loss 0.090511, current_train_items 145440.
I0302 18:59:54.952737 22683591385216 run.py:483] Algo bellman_ford step 4545 current loss 0.001711, current_train_items 145472.
I0302 18:59:54.967888 22683591385216 run.py:483] Algo bellman_ford step 4546 current loss 0.005325, current_train_items 145504.
I0302 18:59:54.989591 22683591385216 run.py:483] Algo bellman_ford step 4547 current loss 0.019688, current_train_items 145536.
I0302 18:59:55.017338 22683591385216 run.py:483] Algo bellman_ford step 4548 current loss 0.049708, current_train_items 145568.
I0302 18:59:55.049624 22683591385216 run.py:483] Algo bellman_ford step 4549 current loss 0.051220, current_train_items 145600.
I0302 18:59:55.068063 22683591385216 run.py:483] Algo bellman_ford step 4550 current loss 0.004621, current_train_items 145632.
I0302 18:59:55.075690 22683591385216 run.py:503] (val) algo bellman_ford step 4550: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 145632, 'step': 4550, 'algorithm': 'bellman_ford'}
I0302 18:59:55.075797 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0302 18:59:55.092285 22683591385216 run.py:483] Algo bellman_ford step 4551 current loss 0.020318, current_train_items 145664.
I0302 18:59:55.115265 22683591385216 run.py:483] Algo bellman_ford step 4552 current loss 0.047116, current_train_items 145696.
I0302 18:59:55.145361 22683591385216 run.py:483] Algo bellman_ford step 4553 current loss 0.054412, current_train_items 145728.
I0302 18:59:55.177552 22683591385216 run.py:483] Algo bellman_ford step 4554 current loss 0.039979, current_train_items 145760.
I0302 18:59:55.196402 22683591385216 run.py:483] Algo bellman_ford step 4555 current loss 0.009136, current_train_items 145792.
I0302 18:59:55.211153 22683591385216 run.py:483] Algo bellman_ford step 4556 current loss 0.027327, current_train_items 145824.
I0302 18:59:55.232613 22683591385216 run.py:483] Algo bellman_ford step 4557 current loss 0.038077, current_train_items 145856.
I0302 18:59:55.263081 22683591385216 run.py:483] Algo bellman_ford step 4558 current loss 0.052792, current_train_items 145888.
I0302 18:59:55.296063 22683591385216 run.py:483] Algo bellman_ford step 4559 current loss 0.082191, current_train_items 145920.
I0302 18:59:55.314861 22683591385216 run.py:483] Algo bellman_ford step 4560 current loss 0.002184, current_train_items 145952.
I0302 18:59:55.330648 22683591385216 run.py:483] Algo bellman_ford step 4561 current loss 0.018992, current_train_items 145984.
I0302 18:59:55.352540 22683591385216 run.py:483] Algo bellman_ford step 4562 current loss 0.029908, current_train_items 146016.
I0302 18:59:55.380982 22683591385216 run.py:483] Algo bellman_ford step 4563 current loss 0.056796, current_train_items 146048.
I0302 18:59:55.413795 22683591385216 run.py:483] Algo bellman_ford step 4564 current loss 0.075592, current_train_items 146080.
I0302 18:59:55.432183 22683591385216 run.py:483] Algo bellman_ford step 4565 current loss 0.007371, current_train_items 146112.
I0302 18:59:55.447516 22683591385216 run.py:483] Algo bellman_ford step 4566 current loss 0.018836, current_train_items 146144.
I0302 18:59:55.470384 22683591385216 run.py:483] Algo bellman_ford step 4567 current loss 0.052266, current_train_items 146176.
I0302 18:59:55.500415 22683591385216 run.py:483] Algo bellman_ford step 4568 current loss 0.086066, current_train_items 146208.
I0302 18:59:55.531151 22683591385216 run.py:483] Algo bellman_ford step 4569 current loss 0.110407, current_train_items 146240.
I0302 18:59:55.549734 22683591385216 run.py:483] Algo bellman_ford step 4570 current loss 0.003282, current_train_items 146272.
I0302 18:59:55.565210 22683591385216 run.py:483] Algo bellman_ford step 4571 current loss 0.018985, current_train_items 146304.
I0302 18:59:55.587807 22683591385216 run.py:483] Algo bellman_ford step 4572 current loss 0.104286, current_train_items 146336.
I0302 18:59:55.616181 22683591385216 run.py:483] Algo bellman_ford step 4573 current loss 0.040947, current_train_items 146368.
I0302 18:59:55.646582 22683591385216 run.py:483] Algo bellman_ford step 4574 current loss 0.091788, current_train_items 146400.
I0302 18:59:55.664929 22683591385216 run.py:483] Algo bellman_ford step 4575 current loss 0.001957, current_train_items 146432.
I0302 18:59:55.680489 22683591385216 run.py:483] Algo bellman_ford step 4576 current loss 0.021347, current_train_items 146464.
I0302 18:59:55.702245 22683591385216 run.py:483] Algo bellman_ford step 4577 current loss 0.057403, current_train_items 146496.
I0302 18:59:55.731522 22683591385216 run.py:483] Algo bellman_ford step 4578 current loss 0.051877, current_train_items 146528.
I0302 18:59:55.763261 22683591385216 run.py:483] Algo bellman_ford step 4579 current loss 0.070325, current_train_items 146560.
I0302 18:59:55.781470 22683591385216 run.py:483] Algo bellman_ford step 4580 current loss 0.016126, current_train_items 146592.
I0302 18:59:55.797176 22683591385216 run.py:483] Algo bellman_ford step 4581 current loss 0.026593, current_train_items 146624.
I0302 18:59:55.819297 22683591385216 run.py:483] Algo bellman_ford step 4582 current loss 0.075285, current_train_items 146656.
I0302 18:59:55.848611 22683591385216 run.py:483] Algo bellman_ford step 4583 current loss 0.039653, current_train_items 146688.
I0302 18:59:55.880614 22683591385216 run.py:483] Algo bellman_ford step 4584 current loss 0.102298, current_train_items 146720.
I0302 18:59:55.899154 22683591385216 run.py:483] Algo bellman_ford step 4585 current loss 0.003281, current_train_items 146752.
I0302 18:59:55.915016 22683591385216 run.py:483] Algo bellman_ford step 4586 current loss 0.013252, current_train_items 146784.
I0302 18:59:55.936375 22683591385216 run.py:483] Algo bellman_ford step 4587 current loss 0.038424, current_train_items 146816.
I0302 18:59:55.965515 22683591385216 run.py:483] Algo bellman_ford step 4588 current loss 0.041817, current_train_items 146848.
I0302 18:59:55.996203 22683591385216 run.py:483] Algo bellman_ford step 4589 current loss 0.073496, current_train_items 146880.
I0302 18:59:56.014636 22683591385216 run.py:483] Algo bellman_ford step 4590 current loss 0.007416, current_train_items 146912.
I0302 18:59:56.030184 22683591385216 run.py:483] Algo bellman_ford step 4591 current loss 0.017341, current_train_items 146944.
I0302 18:59:56.051888 22683591385216 run.py:483] Algo bellman_ford step 4592 current loss 0.032994, current_train_items 146976.
I0302 18:59:56.081753 22683591385216 run.py:483] Algo bellman_ford step 4593 current loss 0.041028, current_train_items 147008.
I0302 18:59:56.114006 22683591385216 run.py:483] Algo bellman_ford step 4594 current loss 0.102355, current_train_items 147040.
I0302 18:59:56.132523 22683591385216 run.py:483] Algo bellman_ford step 4595 current loss 0.004699, current_train_items 147072.
I0302 18:59:56.147678 22683591385216 run.py:483] Algo bellman_ford step 4596 current loss 0.012934, current_train_items 147104.
I0302 18:59:56.172156 22683591385216 run.py:483] Algo bellman_ford step 4597 current loss 0.071279, current_train_items 147136.
I0302 18:59:56.201274 22683591385216 run.py:483] Algo bellman_ford step 4598 current loss 0.042506, current_train_items 147168.
I0302 18:59:56.231268 22683591385216 run.py:483] Algo bellman_ford step 4599 current loss 0.071137, current_train_items 147200.
I0302 18:59:56.250061 22683591385216 run.py:483] Algo bellman_ford step 4600 current loss 0.003108, current_train_items 147232.
I0302 18:59:56.257871 22683591385216 run.py:503] (val) algo bellman_ford step 4600: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 147232, 'step': 4600, 'algorithm': 'bellman_ford'}
I0302 18:59:56.257986 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 18:59:56.274122 22683591385216 run.py:483] Algo bellman_ford step 4601 current loss 0.020032, current_train_items 147264.
I0302 18:59:56.297493 22683591385216 run.py:483] Algo bellman_ford step 4602 current loss 0.074952, current_train_items 147296.
I0302 18:59:56.327028 22683591385216 run.py:483] Algo bellman_ford step 4603 current loss 0.069332, current_train_items 147328.
I0302 18:59:56.359664 22683591385216 run.py:483] Algo bellman_ford step 4604 current loss 0.076607, current_train_items 147360.
I0302 18:59:56.378247 22683591385216 run.py:483] Algo bellman_ford step 4605 current loss 0.002747, current_train_items 147392.
I0302 18:59:56.393738 22683591385216 run.py:483] Algo bellman_ford step 4606 current loss 0.031691, current_train_items 147424.
I0302 18:59:56.415565 22683591385216 run.py:483] Algo bellman_ford step 4607 current loss 0.016302, current_train_items 147456.
I0302 18:59:56.445603 22683591385216 run.py:483] Algo bellman_ford step 4608 current loss 0.082786, current_train_items 147488.
I0302 18:59:56.475226 22683591385216 run.py:483] Algo bellman_ford step 4609 current loss 0.121474, current_train_items 147520.
I0302 18:59:56.493564 22683591385216 run.py:483] Algo bellman_ford step 4610 current loss 0.074359, current_train_items 147552.
I0302 18:59:56.508747 22683591385216 run.py:483] Algo bellman_ford step 4611 current loss 0.043078, current_train_items 147584.
I0302 18:59:56.531487 22683591385216 run.py:483] Algo bellman_ford step 4612 current loss 0.078898, current_train_items 147616.
I0302 18:59:56.561183 22683591385216 run.py:483] Algo bellman_ford step 4613 current loss 0.137901, current_train_items 147648.
I0302 18:59:56.593809 22683591385216 run.py:483] Algo bellman_ford step 4614 current loss 0.052958, current_train_items 147680.
I0302 18:59:56.612181 22683591385216 run.py:483] Algo bellman_ford step 4615 current loss 0.011167, current_train_items 147712.
I0302 18:59:56.627609 22683591385216 run.py:483] Algo bellman_ford step 4616 current loss 0.050947, current_train_items 147744.
I0302 18:59:56.650743 22683591385216 run.py:483] Algo bellman_ford step 4617 current loss 0.044817, current_train_items 147776.
I0302 18:59:56.679306 22683591385216 run.py:483] Algo bellman_ford step 4618 current loss 0.062862, current_train_items 147808.
I0302 18:59:56.710877 22683591385216 run.py:483] Algo bellman_ford step 4619 current loss 0.112807, current_train_items 147840.
I0302 18:59:56.729127 22683591385216 run.py:483] Algo bellman_ford step 4620 current loss 0.010774, current_train_items 147872.
I0302 18:59:56.744330 22683591385216 run.py:483] Algo bellman_ford step 4621 current loss 0.017855, current_train_items 147904.
I0302 18:59:56.766470 22683591385216 run.py:483] Algo bellman_ford step 4622 current loss 0.047060, current_train_items 147936.
I0302 18:59:56.796162 22683591385216 run.py:483] Algo bellman_ford step 4623 current loss 0.052640, current_train_items 147968.
I0302 18:59:56.827054 22683591385216 run.py:483] Algo bellman_ford step 4624 current loss 0.306986, current_train_items 148000.
I0302 18:59:56.845402 22683591385216 run.py:483] Algo bellman_ford step 4625 current loss 0.004139, current_train_items 148032.
I0302 18:59:56.860645 22683591385216 run.py:483] Algo bellman_ford step 4626 current loss 0.029769, current_train_items 148064.
I0302 18:59:56.883602 22683591385216 run.py:483] Algo bellman_ford step 4627 current loss 0.045713, current_train_items 148096.
I0302 18:59:56.913845 22683591385216 run.py:483] Algo bellman_ford step 4628 current loss 0.083084, current_train_items 148128.
I0302 18:59:56.946835 22683591385216 run.py:483] Algo bellman_ford step 4629 current loss 0.077652, current_train_items 148160.
I0302 18:59:56.964866 22683591385216 run.py:483] Algo bellman_ford step 4630 current loss 0.001898, current_train_items 148192.
I0302 18:59:56.980373 22683591385216 run.py:483] Algo bellman_ford step 4631 current loss 0.024138, current_train_items 148224.
I0302 18:59:57.003794 22683591385216 run.py:483] Algo bellman_ford step 4632 current loss 0.057449, current_train_items 148256.
I0302 18:59:57.031856 22683591385216 run.py:483] Algo bellman_ford step 4633 current loss 0.031933, current_train_items 148288.
I0302 18:59:57.063763 22683591385216 run.py:483] Algo bellman_ford step 4634 current loss 0.061424, current_train_items 148320.
I0302 18:59:57.082181 22683591385216 run.py:483] Algo bellman_ford step 4635 current loss 0.000906, current_train_items 148352.
I0302 18:59:57.098007 22683591385216 run.py:483] Algo bellman_ford step 4636 current loss 0.008874, current_train_items 148384.
I0302 18:59:57.120824 22683591385216 run.py:483] Algo bellman_ford step 4637 current loss 0.057187, current_train_items 148416.
I0302 18:59:57.151579 22683591385216 run.py:483] Algo bellman_ford step 4638 current loss 0.049252, current_train_items 148448.
I0302 18:59:57.183783 22683591385216 run.py:483] Algo bellman_ford step 4639 current loss 0.073740, current_train_items 148480.
I0302 18:59:57.202181 22683591385216 run.py:483] Algo bellman_ford step 4640 current loss 0.003844, current_train_items 148512.
I0302 18:59:57.217465 22683591385216 run.py:483] Algo bellman_ford step 4641 current loss 0.017304, current_train_items 148544.
I0302 18:59:57.239964 22683591385216 run.py:483] Algo bellman_ford step 4642 current loss 0.089402, current_train_items 148576.
I0302 18:59:57.269407 22683591385216 run.py:483] Algo bellman_ford step 4643 current loss 0.101505, current_train_items 148608.
I0302 18:59:57.301782 22683591385216 run.py:483] Algo bellman_ford step 4644 current loss 0.053159, current_train_items 148640.
I0302 18:59:57.320367 22683591385216 run.py:483] Algo bellman_ford step 4645 current loss 0.001514, current_train_items 148672.
I0302 18:59:57.335716 22683591385216 run.py:483] Algo bellman_ford step 4646 current loss 0.004593, current_train_items 148704.
I0302 18:59:57.356876 22683591385216 run.py:483] Algo bellman_ford step 4647 current loss 0.032276, current_train_items 148736.
I0302 18:59:57.384353 22683591385216 run.py:483] Algo bellman_ford step 4648 current loss 0.046711, current_train_items 148768.
I0302 18:59:57.414058 22683591385216 run.py:483] Algo bellman_ford step 4649 current loss 0.048429, current_train_items 148800.
I0302 18:59:57.432565 22683591385216 run.py:483] Algo bellman_ford step 4650 current loss 0.001835, current_train_items 148832.
I0302 18:59:57.440687 22683591385216 run.py:503] (val) algo bellman_ford step 4650: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 148832, 'step': 4650, 'algorithm': 'bellman_ford'}
I0302 18:59:57.440794 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 18:59:57.456841 22683591385216 run.py:483] Algo bellman_ford step 4651 current loss 0.022903, current_train_items 148864.
I0302 18:59:57.479469 22683591385216 run.py:483] Algo bellman_ford step 4652 current loss 0.021481, current_train_items 148896.
I0302 18:59:57.509909 22683591385216 run.py:483] Algo bellman_ford step 4653 current loss 0.055832, current_train_items 148928.
I0302 18:59:57.542043 22683591385216 run.py:483] Algo bellman_ford step 4654 current loss 0.063711, current_train_items 148960.
I0302 18:59:57.560349 22683591385216 run.py:483] Algo bellman_ford step 4655 current loss 0.003763, current_train_items 148992.
I0302 18:59:57.575598 22683591385216 run.py:483] Algo bellman_ford step 4656 current loss 0.016959, current_train_items 149024.
I0302 18:59:57.598235 22683591385216 run.py:483] Algo bellman_ford step 4657 current loss 0.089453, current_train_items 149056.
I0302 18:59:57.627135 22683591385216 run.py:483] Algo bellman_ford step 4658 current loss 0.065343, current_train_items 149088.
I0302 18:59:57.657169 22683591385216 run.py:483] Algo bellman_ford step 4659 current loss 0.060472, current_train_items 149120.
I0302 18:59:57.675568 22683591385216 run.py:483] Algo bellman_ford step 4660 current loss 0.001774, current_train_items 149152.
I0302 18:59:57.691271 22683591385216 run.py:483] Algo bellman_ford step 4661 current loss 0.019858, current_train_items 149184.
I0302 18:59:57.714030 22683591385216 run.py:483] Algo bellman_ford step 4662 current loss 0.111998, current_train_items 149216.
I0302 18:59:57.742469 22683591385216 run.py:483] Algo bellman_ford step 4663 current loss 0.082429, current_train_items 149248.
I0302 18:59:57.777151 22683591385216 run.py:483] Algo bellman_ford step 4664 current loss 0.225709, current_train_items 149280.
I0302 18:59:57.795403 22683591385216 run.py:483] Algo bellman_ford step 4665 current loss 0.014691, current_train_items 149312.
I0302 18:59:57.810437 22683591385216 run.py:483] Algo bellman_ford step 4666 current loss 0.010145, current_train_items 149344.
I0302 18:59:57.832114 22683591385216 run.py:483] Algo bellman_ford step 4667 current loss 0.034142, current_train_items 149376.
I0302 18:59:57.861792 22683591385216 run.py:483] Algo bellman_ford step 4668 current loss 0.039097, current_train_items 149408.
I0302 18:59:57.893700 22683591385216 run.py:483] Algo bellman_ford step 4669 current loss 0.072893, current_train_items 149440.
I0302 18:59:57.912150 22683591385216 run.py:483] Algo bellman_ford step 4670 current loss 0.002153, current_train_items 149472.
I0302 18:59:57.927319 22683591385216 run.py:483] Algo bellman_ford step 4671 current loss 0.011580, current_train_items 149504.
I0302 18:59:57.948466 22683591385216 run.py:483] Algo bellman_ford step 4672 current loss 0.035838, current_train_items 149536.
I0302 18:59:57.976168 22683591385216 run.py:483] Algo bellman_ford step 4673 current loss 0.056732, current_train_items 149568.
I0302 18:59:58.007369 22683591385216 run.py:483] Algo bellman_ford step 4674 current loss 0.058159, current_train_items 149600.
I0302 18:59:58.025825 22683591385216 run.py:483] Algo bellman_ford step 4675 current loss 0.004655, current_train_items 149632.
I0302 18:59:58.041804 22683591385216 run.py:483] Algo bellman_ford step 4676 current loss 0.024375, current_train_items 149664.
I0302 18:59:58.064224 22683591385216 run.py:483] Algo bellman_ford step 4677 current loss 0.036832, current_train_items 149696.
I0302 18:59:58.094688 22683591385216 run.py:483] Algo bellman_ford step 4678 current loss 0.058460, current_train_items 149728.
I0302 18:59:58.125960 22683591385216 run.py:483] Algo bellman_ford step 4679 current loss 0.062135, current_train_items 149760.
I0302 18:59:58.143946 22683591385216 run.py:483] Algo bellman_ford step 4680 current loss 0.000895, current_train_items 149792.
I0302 18:59:58.158970 22683591385216 run.py:483] Algo bellman_ford step 4681 current loss 0.010437, current_train_items 149824.
I0302 18:59:58.180751 22683591385216 run.py:483] Algo bellman_ford step 4682 current loss 0.039367, current_train_items 149856.
I0302 18:59:58.209288 22683591385216 run.py:483] Algo bellman_ford step 4683 current loss 0.054375, current_train_items 149888.
I0302 18:59:58.242005 22683591385216 run.py:483] Algo bellman_ford step 4684 current loss 0.049792, current_train_items 149920.
I0302 18:59:58.260608 22683591385216 run.py:483] Algo bellman_ford step 4685 current loss 0.003045, current_train_items 149952.
I0302 18:59:58.275870 22683591385216 run.py:483] Algo bellman_ford step 4686 current loss 0.009508, current_train_items 149984.
I0302 18:59:58.297963 22683591385216 run.py:483] Algo bellman_ford step 4687 current loss 0.042935, current_train_items 150016.
I0302 18:59:58.327345 22683591385216 run.py:483] Algo bellman_ford step 4688 current loss 0.026316, current_train_items 150048.
I0302 18:59:58.359931 22683591385216 run.py:483] Algo bellman_ford step 4689 current loss 0.100909, current_train_items 150080.
I0302 18:59:58.378853 22683591385216 run.py:483] Algo bellman_ford step 4690 current loss 0.011372, current_train_items 150112.
I0302 18:59:58.394418 22683591385216 run.py:483] Algo bellman_ford step 4691 current loss 0.021386, current_train_items 150144.
I0302 18:59:58.415893 22683591385216 run.py:483] Algo bellman_ford step 4692 current loss 0.035019, current_train_items 150176.
I0302 18:59:58.445083 22683591385216 run.py:483] Algo bellman_ford step 4693 current loss 0.115133, current_train_items 150208.
I0302 18:59:58.475852 22683591385216 run.py:483] Algo bellman_ford step 4694 current loss 0.143439, current_train_items 150240.
I0302 18:59:58.493895 22683591385216 run.py:483] Algo bellman_ford step 4695 current loss 0.001684, current_train_items 150272.
I0302 18:59:58.509248 22683591385216 run.py:483] Algo bellman_ford step 4696 current loss 0.015558, current_train_items 150304.
I0302 18:59:58.532170 22683591385216 run.py:483] Algo bellman_ford step 4697 current loss 0.039504, current_train_items 150336.
I0302 18:59:58.560708 22683591385216 run.py:483] Algo bellman_ford step 4698 current loss 0.036312, current_train_items 150368.
I0302 18:59:58.590186 22683591385216 run.py:483] Algo bellman_ford step 4699 current loss 0.058477, current_train_items 150400.
I0302 18:59:58.608634 22683591385216 run.py:483] Algo bellman_ford step 4700 current loss 0.002704, current_train_items 150432.
I0302 18:59:58.616237 22683591385216 run.py:503] (val) algo bellman_ford step 4700: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 150432, 'step': 4700, 'algorithm': 'bellman_ford'}
I0302 18:59:58.616346 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 18:59:58.631861 22683591385216 run.py:483] Algo bellman_ford step 4701 current loss 0.023403, current_train_items 150464.
I0302 18:59:58.654869 22683591385216 run.py:483] Algo bellman_ford step 4702 current loss 0.074702, current_train_items 150496.
I0302 18:59:58.685684 22683591385216 run.py:483] Algo bellman_ford step 4703 current loss 0.063872, current_train_items 150528.
I0302 18:59:58.718619 22683591385216 run.py:483] Algo bellman_ford step 4704 current loss 0.051028, current_train_items 150560.
I0302 18:59:58.736891 22683591385216 run.py:483] Algo bellman_ford step 4705 current loss 0.005877, current_train_items 150592.
I0302 18:59:58.751819 22683591385216 run.py:483] Algo bellman_ford step 4706 current loss 0.006039, current_train_items 150624.
I0302 18:59:58.774499 22683591385216 run.py:483] Algo bellman_ford step 4707 current loss 0.028816, current_train_items 150656.
I0302 18:59:58.804812 22683591385216 run.py:483] Algo bellman_ford step 4708 current loss 0.054290, current_train_items 150688.
I0302 18:59:58.836001 22683591385216 run.py:483] Algo bellman_ford step 4709 current loss 0.042515, current_train_items 150720.
I0302 18:59:58.854295 22683591385216 run.py:483] Algo bellman_ford step 4710 current loss 0.009414, current_train_items 150752.
I0302 18:59:58.869639 22683591385216 run.py:483] Algo bellman_ford step 4711 current loss 0.020490, current_train_items 150784.
I0302 18:59:58.892324 22683591385216 run.py:483] Algo bellman_ford step 4712 current loss 0.049703, current_train_items 150816.
I0302 18:59:58.921778 22683591385216 run.py:483] Algo bellman_ford step 4713 current loss 0.059093, current_train_items 150848.
I0302 18:59:58.951405 22683591385216 run.py:483] Algo bellman_ford step 4714 current loss 0.057408, current_train_items 150880.
I0302 18:59:58.969265 22683591385216 run.py:483] Algo bellman_ford step 4715 current loss 0.002857, current_train_items 150912.
I0302 18:59:58.984727 22683591385216 run.py:483] Algo bellman_ford step 4716 current loss 0.026148, current_train_items 150944.
I0302 18:59:59.005486 22683591385216 run.py:483] Algo bellman_ford step 4717 current loss 0.022234, current_train_items 150976.
I0302 18:59:59.034556 22683591385216 run.py:483] Algo bellman_ford step 4718 current loss 0.028236, current_train_items 151008.
I0302 18:59:59.067882 22683591385216 run.py:483] Algo bellman_ford step 4719 current loss 0.085992, current_train_items 151040.
I0302 18:59:59.085813 22683591385216 run.py:483] Algo bellman_ford step 4720 current loss 0.003881, current_train_items 151072.
I0302 18:59:59.100760 22683591385216 run.py:483] Algo bellman_ford step 4721 current loss 0.006049, current_train_items 151104.
I0302 18:59:59.123555 22683591385216 run.py:483] Algo bellman_ford step 4722 current loss 0.073395, current_train_items 151136.
I0302 18:59:59.151253 22683591385216 run.py:483] Algo bellman_ford step 4723 current loss 0.054512, current_train_items 151168.
I0302 18:59:59.182996 22683591385216 run.py:483] Algo bellman_ford step 4724 current loss 0.070193, current_train_items 151200.
I0302 18:59:59.201026 22683591385216 run.py:483] Algo bellman_ford step 4725 current loss 0.001250, current_train_items 151232.
I0302 18:59:59.216597 22683591385216 run.py:483] Algo bellman_ford step 4726 current loss 0.027500, current_train_items 151264.
I0302 18:59:59.239952 22683591385216 run.py:483] Algo bellman_ford step 4727 current loss 0.105940, current_train_items 151296.
I0302 18:59:59.267463 22683591385216 run.py:483] Algo bellman_ford step 4728 current loss 0.046297, current_train_items 151328.
I0302 18:59:59.295020 22683591385216 run.py:483] Algo bellman_ford step 4729 current loss 0.056079, current_train_items 151360.
I0302 18:59:59.312911 22683591385216 run.py:483] Algo bellman_ford step 4730 current loss 0.001569, current_train_items 151392.
I0302 18:59:59.327910 22683591385216 run.py:483] Algo bellman_ford step 4731 current loss 0.002737, current_train_items 151424.
I0302 18:59:59.349507 22683591385216 run.py:483] Algo bellman_ford step 4732 current loss 0.027388, current_train_items 151456.
I0302 18:59:59.378506 22683591385216 run.py:483] Algo bellman_ford step 4733 current loss 0.039521, current_train_items 151488.
I0302 18:59:59.410100 22683591385216 run.py:483] Algo bellman_ford step 4734 current loss 0.104327, current_train_items 151520.
I0302 18:59:59.427985 22683591385216 run.py:483] Algo bellman_ford step 4735 current loss 0.001599, current_train_items 151552.
I0302 18:59:59.443573 22683591385216 run.py:483] Algo bellman_ford step 4736 current loss 0.025134, current_train_items 151584.
I0302 18:59:59.465412 22683591385216 run.py:483] Algo bellman_ford step 4737 current loss 0.070012, current_train_items 151616.
I0302 18:59:59.494149 22683591385216 run.py:483] Algo bellman_ford step 4738 current loss 0.046070, current_train_items 151648.
I0302 18:59:59.524518 22683591385216 run.py:483] Algo bellman_ford step 4739 current loss 0.063989, current_train_items 151680.
I0302 18:59:59.542589 22683591385216 run.py:483] Algo bellman_ford step 4740 current loss 0.032425, current_train_items 151712.
I0302 18:59:59.557624 22683591385216 run.py:483] Algo bellman_ford step 4741 current loss 0.022326, current_train_items 151744.
I0302 18:59:59.579077 22683591385216 run.py:483] Algo bellman_ford step 4742 current loss 0.089617, current_train_items 151776.
I0302 18:59:59.608903 22683591385216 run.py:483] Algo bellman_ford step 4743 current loss 0.075473, current_train_items 151808.
I0302 18:59:59.638501 22683591385216 run.py:483] Algo bellman_ford step 4744 current loss 0.082993, current_train_items 151840.
I0302 18:59:59.656496 22683591385216 run.py:483] Algo bellman_ford step 4745 current loss 0.011788, current_train_items 151872.
I0302 18:59:59.672290 22683591385216 run.py:483] Algo bellman_ford step 4746 current loss 0.045050, current_train_items 151904.
I0302 18:59:59.695905 22683591385216 run.py:483] Algo bellman_ford step 4747 current loss 0.044211, current_train_items 151936.
I0302 18:59:59.724980 22683591385216 run.py:483] Algo bellman_ford step 4748 current loss 0.065003, current_train_items 151968.
I0302 18:59:59.756103 22683591385216 run.py:483] Algo bellman_ford step 4749 current loss 0.099762, current_train_items 152000.
I0302 18:59:59.774246 22683591385216 run.py:483] Algo bellman_ford step 4750 current loss 0.029184, current_train_items 152032.
I0302 18:59:59.782241 22683591385216 run.py:503] (val) algo bellman_ford step 4750: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 152032, 'step': 4750, 'algorithm': 'bellman_ford'}
I0302 18:59:59.782368 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 18:59:59.798238 22683591385216 run.py:483] Algo bellman_ford step 4751 current loss 0.009817, current_train_items 152064.
I0302 18:59:59.821287 22683591385216 run.py:483] Algo bellman_ford step 4752 current loss 0.029563, current_train_items 152096.
I0302 18:59:59.852580 22683591385216 run.py:483] Algo bellman_ford step 4753 current loss 0.089547, current_train_items 152128.
I0302 18:59:59.883799 22683591385216 run.py:483] Algo bellman_ford step 4754 current loss 0.068490, current_train_items 152160.
I0302 18:59:59.902250 22683591385216 run.py:483] Algo bellman_ford step 4755 current loss 0.006186, current_train_items 152192.
I0302 18:59:59.917431 22683591385216 run.py:483] Algo bellman_ford step 4756 current loss 0.008375, current_train_items 152224.
I0302 18:59:59.940201 22683591385216 run.py:483] Algo bellman_ford step 4757 current loss 0.050661, current_train_items 152256.
I0302 18:59:59.969559 22683591385216 run.py:483] Algo bellman_ford step 4758 current loss 0.064281, current_train_items 152288.
I0302 19:00:00.001339 22683591385216 run.py:483] Algo bellman_ford step 4759 current loss 0.105936, current_train_items 152320.
I0302 19:00:00.019700 22683591385216 run.py:483] Algo bellman_ford step 4760 current loss 0.003665, current_train_items 152352.
I0302 19:00:00.035480 22683591385216 run.py:483] Algo bellman_ford step 4761 current loss 0.020895, current_train_items 152384.
I0302 19:00:00.057608 22683591385216 run.py:483] Algo bellman_ford step 4762 current loss 0.069317, current_train_items 152416.
I0302 19:00:00.086140 22683591385216 run.py:483] Algo bellman_ford step 4763 current loss 0.037019, current_train_items 152448.
I0302 19:00:00.117863 22683591385216 run.py:483] Algo bellman_ford step 4764 current loss 0.084312, current_train_items 152480.
I0302 19:00:00.135739 22683591385216 run.py:483] Algo bellman_ford step 4765 current loss 0.016960, current_train_items 152512.
I0302 19:00:00.150506 22683591385216 run.py:483] Algo bellman_ford step 4766 current loss 0.015776, current_train_items 152544.
I0302 19:00:00.173495 22683591385216 run.py:483] Algo bellman_ford step 4767 current loss 0.070492, current_train_items 152576.
I0302 19:00:00.201353 22683591385216 run.py:483] Algo bellman_ford step 4768 current loss 0.039072, current_train_items 152608.
I0302 19:00:00.231918 22683591385216 run.py:483] Algo bellman_ford step 4769 current loss 0.071001, current_train_items 152640.
I0302 19:00:00.250315 22683591385216 run.py:483] Algo bellman_ford step 4770 current loss 0.001960, current_train_items 152672.
I0302 19:00:00.265727 22683591385216 run.py:483] Algo bellman_ford step 4771 current loss 0.003321, current_train_items 152704.
I0302 19:00:00.286835 22683591385216 run.py:483] Algo bellman_ford step 4772 current loss 0.028823, current_train_items 152736.
I0302 19:00:00.315565 22683591385216 run.py:483] Algo bellman_ford step 4773 current loss 0.074323, current_train_items 152768.
I0302 19:00:00.347482 22683591385216 run.py:483] Algo bellman_ford step 4774 current loss 0.071145, current_train_items 152800.
I0302 19:00:00.365910 22683591385216 run.py:483] Algo bellman_ford step 4775 current loss 0.002732, current_train_items 152832.
I0302 19:00:00.381397 22683591385216 run.py:483] Algo bellman_ford step 4776 current loss 0.020029, current_train_items 152864.
I0302 19:00:00.403262 22683591385216 run.py:483] Algo bellman_ford step 4777 current loss 0.031322, current_train_items 152896.
I0302 19:00:00.432840 22683591385216 run.py:483] Algo bellman_ford step 4778 current loss 0.046128, current_train_items 152928.
I0302 19:00:00.462545 22683591385216 run.py:483] Algo bellman_ford step 4779 current loss 0.032950, current_train_items 152960.
I0302 19:00:00.480476 22683591385216 run.py:483] Algo bellman_ford step 4780 current loss 0.002723, current_train_items 152992.
I0302 19:00:00.495774 22683591385216 run.py:483] Algo bellman_ford step 4781 current loss 0.020879, current_train_items 153024.
I0302 19:00:00.518370 22683591385216 run.py:483] Algo bellman_ford step 4782 current loss 0.052252, current_train_items 153056.
I0302 19:00:00.548329 22683591385216 run.py:483] Algo bellman_ford step 4783 current loss 0.077423, current_train_items 153088.
I0302 19:00:00.580391 22683591385216 run.py:483] Algo bellman_ford step 4784 current loss 0.065758, current_train_items 153120.
I0302 19:00:00.598847 22683591385216 run.py:483] Algo bellman_ford step 4785 current loss 0.022345, current_train_items 153152.
I0302 19:00:00.614759 22683591385216 run.py:483] Algo bellman_ford step 4786 current loss 0.035445, current_train_items 153184.
I0302 19:00:00.637255 22683591385216 run.py:483] Algo bellman_ford step 4787 current loss 0.036955, current_train_items 153216.
I0302 19:00:00.665815 22683591385216 run.py:483] Algo bellman_ford step 4788 current loss 0.052709, current_train_items 153248.
I0302 19:00:00.695685 22683591385216 run.py:483] Algo bellman_ford step 4789 current loss 0.054454, current_train_items 153280.
I0302 19:00:00.713679 22683591385216 run.py:483] Algo bellman_ford step 4790 current loss 0.001300, current_train_items 153312.
I0302 19:00:00.729176 22683591385216 run.py:483] Algo bellman_ford step 4791 current loss 0.019068, current_train_items 153344.
I0302 19:00:00.751255 22683591385216 run.py:483] Algo bellman_ford step 4792 current loss 0.024276, current_train_items 153376.
I0302 19:00:00.780088 22683591385216 run.py:483] Algo bellman_ford step 4793 current loss 0.144422, current_train_items 153408.
I0302 19:00:00.813339 22683591385216 run.py:483] Algo bellman_ford step 4794 current loss 0.062580, current_train_items 153440.
I0302 19:00:00.831005 22683591385216 run.py:483] Algo bellman_ford step 4795 current loss 0.001422, current_train_items 153472.
I0302 19:00:00.846295 22683591385216 run.py:483] Algo bellman_ford step 4796 current loss 0.006029, current_train_items 153504.
I0302 19:00:00.868683 22683591385216 run.py:483] Algo bellman_ford step 4797 current loss 0.031141, current_train_items 153536.
I0302 19:00:00.897341 22683591385216 run.py:483] Algo bellman_ford step 4798 current loss 0.050772, current_train_items 153568.
I0302 19:00:00.927716 22683591385216 run.py:483] Algo bellman_ford step 4799 current loss 0.052631, current_train_items 153600.
I0302 19:00:00.946626 22683591385216 run.py:483] Algo bellman_ford step 4800 current loss 0.001019, current_train_items 153632.
I0302 19:00:00.954349 22683591385216 run.py:503] (val) algo bellman_ford step 4800: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 153632, 'step': 4800, 'algorithm': 'bellman_ford'}
I0302 19:00:00.954456 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 19:00:00.970346 22683591385216 run.py:483] Algo bellman_ford step 4801 current loss 0.047632, current_train_items 153664.
I0302 19:00:00.993108 22683591385216 run.py:483] Algo bellman_ford step 4802 current loss 0.093031, current_train_items 153696.
I0302 19:00:01.022096 22683591385216 run.py:483] Algo bellman_ford step 4803 current loss 0.045103, current_train_items 153728.
I0302 19:00:01.056276 22683591385216 run.py:483] Algo bellman_ford step 4804 current loss 0.061338, current_train_items 153760.
I0302 19:00:01.074552 22683591385216 run.py:483] Algo bellman_ford step 4805 current loss 0.030674, current_train_items 153792.
I0302 19:00:01.090265 22683591385216 run.py:483] Algo bellman_ford step 4806 current loss 0.020921, current_train_items 153824.
I0302 19:00:01.112681 22683591385216 run.py:483] Algo bellman_ford step 4807 current loss 0.051405, current_train_items 153856.
I0302 19:00:01.142007 22683591385216 run.py:483] Algo bellman_ford step 4808 current loss 0.070093, current_train_items 153888.
I0302 19:00:01.173423 22683591385216 run.py:483] Algo bellman_ford step 4809 current loss 0.057990, current_train_items 153920.
I0302 19:00:01.191739 22683591385216 run.py:483] Algo bellman_ford step 4810 current loss 0.001654, current_train_items 153952.
I0302 19:00:01.207051 22683591385216 run.py:483] Algo bellman_ford step 4811 current loss 0.010515, current_train_items 153984.
I0302 19:00:01.228806 22683591385216 run.py:483] Algo bellman_ford step 4812 current loss 0.106245, current_train_items 154016.
I0302 19:00:01.257421 22683591385216 run.py:483] Algo bellman_ford step 4813 current loss 0.090296, current_train_items 154048.
I0302 19:00:01.290266 22683591385216 run.py:483] Algo bellman_ford step 4814 current loss 0.108509, current_train_items 154080.
I0302 19:00:01.308328 22683591385216 run.py:483] Algo bellman_ford step 4815 current loss 0.006300, current_train_items 154112.
I0302 19:00:01.323681 22683591385216 run.py:483] Algo bellman_ford step 4816 current loss 0.066654, current_train_items 154144.
I0302 19:00:01.347107 22683591385216 run.py:483] Algo bellman_ford step 4817 current loss 0.073287, current_train_items 154176.
I0302 19:00:01.376321 22683591385216 run.py:483] Algo bellman_ford step 4818 current loss 0.051458, current_train_items 154208.
I0302 19:00:01.403589 22683591385216 run.py:483] Algo bellman_ford step 4819 current loss 0.066777, current_train_items 154240.
I0302 19:00:01.422252 22683591385216 run.py:483] Algo bellman_ford step 4820 current loss 0.005732, current_train_items 154272.
I0302 19:00:01.437412 22683591385216 run.py:483] Algo bellman_ford step 4821 current loss 0.029438, current_train_items 154304.
I0302 19:00:01.459999 22683591385216 run.py:483] Algo bellman_ford step 4822 current loss 0.050410, current_train_items 154336.
I0302 19:00:01.488686 22683591385216 run.py:483] Algo bellman_ford step 4823 current loss 0.051252, current_train_items 154368.
I0302 19:00:01.520682 22683591385216 run.py:483] Algo bellman_ford step 4824 current loss 0.093482, current_train_items 154400.
I0302 19:00:01.538822 22683591385216 run.py:483] Algo bellman_ford step 4825 current loss 0.005453, current_train_items 154432.
I0302 19:00:01.554715 22683591385216 run.py:483] Algo bellman_ford step 4826 current loss 0.016866, current_train_items 154464.
I0302 19:00:01.576297 22683591385216 run.py:483] Algo bellman_ford step 4827 current loss 0.021989, current_train_items 154496.
I0302 19:00:01.605597 22683591385216 run.py:483] Algo bellman_ford step 4828 current loss 0.038911, current_train_items 154528.
I0302 19:00:01.637380 22683591385216 run.py:483] Algo bellman_ford step 4829 current loss 0.084769, current_train_items 154560.
I0302 19:00:01.655428 22683591385216 run.py:483] Algo bellman_ford step 4830 current loss 0.003297, current_train_items 154592.
I0302 19:00:01.670968 22683591385216 run.py:483] Algo bellman_ford step 4831 current loss 0.028710, current_train_items 154624.
I0302 19:00:01.694149 22683591385216 run.py:483] Algo bellman_ford step 4832 current loss 0.021720, current_train_items 154656.
I0302 19:00:01.723747 22683591385216 run.py:483] Algo bellman_ford step 4833 current loss 0.055741, current_train_items 154688.
I0302 19:00:01.754610 22683591385216 run.py:483] Algo bellman_ford step 4834 current loss 0.045778, current_train_items 154720.
I0302 19:00:01.772376 22683591385216 run.py:483] Algo bellman_ford step 4835 current loss 0.002657, current_train_items 154752.
I0302 19:00:01.787561 22683591385216 run.py:483] Algo bellman_ford step 4836 current loss 0.006246, current_train_items 154784.
I0302 19:00:01.808857 22683591385216 run.py:483] Algo bellman_ford step 4837 current loss 0.030638, current_train_items 154816.
I0302 19:00:01.837861 22683591385216 run.py:483] Algo bellman_ford step 4838 current loss 0.035992, current_train_items 154848.
I0302 19:00:01.867287 22683591385216 run.py:483] Algo bellman_ford step 4839 current loss 0.044885, current_train_items 154880.
I0302 19:00:01.885192 22683591385216 run.py:483] Algo bellman_ford step 4840 current loss 0.003150, current_train_items 154912.
I0302 19:00:01.900288 22683591385216 run.py:483] Algo bellman_ford step 4841 current loss 0.020655, current_train_items 154944.
I0302 19:00:01.922406 22683591385216 run.py:483] Algo bellman_ford step 4842 current loss 0.023621, current_train_items 154976.
I0302 19:00:01.950932 22683591385216 run.py:483] Algo bellman_ford step 4843 current loss 0.060007, current_train_items 155008.
I0302 19:00:01.983585 22683591385216 run.py:483] Algo bellman_ford step 4844 current loss 0.063128, current_train_items 155040.
I0302 19:00:02.001698 22683591385216 run.py:483] Algo bellman_ford step 4845 current loss 0.002032, current_train_items 155072.
I0302 19:00:02.017244 22683591385216 run.py:483] Algo bellman_ford step 4846 current loss 0.011397, current_train_items 155104.
I0302 19:00:02.038826 22683591385216 run.py:483] Algo bellman_ford step 4847 current loss 0.039034, current_train_items 155136.
I0302 19:00:02.067372 22683591385216 run.py:483] Algo bellman_ford step 4848 current loss 0.051527, current_train_items 155168.
I0302 19:00:02.098599 22683591385216 run.py:483] Algo bellman_ford step 4849 current loss 0.075103, current_train_items 155200.
I0302 19:00:02.116595 22683591385216 run.py:483] Algo bellman_ford step 4850 current loss 0.001871, current_train_items 155232.
I0302 19:00:02.124605 22683591385216 run.py:503] (val) algo bellman_ford step 4850: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 155232, 'step': 4850, 'algorithm': 'bellman_ford'}
I0302 19:00:02.124712 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:02.140879 22683591385216 run.py:483] Algo bellman_ford step 4851 current loss 0.028253, current_train_items 155264.
I0302 19:00:02.162597 22683591385216 run.py:483] Algo bellman_ford step 4852 current loss 0.074929, current_train_items 155296.
I0302 19:00:02.191634 22683591385216 run.py:483] Algo bellman_ford step 4853 current loss 0.094683, current_train_items 155328.
I0302 19:00:02.223451 22683591385216 run.py:483] Algo bellman_ford step 4854 current loss 0.057834, current_train_items 155360.
I0302 19:00:02.241905 22683591385216 run.py:483] Algo bellman_ford step 4855 current loss 0.004745, current_train_items 155392.
I0302 19:00:02.257691 22683591385216 run.py:483] Algo bellman_ford step 4856 current loss 0.033224, current_train_items 155424.
I0302 19:00:02.281111 22683591385216 run.py:483] Algo bellman_ford step 4857 current loss 0.043461, current_train_items 155456.
I0302 19:00:02.311067 22683591385216 run.py:483] Algo bellman_ford step 4858 current loss 0.059301, current_train_items 155488.
I0302 19:00:02.344559 22683591385216 run.py:483] Algo bellman_ford step 4859 current loss 0.082101, current_train_items 155520.
I0302 19:00:02.362851 22683591385216 run.py:483] Algo bellman_ford step 4860 current loss 0.004022, current_train_items 155552.
I0302 19:00:02.379320 22683591385216 run.py:483] Algo bellman_ford step 4861 current loss 0.017694, current_train_items 155584.
I0302 19:00:02.401318 22683591385216 run.py:483] Algo bellman_ford step 4862 current loss 0.082273, current_train_items 155616.
I0302 19:00:02.431329 22683591385216 run.py:483] Algo bellman_ford step 4863 current loss 0.059018, current_train_items 155648.
I0302 19:00:02.461457 22683591385216 run.py:483] Algo bellman_ford step 4864 current loss 0.030413, current_train_items 155680.
I0302 19:00:02.479455 22683591385216 run.py:483] Algo bellman_ford step 4865 current loss 0.001876, current_train_items 155712.
I0302 19:00:02.494480 22683591385216 run.py:483] Algo bellman_ford step 4866 current loss 0.038287, current_train_items 155744.
I0302 19:00:02.517274 22683591385216 run.py:483] Algo bellman_ford step 4867 current loss 0.039670, current_train_items 155776.
I0302 19:00:02.547550 22683591385216 run.py:483] Algo bellman_ford step 4868 current loss 0.068682, current_train_items 155808.
I0302 19:00:02.577766 22683591385216 run.py:483] Algo bellman_ford step 4869 current loss 0.027610, current_train_items 155840.
I0302 19:00:02.596904 22683591385216 run.py:483] Algo bellman_ford step 4870 current loss 0.001182, current_train_items 155872.
I0302 19:00:02.612566 22683591385216 run.py:483] Algo bellman_ford step 4871 current loss 0.053686, current_train_items 155904.
I0302 19:00:02.634546 22683591385216 run.py:483] Algo bellman_ford step 4872 current loss 0.045095, current_train_items 155936.
I0302 19:00:02.661728 22683591385216 run.py:483] Algo bellman_ford step 4873 current loss 0.026621, current_train_items 155968.
I0302 19:00:02.691759 22683591385216 run.py:483] Algo bellman_ford step 4874 current loss 0.064529, current_train_items 156000.
I0302 19:00:02.710305 22683591385216 run.py:483] Algo bellman_ford step 4875 current loss 0.001610, current_train_items 156032.
I0302 19:00:02.725595 22683591385216 run.py:483] Algo bellman_ford step 4876 current loss 0.024162, current_train_items 156064.
I0302 19:00:02.747225 22683591385216 run.py:483] Algo bellman_ford step 4877 current loss 0.014681, current_train_items 156096.
I0302 19:00:02.775370 22683591385216 run.py:483] Algo bellman_ford step 4878 current loss 0.040580, current_train_items 156128.
I0302 19:00:02.806953 22683591385216 run.py:483] Algo bellman_ford step 4879 current loss 0.069612, current_train_items 156160.
I0302 19:00:02.825225 22683591385216 run.py:483] Algo bellman_ford step 4880 current loss 0.003520, current_train_items 156192.
I0302 19:00:02.840539 22683591385216 run.py:483] Algo bellman_ford step 4881 current loss 0.011468, current_train_items 156224.
I0302 19:00:02.863549 22683591385216 run.py:483] Algo bellman_ford step 4882 current loss 0.051712, current_train_items 156256.
I0302 19:00:02.893013 22683591385216 run.py:483] Algo bellman_ford step 4883 current loss 0.070119, current_train_items 156288.
I0302 19:00:02.925314 22683591385216 run.py:483] Algo bellman_ford step 4884 current loss 0.068857, current_train_items 156320.
I0302 19:00:02.943701 22683591385216 run.py:483] Algo bellman_ford step 4885 current loss 0.004995, current_train_items 156352.
I0302 19:00:02.959604 22683591385216 run.py:483] Algo bellman_ford step 4886 current loss 0.034985, current_train_items 156384.
I0302 19:00:02.981372 22683591385216 run.py:483] Algo bellman_ford step 4887 current loss 0.057546, current_train_items 156416.
I0302 19:00:03.010190 22683591385216 run.py:483] Algo bellman_ford step 4888 current loss 0.018208, current_train_items 156448.
I0302 19:00:03.041116 22683591385216 run.py:483] Algo bellman_ford step 4889 current loss 0.100934, current_train_items 156480.
I0302 19:00:03.059841 22683591385216 run.py:483] Algo bellman_ford step 4890 current loss 0.001615, current_train_items 156512.
I0302 19:00:03.075300 22683591385216 run.py:483] Algo bellman_ford step 4891 current loss 0.053054, current_train_items 156544.
I0302 19:00:03.097009 22683591385216 run.py:483] Algo bellman_ford step 4892 current loss 0.051119, current_train_items 156576.
I0302 19:00:03.124780 22683591385216 run.py:483] Algo bellman_ford step 4893 current loss 0.085885, current_train_items 156608.
I0302 19:00:03.155000 22683591385216 run.py:483] Algo bellman_ford step 4894 current loss 0.118886, current_train_items 156640.
I0302 19:00:03.173292 22683591385216 run.py:483] Algo bellman_ford step 4895 current loss 0.022375, current_train_items 156672.
I0302 19:00:03.188753 22683591385216 run.py:483] Algo bellman_ford step 4896 current loss 0.008392, current_train_items 156704.
I0302 19:00:03.210054 22683591385216 run.py:483] Algo bellman_ford step 4897 current loss 0.026017, current_train_items 156736.
I0302 19:00:03.238215 22683591385216 run.py:483] Algo bellman_ford step 4898 current loss 0.057513, current_train_items 156768.
I0302 19:00:03.270683 22683591385216 run.py:483] Algo bellman_ford step 4899 current loss 0.075502, current_train_items 156800.
I0302 19:00:03.289371 22683591385216 run.py:483] Algo bellman_ford step 4900 current loss 0.011178, current_train_items 156832.
I0302 19:00:03.296915 22683591385216 run.py:503] (val) algo bellman_ford step 4900: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 156832, 'step': 4900, 'algorithm': 'bellman_ford'}
I0302 19:00:03.297052 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:03.312561 22683591385216 run.py:483] Algo bellman_ford step 4901 current loss 0.026560, current_train_items 156864.
I0302 19:00:03.335525 22683591385216 run.py:483] Algo bellman_ford step 4902 current loss 0.027106, current_train_items 156896.
I0302 19:00:03.367076 22683591385216 run.py:483] Algo bellman_ford step 4903 current loss 0.033926, current_train_items 156928.
I0302 19:00:03.397990 22683591385216 run.py:483] Algo bellman_ford step 4904 current loss 0.052261, current_train_items 156960.
I0302 19:00:03.416394 22683591385216 run.py:483] Algo bellman_ford step 4905 current loss 0.011534, current_train_items 156992.
I0302 19:00:03.431531 22683591385216 run.py:483] Algo bellman_ford step 4906 current loss 0.033081, current_train_items 157024.
I0302 19:00:03.454265 22683591385216 run.py:483] Algo bellman_ford step 4907 current loss 0.038642, current_train_items 157056.
I0302 19:00:03.483768 22683591385216 run.py:483] Algo bellman_ford step 4908 current loss 0.037195, current_train_items 157088.
I0302 19:00:03.514275 22683591385216 run.py:483] Algo bellman_ford step 4909 current loss 0.058777, current_train_items 157120.
I0302 19:00:03.532369 22683591385216 run.py:483] Algo bellman_ford step 4910 current loss 0.002626, current_train_items 157152.
I0302 19:00:03.547872 22683591385216 run.py:483] Algo bellman_ford step 4911 current loss 0.021934, current_train_items 157184.
I0302 19:00:03.570051 22683591385216 run.py:483] Algo bellman_ford step 4912 current loss 0.021488, current_train_items 157216.
I0302 19:00:03.600131 22683591385216 run.py:483] Algo bellman_ford step 4913 current loss 0.055882, current_train_items 157248.
I0302 19:00:03.631181 22683591385216 run.py:483] Algo bellman_ford step 4914 current loss 0.066407, current_train_items 157280.
I0302 19:00:03.649163 22683591385216 run.py:483] Algo bellman_ford step 4915 current loss 0.002781, current_train_items 157312.
I0302 19:00:03.664151 22683591385216 run.py:483] Algo bellman_ford step 4916 current loss 0.024619, current_train_items 157344.
I0302 19:00:03.686379 22683591385216 run.py:483] Algo bellman_ford step 4917 current loss 0.061674, current_train_items 157376.
I0302 19:00:03.714631 22683591385216 run.py:483] Algo bellman_ford step 4918 current loss 0.042553, current_train_items 157408.
I0302 19:00:03.745751 22683591385216 run.py:483] Algo bellman_ford step 4919 current loss 0.055409, current_train_items 157440.
I0302 19:00:03.763684 22683591385216 run.py:483] Algo bellman_ford step 4920 current loss 0.001276, current_train_items 157472.
I0302 19:00:03.778320 22683591385216 run.py:483] Algo bellman_ford step 4921 current loss 0.006575, current_train_items 157504.
I0302 19:00:03.800828 22683591385216 run.py:483] Algo bellman_ford step 4922 current loss 0.066671, current_train_items 157536.
I0302 19:00:03.830184 22683591385216 run.py:483] Algo bellman_ford step 4923 current loss 0.050569, current_train_items 157568.
I0302 19:00:03.861180 22683591385216 run.py:483] Algo bellman_ford step 4924 current loss 0.081050, current_train_items 157600.
I0302 19:00:03.879448 22683591385216 run.py:483] Algo bellman_ford step 4925 current loss 0.005160, current_train_items 157632.
I0302 19:00:03.894815 22683591385216 run.py:483] Algo bellman_ford step 4926 current loss 0.012397, current_train_items 157664.
I0302 19:00:03.917778 22683591385216 run.py:483] Algo bellman_ford step 4927 current loss 0.041057, current_train_items 157696.
I0302 19:00:03.945823 22683591385216 run.py:483] Algo bellman_ford step 4928 current loss 0.089549, current_train_items 157728.
I0302 19:00:03.977345 22683591385216 run.py:483] Algo bellman_ford step 4929 current loss 0.070221, current_train_items 157760.
I0302 19:00:03.995253 22683591385216 run.py:483] Algo bellman_ford step 4930 current loss 0.003092, current_train_items 157792.
I0302 19:00:04.010187 22683591385216 run.py:483] Algo bellman_ford step 4931 current loss 0.016326, current_train_items 157824.
I0302 19:00:04.032286 22683591385216 run.py:483] Algo bellman_ford step 4932 current loss 0.047078, current_train_items 157856.
I0302 19:00:04.060638 22683591385216 run.py:483] Algo bellman_ford step 4933 current loss 0.088687, current_train_items 157888.
I0302 19:00:04.091767 22683591385216 run.py:483] Algo bellman_ford step 4934 current loss 0.088844, current_train_items 157920.
I0302 19:00:04.109808 22683591385216 run.py:483] Algo bellman_ford step 4935 current loss 0.008210, current_train_items 157952.
I0302 19:00:04.125103 22683591385216 run.py:483] Algo bellman_ford step 4936 current loss 0.011812, current_train_items 157984.
I0302 19:00:04.148675 22683591385216 run.py:483] Algo bellman_ford step 4937 current loss 0.034066, current_train_items 158016.
I0302 19:00:04.177557 22683591385216 run.py:483] Algo bellman_ford step 4938 current loss 0.031522, current_train_items 158048.
I0302 19:00:04.210586 22683591385216 run.py:483] Algo bellman_ford step 4939 current loss 0.066724, current_train_items 158080.
I0302 19:00:04.228531 22683591385216 run.py:483] Algo bellman_ford step 4940 current loss 0.002498, current_train_items 158112.
I0302 19:00:04.243599 22683591385216 run.py:483] Algo bellman_ford step 4941 current loss 0.006967, current_train_items 158144.
I0302 19:00:04.265560 22683591385216 run.py:483] Algo bellman_ford step 4942 current loss 0.032955, current_train_items 158176.
I0302 19:00:04.295508 22683591385216 run.py:483] Algo bellman_ford step 4943 current loss 0.067632, current_train_items 158208.
I0302 19:00:04.325224 22683591385216 run.py:483] Algo bellman_ford step 4944 current loss 0.026904, current_train_items 158240.
I0302 19:00:04.343282 22683591385216 run.py:483] Algo bellman_ford step 4945 current loss 0.002701, current_train_items 158272.
I0302 19:00:04.358664 22683591385216 run.py:483] Algo bellman_ford step 4946 current loss 0.025014, current_train_items 158304.
I0302 19:00:04.380550 22683591385216 run.py:483] Algo bellman_ford step 4947 current loss 0.043219, current_train_items 158336.
I0302 19:00:04.409568 22683591385216 run.py:483] Algo bellman_ford step 4948 current loss 0.039313, current_train_items 158368.
I0302 19:00:04.442469 22683591385216 run.py:483] Algo bellman_ford step 4949 current loss 0.047560, current_train_items 158400.
I0302 19:00:04.460618 22683591385216 run.py:483] Algo bellman_ford step 4950 current loss 0.001885, current_train_items 158432.
I0302 19:00:04.468430 22683591385216 run.py:503] (val) algo bellman_ford step 4950: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 158432, 'step': 4950, 'algorithm': 'bellman_ford'}
I0302 19:00:04.468538 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:00:04.484163 22683591385216 run.py:483] Algo bellman_ford step 4951 current loss 0.004488, current_train_items 158464.
I0302 19:00:04.506490 22683591385216 run.py:483] Algo bellman_ford step 4952 current loss 0.024711, current_train_items 158496.
I0302 19:00:04.536308 22683591385216 run.py:483] Algo bellman_ford step 4953 current loss 0.093552, current_train_items 158528.
I0302 19:00:04.570102 22683591385216 run.py:483] Algo bellman_ford step 4954 current loss 0.043749, current_train_items 158560.
I0302 19:00:04.588692 22683591385216 run.py:483] Algo bellman_ford step 4955 current loss 0.004938, current_train_items 158592.
I0302 19:00:04.604171 22683591385216 run.py:483] Algo bellman_ford step 4956 current loss 0.009432, current_train_items 158624.
I0302 19:00:04.627512 22683591385216 run.py:483] Algo bellman_ford step 4957 current loss 0.058855, current_train_items 158656.
I0302 19:00:04.657286 22683591385216 run.py:483] Algo bellman_ford step 4958 current loss 0.121813, current_train_items 158688.
I0302 19:00:04.686362 22683591385216 run.py:483] Algo bellman_ford step 4959 current loss 0.078666, current_train_items 158720.
I0302 19:00:04.704890 22683591385216 run.py:483] Algo bellman_ford step 4960 current loss 0.010856, current_train_items 158752.
I0302 19:00:04.720710 22683591385216 run.py:483] Algo bellman_ford step 4961 current loss 0.010394, current_train_items 158784.
I0302 19:00:04.742845 22683591385216 run.py:483] Algo bellman_ford step 4962 current loss 0.034417, current_train_items 158816.
I0302 19:00:04.770955 22683591385216 run.py:483] Algo bellman_ford step 4963 current loss 0.043658, current_train_items 158848.
I0302 19:00:04.802004 22683591385216 run.py:483] Algo bellman_ford step 4964 current loss 0.064187, current_train_items 158880.
I0302 19:00:04.820203 22683591385216 run.py:483] Algo bellman_ford step 4965 current loss 0.002544, current_train_items 158912.
I0302 19:00:04.835509 22683591385216 run.py:483] Algo bellman_ford step 4966 current loss 0.015966, current_train_items 158944.
I0302 19:00:04.858604 22683591385216 run.py:483] Algo bellman_ford step 4967 current loss 0.033115, current_train_items 158976.
I0302 19:00:04.888933 22683591385216 run.py:483] Algo bellman_ford step 4968 current loss 0.066310, current_train_items 159008.
I0302 19:00:04.918501 22683591385216 run.py:483] Algo bellman_ford step 4969 current loss 0.049747, current_train_items 159040.
I0302 19:00:04.937413 22683591385216 run.py:483] Algo bellman_ford step 4970 current loss 0.001935, current_train_items 159072.
I0302 19:00:04.952927 22683591385216 run.py:483] Algo bellman_ford step 4971 current loss 0.051415, current_train_items 159104.
I0302 19:00:04.975352 22683591385216 run.py:483] Algo bellman_ford step 4972 current loss 0.031606, current_train_items 159136.
I0302 19:00:05.005065 22683591385216 run.py:483] Algo bellman_ford step 4973 current loss 0.041932, current_train_items 159168.
I0302 19:00:05.036080 22683591385216 run.py:483] Algo bellman_ford step 4974 current loss 0.045411, current_train_items 159200.
I0302 19:00:05.054849 22683591385216 run.py:483] Algo bellman_ford step 4975 current loss 0.001566, current_train_items 159232.
I0302 19:00:05.070835 22683591385216 run.py:483] Algo bellman_ford step 4976 current loss 0.012255, current_train_items 159264.
I0302 19:00:05.093966 22683591385216 run.py:483] Algo bellman_ford step 4977 current loss 0.057803, current_train_items 159296.
I0302 19:00:05.123819 22683591385216 run.py:483] Algo bellman_ford step 4978 current loss 0.064556, current_train_items 159328.
I0302 19:00:05.156455 22683591385216 run.py:483] Algo bellman_ford step 4979 current loss 0.035545, current_train_items 159360.
I0302 19:00:05.174656 22683591385216 run.py:483] Algo bellman_ford step 4980 current loss 0.001158, current_train_items 159392.
I0302 19:00:05.190039 22683591385216 run.py:483] Algo bellman_ford step 4981 current loss 0.007156, current_train_items 159424.
I0302 19:00:05.213479 22683591385216 run.py:483] Algo bellman_ford step 4982 current loss 0.025508, current_train_items 159456.
I0302 19:00:05.240997 22683591385216 run.py:483] Algo bellman_ford step 4983 current loss 0.042106, current_train_items 159488.
I0302 19:00:05.272227 22683591385216 run.py:483] Algo bellman_ford step 4984 current loss 0.041064, current_train_items 159520.
I0302 19:00:05.291047 22683591385216 run.py:483] Algo bellman_ford step 4985 current loss 0.000733, current_train_items 159552.
I0302 19:00:05.306745 22683591385216 run.py:483] Algo bellman_ford step 4986 current loss 0.023483, current_train_items 159584.
I0302 19:00:05.327684 22683591385216 run.py:483] Algo bellman_ford step 4987 current loss 0.028990, current_train_items 159616.
I0302 19:00:05.356172 22683591385216 run.py:483] Algo bellman_ford step 4988 current loss 0.030980, current_train_items 159648.
I0302 19:00:05.390342 22683591385216 run.py:483] Algo bellman_ford step 4989 current loss 0.065400, current_train_items 159680.
I0302 19:00:05.408917 22683591385216 run.py:483] Algo bellman_ford step 4990 current loss 0.001941, current_train_items 159712.
I0302 19:00:05.424207 22683591385216 run.py:483] Algo bellman_ford step 4991 current loss 0.016272, current_train_items 159744.
I0302 19:00:05.446425 22683591385216 run.py:483] Algo bellman_ford step 4992 current loss 0.043992, current_train_items 159776.
I0302 19:00:05.475272 22683591385216 run.py:483] Algo bellman_ford step 4993 current loss 0.049081, current_train_items 159808.
I0302 19:00:05.506719 22683591385216 run.py:483] Algo bellman_ford step 4994 current loss 0.066652, current_train_items 159840.
I0302 19:00:05.524927 22683591385216 run.py:483] Algo bellman_ford step 4995 current loss 0.000729, current_train_items 159872.
I0302 19:00:05.540510 22683591385216 run.py:483] Algo bellman_ford step 4996 current loss 0.007953, current_train_items 159904.
I0302 19:00:05.562929 22683591385216 run.py:483] Algo bellman_ford step 4997 current loss 0.035708, current_train_items 159936.
I0302 19:00:05.592678 22683591385216 run.py:483] Algo bellman_ford step 4998 current loss 0.063235, current_train_items 159968.
I0302 19:00:05.625073 22683591385216 run.py:483] Algo bellman_ford step 4999 current loss 0.056318, current_train_items 160000.
I0302 19:00:05.643803 22683591385216 run.py:483] Algo bellman_ford step 5000 current loss 0.004046, current_train_items 160032.
I0302 19:00:05.651479 22683591385216 run.py:503] (val) algo bellman_ford step 5000: {'pi': 0.9716796875, 'score': 0.9716796875, 'examples_seen': 160032, 'step': 5000, 'algorithm': 'bellman_ford'}
I0302 19:00:05.651585 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.972, val scores are: bellman_ford: 0.972
I0302 19:00:05.667686 22683591385216 run.py:483] Algo bellman_ford step 5001 current loss 0.033790, current_train_items 160064.
I0302 19:00:05.690035 22683591385216 run.py:483] Algo bellman_ford step 5002 current loss 0.133497, current_train_items 160096.
I0302 19:00:05.718048 22683591385216 run.py:483] Algo bellman_ford step 5003 current loss 0.016460, current_train_items 160128.
I0302 19:00:05.748709 22683591385216 run.py:483] Algo bellman_ford step 5004 current loss 0.066917, current_train_items 160160.
I0302 19:00:05.767126 22683591385216 run.py:483] Algo bellman_ford step 5005 current loss 0.000762, current_train_items 160192.
I0302 19:00:05.782500 22683591385216 run.py:483] Algo bellman_ford step 5006 current loss 0.020741, current_train_items 160224.
I0302 19:00:05.805387 22683591385216 run.py:483] Algo bellman_ford step 5007 current loss 0.095135, current_train_items 160256.
I0302 19:00:05.836523 22683591385216 run.py:483] Algo bellman_ford step 5008 current loss 0.112017, current_train_items 160288.
I0302 19:00:05.866002 22683591385216 run.py:483] Algo bellman_ford step 5009 current loss 0.037030, current_train_items 160320.
I0302 19:00:05.884006 22683591385216 run.py:483] Algo bellman_ford step 5010 current loss 0.009369, current_train_items 160352.
I0302 19:00:05.899037 22683591385216 run.py:483] Algo bellman_ford step 5011 current loss 0.018659, current_train_items 160384.
I0302 19:00:05.921603 22683591385216 run.py:483] Algo bellman_ford step 5012 current loss 0.022965, current_train_items 160416.
I0302 19:00:05.950247 22683591385216 run.py:483] Algo bellman_ford step 5013 current loss 0.055253, current_train_items 160448.
I0302 19:00:05.982806 22683591385216 run.py:483] Algo bellman_ford step 5014 current loss 0.114010, current_train_items 160480.
I0302 19:00:06.000966 22683591385216 run.py:483] Algo bellman_ford step 5015 current loss 0.003527, current_train_items 160512.
I0302 19:00:06.016979 22683591385216 run.py:483] Algo bellman_ford step 5016 current loss 0.047252, current_train_items 160544.
I0302 19:00:06.038777 22683591385216 run.py:483] Algo bellman_ford step 5017 current loss 0.031049, current_train_items 160576.
I0302 19:00:06.068063 22683591385216 run.py:483] Algo bellman_ford step 5018 current loss 0.069794, current_train_items 160608.
I0302 19:00:06.101097 22683591385216 run.py:483] Algo bellman_ford step 5019 current loss 0.166066, current_train_items 160640.
I0302 19:00:06.119050 22683591385216 run.py:483] Algo bellman_ford step 5020 current loss 0.001208, current_train_items 160672.
I0302 19:00:06.134185 22683591385216 run.py:483] Algo bellman_ford step 5021 current loss 0.018589, current_train_items 160704.
I0302 19:00:06.157390 22683591385216 run.py:483] Algo bellman_ford step 5022 current loss 0.036881, current_train_items 160736.
I0302 19:00:06.186697 22683591385216 run.py:483] Algo bellman_ford step 5023 current loss 0.059817, current_train_items 160768.
I0302 19:00:06.218345 22683591385216 run.py:483] Algo bellman_ford step 5024 current loss 0.041537, current_train_items 160800.
I0302 19:00:06.236214 22683591385216 run.py:483] Algo bellman_ford step 5025 current loss 0.001662, current_train_items 160832.
I0302 19:00:06.251021 22683591385216 run.py:483] Algo bellman_ford step 5026 current loss 0.023343, current_train_items 160864.
I0302 19:00:06.272476 22683591385216 run.py:483] Algo bellman_ford step 5027 current loss 0.074517, current_train_items 160896.
I0302 19:00:06.300106 22683591385216 run.py:483] Algo bellman_ford step 5028 current loss 0.066331, current_train_items 160928.
I0302 19:00:06.331042 22683591385216 run.py:483] Algo bellman_ford step 5029 current loss 0.083612, current_train_items 160960.
I0302 19:00:06.349416 22683591385216 run.py:483] Algo bellman_ford step 5030 current loss 0.012304, current_train_items 160992.
I0302 19:00:06.364716 22683591385216 run.py:483] Algo bellman_ford step 5031 current loss 0.021100, current_train_items 161024.
I0302 19:00:06.387398 22683591385216 run.py:483] Algo bellman_ford step 5032 current loss 0.039630, current_train_items 161056.
I0302 19:00:06.415344 22683591385216 run.py:483] Algo bellman_ford step 5033 current loss 0.038919, current_train_items 161088.
I0302 19:00:06.447291 22683591385216 run.py:483] Algo bellman_ford step 5034 current loss 0.142036, current_train_items 161120.
I0302 19:00:06.465822 22683591385216 run.py:483] Algo bellman_ford step 5035 current loss 0.007708, current_train_items 161152.
I0302 19:00:06.481256 22683591385216 run.py:483] Algo bellman_ford step 5036 current loss 0.007301, current_train_items 161184.
I0302 19:00:06.503550 22683591385216 run.py:483] Algo bellman_ford step 5037 current loss 0.050957, current_train_items 161216.
I0302 19:00:06.533824 22683591385216 run.py:483] Algo bellman_ford step 5038 current loss 0.124854, current_train_items 161248.
I0302 19:00:06.564979 22683591385216 run.py:483] Algo bellman_ford step 5039 current loss 0.047577, current_train_items 161280.
I0302 19:00:06.583124 22683591385216 run.py:483] Algo bellman_ford step 5040 current loss 0.003155, current_train_items 161312.
I0302 19:00:06.597814 22683591385216 run.py:483] Algo bellman_ford step 5041 current loss 0.005304, current_train_items 161344.
I0302 19:00:06.621071 22683591385216 run.py:483] Algo bellman_ford step 5042 current loss 0.044610, current_train_items 161376.
I0302 19:00:06.649044 22683591385216 run.py:483] Algo bellman_ford step 5043 current loss 0.040027, current_train_items 161408.
I0302 19:00:06.679948 22683591385216 run.py:483] Algo bellman_ford step 5044 current loss 0.075088, current_train_items 161440.
I0302 19:00:06.697885 22683591385216 run.py:483] Algo bellman_ford step 5045 current loss 0.001619, current_train_items 161472.
I0302 19:00:06.713132 22683591385216 run.py:483] Algo bellman_ford step 5046 current loss 0.041662, current_train_items 161504.
I0302 19:00:06.734320 22683591385216 run.py:483] Algo bellman_ford step 5047 current loss 0.031005, current_train_items 161536.
I0302 19:00:06.763306 22683591385216 run.py:483] Algo bellman_ford step 5048 current loss 0.078433, current_train_items 161568.
I0302 19:00:06.794691 22683591385216 run.py:483] Algo bellman_ford step 5049 current loss 0.147018, current_train_items 161600.
I0302 19:00:06.812501 22683591385216 run.py:483] Algo bellman_ford step 5050 current loss 0.001575, current_train_items 161632.
I0302 19:00:06.820411 22683591385216 run.py:503] (val) algo bellman_ford step 5050: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 161632, 'step': 5050, 'algorithm': 'bellman_ford'}
I0302 19:00:06.820517 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:06.836248 22683591385216 run.py:483] Algo bellman_ford step 5051 current loss 0.013193, current_train_items 161664.
I0302 19:00:06.859813 22683591385216 run.py:483] Algo bellman_ford step 5052 current loss 0.048597, current_train_items 161696.
I0302 19:00:06.889685 22683591385216 run.py:483] Algo bellman_ford step 5053 current loss 0.031472, current_train_items 161728.
I0302 19:00:06.920011 22683591385216 run.py:483] Algo bellman_ford step 5054 current loss 0.048996, current_train_items 161760.
I0302 19:00:06.938668 22683591385216 run.py:483] Algo bellman_ford step 5055 current loss 0.001374, current_train_items 161792.
I0302 19:00:06.954887 22683591385216 run.py:483] Algo bellman_ford step 5056 current loss 0.021664, current_train_items 161824.
I0302 19:00:06.978393 22683591385216 run.py:483] Algo bellman_ford step 5057 current loss 0.027179, current_train_items 161856.
I0302 19:00:07.007016 22683591385216 run.py:483] Algo bellman_ford step 5058 current loss 0.050471, current_train_items 161888.
I0302 19:00:07.038582 22683591385216 run.py:483] Algo bellman_ford step 5059 current loss 0.038879, current_train_items 161920.
I0302 19:00:07.057001 22683591385216 run.py:483] Algo bellman_ford step 5060 current loss 0.030951, current_train_items 161952.
I0302 19:00:07.072495 22683591385216 run.py:483] Algo bellman_ford step 5061 current loss 0.021108, current_train_items 161984.
I0302 19:00:07.092952 22683591385216 run.py:483] Algo bellman_ford step 5062 current loss 0.015663, current_train_items 162016.
I0302 19:00:07.121757 22683591385216 run.py:483] Algo bellman_ford step 5063 current loss 0.099320, current_train_items 162048.
I0302 19:00:07.151124 22683591385216 run.py:483] Algo bellman_ford step 5064 current loss 0.096606, current_train_items 162080.
I0302 19:00:07.169026 22683591385216 run.py:483] Algo bellman_ford step 5065 current loss 0.008022, current_train_items 162112.
I0302 19:00:07.184223 22683591385216 run.py:483] Algo bellman_ford step 5066 current loss 0.005516, current_train_items 162144.
I0302 19:00:07.207327 22683591385216 run.py:483] Algo bellman_ford step 5067 current loss 0.043850, current_train_items 162176.
I0302 19:00:07.236152 22683591385216 run.py:483] Algo bellman_ford step 5068 current loss 0.079699, current_train_items 162208.
I0302 19:00:07.268342 22683591385216 run.py:483] Algo bellman_ford step 5069 current loss 0.116291, current_train_items 162240.
I0302 19:00:07.286799 22683591385216 run.py:483] Algo bellman_ford step 5070 current loss 0.002864, current_train_items 162272.
I0302 19:00:07.302847 22683591385216 run.py:483] Algo bellman_ford step 5071 current loss 0.014101, current_train_items 162304.
I0302 19:00:07.323415 22683591385216 run.py:483] Algo bellman_ford step 5072 current loss 0.043398, current_train_items 162336.
I0302 19:00:07.353233 22683591385216 run.py:483] Algo bellman_ford step 5073 current loss 0.086290, current_train_items 162368.
I0302 19:00:07.381799 22683591385216 run.py:483] Algo bellman_ford step 5074 current loss 0.039976, current_train_items 162400.
I0302 19:00:07.400141 22683591385216 run.py:483] Algo bellman_ford step 5075 current loss 0.002742, current_train_items 162432.
I0302 19:00:07.415605 22683591385216 run.py:483] Algo bellman_ford step 5076 current loss 0.026761, current_train_items 162464.
I0302 19:00:07.437348 22683591385216 run.py:483] Algo bellman_ford step 5077 current loss 0.022900, current_train_items 162496.
I0302 19:00:07.466190 22683591385216 run.py:483] Algo bellman_ford step 5078 current loss 0.085927, current_train_items 162528.
I0302 19:00:07.496156 22683591385216 run.py:483] Algo bellman_ford step 5079 current loss 0.084496, current_train_items 162560.
I0302 19:00:07.514633 22683591385216 run.py:483] Algo bellman_ford step 5080 current loss 0.001479, current_train_items 162592.
I0302 19:00:07.529592 22683591385216 run.py:483] Algo bellman_ford step 5081 current loss 0.016486, current_train_items 162624.
I0302 19:00:07.551557 22683591385216 run.py:483] Algo bellman_ford step 5082 current loss 0.015805, current_train_items 162656.
I0302 19:00:07.580937 22683591385216 run.py:483] Algo bellman_ford step 5083 current loss 0.051907, current_train_items 162688.
I0302 19:00:07.611993 22683591385216 run.py:483] Algo bellman_ford step 5084 current loss 0.047675, current_train_items 162720.
I0302 19:00:07.630571 22683591385216 run.py:483] Algo bellman_ford step 5085 current loss 0.001363, current_train_items 162752.
I0302 19:00:07.646002 22683591385216 run.py:483] Algo bellman_ford step 5086 current loss 0.020888, current_train_items 162784.
I0302 19:00:07.668342 22683591385216 run.py:483] Algo bellman_ford step 5087 current loss 0.048647, current_train_items 162816.
I0302 19:00:07.697372 22683591385216 run.py:483] Algo bellman_ford step 5088 current loss 0.045554, current_train_items 162848.
I0302 19:00:07.729371 22683591385216 run.py:483] Algo bellman_ford step 5089 current loss 0.047453, current_train_items 162880.
I0302 19:00:07.747555 22683591385216 run.py:483] Algo bellman_ford step 5090 current loss 0.000725, current_train_items 162912.
I0302 19:00:07.763283 22683591385216 run.py:483] Algo bellman_ford step 5091 current loss 0.028670, current_train_items 162944.
I0302 19:00:07.783744 22683591385216 run.py:483] Algo bellman_ford step 5092 current loss 0.018267, current_train_items 162976.
I0302 19:00:07.811702 22683591385216 run.py:483] Algo bellman_ford step 5093 current loss 0.044040, current_train_items 163008.
I0302 19:00:07.841609 22683591385216 run.py:483] Algo bellman_ford step 5094 current loss 0.060717, current_train_items 163040.
I0302 19:00:07.859455 22683591385216 run.py:483] Algo bellman_ford step 5095 current loss 0.000900, current_train_items 163072.
I0302 19:00:07.874730 22683591385216 run.py:483] Algo bellman_ford step 5096 current loss 0.008663, current_train_items 163104.
I0302 19:00:07.897762 22683591385216 run.py:483] Algo bellman_ford step 5097 current loss 0.045656, current_train_items 163136.
I0302 19:00:07.927618 22683591385216 run.py:483] Algo bellman_ford step 5098 current loss 0.051255, current_train_items 163168.
I0302 19:00:07.960038 22683591385216 run.py:483] Algo bellman_ford step 5099 current loss 0.077812, current_train_items 163200.
I0302 19:00:07.978545 22683591385216 run.py:483] Algo bellman_ford step 5100 current loss 0.000946, current_train_items 163232.
I0302 19:00:07.986079 22683591385216 run.py:503] (val) algo bellman_ford step 5100: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 163232, 'step': 5100, 'algorithm': 'bellman_ford'}
I0302 19:00:07.986183 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:00:08.001823 22683591385216 run.py:483] Algo bellman_ford step 5101 current loss 0.052813, current_train_items 163264.
I0302 19:00:08.022782 22683591385216 run.py:483] Algo bellman_ford step 5102 current loss 0.020510, current_train_items 163296.
I0302 19:00:08.052375 22683591385216 run.py:483] Algo bellman_ford step 5103 current loss 0.045118, current_train_items 163328.
I0302 19:00:08.084585 22683591385216 run.py:483] Algo bellman_ford step 5104 current loss 0.058318, current_train_items 163360.
I0302 19:00:08.102850 22683591385216 run.py:483] Algo bellman_ford step 5105 current loss 0.001275, current_train_items 163392.
I0302 19:00:08.118692 22683591385216 run.py:483] Algo bellman_ford step 5106 current loss 0.025008, current_train_items 163424.
I0302 19:00:08.140868 22683591385216 run.py:483] Algo bellman_ford step 5107 current loss 0.057383, current_train_items 163456.
I0302 19:00:08.168635 22683591385216 run.py:483] Algo bellman_ford step 5108 current loss 0.027780, current_train_items 163488.
I0302 19:00:08.200627 22683591385216 run.py:483] Algo bellman_ford step 5109 current loss 0.065312, current_train_items 163520.
I0302 19:00:08.218760 22683591385216 run.py:483] Algo bellman_ford step 5110 current loss 0.001556, current_train_items 163552.
I0302 19:00:08.234077 22683591385216 run.py:483] Algo bellman_ford step 5111 current loss 0.015408, current_train_items 163584.
I0302 19:00:08.255668 22683591385216 run.py:483] Algo bellman_ford step 5112 current loss 0.028418, current_train_items 163616.
I0302 19:00:08.283854 22683591385216 run.py:483] Algo bellman_ford step 5113 current loss 0.033243, current_train_items 163648.
I0302 19:00:08.314442 22683591385216 run.py:483] Algo bellman_ford step 5114 current loss 0.048080, current_train_items 163680.
I0302 19:00:08.332367 22683591385216 run.py:483] Algo bellman_ford step 5115 current loss 0.001144, current_train_items 163712.
I0302 19:00:08.347861 22683591385216 run.py:483] Algo bellman_ford step 5116 current loss 0.017466, current_train_items 163744.
I0302 19:00:08.370595 22683591385216 run.py:483] Algo bellman_ford step 5117 current loss 0.050638, current_train_items 163776.
I0302 19:00:08.398682 22683591385216 run.py:483] Algo bellman_ford step 5118 current loss 0.028911, current_train_items 163808.
I0302 19:00:08.430443 22683591385216 run.py:483] Algo bellman_ford step 5119 current loss 0.037429, current_train_items 163840.
I0302 19:00:08.448580 22683591385216 run.py:483] Algo bellman_ford step 5120 current loss 0.001583, current_train_items 163872.
I0302 19:00:08.463523 22683591385216 run.py:483] Algo bellman_ford step 5121 current loss 0.010040, current_train_items 163904.
I0302 19:00:08.485321 22683591385216 run.py:483] Algo bellman_ford step 5122 current loss 0.039013, current_train_items 163936.
I0302 19:00:08.513131 22683591385216 run.py:483] Algo bellman_ford step 5123 current loss 0.036473, current_train_items 163968.
I0302 19:00:08.544376 22683591385216 run.py:483] Algo bellman_ford step 5124 current loss 0.084567, current_train_items 164000.
I0302 19:00:08.562271 22683591385216 run.py:483] Algo bellman_ford step 5125 current loss 0.002319, current_train_items 164032.
I0302 19:00:08.577378 22683591385216 run.py:483] Algo bellman_ford step 5126 current loss 0.035174, current_train_items 164064.
I0302 19:00:08.599557 22683591385216 run.py:483] Algo bellman_ford step 5127 current loss 0.047109, current_train_items 164096.
I0302 19:00:08.628558 22683591385216 run.py:483] Algo bellman_ford step 5128 current loss 0.031289, current_train_items 164128.
I0302 19:00:08.660917 22683591385216 run.py:483] Algo bellman_ford step 5129 current loss 0.065022, current_train_items 164160.
I0302 19:00:08.678700 22683591385216 run.py:483] Algo bellman_ford step 5130 current loss 0.002441, current_train_items 164192.
I0302 19:00:08.694169 22683591385216 run.py:483] Algo bellman_ford step 5131 current loss 0.022089, current_train_items 164224.
I0302 19:00:08.715240 22683591385216 run.py:483] Algo bellman_ford step 5132 current loss 0.102260, current_train_items 164256.
I0302 19:00:08.745013 22683591385216 run.py:483] Algo bellman_ford step 5133 current loss 0.042220, current_train_items 164288.
I0302 19:00:08.775375 22683591385216 run.py:483] Algo bellman_ford step 5134 current loss 0.041169, current_train_items 164320.
I0302 19:00:08.793528 22683591385216 run.py:483] Algo bellman_ford step 5135 current loss 0.004300, current_train_items 164352.
I0302 19:00:08.808699 22683591385216 run.py:483] Algo bellman_ford step 5136 current loss 0.008976, current_train_items 164384.
I0302 19:00:08.831031 22683591385216 run.py:483] Algo bellman_ford step 5137 current loss 0.045064, current_train_items 164416.
I0302 19:00:08.859664 22683591385216 run.py:483] Algo bellman_ford step 5138 current loss 0.036233, current_train_items 164448.
I0302 19:00:08.891492 22683591385216 run.py:483] Algo bellman_ford step 5139 current loss 0.030035, current_train_items 164480.
I0302 19:00:08.909447 22683591385216 run.py:483] Algo bellman_ford step 5140 current loss 0.000894, current_train_items 164512.
I0302 19:00:08.924964 22683591385216 run.py:483] Algo bellman_ford step 5141 current loss 0.018467, current_train_items 164544.
I0302 19:00:08.947685 22683591385216 run.py:483] Algo bellman_ford step 5142 current loss 0.040744, current_train_items 164576.
I0302 19:00:08.976804 22683591385216 run.py:483] Algo bellman_ford step 5143 current loss 0.049117, current_train_items 164608.
I0302 19:00:09.007761 22683591385216 run.py:483] Algo bellman_ford step 5144 current loss 0.057272, current_train_items 164640.
I0302 19:00:09.025619 22683591385216 run.py:483] Algo bellman_ford step 5145 current loss 0.001782, current_train_items 164672.
I0302 19:00:09.040694 22683591385216 run.py:483] Algo bellman_ford step 5146 current loss 0.018168, current_train_items 164704.
I0302 19:00:09.061355 22683591385216 run.py:483] Algo bellman_ford step 5147 current loss 0.029152, current_train_items 164736.
I0302 19:00:09.089339 22683591385216 run.py:483] Algo bellman_ford step 5148 current loss 0.058506, current_train_items 164768.
I0302 19:00:09.119712 22683591385216 run.py:483] Algo bellman_ford step 5149 current loss 0.079387, current_train_items 164800.
I0302 19:00:09.137594 22683591385216 run.py:483] Algo bellman_ford step 5150 current loss 0.003277, current_train_items 164832.
I0302 19:00:09.145473 22683591385216 run.py:503] (val) algo bellman_ford step 5150: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 164832, 'step': 5150, 'algorithm': 'bellman_ford'}
I0302 19:00:09.145581 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:09.161573 22683591385216 run.py:483] Algo bellman_ford step 5151 current loss 0.050924, current_train_items 164864.
I0302 19:00:09.183784 22683591385216 run.py:483] Algo bellman_ford step 5152 current loss 0.015133, current_train_items 164896.
I0302 19:00:09.211863 22683591385216 run.py:483] Algo bellman_ford step 5153 current loss 0.066644, current_train_items 164928.
I0302 19:00:09.242304 22683591385216 run.py:483] Algo bellman_ford step 5154 current loss 0.090148, current_train_items 164960.
I0302 19:00:09.260950 22683591385216 run.py:483] Algo bellman_ford step 5155 current loss 0.002974, current_train_items 164992.
I0302 19:00:09.276153 22683591385216 run.py:483] Algo bellman_ford step 5156 current loss 0.004696, current_train_items 165024.
I0302 19:00:09.299190 22683591385216 run.py:483] Algo bellman_ford step 5157 current loss 0.024960, current_train_items 165056.
I0302 19:00:09.329566 22683591385216 run.py:483] Algo bellman_ford step 5158 current loss 0.122294, current_train_items 165088.
I0302 19:00:09.359683 22683591385216 run.py:483] Algo bellman_ford step 5159 current loss 0.059790, current_train_items 165120.
I0302 19:00:09.378143 22683591385216 run.py:483] Algo bellman_ford step 5160 current loss 0.001601, current_train_items 165152.
I0302 19:00:09.393550 22683591385216 run.py:483] Algo bellman_ford step 5161 current loss 0.010404, current_train_items 165184.
I0302 19:00:09.416440 22683591385216 run.py:483] Algo bellman_ford step 5162 current loss 0.053453, current_train_items 165216.
I0302 19:00:09.446992 22683591385216 run.py:483] Algo bellman_ford step 5163 current loss 0.040143, current_train_items 165248.
I0302 19:00:09.476792 22683591385216 run.py:483] Algo bellman_ford step 5164 current loss 0.058372, current_train_items 165280.
I0302 19:00:09.494501 22683591385216 run.py:483] Algo bellman_ford step 5165 current loss 0.002141, current_train_items 165312.
I0302 19:00:09.509644 22683591385216 run.py:483] Algo bellman_ford step 5166 current loss 0.007016, current_train_items 165344.
I0302 19:00:09.530654 22683591385216 run.py:483] Algo bellman_ford step 5167 current loss 0.057686, current_train_items 165376.
I0302 19:00:09.559931 22683591385216 run.py:483] Algo bellman_ford step 5168 current loss 0.040704, current_train_items 165408.
I0302 19:00:09.592201 22683591385216 run.py:483] Algo bellman_ford step 5169 current loss 0.047238, current_train_items 165440.
I0302 19:00:09.610728 22683591385216 run.py:483] Algo bellman_ford step 5170 current loss 0.003535, current_train_items 165472.
I0302 19:00:09.626326 22683591385216 run.py:483] Algo bellman_ford step 5171 current loss 0.008157, current_train_items 165504.
I0302 19:00:09.648484 22683591385216 run.py:483] Algo bellman_ford step 5172 current loss 0.085591, current_train_items 165536.
I0302 19:00:09.678238 22683591385216 run.py:483] Algo bellman_ford step 5173 current loss 0.080406, current_train_items 165568.
I0302 19:00:09.708483 22683591385216 run.py:483] Algo bellman_ford step 5174 current loss 0.061720, current_train_items 165600.
I0302 19:00:09.726985 22683591385216 run.py:483] Algo bellman_ford step 5175 current loss 0.001714, current_train_items 165632.
I0302 19:00:09.742645 22683591385216 run.py:483] Algo bellman_ford step 5176 current loss 0.028333, current_train_items 165664.
I0302 19:00:09.764939 22683591385216 run.py:483] Algo bellman_ford step 5177 current loss 0.106079, current_train_items 165696.
I0302 19:00:09.793598 22683591385216 run.py:483] Algo bellman_ford step 5178 current loss 0.110418, current_train_items 165728.
I0302 19:00:09.823543 22683591385216 run.py:483] Algo bellman_ford step 5179 current loss 0.064776, current_train_items 165760.
I0302 19:00:09.841481 22683591385216 run.py:483] Algo bellman_ford step 5180 current loss 0.002884, current_train_items 165792.
I0302 19:00:09.856756 22683591385216 run.py:483] Algo bellman_ford step 5181 current loss 0.017604, current_train_items 165824.
I0302 19:00:09.878793 22683591385216 run.py:483] Algo bellman_ford step 5182 current loss 0.066328, current_train_items 165856.
I0302 19:00:09.907580 22683591385216 run.py:483] Algo bellman_ford step 5183 current loss 0.118779, current_train_items 165888.
I0302 19:00:09.937199 22683591385216 run.py:483] Algo bellman_ford step 5184 current loss 0.083166, current_train_items 165920.
I0302 19:00:09.955588 22683591385216 run.py:483] Algo bellman_ford step 5185 current loss 0.038114, current_train_items 165952.
I0302 19:00:09.971536 22683591385216 run.py:483] Algo bellman_ford step 5186 current loss 0.024851, current_train_items 165984.
I0302 19:00:09.992356 22683591385216 run.py:483] Algo bellman_ford step 5187 current loss 0.060168, current_train_items 166016.
I0302 19:00:10.022135 22683591385216 run.py:483] Algo bellman_ford step 5188 current loss 0.131610, current_train_items 166048.
I0302 19:00:10.052162 22683591385216 run.py:483] Algo bellman_ford step 5189 current loss 0.112979, current_train_items 166080.
I0302 19:00:10.070646 22683591385216 run.py:483] Algo bellman_ford step 5190 current loss 0.001616, current_train_items 166112.
I0302 19:00:10.086131 22683591385216 run.py:483] Algo bellman_ford step 5191 current loss 0.010889, current_train_items 166144.
I0302 19:00:10.108359 22683591385216 run.py:483] Algo bellman_ford step 5192 current loss 0.044529, current_train_items 166176.
I0302 19:00:10.136537 22683591385216 run.py:483] Algo bellman_ford step 5193 current loss 0.055985, current_train_items 166208.
I0302 19:00:10.167461 22683591385216 run.py:483] Algo bellman_ford step 5194 current loss 0.056439, current_train_items 166240.
I0302 19:00:10.185571 22683591385216 run.py:483] Algo bellman_ford step 5195 current loss 0.001272, current_train_items 166272.
I0302 19:00:10.200969 22683591385216 run.py:483] Algo bellman_ford step 5196 current loss 0.009179, current_train_items 166304.
I0302 19:00:10.223454 22683591385216 run.py:483] Algo bellman_ford step 5197 current loss 0.045773, current_train_items 166336.
I0302 19:00:10.253639 22683591385216 run.py:483] Algo bellman_ford step 5198 current loss 0.071582, current_train_items 166368.
I0302 19:00:10.283929 22683591385216 run.py:483] Algo bellman_ford step 5199 current loss 0.067058, current_train_items 166400.
I0302 19:00:10.302308 22683591385216 run.py:483] Algo bellman_ford step 5200 current loss 0.001388, current_train_items 166432.
I0302 19:00:10.310017 22683591385216 run.py:503] (val) algo bellman_ford step 5200: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 166432, 'step': 5200, 'algorithm': 'bellman_ford'}
I0302 19:00:10.310124 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.992, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:10.326210 22683591385216 run.py:483] Algo bellman_ford step 5201 current loss 0.013016, current_train_items 166464.
I0302 19:00:10.349669 22683591385216 run.py:483] Algo bellman_ford step 5202 current loss 0.029619, current_train_items 166496.
I0302 19:00:10.380475 22683591385216 run.py:483] Algo bellman_ford step 5203 current loss 0.054430, current_train_items 166528.
I0302 19:00:10.413352 22683591385216 run.py:483] Algo bellman_ford step 5204 current loss 0.044461, current_train_items 166560.
I0302 19:00:10.432064 22683591385216 run.py:483] Algo bellman_ford step 5205 current loss 0.001865, current_train_items 166592.
I0302 19:00:10.447111 22683591385216 run.py:483] Algo bellman_ford step 5206 current loss 0.015110, current_train_items 166624.
I0302 19:00:10.469695 22683591385216 run.py:483] Algo bellman_ford step 5207 current loss 0.052806, current_train_items 166656.
I0302 19:00:10.497828 22683591385216 run.py:483] Algo bellman_ford step 5208 current loss 0.059924, current_train_items 166688.
I0302 19:00:10.531354 22683591385216 run.py:483] Algo bellman_ford step 5209 current loss 0.066439, current_train_items 166720.
I0302 19:00:10.549770 22683591385216 run.py:483] Algo bellman_ford step 5210 current loss 0.002972, current_train_items 166752.
I0302 19:00:10.564609 22683591385216 run.py:483] Algo bellman_ford step 5211 current loss 0.013528, current_train_items 166784.
I0302 19:00:10.587337 22683591385216 run.py:483] Algo bellman_ford step 5212 current loss 0.047534, current_train_items 166816.
I0302 19:00:10.618226 22683591385216 run.py:483] Algo bellman_ford step 5213 current loss 0.102787, current_train_items 166848.
I0302 19:00:10.649371 22683591385216 run.py:483] Algo bellman_ford step 5214 current loss 0.058724, current_train_items 166880.
I0302 19:00:10.667601 22683591385216 run.py:483] Algo bellman_ford step 5215 current loss 0.000910, current_train_items 166912.
I0302 19:00:10.683181 22683591385216 run.py:483] Algo bellman_ford step 5216 current loss 0.014490, current_train_items 166944.
I0302 19:00:10.705784 22683591385216 run.py:483] Algo bellman_ford step 5217 current loss 0.024395, current_train_items 166976.
I0302 19:00:10.734556 22683591385216 run.py:483] Algo bellman_ford step 5218 current loss 0.051592, current_train_items 167008.
I0302 19:00:10.765934 22683591385216 run.py:483] Algo bellman_ford step 5219 current loss 0.108301, current_train_items 167040.
I0302 19:00:10.784216 22683591385216 run.py:483] Algo bellman_ford step 5220 current loss 0.002947, current_train_items 167072.
I0302 19:00:10.799381 22683591385216 run.py:483] Algo bellman_ford step 5221 current loss 0.009099, current_train_items 167104.
I0302 19:00:10.820904 22683591385216 run.py:483] Algo bellman_ford step 5222 current loss 0.026950, current_train_items 167136.
I0302 19:00:10.851497 22683591385216 run.py:483] Algo bellman_ford step 5223 current loss 0.087655, current_train_items 167168.
I0302 19:00:10.882087 22683591385216 run.py:483] Algo bellman_ford step 5224 current loss 0.066464, current_train_items 167200.
I0302 19:00:10.900350 22683591385216 run.py:483] Algo bellman_ford step 5225 current loss 0.002309, current_train_items 167232.
I0302 19:00:10.915432 22683591385216 run.py:483] Algo bellman_ford step 5226 current loss 0.023492, current_train_items 167264.
I0302 19:00:10.938153 22683591385216 run.py:483] Algo bellman_ford step 5227 current loss 0.034264, current_train_items 167296.
I0302 19:00:10.968847 22683591385216 run.py:483] Algo bellman_ford step 5228 current loss 0.107257, current_train_items 167328.
I0302 19:00:11.001475 22683591385216 run.py:483] Algo bellman_ford step 5229 current loss 0.080456, current_train_items 167360.
I0302 19:00:11.019660 22683591385216 run.py:483] Algo bellman_ford step 5230 current loss 0.001973, current_train_items 167392.
I0302 19:00:11.035049 22683591385216 run.py:483] Algo bellman_ford step 5231 current loss 0.026776, current_train_items 167424.
I0302 19:00:11.057996 22683591385216 run.py:483] Algo bellman_ford step 5232 current loss 0.029371, current_train_items 167456.
I0302 19:00:11.086750 22683591385216 run.py:483] Algo bellman_ford step 5233 current loss 0.035802, current_train_items 167488.
I0302 19:00:11.119817 22683591385216 run.py:483] Algo bellman_ford step 5234 current loss 0.067952, current_train_items 167520.
I0302 19:00:11.137842 22683591385216 run.py:483] Algo bellman_ford step 5235 current loss 0.002303, current_train_items 167552.
I0302 19:00:11.152906 22683591385216 run.py:483] Algo bellman_ford step 5236 current loss 0.007549, current_train_items 167584.
I0302 19:00:11.175384 22683591385216 run.py:483] Algo bellman_ford step 5237 current loss 0.030319, current_train_items 167616.
I0302 19:00:11.206128 22683591385216 run.py:483] Algo bellman_ford step 5238 current loss 0.052626, current_train_items 167648.
I0302 19:00:11.238106 22683591385216 run.py:483] Algo bellman_ford step 5239 current loss 0.076810, current_train_items 167680.
I0302 19:00:11.256295 22683591385216 run.py:483] Algo bellman_ford step 5240 current loss 0.003682, current_train_items 167712.
I0302 19:00:11.271641 22683591385216 run.py:483] Algo bellman_ford step 5241 current loss 0.021765, current_train_items 167744.
I0302 19:00:11.294406 22683591385216 run.py:483] Algo bellman_ford step 5242 current loss 0.067679, current_train_items 167776.
I0302 19:00:11.323560 22683591385216 run.py:483] Algo bellman_ford step 5243 current loss 0.054205, current_train_items 167808.
I0302 19:00:11.352873 22683591385216 run.py:483] Algo bellman_ford step 5244 current loss 0.024189, current_train_items 167840.
I0302 19:00:11.370819 22683591385216 run.py:483] Algo bellman_ford step 5245 current loss 0.007427, current_train_items 167872.
I0302 19:00:11.386140 22683591385216 run.py:483] Algo bellman_ford step 5246 current loss 0.008762, current_train_items 167904.
I0302 19:00:11.409262 22683591385216 run.py:483] Algo bellman_ford step 5247 current loss 0.012966, current_train_items 167936.
I0302 19:00:11.439340 22683591385216 run.py:483] Algo bellman_ford step 5248 current loss 0.039495, current_train_items 167968.
I0302 19:00:11.471177 22683591385216 run.py:483] Algo bellman_ford step 5249 current loss 0.075798, current_train_items 168000.
I0302 19:00:11.489508 22683591385216 run.py:483] Algo bellman_ford step 5250 current loss 0.001507, current_train_items 168032.
I0302 19:00:11.497226 22683591385216 run.py:503] (val) algo bellman_ford step 5250: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 168032, 'step': 5250, 'algorithm': 'bellman_ford'}
I0302 19:00:11.497332 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.992, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:00:11.525505 22683591385216 run.py:483] Algo bellman_ford step 5251 current loss 0.003462, current_train_items 168064.
I0302 19:00:11.548236 22683591385216 run.py:483] Algo bellman_ford step 5252 current loss 0.068755, current_train_items 168096.
I0302 19:00:11.578725 22683591385216 run.py:483] Algo bellman_ford step 5253 current loss 0.030696, current_train_items 168128.
I0302 19:00:11.608399 22683591385216 run.py:483] Algo bellman_ford step 5254 current loss 0.027119, current_train_items 168160.
I0302 19:00:11.626853 22683591385216 run.py:483] Algo bellman_ford step 5255 current loss 0.024369, current_train_items 168192.
I0302 19:00:11.642118 22683591385216 run.py:483] Algo bellman_ford step 5256 current loss 0.003486, current_train_items 168224.
I0302 19:00:11.663693 22683591385216 run.py:483] Algo bellman_ford step 5257 current loss 0.049259, current_train_items 168256.
I0302 19:00:11.693490 22683591385216 run.py:483] Algo bellman_ford step 5258 current loss 0.050093, current_train_items 168288.
I0302 19:00:11.722946 22683591385216 run.py:483] Algo bellman_ford step 5259 current loss 0.034906, current_train_items 168320.
I0302 19:00:11.741551 22683591385216 run.py:483] Algo bellman_ford step 5260 current loss 0.001055, current_train_items 168352.
I0302 19:00:11.757336 22683591385216 run.py:483] Algo bellman_ford step 5261 current loss 0.016838, current_train_items 168384.
I0302 19:00:11.778729 22683591385216 run.py:483] Algo bellman_ford step 5262 current loss 0.062136, current_train_items 168416.
I0302 19:00:11.808842 22683591385216 run.py:483] Algo bellman_ford step 5263 current loss 0.094124, current_train_items 168448.
I0302 19:00:11.838871 22683591385216 run.py:483] Algo bellman_ford step 5264 current loss 0.058989, current_train_items 168480.
I0302 19:00:11.857043 22683591385216 run.py:483] Algo bellman_ford step 5265 current loss 0.002074, current_train_items 168512.
I0302 19:00:11.872423 22683591385216 run.py:483] Algo bellman_ford step 5266 current loss 0.010361, current_train_items 168544.
I0302 19:00:11.895538 22683591385216 run.py:483] Algo bellman_ford step 5267 current loss 0.069957, current_train_items 168576.
I0302 19:00:11.924252 22683591385216 run.py:483] Algo bellman_ford step 5268 current loss 0.131604, current_train_items 168608.
I0302 19:00:11.953835 22683591385216 run.py:483] Algo bellman_ford step 5269 current loss 0.076799, current_train_items 168640.
I0302 19:00:11.972204 22683591385216 run.py:483] Algo bellman_ford step 5270 current loss 0.001142, current_train_items 168672.
I0302 19:00:11.987658 22683591385216 run.py:483] Algo bellman_ford step 5271 current loss 0.009475, current_train_items 168704.
I0302 19:00:12.008800 22683591385216 run.py:483] Algo bellman_ford step 5272 current loss 0.035231, current_train_items 168736.
I0302 19:00:12.037629 22683591385216 run.py:483] Algo bellman_ford step 5273 current loss 0.024040, current_train_items 168768.
I0302 19:00:12.067790 22683591385216 run.py:483] Algo bellman_ford step 5274 current loss 0.045312, current_train_items 168800.
I0302 19:00:12.086315 22683591385216 run.py:483] Algo bellman_ford step 5275 current loss 0.003041, current_train_items 168832.
I0302 19:00:12.101861 22683591385216 run.py:483] Algo bellman_ford step 5276 current loss 0.010844, current_train_items 168864.
I0302 19:00:12.123420 22683591385216 run.py:483] Algo bellman_ford step 5277 current loss 0.039910, current_train_items 168896.
I0302 19:00:12.152377 22683591385216 run.py:483] Algo bellman_ford step 5278 current loss 0.048832, current_train_items 168928.
I0302 19:00:12.184188 22683591385216 run.py:483] Algo bellman_ford step 5279 current loss 0.052913, current_train_items 168960.
I0302 19:00:12.201955 22683591385216 run.py:483] Algo bellman_ford step 5280 current loss 0.001691, current_train_items 168992.
I0302 19:00:12.217475 22683591385216 run.py:483] Algo bellman_ford step 5281 current loss 0.049604, current_train_items 169024.
I0302 19:00:12.241027 22683591385216 run.py:483] Algo bellman_ford step 5282 current loss 0.030063, current_train_items 169056.
I0302 19:00:12.270156 22683591385216 run.py:483] Algo bellman_ford step 5283 current loss 0.051670, current_train_items 169088.
I0302 19:00:12.300486 22683591385216 run.py:483] Algo bellman_ford step 5284 current loss 0.088902, current_train_items 169120.
I0302 19:00:12.319085 22683591385216 run.py:483] Algo bellman_ford step 5285 current loss 0.002061, current_train_items 169152.
I0302 19:00:12.334688 22683591385216 run.py:483] Algo bellman_ford step 5286 current loss 0.011965, current_train_items 169184.
I0302 19:00:12.357973 22683591385216 run.py:483] Algo bellman_ford step 5287 current loss 0.043710, current_train_items 169216.
I0302 19:00:12.386822 22683591385216 run.py:483] Algo bellman_ford step 5288 current loss 0.056250, current_train_items 169248.
I0302 19:00:12.418741 22683591385216 run.py:483] Algo bellman_ford step 5289 current loss 0.055021, current_train_items 169280.
I0302 19:00:12.437061 22683591385216 run.py:483] Algo bellman_ford step 5290 current loss 0.001394, current_train_items 169312.
I0302 19:00:12.452992 22683591385216 run.py:483] Algo bellman_ford step 5291 current loss 0.011878, current_train_items 169344.
I0302 19:00:12.474722 22683591385216 run.py:483] Algo bellman_ford step 5292 current loss 0.053917, current_train_items 169376.
I0302 19:00:12.503800 22683591385216 run.py:483] Algo bellman_ford step 5293 current loss 0.057863, current_train_items 169408.
I0302 19:00:12.534058 22683591385216 run.py:483] Algo bellman_ford step 5294 current loss 0.057919, current_train_items 169440.
I0302 19:00:12.551906 22683591385216 run.py:483] Algo bellman_ford step 5295 current loss 0.001613, current_train_items 169472.
I0302 19:00:12.566923 22683591385216 run.py:483] Algo bellman_ford step 5296 current loss 0.007133, current_train_items 169504.
I0302 19:00:12.588798 22683591385216 run.py:483] Algo bellman_ford step 5297 current loss 0.052946, current_train_items 169536.
I0302 19:00:12.618678 22683591385216 run.py:483] Algo bellman_ford step 5298 current loss 0.041892, current_train_items 169568.
I0302 19:00:12.652216 22683591385216 run.py:483] Algo bellman_ford step 5299 current loss 0.069371, current_train_items 169600.
I0302 19:00:12.670656 22683591385216 run.py:483] Algo bellman_ford step 5300 current loss 0.002449, current_train_items 169632.
I0302 19:00:12.678372 22683591385216 run.py:503] (val) algo bellman_ford step 5300: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 169632, 'step': 5300, 'algorithm': 'bellman_ford'}
I0302 19:00:12.678477 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.993, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:00:12.693881 22683591385216 run.py:483] Algo bellman_ford step 5301 current loss 0.018276, current_train_items 169664.
I0302 19:00:12.717014 22683591385216 run.py:483] Algo bellman_ford step 5302 current loss 0.035803, current_train_items 169696.
I0302 19:00:12.747508 22683591385216 run.py:483] Algo bellman_ford step 5303 current loss 0.044073, current_train_items 169728.
I0302 19:00:12.779984 22683591385216 run.py:483] Algo bellman_ford step 5304 current loss 0.057576, current_train_items 169760.
I0302 19:00:12.798293 22683591385216 run.py:483] Algo bellman_ford step 5305 current loss 0.008025, current_train_items 169792.
I0302 19:00:12.813639 22683591385216 run.py:483] Algo bellman_ford step 5306 current loss 0.016230, current_train_items 169824.
I0302 19:00:12.837029 22683591385216 run.py:483] Algo bellman_ford step 5307 current loss 0.067286, current_train_items 169856.
I0302 19:00:12.867213 22683591385216 run.py:483] Algo bellman_ford step 5308 current loss 0.042168, current_train_items 169888.
I0302 19:00:12.896847 22683591385216 run.py:483] Algo bellman_ford step 5309 current loss 0.052737, current_train_items 169920.
I0302 19:00:12.914916 22683591385216 run.py:483] Algo bellman_ford step 5310 current loss 0.003374, current_train_items 169952.
I0302 19:00:12.929668 22683591385216 run.py:483] Algo bellman_ford step 5311 current loss 0.008488, current_train_items 169984.
I0302 19:00:12.951820 22683591385216 run.py:483] Algo bellman_ford step 5312 current loss 0.032063, current_train_items 170016.
I0302 19:00:12.980866 22683591385216 run.py:483] Algo bellman_ford step 5313 current loss 0.108793, current_train_items 170048.
I0302 19:00:13.013033 22683591385216 run.py:483] Algo bellman_ford step 5314 current loss 0.115188, current_train_items 170080.
I0302 19:00:13.031208 22683591385216 run.py:483] Algo bellman_ford step 5315 current loss 0.010882, current_train_items 170112.
I0302 19:00:13.046411 22683591385216 run.py:483] Algo bellman_ford step 5316 current loss 0.015590, current_train_items 170144.
I0302 19:00:13.068631 22683591385216 run.py:483] Algo bellman_ford step 5317 current loss 0.061785, current_train_items 170176.
I0302 19:00:13.096388 22683591385216 run.py:483] Algo bellman_ford step 5318 current loss 0.058145, current_train_items 170208.
I0302 19:00:13.128189 22683591385216 run.py:483] Algo bellman_ford step 5319 current loss 0.061540, current_train_items 170240.
I0302 19:00:13.146058 22683591385216 run.py:483] Algo bellman_ford step 5320 current loss 0.013731, current_train_items 170272.
I0302 19:00:13.161319 22683591385216 run.py:483] Algo bellman_ford step 5321 current loss 0.035419, current_train_items 170304.
I0302 19:00:13.184433 22683591385216 run.py:483] Algo bellman_ford step 5322 current loss 0.036440, current_train_items 170336.
I0302 19:00:13.212064 22683591385216 run.py:483] Algo bellman_ford step 5323 current loss 0.045320, current_train_items 170368.
I0302 19:00:13.244357 22683591385216 run.py:483] Algo bellman_ford step 5324 current loss 0.063774, current_train_items 170400.
I0302 19:00:13.262533 22683591385216 run.py:483] Algo bellman_ford step 5325 current loss 0.002256, current_train_items 170432.
I0302 19:00:13.277785 22683591385216 run.py:483] Algo bellman_ford step 5326 current loss 0.025288, current_train_items 170464.
I0302 19:00:13.299173 22683591385216 run.py:483] Algo bellman_ford step 5327 current loss 0.048292, current_train_items 170496.
I0302 19:00:13.329393 22683591385216 run.py:483] Algo bellman_ford step 5328 current loss 0.029735, current_train_items 170528.
I0302 19:00:13.360051 22683591385216 run.py:483] Algo bellman_ford step 5329 current loss 0.057078, current_train_items 170560.
I0302 19:00:13.378081 22683591385216 run.py:483] Algo bellman_ford step 5330 current loss 0.008637, current_train_items 170592.
I0302 19:00:13.393440 22683591385216 run.py:483] Algo bellman_ford step 5331 current loss 0.011996, current_train_items 170624.
I0302 19:00:13.415508 22683591385216 run.py:483] Algo bellman_ford step 5332 current loss 0.034980, current_train_items 170656.
I0302 19:00:13.443440 22683591385216 run.py:483] Algo bellman_ford step 5333 current loss 0.035181, current_train_items 170688.
I0302 19:00:13.473572 22683591385216 run.py:483] Algo bellman_ford step 5334 current loss 0.058350, current_train_items 170720.
I0302 19:00:13.491765 22683591385216 run.py:483] Algo bellman_ford step 5335 current loss 0.004926, current_train_items 170752.
I0302 19:00:13.507417 22683591385216 run.py:483] Algo bellman_ford step 5336 current loss 0.031500, current_train_items 170784.
I0302 19:00:13.529261 22683591385216 run.py:483] Algo bellman_ford step 5337 current loss 0.048388, current_train_items 170816.
I0302 19:00:13.557511 22683591385216 run.py:483] Algo bellman_ford step 5338 current loss 0.086213, current_train_items 170848.
I0302 19:00:13.589188 22683591385216 run.py:483] Algo bellman_ford step 5339 current loss 0.072552, current_train_items 170880.
I0302 19:00:13.607119 22683591385216 run.py:483] Algo bellman_ford step 5340 current loss 0.003580, current_train_items 170912.
I0302 19:00:13.622471 22683591385216 run.py:483] Algo bellman_ford step 5341 current loss 0.032553, current_train_items 170944.
I0302 19:00:13.646216 22683591385216 run.py:483] Algo bellman_ford step 5342 current loss 0.061552, current_train_items 170976.
I0302 19:00:13.675584 22683591385216 run.py:483] Algo bellman_ford step 5343 current loss 0.119850, current_train_items 171008.
I0302 19:00:13.706579 22683591385216 run.py:483] Algo bellman_ford step 5344 current loss 0.074412, current_train_items 171040.
I0302 19:00:13.724304 22683591385216 run.py:483] Algo bellman_ford step 5345 current loss 0.005284, current_train_items 171072.
I0302 19:00:13.738968 22683591385216 run.py:483] Algo bellman_ford step 5346 current loss 0.017639, current_train_items 171104.
I0302 19:00:13.761106 22683591385216 run.py:483] Algo bellman_ford step 5347 current loss 0.037307, current_train_items 171136.
I0302 19:00:13.791203 22683591385216 run.py:483] Algo bellman_ford step 5348 current loss 0.076017, current_train_items 171168.
I0302 19:00:13.822631 22683591385216 run.py:483] Algo bellman_ford step 5349 current loss 0.078082, current_train_items 171200.
I0302 19:00:13.840626 22683591385216 run.py:483] Algo bellman_ford step 5350 current loss 0.005455, current_train_items 171232.
I0302 19:00:13.848651 22683591385216 run.py:503] (val) algo bellman_ford step 5350: {'pi': 0.998046875, 'score': 0.998046875, 'examples_seen': 171232, 'step': 5350, 'algorithm': 'bellman_ford'}
I0302 19:00:13.848756 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.993, current avg val score is 0.998, val scores are: bellman_ford: 0.998
I0302 19:00:13.876088 22683591385216 run.py:483] Algo bellman_ford step 5351 current loss 0.008583, current_train_items 171264.
I0302 19:00:13.898913 22683591385216 run.py:483] Algo bellman_ford step 5352 current loss 0.054569, current_train_items 171296.
I0302 19:00:13.927695 22683591385216 run.py:483] Algo bellman_ford step 5353 current loss 0.055221, current_train_items 171328.
I0302 19:00:13.959501 22683591385216 run.py:483] Algo bellman_ford step 5354 current loss 0.073399, current_train_items 171360.
I0302 19:00:13.978244 22683591385216 run.py:483] Algo bellman_ford step 5355 current loss 0.004716, current_train_items 171392.
I0302 19:00:13.993841 22683591385216 run.py:483] Algo bellman_ford step 5356 current loss 0.035082, current_train_items 171424.
I0302 19:00:14.016497 22683591385216 run.py:483] Algo bellman_ford step 5357 current loss 0.063503, current_train_items 171456.
I0302 19:00:14.044825 22683591385216 run.py:483] Algo bellman_ford step 5358 current loss 0.032648, current_train_items 171488.
I0302 19:00:14.075936 22683591385216 run.py:483] Algo bellman_ford step 5359 current loss 0.059619, current_train_items 171520.
I0302 19:00:14.094854 22683591385216 run.py:483] Algo bellman_ford step 5360 current loss 0.005017, current_train_items 171552.
I0302 19:00:14.110595 22683591385216 run.py:483] Algo bellman_ford step 5361 current loss 0.028941, current_train_items 171584.
I0302 19:00:14.133193 22683591385216 run.py:483] Algo bellman_ford step 5362 current loss 0.074008, current_train_items 171616.
I0302 19:00:14.162922 22683591385216 run.py:483] Algo bellman_ford step 5363 current loss 0.058109, current_train_items 171648.
I0302 19:00:14.194344 22683591385216 run.py:483] Algo bellman_ford step 5364 current loss 0.071611, current_train_items 171680.
I0302 19:00:14.212883 22683591385216 run.py:483] Algo bellman_ford step 5365 current loss 0.001757, current_train_items 171712.
I0302 19:00:14.228349 22683591385216 run.py:483] Algo bellman_ford step 5366 current loss 0.021685, current_train_items 171744.
I0302 19:00:14.250732 22683591385216 run.py:483] Algo bellman_ford step 5367 current loss 0.030581, current_train_items 171776.
I0302 19:00:14.279867 22683591385216 run.py:483] Algo bellman_ford step 5368 current loss 0.029808, current_train_items 171808.
I0302 19:00:14.310248 22683591385216 run.py:483] Algo bellman_ford step 5369 current loss 0.036387, current_train_items 171840.
I0302 19:00:14.329105 22683591385216 run.py:483] Algo bellman_ford step 5370 current loss 0.001191, current_train_items 171872.
I0302 19:00:14.344726 22683591385216 run.py:483] Algo bellman_ford step 5371 current loss 0.004018, current_train_items 171904.
I0302 19:00:14.366737 22683591385216 run.py:483] Algo bellman_ford step 5372 current loss 0.016696, current_train_items 171936.
I0302 19:00:14.395849 22683591385216 run.py:483] Algo bellman_ford step 5373 current loss 0.061006, current_train_items 171968.
I0302 19:00:14.424645 22683591385216 run.py:483] Algo bellman_ford step 5374 current loss 0.085994, current_train_items 172000.
I0302 19:00:14.443264 22683591385216 run.py:483] Algo bellman_ford step 5375 current loss 0.001755, current_train_items 172032.
I0302 19:00:14.458965 22683591385216 run.py:483] Algo bellman_ford step 5376 current loss 0.038841, current_train_items 172064.
I0302 19:00:14.481040 22683591385216 run.py:483] Algo bellman_ford step 5377 current loss 0.049348, current_train_items 172096.
I0302 19:00:14.509398 22683591385216 run.py:483] Algo bellman_ford step 5378 current loss 0.031568, current_train_items 172128.
I0302 19:00:14.537614 22683591385216 run.py:483] Algo bellman_ford step 5379 current loss 0.053541, current_train_items 172160.
I0302 19:00:14.556044 22683591385216 run.py:483] Algo bellman_ford step 5380 current loss 0.007512, current_train_items 172192.
I0302 19:00:14.571519 22683591385216 run.py:483] Algo bellman_ford step 5381 current loss 0.025002, current_train_items 172224.
I0302 19:00:14.594768 22683591385216 run.py:483] Algo bellman_ford step 5382 current loss 0.050671, current_train_items 172256.
I0302 19:00:14.623939 22683591385216 run.py:483] Algo bellman_ford step 5383 current loss 0.077770, current_train_items 172288.
I0302 19:00:14.653983 22683591385216 run.py:483] Algo bellman_ford step 5384 current loss 0.049637, current_train_items 172320.
I0302 19:00:14.672792 22683591385216 run.py:483] Algo bellman_ford step 5385 current loss 0.000927, current_train_items 172352.
I0302 19:00:14.688024 22683591385216 run.py:483] Algo bellman_ford step 5386 current loss 0.007040, current_train_items 172384.
I0302 19:00:14.709949 22683591385216 run.py:483] Algo bellman_ford step 5387 current loss 0.014707, current_train_items 172416.
I0302 19:00:14.738040 22683591385216 run.py:483] Algo bellman_ford step 5388 current loss 0.026240, current_train_items 172448.
I0302 19:00:14.769907 22683591385216 run.py:483] Algo bellman_ford step 5389 current loss 0.082829, current_train_items 172480.
I0302 19:00:14.788944 22683591385216 run.py:483] Algo bellman_ford step 5390 current loss 0.006853, current_train_items 172512.
I0302 19:00:14.804963 22683591385216 run.py:483] Algo bellman_ford step 5391 current loss 0.026881, current_train_items 172544.
I0302 19:00:14.826354 22683591385216 run.py:483] Algo bellman_ford step 5392 current loss 0.038955, current_train_items 172576.
I0302 19:00:14.856453 22683591385216 run.py:483] Algo bellman_ford step 5393 current loss 0.035468, current_train_items 172608.
I0302 19:00:14.888677 22683591385216 run.py:483] Algo bellman_ford step 5394 current loss 0.057388, current_train_items 172640.
I0302 19:00:14.907016 22683591385216 run.py:483] Algo bellman_ford step 5395 current loss 0.002374, current_train_items 172672.
I0302 19:00:14.922635 22683591385216 run.py:483] Algo bellman_ford step 5396 current loss 0.027017, current_train_items 172704.
I0302 19:00:14.945176 22683591385216 run.py:483] Algo bellman_ford step 5397 current loss 0.030214, current_train_items 172736.
I0302 19:00:14.974175 22683591385216 run.py:483] Algo bellman_ford step 5398 current loss 0.045393, current_train_items 172768.
I0302 19:00:15.006575 22683591385216 run.py:483] Algo bellman_ford step 5399 current loss 0.149774, current_train_items 172800.
I0302 19:00:15.025352 22683591385216 run.py:483] Algo bellman_ford step 5400 current loss 0.001806, current_train_items 172832.
I0302 19:00:15.033172 22683591385216 run.py:503] (val) algo bellman_ford step 5400: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 172832, 'step': 5400, 'algorithm': 'bellman_ford'}
I0302 19:00:15.033279 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:15.049715 22683591385216 run.py:483] Algo bellman_ford step 5401 current loss 0.037930, current_train_items 172864.
I0302 19:00:15.073365 22683591385216 run.py:483] Algo bellman_ford step 5402 current loss 0.037403, current_train_items 172896.
I0302 19:00:15.104605 22683591385216 run.py:483] Algo bellman_ford step 5403 current loss 0.071627, current_train_items 172928.
I0302 19:00:15.137555 22683591385216 run.py:483] Algo bellman_ford step 5404 current loss 0.041546, current_train_items 172960.
I0302 19:00:15.156161 22683591385216 run.py:483] Algo bellman_ford step 5405 current loss 0.001366, current_train_items 172992.
I0302 19:00:15.171177 22683591385216 run.py:483] Algo bellman_ford step 5406 current loss 0.011253, current_train_items 173024.
I0302 19:00:15.193052 22683591385216 run.py:483] Algo bellman_ford step 5407 current loss 0.011503, current_train_items 173056.
I0302 19:00:15.220557 22683591385216 run.py:483] Algo bellman_ford step 5408 current loss 0.068706, current_train_items 173088.
I0302 19:00:15.251039 22683591385216 run.py:483] Algo bellman_ford step 5409 current loss 0.049004, current_train_items 173120.
I0302 19:00:15.269586 22683591385216 run.py:483] Algo bellman_ford step 5410 current loss 0.001817, current_train_items 173152.
I0302 19:00:15.285357 22683591385216 run.py:483] Algo bellman_ford step 5411 current loss 0.014779, current_train_items 173184.
I0302 19:00:15.308167 22683591385216 run.py:483] Algo bellman_ford step 5412 current loss 0.076922, current_train_items 173216.
I0302 19:00:15.337677 22683591385216 run.py:483] Algo bellman_ford step 5413 current loss 0.043373, current_train_items 173248.
I0302 19:00:15.369628 22683591385216 run.py:483] Algo bellman_ford step 5414 current loss 0.072850, current_train_items 173280.
I0302 19:00:15.388073 22683591385216 run.py:483] Algo bellman_ford step 5415 current loss 0.002678, current_train_items 173312.
I0302 19:00:15.403628 22683591385216 run.py:483] Algo bellman_ford step 5416 current loss 0.009529, current_train_items 173344.
I0302 19:00:15.426562 22683591385216 run.py:483] Algo bellman_ford step 5417 current loss 0.042839, current_train_items 173376.
I0302 19:00:15.456730 22683591385216 run.py:483] Algo bellman_ford step 5418 current loss 0.031888, current_train_items 173408.
I0302 19:00:15.489907 22683591385216 run.py:483] Algo bellman_ford step 5419 current loss 0.075180, current_train_items 173440.
I0302 19:00:15.508224 22683591385216 run.py:483] Algo bellman_ford step 5420 current loss 0.003781, current_train_items 173472.
I0302 19:00:15.523733 22683591385216 run.py:483] Algo bellman_ford step 5421 current loss 0.013072, current_train_items 173504.
I0302 19:00:15.545792 22683591385216 run.py:483] Algo bellman_ford step 5422 current loss 0.019723, current_train_items 173536.
I0302 19:00:15.574641 22683591385216 run.py:483] Algo bellman_ford step 5423 current loss 0.027834, current_train_items 173568.
I0302 19:00:15.604223 22683591385216 run.py:483] Algo bellman_ford step 5424 current loss 0.067224, current_train_items 173600.
I0302 19:00:15.622534 22683591385216 run.py:483] Algo bellman_ford step 5425 current loss 0.002495, current_train_items 173632.
I0302 19:00:15.637578 22683591385216 run.py:483] Algo bellman_ford step 5426 current loss 0.011555, current_train_items 173664.
I0302 19:00:15.660603 22683591385216 run.py:483] Algo bellman_ford step 5427 current loss 0.058968, current_train_items 173696.
I0302 19:00:15.689725 22683591385216 run.py:483] Algo bellman_ford step 5428 current loss 0.037662, current_train_items 173728.
I0302 19:00:15.722515 22683591385216 run.py:483] Algo bellman_ford step 5429 current loss 0.034779, current_train_items 173760.
I0302 19:00:15.740478 22683591385216 run.py:483] Algo bellman_ford step 5430 current loss 0.001710, current_train_items 173792.
I0302 19:00:15.756011 22683591385216 run.py:483] Algo bellman_ford step 5431 current loss 0.046911, current_train_items 173824.
I0302 19:00:15.778821 22683591385216 run.py:483] Algo bellman_ford step 5432 current loss 0.046587, current_train_items 173856.
I0302 19:00:15.810296 22683591385216 run.py:483] Algo bellman_ford step 5433 current loss 0.081107, current_train_items 173888.
I0302 19:00:15.844037 22683591385216 run.py:483] Algo bellman_ford step 5434 current loss 0.083282, current_train_items 173920.
I0302 19:00:15.862352 22683591385216 run.py:483] Algo bellman_ford step 5435 current loss 0.001623, current_train_items 173952.
I0302 19:00:15.877676 22683591385216 run.py:483] Algo bellman_ford step 5436 current loss 0.041858, current_train_items 173984.
I0302 19:00:15.900193 22683591385216 run.py:483] Algo bellman_ford step 5437 current loss 0.115856, current_train_items 174016.
I0302 19:00:15.928686 22683591385216 run.py:483] Algo bellman_ford step 5438 current loss 0.116077, current_train_items 174048.
I0302 19:00:15.963669 22683591385216 run.py:483] Algo bellman_ford step 5439 current loss 0.115696, current_train_items 174080.
I0302 19:00:15.981911 22683591385216 run.py:483] Algo bellman_ford step 5440 current loss 0.010727, current_train_items 174112.
I0302 19:00:15.997080 22683591385216 run.py:483] Algo bellman_ford step 5441 current loss 0.006562, current_train_items 174144.
I0302 19:00:16.019180 22683591385216 run.py:483] Algo bellman_ford step 5442 current loss 0.037391, current_train_items 174176.
I0302 19:00:16.047771 22683591385216 run.py:483] Algo bellman_ford step 5443 current loss 0.073694, current_train_items 174208.
I0302 19:00:16.079507 22683591385216 run.py:483] Algo bellman_ford step 5444 current loss 0.111584, current_train_items 174240.
I0302 19:00:16.097905 22683591385216 run.py:483] Algo bellman_ford step 5445 current loss 0.001338, current_train_items 174272.
I0302 19:00:16.113241 22683591385216 run.py:483] Algo bellman_ford step 5446 current loss 0.051749, current_train_items 174304.
I0302 19:00:16.135951 22683591385216 run.py:483] Algo bellman_ford step 5447 current loss 0.058554, current_train_items 174336.
I0302 19:00:16.166562 22683591385216 run.py:483] Algo bellman_ford step 5448 current loss 0.117411, current_train_items 174368.
I0302 19:00:16.198075 22683591385216 run.py:483] Algo bellman_ford step 5449 current loss 0.075496, current_train_items 174400.
I0302 19:00:16.215790 22683591385216 run.py:483] Algo bellman_ford step 5450 current loss 0.007860, current_train_items 174432.
I0302 19:00:16.223590 22683591385216 run.py:503] (val) algo bellman_ford step 5450: {'pi': 0.9755859375, 'score': 0.9755859375, 'examples_seen': 174432, 'step': 5450, 'algorithm': 'bellman_ford'}
I0302 19:00:16.223697 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.976, val scores are: bellman_ford: 0.976
I0302 19:00:16.239322 22683591385216 run.py:483] Algo bellman_ford step 5451 current loss 0.018910, current_train_items 174464.
I0302 19:00:16.262304 22683591385216 run.py:483] Algo bellman_ford step 5452 current loss 0.062007, current_train_items 174496.
I0302 19:00:16.290697 22683591385216 run.py:483] Algo bellman_ford step 5453 current loss 0.064446, current_train_items 174528.
I0302 19:00:16.322023 22683591385216 run.py:483] Algo bellman_ford step 5454 current loss 0.074331, current_train_items 174560.
I0302 19:00:16.340313 22683591385216 run.py:483] Algo bellman_ford step 5455 current loss 0.003476, current_train_items 174592.
I0302 19:00:16.355786 22683591385216 run.py:483] Algo bellman_ford step 5456 current loss 0.013196, current_train_items 174624.
I0302 19:00:16.378840 22683591385216 run.py:483] Algo bellman_ford step 5457 current loss 0.076049, current_train_items 174656.
I0302 19:00:16.408439 22683591385216 run.py:483] Algo bellman_ford step 5458 current loss 0.061228, current_train_items 174688.
I0302 19:00:16.437477 22683591385216 run.py:483] Algo bellman_ford step 5459 current loss 0.058929, current_train_items 174720.
I0302 19:00:16.455806 22683591385216 run.py:483] Algo bellman_ford step 5460 current loss 0.002688, current_train_items 174752.
I0302 19:00:16.471785 22683591385216 run.py:483] Algo bellman_ford step 5461 current loss 0.032031, current_train_items 174784.
I0302 19:00:16.494598 22683591385216 run.py:483] Algo bellman_ford step 5462 current loss 0.015836, current_train_items 174816.
I0302 19:00:16.523307 22683591385216 run.py:483] Algo bellman_ford step 5463 current loss 0.056113, current_train_items 174848.
I0302 19:00:16.555276 22683591385216 run.py:483] Algo bellman_ford step 5464 current loss 0.110206, current_train_items 174880.
I0302 19:00:16.573767 22683591385216 run.py:483] Algo bellman_ford step 5465 current loss 0.003846, current_train_items 174912.
I0302 19:00:16.588796 22683591385216 run.py:483] Algo bellman_ford step 5466 current loss 0.018528, current_train_items 174944.
I0302 19:00:16.610798 22683591385216 run.py:483] Algo bellman_ford step 5467 current loss 0.039853, current_train_items 174976.
I0302 19:00:16.638866 22683591385216 run.py:483] Algo bellman_ford step 5468 current loss 0.058726, current_train_items 175008.
I0302 19:00:16.670362 22683591385216 run.py:483] Algo bellman_ford step 5469 current loss 0.053784, current_train_items 175040.
I0302 19:00:16.688974 22683591385216 run.py:483] Algo bellman_ford step 5470 current loss 0.004311, current_train_items 175072.
I0302 19:00:16.704697 22683591385216 run.py:483] Algo bellman_ford step 5471 current loss 0.050854, current_train_items 175104.
I0302 19:00:16.726303 22683591385216 run.py:483] Algo bellman_ford step 5472 current loss 0.048255, current_train_items 175136.
I0302 19:00:16.755424 22683591385216 run.py:483] Algo bellman_ford step 5473 current loss 0.028729, current_train_items 175168.
I0302 19:00:16.787663 22683591385216 run.py:483] Algo bellman_ford step 5474 current loss 0.062472, current_train_items 175200.
I0302 19:00:16.806395 22683591385216 run.py:483] Algo bellman_ford step 5475 current loss 0.003354, current_train_items 175232.
I0302 19:00:16.822051 22683591385216 run.py:483] Algo bellman_ford step 5476 current loss 0.054772, current_train_items 175264.
I0302 19:00:16.844111 22683591385216 run.py:483] Algo bellman_ford step 5477 current loss 0.057577, current_train_items 175296.
I0302 19:00:16.873459 22683591385216 run.py:483] Algo bellman_ford step 5478 current loss 0.060360, current_train_items 175328.
I0302 19:00:16.904529 22683591385216 run.py:483] Algo bellman_ford step 5479 current loss 0.082792, current_train_items 175360.
I0302 19:00:16.923049 22683591385216 run.py:483] Algo bellman_ford step 5480 current loss 0.040867, current_train_items 175392.
I0302 19:00:16.939017 22683591385216 run.py:483] Algo bellman_ford step 5481 current loss 0.047670, current_train_items 175424.
I0302 19:00:16.962852 22683591385216 run.py:483] Algo bellman_ford step 5482 current loss 0.080726, current_train_items 175456.
I0302 19:00:16.993135 22683591385216 run.py:483] Algo bellman_ford step 5483 current loss 0.117234, current_train_items 175488.
I0302 19:00:17.026714 22683591385216 run.py:483] Algo bellman_ford step 5484 current loss 0.114007, current_train_items 175520.
I0302 19:00:17.045190 22683591385216 run.py:483] Algo bellman_ford step 5485 current loss 0.005260, current_train_items 175552.
I0302 19:00:17.060502 22683591385216 run.py:483] Algo bellman_ford step 5486 current loss 0.017439, current_train_items 175584.
I0302 19:00:17.082337 22683591385216 run.py:483] Algo bellman_ford step 5487 current loss 0.032642, current_train_items 175616.
I0302 19:00:17.113167 22683591385216 run.py:483] Algo bellman_ford step 5488 current loss 0.047658, current_train_items 175648.
I0302 19:00:17.143479 22683591385216 run.py:483] Algo bellman_ford step 5489 current loss 0.041836, current_train_items 175680.
I0302 19:00:17.162085 22683591385216 run.py:483] Algo bellman_ford step 5490 current loss 0.004990, current_train_items 175712.
I0302 19:00:17.177759 22683591385216 run.py:483] Algo bellman_ford step 5491 current loss 0.013393, current_train_items 175744.
I0302 19:00:17.200424 22683591385216 run.py:483] Algo bellman_ford step 5492 current loss 0.028556, current_train_items 175776.
I0302 19:00:17.229478 22683591385216 run.py:483] Algo bellman_ford step 5493 current loss 0.030678, current_train_items 175808.
I0302 19:00:17.260335 22683591385216 run.py:483] Algo bellman_ford step 5494 current loss 0.053978, current_train_items 175840.
I0302 19:00:17.278471 22683591385216 run.py:483] Algo bellman_ford step 5495 current loss 0.004044, current_train_items 175872.
I0302 19:00:17.294035 22683591385216 run.py:483] Algo bellman_ford step 5496 current loss 0.005753, current_train_items 175904.
I0302 19:00:17.316766 22683591385216 run.py:483] Algo bellman_ford step 5497 current loss 0.021586, current_train_items 175936.
I0302 19:00:17.346011 22683591385216 run.py:483] Algo bellman_ford step 5498 current loss 0.038179, current_train_items 175968.
I0302 19:00:17.378008 22683591385216 run.py:483] Algo bellman_ford step 5499 current loss 0.139771, current_train_items 176000.
I0302 19:00:17.397008 22683591385216 run.py:483] Algo bellman_ford step 5500 current loss 0.005209, current_train_items 176032.
I0302 19:00:17.404678 22683591385216 run.py:503] (val) algo bellman_ford step 5500: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 176032, 'step': 5500, 'algorithm': 'bellman_ford'}
I0302 19:00:17.404784 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:00:17.421084 22683591385216 run.py:483] Algo bellman_ford step 5501 current loss 0.031635, current_train_items 176064.
I0302 19:00:17.444449 22683591385216 run.py:483] Algo bellman_ford step 5502 current loss 0.052893, current_train_items 176096.
I0302 19:00:17.474036 22683591385216 run.py:483] Algo bellman_ford step 5503 current loss 0.021866, current_train_items 176128.
I0302 19:00:17.505921 22683591385216 run.py:483] Algo bellman_ford step 5504 current loss 0.050021, current_train_items 176160.
I0302 19:00:17.524555 22683591385216 run.py:483] Algo bellman_ford step 5505 current loss 0.006136, current_train_items 176192.
I0302 19:00:17.540009 22683591385216 run.py:483] Algo bellman_ford step 5506 current loss 0.041871, current_train_items 176224.
I0302 19:00:17.561881 22683591385216 run.py:483] Algo bellman_ford step 5507 current loss 0.021956, current_train_items 176256.
I0302 19:00:17.590251 22683591385216 run.py:483] Algo bellman_ford step 5508 current loss 0.034599, current_train_items 176288.
I0302 19:00:17.620849 22683591385216 run.py:483] Algo bellman_ford step 5509 current loss 0.029617, current_train_items 176320.
I0302 19:00:17.638998 22683591385216 run.py:483] Algo bellman_ford step 5510 current loss 0.001615, current_train_items 176352.
I0302 19:00:17.654650 22683591385216 run.py:483] Algo bellman_ford step 5511 current loss 0.013803, current_train_items 176384.
I0302 19:00:17.678369 22683591385216 run.py:483] Algo bellman_ford step 5512 current loss 0.059685, current_train_items 176416.
I0302 19:00:17.707026 22683591385216 run.py:483] Algo bellman_ford step 5513 current loss 0.041052, current_train_items 176448.
I0302 19:00:17.736788 22683591385216 run.py:483] Algo bellman_ford step 5514 current loss 0.062371, current_train_items 176480.
I0302 19:00:17.755084 22683591385216 run.py:483] Algo bellman_ford step 5515 current loss 0.003106, current_train_items 176512.
I0302 19:00:17.770076 22683591385216 run.py:483] Algo bellman_ford step 5516 current loss 0.004978, current_train_items 176544.
I0302 19:00:17.792803 22683591385216 run.py:483] Algo bellman_ford step 5517 current loss 0.041059, current_train_items 176576.
I0302 19:00:17.821518 22683591385216 run.py:483] Algo bellman_ford step 5518 current loss 0.083100, current_train_items 176608.
I0302 19:00:17.854370 22683591385216 run.py:483] Algo bellman_ford step 5519 current loss 0.114489, current_train_items 176640.
I0302 19:00:17.872582 22683591385216 run.py:483] Algo bellman_ford step 5520 current loss 0.010636, current_train_items 176672.
I0302 19:00:17.888309 22683591385216 run.py:483] Algo bellman_ford step 5521 current loss 0.007848, current_train_items 176704.
I0302 19:00:17.910297 22683591385216 run.py:483] Algo bellman_ford step 5522 current loss 0.024778, current_train_items 176736.
I0302 19:00:17.940368 22683591385216 run.py:483] Algo bellman_ford step 5523 current loss 0.035454, current_train_items 176768.
I0302 19:00:17.973193 22683591385216 run.py:483] Algo bellman_ford step 5524 current loss 0.067486, current_train_items 176800.
I0302 19:00:17.991618 22683591385216 run.py:483] Algo bellman_ford step 5525 current loss 0.002987, current_train_items 176832.
I0302 19:00:18.006554 22683591385216 run.py:483] Algo bellman_ford step 5526 current loss 0.019219, current_train_items 176864.
I0302 19:00:18.029887 22683591385216 run.py:483] Algo bellman_ford step 5527 current loss 0.028570, current_train_items 176896.
I0302 19:00:18.058463 22683591385216 run.py:483] Algo bellman_ford step 5528 current loss 0.048148, current_train_items 176928.
I0302 19:00:18.088677 22683591385216 run.py:483] Algo bellman_ford step 5529 current loss 0.034139, current_train_items 176960.
I0302 19:00:18.107071 22683591385216 run.py:483] Algo bellman_ford step 5530 current loss 0.000789, current_train_items 176992.
I0302 19:00:18.122482 22683591385216 run.py:483] Algo bellman_ford step 5531 current loss 0.012535, current_train_items 177024.
I0302 19:00:18.145539 22683591385216 run.py:483] Algo bellman_ford step 5532 current loss 0.104023, current_train_items 177056.
I0302 19:00:18.174310 22683591385216 run.py:483] Algo bellman_ford step 5533 current loss 0.090937, current_train_items 177088.
I0302 19:00:18.207609 22683591385216 run.py:483] Algo bellman_ford step 5534 current loss 0.114507, current_train_items 177120.
I0302 19:00:18.225951 22683591385216 run.py:483] Algo bellman_ford step 5535 current loss 0.002087, current_train_items 177152.
I0302 19:00:18.241501 22683591385216 run.py:483] Algo bellman_ford step 5536 current loss 0.023397, current_train_items 177184.
I0302 19:00:18.263628 22683591385216 run.py:483] Algo bellman_ford step 5537 current loss 0.066358, current_train_items 177216.
I0302 19:00:18.291807 22683591385216 run.py:483] Algo bellman_ford step 5538 current loss 0.132824, current_train_items 177248.
I0302 19:00:18.325480 22683591385216 run.py:483] Algo bellman_ford step 5539 current loss 0.055744, current_train_items 177280.
I0302 19:00:18.343649 22683591385216 run.py:483] Algo bellman_ford step 5540 current loss 0.000983, current_train_items 177312.
I0302 19:00:18.359563 22683591385216 run.py:483] Algo bellman_ford step 5541 current loss 0.016307, current_train_items 177344.
I0302 19:00:18.382799 22683591385216 run.py:483] Algo bellman_ford step 5542 current loss 0.047862, current_train_items 177376.
I0302 19:00:18.411885 22683591385216 run.py:483] Algo bellman_ford step 5543 current loss 0.070157, current_train_items 177408.
I0302 19:00:18.443009 22683591385216 run.py:483] Algo bellman_ford step 5544 current loss 0.068761, current_train_items 177440.
I0302 19:00:18.461404 22683591385216 run.py:483] Algo bellman_ford step 5545 current loss 0.002404, current_train_items 177472.
I0302 19:00:18.477270 22683591385216 run.py:483] Algo bellman_ford step 5546 current loss 0.041798, current_train_items 177504.
I0302 19:00:18.499760 22683591385216 run.py:483] Algo bellman_ford step 5547 current loss 0.032040, current_train_items 177536.
I0302 19:00:18.528607 22683591385216 run.py:483] Algo bellman_ford step 5548 current loss 0.026849, current_train_items 177568.
I0302 19:00:18.558882 22683591385216 run.py:483] Algo bellman_ford step 5549 current loss 0.074334, current_train_items 177600.
I0302 19:00:18.576999 22683591385216 run.py:483] Algo bellman_ford step 5550 current loss 0.001723, current_train_items 177632.
I0302 19:00:18.584889 22683591385216 run.py:503] (val) algo bellman_ford step 5550: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 177632, 'step': 5550, 'algorithm': 'bellman_ford'}
I0302 19:00:18.585005 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:18.600837 22683591385216 run.py:483] Algo bellman_ford step 5551 current loss 0.009286, current_train_items 177664.
I0302 19:00:18.624917 22683591385216 run.py:483] Algo bellman_ford step 5552 current loss 0.066948, current_train_items 177696.
I0302 19:00:18.653914 22683591385216 run.py:483] Algo bellman_ford step 5553 current loss 0.050540, current_train_items 177728.
I0302 19:00:18.688625 22683591385216 run.py:483] Algo bellman_ford step 5554 current loss 0.104331, current_train_items 177760.
I0302 19:00:18.707379 22683591385216 run.py:483] Algo bellman_ford step 5555 current loss 0.006871, current_train_items 177792.
I0302 19:00:18.722615 22683591385216 run.py:483] Algo bellman_ford step 5556 current loss 0.033182, current_train_items 177824.
I0302 19:00:18.745492 22683591385216 run.py:483] Algo bellman_ford step 5557 current loss 0.098864, current_train_items 177856.
I0302 19:00:18.773872 22683591385216 run.py:483] Algo bellman_ford step 5558 current loss 0.050422, current_train_items 177888.
I0302 19:00:18.805929 22683591385216 run.py:483] Algo bellman_ford step 5559 current loss 0.068700, current_train_items 177920.
I0302 19:00:18.824723 22683591385216 run.py:483] Algo bellman_ford step 5560 current loss 0.013672, current_train_items 177952.
I0302 19:00:18.840129 22683591385216 run.py:483] Algo bellman_ford step 5561 current loss 0.013129, current_train_items 177984.
I0302 19:00:18.862287 22683591385216 run.py:483] Algo bellman_ford step 5562 current loss 0.058492, current_train_items 178016.
I0302 19:00:18.892287 22683591385216 run.py:483] Algo bellman_ford step 5563 current loss 0.071121, current_train_items 178048.
I0302 19:00:18.925020 22683591385216 run.py:483] Algo bellman_ford step 5564 current loss 0.085587, current_train_items 178080.
I0302 19:00:18.943465 22683591385216 run.py:483] Algo bellman_ford step 5565 current loss 0.004809, current_train_items 178112.
I0302 19:00:18.958683 22683591385216 run.py:483] Algo bellman_ford step 5566 current loss 0.023367, current_train_items 178144.
I0302 19:00:18.981184 22683591385216 run.py:483] Algo bellman_ford step 5567 current loss 0.050407, current_train_items 178176.
I0302 19:00:19.011329 22683591385216 run.py:483] Algo bellman_ford step 5568 current loss 0.069420, current_train_items 178208.
I0302 19:00:19.043345 22683591385216 run.py:483] Algo bellman_ford step 5569 current loss 0.099137, current_train_items 178240.
I0302 19:00:19.062205 22683591385216 run.py:483] Algo bellman_ford step 5570 current loss 0.007049, current_train_items 178272.
I0302 19:00:19.077516 22683591385216 run.py:483] Algo bellman_ford step 5571 current loss 0.016446, current_train_items 178304.
I0302 19:00:19.099349 22683591385216 run.py:483] Algo bellman_ford step 5572 current loss 0.026545, current_train_items 178336.
I0302 19:00:19.128380 22683591385216 run.py:483] Algo bellman_ford step 5573 current loss 0.045590, current_train_items 178368.
I0302 19:00:19.160239 22683591385216 run.py:483] Algo bellman_ford step 5574 current loss 0.068470, current_train_items 178400.
I0302 19:00:19.179172 22683591385216 run.py:483] Algo bellman_ford step 5575 current loss 0.003197, current_train_items 178432.
I0302 19:00:19.194594 22683591385216 run.py:483] Algo bellman_ford step 5576 current loss 0.021713, current_train_items 178464.
I0302 19:00:19.216172 22683591385216 run.py:483] Algo bellman_ford step 5577 current loss 0.027552, current_train_items 178496.
I0302 19:00:19.245504 22683591385216 run.py:483] Algo bellman_ford step 5578 current loss 0.048393, current_train_items 178528.
I0302 19:00:19.278949 22683591385216 run.py:483] Algo bellman_ford step 5579 current loss 0.058294, current_train_items 178560.
I0302 19:00:19.297214 22683591385216 run.py:483] Algo bellman_ford step 5580 current loss 0.002194, current_train_items 178592.
I0302 19:00:19.312083 22683591385216 run.py:483] Algo bellman_ford step 5581 current loss 0.035484, current_train_items 178624.
I0302 19:00:19.335325 22683591385216 run.py:483] Algo bellman_ford step 5582 current loss 0.030265, current_train_items 178656.
I0302 19:00:19.364526 22683591385216 run.py:483] Algo bellman_ford step 5583 current loss 0.041418, current_train_items 178688.
I0302 19:00:19.396058 22683591385216 run.py:483] Algo bellman_ford step 5584 current loss 0.056010, current_train_items 178720.
I0302 19:00:19.414921 22683591385216 run.py:483] Algo bellman_ford step 5585 current loss 0.005846, current_train_items 178752.
I0302 19:00:19.430555 22683591385216 run.py:483] Algo bellman_ford step 5586 current loss 0.015785, current_train_items 178784.
I0302 19:00:19.453969 22683591385216 run.py:483] Algo bellman_ford step 5587 current loss 0.062004, current_train_items 178816.
I0302 19:00:19.483919 22683591385216 run.py:483] Algo bellman_ford step 5588 current loss 0.107416, current_train_items 178848.
I0302 19:00:19.513432 22683591385216 run.py:483] Algo bellman_ford step 5589 current loss 0.086705, current_train_items 178880.
I0302 19:00:19.532087 22683591385216 run.py:483] Algo bellman_ford step 5590 current loss 0.001275, current_train_items 178912.
I0302 19:00:19.547578 22683591385216 run.py:483] Algo bellman_ford step 5591 current loss 0.006966, current_train_items 178944.
I0302 19:00:19.568754 22683591385216 run.py:483] Algo bellman_ford step 5592 current loss 0.077885, current_train_items 178976.
I0302 19:00:19.597982 22683591385216 run.py:483] Algo bellman_ford step 5593 current loss 0.070208, current_train_items 179008.
I0302 19:00:19.630221 22683591385216 run.py:483] Algo bellman_ford step 5594 current loss 0.051356, current_train_items 179040.
I0302 19:00:19.648524 22683591385216 run.py:483] Algo bellman_ford step 5595 current loss 0.001896, current_train_items 179072.
I0302 19:00:19.663677 22683591385216 run.py:483] Algo bellman_ford step 5596 current loss 0.019544, current_train_items 179104.
I0302 19:00:19.686020 22683591385216 run.py:483] Algo bellman_ford step 5597 current loss 0.076454, current_train_items 179136.
I0302 19:00:19.715311 22683591385216 run.py:483] Algo bellman_ford step 5598 current loss 0.078923, current_train_items 179168.
I0302 19:00:19.743155 22683591385216 run.py:483] Algo bellman_ford step 5599 current loss 0.063328, current_train_items 179200.
I0302 19:00:19.761726 22683591385216 run.py:483] Algo bellman_ford step 5600 current loss 0.001075, current_train_items 179232.
I0302 19:00:19.769533 22683591385216 run.py:503] (val) algo bellman_ford step 5600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 179232, 'step': 5600, 'algorithm': 'bellman_ford'}
I0302 19:00:19.769639 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:19.785483 22683591385216 run.py:483] Algo bellman_ford step 5601 current loss 0.008862, current_train_items 179264.
I0302 19:00:19.808490 22683591385216 run.py:483] Algo bellman_ford step 5602 current loss 0.038677, current_train_items 179296.
I0302 19:00:19.838088 22683591385216 run.py:483] Algo bellman_ford step 5603 current loss 0.066092, current_train_items 179328.
I0302 19:00:19.868537 22683591385216 run.py:483] Algo bellman_ford step 5604 current loss 0.053118, current_train_items 179360.
I0302 19:00:19.887187 22683591385216 run.py:483] Algo bellman_ford step 5605 current loss 0.000419, current_train_items 179392.
I0302 19:00:19.902302 22683591385216 run.py:483] Algo bellman_ford step 5606 current loss 0.016239, current_train_items 179424.
I0302 19:00:19.924887 22683591385216 run.py:483] Algo bellman_ford step 5607 current loss 0.019004, current_train_items 179456.
I0302 19:00:19.952932 22683591385216 run.py:483] Algo bellman_ford step 5608 current loss 0.024562, current_train_items 179488.
I0302 19:00:19.985073 22683591385216 run.py:483] Algo bellman_ford step 5609 current loss 0.134926, current_train_items 179520.
I0302 19:00:20.003590 22683591385216 run.py:483] Algo bellman_ford step 5610 current loss 0.007219, current_train_items 179552.
I0302 19:00:20.019032 22683591385216 run.py:483] Algo bellman_ford step 5611 current loss 0.003531, current_train_items 179584.
I0302 19:00:20.040924 22683591385216 run.py:483] Algo bellman_ford step 5612 current loss 0.044767, current_train_items 179616.
I0302 19:00:20.071146 22683591385216 run.py:483] Algo bellman_ford step 5613 current loss 0.040546, current_train_items 179648.
I0302 19:00:20.101315 22683591385216 run.py:483] Algo bellman_ford step 5614 current loss 0.038914, current_train_items 179680.
I0302 19:00:20.119405 22683591385216 run.py:483] Algo bellman_ford step 5615 current loss 0.001215, current_train_items 179712.
I0302 19:00:20.135019 22683591385216 run.py:483] Algo bellman_ford step 5616 current loss 0.008695, current_train_items 179744.
I0302 19:00:20.157866 22683591385216 run.py:483] Algo bellman_ford step 5617 current loss 0.022423, current_train_items 179776.
I0302 19:00:20.186726 22683591385216 run.py:483] Algo bellman_ford step 5618 current loss 0.049887, current_train_items 179808.
I0302 19:00:20.219421 22683591385216 run.py:483] Algo bellman_ford step 5619 current loss 0.090349, current_train_items 179840.
I0302 19:00:20.237850 22683591385216 run.py:483] Algo bellman_ford step 5620 current loss 0.003298, current_train_items 179872.
I0302 19:00:20.253211 22683591385216 run.py:483] Algo bellman_ford step 5621 current loss 0.013318, current_train_items 179904.
I0302 19:00:20.275460 22683591385216 run.py:483] Algo bellman_ford step 5622 current loss 0.029687, current_train_items 179936.
I0302 19:00:20.304321 22683591385216 run.py:483] Algo bellman_ford step 5623 current loss 0.070005, current_train_items 179968.
I0302 19:00:20.336313 22683591385216 run.py:483] Algo bellman_ford step 5624 current loss 0.063326, current_train_items 180000.
I0302 19:00:20.354353 22683591385216 run.py:483] Algo bellman_ford step 5625 current loss 0.001569, current_train_items 180032.
I0302 19:00:20.370079 22683591385216 run.py:483] Algo bellman_ford step 5626 current loss 0.014621, current_train_items 180064.
I0302 19:00:20.392726 22683591385216 run.py:483] Algo bellman_ford step 5627 current loss 0.031284, current_train_items 180096.
I0302 19:00:20.422164 22683591385216 run.py:483] Algo bellman_ford step 5628 current loss 0.065878, current_train_items 180128.
I0302 19:00:20.454189 22683591385216 run.py:483] Algo bellman_ford step 5629 current loss 0.055621, current_train_items 180160.
I0302 19:00:20.472351 22683591385216 run.py:483] Algo bellman_ford step 5630 current loss 0.001486, current_train_items 180192.
I0302 19:00:20.487447 22683591385216 run.py:483] Algo bellman_ford step 5631 current loss 0.003747, current_train_items 180224.
I0302 19:00:20.510216 22683591385216 run.py:483] Algo bellman_ford step 5632 current loss 0.039257, current_train_items 180256.
I0302 19:00:20.539426 22683591385216 run.py:483] Algo bellman_ford step 5633 current loss 0.050153, current_train_items 180288.
I0302 19:00:20.571382 22683591385216 run.py:483] Algo bellman_ford step 5634 current loss 0.084049, current_train_items 180320.
I0302 19:00:20.589632 22683591385216 run.py:483] Algo bellman_ford step 5635 current loss 0.008180, current_train_items 180352.
I0302 19:00:20.605086 22683591385216 run.py:483] Algo bellman_ford step 5636 current loss 0.004845, current_train_items 180384.
I0302 19:00:20.627288 22683591385216 run.py:483] Algo bellman_ford step 5637 current loss 0.029686, current_train_items 180416.
I0302 19:00:20.655367 22683591385216 run.py:483] Algo bellman_ford step 5638 current loss 0.060178, current_train_items 180448.
I0302 19:00:20.685233 22683591385216 run.py:483] Algo bellman_ford step 5639 current loss 0.083929, current_train_items 180480.
I0302 19:00:20.703719 22683591385216 run.py:483] Algo bellman_ford step 5640 current loss 0.000760, current_train_items 180512.
I0302 19:00:20.718755 22683591385216 run.py:483] Algo bellman_ford step 5641 current loss 0.011208, current_train_items 180544.
I0302 19:00:20.741204 22683591385216 run.py:483] Algo bellman_ford step 5642 current loss 0.048183, current_train_items 180576.
I0302 19:00:20.770189 22683591385216 run.py:483] Algo bellman_ford step 5643 current loss 0.057885, current_train_items 180608.
I0302 19:00:20.800845 22683591385216 run.py:483] Algo bellman_ford step 5644 current loss 0.062218, current_train_items 180640.
I0302 19:00:20.819059 22683591385216 run.py:483] Algo bellman_ford step 5645 current loss 0.000741, current_train_items 180672.
I0302 19:00:20.834826 22683591385216 run.py:483] Algo bellman_ford step 5646 current loss 0.019931, current_train_items 180704.
I0302 19:00:20.856770 22683591385216 run.py:483] Algo bellman_ford step 5647 current loss 0.023711, current_train_items 180736.
I0302 19:00:20.885738 22683591385216 run.py:483] Algo bellman_ford step 5648 current loss 0.048842, current_train_items 180768.
I0302 19:00:20.918837 22683591385216 run.py:483] Algo bellman_ford step 5649 current loss 0.067268, current_train_items 180800.
I0302 19:00:20.936784 22683591385216 run.py:483] Algo bellman_ford step 5650 current loss 0.022692, current_train_items 180832.
I0302 19:00:20.944579 22683591385216 run.py:503] (val) algo bellman_ford step 5650: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 180832, 'step': 5650, 'algorithm': 'bellman_ford'}
I0302 19:00:20.944687 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:20.960767 22683591385216 run.py:483] Algo bellman_ford step 5651 current loss 0.015447, current_train_items 180864.
I0302 19:00:20.983075 22683591385216 run.py:483] Algo bellman_ford step 5652 current loss 0.033960, current_train_items 180896.
I0302 19:00:21.014491 22683591385216 run.py:483] Algo bellman_ford step 5653 current loss 0.053767, current_train_items 180928.
I0302 19:00:21.047637 22683591385216 run.py:483] Algo bellman_ford step 5654 current loss 0.048179, current_train_items 180960.
I0302 19:00:21.066310 22683591385216 run.py:483] Algo bellman_ford step 5655 current loss 0.000642, current_train_items 180992.
I0302 19:00:21.081845 22683591385216 run.py:483] Algo bellman_ford step 5656 current loss 0.006952, current_train_items 181024.
I0302 19:00:21.103662 22683591385216 run.py:483] Algo bellman_ford step 5657 current loss 0.030539, current_train_items 181056.
I0302 19:00:21.133991 22683591385216 run.py:483] Algo bellman_ford step 5658 current loss 0.059023, current_train_items 181088.
I0302 19:00:21.165992 22683591385216 run.py:483] Algo bellman_ford step 5659 current loss 0.063061, current_train_items 181120.
I0302 19:00:21.184809 22683591385216 run.py:483] Algo bellman_ford step 5660 current loss 0.011426, current_train_items 181152.
I0302 19:00:21.200214 22683591385216 run.py:483] Algo bellman_ford step 5661 current loss 0.012633, current_train_items 181184.
I0302 19:00:21.221759 22683591385216 run.py:483] Algo bellman_ford step 5662 current loss 0.032694, current_train_items 181216.
I0302 19:00:21.253025 22683591385216 run.py:483] Algo bellman_ford step 5663 current loss 0.075444, current_train_items 181248.
I0302 19:00:21.284610 22683591385216 run.py:483] Algo bellman_ford step 5664 current loss 0.074428, current_train_items 181280.
I0302 19:00:21.302999 22683591385216 run.py:483] Algo bellman_ford step 5665 current loss 0.019665, current_train_items 181312.
I0302 19:00:21.318669 22683591385216 run.py:483] Algo bellman_ford step 5666 current loss 0.006408, current_train_items 181344.
I0302 19:00:21.340783 22683591385216 run.py:483] Algo bellman_ford step 5667 current loss 0.035864, current_train_items 181376.
I0302 19:00:21.371026 22683591385216 run.py:483] Algo bellman_ford step 5668 current loss 0.073448, current_train_items 181408.
I0302 19:00:21.401717 22683591385216 run.py:483] Algo bellman_ford step 5669 current loss 0.062255, current_train_items 181440.
I0302 19:00:21.420412 22683591385216 run.py:483] Algo bellman_ford step 5670 current loss 0.007811, current_train_items 181472.
I0302 19:00:21.435954 22683591385216 run.py:483] Algo bellman_ford step 5671 current loss 0.006805, current_train_items 181504.
I0302 19:00:21.457875 22683591385216 run.py:483] Algo bellman_ford step 5672 current loss 0.038939, current_train_items 181536.
I0302 19:00:21.486697 22683591385216 run.py:483] Algo bellman_ford step 5673 current loss 0.049959, current_train_items 181568.
I0302 19:00:21.517820 22683591385216 run.py:483] Algo bellman_ford step 5674 current loss 0.034959, current_train_items 181600.
I0302 19:00:21.536442 22683591385216 run.py:483] Algo bellman_ford step 5675 current loss 0.004210, current_train_items 181632.
I0302 19:00:21.551870 22683591385216 run.py:483] Algo bellman_ford step 5676 current loss 0.027738, current_train_items 181664.
I0302 19:00:21.573980 22683591385216 run.py:483] Algo bellman_ford step 5677 current loss 0.069064, current_train_items 181696.
I0302 19:00:21.604694 22683591385216 run.py:483] Algo bellman_ford step 5678 current loss 0.090823, current_train_items 181728.
I0302 19:00:21.635823 22683591385216 run.py:483] Algo bellman_ford step 5679 current loss 0.077895, current_train_items 181760.
I0302 19:00:21.654381 22683591385216 run.py:483] Algo bellman_ford step 5680 current loss 0.001623, current_train_items 181792.
I0302 19:00:21.669824 22683591385216 run.py:483] Algo bellman_ford step 5681 current loss 0.063042, current_train_items 181824.
I0302 19:00:21.692369 22683591385216 run.py:483] Algo bellman_ford step 5682 current loss 0.080035, current_train_items 181856.
I0302 19:00:21.721390 22683591385216 run.py:483] Algo bellman_ford step 5683 current loss 0.124584, current_train_items 181888.
I0302 19:00:21.752030 22683591385216 run.py:483] Algo bellman_ford step 5684 current loss 0.098812, current_train_items 181920.
I0302 19:00:21.770464 22683591385216 run.py:483] Algo bellman_ford step 5685 current loss 0.007297, current_train_items 181952.
I0302 19:00:21.786046 22683591385216 run.py:483] Algo bellman_ford step 5686 current loss 0.060260, current_train_items 181984.
I0302 19:00:21.808397 22683591385216 run.py:483] Algo bellman_ford step 5687 current loss 0.065589, current_train_items 182016.
I0302 19:00:21.837781 22683591385216 run.py:483] Algo bellman_ford step 5688 current loss 0.066624, current_train_items 182048.
I0302 19:00:21.868575 22683591385216 run.py:483] Algo bellman_ford step 5689 current loss 0.062078, current_train_items 182080.
I0302 19:00:21.887280 22683591385216 run.py:483] Algo bellman_ford step 5690 current loss 0.001378, current_train_items 182112.
I0302 19:00:21.903434 22683591385216 run.py:483] Algo bellman_ford step 5691 current loss 0.025209, current_train_items 182144.
I0302 19:00:21.925319 22683591385216 run.py:483] Algo bellman_ford step 5692 current loss 0.034494, current_train_items 182176.
I0302 19:00:21.954511 22683591385216 run.py:483] Algo bellman_ford step 5693 current loss 0.048981, current_train_items 182208.
I0302 19:00:21.984687 22683591385216 run.py:483] Algo bellman_ford step 5694 current loss 0.079125, current_train_items 182240.
I0302 19:00:22.003203 22683591385216 run.py:483] Algo bellman_ford step 5695 current loss 0.001769, current_train_items 182272.
I0302 19:00:22.018872 22683591385216 run.py:483] Algo bellman_ford step 5696 current loss 0.030889, current_train_items 182304.
I0302 19:00:22.041127 22683591385216 run.py:483] Algo bellman_ford step 5697 current loss 0.070993, current_train_items 182336.
I0302 19:00:22.071605 22683591385216 run.py:483] Algo bellman_ford step 5698 current loss 0.078032, current_train_items 182368.
I0302 19:00:22.103508 22683591385216 run.py:483] Algo bellman_ford step 5699 current loss 0.074541, current_train_items 182400.
I0302 19:00:22.122026 22683591385216 run.py:483] Algo bellman_ford step 5700 current loss 0.011007, current_train_items 182432.
I0302 19:00:22.129816 22683591385216 run.py:503] (val) algo bellman_ford step 5700: {'pi': 0.9765625, 'score': 0.9765625, 'examples_seen': 182432, 'step': 5700, 'algorithm': 'bellman_ford'}
I0302 19:00:22.129929 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.977, val scores are: bellman_ford: 0.977
I0302 19:00:22.145389 22683591385216 run.py:483] Algo bellman_ford step 5701 current loss 0.012748, current_train_items 182464.
I0302 19:00:22.167393 22683591385216 run.py:483] Algo bellman_ford step 5702 current loss 0.031793, current_train_items 182496.
I0302 19:00:22.197223 22683591385216 run.py:483] Algo bellman_ford step 5703 current loss 0.065845, current_train_items 182528.
I0302 19:00:22.230359 22683591385216 run.py:483] Algo bellman_ford step 5704 current loss 0.094946, current_train_items 182560.
I0302 19:00:22.249008 22683591385216 run.py:483] Algo bellman_ford step 5705 current loss 0.003526, current_train_items 182592.
I0302 19:00:22.264136 22683591385216 run.py:483] Algo bellman_ford step 5706 current loss 0.005265, current_train_items 182624.
I0302 19:00:22.287470 22683591385216 run.py:483] Algo bellman_ford step 5707 current loss 0.039347, current_train_items 182656.
I0302 19:00:22.316334 22683591385216 run.py:483] Algo bellman_ford step 5708 current loss 0.054907, current_train_items 182688.
I0302 19:00:22.347632 22683591385216 run.py:483] Algo bellman_ford step 5709 current loss 0.070067, current_train_items 182720.
I0302 19:00:22.365878 22683591385216 run.py:483] Algo bellman_ford step 5710 current loss 0.001662, current_train_items 182752.
I0302 19:00:22.381077 22683591385216 run.py:483] Algo bellman_ford step 5711 current loss 0.014427, current_train_items 182784.
I0302 19:00:22.402813 22683591385216 run.py:483] Algo bellman_ford step 5712 current loss 0.037901, current_train_items 182816.
I0302 19:00:22.431529 22683591385216 run.py:483] Algo bellman_ford step 5713 current loss 0.052784, current_train_items 182848.
I0302 19:00:22.462521 22683591385216 run.py:483] Algo bellman_ford step 5714 current loss 0.033408, current_train_items 182880.
I0302 19:00:22.481129 22683591385216 run.py:483] Algo bellman_ford step 5715 current loss 0.001559, current_train_items 182912.
I0302 19:00:22.496429 22683591385216 run.py:483] Algo bellman_ford step 5716 current loss 0.017732, current_train_items 182944.
I0302 19:00:22.519536 22683591385216 run.py:483] Algo bellman_ford step 5717 current loss 0.019382, current_train_items 182976.
I0302 19:00:22.548325 22683591385216 run.py:483] Algo bellman_ford step 5718 current loss 0.080880, current_train_items 183008.
I0302 19:00:22.580210 22683591385216 run.py:483] Algo bellman_ford step 5719 current loss 0.054433, current_train_items 183040.
I0302 19:00:22.598502 22683591385216 run.py:483] Algo bellman_ford step 5720 current loss 0.002619, current_train_items 183072.
I0302 19:00:22.613829 22683591385216 run.py:483] Algo bellman_ford step 5721 current loss 0.009043, current_train_items 183104.
I0302 19:00:22.635832 22683591385216 run.py:483] Algo bellman_ford step 5722 current loss 0.058477, current_train_items 183136.
I0302 19:00:22.664687 22683591385216 run.py:483] Algo bellman_ford step 5723 current loss 0.075393, current_train_items 183168.
I0302 19:00:22.695130 22683591385216 run.py:483] Algo bellman_ford step 5724 current loss 0.076925, current_train_items 183200.
I0302 19:00:22.713464 22683591385216 run.py:483] Algo bellman_ford step 5725 current loss 0.002771, current_train_items 183232.
I0302 19:00:22.728795 22683591385216 run.py:483] Algo bellman_ford step 5726 current loss 0.020523, current_train_items 183264.
I0302 19:00:22.751880 22683591385216 run.py:483] Algo bellman_ford step 5727 current loss 0.067624, current_train_items 183296.
I0302 19:00:22.780338 22683591385216 run.py:483] Algo bellman_ford step 5728 current loss 0.062504, current_train_items 183328.
I0302 19:00:22.811886 22683591385216 run.py:483] Algo bellman_ford step 5729 current loss 0.098414, current_train_items 183360.
I0302 19:00:22.830044 22683591385216 run.py:483] Algo bellman_ford step 5730 current loss 0.002463, current_train_items 183392.
I0302 19:00:22.845709 22683591385216 run.py:483] Algo bellman_ford step 5731 current loss 0.006918, current_train_items 183424.
I0302 19:00:22.868686 22683591385216 run.py:483] Algo bellman_ford step 5732 current loss 0.066374, current_train_items 183456.
I0302 19:00:22.897837 22683591385216 run.py:483] Algo bellman_ford step 5733 current loss 0.104501, current_train_items 183488.
I0302 19:00:22.930020 22683591385216 run.py:483] Algo bellman_ford step 5734 current loss 0.074083, current_train_items 183520.
I0302 19:00:22.948312 22683591385216 run.py:483] Algo bellman_ford step 5735 current loss 0.001016, current_train_items 183552.
I0302 19:00:22.963835 22683591385216 run.py:483] Algo bellman_ford step 5736 current loss 0.014612, current_train_items 183584.
I0302 19:00:22.986377 22683591385216 run.py:483] Algo bellman_ford step 5737 current loss 0.038102, current_train_items 183616.
I0302 19:00:23.015037 22683591385216 run.py:483] Algo bellman_ford step 5738 current loss 0.022170, current_train_items 183648.
I0302 19:00:23.046787 22683591385216 run.py:483] Algo bellman_ford step 5739 current loss 0.068440, current_train_items 183680.
I0302 19:00:23.065039 22683591385216 run.py:483] Algo bellman_ford step 5740 current loss 0.056075, current_train_items 183712.
I0302 19:00:23.080716 22683591385216 run.py:483] Algo bellman_ford step 5741 current loss 0.006484, current_train_items 183744.
I0302 19:00:23.104293 22683591385216 run.py:483] Algo bellman_ford step 5742 current loss 0.034132, current_train_items 183776.
I0302 19:00:23.134467 22683591385216 run.py:483] Algo bellman_ford step 5743 current loss 0.055284, current_train_items 183808.
I0302 19:00:23.166570 22683591385216 run.py:483] Algo bellman_ford step 5744 current loss 0.045824, current_train_items 183840.
I0302 19:00:23.184893 22683591385216 run.py:483] Algo bellman_ford step 5745 current loss 0.031503, current_train_items 183872.
I0302 19:00:23.200670 22683591385216 run.py:483] Algo bellman_ford step 5746 current loss 0.027348, current_train_items 183904.
I0302 19:00:23.223313 22683591385216 run.py:483] Algo bellman_ford step 5747 current loss 0.066579, current_train_items 183936.
I0302 19:00:23.252491 22683591385216 run.py:483] Algo bellman_ford step 5748 current loss 0.055195, current_train_items 183968.
I0302 19:00:23.283284 22683591385216 run.py:483] Algo bellman_ford step 5749 current loss 0.045582, current_train_items 184000.
I0302 19:00:23.301672 22683591385216 run.py:483] Algo bellman_ford step 5750 current loss 0.003444, current_train_items 184032.
I0302 19:00:23.309542 22683591385216 run.py:503] (val) algo bellman_ford step 5750: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 184032, 'step': 5750, 'algorithm': 'bellman_ford'}
I0302 19:00:23.309651 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:23.325994 22683591385216 run.py:483] Algo bellman_ford step 5751 current loss 0.006655, current_train_items 184064.
I0302 19:00:23.348109 22683591385216 run.py:483] Algo bellman_ford step 5752 current loss 0.061667, current_train_items 184096.
I0302 19:00:23.376504 22683591385216 run.py:483] Algo bellman_ford step 5753 current loss 0.042178, current_train_items 184128.
I0302 19:00:23.409283 22683591385216 run.py:483] Algo bellman_ford step 5754 current loss 0.068462, current_train_items 184160.
I0302 19:00:23.427843 22683591385216 run.py:483] Algo bellman_ford step 5755 current loss 0.001173, current_train_items 184192.
I0302 19:00:23.443428 22683591385216 run.py:483] Algo bellman_ford step 5756 current loss 0.010995, current_train_items 184224.
I0302 19:00:23.466270 22683591385216 run.py:483] Algo bellman_ford step 5757 current loss 0.060076, current_train_items 184256.
I0302 19:00:23.496220 22683591385216 run.py:483] Algo bellman_ford step 5758 current loss 0.057298, current_train_items 184288.
I0302 19:00:23.527565 22683591385216 run.py:483] Algo bellman_ford step 5759 current loss 0.025918, current_train_items 184320.
I0302 19:00:23.546277 22683591385216 run.py:483] Algo bellman_ford step 5760 current loss 0.015169, current_train_items 184352.
I0302 19:00:23.562201 22683591385216 run.py:483] Algo bellman_ford step 5761 current loss 0.023421, current_train_items 184384.
I0302 19:00:23.584263 22683591385216 run.py:483] Algo bellman_ford step 5762 current loss 0.029719, current_train_items 184416.
I0302 19:00:23.612429 22683591385216 run.py:483] Algo bellman_ford step 5763 current loss 0.060961, current_train_items 184448.
I0302 19:00:23.644411 22683591385216 run.py:483] Algo bellman_ford step 5764 current loss 0.040436, current_train_items 184480.
I0302 19:00:23.663079 22683591385216 run.py:483] Algo bellman_ford step 5765 current loss 0.002476, current_train_items 184512.
I0302 19:00:23.678742 22683591385216 run.py:483] Algo bellman_ford step 5766 current loss 0.008988, current_train_items 184544.
I0302 19:00:23.701363 22683591385216 run.py:483] Algo bellman_ford step 5767 current loss 0.037275, current_train_items 184576.
I0302 19:00:23.731978 22683591385216 run.py:483] Algo bellman_ford step 5768 current loss 0.049646, current_train_items 184608.
I0302 19:00:23.763294 22683591385216 run.py:483] Algo bellman_ford step 5769 current loss 0.066026, current_train_items 184640.
I0302 19:00:23.782162 22683591385216 run.py:483] Algo bellman_ford step 5770 current loss 0.000609, current_train_items 184672.
I0302 19:00:23.798181 22683591385216 run.py:483] Algo bellman_ford step 5771 current loss 0.034368, current_train_items 184704.
I0302 19:00:23.818944 22683591385216 run.py:483] Algo bellman_ford step 5772 current loss 0.028437, current_train_items 184736.
I0302 19:00:23.847686 22683591385216 run.py:483] Algo bellman_ford step 5773 current loss 0.048559, current_train_items 184768.
I0302 19:00:23.878023 22683591385216 run.py:483] Algo bellman_ford step 5774 current loss 0.054875, current_train_items 184800.
I0302 19:00:23.896224 22683591385216 run.py:483] Algo bellman_ford step 5775 current loss 0.000836, current_train_items 184832.
I0302 19:00:23.912286 22683591385216 run.py:483] Algo bellman_ford step 5776 current loss 0.042101, current_train_items 184864.
I0302 19:00:23.933879 22683591385216 run.py:483] Algo bellman_ford step 5777 current loss 0.024468, current_train_items 184896.
I0302 19:00:23.963226 22683591385216 run.py:483] Algo bellman_ford step 5778 current loss 0.045492, current_train_items 184928.
I0302 19:00:23.995065 22683591385216 run.py:483] Algo bellman_ford step 5779 current loss 0.090219, current_train_items 184960.
I0302 19:00:24.012982 22683591385216 run.py:483] Algo bellman_ford step 5780 current loss 0.004035, current_train_items 184992.
I0302 19:00:24.028381 22683591385216 run.py:483] Algo bellman_ford step 5781 current loss 0.049053, current_train_items 185024.
I0302 19:00:24.051196 22683591385216 run.py:483] Algo bellman_ford step 5782 current loss 0.034815, current_train_items 185056.
I0302 19:00:24.079299 22683591385216 run.py:483] Algo bellman_ford step 5783 current loss 0.020613, current_train_items 185088.
I0302 19:00:24.108410 22683591385216 run.py:483] Algo bellman_ford step 5784 current loss 0.076983, current_train_items 185120.
I0302 19:00:24.126762 22683591385216 run.py:483] Algo bellman_ford step 5785 current loss 0.003991, current_train_items 185152.
I0302 19:00:24.142063 22683591385216 run.py:483] Algo bellman_ford step 5786 current loss 0.019058, current_train_items 185184.
I0302 19:00:24.163226 22683591385216 run.py:483] Algo bellman_ford step 5787 current loss 0.063246, current_train_items 185216.
I0302 19:00:24.193067 22683591385216 run.py:483] Algo bellman_ford step 5788 current loss 0.066807, current_train_items 185248.
I0302 19:00:24.223083 22683591385216 run.py:483] Algo bellman_ford step 5789 current loss 0.150612, current_train_items 185280.
I0302 19:00:24.241499 22683591385216 run.py:483] Algo bellman_ford step 5790 current loss 0.012583, current_train_items 185312.
I0302 19:00:24.257893 22683591385216 run.py:483] Algo bellman_ford step 5791 current loss 0.019169, current_train_items 185344.
I0302 19:00:24.278730 22683591385216 run.py:483] Algo bellman_ford step 5792 current loss 0.044211, current_train_items 185376.
I0302 19:00:24.307570 22683591385216 run.py:483] Algo bellman_ford step 5793 current loss 0.048035, current_train_items 185408.
I0302 19:00:24.338545 22683591385216 run.py:483] Algo bellman_ford step 5794 current loss 0.039853, current_train_items 185440.
I0302 19:00:24.356577 22683591385216 run.py:483] Algo bellman_ford step 5795 current loss 0.002334, current_train_items 185472.
I0302 19:00:24.371737 22683591385216 run.py:483] Algo bellman_ford step 5796 current loss 0.009360, current_train_items 185504.
I0302 19:00:24.394057 22683591385216 run.py:483] Algo bellman_ford step 5797 current loss 0.022027, current_train_items 185536.
I0302 19:00:24.423043 22683591385216 run.py:483] Algo bellman_ford step 5798 current loss 0.050160, current_train_items 185568.
I0302 19:00:24.454588 22683591385216 run.py:483] Algo bellman_ford step 5799 current loss 0.070353, current_train_items 185600.
I0302 19:00:24.472878 22683591385216 run.py:483] Algo bellman_ford step 5800 current loss 0.001697, current_train_items 185632.
I0302 19:00:24.480956 22683591385216 run.py:503] (val) algo bellman_ford step 5800: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 185632, 'step': 5800, 'algorithm': 'bellman_ford'}
I0302 19:00:24.481062 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:00:24.497093 22683591385216 run.py:483] Algo bellman_ford step 5801 current loss 0.019212, current_train_items 185664.
I0302 19:00:24.518717 22683591385216 run.py:483] Algo bellman_ford step 5802 current loss 0.012084, current_train_items 185696.
I0302 19:00:24.548257 22683591385216 run.py:483] Algo bellman_ford step 5803 current loss 0.039187, current_train_items 185728.
I0302 19:00:24.581537 22683591385216 run.py:483] Algo bellman_ford step 5804 current loss 0.061981, current_train_items 185760.
I0302 19:00:24.600093 22683591385216 run.py:483] Algo bellman_ford step 5805 current loss 0.002601, current_train_items 185792.
I0302 19:00:24.615617 22683591385216 run.py:483] Algo bellman_ford step 5806 current loss 0.013000, current_train_items 185824.
I0302 19:00:24.638731 22683591385216 run.py:483] Algo bellman_ford step 5807 current loss 0.040617, current_train_items 185856.
I0302 19:00:24.668082 22683591385216 run.py:483] Algo bellman_ford step 5808 current loss 0.054702, current_train_items 185888.
I0302 19:00:24.699195 22683591385216 run.py:483] Algo bellman_ford step 5809 current loss 0.049815, current_train_items 185920.
I0302 19:00:24.717491 22683591385216 run.py:483] Algo bellman_ford step 5810 current loss 0.001208, current_train_items 185952.
I0302 19:00:24.732660 22683591385216 run.py:483] Algo bellman_ford step 5811 current loss 0.008952, current_train_items 185984.
I0302 19:00:24.755529 22683591385216 run.py:483] Algo bellman_ford step 5812 current loss 0.116874, current_train_items 186016.
I0302 19:00:24.785474 22683591385216 run.py:483] Algo bellman_ford step 5813 current loss 0.125391, current_train_items 186048.
I0302 19:00:24.817723 22683591385216 run.py:483] Algo bellman_ford step 5814 current loss 0.071032, current_train_items 186080.
I0302 19:00:24.835856 22683591385216 run.py:483] Algo bellman_ford step 5815 current loss 0.001203, current_train_items 186112.
I0302 19:00:24.851352 22683591385216 run.py:483] Algo bellman_ford step 5816 current loss 0.031694, current_train_items 186144.
I0302 19:00:24.873545 22683591385216 run.py:483] Algo bellman_ford step 5817 current loss 0.085085, current_train_items 186176.
I0302 19:00:24.902067 22683591385216 run.py:483] Algo bellman_ford step 5818 current loss 0.069144, current_train_items 186208.
I0302 19:00:24.934014 22683591385216 run.py:483] Algo bellman_ford step 5819 current loss 0.121544, current_train_items 186240.
I0302 19:00:24.951892 22683591385216 run.py:483] Algo bellman_ford step 5820 current loss 0.001940, current_train_items 186272.
I0302 19:00:24.967173 22683591385216 run.py:483] Algo bellman_ford step 5821 current loss 0.003743, current_train_items 186304.
I0302 19:00:24.989602 22683591385216 run.py:483] Algo bellman_ford step 5822 current loss 0.032118, current_train_items 186336.
I0302 19:00:25.018918 22683591385216 run.py:483] Algo bellman_ford step 5823 current loss 0.088793, current_train_items 186368.
I0302 19:00:25.049636 22683591385216 run.py:483] Algo bellman_ford step 5824 current loss 0.096296, current_train_items 186400.
I0302 19:00:25.067841 22683591385216 run.py:483] Algo bellman_ford step 5825 current loss 0.002105, current_train_items 186432.
I0302 19:00:25.083286 22683591385216 run.py:483] Algo bellman_ford step 5826 current loss 0.007777, current_train_items 186464.
I0302 19:00:25.106267 22683591385216 run.py:483] Algo bellman_ford step 5827 current loss 0.050687, current_train_items 186496.
I0302 19:00:25.136142 22683591385216 run.py:483] Algo bellman_ford step 5828 current loss 0.046751, current_train_items 186528.
I0302 19:00:25.168682 22683591385216 run.py:483] Algo bellman_ford step 5829 current loss 0.082327, current_train_items 186560.
I0302 19:00:25.186892 22683591385216 run.py:483] Algo bellman_ford step 5830 current loss 0.003868, current_train_items 186592.
I0302 19:00:25.202427 22683591385216 run.py:483] Algo bellman_ford step 5831 current loss 0.043922, current_train_items 186624.
I0302 19:00:25.224196 22683591385216 run.py:483] Algo bellman_ford step 5832 current loss 0.092702, current_train_items 186656.
I0302 19:00:25.253331 22683591385216 run.py:483] Algo bellman_ford step 5833 current loss 0.080680, current_train_items 186688.
I0302 19:00:25.281417 22683591385216 run.py:483] Algo bellman_ford step 5834 current loss 0.057576, current_train_items 186720.
I0302 19:00:25.299582 22683591385216 run.py:483] Algo bellman_ford step 5835 current loss 0.022496, current_train_items 186752.
I0302 19:00:25.314191 22683591385216 run.py:483] Algo bellman_ford step 5836 current loss 0.005416, current_train_items 186784.
I0302 19:00:25.337838 22683591385216 run.py:483] Algo bellman_ford step 5837 current loss 0.077920, current_train_items 186816.
I0302 19:00:25.365412 22683591385216 run.py:483] Algo bellman_ford step 5838 current loss 0.054965, current_train_items 186848.
I0302 19:00:25.398067 22683591385216 run.py:483] Algo bellman_ford step 5839 current loss 0.099646, current_train_items 186880.
I0302 19:00:25.416301 22683591385216 run.py:483] Algo bellman_ford step 5840 current loss 0.006996, current_train_items 186912.
I0302 19:00:25.431838 22683591385216 run.py:483] Algo bellman_ford step 5841 current loss 0.017132, current_train_items 186944.
I0302 19:00:25.453428 22683591385216 run.py:483] Algo bellman_ford step 5842 current loss 0.052507, current_train_items 186976.
I0302 19:00:25.481174 22683591385216 run.py:483] Algo bellman_ford step 5843 current loss 0.072062, current_train_items 187008.
I0302 19:00:25.511876 22683591385216 run.py:483] Algo bellman_ford step 5844 current loss 0.059426, current_train_items 187040.
I0302 19:00:25.530070 22683591385216 run.py:483] Algo bellman_ford step 5845 current loss 0.005054, current_train_items 187072.
I0302 19:00:25.545460 22683591385216 run.py:483] Algo bellman_ford step 5846 current loss 0.010736, current_train_items 187104.
I0302 19:00:25.568729 22683591385216 run.py:483] Algo bellman_ford step 5847 current loss 0.070005, current_train_items 187136.
I0302 19:00:25.596405 22683591385216 run.py:483] Algo bellman_ford step 5848 current loss 0.054220, current_train_items 187168.
I0302 19:00:25.628114 22683591385216 run.py:483] Algo bellman_ford step 5849 current loss 0.071868, current_train_items 187200.
I0302 19:00:25.645947 22683591385216 run.py:483] Algo bellman_ford step 5850 current loss 0.006921, current_train_items 187232.
I0302 19:00:25.653987 22683591385216 run.py:503] (val) algo bellman_ford step 5850: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 187232, 'step': 5850, 'algorithm': 'bellman_ford'}
I0302 19:00:25.654093 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 19:00:25.670137 22683591385216 run.py:483] Algo bellman_ford step 5851 current loss 0.009945, current_train_items 187264.
I0302 19:00:25.692361 22683591385216 run.py:483] Algo bellman_ford step 5852 current loss 0.109830, current_train_items 187296.
I0302 19:00:25.720858 22683591385216 run.py:483] Algo bellman_ford step 5853 current loss 0.075215, current_train_items 187328.
I0302 19:00:25.753369 22683591385216 run.py:483] Algo bellman_ford step 5854 current loss 0.099979, current_train_items 187360.
I0302 19:00:25.771972 22683591385216 run.py:483] Algo bellman_ford step 5855 current loss 0.001666, current_train_items 187392.
I0302 19:00:25.787276 22683591385216 run.py:483] Algo bellman_ford step 5856 current loss 0.025006, current_train_items 187424.
I0302 19:00:25.810188 22683591385216 run.py:483] Algo bellman_ford step 5857 current loss 0.024349, current_train_items 187456.
I0302 19:00:25.838256 22683591385216 run.py:483] Algo bellman_ford step 5858 current loss 0.032216, current_train_items 187488.
I0302 19:00:25.869969 22683591385216 run.py:483] Algo bellman_ford step 5859 current loss 0.085478, current_train_items 187520.
I0302 19:00:25.888905 22683591385216 run.py:483] Algo bellman_ford step 5860 current loss 0.002302, current_train_items 187552.
I0302 19:00:25.904735 22683591385216 run.py:483] Algo bellman_ford step 5861 current loss 0.007816, current_train_items 187584.
I0302 19:00:25.926152 22683591385216 run.py:483] Algo bellman_ford step 5862 current loss 0.031860, current_train_items 187616.
I0302 19:00:25.955650 22683591385216 run.py:483] Algo bellman_ford step 5863 current loss 0.027717, current_train_items 187648.
I0302 19:00:25.987806 22683591385216 run.py:483] Algo bellman_ford step 5864 current loss 0.071068, current_train_items 187680.
I0302 19:00:26.005930 22683591385216 run.py:483] Algo bellman_ford step 5865 current loss 0.002296, current_train_items 187712.
I0302 19:00:26.021814 22683591385216 run.py:483] Algo bellman_ford step 5866 current loss 0.027916, current_train_items 187744.
I0302 19:00:26.044219 22683591385216 run.py:483] Algo bellman_ford step 5867 current loss 0.024829, current_train_items 187776.
I0302 19:00:26.072841 22683591385216 run.py:483] Algo bellman_ford step 5868 current loss 0.091726, current_train_items 187808.
I0302 19:00:26.105683 22683591385216 run.py:483] Algo bellman_ford step 5869 current loss 0.055352, current_train_items 187840.
I0302 19:00:26.124748 22683591385216 run.py:483] Algo bellman_ford step 5870 current loss 0.002234, current_train_items 187872.
I0302 19:00:26.140366 22683591385216 run.py:483] Algo bellman_ford step 5871 current loss 0.030886, current_train_items 187904.
I0302 19:00:26.161570 22683591385216 run.py:483] Algo bellman_ford step 5872 current loss 0.021574, current_train_items 187936.
I0302 19:00:26.190268 22683591385216 run.py:483] Algo bellman_ford step 5873 current loss 0.042652, current_train_items 187968.
I0302 19:00:26.221852 22683591385216 run.py:483] Algo bellman_ford step 5874 current loss 0.050283, current_train_items 188000.
I0302 19:00:26.240652 22683591385216 run.py:483] Algo bellman_ford step 5875 current loss 0.001882, current_train_items 188032.
I0302 19:00:26.256184 22683591385216 run.py:483] Algo bellman_ford step 5876 current loss 0.015565, current_train_items 188064.
I0302 19:00:26.277665 22683591385216 run.py:483] Algo bellman_ford step 5877 current loss 0.027021, current_train_items 188096.
I0302 19:00:26.306363 22683591385216 run.py:483] Algo bellman_ford step 5878 current loss 0.039195, current_train_items 188128.
I0302 19:00:26.338582 22683591385216 run.py:483] Algo bellman_ford step 5879 current loss 0.042725, current_train_items 188160.
I0302 19:00:26.356875 22683591385216 run.py:483] Algo bellman_ford step 5880 current loss 0.008543, current_train_items 188192.
I0302 19:00:26.372047 22683591385216 run.py:483] Algo bellman_ford step 5881 current loss 0.015868, current_train_items 188224.
I0302 19:00:26.395092 22683591385216 run.py:483] Algo bellman_ford step 5882 current loss 0.072786, current_train_items 188256.
I0302 19:00:26.424160 22683591385216 run.py:483] Algo bellman_ford step 5883 current loss 0.022852, current_train_items 188288.
I0302 19:00:26.456795 22683591385216 run.py:483] Algo bellman_ford step 5884 current loss 0.128620, current_train_items 188320.
I0302 19:00:26.475618 22683591385216 run.py:483] Algo bellman_ford step 5885 current loss 0.000942, current_train_items 188352.
I0302 19:00:26.491002 22683591385216 run.py:483] Algo bellman_ford step 5886 current loss 0.013050, current_train_items 188384.
I0302 19:00:26.512793 22683591385216 run.py:483] Algo bellman_ford step 5887 current loss 0.029526, current_train_items 188416.
I0302 19:00:26.541433 22683591385216 run.py:483] Algo bellman_ford step 5888 current loss 0.038790, current_train_items 188448.
I0302 19:00:26.571627 22683591385216 run.py:483] Algo bellman_ford step 5889 current loss 0.089828, current_train_items 188480.
I0302 19:00:26.590440 22683591385216 run.py:483] Algo bellman_ford step 5890 current loss 0.000681, current_train_items 188512.
I0302 19:00:26.606011 22683591385216 run.py:483] Algo bellman_ford step 5891 current loss 0.021205, current_train_items 188544.
I0302 19:00:26.627957 22683591385216 run.py:483] Algo bellman_ford step 5892 current loss 0.022571, current_train_items 188576.
I0302 19:00:26.656448 22683591385216 run.py:483] Algo bellman_ford step 5893 current loss 0.014438, current_train_items 188608.
I0302 19:00:26.685853 22683591385216 run.py:483] Algo bellman_ford step 5894 current loss 0.037961, current_train_items 188640.
I0302 19:00:26.704205 22683591385216 run.py:483] Algo bellman_ford step 5895 current loss 0.001702, current_train_items 188672.
I0302 19:00:26.719571 22683591385216 run.py:483] Algo bellman_ford step 5896 current loss 0.043675, current_train_items 188704.
I0302 19:00:26.742630 22683591385216 run.py:483] Algo bellman_ford step 5897 current loss 0.044349, current_train_items 188736.
I0302 19:00:26.772362 22683591385216 run.py:483] Algo bellman_ford step 5898 current loss 0.043566, current_train_items 188768.
I0302 19:00:26.803386 22683591385216 run.py:483] Algo bellman_ford step 5899 current loss 0.038467, current_train_items 188800.
I0302 19:00:26.821878 22683591385216 run.py:483] Algo bellman_ford step 5900 current loss 0.002094, current_train_items 188832.
I0302 19:00:26.829677 22683591385216 run.py:503] (val) algo bellman_ford step 5900: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 188832, 'step': 5900, 'algorithm': 'bellman_ford'}
I0302 19:00:26.829783 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:26.845371 22683591385216 run.py:483] Algo bellman_ford step 5901 current loss 0.014935, current_train_items 188864.
I0302 19:00:26.868472 22683591385216 run.py:483] Algo bellman_ford step 5902 current loss 0.016348, current_train_items 188896.
I0302 19:00:26.898310 22683591385216 run.py:483] Algo bellman_ford step 5903 current loss 0.048112, current_train_items 188928.
I0302 19:00:26.928323 22683591385216 run.py:483] Algo bellman_ford step 5904 current loss 0.034173, current_train_items 188960.
I0302 19:00:26.946814 22683591385216 run.py:483] Algo bellman_ford step 5905 current loss 0.002806, current_train_items 188992.
I0302 19:00:26.962795 22683591385216 run.py:483] Algo bellman_ford step 5906 current loss 0.023881, current_train_items 189024.
I0302 19:00:26.984775 22683591385216 run.py:483] Algo bellman_ford step 5907 current loss 0.027162, current_train_items 189056.
I0302 19:00:27.014345 22683591385216 run.py:483] Algo bellman_ford step 5908 current loss 0.025268, current_train_items 189088.
I0302 19:00:27.044434 22683591385216 run.py:483] Algo bellman_ford step 5909 current loss 0.082545, current_train_items 189120.
I0302 19:00:27.063053 22683591385216 run.py:483] Algo bellman_ford step 5910 current loss 0.002705, current_train_items 189152.
I0302 19:00:27.078668 22683591385216 run.py:483] Algo bellman_ford step 5911 current loss 0.018253, current_train_items 189184.
I0302 19:00:27.101333 22683591385216 run.py:483] Algo bellman_ford step 5912 current loss 0.046789, current_train_items 189216.
I0302 19:00:27.129785 22683591385216 run.py:483] Algo bellman_ford step 5913 current loss 0.081835, current_train_items 189248.
I0302 19:00:27.160257 22683591385216 run.py:483] Algo bellman_ford step 5914 current loss 0.068484, current_train_items 189280.
I0302 19:00:27.178309 22683591385216 run.py:483] Algo bellman_ford step 5915 current loss 0.006925, current_train_items 189312.
I0302 19:00:27.193801 22683591385216 run.py:483] Algo bellman_ford step 5916 current loss 0.024937, current_train_items 189344.
I0302 19:00:27.215958 22683591385216 run.py:483] Algo bellman_ford step 5917 current loss 0.045282, current_train_items 189376.
I0302 19:00:27.244557 22683591385216 run.py:483] Algo bellman_ford step 5918 current loss 0.021874, current_train_items 189408.
I0302 19:00:27.276565 22683591385216 run.py:483] Algo bellman_ford step 5919 current loss 0.066758, current_train_items 189440.
I0302 19:00:27.294950 22683591385216 run.py:483] Algo bellman_ford step 5920 current loss 0.004547, current_train_items 189472.
I0302 19:00:27.310348 22683591385216 run.py:483] Algo bellman_ford step 5921 current loss 0.021480, current_train_items 189504.
I0302 19:00:27.331979 22683591385216 run.py:483] Algo bellman_ford step 5922 current loss 0.024614, current_train_items 189536.
I0302 19:00:27.359608 22683591385216 run.py:483] Algo bellman_ford step 5923 current loss 0.020828, current_train_items 189568.
I0302 19:00:27.390578 22683591385216 run.py:483] Algo bellman_ford step 5924 current loss 0.086323, current_train_items 189600.
I0302 19:00:27.408384 22683591385216 run.py:483] Algo bellman_ford step 5925 current loss 0.001431, current_train_items 189632.
I0302 19:00:27.423326 22683591385216 run.py:483] Algo bellman_ford step 5926 current loss 0.004000, current_train_items 189664.
I0302 19:00:27.444891 22683591385216 run.py:483] Algo bellman_ford step 5927 current loss 0.055227, current_train_items 189696.
I0302 19:00:27.474473 22683591385216 run.py:483] Algo bellman_ford step 5928 current loss 0.070655, current_train_items 189728.
I0302 19:00:27.504627 22683591385216 run.py:483] Algo bellman_ford step 5929 current loss 0.030913, current_train_items 189760.
I0302 19:00:27.522798 22683591385216 run.py:483] Algo bellman_ford step 5930 current loss 0.001434, current_train_items 189792.
I0302 19:00:27.538206 22683591385216 run.py:483] Algo bellman_ford step 5931 current loss 0.013858, current_train_items 189824.
I0302 19:00:27.560337 22683591385216 run.py:483] Algo bellman_ford step 5932 current loss 0.041182, current_train_items 189856.
I0302 19:00:27.588140 22683591385216 run.py:483] Algo bellman_ford step 5933 current loss 0.039815, current_train_items 189888.
I0302 19:00:27.620431 22683591385216 run.py:483] Algo bellman_ford step 5934 current loss 0.068495, current_train_items 189920.
I0302 19:00:27.638950 22683591385216 run.py:483] Algo bellman_ford step 5935 current loss 0.017323, current_train_items 189952.
I0302 19:00:27.653846 22683591385216 run.py:483] Algo bellman_ford step 5936 current loss 0.006008, current_train_items 189984.
I0302 19:00:27.676074 22683591385216 run.py:483] Algo bellman_ford step 5937 current loss 0.079568, current_train_items 190016.
I0302 19:00:27.705994 22683591385216 run.py:483] Algo bellman_ford step 5938 current loss 0.095292, current_train_items 190048.
I0302 19:00:27.735716 22683591385216 run.py:483] Algo bellman_ford step 5939 current loss 0.082712, current_train_items 190080.
I0302 19:00:27.754302 22683591385216 run.py:483] Algo bellman_ford step 5940 current loss 0.002163, current_train_items 190112.
I0302 19:00:27.769355 22683591385216 run.py:483] Algo bellman_ford step 5941 current loss 0.013632, current_train_items 190144.
I0302 19:00:27.791761 22683591385216 run.py:483] Algo bellman_ford step 5942 current loss 0.018970, current_train_items 190176.
I0302 19:00:27.820931 22683591385216 run.py:483] Algo bellman_ford step 5943 current loss 0.039143, current_train_items 190208.
I0302 19:00:27.852749 22683591385216 run.py:483] Algo bellman_ford step 5944 current loss 0.077683, current_train_items 190240.
I0302 19:00:27.871248 22683591385216 run.py:483] Algo bellman_ford step 5945 current loss 0.001926, current_train_items 190272.
I0302 19:00:27.886749 22683591385216 run.py:483] Algo bellman_ford step 5946 current loss 0.011647, current_train_items 190304.
I0302 19:00:27.909396 22683591385216 run.py:483] Algo bellman_ford step 5947 current loss 0.032850, current_train_items 190336.
I0302 19:00:27.937550 22683591385216 run.py:483] Algo bellman_ford step 5948 current loss 0.024127, current_train_items 190368.
I0302 19:00:27.968857 22683591385216 run.py:483] Algo bellman_ford step 5949 current loss 0.062310, current_train_items 190400.
I0302 19:00:27.987278 22683591385216 run.py:483] Algo bellman_ford step 5950 current loss 0.001567, current_train_items 190432.
I0302 19:00:27.995271 22683591385216 run.py:503] (val) algo bellman_ford step 5950: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 190432, 'step': 5950, 'algorithm': 'bellman_ford'}
I0302 19:00:27.995376 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:00:28.011257 22683591385216 run.py:483] Algo bellman_ford step 5951 current loss 0.004064, current_train_items 190464.
I0302 19:00:28.034231 22683591385216 run.py:483] Algo bellman_ford step 5952 current loss 0.040909, current_train_items 190496.
I0302 19:00:28.063727 22683591385216 run.py:483] Algo bellman_ford step 5953 current loss 0.038096, current_train_items 190528.
I0302 19:00:28.095498 22683591385216 run.py:483] Algo bellman_ford step 5954 current loss 0.091534, current_train_items 190560.
I0302 19:00:28.114251 22683591385216 run.py:483] Algo bellman_ford step 5955 current loss 0.001425, current_train_items 190592.
I0302 19:00:28.129948 22683591385216 run.py:483] Algo bellman_ford step 5956 current loss 0.020871, current_train_items 190624.
I0302 19:00:28.151338 22683591385216 run.py:483] Algo bellman_ford step 5957 current loss 0.043943, current_train_items 190656.
I0302 19:00:28.179925 22683591385216 run.py:483] Algo bellman_ford step 5958 current loss 0.033395, current_train_items 190688.
I0302 19:00:28.210993 22683591385216 run.py:483] Algo bellman_ford step 5959 current loss 0.044813, current_train_items 190720.
I0302 19:00:28.229772 22683591385216 run.py:483] Algo bellman_ford step 5960 current loss 0.001188, current_train_items 190752.
I0302 19:00:28.245418 22683591385216 run.py:483] Algo bellman_ford step 5961 current loss 0.013006, current_train_items 190784.
I0302 19:00:28.267808 22683591385216 run.py:483] Algo bellman_ford step 5962 current loss 0.042775, current_train_items 190816.
I0302 19:00:28.295043 22683591385216 run.py:483] Algo bellman_ford step 5963 current loss 0.030012, current_train_items 190848.
I0302 19:00:28.326044 22683591385216 run.py:483] Algo bellman_ford step 5964 current loss 0.071884, current_train_items 190880.
I0302 19:00:28.344283 22683591385216 run.py:483] Algo bellman_ford step 5965 current loss 0.024930, current_train_items 190912.
I0302 19:00:28.359328 22683591385216 run.py:483] Algo bellman_ford step 5966 current loss 0.004651, current_train_items 190944.
I0302 19:00:28.381979 22683591385216 run.py:483] Algo bellman_ford step 5967 current loss 0.047186, current_train_items 190976.
I0302 19:00:28.410529 22683591385216 run.py:483] Algo bellman_ford step 5968 current loss 0.089571, current_train_items 191008.
I0302 19:00:28.442428 22683591385216 run.py:483] Algo bellman_ford step 5969 current loss 0.085935, current_train_items 191040.
I0302 19:00:28.461051 22683591385216 run.py:483] Algo bellman_ford step 5970 current loss 0.001471, current_train_items 191072.
I0302 19:00:28.476567 22683591385216 run.py:483] Algo bellman_ford step 5971 current loss 0.008985, current_train_items 191104.
I0302 19:00:28.498415 22683591385216 run.py:483] Algo bellman_ford step 5972 current loss 0.029852, current_train_items 191136.
I0302 19:00:28.528141 22683591385216 run.py:483] Algo bellman_ford step 5973 current loss 0.076933, current_train_items 191168.
I0302 19:00:28.559227 22683591385216 run.py:483] Algo bellman_ford step 5974 current loss 0.069335, current_train_items 191200.
I0302 19:00:28.577918 22683591385216 run.py:483] Algo bellman_ford step 5975 current loss 0.004922, current_train_items 191232.
I0302 19:00:28.593163 22683591385216 run.py:483] Algo bellman_ford step 5976 current loss 0.002491, current_train_items 191264.
I0302 19:00:28.614505 22683591385216 run.py:483] Algo bellman_ford step 5977 current loss 0.048556, current_train_items 191296.
I0302 19:00:28.643406 22683591385216 run.py:483] Algo bellman_ford step 5978 current loss 0.082812, current_train_items 191328.
I0302 19:00:28.673589 22683591385216 run.py:483] Algo bellman_ford step 5979 current loss 0.023311, current_train_items 191360.
I0302 19:00:28.691590 22683591385216 run.py:483] Algo bellman_ford step 5980 current loss 0.001036, current_train_items 191392.
I0302 19:00:28.707560 22683591385216 run.py:483] Algo bellman_ford step 5981 current loss 0.025471, current_train_items 191424.
I0302 19:00:28.729282 22683591385216 run.py:483] Algo bellman_ford step 5982 current loss 0.018030, current_train_items 191456.
I0302 19:00:28.757806 22683591385216 run.py:483] Algo bellman_ford step 5983 current loss 0.025242, current_train_items 191488.
I0302 19:00:28.788045 22683591385216 run.py:483] Algo bellman_ford step 5984 current loss 0.041381, current_train_items 191520.
I0302 19:00:28.806557 22683591385216 run.py:483] Algo bellman_ford step 5985 current loss 0.006568, current_train_items 191552.
I0302 19:00:28.822625 22683591385216 run.py:483] Algo bellman_ford step 5986 current loss 0.013761, current_train_items 191584.
I0302 19:00:28.844481 22683591385216 run.py:483] Algo bellman_ford step 5987 current loss 0.028772, current_train_items 191616.
I0302 19:00:28.872599 22683591385216 run.py:483] Algo bellman_ford step 5988 current loss 0.044548, current_train_items 191648.
I0302 19:00:28.903046 22683591385216 run.py:483] Algo bellman_ford step 5989 current loss 0.050896, current_train_items 191680.
I0302 19:00:28.921666 22683591385216 run.py:483] Algo bellman_ford step 5990 current loss 0.001237, current_train_items 191712.
I0302 19:00:28.936597 22683591385216 run.py:483] Algo bellman_ford step 5991 current loss 0.024476, current_train_items 191744.
I0302 19:00:28.958804 22683591385216 run.py:483] Algo bellman_ford step 5992 current loss 0.054378, current_train_items 191776.
I0302 19:00:28.989372 22683591385216 run.py:483] Algo bellman_ford step 5993 current loss 0.072156, current_train_items 191808.
I0302 19:00:29.020673 22683591385216 run.py:483] Algo bellman_ford step 5994 current loss 0.055883, current_train_items 191840.
I0302 19:00:29.038604 22683591385216 run.py:483] Algo bellman_ford step 5995 current loss 0.001670, current_train_items 191872.
I0302 19:00:29.053841 22683591385216 run.py:483] Algo bellman_ford step 5996 current loss 0.010381, current_train_items 191904.
I0302 19:00:29.076688 22683591385216 run.py:483] Algo bellman_ford step 5997 current loss 0.028019, current_train_items 191936.
I0302 19:00:29.105472 22683591385216 run.py:483] Algo bellman_ford step 5998 current loss 0.050276, current_train_items 191968.
I0302 19:00:29.135470 22683591385216 run.py:483] Algo bellman_ford step 5999 current loss 0.037878, current_train_items 192000.
I0302 19:00:29.154449 22683591385216 run.py:483] Algo bellman_ford step 6000 current loss 0.015672, current_train_items 192032.
I0302 19:00:29.162237 22683591385216 run.py:503] (val) algo bellman_ford step 6000: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 192032, 'step': 6000, 'algorithm': 'bellman_ford'}
I0302 19:00:29.162343 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:00:29.178271 22683591385216 run.py:483] Algo bellman_ford step 6001 current loss 0.033458, current_train_items 192064.
I0302 19:00:29.200849 22683591385216 run.py:483] Algo bellman_ford step 6002 current loss 0.061823, current_train_items 192096.
I0302 19:00:29.228580 22683591385216 run.py:483] Algo bellman_ford step 6003 current loss 0.041125, current_train_items 192128.
I0302 19:00:29.259121 22683591385216 run.py:483] Algo bellman_ford step 6004 current loss 0.034826, current_train_items 192160.
I0302 19:00:29.277531 22683591385216 run.py:483] Algo bellman_ford step 6005 current loss 0.001737, current_train_items 192192.
I0302 19:00:29.292738 22683591385216 run.py:483] Algo bellman_ford step 6006 current loss 0.009867, current_train_items 192224.
I0302 19:00:29.315800 22683591385216 run.py:483] Algo bellman_ford step 6007 current loss 0.060422, current_train_items 192256.
I0302 19:00:29.346569 22683591385216 run.py:483] Algo bellman_ford step 6008 current loss 0.078292, current_train_items 192288.
I0302 19:00:29.379185 22683591385216 run.py:483] Algo bellman_ford step 6009 current loss 0.029581, current_train_items 192320.
I0302 19:00:29.397209 22683591385216 run.py:483] Algo bellman_ford step 6010 current loss 0.001219, current_train_items 192352.
I0302 19:00:29.412529 22683591385216 run.py:483] Algo bellman_ford step 6011 current loss 0.012910, current_train_items 192384.
I0302 19:00:29.434973 22683591385216 run.py:483] Algo bellman_ford step 6012 current loss 0.039140, current_train_items 192416.
I0302 19:00:29.463034 22683591385216 run.py:483] Algo bellman_ford step 6013 current loss 0.055170, current_train_items 192448.
I0302 19:00:29.496087 22683591385216 run.py:483] Algo bellman_ford step 6014 current loss 0.120357, current_train_items 192480.
I0302 19:00:29.514072 22683591385216 run.py:483] Algo bellman_ford step 6015 current loss 0.003968, current_train_items 192512.
I0302 19:00:29.529534 22683591385216 run.py:483] Algo bellman_ford step 6016 current loss 0.011467, current_train_items 192544.
I0302 19:00:29.551187 22683591385216 run.py:483] Algo bellman_ford step 6017 current loss 0.044675, current_train_items 192576.
I0302 19:00:29.580380 22683591385216 run.py:483] Algo bellman_ford step 6018 current loss 0.080026, current_train_items 192608.
I0302 19:00:29.611329 22683591385216 run.py:483] Algo bellman_ford step 6019 current loss 0.081231, current_train_items 192640.
I0302 19:00:29.629414 22683591385216 run.py:483] Algo bellman_ford step 6020 current loss 0.001926, current_train_items 192672.
I0302 19:00:29.645059 22683591385216 run.py:483] Algo bellman_ford step 6021 current loss 0.005051, current_train_items 192704.
I0302 19:00:29.667556 22683591385216 run.py:483] Algo bellman_ford step 6022 current loss 0.015545, current_train_items 192736.
I0302 19:00:29.698351 22683591385216 run.py:483] Algo bellman_ford step 6023 current loss 0.061125, current_train_items 192768.
I0302 19:00:29.729921 22683591385216 run.py:483] Algo bellman_ford step 6024 current loss 0.066783, current_train_items 192800.
I0302 19:00:29.747750 22683591385216 run.py:483] Algo bellman_ford step 6025 current loss 0.000985, current_train_items 192832.
I0302 19:00:29.763249 22683591385216 run.py:483] Algo bellman_ford step 6026 current loss 0.012631, current_train_items 192864.
I0302 19:00:29.785381 22683591385216 run.py:483] Algo bellman_ford step 6027 current loss 0.028680, current_train_items 192896.
I0302 19:00:29.814160 22683591385216 run.py:483] Algo bellman_ford step 6028 current loss 0.080933, current_train_items 192928.
I0302 19:00:29.845211 22683591385216 run.py:483] Algo bellman_ford step 6029 current loss 0.145244, current_train_items 192960.
I0302 19:00:29.862971 22683591385216 run.py:483] Algo bellman_ford step 6030 current loss 0.003051, current_train_items 192992.
I0302 19:00:29.878025 22683591385216 run.py:483] Algo bellman_ford step 6031 current loss 0.024797, current_train_items 193024.
I0302 19:00:29.899968 22683591385216 run.py:483] Algo bellman_ford step 6032 current loss 0.017428, current_train_items 193056.
I0302 19:00:29.930469 22683591385216 run.py:483] Algo bellman_ford step 6033 current loss 0.056069, current_train_items 193088.
I0302 19:00:29.961182 22683591385216 run.py:483] Algo bellman_ford step 6034 current loss 0.040309, current_train_items 193120.
I0302 19:00:29.979107 22683591385216 run.py:483] Algo bellman_ford step 6035 current loss 0.001472, current_train_items 193152.
I0302 19:00:29.994286 22683591385216 run.py:483] Algo bellman_ford step 6036 current loss 0.016622, current_train_items 193184.
I0302 19:00:30.016601 22683591385216 run.py:483] Algo bellman_ford step 6037 current loss 0.018121, current_train_items 193216.
I0302 19:00:30.046145 22683591385216 run.py:483] Algo bellman_ford step 6038 current loss 0.106415, current_train_items 193248.
I0302 19:00:30.076118 22683591385216 run.py:483] Algo bellman_ford step 6039 current loss 0.038989, current_train_items 193280.
I0302 19:00:30.093908 22683591385216 run.py:483] Algo bellman_ford step 6040 current loss 0.002986, current_train_items 193312.
I0302 19:00:30.109212 22683591385216 run.py:483] Algo bellman_ford step 6041 current loss 0.024995, current_train_items 193344.
I0302 19:00:30.132441 22683591385216 run.py:483] Algo bellman_ford step 6042 current loss 0.048149, current_train_items 193376.
I0302 19:00:30.161242 22683591385216 run.py:483] Algo bellman_ford step 6043 current loss 0.060421, current_train_items 193408.
I0302 19:00:30.195051 22683591385216 run.py:483] Algo bellman_ford step 6044 current loss 0.067575, current_train_items 193440.
I0302 19:00:30.213217 22683591385216 run.py:483] Algo bellman_ford step 6045 current loss 0.003515, current_train_items 193472.
I0302 19:00:30.228765 22683591385216 run.py:483] Algo bellman_ford step 6046 current loss 0.031431, current_train_items 193504.
I0302 19:00:30.250554 22683591385216 run.py:483] Algo bellman_ford step 6047 current loss 0.064420, current_train_items 193536.
I0302 19:00:30.280713 22683591385216 run.py:483] Algo bellman_ford step 6048 current loss 0.054182, current_train_items 193568.
I0302 19:00:30.312771 22683591385216 run.py:483] Algo bellman_ford step 6049 current loss 0.045029, current_train_items 193600.
I0302 19:00:30.330716 22683591385216 run.py:483] Algo bellman_ford step 6050 current loss 0.001736, current_train_items 193632.
I0302 19:00:30.338683 22683591385216 run.py:503] (val) algo bellman_ford step 6050: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 193632, 'step': 6050, 'algorithm': 'bellman_ford'}
I0302 19:00:30.338814 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:00:30.354577 22683591385216 run.py:483] Algo bellman_ford step 6051 current loss 0.008555, current_train_items 193664.
I0302 19:00:30.377741 22683591385216 run.py:483] Algo bellman_ford step 6052 current loss 0.050212, current_train_items 193696.
I0302 19:00:30.407028 22683591385216 run.py:483] Algo bellman_ford step 6053 current loss 0.034136, current_train_items 193728.
I0302 19:00:30.439970 22683591385216 run.py:483] Algo bellman_ford step 6054 current loss 0.039349, current_train_items 193760.
I0302 19:00:30.458487 22683591385216 run.py:483] Algo bellman_ford step 6055 current loss 0.020092, current_train_items 193792.
I0302 19:00:30.473433 22683591385216 run.py:483] Algo bellman_ford step 6056 current loss 0.026235, current_train_items 193824.
I0302 19:00:30.495013 22683591385216 run.py:483] Algo bellman_ford step 6057 current loss 0.077491, current_train_items 193856.
I0302 19:00:30.523587 22683591385216 run.py:483] Algo bellman_ford step 6058 current loss 0.044569, current_train_items 193888.
I0302 19:00:30.552112 22683591385216 run.py:483] Algo bellman_ford step 6059 current loss 0.049506, current_train_items 193920.
I0302 19:00:30.571086 22683591385216 run.py:483] Algo bellman_ford step 6060 current loss 0.001186, current_train_items 193952.
I0302 19:00:30.586758 22683591385216 run.py:483] Algo bellman_ford step 6061 current loss 0.012019, current_train_items 193984.
I0302 19:00:30.607295 22683591385216 run.py:483] Algo bellman_ford step 6062 current loss 0.020190, current_train_items 194016.
I0302 19:00:30.634915 22683591385216 run.py:483] Algo bellman_ford step 6063 current loss 0.034437, current_train_items 194048.
I0302 19:00:30.666596 22683591385216 run.py:483] Algo bellman_ford step 6064 current loss 0.077907, current_train_items 194080.
I0302 19:00:30.684946 22683591385216 run.py:483] Algo bellman_ford step 6065 current loss 0.003365, current_train_items 194112.
I0302 19:00:30.700247 22683591385216 run.py:483] Algo bellman_ford step 6066 current loss 0.019754, current_train_items 194144.
I0302 19:00:30.723309 22683591385216 run.py:483] Algo bellman_ford step 6067 current loss 0.060405, current_train_items 194176.
I0302 19:00:30.752954 22683591385216 run.py:483] Algo bellman_ford step 6068 current loss 0.062856, current_train_items 194208.
I0302 19:00:30.783541 22683591385216 run.py:483] Algo bellman_ford step 6069 current loss 0.070217, current_train_items 194240.
I0302 19:00:30.802103 22683591385216 run.py:483] Algo bellman_ford step 6070 current loss 0.002215, current_train_items 194272.
I0302 19:00:30.817514 22683591385216 run.py:483] Algo bellman_ford step 6071 current loss 0.016872, current_train_items 194304.
I0302 19:00:30.840057 22683591385216 run.py:483] Algo bellman_ford step 6072 current loss 0.031019, current_train_items 194336.
I0302 19:00:30.868462 22683591385216 run.py:483] Algo bellman_ford step 6073 current loss 0.051131, current_train_items 194368.
I0302 19:00:30.900065 22683591385216 run.py:483] Algo bellman_ford step 6074 current loss 0.034173, current_train_items 194400.
I0302 19:00:30.918598 22683591385216 run.py:483] Algo bellman_ford step 6075 current loss 0.002504, current_train_items 194432.
I0302 19:00:30.934088 22683591385216 run.py:483] Algo bellman_ford step 6076 current loss 0.022377, current_train_items 194464.
I0302 19:00:30.954853 22683591385216 run.py:483] Algo bellman_ford step 6077 current loss 0.026097, current_train_items 194496.
I0302 19:00:30.983532 22683591385216 run.py:483] Algo bellman_ford step 6078 current loss 0.056301, current_train_items 194528.
I0302 19:00:31.015558 22683591385216 run.py:483] Algo bellman_ford step 6079 current loss 0.122186, current_train_items 194560.
I0302 19:00:31.033555 22683591385216 run.py:483] Algo bellman_ford step 6080 current loss 0.005040, current_train_items 194592.
I0302 19:00:31.049001 22683591385216 run.py:483] Algo bellman_ford step 6081 current loss 0.005548, current_train_items 194624.
I0302 19:00:31.071166 22683591385216 run.py:483] Algo bellman_ford step 6082 current loss 0.023177, current_train_items 194656.
I0302 19:00:31.101588 22683591385216 run.py:483] Algo bellman_ford step 6083 current loss 0.049944, current_train_items 194688.
I0302 19:00:31.131397 22683591385216 run.py:483] Algo bellman_ford step 6084 current loss 0.052136, current_train_items 194720.
I0302 19:00:31.149993 22683591385216 run.py:483] Algo bellman_ford step 6085 current loss 0.002465, current_train_items 194752.
I0302 19:00:31.165766 22683591385216 run.py:483] Algo bellman_ford step 6086 current loss 0.013988, current_train_items 194784.
I0302 19:00:31.187572 22683591385216 run.py:483] Algo bellman_ford step 6087 current loss 0.031992, current_train_items 194816.
I0302 19:00:31.216260 22683591385216 run.py:483] Algo bellman_ford step 6088 current loss 0.052305, current_train_items 194848.
I0302 19:00:31.248326 22683591385216 run.py:483] Algo bellman_ford step 6089 current loss 0.096789, current_train_items 194880.
I0302 19:00:31.267320 22683591385216 run.py:483] Algo bellman_ford step 6090 current loss 0.003254, current_train_items 194912.
I0302 19:00:31.283052 22683591385216 run.py:483] Algo bellman_ford step 6091 current loss 0.006299, current_train_items 194944.
I0302 19:00:31.304941 22683591385216 run.py:483] Algo bellman_ford step 6092 current loss 0.080535, current_train_items 194976.
I0302 19:00:31.333412 22683591385216 run.py:483] Algo bellman_ford step 6093 current loss 0.081670, current_train_items 195008.
I0302 19:00:31.363985 22683591385216 run.py:483] Algo bellman_ford step 6094 current loss 0.078632, current_train_items 195040.
I0302 19:00:31.382274 22683591385216 run.py:483] Algo bellman_ford step 6095 current loss 0.001457, current_train_items 195072.
I0302 19:00:31.397655 22683591385216 run.py:483] Algo bellman_ford step 6096 current loss 0.009358, current_train_items 195104.
I0302 19:00:31.420357 22683591385216 run.py:483] Algo bellman_ford step 6097 current loss 0.062112, current_train_items 195136.
I0302 19:00:31.448466 22683591385216 run.py:483] Algo bellman_ford step 6098 current loss 0.057400, current_train_items 195168.
I0302 19:00:31.479692 22683591385216 run.py:483] Algo bellman_ford step 6099 current loss 0.060428, current_train_items 195200.
I0302 19:00:31.498353 22683591385216 run.py:483] Algo bellman_ford step 6100 current loss 0.001651, current_train_items 195232.
I0302 19:00:31.506232 22683591385216 run.py:503] (val) algo bellman_ford step 6100: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 195232, 'step': 6100, 'algorithm': 'bellman_ford'}
I0302 19:00:31.506338 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:00:31.521644 22683591385216 run.py:483] Algo bellman_ford step 6101 current loss 0.007980, current_train_items 195264.
I0302 19:00:31.545077 22683591385216 run.py:483] Algo bellman_ford step 6102 current loss 0.028354, current_train_items 195296.
I0302 19:00:31.575617 22683591385216 run.py:483] Algo bellman_ford step 6103 current loss 0.050912, current_train_items 195328.
I0302 19:00:31.606175 22683591385216 run.py:483] Algo bellman_ford step 6104 current loss 0.049381, current_train_items 195360.
I0302 19:00:31.624489 22683591385216 run.py:483] Algo bellman_ford step 6105 current loss 0.001643, current_train_items 195392.
I0302 19:00:31.639260 22683591385216 run.py:483] Algo bellman_ford step 6106 current loss 0.016319, current_train_items 195424.
I0302 19:00:31.661662 22683591385216 run.py:483] Algo bellman_ford step 6107 current loss 0.021557, current_train_items 195456.
I0302 19:00:31.690871 22683591385216 run.py:483] Algo bellman_ford step 6108 current loss 0.070519, current_train_items 195488.
I0302 19:00:31.721298 22683591385216 run.py:483] Algo bellman_ford step 6109 current loss 0.079737, current_train_items 195520.
I0302 19:00:31.739412 22683591385216 run.py:483] Algo bellman_ford step 6110 current loss 0.002759, current_train_items 195552.
I0302 19:00:31.754606 22683591385216 run.py:483] Algo bellman_ford step 6111 current loss 0.018367, current_train_items 195584.
I0302 19:00:31.776662 22683591385216 run.py:483] Algo bellman_ford step 6112 current loss 0.038456, current_train_items 195616.
I0302 19:00:31.805218 22683591385216 run.py:483] Algo bellman_ford step 6113 current loss 0.030255, current_train_items 195648.
I0302 19:00:31.836824 22683591385216 run.py:483] Algo bellman_ford step 6114 current loss 0.051175, current_train_items 195680.
I0302 19:00:31.855216 22683591385216 run.py:483] Algo bellman_ford step 6115 current loss 0.024693, current_train_items 195712.
I0302 19:00:31.870443 22683591385216 run.py:483] Algo bellman_ford step 6116 current loss 0.033671, current_train_items 195744.
I0302 19:00:31.892841 22683591385216 run.py:483] Algo bellman_ford step 6117 current loss 0.041755, current_train_items 195776.
I0302 19:00:31.923165 22683591385216 run.py:483] Algo bellman_ford step 6118 current loss 0.042729, current_train_items 195808.
I0302 19:00:31.953550 22683591385216 run.py:483] Algo bellman_ford step 6119 current loss 0.044087, current_train_items 195840.
I0302 19:00:31.971604 22683591385216 run.py:483] Algo bellman_ford step 6120 current loss 0.011783, current_train_items 195872.
I0302 19:00:31.987083 22683591385216 run.py:483] Algo bellman_ford step 6121 current loss 0.006835, current_train_items 195904.
I0302 19:00:32.008893 22683591385216 run.py:483] Algo bellman_ford step 6122 current loss 0.021204, current_train_items 195936.
I0302 19:00:32.038917 22683591385216 run.py:483] Algo bellman_ford step 6123 current loss 0.061057, current_train_items 195968.
I0302 19:00:32.070044 22683591385216 run.py:483] Algo bellman_ford step 6124 current loss 0.073852, current_train_items 196000.
I0302 19:00:32.088091 22683591385216 run.py:483] Algo bellman_ford step 6125 current loss 0.002243, current_train_items 196032.
I0302 19:00:32.102917 22683591385216 run.py:483] Algo bellman_ford step 6126 current loss 0.026753, current_train_items 196064.
I0302 19:00:32.126929 22683591385216 run.py:483] Algo bellman_ford step 6127 current loss 0.056953, current_train_items 196096.
I0302 19:00:32.156101 22683591385216 run.py:483] Algo bellman_ford step 6128 current loss 0.038722, current_train_items 196128.
I0302 19:00:32.186977 22683591385216 run.py:483] Algo bellman_ford step 6129 current loss 0.063823, current_train_items 196160.
I0302 19:00:32.205138 22683591385216 run.py:483] Algo bellman_ford step 6130 current loss 0.001867, current_train_items 196192.
I0302 19:00:32.220352 22683591385216 run.py:483] Algo bellman_ford step 6131 current loss 0.010486, current_train_items 196224.
I0302 19:00:32.243373 22683591385216 run.py:483] Algo bellman_ford step 6132 current loss 0.034225, current_train_items 196256.
I0302 19:00:32.271472 22683591385216 run.py:483] Algo bellman_ford step 6133 current loss 0.033221, current_train_items 196288.
I0302 19:00:32.301700 22683591385216 run.py:483] Algo bellman_ford step 6134 current loss 0.046564, current_train_items 196320.
I0302 19:00:32.319613 22683591385216 run.py:483] Algo bellman_ford step 6135 current loss 0.019310, current_train_items 196352.
I0302 19:00:32.334444 22683591385216 run.py:483] Algo bellman_ford step 6136 current loss 0.013735, current_train_items 196384.
I0302 19:00:32.356871 22683591385216 run.py:483] Algo bellman_ford step 6137 current loss 0.057807, current_train_items 196416.
I0302 19:00:32.386858 22683591385216 run.py:483] Algo bellman_ford step 6138 current loss 0.049181, current_train_items 196448.
I0302 19:00:32.418177 22683591385216 run.py:483] Algo bellman_ford step 6139 current loss 0.039763, current_train_items 196480.
I0302 19:00:32.436312 22683591385216 run.py:483] Algo bellman_ford step 6140 current loss 0.002018, current_train_items 196512.
I0302 19:00:32.451548 22683591385216 run.py:483] Algo bellman_ford step 6141 current loss 0.033025, current_train_items 196544.
I0302 19:00:32.474415 22683591385216 run.py:483] Algo bellman_ford step 6142 current loss 0.044625, current_train_items 196576.
I0302 19:00:32.503766 22683591385216 run.py:483] Algo bellman_ford step 6143 current loss 0.048185, current_train_items 196608.
I0302 19:00:32.534906 22683591385216 run.py:483] Algo bellman_ford step 6144 current loss 0.047607, current_train_items 196640.
I0302 19:00:32.552877 22683591385216 run.py:483] Algo bellman_ford step 6145 current loss 0.003949, current_train_items 196672.
I0302 19:00:32.568209 22683591385216 run.py:483] Algo bellman_ford step 6146 current loss 0.010478, current_train_items 196704.
I0302 19:00:32.590028 22683591385216 run.py:483] Algo bellman_ford step 6147 current loss 0.013114, current_train_items 196736.
I0302 19:00:32.619008 22683591385216 run.py:483] Algo bellman_ford step 6148 current loss 0.035210, current_train_items 196768.
I0302 19:00:32.651477 22683591385216 run.py:483] Algo bellman_ford step 6149 current loss 0.059372, current_train_items 196800.
I0302 19:00:32.669790 22683591385216 run.py:483] Algo bellman_ford step 6150 current loss 0.001288, current_train_items 196832.
I0302 19:00:32.677863 22683591385216 run.py:503] (val) algo bellman_ford step 6150: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 196832, 'step': 6150, 'algorithm': 'bellman_ford'}
I0302 19:00:32.677980 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:00:32.693806 22683591385216 run.py:483] Algo bellman_ford step 6151 current loss 0.012587, current_train_items 196864.
I0302 19:00:32.715657 22683591385216 run.py:483] Algo bellman_ford step 6152 current loss 0.010513, current_train_items 196896.
I0302 19:00:32.743524 22683591385216 run.py:483] Algo bellman_ford step 6153 current loss 0.078669, current_train_items 196928.
I0302 19:00:32.775051 22683591385216 run.py:483] Algo bellman_ford step 6154 current loss 0.074562, current_train_items 196960.
I0302 19:00:32.793517 22683591385216 run.py:483] Algo bellman_ford step 6155 current loss 0.003601, current_train_items 196992.
I0302 19:00:32.808904 22683591385216 run.py:483] Algo bellman_ford step 6156 current loss 0.064334, current_train_items 197024.
I0302 19:00:32.830965 22683591385216 run.py:483] Algo bellman_ford step 6157 current loss 0.032622, current_train_items 197056.
I0302 19:00:32.859225 22683591385216 run.py:483] Algo bellman_ford step 6158 current loss 0.090715, current_train_items 197088.
I0302 19:00:32.890306 22683591385216 run.py:483] Algo bellman_ford step 6159 current loss 0.037702, current_train_items 197120.
I0302 19:00:32.909005 22683591385216 run.py:483] Algo bellman_ford step 6160 current loss 0.017612, current_train_items 197152.
I0302 19:00:32.924924 22683591385216 run.py:483] Algo bellman_ford step 6161 current loss 0.019350, current_train_items 197184.
I0302 19:00:32.946839 22683591385216 run.py:483] Algo bellman_ford step 6162 current loss 0.071588, current_train_items 197216.
I0302 19:00:32.977367 22683591385216 run.py:483] Algo bellman_ford step 6163 current loss 0.063428, current_train_items 197248.
I0302 19:00:33.009182 22683591385216 run.py:483] Algo bellman_ford step 6164 current loss 0.109723, current_train_items 197280.
I0302 19:00:33.027262 22683591385216 run.py:483] Algo bellman_ford step 6165 current loss 0.004763, current_train_items 197312.
I0302 19:00:33.042667 22683591385216 run.py:483] Algo bellman_ford step 6166 current loss 0.018154, current_train_items 197344.
I0302 19:00:33.065344 22683591385216 run.py:483] Algo bellman_ford step 6167 current loss 0.061978, current_train_items 197376.
I0302 19:00:33.095146 22683591385216 run.py:483] Algo bellman_ford step 6168 current loss 0.036731, current_train_items 197408.
I0302 19:00:33.126761 22683591385216 run.py:483] Algo bellman_ford step 6169 current loss 0.036296, current_train_items 197440.
I0302 19:00:33.145833 22683591385216 run.py:483] Algo bellman_ford step 6170 current loss 0.004479, current_train_items 197472.
I0302 19:00:33.161224 22683591385216 run.py:483] Algo bellman_ford step 6171 current loss 0.019001, current_train_items 197504.
I0302 19:00:33.183680 22683591385216 run.py:483] Algo bellman_ford step 6172 current loss 0.033080, current_train_items 197536.
I0302 19:00:33.213800 22683591385216 run.py:483] Algo bellman_ford step 6173 current loss 0.042137, current_train_items 197568.
I0302 19:00:33.244879 22683591385216 run.py:483] Algo bellman_ford step 6174 current loss 0.056061, current_train_items 197600.
I0302 19:00:33.263633 22683591385216 run.py:483] Algo bellman_ford step 6175 current loss 0.002537, current_train_items 197632.
I0302 19:00:33.279423 22683591385216 run.py:483] Algo bellman_ford step 6176 current loss 0.009093, current_train_items 197664.
I0302 19:00:33.301006 22683591385216 run.py:483] Algo bellman_ford step 6177 current loss 0.080346, current_train_items 197696.
I0302 19:00:33.330299 22683591385216 run.py:483] Algo bellman_ford step 6178 current loss 0.037486, current_train_items 197728.
I0302 19:00:33.361812 22683591385216 run.py:483] Algo bellman_ford step 6179 current loss 0.053565, current_train_items 197760.
I0302 19:00:33.379832 22683591385216 run.py:483] Algo bellman_ford step 6180 current loss 0.003896, current_train_items 197792.
I0302 19:00:33.395208 22683591385216 run.py:483] Algo bellman_ford step 6181 current loss 0.013003, current_train_items 197824.
I0302 19:00:33.417648 22683591385216 run.py:483] Algo bellman_ford step 6182 current loss 0.073179, current_train_items 197856.
I0302 19:00:33.446918 22683591385216 run.py:483] Algo bellman_ford step 6183 current loss 0.119347, current_train_items 197888.
I0302 19:00:33.480323 22683591385216 run.py:483] Algo bellman_ford step 6184 current loss 0.119484, current_train_items 197920.
I0302 19:00:33.499189 22683591385216 run.py:483] Algo bellman_ford step 6185 current loss 0.005534, current_train_items 197952.
I0302 19:00:33.514982 22683591385216 run.py:483] Algo bellman_ford step 6186 current loss 0.039501, current_train_items 197984.
I0302 19:00:33.536358 22683591385216 run.py:483] Algo bellman_ford step 6187 current loss 0.047104, current_train_items 198016.
I0302 19:00:33.564891 22683591385216 run.py:483] Algo bellman_ford step 6188 current loss 0.021219, current_train_items 198048.
I0302 19:00:33.596627 22683591385216 run.py:483] Algo bellman_ford step 6189 current loss 0.039323, current_train_items 198080.
I0302 19:00:33.615627 22683591385216 run.py:483] Algo bellman_ford step 6190 current loss 0.002907, current_train_items 198112.
I0302 19:00:33.631403 22683591385216 run.py:483] Algo bellman_ford step 6191 current loss 0.027454, current_train_items 198144.
I0302 19:00:33.653935 22683591385216 run.py:483] Algo bellman_ford step 6192 current loss 0.021447, current_train_items 198176.
I0302 19:00:33.683620 22683591385216 run.py:483] Algo bellman_ford step 6193 current loss 0.063487, current_train_items 198208.
I0302 19:00:33.713934 22683591385216 run.py:483] Algo bellman_ford step 6194 current loss 0.057581, current_train_items 198240.
I0302 19:00:33.732209 22683591385216 run.py:483] Algo bellman_ford step 6195 current loss 0.003370, current_train_items 198272.
I0302 19:00:33.747008 22683591385216 run.py:483] Algo bellman_ford step 6196 current loss 0.035388, current_train_items 198304.
I0302 19:00:33.770282 22683591385216 run.py:483] Algo bellman_ford step 6197 current loss 0.073237, current_train_items 198336.
I0302 19:00:33.798032 22683591385216 run.py:483] Algo bellman_ford step 6198 current loss 0.049636, current_train_items 198368.
I0302 19:00:33.830551 22683591385216 run.py:483] Algo bellman_ford step 6199 current loss 0.113963, current_train_items 198400.
I0302 19:00:33.849353 22683591385216 run.py:483] Algo bellman_ford step 6200 current loss 0.001296, current_train_items 198432.
I0302 19:00:33.857180 22683591385216 run.py:503] (val) algo bellman_ford step 6200: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 198432, 'step': 6200, 'algorithm': 'bellman_ford'}
I0302 19:00:33.857288 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:33.873350 22683591385216 run.py:483] Algo bellman_ford step 6201 current loss 0.008809, current_train_items 198464.
I0302 19:00:33.896768 22683591385216 run.py:483] Algo bellman_ford step 6202 current loss 0.051292, current_train_items 198496.
I0302 19:00:33.925056 22683591385216 run.py:483] Algo bellman_ford step 6203 current loss 0.038940, current_train_items 198528.
I0302 19:00:33.956088 22683591385216 run.py:483] Algo bellman_ford step 6204 current loss 0.065337, current_train_items 198560.
I0302 19:00:33.974895 22683591385216 run.py:483] Algo bellman_ford step 6205 current loss 0.003182, current_train_items 198592.
I0302 19:00:33.989514 22683591385216 run.py:483] Algo bellman_ford step 6206 current loss 0.006656, current_train_items 198624.
I0302 19:00:34.012243 22683591385216 run.py:483] Algo bellman_ford step 6207 current loss 0.027722, current_train_items 198656.
I0302 19:00:34.041610 22683591385216 run.py:483] Algo bellman_ford step 6208 current loss 0.044867, current_train_items 198688.
I0302 19:00:34.074268 22683591385216 run.py:483] Algo bellman_ford step 6209 current loss 0.090790, current_train_items 198720.
I0302 19:00:34.092377 22683591385216 run.py:483] Algo bellman_ford step 6210 current loss 0.002387, current_train_items 198752.
I0302 19:00:34.107751 22683591385216 run.py:483] Algo bellman_ford step 6211 current loss 0.009372, current_train_items 198784.
I0302 19:00:34.129757 22683591385216 run.py:483] Algo bellman_ford step 6212 current loss 0.050885, current_train_items 198816.
I0302 19:00:34.157508 22683591385216 run.py:483] Algo bellman_ford step 6213 current loss 0.044249, current_train_items 198848.
I0302 19:00:34.190145 22683591385216 run.py:483] Algo bellman_ford step 6214 current loss 0.040817, current_train_items 198880.
I0302 19:00:34.208294 22683591385216 run.py:483] Algo bellman_ford step 6215 current loss 0.006256, current_train_items 198912.
I0302 19:00:34.223586 22683591385216 run.py:483] Algo bellman_ford step 6216 current loss 0.020688, current_train_items 198944.
I0302 19:00:34.245262 22683591385216 run.py:483] Algo bellman_ford step 6217 current loss 0.014057, current_train_items 198976.
I0302 19:00:34.274310 22683591385216 run.py:483] Algo bellman_ford step 6218 current loss 0.037990, current_train_items 199008.
I0302 19:00:34.304666 22683591385216 run.py:483] Algo bellman_ford step 6219 current loss 0.061095, current_train_items 199040.
I0302 19:00:34.323015 22683591385216 run.py:483] Algo bellman_ford step 6220 current loss 0.005478, current_train_items 199072.
I0302 19:00:34.337786 22683591385216 run.py:483] Algo bellman_ford step 6221 current loss 0.031885, current_train_items 199104.
I0302 19:00:34.360621 22683591385216 run.py:483] Algo bellman_ford step 6222 current loss 0.035893, current_train_items 199136.
I0302 19:00:34.388865 22683591385216 run.py:483] Algo bellman_ford step 6223 current loss 0.040544, current_train_items 199168.
I0302 19:00:34.421131 22683591385216 run.py:483] Algo bellman_ford step 6224 current loss 0.047174, current_train_items 199200.
I0302 19:00:34.439700 22683591385216 run.py:483] Algo bellman_ford step 6225 current loss 0.001903, current_train_items 199232.
I0302 19:00:34.454887 22683591385216 run.py:483] Algo bellman_ford step 6226 current loss 0.017615, current_train_items 199264.
I0302 19:00:34.476218 22683591385216 run.py:483] Algo bellman_ford step 6227 current loss 0.022576, current_train_items 199296.
I0302 19:00:34.504046 22683591385216 run.py:483] Algo bellman_ford step 6228 current loss 0.041561, current_train_items 199328.
I0302 19:00:34.533483 22683591385216 run.py:483] Algo bellman_ford step 6229 current loss 0.049073, current_train_items 199360.
I0302 19:00:34.551976 22683591385216 run.py:483] Algo bellman_ford step 6230 current loss 0.001691, current_train_items 199392.
I0302 19:00:34.567437 22683591385216 run.py:483] Algo bellman_ford step 6231 current loss 0.035213, current_train_items 199424.
I0302 19:00:34.590368 22683591385216 run.py:483] Algo bellman_ford step 6232 current loss 0.063494, current_train_items 199456.
I0302 19:00:34.618579 22683591385216 run.py:483] Algo bellman_ford step 6233 current loss 0.046274, current_train_items 199488.
I0302 19:00:34.649651 22683591385216 run.py:483] Algo bellman_ford step 6234 current loss 0.087665, current_train_items 199520.
I0302 19:00:34.667892 22683591385216 run.py:483] Algo bellman_ford step 6235 current loss 0.000458, current_train_items 199552.
I0302 19:00:34.683213 22683591385216 run.py:483] Algo bellman_ford step 6236 current loss 0.012216, current_train_items 199584.
I0302 19:00:34.704666 22683591385216 run.py:483] Algo bellman_ford step 6237 current loss 0.071042, current_train_items 199616.
I0302 19:00:34.734383 22683591385216 run.py:483] Algo bellman_ford step 6238 current loss 0.052429, current_train_items 199648.
I0302 19:00:34.763329 22683591385216 run.py:483] Algo bellman_ford step 6239 current loss 0.076653, current_train_items 199680.
I0302 19:00:34.781543 22683591385216 run.py:483] Algo bellman_ford step 6240 current loss 0.001487, current_train_items 199712.
I0302 19:00:34.796887 22683591385216 run.py:483] Algo bellman_ford step 6241 current loss 0.040147, current_train_items 199744.
I0302 19:00:34.819708 22683591385216 run.py:483] Algo bellman_ford step 6242 current loss 0.015483, current_train_items 199776.
I0302 19:00:34.849235 22683591385216 run.py:483] Algo bellman_ford step 6243 current loss 0.042968, current_train_items 199808.
I0302 19:00:34.879540 22683591385216 run.py:483] Algo bellman_ford step 6244 current loss 0.035995, current_train_items 199840.
I0302 19:00:34.897949 22683591385216 run.py:483] Algo bellman_ford step 6245 current loss 0.010729, current_train_items 199872.
I0302 19:00:34.913251 22683591385216 run.py:483] Algo bellman_ford step 6246 current loss 0.067457, current_train_items 199904.
I0302 19:00:34.935249 22683591385216 run.py:483] Algo bellman_ford step 6247 current loss 0.014577, current_train_items 199936.
I0302 19:00:34.965205 22683591385216 run.py:483] Algo bellman_ford step 6248 current loss 0.050901, current_train_items 199968.
I0302 19:00:34.994885 22683591385216 run.py:483] Algo bellman_ford step 6249 current loss 0.105452, current_train_items 200000.
I0302 19:00:35.012760 22683591385216 run.py:483] Algo bellman_ford step 6250 current loss 0.003425, current_train_items 200032.
I0302 19:00:35.020759 22683591385216 run.py:503] (val) algo bellman_ford step 6250: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 200032, 'step': 6250, 'algorithm': 'bellman_ford'}
I0302 19:00:35.020866 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:35.037071 22683591385216 run.py:483] Algo bellman_ford step 6251 current loss 0.026979, current_train_items 200064.
I0302 19:00:35.059646 22683591385216 run.py:483] Algo bellman_ford step 6252 current loss 0.061962, current_train_items 200096.
I0302 19:00:35.089660 22683591385216 run.py:483] Algo bellman_ford step 6253 current loss 0.073371, current_train_items 200128.
I0302 19:00:35.120101 22683591385216 run.py:483] Algo bellman_ford step 6254 current loss 0.044006, current_train_items 200160.
I0302 19:00:35.138704 22683591385216 run.py:483] Algo bellman_ford step 6255 current loss 0.057439, current_train_items 200192.
I0302 19:00:35.153843 22683591385216 run.py:483] Algo bellman_ford step 6256 current loss 0.022825, current_train_items 200224.
I0302 19:00:35.176880 22683591385216 run.py:483] Algo bellman_ford step 6257 current loss 0.039338, current_train_items 200256.
I0302 19:00:35.205214 22683591385216 run.py:483] Algo bellman_ford step 6258 current loss 0.052623, current_train_items 200288.
I0302 19:00:35.235629 22683591385216 run.py:483] Algo bellman_ford step 6259 current loss 0.067281, current_train_items 200320.
I0302 19:00:35.254506 22683591385216 run.py:483] Algo bellman_ford step 6260 current loss 0.005313, current_train_items 200352.
I0302 19:00:35.270282 22683591385216 run.py:483] Algo bellman_ford step 6261 current loss 0.012939, current_train_items 200384.
I0302 19:00:35.292293 22683591385216 run.py:483] Algo bellman_ford step 6262 current loss 0.014442, current_train_items 200416.
I0302 19:00:35.321518 22683591385216 run.py:483] Algo bellman_ford step 6263 current loss 0.050636, current_train_items 200448.
I0302 19:00:35.352732 22683591385216 run.py:483] Algo bellman_ford step 6264 current loss 0.067225, current_train_items 200480.
I0302 19:00:35.370772 22683591385216 run.py:483] Algo bellman_ford step 6265 current loss 0.005067, current_train_items 200512.
I0302 19:00:35.386741 22683591385216 run.py:483] Algo bellman_ford step 6266 current loss 0.019994, current_train_items 200544.
I0302 19:00:35.409708 22683591385216 run.py:483] Algo bellman_ford step 6267 current loss 0.049508, current_train_items 200576.
I0302 19:00:35.438679 22683591385216 run.py:483] Algo bellman_ford step 6268 current loss 0.046727, current_train_items 200608.
I0302 19:00:35.467026 22683591385216 run.py:483] Algo bellman_ford step 6269 current loss 0.039492, current_train_items 200640.
I0302 19:00:35.485520 22683591385216 run.py:483] Algo bellman_ford step 6270 current loss 0.001200, current_train_items 200672.
I0302 19:00:35.501293 22683591385216 run.py:483] Algo bellman_ford step 6271 current loss 0.032151, current_train_items 200704.
I0302 19:00:35.523030 22683591385216 run.py:483] Algo bellman_ford step 6272 current loss 0.032796, current_train_items 200736.
I0302 19:00:35.551252 22683591385216 run.py:483] Algo bellman_ford step 6273 current loss 0.056488, current_train_items 200768.
I0302 19:00:35.583793 22683591385216 run.py:483] Algo bellman_ford step 6274 current loss 0.086539, current_train_items 200800.
I0302 19:00:35.602463 22683591385216 run.py:483] Algo bellman_ford step 6275 current loss 0.001866, current_train_items 200832.
I0302 19:00:35.618217 22683591385216 run.py:483] Algo bellman_ford step 6276 current loss 0.017652, current_train_items 200864.
I0302 19:00:35.640187 22683591385216 run.py:483] Algo bellman_ford step 6277 current loss 0.061093, current_train_items 200896.
I0302 19:00:35.668593 22683591385216 run.py:483] Algo bellman_ford step 6278 current loss 0.061459, current_train_items 200928.
I0302 19:00:35.701003 22683591385216 run.py:483] Algo bellman_ford step 6279 current loss 0.078539, current_train_items 200960.
I0302 19:00:35.719337 22683591385216 run.py:483] Algo bellman_ford step 6280 current loss 0.001788, current_train_items 200992.
I0302 19:00:35.734470 22683591385216 run.py:483] Algo bellman_ford step 6281 current loss 0.009832, current_train_items 201024.
I0302 19:00:35.756532 22683591385216 run.py:483] Algo bellman_ford step 6282 current loss 0.087792, current_train_items 201056.
I0302 19:00:35.785672 22683591385216 run.py:483] Algo bellman_ford step 6283 current loss 0.095032, current_train_items 201088.
I0302 19:00:35.817942 22683591385216 run.py:483] Algo bellman_ford step 6284 current loss 0.144759, current_train_items 201120.
I0302 19:00:35.836657 22683591385216 run.py:483] Algo bellman_ford step 6285 current loss 0.010936, current_train_items 201152.
I0302 19:00:35.852437 22683591385216 run.py:483] Algo bellman_ford step 6286 current loss 0.016516, current_train_items 201184.
I0302 19:00:35.874672 22683591385216 run.py:483] Algo bellman_ford step 6287 current loss 0.031090, current_train_items 201216.
I0302 19:00:35.904144 22683591385216 run.py:483] Algo bellman_ford step 6288 current loss 0.029227, current_train_items 201248.
I0302 19:00:35.935746 22683591385216 run.py:483] Algo bellman_ford step 6289 current loss 0.061902, current_train_items 201280.
I0302 19:00:35.954242 22683591385216 run.py:483] Algo bellman_ford step 6290 current loss 0.005096, current_train_items 201312.
I0302 19:00:35.969980 22683591385216 run.py:483] Algo bellman_ford step 6291 current loss 0.063404, current_train_items 201344.
I0302 19:00:35.991920 22683591385216 run.py:483] Algo bellman_ford step 6292 current loss 0.040226, current_train_items 201376.
I0302 19:00:36.021990 22683591385216 run.py:483] Algo bellman_ford step 6293 current loss 0.056699, current_train_items 201408.
I0302 19:00:36.054718 22683591385216 run.py:483] Algo bellman_ford step 6294 current loss 0.049940, current_train_items 201440.
I0302 19:00:36.073021 22683591385216 run.py:483] Algo bellman_ford step 6295 current loss 0.001203, current_train_items 201472.
I0302 19:00:36.088980 22683591385216 run.py:483] Algo bellman_ford step 6296 current loss 0.030417, current_train_items 201504.
I0302 19:00:36.110740 22683591385216 run.py:483] Algo bellman_ford step 6297 current loss 0.011956, current_train_items 201536.
I0302 19:00:36.139828 22683591385216 run.py:483] Algo bellman_ford step 6298 current loss 0.038800, current_train_items 201568.
I0302 19:00:36.170284 22683591385216 run.py:483] Algo bellman_ford step 6299 current loss 0.056455, current_train_items 201600.
I0302 19:00:36.188610 22683591385216 run.py:483] Algo bellman_ford step 6300 current loss 0.001464, current_train_items 201632.
I0302 19:00:36.196349 22683591385216 run.py:503] (val) algo bellman_ford step 6300: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 201632, 'step': 6300, 'algorithm': 'bellman_ford'}
I0302 19:00:36.196456 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:00:36.212634 22683591385216 run.py:483] Algo bellman_ford step 6301 current loss 0.012479, current_train_items 201664.
I0302 19:00:36.234669 22683591385216 run.py:483] Algo bellman_ford step 6302 current loss 0.051610, current_train_items 201696.
I0302 19:00:36.263588 22683591385216 run.py:483] Algo bellman_ford step 6303 current loss 0.047062, current_train_items 201728.
I0302 19:00:36.292922 22683591385216 run.py:483] Algo bellman_ford step 6304 current loss 0.033472, current_train_items 201760.
I0302 19:00:36.311465 22683591385216 run.py:483] Algo bellman_ford step 6305 current loss 0.009912, current_train_items 201792.
I0302 19:00:36.326476 22683591385216 run.py:483] Algo bellman_ford step 6306 current loss 0.009541, current_train_items 201824.
I0302 19:00:36.348797 22683591385216 run.py:483] Algo bellman_ford step 6307 current loss 0.072839, current_train_items 201856.
I0302 19:00:36.378623 22683591385216 run.py:483] Algo bellman_ford step 6308 current loss 0.066618, current_train_items 201888.
I0302 19:00:36.409172 22683591385216 run.py:483] Algo bellman_ford step 6309 current loss 0.100180, current_train_items 201920.
I0302 19:00:36.427560 22683591385216 run.py:483] Algo bellman_ford step 6310 current loss 0.007493, current_train_items 201952.
I0302 19:00:36.442712 22683591385216 run.py:483] Algo bellman_ford step 6311 current loss 0.005245, current_train_items 201984.
I0302 19:00:36.463708 22683591385216 run.py:483] Algo bellman_ford step 6312 current loss 0.028043, current_train_items 202016.
I0302 19:00:36.493809 22683591385216 run.py:483] Algo bellman_ford step 6313 current loss 0.109170, current_train_items 202048.
I0302 19:00:36.525499 22683591385216 run.py:483] Algo bellman_ford step 6314 current loss 0.125558, current_train_items 202080.
I0302 19:00:36.543938 22683591385216 run.py:483] Algo bellman_ford step 6315 current loss 0.004009, current_train_items 202112.
I0302 19:00:36.558924 22683591385216 run.py:483] Algo bellman_ford step 6316 current loss 0.009050, current_train_items 202144.
I0302 19:00:36.580841 22683591385216 run.py:483] Algo bellman_ford step 6317 current loss 0.020066, current_train_items 202176.
I0302 19:00:36.609409 22683591385216 run.py:483] Algo bellman_ford step 6318 current loss 0.019046, current_train_items 202208.
I0302 19:00:36.640269 22683591385216 run.py:483] Algo bellman_ford step 6319 current loss 0.059898, current_train_items 202240.
I0302 19:00:36.658762 22683591385216 run.py:483] Algo bellman_ford step 6320 current loss 0.006394, current_train_items 202272.
I0302 19:00:36.674308 22683591385216 run.py:483] Algo bellman_ford step 6321 current loss 0.012814, current_train_items 202304.
I0302 19:00:36.697767 22683591385216 run.py:483] Algo bellman_ford step 6322 current loss 0.025644, current_train_items 202336.
I0302 19:00:36.727682 22683591385216 run.py:483] Algo bellman_ford step 6323 current loss 0.030414, current_train_items 202368.
I0302 19:00:36.759572 22683591385216 run.py:483] Algo bellman_ford step 6324 current loss 0.055673, current_train_items 202400.
I0302 19:00:36.778123 22683591385216 run.py:483] Algo bellman_ford step 6325 current loss 0.003282, current_train_items 202432.
I0302 19:00:36.793135 22683591385216 run.py:483] Algo bellman_ford step 6326 current loss 0.010291, current_train_items 202464.
I0302 19:00:36.816462 22683591385216 run.py:483] Algo bellman_ford step 6327 current loss 0.051619, current_train_items 202496.
I0302 19:00:36.845495 22683591385216 run.py:483] Algo bellman_ford step 6328 current loss 0.039435, current_train_items 202528.
I0302 19:00:36.877290 22683591385216 run.py:483] Algo bellman_ford step 6329 current loss 0.049464, current_train_items 202560.
I0302 19:00:36.895489 22683591385216 run.py:483] Algo bellman_ford step 6330 current loss 0.001311, current_train_items 202592.
I0302 19:00:36.910764 22683591385216 run.py:483] Algo bellman_ford step 6331 current loss 0.010964, current_train_items 202624.
I0302 19:00:36.932812 22683591385216 run.py:483] Algo bellman_ford step 6332 current loss 0.020778, current_train_items 202656.
I0302 19:00:36.961946 22683591385216 run.py:483] Algo bellman_ford step 6333 current loss 0.020902, current_train_items 202688.
I0302 19:00:36.992258 22683591385216 run.py:483] Algo bellman_ford step 6334 current loss 0.040001, current_train_items 202720.
I0302 19:00:37.010624 22683591385216 run.py:483] Algo bellman_ford step 6335 current loss 0.001369, current_train_items 202752.
I0302 19:00:37.026100 22683591385216 run.py:483] Algo bellman_ford step 6336 current loss 0.005039, current_train_items 202784.
I0302 19:00:37.049051 22683591385216 run.py:483] Algo bellman_ford step 6337 current loss 0.022059, current_train_items 202816.
I0302 19:00:37.079076 22683591385216 run.py:483] Algo bellman_ford step 6338 current loss 0.025190, current_train_items 202848.
I0302 19:00:37.109715 22683591385216 run.py:483] Algo bellman_ford step 6339 current loss 0.042806, current_train_items 202880.
I0302 19:00:37.127757 22683591385216 run.py:483] Algo bellman_ford step 6340 current loss 0.002223, current_train_items 202912.
I0302 19:00:37.142648 22683591385216 run.py:483] Algo bellman_ford step 6341 current loss 0.006038, current_train_items 202944.
I0302 19:00:37.165821 22683591385216 run.py:483] Algo bellman_ford step 6342 current loss 0.021234, current_train_items 202976.
I0302 19:00:37.194334 22683591385216 run.py:483] Algo bellman_ford step 6343 current loss 0.013982, current_train_items 203008.
I0302 19:00:37.222728 22683591385216 run.py:483] Algo bellman_ford step 6344 current loss 0.038896, current_train_items 203040.
I0302 19:00:37.241002 22683591385216 run.py:483] Algo bellman_ford step 6345 current loss 0.000577, current_train_items 203072.
I0302 19:00:37.256493 22683591385216 run.py:483] Algo bellman_ford step 6346 current loss 0.040456, current_train_items 203104.
I0302 19:00:37.278908 22683591385216 run.py:483] Algo bellman_ford step 6347 current loss 0.023691, current_train_items 203136.
I0302 19:00:37.307196 22683591385216 run.py:483] Algo bellman_ford step 6348 current loss 0.049620, current_train_items 203168.
I0302 19:00:37.340262 22683591385216 run.py:483] Algo bellman_ford step 6349 current loss 0.032570, current_train_items 203200.
I0302 19:00:37.358328 22683591385216 run.py:483] Algo bellman_ford step 6350 current loss 0.010045, current_train_items 203232.
I0302 19:00:37.366370 22683591385216 run.py:503] (val) algo bellman_ford step 6350: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 203232, 'step': 6350, 'algorithm': 'bellman_ford'}
I0302 19:00:37.366475 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:37.382774 22683591385216 run.py:483] Algo bellman_ford step 6351 current loss 0.009432, current_train_items 203264.
I0302 19:00:37.404761 22683591385216 run.py:483] Algo bellman_ford step 6352 current loss 0.022366, current_train_items 203296.
I0302 19:00:37.434583 22683591385216 run.py:483] Algo bellman_ford step 6353 current loss 0.053939, current_train_items 203328.
I0302 19:00:37.465932 22683591385216 run.py:483] Algo bellman_ford step 6354 current loss 0.074105, current_train_items 203360.
I0302 19:00:37.484463 22683591385216 run.py:483] Algo bellman_ford step 6355 current loss 0.005551, current_train_items 203392.
I0302 19:00:37.499765 22683591385216 run.py:483] Algo bellman_ford step 6356 current loss 0.007533, current_train_items 203424.
I0302 19:00:37.522649 22683591385216 run.py:483] Algo bellman_ford step 6357 current loss 0.039754, current_train_items 203456.
I0302 19:00:37.551570 22683591385216 run.py:483] Algo bellman_ford step 6358 current loss 0.054816, current_train_items 203488.
I0302 19:00:37.583342 22683591385216 run.py:483] Algo bellman_ford step 6359 current loss 0.073380, current_train_items 203520.
I0302 19:00:37.602059 22683591385216 run.py:483] Algo bellman_ford step 6360 current loss 0.000763, current_train_items 203552.
I0302 19:00:37.617831 22683591385216 run.py:483] Algo bellman_ford step 6361 current loss 0.005259, current_train_items 203584.
I0302 19:00:37.639256 22683591385216 run.py:483] Algo bellman_ford step 6362 current loss 0.036554, current_train_items 203616.
I0302 19:00:37.668229 22683591385216 run.py:483] Algo bellman_ford step 6363 current loss 0.044352, current_train_items 203648.
I0302 19:00:37.698416 22683591385216 run.py:483] Algo bellman_ford step 6364 current loss 0.051609, current_train_items 203680.
I0302 19:00:37.716745 22683591385216 run.py:483] Algo bellman_ford step 6365 current loss 0.001995, current_train_items 203712.
I0302 19:00:37.732229 22683591385216 run.py:483] Algo bellman_ford step 6366 current loss 0.005253, current_train_items 203744.
I0302 19:00:37.753962 22683591385216 run.py:483] Algo bellman_ford step 6367 current loss 0.023075, current_train_items 203776.
I0302 19:00:37.784472 22683591385216 run.py:483] Algo bellman_ford step 6368 current loss 0.086398, current_train_items 203808.
I0302 19:00:37.814224 22683591385216 run.py:483] Algo bellman_ford step 6369 current loss 0.024439, current_train_items 203840.
I0302 19:00:37.832667 22683591385216 run.py:483] Algo bellman_ford step 6370 current loss 0.000943, current_train_items 203872.
I0302 19:00:37.848406 22683591385216 run.py:483] Algo bellman_ford step 6371 current loss 0.003715, current_train_items 203904.
I0302 19:00:37.871032 22683591385216 run.py:483] Algo bellman_ford step 6372 current loss 0.083904, current_train_items 203936.
I0302 19:00:37.899353 22683591385216 run.py:483] Algo bellman_ford step 6373 current loss 0.042168, current_train_items 203968.
I0302 19:00:37.930553 22683591385216 run.py:483] Algo bellman_ford step 6374 current loss 0.029457, current_train_items 204000.
I0302 19:00:37.949090 22683591385216 run.py:483] Algo bellman_ford step 6375 current loss 0.001335, current_train_items 204032.
I0302 19:00:37.964381 22683591385216 run.py:483] Algo bellman_ford step 6376 current loss 0.003307, current_train_items 204064.
I0302 19:00:37.987262 22683591385216 run.py:483] Algo bellman_ford step 6377 current loss 0.039144, current_train_items 204096.
I0302 19:00:38.016073 22683591385216 run.py:483] Algo bellman_ford step 6378 current loss 0.201824, current_train_items 204128.
I0302 19:00:38.049440 22683591385216 run.py:483] Algo bellman_ford step 6379 current loss 0.149671, current_train_items 204160.
I0302 19:00:38.067531 22683591385216 run.py:483] Algo bellman_ford step 6380 current loss 0.006019, current_train_items 204192.
I0302 19:00:38.082688 22683591385216 run.py:483] Algo bellman_ford step 6381 current loss 0.041143, current_train_items 204224.
I0302 19:00:38.105051 22683591385216 run.py:483] Algo bellman_ford step 6382 current loss 0.026935, current_train_items 204256.
I0302 19:00:38.133314 22683591385216 run.py:483] Algo bellman_ford step 6383 current loss 0.041347, current_train_items 204288.
I0302 19:00:38.164680 22683591385216 run.py:483] Algo bellman_ford step 6384 current loss 0.094932, current_train_items 204320.
I0302 19:00:38.183355 22683591385216 run.py:483] Algo bellman_ford step 6385 current loss 0.000877, current_train_items 204352.
I0302 19:00:38.198555 22683591385216 run.py:483] Algo bellman_ford step 6386 current loss 0.013965, current_train_items 204384.
I0302 19:00:38.221153 22683591385216 run.py:483] Algo bellman_ford step 6387 current loss 0.059511, current_train_items 204416.
I0302 19:00:38.249737 22683591385216 run.py:483] Algo bellman_ford step 6388 current loss 0.025665, current_train_items 204448.
I0302 19:00:38.280658 22683591385216 run.py:483] Algo bellman_ford step 6389 current loss 0.034823, current_train_items 204480.
I0302 19:00:38.299217 22683591385216 run.py:483] Algo bellman_ford step 6390 current loss 0.002987, current_train_items 204512.
I0302 19:00:38.314965 22683591385216 run.py:483] Algo bellman_ford step 6391 current loss 0.027863, current_train_items 204544.
I0302 19:00:38.336838 22683591385216 run.py:483] Algo bellman_ford step 6392 current loss 0.037436, current_train_items 204576.
I0302 19:00:38.364114 22683591385216 run.py:483] Algo bellman_ford step 6393 current loss 0.030109, current_train_items 204608.
I0302 19:00:38.395674 22683591385216 run.py:483] Algo bellman_ford step 6394 current loss 0.035755, current_train_items 204640.
I0302 19:00:38.414279 22683591385216 run.py:483] Algo bellman_ford step 6395 current loss 0.001791, current_train_items 204672.
I0302 19:00:38.429493 22683591385216 run.py:483] Algo bellman_ford step 6396 current loss 0.011302, current_train_items 204704.
I0302 19:00:38.450985 22683591385216 run.py:483] Algo bellman_ford step 6397 current loss 0.055670, current_train_items 204736.
I0302 19:00:38.479108 22683591385216 run.py:483] Algo bellman_ford step 6398 current loss 0.053886, current_train_items 204768.
I0302 19:00:38.508511 22683591385216 run.py:483] Algo bellman_ford step 6399 current loss 0.075751, current_train_items 204800.
I0302 19:00:38.527030 22683591385216 run.py:483] Algo bellman_ford step 6400 current loss 0.001720, current_train_items 204832.
I0302 19:00:38.534660 22683591385216 run.py:503] (val) algo bellman_ford step 6400: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 204832, 'step': 6400, 'algorithm': 'bellman_ford'}
I0302 19:00:38.534767 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:00:38.549987 22683591385216 run.py:483] Algo bellman_ford step 6401 current loss 0.005923, current_train_items 204864.
I0302 19:00:38.572014 22683591385216 run.py:483] Algo bellman_ford step 6402 current loss 0.011724, current_train_items 204896.
I0302 19:00:38.601482 22683591385216 run.py:483] Algo bellman_ford step 6403 current loss 0.080866, current_train_items 204928.
I0302 19:00:38.632613 22683591385216 run.py:483] Algo bellman_ford step 6404 current loss 0.075699, current_train_items 204960.
I0302 19:00:38.650744 22683591385216 run.py:483] Algo bellman_ford step 6405 current loss 0.002887, current_train_items 204992.
I0302 19:00:38.665798 22683591385216 run.py:483] Algo bellman_ford step 6406 current loss 0.019795, current_train_items 205024.
I0302 19:00:38.687481 22683591385216 run.py:483] Algo bellman_ford step 6407 current loss 0.022215, current_train_items 205056.
I0302 19:00:38.716324 22683591385216 run.py:483] Algo bellman_ford step 6408 current loss 0.037040, current_train_items 205088.
I0302 19:00:38.748990 22683591385216 run.py:483] Algo bellman_ford step 6409 current loss 0.049050, current_train_items 205120.
I0302 19:00:38.766923 22683591385216 run.py:483] Algo bellman_ford step 6410 current loss 0.004034, current_train_items 205152.
I0302 19:00:38.781855 22683591385216 run.py:483] Algo bellman_ford step 6411 current loss 0.009445, current_train_items 205184.
I0302 19:00:38.803196 22683591385216 run.py:483] Algo bellman_ford step 6412 current loss 0.024509, current_train_items 205216.
I0302 19:00:38.832211 22683591385216 run.py:483] Algo bellman_ford step 6413 current loss 0.037295, current_train_items 205248.
I0302 19:00:38.863418 22683591385216 run.py:483] Algo bellman_ford step 6414 current loss 0.039378, current_train_items 205280.
I0302 19:00:38.881394 22683591385216 run.py:483] Algo bellman_ford step 6415 current loss 0.002753, current_train_items 205312.
I0302 19:00:38.896525 22683591385216 run.py:483] Algo bellman_ford step 6416 current loss 0.013581, current_train_items 205344.
I0302 19:00:38.919553 22683591385216 run.py:483] Algo bellman_ford step 6417 current loss 0.044244, current_train_items 205376.
I0302 19:00:38.949880 22683591385216 run.py:483] Algo bellman_ford step 6418 current loss 0.028758, current_train_items 205408.
I0302 19:00:38.981662 22683591385216 run.py:483] Algo bellman_ford step 6419 current loss 0.060426, current_train_items 205440.
I0302 19:00:38.999839 22683591385216 run.py:483] Algo bellman_ford step 6420 current loss 0.003663, current_train_items 205472.
I0302 19:00:39.015156 22683591385216 run.py:483] Algo bellman_ford step 6421 current loss 0.023093, current_train_items 205504.
I0302 19:00:39.037016 22683591385216 run.py:483] Algo bellman_ford step 6422 current loss 0.034837, current_train_items 205536.
I0302 19:00:39.065699 22683591385216 run.py:483] Algo bellman_ford step 6423 current loss 0.079766, current_train_items 205568.
I0302 19:00:39.096522 22683591385216 run.py:483] Algo bellman_ford step 6424 current loss 0.042790, current_train_items 205600.
I0302 19:00:39.114325 22683591385216 run.py:483] Algo bellman_ford step 6425 current loss 0.004482, current_train_items 205632.
I0302 19:00:39.129945 22683591385216 run.py:483] Algo bellman_ford step 6426 current loss 0.010461, current_train_items 205664.
I0302 19:00:39.152184 22683591385216 run.py:483] Algo bellman_ford step 6427 current loss 0.106498, current_train_items 205696.
I0302 19:00:39.181586 22683591385216 run.py:483] Algo bellman_ford step 6428 current loss 0.064528, current_train_items 205728.
I0302 19:00:39.214787 22683591385216 run.py:483] Algo bellman_ford step 6429 current loss 0.129255, current_train_items 205760.
I0302 19:00:39.232873 22683591385216 run.py:483] Algo bellman_ford step 6430 current loss 0.002614, current_train_items 205792.
I0302 19:00:39.247635 22683591385216 run.py:483] Algo bellman_ford step 6431 current loss 0.074571, current_train_items 205824.
I0302 19:00:39.269837 22683591385216 run.py:483] Algo bellman_ford step 6432 current loss 0.027539, current_train_items 205856.
I0302 19:00:39.299180 22683591385216 run.py:483] Algo bellman_ford step 6433 current loss 0.069365, current_train_items 205888.
I0302 19:00:39.329427 22683591385216 run.py:483] Algo bellman_ford step 6434 current loss 0.072566, current_train_items 205920.
I0302 19:00:39.347553 22683591385216 run.py:483] Algo bellman_ford step 6435 current loss 0.001872, current_train_items 205952.
I0302 19:00:39.362697 22683591385216 run.py:483] Algo bellman_ford step 6436 current loss 0.008704, current_train_items 205984.
I0302 19:00:39.384469 22683591385216 run.py:483] Algo bellman_ford step 6437 current loss 0.035298, current_train_items 206016.
I0302 19:00:39.414146 22683591385216 run.py:483] Algo bellman_ford step 6438 current loss 0.047544, current_train_items 206048.
I0302 19:00:39.445527 22683591385216 run.py:483] Algo bellman_ford step 6439 current loss 0.065276, current_train_items 206080.
I0302 19:00:39.463383 22683591385216 run.py:483] Algo bellman_ford step 6440 current loss 0.003746, current_train_items 206112.
I0302 19:00:39.479065 22683591385216 run.py:483] Algo bellman_ford step 6441 current loss 0.010419, current_train_items 206144.
I0302 19:00:39.500253 22683591385216 run.py:483] Algo bellman_ford step 6442 current loss 0.017163, current_train_items 206176.
I0302 19:00:39.527726 22683591385216 run.py:483] Algo bellman_ford step 6443 current loss 0.056432, current_train_items 206208.
I0302 19:00:39.558867 22683591385216 run.py:483] Algo bellman_ford step 6444 current loss 0.044306, current_train_items 206240.
I0302 19:00:39.576953 22683591385216 run.py:483] Algo bellman_ford step 6445 current loss 0.011972, current_train_items 206272.
I0302 19:00:39.592080 22683591385216 run.py:483] Algo bellman_ford step 6446 current loss 0.008841, current_train_items 206304.
I0302 19:00:39.615040 22683591385216 run.py:483] Algo bellman_ford step 6447 current loss 0.053414, current_train_items 206336.
I0302 19:00:39.644192 22683591385216 run.py:483] Algo bellman_ford step 6448 current loss 0.047297, current_train_items 206368.
I0302 19:00:39.675414 22683591385216 run.py:483] Algo bellman_ford step 6449 current loss 0.047270, current_train_items 206400.
I0302 19:00:39.693555 22683591385216 run.py:483] Algo bellman_ford step 6450 current loss 0.002911, current_train_items 206432.
I0302 19:00:39.701708 22683591385216 run.py:503] (val) algo bellman_ford step 6450: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 206432, 'step': 6450, 'algorithm': 'bellman_ford'}
I0302 19:00:39.701814 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:39.717705 22683591385216 run.py:483] Algo bellman_ford step 6451 current loss 0.008348, current_train_items 206464.
I0302 19:00:39.741273 22683591385216 run.py:483] Algo bellman_ford step 6452 current loss 0.054282, current_train_items 206496.
I0302 19:00:39.769424 22683591385216 run.py:483] Algo bellman_ford step 6453 current loss 0.080672, current_train_items 206528.
I0302 19:00:39.798851 22683591385216 run.py:483] Algo bellman_ford step 6454 current loss 0.066833, current_train_items 206560.
I0302 19:00:39.817117 22683591385216 run.py:483] Algo bellman_ford step 6455 current loss 0.047465, current_train_items 206592.
I0302 19:00:39.832339 22683591385216 run.py:483] Algo bellman_ford step 6456 current loss 0.005606, current_train_items 206624.
I0302 19:00:39.853569 22683591385216 run.py:483] Algo bellman_ford step 6457 current loss 0.022140, current_train_items 206656.
I0302 19:00:39.882136 22683591385216 run.py:483] Algo bellman_ford step 6458 current loss 0.019687, current_train_items 206688.
I0302 19:00:39.911714 22683591385216 run.py:483] Algo bellman_ford step 6459 current loss 0.044190, current_train_items 206720.
I0302 19:00:39.930134 22683591385216 run.py:483] Algo bellman_ford step 6460 current loss 0.023954, current_train_items 206752.
I0302 19:00:39.945821 22683591385216 run.py:483] Algo bellman_ford step 6461 current loss 0.002147, current_train_items 206784.
I0302 19:00:39.967736 22683591385216 run.py:483] Algo bellman_ford step 6462 current loss 0.065071, current_train_items 206816.
I0302 19:00:39.995958 22683591385216 run.py:483] Algo bellman_ford step 6463 current loss 0.019749, current_train_items 206848.
I0302 19:00:40.029165 22683591385216 run.py:483] Algo bellman_ford step 6464 current loss 0.084739, current_train_items 206880.
I0302 19:00:40.046988 22683591385216 run.py:483] Algo bellman_ford step 6465 current loss 0.001666, current_train_items 206912.
I0302 19:00:40.062509 22683591385216 run.py:483] Algo bellman_ford step 6466 current loss 0.009045, current_train_items 206944.
I0302 19:00:40.085864 22683591385216 run.py:483] Algo bellman_ford step 6467 current loss 0.040037, current_train_items 206976.
I0302 19:00:40.115808 22683591385216 run.py:483] Algo bellman_ford step 6468 current loss 0.047475, current_train_items 207008.
I0302 19:00:40.148113 22683591385216 run.py:483] Algo bellman_ford step 6469 current loss 0.066805, current_train_items 207040.
I0302 19:00:40.166702 22683591385216 run.py:483] Algo bellman_ford step 6470 current loss 0.001462, current_train_items 207072.
I0302 19:00:40.182781 22683591385216 run.py:483] Algo bellman_ford step 6471 current loss 0.014514, current_train_items 207104.
I0302 19:00:40.204055 22683591385216 run.py:483] Algo bellman_ford step 6472 current loss 0.024829, current_train_items 207136.
I0302 19:00:40.233136 22683591385216 run.py:483] Algo bellman_ford step 6473 current loss 0.065531, current_train_items 207168.
I0302 19:00:40.263297 22683591385216 run.py:483] Algo bellman_ford step 6474 current loss 0.063314, current_train_items 207200.
I0302 19:00:40.281841 22683591385216 run.py:483] Algo bellman_ford step 6475 current loss 0.016200, current_train_items 207232.
I0302 19:00:40.297535 22683591385216 run.py:483] Algo bellman_ford step 6476 current loss 0.047042, current_train_items 207264.
I0302 19:00:40.318549 22683591385216 run.py:483] Algo bellman_ford step 6477 current loss 0.048471, current_train_items 207296.
I0302 19:00:40.347547 22683591385216 run.py:483] Algo bellman_ford step 6478 current loss 0.048680, current_train_items 207328.
I0302 19:00:40.379719 22683591385216 run.py:483] Algo bellman_ford step 6479 current loss 0.049082, current_train_items 207360.
I0302 19:00:40.397940 22683591385216 run.py:483] Algo bellman_ford step 6480 current loss 0.002499, current_train_items 207392.
I0302 19:00:40.412856 22683591385216 run.py:483] Algo bellman_ford step 6481 current loss 0.035511, current_train_items 207424.
I0302 19:00:40.434777 22683591385216 run.py:483] Algo bellman_ford step 6482 current loss 0.032503, current_train_items 207456.
I0302 19:00:40.464150 22683591385216 run.py:483] Algo bellman_ford step 6483 current loss 0.031033, current_train_items 207488.
I0302 19:00:40.496019 22683591385216 run.py:483] Algo bellman_ford step 6484 current loss 0.104336, current_train_items 207520.
I0302 19:00:40.514544 22683591385216 run.py:483] Algo bellman_ford step 6485 current loss 0.002360, current_train_items 207552.
I0302 19:00:40.530272 22683591385216 run.py:483] Algo bellman_ford step 6486 current loss 0.030865, current_train_items 207584.
I0302 19:00:40.553608 22683591385216 run.py:483] Algo bellman_ford step 6487 current loss 0.026701, current_train_items 207616.
I0302 19:00:40.581406 22683591385216 run.py:483] Algo bellman_ford step 6488 current loss 0.044882, current_train_items 207648.
I0302 19:00:40.612826 22683591385216 run.py:483] Algo bellman_ford step 6489 current loss 0.043875, current_train_items 207680.
I0302 19:00:40.631505 22683591385216 run.py:483] Algo bellman_ford step 6490 current loss 0.000877, current_train_items 207712.
I0302 19:00:40.646947 22683591385216 run.py:483] Algo bellman_ford step 6491 current loss 0.012640, current_train_items 207744.
I0302 19:00:40.668126 22683591385216 run.py:483] Algo bellman_ford step 6492 current loss 0.027582, current_train_items 207776.
I0302 19:00:40.696761 22683591385216 run.py:483] Algo bellman_ford step 6493 current loss 0.019014, current_train_items 207808.
I0302 19:00:40.727438 22683591385216 run.py:483] Algo bellman_ford step 6494 current loss 0.093891, current_train_items 207840.
I0302 19:00:40.745256 22683591385216 run.py:483] Algo bellman_ford step 6495 current loss 0.000937, current_train_items 207872.
I0302 19:00:40.760173 22683591385216 run.py:483] Algo bellman_ford step 6496 current loss 0.016990, current_train_items 207904.
I0302 19:00:40.782937 22683591385216 run.py:483] Algo bellman_ford step 6497 current loss 0.024629, current_train_items 207936.
I0302 19:00:40.811103 22683591385216 run.py:483] Algo bellman_ford step 6498 current loss 0.029336, current_train_items 207968.
I0302 19:00:40.843669 22683591385216 run.py:483] Algo bellman_ford step 6499 current loss 0.069248, current_train_items 208000.
I0302 19:00:40.862186 22683591385216 run.py:483] Algo bellman_ford step 6500 current loss 0.001681, current_train_items 208032.
I0302 19:00:40.869890 22683591385216 run.py:503] (val) algo bellman_ford step 6500: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 208032, 'step': 6500, 'algorithm': 'bellman_ford'}
I0302 19:00:40.870007 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:40.885910 22683591385216 run.py:483] Algo bellman_ford step 6501 current loss 0.017965, current_train_items 208064.
I0302 19:00:40.909198 22683591385216 run.py:483] Algo bellman_ford step 6502 current loss 0.055594, current_train_items 208096.
I0302 19:00:40.937938 22683591385216 run.py:483] Algo bellman_ford step 6503 current loss 0.030126, current_train_items 208128.
I0302 19:00:40.970637 22683591385216 run.py:483] Algo bellman_ford step 6504 current loss 0.040745, current_train_items 208160.
I0302 19:00:40.989442 22683591385216 run.py:483] Algo bellman_ford step 6505 current loss 0.002495, current_train_items 208192.
I0302 19:00:41.004053 22683591385216 run.py:483] Algo bellman_ford step 6506 current loss 0.005236, current_train_items 208224.
I0302 19:00:41.026724 22683591385216 run.py:483] Algo bellman_ford step 6507 current loss 0.050607, current_train_items 208256.
I0302 19:00:41.056619 22683591385216 run.py:483] Algo bellman_ford step 6508 current loss 0.098998, current_train_items 208288.
I0302 19:00:41.085965 22683591385216 run.py:483] Algo bellman_ford step 6509 current loss 0.081587, current_train_items 208320.
I0302 19:00:41.104351 22683591385216 run.py:483] Algo bellman_ford step 6510 current loss 0.002287, current_train_items 208352.
I0302 19:00:41.119729 22683591385216 run.py:483] Algo bellman_ford step 6511 current loss 0.038120, current_train_items 208384.
I0302 19:00:41.143119 22683591385216 run.py:483] Algo bellman_ford step 6512 current loss 0.066774, current_train_items 208416.
I0302 19:00:41.171998 22683591385216 run.py:483] Algo bellman_ford step 6513 current loss 0.033871, current_train_items 208448.
I0302 19:00:41.204756 22683591385216 run.py:483] Algo bellman_ford step 6514 current loss 0.051040, current_train_items 208480.
I0302 19:00:41.222733 22683591385216 run.py:483] Algo bellman_ford step 6515 current loss 0.007688, current_train_items 208512.
I0302 19:00:41.237830 22683591385216 run.py:483] Algo bellman_ford step 6516 current loss 0.024693, current_train_items 208544.
I0302 19:00:41.260672 22683591385216 run.py:483] Algo bellman_ford step 6517 current loss 0.039053, current_train_items 208576.
I0302 19:00:41.289920 22683591385216 run.py:483] Algo bellman_ford step 6518 current loss 0.041383, current_train_items 208608.
I0302 19:00:41.320400 22683591385216 run.py:483] Algo bellman_ford step 6519 current loss 0.053504, current_train_items 208640.
I0302 19:00:41.338713 22683591385216 run.py:483] Algo bellman_ford step 6520 current loss 0.018216, current_train_items 208672.
I0302 19:00:41.353987 22683591385216 run.py:483] Algo bellman_ford step 6521 current loss 0.016815, current_train_items 208704.
I0302 19:00:41.376852 22683591385216 run.py:483] Algo bellman_ford step 6522 current loss 0.062990, current_train_items 208736.
I0302 19:00:41.405749 22683591385216 run.py:483] Algo bellman_ford step 6523 current loss 0.049918, current_train_items 208768.
I0302 19:00:41.434814 22683591385216 run.py:483] Algo bellman_ford step 6524 current loss 0.065046, current_train_items 208800.
I0302 19:00:41.453500 22683591385216 run.py:483] Algo bellman_ford step 6525 current loss 0.001583, current_train_items 208832.
I0302 19:00:41.468557 22683591385216 run.py:483] Algo bellman_ford step 6526 current loss 0.015412, current_train_items 208864.
I0302 19:00:41.490782 22683591385216 run.py:483] Algo bellman_ford step 6527 current loss 0.050124, current_train_items 208896.
I0302 19:00:41.518887 22683591385216 run.py:483] Algo bellman_ford step 6528 current loss 0.028492, current_train_items 208928.
I0302 19:00:41.551083 22683591385216 run.py:483] Algo bellman_ford step 6529 current loss 0.046552, current_train_items 208960.
I0302 19:00:41.569237 22683591385216 run.py:483] Algo bellman_ford step 6530 current loss 0.000834, current_train_items 208992.
I0302 19:00:41.584688 22683591385216 run.py:483] Algo bellman_ford step 6531 current loss 0.011276, current_train_items 209024.
I0302 19:00:41.608892 22683591385216 run.py:483] Algo bellman_ford step 6532 current loss 0.045610, current_train_items 209056.
I0302 19:00:41.637160 22683591385216 run.py:483] Algo bellman_ford step 6533 current loss 0.090380, current_train_items 209088.
I0302 19:00:41.668550 22683591385216 run.py:483] Algo bellman_ford step 6534 current loss 0.111647, current_train_items 209120.
I0302 19:00:41.686569 22683591385216 run.py:483] Algo bellman_ford step 6535 current loss 0.002617, current_train_items 209152.
I0302 19:00:41.701955 22683591385216 run.py:483] Algo bellman_ford step 6536 current loss 0.023054, current_train_items 209184.
I0302 19:00:41.724124 22683591385216 run.py:483] Algo bellman_ford step 6537 current loss 0.067064, current_train_items 209216.
I0302 19:00:41.752401 22683591385216 run.py:483] Algo bellman_ford step 6538 current loss 0.057331, current_train_items 209248.
I0302 19:00:41.783910 22683591385216 run.py:483] Algo bellman_ford step 6539 current loss 0.065371, current_train_items 209280.
I0302 19:00:41.802091 22683591385216 run.py:483] Algo bellman_ford step 6540 current loss 0.003613, current_train_items 209312.
I0302 19:00:41.817420 22683591385216 run.py:483] Algo bellman_ford step 6541 current loss 0.034531, current_train_items 209344.
I0302 19:00:41.839745 22683591385216 run.py:483] Algo bellman_ford step 6542 current loss 0.102925, current_train_items 209376.
I0302 19:00:41.869527 22683591385216 run.py:483] Algo bellman_ford step 6543 current loss 0.060468, current_train_items 209408.
I0302 19:00:41.903031 22683591385216 run.py:483] Algo bellman_ford step 6544 current loss 0.107778, current_train_items 209440.
I0302 19:00:41.921166 22683591385216 run.py:483] Algo bellman_ford step 6545 current loss 0.004461, current_train_items 209472.
I0302 19:00:41.936179 22683591385216 run.py:483] Algo bellman_ford step 6546 current loss 0.064341, current_train_items 209504.
I0302 19:00:41.957791 22683591385216 run.py:483] Algo bellman_ford step 6547 current loss 0.017411, current_train_items 209536.
I0302 19:00:41.987875 22683591385216 run.py:483] Algo bellman_ford step 6548 current loss 0.066241, current_train_items 209568.
I0302 19:00:42.020252 22683591385216 run.py:483] Algo bellman_ford step 6549 current loss 0.057008, current_train_items 209600.
I0302 19:00:42.038412 22683591385216 run.py:483] Algo bellman_ford step 6550 current loss 0.014262, current_train_items 209632.
I0302 19:00:42.046350 22683591385216 run.py:503] (val) algo bellman_ford step 6550: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 209632, 'step': 6550, 'algorithm': 'bellman_ford'}
I0302 19:00:42.046457 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:42.062646 22683591385216 run.py:483] Algo bellman_ford step 6551 current loss 0.008413, current_train_items 209664.
I0302 19:00:42.085157 22683591385216 run.py:483] Algo bellman_ford step 6552 current loss 0.041092, current_train_items 209696.
I0302 19:00:42.115568 22683591385216 run.py:483] Algo bellman_ford step 6553 current loss 0.035959, current_train_items 209728.
I0302 19:00:42.149137 22683591385216 run.py:483] Algo bellman_ford step 6554 current loss 0.110742, current_train_items 209760.
I0302 19:00:42.167689 22683591385216 run.py:483] Algo bellman_ford step 6555 current loss 0.004938, current_train_items 209792.
I0302 19:00:42.182800 22683591385216 run.py:483] Algo bellman_ford step 6556 current loss 0.011243, current_train_items 209824.
I0302 19:00:42.205756 22683591385216 run.py:483] Algo bellman_ford step 6557 current loss 0.028967, current_train_items 209856.
I0302 19:00:42.235687 22683591385216 run.py:483] Algo bellman_ford step 6558 current loss 0.110124, current_train_items 209888.
I0302 19:00:42.267566 22683591385216 run.py:483] Algo bellman_ford step 6559 current loss 0.173679, current_train_items 209920.
I0302 19:00:42.286512 22683591385216 run.py:483] Algo bellman_ford step 6560 current loss 0.004036, current_train_items 209952.
I0302 19:00:42.301666 22683591385216 run.py:483] Algo bellman_ford step 6561 current loss 0.015137, current_train_items 209984.
I0302 19:00:42.323203 22683591385216 run.py:483] Algo bellman_ford step 6562 current loss 0.054657, current_train_items 210016.
I0302 19:00:42.351884 22683591385216 run.py:483] Algo bellman_ford step 6563 current loss 0.077889, current_train_items 210048.
I0302 19:00:42.380257 22683591385216 run.py:483] Algo bellman_ford step 6564 current loss 0.031607, current_train_items 210080.
I0302 19:00:42.398880 22683591385216 run.py:483] Algo bellman_ford step 6565 current loss 0.003789, current_train_items 210112.
I0302 19:00:42.413868 22683591385216 run.py:483] Algo bellman_ford step 6566 current loss 0.006083, current_train_items 210144.
I0302 19:00:42.436282 22683591385216 run.py:483] Algo bellman_ford step 6567 current loss 0.032928, current_train_items 210176.
I0302 19:00:42.463826 22683591385216 run.py:483] Algo bellman_ford step 6568 current loss 0.088292, current_train_items 210208.
I0302 19:00:42.495237 22683591385216 run.py:483] Algo bellman_ford step 6569 current loss 0.099169, current_train_items 210240.
I0302 19:00:42.513821 22683591385216 run.py:483] Algo bellman_ford step 6570 current loss 0.005697, current_train_items 210272.
I0302 19:00:42.529381 22683591385216 run.py:483] Algo bellman_ford step 6571 current loss 0.024048, current_train_items 210304.
I0302 19:00:42.551166 22683591385216 run.py:483] Algo bellman_ford step 6572 current loss 0.068737, current_train_items 210336.
I0302 19:00:42.579876 22683591385216 run.py:483] Algo bellman_ford step 6573 current loss 0.035962, current_train_items 210368.
I0302 19:00:42.610868 22683591385216 run.py:483] Algo bellman_ford step 6574 current loss 0.086791, current_train_items 210400.
I0302 19:00:42.629700 22683591385216 run.py:483] Algo bellman_ford step 6575 current loss 0.025741, current_train_items 210432.
I0302 19:00:42.645384 22683591385216 run.py:483] Algo bellman_ford step 6576 current loss 0.022717, current_train_items 210464.
I0302 19:00:42.666813 22683591385216 run.py:483] Algo bellman_ford step 6577 current loss 0.027989, current_train_items 210496.
I0302 19:00:42.695811 22683591385216 run.py:483] Algo bellman_ford step 6578 current loss 0.054640, current_train_items 210528.
I0302 19:00:42.727124 22683591385216 run.py:483] Algo bellman_ford step 6579 current loss 0.068346, current_train_items 210560.
I0302 19:00:42.745576 22683591385216 run.py:483] Algo bellman_ford step 6580 current loss 0.036601, current_train_items 210592.
I0302 19:00:42.760801 22683591385216 run.py:483] Algo bellman_ford step 6581 current loss 0.010242, current_train_items 210624.
I0302 19:00:42.783365 22683591385216 run.py:483] Algo bellman_ford step 6582 current loss 0.063045, current_train_items 210656.
I0302 19:00:42.813569 22683591385216 run.py:483] Algo bellman_ford step 6583 current loss 0.050959, current_train_items 210688.
I0302 19:00:42.844070 22683591385216 run.py:483] Algo bellman_ford step 6584 current loss 0.063718, current_train_items 210720.
I0302 19:00:42.862536 22683591385216 run.py:483] Algo bellman_ford step 6585 current loss 0.030174, current_train_items 210752.
I0302 19:00:42.877598 22683591385216 run.py:483] Algo bellman_ford step 6586 current loss 0.016150, current_train_items 210784.
I0302 19:00:42.898607 22683591385216 run.py:483] Algo bellman_ford step 6587 current loss 0.025610, current_train_items 210816.
I0302 19:00:42.926849 22683591385216 run.py:483] Algo bellman_ford step 6588 current loss 0.052030, current_train_items 210848.
I0302 19:00:42.956522 22683591385216 run.py:483] Algo bellman_ford step 6589 current loss 0.057844, current_train_items 210880.
I0302 19:00:42.975208 22683591385216 run.py:483] Algo bellman_ford step 6590 current loss 0.012750, current_train_items 210912.
I0302 19:00:42.990456 22683591385216 run.py:483] Algo bellman_ford step 6591 current loss 0.007838, current_train_items 210944.
I0302 19:00:43.012637 22683591385216 run.py:483] Algo bellman_ford step 6592 current loss 0.033687, current_train_items 210976.
I0302 19:00:43.041419 22683591385216 run.py:483] Algo bellman_ford step 6593 current loss 0.072570, current_train_items 211008.
I0302 19:00:43.071533 22683591385216 run.py:483] Algo bellman_ford step 6594 current loss 0.096225, current_train_items 211040.
I0302 19:00:43.089880 22683591385216 run.py:483] Algo bellman_ford step 6595 current loss 0.007946, current_train_items 211072.
I0302 19:00:43.104979 22683591385216 run.py:483] Algo bellman_ford step 6596 current loss 0.014298, current_train_items 211104.
I0302 19:00:43.127909 22683591385216 run.py:483] Algo bellman_ford step 6597 current loss 0.034374, current_train_items 211136.
I0302 19:00:43.157368 22683591385216 run.py:483] Algo bellman_ford step 6598 current loss 0.026631, current_train_items 211168.
I0302 19:00:43.185301 22683591385216 run.py:483] Algo bellman_ford step 6599 current loss 0.047625, current_train_items 211200.
I0302 19:00:43.203814 22683591385216 run.py:483] Algo bellman_ford step 6600 current loss 0.004021, current_train_items 211232.
I0302 19:00:43.211579 22683591385216 run.py:503] (val) algo bellman_ford step 6600: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 211232, 'step': 6600, 'algorithm': 'bellman_ford'}
I0302 19:00:43.211686 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:43.227442 22683591385216 run.py:483] Algo bellman_ford step 6601 current loss 0.008478, current_train_items 211264.
I0302 19:00:43.249137 22683591385216 run.py:483] Algo bellman_ford step 6602 current loss 0.032236, current_train_items 211296.
I0302 19:00:43.278711 22683591385216 run.py:483] Algo bellman_ford step 6603 current loss 0.038973, current_train_items 211328.
I0302 19:00:43.310811 22683591385216 run.py:483] Algo bellman_ford step 6604 current loss 0.056139, current_train_items 211360.
I0302 19:00:43.329375 22683591385216 run.py:483] Algo bellman_ford step 6605 current loss 0.001380, current_train_items 211392.
I0302 19:00:43.344403 22683591385216 run.py:483] Algo bellman_ford step 6606 current loss 0.009128, current_train_items 211424.
I0302 19:00:43.366860 22683591385216 run.py:483] Algo bellman_ford step 6607 current loss 0.048137, current_train_items 211456.
I0302 19:00:43.396609 22683591385216 run.py:483] Algo bellman_ford step 6608 current loss 0.034498, current_train_items 211488.
I0302 19:00:43.426085 22683591385216 run.py:483] Algo bellman_ford step 6609 current loss 0.023626, current_train_items 211520.
I0302 19:00:43.444445 22683591385216 run.py:483] Algo bellman_ford step 6610 current loss 0.001874, current_train_items 211552.
I0302 19:00:43.459403 22683591385216 run.py:483] Algo bellman_ford step 6611 current loss 0.036746, current_train_items 211584.
I0302 19:00:43.480839 22683591385216 run.py:483] Algo bellman_ford step 6612 current loss 0.033684, current_train_items 211616.
I0302 19:00:43.510471 22683591385216 run.py:483] Algo bellman_ford step 6613 current loss 0.063567, current_train_items 211648.
I0302 19:00:43.544409 22683591385216 run.py:483] Algo bellman_ford step 6614 current loss 0.051992, current_train_items 211680.
I0302 19:00:43.562949 22683591385216 run.py:483] Algo bellman_ford step 6615 current loss 0.001763, current_train_items 211712.
I0302 19:00:43.578094 22683591385216 run.py:483] Algo bellman_ford step 6616 current loss 0.013646, current_train_items 211744.
I0302 19:00:43.600669 22683591385216 run.py:483] Algo bellman_ford step 6617 current loss 0.023408, current_train_items 211776.
I0302 19:00:43.629176 22683591385216 run.py:483] Algo bellman_ford step 6618 current loss 0.047503, current_train_items 211808.
I0302 19:00:43.661083 22683591385216 run.py:483] Algo bellman_ford step 6619 current loss 0.043133, current_train_items 211840.
I0302 19:00:43.679121 22683591385216 run.py:483] Algo bellman_ford step 6620 current loss 0.001152, current_train_items 211872.
I0302 19:00:43.693989 22683591385216 run.py:483] Algo bellman_ford step 6621 current loss 0.009409, current_train_items 211904.
I0302 19:00:43.715147 22683591385216 run.py:483] Algo bellman_ford step 6622 current loss 0.038265, current_train_items 211936.
I0302 19:00:43.744446 22683591385216 run.py:483] Algo bellman_ford step 6623 current loss 0.071086, current_train_items 211968.
I0302 19:00:43.774606 22683591385216 run.py:483] Algo bellman_ford step 6624 current loss 0.051785, current_train_items 212000.
I0302 19:00:43.792796 22683591385216 run.py:483] Algo bellman_ford step 6625 current loss 0.001209, current_train_items 212032.
I0302 19:00:43.808127 22683591385216 run.py:483] Algo bellman_ford step 6626 current loss 0.002988, current_train_items 212064.
I0302 19:00:43.830574 22683591385216 run.py:483] Algo bellman_ford step 6627 current loss 0.053926, current_train_items 212096.
I0302 19:00:43.858625 22683591385216 run.py:483] Algo bellman_ford step 6628 current loss 0.045045, current_train_items 212128.
I0302 19:00:43.888309 22683591385216 run.py:483] Algo bellman_ford step 6629 current loss 0.040098, current_train_items 212160.
I0302 19:00:43.906529 22683591385216 run.py:483] Algo bellman_ford step 6630 current loss 0.008361, current_train_items 212192.
I0302 19:00:43.921851 22683591385216 run.py:483] Algo bellman_ford step 6631 current loss 0.044050, current_train_items 212224.
I0302 19:00:43.944926 22683591385216 run.py:483] Algo bellman_ford step 6632 current loss 0.035684, current_train_items 212256.
I0302 19:00:43.974815 22683591385216 run.py:483] Algo bellman_ford step 6633 current loss 0.050318, current_train_items 212288.
I0302 19:00:44.004787 22683591385216 run.py:483] Algo bellman_ford step 6634 current loss 0.044397, current_train_items 212320.
I0302 19:00:44.023041 22683591385216 run.py:483] Algo bellman_ford step 6635 current loss 0.001029, current_train_items 212352.
I0302 19:00:44.038409 22683591385216 run.py:483] Algo bellman_ford step 6636 current loss 0.027240, current_train_items 212384.
I0302 19:00:44.060820 22683591385216 run.py:483] Algo bellman_ford step 6637 current loss 0.034968, current_train_items 212416.
I0302 19:00:44.090122 22683591385216 run.py:483] Algo bellman_ford step 6638 current loss 0.045708, current_train_items 212448.
I0302 19:00:44.121790 22683591385216 run.py:483] Algo bellman_ford step 6639 current loss 0.038097, current_train_items 212480.
I0302 19:00:44.140049 22683591385216 run.py:483] Algo bellman_ford step 6640 current loss 0.000670, current_train_items 212512.
I0302 19:00:44.155371 22683591385216 run.py:483] Algo bellman_ford step 6641 current loss 0.006699, current_train_items 212544.
I0302 19:00:44.178483 22683591385216 run.py:483] Algo bellman_ford step 6642 current loss 0.079580, current_train_items 212576.
I0302 19:00:44.207170 22683591385216 run.py:483] Algo bellman_ford step 6643 current loss 0.068363, current_train_items 212608.
I0302 19:00:44.239322 22683591385216 run.py:483] Algo bellman_ford step 6644 current loss 0.059376, current_train_items 212640.
I0302 19:00:44.257327 22683591385216 run.py:483] Algo bellman_ford step 6645 current loss 0.003031, current_train_items 212672.
I0302 19:00:44.272405 22683591385216 run.py:483] Algo bellman_ford step 6646 current loss 0.003861, current_train_items 212704.
I0302 19:00:44.294451 22683591385216 run.py:483] Algo bellman_ford step 6647 current loss 0.053461, current_train_items 212736.
I0302 19:00:44.324043 22683591385216 run.py:483] Algo bellman_ford step 6648 current loss 0.122235, current_train_items 212768.
I0302 19:00:44.356237 22683591385216 run.py:483] Algo bellman_ford step 6649 current loss 0.101899, current_train_items 212800.
I0302 19:00:44.374613 22683591385216 run.py:483] Algo bellman_ford step 6650 current loss 0.004657, current_train_items 212832.
I0302 19:00:44.382474 22683591385216 run.py:503] (val) algo bellman_ford step 6650: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 212832, 'step': 6650, 'algorithm': 'bellman_ford'}
I0302 19:00:44.382581 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:00:44.399012 22683591385216 run.py:483] Algo bellman_ford step 6651 current loss 0.015388, current_train_items 212864.
I0302 19:00:44.422191 22683591385216 run.py:483] Algo bellman_ford step 6652 current loss 0.033925, current_train_items 212896.
I0302 19:00:44.452435 22683591385216 run.py:483] Algo bellman_ford step 6653 current loss 0.032819, current_train_items 212928.
I0302 19:00:44.479951 22683591385216 run.py:483] Algo bellman_ford step 6654 current loss 0.047407, current_train_items 212960.
I0302 19:00:44.498524 22683591385216 run.py:483] Algo bellman_ford step 6655 current loss 0.003868, current_train_items 212992.
I0302 19:00:44.513512 22683591385216 run.py:483] Algo bellman_ford step 6656 current loss 0.021443, current_train_items 213024.
I0302 19:00:44.535727 22683591385216 run.py:483] Algo bellman_ford step 6657 current loss 0.031962, current_train_items 213056.
I0302 19:00:44.565397 22683591385216 run.py:483] Algo bellman_ford step 6658 current loss 0.027736, current_train_items 213088.
I0302 19:00:44.598717 22683591385216 run.py:483] Algo bellman_ford step 6659 current loss 0.086882, current_train_items 213120.
I0302 19:00:44.617707 22683591385216 run.py:483] Algo bellman_ford step 6660 current loss 0.003857, current_train_items 213152.
I0302 19:00:44.633486 22683591385216 run.py:483] Algo bellman_ford step 6661 current loss 0.011704, current_train_items 213184.
I0302 19:00:44.656777 22683591385216 run.py:483] Algo bellman_ford step 6662 current loss 0.055092, current_train_items 213216.
I0302 19:00:44.684825 22683591385216 run.py:483] Algo bellman_ford step 6663 current loss 0.032018, current_train_items 213248.
I0302 19:00:44.716055 22683591385216 run.py:483] Algo bellman_ford step 6664 current loss 0.038358, current_train_items 213280.
I0302 19:00:44.734524 22683591385216 run.py:483] Algo bellman_ford step 6665 current loss 0.001537, current_train_items 213312.
I0302 19:00:44.749804 22683591385216 run.py:483] Algo bellman_ford step 6666 current loss 0.011357, current_train_items 213344.
I0302 19:00:44.773955 22683591385216 run.py:483] Algo bellman_ford step 6667 current loss 0.051906, current_train_items 213376.
I0302 19:00:44.804265 22683591385216 run.py:483] Algo bellman_ford step 6668 current loss 0.058799, current_train_items 213408.
I0302 19:00:44.837168 22683591385216 run.py:483] Algo bellman_ford step 6669 current loss 0.040432, current_train_items 213440.
I0302 19:00:44.856066 22683591385216 run.py:483] Algo bellman_ford step 6670 current loss 0.001643, current_train_items 213472.
I0302 19:00:44.871573 22683591385216 run.py:483] Algo bellman_ford step 6671 current loss 0.013022, current_train_items 213504.
I0302 19:00:44.893975 22683591385216 run.py:483] Algo bellman_ford step 6672 current loss 0.056747, current_train_items 213536.
I0302 19:00:44.923526 22683591385216 run.py:483] Algo bellman_ford step 6673 current loss 0.047965, current_train_items 213568.
I0302 19:00:44.953485 22683591385216 run.py:483] Algo bellman_ford step 6674 current loss 0.067728, current_train_items 213600.
I0302 19:00:44.972234 22683591385216 run.py:483] Algo bellman_ford step 6675 current loss 0.005876, current_train_items 213632.
I0302 19:00:44.987214 22683591385216 run.py:483] Algo bellman_ford step 6676 current loss 0.007176, current_train_items 213664.
I0302 19:00:45.010037 22683591385216 run.py:483] Algo bellman_ford step 6677 current loss 0.069574, current_train_items 213696.
I0302 19:00:45.040196 22683591385216 run.py:483] Algo bellman_ford step 6678 current loss 0.051276, current_train_items 213728.
I0302 19:00:45.073271 22683591385216 run.py:483] Algo bellman_ford step 6679 current loss 0.083262, current_train_items 213760.
I0302 19:00:45.091856 22683591385216 run.py:483] Algo bellman_ford step 6680 current loss 0.000652, current_train_items 213792.
I0302 19:00:45.107195 22683591385216 run.py:483] Algo bellman_ford step 6681 current loss 0.009438, current_train_items 213824.
I0302 19:00:45.130420 22683591385216 run.py:483] Algo bellman_ford step 6682 current loss 0.061134, current_train_items 213856.
I0302 19:00:45.159315 22683591385216 run.py:483] Algo bellman_ford step 6683 current loss 0.041459, current_train_items 213888.
I0302 19:00:45.193250 22683591385216 run.py:483] Algo bellman_ford step 6684 current loss 0.138213, current_train_items 213920.
I0302 19:00:45.211851 22683591385216 run.py:483] Algo bellman_ford step 6685 current loss 0.002363, current_train_items 213952.
I0302 19:00:45.227215 22683591385216 run.py:483] Algo bellman_ford step 6686 current loss 0.039846, current_train_items 213984.
I0302 19:00:45.249428 22683591385216 run.py:483] Algo bellman_ford step 6687 current loss 0.040409, current_train_items 214016.
I0302 19:00:45.279203 22683591385216 run.py:483] Algo bellman_ford step 6688 current loss 0.029020, current_train_items 214048.
I0302 19:00:45.310108 22683591385216 run.py:483] Algo bellman_ford step 6689 current loss 0.047570, current_train_items 214080.
I0302 19:00:45.328600 22683591385216 run.py:483] Algo bellman_ford step 6690 current loss 0.002016, current_train_items 214112.
I0302 19:00:45.343772 22683591385216 run.py:483] Algo bellman_ford step 6691 current loss 0.001935, current_train_items 214144.
I0302 19:00:45.366455 22683591385216 run.py:483] Algo bellman_ford step 6692 current loss 0.028192, current_train_items 214176.
I0302 19:00:45.395914 22683591385216 run.py:483] Algo bellman_ford step 6693 current loss 0.044611, current_train_items 214208.
I0302 19:00:45.427884 22683591385216 run.py:483] Algo bellman_ford step 6694 current loss 0.042713, current_train_items 214240.
I0302 19:00:45.446093 22683591385216 run.py:483] Algo bellman_ford step 6695 current loss 0.001809, current_train_items 214272.
I0302 19:00:45.461538 22683591385216 run.py:483] Algo bellman_ford step 6696 current loss 0.015303, current_train_items 214304.
I0302 19:00:45.484177 22683591385216 run.py:483] Algo bellman_ford step 6697 current loss 0.038078, current_train_items 214336.
I0302 19:00:45.514049 22683591385216 run.py:483] Algo bellman_ford step 6698 current loss 0.092177, current_train_items 214368.
I0302 19:00:45.546408 22683591385216 run.py:483] Algo bellman_ford step 6699 current loss 0.066215, current_train_items 214400.
I0302 19:00:45.564946 22683591385216 run.py:483] Algo bellman_ford step 6700 current loss 0.009996, current_train_items 214432.
I0302 19:00:45.572718 22683591385216 run.py:503] (val) algo bellman_ford step 6700: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 214432, 'step': 6700, 'algorithm': 'bellman_ford'}
I0302 19:00:45.572825 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:00:45.589160 22683591385216 run.py:483] Algo bellman_ford step 6701 current loss 0.057299, current_train_items 214464.
I0302 19:00:45.611978 22683591385216 run.py:483] Algo bellman_ford step 6702 current loss 0.056249, current_train_items 214496.
I0302 19:00:45.641847 22683591385216 run.py:483] Algo bellman_ford step 6703 current loss 0.091849, current_train_items 214528.
I0302 19:00:45.672801 22683591385216 run.py:483] Algo bellman_ford step 6704 current loss 0.094409, current_train_items 214560.
I0302 19:00:45.691287 22683591385216 run.py:483] Algo bellman_ford step 6705 current loss 0.011478, current_train_items 214592.
I0302 19:00:45.706617 22683591385216 run.py:483] Algo bellman_ford step 6706 current loss 0.015696, current_train_items 214624.
I0302 19:00:45.730294 22683591385216 run.py:483] Algo bellman_ford step 6707 current loss 0.043396, current_train_items 214656.
I0302 19:00:45.760078 22683591385216 run.py:483] Algo bellman_ford step 6708 current loss 0.053081, current_train_items 214688.
I0302 19:00:45.792582 22683591385216 run.py:483] Algo bellman_ford step 6709 current loss 0.067397, current_train_items 214720.
I0302 19:00:45.811012 22683591385216 run.py:483] Algo bellman_ford step 6710 current loss 0.003242, current_train_items 214752.
I0302 19:00:45.826480 22683591385216 run.py:483] Algo bellman_ford step 6711 current loss 0.013924, current_train_items 214784.
I0302 19:00:45.848046 22683591385216 run.py:483] Algo bellman_ford step 6712 current loss 0.042157, current_train_items 214816.
I0302 19:00:45.878671 22683591385216 run.py:483] Algo bellman_ford step 6713 current loss 0.072445, current_train_items 214848.
I0302 19:00:45.910050 22683591385216 run.py:483] Algo bellman_ford step 6714 current loss 0.049598, current_train_items 214880.
I0302 19:00:45.928318 22683591385216 run.py:483] Algo bellman_ford step 6715 current loss 0.008661, current_train_items 214912.
I0302 19:00:45.943515 22683591385216 run.py:483] Algo bellman_ford step 6716 current loss 0.005892, current_train_items 214944.
I0302 19:00:45.966160 22683591385216 run.py:483] Algo bellman_ford step 6717 current loss 0.035515, current_train_items 214976.
I0302 19:00:45.995585 22683591385216 run.py:483] Algo bellman_ford step 6718 current loss 0.110676, current_train_items 215008.
I0302 19:00:46.028680 22683591385216 run.py:483] Algo bellman_ford step 6719 current loss 0.124889, current_train_items 215040.
I0302 19:00:46.046705 22683591385216 run.py:483] Algo bellman_ford step 6720 current loss 0.001649, current_train_items 215072.
I0302 19:00:46.062077 22683591385216 run.py:483] Algo bellman_ford step 6721 current loss 0.006547, current_train_items 215104.
I0302 19:00:46.083917 22683591385216 run.py:483] Algo bellman_ford step 6722 current loss 0.032166, current_train_items 215136.
I0302 19:00:46.114222 22683591385216 run.py:483] Algo bellman_ford step 6723 current loss 0.041511, current_train_items 215168.
I0302 19:00:46.144135 22683591385216 run.py:483] Algo bellman_ford step 6724 current loss 0.065136, current_train_items 215200.
I0302 19:00:46.161911 22683591385216 run.py:483] Algo bellman_ford step 6725 current loss 0.002939, current_train_items 215232.
I0302 19:00:46.177259 22683591385216 run.py:483] Algo bellman_ford step 6726 current loss 0.033929, current_train_items 215264.
I0302 19:00:46.199214 22683591385216 run.py:483] Algo bellman_ford step 6727 current loss 0.026088, current_train_items 215296.
I0302 19:00:46.227781 22683591385216 run.py:483] Algo bellman_ford step 6728 current loss 0.032390, current_train_items 215328.
I0302 19:00:46.257601 22683591385216 run.py:483] Algo bellman_ford step 6729 current loss 0.046169, current_train_items 215360.
I0302 19:00:46.276044 22683591385216 run.py:483] Algo bellman_ford step 6730 current loss 0.003581, current_train_items 215392.
I0302 19:00:46.291075 22683591385216 run.py:483] Algo bellman_ford step 6731 current loss 0.004898, current_train_items 215424.
I0302 19:00:46.313875 22683591385216 run.py:483] Algo bellman_ford step 6732 current loss 0.095577, current_train_items 215456.
I0302 19:00:46.341258 22683591385216 run.py:483] Algo bellman_ford step 6733 current loss 0.084736, current_train_items 215488.
I0302 19:00:46.370406 22683591385216 run.py:483] Algo bellman_ford step 6734 current loss 0.071620, current_train_items 215520.
I0302 19:00:46.388544 22683591385216 run.py:483] Algo bellman_ford step 6735 current loss 0.003716, current_train_items 215552.
I0302 19:00:46.403914 22683591385216 run.py:483] Algo bellman_ford step 6736 current loss 0.020651, current_train_items 215584.
I0302 19:00:46.427191 22683591385216 run.py:483] Algo bellman_ford step 6737 current loss 0.044401, current_train_items 215616.
I0302 19:00:46.455125 22683591385216 run.py:483] Algo bellman_ford step 6738 current loss 0.033777, current_train_items 215648.
I0302 19:00:46.488245 22683591385216 run.py:483] Algo bellman_ford step 6739 current loss 0.121553, current_train_items 215680.
I0302 19:00:46.506456 22683591385216 run.py:483] Algo bellman_ford step 6740 current loss 0.014589, current_train_items 215712.
I0302 19:00:46.521940 22683591385216 run.py:483] Algo bellman_ford step 6741 current loss 0.007291, current_train_items 215744.
I0302 19:00:46.545366 22683591385216 run.py:483] Algo bellman_ford step 6742 current loss 0.067183, current_train_items 215776.
I0302 19:00:46.573385 22683591385216 run.py:483] Algo bellman_ford step 6743 current loss 0.033943, current_train_items 215808.
I0302 19:00:46.605779 22683591385216 run.py:483] Algo bellman_ford step 6744 current loss 0.065232, current_train_items 215840.
I0302 19:00:46.624082 22683591385216 run.py:483] Algo bellman_ford step 6745 current loss 0.001507, current_train_items 215872.
I0302 19:00:46.639416 22683591385216 run.py:483] Algo bellman_ford step 6746 current loss 0.065907, current_train_items 215904.
I0302 19:00:46.662078 22683591385216 run.py:483] Algo bellman_ford step 6747 current loss 0.015663, current_train_items 215936.
I0302 19:00:46.690748 22683591385216 run.py:483] Algo bellman_ford step 6748 current loss 0.047117, current_train_items 215968.
I0302 19:00:46.722375 22683591385216 run.py:483] Algo bellman_ford step 6749 current loss 0.074939, current_train_items 216000.
I0302 19:00:46.740435 22683591385216 run.py:483] Algo bellman_ford step 6750 current loss 0.001765, current_train_items 216032.
I0302 19:00:46.748341 22683591385216 run.py:503] (val) algo bellman_ford step 6750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 216032, 'step': 6750, 'algorithm': 'bellman_ford'}
I0302 19:00:46.748447 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:00:46.764382 22683591385216 run.py:483] Algo bellman_ford step 6751 current loss 0.008635, current_train_items 216064.
I0302 19:00:46.787648 22683591385216 run.py:483] Algo bellman_ford step 6752 current loss 0.060160, current_train_items 216096.
I0302 19:00:46.818281 22683591385216 run.py:483] Algo bellman_ford step 6753 current loss 0.052362, current_train_items 216128.
I0302 19:00:46.851150 22683591385216 run.py:483] Algo bellman_ford step 6754 current loss 0.046169, current_train_items 216160.
I0302 19:00:46.869542 22683591385216 run.py:483] Algo bellman_ford step 6755 current loss 0.007568, current_train_items 216192.
I0302 19:00:46.885356 22683591385216 run.py:483] Algo bellman_ford step 6756 current loss 0.022955, current_train_items 216224.
I0302 19:00:46.907374 22683591385216 run.py:483] Algo bellman_ford step 6757 current loss 0.012792, current_train_items 216256.
I0302 19:00:46.937282 22683591385216 run.py:483] Algo bellman_ford step 6758 current loss 0.061040, current_train_items 216288.
I0302 19:00:46.970060 22683591385216 run.py:483] Algo bellman_ford step 6759 current loss 0.115964, current_train_items 216320.
I0302 19:00:46.989022 22683591385216 run.py:483] Algo bellman_ford step 6760 current loss 0.005049, current_train_items 216352.
I0302 19:00:47.004832 22683591385216 run.py:483] Algo bellman_ford step 6761 current loss 0.008708, current_train_items 216384.
I0302 19:00:47.027064 22683591385216 run.py:483] Algo bellman_ford step 6762 current loss 0.019311, current_train_items 216416.
I0302 19:00:47.056820 22683591385216 run.py:483] Algo bellman_ford step 6763 current loss 0.048378, current_train_items 216448.
I0302 19:00:47.087365 22683591385216 run.py:483] Algo bellman_ford step 6764 current loss 0.057862, current_train_items 216480.
I0302 19:00:47.105769 22683591385216 run.py:483] Algo bellman_ford step 6765 current loss 0.002137, current_train_items 216512.
I0302 19:00:47.121316 22683591385216 run.py:483] Algo bellman_ford step 6766 current loss 0.042733, current_train_items 216544.
I0302 19:00:47.143820 22683591385216 run.py:483] Algo bellman_ford step 6767 current loss 0.016847, current_train_items 216576.
I0302 19:00:47.172343 22683591385216 run.py:483] Algo bellman_ford step 6768 current loss 0.026583, current_train_items 216608.
I0302 19:00:47.203341 22683591385216 run.py:483] Algo bellman_ford step 6769 current loss 0.081696, current_train_items 216640.
I0302 19:00:47.222215 22683591385216 run.py:483] Algo bellman_ford step 6770 current loss 0.000970, current_train_items 216672.
I0302 19:00:47.237706 22683591385216 run.py:483] Algo bellman_ford step 6771 current loss 0.026199, current_train_items 216704.
I0302 19:00:47.259190 22683591385216 run.py:483] Algo bellman_ford step 6772 current loss 0.033585, current_train_items 216736.
I0302 19:00:47.288529 22683591385216 run.py:483] Algo bellman_ford step 6773 current loss 0.134564, current_train_items 216768.
I0302 19:00:47.318145 22683591385216 run.py:483] Algo bellman_ford step 6774 current loss 0.083357, current_train_items 216800.
I0302 19:00:47.336685 22683591385216 run.py:483] Algo bellman_ford step 6775 current loss 0.004499, current_train_items 216832.
I0302 19:00:47.352264 22683591385216 run.py:483] Algo bellman_ford step 6776 current loss 0.019752, current_train_items 216864.
I0302 19:00:47.373980 22683591385216 run.py:483] Algo bellman_ford step 6777 current loss 0.012270, current_train_items 216896.
I0302 19:00:47.404350 22683591385216 run.py:483] Algo bellman_ford step 6778 current loss 0.052936, current_train_items 216928.
I0302 19:00:47.434990 22683591385216 run.py:483] Algo bellman_ford step 6779 current loss 0.030263, current_train_items 216960.
I0302 19:00:47.453231 22683591385216 run.py:483] Algo bellman_ford step 6780 current loss 0.001084, current_train_items 216992.
I0302 19:00:47.468253 22683591385216 run.py:483] Algo bellman_ford step 6781 current loss 0.005222, current_train_items 217024.
I0302 19:00:47.489881 22683591385216 run.py:483] Algo bellman_ford step 6782 current loss 0.038042, current_train_items 217056.
I0302 19:00:47.520325 22683591385216 run.py:483] Algo bellman_ford step 6783 current loss 0.072508, current_train_items 217088.
I0302 19:00:47.550902 22683591385216 run.py:483] Algo bellman_ford step 6784 current loss 0.091469, current_train_items 217120.
I0302 19:00:47.569291 22683591385216 run.py:483] Algo bellman_ford step 6785 current loss 0.002075, current_train_items 217152.
I0302 19:00:47.584566 22683591385216 run.py:483] Algo bellman_ford step 6786 current loss 0.005504, current_train_items 217184.
I0302 19:00:47.606288 22683591385216 run.py:483] Algo bellman_ford step 6787 current loss 0.045009, current_train_items 217216.
I0302 19:00:47.634227 22683591385216 run.py:483] Algo bellman_ford step 6788 current loss 0.029552, current_train_items 217248.
I0302 19:00:47.666118 22683591385216 run.py:483] Algo bellman_ford step 6789 current loss 0.088209, current_train_items 217280.
I0302 19:00:47.684553 22683591385216 run.py:483] Algo bellman_ford step 6790 current loss 0.008130, current_train_items 217312.
I0302 19:00:47.699973 22683591385216 run.py:483] Algo bellman_ford step 6791 current loss 0.012814, current_train_items 217344.
I0302 19:00:47.722363 22683591385216 run.py:483] Algo bellman_ford step 6792 current loss 0.022358, current_train_items 217376.
I0302 19:00:47.750814 22683591385216 run.py:483] Algo bellman_ford step 6793 current loss 0.072230, current_train_items 217408.
I0302 19:00:47.782142 22683591385216 run.py:483] Algo bellman_ford step 6794 current loss 0.094694, current_train_items 217440.
I0302 19:00:47.800171 22683591385216 run.py:483] Algo bellman_ford step 6795 current loss 0.003601, current_train_items 217472.
I0302 19:00:47.815219 22683591385216 run.py:483] Algo bellman_ford step 6796 current loss 0.009428, current_train_items 217504.
I0302 19:00:47.838230 22683591385216 run.py:483] Algo bellman_ford step 6797 current loss 0.037583, current_train_items 217536.
I0302 19:00:47.866768 22683591385216 run.py:483] Algo bellman_ford step 6798 current loss 0.025472, current_train_items 217568.
I0302 19:00:47.898117 22683591385216 run.py:483] Algo bellman_ford step 6799 current loss 0.051339, current_train_items 217600.
I0302 19:00:47.916724 22683591385216 run.py:483] Algo bellman_ford step 6800 current loss 0.020884, current_train_items 217632.
I0302 19:00:47.924408 22683591385216 run.py:503] (val) algo bellman_ford step 6800: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 217632, 'step': 6800, 'algorithm': 'bellman_ford'}
I0302 19:00:47.924516 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:47.940212 22683591385216 run.py:483] Algo bellman_ford step 6801 current loss 0.014621, current_train_items 217664.
I0302 19:00:47.963308 22683591385216 run.py:483] Algo bellman_ford step 6802 current loss 0.045301, current_train_items 217696.
I0302 19:00:47.994104 22683591385216 run.py:483] Algo bellman_ford step 6803 current loss 0.088738, current_train_items 217728.
I0302 19:00:48.025681 22683591385216 run.py:483] Algo bellman_ford step 6804 current loss 0.057284, current_train_items 217760.
I0302 19:00:48.044432 22683591385216 run.py:483] Algo bellman_ford step 6805 current loss 0.001784, current_train_items 217792.
I0302 19:00:48.059769 22683591385216 run.py:483] Algo bellman_ford step 6806 current loss 0.008257, current_train_items 217824.
I0302 19:00:48.082487 22683591385216 run.py:483] Algo bellman_ford step 6807 current loss 0.041241, current_train_items 217856.
I0302 19:00:48.110775 22683591385216 run.py:483] Algo bellman_ford step 6808 current loss 0.038232, current_train_items 217888.
I0302 19:00:48.141957 22683591385216 run.py:483] Algo bellman_ford step 6809 current loss 0.092590, current_train_items 217920.
I0302 19:00:48.160131 22683591385216 run.py:483] Algo bellman_ford step 6810 current loss 0.007163, current_train_items 217952.
I0302 19:00:48.174987 22683591385216 run.py:483] Algo bellman_ford step 6811 current loss 0.017052, current_train_items 217984.
I0302 19:00:48.198158 22683591385216 run.py:483] Algo bellman_ford step 6812 current loss 0.051716, current_train_items 218016.
I0302 19:00:48.226987 22683591385216 run.py:483] Algo bellman_ford step 6813 current loss 0.048934, current_train_items 218048.
I0302 19:00:48.256288 22683591385216 run.py:483] Algo bellman_ford step 6814 current loss 0.045933, current_train_items 218080.
I0302 19:00:48.274735 22683591385216 run.py:483] Algo bellman_ford step 6815 current loss 0.004167, current_train_items 218112.
I0302 19:00:48.289965 22683591385216 run.py:483] Algo bellman_ford step 6816 current loss 0.028375, current_train_items 218144.
I0302 19:00:48.311193 22683591385216 run.py:483] Algo bellman_ford step 6817 current loss 0.043026, current_train_items 218176.
I0302 19:00:48.340530 22683591385216 run.py:483] Algo bellman_ford step 6818 current loss 0.056913, current_train_items 218208.
I0302 19:00:48.373678 22683591385216 run.py:483] Algo bellman_ford step 6819 current loss 0.082128, current_train_items 218240.
I0302 19:00:48.392098 22683591385216 run.py:483] Algo bellman_ford step 6820 current loss 0.023102, current_train_items 218272.
I0302 19:00:48.407261 22683591385216 run.py:483] Algo bellman_ford step 6821 current loss 0.006975, current_train_items 218304.
I0302 19:00:48.429152 22683591385216 run.py:483] Algo bellman_ford step 6822 current loss 0.052351, current_train_items 218336.
I0302 19:00:48.458987 22683591385216 run.py:483] Algo bellman_ford step 6823 current loss 0.059667, current_train_items 218368.
I0302 19:00:48.491407 22683591385216 run.py:483] Algo bellman_ford step 6824 current loss 0.069119, current_train_items 218400.
I0302 19:00:48.509637 22683591385216 run.py:483] Algo bellman_ford step 6825 current loss 0.001402, current_train_items 218432.
I0302 19:00:48.525042 22683591385216 run.py:483] Algo bellman_ford step 6826 current loss 0.019717, current_train_items 218464.
I0302 19:00:48.547589 22683591385216 run.py:483] Algo bellman_ford step 6827 current loss 0.056376, current_train_items 218496.
I0302 19:00:48.576381 22683591385216 run.py:483] Algo bellman_ford step 6828 current loss 0.053066, current_train_items 218528.
I0302 19:00:48.609865 22683591385216 run.py:483] Algo bellman_ford step 6829 current loss 0.147938, current_train_items 218560.
I0302 19:00:48.627953 22683591385216 run.py:483] Algo bellman_ford step 6830 current loss 0.004559, current_train_items 218592.
I0302 19:00:48.643079 22683591385216 run.py:483] Algo bellman_ford step 6831 current loss 0.016302, current_train_items 218624.
I0302 19:00:48.667295 22683591385216 run.py:483] Algo bellman_ford step 6832 current loss 0.044817, current_train_items 218656.
I0302 19:00:48.696130 22683591385216 run.py:483] Algo bellman_ford step 6833 current loss 0.048333, current_train_items 218688.
I0302 19:00:48.727376 22683591385216 run.py:483] Algo bellman_ford step 6834 current loss 0.047560, current_train_items 218720.
I0302 19:00:48.745783 22683591385216 run.py:483] Algo bellman_ford step 6835 current loss 0.001122, current_train_items 218752.
I0302 19:00:48.761220 22683591385216 run.py:483] Algo bellman_ford step 6836 current loss 0.036225, current_train_items 218784.
I0302 19:00:48.783612 22683591385216 run.py:483] Algo bellman_ford step 6837 current loss 0.031921, current_train_items 218816.
I0302 19:00:48.813754 22683591385216 run.py:483] Algo bellman_ford step 6838 current loss 0.039887, current_train_items 218848.
I0302 19:00:48.843546 22683591385216 run.py:483] Algo bellman_ford step 6839 current loss 0.052217, current_train_items 218880.
I0302 19:00:48.861739 22683591385216 run.py:483] Algo bellman_ford step 6840 current loss 0.002313, current_train_items 218912.
I0302 19:00:48.877341 22683591385216 run.py:483] Algo bellman_ford step 6841 current loss 0.034050, current_train_items 218944.
I0302 19:00:48.900364 22683591385216 run.py:483] Algo bellman_ford step 6842 current loss 0.019797, current_train_items 218976.
I0302 19:00:48.928599 22683591385216 run.py:483] Algo bellman_ford step 6843 current loss 0.038692, current_train_items 219008.
I0302 19:00:48.959669 22683591385216 run.py:483] Algo bellman_ford step 6844 current loss 0.046402, current_train_items 219040.
I0302 19:00:48.978236 22683591385216 run.py:483] Algo bellman_ford step 6845 current loss 0.002644, current_train_items 219072.
I0302 19:00:48.994073 22683591385216 run.py:483] Algo bellman_ford step 6846 current loss 0.023364, current_train_items 219104.
I0302 19:00:49.017152 22683591385216 run.py:483] Algo bellman_ford step 6847 current loss 0.051134, current_train_items 219136.
I0302 19:00:49.046811 22683591385216 run.py:483] Algo bellman_ford step 6848 current loss 0.055969, current_train_items 219168.
I0302 19:00:49.078648 22683591385216 run.py:483] Algo bellman_ford step 6849 current loss 0.051478, current_train_items 219200.
I0302 19:00:49.096814 22683591385216 run.py:483] Algo bellman_ford step 6850 current loss 0.001415, current_train_items 219232.
I0302 19:00:49.104799 22683591385216 run.py:503] (val) algo bellman_ford step 6850: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 219232, 'step': 6850, 'algorithm': 'bellman_ford'}
I0302 19:00:49.104913 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:00:49.120872 22683591385216 run.py:483] Algo bellman_ford step 6851 current loss 0.025396, current_train_items 219264.
I0302 19:00:49.144033 22683591385216 run.py:483] Algo bellman_ford step 6852 current loss 0.023334, current_train_items 219296.
I0302 19:00:49.173828 22683591385216 run.py:483] Algo bellman_ford step 6853 current loss 0.030924, current_train_items 219328.
I0302 19:00:49.206344 22683591385216 run.py:483] Algo bellman_ford step 6854 current loss 0.038151, current_train_items 219360.
I0302 19:00:49.224718 22683591385216 run.py:483] Algo bellman_ford step 6855 current loss 0.001496, current_train_items 219392.
I0302 19:00:49.239689 22683591385216 run.py:483] Algo bellman_ford step 6856 current loss 0.003678, current_train_items 219424.
I0302 19:00:49.262186 22683591385216 run.py:483] Algo bellman_ford step 6857 current loss 0.026110, current_train_items 219456.
I0302 19:00:49.290625 22683591385216 run.py:483] Algo bellman_ford step 6858 current loss 0.016051, current_train_items 219488.
I0302 19:00:49.321956 22683591385216 run.py:483] Algo bellman_ford step 6859 current loss 0.061849, current_train_items 219520.
I0302 19:00:49.340558 22683591385216 run.py:483] Algo bellman_ford step 6860 current loss 0.001102, current_train_items 219552.
I0302 19:00:49.355963 22683591385216 run.py:483] Algo bellman_ford step 6861 current loss 0.010528, current_train_items 219584.
I0302 19:00:49.377482 22683591385216 run.py:483] Algo bellman_ford step 6862 current loss 0.022717, current_train_items 219616.
I0302 19:00:49.405851 22683591385216 run.py:483] Algo bellman_ford step 6863 current loss 0.036201, current_train_items 219648.
I0302 19:00:49.435831 22683591385216 run.py:483] Algo bellman_ford step 6864 current loss 0.055958, current_train_items 219680.
I0302 19:00:49.454242 22683591385216 run.py:483] Algo bellman_ford step 6865 current loss 0.011644, current_train_items 219712.
I0302 19:00:49.469554 22683591385216 run.py:483] Algo bellman_ford step 6866 current loss 0.007455, current_train_items 219744.
I0302 19:00:49.493107 22683591385216 run.py:483] Algo bellman_ford step 6867 current loss 0.034330, current_train_items 219776.
I0302 19:00:49.522660 22683591385216 run.py:483] Algo bellman_ford step 6868 current loss 0.034793, current_train_items 219808.
I0302 19:00:49.556197 22683591385216 run.py:483] Algo bellman_ford step 6869 current loss 0.058155, current_train_items 219840.
I0302 19:00:49.575127 22683591385216 run.py:483] Algo bellman_ford step 6870 current loss 0.005772, current_train_items 219872.
I0302 19:00:49.590946 22683591385216 run.py:483] Algo bellman_ford step 6871 current loss 0.014007, current_train_items 219904.
I0302 19:00:49.612699 22683591385216 run.py:483] Algo bellman_ford step 6872 current loss 0.046228, current_train_items 219936.
I0302 19:00:49.640882 22683591385216 run.py:483] Algo bellman_ford step 6873 current loss 0.092054, current_train_items 219968.
I0302 19:00:49.671877 22683591385216 run.py:483] Algo bellman_ford step 6874 current loss 0.081756, current_train_items 220000.
I0302 19:00:49.690336 22683591385216 run.py:483] Algo bellman_ford step 6875 current loss 0.004654, current_train_items 220032.
I0302 19:00:49.705534 22683591385216 run.py:483] Algo bellman_ford step 6876 current loss 0.006159, current_train_items 220064.
I0302 19:00:49.727742 22683591385216 run.py:483] Algo bellman_ford step 6877 current loss 0.048159, current_train_items 220096.
I0302 19:00:49.757003 22683591385216 run.py:483] Algo bellman_ford step 6878 current loss 0.020231, current_train_items 220128.
I0302 19:00:49.788678 22683591385216 run.py:483] Algo bellman_ford step 6879 current loss 0.049816, current_train_items 220160.
I0302 19:00:49.806938 22683591385216 run.py:483] Algo bellman_ford step 6880 current loss 0.011223, current_train_items 220192.
I0302 19:00:49.821874 22683591385216 run.py:483] Algo bellman_ford step 6881 current loss 0.003143, current_train_items 220224.
I0302 19:00:49.844989 22683591385216 run.py:483] Algo bellman_ford step 6882 current loss 0.019831, current_train_items 220256.
I0302 19:00:49.873188 22683591385216 run.py:483] Algo bellman_ford step 6883 current loss 0.034498, current_train_items 220288.
I0302 19:00:49.904018 22683591385216 run.py:483] Algo bellman_ford step 6884 current loss 0.091747, current_train_items 220320.
I0302 19:00:49.922644 22683591385216 run.py:483] Algo bellman_ford step 6885 current loss 0.000922, current_train_items 220352.
I0302 19:00:49.938386 22683591385216 run.py:483] Algo bellman_ford step 6886 current loss 0.016383, current_train_items 220384.
I0302 19:00:49.960886 22683591385216 run.py:483] Algo bellman_ford step 6887 current loss 0.033841, current_train_items 220416.
I0302 19:00:49.990856 22683591385216 run.py:483] Algo bellman_ford step 6888 current loss 0.054589, current_train_items 220448.
I0302 19:00:50.022525 22683591385216 run.py:483] Algo bellman_ford step 6889 current loss 0.021015, current_train_items 220480.
I0302 19:00:50.040992 22683591385216 run.py:483] Algo bellman_ford step 6890 current loss 0.027337, current_train_items 220512.
I0302 19:00:50.056864 22683591385216 run.py:483] Algo bellman_ford step 6891 current loss 0.019149, current_train_items 220544.
I0302 19:00:50.078889 22683591385216 run.py:483] Algo bellman_ford step 6892 current loss 0.044082, current_train_items 220576.
I0302 19:00:50.108607 22683591385216 run.py:483] Algo bellman_ford step 6893 current loss 0.063986, current_train_items 220608.
I0302 19:00:50.139096 22683591385216 run.py:483] Algo bellman_ford step 6894 current loss 0.051788, current_train_items 220640.
I0302 19:00:50.157369 22683591385216 run.py:483] Algo bellman_ford step 6895 current loss 0.002852, current_train_items 220672.
I0302 19:00:50.172506 22683591385216 run.py:483] Algo bellman_ford step 6896 current loss 0.007262, current_train_items 220704.
I0302 19:00:50.195409 22683591385216 run.py:483] Algo bellman_ford step 6897 current loss 0.079333, current_train_items 220736.
I0302 19:00:50.223419 22683591385216 run.py:483] Algo bellman_ford step 6898 current loss 0.028866, current_train_items 220768.
I0302 19:00:50.255717 22683591385216 run.py:483] Algo bellman_ford step 6899 current loss 0.116566, current_train_items 220800.
I0302 19:00:50.274525 22683591385216 run.py:483] Algo bellman_ford step 6900 current loss 0.005278, current_train_items 220832.
I0302 19:00:50.282495 22683591385216 run.py:503] (val) algo bellman_ford step 6900: {'pi': 0.978515625, 'score': 0.978515625, 'examples_seen': 220832, 'step': 6900, 'algorithm': 'bellman_ford'}
I0302 19:00:50.282599 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:00:50.298148 22683591385216 run.py:483] Algo bellman_ford step 6901 current loss 0.004650, current_train_items 220864.
I0302 19:00:50.320684 22683591385216 run.py:483] Algo bellman_ford step 6902 current loss 0.040809, current_train_items 220896.
I0302 19:00:50.351219 22683591385216 run.py:483] Algo bellman_ford step 6903 current loss 0.044590, current_train_items 220928.
I0302 19:00:50.384786 22683591385216 run.py:483] Algo bellman_ford step 6904 current loss 0.067714, current_train_items 220960.
I0302 19:00:50.402942 22683591385216 run.py:483] Algo bellman_ford step 6905 current loss 0.003159, current_train_items 220992.
I0302 19:00:50.417782 22683591385216 run.py:483] Algo bellman_ford step 6906 current loss 0.005687, current_train_items 221024.
I0302 19:00:50.440212 22683591385216 run.py:483] Algo bellman_ford step 6907 current loss 0.038254, current_train_items 221056.
I0302 19:00:50.470683 22683591385216 run.py:483] Algo bellman_ford step 6908 current loss 0.096423, current_train_items 221088.
I0302 19:00:50.503647 22683591385216 run.py:483] Algo bellman_ford step 6909 current loss 0.059258, current_train_items 221120.
I0302 19:00:50.522175 22683591385216 run.py:483] Algo bellman_ford step 6910 current loss 0.039189, current_train_items 221152.
I0302 19:00:50.538255 22683591385216 run.py:483] Algo bellman_ford step 6911 current loss 0.027264, current_train_items 221184.
I0302 19:00:50.559514 22683591385216 run.py:483] Algo bellman_ford step 6912 current loss 0.035836, current_train_items 221216.
I0302 19:00:50.589842 22683591385216 run.py:483] Algo bellman_ford step 6913 current loss 0.026627, current_train_items 221248.
I0302 19:00:50.623064 22683591385216 run.py:483] Algo bellman_ford step 6914 current loss 0.038950, current_train_items 221280.
I0302 19:00:50.641355 22683591385216 run.py:483] Algo bellman_ford step 6915 current loss 0.006211, current_train_items 221312.
I0302 19:00:50.656228 22683591385216 run.py:483] Algo bellman_ford step 6916 current loss 0.001975, current_train_items 221344.
I0302 19:00:50.679821 22683591385216 run.py:483] Algo bellman_ford step 6917 current loss 0.056577, current_train_items 221376.
I0302 19:00:50.709854 22683591385216 run.py:483] Algo bellman_ford step 6918 current loss 0.031084, current_train_items 221408.
I0302 19:00:50.742871 22683591385216 run.py:483] Algo bellman_ford step 6919 current loss 0.054535, current_train_items 221440.
I0302 19:00:50.760968 22683591385216 run.py:483] Algo bellman_ford step 6920 current loss 0.002405, current_train_items 221472.
I0302 19:00:50.776225 22683591385216 run.py:483] Algo bellman_ford step 6921 current loss 0.004987, current_train_items 221504.
I0302 19:00:50.798207 22683591385216 run.py:483] Algo bellman_ford step 6922 current loss 0.040583, current_train_items 221536.
I0302 19:00:50.827941 22683591385216 run.py:483] Algo bellman_ford step 6923 current loss 0.055106, current_train_items 221568.
I0302 19:00:50.859307 22683591385216 run.py:483] Algo bellman_ford step 6924 current loss 0.023679, current_train_items 221600.
I0302 19:00:50.877859 22683591385216 run.py:483] Algo bellman_ford step 6925 current loss 0.001632, current_train_items 221632.
I0302 19:00:50.892954 22683591385216 run.py:483] Algo bellman_ford step 6926 current loss 0.008715, current_train_items 221664.
I0302 19:00:50.915638 22683591385216 run.py:483] Algo bellman_ford step 6927 current loss 0.033364, current_train_items 221696.
I0302 19:00:50.945348 22683591385216 run.py:483] Algo bellman_ford step 6928 current loss 0.038568, current_train_items 221728.
I0302 19:00:50.976342 22683591385216 run.py:483] Algo bellman_ford step 6929 current loss 0.037624, current_train_items 221760.
I0302 19:00:50.994518 22683591385216 run.py:483] Algo bellman_ford step 6930 current loss 0.002884, current_train_items 221792.
I0302 19:00:51.010040 22683591385216 run.py:483] Algo bellman_ford step 6931 current loss 0.017729, current_train_items 221824.
I0302 19:00:51.031842 22683591385216 run.py:483] Algo bellman_ford step 6932 current loss 0.029627, current_train_items 221856.
I0302 19:00:51.061336 22683591385216 run.py:483] Algo bellman_ford step 6933 current loss 0.032188, current_train_items 221888.
I0302 19:00:51.093849 22683591385216 run.py:483] Algo bellman_ford step 6934 current loss 0.061337, current_train_items 221920.
I0302 19:00:51.112214 22683591385216 run.py:483] Algo bellman_ford step 6935 current loss 0.010138, current_train_items 221952.
I0302 19:00:51.127210 22683591385216 run.py:483] Algo bellman_ford step 6936 current loss 0.006475, current_train_items 221984.
I0302 19:00:51.149535 22683591385216 run.py:483] Algo bellman_ford step 6937 current loss 0.020573, current_train_items 222016.
I0302 19:00:51.179085 22683591385216 run.py:483] Algo bellman_ford step 6938 current loss 0.024077, current_train_items 222048.
I0302 19:00:51.209563 22683591385216 run.py:483] Algo bellman_ford step 6939 current loss 0.037248, current_train_items 222080.
I0302 19:00:51.227888 22683591385216 run.py:483] Algo bellman_ford step 6940 current loss 0.006180, current_train_items 222112.
I0302 19:00:51.243538 22683591385216 run.py:483] Algo bellman_ford step 6941 current loss 0.015756, current_train_items 222144.
I0302 19:00:51.266400 22683591385216 run.py:483] Algo bellman_ford step 6942 current loss 0.052476, current_train_items 222176.
I0302 19:00:51.295914 22683591385216 run.py:483] Algo bellman_ford step 6943 current loss 0.057835, current_train_items 222208.
I0302 19:00:51.328192 22683591385216 run.py:483] Algo bellman_ford step 6944 current loss 0.084797, current_train_items 222240.
I0302 19:00:51.346591 22683591385216 run.py:483] Algo bellman_ford step 6945 current loss 0.003872, current_train_items 222272.
I0302 19:00:51.362055 22683591385216 run.py:483] Algo bellman_ford step 6946 current loss 0.008990, current_train_items 222304.
I0302 19:00:51.384431 22683591385216 run.py:483] Algo bellman_ford step 6947 current loss 0.120837, current_train_items 222336.
I0302 19:00:51.411652 22683591385216 run.py:483] Algo bellman_ford step 6948 current loss 0.061473, current_train_items 222368.
I0302 19:00:51.443959 22683591385216 run.py:483] Algo bellman_ford step 6949 current loss 0.075426, current_train_items 222400.
I0302 19:00:51.462128 22683591385216 run.py:483] Algo bellman_ford step 6950 current loss 0.006538, current_train_items 222432.
I0302 19:00:51.470030 22683591385216 run.py:503] (val) algo bellman_ford step 6950: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 222432, 'step': 6950, 'algorithm': 'bellman_ford'}
I0302 19:00:51.470136 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:00:51.486142 22683591385216 run.py:483] Algo bellman_ford step 6951 current loss 0.020799, current_train_items 222464.
I0302 19:00:51.509163 22683591385216 run.py:483] Algo bellman_ford step 6952 current loss 0.048125, current_train_items 222496.
I0302 19:00:51.537710 22683591385216 run.py:483] Algo bellman_ford step 6953 current loss 0.065707, current_train_items 222528.
I0302 19:00:51.569996 22683591385216 run.py:483] Algo bellman_ford step 6954 current loss 0.097269, current_train_items 222560.
I0302 19:00:51.588314 22683591385216 run.py:483] Algo bellman_ford step 6955 current loss 0.003166, current_train_items 222592.
I0302 19:00:51.603658 22683591385216 run.py:483] Algo bellman_ford step 6956 current loss 0.021701, current_train_items 222624.
I0302 19:00:51.627177 22683591385216 run.py:483] Algo bellman_ford step 6957 current loss 0.071997, current_train_items 222656.
I0302 19:00:51.656030 22683591385216 run.py:483] Algo bellman_ford step 6958 current loss 0.046900, current_train_items 222688.
I0302 19:00:51.685549 22683591385216 run.py:483] Algo bellman_ford step 6959 current loss 0.052452, current_train_items 222720.
I0302 19:00:51.703836 22683591385216 run.py:483] Algo bellman_ford step 6960 current loss 0.003624, current_train_items 222752.
I0302 19:00:51.719691 22683591385216 run.py:483] Algo bellman_ford step 6961 current loss 0.015131, current_train_items 222784.
I0302 19:00:51.741221 22683591385216 run.py:483] Algo bellman_ford step 6962 current loss 0.021351, current_train_items 222816.
I0302 19:00:51.770140 22683591385216 run.py:483] Algo bellman_ford step 6963 current loss 0.062514, current_train_items 222848.
I0302 19:00:51.803057 22683591385216 run.py:483] Algo bellman_ford step 6964 current loss 0.045745, current_train_items 222880.
I0302 19:00:51.821270 22683591385216 run.py:483] Algo bellman_ford step 6965 current loss 0.002035, current_train_items 222912.
I0302 19:00:51.836579 22683591385216 run.py:483] Algo bellman_ford step 6966 current loss 0.014693, current_train_items 222944.
I0302 19:00:51.858863 22683591385216 run.py:483] Algo bellman_ford step 6967 current loss 0.013604, current_train_items 222976.
I0302 19:00:51.887133 22683591385216 run.py:483] Algo bellman_ford step 6968 current loss 0.037595, current_train_items 223008.
I0302 19:00:51.918184 22683591385216 run.py:483] Algo bellman_ford step 6969 current loss 0.050618, current_train_items 223040.
I0302 19:00:51.936519 22683591385216 run.py:483] Algo bellman_ford step 6970 current loss 0.001264, current_train_items 223072.
I0302 19:00:51.952022 22683591385216 run.py:483] Algo bellman_ford step 6971 current loss 0.011566, current_train_items 223104.
I0302 19:00:51.973784 22683591385216 run.py:483] Algo bellman_ford step 6972 current loss 0.028889, current_train_items 223136.
I0302 19:00:52.002321 22683591385216 run.py:483] Algo bellman_ford step 6973 current loss 0.029924, current_train_items 223168.
I0302 19:00:52.033865 22683591385216 run.py:483] Algo bellman_ford step 6974 current loss 0.032228, current_train_items 223200.
I0302 19:00:52.051952 22683591385216 run.py:483] Algo bellman_ford step 6975 current loss 0.000949, current_train_items 223232.
I0302 19:00:52.067520 22683591385216 run.py:483] Algo bellman_ford step 6976 current loss 0.005957, current_train_items 223264.
I0302 19:00:52.089082 22683591385216 run.py:483] Algo bellman_ford step 6977 current loss 0.027982, current_train_items 223296.
I0302 19:00:52.117153 22683591385216 run.py:483] Algo bellman_ford step 6978 current loss 0.031433, current_train_items 223328.
I0302 19:00:52.149220 22683591385216 run.py:483] Algo bellman_ford step 6979 current loss 0.026003, current_train_items 223360.
I0302 19:00:52.167170 22683591385216 run.py:483] Algo bellman_ford step 6980 current loss 0.013703, current_train_items 223392.
I0302 19:00:52.183026 22683591385216 run.py:483] Algo bellman_ford step 6981 current loss 0.004910, current_train_items 223424.
I0302 19:00:52.204207 22683591385216 run.py:483] Algo bellman_ford step 6982 current loss 0.023943, current_train_items 223456.
I0302 19:00:52.234075 22683591385216 run.py:483] Algo bellman_ford step 6983 current loss 0.053256, current_train_items 223488.
I0302 19:00:52.265386 22683591385216 run.py:483] Algo bellman_ford step 6984 current loss 0.049467, current_train_items 223520.
I0302 19:00:52.283884 22683591385216 run.py:483] Algo bellman_ford step 6985 current loss 0.001915, current_train_items 223552.
I0302 19:00:52.299239 22683591385216 run.py:483] Algo bellman_ford step 6986 current loss 0.012675, current_train_items 223584.
I0302 19:00:52.320997 22683591385216 run.py:483] Algo bellman_ford step 6987 current loss 0.022911, current_train_items 223616.
I0302 19:00:52.349947 22683591385216 run.py:483] Algo bellman_ford step 6988 current loss 0.040201, current_train_items 223648.
I0302 19:00:52.383082 22683591385216 run.py:483] Algo bellman_ford step 6989 current loss 0.055243, current_train_items 223680.
I0302 19:00:52.401553 22683591385216 run.py:483] Algo bellman_ford step 6990 current loss 0.002456, current_train_items 223712.
I0302 19:00:52.416915 22683591385216 run.py:483] Algo bellman_ford step 6991 current loss 0.024580, current_train_items 223744.
I0302 19:00:52.438794 22683591385216 run.py:483] Algo bellman_ford step 6992 current loss 0.052534, current_train_items 223776.
I0302 19:00:52.468561 22683591385216 run.py:483] Algo bellman_ford step 6993 current loss 0.054084, current_train_items 223808.
I0302 19:00:52.502258 22683591385216 run.py:483] Algo bellman_ford step 6994 current loss 0.059083, current_train_items 223840.
I0302 19:00:52.520185 22683591385216 run.py:483] Algo bellman_ford step 6995 current loss 0.002075, current_train_items 223872.
I0302 19:00:52.535656 22683591385216 run.py:483] Algo bellman_ford step 6996 current loss 0.006156, current_train_items 223904.
I0302 19:00:52.556322 22683591385216 run.py:483] Algo bellman_ford step 6997 current loss 0.050338, current_train_items 223936.
I0302 19:00:52.584427 22683591385216 run.py:483] Algo bellman_ford step 6998 current loss 0.021965, current_train_items 223968.
I0302 19:00:52.615595 22683591385216 run.py:483] Algo bellman_ford step 6999 current loss 0.082566, current_train_items 224000.
I0302 19:00:52.633972 22683591385216 run.py:483] Algo bellman_ford step 7000 current loss 0.003764, current_train_items 224032.
I0302 19:00:52.641589 22683591385216 run.py:503] (val) algo bellman_ford step 7000: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 224032, 'step': 7000, 'algorithm': 'bellman_ford'}
I0302 19:00:52.641695 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:00:52.657293 22683591385216 run.py:483] Algo bellman_ford step 7001 current loss 0.025712, current_train_items 224064.
I0302 19:00:52.679376 22683591385216 run.py:483] Algo bellman_ford step 7002 current loss 0.026388, current_train_items 224096.
I0302 19:00:52.707181 22683591385216 run.py:483] Algo bellman_ford step 7003 current loss 0.033963, current_train_items 224128.
I0302 19:00:52.738046 22683591385216 run.py:483] Algo bellman_ford step 7004 current loss 0.037007, current_train_items 224160.
I0302 19:00:52.756350 22683591385216 run.py:483] Algo bellman_ford step 7005 current loss 0.001918, current_train_items 224192.
I0302 19:00:52.771656 22683591385216 run.py:483] Algo bellman_ford step 7006 current loss 0.030562, current_train_items 224224.
I0302 19:00:52.794562 22683591385216 run.py:483] Algo bellman_ford step 7007 current loss 0.065887, current_train_items 224256.
I0302 19:00:52.822353 22683591385216 run.py:483] Algo bellman_ford step 7008 current loss 0.036078, current_train_items 224288.
I0302 19:00:52.856406 22683591385216 run.py:483] Algo bellman_ford step 7009 current loss 0.080778, current_train_items 224320.
I0302 19:00:52.874575 22683591385216 run.py:483] Algo bellman_ford step 7010 current loss 0.002431, current_train_items 224352.
I0302 19:00:52.889971 22683591385216 run.py:483] Algo bellman_ford step 7011 current loss 0.003162, current_train_items 224384.
I0302 19:00:52.911992 22683591385216 run.py:483] Algo bellman_ford step 7012 current loss 0.071736, current_train_items 224416.
I0302 19:00:52.940713 22683591385216 run.py:483] Algo bellman_ford step 7013 current loss 0.068150, current_train_items 224448.
I0302 19:00:52.972572 22683591385216 run.py:483] Algo bellman_ford step 7014 current loss 0.058651, current_train_items 224480.
I0302 19:00:52.990970 22683591385216 run.py:483] Algo bellman_ford step 7015 current loss 0.001489, current_train_items 224512.
I0302 19:00:53.006129 22683591385216 run.py:483] Algo bellman_ford step 7016 current loss 0.016113, current_train_items 224544.
I0302 19:00:53.028583 22683591385216 run.py:483] Algo bellman_ford step 7017 current loss 0.023220, current_train_items 224576.
I0302 19:00:53.056832 22683591385216 run.py:483] Algo bellman_ford step 7018 current loss 0.029787, current_train_items 224608.
I0302 19:00:53.087451 22683591385216 run.py:483] Algo bellman_ford step 7019 current loss 0.046699, current_train_items 224640.
I0302 19:00:53.105695 22683591385216 run.py:483] Algo bellman_ford step 7020 current loss 0.001559, current_train_items 224672.
I0302 19:00:53.120849 22683591385216 run.py:483] Algo bellman_ford step 7021 current loss 0.024864, current_train_items 224704.
I0302 19:00:53.142817 22683591385216 run.py:483] Algo bellman_ford step 7022 current loss 0.057927, current_train_items 224736.
I0302 19:00:53.170907 22683591385216 run.py:483] Algo bellman_ford step 7023 current loss 0.014771, current_train_items 224768.
I0302 19:00:53.203168 22683591385216 run.py:483] Algo bellman_ford step 7024 current loss 0.074810, current_train_items 224800.
I0302 19:00:53.221489 22683591385216 run.py:483] Algo bellman_ford step 7025 current loss 0.001594, current_train_items 224832.
I0302 19:00:53.237092 22683591385216 run.py:483] Algo bellman_ford step 7026 current loss 0.010810, current_train_items 224864.
I0302 19:00:53.260904 22683591385216 run.py:483] Algo bellman_ford step 7027 current loss 0.066082, current_train_items 224896.
I0302 19:00:53.292060 22683591385216 run.py:483] Algo bellman_ford step 7028 current loss 0.078249, current_train_items 224928.
I0302 19:00:53.321816 22683591385216 run.py:483] Algo bellman_ford step 7029 current loss 0.049827, current_train_items 224960.
I0302 19:00:53.340109 22683591385216 run.py:483] Algo bellman_ford step 7030 current loss 0.001306, current_train_items 224992.
I0302 19:00:53.355436 22683591385216 run.py:483] Algo bellman_ford step 7031 current loss 0.007120, current_train_items 225024.
I0302 19:00:53.377506 22683591385216 run.py:483] Algo bellman_ford step 7032 current loss 0.043746, current_train_items 225056.
I0302 19:00:53.406395 22683591385216 run.py:483] Algo bellman_ford step 7033 current loss 0.048012, current_train_items 225088.
I0302 19:00:53.439982 22683591385216 run.py:483] Algo bellman_ford step 7034 current loss 0.058343, current_train_items 225120.
I0302 19:00:53.458135 22683591385216 run.py:483] Algo bellman_ford step 7035 current loss 0.002834, current_train_items 225152.
I0302 19:00:53.475329 22683591385216 run.py:483] Algo bellman_ford step 7036 current loss 0.009848, current_train_items 225184.
I0302 19:00:53.497762 22683591385216 run.py:483] Algo bellman_ford step 7037 current loss 0.037068, current_train_items 225216.
I0302 19:00:53.527902 22683591385216 run.py:483] Algo bellman_ford step 7038 current loss 0.093619, current_train_items 225248.
I0302 19:00:53.559019 22683591385216 run.py:483] Algo bellman_ford step 7039 current loss 0.039684, current_train_items 225280.
I0302 19:00:53.577332 22683591385216 run.py:483] Algo bellman_ford step 7040 current loss 0.004197, current_train_items 225312.
I0302 19:00:53.592590 22683591385216 run.py:483] Algo bellman_ford step 7041 current loss 0.031662, current_train_items 225344.
I0302 19:00:53.615009 22683591385216 run.py:483] Algo bellman_ford step 7042 current loss 0.024060, current_train_items 225376.
I0302 19:00:53.644955 22683591385216 run.py:483] Algo bellman_ford step 7043 current loss 0.058581, current_train_items 225408.
I0302 19:00:53.677033 22683591385216 run.py:483] Algo bellman_ford step 7044 current loss 0.035796, current_train_items 225440.
I0302 19:00:53.695487 22683591385216 run.py:483] Algo bellman_ford step 7045 current loss 0.002273, current_train_items 225472.
I0302 19:00:53.710904 22683591385216 run.py:483] Algo bellman_ford step 7046 current loss 0.006876, current_train_items 225504.
I0302 19:00:53.733016 22683591385216 run.py:483] Algo bellman_ford step 7047 current loss 0.059547, current_train_items 225536.
I0302 19:00:53.762677 22683591385216 run.py:483] Algo bellman_ford step 7048 current loss 0.052968, current_train_items 225568.
I0302 19:00:53.793406 22683591385216 run.py:483] Algo bellman_ford step 7049 current loss 0.047475, current_train_items 225600.
I0302 19:00:53.811849 22683591385216 run.py:483] Algo bellman_ford step 7050 current loss 0.001067, current_train_items 225632.
I0302 19:00:53.819888 22683591385216 run.py:503] (val) algo bellman_ford step 7050: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 225632, 'step': 7050, 'algorithm': 'bellman_ford'}
I0302 19:00:53.820005 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:53.835451 22683591385216 run.py:483] Algo bellman_ford step 7051 current loss 0.003481, current_train_items 225664.
I0302 19:00:53.858717 22683591385216 run.py:483] Algo bellman_ford step 7052 current loss 0.108374, current_train_items 225696.
I0302 19:00:53.889491 22683591385216 run.py:483] Algo bellman_ford step 7053 current loss 0.077104, current_train_items 225728.
I0302 19:00:53.922097 22683591385216 run.py:483] Algo bellman_ford step 7054 current loss 0.121732, current_train_items 225760.
I0302 19:00:53.940389 22683591385216 run.py:483] Algo bellman_ford step 7055 current loss 0.007434, current_train_items 225792.
I0302 19:00:53.955651 22683591385216 run.py:483] Algo bellman_ford step 7056 current loss 0.030718, current_train_items 225824.
I0302 19:00:53.978595 22683591385216 run.py:483] Algo bellman_ford step 7057 current loss 0.071610, current_train_items 225856.
I0302 19:00:54.007531 22683591385216 run.py:483] Algo bellman_ford step 7058 current loss 0.069964, current_train_items 225888.
I0302 19:00:54.040958 22683591385216 run.py:483] Algo bellman_ford step 7059 current loss 0.099752, current_train_items 225920.
I0302 19:00:54.059229 22683591385216 run.py:483] Algo bellman_ford step 7060 current loss 0.012034, current_train_items 225952.
I0302 19:00:54.074879 22683591385216 run.py:483] Algo bellman_ford step 7061 current loss 0.038978, current_train_items 225984.
I0302 19:00:54.097711 22683591385216 run.py:483] Algo bellman_ford step 7062 current loss 0.027849, current_train_items 226016.
I0302 19:00:54.124241 22683591385216 run.py:483] Algo bellman_ford step 7063 current loss 0.040560, current_train_items 226048.
I0302 19:00:54.155537 22683591385216 run.py:483] Algo bellman_ford step 7064 current loss 0.037110, current_train_items 226080.
I0302 19:00:54.173585 22683591385216 run.py:483] Algo bellman_ford step 7065 current loss 0.003778, current_train_items 226112.
I0302 19:00:54.189267 22683591385216 run.py:483] Algo bellman_ford step 7066 current loss 0.025923, current_train_items 226144.
I0302 19:00:54.211117 22683591385216 run.py:483] Algo bellman_ford step 7067 current loss 0.053694, current_train_items 226176.
I0302 19:00:54.242486 22683591385216 run.py:483] Algo bellman_ford step 7068 current loss 0.054032, current_train_items 226208.
I0302 19:00:54.275723 22683591385216 run.py:483] Algo bellman_ford step 7069 current loss 0.043041, current_train_items 226240.
I0302 19:00:54.294200 22683591385216 run.py:483] Algo bellman_ford step 7070 current loss 0.025676, current_train_items 226272.
I0302 19:00:54.309671 22683591385216 run.py:483] Algo bellman_ford step 7071 current loss 0.023503, current_train_items 226304.
I0302 19:00:54.330018 22683591385216 run.py:483] Algo bellman_ford step 7072 current loss 0.024817, current_train_items 226336.
I0302 19:00:54.358294 22683591385216 run.py:483] Algo bellman_ford step 7073 current loss 0.017561, current_train_items 226368.
I0302 19:00:54.389214 22683591385216 run.py:483] Algo bellman_ford step 7074 current loss 0.053031, current_train_items 226400.
I0302 19:00:54.407649 22683591385216 run.py:483] Algo bellman_ford step 7075 current loss 0.002463, current_train_items 226432.
I0302 19:00:54.422562 22683591385216 run.py:483] Algo bellman_ford step 7076 current loss 0.005986, current_train_items 226464.
I0302 19:00:54.444412 22683591385216 run.py:483] Algo bellman_ford step 7077 current loss 0.043002, current_train_items 226496.
I0302 19:00:54.473740 22683591385216 run.py:483] Algo bellman_ford step 7078 current loss 0.052236, current_train_items 226528.
I0302 19:00:54.505817 22683591385216 run.py:483] Algo bellman_ford step 7079 current loss 0.072039, current_train_items 226560.
I0302 19:00:54.523776 22683591385216 run.py:483] Algo bellman_ford step 7080 current loss 0.002835, current_train_items 226592.
I0302 19:00:54.538824 22683591385216 run.py:483] Algo bellman_ford step 7081 current loss 0.005565, current_train_items 226624.
I0302 19:00:54.560751 22683591385216 run.py:483] Algo bellman_ford step 7082 current loss 0.054021, current_train_items 226656.
I0302 19:00:54.590162 22683591385216 run.py:483] Algo bellman_ford step 7083 current loss 0.040117, current_train_items 226688.
I0302 19:00:54.620827 22683591385216 run.py:483] Algo bellman_ford step 7084 current loss 0.045363, current_train_items 226720.
I0302 19:00:54.639612 22683591385216 run.py:483] Algo bellman_ford step 7085 current loss 0.001794, current_train_items 226752.
I0302 19:00:54.654954 22683591385216 run.py:483] Algo bellman_ford step 7086 current loss 0.005444, current_train_items 226784.
I0302 19:00:54.675974 22683591385216 run.py:483] Algo bellman_ford step 7087 current loss 0.010979, current_train_items 226816.
I0302 19:00:54.705072 22683591385216 run.py:483] Algo bellman_ford step 7088 current loss 0.044448, current_train_items 226848.
I0302 19:00:54.735815 22683591385216 run.py:483] Algo bellman_ford step 7089 current loss 0.029211, current_train_items 226880.
I0302 19:00:54.754642 22683591385216 run.py:483] Algo bellman_ford step 7090 current loss 0.002716, current_train_items 226912.
I0302 19:00:54.769860 22683591385216 run.py:483] Algo bellman_ford step 7091 current loss 0.012582, current_train_items 226944.
I0302 19:00:54.792001 22683591385216 run.py:483] Algo bellman_ford step 7092 current loss 0.024427, current_train_items 226976.
I0302 19:00:54.821923 22683591385216 run.py:483] Algo bellman_ford step 7093 current loss 0.083537, current_train_items 227008.
I0302 19:00:54.851512 22683591385216 run.py:483] Algo bellman_ford step 7094 current loss 0.046913, current_train_items 227040.
I0302 19:00:54.869623 22683591385216 run.py:483] Algo bellman_ford step 7095 current loss 0.001513, current_train_items 227072.
I0302 19:00:54.884779 22683591385216 run.py:483] Algo bellman_ford step 7096 current loss 0.015745, current_train_items 227104.
I0302 19:00:54.905826 22683591385216 run.py:483] Algo bellman_ford step 7097 current loss 0.031755, current_train_items 227136.
I0302 19:00:54.935297 22683591385216 run.py:483] Algo bellman_ford step 7098 current loss 0.045725, current_train_items 227168.
I0302 19:00:54.965592 22683591385216 run.py:483] Algo bellman_ford step 7099 current loss 0.088899, current_train_items 227200.
I0302 19:00:54.984204 22683591385216 run.py:483] Algo bellman_ford step 7100 current loss 0.002583, current_train_items 227232.
I0302 19:00:54.991923 22683591385216 run.py:503] (val) algo bellman_ford step 7100: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 227232, 'step': 7100, 'algorithm': 'bellman_ford'}
I0302 19:00:54.992030 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:00:55.007910 22683591385216 run.py:483] Algo bellman_ford step 7101 current loss 0.020890, current_train_items 227264.
I0302 19:00:55.030488 22683591385216 run.py:483] Algo bellman_ford step 7102 current loss 0.015994, current_train_items 227296.
I0302 19:00:55.060579 22683591385216 run.py:483] Algo bellman_ford step 7103 current loss 0.054466, current_train_items 227328.
I0302 19:00:55.093614 22683591385216 run.py:483] Algo bellman_ford step 7104 current loss 0.090264, current_train_items 227360.
I0302 19:00:55.112016 22683591385216 run.py:483] Algo bellman_ford step 7105 current loss 0.019379, current_train_items 227392.
I0302 19:00:55.127393 22683591385216 run.py:483] Algo bellman_ford step 7106 current loss 0.009945, current_train_items 227424.
I0302 19:00:55.149993 22683591385216 run.py:483] Algo bellman_ford step 7107 current loss 0.020069, current_train_items 227456.
I0302 19:00:55.179427 22683591385216 run.py:483] Algo bellman_ford step 7108 current loss 0.088061, current_train_items 227488.
I0302 19:00:55.212780 22683591385216 run.py:483] Algo bellman_ford step 7109 current loss 0.083567, current_train_items 227520.
I0302 19:00:55.231257 22683591385216 run.py:483] Algo bellman_ford step 7110 current loss 0.003084, current_train_items 227552.
I0302 19:00:55.246647 22683591385216 run.py:483] Algo bellman_ford step 7111 current loss 0.013403, current_train_items 227584.
I0302 19:00:55.269168 22683591385216 run.py:483] Algo bellman_ford step 7112 current loss 0.064102, current_train_items 227616.
I0302 19:00:55.299428 22683591385216 run.py:483] Algo bellman_ford step 7113 current loss 0.081840, current_train_items 227648.
I0302 19:00:55.330981 22683591385216 run.py:483] Algo bellman_ford step 7114 current loss 0.106741, current_train_items 227680.
I0302 19:00:55.349542 22683591385216 run.py:483] Algo bellman_ford step 7115 current loss 0.001892, current_train_items 227712.
I0302 19:00:55.364853 22683591385216 run.py:483] Algo bellman_ford step 7116 current loss 0.017414, current_train_items 227744.
I0302 19:00:55.387576 22683591385216 run.py:483] Algo bellman_ford step 7117 current loss 0.017062, current_train_items 227776.
I0302 19:00:55.417487 22683591385216 run.py:483] Algo bellman_ford step 7118 current loss 0.074966, current_train_items 227808.
I0302 19:00:55.449279 22683591385216 run.py:483] Algo bellman_ford step 7119 current loss 0.079628, current_train_items 227840.
I0302 19:00:55.467360 22683591385216 run.py:483] Algo bellman_ford step 7120 current loss 0.049238, current_train_items 227872.
I0302 19:00:55.482841 22683591385216 run.py:483] Algo bellman_ford step 7121 current loss 0.012597, current_train_items 227904.
I0302 19:00:55.504886 22683591385216 run.py:483] Algo bellman_ford step 7122 current loss 0.029748, current_train_items 227936.
I0302 19:00:55.533848 22683591385216 run.py:483] Algo bellman_ford step 7123 current loss 0.030434, current_train_items 227968.
I0302 19:00:55.565629 22683591385216 run.py:483] Algo bellman_ford step 7124 current loss 0.049003, current_train_items 228000.
I0302 19:00:55.583806 22683591385216 run.py:483] Algo bellman_ford step 7125 current loss 0.001436, current_train_items 228032.
I0302 19:00:55.598463 22683591385216 run.py:483] Algo bellman_ford step 7126 current loss 0.006262, current_train_items 228064.
I0302 19:00:55.620147 22683591385216 run.py:483] Algo bellman_ford step 7127 current loss 0.018574, current_train_items 228096.
I0302 19:00:55.648756 22683591385216 run.py:483] Algo bellman_ford step 7128 current loss 0.062762, current_train_items 228128.
I0302 19:00:55.681619 22683591385216 run.py:483] Algo bellman_ford step 7129 current loss 0.054145, current_train_items 228160.
I0302 19:00:55.700025 22683591385216 run.py:483] Algo bellman_ford step 7130 current loss 0.014905, current_train_items 228192.
I0302 19:00:55.715590 22683591385216 run.py:483] Algo bellman_ford step 7131 current loss 0.027550, current_train_items 228224.
I0302 19:00:55.738497 22683591385216 run.py:483] Algo bellman_ford step 7132 current loss 0.035605, current_train_items 228256.
I0302 19:00:55.767599 22683591385216 run.py:483] Algo bellman_ford step 7133 current loss 0.040770, current_train_items 228288.
I0302 19:00:55.798842 22683591385216 run.py:483] Algo bellman_ford step 7134 current loss 0.052647, current_train_items 228320.
I0302 19:00:55.817113 22683591385216 run.py:483] Algo bellman_ford step 7135 current loss 0.021411, current_train_items 228352.
I0302 19:00:55.832071 22683591385216 run.py:483] Algo bellman_ford step 7136 current loss 0.013566, current_train_items 228384.
I0302 19:00:55.853617 22683591385216 run.py:483] Algo bellman_ford step 7137 current loss 0.017372, current_train_items 228416.
I0302 19:00:55.881443 22683591385216 run.py:483] Algo bellman_ford step 7138 current loss 0.047663, current_train_items 228448.
I0302 19:00:55.915236 22683591385216 run.py:483] Algo bellman_ford step 7139 current loss 0.107517, current_train_items 228480.
I0302 19:00:55.933508 22683591385216 run.py:483] Algo bellman_ford step 7140 current loss 0.000703, current_train_items 228512.
I0302 19:00:55.948893 22683591385216 run.py:483] Algo bellman_ford step 7141 current loss 0.026084, current_train_items 228544.
I0302 19:00:55.971449 22683591385216 run.py:483] Algo bellman_ford step 7142 current loss 0.034967, current_train_items 228576.
I0302 19:00:55.999236 22683591385216 run.py:483] Algo bellman_ford step 7143 current loss 0.055351, current_train_items 228608.
I0302 19:00:56.030090 22683591385216 run.py:483] Algo bellman_ford step 7144 current loss 0.077970, current_train_items 228640.
I0302 19:00:56.048729 22683591385216 run.py:483] Algo bellman_ford step 7145 current loss 0.002245, current_train_items 228672.
I0302 19:00:56.064227 22683591385216 run.py:483] Algo bellman_ford step 7146 current loss 0.011144, current_train_items 228704.
I0302 19:00:56.087244 22683591385216 run.py:483] Algo bellman_ford step 7147 current loss 0.022369, current_train_items 228736.
I0302 19:00:56.115157 22683591385216 run.py:483] Algo bellman_ford step 7148 current loss 0.038428, current_train_items 228768.
I0302 19:00:56.144092 22683591385216 run.py:483] Algo bellman_ford step 7149 current loss 0.094708, current_train_items 228800.
I0302 19:00:56.162049 22683591385216 run.py:483] Algo bellman_ford step 7150 current loss 0.000965, current_train_items 228832.
I0302 19:00:56.170029 22683591385216 run.py:503] (val) algo bellman_ford step 7150: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 228832, 'step': 7150, 'algorithm': 'bellman_ford'}
I0302 19:00:56.170134 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:00:56.186429 22683591385216 run.py:483] Algo bellman_ford step 7151 current loss 0.059316, current_train_items 228864.
I0302 19:00:56.209619 22683591385216 run.py:483] Algo bellman_ford step 7152 current loss 0.055993, current_train_items 228896.
I0302 19:00:56.239450 22683591385216 run.py:483] Algo bellman_ford step 7153 current loss 0.065181, current_train_items 228928.
I0302 19:00:56.270305 22683591385216 run.py:483] Algo bellman_ford step 7154 current loss 0.047692, current_train_items 228960.
I0302 19:00:56.288800 22683591385216 run.py:483] Algo bellman_ford step 7155 current loss 0.003748, current_train_items 228992.
I0302 19:00:56.304283 22683591385216 run.py:483] Algo bellman_ford step 7156 current loss 0.032126, current_train_items 229024.
I0302 19:00:56.326345 22683591385216 run.py:483] Algo bellman_ford step 7157 current loss 0.051077, current_train_items 229056.
I0302 19:00:56.354789 22683591385216 run.py:483] Algo bellman_ford step 7158 current loss 0.070662, current_train_items 229088.
I0302 19:00:56.386487 22683591385216 run.py:483] Algo bellman_ford step 7159 current loss 0.101239, current_train_items 229120.
I0302 19:00:56.404825 22683591385216 run.py:483] Algo bellman_ford step 7160 current loss 0.002971, current_train_items 229152.
I0302 19:00:56.420780 22683591385216 run.py:483] Algo bellman_ford step 7161 current loss 0.026278, current_train_items 229184.
W0302 19:00:56.434878 22683591385216 samplers.py:155] Increasing hint lengh from 10 to 11
I0302 19:01:01.839958 22683591385216 run.py:483] Algo bellman_ford step 7162 current loss 0.033306, current_train_items 229216.
I0302 19:01:01.870292 22683591385216 run.py:483] Algo bellman_ford step 7163 current loss 0.040723, current_train_items 229248.
I0302 19:01:01.903195 22683591385216 run.py:483] Algo bellman_ford step 7164 current loss 0.044357, current_train_items 229280.
I0302 19:01:01.921514 22683591385216 run.py:483] Algo bellman_ford step 7165 current loss 0.003190, current_train_items 229312.
I0302 19:01:01.937168 22683591385216 run.py:483] Algo bellman_ford step 7166 current loss 0.016837, current_train_items 229344.
I0302 19:01:01.959394 22683591385216 run.py:483] Algo bellman_ford step 7167 current loss 0.024388, current_train_items 229376.
I0302 19:01:01.989215 22683591385216 run.py:483] Algo bellman_ford step 7168 current loss 0.032540, current_train_items 229408.
I0302 19:01:02.018265 22683591385216 run.py:483] Algo bellman_ford step 7169 current loss 0.043423, current_train_items 229440.
I0302 19:01:02.037152 22683591385216 run.py:483] Algo bellman_ford step 7170 current loss 0.003034, current_train_items 229472.
I0302 19:01:02.053278 22683591385216 run.py:483] Algo bellman_ford step 7171 current loss 0.009369, current_train_items 229504.
I0302 19:01:02.075475 22683591385216 run.py:483] Algo bellman_ford step 7172 current loss 0.042183, current_train_items 229536.
I0302 19:01:02.105125 22683591385216 run.py:483] Algo bellman_ford step 7173 current loss 0.042029, current_train_items 229568.
I0302 19:01:02.137489 22683591385216 run.py:483] Algo bellman_ford step 7174 current loss 0.054220, current_train_items 229600.
I0302 19:01:02.156217 22683591385216 run.py:483] Algo bellman_ford step 7175 current loss 0.001081, current_train_items 229632.
I0302 19:01:02.171569 22683591385216 run.py:483] Algo bellman_ford step 7176 current loss 0.027896, current_train_items 229664.
I0302 19:01:02.193754 22683591385216 run.py:483] Algo bellman_ford step 7177 current loss 0.018896, current_train_items 229696.
I0302 19:01:02.225148 22683591385216 run.py:483] Algo bellman_ford step 7178 current loss 0.033451, current_train_items 229728.
I0302 19:01:02.257800 22683591385216 run.py:483] Algo bellman_ford step 7179 current loss 0.045056, current_train_items 229760.
I0302 19:01:02.276408 22683591385216 run.py:483] Algo bellman_ford step 7180 current loss 0.001486, current_train_items 229792.
I0302 19:01:02.291616 22683591385216 run.py:483] Algo bellman_ford step 7181 current loss 0.005137, current_train_items 229824.
I0302 19:01:02.314583 22683591385216 run.py:483] Algo bellman_ford step 7182 current loss 0.027516, current_train_items 229856.
I0302 19:01:02.343384 22683591385216 run.py:483] Algo bellman_ford step 7183 current loss 0.045354, current_train_items 229888.
I0302 19:01:02.377911 22683591385216 run.py:483] Algo bellman_ford step 7184 current loss 0.082124, current_train_items 229920.
I0302 19:01:02.396357 22683591385216 run.py:483] Algo bellman_ford step 7185 current loss 0.001446, current_train_items 229952.
I0302 19:01:02.411957 22683591385216 run.py:483] Algo bellman_ford step 7186 current loss 0.014911, current_train_items 229984.
I0302 19:01:02.435008 22683591385216 run.py:483] Algo bellman_ford step 7187 current loss 0.040274, current_train_items 230016.
I0302 19:01:02.464745 22683591385216 run.py:483] Algo bellman_ford step 7188 current loss 0.052372, current_train_items 230048.
I0302 19:01:02.497660 22683591385216 run.py:483] Algo bellman_ford step 7189 current loss 0.054185, current_train_items 230080.
I0302 19:01:02.516546 22683591385216 run.py:483] Algo bellman_ford step 7190 current loss 0.001129, current_train_items 230112.
I0302 19:01:02.532450 22683591385216 run.py:483] Algo bellman_ford step 7191 current loss 0.014946, current_train_items 230144.
I0302 19:01:02.554542 22683591385216 run.py:483] Algo bellman_ford step 7192 current loss 0.012554, current_train_items 230176.
I0302 19:01:02.584509 22683591385216 run.py:483] Algo bellman_ford step 7193 current loss 0.025728, current_train_items 230208.
I0302 19:01:02.615883 22683591385216 run.py:483] Algo bellman_ford step 7194 current loss 0.081522, current_train_items 230240.
I0302 19:01:02.634310 22683591385216 run.py:483] Algo bellman_ford step 7195 current loss 0.000784, current_train_items 230272.
I0302 19:01:02.649328 22683591385216 run.py:483] Algo bellman_ford step 7196 current loss 0.006297, current_train_items 230304.
I0302 19:01:02.671518 22683591385216 run.py:483] Algo bellman_ford step 7197 current loss 0.012974, current_train_items 230336.
I0302 19:01:02.701755 22683591385216 run.py:483] Algo bellman_ford step 7198 current loss 0.037241, current_train_items 230368.
I0302 19:01:02.733753 22683591385216 run.py:483] Algo bellman_ford step 7199 current loss 0.056813, current_train_items 230400.
I0302 19:01:02.752570 22683591385216 run.py:483] Algo bellman_ford step 7200 current loss 0.001004, current_train_items 230432.
I0302 19:01:02.761765 22683591385216 run.py:503] (val) algo bellman_ford step 7200: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 230432, 'step': 7200, 'algorithm': 'bellman_ford'}
I0302 19:01:02.761893 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 19:01:02.777794 22683591385216 run.py:483] Algo bellman_ford step 7201 current loss 0.042187, current_train_items 230464.
I0302 19:01:02.800215 22683591385216 run.py:483] Algo bellman_ford step 7202 current loss 0.016002, current_train_items 230496.
I0302 19:01:02.830627 22683591385216 run.py:483] Algo bellman_ford step 7203 current loss 0.075810, current_train_items 230528.
I0302 19:01:02.862279 22683591385216 run.py:483] Algo bellman_ford step 7204 current loss 0.046098, current_train_items 230560.
I0302 19:01:02.880884 22683591385216 run.py:483] Algo bellman_ford step 7205 current loss 0.002637, current_train_items 230592.
I0302 19:01:02.895921 22683591385216 run.py:483] Algo bellman_ford step 7206 current loss 0.022273, current_train_items 230624.
I0302 19:01:02.918473 22683591385216 run.py:483] Algo bellman_ford step 7207 current loss 0.110978, current_train_items 230656.
I0302 19:01:02.947566 22683591385216 run.py:483] Algo bellman_ford step 7208 current loss 0.058003, current_train_items 230688.
I0302 19:01:02.980284 22683591385216 run.py:483] Algo bellman_ford step 7209 current loss 0.099620, current_train_items 230720.
I0302 19:01:02.998851 22683591385216 run.py:483] Algo bellman_ford step 7210 current loss 0.002118, current_train_items 230752.
I0302 19:01:03.014003 22683591385216 run.py:483] Algo bellman_ford step 7211 current loss 0.013936, current_train_items 230784.
I0302 19:01:03.035961 22683591385216 run.py:483] Algo bellman_ford step 7212 current loss 0.079176, current_train_items 230816.
I0302 19:01:03.065095 22683591385216 run.py:483] Algo bellman_ford step 7213 current loss 0.072705, current_train_items 230848.
I0302 19:01:03.094575 22683591385216 run.py:483] Algo bellman_ford step 7214 current loss 0.119155, current_train_items 230880.
I0302 19:01:03.112711 22683591385216 run.py:483] Algo bellman_ford step 7215 current loss 0.009298, current_train_items 230912.
I0302 19:01:03.128030 22683591385216 run.py:483] Algo bellman_ford step 7216 current loss 0.004166, current_train_items 230944.
I0302 19:01:03.150639 22683591385216 run.py:483] Algo bellman_ford step 7217 current loss 0.052041, current_train_items 230976.
I0302 19:01:03.180428 22683591385216 run.py:483] Algo bellman_ford step 7218 current loss 0.036111, current_train_items 231008.
I0302 19:01:03.211264 22683591385216 run.py:483] Algo bellman_ford step 7219 current loss 0.053032, current_train_items 231040.
I0302 19:01:03.229547 22683591385216 run.py:483] Algo bellman_ford step 7220 current loss 0.025167, current_train_items 231072.
I0302 19:01:03.244976 22683591385216 run.py:483] Algo bellman_ford step 7221 current loss 0.007525, current_train_items 231104.
I0302 19:01:03.268473 22683591385216 run.py:483] Algo bellman_ford step 7222 current loss 0.061964, current_train_items 231136.
I0302 19:01:03.298314 22683591385216 run.py:483] Algo bellman_ford step 7223 current loss 0.131637, current_train_items 231168.
I0302 19:01:03.330670 22683591385216 run.py:483] Algo bellman_ford step 7224 current loss 0.116721, current_train_items 231200.
I0302 19:01:03.349023 22683591385216 run.py:483] Algo bellman_ford step 7225 current loss 0.001861, current_train_items 231232.
I0302 19:01:03.364399 22683591385216 run.py:483] Algo bellman_ford step 7226 current loss 0.017946, current_train_items 231264.
I0302 19:01:03.388177 22683591385216 run.py:483] Algo bellman_ford step 7227 current loss 0.053519, current_train_items 231296.
I0302 19:01:03.417845 22683591385216 run.py:483] Algo bellman_ford step 7228 current loss 0.055205, current_train_items 231328.
I0302 19:01:03.447476 22683591385216 run.py:483] Algo bellman_ford step 7229 current loss 0.048975, current_train_items 231360.
I0302 19:01:03.465776 22683591385216 run.py:483] Algo bellman_ford step 7230 current loss 0.003578, current_train_items 231392.
I0302 19:01:03.481301 22683591385216 run.py:483] Algo bellman_ford step 7231 current loss 0.006662, current_train_items 231424.
I0302 19:01:03.501843 22683591385216 run.py:483] Algo bellman_ford step 7232 current loss 0.009238, current_train_items 231456.
I0302 19:01:03.532109 22683591385216 run.py:483] Algo bellman_ford step 7233 current loss 0.049519, current_train_items 231488.
I0302 19:01:03.562685 22683591385216 run.py:483] Algo bellman_ford step 7234 current loss 0.031821, current_train_items 231520.
I0302 19:01:03.581172 22683591385216 run.py:483] Algo bellman_ford step 7235 current loss 0.002147, current_train_items 231552.
I0302 19:01:03.596803 22683591385216 run.py:483] Algo bellman_ford step 7236 current loss 0.019391, current_train_items 231584.
I0302 19:01:03.620217 22683591385216 run.py:483] Algo bellman_ford step 7237 current loss 0.025141, current_train_items 231616.
I0302 19:01:03.651106 22683591385216 run.py:483] Algo bellman_ford step 7238 current loss 0.063593, current_train_items 231648.
I0302 19:01:03.682812 22683591385216 run.py:483] Algo bellman_ford step 7239 current loss 0.035124, current_train_items 231680.
I0302 19:01:03.701354 22683591385216 run.py:483] Algo bellman_ford step 7240 current loss 0.005809, current_train_items 231712.
I0302 19:01:03.716791 22683591385216 run.py:483] Algo bellman_ford step 7241 current loss 0.005469, current_train_items 231744.
I0302 19:01:03.740197 22683591385216 run.py:483] Algo bellman_ford step 7242 current loss 0.046246, current_train_items 231776.
I0302 19:01:03.770026 22683591385216 run.py:483] Algo bellman_ford step 7243 current loss 0.035152, current_train_items 231808.
I0302 19:01:03.803144 22683591385216 run.py:483] Algo bellman_ford step 7244 current loss 0.039753, current_train_items 231840.
I0302 19:01:03.821553 22683591385216 run.py:483] Algo bellman_ford step 7245 current loss 0.001102, current_train_items 231872.
I0302 19:01:03.837034 22683591385216 run.py:483] Algo bellman_ford step 7246 current loss 0.005464, current_train_items 231904.
I0302 19:01:03.859808 22683591385216 run.py:483] Algo bellman_ford step 7247 current loss 0.020466, current_train_items 231936.
I0302 19:01:03.890906 22683591385216 run.py:483] Algo bellman_ford step 7248 current loss 0.053586, current_train_items 231968.
I0302 19:01:03.922944 22683591385216 run.py:483] Algo bellman_ford step 7249 current loss 0.054001, current_train_items 232000.
I0302 19:01:03.941990 22683591385216 run.py:483] Algo bellman_ford step 7250 current loss 0.024185, current_train_items 232032.
I0302 19:01:03.950395 22683591385216 run.py:503] (val) algo bellman_ford step 7250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 232032, 'step': 7250, 'algorithm': 'bellman_ford'}
I0302 19:01:03.950499 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:03.966635 22683591385216 run.py:483] Algo bellman_ford step 7251 current loss 0.019364, current_train_items 232064.
I0302 19:01:03.989142 22683591385216 run.py:483] Algo bellman_ford step 7252 current loss 0.041127, current_train_items 232096.
I0302 19:01:04.017987 22683591385216 run.py:483] Algo bellman_ford step 7253 current loss 0.032131, current_train_items 232128.
I0302 19:01:04.049802 22683591385216 run.py:483] Algo bellman_ford step 7254 current loss 0.064850, current_train_items 232160.
I0302 19:01:04.068016 22683591385216 run.py:483] Algo bellman_ford step 7255 current loss 0.001478, current_train_items 232192.
I0302 19:01:04.082986 22683591385216 run.py:483] Algo bellman_ford step 7256 current loss 0.038867, current_train_items 232224.
I0302 19:01:04.105080 22683591385216 run.py:483] Algo bellman_ford step 7257 current loss 0.045924, current_train_items 232256.
I0302 19:01:04.136809 22683591385216 run.py:483] Algo bellman_ford step 7258 current loss 0.043873, current_train_items 232288.
I0302 19:01:04.168440 22683591385216 run.py:483] Algo bellman_ford step 7259 current loss 0.059130, current_train_items 232320.
I0302 19:01:04.187045 22683591385216 run.py:483] Algo bellman_ford step 7260 current loss 0.003505, current_train_items 232352.
I0302 19:01:04.202711 22683591385216 run.py:483] Algo bellman_ford step 7261 current loss 0.029087, current_train_items 232384.
I0302 19:01:04.224768 22683591385216 run.py:483] Algo bellman_ford step 7262 current loss 0.021257, current_train_items 232416.
I0302 19:01:04.254386 22683591385216 run.py:483] Algo bellman_ford step 7263 current loss 0.041665, current_train_items 232448.
I0302 19:01:04.285492 22683591385216 run.py:483] Algo bellman_ford step 7264 current loss 0.040341, current_train_items 232480.
I0302 19:01:04.303372 22683591385216 run.py:483] Algo bellman_ford step 7265 current loss 0.002368, current_train_items 232512.
I0302 19:01:04.318563 22683591385216 run.py:483] Algo bellman_ford step 7266 current loss 0.033445, current_train_items 232544.
I0302 19:01:04.340399 22683591385216 run.py:483] Algo bellman_ford step 7267 current loss 0.025351, current_train_items 232576.
I0302 19:01:04.369958 22683591385216 run.py:483] Algo bellman_ford step 7268 current loss 0.039451, current_train_items 232608.
I0302 19:01:04.400335 22683591385216 run.py:483] Algo bellman_ford step 7269 current loss 0.061278, current_train_items 232640.
I0302 19:01:04.418694 22683591385216 run.py:483] Algo bellman_ford step 7270 current loss 0.003976, current_train_items 232672.
I0302 19:01:04.434560 22683591385216 run.py:483] Algo bellman_ford step 7271 current loss 0.040071, current_train_items 232704.
I0302 19:01:04.456637 22683591385216 run.py:483] Algo bellman_ford step 7272 current loss 0.010295, current_train_items 232736.
I0302 19:01:04.485638 22683591385216 run.py:483] Algo bellman_ford step 7273 current loss 0.022734, current_train_items 232768.
I0302 19:01:04.516830 22683591385216 run.py:483] Algo bellman_ford step 7274 current loss 0.054006, current_train_items 232800.
I0302 19:01:04.535303 22683591385216 run.py:483] Algo bellman_ford step 7275 current loss 0.001652, current_train_items 232832.
I0302 19:01:04.550710 22683591385216 run.py:483] Algo bellman_ford step 7276 current loss 0.012025, current_train_items 232864.
I0302 19:01:04.572916 22683591385216 run.py:483] Algo bellman_ford step 7277 current loss 0.028881, current_train_items 232896.
I0302 19:01:04.603198 22683591385216 run.py:483] Algo bellman_ford step 7278 current loss 0.043392, current_train_items 232928.
I0302 19:01:04.634752 22683591385216 run.py:483] Algo bellman_ford step 7279 current loss 0.044975, current_train_items 232960.
I0302 19:01:04.652768 22683591385216 run.py:483] Algo bellman_ford step 7280 current loss 0.031552, current_train_items 232992.
I0302 19:01:04.668382 22683591385216 run.py:483] Algo bellman_ford step 7281 current loss 0.018838, current_train_items 233024.
I0302 19:01:04.690086 22683591385216 run.py:483] Algo bellman_ford step 7282 current loss 0.032265, current_train_items 233056.
I0302 19:01:04.719753 22683591385216 run.py:483] Algo bellman_ford step 7283 current loss 0.044568, current_train_items 233088.
I0302 19:01:04.750262 22683591385216 run.py:483] Algo bellman_ford step 7284 current loss 0.035379, current_train_items 233120.
I0302 19:01:04.768685 22683591385216 run.py:483] Algo bellman_ford step 7285 current loss 0.002830, current_train_items 233152.
I0302 19:01:04.785248 22683591385216 run.py:483] Algo bellman_ford step 7286 current loss 0.023536, current_train_items 233184.
I0302 19:01:04.806913 22683591385216 run.py:483] Algo bellman_ford step 7287 current loss 0.054806, current_train_items 233216.
I0302 19:01:04.836128 22683591385216 run.py:483] Algo bellman_ford step 7288 current loss 0.054348, current_train_items 233248.
I0302 19:01:04.866505 22683591385216 run.py:483] Algo bellman_ford step 7289 current loss 0.039839, current_train_items 233280.
I0302 19:01:04.885138 22683591385216 run.py:483] Algo bellman_ford step 7290 current loss 0.022253, current_train_items 233312.
I0302 19:01:04.900845 22683591385216 run.py:483] Algo bellman_ford step 7291 current loss 0.014235, current_train_items 233344.
I0302 19:01:04.921857 22683591385216 run.py:483] Algo bellman_ford step 7292 current loss 0.012596, current_train_items 233376.
I0302 19:01:04.951878 22683591385216 run.py:483] Algo bellman_ford step 7293 current loss 0.035217, current_train_items 233408.
I0302 19:01:04.981580 22683591385216 run.py:483] Algo bellman_ford step 7294 current loss 0.059928, current_train_items 233440.
I0302 19:01:04.999677 22683591385216 run.py:483] Algo bellman_ford step 7295 current loss 0.013803, current_train_items 233472.
I0302 19:01:05.015036 22683591385216 run.py:483] Algo bellman_ford step 7296 current loss 0.016850, current_train_items 233504.
I0302 19:01:05.037341 22683591385216 run.py:483] Algo bellman_ford step 7297 current loss 0.037042, current_train_items 233536.
I0302 19:01:05.066973 22683591385216 run.py:483] Algo bellman_ford step 7298 current loss 0.029945, current_train_items 233568.
I0302 19:01:05.097931 22683591385216 run.py:483] Algo bellman_ford step 7299 current loss 0.053454, current_train_items 233600.
I0302 19:01:05.116349 22683591385216 run.py:483] Algo bellman_ford step 7300 current loss 0.007953, current_train_items 233632.
I0302 19:01:05.124195 22683591385216 run.py:503] (val) algo bellman_ford step 7300: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 233632, 'step': 7300, 'algorithm': 'bellman_ford'}
I0302 19:01:05.124303 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:05.140652 22683591385216 run.py:483] Algo bellman_ford step 7301 current loss 0.005645, current_train_items 233664.
I0302 19:01:05.163577 22683591385216 run.py:483] Algo bellman_ford step 7302 current loss 0.036053, current_train_items 233696.
I0302 19:01:05.193585 22683591385216 run.py:483] Algo bellman_ford step 7303 current loss 0.026589, current_train_items 233728.
I0302 19:01:05.226070 22683591385216 run.py:483] Algo bellman_ford step 7304 current loss 0.042248, current_train_items 233760.
I0302 19:01:05.244928 22683591385216 run.py:483] Algo bellman_ford step 7305 current loss 0.001857, current_train_items 233792.
I0302 19:01:05.260059 22683591385216 run.py:483] Algo bellman_ford step 7306 current loss 0.011335, current_train_items 233824.
I0302 19:01:05.282169 22683591385216 run.py:483] Algo bellman_ford step 7307 current loss 0.041164, current_train_items 233856.
I0302 19:01:05.310728 22683591385216 run.py:483] Algo bellman_ford step 7308 current loss 0.029767, current_train_items 233888.
I0302 19:01:05.343450 22683591385216 run.py:483] Algo bellman_ford step 7309 current loss 0.057821, current_train_items 233920.
I0302 19:01:05.361772 22683591385216 run.py:483] Algo bellman_ford step 7310 current loss 0.001629, current_train_items 233952.
I0302 19:01:05.376915 22683591385216 run.py:483] Algo bellman_ford step 7311 current loss 0.003935, current_train_items 233984.
I0302 19:01:05.401181 22683591385216 run.py:483] Algo bellman_ford step 7312 current loss 0.076343, current_train_items 234016.
I0302 19:01:05.431027 22683591385216 run.py:483] Algo bellman_ford step 7313 current loss 0.087850, current_train_items 234048.
I0302 19:01:05.463536 22683591385216 run.py:483] Algo bellman_ford step 7314 current loss 0.047229, current_train_items 234080.
I0302 19:01:05.481849 22683591385216 run.py:483] Algo bellman_ford step 7315 current loss 0.002372, current_train_items 234112.
I0302 19:01:05.497069 22683591385216 run.py:483] Algo bellman_ford step 7316 current loss 0.003792, current_train_items 234144.
I0302 19:01:05.520498 22683591385216 run.py:483] Algo bellman_ford step 7317 current loss 0.068421, current_train_items 234176.
I0302 19:01:05.550088 22683591385216 run.py:483] Algo bellman_ford step 7318 current loss 0.087940, current_train_items 234208.
I0302 19:01:05.582964 22683591385216 run.py:483] Algo bellman_ford step 7319 current loss 0.065741, current_train_items 234240.
I0302 19:01:05.601065 22683591385216 run.py:483] Algo bellman_ford step 7320 current loss 0.002329, current_train_items 234272.
I0302 19:01:05.616995 22683591385216 run.py:483] Algo bellman_ford step 7321 current loss 0.014504, current_train_items 234304.
I0302 19:01:05.640632 22683591385216 run.py:483] Algo bellman_ford step 7322 current loss 0.049389, current_train_items 234336.
I0302 19:01:05.669772 22683591385216 run.py:483] Algo bellman_ford step 7323 current loss 0.023696, current_train_items 234368.
I0302 19:01:05.702734 22683591385216 run.py:483] Algo bellman_ford step 7324 current loss 0.087959, current_train_items 234400.
I0302 19:01:05.721010 22683591385216 run.py:483] Algo bellman_ford step 7325 current loss 0.000859, current_train_items 234432.
I0302 19:01:05.736663 22683591385216 run.py:483] Algo bellman_ford step 7326 current loss 0.035750, current_train_items 234464.
I0302 19:01:05.759444 22683591385216 run.py:483] Algo bellman_ford step 7327 current loss 0.020171, current_train_items 234496.
I0302 19:01:05.788758 22683591385216 run.py:483] Algo bellman_ford step 7328 current loss 0.048216, current_train_items 234528.
I0302 19:01:05.821386 22683591385216 run.py:483] Algo bellman_ford step 7329 current loss 0.034910, current_train_items 234560.
I0302 19:01:05.839223 22683591385216 run.py:483] Algo bellman_ford step 7330 current loss 0.000668, current_train_items 234592.
I0302 19:01:05.854862 22683591385216 run.py:483] Algo bellman_ford step 7331 current loss 0.037972, current_train_items 234624.
I0302 19:01:05.878574 22683591385216 run.py:483] Algo bellman_ford step 7332 current loss 0.056560, current_train_items 234656.
I0302 19:01:05.909086 22683591385216 run.py:483] Algo bellman_ford step 7333 current loss 0.043624, current_train_items 234688.
I0302 19:01:05.939344 22683591385216 run.py:483] Algo bellman_ford step 7334 current loss 0.041346, current_train_items 234720.
I0302 19:01:05.957560 22683591385216 run.py:483] Algo bellman_ford step 7335 current loss 0.001399, current_train_items 234752.
I0302 19:01:05.972574 22683591385216 run.py:483] Algo bellman_ford step 7336 current loss 0.026266, current_train_items 234784.
I0302 19:01:05.994940 22683591385216 run.py:483] Algo bellman_ford step 7337 current loss 0.043219, current_train_items 234816.
I0302 19:01:06.024992 22683591385216 run.py:483] Algo bellman_ford step 7338 current loss 0.044603, current_train_items 234848.
I0302 19:01:06.058395 22683591385216 run.py:483] Algo bellman_ford step 7339 current loss 0.030594, current_train_items 234880.
I0302 19:01:06.076709 22683591385216 run.py:483] Algo bellman_ford step 7340 current loss 0.002639, current_train_items 234912.
I0302 19:01:06.091928 22683591385216 run.py:483] Algo bellman_ford step 7341 current loss 0.015051, current_train_items 234944.
I0302 19:01:06.115384 22683591385216 run.py:483] Algo bellman_ford step 7342 current loss 0.069052, current_train_items 234976.
I0302 19:01:06.145463 22683591385216 run.py:483] Algo bellman_ford step 7343 current loss 0.045362, current_train_items 235008.
I0302 19:01:06.176707 22683591385216 run.py:483] Algo bellman_ford step 7344 current loss 0.066565, current_train_items 235040.
I0302 19:01:06.195223 22683591385216 run.py:483] Algo bellman_ford step 7345 current loss 0.007290, current_train_items 235072.
I0302 19:01:06.210220 22683591385216 run.py:483] Algo bellman_ford step 7346 current loss 0.007016, current_train_items 235104.
I0302 19:01:06.233718 22683591385216 run.py:483] Algo bellman_ford step 7347 current loss 0.063847, current_train_items 235136.
I0302 19:01:06.263760 22683591385216 run.py:483] Algo bellman_ford step 7348 current loss 0.083079, current_train_items 235168.
I0302 19:01:06.292294 22683591385216 run.py:483] Algo bellman_ford step 7349 current loss 0.034894, current_train_items 235200.
I0302 19:01:06.310477 22683591385216 run.py:483] Algo bellman_ford step 7350 current loss 0.001356, current_train_items 235232.
I0302 19:01:06.318342 22683591385216 run.py:503] (val) algo bellman_ford step 7350: {'pi': 0.9833984375, 'score': 0.9833984375, 'examples_seen': 235232, 'step': 7350, 'algorithm': 'bellman_ford'}
I0302 19:01:06.318450 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.983, val scores are: bellman_ford: 0.983
I0302 19:01:06.334804 22683591385216 run.py:483] Algo bellman_ford step 7351 current loss 0.015242, current_train_items 235264.
I0302 19:01:06.358319 22683591385216 run.py:483] Algo bellman_ford step 7352 current loss 0.059324, current_train_items 235296.
I0302 19:01:06.388935 22683591385216 run.py:483] Algo bellman_ford step 7353 current loss 0.051951, current_train_items 235328.
I0302 19:01:06.420213 22683591385216 run.py:483] Algo bellman_ford step 7354 current loss 0.048595, current_train_items 235360.
I0302 19:01:06.438533 22683591385216 run.py:483] Algo bellman_ford step 7355 current loss 0.003081, current_train_items 235392.
I0302 19:01:06.453592 22683591385216 run.py:483] Algo bellman_ford step 7356 current loss 0.007855, current_train_items 235424.
I0302 19:01:06.477065 22683591385216 run.py:483] Algo bellman_ford step 7357 current loss 0.053711, current_train_items 235456.
I0302 19:01:06.506203 22683591385216 run.py:483] Algo bellman_ford step 7358 current loss 0.057560, current_train_items 235488.
I0302 19:01:06.537314 22683591385216 run.py:483] Algo bellman_ford step 7359 current loss 0.104388, current_train_items 235520.
I0302 19:01:06.555794 22683591385216 run.py:483] Algo bellman_ford step 7360 current loss 0.003974, current_train_items 235552.
I0302 19:01:06.571912 22683591385216 run.py:483] Algo bellman_ford step 7361 current loss 0.047329, current_train_items 235584.
I0302 19:01:06.592986 22683591385216 run.py:483] Algo bellman_ford step 7362 current loss 0.017184, current_train_items 235616.
I0302 19:01:06.624490 22683591385216 run.py:483] Algo bellman_ford step 7363 current loss 0.075397, current_train_items 235648.
I0302 19:01:06.653852 22683591385216 run.py:483] Algo bellman_ford step 7364 current loss 0.062005, current_train_items 235680.
I0302 19:01:06.671931 22683591385216 run.py:483] Algo bellman_ford step 7365 current loss 0.014258, current_train_items 235712.
I0302 19:01:06.687220 22683591385216 run.py:483] Algo bellman_ford step 7366 current loss 0.028192, current_train_items 235744.
I0302 19:01:06.710263 22683591385216 run.py:483] Algo bellman_ford step 7367 current loss 0.013815, current_train_items 235776.
I0302 19:01:06.741749 22683591385216 run.py:483] Algo bellman_ford step 7368 current loss 0.031736, current_train_items 235808.
I0302 19:01:06.771774 22683591385216 run.py:483] Algo bellman_ford step 7369 current loss 0.028624, current_train_items 235840.
I0302 19:01:06.790206 22683591385216 run.py:483] Algo bellman_ford step 7370 current loss 0.002252, current_train_items 235872.
I0302 19:01:06.805883 22683591385216 run.py:483] Algo bellman_ford step 7371 current loss 0.009994, current_train_items 235904.
I0302 19:01:06.827062 22683591385216 run.py:483] Algo bellman_ford step 7372 current loss 0.027596, current_train_items 235936.
I0302 19:01:06.856338 22683591385216 run.py:483] Algo bellman_ford step 7373 current loss 0.030090, current_train_items 235968.
I0302 19:01:06.887043 22683591385216 run.py:483] Algo bellman_ford step 7374 current loss 0.059691, current_train_items 236000.
I0302 19:01:06.905606 22683591385216 run.py:483] Algo bellman_ford step 7375 current loss 0.001749, current_train_items 236032.
I0302 19:01:06.921692 22683591385216 run.py:483] Algo bellman_ford step 7376 current loss 0.044060, current_train_items 236064.
I0302 19:01:06.944186 22683591385216 run.py:483] Algo bellman_ford step 7377 current loss 0.024683, current_train_items 236096.
I0302 19:01:06.973593 22683591385216 run.py:483] Algo bellman_ford step 7378 current loss 0.051946, current_train_items 236128.
I0302 19:01:07.003383 22683591385216 run.py:483] Algo bellman_ford step 7379 current loss 0.079331, current_train_items 236160.
I0302 19:01:07.021233 22683591385216 run.py:483] Algo bellman_ford step 7380 current loss 0.002021, current_train_items 236192.
I0302 19:01:07.036188 22683591385216 run.py:483] Algo bellman_ford step 7381 current loss 0.012497, current_train_items 236224.
I0302 19:01:07.059591 22683591385216 run.py:483] Algo bellman_ford step 7382 current loss 0.038784, current_train_items 236256.
I0302 19:01:07.088302 22683591385216 run.py:483] Algo bellman_ford step 7383 current loss 0.022150, current_train_items 236288.
I0302 19:01:07.118443 22683591385216 run.py:483] Algo bellman_ford step 7384 current loss 0.046401, current_train_items 236320.
I0302 19:01:07.136854 22683591385216 run.py:483] Algo bellman_ford step 7385 current loss 0.000803, current_train_items 236352.
I0302 19:01:07.152505 22683591385216 run.py:483] Algo bellman_ford step 7386 current loss 0.010444, current_train_items 236384.
I0302 19:01:07.175120 22683591385216 run.py:483] Algo bellman_ford step 7387 current loss 0.024957, current_train_items 236416.
I0302 19:01:07.205569 22683591385216 run.py:483] Algo bellman_ford step 7388 current loss 0.035758, current_train_items 236448.
I0302 19:01:07.235725 22683591385216 run.py:483] Algo bellman_ford step 7389 current loss 0.037736, current_train_items 236480.
I0302 19:01:07.254286 22683591385216 run.py:483] Algo bellman_ford step 7390 current loss 0.001703, current_train_items 236512.
I0302 19:01:07.270139 22683591385216 run.py:483] Algo bellman_ford step 7391 current loss 0.026192, current_train_items 236544.
I0302 19:01:07.291974 22683591385216 run.py:483] Algo bellman_ford step 7392 current loss 0.024846, current_train_items 236576.
I0302 19:01:07.320486 22683591385216 run.py:483] Algo bellman_ford step 7393 current loss 0.042937, current_train_items 236608.
I0302 19:01:07.351886 22683591385216 run.py:483] Algo bellman_ford step 7394 current loss 0.071641, current_train_items 236640.
I0302 19:01:07.369769 22683591385216 run.py:483] Algo bellman_ford step 7395 current loss 0.001832, current_train_items 236672.
I0302 19:01:07.385393 22683591385216 run.py:483] Algo bellman_ford step 7396 current loss 0.024933, current_train_items 236704.
I0302 19:01:07.408249 22683591385216 run.py:483] Algo bellman_ford step 7397 current loss 0.054133, current_train_items 236736.
I0302 19:01:07.437046 22683591385216 run.py:483] Algo bellman_ford step 7398 current loss 0.114928, current_train_items 236768.
I0302 19:01:07.466749 22683591385216 run.py:483] Algo bellman_ford step 7399 current loss 0.056177, current_train_items 236800.
I0302 19:01:07.485082 22683591385216 run.py:483] Algo bellman_ford step 7400 current loss 0.001397, current_train_items 236832.
I0302 19:01:07.492958 22683591385216 run.py:503] (val) algo bellman_ford step 7400: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 236832, 'step': 7400, 'algorithm': 'bellman_ford'}
I0302 19:01:07.493063 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:07.509104 22683591385216 run.py:483] Algo bellman_ford step 7401 current loss 0.044268, current_train_items 236864.
I0302 19:01:07.532685 22683591385216 run.py:483] Algo bellman_ford step 7402 current loss 0.069991, current_train_items 236896.
I0302 19:01:07.563254 22683591385216 run.py:483] Algo bellman_ford step 7403 current loss 0.040053, current_train_items 236928.
I0302 19:01:07.595929 22683591385216 run.py:483] Algo bellman_ford step 7404 current loss 0.090154, current_train_items 236960.
I0302 19:01:07.614730 22683591385216 run.py:483] Algo bellman_ford step 7405 current loss 0.001541, current_train_items 236992.
I0302 19:01:07.629699 22683591385216 run.py:483] Algo bellman_ford step 7406 current loss 0.003673, current_train_items 237024.
I0302 19:01:07.652698 22683591385216 run.py:483] Algo bellman_ford step 7407 current loss 0.024072, current_train_items 237056.
I0302 19:01:07.683156 22683591385216 run.py:483] Algo bellman_ford step 7408 current loss 0.045322, current_train_items 237088.
I0302 19:01:07.712796 22683591385216 run.py:483] Algo bellman_ford step 7409 current loss 0.072692, current_train_items 237120.
I0302 19:01:07.731143 22683591385216 run.py:483] Algo bellman_ford step 7410 current loss 0.001194, current_train_items 237152.
I0302 19:01:07.747092 22683591385216 run.py:483] Algo bellman_ford step 7411 current loss 0.004440, current_train_items 237184.
I0302 19:01:07.770985 22683591385216 run.py:483] Algo bellman_ford step 7412 current loss 0.040673, current_train_items 237216.
I0302 19:01:07.801601 22683591385216 run.py:483] Algo bellman_ford step 7413 current loss 0.042125, current_train_items 237248.
I0302 19:01:07.831706 22683591385216 run.py:483] Algo bellman_ford step 7414 current loss 0.045893, current_train_items 237280.
I0302 19:01:07.850091 22683591385216 run.py:483] Algo bellman_ford step 7415 current loss 0.001952, current_train_items 237312.
I0302 19:01:07.865321 22683591385216 run.py:483] Algo bellman_ford step 7416 current loss 0.004973, current_train_items 237344.
I0302 19:01:07.887940 22683591385216 run.py:483] Algo bellman_ford step 7417 current loss 0.039524, current_train_items 237376.
I0302 19:01:07.917496 22683591385216 run.py:483] Algo bellman_ford step 7418 current loss 0.043294, current_train_items 237408.
I0302 19:01:07.947963 22683591385216 run.py:483] Algo bellman_ford step 7419 current loss 0.051365, current_train_items 237440.
I0302 19:01:07.966356 22683591385216 run.py:483] Algo bellman_ford step 7420 current loss 0.003965, current_train_items 237472.
I0302 19:01:07.981546 22683591385216 run.py:483] Algo bellman_ford step 7421 current loss 0.022129, current_train_items 237504.
I0302 19:01:08.004475 22683591385216 run.py:483] Algo bellman_ford step 7422 current loss 0.116085, current_train_items 237536.
I0302 19:01:08.034202 22683591385216 run.py:483] Algo bellman_ford step 7423 current loss 0.087808, current_train_items 237568.
I0302 19:01:08.065364 22683591385216 run.py:483] Algo bellman_ford step 7424 current loss 0.047944, current_train_items 237600.
I0302 19:01:08.083791 22683591385216 run.py:483] Algo bellman_ford step 7425 current loss 0.001444, current_train_items 237632.
I0302 19:01:08.099055 22683591385216 run.py:483] Algo bellman_ford step 7426 current loss 0.007802, current_train_items 237664.
I0302 19:01:08.122609 22683591385216 run.py:483] Algo bellman_ford step 7427 current loss 0.046673, current_train_items 237696.
I0302 19:01:08.151842 22683591385216 run.py:483] Algo bellman_ford step 7428 current loss 0.036361, current_train_items 237728.
I0302 19:01:08.183716 22683591385216 run.py:483] Algo bellman_ford step 7429 current loss 0.060748, current_train_items 237760.
I0302 19:01:08.202158 22683591385216 run.py:483] Algo bellman_ford step 7430 current loss 0.001827, current_train_items 237792.
I0302 19:01:08.217250 22683591385216 run.py:483] Algo bellman_ford step 7431 current loss 0.006560, current_train_items 237824.
I0302 19:01:08.240666 22683591385216 run.py:483] Algo bellman_ford step 7432 current loss 0.041518, current_train_items 237856.
I0302 19:01:08.270883 22683591385216 run.py:483] Algo bellman_ford step 7433 current loss 0.056061, current_train_items 237888.
I0302 19:01:08.300012 22683591385216 run.py:483] Algo bellman_ford step 7434 current loss 0.041912, current_train_items 237920.
I0302 19:01:08.318398 22683591385216 run.py:483] Algo bellman_ford step 7435 current loss 0.001901, current_train_items 237952.
I0302 19:01:08.333838 22683591385216 run.py:483] Algo bellman_ford step 7436 current loss 0.008465, current_train_items 237984.
I0302 19:01:08.357379 22683591385216 run.py:483] Algo bellman_ford step 7437 current loss 0.068437, current_train_items 238016.
I0302 19:01:08.386638 22683591385216 run.py:483] Algo bellman_ford step 7438 current loss 0.084291, current_train_items 238048.
I0302 19:01:08.419276 22683591385216 run.py:483] Algo bellman_ford step 7439 current loss 0.091582, current_train_items 238080.
I0302 19:01:08.437666 22683591385216 run.py:483] Algo bellman_ford step 7440 current loss 0.005219, current_train_items 238112.
I0302 19:01:08.453423 22683591385216 run.py:483] Algo bellman_ford step 7441 current loss 0.013899, current_train_items 238144.
I0302 19:01:08.475215 22683591385216 run.py:483] Algo bellman_ford step 7442 current loss 0.014691, current_train_items 238176.
I0302 19:01:08.505603 22683591385216 run.py:483] Algo bellman_ford step 7443 current loss 0.048991, current_train_items 238208.
I0302 19:01:08.537769 22683591385216 run.py:483] Algo bellman_ford step 7444 current loss 0.055017, current_train_items 238240.
I0302 19:01:08.556301 22683591385216 run.py:483] Algo bellman_ford step 7445 current loss 0.003472, current_train_items 238272.
I0302 19:01:08.571644 22683591385216 run.py:483] Algo bellman_ford step 7446 current loss 0.013064, current_train_items 238304.
I0302 19:01:08.593907 22683591385216 run.py:483] Algo bellman_ford step 7447 current loss 0.015507, current_train_items 238336.
I0302 19:01:08.623340 22683591385216 run.py:483] Algo bellman_ford step 7448 current loss 0.031498, current_train_items 238368.
I0302 19:01:08.656289 22683591385216 run.py:483] Algo bellman_ford step 7449 current loss 0.057896, current_train_items 238400.
I0302 19:01:08.674793 22683591385216 run.py:483] Algo bellman_ford step 7450 current loss 0.003699, current_train_items 238432.
I0302 19:01:08.682771 22683591385216 run.py:503] (val) algo bellman_ford step 7450: {'pi': 0.9951171875, 'score': 0.9951171875, 'examples_seen': 238432, 'step': 7450, 'algorithm': 'bellman_ford'}
I0302 19:01:08.682878 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.995, val scores are: bellman_ford: 0.995
I0302 19:01:08.698973 22683591385216 run.py:483] Algo bellman_ford step 7451 current loss 0.009064, current_train_items 238464.
I0302 19:01:08.723111 22683591385216 run.py:483] Algo bellman_ford step 7452 current loss 0.025351, current_train_items 238496.
I0302 19:01:08.752568 22683591385216 run.py:483] Algo bellman_ford step 7453 current loss 0.030131, current_train_items 238528.
I0302 19:01:08.783237 22683591385216 run.py:483] Algo bellman_ford step 7454 current loss 0.050654, current_train_items 238560.
I0302 19:01:08.801903 22683591385216 run.py:483] Algo bellman_ford step 7455 current loss 0.036136, current_train_items 238592.
I0302 19:01:08.817734 22683591385216 run.py:483] Algo bellman_ford step 7456 current loss 0.028815, current_train_items 238624.
I0302 19:01:08.840239 22683591385216 run.py:483] Algo bellman_ford step 7457 current loss 0.023089, current_train_items 238656.
I0302 19:01:08.870785 22683591385216 run.py:483] Algo bellman_ford step 7458 current loss 0.046900, current_train_items 238688.
I0302 19:01:08.901584 22683591385216 run.py:483] Algo bellman_ford step 7459 current loss 0.070915, current_train_items 238720.
I0302 19:01:08.920479 22683591385216 run.py:483] Algo bellman_ford step 7460 current loss 0.003188, current_train_items 238752.
I0302 19:01:08.936721 22683591385216 run.py:483] Algo bellman_ford step 7461 current loss 0.013964, current_train_items 238784.
I0302 19:01:08.958566 22683591385216 run.py:483] Algo bellman_ford step 7462 current loss 0.049185, current_train_items 238816.
I0302 19:01:08.987807 22683591385216 run.py:483] Algo bellman_ford step 7463 current loss 0.014252, current_train_items 238848.
I0302 19:01:09.019332 22683591385216 run.py:483] Algo bellman_ford step 7464 current loss 0.070380, current_train_items 238880.
I0302 19:01:09.037710 22683591385216 run.py:483] Algo bellman_ford step 7465 current loss 0.001777, current_train_items 238912.
I0302 19:01:09.052927 22683591385216 run.py:483] Algo bellman_ford step 7466 current loss 0.006502, current_train_items 238944.
I0302 19:01:09.075241 22683591385216 run.py:483] Algo bellman_ford step 7467 current loss 0.045236, current_train_items 238976.
I0302 19:01:09.105790 22683591385216 run.py:483] Algo bellman_ford step 7468 current loss 0.079055, current_train_items 239008.
I0302 19:01:09.135386 22683591385216 run.py:483] Algo bellman_ford step 7469 current loss 0.043051, current_train_items 239040.
I0302 19:01:09.154255 22683591385216 run.py:483] Algo bellman_ford step 7470 current loss 0.011180, current_train_items 239072.
I0302 19:01:09.170003 22683591385216 run.py:483] Algo bellman_ford step 7471 current loss 0.009993, current_train_items 239104.
I0302 19:01:09.191669 22683591385216 run.py:483] Algo bellman_ford step 7472 current loss 0.049560, current_train_items 239136.
I0302 19:01:09.221938 22683591385216 run.py:483] Algo bellman_ford step 7473 current loss 0.079223, current_train_items 239168.
I0302 19:01:09.254764 22683591385216 run.py:483] Algo bellman_ford step 7474 current loss 0.041275, current_train_items 239200.
I0302 19:01:09.273599 22683591385216 run.py:483] Algo bellman_ford step 7475 current loss 0.001812, current_train_items 239232.
I0302 19:01:09.289349 22683591385216 run.py:483] Algo bellman_ford step 7476 current loss 0.004460, current_train_items 239264.
I0302 19:01:09.311111 22683591385216 run.py:483] Algo bellman_ford step 7477 current loss 0.020717, current_train_items 239296.
I0302 19:01:09.341303 22683591385216 run.py:483] Algo bellman_ford step 7478 current loss 0.026640, current_train_items 239328.
I0302 19:01:09.375602 22683591385216 run.py:483] Algo bellman_ford step 7479 current loss 0.109976, current_train_items 239360.
I0302 19:01:09.393956 22683591385216 run.py:483] Algo bellman_ford step 7480 current loss 0.002523, current_train_items 239392.
I0302 19:01:09.409519 22683591385216 run.py:483] Algo bellman_ford step 7481 current loss 0.007830, current_train_items 239424.
I0302 19:01:09.433196 22683591385216 run.py:483] Algo bellman_ford step 7482 current loss 0.041508, current_train_items 239456.
I0302 19:01:09.464312 22683591385216 run.py:483] Algo bellman_ford step 7483 current loss 0.037937, current_train_items 239488.
I0302 19:01:09.496402 22683591385216 run.py:483] Algo bellman_ford step 7484 current loss 0.055564, current_train_items 239520.
I0302 19:01:09.514859 22683591385216 run.py:483] Algo bellman_ford step 7485 current loss 0.001952, current_train_items 239552.
I0302 19:01:09.530496 22683591385216 run.py:483] Algo bellman_ford step 7486 current loss 0.006545, current_train_items 239584.
I0302 19:01:09.551953 22683591385216 run.py:483] Algo bellman_ford step 7487 current loss 0.043651, current_train_items 239616.
I0302 19:01:09.581973 22683591385216 run.py:483] Algo bellman_ford step 7488 current loss 0.069106, current_train_items 239648.
I0302 19:01:09.615233 22683591385216 run.py:483] Algo bellman_ford step 7489 current loss 0.049441, current_train_items 239680.
I0302 19:01:09.633807 22683591385216 run.py:483] Algo bellman_ford step 7490 current loss 0.001182, current_train_items 239712.
I0302 19:01:09.649117 22683591385216 run.py:483] Algo bellman_ford step 7491 current loss 0.020508, current_train_items 239744.
I0302 19:01:09.670804 22683591385216 run.py:483] Algo bellman_ford step 7492 current loss 0.010033, current_train_items 239776.
I0302 19:01:09.700351 22683591385216 run.py:483] Algo bellman_ford step 7493 current loss 0.019275, current_train_items 239808.
I0302 19:01:09.731121 22683591385216 run.py:483] Algo bellman_ford step 7494 current loss 0.059255, current_train_items 239840.
I0302 19:01:09.749574 22683591385216 run.py:483] Algo bellman_ford step 7495 current loss 0.001062, current_train_items 239872.
I0302 19:01:09.765048 22683591385216 run.py:483] Algo bellman_ford step 7496 current loss 0.012119, current_train_items 239904.
I0302 19:01:09.787785 22683591385216 run.py:483] Algo bellman_ford step 7497 current loss 0.040934, current_train_items 239936.
I0302 19:01:09.816676 22683591385216 run.py:483] Algo bellman_ford step 7498 current loss 0.043774, current_train_items 239968.
I0302 19:01:09.847197 22683591385216 run.py:483] Algo bellman_ford step 7499 current loss 0.051768, current_train_items 240000.
I0302 19:01:09.866111 22683591385216 run.py:483] Algo bellman_ford step 7500 current loss 0.005030, current_train_items 240032.
I0302 19:01:09.873953 22683591385216 run.py:503] (val) algo bellman_ford step 7500: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 240032, 'step': 7500, 'algorithm': 'bellman_ford'}
I0302 19:01:09.874061 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:09.889792 22683591385216 run.py:483] Algo bellman_ford step 7501 current loss 0.015884, current_train_items 240064.
I0302 19:01:09.912376 22683591385216 run.py:483] Algo bellman_ford step 7502 current loss 0.050029, current_train_items 240096.
I0302 19:01:09.942118 22683591385216 run.py:483] Algo bellman_ford step 7503 current loss 0.039025, current_train_items 240128.
I0302 19:01:09.973530 22683591385216 run.py:483] Algo bellman_ford step 7504 current loss 0.104993, current_train_items 240160.
I0302 19:01:09.992046 22683591385216 run.py:483] Algo bellman_ford step 7505 current loss 0.030337, current_train_items 240192.
I0302 19:01:10.007324 22683591385216 run.py:483] Algo bellman_ford step 7506 current loss 0.009074, current_train_items 240224.
I0302 19:01:10.029191 22683591385216 run.py:483] Algo bellman_ford step 7507 current loss 0.030619, current_train_items 240256.
I0302 19:01:10.058352 22683591385216 run.py:483] Algo bellman_ford step 7508 current loss 0.015710, current_train_items 240288.
I0302 19:01:10.088802 22683591385216 run.py:483] Algo bellman_ford step 7509 current loss 0.042319, current_train_items 240320.
I0302 19:01:10.107063 22683591385216 run.py:483] Algo bellman_ford step 7510 current loss 0.011204, current_train_items 240352.
I0302 19:01:10.122699 22683591385216 run.py:483] Algo bellman_ford step 7511 current loss 0.027617, current_train_items 240384.
I0302 19:01:10.144143 22683591385216 run.py:483] Algo bellman_ford step 7512 current loss 0.018661, current_train_items 240416.
I0302 19:01:10.173399 22683591385216 run.py:483] Algo bellman_ford step 7513 current loss 0.032260, current_train_items 240448.
I0302 19:01:10.203586 22683591385216 run.py:483] Algo bellman_ford step 7514 current loss 0.046395, current_train_items 240480.
I0302 19:01:10.221719 22683591385216 run.py:483] Algo bellman_ford step 7515 current loss 0.003484, current_train_items 240512.
I0302 19:01:10.236347 22683591385216 run.py:483] Algo bellman_ford step 7516 current loss 0.026220, current_train_items 240544.
I0302 19:01:10.259094 22683591385216 run.py:483] Algo bellman_ford step 7517 current loss 0.045343, current_train_items 240576.
I0302 19:01:10.288906 22683591385216 run.py:483] Algo bellman_ford step 7518 current loss 0.052283, current_train_items 240608.
I0302 19:01:10.320295 22683591385216 run.py:483] Algo bellman_ford step 7519 current loss 0.059484, current_train_items 240640.
I0302 19:01:10.338412 22683591385216 run.py:483] Algo bellman_ford step 7520 current loss 0.003772, current_train_items 240672.
I0302 19:01:10.353778 22683591385216 run.py:483] Algo bellman_ford step 7521 current loss 0.011643, current_train_items 240704.
I0302 19:01:10.376012 22683591385216 run.py:483] Algo bellman_ford step 7522 current loss 0.027925, current_train_items 240736.
I0302 19:01:10.405689 22683591385216 run.py:483] Algo bellman_ford step 7523 current loss 0.053648, current_train_items 240768.
I0302 19:01:10.434373 22683591385216 run.py:483] Algo bellman_ford step 7524 current loss 0.030411, current_train_items 240800.
I0302 19:01:10.452405 22683591385216 run.py:483] Algo bellman_ford step 7525 current loss 0.017258, current_train_items 240832.
I0302 19:01:10.467580 22683591385216 run.py:483] Algo bellman_ford step 7526 current loss 0.026492, current_train_items 240864.
I0302 19:01:10.490791 22683591385216 run.py:483] Algo bellman_ford step 7527 current loss 0.043836, current_train_items 240896.
I0302 19:01:10.519420 22683591385216 run.py:483] Algo bellman_ford step 7528 current loss 0.045504, current_train_items 240928.
I0302 19:01:10.550424 22683591385216 run.py:483] Algo bellman_ford step 7529 current loss 0.071143, current_train_items 240960.
I0302 19:01:10.568456 22683591385216 run.py:483] Algo bellman_ford step 7530 current loss 0.005246, current_train_items 240992.
I0302 19:01:10.583639 22683591385216 run.py:483] Algo bellman_ford step 7531 current loss 0.014512, current_train_items 241024.
I0302 19:01:10.605321 22683591385216 run.py:483] Algo bellman_ford step 7532 current loss 0.026646, current_train_items 241056.
I0302 19:01:10.636056 22683591385216 run.py:483] Algo bellman_ford step 7533 current loss 0.061031, current_train_items 241088.
I0302 19:01:10.668184 22683591385216 run.py:483] Algo bellman_ford step 7534 current loss 0.063262, current_train_items 241120.
I0302 19:01:10.686339 22683591385216 run.py:483] Algo bellman_ford step 7535 current loss 0.001176, current_train_items 241152.
I0302 19:01:10.701692 22683591385216 run.py:483] Algo bellman_ford step 7536 current loss 0.006385, current_train_items 241184.
I0302 19:01:10.723994 22683591385216 run.py:483] Algo bellman_ford step 7537 current loss 0.037802, current_train_items 241216.
I0302 19:01:10.754020 22683591385216 run.py:483] Algo bellman_ford step 7538 current loss 0.049903, current_train_items 241248.
I0302 19:01:10.784804 22683591385216 run.py:483] Algo bellman_ford step 7539 current loss 0.036775, current_train_items 241280.
I0302 19:01:10.803294 22683591385216 run.py:483] Algo bellman_ford step 7540 current loss 0.001812, current_train_items 241312.
I0302 19:01:10.819082 22683591385216 run.py:483] Algo bellman_ford step 7541 current loss 0.012392, current_train_items 241344.
I0302 19:01:10.842582 22683591385216 run.py:483] Algo bellman_ford step 7542 current loss 0.060734, current_train_items 241376.
I0302 19:01:10.873167 22683591385216 run.py:483] Algo bellman_ford step 7543 current loss 0.034955, current_train_items 241408.
I0302 19:01:10.904169 22683591385216 run.py:483] Algo bellman_ford step 7544 current loss 0.049803, current_train_items 241440.
I0302 19:01:10.922415 22683591385216 run.py:483] Algo bellman_ford step 7545 current loss 0.002882, current_train_items 241472.
I0302 19:01:10.937696 22683591385216 run.py:483] Algo bellman_ford step 7546 current loss 0.008396, current_train_items 241504.
I0302 19:01:10.960857 22683591385216 run.py:483] Algo bellman_ford step 7547 current loss 0.047802, current_train_items 241536.
I0302 19:01:10.991153 22683591385216 run.py:483] Algo bellman_ford step 7548 current loss 0.057696, current_train_items 241568.
I0302 19:01:11.022407 22683591385216 run.py:483] Algo bellman_ford step 7549 current loss 0.034121, current_train_items 241600.
I0302 19:01:11.040706 22683591385216 run.py:483] Algo bellman_ford step 7550 current loss 0.007358, current_train_items 241632.
I0302 19:01:11.048678 22683591385216 run.py:503] (val) algo bellman_ford step 7550: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 241632, 'step': 7550, 'algorithm': 'bellman_ford'}
I0302 19:01:11.048786 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:11.065168 22683591385216 run.py:483] Algo bellman_ford step 7551 current loss 0.018961, current_train_items 241664.
I0302 19:01:11.088345 22683591385216 run.py:483] Algo bellman_ford step 7552 current loss 0.012367, current_train_items 241696.
I0302 19:01:11.118723 22683591385216 run.py:483] Algo bellman_ford step 7553 current loss 0.074831, current_train_items 241728.
I0302 19:01:11.150408 22683591385216 run.py:483] Algo bellman_ford step 7554 current loss 0.118686, current_train_items 241760.
I0302 19:01:11.169212 22683591385216 run.py:483] Algo bellman_ford step 7555 current loss 0.004315, current_train_items 241792.
I0302 19:01:11.184649 22683591385216 run.py:483] Algo bellman_ford step 7556 current loss 0.071314, current_train_items 241824.
I0302 19:01:11.207683 22683591385216 run.py:483] Algo bellman_ford step 7557 current loss 0.023728, current_train_items 241856.
I0302 19:01:11.238849 22683591385216 run.py:483] Algo bellman_ford step 7558 current loss 0.041936, current_train_items 241888.
I0302 19:01:11.268691 22683591385216 run.py:483] Algo bellman_ford step 7559 current loss 0.042622, current_train_items 241920.
I0302 19:01:11.287241 22683591385216 run.py:483] Algo bellman_ford step 7560 current loss 0.001370, current_train_items 241952.
I0302 19:01:11.303666 22683591385216 run.py:483] Algo bellman_ford step 7561 current loss 0.051761, current_train_items 241984.
I0302 19:01:11.325538 22683591385216 run.py:483] Algo bellman_ford step 7562 current loss 0.032720, current_train_items 242016.
I0302 19:01:11.355712 22683591385216 run.py:483] Algo bellman_ford step 7563 current loss 0.054794, current_train_items 242048.
I0302 19:01:11.387337 22683591385216 run.py:483] Algo bellman_ford step 7564 current loss 0.051446, current_train_items 242080.
I0302 19:01:11.405713 22683591385216 run.py:483] Algo bellman_ford step 7565 current loss 0.001929, current_train_items 242112.
I0302 19:01:11.420832 22683591385216 run.py:483] Algo bellman_ford step 7566 current loss 0.035414, current_train_items 242144.
I0302 19:01:11.443501 22683591385216 run.py:483] Algo bellman_ford step 7567 current loss 0.035219, current_train_items 242176.
I0302 19:01:11.473080 22683591385216 run.py:483] Algo bellman_ford step 7568 current loss 0.084069, current_train_items 242208.
I0302 19:01:11.503552 22683591385216 run.py:483] Algo bellman_ford step 7569 current loss 0.100935, current_train_items 242240.
I0302 19:01:11.522586 22683591385216 run.py:483] Algo bellman_ford step 7570 current loss 0.003558, current_train_items 242272.
I0302 19:01:11.538467 22683591385216 run.py:483] Algo bellman_ford step 7571 current loss 0.009384, current_train_items 242304.
I0302 19:01:11.560330 22683591385216 run.py:483] Algo bellman_ford step 7572 current loss 0.019036, current_train_items 242336.
I0302 19:01:11.589132 22683591385216 run.py:483] Algo bellman_ford step 7573 current loss 0.039266, current_train_items 242368.
I0302 19:01:11.618073 22683591385216 run.py:483] Algo bellman_ford step 7574 current loss 0.033422, current_train_items 242400.
I0302 19:01:11.636766 22683591385216 run.py:483] Algo bellman_ford step 7575 current loss 0.000730, current_train_items 242432.
I0302 19:01:11.653073 22683591385216 run.py:483] Algo bellman_ford step 7576 current loss 0.028204, current_train_items 242464.
I0302 19:01:11.674721 22683591385216 run.py:483] Algo bellman_ford step 7577 current loss 0.071981, current_train_items 242496.
I0302 19:01:11.704781 22683591385216 run.py:483] Algo bellman_ford step 7578 current loss 0.047007, current_train_items 242528.
I0302 19:01:11.737623 22683591385216 run.py:483] Algo bellman_ford step 7579 current loss 0.101499, current_train_items 242560.
I0302 19:01:11.755791 22683591385216 run.py:483] Algo bellman_ford step 7580 current loss 0.001789, current_train_items 242592.
I0302 19:01:11.771413 22683591385216 run.py:483] Algo bellman_ford step 7581 current loss 0.035282, current_train_items 242624.
I0302 19:01:11.794254 22683591385216 run.py:483] Algo bellman_ford step 7582 current loss 0.066041, current_train_items 242656.
I0302 19:01:11.823682 22683591385216 run.py:483] Algo bellman_ford step 7583 current loss 0.072914, current_train_items 242688.
I0302 19:01:11.854125 22683591385216 run.py:483] Algo bellman_ford step 7584 current loss 0.028792, current_train_items 242720.
I0302 19:01:11.872999 22683591385216 run.py:483] Algo bellman_ford step 7585 current loss 0.002346, current_train_items 242752.
I0302 19:01:11.888299 22683591385216 run.py:483] Algo bellman_ford step 7586 current loss 0.029782, current_train_items 242784.
I0302 19:01:11.911111 22683591385216 run.py:483] Algo bellman_ford step 7587 current loss 0.039656, current_train_items 242816.
I0302 19:01:11.938800 22683591385216 run.py:483] Algo bellman_ford step 7588 current loss 0.017640, current_train_items 242848.
I0302 19:01:11.970527 22683591385216 run.py:483] Algo bellman_ford step 7589 current loss 0.047227, current_train_items 242880.
I0302 19:01:11.989176 22683591385216 run.py:483] Algo bellman_ford step 7590 current loss 0.017172, current_train_items 242912.
I0302 19:01:12.004665 22683591385216 run.py:483] Algo bellman_ford step 7591 current loss 0.016434, current_train_items 242944.
I0302 19:01:12.028112 22683591385216 run.py:483] Algo bellman_ford step 7592 current loss 0.059986, current_train_items 242976.
I0302 19:01:12.056628 22683591385216 run.py:483] Algo bellman_ford step 7593 current loss 0.024259, current_train_items 243008.
I0302 19:01:12.088291 22683591385216 run.py:483] Algo bellman_ford step 7594 current loss 0.090955, current_train_items 243040.
I0302 19:01:12.106408 22683591385216 run.py:483] Algo bellman_ford step 7595 current loss 0.003615, current_train_items 243072.
I0302 19:01:12.121637 22683591385216 run.py:483] Algo bellman_ford step 7596 current loss 0.012656, current_train_items 243104.
I0302 19:01:12.144592 22683591385216 run.py:483] Algo bellman_ford step 7597 current loss 0.018480, current_train_items 243136.
I0302 19:01:12.175092 22683591385216 run.py:483] Algo bellman_ford step 7598 current loss 0.047717, current_train_items 243168.
I0302 19:01:12.203552 22683591385216 run.py:483] Algo bellman_ford step 7599 current loss 0.046211, current_train_items 243200.
I0302 19:01:12.222108 22683591385216 run.py:483] Algo bellman_ford step 7600 current loss 0.003999, current_train_items 243232.
I0302 19:01:12.229826 22683591385216 run.py:503] (val) algo bellman_ford step 7600: {'pi': 0.9775390625, 'score': 0.9775390625, 'examples_seen': 243232, 'step': 7600, 'algorithm': 'bellman_ford'}
I0302 19:01:12.229946 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.978, val scores are: bellman_ford: 0.978
I0302 19:01:12.245969 22683591385216 run.py:483] Algo bellman_ford step 7601 current loss 0.007476, current_train_items 243264.
I0302 19:01:12.270384 22683591385216 run.py:483] Algo bellman_ford step 7602 current loss 0.084057, current_train_items 243296.
I0302 19:01:12.299963 22683591385216 run.py:483] Algo bellman_ford step 7603 current loss 0.024836, current_train_items 243328.
I0302 19:01:12.332496 22683591385216 run.py:483] Algo bellman_ford step 7604 current loss 0.054655, current_train_items 243360.
I0302 19:01:12.351325 22683591385216 run.py:483] Algo bellman_ford step 7605 current loss 0.029215, current_train_items 243392.
I0302 19:01:12.366027 22683591385216 run.py:483] Algo bellman_ford step 7606 current loss 0.008270, current_train_items 243424.
I0302 19:01:12.388537 22683591385216 run.py:483] Algo bellman_ford step 7607 current loss 0.018423, current_train_items 243456.
I0302 19:01:12.418779 22683591385216 run.py:483] Algo bellman_ford step 7608 current loss 0.055430, current_train_items 243488.
I0302 19:01:12.451036 22683591385216 run.py:483] Algo bellman_ford step 7609 current loss 0.046088, current_train_items 243520.
I0302 19:01:12.469074 22683591385216 run.py:483] Algo bellman_ford step 7610 current loss 0.000937, current_train_items 243552.
I0302 19:01:12.484807 22683591385216 run.py:483] Algo bellman_ford step 7611 current loss 0.009648, current_train_items 243584.
I0302 19:01:12.507327 22683591385216 run.py:483] Algo bellman_ford step 7612 current loss 0.026246, current_train_items 243616.
I0302 19:01:12.535273 22683591385216 run.py:483] Algo bellman_ford step 7613 current loss 0.047564, current_train_items 243648.
I0302 19:01:12.565678 22683591385216 run.py:483] Algo bellman_ford step 7614 current loss 0.057307, current_train_items 243680.
I0302 19:01:12.584069 22683591385216 run.py:483] Algo bellman_ford step 7615 current loss 0.000925, current_train_items 243712.
I0302 19:01:12.599577 22683591385216 run.py:483] Algo bellman_ford step 7616 current loss 0.002656, current_train_items 243744.
I0302 19:01:12.622182 22683591385216 run.py:483] Algo bellman_ford step 7617 current loss 0.028696, current_train_items 243776.
I0302 19:01:12.651521 22683591385216 run.py:483] Algo bellman_ford step 7618 current loss 0.064771, current_train_items 243808.
I0302 19:01:12.683052 22683591385216 run.py:483] Algo bellman_ford step 7619 current loss 0.115187, current_train_items 243840.
I0302 19:01:12.701360 22683591385216 run.py:483] Algo bellman_ford step 7620 current loss 0.004852, current_train_items 243872.
I0302 19:01:12.716763 22683591385216 run.py:483] Algo bellman_ford step 7621 current loss 0.007022, current_train_items 243904.
I0302 19:01:12.739534 22683591385216 run.py:483] Algo bellman_ford step 7622 current loss 0.044469, current_train_items 243936.
I0302 19:01:12.770746 22683591385216 run.py:483] Algo bellman_ford step 7623 current loss 0.045443, current_train_items 243968.
I0302 19:01:12.801110 22683591385216 run.py:483] Algo bellman_ford step 7624 current loss 0.066754, current_train_items 244000.
I0302 19:01:12.819601 22683591385216 run.py:483] Algo bellman_ford step 7625 current loss 0.005997, current_train_items 244032.
I0302 19:01:12.834244 22683591385216 run.py:483] Algo bellman_ford step 7626 current loss 0.014519, current_train_items 244064.
I0302 19:01:12.857558 22683591385216 run.py:483] Algo bellman_ford step 7627 current loss 0.048912, current_train_items 244096.
I0302 19:01:12.887948 22683591385216 run.py:483] Algo bellman_ford step 7628 current loss 0.031812, current_train_items 244128.
I0302 19:01:12.918449 22683591385216 run.py:483] Algo bellman_ford step 7629 current loss 0.038254, current_train_items 244160.
I0302 19:01:12.936758 22683591385216 run.py:483] Algo bellman_ford step 7630 current loss 0.017795, current_train_items 244192.
I0302 19:01:12.952465 22683591385216 run.py:483] Algo bellman_ford step 7631 current loss 0.011021, current_train_items 244224.
I0302 19:01:12.974818 22683591385216 run.py:483] Algo bellman_ford step 7632 current loss 0.042806, current_train_items 244256.
I0302 19:01:13.005531 22683591385216 run.py:483] Algo bellman_ford step 7633 current loss 0.023704, current_train_items 244288.
I0302 19:01:13.037364 22683591385216 run.py:483] Algo bellman_ford step 7634 current loss 0.049924, current_train_items 244320.
I0302 19:01:13.055929 22683591385216 run.py:483] Algo bellman_ford step 7635 current loss 0.006187, current_train_items 244352.
I0302 19:01:13.071116 22683591385216 run.py:483] Algo bellman_ford step 7636 current loss 0.010956, current_train_items 244384.
I0302 19:01:13.093522 22683591385216 run.py:483] Algo bellman_ford step 7637 current loss 0.014715, current_train_items 244416.
I0302 19:01:13.122232 22683591385216 run.py:483] Algo bellman_ford step 7638 current loss 0.044232, current_train_items 244448.
I0302 19:01:13.156078 22683591385216 run.py:483] Algo bellman_ford step 7639 current loss 0.062080, current_train_items 244480.
I0302 19:01:13.174245 22683591385216 run.py:483] Algo bellman_ford step 7640 current loss 0.000920, current_train_items 244512.
I0302 19:01:13.189376 22683591385216 run.py:483] Algo bellman_ford step 7641 current loss 0.025998, current_train_items 244544.
I0302 19:01:13.210767 22683591385216 run.py:483] Algo bellman_ford step 7642 current loss 0.027110, current_train_items 244576.
I0302 19:01:13.242941 22683591385216 run.py:483] Algo bellman_ford step 7643 current loss 0.060256, current_train_items 244608.
I0302 19:01:13.276105 22683591385216 run.py:483] Algo bellman_ford step 7644 current loss 0.043973, current_train_items 244640.
I0302 19:01:13.294410 22683591385216 run.py:483] Algo bellman_ford step 7645 current loss 0.005649, current_train_items 244672.
I0302 19:01:13.309827 22683591385216 run.py:483] Algo bellman_ford step 7646 current loss 0.029996, current_train_items 244704.
I0302 19:01:13.333423 22683591385216 run.py:483] Algo bellman_ford step 7647 current loss 0.033004, current_train_items 244736.
I0302 19:01:13.364420 22683591385216 run.py:483] Algo bellman_ford step 7648 current loss 0.057545, current_train_items 244768.
I0302 19:01:13.397732 22683591385216 run.py:483] Algo bellman_ford step 7649 current loss 0.073281, current_train_items 244800.
I0302 19:01:13.416045 22683591385216 run.py:483] Algo bellman_ford step 7650 current loss 0.001331, current_train_items 244832.
I0302 19:01:13.424089 22683591385216 run.py:503] (val) algo bellman_ford step 7650: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 244832, 'step': 7650, 'algorithm': 'bellman_ford'}
I0302 19:01:13.424196 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:13.439696 22683591385216 run.py:483] Algo bellman_ford step 7651 current loss 0.004710, current_train_items 244864.
I0302 19:01:13.463155 22683591385216 run.py:483] Algo bellman_ford step 7652 current loss 0.078321, current_train_items 244896.
I0302 19:01:13.492772 22683591385216 run.py:483] Algo bellman_ford step 7653 current loss 0.056016, current_train_items 244928.
I0302 19:01:13.524400 22683591385216 run.py:483] Algo bellman_ford step 7654 current loss 0.100439, current_train_items 244960.
I0302 19:01:13.542821 22683591385216 run.py:483] Algo bellman_ford step 7655 current loss 0.005630, current_train_items 244992.
I0302 19:01:13.558073 22683591385216 run.py:483] Algo bellman_ford step 7656 current loss 0.011145, current_train_items 245024.
I0302 19:01:13.580647 22683591385216 run.py:483] Algo bellman_ford step 7657 current loss 0.027590, current_train_items 245056.
I0302 19:01:13.610845 22683591385216 run.py:483] Algo bellman_ford step 7658 current loss 0.043700, current_train_items 245088.
I0302 19:01:13.643210 22683591385216 run.py:483] Algo bellman_ford step 7659 current loss 0.098255, current_train_items 245120.
I0302 19:01:13.661669 22683591385216 run.py:483] Algo bellman_ford step 7660 current loss 0.002068, current_train_items 245152.
I0302 19:01:13.677369 22683591385216 run.py:483] Algo bellman_ford step 7661 current loss 0.003687, current_train_items 245184.
I0302 19:01:13.699265 22683591385216 run.py:483] Algo bellman_ford step 7662 current loss 0.043199, current_train_items 245216.
I0302 19:01:13.728327 22683591385216 run.py:483] Algo bellman_ford step 7663 current loss 0.049382, current_train_items 245248.
I0302 19:01:13.759664 22683591385216 run.py:483] Algo bellman_ford step 7664 current loss 0.046047, current_train_items 245280.
I0302 19:01:13.777791 22683591385216 run.py:483] Algo bellman_ford step 7665 current loss 0.012605, current_train_items 245312.
I0302 19:01:13.793142 22683591385216 run.py:483] Algo bellman_ford step 7666 current loss 0.014580, current_train_items 245344.
I0302 19:01:13.815119 22683591385216 run.py:483] Algo bellman_ford step 7667 current loss 0.061097, current_train_items 245376.
I0302 19:01:13.846764 22683591385216 run.py:483] Algo bellman_ford step 7668 current loss 0.100628, current_train_items 245408.
I0302 19:01:13.878686 22683591385216 run.py:483] Algo bellman_ford step 7669 current loss 0.075796, current_train_items 245440.
I0302 19:01:13.896996 22683591385216 run.py:483] Algo bellman_ford step 7670 current loss 0.032896, current_train_items 245472.
I0302 19:01:13.913012 22683591385216 run.py:483] Algo bellman_ford step 7671 current loss 0.013081, current_train_items 245504.
I0302 19:01:13.934740 22683591385216 run.py:483] Algo bellman_ford step 7672 current loss 0.032138, current_train_items 245536.
I0302 19:01:13.964692 22683591385216 run.py:483] Algo bellman_ford step 7673 current loss 0.048868, current_train_items 245568.
I0302 19:01:13.996614 22683591385216 run.py:483] Algo bellman_ford step 7674 current loss 0.060895, current_train_items 245600.
I0302 19:01:14.014965 22683591385216 run.py:483] Algo bellman_ford step 7675 current loss 0.001315, current_train_items 245632.
I0302 19:01:14.031098 22683591385216 run.py:483] Algo bellman_ford step 7676 current loss 0.038493, current_train_items 245664.
I0302 19:01:14.053737 22683591385216 run.py:483] Algo bellman_ford step 7677 current loss 0.040176, current_train_items 245696.
I0302 19:01:14.082800 22683591385216 run.py:483] Algo bellman_ford step 7678 current loss 0.042123, current_train_items 245728.
I0302 19:01:14.115124 22683591385216 run.py:483] Algo bellman_ford step 7679 current loss 0.042902, current_train_items 245760.
I0302 19:01:14.133159 22683591385216 run.py:483] Algo bellman_ford step 7680 current loss 0.001613, current_train_items 245792.
I0302 19:01:14.148701 22683591385216 run.py:483] Algo bellman_ford step 7681 current loss 0.011484, current_train_items 245824.
I0302 19:01:14.170597 22683591385216 run.py:483] Algo bellman_ford step 7682 current loss 0.028858, current_train_items 245856.
I0302 19:01:14.200215 22683591385216 run.py:483] Algo bellman_ford step 7683 current loss 0.109973, current_train_items 245888.
I0302 19:01:14.232142 22683591385216 run.py:483] Algo bellman_ford step 7684 current loss 0.082707, current_train_items 245920.
I0302 19:01:14.250711 22683591385216 run.py:483] Algo bellman_ford step 7685 current loss 0.019859, current_train_items 245952.
I0302 19:01:14.266572 22683591385216 run.py:483] Algo bellman_ford step 7686 current loss 0.011528, current_train_items 245984.
I0302 19:01:14.287779 22683591385216 run.py:483] Algo bellman_ford step 7687 current loss 0.069440, current_train_items 246016.
I0302 19:01:14.318211 22683591385216 run.py:483] Algo bellman_ford step 7688 current loss 0.035793, current_train_items 246048.
I0302 19:01:14.349673 22683591385216 run.py:483] Algo bellman_ford step 7689 current loss 0.101499, current_train_items 246080.
I0302 19:01:14.368178 22683591385216 run.py:483] Algo bellman_ford step 7690 current loss 0.007997, current_train_items 246112.
I0302 19:01:14.383860 22683591385216 run.py:483] Algo bellman_ford step 7691 current loss 0.014425, current_train_items 246144.
I0302 19:01:14.405438 22683591385216 run.py:483] Algo bellman_ford step 7692 current loss 0.028379, current_train_items 246176.
I0302 19:01:14.436345 22683591385216 run.py:483] Algo bellman_ford step 7693 current loss 0.073366, current_train_items 246208.
I0302 19:01:14.467260 22683591385216 run.py:483] Algo bellman_ford step 7694 current loss 0.053723, current_train_items 246240.
I0302 19:01:14.485621 22683591385216 run.py:483] Algo bellman_ford step 7695 current loss 0.003515, current_train_items 246272.
I0302 19:01:14.501262 22683591385216 run.py:483] Algo bellman_ford step 7696 current loss 0.008712, current_train_items 246304.
I0302 19:01:14.524194 22683591385216 run.py:483] Algo bellman_ford step 7697 current loss 0.093534, current_train_items 246336.
I0302 19:01:14.554733 22683591385216 run.py:483] Algo bellman_ford step 7698 current loss 0.070095, current_train_items 246368.
I0302 19:01:14.587908 22683591385216 run.py:483] Algo bellman_ford step 7699 current loss 0.092572, current_train_items 246400.
I0302 19:01:14.606607 22683591385216 run.py:483] Algo bellman_ford step 7700 current loss 0.002065, current_train_items 246432.
I0302 19:01:14.614340 22683591385216 run.py:503] (val) algo bellman_ford step 7700: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 246432, 'step': 7700, 'algorithm': 'bellman_ford'}
I0302 19:01:14.614449 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0302 19:01:14.630854 22683591385216 run.py:483] Algo bellman_ford step 7701 current loss 0.015106, current_train_items 246464.
I0302 19:01:14.653741 22683591385216 run.py:483] Algo bellman_ford step 7702 current loss 0.030725, current_train_items 246496.
I0302 19:01:14.682730 22683591385216 run.py:483] Algo bellman_ford step 7703 current loss 0.039905, current_train_items 246528.
I0302 19:01:14.713500 22683591385216 run.py:483] Algo bellman_ford step 7704 current loss 0.087936, current_train_items 246560.
I0302 19:01:14.731968 22683591385216 run.py:483] Algo bellman_ford step 7705 current loss 0.001192, current_train_items 246592.
I0302 19:01:14.747260 22683591385216 run.py:483] Algo bellman_ford step 7706 current loss 0.011785, current_train_items 246624.
I0302 19:01:14.770890 22683591385216 run.py:483] Algo bellman_ford step 7707 current loss 0.014385, current_train_items 246656.
I0302 19:01:14.799692 22683591385216 run.py:483] Algo bellman_ford step 7708 current loss 0.033711, current_train_items 246688.
I0302 19:01:14.830031 22683591385216 run.py:483] Algo bellman_ford step 7709 current loss 0.030190, current_train_items 246720.
I0302 19:01:14.848701 22683591385216 run.py:483] Algo bellman_ford step 7710 current loss 0.001377, current_train_items 246752.
I0302 19:01:14.864190 22683591385216 run.py:483] Algo bellman_ford step 7711 current loss 0.003517, current_train_items 246784.
I0302 19:01:14.886606 22683591385216 run.py:483] Algo bellman_ford step 7712 current loss 0.024012, current_train_items 246816.
I0302 19:01:14.915521 22683591385216 run.py:483] Algo bellman_ford step 7713 current loss 0.044719, current_train_items 246848.
I0302 19:01:14.946460 22683591385216 run.py:483] Algo bellman_ford step 7714 current loss 0.058070, current_train_items 246880.
I0302 19:01:14.964672 22683591385216 run.py:483] Algo bellman_ford step 7715 current loss 0.031071, current_train_items 246912.
I0302 19:01:14.979872 22683591385216 run.py:483] Algo bellman_ford step 7716 current loss 0.003013, current_train_items 246944.
I0302 19:01:15.002659 22683591385216 run.py:483] Algo bellman_ford step 7717 current loss 0.020614, current_train_items 246976.
I0302 19:01:15.032057 22683591385216 run.py:483] Algo bellman_ford step 7718 current loss 0.035892, current_train_items 247008.
I0302 19:01:15.062279 22683591385216 run.py:483] Algo bellman_ford step 7719 current loss 0.036333, current_train_items 247040.
I0302 19:01:15.080717 22683591385216 run.py:483] Algo bellman_ford step 7720 current loss 0.001117, current_train_items 247072.
I0302 19:01:15.095892 22683591385216 run.py:483] Algo bellman_ford step 7721 current loss 0.021957, current_train_items 247104.
I0302 19:01:15.119001 22683591385216 run.py:483] Algo bellman_ford step 7722 current loss 0.019854, current_train_items 247136.
I0302 19:01:15.149762 22683591385216 run.py:483] Algo bellman_ford step 7723 current loss 0.035257, current_train_items 247168.
I0302 19:01:15.181782 22683591385216 run.py:483] Algo bellman_ford step 7724 current loss 0.054167, current_train_items 247200.
I0302 19:01:15.200006 22683591385216 run.py:483] Algo bellman_ford step 7725 current loss 0.001785, current_train_items 247232.
I0302 19:01:15.215365 22683591385216 run.py:483] Algo bellman_ford step 7726 current loss 0.012414, current_train_items 247264.
I0302 19:01:15.236826 22683591385216 run.py:483] Algo bellman_ford step 7727 current loss 0.016583, current_train_items 247296.
I0302 19:01:15.266195 22683591385216 run.py:483] Algo bellman_ford step 7728 current loss 0.027293, current_train_items 247328.
I0302 19:01:15.296651 22683591385216 run.py:483] Algo bellman_ford step 7729 current loss 0.068899, current_train_items 247360.
I0302 19:01:15.314883 22683591385216 run.py:483] Algo bellman_ford step 7730 current loss 0.005985, current_train_items 247392.
I0302 19:01:15.329991 22683591385216 run.py:483] Algo bellman_ford step 7731 current loss 0.005432, current_train_items 247424.
I0302 19:01:15.351979 22683591385216 run.py:483] Algo bellman_ford step 7732 current loss 0.014391, current_train_items 247456.
I0302 19:01:15.381698 22683591385216 run.py:483] Algo bellman_ford step 7733 current loss 0.041623, current_train_items 247488.
I0302 19:01:15.411337 22683591385216 run.py:483] Algo bellman_ford step 7734 current loss 0.033459, current_train_items 247520.
I0302 19:01:15.429249 22683591385216 run.py:483] Algo bellman_ford step 7735 current loss 0.000832, current_train_items 247552.
I0302 19:01:15.444638 22683591385216 run.py:483] Algo bellman_ford step 7736 current loss 0.013760, current_train_items 247584.
I0302 19:01:15.466946 22683591385216 run.py:483] Algo bellman_ford step 7737 current loss 0.018232, current_train_items 247616.
I0302 19:01:15.496069 22683591385216 run.py:483] Algo bellman_ford step 7738 current loss 0.047821, current_train_items 247648.
I0302 19:01:15.527872 22683591385216 run.py:483] Algo bellman_ford step 7739 current loss 0.050271, current_train_items 247680.
I0302 19:01:15.545679 22683591385216 run.py:483] Algo bellman_ford step 7740 current loss 0.002693, current_train_items 247712.
I0302 19:01:15.561182 22683591385216 run.py:483] Algo bellman_ford step 7741 current loss 0.011449, current_train_items 247744.
I0302 19:01:15.584844 22683591385216 run.py:483] Algo bellman_ford step 7742 current loss 0.113319, current_train_items 247776.
I0302 19:01:15.615383 22683591385216 run.py:483] Algo bellman_ford step 7743 current loss 0.094462, current_train_items 247808.
I0302 19:01:15.646835 22683591385216 run.py:483] Algo bellman_ford step 7744 current loss 0.127961, current_train_items 247840.
I0302 19:01:15.665133 22683591385216 run.py:483] Algo bellman_ford step 7745 current loss 0.038764, current_train_items 247872.
I0302 19:01:15.680220 22683591385216 run.py:483] Algo bellman_ford step 7746 current loss 0.005420, current_train_items 247904.
I0302 19:01:15.702482 22683591385216 run.py:483] Algo bellman_ford step 7747 current loss 0.025711, current_train_items 247936.
I0302 19:01:15.733457 22683591385216 run.py:483] Algo bellman_ford step 7748 current loss 0.114703, current_train_items 247968.
I0302 19:01:15.765815 22683591385216 run.py:483] Algo bellman_ford step 7749 current loss 0.041015, current_train_items 248000.
I0302 19:01:15.784135 22683591385216 run.py:483] Algo bellman_ford step 7750 current loss 0.015178, current_train_items 248032.
I0302 19:01:15.792087 22683591385216 run.py:503] (val) algo bellman_ford step 7750: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 248032, 'step': 7750, 'algorithm': 'bellman_ford'}
I0302 19:01:15.792193 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:15.808396 22683591385216 run.py:483] Algo bellman_ford step 7751 current loss 0.017555, current_train_items 248064.
I0302 19:01:15.831011 22683591385216 run.py:483] Algo bellman_ford step 7752 current loss 0.038209, current_train_items 248096.
I0302 19:01:15.860777 22683591385216 run.py:483] Algo bellman_ford step 7753 current loss 0.037183, current_train_items 248128.
I0302 19:01:15.894191 22683591385216 run.py:483] Algo bellman_ford step 7754 current loss 0.065848, current_train_items 248160.
I0302 19:01:15.912696 22683591385216 run.py:483] Algo bellman_ford step 7755 current loss 0.000790, current_train_items 248192.
I0302 19:01:15.927437 22683591385216 run.py:483] Algo bellman_ford step 7756 current loss 0.024784, current_train_items 248224.
I0302 19:01:15.949846 22683591385216 run.py:483] Algo bellman_ford step 7757 current loss 0.015016, current_train_items 248256.
I0302 19:01:15.978366 22683591385216 run.py:483] Algo bellman_ford step 7758 current loss 0.056647, current_train_items 248288.
I0302 19:01:16.009226 22683591385216 run.py:483] Algo bellman_ford step 7759 current loss 0.076894, current_train_items 248320.
I0302 19:01:16.027790 22683591385216 run.py:483] Algo bellman_ford step 7760 current loss 0.002127, current_train_items 248352.
I0302 19:01:16.043378 22683591385216 run.py:483] Algo bellman_ford step 7761 current loss 0.034078, current_train_items 248384.
I0302 19:01:16.065998 22683591385216 run.py:483] Algo bellman_ford step 7762 current loss 0.039522, current_train_items 248416.
I0302 19:01:16.095297 22683591385216 run.py:483] Algo bellman_ford step 7763 current loss 0.047478, current_train_items 248448.
I0302 19:01:16.125178 22683591385216 run.py:483] Algo bellman_ford step 7764 current loss 0.043907, current_train_items 248480.
I0302 19:01:16.143389 22683591385216 run.py:483] Algo bellman_ford step 7765 current loss 0.002051, current_train_items 248512.
I0302 19:01:16.158722 22683591385216 run.py:483] Algo bellman_ford step 7766 current loss 0.041325, current_train_items 248544.
I0302 19:01:16.180687 22683591385216 run.py:483] Algo bellman_ford step 7767 current loss 0.045010, current_train_items 248576.
I0302 19:01:16.210114 22683591385216 run.py:483] Algo bellman_ford step 7768 current loss 0.129585, current_train_items 248608.
I0302 19:01:16.241129 22683591385216 run.py:483] Algo bellman_ford step 7769 current loss 0.090441, current_train_items 248640.
I0302 19:01:16.259577 22683591385216 run.py:483] Algo bellman_ford step 7770 current loss 0.002764, current_train_items 248672.
I0302 19:01:16.275347 22683591385216 run.py:483] Algo bellman_ford step 7771 current loss 0.032302, current_train_items 248704.
I0302 19:01:16.297025 22683591385216 run.py:483] Algo bellman_ford step 7772 current loss 0.028853, current_train_items 248736.
I0302 19:01:16.327038 22683591385216 run.py:483] Algo bellman_ford step 7773 current loss 0.028996, current_train_items 248768.
I0302 19:01:16.357770 22683591385216 run.py:483] Algo bellman_ford step 7774 current loss 0.068251, current_train_items 248800.
I0302 19:01:16.376417 22683591385216 run.py:483] Algo bellman_ford step 7775 current loss 0.004198, current_train_items 248832.
I0302 19:01:16.392092 22683591385216 run.py:483] Algo bellman_ford step 7776 current loss 0.026861, current_train_items 248864.
I0302 19:01:16.415390 22683591385216 run.py:483] Algo bellman_ford step 7777 current loss 0.033787, current_train_items 248896.
I0302 19:01:16.445345 22683591385216 run.py:483] Algo bellman_ford step 7778 current loss 0.047784, current_train_items 248928.
I0302 19:01:16.478358 22683591385216 run.py:483] Algo bellman_ford step 7779 current loss 0.071935, current_train_items 248960.
I0302 19:01:16.496178 22683591385216 run.py:483] Algo bellman_ford step 7780 current loss 0.002074, current_train_items 248992.
I0302 19:01:16.511761 22683591385216 run.py:483] Algo bellman_ford step 7781 current loss 0.006590, current_train_items 249024.
I0302 19:01:16.534123 22683591385216 run.py:483] Algo bellman_ford step 7782 current loss 0.046498, current_train_items 249056.
I0302 19:01:16.564519 22683591385216 run.py:483] Algo bellman_ford step 7783 current loss 0.059024, current_train_items 249088.
I0302 19:01:16.596050 22683591385216 run.py:483] Algo bellman_ford step 7784 current loss 0.045818, current_train_items 249120.
I0302 19:01:16.614706 22683591385216 run.py:483] Algo bellman_ford step 7785 current loss 0.003807, current_train_items 249152.
I0302 19:01:16.630349 22683591385216 run.py:483] Algo bellman_ford step 7786 current loss 0.019927, current_train_items 249184.
I0302 19:01:16.652047 22683591385216 run.py:483] Algo bellman_ford step 7787 current loss 0.032306, current_train_items 249216.
I0302 19:01:16.681418 22683591385216 run.py:483] Algo bellman_ford step 7788 current loss 0.021870, current_train_items 249248.
I0302 19:01:16.710637 22683591385216 run.py:483] Algo bellman_ford step 7789 current loss 0.047654, current_train_items 249280.
I0302 19:01:16.729159 22683591385216 run.py:483] Algo bellman_ford step 7790 current loss 0.002242, current_train_items 249312.
I0302 19:01:16.744370 22683591385216 run.py:483] Algo bellman_ford step 7791 current loss 0.059531, current_train_items 249344.
I0302 19:01:16.766338 22683591385216 run.py:483] Algo bellman_ford step 7792 current loss 0.026008, current_train_items 249376.
I0302 19:01:16.795263 22683591385216 run.py:483] Algo bellman_ford step 7793 current loss 0.029827, current_train_items 249408.
I0302 19:01:16.825857 22683591385216 run.py:483] Algo bellman_ford step 7794 current loss 0.030764, current_train_items 249440.
I0302 19:01:16.843985 22683591385216 run.py:483] Algo bellman_ford step 7795 current loss 0.008238, current_train_items 249472.
I0302 19:01:16.858893 22683591385216 run.py:483] Algo bellman_ford step 7796 current loss 0.020800, current_train_items 249504.
I0302 19:01:16.880431 22683591385216 run.py:483] Algo bellman_ford step 7797 current loss 0.015087, current_train_items 249536.
I0302 19:01:16.909017 22683591385216 run.py:483] Algo bellman_ford step 7798 current loss 0.014595, current_train_items 249568.
I0302 19:01:16.941457 22683591385216 run.py:483] Algo bellman_ford step 7799 current loss 0.050831, current_train_items 249600.
I0302 19:01:16.959909 22683591385216 run.py:483] Algo bellman_ford step 7800 current loss 0.002828, current_train_items 249632.
I0302 19:01:16.967736 22683591385216 run.py:503] (val) algo bellman_ford step 7800: {'pi': 0.990234375, 'score': 0.990234375, 'examples_seen': 249632, 'step': 7800, 'algorithm': 'bellman_ford'}
I0302 19:01:16.967841 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.990, val scores are: bellman_ford: 0.990
I0302 19:01:16.983732 22683591385216 run.py:483] Algo bellman_ford step 7801 current loss 0.036004, current_train_items 249664.
I0302 19:01:17.006271 22683591385216 run.py:483] Algo bellman_ford step 7802 current loss 0.043405, current_train_items 249696.
I0302 19:01:17.037313 22683591385216 run.py:483] Algo bellman_ford step 7803 current loss 0.045442, current_train_items 249728.
I0302 19:01:17.071060 22683591385216 run.py:483] Algo bellman_ford step 7804 current loss 0.052961, current_train_items 249760.
I0302 19:01:17.089500 22683591385216 run.py:483] Algo bellman_ford step 7805 current loss 0.002613, current_train_items 249792.
I0302 19:01:17.105152 22683591385216 run.py:483] Algo bellman_ford step 7806 current loss 0.002289, current_train_items 249824.
I0302 19:01:17.126701 22683591385216 run.py:483] Algo bellman_ford step 7807 current loss 0.018541, current_train_items 249856.
I0302 19:01:17.155272 22683591385216 run.py:483] Algo bellman_ford step 7808 current loss 0.040755, current_train_items 249888.
I0302 19:01:17.187039 22683591385216 run.py:483] Algo bellman_ford step 7809 current loss 0.072929, current_train_items 249920.
I0302 19:01:17.205560 22683591385216 run.py:483] Algo bellman_ford step 7810 current loss 0.000695, current_train_items 249952.
I0302 19:01:17.221215 22683591385216 run.py:483] Algo bellman_ford step 7811 current loss 0.005935, current_train_items 249984.
I0302 19:01:17.243941 22683591385216 run.py:483] Algo bellman_ford step 7812 current loss 0.046074, current_train_items 250016.
I0302 19:01:17.274468 22683591385216 run.py:483] Algo bellman_ford step 7813 current loss 0.055874, current_train_items 250048.
I0302 19:01:17.306006 22683591385216 run.py:483] Algo bellman_ford step 7814 current loss 0.052229, current_train_items 250080.
I0302 19:01:17.323980 22683591385216 run.py:483] Algo bellman_ford step 7815 current loss 0.000979, current_train_items 250112.
I0302 19:01:17.339173 22683591385216 run.py:483] Algo bellman_ford step 7816 current loss 0.030026, current_train_items 250144.
I0302 19:01:17.361796 22683591385216 run.py:483] Algo bellman_ford step 7817 current loss 0.016705, current_train_items 250176.
I0302 19:01:17.390031 22683591385216 run.py:483] Algo bellman_ford step 7818 current loss 0.056991, current_train_items 250208.
I0302 19:01:17.420563 22683591385216 run.py:483] Algo bellman_ford step 7819 current loss 0.049924, current_train_items 250240.
I0302 19:01:17.438846 22683591385216 run.py:483] Algo bellman_ford step 7820 current loss 0.002347, current_train_items 250272.
I0302 19:01:17.454533 22683591385216 run.py:483] Algo bellman_ford step 7821 current loss 0.019028, current_train_items 250304.
I0302 19:01:17.478235 22683591385216 run.py:483] Algo bellman_ford step 7822 current loss 0.029169, current_train_items 250336.
I0302 19:01:17.509348 22683591385216 run.py:483] Algo bellman_ford step 7823 current loss 0.141360, current_train_items 250368.
I0302 19:01:17.539890 22683591385216 run.py:483] Algo bellman_ford step 7824 current loss 0.133485, current_train_items 250400.
I0302 19:01:17.558079 22683591385216 run.py:483] Algo bellman_ford step 7825 current loss 0.003045, current_train_items 250432.
I0302 19:01:17.573834 22683591385216 run.py:483] Algo bellman_ford step 7826 current loss 0.016363, current_train_items 250464.
I0302 19:01:17.596801 22683591385216 run.py:483] Algo bellman_ford step 7827 current loss 0.020344, current_train_items 250496.
I0302 19:01:17.626812 22683591385216 run.py:483] Algo bellman_ford step 7828 current loss 0.055606, current_train_items 250528.
I0302 19:01:17.658282 22683591385216 run.py:483] Algo bellman_ford step 7829 current loss 0.068704, current_train_items 250560.
I0302 19:01:17.676300 22683591385216 run.py:483] Algo bellman_ford step 7830 current loss 0.002195, current_train_items 250592.
I0302 19:01:17.691381 22683591385216 run.py:483] Algo bellman_ford step 7831 current loss 0.013065, current_train_items 250624.
I0302 19:01:17.713938 22683591385216 run.py:483] Algo bellman_ford step 7832 current loss 0.021395, current_train_items 250656.
I0302 19:01:17.744532 22683591385216 run.py:483] Algo bellman_ford step 7833 current loss 0.060798, current_train_items 250688.
I0302 19:01:17.774178 22683591385216 run.py:483] Algo bellman_ford step 7834 current loss 0.063613, current_train_items 250720.
I0302 19:01:17.792404 22683591385216 run.py:483] Algo bellman_ford step 7835 current loss 0.005866, current_train_items 250752.
I0302 19:01:17.807893 22683591385216 run.py:483] Algo bellman_ford step 7836 current loss 0.011460, current_train_items 250784.
I0302 19:01:17.830335 22683591385216 run.py:483] Algo bellman_ford step 7837 current loss 0.010040, current_train_items 250816.
I0302 19:01:17.861470 22683591385216 run.py:483] Algo bellman_ford step 7838 current loss 0.044419, current_train_items 250848.
I0302 19:01:17.894339 22683591385216 run.py:483] Algo bellman_ford step 7839 current loss 0.114234, current_train_items 250880.
I0302 19:01:17.912363 22683591385216 run.py:483] Algo bellman_ford step 7840 current loss 0.004187, current_train_items 250912.
I0302 19:01:17.927808 22683591385216 run.py:483] Algo bellman_ford step 7841 current loss 0.025234, current_train_items 250944.
I0302 19:01:17.950280 22683591385216 run.py:483] Algo bellman_ford step 7842 current loss 0.020538, current_train_items 250976.
I0302 19:01:17.981457 22683591385216 run.py:483] Algo bellman_ford step 7843 current loss 0.040418, current_train_items 251008.
I0302 19:01:18.013881 22683591385216 run.py:483] Algo bellman_ford step 7844 current loss 0.045193, current_train_items 251040.
I0302 19:01:18.032257 22683591385216 run.py:483] Algo bellman_ford step 7845 current loss 0.002528, current_train_items 251072.
I0302 19:01:18.047541 22683591385216 run.py:483] Algo bellman_ford step 7846 current loss 0.011476, current_train_items 251104.
I0302 19:01:18.070209 22683591385216 run.py:483] Algo bellman_ford step 7847 current loss 0.028140, current_train_items 251136.
I0302 19:01:18.100439 22683591385216 run.py:483] Algo bellman_ford step 7848 current loss 0.043963, current_train_items 251168.
I0302 19:01:18.132478 22683591385216 run.py:483] Algo bellman_ford step 7849 current loss 0.062255, current_train_items 251200.
I0302 19:01:18.150464 22683591385216 run.py:483] Algo bellman_ford step 7850 current loss 0.000997, current_train_items 251232.
I0302 19:01:18.158459 22683591385216 run.py:503] (val) algo bellman_ford step 7850: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 251232, 'step': 7850, 'algorithm': 'bellman_ford'}
I0302 19:01:18.158564 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:18.174534 22683591385216 run.py:483] Algo bellman_ford step 7851 current loss 0.020165, current_train_items 251264.
I0302 19:01:18.197276 22683591385216 run.py:483] Algo bellman_ford step 7852 current loss 0.015221, current_train_items 251296.
I0302 19:01:18.228773 22683591385216 run.py:483] Algo bellman_ford step 7853 current loss 0.047638, current_train_items 251328.
I0302 19:01:18.263155 22683591385216 run.py:483] Algo bellman_ford step 7854 current loss 0.063375, current_train_items 251360.
I0302 19:01:18.281694 22683591385216 run.py:483] Algo bellman_ford step 7855 current loss 0.000621, current_train_items 251392.
I0302 19:01:18.297490 22683591385216 run.py:483] Algo bellman_ford step 7856 current loss 0.008515, current_train_items 251424.
I0302 19:01:18.320599 22683591385216 run.py:483] Algo bellman_ford step 7857 current loss 0.076660, current_train_items 251456.
I0302 19:01:18.350472 22683591385216 run.py:483] Algo bellman_ford step 7858 current loss 0.016759, current_train_items 251488.
I0302 19:01:18.383378 22683591385216 run.py:483] Algo bellman_ford step 7859 current loss 0.071811, current_train_items 251520.
I0302 19:01:18.402261 22683591385216 run.py:483] Algo bellman_ford step 7860 current loss 0.005621, current_train_items 251552.
I0302 19:01:18.417658 22683591385216 run.py:483] Algo bellman_ford step 7861 current loss 0.005098, current_train_items 251584.
I0302 19:01:18.440166 22683591385216 run.py:483] Algo bellman_ford step 7862 current loss 0.037510, current_train_items 251616.
I0302 19:01:18.469522 22683591385216 run.py:483] Algo bellman_ford step 7863 current loss 0.125624, current_train_items 251648.
I0302 19:01:18.499643 22683591385216 run.py:483] Algo bellman_ford step 7864 current loss 0.032083, current_train_items 251680.
I0302 19:01:18.518260 22683591385216 run.py:483] Algo bellman_ford step 7865 current loss 0.001105, current_train_items 251712.
I0302 19:01:18.533881 22683591385216 run.py:483] Algo bellman_ford step 7866 current loss 0.021969, current_train_items 251744.
I0302 19:01:18.557302 22683591385216 run.py:483] Algo bellman_ford step 7867 current loss 0.034667, current_train_items 251776.
I0302 19:01:18.586401 22683591385216 run.py:483] Algo bellman_ford step 7868 current loss 0.030140, current_train_items 251808.
I0302 19:01:18.616946 22683591385216 run.py:483] Algo bellman_ford step 7869 current loss 0.064366, current_train_items 251840.
I0302 19:01:18.635813 22683591385216 run.py:483] Algo bellman_ford step 7870 current loss 0.001581, current_train_items 251872.
I0302 19:01:18.651476 22683591385216 run.py:483] Algo bellman_ford step 7871 current loss 0.018189, current_train_items 251904.
I0302 19:01:18.674159 22683591385216 run.py:483] Algo bellman_ford step 7872 current loss 0.033576, current_train_items 251936.
I0302 19:01:18.703254 22683591385216 run.py:483] Algo bellman_ford step 7873 current loss 0.045946, current_train_items 251968.
I0302 19:01:18.734336 22683591385216 run.py:483] Algo bellman_ford step 7874 current loss 0.056433, current_train_items 252000.
I0302 19:01:18.753125 22683591385216 run.py:483] Algo bellman_ford step 7875 current loss 0.036616, current_train_items 252032.
I0302 19:01:18.768350 22683591385216 run.py:483] Algo bellman_ford step 7876 current loss 0.016950, current_train_items 252064.
I0302 19:01:18.790533 22683591385216 run.py:483] Algo bellman_ford step 7877 current loss 0.061599, current_train_items 252096.
I0302 19:01:18.820133 22683591385216 run.py:483] Algo bellman_ford step 7878 current loss 0.029832, current_train_items 252128.
I0302 19:01:18.851633 22683591385216 run.py:483] Algo bellman_ford step 7879 current loss 0.054833, current_train_items 252160.
I0302 19:01:18.869929 22683591385216 run.py:483] Algo bellman_ford step 7880 current loss 0.001551, current_train_items 252192.
I0302 19:01:18.884934 22683591385216 run.py:483] Algo bellman_ford step 7881 current loss 0.004456, current_train_items 252224.
I0302 19:01:18.907071 22683591385216 run.py:483] Algo bellman_ford step 7882 current loss 0.032250, current_train_items 252256.
I0302 19:01:18.937511 22683591385216 run.py:483] Algo bellman_ford step 7883 current loss 0.034141, current_train_items 252288.
I0302 19:01:18.969654 22683591385216 run.py:483] Algo bellman_ford step 7884 current loss 0.080407, current_train_items 252320.
I0302 19:01:18.988672 22683591385216 run.py:483] Algo bellman_ford step 7885 current loss 0.002497, current_train_items 252352.
I0302 19:01:19.004354 22683591385216 run.py:483] Algo bellman_ford step 7886 current loss 0.016400, current_train_items 252384.
I0302 19:01:19.025726 22683591385216 run.py:483] Algo bellman_ford step 7887 current loss 0.079022, current_train_items 252416.
I0302 19:01:19.053685 22683591385216 run.py:483] Algo bellman_ford step 7888 current loss 0.009199, current_train_items 252448.
I0302 19:01:19.085075 22683591385216 run.py:483] Algo bellman_ford step 7889 current loss 0.030220, current_train_items 252480.
I0302 19:01:19.103939 22683591385216 run.py:483] Algo bellman_ford step 7890 current loss 0.002037, current_train_items 252512.
I0302 19:01:19.119333 22683591385216 run.py:483] Algo bellman_ford step 7891 current loss 0.003216, current_train_items 252544.
I0302 19:01:19.141958 22683591385216 run.py:483] Algo bellman_ford step 7892 current loss 0.025135, current_train_items 252576.
I0302 19:01:19.172314 22683591385216 run.py:483] Algo bellman_ford step 7893 current loss 0.036738, current_train_items 252608.
I0302 19:01:19.203277 22683591385216 run.py:483] Algo bellman_ford step 7894 current loss 0.050109, current_train_items 252640.
I0302 19:01:19.221509 22683591385216 run.py:483] Algo bellman_ford step 7895 current loss 0.003606, current_train_items 252672.
I0302 19:01:19.236517 22683591385216 run.py:483] Algo bellman_ford step 7896 current loss 0.011392, current_train_items 252704.
I0302 19:01:19.259524 22683591385216 run.py:483] Algo bellman_ford step 7897 current loss 0.030858, current_train_items 252736.
I0302 19:01:19.289936 22683591385216 run.py:483] Algo bellman_ford step 7898 current loss 0.027895, current_train_items 252768.
I0302 19:01:19.321827 22683591385216 run.py:483] Algo bellman_ford step 7899 current loss 0.047236, current_train_items 252800.
I0302 19:01:19.340470 22683591385216 run.py:483] Algo bellman_ford step 7900 current loss 0.002492, current_train_items 252832.
I0302 19:01:19.348291 22683591385216 run.py:503] (val) algo bellman_ford step 7900: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 252832, 'step': 7900, 'algorithm': 'bellman_ford'}
I0302 19:01:19.348395 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:19.364192 22683591385216 run.py:483] Algo bellman_ford step 7901 current loss 0.012209, current_train_items 252864.
I0302 19:01:19.387487 22683591385216 run.py:483] Algo bellman_ford step 7902 current loss 0.024847, current_train_items 252896.
I0302 19:01:19.418384 22683591385216 run.py:483] Algo bellman_ford step 7903 current loss 0.028859, current_train_items 252928.
I0302 19:01:19.452976 22683591385216 run.py:483] Algo bellman_ford step 7904 current loss 0.062062, current_train_items 252960.
I0302 19:01:19.471373 22683591385216 run.py:483] Algo bellman_ford step 7905 current loss 0.002631, current_train_items 252992.
I0302 19:01:19.487304 22683591385216 run.py:483] Algo bellman_ford step 7906 current loss 0.011147, current_train_items 253024.
I0302 19:01:19.510508 22683591385216 run.py:483] Algo bellman_ford step 7907 current loss 0.030168, current_train_items 253056.
I0302 19:01:19.539919 22683591385216 run.py:483] Algo bellman_ford step 7908 current loss 0.025916, current_train_items 253088.
I0302 19:01:19.569252 22683591385216 run.py:483] Algo bellman_ford step 7909 current loss 0.038859, current_train_items 253120.
I0302 19:01:19.587451 22683591385216 run.py:483] Algo bellman_ford step 7910 current loss 0.006613, current_train_items 253152.
I0302 19:01:19.603368 22683591385216 run.py:483] Algo bellman_ford step 7911 current loss 0.024246, current_train_items 253184.
I0302 19:01:19.626382 22683591385216 run.py:483] Algo bellman_ford step 7912 current loss 0.033850, current_train_items 253216.
I0302 19:01:19.656238 22683591385216 run.py:483] Algo bellman_ford step 7913 current loss 0.022471, current_train_items 253248.
I0302 19:01:19.687890 22683591385216 run.py:483] Algo bellman_ford step 7914 current loss 0.079695, current_train_items 253280.
I0302 19:01:19.706569 22683591385216 run.py:483] Algo bellman_ford step 7915 current loss 0.004451, current_train_items 253312.
I0302 19:01:19.722172 22683591385216 run.py:483] Algo bellman_ford step 7916 current loss 0.017038, current_train_items 253344.
I0302 19:01:19.744824 22683591385216 run.py:483] Algo bellman_ford step 7917 current loss 0.047331, current_train_items 253376.
I0302 19:01:19.774953 22683591385216 run.py:483] Algo bellman_ford step 7918 current loss 0.023781, current_train_items 253408.
I0302 19:01:19.808161 22683591385216 run.py:483] Algo bellman_ford step 7919 current loss 0.037542, current_train_items 253440.
I0302 19:01:19.826656 22683591385216 run.py:483] Algo bellman_ford step 7920 current loss 0.001143, current_train_items 253472.
I0302 19:01:19.842279 22683591385216 run.py:483] Algo bellman_ford step 7921 current loss 0.008478, current_train_items 253504.
I0302 19:01:19.865114 22683591385216 run.py:483] Algo bellman_ford step 7922 current loss 0.034684, current_train_items 253536.
I0302 19:01:19.894558 22683591385216 run.py:483] Algo bellman_ford step 7923 current loss 0.023946, current_train_items 253568.
I0302 19:01:19.926605 22683591385216 run.py:483] Algo bellman_ford step 7924 current loss 0.064389, current_train_items 253600.
I0302 19:01:19.944789 22683591385216 run.py:483] Algo bellman_ford step 7925 current loss 0.007143, current_train_items 253632.
I0302 19:01:19.960225 22683591385216 run.py:483] Algo bellman_ford step 7926 current loss 0.006508, current_train_items 253664.
I0302 19:01:19.983138 22683591385216 run.py:483] Algo bellman_ford step 7927 current loss 0.083341, current_train_items 253696.
I0302 19:01:20.014525 22683591385216 run.py:483] Algo bellman_ford step 7928 current loss 0.048651, current_train_items 253728.
I0302 19:01:20.045506 22683591385216 run.py:483] Algo bellman_ford step 7929 current loss 0.032815, current_train_items 253760.
I0302 19:01:20.063890 22683591385216 run.py:483] Algo bellman_ford step 7930 current loss 0.003522, current_train_items 253792.
I0302 19:01:20.079324 22683591385216 run.py:483] Algo bellman_ford step 7931 current loss 0.014999, current_train_items 253824.
I0302 19:01:20.100886 22683591385216 run.py:483] Algo bellman_ford step 7932 current loss 0.048409, current_train_items 253856.
I0302 19:01:20.131248 22683591385216 run.py:483] Algo bellman_ford step 7933 current loss 0.041574, current_train_items 253888.
I0302 19:01:20.163542 22683591385216 run.py:483] Algo bellman_ford step 7934 current loss 0.034924, current_train_items 253920.
I0302 19:01:20.181667 22683591385216 run.py:483] Algo bellman_ford step 7935 current loss 0.004739, current_train_items 253952.
I0302 19:01:20.196694 22683591385216 run.py:483] Algo bellman_ford step 7936 current loss 0.021213, current_train_items 253984.
I0302 19:01:20.220117 22683591385216 run.py:483] Algo bellman_ford step 7937 current loss 0.027119, current_train_items 254016.
I0302 19:01:20.249655 22683591385216 run.py:483] Algo bellman_ford step 7938 current loss 0.063027, current_train_items 254048.
I0302 19:01:20.282266 22683591385216 run.py:483] Algo bellman_ford step 7939 current loss 0.125826, current_train_items 254080.
I0302 19:01:20.300711 22683591385216 run.py:483] Algo bellman_ford step 7940 current loss 0.016308, current_train_items 254112.
I0302 19:01:20.315574 22683591385216 run.py:483] Algo bellman_ford step 7941 current loss 0.003464, current_train_items 254144.
I0302 19:01:20.338932 22683591385216 run.py:483] Algo bellman_ford step 7942 current loss 0.102591, current_train_items 254176.
I0302 19:01:20.370483 22683591385216 run.py:483] Algo bellman_ford step 7943 current loss 0.222647, current_train_items 254208.
I0302 19:01:20.401643 22683591385216 run.py:483] Algo bellman_ford step 7944 current loss 0.159187, current_train_items 254240.
I0302 19:01:20.419960 22683591385216 run.py:483] Algo bellman_ford step 7945 current loss 0.002976, current_train_items 254272.
I0302 19:01:20.434779 22683591385216 run.py:483] Algo bellman_ford step 7946 current loss 0.026169, current_train_items 254304.
I0302 19:01:20.456490 22683591385216 run.py:483] Algo bellman_ford step 7947 current loss 0.035260, current_train_items 254336.
I0302 19:01:20.485012 22683591385216 run.py:483] Algo bellman_ford step 7948 current loss 0.012521, current_train_items 254368.
I0302 19:01:20.517136 22683591385216 run.py:483] Algo bellman_ford step 7949 current loss 0.081876, current_train_items 254400.
I0302 19:01:20.535229 22683591385216 run.py:483] Algo bellman_ford step 7950 current loss 0.001858, current_train_items 254432.
I0302 19:01:20.543253 22683591385216 run.py:503] (val) algo bellman_ford step 7950: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 254432, 'step': 7950, 'algorithm': 'bellman_ford'}
I0302 19:01:20.543361 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:20.559332 22683591385216 run.py:483] Algo bellman_ford step 7951 current loss 0.012129, current_train_items 254464.
I0302 19:01:20.582588 22683591385216 run.py:483] Algo bellman_ford step 7952 current loss 0.067838, current_train_items 254496.
I0302 19:01:20.613585 22683591385216 run.py:483] Algo bellman_ford step 7953 current loss 0.057619, current_train_items 254528.
I0302 19:01:20.645067 22683591385216 run.py:483] Algo bellman_ford step 7954 current loss 0.043194, current_train_items 254560.
I0302 19:01:20.663606 22683591385216 run.py:483] Algo bellman_ford step 7955 current loss 0.007202, current_train_items 254592.
I0302 19:01:20.679098 22683591385216 run.py:483] Algo bellman_ford step 7956 current loss 0.029077, current_train_items 254624.
I0302 19:01:20.701969 22683591385216 run.py:483] Algo bellman_ford step 7957 current loss 0.045596, current_train_items 254656.
I0302 19:01:20.731974 22683591385216 run.py:483] Algo bellman_ford step 7958 current loss 0.135972, current_train_items 254688.
I0302 19:01:20.763343 22683591385216 run.py:483] Algo bellman_ford step 7959 current loss 0.054389, current_train_items 254720.
I0302 19:01:20.782194 22683591385216 run.py:483] Algo bellman_ford step 7960 current loss 0.002111, current_train_items 254752.
I0302 19:01:20.798324 22683591385216 run.py:483] Algo bellman_ford step 7961 current loss 0.018908, current_train_items 254784.
I0302 19:01:20.820695 22683591385216 run.py:483] Algo bellman_ford step 7962 current loss 0.054210, current_train_items 254816.
I0302 19:01:20.849696 22683591385216 run.py:483] Algo bellman_ford step 7963 current loss 0.045047, current_train_items 254848.
I0302 19:01:20.881696 22683591385216 run.py:483] Algo bellman_ford step 7964 current loss 0.045572, current_train_items 254880.
I0302 19:01:20.899787 22683591385216 run.py:483] Algo bellman_ford step 7965 current loss 0.002876, current_train_items 254912.
I0302 19:01:20.915437 22683591385216 run.py:483] Algo bellman_ford step 7966 current loss 0.015526, current_train_items 254944.
I0302 19:01:20.939202 22683591385216 run.py:483] Algo bellman_ford step 7967 current loss 0.095149, current_train_items 254976.
I0302 19:01:20.968652 22683591385216 run.py:483] Algo bellman_ford step 7968 current loss 0.038170, current_train_items 255008.
I0302 19:01:20.998747 22683591385216 run.py:483] Algo bellman_ford step 7969 current loss 0.049069, current_train_items 255040.
I0302 19:01:21.017447 22683591385216 run.py:483] Algo bellman_ford step 7970 current loss 0.007472, current_train_items 255072.
I0302 19:01:21.033072 22683591385216 run.py:483] Algo bellman_ford step 7971 current loss 0.025240, current_train_items 255104.
I0302 19:01:21.055790 22683591385216 run.py:483] Algo bellman_ford step 7972 current loss 0.088930, current_train_items 255136.
I0302 19:01:21.084956 22683591385216 run.py:483] Algo bellman_ford step 7973 current loss 0.031750, current_train_items 255168.
I0302 19:01:21.117755 22683591385216 run.py:483] Algo bellman_ford step 7974 current loss 0.070193, current_train_items 255200.
I0302 19:01:21.136474 22683591385216 run.py:483] Algo bellman_ford step 7975 current loss 0.008723, current_train_items 255232.
I0302 19:01:21.152251 22683591385216 run.py:483] Algo bellman_ford step 7976 current loss 0.011952, current_train_items 255264.
I0302 19:01:21.174976 22683591385216 run.py:483] Algo bellman_ford step 7977 current loss 0.022115, current_train_items 255296.
I0302 19:01:21.204613 22683591385216 run.py:483] Algo bellman_ford step 7978 current loss 0.037601, current_train_items 255328.
I0302 19:01:21.234183 22683591385216 run.py:483] Algo bellman_ford step 7979 current loss 0.041825, current_train_items 255360.
I0302 19:01:21.252657 22683591385216 run.py:483] Algo bellman_ford step 7980 current loss 0.000783, current_train_items 255392.
I0302 19:01:21.267846 22683591385216 run.py:483] Algo bellman_ford step 7981 current loss 0.019105, current_train_items 255424.
I0302 19:01:21.289741 22683591385216 run.py:483] Algo bellman_ford step 7982 current loss 0.014729, current_train_items 255456.
I0302 19:01:21.318778 22683591385216 run.py:483] Algo bellman_ford step 7983 current loss 0.026711, current_train_items 255488.
I0302 19:01:21.350867 22683591385216 run.py:483] Algo bellman_ford step 7984 current loss 0.054917, current_train_items 255520.
I0302 19:01:21.369421 22683591385216 run.py:483] Algo bellman_ford step 7985 current loss 0.001040, current_train_items 255552.
I0302 19:01:21.385297 22683591385216 run.py:483] Algo bellman_ford step 7986 current loss 0.013449, current_train_items 255584.
I0302 19:01:21.407078 22683591385216 run.py:483] Algo bellman_ford step 7987 current loss 0.029613, current_train_items 255616.
I0302 19:01:21.437231 22683591385216 run.py:483] Algo bellman_ford step 7988 current loss 0.040439, current_train_items 255648.
I0302 19:01:21.468019 22683591385216 run.py:483] Algo bellman_ford step 7989 current loss 0.070003, current_train_items 255680.
I0302 19:01:21.486480 22683591385216 run.py:483] Algo bellman_ford step 7990 current loss 0.001170, current_train_items 255712.
I0302 19:01:21.502219 22683591385216 run.py:483] Algo bellman_ford step 7991 current loss 0.019948, current_train_items 255744.
I0302 19:01:21.524697 22683591385216 run.py:483] Algo bellman_ford step 7992 current loss 0.059346, current_train_items 255776.
I0302 19:01:21.556162 22683591385216 run.py:483] Algo bellman_ford step 7993 current loss 0.034168, current_train_items 255808.
I0302 19:01:21.588615 22683591385216 run.py:483] Algo bellman_ford step 7994 current loss 0.032053, current_train_items 255840.
I0302 19:01:21.606972 22683591385216 run.py:483] Algo bellman_ford step 7995 current loss 0.002129, current_train_items 255872.
I0302 19:01:21.621841 22683591385216 run.py:483] Algo bellman_ford step 7996 current loss 0.005973, current_train_items 255904.
I0302 19:01:21.644387 22683591385216 run.py:483] Algo bellman_ford step 7997 current loss 0.031996, current_train_items 255936.
I0302 19:01:21.674185 22683591385216 run.py:483] Algo bellman_ford step 7998 current loss 0.017873, current_train_items 255968.
I0302 19:01:21.703978 22683591385216 run.py:483] Algo bellman_ford step 7999 current loss 0.054226, current_train_items 256000.
I0302 19:01:21.722746 22683591385216 run.py:483] Algo bellman_ford step 8000 current loss 0.001446, current_train_items 256032.
I0302 19:01:21.730544 22683591385216 run.py:503] (val) algo bellman_ford step 8000: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 256032, 'step': 8000, 'algorithm': 'bellman_ford'}
I0302 19:01:21.730651 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:21.746876 22683591385216 run.py:483] Algo bellman_ford step 8001 current loss 0.009184, current_train_items 256064.
I0302 19:01:21.769057 22683591385216 run.py:483] Algo bellman_ford step 8002 current loss 0.026296, current_train_items 256096.
I0302 19:01:21.800107 22683591385216 run.py:483] Algo bellman_ford step 8003 current loss 0.036392, current_train_items 256128.
I0302 19:01:21.830501 22683591385216 run.py:483] Algo bellman_ford step 8004 current loss 0.048056, current_train_items 256160.
I0302 19:01:21.849285 22683591385216 run.py:483] Algo bellman_ford step 8005 current loss 0.000805, current_train_items 256192.
I0302 19:01:21.864683 22683591385216 run.py:483] Algo bellman_ford step 8006 current loss 0.014144, current_train_items 256224.
I0302 19:01:21.887319 22683591385216 run.py:483] Algo bellman_ford step 8007 current loss 0.011023, current_train_items 256256.
I0302 19:01:21.918652 22683591385216 run.py:483] Algo bellman_ford step 8008 current loss 0.027034, current_train_items 256288.
I0302 19:01:21.950661 22683591385216 run.py:483] Algo bellman_ford step 8009 current loss 0.044430, current_train_items 256320.
I0302 19:01:21.968927 22683591385216 run.py:483] Algo bellman_ford step 8010 current loss 0.001570, current_train_items 256352.
I0302 19:01:21.984078 22683591385216 run.py:483] Algo bellman_ford step 8011 current loss 0.011533, current_train_items 256384.
I0302 19:01:22.007606 22683591385216 run.py:483] Algo bellman_ford step 8012 current loss 0.024103, current_train_items 256416.
I0302 19:01:22.037001 22683591385216 run.py:483] Algo bellman_ford step 8013 current loss 0.031177, current_train_items 256448.
I0302 19:01:22.066566 22683591385216 run.py:483] Algo bellman_ford step 8014 current loss 0.052241, current_train_items 256480.
I0302 19:01:22.085012 22683591385216 run.py:483] Algo bellman_ford step 8015 current loss 0.027334, current_train_items 256512.
I0302 19:01:22.100936 22683591385216 run.py:483] Algo bellman_ford step 8016 current loss 0.011719, current_train_items 256544.
I0302 19:01:22.123049 22683591385216 run.py:483] Algo bellman_ford step 8017 current loss 0.037621, current_train_items 256576.
I0302 19:01:22.152446 22683591385216 run.py:483] Algo bellman_ford step 8018 current loss 0.107454, current_train_items 256608.
I0302 19:01:22.182317 22683591385216 run.py:483] Algo bellman_ford step 8019 current loss 0.029864, current_train_items 256640.
I0302 19:01:22.200381 22683591385216 run.py:483] Algo bellman_ford step 8020 current loss 0.000810, current_train_items 256672.
I0302 19:01:22.215845 22683591385216 run.py:483] Algo bellman_ford step 8021 current loss 0.007384, current_train_items 256704.
I0302 19:01:22.238086 22683591385216 run.py:483] Algo bellman_ford step 8022 current loss 0.021330, current_train_items 256736.
I0302 19:01:22.266977 22683591385216 run.py:483] Algo bellman_ford step 8023 current loss 0.029197, current_train_items 256768.
I0302 19:01:22.300119 22683591385216 run.py:483] Algo bellman_ford step 8024 current loss 0.044062, current_train_items 256800.
I0302 19:01:22.318416 22683591385216 run.py:483] Algo bellman_ford step 8025 current loss 0.015847, current_train_items 256832.
I0302 19:01:22.334004 22683591385216 run.py:483] Algo bellman_ford step 8026 current loss 0.005332, current_train_items 256864.
I0302 19:01:22.356884 22683591385216 run.py:483] Algo bellman_ford step 8027 current loss 0.026147, current_train_items 256896.
I0302 19:01:22.385798 22683591385216 run.py:483] Algo bellman_ford step 8028 current loss 0.024320, current_train_items 256928.
I0302 19:01:22.417268 22683591385216 run.py:483] Algo bellman_ford step 8029 current loss 0.079508, current_train_items 256960.
I0302 19:01:22.435593 22683591385216 run.py:483] Algo bellman_ford step 8030 current loss 0.002225, current_train_items 256992.
I0302 19:01:22.450912 22683591385216 run.py:483] Algo bellman_ford step 8031 current loss 0.019405, current_train_items 257024.
I0302 19:01:22.473843 22683591385216 run.py:483] Algo bellman_ford step 8032 current loss 0.040864, current_train_items 257056.
I0302 19:01:22.504142 22683591385216 run.py:483] Algo bellman_ford step 8033 current loss 0.037496, current_train_items 257088.
I0302 19:01:22.536414 22683591385216 run.py:483] Algo bellman_ford step 8034 current loss 0.042441, current_train_items 257120.
I0302 19:01:22.555067 22683591385216 run.py:483] Algo bellman_ford step 8035 current loss 0.002157, current_train_items 257152.
I0302 19:01:22.570830 22683591385216 run.py:483] Algo bellman_ford step 8036 current loss 0.021447, current_train_items 257184.
I0302 19:01:22.593490 22683591385216 run.py:483] Algo bellman_ford step 8037 current loss 0.044026, current_train_items 257216.
I0302 19:01:22.623066 22683591385216 run.py:483] Algo bellman_ford step 8038 current loss 0.035709, current_train_items 257248.
I0302 19:01:22.656096 22683591385216 run.py:483] Algo bellman_ford step 8039 current loss 0.061396, current_train_items 257280.
I0302 19:01:22.674413 22683591385216 run.py:483] Algo bellman_ford step 8040 current loss 0.001841, current_train_items 257312.
I0302 19:01:22.690055 22683591385216 run.py:483] Algo bellman_ford step 8041 current loss 0.015908, current_train_items 257344.
I0302 19:01:22.712744 22683591385216 run.py:483] Algo bellman_ford step 8042 current loss 0.036405, current_train_items 257376.
I0302 19:01:22.742817 22683591385216 run.py:483] Algo bellman_ford step 8043 current loss 0.047223, current_train_items 257408.
I0302 19:01:22.774756 22683591385216 run.py:483] Algo bellman_ford step 8044 current loss 0.067914, current_train_items 257440.
I0302 19:01:22.793564 22683591385216 run.py:483] Algo bellman_ford step 8045 current loss 0.001133, current_train_items 257472.
I0302 19:01:22.809430 22683591385216 run.py:483] Algo bellman_ford step 8046 current loss 0.024841, current_train_items 257504.
I0302 19:01:22.831568 22683591385216 run.py:483] Algo bellman_ford step 8047 current loss 0.036666, current_train_items 257536.
I0302 19:01:22.862097 22683591385216 run.py:483] Algo bellman_ford step 8048 current loss 0.070201, current_train_items 257568.
I0302 19:01:22.892345 22683591385216 run.py:483] Algo bellman_ford step 8049 current loss 0.042072, current_train_items 257600.
I0302 19:01:22.910578 22683591385216 run.py:483] Algo bellman_ford step 8050 current loss 0.002248, current_train_items 257632.
I0302 19:01:22.918509 22683591385216 run.py:503] (val) algo bellman_ford step 8050: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 257632, 'step': 8050, 'algorithm': 'bellman_ford'}
I0302 19:01:22.918613 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:22.934740 22683591385216 run.py:483] Algo bellman_ford step 8051 current loss 0.020830, current_train_items 257664.
I0302 19:01:22.957546 22683591385216 run.py:483] Algo bellman_ford step 8052 current loss 0.013089, current_train_items 257696.
I0302 19:01:22.987662 22683591385216 run.py:483] Algo bellman_ford step 8053 current loss 0.035137, current_train_items 257728.
I0302 19:01:23.021187 22683591385216 run.py:483] Algo bellman_ford step 8054 current loss 0.122426, current_train_items 257760.
I0302 19:01:23.039906 22683591385216 run.py:483] Algo bellman_ford step 8055 current loss 0.015113, current_train_items 257792.
I0302 19:01:23.054433 22683591385216 run.py:483] Algo bellman_ford step 8056 current loss 0.004730, current_train_items 257824.
I0302 19:01:23.075927 22683591385216 run.py:483] Algo bellman_ford step 8057 current loss 0.024964, current_train_items 257856.
I0302 19:01:23.106928 22683591385216 run.py:483] Algo bellman_ford step 8058 current loss 0.035334, current_train_items 257888.
I0302 19:01:23.140370 22683591385216 run.py:483] Algo bellman_ford step 8059 current loss 0.052430, current_train_items 257920.
I0302 19:01:23.158955 22683591385216 run.py:483] Algo bellman_ford step 8060 current loss 0.006942, current_train_items 257952.
I0302 19:01:23.174254 22683591385216 run.py:483] Algo bellman_ford step 8061 current loss 0.020395, current_train_items 257984.
I0302 19:01:23.197861 22683591385216 run.py:483] Algo bellman_ford step 8062 current loss 0.042718, current_train_items 258016.
I0302 19:01:23.227621 22683591385216 run.py:483] Algo bellman_ford step 8063 current loss 0.053638, current_train_items 258048.
I0302 19:01:23.258315 22683591385216 run.py:483] Algo bellman_ford step 8064 current loss 0.052187, current_train_items 258080.
I0302 19:01:23.276473 22683591385216 run.py:483] Algo bellman_ford step 8065 current loss 0.001579, current_train_items 258112.
I0302 19:01:23.291574 22683591385216 run.py:483] Algo bellman_ford step 8066 current loss 0.022320, current_train_items 258144.
I0302 19:01:23.314239 22683591385216 run.py:483] Algo bellman_ford step 8067 current loss 0.020439, current_train_items 258176.
I0302 19:01:23.343758 22683591385216 run.py:483] Algo bellman_ford step 8068 current loss 0.034532, current_train_items 258208.
I0302 19:01:23.373910 22683591385216 run.py:483] Algo bellman_ford step 8069 current loss 0.035018, current_train_items 258240.
I0302 19:01:23.392842 22683591385216 run.py:483] Algo bellman_ford step 8070 current loss 0.002165, current_train_items 258272.
I0302 19:01:23.409229 22683591385216 run.py:483] Algo bellman_ford step 8071 current loss 0.038280, current_train_items 258304.
I0302 19:01:23.431784 22683591385216 run.py:483] Algo bellman_ford step 8072 current loss 0.021006, current_train_items 258336.
I0302 19:01:23.461972 22683591385216 run.py:483] Algo bellman_ford step 8073 current loss 0.041485, current_train_items 258368.
I0302 19:01:23.494081 22683591385216 run.py:483] Algo bellman_ford step 8074 current loss 0.034715, current_train_items 258400.
I0302 19:01:23.512968 22683591385216 run.py:483] Algo bellman_ford step 8075 current loss 0.001519, current_train_items 258432.
I0302 19:01:23.528506 22683591385216 run.py:483] Algo bellman_ford step 8076 current loss 0.033442, current_train_items 258464.
I0302 19:01:23.550755 22683591385216 run.py:483] Algo bellman_ford step 8077 current loss 0.005058, current_train_items 258496.
I0302 19:01:23.579860 22683591385216 run.py:483] Algo bellman_ford step 8078 current loss 0.061217, current_train_items 258528.
I0302 19:01:23.609391 22683591385216 run.py:483] Algo bellman_ford step 8079 current loss 0.039885, current_train_items 258560.
I0302 19:01:23.627926 22683591385216 run.py:483] Algo bellman_ford step 8080 current loss 0.005034, current_train_items 258592.
I0302 19:01:23.643287 22683591385216 run.py:483] Algo bellman_ford step 8081 current loss 0.005202, current_train_items 258624.
I0302 19:01:23.665913 22683591385216 run.py:483] Algo bellman_ford step 8082 current loss 0.045956, current_train_items 258656.
I0302 19:01:23.696006 22683591385216 run.py:483] Algo bellman_ford step 8083 current loss 0.052796, current_train_items 258688.
I0302 19:01:23.730500 22683591385216 run.py:483] Algo bellman_ford step 8084 current loss 0.054627, current_train_items 258720.
I0302 19:01:23.749464 22683591385216 run.py:483] Algo bellman_ford step 8085 current loss 0.001428, current_train_items 258752.
I0302 19:01:23.765520 22683591385216 run.py:483] Algo bellman_ford step 8086 current loss 0.013592, current_train_items 258784.
I0302 19:01:23.787533 22683591385216 run.py:483] Algo bellman_ford step 8087 current loss 0.036451, current_train_items 258816.
I0302 19:01:23.818947 22683591385216 run.py:483] Algo bellman_ford step 8088 current loss 0.062498, current_train_items 258848.
I0302 19:01:23.849274 22683591385216 run.py:483] Algo bellman_ford step 8089 current loss 0.054162, current_train_items 258880.
I0302 19:01:23.868024 22683591385216 run.py:483] Algo bellman_ford step 8090 current loss 0.001373, current_train_items 258912.
I0302 19:01:23.883209 22683591385216 run.py:483] Algo bellman_ford step 8091 current loss 0.009342, current_train_items 258944.
I0302 19:01:23.906160 22683591385216 run.py:483] Algo bellman_ford step 8092 current loss 0.016426, current_train_items 258976.
I0302 19:01:23.935424 22683591385216 run.py:483] Algo bellman_ford step 8093 current loss 0.028908, current_train_items 259008.
I0302 19:01:23.967519 22683591385216 run.py:483] Algo bellman_ford step 8094 current loss 0.045118, current_train_items 259040.
I0302 19:01:23.985917 22683591385216 run.py:483] Algo bellman_ford step 8095 current loss 0.004755, current_train_items 259072.
I0302 19:01:24.001021 22683591385216 run.py:483] Algo bellman_ford step 8096 current loss 0.014432, current_train_items 259104.
I0302 19:01:24.024971 22683591385216 run.py:483] Algo bellman_ford step 8097 current loss 0.033031, current_train_items 259136.
I0302 19:01:24.055826 22683591385216 run.py:483] Algo bellman_ford step 8098 current loss 0.027897, current_train_items 259168.
I0302 19:01:24.087057 22683591385216 run.py:483] Algo bellman_ford step 8099 current loss 0.031553, current_train_items 259200.
I0302 19:01:24.105391 22683591385216 run.py:483] Algo bellman_ford step 8100 current loss 0.002749, current_train_items 259232.
I0302 19:01:24.113002 22683591385216 run.py:503] (val) algo bellman_ford step 8100: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 259232, 'step': 8100, 'algorithm': 'bellman_ford'}
I0302 19:01:24.113109 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:01:24.129199 22683591385216 run.py:483] Algo bellman_ford step 8101 current loss 0.007621, current_train_items 259264.
I0302 19:01:24.153230 22683591385216 run.py:483] Algo bellman_ford step 8102 current loss 0.054131, current_train_items 259296.
I0302 19:01:24.183138 22683591385216 run.py:483] Algo bellman_ford step 8103 current loss 0.045098, current_train_items 259328.
I0302 19:01:24.214829 22683591385216 run.py:483] Algo bellman_ford step 8104 current loss 0.058207, current_train_items 259360.
I0302 19:01:24.233199 22683591385216 run.py:483] Algo bellman_ford step 8105 current loss 0.003010, current_train_items 259392.
I0302 19:01:24.248641 22683591385216 run.py:483] Algo bellman_ford step 8106 current loss 0.033243, current_train_items 259424.
I0302 19:01:24.270938 22683591385216 run.py:483] Algo bellman_ford step 8107 current loss 0.082056, current_train_items 259456.
I0302 19:01:24.300042 22683591385216 run.py:483] Algo bellman_ford step 8108 current loss 0.137871, current_train_items 259488.
I0302 19:01:24.332120 22683591385216 run.py:483] Algo bellman_ford step 8109 current loss 0.078711, current_train_items 259520.
I0302 19:01:24.350452 22683591385216 run.py:483] Algo bellman_ford step 8110 current loss 0.022667, current_train_items 259552.
I0302 19:01:24.365313 22683591385216 run.py:483] Algo bellman_ford step 8111 current loss 0.034962, current_train_items 259584.
I0302 19:01:24.387100 22683591385216 run.py:483] Algo bellman_ford step 8112 current loss 0.048328, current_train_items 259616.
I0302 19:01:24.415228 22683591385216 run.py:483] Algo bellman_ford step 8113 current loss 0.028651, current_train_items 259648.
I0302 19:01:24.448200 22683591385216 run.py:483] Algo bellman_ford step 8114 current loss 0.067519, current_train_items 259680.
I0302 19:01:24.466419 22683591385216 run.py:483] Algo bellman_ford step 8115 current loss 0.006746, current_train_items 259712.
I0302 19:01:24.481847 22683591385216 run.py:483] Algo bellman_ford step 8116 current loss 0.002898, current_train_items 259744.
I0302 19:01:24.504754 22683591385216 run.py:483] Algo bellman_ford step 8117 current loss 0.040377, current_train_items 259776.
I0302 19:01:24.534867 22683591385216 run.py:483] Algo bellman_ford step 8118 current loss 0.057200, current_train_items 259808.
I0302 19:01:24.565630 22683591385216 run.py:483] Algo bellman_ford step 8119 current loss 0.053650, current_train_items 259840.
I0302 19:01:24.583578 22683591385216 run.py:483] Algo bellman_ford step 8120 current loss 0.003317, current_train_items 259872.
I0302 19:01:24.599019 22683591385216 run.py:483] Algo bellman_ford step 8121 current loss 0.001780, current_train_items 259904.
I0302 19:01:24.621461 22683591385216 run.py:483] Algo bellman_ford step 8122 current loss 0.064416, current_train_items 259936.
I0302 19:01:24.652172 22683591385216 run.py:483] Algo bellman_ford step 8123 current loss 0.055323, current_train_items 259968.
I0302 19:01:24.681760 22683591385216 run.py:483] Algo bellman_ford step 8124 current loss 0.032361, current_train_items 260000.
I0302 19:01:24.700001 22683591385216 run.py:483] Algo bellman_ford step 8125 current loss 0.001092, current_train_items 260032.
I0302 19:01:24.715574 22683591385216 run.py:483] Algo bellman_ford step 8126 current loss 0.014920, current_train_items 260064.
I0302 19:01:24.738763 22683591385216 run.py:483] Algo bellman_ford step 8127 current loss 0.027665, current_train_items 260096.
I0302 19:01:24.769142 22683591385216 run.py:483] Algo bellman_ford step 8128 current loss 0.048781, current_train_items 260128.
I0302 19:01:24.802968 22683591385216 run.py:483] Algo bellman_ford step 8129 current loss 0.116606, current_train_items 260160.
I0302 19:01:24.821244 22683591385216 run.py:483] Algo bellman_ford step 8130 current loss 0.006295, current_train_items 260192.
I0302 19:01:24.836200 22683591385216 run.py:483] Algo bellman_ford step 8131 current loss 0.014136, current_train_items 260224.
I0302 19:01:24.858644 22683591385216 run.py:483] Algo bellman_ford step 8132 current loss 0.036509, current_train_items 260256.
I0302 19:01:24.888332 22683591385216 run.py:483] Algo bellman_ford step 8133 current loss 0.052688, current_train_items 260288.
I0302 19:01:24.919894 22683591385216 run.py:483] Algo bellman_ford step 8134 current loss 0.051583, current_train_items 260320.
I0302 19:01:24.937960 22683591385216 run.py:483] Algo bellman_ford step 8135 current loss 0.001811, current_train_items 260352.
I0302 19:01:24.952948 22683591385216 run.py:483] Algo bellman_ford step 8136 current loss 0.010738, current_train_items 260384.
I0302 19:01:24.975059 22683591385216 run.py:483] Algo bellman_ford step 8137 current loss 0.084558, current_train_items 260416.
I0302 19:01:25.005043 22683591385216 run.py:483] Algo bellman_ford step 8138 current loss 0.066112, current_train_items 260448.
I0302 19:01:25.038119 22683591385216 run.py:483] Algo bellman_ford step 8139 current loss 0.104034, current_train_items 260480.
I0302 19:01:25.056079 22683591385216 run.py:483] Algo bellman_ford step 8140 current loss 0.001836, current_train_items 260512.
I0302 19:01:25.071154 22683591385216 run.py:483] Algo bellman_ford step 8141 current loss 0.025577, current_train_items 260544.
I0302 19:01:25.094260 22683591385216 run.py:483] Algo bellman_ford step 8142 current loss 0.065759, current_train_items 260576.
I0302 19:01:25.124457 22683591385216 run.py:483] Algo bellman_ford step 8143 current loss 0.063415, current_train_items 260608.
I0302 19:01:25.156552 22683591385216 run.py:483] Algo bellman_ford step 8144 current loss 0.084907, current_train_items 260640.
I0302 19:01:25.174585 22683591385216 run.py:483] Algo bellman_ford step 8145 current loss 0.004108, current_train_items 260672.
I0302 19:01:25.190124 22683591385216 run.py:483] Algo bellman_ford step 8146 current loss 0.014974, current_train_items 260704.
I0302 19:01:25.212469 22683591385216 run.py:483] Algo bellman_ford step 8147 current loss 0.056693, current_train_items 260736.
I0302 19:01:25.242462 22683591385216 run.py:483] Algo bellman_ford step 8148 current loss 0.078810, current_train_items 260768.
I0302 19:01:25.273056 22683591385216 run.py:483] Algo bellman_ford step 8149 current loss 0.048973, current_train_items 260800.
I0302 19:01:25.291300 22683591385216 run.py:483] Algo bellman_ford step 8150 current loss 0.002059, current_train_items 260832.
I0302 19:01:25.299222 22683591385216 run.py:503] (val) algo bellman_ford step 8150: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 260832, 'step': 8150, 'algorithm': 'bellman_ford'}
I0302 19:01:25.299326 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:01:25.315178 22683591385216 run.py:483] Algo bellman_ford step 8151 current loss 0.009083, current_train_items 260864.
I0302 19:01:25.338451 22683591385216 run.py:483] Algo bellman_ford step 8152 current loss 0.038040, current_train_items 260896.
I0302 19:01:25.367111 22683591385216 run.py:483] Algo bellman_ford step 8153 current loss 0.014047, current_train_items 260928.
I0302 19:01:25.400215 22683591385216 run.py:483] Algo bellman_ford step 8154 current loss 0.070897, current_train_items 260960.
I0302 19:01:25.419255 22683591385216 run.py:483] Algo bellman_ford step 8155 current loss 0.002113, current_train_items 260992.
I0302 19:01:25.434113 22683591385216 run.py:483] Algo bellman_ford step 8156 current loss 0.021930, current_train_items 261024.
I0302 19:01:25.457086 22683591385216 run.py:483] Algo bellman_ford step 8157 current loss 0.016276, current_train_items 261056.
I0302 19:01:25.486449 22683591385216 run.py:483] Algo bellman_ford step 8158 current loss 0.054367, current_train_items 261088.
I0302 19:01:25.517366 22683591385216 run.py:483] Algo bellman_ford step 8159 current loss 0.091459, current_train_items 261120.
I0302 19:01:25.535885 22683591385216 run.py:483] Algo bellman_ford step 8160 current loss 0.002671, current_train_items 261152.
I0302 19:01:25.551685 22683591385216 run.py:483] Algo bellman_ford step 8161 current loss 0.006562, current_train_items 261184.
I0302 19:01:25.573872 22683591385216 run.py:483] Algo bellman_ford step 8162 current loss 0.036524, current_train_items 261216.
I0302 19:01:25.604190 22683591385216 run.py:483] Algo bellman_ford step 8163 current loss 0.038263, current_train_items 261248.
I0302 19:01:25.632781 22683591385216 run.py:483] Algo bellman_ford step 8164 current loss 0.015314, current_train_items 261280.
I0302 19:01:25.650881 22683591385216 run.py:483] Algo bellman_ford step 8165 current loss 0.002153, current_train_items 261312.
I0302 19:01:25.666260 22683591385216 run.py:483] Algo bellman_ford step 8166 current loss 0.015322, current_train_items 261344.
I0302 19:01:25.688844 22683591385216 run.py:483] Algo bellman_ford step 8167 current loss 0.032294, current_train_items 261376.
I0302 19:01:25.717916 22683591385216 run.py:483] Algo bellman_ford step 8168 current loss 0.030641, current_train_items 261408.
I0302 19:01:25.750406 22683591385216 run.py:483] Algo bellman_ford step 8169 current loss 0.056998, current_train_items 261440.
I0302 19:01:25.769104 22683591385216 run.py:483] Algo bellman_ford step 8170 current loss 0.001259, current_train_items 261472.
I0302 19:01:25.784744 22683591385216 run.py:483] Algo bellman_ford step 8171 current loss 0.003540, current_train_items 261504.
I0302 19:01:25.807548 22683591385216 run.py:483] Algo bellman_ford step 8172 current loss 0.047382, current_train_items 261536.
I0302 19:01:25.837741 22683591385216 run.py:483] Algo bellman_ford step 8173 current loss 0.049754, current_train_items 261568.
I0302 19:01:25.868704 22683591385216 run.py:483] Algo bellman_ford step 8174 current loss 0.029438, current_train_items 261600.
I0302 19:01:25.887632 22683591385216 run.py:483] Algo bellman_ford step 8175 current loss 0.001098, current_train_items 261632.
I0302 19:01:25.903632 22683591385216 run.py:483] Algo bellman_ford step 8176 current loss 0.021156, current_train_items 261664.
I0302 19:01:25.925590 22683591385216 run.py:483] Algo bellman_ford step 8177 current loss 0.038974, current_train_items 261696.
I0302 19:01:25.956593 22683591385216 run.py:483] Algo bellman_ford step 8178 current loss 0.027126, current_train_items 261728.
I0302 19:01:25.985505 22683591385216 run.py:483] Algo bellman_ford step 8179 current loss 0.032520, current_train_items 261760.
I0302 19:01:26.003706 22683591385216 run.py:483] Algo bellman_ford step 8180 current loss 0.000711, current_train_items 261792.
I0302 19:01:26.019004 22683591385216 run.py:483] Algo bellman_ford step 8181 current loss 0.008906, current_train_items 261824.
I0302 19:01:26.041335 22683591385216 run.py:483] Algo bellman_ford step 8182 current loss 0.013128, current_train_items 261856.
I0302 19:01:26.071753 22683591385216 run.py:483] Algo bellman_ford step 8183 current loss 0.033060, current_train_items 261888.
I0302 19:01:26.105370 22683591385216 run.py:483] Algo bellman_ford step 8184 current loss 0.073431, current_train_items 261920.
I0302 19:01:26.124056 22683591385216 run.py:483] Algo bellman_ford step 8185 current loss 0.002619, current_train_items 261952.
I0302 19:01:26.139568 22683591385216 run.py:483] Algo bellman_ford step 8186 current loss 0.007601, current_train_items 261984.
I0302 19:01:26.162262 22683591385216 run.py:483] Algo bellman_ford step 8187 current loss 0.024240, current_train_items 262016.
I0302 19:01:26.192148 22683591385216 run.py:483] Algo bellman_ford step 8188 current loss 0.039049, current_train_items 262048.
I0302 19:01:26.223079 22683591385216 run.py:483] Algo bellman_ford step 8189 current loss 0.040763, current_train_items 262080.
I0302 19:01:26.241737 22683591385216 run.py:483] Algo bellman_ford step 8190 current loss 0.011817, current_train_items 262112.
I0302 19:01:26.257477 22683591385216 run.py:483] Algo bellman_ford step 8191 current loss 0.012269, current_train_items 262144.
I0302 19:01:26.280119 22683591385216 run.py:483] Algo bellman_ford step 8192 current loss 0.038880, current_train_items 262176.
I0302 19:01:26.309840 22683591385216 run.py:483] Algo bellman_ford step 8193 current loss 0.041561, current_train_items 262208.
I0302 19:01:26.341054 22683591385216 run.py:483] Algo bellman_ford step 8194 current loss 0.046208, current_train_items 262240.
I0302 19:01:26.359223 22683591385216 run.py:483] Algo bellman_ford step 8195 current loss 0.002817, current_train_items 262272.
I0302 19:01:26.374567 22683591385216 run.py:483] Algo bellman_ford step 8196 current loss 0.017850, current_train_items 262304.
I0302 19:01:26.396780 22683591385216 run.py:483] Algo bellman_ford step 8197 current loss 0.076541, current_train_items 262336.
I0302 19:01:26.426423 22683591385216 run.py:483] Algo bellman_ford step 8198 current loss 0.040245, current_train_items 262368.
I0302 19:01:26.457401 22683591385216 run.py:483] Algo bellman_ford step 8199 current loss 0.067644, current_train_items 262400.
I0302 19:01:26.476125 22683591385216 run.py:483] Algo bellman_ford step 8200 current loss 0.003246, current_train_items 262432.
I0302 19:01:26.483628 22683591385216 run.py:503] (val) algo bellman_ford step 8200: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 262432, 'step': 8200, 'algorithm': 'bellman_ford'}
I0302 19:01:26.483736 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:26.499303 22683591385216 run.py:483] Algo bellman_ford step 8201 current loss 0.006820, current_train_items 262464.
I0302 19:01:26.522878 22683591385216 run.py:483] Algo bellman_ford step 8202 current loss 0.036330, current_train_items 262496.
I0302 19:01:26.552651 22683591385216 run.py:483] Algo bellman_ford step 8203 current loss 0.029061, current_train_items 262528.
I0302 19:01:26.585515 22683591385216 run.py:483] Algo bellman_ford step 8204 current loss 0.086917, current_train_items 262560.
I0302 19:01:26.604296 22683591385216 run.py:483] Algo bellman_ford step 8205 current loss 0.002035, current_train_items 262592.
I0302 19:01:26.620018 22683591385216 run.py:483] Algo bellman_ford step 8206 current loss 0.004137, current_train_items 262624.
I0302 19:01:26.643507 22683591385216 run.py:483] Algo bellman_ford step 8207 current loss 0.034165, current_train_items 262656.
I0302 19:01:26.673193 22683591385216 run.py:483] Algo bellman_ford step 8208 current loss 0.040673, current_train_items 262688.
I0302 19:01:26.702257 22683591385216 run.py:483] Algo bellman_ford step 8209 current loss 0.042452, current_train_items 262720.
I0302 19:01:26.720683 22683591385216 run.py:483] Algo bellman_ford step 8210 current loss 0.001809, current_train_items 262752.
I0302 19:01:26.735939 22683591385216 run.py:483] Algo bellman_ford step 8211 current loss 0.055760, current_train_items 262784.
I0302 19:01:26.757820 22683591385216 run.py:483] Algo bellman_ford step 8212 current loss 0.043896, current_train_items 262816.
I0302 19:01:26.787396 22683591385216 run.py:483] Algo bellman_ford step 8213 current loss 0.073256, current_train_items 262848.
I0302 19:01:26.818913 22683591385216 run.py:483] Algo bellman_ford step 8214 current loss 0.097467, current_train_items 262880.
I0302 19:01:26.837077 22683591385216 run.py:483] Algo bellman_ford step 8215 current loss 0.001177, current_train_items 262912.
I0302 19:01:26.851870 22683591385216 run.py:483] Algo bellman_ford step 8216 current loss 0.003776, current_train_items 262944.
I0302 19:01:26.873499 22683591385216 run.py:483] Algo bellman_ford step 8217 current loss 0.042313, current_train_items 262976.
I0302 19:01:26.904464 22683591385216 run.py:483] Algo bellman_ford step 8218 current loss 0.065284, current_train_items 263008.
I0302 19:01:26.936292 22683591385216 run.py:483] Algo bellman_ford step 8219 current loss 0.045875, current_train_items 263040.
I0302 19:01:26.954273 22683591385216 run.py:483] Algo bellman_ford step 8220 current loss 0.002979, current_train_items 263072.
I0302 19:01:26.969397 22683591385216 run.py:483] Algo bellman_ford step 8221 current loss 0.020111, current_train_items 263104.
I0302 19:01:26.992590 22683591385216 run.py:483] Algo bellman_ford step 8222 current loss 0.034860, current_train_items 263136.
I0302 19:01:27.022812 22683591385216 run.py:483] Algo bellman_ford step 8223 current loss 0.079442, current_train_items 263168.
I0302 19:01:27.052281 22683591385216 run.py:483] Algo bellman_ford step 8224 current loss 0.040073, current_train_items 263200.
I0302 19:01:27.070808 22683591385216 run.py:483] Algo bellman_ford step 8225 current loss 0.005352, current_train_items 263232.
I0302 19:01:27.086356 22683591385216 run.py:483] Algo bellman_ford step 8226 current loss 0.016746, current_train_items 263264.
I0302 19:01:27.109670 22683591385216 run.py:483] Algo bellman_ford step 8227 current loss 0.012132, current_train_items 263296.
I0302 19:01:27.138632 22683591385216 run.py:483] Algo bellman_ford step 8228 current loss 0.056929, current_train_items 263328.
I0302 19:01:27.170232 22683591385216 run.py:483] Algo bellman_ford step 8229 current loss 0.043911, current_train_items 263360.
I0302 19:01:27.188667 22683591385216 run.py:483] Algo bellman_ford step 8230 current loss 0.001877, current_train_items 263392.
I0302 19:01:27.204032 22683591385216 run.py:483] Algo bellman_ford step 8231 current loss 0.014978, current_train_items 263424.
I0302 19:01:27.227212 22683591385216 run.py:483] Algo bellman_ford step 8232 current loss 0.017733, current_train_items 263456.
I0302 19:01:27.256674 22683591385216 run.py:483] Algo bellman_ford step 8233 current loss 0.027908, current_train_items 263488.
I0302 19:01:27.287888 22683591385216 run.py:483] Algo bellman_ford step 8234 current loss 0.040178, current_train_items 263520.
I0302 19:01:27.305920 22683591385216 run.py:483] Algo bellman_ford step 8235 current loss 0.002384, current_train_items 263552.
I0302 19:01:27.321714 22683591385216 run.py:483] Algo bellman_ford step 8236 current loss 0.009011, current_train_items 263584.
I0302 19:01:27.345038 22683591385216 run.py:483] Algo bellman_ford step 8237 current loss 0.027533, current_train_items 263616.
I0302 19:01:27.375617 22683591385216 run.py:483] Algo bellman_ford step 8238 current loss 0.035105, current_train_items 263648.
I0302 19:01:27.406661 22683591385216 run.py:483] Algo bellman_ford step 8239 current loss 0.046614, current_train_items 263680.
I0302 19:01:27.425019 22683591385216 run.py:483] Algo bellman_ford step 8240 current loss 0.003076, current_train_items 263712.
I0302 19:01:27.440376 22683591385216 run.py:483] Algo bellman_ford step 8241 current loss 0.004920, current_train_items 263744.
I0302 19:01:27.462207 22683591385216 run.py:483] Algo bellman_ford step 8242 current loss 0.026929, current_train_items 263776.
I0302 19:01:27.491468 22683591385216 run.py:483] Algo bellman_ford step 8243 current loss 0.043438, current_train_items 263808.
I0302 19:01:27.524642 22683591385216 run.py:483] Algo bellman_ford step 8244 current loss 0.045500, current_train_items 263840.
I0302 19:01:27.543030 22683591385216 run.py:483] Algo bellman_ford step 8245 current loss 0.016897, current_train_items 263872.
I0302 19:01:27.558354 22683591385216 run.py:483] Algo bellman_ford step 8246 current loss 0.016883, current_train_items 263904.
I0302 19:01:27.579894 22683591385216 run.py:483] Algo bellman_ford step 8247 current loss 0.021898, current_train_items 263936.
I0302 19:01:27.609290 22683591385216 run.py:483] Algo bellman_ford step 8248 current loss 0.027502, current_train_items 263968.
I0302 19:01:27.641126 22683591385216 run.py:483] Algo bellman_ford step 8249 current loss 0.057321, current_train_items 264000.
I0302 19:01:27.659472 22683591385216 run.py:483] Algo bellman_ford step 8250 current loss 0.001282, current_train_items 264032.
I0302 19:01:27.667103 22683591385216 run.py:503] (val) algo bellman_ford step 8250: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 264032, 'step': 8250, 'algorithm': 'bellman_ford'}
I0302 19:01:27.667210 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:27.683100 22683591385216 run.py:483] Algo bellman_ford step 8251 current loss 0.012100, current_train_items 264064.
I0302 19:01:27.706169 22683591385216 run.py:483] Algo bellman_ford step 8252 current loss 0.033893, current_train_items 264096.
I0302 19:01:27.738139 22683591385216 run.py:483] Algo bellman_ford step 8253 current loss 0.030431, current_train_items 264128.
I0302 19:01:27.768919 22683591385216 run.py:483] Algo bellman_ford step 8254 current loss 0.034275, current_train_items 264160.
I0302 19:01:27.787341 22683591385216 run.py:483] Algo bellman_ford step 8255 current loss 0.024031, current_train_items 264192.
I0302 19:01:27.803103 22683591385216 run.py:483] Algo bellman_ford step 8256 current loss 0.032742, current_train_items 264224.
I0302 19:01:27.826401 22683591385216 run.py:483] Algo bellman_ford step 8257 current loss 0.026369, current_train_items 264256.
I0302 19:01:27.856817 22683591385216 run.py:483] Algo bellman_ford step 8258 current loss 0.061295, current_train_items 264288.
I0302 19:01:27.889771 22683591385216 run.py:483] Algo bellman_ford step 8259 current loss 0.061338, current_train_items 264320.
I0302 19:01:27.908432 22683591385216 run.py:483] Algo bellman_ford step 8260 current loss 0.009026, current_train_items 264352.
I0302 19:01:27.924307 22683591385216 run.py:483] Algo bellman_ford step 8261 current loss 0.006714, current_train_items 264384.
I0302 19:01:27.947455 22683591385216 run.py:483] Algo bellman_ford step 8262 current loss 0.051346, current_train_items 264416.
I0302 19:01:27.978071 22683591385216 run.py:483] Algo bellman_ford step 8263 current loss 0.027146, current_train_items 264448.
I0302 19:01:28.011061 22683591385216 run.py:483] Algo bellman_ford step 8264 current loss 0.029835, current_train_items 264480.
I0302 19:01:28.029271 22683591385216 run.py:483] Algo bellman_ford step 8265 current loss 0.007569, current_train_items 264512.
I0302 19:01:28.044244 22683591385216 run.py:483] Algo bellman_ford step 8266 current loss 0.010492, current_train_items 264544.
I0302 19:01:28.066729 22683591385216 run.py:483] Algo bellman_ford step 8267 current loss 0.027515, current_train_items 264576.
I0302 19:01:28.097313 22683591385216 run.py:483] Algo bellman_ford step 8268 current loss 0.044258, current_train_items 264608.
I0302 19:01:28.127378 22683591385216 run.py:483] Algo bellman_ford step 8269 current loss 0.072723, current_train_items 264640.
I0302 19:01:28.146399 22683591385216 run.py:483] Algo bellman_ford step 8270 current loss 0.001847, current_train_items 264672.
I0302 19:01:28.161527 22683591385216 run.py:483] Algo bellman_ford step 8271 current loss 0.004222, current_train_items 264704.
I0302 19:01:28.183756 22683591385216 run.py:483] Algo bellman_ford step 8272 current loss 0.016863, current_train_items 264736.
I0302 19:01:28.212805 22683591385216 run.py:483] Algo bellman_ford step 8273 current loss 0.023670, current_train_items 264768.
I0302 19:01:28.245407 22683591385216 run.py:483] Algo bellman_ford step 8274 current loss 0.068857, current_train_items 264800.
I0302 19:01:28.264338 22683591385216 run.py:483] Algo bellman_ford step 8275 current loss 0.001396, current_train_items 264832.
I0302 19:01:28.280436 22683591385216 run.py:483] Algo bellman_ford step 8276 current loss 0.034900, current_train_items 264864.
I0302 19:01:28.303586 22683591385216 run.py:483] Algo bellman_ford step 8277 current loss 0.022108, current_train_items 264896.
I0302 19:01:28.333175 22683591385216 run.py:483] Algo bellman_ford step 8278 current loss 0.047911, current_train_items 264928.
I0302 19:01:28.363805 22683591385216 run.py:483] Algo bellman_ford step 8279 current loss 0.035866, current_train_items 264960.
I0302 19:01:28.381884 22683591385216 run.py:483] Algo bellman_ford step 8280 current loss 0.003961, current_train_items 264992.
I0302 19:01:28.397581 22683591385216 run.py:483] Algo bellman_ford step 8281 current loss 0.010736, current_train_items 265024.
I0302 19:01:28.419720 22683591385216 run.py:483] Algo bellman_ford step 8282 current loss 0.018513, current_train_items 265056.
I0302 19:01:28.449043 22683591385216 run.py:483] Algo bellman_ford step 8283 current loss 0.030055, current_train_items 265088.
I0302 19:01:28.479612 22683591385216 run.py:483] Algo bellman_ford step 8284 current loss 0.037369, current_train_items 265120.
I0302 19:01:28.498647 22683591385216 run.py:483] Algo bellman_ford step 8285 current loss 0.003124, current_train_items 265152.
I0302 19:01:28.514564 22683591385216 run.py:483] Algo bellman_ford step 8286 current loss 0.027316, current_train_items 265184.
I0302 19:01:28.536744 22683591385216 run.py:483] Algo bellman_ford step 8287 current loss 0.030759, current_train_items 265216.
I0302 19:01:28.567975 22683591385216 run.py:483] Algo bellman_ford step 8288 current loss 0.057109, current_train_items 265248.
I0302 19:01:28.599714 22683591385216 run.py:483] Algo bellman_ford step 8289 current loss 0.029435, current_train_items 265280.
I0302 19:01:28.618364 22683591385216 run.py:483] Algo bellman_ford step 8290 current loss 0.001763, current_train_items 265312.
I0302 19:01:28.634052 22683591385216 run.py:483] Algo bellman_ford step 8291 current loss 0.012088, current_train_items 265344.
I0302 19:01:28.657258 22683591385216 run.py:483] Algo bellman_ford step 8292 current loss 0.035430, current_train_items 265376.
I0302 19:01:28.685577 22683591385216 run.py:483] Algo bellman_ford step 8293 current loss 0.030774, current_train_items 265408.
I0302 19:01:28.718893 22683591385216 run.py:483] Algo bellman_ford step 8294 current loss 0.050242, current_train_items 265440.
I0302 19:01:28.737563 22683591385216 run.py:483] Algo bellman_ford step 8295 current loss 0.009646, current_train_items 265472.
I0302 19:01:28.753136 22683591385216 run.py:483] Algo bellman_ford step 8296 current loss 0.015831, current_train_items 265504.
I0302 19:01:28.776844 22683591385216 run.py:483] Algo bellman_ford step 8297 current loss 0.067122, current_train_items 265536.
I0302 19:01:28.807025 22683591385216 run.py:483] Algo bellman_ford step 8298 current loss 0.081674, current_train_items 265568.
I0302 19:01:28.837709 22683591385216 run.py:483] Algo bellman_ford step 8299 current loss 0.050784, current_train_items 265600.
I0302 19:01:28.856425 22683591385216 run.py:483] Algo bellman_ford step 8300 current loss 0.014895, current_train_items 265632.
I0302 19:01:28.864035 22683591385216 run.py:503] (val) algo bellman_ford step 8300: {'pi': 0.9892578125, 'score': 0.9892578125, 'examples_seen': 265632, 'step': 8300, 'algorithm': 'bellman_ford'}
I0302 19:01:28.864139 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.989, val scores are: bellman_ford: 0.989
I0302 19:01:28.879796 22683591385216 run.py:483] Algo bellman_ford step 8301 current loss 0.011239, current_train_items 265664.
I0302 19:01:28.904477 22683591385216 run.py:483] Algo bellman_ford step 8302 current loss 0.059243, current_train_items 265696.
I0302 19:01:28.935464 22683591385216 run.py:483] Algo bellman_ford step 8303 current loss 0.041189, current_train_items 265728.
I0302 19:01:28.966857 22683591385216 run.py:483] Algo bellman_ford step 8304 current loss 0.067385, current_train_items 265760.
I0302 19:01:28.985424 22683591385216 run.py:483] Algo bellman_ford step 8305 current loss 0.002209, current_train_items 265792.
I0302 19:01:29.000554 22683591385216 run.py:483] Algo bellman_ford step 8306 current loss 0.007294, current_train_items 265824.
I0302 19:01:29.023649 22683591385216 run.py:483] Algo bellman_ford step 8307 current loss 0.044254, current_train_items 265856.
I0302 19:01:29.051482 22683591385216 run.py:483] Algo bellman_ford step 8308 current loss 0.021947, current_train_items 265888.
I0302 19:01:29.084773 22683591385216 run.py:483] Algo bellman_ford step 8309 current loss 0.108269, current_train_items 265920.
I0302 19:01:29.103363 22683591385216 run.py:483] Algo bellman_ford step 8310 current loss 0.003817, current_train_items 265952.
I0302 19:01:29.118674 22683591385216 run.py:483] Algo bellman_ford step 8311 current loss 0.006118, current_train_items 265984.
I0302 19:01:29.141691 22683591385216 run.py:483] Algo bellman_ford step 8312 current loss 0.074059, current_train_items 266016.
I0302 19:01:29.171044 22683591385216 run.py:483] Algo bellman_ford step 8313 current loss 0.039923, current_train_items 266048.
I0302 19:01:29.200968 22683591385216 run.py:483] Algo bellman_ford step 8314 current loss 0.056787, current_train_items 266080.
I0302 19:01:29.219376 22683591385216 run.py:483] Algo bellman_ford step 8315 current loss 0.002566, current_train_items 266112.
I0302 19:01:29.235030 22683591385216 run.py:483] Algo bellman_ford step 8316 current loss 0.022078, current_train_items 266144.
I0302 19:01:29.257629 22683591385216 run.py:483] Algo bellman_ford step 8317 current loss 0.037649, current_train_items 266176.
I0302 19:01:29.285705 22683591385216 run.py:483] Algo bellman_ford step 8318 current loss 0.025345, current_train_items 266208.
I0302 19:01:29.316872 22683591385216 run.py:483] Algo bellman_ford step 8319 current loss 0.112041, current_train_items 266240.
I0302 19:01:29.335021 22683591385216 run.py:483] Algo bellman_ford step 8320 current loss 0.002146, current_train_items 266272.
I0302 19:01:29.350336 22683591385216 run.py:483] Algo bellman_ford step 8321 current loss 0.021048, current_train_items 266304.
I0302 19:01:29.373884 22683591385216 run.py:483] Algo bellman_ford step 8322 current loss 0.024577, current_train_items 266336.
I0302 19:01:29.403520 22683591385216 run.py:483] Algo bellman_ford step 8323 current loss 0.045657, current_train_items 266368.
I0302 19:01:29.436494 22683591385216 run.py:483] Algo bellman_ford step 8324 current loss 0.045380, current_train_items 266400.
I0302 19:01:29.454950 22683591385216 run.py:483] Algo bellman_ford step 8325 current loss 0.004656, current_train_items 266432.
I0302 19:01:29.469807 22683591385216 run.py:483] Algo bellman_ford step 8326 current loss 0.011833, current_train_items 266464.
I0302 19:01:29.492410 22683591385216 run.py:483] Algo bellman_ford step 8327 current loss 0.015137, current_train_items 266496.
I0302 19:01:29.521245 22683591385216 run.py:483] Algo bellman_ford step 8328 current loss 0.036880, current_train_items 266528.
I0302 19:01:29.553493 22683591385216 run.py:483] Algo bellman_ford step 8329 current loss 0.044602, current_train_items 266560.
I0302 19:01:29.572087 22683591385216 run.py:483] Algo bellman_ford step 8330 current loss 0.001981, current_train_items 266592.
I0302 19:01:29.587293 22683591385216 run.py:483] Algo bellman_ford step 8331 current loss 0.038526, current_train_items 266624.
I0302 19:01:29.609083 22683591385216 run.py:483] Algo bellman_ford step 8332 current loss 0.017598, current_train_items 266656.
I0302 19:01:29.639314 22683591385216 run.py:483] Algo bellman_ford step 8333 current loss 0.049153, current_train_items 266688.
I0302 19:01:29.670192 22683591385216 run.py:483] Algo bellman_ford step 8334 current loss 0.051711, current_train_items 266720.
I0302 19:01:29.688582 22683591385216 run.py:483] Algo bellman_ford step 8335 current loss 0.004356, current_train_items 266752.
I0302 19:01:29.703877 22683591385216 run.py:483] Algo bellman_ford step 8336 current loss 0.015441, current_train_items 266784.
I0302 19:01:29.727707 22683591385216 run.py:483] Algo bellman_ford step 8337 current loss 0.019518, current_train_items 266816.
I0302 19:01:29.757364 22683591385216 run.py:483] Algo bellman_ford step 8338 current loss 0.076039, current_train_items 266848.
I0302 19:01:29.788127 22683591385216 run.py:483] Algo bellman_ford step 8339 current loss 0.051470, current_train_items 266880.
I0302 19:01:29.806658 22683591385216 run.py:483] Algo bellman_ford step 8340 current loss 0.001233, current_train_items 266912.
I0302 19:01:29.821867 22683591385216 run.py:483] Algo bellman_ford step 8341 current loss 0.004130, current_train_items 266944.
I0302 19:01:29.843563 22683591385216 run.py:483] Algo bellman_ford step 8342 current loss 0.048886, current_train_items 266976.
I0302 19:01:29.872349 22683591385216 run.py:483] Algo bellman_ford step 8343 current loss 0.137678, current_train_items 267008.
I0302 19:01:29.905079 22683591385216 run.py:483] Algo bellman_ford step 8344 current loss 0.071128, current_train_items 267040.
I0302 19:01:29.923244 22683591385216 run.py:483] Algo bellman_ford step 8345 current loss 0.001874, current_train_items 267072.
I0302 19:01:29.938421 22683591385216 run.py:483] Algo bellman_ford step 8346 current loss 0.002410, current_train_items 267104.
I0302 19:01:29.962237 22683591385216 run.py:483] Algo bellman_ford step 8347 current loss 0.062553, current_train_items 267136.
I0302 19:01:29.992508 22683591385216 run.py:483] Algo bellman_ford step 8348 current loss 0.027453, current_train_items 267168.
I0302 19:01:30.025002 22683591385216 run.py:483] Algo bellman_ford step 8349 current loss 0.063226, current_train_items 267200.
I0302 19:01:30.043317 22683591385216 run.py:483] Algo bellman_ford step 8350 current loss 0.002670, current_train_items 267232.
I0302 19:01:30.050995 22683591385216 run.py:503] (val) algo bellman_ford step 8350: {'pi': 0.982421875, 'score': 0.982421875, 'examples_seen': 267232, 'step': 8350, 'algorithm': 'bellman_ford'}
I0302 19:01:30.051124 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.982, val scores are: bellman_ford: 0.982
I0302 19:01:30.067448 22683591385216 run.py:483] Algo bellman_ford step 8351 current loss 0.011952, current_train_items 267264.
I0302 19:01:30.090822 22683591385216 run.py:483] Algo bellman_ford step 8352 current loss 0.058998, current_train_items 267296.
I0302 19:01:30.121330 22683591385216 run.py:483] Algo bellman_ford step 8353 current loss 0.030297, current_train_items 267328.
I0302 19:01:30.155039 22683591385216 run.py:483] Algo bellman_ford step 8354 current loss 0.115039, current_train_items 267360.
I0302 19:01:30.173504 22683591385216 run.py:483] Algo bellman_ford step 8355 current loss 0.002635, current_train_items 267392.
I0302 19:01:30.188496 22683591385216 run.py:483] Algo bellman_ford step 8356 current loss 0.038560, current_train_items 267424.
I0302 19:01:30.210985 22683591385216 run.py:483] Algo bellman_ford step 8357 current loss 0.023574, current_train_items 267456.
I0302 19:01:30.240947 22683591385216 run.py:483] Algo bellman_ford step 8358 current loss 0.047516, current_train_items 267488.
I0302 19:01:30.271829 22683591385216 run.py:483] Algo bellman_ford step 8359 current loss 0.060705, current_train_items 267520.
I0302 19:01:30.290773 22683591385216 run.py:483] Algo bellman_ford step 8360 current loss 0.002595, current_train_items 267552.
I0302 19:01:30.306162 22683591385216 run.py:483] Algo bellman_ford step 8361 current loss 0.004054, current_train_items 267584.
I0302 19:01:30.329084 22683591385216 run.py:483] Algo bellman_ford step 8362 current loss 0.022002, current_train_items 267616.
I0302 19:01:30.358328 22683591385216 run.py:483] Algo bellman_ford step 8363 current loss 0.043416, current_train_items 267648.
I0302 19:01:30.390433 22683591385216 run.py:483] Algo bellman_ford step 8364 current loss 0.097286, current_train_items 267680.
I0302 19:01:30.408819 22683591385216 run.py:483] Algo bellman_ford step 8365 current loss 0.003358, current_train_items 267712.
I0302 19:01:30.424489 22683591385216 run.py:483] Algo bellman_ford step 8366 current loss 0.003114, current_train_items 267744.
I0302 19:01:30.446379 22683591385216 run.py:483] Algo bellman_ford step 8367 current loss 0.014037, current_train_items 267776.
I0302 19:01:30.475880 22683591385216 run.py:483] Algo bellman_ford step 8368 current loss 0.044582, current_train_items 267808.
I0302 19:01:30.509417 22683591385216 run.py:483] Algo bellman_ford step 8369 current loss 0.060341, current_train_items 267840.
I0302 19:01:30.528419 22683591385216 run.py:483] Algo bellman_ford step 8370 current loss 0.001390, current_train_items 267872.
I0302 19:01:30.544259 22683591385216 run.py:483] Algo bellman_ford step 8371 current loss 0.033676, current_train_items 267904.
I0302 19:01:30.565968 22683591385216 run.py:483] Algo bellman_ford step 8372 current loss 0.051673, current_train_items 267936.
I0302 19:01:30.595500 22683591385216 run.py:483] Algo bellman_ford step 8373 current loss 0.030556, current_train_items 267968.
I0302 19:01:30.627735 22683591385216 run.py:483] Algo bellman_ford step 8374 current loss 0.017364, current_train_items 268000.
I0302 19:01:30.646554 22683591385216 run.py:483] Algo bellman_ford step 8375 current loss 0.001366, current_train_items 268032.
I0302 19:01:30.661827 22683591385216 run.py:483] Algo bellman_ford step 8376 current loss 0.004276, current_train_items 268064.
I0302 19:01:30.683656 22683591385216 run.py:483] Algo bellman_ford step 8377 current loss 0.028218, current_train_items 268096.
I0302 19:01:30.713693 22683591385216 run.py:483] Algo bellman_ford step 8378 current loss 0.042246, current_train_items 268128.
I0302 19:01:30.745383 22683591385216 run.py:483] Algo bellman_ford step 8379 current loss 0.063700, current_train_items 268160.
I0302 19:01:30.763777 22683591385216 run.py:483] Algo bellman_ford step 8380 current loss 0.032669, current_train_items 268192.
I0302 19:01:30.779217 22683591385216 run.py:483] Algo bellman_ford step 8381 current loss 0.011925, current_train_items 268224.
I0302 19:01:30.800904 22683591385216 run.py:483] Algo bellman_ford step 8382 current loss 0.042565, current_train_items 268256.
I0302 19:01:30.831476 22683591385216 run.py:483] Algo bellman_ford step 8383 current loss 0.062972, current_train_items 268288.
I0302 19:01:30.861593 22683591385216 run.py:483] Algo bellman_ford step 8384 current loss 0.030121, current_train_items 268320.
I0302 19:01:30.880230 22683591385216 run.py:483] Algo bellman_ford step 8385 current loss 0.001920, current_train_items 268352.
I0302 19:01:30.895447 22683591385216 run.py:483] Algo bellman_ford step 8386 current loss 0.003425, current_train_items 268384.
I0302 19:01:30.917570 22683591385216 run.py:483] Algo bellman_ford step 8387 current loss 0.019225, current_train_items 268416.
I0302 19:01:30.947387 22683591385216 run.py:483] Algo bellman_ford step 8388 current loss 0.031378, current_train_items 268448.
I0302 19:01:30.978789 22683591385216 run.py:483] Algo bellman_ford step 8389 current loss 0.048795, current_train_items 268480.
I0302 19:01:30.997561 22683591385216 run.py:483] Algo bellman_ford step 8390 current loss 0.001197, current_train_items 268512.
I0302 19:01:31.013287 22683591385216 run.py:483] Algo bellman_ford step 8391 current loss 0.002485, current_train_items 268544.
I0302 19:01:31.034607 22683591385216 run.py:483] Algo bellman_ford step 8392 current loss 0.018804, current_train_items 268576.
I0302 19:01:31.065659 22683591385216 run.py:483] Algo bellman_ford step 8393 current loss 0.057758, current_train_items 268608.
I0302 19:01:31.096040 22683591385216 run.py:483] Algo bellman_ford step 8394 current loss 0.030191, current_train_items 268640.
I0302 19:01:31.114402 22683591385216 run.py:483] Algo bellman_ford step 8395 current loss 0.001796, current_train_items 268672.
I0302 19:01:31.129786 22683591385216 run.py:483] Algo bellman_ford step 8396 current loss 0.031753, current_train_items 268704.
I0302 19:01:31.152407 22683591385216 run.py:483] Algo bellman_ford step 8397 current loss 0.018254, current_train_items 268736.
I0302 19:01:31.182843 22683591385216 run.py:483] Algo bellman_ford step 8398 current loss 0.064503, current_train_items 268768.
I0302 19:01:31.215269 22683591385216 run.py:483] Algo bellman_ford step 8399 current loss 0.049616, current_train_items 268800.
I0302 19:01:31.233674 22683591385216 run.py:483] Algo bellman_ford step 8400 current loss 0.001100, current_train_items 268832.
I0302 19:01:31.241158 22683591385216 run.py:503] (val) algo bellman_ford step 8400: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 268832, 'step': 8400, 'algorithm': 'bellman_ford'}
I0302 19:01:31.241265 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:31.257636 22683591385216 run.py:483] Algo bellman_ford step 8401 current loss 0.021830, current_train_items 268864.
I0302 19:01:31.281431 22683591385216 run.py:483] Algo bellman_ford step 8402 current loss 0.034187, current_train_items 268896.
I0302 19:01:31.312379 22683591385216 run.py:483] Algo bellman_ford step 8403 current loss 0.032249, current_train_items 268928.
I0302 19:01:31.346576 22683591385216 run.py:483] Algo bellman_ford step 8404 current loss 0.081794, current_train_items 268960.
I0302 19:01:31.365340 22683591385216 run.py:483] Algo bellman_ford step 8405 current loss 0.001413, current_train_items 268992.
I0302 19:01:31.380877 22683591385216 run.py:483] Algo bellman_ford step 8406 current loss 0.022597, current_train_items 269024.
I0302 19:01:31.403181 22683591385216 run.py:483] Algo bellman_ford step 8407 current loss 0.030793, current_train_items 269056.
I0302 19:01:31.432382 22683591385216 run.py:483] Algo bellman_ford step 8408 current loss 0.036049, current_train_items 269088.
I0302 19:01:31.465304 22683591385216 run.py:483] Algo bellman_ford step 8409 current loss 0.037160, current_train_items 269120.
I0302 19:01:31.483839 22683591385216 run.py:483] Algo bellman_ford step 8410 current loss 0.000719, current_train_items 269152.
I0302 19:01:31.499213 22683591385216 run.py:483] Algo bellman_ford step 8411 current loss 0.013607, current_train_items 269184.
I0302 19:01:31.522433 22683591385216 run.py:483] Algo bellman_ford step 8412 current loss 0.047838, current_train_items 269216.
I0302 19:01:31.553596 22683591385216 run.py:483] Algo bellman_ford step 8413 current loss 0.059367, current_train_items 269248.
I0302 19:01:31.586090 22683591385216 run.py:483] Algo bellman_ford step 8414 current loss 0.051698, current_train_items 269280.
I0302 19:01:31.604553 22683591385216 run.py:483] Algo bellman_ford step 8415 current loss 0.004331, current_train_items 269312.
I0302 19:01:31.620063 22683591385216 run.py:483] Algo bellman_ford step 8416 current loss 0.007508, current_train_items 269344.
I0302 19:01:31.642536 22683591385216 run.py:483] Algo bellman_ford step 8417 current loss 0.039554, current_train_items 269376.
I0302 19:01:31.673435 22683591385216 run.py:483] Algo bellman_ford step 8418 current loss 0.022322, current_train_items 269408.
I0302 19:01:31.708265 22683591385216 run.py:483] Algo bellman_ford step 8419 current loss 0.087765, current_train_items 269440.
I0302 19:01:31.726440 22683591385216 run.py:483] Algo bellman_ford step 8420 current loss 0.004321, current_train_items 269472.
I0302 19:01:31.741835 22683591385216 run.py:483] Algo bellman_ford step 8421 current loss 0.015461, current_train_items 269504.
I0302 19:01:31.765709 22683591385216 run.py:483] Algo bellman_ford step 8422 current loss 0.033022, current_train_items 269536.
I0302 19:01:31.796166 22683591385216 run.py:483] Algo bellman_ford step 8423 current loss 0.030409, current_train_items 269568.
I0302 19:01:31.828967 22683591385216 run.py:483] Algo bellman_ford step 8424 current loss 0.024836, current_train_items 269600.
I0302 19:01:31.847428 22683591385216 run.py:483] Algo bellman_ford step 8425 current loss 0.002318, current_train_items 269632.
I0302 19:01:31.862392 22683591385216 run.py:483] Algo bellman_ford step 8426 current loss 0.023843, current_train_items 269664.
I0302 19:01:31.885088 22683591385216 run.py:483] Algo bellman_ford step 8427 current loss 0.043251, current_train_items 269696.
I0302 19:01:31.914771 22683591385216 run.py:483] Algo bellman_ford step 8428 current loss 0.053034, current_train_items 269728.
I0302 19:01:31.946276 22683591385216 run.py:483] Algo bellman_ford step 8429 current loss 0.028494, current_train_items 269760.
I0302 19:01:31.964814 22683591385216 run.py:483] Algo bellman_ford step 8430 current loss 0.003159, current_train_items 269792.
I0302 19:01:31.980962 22683591385216 run.py:483] Algo bellman_ford step 8431 current loss 0.022936, current_train_items 269824.
I0302 19:01:32.003625 22683591385216 run.py:483] Algo bellman_ford step 8432 current loss 0.022098, current_train_items 269856.
I0302 19:01:32.034868 22683591385216 run.py:483] Algo bellman_ford step 8433 current loss 0.060047, current_train_items 269888.
I0302 19:01:32.065062 22683591385216 run.py:483] Algo bellman_ford step 8434 current loss 0.024453, current_train_items 269920.
I0302 19:01:32.082963 22683591385216 run.py:483] Algo bellman_ford step 8435 current loss 0.000654, current_train_items 269952.
I0302 19:01:32.098052 22683591385216 run.py:483] Algo bellman_ford step 8436 current loss 0.022954, current_train_items 269984.
I0302 19:01:32.120387 22683591385216 run.py:483] Algo bellman_ford step 8437 current loss 0.041685, current_train_items 270016.
I0302 19:01:32.150331 22683591385216 run.py:483] Algo bellman_ford step 8438 current loss 0.063998, current_train_items 270048.
I0302 19:01:32.180355 22683591385216 run.py:483] Algo bellman_ford step 8439 current loss 0.053447, current_train_items 270080.
I0302 19:01:32.198597 22683591385216 run.py:483] Algo bellman_ford step 8440 current loss 0.002850, current_train_items 270112.
I0302 19:01:32.214106 22683591385216 run.py:483] Algo bellman_ford step 8441 current loss 0.013987, current_train_items 270144.
I0302 19:01:32.236950 22683591385216 run.py:483] Algo bellman_ford step 8442 current loss 0.024718, current_train_items 270176.
I0302 19:01:32.266747 22683591385216 run.py:483] Algo bellman_ford step 8443 current loss 0.049392, current_train_items 270208.
I0302 19:01:32.299202 22683591385216 run.py:483] Algo bellman_ford step 8444 current loss 0.052758, current_train_items 270240.
I0302 19:01:32.317535 22683591385216 run.py:483] Algo bellman_ford step 8445 current loss 0.008553, current_train_items 270272.
I0302 19:01:32.332824 22683591385216 run.py:483] Algo bellman_ford step 8446 current loss 0.013083, current_train_items 270304.
I0302 19:01:32.355724 22683591385216 run.py:483] Algo bellman_ford step 8447 current loss 0.050249, current_train_items 270336.
I0302 19:01:32.385415 22683591385216 run.py:483] Algo bellman_ford step 8448 current loss 0.050795, current_train_items 270368.
I0302 19:01:32.417311 22683591385216 run.py:483] Algo bellman_ford step 8449 current loss 0.063941, current_train_items 270400.
I0302 19:01:32.435681 22683591385216 run.py:483] Algo bellman_ford step 8450 current loss 0.002666, current_train_items 270432.
I0302 19:01:32.443624 22683591385216 run.py:503] (val) algo bellman_ford step 8450: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 270432, 'step': 8450, 'algorithm': 'bellman_ford'}
I0302 19:01:32.443731 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:01:32.459461 22683591385216 run.py:483] Algo bellman_ford step 8451 current loss 0.026480, current_train_items 270464.
I0302 19:01:32.483123 22683591385216 run.py:483] Algo bellman_ford step 8452 current loss 0.030016, current_train_items 270496.
I0302 19:01:32.514022 22683591385216 run.py:483] Algo bellman_ford step 8453 current loss 0.066254, current_train_items 270528.
I0302 19:01:32.546941 22683591385216 run.py:483] Algo bellman_ford step 8454 current loss 0.059055, current_train_items 270560.
I0302 19:01:32.565999 22683591385216 run.py:483] Algo bellman_ford step 8455 current loss 0.001814, current_train_items 270592.
I0302 19:01:32.581592 22683591385216 run.py:483] Algo bellman_ford step 8456 current loss 0.006935, current_train_items 270624.
I0302 19:01:32.604965 22683591385216 run.py:483] Algo bellman_ford step 8457 current loss 0.048814, current_train_items 270656.
I0302 19:01:32.633406 22683591385216 run.py:483] Algo bellman_ford step 8458 current loss 0.042825, current_train_items 270688.
I0302 19:01:32.662856 22683591385216 run.py:483] Algo bellman_ford step 8459 current loss 0.055021, current_train_items 270720.
I0302 19:01:32.681674 22683591385216 run.py:483] Algo bellman_ford step 8460 current loss 0.005139, current_train_items 270752.
I0302 19:01:32.697347 22683591385216 run.py:483] Algo bellman_ford step 8461 current loss 0.004148, current_train_items 270784.
I0302 19:01:32.718667 22683591385216 run.py:483] Algo bellman_ford step 8462 current loss 0.017509, current_train_items 270816.
I0302 19:01:32.747820 22683591385216 run.py:483] Algo bellman_ford step 8463 current loss 0.045734, current_train_items 270848.
I0302 19:01:32.780327 22683591385216 run.py:483] Algo bellman_ford step 8464 current loss 0.050184, current_train_items 270880.
I0302 19:01:32.798409 22683591385216 run.py:483] Algo bellman_ford step 8465 current loss 0.002911, current_train_items 270912.
I0302 19:01:32.814152 22683591385216 run.py:483] Algo bellman_ford step 8466 current loss 0.026723, current_train_items 270944.
I0302 19:01:32.836230 22683591385216 run.py:483] Algo bellman_ford step 8467 current loss 0.019872, current_train_items 270976.
I0302 19:01:32.865305 22683591385216 run.py:483] Algo bellman_ford step 8468 current loss 0.027339, current_train_items 271008.
I0302 19:01:32.896203 22683591385216 run.py:483] Algo bellman_ford step 8469 current loss 0.031724, current_train_items 271040.
I0302 19:01:32.915043 22683591385216 run.py:483] Algo bellman_ford step 8470 current loss 0.003497, current_train_items 271072.
I0302 19:01:32.931138 22683591385216 run.py:483] Algo bellman_ford step 8471 current loss 0.018857, current_train_items 271104.
I0302 19:01:32.953513 22683591385216 run.py:483] Algo bellman_ford step 8472 current loss 0.012573, current_train_items 271136.
I0302 19:01:32.980839 22683591385216 run.py:483] Algo bellman_ford step 8473 current loss 0.012085, current_train_items 271168.
I0302 19:01:33.012741 22683591385216 run.py:483] Algo bellman_ford step 8474 current loss 0.048782, current_train_items 271200.
I0302 19:01:33.031601 22683591385216 run.py:483] Algo bellman_ford step 8475 current loss 0.002702, current_train_items 271232.
I0302 19:01:33.047483 22683591385216 run.py:483] Algo bellman_ford step 8476 current loss 0.013864, current_train_items 271264.
I0302 19:01:33.071337 22683591385216 run.py:483] Algo bellman_ford step 8477 current loss 0.040592, current_train_items 271296.
I0302 19:01:33.101206 22683591385216 run.py:483] Algo bellman_ford step 8478 current loss 0.025860, current_train_items 271328.
I0302 19:01:33.133809 22683591385216 run.py:483] Algo bellman_ford step 8479 current loss 0.026430, current_train_items 271360.
I0302 19:01:33.152047 22683591385216 run.py:483] Algo bellman_ford step 8480 current loss 0.002219, current_train_items 271392.
I0302 19:01:33.167541 22683591385216 run.py:483] Algo bellman_ford step 8481 current loss 0.013772, current_train_items 271424.
I0302 19:01:33.190454 22683591385216 run.py:483] Algo bellman_ford step 8482 current loss 0.044387, current_train_items 271456.
I0302 19:01:33.220444 22683591385216 run.py:483] Algo bellman_ford step 8483 current loss 0.034723, current_train_items 271488.
I0302 19:01:33.250719 22683591385216 run.py:483] Algo bellman_ford step 8484 current loss 0.034716, current_train_items 271520.
I0302 19:01:33.269670 22683591385216 run.py:483] Algo bellman_ford step 8485 current loss 0.001876, current_train_items 271552.
I0302 19:01:33.284959 22683591385216 run.py:483] Algo bellman_ford step 8486 current loss 0.002721, current_train_items 271584.
I0302 19:01:33.307313 22683591385216 run.py:483] Algo bellman_ford step 8487 current loss 0.010740, current_train_items 271616.
I0302 19:01:33.337608 22683591385216 run.py:483] Algo bellman_ford step 8488 current loss 0.041951, current_train_items 271648.
I0302 19:01:33.369266 22683591385216 run.py:483] Algo bellman_ford step 8489 current loss 0.036803, current_train_items 271680.
I0302 19:01:33.388044 22683591385216 run.py:483] Algo bellman_ford step 8490 current loss 0.022816, current_train_items 271712.
I0302 19:01:33.403404 22683591385216 run.py:483] Algo bellman_ford step 8491 current loss 0.008728, current_train_items 271744.
I0302 19:01:33.426275 22683591385216 run.py:483] Algo bellman_ford step 8492 current loss 0.030514, current_train_items 271776.
I0302 19:01:33.455726 22683591385216 run.py:483] Algo bellman_ford step 8493 current loss 0.027015, current_train_items 271808.
I0302 19:01:33.485893 22683591385216 run.py:483] Algo bellman_ford step 8494 current loss 0.072679, current_train_items 271840.
I0302 19:01:33.504008 22683591385216 run.py:483] Algo bellman_ford step 8495 current loss 0.003394, current_train_items 271872.
I0302 19:01:33.519662 22683591385216 run.py:483] Algo bellman_ford step 8496 current loss 0.036492, current_train_items 271904.
I0302 19:01:33.542226 22683591385216 run.py:483] Algo bellman_ford step 8497 current loss 0.022137, current_train_items 271936.
I0302 19:01:33.572499 22683591385216 run.py:483] Algo bellman_ford step 8498 current loss 0.069033, current_train_items 271968.
I0302 19:01:33.602665 22683591385216 run.py:483] Algo bellman_ford step 8499 current loss 0.032406, current_train_items 272000.
I0302 19:01:33.621305 22683591385216 run.py:483] Algo bellman_ford step 8500 current loss 0.001737, current_train_items 272032.
I0302 19:01:33.629086 22683591385216 run.py:503] (val) algo bellman_ford step 8500: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 272032, 'step': 8500, 'algorithm': 'bellman_ford'}
I0302 19:01:33.629191 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:01:33.645485 22683591385216 run.py:483] Algo bellman_ford step 8501 current loss 0.026647, current_train_items 272064.
I0302 19:01:33.669152 22683591385216 run.py:483] Algo bellman_ford step 8502 current loss 0.025388, current_train_items 272096.
I0302 19:01:33.698337 22683591385216 run.py:483] Algo bellman_ford step 8503 current loss 0.034383, current_train_items 272128.
I0302 19:01:33.729244 22683591385216 run.py:483] Algo bellman_ford step 8504 current loss 0.068652, current_train_items 272160.
I0302 19:01:33.747952 22683591385216 run.py:483] Algo bellman_ford step 8505 current loss 0.001019, current_train_items 272192.
I0302 19:01:33.762600 22683591385216 run.py:483] Algo bellman_ford step 8506 current loss 0.003117, current_train_items 272224.
I0302 19:01:33.784028 22683591385216 run.py:483] Algo bellman_ford step 8507 current loss 0.022247, current_train_items 272256.
I0302 19:01:33.814220 22683591385216 run.py:483] Algo bellman_ford step 8508 current loss 0.037748, current_train_items 272288.
I0302 19:01:33.847076 22683591385216 run.py:483] Algo bellman_ford step 8509 current loss 0.045768, current_train_items 272320.
I0302 19:01:33.865425 22683591385216 run.py:483] Algo bellman_ford step 8510 current loss 0.000515, current_train_items 272352.
I0302 19:01:33.880192 22683591385216 run.py:483] Algo bellman_ford step 8511 current loss 0.033817, current_train_items 272384.
I0302 19:01:33.903656 22683591385216 run.py:483] Algo bellman_ford step 8512 current loss 0.027819, current_train_items 272416.
I0302 19:01:33.933104 22683591385216 run.py:483] Algo bellman_ford step 8513 current loss 0.037861, current_train_items 272448.
I0302 19:01:33.965600 22683591385216 run.py:483] Algo bellman_ford step 8514 current loss 0.042926, current_train_items 272480.
I0302 19:01:33.983741 22683591385216 run.py:483] Algo bellman_ford step 8515 current loss 0.000508, current_train_items 272512.
I0302 19:01:33.998940 22683591385216 run.py:483] Algo bellman_ford step 8516 current loss 0.002031, current_train_items 272544.
I0302 19:01:34.021746 22683591385216 run.py:483] Algo bellman_ford step 8517 current loss 0.076405, current_train_items 272576.
I0302 19:01:34.053288 22683591385216 run.py:483] Algo bellman_ford step 8518 current loss 0.041800, current_train_items 272608.
I0302 19:01:34.084199 22683591385216 run.py:483] Algo bellman_ford step 8519 current loss 0.059315, current_train_items 272640.
I0302 19:01:34.102772 22683591385216 run.py:483] Algo bellman_ford step 8520 current loss 0.001446, current_train_items 272672.
I0302 19:01:34.117843 22683591385216 run.py:483] Algo bellman_ford step 8521 current loss 0.007467, current_train_items 272704.
I0302 19:01:34.139875 22683591385216 run.py:483] Algo bellman_ford step 8522 current loss 0.028046, current_train_items 272736.
I0302 19:01:34.169298 22683591385216 run.py:483] Algo bellman_ford step 8523 current loss 0.061400, current_train_items 272768.
I0302 19:01:34.201848 22683591385216 run.py:483] Algo bellman_ford step 8524 current loss 0.071228, current_train_items 272800.
I0302 19:01:34.220302 22683591385216 run.py:483] Algo bellman_ford step 8525 current loss 0.006153, current_train_items 272832.
I0302 19:01:34.235712 22683591385216 run.py:483] Algo bellman_ford step 8526 current loss 0.010923, current_train_items 272864.
I0302 19:01:34.258347 22683591385216 run.py:483] Algo bellman_ford step 8527 current loss 0.039012, current_train_items 272896.
I0302 19:01:34.287938 22683591385216 run.py:483] Algo bellman_ford step 8528 current loss 0.020532, current_train_items 272928.
I0302 19:01:34.320218 22683591385216 run.py:483] Algo bellman_ford step 8529 current loss 0.109969, current_train_items 272960.
I0302 19:01:34.338647 22683591385216 run.py:483] Algo bellman_ford step 8530 current loss 0.002605, current_train_items 272992.
I0302 19:01:34.354089 22683591385216 run.py:483] Algo bellman_ford step 8531 current loss 0.009481, current_train_items 273024.
I0302 19:01:34.375614 22683591385216 run.py:483] Algo bellman_ford step 8532 current loss 0.026844, current_train_items 273056.
I0302 19:01:34.404890 22683591385216 run.py:483] Algo bellman_ford step 8533 current loss 0.032097, current_train_items 273088.
I0302 19:01:34.435572 22683591385216 run.py:483] Algo bellman_ford step 8534 current loss 0.052376, current_train_items 273120.
I0302 19:01:34.453687 22683591385216 run.py:483] Algo bellman_ford step 8535 current loss 0.002574, current_train_items 273152.
I0302 19:01:34.469026 22683591385216 run.py:483] Algo bellman_ford step 8536 current loss 0.019974, current_train_items 273184.
I0302 19:01:34.491647 22683591385216 run.py:483] Algo bellman_ford step 8537 current loss 0.039929, current_train_items 273216.
I0302 19:01:34.521847 22683591385216 run.py:483] Algo bellman_ford step 8538 current loss 0.043868, current_train_items 273248.
I0302 19:01:34.555346 22683591385216 run.py:483] Algo bellman_ford step 8539 current loss 0.062456, current_train_items 273280.
I0302 19:01:34.573950 22683591385216 run.py:483] Algo bellman_ford step 8540 current loss 0.014423, current_train_items 273312.
I0302 19:01:34.589147 22683591385216 run.py:483] Algo bellman_ford step 8541 current loss 0.020767, current_train_items 273344.
I0302 19:01:34.612158 22683591385216 run.py:483] Algo bellman_ford step 8542 current loss 0.013327, current_train_items 273376.
I0302 19:01:34.642711 22683591385216 run.py:483] Algo bellman_ford step 8543 current loss 0.032541, current_train_items 273408.
I0302 19:01:34.674126 22683591385216 run.py:483] Algo bellman_ford step 8544 current loss 0.045191, current_train_items 273440.
I0302 19:01:34.692651 22683591385216 run.py:483] Algo bellman_ford step 8545 current loss 0.001955, current_train_items 273472.
I0302 19:01:34.707505 22683591385216 run.py:483] Algo bellman_ford step 8546 current loss 0.010222, current_train_items 273504.
I0302 19:01:34.730589 22683591385216 run.py:483] Algo bellman_ford step 8547 current loss 0.069404, current_train_items 273536.
I0302 19:01:34.760260 22683591385216 run.py:483] Algo bellman_ford step 8548 current loss 0.049604, current_train_items 273568.
I0302 19:01:34.792240 22683591385216 run.py:483] Algo bellman_ford step 8549 current loss 0.060265, current_train_items 273600.
I0302 19:01:34.810380 22683591385216 run.py:483] Algo bellman_ford step 8550 current loss 0.003205, current_train_items 273632.
I0302 19:01:34.818174 22683591385216 run.py:503] (val) algo bellman_ford step 8550: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 273632, 'step': 8550, 'algorithm': 'bellman_ford'}
I0302 19:01:34.818284 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:34.834573 22683591385216 run.py:483] Algo bellman_ford step 8551 current loss 0.006911, current_train_items 273664.
I0302 19:01:34.857875 22683591385216 run.py:483] Algo bellman_ford step 8552 current loss 0.027059, current_train_items 273696.
I0302 19:01:34.888015 22683591385216 run.py:483] Algo bellman_ford step 8553 current loss 0.032078, current_train_items 273728.
I0302 19:01:34.917696 22683591385216 run.py:483] Algo bellman_ford step 8554 current loss 0.019000, current_train_items 273760.
I0302 19:01:34.936137 22683591385216 run.py:483] Algo bellman_ford step 8555 current loss 0.001464, current_train_items 273792.
I0302 19:01:34.951308 22683591385216 run.py:483] Algo bellman_ford step 8556 current loss 0.016804, current_train_items 273824.
I0302 19:01:34.973391 22683591385216 run.py:483] Algo bellman_ford step 8557 current loss 0.017299, current_train_items 273856.
I0302 19:01:35.002966 22683591385216 run.py:483] Algo bellman_ford step 8558 current loss 0.040116, current_train_items 273888.
I0302 19:01:35.035506 22683591385216 run.py:483] Algo bellman_ford step 8559 current loss 0.048781, current_train_items 273920.
I0302 19:01:35.053918 22683591385216 run.py:483] Algo bellman_ford step 8560 current loss 0.000799, current_train_items 273952.
I0302 19:01:35.069809 22683591385216 run.py:483] Algo bellman_ford step 8561 current loss 0.018765, current_train_items 273984.
I0302 19:01:35.091954 22683591385216 run.py:483] Algo bellman_ford step 8562 current loss 0.036941, current_train_items 274016.
I0302 19:01:35.122619 22683591385216 run.py:483] Algo bellman_ford step 8563 current loss 0.029642, current_train_items 274048.
I0302 19:01:35.152695 22683591385216 run.py:483] Algo bellman_ford step 8564 current loss 0.040237, current_train_items 274080.
I0302 19:01:35.170547 22683591385216 run.py:483] Algo bellman_ford step 8565 current loss 0.001077, current_train_items 274112.
I0302 19:01:35.186425 22683591385216 run.py:483] Algo bellman_ford step 8566 current loss 0.034214, current_train_items 274144.
I0302 19:01:35.209926 22683591385216 run.py:483] Algo bellman_ford step 8567 current loss 0.030296, current_train_items 274176.
I0302 19:01:35.239638 22683591385216 run.py:483] Algo bellman_ford step 8568 current loss 0.030481, current_train_items 274208.
I0302 19:01:35.271603 22683591385216 run.py:483] Algo bellman_ford step 8569 current loss 0.041670, current_train_items 274240.
I0302 19:01:35.290048 22683591385216 run.py:483] Algo bellman_ford step 8570 current loss 0.001811, current_train_items 274272.
I0302 19:01:35.305511 22683591385216 run.py:483] Algo bellman_ford step 8571 current loss 0.023448, current_train_items 274304.
I0302 19:01:35.327025 22683591385216 run.py:483] Algo bellman_ford step 8572 current loss 0.043751, current_train_items 274336.
I0302 19:01:35.356345 22683591385216 run.py:483] Algo bellman_ford step 8573 current loss 0.031293, current_train_items 274368.
I0302 19:01:35.385934 22683591385216 run.py:483] Algo bellman_ford step 8574 current loss 0.058015, current_train_items 274400.
I0302 19:01:35.404454 22683591385216 run.py:483] Algo bellman_ford step 8575 current loss 0.000967, current_train_items 274432.
I0302 19:01:35.419814 22683591385216 run.py:483] Algo bellman_ford step 8576 current loss 0.012942, current_train_items 274464.
I0302 19:01:35.441944 22683591385216 run.py:483] Algo bellman_ford step 8577 current loss 0.032815, current_train_items 274496.
I0302 19:01:35.472032 22683591385216 run.py:483] Algo bellman_ford step 8578 current loss 0.030088, current_train_items 274528.
I0302 19:01:35.503777 22683591385216 run.py:483] Algo bellman_ford step 8579 current loss 0.050548, current_train_items 274560.
I0302 19:01:35.521858 22683591385216 run.py:483] Algo bellman_ford step 8580 current loss 0.005909, current_train_items 274592.
I0302 19:01:35.537235 22683591385216 run.py:483] Algo bellman_ford step 8581 current loss 0.001572, current_train_items 274624.
I0302 19:01:35.558474 22683591385216 run.py:483] Algo bellman_ford step 8582 current loss 0.026061, current_train_items 274656.
I0302 19:01:35.587997 22683591385216 run.py:483] Algo bellman_ford step 8583 current loss 0.041353, current_train_items 274688.
I0302 19:01:35.620426 22683591385216 run.py:483] Algo bellman_ford step 8584 current loss 0.046763, current_train_items 274720.
I0302 19:01:35.638748 22683591385216 run.py:483] Algo bellman_ford step 8585 current loss 0.001265, current_train_items 274752.
I0302 19:01:35.654171 22683591385216 run.py:483] Algo bellman_ford step 8586 current loss 0.003228, current_train_items 274784.
I0302 19:01:35.677417 22683591385216 run.py:483] Algo bellman_ford step 8587 current loss 0.054812, current_train_items 274816.
I0302 19:01:35.706677 22683591385216 run.py:483] Algo bellman_ford step 8588 current loss 0.026063, current_train_items 274848.
I0302 19:01:35.737017 22683591385216 run.py:483] Algo bellman_ford step 8589 current loss 0.028695, current_train_items 274880.
I0302 19:01:35.755403 22683591385216 run.py:483] Algo bellman_ford step 8590 current loss 0.014539, current_train_items 274912.
I0302 19:01:35.770156 22683591385216 run.py:483] Algo bellman_ford step 8591 current loss 0.008417, current_train_items 274944.
I0302 19:01:35.792181 22683591385216 run.py:483] Algo bellman_ford step 8592 current loss 0.012347, current_train_items 274976.
I0302 19:01:35.822316 22683591385216 run.py:483] Algo bellman_ford step 8593 current loss 0.040423, current_train_items 275008.
I0302 19:01:35.853505 22683591385216 run.py:483] Algo bellman_ford step 8594 current loss 0.097044, current_train_items 275040.
I0302 19:01:35.871468 22683591385216 run.py:483] Algo bellman_ford step 8595 current loss 0.000605, current_train_items 275072.
I0302 19:01:35.886481 22683591385216 run.py:483] Algo bellman_ford step 8596 current loss 0.010232, current_train_items 275104.
I0302 19:01:35.908800 22683591385216 run.py:483] Algo bellman_ford step 8597 current loss 0.040349, current_train_items 275136.
I0302 19:01:35.937248 22683591385216 run.py:483] Algo bellman_ford step 8598 current loss 0.050896, current_train_items 275168.
I0302 19:01:35.969346 22683591385216 run.py:483] Algo bellman_ford step 8599 current loss 0.051024, current_train_items 275200.
I0302 19:01:35.987776 22683591385216 run.py:483] Algo bellman_ford step 8600 current loss 0.017863, current_train_items 275232.
I0302 19:01:35.995277 22683591385216 run.py:503] (val) algo bellman_ford step 8600: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 275232, 'step': 8600, 'algorithm': 'bellman_ford'}
I0302 19:01:35.995383 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:36.011581 22683591385216 run.py:483] Algo bellman_ford step 8601 current loss 0.027088, current_train_items 275264.
I0302 19:01:36.035291 22683591385216 run.py:483] Algo bellman_ford step 8602 current loss 0.033880, current_train_items 275296.
I0302 19:01:36.066720 22683591385216 run.py:483] Algo bellman_ford step 8603 current loss 0.025719, current_train_items 275328.
I0302 19:01:36.099227 22683591385216 run.py:483] Algo bellman_ford step 8604 current loss 0.065151, current_train_items 275360.
I0302 19:01:36.117789 22683591385216 run.py:483] Algo bellman_ford step 8605 current loss 0.004732, current_train_items 275392.
I0302 19:01:36.132565 22683591385216 run.py:483] Algo bellman_ford step 8606 current loss 0.010275, current_train_items 275424.
I0302 19:01:36.154844 22683591385216 run.py:483] Algo bellman_ford step 8607 current loss 0.027479, current_train_items 275456.
I0302 19:01:36.184916 22683591385216 run.py:483] Algo bellman_ford step 8608 current loss 0.025326, current_train_items 275488.
I0302 19:01:36.218074 22683591385216 run.py:483] Algo bellman_ford step 8609 current loss 0.059736, current_train_items 275520.
I0302 19:01:36.236768 22683591385216 run.py:483] Algo bellman_ford step 8610 current loss 0.029078, current_train_items 275552.
I0302 19:01:36.252120 22683591385216 run.py:483] Algo bellman_ford step 8611 current loss 0.007018, current_train_items 275584.
I0302 19:01:36.275422 22683591385216 run.py:483] Algo bellman_ford step 8612 current loss 0.082216, current_train_items 275616.
I0302 19:01:36.305542 22683591385216 run.py:483] Algo bellman_ford step 8613 current loss 0.040071, current_train_items 275648.
I0302 19:01:36.335510 22683591385216 run.py:483] Algo bellman_ford step 8614 current loss 0.036594, current_train_items 275680.
I0302 19:01:36.354058 22683591385216 run.py:483] Algo bellman_ford step 8615 current loss 0.002084, current_train_items 275712.
I0302 19:01:36.369362 22683591385216 run.py:483] Algo bellman_ford step 8616 current loss 0.027750, current_train_items 275744.
I0302 19:01:36.392328 22683591385216 run.py:483] Algo bellman_ford step 8617 current loss 0.095401, current_train_items 275776.
I0302 19:01:36.422958 22683591385216 run.py:483] Algo bellman_ford step 8618 current loss 0.089276, current_train_items 275808.
I0302 19:01:36.454930 22683591385216 run.py:483] Algo bellman_ford step 8619 current loss 0.090861, current_train_items 275840.
I0302 19:01:36.473341 22683591385216 run.py:483] Algo bellman_ford step 8620 current loss 0.005773, current_train_items 275872.
I0302 19:01:36.488815 22683591385216 run.py:483] Algo bellman_ford step 8621 current loss 0.057303, current_train_items 275904.
I0302 19:01:36.511934 22683591385216 run.py:483] Algo bellman_ford step 8622 current loss 0.052609, current_train_items 275936.
I0302 19:01:36.542517 22683591385216 run.py:483] Algo bellman_ford step 8623 current loss 0.038568, current_train_items 275968.
I0302 19:01:36.573893 22683591385216 run.py:483] Algo bellman_ford step 8624 current loss 0.053575, current_train_items 276000.
I0302 19:01:36.592494 22683591385216 run.py:483] Algo bellman_ford step 8625 current loss 0.003951, current_train_items 276032.
I0302 19:01:36.607904 22683591385216 run.py:483] Algo bellman_ford step 8626 current loss 0.008515, current_train_items 276064.
I0302 19:01:36.631038 22683591385216 run.py:483] Algo bellman_ford step 8627 current loss 0.014867, current_train_items 276096.
I0302 19:01:36.661804 22683591385216 run.py:483] Algo bellman_ford step 8628 current loss 0.042043, current_train_items 276128.
I0302 19:01:36.692782 22683591385216 run.py:483] Algo bellman_ford step 8629 current loss 0.054033, current_train_items 276160.
I0302 19:01:36.711129 22683591385216 run.py:483] Algo bellman_ford step 8630 current loss 0.001872, current_train_items 276192.
I0302 19:01:36.726665 22683591385216 run.py:483] Algo bellman_ford step 8631 current loss 0.017782, current_train_items 276224.
I0302 19:01:36.749078 22683591385216 run.py:483] Algo bellman_ford step 8632 current loss 0.025125, current_train_items 276256.
I0302 19:01:36.778816 22683591385216 run.py:483] Algo bellman_ford step 8633 current loss 0.033406, current_train_items 276288.
I0302 19:01:36.810323 22683591385216 run.py:483] Algo bellman_ford step 8634 current loss 0.039044, current_train_items 276320.
I0302 19:01:36.829030 22683591385216 run.py:483] Algo bellman_ford step 8635 current loss 0.005633, current_train_items 276352.
I0302 19:01:36.844326 22683591385216 run.py:483] Algo bellman_ford step 8636 current loss 0.003570, current_train_items 276384.
I0302 19:01:36.867820 22683591385216 run.py:483] Algo bellman_ford step 8637 current loss 0.029543, current_train_items 276416.
I0302 19:01:36.896917 22683591385216 run.py:483] Algo bellman_ford step 8638 current loss 0.040443, current_train_items 276448.
I0302 19:01:36.929168 22683591385216 run.py:483] Algo bellman_ford step 8639 current loss 0.051409, current_train_items 276480.
I0302 19:01:36.947596 22683591385216 run.py:483] Algo bellman_ford step 8640 current loss 0.004372, current_train_items 276512.
I0302 19:01:36.963294 22683591385216 run.py:483] Algo bellman_ford step 8641 current loss 0.030917, current_train_items 276544.
I0302 19:01:36.986015 22683591385216 run.py:483] Algo bellman_ford step 8642 current loss 0.020838, current_train_items 276576.
I0302 19:01:37.016452 22683591385216 run.py:483] Algo bellman_ford step 8643 current loss 0.043432, current_train_items 276608.
I0302 19:01:37.048055 22683591385216 run.py:483] Algo bellman_ford step 8644 current loss 0.041231, current_train_items 276640.
I0302 19:01:37.066025 22683591385216 run.py:483] Algo bellman_ford step 8645 current loss 0.002579, current_train_items 276672.
I0302 19:01:37.081382 22683591385216 run.py:483] Algo bellman_ford step 8646 current loss 0.032798, current_train_items 276704.
I0302 19:01:37.104340 22683591385216 run.py:483] Algo bellman_ford step 8647 current loss 0.027302, current_train_items 276736.
I0302 19:01:37.135064 22683591385216 run.py:483] Algo bellman_ford step 8648 current loss 0.063141, current_train_items 276768.
I0302 19:01:37.166389 22683591385216 run.py:483] Algo bellman_ford step 8649 current loss 0.044912, current_train_items 276800.
I0302 19:01:37.184590 22683591385216 run.py:483] Algo bellman_ford step 8650 current loss 0.001555, current_train_items 276832.
I0302 19:01:37.192602 22683591385216 run.py:503] (val) algo bellman_ford step 8650: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 276832, 'step': 8650, 'algorithm': 'bellman_ford'}
I0302 19:01:37.192708 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0302 19:01:37.208760 22683591385216 run.py:483] Algo bellman_ford step 8651 current loss 0.010642, current_train_items 276864.
I0302 19:01:37.231692 22683591385216 run.py:483] Algo bellman_ford step 8652 current loss 0.042036, current_train_items 276896.
I0302 19:01:37.262419 22683591385216 run.py:483] Algo bellman_ford step 8653 current loss 0.029339, current_train_items 276928.
I0302 19:01:37.295375 22683591385216 run.py:483] Algo bellman_ford step 8654 current loss 0.051116, current_train_items 276960.
I0302 19:01:37.313974 22683591385216 run.py:483] Algo bellman_ford step 8655 current loss 0.000792, current_train_items 276992.
I0302 19:01:37.329185 22683591385216 run.py:483] Algo bellman_ford step 8656 current loss 0.017010, current_train_items 277024.
I0302 19:01:37.353487 22683591385216 run.py:483] Algo bellman_ford step 8657 current loss 0.040309, current_train_items 277056.
I0302 19:01:37.383934 22683591385216 run.py:483] Algo bellman_ford step 8658 current loss 0.034117, current_train_items 277088.
I0302 19:01:37.415477 22683591385216 run.py:483] Algo bellman_ford step 8659 current loss 0.066784, current_train_items 277120.
I0302 19:01:37.434469 22683591385216 run.py:483] Algo bellman_ford step 8660 current loss 0.001558, current_train_items 277152.
I0302 19:01:37.450013 22683591385216 run.py:483] Algo bellman_ford step 8661 current loss 0.009281, current_train_items 277184.
I0302 19:01:37.471971 22683591385216 run.py:483] Algo bellman_ford step 8662 current loss 0.021871, current_train_items 277216.
I0302 19:01:37.501362 22683591385216 run.py:483] Algo bellman_ford step 8663 current loss 0.085705, current_train_items 277248.
I0302 19:01:37.534148 22683591385216 run.py:483] Algo bellman_ford step 8664 current loss 0.084520, current_train_items 277280.
I0302 19:01:37.552433 22683591385216 run.py:483] Algo bellman_ford step 8665 current loss 0.000718, current_train_items 277312.
I0302 19:01:37.567276 22683591385216 run.py:483] Algo bellman_ford step 8666 current loss 0.007363, current_train_items 277344.
I0302 19:01:37.589774 22683591385216 run.py:483] Algo bellman_ford step 8667 current loss 0.013755, current_train_items 277376.
I0302 19:01:37.619142 22683591385216 run.py:483] Algo bellman_ford step 8668 current loss 0.017418, current_train_items 277408.
I0302 19:01:37.649795 22683591385216 run.py:483] Algo bellman_ford step 8669 current loss 0.015741, current_train_items 277440.
I0302 19:01:37.668609 22683591385216 run.py:483] Algo bellman_ford step 8670 current loss 0.000607, current_train_items 277472.
I0302 19:01:37.684442 22683591385216 run.py:483] Algo bellman_ford step 8671 current loss 0.012658, current_train_items 277504.
I0302 19:01:37.706576 22683591385216 run.py:483] Algo bellman_ford step 8672 current loss 0.011752, current_train_items 277536.
I0302 19:01:37.736086 22683591385216 run.py:483] Algo bellman_ford step 8673 current loss 0.036792, current_train_items 277568.
I0302 19:01:37.765616 22683591385216 run.py:483] Algo bellman_ford step 8674 current loss 0.046631, current_train_items 277600.
I0302 19:01:37.784245 22683591385216 run.py:483] Algo bellman_ford step 8675 current loss 0.001297, current_train_items 277632.
I0302 19:01:37.799689 22683591385216 run.py:483] Algo bellman_ford step 8676 current loss 0.016457, current_train_items 277664.
I0302 19:01:37.822416 22683591385216 run.py:483] Algo bellman_ford step 8677 current loss 0.041855, current_train_items 277696.
I0302 19:01:37.852396 22683591385216 run.py:483] Algo bellman_ford step 8678 current loss 0.038483, current_train_items 277728.
I0302 19:01:37.883520 22683591385216 run.py:483] Algo bellman_ford step 8679 current loss 0.046415, current_train_items 277760.
I0302 19:01:37.901741 22683591385216 run.py:483] Algo bellman_ford step 8680 current loss 0.001554, current_train_items 277792.
I0302 19:01:37.917223 22683591385216 run.py:483] Algo bellman_ford step 8681 current loss 0.006269, current_train_items 277824.
I0302 19:01:37.940300 22683591385216 run.py:483] Algo bellman_ford step 8682 current loss 0.031682, current_train_items 277856.
I0302 19:01:37.970124 22683591385216 run.py:483] Algo bellman_ford step 8683 current loss 0.037829, current_train_items 277888.
I0302 19:01:38.001133 22683591385216 run.py:483] Algo bellman_ford step 8684 current loss 0.056476, current_train_items 277920.
I0302 19:01:38.019944 22683591385216 run.py:483] Algo bellman_ford step 8685 current loss 0.002713, current_train_items 277952.
I0302 19:01:38.035262 22683591385216 run.py:483] Algo bellman_ford step 8686 current loss 0.024018, current_train_items 277984.
I0302 19:01:38.058450 22683591385216 run.py:483] Algo bellman_ford step 8687 current loss 0.071263, current_train_items 278016.
I0302 19:01:38.089086 22683591385216 run.py:483] Algo bellman_ford step 8688 current loss 0.080446, current_train_items 278048.
I0302 19:01:38.119638 22683591385216 run.py:483] Algo bellman_ford step 8689 current loss 0.056550, current_train_items 278080.
I0302 19:01:38.137947 22683591385216 run.py:483] Algo bellman_ford step 8690 current loss 0.002252, current_train_items 278112.
I0302 19:01:38.153622 22683591385216 run.py:483] Algo bellman_ford step 8691 current loss 0.033301, current_train_items 278144.
I0302 19:01:38.175605 22683591385216 run.py:483] Algo bellman_ford step 8692 current loss 0.032054, current_train_items 278176.
I0302 19:01:38.205194 22683591385216 run.py:483] Algo bellman_ford step 8693 current loss 0.051874, current_train_items 278208.
I0302 19:01:38.236593 22683591385216 run.py:483] Algo bellman_ford step 8694 current loss 0.057188, current_train_items 278240.
I0302 19:01:38.254833 22683591385216 run.py:483] Algo bellman_ford step 8695 current loss 0.001102, current_train_items 278272.
I0302 19:01:38.269875 22683591385216 run.py:483] Algo bellman_ford step 8696 current loss 0.010674, current_train_items 278304.
I0302 19:01:38.292431 22683591385216 run.py:483] Algo bellman_ford step 8697 current loss 0.169728, current_train_items 278336.
I0302 19:01:38.322598 22683591385216 run.py:483] Algo bellman_ford step 8698 current loss 0.056385, current_train_items 278368.
I0302 19:01:38.355784 22683591385216 run.py:483] Algo bellman_ford step 8699 current loss 0.128512, current_train_items 278400.
I0302 19:01:38.374492 22683591385216 run.py:483] Algo bellman_ford step 8700 current loss 0.001233, current_train_items 278432.
I0302 19:01:38.382203 22683591385216 run.py:503] (val) algo bellman_ford step 8700: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 278432, 'step': 8700, 'algorithm': 'bellman_ford'}
I0302 19:01:38.382309 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:01:38.398394 22683591385216 run.py:483] Algo bellman_ford step 8701 current loss 0.010696, current_train_items 278464.
I0302 19:01:38.421155 22683591385216 run.py:483] Algo bellman_ford step 8702 current loss 0.008490, current_train_items 278496.
I0302 19:01:38.451958 22683591385216 run.py:483] Algo bellman_ford step 8703 current loss 0.042564, current_train_items 278528.
I0302 19:01:38.485571 22683591385216 run.py:483] Algo bellman_ford step 8704 current loss 0.083426, current_train_items 278560.
I0302 19:01:38.504163 22683591385216 run.py:483] Algo bellman_ford step 8705 current loss 0.007262, current_train_items 278592.
I0302 19:01:38.519243 22683591385216 run.py:483] Algo bellman_ford step 8706 current loss 0.014916, current_train_items 278624.
I0302 19:01:38.541845 22683591385216 run.py:483] Algo bellman_ford step 8707 current loss 0.023096, current_train_items 278656.
I0302 19:01:38.572536 22683591385216 run.py:483] Algo bellman_ford step 8708 current loss 0.065063, current_train_items 278688.
I0302 19:01:38.604133 22683591385216 run.py:483] Algo bellman_ford step 8709 current loss 0.088045, current_train_items 278720.
I0302 19:01:38.622426 22683591385216 run.py:483] Algo bellman_ford step 8710 current loss 0.012334, current_train_items 278752.
I0302 19:01:38.637937 22683591385216 run.py:483] Algo bellman_ford step 8711 current loss 0.008180, current_train_items 278784.
I0302 19:01:38.660786 22683591385216 run.py:483] Algo bellman_ford step 8712 current loss 0.029710, current_train_items 278816.
I0302 19:01:38.692839 22683591385216 run.py:483] Algo bellman_ford step 8713 current loss 0.051188, current_train_items 278848.
I0302 19:01:38.723464 22683591385216 run.py:483] Algo bellman_ford step 8714 current loss 0.035478, current_train_items 278880.
I0302 19:01:38.741604 22683591385216 run.py:483] Algo bellman_ford step 8715 current loss 0.002389, current_train_items 278912.
I0302 19:01:38.757196 22683591385216 run.py:483] Algo bellman_ford step 8716 current loss 0.023347, current_train_items 278944.
I0302 19:01:38.779589 22683591385216 run.py:483] Algo bellman_ford step 8717 current loss 0.009660, current_train_items 278976.
I0302 19:01:38.808599 22683591385216 run.py:483] Algo bellman_ford step 8718 current loss 0.022356, current_train_items 279008.
I0302 19:01:38.839222 22683591385216 run.py:483] Algo bellman_ford step 8719 current loss 0.039708, current_train_items 279040.
I0302 19:01:38.857681 22683591385216 run.py:483] Algo bellman_ford step 8720 current loss 0.001110, current_train_items 279072.
I0302 19:01:38.873383 22683591385216 run.py:483] Algo bellman_ford step 8721 current loss 0.017515, current_train_items 279104.
I0302 19:01:38.897028 22683591385216 run.py:483] Algo bellman_ford step 8722 current loss 0.025245, current_train_items 279136.
I0302 19:01:38.926369 22683591385216 run.py:483] Algo bellman_ford step 8723 current loss 0.036835, current_train_items 279168.
I0302 19:01:38.957676 22683591385216 run.py:483] Algo bellman_ford step 8724 current loss 0.034473, current_train_items 279200.
I0302 19:01:38.976201 22683591385216 run.py:483] Algo bellman_ford step 8725 current loss 0.002159, current_train_items 279232.
I0302 19:01:38.991699 22683591385216 run.py:483] Algo bellman_ford step 8726 current loss 0.020512, current_train_items 279264.
I0302 19:01:39.014402 22683591385216 run.py:483] Algo bellman_ford step 8727 current loss 0.041756, current_train_items 279296.
I0302 19:01:39.044313 22683591385216 run.py:483] Algo bellman_ford step 8728 current loss 0.044780, current_train_items 279328.
I0302 19:01:39.075456 22683591385216 run.py:483] Algo bellman_ford step 8729 current loss 0.053480, current_train_items 279360.
I0302 19:01:39.094190 22683591385216 run.py:483] Algo bellman_ford step 8730 current loss 0.002560, current_train_items 279392.
I0302 19:01:39.109100 22683591385216 run.py:483] Algo bellman_ford step 8731 current loss 0.000927, current_train_items 279424.
I0302 19:01:39.131842 22683591385216 run.py:483] Algo bellman_ford step 8732 current loss 0.041199, current_train_items 279456.
I0302 19:01:39.162243 22683591385216 run.py:483] Algo bellman_ford step 8733 current loss 0.037416, current_train_items 279488.
I0302 19:01:39.195495 22683591385216 run.py:483] Algo bellman_ford step 8734 current loss 0.060097, current_train_items 279520.
I0302 19:01:39.213501 22683591385216 run.py:483] Algo bellman_ford step 8735 current loss 0.001229, current_train_items 279552.
I0302 19:01:39.228717 22683591385216 run.py:483] Algo bellman_ford step 8736 current loss 0.004884, current_train_items 279584.
I0302 19:01:39.252680 22683591385216 run.py:483] Algo bellman_ford step 8737 current loss 0.065447, current_train_items 279616.
I0302 19:01:39.282174 22683591385216 run.py:483] Algo bellman_ford step 8738 current loss 0.041022, current_train_items 279648.
I0302 19:01:39.314664 22683591385216 run.py:483] Algo bellman_ford step 8739 current loss 0.059891, current_train_items 279680.
I0302 19:01:39.333013 22683591385216 run.py:483] Algo bellman_ford step 8740 current loss 0.001075, current_train_items 279712.
I0302 19:01:39.348299 22683591385216 run.py:483] Algo bellman_ford step 8741 current loss 0.009282, current_train_items 279744.
I0302 19:01:39.370927 22683591385216 run.py:483] Algo bellman_ford step 8742 current loss 0.037527, current_train_items 279776.
I0302 19:01:39.401766 22683591385216 run.py:483] Algo bellman_ford step 8743 current loss 0.029977, current_train_items 279808.
I0302 19:01:39.433536 22683591385216 run.py:483] Algo bellman_ford step 8744 current loss 0.039454, current_train_items 279840.
I0302 19:01:39.452288 22683591385216 run.py:483] Algo bellman_ford step 8745 current loss 0.015336, current_train_items 279872.
I0302 19:01:39.467781 22683591385216 run.py:483] Algo bellman_ford step 8746 current loss 0.058876, current_train_items 279904.
I0302 19:01:39.489820 22683591385216 run.py:483] Algo bellman_ford step 8747 current loss 0.033492, current_train_items 279936.
I0302 19:01:39.520942 22683591385216 run.py:483] Algo bellman_ford step 8748 current loss 0.037531, current_train_items 279968.
I0302 19:01:39.553927 22683591385216 run.py:483] Algo bellman_ford step 8749 current loss 0.030791, current_train_items 280000.
I0302 19:01:39.572124 22683591385216 run.py:483] Algo bellman_ford step 8750 current loss 0.001249, current_train_items 280032.
I0302 19:01:39.580048 22683591385216 run.py:503] (val) algo bellman_ford step 8750: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 280032, 'step': 8750, 'algorithm': 'bellman_ford'}
I0302 19:01:39.580155 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:39.596116 22683591385216 run.py:483] Algo bellman_ford step 8751 current loss 0.025141, current_train_items 280064.
I0302 19:01:39.618762 22683591385216 run.py:483] Algo bellman_ford step 8752 current loss 0.061314, current_train_items 280096.
I0302 19:01:39.649506 22683591385216 run.py:483] Algo bellman_ford step 8753 current loss 0.024467, current_train_items 280128.
I0302 19:01:39.681758 22683591385216 run.py:483] Algo bellman_ford step 8754 current loss 0.034122, current_train_items 280160.
I0302 19:01:39.700187 22683591385216 run.py:483] Algo bellman_ford step 8755 current loss 0.019405, current_train_items 280192.
I0302 19:01:39.715306 22683591385216 run.py:483] Algo bellman_ford step 8756 current loss 0.016390, current_train_items 280224.
I0302 19:01:39.738784 22683591385216 run.py:483] Algo bellman_ford step 8757 current loss 0.036416, current_train_items 280256.
I0302 19:01:39.768040 22683591385216 run.py:483] Algo bellman_ford step 8758 current loss 0.019077, current_train_items 280288.
I0302 19:01:39.799661 22683591385216 run.py:483] Algo bellman_ford step 8759 current loss 0.047986, current_train_items 280320.
I0302 19:01:39.818151 22683591385216 run.py:483] Algo bellman_ford step 8760 current loss 0.004463, current_train_items 280352.
I0302 19:01:39.833936 22683591385216 run.py:483] Algo bellman_ford step 8761 current loss 0.005432, current_train_items 280384.
I0302 19:01:39.856678 22683591385216 run.py:483] Algo bellman_ford step 8762 current loss 0.028242, current_train_items 280416.
I0302 19:01:39.885669 22683591385216 run.py:483] Algo bellman_ford step 8763 current loss 0.030304, current_train_items 280448.
I0302 19:01:39.918658 22683591385216 run.py:483] Algo bellman_ford step 8764 current loss 0.062403, current_train_items 280480.
I0302 19:01:39.936586 22683591385216 run.py:483] Algo bellman_ford step 8765 current loss 0.002071, current_train_items 280512.
I0302 19:01:39.952283 22683591385216 run.py:483] Algo bellman_ford step 8766 current loss 0.012978, current_train_items 280544.
I0302 19:01:39.973596 22683591385216 run.py:483] Algo bellman_ford step 8767 current loss 0.020863, current_train_items 280576.
I0302 19:01:40.002480 22683591385216 run.py:483] Algo bellman_ford step 8768 current loss 0.051488, current_train_items 280608.
I0302 19:01:40.032009 22683591385216 run.py:483] Algo bellman_ford step 8769 current loss 0.034694, current_train_items 280640.
I0302 19:01:40.050557 22683591385216 run.py:483] Algo bellman_ford step 8770 current loss 0.003416, current_train_items 280672.
I0302 19:01:40.066452 22683591385216 run.py:483] Algo bellman_ford step 8771 current loss 0.007266, current_train_items 280704.
I0302 19:01:40.089004 22683591385216 run.py:483] Algo bellman_ford step 8772 current loss 0.021265, current_train_items 280736.
I0302 19:01:40.118945 22683591385216 run.py:483] Algo bellman_ford step 8773 current loss 0.042602, current_train_items 280768.
I0302 19:01:40.149534 22683591385216 run.py:483] Algo bellman_ford step 8774 current loss 0.044162, current_train_items 280800.
I0302 19:01:40.168028 22683591385216 run.py:483] Algo bellman_ford step 8775 current loss 0.004286, current_train_items 280832.
I0302 19:01:40.183685 22683591385216 run.py:483] Algo bellman_ford step 8776 current loss 0.024661, current_train_items 280864.
I0302 19:01:40.205145 22683591385216 run.py:483] Algo bellman_ford step 8777 current loss 0.029870, current_train_items 280896.
I0302 19:01:40.234398 22683591385216 run.py:483] Algo bellman_ford step 8778 current loss 0.077448, current_train_items 280928.
I0302 19:01:40.266548 22683591385216 run.py:483] Algo bellman_ford step 8779 current loss 0.027677, current_train_items 280960.
I0302 19:01:40.284390 22683591385216 run.py:483] Algo bellman_ford step 8780 current loss 0.002667, current_train_items 280992.
I0302 19:01:40.299680 22683591385216 run.py:483] Algo bellman_ford step 8781 current loss 0.082650, current_train_items 281024.
I0302 19:01:40.321950 22683591385216 run.py:483] Algo bellman_ford step 8782 current loss 0.055381, current_train_items 281056.
I0302 19:01:40.352300 22683591385216 run.py:483] Algo bellman_ford step 8783 current loss 0.052379, current_train_items 281088.
I0302 19:01:40.384171 22683591385216 run.py:483] Algo bellman_ford step 8784 current loss 0.086608, current_train_items 281120.
I0302 19:01:40.402974 22683591385216 run.py:483] Algo bellman_ford step 8785 current loss 0.001383, current_train_items 281152.
I0302 19:01:40.418462 22683591385216 run.py:483] Algo bellman_ford step 8786 current loss 0.027752, current_train_items 281184.
I0302 19:01:40.441031 22683591385216 run.py:483] Algo bellman_ford step 8787 current loss 0.100783, current_train_items 281216.
I0302 19:01:40.470435 22683591385216 run.py:483] Algo bellman_ford step 8788 current loss 0.042692, current_train_items 281248.
I0302 19:01:40.501974 22683591385216 run.py:483] Algo bellman_ford step 8789 current loss 0.049220, current_train_items 281280.
I0302 19:01:40.520415 22683591385216 run.py:483] Algo bellman_ford step 8790 current loss 0.042214, current_train_items 281312.
I0302 19:01:40.536359 22683591385216 run.py:483] Algo bellman_ford step 8791 current loss 0.014193, current_train_items 281344.
I0302 19:01:40.558228 22683591385216 run.py:483] Algo bellman_ford step 8792 current loss 0.021254, current_train_items 281376.
I0302 19:01:40.588133 22683591385216 run.py:483] Algo bellman_ford step 8793 current loss 0.038923, current_train_items 281408.
I0302 19:01:40.620282 22683591385216 run.py:483] Algo bellman_ford step 8794 current loss 0.090102, current_train_items 281440.
I0302 19:01:40.638201 22683591385216 run.py:483] Algo bellman_ford step 8795 current loss 0.004951, current_train_items 281472.
I0302 19:01:40.653430 22683591385216 run.py:483] Algo bellman_ford step 8796 current loss 0.018854, current_train_items 281504.
I0302 19:01:40.675599 22683591385216 run.py:483] Algo bellman_ford step 8797 current loss 0.027154, current_train_items 281536.
I0302 19:01:40.705639 22683591385216 run.py:483] Algo bellman_ford step 8798 current loss 0.047908, current_train_items 281568.
I0302 19:01:40.738340 22683591385216 run.py:483] Algo bellman_ford step 8799 current loss 0.075291, current_train_items 281600.
I0302 19:01:40.756701 22683591385216 run.py:483] Algo bellman_ford step 8800 current loss 0.004395, current_train_items 281632.
I0302 19:01:40.764551 22683591385216 run.py:503] (val) algo bellman_ford step 8800: {'pi': 0.9814453125, 'score': 0.9814453125, 'examples_seen': 281632, 'step': 8800, 'algorithm': 'bellman_ford'}
I0302 19:01:40.764659 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.981, val scores are: bellman_ford: 0.981
I0302 19:01:40.780319 22683591385216 run.py:483] Algo bellman_ford step 8801 current loss 0.013917, current_train_items 281664.
I0302 19:01:40.803530 22683591385216 run.py:483] Algo bellman_ford step 8802 current loss 0.047848, current_train_items 281696.
I0302 19:01:40.834936 22683591385216 run.py:483] Algo bellman_ford step 8803 current loss 0.038012, current_train_items 281728.
I0302 19:01:40.866913 22683591385216 run.py:483] Algo bellman_ford step 8804 current loss 0.035252, current_train_items 281760.
I0302 19:01:40.885593 22683591385216 run.py:483] Algo bellman_ford step 8805 current loss 0.002185, current_train_items 281792.
I0302 19:01:40.900838 22683591385216 run.py:483] Algo bellman_ford step 8806 current loss 0.032996, current_train_items 281824.
I0302 19:01:40.923536 22683591385216 run.py:483] Algo bellman_ford step 8807 current loss 0.035420, current_train_items 281856.
I0302 19:01:40.953126 22683591385216 run.py:483] Algo bellman_ford step 8808 current loss 0.064316, current_train_items 281888.
I0302 19:01:40.986737 22683591385216 run.py:483] Algo bellman_ford step 8809 current loss 0.062218, current_train_items 281920.
I0302 19:01:41.005025 22683591385216 run.py:483] Algo bellman_ford step 8810 current loss 0.002083, current_train_items 281952.
I0302 19:01:41.020572 22683591385216 run.py:483] Algo bellman_ford step 8811 current loss 0.013285, current_train_items 281984.
I0302 19:01:41.043401 22683591385216 run.py:483] Algo bellman_ford step 8812 current loss 0.049186, current_train_items 282016.
I0302 19:01:41.073726 22683591385216 run.py:483] Algo bellman_ford step 8813 current loss 0.036279, current_train_items 282048.
I0302 19:01:41.105620 22683591385216 run.py:483] Algo bellman_ford step 8814 current loss 0.061787, current_train_items 282080.
I0302 19:01:41.123874 22683591385216 run.py:483] Algo bellman_ford step 8815 current loss 0.004343, current_train_items 282112.
I0302 19:01:41.139326 22683591385216 run.py:483] Algo bellman_ford step 8816 current loss 0.029767, current_train_items 282144.
I0302 19:01:41.161960 22683591385216 run.py:483] Algo bellman_ford step 8817 current loss 0.037287, current_train_items 282176.
I0302 19:01:41.190912 22683591385216 run.py:483] Algo bellman_ford step 8818 current loss 0.033308, current_train_items 282208.
I0302 19:01:41.226295 22683591385216 run.py:483] Algo bellman_ford step 8819 current loss 0.047617, current_train_items 282240.
I0302 19:01:41.244610 22683591385216 run.py:483] Algo bellman_ford step 8820 current loss 0.002758, current_train_items 282272.
I0302 19:01:41.259865 22683591385216 run.py:483] Algo bellman_ford step 8821 current loss 0.017935, current_train_items 282304.
I0302 19:01:41.282403 22683591385216 run.py:483] Algo bellman_ford step 8822 current loss 0.033650, current_train_items 282336.
I0302 19:01:41.312774 22683591385216 run.py:483] Algo bellman_ford step 8823 current loss 0.039939, current_train_items 282368.
I0302 19:01:41.343407 22683591385216 run.py:483] Algo bellman_ford step 8824 current loss 0.064300, current_train_items 282400.
I0302 19:01:41.361687 22683591385216 run.py:483] Algo bellman_ford step 8825 current loss 0.003445, current_train_items 282432.
I0302 19:01:41.377181 22683591385216 run.py:483] Algo bellman_ford step 8826 current loss 0.037822, current_train_items 282464.
I0302 19:01:41.400226 22683591385216 run.py:483] Algo bellman_ford step 8827 current loss 0.075509, current_train_items 282496.
I0302 19:01:41.430294 22683591385216 run.py:483] Algo bellman_ford step 8828 current loss 0.035575, current_train_items 282528.
I0302 19:01:41.461737 22683591385216 run.py:483] Algo bellman_ford step 8829 current loss 0.050190, current_train_items 282560.
I0302 19:01:41.479941 22683591385216 run.py:483] Algo bellman_ford step 8830 current loss 0.004034, current_train_items 282592.
I0302 19:01:41.495037 22683591385216 run.py:483] Algo bellman_ford step 8831 current loss 0.006211, current_train_items 282624.
I0302 19:01:41.518620 22683591385216 run.py:483] Algo bellman_ford step 8832 current loss 0.047576, current_train_items 282656.
I0302 19:01:41.546962 22683591385216 run.py:483] Algo bellman_ford step 8833 current loss 0.051142, current_train_items 282688.
I0302 19:01:41.577486 22683591385216 run.py:483] Algo bellman_ford step 8834 current loss 0.047653, current_train_items 282720.
I0302 19:01:41.596215 22683591385216 run.py:483] Algo bellman_ford step 8835 current loss 0.004368, current_train_items 282752.
I0302 19:01:41.611418 22683591385216 run.py:483] Algo bellman_ford step 8836 current loss 0.028843, current_train_items 282784.
I0302 19:01:41.634881 22683591385216 run.py:483] Algo bellman_ford step 8837 current loss 0.044812, current_train_items 282816.
I0302 19:01:41.664795 22683591385216 run.py:483] Algo bellman_ford step 8838 current loss 0.046426, current_train_items 282848.
I0302 19:01:41.696609 22683591385216 run.py:483] Algo bellman_ford step 8839 current loss 0.051846, current_train_items 282880.
I0302 19:01:41.715178 22683591385216 run.py:483] Algo bellman_ford step 8840 current loss 0.003831, current_train_items 282912.
I0302 19:01:41.730758 22683591385216 run.py:483] Algo bellman_ford step 8841 current loss 0.009727, current_train_items 282944.
I0302 19:01:41.753875 22683591385216 run.py:483] Algo bellman_ford step 8842 current loss 0.032792, current_train_items 282976.
I0302 19:01:41.783769 22683591385216 run.py:483] Algo bellman_ford step 8843 current loss 0.041326, current_train_items 283008.
I0302 19:01:41.817226 22683591385216 run.py:483] Algo bellman_ford step 8844 current loss 0.059103, current_train_items 283040.
I0302 19:01:41.835635 22683591385216 run.py:483] Algo bellman_ford step 8845 current loss 0.006044, current_train_items 283072.
I0302 19:01:41.850783 22683591385216 run.py:483] Algo bellman_ford step 8846 current loss 0.031989, current_train_items 283104.
I0302 19:01:41.873654 22683591385216 run.py:483] Algo bellman_ford step 8847 current loss 0.022780, current_train_items 283136.
I0302 19:01:41.902406 22683591385216 run.py:483] Algo bellman_ford step 8848 current loss 0.018668, current_train_items 283168.
I0302 19:01:41.934240 22683591385216 run.py:483] Algo bellman_ford step 8849 current loss 0.038266, current_train_items 283200.
I0302 19:01:41.952433 22683591385216 run.py:483] Algo bellman_ford step 8850 current loss 0.001822, current_train_items 283232.
I0302 19:01:41.960253 22683591385216 run.py:503] (val) algo bellman_ford step 8850: {'pi': 0.98046875, 'score': 0.98046875, 'examples_seen': 283232, 'step': 8850, 'algorithm': 'bellman_ford'}
I0302 19:01:41.960360 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.980, val scores are: bellman_ford: 0.980
I0302 19:01:41.976433 22683591385216 run.py:483] Algo bellman_ford step 8851 current loss 0.024394, current_train_items 283264.
I0302 19:01:42.000292 22683591385216 run.py:483] Algo bellman_ford step 8852 current loss 0.038356, current_train_items 283296.
I0302 19:01:42.030910 22683591385216 run.py:483] Algo bellman_ford step 8853 current loss 0.066320, current_train_items 283328.
I0302 19:01:42.063125 22683591385216 run.py:483] Algo bellman_ford step 8854 current loss 0.078843, current_train_items 283360.
I0302 19:01:42.081974 22683591385216 run.py:483] Algo bellman_ford step 8855 current loss 0.000603, current_train_items 283392.
I0302 19:01:42.097529 22683591385216 run.py:483] Algo bellman_ford step 8856 current loss 0.021418, current_train_items 283424.
I0302 19:01:42.120066 22683591385216 run.py:483] Algo bellman_ford step 8857 current loss 0.026267, current_train_items 283456.
I0302 19:01:42.150985 22683591385216 run.py:483] Algo bellman_ford step 8858 current loss 0.030927, current_train_items 283488.
I0302 19:01:42.181071 22683591385216 run.py:483] Algo bellman_ford step 8859 current loss 0.023535, current_train_items 283520.
I0302 19:01:42.199654 22683591385216 run.py:483] Algo bellman_ford step 8860 current loss 0.000491, current_train_items 283552.
I0302 19:01:42.215408 22683591385216 run.py:483] Algo bellman_ford step 8861 current loss 0.004551, current_train_items 283584.
I0302 19:01:42.238266 22683591385216 run.py:483] Algo bellman_ford step 8862 current loss 0.020800, current_train_items 283616.
I0302 19:01:42.267409 22683591385216 run.py:483] Algo bellman_ford step 8863 current loss 0.015562, current_train_items 283648.
I0302 19:01:42.297915 22683591385216 run.py:483] Algo bellman_ford step 8864 current loss 0.051363, current_train_items 283680.
I0302 19:01:42.316416 22683591385216 run.py:483] Algo bellman_ford step 8865 current loss 0.001489, current_train_items 283712.
I0302 19:01:42.331505 22683591385216 run.py:483] Algo bellman_ford step 8866 current loss 0.016701, current_train_items 283744.
I0302 19:01:42.353234 22683591385216 run.py:483] Algo bellman_ford step 8867 current loss 0.037079, current_train_items 283776.
I0302 19:01:42.383450 22683591385216 run.py:483] Algo bellman_ford step 8868 current loss 0.098171, current_train_items 283808.
I0302 19:01:42.414007 22683591385216 run.py:483] Algo bellman_ford step 8869 current loss 0.042118, current_train_items 283840.
I0302 19:01:42.432778 22683591385216 run.py:483] Algo bellman_ford step 8870 current loss 0.011815, current_train_items 283872.
I0302 19:01:42.448356 22683591385216 run.py:483] Algo bellman_ford step 8871 current loss 0.001297, current_train_items 283904.
I0302 19:01:42.471316 22683591385216 run.py:483] Algo bellman_ford step 8872 current loss 0.029429, current_train_items 283936.
I0302 19:01:42.500593 22683591385216 run.py:483] Algo bellman_ford step 8873 current loss 0.017658, current_train_items 283968.
I0302 19:01:42.533263 22683591385216 run.py:483] Algo bellman_ford step 8874 current loss 0.088827, current_train_items 284000.
I0302 19:01:42.552166 22683591385216 run.py:483] Algo bellman_ford step 8875 current loss 0.000954, current_train_items 284032.
I0302 19:01:42.567773 22683591385216 run.py:483] Algo bellman_ford step 8876 current loss 0.013262, current_train_items 284064.
I0302 19:01:42.589672 22683591385216 run.py:483] Algo bellman_ford step 8877 current loss 0.031767, current_train_items 284096.
I0302 19:01:42.619017 22683591385216 run.py:483] Algo bellman_ford step 8878 current loss 0.032209, current_train_items 284128.
I0302 19:01:42.651019 22683591385216 run.py:483] Algo bellman_ford step 8879 current loss 0.062923, current_train_items 284160.
I0302 19:01:42.669289 22683591385216 run.py:483] Algo bellman_ford step 8880 current loss 0.000824, current_train_items 284192.
I0302 19:01:42.684468 22683591385216 run.py:483] Algo bellman_ford step 8881 current loss 0.025349, current_train_items 284224.
I0302 19:01:42.706240 22683591385216 run.py:483] Algo bellman_ford step 8882 current loss 0.027723, current_train_items 284256.
I0302 19:01:42.737334 22683591385216 run.py:483] Algo bellman_ford step 8883 current loss 0.026768, current_train_items 284288.
I0302 19:01:42.768816 22683591385216 run.py:483] Algo bellman_ford step 8884 current loss 0.041497, current_train_items 284320.
I0302 19:01:42.787657 22683591385216 run.py:483] Algo bellman_ford step 8885 current loss 0.002654, current_train_items 284352.
I0302 19:01:42.803774 22683591385216 run.py:483] Algo bellman_ford step 8886 current loss 0.027499, current_train_items 284384.
I0302 19:01:42.826545 22683591385216 run.py:483] Algo bellman_ford step 8887 current loss 0.021349, current_train_items 284416.
I0302 19:01:42.855797 22683591385216 run.py:483] Algo bellman_ford step 8888 current loss 0.051595, current_train_items 284448.
I0302 19:01:42.888523 22683591385216 run.py:483] Algo bellman_ford step 8889 current loss 0.054759, current_train_items 284480.
I0302 19:01:42.906888 22683591385216 run.py:483] Algo bellman_ford step 8890 current loss 0.000872, current_train_items 284512.
I0302 19:01:42.922024 22683591385216 run.py:483] Algo bellman_ford step 8891 current loss 0.012148, current_train_items 284544.
I0302 19:01:42.944702 22683591385216 run.py:483] Algo bellman_ford step 8892 current loss 0.029433, current_train_items 284576.
I0302 19:01:42.976264 22683591385216 run.py:483] Algo bellman_ford step 8893 current loss 0.058658, current_train_items 284608.
I0302 19:01:43.007643 22683591385216 run.py:483] Algo bellman_ford step 8894 current loss 0.049668, current_train_items 284640.
I0302 19:01:43.025977 22683591385216 run.py:483] Algo bellman_ford step 8895 current loss 0.000906, current_train_items 284672.
I0302 19:01:43.041118 22683591385216 run.py:483] Algo bellman_ford step 8896 current loss 0.008275, current_train_items 284704.
I0302 19:01:43.062934 22683591385216 run.py:483] Algo bellman_ford step 8897 current loss 0.035372, current_train_items 284736.
I0302 19:01:43.093931 22683591385216 run.py:483] Algo bellman_ford step 8898 current loss 0.043395, current_train_items 284768.
I0302 19:01:43.127049 22683591385216 run.py:483] Algo bellman_ford step 8899 current loss 0.048011, current_train_items 284800.
I0302 19:01:43.145699 22683591385216 run.py:483] Algo bellman_ford step 8900 current loss 0.000642, current_train_items 284832.
I0302 19:01:43.153431 22683591385216 run.py:503] (val) algo bellman_ford step 8900: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 284832, 'step': 8900, 'algorithm': 'bellman_ford'}
I0302 19:01:43.153538 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:43.169357 22683591385216 run.py:483] Algo bellman_ford step 8901 current loss 0.010201, current_train_items 284864.
I0302 19:01:43.192482 22683591385216 run.py:483] Algo bellman_ford step 8902 current loss 0.023769, current_train_items 284896.
I0302 19:01:43.221855 22683591385216 run.py:483] Algo bellman_ford step 8903 current loss 0.074871, current_train_items 284928.
I0302 19:01:43.255379 22683591385216 run.py:483] Algo bellman_ford step 8904 current loss 0.053710, current_train_items 284960.
I0302 19:01:43.274051 22683591385216 run.py:483] Algo bellman_ford step 8905 current loss 0.000676, current_train_items 284992.
I0302 19:01:43.289499 22683591385216 run.py:483] Algo bellman_ford step 8906 current loss 0.011420, current_train_items 285024.
I0302 19:01:43.312440 22683591385216 run.py:483] Algo bellman_ford step 8907 current loss 0.053091, current_train_items 285056.
I0302 19:01:43.342019 22683591385216 run.py:483] Algo bellman_ford step 8908 current loss 0.059025, current_train_items 285088.
I0302 19:01:43.372160 22683591385216 run.py:483] Algo bellman_ford step 8909 current loss 0.038719, current_train_items 285120.
I0302 19:01:43.390727 22683591385216 run.py:483] Algo bellman_ford step 8910 current loss 0.001522, current_train_items 285152.
I0302 19:01:43.405979 22683591385216 run.py:483] Algo bellman_ford step 8911 current loss 0.015097, current_train_items 285184.
I0302 19:01:43.429147 22683591385216 run.py:483] Algo bellman_ford step 8912 current loss 0.035439, current_train_items 285216.
I0302 19:01:43.457910 22683591385216 run.py:483] Algo bellman_ford step 8913 current loss 0.043973, current_train_items 285248.
I0302 19:01:43.490969 22683591385216 run.py:483] Algo bellman_ford step 8914 current loss 0.040713, current_train_items 285280.
I0302 19:01:43.509644 22683591385216 run.py:483] Algo bellman_ford step 8915 current loss 0.000660, current_train_items 285312.
I0302 19:01:43.524605 22683591385216 run.py:483] Algo bellman_ford step 8916 current loss 0.009389, current_train_items 285344.
I0302 19:01:43.547486 22683591385216 run.py:483] Algo bellman_ford step 8917 current loss 0.051085, current_train_items 285376.
I0302 19:01:43.577209 22683591385216 run.py:483] Algo bellman_ford step 8918 current loss 0.015708, current_train_items 285408.
I0302 19:01:43.608659 22683591385216 run.py:483] Algo bellman_ford step 8919 current loss 0.049921, current_train_items 285440.
I0302 19:01:43.627138 22683591385216 run.py:483] Algo bellman_ford step 8920 current loss 0.001542, current_train_items 285472.
I0302 19:01:43.642604 22683591385216 run.py:483] Algo bellman_ford step 8921 current loss 0.020099, current_train_items 285504.
I0302 19:01:43.666037 22683591385216 run.py:483] Algo bellman_ford step 8922 current loss 0.029208, current_train_items 285536.
I0302 19:01:43.695745 22683591385216 run.py:483] Algo bellman_ford step 8923 current loss 0.027930, current_train_items 285568.
I0302 19:01:43.727836 22683591385216 run.py:483] Algo bellman_ford step 8924 current loss 0.061242, current_train_items 285600.
I0302 19:01:43.745957 22683591385216 run.py:483] Algo bellman_ford step 8925 current loss 0.000748, current_train_items 285632.
I0302 19:01:43.760916 22683591385216 run.py:483] Algo bellman_ford step 8926 current loss 0.009687, current_train_items 285664.
I0302 19:01:43.783820 22683591385216 run.py:483] Algo bellman_ford step 8927 current loss 0.065499, current_train_items 285696.
I0302 19:01:43.812192 22683591385216 run.py:483] Algo bellman_ford step 8928 current loss 0.022344, current_train_items 285728.
I0302 19:01:43.843738 22683591385216 run.py:483] Algo bellman_ford step 8929 current loss 0.050332, current_train_items 285760.
I0302 19:01:43.862208 22683591385216 run.py:483] Algo bellman_ford step 8930 current loss 0.002197, current_train_items 285792.
I0302 19:01:43.878027 22683591385216 run.py:483] Algo bellman_ford step 8931 current loss 0.009031, current_train_items 285824.
I0302 19:01:43.900610 22683591385216 run.py:483] Algo bellman_ford step 8932 current loss 0.023899, current_train_items 285856.
I0302 19:01:43.932011 22683591385216 run.py:483] Algo bellman_ford step 8933 current loss 0.063192, current_train_items 285888.
I0302 19:01:43.964280 22683591385216 run.py:483] Algo bellman_ford step 8934 current loss 0.059465, current_train_items 285920.
I0302 19:01:43.982687 22683591385216 run.py:483] Algo bellman_ford step 8935 current loss 0.003799, current_train_items 285952.
I0302 19:01:43.997740 22683591385216 run.py:483] Algo bellman_ford step 8936 current loss 0.022225, current_train_items 285984.
I0302 19:01:44.020594 22683591385216 run.py:483] Algo bellman_ford step 8937 current loss 0.057250, current_train_items 286016.
I0302 19:01:44.050091 22683591385216 run.py:483] Algo bellman_ford step 8938 current loss 0.032153, current_train_items 286048.
I0302 19:01:44.080749 22683591385216 run.py:483] Algo bellman_ford step 8939 current loss 0.028069, current_train_items 286080.
I0302 19:01:44.099017 22683591385216 run.py:483] Algo bellman_ford step 8940 current loss 0.003986, current_train_items 286112.
I0302 19:01:44.114116 22683591385216 run.py:483] Algo bellman_ford step 8941 current loss 0.005097, current_train_items 286144.
I0302 19:01:44.137193 22683591385216 run.py:483] Algo bellman_ford step 8942 current loss 0.047205, current_train_items 286176.
I0302 19:01:44.167972 22683591385216 run.py:483] Algo bellman_ford step 8943 current loss 0.056604, current_train_items 286208.
I0302 19:01:44.199692 22683591385216 run.py:483] Algo bellman_ford step 8944 current loss 0.042658, current_train_items 286240.
I0302 19:01:44.217719 22683591385216 run.py:483] Algo bellman_ford step 8945 current loss 0.003034, current_train_items 286272.
I0302 19:01:44.232944 22683591385216 run.py:483] Algo bellman_ford step 8946 current loss 0.012727, current_train_items 286304.
I0302 19:01:44.254970 22683591385216 run.py:483] Algo bellman_ford step 8947 current loss 0.027100, current_train_items 286336.
I0302 19:01:44.285089 22683591385216 run.py:483] Algo bellman_ford step 8948 current loss 0.063305, current_train_items 286368.
I0302 19:01:44.316409 22683591385216 run.py:483] Algo bellman_ford step 8949 current loss 0.037916, current_train_items 286400.
I0302 19:01:44.334642 22683591385216 run.py:483] Algo bellman_ford step 8950 current loss 0.002197, current_train_items 286432.
I0302 19:01:44.342659 22683591385216 run.py:503] (val) algo bellman_ford step 8950: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 286432, 'step': 8950, 'algorithm': 'bellman_ford'}
I0302 19:01:44.342763 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:44.358880 22683591385216 run.py:483] Algo bellman_ford step 8951 current loss 0.016736, current_train_items 286464.
I0302 19:01:44.381136 22683591385216 run.py:483] Algo bellman_ford step 8952 current loss 0.028086, current_train_items 286496.
I0302 19:01:44.413815 22683591385216 run.py:483] Algo bellman_ford step 8953 current loss 0.045471, current_train_items 286528.
I0302 19:01:44.447121 22683591385216 run.py:483] Algo bellman_ford step 8954 current loss 0.031355, current_train_items 286560.
I0302 19:01:44.465660 22683591385216 run.py:483] Algo bellman_ford step 8955 current loss 0.002094, current_train_items 286592.
I0302 19:01:44.481406 22683591385216 run.py:483] Algo bellman_ford step 8956 current loss 0.012174, current_train_items 286624.
I0302 19:01:44.504329 22683591385216 run.py:483] Algo bellman_ford step 8957 current loss 0.032874, current_train_items 286656.
I0302 19:01:44.533456 22683591385216 run.py:483] Algo bellman_ford step 8958 current loss 0.028418, current_train_items 286688.
I0302 19:01:44.565829 22683591385216 run.py:483] Algo bellman_ford step 8959 current loss 0.076002, current_train_items 286720.
I0302 19:01:44.584522 22683591385216 run.py:483] Algo bellman_ford step 8960 current loss 0.003202, current_train_items 286752.
I0302 19:01:44.599631 22683591385216 run.py:483] Algo bellman_ford step 8961 current loss 0.011963, current_train_items 286784.
I0302 19:01:44.622781 22683591385216 run.py:483] Algo bellman_ford step 8962 current loss 0.047554, current_train_items 286816.
I0302 19:01:44.652723 22683591385216 run.py:483] Algo bellman_ford step 8963 current loss 0.022040, current_train_items 286848.
I0302 19:01:44.682574 22683591385216 run.py:483] Algo bellman_ford step 8964 current loss 0.026337, current_train_items 286880.
I0302 19:01:44.700624 22683591385216 run.py:483] Algo bellman_ford step 8965 current loss 0.002076, current_train_items 286912.
I0302 19:01:44.716045 22683591385216 run.py:483] Algo bellman_ford step 8966 current loss 0.014937, current_train_items 286944.
I0302 19:01:44.738486 22683591385216 run.py:483] Algo bellman_ford step 8967 current loss 0.025645, current_train_items 286976.
I0302 19:01:44.768319 22683591385216 run.py:483] Algo bellman_ford step 8968 current loss 0.041482, current_train_items 287008.
I0302 19:01:44.799366 22683591385216 run.py:483] Algo bellman_ford step 8969 current loss 0.048290, current_train_items 287040.
I0302 19:01:44.817757 22683591385216 run.py:483] Algo bellman_ford step 8970 current loss 0.002344, current_train_items 287072.
I0302 19:01:44.833287 22683591385216 run.py:483] Algo bellman_ford step 8971 current loss 0.001930, current_train_items 287104.
I0302 19:01:44.855475 22683591385216 run.py:483] Algo bellman_ford step 8972 current loss 0.037411, current_train_items 287136.
I0302 19:01:44.884487 22683591385216 run.py:483] Algo bellman_ford step 8973 current loss 0.015395, current_train_items 287168.
I0302 19:01:44.915240 22683591385216 run.py:483] Algo bellman_ford step 8974 current loss 0.061723, current_train_items 287200.
I0302 19:01:44.933794 22683591385216 run.py:483] Algo bellman_ford step 8975 current loss 0.050758, current_train_items 287232.
I0302 19:01:44.949128 22683591385216 run.py:483] Algo bellman_ford step 8976 current loss 0.008511, current_train_items 287264.
I0302 19:01:44.970381 22683591385216 run.py:483] Algo bellman_ford step 8977 current loss 0.010089, current_train_items 287296.
I0302 19:01:45.000163 22683591385216 run.py:483] Algo bellman_ford step 8978 current loss 0.015686, current_train_items 287328.
I0302 19:01:45.031162 22683591385216 run.py:483] Algo bellman_ford step 8979 current loss 0.062112, current_train_items 287360.
I0302 19:01:45.049179 22683591385216 run.py:483] Algo bellman_ford step 8980 current loss 0.002066, current_train_items 287392.
I0302 19:01:45.064589 22683591385216 run.py:483] Algo bellman_ford step 8981 current loss 0.006765, current_train_items 287424.
I0302 19:01:45.086955 22683591385216 run.py:483] Algo bellman_ford step 8982 current loss 0.017650, current_train_items 287456.
I0302 19:01:45.117084 22683591385216 run.py:483] Algo bellman_ford step 8983 current loss 0.020107, current_train_items 287488.
I0302 19:01:45.148297 22683591385216 run.py:483] Algo bellman_ford step 8984 current loss 0.046901, current_train_items 287520.
I0302 19:01:45.166775 22683591385216 run.py:483] Algo bellman_ford step 8985 current loss 0.017045, current_train_items 287552.
I0302 19:01:45.182299 22683591385216 run.py:483] Algo bellman_ford step 8986 current loss 0.002820, current_train_items 287584.
I0302 19:01:45.204060 22683591385216 run.py:483] Algo bellman_ford step 8987 current loss 0.018829, current_train_items 287616.
I0302 19:01:45.232190 22683591385216 run.py:483] Algo bellman_ford step 8988 current loss 0.038607, current_train_items 287648.
I0302 19:01:45.265444 22683591385216 run.py:483] Algo bellman_ford step 8989 current loss 0.043131, current_train_items 287680.
I0302 19:01:45.283813 22683591385216 run.py:483] Algo bellman_ford step 8990 current loss 0.020853, current_train_items 287712.
I0302 19:01:45.299010 22683591385216 run.py:483] Algo bellman_ford step 8991 current loss 0.008075, current_train_items 287744.
I0302 19:01:45.320800 22683591385216 run.py:483] Algo bellman_ford step 8992 current loss 0.045374, current_train_items 287776.
I0302 19:01:45.350084 22683591385216 run.py:483] Algo bellman_ford step 8993 current loss 0.030208, current_train_items 287808.
I0302 19:01:45.380841 22683591385216 run.py:483] Algo bellman_ford step 8994 current loss 0.060296, current_train_items 287840.
I0302 19:01:45.398564 22683591385216 run.py:483] Algo bellman_ford step 8995 current loss 0.003542, current_train_items 287872.
I0302 19:01:45.413890 22683591385216 run.py:483] Algo bellman_ford step 8996 current loss 0.036891, current_train_items 287904.
I0302 19:01:45.436758 22683591385216 run.py:483] Algo bellman_ford step 8997 current loss 0.037467, current_train_items 287936.
I0302 19:01:45.467848 22683591385216 run.py:483] Algo bellman_ford step 8998 current loss 0.038852, current_train_items 287968.
I0302 19:01:45.497994 22683591385216 run.py:483] Algo bellman_ford step 8999 current loss 0.037823, current_train_items 288000.
I0302 19:01:45.516688 22683591385216 run.py:483] Algo bellman_ford step 9000 current loss 0.003868, current_train_items 288032.
I0302 19:01:45.524458 22683591385216 run.py:503] (val) algo bellman_ford step 9000: {'pi': 0.998046875, 'score': 0.998046875, 'examples_seen': 288032, 'step': 9000, 'algorithm': 'bellman_ford'}
I0302 19:01:45.524564 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.998, val scores are: bellman_ford: 0.998
I0302 19:01:45.540777 22683591385216 run.py:483] Algo bellman_ford step 9001 current loss 0.014302, current_train_items 288064.
I0302 19:01:45.562987 22683591385216 run.py:483] Algo bellman_ford step 9002 current loss 0.023936, current_train_items 288096.
I0302 19:01:45.592362 22683591385216 run.py:483] Algo bellman_ford step 9003 current loss 0.044483, current_train_items 288128.
I0302 19:01:45.624363 22683591385216 run.py:483] Algo bellman_ford step 9004 current loss 0.045039, current_train_items 288160.
I0302 19:01:45.642871 22683591385216 run.py:483] Algo bellman_ford step 9005 current loss 0.003442, current_train_items 288192.
I0302 19:01:45.658404 22683591385216 run.py:483] Algo bellman_ford step 9006 current loss 0.004030, current_train_items 288224.
I0302 19:01:45.680115 22683591385216 run.py:483] Algo bellman_ford step 9007 current loss 0.015541, current_train_items 288256.
I0302 19:01:45.710464 22683591385216 run.py:483] Algo bellman_ford step 9008 current loss 0.024933, current_train_items 288288.
I0302 19:01:45.742413 22683591385216 run.py:483] Algo bellman_ford step 9009 current loss 0.039318, current_train_items 288320.
I0302 19:01:45.760571 22683591385216 run.py:483] Algo bellman_ford step 9010 current loss 0.029997, current_train_items 288352.
I0302 19:01:45.775933 22683591385216 run.py:483] Algo bellman_ford step 9011 current loss 0.020732, current_train_items 288384.
I0302 19:01:45.797693 22683591385216 run.py:483] Algo bellman_ford step 9012 current loss 0.011780, current_train_items 288416.
I0302 19:01:45.826695 22683591385216 run.py:483] Algo bellman_ford step 9013 current loss 0.048640, current_train_items 288448.
I0302 19:01:45.859964 22683591385216 run.py:483] Algo bellman_ford step 9014 current loss 0.046905, current_train_items 288480.
I0302 19:01:45.877935 22683591385216 run.py:483] Algo bellman_ford step 9015 current loss 0.001706, current_train_items 288512.
I0302 19:01:45.893362 22683591385216 run.py:483] Algo bellman_ford step 9016 current loss 0.042849, current_train_items 288544.
I0302 19:01:45.916381 22683591385216 run.py:483] Algo bellman_ford step 9017 current loss 0.027765, current_train_items 288576.
I0302 19:01:45.946505 22683591385216 run.py:483] Algo bellman_ford step 9018 current loss 0.062936, current_train_items 288608.
I0302 19:01:45.979086 22683591385216 run.py:483] Algo bellman_ford step 9019 current loss 0.087223, current_train_items 288640.
I0302 19:01:45.996939 22683591385216 run.py:483] Algo bellman_ford step 9020 current loss 0.001613, current_train_items 288672.
I0302 19:01:46.012181 22683591385216 run.py:483] Algo bellman_ford step 9021 current loss 0.005256, current_train_items 288704.
I0302 19:01:46.035842 22683591385216 run.py:483] Algo bellman_ford step 9022 current loss 0.021930, current_train_items 288736.
I0302 19:01:46.066859 22683591385216 run.py:483] Algo bellman_ford step 9023 current loss 0.040799, current_train_items 288768.
I0302 19:01:46.098470 22683591385216 run.py:483] Algo bellman_ford step 9024 current loss 0.052796, current_train_items 288800.
I0302 19:01:46.116543 22683591385216 run.py:483] Algo bellman_ford step 9025 current loss 0.000730, current_train_items 288832.
I0302 19:01:46.131479 22683591385216 run.py:483] Algo bellman_ford step 9026 current loss 0.018268, current_train_items 288864.
I0302 19:01:46.154263 22683591385216 run.py:483] Algo bellman_ford step 9027 current loss 0.040926, current_train_items 288896.
I0302 19:01:46.184345 22683591385216 run.py:483] Algo bellman_ford step 9028 current loss 0.046456, current_train_items 288928.
I0302 19:01:46.216105 22683591385216 run.py:483] Algo bellman_ford step 9029 current loss 0.077878, current_train_items 288960.
I0302 19:01:46.234609 22683591385216 run.py:483] Algo bellman_ford step 9030 current loss 0.001289, current_train_items 288992.
I0302 19:01:46.249936 22683591385216 run.py:483] Algo bellman_ford step 9031 current loss 0.032256, current_train_items 289024.
I0302 19:01:46.272983 22683591385216 run.py:483] Algo bellman_ford step 9032 current loss 0.096231, current_train_items 289056.
I0302 19:01:46.303419 22683591385216 run.py:483] Algo bellman_ford step 9033 current loss 0.043342, current_train_items 289088.
I0302 19:01:46.337359 22683591385216 run.py:483] Algo bellman_ford step 9034 current loss 0.075573, current_train_items 289120.
I0302 19:01:46.355337 22683591385216 run.py:483] Algo bellman_ford step 9035 current loss 0.001319, current_train_items 289152.
I0302 19:01:46.370295 22683591385216 run.py:483] Algo bellman_ford step 9036 current loss 0.032975, current_train_items 289184.
I0302 19:01:46.393780 22683591385216 run.py:483] Algo bellman_ford step 9037 current loss 0.091734, current_train_items 289216.
I0302 19:01:46.423458 22683591385216 run.py:483] Algo bellman_ford step 9038 current loss 0.110822, current_train_items 289248.
I0302 19:01:46.454780 22683591385216 run.py:483] Algo bellman_ford step 9039 current loss 0.098621, current_train_items 289280.
I0302 19:01:46.472926 22683591385216 run.py:483] Algo bellman_ford step 9040 current loss 0.004629, current_train_items 289312.
I0302 19:01:46.488491 22683591385216 run.py:483] Algo bellman_ford step 9041 current loss 0.024327, current_train_items 289344.
I0302 19:01:46.510549 22683591385216 run.py:483] Algo bellman_ford step 9042 current loss 0.025139, current_train_items 289376.
I0302 19:01:46.540100 22683591385216 run.py:483] Algo bellman_ford step 9043 current loss 0.047177, current_train_items 289408.
I0302 19:01:46.572079 22683591385216 run.py:483] Algo bellman_ford step 9044 current loss 0.019205, current_train_items 289440.
I0302 19:01:46.590239 22683591385216 run.py:483] Algo bellman_ford step 9045 current loss 0.003981, current_train_items 289472.
I0302 19:01:46.605791 22683591385216 run.py:483] Algo bellman_ford step 9046 current loss 0.034645, current_train_items 289504.
I0302 19:01:46.627104 22683591385216 run.py:483] Algo bellman_ford step 9047 current loss 0.040702, current_train_items 289536.
I0302 19:01:46.657160 22683591385216 run.py:483] Algo bellman_ford step 9048 current loss 0.095608, current_train_items 289568.
I0302 19:01:46.688305 22683591385216 run.py:483] Algo bellman_ford step 9049 current loss 0.084237, current_train_items 289600.
I0302 19:01:46.706238 22683591385216 run.py:483] Algo bellman_ford step 9050 current loss 0.002641, current_train_items 289632.
I0302 19:01:46.714091 22683591385216 run.py:503] (val) algo bellman_ford step 9050: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 289632, 'step': 9050, 'algorithm': 'bellman_ford'}
I0302 19:01:46.714199 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:01:46.729876 22683591385216 run.py:483] Algo bellman_ford step 9051 current loss 0.025677, current_train_items 289664.
I0302 19:01:46.753377 22683591385216 run.py:483] Algo bellman_ford step 9052 current loss 0.066962, current_train_items 289696.
I0302 19:01:46.784394 22683591385216 run.py:483] Algo bellman_ford step 9053 current loss 0.049674, current_train_items 289728.
I0302 19:01:46.815145 22683591385216 run.py:483] Algo bellman_ford step 9054 current loss 0.037304, current_train_items 289760.
I0302 19:01:46.833868 22683591385216 run.py:483] Algo bellman_ford step 9055 current loss 0.003524, current_train_items 289792.
I0302 19:01:46.848956 22683591385216 run.py:483] Algo bellman_ford step 9056 current loss 0.018130, current_train_items 289824.
I0302 19:01:46.871107 22683591385216 run.py:483] Algo bellman_ford step 9057 current loss 0.023617, current_train_items 289856.
I0302 19:01:46.899452 22683591385216 run.py:483] Algo bellman_ford step 9058 current loss 0.030640, current_train_items 289888.
I0302 19:01:46.929601 22683591385216 run.py:483] Algo bellman_ford step 9059 current loss 0.038058, current_train_items 289920.
I0302 19:01:46.948273 22683591385216 run.py:483] Algo bellman_ford step 9060 current loss 0.004758, current_train_items 289952.
I0302 19:01:46.964323 22683591385216 run.py:483] Algo bellman_ford step 9061 current loss 0.007505, current_train_items 289984.
I0302 19:01:46.987746 22683591385216 run.py:483] Algo bellman_ford step 9062 current loss 0.024767, current_train_items 290016.
I0302 19:01:47.016372 22683591385216 run.py:483] Algo bellman_ford step 9063 current loss 0.013186, current_train_items 290048.
I0302 19:01:47.048769 22683591385216 run.py:483] Algo bellman_ford step 9064 current loss 0.054479, current_train_items 290080.
I0302 19:01:47.067517 22683591385216 run.py:483] Algo bellman_ford step 9065 current loss 0.001534, current_train_items 290112.
I0302 19:01:47.082823 22683591385216 run.py:483] Algo bellman_ford step 9066 current loss 0.018397, current_train_items 290144.
I0302 19:01:47.103788 22683591385216 run.py:483] Algo bellman_ford step 9067 current loss 0.026911, current_train_items 290176.
I0302 19:01:47.133937 22683591385216 run.py:483] Algo bellman_ford step 9068 current loss 0.018836, current_train_items 290208.
I0302 19:01:47.166525 22683591385216 run.py:483] Algo bellman_ford step 9069 current loss 0.067423, current_train_items 290240.
I0302 19:01:47.185193 22683591385216 run.py:483] Algo bellman_ford step 9070 current loss 0.001643, current_train_items 290272.
I0302 19:01:47.201103 22683591385216 run.py:483] Algo bellman_ford step 9071 current loss 0.038415, current_train_items 290304.
I0302 19:01:47.224005 22683591385216 run.py:483] Algo bellman_ford step 9072 current loss 0.058273, current_train_items 290336.
I0302 19:01:47.254497 22683591385216 run.py:483] Algo bellman_ford step 9073 current loss 0.054323, current_train_items 290368.
I0302 19:01:47.285853 22683591385216 run.py:483] Algo bellman_ford step 9074 current loss 0.043692, current_train_items 290400.
I0302 19:01:47.304277 22683591385216 run.py:483] Algo bellman_ford step 9075 current loss 0.006003, current_train_items 290432.
I0302 19:01:47.319838 22683591385216 run.py:483] Algo bellman_ford step 9076 current loss 0.022303, current_train_items 290464.
I0302 19:01:47.342447 22683591385216 run.py:483] Algo bellman_ford step 9077 current loss 0.046209, current_train_items 290496.
I0302 19:01:47.371984 22683591385216 run.py:483] Algo bellman_ford step 9078 current loss 0.063427, current_train_items 290528.
I0302 19:01:47.404270 22683591385216 run.py:483] Algo bellman_ford step 9079 current loss 0.056029, current_train_items 290560.
I0302 19:01:47.422826 22683591385216 run.py:483] Algo bellman_ford step 9080 current loss 0.001507, current_train_items 290592.
I0302 19:01:47.437965 22683591385216 run.py:483] Algo bellman_ford step 9081 current loss 0.005186, current_train_items 290624.
I0302 19:01:47.459947 22683591385216 run.py:483] Algo bellman_ford step 9082 current loss 0.043504, current_train_items 290656.
I0302 19:01:47.489041 22683591385216 run.py:483] Algo bellman_ford step 9083 current loss 0.026610, current_train_items 290688.
I0302 19:01:47.519656 22683591385216 run.py:483] Algo bellman_ford step 9084 current loss 0.064790, current_train_items 290720.
I0302 19:01:47.538522 22683591385216 run.py:483] Algo bellman_ford step 9085 current loss 0.001657, current_train_items 290752.
I0302 19:01:47.554188 22683591385216 run.py:483] Algo bellman_ford step 9086 current loss 0.015674, current_train_items 290784.
I0302 19:01:47.576941 22683591385216 run.py:483] Algo bellman_ford step 9087 current loss 0.025498, current_train_items 290816.
I0302 19:01:47.606050 22683591385216 run.py:483] Algo bellman_ford step 9088 current loss 0.016401, current_train_items 290848.
I0302 19:01:47.639451 22683591385216 run.py:483] Algo bellman_ford step 9089 current loss 0.069920, current_train_items 290880.
I0302 19:01:47.657894 22683591385216 run.py:483] Algo bellman_ford step 9090 current loss 0.001426, current_train_items 290912.
I0302 19:01:47.673340 22683591385216 run.py:483] Algo bellman_ford step 9091 current loss 0.016584, current_train_items 290944.
I0302 19:01:47.695450 22683591385216 run.py:483] Algo bellman_ford step 9092 current loss 0.031927, current_train_items 290976.
I0302 19:01:47.723166 22683591385216 run.py:483] Algo bellman_ford step 9093 current loss 0.015265, current_train_items 291008.
I0302 19:01:47.757287 22683591385216 run.py:483] Algo bellman_ford step 9094 current loss 0.049707, current_train_items 291040.
I0302 19:01:47.775730 22683591385216 run.py:483] Algo bellman_ford step 9095 current loss 0.004985, current_train_items 291072.
I0302 19:01:47.790968 22683591385216 run.py:483] Algo bellman_ford step 9096 current loss 0.007676, current_train_items 291104.
I0302 19:01:47.813417 22683591385216 run.py:483] Algo bellman_ford step 9097 current loss 0.039181, current_train_items 291136.
I0302 19:01:47.843545 22683591385216 run.py:483] Algo bellman_ford step 9098 current loss 0.047406, current_train_items 291168.
I0302 19:01:47.874279 22683591385216 run.py:483] Algo bellman_ford step 9099 current loss 0.027357, current_train_items 291200.
I0302 19:01:47.893119 22683591385216 run.py:483] Algo bellman_ford step 9100 current loss 0.001573, current_train_items 291232.
I0302 19:01:47.900914 22683591385216 run.py:503] (val) algo bellman_ford step 9100: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 291232, 'step': 9100, 'algorithm': 'bellman_ford'}
I0302 19:01:47.901021 22683591385216 run.py:522] Not saving new best model, best avg val score was 0.998, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:01:47.916970 22683591385216 run.py:483] Algo bellman_ford step 9101 current loss 0.001270, current_train_items 291264.
I0302 19:01:47.940984 22683591385216 run.py:483] Algo bellman_ford step 9102 current loss 0.047284, current_train_items 291296.
I0302 19:01:47.971521 22683591385216 run.py:483] Algo bellman_ford step 9103 current loss 0.033154, current_train_items 291328.
I0302 19:01:48.004994 22683591385216 run.py:483] Algo bellman_ford step 9104 current loss 0.027241, current_train_items 291360.
I0302 19:01:48.023430 22683591385216 run.py:483] Algo bellman_ford step 9105 current loss 0.009020, current_train_items 291392.
I0302 19:01:48.038961 22683591385216 run.py:483] Algo bellman_ford step 9106 current loss 0.017484, current_train_items 291424.
I0302 19:01:48.061722 22683591385216 run.py:483] Algo bellman_ford step 9107 current loss 0.055652, current_train_items 291456.
I0302 19:01:48.091732 22683591385216 run.py:483] Algo bellman_ford step 9108 current loss 0.040829, current_train_items 291488.
I0302 19:01:48.123806 22683591385216 run.py:483] Algo bellman_ford step 9109 current loss 0.054747, current_train_items 291520.
I0302 19:01:48.142320 22683591385216 run.py:483] Algo bellman_ford step 9110 current loss 0.000950, current_train_items 291552.
I0302 19:01:48.157508 22683591385216 run.py:483] Algo bellman_ford step 9111 current loss 0.017472, current_train_items 291584.
I0302 19:01:48.179604 22683591385216 run.py:483] Algo bellman_ford step 9112 current loss 0.042972, current_train_items 291616.
I0302 19:01:48.211272 22683591385216 run.py:483] Algo bellman_ford step 9113 current loss 0.077528, current_train_items 291648.
I0302 19:01:48.244384 22683591385216 run.py:483] Algo bellman_ford step 9114 current loss 0.089464, current_train_items 291680.
I0302 19:01:48.262776 22683591385216 run.py:483] Algo bellman_ford step 9115 current loss 0.013330, current_train_items 291712.
I0302 19:01:48.278339 22683591385216 run.py:483] Algo bellman_ford step 9116 current loss 0.004001, current_train_items 291744.
I0302 19:01:48.300805 22683591385216 run.py:483] Algo bellman_ford step 9117 current loss 0.026939, current_train_items 291776.
I0302 19:01:48.330543 22683591385216 run.py:483] Algo bellman_ford step 9118 current loss 0.024969, current_train_items 291808.
I0302 19:01:48.364064 22683591385216 run.py:483] Algo bellman_ford step 9119 current loss 0.077915, current_train_items 291840.
I0302 19:01:48.382552 22683591385216 run.py:483] Algo bellman_ford step 9120 current loss 0.009362, current_train_items 291872.
I0302 19:01:48.397866 22683591385216 run.py:483] Algo bellman_ford step 9121 current loss 0.017538, current_train_items 291904.
I0302 19:01:48.420025 22683591385216 run.py:483] Algo bellman_ford step 9122 current loss 0.018040, current_train_items 291936.
I0302 19:01:48.451699 22683591385216 run.py:483] Algo bellman_ford step 9123 current loss 0.040233, current_train_items 291968.
I0302 19:01:48.485558 22683591385216 run.py:483] Algo bellman_ford step 9124 current loss 0.048439, current_train_items 292000.
I0302 19:01:48.504144 22683591385216 run.py:483] Algo bellman_ford step 9125 current loss 0.001549, current_train_items 292032.
I0302 19:01:48.519172 22683591385216 run.py:483] Algo bellman_ford step 9126 current loss 0.020696, current_train_items 292064.
I0302 19:01:48.540578 22683591385216 run.py:483] Algo bellman_ford step 9127 current loss 0.030140, current_train_items 292096.
I0302 19:01:48.571470 22683591385216 run.py:483] Algo bellman_ford step 9128 current loss 0.035528, current_train_items 292128.
I0302 19:01:48.602557 22683591385216 run.py:483] Algo bellman_ford step 9129 current loss 0.063177, current_train_items 292160.
I0302 19:01:48.621225 22683591385216 run.py:483] Algo bellman_ford step 9130 current loss 0.002010, current_train_items 292192.
I0302 19:01:48.636582 22683591385216 run.py:483] Algo bellman_ford step 9131 current loss 0.003242, current_train_items 292224.
I0302 19:01:48.660260 22683591385216 run.py:483] Algo bellman_ford step 9132 current loss 0.073371, current_train_items 292256.
I0302 19:01:48.690584 22683591385216 run.py:483] Algo bellman_ford step 9133 current loss 0.106111, current_train_items 292288.
I0302 19:01:48.721163 22683591385216 run.py:483] Algo bellman_ford step 9134 current loss 0.084353, current_train_items 292320.
I0302 19:01:48.739458 22683591385216 run.py:483] Algo bellman_ford step 9135 current loss 0.001302, current_train_items 292352.
I0302 19:01:48.754373 22683591385216 run.py:483] Algo bellman_ford step 9136 current loss 0.001727, current_train_items 292384.
I0302 19:01:48.775967 22683591385216 run.py:483] Algo bellman_ford step 9137 current loss 0.017847, current_train_items 292416.
I0302 19:01:48.805758 22683591385216 run.py:483] Algo bellman_ford step 9138 current loss 0.022271, current_train_items 292448.
I0302 19:01:48.836035 22683591385216 run.py:483] Algo bellman_ford step 9139 current loss 0.035303, current_train_items 292480.
I0302 19:01:48.854533 22683591385216 run.py:483] Algo bellman_ford step 9140 current loss 0.001108, current_train_items 292512.
I0302 19:01:48.869770 22683591385216 run.py:483] Algo bellman_ford step 9141 current loss 0.004954, current_train_items 292544.
I0302 19:01:48.891583 22683591385216 run.py:483] Algo bellman_ford step 9142 current loss 0.016288, current_train_items 292576.
I0302 19:01:48.921013 22683591385216 run.py:483] Algo bellman_ford step 9143 current loss 0.033112, current_train_items 292608.
I0302 19:01:48.953563 22683591385216 run.py:483] Algo bellman_ford step 9144 current loss 0.071743, current_train_items 292640.
I0302 19:01:48.971618 22683591385216 run.py:483] Algo bellman_ford step 9145 current loss 0.001805, current_train_items 292672.
I0302 19:01:48.986988 22683591385216 run.py:483] Algo bellman_ford step 9146 current loss 0.066206, current_train_items 292704.
I0302 19:01:49.009663 22683591385216 run.py:483] Algo bellman_ford step 9147 current loss 0.021132, current_train_items 292736.
I0302 19:01:49.039571 22683591385216 run.py:483] Algo bellman_ford step 9148 current loss 0.021813, current_train_items 292768.
I0302 19:01:49.068120 22683591385216 run.py:483] Algo bellman_ford step 9149 current loss 0.035371, current_train_items 292800.
I0302 19:01:49.086026 22683591385216 run.py:483] Algo bellman_ford step 9150 current loss 0.001615, current_train_items 292832.
I0302 19:01:49.093992 22683591385216 run.py:503] (val) algo bellman_ford step 9150: {'pi': 1.0, 'score': 1.0, 'examples_seen': 292832, 'step': 9150, 'algorithm': 'bellman_ford'}
I0302 19:01:49.094098 22683591385216 run.py:519] Checkpointing best model, best avg val score was 0.998, current avg val score is 1.000, val scores are: bellman_ford: 1.000
I0302 19:01:49.122323 22683591385216 run.py:483] Algo bellman_ford step 9151 current loss 0.009970, current_train_items 292864.
I0302 19:01:49.144508 22683591385216 run.py:483] Algo bellman_ford step 9152 current loss 0.038059, current_train_items 292896.
I0302 19:01:49.175178 22683591385216 run.py:483] Algo bellman_ford step 9153 current loss 0.058173, current_train_items 292928.
I0302 19:01:49.205024 22683591385216 run.py:483] Algo bellman_ford step 9154 current loss 0.027759, current_train_items 292960.
I0302 19:01:49.223587 22683591385216 run.py:483] Algo bellman_ford step 9155 current loss 0.002109, current_train_items 292992.
I0302 19:01:49.239592 22683591385216 run.py:483] Algo bellman_ford step 9156 current loss 0.017546, current_train_items 293024.
I0302 19:01:49.262943 22683591385216 run.py:483] Algo bellman_ford step 9157 current loss 0.055406, current_train_items 293056.
I0302 19:01:49.292419 22683591385216 run.py:483] Algo bellman_ford step 9158 current loss 0.031273, current_train_items 293088.
I0302 19:01:49.325154 22683591385216 run.py:483] Algo bellman_ford step 9159 current loss 0.045736, current_train_items 293120.
I0302 19:01:49.343939 22683591385216 run.py:483] Algo bellman_ford step 9160 current loss 0.007456, current_train_items 293152.
I0302 19:01:49.359817 22683591385216 run.py:483] Algo bellman_ford step 9161 current loss 0.023584, current_train_items 293184.
I0302 19:01:49.382261 22683591385216 run.py:483] Algo bellman_ford step 9162 current loss 0.061991, current_train_items 293216.
I0302 19:01:49.411807 22683591385216 run.py:483] Algo bellman_ford step 9163 current loss 0.061063, current_train_items 293248.
I0302 19:01:49.445086 22683591385216 run.py:483] Algo bellman_ford step 9164 current loss 0.047711, current_train_items 293280.
I0302 19:01:49.463245 22683591385216 run.py:483] Algo bellman_ford step 9165 current loss 0.003633, current_train_items 293312.
I0302 19:01:49.478684 22683591385216 run.py:483] Algo bellman_ford step 9166 current loss 0.026849, current_train_items 293344.
I0302 19:01:49.501226 22683591385216 run.py:483] Algo bellman_ford step 9167 current loss 0.029215, current_train_items 293376.
I0302 19:01:49.531487 22683591385216 run.py:483] Algo bellman_ford step 9168 current loss 0.099357, current_train_items 293408.
I0302 19:01:49.562321 22683591385216 run.py:483] Algo bellman_ford step 9169 current loss 0.102101, current_train_items 293440.
I0302 19:01:49.580839 22683591385216 run.py:483] Algo bellman_ford step 9170 current loss 0.003553, current_train_items 293472.
I0302 19:01:49.596416 22683591385216 run.py:483] Algo bellman_ford step 9171 current loss 0.010513, current_train_items 293504.
I0302 19:01:49.620249 22683591385216 run.py:483] Algo bellman_ford step 9172 current loss 0.049403, current_train_items 293536.
I0302 19:01:49.650613 22683591385216 run.py:483] Algo bellman_ford step 9173 current loss 0.027413, current_train_items 293568.
I0302 19:01:49.683197 22683591385216 run.py:483] Algo bellman_ford step 9174 current loss 0.055248, current_train_items 293600.
I0302 19:01:49.701871 22683591385216 run.py:483] Algo bellman_ford step 9175 current loss 0.022268, current_train_items 293632.
I0302 19:01:49.717713 22683591385216 run.py:483] Algo bellman_ford step 9176 current loss 0.003234, current_train_items 293664.
I0302 19:01:49.738965 22683591385216 run.py:483] Algo bellman_ford step 9177 current loss 0.018176, current_train_items 293696.
I0302 19:01:49.767345 22683591385216 run.py:483] Algo bellman_ford step 9178 current loss 0.028997, current_train_items 293728.
I0302 19:01:49.800425 22683591385216 run.py:483] Algo bellman_ford step 9179 current loss 0.085066, current_train_items 293760.
I0302 19:01:49.818461 22683591385216 run.py:483] Algo bellman_ford step 9180 current loss 0.001825, current_train_items 293792.
I0302 19:01:49.833796 22683591385216 run.py:483] Algo bellman_ford step 9181 current loss 0.051777, current_train_items 293824.
I0302 19:01:49.857445 22683591385216 run.py:483] Algo bellman_ford step 9182 current loss 0.053666, current_train_items 293856.
I0302 19:01:49.886209 22683591385216 run.py:483] Algo bellman_ford step 9183 current loss 0.046481, current_train_items 293888.
I0302 19:01:49.916946 22683591385216 run.py:483] Algo bellman_ford step 9184 current loss 0.045579, current_train_items 293920.
I0302 19:01:49.935481 22683591385216 run.py:483] Algo bellman_ford step 9185 current loss 0.019342, current_train_items 293952.
I0302 19:01:49.951395 22683591385216 run.py:483] Algo bellman_ford step 9186 current loss 0.010444, current_train_items 293984.
I0302 19:01:49.974264 22683591385216 run.py:483] Algo bellman_ford step 9187 current loss 0.021108, current_train_items 294016.
I0302 19:01:50.005595 22683591385216 run.py:483] Algo bellman_ford step 9188 current loss 0.043389, current_train_items 294048.
I0302 19:01:50.039474 22683591385216 run.py:483] Algo bellman_ford step 9189 current loss 0.029125, current_train_items 294080.
I0302 19:01:50.058360 22683591385216 run.py:483] Algo bellman_ford step 9190 current loss 0.023629, current_train_items 294112.
I0302 19:01:50.073796 22683591385216 run.py:483] Algo bellman_ford step 9191 current loss 0.053480, current_train_items 294144.
I0302 19:01:50.096670 22683591385216 run.py:483] Algo bellman_ford step 9192 current loss 0.055212, current_train_items 294176.
I0302 19:01:50.126683 22683591385216 run.py:483] Algo bellman_ford step 9193 current loss 0.036415, current_train_items 294208.
I0302 19:01:50.157459 22683591385216 run.py:483] Algo bellman_ford step 9194 current loss 0.027927, current_train_items 294240.
I0302 19:01:50.175683 22683591385216 run.py:483] Algo bellman_ford step 9195 current loss 0.001987, current_train_items 294272.
I0302 19:01:50.191516 22683591385216 run.py:483] Algo bellman_ford step 9196 current loss 0.013505, current_train_items 294304.
I0302 19:01:50.214585 22683591385216 run.py:483] Algo bellman_ford step 9197 current loss 0.053139, current_train_items 294336.
I0302 19:01:50.243975 22683591385216 run.py:483] Algo bellman_ford step 9198 current loss 0.023477, current_train_items 294368.
I0302 19:01:50.274401 22683591385216 run.py:483] Algo bellman_ford step 9199 current loss 0.056720, current_train_items 294400.
I0302 19:01:50.293223 22683591385216 run.py:483] Algo bellman_ford step 9200 current loss 0.000958, current_train_items 294432.
I0302 19:01:50.301000 22683591385216 run.py:503] (val) algo bellman_ford step 9200: {'pi': 0.984375, 'score': 0.984375, 'examples_seen': 294432, 'step': 9200, 'algorithm': 'bellman_ford'}
I0302 19:01:50.301105 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.984, val scores are: bellman_ford: 0.984
I0302 19:01:50.317104 22683591385216 run.py:483] Algo bellman_ford step 9201 current loss 0.003476, current_train_items 294464.
I0302 19:01:50.339921 22683591385216 run.py:483] Algo bellman_ford step 9202 current loss 0.031836, current_train_items 294496.
I0302 19:01:50.371419 22683591385216 run.py:483] Algo bellman_ford step 9203 current loss 0.037373, current_train_items 294528.
I0302 19:01:50.404348 22683591385216 run.py:483] Algo bellman_ford step 9204 current loss 0.030446, current_train_items 294560.
I0302 19:01:50.423067 22683591385216 run.py:483] Algo bellman_ford step 9205 current loss 0.010764, current_train_items 294592.
I0302 19:01:50.438538 22683591385216 run.py:483] Algo bellman_ford step 9206 current loss 0.020684, current_train_items 294624.
I0302 19:01:50.461351 22683591385216 run.py:483] Algo bellman_ford step 9207 current loss 0.023463, current_train_items 294656.
I0302 19:01:50.491391 22683591385216 run.py:483] Algo bellman_ford step 9208 current loss 0.051498, current_train_items 294688.
I0302 19:01:50.521017 22683591385216 run.py:483] Algo bellman_ford step 9209 current loss 0.065927, current_train_items 294720.
I0302 19:01:50.539072 22683591385216 run.py:483] Algo bellman_ford step 9210 current loss 0.002416, current_train_items 294752.
I0302 19:01:50.554803 22683591385216 run.py:483] Algo bellman_ford step 9211 current loss 0.011078, current_train_items 294784.
I0302 19:01:50.577041 22683591385216 run.py:483] Algo bellman_ford step 9212 current loss 0.035889, current_train_items 294816.
I0302 19:01:50.606786 22683591385216 run.py:483] Algo bellman_ford step 9213 current loss 0.061049, current_train_items 294848.
I0302 19:01:50.637204 22683591385216 run.py:483] Algo bellman_ford step 9214 current loss 0.106766, current_train_items 294880.
I0302 19:01:50.655093 22683591385216 run.py:483] Algo bellman_ford step 9215 current loss 0.025748, current_train_items 294912.
I0302 19:01:50.670578 22683591385216 run.py:483] Algo bellman_ford step 9216 current loss 0.017024, current_train_items 294944.
I0302 19:01:50.692789 22683591385216 run.py:483] Algo bellman_ford step 9217 current loss 0.017183, current_train_items 294976.
I0302 19:01:50.722012 22683591385216 run.py:483] Algo bellman_ford step 9218 current loss 0.036884, current_train_items 295008.
I0302 19:01:50.753195 22683591385216 run.py:483] Algo bellman_ford step 9219 current loss 0.029818, current_train_items 295040.
I0302 19:01:50.771316 22683591385216 run.py:483] Algo bellman_ford step 9220 current loss 0.043772, current_train_items 295072.
I0302 19:01:50.786400 22683591385216 run.py:483] Algo bellman_ford step 9221 current loss 0.022395, current_train_items 295104.
I0302 19:01:50.809164 22683591385216 run.py:483] Algo bellman_ford step 9222 current loss 0.088830, current_train_items 295136.
I0302 19:01:50.838301 22683591385216 run.py:483] Algo bellman_ford step 9223 current loss 0.032383, current_train_items 295168.
I0302 19:01:50.869781 22683591385216 run.py:483] Algo bellman_ford step 9224 current loss 0.035370, current_train_items 295200.
I0302 19:01:50.887868 22683591385216 run.py:483] Algo bellman_ford step 9225 current loss 0.003479, current_train_items 295232.
I0302 19:01:50.903229 22683591385216 run.py:483] Algo bellman_ford step 9226 current loss 0.007140, current_train_items 295264.
I0302 19:01:50.926136 22683591385216 run.py:483] Algo bellman_ford step 9227 current loss 0.049760, current_train_items 295296.
I0302 19:01:50.955847 22683591385216 run.py:483] Algo bellman_ford step 9228 current loss 0.070110, current_train_items 295328.
I0302 19:01:50.984694 22683591385216 run.py:483] Algo bellman_ford step 9229 current loss 0.050028, current_train_items 295360.
I0302 19:01:51.002693 22683591385216 run.py:483] Algo bellman_ford step 9230 current loss 0.004902, current_train_items 295392.
I0302 19:01:51.018428 22683591385216 run.py:483] Algo bellman_ford step 9231 current loss 0.024446, current_train_items 295424.
I0302 19:01:51.041326 22683591385216 run.py:483] Algo bellman_ford step 9232 current loss 0.040375, current_train_items 295456.
I0302 19:01:51.073242 22683591385216 run.py:483] Algo bellman_ford step 9233 current loss 0.079470, current_train_items 295488.
I0302 19:01:51.104834 22683591385216 run.py:483] Algo bellman_ford step 9234 current loss 0.039964, current_train_items 295520.
I0302 19:01:51.122664 22683591385216 run.py:483] Algo bellman_ford step 9235 current loss 0.002874, current_train_items 295552.
I0302 19:01:51.138128 22683591385216 run.py:483] Algo bellman_ford step 9236 current loss 0.019474, current_train_items 295584.
I0302 19:01:51.160579 22683591385216 run.py:483] Algo bellman_ford step 9237 current loss 0.018009, current_train_items 295616.
I0302 19:01:51.191863 22683591385216 run.py:483] Algo bellman_ford step 9238 current loss 0.038334, current_train_items 295648.
I0302 19:01:51.223982 22683591385216 run.py:483] Algo bellman_ford step 9239 current loss 0.035733, current_train_items 295680.
I0302 19:01:51.242468 22683591385216 run.py:483] Algo bellman_ford step 9240 current loss 0.002524, current_train_items 295712.
I0302 19:01:51.258290 22683591385216 run.py:483] Algo bellman_ford step 9241 current loss 0.011954, current_train_items 295744.
I0302 19:01:51.281193 22683591385216 run.py:483] Algo bellman_ford step 9242 current loss 0.050702, current_train_items 295776.
I0302 19:01:51.310966 22683591385216 run.py:483] Algo bellman_ford step 9243 current loss 0.022943, current_train_items 295808.
I0302 19:01:51.341435 22683591385216 run.py:483] Algo bellman_ford step 9244 current loss 0.043184, current_train_items 295840.
I0302 19:01:51.359815 22683591385216 run.py:483] Algo bellman_ford step 9245 current loss 0.002376, current_train_items 295872.
I0302 19:01:51.375224 22683591385216 run.py:483] Algo bellman_ford step 9246 current loss 0.015646, current_train_items 295904.
I0302 19:01:51.396926 22683591385216 run.py:483] Algo bellman_ford step 9247 current loss 0.012831, current_train_items 295936.
I0302 19:01:51.426966 22683591385216 run.py:483] Algo bellman_ford step 9248 current loss 0.035178, current_train_items 295968.
I0302 19:01:51.458904 22683591385216 run.py:483] Algo bellman_ford step 9249 current loss 0.039518, current_train_items 296000.
I0302 19:01:51.477086 22683591385216 run.py:483] Algo bellman_ford step 9250 current loss 0.003013, current_train_items 296032.
I0302 19:01:51.485063 22683591385216 run.py:503] (val) algo bellman_ford step 9250: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 296032, 'step': 9250, 'algorithm': 'bellman_ford'}
I0302 19:01:51.485167 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:51.501046 22683591385216 run.py:483] Algo bellman_ford step 9251 current loss 0.014842, current_train_items 296064.
I0302 19:01:51.524208 22683591385216 run.py:483] Algo bellman_ford step 9252 current loss 0.048328, current_train_items 296096.
I0302 19:01:51.553265 22683591385216 run.py:483] Algo bellman_ford step 9253 current loss 0.029905, current_train_items 296128.
I0302 19:01:51.583460 22683591385216 run.py:483] Algo bellman_ford step 9254 current loss 0.032939, current_train_items 296160.
I0302 19:01:51.601870 22683591385216 run.py:483] Algo bellman_ford step 9255 current loss 0.002516, current_train_items 296192.
I0302 19:01:51.617147 22683591385216 run.py:483] Algo bellman_ford step 9256 current loss 0.031826, current_train_items 296224.
I0302 19:01:51.639809 22683591385216 run.py:483] Algo bellman_ford step 9257 current loss 0.035750, current_train_items 296256.
I0302 19:01:51.669955 22683591385216 run.py:483] Algo bellman_ford step 9258 current loss 0.059497, current_train_items 296288.
I0302 19:01:51.701683 22683591385216 run.py:483] Algo bellman_ford step 9259 current loss 0.029579, current_train_items 296320.
I0302 19:01:51.720820 22683591385216 run.py:483] Algo bellman_ford step 9260 current loss 0.001375, current_train_items 296352.
I0302 19:01:51.737191 22683591385216 run.py:483] Algo bellman_ford step 9261 current loss 0.008304, current_train_items 296384.
I0302 19:01:51.758664 22683591385216 run.py:483] Algo bellman_ford step 9262 current loss 0.029415, current_train_items 296416.
I0302 19:01:51.788661 22683591385216 run.py:483] Algo bellman_ford step 9263 current loss 0.109286, current_train_items 296448.
I0302 19:01:51.819996 22683591385216 run.py:483] Algo bellman_ford step 9264 current loss 0.109437, current_train_items 296480.
I0302 19:01:51.838405 22683591385216 run.py:483] Algo bellman_ford step 9265 current loss 0.005567, current_train_items 296512.
I0302 19:01:51.853809 22683591385216 run.py:483] Algo bellman_ford step 9266 current loss 0.008028, current_train_items 296544.
I0302 19:01:51.875375 22683591385216 run.py:483] Algo bellman_ford step 9267 current loss 0.031318, current_train_items 296576.
I0302 19:01:51.905115 22683591385216 run.py:483] Algo bellman_ford step 9268 current loss 0.048606, current_train_items 296608.
I0302 19:01:51.936495 22683591385216 run.py:483] Algo bellman_ford step 9269 current loss 0.081503, current_train_items 296640.
I0302 19:01:51.955244 22683591385216 run.py:483] Algo bellman_ford step 9270 current loss 0.002057, current_train_items 296672.
I0302 19:01:51.970830 22683591385216 run.py:483] Algo bellman_ford step 9271 current loss 0.012215, current_train_items 296704.
I0302 19:01:51.992884 22683591385216 run.py:483] Algo bellman_ford step 9272 current loss 0.014667, current_train_items 296736.
I0302 19:01:52.024260 22683591385216 run.py:483] Algo bellman_ford step 9273 current loss 0.042548, current_train_items 296768.
I0302 19:01:52.056790 22683591385216 run.py:483] Algo bellman_ford step 9274 current loss 0.028399, current_train_items 296800.
I0302 19:01:52.075666 22683591385216 run.py:483] Algo bellman_ford step 9275 current loss 0.005552, current_train_items 296832.
I0302 19:01:52.091377 22683591385216 run.py:483] Algo bellman_ford step 9276 current loss 0.016916, current_train_items 296864.
I0302 19:01:52.113312 22683591385216 run.py:483] Algo bellman_ford step 9277 current loss 0.041991, current_train_items 296896.
I0302 19:01:52.143718 22683591385216 run.py:483] Algo bellman_ford step 9278 current loss 0.028317, current_train_items 296928.
I0302 19:01:52.175708 22683591385216 run.py:483] Algo bellman_ford step 9279 current loss 0.066533, current_train_items 296960.
I0302 19:01:52.193986 22683591385216 run.py:483] Algo bellman_ford step 9280 current loss 0.002520, current_train_items 296992.
I0302 19:01:52.209534 22683591385216 run.py:483] Algo bellman_ford step 9281 current loss 0.021751, current_train_items 297024.
I0302 19:01:52.232134 22683591385216 run.py:483] Algo bellman_ford step 9282 current loss 0.010535, current_train_items 297056.
I0302 19:01:52.260198 22683591385216 run.py:483] Algo bellman_ford step 9283 current loss 0.016806, current_train_items 297088.
I0302 19:01:52.290921 22683591385216 run.py:483] Algo bellman_ford step 9284 current loss 0.048975, current_train_items 297120.
I0302 19:01:52.309796 22683591385216 run.py:483] Algo bellman_ford step 9285 current loss 0.001860, current_train_items 297152.
I0302 19:01:52.324828 22683591385216 run.py:483] Algo bellman_ford step 9286 current loss 0.004526, current_train_items 297184.
I0302 19:01:52.348326 22683591385216 run.py:483] Algo bellman_ford step 9287 current loss 0.050816, current_train_items 297216.
I0302 19:01:52.377249 22683591385216 run.py:483] Algo bellman_ford step 9288 current loss 0.035392, current_train_items 297248.
I0302 19:01:52.410419 22683591385216 run.py:483] Algo bellman_ford step 9289 current loss 0.034295, current_train_items 297280.
I0302 19:01:52.429007 22683591385216 run.py:483] Algo bellman_ford step 9290 current loss 0.001171, current_train_items 297312.
I0302 19:01:52.444583 22683591385216 run.py:483] Algo bellman_ford step 9291 current loss 0.020978, current_train_items 297344.
I0302 19:01:52.467546 22683591385216 run.py:483] Algo bellman_ford step 9292 current loss 0.053186, current_train_items 297376.
I0302 19:01:52.496235 22683591385216 run.py:483] Algo bellman_ford step 9293 current loss 0.039506, current_train_items 297408.
I0302 19:01:52.528767 22683591385216 run.py:483] Algo bellman_ford step 9294 current loss 0.045994, current_train_items 297440.
I0302 19:01:52.547103 22683591385216 run.py:483] Algo bellman_ford step 9295 current loss 0.001979, current_train_items 297472.
I0302 19:01:52.562478 22683591385216 run.py:483] Algo bellman_ford step 9296 current loss 0.010547, current_train_items 297504.
I0302 19:01:52.584931 22683591385216 run.py:483] Algo bellman_ford step 9297 current loss 0.045266, current_train_items 297536.
I0302 19:01:52.615088 22683591385216 run.py:483] Algo bellman_ford step 9298 current loss 0.065057, current_train_items 297568.
I0302 19:01:52.647115 22683591385216 run.py:483] Algo bellman_ford step 9299 current loss 0.039891, current_train_items 297600.
I0302 19:01:52.665884 22683591385216 run.py:483] Algo bellman_ford step 9300 current loss 0.001882, current_train_items 297632.
I0302 19:01:52.673540 22683591385216 run.py:503] (val) algo bellman_ford step 9300: {'pi': 0.994140625, 'score': 0.994140625, 'examples_seen': 297632, 'step': 9300, 'algorithm': 'bellman_ford'}
I0302 19:01:52.673647 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.994, val scores are: bellman_ford: 0.994
I0302 19:01:52.689679 22683591385216 run.py:483] Algo bellman_ford step 9301 current loss 0.007428, current_train_items 297664.
I0302 19:01:52.712623 22683591385216 run.py:483] Algo bellman_ford step 9302 current loss 0.019523, current_train_items 297696.
I0302 19:01:52.742976 22683591385216 run.py:483] Algo bellman_ford step 9303 current loss 0.039638, current_train_items 297728.
I0302 19:01:52.776118 22683591385216 run.py:483] Algo bellman_ford step 9304 current loss 0.091864, current_train_items 297760.
I0302 19:01:52.794550 22683591385216 run.py:483] Algo bellman_ford step 9305 current loss 0.001886, current_train_items 297792.
I0302 19:01:52.809565 22683591385216 run.py:483] Algo bellman_ford step 9306 current loss 0.020062, current_train_items 297824.
I0302 19:01:52.832418 22683591385216 run.py:483] Algo bellman_ford step 9307 current loss 0.015761, current_train_items 297856.
I0302 19:01:52.862302 22683591385216 run.py:483] Algo bellman_ford step 9308 current loss 0.070161, current_train_items 297888.
I0302 19:01:52.893516 22683591385216 run.py:483] Algo bellman_ford step 9309 current loss 0.034142, current_train_items 297920.
I0302 19:01:52.911623 22683591385216 run.py:483] Algo bellman_ford step 9310 current loss 0.001309, current_train_items 297952.
I0302 19:01:52.926827 22683591385216 run.py:483] Algo bellman_ford step 9311 current loss 0.005654, current_train_items 297984.
I0302 19:01:52.949203 22683591385216 run.py:483] Algo bellman_ford step 9312 current loss 0.027276, current_train_items 298016.
I0302 19:01:52.977917 22683591385216 run.py:483] Algo bellman_ford step 9313 current loss 0.031373, current_train_items 298048.
I0302 19:01:53.009936 22683591385216 run.py:483] Algo bellman_ford step 9314 current loss 0.047123, current_train_items 298080.
I0302 19:01:53.028444 22683591385216 run.py:483] Algo bellman_ford step 9315 current loss 0.002567, current_train_items 298112.
I0302 19:01:53.043529 22683591385216 run.py:483] Algo bellman_ford step 9316 current loss 0.002686, current_train_items 298144.
I0302 19:01:53.065963 22683591385216 run.py:483] Algo bellman_ford step 9317 current loss 0.032420, current_train_items 298176.
I0302 19:01:53.096534 22683591385216 run.py:483] Algo bellman_ford step 9318 current loss 0.036077, current_train_items 298208.
I0302 19:01:53.127932 22683591385216 run.py:483] Algo bellman_ford step 9319 current loss 0.021460, current_train_items 298240.
I0302 19:01:53.146039 22683591385216 run.py:483] Algo bellman_ford step 9320 current loss 0.002061, current_train_items 298272.
I0302 19:01:53.161519 22683591385216 run.py:483] Algo bellman_ford step 9321 current loss 0.015605, current_train_items 298304.
I0302 19:01:53.182676 22683591385216 run.py:483] Algo bellman_ford step 9322 current loss 0.026477, current_train_items 298336.
I0302 19:01:53.211519 22683591385216 run.py:483] Algo bellman_ford step 9323 current loss 0.021073, current_train_items 298368.
I0302 19:01:53.246027 22683591385216 run.py:483] Algo bellman_ford step 9324 current loss 0.047873, current_train_items 298400.
I0302 19:01:53.264135 22683591385216 run.py:483] Algo bellman_ford step 9325 current loss 0.001207, current_train_items 298432.
I0302 19:01:53.279370 22683591385216 run.py:483] Algo bellman_ford step 9326 current loss 0.018374, current_train_items 298464.
I0302 19:01:53.302368 22683591385216 run.py:483] Algo bellman_ford step 9327 current loss 0.048665, current_train_items 298496.
I0302 19:01:53.333349 22683591385216 run.py:483] Algo bellman_ford step 9328 current loss 0.037048, current_train_items 298528.
I0302 19:01:53.361860 22683591385216 run.py:483] Algo bellman_ford step 9329 current loss 0.028797, current_train_items 298560.
I0302 19:01:53.380152 22683591385216 run.py:483] Algo bellman_ford step 9330 current loss 0.000903, current_train_items 298592.
I0302 19:01:53.395672 22683591385216 run.py:483] Algo bellman_ford step 9331 current loss 0.028754, current_train_items 298624.
I0302 19:01:53.417726 22683591385216 run.py:483] Algo bellman_ford step 9332 current loss 0.065845, current_train_items 298656.
I0302 19:01:53.447600 22683591385216 run.py:483] Algo bellman_ford step 9333 current loss 0.059118, current_train_items 298688.
I0302 19:01:53.478427 22683591385216 run.py:483] Algo bellman_ford step 9334 current loss 0.100156, current_train_items 298720.
I0302 19:01:53.496539 22683591385216 run.py:483] Algo bellman_ford step 9335 current loss 0.002259, current_train_items 298752.
I0302 19:01:53.511924 22683591385216 run.py:483] Algo bellman_ford step 9336 current loss 0.016691, current_train_items 298784.
I0302 19:01:53.535251 22683591385216 run.py:483] Algo bellman_ford step 9337 current loss 0.029722, current_train_items 298816.
I0302 19:01:53.565332 22683591385216 run.py:483] Algo bellman_ford step 9338 current loss 0.039287, current_train_items 298848.
I0302 19:01:53.597932 22683591385216 run.py:483] Algo bellman_ford step 9339 current loss 0.082312, current_train_items 298880.
I0302 19:01:53.616086 22683591385216 run.py:483] Algo bellman_ford step 9340 current loss 0.003097, current_train_items 298912.
I0302 19:01:53.631278 22683591385216 run.py:483] Algo bellman_ford step 9341 current loss 0.007066, current_train_items 298944.
I0302 19:01:53.653816 22683591385216 run.py:483] Algo bellman_ford step 9342 current loss 0.020848, current_train_items 298976.
I0302 19:01:53.683572 22683591385216 run.py:483] Algo bellman_ford step 9343 current loss 0.026988, current_train_items 299008.
I0302 19:01:53.715437 22683591385216 run.py:483] Algo bellman_ford step 9344 current loss 0.028559, current_train_items 299040.
I0302 19:01:53.733659 22683591385216 run.py:483] Algo bellman_ford step 9345 current loss 0.001434, current_train_items 299072.
I0302 19:01:53.748927 22683591385216 run.py:483] Algo bellman_ford step 9346 current loss 0.004424, current_train_items 299104.
I0302 19:01:53.771622 22683591385216 run.py:483] Algo bellman_ford step 9347 current loss 0.019414, current_train_items 299136.
I0302 19:01:53.801772 22683591385216 run.py:483] Algo bellman_ford step 9348 current loss 0.033758, current_train_items 299168.
I0302 19:01:53.832299 22683591385216 run.py:483] Algo bellman_ford step 9349 current loss 0.027965, current_train_items 299200.
I0302 19:01:53.850497 22683591385216 run.py:483] Algo bellman_ford step 9350 current loss 0.060221, current_train_items 299232.
I0302 19:01:53.858471 22683591385216 run.py:503] (val) algo bellman_ford step 9350: {'pi': 0.9912109375, 'score': 0.9912109375, 'examples_seen': 299232, 'step': 9350, 'algorithm': 'bellman_ford'}
I0302 19:01:53.858586 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.991, val scores are: bellman_ford: 0.991
I0302 19:01:53.874456 22683591385216 run.py:483] Algo bellman_ford step 9351 current loss 0.001482, current_train_items 299264.
I0302 19:01:53.898387 22683591385216 run.py:483] Algo bellman_ford step 9352 current loss 0.055885, current_train_items 299296.
I0302 19:01:53.929311 22683591385216 run.py:483] Algo bellman_ford step 9353 current loss 0.044761, current_train_items 299328.
I0302 19:01:53.962928 22683591385216 run.py:483] Algo bellman_ford step 9354 current loss 0.065648, current_train_items 299360.
I0302 19:01:53.981575 22683591385216 run.py:483] Algo bellman_ford step 9355 current loss 0.070965, current_train_items 299392.
I0302 19:01:53.996708 22683591385216 run.py:483] Algo bellman_ford step 9356 current loss 0.026296, current_train_items 299424.
I0302 19:01:54.019535 22683591385216 run.py:483] Algo bellman_ford step 9357 current loss 0.100083, current_train_items 299456.
I0302 19:01:54.049633 22683591385216 run.py:483] Algo bellman_ford step 9358 current loss 0.029091, current_train_items 299488.
I0302 19:01:54.080864 22683591385216 run.py:483] Algo bellman_ford step 9359 current loss 0.038610, current_train_items 299520.
I0302 19:01:54.100319 22683591385216 run.py:483] Algo bellman_ford step 9360 current loss 0.020445, current_train_items 299552.
I0302 19:01:54.116104 22683591385216 run.py:483] Algo bellman_ford step 9361 current loss 0.007730, current_train_items 299584.
I0302 19:01:54.138054 22683591385216 run.py:483] Algo bellman_ford step 9362 current loss 0.028284, current_train_items 299616.
I0302 19:01:54.168729 22683591385216 run.py:483] Algo bellman_ford step 9363 current loss 0.037096, current_train_items 299648.
I0302 19:01:54.201351 22683591385216 run.py:483] Algo bellman_ford step 9364 current loss 0.038499, current_train_items 299680.
I0302 19:01:54.219990 22683591385216 run.py:483] Algo bellman_ford step 9365 current loss 0.001503, current_train_items 299712.
I0302 19:01:54.235835 22683591385216 run.py:483] Algo bellman_ford step 9366 current loss 0.032518, current_train_items 299744.
I0302 19:01:54.260177 22683591385216 run.py:483] Algo bellman_ford step 9367 current loss 0.033210, current_train_items 299776.
I0302 19:01:54.291227 22683591385216 run.py:483] Algo bellman_ford step 9368 current loss 0.039495, current_train_items 299808.
I0302 19:01:54.322397 22683591385216 run.py:483] Algo bellman_ford step 9369 current loss 0.043434, current_train_items 299840.
I0302 19:01:54.341678 22683591385216 run.py:483] Algo bellman_ford step 9370 current loss 0.005342, current_train_items 299872.
I0302 19:01:54.357788 22683591385216 run.py:483] Algo bellman_ford step 9371 current loss 0.025716, current_train_items 299904.
I0302 19:01:54.380678 22683591385216 run.py:483] Algo bellman_ford step 9372 current loss 0.010777, current_train_items 299936.
I0302 19:01:54.411362 22683591385216 run.py:483] Algo bellman_ford step 9373 current loss 0.052563, current_train_items 299968.
I0302 19:01:54.442916 22683591385216 run.py:483] Algo bellman_ford step 9374 current loss 0.038525, current_train_items 300000.
I0302 19:01:54.461782 22683591385216 run.py:483] Algo bellman_ford step 9375 current loss 0.017529, current_train_items 300032.
I0302 19:01:54.477014 22683591385216 run.py:483] Algo bellman_ford step 9376 current loss 0.032759, current_train_items 300064.
I0302 19:01:54.499549 22683591385216 run.py:483] Algo bellman_ford step 9377 current loss 0.031589, current_train_items 300096.
I0302 19:01:54.528990 22683591385216 run.py:483] Algo bellman_ford step 9378 current loss 0.036383, current_train_items 300128.
I0302 19:01:54.560981 22683591385216 run.py:483] Algo bellman_ford step 9379 current loss 0.052811, current_train_items 300160.
I0302 19:01:54.579409 22683591385216 run.py:483] Algo bellman_ford step 9380 current loss 0.001107, current_train_items 300192.
I0302 19:01:54.594301 22683591385216 run.py:483] Algo bellman_ford step 9381 current loss 0.028958, current_train_items 300224.
I0302 19:01:54.617048 22683591385216 run.py:483] Algo bellman_ford step 9382 current loss 0.029537, current_train_items 300256.
I0302 19:01:54.646357 22683591385216 run.py:483] Algo bellman_ford step 9383 current loss 0.031579, current_train_items 300288.
I0302 19:01:54.678339 22683591385216 run.py:483] Algo bellman_ford step 9384 current loss 0.041086, current_train_items 300320.
I0302 19:01:54.697446 22683591385216 run.py:483] Algo bellman_ford step 9385 current loss 0.001631, current_train_items 300352.
I0302 19:01:54.713507 22683591385216 run.py:483] Algo bellman_ford step 9386 current loss 0.054646, current_train_items 300384.
I0302 19:01:54.736101 22683591385216 run.py:483] Algo bellman_ford step 9387 current loss 0.037903, current_train_items 300416.
I0302 19:01:54.766165 22683591385216 run.py:483] Algo bellman_ford step 9388 current loss 0.066144, current_train_items 300448.
I0302 19:01:54.799697 22683591385216 run.py:483] Algo bellman_ford step 9389 current loss 0.065904, current_train_items 300480.
I0302 19:01:54.818678 22683591385216 run.py:483] Algo bellman_ford step 9390 current loss 0.005502, current_train_items 300512.
I0302 19:01:54.834600 22683591385216 run.py:483] Algo bellman_ford step 9391 current loss 0.006571, current_train_items 300544.
I0302 19:01:54.857013 22683591385216 run.py:483] Algo bellman_ford step 9392 current loss 0.009774, current_train_items 300576.
I0302 19:01:54.886737 22683591385216 run.py:483] Algo bellman_ford step 9393 current loss 0.076051, current_train_items 300608.
I0302 19:01:54.918209 22683591385216 run.py:483] Algo bellman_ford step 9394 current loss 0.104316, current_train_items 300640.
I0302 19:01:54.936608 22683591385216 run.py:483] Algo bellman_ford step 9395 current loss 0.003451, current_train_items 300672.
I0302 19:01:54.952167 22683591385216 run.py:483] Algo bellman_ford step 9396 current loss 0.056181, current_train_items 300704.
I0302 19:01:54.975833 22683591385216 run.py:483] Algo bellman_ford step 9397 current loss 0.062252, current_train_items 300736.
I0302 19:01:55.005278 22683591385216 run.py:483] Algo bellman_ford step 9398 current loss 0.056656, current_train_items 300768.
I0302 19:01:55.037148 22683591385216 run.py:483] Algo bellman_ford step 9399 current loss 0.059431, current_train_items 300800.
I0302 19:01:55.056009 22683591385216 run.py:483] Algo bellman_ford step 9400 current loss 0.006109, current_train_items 300832.
I0302 19:01:55.063825 22683591385216 run.py:503] (val) algo bellman_ford step 9400: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 300832, 'step': 9400, 'algorithm': 'bellman_ford'}
I0302 19:01:55.063944 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:01:55.080015 22683591385216 run.py:483] Algo bellman_ford step 9401 current loss 0.026756, current_train_items 300864.
I0302 19:01:55.104028 22683591385216 run.py:483] Algo bellman_ford step 9402 current loss 0.057832, current_train_items 300896.
I0302 19:01:55.133972 22683591385216 run.py:483] Algo bellman_ford step 9403 current loss 0.017203, current_train_items 300928.
I0302 19:01:55.166689 22683591385216 run.py:483] Algo bellman_ford step 9404 current loss 0.049233, current_train_items 300960.
I0302 19:01:55.185156 22683591385216 run.py:483] Algo bellman_ford step 9405 current loss 0.003240, current_train_items 300992.
I0302 19:01:55.200504 22683591385216 run.py:483] Algo bellman_ford step 9406 current loss 0.012258, current_train_items 301024.
I0302 19:01:55.224456 22683591385216 run.py:483] Algo bellman_ford step 9407 current loss 0.041660, current_train_items 301056.
I0302 19:01:55.253729 22683591385216 run.py:483] Algo bellman_ford step 9408 current loss 0.038205, current_train_items 301088.
I0302 19:01:55.286145 22683591385216 run.py:483] Algo bellman_ford step 9409 current loss 0.057391, current_train_items 301120.
I0302 19:01:55.304275 22683591385216 run.py:483] Algo bellman_ford step 9410 current loss 0.004838, current_train_items 301152.
I0302 19:01:55.319803 22683591385216 run.py:483] Algo bellman_ford step 9411 current loss 0.005485, current_train_items 301184.
I0302 19:01:55.342847 22683591385216 run.py:483] Algo bellman_ford step 9412 current loss 0.024357, current_train_items 301216.
I0302 19:01:55.372464 22683591385216 run.py:483] Algo bellman_ford step 9413 current loss 0.023189, current_train_items 301248.
I0302 19:01:55.403100 22683591385216 run.py:483] Algo bellman_ford step 9414 current loss 0.056672, current_train_items 301280.
I0302 19:01:55.421227 22683591385216 run.py:483] Algo bellman_ford step 9415 current loss 0.022271, current_train_items 301312.
I0302 19:01:55.436787 22683591385216 run.py:483] Algo bellman_ford step 9416 current loss 0.016443, current_train_items 301344.
I0302 19:01:55.460011 22683591385216 run.py:483] Algo bellman_ford step 9417 current loss 0.059626, current_train_items 301376.
I0302 19:01:55.489238 22683591385216 run.py:483] Algo bellman_ford step 9418 current loss 0.049101, current_train_items 301408.
I0302 19:01:55.518895 22683591385216 run.py:483] Algo bellman_ford step 9419 current loss 0.031227, current_train_items 301440.
I0302 19:01:55.536985 22683591385216 run.py:483] Algo bellman_ford step 9420 current loss 0.004290, current_train_items 301472.
I0302 19:01:55.552620 22683591385216 run.py:483] Algo bellman_ford step 9421 current loss 0.019576, current_train_items 301504.
I0302 19:01:55.575419 22683591385216 run.py:483] Algo bellman_ford step 9422 current loss 0.012938, current_train_items 301536.
I0302 19:01:55.605046 22683591385216 run.py:483] Algo bellman_ford step 9423 current loss 0.027882, current_train_items 301568.
I0302 19:01:55.636445 22683591385216 run.py:483] Algo bellman_ford step 9424 current loss 0.043502, current_train_items 301600.
I0302 19:01:55.654715 22683591385216 run.py:483] Algo bellman_ford step 9425 current loss 0.010079, current_train_items 301632.
I0302 19:01:55.669717 22683591385216 run.py:483] Algo bellman_ford step 9426 current loss 0.013967, current_train_items 301664.
I0302 19:01:55.691742 22683591385216 run.py:483] Algo bellman_ford step 9427 current loss 0.028711, current_train_items 301696.
I0302 19:01:55.722087 22683591385216 run.py:483] Algo bellman_ford step 9428 current loss 0.049674, current_train_items 301728.
I0302 19:01:55.751152 22683591385216 run.py:483] Algo bellman_ford step 9429 current loss 0.076595, current_train_items 301760.
I0302 19:01:55.769292 22683591385216 run.py:483] Algo bellman_ford step 9430 current loss 0.001607, current_train_items 301792.
I0302 19:01:55.784615 22683591385216 run.py:483] Algo bellman_ford step 9431 current loss 0.009379, current_train_items 301824.
I0302 19:01:55.806892 22683591385216 run.py:483] Algo bellman_ford step 9432 current loss 0.034819, current_train_items 301856.
I0302 19:01:55.836088 22683591385216 run.py:483] Algo bellman_ford step 9433 current loss 0.042019, current_train_items 301888.
I0302 19:01:55.868309 22683591385216 run.py:483] Algo bellman_ford step 9434 current loss 0.042782, current_train_items 301920.
I0302 19:01:55.886497 22683591385216 run.py:483] Algo bellman_ford step 9435 current loss 0.000741, current_train_items 301952.
I0302 19:01:55.902067 22683591385216 run.py:483] Algo bellman_ford step 9436 current loss 0.018621, current_train_items 301984.
I0302 19:01:55.924751 22683591385216 run.py:483] Algo bellman_ford step 9437 current loss 0.007948, current_train_items 302016.
I0302 19:01:55.955108 22683591385216 run.py:483] Algo bellman_ford step 9438 current loss 0.040421, current_train_items 302048.
I0302 19:01:55.985560 22683591385216 run.py:483] Algo bellman_ford step 9439 current loss 0.055013, current_train_items 302080.
I0302 19:01:56.003939 22683591385216 run.py:483] Algo bellman_ford step 9440 current loss 0.029059, current_train_items 302112.
I0302 19:01:56.019687 22683591385216 run.py:483] Algo bellman_ford step 9441 current loss 0.009310, current_train_items 302144.
I0302 19:01:56.042248 22683591385216 run.py:483] Algo bellman_ford step 9442 current loss 0.036349, current_train_items 302176.
I0302 19:01:56.072790 22683591385216 run.py:483] Algo bellman_ford step 9443 current loss 0.017774, current_train_items 302208.
I0302 19:01:56.106649 22683591385216 run.py:483] Algo bellman_ford step 9444 current loss 0.040939, current_train_items 302240.
I0302 19:01:56.124689 22683591385216 run.py:483] Algo bellman_ford step 9445 current loss 0.053946, current_train_items 302272.
I0302 19:01:56.139665 22683591385216 run.py:483] Algo bellman_ford step 9446 current loss 0.003869, current_train_items 302304.
I0302 19:01:56.161847 22683591385216 run.py:483] Algo bellman_ford step 9447 current loss 0.015339, current_train_items 302336.
I0302 19:01:56.191307 22683591385216 run.py:483] Algo bellman_ford step 9448 current loss 0.037569, current_train_items 302368.
I0302 19:01:56.221352 22683591385216 run.py:483] Algo bellman_ford step 9449 current loss 0.031833, current_train_items 302400.
I0302 19:01:56.239574 22683591385216 run.py:483] Algo bellman_ford step 9450 current loss 0.002388, current_train_items 302432.
I0302 19:01:56.247520 22683591385216 run.py:503] (val) algo bellman_ford step 9450: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 302432, 'step': 9450, 'algorithm': 'bellman_ford'}
I0302 19:01:56.247628 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:01:56.263724 22683591385216 run.py:483] Algo bellman_ford step 9451 current loss 0.003417, current_train_items 302464.
I0302 19:01:56.285991 22683591385216 run.py:483] Algo bellman_ford step 9452 current loss 0.026186, current_train_items 302496.
I0302 19:01:56.316636 22683591385216 run.py:483] Algo bellman_ford step 9453 current loss 0.022400, current_train_items 302528.
I0302 19:01:56.348886 22683591385216 run.py:483] Algo bellman_ford step 9454 current loss 0.046791, current_train_items 302560.
I0302 19:01:56.367887 22683591385216 run.py:483] Algo bellman_ford step 9455 current loss 0.000751, current_train_items 302592.
I0302 19:01:56.383256 22683591385216 run.py:483] Algo bellman_ford step 9456 current loss 0.004210, current_train_items 302624.
I0302 19:01:56.405969 22683591385216 run.py:483] Algo bellman_ford step 9457 current loss 0.031815, current_train_items 302656.
I0302 19:01:56.435840 22683591385216 run.py:483] Algo bellman_ford step 9458 current loss 0.023171, current_train_items 302688.
I0302 19:01:56.465703 22683591385216 run.py:483] Algo bellman_ford step 9459 current loss 0.028403, current_train_items 302720.
I0302 19:01:56.484812 22683591385216 run.py:483] Algo bellman_ford step 9460 current loss 0.000628, current_train_items 302752.
I0302 19:01:56.500718 22683591385216 run.py:483] Algo bellman_ford step 9461 current loss 0.017755, current_train_items 302784.
I0302 19:01:56.522451 22683591385216 run.py:483] Algo bellman_ford step 9462 current loss 0.012189, current_train_items 302816.
I0302 19:01:56.553412 22683591385216 run.py:483] Algo bellman_ford step 9463 current loss 0.038355, current_train_items 302848.
I0302 19:01:56.584230 22683591385216 run.py:483] Algo bellman_ford step 9464 current loss 0.034132, current_train_items 302880.
I0302 19:01:56.602581 22683591385216 run.py:483] Algo bellman_ford step 9465 current loss 0.001349, current_train_items 302912.
I0302 19:01:56.618335 22683591385216 run.py:483] Algo bellman_ford step 9466 current loss 0.026516, current_train_items 302944.
I0302 19:01:56.642565 22683591385216 run.py:483] Algo bellman_ford step 9467 current loss 0.041147, current_train_items 302976.
I0302 19:01:56.673536 22683591385216 run.py:483] Algo bellman_ford step 9468 current loss 0.022214, current_train_items 303008.
I0302 19:01:56.706066 22683591385216 run.py:483] Algo bellman_ford step 9469 current loss 0.051139, current_train_items 303040.
I0302 19:01:56.725120 22683591385216 run.py:483] Algo bellman_ford step 9470 current loss 0.002185, current_train_items 303072.
I0302 19:01:56.740534 22683591385216 run.py:483] Algo bellman_ford step 9471 current loss 0.009623, current_train_items 303104.
I0302 19:01:56.763077 22683591385216 run.py:483] Algo bellman_ford step 9472 current loss 0.033424, current_train_items 303136.
I0302 19:01:56.793272 22683591385216 run.py:483] Algo bellman_ford step 9473 current loss 0.031359, current_train_items 303168.
I0302 19:01:56.824122 22683591385216 run.py:483] Algo bellman_ford step 9474 current loss 0.043594, current_train_items 303200.
I0302 19:01:56.842867 22683591385216 run.py:483] Algo bellman_ford step 9475 current loss 0.002573, current_train_items 303232.
I0302 19:01:56.858648 22683591385216 run.py:483] Algo bellman_ford step 9476 current loss 0.024918, current_train_items 303264.
I0302 19:01:56.880987 22683591385216 run.py:483] Algo bellman_ford step 9477 current loss 0.016379, current_train_items 303296.
I0302 19:01:56.909978 22683591385216 run.py:483] Algo bellman_ford step 9478 current loss 0.047890, current_train_items 303328.
I0302 19:01:56.940763 22683591385216 run.py:483] Algo bellman_ford step 9479 current loss 0.076026, current_train_items 303360.
I0302 19:01:56.959151 22683591385216 run.py:483] Algo bellman_ford step 9480 current loss 0.002332, current_train_items 303392.
I0302 19:01:56.974488 22683591385216 run.py:483] Algo bellman_ford step 9481 current loss 0.010249, current_train_items 303424.
I0302 19:01:56.998728 22683591385216 run.py:483] Algo bellman_ford step 9482 current loss 0.064921, current_train_items 303456.
I0302 19:01:57.027332 22683591385216 run.py:483] Algo bellman_ford step 9483 current loss 0.031872, current_train_items 303488.
I0302 19:01:57.057534 22683591385216 run.py:483] Algo bellman_ford step 9484 current loss 0.044926, current_train_items 303520.
I0302 19:01:57.075938 22683591385216 run.py:483] Algo bellman_ford step 9485 current loss 0.001065, current_train_items 303552.
I0302 19:01:57.091919 22683591385216 run.py:483] Algo bellman_ford step 9486 current loss 0.006516, current_train_items 303584.
I0302 19:01:57.115731 22683591385216 run.py:483] Algo bellman_ford step 9487 current loss 0.030632, current_train_items 303616.
I0302 19:01:57.146847 22683591385216 run.py:483] Algo bellman_ford step 9488 current loss 0.042387, current_train_items 303648.
I0302 19:01:57.177749 22683591385216 run.py:483] Algo bellman_ford step 9489 current loss 0.063129, current_train_items 303680.
I0302 19:01:57.196339 22683591385216 run.py:483] Algo bellman_ford step 9490 current loss 0.001725, current_train_items 303712.
I0302 19:01:57.211775 22683591385216 run.py:483] Algo bellman_ford step 9491 current loss 0.021198, current_train_items 303744.
I0302 19:01:57.234783 22683591385216 run.py:483] Algo bellman_ford step 9492 current loss 0.012943, current_train_items 303776.
I0302 19:01:57.263855 22683591385216 run.py:483] Algo bellman_ford step 9493 current loss 0.015831, current_train_items 303808.
I0302 19:01:57.295269 22683591385216 run.py:483] Algo bellman_ford step 9494 current loss 0.085705, current_train_items 303840.
I0302 19:01:57.313816 22683591385216 run.py:483] Algo bellman_ford step 9495 current loss 0.012269, current_train_items 303872.
I0302 19:01:57.329735 22683591385216 run.py:483] Algo bellman_ford step 9496 current loss 0.016546, current_train_items 303904.
I0302 19:01:57.352059 22683591385216 run.py:483] Algo bellman_ford step 9497 current loss 0.021200, current_train_items 303936.
I0302 19:01:57.381098 22683591385216 run.py:483] Algo bellman_ford step 9498 current loss 0.041155, current_train_items 303968.
I0302 19:01:57.413274 22683591385216 run.py:483] Algo bellman_ford step 9499 current loss 0.045239, current_train_items 304000.
I0302 19:01:57.432638 22683591385216 run.py:483] Algo bellman_ford step 9500 current loss 0.000617, current_train_items 304032.
I0302 19:01:57.440230 22683591385216 run.py:503] (val) algo bellman_ford step 9500: {'pi': 0.9853515625, 'score': 0.9853515625, 'examples_seen': 304032, 'step': 9500, 'algorithm': 'bellman_ford'}
I0302 19:01:57.440368 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.985, val scores are: bellman_ford: 0.985
I0302 19:01:57.456181 22683591385216 run.py:483] Algo bellman_ford step 9501 current loss 0.012008, current_train_items 304064.
I0302 19:01:57.479508 22683591385216 run.py:483] Algo bellman_ford step 9502 current loss 0.048683, current_train_items 304096.
I0302 19:01:57.509623 22683591385216 run.py:483] Algo bellman_ford step 9503 current loss 0.021225, current_train_items 304128.
I0302 19:01:57.540163 22683591385216 run.py:483] Algo bellman_ford step 9504 current loss 0.029358, current_train_items 304160.
I0302 19:01:57.558852 22683591385216 run.py:483] Algo bellman_ford step 9505 current loss 0.004728, current_train_items 304192.
I0302 19:01:57.574491 22683591385216 run.py:483] Algo bellman_ford step 9506 current loss 0.012160, current_train_items 304224.
I0302 19:01:57.597624 22683591385216 run.py:483] Algo bellman_ford step 9507 current loss 0.018396, current_train_items 304256.
I0302 19:01:57.626350 22683591385216 run.py:483] Algo bellman_ford step 9508 current loss 0.049717, current_train_items 304288.
I0302 19:01:57.659966 22683591385216 run.py:483] Algo bellman_ford step 9509 current loss 0.100412, current_train_items 304320.
I0302 19:01:57.678442 22683591385216 run.py:483] Algo bellman_ford step 9510 current loss 0.000988, current_train_items 304352.
I0302 19:01:57.694367 22683591385216 run.py:483] Algo bellman_ford step 9511 current loss 0.006155, current_train_items 304384.
I0302 19:01:57.717439 22683591385216 run.py:483] Algo bellman_ford step 9512 current loss 0.034442, current_train_items 304416.
I0302 19:01:57.747312 22683591385216 run.py:483] Algo bellman_ford step 9513 current loss 0.041575, current_train_items 304448.
I0302 19:01:57.777677 22683591385216 run.py:483] Algo bellman_ford step 9514 current loss 0.038735, current_train_items 304480.
I0302 19:01:57.795760 22683591385216 run.py:483] Algo bellman_ford step 9515 current loss 0.022116, current_train_items 304512.
I0302 19:01:57.810740 22683591385216 run.py:483] Algo bellman_ford step 9516 current loss 0.006513, current_train_items 304544.
I0302 19:01:57.833323 22683591385216 run.py:483] Algo bellman_ford step 9517 current loss 0.018936, current_train_items 304576.
I0302 19:01:57.863144 22683591385216 run.py:483] Algo bellman_ford step 9518 current loss 0.051011, current_train_items 304608.
I0302 19:01:57.893128 22683591385216 run.py:483] Algo bellman_ford step 9519 current loss 0.029793, current_train_items 304640.
I0302 19:01:57.911159 22683591385216 run.py:483] Algo bellman_ford step 9520 current loss 0.000657, current_train_items 304672.
I0302 19:01:57.925871 22683591385216 run.py:483] Algo bellman_ford step 9521 current loss 0.001898, current_train_items 304704.
I0302 19:01:57.947796 22683591385216 run.py:483] Algo bellman_ford step 9522 current loss 0.017244, current_train_items 304736.
I0302 19:01:57.975912 22683591385216 run.py:483] Algo bellman_ford step 9523 current loss 0.028766, current_train_items 304768.
I0302 19:01:58.006431 22683591385216 run.py:483] Algo bellman_ford step 9524 current loss 0.073692, current_train_items 304800.
I0302 19:01:58.024422 22683591385216 run.py:483] Algo bellman_ford step 9525 current loss 0.001611, current_train_items 304832.
I0302 19:01:58.039839 22683591385216 run.py:483] Algo bellman_ford step 9526 current loss 0.003479, current_train_items 304864.
I0302 19:01:58.062837 22683591385216 run.py:483] Algo bellman_ford step 9527 current loss 0.049742, current_train_items 304896.
I0302 19:01:58.093267 22683591385216 run.py:483] Algo bellman_ford step 9528 current loss 0.043832, current_train_items 304928.
I0302 19:01:58.125789 22683591385216 run.py:483] Algo bellman_ford step 9529 current loss 0.047145, current_train_items 304960.
I0302 19:01:58.144015 22683591385216 run.py:483] Algo bellman_ford step 9530 current loss 0.000915, current_train_items 304992.
I0302 19:01:58.159368 22683591385216 run.py:483] Algo bellman_ford step 9531 current loss 0.011751, current_train_items 305024.
I0302 19:01:58.181426 22683591385216 run.py:483] Algo bellman_ford step 9532 current loss 0.029880, current_train_items 305056.
I0302 19:01:58.212915 22683591385216 run.py:483] Algo bellman_ford step 9533 current loss 0.121757, current_train_items 305088.
I0302 19:01:58.243952 22683591385216 run.py:483] Algo bellman_ford step 9534 current loss 0.057123, current_train_items 305120.
I0302 19:01:58.262411 22683591385216 run.py:483] Algo bellman_ford step 9535 current loss 0.014293, current_train_items 305152.
I0302 19:01:58.277701 22683591385216 run.py:483] Algo bellman_ford step 9536 current loss 0.007741, current_train_items 305184.
I0302 19:01:58.300405 22683591385216 run.py:483] Algo bellman_ford step 9537 current loss 0.057000, current_train_items 305216.
I0302 19:01:58.331063 22683591385216 run.py:483] Algo bellman_ford step 9538 current loss 0.074486, current_train_items 305248.
I0302 19:01:58.360097 22683591385216 run.py:483] Algo bellman_ford step 9539 current loss 0.044912, current_train_items 305280.
I0302 19:01:58.378412 22683591385216 run.py:483] Algo bellman_ford step 9540 current loss 0.002251, current_train_items 305312.
I0302 19:01:58.393624 22683591385216 run.py:483] Algo bellman_ford step 9541 current loss 0.047389, current_train_items 305344.
I0302 19:01:58.415658 22683591385216 run.py:483] Algo bellman_ford step 9542 current loss 0.025028, current_train_items 305376.
I0302 19:01:58.446727 22683591385216 run.py:483] Algo bellman_ford step 9543 current loss 0.060589, current_train_items 305408.
I0302 19:01:58.478231 22683591385216 run.py:483] Algo bellman_ford step 9544 current loss 0.072047, current_train_items 305440.
I0302 19:01:58.496774 22683591385216 run.py:483] Algo bellman_ford step 9545 current loss 0.001380, current_train_items 305472.
I0302 19:01:58.511604 22683591385216 run.py:483] Algo bellman_ford step 9546 current loss 0.027232, current_train_items 305504.
I0302 19:01:58.531979 22683591385216 run.py:483] Algo bellman_ford step 9547 current loss 0.030436, current_train_items 305536.
I0302 19:01:58.561226 22683591385216 run.py:483] Algo bellman_ford step 9548 current loss 0.045390, current_train_items 305568.
I0302 19:01:58.592852 22683591385216 run.py:483] Algo bellman_ford step 9549 current loss 0.033421, current_train_items 305600.
I0302 19:01:58.610972 22683591385216 run.py:483] Algo bellman_ford step 9550 current loss 0.001545, current_train_items 305632.
I0302 19:01:58.618875 22683591385216 run.py:503] (val) algo bellman_ford step 9550: {'pi': 0.9931640625, 'score': 0.9931640625, 'examples_seen': 305632, 'step': 9550, 'algorithm': 'bellman_ford'}
I0302 19:01:58.618994 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.993, val scores are: bellman_ford: 0.993
I0302 19:01:58.635052 22683591385216 run.py:483] Algo bellman_ford step 9551 current loss 0.018234, current_train_items 305664.
I0302 19:01:58.658431 22683591385216 run.py:483] Algo bellman_ford step 9552 current loss 0.070193, current_train_items 305696.
I0302 19:01:58.690432 22683591385216 run.py:483] Algo bellman_ford step 9553 current loss 0.050244, current_train_items 305728.
I0302 19:01:58.721551 22683591385216 run.py:483] Algo bellman_ford step 9554 current loss 0.026603, current_train_items 305760.
I0302 19:01:58.740336 22683591385216 run.py:483] Algo bellman_ford step 9555 current loss 0.001538, current_train_items 305792.
I0302 19:01:58.755200 22683591385216 run.py:483] Algo bellman_ford step 9556 current loss 0.019665, current_train_items 305824.
I0302 19:01:58.778517 22683591385216 run.py:483] Algo bellman_ford step 9557 current loss 0.026597, current_train_items 305856.
I0302 19:01:58.807995 22683591385216 run.py:483] Algo bellman_ford step 9558 current loss 0.050675, current_train_items 305888.
I0302 19:01:58.838886 22683591385216 run.py:483] Algo bellman_ford step 9559 current loss 0.063610, current_train_items 305920.
I0302 19:01:58.857630 22683591385216 run.py:483] Algo bellman_ford step 9560 current loss 0.002548, current_train_items 305952.
I0302 19:01:58.873359 22683591385216 run.py:483] Algo bellman_ford step 9561 current loss 0.004748, current_train_items 305984.
I0302 19:01:58.895802 22683591385216 run.py:483] Algo bellman_ford step 9562 current loss 0.020445, current_train_items 306016.
I0302 19:01:58.924809 22683591385216 run.py:483] Algo bellman_ford step 9563 current loss 0.027020, current_train_items 306048.
I0302 19:01:58.955550 22683591385216 run.py:483] Algo bellman_ford step 9564 current loss 0.057548, current_train_items 306080.
I0302 19:01:58.974078 22683591385216 run.py:483] Algo bellman_ford step 9565 current loss 0.008439, current_train_items 306112.
I0302 19:01:58.989963 22683591385216 run.py:483] Algo bellman_ford step 9566 current loss 0.020529, current_train_items 306144.
I0302 19:01:59.013587 22683591385216 run.py:483] Algo bellman_ford step 9567 current loss 0.057093, current_train_items 306176.
I0302 19:01:59.043960 22683591385216 run.py:483] Algo bellman_ford step 9568 current loss 0.097830, current_train_items 306208.
I0302 19:01:59.075260 22683591385216 run.py:483] Algo bellman_ford step 9569 current loss 0.112933, current_train_items 306240.
I0302 19:01:59.093995 22683591385216 run.py:483] Algo bellman_ford step 9570 current loss 0.003963, current_train_items 306272.
I0302 19:01:59.109680 22683591385216 run.py:483] Algo bellman_ford step 9571 current loss 0.005402, current_train_items 306304.
I0302 19:01:59.132973 22683591385216 run.py:483] Algo bellman_ford step 9572 current loss 0.057941, current_train_items 306336.
I0302 19:01:59.162065 22683591385216 run.py:483] Algo bellman_ford step 9573 current loss 0.030744, current_train_items 306368.
I0302 19:01:59.194139 22683591385216 run.py:483] Algo bellman_ford step 9574 current loss 0.066901, current_train_items 306400.
I0302 19:01:59.213033 22683591385216 run.py:483] Algo bellman_ford step 9575 current loss 0.004299, current_train_items 306432.
I0302 19:01:59.229464 22683591385216 run.py:483] Algo bellman_ford step 9576 current loss 0.018245, current_train_items 306464.
I0302 19:01:59.253081 22683591385216 run.py:483] Algo bellman_ford step 9577 current loss 0.041397, current_train_items 306496.
I0302 19:01:59.282224 22683591385216 run.py:483] Algo bellman_ford step 9578 current loss 0.025675, current_train_items 306528.
I0302 19:01:59.314172 22683591385216 run.py:483] Algo bellman_ford step 9579 current loss 0.057724, current_train_items 306560.
I0302 19:01:59.332635 22683591385216 run.py:483] Algo bellman_ford step 9580 current loss 0.003649, current_train_items 306592.
I0302 19:01:59.347838 22683591385216 run.py:483] Algo bellman_ford step 9581 current loss 0.025369, current_train_items 306624.
I0302 19:01:59.371607 22683591385216 run.py:483] Algo bellman_ford step 9582 current loss 0.020751, current_train_items 306656.
I0302 19:01:59.402782 22683591385216 run.py:483] Algo bellman_ford step 9583 current loss 0.029770, current_train_items 306688.
I0302 19:01:59.435096 22683591385216 run.py:483] Algo bellman_ford step 9584 current loss 0.055111, current_train_items 306720.
I0302 19:01:59.454174 22683591385216 run.py:483] Algo bellman_ford step 9585 current loss 0.022399, current_train_items 306752.
I0302 19:01:59.470047 22683591385216 run.py:483] Algo bellman_ford step 9586 current loss 0.048397, current_train_items 306784.
I0302 19:01:59.492870 22683591385216 run.py:483] Algo bellman_ford step 9587 current loss 0.043601, current_train_items 306816.
I0302 19:01:59.523081 22683591385216 run.py:483] Algo bellman_ford step 9588 current loss 0.027328, current_train_items 306848.
I0302 19:01:59.555578 22683591385216 run.py:483] Algo bellman_ford step 9589 current loss 0.104802, current_train_items 306880.
I0302 19:01:59.574409 22683591385216 run.py:483] Algo bellman_ford step 9590 current loss 0.002875, current_train_items 306912.
I0302 19:01:59.590117 22683591385216 run.py:483] Algo bellman_ford step 9591 current loss 0.008521, current_train_items 306944.
I0302 19:01:59.612773 22683591385216 run.py:483] Algo bellman_ford step 9592 current loss 0.027029, current_train_items 306976.
I0302 19:01:59.642937 22683591385216 run.py:483] Algo bellman_ford step 9593 current loss 0.044823, current_train_items 307008.
I0302 19:01:59.674014 22683591385216 run.py:483] Algo bellman_ford step 9594 current loss 0.030480, current_train_items 307040.
I0302 19:01:59.692808 22683591385216 run.py:483] Algo bellman_ford step 9595 current loss 0.001489, current_train_items 307072.
I0302 19:01:59.708501 22683591385216 run.py:483] Algo bellman_ford step 9596 current loss 0.013579, current_train_items 307104.
I0302 19:01:59.731651 22683591385216 run.py:483] Algo bellman_ford step 9597 current loss 0.022691, current_train_items 307136.
I0302 19:01:59.760610 22683591385216 run.py:483] Algo bellman_ford step 9598 current loss 0.027436, current_train_items 307168.
I0302 19:01:59.791671 22683591385216 run.py:483] Algo bellman_ford step 9599 current loss 0.021872, current_train_items 307200.
I0302 19:01:59.810335 22683591385216 run.py:483] Algo bellman_ford step 9600 current loss 0.000972, current_train_items 307232.
I0302 19:01:59.818204 22683591385216 run.py:503] (val) algo bellman_ford step 9600: {'pi': 0.99609375, 'score': 0.99609375, 'examples_seen': 307232, 'step': 9600, 'algorithm': 'bellman_ford'}
I0302 19:01:59.818311 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.996, val scores are: bellman_ford: 0.996
I0302 19:01:59.834701 22683591385216 run.py:483] Algo bellman_ford step 9601 current loss 0.008178, current_train_items 307264.
I0302 19:01:59.858435 22683591385216 run.py:483] Algo bellman_ford step 9602 current loss 0.020413, current_train_items 307296.
I0302 19:01:59.888302 22683591385216 run.py:483] Algo bellman_ford step 9603 current loss 0.027161, current_train_items 307328.
I0302 19:01:59.921631 22683591385216 run.py:483] Algo bellman_ford step 9604 current loss 0.036980, current_train_items 307360.
I0302 19:01:59.940018 22683591385216 run.py:483] Algo bellman_ford step 9605 current loss 0.001194, current_train_items 307392.
I0302 19:01:59.955146 22683591385216 run.py:483] Algo bellman_ford step 9606 current loss 0.002908, current_train_items 307424.
I0302 19:01:59.978026 22683591385216 run.py:483] Algo bellman_ford step 9607 current loss 0.023838, current_train_items 307456.
I0302 19:02:00.007903 22683591385216 run.py:483] Algo bellman_ford step 9608 current loss 0.027626, current_train_items 307488.
I0302 19:02:00.039342 22683591385216 run.py:483] Algo bellman_ford step 9609 current loss 0.029636, current_train_items 307520.
I0302 19:02:00.058016 22683591385216 run.py:483] Algo bellman_ford step 9610 current loss 0.016420, current_train_items 307552.
I0302 19:02:00.073614 22683591385216 run.py:483] Algo bellman_ford step 9611 current loss 0.014087, current_train_items 307584.
I0302 19:02:00.097119 22683591385216 run.py:483] Algo bellman_ford step 9612 current loss 0.027169, current_train_items 307616.
I0302 19:02:00.127419 22683591385216 run.py:483] Algo bellman_ford step 9613 current loss 0.031761, current_train_items 307648.
I0302 19:02:00.159131 22683591385216 run.py:483] Algo bellman_ford step 9614 current loss 0.069174, current_train_items 307680.
I0302 19:02:00.177609 22683591385216 run.py:483] Algo bellman_ford step 9615 current loss 0.000853, current_train_items 307712.
I0302 19:02:00.193043 22683591385216 run.py:483] Algo bellman_ford step 9616 current loss 0.009672, current_train_items 307744.
I0302 19:02:00.215770 22683591385216 run.py:483] Algo bellman_ford step 9617 current loss 0.031745, current_train_items 307776.
I0302 19:02:00.245507 22683591385216 run.py:483] Algo bellman_ford step 9618 current loss 0.038611, current_train_items 307808.
I0302 19:02:00.277254 22683591385216 run.py:483] Algo bellman_ford step 9619 current loss 0.027781, current_train_items 307840.
I0302 19:02:00.295632 22683591385216 run.py:483] Algo bellman_ford step 9620 current loss 0.000676, current_train_items 307872.
I0302 19:02:00.310933 22683591385216 run.py:483] Algo bellman_ford step 9621 current loss 0.005972, current_train_items 307904.
I0302 19:02:00.334312 22683591385216 run.py:483] Algo bellman_ford step 9622 current loss 0.031558, current_train_items 307936.
I0302 19:02:00.364339 22683591385216 run.py:483] Algo bellman_ford step 9623 current loss 0.041587, current_train_items 307968.
I0302 19:02:00.396490 22683591385216 run.py:483] Algo bellman_ford step 9624 current loss 0.049450, current_train_items 308000.
I0302 19:02:00.414886 22683591385216 run.py:483] Algo bellman_ford step 9625 current loss 0.001022, current_train_items 308032.
I0302 19:02:00.430210 22683591385216 run.py:483] Algo bellman_ford step 9626 current loss 0.005242, current_train_items 308064.
I0302 19:02:00.453035 22683591385216 run.py:483] Algo bellman_ford step 9627 current loss 0.027367, current_train_items 308096.
I0302 19:02:00.482646 22683591385216 run.py:483] Algo bellman_ford step 9628 current loss 0.041746, current_train_items 308128.
I0302 19:02:00.515140 22683591385216 run.py:483] Algo bellman_ford step 9629 current loss 0.051543, current_train_items 308160.
I0302 19:02:00.534059 22683591385216 run.py:483] Algo bellman_ford step 9630 current loss 0.067250, current_train_items 308192.
I0302 19:02:00.549877 22683591385216 run.py:483] Algo bellman_ford step 9631 current loss 0.009937, current_train_items 308224.
I0302 19:02:00.573617 22683591385216 run.py:483] Algo bellman_ford step 9632 current loss 0.093326, current_train_items 308256.
I0302 19:02:00.603073 22683591385216 run.py:483] Algo bellman_ford step 9633 current loss 0.034943, current_train_items 308288.
I0302 19:02:00.635577 22683591385216 run.py:483] Algo bellman_ford step 9634 current loss 0.099661, current_train_items 308320.
I0302 19:02:00.654335 22683591385216 run.py:483] Algo bellman_ford step 9635 current loss 0.001287, current_train_items 308352.
I0302 19:02:00.669526 22683591385216 run.py:483] Algo bellman_ford step 9636 current loss 0.003433, current_train_items 308384.
I0302 19:02:00.692716 22683591385216 run.py:483] Algo bellman_ford step 9637 current loss 0.015861, current_train_items 308416.
I0302 19:02:00.723305 22683591385216 run.py:483] Algo bellman_ford step 9638 current loss 0.028814, current_train_items 308448.
I0302 19:02:00.754927 22683591385216 run.py:483] Algo bellman_ford step 9639 current loss 0.117431, current_train_items 308480.
I0302 19:02:00.773534 22683591385216 run.py:483] Algo bellman_ford step 9640 current loss 0.002264, current_train_items 308512.
I0302 19:02:00.788632 22683591385216 run.py:483] Algo bellman_ford step 9641 current loss 0.007209, current_train_items 308544.
I0302 19:02:00.811985 22683591385216 run.py:483] Algo bellman_ford step 9642 current loss 0.114229, current_train_items 308576.
I0302 19:02:00.843388 22683591385216 run.py:483] Algo bellman_ford step 9643 current loss 0.024757, current_train_items 308608.
I0302 19:02:00.873062 22683591385216 run.py:483] Algo bellman_ford step 9644 current loss 0.025709, current_train_items 308640.
I0302 19:02:00.891479 22683591385216 run.py:483] Algo bellman_ford step 9645 current loss 0.012380, current_train_items 308672.
I0302 19:02:00.906962 22683591385216 run.py:483] Algo bellman_ford step 9646 current loss 0.011381, current_train_items 308704.
I0302 19:02:00.929408 22683591385216 run.py:483] Algo bellman_ford step 9647 current loss 0.025731, current_train_items 308736.
I0302 19:02:00.959951 22683591385216 run.py:483] Algo bellman_ford step 9648 current loss 0.060526, current_train_items 308768.
I0302 19:02:00.992209 22683591385216 run.py:483] Algo bellman_ford step 9649 current loss 0.046637, current_train_items 308800.
I0302 19:02:01.010608 22683591385216 run.py:483] Algo bellman_ford step 9650 current loss 0.002961, current_train_items 308832.
I0302 19:02:01.018691 22683591385216 run.py:503] (val) algo bellman_ford step 9650: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 308832, 'step': 9650, 'algorithm': 'bellman_ford'}
I0302 19:02:01.018798 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:02:01.035004 22683591385216 run.py:483] Algo bellman_ford step 9651 current loss 0.015723, current_train_items 308864.
I0302 19:02:01.059447 22683591385216 run.py:483] Algo bellman_ford step 9652 current loss 0.041424, current_train_items 308896.
I0302 19:02:01.090170 22683591385216 run.py:483] Algo bellman_ford step 9653 current loss 0.141044, current_train_items 308928.
I0302 19:02:01.123833 22683591385216 run.py:483] Algo bellman_ford step 9654 current loss 0.060694, current_train_items 308960.
I0302 19:02:01.142374 22683591385216 run.py:483] Algo bellman_ford step 9655 current loss 0.005863, current_train_items 308992.
I0302 19:02:01.157944 22683591385216 run.py:483] Algo bellman_ford step 9656 current loss 0.013596, current_train_items 309024.
I0302 19:02:01.180816 22683591385216 run.py:483] Algo bellman_ford step 9657 current loss 0.018285, current_train_items 309056.
I0302 19:02:01.211657 22683591385216 run.py:483] Algo bellman_ford step 9658 current loss 0.059887, current_train_items 309088.
I0302 19:02:01.242122 22683591385216 run.py:483] Algo bellman_ford step 9659 current loss 0.039396, current_train_items 309120.
I0302 19:02:01.260755 22683591385216 run.py:483] Algo bellman_ford step 9660 current loss 0.008730, current_train_items 309152.
I0302 19:02:01.276783 22683591385216 run.py:483] Algo bellman_ford step 9661 current loss 0.015340, current_train_items 309184.
I0302 19:02:01.299193 22683591385216 run.py:483] Algo bellman_ford step 9662 current loss 0.086554, current_train_items 309216.
I0302 19:02:01.328865 22683591385216 run.py:483] Algo bellman_ford step 9663 current loss 0.022990, current_train_items 309248.
I0302 19:02:01.361166 22683591385216 run.py:483] Algo bellman_ford step 9664 current loss 0.066795, current_train_items 309280.
I0302 19:02:01.379689 22683591385216 run.py:483] Algo bellman_ford step 9665 current loss 0.006168, current_train_items 309312.
I0302 19:02:01.395143 22683591385216 run.py:483] Algo bellman_ford step 9666 current loss 0.015621, current_train_items 309344.
I0302 19:02:01.417087 22683591385216 run.py:483] Algo bellman_ford step 9667 current loss 0.024817, current_train_items 309376.
I0302 19:02:01.446748 22683591385216 run.py:483] Algo bellman_ford step 9668 current loss 0.032621, current_train_items 309408.
I0302 19:02:01.477998 22683591385216 run.py:483] Algo bellman_ford step 9669 current loss 0.046180, current_train_items 309440.
I0302 19:02:01.497288 22683591385216 run.py:483] Algo bellman_ford step 9670 current loss 0.000694, current_train_items 309472.
I0302 19:02:01.512975 22683591385216 run.py:483] Algo bellman_ford step 9671 current loss 0.008275, current_train_items 309504.
I0302 19:02:01.535648 22683591385216 run.py:483] Algo bellman_ford step 9672 current loss 0.016984, current_train_items 309536.
I0302 19:02:01.564997 22683591385216 run.py:483] Algo bellman_ford step 9673 current loss 0.025981, current_train_items 309568.
I0302 19:02:01.596016 22683591385216 run.py:483] Algo bellman_ford step 9674 current loss 0.035804, current_train_items 309600.
I0302 19:02:01.614387 22683591385216 run.py:483] Algo bellman_ford step 9675 current loss 0.000697, current_train_items 309632.
I0302 19:02:01.630344 22683591385216 run.py:483] Algo bellman_ford step 9676 current loss 0.012825, current_train_items 309664.
I0302 19:02:01.652948 22683591385216 run.py:483] Algo bellman_ford step 9677 current loss 0.020352, current_train_items 309696.
I0302 19:02:01.681559 22683591385216 run.py:483] Algo bellman_ford step 9678 current loss 0.028336, current_train_items 309728.
I0302 19:02:01.713650 22683591385216 run.py:483] Algo bellman_ford step 9679 current loss 0.035870, current_train_items 309760.
I0302 19:02:01.731929 22683591385216 run.py:483] Algo bellman_ford step 9680 current loss 0.001184, current_train_items 309792.
I0302 19:02:01.747596 22683591385216 run.py:483] Algo bellman_ford step 9681 current loss 0.015266, current_train_items 309824.
I0302 19:02:01.770283 22683591385216 run.py:483] Algo bellman_ford step 9682 current loss 0.013020, current_train_items 309856.
I0302 19:02:01.799851 22683591385216 run.py:483] Algo bellman_ford step 9683 current loss 0.037037, current_train_items 309888.
I0302 19:02:01.830617 22683591385216 run.py:483] Algo bellman_ford step 9684 current loss 0.036760, current_train_items 309920.
I0302 19:02:01.849544 22683591385216 run.py:483] Algo bellman_ford step 9685 current loss 0.000632, current_train_items 309952.
I0302 19:02:01.865262 22683591385216 run.py:483] Algo bellman_ford step 9686 current loss 0.008907, current_train_items 309984.
I0302 19:02:01.888022 22683591385216 run.py:483] Algo bellman_ford step 9687 current loss 0.040799, current_train_items 310016.
I0302 19:02:01.917605 22683591385216 run.py:483] Algo bellman_ford step 9688 current loss 0.046307, current_train_items 310048.
I0302 19:02:01.949201 22683591385216 run.py:483] Algo bellman_ford step 9689 current loss 0.049186, current_train_items 310080.
I0302 19:02:01.968175 22683591385216 run.py:483] Algo bellman_ford step 9690 current loss 0.005403, current_train_items 310112.
I0302 19:02:01.983467 22683591385216 run.py:483] Algo bellman_ford step 9691 current loss 0.001709, current_train_items 310144.
I0302 19:02:02.006046 22683591385216 run.py:483] Algo bellman_ford step 9692 current loss 0.007020, current_train_items 310176.
I0302 19:02:02.035368 22683591385216 run.py:483] Algo bellman_ford step 9693 current loss 0.054963, current_train_items 310208.
I0302 19:02:02.067174 22683591385216 run.py:483] Algo bellman_ford step 9694 current loss 0.024902, current_train_items 310240.
I0302 19:02:02.085574 22683591385216 run.py:483] Algo bellman_ford step 9695 current loss 0.001038, current_train_items 310272.
I0302 19:02:02.100918 22683591385216 run.py:483] Algo bellman_ford step 9696 current loss 0.044381, current_train_items 310304.
I0302 19:02:02.122779 22683591385216 run.py:483] Algo bellman_ford step 9697 current loss 0.010529, current_train_items 310336.
I0302 19:02:02.154946 22683591385216 run.py:483] Algo bellman_ford step 9698 current loss 0.071536, current_train_items 310368.
I0302 19:02:02.188255 22683591385216 run.py:483] Algo bellman_ford step 9699 current loss 0.041360, current_train_items 310400.
I0302 19:02:02.206916 22683591385216 run.py:483] Algo bellman_ford step 9700 current loss 0.005359, current_train_items 310432.
I0302 19:02:02.214781 22683591385216 run.py:503] (val) algo bellman_ford step 9700: {'pi': 0.98828125, 'score': 0.98828125, 'examples_seen': 310432, 'step': 9700, 'algorithm': 'bellman_ford'}
I0302 19:02:02.214890 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.988, val scores are: bellman_ford: 0.988
I0302 19:02:02.230921 22683591385216 run.py:483] Algo bellman_ford step 9701 current loss 0.007649, current_train_items 310464.
I0302 19:02:02.254932 22683591385216 run.py:483] Algo bellman_ford step 9702 current loss 0.016160, current_train_items 310496.
I0302 19:02:02.286096 22683591385216 run.py:483] Algo bellman_ford step 9703 current loss 0.086257, current_train_items 310528.
I0302 19:02:02.317923 22683591385216 run.py:483] Algo bellman_ford step 9704 current loss 0.149705, current_train_items 310560.
I0302 19:02:02.336679 22683591385216 run.py:483] Algo bellman_ford step 9705 current loss 0.002546, current_train_items 310592.
I0302 19:02:02.352050 22683591385216 run.py:483] Algo bellman_ford step 9706 current loss 0.015651, current_train_items 310624.
I0302 19:02:02.375928 22683591385216 run.py:483] Algo bellman_ford step 9707 current loss 0.034531, current_train_items 310656.
I0302 19:02:02.406399 22683591385216 run.py:483] Algo bellman_ford step 9708 current loss 0.045936, current_train_items 310688.
I0302 19:02:02.436106 22683591385216 run.py:483] Algo bellman_ford step 9709 current loss 0.048177, current_train_items 310720.
I0302 19:02:02.454512 22683591385216 run.py:483] Algo bellman_ford step 9710 current loss 0.001582, current_train_items 310752.
I0302 19:02:02.469755 22683591385216 run.py:483] Algo bellman_ford step 9711 current loss 0.010768, current_train_items 310784.
I0302 19:02:02.492990 22683591385216 run.py:483] Algo bellman_ford step 9712 current loss 0.018582, current_train_items 310816.
I0302 19:02:02.523763 22683591385216 run.py:483] Algo bellman_ford step 9713 current loss 0.033396, current_train_items 310848.
I0302 19:02:02.556028 22683591385216 run.py:483] Algo bellman_ford step 9714 current loss 0.028701, current_train_items 310880.
I0302 19:02:02.574627 22683591385216 run.py:483] Algo bellman_ford step 9715 current loss 0.017960, current_train_items 310912.
I0302 19:02:02.590379 22683591385216 run.py:483] Algo bellman_ford step 9716 current loss 0.022846, current_train_items 310944.
I0302 19:02:02.612073 22683591385216 run.py:483] Algo bellman_ford step 9717 current loss 0.021668, current_train_items 310976.
I0302 19:02:02.642684 22683591385216 run.py:483] Algo bellman_ford step 9718 current loss 0.028349, current_train_items 311008.
I0302 19:02:02.671789 22683591385216 run.py:483] Algo bellman_ford step 9719 current loss 0.048738, current_train_items 311040.
I0302 19:02:02.690033 22683591385216 run.py:483] Algo bellman_ford step 9720 current loss 0.002387, current_train_items 311072.
I0302 19:02:02.705168 22683591385216 run.py:483] Algo bellman_ford step 9721 current loss 0.002127, current_train_items 311104.
I0302 19:02:02.727227 22683591385216 run.py:483] Algo bellman_ford step 9722 current loss 0.025969, current_train_items 311136.
I0302 19:02:02.757565 22683591385216 run.py:483] Algo bellman_ford step 9723 current loss 0.044072, current_train_items 311168.
I0302 19:02:02.789310 22683591385216 run.py:483] Algo bellman_ford step 9724 current loss 0.067585, current_train_items 311200.
I0302 19:02:02.807729 22683591385216 run.py:483] Algo bellman_ford step 9725 current loss 0.001106, current_train_items 311232.
I0302 19:02:02.822820 22683591385216 run.py:483] Algo bellman_ford step 9726 current loss 0.011057, current_train_items 311264.
I0302 19:02:02.844834 22683591385216 run.py:483] Algo bellman_ford step 9727 current loss 0.007735, current_train_items 311296.
I0302 19:02:02.875401 22683591385216 run.py:483] Algo bellman_ford step 9728 current loss 0.020623, current_train_items 311328.
I0302 19:02:02.907332 22683591385216 run.py:483] Algo bellman_ford step 9729 current loss 0.034652, current_train_items 311360.
I0302 19:02:02.925664 22683591385216 run.py:483] Algo bellman_ford step 9730 current loss 0.001715, current_train_items 311392.
I0302 19:02:02.941344 22683591385216 run.py:483] Algo bellman_ford step 9731 current loss 0.022244, current_train_items 311424.
I0302 19:02:02.963704 22683591385216 run.py:483] Algo bellman_ford step 9732 current loss 0.016028, current_train_items 311456.
I0302 19:02:02.993691 22683591385216 run.py:483] Algo bellman_ford step 9733 current loss 0.039507, current_train_items 311488.
I0302 19:02:03.024255 22683591385216 run.py:483] Algo bellman_ford step 9734 current loss 0.044561, current_train_items 311520.
I0302 19:02:03.042719 22683591385216 run.py:483] Algo bellman_ford step 9735 current loss 0.000708, current_train_items 311552.
I0302 19:02:03.058177 22683591385216 run.py:483] Algo bellman_ford step 9736 current loss 0.008814, current_train_items 311584.
I0302 19:02:03.080698 22683591385216 run.py:483] Algo bellman_ford step 9737 current loss 0.013173, current_train_items 311616.
I0302 19:02:03.112020 22683591385216 run.py:483] Algo bellman_ford step 9738 current loss 0.057939, current_train_items 311648.
I0302 19:02:03.144266 22683591385216 run.py:483] Algo bellman_ford step 9739 current loss 0.038963, current_train_items 311680.
I0302 19:02:03.162678 22683591385216 run.py:483] Algo bellman_ford step 9740 current loss 0.000770, current_train_items 311712.
I0302 19:02:03.178797 22683591385216 run.py:483] Algo bellman_ford step 9741 current loss 0.023288, current_train_items 311744.
I0302 19:02:03.202950 22683591385216 run.py:483] Algo bellman_ford step 9742 current loss 0.034891, current_train_items 311776.
I0302 19:02:03.233307 22683591385216 run.py:483] Algo bellman_ford step 9743 current loss 0.025917, current_train_items 311808.
I0302 19:02:03.264967 22683591385216 run.py:483] Algo bellman_ford step 9744 current loss 0.027448, current_train_items 311840.
I0302 19:02:03.283454 22683591385216 run.py:483] Algo bellman_ford step 9745 current loss 0.010562, current_train_items 311872.
I0302 19:02:03.299432 22683591385216 run.py:483] Algo bellman_ford step 9746 current loss 0.006255, current_train_items 311904.
I0302 19:02:03.322088 22683591385216 run.py:483] Algo bellman_ford step 9747 current loss 0.013841, current_train_items 311936.
I0302 19:02:03.349941 22683591385216 run.py:483] Algo bellman_ford step 9748 current loss 0.018795, current_train_items 311968.
I0302 19:02:03.380554 22683591385216 run.py:483] Algo bellman_ford step 9749 current loss 0.050544, current_train_items 312000.
I0302 19:02:03.399101 22683591385216 run.py:483] Algo bellman_ford step 9750 current loss 0.000418, current_train_items 312032.
I0302 19:02:03.407167 22683591385216 run.py:503] (val) algo bellman_ford step 9750: {'pi': 0.9921875, 'score': 0.9921875, 'examples_seen': 312032, 'step': 9750, 'algorithm': 'bellman_ford'}
I0302 19:02:03.407274 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.992, val scores are: bellman_ford: 0.992
I0302 19:02:03.423088 22683591385216 run.py:483] Algo bellman_ford step 9751 current loss 0.006653, current_train_items 312064.
I0302 19:02:03.447773 22683591385216 run.py:483] Algo bellman_ford step 9752 current loss 0.017426, current_train_items 312096.
I0302 19:02:03.477063 22683591385216 run.py:483] Algo bellman_ford step 9753 current loss 0.027121, current_train_items 312128.
I0302 19:02:03.508368 22683591385216 run.py:483] Algo bellman_ford step 9754 current loss 0.049237, current_train_items 312160.
I0302 19:02:03.526774 22683591385216 run.py:483] Algo bellman_ford step 9755 current loss 0.006109, current_train_items 312192.
I0302 19:02:03.542805 22683591385216 run.py:483] Algo bellman_ford step 9756 current loss 0.013363, current_train_items 312224.
I0302 19:02:03.564754 22683591385216 run.py:483] Algo bellman_ford step 9757 current loss 0.022404, current_train_items 312256.
I0302 19:02:03.595567 22683591385216 run.py:483] Algo bellman_ford step 9758 current loss 0.041736, current_train_items 312288.
I0302 19:02:03.626968 22683591385216 run.py:483] Algo bellman_ford step 9759 current loss 0.045281, current_train_items 312320.
I0302 19:02:03.645725 22683591385216 run.py:483] Algo bellman_ford step 9760 current loss 0.004699, current_train_items 312352.
I0302 19:02:03.661851 22683591385216 run.py:483] Algo bellman_ford step 9761 current loss 0.011196, current_train_items 312384.
I0302 19:02:03.684766 22683591385216 run.py:483] Algo bellman_ford step 9762 current loss 0.012597, current_train_items 312416.
I0302 19:02:03.715368 22683591385216 run.py:483] Algo bellman_ford step 9763 current loss 0.051080, current_train_items 312448.
I0302 19:02:03.744612 22683591385216 run.py:483] Algo bellman_ford step 9764 current loss 0.017843, current_train_items 312480.
I0302 19:02:03.762967 22683591385216 run.py:483] Algo bellman_ford step 9765 current loss 0.010377, current_train_items 312512.
I0302 19:02:03.777974 22683591385216 run.py:483] Algo bellman_ford step 9766 current loss 0.013736, current_train_items 312544.
I0302 19:02:03.800984 22683591385216 run.py:483] Algo bellman_ford step 9767 current loss 0.064891, current_train_items 312576.
I0302 19:02:03.830414 22683591385216 run.py:483] Algo bellman_ford step 9768 current loss 0.035002, current_train_items 312608.
I0302 19:02:03.862256 22683591385216 run.py:483] Algo bellman_ford step 9769 current loss 0.044375, current_train_items 312640.
I0302 19:02:03.880994 22683591385216 run.py:483] Algo bellman_ford step 9770 current loss 0.001526, current_train_items 312672.
I0302 19:02:03.896459 22683591385216 run.py:483] Algo bellman_ford step 9771 current loss 0.005878, current_train_items 312704.
I0302 19:02:03.918215 22683591385216 run.py:483] Algo bellman_ford step 9772 current loss 0.049339, current_train_items 312736.
I0302 19:02:03.949023 22683591385216 run.py:483] Algo bellman_ford step 9773 current loss 0.045741, current_train_items 312768.
I0302 19:02:03.981000 22683591385216 run.py:483] Algo bellman_ford step 9774 current loss 0.065487, current_train_items 312800.
I0302 19:02:03.999865 22683591385216 run.py:483] Algo bellman_ford step 9775 current loss 0.001889, current_train_items 312832.
I0302 19:02:04.015334 22683591385216 run.py:483] Algo bellman_ford step 9776 current loss 0.004898, current_train_items 312864.
I0302 19:02:04.038710 22683591385216 run.py:483] Algo bellman_ford step 9777 current loss 0.027756, current_train_items 312896.
I0302 19:02:04.069537 22683591385216 run.py:483] Algo bellman_ford step 9778 current loss 0.038523, current_train_items 312928.
I0302 19:02:04.101333 22683591385216 run.py:483] Algo bellman_ford step 9779 current loss 0.024135, current_train_items 312960.
I0302 19:02:04.119582 22683591385216 run.py:483] Algo bellman_ford step 9780 current loss 0.003459, current_train_items 312992.
I0302 19:02:04.135309 22683591385216 run.py:483] Algo bellman_ford step 9781 current loss 0.004996, current_train_items 313024.
I0302 19:02:04.157300 22683591385216 run.py:483] Algo bellman_ford step 9782 current loss 0.025680, current_train_items 313056.
I0302 19:02:04.189237 22683591385216 run.py:483] Algo bellman_ford step 9783 current loss 0.066130, current_train_items 313088.
I0302 19:02:04.219521 22683591385216 run.py:483] Algo bellman_ford step 9784 current loss 0.025634, current_train_items 313120.
I0302 19:02:04.238614 22683591385216 run.py:483] Algo bellman_ford step 9785 current loss 0.002334, current_train_items 313152.
I0302 19:02:04.254610 22683591385216 run.py:483] Algo bellman_ford step 9786 current loss 0.013948, current_train_items 313184.
I0302 19:02:04.277238 22683591385216 run.py:483] Algo bellman_ford step 9787 current loss 0.037367, current_train_items 313216.
I0302 19:02:04.306770 22683591385216 run.py:483] Algo bellman_ford step 9788 current loss 0.024102, current_train_items 313248.
I0302 19:02:04.338501 22683591385216 run.py:483] Algo bellman_ford step 9789 current loss 0.033910, current_train_items 313280.
I0302 19:02:04.357265 22683591385216 run.py:483] Algo bellman_ford step 9790 current loss 0.007254, current_train_items 313312.
I0302 19:02:04.372778 22683591385216 run.py:483] Algo bellman_ford step 9791 current loss 0.013535, current_train_items 313344.
I0302 19:02:04.394977 22683591385216 run.py:483] Algo bellman_ford step 9792 current loss 0.027399, current_train_items 313376.
I0302 19:02:04.424476 22683591385216 run.py:483] Algo bellman_ford step 9793 current loss 0.052538, current_train_items 313408.
I0302 19:02:04.454019 22683591385216 run.py:483] Algo bellman_ford step 9794 current loss 0.037240, current_train_items 313440.
I0302 19:02:04.472585 22683591385216 run.py:483] Algo bellman_ford step 9795 current loss 0.003262, current_train_items 313472.
I0302 19:02:04.488315 22683591385216 run.py:483] Algo bellman_ford step 9796 current loss 0.015815, current_train_items 313504.
I0302 19:02:04.510746 22683591385216 run.py:483] Algo bellman_ford step 9797 current loss 0.130020, current_train_items 313536.
I0302 19:02:04.540102 22683591385216 run.py:483] Algo bellman_ford step 9798 current loss 0.096718, current_train_items 313568.
I0302 19:02:04.570742 22683591385216 run.py:483] Algo bellman_ford step 9799 current loss 0.167135, current_train_items 313600.
I0302 19:02:04.589236 22683591385216 run.py:483] Algo bellman_ford step 9800 current loss 0.031822, current_train_items 313632.
I0302 19:02:04.596863 22683591385216 run.py:503] (val) algo bellman_ford step 9800: {'pi': 0.9794921875, 'score': 0.9794921875, 'examples_seen': 313632, 'step': 9800, 'algorithm': 'bellman_ford'}
I0302 19:02:04.596982 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.979, val scores are: bellman_ford: 0.979
I0302 19:02:04.612659 22683591385216 run.py:483] Algo bellman_ford step 9801 current loss 0.017815, current_train_items 313664.
I0302 19:02:04.635995 22683591385216 run.py:483] Algo bellman_ford step 9802 current loss 0.039791, current_train_items 313696.
I0302 19:02:04.667014 22683591385216 run.py:483] Algo bellman_ford step 9803 current loss 0.096084, current_train_items 313728.
I0302 19:02:04.697928 22683591385216 run.py:483] Algo bellman_ford step 9804 current loss 0.135178, current_train_items 313760.
I0302 19:02:04.716781 22683591385216 run.py:483] Algo bellman_ford step 9805 current loss 0.003802, current_train_items 313792.
I0302 19:02:04.732330 22683591385216 run.py:483] Algo bellman_ford step 9806 current loss 0.006698, current_train_items 313824.
I0302 19:02:04.754546 22683591385216 run.py:483] Algo bellman_ford step 9807 current loss 0.042106, current_train_items 313856.
I0302 19:02:04.784487 22683591385216 run.py:483] Algo bellman_ford step 9808 current loss 0.030774, current_train_items 313888.
I0302 19:02:04.818190 22683591385216 run.py:483] Algo bellman_ford step 9809 current loss 0.060463, current_train_items 313920.
I0302 19:02:04.836468 22683591385216 run.py:483] Algo bellman_ford step 9810 current loss 0.001156, current_train_items 313952.
I0302 19:02:04.851542 22683591385216 run.py:483] Algo bellman_ford step 9811 current loss 0.032554, current_train_items 313984.
I0302 19:02:04.875230 22683591385216 run.py:483] Algo bellman_ford step 9812 current loss 0.126820, current_train_items 314016.
I0302 19:02:04.903337 22683591385216 run.py:483] Algo bellman_ford step 9813 current loss 0.046632, current_train_items 314048.
I0302 19:02:04.935376 22683591385216 run.py:483] Algo bellman_ford step 9814 current loss 0.074925, current_train_items 314080.
I0302 19:02:04.953353 22683591385216 run.py:483] Algo bellman_ford step 9815 current loss 0.002431, current_train_items 314112.
I0302 19:02:04.969036 22683591385216 run.py:483] Algo bellman_ford step 9816 current loss 0.016430, current_train_items 314144.
I0302 19:02:04.991369 22683591385216 run.py:483] Algo bellman_ford step 9817 current loss 0.021742, current_train_items 314176.
I0302 19:02:05.022882 22683591385216 run.py:483] Algo bellman_ford step 9818 current loss 0.070960, current_train_items 314208.
I0302 19:02:05.054151 22683591385216 run.py:483] Algo bellman_ford step 9819 current loss 0.064618, current_train_items 314240.
I0302 19:02:05.072487 22683591385216 run.py:483] Algo bellman_ford step 9820 current loss 0.001359, current_train_items 314272.
I0302 19:02:05.087711 22683591385216 run.py:483] Algo bellman_ford step 9821 current loss 0.036885, current_train_items 314304.
I0302 19:02:05.111160 22683591385216 run.py:483] Algo bellman_ford step 9822 current loss 0.034152, current_train_items 314336.
I0302 19:02:05.140393 22683591385216 run.py:483] Algo bellman_ford step 9823 current loss 0.034627, current_train_items 314368.
I0302 19:02:05.171519 22683591385216 run.py:483] Algo bellman_ford step 9824 current loss 0.037366, current_train_items 314400.
I0302 19:02:05.190160 22683591385216 run.py:483] Algo bellman_ford step 9825 current loss 0.002669, current_train_items 314432.
I0302 19:02:05.205296 22683591385216 run.py:483] Algo bellman_ford step 9826 current loss 0.015119, current_train_items 314464.
I0302 19:02:05.228001 22683591385216 run.py:483] Algo bellman_ford step 9827 current loss 0.031595, current_train_items 314496.
I0302 19:02:05.257178 22683591385216 run.py:483] Algo bellman_ford step 9828 current loss 0.065175, current_train_items 314528.
I0302 19:02:05.289460 22683591385216 run.py:483] Algo bellman_ford step 9829 current loss 0.048067, current_train_items 314560.
I0302 19:02:05.307761 22683591385216 run.py:483] Algo bellman_ford step 9830 current loss 0.001720, current_train_items 314592.
I0302 19:02:05.323704 22683591385216 run.py:483] Algo bellman_ford step 9831 current loss 0.016765, current_train_items 314624.
I0302 19:02:05.346368 22683591385216 run.py:483] Algo bellman_ford step 9832 current loss 0.057693, current_train_items 314656.
I0302 19:02:05.376848 22683591385216 run.py:483] Algo bellman_ford step 9833 current loss 0.054413, current_train_items 314688.
I0302 19:02:05.407021 22683591385216 run.py:483] Algo bellman_ford step 9834 current loss 0.066314, current_train_items 314720.
I0302 19:02:05.425311 22683591385216 run.py:483] Algo bellman_ford step 9835 current loss 0.008448, current_train_items 314752.
I0302 19:02:05.440676 22683591385216 run.py:483] Algo bellman_ford step 9836 current loss 0.014187, current_train_items 314784.
I0302 19:02:05.463416 22683591385216 run.py:483] Algo bellman_ford step 9837 current loss 0.033420, current_train_items 314816.
I0302 19:02:05.494680 22683591385216 run.py:483] Algo bellman_ford step 9838 current loss 0.031249, current_train_items 314848.
I0302 19:02:05.528177 22683591385216 run.py:483] Algo bellman_ford step 9839 current loss 0.043929, current_train_items 314880.
I0302 19:02:05.546519 22683591385216 run.py:483] Algo bellman_ford step 9840 current loss 0.015122, current_train_items 314912.
I0302 19:02:05.561970 22683591385216 run.py:483] Algo bellman_ford step 9841 current loss 0.014944, current_train_items 314944.
I0302 19:02:05.585196 22683591385216 run.py:483] Algo bellman_ford step 9842 current loss 0.073262, current_train_items 314976.
I0302 19:02:05.614696 22683591385216 run.py:483] Algo bellman_ford step 9843 current loss 0.021622, current_train_items 315008.
I0302 19:02:05.643936 22683591385216 run.py:483] Algo bellman_ford step 9844 current loss 0.043424, current_train_items 315040.
I0302 19:02:05.662328 22683591385216 run.py:483] Algo bellman_ford step 9845 current loss 0.001323, current_train_items 315072.
I0302 19:02:05.678020 22683591385216 run.py:483] Algo bellman_ford step 9846 current loss 0.006299, current_train_items 315104.
I0302 19:02:05.701558 22683591385216 run.py:483] Algo bellman_ford step 9847 current loss 0.017469, current_train_items 315136.
I0302 19:02:05.731206 22683591385216 run.py:483] Algo bellman_ford step 9848 current loss 0.049721, current_train_items 315168.
I0302 19:02:05.763134 22683591385216 run.py:483] Algo bellman_ford step 9849 current loss 0.057605, current_train_items 315200.
I0302 19:02:05.781935 22683591385216 run.py:483] Algo bellman_ford step 9850 current loss 0.002171, current_train_items 315232.
I0302 19:02:05.789921 22683591385216 run.py:503] (val) algo bellman_ford step 9850: {'pi': 0.986328125, 'score': 0.986328125, 'examples_seen': 315232, 'step': 9850, 'algorithm': 'bellman_ford'}
I0302 19:02:05.790028 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.986, val scores are: bellman_ford: 0.986
I0302 19:02:05.806759 22683591385216 run.py:483] Algo bellman_ford step 9851 current loss 0.030508, current_train_items 315264.
I0302 19:02:05.831344 22683591385216 run.py:483] Algo bellman_ford step 9852 current loss 0.025428, current_train_items 315296.
I0302 19:02:05.860729 22683591385216 run.py:483] Algo bellman_ford step 9853 current loss 0.036653, current_train_items 315328.
I0302 19:02:05.893030 22683591385216 run.py:483] Algo bellman_ford step 9854 current loss 0.037072, current_train_items 315360.
I0302 19:02:05.911720 22683591385216 run.py:483] Algo bellman_ford step 9855 current loss 0.002425, current_train_items 315392.
I0302 19:02:05.927192 22683591385216 run.py:483] Algo bellman_ford step 9856 current loss 0.007645, current_train_items 315424.
I0302 19:02:05.950309 22683591385216 run.py:483] Algo bellman_ford step 9857 current loss 0.025794, current_train_items 315456.
I0302 19:02:05.979971 22683591385216 run.py:483] Algo bellman_ford step 9858 current loss 0.035992, current_train_items 315488.
I0302 19:02:06.011492 22683591385216 run.py:483] Algo bellman_ford step 9859 current loss 0.068586, current_train_items 315520.
I0302 19:02:06.030387 22683591385216 run.py:483] Algo bellman_ford step 9860 current loss 0.002969, current_train_items 315552.
I0302 19:02:06.046090 22683591385216 run.py:483] Algo bellman_ford step 9861 current loss 0.025060, current_train_items 315584.
I0302 19:02:06.069298 22683591385216 run.py:483] Algo bellman_ford step 9862 current loss 0.024322, current_train_items 315616.
I0302 19:02:06.099205 22683591385216 run.py:483] Algo bellman_ford step 9863 current loss 0.044380, current_train_items 315648.
I0302 19:02:06.130480 22683591385216 run.py:483] Algo bellman_ford step 9864 current loss 0.031809, current_train_items 315680.
I0302 19:02:06.148826 22683591385216 run.py:483] Algo bellman_ford step 9865 current loss 0.002456, current_train_items 315712.
I0302 19:02:06.164037 22683591385216 run.py:483] Algo bellman_ford step 9866 current loss 0.020231, current_train_items 315744.
I0302 19:02:06.186903 22683591385216 run.py:483] Algo bellman_ford step 9867 current loss 0.059598, current_train_items 315776.
I0302 19:02:06.217613 22683591385216 run.py:483] Algo bellman_ford step 9868 current loss 0.045531, current_train_items 315808.
I0302 19:02:06.249141 22683591385216 run.py:483] Algo bellman_ford step 9869 current loss 0.035636, current_train_items 315840.
I0302 19:02:06.268316 22683591385216 run.py:483] Algo bellman_ford step 9870 current loss 0.002934, current_train_items 315872.
I0302 19:02:06.284603 22683591385216 run.py:483] Algo bellman_ford step 9871 current loss 0.028447, current_train_items 315904.
I0302 19:02:06.307577 22683591385216 run.py:483] Algo bellman_ford step 9872 current loss 0.057561, current_train_items 315936.
I0302 19:02:06.337870 22683591385216 run.py:483] Algo bellman_ford step 9873 current loss 0.025948, current_train_items 315968.
I0302 19:02:06.368395 22683591385216 run.py:483] Algo bellman_ford step 9874 current loss 0.043487, current_train_items 316000.
I0302 19:02:06.386955 22683591385216 run.py:483] Algo bellman_ford step 9875 current loss 0.001860, current_train_items 316032.
I0302 19:02:06.402541 22683591385216 run.py:483] Algo bellman_ford step 9876 current loss 0.005056, current_train_items 316064.
I0302 19:02:06.423836 22683591385216 run.py:483] Algo bellman_ford step 9877 current loss 0.039354, current_train_items 316096.
I0302 19:02:06.453347 22683591385216 run.py:483] Algo bellman_ford step 9878 current loss 0.026728, current_train_items 316128.
I0302 19:02:06.484188 22683591385216 run.py:483] Algo bellman_ford step 9879 current loss 0.031702, current_train_items 316160.
I0302 19:02:06.502409 22683591385216 run.py:483] Algo bellman_ford step 9880 current loss 0.001554, current_train_items 316192.
I0302 19:02:06.517871 22683591385216 run.py:483] Algo bellman_ford step 9881 current loss 0.038574, current_train_items 316224.
I0302 19:02:06.540483 22683591385216 run.py:483] Algo bellman_ford step 9882 current loss 0.013473, current_train_items 316256.
I0302 19:02:06.570886 22683591385216 run.py:483] Algo bellman_ford step 9883 current loss 0.030357, current_train_items 316288.
I0302 19:02:06.602668 22683591385216 run.py:483] Algo bellman_ford step 9884 current loss 0.058514, current_train_items 316320.
I0302 19:02:06.621724 22683591385216 run.py:483] Algo bellman_ford step 9885 current loss 0.000537, current_train_items 316352.
I0302 19:02:06.637303 22683591385216 run.py:483] Algo bellman_ford step 9886 current loss 0.007144, current_train_items 316384.
I0302 19:02:06.659968 22683591385216 run.py:483] Algo bellman_ford step 9887 current loss 0.004990, current_train_items 316416.
I0302 19:02:06.689630 22683591385216 run.py:483] Algo bellman_ford step 9888 current loss 0.056345, current_train_items 316448.
I0302 19:02:06.720099 22683591385216 run.py:483] Algo bellman_ford step 9889 current loss 0.044689, current_train_items 316480.
I0302 19:02:06.739020 22683591385216 run.py:483] Algo bellman_ford step 9890 current loss 0.005373, current_train_items 316512.
I0302 19:02:06.754651 22683591385216 run.py:483] Algo bellman_ford step 9891 current loss 0.026497, current_train_items 316544.
I0302 19:02:06.776174 22683591385216 run.py:483] Algo bellman_ford step 9892 current loss 0.017825, current_train_items 316576.
I0302 19:02:06.806913 22683591385216 run.py:483] Algo bellman_ford step 9893 current loss 0.042735, current_train_items 316608.
I0302 19:02:06.839363 22683591385216 run.py:483] Algo bellman_ford step 9894 current loss 0.046713, current_train_items 316640.
I0302 19:02:06.858062 22683591385216 run.py:483] Algo bellman_ford step 9895 current loss 0.000599, current_train_items 316672.
I0302 19:02:06.873319 22683591385216 run.py:483] Algo bellman_ford step 9896 current loss 0.014133, current_train_items 316704.
I0302 19:02:06.896602 22683591385216 run.py:483] Algo bellman_ford step 9897 current loss 0.043535, current_train_items 316736.
I0302 19:02:06.926454 22683591385216 run.py:483] Algo bellman_ford step 9898 current loss 0.067967, current_train_items 316768.
I0302 19:02:06.959938 22683591385216 run.py:483] Algo bellman_ford step 9899 current loss 0.035000, current_train_items 316800.
I0302 19:02:06.978647 22683591385216 run.py:483] Algo bellman_ford step 9900 current loss 0.000648, current_train_items 316832.
I0302 19:02:06.986558 22683591385216 run.py:503] (val) algo bellman_ford step 9900: {'pi': 0.9873046875, 'score': 0.9873046875, 'examples_seen': 316832, 'step': 9900, 'algorithm': 'bellman_ford'}
I0302 19:02:06.986669 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.987, val scores are: bellman_ford: 0.987
I0302 19:02:07.002762 22683591385216 run.py:483] Algo bellman_ford step 9901 current loss 0.004478, current_train_items 316864.
I0302 19:02:07.025686 22683591385216 run.py:483] Algo bellman_ford step 9902 current loss 0.017655, current_train_items 316896.
I0302 19:02:07.055772 22683591385216 run.py:483] Algo bellman_ford step 9903 current loss 0.015720, current_train_items 316928.
I0302 19:02:07.088562 22683591385216 run.py:483] Algo bellman_ford step 9904 current loss 0.039190, current_train_items 316960.
I0302 19:02:07.107254 22683591385216 run.py:483] Algo bellman_ford step 9905 current loss 0.000871, current_train_items 316992.
I0302 19:02:07.123087 22683591385216 run.py:483] Algo bellman_ford step 9906 current loss 0.056370, current_train_items 317024.
I0302 19:02:07.146492 22683591385216 run.py:483] Algo bellman_ford step 9907 current loss 0.032091, current_train_items 317056.
I0302 19:02:07.176881 22683591385216 run.py:483] Algo bellman_ford step 9908 current loss 0.028167, current_train_items 317088.
I0302 19:02:07.207072 22683591385216 run.py:483] Algo bellman_ford step 9909 current loss 0.043840, current_train_items 317120.
I0302 19:02:07.225679 22683591385216 run.py:483] Algo bellman_ford step 9910 current loss 0.009224, current_train_items 317152.
I0302 19:02:07.241420 22683591385216 run.py:483] Algo bellman_ford step 9911 current loss 0.004320, current_train_items 317184.
I0302 19:02:07.264332 22683591385216 run.py:483] Algo bellman_ford step 9912 current loss 0.080811, current_train_items 317216.
I0302 19:02:07.294951 22683591385216 run.py:483] Algo bellman_ford step 9913 current loss 0.095443, current_train_items 317248.
I0302 19:02:07.326367 22683591385216 run.py:483] Algo bellman_ford step 9914 current loss 0.092861, current_train_items 317280.
I0302 19:02:07.345043 22683591385216 run.py:483] Algo bellman_ford step 9915 current loss 0.055290, current_train_items 317312.
I0302 19:02:07.360755 22683591385216 run.py:483] Algo bellman_ford step 9916 current loss 0.011089, current_train_items 317344.
I0302 19:02:07.383821 22683591385216 run.py:483] Algo bellman_ford step 9917 current loss 0.025163, current_train_items 317376.
I0302 19:02:07.414068 22683591385216 run.py:483] Algo bellman_ford step 9918 current loss 0.074143, current_train_items 317408.
I0302 19:02:07.444879 22683591385216 run.py:483] Algo bellman_ford step 9919 current loss 0.134438, current_train_items 317440.
I0302 19:02:07.463268 22683591385216 run.py:483] Algo bellman_ford step 9920 current loss 0.000652, current_train_items 317472.
I0302 19:02:07.479001 22683591385216 run.py:483] Algo bellman_ford step 9921 current loss 0.007860, current_train_items 317504.
I0302 19:02:07.501895 22683591385216 run.py:483] Algo bellman_ford step 9922 current loss 0.065116, current_train_items 317536.
I0302 19:02:07.530364 22683591385216 run.py:483] Algo bellman_ford step 9923 current loss 0.079903, current_train_items 317568.
I0302 19:02:07.564163 22683591385216 run.py:483] Algo bellman_ford step 9924 current loss 0.072011, current_train_items 317600.
I0302 19:02:07.582237 22683591385216 run.py:483] Algo bellman_ford step 9925 current loss 0.016693, current_train_items 317632.
I0302 19:02:07.597431 22683591385216 run.py:483] Algo bellman_ford step 9926 current loss 0.034413, current_train_items 317664.
I0302 19:02:07.620657 22683591385216 run.py:483] Algo bellman_ford step 9927 current loss 0.032067, current_train_items 317696.
I0302 19:02:07.652103 22683591385216 run.py:483] Algo bellman_ford step 9928 current loss 0.048370, current_train_items 317728.
I0302 19:02:07.681402 22683591385216 run.py:483] Algo bellman_ford step 9929 current loss 0.147344, current_train_items 317760.
I0302 19:02:07.699588 22683591385216 run.py:483] Algo bellman_ford step 9930 current loss 0.000826, current_train_items 317792.
I0302 19:02:07.714388 22683591385216 run.py:483] Algo bellman_ford step 9931 current loss 0.010584, current_train_items 317824.
I0302 19:02:07.737304 22683591385216 run.py:483] Algo bellman_ford step 9932 current loss 0.046189, current_train_items 317856.
I0302 19:02:07.766648 22683591385216 run.py:483] Algo bellman_ford step 9933 current loss 0.042179, current_train_items 317888.
I0302 19:02:07.796717 22683591385216 run.py:483] Algo bellman_ford step 9934 current loss 0.083550, current_train_items 317920.
I0302 19:02:07.814940 22683591385216 run.py:483] Algo bellman_ford step 9935 current loss 0.001349, current_train_items 317952.
I0302 19:02:07.830119 22683591385216 run.py:483] Algo bellman_ford step 9936 current loss 0.005018, current_train_items 317984.
I0302 19:02:07.852746 22683591385216 run.py:483] Algo bellman_ford step 9937 current loss 0.028337, current_train_items 318016.
I0302 19:02:07.882212 22683591385216 run.py:483] Algo bellman_ford step 9938 current loss 0.054715, current_train_items 318048.
I0302 19:02:07.914477 22683591385216 run.py:483] Algo bellman_ford step 9939 current loss 0.120896, current_train_items 318080.
I0302 19:02:07.933032 22683591385216 run.py:483] Algo bellman_ford step 9940 current loss 0.001890, current_train_items 318112.
I0302 19:02:07.948189 22683591385216 run.py:483] Algo bellman_ford step 9941 current loss 0.016257, current_train_items 318144.
I0302 19:02:07.970650 22683591385216 run.py:483] Algo bellman_ford step 9942 current loss 0.041249, current_train_items 318176.
I0302 19:02:08.000747 22683591385216 run.py:483] Algo bellman_ford step 9943 current loss 0.040392, current_train_items 318208.
I0302 19:02:08.030830 22683591385216 run.py:483] Algo bellman_ford step 9944 current loss 0.051057, current_train_items 318240.
I0302 19:02:08.048694 22683591385216 run.py:483] Algo bellman_ford step 9945 current loss 0.005398, current_train_items 318272.
I0302 19:02:08.063686 22683591385216 run.py:483] Algo bellman_ford step 9946 current loss 0.053088, current_train_items 318304.
I0302 19:02:08.086812 22683591385216 run.py:483] Algo bellman_ford step 9947 current loss 0.026307, current_train_items 318336.
I0302 19:02:08.116879 22683591385216 run.py:483] Algo bellman_ford step 9948 current loss 0.026320, current_train_items 318368.
I0302 19:02:08.149766 22683591385216 run.py:483] Algo bellman_ford step 9949 current loss 0.047546, current_train_items 318400.
I0302 19:02:08.168137 22683591385216 run.py:483] Algo bellman_ford step 9950 current loss 0.001962, current_train_items 318432.
I0302 19:02:08.176082 22683591385216 run.py:503] (val) algo bellman_ford step 9950: {'pi': 0.9990234375, 'score': 0.9990234375, 'examples_seen': 318432, 'step': 9950, 'algorithm': 'bellman_ford'}
I0302 19:02:08.176194 22683591385216 run.py:522] Not saving new best model, best avg val score was 1.000, current avg val score is 0.999, val scores are: bellman_ford: 0.999
I0302 19:02:08.192398 22683591385216 run.py:483] Algo bellman_ford step 9951 current loss 0.010432, current_train_items 318464.
I0302 19:02:08.215494 22683591385216 run.py:483] Algo bellman_ford step 9952 current loss 0.013385, current_train_items 318496.
I0302 19:02:08.246536 22683591385216 run.py:483] Algo bellman_ford step 9953 current loss 0.028825, current_train_items 318528.
I0302 19:02:08.279817 22683591385216 run.py:483] Algo bellman_ford step 9954 current loss 0.066370, current_train_items 318560.
I0302 19:02:08.298315 22683591385216 run.py:483] Algo bellman_ford step 9955 current loss 0.004719, current_train_items 318592.
I0302 19:02:08.313941 22683591385216 run.py:483] Algo bellman_ford step 9956 current loss 0.023033, current_train_items 318624.
I0302 19:02:08.337014 22683591385216 run.py:483] Algo bellman_ford step 9957 current loss 0.016832, current_train_items 318656.
I0302 19:02:08.367381 22683591385216 run.py:483] Algo bellman_ford step 9958 current loss 0.056704, current_train_items 318688.
I0302 19:02:08.399974 22683591385216 run.py:483] Algo bellman_ford step 9959 current loss 0.031556, current_train_items 318720.
I0302 19:02:08.418546 22683591385216 run.py:483] Algo bellman_ford step 9960 current loss 0.001358, current_train_items 318752.
I0302 19:02:08.433691 22683591385216 run.py:483] Algo bellman_ford step 9961 current loss 0.030689, current_train_items 318784.
I0302 19:02:08.457027 22683591385216 run.py:483] Algo bellman_ford step 9962 current loss 0.032200, current_train_items 318816.
I0302 19:02:08.487471 22683591385216 run.py:483] Algo bellman_ford step 9963 current loss 0.020051, current_train_items 318848.
I0302 19:02:08.518757 22683591385216 run.py:483] Algo bellman_ford step 9964 current loss 0.058675, current_train_items 318880.
I0302 19:02:08.536945 22683591385216 run.py:483] Algo bellman_ford step 9965 current loss 0.001571, current_train_items 318912.
I0302 19:02:08.552545 22683591385216 run.py:483] Algo bellman_ford step 9966 current loss 0.011157, current_train_items 318944.
I0302 19:02:08.575752 22683591385216 run.py:483] Algo bellman_ford step 9967 current loss 0.051439, current_train_items 318976.
I0302 19:02:08.605423 22683591385216 run.py:483] Algo bellman_ford step 9968 current loss 0.017814, current_train_items 319008.
I0302 19:02:08.636884 22683591385216 run.py:483] Algo bellman_ford step 9969 current loss 0.031088, current_train_items 319040.
I0302 19:02:08.655788 22683591385216 run.py:483] Algo bellman_ford step 9970 current loss 0.001102, current_train_items 319072.
I0302 19:02:08.671443 22683591385216 run.py:483] Algo bellman_ford step 9971 current loss 0.014880, current_train_items 319104.
I0302 19:02:08.693789 22683591385216 run.py:483] Algo bellman_ford step 9972 current loss 0.013602, current_train_items 319136.
I0302 19:02:08.722817 22683591385216 run.py:483] Algo bellman_ford step 9973 current loss 0.031646, current_train_items 319168.
I0302 19:02:08.754169 22683591385216 run.py:483] Algo bellman_ford step 9974 current loss 0.045277, current_train_items 319200.
I0302 19:02:08.772843 22683591385216 run.py:483] Algo bellman_ford step 9975 current loss 0.000595, current_train_items 319232.
I0302 19:02:08.788669 22683591385216 run.py:483] Algo bellman_ford step 9976 current loss 0.024744, current_train_items 319264.
I0302 19:02:08.811194 22683591385216 run.py:483] Algo bellman_ford step 9977 current loss 0.029174, current_train_items 319296.
I0302 19:02:08.840909 22683591385216 run.py:483] Algo bellman_ford step 9978 current loss 0.018346, current_train_items 319328.
I0302 19:02:08.871021 22683591385216 run.py:483] Algo bellman_ford step 9979 current loss 0.021253, current_train_items 319360.
I0302 19:02:08.889467 22683591385216 run.py:483] Algo bellman_ford step 9980 current loss 0.001785, current_train_items 319392.
I0302 19:02:08.904741 22683591385216 run.py:483] Algo bellman_ford step 9981 current loss 0.002842, current_train_items 319424.
I0302 19:02:08.928033 22683591385216 run.py:483] Algo bellman_ford step 9982 current loss 0.050952, current_train_items 319456.
I0302 19:02:08.959505 22683591385216 run.py:483] Algo bellman_ford step 9983 current loss 0.036006, current_train_items 319488.
I0302 19:02:08.991783 22683591385216 run.py:483] Algo bellman_ford step 9984 current loss 0.049948, current_train_items 319520.
I0302 19:02:09.010305 22683591385216 run.py:483] Algo bellman_ford step 9985 current loss 0.000859, current_train_items 319552.
I0302 19:02:09.025563 22683591385216 run.py:483] Algo bellman_ford step 9986 current loss 0.001188, current_train_items 319584.
I0302 19:02:09.048094 22683591385216 run.py:483] Algo bellman_ford step 9987 current loss 0.014315, current_train_items 319616.
I0302 19:02:09.078280 22683591385216 run.py:483] Algo bellman_ford step 9988 current loss 0.022670, current_train_items 319648.
I0302 19:02:09.109164 22683591385216 run.py:483] Algo bellman_ford step 9989 current loss 0.045492, current_train_items 319680.
I0302 19:02:09.127796 22683591385216 run.py:483] Algo bellman_ford step 9990 current loss 0.016478, current_train_items 319712.
I0302 19:02:09.143458 22683591385216 run.py:483] Algo bellman_ford step 9991 current loss 0.013129, current_train_items 319744.
I0302 19:02:09.165563 22683591385216 run.py:483] Algo bellman_ford step 9992 current loss 0.033545, current_train_items 319776.
I0302 19:02:09.194785 22683591385216 run.py:483] Algo bellman_ford step 9993 current loss 0.021243, current_train_items 319808.
I0302 19:02:09.225381 22683591385216 run.py:483] Algo bellman_ford step 9994 current loss 0.038083, current_train_items 319840.
I0302 19:02:09.243748 22683591385216 run.py:483] Algo bellman_ford step 9995 current loss 0.001615, current_train_items 319872.
I0302 19:02:09.259195 22683591385216 run.py:483] Algo bellman_ford step 9996 current loss 0.044842, current_train_items 319904.
I0302 19:02:09.281803 22683591385216 run.py:483] Algo bellman_ford step 9997 current loss 0.016061, current_train_items 319936.
I0302 19:02:09.311646 22683591385216 run.py:483] Algo bellman_ford step 9998 current loss 0.029538, current_train_items 319968.
I0302 19:02:09.340804 22683591385216 run.py:483] Algo bellman_ford step 9999 current loss 0.060298, current_train_items 320000.
I0302 19:02:09.345771 22683591385216 run.py:527] Restoring best model from checkpoint...
I0302 19:02:11.891638 22683591385216 run.py:542] (test) algo bellman_ford : {'pi': 0.94091796875, 'score': 0.94091796875, 'examples_seen': 320000, 'step': 10000, 'algorithm': 'bellman_ford'}
I0302 19:02:11.891861 22683591385216 run.py:544] Done!
